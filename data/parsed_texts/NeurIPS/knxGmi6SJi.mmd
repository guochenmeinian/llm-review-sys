# shapiq: Shapley Interactions for Machine Learning

 Maximilian Muschalik

LMU Munich, MCML

&Hubert Baniecki

University of Warsaw,

Warsaw University of Technology &Fabian Fumagalli

Bielefeld University

&Patrick Kolpaczki

Paderborn University &Barbara Hammer

Bielefeld University &Eyke Hullermeier

LMU Munich, MCML

###### Abstract

Originally rooted in game theory, the Shapley Value (SV) has recently become an important tool in machine learning research. Perhaps most notably, it is used for feature attribution and data valuation in explainable artificial intelligence. Shapley Interactions (SIs) naturally extend the SV and address its limitations by assigning joint contributions to groups of entities, which enhance understanding of black box machine learning models. Due to the exponential complexity of computing SVs and SIs, various methods have been proposed that exploit structural assumptions or yield probabilistic estimates given limited resources. In this work, we introduce shapiq, an open-source Python package that unifies state-of-the-art algorithms to efficiently compute SVs and any-order SIs in an application-agnostic framework. Moreover, it includes a benchmarking suite containing 11 machine learning applications of SIs with pre-computed games and ground-truth values to systematically assess computational performance across domains. For practitioners, shapiq is able to explain and visualize any-order feature interactions in predictions of models, including vision transformers, language models, as well as XGBoost and LightGBM with TreeSHAP-IQ. With shapiq, we extend shap beyond feature attributions and consolidate the application of SVs and SIs in machine learning that facilitates future research. The source code and documentation are available at https://github.com/mmschlk/shapiq.

## 1 Introduction

Assigning _value_ to entities collectively performing a task is essential in various real-world applications of machine learning (ML) [63, 74]. For instance, when reimbursing data providers based on the _value of data_[30, 80], or justifying a model's prediction based on _value of feature information_[13, 18, 19, 55, 77]. The _fair_ distribution of value among a group of entities is a central aspect of cooperative game theory, where the Shapley Value (SV) [76] defines a _unique_ allocation scheme based on intuitive axioms. The SV is applicable to any _game_, i.e. a function that specifies the worth of all possible groups of entities, called coalitions. In ML, application-specific games were introduced [5, 30, 74, 80, 84], which typically require a definition of the overall worth and a notion of entities' absence [19]. The SV fairly distributes the overall worth among individuals by evaluating the game for all coalitions. However, it does not give insights on _synergies or redundancies_ between entities. For instance, while two features such as _latitude_ and _longitude_ convey separate information, only their joint consideration reveals the synergy of encoding an _exact location_. The value of such a group of entities is known as an _interaction_[33], or in this context _feature interaction_[29], and is crucial to understand predictions of complex ML models [20, 48, 49, 61, 66, 78, 82, 86], as illustrated in Figure 1.

Shapley Interactions (SIs) [10, 33, 78, 81] distribute the overall worth to all groups of entities up to a maximum _explanation order_. They satisfy axioms similar to the SV, to which they reduce for individuals, i.e. the lowest explanation order. In contrast, for the highest explanation order, which comprises an interaction for every coalition, the SIs yield the Mobius Interaction (MI), or Mobius transform [27, 35, 72]. The MIs are a fundamental concept in cooperative game theory that captures the _isolated joint contribution_, which allows to additively describe every coalition's worth by a sum of the contained MIs. With an increasing explanation order, the SIs comprise more components that finally yield the MIs as the most comprehensive explanation of the game at the cost of highest complexity [10, 81]. While the SV and SIs provide an appealing theoretical concept, computing them without structural assumptions on the game requires exponential complexity [22]. For tree-based models, it was shown that SVs [54, 87] and SIs [61, 88] can be efficiently computed by exploiting the architecture. Moreover, game-agnostic stochastic approximators estimate the SV [11, 17, 45, 55, 57, 64, 68] and SIs [28, 29, 47, 78, 81] with a limited budget of game evaluations.

Diverse applications of the SV have led to various techniques for its efficient computation [13]. Recently, extensions to any-order SIs addressed limitations of the SV and complemented interpretation of model predictions with higher-order feature interactions [10, 29, 78, 81, 82]. While stochastic approximators are applicable to any game, their evaluation is typically performed in an isolated application [29, 50], such as feature interactions. Moreover, implementing such algorithms requires a strong mathematical background and specific design choices. Existing Python packages, such as shap[55], provide a relatively small number of approximators, which are limited to the SV and feature attributions.

Figure 1: The shapiq Python package facilitates research on game theory for machine learning, including state-of-the-art approximation algorithms and pre-computed benchmarks. It provides a simple interface for explaining predictions of machine learning models beyond feature attributions **(1)**, enabling researchers to explore higher-order interactions or domain-independent game theory **(2)**.

Contribution.In this work, we present shapiq, an open-source _Python_ library for computing any-order SIs. shapiq consolidates research on SVs and SIs, making these tools accessible across various ML domains. Our core contributions are summarized in Figure 1 and include,

1. a general approximation and computation interface for state-of-the-art SI algorithms and methods without focus on a specific application like explanations or data valuation,
2. an explanation API for using SIs to explain ML models and visualizing interactions,
3. a benchmarking suite to evaluate SI approximators across several real-world scenarios,
4. and a cross-domain empirical evaluation of approximators guiding practitioners.

Related software tools and benchmarks.shapiq extends the popular shap[55] Python package beyond feature attributions aiming to fully embrace the application of SVs and SIs in ML. While shap implements a single index for 2-order feature interactions to explain the predictions of tree-based models, shapiq implements a dozen approximators for any-order SIs (Table 2) and offers a benchmarking suite for these algorithms across 10 different domains (Table 3). Related software such as aix360[4], alibi[42] and dalex[6] are general toolboxes offering the implementation and visualization of the most popular ML explanations for end-users. We specify in SIs to provide a comprehensive tool facilitating research in game theory for ML, including the exact computation of 20 interaction indices and game-theoretic concepts (Table 1). Notably, the investigate[3] and captum[44] Python packages offer feature attribution methods specific to (deep) neural networks. Most recently, quantus[36] implements evaluation metrics for these explanation methods.

We build upon recent advances in benchmarking explainable artificial intelligence (XAI) methods such as feature attributions [2; 51; 53; 63] and algorithms for data valuation [39]. XAI-Bench[53] focuses on synthetic tabular data. OpenXAI[2] provides 7 real-world tabular datasets with pre-trained neural network models, 7 feature attribution methods and 8 metrics to compare them. \(\mathcal{M}^{4}\)[51] extends OpenXAI to benchmark feature attributions of deep neural networks for image and text modalities. In [63], the authors benchmark several algorithms for approximating SVs based on the conditional feature distribution. OpenDataVal[39] provides 9 real-world datasets, 11 data valuation methods and 4 metrics to compare them. shapiq puts more focus on benchmarking higher-order SI algorithms and provides an interface to state-of-the-art explanation methods that base on SIs, e.g. TreeSHAP-IQ[61]. While open data repositories such as OpenML[9] offer easy access to datasets for ML, we pre-compute and share ground-truth SIs for various games (i.e. dataset-model pairs) that saves considerate time and resources when benchmarking approximation algorithms.

## 2 Theoretical Background

In ML, various concepts are based on synergies of entities to optimize performance in a given task. For example, weak learners construct powerful model ensembles [73], collected data instances and features are used to train supervised ML models [16; 30], where feature values collectively predict outputs. To better understand such processes, XAI quantifies the contributions of these entities to the task, most prominently for feature values in predictions (local feature attribution [55; 77]), features in models (global feature importance [16; 17; 69]), and data instances in model training (data valuation [30]). Assigning such contributions is closely related to the field of cooperative game theory [74], which studies the notion of value for players that collectively obtain a payout. To adequately assess the impact of individual players, it is necessary to analyze the payout for different coalitions. More formally, a cooperative game \(\nu:\mathcal{P}(N)\rightarrow\mathbb{R}\) with \(\nu(\emptyset)=0\) is defined by a _value function_ on the power set of \(N:=\{1,\dots,n\}\) entities, which describes such payouts for all possible coalitions of players. We later summarize such prominent examples in the context of ML in Table 3. Here, we summarize existing contribution concepts for individuals and groups of entities, outlined in Table 1.

The **SV**[76] and **Banzhaf Value (BV)**[8] are instances of _semivalues_[23]. Semivalues assign contributions to individual players and adhere to intuitive axioms: _Linearity_ enforces linearly composed contributions for linearly composed games; _Dummy_ requires that players without impact receive zero contribution; _Symmetry_ enforces that entities contributing equally to the payout receive equal value. The SV [76] is the unique semivalue that additionally satisfies _efficiency_, i.e. the sum of all contributions yields the total payout \(\nu(N)\). In contrast, the BV [8] is the unique semivalue that additionally satisfies _2-Efficiency_, i.e. the contributions of two players sum to the contribution of a joint player in a reduced game, where both players are merged. The SV and BV are represented as a weighted average over _marginal contributions_\(\Delta_{i}(T):=\nu(T\cup\{i\})-\nu(T)\) for \(i\in N\) as

\[\phi^{\text{SV}}(i):=\sum_{T\subseteq N\setminus\{i\}}\frac{1}{n \binom{n-1}{|T|}}\Delta_{i}(T)\qquad\text{ and }\qquad\phi^{\text{BV}}(i):=\sum_{T\subseteq N\setminus\{i\}}\frac{1}{2^{n-1}} \Delta_{i}(T)\,.\]

In ML applications, the SV is typically preferred over the BV due to the efficiency axiom [74]. For instance, in local feature attribution, the SV is utilized to fairly distribute the model's prediction to individual features [55, 77]. However, it was shown that the SV is limited when explaining complex decision systems, and _feature interactions_, i.e. the joint contributions of features' groups, are required to understand such processes [20, 29, 48, 49, 61, 66, 78, 81, 82, 86].

The **Generalized Value (GV)**[58] and **Interaction Index (II)**[27] are two paradigms to extend the notion of value to groups of entities. The GVs are based on weighted averages over (joint) marginal contributions \(\nu(T\cup S)-\nu(T)\) for \(S\subseteq N\) given \(T\subseteq N\setminus S\). In contrast, IIs are based on discrete derivatives that account for lower-order effects of subsets of \(S\). For instance, for two players \(i,j\in N\), the discrete derivative \(\Delta_{\{i,j\}}(T)\) is defined as the joint marginal contribution \(\nu(T\cup\{i,j\})-\nu(T)\) minus the individual marginal contributions \(\Delta_{i}(T)\) and \(\Delta_{j}(T)\). More generally, the _discrete derivative_\(\Delta_{S}(T)\) for \(S\subseteq N\) in the presence of \(T\subseteq N\setminus S\) is defined as

\[\Delta_{S}(T):=\sum_{L\subseteq S}(-1)^{|S|-|L|}\nu(T\cup L)\ \ \ \text{with}\ \ \Delta_{S}(T)=\underbrace{\nu(T\cup S)-\nu(T)}_{\text{joint marginal contribution}}-\sum_{\emptyset\neq L\subset S}\underbrace{\Delta_{L}(T)}_{\text{lower-order effects}}\,.\]

A positive value indicates synergy, whereas a negative value indicates redundancy of \(S\) given \(T\). Lastly, a zero value indicates (additive) independence, i.e. the joint marginal contribution is equal to the sum of all lower-order effects. GVs and IIs are uniquely represented [27, 58] by

\[\phi^{\text{GV}}(S):=\sum_{T\subseteq N\setminus S}p_{|T|}^{|S|}(n)\left(\nu (T\cup S)-\nu(T)\right)\quad\text{ and }\quad\phi^{\text{II}}(S):=\sum_{T\subseteq N\setminus S}p_{|T|}^{|S|}(n) \Delta_{S}(T)\,.\]

The most prominent examples are the _Shapley GV (SGV)_[59] and the _Shapley II (SII)_[33] with \(p_{t}^{s}(n)=\left((n-s+1)\binom{n-s}{t}\right)^{-1}\), which naturally extend the SV (cf. Appendix A.1). While the SGV and SII are natural extensions to the SV, they are not suitable for interpretability, since they are defined on the powerset and comprise an exponential number of components. Moreover, neither GVs nor IIs satisfy the efficiency axiom for higher-orders, which is desirable for ML applications.

Shapley Interactions (SIs) for Machine Learningassign joint contribution \(\Phi_{k}\) up to an _explanation order_\(k\), i.e. for all coalitions \(S\subseteq N\) with \(|S|\leq k\), which satisfy generalized _efficiency_\(\nu(N)=\sum_{S\subseteq N,|S|\leq k}\Phi_{k}(S)\,.\) The _\(k\)-SVs (\(k\)-SIIs)_[10] are the unique SIs that coincide with SII for the highest order. The _Shapley Taylor II (STII)_[78] puts a stronger emphasis on the top-order interactions, and _Faithful SII (FSII)_[81] optimizes _Shapley-weighted faithfulness_

\[\mathcal{L}(\nu,\Phi_{k}):=\sum_{T\subseteq N}\mu(t)\left(\nu(T)- \sum_{S\subseteq T,|S|\leq k}\Phi_{k}(S)\right)^{2}\ \text{with}\ \mu(t):=\begin{cases}\mu_{\infty}&\text{if}\ t\in\{0,n\}\\ \frac{1}{\binom{n-1}{t-1}}&\text{else}\end{cases}\,.\]

\begin{table}
\begin{tabular}{l l|l|l} \hline \hline
**Setting** & **Interaction Index (II)**[27] & **Base Semivalue**[23] & **Generalized Value (GV)**[58] \\ \hline \multirow{4}{*}{**Machine Learning**} & \(k\)**-Shapley Values (\(k\)-SII)**[10] & \multirow{4}{*}{**Shapley (SV)**[76]} & \multirow{4}{*}{Joint SV [34]} \\  & **Shapley Taylor II (STII)**[78] & & \\  & **Faithful Shapley II (FSII)**[81] & & \\  & \(k_{\text{ADD}}\)-SHAP [68] & & \\  & Faithful Banzhaf II (FBII) [81] & Banzhaf (BV) [8] & – \\ \hline \multirow{4}{*}{**Game Theory**} & **Möbius (MI)**[27, 35, 72] & \multirow{4}{*}{–} & \multirow{4}{*}{–} & Internal GV (IGV) [58] \\  & Co-Möbius (Co-MI) [32] & & & External GV (EGV) [58] \\ \cline{1-1}  & Shapley II (SII) [33] & & Shapley GV (SGV) [59] \\ \cline{1-1}  & Chaining II (CHII) [60] & & & Chaining GV (CHGV) [58] \\ \cline{1-1}  & Banzhaf II (BII) [33] & & Banzhaf (BV) [8] & Banzhaf GV (BGV) [60] \\ \hline \hline \end{tabular}
\end{table}
Table 1: Available concepts in the ExactComputer class in shapiq with **SIs in bold**.

[MISSING_PAGE_FAIL:5]

interface offering approximation performance increases through sampling procedures like the _border_- and _pairing-tricks_ introduced in [17; 29].

Exact computer.A key functionality of shapiq lies in computing the SIs _exactly_, which is feasible for smaller games, but reaches its limit for growing player numbers. The shapiq.ExactComputer class provides an interface for computing 20 interaction indices and game-theoretic concepts, including the MI, BV, SGV, among others (see Table 1).

Games.Approximators and computers require the specification of a _cooperative game_. Games make up a central level of abstraction in shapiq, and specifying a game only requires the implementation of a domain-specific value function. Table 3 describes in detail 11 benchmark games implemented in shapiq. Beyond synthetic games, our benchmark spans the 5 most prominent domains where SIs can be applied for ML. The shapiq.Game class can be easily extended to include future benchmarks in the package. We pre-compute and share exact SIs for \(2\,042\) benchmark game configurations in total (see Appendix B), facilitating future work on improving the approximators, which we elaborate on further in Section 4.

### Explaining Machine Learning Predictions with shapiq

In addition to facilitating theoretical advancements, shapiq also provides practical tools for applying SIs in ML. These tools streamline the process of explaining feature interactions in model predictions, allowing researchers and practitioners to easily compute and visualize interaction effects across a range of models and data types.

Explainer.The shapiq.Explainer class is a simplified interface to explain any-order feature interactions in ML models. Figure 2 goes through exemplary code used to approximate SIs for a single prediction and visualize them on a graph plot. Currently three classes are further distinguished within the API, but we envision extending shapiq.Explainer to include more data modalities

Figure 3: **Left:** Exemplary code for globally explaining multiple model’s predictions with shapiq. **Right:** Global feature interaction importance visualized as a bar plot.

Figure 2: **Left:** Exemplary code for locally explaining a single model’s prediction with shapiq. **Right:** Local feature interactions visualized on a network plot.

and model algorithms. shapiq.TabularExplainer allows for model-agnostic explanation based on feature marginalization with baseline, marginal, or conditional imputation (refer to Appendix C for details). shapiq.TreeExplainer implements TreeSHAP-IQ [61] for efficient explanations specific to decision tree-based models, e.g. random forest or gradient boosting decision trees, with native support for scikit-learn [67], xgboost [15], and lightgbm [40]. Figure 3 goes through exemplary code for explaining a set of predictions and visualizing their aggregation in a bar plot, which represents the global feature interaction importance. Lastly, shapiq.TabPFNLExplainer is a special case of the shapiq.TabularExplainer handling explanations for the TabPFN [37] model architecture akin to [75] with a _remove-and-recontextualize_ paradigm.

Visualizing interactions.shapiq supports the plotting and analysis of interaction values with different visualizations techniques. shapiq.plot offers custom visualizations including our custom network and SI graph plot, but also wraps established visualizations from shap [55] like the force and bar plots. For a detailed guide and summary of the visualizations, we kindly refer to Appendix D.

Utility functions.shapiq offers additional useful tools that are described in detail in the documentation. Interaction values are stored and processed using the shapiq.InteractionValues data class, which is rich in utility functions. Notably, useful set-based operators and generators exist for handling player sets \(S\subseteq N\) or iterating over power sets \(\mathcal{P}(N)\) of certain sizes with shapiq.powerset. Finally, shapiq.datasets loads datasets used for testing and examples.

## 4 Benchmarking Analysis

The shapiq library enables computation of various SIs for a broad class of application domains. To illustrate its versatility, we conduct benchmarks across a wide variety of traditional ML-based SV application scenarios. The ML benchmark demonstrates how higher-order SIs enable an accuracy-complexity trade-off for model interpretability (Section 4.1) and highlights that different approximation techniques in shapiq achieve the state-of-the-art performance depending on the application domains (Section 4.2). Tables 3, 4 and 5 present an overview of different application domains and associated benchmarks. Depending on the benchmark, it can be instantiated with different datasets, models, player numbers or benchmark-specific configuration parameters, e.g. uncertainty type: _epistemic_ for Uncertainty Explanation or imputer: _conditional_ for Local Explanation. In total, shapiq offers 100 unique benchmark games, i.e. applications times dataset-model pairs.

For all games that include \(n\leq 16\) players, the value functions have been pre-computed by evaluating all coalitions and storing the games to file. Reading a pre-computed game from file, instead of performing up to \(2^{16}=65\,536\) value function calls with each new experiment run, saves valuable computational time and contributes to reproducibility as well as sustainability. This is particularly

\begin{table}
\begin{tabular}{c l l l l} \hline \hline
**Domain** & **Benchmark (Game)** & **Source** & **Players** & **Coalition Worth** \\ \hline \multirow{4}{*}{**XAI**} & Local Explanation & [55, 77] & Features & Model Output \\  & Global Explanation & [18] & Features & Model Loss \\  & Tree Explanation & [54, 61] & Features & Model Output \\ \multirow{2}{*}{**Uncertainty**} & Uncertainty Explanation & [84] & Features & Prediction Entropy \\  & Feature Selection & [16, 69] & Features & Performance \\  & Ensemble Selection & [73] & Weak Learners & Performance \\  & RF Ensemble Selection & [73] & Tree Models & Performance \\ \multirow{2}{*}{**Valuation**} & Data Valuation & [30] & Data Points & Performance \\  & Dataset Valuation & [80] & Data Subsets & Performance \\ \multirow{2}{*}{**Unsupervised Learning**} & Cluster Explanation & – & Features & Cluster Score \\  & Unsupervised Feature Importance & [5] & Features & Total Correlation \\ \hline
**Synthetic** & Sum of Unanimity Model & [28, 81] & Players & Sum of Unanimous Votes \\ \hline \hline \end{tabular}
\end{table}
Table 3: Overview of the available benchmark games and domains in shapiq. Each benchmark can be instantiated with different datasets, models, player sizes, or benchmark-specific configuration parameters. This results in \(2\,042\) pre-computed individual configurations (see Tables 4 and 5).

beneficial for tasks that involve remove-and-refit strategies [19], such as Data Valuation or Feature Selection. For \(n>16\), where pre-computing a game and ground truth values becomes computationally prohibitive, we rely on analytical solutions to compute the ground truth like TreeSHAP-IQ [61] for tree-based ensembles or the MI representation of Sum of Unanimity Models [28; 29; 81]. For details regarding the experimental setting and reproducibility, we refer to Appendix E.

### Faithfulness and Complexity of Shapley Interactions

In this experiment, we empirically assess how _faithfully_ lower-order SIs representations capture higher-order effects for varying explanation orders \(k\) and choice of interaction index. To this end, we rely on the Shapley-weighted faithfulness \(\mathcal{L}(\nu,\Phi_{k})\) introduced in Section 2. The complexity of SIs ranges from SVs (least complex) to MIs (most complex), where SVs minimize \(\mathcal{L}(\nu,\Phi_{1})\) for \(k=1\)[12; 55], while MIs perfectly capture all game values with \(\mathcal{L}(\nu,\Phi_{n})=0\) for \(k=n\)[10; 81]. To quantify the faithfulness of SIs across different domains, we calculate interaction values for a given index \(\Phi_{k}\) and explanation order \(k\). We then approximate the game values for each subset \(T\subseteq N\) of players as \(\hat{\nu}_{k}(T):=\sum_{S\subseteq T:|S|\leq k}\Phi_{k}(S)\) and compare them to the ground truths using \(\mathcal{L}(\nu,\Phi_{k})\) through a Shapley-weighted \(R^{2}\left(\nu(T),\hat{\nu}_{k}(T)\right)\) for all \(T\subseteq N\). For context, a game with no interactions (a 1-additive game) will be perfectly reproduced by a 1-additive explanation, yielding \(\mathcal{L}(\nu,\Phi_{1})=0\) and \(R^{2}=1\). Conversely, games with substantial higher-order interactions will result in larger errors for lower-order explanations, with \(\mathcal{L}(\nu,\Phi_{k})\gg 0\) and \(R^{2}<1\).

Figure 4 shows the Shapley-weighted \(R^{2}\) value for \(k=1,\ldots,n\) for a synthetic game with a single interaction of varying size (a) and real-world ML applications (b). Here, we used \(\mu_{\infty}=1\) instead of \(\mu_{\infty}\gg 1\), which affects FBII that violates efficiency. The results show that in general SIs become more faithful with higher explanation order. Notably, the difference between pairwise SIs and SVs (SHAP textbox) is remarkable, where pairwise interactions (\(k=2\)) already yield a strong improvement in faithfulness. If higher-order interactions dominate, then SIs require a larger explanation order to maintain faithfulness. While FSII and FBII are optimized for faithfulness, STII and \(k\)-SII do not necessarily yield a strict improvement in this metric. In fact, it was shown that SII and \(k\)-SII optimize a slightly different faithfulness metric, which changes for every order [28]. Yet, we observe a consistent strong improvement of pairwise \(k\)-SII over the SV (SHAP). While FSII and FBII optimize faithfulness, \(k\)-SII and STII adhere to strict structural assumptions, where STII projects all higher-order interactions to the top-order SIs, and \(k\)-SII is consistent with SII. Practitioners may choose SIs tailored to their specific application, where \(k\)-SII is a good default choice for shapiq.

### Comparison of Approximation Methods

Various approximation methods for computing SIs are included in shapiq for a variety of SIs (cf. Table 2). The possibility of attributing (domain-specific) state-of-the-art performance to a single algorithm has been investigated empirically by multiple works [28; 29; 45; 46; 47; 57; 64; 81; 83; 89]. We use the collection of 100 unique benchmark games in shapiq to evaluate the performance of different SV and SI approximation methods on a broad spectrum of ML applications. For each domain and configuration (see Table 4 and 5 in Appendix B), we compute ground truth SVs, \(2\)-SIIs,

Figure 4: Shapley-weighted \(R^{2}\) of interaction indices by explanation order for **(a)** single synthetic interactions and **(b)** ML applications. FSII is optimized for this metric, and increases faithfulness with each order. Interactions improve faithfulness over SHAP and yield an exact decomposition for the highest order. However, increasing interaction size negatively affects faithfulness.

and \(3\)-SIIs and compare them with estimates provided by all approximators from Table 2. The approximators are run with a wide range of budget values and assessed by their achieved mean squared error (MSE) or precision at five (Precision@5). Figure 5 summarizes the approximation results.

Most notably, the ranking of approximators varies strongly between the different applications domains, which is depicted in Figure 5 (a) and (b). This observation holds for both SVs and SIs. In general, two types of approximation methods dominate the application landscape in terms of MSE and Precision@5. First, _kernel-based_ approaches including KernelSHAP, \(k_{\text{ADD}}\)-SHAP, KernelSHAP-IQ and Inconsistent KernelSHAP-IQ perform best for Local Explanation, Uncertainty Explanation, and Unsupervised Feature Importance. Second, the two _stratification-based_ estimators SVARM and SVARM-IQ achieve state-of-the-art performance for Data Valuation, Dataset Valuation, or Ensemble Selection. Traditional _mean-estimation_ methods including Permutation Sampling (SV and SII), Unbiased KernelSHAP, SHAPIQ, and Owen Sampling achieve moderate estimation qualities in comparison. Our findings give rise to the conclusion that _stratification-based_ estimators perform superior in settings where the size of a coalition naturally impacts its worth (e.g. training size for Dataset Valuation), which is plausible as these methods group coalitions by size and thus leverage this dependency. Meanwhile, _kernel-based_ estimators achieve state-of-the-art in settings where the dependency between size and worth of a coalition is less pronounced (e.g. sudden jumps of model predictions in Local Explanation).

Interestingly, the settings where _stratified-sampling_ outperforms _kernel-based_ variants exhibit different internal structures in the games' MIs. Generally, MIs disentangle a game into all of its additive components (cf. Section 2) and can be computed exactly with shapiq's pre-computed games. The accuracy of _kernel-based_ estimators drops when higher-order interactions dominate the games' structure instead of lower-order interactions. This is depicted by Figure 5 (c) where the MIs for Local Explanation are of lower order than the Data Valuation games.

Figure 5: Overview of the benchmark results containing **(a)** budget-dependent MSE approximation curves on different benchmark settings, **(b)** a summary of the best performing approximators per setting over all 100 benchmark games measured by MSE (left) and Precision@5 (right), and **(c)** exemplary MIs for ten games of Data Valuation (top) and Local Explanation (bottom).

Conclusion

As SIs are increasingly employed to analyze ML models, it becomes pivotal to ensure that these are accurately and efficiently approximated. To this end, we contributed shapiq, an open-source toolbox that implements state-of-the-art algorithms, defines a dozen of benchmark games, and provides ready-to-use explanations of any-order feature interactions. shapiq contains a comprehensive documentation and is designed to be extendable by contributors.

Limitations and future work.We identify three main limitations of shapiq that provide natural opportunities for future work. First, the TreeSHAP-IQ algorithm is currently implemented in Python, but by-design requires no access to model inference, which allows for a more efficient implementation in C++ alike TreeSHAP [54, 87]. Second, SIs can be misinterpreted based on choosing the wrong index for the application scenario, which we comment on across Sections 2 and 4.1. The selection of a particular SI index, enabled by shapiq, offers great opportunities for application-specific research. We also acknowledge that visualization of higher-order feature interactions is itself challenging and a potential research direction in human-computer interaction. Certainly, a human-centric evaluation of explanations may be required for their broader adoption in practical applications [71].

Broader impact.A potential negative societal implication of visualizing higher-order feature interactions may be an _information overload_[7, 70] that leads to users misinterpreting model explanations. Nevertheless, we hope our contribution sparks the advancement of game-theoretical indices motivated by various applications in ML. Specifically in the context of explainability, shapiq may impact the way users interact with ML models when having access to previously inaccessible information, e.g. higher-order feature interactions.

## Acknowledgments and Disclosure of Funding

We gratefully thank the anonymous reviewers for their valuable feedback for improving this work. We also gratefully thank Santo Thies for supporting the implementation. Fabian Fumagalli and Maximilian Muschalik gratefully acknowledge funding by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation): TRR 318/1 2021 - 438445824. Hubert Baniecki was supported from the state budget within the Polish Ministry of Education and Science program "Pearls of Science" project number PN/01/0087/2022. Patrick Kolpaczki was supported by the research training group Dataninja (Trustworthy AI for Seamless Problem Solving: Next Generation Intelligence Joins Robust Data Analysis) funded by the German federal state of North Rhine-Westphalia.

## References

* A. Aas, M. Jullum, and A. Loland (2021)Explaining individual predictions when features are dependent: more accurate approximations to Shapley values. Artificial Intelligence298, pp. 103502. Cited by: SS1.
* C. Agarwal, S. Krishna, E. Saxena, M. Pawelczyk, N. Johnson, I. Puri, M. Zitnik, and H. Lakkaraju (2022)OpenXAI: towards a transparent evaluation of model explanations. In Proceedings of Advances in Neural Information Processing Systems (NeurIPS), Cited by: SS1.
* M. Alber, S. Lapuschkin, P. Seegerer, M. Hagele, K. T. Schutt, G. Montavon, W. Samek, K. Muller, S. Dahne, and P. Kindermans (2019)iNNvestigate neural networks!. Journal of Machine Learning Research20 (93), pp. 1-8. Cited by: SS1.
* V. Arya, R. K. Elamy, P. Chen, A. Dhurandhar, M. Hind, S. C. Hoffman, S. Houde, Q. V. Liao, R. Luss, A. Moysilovic, S. Mourad, P. Pedemonte, R. Raghavendra, J. T. Richards, P. Sattigeri, K. Singh, M. Varshney, D. Welsh, and Y. Zhang (2020)AI explainability 360: an extensible toolkit for understanding data and machine learning models. Journal of Machine Learning Research21 (130), pp. 1-6. Cited by: SS1.
* C. Balestra, F. Huber, A. Mayr, and E. Muller (2022)Unsupervised features ranking via coalitional game theory for categorical data. In Proceedings of Big Data Analytics and Knowledge Discovery (DaWaK), pp. 97-111.

* Baniecki et al. [2021] Baniecki, H., Kretowicz, W., Piatyszek, P., Wisniewski, J., and Biecek, P. (2021). dalex: Responsible machine learning with interactive explainability and fairness in Python. _Journal of Machine Learning Research_, 22(214):1-7.
* Baniecki et al. [2024] Baniecki, H., Parzych, D., and Biecek, P. (2024). The grammar of interactive explanatory model analysis. _Data Mining and Knowledge Discovery_, 38:2596-2632.
* Banzhaf III [1964] Banzhaf III, J. F. (1964). Weighted voting doesn't work: A mathematical analysis. _Rutgers Law Review_, 19:317.
* Bischl et al. [2021] Bischl, B., Casalicchio, G., Feurer, M., Gijsbers, P., Hutter, F., Lang, M., Mantovani, R. G., van Rijn, J. N., and Vanschoren, J. (2021). OpenML benchmarking suites. In _Proceedings of Advances in Neural Information Processing Systems (NeurIPS)_.
* Bordt and von Luxburg [2023] Bordt, S. and von Luxburg, U. (2023). From Shapley Values to Generalized Additive Models and back. In _Proceedings of the International Conference on Artificial Intelligence and Statistics (AISTATS)_, pages 709-745.
* Castro et al. [2009] Castro, J., Gomez, D., and Tejada, J. (2009). Polynomial calculation of the Shapley value based on sampling. _Computers & Operations Research_, 36(5):1726-1730.
* Charnes et al. [1988] Charnes, A., Golany, B., Keane, M., and Rousseau, J. (1988). _Extremal Principle Solutions of Games in Characteristic Function Form: Core, Chebychev and Shapley Value Generalizations_, volume 11, page 123-133. Springer Netherlands.
* Chen et al. [2023] Chen, H., Covert, I., Lundberg, S., et al. (2023). Algorithms to estimate Shapley value feature attributions. _Nature Machine Intelligence_, 5:590-601.
* Chen et al. [2020] Chen, H., Janizek, J. D., Lundberg, S. M., and Lee, S. (2020). True to the Model or True to the Data? _CoRR_, 2006.16234.
* Chen and Guestrin [2016] Chen, T. and Guestrin, C. (2016). XGBoost: A scalable tree boosting system. In _Proceedings of International Conference on Knowledge Discovery and Data Mining (KDD)_, pages 785-794.
* Cohen et al. [2007] Cohen, S. B., Dror, G., and Ruppin, E. (2007). Feature selection via coalitional game theory. _Neural Computing_, 19(7):1939-1961.
* Covert and Lee [2021] Covert, I. and Lee, S. (2021). Improving KernelSHAP: Practical Shapley Value Estimation Using Linear Regression. In _Proceedings of the International Conference on Artificial Intelligence and Statistics (AISTATS)_, pages 3457-3465.
* Covert et al. [2020] Covert, I., Lundberg, S. M., and Lee, S. (2020). Understanding global feature contributions with additive importance measures. In _Proceedings of Advances in Neural Information Processing Systems (NeurIPS)_.
* Covert et al. [2021] Covert, I., Lundberg, S. M., and Lee, S. (2021). Explaining by Removing: A Unified Framework for Model Explanation. _Journal of Machine Learning Research_, 22(209):1-90.
* Deng et al. [2022] Deng, H., Ren, Q., Zhang, H., and Zhang, Q. (2022). Discovering and explaining the representation bottleneck of DNNNS. In _Proceedings of the International Conference on Learning Representations (ICLR)_.
* Deng et al. [2009] Deng, J., Dong, W., Socher, R., Li, L., Li, K., and Fei-Fei, L. (2009). Imagenet: A large-scale hierarchical image database. In _Proceedings of IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 248-255.
* Deng and Papadimitriou [1994] Deng, X. and Papadimitriou, C. H. (1994). On the complexity of cooperative solution concepts. _Mathematics of Operations Research_, 19(2):257-266.
* Dubey et al. [1981] Dubey, P., Neyman, A., and Weber, R. J. (1981). Value theory without efficiency. _Mathematics of Operations Research_, 6(1):122-128.
* Fanaee-T and Gama [2014] Fanaee-T, H. and Gama, J. (2014). Event Labeling Combining Ensemble Detectors and Background Knowledge. _Progress in Artificial Intelligence_, 2(2):113-127.

* Feurer et al. [2020] Feurer, M., van Rijn, J. N., Kadra, A., Gijsbers, P., Mallik, N., Ravi, S., Mueller, A., Vanschoren, J., and Hutter, F. (2020). OpenML-Python: an extensible Python API for OpenML. _CoRR_, abs/1911.02490.
* Frye et al. [2021] Frye, C., de Mijolla, D., Begley, T., Cowton, L., Stanley, M., and Feige, I. (2021). Shapley explainability on the data manifold. In _Proceedings of the International Conference on Learning Representations (ICLR)_.
* Fujimoto et al. [2006] Fujimoto, K., Kojadinovic, I., and Marichal, J. (2006). Axiomatic characterizations of probabilistic and cardinal-probabilistic interaction indices. _Games and Economic Behavior_, 55(1):72-99.
* Fumagalli et al. [2024] Fumagalli, F., Muschalik, M., Kolpaczki, P., Hullermeier, E., and Hammer, B. (2024). Kernelshap-iq: Weighted least square optimization for shapley interactions. In _Forty-first International Conference on Machine Learning, (ICML 2024)_.
* Fumagalli et al. [2023] Fumagalli, F., Muschalik, M., Kolpaczki, P., Hullermeier, E., and Hammer, B. E. (2023). SHAP-IQ: Unified approximation of any-order shapley interactions. In _Proceedings of Advances in Neural Information Processing Systems (NeurIPS)_.
* Ghorbani and Zou [2019] Ghorbani, A. and Zou, J. Y. (2019). Data shapley: Equitable valuation of data for machine learning. In _Proceedings of the International Conference on Machine Learning (ICML)_, pages 2242-2251.
* Grabisch [2016] Grabisch, M. (2016). _Set Functions, Games and Capacities in Decision Making_, volume 46. Springer International Publishing Switzerland.
* Grabisch et al. [2000] Grabisch, M., Marichal, J., and Roubens, M. (2000). Equivalent representations of set functions. _Mathematics of Operations Research_, 25(2):157-178.
* Grabisch and Roubens [1999] Grabisch, M. and Roubens, M. (1999). An axiomatic approach to the concept of interaction among players in cooperative games. _International Journal of Game Theory_, 28(4):547-565.
* Harris et al. [2022] Harris, C., Pymar, R., and Rowat, C. (2022). Joint shapley values: a measure of joint feature importance. In _Proceedings of the International Conference on Learning Representations (ICLR)_.
* Harsanyi [1963] Harsanyi, J. C. (1963). A simplified bargaining model for the n-person cooperative game. _International Economic Review_, 4(2):194-220.
* Hedstrom et al. [2023] Hedstrom, A., Weber, L., Krakowczyk, D., Bareeva, D., Motzkus, F., Samek, W., Lapuschkin, S., and Hohne, M. M.-C. (2023). Quantus: An explainable AI toolkit for responsible evaluation of neural network explanations and beyond. _Journal of Machine Learning Research_, 24(34):1-11.
* Hollmann et al. [2025] Hollmann, N., Muller, S., Purucker, L., Krishnakumar, A., Korfer, M., Hoo, S. B., Schirrmeister, R. T., and Hutter, F. (2025). Accurate predictions on small data with a tabular foundation model. _Nature_, 637(8045):319-326.
* Inglis et al. [2022] Inglis, A., Parnell, A., and Hurley, C. B. (2022). Visualizing Variable Importance and Variable Interaction Effects in Machine Learning Models. _Journal of Computational and Graphical Statistics_, 31(3):766-778.
* Jiang et al. [2023] Jiang, K. F., Liang, W., Zou, J., and Kwon, Y. (2023). OpenDataVal: a unified benchmark for data valuation. In _Proceedings of Advances in Neural Information Processing Systems (NeurIPS)_.
* Ke et al. [2017] Ke, G., Meng, Q., Finley, T., Wang, T., Chen, W., Ma, W., Ye, Q., and Liu, T.-Y. (2017). LightGBM: A highly efficient gradient boosting decision tree. In _Proceedings of Advances in Neural Information Processing Systems (NeurIPS)_, pages 3149-3157.
* Kelley Pace and Barry [1997] Kelley Pace, R. and Barry, R. (1997). Sparse spatial autoregressions. _Statistics & Probability Letters_, 33(3):291-297.
* Klaise et al. [2021] Klaise, J., Looveren, A. V., Vacanti, G., and Coca, A. (2021). Alibi explain: Algorithms for explaining machine learning models. _Journal of Machine Learning Research_, 22(181):1-7.

* Kohavi [1996] Kohavi, R. (1996). Scaling up the accuracy of naive-bayes classifiers: A decision-tree hybrid. In _Proceedings of International Conference on Knowledge Discovery and Data Mining (KDD)_, pages 202-207.
* Kokhlikyan et al. [2020] Kokhlikyan, N., Miglani, V., Martin, M., Wang, E., Alsallakh, B., Reynolds, J., Melnikov, A., Kliushkina, N., Araya, C., Yan, S., and Reblitz-Richardson, O. (2020). Captum: A unified and generic model interpretability library for PyTorch. _arXiv preprint arXiv:2009.07896_.
* Kolpaczki et al. [2024a] Kolpaczki, P., Bengs, V., Muschalik, M., and Hullermeier, E. (2024a). Approximating the shapley value without marginal contributions. In _Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)_, pages 13246-13255.
* Kolpaczki et al. [2024b] Kolpaczki, P., Haselbeck, G., and Hullermeier, E. (2024b). How much can stratification improve the approximation of shapley values? In _Proceeedings of World Conference on Explainable Artifical Intelligence (xAI)_, pages 489-512.
* Kolpaczki et al. [2024c] Kolpaczki, P., Muschalik, M., Fumagalli, F., Hammer, B., and Hullermeier, E. (2024c). SVARM-IQ: efficient approximation of any-order shapley interactions through stratification. In _Proceedings of the International Conference on Artificial Intelligence and Statistics (AISTATS)_, pages 3520-3528.
* Kumar et al. [2021] Kumar, I., Scheidegger, C., Venkatasubramanian, S., and Friedler, S. A. (2021). Shapley Residuals: Quantifying the limits of the Shapley value for explanations. In _Proceedings of Advances in Neural Information Processing Systems (NeurIPS)_, pages 26598-26608.
* Kumar et al. [2020] Kumar, I. E., Venkatasubramanian, S., Scheidegger, C., and Friedler, S. A. (2020). Problems with Shapley-value-based explanations as feature importance measures. In _Proceedings of the International Conference on Machine Learning (ICML)_, pages 5491-5500.
* Li and Yu [2024] Li, W. and Yu, Y. (2024). Faster approximation of probabilistic and distributional values via least squares. In _Proceedings of the International Conference on Learning Representations (ICLR)_.
* Li et al. [2023] Li, X., Du, M., Chen, J., Chai, Y., Lakkaraju, H., and Xiong, H. (2023). \(\mathcal{M}^{4}\): A unified XAI benchmark for faithfulness evaluation of feature attribution methods across metrics, modalities and models. In _Proceedings of Advances in Neural Information Processing Systems (NeurIPS)_.
* Lin et al. [2023] Lin, C., Covert, I., and Lee, S.-I. (2023). On the robustness of removal-based feature attributions. In _Proceedings of Advances in Neural Information Processing Systems (NeurIPS)_.
* Liu et al. [2021] Liu, Y., Khandagale, S., White, C., and Neiswanger, W. (2021). Synthetic benchmarks for scientific research in explainable machine learning. In _Proceedings of Advances in Neural Information Processing Systems (NeurIPS)_.
* Lundberg et al. [2020] Lundberg, S. M., Erion, G. G., Chen, H., DeGrave, A. J., Prutkin, J. M., Nair, B., Katz, R., Himmelfarb, J., Bansal, N., and Lee, S. (2020). From local explanations to global understanding with explainable AI for trees. _Nature Machine Intelligence_, 2(1):56-67.
* Lundberg and Lee [2017] Lundberg, S. M. and Lee, S. (2017). A Unified Approach to Interpreting Model Predictions. In _Proceedings of Advances in Neural Information Processing Systems (NeurIPS)_, pages 4765-4774.
* Maas et al. [2011] Maas, A. L., Daly, R. E., Pham, P. T., Huang, D., Ng, A. Y., and Potts, C. (2011). Learning word vectors for sentiment analysis. In _Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (HLT)_, pages 142-150.
* Maleki et al. [2013] Maleki, S., Tran-Thanh, L., Hines, G., Rahwan, T., and Rogers, A. (2013). Bounding the estimation error of sampling-based shapley value approximation with/without stratifying. _CoRR_, abs/1306.4265.
* Marichal et al. [2007] Marichal, J., Kojadinovic, I., and Fujimoto, K. (2007). Axiomatic characterizations of generalized values. _Discrete Applied Mathematics_, 155(1):26-43.
* Marichal [2000] Marichal, J.-L. (2000). The influence of variables on pseudo-boolean functions with applications to game theory and multicriteria decision making. _Discrete Applied Mathematics_, 107:139-164.

* Marichal and Roubens [1999] Marichal, J.-L. and Roubens, M. (1999). _The Chaining Interaction Index among Players in Cooperative Games_, page 69-85. Springer Netherlands.
* Muschalik et al. [2024] Muschalik, M., Fumagalli, F., Hammer, B., and Hullermeier, E. (2024). Beyond tree-snap: Efficient computation of any-order shapley interactions for tree ensembles. In _Proceeedings of the AAAI Conference on Artificial Intelligence (AAAI)_, pages 14388-14396.
* Olsen et al. [2022] Olsen, L. H. B., Glad, I. K., Jullum, M., and Aas, K. (2022). Using Shapley values and variational autoencoders to explain predictive models with dependent mixed features. _Journal of Machine Learning Research_, 23(213):1-51.
* Olsen et al. [2024] Olsen, L. H. B., Glad, I. K., Jullum, M., and Aas, K. (2024). A comparative study of methods for estimating model-agnostic Shapley value explanations. _Data Mining and Knowledge Discovery_, pages 1-48.
* Owen [2014] Owen, A. B. (2014). Sobol' indices and shapley value. _SIAM/ASA Journal for Uncertainty Quantification_, 2(1):245-251.
* Paszke et al. [2019] Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., Kopf, A., Yang, E., DeVito, Z., Raison, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L., Bai, J., and Chintala, S. (2019). Pytorch: An imperative style, high-performance deep learning library. In _Proceedings of Advances in Neural Information Processing Systems (NeurIPS)_, pages 8024-8035.
* Patel et al. [2021] Patel, N., Strobel, M., and Zick, Y. (2021). High dimensional model explanations: An axiomatic approach. In _Proceedings of ACM Conference on Fairness, Accountability, and Transparency, Virtual Event (FAccT)_, pages 401-411.
* Pedregosa et al. [2011] Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P., Weiss, R., Dubourg, V., VanderPlas, J., Passos, A., Cournapeau, D., Brucher, M., Perrot, M., and Duchesnay, E. (2011). Scikit-learn: Machine learning in python. _Journal of Machine Learning Research_, 12:2825-2830.
* Pelegrina et al. [2023] Pelegrina, G. D., Duarte, L. T., and Grabisch, M. (2023). A \(k\)-additive choquet integral-based approach to approximate the SHAP values for local interpretability in machine learning. _Artificial Intelligence_, 325:104014.
* Pfannschmidt et al. [2016] Pfannschmidt, K., Hullermeier, E., Held, S., and Neiger, R. (2016). Evaluating tests in medical diagnosis: Combining machine learning with game-theoretical concepts. In _Proceedings of Information Processing and Management of Uncertainty in Knowledge-Based Systems (IPMU)_, pages 450-461.
* Poursabzi-Sangdeh et al. [2021] Poursabzi-Sangdeh, F., Goldstein, D. G., Hofman, J. M., Vaughan, J. W., and Wallach, H. (2021). Manipulating and measuring model interpretability. In _Proceedings of CHI Conference on Human Factors in Computing Systems (CHI)_.
* Rong et al. [2024] Rong, Y., Leemann, T., Nguyen, T.-T., Fiedler, L., Qian, P., Unhelkar, V., Seidel, T., Kasneci, G., and Kasneci, E. (2024). Towards human-centered explainable AI: A survey of user studies for model explanations. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 46(4):2104-2122.
* Rota [1964] Rota, G.-C. (1964). On the foundations of combinatorial theory: I. theory of mobius functions. In _Classic Papers in Combinatorics_, pages 332-360. Springer.
* Rozemberczki and Sarkar [2021] Rozemberczki, B. and Sarkar, R. (2021). The shapley value of classifiers in ensemble games. In _Proceedings of the International Conference on Information and Knowledge Management (CIKM)_, pages 1558-1567.
* Rozemberczki et al. [2022] Rozemberczki, B., Watson, L., Bayer, P., Yang, H., Kiss, O., Nilsson, S., and Sarkar, R. (2022). The shapley value in machine learning. In _Proceedings of International Joint Conference on Artificial Intelligence (IJCAI)_, pages 5572-5579.

- Second World Conference (xAI 2024) Proceedings, Part II_, volume 2154 of _Communications in Computer and Information Science_, pages 465-476. Springer.
* Shapley [1953] Shapley, L. S. (1953). A Value for n-Person Games. In _Contributions to the Theory of Games (AM-28), Volume II_, pages 307-318. Princeton University Press.
* Strumbelj and Kononenko [2014] Strumbelj, E. and Kononenko, I. (2014). Explaining prediction models and individual predictions with feature contributions. _Knowledge and Information Systems_, 41(3):647-665.
* Sundararajan et al. [2020] Sundararajan, M., Dhamdhere, K., and Agarwal, A. (2020). The Shapley Taylor Interaction Index. In _Proceedings of the International Conference on Machine Learning (ICML)_, pages 9259-9268.
* Sundararajan and Najmi [2020] Sundararajan, M. and Najmi, A. (2020). The Many Shapley Values for Model Explanation. In _Proceedings of the International Conference on Machine Learning (ICML)_, pages 9269-9278.
* Tay et al. [2022] Tay, S. S., Xu, X., Foo, C. S., and Low, B. K. H. (2022). Incentivizing collaboration in machine learning via synthetic data rewards. In _Proceeedings of the AAAI Conference on Artificial Intelligence (AAAI)_, pages 9448-9456.
* Tsai et al. [2023] Tsai, C., Yeh, C., and Ravikumar, P. (2023). Faith-Shap: The Faithful Shapley Interaction Index. _Journal of Machine Learning Research_, 24(94):1-42.
* Tsang et al. [2020] Tsang, M., Rambhatla, S., and Liu, Y. (2020). How does This Interaction Affect Me? Interpretable Attribution for Feature Interactions. In _Proceedings of Advances in Neural Information Processing Systems (NeurIPS)_, pages 6147-6159.
* van Campen et al. [2018] van Campen, T., Hamers, H., Husslage, B., and Lindelauf, R. (2018). A new approximation method for the shapley value applied to the WTC 9/11 terrorist attack. _Social Network Analysis and Mining_, 8(1):3:1-3:12.
* Watson et al. [2023] Watson, D. S., O'Hara, J., Tax, N., Mudd, R., and Guy, I. (2023). Explaining predictive uncertainty with information theoretic shapley values. In _Proceedings of Advances in Neural Information Processing Systems (NeurIPS)_.
* Wolf et al. [2020] Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac, P., Rault, T., Louf, R., Funtowicz, M., Davison, J., Shleifer, S., von Platen, P., Ma, C., Jernite, Y., Plu, J., Xu, C., Scao, T. L., Gugger, S., Drame, M., Lhoest, Q., and Rush, A. M. (2020). Transformers: State-of-the-art natural language processing. In _Proceedings of the Conference on Empirical Methods in Natural Language Processing: System Demonstrations (EMNLP)_, pages 38-45.
* Wright et al. [2016] Wright, M. N., Ziegler, A., and Konig, I. R. (2016). Do little interactions get lost in dark random forests? _BMC Bioinformatics_, 17:145.
* Yu et al. [2022] Yu, P., Bifet, A., Read, J., and Xu, C. (2022). Linear tree shap. In _Proceedings of Advances in Neural Information Processing Systems (NeurIPS)_.
* Zern et al. [2023] Zern, A., Broelemann, K., and Kasneci, G. (2023). Interventional SHAP values and interaction values for piecewise linear regression trees. In _Proceeedings of the AAAI Conference on Artificial Intelligence (AAAI)_, pages 11164-11173.
* Zhou et al. [2023] Zhou, Z., Xu, X., Sim, R. H. L., Foo, C. S., and Low, B. K. H. (2023). Probably approximate shapley fairness with applications in machine learning. In _Proceeedings of the AAAI Conference on Artificial Intelligence (AAAI)_, pages 5910-5918.

### Organization of the Supplementary Material

The supplementary material is organized as follows:

* Extended Theoretical Background
* 1 Probabilistic and Cardinal-Probabilistic GVs and IIs
	* 1.2 Mobius Interactions (MIs)
* Detailed Overview of all Benchmark Games and Configurations
	* 1.1 Benchmark Overview
	* 1.2 Datasets and Models Used
	* 1.3 Benchmarking Approximators of SIs with shapiq
* 3 Marginal and Conditional Imputers
* 4 Guide for Interpreting Shapley Interaction Visualizations
* 5 Details of the Experimental Setting and Reproducibility
	* 1.1 Generated Cooperative Games
	* 1.2 Computational Resources
	* 1.3 Data Availability and Reproducability
* 6 Benchmarking Analysis: Additional Results
* 7 Glossary of AcronymsExtended Theoretical Background

In this section, we introduce further theoretical background. Specifically, we discuss in more detail the class of GVs and IIs in Appendix A.1, and the MIs in Appendix A.2.

### Probabilistic and Cardinal-Probabilistic GVs and IIs

Probabilistic GVs [58] extend semivalues with a focus on _monotonicity_, i.e. games that satisfy \(\nu(S)\leq\nu(T)\), if \(S\subseteq T\subseteq N\). GVs satisfy the _positivity axiom_, which requires non-negative joint contributions, i.e. \(\phi^{\text{\rm GV}}(S)\geq 0\), for all \(S\subseteq N\) in monotone games [58]. It was shown that GVs are uniquely represented as weighted averages over (joint) marginal contributions \(\nu(T\cup S)-\nu(T)\). On the other hand, cardinal-probabilistic IIs [27] are centered around synergy, independence and redundancy between entities. IIs are based on discrete derivatives, which extend (joint) marginal contributions by accounting for lower-order effects. IIs focus on \(k\)-_monotonicity_, i.e. games that have non-negative discrete derivatives \(\Delta_{S}(T)\geq 0\) for \(S\subseteq U\subseteq N\) with \(2\leq|S|\leq k\). IIs satisfy the \(k\)-_monotonicity axiom_, i.e. non-negative interactions \(\phi^{\text{\rm II}}(S)\geq 0\) for \(k\)-monotone games. Both, GVs and IIs are uniquely represented [27, 58] as

\[\phi^{\text{\rm GV}}(S):=\sum_{T\subseteq N\setminus S}p_{|T|}^{|S|}(n)\cdot( \nu(T\cup S)-\nu(T))\quad\text{ and }\quad\phi^{\text{\rm II}}(S):=\sum_{T\subseteq N \setminus S}p_{|T|}^{|S|}(n)\cdot\Delta_{S}(T),\]

where \(p_{t}^{s}(n)\) are index-specific weights based on the sizes of \(S,T\) and \(N\). The _SGV_[59] and the _SII_[33] with

\[\textbf{Shapley:}p_{t}^{s}(n)=\frac{1}{n-s+1}\binom{n-s}{t}^{-1}\]

naturally extend the SV. An alternative extension for the SV is the _Chaining GV (CHGV)_[58] and _Chaining II (CHII)_[60] with

\[\textbf{Chaining:}p_{t}^{s}(n)=\frac{s}{s+t}\binom{n}{s+t}^{-1}.\]

The main difference of the SGV/SII and the CHGV/CHII is the quantification of so-called _partnerships_[27, 58], i.e. coalitions that only influence the value of the game if all members of the partnership are present. The CHGV and CHII adhere to the _partnership-allocation axiom_[58, 27], which states that the contribution of an individual member of the partnership and the interaction of the whole partnership are proportional. In contrast, the SGV and SII satisfy the _reduced partnership consistency axiom_[58, 27], which states that the interaction of the whole partnership is equal to the contribution of the partnership in a game, where the partnership is a single player.

On the other hand, the _Banzhaf GV (BGV)_[59] and _Banzhaf II (BII)_[33] extend the BV with

\[\textbf{Banzhaf:}p_{t}^{s}(n):=\frac{1}{2^{n-s}}.\]

### Mobius Interactions (MIs)

The MIs \(\Phi_{n}\) are a prominent concept in discrete mathematics, which appears in many different forms. In discrete mathematics, it is also known as the Mobius transform [72]. In cooperative game theory, the concept is known as Harsanyi dividend [35] or internal II [27]. The MI for \(S\subseteq N\) is defined as

\[\Phi_{n}(S):=\sum_{T\subseteq S}(-1)^{|S|-|T|}\nu(T).\]

In this context, the MIs are the unique measure that satisfy the _recovery property_

\[\nu(T)=\sum_{S\subseteq T}\Phi_{n}(S)\text{ for every }T\subseteq N.\]

The MIs are a basis of the vector space of cooperative games, and thus every game can be uniquely represented in terms of its MIs. The _Co-Mobius transform (Co-MI)_[32] is another fundamental concept linked to the MIs of the conjugate game, i.e. \(\bar{\nu}(T):=\nu(N\setminus T)\)[31].

Detailed Overview of all Benchmark Games and Configurations

### Benchmark Overview

We list in Table 4 and 5 all configurations available within our benchmark. Depending on the application task, a configuration represents a combination of multiple parameters that specify the generated cooperative games. For ML games, such a combination includes at least the used dataset and a number of features or datapoints. If a prediction model or imputer for feature values is employed, as for example for Local XAI games, these are also specified.

\begin{table}
\begin{tabular}{l l l l l l l l l} \hline \hline
**ID** & **Benchmark** & **Data** & **P.** & \(n\) & \(|\mathcal{P}(N)|\) & \(g\) & **Game Configuration** \\ \hline
**1** & Data Valuation & AD & ✓ & 15 & 32768 & 10 & model\_name=decision\_tree, n\_data\_points=15 \\
**2** & Data Valuation & AD & ✓ & 15 & 32768 & 10 & model\_name=random\_forest, n\_data\_points=15 \\ \hline
**3** & Data Valuation & AD & ✓ & 10 & 1024 & 30 & model\_name=decision\_tree, player\_sizes=increasing, n\_players=10 \\
**4** & Data Valuation & AD & ✓ & 10 & 1024 & 30 & model\_name=random\_forest, player\_sizes=increasing, n\_players=10 \\
**5** & Data Valuation & AD & ✓ & 10 & 1024 & 30 & model\_name=gradient\_boosting, player\_sizes=increasing, n\_players=10 \\
**6** & Data Valuation & AD & ✓ & 14 & 16384 & 5 & model\_name=decision\_tree, player\_sizes=increasing, n\_players=14 \\ \hline
**7** & Ensemble Selection & AD & ✓ & 10 & 1024 & 30 & loss\_function=accuracy\_score, n\_members=10 \\ \hline
**8** & Feature Selection & AD & ✓ & 14 & 16384 & 30 & model\_name=decision\_tree \\
**9** & Feature Selection & AD & ✓ & 14 & 16384 & 30 & model\_name=random\_forest \\
**10** & Feature Selection & AD & ✓ & 14 & 16384 & 30 & model\_name=gradient\_boosting \\ \hline
**11** & Global Explanation & AD & ✓ & 14 & 16384 & 30 & model\_name=decision\_tree, loss\_function=accuracy\_score \\
**12** & Global Explanation & AD & ✓ & 14 & 16384 & 30 & model\_name=random\_forest, loss\_function=accuracy\_score \\
**13** & Global Explanation & AD & ✓ & 14 & 16384 & 30 & model\_name=gradient\_boosting, loss\_function=accuracy\_score \\ \hline
**14** & Local Explanation & AD & ✓ & 14 & 16384 & 30 & model\_name=decision\_tree, imputer=marginal \\
**15** & Local Explanation & AD & ✓ & 14 & 16384 & 30 & model\_name=random\_forest, imputer=marginal \\
**16** & Local Explanation & AD & ✓ & 14 & 16384 & 30 & model\_name=gradient\_boosting, imputer=marginal \\
**17** & Local Explanation & AD & ✓ & 14 & 16384 & 30 & model\_name=decision\_tree, imputer=conditional \\
**18** & Local Explanation & AD & ✓ & 14 & 16384 & 30 & model\_name=random\_forest, imputer=conditional \\
**19** & Local Explanation & AD & ✓ & 14 & 16384 & 30 & model\_name=gradient\_boosting, imputer=conditional \\ \hline
**20** & RF Ensemble Selection & AD & ✓ & 10 & 1024 & 30 & loss\_function=accuracy\_score, n\_members=10 \\ \hline
**21** & Uncertainty Explanation & AD & ✓ & 14 & 16384 & 30 & uncertainty\_to\_explain=total, imputer=marginal \\
**22** & Uncertainty Explanation & AD & ✓ & 14 & 16384 & 30 & uncertainty\_to\_explain=total, imputer=conditional \\
**23** & Uncertainty Explanation & AD & ✓ & 14 & 16384 & 30 & uncertainty\_to\_explain=calculate, imputer=marginal \\
**24** & Uncertainty Explanation & AD & ✓ & 14 & 16384 & 30 & uncertainty\_to\_explain=equier, imputer=marginal \\
**25** & Uncertainty Explanation & AD & ✓ & 14 & 16384 & 30 & uncertainty\_to\_explain=equier, imputer=conditional \\
**26** & Uncertainty Explanation & AD & ✓ & 14 & 16384 & 30 & uncertainty\_to\_explain=equier, imputer=conditional \\ \hline
**27** & Unsupervised FI & AD & ✓ & 14 & 16384 & 1 & - \\ \hline
**28** & Cluster Explanation & BS & ✓ & 12 & 4096 & 1 & cluster\_method=means, score\_method=subroutine\_score \\
**29** & Cluster Explanation & BS & ✓ & 12 & 4096 & 1 & cluster\_method=subroutine\_score, method=callis\_harabase\_score \\ \hline
**30** & Data Valuation & BS & ✓ & 15 & 32768 & 10 & model\_name=decision\_tree, n\_data\_points=15 \\
**31** & Data Valuation & BS & ✓ & 15 & 32768 & 10 & model\_name=random\_forest, n\_data\_points=15 \\ \hline
**32** & Data Valuation & BS & ✓ & 10 & 1024 & 30 & model\_name=decision\_tree, player\_sizes=increasing, n\_players=10 \\
**33** & Data Valuation & BS & ✓ & 10 & 1024 & 30 & model\_name=random\_forest, player\_sizes=increasing, n\_players=10 \\
**34** & Data Valuation & BS & ✓ & 10 & 1024 & 30 & model\_name=random\_forest, player\_sizes=increasing, n\_players=10 \\
**35** & Data Valuation & BS & ✓ & 14 & 16384 & 5 & model\_name=decision\_tree, player\_sizes=increasing, n\_players=14 \\ \hline
**36** & Ensemble Selection & BS & ✓ & 10 & 1024 & 30 & loss\_function=r2\_score, n\_members=10 \\ \hline
**37** & Feature Selection & BS & ✓ & 12 & 4096 & 30 & model\_name=decision\_tree \\
**38** & Feature Selection & BS & ✓ & 12 & 4096 & 30 & model\_name=random\_forest \\
**39** & Feature Selection & BS & ✓ & 12 & 4096 & 30 & model\_name=gradient\_boosting \\ \hline
**40** & Global Explanation & BS & ✓ & 12 & 4096 & 30 & model\_name=decision\_tree, loss\_function=r2\_score \\
**41** & Global Explanation & BS & ✓ & 12 & 4096 & 30 & model\_name=random\_forest, loss\_function=r2\_score \\
**42** & Global Explanation & BS & ✓ & 12 & 4096 & 30 & model\_name=gradient\_boosting, loss\_function=r2\_score \\ \hline
**43** & Local Explanation & BS & ✓ & 12 & 4096 & 30 & model\_name=decision\_tree, imputer=marginal \\
**44** & Local Explanation & BS & ✓ & 12 & 4096 & 30 & model\_name=random\_forest, imputer=marginal \\
**45** & Local Explanation & BS & ✓ & 12 & 4096 & 30 & model\_name=gradient\_boosting, imputer=conditional \\
**46** & Local Explanation & BS & ✓ & 12 & 4096 & 30 & model\_name=decision\_tree, imputer=conditional \\
**47** & Local Explanation & BS & ✓ & 12 & 4096 & 30 & model\_name=random\_forest, imputer=conditional \\
**48** & Local Explanation & BS & ✓ & 12 & 4096 & 30 & model\_name=gradient\_boosting, imputer=conditional \\ \hline
**49** & RF Ensemble Selection & BS & ✓ & 10 & 1024 & 30 & loss\_function=r2\_score, n\_members=10 \\ \hline
**50** & Unsupervised FI & BS & ✓ & 12 & 4096 & 1 & - \\ \hline \hline \end{tabular}
\end{table}
Table 4: Overview of all Benchmark Configurations: Each configuration is assigned a distinctive identifier (ID), has a name (Benchmark) indicating dataset and application if available, is pre-computed (P.) if the player number \(n\) does not exceed 16, consists of \(|\mathcal{P}(N)|\) many coalitions to be evaluated, is iterated over multiple game instances (\(g\)), and has a set of parameters (Game Configuration).

[MISSING_PAGE_FAIL:19]

### Datasets and Models Used

Our benchmark games are based on five datasets. All of these datasets are publicly available. The following contains a small description of all datasets:

* The _AdultCensus_(Krizhevsky et al., 2015, CC BY 4.0 license) dataset is a tabular classification dataset containing \(n=14\) features. The dataset was obtained from openml (Krizhevsky et al., 2015) (id: _1590_) and is available at https://github.com/mmschlk/shapiq/blob/v1/data/adult_census.csv for reproducibility.
* The _BikeSharing_(Krizhevsky et al., 2015, CC BY 4.0 license) dataset is a tabular regression dataset containing \(n=12\) features. The dataset was obtained from openml (Krizhevsky et al., 2015) (id: _42712_) and is available at https://github.com/mmschlk/shapiq/blob/v1/data/bike.csv for reproducibility.
* The _CaliforniaHousing_(Krizhevsky et al., 2015, CC0 public domain) dataset is a tabular regression dataset containing \(n=8\) features. The target of this dataset is to predict property prices. The dataset was obtained from scikit-learn (Krizhevsky et al., 2015) and is available at https://github.com/mmschlk/shapiq/blob/v1/data/california_housing.csv for reproducibility.
* The _MovieReview_ is also known as the IMBD dataset (Krizhevsky et al., 2015, custom research license) contains movie review excerpts. We simplify the dataset to only contain sentence parts with \(n\leq 14\) words. The simplified dataset can be found at https://github.com/mmschlk/shapiq/blob/v1/benchmark/data/simplified_imdb.csv for reproducibility.
* The _ImageClassification_ data contains test images from _Imagenet_(Krizhevsky et al., 2015, custom research license). The example images can be found at https://github.com/mmschlk/shapiq/tree/v1/shapiq/games/benchmark/imagenet_examples for reproducibility.

All models used for the benchmark games are defined in the code repository. We use decision tree, random forest, k-nearest neighbour, linear/logistic regression models from scikit-learn(Krizhevsky et al., 2015). Moreover, we use gradient-boosted tree classifiers and regressors from xgboost(Krizhevsky et al., 2015). We train small neural networks with PyTorch(Krizhevsky et al., 2015) and use PyTorch's ResNet18 architecture. The movie review language model and the vision transformer are derived from the transformers API (Krizhevsky et al., 2015).

### Benchmarking Approximators of SIs with shapiq

Listing 1 shows an API for benchmarking 4 approximation algorithms on a Dataset Valuation game based on the _AdultCensus_ dataset and a gradient boosting decision tree model.2

Footnote 2: For details, refer to the notebook examples at https://shapiq.readthedocs.io referring to the benchmark capabilities.

``` importshapiq fromshapiq.benchmarkimport( load_games_from_configuration, print_benchmark_configurations, plot_approximation_quality, run_benchmark ) ) #printallavailablegamesandbenchmarkconfigurations print_benchmark_configurations() >>Game:AdultCensusDatasetValuation >>PlayerID: >>Numberofplayers:10 >>Numberofconfigurations:3 >>IstheBenchmarkPre-computed:True >>IterationParameter:random_state >>Configurations: >>Configuration:1:{'model_name':'decision_tree','player_sizes':'increasing','n_players':10} >>Configuration2:{'model_name':'random_forest','player_sizes':'increasing','n_players':10} >>Configuration3:{'model_name':'gradient_boosting','player_sizes':'increasing','n_players':10} >>... #loadthegamefilesfromdisk/ordownload games-load_games_from_configuration(game_class-"AdultCensusDataValuation",n_player_id=0,config_id=2) games-list(games)#converttolist(thegeneratorisconsumed) n_players-games[0].n_players
#definetheprogramstoresubenchmark sv_approximators'={ shapiq.PermutationSamplingSII(n=n_players,index="k-SII",random_state=0), shapiq.SHAPIQ(n=n_players,random_state=0), shapiq.SVAMWIQ(n=n_players,random_state=0), shapiq.KernelSHAPIQ(n=n_players,random_state=0) ] ) #runthebenchmarkwiththechosenparameters results=run_benchmark( index="k-SII", order=2, games=games, approximators=sv_approximators, save_path="benchmark_results.json", budget_steps=[500, 1000, 2000, 4000], n_jobs=8 ) ) #plottheresults plot_approximation_quality(results) ```

Listing 1: Exemplary code for benchmarking approximators with shapiq.

Marginal and Conditional Imputers

When computing SVs and SIs, especially for structured tabular data that has a natural interpretation of feature distribution, there is a choice for marginalizing feature influence over either a marginal or a conditional distribution [1, 18, 52, 54, 55, 62, 63, 79].

For a concrete example [52], consider a supervised learning task where a model \(f:\mathcal{X}\rightarrow\mathbb{R}\) is used to predict the response variable given an input \(\mathbf{x}\), which consists of individual features \((\mathbf{x}_{1},\mathbf{x}_{2},\ldots,\mathbf{x}_{n})\). Let \(p(\mathbf{x})\) to represent the data distribution with support on \(\mathcal{X}\subseteq\mathbb{R}^{n}\). We use bold symbols \(\mathbf{x}\) to denote random variables and normal symbols \(x\) to denote values. Let \(\mathbf{x}_{S}\) and \(x_{S}\) denote a subset of features, i.e. players in a game, and values for different \(S\subseteq N\), respectively. Then, a cooperative game \(\nu:\mathcal{P}(N)\rightarrow\mathbb{R}\) for estimating Shapley-based feature attributions and interactions is defined as

\[\nu(S)\coloneqq f_{S}(x_{S})\coloneqq\mathbb{E}_{q(\mathbf{x}_{S})}\big{[}f( x_{S},\mathbf{x}_{S})\big{]}=\int f(x_{S},x_{S})q(x_{S})dx_{S},\]

where \(\bar{S}=N\setminus S\) denotes the set complement. The feature distribution \(q(\mathbf{x}_{S})\) most often considered in the literature is either a _marginal distribution_ when \(q(\mathbf{x}_{S})\coloneqq p(\mathbf{x}_{S})\)[18, 55], or a _conditional distribution_ when \(q(\mathbf{x}_{S})\coloneqq p(\mathbf{x}_{S}\mid x_{S})\)[1, 26, 62].

In general, empirical estimation of a conditional feature distribution is challenging [1, 18, 55, 62]. Most recently in [63], the authors benchmark several methods for approximating SVs based on marginalizing features with a conditional distribution \(p(\mathbf{x}_{S}\mid x_{S})\), without a clear best, i.e. different methods are appropriate in different practical situations. Thus, we combine the decision tree-based and sampling approaches [63] to implement a baseline _conditional imputer_ in shapiq.ConditionalImputer. The class can be easily extended to include more algorithms, which we leave as future work. The rather standard imputation with a marginal distribution \(p(\mathbf{x}_{\bar{S}})\) is implemented in shapiq.MarginalImputer, as well as imputation with predefined baseline values with shapiq.BaselineImputer. All imputers are used by the appropriate game benchmarks, and available for approximating feature interaction explanations in shapiq.TabularExplainer via the imputer parameter.

Listing 2 shows a more advanced API for setting a specific imputer and approximator in shapiq.3

Footnote 3: For details, refer to the notebook examples at https://shapiq.readthedocs.io referring to imputers.

X, model =... import shapiq # create an imputer object  imputer = shapiq.ConditionalImputer(model=model, data=X, sample_size=100) # create an approximator object  approximator = shapiq.KernelSHAPIQ(n=X.shape[1], index="SII", max_order=3) # create an explainer object  explainer = shapiq.Explainer(model, X, imputer-imputer, approximator-approximator) # choose a samplepoint to be explained x = X[0] # approximate feature interactions given the specified budget  interaction_values = explainer.explain(x=x, budget=1024) # retrieve 3-order feature interactions  interaction_values.get_n_order_values(3) # visualize 1-order and 2-order feature interactions on a graph  interaction_values.plot_network(feature_names=...)

**Listing 2:** Exemplary code for defining an imputer and approximator for explanation with shapiq.

Guide for Interpreting Shapley Interaction Visualizations

This section provides information for interpreting visual representations of SIs, offering insights into how interactions between players--such as features in XAI or individual observations for data valuation--are depicted in network and SI graph plots. We propose two types of visualizations: the **network plot**[38; 61] and the **SI graph plot**, with the latter generalizing the former.

General description.In both visualizations, players are represented as nodes, while explanations in the form of interactions are depicted as edges linking these nodes. The network plot is limited to second-order interactions, meaning it only displays edges between two nodes, whereas the SI graph plot accommodates interactions of any order, with interactions involving more than two players represented as hyper-edges connecting three or more nodes. Single-order interactions are represented by the size of the nodes, with larger nodes indicating stronger main effects. The strength and direction of these interactions are encoded through the color, transparency, and thickness of the edges; stronger interactions are shown as thicker and more opaque edges, while weaker interactions are represented by thinner and more transparent edges. Consistent with established conventions from shap[55] visualizations, red indicates positive interactions, and blue indicates negative interactions. In both visualizations, nodes are drawn in a circular layout by default but can also be positioned based on a predefined graphical structure. The network plot originates in [38] to illustrate second-order interactions for global effects. It was further adapted for local SIs in [61], whereas the SI graph plot extends this concept by allowing for the visualization of higher-order interactions, thus providing a more comprehensive view of the cooperative game structure.

Figure 6: Network plots (top row) and SI graph plots (bottom row) for \(2\)-SII scores (left) and MIs (right) as explanations for an observation from the _CaliforniaHousing_ dataset and a random forest.

Interpretation of the example visualizations.Figure 6 shows example network and SI graph plots. We divide the _CaliforniaHousing_ dataset into train and test splits. A RandomForestRegressor from scikit-learn[67] is fitted to the training data, achieving an \(R^{2}\) score of 80%. We select an observation from the test set and compute \(2\)-SII scores and MIs using shapiq's implementation of TreeSHAP-IQ[61]. For further details regarding the dataset, training, and comparison with other visualizations, we refer to the accompanying notebook on "Visualizing Shapley Interactions" in shapiq's documentation. The observation to be explained has a ground truth property value of \(1.575\) (in \(100,000\) USD) and is predicted to be worth \(1.62\) (in \(100,000\) USD). The baseline prediction of the model is around \(2.071\) (in \(100,000\) USD). This means that with all features provided, the model predicts the property to be worth less than the average house in the dataset. From both the network plots and SI graph plots in Figure 6, it is clear that the _AveOccup_ (AO) feature has a strong negative influence on the prediction compared to the baseline, as indicated by the large blue node. However, some features and interactions have positive effects (red edges). Specifically, the interaction between _Latitude_ (Lat.) and _Longitude_ (Lon.), which encodes the exact location of the property, has a positive influence on the property's valuation. The MI graph plot further reveals that higher-order interactions exist, as shown by hyperedges connecting more than two features. For example, there is a sizable positive third-order interaction between _Longitude_ (Lon.), _Latitude_ (Lat.), and _MedianIncome_ (MI). A positive fourth-order interaction involving the same three features and the _HouseAge_ (HA) feature also exists.

Details of the Experimental Setting and Reproducibility

This section contains additional information regarding the experimental setup and definition of the cooperative games. For further information we refer to the benchmark configuration as part of shapiq.benchmark.

### Generated Cooperative Games

The game-theoretical quantification of interaction demands a formal cooperative game specified by a player set \(N\) and value function \(\nu:\mathcal{P}(N)\rightarrow\mathbb{R}\). The players for each benchmark game are already given in Table 3, leaving the value functions open to be specified with what we catch up on here.

Local Explanation.For a specified datapoint \(x\), the worth of a coalition of features \(S\) is given by the model's predicted value \(h(x_{S})\) using only the features in \(S\). The features outside of \(S\) are made absent in \(x_{S}\) by imputing them with a surrogate value in order to remove their information. For tabular datasets such as _AdultCensus_, _BikeSharing_, and _CaliforniaHousing_ this is done by marginal or conditional imputation. For the language model predicting the sentiment of movie review excerpts, missing words are set to the masked token. Missing pixels (patches) for the vision transformer image classifier are also set to the masked token. For the ResNet image classifier, removed superpixels are collectively set to a mean value (gray).

Global Explanation.Instead of specifying a single datapoint and considering the model's output, the model's loss is averaged over a number of fixed datapoints \(x_{1},\dots,x_{M}\). The model's loss for a coalition \(S\) and datapoint \(x_{m}\) is computed by comparing its prediction \(h(x_{mS})\) with the ground truth target value. The imputation of absent features is done as for local explanations.

Tree Explanation.This is a specialization of local explanations for tree models, made feasible by the capabilities of TreeSHAP-IQ to compute ground-truth SVs and SIs values, which allows the evaluation games with substantially more features. Features are imputed according to the tree distribution [54, 61]. Consequently, the worth of the empty coalition containing no features is the tree's average prediction, e.g. baseline value.

Uncertainty Explanation.Similar to local explanations, the model's prediction with missing features imputed to a fixed datapoint is evaluated. Instead of referring to the predicted value, the value function is given by the prediction's uncertainty for which three measures are available: total, epistemic, and aleatoric uncertainty. Hence, the Shapley values of the features attribute their individual contribution to the decrease in uncertainty caused by their information.

Feature Selection.The available data is split into a training set \(\mathcal{D}_{\text{Train}}\) and test set \(\mathcal{D}_{\text{Test}}\). Given a learning algorithm \(\mathcal{A}\), a coalitions worth \(\nu(S)\) is given by the generalization performance of the model \(h_{S}\) on \(\mathcal{D}_{\text{Test}}\) that results from applying \(\mathcal{A}\) on \(\mathcal{D}_{\text{Train}}\) using only features in \(S\), known as _remove-and-refit_. The worth of the empty coalition is set to 0.

Ensemble Selection.Replacing features in feature selection by weak learners, and adapting the learning algorithm to construct an ensemble out of those, leads to ensemble selection. Each coalition \(S\) of base learners is evaluated by the performance of the resulting ensemble on a separate test set, known as _remove-and-re-evaluate_. Likewise, we set \(\nu(\emptyset)=0\).

Data Valuation.Continuing in the spirit of _remove-and-refit_, a new model is fitted to each coalition of datapoints. The generalization performance of the resulting model on a separate test set is set to be the coalition's worth. The value of the empty coalition is set to 0.

Dataset Valuation.The setup is analogous to data valuation, where instead of single datapoints being understood as players, the available data is partioned and each subset is viewed as a player.

Cluster Explanation.Similar to feature selection, _remove-and-refit_ is applied. Instead of fitting a model, a clustering algorithm forms multiple clusters on the dataset using only the available features of a coalition \(S\). The worth \(\nu(s)\) is given by a cluster evaluation score (see Tables 4 and 5 for details). A cluster score of 0 is assigned to the empty coalition.

Unsupervised Feature Importance.Given a coalition of features \(S\), a set of datapoints can be understood as observations generated by a joint distribution of \(S\) and used to estimate this distribution by measuring the frequencies of feature values. This, in turn, allows to measure the entropy and thus also total correlation of a subset of features which is used as the worth of \(S\). Since the total correlation measures the amount of shared information, each feature's assigned Shapley value quantifies its contributed information to the group. The total correlation of the empty set is naturally 0.

Sum of Unanimity Models (SOUMs).A unanimity game is a synthetic game for a coalition \(U\subseteq N\) with \(\nu_{U}(T)\coloneqq\mathbf{1}_{T\supseteq U}\), i.e. outputs one, if all players of \(U\) are present, and zero otherwise. The sum of unanimity model (SOUM) is a linear combination of randomly sampled unanimity games. For uniformly sampled coefficients \(a_{1},\ldots,a_{m}\in[-1,1]\) and subsets \(U_{1},\ldots,U_{m}\subseteq N\) uniformly sampled by size, where we restrict the SOUM to specific maximum subset sizes. The value function then reads as

\[\nu(T):=\sum_{\ell=1}^{m}a_{\ell}\nu_{U_{\ell}}(T).\]

For SOUMs, the MIs as well as all SIs can be efficiently computed in linear time, cf. [28, Appendix B.7].

### Computational Resources

This section contains additional information regarding the computational resources required for the empirical evaluation of this work. The main computational burden stems from pre-computing the benchmark games for \(n\leq 16\) players and from running all of shapiq's SV and \(2\)-SII approximation methods. Still, the experiments require only a modest range of computational resources. The games are pre-computed on a "11th Gen Intel(R) Core(TM) i7-11800H 2.30GHz" machine requiring around 240 CPU hours. The approximation experiments have been run on a compute cluster using 80 CPUs of four "AMD EPYC 7513 32-Core Processor" units for 24 hours resulting in about 1920 CPU hours.

### Data Availability and Reproducability

The data to the pre-computed games is available at https://github.com/mmschlk/shapiq/tree/v1. Utility functions exist in shapiq that automatically download and instantiate the games. The code for reproducing the experimental evaluation can be found at https://github.com/mmschlk/shapiq/tree/v1/benchmark and https://github.com/mmschlk/shapiq/tree/v1/complexity_accuracy.

[MISSING_PAGE_EMPTY:27]

Figure 8: Approximation qualities in terms of MSE **(top)** and Precision@5 **(bottom)** for \(3\)-SII higher-order interactions for three benchmark settings based on the _AdultCensus_ (AC) dataset including Local Explanation **(left)**, Global Explanation **(middle)**, and Data Valuation **(right)**.

## 6 Conclusion

Figure 9: Additional SV (column one and two) and SI (column three and four) approximation results for different benchmark games from the Local Explanation (first row, vision transformer image classifier with \(n=16\) patches), Local Explanation (second row, language model predicting movie review sentiment with \(n=14\) words), Local Explanation (third row, dataset _CaliforniaHousing_ with \(n=8\) features) and Global Explanation (fourth row, dataset _AdultCensus_ with \(n=14\) features) domain.

Figure 10: Additional SV (column one and two) and SI (column three and four) approximation results for different benchmark games from the Data Valuation (first row, _BikeSharing_ with \(n=12\) features), Dataset Valuation (second row, _CaliforniaHousing_ with \(n=8\) features), Ensemble Selection (third row, dataset _BikeSharing_ with \(n=12\) features) and Random Forest Ensemble Selection (fourth row, dataset _CaliforniaHousing_ with \(n=8\) features) domain.

Figure 11: Additional SV (column one and two) and SI (column three and four) approximation results for different benchmark games from the Uncertainty Explanation (first row, _AdultCensus_ with \(n=14\) features), Cluster Explanation (second row, _BikeSharing_ with \(n=12\) features), and Unsupervised Feature Importance (third row, dataset _AdultCensus_ with \(n=14\) features) domain.

Glossary of Acronyms

\(k\)**-Sii**: \(k\)-Sv. 4, 5, 8
**BGV**: Banzhaf GV. 17
**BII**: Banzhaf II. 17
**BV**: Banzhaf Value. 3-6, 17
**CHGV**: Chaining GV. 17
**CHII**: Chaining II. 17
**FBII**: Faithful BII. 5, 8
**FSII**: Faithful SII. 4, 5, 8
**GV**: Generalized Value. 4, 5, 17
**II**: Interaction Index. 4, 5, 17
**MAE**: mean absolute error. 27
**MI**: Mobius Interaction. 2, 5, 6, 8, 9, 17, 23, 24, 26
**ML**: machine learning. 1, 3-8, 10, 18
**MSE**: mean squared error. 9, 27, 28
**Precision@5**: precision at five. 9, 27, 28
**SGV**: Shapley GV. 4, 6, 17
**SI**: Shapley Interaction. 1-10, 22-26, 29-31
**SII**: Shapley II. 4, 8, 17
**STII**: Shapley Taylor II. 4, 8
**SV**: Shapley Value. 1-5, 7-9, 17, 22, 25, 26, 29-31
**XAI**: explainable artificial intelligence. 3, 23

## Checklist

The checklist follows the references. Please read the checklist guidelines carefully for information on how to answer these questions. For each question, change the default [**TODO**] to [Yes], [No], or [N/A]. You are strongly encouraged to include a **justification to your answer**, either by referencing the appropriate section of your paper or providing a brief inline description. For example:

* Did you include the license to the code and datasets? [Yes] See Section 1.
* Did you include the license to the code and datasets? [No] The code and the data are proprietary.
* Did you include the license to the code and datasets? [N/A]

Please do not modify the questions and only use the provided macros for your answers. Note that the Checklist section does not count towards the page limit. In your paper, please delete this instructions block and only keep the Checklist section heading above along with the questions/answers below.

1. For all authors... 1. Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? [Yes] 2. Did you describe the limitations of your work? [Yes] _See Section 5, Limitations._ 3. Did you discuss any potential negative societal impacts of your work? [Yes] _See Section 5, Broader impact._ 4. Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes] _We read the ethics review guidelines._
2. If you are including theoretical results... 1. Did you state the full set of assumptions of all theoretical results? [N/A] 2. Did you include complete proofs of all theoretical results? [N/A]
3. If you ran experiments (e.g. for benchmarks)... 1. Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? [Yes] _We ran experiments. All of the experimental data is available online at_https://github.com/mmschlk/shapiq/tree/v1 _(directories benchmark & complexity accuracy), and described in Appendix E.3._ 2. Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [Yes] _We train a collection of off-the-shelve models with mostly default parameters. All parameters are described in the technical supplement and or in the aformentioned code repository._ 3. Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? [Yes] _Experimental evaluations show also error bars_. 4. Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes] _We use no GPUs in the experiments. Compute resources used are outlined in Appendix E.2._
4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets... 1. If your work uses existing assets, did you cite the creators? [Yes] _We cite all datasets used in Appendix B.2._ 2. Did you mention the license of the assets? [Yes] _We mention the licenses of all datasets used for which we could directly locate them in Appendix B.2._ 3. Did you include any new assets either in the supplemental material or as a URL? [Yes] _We included URLs to the source code and documentation in Section 3._ 4. Did you discuss whether and how consent was obtained from people whose data you're using/curating? [N/A] 5. Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [N/A]
5. If you used crowdsourcing or conducted research with human subjects...

1. Did you include the full text of instructions given to participants and screenshots, if applicable? [N/A]
2. Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A]
3. Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A]