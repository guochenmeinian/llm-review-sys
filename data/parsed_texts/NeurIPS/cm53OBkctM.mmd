Bayesian Learning of Optimal Policies in Markov Decision Processes with Countably Infinite State-Space

Saghar Adler

University of Michigan

Vijay Subramanian

University of Michigan

###### Abstract

Models of many real-life applications, such as queueing models of communication networks or computing systems, have a countably infinite state-space. Algorithmic and learning procedures that have been developed to produce optimal policies mainly focus on finite state settings, and do not directly apply to these models. To overcome this lacuna, in this work we study the problem of optimal control of a family of discrete-time countable state-space Markov Decision Processes (MDPs) governed by an unknown parameter \(\theta\in\Theta\), and defined on a countably-infinite state-space \(\mathcal{X}=\mathbb{Z}_{+}^{d}\), with finite action space \(\mathcal{A}\), and an unbounded cost function. We take a Bayesian perspective with the random unknown parameter \(\boldsymbol{\theta}^{*}\) generated via a given fixed prior distribution on \(\Theta\). To optimally control the unknown MDP, we propose an algorithm based on Thompson sampling with dynamically-sized episodes: at the beginning of each episode, the posterior distribution formed via Bayes' rule is used to produce a parameter estimate, which then decides the policy applied during the episode. To ensure the stability of the Markov chain obtained by following the policy chosen for each parameter, we impose ergodicity assumptions. From this condition and using the solution of the average cost Bellman equation, we establish an \(\tilde{O}(dh^{d}\sqrt{|\mathcal{A}|T})\) upper bound on the Bayesian regret of our algorithm, where \(T\) is the time-horizon. Finally, to elucidate the applicability of our algorithm, we consider two different queueing models with unknown dynamics, and show that our algorithm can be applied to develop approximately optimal control algorithms.

## 1 Introduction

Many real-life applications, such as communication networks, supply chains, and computing systems, are modeled using queueing models with countably infinite state-space. In the existing analysis of these systems, the models are assumed to be known, but despite this, developing optimal control schemes is hard, with only a few examples worked out [35, 9, 54]. However, knowing the model, algorithmic procedures exist to produce approximately optimal policies [35] (such as value iteration and linear programming). Given the success of data-driven optimal control design, in particular Reinforcement Learning (RL), we explore the use of such methods for the countable state-space controlled Markov processes. However, current RL methods that focus on finite-state settings do not apply to the mentioned queueing models. With the model unknown, our goal is to develop a meta-learning scheme that is RL-based but obtains good performance by utilizing algorithms developed when models are known. Specifically, we study the problem of optimal control of a family of discrete-time countable state-space MDPs governed by an unknown parameter \(\theta\) from a general space \(\Theta\) with each MDP evolving on the countable state-space \(\mathcal{X}=\mathbb{Z}_{+}^{d}\) and finite action space \(\mathcal{A}\). The cost function is unbounded and polynomially dependent on the state, following the examples of minimizing waiting times in queueing systems. Taking a Bayesian view, we assume the model is governed by an unknown parameter \(\boldsymbol{\theta}^{*}\in\Theta\) generated from a fixed and known prior distribution. We aim to learn a policy \(\pi\) that minimizes the optimal infinite-horizon average cost over a given class of policies \(\Pi\) with low Bayesian regret with respect to the (parameter-dependent) optimal policy in \(\Pi\).

To avoid many technical difficulties in countably infinite state-space settings, it is crucial to establish certain assumptions regarding the class of models from which the unknown system is drawn; some examples are: i) the number of deterministic stationary policies is not finite; and ii) in average cost optimal control problems, without stability/ergodicity assumptions, an optimal policy may not exist [40], and when it exists, it may not be stationary or deterministic [20]. With these in mind, we assume that for any state-action pair, the transition kernels in the model class are categorical and skip-free to the right, i.e., with finite support with a bound depending on the state only in an additive manner; both are common features of queueing models where an increase in state is due to arrivals. A second set of assumptions ensure stability by assuming that the Markov chains obtained by using different policies in \(\Pi\) are geometrically ergodic with uniformity across \(\Theta\). From these assumptions, moments on hitting times are derived in terms of Lyapunov functions for polynomial ergodicity. These assumptions also yield a solution to the average cost optimality equation (ACOE) [9].

**Contributions:** To optimally control the unknown MDP, we propose an algorithm based on Thompson sampling with dynamically-sized episodes; posterior sampling is used based on its broad applicability and computational efficiency [46, 47]. At the beginning of each episode, a posterior distribution is formed using Bayes' rule, and an estimate is realized from this distribution which then decides the policy used throughout the episode. To evaluate the performance of our proposed algorithm, we use the metric of Bayesian regret, which compares the expected total cost achieved by a learning policy \(\pi_{L}\) until time horizon \(T\) with the policy achieving the optimal infinite-horizon average cost in the policy class \(\Pi\). We consider regret guarantees in three different settings as follows:

1. In Theorem 1, for \(\Pi\) being the set of all policies and assuming that we have oracle access to the optimal policy for each parameter, we establish an \(\tilde{O}(dh^{d}\sqrt{|\mathcal{A}|T})\) upper bound on the Bayesian regret of this algorithm compared to the optimal policy.

2. In Corollary 1, where class \(\Pi\) is a subset of all stationary policies and where we know the best policy within this subset for each parameter via an oracle, we prove an \(\tilde{O}(dh^{d}\sqrt{|\mathcal{A}|T})\) upper bound on the Bayesian regret of our proposed algorithm, relative to the best-in-class policy.

3. In Theorem 2, we explore a scenario where we have access to an approximately optimal policy, rather than the optimal policy in set \(\Pi\) (which are all assumed to be stationary policies). When the approximately optimal policies satisfy Assumptions 3-4, we prove an \(\tilde{O}(dh^{d}\sqrt{|\mathcal{A}|T})\) regret bound, relative to the optimal policy in set \(\Pi\).

Finally, to provide examples of our framework for developing approximately optimal control algorithms for stochastic systems with unknown dynamics, we study two different queueing models that meet our technical conditions. The first example is a continuous-time queueing system with two heterogeneous servers with unknown service rates and a common infinite buffer with the decision being the use of the slower server. Here, the optimal policy that minimizes the average waiting time is a threshold policy [38] which yields a queue-length after which the slower server is always used. The second model is a two-server queueing system, each with separate infinite buffers, to one of which a dispatcher routes an incoming arrival. Here, the optimal policy minimizing the waiting time is a switching-curve [26] with the specifics unknown for general parameter values, so we find the best policy within a commonly used set of switching-curve policies (Max-Weight policies [58, 59]), and assign the arrival to the queue with minimum weighted queue-length. For both models, we verify our assumptions for the class of optimal/best-in-class policies corresponding to different service rates and conclude that our proposed algorithm can be used to learn the optimal/best-in-class policy.

**Related Work:** Thompson sampling [62], or posterior sampling, has been applied to RL in many contexts of unknown MDPs [55, 45] and partially observed MDPs [28]; see tutorials [22, 50] for a comprehensive survey. It has been used in the parametric learning context [6] to minimize either Bayesian [46, 47, 49, 1, 60, 61] or frequentist [5, 23] regret. The bulk of the literature, including [5, 23, 49], analyzes finite-state and finite-action models but with different parameterizations such that a general dependence of the models on the parameters is allowed. The work in [61] studies general state-space MDPs but with a scalar parameterization with a Lipschitz dependence of the underlying models. Our problem formulation specifically considers countable state-space models with the models related via ergodicity, which we believe is a natural choice. Our focus on parametric learning is also connected to older work in adaptive control [3, 24] which studies asymptotically optimal learning for general parameter settings but with either a finite or countably infinite number of policies. Learning-based asymptotically optimal control in queues has a long history [36, 35] but recently there is increased work that also characterizes finite-time regret performance with respect to a well-known good policy or the optimal policy; see [63] for a survey. A series of work has studied learning with Max-Weight policies to get stability and linear regret [44; 30] or just stability [65]. A recent related work [18] considers learning optimal paramterized policies in queueing networks when the MDP is known. In a finite or countable state-space setting of specific queueing models where the parameters can be estimated, many works [2; 17; 53; 32; 31; 14; 21; 16] have used forced exploration type schemes to obtain either regret that is constant or scaling logarithmically in the time-horizon.

Another line of work studies the problem of learning the optimal policy in an undiscounted finite-horizon MDP with a bounded reward function. Reference [66] uses a Thompson sampling-based learning algorithm with linear value function approximation to study an MDP with a bounded reward function in a finite-horizon setting. Reference [15] considers an episodic finite-horizon MDP with known bounded rewards but unknown transition kernels modeled using linearly parameterized exponential families. A maximum likelihood (ML) based algorithm coupled with exploration done by constructing high probability confidence sets around the ML estimate is used to learn the unknown parameters. In another work, [48] extends the problem setting of [15] to an episodic finite-horizon MDP with unknown rewards and transitions modeled using parametric bilinear exponential families. To learn the unknown parameters, they use a ML based algorithm with exploration done with explicit perturbation. We note that all mentioned works consider a finite-horizon problem. In contrast, our work considers an average cost problem, an infinite-horizon setting, and provides finite-time performance guarantees. In addition, these works focus on an MDP with a bounded reward function. Our focus, however, is learning in MDPs with unbounded rewards with the goal of covering practical queueing examples. We note that the parameterization of transitions used in [48; 15] can be used within our framework. However, similar to our work, additional stability assumptions are necessary to guarantee asymptotic learning and sub-linear regret. Another issue with exponential transition families is that they do not allow for \(0\) entries, which limits their applicability in queueing models.

In another work, [51] studies discounted MDPs with unknown dynamics, and unbounded state-space, but with bounded rewards, and learns an online policy that satisfies a specific notion of stability. It is also assumed that a Lyapunov function ensuring stability for the optimal policy exists. We note that [51] ignores optimality and focuses on finding a stable policy, which contrasts with our work that evaluates performance relative to the optimal policy. Secondly, [51] considers a discounted reward problem, essentially a finite-time horizon problem. Average cost problems, such as ours, are infinite-time horizon problems, so connections to discounted problems can only be made in the limit of the discount parameter going to \(1\). Moreover, [51] considers a bounded reward function, simplifying their analysis but not practical for many queueing examples. Further, the assumption of a stable optimal policy with a Lyapunov function (as in [51]) is highly restrictive for bounded reward settings with discounting. Additionally, average cost problems with bounded costs need strong state-independent recurrence conditions for the existence of (stationary) optimal solutions, which many queueing examples don't satisfy; see [12]. Further complications can also arise with bounded costs: e.g., [20] shows that a stationary average cost optimal policy may not exist.

## 2 Problem formulation

We consider a family of discrete-time Markov Decision Processes (MDPs) governed by parameter \(\theta\in\Theta\) with the MDP for parameter \(\theta\) described by \((\mathcal{X},\mathcal{A},c,P_{\theta})\). For exposition purposes, we assume that all the MDPs are on (a common) countably infinite state-space \(\mathcal{X}=\mathbb{Z}_{+}^{d}\). We denote the finite action space by \(\mathcal{A}\), the transition kernel by \(P_{\theta}:\mathcal{X}\times\mathcal{A}\rightarrow\Delta(\mathcal{X})\), and the cost function by \(c:\mathcal{X}\times\mathcal{A}\rightarrow\mathbb{R}_{+}\). As mentioned earlier, we will take a Bayesian view of the problem and assume that the model is generated using an unknown parameter \(\theta^{*}\in\Theta\), which is generated from a given fixed prior distribution \(\nu(\cdot)\) on \(\Theta\). Our goal is to find a policy \(\pi:\mathcal{X}\rightarrow\mathcal{A}\) that tries to achieve Bayesian optimal performance in policy class \(\Pi\), i.e., minimizes the expected regret with \(\theta^{*}\) chosen from the prior distribution \(\nu(\cdot)\). For each value \(\theta\in\Theta\), the minimum infinite-horizon average cost is defined as

\[J(\theta)=\inf_{\pi\in\Pi}\limsup_{T\rightarrow\infty}\frac{1}{T}\operatorname {\mathbb{E}}\big{[}\sum_{t=1}^{T}c(\mathbf{X}(t),A(t))\big{]}, \tag{1}\]

where we optimize over a given class of policies \(\Pi\) and \(\mathbf{X}(t)=(X_{1}(t),\ldots,X_{d}(t))\in\mathcal{X}\) and \(A(t)\in\mathcal{A}\) are the state and action at \(t\in\mathbb{N}\). Typically, we set this class to be all (causal) policies, but it is also possible to consider \(\Pi\) to be a proper subset of all policies as we will explore in our results. For a learning policy \(\pi_{L}\) that aims to select the optimal control without model knowledge but with knowledge of \(\Theta\) and the prior \(\nu\), the Bayesian regret until time horizon \(T\geq 2\) is defined as

\[R(T,\pi_{L})=\mathbb{E}\big{[}\sum_{t=1}^{T}\big{[}c(\mathbf{X}(t),A(t))-J(\mathbf{\theta }^{*})\big{]}\big{]}, \tag{2}\]

where the expectation is taken over \(\mathbf{\theta}^{*}\sim\nu\) and the dynamics induced by \(\pi_{L}\). Owing to underlying challenges in countable state-space MDPs, we require the below assumptions on the cost function.

**Assumption 1**.: _The cost function \(c:\mathcal{X}\times\mathcal{A}\rightarrow\mathbb{R}_{+}\) is assumed to satisfy the following two conditions:_

1. _For every number_ \(z\geq 0\) _and action_ \(a\in\mathcal{A}\)_,_ \(c(\mathbf{x},a)\geq z\) _outside a finite subset of_ \(\mathcal{X}\)_._
2. _The cost function is upper-bounded by a multivariate polynomial_ \(f_{c}:\mathbb{Z}_{+}^{d}\rightarrow\mathbb{R}_{+}\) _which is increasing in every component on_ \(\mathbf{x}\in\mathbb{Z}_{+}^{d}\) _and has maximum degree of_ \(r\)__\((\geq 1)\) _in any dimension. We can assume that_ \(f_{c}(\mathbf{x})=K\sum_{i=1}^{d}(x_{i})^{r}\) _for some_ \(K>0\)_, where_ \(\mathbf{x}=(x_{1},\ldots,x_{d})\)_._

Thus, the cost function increases without bound (in the state) at a polynomial rate. This assumption is common in practice--holding costs in queueing models are polynomial in the state components. To avoid technical issues the infinite state-space setting also necessitates some assumptions on the class from which the unknown model is drawn. For instance, irreducibility of Markov chains on such state-spaces does not ensure positive recurrence (and ergodicity). Moreover, for average cost optimal control problems, without stability even the existence of an optimal policy is not guaranteed, and we need more conditions. The following assumption ensures a skip-free behaviour for transitions, which holds in many queueing models, where an increase in state corresponds to (new) arrivals.

**Assumption 2**.: _From any state-action pair \((\mathbf{x},a)\), the transition is to a finite number of states. We also assume that all transition kernels are skip-free to the right: for some \(h\geq 1\) which is independent of \(\theta\in\Theta\) and \((\mathbf{x},a)\in\mathcal{X}\times\mathcal{A}\), we have \(P_{\theta}(\mathbf{x}^{\prime};\mathbf{x},a)=0\) for all \(\mathbf{x}^{\prime}\in\{\tilde{\mathbf{x}}\in\mathbb{Z}_{+}^{d}:\left\|\tilde{\mathbf{x}} \right\|_{1}>\left\|\mathbf{x}\right\|_{1}+h\}\)._

Learning necessitates some commonalities within the class of models so that using a policy well-suited to one model provides information on other models too. For us, these are in the form of constraints on the transition kernels of the models and stability assumptions. As simple union bound arguments don't work in the countably infinite state-space setting, we will use the stability assumptions instead. In our setting, we consider a class of models, each with a policy being well-suited to at least one model in the class, and use the set of policies to search within. Using a reduced set of policies is necessary as the number of deterministic stationary policies is infinite. To learn correctly while restricting attention to this subset policy class, requires some regularity assumptions when a policy well-suited to one model is tried on a different model. Our ergodicity assumptions are one convenient choice; see Appendix A.1 for details. These assumptions let us characterize the distributions of the first passage times of the Markov processes via stability conditions; see Lemmas 10 and 11.

**Assumption 3**.: _For any MDP \((\mathcal{X},\mathcal{A},c,P_{\theta})\) with parameter \(\theta\in\Theta\), there exists a unique optimal policy \(\pi_{\theta}^{*}\) that minimizes the infinite-horizon average cost within the class of policies \(\Pi\). Furthermore, for any \(\theta_{1},\theta_{2}\in\Theta\), the Markov process with transition kernel \(P_{\theta_{1}}^{\pi_{2}^{*}}\) obtained from the MDP \((\mathcal{X},\mathcal{A},c,P_{\theta_{1}})\) by following policy \(\pi_{\theta_{2}}^{*}\) is irreducible, aperiodic, and geometrically ergodic with geometric ergodicity coefficient \(\gamma_{\theta_{1},\theta_{2}}^{g}\in(0,1)\) and stationary distribution \(\mu_{\theta_{1},\theta_{2}}\). This is equivalent to the existence of finite set \(C_{\theta_{1},\theta_{2}}^{g}\) and Lyapunov function \(V_{\theta_{1},\theta_{2}}^{g}:\mathcal{X}\rightarrow[1,+\infty)\) satisfying_

\[\Delta V_{\theta_{1},\theta_{2}}^{g}(\mathbf{x})\leq-\big{(}1-\gamma_{\theta_{1}, \theta_{2}}^{g}\big{)}V_{\theta_{1},\theta_{2}}^{g}(\mathbf{x}),\ \mathbf{x}\in \mathcal{X}\setminus C_{\theta_{1},\theta_{2}}^{g}\text{ and }P_{\theta_{1}}^{ \pi_{\theta_{2}}^{*}}V_{\theta_{1},\theta_{2}}^{g}(\mathbf{x})<+\infty,\ \mathbf{x}\in C_{\theta_{1},\theta_{2}}^{g},\]

_where \(\Delta V_{\theta_{1},\theta_{2}}^{g}(\mathbf{x}):=P_{\theta_{1}}^{\pi_{\theta_{2}}^ {*}}V_{\theta_{1},\theta_{2}}^{g}(\mathbf{x})-V_{\theta_{1},\theta_{2}}^{g}(\mathbf{ x})\). Setting \(b_{\theta_{1},\theta_{2}}^{g}:=\max_{\mathbf{x}\in C_{\theta_{1},\theta_{2}}^{g}}P_{ \theta_{1}}^{\pi_{\theta_{2}}^{*}}V_{\theta_{1},\theta_{2}}^{g}(\mathbf{x})+V_{ \theta_{1},\theta_{2}}^{g}(\mathbf{x})\) yields_

\[\Delta V_{\theta_{1},\theta_{2}}^{g}(\mathbf{x})\leq-\big{(}1-\gamma_{\theta_{1}, \theta_{2}}^{g}\big{)}V_{\theta_{1},\theta_{2}}^{g}(\mathbf{x})+b_{\theta_{1}, \theta_{2}}^{g}\mathbb{I}_{C_{\theta_{1},\theta_{2}}^{g}}(\mathbf{x}),\quad\mathbf{ x}\in\mathcal{X}. \tag{3}\]

_Then, we have the following assumptions relating all the models in \(\Theta\):_

1. _The geometric ergodicity coefficient is uniformly bounded below_ \(1\)_:_ \(\gamma_{*}^{g}:=\sup_{\theta_{1},\theta_{2}\in\Theta}\gamma_{\theta_{1},\theta_ {2}}^{g}<1\)_._
2. _We assume that_ \(\{0^{d}\}\subseteq\cap_{\theta_{1},\theta_{2}\in\Theta}C_{\theta_{1},\theta_{2}} ^{g}\) _and_ \(C_{*}^{g}=\cup_{\theta_{1},\theta_{2}\in\Theta}C_{\theta_{1},\theta_{2}}^{g}\) _is a finite set. We further assume that_ \(b_{*}^{g}:=\sup_{\theta_{1},\theta_{2}}b_{\theta_{1},\theta_{2}}^{g}<+\infty\)

**Remark 1**.: _The uniqueness of the optimal policy is not essential for the validity of our results, provided that all optimal policies satisfy our assumptions. When this condition is not met, we need to select an optimal policy that is geometrically ergodic for all \(\theta\in\Theta\). This issue can be avoided by using a smaller subset of policies for which ergodicity can be shown, such as Max-Weight policies._

Geometric ergodicity implies that all moments of the hitting time of state \(0^{d}\), say \(\tau_{0^{d}}\), from any initial state \(\mathbf{x}\neq 0^{d}\) are finite as \(\mathbb{E}_{\mathbf{x}}[\kappa^{\tau_{0^{d}}}]\leq c_{1}V^{g}(\mathbf{x})\) (for specific \(\kappa>1\) and \(c_{1}\)), and so, \(\mathbb{E}_{\mathbf{x}}[\tau_{0^{d}}^{k}]\leq c_{1}V^{g}(\mathbf{x})k^{1}/\log^{k}( \kappa)<+\infty\) for all \(k\in\mathbb{N}\); see Appendix A.2. Function \(V^{g}\) is typically exponential in some norm of the state and yields an exponential bound for moments of hitting times, and a poor regret bound. To improve the regret bound, we need a different drift equation with function \(V^{p}\) with polynomial dependence on a norm of the state that bounds certain polynomial moments of \(\tau_{0^{d}}\).

**Assumption 4**.: _Given \(\theta_{1},\theta_{2}\in\Theta\), Markov process obtained from MDP \((\mathcal{X},\mathcal{A},c,P_{\theta_{1}})\) by following policy \(\pi_{\theta_{2}}^{*}\) is polynomially ergodic through the Foster-Lyapunov criteria: there exists a finite set \(C^{p}_{\theta_{1},\theta_{2}}\), constants \(\beta^{p}_{\theta_{1},\theta_{2}}\), \(b^{p}_{\theta_{1},\theta_{2}}>0\), \(\alpha^{p}_{\theta_{1},\theta_{2}}\in[\frac{r}{r+1},1)\), and function \(V^{p}_{\theta_{1},\theta_{2}}:\mathcal{X}\rightarrow[1,+\infty)\) satisfying_

\[\Delta V^{p}_{\theta_{1},\theta_{2}}(\mathbf{x})\leq-\beta^{p}_{\theta_{1},\theta_ {2}}\big{(}V^{p}_{\theta_{1},\theta_{2}}(\mathbf{x})\big{)}^{\alpha^{p}_{\theta_{1},\theta_{2}}}+b^{p}_{\theta_{1},\theta_{2}}\mathbb{I}_{C^{p}_{\theta_{1},\theta _{2}}}(\mathbf{x}),\quad\mathbf{x}\in\mathcal{X}. \tag{4}\]

_Then, we have the following assumptions relating all the models in \(\Theta\):_

1. \(V^{p}_{\theta_{1},\theta_{2}}\) _is a polynomial with positive coefficients, maximum degree (in any dimension)_ \(r^{p}_{\theta_{1},\theta_{2}}\)_, and sum of coefficients_ \(s^{p}_{\theta_{1},\theta_{2}}\)_. We assume_ \(r^{p}_{*}=\sup_{\theta_{1},\theta_{2}}r^{p}_{\theta_{1},\theta_{2}}<\infty\) _and_ \(s^{p}_{*}=\sup_{\theta_{1},\theta_{2}}s^{p}_{\theta_{1},\theta_{2}}<\infty\)_._
2. _We assume that_ \(\{0^{d}\}\subseteq\cap_{\theta_{1},\theta_{2}\in\Theta}C^{p}_{\theta_{1}, \theta_{2}}\) _and_ \(C^{p}_{*}=\cup_{\theta_{1},\theta_{2}\in\Theta}C^{p}_{\theta_{1},\theta_{2}}\) _is a finite set. We further assume that_ \(\beta^{p}_{*}:=\inf_{\theta_{1},\theta_{2}}\beta^{p}_{\theta_{1},\theta_{2}}>0\) _and_ \(b^{p}_{*}:=\sup_{\theta_{1},\theta_{2}}b^{p}_{\theta_{1},\theta_{2}}<\infty\)_._
3. _Let_ \(K_{\theta_{1},\theta_{2}}(\mathbf{x}):=\sum_{n=0}^{\infty}2^{-n-2}\big{(}P^{\pi_{ \theta_{2}}^{*}}_{\theta_{1}}\big{)}^{n}(\mathbf{x},0^{d})\)_, which is positive for any pair_ \(\theta_{1},\theta_{2}\in\Theta\) _by irreducibility. We assume that it is strictly positive in_ \(\Theta\)_:_ \(K_{*}:=\inf_{\theta_{1},\theta_{2}}\min_{\mathbf{x}\in C^{p}_{*}}K_{\theta_{1}, \theta_{2}}(\mathbf{x})>0\)_._

Assumptions 3-4 hold in many models of interest; see Appendix E. As average cost optimality is our design criterion, we need to ensure the existence of solutions to ACOE when \(\Pi\) is the set of all policies, or Poisson equation when \(\Pi\) is a subset of all policies. We discuss these two cases separately.

_Case 1: \(\Pi\) is the set of all policies._ For any parameter \(\theta\in\Theta\), the MDP \((\mathcal{X},\mathcal{A},c,P_{\theta})\) is said to satisfy the ACOE if there exists a constant \(J(\theta)\) and a unique function \(v(\cdot;\theta):\mathcal{X}\rightarrow\mathbb{R}\) such that

\[J(\theta)+v(\mathbf{x};\theta)=\min_{a\in\mathcal{A}}\big{\{}c(\mathbf{x},a)+\sum_{ \mathbf{y}\in\mathcal{X}}P_{\theta}(\mathbf{y}|\mathbf{x},a)v(\mathbf{y};\theta)\big{\}}\text{ with }\ v(0^{d};\theta)=0.\]

From [13] if the following conditions hold, ACOE has a solution, \(J_{\theta}\) is the optimal infinite-horizon average cost, and there is an optimal stationary policy with ACOE becoming (5): (i) for every \((\mathbf{x},a)\) and \(z\geq 0\), cost function \(c(\mathbf{x},a)\geq z\) outside a finite subset of \(\mathcal{X}\); (ii) there is a stationary policy with an irreducible and aperiodic Markov process with finite average cost; and (iii) from every \((\mathbf{x},a)\) transition to a finite number of states is possible. From Assumptions 1-3, the above conditions hold.

_Case 2: \(\Pi\) is a proper subset of all policies._ Here, we posit that for every \(\theta\in\Theta\) and its best in-class policy \(\pi_{\theta}^{*}\), there exists a constant \(J(\theta)\), the average cost of \(\pi_{\theta}^{*}\), and a function \(v(\cdot;\theta):\mathcal{X}\rightarrow\mathbb{R}\) with

\[J(\theta)+v(\mathbf{x};\theta)=c(\mathbf{x},\pi_{\theta}^{*}(\mathbf{x}))+\sum_{\mathbf{y} \in\mathcal{X}}P_{\theta}(\mathbf{y}|\mathbf{x},\pi_{\theta}^{*}(\mathbf{x}))v(\mathbf{y}; \theta). \tag{5}\]

This holds by the solution of the Poisson equation with the appropriate forcing function. For a Markov process \(\mathbf{X}\) on the space \(\mathcal{X}\) with transition kernel \(P\) and cost function \(\bar{c}(\cdot)\), a solution to the Poisson equation [41] is a scalar \(J\) and function \(v(\cdot):\mathcal{X}\mapsto\mathbb{R}\) such that \(J+v=\bar{c}+Pv\), where \(v(\mathbf{z})=0\) for some \(\mathbf{z}\in\mathcal{X}\). In our setting using [41, Sections 9.6-9.8], for a model governed by \(\theta\in\Theta\) following policy \(\pi_{\theta}^{*}\), we show a solution to the Poisson equation exists and is given by \(v^{\pi_{\theta}^{*}}(0^{d})=0\), and

\[J(\theta)=\bar{C}^{\pi_{\theta}^{*}}(0^{d})/\mathbb{E}^{\pi_{\theta}^{*}}_{ \theta^{*}}[\tau_{0^{d}}]\text{ and }v^{\pi_{\theta}^{*}}(\mathbf{x})=\bar{C}^{\pi_{\theta}^{*}}(\mathbf{x})-J(\theta) \mathbb{E}^{\pi_{\theta}^{*}}_{\mathbf{x}}[\tau_{0^{d}}],\quad\forall\mathbf{x}\in \mathcal{X}, \tag{6}\]

where \(\bar{C}^{\pi_{\theta}^{*}}(\mathbf{x})=\mathbb{E}^{\pi_{\theta}^{*}}_{\mathbf{x}}\big{[} \sum_{i=0}^{\tau_{0^{d}}-1}c(\mathbf{X}(i),\pi_{\theta}^{*}(\mathbf{X}(i)))\big{]},\) and expectation is over trajectories of Markov chain \(\mathbf{X}\) with transition kernel \(P^{\pi_{\theta}^{*}}_{\theta}\) starting in state \(\mathbf{x}\). In Appendix A.3, we present related definitions and show that from Assumptions 3-4, the requirements for the existence and finiteness of the solutions to Poisson equation are satisfied. Finally, we assume \(\sup_{\theta\in\Theta}J(\theta)\) is finite, which typically holds as a result of the boundedness assumptions stated in Assumptions 3 or 4, along with Assumption 1.

**Remark 2**.: _In Assumption 4 we can use any other policy \(\pi_{\theta_{2}}\) such that the Markov process obtained from MDP \((\mathcal{X},\mathcal{A},c,P_{\theta_{1}})\) by following policy \(\pi_{\theta_{2}}\) is irreducible and polynomially ergodic via the Foster-Lyapunov criteria with the uniformity discussed. Irreducibility is important as the policy will be used at times when the state is not known in advance, specifically at Steps 14-17 in Algorithm 1._

**Assumption 5**.: _We assume that \(J^{*}:=\sup_{\theta\in\Theta}J(\theta)<+\infty\)._

## 3 Thompson sampling based learning algorithm

We will use the learning algorithm Thompson sampling with dynamically-sized episodes from [49] to learn the unknown parameter \(\boldsymbol{\theta}^{*}\in\Theta\) and the corresponding policy, \(\pi_{\boldsymbol{\theta}^{*}}^{*}\), but suitably modify it for our countable state-space setting. Consider the prior distribution \(\nu_{0}=\nu\) defined on \(\Theta\) from which \(\boldsymbol{\theta}^{*}\) is sampled. At each time \(t\in\mathbb{N}\), the posterior distribution \(\nu_{t}\) is updated according to Bayes' rule as

\[\nu_{t+1}(d\theta)=\frac{\mathbb{P}_{\theta}\left(\boldsymbol{X}\left(t+1 \right)|\boldsymbol{X}\left(t\right),A\left(t\right)\right)\nu_{t}(d\theta)} {\int_{\theta^{\prime}\in\Theta}\mathbb{P}_{\theta^{\prime}}\left( \boldsymbol{X}\left(t+1\right)|\boldsymbol{X}\left(t\right),A\left(t\right) \right)\nu_{t}(d\theta^{\prime})}, \tag{7}\]

and the posterior estimate \(\theta_{t+1}\), if generated, is from the posterior distribution \(\nu_{t+1}\). The modified Thompson-sampling with dynamically-sized episodes algorithm (TSDE) is presented in Algorithm 1. The TSDE algorithm operates in episodes: at the beginning of episode \(k\), parameter \(\theta_{k}\) is sampled from the posterior distribution \(\nu_{t_{k}}\) and during episode \(k\), actions are generated from the stationary policy according to \(\theta_{k}\), i.e., \(\pi_{\theta_{k}}^{*}\). Let \(t_{k}\) be the time the \(k\)-th episode begins. Define \(\tilde{t}_{k+1}\) as the first time after \(t_{k}\) that the conditions of Line 6 of Algorithm 1 is triggered and \(t_{k+1}\) as the first time at or after \(\tilde{t}_{k+1}\) where state \(0^{d}\) is visited; for the last episode started before or at \(T\), we ensure that \(t_{k}\) and \(\tilde{t}_{k}\) are less than or equal \(T+1\). Explicitly, \(t_{1}=1\) and for \(k>1\), \(t_{k}=\min\{t\geq\tilde{t}_{k}:\,\boldsymbol{X}\left(t\right)=0^{d}\text{ or }t>T\}\). Let \(T_{k}=t_{k+1}-t_{k}\) be the length of the \(k\)-th episode and set \(\tilde{T}_{k}=\tilde{t}_{k+1}-t_{k}\) with the convention \(\tilde{T}_{0}=1\). For any state-action pair \((\boldsymbol{x},a)\), we define \(N_{1}(\boldsymbol{x},a)=0\) and for \(t>1\),

\[N_{t}(\boldsymbol{x},a)=\big{|}\{t_{k}\leq i<\tilde{t}_{k+1}\leq t\text{ for some }k\geq 1:(\boldsymbol{X}(i),A(i))=(\boldsymbol{x},a)\}\big{|}.\]

Notice that for all state-action pairs \((\boldsymbol{x},a)\) and \(\tilde{t}_{k+1}\leq t\leq t_{k+1}\), we have \(N_{t}(\boldsymbol{x},a)=N_{t_{k+1}}(\boldsymbol{x},a)\). We denote \(K_{T}\) as the number of episodes started by or at time \(T\), or \(K_{T}=\max\{k:t_{k}\leq T\}\). The length of episode \(k<K_{T}\) is not fixed and is determined according to two stopping criteria: (1) \(t>t_{k}+\tilde{T}_{k-1}\), (2) \(N_{t}(\boldsymbol{x},a)>2N_{t_{k}}(\boldsymbol{x},a)\) for some state-action pair \((\boldsymbol{x},a)\). After either criterion is met, the system will still follow policy \(\pi_{\theta_{k}}^{*}\) until the first time at which state \(0^{d}\) is visited; see Line 14 and Figure 1. We use this settling period to \(0^{d}\) because the system state can be arbitrary when thefirst stopping criterion is met. As the countable state-space setting precludes a simple union-bound argument to overcome this uncertainty (as in the literature for finite state settings), we let the system reach the special state \(0^{d}\). Another (essentially equivalent) option is to wait until the state hits the finite set \(C^{g}_{s}\) or \(C^{p}_{s}\) and then use a union bound argument for all states in either set. For analytical convenience, we only use the state samples observed before arrival \(\tilde{t}_{k+1}\) to update the posterior distribution. The posterior update is halted during the settling period to \(0^{d}\) as we have no control on the states visited during it, despite it being finite in duration (by our assumptions).

## 4 Regret analysis of Algorithm 1

The performance of any learning policy \(\pi_{L}\) is evaluated using the metric of expected regret compared to the optimal expected average cost of true parameter \(\mathbf{\theta}^{*}\), namely, \(J(\mathbf{\theta}^{*})\). In this section, we evaluate the performance of Algorithm 1 and derive an upper bound for \(R(T,\pi_{TSDE})\), its expected regret up to time \(T\). In Section 2, we argued that at time \(t\) in episode \(k\) (\(t_{k}\leq t<t_{k+1}\)), there exist a constant \(J(\theta_{k})\) and a unique function \(v(\cdot;\theta_{k}):\mathcal{X}\to\mathbb{R}\) such that \(v\left(0^{d};\theta_{k}\right)=0\) and

\[J(\theta_{k})+v(\mathbf{X}(t);\theta_{k})=c(\mathbf{X}(t),\pi^{*}_{\theta_{k}}(\mathbf{X}( t)))+\sum_{\mathbf{y}\in\mathcal{X}}P_{\theta_{k}}(\mathbf{y}|\mathbf{X}(t),\pi^{*}_{\theta_ {k}}(\mathbf{X}(t)))v(\mathbf{y};\theta_{k}), \tag{8}\]

in which \(\pi^{*}_{\theta_{k}}\) is the optimal or best-in-class policy (depending on the context) according to parameter \(\theta_{k}\) and \(J(\theta_{k})\) is the average cost for the Markov process obtained from MDP \((\mathcal{X},\mathcal{A},c,P_{\theta_{k}})\) by following \(\pi^{*}_{\theta_{k}}\). We derive a bound for the expected regret \(R(T,\pi_{TSDE})\) following the proof steps of [49] while extending it to the countable state-space setting of our problem. Using (8), the regret is decomposed into three terms and each term is bounded separately:

\[R(T,\pi_{TSDE})=\mathbb{E}\left[\sum_{k=1}^{K_{T}}\sum_{t=t_{k}} ^{t_{k+1}-1}c(\mathbf{X}(t),\pi^{*}_{\theta_{k}}(\mathbf{X}(t)))\right]-T\,\mathbb{E} \left[J\left(\mathbf{\theta}^{*}\right)\right]=R_{0}+R_{1}+R_{2}, \tag{9}\] \[\text{with }R_{0}= \mathbb{E}\left[\sum_{k=1}^{K_{T}}T_{k}J(\theta_{k})\right]-T\, \mathbb{E}[J(\mathbf{\theta}^{*})],\] (10) \[R_{1}= \mathbb{E}\left[\sum_{k=1}^{K_{T}}\sum_{t=t_{k}}^{t_{k+1}-1} \big{[}v(\mathbf{X}(t);\theta_{k})-v(\mathbf{X}(t+1);\theta_{k})\big{]}\right],\] (11) \[R_{2}= \mathbb{E}\left[\sum_{k=1}^{K_{T}}\sum_{t=t_{k}}^{t_{k+1}-1} \big{[}v(\mathbf{X}(t+1);\theta_{k})-\sum_{\mathbf{y}\in\mathcal{X}}P_{\theta_{k}}(\bm {y}|\mathbf{X}(t),\pi^{*}_{\theta_{k}}(\mathbf{X}(t)))v(\mathbf{y};\theta_{k})\big{]}\right]. \tag{12}\]

Before bounding the above regret terms, we address the complexities arising from the countable state-space setting. Firstly, we need to study the maximum state (with respect to the \(\ell_{\infty}\)-norm) visited up to time \(T\) in the MDP \((\mathcal{X},\mathcal{A},c,P_{\mathbf{\theta}^{*}})\) following Algorithm 1; we denote this maximum state by \(M^{T}_{\mathbf{\theta}^{*}}\). In Appendix C, we derive upper bounds on the moments of hitting times of state \(0^{d}\) and utilize this to bound the moments of random variable \(M^{T}_{\mathbf{\theta}^{*}}\), which then lets us study the number of episodes \(K_{T}\) by time \(T\). Another challenge in analyzing the regret is that the relative value function \(v(\mathbf{x};\theta)\) is unlikely to be bounded in the countable state-space setting. Hence, in (13) and (14), we find bounds for the relative value function in terms of hitting time \(\tau_{0^{d}}\) from the initial state \(\mathbf{x}\). Based on these results, we provide an upper bound for the regret of Algorithm 1 in Theorem 1.

_Maximum state norm under polynomial and geometric ergodicity._ Here we state the results that characterize the maximum \(l_{\infty}\)-norm of the state vector achieved up until and including time \(T\), and

Figure 1: MDP evolution in episode \(k<K_{T}\).

the resulting bounds on the number of episodes executed until time \(T\). Owing to space constraints the details (including formal statements) are presented in Appendix B. The results are listed as below:

(i) In Lemma 6, we bound the moments of the maximum length of recurrence times of state \(0^{d}\), using the ergodicity assumptions 3 and 4. This, along with the skip-free property, allows us to prove that the \(p\)-th moment of \(\max_{1\leq i\leq T}\tau_{0^{d}}^{(i)}\) and \(M_{\mathbf{\theta}^{*}}^{T}\) are both of order \(O(\log^{p}T)\).

(ii) In Lemma 7, we find an upper bound for the number of episodes in which the second stopping criterion is met or there exists a state-action pair for which \(N_{t}(\mathbf{x},a)\) has increased more than twice.

(iii) In Lemma 8, we bound the total number of episodes \(K_{T}\) by time \(T\) by bounding the number of episodes triggered by the first stopping criterion, using the fact that in such episodes, \(\widehat{T}_{k}=\widehat{T}_{k-1}+1\). Moreover, to account for the settling time of each episode, we use geometric ergodicity and Lemma 6. It follows that the expected value of the number of episodes \(K_{T}\) is of the order \(\tilde{O}(\sqrt{h^{d}|\mathcal{A}|T})\).

_Regret analysis._ Next, we bound regret terms \(R_{0}\), \(R_{1}\) and \(R_{2}\) using the approach of [49] along with additional arguments to extend their result to a countably infinite state-space. We consider the relative value function \(v(\mathbf{x};\theta)\) of policy \(\pi_{\theta}^{*}\) introduced for the optimal policy in ACOE or for the best in-class policy in the Poisson equation. In either of these cases, policy \(\pi_{\theta}^{*}\) satisfies (5), which is the corresponding Poisson equation with forcing function \(c(\mathbf{x},\pi_{\theta}^{*}(\mathbf{x}))\) in a Markov chain with transition matrix \(P_{\theta}^{\pi_{\theta}^{*}}\). In (6), we presented the solution \((J,v)\) to the Poisson equation, which yields the following upper bound for the relative value function, as argued in Appendix A.3:

\[v(\mathbf{x};\theta)\leq\bar{C}^{\pi_{\theta}^{*}}(\mathbf{x})\leq\mathbb{E}_{\mathbf{x}}^ {\pi_{\theta}^{*}}\left[Kd\left(\|\mathbf{x}\|_{\infty}+h\tau_{0^{d}}\right)^{r} \tau_{0^{d}}\right]. \tag{13}\]

We can similarly lower bound the relative value function using Assumption 5 as

\[v(\mathbf{x};\theta)\geq-J(\theta)\mathbb{E}_{\mathbf{x}}^{\pi_{\theta}^{*}}[\tau_{0^{ d}}]\geq-J^{*}\mathbb{E}_{\mathbf{x}}^{\pi_{\theta}^{*}}[\tau_{0^{d}}]. \tag{14}\]

From Assumption 3, all moments of \(\tau_{0^{d}}\) and thus, the derived bounds are finite. Also, in Lemma 10 we bound the moments of \(\tau_{0^{d}}\) of order \(i\leq r+1\) using the polynomial Lyapunov function \(V_{\theta_{1},\theta_{2}}^{p}\), which is then used to bound the expected regret. We next bound the first regret term \(R_{0}\) from the first stopping criterion in terms of the number of episodes \(K_{T}\) and the settling time of each episode \(k\).

**Lemma 1**.: _The first regret term \(R_{0}\) satisfies \(R_{0}\leq J^{*}\,\mathbb{E}[K_{T}(\max_{1\leq i\leq T}\tau_{0^{d}}^{(i)}+1)]\)._

Proof of Lemma 1 is given in Appendix B.4. From Lemma 6, all moments of \(\max_{1\leq i\leq T}\tau_{0^{d}}^{(i)}\) are bounded by a polylogarithmic function. Futhermore, as a result of Lemma 8, expected value of the number of episodes \(K_{T}\) is of the order \(\tilde{O}(\sqrt{h^{d}|\mathcal{A}|T})\), which leads to a \(\tilde{O}(\sqrt{h^{d}|\mathcal{A}|T})\) regret term \(R_{0}\). Next, an upper bound on \(R_{1}\) defined in (11) is derived. In the proof of Lemma 2 we argue that as the relative value function is equal to \(0\) at all time instances \(t_{k}\) for \(k\leq K_{T}\), the only term that contributes to the regret is the value function at the end of time horizon \(T\). We use the lower bound derived in (14) to show that the second regret term \(R_{1}\) is \(\tilde{O}(1)\); the proof is given in Appendix B.5.

**Lemma 2**.: _The second regret term \(R_{1}\) satisfies \(R_{1}\leq c_{2}\,\mathbb{E}[(M_{\mathbf{\theta}^{*}}^{T})^{r^{*}}]+c_{3}\), where \(c_{2}=J^{*}2^{r^{*}_{*}}s_{*}^{p}(\beta_{*}^{p})^{-1}\) and \(c_{3}=J^{*}(\beta_{*}^{p})^{-1}\big{(}s_{*}^{p}\left(2h\right)^{r^{p}_{*}}+b_ {*}^{p}(K_{*})^{-1}\big{)}\)._

From Lemma 6, \(\mathbb{E}[(M_{\mathbf{\theta}^{*}}^{T})^{r^{*}}]\) is \(O(\log^{r^{p}_{*}}T)\); hence, \(R_{1}\) is upper bounded by a polylogarithmic function of the order \(r^{p}_{*}\). Finally, in Lemma 3, we derive an upper bound for the third regret term \(R_{2}\) defined in (12) using the bound derived for the relative value function in (13). To bound \(R_{2}\), we characterize it in terms of the difference between the empirical and true unknown transition kernel and following the concentration method used in [64, 10, 49, 7, 8], we argue that with high probability the total variation distance between the two distributions is small; for proof, see Appendix B.6.

**Lemma 3**.: _For problem-dependent constant \(c_{p_{3}}\) and polynomial \(Q(T)=c_{p_{3}}(Th)^{r+r^{p}_{*}}/48\), we have_

\[R_{2}\leq(\log(hT+h)+1)^{d}+c_{p_{3}}\sqrt{|\mathcal{A}|T}\log_{2}\left(2| \mathcal{A}|T^{2}Q(T)\right)\mathbb{E}\left[(M_{\mathbf{\theta}^{*}}^{T}+h)^{d+r +r^{p}_{*}}\big{(}\max_{1\leq i\leq T}\tau_{0^{d}}^{(i)}\big{)}\right].\]

The above Lemma results in a \(\tilde{O}(KrdJ^{*}h^{d+2r+r^{p}_{*}}\sqrt{|\mathcal{A}|T})\) regret term as a result of Lemma 6, where \(h\) is the skip-free parameter defined in Assumption 2, \(d\) is the dimension of the state-space, \(K\) and \(r\) are the cost function parameters defined in Assumption 1, \(J^{*}\) is the supremum on the optimal cost, \(r^{p}_{*}\) is defined in Assumption 4, and where \(\tilde{O}\) hides logarithmic factors in problem parameters one of which is \(\log^{d+r+r^{p}_{*}+2}(T)\). For simplicity, we have not included the Lyapunovfunctions related parameters in the regret. Finally, from Lemmas 1, 2, 3, along with the Cauchy-Schwarz inequality, we conclude that the regret of Algorithm 1\(R(T,\pi_{TSDE})(=R_{0}+R_{1}+R_{2})\) is \(\tilde{O}(KrdJ^{*}h^{d+2r+r_{*}^{2}}\sqrt{|\mathcal{A}|T})\); for brevity, we will state that regret is of the order \(\tilde{O}(dh^{d}\sqrt{|\mathcal{A}|T})\).

**Theorem 1**.: _Under Assumptions 1-5, the regret of Algorithm 1, \(R(T,\pi_{TSDE})\), is \(\tilde{O}(dh^{d}\sqrt{|\mathcal{A}|T})\)._

Theorem 1 can be extended to the problem of finding the best policy within a sub-class of policies in set \(\Pi\), which may or may not contain the optimal policy. In Section 2, we stated that Assumptions 3 and 4 hold for policies in \(\Pi\) and we used this to argue that the Poisson equation has a solution given in (6). As a result, repeating the same arguments as in Theorem 1 with the modification that \(\pi_{\theta}^{*}\) is the best in-class policy of the MDP governed by parameter \(\theta\), yields the following corollary.

**Corollary 1**.: _Under Assumptions 1 through 5, the regret of Algorithm 1 when using the best in-class policy is \(\tilde{O}(dh^{d}\sqrt{|\mathcal{A}|T})\)._

**Requirement of an optimal policy oracle.** To implement our algorithm, we need to find the optimal policy for each model sampled by the algorithm--optimal policy for Theorem 1 and optimal policy within policy class \(\Pi\) for Corollary 1. In the finite state-space setting, [49] provides a schedule of \(\epsilon\) values and selects \(\epsilon\)-optimal policies to obtain \(\tilde{O}(\sqrt{T})\) regret guarantees. The issue with extending the analysis of [49] to the countable state-space setting is that we need to ensure (uniform) ergodicity for the chosen \(\epsilon\)-optimal policies. Another issue is that, to the best of our knowledge, there isn't a general structural characterization of all \(\epsilon\)-optimal stationary policies for countable state-space MDPs or even a characterization of the policy within this set that is selected by any computational procedure in the literature; current results only discuss characterization of the stationary optimal policy. In the absence of such results, stability assumptions with the same uniformity across models as in our submission will be needed, which are likely too strong to be useful. However, if we could verify the stability requirements of Assumptions 3 and 4 for a subset of policies, the optimal oracle is not needed, and instead, by choosing approximately optimal policies within this subset, we can follow the same proof steps as [49] to guarantee regret performance similar to Corollary 1 (without knowledge of model parameters). Thus, in Theorem 2 we extend the previous regret guarantees to the algorithm employing \(\epsilon\)-optimal policy; proof is given in Appendix B.8.

**Theorem 2**.: _Consider a non-negative sequence \(\{\epsilon_{k}\}_{k=1}^{\infty}\) such that for every \(k\in\mathbb{N}\), \(\epsilon_{k}\) is bounded above by \(\frac{1}{k+1}\) and an \(\epsilon_{k}\)-optimal policy satisfying Assumptions 3 and 4 is given. The regret incurred by Algorithm 1 while using the \(\epsilon_{k}\)-optimal policy during any episode \(k\) is \(\tilde{O}(dh^{d}\sqrt{|\mathcal{A}|T})\)._

## 5 Evaluation and Conclusion: Application of Algorithm 1 to queueing models

Next, we present an evaluation of our algorithm. We study two different queueing models shown in Figure 2, each with Poisson arrivals at rate \(\lambda\), and two heterogeneous servers with exponentially distributed services times with unknown service rate vector \(\boldsymbol{\theta}^{*}=(\theta_{1}^{*},\theta_{2}^{*})\). Vector \(\boldsymbol{\theta}^{*}\) is sampled from the prior distribution \(\nu\) defined on the space \(\Theta\) given as \(\Theta=\left\{(\theta_{1},\theta_{2})\in\mathbb{R}_{+}^{2}:\frac{\lambda}{ \theta_{1}+\theta_{2}}\leq\frac{1-\delta}{1+\delta},1\leq\frac{\theta_{1}}{ \theta_{2}}\leq R\right\}\), for fixed \(R\geq 1\) and \(\delta\in(0,0.5)\). The first condition ensures the stability of the queueing models, while the second guarantees the compactness of the parameter space of the parameterized policies. In both systems, the goal of the dispatcher is to minimize the expected sojourn time of jobs, which by Little's law [52] is equivalent to minimizing the average number of jobs in the system. After verifying Assumptions 1-5 in Appendix E for the cost function \(c(\boldsymbol{x})=\|\boldsymbol{x}\|_{1}\), Theorem 1 yields a Bayesian regret of order \(\tilde{O}(\sqrt{|\mathcal{A}|T})\) for Algorithm 1.

**Model 1**.: _Two-server queueing system with a common buffer._ We consider the continuous-time queueing system of Figure 1(a), where the countable state space is \(\mathcal{X}=\{\boldsymbol{x}=(x_{0},x_{1},x_{2})\in\mathbb{Z}_{+}\times\{0,1\} ^{2}\}\), where \(x_{0}\) is the queue length, and \(x_{i}\), \(i=1,2\) equal \(1\) if server \(i\) is busy. The action space is \(\mathcal{A}=\{h,b,1,2\}\), where \(h\) means no action, \(b\) sends a job to both servers, and \(i=1,2\) assigns a job to server \(i\). In [38], it is shown that by uniformization [39] and sampling the continuous-time Markov process at rate \(\lambda+\theta_{1}^{*}+\theta_{2}^{*}\), a discrete-time Markov chain is obtained, which converts the original continuous-time problem to an equivalent discrete-time problem where we need to minimize \(\limsup_{T\rightarrow\infty}T^{-1}\sum_{t=0}^{T-1}\|\boldsymbol{X}(t)\|_{1}\). Further, [38] shows that the optimal policy is a threshold policy \(\pi_{t(\boldsymbol{\theta}^{*})}\) with optimal finite threshold \(t(\boldsymbol{\theta}^{*})\in\mathbb{N}\): always assign a job to the faster (first) server when free, and to the second server if it is free and \(\|\boldsymbol{x}\|_{1}>t(\boldsymbol{\theta}^{*})\), and take no action otherwise. InAppendix E.1, we argue that the discrete-time Markov process governed by \(\theta\in\Theta\) and following threshold policy \(\pi_{t}\) for any threshold \(t\) belonging to a compact set satisfies Assumptions 1-5.

_Model 2. Two heterogeneous parallel queues._ We consider the continuous-time queueing system of Figure 1(b) with countable state space \(\mathcal{X}=\{\pi=(x_{1},x_{2})\in\mathbb{Z}_{+}^{2}\}\), where \(x_{i}\) is the number of jobs in the server-queue pair \(i\). The action space is \(\mathcal{A}=\{1,2\}\), where action \(i\) sends the arrival to queue \(i\). We obtain the discrete-time MDP by sampling the queueing system at the arrivals, and then aim to find the average cost minimizing policy within the class \(\Pi=\{\pi_{\omega};\omega\in[(c_{R}R)^{-1},c_{R}R]\}\), \(c_{R}\geq 1\). Policy \(\pi_{\omega}:\mathcal{X}\rightarrow\mathcal{A}\) routes arrivals based on the weighted queue lengths: \(\pi_{\omega}(\mathbf{x})=\arg\min\left(1+x_{1},\omega\left(1+x_{2}\right)\right)\) with ties broken for \(1\). Even with the transition kernel fully specified (by the values of arrival and service rates), the optimal policy in \(\Pi\) is not known except when \(\theta_{1}=\theta_{2}\) where the optimal value is \(\omega=1\), and so, to learn it, we will use Proximal Policy Optimization for countable state-space MDPs [18]. Note that [18] requires full model knowledge, which holds in our scheme as we use parameters sampled from the posterior for choosing the policy at the beginning of each episode. In Appendix E.2, we argue that the discrete-time Markov process governed by parameter \(\theta\in\Theta\) and following policy \(\pi_{\omega}\) for \(\omega\in[(c_{R}R)^{-1},c_{R}R]\) satisfies Assumptions 1-5.

Next, we report the numerical results of Algorithm 1 in the two queueing models of Figure 2 and calculate regret using (2). The regret is averaged over 2000 simulation runs and plotted against the number of transitions in the sampled discrete-time Markov process. Figure 3 shows the behavior of the regret of the two queuing models for three different arrival rates and service rates distributed according to a Dirichlet prior over \([0.5,1.9]^{2}\). We observe that the regret is sub-linear in time and grows as the arrival rate increases. For the queueing model of Figure 1(a), the minimum average cost \(J(\theta)\) and optimal policy \(\pi_{\theta}^{*}\) are known explicitly [38] for every \(\theta\in\Theta\), which are used in Algorithm 1 and for regret calculation. Conversely, for the second queueing model, \(J(\theta)\) and \(\pi_{\theta}^{*}\) are not known. The PPO algorithm [18] is used to empirically find both the optimal weight and the policy's average cost. Additional details of the simulations and more plots are presented in Appendix G.

**Conclusions and future work.** We studied the problem of learning optimal policies in countable state-space MDPs governed by unknown parameters. We proposed a learning policy based on Thompson sampling and established finite-time performance guarantees on the Bayesian regret. We highlighted the practicality of our proposed algorithm by considering two different queuing models and showing that our algorithm can be applied to develop optimal control policies. For future work we plan two directions to explore: to generalize our algorithm to consider polices that might not all be stabilizing, and also to simplify the algorithm using ideas from [61, 57].

Figure 3: Regret performance for \(\lambda=0.3,0.5,0.7\). Shaded region shows the \(\pm\sigma\) area of mean regret.

Figure 2: Two-server queueing systems with heterogeneous service rates.

## Disclosure of Funding

SA's research was supported by NSF via grants ECCS2038416, CCF2008130, and CNS1955777, and a grant from General Dynamics via MIDAS at the University of Michigan, Ann Arbor. VS's research is supported in part by NSF via grants ECCS2038416, CCF2008130, CNS1955777, and CMMI2240981.

## References

* Abbasi-Yadkori and Szepesvari [2014] Yasin Abbasi-Yadkori and Csaba Szepesvari. Bayesian optimal control of smoothly parameterized systems: The lazy posterior sampling algorithm. _arXiv preprint arXiv:1406.3926_, 2014.
* Adler et al. [2022] Saghar Adler, Mehrdad Moharrami, and Vijay Subramanian. Learning a discrete set of optimal allocation rules in queueing systems with unknown service rates. _arXiv preprint arXiv:2202.02419_, 2022.
* Agrawal et al. [1989] R. Agrawal, D. Teneketzis, and V. Anantharam. Asymptotically efficient adaptive allocation schemes for controlled Markov chains: Finite parameter space. _IEEE Transactions on Automatic Control_, 34(12):1249-1259, 1989.
* Agrawal and Teneketzis [1989] Rajeev Agrawal and Demosthenis Teneketzis. Certainty equivalence control with forcing: Revisited. _Systems & Control Letters_, 13(5):405-412, 1989.
* Agrawal and Jia [2017] Shipra Agrawal and Randy Jia. Optimistic posterior sampling for reinforcement learning: Worst-case regret bounds. _Advances in Neural Information Processing Systems_, 30, 2017.
* Agrawal and Jia [2019] Shipra Agrawal and Randy Jia. Learning in structured MDPs with convex cost functions: Improved regret bounds for inventory management. In _Proceedings of the 2019 ACM Conference on Economics and Computation_, pages 743-744, 2019.
* Akbarzadeh and Mahajan [2022] Nima Akbarzadeh and Aditya Mahajan. On learning Whittle index policy for restless bandits with scalable regret. _arXiv preprint arXiv:2202.03463_, 2022.
* Akbarzadeh and Mahajan [2023] Nima Akbarzadeh and Aditya Mahajan. On learning Whittle index policy for restless bandits with scalable regret. _IEEE Transactions on Control of Network Systems_, 2023.
* Arapostathis et al. [1993] Aristotle Arapostathis, Vivek S Borkar, Emmanuel Fernandez-Gaucherand, Mrinal K Ghosh, and Steven I Marcus. Discrete-time controlled Markov processes with average cost criterion: A survey. _SIAM Journal on Control and Optimization_, 31(2):282-344, 1993.
* Auer et al. [2008] Peter Auer, Thomas Jaksch, and Ronald Ortner. Near-optimal regret bounds for reinforcement learning. _Advances in Neural Information Processing Systems_, 21, 2008.
* Borkar [1990] V S Borkar. The Kumar-Becker-Lin scheme revisited. _Journal of Optimization Theory and Applications_, 66:289-309, 1990.
* Cavazos-Cadena [1989] Rolando Cavazos-Cadena. Necessary conditions for the optimality equation in average-reward Markov decision processes. _Applied Mathematics and Optimization_, 19(1):97-112, 1989.
* Cavazos-Cadena [1989] Rolando Cavazos-Cadena. Weak conditions for the existence of optimal stationary policies in average Markov decision chains with unbounded costs. _Kybernetika_, 25(3):145-156, 1989.
* Choudhury et al. [2021] Tuhinangshu Choudhury, Gauri Joshi, Weina Wang, and Sanjay Shakkottai. Job dispatching policies for queueing systems with unknown service rates. In _Proceedings of the Twenty-Second International Symposium on Theory, Algorithmic Foundations, and Protocol Design for Mobile Networks and Mobile Computing_, MobiHoc '21, page 181-190, New York, NY, USA, 2021. Association for Computing Machinery.
* Chowdhury et al. [2021] Sayak Ray Chowdhury, Aditya Gopalan, and Odalric-Ambrym Maillard. Reinforcement learning in parametric MDPs with exponential families. In _International Conference on Artificial Intelligence and Statistics_, pages 1855-1863. PMLR, 2021.
* Cohen et al. [2024] Asaf Cohen, Vijay Subramanian, and Yili Zhang. Learning-based optimal admission control in a single-server queuing system. _Stochastic Systems_, 2024.
* Cohen et al. [2022] Asaf Cohen, Vijay G. Subramanian, and Yili Zhang. Learning-based optimal admission control in a single server queuing system. [https://arxiv.org/abs/2212.11316](https://arxiv.org/abs/2212.11316), 2022.
* Dai and Gluzman [2022] Jim G Dai and Mark Gluzman. Queueing network controls via deep reinforcement learning. _Stochastic Systems_, 12(1):30-67, 2022.

* Ephremides et al. [1980] Anthony Ephremides, Pravin Varaiya, and Jean Walrand. A simple dynamic routing problem. _IEEE Transactions on Automatic Control_, 25(4):690-693, 1980.
* Fisher and Ross [1968] Lloyd Fisher and Sheldon M Ross. An example in denumerable decision processes. _The Annals of Mathematical Statistics_, 39(2):674-675, 1968.
* Freund et al. [2022] Daniel Freund, Thodoris Lykouris, and Wentao Weng. Efficient decentralized multi-agent learning in asymmetric queuing systems. In _Conference on Learning Theory_, pages 4080-4084. PMLR, 2022.
* Ghavamzadeh et al. [2015] Mohammad Ghavamzadeh, Shie Mannor, Joelle Pineau, and Aviv Tamar. Bayesian reinforcement learning: A survey. _Foundations and Trends(r) in Machine Learning_, 8(5-6):359-483, 2015.
* Gopalan and Mannor [2015] Aditya Gopalan and Shie Mannor. Thompson sampling for learning parameterized Markov decision processes. In _Conference on Learning Theory_, pages 861-898. PMLR, 2015.
* Graves and Lai [1997] Todd L Graves and Tze-Leung Lai. Asymptotically efficient adaptive choice of control laws incontrolled Markov chains. _SIAM Journal on Control and Optimization_, 35(3):715-743, 1997.
* Hajek [1982] Bruce Hajek. Hitting-time and occupation-time bounds implied by drift analysis with applications. _Advances in Applied Probability_, 14(3):502-525, 1982.
* Hajek [1984] Bruce Hajek. Optimal control of two interacting service stations. _IEEE Transactions on Automatic Control_, 29(6):491-499, 1984.
* Hordijk and Spieksma [1992] Arie Hordijk and Flora Spieksma. On ergodicity and recurrence properties of a Markov chain by an application to an open Jackson network. _Advances in Applied Probability_, 24(2):343-376, 1992.
* Jahromi et al. [2022] Mehdi Jafamia Jahromi, Rahul Jain, and Ashutosh Nayyar. Online learning for unknown partially observable MDPs. In _International Conference on Artificial Intelligence and Statistics_, pages 1712-1732. PMLR, 2022.
* Jarner and Roberts [2002] Soren F Jarner and Gareth O Roberts. Polynomial convergence rates of Markov chains. _The Annals of Applied Probability_, 12(1):224-247, 2002.
* Krishnasamy et al. [2018] Subhashini Krishnasamy, PT Akhil, Ari Arapostathis, Rajesh Sundaresan, and Sanjay Shakkottai. Augmenting Max-Weight with explicit learning for wireless scheduling with switching costs. _IEEE/ACM Transactions on Networking_, 26(6):2501-2514, 2018.
* Krishnasamy et al. [2018] Subhashini Krishnasamy, Ari Arapostathis, Ramesh Johari, and Sanjay Shakkottai. On learning the c\(\mu\) rule in single and parallel server networks. [https://arxiv.org/abs/1802.06723](https://arxiv.org/abs/1802.06723), 2018.
* Krishnasamy et al. [2021] Subhashini Krishnasamy, Rajat Sen, Ramesh Johari, and Sanjay Shakkottai. Learning unknown service rates in queues: A multiarmed bandit approach. _Operations Research_, 69(1):315-330, 2021.
* Kumar and Becker [1982] P R Kumar and A Becker. A new family of optimal adaptive controllers for Markov chains. _IEEE Transactions on Automatic Control_, 27(1):137-146, 1982.
* Kumar and Lin [1982] P R Kumar and Woei Lin. Optimal adaptive controllers for unknown Markov chains. _IEEE Transactions on Automatic Control_, 27(4):765-774, 1982.
* Kumar and Varaiya [2015] P R Kumar and Pravin Varaiya. _Stochastic systems: Estimation, identification, and adaptive control_. SIAM, 2015.
* Lai and Yakowitz [1995] Tze-Leung Lai and Sidney Yakowitz. Machine learning and nonparametric bandit theory. _IEEE Transactions on Automatic Control_, 40(7):1199-1209, 1995.
* Larsen [1981] Ronald Larsen. _Control of multiple exponential servers with application to computer systems_. PhD thesis, University of Maryland, 1981.
* Lin and Kumar [1984] Woei Lin and P R Kumar. Optimal control of a queueing system with two heterogeneous servers. _IEEE Transactions on Automatic Control_, 29(8):696-703, 1984.

* [39] Steven A Lippman. Applying a new device in the optimization of exponential queuing systems. _Operations Research_, 23(4):687-710, 1975.
* [40] Ashok P. Maitra. _Dynamic programming for countable state systems_. PhD thesis, University of California, Berkeley, 1963.
* [41] Armand M Makowski and Adam Shwartz. The Poisson equation for countable Markov chains: Probabilistic methods and interpretations. _Handbook of Markov Decision Processes: Methods and Applications_, pages 269-303, 2002.
* [42] Akshay Mete, Rahul Singh, Xi Liu, and P R Kumar. Reward biased maximum likelihood estimation for reinforcement learning. In _Learning for Dynamics and Control_, pages 815-827. PMLR, 2021.
* [43] Sean P Meyn and Richard L Tweedie. _Markov chains and stochastic stability_. Springer Science & Business Media, 2012.
* [44] Michael J Neely, Scott T Rager, and Thomas F La Porta. Max-Weight learning algorithms for scheduling in unknown environments. _IEEE Transactions on Automatic Control_, 57(5):1179-1191, 2012.
* [45] Pedro A Ortega and Daniel A Braun. A minimum relative entropy principle for learning and acting. _Journal of Artificial Intelligence Research_, 38:475-511, 2010.
* [46] Ian Osband, Daniel Russo, and Benjamin Van Roy. (More) Efficient reinforcement learning via posterior sampling. _Advances in Neural Information Processing Systems_, 26, 2013.
* [47] Ian Osband and Benjamin Van Roy. Why is posterior sampling better than optimism for reinforcement learning? In _International Conference on Machine Learning_, pages 2701-2710. PMLR, 2017.
* [48] Reda Ouhamma, Debabrota Basu, and Odalric Maillard. Bilinear exponential family of MDPs: Frequentist regret bound with tractable exploration and planning. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 37, pages 9336-9344, 2023.
* [49] Yi Ouyang, Mukul Gagrani, Ashutosh Nayyar, and Rahul Jain. Learning unknown Markov decision processes: A Thompson sampling approach. _Advances in Neural Information Processing Systems_, 30, 2017.
* [50] Daniel J Russo, Benjamin Van Roy, Abbas Kazerouni, Ian Osband, and Zheng Wen. A tutorial on Thompson sampling. _Foundations and Trends(r) in Machine Learning_, 11(1):1-96, 2018.
* [51] Devavrat Shah, Qiaomin Xie, and Zhi Xu. Stable reinforcement learning with unbounded state space. _arXiv preprint arXiv:2006.04353_, 2020.
* [52] R. Srikant and Lei Ying. _Communication networks: An optimization, control, and stochastic networks perspective_. Cambridge University Press, 2013.
* [53] Thomas Stahlbuhk, Brooke Shrader, and Eytan Modiano. Learning algorithms for minimizing queue length regret. _IEEE Transactions on Information Theory_, 67(3):1759-1781, 2021.
* [54] Shaler Stidham and Richard Weber. A survey of Markov decision models for control of networks of queues. _Queueing Systems_, 13:291-314, 1993.
* [55] Malcolm Strens. A Bayesian framework for reinforcement learning. In _ICML_, volume 2000, pages 943-950, 2000.
* [56] Wojciech Szpankowski and Vernon Rego. Yet another application of a binomial recurrence order statistics. _Computing_, 43(4):401-410, 1990.
* [57] Dengwang Tang, Rahul Jain, Botao Hao, and Zheng Wen. Efficient online learning with offline datasets for infinite horizon MDPs: A Bayesian approach. _arXiv preprint arXiv:2310.11531_, 2023.

* [58] Leandros Tassiulas and Anthony Ephremides. Jointly optimal routing and scheduling in packet ratio networks. _IEEE Transactions on Information Theory_, 38(1):165-168, 1992.
* [59] Leandros Tassiulas and Anthony Ephremides. Dynamic server allocation to parallel queues with randomly varying connectivity. _IEEE Transactions on Information Theory_, 39(2):466-478, 1993.
* [60] Georgios Theocharous, Zheng Wen, Yasin Abbasi-Yadkori, and Nikos Vlassis. Posterior sampling for large scale reinforcement learning. _arXiv preprint arXiv:1711.07979_, 2017.
* [61] Georgios Theocharous, Zheng Wen, Yasin Abbasi Yadkori, and Nikos Vlassis. Scalar posterior sampling with applications. _Advances in Neural Information Processing Systems_, 31, 2018.
* [62] William R Thompson. On the likelihood that one unknown probability exceeds another in view of the evidence of two samples. _Biometrika_, 25(3-4):285-294, 1933.
* [63] Neil Walton and Kuang Xu. Learning and information in stochastic networks and queues. In _Tutorials in Operations Research: Emerging Optimization Methods and Modeling Techniques with Applications_, pages 161-198. INFORMS, 2021.
* [64] Tsachy Weissman, Erik Ordentlich, Gadiel Seroussi, Sergio Verdu, and Marcelo J Weinberger. Inequalities for the \(L_{1}\) deviation of the empirical distribution. _Hewlett-Packard Labs, Tech. Rep_, 2003.
* [65] Zixian Yang, R Srikant, and Lei Ying. Learning while scheduling in multi-server systems with unknown statistics: Maxweight with discounted UCB. In _International Conference on Artificial Intelligence and Statistics_, pages 4275-4312. PMLR, 2023.
* [66] Andrea Zanette, David Brandfonbrener, Emma Brunskill, Matteo Pirotta, and Alessandro Lazaric. Frequentist regret bounds for randomized least-squares value iteration. In _International Conference on Artificial Intelligence and Statistics_, pages 1954-1964. PMLR, 2020.

Proofs related to problem formulation

### Ergodicity definitions

Suppose that Markov process \(\mathbf{X}\) on \(\mathcal{X}\) with transition kernel \(P\) is irreducible, aperiodic and positive recurrent with stationary distribution \(\mu\) and let \(f:\mathcal{X}\mapsto[1,\infty)\) be a measurable function such that \(\mu(f):=\mathbb{E}_{\mu}[f(Y)]<+\infty\) with \(Y\sim\mu\). We are interested in conditions under which for a sequence of positive numbers \(\rho:=(\rho(n))_{n\geq 0}\),

\[\lim_{n\to\infty}\rho(n)\|P^{n}(\mathbf{x},\cdot)-\mu(\cdot)\|_{f}=0,\qquad\forall \mathbf{x}\in\mathcal{X}, \tag{15}\]

where for a signed measure \(\tilde{\mu}\) on \(\mathcal{X}\), \(\|\tilde{\mu}\|_{f}:=\sup_{|g|\leq f}|\tilde{\mu}(g)|.\) The sequence \(\rho\) is interpreted as the rate function, and three different notions of ergodicity are distinguished based on the following rate functions: \(\rho(n)\equiv 1\), \(\rho(n)=\zeta^{n}\) for \(\zeta>1\), and \(\rho(n)=n^{\zeta-1}\) for \(\zeta\geq 1\). Further, for each rate function \(\rho\), we state the Foster-Lyapunov characterization of ergodicity of the Markov process \(\mathbf{X}\), which provides sufficient conditions for (15) to hold.

1. If \(\rho(n)\equiv 1\) for all \(n\geq 0\), the Markov process \(\mathbf{X}\) satisfying (15) is said to be \(f\)-**ergodic**. From [43], for an irreducible and aperiodic chain, \(f\)-ergodicity is _equivalent_ to the existence of a function \(V:\mathcal{X}\mapsto[0,\infty)\), a finite set \(C\), and positive constant \(b\) such that \[\Delta V\leq-f+b\mathbb{I}_{C},\] (16) where \(\Delta V:=PV-V\) with \(PV(\mathbf{x}):=\sum_{\mathbf{x}^{\prime}\in\mathcal{X}}P(\mathbf{x},\mathbf{x}^{\prime})V(\bm {x}^{\prime})\). The drift condition (16) implies positive recurrence of the Markov process, existence of a unique stationary distribution \(\mu\), and \(\mu(f)\leq b<+\infty\) ([43], Theorem 14.3.7).
2. If \(\rho(n)=\zeta^{n}\) for some \(\zeta>1\), the Markov process \(\mathbf{X}\) satisfying (15) is said to be \(f\)-**geometrically ergodic**. From [43], for an irreducible and aperiodic chain, \(f\)-geometric ergodicity is _equivalent_ to the existence of a function \(V:\mathcal{X}\mapsto[1,\infty)\), a finite set \(C\), a constant \(\gamma\in(0,1)\) and positive constant \(b\) such that \[\Delta V\leq-(1-\gamma)V+b\mathbb{I}_{C}.\] (17) The drift condition (17) implies positive recurrence of the Markov process, existence of a unique stationary distribution \(\mu\), and \(\mu(V)\leq\frac{b}{1-\gamma}<+\infty\) ([43], Theorem 14.3.7). Moreover, if \(f(\cdot)\equiv 1\) in (15), then the Markov process \(\mathbf{X}\) is called **geometrically ergodic**.
3. If \(\rho(n)=n^{\zeta-1}\) for some \(\zeta\geq 1\), the Markov process \(\mathbf{X}\) satisfying (15) is said to be \(f\)**-**polynomially ergodic**. From [43, 29], for an irreducible and aperiodic chain, the existence of a function \(V:\mathcal{X}\mapsto[1,\infty)\), a finite set \(C\), a constant \(\alpha\in[0,1)\), and positive constants \(c\) and \(b\) such that \[\Delta V\leq-cV^{\alpha}+b\mathbb{I}_{C}\] (18) _implies_\(V_{\zeta}\)-polynomial ergodicity of \(\mathbf{X}\) at rate \(\rho(n)=n^{\zeta-1}\) for all \(\zeta\in[1,1/(1-\alpha)]\) with \(V_{\zeta}=V^{1-\zeta(1-\alpha)}\). The drift condition (18) implies positive recurrence of the Markov process, existence of a unique stationary distribution \(\mu\), and \(\mu(V^{\alpha})\leq\frac{b}{c}<+\infty\).

### Lemma 4

**Lemma 4**.: _For any state \(\mathbf{x}\neq 0^{d}\), there exists constants \(\kappa>1\) and \(c_{1}\) such that the following holds for the hitting time of state \(0^{d}\), \(\tau_{0^{d}}\),_

\[\mathbb{E}_{\mathbf{x}}[\kappa^{\tau_{0^{d}}}]\leq c_{1}V^{g}(\mathbf{x}).\]

Proof.: We define \(\tilde{V}:=\sum_{n=0}^{\infty}{}_{0^{d}}P^{n}V^{g}\) where \({}_{0^{d}}P^{n}\) is the \(n\)-step taboo probability [43] defined as

\[{}_{A}P^{n}_{\mathbf{x}B}=\mathbb{P}_{\mathbf{x}}\left(\mathbf{X}_{n}\in B,\tau_{A}>n \right),\]

for \(A,B\subseteq\mathcal{X},\) and \(\tau_{A}\) is the first hitting time of set \(A\). We also let \({}_{A}P^{0}_{\mathbf{x}B}=\mathbb{I}_{B}(\mathbf{x})\). We have

\[{}_{0^{d}}P\tilde{V}(\mathbf{x}) =\sum_{\mathbf{y}\neq 0^{d}}P_{\mathbf{x}\mathbf{y}}\tilde{V}(\mathbf{y})=\sum_{n= 0}^{\infty}\sum_{\mathbf{y},\mathbf{z}\neq 0^{d}}P_{\mathbf{x}\mathbf{y}\;\mathbf{0}^{d}}P^{n}_{\mathbf{y} \mathbf{z}}V^{g}(\mathbf{z})\] \[=\sum_{n=0}^{\infty}\sum_{\mathbf{z}\neq 0^{d}}{}_{0^{d}}P^{n+1}_{\mathbf{x} \mathbf{z}}V^{g}(\mathbf{z})=\tilde{V}(\mathbf{x})-V^{g}(\mathbf{x}).\]In Appendix D.3, we argue that there exists \(\tilde{b}^{g}>1\) such that \(\tilde{V}(\mathbf{y})\leq\tilde{b}^{g}V^{g}(\mathbf{y})\) for all \(\mathbf{y}\in\mathcal{X}\), which leads to

\[{}_{0^{d}}P\tilde{V}=\tilde{V}-V^{g}\leq\tilde{V}-\frac{1}{\tilde{b}^{g}} \tilde{V}=\left(1-\frac{1}{\tilde{b}^{g}}\right)\tilde{V}. \tag{19}\]

Define Lyapunov function

\[\tilde{V}^{g}(\mathbf{x})=\begin{cases}(1+2\tilde{b}^{g})\tilde{V}(\mathbf{x}),&\text{ if }\mathbf{x}\neq 0^{d},\\ 1+\left(2\tilde{b}^{g}\right)^{-1},&\text{ if }\mathbf{x}=0^{d}.\end{cases}\]

From the above equation and (19), we get

\[P\tilde{V}^{g}(\mathbf{x}) =\sum_{\mathbf{y}\neq 0^{d}}P_{\mathbf{x}\mathbf{y}}\tilde{V}^{g}(\mathbf{y})+P_{ \mathbf{x}\mathbf{0}^{d}}\tilde{V}^{g}(0^{d})\] \[=\sum_{\mathbf{y}\neq 0^{d}}P_{\mathbf{x}\mathbf{y}}(1+2\tilde{b}^{g})\tilde{V }(\mathbf{y})+P_{\mathbf{x}\mathbf{0}^{d}}\left(1+\frac{1}{2\tilde{b}^{g}}\right)\] \[\leq\left(1-\frac{1}{\tilde{b}^{g}}\right)(1+2\tilde{b}^{g}) \tilde{V}(\mathbf{x})+1+\frac{1}{2\tilde{b}^{g}}\] \[\leq\left(1-\frac{1}{\tilde{b}^{g}}\right)(1+2\tilde{b}^{g}) \tilde{V}(\mathbf{x})+\left(1+\frac{1}{2\tilde{b}^{g}}\right)\tilde{V}(\mathbf{x})\] \[=\left(1-\frac{1}{2\tilde{b}^{g}}\right)(1+2\tilde{b}^{g})\tilde{ V}(\mathbf{x}).\]

Thus,

\[P\tilde{V}^{g}(\mathbf{x})\leq\left(1-\frac{1}{2\tilde{b}^{g}}\right)\tilde{V}^{g}( \mathbf{x})+\left(1-\frac{1}{2\tilde{b}^{g}}\right)(1+2\tilde{b}^{g})\tilde{V}(0^ {d})\mathbb{I}_{0^{d}}(\mathbf{x}),\quad\mathbf{x}\in\mathcal{X}\,.\]

To find an upper bound for \(\mathbb{E}_{\mathbf{x}}[\kappa^{\tau_{0^{d}}}]\), we apply [43, Theorem 15.2.5], which is a generalization of Lemma 12. For any \(1\leq\kappa\leq\frac{2\tilde{b}^{g}}{2\tilde{b}^{g}-1}\), there exists \(\epsilon>0\) such that

\[\mathbb{E}_{\mathbf{x}}\Big{[}\sum_{i=0}^{\tau_{0^{d}}-1}\tilde{V}^{g}(\mathbf{X}_{i}) \kappa^{i}\Big{]}\leq\epsilon^{-1}\kappa^{-1}\tilde{V}^{g}(\mathbf{x}).\]

As \(\tilde{V}^{g}(\mathbf{y})\geq 1\) for all \(\mathbf{y}\in\mathcal{X}\), we have

\[\mathbb{E}_{\mathbf{x}}[\kappa^{\tau_{0^{d}}}] \leq\kappa\mathbb{E}_{\mathbf{x}}\Big{[}\sum_{i=0}^{\tau_{0^{d}}-1} \tilde{V}^{g}(\mathbf{X}_{i})\kappa^{i}\Big{]}\leq\epsilon^{-1}\tilde{V}^{g}(\bm {x})\] \[=\epsilon^{-1}\left(1+2\tilde{b}^{g}\right)\tilde{V}(\mathbf{x})\leq \tilde{b}^{g}\epsilon^{-1}\left(1+2\tilde{b}^{g}\right)V^{g}(\mathbf{x}),\]

and the claim holds for any \(\kappa\in[1,\frac{2\tilde{b}^{g}}{2\tilde{b}^{g}-1}]\) and \(c_{1}=\tilde{b}^{g}\epsilon^{-1}\left(1+2\tilde{b}^{g}\right)\). 

### Poisson equation

For an irreducible Markov process on the countably-infinite space \(\mathcal{X}\) with time-homogeneous transition kernel \(P\) and cost function \(\bar{c}(\cdot)\), a solution pair to the Poisson equation [41] is a scalar \(J\) and function \(v(\cdot):\mathcal{X}\mapsto\mathbb{R}\) such that \(J+v=\bar{c}+Pv\), where \(v(\mathbf{z})=0\) for some \(\mathbf{z}\in\mathcal{X}\). If the Markov process is also positive recurrent and \(\mathbb{E}_{\mathbf{x}}\left[\sum_{i=0}^{\tau_{\mathbf{y}}-1}|\bar{c}(\mathbf{X}(i))| \right]<\infty\), where \(\tau_{\mathbf{y}}\) is the first hitting time of some state \(\mathbf{y}\in\mathcal{X}\), then solution pair \((J,v)\) given as

\[J=\frac{\mathbb{E}_{\mathbf{y}}\left[\sum_{i=0}^{\tau_{\mathbf{y}}-1}|\bar{c}(\mathbf{X}(i ))|\right]}{\mathbb{E}_{\mathbf{y}}[\tau_{\mathbf{y}}]}\text{ and }v(\mathbf{x})= \mathbb{E}_{\mathbf{y}}\Big{[}\sum_{i=0}^{\tau_{\mathbf{x}}-1}|\bar{c}(\mathbf{X}(i))| \Big{]}-J\mathbb{E}_{\mathbf{x}}[\tau_{\mathbf{y}}],\quad\forall\mathbf{x}\in\mathcal{X},\]

is a solution to the Poisson equation \(J+v=\bar{c}+Pv\) with \(v(\mathbf{z})=0\)[41, Theorem 9.5].

**Lemma 5**.: _Consider Markov Decision Processes \(\left(\mathcal{X},\mathcal{A},c,P_{\theta}\right)\) governed by parameter \(\theta\in\Theta\) following the best-in-class policy \(\pi_{\theta}^{*}\). Then the pair \(\left(J\left(\theta\right),v^{\pi_{\theta}^{*}}\right)\) given as_

\[J(\theta):=\frac{\bar{C}^{\pi_{\theta}^{*}}(0^{d})}{\mathbb{E}_{0^{d}}^{\pi_{ \theta}^{*}}[\tau_{0^{d}}]}\text{ and }v^{\pi_{\theta}^{*}}(\mathbf{x})=\bar{C}^{\pi_{ \theta}^{*}}(\mathbf{x})-J(\theta)\mathbb{E}_{x}^{\pi_{\theta}^{*}}[\tau_{0^{d}}],\quad\forall\mathbf{x}\in\mathcal{X},\]

_is a solution to the Poisson equation \(v+J=c+P_{\theta}^{\pi_{\theta}^{*}}v\), where \(v^{\pi_{\theta}^{*}}(0^{d})=0\) and \(\bar{C}^{\pi_{\theta}^{*}}(\mathbf{x})=\mathbb{E}_{x}^{\pi_{\theta}^{*}}\Big{[} \sum_{i=0}^{\tau_{0^{d}}-1}c(\mathbf{X}(i),\pi_{\theta}^{*}(\mathbf{X}(i)))\Big{]}\)._

Proof.: From [41, Theorem 9.5], a solution pair to the Poisson equation exists if \(\mathbb{E}_{\mathbf{x}}^{\pi_{\theta}^{*}}[\tau_{0^{d}}]\) and \(\bar{C}^{\pi_{\theta}^{*}}(\mathbf{x})\) are finite for all \(\mathbf{x}\in\mathcal{X}\). The former follows from positive recurrence assumed in Assumption 3 and for the latter, from Assumptions 1 and 2,

\[\bar{C}^{\pi_{\theta}^{*}}(\mathbf{x}) =\mathbb{E}_{\mathbf{x}}^{\pi_{\theta}^{*}}\Big{[}\sum_{i=0}^{\tau_{0^ {d}}-1}c(\mathbf{X}\left(i\right),\pi_{\theta}^{*}(\mathbf{X}\left(i\right)))\Big{]} \leq\mathbb{E}_{\mathbf{x}}^{\pi_{\theta}^{*}}\Big{[}\sum_{i=0}^{\tau_{0^{d}}-1} \sum_{j=1}^{d}K\left(X_{j}\left(i\right)\right)^{\tau}\Big{]}\] \[\leq\mathbb{E}_{\mathbf{x}}^{\pi_{\theta}^{*}}\Big{[}\sum_{i=0}^{\tau _{0^{d}}-1}Kd\left(\|\mathbf{x}\|_{\infty}+hi\right)^{\tau}\Big{]}\leq\mathbb{E}_{ \mathbf{x}}^{\pi_{\theta}^{*}}\left[Kd\left(\|\mathbf{x}\|_{\infty}+h\tau_{0^{d}} \right)^{\tau}\tau_{0^{d}}\right],\]

which is finite from geometric ergodicity (Assumption 3) and the discussion following that. 

## Appendix B Proofs of regret analysis

In this section, we state the proofs related to regret analysis of Section 4. We first note a key property of Thompson sampling from [49], which states that for any episode \(k\), measurable function \(f\), and \(\mathcal{H}_{t_{k}}-\)measurable random variable \(Y\), we have

\[\mathbb{E}\left[f(\theta_{k},Y)\right]= \mathbb{E}\left[f(\mathbf{\theta}^{*},Y)\right], \tag{20}\]

where \(\mathcal{H}_{t}:=\sigma\left(\mathbf{X}\left(1\right),\ldots,\mathbf{X}\left(t\right), A\left(1\right),\ldots,A\left(t-1\right)\right)\) for all \(t\in\mathbb{N}\). We start with deriving upper bounds on the hitting times of state \(0^{d}\) using the ergodicity conditions of Assumptions 3 and 4. Previous works [25; 27; 29] have already established bounds on hitting times in geometrically and polynomially ergodic chains in terms of their corresponding Lyapunov function. However, our objective is to provide a precise characterization of all constants included in these bounds in terms of the constants of the drift equations 3 and 4. This characterization allows us to derive uniform bounds across the model class. In Appendix C.1, using the polynomial Lyapunov function provided in Assumption 4, we establish upper bounds on the \(i\)-th moment of hitting time of state \(0^{d}\) from any state \(\mathbf{x}\in\mathcal{X}\) and for \(1\leq i\leq r+1\). Importantly, the derived bound is polynomial in terms of any component of the state \(x_{i}\). Additionally, in Appendix C.2, we characterize the tail probabilities of the return time to state \(0^{d}\) starting from \(0^{d}\) in terms of the geometric Lyapunov function of Assumption 3. The derived tail bounds will be used in Lemma 6 to derive upper bounds for all moments of hitting times in the model class. These bounds, along with the skip-free behavior of the model, allow us to study the maximum state (with respect to \(\ell_{\infty}\)-norm) achieved up to time \(T\) in MDP \(\left(\mathcal{X},\mathcal{A},c,P_{\theta^{*}}\right)\) following Algorithm 1 as follows.

**Lemma 6**.: _For \(p\in\mathbb{N}\), the \(p\)-th moment of \(\max_{1\leq i\leq T}\tau_{0^{d}}^{(i)}\) and \(M_{\mathbf{\theta}^{*}}^{T}\), that is the maximum \(\ell_{\infty}\)-norm of the state vector achieved up until and including time \(T\) is \(O(\log^{p}T)\)._

In the proof of Lemma 6 given in Appendix B.1, we make use of geometric ergodicity of the chain and the fact that hitting times have geometric tails to find an upper bound for moments of \(M_{\mathbf{\theta}^{*}}^{T}\). Using this, we aim to bound the number of episodes started before or at \(T\), denoted by \(K_{T}\). We first find an upper bound for the number of episodes in which the second stopping criterion is met or there exists a state-action pair for which \(N_{t}(\mathbf{x},a)\) has increased more than twice. In the following lemma, we bound the number of such episodes, which we denote by \(K_{M}\), in terms of random variable \(M_{\mathbf{\theta}^{*}}^{T}\) and other problem-dependent constants. Proof of Lemma 7 is given in Appendix B.2.

**Lemma 7**.: _The number of episodes triggered by the second stopping criterion and started before or at time \(T\), denoted by \(K_{M}\), satisfies \(K_{M}\leq 2|\mathcal{A}|(M_{\mathbf{\theta}^{*}}^{T}+1)^{d}\log_{2}T\) a.s._We next bound the total number of episodes \(K_{T}\) by bounding the number of episodes triggered by the first stopping criterion, using the fact that in such episodes, \(\tilde{T}_{k}=\tilde{T}_{k-1}+1\). Moreover, to address the settling time of each episode \(k\), shown by \(E_{k}=T_{k}-\tilde{T}_{k}\), we use the geometric ergodicity property and Lemma 6. Finally, the proof of Lemma 8 is given in Appendix B.3.

**Lemma 8**.: _The number of episodes started by \(T\) satisfies \(K_{T}\leq 2\sqrt{|\mathcal{A}|(M^{T}_{\mathbf{\theta}^{*}}+1)^{d}T\log_{2}T}\) a.s._

From Lemma 8, the upper bound given in Lemma 6 for moments of \(M^{T}_{\mathbf{\theta}^{*}}\), and Cauchy-Schwarz inequality, it follows that the expected value of the number of episodes \(K_{T}\) is of the order \(\tilde{O}(\sqrt{h^{d}|\mathcal{A}|T})\). This term has a crucial role in determining the overall order of the total regret up to time \(T\). In the rest of this section, we present a detailed proof of the lemmas and other results used to prove Theorem 1.

**Remark 3**.: _The skip-free to the right property in Assumption 2 yields a polynomially-sized subset of the underlying state-space that can be explored as a function of \(T\). This polynomially-sized subset can be viewed as the effective finite-size of the system in the worst-case, and then, directly applying finite-state problem bounds [49] would result in a regret of order \(\tilde{O}(T^{d+0.5})\); since \(d\geq 1\), such a coarse bound is not helpful even for asserting asymptotic optimality! However, to achieve a regret of \(\tilde{O}(\sqrt{T})\), it is essential to carefully understand and characterize the distribution of \(M^{T}_{\mathbf{\theta}^{*}}\) and then its moments, as demonstrated in Lemma 6._

**Remark 4**.: _The derived regret bound can be extended to a larger class of MDPs which consist of transient states in addition to the single irreducible class. Specifically, for any \(\theta_{1},\theta_{2}\in\Theta\), the Markov process with transition kernel \(P^{\pi^{*}_{\theta_{2}}}_{\theta_{1}}\) obtained from the MDP \((\mathcal{X},\mathcal{A},c,P_{\theta_{1}})\) by following policy \(\pi^{*}_{\theta_{2}}\) has a single irreducible class \(I_{\theta_{1},\theta_{2}}\) and a set of transient states \(T_{\theta_{1},\theta_{2}}\). Furthermore, Assumptions 3 and 4 hold for the single irreducible class. The reasoning behind the proof remains true in this case using the following argument: each episode \(k\) starts at \(0^{d}\) which is in the irreducible set for the chosen policy \(\pi^{*}_{\theta_{k}}\), hence, throughout the episode the algorithm remains in the irreducible set that is positive recurrent and never visits any transient states. In other words, episodes starting and ending at \(0^{d}\) with a fixed episode dependent policy implies that reachable set of \(0^{d}\) is all that can be explored, which is positive recurrent by our assumptions. As a result, we can restrict our proof derivations to the subset that is reachable from \(0^{d}\) in each episode and follow the same analysis. The Lyapunov function based bounds apply to the positive recurrent states, and hence, restricting attention to states reachable from \(0^{d}\) within each episode, we can use these bounds for our assessment of regret using norms of the state. Thereafter, the coarse bounds on the norms of the state can be applied as carried out in our proof._

**Remark 5**.: _By problem-dependent parameters, we refer to the parameters that characterize the complexity or size of the model class \(\Theta\). These parameters are not just a function of the size of the state-space and diameter of the MDP (as mentioned in the literature on finite-size problems[5, 23, 49]), as stability needs to be accounted for in the countable state-space setting. The dependence is, thus, more complex and requires the inclusion of stability parameters, such as Lyapunov functions, petite sets, and ergodicity coefficients that are discussed in Assumptions 1-4._

**Remark 6**.: _In the subsequent sections, several equalities and inequalities in the proofs are between random variables and hold almost surely (a.s.). Throughout the remainder, we will omit the explicit mention of a.s., but any such statement should be interpreted in this context._

### Proof of Lemma 6

Proof.: Let \(\{\alpha_{i}\}_{i\geq 0}\) be the sequence of hitting times of state \(0^{d}\) starting from \(0^{d}\) (set \(\alpha_{0}=0\)). Define \(\tau^{(i)}_{0^{d}}\) as the length of the \(i\)-th recurrence time of state \(0^{d}\) for \(i\in\mathbb{N}\), i.e., \(\tau^{(i)}_{0^{d}}=\alpha_{i}-\alpha_{i-1}\). For simplicity, we take \(\tau_{0^{d}}=\tau^{(1)}_{0^{d}}\). Each such recurrence time is generated using policy \(\pi^{*}_{\theta_{i}}\) that is determined using the algorithm in operation in an MDP governed by parameter \(\mathbf{\theta}^{*}\). Furthermore, \(\{\tau^{(i)}_{0^{d}}\}_{i\in\mathbb{N}}\) are independent with length at least \(1\), but they need not be identically distributed. The time \(T\) can be in the middle of one of these recurrence times, hence the current recurrence interval count is \(N(T)=\inf\{n:\sum_{i=1}^{n}\tau^{(i)}_{0^{d}}\geq T\}\). Note that the lower bound of \(1\) on every \(\tau^{(i)}_{0^{d}}\) says that \(N(T)\leq T\) a.s. Further, from the skip-free to the right property, the most any component of state can increase in during recurrence time \(\tau^{(i)}_{0^{d}}\) is \(h\tau^{(i)}_{0^{d}}\). Hence, the most any component of the state (and also the \(\|\cdot\|_{\infty}\) norm of the state) can increase is given by \(h\max_{i=1,\ldots,T}\tau^{(i)}_{0^{d}}\) where the random variablesare independent with geometrically decaying tails with a worst case rate of

\[\sup_{\theta_{1},\theta_{2}\in\Theta}\tilde{\gamma}_{\theta_{1},\theta_{2}}^{g}=1- \Big{(}\sup_{\theta_{1},\theta_{2}\in\Theta}\tilde{b}_{\theta_{1},\theta_{2}}^{g }\Big{)}^{-1};\]

see Lemma 11. From Lemma 10, we have

\[\tilde{b}_{\theta_{1},\theta_{2}}^{g} =\frac{3b_{\theta_{1},\theta_{2}}^{g}+1}{1-\gamma_{\theta_{1}, \theta_{2}}^{g}}\Big{(}\mid\!\!C_{\theta_{1},\theta_{2}}^{g}\!\!\mid^{2}\max \Big{(}1,\max_{\mathbf{u}\in C_{\theta_{1},\theta_{2}}^{g}\setminus\{0^{d}\}} \mathbb{E}_{\mathbf{u}}^{\pi_{2}^{a}}[\tau_{0^{d}}]\Big{)}\Big{)}\] \[\leq\frac{3b_{\ast}^{g}+1}{1-\gamma_{\ast}^{g}}\Bigg{(}\mid\!\!C_ {\ast}^{g}\!\!\mid^{2}\max\Big{(}1,\sup_{\mathbf{u}\in C_{\ast}^{g}\setminus\{0^{d} \}}\phi_{\theta_{1},\theta_{2}}^{p}(1)\Big{(}V_{\theta_{1},\theta_{2}}^{p}(\bm {u})+b_{\theta_{1},\theta_{2}}^{p}\alpha_{C_{\theta_{1},\theta_{2}}^{p}}\Big{)} \Big{)}\Bigg{)}\] \[\leq\frac{3b_{\ast}^{g}+1}{1-\gamma_{\ast}^{g}}\Bigg{(}\mid\!\!C_ {\ast}^{g}\!\!\mid^{2}\max\Big{(}1,\sup_{\mathbf{u}\in C_{\ast}^{g}\setminus\{0^{d} \}}\frac{1}{\beta_{\theta_{1},\theta_{2}}^{p}}\Big{(}s_{\theta}^{p}\|\mathbf{u}\|_ {\infty}^{r_{\infty}^{p},\theta_{2}}^{r}+\frac{b_{\theta_{1},\theta_{2}}^{p}}{ \min_{\mathbf{y}\in C_{\theta_{1},\theta_{2}}^{p}}K_{\theta_{1},\theta_{2}}(\mathbf{y} )}\Big{)}\Big{)}\Bigg{)}\] \[\leq\frac{3b_{\ast}^{g}+1}{1-\gamma_{\ast}^{g}}\Bigg{(}\mid\!\!C_ {\ast}^{g}\!\!\mid^{2}\max\Big{(}1,\sup_{\mathbf{u}\in C_{\ast}^{g}\setminus\{0^{d} \}}\frac{1}{\beta_{\ast}^{p}}\Big{(}s_{\theta}^{p}\|\mathbf{u}\|_{\infty}^{r_{ \infty}^{p}}+\frac{b_{\ast}^{p}}{K_{\ast}}\Big{)}\Big{)}\Bigg{)} \tag{21}\]

and we define \(\tilde{\gamma}_{\ast}^{g}:=1-(\tilde{b}_{\ast}^{g})^{-1}\). From the definition of \(b_{\theta_{1},\theta_{2}}^{g}\) in Assumption 3, \(b_{\theta_{1},\theta_{2}}^{g}\) is greater than or equal to \(2\). Thus, \(\tilde{b}_{\theta_{1},\theta_{2}}^{g}\geq 7\) and we have

\[\sup_{\theta_{1},\theta_{2}\in\Theta}c_{\theta_{1},\theta_{2}}^{g}=\sup_{\theta _{1},\theta_{2}\in\Theta}\frac{b_{\theta_{1},\theta_{2}}^{g}\left(\tilde{b}_{ \theta_{1},\theta_{2}}^{g}\right)^{2}}{\tilde{b}_{\theta_{1},\theta_{2}}^{g}- 1}\leq\frac{b_{\ast}^{g}\left(\tilde{b}_{\ast}^{g}\right)^{2}}{6}:=c_{\ast}^{g},\]

and as a result of Lemma 11,

\[\mathbb{P}_{0^{d}}(\tau_{0^{d}}^{(i)}>n)\leq c_{\ast}^{g}\left(\gamma_{\ast}^{ g}\right)^{n},\qquad 1\leq i\leq T. \tag{22}\]

We upper bound \(\mathbb{E}\left[M_{\mathbf{\theta}^{\star}}^{T}\right]\) using the independence of \(\{\tau_{0^{d}}^{(i)}\}_{i\in\mathbb{N}}\) and the above equation,

\[\mathbb{E}\left[M_{\mathbf{\theta}^{\star}}^{T}\right] \leq h\,\mathbb{E}[\max_{1\leq i\leq T}\tau_{0^{d}}^{(i)}]=h\sum_ {n=0}^{\infty}\mathbb{P}(\max_{1\leq i\leq T}\tau_{0^{d}}^{(i)}>n)\] \[=h\sum_{n=0}^{\infty}\left(1-\mathbb{P}(\max_{1\leq i\leq T}\tau_ {0^{d}}^{(i)}\leq n)\right)=h\sum_{n=0}^{\infty}\left(1-\prod_{i=1}^{T}\mathbb{ P}\left(\tau_{0^{d}}^{(i)}\leq n\right)\right)\] \[\leq hn_{0}+h\sum_{n=n_{0}}^{\infty}1-\left(1-c_{\ast}^{g}\left( \gamma_{\ast}^{g}\right)^{n_{0}}\left(\gamma_{\ast}^{g}\right)^{n-n_{0}}\right) ^{T}\] \[\leq h(n_{0}+1)+h\sum_{n=n_{0}+1}^{\infty}1-\left(1-\left(\gamma_{ \ast}^{g}\right)^{n-n_{0}}\right)^{T},\]

where \(n_{0}\) is the smallest \(n\geq 0\) such that \(c_{\ast}^{g}\left(\gamma_{\ast}^{g}\right)^{n}<1\). By Reimann sum approximation, we get

\[\mathbb{E}\left[M_{\mathbf{\theta}^{\star}}^{T}\right] \leq h(n_{0}+1)+h\sum_{n=1}^{\infty}1-\left(1-\left(\gamma_{\ast}^ {g}\right)^{n}\right)^{T}\] \[<h(n_{0}+1)+h\int_{0}^{\infty}1-\left(1-\left(\gamma_{\ast}^{g} \right)^{u}\right)^{T}\,du\] \[=h(n_{0}+1)+\frac{h}{\log\gamma_{\ast}^{g}}\int_{0}^{1}\frac{1-u^{T }}{1-u}\,du\] \[\leq h(n_{0}+1)+\frac{h}{\log\gamma_{\ast}^{g}}\left(\log T+1\right),\]where the last inequality follows from \(\sum_{n=1}^{T}n^{-1}\leq\log T+1\) and thus \(\mathbb{E}\left[M_{\mathbf{\theta}^{*}}^{T}\right]\) is \(O(h\log T)\). We now extend the result to moments of order greater than one. From (22), for \(1\leq i\leq T\),

\[\mathbb{P}_{0^{d}}(\tau_{0^{d}}^{(i)}>n)\leq c_{*}^{g}\left(\gamma_{*}^{g} \right)^{n}=c_{*}^{g}\left(\gamma_{*}^{g}\right)^{n_{0}}\left(\gamma_{*}^{g} \right)^{n-n_{0}}<\left(\gamma_{*}^{g}\right)^{n-n_{0}}.\]

For \(n\geq n_{0}\), let \(t=n-n_{0}\geq 0\) and \(Y_{i}=\max(\tau_{0^{d}}^{(i)}-n_{0},0)\) to get

\[\mathbb{P}_{0^{d}}(Y_{i}>t)=\mathbb{P}_{0^{d}}(\tau_{0^{d}}^{(i)}-n_{0}>t)< \left(\gamma_{*}^{g}\right)^{t},\]

which means random variables \(\{Y_{i}\}_{i=1}^{T}\) are stochastically dominated by independent and identically distributed geometric random variables with parameter \(1-\gamma_{*}^{g}\). Furthermore, [56] argues that the \(p\)-th moment of the maximum of \(T\) independent and identically distributed geometric random variables is \(O(\log^{p}T)\). Thus, the \(p\)-th moment of \(\max_{1\leq i\leq T}Y_{i}\) is \(O(\log^{p}T)\) and

\[\max_{1\leq i\leq T}Y_{i} =\max(\tau_{0^{d}}^{(1)}-n_{0},\ldots,\tau_{0^{d}}^{(T)}-n_{0},0) =\max(\tau_{0^{d}}^{(1)},\ldots,\tau_{0^{d}}^{(T)},n_{0})-n_{0}\] \[\geq\max(\tau_{0^{d}}^{(1)},\ldots,\tau_{0^{d}}^{(T)})-n_{0}\geq h ^{-1}M_{\mathbf{\theta}^{*}}^{T}-n_{0},\]

which gives

\[\mathbb{E}\left[\left(M_{\mathbf{\theta}^{*}}^{T}\right)^{p}\right]\leq h^{p} \,\mathbb{E}\left[\left(\max_{1\leq i\leq T}\tau_{0^{d}}^{(i)}\right)^{p} \right]\leq h^{p}\,\mathbb{E}\left[\left(\max_{1\leq i\leq T}Y_{i}+n_{0}\right) ^{p}\right].\]

Since the right-hand side of the above equation is \(O(h^{p}\log^{p}T)\), the claim is proved. 

### Proof of Lemma 7

Proof.: Let \(K_{M}(\mathbf{x},a)\) be the number of episodes \(k\) such that \(1\leq k\leq K_{T}\) and in which the number of visits to the state-action pair \((\mathbf{x},a)\) is increased more than twice at episode \(k\), or

\[K_{M}(\mathbf{x},a)=|\{k\leq K_{T}:N_{\tilde{t}_{k+1}}(\mathbf{x},a)>2N_{t_{k}}(\mathbf{x},a)\}|.\]

As for every episode in the above set the number of visits to \((x,a)\) doubles,

\[K_{M}(\mathbf{x},a)\leq\log_{2}(N_{T+1}(\mathbf{x},a))+1,\]

and we can upper bound \(K_{M}\) as follows

\[K_{M} =\sum_{\mathbf{x}\in\mathcal{X},a\in\mathcal{A}}K_{M}(\mathbf{x},a)=\sum_ {\begin{subarray}{c}\|\mathbf{x}\|_{\infty}\leq M_{\mathbf{\theta}^{*}}^{T}\\ a\in\mathcal{A}\end{subarray}}K_{M}(\mathbf{x},a)\] \[\leq\sum_{\begin{subarray}{c}\|\mathbf{x}\|_{\infty}\leq M_{\mathbf{ \theta}^{*}}^{T}\\ a\in\mathcal{A}\end{subarray}}(1+\log_{2}N_{T+1}(\mathbf{x},a))\leq|\,\mathcal{A} \,|\left(M_{\mathbf{\theta}^{*}}^{T}+1\right)^{d}(1+\log_{2}T).\]

This completes the proof. 

### Proof of Lemma 8

Proof.: We define macro episodes with start times \(t_{n_{k}}\), \(k=1,2,\ldots,K_{M}+1\) where \(t_{n_{1}}=t_{1}\), \(t_{n_{K_{M}+1}}=T+1\) (which is equivalent to \(n_{K_{M}+1}=K_{T}+1\)), and for \(1<k<K_{M}+1\)

\[t_{n_{k+1}}=\min\{t_{j}>t_{n_{k}}:\quad N_{t_{j}}(\mathbf{x},a)>2N_{t_{j-1}}(\mathbf{x},a)\text{ for some }(\mathbf{x},a)\},\]

which are episodes wherein the second stopping criterion is triggered. Any episode (except for the last episode) in a macro episode must be triggered by the first stopping criterion; equivalently, \(\tilde{T}_{j}=\tilde{T}_{j-1}+1\) for all \(j=n_{k},n_{k}+1,\ldots,n_{k+1}-2\). For \(1\leq k\leq K_{M}\), let \(T_{k}^{M}=\sum_{j=n_{k}}^{n_{k+1}-1}T_{j}\) be the length of the \(k\)-th macro episode. We have

\[T_{k}^{M}=\sum_{j=n_{k}}^{n_{k+1}-1}T_{j}\geq\sum_{j=n_{k}}^{n_{k+1}-1}\tilde{ T}_{j}\geq 1+\sum_{j=n_{k}}^{n_{k+1}-2}(j-n_{k}+2)=0.5(n_{k+1}-n_{k})(n_{k+1}-n_{k} +1).\]Consequently, \(n_{k+1}-n_{k}\leq\sqrt{2T_{k}^{M}}\) for all \(1\leq k\leq K_{M}\). From this, we obtain

\[K_{T}= n_{K_{M}+1}-1=\sum_{k=1}^{K_{M}}(n_{k+1}-n_{k})\leq\sum_{k=1}^{K_{M}} \sqrt{2T_{k}^{M}}.\]

Using the above equation and the fact that \(\sum_{k=1}^{K_{M}}T_{k}^{M}=T\) we get

\[K_{T}\leq\sum_{k=1}^{K_{M}}\sqrt{2T_{k}^{M}}\leq \sqrt{K_{M}\sum_{k=1}^{K_{M}}2T_{k}^{M}}=\sqrt{2K_{M}T}.\]

Finally, from Lemma 7 we get

\[K_{T}\leq\sqrt{2K_{M}T}\leq 2\sqrt{\left|\,\mathcal{A}\,\right|\left(M_{ \boldsymbol{\theta}^{*}}^{T}+1\right)^{d}T\log_{2}T}.\]

This completes the proof. 

### Proof of Lemma 1

Proof.: Let \(E_{k}=T_{k}-\tilde{T}_{k}\geq 0\) be the settling time needed to return to state \(0^{d}\) after a stopping criterion is realized in episode \(k\). We have

\[R_{0} =\mathbb{E}\left[\sum_{k=1}^{K_{T}}T_{k}J(\theta_{k})\right]-T\, \mathbb{E}\left[J(\boldsymbol{\theta}^{*})\right]\] \[=\mathbb{E}\left[\sum_{k=1}^{K_{T}}\tilde{T}_{k}J(\theta_{k}) \right]+\mathbb{E}\left[\sum_{k=1}^{K_{T}}E_{k}J(\theta_{k})\right]-T\, \mathbb{E}\left[J(\boldsymbol{\theta}^{*})\right]. \tag{23}\]

We first simplify the first term in the above summation. From the monotone convergence theorem,

\[\mathbb{E}\left[\sum_{k=1}^{K_{T}}\tilde{T}_{k}J(\theta_{k})\right]=\sum_{k=1} ^{\infty}\mathbb{E}\left[\mathbb{I}_{\{t_{k}\leq T\}}\tilde{T}_{k}J(\theta_{k} )\right].\]

Note that the first stopping criterion of Algorithm 1 ensures that \(\tilde{T}_{k}\leq\tilde{T}_{k-1}+1\) at all episodes \(k\geq 1\). Hence

\[\mathbb{E}\left[\mathbb{I}_{\{t_{k}\leq T\}}\tilde{T}_{k}J(\theta_{k})\right] \leq\mathbb{E}\left[\mathbb{I}_{\{t_{k}\leq T\}}(\tilde{T}_{k-1}+1)J(\theta_{ k})\right].\]

Since \(\mathbb{I}_{\{t_{k}\leq T\}}(\tilde{T}_{k-1}+1)\) is measurable with respect to \(\mathcal{H}_{t_{k}}\), by (20) we get

\[\mathbb{E}\left[\mathbb{I}_{\{t_{k}\leq T\}}(\tilde{T}_{k-1}+1)J(\theta_{k}) \right]= \mathbb{E}\left[\mathbb{I}_{\{t_{k}\leq T\}}(\tilde{T}_{k-1}+1)J( \boldsymbol{\theta}^{*})\right].\]

Therefore,

\[\mathbb{E}\left[\sum_{k=1}^{K_{T}}\tilde{T}_{k}J(\theta_{k})\right]\leq\sum_{k =1}^{\infty}\mathbb{E}\left[\mathbb{I}_{\{t_{k}\leq T\}}(\tilde{T}_{k-1}+1)J( \boldsymbol{\theta}^{*})\right]=\mathbb{E}\left[\sum_{k=1}^{K_{T}}(\tilde{T}_ {k-1}+1)J(\boldsymbol{\theta}^{*})\right].\]

Thus,

\[\mathbb{E}\left[\sum_{k=1}^{K_{T}}\tilde{T}_{k}J(\theta_{k})\right] -T\,\mathbb{E}\left[J(\boldsymbol{\theta}^{*})\right] \leq\mathbb{E}\left[J(\boldsymbol{\theta}^{*})\sum_{k=1}^{K_{T} }(\tilde{T}_{k-1}+1)\right]-\mathbb{E}\left[J(\boldsymbol{\theta}^{*})\sum_{k =1}^{K_{T}}T_{k}\right]\] \[=\mathbb{E}\left[J(\boldsymbol{\theta}^{*})\Big{(}K_{T}+1-T_{K_{ T}}-\sum_{k=1}^{K_{T}-1}E_{k}\Big{)}\right]\] \[\leq\mathbb{E}\left[J(\boldsymbol{\theta}^{*})K_{T}\right]. \tag{24}\]For the second term in (23), from Assumption 5

\[\mathbb{E}\left[\sum_{k=1}^{K_{T}}E_{k}J(\theta_{k})\right]\leq J^{*}\,\mathbb{E} \left[\sum_{k=1}^{K_{T}}E_{k}\right]\leq J^{*}\,\mathbb{E}[K_{T}\max_{1\leq i \leq T}\tau_{0^{d}}^{(i)}]. \tag{25}\]

Substitutinh (24) and (25) in (23), we get

\[R_{0} \leq\mathbb{E}\left[K_{T}J(\boldsymbol{\theta}^{*})\right]+J^{*} \,\mathbb{E}[K_{T}\max_{1\leq i\leq T}\tau_{0^{d}}^{(i)}]\] \[\leq J^{*}\,\mathbb{E}\left[K_{T}\right]+J^{*}\,\mathbb{E}[K_{T} \max_{1\leq i\leq T}\tau_{0^{d}}^{(i)}]\] \[=J^{*}\,\mathbb{E}\left[K_{T}\Big{(}\max_{1\leq i\leq T}\tau_{0^{ d}}^{(i)}+1\Big{)}\right].\]

### Proof of Lemma 2

Proof.: We note that the state of the MDP is equal to \(0^{d}\) at the beginning of all episodes and the relative value function \(v(\boldsymbol{x};\theta)\) is equal to \(0\) at \(\boldsymbol{x}=0^{d}\) for all \(\theta\). Thus,

\[R_{1} =\mathbb{E}\,\Big{[}\sum_{k=1}^{K_{T}}\sum_{t=t_{k}-1}^{t_{k+1}-1 }\Big{[}v\left(\boldsymbol{X}\left(t\right);\theta_{k}\right)-v\left( \boldsymbol{X}\left(t+1\right);\theta_{k}\right)\Big{]}\Big{]}\] \[=\mathbb{E}\,\Big{[}\sum_{k=1}^{K_{T}}\Big{[}v\left(\boldsymbol{X} \left(t_{k}\right);\theta_{k}\right)-v\left(\boldsymbol{X}\left(t_{k+1}\right) ;\theta_{k}\right)\Big{]}\Big{]}\] \[=\mathbb{E}\,\Big{[}\sum_{k=1}^{K_{T}-1}\Big{[}v\left(0^{d};\theta _{k}\right)-v\left(0^{d};\theta_{k}\right)\Big{]}+v\left(0^{d};\theta_{K_{T}} \right)-v\left(\boldsymbol{X}(T+1);\theta_{K_{T}}\right)\Big{]}\] \[=-\mathbb{E}[v\left(\boldsymbol{X}(T+1);\theta_{K_{T}}\right)].\]

From the lower bound derived for the relative value function in (14),

\[-v(\boldsymbol{x};\theta)\leq J^{*}\mathbb{E}_{\boldsymbol{x}}^{\pi_{ \boldsymbol{x}}^{\pi_{\boldsymbol{x}}^{\pi_{\boldsymbol{x}}^{\pi_{\boldsymbol {x}}^{\pi_{\boldsymbol{x}}^{\pi_{\boldsymbol{x}}^{\pi_{\boldsymbol{x}}^{\pi_{ \boldsymbol{x}}^{\pi_{\boldsymbol{x}}^{\pi_{\boldsymbol{x}}^{\pi_{\boldsymbol{x} }^{\pi_{\boldsymbol{x}}^{\pi_{\boldsymbol{x}}^{\pi_{\boldsymbol{x}}^{\pi_{ \boldsymbol{x}}^{\pi_{\boldsymbol{x}}^{\pi_{\boldsymbol{x}}^{\pi_{\boldsymbol{x}}^{ \pi}_{\boldsymbol{x}}^{\pi_{\boldsymbol{x}}^{\pi}_{\boldsymbol{x}}^{\pi}_{ \boldsymbol{x}}^{\pi}_{\boldsymbol{x}}^{\pi}_{\boldsymbol{x}}^{\pi}_{\boldsymbol{x}}^{ \pi}_{\boldsymbol{x}}^{\pi}_{\boldsymbol{x}}^{\pi}_{\boldsymbol{x}}^{\pi}_{ \boldsymbol{x}}^{\pi}_{\boldsymbol{x}}^{\pi}_{\boldsymbol{x}}^{\pi}_{\boldsymbol{x}}^{ \pi}_{\boldsymbol{x}}^{\pi}_{\boldsymbol{x}}^{\pi}_{\boldsymbol{x}}^{\pi}_{ \boldsymbol{x}}^{\pi}_{\boldsymbol{x}}^{\pi}_{\boldsymbol{x}}^{\pi}_{\boldsymbol{x}}^{ \pi}_{\boldsymbol{x}}^{\pi}_{\boldsymbol{x}}^{\pi}_{\boldsymbol{x}}^{\pi}_{ \boldsymbol{x}}^{\pi}_{\boldsymbol{x}}^{\pi}_{\boldsymbol{x}}^{\pi}_{\boldsymbol{x}}^{ \pi}_{\boldsymbol{x}}^{\pi}}\Big{)},\]

where the second inequality follows from (21) in the proof of Lemma 6. We also note that \(\|\boldsymbol{X}(T+1)\|_{\infty}\leq M_{\boldsymbol{\theta}^{*}}^{T}+h\). Thus,

\[R_{1}=-\,\mathbb{E}[v\left(\boldsymbol{X}(T+1);\theta_{K_{T}}\right)]\leq \mathbb{E}\,\Big{[}\frac{J^{*}}{\beta_{*}^{p}}\Big{(}s_{*}^{p}(M_{ \boldsymbol{\theta}^{*}}^{T}+h)^{r_{*}^{p}}+\frac{b_{*}^{p}}{K_{*}}\Big{)} \Big{]}.\]

From the inequality \((a+b)^{r}\leq 2^{r}(a^{r}+b^{r})\), we have

\[R_{1}\leq\frac{J^{*}2^{r_{*}^{p}}s_{*}^{p}}{\beta_{*}^{p}}\,\mathbb{E}\,\Big{[} \left(M_{\boldsymbol{\theta}^{*}}^{T}\right)^{r_{*}^{p}}\Big{]}+\frac{J^{*}}{ \beta_{*}^{p}}\left(s_{*}^{p}\left(2h\right)^{r_{*}^{p}}+\frac{b_{*}^{p}}{K_{* }}\right).\]

### Proof of Lemma 3

Proof.: Let \(\boldsymbol{Z}\left(t\right)=\left(\boldsymbol{X}\left(t\right),\pi_{\theta_{k} }^{*}\left(\boldsymbol{X}\left(t\right)\right)\right)\) be the state-action pair at \(t_{k}\leq t<t_{k+1}\). \(R_{2}\) can be upper bounded as

\[R_{2} =\mathbb{E}\,\Big{[}\sum_{k=1}^{K_{T}}\sum_{t=t_{k}}^{t_{k+1}-1} \Big{[}v\left(\boldsymbol{X}\left(t+1\right);\theta_{k}\right)-\sum_{ \boldsymbol{y}\in\mathcal{X}}P_{\theta_{k}}\left(\boldsymbol{y}\,\Big{|} \,\boldsymbol{X}\left(t\right),\pi_{\theta_{k}}^{*}\left(\boldsymbol{X}\left( t\right)\right)\right)v\left(\boldsymbol{y};\theta_{k}\right)\Big{]}\Big{]}\] \[\leq\mathbb{E}\,\Big{[}\sum_{k=1}^{K_{T}}\sum_{t=t_{k}}^{t_{k+1}-1 }\Big{[}\sum_{\boldsymbol{y}\in\mathcal{X}}\left|P_{\boldsymbol{\theta}^{*}}( \boldsymbol{y}|\boldsymbol{Z}\left(t\right))-P_{\theta_{k}}(\boldsymbol{y}| \boldsymbol{Z}\left(t\right))\right|v(\boldsymbol{y};\theta_{k})\big{|}\,\Big{]} \Big{]}\] \[\leq\sum_{t=1}^{T}\mathbb{E}\,\Big{[}\Big{(}\max_{\begin{subarray}{ c}1\leq k\leq K_{T}\\ \|\boldsymbol{x}\|_{\infty}\leq M_{\boldsymbol{\theta}^{*}}^{T}\end{subarray}}\left|v( \boldsymbol{x};\theta_{k})\right|\Big{)}\|P_{\boldsymbol{\theta}^{*}}(\cdot| \boldsymbol{Z}\left(t\right))-P_{\theta_{k}}(\cdot|\boldsymbol{Z}\left(t\right)) \|_{1}\Big{]}. \tag{26}\]We have

\[\|P_{\mathbf{\theta}^{*}}(\cdot|\mathbf{Z}(t))-P_{\theta_{k}}(\cdot|\mathbf{Z}(t))\|_{1}\leq\|P _{\mathbf{\theta}^{*}}(\cdot|\mathbf{Z}(t))-P_{\hat{\theta}_{k}}(\cdot|\mathbf{Z}(t))\|_{1}+ \|P_{\theta_{k}}(\cdot|\mathbf{Z}(t))-P_{\hat{\theta}_{k}}(\cdot|\mathbf{Z}(t))\|_{1},\]

where \(P_{\hat{\theta}_{k}}(\mathbf{y}|\mathbf{Z}\left(t\right))\) is the empirical transition probability defined as

\[P_{\hat{\theta}_{k}}(\mathbf{y}|\mathbf{Z}\left(t\right))=\frac{N_{t_{k}}\left(\mathbf{Z} \left(t\right),\mathbf{y}\right)}{\max\left(1,N_{t_{k}}\left(\mathbf{Z}\left(t\right) \right)\right)},\]

and for any tuple \((\mathbf{x},a,\mathbf{y})\), we define \(N_{1}(\mathbf{x},a,\mathbf{y})=0\) and for \(t>1\),

\[N_{t}(\mathbf{x},a,\mathbf{y})=|\{t_{k}\leq i<\tilde{t}_{k+1}\leq t\,\,\text{for some}\, \,k\geq 1:\left(\mathbf{X}\left(i\right),A\left(i\right),\mathbf{X}\left(i+1\right) \right)=(\mathbf{x},a,\mathbf{y})\}|.\]

Thus, from (26) and defining random variable \(v_{M}=\max\limits_{\begin{subarray}{c}1\leq k\leq K_{T}\\ \|\mathbf{x}\|_{\infty}\preceq M_{\mathbf{\theta}^{*}}^{2}\end{subarray}}|v(\mathbf{x}; \theta_{k})|\),

\[R_{2}\leq\sum_{t=1}^{T}\mathbb{E}\left[v_{M}\|P_{\mathbf{\theta}^{*}}(\cdot|\mathbf{Z} \left(t\right))-P_{\hat{\theta}_{k}}(\cdot|\mathbf{Z}\left(t\right))\|_{1}\right]+ \sum_{t=1}^{T}\mathbb{E}\left[v_{M}\|P_{\theta_{k}}(\cdot|\mathbf{Z}\left(t\right) )-P_{\hat{\theta}_{k}}(\cdot|\mathbf{Z}\left(t\right))\|_{1}\right]. \tag{27}\]

We define set \(B_{k}\) as the set of parameters \(\theta\) for which the transition kernel \(P_{\theta}(\cdot|\mathbf{z})\) is close to the empirical transition kernel \(P_{\hat{\theta}_{k}}(\cdot|\mathbf{z})\) at episode \(k\) for every state-action pair \(\mathbf{z}=(\mathbf{x},a)\in\mathcal{X}\times\mathcal{A}\), or

\[B_{k}=\left\{\theta:\|P_{0}(\cdot|\mathbf{z})-P_{\hat{\theta}_{k}}(\cdot|\mathbf{z})\| _{1}\leq\beta_{k}(\mathbf{z}),\,\,\mathbf{z}=(\mathbf{x},a)\in\{0,1,\cdots,hT\}^{d}\times \mathcal{A}\right\},\]

where \(\beta_{k}(\mathbf{z})=\sqrt{\frac{14\prod_{i=1}^{d}(x_{i}+h)}{\max\left(1,N_{t_{k}} (\mathbf{z})\right)}\log\left(\frac{2|\mathcal{A}|T}{\delta}\right)}\) for \(\mathbf{x}=(x_{1},\ldots,x_{d})\) and some \(0<\tilde{\delta}<1\), which will be determined later. We simplify the \(\ell_{1}\)-difference of the real and empirical transition kernels as follows

\[\|P_{\mathbf{\theta}^{*}}(\cdot|\mathbf{Z}\left(t\right))-P_{\hat{\theta}_ {k}}(\cdot|\mathbf{Z}\left(t\right))\|_{1}\] \[=\mathbb{I}_{\{\mathbf{\theta}^{*}\notin B_{k}\}}\|P_{\mathbf{\theta}}( \cdot|\mathbf{Z}\left(t\right))-P_{\hat{\theta}_{k}}(\cdot|\mathbf{Z}\left(t\right))\|_ {1}+\mathbb{I}_{\{\mathbf{\theta}^{*}\in B_{k}\}}\|P_{\mathbf{\theta}_{*}}(\cdot|\mathbf{Z }\left(t\right))-P_{\hat{\theta}_{k}}(\cdot|\mathbf{Z}\left(t\right))\|_{1}\] \[\leq 2\mathbb{I}_{\{\mathbf{\theta}^{*}\notin B_{k}\}}+\beta_{k}\left( \mathbf{Z}\left(t\right)\right).\]

Similarly, we have

\[\|P_{\theta_{k}}(\cdot|\mathbf{Z}\left(t\right))-P_{\hat{\theta}_{k}}(\cdot|\mathbf{Z} \left(t\right))\|_{1}\leq 2\mathbb{I}_{\{\theta_{k}\notin B_{k}\}}+\beta_{k}\left( \mathbf{Z}\left(t\right)\right).\]

Substituting in (27), we get

\[R_{2}\leq\mathbb{E}\Big{[}\sum_{k=1}^{K_{T}}\sum_{t=t_{k}}^{t_{k+1}-1}2v_{M} \left[\mathbb{I}_{\{\mathbf{\theta}^{*}\notin B_{k}\}}+\mathbb{I}_{\{\theta_{k} \notin B_{k}\}}\right]\Big{]}+\mathbb{E}\Big{[}\sum_{k=1}^{K_{T}}\sum_{t=t_{k }}^{t_{k+1}-1}2v_{M}\beta_{k}\left(\mathbf{Z}\left(t\right)\right)\Big{]}. \tag{28}\]

We first find an upper bound for \(v_{M}=\max\limits_{\begin{subarray}{c}1\leq k\leq K_{T}\\ \|\mathbf{x}\|_{\infty}\preceq M_{\mathbf{\theta}^{*}}^{2}\end{subarray}}|v(\mathbf{x}; \theta_{k})|\) using the bounds derived in (13) and (14). From (13),

\[v(\mathbf{x};\theta_{k}) \leq\mathbb{E}_{\mathbf{x}}^{\pi_{\theta_{k}}^{*}}\left[Kd\left(\|\mathbf{ x}\|_{\infty}+h\tau_{04}\right)^{r}\tau_{04}\right]\] \[\leq\mathbb{E}_{\mathbf{x}}^{\pi_{\theta_{k}}^{*}}\left[2^{r}Kd\left( \|\mathbf{x}\|_{\infty}^{r}+h^{r}(\tau_{04})^{r}\right)\tau_{04}\right]\] \[=Kd(2\|\mathbf{x}\|_{\infty})^{r}\mathbb{E}_{\mathbf{x}}^{\pi_{\theta_{k} }^{*}}\left[\tau_{04}\right]+Kd(2h)^{r}\mathbb{E}_{\mathbf{x}}^{\pi_{\theta_{k}}^{* }}\left[(\tau_{04})^{r+1}\right]\] \[\leq Kd2^{r}\left(\|\mathbf{x}\|_{\infty}^{r}+h^{r}\right)\mathbb{E} _{\mathbf{x}}^{\pi_{\theta_{k}}^{*}}\left[(\tau_{04})^{r+1}\right]\] \[\leq Kd(r+1)2^{r}\left(\|\mathbf{x}\|_{\infty}^{r}+h^{r}\right)\phi_{ \theta_{k}}^{p}(r+1)\left(V_{\theta_{k}}^{p}(\mathbf{x})+b_{\theta_{k}}^{p}\alpha _{C_{\theta_{k}^{p}}}\right)\] \[\leq Kd(r+1)2^{r}\left(\|\mathbf{x}\|_{\infty}^{r}+h^{r}\right)\phi_{ \theta_{k}}^{p}(r+1)\left(g_{*}^{p}\|\mathbf{x}\|_{\infty}^{r}+b_{*}^{p}(K_{*})^{-1 }\right), \tag{29}\]

where the second line follows from the inequality \((a+b)^{r}\leq 2^{r}(a^{r}+b^{r})\), the fifth line from Lemma 10, and the last line from Assumption 4 and (21). We further have

\[\phi_{\theta_{1},\theta_{2}}^{p}(r+1) =\prod_{j=1}^{r+1}\frac{1}{\beta_{\theta_{1},\theta_{2}}^{n_{j}}} \left(2^{j-1}+(j-1)\,\alpha_{C_{\theta_{1},\theta_{2}}^{p}}b_{\theta_{1}, \theta_{2}}^{\eta_{j}}\right)\] \[\leq\prod_{j=1}^{r+1}\frac{r+1}{\min(1,\beta_{*}^{p})}\left(2^{j -1}+(j-1)\,(K_{*})^{-1}b_{\theta_{1},\theta_{2}}^{\eta_{j}}\right),\]where using the definition of \(b_{\theta_{1},\theta_{2}}^{\eta_{j}}\) in (38),

\[b_{\theta_{1},\theta_{2}}^{\eta_{j}}=\left(b_{\theta_{1},\theta_{2}}^{\rho}\right) ^{\eta_{j}}+\eta_{j}\tilde{\beta}_{\theta_{1},\theta_{2}}^{p}\max\left(1,\left( \tilde{\beta}_{\theta_{1},\theta_{2}}^{p}\right)^{(\alpha_{\theta_{1},\theta_{2 }}^{p}+\eta_{j}-1)/(1-\alpha_{\theta_{1},\theta_{2}}^{p}}\right)\leq 1+b_{*}^{p}+ \beta_{*}^{p}.\]

We also define

\[\phi_{*}^{p}(r+1):=\prod_{j=1}^{r+1}\frac{r+1}{\min(1,\beta_{*}^{p})}\left(2^{j -1}+(j-1)\left(K_{*}\right)^{-1}(1+b_{*}^{p}+\beta_{*}^{p})\right).\]

We next find a lower bound for \(v(\mathbf{x};\theta_{k})\) using (14) as follows:

\[v(\mathbf{x};\theta_{k})\geq-J^{*}\mathbb{E}_{\mathbf{x}}^{\pi_{\theta_{k}}^{*}}\left[ \tau_{04}\right]\geq-\frac{J^{*}}{\beta_{*}^{p}}\Big{(}s_{*}^{p}\|\mathbf{x}\|_{ \infty}^{r^{p}}+\frac{b_{*}^{p}}{K_{*}}\Big{)}.\]

Combining (29) and the above equation, we get a uniform upper bound for \(|v(\mathbf{x};\theta_{k})|\) over \(\Theta\), which we use to upper bound \(v_{M}=\max\limits_{\begin{subarray}{c}1\leq k\leq K_{T_{m}}\\ \|\mathbf{x}\|_{\infty}\leq M_{\mathbf{\theta}^{*}}^{p}\end{subarray}}|v(\mathbf{x};\theta _{k})|\) as below

\[v_{M} \leq(J^{*}+Kd(r+1)2^{r})\,\phi_{*}^{p}(r+1)\left(\left(M_{\mathbf{ \theta}^{*}}^{T}\right)^{r}+h^{r}\right)\left(s_{*}^{p}\left(M_{\mathbf{\theta}^{* }}^{T}\right)^{r^{p}}+b_{*}^{p}(K_{*})^{-1}\right)\] \[=c_{p_{1}}\left(\left(M_{\mathbf{\theta}^{*}}^{T}\right)^{r}+h^{r} \right)\left(s_{*}^{p}\left(M_{\mathbf{\theta}^{*}}^{T}\right)^{r^{p}}+b_{*}^{p}(K _{*})^{-1}\right)\] \[\leq c_{p_{2}}\left(M_{\mathbf{\theta}^{*}}^{T}\right)^{r+r^{p}}, \tag{30}\]

where the constant terms are defined as

\[c_{p_{1}}:=\left(J^{*}+Kd(r+1)2^{r}\right)\phi_{*}^{p}(r+1),\quad c_{p_{2}}:= \max\left(1,c_{p_{1}}(h^{r}+1)(s_{*}^{p}+b_{*}^{p}(K_{*})^{-1})\right).\]

A deterministic upper bound on \(v_{M}\) can also be found from the above equation. Noting that from Assumption 2, until time \(T\) only states with each component less than or equal to \(hT\) are visited, we have

\[v_{M}\leq c_{p_{2}}\left(M_{\mathbf{\theta}^{*}}^{T}\right)^{r+r^{p}}\leq c_{p_{2} }(Th)^{r+r^{p}}:=Q(T),\]

where \(Q(T)\) is a polynomial defined as above. Using the bounds derived for \(v_{M}\), we bound \(R_{2}\) starting with the first term on the right-hand side of (28). We have

\[\mathbb{E}\left[\,\sum_{k=1}^{K_{T}}\sum_{t=t_{k}}^{t_{k+1}-1}2v_{ M}\left[\mathbb{I}_{\{\mathbf{\theta}^{*}\notin B_{k}\}}+\mathbb{I}_{\{\theta_{k} \notin B_{k}\}}\right]\,\right] \leq 2Q(T)\,\mathbb{E}\left[\,\sum_{k=1}^{K_{T}}\sum_{t=t_{k}}^{t _{k+1}-1}\mathbb{I}_{\{\mathbf{\theta}^{*}\notin B_{k}\}}+\mathbb{I}_{\{\theta_{k} \notin B_{k}\}}\right]\] \[\leq 2TQ(T)\,\mathbb{E}\left[\,\sum_{k=1}^{K_{T}}\mathbb{I}_{\{ \mathbf{\theta}^{*}\notin B_{k}\}}+\mathbb{I}_{\{\theta_{k}\notin B_{k}\}}\right]\] \[\leq 4TQ(T)\sum_{k=1}^{T}\mathbb{P}\{\mathbf{\theta}^{*}\notin B_{k}\}, \tag{31}\]

where the last inequality follows from (20) and the fact that set \(B_{k}\) is \(\mathcal{H}_{t_{k}}-\)measurable. To further simplify the first term in (28), we find an upper bound for \(\mathbb{P}\left\{\mathbf{\theta}^{*}\notin B_{k}\right\}\) using [64]. For a fixed \(\mathbf{z}=(\mathbf{x},a)\) and \(n\) independent samples of the distribution \(P_{\mathbf{\theta}^{*}}(.|\mathbf{z})\), the \(L^{1}\)-deviation of the true distribution \(P_{\mathbf{\theta}^{*}}(.|\mathbf{z})\) and empirical distribution at the end of episode \(k\), \(P_{\tilde{\theta}_{k}}(.|\mathbf{z})\), is bounded in [10] as

\[\mathbb{P}\left\{\|P_{\mathbf{\theta}^{*}}(\cdot|\mathbf{z})-P_{\tilde{\theta}_{k}}( \cdot|\mathbf{z})\|_{1}\geq\sqrt{\frac{14\prod_{i=1}^{d}(x_{i}+h)}{n}\log\left(\frac {2|\mathcal{A}|T}{\tilde{\delta}}\right)}\right\}\leq\frac{\tilde{\delta}}{20| \mathcal{A}|T^{7}\prod_{i=1}^{d}(x_{i}+h)}.\]

Therefore,

\[\mathbb{P}\left\{\|P_{\mathbf{\theta}^{*}}(\cdot|\mathbf{z})-P_{\tilde{\theta}_{k}}( \cdot|\mathbf{z})\|_{1}\geq\beta_{k}(\mathbf{z})\,\Big{|}\,N_{t_{k}}(\mathbf{z})=n\right\} \leq\frac{\tilde{\delta}}{20|\mathcal{A}|T^{7}\prod_{i=1}^{d}(x_{i}+h)},\]and

\[\mathbb{P}\left\{\|P_{\mathbf{\theta}^{\star}}(\cdot|\mathbf{z})-P_{\hat{ \theta}_{k}}(\cdot|\mathbf{z})\|_{1}\geq\beta_{k}(\mathbf{z})\right\}\] \[=\sum_{n=1}^{T}\mathbb{P}\left\{\|P_{\mathbf{\theta}^{\star}}(\cdot| \mathbf{z})-P_{\hat{\theta}_{k}}(\cdot|\mathbf{z})\|_{1}\geq\beta_{k}(\mathbf{z})\,\Big{|} \,N_{t_{k}}(\mathbf{z})=n\right\}\mathbb{P}\left\{N_{t_{k}}(\mathbf{z})=n\right\}\] \[\leq\frac{\tilde{\delta}}{20|\mathcal{A}|T^{6}\prod_{i=1}^{d}(x_{ i}+h)}.\]

The probability that at episode \(k\leq T\), the true parameter \(\mathbf{\theta}^{\star}\) does not belong to the confidence set \(B_{k}\) can be bounded using the above and union bound as

\[\mathbb{P}\{\mathbf{\theta}^{\star}\notin B_{k}\} \leq\sum_{\mathbf{z}\in\{0,1,\cdots,hT\}^{d}\times\mathcal{A}}\mathbb{ P}\left\{\|P_{\mathbf{\theta}^{\star}}(\cdot|\mathbf{z})-P_{\hat{\theta}_{k}}(\cdot| \mathbf{z})\|_{1}\geq\beta_{k}(\mathbf{z})\right\}\] \[\leq\sum_{\mathbf{z}\in\{0,1,\cdots,hT\}^{d}\times\mathcal{A}}\frac{ \tilde{\delta}}{20|\mathcal{A}|T^{6}\prod_{i=1}^{d}(x_{i}+h)}\] \[=\sum_{\mathbf{x}\in\{0,1,\cdots,hT\}^{d}}\frac{\tilde{\delta}}{20T^{ 6}\prod_{i=1}^{d}(x_{i}+h)}\] \[\leq\frac{\tilde{\delta}}{20T^{6}}\left(\log\left(h(T+1)\right)+1 \right)^{d}\] \[\leq\frac{\tilde{\delta}}{20k^{6}}\left(\log\left(h(T+1)\right)+1 \right)^{d}.\]

In the summation in the above equation, we have simplified the expression by summing over \(x_{i}\leq hT\) instead of considering the more detailed summation over \(x_{i}\leq M_{\mathbf{\theta}}^{T}\). However, this simplification does not affect the final evaluation of regret, as this term is not dominant and only contributes to a logarithmic term in the regret bound. Substituting in (31),

\[\mathbb{E}\left[\sum_{k=1}^{K_{T}}\sum_{t=t_{k}}^{t_{k+1}-1}2v_{M }\left[\mathbb{I}_{\{\mathbf{\theta}^{\star}\notin B_{k}\}}+\mathbb{I}_{\{\theta_ {k}\notin B_{k}\}}\right]\,\right] \leq 4TQ(T)\sum_{k=1}^{T}\mathbb{P}\{\mathbf{\theta}^{\star}\notin B _{k}\}\] \[\leq\frac{\tilde{\delta}\left(\log\left(h(T+1)\right)+1\right)^{d} TQ(T)}{5}\sum_{k=1}^{\infty}\frac{1}{k^{6}}\] \[<\tilde{\delta}\left(\log\left(h(T+1)\right)+1\right)^{d}TQ(T). \tag{32}\]

We now upper bound the second term in (28). From (30),

\[\mathbb{E}\left[\sum_{k=1}^{K_{T}}\sum_{t=t_{k}}^{t_{k+1}-1}2v_{M }\beta_{k}\left(\mathbf{Z}\left(t\right)\right)\right]\leq 2c_{p_{2}}\,\mathbb{E} \left[\left(M_{\mathbf{\theta}^{\star}}^{T}\right)^{r+r}\sum_{k=1}^{K_{T}}\sum_{t =t_{k}}^{t_{k+1}-1}\beta_{k}\left(\mathbf{Z}\left(t\right)\right)\right]\!. \tag{33}\]

To bound the regret term resulting from the summation of \(\beta_{k}\left(\mathbf{Z}\left(t\right)\right)\), we note that from the second stopping criterion, \(N_{t}\left(\mathbf{Z}\left(t\right)\right)\leq 2N_{t_{k}}\left(\mathbf{Z}\left(t \right)\right)\) for all \(t_{k}\leq t<t_{k+1}\) and

\[\sum_{k=1}^{K_{T}}\sum_{t=t_{k}}^{t_{k+1}-1}\beta_{k}\left(\mathbf{Z} \left(t\right)\right)\] \[=\sum_{k=1}^{K_{T}}\sum_{t=t_{k}}^{t_{k+1}-1}\sqrt{\frac{14\prod_{ i=1}^{d}(\mathbf{X}_{i}\left(t\right)+h)}{\max(1,N_{t_{k}}(\mathbf{Z}\left(t\right)))} \log\left(\frac{2|\mathcal{A}|T}{\tilde{\delta}}\right)}\] \[\leq\sqrt{14\log\left(\frac{2|\mathcal{A}|T}{\tilde{\delta}} \right)}\left[\sum_{k=1}^{K_{T}}\sum_{t=t_{k}}^{\tilde{t}_{k+1}-1}\sqrt{ \frac{2\prod_{i=1}^{d}(\mathbf{X}_{i}\left(t\right)+h)}{\max(1,N_{t}(\mathbf{Z}\left( t\right)))}}+\sum_{k=1}^{K_{T}}\sum_{t=t_{k+1}}^{t_{k+1}-1}\sqrt{\prod_{i=1}^{d}( \mathbf{X}_{i}\left(t\right)+h)}\right]. \tag{34}\]The first summation can be simplified as

\[\sum_{k=1}^{K_{T}}\sum_{t=t_{k}}^{t_{k+1}-1}\sqrt{\frac{2\prod_{i=1}^{ d}(\mathbf{X}_{i}\left(t\right)+h)}{\max(1,N_{t}(\mathbf{Z}\left(t\right)))}} \leq\sqrt{2(M_{\mathbf{\theta}^{\star}}^{T}+h)^{d}}\sum_{k=1}^{K_{T}} \sum_{t=t_{k}}^{t_{k+1}-1}\frac{1}{\sqrt{\max(1,N_{t}(\mathbf{Z}\left(t\right)))}}\] \[\leq 3\sqrt{2(M_{\mathbf{\theta}^{\star}}^{T}+h)^{d}}\sum_{\mathbf{z}\in \{0,1,\cdots,M_{\mathbf{\theta}^{\star}}^{T}\}^{d}\times\mathcal{A}}\sqrt{N_{T+1}( \mathbf{z})}\] \[\leq 3\sqrt{2|\mathcal{A}|}(M_{\mathbf{\theta}^{\star}}^{T}+h)^{d} \sqrt{\sum_{\mathbf{z}\in\{0,1,\cdots,M_{\mathbf{\theta}^{\star}}^{T}\}^{d}\times \mathcal{A}}N_{T+1}(\mathbf{z})}\Big{]}\] \[\leq 3\sqrt{2|\mathcal{A}|}T(M_{\mathbf{\theta}^{\star}}^{T}+h)^{d},\]

where the second inequality is due to the following arguments,

\[\sum_{k=1}^{K_{T}}\sum_{t=t_{k}}^{t_{k+1}-1}\frac{1}{\sqrt{\max(1, N_{t}(\mathbf{Z}\left(t\right)))}} =\sum_{\mathbf{z}\in\{0,1,\cdots,M_{\mathbf{\theta}^{\star}}^{T}\}^{d} \times\mathcal{A}}\left(\mathbb{I}_{\{N_{T+1}(\mathbf{z})>0\}}+\sum_{i=1}^{N_{T+1} (\mathbf{z})-1}\frac{1}{\sqrt{i}}\right)\] \[\leq 3\sum_{\mathbf{z}\in\{0,1,\cdots,M_{\mathbf{\theta}^{\star}}^{T}\}^{ d}\times\mathcal{A}}\sqrt{N_{T+1}(\mathbf{z})}.\]

For the second term in (34), we get

\[\sum_{k=1}^{K_{T}}\sum_{t=\tilde{t}_{k+1}}^{t_{k+1}-1}\sqrt{\prod _{i=1}^{d}(\mathbf{X}_{i}\left(t\right)+h)} =\sqrt{(M_{\mathbf{\theta}^{\star}}^{T}+h)^{d}}\sum_{k=1}^{K_{T}}E_{k}\] \[\leq K_{T}\left(\max_{1\leq i\leq T}\tau_{0^{d}}^{(i)}\right)\sqrt {(M_{\mathbf{\theta}^{\star}}^{T}+h)^{d}}\] \[\leq 2\sqrt{|\mathcal{A}|T\log_{2}T}\left(\max_{1\leq i\leq T}\tau _{0^{d}}^{(i)}\right)(M_{\mathbf{\theta}^{\star}}^{T}+h)^{d},\]

where \(E_{k}=T_{k}-\tilde{T}_{k}\), and \(K_{T}\) is bounded from Lemma 8. Thus \(\sum_{k=1}^{K_{T}}\sum_{t=t_{k}}^{t_{k+1}-1}\beta_{k}\left(\mathbf{Z}\left(t\right)\right)\) is bounded as

\[\sum_{k=1}^{K_{T}}\sum_{t=t_{k}}^{t_{k+1}-1}\beta_{k}\left(\mathbf{Z}\left(t\right) \right)\leq 24\sqrt{|\mathcal{A}|T\log_{2}T\log\left(\frac{2|\mathcal{A}|T}{ \tilde{\delta}}\right)}\left(\max_{1\leq i\leq T}\tau_{0^{d}}^{(i)}\right)(M _{\mathbf{\theta}^{\star}}^{T}+h)^{d}.\]

Substituting the above bound in (33),

\[\mathbb{E}\left[\sum_{k=1}^{K_{T}}\sum_{t=t_{k}}^{t_{k+1}-1}2v_{M }\beta_{k}\left(\mathbf{Z}\left(t\right)\right)\right]\] \[\leq 48c_{p_{2}}\sqrt{|\mathcal{A}|T\log_{2}T\log\left(\frac{2| \mathcal{A}|T}{\tilde{\delta}}\right)}\,\mathbb{E}\left[\left(M_{\mathbf{\theta}^ {\star}}^{T}\right)^{\tau+r_{*}^{p}}(M_{\mathbf{\theta}^{\star}}^{T}+h)^{d}\left( \max_{1\leq i\leq T}\tau_{0^{d}}^{(i)}\right)\right]\] \[\leq c_{p_{3}}\sqrt{|\mathcal{A}|T\log_{2}T\log\left(\frac{2| \mathcal{A}|T}{\tilde{\delta}}\right)}\,\mathbb{E}\left[(M_{\mathbf{\theta}^{\star }}^{T}+h)^{d+\tau+r_{*}^{p}}\left(\max_{1\leq i\leq T}\tau_{0^{d}}^{(i)}\right) \right],\]

where \(c_{p_{3}}:=48c_{p_{2}}\). Finally, from the above equation, (32), and (28),

\[R_{2} \leq\tilde{\delta}\left(\log\left(h(T+1)\right)+1\right)^{d}TQ(T)\] \[+c_{p_{3}}\sqrt{|\mathcal{A}|T\log_{2}T\log\left(\frac{2| \mathcal{A}|T}{\tilde{\delta}}\right)}\,\mathbb{E}\left[(M_{\mathbf{\theta}^{\star }}^{T}+h)^{d+\tau+r_{*}^{p}}\Big{(}\max_{1\leq i\leq T}\tau_{0^{d}}^{(i)}\Big{)} \right].\]

By choosing \(\tilde{\delta}=\frac{1}{TQ(T)}\), we get

\[R_{2}\] \[\leq\left(\log(h(T+1))+1\right)^{d}+c_{p_{3}}\sqrt{|\mathcal{A}|T \log_{2}T\log(2|\mathcal{A}|T^{2}Q(T))}\,\mathbb{E}\left[(M_{\mathbf{\theta}^{ \star}}^{T}+h)^{d+\tau+r_{*}^{p}}\Big{(}\max_{1\leq i\leq T}\tau_{0^{d}}^{(i)} \Big{)}\right],\]where \(Q(T)=c_{p_{2}}(Th)^{r+r_{*}^{p}}\). 

### Proof of Theorem 1

Proof.: Lemmas 1, 2, and 3 along with Cauchy-Schwarz inequality showed that the regret terms \(R_{0}\) and \(R_{2}\) are of the order \(\tilde{O}(KrdJ^{*}h^{d+2r+r_{*}^{p}}\sqrt{|\mathcal{A}|T})\) and the term \(R_{1}\) is \(\tilde{O}(J^{*}(h)^{r_{*}^{2}})\). Therefore, from \(R(T,\pi_{TSDE})=R_{0}+R_{1}+R_{2}\), the regret of Algorithm 1, \(R(T,\pi_{TSDE})\), is \(\tilde{O}(KrdJ^{*}h^{d+2r+r_{*}^{p}}\sqrt{|\mathcal{A}|T})\). 

### Requirement of an optimal policy oracle.

To implement our algorithm, we need to find the optimal policy for each model sampled by the algorithm--optimal policy for Theorem 1 and optimal policy within policy class \(\Pi\) for Corollary 1; this has also been used in past work [23; 24; 36]. In the finite state-space setting, [49] provides a schedule of \(\epsilon\) values and selects \(\epsilon\)-optimal policies to obtain \(\tilde{O}(\sqrt{T})\) regret guarantees. The issue with extending the analysis of [49] to the countable state-space setting is that we need to ensure (uniform) ergodicity for the chosen \(\epsilon\)-optimal policies; the \(\limsup\) or \(\liminf\) of the time-average expected reward (used to define the average cost problem) being finite doesn't imply ergodicity. In other words, we must formulate (and verify) ergodicity assumptions for a potentially large set of close-to-optimal algorithms whose structure is undetermined. Another issue is that, to the best of our knowledge, there isn't a general structural characterization of all \(\epsilon\)-optimal stationary policies for countable state-space MDPs or even a characterization of the policy within this set that is selected by any computational procedure in the literature; current results only discuss existence and characterization of the stationary optimal policy. In the absence of such results, stability assumptions with the same uniformity across models as in our submission will be needed, which are likely too strong to be useful.

If we could verify the stability requirements of Assumptions 3 and 4 for a subset of policies, the optimal oracle is not needed, and instead, by choosing approximately optimal policies within this subset, we can follow the same proof steps as [49] to guarantee regret performance similar to Corollary 1 (without knowledge of model parameters). To theoretically analyze the performance of the algorithm that follows an approximately optimal policy rather than the optimal one, we assume that for a specific sequence of \(\{\epsilon_{k}\}_{k=1}^{\infty}\), an \(\epsilon_{k}\)-optimal policy is given, which is defined below.

**Definition 1**.: _Policy \(\pi\in\Pi\) is called an \(\epsilon\)-optimal policy if for every \(\theta\in\Theta\),_

\[c(\mathbf{x},\pi(\mathbf{x}))+\sum_{\mathbf{y}\in\mathcal{X}}P_{\theta}(\mathbf{y}|\mathbf{x},\pi (\mathbf{x}))v(\mathbf{y};\theta)\leq c(\mathbf{x},\pi_{\theta}^{*}(\mathbf{x}))+\sum_{\mathbf{y} \in\mathcal{X}}P_{\theta}(\mathbf{y}|\mathbf{x},\pi_{\theta}^{*}(\mathbf{x}))v(\mathbf{y}; \theta)+\epsilon,\]

_where \(\pi_{\theta}^{*}\) is the optimal policy in the policy class \(\Pi\) corresponding to parameter \(\theta\) and \(v(.;\theta)\) is the solution to Poisson equation (5)._

Given \(\epsilon\)-optimal policies that satisfy Assumptions 3 and 4, in Theorem 2 we extend the regret guarantees of Corollary 1 to the algorithm employing \(\epsilon\)-optimal policy, instead of the best-in-class policy, and show that the same regret upper bounds continue to apply.

**Theorem 3**.: _Consider a non-negative sequence \(\{\epsilon_{k}\}_{k=1}^{\infty}\) such that for every \(k\in\mathbb{N}\), \(\epsilon_{k}\) is bounded above by \(\frac{1}{k+1}\) and an \(\epsilon_{k}\)-optimal policy satisfying Assumptions 3 and 4 is given. The regret incurred by Algorithm 1 while using the \(\epsilon_{k}\)-optimal policy during any episode \(k\) is \(\tilde{O}(dh^{d}\sqrt{|\mathcal{A}|T})\)._

Proof.: For the \(\epsilon_{k}\)-optimal policy used in episode \(k\), shown by \(\pi^{\epsilon_{k}}\), we have

\[c(\mathbf{x},\pi^{\epsilon_{k}}(\mathbf{x}))+\sum_{\mathbf{y}\in\mathcal{X}} P_{\theta_{k}}(\mathbf{y}|\mathbf{x},\pi^{\epsilon_{k}}(\mathbf{x}))v(\mathbf{y};\theta_{k}) \leq c(\mathbf{x},\pi_{\theta_{k}}^{*}(\mathbf{x}))+\sum_{\mathbf{y}\in \mathcal{X}}P_{\theta_{k}}(\mathbf{y}|\mathbf{x},\pi_{\theta_{k}}^{*}(\mathbf{x}))v(\mathbf{y };\theta_{k})+\epsilon_{k}\] \[=J(\theta_{k})+v(\mathbf{x};\theta_{k})+\epsilon_{k}.\]Thus,

\[R(T,\pi_{TSDE}) =\mathbb{E}\,\Big{[}\sum_{k=1}^{K_{T}}\sum_{t=t_{k}}^{t_{k+1}-1}c( \boldsymbol{X}(t),\pi^{\epsilon_{k}}(\boldsymbol{X}(t)))\Big{]}-T\,\mathbb{E}\, [J\,(\boldsymbol{\theta}^{*})]=R_{0}+R_{1}+R_{2}+\mathbb{E}\,\Big{[}\sum_{k=1} ^{K_{T}}T_{k}\epsilon_{k}\Big{]}\] \[\text{with }R_{0} =\mathbb{E}\,\Big{[}\sum_{k=1}^{K_{T}}T_{k}J(\theta_{k})\Big{]}-T \,\mathbb{E}\,\Big{[}J(\boldsymbol{\theta}^{*})\Big{]},\] \[R_{1} =\mathbb{E}\,\Big{[}\sum_{k=1}^{K_{T}}\sum_{t=t_{k}}^{t_{k+1}-1} \Big{[}v(\boldsymbol{X}(t);\theta_{k})-v(\boldsymbol{X}(t+1);\theta_{k})\Big{]} \Big{]},\] \[R_{2} =\mathbb{E}\,\Big{[}\sum_{k=1}^{K_{T}}\sum_{t=t_{k}}^{t_{k+1}-1} \Big{[}v(\boldsymbol{X}(t+1);\theta_{k})-\sum_{\boldsymbol{y}\in\mathcal{X}}P_ {\theta_{k}}(\boldsymbol{y}|\boldsymbol{X}(t),\pi^{\epsilon_{k}}(\boldsymbol{X }(t)))v(\boldsymbol{y};\theta_{k})\Big{]}\Big{]}.\]

We assumed that given \(\epsilon\)-optimal policies satisfy Assumptions 3 and 4. As a result, we can utilize the proof of Theorem 1 to deduce that the term \(R_{0}+R_{1}+R_{2}\) is of the order \(\tilde{O}(dh^{4}\sqrt{|\mathcal{A}|T})\). Moreover, we can simplify the term \(\mathbb{E}\,\Big{[}\sum_{k=1}^{K_{T}}T_{k}\epsilon_{k}\Big{]}\) as below:

\[\mathbb{E}\,\Big{[}\sum_{k=1}^{K_{T}}T_{k}\epsilon_{k}\Big{]}= \mathbb{E}\,\Big{[}\sum_{k=1}^{K_{T}}\tilde{T}_{k}\epsilon_{k}\Big{]}+\mathbb{ E}\,\Big{[}\sum_{k=1}^{K_{T}}E_{k}\epsilon_{k}\Big{]}. \tag{35}\]

From the second stopping condition of Algorithm 1, we have \(\tilde{T}_{k}\leq\tilde{T}_{k-1}+1\leq\ldots\leq k+1\) and

\[\mathbb{E}\,\Big{[}\sum_{k=1}^{K_{T}}T_{k}\epsilon_{k}\Big{]}\leq \mathbb{E}[K_{T}],\]

where we have used the assumption that \(\epsilon_{k}\leq\frac{1}{k+1}\). For the second term of (35), from (25)

\[\mathbb{E}\,\Big{[}\sum_{k=1}^{K_{T}}E_{k}\epsilon_{k}\Big{]}\leq \mathbb{E}\,\Big{[}\sum_{k=1}^{K_{T}}\frac{E_{k}}{k+1}\Big{]}\leq\mathbb{E}\, \Big{[}\max_{1\leq i\leq T}\sigma_{0^{d}}^{(i)}\sum_{k=1}^{K_{T}}\frac{1}{k+1} \Big{]}\leq\mathbb{E}\,\Big{[}\max_{1\leq i\leq T}\tau_{0^{d}}^{(i)}\log(K_{T} +1)\Big{]}, \tag{36}\]

where in the last inequality we have used \(\sum_{i=1}^{n}\frac{1}{n}\leq 1+\log(n)\). Finally, as a result of Lemma 6 and Lemma 8, the result follows. 

## Appendix C Bounds on hitting times under polynomial and geometric ergodicity

### Polynomial upper bounds for the moments of hitting time of state \(0^{d}\)

For any \(\theta_{1},\theta_{2}\in\Theta\), consider the Markov process with transition kernel \(P_{\theta_{1}}^{\pi_{\theta_{2}}^{*}}\) obtained from the MDP \((\mathcal{X},\mathcal{A},c,P_{\theta_{1}})\) by following policy \(\pi_{\theta_{2}}^{*}\). [29, Lemma 3.5] establishes that if the process is polynomially ergodic, equivalently satisfies (4), then for every \(0<\eta\leq 1\), there exists constants \(\beta_{\theta_{1},\theta_{2}}^{\eta}\), \(b_{\theta_{1},\theta_{2}}^{\eta}>0\) such that the following holds:

\[\Delta\left(V_{\theta_{1},\theta_{2}}^{p}\right)^{\eta}(\boldsymbol{x})\leq- \beta_{\theta_{1},\theta_{2}}^{\eta}\left(V_{\theta_{1},\theta_{2}}^{p}( \boldsymbol{x})\right)^{\alpha_{\theta_{1},\theta_{2}}^{p}+\eta-1}+b_{\theta_ {1},\theta_{2}}^{\eta}\mathbb{I}_{C_{\theta_{1},\theta_{2}}^{p}}(\boldsymbol {x}),\quad\boldsymbol{x}\in\mathcal{X}, \tag{37}\]

where for \(\eta\in(0,1)\), \(\tilde{\beta}_{\theta_{1},\theta_{2}}^{p}:=\min(\beta_{\theta_{1},\theta_{2}} ^{p},1)\) and

\[\beta_{\theta_{1},\theta_{2}}^{\eta}=\eta\tilde{\beta}_{\theta_{1},\theta_{2} }^{p},\;b_{\theta_{1},\theta_{2}}^{\eta}=\Big{(}b_{\theta_{1},\theta_{2}}^{p} \Big{)}^{\eta}+\eta\tilde{\beta}_{\theta_{1},\theta_{2}}^{p}\max\left(1,\Big{(} \tilde{\beta}_{\theta_{1},\theta_{2}}^{p}\Big{)}^{(\alpha_{\theta_{1},\theta_ {2}}^{p}+\eta-1)/(1-\alpha_{\theta_{1},\theta_{2}}^{p})}\right), \tag{38}\]

and for \(\eta=1\), \(\beta_{\theta_{1},\theta_{2}}^{\eta}=\beta_{\theta_{1},\theta_{2}}^{p}\) and \(b_{\theta_{1},\theta_{2}}^{\eta}=b_{\theta_{1},\theta_{2}}^{p}\). Consequently, the following result is immediate from the proof of [29, Theorem 3.6]; for completeness, we provide the proof in Appendix D.1.

**Lemma 9**.: _Suppose a finite set \(C^{p}_{\theta_{1},\theta_{2}}\), constants \(\beta^{p}_{\theta_{1},\theta_{2}},b^{p}_{\theta_{1},\theta_{2}}>0\), \(r/(r+1)\leq\alpha^{p}_{\theta_{1},\theta_{2}}<1\), and a function \(V^{p}_{\theta_{1},\theta_{2}}:\mathcal{X}\rightarrow[1,+\infty)\) exist such that (4) holds. Then, there exist a sequence of non-negative functions \(V^{i}_{\theta_{1},\theta_{2}}:\mathcal{X}\rightarrow[1,+\infty)\) for \(i=0,\ldots,r+1\) that satisfy the following system of drift equations for finite sets \(C^{i}_{\theta_{1},\theta_{2}}\), constants \(b^{i}_{\theta_{1},\theta_{2}}\geq 0\) and \(\beta^{i}_{\theta_{1},\theta_{2}}>0\):_

\[\Delta V^{i-1}_{\theta_{1},\theta_{2}}(\mathbf{x})\leq-\beta^{i}_{ \theta_{1},\theta_{2}}V^{i}_{\theta_{1},\theta_{2}}(\mathbf{x})+b^{i}_{\theta_{1},\theta_{2}}\mathbb{I}_{C^{i}_{\theta_{1},\theta_{2}}}(\mathbf{x}),\qquad\mathbf{x} \in\mathcal{X},\;i=1,\ldots,r+1. \tag{39}\]

Notice that \(r\) is the maximum degree of the cost function \(c\) defined in Assumption 1. Following the proof and approach of [29] and using the set of equations (39), we can find an upper-bound for \(\mathbb{E}_{\mathbf{x}}[\tau^{i}_{0^{d}}]\) for \(i=1,\ldots,r+1\) in Lemma 10. In order to establish upper bounds for the first \(r+1\) moments of \(\tau_{0^{d}}\), it is crucial to choose the value of \(\alpha^{p}_{\theta_{1},\theta_{2}}\) greater than or equal to \(\frac{r}{r+1}\), as demonstrated in the proof of Lemma 10 in Appendix D.2

**Lemma 10**.: _For \(i=1,\ldots,r+1\), and for all \(\mathbf{x}\in\mathcal{X}\)_

\[\mathbb{E}_{\mathbf{x}}^{\pi^{s}_{\theta_{2}}}[(\tau_{0^{d}})^{i}]\leq i \phi^{p}_{\theta_{1},\theta_{2}}(i)\left(V^{p}_{\theta_{1},\theta_{2}}(\mathbf{x}) +b^{p}_{\theta_{1},\theta_{2}}\alpha_{C^{p}_{\theta_{1},\theta_{2}}}\right),\]

_where \(\phi^{p}_{\theta_{1},\theta_{2}}(i):=\prod_{j=1}^{i}\frac{1}{\beta^{p}_{\theta _{1},\theta_{2}}}\left(2^{j-1}+(j-1)\,\alpha_{C^{p}_{\theta_{1},\theta_{2}}}b ^{\eta_{j}}_{\theta_{1},\theta_{2}}\right)\), \(\eta_{i}=1-(i-1)(1-\alpha^{p}_{\theta_{1},\theta_{2}})\), \(b^{\eta_{i}}_{\theta_{1},\theta_{2}}\) and \(\beta^{\eta_{i}}_{\theta_{1},\theta_{2}}\) defined in (38), and \(\alpha_{C^{p}_{\theta_{1},\theta_{2}}}=\left(\min_{\mathbf{y}\in C^{p}_{\theta_{1 },\theta_{2}}}K_{\theta_{1},\theta_{2}}(\mathbf{y})\right)^{-1}\)._

Based on Lemma 10, we impose the conditions of Assumption 4 to obtain uniform (over model class) and polynomial (in norm of the state) upper-bounds on the moments of hitting times to \(0^{d}\). Moreover, these conditions lead to a uniform characterization of parameters of Lemma 10 over all models in our class.

### Distribution of return times to state \(0^{d}\)

For any \(\theta_{1},\theta_{2}\in\Theta\), consider the Markov process with transition kernel \(P^{\pi^{*}_{\theta_{2}}}_{\theta_{1}}\) obtained from the MDP \((\mathcal{X},\mathcal{A},c,P_{\theta_{1}})\) by following policy \(\pi^{*}_{\theta_{2}}\). In the following lemma, we show that the tail probabilities of the return times to the common state \(0^{d}\), again \(\tau_{0^{d}}\), converge geometrically fast to \(0\), and characterize the convergence parameters in terms of the constants given in Assumption 3. Explicitly, we show

\[\mathbb{P}_{0^{d}}(\tau_{0^{d}}>n)\leq c^{g}_{\theta_{1},\theta_{ 2}}\left(\tilde{\gamma}^{g}_{\theta_{1},\theta_{2}}\right)^{n},\]

for problem and policy dependent constants \(c^{g}_{\theta_{1},\theta_{2}}\) and \(\tilde{\gamma}^{g}_{\theta_{1},\theta_{2}}\). We will follow the method outlined in [27] with the goal to identify problem dependent parameters that will be relevant to our results. Proof of the following lemma is given in Appendix D.3 and follows the methodology of [27].

**Lemma 11**.: _For every \(\theta_{1},\theta_{2}\in\Theta\) in the Markov process obtained from the Markov decision process \((\mathcal{X},\mathcal{A},c,P_{\theta_{1}})\) following policy \(\pi^{*}_{\theta_{2}}\), the return time to state \(0\) starting from state \(0\) satisfies the following:_

\[\mathbb{P}_{0^{d}}(\tau_{0^{d}}>n)\leq c^{g}_{\theta_{1},\theta_{ 2}}\left(\tilde{\gamma}^{g}_{\theta_{1},\theta_{2}}\right)^{n},\]

_where_

\[c^{g}_{\theta_{1},\theta_{2}}=\frac{b^{g}_{\theta_{1},\theta_{ 2}}\left(\tilde{b}^{g}_{\theta_{1},\theta_{2}}\right)^{2}}{\tilde{b}^{g}_{ \theta_{1},\theta_{2}}-1}\quad\text{ and }\quad\tilde{\gamma}^{g}_{\theta_{1}, \theta_{2}}=1-\frac{1}{\tilde{b}^{g}_{\theta_{1},\theta_{2}}},\]

_with_

\[\tilde{b}^{g}_{\theta_{1},\theta_{2}}=\frac{3b^{g}_{\theta_{1}, \theta_{2}}+1}{1-\gamma^{g}_{\theta_{1},\theta_{2}}}\left(\,|C^{g}_{\theta_{1 },\theta_{2}}|^{2}\max\left(1,\max_{\mathbf{u}\in C^{g}_{\theta_{1},\theta_{2}} \setminus\{0^{d}\}}\mathbb{E}_{\mathbf{u}}^{\pi^{*}_{\theta_{2}}}[\tau_{0^{d}}] \right)\right).\]

Based on Lemma 11, it is necessary to impose the conditions in Assumption 3 to obtain uniform tail probability bounds on \(\tau_{0^{d}}\) for all model parameters and policy choices in \(\Theta\). Moreover, these conditions lead to a uniform characterization of \(c^{g}_{\theta_{1},\theta_{2}}\) and \(\tilde{\gamma}^{g}_{\theta_{1},\theta_{2}}\) over \(\Theta\). Furthermore, as a result of Lemma 10 and uniformity conditions of Assumption 4, \(\mathbb{E}_{\mathbf{u}}^{\pi^{*}_{\theta_{2}}}[\tau_{0^{d}}]\) has a uniform bound over \(\Theta\) and \(C^{g}_{\theta_{1},\theta_{2}}\setminus\{0^{d}\}\), which can be characterized in terms of the polynomial Lyapunov function.

Proofs of hitting time bounds

### Proof of Lemma 9

Proof.: In the proof, to avoid cumbersome notation we will drop the indices \(\theta_{1},\theta_{2}\). Following the proof of Theorem 3.6 in [29], we choose \(\eta_{i}=1-(i-1)(1-\alpha^{p})\) for \(i=1,\ldots,r+1\) and note that as \(\alpha^{p}\in[\frac{r}{r+1},1)\), we have \(\eta_{i}\in[\frac{1}{r+1},1]\). As a result, we can apply (37) to each \(\eta_{i}\) to get

\[\Delta\left(V^{p}\right)^{\eta_{i}}(\mathbf{x})\leq-\beta^{\eta_{i}}\left(V^{p}( \mathbf{x})\right)^{i\alpha^{p}-i+1}+b^{\eta_{i}}\mathbb{I}_{C^{p}}(\mathbf{x}),\quad i =1,\ldots,r+1.\]

Thus, the system of drift equations (39) hold for

\[V_{i} =\left(V^{p}\right)^{1-i(1-\alpha^{p})}, i=0,\ldots,r+1,\] \[\beta_{i} =\beta^{\eta_{i}}, i=1,\ldots,r+1,\] \[b_{i} =b^{\eta_{i}}, i=1,\ldots,r+1,\] \[C_{i} =C^{p}, i=1,\ldots,r+1,\]

where \(\beta^{\eta_{i}}\) and \(b^{\eta_{i}}\) are defined in (38). 

### Proof of Lemma 10

The proof of Lemma 10 uses the following lemma.

**Lemma 12** (Proposition 11.3.2, [43]).: _Suppose for nonnegative functions \(f\), \(g\), and \(V\) on the state space \(\mathcal{X}\) and every \(k\in\mathbb{Z}_{+}\), the following holds:_

\[\mathbb{E}[V(X_{k+1})|\mathcal{F}_{k}]\leq V(X_{k})-f(X_{k})+g(X_{k}).\]

_Then, for any initial condition \(x\) and stopping time \(\tau\)_

\[\mathbb{E}_{x}\left[\sum_{k=0}^{\tau-1}f(X_{k})\right]\leq V(x)+\mathbb{E}_{x} \left[\sum_{k=0}^{\tau-1}g(X_{k})\right].\]

Proof of Lemma 10.: Following [29], the proof uses an induction argument. We will use the notation of Lemma 9 for simplicity. Similarly, in this proof we will also denote \(\phi_{\theta_{1},\theta_{2}}^{p}(i)\) as \(\phi(i)\), \(K_{\theta_{1},\theta_{2}}(\cdot)\) as \(K(\cdot)\), and \(V_{\theta_{1},\theta_{2}}^{i}\), \(b_{\theta_{1},\theta_{2}}^{i}\), \(\beta_{\theta_{1},\theta_{2}}^{i}\), \(C_{\theta_{1},\theta_{2}}^{i}\) as \(V_{i}\), \(b_{i}\), \(\beta_{i}\), \(C_{i}\).

From irreducibility, for all \(\mathbf{x}\in\mathcal{X}\), \(K(\mathbf{x})\) is positive and finite. Considering the system of drift equations found in Lemma 9, \(C_{i}=C^{p}\) is a finite set for all \(i=1,\ldots,r+1\). Thus, \(\min_{\mathbf{y}\in C_{i}}K(\mathbf{y})\) is strictly positive. For all \(\mathbf{x}\in\mathcal{X}\) and \(i=1,\ldots,r+1\), we have

\[\mathbb{I}_{C_{i}}(\mathbf{x})\leq\left(\min_{\mathbf{y}\in C_{i}}K(\mathbf{y})\right)^{- 1}K(\mathbf{x}). \tag{40}\]

We set \(\alpha_{C^{p}}:=\left(\min_{\mathbf{y}\in C_{i}}K(\mathbf{y})\right)^{-1}=\left(\min_{ \mathbf{y}\in C^{p}}K(\mathbf{y})\right)^{-1}\). From Lemma 9, for \(j=1\) and \(\mathbf{x}\in\mathcal{X}\)

\[\Delta V_{0}(\mathbf{x})\leq-\beta_{1}V_{1}(\mathbf{x})+b_{1}\mathbb{I}_{C_{1}}(\mathbf{x}).\]

By applying Lemma 12, for all \(\mathbf{x}\in\mathcal{X}\) we get

\[\beta_{1}\mathbb{E}_{\mathbf{x}}\left[\sum_{k=0}^{\tau_{qd}-1}V_{1}\left(\mathbf{X}_{k }\right)\right]\leq V_{0}(\mathbf{x})+b_{1}\mathbb{E}_{\mathbf{x}}\left[\sum_{k=0}^{ \tau_{qd}-1}\mathbb{I}_{C_{1}}\left(\mathbf{X}_{k}\right)\right]. \tag{41}\]

Using (40) and (41), followed by noting that

\[K(\mathbf{x})=\sum_{n=0}^{\infty}2^{-n-2}P^{n}(\mathbf{x},0^{d})=\sum_{n=0}^{\infty}2^ {-n-2}\mathbb{E}_{\mathbf{x}}[\mathbb{I}_{0^{d}}\left(\mathbf{X}_{n}\right)],\]we get

\[\mathbb{E}_{\mathbf{x}}\left[\sum_{k=0}^{\tau_{0d}-1}V_{1}\left(\mathbf{X}_{k }\right)\right] \leq\frac{1}{\beta_{1}}V_{0}(\mathbf{x})+\frac{b_{1}\alpha_{C^{p}}}{ \beta_{1}}\mathbb{E}_{\mathbf{x}}\left[\sum_{n=0}^{\infty}2^{-n-2}\sum_{k=0}^{\tau _{0d}-1}\mathbb{I}_{0^{d}}\left(\mathbf{X}_{k+n}\right)\right]\] \[=\frac{1}{\beta_{1}}V_{0}(\mathbf{x})+\frac{b_{1}\alpha_{C^{p}}}{ \beta_{1}}\mathbb{E}_{\mathbf{x}}\left[\sum_{n=0}^{\infty}2^{-n-2}\sum_{k=n}^{\tau _{0d}-1+n}\mathbb{I}_{0^{d}}\left(\mathbf{X}_{k}\right)\right]\] \[\leq\frac{1}{\beta_{1}}V_{0}(\mathbf{x})+\frac{b_{1}\alpha_{C^{p}}}{ \beta_{1}}\sum_{n=0}^{\infty}2^{-n-2}(n+1)\] \[=\frac{1}{\beta_{1}}V_{0}(\mathbf{x})+\frac{b_{1}\alpha_{C^{p}}}{ \beta_{1}}.\]

As \(V_{1}(\mathbf{x})\geq 1\), this gives us a bound on \(\mathbb{E}_{\mathbf{x}}[\tau_{0^{d}}]\) as follows:

\[\mathbb{E}_{\mathbf{x}}[\tau_{0^{d}}]\leq\frac{1}{\beta_{1}}V_{0}(\mathbf{x})+\frac{b_ {1}\alpha_{C^{p}}}{\beta_{1}}.\]

Assume for \(i\geq 1\), by the induction assumption we have

\[\mathbb{E}_{\mathbf{x}}\left[\sum_{k=0}^{\tau_{0^{d}}-1}(k+1)^{i-1}V_{i}\left(\mathbf{ X}_{k}\right)\right]\leq\phi(i)\left(V_{0}(\mathbf{x})+b_{1}\alpha_{C^{p}}\right). \tag{42}\]

Set \(j=i+1\) in (39), which yields

\[\Delta V_{i}(\mathbf{x})\leq-\beta_{i+1}V_{i+1}(\mathbf{x})+b_{i+1}\mathbb{I}_{C^{p}}( \mathbf{x}).\]

Define \(Z_{k}=k^{i}V_{i}(\mathbf{X}_{k})\). From the above equation, we have

\[\mathbb{E}[Z_{k+1}|\mathbf{X}_{k}] \leq(k+1)^{i}\left(V_{i}\left(\mathbf{X}_{k}\right)-\beta_{i+1}V_{i+1 }(\mathbf{X}_{k})+b_{i+1}\mathbb{I}_{C^{p}}(\mathbf{X}_{k})\right)\] \[\leq Z_{k}+2^{i}(k+1)^{i-1}V_{i}\left(\mathbf{X}_{k}\right)+(k+1)^{i} b_{i+1}\mathbb{I}_{C^{p}}(\mathbf{X}_{k})-(k+1)^{i}\beta_{i+1}V_{i+1}(\mathbf{X}_{k}).\]

By applying Lemma 12 to the above equation, we get

\[\beta_{i+1}\mathbb{E}_{\mathbf{x}}\left[\sum_{k=0}^{\tau_{0^{d}}-1}(k +1)^{i}V_{i+1}\left(\mathbf{X}_{k}\right)\right]\] \[\leq 2^{i}\mathbb{E}_{\mathbf{x}}\left[\sum_{k=0}^{\tau_{0^{d}}-1}(k +1)^{i-1}V_{i}\left(\mathbf{X}_{k}\right)\right]+b_{i+1}\mathbb{E}_{\mathbf{x}}\left[ \sum_{k=0}^{\tau_{0^{d}}-1}(k+1)^{i}\mathbb{I}_{C^{p}}\left(\mathbf{X}_{k}\right)\right]\] \[\leq 2^{i}\phi(i)\left(V_{0}(\mathbf{x})+b_{1}\alpha_{C^{p}}\right)+ \alpha_{C^{p}}b_{i+1}\mathbb{E}_{\mathbf{x}}[(\tau_{0^{d}})^{i}], \tag{43}\]

where the second inequality follows from (40) and the induction hypothesis (42). Thereafter, from (42) (by using integral lower bound after using \(V_{i}\geq 1\)), we have

\[\frac{1}{i}\mathbb{E}_{\mathbf{x}}[(\tau_{0^{d}})^{i}]\leq\mathbb{E}_{\mathbf{x}}\left[ \sum_{k=0}^{\tau_{0^{d}}-1}(k+1)^{i-1}V_{i}\left(\mathbf{X}_{k}\right)\right]\leq \phi(i)\left(V_{0}(\mathbf{x})+b_{1}\alpha_{C^{p}}\right).\]

Substituting in (43), we get

\[\beta_{i+1}\mathbb{E}_{\mathbf{x}}\left[\sum_{k=0}^{\tau_{0^{d}}-1}(k +1)^{i}V_{i+1}\left(\mathbf{X}_{k}\right)\right] \leq 2^{i}\phi(i)\left(V_{0}(\mathbf{x})+b_{1}\alpha_{C^{p}}\right)+ib _{i+1}\alpha_{C^{p}}\phi(i)\left(V_{0}(\mathbf{x})+b_{1}\alpha_{C^{p}}\right)\] \[=\left(2^{i}+ib_{i+1}\alpha_{C^{p}}\right)\phi(i)\left(V_{0}(\mathbf{x })+b_{1}\alpha_{C^{p}}\right)\] \[=\beta_{i+1}\phi(i+1)\left(V_{0}(\mathbf{x})+b_{1}\alpha_{C^{p}} \right).\]

This completes the proof.

### Proof of Lemma 11

Proof.: In the proof, to avoid cumbersome notation we will drop the indices \(\theta_{1},\theta_{2}\). Based on Assumption 3, there exists a finite set \(C^{g}\), constants \(b^{g}\), \(\gamma^{g}\in(0,1)\), and a function \(V^{g}:\mathcal{X}\to[1,+\infty)\) satisfying

\[\Delta V^{g}(\mathbf{x})\leq-\left(1-\gamma^{g}\right)V^{g}(\mathbf{x})+b^{g}\mathbb{I }_{C^{g}}(\mathbf{x}),\quad\mathbf{x}\in\mathcal{X}. \tag{44}\]

For \(n\geq 1\), define the \(n\)-step taboo probabilities [43] as

\[{}_{A}P^{n}_{\mathbf{x}B}=\mathbb{P}_{\mathbf{x}}\left(\mathbf{X}_{n}\in B,\tau_{A}>n\right),\]

where \(A,B\subseteq\mathcal{X}\), and \(\tau_{A}\) is the first hitting time of set \(A\). We also let \({}_{A}P^{0}_{\mathbf{x}B}=\mathbb{I}_{B}(\mathbf{x})\) and \(\tilde{V}^{g}=\sum_{n=0\ 0^{d}}^{\infty}P^{n}V^{g}\). Applying the last exit decomposition on \(C^{g}\setminus\{0^{d}\}\) for all \(x\in\mathcal{X}\), we obtain

\[\tilde{V}^{g}(\mathbf{x})\] \[=\sum_{n=0}^{\infty}\sum_{\mathbf{y}\in\mathcal{X}}\,_{0^{d}}P^{n}_{ \mathbf{x}\mathbf{y}}V^{g}(\mathbf{y})\] \[=V^{g}(\mathbf{x})+\sum_{n=1}^{\infty}\sum_{\mathbf{y}\in\mathcal{X}}\sum_ {C^{g}}P^{n}_{\mathbf{x}\mathbf{y}}V^{g}(\mathbf{y})\] \[+\sum_{n=1}^{\infty}\sum_{\mathbf{y}\in\mathcal{X}}\sum_{m=1}^{n-1} \sum_{\mathbf{z}\in C^{g}\setminus\{0^{d}\}}\,_{0^{d}}P^{m}_{\mathbf{x}\mathbf{z}}\,_{C^{g} }P^{n-m}_{\mathbf{z}\mathbf{y}}V^{g}(\mathbf{y})+\sum_{n=1}^{\infty}\sum_{\mathbf{y}\in\mathcal{ X}}\sum_{\mathbf{z}\in C^{g}\setminus\{0^{d}\}}\,_{0^{d}}P^{n}_{\mathbf{x}\mathbf{z}}\,_{C^{g} }P^{0}_{\mathbf{z}\mathbf{y}}V^{g}(\mathbf{y})\] \[=V^{g}(\mathbf{x})+\sum_{n=1}^{\infty}\sum_{\mathbf{y}\in\mathcal{X}}C^{g }P^{n}_{\mathbf{z}\mathbf{y}}V^{g}(\mathbf{y}) \tag{45}\] \[+\underbrace{\sum_{\mathbf{y}\in\mathcal{X}}\sum_{\mathbf{z}\in C^{g} \setminus\{0^{d}\}}\left(\sum_{m=1}^{\infty}\,_{0^{d}}P^{m}_{\mathbf{x}\mathbf{z}} \right)\left(\sum_{n=1}^{\infty}\,_{C^{g}}P^{n}_{\mathbf{z}\mathbf{y}}V^{g}(\mathbf{y}) \right)}_{\text{Term 1}}+\underbrace{\sum_{n=1}^{\infty}\sum_{\mathbf{z}\in C^{g} \setminus\{0^{d}\}}\,_{0^{d}}P^{n}_{\mathbf{x}\mathbf{z}}V^{g}(\mathbf{z})}_{\text{ Term 2}}, \tag{46}\]

where we break up the trajectories starting at state \(\mathbf{x}\) and reaching state \(\mathbf{y}\) while avoiding state \(0^{d}\) into two: ones that never visit the set \(C^{g}\), and the others that visit \(C^{g}\setminus\{0^{d}\}\) up until time \(m\) but not afterwards and exit \(C^{g}\setminus\{0^{d}\}\) at time \(m\).

We first bound Term 1 in (46) by finding an upper bound for the probability term \(\sum_{m=1\ 0^{d}}^{\infty}P^{m}_{\mathbf{x}\mathbf{z}}\) using the first entrance decomposition on \(C^{g}\setminus\{0^{d}\}\) while noting that \(\mathbf{z}\in C^{g}\setminus\{0^{d}\}\):

\[\sum_{m=1}^{\infty}\,_{0^{d}}P^{m}_{\mathbf{x}\mathbf{z}} =\sum_{m=1}^{\infty}\sum_{l=1}^{m}\sum_{\begin{subarray}{c}\mathbf{ u}\in C^{g}\setminus\{0^{d}\}\\ \mathbf{v}\notin C^{g}\end{subarray}}\,_{C^{g}}P^{l-1}_{\mathbf{x}\mathbf{v}}\,_{0^{d}}P^{m -l}_{\mathbf{u}\mathbf{z}}\] \[=\sum_{\mathbf{u}\in C^{g}\setminus\{0^{d}\}}\left(\sum_{l=0}^{\infty }\sum_{\mathbf{v}\notin C^{g}}\,_{\mathbf{x}\mathbf{v}}P^{l}_{\mathbf{v}\mathbf{u}}\right)\left( \sum_{m=0}^{\infty}\,_{0^{d}}P^{m}_{\mathbf{u}\mathbf{z}}\right)\] \[\leq\sum_{\mathbf{u}\in C^{g}\setminus\{0^{d}\}}\sum_{m=0}^{\infty}\, _{0^{d}}P^{m}_{\mathbf{u}\mathbf{z}}\] \[\leq\sum_{\mathbf{u}\in C^{g}\setminus\{0^{d}\}}\sum_{m=0}^{\infty} \mathbb{P}_{\mathbf{u}}(\tau_{0^{d}}>m)\] \[\leq|C^{g}|\,_{\mathbf{u}\in C^{g}\setminus\{0^{d}\}}\mathbb{E}_{\mathbf{u }}[\tau_{0^{d}}], \tag{47}\]

where the third line follows from the fact that \(\sum_{l=0}^{\infty}\sum_{\mathbf{v}\notin C^{g}\ \subsetneq C^{g}}P^{l}_{\mathbf{x}\mathbf{v}}P _{\mathbf{v}\mathbf{u}}\) is the probability of entrance to \(C^{g}\) through \(\mathbf{u}\in C^{g}\setminus\{0\}\), so it is less than \(1\). Irreducibility and positive recurrence combined with \(|C^{g}|<\infty\) imply that \(\max_{\mathbf{u}\in C^{g}\setminus\{0^{d}\}}\mathbb{E}_{\mathbf{u}}[\tau_{0^{d}}]<\infty\), which shows \(\sum_{m=0\ 0^{d}}^{\infty}P^{m}_{\mathbf{x}\mathbf{z}}\) is finite. Next,by induction we prove that for \(n\geq 1\) and \(\mathbf{z}\in C^{g}\setminus\{0^{d}\}\) we have

\[\sum_{\mathbf{y}\in\mathcal{X}}C^{g}P^{n}_{\mathbf{zy}}V^{g}(\mathbf{y})\leq(\gamma^{g})^{n-1 }\,b^{g}. \tag{48}\]

For \(n=1\), we have using Assumption 3 that

\[\sum_{\mathbf{y}\in\mathcal{X}}C^{g}P_{\mathbf{zy}}V^{g}(\mathbf{y})\leq\sum_{\mathbf{y}\in \mathcal{X}}P_{\mathbf{zy}}V^{g}(\mathbf{y})\leq b^{g}.\]

Assuming that (48) holds for \(n\), for \(n+1\) we have

\[\sum_{\mathbf{y}\in\mathcal{X}}C^{g}P^{n+1}_{\mathbf{zy}}V^{g}(\mathbf{y}) \leq\sum_{\begin{subarray}{c}\mathbf{y}\in\mathcal{X}\\ \mathbf{v}\in C^{g}\end{subarray}}C^{g}P^{n}_{\mathbf{zy}}P_{\mathbf{vy}}V^{g}(\mathbf{y})\leq \gamma^{g}\sum_{\mathbf{v}\notin C^{g}}C^{g}P^{n}_{\mathbf{zy}}V^{g}(\mathbf{v})\] (Using ( 44 ) \[\leq\gamma\sum_{\mathbf{v}\in\mathcal{X}}C^{g}P^{n}_{\mathbf{zy}}V^{g}(\bm {v})\leq\left(\gamma^{g}\right)^{n}b^{g},\] (By induction step)

so (48) is shown. We collect these bounds later on for our result on Term 2.

We now simplify the summation in (45). Similar to previous arguments, we will use induction for \(n\geq 1\) and show for all \(\mathbf{x}\in\mathcal{X}\)

\[\sum_{\mathbf{y}\in\mathcal{X}}C^{g}P^{n}_{\mathbf{zy}}V^{g}(\mathbf{y})\leq\left(\gamma^{ g}\right)^{n-1}\left(\gamma^{g}V^{g}(\mathbf{x})+b^{g}\right). \tag{49}\]

For \(n=1\), we have

\[\sum_{\mathbf{y}\in\mathcal{X}}C^{g}P_{\mathbf{zy}}V^{g}(\mathbf{y})\leq\sum_{\mathbf{y}\in \mathcal{X}}P_{\mathbf{zy}}V^{g}(\mathbf{y})\leq\gamma^{g}V^{g}(\mathbf{x})+b^{g}.\]

Assuming that (49) holds for \(n\), for \(n+1\) we have

\[\sum_{\mathbf{y}\in\mathcal{X}}C^{g}P^{n+1}_{\mathbf{zy}}V^{g}(\mathbf{y}) \leq\sum_{\mathbf{z}\notin C^{g}}C^{g}P^{n}_{\mathbf{zy}}\sum_{\mathbf{y}\in \mathcal{Y}}P_{\mathbf{zy}}V^{g}(\mathbf{y})\leq\gamma^{g}\sum_{\mathbf{z}\notin C^{g}}C^ {g}P^{n}_{\mathbf{zy}}V^{g}(\mathbf{z})\] \[\leq\gamma^{g}\sum_{\mathbf{z}\in\mathcal{X}}C^{g}P^{n}_{\mathbf{zy}}V^{g }(\mathbf{z})\leq\left(\gamma^{g}\right)^{n}\left(\gamma^{g}V^{g}(\mathbf{x})+b^{g} \right),\]

where the first and second inequalities follow from the definition of taboo probabilities and (44). Thus, (49) is proved. Lastly, for Term 2 in (46), we note

\[\sum_{n=1}^{\infty}\sum_{\mathbf{z}\in C^{g}\setminus\{0^{d}\}}0^{d}P ^{n}_{\mathbf{zy}}V^{g}(\mathbf{z}) \leq\max_{\mathbf{y}\in C^{g}\setminus\{0^{d}\}}V^{g}(\mathbf{y})\sum_{\bm {z}\in C^{g}\setminus\{0^{d}\}}\sum_{n=1}^{\infty}\sum_{0^{d}}P^{n}_{\mathbf{zy}}\] \[\leq b^{g}|C^{g}|^{2}\max_{\mathbf{u}\in C^{g}\setminus\{0^{d}\}} \mathbb{E}_{\mathbf{u}}[\tau_{0^{d}}]\] (From ( 47 )).

From the above equation, (47), (48), and (49), we bound \(\tilde{V}^{g}(\mathbf{x})\) as follows:

\[\tilde{V}^{g}(\mathbf{x})\] \[\leq V^{g}(\mathbf{x})+\left(\gamma^{g}V^{g}(\mathbf{x})+b^{g}\right)\sum _{n=1}^{\infty}\left(\gamma^{g}\right)^{n-1}+|C^{g}|^{2}b^{g}\max_{\mathbf{u}\in C ^{g}\setminus\{0^{d}\}}\mathbb{E}_{\mathbf{u}}[\tau_{0^{d}}]\left(1+\sum_{n=1}^{ \infty}\left(\gamma^{g}\right)^{n-1}\right)\] \[\leq\frac{V^{g}(\mathbf{x})}{1-\gamma^{g}}+\frac{3|C^{g}|^{2}b^{g}}{1 -\gamma^{g}}\max\left(1,\max_{\mathbf{u}\in C^{g}\setminus\{0^{d}\}}\mathbb{E}_{ \mathbf{u}}[\tau_{0^{d}}]\right)\] \[\leq V^{g}(\mathbf{x})\left(\frac{3b^{g}+1}{1-\gamma^{g}}\left(\ |C^{g}|^{2}\max\left(1,\max_{\mathbf{u}\in C^{g}\setminus\{0^{d}\}}\mathbb{E}_{ \mathbf{u}}[\tau_{0^{d}}]\right)\right)\right),\]

where the last line is due to \(V^{g}(\mathbf{x})\geq 1\). Taking

\[\tilde{b}^{g}:=\frac{3b^{g}+1}{1-\gamma^{g}}\left(\ |C^{g}|^{2}\max\left(1,\max_{\mathbf{u}\in C^{g}\setminus\{0^{d}\}}\mathbb{E}_{ \mathbf{u}}[\tau_{0^{d}}]\right)\right)>1,\]we have shown that

\[\tilde{V}^{g}(\mathbf{x})\leq\tilde{b}^{g}V^{g}(\mathbf{x}),\quad\mathbf{x}\in\mathcal{X}. \tag{50}\]

We now upper-bound \(\mathbb{P}_{0^{d}}(\tau_{0^{d}}>n)\) for all \(n\geq 1\) in an inductive manner, starting with \(\mathbb{P}_{0^{d}}(\tau_{0^{d}}>1)\). As a part of showing this, for every \(\mathbf{x}\neq 0^{d}\) we argue that for all \(n\geq 1\)

\[\mathbb{P}_{\mathbf{x}}(\tau_{0^{d}}>n)\leq\tilde{V}^{g}(\mathbf{x})\left(1-\frac{1}{ \tilde{b}^{g}}\right)^{n}. \tag{51}\]

First note that

\[\tilde{V}^{g}(\mathbf{x})\geq V^{g}(\mathbf{x})\geq 1. \tag{52}\]

Thus,

\[\mathbb{P}_{\mathbf{x}}(\tau_{0^{d}}>1) =\sum_{\mathbf{y}\in\mathcal{X}}\sum_{0^{d}}P_{\mathbf{x}\mathbf{y}}\leq\sum_ {\mathbf{y}\in\mathcal{X}}\,_{0^{d}}P_{\mathbf{x}\mathbf{y}}\tilde{V}^{g}(\mathbf{y})\] \[=\sum_{\mathbf{y}\in\mathcal{X}}\sum_{0^{d}}P_{\mathbf{x}\mathbf{y}}\sum_{n=0 }^{\infty}\sum_{\mathbf{z}\in\mathcal{X}}\,_{0^{d}}P_{\mathbf{y}\mathbf{z}}^{n}V^{g}(\mathbf{z })=\sum_{\mathbf{z}\in\mathcal{X}}\sum_{n=1}^{\infty}\,_{0^{d}}P_{\mathbf{x}\mathbf{z}}^{n }V^{g}(\mathbf{z}). \tag{53}\]

We now apply the bound in (50) to get

\[\mathbb{P}_{\mathbf{x}}(\tau_{0^{d}}>1)\leq\sum_{\mathbf{z}\in\mathcal{X}}\sum_{n=1}^{ \infty}\,_{0^{d}}P_{\mathbf{x}\mathbf{z}}^{n}V^{g}(\mathbf{z})=\tilde{V}^{g}(\mathbf{x})-V^{g} (\mathbf{x})\leq\tilde{V}^{g}(\mathbf{x})\left(1-\frac{1}{\tilde{b}^{g}}\right). \tag{54}\]

With the base of induction established, we assume the statement in (51) is true for \(n\), and show that it continues to hold for \(n+1\) as follows:

\[\mathbb{P}_{\mathbf{x}}(\tau_{0^{d}}>n+1) =\sum_{\mathbf{y}\neq 0^{d}}P_{\mathbf{x}\mathbf{y}}\mathbb{P}_{\mathbf{y}}(\tau_{0 ^{d}}>n)\] \[\leq\left(1-\frac{1}{\tilde{b}^{g}}\right)^{n}\sum_{\mathbf{y}\neq 0^{d }}P_{\mathbf{x}\mathbf{y}}\tilde{V}^{g}(\mathbf{y})\] \[\leq\tilde{V}^{g}(\mathbf{x})\left(1-\frac{1}{\tilde{b}^{g}}\right)^{ n+1},\]

where the final inequality uses the same arguments as in (53) and (54).

Finally, using the tail probabilities of hitting time of state \(0^{d}\) from any state \(\mathbf{x}\neq 0^{d}\), we bound the tail probability of the return time to state \(0^{d}\) (starting from \(0^{d}\)) as follows

\[\mathbb{P}_{0^{d}}(\tau_{0^{d}}>n+1) =\sum_{\mathbf{x}\neq 0^{d}}P_{0\mathbf{x}}\mathbb{P}_{\mathbf{x}}(\tau_{0^{ d}}>n)\leq\left(1-\frac{1}{\tilde{b}^{g}}\right)^{n}\sum_{\mathbf{x}\neq 0^{d}}P_{0 \mathbf{x}}\tilde{V}^{g}(\mathbf{x})\] \[\leq\tilde{b}^{g}\left(1-\frac{1}{\tilde{b}^{g}}\right)^{n}\sum_ {\mathbf{x}\neq 0^{d}}P_{0\mathbf{x}}V^{g}(\mathbf{x})\leq b^{g}\tilde{b}^{g}\left(1- \frac{1}{\tilde{b}^{g}}\right)^{n},\]

where the final inequality follows from the definition of \(b^{g}\), and we have

\[\tilde{\gamma}^{g}=1-\frac{1}{\tilde{b}^{g}},\text{ and }c^{g}=\frac{b^{g} \left(\tilde{b}^{g}\right)^{2}}{\tilde{b}^{g}-1},\]

and the proof is complete. 

## Appendix E Queueing model examples

### Model 1: Two-server queueing system with a common buffer

We consider a continuous-time queueing system with two heterogeneous servers with unknown service rate vector \(\mathbf{\theta}^{*}=(\theta_{1}^{*},\theta_{2}^{*})\) and a common infinite buffer, shown in Figure 1(a). Arrivals to the system are according to a Poisson process with rate \(\lambda\) and service times are exponentially distributed with parameter \(\theta_{i}^{*}\), depending on the assigned server. The service rate vector \(\mathbf{\theta}^{*}\) is sampled from the prior distribution \(\nu_{0}\) defined on the space \(\Theta\) given as

\[\Theta=\left\{\left(\theta_{1},\theta_{2}\right)\in\mathbb{R}_{+}^{2}:\frac{ \lambda}{\theta_{1}+\theta_{2}}\leq\frac{1-\delta}{1+\delta},1\leq\frac{\theta _{1}}{\theta_{2}}\leq R\right\}, \tag{55}\]

for fixed \(\delta\in(0,0.5)\) and \(R\geq 1\). Note that for any \((\theta_{1},\theta_{2})\in\Theta\), we have \(\theta_{1}\geq\theta_{2}\) and the stability requirement \(\lambda<\theta_{1}+\theta_{2}\) holds. The countable state space \(\mathcal{X}\) is defined as \(\mathcal{X}=\left\{\mathbf{x}=\left(x_{0},x_{1},x_{2}\right):x_{0}\in\mathbb{N} \cup\left\{0\right\},x_{1},x_{2}\in\left\{0,1\right\}\right\}\), in which \(x_{0}\) is the length of the queue, and \(x_{i},i=1,2\) is equal to 1 if server \(i\) is busy serving a job. At each time instance \(r\in\mathbb{R}_{+}\), the dispatcher can assign jobs from the (non-empty) buffer to an available server. Thus, the action space \(\mathcal{A}\) is equal to

\[\mathcal{A}=\{h,b,1,2\},\]

where \(h\) indicates no action, \(b\) sends a job to both of the servers, and \(i=1,2\) assigns a job to server \(i\). The goal of the dispatcher is to minimize the expected sojourn time of customers, which by Little's law [52] is equivalent to minimizing the average number of customers in the system, or

\[\inf_{\pi\in\Pi}\limsup_{T\rightarrow\infty}\frac{1}{T}\int_{0}^{T}\|\mathbf{X}(r )\|_{1}\,dr, \tag{56}\]

where \(\mathbf{X}(r)\) is the state of the system at time \(r\in\mathbb{R}_{+}\), immediately after the arrival/departure and just before the action is taken. In [38], it is argued that from uniformization [39] and sampling the continuous-time Markov process at a rate of \(\lambda+\theta_{1}^{*}+\theta_{2}^{*}\), a discrete-time Markov chain is obtained, which converts the original continuous-time problem shown in (56) to an equivalent discrete-time problem as below:

\[\inf_{\pi\in\Pi}\limsup_{T\rightarrow\infty}\frac{1}{T}\int_{0}^{T}\|\mathbf{X}(r )\|_{1}\,dr=\inf_{\pi\in\Pi}\limsup_{T\rightarrow\infty}\frac{1}{T}\sum_{i=0} ^{T-1}\|\mathbf{X}(i)\|_{1}. \tag{57}\]

To obtain a uniform sampling rate of \(\lambda+\theta_{1}^{*}+\theta_{2}^{*}\), the continuous-time system is sampled at arrivals, real and dummy customer departures. In [38], it is further shown that the optimal policy that achieves the infimum in (57) is a threshold policy \(\pi_{t}\) with the optimal finite threshold \(t(\theta)\in\mathbb{N}\), with the policy defined as below:

\[\pi_{t}(\mathbf{x})=\begin{cases}\text{h if }\{x_{0}=0\}\text{ or }\{\|\mathbf{x}\|_{1} \leq t,x_{1}=1\}\text{ or }\{x_{1}=x_{2}=1\}\\ 1\text{ if }\{x_{0}\geq 1,x_{1}=0\}\\ 2\text{ if }\{x_{0}\geq 1,\|\mathbf{x}\|_{1}\geq t+1,x_{1}=1,x_{2}=0\};\end{cases}\]

note that action \(b\) is not used. Policy \(\pi_{t}\) assigns a job to the faster (first) server whenever there is a job waiting in the queue and the first server is available. In contrast, \(\pi_{t}\) dispatches a job to the second server only if the number of jobs in the system are greater than threshold \(t\) and the second server is available. If neither of these conditions hold, no action or \(h\) is taken. Consequently, we can restrict the set of all policies \(\Pi\) in (57) to the set \(\Pi_{t}\), which is the set of all possible threshold policies corresponding to some \(t\in\mathbb{N}\).

In the rest of this subsection, our aim is to show that Assumptions 1-5 are satisfied for the discrete-time Markov process obtained by uniformization of the described queueing system and hence, conclude that Algorithm 1 can be used to learn the unknown service rate vector \(\mathbf{\theta}^{*}\) with the expected regret of order \(\tilde{O}(\sqrt{T})\).

**Assumption 1**.: Cost function is given as \(c(\mathbf{x},a)=\|\mathbf{x}\|_{1}\), which satisfies Assumption 1 with \(f_{c}(\mathbf{x})=x_{0}+x_{1}+x_{2}\) and \(K=r=1\).

**Assumption 2**.: For any state-action pair \((\mathbf{x},a)\) and \(\theta\in\Theta\), we have \(P_{\theta}(A(\mathbf{x});\mathbf{x},a)=0\) where \(A(\mathbf{x})=\left\{\mathbf{y}\in\mathcal{X}:\|\mathbf{y}\|_{1}-\|\mathbf{x}\|_{1}|>1\right\}\); thus, Assumption 2 holds with \(h=1\).

**Assumption 3**.: Consider a queueing system with parameter \(\theta\) following threshold policy \(\pi_{t}\) for some \(t\in\mathbb{N}\). The uniformized discrete-time Markov chain is irreducible and aperiodic on a subset of state space given as \(\mathcal{X}_{t}=\mathcal{X}\setminus\left(\{(i,0,0):i\geq\min(t,2)\}\cup\{(0, 1,1)\}\right)\). In [38], it is proved that for every \(t\), the chain consists of a single positive recurrent class and the corresponding average number of customers, depicted by \(J^{t}(\theta)\), is calculated. Moreover, it is shown that for every \(\theta\in\Theta\) the optimal threshold \(t(\theta)\) can be numerically found as the smallest \(i\in\mathbb{N}\) for which \(J^{i}(\theta)<J^{i+1}(\theta)\). Define the set \(T^{*}\) as the set of all optimal thresholds corresponding to at least one \(\theta\in\Theta\), or

\[T^{*}=\{t:t=t(\theta)\text{ for }\theta\in\Theta\}.\]

**Remark 7**.: _There is a discrepancy between the class of MDPs defined in this section and in Section 2, as in the former the MDPs are not irreducible in the whole state space \(\mathcal{X}\). Specifically, for every Markov process generated by a queueing system with parameter \(\theta\) following threshold policy \(\pi_{t}\), irreducibility holds on \(\mathcal{X}_{t}\subset\mathcal{X}\). Nevertheless, the results of Section 4 are valid as starting from state \((0,0)\), the visited states are positive recurrent; see Remark 4._

In the following proposition, we verify the geometric ergodicity of the discrete-time chain governed by any parameter \(\theta\in\Theta\) and obtained by following any threshold policy \(\pi_{t}\) for \(t\in T^{*}\); proof is given in Appendix F.1.

**Proposition 1**.: _The discrete-time Markov process obtained from the queueing system governed by parameter \(\theta=(\theta_{1},\theta_{2})\in\Theta\) and following threshold policy \(\pi_{t}\) for some \(t\in T^{*}\) is geometrically ergodic. Equivalently, the following holds_

\[\Delta V^{g}_{\theta,t}(\mathbf{x})\leq-\left(1-\gamma^{g}_{\theta,t} \right)V^{g}_{\theta,t}(\mathbf{x})+b^{g}_{\theta,t}\mathbb{I}_{C^{g}_{\theta,t}}( \mathbf{x}),\quad\mathbf{x}\in\mathcal{X}_{t},\]

_for_

\[V^{g}_{\theta,t}(\mathbf{x})=\exp(-\log(1-\delta)\|\mathbf{x}\|_{1}),\] \[C^{g}_{\theta,t}=\left\{(x_{0},x_{1},0):x_{0}<t\right\}\cup \left\{(0,0,1)\right\}, \tag{58}\] \[b^{g}_{\theta,t}=\max_{\mathbf{x}\in C^{g}_{\theta,t}}\exp\left(- \log(1-\delta)\left(\|\mathbf{x}\|_{1}+1\right)\right),\] (59) \[\gamma^{g}_{\theta,t}=\frac{1}{2}-\frac{1}{2(\theta_{1}+\theta_{2 }+\lambda)}\left((\theta_{1}+\theta_{2})(1-\delta)+\lambda\left(1-\delta\right) ^{-1}\right). \tag{60}\]

Having described all the terms explicitly, we verify the rest of the conditions of Assumption 3, which lead to uniform (over model class) upper-bounds on the moments of hitting time to \(0^{d}\) as follows:

1. From (60), \(\sup_{\theta\in\Theta,t\in T^{*}}\gamma^{g}_{\theta,t}\leq 1/2<1\).
2. From (58), we can see that state \((0,0)\) belongs to \(C^{g}_{\theta,t}\) for all \(\theta\in\Theta\) and \(t\in T^{*}\). In order for \(C^{g}_{*}=\cup_{\theta\in\Theta,t\in T^{*}}C^{g}_{\theta,t}\) to be a finite set, the supremum of the optimal threshold \(t(\theta)\) over \(\Theta\) should be finite. In [37] with service rate vector \((\theta_{1},\theta_{2})\), it is shown that the optimal threshold is bounded above by \(\sqrt{2}\theta_{1}/\theta_{2}\), which further gives \[t(\theta)\leq\sqrt{2}\frac{\theta_{1}}{\theta_{2}}\leq\sqrt{2}R.\] (61) Thus, \(\sup_{\theta\in\Theta}t(\theta)\leq\sqrt{2}R\), which is finite. To confirm a uniform upper bound for \(b^{g}_{\theta,t}\), we note that from (59), \[\sup_{\theta\in\Theta,t\in T^{*}}b^{g}_{\theta,t}=\frac{2-\delta}{1-\delta} \max_{x\in C^{\frac{g}{2}}_{t}}\exp(-\log(1-\delta)\|\mathbf{x}\|_{1}),\] which is finite as \(|C^{g}_{*}|<\infty\).

**Assumption 4**.: To find an upper bound on the second moment of hitting times, we verify Assumption 4 and show that there exists a finite set \(C^{p}_{\theta,t}\), constants \(\beta^{p}_{\theta,t}\), \(b^{p}_{\theta,t}>0\), \(r/(r+1)\leq\alpha^{p}_{\theta,t}<1\), and a function \(V^{p}_{\theta,t}:\mathcal{X}_{t}\rightarrow[1,+\infty)\) satisfying

\[\Delta V^{p}_{\theta,t}(\mathbf{x})\leq-\beta^{p}_{\theta,t}\left(V^{p}_{\theta,t} (\mathbf{x})\right)^{\alpha^{p}_{\theta,t}}+b^{p}_{\theta,t}\mathbb{I}_{C^{p}_{ \theta,t}}(\mathbf{x}),\quad\mathbf{x}\in\mathcal{X}_{t}. \tag{62}\]

**Proposition 2**.: _The discrete-time Markov process obtained from the queueing system governed by parameter \(\theta=(\theta_{1},\theta_{2})\in\Theta\) and following threshold policy \(\pi_{t}\) for some \(t\in T^{*}\) is polynomiallyergodic. This is true because (62) holds for_

\[V_{\theta,t}^{p}(\mathbf{x})=\|\mathbf{x}\|_{1}^{2}, \tag{63}\] \[C_{\theta,t}^{p}=\left\{(x_{0},x_{1},0):x_{0}<t\right\}\cup\left\{ (x_{0},x_{1},x_{2}):x_{0}<\frac{2\lambda}{\theta_{1}+\theta_{2}-\lambda},x_{1}+ x_{2}\geq 1\right\},\] (64) \[b_{\theta,t}^{p}=\max_{\mathbf{x}\in C_{\theta,t}^{p}}(\|\mathbf{x}\|_{1} +1)^{2}),\] (65) \[\beta_{\theta,t}^{p}=1-\frac{2\lambda}{\theta_{1}+\theta_{2}+ \lambda},\] (66) \[\alpha_{\theta,t}^{p}=\frac{1}{2}. \tag{67}\]

Proof of Proposition 2 is given in Appendix F.2. We define the normalized rates as \(\tilde{\lambda}=\frac{\lambda}{\lambda+\theta_{1}+\theta_{2}}\) and \(\tilde{\theta}_{i}=\frac{\theta_{i}}{\lambda+\theta_{i}+\theta_{2}}\), for \(i=1,2\). From the choice of parameter space \(\Theta\), we have \(\tilde{\lambda}\leq 0.5-0.5\delta\), \(\tilde{\theta}_{1}+\tilde{\theta}_{2}\geq 0.5+0.5\delta\), and \(\tilde{\theta}_{1}\geq 0.25+0.25\delta\). We verify the remaining conditions of Assumption 4 as follows:

1. From (63), the first condition holds with \(r_{*}^{p}=2\) and \(s_{*}^{p}=2\).
2. From (64), we can see that state \((0,0)\) belongs to \(C_{\theta,t}^{p}\) for all \(\theta\in\Theta\) and \(t\in T^{*}\). Furthermore, \[\sup_{\theta\in\Theta,t\in T^{*}}\frac{2\lambda}{\theta_{1}+\theta_{2}-\lambda }\leq\frac{1-\delta}{\delta},\] which follows from the stability condition \(\tilde{\lambda}\leq 0.5-0.5\delta\). Thus, from the definition of \(C_{\theta,t}^{p}\) in (64), and the fact that \(\sup_{\theta\in\Theta}t(\theta)\leq\sqrt{2}R\) as argued in in (61), \(C_{*}^{p}=\cup_{\theta\in\Theta,t\in T^{*}}C_{\theta,t}^{p}\) is a finite set. We also note that \(\sup_{\theta\in\Theta,t\in T^{*}}b_{\theta,t}^{p}\) is finite as \(|C_{*}^{p}|<\infty\). It remains to show that \(\inf_{\theta\in\Theta,t\in T^{*}}\beta_{\theta,t}^{p}\) is positive, which is equivalent to verifying that \(\sup_{\theta\in\Theta,t\in T^{*}}\tilde{\lambda}<1/2\), which follows from the stability condition \(\tilde{\lambda}\leq 0.5-0.5\delta\).
3. We need to show that \(K_{\theta,t}(\mathbf{x}):=\sum_{n=0}^{\infty}2^{-n-2}\left(P_{\theta}^{t}\right)^ {n}(\mathbf{x},0^{d})\) is strictly bounded away from zero. We notice that from any non-zero state \(\mathbf{x}\), the queueing system hits \(0^{d}\) in \(\|\mathbf{x}\|_{1}\) transitions only if all transitions are real departures. Hence, \[K_{\theta,t}(\mathbf{x}) \geq 2^{-\|\mathbf{x}\|_{1}-2}\left(P_{\theta}^{t}\right)^{\|\mathbf{x}\|_{ 1}}(\mathbf{x},0^{d})\] \[\geq 2^{-\|\mathbf{x}\|_{1}-2}\left(\tilde{\theta}_{1}\right)^{\|\mathbf{ x}\|_{1}}\left(\tilde{\theta}_{2}\right)^{\|\mathbf{x}\|_{1}}\] \[\geq 2^{-\|\mathbf{x}\|_{1}-2}R^{-\|\mathbf{x}\|_{1}}\left(\tilde{\theta}_ {1}\right)^{2\|\mathbf{x}\|_{1}}\] \[\geq 2^{-\|\mathbf{x}\|_{1}-2}R^{-\|\mathbf{x}\|_{1}}\left(\frac{1}{4}+ \frac{\delta}{4}\right)^{2\|\mathbf{x}\|_{1}},\] where the third and fourth inequalities follow from the definition of \(\Theta\) in (55). Thus, the infimum of \(K_{\theta,t}(\mathbf{x})\) over the finite set \(C_{*}^{p}\) and sets \(\Theta\) and \(T^{*}\) is strictly greater than zero.

**Assumption 5**.: We finally verify Assumption 5, which asserts that \(\sup_{\theta\in\Theta}J(\theta)\) is finite. We have

\[J(\theta)=\mathbb{E}_{\mathbf{X}\sim\mu\theta,\epsilon(\theta)}\left[c(\mathbf{X}) \right]=\mathbb{E}_{\mathbf{X}\sim\mu\theta,\epsilon(\theta)}\left[\|\mathbf{X}\|_{1} \right]=\mathbb{E}_{\mathbf{X}\sim\mu\theta,\epsilon(\theta)}\left[\sqrt{V_{\theta,t(\theta)}^{p}(\mathbf{X})}\right],\]

where \(\mu_{\theta,t(\theta)}\) is the stationary distribution of the discrete-time process governed by parameter \(\theta\) and following the optimal policy according to \(\theta\). From (62) and [43, Theorem 14.3.7],

\[\mu_{\theta,t(\theta)}\left(\sqrt{V_{\theta,t(\theta)}^{p}(\mathbf{X})}\right)\leq \frac{b_{*}^{p}}{\beta_{*}^{p}},\]

which is finite from the the previously verified assumption. Consequently,

\[\sup_{\theta\in\Theta}J(\theta)\leq\frac{b_{*}^{p}}{\beta_{*}^{p}}<\infty.\]

### Model 2: Two heterogeneous parallel queues

We consider two parallel queues with infinite buffers, each with its own single server, and unknown service rate vector \(\mathbf{\theta}^{*}=(\theta_{1}^{*},\theta_{2}^{*})\), shown in Figure 2b. The service rate vector \(\mathbf{\theta}^{*}\) is sampled from the prior distribution \(\nu_{0}\) defined on the space \(\Theta\) given as

\[\Theta=\left\{(\theta_{1},\theta_{2})\in\mathbb{R}_{+}^{2}:\frac{\lambda}{ \theta_{1}+\theta_{2}}\leq\frac{1-\delta}{1+\delta},1\leq\frac{\theta_{1}}{ \theta_{2}}\leq R\right\}, \tag{68}\]

for fixed \(\delta\in(0,0.5)\) and \(R\geq 1\), which ensures the stability of the queueing system. Consider the discrete-time MDP \((\mathcal{X},\mathcal{A},P_{\mathbf{\theta}^{*}},c)\) obtained by sampling the queueing system at the Poisson arrival sequence. The countably infinite state space \(\mathcal{X}\) is defined as below

\[\mathcal{X}=\left\{\mathbf{x}=(x_{1},x_{2}):x_{i}\in\mathbb{N}\cup\{0\}\right\},\]

where the state of the system is the number of jobs in the server-queue pair \(i\) just before an arrival. Furthermore, the action space \(\mathcal{A}\) is equal to

\[\mathcal{A}=\{1,2\},\]

where action \(i\in\mathcal{A}\) indicates the arrival dispatched to queue \(i\). The unbounded cost function \(c:\mathcal{X}\times\mathcal{A}\rightarrow\mathbb{N}\cup\{0\}\) is defined as the total number of jobs in the queueing system, i.e., \(c(\mathbf{x},a)=\|\mathbf{x}\|_{1}\). For every \(\omega\in\mathbb{R}_{+}\), we define policy \(\pi_{\omega}:\mathcal{X}\rightarrow\mathcal{A}\), which routes the arrival according to the weighted queue lengths, as

\[\pi_{\omega}(\mathbf{x})=\arg\min\left(1+x_{1},\omega\left(1+x_{2}\right)\right),\]

where the tie is broken in favor of the first server. We also define policy class \(\tilde{\Pi}\) as the set of policies \(\pi_{\omega}\) such that \(\omega\) belongs to a compact interval; in other words,

\[\tilde{\Pi}=\left\{\pi_{\omega};\,\omega\in\left[\frac{1}{c_{R}R},c_{R}R\right] \right\},\]

where \(R\) is defined in (68) and \(c_{R}\geq 1\). We aim to minimize the infinite-horizon average cost in the policy class \(\tilde{\Pi}\), that is,

\[J(\theta)=\inf_{\pi\in\tilde{\Pi}}\limsup_{T\rightarrow\infty}\frac{1}{T}\, \mathbb{E}\left[\sum_{t=1}^{T}c\left(\mathbf{X}\left(t\right),A\left(t\right) \right)\right], \tag{69}\]

where \(\mathbf{X}(t)=(X_{1}(t),X_{2}(t))\) is the occupancy vector of the queueing system just before arrival \(t\). Even with the controlled Markov process transition kernel fully-specified (by the values of the arrival rate and the two service rates), the optimal policy1 that satisfies (69) in policy class \(\tilde{\Pi}\) is not known except when \(\theta_{1}=\theta_{2}\) where the optimal value is \(\omega=1\), and so, to learn it, we will use Proximal Policy Optimization for countable state-space controlled Markov processes as developed in [18]. Note that [18] requires full knowledge of the controlled Markov process, which holds in our learning scheme since we use the parameters sampled from the posterior for determining the policy at the beginning of each episode. Furthermore, for each policy in the set of applicable policies \(\tilde{\Pi}\), [18] also requires that the resulting Markov process be geometrically ergodic, which we will establish below.

Footnote 1: When \(\theta_{1}=\theta_{2}\), then the policy with \(\omega=1\) (Join-the-Shortest-Queue) is the optimal policy [19] for the underling MDP.

**Proposition 3**.: _The discrete-time Markov process obtained from the queueing system governed by parameter \(\theta=(\theta_{1},\theta_{2})\in\Theta\) and following policy \(\pi_{\omega}\in\tilde{\Pi}\) is geometrically ergodic. Equivalently, the following holds_

\[\Delta V^{g}_{\theta,\omega}(\mathbf{x})\leq-\left(1-\gamma^{g}_{\theta,\omega} \right)V^{g}_{\theta,\omega}(\mathbf{x})+b^{g}_{\theta,\omega}\mathbb{I}_{C^{g}_{ \theta,\omega}}(\mathbf{x}),\quad\mathbf{x}\in\mathcal{X}, \tag{70}\]\[V_{\theta,\omega}^{g}(\mathbf{x})=\frac{\omega}{\omega+1}\exp\left(a_{ \theta,\omega}^{g}\frac{x_{1}+1}{\omega}\right)+\frac{1}{\omega+1}\exp\left(a_{ \theta,\omega}^{g}\left(x_{2}+1\right)\right),\] \[a_{\theta,\omega}^{g}=\min\left(\omega\log(1+\delta),\log(1+ \delta),\omega\log\frac{1-0.5\delta}{1-\delta},\log\frac{1-0.5\delta}{1-\delta },\frac{\delta(1-\delta^{2})}{4c_{R}R(1-0.5\delta)}\right), \tag{71}\] \[C_{\theta,\omega}^{g}=\left\{(x_{1},x_{2})\in\mathcal{X}:x_{i} \leq\max\left(x_{i,\theta,\omega}^{g_{j}},0\right),i,j=1,2\right\},\] (72) \[b_{\theta,\omega}^{g}=\max_{\mathbf{x}\in C_{\theta,\omega}^{g}}\left( \frac{2\omega}{\omega+1}\exp\left(a_{\theta,\omega}^{g}\frac{x_{1}+2}{\omega} \right)+\frac{2}{\omega+1}\exp\left(a_{\theta,\omega}^{g}\left(x_{2}+2\right) \right)\right),\] (73) \[\gamma_{\theta,\omega}^{g}=\frac{1}{2}+\frac{1}{2}\max\left( \zeta_{1,\theta,\omega},\zeta_{2,\theta,\omega},\frac{\zeta_{1,\theta,\omega} \omega}{1+\omega}\exp\left(\frac{a_{\theta,\omega}^{g}}{\omega}\right)+\frac{ \zeta_{2,\theta,\omega}}{1+\omega},\frac{\zeta_{1,\theta,\omega}\omega}{1+ \omega}+\frac{\zeta_{2,\theta,\omega}}{1+\omega}\exp\left(a_{\theta,\omega}^{g} \right)\right), \tag{74}\]

_and problem-dependent constants \(x_{i,\theta,\omega}^{g_{j}}\) and \(\zeta_{i,\theta,\omega}\) for \(i,j=1,2\)._

Proof of Proposition 3 is given in Appendix F.3. In the rest of this subsection, our aim is to show that Assumptions 1-5 are satisfied for the discrete-time MDP and conclude that Algorithm 1 can be used to learn the unknown service rate vector \(\mathbf{\theta}^{*}\) with expected regret of order \(\tilde{O}(\sqrt{T})\).

**Assumption 1.** Cost function is given as \(c(\mathbf{x},a)=\|\mathbf{x}\|_{1}\), which satisfies Assumption 1 with \(f_{c}(\mathbf{x})=x_{0}+x_{1}+x_{2}\) and \(K=r=1\).

**Assumption 2.** For any state-action pair \((\mathbf{x},a)\) and \(\theta\in\Theta\), we have \(P_{\theta}(A(\mathbf{x});\mathbf{x},a)=0\) where \(A(\mathbf{x})=\left\{\mathbf{y}\in\mathcal{X}:\left\|\mathbf{y}\right\|_{1}-\left\|\mathbf{x} \right\|_{1}>1\right\}\); thus, the MDP is skip-free to the right with \(h=1\). Moreover, from any \((\mathbf{x},a)\), the finite set \(\left\{\mathbf{y}\in\mathcal{X}:\left\|\mathbf{y}\right\|_{1}\leq\left\|\mathbf{x}\right\| _{1}+1\right\}\) is only accessible in one step; thus, Assumption 2 holds.

**Assumption 3.** In Proposition 3, we verified the geometric ergodicity of the discrete-time chain governed by parameter \(\theta=(\theta_{1},\theta_{2})\in\Theta\) and following policy \(\pi_{\omega}\in\tilde{\Pi}\) and thus, it only remains to verify the uniform model conditions. We define the normalized rates as \(\tilde{\lambda}=\frac{\lambda}{\lambda+\theta_{1}+\theta_{2}}\) and \(\tilde{\theta}_{i}=\frac{\theta_{i}}{\lambda+\theta_{1}+\theta_{2}}\), for \(i=1,2\). From the choice of parameter space \(\Theta\), we have \(\tilde{\lambda}\leq 0.5-0.5\delta\), \(\tilde{\theta}_{1}+\tilde{\theta}_{2}\geq 0.5+0.5\delta\), and \(\tilde{\theta}_{1}\geq 0.25+0.25\delta\).

1. We first argue that \(\zeta_{1,\theta,\omega}\) is bounded away from \(1\) as follows \[1-\zeta_{1,\theta,\omega} =1-\frac{\frac{\lambda}{\theta_{1}+\lambda}}{1-\exp\left(-\frac{a _{\theta,\omega}^{g}}{\omega}\right)\frac{\theta_{1}}{\theta_{1}+\lambda}}= \frac{\frac{\theta_{1}}{\theta_{1}+\lambda}\left(1-\exp\left(-\frac{a_{\theta, \omega}^{g}}{\omega}\right)\right)}{1-\exp\left(-\frac{a_{\theta,\omega}^{g}} {\omega}\right)\frac{\theta_{1}}{\theta_{1}+\lambda}}\] \[\geq\frac{\theta_{1}}{\theta_{1}+\lambda}\left(1-\exp\left(-\frac{ a_{\theta,(c_{R}R)-1}^{g}}{c_{R}R}\right)\right)>\tilde{\theta}_{1}\left(1-\exp \left(-\frac{a_{\theta,(c_{R}R)-1}^{g}}{c_{R}R}\right)\right)\] \[>(0.25+0.25\delta)\left(1-\exp\left(-\frac{a_{\theta,(c_{R}R)-1 }^{g}}{c_{R}R}\right)\right),\] where the first line follows from the definition of \(\zeta_{1,\theta,\omega}\) in Appendix F.3, the second line from (71) and the definition of policy class \(\tilde{\Pi}\). As \(a_{\theta,\omega}^{g}\) does not depend on \(\theta\), \(\sup_{\theta\in\Theta,\omega\in\left[\frac{\pi}{2R},c_{R}R\right]}\zeta_{1, \theta,\omega}<1\). Furthermore, by similar arguments it can be shown that \(\zeta_{2,\theta,\omega}\) is bounded away from \(1\). We next argue that \(\frac{\zeta_{1,\theta,\omega}\omega}{1+\omega}\exp\left(\frac{a_{\theta,\omega}^ {g}}{\omega}\right)+\frac{\zeta_{2,\theta,\omega}}{1+\omega}\) is bounded away from \(1\) using an upper bound found in Appendix F.3 as below, \[1-\frac{\zeta_{1,\theta,\omega}\omega}{1+\omega}\exp\left(\frac{a_{ \theta,\omega}^{g}}{\omega}\right)-\frac{\zeta_{2,\theta,\omega}}{1+\omega}\] \[\geq 1-\frac{\frac{\lambda}{1+\omega}\left(\omega+a_{\theta,\omega} ^{g}\zeta_{4}\right)}{\lambda+\frac{\theta_{1}a_{\theta,\omega}^{g}\zeta_{3}}{ \omega}}-\frac{\frac{\lambda}{1+\omega}}{\lambda+\theta_{2}a_{\theta,\omega}^{g }\zeta_{3}}\] \[=\frac{a_{\theta,\omega}^{g}\left(-a_{\theta,\omega}^{g}\zeta_{3} \theta_{2}\left(\lambda\zeta_{4}-\frac{\zeta_{2}\theta_{1}\left(1+\omega\right) }{\omega}\right)+\lambda\zeta_{3}\left(\theta_{1}+\theta_{2}\right)-\lambda^{2 }\zeta_{4}\right)}{(1+\omega)(\lambda+\theta_{1}a_{\theta,\omega}^{g}\zeta_{3} \omega^{-1})(\lambda+\theta_{2}a_{\theta,\omega}^{g}\zeta_{3})}\] \[>\frac{(\zeta_{3}a_{\theta,\omega}^{g})^{2}\;\tilde{\theta}_{1} \tilde{\theta}_{2}}{\omega(\tilde{\lambda}+\tilde{\theta}_{1}a_{\theta,\omega} ^{g}\zeta_{3}\omega^{-1})(\tilde{\lambda}+\tilde{\theta}_{2}a_{\theta,\omega}^ {g}\zeta_{3})}\] \[>\frac{(\zeta_{3}a_{\theta,(cR)}^{g})^{2})(0.25+0.25\delta)^{2}}{c _{R}R^{2}(1+c_{R}R\zeta_{3}a_{\theta,c_{R}R}^{g})^{2}},\] (75) where \(\zeta_{3}=(1+\delta)^{-1}\), \(\zeta_{4}=\frac{1-0.5\delta}{1-\delta}\), and we have used the arguments of Appendix F.3 and the definition of \(\Theta\). Using a similar argument, we can show that \(\frac{\zeta_{1,\theta,\omega}\omega}{1+\omega}+\frac{\zeta_{2,\theta,\omega}}{ 1+\omega}\exp\left(a_{\theta,\omega}^{g}\right)\) is bounded away from one, and finally, we conclude that \(\sup_{\theta\in\Theta,\omega\in[\frac{1}{c_{R}R},c_{R}R]}\gamma_{\theta,\omega }^{g}<1\).
2. From (72), we can see that state \((0,0)\) belongs to \(C_{\theta,\omega}^{g}\) for all \(\theta\in\Theta\) and \(\omega\in[\frac{1}{c_{R}R},c_{R}R]\). In order for \(C_{\theta}^{g}\) to be a finite set, the supremum of \(x_{i,\theta,\omega}^{g_{j}}\) over \(\Theta\) and \(\tilde{\Pi}\) should be finite. From the definition of \(x_{1,\theta,\omega}^{g_{1}}\) in Appendix F.3, \[x_{1,\theta,\omega}^{g_{1}} =\frac{\omega}{a_{\theta,\omega}^{g}}\log\frac{(c_{R}R+1)\exp(c_ {R}Ra_{\theta,\omega}^{g})}{(\omega+1)\gamma_{\theta,\omega}^{g}-\omega\zeta_ {1,\theta,\omega}\exp\left(\frac{a_{\theta,\omega}^{g}}{\omega}\right)-\zeta_ {2,\theta,\omega}}\] \[\leq\frac{c_{R}R}{a_{\theta,(c_{R}R)^{-1}}^{g}}\log\frac{(c_{R}R+ 1)\exp(c_{R}Ra_{\theta,c_{R}R}^{g})}{(\omega+1)\gamma_{\theta,\omega}^{g}- \omega\zeta_{1,\theta,\omega}\exp\left(\frac{a_{\theta,\omega}^{g}}{\omega} \right)-\zeta_{2,\theta,\omega}},\] and we can derive a lower bound for the denominator from (75). Similarly, we can show that \(\sup_{\theta\in\Theta,\omega\in[\frac{1}{c_{R}R},c_{R}R]}x_{2,\theta,\omega}^ {g_{2}}\) is finite. We next find a uniform upper bound for \(x_{2,\theta,\omega}^{g_{1}}\) from Appendix F.3, \[x_{2,\theta,\omega}^{g_{1}}\] \[=\frac{1}{a_{\theta,\omega}^{g}}\log\frac{(c_{R}R+1)\exp(c_{R} Ra_{\theta,\omega}^{g})+\omega\exp\left(a_{\theta,\omega}^{g}\frac{x_{1,\theta, \omega}^{g_{1}}+1}{\omega}\right)\left(\zeta_{1,\theta,\omega}\exp\left(\frac {a_{\theta,\omega}^{g}}{\omega}\right)-\gamma_{\theta,\omega}^{g}\right)}{ \gamma_{\theta,\omega}^{g}-\zeta_{2,\theta,\omega}}\] \[\leq\frac{1}{a_{\theta,(c_{R}R)^{-1}}^{g}}\log\frac{(2c_{R}R+1) \exp\left(c_{R}Ra_{\theta,c_{R}R}^{g}\left(x_{1,\theta,\omega}^{g_{1}}+2\right) \right)}{1-\gamma_{\theta,\omega}^{g}},\] which is uniformly bounded as \(\gamma_{\theta,\omega}^{g}\) is uniformly bounded away from 1 and the second line follows from (74) and the fact that \(\gamma_{\theta,\omega}^{g}-\zeta_{2,\theta,\omega}\geq 1-\gamma_{\theta,\omega}^{g}\). Arguments verifying the finiteness of the supremum of \(x_{1,\theta,\omega}^{g_{2}}\) follow similarly, and we conclude that \(|C_{*}^{g}|<\infty\). To confirm a uniform upper bound for \(b_{\theta,\omega}^{g}\), we note that from (73), \[\sup_{\theta\in\Theta,\omega\in[\frac{1}{c_{R}R},c_{R}]}b_{\theta,\omega}^{g} \leq\max_{x\in C_{*}^{g}}\left(2\exp\left(c_{R}Ra_{\theta,c_{R}R}^{g}(x_{1}+2) \right)+2\exp\left(a_{\theta,c_{R}R}^{g}\left(x_{2}+2\right)\right)\right),\] which is finite as \(a_{\theta,c_{R}R}^{g}\) is independent of the choice of \(\theta\) and \(|C_{*}^{g}|<\infty\).

**Assumption 4**.: We next verify Assumption 4 and show that there exists a finite set \(C_{\theta,\omega}^{p}\), constants \(\beta_{\theta,\omega}^{p}\), \(b_{\theta,\omega}^{p}>0\), \(r/(r+1)\leq\alpha_{\theta,\omega}^{p}<1\), and a function \(V_{\theta,\omega}^{p}:\mathcal{X}\rightarrow[1,+\infty)\) satisfying

\[\Delta V_{\theta,\omega}^{p}(\boldsymbol{x})\leq-\beta_{\theta,\omega}^{p} \left(V_{\theta,\omega}^{p}(\boldsymbol{x})\right)^{\alpha_{\theta,\omega}^{p}}+b _{\theta,\tau}^{p}\mathbb{L}_{C_{\theta,\omega}^{p}}(\boldsymbol{x}),\quad \boldsymbol{x}\in\mathcal{X}. \tag{76}\]

**Proposition 4**.: _The discrete-time Markov process obtained from the queueing system governed by parameter \(\theta=(\theta_{1},\theta_{2})\in\Theta\) and following policy \(\pi_{\omega}\in\tilde{\Pi}\) is polynomially ergodic. This follow because (76) holds for_

\[V^{p}_{\theta,\omega}(\mathbf{x})=\frac{x_{1}^{2}}{\omega}+x_{2}^{2}, \tag{77}\] \[C^{p}_{\theta,\omega}=\left\{(x_{1},x_{2})\in\mathcal{X}:x_{i} \leq\left(16c_{R}^{2}R^{3-i}+101c_{R}R\right)\frac{\lambda+\theta_{i}}{\theta_{ i}},i=1,2\right\},\] (78) \[\beta^{p}_{\theta,\omega}=\min\left(\frac{\theta_{2}}{2(\theta_{2 }+\lambda)\sqrt{\omega+1}},\frac{\theta_{1}+\theta_{2}-\lambda}{(\theta_{1}+ \theta_{2}+\lambda)\sqrt{\omega+1}},\frac{\theta_{2}}{2(\theta_{2}+\lambda)}, \frac{\theta_{1}}{2(\theta_{1}+\lambda)\sqrt{\omega}}\right),\] (79) \[b^{p}_{\theta,\omega}=(\beta^{p}_{\theta,\omega}+1)\max_{\mathbf{x} \in C^{p}_{\theta,\omega}}\left(\frac{(x_{1}+1)^{2}}{\omega}+(x_{2}+1)^{2} \right),\] (80) \[\alpha^{p}_{\theta,\omega}=\frac{1}{2}. \tag{81}\]

Proof of Proposition 4 is given in Appendix F.4. Next, we verify the remaining conditions of Assumption 4.

1. From (77) and the fact that \(\omega\in[\frac{1}{c_{R}R},c_{R}R]\), the first condition holds with \(r^{p}_{*}=2\) and \(s^{p}_{*}=\sup_{\theta\in\Theta,\omega\in[\frac{1}{c_{R}R},c_{R}R]}s_{\theta, \omega}=c_{R}R+1\).
2. From (78), state \((0,0)\) belongs to \(C^{p}_{\theta,\omega}\) for all \(\theta\in\Theta\) and \(\omega\in[\frac{1}{c_{R}R},c_{R}R]\). Furthermore, for \(i=1,2\), \[\sup_{\theta\in\Theta,\omega\in[\frac{1}{c_{R}R},c_{R}R]}\frac{\lambda+\theta_{ i}}{\theta_{i}}\leq\sup_{\theta\in\Theta}\frac{1}{\theta_{2}}\leq\sup_{\theta \in\Theta}\frac{R}{\theta_{1}}\leq\frac{4R}{1+\delta},\] (82) which follows from the fact that \(\theta_{1}\leq R\theta_{2}\) and \(\tilde{\theta}_{1}\geq 0.25+0.25\delta\). Thus, from the definition of \(C^{p}_{\theta,\omega}\) in (78), \(C^{p}_{*}=\cup_{\theta\in\Theta,\omega\in[\frac{1}{c_{R}R},c_{R}R]}C^{p}_{ \theta,\omega}\) is a finite set. We next verify that the infimum of \(\beta^{p}_{\theta,\omega}\), found in (79), is positive. In (82), we showed that infimum of \(\frac{\lambda+\theta_{i}}{\theta_{i}}\) over \(\Theta\) is lower bounded by \(\frac{1+\delta}{4}\). From this, the fact that \(\omega\) belongs to a compact set, and \(\theta_{1}+\theta_{2}+\lambda\geq\delta\), it follows that \(\inf_{\theta\in\Theta,\omega\in[\frac{1}{c_{R}R},c_{R}R]}\beta^{p}_{\theta, \omega}>0\). Furthermore, it is easy to see that \(\beta^{p}_{\theta,\omega}\leq\sqrt{c_{R}R}\). Hence, from (80), \[\sup_{\theta\in\Theta,\omega\in[\frac{1}{c_{R}R},c_{R}R]}b^{p}_{ \theta,\omega} =\sup_{\theta\in\Theta,\omega\in[\frac{1}{c_{R}R},c_{R}R]}(\beta ^{p}_{\theta,\omega}+1)\max_{\mathbf{x}\in C^{p}_{\theta,\omega}}\left(\frac{(x_{1 }+1)^{2}}{\omega}+(x_{2}+1)^{2}\right)\] \[\leq(\sqrt{c_{R}R}+1)\max_{\mathbf{x}\in C^{p}_{*}}\left(C_{R}R(x_{1 }+1)^{2}+(x_{2}+1)^{2}\right),\] which is finite as \(|C^{p}_{*}|<\infty\).
3. We need to show that \(K_{\theta,\omega}(\mathbf{x}):=\sum_{n=0}^{\infty}2^{-n-2}\left(P^{\pi_{\omega}}_{ \theta}\right)^{n}\left(\mathbf{x},0^{d}\right)\) is strictly bounded away from zero. We show this using the fact that from any state \(\mathbf{x}\), the queueing system hits \((0,0)\) in one step with positive probability. Take \(x_{i,\theta,\omega}=\max_{\mathbf{x}\in C_{\theta,\omega}}x_{i}\) for \(i=1,2\). We have \[\inf_{\theta\in\Theta,\omega\in[\frac{1}{c_{R}R},c_{R}R]}\min_{\bm {x}\in C_{\theta,\omega}}K(\mathbf{x}) \geq\inf_{\theta\in\Theta,\omega\in[\frac{1}{c_{R}R},c_{R}R]} \min_{\mathbf{x}\in C_{\theta,\omega}}P(\mathbf{x},0^{d})\] \[\geq\inf_{\theta\in\Theta,\omega\in[\frac{1}{c_{R}R},c_{R}R]}P \left(\left(x_{1,\theta,\omega},x_{2,\theta,\omega}\right),0^{d}\right).\] The infimum in the right-hand side of the above equation is attained for the minimum normalized service rates possible for each server, or \(\tilde{\theta}_{1}=\frac{1+\delta}{4}\) and \(\tilde{\theta}_{2}=\frac{1+\delta}{4R}\). Therefore, the infimum of \(K_{\theta,\omega}(\mathbf{x})\) over the finite set \(C^{p}_{*}\), \(\Theta\), and interval \([\frac{1}{c_{R}R},c_{R}R]\) is strictly greater than zero.

**Assumption 5**.: We finally verify that \(\sup_{\theta\in\Theta}J(\theta)\) is finite. We first note that for \(\mathbf{x}=(x_{1},x_{2})\),

\[(x_{1}+x_{2})^{2}\leq 2\max(\omega^{*}(\theta),1)\left(\frac{x_{1}^{2}}{\omega^{*}( \theta)}+x_{2}^{2}\right)=2\max(\omega^{*}(\theta),1)V^{p}_{\theta,\omega^{*}( \theta)}\left(\mathbf{x}\right).\]From the above equation,

\[J(\theta) =\mathbb{E}_{\mathbf{X}\sim\mu_{\theta,\omega^{*}(\theta)}}\left[c(\mathbf{X})\right]\] \[=\mathbb{E}_{\mathbf{X}\sim\mu_{\theta,\omega^{*}(\theta)}}\left[\|\mathbf{X }\|_{1}\right]\] \[\leq\sqrt{2\max(\omega^{*}(\theta),1)}\mathbb{E}_{\mathbf{X}\sim\mu_{ \theta,\omega^{*}(\theta)}}\left[\sqrt{V_{\theta,\omega^{*}(\theta)}^{p}\left( \mathbf{X}\right)}\right],\]

where \(\mu_{\theta,\omega^{*}(\theta)}\) is the stationary distribution of the discrete-time process governed by parameter \(\theta\) and following the best in-class policy according to \(\theta\), shown by \(\pi_{\omega^{*}(\theta)}\). From [43], Theorem 14.3.7,

\[\mu_{\theta,\omega^{*}(\theta)}\left(\sqrt{V_{\theta,\omega^{*}(\theta)}^{p} \left(\mathbf{X}\right)}\right)\leq\frac{\sup_{\theta\in\Theta,\omega\in[\frac{1}{ \alpha R},c_{R}R]}b_{\theta,\omega}^{p}}{\beta_{*}^{p}},\]

which is finite from the the previous verified assumption. Thus,

\[\sup_{\theta\in\Theta}J(\theta)\leq\frac{\sqrt{2c_{R}R}\left(\sup_{\theta\in \Theta,\omega\in[\frac{1}{\alpha R},c_{R}R]}b_{\theta,\omega}^{p}\right)}{ \beta_{*}^{p}}<\infty.\]

## Appendix F Proofs related to the queueing model examples

### Proof of Proposition 1

Proof.: We define the normalized rates as

\[\tilde{\lambda}=\frac{\lambda}{\lambda+\theta_{1}+\theta_{2}},\quad\tilde{ \theta}_{i}=\frac{\theta_{i}}{\lambda+\theta_{1}+\theta_{2}}, \tag{83}\]

for \(i=1,2\). From the choice of parameter space \(\Theta\), we have \(\tilde{\lambda}\leq 0.5-0.5\delta\), \(\theta_{1}+\theta_{2}\geq 0.5+0.5\delta\), and \(\theta_{1}\geq 0.25+0.25\delta\). To prove geometric ergodicity, from the discussions of Section 2, it suffices to show that there exists a finite set \(C_{\theta,t}^{g}\), constants \(b_{\theta,t}^{g}>0\), \(\gamma_{\theta,t}^{g}\in(0,1)\), and a function \(V_{\theta,t}^{g}:\mathcal{X}_{t}\rightarrow[1,+\infty)\) satisfying

\[\Delta V_{\theta,t}^{g}(\mathbf{x})\leq-\left(1-\gamma_{\theta,t}^{g}\right)V_{ \theta,t}^{g}(\mathbf{x})+b_{\theta,t}^{g}\mathbb{I}_{C_{\theta,t}^{g}}(\mathbf{x}), \quad\mathbf{x}\in\mathcal{X}_{t}. \tag{84}\]

Take \(V_{\theta,t}^{g}(\mathbf{x})=\exp(a_{\theta,t}^{g}\|\mathbf{x}\|_{1})\) for some \(a_{\theta,t}^{g}>0\). For \(i\geq 1\) and \(\mathbf{x}=(i,1,1)\),

\[P_{\theta}^{t}V_{\theta,t}^{g}(i,1,1)=\tilde{\lambda}V_{\theta,t}^{g}(i+1,1,1) +\tilde{\theta}_{1}V_{\theta,t}^{g}(i,0,1)+\tilde{\theta}_{2}V_{\theta,t}^{g} (i,1,0),\]

where \(P_{\theta}^{t}\) is the corresponding transition kernel. Thus,

\[P_{\theta}^{t}V_{\theta,t}^{g}(i,1,1)-(1-\gamma_{\theta,t}^{g}) V_{\theta,t}^{g}(i,1,1)\] \[=\tilde{\lambda}\exp\left(a_{\theta,t}^{g}\left(i+3\right)\right) +\left(\tilde{\theta}_{1}+\tilde{\theta}_{2}\right)\exp\left(a_{\theta,t}^{g} \left(i+1\right)\right)-(1-\gamma_{\theta,t}^{g})\exp\left(a_{\theta,t}^{g} \left(i+2\right)\right)\] \[=\exp\left(a_{\theta,t}^{g}\left(i+1\right)\right)\left(\tilde{ \lambda}\exp(2a_{\theta,t}^{g})+\tilde{\theta}_{1}+\tilde{\theta}_{2}-(1- \gamma_{\theta,t}^{g})\exp(a_{\theta,t}^{g})\right).\]

Take \(\tilde{a}_{\theta,t}=\exp(a_{\theta,t}^{g})\). We need to find \(\tilde{a}_{\theta,t}>1\) and \(0<\gamma_{\theta,t}^{g}<1\) such that

\[\tilde{\lambda}\tilde{a}_{\theta,t}^{2}-(1-\gamma_{\theta,t}^{g})\tilde{a}_{ \theta,t}+\tilde{\theta}_{1}+\tilde{\theta}_{2}<0. \tag{85}\]

Take \(\tilde{a}_{\theta,t}=\left(1-\delta\right)^{-1}>1\) and

\[\tilde{\gamma}_{\theta,t}:=1-\gamma_{\theta,t}^{g}=\frac{1}{2}\left(1+(1- \tilde{\lambda})(1-\delta)+\tilde{\lambda}\left(1-\delta\right)^{-1}\right).\]

We need to have \(\tilde{\gamma}_{\theta,t}<1\) which follows from the stability condition \(\tilde{\lambda}\leq 0.5-0.5\delta\) as below:

\[\tilde{\gamma}_{\theta,t} =\frac{1}{2}+\frac{1}{2}\left((1-\tilde{\lambda})(1-\delta)+ \frac{\tilde{\lambda}}{1-\delta}\right)=\frac{1}{2}+\frac{1}{2}\left(1- \delta-\tilde{\lambda}(1-\delta)+\frac{\tilde{\lambda}}{1-\delta}\right)\] \[=\frac{1}{2}+\frac{1}{2}\left(1-\delta+\tilde{\lambda}\frac{1-(1 -\delta)^{2}}{1-\delta}\right)=\frac{1}{2}+\frac{1}{2}\left(1-\delta+ \tilde{\lambda}\frac{\delta(2-\delta)}{1-\delta}\right)\] \[\leq\frac{1}{2}+\frac{1}{2}\left(1-\delta+\frac{\delta(2-\delta) }{2}\right)=1-\frac{\delta^{2}}{4}<1.\]We now verify (85):

\[\tilde{\lambda}\tilde{a}_{\theta,t}^{2}-(1-\gamma_{\theta,t}^{g}) \tilde{a}_{\theta,t}+\tilde{\theta}_{1}+\tilde{\theta}_{2} =\frac{\tilde{\lambda}}{(1-\delta)^{2}}-\frac{1}{2(1-\delta)}- \frac{1-\tilde{\lambda}}{2}-\frac{\tilde{\lambda}}{2(1-\delta)^{2}}+1-\tilde{\lambda}\] \[=\frac{\tilde{\lambda}}{2(1-\delta)^{2}}+\frac{1-\tilde{\lambda}} {2}-\frac{1}{2(1-\delta)}\] \[=\frac{1}{2(1-\delta)^{2}}\left(\tilde{\lambda}+(1-\tilde{\lambda })(1-\delta)^{2}-(1-\delta)\right)\] \[=\frac{\delta}{2(1-\delta)^{2}}\left(\delta-1-\tilde{\lambda} \delta+2\tilde{\lambda}\right)\] \[=\frac{\delta}{2(1-\delta)^{2}}\left(\tilde{\lambda}\left(2- \delta\right)+\delta-1\right)\] \[<0,\]

where the last line follows from \(\tilde{\lambda}\leq 0.5-0.5\delta<\left(1-\delta\right)/\left(2-\delta\right)\).

For \(\mathbf{x}=(i,0,1)\) and \(i\geq 1\), we have

\[P_{\theta}^{t}V_{\theta,t}^{g}(i,0,1)=\tilde{\lambda}V_{\theta,t}^{g}(i,1,1)+ \tilde{\theta}_{1}V_{\theta,t}^{g}(i-1,0,1)+\tilde{\theta}_{2}V_{\theta,t}^{g }(i-1,1,0),\]

and

\[P_{\theta}^{t}V_{\theta,t}^{g}(i,0,1)-(1-\gamma_{\theta,t}^{g}) V_{\theta,t}^{g}(i,0,1)\] \[=\tilde{\lambda}\exp\left(a_{\theta,t}^{g}\left(i+2\right)\right) +(\tilde{\theta}_{1}+\tilde{\theta}_{2})\exp\left(a_{\theta,t}^{g}i\right)-(1- \gamma_{\theta,t}^{g})\exp\left(a_{\theta,t}^{g}\left(i+1\right)\right)\] \[=\exp\left(a_{\theta,t}^{g}i\right)\left(\tilde{\lambda}\exp(2a_{ \theta,t}^{g})+\tilde{\theta}_{1}+\tilde{\theta}_{2}-(1-\gamma_{\theta,t}^{g}) \exp(a_{\theta,t}^{g})\right),\]

which results in the same conditions as previously discussed. When \(\mathbf{x}=(i,1,0)\) and \(i\geq t\) also same argument holds.

Finally, (84) holds for

\[C_{\theta,t}^{g} =\{(x_{0},x_{1},0):x_{0}<t\}\cup\{(0,0,1)\},\] \[a_{\theta,t}^{g} =-\log(1-\delta),\] \[\gamma_{\theta,t}^{g} =\frac{1}{2}-\frac{1}{2}\left((1-\tilde{\lambda})(1-\delta)+ \tilde{\lambda}\left(1-\delta\right)^{-1}\right),\] \[V_{\theta,t}^{g}(\mathbf{x}) =\exp(a_{\theta,t}^{g}\|\mathbf{x}\|_{1}),\] \[b_{\theta,t}^{g} =\max_{\mathbf{x}\in C_{\theta,t}^{g}}\exp(a_{\theta,t}^{g}\|\mathbf{x}\| _{1})\left(\exp(a_{\theta,t}^{g})+1\right),\]

where the last line holds because \(PV_{\theta,t}^{g}(\mathbf{x})\leq V_{\theta,t}^{g}(\mathbf{y})\) for \(\mathbf{y}\) such that \(\|\mathbf{y}\|_{1}=\|\mathbf{x}\|_{1}+1\). 

### Proof of Proposition 2

Proof.: In order to show polynomially ergodicity, we will verify (62). We define \(V_{\theta,t}^{p}(\mathbf{x})=\|\mathbf{x}\|_{1}^{2}\) and \(\alpha_{\theta,t}^{p}=1/2\), which is equal to \(r/(r+1)\) for \(r=1\); \(r\) is defined in Assumption 1. For \(\mathbf{x}=(i,0,1)\) and \(i\geq 1\),

\[P_{\theta}^{t}V_{\theta,t}^{p}(i,0,1)=\tilde{\lambda}V_{\theta,t}^{p}(i,1,1)+ \tilde{\theta}_{1}V_{\theta,t}^{p}(i-1,0,1)+\tilde{\theta}_{2}V_{\theta,t}^{ p}(i-1,1,0),\]

in which \(\tilde{\lambda}\), \(\tilde{\theta}_{1}\), and \(\tilde{\theta}_{2}\) are the normalized rates defined in (83). Thus,

\[P_{\theta}^{t}V_{\theta,t}^{p}(i,0,1)-V_{\theta,t}^{p}(i,0,1)+ \beta_{\theta,t}^{p}\sqrt{V_{\theta,t}^{p}(i,0,1)}\] \[=\tilde{\lambda}(i+2)^{2}+(\tilde{\theta}_{1}+\tilde{\theta}_{2} )i^{2}-(i+1)^{2}+\beta_{\theta,t}^{p}(i+1)\] \[=i(4\tilde{\lambda}-2+\beta_{\theta,t}^{p})+4\tilde{\lambda}-1+ \beta_{\theta,t}^{p}.\]For \(\beta_{\theta,t}^{p}=1-2\tilde{\lambda}\), the right-hand side of above equation is non-positive for \(i\geq\frac{2\tilde{\lambda}}{1-2\tilde{\lambda}}\). For \(\mathbf{x}=(i,1,0)\) and \(i\geq t\),

\[P_{\theta}^{t}V_{\theta,t}^{p}(i,1,0)=\tilde{\lambda}V_{\theta,t}^{p}(i,1,1)+ \tilde{\theta}_{1}V_{\theta,t}^{p}(i-1,0,1)+\tilde{\theta}_{2}V_{\theta,t}^{p} (i-1,1,0).\]

Thus,

\[P_{\theta}^{t}V_{\theta,t}^{p}(i,1,0)-V_{\theta,t}^{p}(i,1,0)+ \beta_{\theta,t}^{p}\sqrt{V_{\theta,t}^{p}(i,1,0)}\] \[=\tilde{\lambda}(i+2)^{2}+(\tilde{\theta}_{1}+\tilde{\theta}_{2} )i^{2}-(i+1)^{2}+\beta_{\theta,t}^{p}(i+1)\] \[=i(4\tilde{\lambda}-2+\beta_{\theta,t}^{p})+4\tilde{\lambda}-1+ \beta_{\theta,t}^{p},\]

which is also non-positive under the same conditions as the previous case. For \(i\geq 1\) and \(\mathbf{x}=(i,1,1)\),

\[P_{\theta}^{t}V_{\theta,t}^{p}(i,1,1)=\tilde{\lambda}V_{\theta,t}^{p}(i+1,1,1 )+\tilde{\theta}_{1}V_{\theta,t}^{p}(i,0,1)+\tilde{\theta}_{2}V_{\theta,t}^{p} (i,1,0).\]

Thus,

\[P_{\theta}^{t}V_{\theta,t}^{p}(i,1,1)-V_{\theta,t}^{p}(i,1,1)+ \beta_{\theta,t}^{p}\sqrt{V_{\theta,t}^{p}(i,1,1)}\] \[=\tilde{\lambda}(i+3)^{2}+(\tilde{\theta}_{1}+\tilde{\theta}_{2} )(i+1)^{2}-(i+2)^{2}+\beta_{\theta,t}^{p}(i+2)\] \[=i(4\tilde{\lambda}-2+\beta_{\theta,t}^{p})+8\tilde{\lambda}-3+2 \beta_{\theta,t}^{p},\]

which is non-positive under the same conditions as the first case. Finally, (62) holds for

\[C_{\theta,t}^{p} =\left\{(x_{0},x_{1},0):x_{0}<t\right\}\cup\left\{(x_{0},x_{1},x_ {2}):x_{0}<\frac{2\tilde{\lambda}}{1-2\tilde{\lambda}},x_{1}+x_{2}\geq 1 \right\},\] \[\beta_{\theta,t}^{p} =1-2\tilde{\lambda},\] \[\alpha_{\theta,t}^{p} =\frac{1}{2},\] \[V_{\theta,t}^{p}(\mathbf{x}) =\|\mathbf{x}\|_{1}^{2},\] \[b_{\theta,t}^{p} =\max_{\mathbf{x}\in C_{\theta,t}^{p}}\left(\|\mathbf{x}\|_{1}+1)^{2}\right)\]

where the last line holds because \(PV_{\theta,t}^{p}(\mathbf{x})\leq V_{\theta,t}^{p}(\mathbf{y})\) for \(\mathbf{y}\) such that \(\|\mathbf{y}\|_{1}=\|\mathbf{x}\|_{1}+1\). 

### Proof of Proposition 3

Proof.: To show geometric ergodicity of the chain that follows \(\pi_{\omega}\), we verify (70). Take \(a_{\theta,\omega}^{g}>0\) and

\[V_{\theta,\omega}^{g}(\mathbf{x})=\frac{\omega}{\omega+1}\exp\left(a_{\theta, \omega}^{g}\frac{x_{1}+1}{\omega}\right)+\frac{1}{\omega+1}\exp\left(a_{ \theta,\omega}^{g}\left(x_{2}+1\right)\right). \tag{86}\]

First, we find \(PV_{\theta,\omega}^{g}(\mathbf{x})\) for the function defined above. We have

\[PV_{\theta,\omega}^{g}(\mathbf{x})=\mathbb{E}_{\mathbf{x}}^{\pi_{\omega}}\left[\frac{ \omega}{\omega+1}\exp\left(a_{\theta,\omega}^{g}\frac{X_{1}(2)+1}{\omega} \right)\right]+\mathbb{E}_{\mathbf{x}}^{\pi_{\omega}}\left[\frac{1}{\omega+1}\exp \left(a_{\theta,\omega}^{g}\left(X_{2}(2)+1\right)\right)\right], \tag{87}\]

where \(\mathbf{X}(2)=(X_{1}(2),X_{2}(2))\) is the state of the system at the second arrival, starting from state \(\mathbf{x}\). To find the above expectations, we first find the corresponding transition probabilities. If the number of departures from server \(i\) during a fixed interval with length \(t\) is less than the total number of jobs in the queue of that server, the number of departures follows a Poisson distribution with parameter \(\theta_{i}t\). Let \(\mathbb{P}\left((x_{1},x_{2})\rightarrow(x_{1}^{\prime},\mathcal{X})\right)\) be the probability of transitioning from a system with \(x_{i}\) jobs in server-queue pair \(i\) (just after the assignment of the arrival) to a queueing system with \(x_{1}^{\prime}\) jobs in the first server-queue pair (just before the upcoming arrival). For \(1\leq x_{1}^{\prime}\leq x_{1}\), we have

\[\mathbb{P}\left((x_{1},x_{2})\rightarrow(x_{1}^{\prime},\mathcal{X})\right)= \int_{0}^{\infty}\lambda\exp(-\lambda t)\frac{(\theta_{1}t)^{x_{1}-x_{1}^{ \prime}}}{(x_{1}-x_{1}^{\prime})!}\exp(-\theta_{1}t)\ dt=\frac{\lambda}{ \theta_{1}+\lambda}\left(\frac{\theta_{1}}{\theta_{1}+\lambda}\right)^{x_{1}- x_{1}^{\prime}}, \tag{88}\]and

\[\mathbb{P}\left(\left(x_{1},x_{2}\right)\rightarrow\left(0,\mathcal{X}\right) \right)=1-\sum_{i=1}^{x_{1}}\frac{\lambda}{\theta_{1}+\lambda}\left(\frac{ \theta_{1}}{\theta_{1}+\lambda}\right)^{x_{1}-i}=\left(\frac{\theta_{1}}{ \theta_{1}+\lambda}\right)^{x_{1}}. \tag{89}\]

Assume \(1+x_{1}\leq\omega(1+x_{2})\), which results in the new arrival being assigned to the first server. For the first term in (87), we have

\[\mathbb{E}_{\mathbf{x}}^{\pi_{\omega}}\left[\exp\left(a_{\theta,\omega }^{g}\frac{X_{1}(2)}{\omega}\right)\right]\] \[=\sum_{i=0}^{x_{1}+1}\mathbb{P}\left(\left(x_{1}+1,x_{2}\right) \rightarrow\left(i,\mathcal{X}\right)\right)\exp\left(a_{\theta,\omega}^{g} \frac{i}{\omega}\right)\] \[=\left(\frac{\theta_{1}}{\theta_{1}+\lambda}\right)^{x_{1}+1}+ \sum_{i=1}^{x_{1}+1}\exp\left(a_{\theta,\omega}^{g}\frac{i}{\omega}\right)\frac {\lambda}{\theta_{1}+\lambda}\left(\frac{\theta_{1}}{\theta_{1}+\lambda}\right) ^{x_{1}+1-i}\] \[=\left(\frac{\theta_{1}}{\theta_{1}+\lambda}\right)^{x_{1}+1}+ \frac{\lambda}{\theta_{1}+\lambda}\exp\left(a_{\theta,\omega}^{g}\frac{x_{1}+1 }{\omega}\right)\frac{1-\exp\left(-a_{\theta,\omega}^{g}\frac{x_{1}+1}{\omega} \right)\left(\frac{\theta_{1}}{\theta_{1}+\lambda}\right)^{x_{1}+1}}{1-\exp \left(-\frac{a_{\theta,\omega}^{g}}{\omega}\right)\frac{\theta_{1}}{\theta_{1} +\lambda}},\] \[<\left(\frac{\theta_{1}}{\theta_{1}+\lambda}\right)^{x_{1}+1}+ \frac{\lambda}{\theta_{1}+\lambda}\exp\left(a_{\theta,\omega}^{g}\frac{x_{1}+1 }{\omega}\right)\frac{1}{1-\exp\left(-\frac{a_{\theta,\omega}^{g}}{\omega} \right)\frac{\theta_{1}}{\theta_{1}+\lambda}}. \tag{90}\]

Similarly, for the second term in (87), we have

\[\mathbb{E}_{\mathbf{x}}^{\pi_{\omega}}\left[\exp\left(a_{\theta,\omega}^{g}X_{2}(2 )\right)\right]\leq\left(\frac{\theta_{2}}{\theta_{2}+\lambda}\right)^{x_{2}} +\frac{\lambda}{\theta_{2}+\lambda}\exp\left(a_{\theta,\omega}^{g}x_{2}\right) \frac{1}{1-\exp\left(-a_{\theta,\omega}^{g}\right)\frac{\theta_{2}}{\theta_{2} +\lambda}}. \tag{91}\]

To satisfy (70), for some \(0<\gamma_{\theta,\omega}^{g}<1\) and all but finitely many \(\mathbf{x}\), the following should hold,

\[PV_{\theta,\omega}^{g}(\mathbf{x})\leq\gamma_{\theta,\omega}^{g}V_{\theta,\omega}^ {g}(\mathbf{x}),\]

or from (86) and (87),

\[\mathbb{E}_{\mathbf{x}}^{\pi_{\omega}}\left[\omega\exp\left(a_{\theta,\omega}^{g}\frac{X_{1}(2)+1}{\omega}\right)\right]+\mathbb{E}_{\mathbf{x}}^{\pi_ {\omega}}\left[\exp\left(a_{\theta,\omega}^{g}\left(X_{2}(2)+1\right)\right)\right]\] \[\leq\gamma_{\theta,\omega}^{g}\left(\omega\exp\left(a_{\theta, \omega}^{g}\frac{x_{1}+1}{\omega}\right)+\exp\left(a_{\theta,\omega}^{g}\left( x_{2}+1\right)\right)\right).\]

Notice that

\[\omega\left(\frac{\theta_{1}}{\theta_{1}+\lambda}\right)^{x_{1}+1}+\left( \frac{\theta_{2}}{\theta_{2}+\lambda}\right)^{x_{2}}\leq c_{R}R+1.\]

From (90) and (91), it suffices to have

\[\left(c_{R}R+1\right)\exp(c_{R}Ra_{\theta,\omega}^{g})+\frac{ \omega\frac{\lambda}{\theta_{1}+\lambda}\exp\left(a_{\theta,\omega}^{g}\frac{ x_{1}+2}{\omega}\right)}{1-\exp\left(-\frac{a_{\theta,\omega}^{g}}{\omega}\right) \frac{\theta_{1}}{\theta_{1}+\lambda}}+\frac{\frac{\lambda}{\theta_{2}+ \lambda}\exp\left(a_{\theta,\omega}^{g}\left(x_{2}+1\right)\right)}{1-\exp \left(-a_{\theta,\omega}^{g}\right)\frac{\theta_{2}}{\theta_{2}+\lambda}}\] \[\leq\gamma_{\theta,\omega}^{g}\left(\omega\exp\left(a_{\theta, \omega}^{g}\frac{x_{1}+1}{\omega}\right)+\exp\left(a_{\theta,\omega}^{g}\left( x_{2}+1\right)\right)\right). \tag{92}\]

Define

\[\zeta_{1,\theta,\omega}=\frac{\frac{\lambda}{\theta_{1}+\lambda}}{1-\exp\left( -\frac{a_{\theta,\omega}^{g}}{\omega}\right)\frac{\theta_{1}}{\theta_{1}+ \lambda}},\qquad\zeta_{2,\theta,\omega}=\frac{\frac{\lambda}{\theta_{2}+ \lambda}}{1-\exp\left(-a_{\theta,\omega}^{g}\right)\frac{\theta_{2}}{\theta_ {2}+\lambda}}.\]

Simplifying (92), we need the following to hold

\[\left(c_{R}R+1\right)\exp(c_{R}Ra_{\theta,\omega}^{g})+\omega\exp \left(a_{\theta,\omega}^{g}\frac{x_{1}+1}{\omega}\right)\left(\zeta_{1,\theta, \omega}\exp\left(\frac{a_{\theta,\omega}^{g}}{\omega}\right)-\gamma_{\theta, \omega}^{g}\right)\] \[+\exp\left(a_{\theta,\omega}^{g}\left(x_{2}+1\right)\right) \left(\zeta_{2,\theta,\omega}-\gamma_{\theta,\omega}^{g}\right)\leq 0. \tag{93}\]As \(\zeta_{i,\theta,\omega}<1\), there exists \(\gamma^{g}_{\theta,\omega}\) such that

\[\zeta_{2,\theta,\omega}<\gamma^{g}_{\theta,\omega}<1.\]

From the assumption \(1+x_{1}\leq\omega(1+x_{2})\) and the above equation, (93) can be further simplified as

\[(c_{R}R+1)\exp(c_{R}Ra^{g}_{\theta,\omega})+\exp\left(a^{g}_{\theta,\omega} \frac{x_{1}+1}{\omega}\right)\left(\omega\zeta_{1,\theta,\omega}\exp\left(\frac {a^{g}_{\theta,\omega}}{\omega}\right)+\zeta_{2,\theta,\omega}-(\omega+1)\gamma ^{g}_{\theta,\omega}\right)\leq 0. \tag{94}\]

For the above to hold outside a finite set, we need to have

\[\frac{\zeta_{1,\theta,\omega}\omega}{1+\omega}\exp\left(\frac{a^{g}_{\theta, \omega}}{\omega}\right)+\frac{\zeta_{2,\theta,\omega}}{1+\omega}<\gamma^{g}_{ \theta,\omega}. \tag{95}\]

Define

\[\zeta_{3}=\frac{1}{1+\delta},\quad\zeta_{4}=\frac{1-0.5\delta}{1-\delta}. \tag{96}\]

Note that \(\zeta_{3}<1\) and \(\zeta_{4}>1\). Defining function \(f(y):=1+\zeta_{4}y-\exp(y)\), we note that for \(y\leq\log\zeta_{4}\), \(f(y)>0\), where \(\log\zeta_{4}\) is the maximizer of \(f(y)\). Similarly, taking \(g(y):=1-\zeta_{3}y-\exp(-y)\), for \(y\leq-\log\zeta_{3}\), \(g(y)>0\), where \(-\log\zeta_{3}\) is the maximizer of \(g(y)\). Thus, we conclude that for \(a^{g}_{\theta,\omega}\leq\min\left(-\omega\log\zeta_{3},-\log\zeta_{3},\omega \log\zeta_{4}\right)\),

\[\exp(-y)\leq 1-\zeta_{3}y\quad\text{holds for}\quad\ y\leq\max\left(\frac{a ^{g}_{\theta,\omega}}{\omega},a^{g}_{\theta,\omega}\right), \tag{97}\]

\[\exp(y)\leq 1+\zeta_{4}y\quad\text{holds for}\quad\ y\leq\frac{a^{g}_{\theta, \omega}}{\omega}. \tag{98}\]

To guarantee the existence of \(0<\gamma^{g}_{\theta,\omega}<1\) that satisfies (95), we need to ensure the left-hand side of (95) is strictly less than 1. Using the bounds found in (97) and (98) and the definition of \(\zeta_{1,\theta,\omega}\) and \(\zeta_{2,\theta,\omega}\), we simplify (95) to get

\[\frac{\frac{\lambda}{1+\omega}\left(\omega+a^{g}_{\theta,\omega}\zeta_{4} \right)}{\lambda+\frac{\theta_{1}a^{g}_{\theta,\omega}\zeta_{3}}{\omega}}+ \frac{\frac{\lambda}{1+\omega}}{\lambda+\theta_{2}a^{g}_{\theta,\omega}\zeta_{ 3}}<1,\]

which is equivalent to

\[a^{g}_{\theta,\omega}\zeta_{3}\theta_{2}\left(\lambda\zeta_{4}-\frac{\zeta_{3} \theta_{1}(1+\omega)}{\omega}\right)<\lambda\zeta_{3}\left(\theta_{1}+\theta_ {2}\right)-\lambda^{2}\zeta_{4}. \tag{99}\]

To make sure there exists \(a^{g}_{\theta,\omega}>0\) that satisfies (99), the right-hand side of (99) needs to be positive, which follows as below:

\[\lambda\zeta_{3}\left(\theta_{1}+\theta_{2}\right)-\lambda^{2} \zeta_{4} =\lambda\left(\frac{\theta_{1}+\theta_{2}}{1+\delta}-\lambda \frac{1-0.5\delta}{1-\delta}\right)\] \[=\lambda(\theta_{1}+\theta_{2}+\lambda)\left(\frac{1-\tilde{ \lambda}}{1+\delta}-\tilde{\lambda}\frac{1-0.5\delta}{1-\delta}\right)\] \[=\lambda(\theta_{1}+\theta_{2}+\lambda)\left(\frac{1}{1+\delta} -\tilde{\lambda}\left(\frac{1}{1+\delta}+\frac{1-0.5\delta}{1-\delta}\right)\right)\] \[\geq\lambda(\theta_{1}+\theta_{2}+\lambda)\left(\frac{1}{1+ \delta}-\frac{1-\delta}{2}\left(\frac{1}{1+\delta}+\frac{1-0.5\delta}{1-\delta }\right)\right)\] \[=\frac{\delta}{4}\lambda(\theta_{1}+\theta_{2}+\lambda) \tag{100}\]

where \(\tilde{\lambda}\), \(\tilde{\theta}_{1}\), and \(\tilde{\theta}_{2}\) are the normalized rates defined in (83) and we have used the stability condition \(\tilde{\lambda}\leq 0.5-0.5\delta\). We further simplify the left-hand side of (99) as

\[\zeta_{3}\theta_{2}\left(\lambda\zeta_{4}-\frac{\zeta_{3}\theta_{1}(1+\omega)}{ \omega}\right)<\theta_{2}\lambda\zeta_{3}\zeta_{4}<\frac{1-0.5\delta}{1-\delta ^{2}}(\theta_{1}+\theta_{2}+\lambda)\lambda.\]From the above equation and (100), \(a^{g}_{\theta,\omega}\) needs to satisfy

\[a^{g}_{\theta,\omega}\leq\frac{\delta(1-\delta^{2})}{8(1-0.5\delta)}.\]

Finally, we take \(a^{g}_{\theta,\omega}\) as

\[a^{g}_{\theta,\omega}=\min\left(-\omega\log\zeta_{3},-\log\zeta_{3},\omega\log \zeta_{4},\frac{\delta(1-\delta^{2})}{8(1-0.5\delta)}\right).\]

After finding an appropriate \(a^{g}_{\theta,\omega}\), we can choose \(0<\gamma^{g}_{\theta,\omega}<1\) such that (95) holds or

\[\gamma^{g}_{\theta,\omega}\geq\frac{1}{2}\left(1+\frac{\zeta_{1,\theta,\omega} \omega}{1+\omega}\exp\left(\frac{a^{g}_{\theta,\omega}}{\omega}\right)+\frac{ \zeta_{2,\theta,\omega}}{1+\omega}\right).\]

Moreover, from (94) a lower bound \(x^{g_{1}}_{1,\theta,\omega}\) for \(x_{1}\) is derived; In other words,(94) holds for \(x_{1}>x^{g_{1}}_{1,\theta,\omega}\). From (93), we can find the corresponding \(x^{g_{1}}_{2,\theta,\omega}\) and take \(\mathbf{x}^{g_{1}}_{\theta,\omega}=(x^{g_{1}}_{1,\theta,\omega},x^{g_{1}}_{2, \theta,\omega})\). By repeating the same arguments when \(1+x_{1}<\omega(1+x_{2})\), we finally conclude that

\[\Delta V^{g}_{\theta,\omega}(\mathbf{x})\leq-\left(1-\gamma^{g}_{\theta,\omega} \right)V^{g}_{\theta,\omega}(\mathbf{x})+b^{g}_{\theta,\omega}\mathbb{I}_{C^{g}_{ \theta,\omega}}(\mathbf{x}),\quad\mathbf{x}\in\mathcal{X},\]

for

\[V^{g}_{\theta,\omega}(\mathbf{x})=\frac{\omega}{\omega+1}\exp\left(a ^{g}_{\theta,\omega}\frac{x_{1}+1}{\omega}\right)+\frac{1}{\omega+1}\exp\left( a^{g}_{\theta,\omega}\left(x_{2}+1\right)\right),\] \[a^{g}_{\theta,\omega}=\min\left(\omega\log(1+\delta),\log(1+ \delta),\omega\log\frac{1-0.5\delta}{1-\delta},\log\frac{1-0.5\delta}{1- \delta},\frac{\delta(1-\delta^{2})}{4c_{R}R(1-0.5\delta)}\right),\] \[C^{g}_{\theta,\omega}=\left\{(x_{1},x_{2})\in\mathcal{X}:x_{i} \leq\max\left(x^{g_{j}}_{i,\theta,\omega},0\right),i,j=1,2\right\},\] \[\gamma^{g}_{\theta,\omega}=\frac{1}{2}+\frac{1}{2}\max\left(\zeta _{1,\theta,\omega},\zeta_{2,\theta,\omega},\frac{\zeta_{1,\theta,\omega}\omega }{1+\omega}\exp\left(\frac{a^{g}_{\theta,\omega}}{\omega}\right)+\frac{\zeta_ {2,\theta,\omega}}{1+\omega},\frac{\zeta_{1,\theta,\omega}\omega}{1+\omega}+ \frac{\zeta_{2,\theta,\omega}}{1+\omega}\exp\left(a^{g}_{\theta,\omega}\right) \right),\] \[b^{g}_{\theta,\omega}=\max_{\mathbf{x}\in C^{g}_{\theta,\omega}} \left(\frac{2\omega}{\omega+1}\exp\left(a^{g}_{\theta,\omega}\frac{x_{1}+2}{ \omega}\right)+\frac{2}{\omega+1}\exp\left(a^{g}_{\theta,\omega}\left(x_{2}+2 \right)\right)\right),\] \[\zeta_{1,\theta,\omega}=\frac{\frac{\lambda}{\theta_{1}+\lambda} }{1-\exp\left(-\frac{a^{g}_{\theta,\omega}}{\omega}\right)\frac{\theta_{1}}{ \theta_{1}+\lambda}},\] \[\zeta_{2,\theta,\omega}=\frac{\frac{\lambda}{\theta_{2}+\lambda} }{1-\exp\left(-a^{g}_{\theta,\omega}\right)\frac{\theta_{2}}{\theta_{2}+ \lambda}},\] \[x^{g_{1}}_{1,\theta,\omega}=\frac{\omega}{a^{g}_{\theta,\omega}} \log\frac{(c_{R}R+1)\exp(c_{R}Ra^{g}_{\theta,\omega})}{(\omega+1)\gamma^{g}_{ \theta,\omega}-\omega\zeta_{1,\theta,\omega}\exp\left(\frac{a^{g}_{\theta, \omega}}{\omega}\right)-\zeta_{2,\theta,\omega}},\] \[x^{g_{1}}_{2,\theta,\omega}=\frac{1}{a^{g}_{\theta,\omega}} \log\frac{(c_{R}R+1)\exp(c_{R}Ra^{g}_{\theta,\omega})+\omega\exp\left(a^{g}_{ \theta,\omega}\frac{x^{g_{1}}_{1,\theta,\omega}+1}{\omega}\right)\left(\zeta_ {1,\theta,\omega}\exp\left(\frac{a^{g}_{\theta,\omega}}{\omega}\right)-\gamma^ {g}_{\theta,\omega}\right)}{\gamma^{g}_{\theta,\omega}-\zeta_{2,\theta,\omega}},\] \[x^{g_{2}}_{2,\theta,\omega}=\frac{1}{a^{g}_{\theta,\omega}}\log \frac{(c_{R}R+1)\exp(c_{R}Ra^{g}_{\theta,\omega})}{(\omega+1)\gamma^{g}_{ \theta,\omega}-\omega\zeta_{1,\theta,\omega}-\zeta_{2,\theta,\omega}\exp\left( a^{g}_{\theta,\omega}\right)},\] \[x^{g_{2}}_{1,\theta,\omega}=\frac{\omega}{a^{g}_{\theta,\omega}} \log\frac{(c_{R}R+1)\exp(c_{R}Ra^{g}_{\theta,\omega})+\exp\left(a^{g}_{\theta, \omega}(x^{g_{2}}_{2,\theta,\omega}+1)\right)\left(\zeta_{2,\theta,\omega}\exp \left(a^{g}_{\theta,\omega}\right)-\gamma^{g}_{\theta,\omega}\right)}{ \omega\left(\gamma^{g}_{\theta,\omega}-\zeta_{1,\theta,\omega}\right)}.\]

### Proof of Proposition 4

Proof.: Define \(V^{p}_{\theta,\omega}(\mathbf{x})=\frac{\pi_{1}^{2}}{\omega}+x_{2}^{2}\), and \(\alpha^{p}_{\theta,\omega}=1/2\). Assume that \(x_{1}=0\) and \(x_{2}>(1-\omega)/\omega\); which means the new job will be assigned to the first server. The transition probabilities of the discrete-time chain sampled at Poisson arrivals is given in (88) and (89), and we calculate \(PV^{p}_{\theta,\omega}(\mathbf{x})\) as

\[PV^{p}_{\theta,\omega}(\mathbf{x})=\frac{\lambda}{\omega(\lambda+ \theta_{1})}+\sum_{i=1}^{x_{2}}i^{2}\frac{\lambda}{\lambda+\theta_{2}}\left( \frac{\theta_{2}}{\theta_{2}+\lambda}\right)^{x_{2}-i}<c_{R}R+\sum_{i=1}^{x_{2 }}i^{2}\frac{\lambda}{\lambda+\theta_{2}}\left(\frac{\theta_{2}}{\theta_{2}+ \lambda}\right)^{x_{2}-i}. \tag{101}\]

We define \(d_{i}:=\theta_{i}/(\theta_{i}+\lambda)\) for \(i=1,2\) and

\[\sum_{i=1}^{x_{2}}i^{2}\frac{\lambda}{\lambda+\theta_{2}}\left( \frac{\theta_{2}}{\theta_{2}+\lambda}\right)^{x_{2}-i}\] \[=\frac{1}{(1-d_{2})^{2}}\left(-d_{2}^{x_{2}}\left(d_{2}+d_{2}^{2} \right)+d_{2}^{2}\left(x_{2}^{2}+2x_{2}+1\right)+d_{2}\left(-2x_{2}^{2}-2x_{2} +1\right)+x_{2}^{2}\right)\] \[=\frac{1}{(1-d_{2})^{2}}\left(\left(1-d_{2}^{x_{2}}\right)\left(d _{2}+d_{2}^{2}\right)+x_{2}^{2}\left(d_{2}^{2}-2d_{2}+1\right)+x_{2}\left(2d_{ 2}^{2}-2d_{2}\right)\right)\] \[=x_{2}^{2}-\frac{2d_{2}}{1-d_{2}}x_{2}+\frac{(1-d_{2}^{x_{2}}) \left(d_{2}+d_{2}^{2}\right)}{(1-d_{2})^{2}}. \tag{102}\]

From (101),

\[PV^{p}_{\theta,\omega}(\mathbf{x})-V^{p}_{\theta,\omega}(\mathbf{x})+ \beta^{p}_{\theta,\omega}x_{2}<\left(-\frac{2d_{2}}{1-d_{2}}+\beta^{p}_{\theta,\omega}\right)x_{2}+\frac{(1-d_{2}^{x_{2}})\left(d_{2}+d_{2}^{2}\right)}{(1- d_{2})^{2}}+c_{R}R.\]

Outside a finite set, we need the above equation to be non-positive; which is equivalent to

\[\left(-2+\beta^{p}_{\theta,\omega}\frac{1-d_{2}}{d_{2}}\right)x_{2}+\frac{(1-d _{2}^{x_{2}})\left(1+d_{2}\right)}{1-d_{2}}+c_{R}R\frac{1-d_{2}}{d_{2}}\leq 0.\]

As \(d_{2}<1\),

\[\frac{1-d_{2}^{y}}{1-d_{2}}=1+d_{2}+\ldots+d_{2}^{y-1}\leq y\qquad\text{for} \,\,\,y\geq 1. \tag{103}\]

Thus,

\[\left(-2+\beta^{p}_{\theta,\omega}\frac{1-d_{2}}{d_{2}}\right)x_{2 }+\frac{(1-d_{2}^{x_{2}})\left(1+d_{2}\right)}{1-d_{2}}+c_{R}R\frac{1-d_{2}}{d _{2}}\] \[\leq\left(d_{2}-1+\beta^{p}_{\theta,\omega}\frac{1-d_{2}}{d_{2}} \right)x_{2}+c_{R}R\frac{1-d_{2}}{d_{2}}.\]

By taking \(\beta^{p}_{\theta,\omega}\leq d_{2}/2\), it suffices for the following to be non-positive,

\[-\frac{1-d_{2}}{2}x_{2}+c_{R}R\frac{1-d_{2}}{d_{2}}\leq 0,\]

which holds for \(x_{2}\geq 2c_{R}R/d_{2}\). Thus, for \(x_{1}=0\) and \(x_{2}\geq\max\left(2c_{R}R(\lambda+\theta_{2})/\theta_{2},(1-\omega)/\omega \right)=2c_{R}R(\lambda+\theta_{2})/\theta_{2}\), (76) holds. The case of \(x_{2}=0\) and non-zero \(x_{1}\) follows same arguments and (76) holds for \(\beta^{p}_{\theta,\omega}\leq d_{1}/2\sqrt{\omega}\), \(x_{2}=0\), and \(x_{1}\geq\max\left(2c_{R}R(\lambda+\theta_{1})/\theta_{1},\omega-1\right)=2c_{R} R(\lambda+\theta_{1})/\theta_{1}\). We now consider the case of \(x_{1},x_{2}>0\) and \(x_{1}+1\leq\omega(x_{2}+1)\), and note that

\[\sqrt{V^{p}_{\theta,\omega}(\mathbf{x})}=\sqrt{\frac{x_{1}^{2}}{\omega}+x_{2}^{2}} \leq\sqrt{\frac{(x_{1}+1)^{2}}{\omega}+(x_{2}+1)^{2}}\leq\sqrt{\omega+1}(x_{2} +1).\]

Hence, it suffices to find finite set \(C^{p}_{\theta,\omega}\), constants \(b^{p}_{\theta,\omega}\) and \(\beta^{p}_{\theta,\omega}>0\), such that the following holds for \(V^{p}_{\theta,\omega}(\mathbf{x})=\frac{x_{1}^{2}}{\omega}+x_{2}^{2}\),

\[\Delta V^{p}_{\theta,\omega}(\mathbf{x})\leq-\sqrt{\omega+1}\beta^{p}_{\theta, \omega}(x_{2}+1)+b^{p}_{\theta,\omega}\mathbb{I}_{C^{p}_{\theta,\omega}}(\mathbf{x }).\]As \(x_{1}+1\leq\omega(x_{2}+1)\), the new arrival is assigned to the first queue and we find \(\Delta V^{p}_{\theta,\omega}(\mathbf{x})+\sqrt{\omega+1}\beta^{p}_{\theta,\omega}(x_{ 2}+1)\) using the same calculations as (102).

\[\Delta V^{p}_{\theta,\omega}(\mathbf{x})+\sqrt{\omega+1}\beta^{p}_{ \theta,\omega}(x_{2}+1)\] \[=\frac{1}{\omega}\left((x_{1}+1)^{2}-\frac{2d_{1}}{1-d_{1}}(x_{1} +1)+\frac{\left(1-d_{1}^{x_{1}+1}\right)\left(d_{1}+d_{1}^{2}\right)}{(1-d_{1} )^{2}}-x_{1}^{2}\right)\] \[-\frac{2d_{2}}{1-d_{2}}x_{2}+\frac{(1-d_{2}^{x_{2}})\left(d_{2}+d _{2}^{2}\right)}{(1-d_{2})^{2}}+\sqrt{\omega+1}\beta^{p}_{\theta,\omega}(x_{2} +1)\] \[=\frac{x_{1}}{\omega}\left(2-\frac{2d_{1}}{1-d_{1}}\right)+\frac{ 1-3d_{1}}{\omega(1-d_{1})}+\frac{\left(1-d_{1}^{x_{1}+1}\right)\left(d_{1}+d_ {1}^{2}\right)}{\omega(1-d_{1})^{2}} \tag{104}\] \[+(x_{2}+1)\left(-\frac{2d_{2}}{1-d_{2}}+\sqrt{\omega+1}\beta^{p}_ {\theta,\omega}\right)+\frac{2d_{2}}{1-d_{2}}+\frac{(1-d_{2}^{x_{2}})\left(d_{2 }+d_{2}^{2}\right)}{(1-d_{2})^{2}}. \tag{105}\]

We next consider two different cases based on the value of \(d_{1}\) and analyze them separately.

**One.**\(0.8\leq d_{1}<1:\) We first notice that the coefficient of \(x_{1}\) in (104) is negative, as \(d_{1}>1/2\). For \(x_{1}\geq 1\), (104) is equal to

\[\frac{1}{\omega(1-d_{1})}\left((2-4d_{1})x_{1}+1-3d_{1}+(d_{1}+d_ {1}^{2})\sum_{i=0}^{x_{1}}d_{1}^{i}\right)\] \[=\frac{1}{\omega(1-d_{1})}\left((2-4d_{1})(x_{1}-1)+d_{1}^{3}(1+d _{1})\sum_{i=0}^{x_{1}-2}d_{1}^{i}+d_{1}(1+d_{1})^{2}+3-7d_{1}\right)\] \[\leq\frac{1}{\omega(1-d_{1})}\left((2-4d_{1})(x_{1}-1)+d_{1}^{3}( 1+d_{1})(x_{1}-1)+d_{1}(1+d_{1})^{2}+3-7d_{1}\right)\] \[=\frac{1}{\omega(1-d_{1})}\left((d_{1}^{4}+d_{1}^{3}-4d_{1}+2)(x_ {1}-1)+d_{1}(1+d_{1})^{2}+3-7d_{1}\right)\] \[=\frac{-d_{1}^{3}-2d_{1}^{2}-2d_{1}+2}{\omega}(x_{1}-1)+\frac{-d_ {1}^{2}-3d_{1}+3}{\omega}\] \[<0,\]

where the third line follows from (103), and the last line from the fact that when \(0.8\leq d_{1}<1\), both terms \(-d_{1}^{3}-2d_{1}^{2}-2d_{1}+2\) and \(-d_{1}^{2}-3d_{1}+3\) are negative. Next, we notice that (105) is equal to

\[x_{2}\left(-\frac{2d_{2}}{1-d_{2}}+\sqrt{\omega+1}\beta^{p}_{ \theta,\omega}\right)+\sqrt{\omega+1}\beta^{p}_{\theta,\omega}+\frac{(1-d_{2}^ {x_{2}})\left(d_{2}+d_{2}^{2}\right)}{(1-d_{2})^{2}}\] \[\leq x_{2}\left(-\frac{2d_{2}}{1-d_{2}}+\sqrt{\omega+1}\beta^{p}_ {\theta,\omega}\right)+\frac{d_{2}+d_{2}^{2}}{1-d_{2}}x_{2}+\sqrt{\omega+1} \beta^{p}_{\theta,\omega}\] \[=x_{2}\left(-\frac{2d_{2}}{1-d_{2}}+\frac{d_{2}+d_{2}^{2}}{1-d_{2 }}+\sqrt{\omega+1}\beta^{p}_{\theta,\omega}\right)+\sqrt{\omega+1}\beta^{p}_ {\theta,\omega}\] \[=x_{2}\left(-d_{2}+\sqrt{\omega+1}\beta^{p}_{\theta,\omega} \right)+\sqrt{\omega+1}\beta^{p}_{\theta,\omega},\]

where the second line follows from (103). Taking \(\beta^{p}_{\theta,\omega}\leq d_{2}/2\sqrt{\omega+1}\), we get

\[x_{2}\left(-\frac{2d_{2}}{1-d_{2}}+\sqrt{\omega+1}\beta^{p}_{ \theta,\omega}\right)+\sqrt{\omega+1}\beta^{p}_{\theta,\omega}+\frac{(1-d_{2} ^{x_{2}})\left(d_{2}+d_{2}^{2}\right)}{(1-d_{2})^{2}}\leq-\frac{d_{2}}{2}x_{2} +\frac{d_{2}}{2},\]

which is non-positive for \(x_{2}\geq 1\). Finally, when \(0.8\leq d_{1}<1\), \(x_{1},x_{2}>0\), and \(x_{1}+1\leq\omega(x_{2}+1)\), (76) holds for \(\beta^{p}_{\theta,\omega}\leq d_{2}/2\sqrt{\omega+1}\).

**Two.**\(d_{1}<0.8\) : Taking \(\beta^{p}_{\theta,\omega}\leq\frac{d_{2}}{\sqrt{\omega+1}(1-d_{2})}\), we note that the coefficient of \(x_{2}\) in (105) is negative. Thus, from \(x_{1}+1\leq\omega(x_{2}+1)\), (104) and (105),

\[\Delta V^{p}_{\theta,\omega}(\mathbf{x})+\sqrt{\omega+1}\beta^{p}_{ \theta,\omega}(x_{2}+1)\] \[\leq\frac{x_{1}+1}{\omega}\left(2-\frac{2d_{1}}{1-d_{1}}\right)- \frac{1}{\omega}+\frac{\left(1-d_{1}^{x_{1}+1}\right)\left(d_{1}+d_{1}^{2} \right)}{\omega(1-d_{1})^{2}}\] \[+\frac{x_{1}+1}{\omega}\left(-\frac{2d_{2}}{1-d_{2}}+\sqrt{ \omega+1}\beta^{p}_{\theta,\omega}\right)+\frac{2d_{2}}{1-d_{2}}+\frac{\left(1 -d_{2}^{x_{2}}\right)\left(d_{2}+d_{2}^{2}\right)}{(1-d_{2})^{2}}\] \[<\frac{x_{1}+1}{\omega}\left(2-\frac{2d_{1}}{1-d_{1}}-\frac{2d_{2 }}{1-d_{2}}+\sqrt{\omega+1}\beta^{p}_{\theta,\omega}\right)+\frac{2d_{2}}{1-d _{2}}+\frac{d_{1}+d_{1}^{2}}{\omega(1-d_{1})^{2}}+\frac{d_{2}+d_{2}^{2}}{(1-d _{2})^{2}}. \tag{106}\]

As \(d_{i}=\tilde{\theta}_{i}/(\tilde{\theta}_{i}+\tilde{\lambda})\) in terms of the normalized rates, we get

\[2-\frac{2d_{1}}{1-d_{1}}-\frac{2d_{2}}{1-d_{2}}=2-\frac{2\tilde{ \theta}_{1}}{\tilde{\lambda}}-\frac{2\tilde{\theta}_{1}}{\tilde{\lambda}}= \frac{-2(\tilde{\theta}_{1}+\tilde{\theta}_{2}-\tilde{\lambda})}{\tilde{ \lambda}},\]

which is negative from the stability condition. For \(\beta^{p}_{\theta,\omega}\leq\frac{\tilde{\theta}_{1}+\tilde{\theta}_{2}- \tilde{\lambda}}{\lambda\sqrt{\omega+1}}\), from (106) we get

\[\Delta V^{p}_{\theta,\omega}(\mathbf{x})+\sqrt{\omega+1}\beta^{p}_{ \theta,\omega}(x_{2}+1)\] \[<\frac{-(\tilde{\theta}_{1}+\tilde{\theta}_{2}-\tilde{\lambda}) }{\omega\tilde{\lambda}}(x_{1}+1)+\frac{2d_{2}}{1-d_{2}}+\frac{d_{1}+d_{1}^{2 }}{\omega(1-d_{1})^{2}}+\frac{d_{2}+d_{2}^{2}}{(1-d_{2})^{2}}\] \[=\frac{-(\tilde{\theta}_{1}+\tilde{\theta}_{2}-\tilde{\lambda}) }{\omega\tilde{\lambda}}(x_{1}+1)+\frac{2\tilde{\theta}_{2}}{\tilde{\lambda}} +\frac{\tilde{\theta}_{1}(2\tilde{\theta}_{1}+\tilde{\lambda})}{\omega\tilde {\lambda}^{2}}+\frac{\tilde{\theta}_{2}(2\tilde{\theta}_{2}+\tilde{\lambda})} {\tilde{\lambda}^{2}},\]

which is non-positive for

\[x_{1}+1\geq\frac{\tilde{\theta}_{1}(2\tilde{\theta}_{1}+\tilde{ \lambda})+\omega\tilde{\theta}_{2}(2\tilde{\theta}_{2}+3\tilde{\lambda})}{ \tilde{\lambda}(\tilde{\theta}_{1}+\tilde{\theta}_{2}-\tilde{\lambda})}.\]

As \(d_{1}<0.8\), we can see that \(\tilde{\lambda}>\tilde{\theta}_{1}/4\); thus,

\[\frac{\tilde{\theta}_{1}(2\tilde{\theta}_{1}+\tilde{\lambda}) +\omega\tilde{\theta}_{2}(2\tilde{\theta}_{2}+3\tilde{\lambda})}{\tilde{ \lambda}(\tilde{\theta}_{1}+\tilde{\theta}_{2}-\tilde{\lambda})}<\frac{4\tilde {\theta}_{1}(2\tilde{\theta}_{1}+\tilde{\lambda})+4\omega\tilde{\theta}_{2}(2 \tilde{\theta}_{2}+3\tilde{\lambda})}{\tilde{\theta}_{1}(\tilde{\theta}_{1}+ \tilde{\theta}_{2}-\tilde{\lambda})}<\frac{4c_{R}R(1+2\tilde{\lambda})}{ \delta}\leq 4c_{R}R,\]

where we have used the fact that \(\tilde{\theta}_{1}\geq\tilde{\theta}_{2}\), \(\omega\leq c_{R}R\), \(\tilde{\theta}_{1}+\tilde{\theta}_{2}-\tilde{\lambda}\geq\delta\), and \(\tilde{\lambda}\leq 0.5-0.5\delta\) and it suffices for \(x_{1}\) to be greater than or equal to \(4c_{R}R\). For \(x_{1}<4c_{R}R\), (104) can be upper bounded as

\[\frac{8c_{R}R}{\omega}+\frac{1-3d_{1}}{\omega(1-d_{1})}+\frac{d_ {1}+d_{1}^{2}}{\omega(1-d_{1})^{2}}\leq\frac{8c_{R}R}{\omega}+\frac{2}{\omega( 1-d_{1})^{2}}<\frac{8c_{R}R+50}{\omega},\]

where in the last inequality we have used \(d_{1}<0.8\). From (105) and taking \(\beta^{p}_{\theta,\omega}\leq d_{2}/2\sqrt{\omega+1}\),

\[\Delta V^{p}_{\theta,\omega}(\mathbf{x})+\sqrt{\omega+1}\beta^{p}_{ \theta,\omega}(x_{2}+1)\] \[\leq\frac{8c_{R}R+50}{\omega}+\left(-\frac{2d_{2}}{1-d_{2}}+ \frac{d_{2}}{2}\right)(x_{2}+1)+\frac{2d_{2}}{1-d_{2}}+\frac{(1-d_{2}^{x_{2}}) \left(d_{2}+d_{2}^{2}\right)}{(1-d_{2})^{2}}\] \[\leq\left(-\frac{2d_{2}}{1-d_{2}}+\frac{d_{2}}{2}+\frac{d_{2}+d_{ 2}^{2}}{1-d_{2}}\right)x_{2}+\frac{d_{2}}{2}+\frac{8c_{R}R+50}{\omega}\] \[=-\frac{d_{2}}{2}x_{2}+\frac{d_{2}}{2}+\frac{8c_{R}R+50}{\omega},\]

which is negative for

\[x_{2}\geq 1+\frac{16c_{R}R+100}{\omega d_{2}}.\]Finally, when \(x_{1}\!+\!1\leq\omega(x_{2}+1)\) and \(x_{1},x_{2}>0\), (76) holds for \(\beta_{\theta,\omega}^{p}\leq\frac{1}{\sqrt{\omega+1}}\min\left(\frac{\tilde{ \theta}_{2}}{2(\tilde{\theta}_{2}+\lambda)},\tilde{\theta}_{1}+\tilde{\theta}_ {2}\!-\!\tilde{\lambda}\right)\), \(x_{1}\geq 4c_{R}R\), and \(x_{2}\geq 1+\frac{16c_{R}R+100}{\omega d_{2}}\). Repeating the same arguments when \(x_{1},x_{2}>0\) and \(x_{1}\!+\!1>\omega(x_{2}+1)\), (76) holds for \(\beta_{\theta,\omega}^{p}\leq\frac{1}{\sqrt{\omega+1}}\min\left(\frac{\tilde{ \theta}_{1}}{2(\tilde{\theta}_{1}+\lambda)},\tilde{\theta}_{1}+\tilde{\theta}_ {2}\!-\!\tilde{\lambda}\right)\), \(x_{1}\geq 1+\frac{\omega(16c_{R}R^{2}+100)}{d_{1}}\), and \(x_{2}\geq 4c_{R}R^{2}\). Finally, (76) holds with

\[V_{\theta,\omega}^{p}(\mathbf{x})=\frac{x_{1}^{2}}{\omega}+x_{2}^{2},\] \[C_{\theta,\omega}^{p}=\left\{(x_{1},x_{2})\in\mathcal{X}:x_{i} \leq\left(16c_{R}^{2}R^{3-i}+101c_{R}R\right)\frac{\lambda+\theta_{i}}{\theta_ {i}},i=1,2\right\},\] \[\beta_{\theta,\omega}^{p}=\min\left(\frac{\tilde{\theta}_{2}}{2( \tilde{\theta}_{2}+\tilde{\lambda})\sqrt{\omega+1}},\frac{\tilde{\theta}_{1}+ \tilde{\theta}_{2}-\tilde{\lambda}}{\sqrt{\omega+1}},\frac{\tilde{\theta}_{2}}{ 2(\tilde{\theta}_{2}+\tilde{\lambda})},\frac{\tilde{\theta}_{1}}{2(\tilde{ \theta}_{1}+\tilde{\lambda})\sqrt{\omega}}\right),\] \[b_{\theta,\omega}^{p}=(\beta_{\theta,\omega}^{p}+1)\max_{\mathbf{x} \in C_{\theta,\omega}^{p}}\left(\frac{(x_{1}+1)^{2}}{\omega}+(x_{2}+1)^{2} \right),\] \[\alpha_{\theta,\omega}^{p}=\frac{1}{2},\]

where the fourth line holds since \(PV_{\theta,\omega}^{p}(\mathbf{x})\leq V_{\theta,\omega}^{p}(\mathbf{y})\) for \(\mathbf{y}=(y_{1},y_{2})\) such that \(y_{i}=x_{i}+1\) for \(i=1,2\). 

## Appendix G Numerical results

### Comparison of Algorithm 1 with other learning algorithms

We first note that due to the countably infinite state-space setting of our problem, we are unable to directly compare our algorithm to other learning algorithms proposed in the literature. One potential candidate algorithm uses the reward biased maximum likelihood estimation (RBMLE) [33, 34, 11, 42], which estimates the unknown model parameter with the likelihood perturbed a vanishing bias towards parameters with a larger long-term average reward (i.e., optimal value). This scheme also uses the principle of "optimism in the face of uncertainty" in how it perturbs the maximum likelihood estimate. The naive version of the RMBLE algorithm does not apply to our examples due the following key assumption: over all parameters (and the control policies used for them), the transition probabilities are assumed to be mutually absolutely continuous; this is critical for the proofs and also allows the use of log-likelihood functions for computations. Similarly, naive use of the algorithms in [36] and [24] is not possible, again due to a similar absolutely continuity assumption which is critical for the proofs. Our posterior computations avoid such issues as the true parameter always has non-zero mass during the execution of the algorithm: episode \(k\) always starts in state \(0^{d}\) which is positive recurrent for the Markov chain with true parameter \(\mathbf{\theta}^{*}\) and policy used \(\pi_{\theta_{k}}^{*}\). The RBMLE algorithm has yet another issue in that it requires knowledge of the optimal value function, and hence, for our examples, it may only apply to Model 1 for which the value function is known analytically. Finally, whereas we do get to observe inter-arrival times for both model, we never directly observe completed service times owing to the sampling employed, and this precludes the direct use of Upper-Confidence-Bound based parameter estimation followed by certainty equivalent control algorithms. Owing to these issues, at this point in time, we're unable to perform empirical comparisons of Algorithm 1 to other candidate algorithms with theoretical performance guarantees in a countable state setting.

As discussed in the previous paragraph, learning algorithms with theoretical performance guarantees are established in the finite state setting. One such algorithm is the certainty equivalence control with forcing, which is proposed and discussed in detail in [4]. To assess the finite-time performance of our algorithm, in Figure 4, we compare the performance of our proposed learning algorithm, denoted as TSDE, with the algorithm introduced in [4], referred to as AgrawalTeneketzis. Reference [4] proposes a certainty equivalence control law with forced exploration, which operates in episodes with increasing lengths and a priori fixed sequences of forcing times. Specifically, at the beginning of each episode, all possible stationary control laws are explored for one recurrence interval of state \((0,0)\). Subsequently, based on this exploration, an empirical estimate of the average collected reward is formed, and the control law resulting in the maximum average reward is implemented for the remainder of the episode. The length of the episodes are determined according to sequence \(\{a_{i}\}_{i=0}^{\infty}\) defined as following:

\[a_{0} =0,\] \[a_{i} =\sum_{k=1}^{i}b_{k}+ip,\qquad\text{ for }i\geq 1,\]

where \(p\) is the number of possible stationary control laws and \(b_{i}=\big{\lfloor}\exp\left(i^{\frac{1}{1+\delta}}\right)\big{\rfloor}\) for any \(\delta>0\). Specifically, episode \(i\) terminates after completing additional \(a_{i}-a_{i-1}\) recurrence intervals to state \((0,0)\). Both algorithms are implemented in the two queueing systems of Figure 2, where the arrival rate is \(\lambda=0.5\) and service rates are distributed according to a Dirichlet prior over \([0.5,1.9]^{2}\). In Figures 3(a) and 3(b), we set \(\delta=3.5\) and \(\delta=3\), respectively. Moreover, in Figure 3(b), the goal is to find the optimal weight \(w\) in the set \(\{1.5,2,2.5,3,3.5\}\). The results in Figure 4 show that both algorithms exhibit a sublinear regret performance. Specifically, Algorithm 1, TSDE, achieves an \(\tilde{O}(\sqrt{T})\) as predicted in our theoretical results of Theorem 1 and Corollary 1. Furthermore, in both queueing models, our proposed algorithm consistently outperforms the algorithm presented in [4] in terms of regret order.

Figure 4: Comparison of the regret performance of Algorithm 1 (referred to as TSDE) with the algorithm proposed by [4] (denoted as AgrawalTeneketzis) for the queueing models of Figure 2.

Figure 5: Total variation distance between the posterior and real distribution for \(\lambda=0.3,0.5,0.7\). The y axis is plotted on a logarithmic scale to display the differences clearly.

### Model 1: Two-server queueing system with a common buffer.

Figure 2(b) illustrates the behavior of the regret of Model 1 for three different arrival rate values and averaged over 2000 simulation runs. In these simulations, the parameter space is selected as

\[\Theta=\left\{(\theta_{1},\theta_{2})\in[0.5,0.6,\ldots,1.9]^{2}:\lambda<\theta_{ 1}+\theta_{2},\theta_{2}<\theta_{1}\right\},\]

which results in a prior size of \(105\). As depicted in Figure 2(a), the regret has a sub-linear behavior and increases with the arrival rate. The total variation distance between the posterior and real distribution, a point-mass on the random \(\mathbf{\theta}^{*}\), are plotted in Figure 4(a). As expected, the distance diminishes towards 0, indicating the learning of the true parameter. As mentioned in Appendix E.1, the optimal policy minimizing the average number of jobs in a system with parameter \(\theta\), is a threshold policy \(\pi_{t(\theta)}\) with optimal finite threshold \(t(\theta)\in\mathbb{N}\), which can be numerically determined as the smallest \(i\in\mathbb{N}\) for which \(J^{i}(\theta)<J^{i+1}(\theta)\), calculated in [38]. We compute the optimal threshold \(t(\theta)\) for every \(\theta\in\Theta\) and present the results in Figure 5(a). We can see that the threshold increases as the ratio of the service rates grows. Specifically, this is why in Appendix G, we imposed conditions on \(\Theta\) to ensure that the ratio between the service rates is both upper and lower bounded.

### Model 2: Two heterogeneous parallel queues

Figure 2(b) illustrates the behavior of the regret of Model 2 for three different arrival rate values and averaged over 2000 simulation runs. We note that the regret is sub-linear and increases with higher arrival rates. In these simulations, the parameter space is selected as

\[\Theta=\left\{(\theta_{1},\theta_{2})\in[0.5,0.7,\ldots,1.9]^{2}:\lambda<\theta _{1}+\theta_{2},\theta_{2}<\theta_{1}\right\},\]

which results in a prior size of \(28\). As discussed earlier, our goal is to find the average cost minimizing policy within the class of policies \(\Pi=\{\pi_{\omega};\omega\in[(c_{R}R)^{-1},c_{R}R]\}\), \(c_{R}\geq 1\), where

Figure 6: Optimal policy parameters for different service rate vectors in the two exemplary queuing systems in Model 1 and Model 2 with \(\lambda=0.5\).

Figure 7: Estimated average cost of Model 2 for three different service rate vectors.

[MISSING_PAGE_FAIL:55]