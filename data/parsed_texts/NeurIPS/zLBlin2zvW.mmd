# Improving Sparse Decomposition of Language Model Activations with Gated Sparse Autoencoders

Senthooran Rajamanoharan Arthur Conmy\({}^{*}\)

Google DeepMind

&Lewis Smith

Google DeepMind

&Tom Lieberum

Google DeepMind

&Vikrant Varma\({}^{\dagger}\)

Google DeepMind

&Janos Kramar

Google DeepMind

&Rohin Shah

Google DeepMind

&Neel Nanda

Google DeepMind

&Nael Nanda

Google DeepMind

###### Abstract

Recent work has found that sparse autoencoders (SAEs) are an effective technique for unsupervised discovery of interpretable features in language models' (LMs) activations, by finding sparse, linear reconstructions of those activations. We introduce the Gated Sparse Autoencoder (Gated SAE), which achieves a Pareto improvement over training with prevailing methods. In SAEs, the L1 penalty used to encourage sparsity introduces many undesirable biases, such as _shrinkage_ - systematic underestimation of feature activations. The key insight of Gated SAEs is to separate the functionality of (a) determining which directions to use and (b) estimating the magnitudes of those directions: this enables us to apply the L1 penalty only to the former, limiting the scope of undesirable side effects. Through training SAEs on LMs of up to 7B parameters we find that, in typical hyper-parameter ranges, Gated SAEs solve shrinkage, are similarly interpretable, and require half as many firing features to achieve comparable reconstruction fidelity.

## 1 Introduction

Mechanistic interpretability aims to explain how neural networks produce outputs in terms of the learned algorithms executed during a forward pass [33, 34]. Much work makes use of the fact that many concept representations appear to be linear [14, 19, 34, 39], i.e. that they correspond to interpretable directions in activation space. However, finding the set of all interpretable directions is a highly non-trivial problem. Classic approaches, like interpreting neurons (i.e. directions in the standard basis) are insufficient, as many are polysemantic and tend to activate for a range of different seemingly unrelated concepts [7, 15, 16]. Within the field, there has recently been much interest [8, 11, 21, 22, 4] in using sparse autoencoders (SAEs; [32]) as an unsupervised method for finding causally relevant, and ideally interpretable, directions in a language model's activations.

Although SAEs show promise in this regard [26, 31], the L1 penalty used in the prevailing training method to encourage sparsity also introduces biases that harm the accuracy of SAE reconstructions, as the loss can be decreased by trading-off some reconstruction accuracy for lower L1. In this paper, we introduce a modification to the baseline SAE architecture - a _Gated SAE_ - along with an accompanying loss function, which partially overcomes these limitations. Our key insight is to use separate affine transformations for (a) determining which dictionary elements to use in a reconstruction and (b) estimating the coefficients of active elements, and to apply the sparsity penalty only to the former task. We share a subset of weights between these transformations to avoid significantlyincreasing the parameter count and inference-time compute requirements of a Gated SAE compared to a baseline SAE of equivalent width.2

Footnote 2: Although due to an auxiliary loss term, computing the Gated SAE loss for training purposes does require 50% more compute than computing the loss for a matched-width baseline SAE.

We evaluate Gated SAEs on multiple models: a one layer GELU activation language model [28], Pythia-2.8B [3] and Gemma-7B [18], and on multiple sites within models: MLP layer outputs, attention layer outputs, and residual stream activations. Across these models and sites, we find Gated SAEs to be a Pareto improvement over baseline SAEs holding training compute fixed (Fig. 1): they yield sparser decompositions at any desired level of reconstruction fidelity. We also conduct further follow up ablations and investigations on a subset of these models and sites to better understand the differences between Gated SAEs and baseline SAEs.

Overall, the key contributions of this work are that we:

1. Introduce the Gated SAE, a modification to the standard SAE architecture that decouples detection of which features are present from estimating their magnitudes (Section 3.2);
2. Show that Gated SAEs Pareto improve the sparsity and reconstruction fidelity trade-off compared to baseline SAEs (Section 4.1);
3. Confirm that Gated SAEs overcome shrinkage while outperforming other methods that also address this problem (Section 5.2);
4. Provide evidence from a small double-blind study that Gated SAE features are comparably interpretable to baseline SAE features (Section 4.2).

## 2 Preliminaries

In this section we summarise the concepts and notation necessary to understand existing SAE architectures and training methods following Bricken et al. [8], which we call the _baseline SAE_. We define Gated SAEs in Section 3.2.

As motivated in Section 1, we wish to decompose a model's activation \(\mathbf{x}\in\mathbb{R}^{n}\) into a sparse, linear combination of feature directions:

\[\mathbf{x}\approx\mathbf{x}_{0}+\sum_{i=1}^{M}f_{i}(\mathbf{x})\mathbf{d}_{i},\] (1)

where \(\mathbf{d}_{i}\) are dictionary of \(M\gg n\) latent unit-norm _feature directions_, and the sparse coefficients \(f_{i}(\mathbf{x})\geq 0\) are the corresponding _feature activations_ for \(\mathbf{x}\).3 The right-hand side of Eq. (1) naturally has the structure of an autoencoder: an input activation \(\mathbf{x}\) is encoded into a (sparse) feature activations vector \(\mathbf{f}(\mathbf{x})\in\mathbb{R}^{M}\), which in turn is linearly decoded to reconstruct \(\mathbf{x}\).

Footnote 3: In this work, we use the term _feature_ to refer only to the _learned features_ of SAEs, i.e. the overcomplete basis directions that are linearly combined to produce reconstructions. In particular, _learned features_ are always linear and not necessarily interpretable.

Figure 1: Gated SAEs consistently offer improved reconstruction fidelity for a given level of sparsity compared to prevailing (baseline) approaches. These plots compare Gated SAEs to baseline SAEs at Layer 20 in Gemma-7B. Gated SAEs’ dictionaries are of size \(2^{17}\approx 131\)k whereas baseline dictionaries are 50% larger, so that both types are trained with equal compute. This performance improvement holds in layers throughout GELU-1L, Pythia-2.8B and Gemma-7B (see Appendix E).

Baseline architectureUsing this correspondence, Bricken et al. [8] and subsequent works attempt to learn a suitable sparse decomposition by parameterizing a single-layer autoencoder \((\mathbf{f},\hat{\mathbf{x}})\) defined by:

\[\mathbf{f}(\mathbf{x}) :=\text{ReLU}\left(\mathbf{W}_{\text{enc}}\left(\mathbf{x}- \mathbf{b}_{\text{dec}}\right)+\mathbf{b}_{\text{enc}}\right)\] (2) \[\hat{\mathbf{x}}(\mathbf{f}) :=\mathbf{W}_{\text{dec}}\mathbf{f}+\mathbf{b}_{\text{dec}}\] (3)

and training it using gradient descent to reconstruct samples \(\mathbf{x}\sim\mathcal{D}\) from a large dataset \(\mathcal{D}\) of activations collected from a single site and layer of a trained language model, constraining the hidden representation \(\mathbf{f}\) to be sparse. Once the sparse autoencoder has been trained, we obtain a decomposition of the form of Eq. (1) by identifying the (suitably normalised) columns of the decoder weight matrix \(\mathbf{W}_{\text{dec}}\in\mathbb{R}^{M\times n}\) with the dictionary of feature directions \(\mathbf{d_{i}}\), the decoder bias \(\mathbf{b}_{\text{dec}}\in\mathbb{R}^{n}\) with the centering term \(\mathbf{x}_{0}\), and the (suitably normalised) entries of the latent representation \(\mathbf{f}(\mathbf{x})\in\mathbb{R}^{M}\) with the feature activations \(f_{i}(\mathbf{x})\).

Baseline training methodologyTo train sparse autoencoders, Bricken et al. [8] use a loss function with two terms that respectively encourage faithful reconstruction and sparsity:4

Footnote 4: Note that we cannot directly optimize the L0 norm (i.e. the number of active features) since this is not a differentiable function. We do however use the L0 norm to evaluate SAE sparsity.

(4)

Since it is possible to arbitrarily reduce the L1 sparsity loss term without affecting reconstructions or sparsity by simply scaling down encoder outputs and scaling up the norm of the decoder weights, it is important to constrain the norms of the columns of \(\mathbf{W}_{\text{dec}}\) during training. Following Bricken et al. [8], we constrain norms to one. See Appendix G for further details on SAE training.

Evaluating SAEsTwo metrics are primarily used to get a sense of SAE quality [8]: _L0_, a measure of SAE sparsity, and _loss recovered_, a measure of SAE reconstruction fidelity. L0 measures the average number of features used by a SAE to reconstruct input activations. Loss recovered is a normalised measure of the increase induced in a LM's cross entropy loss when we replace its original activations with the corresponding SAE reconstructions during the model's forward pass. Both these metrics are formally defined in Appendix B, where we also discuss shortcomings of and alternatives to the loss recovered metric as it is defined in Bricken et al. [8]. Since it is possible for SAEs to score well on these metrics and still fail to be useful for interpretability-related tasks [47], we perform manual analysis of SAE interpretability in Section 4.2.

## 3 Gated SAEs

### Motivation

The intention behind how SAEs are trained is to maximise reconstruction fidelity at a given level of sparsity, as measured by L0, although in practice we optimize a mixture of reconstruction fidelity and L1 regularization. This difference is a source of unwanted bias in the training of a sparse autoencoder: for any fixed level of sparsity, a trained SAE can achieve lower loss (as defined in Eq. (4)) by trading off a little reconstruction fidelity to perform better on the L1 sparsity penalty.

The clearest consequence of this bias is _shrinkage_[51]. Holding the decoder \(\hat{\mathbf{x}}(\bullet)\) fixed, the L1 penalty pushes feature activations \(\mathbf{f}(\mathbf{x})\) towards zero, while the reconstruction loss pushes \(\mathbf{f}(\mathbf{x})\) high enough to produce an accurate reconstruction. Thus, the optimal value falls somewhere in between, and as a result the SAE systematically underestimates the magnitude of feature activations, without necessarily providing any compensatory benefit for sparsity.5

Footnote 5: Conversely, rescaling the shrunk feature activations [51] is not necessarily enough to overcome the bias induced by by L1 penalty: a SAE trained with the L1 penalty could have learnt sub-optimal encoder and decoder directions that are not improved by such a fix. In Section 5.2 and Fig. 7 we provide empirical evidence that this is true in practice.

How can we reduce the bias introduced by the L1 penalty? The output of the encoder \(\mathbf{f}(\mathbf{x})\) of a baseline SAE (Section 2) has two roles:

1. It _detects_ which features are active (according to whether the outputs are zero or strictly positive). For this role, the L1 penalty is necessary to ensure the decomposition is sparse.

2. It _estimates_ the magnitudes of active features. For this role, the L1 penalty is a source of unwanted bias.

If we could separate out these two functions of the SAE encoder, we could design a training loss that narrows down the scope of SAE parameters that are affected (and therefore to some extent biased) by the L1 sparsity penalty to precisely those parameters that are involved in feature detection, minimising its impact on parameters used in feature magnitude estimation.

### Gated SAEs

ArchitectureHow should we modify the baseline SAE encoder to achieve this separation of concerns? Our solution is to replace the single-layer ReLU encoder of a baseline SAE with a _gated_ ReLU encoder. Taking inspiration from Gated Linear Units [43, 12], we define the gated encoder as

\[\tilde{\mathbf{f}}(\mathbf{x}):=\underbrace{\mathbbm{1}[\left(\overline{ \mathbf{W}_{\text{gate}}(\mathbf{x}-\mathbf{b}_{\text{dec}})+\mathbf{b}_{ \text{gate}}}\right)>\mathbf{0}]}_{\mathbf{f}_{\text{end}}(\mathbf{x})} \odot\underbrace{\text{ReLU}(\mathbf{W}_{\text{mag}}(\mathbf{x}-\mathbf{b}_{ \text{dec}})+\mathbf{b}_{\text{mag}})}_{\mathbf{f}_{\text{end}}(\mathbf{x})},\] (5)

where \(\mathbbm{1}[\bullet>\mathbf{0}]\) is the (pointwise) Heaviside step function and \(\odot\) denotes elementwise multiplication. Here, \(\mathbf{f}_{\text{gate}}\) determines which features are deemed to be active, while \(\mathbf{f}_{\text{mag}}\) estimates feature activation magnitudes (which only matter for features that have been deemed to be active); \(\mathbf{\pi}_{\text{gate}}(\mathbf{x})\) are the \(\mathbf{f}_{\text{gate}}\) sub-layer's pre-activations, which are used in the gated SAE loss, defined below.

TrainingA naive guess at a loss function for training Gated SAEs would be to replace the sparsity penalty in Eq. (4) with the L1 norm of \(\mathbf{f}_{\text{gate}}(\mathbf{x})\). Unfortunately, due to the Heaviside step activation function in \(\mathbf{f}_{\text{gate}}\), no gradients would propagate to \(\mathbf{W}_{\text{gate}}\) and \(\mathbf{b}_{\text{gate}}\). To mitigate this, we instead apply the L1 norm to the positive parts of the preactivation, \(\text{ReLU}\left(\mathbf{\pi}_{\text{gate}}(\mathbf{x})\right)\). To ensure \(\mathbf{f}_{\text{gate}}\) aids reconstruction by detecting active features, we add an auxiliary task requiring that these same rectified preactivations can be used by the decoder to produce a good reconstruction:

\[\mathcal{L}_{\text{gated}}(\mathbf{x}):=\underbrace{\left\|\mathbf{x}-\hat{ \mathbf{x}}\left(\tilde{\mathbf{f}}(\mathbf{x})\right)\right\|_{2}^{2}}_{ \mathcal{L}_{\text{reconstruct}}}+\underbrace{\lambda\left\|\text{ReLU}( \mathbf{\pi}_{\text{gate}}(\mathbf{x}))\right\|_{1}}_{\mathcal{L}_{\text{ capacity}}}+\underbrace{\left\|\mathbf{x}-\hat{\mathbf{x}}_{\text{frozen}}\left(\text{ReLU}\left(\mathbf{\pi}_{\text{ gate}}(\mathbf{x})\right)\right)\right\|_{2}^{2}}_{\mathcal{L}_{\text{max}}}\] (6)

where \(\hat{\mathbf{x}}_{\text{frozen}}\) is a frozen copy of the decoder, \(\hat{\mathbf{x}}_{\text{frozen}}(\mathbf{f}):=\mathbf{W}_{\text{dec}}^{ \text{copy}}\mathbf{f}+\mathbf{b}_{\text{dec}}^{\text{copy}}\), to ensure that gradients from \(\mathcal{L}_{\text{aux}}\) do not propagate back to \(\mathbf{W}_{\text{dec}}\) or \(\mathbf{b}_{\text{dec}}\). This can be implemented by stop gradient operations rather than creating copies. See Appendix J for pseudo-code for the forward pass and loss function.

To calculate this loss (or its gradient), we have to run the decoder twice: once to perform the main reconstruction for \(\mathcal{L}_{\text{reconstruct}}\) and once to perform the auxiliary reconstruction for \(\mathcal{L}_{\text{aux}}\). This leads to a 50% increase in the compute required to perform a training update step. However, the increase in overall training time is typically much less, as in our experience much of the training wall clock time goes to generating language model activations (if these are being generated on the fly) or disk I/O (if training on saved activations).

Figure 2: The Gated SAE architecture with weight sharing between the gating and magnitude paths, shown with an example input. See Appendix J for a pseudo-code implementation.

Parameter reduction through weight-tyingNaively, we appear to have doubled the number of parameters in the encoder, increasing the total number of parameters by 50% with respect to baseline SAEs. We mitigate this through weight sharing: we parameterize these layers so that the two layers share the same projection directions, but allow the norms of these directions as well as the layer biases to differ. Concretely, we define \(\mathbf{W}_{\text{mag}}\) in terms of \(\mathbf{W}_{\text{gate}}\) and an additional vector-valued rescaling parameter \(\mathbf{r}_{\text{mag}}\in\mathbb{R}^{M}\) as follows:

\[\left(\mathbf{W}_{\text{mag}}\right)_{ij}:=\left(\exp(\mathbf{r}_{\text{mag}} )\right)_{i}\cdot\left(\mathbf{W}_{\text{gate}}\right)_{ij}.\] (7)

See Fig. 2 for an illustration of the tied-weight Gated SAEs architecture. With this weight tying scheme, the Gated SAE has only \(2\times M\) more parameters than a baseline SAE. In Section 5.1, we show that this weight tying scheme does not harm performance.

With tied weights, the gated encoder can be reinterpreted as a single-layer linear encoder with a non-standard and discontinuous "JumpReLU" activation function [17], \(\sigma_{\theta}(z)\), illustrated in Fig. 12. To be precise, using the weight tying scheme of Eq. (7), \(\tilde{\mathbf{f}}(\mathbf{x})\) can be re-expressed as \(\tilde{\mathbf{f}}(\mathbf{x})=\sigma_{\boldsymbol{\theta}}(\mathbf{W}_{\text {mag}}\cdot\mathbf{x}+\mathbf{b}_{\text{mag}})\), with the JumpReLU gap given by \(\boldsymbol{\theta}=\mathbf{b}_{\text{mag}}-e^{\mathbf{r}_{\text{mag}}} \odot\mathbf{b}_{\text{gate}}\); see Appendix H for an explanation. We think this is a useful intuition for reasoning about how Gated SAEs reconstruct activations in practice.

## 4 Evaluating Gated SAEs

In this section we benchmark Gated SAEs against baseline SAEs across a large variety of models and at different sites. We show that they produce more faithful reconstructions at equal sparsity and that they resolve shrinkage. Through a double-blind manual interpretability study, we find that Gated SAEs produce features that are similarly interpretable to baseline SAE features.

### Benchmarking Gated SAEs

MethodologyWe trained a suite of Gated and baseline SAEs, a family of each type to reconstruct each of the following activations:

1. The MLP neuron activations in GELU-1L, which is the closest direct comparison to Bricken et al. [8];
2. The MLP outputs, attention layer outputs (taken pre-\(W_{O}\)[21]) and residual stream activations in 5 different layers throughout Pythia-2.8B and four different layers in the Gemma-7B base model.

For each model and reconstruction site, we trained multiple SAEs using different values of \(\lambda\) (and therefore L0), allowing us to compare the Pareto frontiers of L0 and loss recovered between Gated and baseline SAEs. We also use the _relative reconstruction bias_ metric, \(\gamma\), defined in Appendix C to measure shrinkage in our trained SAEs. This metric measures the relative bias in the norm of an SAE's reconstructions; unbiased SAEs obtain \(\gamma=1\), whereas SAEs affected by shrinkage (which causes reconstruction norms to be systematically too small) have \(\gamma<1\).

Since Gated SAEs require at most 1.5\(\times\) more compute to train than regular SAEs (Section 3.2) of the same width, we compare Gated SAEs to baseline SAEs that have a 50% larger dictionary (hidden dimension \(M\)) to ensure fair comparison in our evaluations.6

Footnote 6: Since wider SAEs provide better reconstructions (all else being equal), the gap between Gated SAEs’ and baseline SAEs’ performance is even wider when we use baseline SAEs with equal width in the comparison. This can be seen in the difference between the “\(1.5\times\) width” and “equal width” baseline curves in Fig. 5.

ResultsWe plot sparsity against reconstruction fidelity for SAEs with different values of \(\lambda\). Higher \(\lambda\) corresponds to increased sparsity and worse reconstruction, so as in Bricken et al. [8] we observe a Pareto frontier of possible trade-offs. We plot Pareto curves for GELU-1L in Fig. 2(a) and Pythia-2.8B and Gemma-7B in Appendix E. At all sites tested, Gated SAEs are a Pareto improvement over regular SAEs: they provide better reconstruction fidelity at any fixed level of sparsity.7 For some sites in Pythia-2.8B and Gemma-7B, loss recovered does not monotonically increase with L0; we attribute this to difficulties training SAEs (Appendix G.1.3). Finally, full tables of results for Pythia and Gemma can be found in Appendix K.

Footnote 7: Although both Gated and baseline SAEs have loss recovered tending to one for high enough L0.

As shown in Fig. 2(b), Gated SAEs' reconstructions are unbiased, with \(\gamma\approx 1\), whereas baseline SAEs exhibit shrinkage (\(\gamma<1\)), with the impact of shrinkage getting worse as the L1 coefficient \(\lambda\) increases (and L0 consequently decreases). Fig. 10 shows that this result generalizes to Pythia-2.8B.

### Interpretability

Although Gated SAEs provide more faithful reconstructions than baselines at equal sparsity, it does not necessarily follow that these reconstructions are better suited to downstream interpretability-related tasks. Currently, there is no consensus on how to systematically assess the degree to which a SAE's features are useful for downstream tasks, but a plausible proxy is to assess the extent to which these features are human interpretable [8]. Therefore, to gain a more qualitative understanding of the differences between their learned features, we conduct a blinded human study in which we rate and compare the interpretability of randomly sampled Gated and baseline SAE features.

MethodologyWe study a variety of SAEs from different layers and sites. For Pythia-2.8B we had 5 raters, who each rated one feature from baseline and Gated SAEs trained on each (site, layer) pair from Fig. 8, for a total of 150 features. For Gamma-7B we had 7 raters; one rated 2 features each, and the rest 1 feature each, from baseline or Gated SAEs trained on each (site, layer) pair from Fig. 9, for a total of 192 features.

For each model, raters are shown the features in random order, without revealing which SAE, site, or layer they came from.8 To assess a feature, the rater decides whether there is an explanation of the feature's behavior, in particular for its highest activating examples. The rater then enters that explanation (if applicable) and selects whether the feature is interpretable ('Yes'), uninterpretable ('No') or maybe interpretable ('Maybe'). All raters are either authors of this paper or colleagues, who have prior experience interpreting SAE features. As an interface we use an open source SAE visualizer library [27]; representative screenshots of the dashboards produced by this library are shown in Fig. 14.

Footnote 8: Although due to a debugging issue, Gamma-7B attention SAEs were rated separately, so raters were not blind to that.

Results & analysisFig. 4 shows interpretability rating distributions by SAE type and LM, marginalising over layers, sites and raters.9 To compare the interpretability of baseline and Gated SAEs, we first pair our datapoints according to all covariates (model, layer, site, rater); this lets us

Figure 3: (a) Gated SAEs offer better reconstruction fidelity (as measured by loss recovered) at any given level of feature sparsity (as measured by L0); (b) Gated SAEs address shrinkage. These plots compare Gated and baseline SAEs trained on GELU-1L neuron activations; see Appendix E for comparisons on Pythia-2.8B and Gamma-7B.

control for all of them without making any parametric assumptions, and thus reduces variance in the comparison. We then measure the mean difference between baseline and Gated labels, where we count 'No' as 0, 'Maybe' as 1, and 'Yes' as 2, and compute a 90% BCa bootstrap confidence interval. Thus we find that the mean difference in label scores is \(0.13\) (90% CI \([0,0.26]\)) in favour of Gated SAEs, breaking down to mean difference CIs of \([-0.07,0.33]\) and \([-0.04,0.29]\) on just the Pythia-2.8B data and Gemma-7B data respectively. Since our central estimate for the mean difference in scores is positive, we also test the hypothesis that Gated SAEs may be _more_ interpretable than baseline SAEs. However, a one-sided Wilcoxon-Pratt signed-rank test on the paired scores does not reject the null hypothesis that they are equally interpretable (\(p=0.06\)). The contingency tables used for these results are shown in Fig. 13. The overall conclusion is that Gated SAE features are similarly interpretable to baseline SAE features, while also providing better reconstruction fidelity (at fixed sparsity), as shown in the previous section. We provide more analysis of how these break down by site and layer in Appendix I.

## 5 Why do Gated SAEs improve SAE training?

### Ablation study

In this section, we vary several parts of the Gated SAE training methodology to gain insight into which aspects of the training drive the observed improvement in performance. Gated SAEs differ from baseline SAEs in many respects, making it easy to incorrectly attribute the performance gains to spurious details without a careful ablation study. Fig. 5a shows Pareto frontiers for these variations; below we describe each variation in turn and discuss our interpretation of the results.

**Unfreeze decoder**: Here we unfreeze the decoder weights in \(\mathcal{L}_{\text{aux}}\) - i.e. allow this auxiliary task to update the decoder weights in addition to training \(\mathbf{f}_{\text{gate}}\)'s parameters. Although this (slightly) simplifies the loss, there is a reduction in performance, suggesting that it is beneficial to limit the impact of the L1 sparsity penalty to just those parameters in the SAE that need it - i.e. those used to detect which features are active.

**No \(\mathbf{r_{mag}}\)**: Here we remove the \(\mathbf{r}_{\text{mag}}\) scaling parameter in Eq. (7), effectively setting it to zero, further tying \(\mathbf{f}_{\text{gate}}\)'s and \(\mathbf{f}_{\text{mag}}\)'s parameters together. With this change, the two encoder sublayers' preactivations can at most differ by an elementwise shift.10 There is a slight drop in performance, suggesting \(\mathbf{r}_{\text{mag}}\) contributes somewhat to the improved performance of the Gated SAE.

Footnote 10: Because the two biases \(\mathbf{b}_{\text{gate}}\) and \(\mathbf{b}_{\text{mag}}\) can still differ.

**Untied encoders**: Here we check whether our choice to share the majority of parameters between the two encoders has meaningfully hurt performance, by training Gated SAEs with gating and ReLU encoder parameters completely untied. Despite the greater expressive power of an untied encoder, we see no improvement in performance - in fact a slight deterioration. This suggests our tying scheme (Eq. (7)) - where encoder directions are shared, but magnitudes and biases aren't - is effec

Figure 4: Proportions of SAE features rated as interpretable / uninterpretable / maybe interpretable by SAE type (Gated or baseline) and language model. Gated and baseline SAEs are similarly interpretable, with a mean difference (in favor of Gated SAEs) of 0.13 (95% CI \([0,0.26]\)) after aggregating ratings for both models.

tive at capturing the advantages of using a gated SAE while avoiding the 50% increase in parameter count and inference-time compute of using an untied SAE.

### Is it sufficient to just address shrinkage?

As explained in Section 3.1, SAEs trained with the baseline architecture and L1 loss systematically underestimate the magnitudes of latent features' activations (i.e. shrinkage). Gated SAEs, through modifications to their architecture and loss function, overcome these limitations.

It is natural to ask to what extent the performance improvement of Gated SAEs is solely attributable to addressing shrinkage. Although addressing shrinkage would - all else staying equal - improve reconstruction fidelity, it is not the only way to improve SAEs' performance: for example, Gated SAEs could also improve upon baseline SAEs by learning better encoder directions (for estimating when features are active and their magnitudes) or by learning better decoder directions (i.e. better dictionaries for reconstructing activations).

Here we try to answer this question by comparing Gated SAEs trained as described in Section 3.2 with an alternative (architecturally equivalent) approach that also addresses shrinkage, but in a way that uses frozen encoder and decoder directions from a baseline SAE of equal dictionary size.11 Any performance improvement over baseline SAEs obtained by this alternative approach (which we dub "baseline + rescale & shift") can only be due to better estimations of active feature magnitudes, since by construction an SAE parameterized by "baseline + rescale & shift" shares the same encoder and decoder directions as a baseline SAE.

Footnote 11: Concretely, we do this by training baseline SAEs, freezing their weights, and then learning additional rescale and shift parameters (similar to Wright and Sharkey [51]) to be applied to the (frozen) encoder pre-activations before estimating feature magnitudes.

As shown in Fig. 4(b), although resolving shrinkage only ("baseline + rescale & shift") does improvement baseline SAEs' performance a little, a significant gap remains with respect to the performance of Gated SAEs. This suggests that the benefit of the gated architecture and loss comes from learning better encoder and decoder directions, not just from overcoming shrinkage. In Appendix D we ex

Figure 5: (a) Our ablation study on GELU-1L MLP neuron activations indicates: (i) the importance of freezing the decoder in the auxiliary task \(\mathcal{L}_{\text{aux}}\) used to train \(\mathbf{f}_{\text{gate}}\)’s parameters; (ii) tying encoder weights according to Eq. (7) is slightly beneficial for performance (in addition to yielding a significant reduction in parameter count and inference compute); (iii) further simplifying the encoder weight tying scheme in Eq. (7) by removing \(\mathbf{r}_{\text{mag}}\) is mildly harmful to performance. (b) Evidence from GELU-1L that the performance improvement of gated SAEs does not solely arise from addressing shrinkage (systematic underestimation of latent feature activations): taking a frozen baseline SAE’s parameters and learning \(\mathbf{r}_{\text{mag}}\) and \(\mathbf{b}_{\text{mag}}\) parameters on top of them (green line) does successfully resolve shrinkage, by decoupling feature magnitude estimation from active feature detection; however, it explains only a small part of the performance increase of gated SAEs (red line) over baseline SAEs (blue line).

plore further how Gated and baseline SAEs' decoders differ by replacing their respective encoders with an optimization algorithm at inference time.

## 6 Related work

Mechanistic interpretabilityRecent work in mechanistic interpretability has found recurring components in small and large LMs [38], identified computational subgraphs that carry out specific tasks in small LMs (circuits; [50]) and reverse-engineered how toy tasks are carried out in small transformers [30]. A central difficulty in this kind of work is choosing the right units of analysis. Sparse linear features have been identified as a promising candidate in prior work [52; 46]. The superposition hypothesis outlined by Elhage et al. [16] also provided a theoretical basis for this theory, sparking a new interest in using SAEs specifically to learn a feature basis [42; 8; 11; 21; 22; 4], as well as using SAEs directly for circuit analysis [26]. Other work has drawn awareness to issues or drawbacks with SAE training for this purpose, some of which our paper mitigates. Wright and Sharkey [51] raised awareness of shrinkage and proposed addressing this via fine-tuning. Gated SAEs, as discussed, resolve shrinkage during training. [35; 47; 2; 36] have also proposed general SAE training methodology improvements, which are mostly orthogonal to the architectural changes discussed in this work. In parallel work, Taggart [45] finds early improvements using a Jump ReLU [17], but with a different loss function, and without addressing the problems of the L1 penalty.

Classical dictionary learningResearch into the general problem of sparse dictionary learning precedes transformers, and even deep learning. For example, sparse coding [13] studies how discrete and continuous representations can involve more representations than basis vectors, and sparse representations are also studied in neuroscience [48; 37]. One dictionary learning algorithm, k-SVD [1] also uses two stages to learn a dictionary like Gated SAEs. Although classical dictionary learning algorithms can be more powerful than SAEs (Appendix D), they are less suited for downstream uses like weights-based circuit analysis or attribution patching [44; 24], because they typically use an iterative algorithm to decompose activations, whereas SAEs make feature extraction explicit via the encoder. Bricken et al. [8] have also argued that classical algorithms may be 'too strong', in the sense they may learn features the LM itself could not access, whereas SAEs uses components similar to a LM's MLP layer to decompose activations.

## 7 Conclusion

In this work we introduced Gated SAEs which are a Pareto improvement in terms of reconstruction quality and sparsity compared to baseline SAEs (Section 4.1), and are comparably interpretable (Section 4.2). We showed via an ablation study that every key part of the Gated SAE methodology was necessary for strong performance (Section 5.1). This represents significant progress on improving Dictionary Learning on LMs - at many sites, Gated SAEs require half the L0 to achieve the same loss recovered (Fig. 8). This is likely to improve work that uses SAEs to steer language models [31], interpret circuits [26], or understand LM components across the full distribution [8].

**Limitations & future work**. Our benchmarking study focused on GELU-1L and models in the Pythia and Gamma families. It is therefore not certain that these results will generalise to other model families. On the other hand, the theoretical underpinnings of the Gated SAE architecture (Section 3) make no assumptions about LM architecture, suggesting Gated SAEs should be a Pareto improvement more generally. While we have confirmed that Gated SAE features are comparably interpretable to baseline SAE features, it does not necessarily follow that Gated SAE decompositions are equally useful for mechanistic interpretability. It is certainly possible that human interpretability of SAE features is only weakly correlated with either: (i) identification of the causally meaningful directions in a LM's activations; or (ii) usefulness on downstream tasks like circuit analysis or steering. A framework for scalably and objectively evaluating the usefulness of SAE decompositions (gated or otherwise) is still in its early stages [25] and further progress in this area would be highly valuable. It is plausible that some of the performance gap between Gated and baseline SAEs could be closed by inexpensive inference-time interventions that prune the many low activating features that tend to appear in baseline SAEs, mimicking Gated SAEs' thresholding mechanism. Finally, we would be most excited to see progress on using dictionary learning techniques to further inter pretability in general, such as to improve circuit finding [10, 26] or steering [49] in language models, and hope that Gated SAEs can serve to accelerate such work.

## Acknowledgements

We would like to thank Romeo Valentin for conversations that got us thinking about k-SVD in the context of SAEs, which inspired part of our work. Additionally, we are grateful for Vladimir Mikulik's detailed feedback on a draft of this work which greatly improved our presentation, and Nicholas Sonnerat's work on our codebase and help with feature labelling. We would also like to thank Glen Taggart who found in parallel work [45] that a similar method gave improvements to SAE training, helping give us more confidence in our results. We are grateful to Sam Marks for pointing out an error in the derivation of relative reconstruction bias in an earlier version of this paper and Leo Gao and Goncalo Paulo for discussions. Finally, we thank our anonymous reviewers for their valuable feedback, which helped improve the quality of this paper.

## Author contributions

Senthooran Rajamanoharan developed the Gated SAE architecture and training methodology, inspired by discussions with Lewis Smith on the topic of shrinkage. Arthur Conmy and Senthooran Rajamanoharan performed the mainline experiments in Section 4 and Section 5 and led the writing of all sections of the paper. Tom Lieberum implemented the manual interpretability study of Section 4.2, which was designed and analysed by Janos Kramar. Tom Lieberum also created Fig. 2 and Lewis Smith contributed Appendix D. Our SAE codebase was designed by Vikrant Varma who implemented it with Tom Lieberum, and was scaled to Gemma by Arthur Conmy, with contributions from Senthooran Rajamanoharan and Lewis Smith. Janos Kramar built most of our underlying interpretability infrastructure. Rohin Shah and Neel Nanda edited the manuscript and provided leadership and advice throughout the project.

## References

* [1] M. Aharon, M. Elad, and A. Bruckstein. K-SVD: An algorithm for designing overcomplete dictionaries for sparse representation. _IEEE Transactions on Signal Processing_, 54(11):4311-4322, 2006. doi: 10.1109/TSP.2006.881199.
* March 2024. _Transformer Circuits Thread_, 2024. URL https://transformer-circuits.pub/2024/mar-update/index.html.
* [3] S. Biderman, H. Schoelkopf, Q. G. Anthony, H. Bradley, K. O'Brien, E. Hallahan, M. A. Khan, S. Purohit, U. S. Prashanth, E. Raff, et al. Pythia: A suite for analyzing large language models across training and scaling. In _International Conference on Machine Learning_, pages 2397-2430. PMLR, 2023. Apache License 2.0.
* [4] J. Bloom. Open Source Sparse Autoencoders for all Residual Stream Layers of GPT-2 Small, 2024. https://www.alignmentforum.org/posts/f9EgfLSurAiqRJySD/open-source-sparse-autoencoders-for-all-residual-stream.
* [5] J. Bloom. SAELens, 2024. https://github.com/jbloomAus/SAELens.
* [6] T. Blumensath and M. E. Davies. Gradient pursuits. _IEEE Transactions on Signal Processing_, 56(6):2370-2382, 2008.
* [7] T. Bolukbasi, A. Pearce, A. Yuan, A. Coenen, E. Reif, F. Viegas, and M. Wattenberg. An interpretability illusion for bert. _arXiv preprint arXiv:2104.07143_, 2021.
* [8] T. Bricken, A. Templeton, J. Batson, B. Chen, A. Jermyn, T. Conerly, N. Turner, C. Anil, C. Denison, A. Askell, R. Lasenby, Y. Wu, S. Kravec, N. Schiefer, T. Maxwell, N. Joseph, Z. Hatfield-Dodds, A. Tamkin, K. Nguyen, B. McLean, J. E. Burke, T. Hume, S. Carter,T. Henighan, and C. Olah. Towards monosemanticity: Decomposing language models with dictionary learning. _Transformer Circuits Thread_, 2023. https://transformercircuits.pub/2023/monosemantic-features/index.html.
* [9] A. Conmy. My best guess at the important tricks for training 1L SAEs, Dec 2023. https://www.lesswrong.com/posts/yJsLNWtmzcgPJgvro/my-best-guess-at-the-important-tricks-for-training-1l-saes.
* [10] A. Conmy, A. N. Mavor-Parker, A. Lynch, S. Heimersheim, and A. Garriga-Alonso. Towards automated circuit discovery for mechanistic interpretability, 2023.
* [11] H. Cunningham, A. Ewart, L. Riggs, R. Huben, and L. Sharkey. Sparse autoencoders find highly interpretable features in language models, 2023.
* Volume 70_, ICML'17, page 933-941. JMLR.org, 2017.
* [13] M. Elad. _Sparse and Redundant Representations: From Theory to Applications in Signal and Image Processing_. Springer, New York, 2010. ISBN 978-1-4419-7010-7. doi: 10.1007/978-1-4419-7011-4.
* [14] N. Elhage, N. Nanda, C. Olsson, T. Henighan, N. Joseph, B. Mann, A. Askell, Y. Bai, A. Chen, T. Conerly, N. DasSarma, D. Drain, D. Ganguli, Z. Hatfield-Dodds, D. Hernandez, A. Jones, J. Kernion, L. Lovitt, K. Ndousse, D. Amodei, T. Brown, J. Clark, J. Kaplan, S. McCandlish, and C. Olah. A mathematical framework for transformer circuits. _Transformer Circuits Thread_, 2021. URL https://transformer-circuits.pub/2021/framework/index.html.
* [15] N. Elhage, T. Hume, C. Olsson, N. Nanda, T. Henighan, S. Johnston, S. ElShowk, N. Joseph, N. DasSarma, B. Mann, D. Hernandez, A. Askell, K. Ndousse, A. Jones, D. Drain, A. Chen, Y. Bai, D. Ganguli, L. Lovitt, Z. Hatfield-Dodds, J. Kernion, T. Conerly, S. Kravec, S. Fort, S. Kadavath, J. Jacobson, E. Tran-Johnson, J. Kaplan, J. Clark, T. Brown, S. McCandlish, D. Amodei, and C. Olah. Softmax linear units. _Transformer Circuits Thread_, 2022. https://transformer-circuits.pub/2022/solu/index.html.
* [16] N. Elhage, T. Hume, C. Olsson, N. Schiefer, T. Henighan, S. Kravec, Z. Hatfield-Dodds, R. Lasenby, D. Drain, C. Chen, et al. Toy Models of Superposition. _arXiv preprint arXiv:2209.10652_, 2022.
* [17] N. B. Erichson, Z. Yao, and M. W. Mahoney. Jumprelu: A retrofit defense strategy for adversarial attacks, 2019.
* [18] Gemma Team, T. Mesnard, C. Hardin, R. Dadashi, S. Bhupatiraju, L. Sifre, M. Riviere, M. S. Kale, J. Love, P. Tafti, L. Hussenot, and et al. Gemma, 2024. URL https://www.kaggle.com/m/3301. Apache License 2.0.
* [19] W. Gurnee, N. Nanda, M. Pauly, K. Harvey, D. Troitskii, and D. Bertsimas. Finding neurons in a haystack: Case studies with sparse probing, 2023.
* [20] N. P. Jouppi, D. H. Yoon, G. Kurian, S. Li, N. Patil, J. Laudon, C. Young, and D. Patterson. A domain-specific supercomputer for training deep neural networks. _Communications of the ACM_, 63(7):67-78, 2020.
* [21] C. Kissane, R. Krzyzanowski, A. Conmy, and N. Nanda. Sparse autoencoders work on attention layer outputs. Alignment Forum, 2024. https://www.alignmentforum.org/posts/DtdzGwFh9dCfsekZZ.
* [22] C. Kissane, R. Krzyzanowski, A. Conmy, and N. Nanda. Attention SAEs scale to GPT-2 Small. Alignment Forum, 2024. https://www.alignmentforum.org/posts/FSTRedtjuHa4Gfdbr.
* [23] K. Konda, R. Memisevic, and D. Krueger. Zero-bias autoencoders and the benefits of co-adapting features, 2015. URL https://arxiv.org/abs/1402.3337.

* Kramar et al. [2024] J. Kramar, T. Lieberum, R. Shah, and N. Nanda. Atp*: An efficient and scalable method for localizing llm behaviour to components. _arXiv preprint arXiv:2403.00745_, 2024.
* Makelov et al. [2024] A. Makelov, G. Lange, and N. Nanda. Towards principled evaluations of sparse autoencoders for interpretability and control. In _ICLR 2024 Workshop on Secure and Trustworthy Large Language Models_, 2024. URL https://openreview.net/forum?id=MHIX9H8aYF.
* Marks et al. [2024] S. Marks, C. Rager, E. J. Michaud, Y. Belinkov, D. Bau, and A. Mueller. Sparse feature circuits: Discovering and editing interpretable causal graphs in language models, 2024.
* McDougall [2024] C. McDougall. SAE Visualizer, 2024. https://github.com/callummcdougall/sae_vis.
* Nanda [2022] N. Nanda. GELU-1L, 2022. URL https://huggingface.co/NeelNanda/GELU_1L512W_C4_Code. MIT License.
* Nanda [2023] N. Nanda. Open Source Replication & Commentary on Anthropic's Dictionary Learning Paper, Oct 2023. https://www.alignmentform.org/posts/aPTgTKC45dWvL9XBF/open-source-replication-and-commentary-on-anthropic-s.
* Nanda et al. [2023] N. Nanda, L. Chan, T. Lieberum, J. Smith, and J. Steinhardt. Progress measures for grokking via mechanistic interpretability. In _The Eleventh International Conference on Learning Representations_, 2023. URL https://openreview.net/forum?id=9XFSbDPmdW.
* Nanda et al. [2024] N. Nanda, A. Conmy, L. Smith, S. Rajamanoharan, T. Lieberum, J. Kramar, and V. Varma. [Summary] Progress Update #1 from the GDM Mech Interp Team. Alignment Forum, 2024. https://www.alignmentforum.org/posts/HpAr8k74mW4ivCvCu/summary-progress-update-1-from-the-gdm-mech-interp-team.
* Ng [2011] A. Ng. Sparse autoencoder, 2011. CS294A Lecture notes, http://web.stanford.edu/class/cs294a/sparseAutoencoder.pdf.
* Olah [2022] C. Olah. Mechanistic interpretability, variables, and the importance of interpretable bases, 2022. https://www.transformer-circuits.pub/2022/mech-interp-essay.
* Olah et al. [2020] C. Olah, N. Cammarata, L. Schubert, G. Goh, M. Petrov, and S. Carter. Zoom in: An introduction to circuits. _Distill_, 2020. doi: 10.23915/distill.00024.001.
* January 2024. _Transformer Circuits Thread_, 2024. URL https://transformer-circuits.pub/2024/jan-update/index.html.
* April 2024. _Transformer Circuits Thread_, 2024. URL https://transformer-circuits.pub/2024/april-update/index.html.
* Olshausen and Field [1997] B. A. Olshausen and D. J. Field. Sparse coding with an overcomplete basis set: A strategy employed by v1? _Vision Research_, 37(23):3311-3325, 1997. doi: 10.1016/S0042-6989(97)00169-7.
* Olsson et al. [2022] C. Olsson, N. Elhage, N. Nanda, N. Joseph, N. DasSarma, T. Henighan, B. Mann, A. Askell, Y. Bai, A. Chen, T. Conerly, D. Drain, D. Ganguli, Z. Hatfield-Dodds, D. Hernandez, S. Johnston, A. Jones, J. Kernion, L. Lovitt, K. Ndousse, D. Amodei, T. Brown, J. Clark, J. Kaplan, S. McCandlish, and C. Olah. In-context learning and induction heads. _Transformer Circuits Thread_, 2022. https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html.
* Park et al. [2023] K. Park, Y. J. Choe, and V. Veitch. The linear representation hypothesis and the geometry of large language models, 2023.
* Pati et al. [1993] Y. Pati, R. Rezaiifar, and P. Krishnaprasad. Orthogonal matching pursuit: recursive function approximation with applications to wavelet decomposition. In _Proceedings of 27th Asilomar Conference on Signals, Systems and Computers_, pages 40-44 vol.1, 1993. doi: 10.1109/ACSSC.1993.342465.

* Rajamanoharan et al. [2024] S. Rajamanoharan, T. Lieberum, N. Sonnerat, A. Conmy, V. Varma, J. Kramar, and N. Nanda. Jumping ahead: Improving reconstruction fidelity with jumprelu sparse autoencoders, 2024. URL https://arxiv.org/abs/2407.14435.
* Sharkey et al. [2022] L. Sharkey, D. Braun, and B. Millidge. [interim research report] taking features out of superposition with sparse autoencoders, 2022. https://www.alignmentforum.org/posts/z6QQJbtpkEAX3Aojj/interim-research-report-taking-features-out-of-superposition.
* Shazeer [2020] N. Shazeer. GLU variants improve transformer. _CoRR_, abs/2002.05202, 2020. URL https://arxiv.org/abs/2002.05202.
* Syed et al. [2023] A. Syed, C. Rager, and A. Conmy. Attribution patching outperforms automated circuit discovery. _arXiv preprint arXiv:2310.10348_, 2023.
* Taggart [2024] G. M. Taggart. Prolu: A nonlinearity for sparse autoencoders, 2024. https://www.lesswrong.com/posts/HEpufTdaKOTKgoYF/prolu-a-pareto-improvement-for-sparse-autoencoders.
* Tamkin et al. [2023] A. Tamkin, M. Taufeeque, and N. D. Goodman. Codebook features: Sparse and discrete interpretability for neural networks, 2023.
* February 2024. _Transformer Circuits Thread_, 2024. https://transformer-circuits.pub/2024/feb-update/index.html.
* Thorpe [1989] S. J. Thorpe. Local vs. distributed coding. _Intellectica_, 8:3-40, 1989.
* Turner et al. [2023] A. M. Turner, L. Thiergart, D. Udell, G. Leech, U. Mini, and M. MacDiarmid. Activation addition: Steering language models without optimization, 2023.
* Wang et al. [2023] K. R. Wang, A. Variengien, A. Conmy, B. Shlegeris, and J. Steinhardt. Interpretability in the wild: a circuit for indirect object identification in GPT-2 small. In _The Eleventh International Conference on Learning Representations_, 2023. URL https://openreview.net/forum?id=NpsVSN6o4ul.
* Wright and Sharkey [2024] B. Wright and L. Sharkey. Addressing feature suppression in SAEs, Feb 2024. https://www.alignmentforum.org/posts/3JuSjTZyMzaSeTxKk/addressing-feature-suppression-in-saes.
* Yun et al. [2023] Z. Yun, Y. Chen, B. A. Olshausen, and Y. LeCun. Transformer visualization via dictionary learning: contextualized embedding as a linear superposition of transformer factors, 2023.

## Appendix A Impact statement

This work introduces a method to obtain higher fidelity sparse decompositions of LM activations, under the hypothesis that progress in this area will ultimately help us understand the representations used by LMs. If successful, this could lead to greater understanding of how LMs complete tasks and novel mechanisms for controlling their behavior. Greater understanding and control could be put to beneficial uses such as mitigating the harms caused by current and future models, although bad actors could also misuse these tools, for example to circumvent safety training and steer models towards harmful behaviors. Currently, the SAE research program is in its early stages. For any potential misuse of SAEs, there is typically a more practical and effective way to achieve the same end using existing tooling, e.g. fine tuning or activation editing. Therefore, we see negligible negative societal impact in the short term. Longer term, advances in LM interpretability and control pose similar benefits and risks to advances in AI capabilities in general.

## Appendix B Metrics for evaluating SAEs

SAEs are expected to decompose input activations sparsely, and yet in a manner that allows for faithful reconstruction. L0 and loss recovered are two metrics typically used [8] to measure sparsity and reconstruction fidelity respectively. These are defined as follows:

* The **L0** of a SAE is defined by the average number of active features on a given input, i.e \(\mathbb{E}_{\mathbf{x}\sim\mathcal{D}}\left\|\mathbf{f}(\mathbf{x})\right\|_ {0}\).
* The **loss recovered** of a SAE is calculated from the average cross-entropy loss of the language model on an evaluation dataset, when the SAE's reconstructions are spliced into it. If we denote by \(\text{CE}(\phi)\) the average loss of the language model when we splice in a function \(\phi:\mathbb{R}^{n}\rightarrow\mathbb{R}^{n}\) at the SAE's site during the model's forward pass, then loss recovered is \[1-\frac{\text{CE}(\mathbf{\hat{x}}\circ\mathbf{f})-\text{CE}(\text{Id})}{ \text{CE}(\zeta)-\text{CE}(\text{Id})},\] (8) where \(\mathbf{\hat{x}}\circ\mathbf{f}\) is the autoencoder function, \(\zeta:\mathbf{x}\mapsto\mathbf{0}\) the zero-ablation function and \(\text{Id}:\mathbf{x}\mapsto\mathbf{x}\) the identity function. According to this definition, a SAE that always outputs the zero vector as its reconstruction would get a loss recovered of 0%, whereas a SAE that reconstructs its inputs perfectly would get a loss recovered of 100%.

### Issues with the loss recovered metric

In this paper, we have used loss recovered as defined in Bricken et al. [8] to measure reconstruction fidelity. However, there are deficiencies with this metric:

* Firstly, zero-ablation is arguably too poor a baseline for defining the zero-point of this metric and mean-ablation is better justified. Using the mean-ablation function \(\mu:\mathbf{x}\mapsto\mathbb{E}_{\mathbf{x}^{\prime}\sim\mathcal{D}}\mathbf{ x}^{\prime}\), instead of \(\zeta\) in the definition of loss recovered above would also have the benefit that SAEs' loss recovered would tend towards zero in the limit \(L0\to 0\), instead of tending to a positive value as it does when computing loss recovered using zero-ablation.
* Furthermore, the very fact we normalise the increase in the spliced LM's loss when computing loss recovered makes it difficult to compare the impact of splicing SAEs at different sub-layers of the model. For example, mean or zero-ablating the output of a MLP layer typically has a much milder impact on LM loss than mean or zero-ablating the residual stream, making the denominator in Eq. (8) smaller for MLP SAEs than for residual stream SAEs. So we unsurprisingly find that residual stream SAEs' loss recovered tend to be much higher than MLP or attention SAEs. This suggests that it may be more informative to report raw changes in cross-entropy loss ("delta LM loss") instead of using a normalised metric like loss recovered, since these are directly comparable across SAEs trained on different sub-layers of the same LM.

In practice however, both mean-ablated loss recovered and delta LM loss are related to zero-ablated loss recovered (the metric used in this paper) by an affine transformation. In other words, all theloss recovered versus L0 figures in this paper would look identical if we had used one of these other metrics instead, with the only difference being the tick labels on the y-axis. Consequently, none of the conclusions we draw in this paper would be affected by using one of these other reconstruction fidelity metrics instead. Nevertheless, we draw the reader's attention to our subsequent work [41], which compares Gated SAEs to other SAE varieties adopting the delta LM loss metric, instead of loss recovered, for measuring reconstruction fidelity.12

Footnote 12: We also provide in Table 1 cross entropy losses for the LMs used in our experiments, both with and without zero-ablation, which could in principle be used to translate the loss recovered results in this paper to delta LM loss.

## Appendix C Measuring shrinkage

As described in Section 3.1, the L1 sparsity penalty used to train baseline SAEs causes feature activations to be systematically underestimated, a phenomenon called _shrinkage_. Since this in turn shrinks the reconstructions produced by the SAE decoder, we can observe the extent to which a trained SAE is affected by shrinkage by measuring the average norm of its reconstructions.

Concretely, the metric we use is the _relative reconstruction bias_,

\[\gamma:=\operatorname*{arg\,min}_{\gamma^{\prime}}\mathbb{E}_{\mathbf{x} \sim\mathcal{D}}\left[\left\|\hat{\mathbf{x}}_{\text{SAE}}(\mathbf{x})/ \gamma^{\prime}-\mathbf{x}\right\|_{2}^{2}\right],\] (9)

i.e. \(\gamma^{-1}\) is the optimum multiplicative factor by which an SAE's reconstructions should be rescaled in order to minimise the L2 reconstruction loss; \(\gamma=1\) for an unbiased SAE and \(\gamma<1\) when there's shrinkage.13 Explicitly solving the optimization problem in Eq. (9), the relative reconstruction bias can be expressed analytically in terms of the mean SAE reconstruction loss, the mean squared norm of input activations and the mean squared norm of SAE reconstructions, making \(\gamma\) easy to compute and track during training:

Footnote 13: We have defined \(\gamma\) this way round so that \(\gamma<1\) intuitively corresponds to shrinkage.

\[\gamma=\frac{\mathbb{E}_{\mathbf{x}\sim\mathcal{D}}\left[\left\|\hat{ \mathbf{x}}_{\text{SAE}}\left(\mathbf{x}\right)\right\|_{2}^{2}\right]}{ \mathbb{E}_{\mathbf{x}\sim\mathcal{D}}\left[\left\|\hat{\mathbf{x}}_{\text{ SAE}}\left(\mathbf{x}\right)\right\|_{2}^{2}\right]+\mathbb{E}_{\mathbf{x}\sim \mathcal{D}}\left[\left\|\mathbf{x}_{2}^{2}\right\|-\mathbb{E}_{\mathbf{x}\sim \mathcal{D}}\left[\left\|\hat{\mathbf{x}}_{\text{SAE}}\left(\mathbf{x} \right)-\mathbf{x}\right\|_{2}^{2}\right]},\] (10)

where the second equality makes use of the identity \(2\mathbf{a}\cdot\mathbf{b}\equiv\left\|\mathbf{a}\right\|_{2}^{2}+\left\| \mathbf{b}\right\|_{2}^{2}-\left\|\mathbf{a}-\mathbf{b}\right\|_{2}^{2}\). Notice from the second expression for \(\gamma\) that an unbiased reconstruction (\(\gamma=1\)) therefore satisfies

\[\mathbb{E}_{\mathbf{x}\sim\mathcal{D}}\left[\left\|\hat{\mathbf{x}}_{\text{ SAE}}\left(\mathbf{x}\right)\right\|_{2}^{2}\right]=\mathbb{E}_{\mathbf{x}\sim \mathcal{D}}\left[\left\|\mathbf{x}\right\|_{2}^{2}\right]-\mathbb{E}_{\mathbf{ x}\sim\mathcal{D}}\left[\left\|\hat{\mathbf{x}}_{\text{SAE}}\left(\mathbf{x} \right)-\mathbf{x}\right\|_{2}^{2}\right].\]

In other words, an unbiased but imperfect SAE (i.e. one that has non-zero reconstruction loss) must have mean squared reconstruction norm that is strictly _less than_ the mean squared norm of its inputs _even without shrinkage_. Shrinkage makes the mean squared reconstruction norm even smaller.

## Appendix D Inference-time optimization

The task SAEs perform can be split into two sub-tasks: sparse coding, or learning a set of features from a dataset, and sparse approximation, where a given datapoint is approximated as a sparse linear combination of these features. The decoder weights are the set of learned features, and the mapping represented by the encoder is a sparse approximation algorithm. Formally, sparse approximation is the problem of finding a vector \(\boldsymbol{\alpha}\) that minimises;

\[\boldsymbol{\alpha}=\operatorname*{arg\,min}\left\|\mathbf{x}-\mathbf{D} \boldsymbol{\alpha}\right\|_{2}^{2}\ \ s.t.\ \ \left\|\boldsymbol{\alpha}\right\|_{0}<\gamma\] (11)

i.e. that best reconstructs the signal \(\mathbf{x}\) as a linear combination of vectors in a dictionary \(\mathbf{D}\), subject to a constraint on the L0 pseudo-norm on \(\boldsymbol{\alpha}\). Sparse approximation is a well studied problem, and SAEs are a _weak_ sparse approximation algorithm. SAEs, at least in the formulation conventional in dictionary learning for language models, in fact solve a slightly more restricted version of this problem where the weights \(\boldsymbol{\alpha}\) on each feature are constrained to be non-negative, leading to the related problem

\[\boldsymbol{\alpha}=\operatorname*{arg\,min}\left\|\mathbf{x}-\mathbf{D} \boldsymbol{\alpha}\right\|_{2}^{2}\ \ s.t.\ \ \left\|\boldsymbol{\alpha}\right\|_{0}<\gamma,\boldsymbol{\alpha}>0\] (12)In this paper, we do not explore using more powerful algorithms for sparse coding. This is partly because we are using SAEs not just to recover \(a\) sparse reconstruction of activations of a LM; ideally we hope that the learned features will coincide with the linear representations actually used by the LM, under the superposition hypothesis. Prior work [8] has argued that SAEs are more likely to recover these due to the correspondence between the SAE encoder and the structure of the network itself; the argument is that it is implausible that the network can make use of features which can only be recovered from the vector via an iterative optimisation algorithm, whereas the structure of the SAE means that it can only find features whose presence can be predicted well by a simple linear mapping. Whether this is true remains, in our view, an important question for future work, but we do not address it in this paper.

In this section we discuss some results obtained by using the dictionaries learned via SAE training, but replacing the encoder with a different sparse approximation algorithm at inference time. This allows us to compare the dictionaries learned by different SAE training regimes independently of the quality of the encoder. It also allows us to examine the gap between the sparse reconstruction performed by the encoder against the baseline of a more powerful sparse approximation algorithm. As mentioned, for a fair comparison to the task the encoder is trained for, it is important to solve the sparse approximation problem of Eq. (12), rather than the more conventional formulation of Eq. (11), but most sparse approximation algorithms can be modified to solve this with relatively minor changes.

Solving Eq. (12) exactly is equivalent to integer linear programming, and is NP hard. The integer linear programs in question would be large, as our SAE decoders routinely have hundreds of thousands of features, and solving them to guaranteed optimality would likely be intractable. Instead, as is commonly done, we use iterative greedy algorithms to find an approximate solution. While the solution found by these sparse approximation algorithms is not guaranteed to be the global optimum, these are significantly more powerful than the SAE encoder, and we feel it is acceptable in practice to treat them as an upper bound on possible encoder performance.

For all results in this section, we use gradient pursuit, as described in Blumensath and Davies [6], as our inference time optimisation (ITO) algorithm. This algorithm is a variant of orthogonal matching pursuit [40] which solves the orgothonalisation of the residual to the span of chosen dictionary elements approximately at every step rather than exactly, but which only requires matrix multiplies rather than matrix solves and is easier to implement on accelerators as a result. It is possibly not crucial for performance that our optimisation algorithm be implementable on TPUs, but being able to avoid a host-device transfer when splicing this into the forward pass allowed us to re-use our existing evaluation pipeline with minimal changes.

When we use a sparse approximation algorithm at test time, we simply use the decoder of a trained SAE as a dictionary, ignoring the encoder. This allows us to sweep the target sparsity at test time without retraining the model, meaning that we can plot an entire Pareto frontier of loss recovered against sparsity for a single decoder, as in done in Fig. 7.

Fig. 6 compares the loss recovered when using ITO for a suite of SAEs decoders trained with both methods at three different test time L0 thresholds. This graph shows a somewhat surprising result; while Gated SAEs learn better decoders generally, and often achieve the best loss recovered using ITO close to their training sparsity, SAE decoders are often outperformed by decoders which achieved a higher test time L0; it's better to do ITO with a target L0 of 10 with an decoder with an achieved L0 of around 100 during training than one which was actually trained with this level of sparsity. For instance, the left hand panel in Fig. 6 shows that SAEs with a training L0 of 100 are better than those with an L0 of around 10 at almost every sparsity level in terms of ITO reconstruction. However, gated SAE dictionaries have a small but real advantage over standard SAEs in terms of loss recovered at most target sparsity levels, suggesting that part of the advantage of gated SAEs is that they learn better dictionaries as well as addressing issues with shrinkage. However, there are some subtleties here; for example, we find that baseline SAEs trained with a lower sparsity penalty (higher training L0) often outperform more sparse baseline SAEs according to this measure, and the best performing baseline SAE (L0 \(\approx 99\)) is comparable to the best performing Gated SAE (L0 \(\approx 20\)).

Fig. 7 compares the Pareto frontiers of a baseline model and a gated model to the Pareto frontier of an ITO sweep of the best performing dictionary of each. Note that, while the Pareto curve of the baseline dictionary is formed by several models as each encoder is specialised to a given sparsity level, as mentioned, ITO lets us plot a Pareto frontier by sweeping the target sparsity with a single dictionary; here we plot only the best performing dictionary from each model type to avoid cluttering the figure. This figure suggests that the performance gap between the encoder and using ITO is smaller for the gated model. Interestingly, this cannot solely be explained by addressing shrinkage, as we demonstrate by experimenting with a baseline model which learns a rescale and shift with a frozen encoder and decoder directions.

Figure 6: This figure compares the ITO performance of different decoders across a sweep for decoders trained using a baseline SAE and the gated method, at three different test time target sparsities. Gated SAEs trained at lower target sparsities consistently achieve better dictionaries by this measure. Interestingly, the best performing baseline dictionary by this measure often has a much higher test time sparsity than the target; for instance, at a test time sparsity of 30, the best baseline SAE was the one that had a test time sparsity of more like 100. This could be an artifact of the fact that the L0 measure is quite sensitive to noise, and standard SAE architectures tend to have a reasonable number of features with very low activation.

Figure 7: Pareto frontiers of a baseline SAE, a baseline SAE with learned rescale and shift (to account for shrinkage) and a gated SAE across different sparsity lambdas, compared to the ITO Pareto frontier of the best decoder of each type with ITO, varying the target sparsity. The best gated encoder is better than the best standard encoder by this measure, but the difference is marginal. As shown in the plot above, the best baseline encoder by the ITO measure had a much larger test time sparsity (around 100) than the best gated model (around 30). This figure suggests that the gap between SAE performance and ’optimal’ performance, if we assume that ITO is close to the maximum possible reconstruction using the given encoder, is much smaller for the gated model.

## Appendix E More loss recovered / L0 Pareto frontiers

In Fig. 8 we show that Gated SAEs outperform baseline SAEs. In Fig. 9 we show that Gated SAEs outperform baseline SAEs at all but one MLP output or residual stream site that we tested on.

In Fig. 9 at the attention output pre-linear site at layer 27, loss recovered is bigger than 1.0. On investigation, we found that the dataset used to train the SAE was not identical to Gemma's pretraining dataset, and at this site it was possible to mean ablate this quantity and decrease loss - explaining why SAE reconstructions had lower loss than the original model.

Table 1 provides cross-entropy losses for the Gemma-7B and Pythia-2.8B, both before and after zero-ablating specific sub-layers of these models, to help provide further context for interpreting the loss recovered results presented in this paper; 100% loss recovered corresponds to the SAE-spliced language model attaining a loss matching the original language model, whereas 0% loss recovered corresponds to the SAE-spliced language model attaining a loss matching the language model with the corresponding sub-layer zero-ablated.

## Appendix F Further shrinkage plots

In Fig. 10, we show that Gated SAEs resolve shrinkage, as measured by relative reconstruction bias (Appendix C), in Pythia-2.8B.

## Appendix G Training and evaluation: hyperparameters and other details

### Training

#### g.1.1 General training details

Other details of SAE training are:

* **SAE Widths**. Our SAEs have width \(2^{17}\) for most baseline SAEs, \(3\times 2^{16}\) for Gated SAEs, except for the (Pythia-2.8B, Residual Stream) sites we used \(2^{15}\) for baseline and \(3\times 2^{14}\) for Gated since early runs at these sites had lots of learned feature death.
* **Training data**. We use activations from hundreds of millions to billions of activations from LM forward passes as input data to the SAE. Following Nanda [29], we use a shuffled buffer of these activations, so that optimization steps don't use data from highly correlated activations.14 Footnote 14: In contrast to earlier findings [9], we found that when using Pythia-2.8B’s activations from sequences of length 2048, rather than GELU-1L’s activations from sequences of length 128, it was important to shuffle the \(10^{6}\) length activation buffer used to train our SAEs.
* **Resampling**. We used _resampling_, a technique which at a high-level reinitializes features that activate extremely rarely on SAE inputs periodically throughout training. We mostly

\begin{table}
\begin{tabular}{c c c c c c} \hline \hline \multirow{2}{*}{Model} & \multirow{2}{*}{Layer} & \multirow{2}{*}{Original CE Loss} & \multicolumn{3}{c}{Zero Ablation CE Loss} \\ \cline{4-6}  & & & MLP & Attention & Residual \\ \hline \multirow{4}{*}{\begin{tabular}{c} Gemma-7B \\ (1024 length context) \\ \end{tabular} } & 6 & 2.5426 & 2.7764 & 2.7295 & 16.1549 \\  & 13 & 2.5426 & 2.5878 & 2.5566 & 30.3588 \\ \cline{1-1}  & 20 & 2.5426 & 2.6881 & 2.5726 & 19.5891 \\ \cline{1-1}  & 27 & 2.5426 & 26.1114 & 3.0819 & 12.4534 \\ \hline \multirow{4}{*}{
\begin{tabular}{c} Pythia-2.8B \\ (2048 length context) \\ \end{tabular} } & 4 & 1.9699 & 2.0460 & 2.0361 & 13.0434 \\  & 12 & 1.9699 & 2.0167 & 2.0131 & 10.6558 \\ \cline{1-1}  & 16 & 1.9699 & 2.0098 & 2.0046 & 11.6820 \\ \cline{1-1}  & 20 & 1.9699 & 2.1022 & 2.0269 & 10.4578 \\ \cline{1-1}  & 28 & 1.9699 & 2.0145 & 1.9760 & 27.8663 \\ \hline \hline \end{tabular}
\end{table}
Table 1: Cross-entropy losses for the original language model and after zero-ablating specified sub-layers of Pythia-2.8B and Gemma-7B.

Figure 8: Gated SAEs throughout Pythia-2.8B. At all sites we tested, Gated SAEs are a Pareto improvement. In every plot, the SAE with maximal loss recovered was a Gated SAE.

Figure 9: Gated and Normal Pareto-Optimal SAEs for Gamma-7B – see Appendix E for a discussion of the anomalies (such as the Layer 27 attention output SAEs).

Figure 10: Gated SAEs address the problem of shrinkage in Pythia-2.8B.

follow the approach described in the 'Neuron Resampling' appendix of Bricken et al. [8], except we reapply learning rate warm-up after each resampling event, reducing learning rate to 0.1x the ordinary value, and, increasing it with a cosine schedule back to the ordinary value over the next 1000 training steps.
* **Optimizer hyperparameters**. We use the Adam optimizer with \(\beta_{2}=0.999\) and \(\beta_{1}=0.0\), following Templeton et al. [47], as we also find this to be a slight improvement to training. We use a learning rate warm-up. See Appendix G.1.2 for learning rates of different experiment.
* **Decoder weight norm constraints**. Templeton et al. [47] suggest constraining columns to have _at most_ unit norm (instead of exactly unit norm), which can help distinguish between productive and unproductive feature directions (although it should have no systematic impact on performance). However, we follow the original approach of constraining columns to have exact unit norms in this work for the sake of simplicity.
* **Compute resources**. Individual SAEs were each trained on TPU-v3 slices with a 2x2 topology [20]. The same chips were used to generate LM activations on-the-fly, train SAE parameters and evaluate SAEs during training, using up to 8-way model parallelism. With this setup, the time to train a SAE varies by SAE width, LM residual stream dimension, sequence length, layer and site.15 We also used a negligible amount of compute on resampling (Appendix G), evaluation (e.g. Figure 1) and interpretability experiments (Section 4.2). Training wall clock time ranges from around 7 hours to train on GELU-1L MLP activations to around 47 hours to train on Gemma-7B sites at layer 27. We estimate that we used twice as much compute as used in the paper on preliminary experiments.

Footnote 15: https://github.com/google-learning/

#### g.1.2 Experiment-specific training details

* We use learning rate 0.0003 for all Gated SAE experiments, and the GELU-1L baseline experiment. We swept for optimal baseline learning rates for the GELU-1L baseline to generate this value. For the Pythia-2.8B and Gemma-7B baseline SAE experiments, we divided the L2 loss by \(\mathbb{E}||x||_{2}\), motivated by better hyperparameter transfer, and so changed learning rate to 0.001 and 0.00075. We didn't see noticeable difference in the Pareto frontier and so did not sweep this hyperparameter further.
* We generate activations from sequences of length 128 for GELU-1L, 2048 for Pythia-2.8B and 1024 for Gemma-7B.
* We use a batch size of 4096 for all runs. We use 300,000 training steps for GELU-1L and Gemma-7B runs, and 400,000 steps for Pythia-2.8B runs.

#### g.1.3 Lessons learned scaling SAEs

* **Learned feature death is unpredictable**. In Fig. 11 there are few patterns that can be gleaned from staring at which runs have high numbers of dead learned features (called dead neurons in Bricken et al. [8]).
* **Resampling makes hyperparameter sweeps difficult**. We found that resampling caused L0 and loss recovered to increase, similar to Conmy [9].
* **Training appears to converge earlier than expected**. We found that we did not need 20B tokens as in Bricken et al. [8], as generally resampling had stopped causing gains and loss curves plateaued after just over one billion tokens.

### Evaluation

We evaluated the models on over a million held-out tokens.

Equivalence between gated encoder with tied weights and linear encoder with non-standard activation function

In this section we show under the weight sharing scheme defined in Eq. (7), a gated encoder as defined in Eq. (5) is equivalent to a linear layer with a non-standard (and parameterized) activation function.

Without loss of generality, consider the case of a single latent feature (\(M=1\)) and set the pre-encoder bias to zero. In this case, the gated encoder is defined as

\[\tilde{f}(\mathbf{x}):=\mathbbm{1}_{\mathbf{w}_{\text{pte}}\cdot\mathbf{x}+b_ {\text{pte}}>\mathbf{0}}\text{ ReLU}\left(\mathbf{w}_{\text{mag}}\cdot\mathbf{x}+b_ {\text{mag}}\right)\] (13)

and the weight sharing scheme becomes

\[\mathbf{w}_{\text{mag}}:=\rho_{\text{mag}}\mathbf{w}_{\text{gate}}\] (14)

with a non-negative parameter \(\rho_{\text{mag}}\equiv\exp(\mathbf{r}_{\text{mag}})\).

Substituting Eq. (14) into Eq. (13) and re-arranging, we can re-express \(\tilde{f}(\mathbf{x})\) as a single linear layer

\[\tilde{f}(\mathbf{x}):=\sigma_{b_{\text{mag}}-\rho_{\text{mag}}b_{\text{mag}} }\left(\mathbf{w}_{\text{mag}}\cdot\mathbf{x}+b_{\text{mag}}\right)\] (15)

with the parameterized activation function

\[\sigma_{\theta}(z):=\mathbbm{1}_{z>\theta}\text{ ReLU}\left(z\right).\] (16)

called JumpReLU in a different context [17]. Fig. 12 illustrates the shape of this activation function.

## Appendix I Further analysis of the human interpretability study

We perform some further analysis on the data from Section 4.2, to understand the impact of different sites, layers, and raters.

### Sites

We first pose the question of whether there's evidence that the sites had different interpretability outcomes. A Friedman test across sites shows significant differences (at \(p=0.047\)) between the Gated-vs-Baseline differences, though not (\(p=0.92\)) between the raw labels.

Breaking down by site and repeating the Wilcoxon-Pratt one-sided tests and computing confidence intervals, we find the result on MLP outputs is strongest, with mean 0.40, significance \(p=0.003\), and CI [0.18, 0.63]; this is as compared with the attention outputs (\(p=0.47\), mean.05, CI [-0.16, 0.26]) and final residual (\(p=0.59\), mean -0.07, CI [-0.28, 0.12]) SAEs.

### Layers

Next we test whether different layers had different outcomes. We do this separately for the 2 models, since the layers aren't directly comparable. We run 2 tests in each setting: Page's trend test (which tests for a monotone trend across layers) and the Friedman test (which tests for any difference, without any expectation of a monotone trend).

Results are presented in Table 2; they suggest there are some significant nonmonotone differences between layers. To elucidate this, we present 90% BCa bootstrap confidence intervals of the mean raw label (where 'No'=0, 'Maybe'=1, 'Yes'=2) and the Gated-vs-Baseline difference, per layer, in Fig. 15 and Fig. 16, respectively.

### Raters

In Table 3 we present test results weakly suggesting that the raters differed in their judgments. This underscores that there's still a significant subjective component to this interpretability labeling. (Notably, different raters saw different proportions of Pythia vs Gamma features, so aggregating across the models is partially confounded by that.)

\begin{table}
\begin{tabular}{l l l} \hline \hline \(p\)-values & Raw label & Delta from Baseline to Gated \\ \hline Pythia-2.8B (Page’s trend test) & 0.50 & 0.13 \\ Pythia-2.8B (Friedman test) & 0.57 & 0.05 \\ Gemma-7B (Page’s trend test) & 0.037 & 0.31 \\ Gemma-7B (Friedman test) & 0.003 & 0.64 \\ \hline \hline \end{tabular}
\end{table}
Table 2: Layer significance tests

Figure 11: Feature death in Gemma-7B.

\begin{table}
\begin{tabular}{l l l} \hline \hline \(p\)-values & Raw label & Delta from Baseline to Gated \\ \hline Across models (Kruskal-Wallis H-test) & 0.01 & 0.71 \\ Pythia-2.8B (Friedman test) & 0.13 & 0.05 \\ Gemma-7B (Friedman test) & 0.03 & 0.76 \\ \hline \hline \end{tabular}
\end{table}
Table 3: Rater significance testsFigure 12: After applying the weight sharing scheme of Eq. (7), a gated encoder becomes equivalent to a single layer linear encoder with a JumpReLU (Erichson et al. [17], previously named TRec by Konda et al. [23]) activation function \(\sigma_{\theta}\), illustrated above.

Figure 13: Contingency table showing Gated vs Baseline interpretability labels from our paired study results, for Pythia-2.8B and Gemma-7B.

[MISSING_PAGE_EMPTY:26]

Figure 16: Per-layer 90% confidence intervals for the Gated-vs-Baseline label difference

Figure 15: Per-layer 90% confidence intervals for the mean interpretability label

Figure 17: Contingency tables for the paired (gated vs baseline) interpretability labels, for Pythai-2.8B

Figure 18: Contingency tables for the paired (gated vs baseline) interpretability labels, for German-7B

## Appendix J Pseudo-code for Gated SAEs and the Gated SAE loss function

## Appendix K Tables of Results

We evaluated the models on over a million held-out tokens. Tables 4-11 show summary stats from training runs on the Pareto frontier.

Figure 19: Pseudo-code for the Gated SAE forward pass.

Figure 20: Pseudo-code for the Gated SAE loss function. Note that this pseudo-code is written for expositional clarity. In practice, taking into account parameter tying, it would be more efficient to rearrange the computation to avoid unnecessarily duplicated operations.

[MISSING_PAGE_FAIL:30]

\begin{table}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|} \hline
**Site** & **Layer** & \begin{tabular}{c} **Sparsity** \\ \(\lambda\) \\ \end{tabular} & **LR** & **L0** & \begin{tabular}{c} **\% CE** \\ **Recovered** \\ \end{tabular} & \begin{tabular}{c} **Clean** \\ **CE Loss** \\ \end{tabular} & \begin{tabular}{c} **SAE** \\ **CE Loss** \\ \end{tabular} & \begin{tabular}{c} **0 Abl.** \\ **CE Loss** \\ \end{tabular} & \begin{tabular}{c} **Width** \\ **CE Loss** \\ \end{tabular} & \begin{tabular}{c} **\% Alive** \\ **Features** \\ \end{tabular} & 
\begin{tabular}{c} **Shrinkage** \\ \(\gamma\) \\ \end{tabular} \\ \hline MLP & 20 & 5e-05 & 0.00075 & 26.0 & 82.36\% & 2.5426 & 2.5682 & 2.6881 & 196608 & 97.96\% & 0.865 \\ _MLP_ & _20_ & _4.5e-05_ & _0.00075_ & _31.4_ & _83.94\%_ & _2.5426_ & _2.5659_ & _2.6881_ & _196608_ & _99.24\%_ & _0.877_ \\ MLP & 20 & 3e-05 & 0.001 & 39.5 & 83.12\% & 2.5426 & 2.5671 & 2.6881 & 196608 & 46.33\% & 0.924 \\ _MLP_ & _20_ & _4e-05_ & _0.00075_ & _38.3_ & _85.18\%_ & _2.5426_ & _2.5641_ & _2.6881_ & _196608_ & _95.73\%_ & _0.889_ \\ MLP & 20 & 3e-05 & _0.00075_ & _43.2_ & 84.11\% & 2.5426 & 2.5657 & 2.6881 & 196608 & 94.62\% & 0.874 \\ _MLP_ & _20_ & _3e-05_ & _0.00075_ & _56.8_ & _87.23\%_ & _2.5426_ & _2.5612_ & _2.6881_ & _196608_ & _96.88\%_ & _0.894_ \\ MLP & 20 & 2e-05 & 0.00075 & 68.1 & 84.18\% & 2.5426 & 2.5656 & 2.6881 & 196608 & 53.42\% & 0.898 \\ MLP & 20 & 2e-05 & 0.00075 & 75.6 & 85.63\% & 2.5426 & 2.5635 & 2.6881 & 196608 & 66.29\% & 0.899 \\ MLP & 20 & 1.5e-05 & 0.00075 & 104.6 & 85.71\% & 2.5426 & 2.5634 & 2.6881 & 196608 & 41.7\% & 0.965 \\ _MLP_ & _20_ & _1e-05_ & _0.00075_ & _321.1_ & _90.3\%_ & _2.5426_ & _2.5567_ & _2.6881_ & _196608_ & _56.83\%_ & _0.911_ \\ _MLP_ & _27_ & _1.2e-05_ & _0.001_ & _10.2_ & _86.28\%_ & _2.5426_ & _2.57751_ & _26.1114_ & _196608_ & _0.6\%_ & _1.019_ \\ _MLP_ & _27_ & _1e-05_ & _0.001_ & _20.5_ & _95.05\%_ & _2.5426_ & _3.7081_ & _26.1114_ & _196608_ & _1.73\%_ & _1.002_ \\ MLP & 27 & 8e-06 & 0.001 & 21.3 & 93.55\% & 2.5426 & 4.0623 & 26.1114 & 196608 & 0.66\% & 0.988 \\ MLP & 27 & 6e-06 & 0.00075 & 26.4 & 91.19\% & 2.5426 & 4.6185 & 26.1114 & 196608 & 0.57\% & 0.973 \\ MLP & 27 & 5.5e-06 & 0.00075 & 18.1 & 85.53\% & 2.5426 & 5.9522 & 26.1114 & 196608 & 0.58\% & 0.994 \\ MLP & 27 & 3e-06 & 0.00075 & _26.9_ & 90.82\% & 2.5426 & 4.706 & 26.1114 & 196608 & 0.98\% & 1.024 \\ _Attn_ & \(6\) & _7e-05_ & _0.00075_ & _15.4_ & _69.89\%_ & _2.5426_ & _2.5989_ & _2.7295_ & _196608_ & _96.78\%_ & _0.72_ \\ _Attn_ & \(6\) & _5e-05_ & _0.00075_ & _26.4_ & _78.08\%_ & _2.5426_ & _2.5836_ & _2.7295_ & _196608_ & _98.97\%_ & _0.777_ \\ _Attn_ & \(6\) & _3e-05_ & _0.00075_ & _54.6_ & _85.42\%_ & _2.5426_ & _2.5698_ & _2.7295_ & _196608_ & _99.7\%_ & _0.846_ \\ _Attn_ & _13_ & _7e-05_ & _0.00075_ & _22.6_ & _60.79\%_ & _2.5426_ & _2.5481_ & _2.5566_ & _196608_ & _93.47\%_ & _0.721_ \\ _Attn_ & _13_ & _7e-05_ & _0.00075_ & _36.5_ & _65.45\%_ & _2.5426_ & _2.5474_ & _2.5566_ & _196608_ & _97.59\%_ & _0.786_ \\ _Attn_ & _13_ & _7e-05_ & _0.00075_ & _68.8_ & _81.03\%_ & _2.5426_ & _2.5452_ & _2.5566_ & _196608_ & _99.19\%_ & _0.804_ \\ _Attn_ & _20_ & _9e-05_ & _0.00075_ & _10.8_ & _68.98\%_ & _2.5426_ & _2.5519_ & _2.5726_ & _196608_ & _79.34\%_ & _0.715_ \\ _Attn_ & _20_ & _8e-05_ & _0.00075_ & _12.3_ & _72.48\%_ & _2.5426_ & _2.5508_ & _2.5726_ & _196608_ & _83.58\%_ & _0.723_ \\ _Attn_ & _20_ & _7e-05_ & _0.00075_ & _15.9_ & _75.83\%_ & _2.5426_ & _2.5498_ & _2.5726_ & _196608_ & _87.54\%_ & _0.755_ \\ _Attn_ & _20_ & _6e-05_ & _0.00075_ & _18.7_ & _78.38\%_ & _2.5426_ & _2.5491_ & _2.5726_ & _196608_ & _89.49\%_ & _0.759_ \\ _Attn_ & _20_ & _5e-05_ & _0.00075_ & _25.1_ & _82.96\%_ & _2.5426_ & _2.5477_ & _2.5726_ & _196608_ & _92.36\%_ & _0.786_ \\ _Attn_ & _20_ & _4e-05_ & _0.00075_ & _32.6_ & _85.95\%_ & _2.5426_ & _2.5468_ & _2.5726_ & _196608_ & _95.14\%_ & _0.802_ \\ _Attn_ & _20_ & _3e-05_ & _0.00075_ & _50.3_ & _89.52\%_ & _2.5

\begin{table}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|} \hline
**Site** & **Layer** & \begin{tabular}{c} **Sparsity** \\ \(\lambda\) \\ \end{tabular} & **LR** & **L0** & \begin{tabular}{c} **\% CE** \\ **Recovered** \\ \end{tabular} & \begin{tabular}{c} **Clean** \\ **CE Loss** \\ \end{tabular} & \begin{tabular}{c} **SAE** \\ **CE Loss** \\ \end{tabular} & \begin{tabular}{c} **0 Abl.** \\ **CE Loss** \\ \end{tabular} & \begin{tabular}{c} **Width** \\ **Features** \\ \end{tabular} & 
\begin{tabular}{c} **\% Alive** \\ \(\gamma\) \\ \end{tabular} \\ \hline _Resid_ & \(6\) & _0.0012_ & _0.0003_ & _2.2_ & _95.55\%_ & _2.5426_ & _3.1483_ & _16.1549_ & _131072_ & _93.94\%_ & _1.006_ \\ _Resid_ & \(6\) & _0.001_ & _0.0003_ & _3.0_ & _96.67\%_ & _2.5426_ & _2.9954_ & _16.1549_ & _131072_ & _96.24\%_ & _1.006_ \\ _Resid_ & \(6\) & _0.0008_ & _0.0003_ & _4.3_ & _97.83\%_ & _2.5426_ & _2.8382_ & _16.1549_ & _131072_ & _97.52\%_ & _1.003_ \\ _Resid_ & \(6\) & _0.0006_ & _0.0003_ & _7.0_ & _98.76\%_ & _2.5426_ & _2.7108_ & _16.1549_ & _131072_ & _98.3\%_ & _0.996_ \\ _Resid_ & \(6\) & _0.0004_ & _0.0003_ & _14.3_ & _99.35\%_ & _2.5426_ & _2.6312_ & _16.1549_ & _131072_ & _98.68\%_ & _0.996_ \\ _Resid_ & \(6\) & _0.0002_ & _0.0003_ & _45.9_ & _99.77\%_ & _2.5426_ & _2.5735_ & _16.1549_ & _131072_ & _99.51\%_ & _0.999_ \\ _Resid_ & \(6\) & _2e-05_ & _0.0003_ & _95.2_ & _98.62\%_ & _2.5426_ & _2.7302_ & _16.1549_ & _131072_ & _45.13\%_ & _1.148_ \\ _Resid_ & \(6\) & _4e-05_ & _0.0003_ & _144.0_ & _99.35\%_ & _2.5426_ & _2.6313_ & _16.1549_ & _131072_ & _36.05\%_ & _1.038_ \\ _Resid_ & \(6\) & _8e-06_ & _0.0003_ & _177.5_ & _99.29\%_ & _2.5426_ & _2.6386_ & _16.1549_ & _131072_ & _53.36\%_ & _1.086_ \\ _Resid_ & \(6\) & _0.0001_ & _0.0003_ & _131.8_ & _99.94\%_ & _2.5426_ & _2.5511_ & _16.1549_ & _131072_ & _99.47\%_ & _1.005_ \\ _Resid_ & \(6\) & _8e-05_ & _0.0003_ & _153.2_ & _99.93\%_ & _2.5426_ & _2.5524_ & _16.1549_ & _131072_ & _98.14\%_ & _0.984_ \\ _Resid_ & \(6\) & _6e-05_ & _0.0003_ & _215.7_ & _99.93\%_ & _2.5426_ & _2.5521_ & _16.1549_ & _131072_ & _93.91\%_ & _0.982_ \\ _Resid_ & \(6\) & _4e-05_ & _0.0003_ & _284.5_ & _99.62\%_ & _2.5426_ & _2.5948_ & _16.1549_ & _131072_ & _84.71\%_ & _2.56_ \\ _Resid_ & \(6\) & _2e-05_ & _0.0003_ & _801.3_ & _99.82\%_ & _2.5426_ & _2.5673_ & _16.1549_ & _131072_ & _91.71\%_ & _1.272_ \\ _Resid_ & \(6\) & _8e-06_ & _0.0003_ & _-288.2_ & _99.7\%_ & _2.5426_ & _2.5835_ & _16.1549_ & _131072_ & _85.02\%_ & _1.006_ \\ _Resid_ & _13_ & _0.0008_ & _0.0003_ & _5.4_ & _98.3\%_ & _2.5426_ & _3.0149_ & _30.3588_ & _131072_ & _98.15\%_ & _1.008_ \\ _Resid_ & _13_ & _0.0005_ & _0.0003_ & _13.1_ & _99.25\%_ & _2.5426_ & _2.5714_ & _30.3588_ & _131072_ & _98.71\%_ & _0.998_ \\ _Resid_ & _13_ & _0.0003_ & _0.0003_ & _31.8_ & _99.62\%_ & _2.5426_ & _2.5483_ & _30.3588_ & _131072_ & _99.31\%_ & _0.992_ \\ _Resid_ & _13_ & _0.0002_ & _0.0003_ & _62.6_ & _99.76\%_ & _2.5426_ & _2.6083_ & _30.3588_ & _131072_ & _99.69\%_ & _0.993_ \\ _Resid_ & _13_ & _0.0002_ & _0.0003_ & _63.7_ & _99.77\%_ & _2.5426_ & _2.6067_ & _30.3588_ & _131072_ & _99.68\%_ & _0.997_ \\ _Resid_ & _13_ & _0.0001_ & _0.0003_ & _146.1_ & _99.87\%_ & _2.5426_ & _2.5788_ & _30.3588_ & _131072_ & _67.47\%_ & _1.056_ \\ _Resid_ & _13_ & _0.0001_ & _0.0003_ & _96.8_ & _99.64\%_ & _2.5426_ & _2.6421_ & _30.3588_ & _131072_ & _64.18\%_ & _0.934_ \\ _Resid_ & _20_ & _0.001_ & _0.0003_ & _8.2_ & _96.15\%_ & _2.5426_ & _3.1995_ & _19.5891_ & _131072_ & _96.49\%_ & _1.004_ \\ _Resid_ & _20_ & _0.0009_ & _0.0003_ & _10.0_ & _96.7\%_ & _2.5426_ & _3.1059_ & _19.5891_ & _131072_ & _96.89\%_ & _1.003_ \\ _Resid_ & _20_ & _0.0008_ & _0.0003_ & _12.3_ & _97.14\%_ & _2.5426_ & _3.0293_ & _19.5891_ & _131072_ & _97.46\%_ & _0.997_ \\ _Resid_ & _20_ & _0.0007_ & _0.0003_ & _15.6_ & _97.7\%_ & _2.5426_ & _2.9353_ & _19.5891_ & _131072_ & _98.02\%_ & _0.997_ \\ _Resid_ & _20_ & _0.0005_ & _0.0003_ & _29.3_ & _98.62\%_ & _2.5426_ & _2.7775_

[MISSING_PAGE_FAIL:33]

\begin{table}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|} \hline \multirow{2}{*}{**Site**} & \multirow{2}{*}{**Layer**} & **Sparsity** & \multirow{2}{*}{**LR**} & \multirow{2}{*}{**L0**} & **\% CE** & **Clean** & **SAE** & **0 Abl.** & \multirow{2}{*}{**Width**} & **\% alive** & **Shrinkage** \\  & & \(\lambda\) & & & **Recovered** & **CE Loss** & **CE Loss** & **CE Loss** & **Width** & **Features** & \(\gamma\) \\ \hline _Attn_ & \(4\) & _8e-05_ & _0.001_ & _17.6_ & _81.04\%_ & _1.9699_ & _1.9824_ & _2.0361_ & _196608_ & _94.29\%_ & _0.827_ \\ _Attn_ & \(4\) & _6e-05_ & _0.001_ & _24.2_ & _84.12\%_ & _1.9699_ & _1.9804_ & _2.0361_ & _196608_ & _95.76\%_ & _0.848_ \\ _Attn_ & \(4\) & _3e-05_ & _0.001_ & _62.1_ & _90.96\%_ & _1.9699_ & _1.9759_ & _2.0361_ & _196608_ & _96.72\%_ & _0.93_ \\ _Attn_ & _12_ & _8e-05_ & _0.001_ & _16.1_ & _51.88\%_ & _1.9699_ & _1.9907_ & _2.0131_ & _196608_ & _65.73\%_ & _0.78_ \\ _Attn_ & _12_ & _6e-05_ & _0.001_ & _24.0_ & _58.46\%_ & _1.9699_ & _1.9878_ & _2.0131_ & _196608_ & _69.85\%_ & _0.802_ \\ _Attn_ & _12_ & _3e-05_ & _0.001_ & _75.0_ & _72.84\%_ & _1.9699_ & _1.9816_ & _2.0131_ & _196608_ & _73.04\%_ & _0.848_ \\ _Attn_ & _16_ & _0.00045_ & _0.001_ & _0.3_ & _-3.54\%_ & _1.9699_ & _2.0058_ & _2.0046_ & _49152_ & _20.1\%_ & _0.554_ \\ _Attn_ & _16_ & _8e-05_ & _0.001_ & _14.6_ & _67.69\%_ & _1.9699_ & _1.9811_ & _2.0046_ & _196608_ & _64.35\%_ & _0.798_ \\ _Attn_ & _16_ & _3e-05_ & _0.001_ & _63.0_ & _81.78\%_ & _1.9699_ & _1.9762_ & _2.0046_ & _196608_ & _70.75\%_ & _0.868_ \\ _Attn_ & _16_ & _6e-05_ & _0.001_ & _20.8_ & _72.07\%_ & _1.9699_ & _1.9796_ & _2.0046_ & _196608_ & _69.92\%_ & _0.813_ \\ _Attn_ & _16_ & _0.0001_ & _0.001_ & _9.5_ & _60.16\%_ & _1.9699_ & _1.9837_ & _2.0046_ & _49152_ & _88.32\%_ & _0.754_ \\ _Attn_ & _16_ & _9e-05_ & _0.001_ & _11.3_ & _62.62\%_ & _1.9699_ & _1.9829_ & _2.0046_ & _49152_ & _89.87\%_ & _0.769_ \\ _Attn_ & _20_ & _6e-05_ & _0.001_ & _18.3_ & _87.49\%_ & _1.9698_ & _1.9769_ & _2.0269_ & _196608_ & _63.81\%_ & _0.87_ \\ _Attn_ & _20_ & _8e-05_ & _0.001_ & _13.6_ & _85.63\%_ & _1.9698_ & _1.978__2_ & _2.0269_ & _196608_ & _60.17\%_ & _0.871_ \\ _Attn_ & _20_ & _3e-05_ & _0.001_ & _52.0_ & _91.92\%_ & _1.9698_ & _1.9744_ & _2.0269_ & _196608_ & _65.83\%_ & _0.899_ \\ _Attn_ & _28_ & _3e-05_ & _0.001_ & _91.9_ & _73.29\%_ & _1.9698_ & _1.9715_ & _1.976_ & _196608_ & _71.36\%_ & _0.817_ \\ _Attn_ & _28_ & _6e-05_ & _0.001_ & _20.6_ & _57.17\%_ & _1.9698_ & _1.9725_ & _1.976_ & _196608_ & _64.79\%_ & _0.771_ \\ _Attn_ & _28_ & _8e-05_ & _0.001_ & _12.5_ & _49.8\%_ & _1.9698_ & _1.9729_ & _1.976_ & _196608_ & _55.92\%_ & _0.747_ \\ _MLP_ & \(4\) & _3.5e-05_ & _0.001_ & _20.0_ & _86.36\%_ & _1.9698_ & _1.9802_ & _2.046_ & _196608_ & _95.6\%_ & _0.954_ \\ _MLP_ & \(4\) & _1e-05_ & _0.001_ & _64.5_ & _83.61\%_ & _1.9698_ & _1.9823_ & _2.046_ & _196608_ & _42.92\%_ & _0.977_ \\ _MLP_ & \(4\) & _2e-05_ & _0.001_ & _43.3_ & _87.2\%_ & _1.9698_ & _1.9796_ & _2.046_ & _196608_ & _74.78\%_ & _0.986_ \\ _MLP_ & _12_ & _3e-05_ & _0.001_ & _77.8_ & _81.95\%_ & _1.9698_ & _1.9783_ & _2.0167_ & _196608_ & _99.58\%_ & _0.932_ \\ \hline \end{tabular}
\end{table}
Table 8: Pythia-2.8B baseline SAEs (2048 sequence length). Continued in Table 9.

[MISSING_PAGE_FAIL:35]

\begin{table}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|} \hline
**Site** & **Layer** & \begin{tabular}{c} **Sparsity** \\ \(\lambda\) \\ \end{tabular} & **LR** & **L0** & \begin{tabular}{c} **\% CE** \\ **Recovered** \\ \end{tabular} & \begin{tabular}{c} **Clean** \\ **CE Loss** \\ \end{tabular} & \begin{tabular}{c} **SAE** \\ **CE Loss** \\ \end{tabular} & \begin{tabular}{c} **0 Abl.** \\ **CE Loss** \\ \end{tabular} & \begin{tabular}{c} **Width** \\ **Features** \\ \end{tabular} & 
\begin{tabular}{c} **\% Alive** \\ \(\gamma\) \\ \end{tabular} \\ \hline _Attn_ & \(4\) & _0.0006_ & _0.0003_ & _38.2_ & _92.85\%_ & _1.9699_ & _1.9746_ & _2.0361_ & _131072_ & _93.76\%_ & _1.0006_ \\ _Attn_ & \(4\) & _0.0004_ & _0.0003_ & _69.8_ & _94.82\%_ & _1.9699_ & _1.9733_ & _2.0361_ & _131072_ & _96.29\%_ & _1.0_ \\ _Attn_ & \(4\) & _0.0008_ & _0.0003_ & _24.7_ & _90.94\%_ & _1.9699_ & _1.9759_ & _2.0361_ & _131072_ & _91.45\%_ & _1.007_ \\ _Attn_ & _12_ & _0.0006_ & _0.0003_ & _64.5_ & _82.04\%_ & _1.9699_ & _1.9776_ & _2.0131_ & _131072_ & _74.48\%_ & _0.99_ \\ _Attn_ & _12_ & _0.001_ & _0.0003_ & _27.1_ & _73.09\%_ & _1.9699_ & _1.9815_ & _2.0131_ & _131072_ & _63.68\%_ & _0.987_ \\ _Attn_ & _12_ & _0.0008_ & _0.0003_ & _40.5_ & _77.52\%_ & _1.9699_ & _1.9796_ & _2.0131_ & _131072_ & _67.74\%_ & _0.998_ \\ _Attn_ & _16_ & _0.0001_ & _0.0003_ & _17.2_ & _79.67\%_ & _1.9699_ & _1.9769_ & _2.0046_ & _32768_ & _89.76\%_ & _0.988_ \\ _Attn_ & _16_ & _0.0006_ & _0.0003_ & _39.1_ & _87.21\%_ & _1.9699_ & _1.9743_ & _2.0046_ & _313072_ & _80.93\%_ & _0.985_ \\ _Attn_ & _16_ & _0.0009_ & _0.0003_ & _20.8_ & _81.8\%_ & _1.9699_ & _1.9762_ & _2.0046_ & _32768_ & _91.0\%_ & _0.993_ \\ _Attn_ & _16_ & _0.0004_ & _0.0003_ & _77.2_ & _90.56\%_ & _1.9699_ & _1.9732_ & _2.0046_ & _131072_ & _85.48\%_ & _0.987_ \\ _Attn_ & _16_ & _0.0008_ & _0.0003_ & _25.0_ & _83.57\%_ & _1.9699_ & _1.9756_ & _2.0046_ & _131072_ & _79.41\%_ & _0.993_ \\ _Attn_ & _16_ & _0.0005_ & _0.0003_ & _57.8_ & _88.63\%_ & _1.9699_ & _1.9738_ & _2.0046_ & _32768_ & _96.08\%_ & _0.992_ \\ _Attn_ & _20_ & _0.0004_ & _0.0003_ & _71.2_ & _96.25\%_ & _1.9698_ & _1.972_ & _2.0269_ & _131072_ & _88.74\%_ & _0.992_ \\ _Attn_ & _20_ & _0.0006_ & _0.0003_ & _36.5_ & _94.34\%_ & _1.9698_ & _1.973_ & _2.0269_ & _131072_ & _85.88\%_ & _0.986_ \\ _Attn_ & _20_ & _0.0008_ & _0.0003_ & _24.0_ & _93.05\%_ & _1.9698_ & _1.9738_ & _2.0269_ & _131072_ & _83.05\%_ & _0.994_ \\ _Attn_ & _28_ & _0.0008_ & _0.0003_ & _27.8_ & _73.39\%_ & _1.9698_ & _1.9715_ & _1.976_ & _131072_ & _68.41\%_ & _0.988_ \\ _Attn_ & _28_ & _0.001_ & _0.0003_ & _17.7_ & _68.35\%_ & _1.9698_ & _1.9718_ & _1.976_ & _131072_ & _68.14\%_ & _0.991_ \\ _Attn_ & _28_ & _0.0006_ & _0.0003_ & _51.2_ & _78.11\%_ & _1.9698_ & _1.9712_ & _1.976_ & _131072_ & _72.44\%_ & _0.986_ \\ _MLP_ & \(4\) & _0.0006_ & _0.0003_ & _28.6_ & _89.28\%_ & _1.9698_ & _1.978_ & _2.046_ & _131072_ & _99.16\%_ & _1.011_ \\ _MLP_ & \(4\) & _0.0004_ & _0.0003_ & _66.5_ & _92.74\%_ & _1.9698_ & _1.9754_ & _2.046_ & _131072_ & _99.52\%_ & _1.002_ \\ _MLP_ & \(4\) & _0.0008_ & _0.0003_ & _15.8_ & _87.13\%_ & _1.9698_ & _1.9796_ & _2.046_ & _131072_ & _98.46\%_ & _1.007_ \\ _MLP_ & _12_ & _0.001_ & _0.0003_ & _35.0_ & _81.33\%_ & _1.9698_ & _1.9786_ & _2.0167_ & _131072_ & _97.55\%_ & _1.011_ \\ _MLP_ & _12_ & _0.002_ & _0.0003_ & _8.2_ & _72.1\%_ & _1.9698_ & _1.9829_ & _2.0167_ & _131072_ & _94.68\%_ & _1.002_ \\ _MLP_ & _12_ & _0.0008_ & _0.0003_ & _55.7_ & _84.15\%_ & _1.9698_ & _1.9773_ & _2.0167_ & _131072_ & _98.23\%_ & _1.004_ \\ \hline \end{tabular}
\end{table}
Table 10: Pythia-2.8B Gated SAEs (2048 sequence length). Continued in Table 11.

[MISSING_PAGE_FAIL:37]

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: We have clearly stated our main contributions in the abstract and introduction, and have taken care to ensure they accurately reflect the contents of the paper. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We include a limitations section under the conclusion, discussing the limitations of our experimental design and of open questions around SAEs that our work does not address. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs**

[MISSING_PAGE_FAIL:39]

2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [No] Justification: We are unable to provide open access to the activation datasets or code used to train the SAEs in our experiments. However, the experiments we have reported involved training SAEs on solely open-source language models, and we have provided pseudo-code for our main contributions (Appendix A). These - alongside open-source codebases that are available for training SAEs (e.g. [5]) - should be enough to reproduce our main experimental results. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes]. Justification: The main body of the paper - in particular the training and evaluation subsections of Section 2, the Gated SAE definition of Section 3.2 and the experimental methodology subsections of Section 4 - explain training and evaluation details to a sufficient level of detail to be able to understand and appreciate the results. Details of hyperparameters, optimizer type, etc are provided in Appendix G. Guidelines: * The answer NA means that the paper does not include experiments.

* The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.
* The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [No] Justification: We do measure statistical significance in our interpretability study. Section 4.2 includes a detailed explanation of the statistical analysis undertaken to compare Gated and baseline SAEs (continued in Appendix I). The error bar methodology for Fig. 4 is explained in Footnote 9. However, error bars are not shown on the Pareto curve plots because it would have been computationally prohibitive. Nevertheless, the smoothness (or otherwise) of these curves across multiple values of \(\lambda\) does informally convey the noisiness of these curves. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors). * It should be clear whether the error bar is the standard deviation or the standard error of the mean. * It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. * For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). * If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: The final bullet in Appendix G covers the requirements below. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics**Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We have reviewed the Code and believe our research conforms to its requirements. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: We have discussed potential positive and negative societal impacts of our work in our impact statement (Appendix A). Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. * The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. * The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. * If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: We are not releasing models or data with this paper. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.

* Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.
* We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: The creators / owners of the three open-source model families used in our experiments [28, 3, 18] are credited in the main text, with license details (which have been respected) provided as part of their entries in the Bibliography. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. * If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: No new assets are being introduced in the paper. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The interpretability experiment (Section 4.2) did not use any external human subjects. All raters were permanent members of the research group conducting the study, hence no compensations details are provided. The instructions given to raters are provided in Section 4.2. The feature dashboards used by the raters were generated using a popular SAE visualisation library [27] that we have referenced when describing the experimental methodology; representative screenshots are available at the library's GitHub page (provided in its Bibliography entry). Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: IRB approvals were not required, as no external human subjects were used in the interpretability study. Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.