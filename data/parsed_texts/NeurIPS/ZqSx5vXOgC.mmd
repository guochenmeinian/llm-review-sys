Bypass Exponential Time Preprocessing: Fast Neural Network Training via Weight-Data Correlation Preprocessing

Josh Alman

josh@cs.columbia.edu. Columbia University.

Jiehao Liang

jiehao.liang@berkeley.edu. University of California, Berkeley.

Zhao Song

zsong@adobe.com. Adobe Research.

Ruizhe Zhang

ruizhe@utexas.edu. Simons Institute for the Theory of Computing.

Danyang Zhuo

danyang@cs.duke.edu. Duke University.

###### Abstract

Over the last decade, deep neural networks have transformed our society, and they are already widely applied in various machine learning applications. State-of-the-art deep neural networks are becoming larger in size every year to deliver increasing model accuracy, and as a result, model training consumes substantial computing resources and will only consume more in the future. Using current training methods, in each iteration, to process a data point \(x\in\mathbb{R}^{d}\) in a layer, we need to spend \(\Theta(md)\) time to evaluate all the \(m\) neurons in the layer. This means processing the entire layer takes \(\Theta(nmd)\) time for \(n\) data points. Recent work [20] reduces this time per iteration to \(o(nmd)\) but requires exponential time to preprocess either the data or the neural network weights, making it unlikely to have practical usage.

In this work, we present a new preprocessing method that simply stores the weight-data correlation in a tree data structure in order to quickly, and dynamically detect which neurons fire at each iteration. Our method requires only \(O(nmd)\) time in preprocessing and still achieves \(o(nmd)\) time per iteration. We complement our new algorithm with a lower bound, proving that assuming a popular conjecture from complexity theory, one could not substantially speed up our algorithm for dynamic detection of firing neurons.

## 1 Introduction

Machine learning applications are requiring larger and larger neural network size, and the computing resources required to train these large models is growing correspondingly. Determining how to train these models quickly has become an important research challenge.

Training a neural network is an iterative algorithm, and in each iteration, we need to process each of the \(m\) neurons on each of the \(n\) data points. Assuming each data point has a length of \(d\) (e.g., \(d\) could be the size of an input image), this means the per-iteration training time of the straightforward algorithm is at least \(\Omega(nmd)\) just to compute the activations. As we train larger neural networks on more training data, this running time can become a significant obstacle.

Recent work by Song, Yang, and Zhang [21] gave the first training algorithm that reduces this per iteration training time to \(o(nmd)\). The high-level idea of their algorithm is to use a nearest neighbor search data structure that stores the neural network weights and training data. This allows the training method to have fast access to the inner products of the training data with the current weight of the iteration. However, their algorithm's initial preprocessing time to set up the data structure is _exponential_ in the dimension \(d\), making it too slow in most applications. This raises a natural _theoretical_ question:

_Is it possible to design an algorithm that spends polynomial time to preprocess the weights and data, and which achieves a training time of \(o(nmd)\) per iteration?_

This question is important for two reasons. First, speeding up neural network training is a fundamental research challenge with real-world value. Second, dynamic data structures have been successfully used to speed up computations in many contexts throughout computer science, yet their power and limitations when applied to the training of neural networks are currently poorly understood.

### Our Result: An Upper Bound

Our main result answers this question in the affirmative, giving a new algorithm with efficient preprocessing and faster training in the natural over-parameterization regime (which has \(m\gg n\)):

**Theorem 1.1** (Main result).: _There is a data structure which preprocesses \(n\) data points in \(d\)-dimensional space, and \(m\) initialization weights points in \(d\)-dimensional space, in \(O(mnd)\) preprocessing time and \(O(mn+md+nd)\) space, which can be used to speed up neural network training: Running the gradient descent algorithm on a two-layer, \(m\)-width, over-parameterized ReLU neural network, which will minimize the training loss to zero, can be performed with an expected running time (of the gradient descent algorithm per iteration) of_

\[\widetilde{O}(m^{4/5}n^{2}d).\]

The following remark gives a comparison between our result and a closely related work [10]:

**Remark 1.2**.: _The prior work [10] presented two algorithms. Their first result (see Theorem 6.1 and Part 1 of Corollary B.6 in [10]) has \(O(2^{d})\) preprocessing time and uses \(O(m^{1-1/d}nd)\) cost per iteration. Their second result (see Theorem 6.2 and Part 2 of Corollary B.6 of [10]) has \(O(n^{d})\) preprocessing time and uses \(O(m^{4/5}nd)\) time per iteration. Our result exponentially improves the running time of the data structure in [10] in terms of the dimension \(d\). Notably, unlike [10], we do not use any complicated geometric data structure in previous work, and our algorithms are much easier to implement (see Algorithms 1 and 2). Moreover, as we discussed in Section 5, they can be parallelized to further reduce the cost-per-iteration to \(\widetilde{O}(m^{4/5}nd)\)._

Our key observation is that in each iteration of the training process, the weight updates are mostly sparse, and only a small fraction of neurons are activated for each training data point. Given this observation, we construct a binary search tree for each training data point (or neuron) to detect which neurons will fire. Our data structure and the corresponding algorithms are _deterministic_, not relying on any randomness, and solve the following dynamic algorithms problem which we prove appears as a key subroutine of the training process.

**Definition 1.3** (Dynamic Detection of Firing Neurons (DDFN)).: _Given two set of points \(X=\{x_{1},\ldots,x_{n}\}\subset\mathbb{Z}^{d}\), \(Y=\{y_{1},\ldots,y_{m}\}\subset\mathbb{Z}^{d}\) and a threshold \(b\in\mathbb{R}\), design a data structure to support the following operations:_

* \(\textsc{Update}(j\in[m],z\in\mathbb{Z}^{d})\)_, set_ \(y_{j}\) _to_ \(z\)__
* \(\textsc{Query}()\)_, either output the set_ \[Q=\{(i,j)\in[n]\times[m]\mid\langle x_{i},y_{j}\rangle\geq b\},\] _or report that_ \(|Q|>m^{4/5}n\)_._

We give a data structure for DDFN which takes \(O(mnd)\)-time for preprocessing, \(\widetilde{O}(nd)\)-time per update, and \(O(\min\{|Q|,m^{4/5}n\})\)-time per query. At a high level, our data structure works as follows.

PreprocessingWe build \(n\) binary search trees to maintain the \((x_{i},y_{j})\) pairs for \(i\in[n]\) and \(j\in[m]\). More specifically, the \(i\)-th tree has \(m\) leaf nodes, storing the inner-products between \(x_{i}\) and \(\{y_{j}\}_{j\in[m]}\). Each internal node stores the larger value of its two child nodes. The preprocessing time for the binary search trees for all the input data and neurons takes \(O(nmd)\) time and \(O(mn)\) space.

[MISSING_PAGE_FAIL:3]

[MISSING_PAGE_FAIL:4]

[MISSING_PAGE_FAIL:5]

The following lemma upper bounds the sparsity after initialization.

**Lemma 2.5** (Sparsity after initialization, informal version of Lemma B.3, [13]).: _Let \(b>0\) be a tunable parameter. If we setup the neural network as in Definition 2.1, then after the randomized initialization, with probability at least \(1-\exp(-\Omega(m\cdot\exp(-b^{2}/2)))\), it holds that for any input data \(x\), the number of activated neurons is at most \(O(m\cdot\exp(-b^{2}/2))\), where \(m\) is the total number of neurons._

**Remark 2.6**.: _This suggests that if we take \(b=\sqrt{0.4\log m}\), we achieve a sublinear number, \(O(m^{4/5})\), of activated neurons._

We can similarly control the sparsity in each iteration, and not just the first iteration; we defer the details to Section B.2.

In the next section, we will show how our weight-tree correlation data structure can take advantage of this sparsity phenomenon.

## 3 Correlation Tree Data Structure

In this section, we consider a neural network \(2\mathrm{NN}(m,b)\) (Definition 2.1) with \(n\) data points. We let \(\{w_{1},\cdots,w_{m}\}\subset\mathbb{R}^{d}\) be the weights, \(\{x_{1},\cdots,x_{n}\}\subset\mathbb{R}^{d}\) be the data points, and \(\{(w_{r},x_{i})\}_{r\in[m],i\in[n]}\subset\mathbb{R}^{m+n}\) be the weight-data pairs.

We propose two data structures: Correlation DTree and Correlation WTree. The DTree data structure has \(n\) trees, and its \(i\)-th tree has \(m\) leaf nodes corresponding to the set of inner-products between \(x_{i}\) and all hidden neurons, i.e., \(\{\langle w_{r},x_{i}\rangle\}_{r\in[m]}\). Similarly, the WTree data structure consists of \(m\) trees, and its \(r\)-th tree has \(n\) leaf nodes corresponding to the set of inner-products between the \(r\)-th neuron and all data points, i.e., \(\{\langle w_{r},x_{i}\rangle\}_{i\in[n]}\).

The Correlation Tree is a simple binary tree data structure. At a high level, it works as follows:

* **Tree construction** We first calculate the inner products of all weight-data pairs \(\langle w_{i},x_{j}\rangle\), each representing the evaluation of a neuron at a data point. To search activated neurons efficiently, we create a tree structure in the following way (taking the Correlation DTree as an example): we first build \(m\) leaf nodes, where the \(r\)-th leaf stores \(\langle w_{r},x_{i}\rangle\) for \(r\in[m]\). Then, we recursively construct a binary tree such that a parent node takes the larger value from its two child nodes. Finally, we obtain a tree with root having value \(\max_{r\in[m]}\{\langle w_{r},x_{i}\rangle\}\). Moreover, the value of each internal node equals to the maximum value among the leaf nodes in this subtree.
* **Efficient search** Given a threshold \(b\), the data structure can find all the pairs of vectors whose inner product is greater than \(b\). Take the Correlation DTree as an example. It outputs the indices of those activated neurons (i.e., \(\langle w_{r},x_{i}\rangle>b\)) in a recursive way: starting from the root, it checks whether it is "activated" (i.e., with value \(>b\)). If not, the search ends. Otherwise, it moves to each of the child nodes and repeats this searching process until stops. This is a typical depth-first search strategy. Its running time is determined by how many nodes it visits during searching. The number of visited nodes has the same magnitude as the number of visited leaf nodes, i.e., the number of activated neurons. Hence, the efficiency of our data structures relies on the sparsity phenomenon of the training process.
* **Relation between DTree and WTree** In the Correlation DTree, each weight vector \(w_{r}\) appears only in \(n\) different trees. In the Correlation WTree, each weight vector \(w_{r}\) appears only in one of the \(m\) trees. When \(w_{r}\) is updated, DTree will change the nodes along a root-to-leaf path in \(n\) trees, whereas WTree only changes such paths in the \(r\)-th tree.

### Correlation DTree data structure

We now state our main theorem summarizing the correlation DTree data structure. Its pseudocode is given in Algorithms 1 and 2 below. Its proof are deferred to Section D.1.

**Theorem 3.1** (Correlation DTree data structure).: _There exists a data structure with the following procedures:_* \(\textsc{Init}(\{w_{1},w_{2},\cdots,w_{m}\}\subset\mathbb{R}^{d},\{x_{1},x_{2}, \cdots,x_{n}\}\subset\mathbb{R}^{d},n\in\mathbb{N},m\in\mathbb{N},d\in\mathbb{N})\). _Given a series of weights_ \(w_{1},w_{2},\cdots,w_{m}\) _and data_ \(x_{1},x_{2},\cdots,x_{n}\) _in d-dimensional space, it performs preprocessing in time_ \(O(nmd)\)_._
* \(\textsc{Update}(z\in\mathbb{R}^{d},r\in[m])\)_. Given a weight_ \(z\) _and an index_ \(r\)_, it updates weight_ \(w_{r}\) _to_ \(z\) _in time_ \(O(n\cdot(d+\log m))\)_._
* \(\textsc{Query}(i\in[n],\tau\in\mathbb{R})\)_. Given an index_ \(i\) _indicating data point_ \(x_{i}\) _and a threshold_ \(\tau\)_, it finds all indices_ \(r\in[m]\) _such that_ \(\langle w_{r},x_{i}\rangle>\tau\) _in time_ \(O(|\widetilde{S}(\tau)|\cdot\log m)\)_, where_ \[\widetilde{S}(\tau):=\{r:\langle w_{r},x_{i}\rangle>\tau\}.\]

```
1:data structureCorrelationDTree\(\triangleright\) Theorem 3.1
2:members
3:\(W\in\mathbb{R}^{m\times d}\) (\(m\) weight vectors )
4:\(X\in\mathbb{R}^{n\times d}\) (\(n\) data points)
5: Binary tree \(T_{1},T_{2},\cdots,T_{n}\)\(\triangleright\)\(n\) binary search trees
6:end members
7:procedureInit(\(w_{1},w_{2},\cdots,w_{m}\in\mathbb{R}^{d},m,x_{1},x_{2},\cdots,x_{n}\in \mathbb{R}^{d}\), \(n\), \(m\), d) \(\triangleright\) Lemma D.2
8:for\(i=1\to n\)do
9:\(x_{i}\gets x_{i}\)
10:endfor
11:for\(j=1\to m\)do
12:\(w_{j}\gets w_{j}\)
13:endfor
14:for\(i=1\to n\)do\(\triangleright\) for data point, we create a tree
15:for\(j=1\to m\)do
16:\(u_{j}\leftarrow\langle x_{i},w_{j}\rangle\)
17:endfor
18:\(T_{i}\leftarrow\textsc{MakeMaxTree}(u_{1},\cdots,u_{m})\)\(\triangleright\) Each node stores the maximum value for his two children, Algorithm 7
19:endfor
20:endprocedure
21:procedureUpdate(\(z\in\mathbb{R}^{d},r\in[m]\))\(\triangleright\) Lemma D.3
22:\(w_{r}\gets z\)
23:for\(i=1\to n\)do
24:\(l\leftarrow\) the \(l\)-th leaf of tree \(T_{i}\)
25:\(l.\)value = \(\langle z,x_{i}\rangle\)
26:while\(l\) is not root do
27:\(p\leftarrow\) parent of \(l\)
28:\(p.\)value \(\leftarrow\max\{p.\)value, \(l.\)value\(\}\)
29:\(l\gets p\)
30:endwhile
31:endfor
32:endprocedure
33:endata structure ```

**Algorithm 1** Correlation DTree data structure

### Correlation WTree data structure

We next state the main theorem summarizing our similar Correlation WTree data structure. Both the Correlation DTree and Correlation WTree have a query time that is roughly equal to the output size, but since they have different outputs, each can be faster than the other depending on the setting. The pseudocode and proof for Correlation WTree are deferred to Section D.3.

**Theorem 3.2** (Correlation WTree data structure).: _There exists a data structure with the following procedures:_* \(\textsc{Init}(\{w_{1},w_{2},\cdots,w_{m}\}\subset\mathbb{R}^{d},\{x_{1},x_{2}, \cdots,x_{n}\}\subset\mathbb{R}^{d},n\in\mathbb{N},m\in\mathbb{N},d\in\mathbb{N})\). _Given a series of weights_ \(w_{1},w_{2},\cdots,w_{m}\) _and data_ \(x_{1},x_{2},\cdots,x_{n}\) _in d-dimensional space, it performs preprocessing in time_ \(O(nmd)\)_._
* \(\textsc{Update}(z\in\mathbb{R}^{d},r\in[m])\)_. Given a weight_ \(z\) _and index_ \(r\)_, it updates weight_ \(w_{r}\) _to_ \(z\) _in time_ \(O(nd)\)_._
* \(\textsc{Query}(r\in[m],\tau\in\mathbb{R})\)_. Given an index_ \(r\) _indicating weight_ \(w_{r}\) _and a threshold_ \(\tau\)_, it finds all indices_ \(i\in[n]\) _such that_ \(\langle w_{r},x_{i}\rangle>\tau\) _in time_ \(O(|S(\tau)|\cdot\log m)\)_, where_ \(S(\tau):=\{i:\langle w_{r},x_{i}\rangle>\tau\}\)_._

## 4 Running Time of Our Algorithm

In this section, we show how to apply the Correlation Tree data structures developed in Section 3 to speed up neural network training.

### Weights Preprocessing

```
1:procedureTrainingWithDTree(\(\{(x_{i},y_{i})\}_{i\in[n]}\),\(n\),\(m\),\(d\))\(\triangleright\)Theorem 4.1
2: Initialize \(w_{r},a_{r}\) for \(r\in[m]\) and \(b\) according to Section 2
3:\(\textsc{DTree.Init}(\{w_{r}(0)\}_{r\in[m]},m,d)\)\(\triangleright\)Algorithm 10
4:for\(t=1\to T\)do
5:\(S_{i,\mathrm{fire}}\leftarrow\textsc{DTree.Query}(x_{i},b)\) for \(i\in[n]\)
6: Forward pass for \(x_{i}\) only on neurons in \(S_{i,\mathrm{fire}}\) for \(i\in[n]\)
7: Calculate gradient for \(x_{i}\) only on neurons in \(S_{i,\mathrm{fire}}\) for \(i\in[n]\)
8: Gradient update for the neurons in \(\cup_{i\in[n]}S_{i,\mathrm{fire}}\)
9:\(\textsc{DTree.Update}(w_{r}(t+1),r)\)
10:endfor
11:return Trained weights \(w_{r}(T+1)\) for \(r\in[m]\)
12:endprocedure ```

**Algorithm 3** Training Neural Network based on Correlation DTree

In Algorithm 8, we use DTree structure to speed up the training process. We preprocess weights \(w_{r},r\in[m]\) for each data point \(x_{i},i\in[n]\) by constructing \(n\) weight-data correlation trees. In each iteration, Query finds the set of activated neurons \(S_{i,\mathrm{fire}}\) (Definition 2.4) efficiently for each data point \(x_{i}\) and Update helps change the weights in backward propagation.

Our main result for weight preprocessing is as follows.

**Theorem 4.1** (Running time part, informal version of Theorem E.1).: _Given \(n\) data points in \(\mathbb{R}^{d}\), gradient descent using the DTree data structure (Algorithm 8) for the neural network \(2\mathrm{NN}(m,b=\sqrt{0.4\log m})\) (Definition 2.1) takes \(O(m^{4/5}n^{2}d)\) time per iteration in expectation._

### Data Preprocessing

```
1:procedureTrainingWithWTree(\(\{(x_{i},y_{i})\}_{i\in[n]}\),\(n\),\(m\),\(d\))\(\triangleright\) Theorem 4.2
2: Initialize \(w_{r},a_{r}\) for \(r\in[m]\) and \(b\) according to Section 2
3:\(\texttt{WTree.Init}(\{x_{i}\}_{i\in[n]},n,d)\)\(\triangleright\) Algorithm 13
4:\(\widetilde{S}_{r,\mathrm{fire}}\leftarrow\texttt{WT.Query}(w_{r}(0),b)\) for \(r\in[m]\)\(\triangleright\) Data points fire set
5:\(S_{i,\mathrm{fire}}\leftarrow\{r\mid i\in\widetilde{S}_{r,\mathrm{fire}}\}\)\(\triangleright\) Hidden neurons fire set
6:for\(t=1\to T\)do
7: Forward pass for \(x_{i}\) only on neurons in \(S_{i,\mathrm{fire}}\) for \(i\in[n]\)
8: Calculate gradient for \(x_{i}\) only on neurons in \(S_{i,\mathrm{fire}}\) for \(i\in[n]\)
9:for\(r\in\cup_{i\in[n]}\mathcal{S}_{i,\mathrm{fire}}\)do
10:\(\widetilde{S}_{r,\mathrm{fire}}\leftarrow\texttt{WTree.Query}(w_{r}(t+1),b)\)
11: Update \(S_{i,\mathrm{fire}}\) for each \(i\in\widetilde{S}_{r,\mathrm{fire}}\)
12:endfor
13:endfor
14:return Trained weights \(w_{r}(T+1)\) for \(r\in[m]\)
15:endprocedure ```

**Algorithm 4** Training Neural Network based on Correlation WTree

Preprocessing weights based on data points is a common practice for neural networks. Here we consider its dual form: preprocessing input data \(x_{i},i\in[n]\) based on neural network weights \(w_{r},r\in[m]\). This can be easily done due to the symmetric property of the inner product that we used in the correlation tree structure.

Given a weight vector \(w_{r}\), we can quickly find \(\widetilde{S}_{i,\mathrm{fire}}\) (Definition 2.4) which contains the indices of data points that "fire" for weight \(w_{r}\). By the dual relationship between \(\widetilde{S}_{i,\mathrm{fire}}\) and \(S_{i,\mathrm{fire}}\), we can recover \(S_{i,\mathrm{fire}}\) easily.

One advantage of the data preprocessing approach is that the data structure only depends on the training dataset, instead of the neural network architecture. Therefore, the data structure could be pre-computed and stored in cloud platforms.

The performance guarantee of our data preprocessing training algorithm is shown as follows:

**Theorem 4.2** (Running time part, informal version of Theorem E.2).: _Given \(n\) data points in \(\mathbb{R}^{d}\), gradient descent algorithm using the WTree data structure (Algorithm 9) for the neural network \(2\mathrm{NN}(m,b=\sqrt{0.4\log m})\) takes \(O(m^{4/5}n\cdot\log n)\)-time per iteration to initialize \(\widetilde{S}_{r,\mathrm{fire}},S_{i,\mathrm{fire}}\) for \(r\in[m],i\in[n]\), and the total running time per iteration is_

\[O(m^{4/5}n^{2}d)\]

_in expectation._

## 5 Conclusion

Deep neural networks are becoming larger every year to offer improved model accuracy. Training these models consumes substantial resources, and resource consumption will only increase as these models grow. In traditional training methods, for each iteration, we need to spend \(\Theta(nmd)\) time to evaluate the \(m\) neurons on \(n\) data points with dimension \(d\). Recent work [20] reduced the per-iteration cost to \(o(nmd)\), but required exponential time to preprocess either the data or the neural weights. We develop a new method that reduces the preprocessing cost to \(O(nmd)\) while keeping the per-iteration running time at \(o(nmd)\). One limitation of our algorithm is that it has an \(n^{2}\) dependence in the cost-per-iteration. However, for very wide neural networks (with \(m\gg n\)), the runtime is still sublinear. More importantly, we design a simple binary tree-based dynamic geometric data structure that can efficiently identify all the activated neurons in each training iteration and bypass the high-dimensional barrier of the prior approach. We further remark that the Update procedure of DTree/Wtree structure (Algorithm 1 and 14) can be parallelized, where we can update all the correlation trees using distributed computing simultaneously. It will improve the running time from \(O(nd)\) to \(O(d)\), resulting in a total running time \(O(m^{4/5}nd)\) per iteration.

Our work naturally raises some open questions for future study:

* First, can we apply our data structure, together with an analysis of the sparsity in training over-parameterized neural networks [11, 12], to speed up neural network training with more than two layers? Giving a provable, theoretical backing for quickly training multi-layer networks remains an open, difficult challenge.
* Second, many empirical results (e.g., [13, 14]) indicate that only _approximately_ identifying the activated neurons (i.e., neurons with top-\(k\) inner products) in each iteration may still be enough to train a neural network. Can we provide a more theoretical understanding of these approaches?
* Third, our current algorithms use more memory (i.e., \(O(mn)\) space) to store the correlation tree data structure. Is it possible to reduce the space complexity of the algorithms?
* Fourth, we think it is possible that our data structures will work for more general activation functions. Roughly speaking, as long as the activated neurons are sparse or approximately sparse, our data structures will be able to theoretically reduce the cost-per-iteration. However, we need to re-prove the sparsification results in [11] for the activation function other than ReLU.

AcknowledgementsThe authors would like to thank Lichen Zhang for his helpful discussions. JA was partly supported by a grant from the Simons Foundation (Grant Number 825870 JA). Part of this research was performed while RZ was visiting the Institute for Pure and Applied Mathematics (IPAM), which is supported by the National Science Foundation (Grant No. DMS-1925919).

## References

* [AA22] Amol Aggarwal and Josh Alman. Optimal-degree polynomial approximations for exponentials and gaussian kernel density estimation. In _37th Computational Complexity Conference (CCC 2022)_. Schloss Dagstuhl-Leibniz-Zentrum fur Informatik, 2022.
* [ABH\({}^{+}\)18] Amir Abboud, Arturs Backurs, Thomas Dueholm Hansen, Virginia Vassilevska Williams, and Or Zamir. Subtree isomorphism revisited. _ACM Transactions on Algorithms (TALG)_, 14(3):1-23, 2018.
* [ABW15] Amir Abboud, Arturs Backurs, and Virginia Vassilevska Williams. Tight hardness results for lcs and other sequence similarity measures. In _2015 IEEE 56th Annual Symposium on Foundations of Computer Science_, pages 59-78. IEEE, 2015.
* [AC09] Peyman Afshani and Timothy M Chan. Optimal halfspace range reporting in three dimensions. In _Proceedings of the twentieth annual ACM-SIAM symposium on Discrete algorithms_, pages 180-186. SIAM, 2009.
* [ACSS20] Josh Alman, Timothy Chu, Aaron Schild, and Zhao Song. Algorithms and hardness for linear algebra on geometric graphs. In _2020 IEEE 61st Annual Symposium on Foundations of Computer Science (FOCS)_, pages 541-552. IEEE, 2020.
* [ADBMS98] Pankaj K Agarwal, Mark De Berg, Jiri Matousek, and Otfried Schwarzkopf. Constructing levels in arrangements and higher order voronoi diagrams. _SIAM journal on computing_, 27(3):654-667, 1998.
* [ADH\({}^{+}\)19a] Sanjeev Arora, Simon Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of optimization and generalization for overparameterized two-layer neural networks. In _International Conference on Machine Learning_, pages 322-332, 2019.

* [ADH\({}^{+}\)19b] Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, Ruslan Salakhutdinov, and Ruosong Wang. On exact computation with an infinitely wide neural net. In _NeurIPS_, 2019.
* [AEM92] Pankaj K Agarwal, David Eppstein, and Jiri Matousek. Dynamic half-space reporting, geometric optimization, and minimum spanning trees. In _Annual Symposium on Foundations of Computer Science (FOCS)_, volume 33, pages 80-80, 1992.
* [AIL\({}^{+}\)15] Alexandr Andoni, Piotr Indyk, TMM Laarhoven, Ilya Razenshteyn, and Ludwig Schmidt. Practical and optimal lsh for angular distance. In _Advances in Neural Information Processing Systems (NIPS)_, pages 1225-1233. Curran Associates, 2015.
* [AIR18] Alexandr Andoni, Piotr Indyk, and Ilya Razenshteyn. Approximate nearest neighbor search in high dimensions. _arXiv preprint arXiv:1806.09823_, 7, 2018.
* [AR15] Alexandr Andoni and Ilya Razenshteyn. Optimal data-dependent hashing for approximate near neighbors. In _Proceedings of the forty-seventh annual ACM symposium on Theory of computing (STOC)_, pages 793-801, 2015.
* [ARN17] Alexandr Andoni, Ilya Razenshteyn, and Negev Shekel Nosatzki. Lsh forest: Practical algorithms made theoretical. In _Proceedings of the Twenty-Eighth Annual ACM-SIAM Symposium on Discrete Algorithms (SODA)_, pages 67-78. SIAM, 2017.
* [AS23a] Josh Alman and Zhao Song. Fast attention requires bounded entries. In _NeurIPS_. arXiv preprint arXiv:2302.13214, 2023.
* [AS23b] Josh Alman and Zhao Song. How to capture higher-order correlations? generalizing matrix softmax attention to kronecker computation. _arXiv preprint arXiv:2310.04064_, 2023.
* [AWW14] Amir Abboud, Virginia Vassilevska Williams, and Oren Weimann. Consequences of faster alignment of sequences. In _International Colloquium on Automata, Languages, and Programming_, pages 39-51. Springer, 2014.
* [AWY14] Amir Abboud, Ryan Williams, and Huacheng Yu. More applications of the polynomial method to algorithm design. In _Proceedings of the twenty-sixth annual ACM-SIAM symposium on Discrete algorithms_, pages 218-230. SIAM, 2014.
* [AZLS19a] Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-parameterization. In _ICML_, 2019.
* [AZLS19b] Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. On the convergence rate of training recurrent neural networks. In _NeurIPS_, 2019.
* [BBK\({}^{+}\)16] Kevin Buchin, Maike Buchin, Maximilian Konzack, Wolfgang Mulzer, and Andre Schulz. Fine-grained analysis of problems on curves. _EuroCG, Lugano, Switzerland_, 2016.
* [Ber24] Sergei Bernstein. On a modification of chebyshev's inequality and of the error formula of laplace. _Ann. Sci. Inst. Sav. Ukraine, Sect. Math_, 1(4):38-49, 1924.
* [BGL17] Karl Bringmann, Allan Gronlund, and Kasper Green Larsen. A dichotomy for regular expression membership testing. In _2017 IEEE 58th Annual Symposium on Foundations of Computer Science (FOCS)_, pages 307-318. IEEE, 2017.
* [BI15] Arturs Backurs and Piotr Indyk. Edit distance cannot be computed in strongly subquadratic time (unless seth is false). In _Proceedings of the forty-seventh annual ACM symposium on Theory of computing_, pages 51-58, 2015.
* [BI16] Arturs Backurs and Piotr Indyk. Which regular expression patterns are hard to match? In _2016 IEEE 57th Annual Symposium on Foundations of Computer Science (FOCS)_, pages 457-466. IEEE, 2016.
* [BIP19] Arturs Backurs, Piotr Indyk, and Tal Wagner. Space and time efficient kernel density estimation in high dimensions. In _NeurIPS_, pages 15773-15782, 2019.

* [BK18] Karl Bringman and Marvin Kunnemann. Multivariate fine-grained complexity of longest common subsequence. In _Proceedings of the Twenty-Ninth Annual ACM-SIAM Symposium on Discrete Algorithms_, pages 1216-1235. SIAM, 2018.
* [BM16] Karl Bringmann and Wolfgang Mulzer. Approximability of the discrete frechet distance. _Journal of Computational Geometry_, 7(2):46-76, 2016.
* [BPSW21] Jan van den Brand, Binghui Peng, Zhao Song, and Omri Weinstein. Training (overparametrized) neural networks in near-linear time. In _12th Innovations in Theoretical Computer Science Conference (ITCS)_, 2021.
* [Bri14] Karl Bringmann. Why walking the dog takes time: Frechet distance has no strongly subquadratic algorithms unless seth fails. In _2014 IEEE 55th Annual Symposium on Foundations of Computer Science_, pages 661-670. IEEE, 2014.
* [BSZ23] Jan van den Brand, Zhao Song, and Tianyi Zhou. Algorithm and hardness for dynamic attention maintenance in large language models. _arXiv preprint arXiv:2304.02207_, 2023.
* [CG19] Yuan Cao and Quanquan Gu. Generalization bounds of stochastic gradient descent for wide and deep neural networks. In _NeurIPS_, pages 10835-10845, 2019.
* [CGH\({}^{+}\)19] Tianle Cai, Ruiqi Gao, Jikai Hou, Siyu Chen, Dong Wang, Di He, Zhihua Zhang, and Liwei Wang. Gram-gauss-newton method: Learning overparameterized neural networks for regression problems. _arXiv preprint arXiv:1905.11675_, 2019.
* [Cha00] Timothy M Chan. Random sampling, halfspace range reporting, and construction of (\(\leq k\))-levels in three dimensions. _SIAM Journal on Computing_, 30(2):561-575, 2000.
* [Cha02] Moses S Charikar. Similarity estimation techniques from rounding algorithms. In _Proceedings of the thiry-fourth annual ACM symposium on Theory of computing (STOC)_, pages 380-388, 2002.
* [Cha12] Timothy M Chan. Optimal partition trees. _Discrete & Computational Geometry_, 47(4):661-690, 2012.
* [Cha19] Timothy M Chan. Orthogonal range searching in moderate dimensions: kd trees and range trees strike back. _Discrete & Computational Geometry_, 61(4):899-922, 2019.
* [Che20] Lijie Chen. On the hardness of approximate and exact (bichromatic) maximum inner product. _Theory OF Computing_, 16(4):1-50, 2020.
* [CIP09] Chris Calabro, Russell Impagliazzo, and Ramamohan Paturi. The complexity of satisfiability of small depth circuits. In _International Workshop on Parameterized and Exact Computation_, pages 75-85. Springer, 2009.
* [CLP\({}^{+}\)21] Beidi Chen, Zichang Liu, Binghui Peng, Zhaozhuo Xu, Jonathan Lingjie Li, Tri Dao, Zhao Song, Anshumali Shrivastava, and Christopher Re. Mongoose: A learnable lsh framework for efficient neural network training. In _ICLR oral_, 2021.
* [CLS19] Michael B Cohen, Yin Tat Lee, and Zhao Song. Solving linear programs in the current matrix multiplication time. In _STOC_, 2019.
* [CMF\({}^{+}\)20] Beidi Chen, Tharun Medini, James Farwell, Sameh Gobriel, Charlie Tai, and Anshumali Shrivastava. Slide: In defense of smart algorithms over hardware acceleration for large-scale deep learning systems. In _In Proceedings of the 3rd Conference on Machine Learning and Systems (MLSys)_, 2020.
* [CT17] Timothy M Chan and Konstantinos Tsakalidis. Dynamic orthogonal range searching on the ram, revisited. _Leibniz International Proceedings in Informatics, LIPIcs_, 77:281-2813, 2017.

* [CW16] Timothy M Chan and Ryan Williams. Deterministic apsp, orthogonal vectors, and more: Quickly derandomizing razborov-smolensky. In _Proceedings of the twenty-seventh annual ACM-SIAM symposium on Discrete algorithms_, pages 1246-1255. SIAM, 2016.
* [CW19] Lijie Chen and Ryan Williams. An equivalence class for orthogonal vectors. In _Proceedings of the Thirtieth Annual ACM-SIAM Symposium on Discrete Algorithms_, pages 21-40. SIAM, 2019.
* [CWX22] Timothy M Chan, Virginia Vassilevska Williams, and Yinzhan Xu. Hardness for triangle problems under even more believable hypotheses: Reductions from real apsp, real 3sum, and ov. _arXiv preprint arXiv:2203.08356_, 2022.
* [DCL\({}^{+}\)22] Tri Dao, Beidi Chen, Kaizhao Liang, Jiaming Yang, Zhao Song, Atri Rudra, and Christopher Re. Pixelated butterfly: Simple and efficient sparse training for neural network models. In _ICLR_. arXiv preprint arXiv:2112.00029, 2022.
* [DIIM04] Mayur Datar, Nicole Immorlica, Piotr Indyk, and Vahab S Mirrokni. Locality-sensitive hashing scheme based on p-stable distributions. In _Proceedings of the twentieth annual symposium on Computational geometry (SoCG)_, pages 253-262, 2004.
* [DIRW20] Yihe Dong, Piotr Indyk, Ilya Razenshteyn, and Tal Wagner. Learning space partitions for nearest neighbor search. In _ICLR_. arXiv preprint arXiv:1901.08544, 2020.
* [DLW22] Mina Dalirrooyfard, Ray Li, and Virginia Vassilevska Williams. Hardness of approximate diameter: Now for undirected graphs. In _2021 IEEE 62nd Annual Symposium on Foundations of Computer Science (FOCS)_, pages 1021-1032. IEEE, 2022.
* [DMS23] Yichuan Deng, Sridhar Mahadevan, and Zhao Song. Randomized and deterministic attention sparsification algorithms for over-parameterized feature dimension. _arXiv preprint arXiv:2304.04397_, 2023.
* [DSZ23] Yichuan Deng, Zhao Song, and Tianyi Zhou. Superiority of softmax: Unveiling the performance edge over linear attention. _arXiv preprint arXiv:2310.11685_, 2023.
* [DZPS19] Simon S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes over-parameterized neural networks. In _ICLR_, 2019.
* [GIKW18] Jiawei Gao, Russell Impagliazzo, Antonina Kolokolova, and Ryan Williams. Completeness for first-order properties on sparse structures with algorithmic applications. _ACM Transactions on Algorithms (TALG)_, 15(2):1-35, 2018.
* [GMS23] Yeqi Gao, Sridhar Mahadevan, and Zhao Song. An over-parameterized exponential regression. _arXiv preprint arXiv:2303.16504_, 2023.
* [GS22] Yuzhou Gu and Zhao Song. A faster small treewidth sdp solver. _arXiv preprint arXiv:2211.06033_, 2022.
* [GSY23] Yeqi Gao, Zhao Song, and Junze Yin. Gradientcoin: A peer-to-peer decentralized large language models. _arXiv preprint arXiv:2308.10502_, 2023.
* [GSYZ23] Yeqi Gao, Zhao Song, Xin Yang, and Ruizhe Zhang. Fast quantum algorithm for attention computation. _arXiv preprint arXiv:2307.08045_, 2023.
* [GSZ23] Yuzhou Gu, Zhao Song, and Lichen Zhang. A nearly-linear time algorithm for structured support vector machines. _arXiv preprint arXiv:2307.07735_, 2023.
* [HLSY21] Baihe Huang, Xiaoxiao Li, Zhao Song, and Xin Yang. Fl-ntk: A neural tangent kernel-based framework for federated learning convergence analysis. In _ICML_, 2021.
* [HSWZ22] Hang Hu, Zhao Song, Omri Weinstein, and Danyang Zhuo. Training overparametrized neural networks in sublinear time. _arXiv preprint arXiv:2208.04508_, 2022.

* [IM98] Piotr Indyk and Rajeev Motwani. Approximate nearest neighbors: towards removing the curse of dimensionality. In _Proceedings of the thirtieth annual ACM symposium on Theory of computing (STOC)_, pages 604-613, 1998.
* [IP01] Russell Impagliazzo and Ramamohan Paturi. On the complexity of k-sat. _Journal of Computer and System Sciences_, 62(2):367-375, 2001.
* [JGH18] Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and generalization in neural networks. In _Advances in neural information processing systems_, pages 8571-8580, 2018.
* [JT20] Ziwei Ji and Matus Telgarsky. Polylogarithmic width suffices for gradient descent to achieve arbitrarily small test error with shallow relu networks. In _ICLR_, 2020.
* [KKL20] Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. _arXiv preprint arXiv:2001.04451_, 2020.
* [KM20] CS Karthik and Pasin Manurangsi. On closest pair in euclidean metric: Monochromatic is as hard as bichromatic. _Combinatorica_, 40(4):539-573, 2020.
* [KT18] Robert Krauthgamer and Ohad Trabelsi. Conditional lower bounds for all-pairs maxflow. _ACM Transactions on Algorithms (TALG)_, 14(4):1-15, 2018.
* [LL18] Yuanzhi Li and Yingyu Liang. Learning overparameterized neural networks via stochastic gradient descent on structured data. In _NeurIPS_, 2018.
* [LS01] W.V. Li and Q.-M. Shao. Gaussian processes: Inequalities, small ball probabilities and applications. In _Stochastic Processes: Theory and Methods_, volume 19 of _Handbook of Statistics_, pages 533-597. Elsevier, 2001.
* [LSS\({}^{+}\)20] Jason D Lee, Ruoqi Shen, Zhao Song, Mengdi Wang, and Zheng Yu. Generalized leverage score sampling for neural networks. In _NeurIPS_, 2020.
* [LSY23] Xiaoxiao Li, Zhao Song, and Jiaming Yang. Federated adversarial learning: A framework with convergence analysis. In _International Conference on Machine Learning_, pages 19932-19959. PMLR, 2023.
* [LWD\({}^{+}\)23] Zichang Liu, Jue Wang, Tri Dao, Tianyi Zhou, Binhuang Yuan, Zhao Song, Anshumali Shrivastava, Yuandong Tian Ce Zhang, Christopher Re, and Beidi Chen. Deja vu: Contextual sparsity for efficient lms at inference time. In _ICML_, 2023.
* [Mat92a] Jiri Matousek. Efficient partition trees. _Discrete & Computational Geometry_, 8(3):315-334, 1992.
* [Mat92b] Jiri Matousek. Reporting points in halfspaces. _Computational Geometry_, 2(3):169-186, 1992.
* [MOSW22] Alexander Munteanu, Simon Omlor, Zhao Song, and David Woodruff. Bounding the width of neural networks via coupled initialization a worst case analysis. In _International Conference on Machine Learning (ICML)_, pages 16083-16122. PMLR, 2022.
* [OS20] Samet Oymak and Mahdi Soltanolkotabi. Toward moderate overparameterization: Global convergence guarantees for training shallow neural networks. _IEEE Journal on Selected Areas in Information Theory_, 1(1):84-105, 2020.
* [QSS23] Lianke Qin, Zhao Song, and Baocheng Sun. Is solving graph neural tangent kernel equivalent to training graph neural network? _arXiv preprint arXiv:2309.07452_, 2023.
* [QSY23] Lianke Qin, Zhao Song, and Yuanyuan Yang. Efficient sgd neural network training via sublinear activated neuron identification. _arXiv preprint arXiv:2307.06565_, 2023.
* [Raz17] Ilya Razenshteyn. _High-dimensional similarity search and sketching: algorithms and hardness_. PhD thesis, Massachusetts Institute of Technology, 2017.

* [Rub18] Aviad Rubinstein. Hardness of approximate nearest neighbor search. In _Proceedings of the 50th annual ACM SIGACT symposium on theory of computing_, pages 1260-1268, 2018.
* [RVW13] Liam Roditty and Virginia Vassilevska Williams. Fast approximation algorithms for the diameter and radius of sparse graphs. In _Proceedings of the forty-fifth annual ACM symposium on Theory of computing_, pages 515-524, 2013.
* [SL14] Anshumali Shrivastava and Ping Li. Asymmetric lsh (alsh) for sublinear time maximum inner product search (mips). _Advances in Neural Information Processing Systems (NIPS)_, pages 2321-2329, 2014.
* [SL15a] Anshumali Shrivastava and Ping Li. Asymmetric minwise hashing for indexing binary inner products and set containment. In _Proceedings of the 24th international conference on world wide web (WWW)_, pages 981-991, 2015.
* [SL15b] Anshumali Shrivastava and Ping Li. Improved asymmetric locality sensitive hashing (alsh) for maximum inner product search (mips). In _Proceedings of the Thirty-First Conference on Uncertainty in Artificial Intelligence (UAI)_, pages 812-821, 2015.
* [SWYZ23] Zhao Song, Yitan Wang, Zheng Yu, and Lichen Zhang. Sketching for first order method: Efficient algorithm for low-bandwidth channel and vulnerability. In _International Conference on Machine Learning_, pages 32365-32417. PMLR, 2023.
* [SY19] Zhao Song and Xin Yang. Quadratic suffices for over-parametrization via matrix chernoff bound. _arXiv preprint arXiv:1906.03593_, 2019.
* [SYZ21] Zhao Song, Shuo Yang, and Ruizhe Zhang. Does preprocessing help training over-parameterized neural networks? _Advances in Neural Information Processing Systems_, 34, 2021.
* [SZZ21] Zhao Song, Lichen Zhang, and Ruizhe Zhang. Training multi-layer over-parametrized neural network in subquadratic time. _arXiv preprint arXiv:2112.07628_, 2021.
* [TOG17] Csaba D Toth, Joseph O'Rourke, and Jacob E Goodman. _Handbook of discrete and computational geometry_. CRC press, 2017.
* [Wil05] Ryan Williams. A new algorithm for optimal 2-constraint satisfaction and its implications. _Theoretical Computer Science_, 348(2-3):357-365, 2005.
* [Wil18a] Ryan Williams. On the difference between closest, furthest, and orthogonal pairs: Nearly-linear vs barely-subquadratic complexity. In _Proceedings of the Twenty-Ninth Annual ACM-SIAM Symposium on Discrete Algorithms_, pages 1207-1215. SIAM, 2018.
* [Wil18b] Virginia Vassilevska Williams. On some fine-grained questions in algorithms and complexity. In _Proceedings of the International Congress of Mathematicians: Rio de Janeiro 2018_, pages 3447-3487. World Scientific, 2018.
* [YJZ\({}^{+}\)23] Hongru Yang, Ziyu Jiang, Ruizhe Zhang, Zhangyang Wang, and Yingbin Liang. Convergence and generalization of wide neural networks with large bias. _CoRR_, abs/2301.00327, 2023.
* [ZG19] Difan Zou and Quanquan Gu. An improved analysis of training over-parameterized deep neural networks. In _NeurIPS_, pages 2053-2062, 2019.
* [Zha22] Lichen Zhang. Speeding up optimizations via data structures: Faster search, sample and maintenance. Master's thesis, Carnegie Mellon University, 2022.
* [ZMG19] Guodong Zhang, James Martens, and Roger B Grosse. Fast convergence of natural gradient descent for over-parameterized neural networks. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2019.

* [ZPD\({}^{+}\)20] Yi Zhang, Orestis Plevrakis, Simon S Du, Xingguo Li, Zhao Song, and Sanjeev Arora. Over-parameterized adversarial training: An analysis overcoming the curse of dimensionality. In _NeurIPS_. arXiv preprint arXiv:2002.06668, 2020.
* [ZSZ\({}^{+}\)23] Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao Song, Yuandong Tian, Christopher Re, Clark Barrett, Zhangyang Wang, and Beidi Chen. H\({}_{2}\)O : Heavy-hitter oracle for efficient generative inference of large language models. In _NeurIPS_. arXiv preprint arXiv:2306.14048, 2023.

## Appendix

Roadmap.We restate our notation and provide additional tools about probability in section A. Then we present the results about sparsity in section B. In section C, we demonstrate the idea of two different correlation trees, DTree and WTree, and present the full version of training algorithms using our data structure. In section D, we provide detailed implementation and analysis of running time for our data structure. Section E presents the proof of running time for training a \(2\mathrm{NN}(m,b)\) using DTree and WTree. Section F shows a formal version of lower bound for dynamic detection of firing neurons.

## Appendix A Preliminary

### Basic Notation

For any positive integer \(n\), we use \([n]\) to denote the set \(\{1,2,\cdots,n\}\). We use \(\mathbb{E}[X]\) to denote the expected value of a random variable \(X\), and \(\Pr[Y]\) to denote the probability of a random event \(Y\). For a matrix \(M\), we write \(M^{\top}\) to denote the transpose of \(M\). We use \(x^{\top}y\) to denote the inner product between vectors \(x\) and \(y\). We use \(I_{d}\) to denote a \(d\)-dimensional identity matrix. We use \(\mathcal{N}(\mu;\sigma^{2})\) to denote the Gaussian distribution with mean \(\mu\) and variance \(\sigma^{2}\).

### Upper bound on the movement of weights per iteration

The following Claim is quite standard in the literature, we omit the details.

**Claim A.1** (Corollary 4.1 in [10], Lemma 3.8 in [11]).: _Let \(\mathrm{err}(i)\) be defined as Definition A.2. If \(\forall i\in[t],\|\mathrm{err}(i)\|_{2}^{2}\leq(1-\eta\lambda/2)^{i}\cdot\| \mathrm{err}(0)\|_{2}^{2}\), then_

\[\|W(t+1)-W_{r}(0)\|_{\infty,2}\leq 4\lambda^{-1}m^{-1/2}\cdot\sqrt{n}\cdot\| \mathrm{err}(0)\|_{2}:=D.\]

This claim shows a uniform bound on the movement of weights.

Next, we introduce the definition of error of prediction.

**Definition A.2** (Error of prediction).: _For each \(t\in\{0,1,\cdots,T\}\), we define \(\mathrm{err}(t)\in\mathbb{R}^{n}\) to be the error of prediction \(\mathrm{err}(t)=y-u(t)\), where \(u(t):=f(W(t),a,X)\in\mathbb{R}^{n}\)_

### Probabilities

We introduce the classical Bernstein inequality here.

**Lemma A.3** (Bernstein inequality [1]).: _Assume \(Z_{1},\cdots,Z_{n}\) are \(n\) i.i.d. random variables. \(\forall i\in[n]\), \(\mathbb{E}[Z_{i}]=0\) and \(|Z_{i}|\leq M\) almost surely. Let \(Z=\sum_{i=1}^{n}Z_{i}\). Then,_

\[\Pr\left[Z>t\right]\leq\exp\left(-\frac{t^{2}/2}{\sum_{j=1}^{n}\mathbb{E}[Z_{j }^{2}]+Mt/3}\right),\forall t>0.\]

Next, we show an inequality on a shifted small ball with a Gaussian distribution.

**Claim A.4** (Theorem 3.1 in [11]).: _Let \(b>0\) and \(r>0\). Then,_

\[\exp(-b^{2}/2)\Pr_{x\sim\mathcal{N}(0,1)}[|x|\leq r]\leq\ \Pr_{x\sim\mathcal{N}(0,1)}[|x-b| \leq r]\leq\ \Pr_{x\sim\mathcal{N}(0,1)}[|x|\leq r].\]

We state the anti-concentration inequality here.

**Lemma A.5** (Anti-concentration for Gaussian distribution).: _Let \(Z\sim\mathcal{N}(0,\sigma^{2})\). Then, for \(t>0\),_

\[\Pr[|Z|\leq t]\leq\frac{2t}{\sqrt{2\pi}\sigma}.\]

## Appendix B Sparsity

In this section, we start by restating the result about sparsity after initialization in Section 2. Then we show how to bound the number of fire neurons per iteration in Section B.2.

### Sparsity after initialization

The goal of this section is to prove the sparsity after the initialization of the neural network.

We start by defining the "fire" set.

**Definition B.1** (fire set, Definition 3.7 in [20]).: _Fix a query point \(x\in\mathbb{R}^{d}\), let \(\mathcal{S}_{\mathrm{fire}}(x)\) denote the set of neurons that are "fire", i.e.,_

\[\mathcal{S}_{\mathrm{fire}}(x):=\{i\in[m]:\langle x,w_{i}\rangle>b\}\]

Next, we introduce the fire set for each training iteration.

**Definition B.2** (fire set per iteration, Definition 3.7 in [20]).: _For each data point \(x_{i}\in\mathbb{R}^{d},i\in[n]\), weight \(w_{r}\in\mathbb{R}^{d},r\in[m]\) and each iteration \(t\in\{0,1,\cdots,T\}\), we define_

\[S_{i,\mathrm{fire}}(t) :=r\in[m]:\langle x_{i},w_{r}(t)\rangle\] \[\widetilde{S}_{r,\mathrm{fire}}(t) :=i\in[n]:\langle x_{i},w_{r}(t)\rangle\]

_Also, we define \(k_{i,t}:=|S_{i,\mathrm{fire}}(t)|\) and \(\widetilde{k}_{r,t}:=|\widetilde{S}_{r,\mathrm{fire}}(t)|\)_

With the above definitions, we can state the sparsity after initialization.

**Lemma B.3** (Sparsity after initialization, formal version of Lemma 2.5, Lemma 3.8 in [20]).: _Let \(b>0\) be a tunable parameter. If we use the \(\Phi_{b}\) as the activation function, then after the initialization, with probability at least \(1-\exp(-\Omega(m\cdot\exp(-b^{2}/2)))\), it holds that for input data \(x\), the number of activated neurons \(k_{x}\) is at most \(O(m\cdot\exp(-b^{2}/2))\), where \(m\) is the total number of neurons._

### Bounding the number of fired neuron per iteration per level

In this section, we will show that for \(t=0,1,\ldots,T,k=0,1,\cdots,\log m\), the number of fire neurons \(k_{i,k,t}=|\mathcal{S}_{i,k,\mathrm{fire}}(t)|\) is small with high probability.

We define the set of neurons that are flipping at time \(t\):

**Definition B.4** (flip set, Definition C.8 in [20]).: _For each \(i\in[n]\), for each time \(t\in[T]\) let \(\mathcal{S}_{i,\mathrm{flip}}(t)\subset[m]\) denote the set of neurons that are never flipped during the entire training process,_

\[\mathcal{S}_{i,\mathrm{flip}}(t):=\{r\in[m]:\ \mathrm{sgn}(\langle w_{r}(t),x_{i} \rangle-b)\neq\mathrm{sgn}(\langle w_{r}(t-1),x_{i}\rangle-b)\}.\]

Over all the iterations of training algorithm, there are some neurons that never flip states. We provide a mathematical formulation of that set,

**Definition B.5** (noflip set, Definition C.9 in [20]).: _For each \(i\in[n]\), let \(S_{i}\subset[m]\) denote the set of neurons that are never flipped during the entire training process,_

\[S_{i}:=\{r\in[m]:\forall t\in[T]\ \mathrm{sgn}(\langle w_{r}(t),x_{i} \rangle-b)=\mathrm{sgn}(\langle w_{r}(0),x_{i}\rangle-b)\}.\] (1)

In Lemma 2.5, we already show that \(k_{i,0}=O(m\cdot\exp(-b^{2}/2))\) for all \(i\in[n]\) with high probability. We can show that it also holds for \(t>0\).

**Lemma B.6** (Bounding the number of fired neuron per iteration, Lemma C.10 in [20]).: _Let \(b\geq 0\) be a parameter, and let \(\sigma_{b}(x)=\max\{x,b\}\) be the activation function. For each \(i\in[n],t\in[T]\), \(k_{i,t}\) is the number of activated neurons at the \(t\)-th iteration. For \(0<t\leq T\), with probability at least \(1-n\cdot\exp\left(-\Omega(m)\cdot\min\{R,\exp(-b^{2}/2)\}\right)\), \(k_{i,t}\) is at most \(O(m\exp(-b^{2}/2))\) for all \(i\in[n]\)._

## Appendix C Algorithm

In this section, we explain the procedures for two correlation tree data structure. We use the same setting as Section 3

For the DTree data structure, it contains \(n\) binary trees indexed by \(n\) data points and supports the following operations:* **Initialize** It takes data points \(\{x_{1},\cdots,x_{n}\}\subset\mathbb{R}^{d}\) and weights \(\{w_{1},\cdots,w_{m}\}\subset\mathbb{R}^{d}\) as input and compute inner products of all weight-data pairs \((w_{r},x_{i})\). It uses these inner products to create \(n\) different trees. For the \(i\)-th tree based on data point \(x_{i}\), it is constructed from \(m\) leaf nodes \(\langle w_{r},x_{i}\rangle,r\in[m]\) and satisfies the property that the value of parent node is the maximum value of its child nodes.
* **Update** It takes a new weight \(z\in\mathbb{R}^{d}\) and an index \(r\in[m]\) as input. For the \(i\)-th tree, it calculate the new inner product \(\langle z,x_{i}\rangle\) and stores the value into the \(r\)-th leaf node. Then it compares the new value with its parent node. It replaces parent node with new value if it is larger and continue this comparing process. Otherwise it stops. Repeat the same operation for all \(n\) trees.
* **Query** It takes a threshold \(b\in\mathbb{R}_{\geq 0}\) and an index \(i\in[n]\) as input. Starting from the root of the \(i\)-th tree, it checks if its value is greater than threshold \(b\). If no, search ends. If yes, it treats the child nodes as the root of a new subtree and repeat this searching process until stop. Then it finds all indices \(r\in[m]\) that satisfy \(\{w_{r}:\operatorname{sgn}(\langle w_{r},x_{i}\rangle-b)\geq 0\}\).

For the WTree data structure, it contains \(m\) binary trees indexed by \(m\) weights and supports the following operations:

* **Initialize** Similar to DTree, it takes data points \(\{x_{1},\cdots,x_{n}\}\subset\mathbb{R}^{d}\) and weights \(\{w_{1},\cdots,w_{m}\}\subset\mathbb{R}^{d}\) as input and compute inner products of all weight-data pairs \((w_{r},x_{i})\). It uses these inner products to create \(nm\) different trees. For the \(r\)-th tree based on weight \(w_{i}\), it is constructed from \(n\) leaf nodes \(\langle w_{r},x_{i}\rangle,i\in[n]\) and satisfies the property that the value of parent node is the maximum value of its child nodes.
* **Update** It takes a new weight \(z\in\mathbb{R}^{d}\) and an index \(r\in[m]\) as input. Then it re-constructs the \(r\)-th tree with weight \(z\).
* **Query** It takes a threshold \(b\in\mathbb{R}_{\geq 0}\) and an index \(r\in[m]\) as input. Starting from the root of the \(r\)-th tree, it checks if its value is greater than threshold \(b\). If no, search ends. If yes, it treats the child nodes as the root of a new subtree and repeat this searching process until stop. Then it finds all indices \(i\in[n]\) that satisfy \(\{w_{r}:\operatorname{sgn}(\langle w_{r},x_{i}\rangle-b)\geq 0\}\).

```
1:data structure CorrelationDTree
2:procedes:
3:Init(\(S\subset\mathbb{R}^{d},W\subset\mathbb{R}^{d},n,m,d\))\(\triangleright\) Initialize the data structure via building \(n\) trees
4:Query(\(i,b\))\(\triangleright\)\(i\in[n],b\in\mathbb{R}\). Output the set \(\{r\in[m]:\operatorname{sgn}(\langle w_{r},x_{i}\rangle-b)\geq 0\}\)
5:Update(\(x,i\))\(\triangleright\) Update the \(i\)'th point in \(\mathbb{R}^{d}\) with \(x\)
6:endata structure ```

**Algorithm 5** Correlation DTree Data Structure

We present MakeMaxTree algorithm (Algorithm 7) which shows how to construct a tree satisfying the property that the value of parent node is the max value of its child node.

We then give two training algorithms (Algorithm 8 and Algorithm 9) to show how DTree and WTree help in training neural network efficiently.

## Appendix D Correlation Tree Data Structure

In this section, we demonstrate detailed results for DTree and WTree data structures.

### Correlation DTree data structure

We start by stating the main theorem of correlation DTree data structure.

**Theorem D.1** (Correlation DTree data structure).: _There exists a data structure with the following procedures:_

* \(\textsc{Init}(\{w_{1},w_{2},\cdots,w_{m}\}\subset\mathbb{R}^{d},\{x_{1},x_{2}, \cdots,x_{n}\}\subset\mathbb{R}^{d},n\in\mathbb{N},m\in\mathbb{N},d\in\mathbb{N})\)_. Given a series of weights_ \(w_{1},w_{2},\cdots,w_{m}\) _and datas_ \(x_{1},x_{2},\cdots,x_{n}\) _in d-dimensional space, it preprocesses in time_ \(O(nmd)\)__
* \(\textsc{Update}(z\in\mathbb{R}^{d},r\in[m])\)_. Given a weight_ \(z\) _and index_ \(r\)_, it updates weight_ \(w_{r}\) _with_ \(z\) _in time_ \(O(n\cdot(d+\log m))\)__
* \(\textsc{Query}(i\in[n],\tau\in\mathbb{R})\)_. Given an index_ \(i\) _indicating data point_ \(x_{i}\) _and a threshold_ \(\tau\)_, it finds all index_ \(r\in[m]\) _such that_ \(\langle w_{r},x_{i}\rangle>\tau\) _in time_ \(O(|\widetilde{S}(\tau)|\cdot\log m)\)_, where_ \(\widetilde{S}(\tau):=\{r:\langle w_{r},x_{i}\rangle>\tau\}\)__

### Running time for CorrelationDTree

The goal of this section is to prove the running time of Init, Update and Query.

We start by showing the running time of Init.

**Lemma D.2** (Running time of Init).: _Given a series of weights \(\{w_{1},w_{2},\cdots,w_{m}\}\subset\mathbb{R}^{d}\) and datas \(\{x_{1},x_{2},\cdots,x_{n}\}\subset\mathbb{R}^{d}\), it preprocesses in time \(O(nmd)\)_

Proof.: The Init consists of two independent forloop and two recursive forloops. The first forloop (start from line 8) has \(n\) interations, which takes \(O(n)\) time. The second forloop (start from line 11) has \(m\) iterations, which takes \(O(m)\) time. Now we consider the recursive forloop. The outer loop (line 14) has \(n\) iterations. In inner loop has \(m\) iterations. In each iteration of the inner loop, line 16 takes \(O(d)\) time. Line 18 takes \(O(m)\) time. Putting it all together, the running time of Init is

\[O(n+m+n(md+m))\]\[=O(nmd)\]

Thus, we complete the proof. 

Next, we analyze the running time of Update.

**Lemma D.3** (Running time of Update).: _Given a weight \(z\in\mathbb{R}^{d}\) and index \(j\in[m]\), it updates weight \(w_{j}\) with \(z\) in time \(O(n\cdot(d+\log m))\)_

Proof.: The running time of Update mainly comes from the forloop (line 23), which consists of \(n\) iterations. In each iteration, line 24 takes \(O(\log m)\) time, line 24 takes \(O(d)\) time and the while loop takes \(O(\log m)\) time since it go through a path bottom up. Putting it together, the running time of Update is \(O(n(d+\log m))\). 

Finally, we state the running time for Query procedure.

**Lemma D.4** (Running time of Query).: _Given a query \(q\in\mathbb{R}^{d}\) and a threshold \(\tau>0\), it finds all index \(i\in[n]\) such that \(\langle w_{i},q\rangle>\tau\) in time \(O(|S(\tau)|\cdot\log m)\), where \(S(\tau):=\{i:\langle w_{i},q\rangle>\tau\}\)_

Proof.: The running time comes from Find with input \(\tau\) and \(\operatorname{root}(T_{i})\). In Find, we start from the root node \(r\) and find indices in a recursive way. The Init guarantees that for a node \(r\) satisfying \(\operatorname{r.value}>\tau\), the sub-tree with root \(r\) must contains a leaf whose value is greater than \(\tau\) If not satisfied, all the values of the nodes in the sub-tree with root \(r\) is less than\(\tau\). This guarantees that all the paths it search does not have any branches that leads to the leaf we don't want and it will report all the indices \(i\) satisfying \(\langle w_{i},q\rangle>0\). Note that the depth of \(T\) is \(O(\log n)\), the running time of Query is \(O(|S(\tau)|\cdot\log n)\) 

### Correlation WTree data structure

In this section, we state the main theorem of correlation wtree data structure.

```
1:data structure CorrelationDTree \(\triangleright\) Theorem D.1
2:members
3:\(W\in\mathbb{R}^{m\times d}\) (\(m\) weight vectors )
4:\(X\in\mathbb{R}^{n\times d}\) (\(n\) data points)
5: Binary tree \(T_{1},T_{2},\cdots,T_{n}\)\(\triangleright\)\(n\) binary search trees
6:endmembers
7:
8:public:
9:procedureInit(\(w_{1},w_{2},\cdots,w_{m}\in\mathbb{R}^{d},m,x_{1},x_{2},\cdots,x_{n}\in\mathbb{R}^{d}\), \(n\), \(m\), \(d\)) \(\triangleright\) Lemma D.2
10:for\(i=1\to n\)do
11:\(x_{i}\gets x_{i}\)
12:endfor
13:for\(j=1\to m\)do
14:\(w_{j}\gets w_{j}\)
15:endfor
16:for\(i=1\to n\)do\(\triangleright\) for data point, we create a tree
17:for\(j=1\to m\)do
18:\(u_{j}\leftarrow\langle x_{i},w_{j}\rangle\)
19:endfor
20:\(T_{i}\leftarrow\textsc{MakeTree}(u_{1},\cdots,u_{m})\)\(\triangleright\) Each node stores the maximum value for his two children
21:endfor
22:endprocedure
23:endata structure ```

**Algorithm 10** Correlation DTree data structure

```
1:data structure CorrelationTree \(\triangleright\) Theorem D.1
2:public:
3:procedureUpdate(\(z\in\mathbb{R}^{d},r\in[m]\)) \(\triangleright\) Lemma D.3
4:\(w_{r}\gets z\)
5:for\(i=1\to n\)do
6:\(l\leftarrow\) the \(l\)-th leaf of tree \(T_{i}\)
7:\(l.\)value \(=\langle z,x_{i}\rangle\)
8:while\(l\) is not root do
9:\(p\leftarrow\) parent of \(l\)
10:\(p.\)value \(\leftarrow\max\{p.\)value, \(l.\)value\(\}\)
11:\(l\gets p\)
12:endwhile
13:endfor
14:endprocedure
15:endata structure ```

**Algorithm 11** Correlation DTrees

**Theorem D.5** (Correlation WTree data structure).: _There exists a data structure with the following procedures:_

* \(\textsc{Init}(\{w_{1},w_{2},\cdots,w_{m}\}\subset\mathbb{R}^{d},\{x_{1},x_{2}, \cdots,x_{n}\}\subset\mathbb{R}^{d},n\in\mathbb{N},m\in\mathbb{N},d\in\mathbb{N})\)_. Given a series of weights_ \(w_{1},w_{2},\cdots,w_{m}\) _and datas_ \(x_{1},x_{2},\cdots,x_{n}\) _in d-dimensional space, it preprocesses in time_ \(O(nmd)\)__
* \(\textsc{Update}(z\in\mathbb{R}^{d},r\in[m])\)_. Given a weight_ \(z\) _and index_ \(r\)_, it updates weight_ \(w_{r}\) _with_ \(z\) _in time_ \(O(nd)\)__
* \(\textsc{Query}(r\in[m],\tau\in\mathbb{R})\)_. Given an index_ \(r\) _indicating weight_ \(w_{r}\) _and a threshold_ \(\tau\)_, it finds all index_ \(i\in[n]\) _such that_ \(\langle w_{r},x_{i}\rangle>\tau\) _in time_ \(O(|S(\tau)|\cdot\log m)\)_, where_ \(S(\tau):=\{i:\langle w_{r},x_{i}\rangle>\tau\}\)```
1:data structure CorrelationDTree \(\triangleright\) Theorem D.1
2:public:
3:procedureQuery(\(i\in[n],\tau\in\mathbb{R}_{\geq 0}\)) \(\triangleright\) Lemma D.4
4:returnFind(\(\tau,\operatorname{root}(T_{i})\))
5:endprocedure
6:
7:private:
8:procedureFind(\(\tau\in\mathbb{R}_{\geq 0},r\in T\))
9:if\(r\) is leaf then
10:return\(r\)
11:else
12:\(r_{1}\leftarrow\) left child of \(r\), \(r_{2}\leftarrow\) right child of \(r\)
13:if\(r_{1}.\)value \(\geq\tau\)then
14:\(S_{1}\leftarrow\)Find(\(\tau,r_{1}\))
15:endif
16:if\(r_{2}.\)value \(\geq\tau\)then
17:\(S_{2}\leftarrow\)Find(\(\tau,r_{2}\))
18:endif
19:endif
20:return\(S_{1}\cup S_{2}\)
21:endprocedure
22:end data structure ```

**Algorithm 12** Correlation DITrees

```
1:data structure CorrelationWTree \(\triangleright\) Theorem D.5
2:members
3:\(W\in\mathbb{R}^{m\times d}\) (\(m\) weight vectors )
4:\(X\in\mathbb{R}^{n\times d}\) (\(n\) data points)
5: Binary tree \(T_{1},T_{2},\cdots,T_{M}\)\(\triangleright\)\(m\) binary search trees
6:end members
7:
8:public:
9:procedureInit(\(w_{1},w_{2},\cdots,w_{m}\in\mathbb{R}^{d},m,x_{1},x_{2},\cdots,x_{n}\in \mathbb{R}^{d}\), \(n\), \(m\), d) \(\triangleright\) Lemma D.6
10:for\(i=1\to n\)do
11:\(x_{i}\gets x_{i}\)
12:endfor
13:for\(j=1\to m\)do
14:\(w_{j}\gets w_{j}\)
15:endfor
16:for\(i=1\to m\)do\(\triangleright\) for weight, we create a tree
17:for\(j=1\to n\)do
18:\(u_{j}\leftarrow\langle x_{i},w_{j}\rangle\)
19:endfor
20:\(T_{i}\leftarrow\) MakeTree(\(u_{1},\cdots,u_{n}\)) \(\triangleright\) Each node stores the maximum value for his two children
21:endfor
22:endprocedure
23:end data structure ```

**Algorithm 13** Correlation WTree data structure

### Running time for Correlation WTree

The goal of this secion is to prove the running time of Init, Update and Query.

As in DTree, we first show the running time for Init.

**Lemma D.6** (Running time of Init).: _Given a series of weights \(\{w_{1},w_{2},\cdots,w_{m}\}\subset\mathbb{R}^{d}\) and datas \(\{x_{1},x_{2},\cdots,x_{n}\}\subset\mathbb{R}^{d}\), it preprocesses in time \(O(nmd)\)_Proof.: The Init consists of two independent forloop and two recursive forloops. The first forloop (start from line \(10\)) has \(n\) interations, which takes \(O(n)\) time. The second forloop (start from line \(13\)) has \(m\) iterations, which takes \(O(m)\) time. Now we consider the recursive forloop. The outer loop (line \(16\)) has \(m\) iterations. In inner loop has \(n\) iterations. In each iteration of the inner loop, line \(18\) takes \(O(d)\) time. Line \(20\) takes \(O(n)\) time. Putting it all together, the running time of Init is

\[O(n+m+m(nd+n))\] \[=O(nmd)\]

Thus, we complete the proof. 

Next, we turn to the running time for Update.

**Lemma D.7** (Running time of Update).: _Given a weight \(z\in\mathbb{R}^{d}\) and index \(r\in[m]\), it updates weight \(w_{j}\) with \(z\) in time \(O(nd)\)_

Proof.: In this procedure, it generates a new tree for weight \(w_{r}\) with \(n\) leaves, which takes \(O(nd)\) time. Thus, we complete the proof. 

Finally, we present the running time of Query.

**Lemma D.8** (Running time of Query).: _Given a query \(q\in\mathbb{R}^{d}\) and a threshold \(\tau>0\), it finds all index \(i\in[n]\) such that \(\langle w_{i},q\rangle>\tau\) in time \(O(|S(\tau)|\cdot\log m)\), where \(S(\tau):=\{i:\langle w_{i},q\rangle>\tau\}\)_

Proof.: The running time comes from Find with input \(\tau\) and \(\operatorname{root}(T_{i})\). In Find, we start from the root node \(r\) and find indices in a recursive way. The Init guarantees that for a node \(r\) satisfying \(\operatorname{r.value}>\tau\), the sub-tree with root \(r\) must contains a leaf whose value is greater than \(\tau\) If not satisfied, all the values of the nodes in the sub-tree with root \(r\) is less than\(\tau\). This guarantees that all the paths it search does not have any branches that leads to the leaf we don't want and it will report all the index \(i\) satisfying \(\langle w_{i},q\rangle>0\). Note that the depth of \(T\) is \(O(\log n)\), the running time of Query is \(O(|S(\tau)|\cdot\log n)\) 

## Appendix E More Details of Our Training Algorithms

### Weights Preprocessing

In this section, we present the formal version of our training algorithm using DTree, which preprocessing weights for each data point.

**Theorem E.1** (Running time part, formal version of Theorem 4.1).: _Given \(n\) data points in \(\mathbb{R}^{d}\). Running gradient descent algorithm (Algorithm 8) on \(2\mathrm{NN}(m,b=\sqrt{0.4\log m})\) (Definition 2.1) the expected cost per-iteration of the gradient descent algorithm is_

\[O(m^{4/5}n^{2}d)\]

Proof.: The per-step time complexity is

\[\mathcal{T} =\mathcal{T}_{1}+\mathcal{T}_{2}+\mathcal{T}_{3}\] \[=\ \sum_{i=1}^{n}\mathcal{T}_{\textsc{Query}}(m,d,k_{i,t})+ \mathcal{T}_{\textsc{Update}}\cdot|\cup_{i\in[n]}S_{i,\mathrm{fire}}(t)|+d\sum _{i\in[n]}k_{i,t}\]

The first term \(\mathcal{T}_{1}=\sum_{i=1}^{n}\mathcal{T}_{\textsc{Query}}(m,d,k_{i,t})\) corresponds to the running time of querying the active neuron set \(S_{i,\mathrm{fire}}(t)\) for all training samples \(i\in[n]\). With the first result in Theorem 3.1, the complexity is bounded by \(O(m^{4/5}n\log m)\).

The second term \(\mathcal{T}_{2}=\mathcal{T}_{\textsc{Update}}\cdot|\cup_{i\in[n]}S_{i, \mathrm{fire}}(t)|\) corresponds to updating \(w_{r}\) in the high-dimensional search data-structure (Line 28). Again with the first result in Theorem 3.1, we have \(\mathcal{T}_{\textsc{Update}}=O(n(d+\log m))\). Combining with the fact that \(|\cup_{i\in[n]}S_{i,\mathrm{fire}}(t)|\leq|\cup_{i\in[n]}S_{i,\mathrm{fire}}( 0)|\leq O(m^{4/5}n)\), the second term is bounded by \(O(m^{4/5}n^{2}d)\).

The third term is the time complexity of gradient calculation restricted to the set \(S_{i,\mathrm{fire}}(t)\). With the bound on \(\sum_{i\in[n]}k_{i,t}\) (Lemma B.6), we have \(d\sum_{i\in[n]}k_{i,t}\leq O(m^{4/5}nd)\)

Putting them together, we have

\[\mathcal{T} \leq O(m^{4/5}n\log m)+O(m^{4/5}n^{2}d)+O(m^{4/5}nd)\] \[=O(m^{4/5}n^{2}d)\]

Thus, we complete the proof. 

### Data Preprocessing

In this section, we describe a similar version of training algorithm aforementioned but it uses WTree to preprocess data points based on weights.

**Theorem E.2** (Running time part, formal version of Theorem 4.2).: _Given \(n\) data points in \(\mathbb{R}^{d}\). Running gradient descent algorithm (Algorithm 9) on \(2\mathrm{NN}(m,b=\sqrt{0.4\log m})\), the expected per-iteration running time of initializing \(\widetilde{S}_{r,\mathrm{fire}},S_{i,\mathrm{fire}}\) for \(r\in[m],i\in[n]\) is \(O(m^{4/5}n\cdot\log n)\). The cost per-iteration of the training algorithm is \(O(m^{4/5}n^{2}d)\)._Proof.: We analyze the initialization and training parts separately.

**Initialization** From Line 10 to Line 17, the sets \(\widetilde{S}_{r,\mathrm{fire}},S_{i,\mathrm{fire}}\) for \(r\in[m],i\in[n]\) are initialized. For each \(r\in[m]\), we need to query the data structure the set of data points \(x\)'s such that \(\sigma_{b}(w_{r}(0)^{\top}x)>0\). Hence the running time of this step is

\[\sum_{r=1}^{m}\mathcal{T}_{\text{\sc{query}}}(n,\widetilde{k}_{r,0}) = O(\sum_{r=1}^{m}\widetilde{k}_{r,0}\cdot\log n)\] \[= O(\sum_{i=1}^{n}k_{i,0}\cdot\log n)\] \[= O(m^{4/5}n\cdot\log n)\]

where the second step follows from \(\sum_{r=1}^{m}\widetilde{k}_{r,0}=\sum_{i=1}^{n}k_{i,0}\).

**Training** Consider training the neural networkfor \(T\) steps. For each step, first notice that the forward and backward computation parts (Line 21 - Line 33) are the same as previous algorithm. The time complexity is \(O(m^{4/5}n)\).

We next show that maintaining \(\widetilde{S}_{r,\mathrm{fire}},r\in[m]\) and \(S_{i,\mathrm{fire}},i\in[n]\) (Line 36 - Line 45) takes \(O(m^{4/5}nd)\) time. For each fired neuron \(r\in[m]\), we first remove the indices of data in the sets \(S_{i,\mathrm{fire}}\), which takes time

\[O(1)\cdot\sum_{r\in\cup_{i\in[n]}S_{i,\mathrm{fire}}}\widetilde{k}_{r,t}\;=\;O (1)\cdot\sum_{r=1}^{m}\widetilde{k}_{r,t}=O(m^{4/5}n)\]

Then, we find the new set of \(x\)'s such that \(\sigma_{b}(\langle w_{r}(t+1),x\rangle)>0\) by querying the correlation tree data structure. The total running time for all fired neurons is

\[\sum_{r\in\cup_{i\in[n]}S_{i,\mathrm{fire}}}\mathcal{T}_{\text{ \sc{update}}}(n,d)+\mathcal{T}_{\text{\sc{query}}}(n,\widetilde{k}_{r,t+1}) \lesssim m^{4/5}n^{2}(d+\log m)+\sum_{r\in\cup_{i\in[n]}S_{i, \mathrm{fire}}}\widetilde{k}_{r,t+1}\cdot\log n\] \[=O(m^{4/5}n^{2}d)\]

Then, we update the index sets \(S_{i,\mathrm{fire}}\) in time \(O(m^{4/5}n)\). Therefore, each training step takes \(O(m^{4/5}n^{2}d)\) time, which completes the proof. 

## Appendix F Lower Bound for Dynamic Detection of Firing Neurons

The goal of this section is to prove the lower bound for Dynamic Detection of Firing Neurons.

We start by introducing the strong exponential time hypothesis, \(\mathsf{SETH}\) in abbreviation.

**Definition F.1** (Strong exponential time hypothesis, \(\mathsf{SETH}\), [14, 14]).: _For every \(\epsilon>0\), there exists a \(k=k(\epsilon)\in\mathbb{N}\) such that no algorithm can solve \(k\)-SAT (i.e., satisfiability on a CNF of width \(k\)) in \(O(2^{(1-\epsilon)n})\) time where \(n\) is the number of variables._

We present another relative concept called orthogonal vector conjecture, \(\mathsf{OVC}\) in abbreviation.

**Definition F.2** (Orthogonal vector conjecture, \(\mathsf{OVC}\), [13, 1, 1]).: _For every \(\epsilon>0\), there exists a \(c\geq 1\) such that the orthogonal vector problem of size \(n\) in \(d\)-dimension requires \(n^{2-\epsilon}\)-time when \(d=c\log n\)._

We refer to a theorem about maximum bichromatic inner product lower bound in [1].

**Theorem F.3** (Maximum bichromatic inner product lower bound, [1]).: _Assuming \(\mathsf{SETH}\) (Definition F.1) or \(\mathsf{OVC}\) (Definition F.2), there is a constant \(c\) such that any exact algorithm for \(\mathbb{Z}\)-\(\mathsf{Max}\)-\(\mathsf{IP}_{n,d}\) in dimension \(d=c^{\log^{*}n}\) requires \(n^{2-o(1)}\) time, with vectors of \(O(\log n)\)-bit entries._

Putting things together, we state the main result for the lower bound for Dynamic Detection of Firing Neurons.

**Theorem F.4** (Lower Bound for Dynamic Detection of Firing Neurons, Formal version of Theorem 1.4).: _Let \(d=2^{O(\log^{*}n)}\). Unless_ OVC _or_ SETH _fails, for any constants \(c\in(0,1)\), no data structure can solve DDFN with less than \(m^{1-c}n^{c-o(1)}\)-time per update and \(m^{1-c}n^{1+c-o(1)}\)-time per query._

Proof.: Without loss of generality, we assume that \(m>n\). Let \(d=c^{\log^{*}m}\), where \(c\) is defined in Theorem F.3.

Suppose there exists a data structure that for \((m,n,d+1)\)-sized instance, can perform updates in \(m^{1-c}n^{c-\epsilon}\)-time and answer queries in \(m^{1-c}n^{1+c-\epsilon}\)-time, for some \(c\in(0,1)\) and \(\epsilon\in(0,c)\).

Let \(X=\{x_{1},\ldots,x_{m}\}\subset\mathbb{Z}^{d}\), \(Y=\{y_{1},\ldots,y_{m}\}\subset\mathbb{Z}^{d}\) be a hard instance of \(\mathbb{Z}\)-Max-IP\({}_{m,d}\) problem constructed in Theorem F.3. For each vector \(x_{i}\) (or \(y_{j}\)), we construct a new vector \(\widetilde{x}_{i}\) (or \(\widetilde{y}_{j}\)) in \((d+1)\)-dimension such that \((\widetilde{x}_{i})_{d+1}=-1\) and \((\widetilde{y}_{j})_{d+1}=w\), where \(w\) is a parameter to be chosen later.

Then, we construct \(k=\lceil m/\rceil\) instances of the DDFN problem in Definition 1.3 as follows:

\[\widetilde{X}^{(i)}:=\{\widetilde{x}_{1},\ldots,\widetilde{x}_{n}\},\ \ \widetilde{Y}^{(i)}:=\{\widetilde{y}_{1},\ldots,\widetilde{y}_{m}\},\]

and \(b=0\).

We show that the data structures for these instances \(\{(\widetilde{X}^{(i)},\widetilde{Y}^{(i)},b)\}_{i\in[k]}\) can be used to solve \(\mathbb{Z}\)-Max-IP\({}_{n,d}(X,Y)\).

We perform a binary search for the value of \(\mathbb{Z}\)-Max-IP\({}_{n,d}(X,Y)\). Note that at most \(O(\log n)\) iterations suffice to find the exact answer.

Suppose the current value in the binary search is \(t\in\mathbb{Z}\). Consider the \(i\)-th instance \((\widetilde{X}^{(i)},\widetilde{Y}^{(i)},b)\) for any \(i\in[k]\). We first call Update() to set \((\widetilde{y}_{j})_{d+1}=t\) for each \(j\in[m]\). By the data structure's guarantee, this step takes \(O(m\cdot m^{1-c}n^{c-\epsilon})=O(m^{2-c}n^{c-\epsilon})\) time. Then, we call Query(). Notice that

\[\langle\widetilde{x}_{i},\widetilde{y}_{j}\rangle=\langle x_{i},y_{j}\rangle- t\geq 0\iff\langle x_{i},y_{j}\rangle\geq t.\]

Hence, Query() will return all pairs of \((i,j)\) such that \(\langle x_{i},y_{j}\rangle\geq t\). This step runs in \(O(m^{1-c}n^{1+c-\epsilon})\)-time. We repeat this process for all \(k\) instances. And based on whether the outputs of all the Query() are empty or not, we know the direction of the binary search for the next iteration.

Hence, the total running time of each iteration is

\[O(k\cdot(m^{2-c}n^{c-\epsilon}+m^{1-c}n^{1+c-\epsilon}))\] \[=O(m^{3-c}n^{c-\epsilon-1}+m^{2-c}n^{c-\epsilon})\] \[\leq O(m^{2-\epsilon}),\]

which follows from the assumption of \(m\geq n\). Thus, we can solve \(\mathbb{Z}\)-Max-IP\({}_{n,d}(X,Y)\) in \(\widetilde{O}(m^{2-\epsilon})<m^{2-o(1)}\)-time, which contradicts to the lower bound for \(\mathbb{Z}\)-Max-IP\({}_{n,d}\) in Theorem F.3. Therefore, no such data structure can exist.