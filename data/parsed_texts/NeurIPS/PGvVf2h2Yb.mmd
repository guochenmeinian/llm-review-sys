# Adaptive Transductive Inference via Sequential Experimental Design with Contextual Retention

Tareq Si Salem

Huawei Technologies, Paris Research Center, France

tareq.si.salem@huawei.com

###### Abstract

This paper presents a three-stage framework for active learning, encompassing data collection, model retraining, and deployment phases. The framework's primary objective is to optimize data acquisition, data freshness, and model selection methodologies. To achieve this, we propose an online policy with performance guarantees, ensuring optimal performance in dynamic environments. Our approach integrates principles of sequential optimal experimental design and online learning. Empirical evaluations validate the efficacy of our proposed method in comparison to existing baselines.

## 1 Introduction

The development of Machine Learning (ML) models is intrinsically reliant on the availability of data. Data plays a critical role in various stages of the ML model lifecycle, including parameter optimization, evaluation of inferential capabilities, and potential refinements to the model architecture. However, the acquisition of suitable data frequently constitutes a significant bottleneck in the training pipeline. The process of obtaining relevant data or measurements can be both costly and time-intensive and is often subject to resource constraints. These constraints may manifest in diverse forms, including restrictions on sample size, temporal constraints, or computational limitations. The challenge is further compounded when labels for these datasets are unavailable and are costly to acquire (e.g., clinical trials [42], drug discovery [5]). In such scenarios, strategic decisions regarding data collection or experimental design become essential. The selection of the most informative samples for a given task is known as _optimal experimental design_ (OED) [40, 10] within the field of statistics. The primary objective in OED is to maximize information gain about an unknown model within the confines of a limited budget. OED has long been an essential part of statistical modeling, from the design of clinical trials [42, 14, 46], medical imaging [39], materials science [19], biological process models [41, 18], networked systems [32], bandits [17, 31], and regression problems in general [16, 27, 49, 21]. For a comprehensive overview of OED methodologies and applications, the reader is directed to the following surveys [47, 40, 25].

In many real-world applications of ML, the performance of a model remain unknown until it is deployed and interacts with its operational environment. This inherent uncertainty necessitates an iterative approach to experimental design, where subsequent experiments are informed by the outcomes of previous ones. This adaptive methodology, known as sequential optimal experimental design (SOED), aims to effectively mitigate uncertainty by dynamically adjusting the experimental design based on accumulating knowledge. SOED presents a unique challenge in that the optimal design at any given stage depends on the anticipated sequence of future predictions. A common suboptimal approach is to employ a greedy strategy, utilizing historical data to determine the seemingly best experiment at each step without explicitly considering the long-term consequences of this decision [22, 30, 15, 9]. This myopic approach fails to account for the potential impact ofearly design choices on the overall optimization process. Recently, online learning has emerged as a promising framework for addressing SOED by incorporating feedback mechanisms into the design process [24; 26; 45]. By learning from past experiences and adapting its strategy accordingly, online learning offers a more sophisticated approach to sequential decision-making in experimental design. Furthermore, the deployment of machine learning models often encounters the challenge of concept drift, which refers to the dynamic nature of the relationship between input features and the target variable [20; 34; 43; 50; 36]. Significant disruptions, such as the COVID-19 pandemic, can precipitate substantial alterations in traffic patterns. These alterations subsequently introduce significant concept drift in traffic forecasting methodologies, thereby challenging the efficacy of predictive models [33; 35]. Consequently, this necessitates the development of training strategies that effectively tune data freshness and recency to maintain predictive accuracy. Traditional approaches, predominantly focused on variance minimization through experimental design, are insufficient in addressing the bias introduced by concept drift. Consequently, a crucial question arises:

_How can we effectively optimize data acquisition, data freshness, and model selection methodologies in dynamic environments characterized by concept drifts?_

To address this challenge, this work introduces a novel active learning framework. This framework operates in three distinct stages: data collection, model retraining, and deployment. In the initial data collection phase, a policy guides the selection of informative experiments from a predefined pool of potential candidates. This selection process aims to maximize the value of acquired labels, thereby enhancing learning efficiency. The policy can request labels for multiple experiments concurrently, subject to a constraint on the number of simultaneous queries. These labeled datasets are then stored in a local repository with a fixed capacity, utilizing an eviction strategy to manage storage limitations. Periodically, the accumulated labeled data is used to retrain a machine learning model. This model, updated with an appropriate selection of fresh dataset from the local repository, is then deployed to predict labels for new, incoming queries. Unlike traditional inductive learning approaches that focus solely on the initial pool of experiments, this framework adopts a transductive learning perspective [11; 48; 6; 51; 12]. This means that the policy's experimental design choices are optimized not only with respect to the initial pool but also in relation to the sequence of revealed queries. The policy receives feedback on its predictions in the form of noisy prediction errors, allowing it to adapt and refine its strategy over time. The ultimate goal of the policy is to minimize its regret, which quantifies the performance difference between the policy's predictions and those of an optimal policy possessing complete information about the underlying data generating process. This minimization of regret ensures that the active learning framework efficiently learns and adapts to the underlying phenomenon, even in the presence of noise and uncertainty.

The remainder of this paper is organized as follows. Section 2 presents the problem formulation. A theoretical analysis of the problem is conducted in Section 3. Finally, the effectiveness of the proposed approach is numerically demonstrated in Section 4.

## 2 Problem Formulation

### System Model

The overall system model is illustrated in Figure 1. A list of the notation employed throughout this paper can be found in the Appendix.

**Data Collection.** The policy has access to a pool of experiments \(\mathcal{X}\subset\mathbb{R}^{d}\) to collect labels from a variety of experimental sources, such as sensors, surveys, and databases. A data retention policy is implemented, periodically purging datasets that exceed a predetermined age threshold \(\tau\in\mathbb{N}\). This practice adheres to data privacy regulations (e.g., GDPR [13], CCPA/CPRA[1; 38]). At each time slot \(t\), the policy is allocated a fixed experimental budget of \(M\in\mathbb{N}\) experiments. The set of all feasible experimental designs is defined as

\[\left.\left.\mathbb{\cup_{\mathcal{X}}}\triangleq\left\{\boldsymbol{\pi}\in[0,1]^{\mathcal{X}}:\left\|\boldsymbol{\pi}\right\|_{1}=1\right\}.\right.\]

For a given continuous design \(\boldsymbol{\pi}\in\left\{\mathbb{\cup_{\mathcal{X}}}\right\}\), the learner allocates \(M\pi_{\boldsymbol{x}}\in[0,M]\) experiments to type \(\boldsymbol{x}\in\mathcal{X}\). At time \(t\), the system acquires a dataset \(\mathcal{D}_{t}\) of \(M\) experiment-labels pairs in \(\mathcal{X}\times\mathbb{R}\), following a design \(\boldsymbol{\pi}_{t}\in\left\{\mathbb{\cup_{\mathcal{X}}}\right\}\). A rolling window of \(\tau+1\) datasets is maintained, discarding older ones. The labels \(y\) are related to the experiment \(\boldsymbol{x}\) according to a noisy linear model \(y=\boldsymbol{x}\cdot\boldsymbol{\beta}_{t}^{*}+n\), where \(n\sim\mathcal{N}(0,\sigma^{2})\) is a Gaussian noise and \(\boldsymbol{\beta}_{t}^{*}\) is the _time-varying_ true model.

**Model Retraining.** To address the challenge of concept drift during the model updating phase, we introduce a data-freshness parameter, denoted as \(\tau_{t}\in\mathcal{T}\triangleq\{0,1,\ldots,\tau\}\). This parameter dictates the recency of the data used for model training. Specifically, at each time slot \(t\), we employ a least-squares estimator (LSE), denoted as \(\hat{\bm{\beta}}_{t}\), to generate future predictions. The LSE model is trained exclusively on the \((\tau_{t}+1)\) most recent data points available at time \(t\). The model is given by \(\hat{\bm{\beta}}_{t}=\frac{1}{M}\left(\bm{X}^{\intercal}\mathrm{diag}\left( \bm{\pi}_{t-\tau_{t}:t}\right)\bm{X}\right)^{-1}\bm{X}^{\intercal}\bm{y}\left( \mathcal{D}_{t-\tau_{t}:t}\right)\), where \(\bm{X}=\left(\bm{x}^{\intercal}\right)_{\bm{x}\in\mathcal{X}}\in\mathbb{R}^{ \mathcal{X}\times d}\) is the experiments' matrix, and \(\bm{y}(\mathcal{D})=\left(\sum_{(\bm{x},y_{i})\in\mathcal{D}_{\bm{x}}}y_{i} \right)_{\bm{x}\in\mathcal{X}}\in\mathbb{R}^{\mathcal{X}}\) is the labels' vector.

**Model Deployment.** During deployment, the trained model \(\hat{\bm{\beta}}_{t}\) is used to predict labels for experiments \(\bm{x}_{t}\in\mathcal{Z}\subset\mathbb{R}^{d}\). User feedback in the form of prediction errors is collected to refine the model. Specifically, the squared error \(\xi_{t}=(y_{t}-\hat{y}_{t})^{2}\) is provided, where \(\hat{y}_{t}=\bm{x}_{t}\cdot\hat{\bm{\beta}}_{t}\) is the predicted label and \(y_{t}=\bm{x}_{t}\cdot\bm{\beta}_{t}^{\star}+n\) is the label with noise \(n\sim\mathcal{N}(0,\sigma^{2})\). This feedback signal informs the learner about the model's performance and guides future data collection to improve accuracy. Both the feedback signal \(\xi_{t}\) and the corresponding query \(\bm{x}_{t}\) are used to guide future model selection.

### Policies and Performance Metric

In this section, we provide a formal description of the policy that governs the collection of data through experimental designs and the freshness of data used to retrain the most recent model.

**Prediction Error and Bias-Variance Tradeoff.** The accuracy of the model \(\hat{\bm{\beta}}_{t}\) for a query point \(\bm{x}_{t}\in\mathcal{Z}\) with noisy label \(y_{t}=\bm{x}_{t}\cdot\bm{\beta}_{t}^{\star}+n\sim\mathcal{N}(\bm{x}_{t}\cdot \bm{\beta}_{t}^{\star},\sigma^{2})\) is measured by its expected prediction error (EPE). This EPE depends on the experimental designs \((\bm{\pi}_{t-\tau},\ldots,\bm{\pi}_{t})\) and a data-freshness parameter \(\tau_{t}\in\mathcal{T}\) controlling the influence of past data. The EPE is defined as follows: \(f_{t}(\bm{\pi}_{t-\tau_{t}},\ldots,\bm{\pi}_{t})\triangleq\mathbb{E}[\xi_{t}] =\mathbb{E}[\left(y_{t}-\hat{y}_{t}\right)^{2}]\). In the following Proposition, we clearly delineate the contributions of experimental design selection and data-freshness selection, by decomposing the EPE on query \(\bm{x}_{t}\) into its variance and bias components.

**Proposition 1**.: _Under designs \(\left\{\bm{\pi}_{s}\right\}_{s=t-\tau}^{t}\in\mathop{\cup}_{\mathcal{X}}^{ \tau+1}\), the EPE of the LSE on experiment \(\bm{x}_{t}\in\mathcal{Z}\) at time \(t\) under data-freshness window size \(\tau+1\) is_

\[f_{t}(\bm{\pi}_{t-\tau},\ldots,\bm{\pi}_{t})=\sigma^{2}+\bm{x}_{t}^{\intercal} \mathrm{cov}(\hat{\bm{\beta}}_{t})\bm{x}_{t}+\left(\bm{x}_{t}\cdot\left( \mathbb{E}\left[\hat{\bm{\beta}}_{t}\right]-\bm{\beta}_{t}^{\star}\right) \right)^{2},\] (1)

_where \(\mathbb{E}\left[\hat{\bm{\beta}}_{t}\right]=\bm{V}^{-1}(\bm{\pi}_{t-\tau_{t}:t })\left(\sum_{\bm{x}\in\mathcal{X}}\bm{x}\bm{x}^{\intercal}\sum_{s=t-\tau}^{t} \pi_{s,\bm{x}}\bm{\beta}_{s}^{\star}\right)\) and \(\mathrm{cov}(\hat{\bm{\beta}}_{t})=\frac{\sigma^{2}}{M}\bm{V}^{-1}(\bm{\pi}_{t- \tau_{t}:t})\)._

The proof is provided in Appendix B.1. The bias-variance trade-off is evident in the decomposition. The expected prediction error is divided into three components: (a) irreducible variance due to noise in the labels, (b) variance related to data-freshness and experimental designs, and (c) bias reflecting model drift. In the absence of significant drift, a larger data-freshness window is beneficial. However, under significant drift, a smaller window is preferable, though increasing variance. Minimizing variance through careful experimental design is always advantageous.

**Online Policies.** The role of a policy is to select appropriately experimental designs \(\bm{\pi}_{t}\in\mathop{\cup}_{\mathcal{X}}\) and data-freshness parameter \(\tau_{t}\in\mathcal{T}\) at every timeslot \(t\), and adapts its decisions upon seeing the query \(\bm{x}_{t}\) and feedback \(\xi_{t}\). Formally, at timeslot \(t\), the system adapts its state according to a randomized policy \(\mathcal{P}_{t}:\left(\mathop{\cup}_{\mathcal{X}}\times\mathcal{T}\times \mathcal{Z}\times\mathbb{R}\right)^{t}\rightarrow\mathop{\cup}_{\mathcal{X}} \times\mathcal{T}\), defined as \((\bm{\pi}_{t+1},\tau_{t+1})=\mathcal{P}_{t}\left(\left\{\bm{\pi}_{s},\tau_{s}, \bm{x}_{s},\xi_{t}\right\}_{s=1}^{t}\right)\).

**Performance Metric.** We compare the performance of the sequence of designs and data-freshness parameters w.r.t. the best design in hindsight and data-freshness window size after seeing all the queries in terms of the EPE. Formally,

\[\mathfrak{R}_{T}\left(\bm{\mathcal{P}}\right)\triangleq\mathbb{E}\left[\sum_{t= \tau+1}^{T}f_{t}(\bm{\pi}_{t-\tau_{t}},\ldots,\bm{\pi}_{t})-\sum_{t=\tau+1}^{T} f_{t}(\bm{\pi}_{t-\tau_{t}^{\star}}^{\star},\ldots,\bm{\pi}_{t}^{\star})\right],\] (2)

Figure 1: System Modelwhere \(\bm{\pi}_{t}^{*}\) and \(\tau_{t}^{*}\) for \(t\in[T]\) are the minimizers of the aggregate expected prediction error, and the expectation is taken with respect to the randomness in the environment and the policy \(\bm{\mathcal{P}}\).

If the regret \(\mathfrak{R}_{T}(\bm{\mathcal{P}})\) is sublinear, then the policy asymptotically achieves on average the performance of a policy that selects the optimal experimental designs and data-freshness parameters in hindsight.

## 3 Theoretical Analysis

Our theoretical analysis reveals that the optimization of experimental design and data-freshness can be efficiently decoupled into two interrelated subproblems. We propose a policy utilizing Online Mirror Descent (OMD) [8; 44; 23] to simultaneously address these subproblems.

### Decoupling of Experimental Design and Data-freshness Decisions

We propose a decoupled approach to experimental design and data-freshness parameter selection. Firstly, we employ a variance reduction policy within the full-information online learning framework [23; 37] to determine the optimal experimental design. Secondly, we formulate a multi-armed bandit problem [31] to select the data-freshness parameters, thereby mitigating bias.

**Variance Reduction Policy.** The variance reduction policy selects a new design at time \(t\) according to a mapping \(\mathcal{P}_{t}^{v}\) that maps the past experimental designs \(\{\bm{\pi}_{s}\}_{s=1}^{t}\in\otimes_{\mathcal{X}}^{t}\), and past experiment queries \(\{\bm{x}_{s}\}_{s=1}^{t}\in\mathcal{Z}^{t}\) to a new design \(\bm{\pi}_{t+1}\) given by \(\bm{\pi}_{t+1}=\mathcal{P}_{t}^{v}(\{\bm{\pi}_{s},\bm{x}_{s}\}_{s=1}^{t})\). The policy incurs costs in the form of the \(\bm{x}_{t}\)-optimal design objective in Definition 2, and has the following regret: \(\mathfrak{R}_{T}^{v}\left(\bm{\mathcal{P}}^{v}\right)\triangleq\sum_{t=1}^{T }v_{t}(\bm{\pi}_{t})-\min_{\{\bm{\pi}_{t}^{*}\}_{t=1}^{T}\in\otimes_{\mathcal{ X}}^{T}}\sum_{t=1}^{T}v_{t}(\bm{\pi}_{t}^{*})\). Note that the cost \(v_{t}\) are fully determined once \(\bm{x}_{t}\) is made available.

**Bias Reduction Policy.** The bias reduction policy operates on top of the variance reduction policy. In particular, at timeslot \(t\), the data-freshness parameter \(\tau_{t+1}\) is the output of the mapping \(\mathcal{P}_{t}^{b}\) that maps the past data-freshness parameters \(\{\tau_{s}\}_{s=1}^{t}\in\mathcal{T}^{t}\) and prediction error feedback \(\{\xi_{s}\}_{s=1}^{t}\in\mathbb{R}^{t}\) according to the mapping \(\tau_{t+1}=\mathcal{P}_{t}^{b}(\{\tau_{s},\xi_{s}\}_{s=1}^{t})\). Note that the coupling between the variance reduction policy and bias reduction policy is implicitly encoded in the prediction error \(\xi_{t}\) as this error depends on both decisions. The policy incurs the prediction errors, and has the following regret: \(\mathfrak{R}_{T}^{b}(\bm{\mathcal{P}}^{b})\triangleq\mathbb{E}\left[\sum_{t= \tau+1}^{T}\xi_{t}-\min_{\{\tau_{t}^{*}\}_{t=1}^{T}\in\mathcal{T}^{T}}\sum_{t= \tau_{t}^{*}+1}^{T}f_{t}(\bm{\pi}_{t-\tau_{t}^{*}},\ldots,\bm{\pi}_{t})\right]\). The setup of bias reduction policy corresponds to a non-stationary setup of the multi-armed bandit problem where \(\mathcal{T}\) is the set of the arms [31; 7].

**VBR Policy.** The variance and bias reduction (VBR) policy denoted by \(\bm{\mathcal{P}}^{v+b}\) is the policy that determines experimental designs \(\bm{\pi}_{t}\) according to the variance reduction policy \(\bm{\mathcal{P}}^{v}\) and the data-freshness parameter \(\tau_{t}\) according to the bias reduction policy \(\bm{\mathcal{P}}^{b}\) for any \(t\in[T]\). Formally, at timeslot \(t\), the policy is given by the mapping \(\mathcal{P}_{t}^{v+b}:(\ll_{\mathcal{X}}\times\mathcal{T}\times\mathcal{Z} \times\mathbb{R})^{t}\rightarrow\ll_{\mathcal{X}}\times\mathcal{T}\) given by \(\mathcal{P}_{t}^{v+b}\triangleq(\mathcal{P}_{t}^{v},\mathcal{P}_{t}^{b})\). The variance and bias reduction policy enjoys the following regret guarantee:

**Theorem 1**.: _Under Assumptions 1-3, let \(\{\bm{x}_{t}\}_{t=1}^{T}\in\mathcal{Z}^{T}\) be the sequence of queries, \(\{\bm{\pi}_{t}^{*}\}_{t=1}^{T}\in\otimes_{\mathcal{X}}^{T}\) be the sequence of optimal experimental designs and \(\{\tau_{t}^{*}\}_{t=1}^{T}\in\mathcal{T}^{T}\) is the sequence of data-freshness windows. The regret (2) of the variance and bias reduction policy \(\bm{\mathcal{P}}^{v+b}\) satisfies:_

\[\mathfrak{R}_{T}(\bm{\mathcal{P}}^{v+b})=\mathfrak{R}_{T}^{b}(\bm{\mathcal{P}} ^{b})+\mathfrak{R}_{T}^{v}(\bm{\mathcal{P}}^{v})+\mathcal{O}\left(P_{T}^{v}+P_{ T}^{*,v}\right),\] (3)

_where \(P_{T}^{*,v}=\sum_{t=1}^{T}\left\lVert\bm{\pi}_{t}^{*}-\bm{\pi}_{t+1}^{*}\right\rVert_{1}\) and \(P_{T}^{v}=\sum_{t=1}^{T}\left\lVert\bm{\pi}_{t}-\bm{\pi}_{t+1}\right\rVert_{1}\) are the path-lengths of the OEDs and variance reduction policy._

The proof is provided in Appendix E. In the next section, we provide a specific instantiations of the variance reduction and bias reduction policies.

### Entropic-VBR Policy

We introduce the Entropic-VBR policy (Appendix C.4), which achieves sublinear regret. Our approach provides a unified treatment of both full-information and bandit settings. To overcome the challenges of a non-stationary environment and bias reduction in the bandit setting, we meticulously instantiate the OMD framework. This involves carefully constructing gradient estimates and selecting an appropriate mirror map to ensure simultaneous regret guarantees. Leveraging the results of Corollary 2, Theorem 4, and Theorem 1, we establish a comprehensive regret guarantee for the Entropic-VBR policy. Formally,

**Corollary 1**.: _Under Assumptions 1-3, let \(\left\{\bm{x}_{t}\right\}_{t=1}^{T}\in\mathcal{Z}^{T}\) be the sequence of queries, \(\left\{\bm{\pi}_{t}^{\star}\right\}_{t=1}^{T}\in\otimes_{\mathcal{X},\sigma}^{ T}\) be the sequence of optimal experimental designs and \(\left\{\bm{p}_{t}^{\star}\right\}_{t=1}^{T}\in\otimes_{\mathcal{T}}^{\mathcal{ T}}\) is the sequence of comparator data-freshness windows, with path lengths \(P_{T}^{\star,v}\) and \(P_{T}^{\star,b}\), respectively. The Entropic-VBR Policy (Appendix C.4) configured with learning rates \(\eta_{\mathcal{X}}=\Theta\left(\log(1/\sigma)P_{T}^{\star,v}T^{-1}\right)\) and \(\eta_{\mathcal{T}}=\Theta\left(\log(1/\sigma^{\prime})P_{T}^{\star,b}T^{-1}\right)\) and \(\sigma^{\prime}=\Theta\left(T^{-1}\right)\) achieves the following regret:_

\[\mathfrak{R}_{T}(\bm{\mathcal{P}}^{\mathrm{Entropic-VBR}})=\mathcal{O}\left( \sqrt{\log(1/\sigma)P_{T}^{\star,v}T}+\sqrt{\log(T)P_{T}^{\star,b}T}+P_{T}^{ \star,v}\right).\] (4)

The sequences in \(\left\vartriangle_{\mathcal{X},\sigma}\) are \((1+\sigma)\)-competitive w.r.t. sequences in \(\left\vartriangle_{\mathcal{X}}\) (Prop. 2 in the Appendix).

## 4 Numerical Experiments

**Experimental Setup.** To evaluate the performance of our proposed methodology, we constructed a synthetic experimental setting, as illustrated in Figure 2 (a). Full description is in Appendix F.

**Discussion.** Our evaluation in Figure 2 (b) shows that a uniform experimental design with the maximum freshness window is the least effective baseline (indexed as \(\beta\)). Optimizing the data-freshness window improves performance (indexed as \(\gamma\)), and further gains are achieved by optimizing the experimental design (indexed as \(\delta\)). Our proposed policy (indexed as \(\alpha\)) outperforms the baseline, demonstrating its ability to identify optimal sequences of designs and freshness windows. Figure 2 (c) represents how the policy adapts to evolving query distributions. The temporal evolution of the learned distribution over window sizes in Figure 3 (Appendix) reveals that the optimal window size under these conditions is not immediately apparent.

## 5 Conclusion

This work introduced a novel framework that explicitly accounts for the evolving relationship between data freshness and model performance, encompassing data collection, data freshness decisions, and model retraining within a limited-capacity cache. A rigorous theoretical analysis revealed the inherent variance-bias trade-off and motivated a decoupled approach to address this challenge. This approach involved leveraging OCO for variance reduction in experimental design and formulating a non-stationary MAB problem for bias mitigation through data freshness parameter selection.

As avenues for future work, extending the inference model to encompass non-linear relationships is of considerable interest. Reproducing kernel methods [3; 4] present a promising initial direction for leveraging the proposed framework, as they permit analogous derivations. Additionally, exploring more general noise models beyond the Gaussian noise considered herein would enhance the framework's applicability.

Figure 2: Subfig. (a): Query distributions, and true model drift (\(10^{3}\) initial iterations). Subfig. (b): Time-ayged prediction error for three intervals: \(0\leq t\leq\frac{1}{3}T\), \(\frac{1}{3}T<t\leq\frac{2}{3}T\), and \(\frac{2}{3}T<t\leq T\). Subfig. (c): Initial and selected designs at \(t=\frac{1}{3}T\), \(t=\frac{2}{3}T\), and \(t=T\).

## References

* [1] California Consumer Privacy Act. Assembly Bill No. 375, Chapter 55, December 2018.
* [2] Zeyuan Allen-Zhu, Yuanzhi Li, Aarti Singh, and Yining Wang. Near-optimal discrete optimization for experimental design: A regret minimization approach. _Mathematical Programming_, 186:439-478, 2021.
* [3] Mauricio A Alvarez, Lorenzo Rosasco, Neil D Lawrence, et al. Kernels for vector-valued functions: A review. _Foundations and Trends(r) in Machine Learning_, 4(3):195-266, 2012.
* [4] Nachman Aronszajn. Theory of reproducing kernels. _Transactions of the American mathematical society_, 68(3):337-404, 1950.
* [5] Julio R Banga and Eva Balsa-Canto. Parameter estimation and optimal experimental design. _Essays in biochemistry_, 45:195-210, 2008.
* [6] M Belin, P Niyogi, and V Sindhwani. Manifold regularization: a geometric framework for learning from examples. _Journal of Machine Learning Research_, 7:2399-2434, 2006.
* [7] Sebastien Bubeck, Nicolo Cesa-Bianchi, et al. Regret analysis of stochastic and nonstochastic multi-armed bandit problems. _Foundations and Trends(r) in Machine Learning_, 5(1):1-122, 2012.
* [8] Sebastien Bubeck et al. Convex optimization: Algorithms and complexity. _Foundations and Trends(r) in Machine Learning_, 8(3-4):231-357, 2015.
* [9] Daniel R Cavagnaro, Jay I Myung, Mark A Pitt, and Janne V Kujala. Adaptive design optimization: A mutual information-based approach to model discrimination in cognitive science. _Neural computation_, 22(4):887-905, 2010.
* [10] Kathryn Chaloner and Isabella Verdinelli. Bayesian Experimental Design: A Review. _Statistical science_, pages 273-304, 1995.
* [11] Olivier Chapelle, Vladimir Vapnik, and Jason Weston. Transductive inference for estimating values of functions. _Advances in Neural Information Processing Systems_, 12, 1999.
* [12] Corinna Cortes and Mehryar Mohri. On transductive regression. _Advances in neural information processing systems_, 19, 2006.
* [13] Council of European Union. Regulation (eu) 2016/679 of the european parliament and of the council of 27 april 2016, on the protection of natural persons with regard to the processing of personal data and on the free movement of such data, and repealing directive 95/46/ec (general data protection regulation), 2016.
* [14] Meichun Ding, Gary L Rosner, and Peter Muller. Bayesian optimal design for phase ii screening trials. _Biometrics_, 64(3):886-894, 2008.
* [15] Hovav A Dror and David M Steinberg. Sequential experimental designs for generalized linear models. _Journal of the American Statistical Association_, 103(481):288-298, 2008.
* [16] Gustav Elfving. Optimum allocation in linear regression theory. _The Annals of Mathematical Statistics_, pages 255-262, 1952.
* [17] Tanner Fiez, Lalit Jain, Kevin G Jamieson, and Lillian Ratliff. Sequential experimental design for transductive linear bandits. _Advances in neural information processing systems_, 32, 2019.
* [18] Patrick Flaherty, Adam Arkin, and Michael Jordan. Robust design of biological experiments. _Advances in neural information processing systems_, 18, 2005.
* [19] Peter I Frazier and Jialei Wang. Bayesian optimization for materials design. _Information science for materials discovery and design_, pages 45-75, 2016.
* [20] Joao Gama, Indre Zliobaite, Albert Bifet, Mykola Pechenizkiy, and Abdelhamid Bouchachia. A survey on concept drift adaptation. _ACM computing surveys (CSUR)_, 46(4):1-37, 2014.

* [21] Nicolas Gast, Stratis Ioannidis, Patrick Loiseau, and Benjamin Roussillon. Linear regression from strategic data sources. _ACM Transactions on Economics and Computation (TEAC)_, 8(2):1-24, 2020.
* [22] Markus Hainy, Christopher C Drovandi, and James M McGree. Likelihood-free extensions for bayesian sequentially designed experiments. In _mODa 11-Advances in Model-Oriented Design and Analysis: Proceedings of the 11th International Workshop in Model-Oriented Design and Analysis held in Hamminkeln, Germany, June 12-17, 2016_, pages 153-161. Springer, 2016.
* [23] Elad Hazan et al. Introduction to online convex optimization. _Foundations and Trends(r) in Optimization_, 2(3-4):157-325, 2016.
* [24] Xun Huan. _Numerical approaches for sequential Bayesian optimal experimental design_. PhD thesis, Massachusetts Institute of Technology, 2015.
* [25] Xun Huan, Jayanth Jagalur, and Youssef Marzouk. Optimal experimental design: Formulations and computations. _Acta Numerica_, 33:715-840, 2024.
* [26] Xun Huan and Youssef M Marzouk. Sequential bayesian optimal experimental design via approximate dynamic programming. _arXiv preprint arXiv:1604.08320_, 2016.
* [27] Jack Kiefer and Jacob Wolfowitz. Optimum designs in regression problems. _The annals of mathematical statistics_, 30(2):271-294, 1959.
* [28] Jack Kiefer and Jacob Wolfowitz. The equivalence of two extremum problems. _Canadian Journal of Mathematics_, 12:363-366, 1960.
* [29] Krzysztof C. Kiwiel. Proximal Minimization Methods with Generalized Bregman Functions. _SIAM Journal on Control and Optimization_, 1997.
* [30] Steven Kleinegesse and Michael U Gutmann. Bayesian experimental design for implicit models by mutual information neural estimation. In _International conference on machine learning_, pages 5316-5326. PMLR, 2020.
* [31] Tor Lattimore and Csaba Szepesvari. _Bandit algorithms_. Cambridge University Press, 2020.
* [32] Yuanyuan Li, Yuezhou Liu, Lili Su, Edmund Yeh, and Stratis Ioannidis. Experimental design networks: A paradigm for serving heterogeneous learners under networking constraints. _IEEE/ACM Transactions on Networking_, 31(5):2236-2250, 2023.
* [33] Shinan Liu, Paul Schmitt, Francesco Bronzino, and Nick Feamster. Characterizing service provider response to the covid-19 pandemic in the united states. In _Passive and Active Measurement: 22nd International Conference, PAM 2021, Virtual Event, March 29-April 1, 2021, Proceedings 22_, pages 20-38. Springer, 2021.
* [34] Jie Lu, Anjin Liu, Fan Dong, Feng Gu, Joao Gama, and Guangquan Zhang. Learning under concept drift: A review. _IEEE transactions on knowledge and data engineering_, 31(12):2346-2363, 2018.
* [35] Andra Lutu, Diego Perino, Marcelo Bagnulo, Enrique Frias-Martinez, and Javad Khangosstar. A characterization of the covid-19 pandemic impact on a mobile network operator traffic. In _Proceedings of the ACM internet measurement conference_, pages 19-33, 2020.
* [36] Ankur Mallick, Kevin Hsieh, Behnaz Arzani, and Gauri Joshi. Matchmaker: Data drift mitigation in machine learning for large-scale systems. _Proceedings of Machine Learning and Systems_, 4:77-94, 2022.
* [37] H Brendan McMahan. A survey of algorithms and analysis for adaptive online learning. _The Journal of Machine Learning Research_, 18(1):3117-3166, 2017.
* [38] The California Privacy Rights Act of 2020, 2020.

* [39] David Owen, Andrew Melbourne, David Thomas, Enrico De Vita, Jonathan Rohrer, and Sebastien Ourselin. Optimisation of arterial spin labelling using bayesian experimental design. In _Medical Image Computing and Computer-Assisted Intervention-MICCAI 2016: 19th International Conference, Athens, Greece, October 17-21, 2016, Proceedings, Part III 19_, pages 511-518. Springer, 2016.
* [40] Friedrich Pukelsheim. _Optimal design of experiments_. SIAM, 2006.
* [41] CM Ryan, CC Drovandi, and AN Pettitt. Bayesian experimental design for models with intractable likelihoods using indirect inference. _Bayesian Analysis_, 11(3):857-883, 2014.
* [42] Elizabeth G Ryan, Christopher C Drovandi, and Anthony N Pettitt. Fully bayesian experimental design for pharmacokinetic studies. _Entropy_, 17(3):1063-1089, 2015.
* [43] Jeffrey C Schlimmer and Richard H Granger. Incremental learning from noisy data. _Machine learning_, 1:317-354, 1986.
* [44] Shai Shalev-Shwartz et al. Online learning and online convex optimization. _Foundations and Trends(r) in Machine Learning_, 4(2):107-194, 2012.
* [45] Wanggang Shen and Xun Huan. Bayesian sequential optimal experimental design for nonlinear models using policy gradient reinforcement learning. _Computer Methods in Applied Mechanics and Engineering_, 416:116304, 2023.
* [46] David J Spiegelhalter. Incorporating bayesian ideas into health-care evaluation. _Statist. Sci._, 19(1):156-174, 2004.
* [47] David M Steinberg and William G Hunter. Experimental design: review and comment. _Technometrics_, 26(2):71-97, 1984.
* [48] Vladimir Vapnik. Statistical learning theory. 1998.
* [49] J Kiefer J Wolfowitz. Optimum design in regression problems. _Ann. Math. Statist_, 30:271-294, 1959.
* [50] Xiaoyu You, Mi Zhang, Daizong Ding, Fuli Feng, and Yuanmin Huang. Learning to learn the future: Modeling concept drifts in time series prediction. In _Proceedings of the 30th ACM International Conference on Information & Knowledge Management_, pages 2434-2443, 2021.
* [51] Kai Yu, Jinbo Bi, and Volker Tresp. Active learning via transductive experimental design. In _Proceedings of the 23rd international conference on Machine learning_, pages 1081-1088, 2006.

## Appendix A Formal Assumptions and Definitions

We provide a summary of the notation used in this document.

### Technical Assumptions

We impose the following technical assumptions for our theoretical analysis.

**Assumption 1**.: _(Compact Experiments and Query Sets) Experiments \(\bm{x}\in\mathcal{X}\) and \(\bm{x}^{\prime}\in\mathcal{Z}\) are uniformly bounded under the \(\ell_{2}\) norm by \(D_{\mathcal{X}}\) and \(D_{\mathcal{Z}}\), respectively. Formally, \(\left\lVert\bm{x}\right\rVert_{2}\leq D_{\mathcal{X}},\left\lVert\bm{x}^{ \prime}\right\rVert_{2}\leq D_{\mathcal{Z}}\) for all \(\bm{x}\in\mathcal{X},\bm{x}^{\prime}\in\mathcal{Z}\)._

**Assumption 2**.: _(Compact Parameter Set) We assume that the true model parameters \(\bm{\beta}_{t}^{\star}\) for \(t\in[T]\), are uniformly bounded. Specifically, there exists a positive constant \(B^{\star}\) such that \(\left\lVert\bm{\beta}_{t}^{\star}\right\rVert_{2}\leq B^{\star}\) for all \(t\in[T]\)._

**Assumption 3**.: _(Invertible Design Matrices) The matrix \(\sum_{\bm{x}\in\mathcal{X}}\bm{x}\bm{x}^{\intercal}\) is non-singular, meaning that there exists a positive constant \(\omega\in\mathbb{R}_{>0}\) such that the following inequality holds: \(\sum_{\bm{x}\in\mathcal{X}}\bm{x}\bm{x}^{\intercal}\succeq\left\lvert \mathcal{X}\right\rvert\omega\bm{I}\succ 0\)._

Assumptions 1-2 guarantee the compactness of the experimental design space, the query space, and the model space. This compactness assumption is frequently employed in the analysis of learning problems [23, 44], facilitating the establishment of various theoretical properties. Furthermore, Assumption 3 ensures the invertibility of the covariance matrix \(\bm{V}(\bm{\pi})\), enabling the well-definedness of the LSE.

### Technical Definitions

We introduce the following key concepts foundational to our theoretical analysis.

**Definition 1**.: _Let \(\sigma\in(0,1]\) a positive real number. We define the \(\sigma\)-regularized simplex \(\zeta_{\mathcal{X},\sigma}\) as follows: \(\zeta_{\mathcal{X},\sigma}\triangleq\{T(\bm{\pi}):\bm{\pi}\in\zeta_{\mathcal{ X}}\}\), where \(T\) is a map given by \(T:\bm{\pi}\in\zeta_{\mathcal{X}}\rightarrow\frac{\bm{\pi}+\frac{1}{\sigma^{2}} \bm{1}}{1+\sigma}\) and \(\bm{1}\) denotes the all-ones vector in \(\mathbb{R}^{\mathcal{X}}\)._

\begin{table}
\begin{tabular}{l l} \hline  & **Notational Conventions** \\ \hline \([n]\) & Set \(\{1,2,\ldots,n\}\) \\ \(S^{V}\) & Set of functions from set \(V\) to set \(S\) \\ \(x,\bm{x},\bm{X}\) & Scalar, column vector, matrix \\ \(\left\lVert\bm{x}\right\rVert_{\bm{A}}\) & Norm \(\left\lVert\bm{x}\right\rVert_{\bm{A}}=\sqrt{\bm{x}^{\intercal}\bm{A}\bm{x}}\) for p.d. matrix \(\bm{A}\) \\ \(\bm{x}_{t^{\prime}:t}\) & Summation \(\sum_{s=t^{\prime}}^{t}\bm{x}_{s}\) \\ \(\mathcal{D}_{t^{\prime}:t}\) & Union of sets \(\bigcup_{s\in\{t^{\prime},t^{\prime}+1,\ldots,t\}}\mathcal{D}_{s}\) \\ \(\bm{A}\succ(\succeq)\ 0\) & Positive (semi-)definite matrix satisfying \(\bm{x}^{\intercal}\bm{A}\bm{x}>(\geq)\ 0\) for \(\bm{x}\in\mathbb{R}^{d}\setminus\{\bm{0}\}\) \\ \hline \multicolumn{3}{c}{**Experimental Design**} \\ \hline \(\mathcal{X}\) & Set of experiments \(\mathcal{X}\subset\mathbb{R}^{d}\) \\ \(\mathcal{Z}\) & Set of test experiments \(\mathcal{Z}\subseteq\mathbb{R}^{d}\) \\ \(\bm{\beta}_{t}^{\star}\) & True model at time \(t\) \\ \(\hat{\bm{\beta}}_{t}\) & Estimation of true model at time \(t\) \\ \(\zeta_{\mathcal{X}}\) & Experimental design space \(\zeta_{\mathcal{X}}=\left\{\bm{\pi}\in[0,1]^{\mathcal{X}}:\left\lVert\bm{\pi} \right\rVert_{1}=1\right\}\) \\ \(\zeta_{\mathcal{X},\sigma}\) & Restricted experimental design space \(\zeta_{\mathcal{X},\sigma}=\left\{\frac{\bm{\pi}+\frac{1}{\sigma^{2}}}{1+ \sigma}:\bm{\pi}\in\zeta_{\mathcal{X}}\right\}\) \\ \(\bm{\pi}\) & Experiemental design \\ \(M\) & Number of conducted experiments under a selected design \\ \((\bm{x},y)\) & Experiment \(\bm{x}\in\mathcal{X}\) and label \(y\in\mathbb{R}\) pair \\ \(\mathcal{D}\) & Dataset \(\mathcal{D}=\{(x_{i},y_{i}):i\in[M]\}\) \\ \(\mathcal{D}_{\bm{x}}\) & Dataset \(\mathcal{D}_{\bm{x}}=\left\{(x^{\prime},y)\in\mathcal{D}:\bm{x}^{\prime}=\bm{x}\right\}\) \\ \hline \end{tabular}
\end{table}
Table 1: Notation summaryIt is easy to verify that \(\lozenge_{\mathcal{X},\sigma}\subseteq\lozenge_{\mathcal{X}}\). Remark that selecting designs in the above set implicitly introduces ridge-regularization/Bayesian prior (with parameter \(\lambda\)), which allows the covariance matrix \(\bm{V}\) to be invertible for every design \(\bm{\pi}\in\lozenge_{\mathcal{X},\sigma}\). We define the following \(\bm{x}_{t}\)-optimal design objective (see the discussion in Sec. A.3) determined by the designs \(\bm{\pi}_{t-\tau},\dots,\bm{\pi}_{t}\in\lozenge_{\mathcal{X},\sigma}\) and the experiment \(\bm{x}_{t}\in\mathcal{Z}\) for \(t\in[T]\).

**Definition 2**.: _Consider Assumptions 1 and 3 hold. Consider \(\bm{x}_{t}\in\mathcal{Z}\) a query experiment at timeslot \(t\) and experimental designs \(\bm{\pi}_{t-\tau},\dots,\bm{\pi}_{t}\in\lozenge_{\mathcal{X},\sigma}\) at timeslot \(t\in[T]\). The \(\bm{x}_{t}\)-optimal design objective at time \(t\in[T]\) under the LSE estimator is given by \(v_{t}(\bm{\pi}_{t-\tau},\dots,\bm{\pi}_{t})\triangleq\bm{x}_{t}^{\intercal}\bm {V}^{-1}(\bm{\pi}_{t-\tau:t})\bm{x}_{t}\)._

Restricting the design space to \(\lozenge_{\mathcal{X},\sigma}\) from the original design space \(\lozenge_{\mathcal{X}}\) provides the following approximation guarantee:

**Proposition 2**.: _(Allen-Zhu et al. [2, Proposition 3.6]) For any \(\sigma\in(0,1)\), it holds for the \(\bm{x}_{t}\)-optimal design objective: \(v_{t}(\bm{\pi}_{\star})\leq v_{t}(\bm{\pi}_{\star,\sigma})\leq(1+\sigma)v_{t} (\bm{\pi}_{\star})\), where \(\bm{\pi}^{\star}\in\operatorname*{argmin}_{\bm{\pi}\in\lozenge_{\mathcal{X} }}v_{t}(\bm{\pi})\) and \(\bm{\pi}_{\star,\sigma}\in\operatorname*{argmin}_{\bm{\pi}\in\lozenge_{ \mathcal{X},\sigma}}v_{t}(\bm{\pi})\)._

The above proposition implies that if the designs \(\bm{\pi}_{t}\in\lozenge_{\mathcal{X},\sigma}\) selected at timeslots \(t\in[T]\) are competitive with the best design in \(\lozenge_{\mathcal{X},\sigma}\), then they are also competitive with the best design in \(\lozenge_{\mathcal{X}}\), penalized by the factor \((1+\sigma)\geq 1\).

### Experimental Design Landscape

Experimental design deals with the design of experiments in order to maximize the statistical efficiency of the resulting estimates. The objective is evaluated as a transformation of the covariance matrix of the LSE estimator. Popular choices [40] include: G(lobal)-optimality \(v_{G}(\bm{V})=\max\operatorname*{diag}\big{(}\bm{X}\bm{V}^{-1}\bm{X}^{ \intercal}\big{)}\), A(verage)-optimality \(v_{A}(\bm{V})=\operatorname*{tr}\big{(}\bm{V}^{-1}\big{)}\,/\,|\mathcal{X}|\), D(eterminant)-optimality \(v_{D}(\bm{V})=(\det\bm{V})^{-1/|\mathcal{X}|}\), and \(\bm{c}\)-optimality \(v_{\bm{c}}(\bm{V})=\bm{c}^{\intercal}\bm{V}^{-1}\bm{c}\). The choice of criterion depends on the specific goals of the experiment. For example, if the goal is to reduce the maximum estimation error of the model over all possible experiments, then D-optimality or G-optimality would be a good choice.1 If the goal is to to reduce the estimation error on average over all possible experiments then \(V\)-optimality is a good choice. If the goal is to estimate the outcome of a specific experiment \(\bm{c}\) of interest with as much precision as possible, then \(\bm{c}\)-optimality would be a good choice.

Footnote 1: Note that D-optimality and G-optimality are interchangeable when \(\mathcal{X}\subset\mathbb{R}^{d}\) is compact and \(\operatorname*{span}(\mathcal{X})=\mathbb{R}^{d}\)[28].

## Appendix B Technical Lemmas

### Proof of Proposition 3.1

Proof.: The EPE can be decomposed to variance and bias term in the following fashion. Given a query \(\bm{x}_{t}\), the following equality holds:

\[\mathbb{E}\left[\left(y_{t}-\bm{x}_{t}\cdot\bm{\hat{\beta}}_{t} \right)^{2}\right]=\mathbb{E}\left[\left(y_{t}-\bm{x}_{t}\cdot\bm{\beta}_{t}^ {\star}\right)^{2}\right]+\mathbb{E}\left[\left(\bm{x}_{t}\cdot\left(\bm{\beta }_{t}^{\star}-\bm{\hat{\beta}}_{t}^{\star}\right)\right)^{2}\right]=\sigma^{2 }+\left(\bm{x}_{t}\cdot\left(\bm{\beta}_{t}^{\star}-\mathbb{E}\left[\bm{\hat{ \beta}}_{t}\right]\right)\right)^{2}\] \[+\mathbb{E}\left[\left(\bm{x}_{t}\cdot\left(\mathbb{E}\left[\bm{ \hat{\beta}}_{t}\right]-\bm{\hat{\beta}}_{t}\right)\right)^{2}\right]=\sigma^{2 }+\left(\bm{x}_{t}\cdot\left(\mathbb{E}\left[\bm{\hat{\beta}}_{t}\right]- \bm{\hat{\beta}}_{t}\right)\right)^{2}+\bm{x}_{t}^{\intercal}\mathrm{cov}( \bm{\hat{\beta}}_{t})\bm{x}_{t}.\] (5)

Applying the identity \((a+b)^{2}=a^{2}+b^{2}+2ab\) twice, we can further analyze the expression. The first application of this identity is valid due to the properties of the label noise, specifically \(\mathbb{E}[\eta_{t}]=0\) and \(\mathbb{E}[\eta_{t}^{2}]=\sigma^{2}\). Subsequently, the second equality is derived by subtracting and adding the expected value of the estimator \(\mathbb{E}[\bm{\hat{\beta}}_{t}]\), to the expression. Note that \(\mathrm{cov}(\bm{A}\bm{y})=\bm{A}\mathrm{cov}\left(\bm{y}\right)\bm{A}^{\intercal}\) for some deterministic matrix \(\bm{A}\). Take \(\bm{A}=\bm{V}^{-1}(\bm{\pi}_{t-\tau:t})\bm{X}^{\intercal}\). The covariance is given by the following

\[\mathrm{cov}(\bm{\hat{\beta}}_{t})=\bm{A}\mathrm{cov}\left(\bm{y}\right)\bm{A}^{ \intercal}=\bm{A}\mathrm{cov}\Big{(}\tfrac{1}{M}\sum_{s=t-\tau}^{t}\sum_{(x,y) \in\mathcal{D}_{s,\bm{x}}}\bm{y}\Big{)}_{\bm{x}\in\mathcal{X}}\bm{A}^{ \intercal}=\tfrac{\sigma^{2}}{M}\bm{A}\mathrm{diag}\left((\bm{\pi}_{\bm{x},t- \tau:t})_{\bm{x}\in\mathcal{X}}\right)\bm{A}^{\intercal}\] (6)

\[=\sigma^{2}\left(\bm{V}^{-1}(\bm{\pi}_{t-\tau:t})\bm{X}\right)^{\intercal} \operatorname*{diag}\left((\bm{\pi}_{\bm{x},t-\tau:t})_{\bm{x}\in\mathcal{X}} \right)\big{(}\bm{V}^{-1}(\bm{\pi}_{t-\tau:t})\bm{X}^{\intercal}\big{)}^{ \intercal}=\frac{\sigma^{2}}{M}\bm{V}^{-1}(\bm{\pi}_{t-\tau:t}),\] (7)where we used \(\operatorname{var}\left(a(y_{1}+y_{2})\right)=a^{2}\) for two i.i.d. r.v.s with variance \(1\) in Eq. (6), \((\bm{A}\bm{B})^{\intercal}=\bm{B}^{\intercal}\bm{A}^{\intercal}\) and the symmetry of \(\bm{V}^{-1}(\bm{\pi}_{t-\tau:t})\) in Eq. (7). Further note that we just proved \(\bm{x}_{t}^{\intercal}\mathrm{cov}(\bm{\hat{\beta}}_{t})\bm{x}_{t}=\frac{ \sigma^{2}}{M}\left\|\bm{x}_{t}\right\|_{\bm{V}^{-1}(\bm{\pi}_{t-\tau:t})}\).

We have the following \(\mathbb{E}\left[\bm{\hat{\beta}}_{t}\right]=\bm{V}^{-1}(\bm{\pi}_{t-\tau:t})\bm {X}^{\intercal}\mathbb{E}\left[\bm{y}\right]\). We compute the expectation of the vector of labels \(\bm{y}\): \(\mathbb{E}\left[\bm{y}\right]=\left(\mathbb{E}\left[\sum_{s=t-\tau}^{t}\sum_{ (x,y)\in\mathcal{D}_{s,\bm{x}}}y\right]\right)_{\bm{x}\in\mathcal{X}}=\left( \sum_{s=t-\tau}^{t}\mathbb{E}\left[\sum_{(x,y)\in\mathcal{D}_{s,\bm{x}}}y \right]\right)_{\bm{x}\in\mathcal{X}}=\left(\bm{x}^{\intercal}\sum_{s=t-\tau }^{t}\pi_{s,\bm{x}}\bm{\beta}_{t}^{*}\right)_{\bm{x}\in\mathcal{X}}\). This concludes the proof. 

**Lemma 1**.: _Consider an invertible square matrix \(\bm{A}(\pi)\in\mathbb{R}^{d\times d}\) parameterized by \(\pi\in\mathbb{R}\). The derivative of \(\bm{A}^{-1}(\pi)\) w.r.t \(\pi\) is_

\[\frac{d\bm{A}^{-1}(\pi)}{d\pi}=-\bm{A}^{-1}\frac{d\bm{A}(\pi)}{d \pi}\bm{A}^{-1}.\] (8)

Proof.: Matrix \(\bm{A}(\pi)\) is invertible, so \(\bm{A}^{-1}(\pi)\bm{A}(\pi)=\bm{I}\). Let \(A_{i,j}\) and \(A_{i,j}^{-1}\) be the i, j-th entry of the matrices \(\bm{A}(\pi)\) and its inverse \(\bm{A}^{-1}(\pi)\). This gives the following set of equations:

\[\sum_{j=1}^{d}A_{i,j}(\pi)A_{j,k}^{-1}(\pi)=\mathds{1}\left(i=k \right),\qquad\text{for }i\in[d].\] (9)

Note that \(\mathds{1}\left(i=k\right)\) is a constant w.r.t. \(\pi\). Hence,

\[\sum_{j=1}^{d}\left(\frac{dA_{i,j}(\pi)}{d\pi}A_{j,k}^{-1}(\pi)+ \frac{dA_{j,k}^{-1}(\pi)}{d\pi}A_{i,j}(\pi)\right)=0,\qquad\text{for }i\in[d].\] (10)

This gives

\[\frac{d\bm{A}^{-1}(\pi)}{d\pi}=-\bm{A}^{-1}\frac{d\bm{A}(\pi)}{d \pi}\bm{A}^{-1}.\] (11)

We conclude the proof. 

**Lemma 2**.: _The \(\bm{x}_{t}\)-optimal design objective \(v_{t}:\odot_{\mathcal{X},\sigma}\to\mathbb{R}\) is differentiable with gradient at point \(\bm{\pi}\in\odot_{\mathcal{X},\sigma}\) given by_

\[\nabla_{\bm{\pi}}v_{t}(\bm{\pi})=\left(-\left(\bm{x}_{t}^{\intercal }\left(\bm{V}(\bm{\pi})\right)^{-1}\bm{x}\right)^{2}\right)_{\bm{x}\in \mathcal{X}}.\] (12)

Proof.: Consider the partial derivative w.r.t. \(\bm{x}\in\mathcal{X}\)

\[\frac{\partial v_{t}(\bm{\pi})}{\partial\pi_{\bm{\pi}}} =\bm{x}_{t}^{\intercal}\frac{\partial\left(\bm{V}^{-1}(\bm{\pi}) \right)}{\partial\pi_{\bm{\pi}}}\bm{x}_{t}=-\bm{x}_{t}^{\intercal}\left(\bm{V} ^{-1}(\bm{\pi})\frac{\partial\bm{V}(\bm{\pi})}{\partial\pi_{\bm{\pi}}}\bm{V}^ {-1}(\bm{\pi})\right)\bm{x}_{t}=-\bm{x}_{t}^{\intercal}\left(\bm{V}^{-1}(\bm{ \pi})\bm{x}\bm{x}^{\intercal}\bm{V}^{-1}(\bm{\pi})\right)\bm{x}_{t}\] (13) \[=-\left(\bm{x}_{t}^{\intercal}\bm{V}^{-1}(\bm{\pi})\bm{x}\right) \left(\bm{x}_{t}^{\intercal}\bm{V}^{-1}(\bm{\pi})\bm{x}\right)=-\left(\bm{x}_ {t}^{\intercal}\bm{V}^{-1}(\bm{\pi})\bm{x}\right)^{2}.\] (14)

The first equality is obtained using Lemma 1 in the Appendix. This concludes the proof. 

**Theorem 2**.: _Suppose that Assumptions 1 and 3 hold. The \(\bm{x}_{t}\)-optimal design objective \(v_{t}:\odot_{\mathcal{X},\sigma}\to\mathbb{R}\) is convex, and \(L_{\sigma}\)-Lipschitz continuous w.r.t. the norm \(\|\cdot\|_{1}\) for \(L_{\sigma}\triangleq\frac{D_{\sigma}^{2}D_{\sigma}^{2}}{\sigma^{2}\omega^{2}}\)._

Proof.: We first prove the convexity of \(v_{t,\sigma}\). Let \(\bm{X},\bm{Y}\) be positive definite matrices. The inverse of a p.d. matrix is a convex operation. The following holds

\[\left(\lambda\bm{X}+(1-\lambda)\bm{Y}\right)^{-1}\preceq\lambda \bm{X}^{-1}+(1-\lambda)\bm{Y}^{-1}\qquad\text{for }\lambda\in[0,1]\text{,}\] (15)

where \(\bm{X}\preceq\bm{Y}\) denotes that \(\bm{X}-\bm{Y}\) is positive semi definite. We have by considering \(\lambda\in[0,1]\)

\[v_{t}(\lambda\bm{\pi}+(1-\lambda)\bm{\pi}^{\prime})) =\bm{x}_{t}^{\intercal}\left(\bm{V}(\lambda\bm{\pi}+(1-\lambda) \bm{\pi}^{\prime})\right)^{-1}\bm{x}_{t}=\bm{x}_{t}^{\intercal}\left(\bm{V}( \lambda\bm{\pi})+\bm{V}((1-\lambda)\bm{\pi}^{\prime})\right)^{-1}\bm{x}_{t}\] (16) \[\leq\lambda\bm{x}_{t}^{\intercal}\left(\bm{V}(\bm{\pi})\right)^{-1} \bm{x}_{t}+(1-\lambda)\bm{x}_{t}^{\intercal}\left(\bm{V}(\bm{\pi}^{\prime}) \right)^{-1}\bm{x}_{t}=\lambda v_{t}(\bm{\pi})+(1-\lambda)v_{t}(\bm{\pi}^{ \prime}).\] (17)The equality follows from the definitions of \(v_{t}\) and \(\bm{V}(\bm{\pi})\). The inequality holds from the property of the inverse of positive definite matrices. This concludes the convexity proof.

Secondly, we prove the Lipschitzness of the function \(v_{t}\). Let \(\bm{\pi}\in(\tau+1)\dot{\cup}_{\mathcal{X},\sigma}\). We have the following from the gradient expression

\[\left\|\nabla_{\bm{\pi}}v_{t}(\bm{\pi})\right\|_{\infty}=\max_{ \bm{x}\in\mathcal{X}}\left\{\left(\bm{x}_{t}^{\intercal}\bm{V}^{-1}(\bm{\pi}) \bm{x}\right)^{2}\right\}\leq D_{Z}^{2}\max_{\bm{x}\in\mathcal{X}}\left\{\left\| \bm{V}^{-1}(\bm{\pi})\bm{x}\right\|_{2}^{2}\right\}\leq D_{Z}^{2}D_{\mathcal{X }}^{2}\left\|\bm{V}^{-1}(\bm{\pi})\right\|_{2}^{2}\] \[=D_{Z}^{2}D_{\mathcal{X}}^{2}\left(\lambda_{\max}(\bm{V}^{-1}( \bm{\pi}))\right)^{2}=D_{Z}^{2}D_{\mathcal{X}}^{2}\left(\lambda_{\min}^{-1}( \bm{V}(\bm{\pi}))\right)^{2}\leq\frac{4D_{\mathcal{X}}^{2}D_{\mathcal{Z}}^{2} }{\sigma^{2}\omega^{2}}=\frac{4D_{\mathcal{X}}^{2}D_{\mathcal{Z}}^{2}}{\sigma^ {2}\omega^{2}}.\]

Eq. (1) follows from the gradient expression in Eq. (12). Eq. (2) is obtained using the Cauchy-Schwarz inequality. Eq. (3) and Eq. (4) follow from the definition of the spectral norm of a matrix. Eq. (5) uses the fact that the maximum eigenvalue of an invertible matrix is equal to the reciprocal of the minimum eigenvalue of its inverse. Finally, Eq. (6) follows from \(\bm{V}(\bm{\pi})\succeq\frac{\sigma}{\sigma+1}\omega\bm{I}\succeq\frac{\sigma \omega}{2}\bm{I}\), i.e., \(\lambda_{\min}\left(\bm{V}(\bm{\pi})\right)\geq\lambda_{\min}\left(\frac{ \sigma\omega}{2}\bm{I}\right)=\frac{\sigma\omega}{2}\). This part concludes the proof. 

**Lemma 3**.: _Let \(\lambda_{1},\lambda_{2},\ldots,\lambda_{T}\) be a non-negative sequence of real numbers bounded by some constant, and let \(\tau\) be a positive integer bounded by a constant. If the sum \(\sum_{t=1}^{T}\lambda_{t}\) is \(O(T^{\alpha})\) for some \(\alpha\in[0,1)\), then the sum \(\sum_{t=\tau+1}^{T}\left(\sum_{s=t-\tau}^{t}\lambda_{s}\right)^{2}\) is \(\mathcal{O}\left(T^{\alpha}\right)\)._

Proof.: Consider the upper bound \(\left(\sum_{s=t-\tau}^{t}\lambda_{t}\right)^{2}\leq(\tau+1)\sum_{s=t-\tau}^{ t}\lambda_{t}^{2}\). Apply Cauchy-Schwartz inequality to obtain

\[\left(\sum_{s=t-\tau}^{t}1\cdot\lambda_{t}\right)^{2}\leq(\tau+1)\sum_{s=t- \tau}^{t}\lambda_{t}^{2}\leq\left(\sum_{s=t-\tau}^{t}1\right)\cdot\left(\sum_{ s=t-\tau}^{t}\lambda_{t}^{2}\right).\] (18)

Thus, it holds

\[\sum_{t=\tau+1}^{T}\left(\sum_{s=t-\tau}^{t}\lambda_{t}\right)^{2}\leq(\tau+1 )\sum_{t=\tau+1}^{T}\sum_{s=t-\tau}^{t}\lambda_{t}^{2}\leq(\tau+1)^{2}\sum_{t =1}^{T}\lambda_{t}^{2}.\] (19)

It is sufficient to show that \(\sum_{t=1}^{T}\lambda_{t}^{2}=\mathcal{O}\left(T^{\alpha}\right)\) is true for \(\sum_{t=\tau+1}^{T}\left(\sum_{s=t-\tau}^{t}\lambda_{t}\right)^{2}=\mathcal{O }\left(T^{\alpha}\right)\) to be true.

Consider the following set \(\mathcal{I}=\left\{t\in\left[T\right]:\lambda_{t}\geq 1\right\}\). We claim that \(\sum_{t=1}^{T}\lambda_{t}=\mathcal{O}\left(T^{\alpha}\right)\) implies \(\left|\mathcal{I}\right|=\mathcal{O}\left(T^{\alpha}\right)\). Assume that \(\left|\mathcal{I}\right|=\Omega\left(T^{\alpha^{\prime}}\right)\) and \(\alpha^{\prime}>\alpha\), but observe \(\sum_{t=1}^{T}\lambda_{t}\geq\sum_{t\in\mathcal{I}}\lambda_{t}\geq\sum_{t\in \mathcal{I}}1=\left|\mathcal{I}\right|\) by the definition of \(\mathcal{I}\). This means that \(\sum_{t=1}^{T}\lambda_{t}=\Omega\left(T^{\alpha^{\prime}}\right)\) which contradicts the assumption that \(\sum_{t=1}^{T}\lambda_{t}=\mathcal{O}\left(T^{\alpha}\right)\). Therefore, we must have \(\left|\mathcal{I}\right|=\mathcal{O}\left(T^{\alpha}\right)\).

Consider now the sum

\[\sum_{t=1}^{T}\lambda_{t}^{2}=\sum_{t\in\mathcal{I}}\lambda_{t}^{2}+\sum_{t\in \left[T\right]\setminus\mathcal{I}}\lambda_{t}^{2}\leq\left|\mathcal{I} \right|\max\left\{\lambda_{t}^{2}:t\in\mathcal{I}\right\}+\sum_{t\in\left[T \right]\setminus\mathcal{I}}\lambda_{t}=\mathcal{O}\left(T^{\alpha}\right).\] (20)

Since \(\left|\mathcal{I}\right|=\mathcal{O}\left(T^{\alpha}\right)\), the first sum is \(\mathcal{O}\left(T^{\alpha}\right)\). The second sum is bounded by \(\sum_{t\in\left[T\right]}\lambda_{t}\), which is \(\mathcal{O}\left(T^{\alpha}\right)\) by the assumption that \(\sum_{t=1}^{T}\lambda_{t}=\mathcal{O}\left(T^{\alpha}\right)\). Therefore, we have \(\sum_{t=1}^{T}\lambda_{t}^{2}=\mathcal{O}\left(T^{\alpha}\right)\). We conclude the proof. 

**Lemma 4**.: _Consider experimental designs \(\bm{\pi}_{1},\ldots,\bm{\pi}_{T}\in\dot{\cup}_{\mathcal{X}}\). For \(t\in\left\{1,2,...,T-1\right\}\), define \(\lambda_{t}=\left\|\bm{\pi}_{t}-\bm{\pi}_{t+1}\right\|_{1}\). Then, for any \(\tau\in\left\{1,2,...,T\right\}\), the following inequality holds:_

\[\sum_{s=t-\tau}^{t}\left\|\bm{\pi}_{s}-\bm{\pi}_{t}\right\|_{1}\leq(\tau+1)\sum_{ s=t-\tau}^{t}\lambda_{s}.\] (21)Proof.: Use the triangle inequality to obtain the following:

\[\sum_{s=t-\tau}^{t}\left\|\boldsymbol{\pi}_{s}-\boldsymbol{\pi}_{t} \right\|_{1} \leq\sum_{s=t-\tau}^{t}\sum_{s^{\prime}=s}^{t-1}\left\|\boldsymbol{ \pi}_{s^{\prime}+1}-\boldsymbol{\pi}_{s^{\prime}}\right\|_{1}=\sum_{s=t-\tau}^ {t}\sum_{s^{\prime}=s}^{t-1}\lambda_{s^{\prime}}\] (22) \[\leq\sum_{s=t-\tau}^{t}\sum_{s^{\prime}=t-\tau}^{t}\lambda_{s^{ \prime}}=(\tau+1)\sum_{t^{\prime}=t-\tau}^{t}\lambda_{t^{\prime}}.\] (23)

The proof is concluded. 

**Lemma 5**.: _Under Assumptions 1-3, a fixed design \(\boldsymbol{\pi}\in\left\langle\times,\sigma\right\rangle\), and a retention period \(\tau\in\mathbb{N}\), the expected prediction error at time \(t\) of the LSE model on experiment \(\boldsymbol{x}_{t}\in\mathcal{Z}\) is_

\[\mathbb{E}\left[\left(y_{t}-\boldsymbol{x}_{t}\cdot\boldsymbol{\beta}_{t} \right)^{2}\right]=\sigma^{2}+\frac{\sigma^{2}\boldsymbol{x}_{t}^{\intercal} \left(\boldsymbol{X}^{\intercal}\mathrm{diag}\left(\boldsymbol{\pi}\right) \boldsymbol{X}\right)^{-1}\boldsymbol{x}_{t}}{M(\tau+1)}+\left(\boldsymbol{x}_ {t}\cdot\left(\boldsymbol{\beta}_{t}^{\star}-\frac{1}{\tau+1}\sum_{s=t-\tau}^ {t}\boldsymbol{\beta}_{s}^{\star}\right)\right)^{2}.\]

Proof.: We can rewrite Eq. (1) as follows. It is easy to see that \(\boldsymbol{x}_{t}^{\intercal}\left(\boldsymbol{X}^{\intercal}\mathrm{diag} \left((\tau+1)\boldsymbol{\pi}\right)\boldsymbol{X}\right)^{-1}\boldsymbol{x }_{t}=\frac{\boldsymbol{x}_{t}^{\intercal}\boldsymbol{V}^{-1}(\boldsymbol{ \pi})\boldsymbol{x}_{t}}{\tau+1}\). Also, it holds

\[\frac{1}{\tau+1}\left(\boldsymbol{X}^{\intercal}\mathrm{diag} \left(\boldsymbol{\pi}\right)\boldsymbol{X}\right)^{-1}\left(\sum_{ \boldsymbol{x}\in\mathcal{X}}\boldsymbol{x}\boldsymbol{x}^{\intercal}\sum_{s=t -\tau}^{t}\pi_{\boldsymbol{x}}\boldsymbol{\beta}_{s}^{\star}\right)\] (24) \[=\left(\boldsymbol{X}^{\intercal}\mathrm{diag}\left(\boldsymbol{ \pi}\right)\boldsymbol{X}\right)^{-1}\left(\sum_{\boldsymbol{x}\in\mathcal{X} }\boldsymbol{x}\boldsymbol{x}^{\intercal}\boldsymbol{\pi}_{\boldsymbol{x}} \left(\frac{1}{\tau+1}\sum_{s=t-\tau}^{t}\boldsymbol{\beta}_{s}^{\star} \right)\right)\] (25) \[=\left(\boldsymbol{X}^{\intercal}\mathrm{diag}\left(\boldsymbol{ \pi}\right)\boldsymbol{X}\right)^{-1}\left(\boldsymbol{X}^{\intercal}\mathrm{ diag}\left(\boldsymbol{\pi}\right)\boldsymbol{X}\right)\left(\frac{1}{\tau+1}\sum_{s=t- \tau}^{t}\boldsymbol{\beta}_{s}^{\star}\right)=\boldsymbol{I}\left(\frac{1}{ \tau+1}\sum_{s=t-\tau}^{t}\boldsymbol{\beta}_{s}^{\star}\right)\] (26) \[=\left(\frac{1}{\tau+1}\sum_{s=t-\tau}^{t}\boldsymbol{\beta}_{s}^ {\star}\right).\] (27)

This concludes the proof. 

**Lemma 6**.: _Under Assumptions 1-3, let \(\left\{\boldsymbol{\pi}_{s}\right\}_{s=t-\tau}^{t}\in\left\langle\right._{ \mathcal{X},\sigma}^{t}\) be a sequence of designs, and let \(\lambda_{t}=\left\|\boldsymbol{\pi}_{t}-\boldsymbol{\pi}_{t+1}\right\|_{1}\). Then, the expected prediction error of the LSE model on experiment \(\boldsymbol{x}_{t}\in\mathcal{Z}\) at time \(t\) under an inference window size \(\tau\) is bounded as follows:_

\[|f_{t}(\boldsymbol{\pi}_{t},\ldots,\boldsymbol{\pi}_{t})-f_{t}(\boldsymbol{ \pi}_{t-\tau},\ldots,\boldsymbol{\pi}_{t})|\leq\epsilon_{\boldsymbol{\lambda},t,\tau},\] (28)

_where \(\epsilon_{\boldsymbol{\lambda},t,\tau}=8\left(\frac{|\mathcal{X}|D_{Z}B_{ \mathcal{X},\nu,Z}D_{\mathcal{X}}}{\omega\sigma}\sum_{t^{\prime}=t-\tau}^{t} \lambda_{t^{\prime}}\right)^{2}\)\(+\)\(16\frac{|\mathcal{X}|D_{Z}B_{\mathcal{X},\nu,Z}^{2}D_{\mathcal{X}}}{\omega\sigma} \sum_{t^{\prime}=t-\tau}^{t}\lambda_{t^{\prime}}\)\(+\)\(\frac{4D_{\mathcal{X}}^{2}D_{\mathcal{X}}^{2}}{\sigma^{2}\omega^{2}M}\sum_{t^{ \prime}=t-\tau}^{t}\lambda_{t^{\prime}}\)._

Proof.: The proof is divided into two parts. In the first part we bound the variance term, and in the second part we bound the bias term.

**Part 1.** We bound the quantity \(|v_{t}(\bm{\pi}_{t-\tau},\ldots,\bm{\pi}_{t})-v_{t}(\bm{\pi}_{t},\ldots,\bm{\pi}_ {t})|\). Lemma 2 shows that \(v_{t}\) is Lipschitz continuous over \(\dot{\otimes}_{\mathcal{X},\sigma}\) with parameter \(L_{\sigma}=\frac{4D_{\mathcal{X}}^{2}D_{\mathcal{Z}}^{2}}{\sigma^{2}\omega^{2} M(\tau+1)}\). Thus, we have the following:

\[|v_{t}(\bm{\pi}_{t-\tau},\ldots,\bm{\pi}_{t})-v_{t}(\bm{\pi}_{t}, \ldots,\bm{\pi}_{t})| =|v_{t}(\bm{\pi}_{t-\tau:t})-v_{t}((\tau+1)\bm{\pi}_{t})|\] (29) \[\leq L_{\sigma}\left\|\bm{\pi}_{t-\tau:t}-(\tau+1)\bm{\pi}_{t} \right\|_{1}\] (30) \[\leq L_{\sigma}\sum_{s=t-\tau}^{t}\left\|\bm{\pi}_{s}-\bm{\pi}_{t }\right\|_{1}\] (31) \[\leq L_{\sigma}(\tau+1)\sum_{t^{\prime}=t-\tau}^{t}\lambda_{t^{ \prime}}\hskip 42.679134pt(\text{Lemma \ref{lem:2011}})\] (32) \[=\frac{4D_{\mathcal{X}}^{2}D_{\mathcal{Z}}^{2}}{\sigma^{2}\omega^ {2}M}\left(\sum_{t^{\prime}=t-\tau}^{t}\lambda_{t^{\prime}}\right).\hskip 42.679134pt( \text{Lemma \ref{lem:2011}})\] (33)

**Part 2.** We bound the quantity \(|b_{t}(\bm{\pi}_{t-\tau},\ldots,\bm{\pi}_{t})-b_{t}(\bm{1}/\left|\mathcal{X} \right|,\ldots,\bm{1}/\left|\mathcal{X}\right|)|\). Recall that

\[b_{t}(\bm{\pi}_{t-\tau},\ldots,\bm{\pi}_{t})=\left(\bm{x}_{t}\cdot\left(\bm{V} ^{-1}(\bm{\pi}_{t-\tau:t})\left(\sum_{\bm{x}\in\mathcal{X}}\bm{x}\bm{x}^{ \intercal}\sum_{s=t-\tau}^{t}\pi_{s,\bm{x}}\bm{\beta}_{s}^{\star}\right)-\bm{ \beta}_{t}^{\star}\right)\right)^{2}.\] (34)

Observe the following:

\[\mathbb{E}\left[\bm{x}_{t}\cdot\hat{\bm{\beta}}_{t}\right] =\bm{x}_{t}\cdot\left(\bm{V}^{-1}(\bm{\pi}_{t-\tau:t})\left(M \sum_{\bm{x}\in\mathcal{X}}\bm{x}\bm{x}^{\intercal}\sum_{s=t-\tau}^{t}\pi_{s, \bm{x}}\bm{\beta}_{s}^{\star}\right)\right)\] (35) \[=\bm{x}_{t}\cdot\bm{V}^{-1}(\bm{\pi}_{t-\tau:t})\left(M\sum_{\bm {x}\in\mathcal{X}}\bm{x}\bm{x}^{\intercal}\frac{\pi_{t-\tau:t,\bm{x}}}{\tau+1} \sum_{s=t-\tau}^{t}\bm{\beta}_{s}^{\star}\right)+\bm{x}_{t}\cdot\left(\bm{ \Delta}_{1}+\bm{\Delta}_{2}\right),\] (36)

where

\[\bm{\Delta}_{1} =\bm{V}^{-1}(\bm{\pi}_{t-\tau:t})\left(M\sum_{\bm{x}\in\mathcal{X }}\bm{x}\bm{x}^{\intercal}\sum_{s=t-\tau}^{t}\left(\pi_{s,\bm{x}}-\pi_{t,\bm{ x}}\right)\bm{\beta}_{s}^{\star}\right)\!,\] (37) \[\bm{\Delta}_{2} =\bm{V}^{-1}(\bm{\pi}_{t-\tau:t})\left(M\sum_{\bm{x}\in\mathcal{X }}\bm{x}\bm{x}^{\intercal}\frac{(\tau+1)\pi_{t,\bm{x}}-\pi_{t-\tau:\tau,\bm{x} }}{\tau+1}\sum_{s=t-\tau}^{t}\bm{\beta}_{s}^{\star}\right)\!.\] (38)

Moreover, note that the first term in Eq. (36) satisfies

\[\bm{x}_{t}\cdot\left(\bm{V}^{-1}(\bm{\pi}_{t-\tau:t})\left(M\sum _{\bm{x}\in\mathcal{X}}\bm{x}\bm{x}^{\intercal}\frac{\pi_{t-\tau:t,\bm{x}}}{ \tau+1}\sum_{s=t-\tau}^{t}\bm{\beta}_{s}^{\star}\right)\right)\] (39) \[=\bm{x}_{t}\cdot\left(\bm{V}^{-1}(\bm{\pi}_{t-\tau:t})\bm{V}(\bm {\pi}_{t-\tau:t})\left(\frac{1}{\tau+1}\sum_{s=t-\tau}^{t}\bm{\beta}_{s}^{ \star}\right)\right)=\bm{x}_{t}\cdot\left(\frac{1}{\tau+1}\sum_{s=t-\tau}^{t} \bm{\beta}_{s}^{\star}\right).\] (40)

We establish upper bounds on the remaining terms:

\[|\bm{x}_{t}\cdot\bm{\Delta}_{1}| \leq\sum_{\bm{x}\in\mathcal{X}}\sum_{s=t-\tau}^{t}\left|\bm{V}^{- 1}(\bm{\pi}_{t-\tau:t})\left(M\bm{x}\bm{x}^{\intercal}\left((\tau+1)\pi_{t, \bm{x}}-\pi_{t-\tau:\tau,\bm{x}}\right)\bm{\beta}_{s}^{\star}\right)\cdot\bm{x }_{t}\right|\] (41) \[\leq\sum_{\bm{x}\in\mathcal{X}}\sum_{s=t-\tau}^{t}D_{\mathcal{Z}} \left\|\bm{V}^{-1}(\bm{\pi}_{t-\tau:t})\left(M\bm{x}\bm{x}^{\intercal}\left(( \tau+1)\pi_{t,\bm{x}}-\pi_{t-\tau:\tau,\bm{x}}\right)\bm{\beta}_{s}^{\star} \right)\right\|_{2}\] (42) \[\leq 2\sum_{\bm{x}\in\mathcal{X}}\frac{D_{\mathcal{Z}}}{\omega \sigma(\tau+1)}B_{\mathcal{X}\cup\mathcal{Z}}D_{\mathcal{X}}\left|(\tau+1)\pi_{t,\bm{x}}-\pi_{t-\tau:\tau,\bm{x}}\right|\] (43) \[\leq\frac{2\left|\mathcal{X}\right|D_{\mathcal{Z}}B_{\mathcal{X} \cup\mathcal{Z}}D_{\mathcal{X}}}{\omega\sigma}\sum_{t^{\prime}=t-\tau}^{t} \lambda_{t^{\prime}}.\] (44)Following the same steps, we obtain

\[\left|\bm{x}_{t}\cdot\bm{\Delta}_{2}\right|\leq\sum_{\bm{x}\in\mathcal{X}}\sum_{s =t-\tau}^{t}\frac{2D_{\mathcal{Z}}B_{\mathcal{X}\cup\mathcal{Z}}D_{\mathcal{X}} }{\omega\sigma(\tau+1)}\left|\pi_{s,\bm{x}}-\pi_{t,\bm{x}}\right|\leq\frac{2 \left|\mathcal{X}\right|D_{\mathcal{Z}}B_{\mathcal{X}\cup\mathcal{Z}}D_{ \mathcal{X}}}{\omega\sigma}\sum_{t^{\prime}=t-\tau}^{t}\lambda_{t^{\prime}}.\] (45)

Use the simple fact that \((x+y)^{2}=x^{2}+y^{2}+2xy\), and \((x+y)^{2}\leq 2x^{2}+2y^{2}\) to obtain

\[\left|b_{t}(\bm{\pi}_{t-\tau},\ldots,\bm{\pi}_{t})-b_{t}(\bm{1}/ \left|\mathcal{X}\right|,\ldots,\bm{1}/\left|\mathcal{X}\right|)\right|\] (46) \[=\left|\left(\bm{x}_{t}\cdot\left(\mathbb{E}\left[\bm{\hat{\beta} }_{t}\right]-\bm{\beta}_{t}^{\star}\right)\right)^{2}-\left(\bm{x}_{t}\cdot \left(\frac{1}{\tau+1}\sum_{s=t-\tau}^{t}\bm{\beta}_{s}^{\star}-\bm{\beta}_{t}^ {\star}\right)\right)^{2}\right|\] (47) \[=\left|\left(\bm{x}_{t}\cdot\left(\left(\frac{1}{\tau+1}\sum_{s=t -\tau}^{t}\bm{\beta}_{s}^{\star}+\bm{\Delta}_{1}+\bm{\Delta}_{2}\right)-\bm{ \beta}_{t}^{\star}\right)\right)^{2}-\left(\bm{x}_{t}\cdot\left(\frac{1}{\tau+ 1}\sum_{s=t-\tau}^{t}\bm{\beta}_{s}^{\star}-\bm{\beta}_{t}^{\star}\right) \right)^{2}\right|\] (48) \[\leq 8\left(\frac{\left|\mathcal{X}\right|D_{\mathcal{Z}}B_{ \mathcal{X}\cup\mathcal{Z}}D_{\mathcal{X}}}{\omega\sigma}\sum_{t^{\prime}=t- \tau}^{t}\lambda_{t^{\prime}}\right)^{2}+16\frac{\left|\mathcal{X}\right|D_{ \mathcal{Z}}B_{\mathcal{X}\cup\mathcal{Z}}^{2}D_{\mathcal{X}}}{\omega\sigma} \sum_{t^{\prime}=t-\tau}^{t}\lambda_{t^{\prime}}.\] (50)

Finally, combine Eqs. (33) and (50)

\[\left|f_{t}(\bm{\pi}_{t},\ldots,\bm{\pi}_{t})-f_{t}(\bm{\pi}_{t- \tau},\ldots,\bm{\pi}_{t})\right|\] (51) \[\leq 8\left(\frac{\left|\mathcal{X}\right|D_{\mathcal{Z}}B_{ \mathcal{X}\cup\mathcal{Z}}D_{\mathcal{X}}}{\omega\sigma}\sum_{t^{\prime}=t- \tau}^{t}\lambda_{t^{\prime}}\right)^{2}+16\frac{\left|\mathcal{X}\right|D_{ \mathcal{Z}}B_{\mathcal{X}\cup\mathcal{Z}}^{2}D_{\mathcal{X}}}{\omega\sigma} \sum_{t^{\prime}=t-\tau}^{t}\lambda_{t^{\prime}}+\frac{4D_{\mathcal{X}}^{2}D_{ \mathcal{Z}}^{2}}{\sigma^{2}\omega^{2}M}\left(\sum_{t^{\prime}=t-\tau}^{t} \lambda_{t^{\prime}}\right).\] (52)

We conclude the proof. 

**Proposition 3**.: _Under Assumptions 1-3, let \(\left\{\bm{\pi}_{s}\right\}_{s=t-\tau}^{t}\in\otimes_{\mathcal{X},\sigma}\) be a sequence of designs, and let \(P_{T}=\sum_{t=1}^{T}\left\|\bm{\pi}_{t}-\bm{\pi}_{t+1}\right\|_{1}\). Then, the EPE of the LSE on sequence of experiments \(\left\{\bm{x}_{t}\right\}_{t=1}^{T}\in\mathcal{Z}\) at time \(t\) under an inference window size \(\tau\) is bounded as follows:_

\[\sum_{t=\tau+1}^{T}\left|f_{t}(\bm{\pi}_{t},\ldots,\bm{\pi}_{t})-f_{t}(\bm{\pi} _{t-\tau},\ldots,\bm{\pi}_{t})\right|=\mathcal{O}\left(P_{T}\right).\] (53)

Proof.: From Lemma 6 we have

\[\sum_{t=\tau+1}^{T}\left|f_{t}(\bm{\pi}_{t},\ldots,\bm{\pi}_{t})-f _{t}(\bm{\pi}_{t-\tau},\ldots,\bm{\pi}_{t})\right| =\sum_{t=\tau+1}^{T}\epsilon_{\bm{\lambda},t,\tau}=\mathcal{O} \left(\sum_{t=\tau+1}^{T}\lambda_{t}+\sum_{t=\tau+1}^{T}\left(\sum_{st-\tau}^{ t}\lambda_{s}\right)^{2}\right)\] (54) \[=\mathcal{O}\left(P_{T}\right)+\mathcal{O}\left(\sum_{t=\tau+1} ^{T}\left(\sum_{st-\tau}^{t}\lambda_{s}\right)^{2}\right).\] (55)

Lemma 3 gives \(\mathcal{O}\left(\sum_{t=\tau+1}^{T}\left(\sum_{st-\tau}^{t}\lambda_{s}\right)^{ 2}\right)=\mathcal{O}\left(P_{T}\right)\). This concludes the proof. 

## Appendix C A Unified Analysis via Mirror Descent Schemes

**Mirror Descent Parametrization.** We provide the set of assumptions relevant to correct parametrization of the online mirror descent family of gradient-based policies.

**Assumption 4**.: _The map \(\Phi:\mathcal{S}_{\Phi}\rightarrow\mathbb{R}\) satisfies the following properties:_* _The domain_ \(\mathcal{S}_{\Phi}\) _of_ \(\Phi\) _is a convex and open set such that the decision set_ \(\mathcal{S}\) _is included in its closure, i.e.,_ \(\mathcal{S}\subseteq\mathrm{closure}(\mathcal{D})\)_, and their intersection is nonempty_ \(\mathcal{S}\cap\mathcal{S}_{\Phi}\neq\emptyset\)_._
* _The map_ \(\Phi\) _is_ \(\rho\) _strongly-convex over_ \(\mathcal{S}_{\Phi}\) _w.r.t. a norm_ \(\|\cdot\|\) _and differentiable over_ \(\mathcal{S}_{\Phi}\)_._
* _The map_ \(\nabla\Phi(\boldsymbol{\pi}):\mathcal{S}_{\Phi}\to\mathbb{R}^{n}\) _is surjective._
* _The gradient of_ \(\Phi\) _diverges on the boundary of_ \(\mathcal{S}_{\Phi}\)_, i.e.,_ \(\lim_{\boldsymbol{\pi}\to\partial\mathcal{S}_{\Phi}}\|\nabla\Phi(\boldsymbol{ \pi})\|=+\infty,\) _where_ \(\partial\mathcal{S}_{\Phi}=\mathrm{closure}(\mathcal{S}_{\Phi})\setminus \mathcal{S}_{\Phi}\)_._

A map \(\Phi:\mathcal{S}_{\Phi}\to\mathbb{R}\) is said to be a mirror map if it satisfies Assumption 4.

**Assumption 5**.: _Consider a decision set \(\mathcal{S}\) and a mirror map \(\Phi\). The dual norm \(\|\cdot\|\), of the gradient of the mirror map \(\Phi\) is bounded by \(L_{\Phi}\in\mathbb{R}_{\geq 0}\), i.e., \(\left\|\nabla\Phi(\boldsymbol{\pi})\right\|_{\star}\leq L_{\Phi}\) for every \(\boldsymbol{\pi}\in\mathcal{S}\)._

**Assumption 6**.: _Consider a decision set \(\mathcal{S}\), a map \(\Phi\), and \(\boldsymbol{\pi}_{1}=\mathrm{argmin}_{\mathcal{S}}\Phi(\boldsymbol{\pi})\). The Bregman divergence \(D_{\Phi}\) is bounded over \(\mathcal{S}\), i.e., there exits \(D_{\Phi,\max}\) s.t. \(D_{\Phi}(\boldsymbol{\pi},\boldsymbol{\pi}_{1})\leq D_{\Phi,\max}^{2}<\infty\) for any \(\boldsymbol{\pi}\in\mathcal{S}\)._

**Definitions.** We present formal definitions that are crucial for our subsequent analysis of regret bounds associated with mirror descent algorithms.

**Definition 3**.: _The Bregman projection [29] associated to a map \(\Phi\) onto a convex set \(\mathcal{S}\) is denoted by \(\Pi_{\mathcal{S}}^{\Phi}:\mathbb{R}^{n}\to\mathcal{S}\), is defined as_

\[\Pi_{\mathcal{S}}^{\Phi}(\boldsymbol{\pi}^{\prime})=\operatorname*{arg\,min}_{ \boldsymbol{\pi}\in\mathcal{S}}D_{\Phi}(\boldsymbol{\pi},\boldsymbol{\pi}^{ \prime}),\quad\text{where}\quad D_{\Phi}(\boldsymbol{\pi},\boldsymbol{\pi}^{ \prime})=\Phi(\boldsymbol{\pi})-\Phi(\boldsymbol{\pi}^{\prime})-\nabla\Phi( \boldsymbol{\pi}^{\prime})\cdot(\boldsymbol{\pi}-\boldsymbol{\pi}^{\prime}).\] (56)

**Definition 4**.: _Let \(\Phi:\mathcal{S}_{\Phi}\to\mathbb{R}\) be a mirror map satisfying Assumption 4, and \(\eta\in\mathbb{R}_{>0}\) be the learning rate. At timeslot \(t\in[T]\), Online Mirror Descent upon receiving cost function \(f_{t}\) it updates the decision \(\boldsymbol{\pi}_{t}\) according to the mapping_

\[\boldsymbol{\pi}_{t+1}=\Pi_{\mathcal{S}\cap\mathcal{S}_{\Phi}}\left(\left( \nabla\Phi\right)^{-1}\left(\nabla\Phi(\boldsymbol{\pi}_{t})-\eta\nabla_{ \boldsymbol{\pi}}f_{t}(\boldsymbol{\pi}_{t})\right)\right).\] (57)

### Technical Lemmas

**Lemma 7**.: _(First Order Optimality Condition) Let \(f:\mathcal{S}\to\mathbb{R}\) be convex and \(\mathcal{S}\) a closed convex set on which \(f\) is differentiable. Then_

\[\boldsymbol{\pi}^{\star}\in\mathrm{argmin}_{\boldsymbol{\pi}\in\mathcal{S}}f( \boldsymbol{\pi})\iff\nabla f(\boldsymbol{\pi}^{\star})\cdot\left(\boldsymbol {\pi}^{\star}-\boldsymbol{\pi}^{\prime}\right),\forall\boldsymbol{\pi}^{ \prime}\in\mathcal{S}.\] (58)

**Lemma 8**.: _Assume that \(\Phi\) is \(\rho\)-strongly convex w.r.t \(\|\cdot\|\). Let \(\|\cdot\|_{\star}\) be the dual norm of \(\|\cdot\|\) and \(\eta\in\mathbb{R}_{>0}\), and \(\boldsymbol{g}_{t}\in\mathbb{R}^{d}\) be the gradient at time \(t\). The upper bound \(\Lambda(\boldsymbol{\pi}_{t},\boldsymbol{g}_{t})\geq\frac{D_{\Phi}(\boldsymbol {\pi}_{t},\boldsymbol{z}_{t+1})}{\eta^{2}}\) can be taken as_

\[\Lambda(\boldsymbol{\pi}_{t},\boldsymbol{g}_{t})=\frac{\|\boldsymbol{g}_{t}\|_ {\star}^{2}}{2\rho}.\] (59)

Proof.: Expand \(D_{\Phi}(\boldsymbol{\pi}_{t},\boldsymbol{z}_{t+1})\) to obtain

\[D_{\Phi}(\boldsymbol{\pi}_{t},\boldsymbol{z}_{t+1}) =\Phi(\boldsymbol{\pi}_{t})-\Phi(\boldsymbol{z}_{t+1})-\nabla \Phi(\boldsymbol{z}_{t+1})\cdot(\boldsymbol{\pi}_{t}-\boldsymbol{z}_{t+1})\] (60) \[=\Phi(\boldsymbol{\pi}_{t})-\Phi(\boldsymbol{z}_{t+1})+\nabla \Phi(\boldsymbol{\pi}_{t})\cdot(\boldsymbol{z}_{t+1}-\boldsymbol{\pi}_{t})+ \nabla\Phi(\boldsymbol{\pi}_{t})-\nabla\Phi(\boldsymbol{z}_{t+1})\cdot( \boldsymbol{\pi}_{t}-\boldsymbol{z}_{t+1})\] (61) \[\leq-\frac{\rho}{2}\left\|\boldsymbol{\pi}_{t}-\boldsymbol{z}_{t +1}\right\|^{2}+\eta\boldsymbol{g}_{t}\cdot(\boldsymbol{\pi}_{t}-\boldsymbol{z} _{t+1}).\] (62)

By the strong convexity of \(\Phi\) and the gradient step. Use Cauchy-Schwarz inequality to obtain

\[D_{\Phi}(\boldsymbol{\pi}_{t},\boldsymbol{z}_{t+1})\leq\eta\left\|\boldsymbol{g }_{t}\right\|_{\star}\left\|\boldsymbol{\pi}_{t}-\boldsymbol{z}_{t+1}\right\| -\frac{\rho}{2}\left\|\boldsymbol{\pi}_{t}-\boldsymbol{z}_{t+1}\right\|^{2}\leq \frac{\eta^{2}\left\|\boldsymbol{g}_{t}\right\|_{\star}^{2}}{2\rho},\] (63)

since \(\max_{z}(az-bz^{2})=a^{2}/4b\). Combine (63) and (62) to conclude the proof.

**Lemma 9**.: _When \(\Phi\) is the negative entropy, \(\bm{\pi}\in\mathbb{R}_{>0}^{n}\), and \(\bm{g}_{t}\in\mathbb{R}_{\geq 0}^{n}\). The upper bound \(\Lambda(\bm{\pi}_{t},\bm{g}_{t})\geq\frac{D_{\Phi}(\bm{\pi}_{t},\bm{z}_{t+1})}{ \eta^{2}}\) can be taken as_

\[\Lambda(\bm{\pi}_{t},\bm{g}_{t})=\sum_{i=1}^{n}\frac{x_{i}\left(g_{t,i} \right)^{2}}{2}.\] (64)

Proof.: Expand \(D_{\Phi}(\bm{\pi}_{t},\bm{z}_{t+1})\) to obtain

\[D_{\Phi}(\bm{\pi}_{t},\bm{z}_{t+1})=\sum_{i=1}^{n}x_{t,i}\left( \exp(-\eta g_{t,i})+\eta g_{t,i}-1\right).\] (65)

Since \(\exp(x)-x-1\leq x^{2}/2\) for \(x\leq 0\), it holds \(D_{\Phi}(\bm{\pi}_{t},\bm{z}_{t+1})\leq\eta^{2}\sum_{i=1}^{n}x_{t,i}(g_{t,i})^ {2}/2=\eta^{2}\Lambda(\bm{\pi}_{t},\bm{g}_{t})\). We conclude the proof. 

**Lemma 10**.: _Fix \(\bm{z}\in\mathcal{D}\), and let \(\bm{\pi}=\Pi_{\mathcal{X}}^{\Phi}(\bm{z})\). Then_

\[D_{\Phi}(\bm{\pi}^{\prime},\bm{z})\geq D_{\Phi}(\bm{\pi}^{\prime },\bm{\pi})\qquad\forall\bm{\pi}^{\prime}\in\mathcal{X}.\] (66)

Proof.: The generalized Pythagorean equality given by

\[D_{\Phi}(\bm{\pi},\bm{\pi}^{\prime})+D_{\Phi}(\bm{\pi}^{\prime \prime},\bm{\pi})-D_{\Phi}(\bm{\pi}^{\prime\prime},\bm{\pi}^{\prime})=(\nabla \Phi(\bm{\pi})-\nabla\Phi(\bm{\pi}^{\prime}))\cdot(\bm{\pi}-\bm{\pi}^{\prime \prime})\] (67)

from the definition of the Bregman divergence, and the first order optimality condition [8], the following holds

\[D_{\Phi}(\bm{\pi},\bm{z})+D_{\Phi}(\bm{\pi}^{\prime},\bm{\pi})-D _{\Phi}(\bm{\pi}^{\prime},\bm{z})=(\nabla\Phi(\bm{\pi})-\nabla\Phi(\bm{z})) \cdot(\bm{\pi}-\bm{\pi}^{\prime})\leq 0.\] (68)

The proof is concludes by noting that \(D_{\Phi}(\bm{\pi}^{\prime},\bm{z})\geq 0\) for any \(\bm{\pi}^{\prime}\in\mathcal{D}\). 

**Lemma 11**.: _At time \(t\) under a gradient \(\bm{g}_{t}\), OMD update rule with state \(\bm{\pi}_{t}\) satisfies the following_

\[\bm{g}_{t}\cdot(\bm{\pi}_{t}-\bm{\pi}) \leq\frac{1}{\eta}\left(\eta^{2}\Lambda(\bm{g}_{t},\bm{\pi}_{t}) +D_{\Phi}(\bm{\pi},\bm{\pi}_{t})-D_{\Phi}(\bm{\pi},\bm{\pi}_{t+1})\right).\] (69)

Proof.: By the gradient step, \(\bm{g}_{t}=(\bm{\pi}_{t}-\bm{z}_{t+1})/\eta\), so

\[\bm{g}_{t}\cdot(\bm{\pi}_{t}-\bm{\pi}) =(\nabla\Phi(\bm{\pi}_{t})-\nabla\Phi(\bm{z}_{t+1}))\left(\bm{\pi }_{t}-\bm{\pi}\right)\] (70) \[=\frac{1}{\eta}\left(D_{\Phi}(\bm{\pi}_{t},\bm{z}_{t+1})+D_{\Phi} (\bm{\pi},\bm{\pi}_{t})-D_{\Phi}(\bm{\pi},\bm{z}_{t+1})\right)\] (71) \[\leq\frac{1}{\eta}\left(D_{\Phi}(\bm{\pi}_{t},\bm{z}_{t+1})+D_{ \Phi}(\bm{\pi},\bm{\pi}_{t})-D_{\Phi}(\bm{\pi},\bm{\pi}_{t+1})\right)\] (72)

The first equality is obtained using Eq (67). The inequality is obtained using Lemma 10. This concludes the proof. 

### Regret Guarantee

**Theorem 3**.: _Consider \(\mathcal{S}\subseteq\mathbb{R}^{d}\) as the decision set, and a comparator sequence \(\left\{\bm{\pi}_{t}^{\star}\right\}_{t=1}^{T}\in\mathcal{S}^{T}\) with path-length \(P_{T}=\sum_{t=1}^{T}\left\|\bm{\pi}_{t}^{\star}-\bm{\pi}_{t+1}^{\star}\right\|_ {1}\). Under Assumptions (4), and (5), Online Mirror Descent (57) configured with mirror map \(\Phi:\mathcal{S}_{\Phi}\rightarrow\mathbb{R}\) and learning rate \(\eta\in\mathbb{R}_{\geq 0}\) has the following regret guarantee against \(L\)-Lipschitz (w.r.t. \(\|\cdot\|\)) differentiable convex cost functions \(f_{1},\ldots,f_{T}\):_

\[\sum_{t=1}^{T}f_{t}(\bm{\pi}_{t})-\sum_{t=1}^{T}f_{t}(\bm{\pi}_{t} ^{\star})\leq\tfrac{1}{\eta}\left(D_{\Phi,\max}^{2}+2L_{\Phi}P_{T}\right)+ \eta\left(\tfrac{L^{2}T}{2\rho}\right).\] (74)

_The policy-induced decisions exhibit a path length of \(\sum_{t=1}^{T}\left\|\bm{\pi}_{t+1}-\bm{\pi}_{t}\right\|=\mathcal{O}\left(\eta T\right)\)._Proof.: The cost functions \(f_{t}\) are convex and differentiable. So, it holds \(\sum_{t=1}^{T}f_{t}(\bm{\pi}_{t})-\sum_{t=1}^{T}f_{t}(\bm{\pi}_{t}^{\star})\leq \nabla_{\bm{\pi}}f_{t}(\bm{\pi}_{t})\cdot(\bm{\pi}_{t}-\bm{\pi}_{t}^{\star})\). Lemma 11 gives:

\[\sum_{t=1}^{T}f_{t}(\bm{\pi}_{t})-\sum_{t=1}^{T}f_{t}(\bm{\pi}_{t}^{\star})\leq \sum_{t=1}^{T}\frac{1}{\eta}\left(D_{\Phi}(\bm{\pi},\bm{\pi}_{t})-D_{\Phi}(\bm{ \pi},\bm{\pi}_{t+1})\right)+\eta\sum_{t=1}^{T}\Lambda\left(\nabla_{\bm{\pi}}f_{ t}(\bm{\pi}_{t})\right).\] (75)

Bounding the terms \(D_{\Phi}(\bm{\pi},\bm{\pi}_{t})-D_{\Phi}(\bm{\pi},\bm{\pi}_{t+1})\):

\[\sum_{t\in[T]}\left(D_{\Phi}(\bm{\pi}_{t}^{\star},\bm{\pi}_{t-1})-D _{\Phi}(\bm{\pi}_{t}^{\star},\bm{\pi}_{t})\right)\leq D_{\Phi}(\bm{\pi}_{1}^{ \star},\bm{\pi}_{0})+\sum_{t\in[T]}\left(D_{\Phi}(\bm{\pi}_{t+1}^{\star},\bm{ \pi}_{t})-D_{\Phi}(\bm{\pi}_{t}^{\star},\bm{\pi}_{t})\right)\] (76) \[\leq D_{\Phi,\max}^{2}+\sum_{t\in[T]}\underbrace{\left(\nabla\Phi (\bm{\pi}_{t+1}^{\star})-\nabla\Phi(\bm{\pi}_{t})\right)\cdot\left(\bm{\pi}_{t +1}^{\star}-\bm{\pi}_{t}^{\star}\right)}_{\text{Cauchy-Schwarz's Inequ}}- \underbrace{D_{\Phi}(\bm{\pi}_{t}^{\star},\bm{\pi}_{t+1}^{\star})}_{\geq 0}\] (77) \[\leq D_{\Phi,\max}^{2}+\sum_{t\in[T]}\underbrace{\left\|\nabla\Phi (\bm{\pi}_{t+1}^{\star})-\nabla\Phi(\bm{\pi}_{t})\right\|_{\star}}_{\leq L_{ \Phi}}\left\|\bm{\pi}_{t+1}^{\star}-\bm{\pi}_{t}^{\star}\right\|\] (78) \[\leq D_{\Phi,\max}^{2}+2L_{\Phi}\sum_{t\in[T]}\left\|\bm{\pi}_{t+ 1}^{\star}-\bm{\pi}_{t}^{\star}\right\|.\] (79)

Combine Eq. (75) and (79) to obtain \(\sum_{t=1}^{T}\bm{g}_{t}\cdot(\bm{\pi}_{t}-\bm{\pi})\leq\frac{1}{\eta}\left(D _{\Phi,\max}^{2}+2L_{\Phi}P_{T}\right)+\eta\sum_{t=1}^{T}\Lambda\left(\nabla_{ \bm{\pi}}f_{t}(\bm{\pi}_{t}),\bm{\pi}_{t}\right)\). Consider the bound on \(\Lambda\left(\nabla_{\bm{\pi}}f_{t}(\bm{\pi}_{t}),\bm{\pi}_{t}\right)\) in Lemma 8. We obtain

\[\sum_{t=1}^{T}\bm{g}_{t}\cdot(\bm{\pi}_{t}-\bm{\pi})\leq\frac{1}{\eta}\left(D_{ \Phi,\max}^{2}+2L_{\Phi}P_{T}\right)+\eta\sum_{t=1}^{T}\frac{\left\|\bm{g}_{t }\right\|_{\star}^{2}}{2\rho}\leq\frac{1}{\eta}\left(D_{\Phi,\max}^{2}+2L_{ \Phi}P_{T}\right)+\eta\left(\frac{L^{2}T}{2\rho}\right).\] (80)

Moreover, the following holds:

\[\left\|\bm{\pi}_{t+1}-\bm{\pi}_{t}\right\|_{1} \leq\sqrt{\frac{2}{\rho}D_{\Phi}(\bm{\pi}_{t},\bm{\pi}_{t+1})} \leq\sqrt{\frac{2}{\rho}D_{\Phi}(\bm{\pi}_{t},\bm{z}_{t+1})-\frac{2}{\rho}D_{ \Phi}(\bm{\pi}_{t+1},\bm{z}_{t+1})}\leq\sqrt{\frac{2}{\rho}D_{\Phi}(\bm{\pi}_{ t},\bm{z}_{t+1})}\] (81) \[\leq\sqrt{2\eta^{2}\frac{L^{2}}{2\rho^{2}}}\leq\frac{L\eta}{\rho}.\] (82)

The above chain of inequalities is obtained through: the strong convexity of \(\Phi\), the generalized Pythagorean equality (67), non-negativity of the Bregman divergence of a convex function, and Lemma 8, in respective order. Thus, it holds \(\sum_{t=1}^{T}\left\|\bm{\pi}_{t+1}-\bm{\pi}_{t}\right\|=\mathcal{O}\left(\eta T\right)\). We conclude the proof. 

### Entropic OMD Instantiation

**Definition 5**.: _The entropic OMD (57) is defined for the negative entropy mirror map_

\[\Phi(\bm{\pi}):\bm{\pi}\in\mathbb{R}_{>0}^{\mathcal{X}}\rightarrow\sum_{i=1}^{| \mathcal{X}|}\pi_{i}\log(\pi_{i}).\] (83)

The Entropic OMD algorithm is an attractive choice for simplex decision sets because its regret bounds exhibit better dependence on the problem dimension than those of OGD (Online Gradient Descent) [23]. Additionally, our restriction of the simplex decision set \(\left\langle\right\rangle_{\mathcal{X},\sigma}\subseteq\left\langle\right\rangle _{\mathcal{X}}\) allows us to extend Entropic OMD to the dynamic regret setting while preserving its aforementioned advantage. Formally,

**Proposition 4**.: _The Entropic OMD algorithm initialized with the state \(\bm{\pi}_{1}=\frac{\bm{1}}{|\mathcal{X}|}\) and configured over the decision set \(\left\langle\right\rangle_{\mathcal{X},\sigma}\) satisfies Assumptions (4), (5), and (6) with the following quantities:_

* _The map_ \(\Phi\) _is 1-strongly convex w.r.t. the_ \(\left\|\cdot\right\|_{1}\) _over the_ \(|\mathcal{X}|\)_-dimensional subset of the simplex_ \(\left\langle\right\rangle_{\mathcal{X},\sigma}\)_.__._
* _The map_ \(\Phi\) _has bounded gradient over_ \(\left\langle\raisebox{-1.0pt}{$\Diamond$}\chi,\sigma\right\rangle\) _given by_ \(L_{\Phi}=\left|\log(1/\sigma)+1\right|\)_._
* _The Bregman divergence_ \(D_{\Phi}(\boldsymbol{\pi},\boldsymbol{\pi}_{1})\) _associated to_ \(\Phi\) _is bounded over_ \(\mathcal{S}\) _by_ \(D_{\Phi,\max}=\log\left(\left|\mathcal{X}\right|\right)\)_._

Proof.: The negative entropy is 1-strongly convex over the simplex, a result that can be induced from Pinker's inequality. The gradient of the mirror map is given by \(\nabla\Phi(\boldsymbol{\pi})=\left(\log(\boldsymbol{\pi}_{\boldsymbol{x}})+1 \right)_{\boldsymbol{x}\in\mathcal{X}}\), which can take at most a value of \(L_{\Phi}=\log(1/\sigma)\) under the \(\left\|\cdot\right\|_{\infty}\) over \(\left\langle\raisebox{-1.0pt}{$\Diamond$}\chi,\sigma\right\rangle\). The Bregman divergence associated to \(\Phi\) is given by

\[D_{\Phi}(\boldsymbol{\pi},\boldsymbol{\pi}_{1})=\Phi(\boldsymbol{\pi})-\Phi( \boldsymbol{\pi}_{1})-\nabla\Phi(\boldsymbol{\pi}_{1})(\boldsymbol{\pi}- \boldsymbol{\pi}_{1})\leq\Phi(\boldsymbol{\pi})-\boldsymbol{\pi}(\boldsymbol {\pi}_{1})\leq-\Phi(\boldsymbol{\pi}_{1})=\log\left(\left|\mathcal{X}\right| \right).\]

The first inequality is obtained from first order optimality condition combined with the fact that \(\boldsymbol{\pi}_{1}=\operatorname*{argmin}_{\boldsymbol{x}\in\left\langle \raisebox{-1.0pt}{$\Diamond$}\chi,\sigma\right\rangle}\Phi(\boldsymbol{\pi})\). The second inequality is obtain considering that the map \(\Phi\) is non-positive for every \(\boldsymbol{\pi}\in\left\langle\raisebox{-1.0pt}{$\Diamond$}\chi,\sigma\right\rangle\). 

The same proposition evidently holds over the set \(\left\langle\raisebox{-1.0pt}{$\Diamond$}\tau,\sigma^{\prime}\right\rangle\) for \(\sigma^{\prime}\in[0,1]\).

### Entropic-VBR Policy

The Entropic-VBR policy is designed to operate in two distinct but interrelated settings, as outlined in Section 3. This policy is defined by a pair of mirror descent algorithms, each employing distinct gradients and decision sets. The first decision set, \(\left\langle\raisebox{-1.0pt}{$\Diamond$}\chi\right\rangle\), constructs distributions over the experiment space, while the second, \(\left\langle\raisebox{-1.0pt}{$\Diamond$}\tau\right\rangle\), generates distributions over the data freshness window space. The policy iteratively updates its decisions, initialized with uniform distributions \(\boldsymbol{\pi}_{1}=\boldsymbol{1}/|\mathcal{X}|\) and \(\boldsymbol{p}_{1}=\boldsymbol{1}/(\tau+1)\). At time step \(t\), the policy is refined according to the following procedure:

\[\pi_{t+1,\boldsymbol{x}} =\Pi_{\left\langle\raisebox{-1.0pt}{$\Diamond$}\chi,\sigma \right\rangle}\left(\pi_{\boldsymbol{x}}\exp\left(\eta_{\mathcal{X}}\left( \boldsymbol{x}_{t}^{\intercal}\left(\boldsymbol{V}((\tau+1)\boldsymbol{\pi}_{ t})\right)^{-1}\boldsymbol{x}\right)^{2}\right)\right), \text{for }\boldsymbol{x}\in\mathcal{X}\] (84) \[p_{t+1,\tau} =\Pi_{\left\langle\raisebox{-1.0pt}{$\Diamond$}\tau\cap[\sigma^ {\prime},1]\right\rangle^{\tau}}\left(p_{t,\tau}\exp\left(-\eta_{\mathcal{T}} \frac{\xi_{t}}{p_{\tau}}\mathds{1}\left(\tau=\tau_{t}\right)\right)\right), \text{for }\tau\in\mathcal{T},\]

where \(\tau_{t}\sim\boldsymbol{p}_{t}\) is sampled at every \(t\), \(\sigma\) is the regularization parameter, and \(\sigma^{\prime}\) is a tuneable parameter.

### Entropic Variance Reduction Policy

Given the above proposition we obtain the following regret bound for the variance-reduction policy:

**Corollary 2**.: _Under Assumptions 1-3, let \(\left\langle\raisebox{-1.0pt}{$\Diamond$}\chi,\sigma\right\rangle\) be the decision set, and let \(\left\{\boldsymbol{\pi}_{t}^{\star}\right\}_{t=1}^{T}\in\mathcal{C}(\left\langle \raisebox{-1.0pt}{$\Diamond$}\chi^{T}_{\mathcal{X},\sigma},P^{\star,\nu}_{T}\right\rangle\) be a comparator sequence. Consider the OMD (57) policy with the negative entropy mirror map (83) and learning rate \(\eta_{\mathcal{X}}=\Theta\left(\sqrt{\log(1/\sigma)P^{\star,v}_{T}T-1}\right)\). If the Entropic OMD is configured to run as a variance reduction policy \(\boldsymbol{\mathcal{L}}^{v}\) against the costs \(v_{1},\ldots,v_{t}\), then it has the following regret guarantee:_

\[\mathfrak{R}^{v}\left(\boldsymbol{\mathcal{L}}^{v}\right)=\mathcal{O}\left( \sqrt{\log(1/\sigma)P^{\star,v}_{T}T}\right).\] (85)

_The policy-induced decisions exhibit a path length (switching cost) of:_

\[\sum_{t=1}^{T}\left\|\boldsymbol{\pi}_{t+1}-\boldsymbol{\pi}_{t}\right\|= \mathcal{O}\left(\sqrt{P^{\star,v}_{T}T}\right).\] (86)

This is a direct result from Theorem 3.

### Entropic Bias Reduction Policy

In this section, we show that the bandit setting can be reduced to the full-information setting via unbiased gradient estimates. In particular, at time \(t\) an OMD (57) policy with the negative entropy mirror map (83) and state \(\boldsymbol{p}_{t}\), adapts its state according to the gradient estimate

\[\tilde{\boldsymbol{g}}_{t}=\frac{\xi_{t,\tau_{t}}}{p_{\tau_{t}}}\boldsymbol{e}_{ \tau_{t}},\] (87)

where \(\xi_{t,\tau_{t}}\) is the feedback for inference window size \(\tau_{t}\in\mathcal{T}\). The estimate satisfies the following:

[MISSING_PAGE_FAIL:20]

[MISSING_PAGE_FAIL:21]

where \(\Pi_{[0,5]^{2}}\) is a projection operator that ensures the model parameters remain within the interval \([0,5]\), \(\eta_{t}\) is a Rademacher random variable taking values from \(\{-1,1\}\). The evolution of the model parameters is depicted in Figure 2 (a). For our experiments, we set the experimental budget to \(M=100\), the data-freshness window size to \(\tau=10\), and the noise of the labels to a standard Gaussian scaled by 16.

Figure 3: Evolution of the policy-learned distribution over time, illustrating the non-trivial nature of the optimal window size.