[MISSING_PAGE_FAIL:1]

Bayesian deep learning (BDL) promises to fix this overconfidence problem by marginalizing over the posterior of the model's parameters. This process takes all explanations that are compatible with the training data into account. As desired, explanations will disagree on o.o.d. data, so that predictions will have low confidence in this regime. While computing the exact parameter posterior in BDL is infeasible, many approximate inference procedures exist to tackle this problem, aiming at making BDL applicable to real-world problems. Yet, recent BDL algorithms are typically only evaluated on the comparatively small and curated MNIST [52], UCI [16], and CIFAR [48] datasets with artificial o.o.d. splits. Existing BDL surveys [19; 20; 29; 72] concentrate on a few popular but relatively old algorithms such as Bayes By Backprop, Deep Ensembles, and Monte Carlo Dropout. In the light of recent calls for more realistic benchmarks of state-of-the-art (SOTA) algorithms [1] - with some experts going as far as calling the current state of BDL a "replication crisis"2 - we aim to provide a large-scale evaluation of recent BDL algorithms on complex tasks with large, diverse neural networks.

Contributions.**i)** We systematically evaluate a comprehensive selection of modern, scalable BDL algorithms on large image- and text-based classification and regression datasets from the WILDS collection [47] that originate from real-world, safety-critical applications of deep learning (Section 5). In the spirit of Ovadia et al. [72], we focus on generalization capability and calibration on o.o.d. data, but consider more diverse and modern algorithms (Section 3) on realistic datasets with distribution shift. In particular, we include recent advances in variational inference such as natural gradient descent (iVON [53]) and low-rank posterior approximations (Rank-1 VI [17]). Furthermore, we use modern neural network architectures such as various ResNets [32], a DenseNet [35], and a transformer architecture [87]. **ii)** We present the first systematic evaluation of BDL for finetuning large pre-trained models, a setting that has recently gained attention in the context of BDL [79; 81]. We show that using BDL for finetuning gives a significant performance boost across a wide variety of tasks compared to standard deterministic finetuning (Section 5). **iii)** Inspired by the success of Deep Ensembles [51], we systematically evaluate the benefit of ensembling single-mode posterior approximations [4] (Section 5). **iv)** We use a signed extension of the expected calibration error (ECE) called the signed expected calibration error (sECE) that can differentiate between overconfidence and underconfidence, allowing us to better understand in which ways models are miscalibrated (Section 4). **v)** We compare the posterior approximation quality of the considered algorithms using the HMC samples from Izmailov et al. [37] (Section 5) and show that modern single-mode BDL algorithms approximate the parameter posterior better than Deep Ensembles, with further gains being achieved by ensembling these algorithms. Overall, our work is similar in spirit to Band et al. [4], but we compare the algorithms on more diverse datasets and focus on pure calibration metrics, thereby revealing failure modes of SOTA BDL algorithms that are not yet present in the literature. We provide code for all implemented algorithms and all evaluations3.

Footnote 2: https://nips.cc/Conferences/2021/Schedule?showEvent=21827

Footnote 3: https://github.com/Feuermagier/Beyond_Deep_Ensembles

## 2 Related Work

Several recent publications [1; 25] review the SOTA in uncertainty quantification using Bayesian models without providing experimental results. Yao et al. [93] compare a wide range of Markov Chain Monte Carlo [31] and approximate inference [6] methods on toy classification and regression datasets. Ovadia et al. [72] perform a large-scale experimental evaluation of a small selection of popular BDL algorithms on o.o.d. data and conclude that Deep Ensembles [51] perform best while stochastic variational inference [26] performs worst. Filos et al. [19] use a similar selection of algorithms but only evaluate on a single, large computer vision task not considering o.o.d. data. Foong et al. [20] artificially create o.o.d. splits for UCI datasets [16] and again find that variational inference performs worse than the Laplace approximation [57]. Mukhoti and Gal [63] and Gustafsson et al. [29] compare Monte Carlo Dropout [22] and Deep Ensembles in the context of semantic segmentation and depth completion, but, again, do not consider o.o.d. data. Nado et al. [65] evaluate many popular BDL algorithms on a small number of mostly artificially created o.o.d. image and text classification tasks. The work of Band et al. [4] is the most similar to ours, as they evaluate several BDL algorithms, including ensembles of single-mode posterior approximations, on two large image-classification datasets and consider o.o.d. data. Compared to Band et al. [4], we evaluate a different set of algorithms such as SWAG [59] and natural gradient descent variational inference [53]on a more diverse selection of datasets and network architectures, including transformer-based models and finetuning tasks, thereby revealing new failure modes of SOTA BDL methods. Competitions such as Wilson et al. [90] and Malinin et al. [60] also provide insights into the performance of different algorithms. However, the employed algorithms are typically highly tuned and modified for the specific tasks and thus of limited use to assess the general quality of the underlying methods in more diverse settings. Importantly, all winners of Wilson et al. [90] use ensemble-based algorithms.

## 3 Bayesian Deep Learning Algorithms

We assume a neural network with parameters \(\bm{\theta}\) that models the likelihood \(p(\bm{y}\mid\bm{x},\bm{\theta})\) of an output \(\bm{y}\) given an input \(\bm{x}\). Treating \(\bm{\theta}\) as a random variable, the parameter posterior \(p(\bm{\theta}\mid\mathcal{D})\) given a training dataset \(\mathcal{D}=\{(\bm{x}_{i},\bm{y}_{i})\mid i=1,\ldots,N\}\) of input-output pairs is defined by Bayes' theorem as

\[p(\bm{\theta}\mid\mathcal{D})=\frac{\prod_{i}p(\bm{y}_{i}\mid\bm{x}_{i},\bm{ \theta})\;p(\bm{\theta})}{\int\prod_{i}p(\bm{y}_{i}\mid\bm{x}_{i},\bm{\theta })\;p(\bm{\theta})\,\mathrm{d}\bm{\theta}},\] (1)

where \(p(\bm{\theta})\) is a prior over parameters. The posterior \(p(\bm{\theta}\mid\mathcal{D})\) assigns higher probability to parameter vectors that fit the training data well and conform to our prior beliefs. Using \(p(\bm{\theta}\mid\mathcal{D})\), a prediction \(\bm{y}\) given an input vector \(\bm{x}\) is defined as

\[p(\bm{y}\mid\bm{x},\mathcal{D})=\int p(\bm{y}\mid\bm{x},\bm{\theta})\;p(\bm{ \theta}\mid\mathcal{D})\;\mathrm{d}\bm{\theta}=\mathop{\mathbb{E}}_{\bm{ \theta}\sim p(\bm{\theta}\mid\mathcal{D})}\left[p(\bm{y}\mid\bm{x},\bm{\theta })\right].\] (2)

This so-called Bayesian model average (BMA) [6; 58; 64; 89] encompasses the information of all explanations of the training data that are consistent with the parameter posterior. The BMA is especially valuable when dealing with large neural networks that are typically underspecified by the training data, where marginalizing over parameters can mitigate overfitting and promises significant accuracy and calibration gains [89]. While recent work has shown problems with certain types of covariate shift [36] and o.o.d. data [11], BDL not only promises calibration but also generalization gains [89].

### Scalable Approximations for Bayesian Deep Learning

As computing the marginalization integral defining the normalization constant of the parameter posterior (Equation (1)) is intractable for neural networks, we have to resort to approximations. Approximate inference algorithms approximate the posterior, either by sampling from it or by computing an approximate distribution. Sampling-based Markov Chain Monte Carlo (MCMC) methods [31] such as Hamiltonian Monte Carlo (HMC) [67] sample directly from the true posterior and are therefore asymptotically exact. However, they are computationally very expensive and hence typically intractable in the context of BDL. Deterministic methods such as variational inference construct local approximations at a mode of the parameter posterior and are generally more computationally performant than MCMC [37], as they transform the posterior inference problem into an optimization problem that can be efficiently solved with standard gradient-based optimization techniques [6; 58; 64]. Therefore, we focus on these algorithms in this work. This framework also encompasses standard deep learning, which is equivalent to a "Maximum A Posteriori" (MAP) estimate, i.e., a point estimate at the posterior maximum. In this section, we give a brief overview of the algorithms that we evaluate. See Appendix A for more detailed explanations and Appendix D for implementation details.

Figure 1: Posterior Approximation Types. MAP approximates a single posterior mode with a point estimate, while probabilistic single-mode approximations additionally capture the shape of the mode. Deep Ensembles approximate multiple modes with a mixture of point estimates. Likewise, MultiX employs a mixture of single-mode approximations to capture the shape of multiple modes. Figure adapted from Wilson and Izmailov [89].

Variational Inference.Variational inference (VI) minimizes the Kullback-Leibler divergence [50] between the approximate posterior and the true posterior [26]. Bayes By Backprop (**BBB**) [7] approximates the posterior with a diagonal Gaussian distribution and optimizes the mean and variance parameters with Stochastic Gradient Descent (SGD) [42]. Rank-1 variational inference (**Rank-1 VI**) [17] in contrast uses a low-rank posterior approximation, which reduces the number of additional parameters and allows the use of multiple components in the low-rank subspace. The improved Variational Online Newton (**iVON**) algorithm [53] still uses a diagonal Gaussian posterior but uses second-order information to better optimize the distribution parameters with natural gradients. Stein Variational Gradient Descent (**SVGD**) [55] is a non-parametric VI algorithm that approximates the posterior with multiple point estimates. SVGD is similar to a Deep Ensemble (see below) but adds repulsive forces between the particles to push them away from each other in parameter space.

Other Algorithms.Lakshminarayanan et al. [51] introduce **Deep Ensembles** that approximate the posterior with a few, typically five to ten, independently trained MAP models. As such, Deep Ensembles were originally considered a competing approach to Bayesian models [51] but can be viewed as Bayesian as they form a sum of delta distributions that approximate the posterior [89]. We follow this interpretation. The **Laplace** approximation [57] approximates the posterior with a second-order Taylor expansion around the parameters of a MAP model. We only consider the last-layer Laplace approximation [13] with diagonal and Kronecker-factorized [74] posterior approximations, which Daxberger et al. [13] find to achieve the best tradeoff between performance and calibration. Monte Carlo Dropout (**MCD**) [22] utilizes the probabilistic nature of dropout units that are part of many common network architectures to construct an approximation of a posterior mode. Stochastic Weight Averaging-Gaussian (**SWAG**) [59] periodically stores the parameters during SGD training and uses them to build a low-rank Gaussian posterior approximation. Finally, we evaluate Spectrally-Normalized Gaussian Processes (**SNGP**) [54] as a Bayesian baseline that does not infer a distribution over the model's parameters, but replaces the last layer by a Gaussian Process. The results for SNGP are therefore not directly comparable to the other algorithms' results.

### MultiX

While single-mode posterior approximations such as BBB and SWAG capture the shape of a single mode of the parameter posterior, Deep Ensembles cover multiple modes but approximate each with a single point estimate. Hence, ensembling single-mode approximations promises even better posterior coverage and therefore improved uncertainty estimates (see Figure 1). This concept is not new: Tomczak et al. [86] experiment with an ensemble of BBB models on small datasets. Cobb et al. [10] use an ensemble of Concrete Dropout [23] models and Filos et al. [19] use MCD models. Both report accuracy improvements compared to a Deep Ensemble. Wilson and Izmailov [89] introduce MultiSWAG, an ensemble of SWAG [59] models. The winning teams of Wilson et al. [90] also show that ensembling Bayesian neural networks yields good posterior approximations. Similar to Band et al. [4] and Mehrtens et al. [61], we ensemble all considered single-mode posterior approximations (Section 3.1) except for SNGP to assess the performance gains on a per-algorithm basis. We use the term "MultiX" to refer to an ensemble of models trained with algorithm "X". We make an exception for "MultiMAP", which we keep referring to as Deep Ensemble for consistency with the existing literature.

## 4 Calibration Metrics

A calibrated model is defined as a model that makes confident predictions if and only if they will likely be accurate. While this definition directly implies a calibration metric for classification tasks [66], it has to be adapted for regression tasks, as "being accurate" is not a binary property in the regression case.

### Unsigned Calibration metrics

Calibrated Classification.The calibration of a _classification_ model can be measured with the expected calibration error (ECE) [28; 66]. By partitioning the interval \([0,1]\) into \(M\) equally spaced bins and grouping the model's predictions into those bins based on their confidence values, we can calculate the average accuracy and confidence of each bin. The expected calibration error is then given by \(\text{ECE}=\sum_{m=1}^{M}|B_{m}|/|\mathcal{D}^{\prime}|\text{acc}(B_{m})-\text{ conf}(B_{m})|\) where \(B_{m}\) is the set of predictions in the \(m\)-th bin, and \(\text{acc}(B_{m})\) and \(\text{conf}(B_{m})\) are the average accuracy and confidence of the predictions in \(B_{m}\) (see Appendix B for details). An ECE of zero indicates perfect calibration.

Calibrated Regression.The confidence intervals of the predictive distribution can be used to measure the calibration of a _regression_ model. Selecting \(M\) confidence levels \(\rho_{m}\) allows the computation of a calibration error based on the observed probability \(p_{\text{obs}}(\rho_{m})\), calculated as the fraction of predictions that fall into the \(\rho_{m}\)-confidence interval of their respective predictive distributions: \(\text{QCE}=\nicefrac{{1}}{{M}}\sum_{m=1}^{M}|p_{\text{obs}}(\rho_{m})-\rho_{m}|\). We refer to this as the quantile calibration error (QCE), which simply replaces the quantiles in the definition of the calibration error from Kuleshov et al. [49] by confidence intervals. Using the confidence intervals allows a simpler interpretation of the resulting reliability diagrams (see Appendix B).

### Signed Calibration Metrics

Models can be miscalibrated in two distinct ways: Overconfident models make inaccurate predictions with high confidence, and underconfident models make accurate predictions with low confidence. Arguably, overconfidence is worse in practice when applicants want to rely on the model's confidence to assess whether they can trust a prediction, for example in safety-critical applications of deep learning. However, none of the presented metrics can differentiate between overconfidence and underconfidence. Until now, this information was only apparent in reliability diagrams [28]. We propose two simple extensions of the ECE and the QCE that condense the information about overconfidence and underconfidence into a single scalar value by removing the absolute values: sECE and sQCE. We define these signed calibration metrics as

\[\text{sECE}=\sum_{m=1}^{M}\frac{|B_{m}|}{|\mathcal{D}^{\prime}|}\big{(}\text{ acc}(B_{m})-\text{conf}(B_{m})\big{)}\quad\text{and}\quad\text{sQCE}=\frac{1}{M} \sum_{m=1}^{M}\big{(}p_{\text{obs}}(\rho_{m})-\rho_{m}\big{)}.\] (3)

A positive signed calibration error indicates that a model makes predominantly underconfident predictions and a negative signed calibration error indicates predominantly overconfident predictions. Perfectly calibrated models have a sECE/sQCE of zero. For models that are overconfident for some inputs but underconfident for others the signed calibration metrics may be zero, even though the model is not perfectly calibrated. This is typically not an issue in practice, as our experiments in Appendix C show that the absolute value of the signed metrics is usually very close to the absolute value of the corresponding unsigned metric, as most models are either overconfident or underconfident for nearly all predictions. Nevertheless, we always report the signed calibration metrics together with the unsigned calibration metrics to avoid any ambiguity.

## 5 Empirical Evaluation

For our comparison of the BDL algorithms introduced in Section 3.1, we focus on i) the ability of the models to generalize to realistic distribution-shifted data, **ii)** the calibration of the models under distribution shift, and **iii)** how well the models approximate the true parameter posterior. To assess the generalization capability and calibration of the models under realistic distribution shift, we use a subset of the WILDS dataset collection [47]. We assess the posterior approximation quality by comparing the model's predictive distributions to those of the HMC approximation provided by Izmailov et al. [37] for CIFAR-10 [48]. See Section 5.1 and Section 5.2 for details about the task, Section 5.3 for generalization results, Section 5.4 for calibration results, and Section 5.5 for posterior approximation quality results. All results for WILDS are directly comparable to the respective non-BDL o.o.d. detection algorithms on the WILDS leaderboard4, since we strictly follow their training and evaluation protocol. Whenever the BDL models' accuracy is competitive with the best performing algorithm on the WILDS leaderboard, the WILDS result is marked in the respective plot. We also report results for a subset of the smaller UCI [16] and UCI-Gap [20] tabular regression datasets in Appendix G.1. Appendix F contains information about the used computational resources and training times. Details regarding hyperparameters, training procedures, and additional results can be found in Appendix G. The results on all datasets are reported with a \(95\%\) confidence interval.

Footnote 4: https://wilds.stanford.edu/leaderboard/, last accessed on August 31, 2023

### The WILDS Datasets

WILDS consists of ten diverse datasets that originate from real-world applications of deep learning in which models need to perform well under distribution shift. Standard o.o.d. datasets such as MNIST-C [62], CIFAR-10-C [33] and UCI-Gap [20] create distribution-shifted data by selectively removing data from the training split or artificially adding data corruptions onto the data in the evaluation split. WILDS represents real-world distribution shifts and is therefore more suitable for an application-oriented evaluation of BDL. We systematically evaluate all considered algorithms (see Section 3.1) on six of the ten datasets: The image-based regression task PovertyMap, the image classification tasks iWildCam, FMoW, and RxRx1, and the text classification tasks CivilComments and Amazon. Aside from PovertyMap, all datasets are finetuning tasks, where we initialize the model's parameters from a model that has been pre-trained on a similar task. We also evaluate some algorithms on the Camelyon17 image classification dataset but find that the performance degradation on the o.o.d. evaluation split is to a large part a consequence of the use of batch normalization rather than the o.o.d. data, making the dataset less interesting for a fair comparison on o.o.d. data (see Appendix E for details).

As we want to evaluate the posterior approximation, generalization, and calibration capability of all models given the true parameter posterior, none of our models use the metadata (e.g. location, time) associated with the input data, nor do we consider approaches that are specifically designed for o.o.d. generalization or augment the dataset, for example by re-weighting underrepresented classes, contrary to the algorithms evaluated by Koh et al. [47].

Large-Scale Regression.**PovertyMap-wilds**[94] is an image-based regression task, where the goal is to better target humanitarian aid in Africa by estimating the asset wealth index of an area using satellite images. As the task is significantly easier when buildings are visible in the images, the evaluation set is split into images containing urban and images containing rural areas. The accuracy of the models is evaluated on both splits by the Pearson coefficient between their predictions and the ground truth, and the worst Pearson coefficient is used as the main evaluation metric. All models are based on a ResNet-18 [32]. See Figure 3 for the Pearson coefficient and sQCE on the o.o.d. evaluation split and Appendix G.3.2 for further details.

Finetuning of CNNs.**iWildCam-wilds**[5] is an image classification task that consists of animal photos taken by camera traps across the world. The model's task is to determine which of 182 animal species can be seen in the image. As rare animal species, which are of special interest to researchers, are naturally underrepresented in the dataset, the macro F1 score is used to evaluate the predictive performance. The o.o.d. evaluation split consists of images from new camera locations. All models are based on a ResNet-50 [32]. See Figure 2 for the macro F1 score and sECE on the o.o.d. evaluation split and Appendix G.3.3 for further details. **FMoW-wilds** (Functional Map of the World) [9] is an image classification task, where the inputs are satellite images and the class is one of \(62\) building and land use categories. The o.o.d. evaluation split consists of images from different years than the images in the training set. Models are separately evaluated on five geographical regions of the world, with the lowest accuracy taken as the main evaluation metric. All models are based on a DenseNet-121 [35]. See Figure 2 for the accuracy and sECE for the region of the o.o.d. evaluation split the models perform worst on and Appendix G.3.4 for further details. **RxRx1-wilds**[85] is an image classification task, where the inputs are three-channel images of cells, and the classes are 1139 applied genetic treatments. The o.o.d. evaluation split is formed by images from different experimental batches than the training data. Following Koh et al. [47], we only use three of the six available input channels to limit the computational complexity of the models. This makes the task considerably harder and leads to the low accuracy of the models, but makes our results comparable to those of Koh et al. [47]. All models are based on a ResNet-50 [32]. See Figure 2 for the accuracy and sECE on the o.o.d. evaluation split and Appendix G.3.5 for further details.

Finetuning of Transformers.**CivilComments-wilds**[8] is a binary text classification dataset, where the model's task is to classify whether a given comment is toxic or not. The comments are grouped based on whether they mention certain demographic groups, such as LGBTQ or Muslim identities. Models are evaluated based on the group on which they achieve the lowest accuracy on the evaluation set. All models are based on the DistilBERT architecture [76]. See Figure 4 for the accuracy and sECE on the group of the o.o.d. evaluation split the models perform worst on and Appendix G.3.6 for further details. **Amazon-wilds**[68] consists of textual product reviews, wherethe task is to predict the star rating from one to five. The o.o.d. evaluation split consists of reviews from reviewers that are not part of the training split. Models are evaluated based on the accuracy of the reviewer at the \(10\%\) quantile. All models are based on DistilBERT [76]. See Figure 3(b) for the accuracy and sECE on the o.o.d. evaluation split and Appendix G.3.7 for further details.

### The Corrupted CIFAR-10 Dataset

CIFAR-10-C [33] is a corrupted version of the evaluation split of the image classification dataset CIFAR-10 [48], where images are corrupted with increasing levels of noise, blur, and weather and digital artifacts. We compare the considered algorithms on the standard evaluation split of CIFAR-10 as well as the corruption levels \(1\), \(3\), and \(5\) of CIFAR-10-C. Following Izmailov et al. [37], all of our models on CIFAR-10-(C) are based on the ResNet-20 architecture. See Appendix G.3.6 for details.

### Generalization to Realistic Distribution Shift

We measure the generalization capability of the models with the task-specific accuracy metrics proposed by Koh et al. [47] that are based on the real-world origin of the respective tasks. The metrics typically emphasize the performance on groups or classes that are underrepresented in the training data, as avoiding bias against these groups is crucial in safety-critical applications of BDL.

Except for the text classification tasks, MultiX always generalizes better than single-mode posterior approximations. Overall, the relative ordering of the MultiX models depends on the dataset and in many cases does not correlate with the relative ordering of the corresponding single-mode approximations.

Large-Scale Regression.All models achieve similar Pearson coefficients, with MultiX being slightly more accurate. The Deep Ensemble is competitive with the best performing algorithm of the WILDS leaderboard with a Pearson coefficient of \(0.52\) compared to \(0.53\) of C-Mixup [92]. However, due to the large standard errors resulting from the different difficulties of the folds, the results are not significant. Note that Koh et al. [47] report similarly large standard errors.

Finetuning of CNNs.Confirming the overall trend, MultiX models generalize better than single-mode models, with MultiSWAG and MultiMCD performing particularly well. Except for RxRx1 the

Figure 2: Accuracy Metrics vs. sECE on the o.o.d. evaluation splits of the image classification finetuning tasks iWildCam-wilds, FMoW-wilds, and RxRx1-wilds. Note the split y-axis in Figure 1(c). All MultiX algorithms are more accurate and better calibrated than any single-mode approximation. Except for RxRx1, BBB is better calibrated than MCD and SWAG. iVON’s calibration is inconsistent: On iWildCam it is better calibrated than BBB, but on FMoW it barely performs better than MAP. On RxRx1, the VI algorithms except for SVGD are significantly less accurate than all other models. We experimented with different hyperparameters in Appendix G.3.5. Laplace is very well calibrated on iWildCam, but underperforms on FMoW and RxRx1. SVGD performs very similarly to MAP regarding both metrics, even though it uses a multi-mode posterior approximation.

models perform competitively with the best models from the WILDS leaderboard. On iWildCam, the single-mode posterior approximations SWAG and MCD are competitive with the Deep Ensemble. SVGD performs similarly to MAP, even though it is based on an ensemble, likely due to the repulsive forces pushing the particles away from the well-performing pre-trained model. While the VI algorithms' accuracy is similar to the accuracy of MAP on iWildCam and FMoW, all VI algorithms except SVGD perform significantly worse than the non-VI algorithms on RxRx1. Laplace is well calibrated on iWildCam, but significantly less accurate than the other algorithms on FMoW and RxRx1. This seems to represent a fundamental approximation failure of Laplace, and not only a sampling issue, since increasing the number of samples lead to only a small increase in accuracy (see Appendix G.3.4 and Appendix G.3.5).

Finetuning of Transformers.BBB and Rank-1 VI are the most accurate models on both tasks, with no benefit from the multiple components of Rank-1 VI. Interestingly, iVON is significantly less accurate than BBB, even though it is also based on mean-field VI, indicating that the natural gradient-based training is disadvantageous on the transformer-based BERT architecture. To see whether the better performance of BBB is due to less regularization compared to MAP, we also experiment with a smaller weight decay factor for MAP on CivilComments. While we find that the accuracy increases, BBB is still more accurate (see Appendix G.3.6). Finally, we also check whether the better performance of BBB is due to its last-layer nature. We experiment with last-layer versions of MCD and SWAG on Amazon (see Appendix G.3.7), but find that both are still significantly less accurate than BBB.

MultiX is no more accurate than the corresponding single-mode approximation, contrary to the results on all other datasets. We suspect that this effect is to a large part due to the finetuning nature of the tasks, where all ensemble members start close to each other in parameter space and therefore converge to the same posterior mode. Note that the failure of ensembles is most likely due to the task and the network architecture and not due to the training procedure: While we train for fewer epochs than on the image classification tasks, the datasets are larger. On iWildCam we perform 97k parameter updates, compared to 84k parameter updates on CivilComments.

### Calibration under Realistic Distribution Shift

We measure calibration with the sECE for classification tasks and with the sQCE for regression tasks (see Section 4). We additionally report the unsigned ECE/QCE and the log-likelihood for the regression task in Appendix G.

MultiX is almost always less overconfident than single-mode approximations. When all models are already comparatively well calibrated, MultiX tends to become underconfident. Thus, we find that MultiX is typically only less confident, but not automatically better calibrated than single-mode approximations. On the transformer-based text classification tasks, MultiX is almost never better calibrated than the respective single-mode approximation.

Figure 3: PovertyMap-wilds: Worst urban/rural Pearson coefficient between the model’s predictions and the ground truth plotted against the sQCE on the o.o.d. test split of the image-based regression task. All models achieve similar, but noisy [47], Pearson coefficients, indicating similar generalization capabilities. Multi-mode approximations are consistently better calibrated than single-mode approximations (note that Rank-1 VI’s components and SVGD’s particles give them multi-mode approximation capabilities). Regarding calibration, the relative ordering of the single-mode models does not translate to the MultiX models: BBB is among the best-calibrated single-mode models, but MultiBBB is the worst calibrated MultiX model. Laplace and SWAG are very similarly calibrated, therefore the data points of SWAG are hidden behind the data points of Laplace. iVON performs significantly worse than the other algorithms and is therefore excluded.

Large-Scale Regression.MultiX, when based on a probabilistic single-mode approximation, is generally better calibrated than the Deep Ensemble. SVGD is better calibrated than the Deep Ensemble, showing the benefit of the repulsive forces between the particles. Rank-1 VI is the best calibrated model, indicating that multi-modality over all parameters as with the Deep Ensemble is not necessary to capture the multi-modality of the parameter posterior.

Finetuning of CNNs.Again, we find that MultiX generally performs better than a Deep Ensemble. However, MultiSWAG in particular is more overconfident than the Deep Ensemble, even though SWAG is better calibrated than MAP. BBB is better calibrated than other single-mode approximations such as SWAG and MCD. Rank-1 VI performs similar to BBB on all tasks, indicating that the low-rank components are not sufficient to capture the multi-modality of the parameter posterior in the finetuning setting. On iWildCam, Laplace is the best calibrated single-mode approximation, and correspondingly Multi-Laplace is the most underconfident multi-mode approximation. This result is unique to iWildCam, as Laplace tends to be overconfident on the other image classification datasets.

Finetuning of Transformers.Except for MultiBBB on Amazon, ensembles are similarly calibrated than the respective single-mode approximations. SWAG is the least confident model on both tasks, which leads to underconfidence on Amazon. MCD's calibration is inconclusive, as it is better calibrated than MAP on Amazon, but more overconfident on CivilComments. BBB and Rank-1 VI are not better calibrated than MAP and on Amazon significantly more overconfident than MAP.

### Posterior Approximation Quality

While approximate inference is commonplace in BDL, the large size of the neural networks typically makes it computationally intractable to measure how well a model approximates the true parameter posterior. Following Wilson et al. [90] and using the HMC samples provided by Izmailov et al. [37], we measure how well the models approximate the predictive distribution of HMC by the total variation (TV) between the model's predictions and HMC and the top-1 agreement with HMC on CIFAR-10-(C) [33; 48]. While the TV in the predictive space is indicative of the parameter posterior approximation quality, it does not allow for definite conclusions about the parameter space. Figure 5 displays the TV of the evaluated models under increasing levels of image corruption. For further results regarding the accuracy, sECE, ECE, and top-1 agreement with HMC see Appendix G.2.

Overall, a good probabilistic single-mode approximation is the most important factor for a good posterior approximation. MultiX, when based on probabilistic single-mode approximations, consistently approximates the parameter posterior better than single-mode-only approximations and the Deep Ensemble. MultiVON approximates the posterior best across all corruption levels as measured by the TV, with MultiSWAG being a close contender. Even single-mode approximations such as MCD and SWAG achieve better TVs

Figure 4: Text classification with pretrained transformers. Except for MultiBBB on Amazon-wilds, MultiX performs nearly identically to the corresponding single-mode approximation. VI improves the accuracy of the models. MCD is the least accurate model on CivilComments. We experiment with different dropout rates in Appendix G.3.6 but find that MCD never outperforms MAP.

under data corruption than the Deep Ensemble. As expected, MAP has the highest TV, with only a small improvement made by Laplace.

## 6 Conclusion

We presented a comprehensive evaluation of a wide range of modern, scalable BDL algorithms, using distribution-shifted data based on real-world applications of deep learning. We focused on the generalization capability, calibration, and posterior approximation quality under distribution shift. Overall, our analysis resulted in the following takeaway messages:

1. Finetuning only the last layers of pre-trained models with BDL algorithms gives a significant boost of generalization accuracy and calibration on realistic distribution-shifted data, while incurring a comparatively small runtime overhead. These models are in many cases competitive to or even outperform methods that are specially designed for OOD generalization such as IRM [3] and Fish [80].
2. For CNNs, ensembles are more accurate and better calibrated on OOD data than single-mode posterior approximations by a wide margin, even when initializing all ensemble members from the same pre-trained checkpoint with only the last layers differently initialized, i.e. when not using the standard protocol of randomly initializing all ensemble members. Ensembling probabilistic single-mode posterior approximations such as SWAG or MCD yields only a small additional increase in accuracy and calibration.
3. When finetuning large transformers, ensembles, which are typically considered to be the SOTA in BDL, yield no benefit. Compared to all other evaluated BDL algorithms, classical mean-field variational inference achieves significant accuracy gains under distribution shift.

Limitations.While we evaluate on a wide range of datasets from different domains and using different network architectures, the choice of tasks is still limited. In particular, we do not consider LSTMs [34] as Ovadia et al. [72] do. Given the limitations of WILDS [47], we evaluate on a single large-scale regression dataset. As both text classification experiments use DistilBERT [76], it is conceivable that the failure of ensembles is limited to this particular architecture. We do not include algorithms that are based on function-space priors [30; 56; 75; 84]. Except for the results on CIFAR-10-(C) and PovertyMap, all results were obtained by finetuning pre-trained models and are therefore only valid in this setting. The HMC samples used in Section 5.5 have been criticized for not faithfully representing the true parameter posterior due to low agreement between the predictions of different chains [78].

Broader Context.Bayesian deep learning aims to provide reasonable uncertainty estimates in safety-critical applications. Hence, we do not expect any societal harm from our work, as long as it is ensured by proper evaluation that accuracy and calibration requirements are met before deployment.

Figure 5: CIFAR-10-(C): Total variation (TV, smaller is better) between the model’s predictions and HMC’s predictions on the evaluation split of CIFAR-10 (corruption \(0\)) and under increasing levels of image corruption by noise, blur, and weather and digital artifacts (corruption \(1,3,5\)). MultiX consistently approximates the posterior better than the corresponding single-mode approximations, with MultiVON and MultiSWAG achieving the smallest TV. The single-mode approximations SWAG, MCD, and iVON approximate the posterior better than the Deep Ensemble.

[MISSING_PAGE_FAIL:11]

* [24] Jacob R Gardner et al. "GPyTorch: Blackbox Matrix-Matrix Gaussian Process Inference with GPU Acceleration". In: _Advances in Neural Information Processing Systems_. 2018.
* [25] Jakob Gawlikowski et al. "A survey of uncertainty in deep neural networks". In: _arXiv preprint arXiv:2107.03342_ (2021).
* [26] Alex Graves. "Practical Variational Inference for Neural Networks". In: _Advances in Neural Information Processing Systems_. Vol. 24. Curran Associates, Inc., 2011.
* [27] Sorin Grigorescu et al. "A survey of deep learning techniques for autonomous driving". In: _Journal of Field Robotics_ 37 (Nov. 2019). doi: 10.1002/rob.21918.
* [28] Chuan Guo et al. "On Calibration of Modern Neural Networks". In: _Proceedings of the 34th International Conference on Machine Learning_. Vol. 70. PMLR, 2017, pp. 1321-1330.
* [29] Fredrik K Gustafsson, Martin Danelljan, and Thomas B Schon. "Evaluating Scalable Bayesian Deep Learning Methods for Robust Computer Vision". In: _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops_. 2020, pp. 318-319.
* [30] Danjar Hafner et al. "Noise Contrastive Priors for Functional Uncertainty". In: _arXiv preprint arXiv:1807.09289_ (2018).
* [31] W. K. Hastings. "Monte Carlo Sampling Methods Using Markov Chains and Their Applications". In: _Biometrika_ 57.1 (1970), pp. 97-109.
* [32] Kaiming He et al. "Deep Residual Learning for Image Recognition". In: June 2016, pp. 770-778.
* [33] Dan Hendrycks and Thomas Dietterich. "Benchmarking Neural Network Robustness to Common Corruptions and Perturbations". In: _arXiv preprint arXiv:1903.12261_ (2019).
* [34] Sepp Hochreiter and Jurgen Schmidhuber. "Long Short-term Memory". In: _Neural computation_ 9 (Dec. 1997), pp. 1735-80.
* [35] G. Huang et al. "Densely Connected Convolutional Networks". In: _2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_. IEEE Computer Society, 2017, pp. 2261-2269.
* [36] Pavel Izmailov et al. "Dangers of Bayesian model averaging under covariate shift". In: _Advances in Neural Information Processing Systems_ 34 (2021), pp. 3309-3322.
* [37] Pavel Izmailov et al. "What Are Bayesian Neural Network Posteriors Really Like?" In: _Proceedings of the 38th International Conference on Machine Learning_. Vol. 139. PMLR, 2021, pp. 4629-4640.
* [38] Alex Kendall and Yarin Gal. "What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision?" In: _Proceedings of the 31st International Conference on Neural Information Processing Systems_. NIPS'17. Curran Associates Inc., 2017, pp. 5580-5590.
* [39] Mohammad Emtiyaz Khan and Havard Rue. "The Bayesian Learning Rule". In: _arXiv preprint arXiv:2107.04562_ (2021).
* [40] Mohammad Emtiyaz Khan et al. "Fast and Scalable Bayesian Deep Learning by Weight-Perturbation in Adam". In: _ICML_. 2018.
* [41] Mohammad Emtiyaz Khan et al. "Faster Stochastic Variational Inference Using Proximal-Gradient Methods with General Divergence Functions". In: _Proceedings of the Thirty-Second Conference on Uncertainty in Artificial Intelligence_. UAI'16. AUAI Press, 2016, pp. 319-328.
* [42] J. Kiefer and J. Wolfowitz. "Stochastic Estimation of the Maximum of a Regression Function". In: _The Annals of Mathematical Statistics_ 23.3 (1952), pp. 462-466.
* [43] Diederik Kingma and Jimmy Ba. "Adam: A Method for Stochastic Optimization". In: _International Conference on Learning Representations_ (Dec. 2014).
* [44] Diederik P. Kingma and Max Welling. "Auto-Encoding Variational Bayes". In: _CoRR_ abs/1312.6114 (2014).
* [45] Durk P Kingma, Tim Salimans, and Max Welling. "Variational Dropout and the Local Reparameterization Trick". In: _Advances in Neural Information Processing Systems_. Vol. 28. Curran Associates, Inc., 2015.
* [46] Pang Wei Koh et al. _WILDS Leaderboard_. 2020. url: https://wilds.stanford.edu/leaderboard/ (visited on 05/08/2023).
* [47] Pang Wei Koh et al. "WILDS: A Benchmark of in-the-Wild Distribution Shifts". In: _Proceedings of the 38th International Conference on Machine Learning_. Vol. 139. PMLR, 2021, pp. 5637-5664.

* [48] Alex Krizhevsky. _Learning multiple layers of features from tiny images_. Tech. rep. 2009.
* [49] Volodymyr Kuleshov, Nathan Fenner, and Stefano Ermon. "Accurate Uncertainties for Deep Learning Using Calibrated Regression". In: _Proceedings of the 35th International Conference on Machine Learning_. Vol. 80. PMLR, 2018, pp. 2796-2804.
* [50] S. Kullback and R. A. Leibler. "On Information and Sufficiency". In: _Ann. Math. Statist._ 22.1 (1951), pp. 79-86.
* [51] Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. "Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles". In: _Proceedings of the 31st International Conference on Neural Information Processing Systems_. NIPS'17. Curran Associates Inc., 2017, pp. 6405-6416.
* [52] Y. LeCun et al. "Gradient-based learning applied to document recognition". In: _Proceedings of the IEEE_ 86.11 (1998), pp. 2278-2324.
* [53] Wu Lin, Mark Schmidt, and Mohammad Emtiyaz Khan. "Handling the Positive-Definite Constraint in the Bayesian Learning Rule". In: _Proceedings of the 37th International Conference on Machine Learning_. ICML'20. JMLR.org, 2020.
* [54] Jeremiah Zhe Liu et al. "Simple and Principled Uncertainty Estimation with Deterministic Deep Learning via Distance Awareness". In: NIPS'20. Curran Associates Inc., 2020.
* [55] Qiang Liu and Dilin Wang. "Stein Variational Gradient Descent: A General Purpose Bayesian Inference Algorithm". In: _Advances in Neural Information Processing Systems_. Vol. 29. Curran Associates, Inc., 2016.
* [56] Chao Ma and Jose Miguel Hernandez-Lobato. "Functional Variational Inference based on Stochastic Process Generators". In: _Advances in Neural Information Processing Systems_. Vol. 34. 2021, pp. 21795-21807.
* [57] David J. C. MacKay. "A Practical Bayesian Framework for Backpropagation Networks". In: _Neural Comput._ 4.3 (1992), pp. 448-472.
* [58] David J. C. MacKay. "Bayesian Methods for Adaptive Models". PhD thesis. California Institute of Technology, 1992.
* [59] Wesley J Maddox et al. "A Simple Baseline for Bayesian Uncertainty in Deep Learning". In: _Advances in Neural Information Processing Systems_. Vol. 32. Curran Associates, Inc., 2019.
* [60] Andrey Malinin et al. "Shifts: A Dataset of Real Distributional Shift Across Multiple Large-Scale Tasks". In: _Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks_. Vol. 1. 2021.
* [61] Hendrik A. Mehrtens et al. _Benchmarking common uncertainty estimation methods with histopathological images under domain shift and label noise_. 2023. arXiv: 2301.01054.
* [62] Norman Mu and Justin Gilmer. "MNIST-C: A Robustness Benchmark for Computer Vision". In: _arXiv preprint arXiv:1906.02337_ abs/1906.02337 (2019).
* [63] Jishnu Mukhoti and Yarin Gal. "Evaluating bayesian deep learning methods for semantic segmentation". In: _arXiv preprint arXiv:1811.12709_ (2018).
* [64] Allan H. Murphy and Edward S. Epstein. "Verification of Probabilistic Predictions : A Brief Review". In: _Journal of Applied Meteorology (1962-1982) 6.5_ (1967), pp. 748-755.
* [65] Zachary Nado et al. "Uncertainty Baselines: Benchmarks for Uncertainty & Robustness in Deep Learning". In: _arXiv preprint arXiv:2106.04015_ (2021).
* [66] Mahdi Pakdaman Naeini, Gregory F. Cooper, and Milos Hauskrecht. "Obtaining Well Calibrated Probabilities Using Bayesian Binning". In: AAAI'15. 2015, pp. 2901-2907.
* [67] Radford M. Neal. _Bayesian Learning for Neural Networks_. Springer-Verlag, 1996. isbn: 0387947248.
* [68] Jianmo Ni, Jiacheng Li, and Julian McAuley. "Justifying Recommendations using Distantly-Labeled Reviews and Fine-Grained Aspects". In: _Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)_. Association for Computational Linguistics, 2019, pp. 188-197.
* [69] Jeremy Nixon et al. "Measuring Calibration in Deep Learning". In: _arXiv preprint arXiv:1904.01685_ (2019).
* [70] Kazuki Osawa et al. "Practical Deep Learning with Bayesian Principles". In: _Advances in Neural Information Processing Systems_. Vol. 32. Curran Associates, Inc., 2019.

* [71] Ian Osband. "Risk versus Uncertainty in Deep Learning: Bayes, Bootstrap and the Dangers of Dropout". In: _Workshop on Bayesian Deep Learning, NIPS_ (2016).
* [72] Yaniv Ovadia et al. "Can you trust your model's uncertainty? Evaluating predictive uncertainty under dataset shift". In: _Advances in Neural Information Processing Systems_. Vol. 32. Curran Associates, Inc., 2019.
* [73] Prajit Ramachandran, Barret Zoph, and Quoc V. Le. _Searching for Activation Functions_. 2017. arXiv: 1710.05941.
* [74] Hippolyt Ritter, Aleksandar Botev, and David Barber. "A Scalable Laplace Approximation for Neural Networks". In: _International Conference on Learning Representations_. 2018.
* [75] Tim G. J. Rudner et al. "Tractable Function-Space Variational Inference in Bayesian Neural Networks". In: _Advances in Neural Information Processing Systems_. Vol. 35. 2022, pp. 22686-22698.
* [76] Victor Sanh et al. "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter". In: _arXiv preprint arXiv:1910.01108_ (2019).
* [77] Steffen Schneider et al. "Removing covariate shift improves robustness against common corruptions". In: _CoRR_ abs/2006.16971 (2020).
* [78] Mrinank Sharma et al. "Do Bayesian Neural Networks Need To Be Fully Stochastic?" In: _Proceedings of The 26th International Conference on Artificial Intelligence and Statistics_. Vol. 206. Proceedings of Machine Learning Research. PMLR, 2023, pp. 7694-7722.
* [79] Mrinank Sharma et al. _Incorporating Unlabelled Data into Bayesian Neural Networks_. 2023. arXiv: 2304.01762.
* [80] Yuge Shi et al. "Gradient Matching for Domain Generalization". In: _International Conference on Learning Representations_. 2022.
* [81] Ravid Shwartz-Ziv et al. "Pre-Train Your Loss: Easy Bayesian Transfer Learning with Informative Prior". In: _First Workshop on Pre-training: Perspectives, Pitfalls, and Paths Forward at ICML 2022_. 2022.
* [82] Saurabh Singh and Shankar Krishnan. "Filter Response Normalization Layer: Eliminating Batch Dependence in the Training of Deep Neural Networks". In: _2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_. 2020, pp. 11234-11243.
* [83] Nitish Srivastava et al. "Dropout: A Simple Way to Prevent Neural Networks from Overfitting". In: 15.1 (2014), pp. 1929-1958.
* [84] Shengyang Sun et al. "Functional Variational Bayesian Neural Networks". In: _arXiv preprint arXiv:1903.05779_ (2019).
* [85] James Taylor et al. "Rxx1: An image set for cellular morphological variation across many experimental batches". In: _International Conference on Learning Representations (ICLR)_. Vol. 22. 2019, p. 23.
* [86] Marcin B Tomczak, Siddharth Swaroop, and Richard E Turner. "Neural network ensembles and variational inference revisited". In: _1st Symposium on Advances in Approximate Bayesian Inference_. 2018, pp. 1-11.
* [87] Ashish Vaswani et al. "Attention is All you Need". In: _Advances in Neural Information Processing Systems_. Vol. 30. 2017.
* [88] Yeming Wen et al. "Flipout: Efficient Pseudo-Independent Weight Perturbations on Mini-Batches". In: _International Conference on Learning Representations (ICLR)_. 2018.
* [89] Andrew Gordon Wilson and Pavel Izmailov. _Bayesian Deep Learning and a Probabilistic Perspective of Generalization_. NIPS'20. Curran Associates Inc., 2020.
* [90] Andrew Gordon Wilson et al. _Evaluating Approximate Inference in Bayesian Deep Learning_. 2021. URL: https://imailovpavel.github.io/neurips_bdl_competition/files/BDL_NeurIPS_Competition.pdf.
* [91] Thomas Wolf et al. "Transformers: State-of-the-Art Natural Language Processing". In: _Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations_. Online: Association for Computational Linguistics, Oct. 2020, pp. 38-45.
* [92] Huaxiu Yao et al. "C-Mixup: Improving Generalization in Regression". In: _Advances in Neural Information Processing Systems_. Vol. 35. 2022, pp. 3361-3376.

* [93] J. Yao et al. "Quality of Uncertainty Quantification for Bayesian Neural Network Inference". In: _Proceedings at the International Conference on Machine Learning: Workshop on Uncertainty & Robustness in Deep Learning (ICML)_. 2019.
* [94] Christopher Yeh et al. "Using publicly available satellite imagery and deep learning to understand economic well-being in Africa". In: _Nature Communications_ 11.1 (May 2020).
* [95] Wei Zhao et al. "3D Deep Learning from CT Scans Predicts Tumor Invasiveness of Subcentimeter Pulmonary Adenocarcinoma". In: _Cancer Research_ 78.24 (2018), pp. 6881-6889.

Approximate Inference

This section provides further details on the algorithms introduced in Section 3.1.

### Variational Inference

Variational inference (VI) minimizes the KL divergence [50] between the true posterior \(p(\boldsymbol{\theta}\mid\mathcal{D})\) and the approximate posterior \(q(\boldsymbol{\theta}\mid\mathcal{D})\)[6]. While the KL divergence cannot be computed by itself, as the true posterior is unknown, it can still be minimized by maximizing the evidence lower bound (ELBO) given the parameter prior \(p(\boldsymbol{\theta})\):

\[\text{ELBO}=\mathop{\mathbb{E}}_{\boldsymbol{\theta}\sim q(\boldsymbol{\theta }\mid\mathcal{D})}\left[\log p(\mathcal{D}\mid\boldsymbol{\theta})\right]- \text{KL}\left[q(\boldsymbol{\theta}\mid\mathcal{D})\mid\mid p(\boldsymbol{ \theta})\right]\] (4)

Maximizing the ELBO means maximizing the likelihood of the training data, therefore fitting the data well, while staying close to the parameter prior [6].

Bayes By Backprop (BBB).BBB [7] is an application of VI to deep neural network. BBB approximates the parameter posterior with a diagonal Gaussian distribution that cannot model covariances between parameters. The per-parameter means and variances are learned with standard Stochastic Gradient Descent (SGD) [42] using the negative of the ELBO as the loss function. The ELBO by itself is not differentiable as it depends on the randomly chosen parameters. However, the reparameterization trick [44] applies to diagonal Gaussians and allows us to use the negative ELBO as the loss function. Further runtime performance improvements are possible by using the local reparameterization trick [45] or Flipout [88].

While there have been reports of BBB performing well when used on neural networks [7, 86], the current consensus of the research community seems to be that BBB falls short when compared to e.g. ensembles [20, 72, 89], even though it has been shown that the diagonal Gaussian posterior is not significantly less expressive than a posterior that models covariances [18, 20]. In recent years significant work has been done to improve the performance of VI in a deep learning setting. To assess whether these improved algorithms can compete with SOTA Bayesian algorithms, we also evaluate promising improvements on posterior parameterizations (Rank-1 VI, SVGD) and optimization procedures (iVON).

Rank-1 Variational Inference (Rank-1 VI).Rank-1 VI [17] enhances the posterior approximation of BBB by approximating a full-rank covariance matrix with a low-rank approximation. Rank-1 VI learns a diagonal Gaussian distribution over two vectors per layer, whose outer product is then element-wise multiplied to a learned point estimate of the layer's weights. The bias vector is kept as a point estimate. The limited number of additional parameters allows Rank-1 VI to learn a multi-component Gaussian distribution for the two low-rank vectors, which gives Rank-1 VI ensemble-like properties. Rank-1 VI is both less expressive than BBB with the mean field approximation in the sense that it has fewer variational parameters, and is more expressive as it can model covariances between parameters within a layer and can express multi-modality in a limited way.

Improved Variational Online Newton (iVON).The usage of SGD for the optimization of variational parameters is problematic, as these parameters form a complex, non-euclidean manifold [41]. Natural gradient descent (NGD), recently formalized as the Bayesian learning rule [39], exploits this structure to speed up training. VOGN [41, 70] applies NGD to neural networks but has scaling problems, as it requires per-example gradients in minibatch training. iVON, based on the improved Bayesian learning rule [53], no longer has this problem. While iVON still uses the mean-field approximation of BBB, it is expected to converge faster, and, importantly, halves the number of trainable parameters by implicitly learning per-parameter variances.

Stein Variational Gradient Descent (SVGD).SVGD [55] is a non-parametric VI algorithm that does not assume the posterior to be of a particular shape but approximates it with \(p\) particles (i.e. point estimates). The particles can be viewed as members of a Deep Ensemble [51], and the use of VI adds a repulsive component to the loss function based on the RBF kernel distance between the parameters of the particles. While this repulsive component can prevent the particles from converging to the same posterior mode, it prohibits the independent training of the particles.

### Other Algorithms

Deep Ensembles.Lakshminarayanan et al. [51] introduce Deep Ensembles that combine the predictions of multiple independently trained neural networks to improve uncertainty estimates. Originally, Deep Ensembles have been seen as a competing approach to Bayesian algorithms [51]. However, ensembles can be considered to be a Bayesian algorithm that approximates the posterior with a sum of delta distributions [89]. We consider all ensembles to be Bayesian: While they are missing the principled posterior approximation approach of VI, basically hoping that the members converge to different posterior modes, the approach results in a posterior approximation that is in many cases better than the approximation of for example BBB (Section 5, [72, 89]).

Ensembles are usually considered SOTA in uncertainty estimation [72, 89]. However, the training time scales linearly in the number of ensemble members. This makes them highly expensive in cases where training a single member is already expensive, such as with large networks, and opens the space for new, cheaper posterior approximations.

Monte Carlo Dropout (MCD).MCD [22] uses dropout [83] to form a Bernoulli distribution over network parameters. The dropout rates are typically not learned, but the dropout units that are present in many network architectures are simply applied during the evaluation of the model. This very cheap posterior approximation has been criticized for not being truly Bayesian [71]. Despite this criticism, it is still widely used, including in practical applications [10]. When the dropout rate is learned, MCD can be considered to implicitly perform VI [21].

Stochastic Weight Averaging-Gaussian (SWAG).SWAG [59] forms its posterior approximations from the parameter vectors that are traversed during the training of a standard neural network. During the last epochs of SGD training, SWAG periodically stores the current parameters of the neural network to build a low-rank Gaussian distribution over model parameters. While SWAG has only a very small performance overhead during training, storing the additional parameters requires a significant amount of additional memory, and sampling parameters from the low-rank Gaussian distribution incurs a performance overhead during evaluation.

Laplace Approximation.The Laplace approximation [57] builds a local posterior approximation from a second-order Taylor expansion around a MAP model. We always use the last-layer Laplace approximation and switch between a full-rank posterior, diagonal posterior, and a Kronecker-factorized posterior [74] depending on the task. In this configuration, the Laplace approximation is the only post-hoc algorithm that we consider: It can be fitted on top of an existing MAP model by performing a single pass on the training dataset.

## Appendix B Unsigned Calibration Metrics

As mentioned in the main paper (Section 4), a calibrated model makes confident predictions if and only if they will likely be accurate. Based on this definition, we can directly derive a calibration metric for classification models: The expected calibration error (ECE) [28, 66]. In the regression case, neither "accuracy" nor "confidence" are well-defined properties of a prediction. The notion of calibration must therefore be adapted for regression tasks. In addition, the log marginal likelihood is commonly used to jointly evaluate the accuracy and the calibration in regression tasks. See Appendix G.3.2 for details.

Calibrated Classification.In the classification case, each data point has an associated distribution \(Y\) over the possible labels. \(Y\) represents the inherent aleatoric uncertainty of the label. Given a prediction \(\hat{y}=\operatorname*{arg\,max}_{y}p(y\mid\boldsymbol{x},\mathcal{D})\) made with confidence \(\hat{p}=\max_{y}p(y\mid\boldsymbol{x},\mathcal{D})\), the model is perfectly calibrated if and only if

\[\mathbb{P}\left(\hat{y}=Y\mid\hat{p}=p\right)=p\qquad\forall p\in[0,1]\] (5)

holds for every data point [12, 28, 66]. Informally speaking, this means that if the model makes 100 predictions with a confidence of \(0.8\), \(80\) of these predictions should be correct. The expected difference between the left and the right side of Equation (5) is called the expected calibration error (ECE) of the model:

\[\text{ECE}=\operatorname*{\mathbb{E}}_{p\sim\mathcal{U}([0,1])}\left[\,| \mathbb{P}\left(\hat{y}=Y\mid\hat{p}=p\right)-p|\,\right]\] (6)It implies two properties of a well-calibrated model: If the accuracy is low, the confidence should also be low. This means that the model must not be overconfident in its predictions. Conversely, if the accuracy is high, the confidence should also be high, meaning that the model must not be underconfident in its predictions.

In practice, a model does not make enough predictions of the same confidence to calculate the calibration error exactly. Therefore, the model's predictions on an evaluation set \(\mathcal{D}^{\prime}\) are commonly grouped into \(M\) equally spaced bins \(B_{m}\) based on their confidence values, and the average accuracy and confidence of each bin are used to calculate the ECE [28, 66]:

\[\text{ECE}\approx\sum_{m=1}^{M}\frac{|B_{m}|}{|\mathcal{D}^{\prime}|}|\text{ acc}(B_{m})-\text{conf}(B_{m})|,\] (7)

where \(B_{m}\) is the set of predictions in the \(m\)-th bin, and \(\text{acc}(B_{m})\) and \(\text{conf}(B_{m})\) are the average accuracy and confidence of the predictions in \(B_{m}\):

\[\text{acc}(B_{m}) =\frac{1}{|B_{m}|}\sum_{(\bm{x},y)\in B_{m}}\bm{1}\big{(}y=\operatorname {arg\,max}_{y^{\prime}}p(y^{\prime}\mid\bm{x},\mathcal{D})\big{)}\] (8) \[\text{conf}(B_{m}) =\frac{1}{|B_{m}|}\sum_{(\bm{x},y)\in B_{m}}\max_{y^{\prime}}p(y ^{\prime}\mid\bm{x},\mathcal{D})\] (9)

An ECE of zero indicates perfect calibration. We always use ten bins (\(M=10\)).

A main problem of the ECE is that bins with few predictions in them may exhibit a high variance [69]. Therefore, Nixon et al. [69] proposed an extension of the ECE that uses bins of adaptive width.

Calibrated Regression.The confidence intervals of the predictive distribution can be used to measure the calibration of a _regression_ model [49]. The probability of the ground-truth output \(\bm{y}\) laying inside of the \(\rho\)-confidence interval of the predictive distribution of the model for input \(\bm{x}\) should be exactly \(\rho\). Formally, we say a regression model is perfectly calibrated on an evaluation dataset \(\mathcal{D}^{\prime}\) if and only if

\[\mathbb{P}(Q_{\rho^{\prime}}(\bm{x})\leq\bm{y}\leq Q_{1-\rho^{\prime}}(\bm{x} ))=\rho\qquad\forall(\bm{x},\bm{y})\in\mathcal{D}^{\prime}\] (11)

holds for every \(q\)-quantile \(Q_{q}(\bm{x})\) of the predictive distribution for input \(\bm{x}\) with \(\rho^{\prime}=\nicefrac{{(1-\rho)}}{{2}}\).

Selectively evaluating Equation (11) for \(M\) confidence values \(\rho_{m}\) allows the practical computation of a quantile calibration error (QCE) on an evaluation dataset \(\mathcal{D}^{\prime}\)

\[\text{QCE}=\frac{1}{M}\sum_{m=1}^{M}|(\rho_{m}-p_{\text{obs}}(\rho_{m}))|\] (12)

with

\[p_{\text{obs}}(\rho_{m})=\frac{1}{|\mathcal{D}^{\prime}|}\sum_{(\bm{x},\bm{y} )\in\mathcal{D})}\bm{1}(Q_{\rho^{\prime}}(\bm{x})\leq\bm{y}\leq Q_{1-\rho^{ \prime}}(\bm{x})).\] (13)

Figure 6: Reliability plots of fictional, overconfident regression models when using (a) quantiles and (b) confidence intervals (CI).

The QCE simply replaces the quantiles in the definition of the calibration error from Kuleshov et al. [49] by confidence intervals. Using the confidence intervals allows a simpler interpretation of the resulting reliability diagrams: With the calibration error proposed by Kuleshov et al. [49], the reliability diagram of a perfectly calibrated regression model is a horizontally mirrored version of the reliability diagram of a perfectly calibrated classification model, as there are too many ground-truth values below the lower quantiles of their predictive distributions, and too few above the higher quantiles (Figure 5(a)). Using confidence intervals for the reliability diagram results in a plot that can be interpreted in the same way as a reliability diagram of a classification model (Figure 5(b)). We always use ten equally-spaced confident levels between \(0\) and \(1\) (\(M=10\)).

## Appendix C Signed Calibration Metrics

As described in the main paper, our signed calibration metrics (sECE and sQCE) may be zero even though the model is not perfectly calibrated. However, we show that this is typically not an issue in practice, as for most models nearly all predictions are overconfident or nearly all predictions are underconfident. The reliability diagrams in Figure 7 confirm this for a representative selection of overconfident and underconfident models. We always report the unsigned calibration metrics in Appendix G in addition to the signed calibration metrics mentioned in the main paper. The unsigned metrics are in almost all cases very close to the absolute value of the signed metric, resulting in the same relative ordering of the algorithms. On the other hand, the sECE provides valuable insights into the underconfidence of some algorithms such as MultiSWAG on CIFAR-10 and SWAG on Amazon-wilds.

## Appendix D Implementation Details

Except for Laplace, we implement all algorithms ourselves as PyTorch [24] optimizers. The implementation of the algorithms as well as code to reproduce all experiments is available at https://github.com/bdl-authors/beyond-ensembles, where we also provide a short tutorial on the usage of our implementation.

Bayes By Backprop.We use the local reparameterization trick [45]. As it is standard today [20, 72, 89], we do not use the scale mixture prior introduced by BBB's original authors [7], but a unit Gaussian prior. For the experiments on CIFAR-10, we make the parameters of the Filter Response Normalization layers variational.

Rank-1 Vi.Following Dusenberry et al. [17], we keep the bias of each layer as a point estimate. We also keep the learned parameters of batch normalization and Filter Response Normalization layers as point estimates. We use five components in most cases which is close to the four components recommended by Dusenberry et al. [17] and make Rank-1 VI directly comparable to other ensemble-based models that use five members.

Figure 7: Reliability diagrams of different models on a variety of datasets. No data point is drawn for empty bins. The number of predictions in each bin is denoted at the top of each plot. The dashed line corresponds to a perfectly calibrated model. In all cases, either nearly all of the model’s predictions are overconfident, or nearly all are underconfident. Therefore, the ECE is close to the absolute value of the sECE, indicating that the sECE is a reasonable calibration metric.

iVON.We adapt the data augmentation factor that Osawa et al. [70] introduce for VOGN [40] to iVON. We do not use the tempering parameter from VOGN.

Laplace.We use the Laplace library from Daxberger et al. [13] due to the difficulty of implementing second-order optimization in PyTorch. In all cases except for CivilComments-wilds, we use a Kronecker-factorized last-layer Laplace approximation. On CivilComments-wilds, we use a diagonal last-layer Laplace approximation as the Kronecker-factorized approximation frequently leads to diverging parameters. We do not use the GLM approximation as proposed by Daxberger et al. [13] but use Monte Carlo sampling to stay consistent with the other evaluated algorithms. In all experiments we use the Laplace library's functions to tune the prior precision after fitting the Laplace approximation.

Swag.While the authors of SWAG argue that SWAG benefits from a special learning rate schedule [59], they do not use such a schedule in most of their experiments with SWAG and MultiSWAG [89]. Correspondingly, we use the same schedule with SWAG as with any other algorithm. We use 30 parameter samples for building the mean and the low-rank covariance matrix of SWAG. On CivilComments-wilds, we only use 10 parameter samples due to the storage size of the samples.

## Appendix E Batch Normalization, Distribution Shift, and Bayesian Deep Learning

Schneider et al. [77] find that a significant part of the accuracy loss on o.o.d. data is due to changing batch statistics that cannot be adequately normalized by the running batch normalization statistics that are based on the training data. The authors propose to re-initialize the running statistics on a subset of the evaluation dataset.

We are able to reproduce the issue with o.o.d. data on the Camelyon17-wilds dataset from the WILDS collection [47] (Figure 8). The o.o.d. evaluation set of Camelyon17 has been generated by selecting the images that were most visually distinct from the other images. In addition, the employed ResNet-20 [32] architecture includes batch normalization layers. We find that using only batch statistics, thereby essentially using the batch normalization layers in training mode during evaluation, entirely alleviates the i.d. - o.o.d. performance gap on Camelyon17, as well as the large standard deviations on the o.o.d. dataset. Coincidentally, the WILDS leaderboard [46] shows that models that do not include batch normalization, such as a model based on the vision transformer [15], or that use extensive data augmentation, perform best.

The running statistics of batch normalization layers also pose problems with Bayesian neural networks that sample parameters, as the running statistics depend on the parameters of the neural network. Wilson and Izmailov [89] therefore propose to recalculate the batch normalization statistics for each parameter sample. This is not necessary in our case as we never use running statistics for normalization layers. By doing so we also avoid the aforementioned distribution-shift problem without requiring additional o.o.d. data during evaluation, and do not add any computation overhead.

Figure 8: Camelyon17-wilds: Average accuracy vs. sECE on the o.o.d. test split. The models that use no running statistics (static BN) are significantly more accurate and better calibrated, while exhibiting a smaller variance.

Computational Resources

We use single NVIDIA Tesla V100, A100, and H100 GPUs for all tasks from Wilds [47] and CIFAR-10-(C) [33; 48]. See Table 1 for the GPUs that we use on the individual datasets as well as the runtime of MAP. Table 2 displays the relative runtime of the BDL algorithms. In total, we estimate that the evaluation required about \(1600\,\mathrm{h}\) of GPU time, of which about \(25\%\) were consumed during implementation, testing and hyperparameter optimization. Training and hyperparameter optimization of the UCI models was performed on a single CPU in about \(20\,\mathrm{h}\). Table 3 shows the GPU memory overhead of the BDL algorithms.

\begin{table}
\begin{tabular}{l|c c} Dataset & GPU & Runtime of MAP \\ \hline CIFAR-10 & NVIDIA V100 & \(50\,\mathrm{min}\) \\ PovertyMap-wilds & NVIDIA V100 & \(50\,\mathrm{min}\) \\ iWildCam-wilds & NVIDIA A100 & \(150\,\mathrm{min}\) \\ FMoW-wilds & NVIDIA V100 & \(150\,\mathrm{min}\) \\ RxRx1-wilds & NVIDIA V100 & \(140\,\mathrm{min}\) \\ CivilComments-wilds & NVIDIA A100 & \(60\,\mathrm{min}\) \\ Amazon-wilds & NVIDIA H100 & \(90\,\mathrm{min}\) \\ \end{tabular}
\end{table}
Table 1: Hardware and runtime for MAP for each dataset. The results are rounded to the next \(10\) minutes.

\begin{table}
\begin{tabular}{l|c c} Model & Memory Overhead \\ \hline MAP & \(1.0\) \\ MCD & \(1.0\) \\ SWAG & \(\sim 1.0\) \\ Laplace & \(\sim 1.0\) \\ BBB & \(\sim 2\) \\ Rank-1 VI & \(\sim 1+\#\)components \(\cdot\sqrt{\text{parameter count}}\) \\ iVON & \(\sim 2\)5 \\ SVGD & \(\sim\#\)particles \\ \end{tabular}
\end{table}
Table 3: GPU memory requirements of different algorithms relative to MAP. The numbers are estimates are based on a theoretical analysis of the algorithms, not on measurements. The memory consumption of MultiX is the same as for the respective single-mode approximation, since all members can be trained indenpendently.

\begin{table}
\begin{tabular}{l|r r r r r r r} Model & PovertyMap & iWildCam & FMoW & RxRx1 & CivilComments & Amazon & CIFAR-10 \\ \hline MAP & \(1.0\) & \(1.0\) & \(1.0\) & \(1.0\) & \(1.0\) & \(1.0\) & \(1.0\) \\ MCD & \(1.0\) & \(1.0\) & \(1.0\) & \(1.0\) & \(1.0\) & \(1.0\) & \(\sim 1.0\) \\ SWAG & \(1.3\) & \(1.5\) & \(1.0\) & \(1.2\) & \(1.3\) & \(1.5\) & \(\sim 1.0\) \\ Laplace & \(1.0\) & \(1.0\) & \(1.0\) & \(1.0\) & \(1.0\) & \(1.0\) & \(\sim 1.0\) \\ BBB & \(5.7\) & - & - & - & - & - & \(\sim 5.0\) \\ LL BBB & - & \(1.6\) & \(2.0\) & \(3.7\) & \(1.8\) & \(2.0\) & - \\ Rank-1 VI & \(3.9\) & - & - & - & - & - & \(\sim 4.0\) \\ LL Rank-1 VI & - & - & \(2.0\) & \(2.0\) & \(3.7\) & \(1.9\) & \(2.0\) & - \\ iVON & \(2.8\) & - & - & - & - & - & \(\sim 3.0\) \\ LL iVON & - & \(3.0\) & \(2.9\) & \(3.6\) & \(5.6\) & \(6.6\) & - \\ SVGD & \(9.2\) & \(4.9\) & \(7.3\) & \(8.9\) & \(9.2\) & \(10.0\) & \(\sim 8.0\) \\ \end{tabular}
\end{table}
Table 2: Runtime of different algorithms relative to MAP. The numbers on CIFAR-10 are conservative estimates as exact numbers were no longer available. Note that the runtime also depends on whether we are able to use mixed precision training, which was not possible with the VI algorithms. The training time of a MultiX model with \(n\) members is \(n\) times the training time of the respective single-mode approximation. LL = Last-Layer.

[MISSING_PAGE_FAIL:23]

\begin{table}
\begin{tabular}{l|c c c c} Model & LML & MSE & QCE & sQCE \\ \hline MAP & \(-7.723\pm 7.553\) & \(\mathbf{34.444\pm 41.620}\) & \(0.247\pm 0.005\) & \(\mathbf{0.043\pm 0.195}\) \\ Deep Ensemble & \(-4.360\pm 3.066\) & \(\mathbf{31.419\pm 36.845}\) & \(0.272\pm 0.060\) & \(\mathbf{0.072\pm 0.207}\) \\ MCD & \(-10.299\pm 10.685\) & \(48.491\pm 58.682\) & \(0.261\pm 0.065\) & \(\mathbf{0.041\pm 0.206}\) \\ MultiMCD & \(-6.744\pm 6.151\) & \(41.030\pm 49.269\) & \(0.272\pm 0.061\) & \(\mathbf{0.073\pm 0.207}\) \\ SWAG & \(\mathbf{-3.655\pm 1.469}\) & \(\mathbf{30.372\pm 28.902}\) & \(0.218\pm 0.084\) & \(\mathbf{0.011\pm 0.183}\) \\ MultiSWAG & \(\mathbf{-3.110\pm 0.815}\) & \(\mathbf{25.362\pm 22.428}\) & \(0.192\pm 0.059\) & \(\mathbf{0.034\pm 0.152}\) \\ LL Laplace & \(-7.009\pm 4.256\) & \(45.505\pm 37.787\) & \(0.247\pm 0.040\) & \(\mathbf{0.116\pm 0.119}\) \\ LL MultiLaplace & \(-5.549\pm 2.983\) & \(38.452\pm 31.657\) & \(0.270\pm 0.046\) & \(\mathbf{0.142\pm 0.127}\) \\ BBB & \(-64.268\pm 79.182\) & \(43.670\pm 52.833\) & \(0.199\pm 0.131\) & \(\mathbf{-0.101\pm 0.184}\) \\ MultiBBB & \(-22.150\pm 25.068\) & \(50.502\pm 58.432\) & \(0.236\pm 0.110\) & \(\mathbf{-0.073\pm 0.202}\) \\ Rank-1 VI & \(-72.412\pm 92.191\) & \(49.099\pm 60.606\) & \(0.191\pm 0.133\) & \(\mathbf{-0.109\pm 0.178}\) \\ iVON & \(\mathbf{-3.367\pm 0.903}\) & \(\mathbf{21.546\pm 13.347}\) & \(\mathbf{0.109\pm 0.025}\) & \(\mathbf{0.038\pm 0.074}\) \\ SVGD & \(-9.945\pm 10.449\) & \(46.757\pm 56.551\) & \(0.227\pm 0.067\) & \(\mathbf{0.037\pm 0.182}\) \\ \end{tabular}
\end{table}
Table 7: UCI-energy (gap splits)

### Cifar-10

Following Wilson et al. [90], we train a ResNet-20 [32] with Swish activations [73] and Filter Response Normalization [82]. The use of Filter Response Normalization instead of batch normalization, which only uses batch statistics, eliminates the problems mentioned in Appendix E. We train all models except iVON with SGD and a learning rate of \(0.05\) and Nesterov momentum of strength \(0.9\) for 300 epochs. We use the learning rate schedule from Maddox et al. [59]: The learning rate is kept at its initial value for the first 150 epochs, then linearly reduced to a learning rate of \(0.005\) at epoch 270 at which it is kept constant for the remaining 30 epochs. For MCD, we use a dropout rate of \(0.1\) and insert dropout units after every linear and convolutional layer of the ResNet-20. For BBB, we temper the KL divergence in the ELBO with a factor of \(0.2\). Rank-1 VI uses an untempered posterior and four components. BBB and iVON use two Monte Carlo samples during training. The Laplace approximation is based on a diagonal last-layer approximation. iVON is also trained for 300 epochs with a learning rate of \(1\cdot 10^{-4}\), a prior precision of \(50\), and a data augmentation factor of \(10\) (see Osawa et al. [70] for details), but uses no learning rate schedule. We found these changes to be necessary to ensure that iVON performs well, likely because iVON is much more similar to Adam [43] than to SGD and therefore needs a smaller learning rate. Following Nado et al. [65], SNGP uses a spectral normalization factor of \(6.0\) and mean field factor of \(20\). We did not perform any additional tuning of the mean field factor. We always use 50 parameter samples during evaluation.

Figure 9 displays the accuracy, ECE, sECE, agreement with HMC, and TV compared to HMC. MultiX models tend to become underconfident. Table 8 shows detailed numerical results for all algorithms and corruption levels.

\begin{table}

\end{table}
Table 8: CIFAR-10: Detailed results on the standard evaluation split and the corruption levels \(1\), \(3\), and \(5\) of CIFAR-10-C.

### Wilds

We strictly follow the training and evaluation protocol of Koh et al. [47] by reusing their data folds for training, validation, and testing. We use the hyperparameters proposed by Koh et al. [47] where applicable, and set the other hyperparameters to standard values as suggested by the developers of the respective algorithms. If the standard values lead to unexpectedly bad results, we tune the hyperparameters through a grid search. Hyperparameter tuning was performed on the i.d. validation and, where available, o.o.d. validation splits, but never on testing splits. In particular, we select the prior precision of iVON through a grid search over the values \(1,10,100\), and \(500\) per model architecture. We find the prior precision of iVON to be hard to tune, as iVON frequently diverges for comparatively small prior precisions such as \(1\) and \(10\). BBB works always well with the standard unit prior. We also experiment with other priors but find no difference in performance except on RxRx1-wilds (see Figure 14). BBB and iVON use two Monte Carlo samples during training. See the sections below for the hyperparameters that were chosen on the individual datasets. We use mixed precision training whenever possible. The VI algorithms as well as the Laplace approximations are mostly trained without mixed precision, as this leads to unstable training.

SNGP uses the same learning rate, weight decay, and number of epochs as the other algorithms. Following the recommendations by Liu et al. [54] and the tuning done by Nado et al. [65], we use a spectral normalization factor of \(6.0\) for the computer vision tasks and \(0.95\) for the text classification tasks. On the image classification tasks, SNGP performs significantly better when limiting the input

Figure 9: CIFAR-10-(C): All results for the corruption intensities \(0\), \(1\), \(3\), and \(5\). The corruption intensities are denoted on the y-axis. A negative sECE indicates overconfidence, a positive sECE indicates underconfidence. The plot for the TV is repeated from Figure 5.

dimension of the Gaussian Process to \(128\) or \(256\) instead of using the output dimension of the previous network layer.

We use 10 posterior samples per prediction during evaluation to constrain the computational overhead of the Bayesian algorithms, which is generally sufficient to capture the predictive distribution [72]. Note that our results are not directly comparable to the results of Daxberger et al. [13], as they build their Deep Ensembles and Laplace approximations from the pretrained models provided by Koh et al. [47]. When comparing our results with the best performing algorithms on the WILDS leaderboard, we only consider the algorithms on the "overall leaderboard", i.e. the algorithms that conform to the official submission guidelines of Koh et al. [47].

#### g.3.1 Camelyon17-Wilds

Following Koh et al. [47], we train a DenseNet-121 [35] with SGD for \(5\) epochs with a learning rate of \(0.001\), weight decay \(0.01\) and momentum \(0.9\). SWAG collects \(30\) parameter samples during the last epoch.

#### g.3.2 PovertyMAP-Wilds

We train a ResNet-18 [32] using the same hyperparameters as Koh et al. [47] where applicable: A learning rate of \(10^{-3}\) and no weight decay. We only train for 100 epochs as all models were converged after that. SWAG collects 30 parameter samples starting at epoch 50. For BBB, we scale the KL divergence down with a factor of \(0.2\), as this significantly improves the MSE. Rank-1 VI uses an unscaled KL divergence. The ensembles, Rank-1 VI and SVGD use five members/components. We optimize the log likelihood of the training data and represent the aleatoric uncertainty with a fixed standard deviation of \(0.1\), as this is the value MAP converges to when jointly optimizing the standard deviation and the model's parameters. For the final evaluations, we do not optimize the standard deviation, as this leads to unstable training with the VI algorithms. Following Koh et al. [47], we aggregate all results over the five folds of PovertyMap, with one seed per fold.

As mentioned in the main paper, iVON performs significantly worse than the other algorithms. We conducted a grid search over prior precisions \(1\), \(10\), \(100\) and \(500\) with a single seed per value, and found that for \(1\) and \(10\) iVON diverges, for \(100\) iVON achieves an o.o.d. Pearson coefficient on the "A" split of \(0.21\) and for \(500\) it achieves a Pearson coefficient of \(0.25\). Most likely due to their underfitting the non-diverged models are comparatively well calibrated with sECEs of \(-0.21\) for a prior precision of \(100\) and \(-0.24\) for a prior precision of \(500\).

Log Marginal Likelihood.The log marginal likelihood is commonly used to jointly evaluate the accuracy and calibration of a regression model. On an evaluation dataset \(\mathcal{D}^{\prime}\), the log marginal likelihood (LML) is given by

\[\text{LML}=\log p(\mathcal{D}^{\prime}\mid\mathcal{D})=\log\int p(\mathcal{D} ^{\prime}\mid\boldsymbol{\theta})p(\boldsymbol{\theta}\mid\mathcal{D})\, \text{d}\boldsymbol{\theta}\approx\log\sum_{n}p(\mathcal{D}^{\prime}\mid \boldsymbol{\theta}_{n}),\] (14)

where the \(\theta_{n}\) are samples from the parameter posterior. When only few predictions are available because sampling parameters \(\theta_{n}\) or evaluating the likelihood \(p(\mathcal{D}^{\prime}\mid\boldsymbol{\theta}_{n})\) is expensive, the LML may become very noisy. We therefore also report the per-sample log marginal likelihood

\[\text{psLML} =\sum_{(\boldsymbol{x}_{i},\boldsymbol{y}_{i})\in\mathcal{D}^{ \prime}}\log p(\boldsymbol{y}_{i}\mid\boldsymbol{x}_{i},\mathcal{D})\] (15) \[=\sum_{(\boldsymbol{x}_{i},\boldsymbol{y}_{i})\in\mathcal{D}^{ \prime}}\log\int p(\boldsymbol{y}_{i}\mid\boldsymbol{x}_{i},\boldsymbol{ \theta})p(\boldsymbol{\theta}\mid\mathcal{D})\,\text{d}\boldsymbol{\theta}\] \[\approx\sum_{(\boldsymbol{x}_{i},\boldsymbol{y}_{i})\in\mathcal{D }^{\prime}}\log\sum_{n}p(\boldsymbol{y}_{i}\mid\boldsymbol{x}_{i},\boldsymbol{ \theta}_{n}),\]

which has a lower variance than the LML. We present the results for the LML, the psLML, the urban/rural Pearson coefficient (see Section 5.1), and the sQCE in Figure 10 and Figure 11. Table 9 shows detailed numerical results.

[MISSING_PAGE_EMPTY:29]

#### g.3.3 1WildCam-wilds

Following Koh et al. [47], we finetune a ResNet-50 [32], pretrained on ImageNet [14], for 12 epochs with the Adam optimizer [43]. For each model, we replace the linear classification layer of the ResNet-50 by a randomly initialized one of the appropriate output dimension. We use the hyperparameters that Koh et al. [47] found to work best based on their grid search: A learning rate of \(3\cdot 10^{-5}\) and no weight decay. For MCD, we try dropout rates of \(0.1\) and \(0.2\) and select \(0.1\) due to a slightly better macro F1 score on the evaluation split. iVON uses a prior precision of \(100\), as optimized by a grid search. We use three seeds per model and build all ensembles by training six models independently and leaving out a different model for each of the three evaluation runs. Figure 12 shows the results on the o.o.d. evaluation split that are not presented in the main paper. Table 10 displays detailed numerical results on the o.o.d. evaluation split and on the i.d. validation split.

Figure 11: PovertyMap-wilds: Worst urban/rural pearson coefficient vs. sQCE, LML and psLML on the i.d. evaluation split. The WILDS leaderboard [46] does not report the i.d. pearson coefficient.

Figure 12: iWildCam-wilds: Macro F1 score, accuracy, sECE and ECE on the o.o.d. evaluation split and the i.d. validation split (see Figure 1(a) for Macro F1 vs. sECE on the o.o.d. evaluation split). MultiX is less accurate than single-mode approximations on the i.d. split, but better calibrated.

[MISSING_PAGE_FAIL:31]

[MISSING_PAGE_FAIL:32]

[MISSING_PAGE_FAIL:33]

[MISSING_PAGE_EMPTY:34]