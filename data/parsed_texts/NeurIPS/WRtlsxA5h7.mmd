# Orthogonal Non-negative Tensor Factorization based Multi-view Clustering

 Jing Li

Xidian University

Xi'an, Shaanxi, China

jinglxd@stu.xidian.edu.cn

&Quanxue Gao

Xidian University

Xi'an, Shaanxi, China

qxgao@xidian.edu.cn

&Qianqian Wang

Xidian University

Xi'an, Shaanxi, China

qqwang@xidian.edu.cn

&Ming Yang

Harbin Engineering University

Harbin, Heilongjiang, China

yangmingmath@gmail.com

&Wei Xia

Xidian University

Xi'an, Shaanxi, China

xdweixia@gmail.com

Corresponding author.

###### Abstract

Multi-view clustering (MVC) based on non-negative matrix factorization (NMF) and its variants have attracted much attention due to their advantages in clustering interpretability. However, existing NMF-based multi-view clustering methods perform NMF on each view respectively and ignore the impact of between-view. Thus, they can't well exploit the within-view spatial structure and between-view complementary information. To resolve this issue, we present orthogonal non-negative tensor factorization (Orth-NTF) and develop a novel multi-view clustering based on Orth-NTF with one-side orthogonal constraint. Our model directly performs Orth-NTF on the 3rd-order tensor which is composed of anchor graphs of views. Thus, our model directly considers the between-view relationship. Moreover, we use the tensor Schatten \(p\)-norm regularization as a rank approximation of the 3rd-order tensor which characterizes the cluster structure of multi-view data and exploits the between-view complementary information. In addition, we provide an optimization algorithm for the proposed method and prove mathematically that the algorithm always converges to the stationary KKT point. Extensive experiments on various benchmark datasets indicate that our proposed method is able to achieve satisfactory clustering performance.

## 1 Introduction

As one of the most typical methods in unsupervised learning, clustering has a wide scope of application [26; 4; 1] to assign data to different clusters according to the information describing the objects and their relationships. Non-negative matrix factorization (NMF) [19] is one of the representative methods of clustering, which is proved to be equivalent to K-means clustering [7]. Despite the widespread use of NMF, there are some drawbacks that have prompted some variants of NMF [8; 5; 27; 9; 3].

In particular, the one-side G-orthogonal NMF [8] can guarantee the uniqueness of the solution of matrix factorization and has excellent clustering interpretation. Also, Ding _et al._proposed the semi-NMF [9]. The data matrix and one of the factor matrices are unconstrained, which allows semi-NMF to be more suitable for applications where the input data is mixed with positive and negative numbers.

Although the above methods can achieve outstanding clustering performance, they are all single-view clustering methods and cannot be adopted straightforwardly for multi-view clustering.

Multi-view clustering tends to achieve superior performance compared to traditional single-view clustering owing to the capability to leverage the complementary information embedded in the different views. Considering the superiority of MVC and NMF, lots of NMF-based multi-view clustering methods have been proposed [13; 28; 23; 33; 15; 29; 14; 37]. The NMF-based multi-view clustering methods can save time and space because it is unnecessary to construct affinity graphs while graph-based methods have to. However, usually, they decompose the original data matrix directly, which leads to a dramatic reduction in the efficiency of the algorithm when the dimension of the original data is huge.

Inspired by the idea of anchor graph. the above issues can be solve by carrying out NMF on the anchor graph [37]. Due to the fact that the dimension of the anchor graph is considerably smaller than the original affinity graph, it follows that the clustering efficiency can be improved. However, as is well-known, there exist two ways of NMF-based multi-view clustering methods. One is to integrate different views first and then implement the NMF on the integrated matrix; the other is to perform the NMF on different views separately and then integrate the result from each view. Both ways are essentially applications of NMF on a single view, and both need to reduce the multi-view data into matrices in the process, which causes the loss of the original spatial information.

To fix the aforesaid issues, we proposed a novel multi-view clustering based on Orth-NTF with one-side orthogonal constraint. Specifically, Non-negative Matrix Factorization (NMF) is tailored primarily for second-order matrices. When processing third-order tensors, there's a need to first transform the tensor into a matrix before applying NMF. This step can lead to a loss of inherent spatial structural information from the third-order tensor. In contrast, Non-negative Tensor Factorization (NTF) sidesteps this issue. NTF directly decomposes third-order tensors. This ensures that the NTF not only acknowledges the relationships between the views but also harnesses the complementary information they offer. Fig 1 delineates the distinction between traditional NMF-based clustering techniques and our NTF-based approach. Furthermore, by incorporating an orthogonal constraint, our model offers distinct physical interpretability for clustering. This suggests that each row of the indicator matrix contains a single non-zero element, and the position of this element directly corresponds to the label of the respective sample. A large number of experiments have shown that our methods have excellent clustering performance.

The main contributions are summarized below:

* We introduce orthogonal non-negative tensor factorization, which considers the between-view relationship directly. Also, we use tensor Schatten \(p\)-norm regularization to characterize the cluster structure of multi-view data and can exploit the complementary information of between views.
* We regard the anchor graph obtained from the original data as the input of the non-negative matrix factorization, which reduces the complexity of our proposed algorithm considerably.
* We provide an optimization algorithm for the proposed method and prove it always converges to the KKT stationary point mathematically. The effectiveness of its application on tensorial G-orthogonal non-negative matrix factorization is demonstrated by extensive experiments.

## 2 Related work

In recent years, multi-view clustering (MVC) has received increasing attention due to its excellent clustering performance. Also, non-negative matrix factorization (NMF) is an efficient technique in single-view clustering, which can generate excellent clustering results that are easy to interpret, and many NMF-based variants have been proposed. Therefore, multi-view clustering-based NMF and its variants have attracted tremendous interest recently.

As the first investigation of the multi-view clustering method based on joint NMF, multiNMF [23] implements NMF at each view and pushes the different clustering results of each view to a consensus. It provides a new viewpoint for the subsequent NMF-based MVC methods. Influenced by multiNMF, He _et al._proposed a multi-view clustering method combining NMF with similarity [15]. It implements NMF on each view as in multiNMF. In addition, it sets a weight for a different view and introducesa similarity matrix of data points to extract consistent information from different views. To better detect the geometric structure of inner-view space, Wang _et al._[33] introduced graph regularization into the NMF-based multi-view clustering method to improve clustering performance. Considering the above work, Wang _et al._[29] proposed a graph regularization multi-view clustering method based on concept factorization (CF). CF is a variant of NMF and it is suitable for handling data containing negative.

As the size of data grows, lots of methods to accelerate matrix factorization are presented. Wang _et al._[28] proposed a fast non-negative matrix triple factorization method. It constrains the factor matrix of NMF to a clustering indicator matrix, thereby avoiding the post-processing of the factor matrix. Inspired by the work of Wang, Han _et al._[14] constrained the intermediate factor matrix in the triple factorization to a diagonal matrix, reducing the number of matrix multiplications in the solution process. Another idea to deal with large-scale multi-view data is to introduce anchor graphs, since the number of anchor points is much smaller than the number of original data, multi-view clustering methods based on anchor graphs tend to reduce the computational complexity and thus are able to deal with large-scale data [22; 17; 21]. Considering that previous NMF-based multi-view clustering methods are performed directly on the original data, Yang _et al._[37] introduced an anchor graph as the input of G-orthogonal NMF. The efficiency of matrix factorization is indeed improved due to the introduction of anchor graph.

Despite the fact that existing NMF-based multi-view clustering methods can perform the clustering tasks excellently, they apply NMF to each view independently. Subsequently, they combine the low-dimensional representations from different perspectives to arrive at a unified shared representation. This approach often overlooks the interrelationships between the views, which are crucial for clustering.

## 3 Notations

We introduce the notations used throughout this paper. We use bold calligraphy letters for 3rd-order tensors, \(\bm{\mathcal{H}}\in\mathbb{R}^{n_{1}\times n_{2}\times n_{3}}\), bold upper case letters for matrices, \(\mathbf{H}\), bold lower case letters for vectors, \(\mathbf{h}\), and lower case letters such as \(h_{ijk}\) for the entries of \(\bm{\mathcal{H}}\). Moreover, the \(i\)-th frontal slice of \(\bm{\mathcal{H}}\) is \(\bm{\mathcal{H}}^{(i)}\). \(\overline{\bm{\mathcal{H}}}\) is the discrete Fourier transform (DFT) of \(\bm{\mathcal{H}}\) along the third dimension, \(\overline{\bm{\mathcal{H}}}=\mathrm{fft}(\bm{\mathcal{H}},[\ ],3)\). Thus, \(\bm{\mathcal{H}}=\mathrm{iff}(\overline{\bm{\mathcal{H}}},[\ ],3)\). The trace and transpose of matrix \(\mathbf{H}\) are expressed as \(\mathrm{tr}(\mathbf{H})\) and \(\mathbf{H}^{\mathrm{T}}\). The F-norm of \(\bm{\mathcal{H}}\) is denoted by \(\|\bm{\mathcal{H}}\|_{F}\).

**Definition 1** (t-product [18]).: _Suppose \(\bm{\mathcal{A}}\in\mathbb{R}^{n_{1}\times m\times n_{3}}\) and \(\bm{\mathcal{B}}\in\mathbb{R}^{m\times n_{2}\times n_{3}}\), the t-product \(\bm{\mathcal{A}}*\bm{\mathcal{B}}\in\mathbb{R}^{n_{1}\times n_{2}\times n_{3}}\) is given by_

\[\bm{\mathcal{A}}*\bm{\mathcal{B}}=\mathrm{iff}(\mathrm{bdiag}(\overline{\bm{ \mathcal{A}}\mathbf{B}}),[\ ],3),\]

_where \(\overline{\bm{\Lambda}}=\mathrm{bdiag}(\overline{\bm{\mathcal{A}}})\) and it denotes the block diagonal matrix. The blocks of \(\overline{\bm{\Lambda}}\) are frontal slices of \(\overline{\bm{\mathcal{A}}}\)._

**Definition 2**.: _[_12_]_ _Given \(\bm{\mathcal{H}}\in\mathbb{R}^{n_{1}\times n_{2}\times n_{3}}\), \(h=\min(n_{1},n_{2})\), the tensor Schatten \(p\)-norm of \(\bm{\mathcal{H}}\) is defined as_

\[\left\|\bm{\mathcal{H}}\right\|_{\tiny\begin{pmatrix}\bm{\mathcal{G}}\end{pmatrix}}= \left(\sum\limits_{i=1}^{n_{3}}\left\|\overline{\bm{\mathcal{H}}}^{(i)}_{ \tiny\begin{pmatrix}\bm{\mathcal{G}}\end{pmatrix}}\right\|_{\tiny\begin{pmatrix} \bm{\mathcal{G}}\end{pmatrix}}^{p}\right)^{\frac{1}{p}}=\left(\sum\limits_{i= 1}^{n_{3}}\sum\limits_{j=1}^{h}\sigma_{j}\Big{(}\overline{\bm{\mathcal{H}}}^{( i)}\Big{)}^{p}\right)^{\frac{1}{p}},\] (1)

_where, \(0<p\leqslant 1\), \(\sigma_{j}(\overline{\bm{\mathcal{H}}}^{(i)})\) denotes the \(j\)-th singular value of \(\overline{\bm{\mathcal{H}}}^{(i)}\)._

It should be pointed out that for \(0<p\leqslant 1\) when \(p\) is appropriately chosen, the Schatten \(p\)-norm provides quite effective improvements for a tighter approximation of the rank function [39; 36].

## 4 Methodology

### Motivation and Objective

Non-negative matrix factorization (NMF) was initially presented as a dimensionality reduction method, and it is commonly employed as an efficient latent feature learning technique recently. Generally speaking, given a non-negative matrix \(\mathbf{X}\), the target of NMF is to decompose \(\mathbf{X}\) into two non-negative matrices,

\[\mathbf{X}\approx\mathbf{HG}^{\mathrm{T}}\] (2)

where \(\mathbf{X}\in\mathbb{R}_{+}^{n\times p}\), \(\mathbf{H}\in\mathbb{R}_{+}^{n\times k}\) and \(\mathbf{G}\in\mathbb{R}_{+}^{p\times k}\). \(\mathbb{R}_{+}^{n\times p}\) means \(n\)-by-\(p\) matrices with elements are all nonnegative. \(n\) and \(k\) means the number of samples and the number of clusters, respectively.

In order to approximate the matrix before and after factorization, \(\ell_{2}\)-norm and F-norm are frequently adopted as the objective function for the NMF. Considering that F-norm can make the model optimization easier, we use F-norm to construct the objective function.

With the extensive use of NMF, more and more variants of NMF have emerged, among which are G-orthogonal NMF [8] and Semi-NMF [9]. By imposing an orthogonality constraint on one of the factor matrices in NMF, we obtain the objective function of the one-side orthogonal NMF,

\[\min_{\mathbf{H}\geqslant 0,\mathbf{G}\geqslant 0}\left\|\mathbf{X}- \mathbf{HG}^{\mathrm{T}}\right\|_{F}^{2},\quad\text{s.t.}\quad\mathbf{H}^{ \mathrm{T}}\mathbf{H}=\mathbf{I}.\] (3)

If we relax the nonnegative constraint on one of the factor matrices in the NMF and the input matrix \(\mathbf{X}\) can also be mixed positive and negative, then we can get Semi-NMF. Semi-NMF can be adapted to process input data that has mixed symbols. For G-orthogonal NMF and Semi-NMF, Ding _et al._[8] presented the following lemma:

**Lemma 1**.: _G-orthogonal NMF and Semi-NMF are all relaxation of K-means clustering, and the main advantages of G-orthogonal NMF are (1) Uniqueness of the solution; (2) Excellent clustering interpretability._

Taking into account the one-side orthogonal NMF, we relax the nonnegative constraints on \(\mathbf{X}\) and \(\mathbf{G}\). Moreover, inspired by FMCNOF [37], we construct the anchor graph \(\mathbf{S}\) obtained from the original data \(\mathbf{X}\) as the input of matrix factorization. Compared with the original data, the number of anchors is much smaller, therefore, by adopting the anchor graph constructed by anchors and original data points as the input of matrix factorization, we can reduce the computational complexity of the algorithm effectively.

\[\min_{\mathbf{H}\geqslant 0}\left\|\mathbf{S}-\mathbf{HG}^{\mathrm{T}} \right\|_{F}^{2},\quad\text{s.t.}\quad\mathbf{H}^{\mathrm{T}}\mathbf{H}= \mathbf{I},\] (4)

where \(\mathbf{S}\in\mathbb{R}^{n\times m}\), \(\mathbf{H}\in\mathbb{R}^{n\times k}\) and \(\mathbf{G}\in\mathbb{R}^{m\times k}\), \(m\) is the number of anchors and we consider \(\mathbf{H}\) as the cluster indicator matrix for clustering rows as described in [8]. We will introduce the details of anchor selection and the construction of the anchor graph in the appendix.

As described in the previous section, the existing NMF-based multi-view clustering methods are essentially a matrix factorization on a single view combined with the integration of multiple views. It causes the loss of the original spatial structure of the multi-view data. We extend NMF to the 3rd-order tensor, which can process the multi-view data directly and can also take full advantage of the original spatial structure of the multi-view data. The objective function of tensorial one-side orthogonal non-negative matrix factorization is written in the following form:

\[\min_{\bm{\mathcal{H}}\geqslant 0}\left\|\bm{\mathcal{S}}-\bm{\mathcal{H}}*\bm{ \mathcal{G}}^{\mathrm{T}}\right\|_{F}^{2},\quad\text{s.t.}\quad\bm{\mathcal{H} }^{\mathrm{T}}*\bm{\mathcal{H}}=\bm{\mathcal{I}},\] (5)The 3rd-order tensor construction process is illustrated in Fig 2.

In order to better exploit the complementary information and spatial structure between different views, we get inspiration from the excellent performance of the tensor Schatten p-norm [12; 38]. We introduce tensor Schatten p-norm regularization on the tensorial form of the cluster indicator matrix. Our objective function is formulated as follows:

\[\min\left\|\bm{\mathcal{S}}-\bm{\mathcal{H}}*\bm{\mathcal{G}}^{\mathrm{T}} \right\|_{F}^{2}+\lambda\|\bm{\mathcal{H}}\|_{\bm{\mathcal{G}}}^{p}\qquad \text{s.t.}\quad\bm{\mathcal{H}}\geqslant 0,\bm{\mathcal{H}}^{\mathrm{T}}*\bm{ \mathcal{H}}=\bm{\mathcal{I}}\] (6)

where \(0<p\leqslant 1\), \(\lambda\) is the hyperparameter of the Schatten \(p\)-norm term.

**Remark 1**.: _The regularizer in the proposed objective (6) is used to explore the complementary information embedded in inter-views cluster assignment matrices \(\bm{H}^{(v)}\) (\(v=1,2,\cdots,V\)). Fig. 2 shows the construction of tensor \(\bm{\mathcal{H}}\), it can be seen that the \(k\)-th frontal slice \(\bm{\Delta}^{(k)}\) describes the similarity between \(N\) sample points and the \(k\)-th cluster in different views. The idea cluster assignment matrix \(\bm{H}^{(v)}\) should satisfy that the relationship between \(N\) data points and the \(k\)-th cluster is consistent in different views. Since different views usually show different cluster structures, we impose tensor Schatten p-norm minimization [12] constraint on \(\bm{\mathcal{H}}\), which can make sure each \(\bm{\Delta}^{(k)}\) has spatial low-rank structure. Thus \(\bm{\Delta}^{(k)}\) can well characterize the complementary information embedded in inter-views._

### Optimization

Inspired by Augmented Lagrange Multiplier (ALM), we introduce two auxiliary variables \(\bm{\mathcal{Q}}\) and \(\bm{\mathcal{J}}\) and let \(\bm{\mathcal{H}}=\bm{\mathcal{Q}}\), \(\bm{\mathcal{H}}=\bm{\mathcal{J}}\), respectively, where \(\bm{\mathcal{Q}}\geqslant 0\). Then, we rewrite the model as the following unconstrained problem:

\[\min\mathcal{L}(\bm{\mathcal{Q}},\bm{\mathcal{H}},\bm{\mathcal{G }},\bm{\mathcal{J}})\] (7) \[=\min_{\bm{\mathcal{Q}}\geqslant 0,\bm{\mathcal{H}}^{\mathrm{T}} *\bm{\mathcal{H}}=\bm{\mathcal{I}}}\left\|\bm{\mathcal{S}}-\bm{\mathcal{H}}* \bm{\mathcal{G}}^{\mathrm{T}}\right\|_{F}^{2}+\lambda\|\bm{\mathcal{J}}\|_{ \bm{\mathcal{G}}}^{p}+\frac{\mu}{2}\left\|\bm{\mathcal{H}}-\bm{\mathcal{Q}}+ \frac{\bm{\mathcal{Y}_{1}}}{\mu}\right\|_{F}^{2}+\frac{\rho}{2}\left\|\bm{ \mathcal{H}}-\bm{\mathcal{J}}+\frac{\bm{\mathcal{Y}_{2}}}{\rho}\right\|_{F}^{2},\]

where \(\bm{\mathcal{Y}_{1}}\), \(\bm{\mathcal{Y}_{2}}\) represent Lagrange multipliers and \(\mu\), \(\rho\) are the penalty parameters. The optimization process can therefore be separated into four steps:

\(\bullet\)**Solve \(\bm{\mathcal{G}}\) with fixed \(\bm{\mathcal{Q}},\bm{\mathcal{H}},\bm{\mathcal{J}}\).** (7) becomes:

\[\min\left\|\bm{\mathcal{S}}-\bm{\mathcal{H}}*\bm{\mathcal{G}}^{\mathrm{T}} \right\|_{F}^{2}\] (8)

After being implemented with discrete Fourier transform (DFT) along the third dimension. the equivalent representation of (8) in the frequency domain becomes:

\[\min\sum_{v=1}^{V}\left\|\bm{\overline{\mathcal{S}}}^{(v)}-\bm{\overline{ \mathcal{H}}}^{(v)}(\bm{\overline{\mathcal{G}}}^{(v)})^{\mathrm{T}}\right\|_{ F}^{2},\] (9)

where \(\bm{\overline{\mathcal{G}}}=\mathrm{fft}(\bm{\mathcal{G}},[\,],3)\), and the others in the same way.

Let \(\Phi=\left\|\bm{\overline{\mathcal{S}}}^{(v)}-\bm{\overline{\mathcal{H}}}^{(v )}(\bm{\overline{\mathcal{G}}}^{(v)})^{\mathrm{T}}\right\|_{F}^{2}\), we can obviously get the following equation:

\[\Phi=\mathrm{tr}\left((\bm{\overline{\mathcal{G}}}^{(v)})^{\mathrm{T}}\bm{ \overline{\mathcal{S}}}^{(v)}\right)-2\mathrm{tr}\left((\bm{\overline{\mathcal{ H}}}^{(v)})^{\mathrm{T}}\bm{\overline{\mathcal{S}}}^{(v)}\bm{\overline{ \mathcal{G}}}^{(v)}\right)+\mathrm{tr}\left((\bm{\overline{\mathcal{G}}}^{(v)}) ^{\mathrm{T}}\bm{\overline{\mathcal{G}}}^{(v)}\right).\] (10)

Setting the derivative \(\partial\Phi/\partial\bm{\overline{\mathcal{G}}}^{(v)}=0\) gives \(2\bm{\overline{\mathcal{G}}}^{(v)}-2(\bm{\overline{\mathcal{S}}}^{(v)})^{ \mathrm{T}}\bm{\overline{\mathcal{H}}}^{(v)}=0\). So the solution of (9) is:

\[\bm{\overline{\mathcal{G}}}^{(v)}=(\bm{\overline{\mathcal{S}}}^{(v)})^{\mathrm{T }}\bm{\overline{\mathcal{H}}}^{(v)}\] (11)

\(\bullet\)**Solve \(\bm{\mathcal{H}}\) with fixed \(\bm{\mathcal{Q}},\bm{\mathcal{G}},\bm{\mathcal{J}}\).** (7) becomes:

\[\min_{\bm{\mathcal{H}}^{\mathrm{T}}*\bm{\mathcal{H}}=\bm{\mathcal{I}}}\left\| \bm{\mathcal{S}}-\bm{\mathcal{H}}*\bm{\mathcal{G}}^{\mathrm{T}}\right\|_{F}^{2 }+\frac{\mu}{2}\left\|\bm{\mathcal{H}}-\bm{\mathcal{Q}}+\frac{\bm{\mathcal{Y} }_{1}}{\mu}\right\|_{F}^{2}+\frac{\rho}{2}\left\|\bm{\mathcal{H}}-\bm{ \mathcal{J}}+\frac{\bm{\mathcal{Y}_{2}}}{\rho}\right\|_{F}^{2}\] (12)And (12) is equivalent to the following in the frequency domain:

\[\begin{split}\min_{(\overline{\bm{\mathcal{H}}}^{(v)})^{\mathrm{T}} \overline{\bm{\mathcal{H}}}^{(v)}=\mathbf{I}}&\sum_{v=1}^{V}\left\| \overline{\bm{\mathcal{S}}}^{(v)}-\overline{\bm{\mathcal{H}}}^{(v)}(\overline {\bm{\mathcal{G}}}^{(v)})^{\mathrm{T}}\right\|_{F}^{2}\\ &+\sum_{v=1}^{V}\frac{\mu}{2}\bigg{\|}\overline{\bm{\mathcal{H}}}^ {(v)}-\overline{\bm{\mathcal{Q}}}^{(v)}+\frac{\overline{\bm{\mathcal{Y}}}_{1}^ {(v)}}{\mu}\bigg{\|}_{F}^{2}+\sum_{v=1}^{V}\frac{\rho}{2}\bigg{\|}\overline{ \bm{\mathcal{H}}}^{(v)}-\overline{\bm{\mathcal{J}}}^{(v)}+\frac{\overline{\bm {\mathcal{Y}}}_{2}^{(v)}}{\rho}\bigg{\|}_{F}^{2},\end{split}\] (13)

where \(\overline{\bm{\mathcal{H}}}=\mathrm{fft}(\bm{\mathcal{H}},[\,],3)\), and the others in the same way.

And (13) can be reduced to:

\[\min_{(\overline{\bm{\mathcal{H}}}^{(v)})^{\mathrm{T}}\overline{\bm{\mathcal{ H}}}^{(v)}=\mathbf{I}}-2\mathrm{tr}\left(\overline{\bm{\mathcal{G}}}^{(v)}( \overline{\bm{\mathcal{H}}}^{(v)})^{\mathrm{T}}\overline{\bm{\mathcal{S}}}^{( v)}\right)-\mu\mathrm{tr}\left((\overline{\bm{\mathcal{H}}}^{(v)})^{\mathrm{T}} \overline{\bm{\mathcal{W}}}_{1}^{(v)}\right)-\rho\mathrm{tr}\left((\overline{ \bm{\mathcal{H}}}^{(v)})^{\mathrm{T}}\overline{\bm{\mathcal{W}}}_{2}^{(v)}\right)\] (14)

where \(\overline{\bm{\mathcal{W}}}_{1}^{(v)}=\overline{\bm{\mathcal{Q}}}^{(v)}- \frac{\overline{\bm{\mathcal{Y}}}_{1}^{(v)}}{\mu}\) and \(\overline{\bm{\mathcal{W}}}_{2}^{(v)}=\overline{\bm{\mathcal{J}}}^{(v)}- \frac{\overline{\bm{\mathcal{Y}}}_{2}^{(v)}}{\rho}\).

and it also can be reduced to:

\[\max_{(\overline{\bm{\mathcal{H}}}^{(v)})^{\mathrm{T}}\overline{\bm{\mathcal{ H}}}^{(v)}=\mathbf{I}}\mathrm{tr}\left((\overline{\bm{\mathcal{H}}}^{(v)})^{ \mathrm{T}}\overline{\bm{\mathcal{B}}}^{(v)}\right)\] (15)

where \(\overline{\bm{\mathcal{B}}}^{(v)}=2\overline{\bm{\mathcal{S}}}^{(v)}\overline {\bm{\mathcal{G}}}^{(v)}+\mu\overline{\bm{\mathcal{W}}}_{1}^{(v)}+\rho \overline{\bm{\mathcal{W}}}_{2}^{(v)}\).

To solve (15), we introduce the following Theorem:

**Theorem 1**.: _Given \(\mathbf{G}\) and \(\mathbf{P}\), where \(\mathbf{G}(\mathbf{G})^{\mathrm{T}}=\mathbf{I}\) and \(\mathbf{P}\) has the singular value decomposition \(\mathbf{P}=\bm{\Lambda}\mathbf{S}(\mathbf{V})^{\mathrm{T}}\), then the optimal solution of_

\[\max_{\mathbf{G}(\mathbf{G})^{\mathrm{T}}=\mathbf{I}}\mathrm{tr}(\mathbf{G} \mathbf{P})\] (16)

_is \(\mathbf{G}^{*}=\mathbf{V}[\mathbf{I},\mathbf{0}](\bm{\Lambda})^{\mathrm{T}}\)._

Proof.: From the SVD \(\mathbf{P}=\bm{\Lambda}\mathbf{S}(\mathbf{V})^{\mathrm{T}}\) and together with (16), it is evident that

\[\mathrm{tr}(\mathbf{G}\mathbf{P})=\mathrm{tr}(\mathbf{G}\bm{\Lambda}\mathbf{S }(\mathbf{V})^{\mathrm{T}})=\mathrm{tr}(\mathbf{S}(\mathbf{V})^{\mathrm{T}} \mathbf{G}\bm{\Lambda})=\mathrm{tr}(\mathbf{S}\mathbf{H})=\sum_{i}s_{ii}h_{ii},\] (17)

where \(\mathbf{H}=(\mathbf{V})^{\mathrm{T}}\mathbf{G}\bm{\Lambda}\), \(s_{ii}\) and \(h_{ii}\) are the \((i,i)\) elements of \(\mathbf{S}\) and \(\mathbf{H}\), respectively. It can be easily verified that \(\mathbf{H}(\mathbf{H})^{\mathrm{T}}=\mathbf{I}\), where \(\mathbf{I}\) is an identity matrix. Therefore \(-1\leqslant h_{ii}\leqslant 1\) and \(s_{ii}\geqslant 0\), Thus we have:

\[\mathrm{tr}(\mathbf{G}\mathbf{P})=\sum_{i}s_{ii}h_{ii}\leqslant\sum_{i}s_{ii}.\] (18)

The equality holds when \(\mathbf{H}\) is an identity matrix. \(\mathrm{tr}(\mathbf{G}\mathbf{P})\) reaches the maximum when \(\mathbf{H}=[\mathbf{I},\mathbf{0}]\). 

So the solution of (15) is:

\[\overline{\bm{\mathcal{H}}}^{(v)}=\overline{\bm{\Lambda}}^{(v)}(\overline{V}^{ (v)})^{\mathrm{T}}\] (19)

where \(\overline{\bm{\Lambda}}^{(v)}\) and \(\overline{\bm{V}}^{(v)}\) can be obtained by SVD \(\overline{\bm{\mathcal{B}}}^{(v)}=\overline{\bm{\Lambda}}^{(v)}\mathbf{X}( \overline{\bm{V}}^{(v)})^{\mathrm{T}}\)

\(\bullet\)**Solve \(\bm{\mathcal{Q}}\) with fixed \(\bm{\mathcal{H}},\bm{\mathcal{G}},\bm{\mathcal{J}}\).** (7) becomes:

\[\min_{\overline{\bm{\mathcal{Q}}}\geqslant 0}\frac{\mu}{2}\bigg{\|}\bm{ \mathcal{H}}-\bm{\mathcal{Q}}+\frac{\bm{\mathcal{Y}}_{1}}{\mu}\bigg{\|}_{F}^{2}\] (20)

(20) is obviously equivalent to:

\[\min_{\overline{\bm{\mathcal{Q}}}\geqslant 0}\frac{\mu}{2}\bigg{\|}\bm{ \mathcal{Q}}-(\bm{\mathcal{H}}+\frac{\bm{\mathcal{Y}}_{1}}{\mu})\bigg{\|}_{F}^{2}\] (21)According to [37], the solution of (21) is:

\[\bm{\mathcal{Q}}=\left(\bm{\mathcal{H}}+\frac{\bm{\mathcal{Y}}_{1}}{\mu}\right)_{+}\] (22)

\(\bullet\)**Solve \(\bm{\mathcal{J}}\) with fixed \(\bm{\mathcal{Q}},\bm{\mathcal{H}},\bm{\mathcal{G}}\).** (7) becomes:

\[\min\lambda\|\bm{\mathcal{J}}\|_{\bm{\mathcal{G}}}^{p}+\frac{\rho}{2}\bigg{\|} \bm{\mathcal{H}}-\bm{\mathcal{J}}+\frac{\bm{\mathcal{Y}}_{2}}{\rho}\bigg{\|}_ {F}^{2},\] (23)

after completing the square regarding \(\bm{\mathcal{J}}\), we can deduce

\[\bm{\mathcal{J}}^{*}=\arg\min\frac{1}{2}\left\|\bm{\mathcal{H}}+\frac{\bm{ \mathcal{Y}}_{2}}{\rho}-\bm{\mathcal{J}}\right\|_{F}^{2}+\frac{\lambda}{\rho} \|\bm{\mathcal{J}}\|_{\bm{\mathcal{G}}}^{p},\] (24)

which has a closed-form solution as Lemma 2[12]:

**Lemma 2**.: _Let \(\mathcal{Z}\in\mathbb{R}^{n_{1}\times n_{2}\times n_{3}}\) have a t-SVD \(\mathcal{Z}=\mathcal{U}*\mathcal{S}*\mathcal{V}^{\mathrm{T}}\), then the optimal solution for_

\[\min_{\mathcal{X}}\tfrac{1}{2}\left\|\mathcal{X}-\mathcal{Z}\right\|_{F}^{2}+ \tau\left\|\mathcal{X}\right\|_{\bm{\mathcal{G}}}^{p}.\] (25)

_is \(\mathcal{X}^{*}=\Gamma_{\tau}(\mathcal{Z})=\mathcal{U}*\mathrm{ift}(P_{\tau} (\overline{\mathcal{Z}}))*\mathcal{V}^{\mathrm{T}}\), where \(P_{\tau}(\overline{\mathcal{Z}})\) is an f-diagonal 3rd-order tensor, whose diagonal elements can be found by using the GST algorithm introduced in [12]._

Now the solution of (24) is:

\[\bm{\mathcal{J}}^{*}=\Gamma_{\frac{\lambda}{\rho}}(\bm{\mathcal{H}}+\frac{ \bm{\mathcal{Y}}_{2}}{\rho}).\] (26)

Finally, the optimization procedure for Multi-View Clustering via Orthogonal non-negative Tensor Factorization (Orth-NTF) is outlined in Algorithm 1.

``` Input: Data matrices \(\{\mathbf{X}^{(v)}\}_{v=1}^{V}\in\mathbb{R}^{N\times d_{v}}\); anchors numbers \(m\); cluster number \(K\). Output: Cluster labels \(\mathbf{Y}\) of each data points. Initialize:\(\mu=10^{-5}\), \(\rho=10^{-5}\), \(\eta=1.6\), \(\bm{\mathcal{Y}}_{1}=0\), \(\bm{\mathcal{Y}}_{2}=0\), \(\overline{\mathbf{Q}}^{(*)}\) is identity matrix;
1: Compute graph matrix \(\mathbf{S}^{(v)}\) of each views;
2:while not condition do
3: Update \(\overline{\bm{\mathcal{G}}}^{(*)}\) by solving (11);
4: Update \(\overline{\bm{\mathcal{H}}}^{(*)}\) by solving (19);
5: Update \(\overline{\bm{\mathcal{G}}}^{(*)}\) by solving (22);
6: Update \(\bm{\mathcal{J}}\) by using (24);
7: Update \(\bm{\mathcal{Y}}_{1}\), \(\bm{\mathcal{Y}}_{2}\), \(\mu\) and \(\rho\): \(\bm{\mathcal{Y}}_{1}=\bm{\mathcal{Y}}_{1}+\mu(\bm{\mathcal{H}}-\bm{\mathcal{ Q}})\), \(\mathcal{Y}_{2}=\bm{\mathcal{Y}}_{2}+\mu(\bm{\mathcal{H}}-\bm{\mathcal{J}})\), \(\mu=\min(\eta\mu,10^{13})\);
8:endwhile
9: Calculate the \(K\) clusters by using \(\mathbf{H}=\sum_{v=1}^{V}\mathbf{H}^{(v)}/V\);
10:return Clustering result (The position of the largest element in each row of the indicator matrix is the label of the corresponding sample). ```

**Algorithm 1** Multi-View Clustering via Orthogonal non-negative Tensor Factorization (Orth-NTF)

### Convergence Analysis

**Theorem 2**.: _[Convergence Analysis of Algorithm 1] Let \(\mathcal{P}_{k}=\{\bm{\mathcal{Q}}_{k},\bm{\mathcal{H}}_{k},\bm{\mathcal{G}}_ {k},\bm{\mathcal{J}}_{k},\bm{\mathcal{Y}}_{2,k},\bm{\mathcal{Y}}_{1,k}\},\;1 \leq k<\infty\) in (7) be a sequence generated by Algorithm 1, then_

1. \(\mathcal{P}_{k}\) _is bounded with the assumption_ \(\lim_{k\to 0}\max\{\mu_{k},\rho_{k}\}(\bar{\mathcal{H}}_{k+1}^{(v)}-\bar{ \mathcal{H}}_{k}^{(v)})=0\)_;_
2. _Any accumulation point of_ \(\mathcal{P}_{k}\) _is a stationary KKT point of (_7_)._

The proof will be provided in the appendix and we need to mention that the KKT conditions can be used to determine the stop conditions for Algorithm 1, which are \(\|\bm{\mathcal{Q}}_{k}-\bm{\mathcal{H}}_{k}\|_{\infty}\leq\varepsilon\), \(\|\bm{\mathcal{Q}}_{k}-\bm{\mathcal{J}}_{k}\|_{\infty}\leq\varepsilon\).

### Complexity Analysis

For Orth-NTF, the storage requirements for \(\mathcal{G}\), \(\mathcal{H}\), \(\mathcal{Q}\), \(\mathcal{J}\), \(\mathcal{Y}_{\mathbf{1}}\) and \(\mathcal{Y}_{\mathbf{2}}\) have complexities of \(\mathcal{O}(V(m+k)n)\), \(\mathcal{O}(V(n+k)k)\), \(\mathcal{O}(Vnk)\), \(\mathcal{O}(Vnk)\), \(\mathcal{O}(Vnk)\) and \(\mathcal{O}(Vnk)\), respectively. Combining these, the total storage complexity for Orth-NTF is \(\mathcal{O}(Vnm+vk^{2}+6Vnk)\).

For the computational complexity, the process of constructing \(\mathcal{S}\) has a computational complexity of \(\mathcal{O}(Vnmd+Vnm\log(m))\). When updating the four variables, \(\mathcal{G}\), \(\mathcal{H}\), \(\mathcal{Q}\) and \(\mathcal{J}\), their respective computational complexities are \(\mathcal{O}(Vnmd+Vnm\log(m))\), \(\mathcal{O}(Vm^{2}k+Vmk^{2})\), \(\mathcal{O}(Vnk)\) and \(\mathcal{O}(2Vnk\log(Vk)+V^{2}kn)\). Given that \(m\), \(n\), \(k\) and \(V\) are relatively small constants, the primary computational cost associated with updating the variables stands at \(\mathcal{O}(Vnkm+Vm^{2}k)\). Summing it all up, the overall computational complexity of our proposed method is \(\mathcal{O}(Vnmd+Vm^{2}k)\).

## 5 Experiments

In this section, we demonstrate the performance of our proposed method through extensive experiments. It is compared with plenty of state-of-art multi-view clustering algorithms on some multi-view datasets. We evaluate the clustering performance by applying 7 metrics used widely, _i.e._, 1) ACC; 2) NMI; 3) Purity; 4) PRE; 5) REC; 6) F-score; and 7) ARI. The higher the value the better the clustering results for all metrics mentioned above. Detailed experimental configurations and hyper-parameters on each dataset are in the appendix.

### Datasets and Compared Baselines Methods

The following multi-view datasets are selected to examine our proposed method. The details of the datasets are shown in Table 1. **MSRC**[34]; **HandWritten4**[10]; **Mnist4**[6]; **AWA**[11]; **Reuters**[2]; **Noisy MNIST**[32]; We choose the following 8 state-of-art multi-view clustering algorithms to compare with our proposed methods: **AMGL**[25]; **MVGL**[40]; **CSMSC**[24]; **GMC**[30]; **LMVSC**[17]; **SMSC**[16]; **SFMC**[21] **FMCNOF**[37]; **FPMVS-CAG**[31]; **ETLMSC**[35]; **MSC-BG**[38];

### Experiments Result

The clustering performances are listed in Table 2 and Table 3. They contain four medium-scale datasets and two large-scale datasets. The corresponding experimental configurations and descriptions are included in the appendix. It is clear that our algorithm outperforms the other baseline algorithms on most of the datasets.

This advantage may stem from the fact that our model directly factorizes the tensorized anchor graph--comprised of anchor graphs from various views--into the product of two non-negative tensors, one being an index tensor. As a result, our model effectively captures both the spatial structural information and the complementary data present in the anchor graphs from different

\begin{table}
\begin{tabular}{c c c c c} \hline \hline \#Dataset & \#Samples & \#View & \#Class & \#Feature \\ \hline MSRC & 210 & 5 & 7 & 24,576,512, 256,254 \\ HandWritten4 & 2000 & 4 & 10 & 76,216, 47, 6 \\ Mnist4 & 4000 & 3 & 4 & 30, 9, 30 \\ Reuters & 18758 & 5 & 6 & 21531, 24892, 34251, 15506, 11547 \\ Noisy MNIST & 50000 & 2 & 10 & 784, 784 \\ \hline \hline \end{tabular}
\end{table}
Table 1: Multi-view datasets used in our experiments

Figure 3: Convergence experiments on MSRC, HandWritten4, Mnist4 and AWA.

[MISSING_PAGE_FAIL:9]

[MISSING_PAGE_FAIL:10]

## References

* [1] Alashwal, H., El Halaby, M., Crouse, J. J., Abdalla, A., and Moustafa, A. A. The application of unsupervised clustering methods to alzheimer's disease. _Frontiers in computational neuroscience_, 13:31, 2019.
* [2] Apte, C., Damerau, F., and Weiss, S. M. Automated learning of decision rules for text categorization. _ACM Trans. Inf. Syst._, 12(3):233-251, 1994.
* [3] Cai, D., He, X., Han, J., and Huang, T. S. Graph regularized nonnegative matrix factorization for data representation. _IEEE transactions on pattern analysis and machine intelligence_, 33(8):1548-1560, 2010.
* [4] Chang, W.-Y., Wei, C.-P., and Wang, Y.-C. F. Multi-view nonnegative matrix factorization for clothing image characterization. In _2014 22nd International Conference on Pattern Recognition_, pp. 1272-1277. IEEE, 2014.
* [5] Cichocki, A., Zdunek, R., et al. Multilayer nonnegative matrix factorisation. _ELECTRONICS LETTERS-IEE_, 42(16):947, 2006.
* [6] Deng, L. The MNIST database of handwritten digit images for machine learning research [best of the web]. _IEEE Signal Process. Mag._, 29(6):141-142, 2012.
* [7] Ding, C., He, X., and Simon, H. D. On the equivalence of nonnegative matrix factorization and spectral clustering. In _Proceedings of the 2005 SIAM international conference on data mining_, pp. 606-610. SIAM, 2005.
* [8] Ding, C., Li, T., Peng, W., and Park, H. Orthogonal nonnegative matrix t-factorizations for clustering. In _Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining_, pp. 126-135, 2006.
* [9] Ding, C. H., Li, T., and Jordan, M. I. Convex and semi-nonnegative matrix factorizations. _IEEE transactions on pattern analysis and machine intelligence_, 32(1):45-55, 2008.
* [10] Dua, D. and Graff, C. UCI machine learning repository, 2017. URL http://archive.ics.uci.edu/ml.
* [11] Fu, Y., Hospedales, T. M., Xiang, T., and Gong, S. Transductive multi-view zero-shot learning. _IEEE transactions on pattern analysis and machine intelligence_, 37(11):2332-2345, 2015.
* [12] Gao, Q., Zhang, P., Xia, W., Xie, D., Gao, X., and Tao, D. Enhanced tensor rpca and its application. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 43(6):2133-2140, 2020.
* [13] Greene, D. and Cunningham, P. A matrix factorization approach for integrating multiple data views. In _Joint European conference on machine learning and knowledge discovery in databases_, pp. 423-438. Springer, 2009.
* [14] Han, J., Song, K., Nie, F., and Li, X. Bilateral k-means algorithm for fast co-clustering. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 31, 2017.
* [15] He, M., Yang, Y., and Wang, H. Learning latent features for multi-view clustering based on nmf. In _International Joint Conference on Rough Sets_, pp. 459-469. Springer, 2016.
* [16] Hu, Z., Nie, F., Wang, R., and Li, X. Multi-view spectral clustering via integrating nonnegative embedding and spectral embedding. _Information Fusion_, 55:251-259, 2020.
* [17] Kang, Z., Zhou, W., Zhao, Z., Shao, J., Han, M., and Xu, Z. Large-scale multi-view subspace clustering in linear time. In _Proceedings of the AAAI conference on Artificial Intelligence_, volume 34, pp. 4412-4419, 2020.
* [18] Kilmer, M. E. and Martin, C. D. Factorization strategies for third-order tensors. _Linear Algebra and its Applications_, 435(3):641-658, 2011.
* [19] Lee, D. D. and Seung, H. S. Learning the parts of objects by non-negative matrix factorization. _Nature_, 401(6755):788-791, 1999.
* [20] Lewis, A. S. and Sendov, H. S. Nonsmooth analysis of singular values. part i: Theory. _Set-Valued Analysis_, 13(3):213-241, 2005.
* [21] Li, X., Zhang, H., Wang, R., and Nie, F. Multiview clustering: A scalable and parameter-free bipartite graph fusion method. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 44(1):330-344, 2022.

* [22] Li, Y., Nie, F., Huang, H., and Huang, J. Large-scale multi-view spectral clustering via bipartite graph. In _Twenty-Ninth AAAI Conference on Artificial Intelligence_, 2015.
* [23] Liu, J., Wang, C., Gao, J., and Han, J. Multi-view clustering via joint nonnegative matrix factorization. In _Proceedings of the 2013 SIAM international conference on data mining_, pp. 252-260. SIAM, 2013.
* [24] Luo, S., Zhang, C., Zhang, W., and Cao, X. Consistent and specific multi-view subspace clustering. In _Thirty-second AAAI conference on artificial intelligence_, 2018.
* [25] Nie, F., Li, J., Li, X., et al. Parameter-free auto-weighted multiple graph learning: a framework for multiview clustering and semi-supervised classification. In _IJCAI_, pp. 1881-1887, 2016.
* [26] Oyelade, O. J., Oladipupo, O. O., and Obagbuwa, I. C. Application of k means clustering algorithm for prediction of students academic performance. _arXiv preprint arXiv:1002.2425_, 2010.
* [27] Pauca, V. P., Piper, J., and Plemmons, R. J. Nonnegative matrix factorization for spectral data analysis. _Linear algebra and its applications_, 416(1):29-47, 2006.
* [28] Wang, H., Nie, F., Huang, H., and Makedon, F. Fast nonnegative matrix tri-factorization for large-scale data co-clustering. In _Twenty-Second International Joint Conference on Artificial Intelligence_, 2011.
* [29] Wang, H., Yang, Y., and Li, T. Multi-view clustering via concept factorization with local manifold regularization. In _2016 IEEE 16th International Conference on Data Mining (ICDM)_, pp. 1245-1250. IEEE, 2016.
* [30] Wang, H., Yang, Y., and Liu, B. Gmc: Graph-based multi-view clustering. _IEEE Transactions on Knowledge and Data Engineering_, 32(6):1116-1129, 2019.
* [31] Wang, S., Liu, X., Zhu, X., Zhang, P., Zhang, Y., Gao, F., and Zhu, E. Fast parameter-free multi-view subspace clustering with consensus anchor guidance. _IEEE Transactions on Image Processing_, 31:556-568, 2021.
* [32] Wang, W., Arora, R., Livescu, K., and Bilmes, J. A. On deep multi-view representation learning. In _ICML_, volume 37, pp. 1083-1092, 2015.
* [33] Wang, Z., Kong, X., Fu, H., Li, M., and Zhang, Y. Feature extraction via multi-view non-negative matrix factorization with local graph regularization. In _2015 IEEE International conference on image processing (ICIP)_, pp. 3500-3504. IEEE, 2015.
* [34] Winn, J. M. and Jojic, N. LOCUS: learning object classes with unsupervised segmentation. pp. 756-763, 2005.
* [35] Wu, J., Lin, Z., and Zha, H. Essential tensor learning for multi-view spectral clustering. 28(12):5910-5922, 2019.
* [36] Xie, Y., Gu, S., Liu, Y., Zuo, W., Zhang, W., and Zhang, L. Weighted schatten \(p\)-norm minimization for image denoising and background subtraction. _IEEE Trans. Image Process._, 25(10):4842-4857, 2016.
* [37] Yang, B., Zhang, X., Nie, F., Wang, F., Yu, W., and Wang, R. Fast multi-view clustering via nonnegative and orthogonal factorization. _IEEE transactions on image processing_, 30:2575-2586, 2021.
* [38] Yang, H., Gao, Q., Xia, W., Yang, M., and Gao, X. Multiview spectral clustering with bipartite graph. _IEEE Transactions on Image Processing_, 31:3591-3605, 2022.
* [39] Zha, Z., Yuan, X., Wen, B., Zhou, J., Zhang, J., and Zhu, C. A benchmark for sparse coding: When group sparsity meets rank minimization. _IEEE Transactions on Image Processing_, 29:5094-5109, 2020.
* [40] Zhan, K., Zhang, C., Guan, J., and Wang, J. Graph learning for multiview clustering. _IEEE transactions on cybernetics_, 48(10):2887-2895, 2017.

Proof of Convergence

### Proof of the 1st part

**Lemma 3** (Proposition 6.2 of [20]).: _Suppose \(F:\mathbb{R}^{n_{1}\times n_{2}}\to\mathbb{R}\) is represented as \(F(X)=f\circ\sigma(X)\), where \(X\in\mathbb{R}^{n_{1}\times n_{2}}\) with SVD \(X=U{\rm diag}(\sigma_{1},\ldots,\sigma_{n})V^{\rm T}\), \(n=\min(n_{1},n_{2})\), and \(f\) is differentiable. The gradient of \(F(X)\) at \(X\) is_

\[\frac{\partial F(X)}{\partial X}=U{\rm diag}(\theta)V^{\rm T},\] (27)

_where \(\theta=\frac{\partial f(y)}{\partial y}|_{y=\sigma(X)}\)._

To minimize \(\bm{\bar{\mathcal{Q}}}^{(v)}\) at step \(k+1\) in (21) in the main body, the optimal \(\bm{\bar{\mathcal{Q}}}_{k+1}^{(v)}\) needs to satisfy the first-order optimal condition

\[\bm{\bar{\mathcal{Q}}}_{k+1}^{(v)}=\bm{\bar{\mathcal{H}}}_{k}^{(v)}+\frac{ \bm{\bar{\mathcal{Y}}}_{1,k}}{\mu_{k}}.\]

By using the updating rule \(\bm{\bar{\mathcal{Y}}}_{1,k+1}^{(v)}=\bm{\bar{\mathcal{Y}}}_{1,k}^{(v)}+\mu_{ k}(\bm{\bar{\mathcal{H}}}_{k}^{(v)}-\bm{\bar{\mathcal{Q}}}_{k}^{(v)})\), we have

\[\frac{\bm{\bar{\mathcal{Y}}}_{1,k+1}^{(v)}}{\mu_{k}}+(\bm{\bar{\mathcal{Q}}}_{ k}^{(v)}-\bm{\bar{\mathcal{Q}}}_{k+1}^{(v)})=0.\]

According to our assumption \(\lim_{k\to 0}\mu_{k}(\bm{\bar{\mathcal{Q}}}_{k+1}^{(v)}-\bm{\bar{\mathcal{Q}}}_{ k}^{(v)})=0\), we know \(\bm{\mathcal{Y}}_{1,k+1}\) is bounded.

To minimize \(\mathcal{J}\) at step \(k+1\) in (24) in the main body, the optimal \(\mathcal{J}_{k+1}\) needs to satisfy the first-order optimal condition

\[\lambda\nabla_{\bm{\mathcal{J}}}\|\bm{\mathcal{J}}_{k+1}\|_{\bm{\bar{\mathcal{ Q}}}}^{p}+\rho_{k}(\bm{\mathcal{J}}_{k+1}-\bm{\mathcal{H}}_{k+1}-\frac{1}{\rho_{ k}}\bm{\mathcal{Y}}_{2,k})=0.\]

Recall that when \(0<p<1\), in order to overcome the singularity of \((|\eta|^{p})^{\prime}=p\eta/|\eta|^{2-p}\) near \(\eta=0\), we consider for \(0<\epsilon\ll 1\) the approximation

\[\partial|\eta|^{p}\approx\frac{p\eta}{\max\{\epsilon^{2-p},|\eta|^{2-p}\}}.\]

Letting \(\overline{\bm{\mathcal{J}}}^{(i)}=\overline{\bm{U}}^{(i)}{\rm diag}\left( \sigma_{j}(\overline{\bm{\mathcal{J}}}^{(i)})\right)\overline{\bm{\mathcal{V} }}^{(i){\rm H}},\) then it follows from Lemma 3 that

\[\frac{\partial\|\overline{\bm{\mathcal{J}}}^{(i)}\|_{\bm{\bar{\mathcal{Q}}}}^{ p}}{\partial\overline{\bm{\mathcal{J}}}^{(i)}}=\overline{\bm{U}}^{(i)}{\rm diag }\left(\frac{p\sigma_{j}(\overline{\bm{\mathcal{J}}}^{(i)})}{\max\{\epsilon^ {2-p},|\sigma_{j}(\overline{\bm{\mathcal{J}}}^{(i)})|^{2-p}\}}\right)\overline {\bm{\mathcal{V}}}^{(i){\rm H}}.\]

And then one can obtain

\[\frac{p\sigma_{j}(\overline{\bm{\mathcal{J}}}^{(i)})}{\max\{ \epsilon^{2-p},|\sigma_{j}(\overline{\bm{\mathcal{J}}}^{(i)})|^{2-p}\}}\leq \frac{p}{\epsilon^{1-p}}\] \[\Longrightarrow\left\|\frac{\partial\|\overline{\bm{\mathcal{J}}} ^{(i)}\|_{\bm{\bar{\mathcal{Q}}}}^{p}}{\partial\overline{\bm{\mathcal{J}}}^{(i )}}\right\|_{F}^{2}\leq\sum_{i=1}^{n}\frac{p^{2}}{\epsilon^{2(1-p)}}.\]

So \(\frac{\partial\|\overline{\bm{\mathcal{J}}}\|_{\bm{\bar{\mathcal{Q}}}}^{p}}{ \partial\overline{\bm{\mathcal{J}}}}\) is bounded.

Let us denote \(\widetilde{\mathbf{F}}_{V}=\frac{1}{\sqrt{V}}\mathbf{F}_{V}\), \(\mathbf{F}_{V}\) is the discrete Fourier transform matrix of size \(V\times V\), \(\mathbf{F}_{V}^{\rm H}\) denotes its conjugate transpose. For \(\bm{\mathcal{J}}=\overline{\bm{\mathcal{J}}}\times_{3}\widetilde{\mathbf{F}}_{V}\) and using the chain rule in matrix calculus, one can obtain that

\[\nabla_{\bm{\mathcal{J}}}\|\bm{\mathcal{J}}\|_{\bm{\bar{\mathcal{Q}}}}^{p}= \frac{\partial\|\bm{\mathcal{J}}\|_{\bm{\bar{\mathcal{Q}}}}^{p}}{\partial \overline{\bm{\mathcal{J}}}}\times_{3}\widetilde{\mathbf{F}}_{V}^{\rm H}\]

is bounded.

And it follows that

\[\bm{\mathcal{Y}}_{1,k+1}=\bm{\mathcal{Y}}_{2,k}+\rho_{k}(\bm{\mathcal{ H}}_{k+1}-\bm{\mathcal{J}}_{k+1})\] \[\implies\lambda\nabla_{\bm{\mathcal{J}}}\|\bm{\mathcal{J}}_{k+1} \|_{\bm{\mathcal{G}}\bm{\mathcal{G}}}^{p}=\bm{\mathcal{Y}}_{2,k+1},\]

thus \(\bm{\mathcal{Y}}_{2,k+1}\) appears to be bounded.

Moreover, by using the updating rule \(\bm{\mathcal{Y}}_{1}=\bm{\mathcal{Y}}_{1}+\mu(\bm{\mathcal{H}}-\bm{\mathcal{ Q}})\), \(\bm{\mathcal{Y}}_{2}=\bm{\mathcal{Y}}_{2}+\rho(\bm{\mathcal{H}}-\bm{\mathcal{J}})\), we can deduce (\(i=1,2\))

\[\bm{\mathcal{L}}(\bm{\mathcal{Q}}_{k+1},\bm{\mathcal{G}}_{k+1}, \bm{\mathcal{H}}_{k+1},\bm{\mathcal{J}}_{k+1};\bm{\mathcal{Y}}_{i,k})\] (28) \[\leq\bm{\mathcal{L}}(\bm{\mathcal{Q}}_{k},\bm{\mathcal{G}}_{k}, \bm{\mathcal{H}}_{k},\bm{\mathcal{J}}_{k};\bm{\mathcal{Y}}_{i,k})\] \[=\bm{\mathcal{L}}(\bm{\mathcal{Q}}_{k},\bm{\mathcal{G}}_{k},\bm{ \mathcal{H}}_{k},\bm{\mathcal{J}}_{k};\bm{\mathcal{Y}}_{i,k-1})\] \[+\frac{\rho_{k}+\rho_{k-1}}{2\rho_{k-1}^{2}}\|\bm{\mathcal{Y}}_{ 2,k}-\bm{\mathcal{Y}}_{2,k-1}\|_{F}^{2}+\frac{\|\bm{\mathcal{Y}}_{2,k}\|_{F}^ {2}}{2\rho_{k}}-\frac{\|\bm{\mathcal{Y}}_{2,k-1}\|_{F}^{2}}{2\rho_{k-1}}\] \[+\frac{\mu_{k}+\mu_{k-1}}{2\mu_{k-1}^{2}}\|\bm{\mathcal{Y}}_{1,k} -\bm{\mathcal{Y}}_{1,k-1}\|_{F}^{2}+\frac{\|\bm{\mathcal{Y}}_{1,k}\|_{F}^{2}}{ 2\mu_{k}}-\frac{\|\bm{\mathcal{Y}}_{1,k-1}\|_{F}^{2}}{2\mu_{k-1}}.\]

Thus, summing two sides of (28) from \(k=1\) to \(n\), we have

\[\bm{\mathcal{L}}(\bm{\mathcal{Q}}_{n+1},\bm{\mathcal{G}}_{n+1}, \bm{\mathcal{H}}_{n+1},\bm{\mathcal{J}}_{n+1};\bm{\mathcal{Y}}_{i,n})\] \[\leq \bm{\mathcal{L}}(\bm{\mathcal{Q}}_{1},\bm{\mathcal{G}}_{1},\bm{ \mathcal{H}}_{1},\bm{\mathcal{J}}_{1};\bm{\mathcal{Y}}_{i,0}))\] \[+\frac{\|\bm{\mathcal{Y}}_{2,n}\|_{F}^{2}}{2\rho_{n}}-\frac{\| \bm{\mathcal{Y}}_{2,0}\|_{F}^{2}}{2\rho_{0}}+\sum_{k=1}^{n}\left(\frac{\rho_{k }+\rho_{k-1}}{2\rho_{k-1}^{2}}\|\bm{\mathcal{Y}}_{2,k}-\bm{\mathcal{Y}}_{2,k-1 }\|_{F}^{2}\right)\] (29) \[+\frac{\|\bm{\mathcal{Y}}_{1,n}\|_{F}^{2}}{2\mu_{n}}-\frac{\| \bm{\mathcal{Y}}_{1,0}\|_{F}^{2}}{2\mu_{0}}+\sum_{k=1}^{n}\left(\frac{\mu_{k }+\mu_{k-1}}{2\mu_{k-1}^{2}}\|\bm{\mathcal{Y}}_{1,k}-\bm{\mathcal{Y}}_{1,k-1} \|_{F}^{2}\right).\]

Observe that

\[\sum_{k=1}^{\infty}\frac{\rho_{k}+\rho_{k-1}}{2\rho_{k-1}^{2}}<\infty,\sum_{k= 1}^{\infty}\frac{\mu_{k}+\mu_{k-1}}{2\mu_{k-1}^{2}}<\infty,\]

we have the right-hand side of (29) is finite and thus \(\bm{\mathcal{L}}(\bm{\mathcal{Q}}_{n+1},\bm{\mathcal{G}}_{n+1},\bm{\mathcal{H} }_{n+1},\bm{\mathcal{J}}_{n+1};\bm{\mathcal{Y}}_{i,n})\) is bounded. Notice from (7) in the main body

\[\bm{\mathcal{L}}(\bm{\mathcal{Q}}_{n+1},\bm{\mathcal{G}}_{n+1}, \bm{\mathcal{H}}_{n+1},\bm{\mathcal{J}}_{n+1};\bm{\mathcal{Y}}_{i,n})\] \[=\sum_{v=1}^{V}\left\|\bm{\tilde{\mathcal{S}}}^{(v)}-\bm{\bar{ \mathcal{H}}}_{n+1}^{(v)}(\bm{\bar{\mathcal{G}}}_{n+1}^{(v)})^{T}\right\|_{F}^{2}\] \[+\lambda\|\bm{\mathcal{J}}_{n+1}\|_{\bm{\mathcal{G}}\bm{\mathcal{ G}}}^{p}+\frac{\rho_{n}}{2}\|\bm{\mathcal{H}}_{n+1}-\bm{\mathcal{J}}_{n+1}+\frac{\bm{ \mathcal{Y}}_{2,n}}{\rho_{n}}\|_{F}^{2}\] \[+\frac{\mu_{n}}{2}\sum_{v=1}^{V}\|\bm{\bar{\mathcal{H}}}_{n+1}^{( v)}-\bm{\bar{\mathcal{Q}}}_{n+1}^{(v)}+\frac{\bm{\bar{\mathcal{Y}}}_{1,n+1}^{(v)}}{\mu_{n}}\|_{F }^{2},\] (30)

and each term of (30) is nonnegative, following from the boundedness of \(\bm{\mathcal{L}}(\bm{\mathcal{Q}}_{n+1},\bm{\mathcal{G}}_{n+1},\bm{\mathcal{H}}_{n +1},\bm{\mathcal{J}}_{n+1};\bm{\mathcal{Y}}_{i,n})\), we can deduce each term of (30) is bounded. And \(\|\bm{\mathcal{J}}_{n+1}\|_{\bm{\mathcal{G}}\bm{\mathcal{G}}}^{p}\) being bounded implies that all singular values of \(\bm{\mathcal{J}}_{n+1}\) are bounded and hence \(\|\bm{\mathcal{J}}_{n+1}\|_{F}^{2}\) (the sum of squares of singular values) is bounded. Therefore, the sequence \(\{\bm{\mathcal{J}}_{k}\}\) is bounded.

Because

\[\bm{\mathcal{Y}}_{1,k+1}=\bm{\mathcal{Y}}_{1,k}+\mu_{k}(\bm{\mathcal{Q}}_{k}-\bm{ \mathcal{H}}_{k})\implies\bm{\mathcal{H}}_{k}=\bm{\mathcal{Q}}_{k}+\frac{\bm{ \mathcal{Y}}_{1,k+1}-\bm{\mathcal{Y}}_{1,k}}{\mu_{k}},\]

and in light of the boundedness of \(\bm{\mathcal{Q}}_{k},\bm{\mathcal{Y}}_{1,k}\), it is clear that \(\bm{\mathcal{H}}_{k}\) is also bounded.

And from (8) in the main body, it is evident that \(\|\bm{\bar{\mathcal{G}}}_{k}^{(v)}\|_{F}^{2}\leq\|(\bm{\bar{\mathcal{S}}}^{(v)})^{ \mathsf{T}}\|_{F}^{2}\|\bm{\bar{\mathcal{H}}}_{k}^{(v)}\|_{F}^{2}\), so \(\bm{\bar{\mathcal{G}}}_{k}^{(v)}\) is also bounded. So \(\bm{\mathcal{G}}_{k}\) is bounded.

### Proof of the 2nd part

From Weierstrass-Bolzano theorem, there exists at least one accumulation point of the sequence \(\mathcal{P}_{k}\). We denote one of the points \(\mathcal{P}^{*}=\{\boldsymbol{\mathcal{H}}^{*},\boldsymbol{\mathcal{Q}}^{*}, \boldsymbol{\mathcal{G}}^{*},\boldsymbol{\mathcal{J}}^{*},\boldsymbol{\mathcal{ Y}}_{1}^{*},\boldsymbol{\mathcal{Y}}_{2}^{*}\}\). Without loss of generality, we assume \(\{\mathcal{P}_{k}\}_{k=1}^{+\infty}\) converge to \(P^{*}\).

Note that from the updating rule for \(\boldsymbol{\mathcal{Y}}_{1}\), we have

\[\boldsymbol{\mathcal{Y}}_{1,k+1}=\boldsymbol{\mathcal{Y}}_{1,k}+\mu_{k}( \boldsymbol{\mathcal{H}}_{k}-\boldsymbol{\mathcal{Q}}_{k})\Longrightarrow \boldsymbol{\mathcal{Q}}^{*}=\boldsymbol{\mathcal{H}}^{*}.\]

Note that from the updating rule for \(\boldsymbol{\mathcal{Y}}_{2}\), we have

\[\boldsymbol{\mathcal{Y}}_{2,k+1}=\boldsymbol{\mathcal{Y}}_{2,k}+\rho_{k}( \boldsymbol{\mathcal{H}}_{k}-\boldsymbol{\mathcal{J}}_{k})\Longrightarrow \boldsymbol{\mathcal{J}}^{*}=\boldsymbol{\mathcal{H}}^{*}.\]

In the \(\boldsymbol{\bar{\mathcal{G}}}^{(v)}\)-subproblem (8) in the main body, we have

\[\boldsymbol{\bar{\mathcal{G}}}_{k}^{(v)}=(\boldsymbol{\bar{\mathcal{S}}}^{(v )})^{\mathrm{T}}\boldsymbol{\bar{\mathcal{H}}}_{k}^{(v)}\Longrightarrow \boldsymbol{\bar{\mathcal{G}}}^{(v)*}=(\boldsymbol{\bar{\mathcal{S}}}^{(v)})^ {\mathrm{T}}\boldsymbol{\bar{\mathcal{H}}}^{(v)*}.\]

In the \(\boldsymbol{\mathcal{J}}\)-subproblem (24) in the main body, we have

\[\lambda\nabla\boldsymbol{\mathcal{J}}\|\boldsymbol{\mathcal{J}}_{k+1}\|_{ \boldsymbol{\bar{\Theta}}}^{p}=\boldsymbol{\mathcal{Y}}_{2,k}\Longrightarrow \boldsymbol{\mathcal{Y}}_{1}^{*}=\lambda\nabla_{\boldsymbol{\mathcal{J}}}\| \boldsymbol{\mathcal{J}}^{*}\|_{\boldsymbol{\bar{\Theta}}}^{p}.\]

Therefore, one can see that the sequences \(\boldsymbol{\mathcal{H}}^{*}\), \(\boldsymbol{\mathcal{Q}}^{*},\boldsymbol{\mathcal{G}}^{*},\boldsymbol{ \mathcal{J}}^{*},\boldsymbol{\mathcal{Y}}_{1}^{*},\boldsymbol{\mathcal{Y}}_{2}^ {*}\) satisfy the KKT conditions of the Lagrange function (7) in the main body.

## Appendix B Anchor Selection And Graph Construction

Inspired by [21], we adopt directly alternate sampling (DAS) to select anchors.

First of all, with the given data matrices \(\{\mathbf{X}^{(v)}\}_{v=1}^{V}\), we concatenate the data matrix of each view along the feature dimension. The connected feature matrix \(\mathbf{X}\in\mathbb{R}^{n\times d}\) can be represented as \(\mathbf{X}=[\mathbf{X}^{(1)};\mathbf{X}^{(2)};\cdots;\mathbf{X}^{(v)}]\), where \(d\) is the sum of the number of features in each view. Let \(\theta_{i}\) represent the \(i\)-th sample of the \(d\)-dimensional features, which can be calculated as

\[\theta_{i}=\sum_{j=1}^{dT}Tra(X_{ij}),\] (31)

where \(dT=\sum_{v=1}^{V}d_{v}\), and \(Tra(\cdot)\) represents the transformation of the raw features. Specifically, if the features are negative, we process the features of each dimension by subtracting the minimum value in each dimension. Then we obtain the score vector \(\boldsymbol{\theta}=[\theta_{1},\theta_{2},\cdots,\theta_{n}]\in\mathbb{R}^{n}\). We choose the point where the maximum score is located as the anchor. The position of the largest score is

\[Index=\arg\max_{i}\theta_{i}.\] (32)

Then the 1st anchor of the \(v\)-th view is \(b_{1}^{(v)}=x_{Index}^{(v)}\).

After that, let \(\theta_{Index}\) be the score of the anchor selected from the last round, then we normalize the score of each sample by:

\[\theta_{i}\leftarrow\frac{\theta_{i}}{\max\boldsymbol{\theta}},(i=1,2,\cdots,n)\] (33)

Then the score \(\theta_{i}\) can be updated as

\[\theta_{i}\leftarrow\theta_{i}\times(1-\theta_{i}).\] (34)

Finally, we repeat (32) - (34) \(m\) times to select \(m\) anchors. After selecting \(m\) anchors, we construct an anchor graph of each view \(\mathbf{S}^{(v)}\), in the same way, as [21].

[MISSING_PAGE_FAIL:16]

Figure 5: Time (sec.) with different number of anchors on MSRC, HandWritten4, Mnist4 and AWA.

Figure 6: The influence of \(p\) on clustering results on MSRC, HandWritten4, Mnist4 and AWA.

Figure 7: The influence of \(\lambda\) and \(p\) on clustering results on MSRC, HandWritten4, Mnist4 and AWA.