[MISSING_PAGE_FAIL:1]

## 1 Introduction

With the exhilarating progress in foundation models across the vision and language domains, such as GPT4(V) [(30)], DALLE-3 [(31)], SAM [(19)], and LLaMA [(38)], _etc._, we have reached a stage where deep learning models achieve remarkable performances on both vision and language domains [(5; 22)]. Specifically, models like GPT-4(V) [(30)] have showcased human-level perception and reasoning skills [(46)].

Despite their impressive capabilities in information memorization, processing, and reasoning, these models tend to be specialized for specific output types. However, their output types are limited to language for GPT, images for DALLE, masks for SAM, _etc._ In this work, we aim to leverage the privileged properties of foundation models' embeddings to expand their output space (e.g., extend to pixel-level outputs), unlocking their potential for interleaved understanding and reasoning.

To accomplish this, we introduce an INterface for Foundation models' embeDdings (FIND), which utilizes the pre-trained foundational model embeddings to jointly handle downstream tasks of varying granularities (from pixel to image) in an interleaved manner. As illustrated in Fig.2.1, the _FIND_ interface processes embeddings from vision and language foundation models, and outputs segmentation, grounding, and retrieval results.

As all vision-language tasks are trained uniformly in _FIND_, an interleaved shared embedding space is created where vision and language references can be interchanged and augmented. For example, in Fig.2.2, during mapping an interleaved representation loosens the single-modality constraint on the source and target domain. And during reasoning, interleaved sequences enhance information exchange between vision and language compared to multimodal sequences.

To effectively align and evaluate the interleaved embedding space, we construct a new dataset named FIND-Bench. This dataset uses COCO images and includes new annotations for integrated grounding and segmentation. These annotations are generated by GPT-4, which, despite not processing visual input, can directly link specific image segments and annotation IDs with generated descriptions (e.g., <id>(the golden retriever)...). This unique capability enables the creation of training and evaluation datasets for retrieval and grounding in an interleaved context.

In summary, we claim the following contributions:

* We introduce the _FIND_ interface that is is generalizable, flexible, and extendable to various downstream tasks and foundation models.
* Through the effective training scheme of _FIND_, an interleaved shared embedding space is created interfacing foundation models.
* We propose a new Benchmark, _FIND_-Bench, which includes new training and evaluation ground truths for interleave segmentation and retrieval.
* Our model achieves SoTA performance on interleave retrieval and grounding and shows better or comparable performance on generic, interactive, grounded segmentation and image-text retrieval.

## 2 Related Work

**Foundation Models.** Recent years have seen a speedy evolution of foundation models in diverse areas such as computer vision [(47)], natural language processing [(39; 10; 4; 30)], and their interactions [(1;

Figure 2: (1) The concept of interfacing foundation models embedding, the black arrow means active attached modules and the gray arrow means the option that it can switch to. On the right, we show the difference of Multimodal and Interleave (2.a) in the context of embeddings matching; (2.b) in the context of embeddings interaction for reasoning and generation.

23; 44). For example, GPT-3 (4) heralds breakthroughs in natural language understanding and generation tasks, As a vision foundation model, Florence [47; 42] can be easily adapted for various computer vision tasks, such as classification, retrieval, object detection, etc.Flamingo (1) bridges powerful pre-trained vision-only and language-only models by token fusion with cross-attention. BLIP-2 (23) proposes an efficient pretraining strategy that bootstraps vision-language pre-training with a lightweight Q-Former in two stages. Different from previous multi-modal approaches, such as Flamingo (1), LLaVA (26) and Q-Former (BLIP-2) (23) that feed the vision foundation model output into a language decoder and use the LLM as an interpreter, our goal is to interface foundation model embeddings so that LLMs and vision models can be unified in the embedding space.

**Interleaved Image-Text Understanding.** Previous works have explored interleaved visual understanding in the context of visual question answering, visual dialogue, image captioning, and interleaved image retrieval (20; 13; 1). In addition, recent works (48) explore contextual detection that associates phrases with visual content in a sentence. We notice that these earlier works, though reveal interleaved capabilities for image understanding, lack an evaluation benchmark, as well as a complete training dataset. [(51; 21; 2)] propose a new benchmark on interleaved generation and understanding of image and document level, while there is no benchmark available for the interleaved tasks between interactive image parts and phrases. To this end, we introduce the interleaved segmentation and interleaved retrieval tasks with our carefully designed benchmark _FIND_-Bench, which we believe to be essential for the field.

**Image Understanding.** Vision Transformers [(16; 37; 40; 36; 41; 12; 15; 49; 33; 34)] have dominated a wide range of key image understanding tasks, such as image retrieval, detection, and segmentation. Some multimodal methods [(7; 24; 50)] have shown good performance for retrieval tasks. On the other hand, open-vocabulary segmentation methods have recently drawn much attention, including generic segmentation [(6; 53; 11)], interactive segmentation [(14; 19)] that separates objects by actively integrating user inputs, and grounded segmentation [(53; 52)] that grounds object segments from language descriptions. We notice that there is currently no available work that achieves image-level retrieval, pixel-level segmentation, and interleaved vision-language understanding in a single model. In this work, we propose _FIND_ as a unified interface that can support all the above tasks, while maintaining good performance, and further enabling two new tasks of interleaved segmentation and interleaved retrieval. We unify these tasks by interfacing foundation models' embeddings.

## 3 Method

Foundation models such as CLIP (32), SAM (19), LLaMA (38), etc. can process vision or language inputs for reasoning, understanding, and generation. The embeddings generated by these models contain rich and structured information [(35; 3)], making them extremely well-suited for understanding tasks. Aligned with the Platonic Representation Hypothesis (17), we believe foundation models can easily communicate with each other. Therefore, we designed the FIND interface to project vision and language embeddings from foundation models into a unified space. The created space enhances both multimodal and interleaved understanding.

Since no prior benchmark exists for interleave understanding, we believe it is meaningful to formally define the interleave retrieval and segmentation problems and create a dataset for benchmarking them.

### _Find_ Benchmark

Our new benchmark supports two tasks: interleave retrieval and interleave grounding. It evaluates both dataset-level and image-level interleave alignment, focusing on reasoning and matching capabilities. Additionally, we created training and evaluation datasets to further enhance interleave understanding.

#### 3.1.1 Task Definition

**Interleave Retrieval 1.** An interleave entry (\(E\)) consists of a sequence of images (I), texts (T), and connections (C), and can be represented as \(E=\langle N_{1},N_{2},\ldots,N_{n}\mid N_{i}\in\{I,T,C\}\rangle\), where \(\langle\cdot\rangle\) is an ordered sequence. The bottom part of the Table. 1 clearly illustrates an example of an interleave entry. We denote the source domain (\(\mathcal{D}_{s}\)) of interleave retrieval as \(\mathcal{D}_{s}=\{E_{1},E_{2},\ldots,E_{n}\}\), as shown in Fig. 3.1 (Left), and the target domain (\(\mathcal{D}_{t}\)) as \(\mathcal{D}_{t}=\{I_{1},I_{2},\ldots,I_{n}\}\), as shown in Fig. 3.1 (Right). The task of interleave retrieval is to find the closest entry \(I_{*}\in\mathcal{D}_{t}\) for each \(E\in\mathcal{D}_{s}\), excluding itself. Formally, we define this as \(\forall E\in\mathcal{D}_{s},\quad I_{*}=\arg\max_{I\in\mathcal{D}_{t},\,I\notin E} \textbf{sim}(E,I)\).

Footnote 1: Unless we stated as interleave text retrieval, we refer to interleave visual retrieval as Fig. 3.1 shown.

**Context for Image**

1. **GT**: Ground Truth image caption labeled by human.
2. **PD**: Pseudo image Description generated by VLM model.
3. **Box**: All Ground truth bounding box labeled by human.
4. **SI**: Segment Information for each box area including index, bbox, category, descriptions, etc.
5. **SP**: Segment Proposal for the generated description.

**Prompt for GPT4 Engine**

":Generate image captions with grounded entities and attributes with the following information:

ground truth image captions: <[ ]>,

pseudo image description: <[ ]>,

ground truth bounding boxes (\(\{x_{0},y_{0},w,h\}\): \((x_{0},y_{0})\) is the top-left corner; \((w,h)\) is box size);

segment_info: <[ ] >, and segment_proposal: <[ ]>.

An example output format would be: _"[index]<A woman> sitting next to [index]<a handsome man>, with their hands holding together under [index]<the blue sky>."_, where _[index]_ and _<xxxx>_ are associated with the ground truth bounding boxes.

Generated caption constraints: _(1-6) Please refer to appendix.""_.format(**GT**, **PD**, **Box**, **SI**, **SP**)

**Retrieve Visual Sample with SEEM**

Given the search dataset (**Q**) with the segments in all images denoted as (**SD**), we compute all embeddings **S** representing each segment using SEEM (53) with \(\textbf{S}=\text{SEEM}(\textbf{SD})\in\mathbb{R}^{n\times d}\).

Given the similarity matrix \(W=\textbf{S}\times\textbf{S}^{\text{T}}\), where \(W_{ij}\) represents the similarity between segment \(i\) and segment \(j\), the index of the closest segment for segment \(i\) is \(\text{Match}(i)=\arg\max_{j\neq i}W_{i}\) where \(\text{Match}(i)\) returns the index \(j\) that has the highest similarity to segment \(i\).

**Integrated Response of GPT4 and SEEM**

[5721674]

\begin{table}
\begin{tabular}{l} \hline
**Interleave Grounding**2. An image contains a sequence of objects or segments (\(O\)) represented as \(I=\{O_{1},O_{2},\ldots,O_{n}\}\). We provide an example of objects in the bakery image in Fig. 3.2 upper part. These objects form the target domain \(\mathcal{D}_{t}=I=\{O_{1},O_{2},\ldots,O_{n}\}\) for interleave grounding. Unlike interleave retrieval, where interleave entries constitute the source domain, interleave grounding focuses on each component of the interleave entry, with the entities (\(N\)) in the interleave entry forming the source domain. Specifically, \(\mathcal{D}_{s}=\{N_{1},N_{2},\ldots,N_{n}\mid N_{i}\in\{I,T\}\}\subseteq E\). We show an example of interleave entry decomposition in the lower part of Fig. 3.2. The task of interleave grounding is to find the closest entry \(O_{*}\in\mathcal{D}_{t}\) for each \(N\in\mathcal{D}_{s}\), excluding itself. Formally, we define this as \(\forall N\in\mathcal{D}_{s},\quad O_{*}=\arg\max_{O\in\mathcal{D}_{t},O\notin N }\textbf{sim}(N,O)\). \\ \hline \end{tabular}
\end{table}
Table 1: Pseudo code for Data Engine. We show the pipeline to create the _FIND_-Bench from data preparation, text prompting using GPT4, visual prompting with SEEM to integrated result.

associated with the COCO annotation ID [3171126] and a similar playing field (marked in blue) in another image. In this way, the data engine can generate comprehensive interleaved descriptions for each image in the COCO dataset. This is sufficient to build \(\mathcal{D}_{s}\) and \(\mathcal{D}_{t}\) for the interleave retrieval and grounding tasks introduced in Sec. 3.1.1.

### _Find_ Approach

With benchmarks introduced in Sec. 3.1 to evaluate the model's interleaved visual understanding capability, we now present our approach for interfacing foundation models' embeddings on multimodal and interleave understanding. We begin with the preliminaries on task unification and terminology.

#### 3.2.1 Preliminary

**Task Unification.** In this work, we focus on retrieval, grounding, and segmentation in both multimodal and interleaved manners. In Fig. 3, we demonstrate four example tasks: interleave retrieval, interleave grounding, interactive segmentation, and generic segmentation. From an abstract perspective, we can regard all visual understanding tasks as the problem of matching candidates from the source domain to the target domain. Formally, we define the source domain as \(\mathcal{D}_{s}\) and the target domain as \(\mathcal{D}_{t}\). Example elements in \(\mathcal{D}_{s}\) or \(\mathcal{D}_{t}\) includes interleaved entry \(E\), an image \(I\), an object or segment \(O\), texts \(T\). For each visual understanding task \(\mathcal{U}(\mathcal{D}_{s},\mathcal{D}_{t})\), the goal is to find the closest \(Y\in\mathcal{D}_{t}\) for each \(X\in\mathcal{D}_{s}\). Formally we write:

\[\forall X\in\mathcal{D}_{s},\quad Y^{*}=\arg\max_{Y\in\mathcal{D}_{t}}\textbf{ sim}(X,Y)\]

where \(\boldsymbol{X}\), and \(\boldsymbol{Y}\) are base element of \(\mathcal{D}_{s}\), and \(\mathcal{D}_{t}\) respectively, and \(\textbf{sim}(X,Y)\) denotes the similarity between \(X\) and \(Y\). For example, in generic segmentation (Fig. 3.4), \(\mathcal{D}_{s}\) is the set of all objects (segments) in the image: \(\mathcal{D}_{s}=\{O_{1},\ldots,O_{n_{s}}\}\), and \(\mathcal{D}_{t}\) is the set of category names: \(\mathcal{D}_{t}=\{T_{1},\ldots,T_{n}\}\). For each object \(\hat{O}\) in \(\mathcal{D}_{s}\), we will find the corresponding category \(\bar{T}\in\mathcal{D}_{t}\).

**Terminology.** Here we will introduce important model terminology, including prompts (\(P\)) and queries (\(Q\)). Our model supports three kinds of inputs: vision (I), language (T), and interleaved vision-language (E). The vision and language foundation models predict the embeddings for those inputs. As shown in Fig. 4.1, by sampling the embeddings, we obtain vision prompts (\(P_{I}\)), language prompts (\(P_{T}\)), and interleave prompts (\(P_{E}\)). Additionally, trainable queries initialized with random parameters will accumulate information from the prompts. For example, in generic segmentation, object queries (\(Q_{O}\)) gather information from visual prompts. Interestingly, queries just act like "buckets" accumulating "water" (prompts) in the _FIND_ interface, as shown in Fig. 4.1.

#### 3.2.2 Model Pipeline

Our model is designed to interface with a pair of arbitrary vision and language foundation models. **Prompts and Queries Preparation.** Given image (I), text (T), and interleave (E) inputs, the vision encoder (\(\textbf{F}_{v}\)) and language encoder (\(\textbf{F}_{l}\)) will encode these inputs to sequences of embeddings \(M\):

\[M_{I}=\textbf{F}_{v}(I),\ \ M_{T}=\textbf{F}_{l}(T),\ \ M_{E}=\{\textbf{F}_{v}, \textbf{F}_{l}\}(E)\] (1)

where, \(M\in\mathbb{R}^{n\times d}\), and \(n,d\) is the embedding number and dimension respectively. Similar to SEEM (53), we use an embedding sampler to sample customized prompts for downstream tasks. Example sampling strategies include downsampling, ROI pooling for the region, and rearrangement of embeddings for interleave prompt. The sampling procedure does not alter the embedding distribution. After sampling, we obtain \(\{P_{E},P_{T},P_{I},\ldots\}=\textbf{Emb\_Sample}(M_{I},M_{T},M_{E})\). Additionally, the embedding sampler is responsible for sampling queries (\(\{Q_{E},Q_{T},Q_{I},\ldots\}\)) from the pool of learnable queries. We allow duplication in the sampling procedure of learnable queries. These queries

Figure 3: Task Unification for retrieval, grounding, and segmentation. The corresponding components are labeled with the same color or connected with a line or arrow.

and prompts are the inputs of _FIND_ interface. Technically, the embedding sampler is usually an interpolation or grid sample layer in PyTorch.

_FIND Interface._ The _FIND_ interface primarily consists of two operations: content attention \(\mathbf{A}_{t}\) and conditional attention \(\mathbf{A}_{d}\), as shown in Fig. 4.3. Content attention allows queries to accumulate information from the corresponding prompts, while conditional attention enables prompts and queries to reason internally (e.g. self-attention on object queries to avoid duplication). With initial prompts \(\mathbf{P}^{0}=\{P_{E}^{0},P_{T}^{0},P_{I}^{0},\dots\}\), and initial learnable queries \(\mathbf{Q}^{0}=\{Q_{E}^{0},Q_{T}^{0},Q_{I}^{0},\dots\}\), content attention and conditional attention are formally defined as:

\[\mathbf{Q}^{l+1}=\mathbf{A}_{t}(\mathbf{P}^{l},\mathbf{Q}^{l};[\mathbf{P}^{l} \rightarrow\mathbf{Q}^{l}]),\ \ \ \mathbf{Q}^{l+1},\mathbf{P}^{l+1}=\mathbf{A}_{d}(\mathbf{P}^{l},\mathbf{Q}^{l};[ \mathcal{S}^{l}\rightarrow\mathbf{Q}^{l}],[\mathbf{P}^{l}\rightarrow\mathbf{P} ^{l}])\] (2)

where \(\mathcal{S}^{l}\subseteq\{\mathbf{P}^{l},\mathbf{Q}^{l}\}\) is a subset of queries and prompts, \(\rightarrow\) represents the attention mask. For example, \([\mathbf{P}\rightarrow\mathbf{Q}]\) means that \(\mathbf{Q}\) is able to attend \(\mathbf{P}\) during the attention. In this way, prompts act as the information source, and queries act as the bucket. In Fig. 4.2, we unfold the prompts and queries for some tasks supported by _FIND_ interface.

**Projection** The outputs of the _FIND_ interface are a sequence of queries: \(\mathbf{Q}^{L}=\{Q_{E}^{L},Q_{T}^{L},Q_{I}^{L},Q_{E}^{L},\dots\}\). We then project the queries using linear layers, \(\mathbf{MLP}_{s}\) and \(\mathbf{MLP}_{p}\), for semantic and pixel projection, respectively. The semantic and pixel queries are computed as \(Q^{s}=\mathbf{MLP}_{s}(\mathbf{Q}^{L})\in\mathbb{R}^{n_{t}\times d}\) and \(Q^{p}=\mathbf{MLP}_{p}(\mathbf{Q}^{L})\in\mathbb{R}^{n_{t}\times d}\), where \(n_{t}\) is the total instance number, and \(d\) is the embedding dimension. The semantic outputs are used for retrieval, category mapping, etc., while the pixel outputs are used for mask prediction.

**Task Head** With the projected queries, as illustrated Sec. 3.2.1 each understanding task can be represented as a similarity mapping procedure. Formally, segmentation result (**Mask**) can be computed given initial image embedding \(M_{I}\in\mathbb{R}^{n_{p}\times d}\), where \(n_{p}\) is the pixel number. The similarity scores (**Score**) can be computed directly from \(Q^{s}\). The outputs for each task is a subset of \(\{\text{Mask, Score}\}\).

\[\text{Mask}=Q^{p}\times M_{I}^{\top}\in\mathbb{R}^{n_{t}\times n_{p}},\ \ \ \text{ Score}=Q^{s}\times Q^{s\top}\in\mathbb{R}^{n_{t}\times n_{t}}\] (3)

**Loss _FIND_ is trained with a linear combination of losses for panoptic segmentation, grounded segmentation, interactive segmentation, image-text retrieval, interleave retrieval with visual entities from the same image, and interleave grounding. We demonstrate the loss details in the Appendix.

## 4 Experiments

**Datasets.** We use COCO (25) as our main training and evaluation dataset, which spans diverse annotation types. We make use of the annotations from COCO-panoptic, Ref-COCO (45; 28; 29), COCO-Karpathy (18), and the new datasets generated with the data engine in _FIND_-Bench. We generate two sets of new annotations, including COCO-Entity and COCO-Paragraph, the detailed statistics are shown in the table below:

\begin{tabular}{c|c c c c c c|c c c} \hline  & \multicolumn{3}{c}{Training} & \multicolumn{3}{c}{Evaluation} & \multicolumn{3}{c}{Entity Association} & \multicolumn{3}{c}{Average} \\  & Images & Captions & Entire & Images & Captions & Entire & Mask & Private & Visual & Entire/Image \\ \hline COCO-Entity & 118189 & 353219 & 1104907 & 4950 & 4950 & 13305 & & & & 4 \\ COCO-Paragraph & - & - & - & 4981 & 4981 & 22569 & ✓ & ✓ & ✓ & ✓ & 7 \\ \hline \end{tabular}

**Settings.** We benchmark our method on three different model sizes: Tiny (FocalNet), Base (Davit-d3), and Large (Davit-d3). The vision backbone is fixed and reuses the X-Decoder pre-trained weights

Figure 4: (a) Preliminaries on the terminology of prompts and queries. (b) _FIND_ approach pipeline. The shape of different polygons represents different embedding types, and the color (vision, language) of the polygons represents input modality. (c) Detailed architecture of the _FIND_ Interface.

unless specified as SAM. The language backbone is a fixed LLaMA-7B, unless specified as UniCL. During training, we train the FIND-Interface jointly on all the tasks unless specified.

**Metrics.** We evaluate all the tasks with their standard evaluation metrics. For the newly proposed interleave retrieval, we use IR@5 and IR@10 (Interleave-to-image Retrieval accuracy at rank 5/10). For interleave grounding, we evaluate based on cIoU (pixel-wise IoU), and mIoU (image-wise IoU) between the predicted interleave masks and the ground truth masks.

**Baselines.** We use ImageBind (13), FROMAGe (20), BLIP2 (23) as baselines for the interleave retrieval task; Grounding-SAM (27), SEEM (53) for interleave grounding. We claim to make every effort to design the baseline evaluation protocol to achieve the best possible performance.

### Main Results

In the main experiments, we focus on evaluating _FIND_ on Generalizable, Interleavable, and Extendable capabilities as claimed in the abstract.

**(1)** Generalizable to **Segmentation, Grounding, and Retrieval.** Table 2 compares _FIND_ with strong baselines on generic segmentation tasks including panoptic segmentation, instance segmentation, and semantic segmentation. In addition, we demonstrate the segmentation capability in both referring segmentation (RefCOCO-g: one sentence is associated with one instance) and grounded segmentation (COCO-Entity and COCO-Paragraph: one sentence is associated with multiple instances) settings. Moreover, we also benchmark _FIND_'s performance in image-text retrieval on three different ground truth types on COCO, where the average sentence length for the splits (Karpathy, Entity, and Paragraph) gradually increases. Below are the takeaways:

_The instance segmentation result stands out:_ Our approach with a large vision encoder outperforms similar models like Mask2Former, X-Decoder, and SEEM, achieving a performance 2.2 points higher than Mask2Former (L), which additionally uses deformable convolution. Notably, the segmentation training data is identical for both Mask2Former and _FIND_. The performance gain likely results from our unified segmentation and grounding pipeline, which mutually benefits from the semantic ground truth of each domain.

_Mutual benefits of grounded and referring segmentation:_ In _FIND_, we unify grounded and referring segmentation using queries and prompts. As shown in Table 2, our model achieves state-of-the-art performance on COCO-Entity and COCO-Paragraph and outperforms strong baselines on the Ref-COCOg dataset.

_Interactive segmentation performance is preserved in the unified settings._ Unlike SEEM which is only trained on image-only tasks, _FIND_ is trained also on image-text tasks, such as image-text retrieval. With the smart design of queries, prompts, and attention mechanisms, training interactive segmentation and image-text retrieval does not interfere. Thus, it enables our approach to achieve competitive performances (i.e. _FIND_ 88.5/89.5/77.4 vs. SEEM 88.5/89.6/76.5).

_Less optimal image-text retrieval results:_ The sub-optimal performance in image-text retrieval is due to batch size during fine-tuning. Pilot experiments with X-Decoder showed that different resolutions (e.g., 1024 for images and 224 for language) do not generalize well across tasks. Thus, _FIND_ is trained with the same resolution for all tasks. In Table 2, models are either 384x384 with batch size 384 or 1024x1024 with batch size 192 for all tasks. Other tables show results with a 640x640 training resolution and a 192 batch size.

\begin{table}
\begin{tabular}{c|c|c|c|c|c|c|c|c|c|c|c|c|c|c} \hline \multicolumn{1}{c|}{\multirow{2}{*}{**Metrics.**}} & \multicolumn{1}{c|}{\multirow{2}{*}{**Metrics.**}} & \multicolumn{1}{c|}{\multirow{2}{*}{**Metrics.

**(2)** Interleavable **on vision and language modalities.** In Table. 3, we evaluate _FIND_ on the interleaved dataset- and image-level understanding tasks in _FIND_-Bench. In the columns of COCO-Entity and COCO-Paragraph, we replace the text entity with visual reference on 0.5 probability, unlike Table. 2 the columns are purely evaluated on language-based data.

_Interleaved Segmentation:_ We build an interleaved segmentation baseline using the SEEM model. Instead of formulating the grounding task in an interleaved format that SEEM doesn't support, we simply separately infer visual, and text entities using the interactive or grounding function of SEEM. As shown in Table 3, _FIND_ outperforms SEEM on interleave segmentation with around +8 points on both COCO-Entity and COCO-Paragraph under cIoU metrics.

_Interleaved Retrieval:_ We also explore cross-image interleave retrieval on _FIND_. Since the interleaved reference objects are from the same validation set, IR@1 is not meaningful, so we report IR@5 and IR@10 in this setting. For ImageBind and BLIP-2, we use ensemble scores of texts, sentences, and images. Following FROMAGe's settings for interleaved image-text retrieval, our performance is significantly higher than the baselines, demonstrating the effectiveness of our interleaved shared embedding space.

_Generic Segmentation:_ Beyond classic evaluations using class names or fixed indices, we replace categories with class descriptions (long descriptions) or visual prompts (average features for object queries for each class). Leveraging LLMs, _FIND_ excels in description-based segmentation, benefiting from smoother representations and better handling of long contexts. We also demonstrate _FIND_'s effectiveness in the visual context setting.

**(3)** Extendable **to arbitrary foundation models and tasks.** In the main experiments, we use X-Decoder as the vision encoder, and LLaMA as the language encoder, which shows convincing performance on all the tasks. X-Decoder has been trained to pair up vision and language embeddings, however, SAM is only trained on segmentation data without any semantic meaning. Thus, we use SAM as an ablation vision foundation model, to study how important is vision encoder trained with semantic data. For the language encoder, we adopt UniCL which has the same size as Bert to study the difference between a standard language encoder, and an LLM encoder. As shown in Table 4, UniCL and LLaMA usually have very similar performance with X-Decoder as vision encoder, except that LLaMA is extremely effective on long description reasoning. Although the performance of SAM is much worse than its counterpart X-Decoder on semantic understanding after training the interface, our approach also shows that without any modification to SAM, it applies to semantic understanding tasks on generic, grounded segmentation, and image-text retrieval.

### Ablation Study

We ablate our approach from two perspectives: (1) What is the effectiveness of each task in the unified pipeline? (2) The effectiveness of using intermediate layers of the LLM representation.

_Independent task effectiveness:_ We assess task effectiveness by gradually removing tasks in Table 5. Removing image-text retrieval significantly reduces interleave retrieval performance. Further remov

\begin{table}
\begin{tabular}{l c|c c c c c|c c c c|c c c} \hline \multirow{3}{*}{} & \multicolumn{6}{c|}{**Generic Segmentation**} & \multicolumn{6}{c|}{**Grounding Interactive**} & \multicolumn{6}{c}{**Retrieval**} \\  & \multicolumn{3}{c}{Class} & \multicolumn{3}{c|}{Description} & \multicolumn{3}{c|}{g-Ref} & \multicolumn{3}{c|}{VOC} & \multicolumn{3}{c}{COCO-Karpathy} \\ \cline{3-13} \multirow{-2}{*}{Vision} & \multirow{-2}{*}{Language} & PQ & mAP & mIoU & PQ & mAP & mIoU & cIoU & 1-IoU & IR@1 & TR@1 \\ \hline X-Decoder (T) (52) & UniCL (43) & 48.5 & 39.0 & 61.4 & 12.4 & 20.7 & 18.9 & 61.3 & 82.6 & 40.4 & 54.0 \\ X-Decoder (T) (52) & LLMaMa (38) & 48.5 & 38.9 & 61.2 & 19.5 & 30.2 & 35.5 & 61.6 & 82.5 & 40.2 & 52.2 \\ SAM (B) (19) & UniCL (43) & 42.5 & 37.6 & 53.6 & 4.5 & 17.7 & 17.9 & 64.9 & 81.6 & 29.1 & 39.5 \\ SAM (B) (19) & LLMaMa (38) & 42.5 & 36.9 & 53.0 & 6.1 & 15.6 & 16.6 & 58.9 & 81.5 & 27.0 & 35.5 \\ \hline \end{tabular}
\end{table}
Table 4: Ablation study on different foundation model architectures.

\begin{table}
\begin{tabular}{l c c c c c c c c c c c c c c c} \hline \multirow{3}{*}{} & \multicolumn{6}{c|}{**Generic Segmentation**} & \multicolumn{6}{c|}{**Generic Segmentation**} & \multicolumn{6}{c}{**Generic Segmentation**} \\  & COCO-Paragraph & COCO-Paragraph & COCO-Paragraph & COCO-Paragraph & COCO-Paragraph & \multicolumn{6}{c|}{VOC} & \multicolumn{6}{c|}{COCO-Paragraph} & \multicing the grounding task decreases entity-based grounding performance. Since interleave grounding is related to interactive segmentation, removing it also reduces interleave segmentation performance. Finally, training only panoptic segmentation yields similar performance to other settings, indicating the unified interface's consistency with basic task training.

_Varying the feature embeddings layer for LLM:_ LLMs process language tokens, with embeddings near input and output layers being less semantic. We hypothesize that intermediate layers align better with vision embeddings. Table 5 shows performance across tasks using emebddings from layers -1 (output) to -30 (input). Layer -12 emebddings perform best, while top and bottom layers perform worse for image-text retrieval on COCO-Karparthy splits. Thus, we use layer -12 emebddings for LLaMA throughout the paper.

### Demonstration Results

Interleave Album Search.The queries in our _FIND_ approach support linear complexity interleave album search. Given an image, interleave, or text input, our model can retrieve and segment all the photos in the album. Below, we show an example using the COCO validation set as the search space.

Interleave Video Localization.We can formulate the video frame localization problem as an image-text retrieval task. This allows us to reason about and identify corresponding objects based on given instructions, as illustrated below. We believe _FIND_ is useful for robot navigation.

3D Feature Field.Foundation model embeddings are utilized to create a 3D feature field for robot manipulation, localization, and reasoning. We believe that the interleave embedding space, with its pixel-level understanding capabilities, has significant potential in the 3D feature field. Below, we compare a scene trained with FIND embeddings versus CLIP embeddings.

**Conclusions and Future Work.** This work introduces the _FIND_ Interface, a generalized interface for aligning foundation models' embeddings, along with the _FIND_ Benchmark for training and evaluation. In Sec. 4.3, we demonstrate potential applications such as interleave album search, video localization, and 3D feature fields. These examples clearly illustrate the potential of our model for personalized foundation models and robotics.

**Limitations.** Our model is only trained and evaluated on the COCO dataset. With the limitation of data quantity, we mention that the method may not be well adapted to the in-the-wild settings.

**Broader Impact.** Our proposed approach inherits ethical or social issues (e.g. bias amplification, privacy risks, energy consumption) of foundational models.

\begin{table}
\begin{tabular}{l|c|c c c c|c c c c c} \hline \multicolumn{2}{c|}{} & \multicolumn{2}{c|}{COCO} & \multicolumn{2}{c|}{p-Ref} & \multicolumn{2}{c|}{Empty} & \multicolumn{2}{c|}{COE (\%)} & \multicolumn{2}{c}{Ensity} & \multicolumn{2}{c}{Entity} \\ \cline{3-11} \multicolumn{2}{c|}{} & & \multicolumn{1}{c}{PO} & \multicolumn{1}{c}{PAP} & \multicolumn{1}{c}{dU} & \multicolumn{1}{c}{CU} & \multicolumn{1}{c|}{PU} & \multicolumn{1}{c|}{PU} & \multicolumn{1}{c|}{PU} & \multicolumn{1}{c}{PU} & \multicolumn{1}{c}{PU} & \multicolumn{1}{c}{PU} & \multicolumn{1}{c}{PU} & \multicolumn{1}{c}{PU} \\ \hline \multirow{4}{*}{Task} & All & 48.5 & 39.0 & **61.4** & **61.3** & 73.0 & 82.6 & **40.4** & **54.0** & **50.8** & **51.9** \\  & - Retrieval & 48.5 & 39.0 & 61.1 & 60.6 & **73.2** & **82.8** & - & - & 44.3 & 44.8 \\  & - Grounding & 48.6 & 39.1 & 61.3 & - & 40.9 & 28.8 & - & - & 45.3 & 46.2 \\  & - Interactive & 48.6 & 38.8 & 61.0 & - & 36.5 & - & - & - & 31.4 & 33.4 \\  & - Interactive & **48.9** & **39.3** & 61.0 & - & - & - & - & - & - & - \\ \hline \multirow{4}{*}{Language Level} & [-1] & 48.3 & 39.1 & 61.2 & 61.3 & 73.0 & 82.6 & 38.9 & 52.2 & 50.3 & 50.8 \\  & [-6] & 47.8 & 38.8 & 60.4 & 60.3 & 72.9 & 81.3 & 38.1 & 49.9 & 48.1 & 47.5 \\  & [-12] & **48.5** & **39.0** & **61.4** & 61.3 & **73.0** & **82.6** & **40.4** & **54.0** & **50.8** & **51.9** \\  & [-18] & 48.2 & 39.0 & 61.1 & 62.2 & 72.6 & 82.2 & 40.1 & 52.7 & 50.6 & 50.5 \\  & [-24] & 48.5 & 38.8 & 61.5 & **61.6** & 72.9 & 82.6 & 40.2 & 52.2 & 50.5 & 51.3 \\  & [-30] & 48.1 & 39.2 & 61.1 & 60.1 & 73.3 & 82.4 & 37.9 & 49.3 & 49.4 & 50.0 \\ \hline \end{tabular}
\end{table}
Table 5: Ablate on each training task and language encoder feature level.

**Acknowledgement.** This work was supported in part by NSF CAREER IIS2150012, NASA 80NSSC21K0295, the Institute of Information and communications Technology Planning and Evaluation (IITP) grant funded by the Korea government (MSIT) (No. 2022-0-00871, Development of AI Autonomy and Knowledge Enhancement for AI Agent Collaboration). This research project has benefitted from the Microsoft Accelerate Foundation Models Research (AFMR) grant program.

## References

* [1]J. B. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr, Y. Hasson, K. Lenc, A. Mensch, K. Millican, M. Reynolds, et al. (2022) Flamingo: a visual language model for few-shot learning. Advances in Neural Information Processing Systems35, pp. 23716-23736. Cited by: SS1.
* [2]J. An, Z. Yang, L. Li, J. Wang, K. Lin, Z. Liu, L. Wang, and J. Luo (2023) Openleaf: open-domain interleaved image-text generation and evaluation. arXiv preprint arXiv:2310.07749. Cited by: SS1.
* [3]P. BehnamGhader, V. Adlakha, M. Mosbach, D. Bahdanau, N. Chapados, and S. Reddy (2024) Llm2vec: large language models are secretly powerful text encoders. arXiv preprint arXiv:2404.05961. Cited by: SS1.
* [4]T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. (2020) Language models are few-shot learners. Advances in neural information processing systems33, pp. 1877-1901. Cited by: SS1.
* [5]S. Bubeck, V. Chandrasekaran, R. Eldan, J. Gehrke, E. Horvitz, E. Kamar, P. Lee, Y. T. Lee, Y. Lundberg, S. Sparks of artificial general intelligence: early experiments with gpt-4. arXiv preprint arXiv:2303.12712. Cited by: SS1.
* [6]L. C. Chen, G. Papandreou, F. Schroff, and H. Adam (2017) Rethinking atrous convolution for semantic image segmentation. arXiv preprint arXiv:1706.05587. Cited by: SS1.
* [7]Y. Chen, L. Li, L. Yu, A. E. Kholy, F. Ahmed, Z. Gan, Y. Cheng, and J. Liu (2020) UNITER: universal image-text representation learning. In ECCV, pp. 104-120. Cited by: SS1.
* [8]B. Cheng, I. Misra, A. G. Schwing, A. Kirillov, and R. Girdhar (2022) Masked-attention mask transformer for universal image segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1290-1299. Cited by: SS1.
* [9]M. Cherti, R. Beaumont, R. Wightman, M. Wortsman, G. Ilharco, C. Gordon, C. Schuhmann, L. Schmidt, and J. Jitsev (2023) Reproducible scaling laws for contrastive language-image learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2818-2829. Cited by: SS1.
* [10]J. Devlin, M. Chang, K. Lee, and K. Toutanova (2019) Bert: pre-training of deep bidirectional transformers for language understanding. In NAACL-HLT (1), Cited by: SS1.
* [11]M. Ding, X. Liao, L. Yang, P. Wang, X. Jin, Z. Lu, and P. Luo (2021) Hr-nas: searching efficient high-resolution neural architectures with lightweight transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 2982-2992. Cited by: SS1.
* [12]M. Ding, B. Xiao, N. Codella, P. Luo, J. Wang, and L. Yuan (2022) Davit: dual attention vision transformers. In European Conference on Computer Vision, pp. 74-92. Cited by: SS1.
* [13]R. Girdhar, A. El-Nouby, Z. Liu, M. Singh, K. V. Alwala, A. Joulin, and I. Misra (2023) ImageBind: one embedding space to bind them all. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 15180-15190. Cited by: SS1.
* [14]L. Grady (2006) Random walks for image segmentation. IEEE transactions on pattern analysis and machine intelligence28 (11), pp. 1768-1783. Cited by: SS1.

[MISSING_PAGE_POST]

* [17] Huh, M., Cheung, B., Wang, T., Isola, P.: The platonic representation hypothesis. arXiv preprint arXiv:2405.07987 (2024)
* [18] Karpathy, A., Fei-Fei, L.: Deep visual-semantic alignments for generating image descriptions. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 3128-3137 (2015)
* [19] Kirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C., Gustafson, L., Xiao, T., Whitehead, S., Berg, A.C., Lo, W.Y., et al.: Segment anything. arXiv preprint arXiv:2304.02643 (2023)
* [20] Koh, J.Y., Salakhutdinov, R., Fried, D.: Grounding language models to images for multimodal inputs and outputs (2023)
* [21] Laurencon, H., Saulnier, L., Tronchon, L., Bekman, S., Singh, A., Lozhkov, A., Wang, T., Karamcheti, S., Rush, A.M., Kiela, D., et al.: Obelics: An open web-scale filtered dataset of interleaved image-text documents. In: Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track (2023)
* [22] Li, C., Gan, Z., Yang, Z., Yang, J., Li, L., Wang, L., Gao, J.: Multimodal foundation models: From specialists to general-purpose assistants. arXiv preprint arXiv:2309.10020 **1**, 2 (2023)
* [23] Li, J., Li, D., Savarese, S., Hoi, S.: Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597 (2023)
* [24] Li, X., Yin, X., Li, C., Zhang, P., Hu, X., Zhang, L., Wang, L., Hu, H., Dong, L., Wei, F., Choi, Y., Gao, J.: Oscar: Object-semantics aligned pre-training for vision-language tasks. In: ECCV. pp. 121-137 (2020)
* [25] Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollar, P., Zitnick, C.L.: Microsoft coco: Common objects in context. In: Computer Vision-ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13. pp. 740-755. Springer (2014)
* [26] Liu, H., Li, C., Wu, Q., Lee, Y.J.: Visual instruction tuning. arXiv preprint arXiv:2304.08485 (2023)
* [27] Liu, S., Zeng, Z., Ren, T., Li, F., Zhang, H., Yang, J., Li, C., Yang, J., Su, H., Zhu, J., et al.: Grounding dino: Marrying dino with grounded pre-training for open-set object detection. arXiv preprint arXiv:2303.05499 (2023)
* [28] Mao, J., Huang, J., Toshev, A., Camburu, O., Yuille, A.L., Murphy, K.: Generation and comprehension of unambiguous object descriptions. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 11-20 (2016)
* [29] Nagaraja, V.K., Morariu, V.I., Davis, L.S.: Modeling context between objects for referring expression understanding. In: Computer Vision-ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part IV 14. pp. 792-807. Springer (2016)
* [30] OpenAI: Gpt-4 technical report. Tech. rep., OpenAI (2023)
* [31] OpenAI: Improving image generation with better captions. Tech. rep., OpenAI (2023)
* [32] Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al.: Learning transferable visual models from natural language supervision. In: International conference on machine learning. pp. 8748-8763. PMLR (2021)
* [33] Riquelme, C., Puigcerver, J., Mustafa, B., Neumann, M., Jenatton, R., Susano Pinto, A., Keysers, D., Houlsby, N.: Scaling vision with sparse mixture of experts. NeurIPS **34** (2021)
* [34] Ryoo, M.S., Piergiovanni, A., Arnab, A., Dehghani, M., Angelova, A.: Tokenlearner: What can 8 learned tokens do for images and videos? arXiv: Computer Vision and Pattern Recognition (2021)* [35] Saunshi, N., Plevrakis, O., Arora, S., Khodak, M., Khandeparkar, H.: A theoretical analysis of contrastive unsupervised representation learning. In: International Conference on Machine Learning. pp. 5628-5637. PMLR (2019)
* [36] Srinivas, A., Lin, T.Y., Parmar, N., Shlens, J., Abbeel, P., Vaswani, A.: Bottleneck transformers for visual recognition. In: CVPR. pp. 16519-16529 (2021)
* [37] Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., Jegou, H.: Training data-efficient image transformers & distillation through attention. In: ICML. pp. 10347-10357. PMLR (2021)
* [38] Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.A., Lacroix, T., Roziere, B., Goyal, N., Hambro, E., Azhar, F., et al.: Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971 (2023)
* [39] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, L., Polosukhin, I.: Attention is all you need. In: NeurIPS (2017)
* [40] Wang, W., Xie, E., Li, X., Fan, D.P., Song, K., Liang, D., Lu, T., Luo, P., Shao, L.: Pyramid vision transformer: A versatile backbone for dense prediction without convolutions. In: ICCV (2021)
* [41] Wu, K., Peng, H., Chen, M., Fu, J., Chao, H.: Rethinking and improving relative position encoding for vision transformer. In: ICCV. pp. 10033-10041 (2021)
* [42] Xiao, B., Wu, H., Xu, W., Dai, X., Hu, H., Lu, Y., Zeng, M., Liu, C., Yuan, L.: Florence-2: Advancing a unified representation for a variety of vision tasks. arXiv preprint arXiv:2311.06242 (2023)
* [43] Yang, J., Li, C., Zhang, P., Xiao, B., Liu, C., Yuan, L., Gao, J.: Unified contrastive learning in image-text-label space. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 19163-19173 (2022)
* [44] Yao, L., Huang, R., Hou, L., Lu, G., Niu, M., Xu, H., Liang, X., Li, Z., Jiang, X., Xu, C.: FILIP: fine-grained interactive language-image pre-training. In: ICLR (2022)
* [45] Yu, L., Poirson, P., Yang, S., Berg, A.C., Berg, T.L.: Modeling context in referring expressions. In: Computer Vision-ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part II 14. pp. 69-85. Springer (2016)
* [46] Yu, W., Yang, Z., Li, L., Wang, J., Lin, K., Liu, Z., Wang, X., Wang, L.: Mm-vet: Evaluating large multimodal models for integrated capabilities (2023)
* [47] Yuan, L., Chen, D., Chen, Y.L., Codella, N., Dai, X., Gao, J., Hu, H., Huang, X., Li, B., Li, C., et al.: Florence: A new foundation model for computer vision. arXiv preprint arXiv:2111.11432 (2021)
* [48] Zang, Y., Li, W., Han, J., Zhou, K., Loy, C.C.: Contextual object detection with multimodal large language models. arXiv preprint arXiv:2305.18279 (2023)
* [49] Zhai, X., Kolesnikov, A., Houlsby, N., Beyer, L.: Scaling vision transformers. arXiv: Computer Vision and Pattern Recognition (2021)
* [50] Zhang, P., Li, X., Hu, X., Yang, J., Zhang, L., Wang, L., Choi, Y., Gao, J.: Vinvl: Making visual representations matter in vision-language models. arXiv preprint arXiv:2101.00529 (2021)
* [51] Zhu, W., Hessel, J., Awadalla, A., Gadre, S.Y., Dodge, J., Fang, A., Yu, Y., Schmidt, L., Wang, W.Y., Choi, Y.: Multimodal c4: An open, billion-scale corpus of images interleaved with text. arXiv preprint arXiv:2304.06939 (2023)
* [52] Zou, X., Dou, Z.Y., Yang, J., Gan, Z., Li, L., Li, C., Dai, X., Behl, H., Wang, J., Yuan, L., et al.: Generalized decoding for pixel, image, and language. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 15116-15127 (2023)
* [53] Zou, X., Yang, J., Zhang, H., Li, F., Li, L., Gao, J., Lee, Y.J.: Segment everything everywhere all at once. arXiv preprint arXiv:2304.06718 (2023)

[MISSING_PAGE_FAIL:13]

baseball player in a black and white uniform,, which is the total number of \([T]\) and \([I]\) referencing to Fig. 3.

After getting a full sense of the input embeddings of interleave grounding, including p.image, p.interleave, q.entity, q.interleave. We then introduce the operation on top of those embeddings. As introduced in Sec. 3.2.2, the operations contain content attention \(\mathbf{A}_{t}\) and conditional attention \(\mathbf{A}_{d}\). Formally we could write the attention mechanism for the specific input embeddings of interleave grounding with the following equations:

\[\texttt{q.entity, q.interleave}=\mathbf{A}_{t}(\texttt{[q.entity,q.interleave] };\texttt{[p.image,p.interleave]};\mathbf{M}_{t}),\] (5)

\[\texttt{q.*, p.*}=\mathbf{A}_{t}(\texttt{[q.*, p.*]};\texttt{[q.*, p.*]};\mathbf{M}_{d})\] (6)

where \(\mathbf{A}(\texttt{query};\texttt{key}=\texttt{value};\mathbf{M})\) is the attention operator with query, key, value and mask. Given the order p.image, p.interleave, q.entity, q.interleave, the content and condition attention masks are written below:

\[\mathbf{M}_{t}=\begin{bmatrix}F&F&F&F\\ F&F&F&F\\ \mathbf{T}&F&F&F\\ F&\mathbf{T}&F&F\end{bmatrix}\mathbf{M}_{d}=\begin{bmatrix}F&F&F&F\\ F&\mathbf{T}&F&F\\ \mathbf{T}&F&\mathbf{T}&F\\ F&\mathbf{T}&F&\mathbf{T}\end{bmatrix}\] (7)

The index of matrix coordinates follows the input order. After the input prompts and queries are fully communicated, we will compute the projected pixel and semantic embeddings for output in the following manner:

\[\texttt{q.entity}^{s},\texttt{q.interleave}^{s}=\mathbf{MLP}_{s}( \texttt{q.entity, q.interleave})\] (8) \[\texttt{q.entity}^{p}=\mathbf{MLP}_{p}(\texttt{q.entity})\] (9)

where \({}^{s},{}^{p}\) are semantic and pixel projection respectively. This way, queries are projected into semantic and pixel space to compute the final output. The dimension of \(\texttt{q.entity}^{s}\) and \(\texttt{q.entity}^{p}\) are both \([100,512]\). In addition, \(\texttt{q.interleave}^{s}\) has dimension \([n,512]\) where n is the entity number. With those projected queries and image features \(M_{I}\) in the pixel projection space with shape \([h,w,512]\). We could get the final output mask associated with each entity with the following operation:

\[\texttt{Index}=\arg\max_{\texttt{dim=0}}\texttt{sim}(\texttt{ q.entity}^{s},\texttt{q.interleave}^{s})\] (10) \[Q_{p}^{*}=\texttt{q.entity}^{p}[\texttt{Index}]\] (11) \[\texttt{Mask}=Q_{p}^{*}\times M_{I}\] (12)

In this way, we associate the grounding entity with the desired mask segment of the image, as shown in the top right figure in Table. 1.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The claims generalizable, prototypable, extendable, and interleavable are clearly demonstrated in the method and experiment section. The contribution mentioned is also proved in method and experiment section. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We have a paragraph on limitation. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?Answer: [NA] Justification: Our paper is an application-based paper, and does not have a theoretical result. Guidelines:

* The answer NA means that the paper does not include theoretical results.
* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: Our code is public available. Guidelines:

* The answer NA means that the paper does not include experiments.
* If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.
* If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.
* Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.
* While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code**Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: We provide open access to the code with training details in the supplementary material. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We specify them in the training and inference code public available. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [No] Justification: Our model is evaluated on a large number of datasets with enough data points. The number is empirically very stable. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).

* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We have provided the details in the supplementary material. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We follow the code of ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: We have discussed the border impacts in the last paragraph. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.

* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: Our model is on the side of understanding instead of generation, so that safeguards is not applied. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licensees for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We have properly cited and acknowledged the prior works. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.

* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: The new benchmark is documented in the paper on the code base. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: Our work does not need crowdsourcing. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: Our work does not contain human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.