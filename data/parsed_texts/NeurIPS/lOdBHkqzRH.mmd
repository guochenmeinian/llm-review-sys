# Contextual Linear Optimization with Bandit Feedback

 Yichun Hu\({}^{1}\)   Nathan Kallus\({}^{1}\)   Xiaojie Mao\({}^{2}\)   Yanchen Wu\({}^{2}\)

\({}^{1}\) Cornell University  \({}^{2}\) Tsinghua University

{yh767, kallus}@cornell.edu

maoxj@sem.tsinghua.edu.cn

wu-yc23@mails.tsinghua.edu.cn

Authors are listed in alphabetical order. Correspondence to Xiaojie Mao: maoxj@sem.tsingua.edu.cn.

###### Abstract

Contextual linear optimization (CLO) uses predictive contextual features to reduce uncertainty in random cost coefficients and thereby improve average-cost performance. An example is the stochastic shortest path problem with random edge costs (e.g., traffic) and contextual features (e.g., lagged traffic, weather). Existing work on CLO assumes the data has fully observed cost coefficient vectors, but in many applications, we can only see the realized cost of a historical decision, that is, just one projection of the random cost coefficient vector, to which we refer as bandit feedback. We study a class of offline learning algorithms for CLO with bandit feedback, which we term induced empirical risk minimization (IERM), where we fit a predictive model to directly optimize the downstream performance of the policy it induces. We show a fast-rate regret bound for IERM that allows for misspecified model classes and flexible choices of the optimization estimate, and we develop computationally tractable surrogate losses. A byproduct of our theory of independent interest is fast-rate regret bound for IERM with full feedback and misspecified policy class. We compare the performance of different modeling choices numerically using a stochastic shortest path example and provide practical insights from the empirical results.

## 1 Introduction

Contextual linear optimization (CLO) models the use of predictive features (context variables) to improve decision making in linear optimization with random coefficients. In CLO, we consider a decision \(z\in\mathcal{Z}\) that incurs an uncertain cost \(Y^{\top}z\) determined by a random cost vector \(Y\in\mathbb{R}^{d}\) that is not observed at decision time. We do, however, observe predictive features \(X\in\mathbb{R}^{p}\) prior to decision, which help reduce uncertainty. CLO can be expressed either as a contextual stochastic optimization problem or a linear optimization problem where the cost vector is a conditional expectation:

\[v^{*}(x)=\min_{z\in\mathcal{Z}}\mathbb{E}\big{[}Y^{\top}z\mid X=x\big{]}=\min_ {z\in\mathcal{Z}}f_{0}(x)^{\top}z,\quad\text{where}\quad f_{0}(x)=\mathbb{E}[Y \mid X=x].\] (1)

We assume throughout that \(\mathcal{Z}\) is a polytope (\(\mathcal{Z}=\operatorname{Conv}(\mathcal{Z}^{\angle})\) for some vertex set \(\big{|}\mathcal{Z}^{\angle}\big{|}<\infty\) with \(\sup_{z\in\mathcal{Z}^{\angle}}\lVert z\rVert\leq B\)) and \(Y\) is bounded (without loss of generality, \(Y\in\mathcal{Y}=\{y:\lVert y\rVert\leq 1\}\)).

CLO has been the focus of much recent work [e.g., 9, 11, 14, 27, 30], due to its relevance to data-driven operational decision making in many applications such as network optimization, portfolio optimization, product recommendation, etc. The key question in these is how to use _data_ to learn a good policy, \(\pi:\mathbb{R}^{p}\to\mathcal{Z}\), mapping feature observations to effective decisions. All of this literature studies an offline setting, where a batch of existing data \((X_{i},Y_{i})\) for \(i=1,\ldots,n\) is observed. These data are viewed as random draws from the joint distribution of \((X,Y)\), and the goal is to learn an effective decision policy from them. Crucially, this assumes that we fully observe the random costvector \(Y_{i}\)'s, meaning that we fully know the corresponding costs \(Y_{i}^{\top}z\) for _any_ potential decision \(z\). This may be unrealistic in many applications.

Consider for example the instantiation of Eq. (1) as a stochastic shortest path problem of transporting one unit from a start node to a desired end node, where \(Y\) contains the travel time on each of \(d\) edges, \(z\) the binary decision whether to traverse each edge, and \(\mathcal{Z}\) constrains flow conservation. The data we would need in order to apply existing approaches here would consist of simultaneous observations of the travel times on every single edge, encapsulated in the elements of \(Y_{i}\)'s. However, such ideal observations may not be available. Indeed, modern travel time prediction models are often based on data from historical individual trips. For example, such is the case with Uber Movement data [26; 33], which is based on the total length of historical rides. Namely, instead of observing the entire \(Y_{i}\), we _only_ observe the total travelling time \(C_{i}=Y_{i}^{\top}Z_{i}\) for the path \(Z_{i}\) in a historical trip \(i\). We term this observation model _bandit feedback_ as it corresponds to observing the cost only of a given decision and not the counterfactual cost of any alternative decision, as in bandit problems [25].

In this paper, we study the contextual linear optimization problem with bandit feedback and make several contributions. First, we adapt the end-to-end induced empirical risk minimization approach to the bandit feedback setting. This approach allows us to optimize decision policies by directly targeting the decision objective. We provide three different methods to identify the expected cost of any given policy and show how to estimate the expected decision cost from bandit-feedback data. Second, we derive upper bounds on the regret (_i.e._, decision sub-optimality) of the policy that minimizes the estimated objective. Our regret analysis accounts for the misspecification of the policy class and incorporates a margin condition that potentially enables a faster regret rate. This significantly extends existing theory for full-feedback CLO, and as a byproduct, we provide a novel fast rate bound for full-feedback CLO with misspecification. Finally, we demonstrate that existing surrogate losses for full-feedback CLO (such as the well-known SPO+ loss in [11]) can be adapted to our setting, enabling efficient policy optimization. We empirically test this in simulated shortest path problems and provide practical insights.

### Background: The Full Feedback Setting

We first review two major approaches to the CLO problem with full feedback: an estimate-then-optimize (ETO) approach and an end-to-end induced empirical risk minimization (IERM) approach. To this end, we need to define the generic plug-in policy \(\pi_{f}\) for any given \(f:\mathbb{R}^{p}\rightarrow\mathbb{R}^{d}\):

\[\pi_{f}(x)\in\arg\min_{z\in\mathcal{Z}}f(x)^{\top}z.\] (2)

Note that for any given covariate value \(x\), this corresponds to a linear programming problem with coefficients given by \(f(x)\). An optimal policy in Eq. (1) corresponds to \(\pi_{f_{0}}\). Without loss of generaility, we restrict the value of \(\pi_{f}(x)\) to the set of vertices \(\mathcal{Z}^{\angle}\) of the polytope \(\mathcal{Z}\) with ties broken according to some fixed rule (e.g., a total ordering over \(\mathcal{Z}^{\angle}\) such as lexicographic).

The ETO approach starts with estimating \(f_{0}\) in Eq. (1) by any supervised learning method for predicting \(Y\) given \(X\). This can, _e.g._, be implemented by minimizing a prediction fitness criterion (e.g., sum of squared errors) over a hypothesis class of functions \(\mathcal{F}\subseteq[\mathbb{R}^{p}\rightarrow\mathbb{R}^{d}]\) (e.g., linear functions, decision trees, neural networks). After training an estimator \(\hat{f}\) of \(f_{0}\), the ETO approach then makes decisions according to the induced policy \(\pi_{\hat{f}}\). This approach is a straightforward two-step approach, but it ignores the downstream optimization task in choosing \(\hat{f}\in\mathcal{F}\).

In contrast, some recent literature propose to integrate the estimation and the optimization, directly searching for a decision policy to minimize the decision cost. Following [14], we consider an IERM formulation that minimizes the sample average cost over the class of plug-in policies induced by \(\mathcal{F}\):

\[\hat{\pi}^{\text{F}}\in\arg\min_{\pi\in\Pi_{\mathcal{F}}}\frac{1}{n}\sum_{i=1} ^{n}Y_{i}^{\top}\pi(X_{i}),\ \text{ where }\Pi_{\mathcal{F}}=\{\pi_{f}:f\in\mathcal{F}\}.\] (3)

This approach is end-to-end in that it directly targets the decision-making objective. Recent literature demonstrates that end-to-end approaches like IERM often outperform the ETO approach [12]. The benefits of end-to-end approaches are significant especially in the misspecified setting - that is, when the function class \(\mathcal{F}\) fails to contain the true expected cost function \(f_{0}\), and the policy class \(\Pi_{\mathcal{F}}\) does not include the globally optimal decision policy \(\pi_{f_{0}}\) [e.g., 11; 14]. While Eq. (3) involves a challenging bi-level optimization due to the definition of \(\Pi_{\mathcal{F}}\), practical computational approximations have been proposed [11; 16].

Nonetheless, the IERM formulation in Eq. (3) requires observing the full feedback \(Y_{i}\)'s in the data. In this paper, we study how to extend this approach to the setting with bandit feedback.

### Our Problem: CLO with Bandit Feedback

We now formally define the data generating process in the bandit feedback setting. Assume we have an offline dataset consisting of \(n\) data points \(\mathcal{D}=\{(X_{1},Z_{1},C_{1}),\ldots,(X_{n},Z_{n},C_{n})\}\) that are independent draws from a distribution \(P\) on \((X,Z,C)\) generated in the following way. We first draw \((X,Z,Y)\) where the \((X,Y)\) distribution is as in the full feedback setting and \(Z\) is generated according to some historical decision policies for data collection. Then we set \(C=Z^{\top}Y\) and omit the full \(Y\). Below we impose some basic assumptions on the generating process of the historical decision \(Z\).

**Assumption 1**.: _The data generating process satisfies the following two properties:_

1. _(Ignorability)_ \(\mathbb{E}[C\mid Z,X]=Z^{\top}f_{0}(X)\) _(which could follow from_ \(Z\perp Y\mid X\)_)._
2. _(Coverage)_ \(\inf_{z\in\operatorname{span}(Z):\|z\|=1}\mathbb{E}[(Z^{\top}z)^{2}\mid X] >0\) _almost surely._

The ignorability condition is a common assumption that plays a fundamental role in learning with partial feedback and causal inference (see the literature in Section 1.3). This condition requires that the assignments of the historical decisions \(Z\) do not depend on any unobserved factor potentially dependent on \(Y\). The coverage condition requires that given the covariates \(X\), historical decisions \(Z\) can explore all directions of the linear span of the constraint set \(\mathcal{Z}\); otherwise it may be impossible to evaluate certain policies from the observed costs of historical decisions. This is an analogue of the overlap assumption in learning with partial feedback and causal inference [17].

The learning task is to use these data \(\mathcal{D}\) to come up with a data-driven policy \(\hat{\pi}(x)\) that has low regret

\[\text{Reg}(\pi)=\mathbb{E}_{X}\big{[}f_{0}(X)^{\top}\pi(X)-\min_{z\in\mathcal{ Z}}f_{0}(X)^{\top}z\big{]}=V(\pi)-V(\pi_{f_{0}}),\] (4)

where \(V(\pi)=\mathbb{E}_{X}\big{[}f_{0}(X)^{\top}\pi(X)\big{]}\).

### Background: Off-Policy Evaluation and Optimization

An important special case is when \(\mathcal{Z}^{\angle}=\{(1,0,\ldots,0),\ldots,(0,\ldots,0,1)\}\) is the canonical basis and \(\mathcal{Z}\) is the simplex. This corresponds to choosing one of \(d\) actions (or randomizing between them). The full-feedback problem in this case is cost-sensitive classification [10]. In the bandit feedback case, with the restriction \(Z\in\mathcal{Z}^{\angle}\), it is known as a logged or offline contextual bandit, and a long line of literature studies the problems of estimating and optimizing \(V(\pi)\)[1, 8, 18, 19, 20, 23, 24, 32, 34, 35, 36, 42, 43, 44, 45]. It is closely related to causal inference, viewing each component of \(Y\) as the potential outcome for the corresponding action (or treatment), where we only see the realized outcome \(C\) corresponding to the factual action with index \(1\) in \(Z\in\mathcal{Z}^{\angle}\) and see nothing about counterfactual actions \(\mathcal{Z}^{\angle}\setminus\{Z\}\). Two key differentiating characteristics of our problem with a general polytope constraint \(\mathcal{Z}\) are the opportunity to leverage the linear cost structure to extrapolate from the costs of historical decisions to costs of other decisions, and the presence of non-trivial constraints on the output of decision policies.

Going beyond just finite discrete arms, [21, 22, 29] consider the logged contextual bandit with a _continuum_ of arms, which is not generally representable in terms of the problem in Section 1.2. These works make no restrictions on the relationship between the expected potential outcome and the corresponding action except generic smoothness, and leverage nonparametric approaches such as kernel weighting. Closest to our work is [6], which imposes a _semiparametric_ assumption on this relationship. This includes our problem (Section 1.2) as a special case under the restriction that, conditional on \(X\), expected outcomes are linear in actions (note that our \(\mathbb{E}[C\mid Z,X]\) is linear in \(Z\) according to ignorability in Section 1.2). Relative to this work, our unique contributions are the use of induced policy classes to naturally deal with the constraints in \(\mathcal{Z}\), the adaptation of computational approximation such as SPO+, and obtaining the fast rates for the regret under misspecification.

## 2 Induced Empirical Risk Minimization with Bandit Feedback

Our IERM approach in the bandit setting follows the same idea of Eq. (3), directly minimizing an estimate of expected costs. However, since we do not observe \(Y_{i}\) in our data, it is not as straightforward as a sample average, \(\sum_{i=1}^{n}Y_{i}^{\top}\pi(X_{i})/n\). Instead, we need alternative ways to identify \(V(\pi)\).

Define \(\Sigma_{0}(X)=\mathbb{E}\left[ZZ^{\top}\mid X\right]\) and recall \(f_{0}(X)=\mathbb{E}[Y\mid X]\), which we will use to characterize the policy value \(V(\pi)\). We refer to these functions as _nuisance functions_ following the existing literature on off-policy evaluation and learning in contextual bandits or reinforcement learning [39], because they are not directly used for decision-making but serve merely as intermediaries for evaluating the policy value. Let \(\theta(x,z,c;f,\Sigma)\) be a score function such that

\[\mathbb{E}_{P}\big{[}\theta(X,Z,C;f_{0},\Sigma_{0})^{\top}\pi(X)\big{]}=V(\pi),\ \ \forall\text{ fixed policy }\pi,\] (5)

where \(\mathbb{E}_{P}\) denotes taking expectation over \(P\). Proposition 1 summarizes a few possible choices.

**Proposition 1**.: _The following choices of \(\theta(x,z,c;f,\Sigma)\) all satisfy Eq. (5):_

1. _(Direct Method)_ \(\theta_{\text{DM}}(x,z,c;f,\Sigma)=f(x)\)_;_
2. _(Inverse Spectral Weighting)_ \(\theta_{\text{ISW}}(x,z,c;f,\Sigma)=\Sigma^{\dagger}(x)zc\)_;_
3. _(Doubly Robust)_ \(\theta_{\text{DR}}(x,z,c;f,\Sigma)=f(x)+\Sigma^{\dagger}(x)z(c-z^{\top}f(x))\)_._

_Here \(\Sigma^{\dagger}\) denotes the Moore-Penrose pseudo-inverse of matrix \(\Sigma\)._

The doubly robust identification in Proposition 1 is a generalization of the identification in [6] in that we allow for rank-deficient \(\Sigma_{0}\) by using a pseudo-inverse. Otherwise, our work significantly differs from [6] in terms of policy class specification, computation and theoretical analyses (see Section 1.3). Note that identification in Proposition 1 exploits the linearity of the decision cost, which significantly improve upon algorithms that ignore the linear structure and naively extend existing offline bandit learning using discrete actions \(\mathcal{Z}^{\angle}\); see Section 5 and Appendix C.2 for numerical evidence.

It remains to estimate Eq. (5), since we know neither \(P\) nor the nuisance functions \(f_{0},\Sigma_{0}\). Following the approach of [1, 45] for logged contextual bandits, we adapt the cross-fitting procedure advocated in [5]. For simplicity, we focus on the twofold version and assume \(n\) is even. The extension to \(K\)-fold version is straightforward.

First, we split \(\mathcal{D}\) into two equal-sized subsamples, \(\mathcal{D}_{1}\) and \(\mathcal{D}_{2}\), each with \(n/2\) data points. We can use \(\mathcal{D}_{1}\) as an auxiliary sample to estimate the nuisance functions \(f_{0}\) and \(\Sigma_{0}\), denoting the estimates as \(\hat{f}_{1}\) and \(\hat{\Sigma}_{1}\). We then use \(\mathcal{D}_{2}\) as a main sample to obtain an estimate of \(V(\pi)\) using \(\hat{f}_{1}\) and \(\hat{\Sigma}_{1}\):

\[\tfrac{2}{n}\sum_{i\in\mathcal{D}_{2}}\theta(X_{i},Z_{i},C_{i};\hat{f}_{1}, \hat{\Sigma}_{1})^{\top}\pi(X_{i}).\]

Of course, we can switch the roles of \(\mathcal{D}_{1}\) and \(\mathcal{D}_{2}\), _i.e._, we use \(\mathcal{D}_{2}\) to get nuisance estimates \(\hat{f}_{2}\) and \(\hat{\Sigma}_{2}\), then use \(\mathcal{D}_{1}\) to estimate \(V(\pi)\). Finally, given an induced policy class \(\Pi_{\mathcal{F}}\), the IERM policy \(\hat{\pi}\) minimizes the average of the above two \(V(\pi)\) estimates over \(\Pi_{\mathcal{F}}\):

\[\hat{\pi}\in\arg\min_{\pi\in\Pi_{\mathcal{F}}}\tfrac{1}{n}\sum_{j=1}^{2}\sum_ {i\in\mathcal{D}_{j}}\theta(X_{i},Z_{i},C_{i};\hat{f}_{3-j},\hat{\Sigma}_{3-j })^{\top}\pi(X_{i}).\] (6)

**Remark 1** (Estimation of \(f_{0}\)).: _The estimator \(\hat{f}(x)\) can be obtained by minimizing the least square loss over some appropriate function class \(\mathcal{F}^{\text{N}}\):_

\[\hat{f}\in\arg\min_{f\in\mathcal{F}^{\text{N}}}\sum_{i\in\mathcal{D}}\bigl{(} C_{i}-Z_{i}^{\top}f(X_{i})\bigr{)}^{2}.\] (7)

_Note that two places in Eq. (6) require the specification of a function class for modeling \(f_{0}\): the \(\mathcal{F}\) class used to induce the policy class \(\Pi_{\mathcal{F}}\), and the \(\mathcal{F}^{\text{N}}\) class used to construct nuisance estimators to evaluate the expected cost of induced policies. In practice, we do not need to use the same class for \(\mathcal{F}\) and \(\mathcal{F}^{\text{N}}\). In fact, it might be desirable to use a more flexible function class for \(\mathcal{F}^{\text{N}}\) to make sure it is well-specified for accurate policy evaluation, and a simpler class for \(\mathcal{F}\) to make the policy optimization more tractable. We numerically test out different choices of \(\mathcal{F}\) and \(\mathcal{F}^{\text{N}}\) in Section 5._

**Remark 2** (Estimation of \(\Sigma_{0}\)).: _There are multiple ways to estimate \(\Sigma_{0}\). For example, [6] suggest estimating \(\Sigma_{0}\) by running a multi-task regression for all \((j,k)\) entries to the matrix over some appropriate hypothesis spaces \(\mathcal{S}_{jk}\): \(\hat{\Sigma}=\arg\min_{\Sigma_{1}\in\mathcal{S}_{11},\dots,\Sigma_{dd}\in \mathcal{S}_{dd}}\sum_{i\in\mathcal{D}}\left\|Z_{i}Z_{i}^{\top}-\Sigma(X_{i}) \right\|_{Fro}^{2}\). To ensure a positive semi-definite estimator, we may posit each hypothesis \(\Sigma\) to be the outer product of some matrix-valued hypothesis of appropriate dimension. Alternatively, we may only need to consider finitely many feasible decisions \(z_{1},\dots,z_{m}\), such as the feasible paths for stochastic shortest path problems (see experiments in Section 5) or more generally the vertices in \(\mathcal{Z}^{\angle}\). Then we can first estimate the propensity scores \(e_{0}(z\mid X)\) for \(z=z_{1},\dots,z_{m}\) using an suitable estimator \(\hat{e}(z\mid X)\) and then estimate \(\Sigma_{0}(X)=\sum_{j=1}^{m}z_{j}z_{j}^{\top}e_{0}(z_{j}\mid X)\) by \(\hat{\Sigma}(X)=\sum_{j=1}^{m}z_{j}z_{j}^{\top}\hat{e}(z_{j}\mid X)\)._Theoretical Analysis

In this section, we provide a theoretical regret analysis for the IERM approach with bandit feedback, allowing for model misspecification in the induced policy class \(\Pi_{\mathcal{F}}\). That is, we allow the globally optimal policy \(\pi_{f_{0}}\) to be not included in the class \(\Pi_{\mathcal{F}}\). We derive an upper bound on the regret of the IERM approach, in terms of the complexity of the policy class \(\Pi_{\mathcal{F}}\), its misspecification bias, and the estimation errors of nuisance functions.

Before stating the main theorem, we define a few important notations. Let \(\tilde{\pi}^{*}\) be the best-in-class policy that minimizes expected regret over \(\Pi_{\mathcal{F}}\):

\[\tilde{\pi}^{*}\in\arg\min_{\pi\in\Pi_{\mathcal{F}}}\mathbb{E}\big{[}f_{0}(X)^ {\top}\pi(X)\big{]}.\]

We note that \(\tilde{\pi}^{*}\) can be different from the global optimal policy \(\pi_{f_{0}}\) if \(\pi_{f_{0}}\not\in\Pi_{\mathcal{F}}\), and \(\text{Reg}(\tilde{\pi}^{*})\) characterizes the extent of misspecification in the induced policy class \(\Pi_{\mathcal{F}}\). For \(j=1,2\), we define the function class

\[\mathcal{G}_{j}=\Bigg{\{}(x,z,c)\to\frac{\theta(x,z,c;\hat{f}_{j},\hat{\Sigma }_{j})^{\top}(\pi(x)-\tilde{\pi}^{*}(x))\rho}{2B\Theta}:\pi\in\Pi_{\mathcal{F}},\rho\in[0,1]\Bigg{\}}.\] (8)

For any function class \(\mathcal{G}:\mathcal{X}\times\mathcal{Z}\times\mathcal{C}\to\mathbb{R}\), we define the local Rademacher complexity as

\[\mathcal{R}_{n}(\mathcal{G},r)=\mathbb{E}\Big{[}\sup_{g\in\mathcal{G},\|g\|_{ 2}\leq r}\big{|}\tfrac{1}{n}\sum_{i=1}^{n}\epsilon_{i}g(X_{i},Z_{i},C_{i}) \big{|}\Big{]},\]

where \(\epsilon_{i}\) are i.i.d. Rademacher variables, and \(\|g\|_{2}=\sqrt{\mathbb{E}_{P}[g^{2}(X,Z,C)]}\).

Throughout Section 3, we impose two assumptions. Assumption 2 concerns the algorithms we use to estimate the nuisance functions \(f_{0},\Sigma_{0}\). It requires that the nuisance estimates are close enough to their true values, and \(\|\theta(x,z,c;\hat{f},\hat{\Sigma})\|\) is bounded.

**Assumption 2** (Nuisance Estimation).: _The nuisance estimators \(\hat{f},\hat{\Sigma}\) trained on a sample of size \(n\) satisfy that \(\|\theta(x,z,c;\hat{f},\hat{\Sigma})\|\leq\Theta\) for all \(x,z,c\), and for any \(\delta\in(0,1)\) and \(\pi\in\Pi_{\mathcal{F}}\), with probability at least \(1-\delta\),_

\[\mathbb{E}_{P}\bigg{[}\Big{(}\theta(X,Z,C;f_{0},\Sigma_{0})-\theta(X,Z,C;\hat {f},\hat{\Sigma})\Big{)}^{\top}(\pi(X)-\tilde{\pi}^{*}(X))\bigg{]}\leq\mathrm{ Rate}^{\emph{N}}(n,\delta).\]

In Section 3.1, we will relate \(\mathrm{Rate}^{\emph{N}}(n,\delta)\) to the errors in estimating the individual nuisance functions \(f_{0}\) and \(\Sigma_{0}\) for different choices of score function \(\theta\).

Assumption 3, which we term the margin condition, controls the density of the sub-optimality gap near zero in the CLO problem instance and allows us to get faster regret rates. This type of condition was originally considered in the binary classification literature [2; 38]. It is recently extended to contextual linear optimization by [14], which we describe below.

**Assumption 3** (Margin Condition).: _Let \(\mathcal{Z}^{*}(x)=\arg\min_{z\in\mathcal{Z}}f_{0}(x)^{\top}z\), and \(\Delta(x)=\inf_{z\in\mathcal{Z}^{\prime}\setminus\mathcal{Z}^{*}(x)}f_{0}(x)^ {\top}z-\inf_{z\in\mathcal{Z}}f_{0}(x)^{\top}z\) if \(\mathcal{Z}^{*}(x)\neq\mathcal{Z}\) and \(\Delta(x)=0\) otherwise. Assume for some \(\alpha,\gamma\geq 0\),_

\[\mathbb{P}_{X}(0<\Delta(X)\leq\delta)\leq(\gamma\delta/B)^{\alpha}\quad\forall \delta>0.\]

Lemmas 4 and 5 in [15] show that Assumption 3 generally holds with \(\alpha=1\) for sufficiently well-behaved \(f_{0}\) and continuous \(X\) and with \(\alpha=\infty\) for discrete \(X\). Moreover, any CLO instance trivially satisfies \(\alpha=0\). Overall, a larger \(\alpha\) means that the sub-optimality gap between the best and second-best decisions tends to be large in more contexts, so it is easier to distinguish the optimal decisions from others. We will show that a larger \(\alpha\) parameter could lead to faster regret rates.

### Main Theorem

We now provide an upper bound on the regret of the IERM policy \(\hat{\pi}\) in Eq. (6).

**Theorem 1**.: _Suppose Assumptions 2 and 3 hold, and \(\mathcal{Z}^{*}(X)\) defined in Assumption 3 is a singleton almost surely. Suppose there exists a positive number \(\tilde{r}\) (that depends on \(n\)) that upper bounds the critical radii of the function classes \(\mathcal{G}_{1},\mathcal{G}_{2}\) almost surely (i.e., \(\mathcal{R}_{n/2}(\mathcal{G}_{1},\tilde{r})\leq\tilde{r}^{2}\) and \(\mathcal{R}_{n/2}(\mathcal{G}_{2},\tilde{r})\leq\tilde{r}^{2}\))._\(\tilde{r}^{2}\)) and satisfies the inequalities \(3n\tilde{r}^{2}/128\geq\log\log_{2}(1/\tilde{r})\), and \(2\exp\bigl{(}-3n\tilde{r}^{2}/128\bigr{)}\leq\delta/4\). Then, there exists a positive constant \(\tilde{C}(\alpha,\gamma)\) such that with probability at least \(1-\delta\), we have_

\[\text{Reg}(\hat{\pi})\leq B\biggl{(}12\Theta\sqrt{\tilde{C}(\alpha,\gamma)\tilde{r}} \biggr{)}^{\frac{2\alpha+2}{\alpha+2}}+24B\Theta\Biggl{(}\sqrt{\tilde{C}( \alpha,\gamma)}\biggl{(}\frac{\text{Reg}(\tilde{\pi}^{*})}{B}\biggr{)}^{\frac{ \alpha}{2(1+\alpha)}}\tilde{r}+\tilde{r}^{2}\Biggr{)}\] \[+2\text{Rate}^{\textsf{N}}(n/2,\delta/4)+2\text{Reg}(\tilde{\pi }^{*}).\]

The upper bound in Theorem 1 involves several different types of error terms. The first type is the critical radii \(\tilde{r}\) that characterize the complexity of the function classes \(\mathcal{G}_{1},\mathcal{G}_{2}\). This is a common complexity measure for function classes in statistics and machine learning [41]. For example, as we will discuss below, the critical radii scale as \(\tilde{O}(\sqrt{\eta/n})\) if \(\mathcal{G}_{1},\mathcal{G}_{2}\) are VC subgraph classes of dimension \(\eta\). The second type is the term \(\text{Rate}^{\textsf{N}}(n/2,\delta/4)\) resulted from the errors in estimating the nuisance functions \(f_{0},\Sigma_{0}\). Similar nuisance estimation errors also appear in the previous literature on offline contextual bandit learning [1, 5, 45] or more general learning problems that involve nuisance functions [13]. Below we will further discuss this term for different choices of score functions \(\theta\). The third type is the misspecification error term \(\text{Reg}(\tilde{\pi}^{*})\). It is natural to expect a bigger decision regret when the misspecification error \(\text{Reg}(\tilde{\pi}^{*})\) is higher. In particular, when the policy class \(\Pi_{\mathcal{F}}\) is well-specified, i.e., \(\pi_{f_{0}}\in\Pi_{\mathcal{F}}\), we would have \(\text{Reg}(\tilde{\pi}^{*})=0\), and the regret upper bound would scale with the function class complexity through a fast rate \(O(\tilde{r}^{\frac{2\alpha+2}{\alpha+2}})\). For VC subgraph classes with \(\tilde{r}=\tilde{O}(\sqrt{\eta/n})\), the rate can range from \(\tilde{O}(\sqrt{\eta/n})\) for \(\alpha=0\) to \(\tilde{O}(\eta/n)\) for \(\alpha=\infty\). However, if the policy class is misspecified so that \(\text{Reg}(\tilde{\pi}^{*})>0\), then the dominating term related to \(\tilde{r}\) would be a slow rate \(O(\tilde{r})\). This reveals an interesting phase transition between the correctly specified and misspecified settings, which is not discovered in the previous theory that considers only well-specification [14].

**Remark 3**.: _The constant coefficients in the regret upper bound can be improved when \(2(1+\alpha)/\alpha\) is an integer (which accommodates the case of \(\alpha=1\) justified in [15]):_

\[\text{Reg}(\hat{\pi})\leq B\biggl{(}12\Theta\sqrt{\tilde{C}(\alpha,\gamma)\tilde{r}} \biggr{)}^{\frac{2\alpha+2}{\alpha+2}}+\frac{24(\alpha+1)}{\alpha+2}B\Theta \Biggl{(}\sqrt{\tilde{C}(\alpha,\gamma)}\biggl{(}\frac{\text{Reg}(\tilde{\pi} ^{*})}{B}\biggr{)}^{\frac{\alpha}{2(1+\alpha)}}\tilde{r}+\tilde{r}^{2}\Biggr{)}\] \[+\frac{2\alpha+2}{\alpha+2}\text{Rate}^{\textsf{N}}(n/2,\delta/ 4)+\text{Reg}(\tilde{\pi}^{*}).\]

_Notably, the constant in front of the misspecification error \(\text{Reg}(\tilde{\pi}^{*})\) becomes \(1\) instead of \(2\), which we believe is tight. This upper bound follows from nearly the same proof as Theorem 1 except that it handles an inequality slightly differently. Specifically, the proof of Theorem 1 involves a transcendental inequality of the form \(\text{Reg}(\hat{\pi})\leq c_{1}\text{Reg}(\hat{\pi})^{\frac{1}{2(1+\alpha)}} \tilde{r}+c_{2}\) for certain positive terms \(c_{1},c_{2}\). This inequality is difficult to solve exactly, so we can only get an upper bound on its solution. It turns out that we can get a better upper bound when \(2(1+\alpha)/\alpha\) is an integer._

The nuisance estimation rate.We now show that \(\text{Rate}^{\textsf{N}}(n,\delta)\) can be effectively controlled for the DM, ISW and DR score functions. In pariticular, \(\text{Rate}^{\textsf{N}}(n,\delta)\) is can be bounded by the estimation errors of the nuisance functions \(f_{0}\) and \(\Sigma_{0}\).

**Proposition 2**.: _For any given \(\delta\in(0,1)\), let \(\chi_{n,\delta}\) be a positive sequence converging to \(0\) as \(n\to\infty\), such that the mean square errors of the nuisance estimates satisfy the following with probability at least \(1-\delta\):_

\[\max\Bigl{\{}\mathbb{E}_{X}[\|\mathrm{Proj}_{\mathrm{span}(\mathcal{Z})}(\hat{f }(X)-f_{0}(X))\|^{2}],\mathbb{E}_{X}[\|\hat{\Sigma}^{\dagger}(X)-\Sigma_{0}^{ \dagger}(X)\|_{\text{Fro}}^{2}]\Bigr{\}}\leq\chi_{n,\delta}^{2},\]

_where \(\mathrm{Proj}_{\mathrm{span}(\mathcal{Z})}(\hat{f}(X)-f_{0}(X))\) is the projection of \(\hat{f}(X)-f_{0}(X)\) onto \(\mathrm{span}(\mathcal{Z})\). Then,_

1. _If we take_ \(\theta=\theta_{\text{DM}}\)_, we have_ \(\text{Rate}^{\textsf{N}}_{\text{DM}}(n,\delta)=O(\chi_{n,\delta})\)_;_
2. _If we take_ \(\theta=\theta_{\text{ISW}}\)_, we have_ \(\text{Rate}^{\textsf{N}}_{\text{ISW}}(n,\delta)=O(\chi_{n,\delta})\)_;_
3. _If we take_ \(\theta=\theta_{\text{DR}}\)_, we have_ \(\text{Rate}^{\textsf{N}}_{\text{DR}}(n,\delta)=O(\chi_{n,\delta}^{2}).\)__

Compared to the DM and ISW scores, the impact of the estimation errors of the nuisances in the DR score is of only second order, _i.e._, \(O(\chi_{n,\delta}^{2})\) instead of \(O(\chi_{n,\delta})\). This echos the benefit of DR methodsin causal inference and offline contextual bandit learning [1; 5; 6]. Notably, here we only need to bound the projected error on the nuisance estimator \(\hat{f}\), which handles the setting when \(\text{span}(\mathcal{Z})\) does not cover the whole \(\mathbb{R}^{d}\) space, as is the case with the shortest path problem in Section 5.

Computing the critical radius.The critical radius, \(\tilde{r}\) characterizes the complexity of the function classes \(\mathcal{G}_{1}\) and \(\mathcal{G}_{2}\) defined in Eq. (8). The next proposition shows that \(\tilde{r}\) is of order \(\tilde{O}(1/\sqrt{n})\) if the function classes have finite VC-subgraph dimensions. For simplicity, we focus on \(\mathcal{G}_{1}\) only.

**Proposition 3**.: _Suppose \(\mathcal{G}_{1}\) has VC-subgraph dimension \(\eta\) almost surely. Then for any \(\delta\in(0,1)\), there exists a universal positive constant \(\tilde{C}\) such that_

\[\tilde{r}=\tilde{C}\sqrt{\frac{\eta\log(n+1)+\log(8/\delta)}{n}}\] (9)

_satisfies the inequalities \(\mathcal{R}_{n}(\mathcal{G}_{1},\tilde{r})\leq\tilde{r}^{2}\), \(3n\tilde{r}^{2}/64\geq\log\log_{2}(1/\tilde{r})\), and \(2\exp\bigl{(}-3n\tilde{r}^{2}/64\bigr{)}\leq\delta/4\)._

### Byproduct: Fast Rates in the Full Feedback Setting with Misspecification

Although our main theorem is stated for the bandit feedback setting, our regret analysis techniques can be easily adapted to the full feedback setting. The following theorem states a similar regret upper bound. To our best knowledge, this is the first result for CLO that shows a margin-dependent fast rate with potential policy misspecification in the full feedback setting.

**Theorem 2**.: _Suppose \(\mathcal{Z}^{*}(X)\) defined in Assumption 3 is a singleton almost surely. Define_

\[\mathcal{G}^{\text{F}}=\Bigl{\{}(x,y)\to\tfrac{y^{\top}(\pi(x)-\tilde{\pi}^{* }(x))\rho}{2B}:\pi\in\Pi_{\mathcal{F}},\rho\in[0,1]\Bigr{\}}\]

_and \(\tilde{r}^{\text{F}}\) be any solution to the inequality \(\mathcal{R}_{n}(\mathcal{G}^{\text{F}},r)\leq r^{2}\) satisfying \(3n(\tilde{r}^{\text{F}})^{2}/64\geq\log\log_{2}(1/\tilde{r}^{\text{F}})\) and \(2\exp\bigl{(}-3n(\tilde{r}^{\text{F}})^{2}/64\bigr{)}\leq\delta\). If Assumption 3 further holds, then, with probability at least \(1-\delta\),_

\[\text{Reg}(\hat{\pi}^{\text{F}})\leq B\biggl{(}12\sqrt{\tilde{C}(\alpha, \gamma)}\tilde{r}^{\text{F}}\biggr{)}^{\frac{2\alpha+2}{\alpha+2}}+24B\biggl{(} \sqrt{\tilde{C}(\alpha,\gamma)}\Bigl{(}\tfrac{\text{Reg}(\tilde{\pi}^{*})}{B }\Bigr{)}^{\frac{\alpha}{2(1+\alpha)}}\tilde{r}^{\text{F}}+(\tilde{r}^{\text{ F}})^{2}\biggr{)}+2\text{Reg}(\tilde{\pi}^{*}).\]

The regret bound is similar to that in Theorem 1 for the bandit setting, except that it does not have the nuisance error term \(2\text{Rate}^{\text{N}}(n/2,\delta/4)\). This is because, in the full feedback setting, we observe the entire \(Y\) vector, so we do not need to estimate any nuisance functions and can consider the nuisance estimation error term \(\text{Rate}^{\text{N}}(n/2,\delta/4)\) as zero. When \(\Pi_{\mathcal{F}}\) is a well-specified VC-subgraph class with dimension \(\eta\), we have \(\text{Reg}(\tilde{\pi}^{*})=0\), and \(\tilde{r}^{\text{F}}=\tilde{O}(\sqrt{\eta/n})\), so the bound in Theorem 2 reduces to \(O((\eta/n)^{(\alpha+1)/(\alpha+2)}+\eta/n)\). This bound interpolates between \(O(n^{-1/2})\) and \(O(n^{-1})\) according to the margin parameter \(\alpha\), recovering the fast rate in the full-feedback setting without misspecification as given in [14]. In contrast, our bound in Theorem 2 additionally quantifies the impact of policy misspecification, and its generalization Theorem 1 further incorporates the impact of nuisance estimation errors in the bandit-feedback setting.

## 4 Computationally Tractable Surrogate Loss

The IERM objective is generally nonconvex in \(f\in\mathcal{F}\), making it computationally intractable to optimize. In the full feedback setting, tractable surrogate losses have been proposed [11; 16]. In this section, we briefly explain the SPO+ loss in [11] and how it can be used in the bandit setting.

The full feedback IERM problem in Eq. (3) can be viewed as minimizing the following loss over \(\mathcal{F}\):

\[\min_{f\in\mathcal{F}}\tfrac{1}{n}\sum_{i=1}^{n}l_{\text{IERM}}(f(X_{i}),Y_{i }),\quad\text{where }l_{\text{IERM}}(f(x),y)=y^{\top}\pi_{f}(x)-\min_{z\in \mathcal{Z}}y^{\top}z.\]

This IERM loss is equivalent to the "smart predict-then-optimize" (SPO) loss in Definition 1 of [11]. Letting \(z^{*}(y)\in\arg\min_{z\in\mathcal{Z}}y^{\top}z\) with the same tie-breaking rule as in \(\pi_{f}\), [11] propose the SPO+ surrogate loss:

\[\min_{f\in\mathcal{F}}\tfrac{1}{n}\sum_{i=1}^{n}l_{\text{SPO+}}(f(X_{i}),Y_{i}),\text{ where }l_{\text{SPO+}}(f(x),y)=\max_{z\in\mathcal{Z}}\;(y-2f(x))^{\top}z-(y-2f(x))^{ \top}z^{*}(y).\]

The SPO+ loss has many desirable properties: given any fixed \(y\), it is an upper bound for the IERM loss, it is convex in \(f(x)\), and its subgradient at \(f(x)\) has a closed form \(2(z^{*}(y)-z^{*}(2f(x)-y))\).

In the bandit setting, although \(Y_{i}\) is not observed, the score \(\theta(X_{i},Z_{i},C_{i};\hat{f},\hat{\Sigma})\) plays the same role (see Eqs. (3) and (6)). So it is natural to adapt the SPO+ loss to the bandit setting by replacing the unobserved cost vector \(Y_{i}\) by the corresponding score \(\theta(X_{i},Z_{i},C_{i};\hat{f},\hat{\Sigma})\):

\[\hat{f}_{\text{SPO+}}\in\arg\min_{f\in\mathcal{F}}\ \frac{1}{n}\sum_{j=1,2} \sum_{i\in\mathcal{D}_{j}}l_{\text{SPO+}}\Big{(}f(X_{i}),\theta(X_{i},Z_{i},C_ {i};\hat{f}_{3-j},\hat{\Sigma}_{3-j})\Big{)}.\]

Then we use the plug-in policy \(\pi_{\text{f}_{\text{SPO+}}}\) as the decision policy. This is implemented in the experiments in Section 5. We can similarly adapt any surrogate loss for the full-feedback IERM problem to the bandit feedback setting, simply replacing the cost vector \(Y_{i}\)'s by the corresponding scores.

## 5 Numerical Experiments

We now test the performance of our proposed methods in a simulated stochastic shortest path problem following [11; 14]. Specifically, we aim to go from the start node \(s\) to the end node \(t\) on a \(5\times 5\) grid consisting of \(d=40\) edges, where the costs of traveling on the edges are given by the random vector \(Y\in\mathbb{R}^{40}\) (see Fig. 1). We consider covariates \(X\in\mathbb{R}^{3}\) and a function \(f_{0}(x)=\mathbb{E}[Y\mid X=x]\) whose components are cubic polynomials. The corresponding shortest path problem can be easily formulated into a CLO problem with the constraint set \(\mathcal{Z}\) given by standard flow preservation constraints. The resulting optimal solution \(z\) belongs to \(\{0,1\}\)4, indicating whether passing each edge or not. We note that there are \(m=70\) feasible paths \(z_{1},\dots,z_{m}\in\mathcal{Z}\) from the source node to the target node, and the feasible paths are linearly dependent with a rank of \(18\).

Footnote 4: The code scripts for all experiments can be found at https://github.com/CausalML/CLOBandit.

We consider a bandit feedback setting, observing only the total traveling costs \(C\) of the historical decisions \(Z\) generated by a certain logging policy but not the edge-wise costs \(Y\). We consider different types of logging policies that generate the decisions in the observed data. In this section, we report results using a random logging policy that picks a path from all feasible ones at random regardless of the covariate value. In Appendix C, we further study the performance of two different covariate-dependent logging policies: one that picks paths according to the sign of the first covariate \(X_{1}\), and one that depends on the signs of both \(X_{1}\) and \(X_{2}\). The empirical insights from the covariate-dependent logging policies are qualitatively the same as those for the random logging policy.

We numerically evaluate the performance of the ETO approach and the IERM approach2. For both approaches, we use the same class \(\mathcal{F}\) to construct the decision policies. We consider three different classes for \(\mathcal{F}\): a correctly specified polynomial class, a misspecified class that omits two high-order terms (termed degree-2 misspecification), and a misspecified class that omits four high-order terms (termed degree-4 misspecification). For the IERM approach, we consider DM and DR here but defer the results of ISW to Appendix C.2 Table 3 for its significantly worse performance. In both DM and DR, the nuisance \(f_{0}\) is estimated by a bandit-feedback regression given in Eq. (7) with a ridge penalty, and we test out the three aforementioned function classes for the nuisance class \(\mathcal{F}^{\text{N}}\) as well. In DR, the nuisance \(\Sigma_{0}(x)\) is estimated by the propensity score approach described in Remark 2,

Figure 1: Stochastic Shortest path problem on a \(5\times 5\) grid with uncertain edge cost \(Y_{j}\) and decision \(z_{j}\) for \(j=1,\dots,40\).

with the propensity scores estimated by either sample frequencies (for the random logging policy) or suitable decision tree models (for covariate-dependent logging policies). We further consider three variants when plugging \(\hat{\Sigma}\) into the doubly robust score: pseudo-inverse (DR PI) as in the original \(\theta_{\text{DR}}\) definition; lambda regularization (DR Lambda), where we replace \(\Sigma^{\dagger}\) with \((\Sigma+\lambda I)^{-1}\) for a positive constant \(\lambda\); and clipping (DR Clip), where eigenvalues of \(\Sigma\) below a certain threshold are clipped to the threshold before taking the pseudo-inverse. For all IERM variants, we optimize the SPO+ losses as discussed in Section 4. Further details on the experimentation setup and implementation are summarized in Appendix C. Finally, we consider some naive extensions of the offline contextual bandit learning with finite discrete actions (Naive ETO and Naive SPO+), where we view the feasible paths as separate discrete actions, without considering the linear structure of the decision-making problem. See Appendix A for details and Section 1.3 for background on offline bandits.

We test the performance of different methods on an independent testing sample of size \(2000\), and evaluate the ratio of their regrets relative to the expected cost of the global optimal policy \(\pi_{f_{0}}\). Table 1 shows the average relative regret ratios of different methods across \(50\) replications of the experiment for a random logging policy. Due to space limitations, we include results for the training data size \(n=400,1000,1600\), and defer results for other sizes to Appendix C.2. The relative regret of all methods properly decrease with the training data size \(n\). In particular, the SPO+ approximation for the end-to-end IERM approach perform better than the ETO method. Among the SPO+ methods, the DM score achieves the best performance, while the DR score based pesudo-inverse performs the worst. Through a close inspection, we found that the bias adjustment term that involves the pseudo-inverse in the DR score causes a significant variance inflation. In fact, the ISW score also performs badly due to the high variance (see Appendix C.2). The Lambda regularization and Clip techniques can effectively reduce the variance and result in improved decision-making. Moreover, the naive benchmarks that ignore the linear structure of the decision costs have much worse performance. This shows the importance of leveraging the linear problem structure.

In Table 2, we show the performance of different methods when either the policy-inducing model \(\mathcal{F}\) or the nuisance model \(\mathcal{F}^{\text{N}}\) or both are misspecified. We observe that when the policy-inducing model is misspecified, the end-to-end SPO+ methods perform much better than the ETO method, provided that the nuisance model for SPO+ is correctly specified. This is consistent with the findings in the previous full-feedback CLO literature (e.g., 11; 12; 14), showing the benefit of integrating

\begin{table}
\begin{tabular}{c|c|c c|c c c|} \hline \hline  & Methods & \multicolumn{3}{c|}{Training Data \(n\)} & \multicolumn{3}{c|}{Training Data \(n\)} \\ \cline{3-8}  & & 400 & 1000 & 1600 & 400 & 1000 & 1600 \\ \hline \multirow{4}{*}{\begin{tabular}{c} Well-specified \\ Nuisance Model \\ \(\mathcal{F}^{\text{N}}\) \\ \end{tabular} } & ETO & \multicolumn{3}{c|}{\(\mathcal{F}\) misspecified degree 2} & \multicolumn{3}{c|}{\(\mathcal{F}\) misspecified degree 4} \\ \cline{3-8}  & & 11.04\% & 9.14\% & 8.34\% & 12.35\% & 11.42\% & 10.39\% \\ \hline \multirow{4}{*}{\begin{tabular}{c} Well-specified \\ Nuisance Model \\ \(\mathcal{F}^{\text{N}}\) \\ \end{tabular} } & SPO+ DM & 2.81\% & 0.80\% & 0.54\% & 4.06\% & 2.21\% & 2.06\% \\  & SPO+ DR PI & 3.27\% & 1.36\% & 1.05\% & 4.83\% & 2.95\% & 2.71\% \\  & SPO+ DR Lambda & 2.83\% & 0.97\% & 0.73\% & 4.33\% & 2.45\% & 2.25\% \\  & SPO+ DR Clip & 3.05\% & 1.09\% & 0.84\% & 4.59\% & 2.62\% & 2.38\% \\ \hline \multirow{4}{*}{\begin{tabular}{c} Well-specified \\ Policy-inducing \\ Model \(\mathcal{F}\) \\ \end{tabular} } & SPO+ DM & 10.01\% & 8.37\% & 7.47\% & 12.51\% & 11.22\% & 9.68\% \\  & SPO+ DR PI & 9.11\% & 7.02\% & 6.44\% & 11.69\% & 10.19\% & 9.02\% \\  & SPO+ DR Lambda & 9.05\% & 7.52\% & 6.68\% & 12.31\% & 10.38\% & 8.96\% \\  & SPO+ DR Clip & 9.02\% & 7.28\% & 6.36\% & 11.87\% & 10.04\% & 8.70\% \\ \hline \multirow{4}{*}{
\begin{tabular}{c} Both \(\mathcal{F},\mathcal{F}^{\text{N}}\) \\ Misspecified \\ \end{tabular} } & SPO+ DM & 9.90\% & 8.34\% & 7.41\% & 12.45\% & 11.16\% & 9.69\% \\  & SPO+ DR PI & 9.15\% & 7.23\% & 6.52\% & 11.92\% & 10.46\% & 9.42\% \\  & SPO+ DR Lambda & 9.03\% & 7.46\% & 6.74\% & 12.01\% & 10.72\% & 9.25\% \\  & SPO+ DR Clip & 8.97\% & 7.22\% & 6.46\% & 11.75\% & 10.31\% & 8.95\% \\ \hline \hline \end{tabular}
\end{table}
Table 2: Mean relative regret ratio of different methods when the nuisance model \(\mathcal{F}^{\text{N}}\) and the policy-inducing model \(\mathcal{F}\) are misspecified to different degrees. The logging policy is a random policy.

estimation and optimization for misspecified policy models. However, the advantage of the end-to-end approaches is weakened dramatically once the nuisance model is misspecified. In this case, the evaluation of decision policies is biased, so the end-to-end approaches also target a "wrong" objective that may not accurately capture the decision quality. Moreover, we observe that when the nuisance model is misspecified, the DR score can somewhat outperform the DM score, because it can debias the misspecified nuisance to some extent. These results demonstrate new challenges with the bandit-feedback CLO: the end-to-end approaches are sensitive to the misspecification of nuisance models, and the DM and DR scores face different bias-and-variance tradeoffs under nuisance model misspecification. Therefore, in practice we may prefer more flexible models for accurate nuisance modeling, while using simpler policy-inducing models for tractable end-to-end optimization.

## 6 Conclusions

This paper studies the bandit-feedback setting for contextual linear optimization for the first time. We adapt the induced empirical risk minimization approach to this setting, provide a novel theoretical analysis for the regret of the resulting policies, leverage surrogate losses for efficient optimization, and empirically demonstrate the performance of the proposed methods across different model specifications. Our paper has a few limitations that we aim to address in the future. First, we primarily consider parametric induced policy classes, and it would be interesting to accommodate more general nonparametric classes. Second, we focus mainly on the SPO+ surrogate loss, and investigating other surrogate losses in the bandit feedback setting would also be of great interest.

## Acknowledgments and Disclosure of Funding

Nathan Kallus acknowledges that this material is based upon work supported by the National Science Foundation under Grant No. 1846210. Xiaojie Mao is supported in part by National Natural Science Foundation of China (grant numbers 72322001, 72201150, and 72293561) and National Key R&D Program of China (grant number 2022ZD0116700).

## References

* [1] Susan Athey and Stefan Wager. Policy learning with observational data. _Econometrica_, 89(1):133-161, 2021.
* [2] Jean-Yves Audibert and Alexandre B Tsybakov. Fast learning rates for plug-in classifiers. _The Annals of statistics_, 35(2):608-633, 2007.
* [3] Stephane Boucheron, Gabor Lugosi, Pascal Massart, et al. Concentration inequalities using the entropy method. _The Annals of Probability_, 31(3):1583-1614, 2003.
* [4] Olivier Bousquet. Concentration inequalities for sub-additive functions using the entropy method. In _Stochastic inequalities and applications_, pages 213-247. Springer, 2003.
* [5] Victor Chernozhukov, Denis Chetverikov, Mert Demirer, Esther Duflo, Christian Hansen, Whitney Newey, and James Robins. Double/debiased machine learning for treatment and structural parameters. _The Econometrics Journal_, 21(1):C1-C68, 2018.
* [6] Victor Chernozhukov, Mert Demirer, Greg Lewis, and Vasilis Syrgkanis. Semi-parametric efficient policy learning with continuous actions. _Advances in Neural Information Processing Systems_, 32, 2019.
* [7] Miroslav Dudik, John Langford, and Lihong Li. Doubly robust policy evaluation and learning. _arXiv preprint arXiv:1103.4601_, 2011.
* [8] Miroslav Dudik, Dumitru Erhan, John Langford, and Lihong Li. Doubly robust policy evaluation and optimization. 2014.
* [9] Othman El Balghiti, Adam N Elmachtoub, Paul Grigas, and Ambuj Tewari. Generalization bounds in the predict-then-optimize framework. In _NeurIPS_, pages 14412-14421, 2019.
* [10] Charles Elkan. The foundations of cost-sensitive learning. In _International joint conference on artificial intelligence_, volume 17, pages 973-978. Lawrence Erlbaum Associates Ltd, 2001.

* [11] Adam N Elmachtoub and Paul Grigas. Smart "predict, then optimize". _Management Science_, 68(1):9-26, 2022.
* [12] Adam N Elmachtoub, Henry Lam, Haofeng Zhang, and Yunfan Zhao. Estimate-then-optimize versus integrated-estimation-optimization: A stochastic dominance perspective. _arXiv preprint arXiv:2304.06833_, 2023.
* [13] Dylan J Foster and Vasilis Syrgkanis. Orthogonal statistical learning. _The Annals of Statistics_, 51(3):879-908, 2023.
* [14] Yichun Hu, Nathan Kallus, and Xiaojie Mao. Fast rates for contextual linear optimization. _Management Science_, 68(6):4236-4245, 2022.
* [15] Yichun Hu, Nathan Kallus, and Masatoshi Uehara. Fast rates for the regret of offline reinforcement learning. _Mathematics of Operations Research_, 2024.
* [16] Michael Huang and Vishal Gupta. Learning best-in-class policies for the predict-then-optimize framework. _arXiv preprint arXiv:2402.03256_, 2024.
* [17] Guido W Imbens and Donald B Rubin. _Causal inference in statistics, social, and biomedical sciences_. Cambridge university press, 2015.
* [18] Nathan Kallus. Recursive partitioning for personalization using observational data. In _Proceedings of the 34th International Conference on Machine Learning-Volume 70_, pages 1789-1798. JMLR. org, 2017.
* [19] Nathan Kallus. Balanced policy evaluation and learning. In _Advances in Neural Information Processing Systems_, pages 8895-8906, 2018.
* [20] Nathan Kallus. More efficient policy learning via optimal retargeting. _Journal of the American Statistical Association_, 116(534):646-658, 2021.
* [21] Nathan Kallus and Masatoshi Uehara. Doubly robust off-policy value and gradient estimation for deterministic policies. _Advances in Neural Information Processing Systems_, 33:10420-10430, 2020.
* [22] Nathan Kallus and Angela Zhou. Policy evaluation and optimization with continuous treatments. In _International conference on artificial intelligence and statistics_, pages 1243-1251. PMLR, 2018.
* [23] Nathan Kallus, Xiaojie Mao, Kaiwen Wang, and Zhengyuan Zhou. Doubly robust distributionally robust off-policy evaluation and learning. In _International Conference on Machine Learning_, pages 10598-10632. PMLR, 2022.
* [24] Toru Kitagawa and Aleksey Tetenov. Who should be treated? empirical welfare maximization methods for treatment choice. _Econometrica_, 86(2):591-616, 2018.
* [25] Tor Lattimore and Csaba Szepesvari. _Bandit algorithms_. Cambridge University Press, 2020.
* [26] Kelsey Maass, Arun V Sathanur, Arif Khan, and Robert Rallo. Street-level travel-time estimation via aggregated user data. In _2020 Proceedings of the SIAM Workshop on Combinatorial Scientific Computing_, pages 76-84. SIAM, 2020.
* [27] Jayanta Mandi, James Kotary, Senne Berden, Maxime Mulamba, Victor Bucarey, Tias Guns, and Ferdinando Fioretto. Decision-focused learning: Foundations, state of the art, benchmark and future opportunities. _arXiv preprint arXiv:2307.13565_, 2023.
* [28] David Pollard. Empirical processes: theory and applications. In _NSF-CBMS regional conference series in probability and statistics_, pages i-86. JSTOR, 1990.
* [29] Noveen Sachdeva, Lequn Wang, Dawen Liang, Nathan Kallus, and Julian McAuley. Off-policy evaluation for large action spaces via policy convolution. In _Proceedings of the ACM on Web Conference 2024_, page 3576-3585, 2024.

* [30] Utsav Sadana, Abhilash Chenreddy, Erick Delage, Alexandre Forel, Emma Frejinger, and Thibaut Vidal. A survey of contextual optimization methods for decision-making under uncertainty. _European Journal of Operational Research_, 2024.
* [31] Bodhisattva Sen. A gentle introduction to empirical process theory and applications. _Lecture Notes, Columbia University_, 11:28-29, 2018.
* [32] Nian Si, Fan Zhang, Zhengyuan Zhou, and Jose Blanchet. Distributionally robust batch contextual bandits. _Management Science_, 69(10):5772-5793, 2023.
* [33] Yeran Sun, Yinming Ren, and Xuan Sun. Uber movement data: A proxy for average one-way commuting times by car. _ISPRS International Journal of Geo-Information_, 9(3):184, 2020.
* [34] Adith Swaminathan and Thorsten Joachims. Batch learning from logged bandit feedback through counterfactual risk minimization. _The Journal of Machine Learning Research_, 16(1):1731-1755, 2015.
* [35] Adith Swaminathan and Thorsten Joachims. Counterfactual risk minimization: Learning from logged bandit feedback. In _International Conference on Machine Learning_, pages 814-823. PMLR, 2015.
* [36] Adith Swaminathan and Thorsten Joachims. The self-normalized estimator for counterfactual learning. _advances in neural information processing systems_, 28, 2015.
* [37] Michel Talagrand. New concentration inequalities in product spaces. _Inventiones mathematicae_, 126(3):505-563, 1996.
* [38] Alexander B Tsybakov. Optimal aggregation of classifiers in statistical learning. _The Annals of Statistics_, 32(1):135-166, 2004.
* [39] Masatoshi Uehara, Chengchun Shi, and Nathan Kallus. A review of off-policy evaluation in reinforcement learning. _arXiv preprint arXiv:2212.06355_, 2022.
* [40] Aad W Van Der Vaart and Jon A Wellner. _Weak convergence and empirical processes_. Springer, 1996.
* [41] Martin J Wainwright. _High-dimensional statistics: A non-asymptotic viewpoint_, volume 48. Cambridge University Press, 2019.
* [42] Baqun Zhang, Anastasios A Tsiatis, Marie Davidian, Min Zhang, and Eric Laber. Estimating optimal treatment regimes from a classification perspective. _Stat_, 1(1):103-114, 2012.
* [43] Yingqi Zhao, Donglin Zeng, A John Rush, and Michael R Kosorok. Estimating individualized treatment rules using outcome weighted learning. _Journal of the American Statistical Association_, 107(499):1106-1118, 2012.
* [44] Xin Zhou, Nicole Mayer-Hamblett, Umer Khan, and Michael R Kosorok. Residual weighted learning for estimating individualized treatment rules. _Journal of the American Statistical Association_, 112(517):169-187, 2017.
* [45] Zhengyuan Zhou, Susan Athey, and Stefan Wager. Offline multi-action policy learning: Generalization and optimization. _Operations Research_, 71(1):148-183, 2023.

Naive Extensions of Offline Contextual Bandit Learning

In this section, we show some alternative approaches to solve the stochastic shortest path problem in Section 5. These approaches can be viewed as naive extensions of offline contextual bandit learning with discrete actions.

Specifically, consider the feasible paths \(z_{1},\ldots,z_{m}\) for \(m=70\). We can view them as separate discrete actions, and a feasible decision policy \(\pi\) is a mapping from the covariates \(X\) to one of the \(m=70\) feasible paths. We now adopt one-hot-encoding for the decisions. Consider a simplex \(\mathcal{Z}^{\text{simplex}}\) in \(\mathbb{R}^{m}\) and zero-one vector \(\tilde{z}\in\{0,1\}^{m}\) that takes the value \(1\) on one and only one of its coordinate. Each feasible decision in \(z_{1},\ldots,z_{m}\) corresponds to one vector \(\tilde{z}\), e.g., the decision \(z_{j}\) corresponds to the \(\tilde{z}\) vector whose \(j\)-th entry is \(1\) and other entries are all \(0\).

Our decision-making problem restricted to the feasible decisions \(z_{1},\ldots,z_{m}\) can be written as follows:

\[\min_{\tilde{z}\in\mathcal{Z}^{\text{simplex}}}\sum_{j=1}^{m}\tilde{z}_{j} \mathbb{E}[C\mid Z=z_{j},X=x]\iff\min_{\tilde{z}\in\mathcal{Z}^{\text{ simplex}}}\tilde{z}^{\top}\tilde{f}_{0}(x),\]

where \(\tilde{f}_{0}(x)=(\mathbb{E}[C\mid Z=z_{1},X=x],\ldots,\mathbb{E}[C\mid Z=z_{ m},X=x])^{\top}\). For any given \(x\), the resulting decision will be an one-hot vector that corresponds to an optimal decision at \(x\) (which can be equivalently given by \(\pi_{f_{0}}(x)\) for an plug-in policy in Eq. (2) at the true \(f_{0}(x)=\mathbb{E}[Y\mid X=x]\)). In this formulation, we view the decisions \(z_{1},\ldots,z_{m}\) as separate discrete actions, and we do not necessarily take into account the linear structure of the decision cost.

We can easily adapt the bandit-feedback ETO to this new formulation. Specifically, we can first construct an estimator \(\hat{\tilde{f}}\) for the \(\tilde{f}_{0}\) function. For offline bandit learning with discrte actions, this would usually be implemented by regressing the observed total cost \(C\) with respect to the covariates \(X\), within each subsample for each of the feasible decisions respectively. Given the estimator \(\hat{\tilde{f}}\), we can then solve the optimization problem \(\min_{\tilde{z}\in\mathcal{Z}^{\text{simplex}}}\tilde{z}^{\top}\hat{\tilde{f}}_ {0}(x)\). We finally inspect which coordinate of the resulting solution is equal to \(1\) and choose it as the decision.

To adapt the IERM approach, we similarly define the plug-in policy for any given hypothesis \(\tilde{f}(x):\mathbb{R}^{p}\rightarrow\mathbb{R}^{m}\) for the function \(\tilde{f}_{0}(x)=(\mathbb{E}[C\mid Z=z_{1},X=x],\ldots,\mathbb{E}[C\mid Z=z_{ m},X=x])^{\top}\):

\[\tilde{\pi}_{\tilde{f}}(x)\in\arg\min_{\tilde{z}\in\mathcal{Z}^{\text{simplex}}}\tilde{f}(x)^{\top}\tilde{z},\]

where ties are again broken by some fixed rules. Given a function class \(\tilde{\mathcal{F}}\), we can then consider the induced policy class \(\tilde{\Pi}_{\tilde{\mathcal{F}}}=\{\tilde{\pi}_{\tilde{f}}:\tilde{f}\in \tilde{\mathcal{F}}\}\). For any policy \(\tilde{\pi}\in\tilde{\Pi}_{\tilde{\mathcal{F}}}\), its output is an one-hot vector, whose entry with value \(1\) corresponds to the chosen decision among \(z_{1},\ldots,z_{m}\). For any observed decision \(Z\in\{z_{1},\ldots,z_{m}\}\), we denote its one-hot transformation \(\tilde{Z}\) as the zero-one vector whose value-one entry corresponds to the value of \(Z\). For a given observed total cost \(C\), we denote \(\tilde{C}\) as the vector all of whose entries are equal to \(C\). In the lemma below, we show that the value of each policy \(\tilde{\pi}\) can be also identified by some score funtions.

**Lemma 1**.: _For any given policy \(\tilde{\pi}\) that maps any covariate value \(x\) to an \(m\)-dimensional one-hot vector, its policy value can be written as follows:_

\[V(\tilde{\pi}) =\mathbb{E}\Big{[}\tilde{\theta}(X,\tilde{Z},\tilde{C};\tilde{f} _{0},\tilde{e}_{0})^{\top}\tilde{\pi}(X)\Big{]},\] \[\text{where }\tilde{f}_{0}(x) =(\mathbb{E}[C\mid Z=z_{1},X=x],\ldots,\mathbb{E}[C\mid Z=z_{m},X= x])^{\top}\] \[\tilde{e}_{0}(x) =(e_{0}(z_{1}\mid x,\ldots,e_{0}(z_{m}\mid x)))^{\top},\;e_{0}(z \mid x)=\mathbb{P}(Z=z_{j}\mid X=x),\]

_and the score function \(\tilde{\theta}\) can take three different forms:_

1. _(Direct Method)_ \(\tilde{\theta}_{\text{DM}}(x,\tilde{z},\tilde{c};\tilde{f},\tilde{e})=\tilde{ f}(x)\)_;_
2. _(Inverse Propensity Weighting)_ \(\tilde{\theta}_{\text{DW}}(x,\tilde{z},\tilde{c};\tilde{f},\tilde{e})=\frac{ \tilde{z}}{\tilde{c}(x)}\tilde{c}\)_;_
3. _(Doubly Robust)_ \(\theta_{\text{DR}}(x,\tilde{z},\tilde{c};\tilde{f},\tilde{e})=\tilde{f}(x)+ \frac{\tilde{z}}{\tilde{c}(x)}(\tilde{c}-\tilde{f}(x))\)_._

_In the three score functions above, all vector operations are entry-wise operations._From Lemma 1, the new formulations above have very similar structure as our previous formulation in Section 2 Proposition 1. The major differences are that we restrict to the discrete actions \(z_{1},\ldots,z_{m}\), redefine certain variables accordingly, and consider a simplex set as the constraint. We note that the identification in Lemma 1 mimics the DM, IPW and DR identification in offline contextual bandit learning (e.g., 1, 7, 43). Since the identification formulae are analogous to those in Section 2, we can easily apply the same policy learning methods in Section 2 and the SPO+ relaxation in Section 4.

## Appendix B Omitted Proofs

### Supporting Lemmas

**Lemma 2** (Talagrand's inequality, [31, 4, 37]).: _Let \(U_{i},i=1,\ldots,n\) be independent \(\mathcal{U}\)-valued random variables. Let \(\mathcal{H}\) be a countable family of measurable real-valued functions on \(\mathcal{U}\) such that \(\left\|h\right\|_{\infty}\leq v\) and \(\mathbb{E}[h(U_{1})]=\cdots=\mathbb{E}[h(U_{n})]=0\), for all \(h\in\mathcal{H}\). Define_

\[v_{n}=2v\mathbb{E}\left[\sup_{h\in\mathcal{H}}\biggl{|}\sum_{i=1}^{n}h(U_{i}) \biggr{|}\right]+\sum_{i=1}^{n}\sup_{h\in\mathcal{H}}\mathbb{E}\bigl{[}h^{2}( U_{i})\bigr{]}.\]

_Then, for all \(t\geq 0\),_

\[\mathbb{P}\Biggl{(}\sup_{h\in\mathcal{H}}\biggl{|}\sum_{i=1}^{n}h(U_{i}) \biggr{|}\geq\mathbb{E}\Biggl{[}\sup_{h\in\mathcal{H}}\biggl{|}\sum_{i=1}^{n}h( U_{i})\biggr{|}\Biggr{]}+t\Biggr{)}\leq\exp\biggl{(}\frac{-t^{2}}{2v_{n}+2tv/3} \biggr{)}.\]

**Lemma 3**.: _Fix functions \(f,\Sigma\) independent of \(\{(X_{i},Z_{i},C_{i})\}_{i=1}^{n}\) such that \(\left\|\theta(x,z,c;f,\Sigma)\right\|\leq\Theta\) for all \(x,z,c\). Define the function class_

\[\mathcal{G}=\biggl{\{}(x,z,c)\to\frac{\theta(x,z,c;f,\Sigma)^{ \top}(\pi(x)-\tilde{\pi}^{*}(x))\rho}{2B\Theta}:\pi\in\Pi_{\mathcal{F}},\rho \in[0,1]\biggr{\}}.\]

_Let \(\tilde{r}\) be any solution to the inequality \(\mathcal{R}_{n}(\mathcal{G},r)\leq r^{2}\) satisfying \(3n\tilde{r}^{2}/64\geq\log\log_{2}(1/\tilde{r})\). Then we have_

\[\mathbb{P}\biggl{(}\sup_{g\in\mathcal{G}}\frac{|(\mathbb{E}_{n}- \mathbb{E}_{P})g|}{\left\|g\right\|_{2}+\tilde{r}}\geq 6\tilde{r}\biggr{)}\leq 2 \exp\biggl{(}-\frac{3}{64}n\tilde{r}^{2}\biggr{)},\]

_where_

\[\mathbb{E}_{n}(g)=\frac{1}{n}\sum_{i=1}^{n}g(X_{i},Z_{i},C_{i}).\]

Proof of Lemma 3.: When \(\sup_{g\in\mathcal{G}}\lvert(\mathbb{E}_{n}-\mathbb{E}_{P})g\rvert/(\left\|g \right\|_{2}+\tilde{r})>6\tilde{r}\), one of the following two events must hold true:

\[\mathcal{E}_{1} =\bigl{\{}\lvert(\mathbb{E}_{n}-\mathbb{E}_{P})g\rvert\geq 6 \tilde{r}^{2}\text{ for some }g\in\mathcal{G}\text{ such that }\left\|g\right\|_{2}\leq\tilde{r}\bigr{\}},\] \[\mathcal{E}_{2} =\bigl{\{}\lvert(\mathbb{E}_{n}-\mathbb{E}_{P})g\rvert\geq 6 \lVert g\rVert_{2}\tilde{r}\text{ for some }g\in\mathcal{G}\text{ such that }\left\|g\right\|_{2}\geq\tilde{r}\bigr{\}}.\]

Define

\[\mathcal{Z}_{n}(r)=\sup_{g\in\mathcal{G},\left\|g\right\|_{2}\leq r}\lvert( \mathbb{E}_{n}-\mathbb{E}_{P})g\rvert.\]

Note that \(\left\|g\right\|_{2}\leq r\) implies \(\mathbb{E}[(g-\mathbb{E}_{P}(g))^{2}]\leq r^{2}\), and we also have \(\left\|g-\mathbb{E}_{P}(g)\right\|_{\infty}\leq 2\). By Talagrand's inequality (Lemma 2) over the function class \(\{g-\mathbb{E}_{P}(g):g\in\mathcal{G}\}\),

\[\mathbb{P}(\mathcal{Z}_{n}(r)\geq\mathbb{E}[\mathcal{Z}_{n}(r)]+t)\leq\exp \biggl{(}-\frac{nt^{2}}{8\mathbb{E}[\mathcal{Z}_{n}(r)]+2r^{2}+4t/3}\biggr{)}.\]

We now bound the expectation \(\mathbb{E}[\mathcal{Z}_{n}(r)]\). Since \(\mathcal{G}\) is star-shaped3, by [41, Lemma 13.6], \(r\to\mathcal{R}_{n}(\mathcal{G},r)/r\) is non-increasing. Thus, for any \(r\geq\tilde{r}\),

Footnote 3: A function class \(\mathcal{G}\) is star-shaped if for any \(g\in\mathcal{G}\) and \(\rho\in[0,1]\), we have \(\rho g\in\mathcal{G}\).

\[\mathbb{E}[\mathcal{Z}_{n}(r)]\leq 2\mathcal{R}_{n}(\mathcal{G},r)\leq\frac{2r}{ \tilde{r}}\mathcal{R}_{n}(\mathcal{G},\tilde{r})\leq 2r\tilde{r},\]where the first inequality comes from a symmetrization argument, the second inequality uses the fact that \(r\rightarrow\mathcal{R}_{n}(\mathcal{G},r)/r\) is non-increasing, and the third inequality is by definition of \(\tilde{r}\).

The Talagrand's then implies

\[\mathbb{P}(\mathcal{Z}_{n}(r)\geq 2r\tilde{r}+t)\leq\exp\biggl{(}-\frac{ nt^{2}}{16r\tilde{r}+2r^{2}+4t/3}\biggr{)}.\] (10)

We first bound \(\mathbb{P}(\mathcal{E}_{1})\). Taking \(r=\tilde{r}\) and \(t=4\tilde{r}^{2}\) in Eq. (10), we get

\[\mathbb{P}(\mathcal{E}_{1})\leq \mathbb{P}\bigl{(}\mathcal{Z}_{n}(\tilde{r})\geq 6\tilde{r}^{2} \bigr{)}\leq\exp\biggl{(}-\frac{24}{35}n\tilde{r}^{2}\biggr{)}.\]

We now bound \(\mathbb{P}(\mathcal{E}_{2})\). Note that

\[\mathbb{P}(\mathcal{E}_{2})\leq\mathbb{P}(\mathcal{Z}_{n}(\left\|g\right\|_{2 })\geq 6\|g\|_{2}\tilde{r}\text{ for some }g\in\mathcal{G},\left\|g\right\|_{2}\geq \tilde{r}).\]

Define

\[\mathcal{G}_{m}=\bigl{\{}g\in\mathcal{G}:2^{m-1}\tilde{r}\leq\left\|g\right\|_ {2}\leq 2^{m}\tilde{r}\bigr{\}}.\]

Since \(\left\|g\right\|_{2}\leq 1\), there exists \(M\leq\log_{2}(1/\tilde{r})\) such that

\[\mathcal{G}\cap\{g:\left\|g\right\|_{2}\geq\tilde{r}\}\subseteq\cup_{m=1}^{M} \mathcal{G}_{m}.\]

Therefore,

\[\mathbb{P}(\mathcal{E}_{2})\leq\sum_{m=1}^{M}\mathbb{P}(\mathcal{Z}_{n}( \left\|g\right\|_{2})\geq 6\|g\|_{2}\tilde{r}\text{ for some }g\in\mathcal{G}_{m}).\]

We now bound each term in the summation above. If there exists \(g\in\mathcal{G}_{m}\) such that \(\mathcal{Z}_{n}(\left\|g\right\|_{2})\geq 6\|g\|_{2}\tilde{r}\), then we have

\[\mathcal{Z}_{n}(2^{m}\tilde{r})\geq\mathcal{Z}_{n}(\left\|g\right\|_{2})\geq 6 \|g\|_{2}\tilde{r}\geq 3\cdot 2^{m}\tilde{r}^{2}.\]

Thus,

\[\mathbb{P}(\mathcal{Z}_{n}(\left\|g\right\|_{2})\geq 6\|g\|_{2}\tilde{r}\text{ for some }g\in\mathcal{G}_{m})\leq\mathbb{P}\bigl{(}\mathcal{Z}_{n}(2^{m}\tilde{r}) \geq 3\cdot 2^{m}\tilde{r}^{2}\bigr{)}.\]

Now, taking \(r=2^{m}\tilde{r}\) and \(t=2^{m}\tilde{r}^{2}\) in Eq. (10), we get

\[\mathbb{P}\bigl{(}\mathcal{Z}_{n}(2^{m}\tilde{r})\geq 3\cdot 2^{m}\tilde{r}^{2} \bigr{)}\leq\exp\biggl{(}-\frac{3n\tilde{r}^{2}}{13\cdot 2^{2-m}+6}\biggr{)} \leq\exp\biggl{(}-\frac{3}{32}n\tilde{r}^{2}\biggr{)}.\]

Therefore, if \(\frac{3}{64}n\tilde{r}^{2}\geq\log\log_{2}(1/\tilde{r})\), then

Combining the bounds on \(\mathbb{P}(\mathcal{E}_{1})\) and \(\mathbb{P}(\mathcal{E}_{2})\) leads to the final conclusion. 

**Lemma 4**.: _Suppose Assumption 3 holds and \(\mathbb{P}(|\mathcal{Z}^{*}(X)|>1)=0\). Then there exists a constant \(\tilde{C}(\alpha,\gamma)\) such that for any \(\pi\in\Pi_{\mathcal{F}}\),_

\[\mathbb{P}(\pi(X)\neq\pi_{f_{0}}(X))\leq\tilde{C}(\alpha,\gamma)\biggl{(} \frac{\text{Reg}(\pi)}{B}\biggr{)}^{\frac{\alpha}{1+\alpha}}.\]

Proof of Lemma 4.: This follows directly from [14, Lemma 1]. 

**Lemma 5**.: _Let \(c_{1},c_{2},r\) be positive constants. For any \(\alpha>0\), if a positive number \(x\) satisfies_

\[x\leq c_{1}x^{\frac{\alpha}{2(1+\alpha)}}r+c_{2},\]

_we have_

\[x\leq(c_{1}r)^{\frac{2\alpha+2}{\alpha+2}}+2c_{2}.\]Proof of Lemma 5.: First, note that

\[\frac{\partial}{\partial y}\Big{(}y-c_{1}y^{\frac{\alpha}{2(1+\alpha)}}r-c_{2} \Big{)}=1-\frac{c_{1}r\alpha}{2(1+\alpha)}y^{-\frac{2+\alpha}{2+2\alpha}}.\]

The derivative is strictly increasing in \(y\) and is eventually positive. Note the function \(y-c_{1}y^{\alpha/(2+2\alpha)}r-c_{2}\) takes a negative value at \(y=0\). Then as \(y\) increases, the value of \(y-c_{1}y^{\alpha/(2+2\alpha)}r-c_{2}\) first decreases and then increases. Therefore, if \(y>0\) satisfies the inequality \(y-c_{1}y^{\alpha/(2+2\alpha)}r-c_{2}\geq 0\), then such \(y\) also provides an upper bound on \(x\). Hence it is sufficient to show that \(y=(c_{1}r)^{\frac{2\alpha+2}{\alpha+2}}+2c_{2}\) satisfies the inequality, or equivalently,

\[(c_{1}r)^{\frac{2\alpha+2}{\alpha+2}}+c_{2}\geq c_{1}\Big{(}(c_{1}r)^{\frac{2 \alpha+2}{\alpha+2}}+2c_{2}\Big{)}^{\frac{\alpha}{2(1+\alpha)}}r.\] (11)

Suppose \(\alpha/(2+2\alpha)\) is a rational number. In this case, we can write \(\alpha/(2+2\alpha)=m_{1}/m_{2}\), where \(m_{1}\) and \(m_{2}\) are positive integers such that \(m_{2}\geq 2m_{1}+1\). Eq. (11) is then equivalent to

\[\Big{(}(c_{1}r)^{\frac{m_{2}}{m_{2}-m_{1}}}+c_{2}\Big{)}^{m_{2}}\geq c_{1}^{m _{2}}\Big{(}(c_{1}r)^{\frac{m_{2}}{m_{2}-m_{1}}}+2c_{2}\Big{)}^{m_{1}}r^{m_{2}}.\] (12)

Using the multinomial theorem, we have

\[\Big{(}(c_{1}r)^{\frac{m_{2}}{m_{2}-m_{1}}}+c_{2}\Big{)}^{m_{2}}- c_{1}^{m_{2}}\Big{(}(c_{1}r)^{\frac{m_{2}}{m_{2}-m_{1}}}+2c_{2}\Big{)}^{m_{1}}r^{m_{ 2}}\] \[> \sum_{i=0}^{m_{1}}\!\!\left(\frac{m_{2}!}{(i+m_{2}-m_{1})!(m_{1}-i )!}-\frac{m_{1}!^{2m_{1}-i}}{(m_{1}-i)!i!}\right)\!(c_{1}r)^{\frac{im_{2}}{m_{ 2}-m_{1}}+m_{2}}c_{2}^{m_{1}-i}.\]

Since \(m_{2}\geq 2m_{1}+1\), we have for any \(i=0,\ldots,m_{1}\),

\[\frac{m_{2}!}{(i+m_{2}-m_{1})!(m_{1}-i)!}\geq\frac{m_{1}!^{2m_{1}-i}}{(m_{1}- i)!i!}.\]

This can be proved by showing the ratio of LHS over RHS is larger than \(1\). Hence, Eq. (11) holds true when \(\alpha/(2+2\alpha)\) is a rational number.

Finally, note that

\[(c_{1}r)^{\frac{2\alpha+2}{\alpha+2}}+c_{2}-c_{1}\Big{(}(c_{1}r)^{\frac{2 \alpha+2}{\alpha+2}}+2c_{2}\Big{)}^{\frac{\alpha}{2(1+\alpha)}}r\]

is continuous in \(\alpha\). Since any real number \(\alpha\) can be viewed as the limit of a sequence of rational numbers and Eq. (11) holds for all rational numbers, it also holds true for all \(\alpha>0\). 

**Lemma 6**.: _Let \(c_{1},c_{2},r,y,z\) be positive constants, and let \(\alpha\) be a positive constant such that \(2(1+\alpha)/\alpha\) is an integer. If a positive number \(x\) satisfies_

\[x\leq c_{1}x^{\frac{\alpha}{2(1+\alpha)}}r+c_{1}y^{\frac{\alpha}{2(1+\alpha)} }r+c_{2}r^{2}+z+y,\]

_we have_

\[x\leq c_{1}^{\frac{2\alpha+2}{\alpha+2}}r^{\frac{2\alpha+2}{\alpha+2}}+\frac {2\alpha+2}{\alpha+2}c_{1}y^{\frac{\alpha}{2\alpha+2}}r+\frac{2\alpha+2}{ \alpha+2}c_{2}r^{2}+\frac{2\alpha+2}{\alpha+2}z+y.\]

Proof of Lemma 6.: Let \(2(\alpha+1)/\alpha=m\), where \(m\) is an integer by assumption. Using similar arguments as in the proof of Lemma 5, it is sufficient to show that for \(w=c_{1}^{\frac{2\alpha+2}{\alpha+2}}r^{\frac{2\alpha+2}{\alpha+2}}+\frac{2 \alpha+2}{\alpha+2}c_{1}y^{\frac{\alpha}{2\alpha+2}}r+\frac{2\alpha+2}{\alpha+ 2}c_{2}r^{2}+\frac{2\alpha+2}{\alpha+2}z+y\) and \(c_{2}^{\prime}=c_{1}y^{\frac{\alpha}{2(1+\alpha)}}r+c_{2}r^{2}+z+y\), we have

\[w-c_{1}w^{\frac{\alpha}{2(1+\alpha)}}r-c_{2}^{\prime}\geq 0.\]

This is equivalent to showing

\[c_{1}^{\frac{2\alpha+2}{\alpha+2}}r^{\frac{2\alpha+2}{\alpha+2}}+ \frac{2\alpha+2}{\alpha+2}c_{1}y^{\frac{\alpha}{2\alpha+2}}r+\frac{2\alpha+2}{ \alpha+2}c_{2}r^{2}+\frac{2\alpha+2}{\alpha+2}z+y\] \[\geq c_{1}\bigg{(}c_{1}^{\frac{2\alpha+2}{\alpha+2}}r^{\frac{2\alpha+2}{ \alpha+2}}+\frac{2\alpha+2}{\alpha+2}c_{1}y^{\frac{\alpha}{2\alpha+2}}r+ \frac{2\alpha+2}{\alpha+2}c_{2}r^{2}+\frac{2\alpha+2}{\alpha+2}z+y\bigg{)}^{ \frac{\alpha}{2(1+\alpha)}}r+c_{1}y^{\frac{\alpha}{2(1+\alpha)}}r+c_{2}r^{2}+z +y.\]Since \(\alpha=2/(m-2)\), the above inequality is equivalent to

\[\bigg{(}c_{1}^{\frac{m}{m-1}}r^{\frac{m}{m-1}}+\frac{1}{m-1}c_{1}y^{ \frac{1}{m}}r+\frac{1}{m-1}c_{2}r^{2}+\frac{1}{m-1}z\bigg{)}^{m}\] \[\geq c_{1}^{m}r^{m}\bigg{(}c_{1}^{\frac{m}{m-1}}r^{\frac{m}{m-1}}+ \frac{m}{m-1}c_{1}y^{\frac{1}{m}}r+\frac{m}{m-1}c_{2}r^{2}+\frac{m}{m-1}z+y \bigg{)}.\]

Using the multinomial theorem, it is easy to see that the expansion of LHS contains all terms on the RHS (plus additional positive terms). This finishes proving our conclusion. 

### Proof of Main Theorem

Proof of Theorem 1.: To simplify notation, we define

\[\mathbb{E}_{n_{j}}\Big{[}\theta(X,Z,C;\hat{f}_{3-j},\hat{\Sigma}_ {3-j})^{\top}(\hat{\pi}(X)-\tilde{\pi}^{*}(X))\Big{]}\] \[= \frac{2}{n}\sum_{i\in\mathcal{D}_{j}}\theta(X_{i},Z_{i},C_{i}; \hat{f}_{3-j},\hat{\Sigma}_{3-j})^{\top}(\hat{\pi}(X_{i})-\tilde{\pi}^{*}(X_{ i})).\]

We can decompose the regret as

\[\text{Reg}(\hat{\pi})= \mathbb{E}_{P}\big{[}f_{0}(X)^{\top}(\hat{\pi}(X)-\pi_{f_{0}}(X)) \big{]}\] \[= \mathbb{E}_{P}\big{[}\theta(X,Z,C;f_{0},\Sigma_{0})^{\top}(\hat{ \pi}(X)-\tilde{\pi}^{*}(X))\big{]}+\mathbb{E}\big{[}f_{0}(X)^{\top}(\tilde{\pi} ^{*}(X)-\pi_{f_{0}}(X))\big{]}\] \[\leq \frac{1}{2}\sum_{j=1}^{2}\mathbb{E}_{P}\bigg{[}\Big{(}\theta(X,Z, C;f_{0},\Sigma_{0})-\theta(X,Z,C;\hat{f}_{j},\hat{\Sigma}_{j})\Big{)}^{\top}( \hat{\pi}(X)-\tilde{\pi}^{*}(X))\bigg{]}\] \[+\text{Reg}(\hat{\pi}^{*}),\]

where the inequality follows from the definition of \(\hat{\pi}\).

By Assumption 2, with probability at least \(1-\delta/2\),

\[\frac{1}{2}\sum_{j=1}^{2}\mathbb{E}_{P}\bigg{[}\Big{(}\theta(X,Z,C;f_{0}, \Sigma_{0})-\theta(X,Z,C;\hat{f}_{j},\hat{\Sigma}_{j})\Big{)}^{\top}(\hat{\pi} (X)-\tilde{\pi}^{*}(X))\bigg{]}\leq\text{Rate}^{\textbf{N}}(n/2,\delta/4).\]

We now bound \((\mathbb{E}_{P}-\mathbb{E}_{n_{1}})\Big{[}\theta(X,Z,C;\hat{f}_{2},\hat{ \Sigma}_{2})^{\top}(\hat{\pi}(X)-\tilde{\pi}^{*}(X))\Big{]}\). By Lemma 3 and the assumption that \(2\exp\bigl{(}-3n\tilde{r}^{2}/128\bigr{)}\leq\delta/4\), we have that with probability at least \(1-\delta/4\),

\[\sup_{g\in\mathcal{G}_{2}}\frac{|(\mathbb{E}_{n_{1}}-\mathbb{E}_{P})g|}{\left\| g\right\|_{2}+\tilde{r}}\leq 6\tilde{r}.\] (13)

Assuming Eq. (13) holds,

\[(\mathbb{E}_{P}-\mathbb{E}_{n_{1}})\Big{[}\theta(X,Z,C;\hat{f}_{ 2},\hat{\Sigma}_{2})^{\top}(\hat{\pi}(X)-\tilde{\pi}^{*}(X))\Big{]}\] \[\leq 12B\Theta\Bigg{(}\Bigg{\|}\frac{\theta(X,Z,C;\hat{f}_{2},\hat{ \Sigma}_{2})^{\top}(\hat{\pi}(X)-\tilde{\pi}^{*}(X))}{2B\Theta}\Bigg{\|}_{2} \tilde{r}+\hat{r}^{2}\Bigg{)}\] \[\leq 12B\Theta\Bigg{(}\Bigg{\|}\frac{\theta(X,Z,C;\hat{f}_{2},\hat{ \Sigma}_{2})^{\top}(\hat{\pi}(X)-\pi_{f_{0}}(X))}{2B\Theta}\Bigg{\|}_{2} \tilde{r}+\Bigg{\|}\frac{\theta(X,Z,C;\hat{f}_{2},\hat{\Sigma}_{2})^{\top}( \pi_{f_{0}}(X)-\tilde{\pi}^{*}(X))}{2B\Theta}\Bigg{\|}_{2}\tilde{r}+\tilde{r} ^{2}\Bigg{)}.\] (14)For any \(\pi\in\Pi_{\mathcal{F}}\),

\[\left\|\frac{\theta(X,Z,C;\hat{f}_{2},\hat{\Sigma}_{2})^{\top}(\pi(X )-\pi_{f_{0}}(X))}{2B\Theta}\right\|_{2}^{2}\] \[= \mathbb{E}\Bigg{[}\Bigg{(}\frac{\theta(X,Z,C;\hat{f}_{2},\hat{ \Sigma}_{2})^{\top}(\pi(X)-\pi_{f_{0}}(X))}{2B\Theta}\Bigg{)}^{2}\mathbb{I}\{ \pi(X)\neq\pi_{f_{0}}(X)\}\Bigg{]}\] \[\leq \mathbb{P}(\pi(X)\neq\pi_{f_{0}}(X))\] \[\leq \tilde{C}(\alpha,\gamma)\bigg{(}\frac{\text{Reg}(\pi)}{B}\bigg{)} ^{\frac{\alpha}{1+\alpha}},\]

where the last inequality follows from Lemma 4.

Applying the inequality above for both \(\hat{\pi}\) and \(\pi_{f_{0}}\) and plug the bounds into Eq. (14), we get

\[(\mathbb{E}_{P}-\mathbb{E}_{n_{1}})\Big{[}\theta(X,Z,C;\hat{f}_{2 },\hat{\Sigma}_{2})^{\top}(\hat{\pi}(X)-\tilde{\pi}^{*}(X))\Big{]}\] \[\leq 12B\Theta\Bigg{(}\sqrt{\tilde{C}(\alpha,\gamma)}\bigg{(}\frac{ \text{Reg}(\hat{\pi})}{B}\bigg{)}^{\frac{\alpha}{2(1+\alpha)}}\tilde{r}+\sqrt{ \tilde{C}(\alpha,\gamma)}\bigg{(}\frac{\text{Reg}(\tilde{\pi}^{*})}{B}\bigg{)} ^{\frac{\alpha}{2(1+\alpha)}}\tilde{r}+\tilde{r}^{2}\Bigg{)}.\]

We can similarly bound \((\mathbb{E}_{P}-\mathbb{E}_{n_{2}})\Big{[}\theta(X,Z,C;\hat{f}_{1},\hat{ \Sigma}_{1})^{\top}(\hat{\pi}(X)-\tilde{\pi}^{*}(X))\Big{]}\).

Combining all pieces together, we get that with probability at least \(1-\delta\),

\[\frac{\text{Reg}(\hat{\pi})}{B}\leq 12\Theta\Bigg{(}\sqrt{\tilde{C}(\alpha,\gamma)}\bigg{(}\frac{ \text{Reg}(\hat{\pi})}{B}\bigg{)}^{\frac{\alpha}{2(1+\alpha)}}\tilde{r}+\sqrt{ \tilde{C}(\alpha,\gamma)}\bigg{(}\frac{\text{Reg}(\tilde{\pi}^{*})}{B}\bigg{)} ^{\frac{\alpha}{2(1+\alpha)}}\tilde{r}+\tilde{r}^{2}\Bigg{)}\] \[+\frac{\text{Rate}^{\textbf{N}}(n/2,\delta/4)}{B}+\frac{\text{Reg }(\tilde{\pi}^{*})}{B}.\]

Solving the above inequality with respect to \(\text{Reg}(\hat{\pi})/B\) using Lemma 5, we have

\[\text{Reg}(\hat{\pi})\leq B\bigg{(}12\Theta\sqrt{\tilde{C}(\alpha,\gamma)}\tilde{r}\bigg{)}^{ \frac{2\alpha+2}{\alpha+2}}+24B\Theta\Bigg{(}\sqrt{\tilde{C}(\alpha,\gamma)} \bigg{(}\frac{\text{Reg}(\tilde{\pi}^{*})}{B}\bigg{)}^{\frac{\alpha}{2(1+ \alpha)}}\tilde{r}+\tilde{r}^{2}\Bigg{)}\] \[+2\text{Rate}^{\textbf{N}}(n/2,\delta/4)+2\text{Reg}(\tilde{\pi} ^{*}).\]

### Proofs of Propositions

Proof of Proposition 1.: For direct method, the conclusion is obvious.

For ISW, we have for any \(\Sigma\),

\[\mathbb{E}\Big{[}\big{(}\Sigma^{+}(X)ZC\big{)}^{\top}\pi(X)\Big{]}-\mathbb{E }\big{[}f_{0}(X)^{\top}\pi(X)\big{]}=\mathbb{E}\big{[}f_{0}(X)^{\top}(I-\Sigma ^{+}(X)\Sigma_{0}(X))^{\top}\pi(X)\big{]}.\]

Let \(M(x)\) be a matrix whose columns include all basis vectors of the span of \(\mathcal{Z}\). Then \(\pi(X)\in\text{Range}(M(x))\). According to the coverage assumption, the column space of \(\Sigma_{0}(x)\) is identical to the column space of \(M(x)\). By the property of pseudo-inverse, the column space of \((I-\Sigma_{0}^{\dagger}\Sigma_{0})\) is orthogonal to the column space of \(M\). Therefore, \((I-\Sigma^{+}(X)\Sigma_{0}(X))^{\top}\pi(X)=0\) for any \(\pi\in\mathcal{Z}\).

For doubly robust score, we have that for any function \(f,\Sigma\),

\[\mathbb{E}\big{[}\pi(X)^{\top}\big{(}f(X)+\Sigma(X)^{+}Z(C-Z^{ \top}f(X))\big{)}\big{]}-\mathbb{E}[\pi(X)^{\top}f_{0}(X)]\] \[= \mathbb{E}\big{[}\pi(X)^{\top}\big{(}(I-\Sigma^{+}(X)\Sigma_{0}(X ))(f(X)-f_{0}(X))\big{)}\big{]}.\]

Taking either \(f=f_{0}\) or \(\Sigma=\Sigma_{0}\) gives \(0\)Proof of Proposition 2.: Because \(\pi(x)-\tilde{\pi}^{*}\in\mathrm{span}(\mathcal{Z})\), for the DM score we have

\[\mathbb{E}_{P}\bigg{[}\Big{(}\theta_{DM}(X,Z,C;f_{0},\Sigma_{0})- \theta_{DM}(X,Z,C;\hat{f},\hat{\Sigma})\Big{)}^{\top}(\pi(X)-\tilde{\pi}^{*}(X) )\bigg{]}\] \[\leq \mathbb{E}_{P}\bigg{[}\mathrm{Proj}_{\mathrm{span}(\mathcal{Z})} \Big{(}\theta_{DM}(X,Z,C;f_{0},\Sigma_{0})-\theta_{DM}(X,Z,C;\hat{f},\hat{ \Sigma})\Big{)}^{\top}(\pi(X)-\tilde{\pi}^{*}(X))\bigg{]}\] \[\leq 2B\Big{\{}\mathbb{E}_{X}[\|\mathrm{Proj}_{\mathrm{span}( \mathcal{Z})}(\hat{f}(X)-f_{0}(X))\|^{2}]\Big{\}}^{1/2}=O(\chi_{n,\delta}).\]

For the ISW score, we have

\[\mathbb{E}_{P}\bigg{[}\Big{(}\theta_{ISW}(X,Z,C;f_{0},\Sigma_{0}) -\theta_{ISW}(X,Z,C;\hat{f},\hat{\Sigma})\Big{)}^{\top}(\pi(X)-\tilde{\pi}^{*} (X))\bigg{]}\] \[\leq 2B\Big{\{}\mathbb{E}_{X}[\|(\hat{\Sigma}^{\dagger}(X)-\Sigma_{0 }^{\dagger}(X))\Sigma_{0}(X)\|_{\mathrm{Proj}}^{2}]\Big{\}}^{1/2}=O(\chi_{n, \delta}).\]

For the doubly robust score, we can easily get

\[\mathbb{E}_{P}\bigg{[}\Big{(}\theta_{DR}(X,Z,C;f_{0},\Sigma_{0}) -\theta_{DR}(X,Z,C;\hat{f},\hat{\Sigma})\Big{)}^{\top}(\pi(X)-\tilde{\pi}^{*}( X))\bigg{]}\] \[= \mathbb{E}_{P}\Big{[}(\pi(X)-\tilde{\pi}^{*}(X))^{\top}\Big{(} \Sigma_{0}^{+}(X)-\hat{\Sigma}^{+}(X)\Big{)}\Sigma_{0}(X)(\hat{f}(X)-f_{0}(X) )\Big{]}\] \[= \mathbb{E}_{P}\Big{[}(\pi(X)-\tilde{\pi}^{*}(X))^{\top}\Big{(} \Sigma_{0}^{+}(X)-\hat{\Sigma}^{+}(X)\Big{)}\Sigma_{0}(X)\operatorname{Proj}_{ \mathrm{Span}(\mathcal{Z})}(\hat{f}(X)-f_{0}(X))\Big{]}\] \[\lesssim\]

Here the second equation holds because \(\pi(x)-\tilde{\pi}^{*}(x)\) belongs to the linear span of \(\mathcal{Z}\), but \(I-\Sigma_{0}^{+}(x)\Sigma_{0}\) is orthogonal to the linear span of \(\mathcal{Z}\), as we already argued in the proof of Proposition 1. The third equation holds because the column space of \(\Sigma_{0}(X)\) is identical to \(\mathrm{span}(\mathcal{Z})\) according to the coverage assumption. 

Proof of Proposition 3.: Define

\[\Psi(t)=\frac{1}{5}\exp(t^{2}).\]

Note that whenever \(\mathbb{E}\Psi(|W|/w)\leq 1\) for some random variable \(W\), we have by Markov's inequality that

\[\mathbb{P}(|W|>t)\leq 5\exp(-t^{2}/w^{2}),\] \[\mathbb{E}|W|=\int_{0}^{\infty}\mathbb{P}(|W|>t)dt\leq 5w.\] (15)

Throughout the proof, we condition on the event that \(\mathcal{G}_{1}\) has VC-subgraph dimension \(\eta\). We finish the proof in three steps.

Step I: Critical radius for empirical Rademacher complexity.Define the localized empirical Rademacher complexity

\[\hat{\mathcal{R}}_{n}(\mathcal{G}_{1},r)=\mathbb{E}_{\epsilon}\Bigg{[}\sup_{g \in\mathcal{G},\|g\|_{n}\leq r}\Bigg{|}\frac{1}{n}\sum_{i=1}^{n}\epsilon_{i}g (X_{i},Z_{i},C_{i})\Bigg{|}\Bigg{]},\]

where \(\epsilon_{1},\ldots,\epsilon_{n}\) are i.i.d. Rademacher random variables, and \(\big{\|}g\big{\|}_{n}=\sqrt{\sum_{i=1}^{n}g^{2}(X_{i},Z_{i},C_{i})/n}\). Let \(\hat{r}_{n}^{*}\) be the smallest positive solution to \(\hat{\mathcal{R}}_{n}(\mathcal{G}_{1},r)\leq r^{2}/32\). In what follows, we show that there exists a universal constant \(C\) such that

\[\mathbb{P}\Bigg{(}\hat{r}_{n}^{*}\leq\tilde{C}\sqrt{\frac{\eta\log(n+1)}{n}} \Bigg{)}=1.\] (16)For any \(g\in\mathcal{G}_{1}\), define set

\[\mathbf{G}=\{(g(X_{1},Z_{1},C_{1}),\ldots,g(X_{n},Z_{n},C_{n})):g\in\mathcal{G}_{1 },\left\|g\right\|_{n}\leq r\}.\]

Let \(D(t,\mathbf{G})\) be the \(t\)-packing number of \(\mathbf{G}\) and \(N(t,\mathbf{G})\) be the \(t\)-covering number. Note that \(\left\|\mathbf{g}\right\|\leq\sqrt{n}r\) for all \(\mathbf{g}\in\mathbf{G}\). By [28, Theorem 3.5],

\[\mathbb{E}_{\epsilon}\Psi\Bigg{(}\frac{1}{J}\sup_{g\in\mathcal{G}_{1},\left\| g\right\|_{n}\leq r}\left|\sum_{i=1}^{n}\epsilon_{i}g(X_{i},Z_{i},C_{i}) \right|\Bigg{)}\leq 1,\]

where

\[J=9\int_{0}^{\sqrt{n}r}\sqrt{\log D(t,\mathbf{G})}dt.\]

So by Eq. (15),

\[\hat{\mathcal{R}}_{n}(\mathcal{G}_{1},r)\leq\frac{5}{n}J.\]

Consider the function class

\[\mathcal{G}_{1}^{\prime}=\{g:g\in\mathcal{G}_{1},\left\|g\right\|_{n}\leq r\}.\]

Note that \(\sqrt{n}r\) is the envelope of \(\mathcal{G}_{1}^{\prime}\) on \((X_{1},Z_{1},C_{1}),\ldots,(X_{n},Z_{n},C_{n})\). Applying [40, Theorem 2.6.7] gives

\[D(\sqrt{n}rt,\mathbf{G}) \leq N(\sqrt{n}rt/2,\mathbf{G})\] \[\leq\tilde{C}(\eta+1)(16e)^{\eta+1}\bigg{(}\frac{4n}{t^{2}}\bigg{)} ^{\eta}\]

for a universal constant \(\tilde{C}\). Thus,

\[J= 9\sqrt{n}r\int_{0}^{1}\sqrt{\log D(\sqrt{n}rt,\mathbf{G})}dt\] \[\leq 9\sqrt{n}r\int_{0}^{1}\sqrt{\log C+\log(\eta+1)+(\eta+1)\log(16e )+\eta\log n+\eta\log 4-2\eta\log t}dt\] \[\leq 9\int_{0}^{1}\sqrt{2\log C+15-3\log t}dt\sqrt{\eta\log(n+1)n}r,\]

where \(\int_{0}^{1}\sqrt{2\log C+15-3\log t}dt<\infty\). We then obtain that for a (different) universal constant \(\tilde{C}\),

\[\hat{\mathcal{R}}_{n}(\mathcal{G}_{1},r)\leq\frac{\tilde{C}}{32}\sqrt{\frac{ \eta\log(n+1)}{n}}r.\]

Therefore, for any samples \(\left(X_{i},Z_{i},C_{i}\right)_{i=1}^{n}\), any \(\hat{r}_{n}\geq C\sqrt{\eta\log(n+1)/n}\) is a valid solution to \(\hat{\mathcal{R}}_{n}(\mathcal{G}_{1},r)\leq r^{2}/32\), which implies Eq. (16).

Step II: Critical radius for Rademacher complexity.Let \(r_{n}^{*}\) be the smallest positive solution to the inequality \(\mathcal{R}_{n}(\mathcal{G}_{1},r)\leq r^{2}/32\). We now bound \(r_{n}^{*}\).

For any \(t>0\), define the random variable

\[W_{n}(t)=\mathbb{E}_{\epsilon}\Bigg{[}\sup_{g\in\mathcal{G}_{1},\left\|g \right\|_{2}\leq t}\left|\frac{1}{n}\sum_{i=1}^{n}\epsilon_{i}g(X_{i},Z_{i},C_{ i})\right|\Bigg{]},\]

so that \(\mathcal{R}_{n}(\mathcal{G}_{1},r)=\mathbb{E}_{P}[W_{n}(r)]\) by construction. Define the events

\[\mathcal{E}_{3}(t)=\bigg{\{}\left|W_{n}(t)-\mathcal{R}_{n}( \mathcal{G}_{1},t)\right|\leq\frac{r_{n}^{*}t}{112}\bigg{\}},\] \[\mathcal{E}_{4}=\Bigg{\{}\sup_{g\in\mathcal{G}_{1}}\frac{\left\| g\right\|_{n}^{2}-\left\|g\right\|_{2}^{2}}{\left\|g\right\|_{2}^{2}+(r_{n}^{*})^{2}} \leq\frac{1}{2}\Bigg{\}}.\]Following the proof of [14, Lemma EC.12],

\[\mathbb{P}\bigg{(}\frac{r_{n}^{*}}{5}\leq\hat{r}_{n}^{*}\leq 3r_{n}^{*}\bigg{)} \geq\mathbb{P}(2\mathcal{E}_{3}(r_{n}^{*})\cap\mathcal{E}_{3}(7r_{n}^{*})\cap \mathcal{E}_{4}).\]

[14, Lemma EC.10] implies that

\[\mathbb{P}(\mathcal{E}_{4}^{c})\leq 2e^{-\tilde{c}_{1}n(r_{n}^{*})^{2}}\]

for some universal constant \(\tilde{c}_{1}>0\). Moreover, for any \(\zeta\geq 1\), we have \(\mathcal{R}_{n}(\mathcal{G}_{1},\zeta r_{n}^{*})\geq\mathcal{R}_{n}(r_{n}^{*} )\geq(r_{n}^{*})^{2}/32\). By [3, Theorem 16],

\[\mathbb{P}(\mathcal{E}_{3}^{c}(\zeta r_{n}^{*}))\leq 2e^{-\tilde{c}_{2}n(r_{n}^ {*})^{2}}\]

for some universal constant \(\tilde{c}_{2}>0\). Combining all pieces we have

\[\mathbb{P}\bigg{(}\frac{r_{n}^{*}}{5}\leq\hat{r}_{n}^{*}\leq 3r_{n}^{*} \bigg{)}\geq 1-6e^{-(\tilde{c}_{1}\wedge\tilde{c}_{2})n(r_{n}^{*})^{2}}.\] (17)

By step I in the proof, \(\mathbb{P}\Big{(}\hat{r}_{n}^{*}\leq\tilde{C}_{0}\sqrt{\eta\log(n+1)/n}\Big{)}=1\) for some constant \(\tilde{C}_{0}\). Let \(\tilde{C}>5\tilde{C}_{0}\) be a constant such that \(2^{-\tilde{C}(\tilde{c}_{1}\wedge\tilde{c}_{2})}<1/6\). If \(r_{n}^{*}>C\sqrt{\eta\log(n+1)/n}\), by Eq. (17) we have \(\mathbb{P}\Big{(}\hat{r}_{n}^{*}>\tilde{C}_{0}\sqrt{\eta\log(n+1)/n}\Big{)}>0\), which leads to contradiction. Thus,

\[r_{n}^{*}\leq\tilde{C}\sqrt{\eta\log(n+1)/n}.\]

Finally, any \(r\geq\tilde{C}\sqrt{\eta\log(n+1)/n}\) solves the inequality \(\mathcal{R}_{n}(\mathcal{G}_{1},r)\leq r^{2}\).

Step III: Checking other conditions.The other two inequalities, \(3n\tilde{r}^{2}/64\geq\log\log_{2}(1/\tilde{r})\) and \(2\exp\bigl{(}-3n\tilde{r}^{2}/64\bigr{)}\leq\delta/2\), are easily satisfied as long as we take \(\tilde{C}\) big enough. 

### Proof for Full Feedback Setting

Proof of Theorem 1.: To simplify notation, we write

\[\mathbb{E}_{n}\big{[}Y^{\top}\big{(}\tilde{\pi}^{\mathrm{F}}(X)-\tilde{\pi} ^{*}(X)\big{)}\big{]}=\frac{1}{n}\sum_{i=1}^{n}Y_{i}^{\top}\big{(}\tilde{\pi} ^{\mathrm{F}}(X_{i})-\tilde{\pi}^{*}(X_{i})\big{)}.\]

We can decompose the regret as

\[\text{Reg}\big{(}\hat{\pi}^{\mathrm{F}}\big{)}= \mathbb{E}_{P}\big{[}Y^{\top}\big{(}\tilde{\pi}^{\mathrm{F}}(X)- \pi_{f_{0}}(X)\big{)}\big{]}\] \[= \mathbb{E}_{P}\big{[}Y^{\top}\big{(}\tilde{\pi}^{\mathrm{F}}(X)- \tilde{\pi}^{*}(X)\big{)}\big{]}+\mathbb{E}\big{[}Y^{\top}(\tilde{\pi}^{*}(X) -\pi_{f_{0}}(X))\big{]}\] \[\leq (\mathbb{E}_{P}-\mathbb{E}_{n})\big{[}Y^{\top}\big{(}\tilde{\pi} ^{\mathrm{F}}(X)-\tilde{\pi}^{*}(X)\big{)}\big{]}+\text{Reg}(\tilde{\pi}^{*}),\]

where the inequality follows from the definition of \(\hat{\pi}^{\mathrm{F}}\).

We now bound \((\mathbb{E}_{P}-\mathbb{E}_{n})\big{[}Y^{\top}\big{(}\tilde{\pi}^{\mathrm{F}} (X)-\tilde{\pi}^{*}(X)\big{)}\big{]}\). Following a similar proof as in the proof of Lemma 3, we have that with probability at least \(1-\delta\),

\[\sup_{g\in\tilde{\mathcal{G}}}\frac{|(\mathbb{E}_{n}-\mathbb{E}_{P})g|}{\left\| g\right\|_{2}+\bar{r}^{\mathrm{F}}}\leq 6\bar{r}^{\mathrm{F}}.\] (18)

Assuming Eq. (18) holds,

\[(\mathbb{E}_{P}-\mathbb{E}_{n})\big{[}Y^{\top}\big{(}\tilde{\pi} ^{\mathrm{F}}(X)-\tilde{\pi}^{*}(X)\big{)}\big{]}\] \[\leq 12B\Bigg{(}\Bigg{\|}\frac{Y^{\top}\big{(}\tilde{\pi}^{\mathrm{F} }(X)-\tilde{\pi}^{*}(X)\big{)}}{2B}\Bigg{\|}_{2}\,\bar{r}^{\mathrm{F}}+(\bar{r }^{\mathrm{F}})^{2}\Bigg{)}\] \[\leq 12B\Bigg{(}\Bigg{\|}\frac{Y^{\top}\big{(}\tilde{\pi}^{\mathrm{F} }(X)-\pi_{f_{0}}(X)\big{)}}{2B}\Bigg{\|}_{2}\,\bar{r}^{\mathrm{F}}+\bigg{\|} \frac{Y^{\top}(\pi_{f_{0}}(X)-\tilde{\pi}^{*}(X))}{2B}\Bigg{\|}_{2}\,\bar{r}^{ \mathrm{F}}+(\bar{r}^{\mathrm{F}})^{2}\Bigg{)}.\]For any \(\pi\in\Pi_{F}\),

\[\left\|\frac{Y^{\top}(\pi(X)-\pi_{f_{0}}(X))}{2B}\right\|_{2}^{2}= \mathbb{E}\Bigg{[}\bigg{(}\frac{Y^{\top}(\pi(X)-\pi_{f_{0}}(X))}{2B} \bigg{)}^{2}\mathbb{I}\{\pi(X)\neq\pi_{f_{0}}(X)\}\Bigg{]}\] \[\leq \mathbb{P}(\pi(X)\neq\pi_{f_{0}}(X))\] \[\leq \tilde{C}(\alpha,\gamma)\bigg{(}\frac{\text{Reg}(\pi)}{B}\bigg{)} ^{\frac{\alpha}{1+\alpha}},\]

where the last inequality follows from Lemma 4. Thus,

\[(\mathbb{E}_{P}-\mathbb{E}_{n})\big{[}Y^{\top}\big{(}\tilde{\pi}^ {\text{F}}(X)-\tilde{\pi}^{*}(X)\big{)}\big{]}\] \[\leq\]

Combining all pieces together, we get that with probability at least \(1-\delta\),

\[\frac{\text{Reg}(\hat{\pi}^{\text{F}})}{B}\leq\] \[\quad+\frac{\text{Reg}(\tilde{\pi}^{*})}{B}.\]

Solving the above inequality with respect to \(\text{Reg}(\hat{\pi}^{\text{F}})/B\) using Lemma 5, we have

\[\text{Reg}(\hat{\pi}^{\text{F}})\leq B\bigg{(}12\sqrt{\tilde{C}(\alpha,\gamma)}\tilde{r}^{\text{F}} \bigg{)}^{\frac{2\alpha+2}{\alpha+2}}+24B\bigg{(}\sqrt{\tilde{C}(\alpha,\gamma )}\bigg{(}\frac{\text{Reg}(\tilde{\pi}^{*})}{B}\bigg{)}^{\frac{\alpha}{2(1+ \alpha)}}\tilde{r}^{\text{F}}+(\tilde{r}^{\text{F}})^{2}\bigg{)}+2\text{Reg}( \tilde{\pi}^{*}).\]

## Appendix C Additional Experimental Details

In Section 5, we provide experimental results for different methods under various model specifications and different logging policies. In this section, we further explain the details of experiment setup, implementation, and provide additional experimental results. All experiments in the paper are implemented on a cloud computing platform with 128 CPUs of model Intel(R) Xeon(R) Platinum 8369B CPU @ 2.70GHz, 250GB RAM and 500GB storage. The experiment for each specification of \(\mathcal{F}\) and \(\mathcal{F}^{\text{N}}\) and each logging policy takes around 2 days of running time based on parallel computing on 100 CPUs.

### Experimental Setup and Implementation Details

Data generating process.We first generate i.i.d draws of the covariates \(X=(X_{1},X_{2},X_{3})^{\top}\in\mathbb{R}^{3}\) from independent standard normal distribution. Then we simulate the full feedback \(Y\) according to the equation \(Y=f_{0}(X)+\epsilon\), where \(f_{0}(X)=3+W_{1}^{*}X_{1}+W_{2}^{*}X_{2}+W_{3}^{*}X_{3}+W_{4}^{*}X_{1}X_{2}+W_ {5}^{*}X_{2}X_{3}+W_{6}^{*}X_{1}X_{3}+W_{7}^{*}X_{1}X_{2}X_{3}\) for coefficient vectors \(W_{1}^{*},\dots,W_{7}^{*}\in\mathbb{R}^{d}\) and a random noise \(\epsilon\) drawn from the \(\text{Unif}[-0.5,0.5]\). To fix the coefficient vectors \(W_{1}^{*},\dots,W_{7}^{*}\), we draw their entries independently the from the \(\text{Unif}[0,1]\) distribution once, and then use the resulting fixed coefficient vectors throughout the experiment. We sample observed decisions \(Z\) from the set of all feasible path decisions \(\{z_{1},\dots,z_{m}\}\) for \(m=70\), according to different logging policies that will be described shortly. Then the total cost \(C=Y^{\top}Z\) is recorded in the observed data.

We consider three different logging policies. One is a random logging policy that uniformly samples each decision from the feasible decisions, regardless of the covariate value. The other two are covariate-dependent logging policies. For these two policies, we first remove \(20\) feasible decisions that correspond to the optimal decisions for some covariate observations in the testing data, and then randomly sample the observed decisions from the rest of \(50\) feasible decisions according to different rules. This means that many promising decisions are not explored by the logging policies at all. We hope to use these logging policies to demonstrate the value of leveraging the linear structure of the decision-making problem, since exploiting the linear structure allows to extrapolate the feedback from the logged decisions to decisions never explored by the policies. Specifically, we further divide the remaining \(50\) feasible decisions into two even groups. Then the two different logging policies sample decisions from the two groups according to different rules depending on the covariate value.

* One covariate-dependent logging policy samples the decisions according to the sign of the first covariate \(X_{1}\). When \(X_{1}>0\), the logging policy chooses the first group with probability \(2/3\) and the second group with probability \(1/3\). When \(X_{1}\leq 0\), the logging policy chooses the first group with probability \(1/3\) and the second group with probability \(2/3\). Once deciding the group, the policy then uniformly samples one decision from the chosen group. For concreteness, we will refer to this policy as the \(X_{1}\)-policy.
* The other covariate-dependent policy samples the decisions according to the signs of both \(X_{1}\) and \(X_{2}\). When \(X_{1}>0\) and \(X_{2}>0\), the logging policy chooses the two groups with probabilities \(2/3\) and \(1/3\) respectively. When \(X_{1}>0\) and \(X_{2}\leq 0\), the logging policy chooses the two groups with probabilities \(1/3\) and \(2/3\) respectively. When \(X_{1}\leq 0\) and \(X_{2}>0\), the policy chooses the two groups with probabilities \(3/4\) and \(1/4\) respectively. When \(X_{1}\leq 0\) and \(X_{2}\leq 0\), the policy chooses the two groups with probabilities \(1/4\) and \(3/4\) respectively. Once deciding the group, the policy again uniformly samples one decision from the chosen group. We will refer to this policy as the \(X_{1}X_{2}\)-policy.

Specification of the policy-inducing model and nuisance model.For the policy-inducing model and nuisance model, we consider three different classes. One is the correctly specified class \(\{(x_{1},x_{3},x_{3})\mapsto W_{0}+W_{1}x_{1}+W_{2}x_{2}+W_{3}x_{3}+W_{4}x_{1}x _{2}+W_{5}x_{2}x_{3}+W_{6}x_{1}x_{3}+W_{7}x_{1}x_{2}x_{3}:W_{0},\ldots,W_{7} \in\mathbb{R}\}\). The second model class \(\{(x_{1},x_{3},x_{3})\mapsto W_{0}+W_{1}x_{1}+W_{2}x_{2}+W_{3}x_{3}+W_{4}x_{1}x _{2}+W_{5}x_{2}x_{3}:W_{0},\ldots,W_{5}\in\mathbb{R}\}\) omits two interaction terms and is thus misspecified (which we refer to as degree-2 misspecification). The third model class \(\{(x_{1},x_{3},x_{3})\mapsto W_{0}+W_{1}x_{1}+W_{2}x_{2}+W_{3}x_{3}:W_{1},W_{2},W_{3}\in\mathbb{R}\}\) omits all four interaction terms (which we refer to as degree-4 misspecification).

Nuisance estimation for the SPO+ methods.The SPO+ methods involve two different nuisances. One is the function \(f_{0}(x)=\mathbb{E}[Y\mid X=x]\). We estimate this by the least squares regression in Eq. (7), with the \(\mathcal{F}^{\text{N}}\) class being one of the three classes described above (i.e., correct specification, degree-2 misspecification, degree-4 misspecification). We incorporate an additional ridge penalty with a coefficient \(1\). We estimate the the nuisance \(\Sigma_{0}(x)\) using the propensity score approach described in Remark 2, so we need to estimate the propensity scores \(\mathbb{P}(Z=z_{j}\mid X=x)\) for \(j=1,\ldots,m\). For the random logging policy, we simply estimate \(\mathbb{P}(Z=z_{j}\mid X=x)\) for any \(x\) by the empirical frequency of the decision \(z_{j}\) in the observed data. For the \(X_{1}\)-policy and \(X_{1}X_{2}\)-policy, we estimate the propensity scores by classification decision trees of depth \(3\) trained to classify each instance to one of the observed classes among \(z_{1},\ldots,z_{m}\). These nuisances are all estimated using the two-fold cross-fitting described in Section 1.2.

Since the \(\Sigma_{0}\) matrix is rank-deficient, we cannot directly invert it. Besides taking the pseudo-inverse, we also implement the Lambda regularization and clipping techniques described in Section 5. For Lambda regularization, we set the regularization parameter \(\lambda\) to \(1\). For the clipping technique, we consider the eigen-decomposition of \(\Sigma_{0}\), set all eigenvalues below \(1\) to \(1\), and then take a pseudo-inverse of the transformed matrix. These two lead to the SPO+ DR Lambda and SPO+ DR Clip methods in Section 5 and this section.

Naive Benchmarks.We also implemented the benchmarks in Appendix A. For ETO, SPO+ DM, and SPO+ DR, we estimate \(\tilde{f}_{0}(x)\) by regressing \(C\) with respect to \(X\) using data for each observed decision respectively. The regression function class uses similar correctly specified class, degree-2 misspecified class, degree-4 misspecified class mentioned above, with only slight difference in the dimension since the output of \(\tilde{f}_{0}\) is \(m\)-dimensional while the output of \(f_{0}\) is \(d\)-dimensional. For SPO+ DR and SPO+ IPW, we need to estimate the propensity scores. These are again estimated by either sample frequency or decision trees. Note that some feasible decisions are never observed in the training data. For these decisions, the corresponding component of \(\tilde{f}_{0}\) is heuristically imputed by a pooled regression of \(C\) against \(X\) using all observed data. For SPO+ IPW and SPO+ DR, although the propensity scores for the unseen decisions are zero, they do not impact the policy evaluation since they only need the propensity score for decisions observed in the data.

[MISSING_PAGE_FAIL:24]

\begin{table}
\begin{tabular}{c|c|c c c|c c c} \hline \hline  & \multirow{2}{*}{Methods} & \multicolumn{4}{c|}{Training Data \(n\)} & \multicolumn{4}{c|}{Training Data \(n\)} \\  & & 400 & 1000 & 1600 & 400 & 1000 & 1600 \\ \hline  & ETO & \multicolumn{4}{c|}{\(\mathcal{F}\) misspecified degree 2} & \multicolumn{4}{c|}{\(\mathcal{F}\) misspecified degree 4} \\ \cline{3-8}  & & 11.04\% & 9.14\% & 8.34\% & 12.35\% & 11.42\% & 10.39\% \\ \hline  & \multicolumn{4}{c|}{\(\mathcal{F}\) misspecified degree 2} & \multicolumn{4}{c|}{\(\mathcal{F}\) misspecified degree 4} \\ \cline{2-8}  & SPO+ DM & 2.81\% & 0.80\% & 0.54\% & 4.06\% & 2.21\% & 2.06\% \\ Well-specified & SPO+ DR PI & 3.27\% & 1.36\% & 1.05\% & 4.83\% & 2.95\% & 2.71\% \\ Nuisance Model & SPO+ DR Lambda & 2.83\% & 0.97\% & 0.73\% & 4.33\% & 2.45\% & 2.25\% \\ \(\mathcal{F}^{\text{N}}\) & SPO+ DR Clip & 3.05\% & 1.09\% & 0.84\% & 4.59\% & 2.62\% & 2.38\% \\ Naive SPO+ DM & 14.97\% & 12.78\% & 5.68\% & 15.27\% & 13.20\% & 6.42\% \\ Naive SPO+ DR & 15.00\% & 13.03\% & 6.31\% & 15.27\% & 13.48\% & 7.26\% \\ \hline  & \multicolumn{4}{c|}{\(\mathcal{F}^{\text{N}}\) misspecified degree 2} & \multicolumn{4}{c|}{\(\mathcal{F}^{\text{N}}\) misspecified degree 4} \\ \cline{2-8}  & SPO+ DM & 10.01\% & 8.37\% & 7.47\% & 12.51\% & 11.22\% & 9.68\% \\ Well-specified & SPO+ DR PI & 9.11\% & 7.02\% & 6.44\% & 11.69\% & 10.19\% & 9.02\% \\ Policy-inducing & SPO+ DR Lambda & 9.05\% & 7.52\% & 6.68\% & 12.31\% & 10.38\% & 8.96\% \\ Model \(\mathcal{F}\) & SPO+ DR Clip & 9.02\% & 7.28\% & 6.36\% & 11.87\% & 10.04\% & 8.70\% \\ Naive SPO+ DM & 15.56\% & 14.23\% & 12.96\% & 15.22\% & 14.51\% & 13.85\% \\ Naive SPO+ DR & 15.64\% & 14.36\% & 13.31\% & 15.17\% & 14.67\% & 14.12\% \\ \hline  & \multicolumn{4}{c|}{\(\mathcal{F},\mathcal{F}^{\text{N}}\) misspecified degree 2} & \multicolumn{4}{c|}{\(\mathcal{F},\mathcal{F}^{\text{N}}\) misspecified degree 4} \\ \cline{2-8}  & SPO+ DM & 9.90\% & 8.34\% & 7.41\% & 12.45\% & 11.16\% & 9.69\% \\ Both \(\mathcal{F},\mathcal{F}^{\text{N}}\) & SPO+ DR PI & 9.15\% & 7.23\% & 6.52\% & 11.92\% & 10.46\% & 9.42\% \\ Stop+ DR Lambda & 9.03\% & 7.46\% & 6.74\% & 12.01\% & 10.72\% & 9.25\% \\ SPO+ DR Clip & SPO+ DR Clip & 8.97\% & 7.22\% & 6.46\% & 11.75\% & 10.31\% & 8.95\% \\ Naive SPO+ DM & 15.65\% & 14.23\% & 13.02\% & 15.16\% & 14.53\% & 13.84\% \\ Naive SPO+ DR & 15.63\% & 14.42\% & 13.33\% & 15.26\% & 14.74\% & 13.95\% \\ \hline \hline \end{tabular}
\end{table}
Table 4: Mean relative regret ratio of different methods when the nuisance model \(\mathcal{F}^{\text{N}}\) and the policy-inducing model \(\mathcal{F}\) are misspecified to different degrees. The logging policy is a random policy.

\begin{table}
\begin{tabular}{c|c c c c c c c} \hline \hline \multirow{2}{*}{Methods} & \multicolumn{4}{c}{Training Data \(n\)} \\  & 400 & 600 & 800 & 1000 & 1200 & 1400 & 1600 \\ \hline ETO & 3.20\% & 1.83\% & 1.05\% & 0.72\% & 0.56\% & 0.47\% & 0.38\% \\ SPO+ DM & 2.16\% & 1.12\% & 0.60\% & 0.40\% & 0.30\% & 0.25\% & 0.20\% \\ SPO+ DR PI & 2.51\% & 1.50\% & 0.97\% & 0.73\% & 0.59\% & 0.50\% & 0.44\% \\ SPO+ DR Lambda & 2.14\% & 1.10\% & 0.62\% & 0.41\% & 0.34\% & 0.28\% & 0.23\% \\ SPO+ DR Clip & 2.15\% & 1.18\% & 0.65\% & 0.44\% & 0.37\% & 0.31\% & 0.26\% \\ SPO+ ISW & 15.49\% & 15.39\% & 15.46\% & 15.43\% & 15.63\% & 15.75\% & 15.75\% \\ \hline Naive ETO & 15.44\% & 14.92\% & 10.77\% & 6.56\% & 4.59\% & 3.39\% & 2.87\% \\ Naive SPO+ DM & 15.46\% & 15.21\% & 11.56\% & 7.49\% & 5.57\% & 4.32\% & 3.67\% \\ Naive SPO+ DR & 15.47\% & 15.19\% & 11.91\% & 8.09\% & 6.04\% & 4.81\% & 4.18\% \\ Naive SPO+ IPW & 15.57\% & 15.67\% & 15.71\% & 15.74\% & 15.66\% & 15.79\% & 15.80\% \\ \hline \hline \end{tabular}
\end{table}
Table 5: Mean relative regret ratio of different methods when the nuisance model \(\mathcal{F}^{\text{N}}\) and the policy-inducing model \(\mathcal{F}\) are correctly specified. The logging policy is a \(X_{1}\)-policy.

\begin{table}
\begin{tabular}{c|c|c c c|c c c} \hline \hline  & \multirow{2}{*}{Methods} & \multicolumn{4}{c|}{Training Data \(n\)} & \multicolumn{4}{c|}{Training Data \(n\)} \\  & & 400 & 1000 & 1600 & 400 & 1000 & 1600 \\ \hline \multirow{6}{*}{Well-specified} & \multirow{2}{*}{ETO} & \multicolumn{4}{c|}{\(\mathcal{F}\) misspecified degree 2} & \multicolumn{4}{c|}{\(\mathcal{F}\) misspecified degree 4} \\ \cline{3-8}  & & 11.55\% & 9.56\% & 8.68\% & 11.41\% & 10.03\% & 9.60\% \\ \hline \multirow{6}{*}{Well-specified} & \multicolumn{4}{c|}{\(\mathcal{F}\) misspecified degree 2} & \multicolumn{4}{c|}{\(\mathcal{F}\) misspecified degree 4} \\ \cline{2-8}  & SPO+ DM & 2.86\% & 1.10\% & 0.86\% & 4.19\% & 2.53\% & 2.27\% \\  & SPO+ DR PI & 3.57\% & 1.74\% & 1.39\% & 5.11\% & 3.36\% & 3.10\% \\ Nuisance Model & SPO+ DR Lambda & 2.93\% & 1.26\% & 1.03\% & 4.45\% & 2.79\% & 2.53\% \\ \cline{2-8} \cline{1-10} \multirow{2}{*}{\(\mathcal{F}^{\text{N}}\)} & SPO+ DR Clip & 3.05\% & 1.34\% & 1.11\% & 4.57\% & 2.98\% & 2.66\% \\  & Naive SPO+ DM & 15.59\% & 7.80\% & 4.37\% & 15.43\% & 8.08\% & 5.19\% \\  & Naive SPO+ DR & 15.60\% & 8.45\% & 5.00\% & 15.38\% & 8.83\% & 5.90\% \\ \hline \multirow{6}{*}{Well-specified} & \multicolumn{4}{c|}{\(\mathcal{F}^{\text{N}}\) misspecified degree 2} & \multicolumn{4}{c|}{\(\mathcal{F}^{\text{N}}\) misspecified degree 4} \\ \cline{2-8}  & SPO+ DM & 10.47\% & 8.31\% & 7.68\% & 10.77\% & 9.37\% & 8.82\% \\ \cline{1-1}  & SPO+ DR PI & 9.32\% & 7.48\% & 6.97\% & 10.69\% & 9.80\% & 9.43\% \\ Policy-inducing & SPO+ DR Lambda & 9.15\% & 7.48\% & 6.98\% & 10.36\% & 9.10\% & 8.72\% \\ Model \(\mathcal{F}\) & SPO+ DR Clip & 9.23\% & 7.37\% & 6.78\% & 10.37\% & 8.94\% & 8.46\% \\  & Naive SPO+ DM & 15.63\% & 14.58\% & 13.78\% & 14.93\% & 13.08\% & 12.71\% \\  & Naive SPO+ DR & 15.56\% & 14.54\% & 13.95\% & 14.92\% & 13.57\% & 13.30\% \\ \hline \multirow{6}{*}{Both \(\mathcal{F},\mathcal{F}^{\text{N}}\) Misspecified} & \multicolumn{4}{c|}{\(\mathcal{F},\mathcal{F}^{\text{N}}\) misspecified degree 2} & \multicolumn{4}{c|}{\(\mathcal{F},\mathcal{F}^{\text{N}}\) misspecified degree 4} \\ \cline{2-8}  & SPO+ DM & 10.36\% & 8.27\% & 7.58\% & 10.66\% & 9.35\% & 8.90\% \\ \cline{1-1}  & SPO+ DR PI & 9.19\% & 7.21\% & 6.46\% & 10.35\% & 8.96\% & 8.55\% \\ \cline{1-1}  & SPO+ DR Lambda & 9.24\% & 7.47\% & 6.98\% & 10.36\% & 9.18\% & 8.75\% \\ \cline{1-1}  & SPO+ DR Clip & 9.38\% & 7.39\% & 6.76\% & 10.28\% & 8.94\% & 8.55\% \\ \cline{1-1}  & Naive SPO+ DM & 15.61\% & 14.56\% & 13.88\% & 14.96\% & 13.25\% & 12.77\% \\ \cline{1-1}  & Naive SPO+ DR & 15.49\% & 14.62\% & 14.12\% & 14.96\% & 13.48\% & 13.23\% \\ \hline \hline \end{tabular}
\end{table}
Table 6: Mean relative regret ratio of different methods when the nuisance model \(\mathcal{F}^{\text{N}}\) and the policy-inducing model \(\mathcal{F}\) are misspecified to different degrees. The logging policy is a \(X_{1}\)-policy.

\begin{table}
\begin{tabular}{c|c c c c c c c} \hline \hline \multirow{2}{*}{Methods} & \multicolumn{4}{c}{Training Data \(n\)} & \multicolumn{4}{c}{Training Data \(n\)} \\  & & 400 & 600 & 800 & 1000 & 1200 & 1400 & 1600 \\ \hline ETO & 3.09\% & 1.92\% & 1.28\% & 0.86\% & 0.58\% & 0.41\% & 0.34\% \\ SPO+ DM & 2.19\% & 1.19\% & 0.66\% & 0.43\% & 0.32\% & 0.24\% & 0.19\% \\ SPO+ DR PI & 2.49\% & 1.42\% & 1.00\% & 0.72\% & 0.60\% & 0.46\% & 0.45\% \\ SPO+ DR Lambda & 2.08\% & 1.15\% & 0.67\% & 0.46\% & 0.35\% & 0.26\% & 0.21\% \\ SPO+ DR Clip & 2.17\% & 1.21\% & 0.73\% & 0.51\% & 0.36\% & 0.30\% & 0.25\% \\ SPO+ ISW & 14.57\% & 14.65\% & 14.49\% & 14.04\% & 14.36\% & 14.26\% & 14.30\% \\ \hline Naive ETO & 14.30\% & 13.99\% & 11.01\% & 7.55\% & 5.21\% & 3.76\% & 3.11\% \\ Naive SPO+ DM & 14.31\% & 14.21\% & 11.89\% & 8.44\% & 6.16\% & 4.69\% & 3.97\% \\ Naive SPO+ DR & 14.29\% & 14.23\% & 12.12\% & 8.77\% & 6.52\% & 5.12\% & 4.45\% \\ Naive SPO+ IPW & 15.52\% & 15.64\% & 15.64\% & 15.66\% & 15.76\% & 15.74\% & 15.70\% \\ \hline \hline \end{tabular}
\end{table}
Table 7: Mean relative regret ratio of different methods when the nuisance model \(\mathcal{F}^{\text{N}}\) and the policy-inducing model \(\mathcal{F}\) are correctly specified. The logging policy is a \(X_{1}X_{2}\)-policy.

\begin{table}
\begin{tabular}{c|c|c c c|c c c|} \hline \hline  & \multirow{2}{*}{Methods} & \multicolumn{3}{c|}{Training Data \(n\)} & \multicolumn{3}{c|}{Training Data \(n\)} \\  & & 400 & 1000 & 1600 & 400 & 1000 & 1600 \\ \hline  & \multirow{2}{*}{ETO} & \multicolumn{3}{c|}{\(\mathcal{F}\) misspecified degree 2} & \multicolumn{3}{c|}{\(\mathcal{F}\) misspecified degree 4} \\ \cline{3-8}  & & 10.08\% & 8.86\% & 8.18\% & 12.03\% & 11.15\% & 11.03\% \\ \hline  & \multicolumn{3}{c|}{\(\mathcal{F}\) misspecified degree 2} & \multicolumn{3}{c|}{\(\mathcal{F}\) misspecified degree 4} \\ \cline{2-8}  & SPO+ DM & 2.91\% & 1.10\% & 0.83\% & 4.32\% & 2.56\% & 2.24\% \\ Well-specified & SPO+ DR PI & 3.55\% & 1.73\% & 1.42\% & 5.27\% & 3.36\% & 3.04\% \\ Nuisance Model & SPO+ DR Lambda & 2.91\% & 1.28\% & 1.01\% & 4.50\% & 2.78\% & 2.49\% \\ \(\mathcal{F}^{\text{N}}\) & SPO+ DR Clip & 3.07\% & 1.37\% & 1.12\% & 4.67\% & 2.94\% & 2.69\% \\ Naive SPO+ DM & 14.32\% & 8.92\% & 4.69\% & 14.52\% & 9.32\% & 5.46\% \\ Naive SPO+ DR & 14.34\% & 9.33\% & 5.37\% & 14.56\% & 9.78\% & 6.13\% \\ \hline  & \multicolumn{3}{c|}{\(\mathcal{F}^{\text{N}}\) misspecified degree 2} & \multicolumn{3}{c|}{\(\mathcal{F}^{\text{N}}\) misspecified degree 4} \\ \cline{2-8}  & SPO+ DM & 8.78\% & 7.90\% & 7.11\% & 11.93\% & 11.08\% & 10.55\% \\ Well-specified & SPO+ DR PI & 8.23\% & 6.97\% & 6.51\% & 11.91\% & 11.36\% & 11.07\% \\ Policy-inducing & SPO+ DR Lambda & 8.28\% & 7.14\% & 6.60\% & 11.92\% & 11.01\% & 10.38\% \\ Model \(\mathcal{F}\) & SPO+ DR Clip & 8.19\% & 7.04\% & 6.37\% & 11.72\% & 10.95\% & 10.36\% \\ Naive SPO+ DM & 14.86\% & 12.50\% & 12.07\% & 14.68\% & 13.84\% & 13.09\% \\ Naive SPO+ DR & 14.77\% & 12.78\% & 12.26\% & 14.65\% & 14.19\% & 13.70\% \\ \hline  & \multicolumn{3}{c|}{\(\mathcal{F},\mathcal{F}^{\text{N}}\) misspecified degree 2} & \multicolumn{3}{c|}{\(\mathcal{F},\mathcal{F}^{\text{N}}\) misspecified degree 4} \\ \cline{2-8}  & SPO+ DM & 8.75\% & 7.83\% & 7.09\% & 11.95\% & 11.07\% & 10.53\% \\ Both \(\mathcal{F},\mathcal{F}^{\text{N}}\) & SPO+ DR PI & 8.07\% & 6.66\% & 6.03\% & 11.79\% & 10.84\% & 10.40\% \\ Stop+ DR Lambda & 8.23\% & 7.14\% & 6.58\% & 11.90\% & 10.87\% & 10.43\% \\ SNSPO+ DR Clip & 8.09\% & 6.95\% & 6.41\% & 11.90\% & 10.74\% & 10.22\% \\ Naive SPO+ DM & 14.85\% & 12.66\% & 12.14\% & 14.70\% & 14.02\% & 13.11\% \\ Naive SPO+ DR & 14.75\% & 12.78\% & 12.26\% & 14.67\% & 14.17\% & 13.41\% \\ \hline \hline \end{tabular}
\end{table}
Table 8: Mean relative regret ratio of different methods when the nuisance model \(\mathcal{F}^{\text{N}}\) and the policy-inducing model \(\mathcal{F}\) are misspecified to different degrees. The logging policy is a \(X_{1}X_{2}\)-policy.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: We provide the new methodology in Section 2, main theoretical results in Section 3, and numerical findings in Section 5.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We discuss the limitations of our paper in Section 6.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: Our main theoretical results are in Section 3, and we have a proposition in Section 2. All detailed proofs are provided in Appendix B.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We provide open access to data and code at https://github.com/CausalML/CLOBandit.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: We provide open access to data and code at https://github.com/CausalML/CLOBandit
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We provide detailed explanations for the experiment setup in Section 5 and Appendix C.2.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [No] Justification: We have statistical significance results in our experiments, and all our results are significant. We did not report them in the paper due to limited space, but can provide them upon request.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments?Answer: [Yes] Justification: Computer resources are reported in Appendix C.
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: The authors have reviewed the NeurIPS Code of Ethics and the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics.
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: This paper is foundational research and not tied to particular applications.
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: The paper poses no such risks.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [NA] Justification: The paper does not use existing assets
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: The paper does not release new assets.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects.