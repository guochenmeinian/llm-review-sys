# Imitation Learning from Vague Feedback

 Xin-Qiang Cai\({}^{1}\), Yu-Jie Zhang\({}^{1}\), Chao-Kai Chiang\({}^{1}\), Masashi Sugiyama\({}^{2,}\)\({}^{1}\)

\({}^{1}\) The University of Tokyo, Tokyo, Japan

\({}^{2}\) RIKEN AIP, Tokyo, Japan

###### Abstract

Imitation learning from human feedback studies how to train well-performed imitation agents with an annotator's relative comparison of two demonstrations (one demonstration is better/worse than the other), which is usually easier to collect than the perfect expert data required by traditional imitation learning. However, in many real-world applications, it is still expensive or even impossible to provide a clear pairwise comparison between two demonstrations with similar quality. This motivates us to study the problem of imitation learning with vague feedback, where the data annotator can only distinguish the paired demonstrations correctly when their quality differs significantly, i.e., one from the expert and another from the non-expert. By modeling the underlying demonstration pool as a mixture of expert and non-expert data, we show that the expert policy distribution can be recovered when the proportion \(\alpha\) of expert data is known. We also propose a mixture proportion estimation method for the unknown \(\alpha\) case. Then, we integrate the recovered expert policy distribution with generative adversarial imitation learning to form an end-to-end algorithm1. Experiments show that our methods outperform standard and preference-based imitation learning methods on various tasks.

Footnote 1: The code is available on https://github.com/caixq1996/COMPILER.

## 1 Introduction

Imitation learning (IL) is a popular approach for training agents to perform tasks by learning from expert demonstrations [1; 2; 3]. It has been applied successfully to a variety of tasks, including robot control [1], autonomous driving [4], and game playing [5]. However, traditional IL methods struggle when presented with both expert and non-expert demonstrations, as the agents may learn incorrect behaviors from the non-expert demonstrations [6]. This problem, which researchers refer to as "Imitation Learning from Imperfect Demonstration" (ILfID), arises when the demonstrations used to train the model may contain some non-expert data [6; 7; 8; 9].

One popular solution to ILfID is resorting to an oracle to provide specific information, such as explicit labels (confidence from an expert) of each demonstration, as in Figure 0(a). However, such a specific oracle is quite expensive. A more recent framework, Reinforcement Learning from Human Feedback (RLHF) [10; 11], incorporates human feedback into the learning process, including a key part of the pipeline used for training ChatGPT [12]. There exist two widely studied paradigms of feedback: full ranking and global binary comparison, as in Figures 0(b) and 0(c). Methods that use full-ranked demonstrations assumed that the feedback between every pair of trajectories is available, and further employed preference-based methods to solve the problem [13; 14]. Other studies have investigated situations where demonstrations can be divided into two global binary datasets, allowing the learner to filter out non-expert data from these global comparisons [7; 8]. However, data processed by any feedback of Figures 0(a), 0(b), and 0(c) require the guarantee that clear ranking information or at least one set consisting of pure expert demonstrations exists, so that off-the-shall IL methods can be applied immediately. The availability of expert demonstrations raises the question of what if, in practice, the feedback is not clear enough to provide purely expert demonstrations but mixed with non-expert demonstrations? The question poses a challenge that previous methods fail to address since none ofthe previous IL algorithms handles the issue of the _mixture_ of expert and non-expert demonstrations without other information. Their conventional strategies are to deal with the _vagueness_ in the data processing stage (relying on human feedback) not during the learning stage (by the learning agent).

Therefore, we consider a kind of weaker feedback than those previously formulated in the literature, which unintentionally mixes expert and non-expert demonstrations due to vague feedback. Specifically, the human annotator demonstrates in a pairwise way and only distinguishes the expert demonstration from the non-expert one. However, the annotator cannot tell the origin when both are from an expert or a non-expert. As depicted in Figure 0(d), this annotation process results in two datasets: \(\Gamma^{+}\), containing demonstrations the annotator believes to be most likely from experts, and \(\Gamma^{-}\), containing non-expert-like demonstrations. If one only places the distinguishable demonstrations into the pools and discards the indistinguishable ones, it results in Figure 0(c). Since we aim to investigate the potential of IL under the presence of non-expert demonstrations, we distribute the indistinguishable demonstrations randomly, one to \(\Gamma^{+}\) and the other to \(\Gamma^{-}\). This step embodies the _vagueness_ we wish to study in this paper. Note that either dataset contains non-expert demonstrations, and thus, a direct application of the existing IL method might be inappropriate due to the danger of learning from undesirable demonstrations.

_Vague feedback_ commonly exists in many scenarios, especially regarding RLHF [12]. In a crowd-sourcing example of routes navigation, as shown in Figure 0(e), most of the time crowd-workers may lack domain expertise. Meanwhile, it is quite difficult to distinguish every pair of demonstrations (when facing two trajectories from the same source, such as (\(\mathbb{I}\),\(\mathbb{I}\)) or (\(\mathbb{I}\),\(\mathbb{I}\))). Therefore, we cannot obtain high-quality crowd-sourcing labels when the data are annotated [15] as in Figures 0(a), 0(b), and 0(c). On the other hand, it is natural for workers to provide _vague_ comparisons as in Figure 0(d).

In this work, we formulate the problem of _Vaguely Pairwise Imitation Learning (VPIL)_, in which the human annotator can only distinguish the paired demonstrations correctly when their quality differs significantly. In Section 4, we analyze two situations within this learning problem: V PIL with known expert ratio \(\alpha\) and unknown \(\alpha\). For VPIL with known \(\alpha\), we provide rigorous mathematical analysis and show that the expert policy distribution can be empirically estimated with the datasets \(\Gamma^{+}\) and \(\Gamma^{-}\); for the more challenging situation of VPIL with unknown \(\alpha\), we propose a reduction of the problem of estimating \(\alpha\) to a mixture proportion estimation problem [16] and develop an iterative update procedure to estimate \(\alpha\). In Section 5, we integrate our algorithm with an off-the-shelf IL method to solve VPIL problems. In Section 6, we evaluate our methods with state-of-the-art ILfID methods on a variety of tasks of MuJoCo [17] with different \(\alpha\) ratios and find that our methods obtained the best performance.

## 2 Related Work

In imitation learning scenarios, a model is trained to mimic the actions of expert demonstrations. One of the main challenges of imitation learning is that the gathered demonstrations can be imprecise, making it difficult for the learner to accurately replicate the underlying policy of the demonstrations. To overcome this issue, various works have utilized an annotator, a source of supplementary supervi

Figure 1: (a)â€“(d): The comparisons among the imitation learning with the explicit label, full ranking, global comparison, and our setting. \(\mathbb{I}\) denotes the expert data while \(\mathbb{I}\) denotes the non-expert one. (e) An example of labeling route navigation. In the example, Path 1 and Path 2 share the same distance. A similar situation lies in Path 3 and Path 4. The data collectors cannot provide explicit, ranking, or pairwise information as in (a), (b), and (c), while they can only provide vague comparisons that \(\Gamma^{+}\) can be more expert-like than \(\Gamma^{-}\).

sion that can aid the learner in better understanding the expert's intent. For example, [18] used explicit action signals of the annotator; [14], [19], and [13] used ranking/preference information from the annotator to learn the policy; [6] utilized confidence information from the annotator to re-weight the unlabeled demonstrations and further learned the policy. However, as illustrated in Section 1, in some cases, the annotator may not be able to provide explicit information for the imperfect demonstrations, especially when the demonstrations come from different sources.

Alternative strategies for IL by imposing prior knowledge instead of an annotator have also been put forward, such as state density estimation for importance weighting [20], state-action extrapolating [21], and adding noise to recover ranking [22]. But this research line relies on certain assumptions about the state/state-action spaces, which may not always hold in many real-world applications. In this work, we focus on using an annotator with vague information, which is also low-cost.

Our methods drew inspiration from the risk rewriting technique in weakly supervised learning literature [23], which aims to establish an unbiased risk estimator for evaluating the model's quality only with weak supervision. Although the technique has been successfully applied to various specific weakly supervised learning problems [24; 25; 26; 27], these methods typically require knowledge of a parameter called the mixture proportion, i.e., the expert ratio in our problem [28], the estimation of which can be challenging in the imitation learning problem (see Section 6.3 for more details). To overcome this issue, we introduce an iterative data selection procedure to better estimate the expert ratio, leading to superior empirical performance. It is worth noting that [6] also used the risk rewriting technique to identify the expert demonstrations, but their algorithm does not require estimation of the expert ratio as the confidence score is given. However, such information is unavailable in VPIL, making the problem more challenging.

## 3 Preliminaries and Problem Setting

In this section, we first introduce the IL process. Then we formulate the VPIL problem.

### Preliminaries

Markov Decision Process.In policy learning problems, a Markov Decision Process (MDP) can be represented by a tuple \(\langle\mathcal{S},\mathcal{A},\mathcal{P},\gamma,r,T\rangle\), where \(\mathcal{S}\) is the state space; \(\mathcal{A}\) is the action space; \(\mathcal{P}:\mathcal{S}\times\mathcal{A}\times\mathcal{S}\rightarrow[0,1]\) is the transition probability distribution; \(\gamma\) is a discount factor in the range \((0,1]\); \(r:\mathcal{S}\rightarrow\mathbb{R}\) is the reward function; and \(T\) is the horizon. The goal is to learn a policy \(\pi\) that maximizes the expected returns \(\mathbb{E}[\sum_{t=0}^{\infty}\gamma^{t}r(s_{t},a_{t})]\), where \(\mathbb{E}\) denotes the expectation.

In IL, the learner does not have access to the reward function \(r\). Instead, the learner is given \(m\) expert demonstrations \(\tau_{\mathbb{E},1},\tau_{\mathbb{E},2},\ldots,\tau_{\mathbb{E},m}\), where \(\tau_{\mathbb{E},i},i\in\{1,\ldots,m\}\) is the \(i\)-th trajectory (a series of state-action pairs) drawn independently following the demonstrator's policy \(\tau_{\mathbb{E}}\). The goal of the learner is to learn a policy \(\pi_{\theta}\) to mimic \(\pi_{\mathbb{E}}\).

Occupancy Measure.Since in IL we do not have reward functions, we need to measure the policy performance in the state-action space, i.e., occupancy measure \(\rho_{\pi}\). The occupancy measure \(\rho_{\pi}:\mathcal{S}\times\mathcal{A}\mapsto\mathbb{R}\) was introduced to characterize the distribution of state-action pairs generated by a given policy \(\pi\), which was defined with the discount factor: \(\rho_{\pi}(s,a)=\pi(a|s)\sum_{t=1}^{\infty}\gamma^{t}\Pr[s_{t}=s|\pi]\), where \(\Pr[s_{t}=s|\ \pi]\) is the probability of reaching state \(s\) at time \(t\) following policy \(\pi\)[29].

Moreover, we know that there is a one-to-one correspondence between the occupancy measure and the policy. Specifically, we have the following theorem.

Figure 2: The description of the data collection process.

**Theorem 1** (Theorem 2 of [29]).: _Suppose \(\rho\) is the occupancy measure for \(\pi_{\rho}(a\,|\,s)\coloneqq\frac{\rho(s,a)}{\sum_{a^{\prime}\in A}\rho(s,a^{ \prime})}\). Then, \(\pi_{\rho}\) is the only policy whose occupancy measure is \(\rho\)._

We can also define a normalized version of the occupancy measure by \(p_{\pi}(s,a)\triangleq\frac{p_{\pi}(s,a)}{\sum_{s^{\prime}\in S,a^{\prime}\in A }p_{\pi}(s^{\prime},a^{\prime})}=(1-\gamma)\rho_{\pi}(s,a)\). If we have \(m\) expert demonstrations \(\tau_{\mathtt{E},1},\tau_{\mathtt{E},2},\ldots,\tau_{\mathtt{E},m}\), the occupancy measure \(p_{\pi_{\mathtt{E}}}\) can be empirically estimated by the trajectories as \(\widehat{p}_{\pi_{\mathtt{E}}}(s,a)=\frac{1-\gamma}{m}\sum\limits_{i=1}^{m} \sum\limits_{t=0}^{\infty}\gamma^{t}\mathbbm{1}\bigg{[}(s_{t}^{(i)},a_{t}^{(i )})=(s,a)\bigg{]}\), where \(\mathbbm{1}[\cdot]\) is an indicator function that returns 1 if \(\cdot\) is true and returns 0 otherwise. Finding out the underlying \(p_{\pi_{\mathtt{E}}}\) is the key for solving IL problems [1, 30].

### Vaguely Pairwise Imitation Learning

In this work, we focus on the ILfID problem with pairwise information, where the learner aims to learn a good policy with a pairwise dataset \((\Gamma^{+},\Gamma^{-})\), generated from a mixture demonstration pool performed by an expert policy \(\pi_{\mathtt{E}}\) and a set of non-expert policies \(\{\pi_{\mathtt{NE}}^{(k)}\}_{k=1}^{K}\). In this work, we consider the mixed occupancy measure of the non-expert policy set as \(p_{\pi_{\mathtt{NE}}}\). The proportion of the expert data within the pool is denoted as \(\alpha\in(0,1]\). The data collection process is shown in Figure 2. To clearly reveal the effect of mixed demonstrations and explain the proposed framework, we choose to assume that the data collector is not an attacker and will make mistakes, so there will be no noise during the collection process. In Section 6.4, we discuss how our method can easily be extended to handle the presence of human error.

Also, in this work, we do not consider that the data collector could be an attacker or make mistakes, so there will be no noise during the collection process.

Collection of Pairwise Datasets.The data collector would first sample a pair of trajectories \((\tau_{i},\tau_{j})\) independently from the mixture pool. If \((\tau_{i},\tau_{j})\) are from different sources, i.e., one is from the expert, and another is not, then the collector will take the expert one \(\tau_{i}\) into \(\Gamma^{+}\) and the non-expert one \(\tau_{j}\) into \(\Gamma^{-}\). Otherwise, the collector randomly puts them into \(\Gamma^{+}\) and \(\Gamma^{-}\). Under such a data generation process, the expertise probabilities of this pair are as follows:

\[\left\{\begin{array}{l}\text{Pr}[\tau_{i}\sim p_{\pi_{\mathtt{E}}},\tau_{j} \sim p_{\pi_{\mathtt{E}}}]=\alpha^{2},\\ \text{Pr}[\tau_{i}\sim p_{\pi_{\mathtt{NE}}},\tau_{j}\sim p_{\pi_{\mathtt{NE}}} ]=(1-\alpha)^{2},\\ \text{Pr}[\tau_{i}\sim p_{\pi_{\mathtt{E}}},\tau_{j}\sim p_{\pi_{\mathtt{NE}}} ]=2\alpha(1-\alpha).\end{array}\right.\] (1)

In such a case, \(\Gamma^{+}\) is always "not worse" than \(\Gamma^{-}\) as it contains more expert data.

## 4 Learning frameworks for Solving VPIL Problems

In this section, we analyze the core challenge in VPIL problems with the pairwise datasets \(\Gamma^{+}\) and \(\Gamma^{-}\), i.e., recovering the occupancy measure of the expert policy \(p_{\pi_{\mathtt{E}}}\). To this end, we propose two learning frameworks for VPIL with known \(\alpha\) and unknown \(\alpha\), respectively.

### VPIL with Known \(\alpha\)

Most of the IL methods based on the occupancy measure need to assume that the demonstrations are sampled only from the expert policy, so that they can directly estimate \(p_{\pi_{\mathtt{E}}}\) and match the learner's distribution with \(p_{\pi_{\mathtt{E}}}\). However, the occupancy measure of the expert policy \(p_{\pi_{\mathtt{E}}}\) is inaccessible in the VPIL problem, since both \(\Gamma^{+}\) and \(\Gamma^{-}\) contain non-expert data while the label of each data is unavailable. Below, we attempt to approximate \(p_{\pi_{\mathtt{E}}}\) with \(\{\Gamma^{+},\Gamma^{-}\}\).

Let \(n_{+}=|\Gamma^{+}|\); \(\widehat{p}_{i}^{+}(s,a)=(1-\gamma)\sum_{t=0}^{\infty}\gamma^{t}\mathbbm{1} \left[(s_{t,i},a_{t,i})=(s,a)\right]\) for \(i=1,\ldots,n_{+}\) be the empirical occupancy measure of a trajectory \(\tau_{i}\in\Gamma^{+}\), where \((s_{t,i},a_{t,i})\in\tau_{i}\) is a state action pair at time \(t\). Let \(\widehat{p}_{\tau_{+}}(s,a)=\sum_{i=1}^{n_{+}}\widehat{p}_{i}^{+}(s,a)/n_{+}\). Define \(\widehat{p}_{\pi_{-}}(s,a)\) similarly. We have the following theorem.

**Theorem 2**.: _Assume the pairwise datasets \(\left(\Gamma^{+},\Gamma^{-}\right)\) are generated following the procedure in Section 3.2. Let \(p_{\pi_{+}}(s,a)=\mathbb{E}_{\Gamma^{+}}\left[\widehat{p}_{\pi_{+}}(s,a)\right]\) and \(p_{\pi_{-}}(s,a)=\mathbb{E}_{\Gamma^{-}}\left[\widehat{p}_{\pi_{-}}(s,a)\right]\) be the expected occupancy measures of \(\Gamma^{+}\) and \(\Gamma^{-}\), where the randomness is taken over the draws of \(\Gamma^{+}\) and \(\Gamma^{-}\). Then, we have_

\[\left\{\begin{array}{l}p_{\pi_{+}}(s,a)=\ \ \left(2\alpha-\alpha^{2}\right)p_{\pi_{ \mathtt{E}}}(s,a)+(1-\alpha)^{2}p_{\pi_{\mathtt{NE}}}(s,a),\\ p_{\pi_{-}}(s,a)=\ \ \alpha^{2}p_{\pi_{\mathtt{E}}}(s,a)+(1-\alpha^{2})p_{\pi_{ \mathtt{NE}}}(s,a),\end{array}\right.\] (2)\[\begin{cases}p_{\pi_{\varepsilon}}(s,a)=&\frac{1+\alpha}{2\alpha}p_{\pi_{+}}(s,a)- \frac{1-\alpha}{2\alpha}p_{\pi_{-}}(s,a),\\ p_{\pi_{NE}}(s,a)=&-\frac{\alpha}{2(1-\alpha)}p_{\pi_{+}}(s,a)+\frac{2-\alpha} {2-2\alpha}p_{\pi_{-}}(s,a).\end{cases}\] (3)

A proof can be found in Appendix A. The Theorem provides a feasible way to recover the unknown occupancy measure of the expert policy from contaminated data pools \(\Gamma^{+}\) and \(\Gamma^{-}\). Thus, we can use an off-the-shelf IL method to learn the policy.

Once we have recovered \(p_{\pi_{\varepsilon}}\) by Eq. (10), we can use an off-the-shelf IL method to learn the policy.

### VPiL with Unknown \(\alpha\)

When facing a more challenging problem in which the expert ratio \(\alpha\) is unknown, a straightforward way is to estimate the ratio first and then reconstruct the expert policy by the approach developed for known \(\alpha\) cases. Here, we will first introduce how to estimate the expert ratio by reducing it into a mixture proportion estimation (MPE) problem [28] and then identify that a direct application of the estimated ratio is not accurate enough to reconstruct the expert policy. To this end, we further propose an iterative sample selection procedure to exploit the estimated expert ratio, which finally leads to a better approximation of the expert policy.

Estimation of the Expert Ratio \(\alpha\).In this paragraph, we show that the estimation of the expert ratio \(\alpha\) can be reduced to two MPE problems [28]. Let \(\mathcal{P}\) and \(\mathcal{N}\) be two probability distributions, and

\[\mathcal{U}=\beta\mathcal{P}+(1-\beta)\mathcal{N}\] (4)

be a mixture of them with a certain proportion \(\beta\in(0,1)\). The MPE problem studies how to estimate mixture proportion \(\beta\) with the empirical observations sampled from \(\mathcal{U}\) and \(\mathcal{P}\) (not from \(\mathcal{N}\)). Over the decades, various algorithms were proposed with sound theoretical and empirical studies [31, 32].

To see how the estimation of the expert ratio \(\alpha\) is related to the MPE problem, we rewrite (2) as

\[p_{\pi_{+}}(s,a)=\beta_{1}p_{\pi_{-}}(s,a)+(1-\beta_{1})p_{\pi_{\varepsilon}} \ \ \text{and}\ \ \ p_{\pi_{-}}(s,a)=\beta_{2}p_{\pi_{+}}(s,a)+(1-\beta_{2})p_{\pi_{NE}},\] (5)

where \(\beta_{1}=\frac{1-\alpha}{1+\alpha}\) and \(\beta_{2}=\frac{\alpha}{2-\alpha}\). By taking \(p_{\pi_{+}}\) as \(\mathcal{U}\) and \(p_{\pi_{-}}\) as \(\mathcal{P}\), the first line of (5) shares the same formulation as the MPE problem (4). Since \(p_{\pi_{+}}\) and \(p_{\pi_{-}}\) are empirically accessible via \(\Gamma^{+}\) and \(\Gamma^{+}\), we can estimate \(\beta_{1}\) by taking \(\Gamma^{+}\) and \(\Gamma^{-}\) as the input of any MPE solver and obtain the expert ratio by \(\alpha=\frac{1-\beta_{1}}{1+\beta_{1}}\). The same argument also holds for the second line of (5), where we can take \(p_{\pi_{-}}\) as \(\mathcal{U}\) and \(p_{\pi_{+}}\) as \(\mathcal{P}\) to estimate \(\beta\). Then, the expert ratio can also be obtained by \(\alpha=\frac{2\beta_{2}}{1+\beta_{2}}\).

One might worry about the identifiability of the expert ratio \(\alpha\) by estimating it with MPE techniques [28]. We can show that the true parameter is identifiable if the distribution \(p_{\pi_{+}}\) and distribution \(p_{\pi_{-}}\) are mutually irreducible [16] as follows:

**Proposition 1**.: _Suppose the distributions \(p_{\pi_{\varepsilon}}\) and \(p_{\pi_{NE}}\) are mutually irreducible such that there exists no decomposition of the form \(p_{\pi_{\varepsilon}}=(1-\eta)Q+\eta p_{\pi_{NE}}\) and \(p_{\pi_{NE}}=(1-\eta^{\prime})Q^{\prime}+\eta^{\prime}p_{\pi_{E}}\) for any probability distributions \(Q,Q^{\prime}\) and scalars \(\eta,\eta^{\prime}\in(0,1]\). Then, the true mixture proportions \(\beta_{1}\) and \(\beta_{2}\) are unique and can be identified by_

\[\left\{\begin{array}{c}\beta_{1}=\sup\{\eta|p_{\pi_{+}}=\eta p_{\pi_{-}}+(1 -\eta)K,K\in\mathcal{C}\},\\ \beta_{2}=\sup\{\eta^{\prime}|p_{\pi_{-}}=\eta^{\prime}p_{\pi_{+}}+(1-\eta^{ \prime})K^{\prime},K^{\prime}\in\mathcal{C}\},\end{array}\right.\] (6)

_where \(\mathcal{C}\) is the set containing all possible probability distributions. Thus, \(\alpha\) is identifiable by_

\[\alpha=(1-\beta_{1})/(1+\beta_{1})\ \ \ \text{or}\ \ \ \ \alpha=2\beta_{2}/(1+\beta_{2}).\] (7)

Proposition 1 demonstrates that the true mixture proportion \(\beta_{1}\) (resp. \(\beta_{2}\)) can be identified by finding the maximum proportion of \(p_{\pi_{-}}\) contained in \(p_{\pi_{+}}\) (resp. \(p_{\pi_{+}}\) contained in \(p_{\pi_{-}}\)). This idea can be empirically implemented via existing MPE methods [28, 33]. We note that the expert ratio is identifiable by either estimating \(\beta_{1}\) or \(\beta_{2}\) when we have infinite samples. In the finite sample case, the two estimators coming from different distribution components could lead to different estimation biases. Since the MPE solutions tend to have a positive estimation bias on the true value \(\beta_{1}\) and \(\beta_{2}\) as shown by [28, Theorem 12] and [31, Corollary 1], the estimation \(\alpha=(1-\beta_{1})/(1+\beta_{1})\) tends to yield an underestimated \(\alpha\) while that with \(\beta_{2}\) will lead to an overestimation. Besides, when the number of expert data is quite small, the underlying true parameter \(\beta_{2}\) would also have a small value,and its estimation would be highly unstable. Thus, we choose to estimate \(\alpha\) with \(\beta_{1}\) in our algorithm. The empirical studies in Section 6.3 also supported our choice.

The relationship (2) between the expert and non-expert data shares a similar formulation to that of the unlabeled-unlabeled (UU) classification [34], which is a specific kind of weakly supervised learning problem studying how to train a classifier from two unlabeled datasets and requires the knowledge of mixture proportions. However, how to estimate these mixture proportions is still an open problem in the weakly supervised learning literature. Although our reduction is developed for estimating the expert ratio for IL problems, it can be applied to a UU learning setting for independent interest.

Reconstruct Expert Policy by Data Selection.To handle the MPE task for high dimensional data, a practice is to first train a classifier with probability output and then conduct the MPE on the probability outputs of samples [35]. However, the quality of the trained classifier turns out to depend on the estimation accuracy of \(\alpha\). On the other hand, we found that it is possible to filter out the undesired component of the pairwise datasets (\(\widehat{p}_{\pi_{\text{E}}}\) in \(\Gamma^{-}\) and \(\widehat{p}_{\pi_{\text{E}}}\) in \(\Gamma^{+}\)) and keep the desired data to enhance estimating \(\alpha\). Therefore, also inspired by [31, 36] of the distribution shift problem and weakly supervised learning, we propose an iteration-based learning framework. In each iteration, we throw away the non-expert data with higher confidence in \(\Gamma^{+}\) and the expert data with higher confidence in \(\Gamma^{-}\) after estimating \(\alpha\), and train a classifier with the datasets after selection. The detailed learning process can be found in Algorithm 1.

```
0: Pairwise Demonstrations \(\Gamma^{+}\), \(\Gamma^{-}\); Expert ratio \(\alpha\) (Unknown for COMPILER-E); Environment env.
0: The learner policy \(\pi_{\theta}\).
1: Initialize the learner policy \(\pi_{\theta}\) and the discriminator \(D_{\omega}\).
2:if\(\alpha\) is unknown then\(\triangleright\) COMPILER-E
3: Obtain the estimated expert ratio \(\alpha\leftarrow\texttt{ExpertRatioEstimation}(\Gamma^{+},\Gamma^{-})\).
4:else\(\triangleright\) COMPILER
5:\(\alpha\leftarrow\) input(\(\alpha\)).
6:endif
7:for each training steps do
8: Sample a batch of learner's data \((s,a)\sim p_{\pi_{\theta}}\) by the interactions between \(\pi_{\theta}\) and env.
9: Sample a batch of demonstrations data \((s,a)\sim\widehat{p}_{\pi_{+}}\) and \((s,a)\sim\widehat{p}_{\pi_{-}}\) from \(\Gamma^{+}\) and \(\Gamma^{-}\).
10: Update \(D_{\omega}\) by maximizing (8).
11: Update \(\pi_{\theta}\) with \((s,a)\sim p_{\pi_{\theta}}\) and the reward \(-\log D_{\omega}(s,a)\) using off-the-shelf RL algorithm.
12:endfor ```

**Algorithm 1**ExpertRatioEstimation

After we obtained the estimated \(\widehat{\alpha}\), we can recover \(p_{\pi_{\text{E}}}\) as by Eq. (10).

COMPParative Imitation LEarning with Risk rewriting (COMPILER)

We have illustrated how to estimate the occupancy measure of the expert policy \(p_{\pi_{\text{e}}}\) from \((\Gamma^{+},\Gamma^{-})\) under known and unknown \(\alpha\) respectively. Here we describe how to adopt these learning frameworks into end-to-end algorithms for learning an policy. We name our algorithm for VPIL with known \(\alpha\)**COMParative Imitation LEarning with Risk-rewriting (COMPILER)**, and with unknown \(\alpha\)**COMParative Imitation LEarning with Risk-rewriting by Estimation (COMPILER-E)**.

Current state-of-the-art adversarial IL methods [37; 1] aim to learn a policy by matching the occupancy measure between the learner and the expert policies. We fuse our methods with one of the representative adversarial IL methods, Generative Adversarial Imitation Learning (GAIL) [1], whose optimization problem is given as follows:

\[\min_{\theta\in\Theta}\max_{w\in\mathcal{W}}\mathbb{E}_{(s,a)\sim p_{\pi_{ \theta}}}\log D_{w}(s,a)+\mathbb{E}_{(s,a)\sim p_{\pi_{\text{e}}}}\log(1-D_{w} (s,a)),\]

where \(D_{w}:\mathcal{S}\times\mathcal{A}\mapsto[0,1]\) parameterized by \(w\) is a discriminator. \(p_{\pi_{\theta}}\) parameterized by \(\theta\) is the occupancy measure of a policy. In our setting, the expert demonstration of \(p_{\pi_{\text{e}}}\) is unavailable.

COMPILER.As suggested by Theorem 2, we can recover \(p_{\pi_{\text{e}}}\) with the occupancy measures of \(\Gamma^{+}\) and \(\Gamma^{-}\). Then, we can train the policy \(p_{\pi_{\theta}}\) by mincing \(p_{\pi_{\text{e}}}=\frac{1+\alpha}{2\alpha}p_{\pi_{+}}-\frac{1-\alpha}{2\alpha }p_{\pi_{-}}\) in Eq. (10). Specifically, plugging (2) into (8) and approximating \(p_{\pi_{+}}\) and \(p_{\pi_{-}}\) with their empirical versions, we can obtain the desirable target. However, such rewriting can introduce a negative term and lead to overfitting [38; 39]. To avoid such an undesired phenomenon, instead of training the policy \(\pi_{\theta}\) to make \(p_{\pi_{\theta}}\) match \(\frac{1+\alpha}{2\alpha}\hat{p}_{\pi_{+}}-\frac{1-\alpha}{2\alpha}\hat{p}_{ \pi_{-}}\), we twist the objective function of GAIL (8) to minimize the discrepancy between \(\frac{2\alpha}{1+\alpha}p_{\pi_{\theta}}+\frac{1-\alpha}{1+\alpha}\hat{p}_{ \pi_{-}}\) and \(\hat{p}_{\pi_{+}}\), which gives the following optimization problem without the negative term:

\[\min_{\theta\in\Theta}\max_{w\in\mathcal{W}}2\alpha\mathbb{E}_{( s,a)\sim p_{\pi_{\theta}}}\log D_{w}(s,a) +(1-\alpha)\mathbb{E}_{(s,a)\sim\widehat{p}_{\pi_{-}}}\log(D_{w} (s,a))\] \[+(1+\alpha)\mathbb{E}_{(s,a)\sim\widehat{p}_{\pi_{+}}}\log(1-D_{ w}(s,a)).\] (8)

Let \(\widehat{V}(\pi_{\theta},D_{w})\) be the objective function in (8) and \(V(\pi_{\theta},D_{w})\) be its expectation established on \(p_{\pi_{+}}\) and \(p_{\pi_{-}}\). We have the following theorem for the estimation error of the discriminator trained by (8).

**Theorem 3**.: _Let \(\mathcal{W}\) be a parameter space for training the discriminator and \(D_{\mathcal{W}}=\{D_{w}\mid w\in\mathcal{W}\}\) be the hypothesis space. Assume the functions \(\left|\log D_{w}(s,a)\right|\leq B\) and \(\left|\log(1-D_{w}(s,a))\right|\leq B\) are upper-bounded for any state-action pair \((s,a)\in\mathcal{S}\times\mathcal{A}\) and \(w\in\mathcal{W}\). Further assume both the functions \(\log D_{w}(s,a)\) and \(\log(1-D_{w}(s,a))\) are \(L\)-Lipschitz continuous in the state-action space. For a fixed policy \(\pi_{\theta}\), let \(\Gamma^{\theta}=\{\tau_{i}^{\theta}\}_{i=1}^{n_{\theta}}\) be trajectories generated from \(\pi_{\theta}\). Then, for any \(\delta\in(0,1)\), with probability at least \(1-\delta\), we have_

\[V(\pi_{\theta},D_{w}^{*})-V(\pi_{\theta},\widehat{D}_{w}) \leq 4L(1+\alpha)\mathcal{R}_{n_{+}}(D_{\mathcal{W}})+4L(1-\alpha) \mathcal{R}_{n_{-}}(D_{\mathcal{W}}))\] \[+8L\mathcal{R}_{n_{\theta}}(D_{\mathcal{W}})+C(\delta)\left( \frac{1}{\sqrt{n_{\theta}}}+\frac{1}{\sqrt{n_{+}}}+\frac{1}{\sqrt{n_{-}}} \right),\]

_where \(\widehat{D}_{w}=\operatorname*{arg\,max}_{w\in\mathcal{W}}\widehat{V}(\pi_{ \theta},D_{w})\) and \(D_{w}^{*}=\operatorname*{arg\,max}_{w\in\mathcal{W}}V(\pi_{\theta},D_{w})\). The constants \(n_{+}\) and \(n_{-}\) are the number of trajectories in \(\Gamma^{+}\) and \(\Gamma^{-}\). We define \(C(\delta)=4B\sqrt{\log(6/\delta)}\). The empirical Radamacher complexities [40] on datasets \(\Gamma^{+}\), \(\Gamma^{-}\), and \(\Gamma^{\theta}\) are denoted by \(\mathcal{R}_{n_{+}}\), \(\mathcal{R}_{n_{-}}\), and \(\mathcal{R}_{n_{\theta}}\)._

A proof is given in Appendix A. The theorem shows that the discriminator trained by (8) converges to the one optimized with the true distribution \(p_{\pi_{+}}\) and \(p_{\pi_{-}}\) at each step of the training.

COMPILER-E.For the VPIL problem with unknown \(\alpha\), first we estimate \(\widehat{\alpha}\) by Algorithm 1, then we learn the policy by (8) with \(\widehat{\alpha}\).

We integrate the two algorithmic processes COMPILER and COMPILER-E into Algorithm 2.

## 6 Experiments

In this section, we conducted extensive experiments under the setting of VPIL. Through the experiments, we want to investigate the following questions: (1) Can COMPILER and COMPILER-E solve VPIL problems under various expert ratios in demonstrations? (2) Is COMPILER-E still valid when using different MPE estimators to obtain \(\widehat{\alpha}\)? (3) How is the \(\alpha\) estimation with \(\beta_{1}\) and \(\beta_{2}\) as in (7)?

### Setup

To investigate the answer to the first question, we chose 20 different VPIL tasks with four MuJoCo benchmark environments to evaluate the performance of the contenders and our approaches under five different \(\alpha\) levels. The detailed setups are reported as follows.

Environments and Demonstrations.We set _HalfCheetah, Hopper, Swimmer_, and _Walker2d_ in MuJoCo as the basic environments. For each experiment, the demonstration pool contains 100 trajectories with different expert ratios \(\alpha=\{0.1,0.2,0.3,0.4,0.5\}\), in which \(\alpha=0.1\) is the most difficult situation of the VPIL problem. We also trained an expert-level RL agent and two non-expert RL agents as demonstrators by DDPG algorithm [41]. The details of the environment and the demonstrations can be found in Table 1.

Dataset generation process.We start with the demonstration pool \(\{\tau_{i}\}_{i=1}^{N}\), which contains \(\alpha N\) trajectories sampled from the optimal policy \(\pi_{\text{E}}\) and \((1-\alpha)N\) trajectories from the non-optimal policy \(\pi_{\text{NE}}\). For notation simplicity, we denote by \(\mathcal{D}_{+}\) the part containing expert demonstrations only and by \(\mathcal{D}_{-}\) the part for the non-expert demonstrations. Then, we can generate the dataset as follows,

\[\begin{cases}\Gamma^{+}=&\operatorname{sample}(\mathcal{D}_{+},(2\alpha- \alpha^{2})N)\bigcup\operatorname{sample}(\mathcal{D}_{-},(1-\alpha)^{2}N), \\ \Gamma^{-}=&\operatorname{sample}(\mathcal{D}_{+},\alpha^{2}N)\bigcup \operatorname{sample}(\mathcal{D}_{-},(1-\alpha^{2})N),\end{cases}\]

where \(\operatorname{sample}(\mathcal{D},N)\) is the function that samples \(N\) trajectories from the dataset \(\mathcal{D}\).

Contenders.Since \(\Gamma^{+}\) contains more expert demonstrations, here we use Behavior Cloning [42], GAIL [1], and AIRL [43] with \(\Gamma^{+}\) only as basic baselines. Also, we provide GAIL with expert-level demonstrations (GAIL_expert) as the skyline of all methods. Besides, we set T-REX [13], a state-of-the-art preference-based IL method, by taking that every data in \(\Gamma^{+}\) is more expert-like than that in \(\Gamma^{-}\) as a form of preference to train its reward function. We also set the variants of T-REX algorithms, D-REX [22] and SSRR [44], that directly generate the ranking information through the imperfect datasets. CAIL proposed a confidence-based method to imitate from mixed demonstrations [14]. In the experiment, we provide 5% trajectory labels to meet its requirement as suggested in their paper. Each method ran 4e7 steps. 5 trials were conducted with different seeds for each task.

Meanwhile, to investigate the answer to the second question, we conducted Best Bin Estimation (BBE) [31] and Kernel Mean (KM) [28] algorithm to estimate \(\beta\) in algorithm 1 of COMPILER-E, as COMPILER-E (BBE) and COMPILER-E (KM) respectively. We note that COMPILER-E cannot obtain the true \(\alpha\) during the whole experiment.

Implementation of \(\alpha\) estimation.We implement a four-layer fully connected neural network as the binary classifier \(f_{\psi}\) in Algorithm 1. The neurons in each layer are 1000, 1000, 100, and 50 respectively. The activation function of each layer is ReLU, and the output value will go through the sigmoid function to obtain the score for \(\Gamma^{+}\) and \(\Gamma^{-}\) as \(S^{+}\) and \(S^{-}\) respectively. The optimizer for \(f_{\psi}\) is stochastic gradient descent. We totally train \(f_{\psi}\) for 1000 epochs with bath size 1024. The initial learning rate is 5e-4, with an exponential decay rate of 0.99 at each step.

Implementation of policy training.We choose Proximal Policy Optimization (PPO) [45] as the basic RL algorithm, and set all hyper-parameters, update frequency, and network architectures of the policy part the same as [46]. Besides, the hyper-parameters of the discriminator for all methods were the same: The discriminator was updated using Adam with a decayed learning rate of \(3\times 10^{-4}\); the batch size was 256. The ratio of update frequency between the learner and discriminator was 3: 1. For T-REX algorithm [13], we use the codes with default hyperparameters and model architecture of their official implementation.

### Empirical Results

The final results are gathered in Table 2. We can see that since AIRL and GAIL are standard IL algorithms, they cannot adaptively filter out non-expert data, so the underlying non-expert data reduces the performance of them. T-REX did not obtain promising results on VPIL tasks, which

\begin{table}
\begin{tabular}{c|c c c c c c} \hline \hline  & Dimension of \(S\) & Dimension of \(\mathcal{A}\) & Non-Expert 1 & Non-Expert 2 & Expert & \(\alpha\) \\ \hline Hopper & 11 & 3 & 1142.16\(\pm\)159.28 & 1817.83\(\pm\)819.69 & 3606.11\(\pm\)43.95 & (0.1, 0.2, 0.3, 0.4, 0.5) \\ Swimmer & 8 & 2 & 46.28\(\pm\)18.7 & 62.64\(\pm\)16.82 & 100.63\(\pm\)469 & (0.1, 0.2, 0.3, 0.4, 0.5) \\ Walker2d & 17 & 6 & 410.98\(\pm\)36.59 & 766.67\(\pm\)398.85 & 3204.37\(\pm\)848.55 & (0.1, 0.2, 0.3, 0.4, 0.5) \\ HalfCheetah & 17 & 6 & 532.50\(\pm\)58.66 & 864.18\(\pm\)335.90 & 1599.06\(\pm\)41.97 & (0.1, 0.2, 0.3, 0.4, 0.5) \\ \hline \hline \end{tabular}
\end{table}
Table 1: The detailed information of demonstrations in the empirical studies.

[MISSING_PAGE_FAIL:9]

bigger as true \(\alpha\) decreased. On the other hand, although the underestimated method also overthrew the non-expert data in \(\Gamma^{+}\), the proportion of these data was relatively high. So even if more were thrown away at the beginning, it will not affect the accuracy of the estimation. This is the reason why using \(\frac{1-\beta}{1+\beta}\) to estimate \(\widehat{\alpha}\) is relatively stable and accurate.

### Case Study: Assessing the Robustness of the COMPILER Algorithm

To evaluate the robustness of the COMPILER algorithm, a comprehensive set of experiments were conducted. These were carried out in four distinct environments, implementing tasks with \(\alpha=0.5\). Each experiment was repeated five times to ensure the reliability of the results. To emulate real-world conditions, different levels of noise were introduced to the parameter \(\alpha\), resulting in the generation of corresponding noisy datasets. This noise was denoted as \(\varepsilon\), thereby resulting in \(\widehat{\alpha}=\alpha+\varepsilon\).

Figure 4 presents the findings from these experiments. Remarkably, the COMPILER algorithm demonstrated consistent performance, achieving at least 96% of the standard performance even in the presence of various noise levels. This robustness, as exhibited in the results, underlines the capacity of COMPILER to effectively handle and perform under noisy conditions, specifically when the provided \(\alpha\) is subject to disturbances. This emphasizes not only the resilience of the algorithm but also its potential for deployment in real-world situations where data is seldom perfect.

## 7 Conclusion

In this work, we formulated the problem of _Vaguely Pairwise Imitation Learning (VPIL)_, in which mixed expert and non-expert demonstrations are present, and the data collector only provides vague pairwise information of demonstrations. To solve this problem, we proposed two learning paradigms, with risk rewriting and mixture proportion estimations (MPE), to recover the expert distribution with the known expert ratio \(\alpha\) and unknown one respectively. Afterward, we showed that these paradigms can be integrated with off-the-shelf IL methods, such as GAIL, to form the algorithm COMParative Imitation LEearning with Risk rewriting (COMPILER) and that by Estimation (COMPILER-E) to solve the VPIL problem with known and unknown \(\alpha\) respectively. The experimental results showed that our methods outperformed standard and preference-based IL methods on a variety of tasks. In the future, we hope to use our algorithms to address more challenging problems, such as VPIL with multiple and noisy annotators.

Figure 4: The performance of COMPILER under different noise levels.

Figure 3: (a, b) The comparisons of overestimated (\(\frac{2\beta}{1+\beta}\)) and underestimated (\(\frac{1-\beta}{1+\beta}\)) methods on various tasks with BBE and KM estimators. (c,d) The distribution map of thrown data ratio using estimated \(\widehat{\alpha}\) and true \(\alpha\). The thrown data ratio difference on \(\Gamma^{+}\) is \((1-\widehat{\alpha})^{2}-(1-\alpha)^{2}\), while that on \(\Gamma^{-}\) is \(\widehat{\alpha}^{2}-\alpha^{2}\). The range marked by the red box is considered in our experiments, in which the expert ratio is smaller than (or equal to) the non-expert one.

## Acknowledgments

This research was supported by the Institute for AI and Beyond, UTokyo; JST SPRING, Grant Number JPMJSP2108. The authors would like to thank Johannes Ackermann and the anonymous reviewers for their insightful comments and suggestions.

## References

* [1] Jonathan Ho and Stefano Ermon. Generative adversarial imitation learning. In _Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain_, pages 4565-4573, 2016.
* [2] Zi-Xuan Chen, Xin-Qiang Cai, Yuan Jiang, and Zhi-Hua Zhou. Anomaly guided policy learning from imperfect demonstrations. In _21st International Conference on Autonomous Agents and Multiagent Systems, AAMAS 2022, Auckland, New Zealand, May 9-13, 2022_, pages 244-252.
* [3] Xin-Qiang Cai, Yao-Xiang Ding, Zi-Xuan Chen, Yuan Jiang, Masashi Sugiyama, and Zhi-Hua Zhou. Seeing differently, acting similarly: Heterogeneously observable imitation learning. In _The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023_. OpenReview.net, 2023.
* November 1, 2019, Proceedings_, volume 100 of _Proceedings of Machine Learning Research_, pages 66-75. PMLR, 2019.
* [5] Xin-Qiang Cai, Yao-Xiang Ding, Yuan Jiang, and Zhi-Hua Zhou. Imitation learning from pixel-level demonstrations by hashreward. In _AAMAS '21: 20th International Conference on Autonomous Agents and Multiagent Systems, Virtual Event, United Kingdom, May 3-7, 2021_, pages 279-287. ACM, 2021.
* [6] Yueh-Hua Wu, Nontawat Charoenphakdee, Han Bao, Voot Tangkaratt, and Masashi Sugiyama. Imitation learning from imperfect demonstration. In _Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA_, volume 97 of _Proceedings of Machine Learning Research_, pages 6818-6827. PMLR, 2019.
* [7] Yunke Wang, Chang Xu, and Bo Du. Robust adversarial imitation learning via adaptively-selected demonstrations. In _Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI 2021, Virtual Event / Montreal, Canada, 19-27 August 2021_, pages 3155-3161. ijcai.org, 2021.
* [8] Mostafa Hussein and Momotaz Begum. Detecting incorrect visual demonstrations for improved policy learning. In Karen Liu, Dana Kulic, and Jeffrey Ichnowski, editors, _Conference on Robot Learning, CoRL 2022, 14-18 December 2022, Auckland, New Zealand_, volume 205 of _Proceedings of Machine Learning Research_, pages 1817-1827. PMLR, 2022.
* [9] Yunke Wang, Bo Du, and Chang Xu. Unlabeled imperfect demonstrations in adversarial imitation learning. In _Thirty-Seventh AAAI Conference on Artificial Intelligence, AAAI 2023, Thirty-Fifth Conference on Innovative Applications of Artificial Intelligence, IAAI 2023, Thirteenth Symposium on Educational Advances in Artificial Intelligence, EAAI 2023, Washington, DC, USA, February 7-14, 2023_, pages 10262-10270. AAAI Press, 2023.
* [10] Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, Carol Chen, Catherine Olsson, Christopher Olah, Danny Hernandez, Dawn Drain, Deep Ganguli, Dustin Li, Eli Tran-Johnson, Ethan Perez, Jamie Kerr, Jared Mueller, Jeffrey Ladish, Joshua Landau, Kamal Mousse, Kamille Lukosiute, Liane Lovitt, Michael Sellitto, Nelson Elhage, Nicholas Schiefer, Noemi Mercado, Nova DasSarma, Robert Lasenby, Robin Larson, Sam Ringer, Scott Johnston, Shauna Kravec, Sheer El Showk, Stanislav Fort, Tamera Lanham, Timothy Telleen-Lawton, Tom Conerly, Tom Henighan, Tristan Hume, Samuel R. Bowman, Zac Hatfield-Dodds, Ben Mann, Dario Amodei, Nicholas Joseph, Sam McCandlish, Tom Brown, and Jared Kaplan. Constitutional AI: harmlessness from AI feedback. _CoRR_, abs/2212.08073, 2022.

* [11] Paul F. Christiano, Jan Leike, Tom B. Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences. In _Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA_, pages 4302-4310, 2017.
* December 9, 2022, New Orleans, Louisiana, USA_, 2022.
* [13] Daniel S. Brown, Wonjoon Goo, Prabhat Nagarajan, and Scott Niekum. Extrapolating beyond suboptimal demonstrations via inverse reinforcement learning from observations. In _Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA_, pages 783-792, 2019.
* [14] Songyuan Zhang, Zhangjie Cao, Dorsa Sadigh, and Yanan Sui. Confidence-aware imitation learning from demonstrations with varying optimality. In _Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual_, pages 12340-12350, 2021.
* [15] Wei Wang and Zhi-Hua Zhou. Crowdsourcing label quality: a theoretical analysis. _Sci. China Inf. Sci._, 58(11):1-12, 2015.
* [16] Gilles Blanchard, Gyemin Lee, and Clayton Scott. Semi-supervised novelty detection. _Journal of Machine Learning Research_, 13:2973-3009, 2010.
* [17] Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. In _2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, IROS 2012, Vilamoura, Algarve, Portugal, October 7-12, 2012_, pages 5026-5033, 2012.
* [18] Stephane Ross, Geoffrey J. Gordon, and Drew Bagnell. A reduction of imitation learning and structured prediction to no-regret online learning. In _Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics, AISTATS 2011, Fort Lauderdale, USA, April 11-13, 2011_, pages 627-635, 2011.
* [19] Borja Ibarz, Jan Leike, Tobias Pohlen, Geoffrey Irving, Shane Legg, and Dario Amodei. Reward learning from human preferences and demonstrations in atari. In _Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montreal, Canada._, pages 8022-8034, 2018.
* [20] Yunke Wang, Chang Xu, Bo Du, and Honglak Lee. Learning to weight imperfect demonstrations. In _Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event_, volume 139 of _Proceedings of Machine Learning Research_, pages 10961-10970. PMLR, 2021.
* [21] Yuanying Cai, Chuheng Zhang, Wei Shen, Xiaonan He, Xuyun Zhang, and Longbo Huang. Imitation learning to outperform demonstrators by directly extrapolating demonstrations. In _Proceedings of the 31st ACM International Conference on Information & Knowledge Management, Atlanta, GA, USA, October 17-21, 2022_, pages 128-137. ACM, 2022.
* [22] Daniel S. Brown, Wonjoon Goo, and Scott Niekum. Better-than-demonstrator imitation learning via automatically-ranked demonstrations. In _Proceedings of the 3rd Conference on Robot Learning_, 2019.
* [23] Masashi Sugiyama, Han Bao, Takashi Ishida, Nan Lu, Tomoya Sakai, and Niu Gang. _Machine Learning from Weak Supervision: An Empirical Risk Minimization Approach_. MIT Press, 2022.
* [24] Takashi Ishida, Gang Niu, and Masashi Sugiyama. Binary classification from positive-confidence data. In _Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, 3-8 December 2018, Montreal, Canada._, pages 5921-5932, 2018.

* [25] Takashi Ishida, Gang Niu, Aditya Krishna Menon, and Masashi Sugiyama. Complementary-label learning for arbitrary losses and models. In _Proceedings of the 36th International Conference on Machine Learning (ICML)_, pages 2971-2980, 2019.
* [26] Nan Lu, Gang Niu, Aditya Krishna Menon, and Masashi Sugiyama. On the minimal supervision for training any binary classifier from only unlabeled data. In _7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019_, 2019.
* [27] Lei Feng, Senlin Shu, Nan Lu, Bo Han, Miao Xu, Gang Niu, Bo An, and Masashi Sugiyama. Pointwise binary classification with pairwise confidence comparisons. In _Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event_, volume 139 of _Proceedings of Machine Learning Research_, pages 3252-3262. PMLR, 2021.
* [28] Harish G. Ramaswamy, Clayton Scott, and Ambui Tewari. Mixture proportion estimation via kernel embeddings of distributions. In _Proceedings of the 33nd International Conference on Machine Learning, ICML 2016, New York City, NY, USA, June 19-24, 2016_, volume 48 of _JMLR Workshop and Conference Proceedings_, pages 2052-2060. JMLR.org, 2016.
* [29] Umar Syed, Michael H. Bowling, and Robert E. Schapire. Apprenticeship learning using linear programming. In _Machine Learning, Proceedings of the Twenty-Fifth International Conference (ICML 2008), Helsinki, Finland, June 5-9, 2008_, volume 307 of _ACM International Conference Proceeding Series_, pages 1032-1039. ACM, 2008.
* [30] Chelsea Finn, Paul F. Christiano, Pieter Abbeel, and Sergey Levine. A connection between generative adversarial networks, inverse reinforcement learning, and energy-based models. _CoRR_, abs/1611.03852, 2016.
* [31] Saurabh Garg, Yifan Wu, Alexander J. Smola, Sivaraman Balakrishnan, and Zachary C. Lipton. Mixture proportion estimation and PU learning: A modern approach. In _Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual_, pages 8532-8544, 2021.
* [32] Yu Yao, Tongliang Liu, Bo Han, Mingming Gong, Gang Niu, Masashi Sugiyama, and Dacheng Tao. Rethinking class-prior estimation for positive-unlabeled learning. In _The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022_. OpenReview.net, 2022.
* [33] Yu Yao, Tongliang Liu, Bo Han, Mingming Gong, Gang Niu, Masashi Sugiyama, and Dacheng Tao. Rethinking class-prior estimation for positive-unlabeled learning. In _10th International Conference on Learning Representations, ICLR 2022, virtual, April 25-29, 2022_. OpenReview.net, 2022.
* [34] Nan Lu, Gang Niu, Aditya Krishna Menon, and Masashi Sugiyama. On the minimal supervision for training any binary classifier from only unlabeled data. In _7th International Conference on Learning Representations, ICLR 2019, New Orleans, Louisiana, US, May 6-9, 2019_, 2019.
* [35] Yu-Jie Zhang, Peng Zhao, Lijun zhang, and Zhi-Hua Zhou. An unbiased risk estimator for learning with augmented classes. In _Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual_, pages 10247-10258, 2020.
* [36] Tongtong Fang, Nan Lu, Gang Niu, and Masashi Sugiyama. Rethinking importance weighting for deep learning under distribution shift. In _Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual_, 2020.
* May 3, 2018_. OpenReview.net, 2018.
* [38] Ryuichi Kiryo, Gang Niu, Marthinus Christoffel du Plessis, and Masashi Sugiyama. Positive-unlabeled learning with non-negative risk estimator. In _Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, NIPS 2017, December 4-9, 2017, Long Beach, CA, USA_, pages 1675-1685, 2017.
* [39] Nan Lu, Tianyi Zhang, Gang Niu, and Masashi Sugiyama. Mitigating overfitting in supervised classification from two unlabeled datasets: A consistent risk correction approach. In _The 23rd International Conference on Artificial Intelligence and Statistics (AISTATS)_, pages 1115-1125, 2020.
* [40] Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. _Foundations of Machine Learning_. The MIT press, 2012.
* [41] Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. In _4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016_, 2016.
* [42] Michael Bain and Claude Sammut. A framework for behavioural cloning. In _Machine Intelligence 15, Intelligent Agents [St. Catherine's College, Oxford, UK, July 1995]_, pages 103-129. Oxford University Press, 1995.
* [43] Justin Fu, Katie Luo, and Sergey Levine. Learning robust rewards with adverserial inverse reinforcement learning. In _International Conference on Learning Representations_, 2018.
* [44] Letian Chen, Rohan R. Paleja, and Matthew C. Gombolay. Learning from suboptimal demonstration via self-supervised reward regression. In _4th Conference on Robot Learning, CoRL 2020, 16-18 November 2020, Virtual Event / Cambridge, MA, USA_, volume 155 of _Proceedings of Machine Learning Research_, pages 1262-1277. PMLR, 2020.
* [45] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. _CoRR_, abs/1707.06347, 2017.
* [46] Prafulla Dhariwal, Christopher Hesse, Oleg Klimov, Alex Nichol, Matthias Plappert, Alec Radford, John Schulman, Szymon Sidor, Yuhuai Wu, and Peter Zhokhov. Openai baselines, 2017.
* From Theory to Algorithms_. Cambridge University Press, 2014.
Proof for the Theorems

In this section, we provide the proof for Theorem 2, Proposition 1, and Theorem 3 in the main body of the paper.

**Theorem 2**.: _Assume the pairwise datasets \((\Gamma^{+},\Gamma^{-})\) are generated following the procedure in Section 3.2 in the main paper. Let \(p_{\pi_{+}}(s,a)=\mathbb{E}_{\Gamma^{+}}\left[\widehat{p}_{\pi_{+}}(s,a)\right]\) and \(p_{\pi_{-}}(s,a)=\mathbb{E}_{\Gamma^{-}}\left[\widehat{p}_{\pi_{-}}(s,a)\right]\) be the expected occupancy measures of \(\Gamma^{+}\) and \(\Gamma^{-}\), where the randomness is taken over the draws of \(\Gamma^{+}\) and \(\Gamma^{-}\). Then, we have_

\[\begin{cases}p_{\pi_{+}}(s,a)=&\left(2\alpha-\alpha^{2}\right)p_{\pi_{\text{E} }}(s,a)+(1-\alpha)^{2}p_{\pi_{\text{E}}}(s,a),\\ p_{\pi_{-}}(s,a)=&\alpha^{2}p_{\pi_{\text{E}}}(s,a)+(1-\alpha^{2})p_{\pi_{ \text{E}}}(s,a),\end{cases}\] (9)

\[\begin{cases}p_{\pi_{\text{E}}}(s,a)=&\frac{1+\alpha}{2\alpha}p_{\pi_{+}}(s,a )-\frac{1-\alpha}{2\alpha}p_{\pi_{-}}(s,a),\\ p_{\pi_{\text{E}}}(s,a)=&-\frac{\alpha}{2(1-\alpha)}p_{\pi_{+}}(s,a)+\frac{2- \alpha}{2-2\alpha}p_{\pi_{-}}(s,a).\end{cases}\] (10)

Proof.: We first prove the first line of (9). Considering the data collection process introduced in Section 3.2, where a pair of trajectories \((\tau^{(1)},\tau^{(2)})\) is independently sampled from the demonstrator pool. When one trajectory enjoys a better quality than another, the collector would put the better one into \(\Gamma^{+}\). Otherwise, the two trajectories will be randomly allocated. Let \(\widehat{p}_{i}^{+}(s,a)=(1-\gamma)\sum_{t=0}^{\infty}\gamma^{t}\operatorname{ \mathbb{I}}\left[(s_{t,i},a_{t,i})=(s,a)\right]\) be the empirical occupancy measure for the trajectory \(\tau_{i}\in\Gamma^{+}\). Then, further define the even \(E_{1}=\{\tau_{i}\text{ is sampled from }\pi_{\text{E}}\}\) and event \(E_{1}=\{\tau_{i}\text{ is sampled from }\pi_{\text{E}}\}\). We have

\[\mathbb{E}[\widehat{p}_{i}^{+}(s,a)] =\text{Pr}[E_{1}]\mathbb{E}_{\Gamma^{+}}\left[\widehat{p}_{i}^{+ }(s,a)\mid E_{1}\right]+\text{Pr}[E_{2}]\mathbb{E}_{\Gamma^{+}}\left[\widehat {p}_{i}^{+}(s,a)\mid E_{2}\right]\] \[=\text{Pr}[E_{1}]p_{\pi_{\text{E}}}+\text{Pr}[E_{2}]p_{\pi_{ \text{E}}}.\] (11)

The second equality is due to \(\tau_{i}\) is i.i.d. sampled from \(p_{\pi_{\text{E}}}\) under event \(E_{1}\) and is i.i.d. sampled from \(p_{\pi_{-}}\) under event \(E_{2}\). Then, let the even \(A_{1}=\{\text{both }\tau^{(1)},\tau^{(2)}\) are sampled from \(\pi_{\text{E}}\}\), \(A_{2}=\{\text{both }\tau^{(1)},\tau^{(2)}\) are sampled from \(\pi_{\text{NE}}\}\) and \(A_{3}=\{\text{other situation}\}\). Then, we have

\[\text{Pr}[E_{1}] =\text{Pr}[A_{1}]\text{Pr}[E_{1}\mid A_{1}]+\text{Pr}[A_{2}] \text{Pr}[E_{1}\mid A_{2}]+\text{Pr}[A_{3}]\text{Pr}[E_{1}\mid A_{3}]\] \[=\alpha^{2}+2\alpha(1-\alpha)=2\alpha-\alpha^{2},\]

where the second inequality is due to \(\text{Pr}[E_{1}\mid A_{1}]=\text{Pr}[E_{3}\mid A_{3}]=1\) and \(\text{Pr}[E_{1}\mid A_{2}]=0\) according to the data collection process. We can also show the probability of event \(E_{2}\) is \(\text{Pr}[E_{1}]=(1-\alpha)^{2}\), then we have

\[\mathbb{E}[\widehat{p}_{i}^{+}(s,a)]=\left(2\alpha-\alpha^{2}\right)p_{\pi_{ \text{E}}}(s,a)+(1-\alpha)^{2}p_{\pi_{\text{NE}}}(s,a).\]

Since \(\widehat{p}_{\pi_{+}}(s,a)=\sum_{i=1}^{n_{+}}\widehat{p}_{i}^{+}(s,a)/n_{+}\), we have \(\mathbb{E}[\widehat{p}_{\pi_{+}}(s,a)]=\sum_{i=1}^{n_{+}}\mathbb{E}[\widehat{p }_{i}^{+}(s,a)]/n_{+}=\left(2\alpha-\alpha^{2}\right)p_{\pi_{\text{E}}}(s,a) +(1-\alpha)^{2}p_{\pi_{\text{NE}}}(s,a)\), which completes the proof for the first equality of (9). The second equality of (9) can be proved following a similar arguments

**Proposition 2**.: _Suppose the distributions \(p_{\pi_{\text{E}}}\) and \(p_{\pi_{\text{NE}}}\) are mutually irreducible such that there exists no decomposition of the form \(p_{\pi_{\text{E}}}=(1-\eta)Q+\eta p_{\pi_{\text{NE}}}\) and \(p_{\pi_{\text{NE}}}=(1-\eta^{\prime})Q^{\prime}+\eta^{\prime}p_{\pi_{\text{E}}}\) for any probability distributions \(Q,Q^{\prime}\) and scalars \(\eta,\eta^{\prime}\in(0,1]\). Then, the true mixture proportions \(\beta_{1}\) and \(\beta_{2}\) are unique and can be identified by_

\[\begin{cases}&\beta_{1}=\sup\{\eta|p_{\pi_{+}}=\eta p_{\pi_{-}}+(1-\eta)K,K\in \mathcal{C}\},\\ &\beta_{2}=\sup\{\eta^{\prime}|p_{\pi_{+}}=\eta^{\prime}p_{\pi_{+}}+(1-\eta^{ \prime})K^{\prime},K^{\prime}\in\mathcal{C}\},\end{cases}\] (12)

_where \(\mathcal{C}\) is the set containing all possible probability distributions. Thus, \(\alpha\) is identifiable by_

\[\alpha=(1-\beta_{1})/(1+\beta_{1})\quad\text{or }\quad\alpha=2\beta_{2}/(1+\beta_{2}).\] (13)

Proof.: We first prove the first line of (12). Since \(p_{\pi_{\text{E}}}\) is irreducible w.r.t. \(p_{\pi_{\text{NE}}}\), such that there exists no decomposition \(p_{\pi_{\text{E}}}=(1-\eta)Q+\eta p_{\pi_{\text{NE}}}\) for any \(\eta\in(0,1]\) and distribution \(Q\), we have \(p_{\pi_{\text{E}}}\) is also irreducible w.r.t. \(p_{\pi_{-}}\), such that \(p_{\pi_{\text{E}}}\) can not be rewritten as a mixture \(p_{\pi_{\text{E}}}=(1-\eta^{\prime})Q^{\prime}+\eta^{\prime}p_{\pi_{-}}\) for any distribution \(Q^{\prime}\) and \(\eta^{\prime}\in(0,1]\). Then, according to the relationship (5) of the main paper and Proposition 5 of [16], we can show \(\beta_{1}\) is identifiable and \(\beta_{1}=\sup\{\eta|p_{\pi_{+}}=\eta p_{\pi_{-}}+(1-\eta)K,K\in\mathcal{C}\}\). We can prove the second line of (12) by a similar argument. Then, combined with \(\beta_{1}=\frac{1-\alpha}{1+\alpha}\) and \(\beta_{2}=\frac{\alpha}{2-\alpha}\) in (5) of the main paper, we can obtain identifiable \(\alpha\) estimation in (13).

**Theorem 3**.: _Let \(\mathcal{W}\) be a parameter space for training the discriminator and \(D_{\mathcal{W}}=\{D_{w}\mid w\in\mathcal{W}\}\) be the hypothesis space. Assume the functions \(|\!\log D_{w}(s,a)|\leq B\) and \(|\!\log(1-D_{w}(s,a))|\leq B\) are upper-bounded for any state-action pair \((s,a)\in\mathcal{S}\times\mathcal{A}\) and \(w\in\mathcal{W}\). Further assume both the functions \(\log D_{w}(s,a)\) and \(\log(1-D_{w}(s,a))\) are \(L\)-Lipschitz continuous in the state-action space. For a fixed policy \(\pi_{\theta}\), let \(\Gamma^{\theta}=\{\tau_{i}^{\theta}\}_{i=1}^{n_{\theta}}\) be trajectories generated from \(\pi_{\theta}\). Then, for any \(\delta\in(0,1)\), with probability at least \(1-\delta\), we have_

\[V(\pi_{\theta},D_{w}^{*})-V(\pi_{\theta},\widehat{D}_{w}) \leq 4L(1+\alpha)\mathcal{R}_{n_{+}}(D_{\mathcal{W}})+4L(1-\alpha )\mathcal{R}_{n_{-}}(D_{\mathcal{W}}))\] \[+8L\mathcal{R}_{n_{\theta}}(D_{\mathcal{W}})+C(\delta)\left( \frac{1}{\sqrt{n_{\theta}}}+\frac{1}{\sqrt{n_{+}}}+\frac{1}{\sqrt{n_{-}}} \right),\]

_where \(\widehat{D}_{w}=\arg\max_{w\in\mathcal{W}}\widehat{V}(\pi_{\theta},D_{w})\) and \(D_{w}^{*}=\arg\max_{w\in\mathcal{W}}V(\pi_{\theta},D_{w})\). The constants \(n_{+}\) and \(n_{-}\) are the number of trajectories in \(\Gamma^{+}\) and \(\Gamma^{-}\). We define \(C(\delta)=4B\sqrt{\log(6/\delta)}\). The empirical Radamacher complexities [40] on datasets \(\Gamma^{+}\), \(\Gamma^{-}\), and \(\Gamma^{\theta}\) are denoted by \(\mathcal{R}_{n_{+}}\), \(\mathcal{R}_{n_{-}}\), and \(\mathcal{R}_{n_{\theta}}\)._

Proof.: Since \(\widehat{D}_{w}\) and \(D_{w}^{*}\) are the maximizer of the objective functions \(\widehat{V}(\pi_{\theta},D_{w})\) and \(V(\pi_{\theta},D_{w})\), respectively.

\[V(\pi_{\theta},D_{w}^{*})-V(\pi_{\theta},\widehat{D}_{w}) =V(\pi_{\theta},D_{w}^{*})-\widehat{V}(\pi_{\theta},D_{w}^{*})+ \widehat{V}(\pi_{\theta},D_{w}^{*})-\widehat{V}(\pi_{\theta},\widehat{D}_{w})\] (14) \[\quad+\widehat{V}(\pi_{\theta},\widehat{D}_{w})-V(\pi_{\theta}, \widehat{D}_{w})\] \[\leq V(\pi_{\theta},D_{w}^{*})-\widehat{V}(\pi_{\theta},D_{w}^{*})+ \widehat{V}(\pi_{\theta},\widehat{D}_{w})-V(\pi_{\theta},\widehat{D}_{w})\] \[\leq 2\sup_{w\in\mathcal{W}}\lvert V(\pi_{\theta},D_{w})-\widehat{V }(\pi_{\theta},D_{w})\rvert,\] (15)

where the inequality is due to the optimality of \(\widehat{D}_{w}\). The according to the definition of \(V(\pi_{\theta},D_{w})\) and \(\widehat{V}(\pi_{\theta},D_{w})\),

\[V(\pi_{\theta},D_{w})=2\alpha\mathbb{E}_{(s,a)\sim p_{\pi_{ \theta}}}[\log D_{w}(s,a)] +(1-\alpha)\mathbb{E}_{(s,a)\sim p_{\pi_{-}}}[\log(D_{w}(s,a))]\] (16) \[+(1+\alpha)\mathbb{E}_{(s,a)\sim p_{\pi_{+}}}[\log(1-D_{w}(s,a))]\]

and

\[\widehat{V}(\pi_{\theta},D_{w})=2\alpha\mathbb{E}_{(s,a)\sim \widehat{p}_{\pi_{\theta}}}[\log D_{w}(s,a)] +(1-\alpha)\mathbb{E}_{(s,a)\sim\widehat{p}_{\pi_{-}}}[\log(D_{w}( s,a))]\] (17) \[+(1+\alpha)\mathbb{E}_{(s,a)\sim\widehat{p}_{\pi_{+}}}[\log(1-D_{ w}(s,a))],\]

where \(\widehat{p}_{\pi_{\theta}}(s,a)=\frac{1}{n_{\theta}}\sum_{i=1}^{n_{\theta}} \widehat{p}_{i}^{\theta}(s,a)\) and \(\widehat{p}_{i}^{\theta}(s,a)=(1-\gamma)\sum_{t=0}^{\infty}\gamma^{t}\mathds{1 }\left[(s_{t,i},a_{t,i})=(s,a)\right]\) is the empirical occupancy measure for the trajectory \(\tau_{i}\) sampled from \(\pi_{\theta}\)The empirical occupancy measure is unbiased w.r.t. \(\pi_{\theta}\) such that \(\mathbb{E}[\widehat{p}_{i}^{\theta}(s,a)]=p_{\pi_{\theta}}(s,a)\). Then, we can decompose the R.H.S. of (15) as

\[\sup_{w\in\mathcal{W}}\lvert V(\pi_{\theta},D_{w})-\widehat{V}(\pi_{\theta},D_{ w})\rvert=\texttt{term (a)}+\texttt{term (b)}+\texttt{term (c)}.\]

In above, the first term

\[\texttt{term (a)} =2\alpha\left\lvert\mathbb{E}_{(s,a)\sim\widehat{p}_{\pi_{\theta}}} [\log D_{w}(s,a)]-\mathbb{E}_{(s,a)\sim p_{\pi_{\theta}}}[\log D_{w}(s,a)]\right\rvert\] \[=2\alpha\left\lvert\frac{1}{n_{\theta}}\sum_{i=1}^{n_{\theta}} \mathbb{E}_{(s,a)\sim\widehat{p}_{i}^{\theta}}[\log D_{w}(s,a)]-\mathbb{E}_{(s,a )\sim p_{\pi_{\theta}}}[\log D_{w}(s,a)]\right\rvert\]

measures the generalization gap between \(\widehat{p}_{\pi_{\theta}}\) and \(p_{\pi_{\theta}}\). The second term

\[\texttt{term (b)} =(1-\alpha)\left\lvert\frac{1}{n_{-}}\sum_{i=1}^{n_{-}}\mathbb{E }_{(s,a)\sim\widehat{p}_{i}^{-}}[\log D_{w}(s,a)]-\mathbb{E}_{(s,a)\sim p_{\pi_{ -}}}[\log D_{w}(s,a)]\right\rvert\]

measures the generalization gap between \(\widehat{p}_{\pi_{-}}\) and \(p_{\pi_{-}}\) and the last term

\[\texttt{term (c)} =(1+\alpha)\left\lvert\frac{1}{n_{+}}\sum_{i=1}^{n_{-}}\mathbb{E }_{(s,a)\sim\widehat{p}_{i}^{+}}[\log(1-D_{w}(s,a))]-\mathbb{E}_{(s,a)\sim p_{ \pi_{+}}}[\log(1-D_{w}(s,a))]\right\rvert\]measures the gap between \(\widehat{p}_{\pi_{+}}\) and \(p_{\pi_{+}}\).

Under the condition that \(|\log D_{w}(s,a)|\leq B\) for any \((s,a)\in\mathcal{S}\times\mathcal{A}\) and \(w\in\mathcal{W}\), the standard analysis generalization analysis with Rademacher complexity shows (e.g. Theorem 26.5 of [47]), for all \(w\in\mathcal{W}\), we have

\[\texttt{term (a)}\leq 8\alpha\mathcal{R}_{n_{\theta}}(\log\circ D_{ \mathcal{W}})+4\alpha B\sqrt{\frac{2\ln(2/\delta^{\prime})}{n_{\theta}}},\]

with probability at least \(1-\delta^{\prime}\), where \(\mathcal{R}_{n_{\theta}}\) is the empirical Rademacher complexity. Then, according to the Talagrand's contraction inequality (Lemma 26.9 of [47]), we have

\[\mathcal{R}_{n_{\theta}}(\log\circ D_{\mathcal{W}})\leq L\mathcal{R}_{n_{ \theta}}(D_{\mathcal{W}}),\]

Then, we obtain

\[\texttt{term (a)}\leq 8\alpha L\mathcal{R}_{n_{\theta}}(D_{ \mathcal{W}})+4\alpha B\sqrt{\frac{2\ln(2/\delta^{\prime})}{n_{\theta}}}.\] (18)

Since \(|\log(1-D_{w})|\leq B\) and \(\log(1-D_{w})\) is \(L\)-Lipschitz continuous, the a similar arguments ensures,

\[\texttt{term (b)}\leq 4(1-\alpha)L\mathcal{R}_{n_{-}}(D_{ \mathcal{W}})+2(1-\alpha)B\sqrt{\frac{2\ln(2/\delta^{\prime})}{n_{-}}}.\] (19)

and

\[\texttt{term (c)}\leq 4(1+\alpha)\mathcal{R}_{n_{+}}(D_{ \mathcal{W}})+2(1+\alpha)B\sqrt{\frac{2\ln(2/\delta^{\prime})}{n_{+}}},\] (20)

with probability at least \(1-\delta^{\prime}\). Let \(\delta^{\prime}=\delta/3\), we complete the proof by combining (18), (19), and (20) with (15).

## Appendix B Full Results of \(\alpha\) Estimations

In the main body of the paper, we only reported the effect of overestimation and underestimation on HalfCheetah environment due to the space limitation. Here we provide the full results on four environments in our experiments, as shown in Figure 5. The experimental results demonstrated the same conclusion as in the main body of the paper, that the underestimation method is better than the overestimation one.

## 6 Conclusion

Figure 5: The comparisons of overestimated (\(\frac{2\beta}{1+\beta}\)) and underestimated (\(\frac{1-\beta}{1+\beta}\)) methods on various tasks with BBE and KM estimators.