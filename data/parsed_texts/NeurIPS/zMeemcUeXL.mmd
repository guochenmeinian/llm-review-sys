# FAMO: Fast Adaptive Multitask Optimization

 Bo Liu, Yihao Feng, Peter Stone, Qiang Liu

The University of Texas at Austin, Salesforce AI Research, Sony AI

{bliu, pstone, lqiang}@cs.utexas.edu, yihaof@salesforce.com

###### Abstract

One of the grand enduring goals of AI is to create generalist agents that can learn multiple different tasks from diverse data via multitask learning (MTL). However, in practice, applying gradient descent (GD) on the average loss across all tasks may yield poor multitask performance due to severe under-optimization of certain tasks. Previous approaches that manipulate task gradients for a more balanced loss decrease require storing and computing all task gradients (\(\mathcal{O}(k)\) space and time where \(k\) is the number of tasks), limiting their use in large-scale scenarios. In this work, we introduce Fast Adaptive Multitask Optimization (FAMO), a dynamic weighting method that decreases task losses in a balanced way using \(\mathcal{O}(1)\) space and time. We conduct an extensive set of experiments covering multi-task supervised and reinforcement learning problems. Our results indicate that FAMO achieves comparable or superior performance to state-of-the-art gradient manipulation techniques while offering significant improvements in space and computational efficiency. Code is available at https://github.com/Cranial-XIX/FAMO.

## 1 Introduction

Large models trained on diverse data have advanced both computer vision [20] and natural language processing [4], paving the way for generalist agents capable of multitask learning (MTL) [5]. Given the substantial size of these models, it is crucial to design MTL methods that are _effective_ in terms of task performance and _efficient_ in terms of space and time complexities for managing training costs and environmental impacts. This work explores such methods through the lens of optimization.

Perhaps the most intuitive way of solving an MTL problem is to optimize the average loss across all tasks. However, in practice, doing so can lead to models with poor multitask performance: a subset of tasks are _severely under-optimized_. A major reason behind such optimization failure is that a subset of tasks are under-optimized because the average gradient constantly results in small (or even negative) progress on these tasks (see details in Section 2).

To mitigate this problem, gradient manipulation methods [43, 25, 7, 24] compute a new update vector in place of the gradient to the average loss, such that all task losses decrease in a more balanced way. The new update vector is often determined by solving an additional optimization problem that involves all task gradients. While these approaches exhibit improved performance, they become computationally expensive when the number of tasks and the model size are large [41]. This is because they require computing and storing all task gradients at each iteration, thus demanding \(\mathcal{O}(k)\) space and time complexities, not to mention the overhead introduced by solving the additional optimization problem. In contrast, the average gradient can be efficiently computed in \(\mathcal{O}(1)\) space and time per iteration because one can first average the task losses and then take the gradient of the average loss.1 To this end, we ask the following question:

Footnote 1: Here, we refer to the situation where a single data \(x\) can be used to compute all task losses.

* _Is it possible to design a multi-task learning optimizer that ensures a balanced reduction in losses across all tasks while utilizing_ \(\mathcal{O}(1)\) _space and time per iteration?_

In this work, we present Fast Adaptive Multitask Optimization (FAMO), a simple yet effective adaptive task weighting method to address the above question. On the one hand, FAMO is designed to ensure that all tasks are optimized with approximately similar progress. On the other hand, FAMO leverages the loss history to update the task weighting, hence bypassing the necessity of computing all task gradients. To summarize, our contributions are:

1. We introduce FAMO, an MTL optimizer that decreases task losses approximately at _equal rates_ while using only \(\mathcal{O}(1)\) space and time per iteration.
2. We demonstrate that FAMO performs comparably to or better than existing gradient manipulation methods on a wide range of standard MTL benchmarks, in terms of standard MTL metrics, while being significantly computationally cheaper.

## 2 Background

In this section, we provide the formal definition of multitask learning, then discuss its optimization challenge, and provide a brief overview of the gradient manipulation methods.

Multitask Learning (MTL)MTL considers optimizing a _single_ model with parameter \(\theta\in\mathbb{R}^{m}\) that can perform \(k\geq 2\) tasks well, where each task is associated with a loss function \(\ell_{i}(\theta):\mathbb{R}^{m}\rightarrow\mathbb{R}_{\geq 0}\).2 Then, it is common to optimize the average loss across all tasks:

Footnote 2: In this work, we assume \(\forall\ i,\ \ell_{i}(\theta)\geq 0\), which is true for typical loss functions including mean square and cross-entropy losses. Note that one can always transform \(\ell_{i}\) to be non-negative if a loss lower bound is known.

\[\min_{\theta\in\mathbb{R}^{m}}\left\{\ell_{0}(\theta):=\frac{1}{k}\sum_{i=1} ^{k}\ell_{i}(\theta)\right\}.\] (1)

Optimization ChallengeDirectly optimizing (1) can result in severe under-optimization of a subset of tasks. A major reason behind this optimization challenge is the "generalized" conflicting gradient phenomenon, which we explain in the following. At any time step \(t\), assume one updates the model

Figure 1: **Top left:** The loss landscape, and individual task losses of a toy 2-task learning problem (\(\star\) represents the minimum of task losses). **Top right:** the runtime of different MTL methods for 50000 steps. **Bottom:** the loss trajectories of different MTL methods. Adam fails in 1 out of 5 runs to reach the Pareto front due to CG. FAMO decreases task losses in a balanced way and is the only method matching the \(\mathcal{O}(1)\) space/time complexity of Adam. Experimental details and analysis are provided in Section 5.1.

parameter using a gradient descent style iterative update: \(\theta_{t+1}=\theta_{t}-\alpha d_{t}\) where \(\alpha\) is the step size and \(d_{t}\) is the update at time \(t\). Then, we say that conflicting gradients (CG) [24; 43] happens if

\[\exists i,\ \ \ell_{i}(\theta_{t+1})-\ell_{i}(\theta_{t})\approx-\alpha\nabla \ell_{i}(\theta_{t})^{\top}d_{t}>0.\]

In other words, certain task's loss is increasing. CG often occurs during optimization and is not inherently detrimental. However, it becomes undesirable when a subset of tasks persistently undergoes under-optimization due to CG. In a more general sense, it is not desirable if a subset of tasks has much slower learning progress compared to the rest of the tasks (even if all task losses are decreasing). This very phenomenon, which we call the "generalized" conflicting gradient, has spurred previous research to mitigate it at each optimization stage [43].

Gradient Manipulation MethodsGradient manipulation methods aim to decrease all task losses in a more balanced way by finding a new update \(d_{t}\) at each step. \(d_{t}\) is usually a convex combination of task gradients, and therefore the name gradient manipulation (denote \(\nabla\ell_{i,t}=\nabla_{\theta}\ell_{i}(\theta_{t})\) for short):

\[d_{t}=\begin{bmatrix}\nabla\ell_{1,t}^{\top}\\ \vdots\\ \nabla\ell_{k,t}^{\top}\end{bmatrix}^{\top}w_{t},\ \ \ \text{where}\ \ w_{t}= \begin{bmatrix}w_{1,t}\\ \vdots\\ w_{k,t}\end{bmatrix}=f\big{(}\nabla\ell_{1,t},\dots,\nabla\ell_{k,t}\big{)}\in \mathbb{S}_{k}.\] (2)

Here, \(\mathbb{S}_{k}=\{w\in\mathbb{R}_{\geq 0}^{k}\mid w^{\top}\mathbf{1}=1\}\) is the probabilistic simplex, and \(\bm{w}_{t}\) is the task weighting across all tasks. Please refer to Appendix A for details of five state-of-the-art gradient manipulation methods (MGDA, PCGrad, CAGGrad, IMTL-G, NashMTL) and their corresponding \(f\). Note that existing gradient manipulation methods require computing and storing \(k\) task gradients before applying \(f\) to compute \(d_{t}\), which often involves solving an additional optimization problem. As a result, we say these methods require at least \(\mathcal{O}(k)\) space and time complexity, which makes them slow and memory inefficient when \(k\) and model size \(m\) are large.

## 3 Fast Adaptive Multitask Optimization (FAMO)

In this section, we introduce FAMO that addresses question \(Q\), which involves two main ideas:

1. At each step, decrease all task losses at _an equal rate_ as much as possible (Section 3.1).
2. Amortize the computation in 1. over time (Section 3.2).

### Balanced Rate of Loss Improvement

At time \(t\), assume we perform the update \(\theta_{t+1}=\theta_{t}-\alpha d_{t}\), we define the rate of improvement for task \(i\) as

\[r_{i}(\alpha,d_{t})=\frac{\ell_{i,t}-\ell_{i,t+1}}{\ell_{i,t}}.\lx@note{ footnote}{To avoid division by zero, in practice we add a small constant (e.g., $1e-8$) to all losses. For the ease of notation (e.g., $\ell_{i}(\cdot)\leftarrow\ell_{i}(\cdot)+1e-8$, we omit it throughout the paper.}\] (3)

FAMO then seeks an update \(d_{t}\) that results in the largest _worst-case improvement rate_ across all tasks (\(\frac{1}{2}\big{\|}d_{t}\big{\|}\) is subtracted to prevent an under-specified optimization problem where the objective can be infinitely large):

\[\max_{d_{t}\in\mathbb{R}^{m}}\min_{i\in\{k\}}\frac{1}{\alpha}r_{i}(\alpha,d_{t })-\frac{1}{2}\big{\|}d_{t}\big{\|}^{2}.\] (4)

When the step size \(\alpha\) is small, using Taylor approximation, the problem (4) can be approximated by

\[\max_{d_{t}\in\mathbb{R}^{m}}\min_{i\in\{K\}}\frac{\nabla\ell_{i,t}^{\top}d_{ t}}{\ell_{i,t}}-\frac{1}{2}\big{\|}d_{t}\big{\|}^{2}=\big{(}\nabla\log\ell_{i,t} \big{)}^{\top}d_{t}-\frac{1}{2}\big{\|}d_{t}\big{\|}^{2}.\] (5)

Instead of solving the primal problem in (5) where \(d\in\mathbb{R}^{m}\) (\(m\) can be millions if \(\theta\) is the parameter of a neural network), we consider its dual problem:

**Proposition 3.1**.: _The dual objective of (5) is_

\[z_{t}^{*}\in\operatorname*{arg\,min}_{z\in\mathbb{S}_{k}}\frac{1}{2}\big{\|}J _{t}z\big{\|}^{2},\ \ \text{where}\ \ J_{t}=\begin{bmatrix}\nabla\log\ell_{1,t}^{\top}\\ \vdots\\ \nabla\log\ell_{k,t}^{\top}\end{bmatrix},\] (6)

_where \(z_{t}^{*}=\big{[}z_{t,i}^{*}\big{]}\) is the optimal combination weights of the gradients, and the optimal update direction is \(d_{t}^{*}=J_{t}z_{t}^{*}\)._

Proof.: \[\max_{d\in\mathbb{R}^{m}}\min_{i\in\{k\}}\bigl{(}\nabla\log\ell_ {i,t}\big{)}^{\top}d-\frac{1}{2}\big{\|}d\big{\|}^{2}\] \[=\max_{d\in\mathbb{R}^{m}}\min_{z\in\mathbb{S}_{k}}\bigl{(}\sum_{i =1}^{k}z_{i}\nabla\log\ell_{i,t}\big{)}^{\top}d-\frac{1}{2}\big{\|}d\big{\|}^{2}\] \[=\min_{z\in\mathbb{S}_{k}}\max_{d\in\mathbb{R}^{m}}\bigl{(}\sum_{i =1}^{k}z_{i}\nabla\log\ell_{i,t}\big{)}^{\top}d-\frac{1}{2}\big{\|}d\big{\|}^{2 }\qquad\text{(strong duality)}\]

Write \(g(d,z)=\big{(}\sum_{i=1}^{k}z_{i}\nabla\log\ell_{i,t}\big{)}^{\top}d-\frac{1} {2}\big{\|}d\big{\|}^{2}\), then by setting

\[\frac{\partial g}{\partial d}=0\quad\Longrightarrow\quad d^{*}=\sum_{i=1}^{k} z_{i}\nabla\log\ell_{i,t}.\]

Plugging in \(d^{*}\) back, we have

\[\max_{d\in\mathbb{R}^{m}}\min_{i\in\{k\}}\bigl{(}\nabla\log\ell_{i,t}\big{)}^ {\top}d-\frac{1}{2}\big{\|}d\big{\|}^{2}=\min_{z\in\mathbb{S}_{k}}\frac{1}{2} \bigg{\|}\sum_{i=1}^{k}z_{i}\nabla\log\ell_{i,t}\bigg{\|}^{2}=\min_{z\in \mathbb{S}_{k}}\frac{1}{2}\big{\|}J_{t}z\big{\|}^{2}.\]

At the optimum, we have \(d_{t}^{*}=J_{t}z_{t}^{*}\). 

The dual problem in (6) can be viewed as optimizing the log objective of the multiple gradient descent algorithm (MGDA) [9, 35]. Similar to MGDA, (6) only involves a decision variable of dimension \(k\ll m\). Furthermore, if the optimal combination weights \(z_{t}^{*}\) is an interior point of \(\mathbb{S}_{k}\), then the improvement rates \(r_{i}(\alpha,d_{t}^{*})\) of the different tasks \(i\) equal, as we show in the following result.

**Proposition 3.2**.: _Assume \(\{\ell_{i}\}_{i=1}^{k}\) are smooth and the optimal weights \(z_{t}^{*}\) in (6) is an interior point of \(\mathbb{S}_{k}\), then_

\[\forall\ i\neq j\in[k],\qquad r_{i}^{*}(d_{t}^{*})=r_{j}^{*}(d_{t}^{*}),\]

_where \(r_{i}^{*}(d_{t}^{*})=\lim_{\alpha\to 0}\frac{1}{\alpha}r_{i}(\alpha,d_{t}^{*})\)._

Proof.: Consider the Lagrangian form of (6)

\[\mathcal{L}(z,\lambda,\mu)=\frac{1}{2}\biggl{\|}\sum_{i=1}^{k}z_{i}\nabla\log \ell_{i,t}\biggr{\|}^{2}+\lambda\Bigl{(}\sum_{i=1}^{k}z_{i}-1\Bigr{)}-\sum_{i= 1}^{k}\mu_{i}z_{i},\ \ \text{where}\ \forall i,\mu_{i}\geq 0.\] (7)

When \(z^{*}\) reaches the optimum, we have \(\partial\mathcal{L}(z,\lambda,\mu)/\partial z=0\), recall that \(d_{t}^{*}=J_{t}z_{t}^{*}\), then

\[J_{t}^{\top}J_{t}z^{*}=-\mu-\lambda,\ \ \text{where}\ \ \ J_{t}=\begin{bmatrix} \nabla\log\ell_{1,t}^{\top}\\ \vdots\\ \nabla\log\ell_{k,t}^{\top}\end{bmatrix}\ \ \Longrightarrow\ \ \ J_{t}^{\top}d_{t}^{*}=-(\mu+\lambda).\]

When \(z_{t}^{*}\) is an interior point of \(\mathbb{S}_{k}\), we know that \(\mu=0\). Hence \(J_{t}^{\top}d_{t}^{*}=-\lambda\). This means,

\[\forall i\neq j,\qquad\lim_{\alpha\to 0}\frac{1}{\alpha}r_{i}(\alpha,d_{t}^{*} )=\nabla\log\ell_{i,t}^{\top}d_{t}^{*}=\nabla\log\ell_{j,t}^{\top}d_{t}^{*}= \lim_{\alpha\to 0}\frac{1}{\alpha}r_{j}(\alpha,d_{t}^{*}).\]

### Fast Approximation by Amortizing over Time

Instead of fully solving (6) at each optimization step, FAMO performs a single-step gradient descent on \(z\), which amortizes the computation over the optimization trajectory:

\[z_{t+1}=z_{t}-\alpha_{z}\tilde{\delta},\quad\text{where}\ \ \tilde{\delta}=\nabla_{z}\frac{1}{2}\biggl{\|}\sum_{i=1}^{k}z_{i,t}\nabla\log \ell_{i,t}\biggr{\|}^{2}=J_{t}^{\top}J_{t}z_{t}.\] (8)

But then, note that

\[\frac{1}{\alpha}\begin{bmatrix}\log\ell_{1,t}-\log\ell_{1,t+1}\\ \vdots\\ \log\ell_{k,t}-\log\ell_{k,t+1}\end{bmatrix}\approx J_{t}^{\top}d_{t}=J_{t}^{ \top}J_{t}z_{t},\] (9)

so we can use the change in log losses to approximate the gradient.

In practice, to ensure that \(z\) always stays in \(\mathbb{S}_{k}\), we re-parameterize \(z\) by \(\xi\) and let \(z_{t}=\text{\bf Softmax}(\xi_{t})\), where \(\xi_{t}\in\mathbb{R}^{K}\) are the unconstrained softmax logits. Consequently, we have the following approximate update on \(\xi\) from (8):

\[\xi_{t+1}=\xi_{t}-\beta\delta,\quad\text{where}\ \ \delta=\begin{bmatrix} \nabla^{\top}z_{1,t}(\xi)\\ \vdots\\ \nabla^{\top}z_{k,t}(\xi)\end{bmatrix}^{\top}\begin{bmatrix}\log\ell_{1,t}- \log\ell_{1,t+1}\\ \vdots\\ \log\ell_{k,t}-\log\ell_{k,t+1}\end{bmatrix}.\] (10)

**Remark:** While it is possible to perform gradient descent on \(z\) for other gradient manipulation methods in principle, we will demonstrate in Appendix B that not all such updates can be easily approximated using the change in losses.

### Practical Implementation

To facilitate practical implementation, we present two modifications to the update in (10).

Re-normalizationThe suggested update above is a convex combination of the gradients of the log loss, e.g.,

\[d^{*}=\sum_{i=1}^{k}z_{i,t}\nabla\log\ell_{i,t}=\sum_{i=1}^{k}\bigl{\{}\frac{ z_{i,t}}{\ell_{i,t}}\bigr{\}}\nabla\ell_{i,t}.\]

When \(\ell_{i,t}\) is small, the multiplicative coefficient \(\frac{z_{i,t}}{\ell_{i,t}}\) can be quite large and result in unstable optimization. Therefore, we propose to multiply \(d^{*}\) by a constant \(c_{t}\), such that \(c_{t}d^{*}\) can be written as a convex combination of the task gradients just as in other gradient manipulation algorithms (see (2) and we provide the corresponding definition of \(w\) in the following):

\[c_{t}=\bigl{(}\sum_{i=1}^{k}\frac{z_{i,t}}{\ell_{i,t}}\bigr{)}^{-1}\quad\text{ and}\quad d_{t}=c_{t}d^{*}=\sum_{i=1}^{k}w_{i}\nabla\ell_{i,t},\ \ \text{where}\ \ w_{i}=c_{t}\frac{z_{i,t}}{\ell_{i,t}}.\] (11)RegularizationAs we are amortizing the computation over time and the loss objective \(\{\ell_{i}(\cdot)\}\)s are changing dynamically, it makes sense to focus more on the recent updates of \(\xi\)[46]. To this end, we put a decay term on \(w\) such that the resulting \(\xi_{t}\) is an exponential moving average of its gradient updates:

\[\xi_{t+1}=\xi_{t}-\beta(\delta_{t}+\gamma\xi_{t})=-\beta\big{(}\delta_{t}+(1- \beta\gamma)\delta_{t-1}+(1-\beta\gamma)^{2}\delta_{t-2}+\dots\big{)}.\] (12)

We provide the complete FAMO algorithm in Algorithm 1 and its pseudocode in Appendix C.

### The Continuous Limit of FAMO

One way to characterize FAMO's behavior is to understand the stationary points of the continuous-time limit of FAMO (i.e. when step sizes \((\alpha,\beta)\) shrink to zero). From Algorithm 1, one can derive the following non-autonomous dynamical system (assuming \(\{\ell_{i}\}\) are all smooth):

\[\left[\begin{array}{c}\dot{\theta}\\ \dot{\xi}\end{array}\right]=-c_{t}\left[\begin{array}{c}J_{t}z_{t}\\ A_{t}J_{t}^{\top}J_{t}z_{t}+\frac{\gamma}{c_{t}}\xi_{t}\end{array}\right],\;\; \text{where}\;A_{t}=\begin{bmatrix}\nabla^{\top}z_{1,t}(\xi_{t})\\ \vdots\\ \nabla^{\top}z_{k,t}(\xi_{t})\end{bmatrix}.\] (13)

(13) reaches its stationary points (or fixed points) when (note that \(c_{t}>0\))

\[\left[\begin{array}{c}\dot{\theta}\\ \dot{\xi}\end{array}\right]=0\;\;\Longrightarrow\;\;J_{t}z_{t}=0\;\;\text{and} \;\;\xi_{t}=0\;\;\Longrightarrow\;\;\sum_{i=1}^{k}\nabla\log\ell_{i,t}=0.\] (14)

Therefore, the minimum points of \(\sum_{i=1}^{k}\log\ell_{i}(\theta)\) are all stationary points of (13).

## 4 Related Work

In this section, we summarize existing methods that tackle learning challenges in multitask learning (MTL). The general idea of most existing works is to encourage positive knowledge transfer by sharing parameters while decreasing any potential negative knowledge transfer (a.k.a, interference) during learning. There are three major ways of doing so: task grouping, designing network architectures specifically for MTL, and designing multitask optimization methods.

Task GroupingTask grouping refers to grouping \(K\) tasks into \(N<K\) clusters and learning \(N\) models for each cluster. The key is estimating the amount of positive knowledge transfer incurred by grouping certain tasks together and then identifying which tasks should be grouped [39; 45; 38; 36; 11].

Multitask ArchitectureNovel neural architectures for MTL include _hard-parameter-sharing_ methods, which decompose a neural network into task-specific modules and a shared feature extractor using manually designed heuristics [21; 29; 2], and _soft-parameter-sharing_ methods, which learn which parameters to share [30; 34; 12; 27]. Recent studies extend neural architecture search for MTL by learning where to branch a network to have task-specific modules [14; 3].

Multitask OptimizationThe most relevant approach to our method is MTL optimization via task balancing. These methods dynamically re-weight all task losses to mitigate the conflicting gradient issue [40; 43]. The simplest form of gradient manipulation is to re-weight the task losses based on manually designed criteria [6; 13; 18], but these methods are often heuristic and lack theoretical support. Gradient manipulation methods [35; 43; 25; 7; 16; 24; 32; 26; 47] propose to form a new update vector at each optimization by linearly combining task gradients. The local improvements across all tasks using the new update can often be explicitly analyzed, making these methods better understood in terms of convergence. However, it has been observed that gradient manipulation methods are often slow in practice, which may outweigh their performance benefits [22]. By contrast, FAMO is designed to match the performance of these methods while remaining efficient in terms of memory and computation. Another recent work proposes to sample random task weights at each optimization step for MTL [23], which is also computationally efficient. However, we will demonstrate empirically that FAMO performs better than this method.

## 5 Empirical Results

We conduct experiments to answer the following question:

_How does_ FAMO _perform in terms of space/time complexities and standard MTL metrics against prior MTL optimizers on standard benchmarks (e.g., supervised and reinforcement MTL problems)?_

In the following, we first use a toy 2-task problem to demonstrate how FAMO mitigates CG while being efficient. Then we show that FAMO performs comparably or even better than state-of-the-art gradient manipulation methods on standard multitask supervised and reinforcement learning benchmarks. In addition, FAMO requires significantly lower computation time when \(K\) is large compared to other methods. Lastly, we conduct an ablation study on how robust FAMO is to \(\gamma\). Each subsection first details the experimental setup and then analyzes the results.

### A Toy 2-Task Example

To better understand the optimization trajectory of FAMO, we adopt the same 2D multitask optimization problem from NashMTL [32] to visualize how FAMO balances different loss objectives. The model parameter \(\theta=(\theta_{1},\theta_{2})\in\mathbb{R}^{2}\). The two tasks' objectives and their surface plots are provided in Appendix D and Figure 2. We compare FAMO against Adam[19], MGDA [35], PCGrad[43], CAGrad[24], and NashMTL [32]. We then pick 5 initial points \(\theta_{\text{init}}\in\{(-8.5,7.5),(-8.5,5),(0,0),(9,9),(10,-8)\}\) and plot the corresponding optimization trajectories with different methods in Figure 1. Note that the toy example is constructed such that naively applying Adam on the average loss can cause the failure of optimization for task 1.

**Findings:** From Figure 1, we observe that FAMO, like all other gradient manipulation methods, mitigates the CG and reaches the Pareto front for all five runs. In the meantime, FAMO performs similarly to NashMTL and achieves a balanced loss decrease even when the two task losses are improperly scaled. Finally, as shown in the top-right of the plot, FAMO behaves similarly to Adam in terms of the training time, which is 25\(\times\) faster than NashMTL.

### MTL Performance

Multitask Supervised Learning.We consider four supervised benchmarks commonly used in prior MTL research [24; 27; 32; 33]: NYU-v2 [31] (3 tasks), CityScapes [8] (2 tasks), QM-9 [1] (11 tasks), and CelebA [28] (40 tasks). Specifically, NYU-v2 is an indoor scene dataset consisting of 1449 RGBD images and dense per-pixel labeling with 13 classes. The learning objectives include image segmentation, depth prediction, and surface normal prediction based on any scene image. CityScapes dataset is similar to NYU-v2 but contains 5000 street-view RGBD images with per-pixel annotations. QM-9 dataset is a widely used benchmark in graph neural network learning. It consists of >130K molecules represented as graphs annotated with node and edge features. We follow the same experimental setting used in NashMTL [32], where the learning objective is to predict 11 properties of molecules. We use 110K molecules from the QM9 example in PyTorch Geometric [10], 10K molecules for validation, and the rest of 10K molecules for testing. The characteristic of this dataset is that the 11 properties are at different scales, posing a challenge for task balancing in MTL. Lastly, CelebA dataset contains 200K face images of 10K different celebrities, and each face image is provided with 40 facial binary attributes. Therefore, CelebA can be viewed as a 40-task MTL problem. Different from NYU-v2, CityScapes, and QM-9, the number of tasks (\(K\)) in CelebA is much larger, hence posing a challenge to learning efficiency.

Figure 2: The average loss \(L^{0}\) and the two task losses \(L^{1}\) and \(L^{2}\) for the toy example.

We compare FAMO against 11 MTL optimization methods and a single-task learning baseline: **(1)** Single task learning (STL), training an independent model (\(\theta\) for each task; **(2)** Linear scalarization (LS) baseline that minimizes \(L^{0}\); **(3)** Scale-invariant (SI) baseline that minimizes \(\sum_{k}\log L^{k}(\theta)\), as SI is invariant to any scalar multiplication of task losses; **(4)** Dynamic Weight Average (DWA) [27], a heuristic for adjusting task weights based on rates of loss changes; **(5)** Uncertainty Weighting (UW) [18] uses task uncertainty as a proxy to adjust task weights; **(6)** Random Loss Weighting (RLW) [23] that samples task weighting whose log-probabilities follow the normal distribution; **(7)** MGDA [35] that finds the equal descent direction for each task; **(8)** PCGrad[43] proposes to project each task gradient to the normal plan of that of other tasks and combining them together in the end; **(9)** CAGrad[24] optimizes the average loss while explicitly controls the minimum decrease across tasks; **(10)** IMTL-G [25] finds the update direction with equal projections on task gradients; **(11)** GradDrop[7] that randomly dropout certain dimensions of the task gradients based on how much they conflict; **(12)** NashMTL [32] formulates MTL as a bargaining game and finds the solution to the game that benefits all tasks. For FAMO, we choose the best hyperparameter \(\gamma\in\{0.0001,0.001,0.01\}\) based on the validation loss. Specifically, we choose \(\gamma\) equals \(0.01\) for the CityScapes dataset and \(0.001\) for the rest of the datasets. See Appendix E for results with error bars.

Evaluations: We consider two metrics [32] for MTL: **1) \(\bm{\Delta m}\%\)**, the average per-task performance drop of a method \(m\) relative to the STL baseline denoted as \(b\): \(\bm{\Delta m}\%=\frac{1}{K}\sum_{k=1}^{K}(-1)^{\delta_{k}}\big{(}M_{m,k}-M_{b, k}\big{)}/M_{b,k}\times 100\), where \(M_{b,k}\) and \(M_{m,k}\) are the STL and \(m\)'s value for metric \(M_{k}\). \(\delta_{k}=1\) (or \(0\)) if the \(M_{k}\) is higher (or lower) the better. **2) Mean Rank (MR)**: the average rank of each method across tasks. For instance, if a method ranks first for every task, **MR** will be 1.

Findings: Results on the four benchmark datasets are provided in Table 1, 2 and 3. We observe that FAMO performs consistently well across different supervised learning MTL benchmarks compared

\begin{table}
\begin{tabular}{l c c c c c c c c c c c c} \hline \hline  & \multicolumn{3}{c}{Segmentation} & \multicolumn{3}{c}{Depth} & \multicolumn{6}{c}{Surface Normal} \\ \cline{2-13}
**Method** & mIoU \(\uparrow\) & Pix Acc \(\uparrow\) & Abs Err \(\downarrow\) & Rel Err \(\downarrow\) & \multicolumn{3}{c}{Angle Dist \(\downarrow\)} & \multicolumn{3}{c}{Within \(t^{\circ}\uparrow\)} & \multicolumn{3}{c}{**MR \(\downarrow\)**} & \(\bm{\Delta m}\%\downarrow\) \\ \cline{2-13}  & & & & & Mean & Median & 11.25 & 22.5 & 30 & \\ \hline STL & 38.30 & 63.76 & 0.6754 & 0.2780 & 25.01 & 19.21 & 30.14 & 57.20 & 69.15 & \\ \hline LS & 39.29 & 65.33 & 0.5493 & 0.2263 & 28.15 & 23.96 & 22.09 & 47.50 & 61.08 & 8.89 & 5.59 \\ SI & 38.45 & 64.27 & 0.5354 & 0.2201 & 27.60 & 23.37 & 22.53 & 48.57 & 62.32 & 7.89 & 4.39 \\ RLW & 37.17 & 63.77 & 0.5759 & 0.2410 & 28.27 & 24.18 & 22.26 & 47.05 & 60.62 & 11.22 & 7.78 \\ DWA & 39.11 & 65.31 & 0.5510 & 0.2285 & 27.61 & 23.18 & 24.17 & 50.18 & 62.39 & 7.67 & 3.57 \\ UW & 36.87 & 63.17 & 0.5446 & 0.2260 & 27.04 & 22.61 & 23.54 & 49.05 & 63.65 & 7.44 & 4.05 \\ MGDA & 30.47 & 59.90 & 0.6070 & 0.2555 & **24.88** & **19.45** & **29.18** & **56.88** & **69.36** & 6.00 & 1.38 \\ PCGrad & 38.06 & 64.64 & 0.5550 & 0.2325 & 27.41 & 22.80 & 23.86 & 49.83 & 63.14 & 8.00 & 3.97 \\ GradDrop & 39.39 & 65.12 & 0.5455 & 0.2279 & 27.48 & 22.96 & 23.38 & 49.44 & 62.87 & 7.00 & 3.58 \\ CAGrad & 39.79 & 65.49 & 0.5486 & 0.2250 & 26.31 & 21.58 & 25.61 & 52.36 & 65.58 & 4.56 & 0.20 \\ IMTL-G & 39.35 & 65.00 & 0.5426 & 0.2256 & 26.02 & 21.19 & 26.20 & 53.13 & 66.24 & 3.78 & -0.76 \\ NashMTL & **40.13** & 65.93 & 0.5261 & 0.2171 & 25.26 & 20.08 & 28.40 & 55.47 & 68.15 & 2.11 & -4.04 \\ \hline FAMO & 38.88 & 64.90 & 0.5474 & 0.2194 & 25.06 & 19.57 & 29.21 & 56.61 & 68.98 & 3.44 & **-4.10** \\ \hline \hline \end{tabular}
\end{table}
Table 1: Results on NYU-v2 dataset (3 tasks). Each experiment is repeated over 3 random seeds and the mean is reported. The best average result is marked in bold. **MR** and \(\bm{\Delta m}\%\) are the main metrics for MTL performance.

\begin{table}
\begin{tabular}{l c c c c c c c c c c c c} \hline \hline
**Method** & \(\mu\) & \(\alpha\) & \(\epsilon_{\text{HOMO}}\) & \(\epsilon_{\text{LUMO}}\) & \((R^{2})\) & ZPVE & \(U_{0}\) & \(U\) & \(H\) & \(G\) & \(c_{v}\) & **MR \(\downarrow\)** & \(\bm{\Delta m}\%\downarrow\) \\ \cline{3-13}  & & & & & MAE \(\downarrow\) & & & & & & & \\ \hline STL & 0.07 & 0.18 & 60.6 & 53.9 & 0.50 & 4.53 & 58.8 & 64.2 & 63.8 & 66.2 & 0.07 & & \\ \hline LS & 0.11 & 0.33 & 73.6 & 89.7 & 5.20 & 14.06 & 143.4 & 144.2 & 144.6 & 140.3 & 0.13 & 6.45 & 177.6 \\ SI & 0.31 & 0.35 & 149to other gradient manipulation methods. In particular, it achieves state-of-the-art results in terms of \(\bm{\Delta m}\%\) on the NYU-v2 and QM-9 datasets.

Multitask Reinforcement Learning.We further apply FAMO to multitask reinforcement learning (MTRL) problems as MTRL often suffers more from conflicting gradients due to the stochastic nature of reinforcement learning [43]. Following CAGrad[24], we apply FAMO on the MetaWorld [44] MT10 benchmark, which consists of 10 robot manipulation tasks with different reward functions. Following [37], we use Soft Actor-Critic (SAC) [15] as the underlying RL algorithm, and compare against baseline methods including LS (SAC with a shared model) [44], Soft Modularization [42] (an MTL network that routes different modules in a shared model to form different policies), PCGrad[43], CAGrad and NashMTL [32]. The experimental setting and hyperparameters all match exactly with those in CAGrad. For NashMTL, we report the results of applying the NashMTL update once per \(\{1,50,100\}\) iterations.4 The results for all methods are provided in Table 4.

Footnote 4: We could not reproduce the MTRL results of NashMTL exactly, so we report both the results from the original paper and our reproduced results.

\begin{table}
\begin{tabular}{l c} \hline \hline
**Method** & Success \(\uparrow\) \\  & (mean \(\pm\) stderr) \\ \hline LS (lower bound) & 0.49 \(\pm\)0.07 \\ STL (proxy for upper bound) & 0.90 \(\pm\)0.03 \\ \hline PCGrad[43] & 0.72 \(\pm\)0.02 \\ Soft Modularization[42] & 0.73 \(\pm\)0.04 \\ CAGrad & 0.83 \(\pm\)0.05 \\ NashMTL [32] (every 1) & 0.91 \(\pm\)0.03 \\ NashMTL [32] (every 50) & 0.85 \(\pm\)0.02 \\ NashMTL [32] (every 100) & 0.87 \(\pm\)0.03 \\ \hline NashMTL (ours) (every 1) & 0.80 \(\pm\)0.13 \\ NashMTL (ours) (every 50) & 0.76 \(\pm\)0.10 \\ NashMTL (ours) (every 100) & 0.80 \(\pm\)0.12 \\ UW[18] & 0.77 \(\pm\)0.05 \\ \hline FAMO (ours) & 0.83 \(\pm\)0.05 \\ \hline \hline \end{tabular}
\end{table}
Table 4: MTRL results (averaged over 10 runs) on the Metaworld-10 benchmark.

\begin{table}
\begin{tabular}{l c c c c c c c c} \hline \hline  & \multicolumn{6}{c}{**CityScapes**} & \multicolumn{3}{c}{**Celeba**} \\ \cline{2-9}
**Method** & \multicolumn{2}{c}{Segmentation} & \multicolumn{2}{c}{Depth} & \multicolumn{1}{c}{**MR \(\downarrow\)**} & \multicolumn{1}{c}{\(\bm{\Delta m}\%\downarrow\)**} & \multicolumn{1}{c}{**MR \(\downarrow\)**} & \multicolumn{1}{c}{\(\bm{\Delta m}\%\downarrow\)**} \\ \cline{2-9}  & mIoU \(\uparrow\) & Pix Acc \(\uparrow\) & Abs Err \(\downarrow\) & Rel Err \(\downarrow\) & & & & \\ \hline STL & 74.01 & 93.16 & 0.0125 & 27.77 & & & & \\ \hline LS & 70.95 & 91.73 & 0.0161 & 33.83 & 6.50 & 14.11 & 4.15 & 6.28 \\ SI & 70.95 & 91.73 & 0.0161 & 33.83 & 9.25 & 14.11 & 7.20 & 7.83 \\ RLW & 74.57 & 93.41 & 0.0158 & 47.79 & 9.25 & 24.38 & 1.46 & 5.22 \\ DWA & 75.24 & 93.52 & 0.0160 & 44.37 & 6.50 & 21.45 & 3.20 & 6.95 \\ UW & 72.02 & 92.85 & 0.0140 & 30.13 & 6.00 & 5.89 & 3.23 & 5.78 \\ MGDA & 68.84 & 91.54 & 0.0309 & 33.50 & 9.75 & 44.14 & 14.85 & 10.93 \\ PCGrad & 75.13 & 93.48 & 0.0154 & 42.07 & 6.75 & 18.29 & 3.17 & 6.65 \\ GradDrop & 75.27 & 93.53 & 0.0157 & 47.54 & 6.00 & 23.73 & 3.29 & 7.80 \\ CAGrad & 75.16 & 93.48 & 0.0141 & 37.60 & 5.75 & 11.64 & 2.48 & 6.20 \\ IMTL-G & 75.33 & 93.49 & 0.0135 & 38.41 & 4.00 & 11.10 & 0.84 & 4.67 \\ NashMTL & 75.41 & 93.66 & 0.0129 & 35.02 & 2.00 & 6.82 & 2.84 & 4.97 \\ \hline FAMO & 74.54 & 93.29 & 0.0145 & 32.59 & 6.25 & 8.13 & 1.21 & 4.72 \\ \hline \hline \end{tabular}
\end{table}
Table 3: Results on CityScapes (2 tasks) and CelebA (40 tasks) datasets. Each experiment is repeated over 3 random seeds and the mean is reported. The best average result is marked in bold. **MR** and \(\bm{\Delta m}\%\) are the main metrics for MTL performance.

Figure 3: Training Success Rate and Time.

Findings: From Table 5.2, we observe that FAMO performs comparably to CAGrad and outperforms PCGrad and the average gradient descent baselines by a large margin. FAMO also outperforms NashMTL based on our implementation. Moreover, FAMO is significantly faster than NashMTL, even when it is applied once every 100 steps.

### MTL Efficiency (Training Time Comparison)

Figure 4 provides the FAMO's average training time per epoch against that of the baseline methods.

**Findings:** From the figure, we observe that FAMO introduces negligible overhead across all benchmark datasets compared to the LS method, which is, in theory, the lower bound for computation time. In contrast, methods like NashMTL have much longer training time compared to FAMO. More importantly, the computation cost of these methods scales with the number of tasks. In addition, note that these methods also take at least \(\mathcal{O}(K)\) space to store the task gradients, which is implausible for large models in the many-task setting (i.e., when \(m=|\theta|\) and \(K\) are large).

### Ablation on \(\gamma\)

In this section, we provide the ablation study on the regularization coefficient \(\gamma\) in Figure 5.

**Findings:** From Figure 5, we can observe that choosing the right regularization coefficient can be crucial. But except for CityScapes, FAMO performs reasonably well using all different \(\gamma\)s. The problem with CityScapes is that one of the task losses is close to \(0\) at the very beginning, hence small changes in task weighting can result in very different loss improvement. Therefore we conjecture that using a larger \(\gamma\), in this case, can help stabilize MTL.

## 6 Conclusion and Limitations

In this work, we introduce FAMO, a fast optimization method for multitask learning (MTL) that mitigates the conflicting gradients using \(\mathcal{O}(1)\) space and time. As multitasking large models gain more attention, we believe designing efficient but effective optimizers like FAMO for MTL is crucial. FAMO balances task losses by ensuring each task's loss decreases approximately at an equal rate. Empirically, we observe that FAMO can achieve competitive performance against the state-of-the-art MTL gradient manipulation methods. One limitation of FAMO is its dependency on the regularization parameter \(\gamma\), which is introduced due to the stochastic update of the task weighting logits \(\bm{w}\). Future work can investigate a more principled way of determining \(\gamma\).

Figure 4: Average training time per epoch for different MTL optimization methods. We report the relative training time of a method to that of the linear scalarization (LS) method (which uses the average gradient).

Figure 5: Ablation over \(\gamma\): we plot the performance of FAMO (in terms of \(\bm{\Delta}\bm{m}\%\) using different values of \(\gamma\) from \(\{0.0001,0.001,0.01\}\) on the four supervised MTL benchmarks.

## References

* [1] L. C. Blum and J.-L. Reymond. 970 million druglike small molecules for virtual screening in the chemical universe database GDB-13. _J. Am. Chem. Soc._, 131:8732, 2009.
* [2] Felix JS Bragman, Ryutaro Tanno, Sebastien Ourselin, Daniel C Alexander, and Jorge Cardoso. Stochastic filter groups for multi-task cnns: Learning specialist and generalist convolution kernels. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 1385-1394, 2019.
* [3] David Bruggemann, Menelaos Kanakis, Stamatios Georgoulis, and Luc Van Gool. Automated search for resource-efficient branched multi-task networks. _arXiv preprint arXiv:2008.10292_, 2020.
* [4] Sebastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial general intelligence: Early experiments with gpt-4. _arXiv preprint arXiv:2303.12712_, 2023.
* [5] Rich Caruana. Multitask learning. _Machine learning_, 28(1):41-75, 1997.
* [6] Zhao Chen, Vijay Badrinarayanan, Chen-Yu Lee, and Andrew Rabinovich. Gradnorm: Gradient normalization for adaptive loss balancing in deep multitask networks. In _International Conference on Machine Learning_, pages 794-803. PMLR, 2018.
* [7] Zhao Chen, Jiquan Ngiam, Yanping Huang, Thang Luong, Henrik Kretzschmar, Yuning Chai, and Dragomir Anguelov. Just pick a sign: Optimizing deep multitask models with gradient sign dropout. _arXiv preprint arXiv:2010.06808_, 2020.
* [8] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 3213-3223, 2016.
* [9] Jean-Antoine Desideri. Multiple-gradient descent algorithm (mgda) for multiobjective optimization. _Comptes Rendus Mathematique_, 350(5-6):313-318, 2012.
* [10] Matthias Fey and Jan Eric Lenssen. Fast graph representation learning with pytorch geometric. _arXiv preprint arXiv:1903.02428_, 2019.
* [11] Chris Fifty, Ehsan Amid, Zhe Zhao, Tianhe Yu, Rohan Anil, and Chelsea Finn. Efficiently identifying task groupings for multi-task learning. _Advances in Neural Information Processing Systems_, 34:27503-27516, 2021.
* [12] Yuan Gao, Haoping Bai, Zequn Jie, Jiayi Ma, Kui Jia, and Wei Liu. Mtl-nas: Task-agnostic neural architecture search towards general-purpose multi-task learning. In _Proceedings of the IEEE/CVF Conference on computer vision and pattern recognition_, pages 11543-11552, 2020.
* [13] Michelle Guo, Albert Haque, De-An Huang, Serena Yeung, and Li Fei-Fei. Dynamic task prioritization for multitask learning. In _Proceedings of the European conference on computer vision (ECCV)_, pages 270-287, 2018.
* [14] Pengsheng Guo, Chen-Yu Lee, and Daniel Ulbricht. Learning to branch for multi-task learning. In _International Conference on Machine Learning_, pages 3854-3863. PMLR, 2020.
* [15] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. In _International Conference on Machine Learning_, pages 1861-1870. PMLR, 2018.
* [16] Adrian Javaloy and Isabel Valera. Rotograd: Dynamic gradient homogenization for multi-task learning. _arXiv preprint arXiv:2103.02631_, 2021.
* [17] Alexandr Katrutsa, Daniil Merkulov, Nurislam Turysnbek, and Ivan Oseledets. Follow the bisector: a simple method for multi-objective optimization. _arXiv preprint arXiv:2007.06937_, 2020.

* [18] Alex Kendall, Yarin Gal, and Roberto Cipolla. Multi-task learning using uncertainty to weigh losses for scene geometry and semantics. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 7482-7491, 2018.
* [19] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. _arXiv preprint arXiv:1412.6980_, 2014.
* [20] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. _arXiv preprint arXiv:2304.02643_, 2023.
* [21] Iasonas Kokkinos. Ubernet: Training a universal convolutional neural network for low-, mid-, and high-level vision using diverse datasets and limited memory. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 6129-6138, 2017.
* [22] Vitaly Kurin, Alessandro De Palma, Ilya Kostrikov, Shimon Whiteson, and Pawan K Mudigonda. In defense of the unitary scalarization for deep multi-task learning. _Advances in Neural Information Processing Systems_, 35:12169-12183, 2022.
* [23] Baijiong Lin, Feiyang Ye, and Yu Zhang. A closer look at loss weighting in multi-task learning. _arXiv preprint arXiv:2111.10603_, 2021.
* [24] Bo Liu, Xingchao Liu, Xiaojie Jin, Peter Stone, and Qiang Liu. Conflict-averse gradient descent for multi-task learning. _Advances in Neural Information Processing Systems_, 34:18878-18890, 2021.
* [25] Liyang Liu, Yi Li, Zhanghui Kuang, Jing-Hao Xue, Yimin Chen, Wenming Yang, Qingmin Liao, and Wayne Zhang. Towards impartial multi-task learning. In _International Conference on Learning Representations_, 2020.
* [26] Shikun Liu, Stephen James, Andrew J Davison, and Edward Johns. Auto-lambda: Disentangling dynamic task relationships. _arXiv preprint arXiv:2202.03091_, 2022.
* [27] Shikun Liu, Edward Johns, and Andrew J Davison. End-to-end multi-task learning with attention. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 1871-1880, 2019.
* [28] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In _Proceedings of International Conference on Computer Vision (ICCV)_, December 2015.
* [29] Mingsheng Long, Zhangjie Cao, Jianmin Wang, and Philip S Yu. Learning multiple tasks with multilinear relationship networks. _Advances in neural information processing systems_, 30, 2017.
* [30] Ishan Misra, Abhinav Shrivastava, Abhinav Gupta, and Martial Hebert. Cross-stitch networks for multi-task learning. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 3994-4003, 2016.
* [31] Pushmeet Kohli Nathan Silberman, Derek Hoiem and Rob Fergus. Indoor segmentation and support inference from rgbd images. In _ECCV_, 2012.
* [32] Aviv Navon, Aviv Shamsian, Idan Achituve, Haggai Maron, Kenji Kawaguchi, Gal Chechik, and Ethan Fetaya. Multi-task learning as a bargaining game. _arXiv preprint arXiv:2202.01017_, 2022.
* [33] Lucas Pascal, Pietro Michiardi, Xavier Bost, Benoit Huet, and Maria A Zuluaga. Improved optimization strategies for deep multi-task networks. _arXiv preprint arXiv:2109.11678_, 2021.
* [34] Sebastian Ruder, Joachim Bingel, Isabelle Augenstein, and Anders Sogaard. Latent multi-task architecture learning. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 33, pages 4822-4829, 2019.
* [35] Ozan Sener and Vladlen Koltun. Multi-task learning as multi-objective optimization. _arXiv preprint arXiv:1810.04650_, 2018.

* Shen et al. [2021] Jiayi Shen, Xiantong Zhen, Marcel Worring, and Ling Shao. Variational multi-task learning with gumbel-softmax priors. _Advances in Neural Information Processing Systems_, 34:21031-21042, 2021.
* Sodhani et al. [2021] Shagun Sodhani, Amy Zhang, and Joelle Pineau. Multi-task reinforcement learning with context-based representations. _arXiv preprint arXiv:2102.06177_, 2021.
* Standley et al. [2020] Trevor Standley, Amir Zamir, Dawn Chen, Leonidas Guibas, Jitendra Malik, and Silvio Savarese. Which tasks should be learned together in multi-task learning? In _International Conference on Machine Learning_, pages 9120-9132. PMLR, 2020.
* Thrun and O'Sullivan [1996] Sebastian Thrun and Joseph O'Sullivan. Discovering structure in multiple learning tasks: The tc algorithm. In _ICML_, volume 96, pages 489-497, 1996.
* Vandenhende et al. [2021] Simon Vandenhende, Stamatios Georgoulis, Wouter Van Gansbeke, Marc Proesmans, Dengxin Dai, and Luc Van Gool. Multi-task learning for dense prediction tasks: A survey. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 2021.
* Xin et al. [2022] Derrick Xin, Behrooz Ghorbani, Justin Gilmer, Ankush Garg, and Orhan Firat. Do current multi-task optimization methods in deep learning even help? _Advances in Neural Information Processing Systems_, 35:13597-13609, 2022.
* Yang et al. [2020] Ruihan Yang, Huazhe Xu, Yi Wu, and Xiaolong Wang. Multi-task reinforcement learning with soft modularization. _arXiv preprint arXiv:2003.13661_, 2020.
* Yu et al. [2020] Tianhe Yu, Saurabh Kumar, Abhishek Gupta, Sergey Levine, Karol Hausman, and Chelsea Finn. Gradient surgery for multi-task learning. _arXiv preprint arXiv:2001.06782_, 2020.
* Yu et al. [2020] Tianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian, Karol Hausman, Chelsea Finn, and Sergey Levine. Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning. In _Conference on Robot Learning_, pages 1094-1100. PMLR, 2020.
* Zamir et al. [2018] Amir R Zamir, Alexander Sax, William Shen, Leonidas J Guibas, Jitendra Malik, and Silvio Savarese. Taskonomy: Disentangling task transfer learning. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 3712-3722, 2018.
* Zhou et al. [2022] Shiji Zhou, Wenpeng Zhang, Jiyan Jiang, Wenliang Zhong, Jinjie Gu, and Wenwu Zhu. On the convergence of stochastic multi-objective gradient manipulation and beyond. _Advances in Neural Information Processing Systems_, 35:38103-38115, 2022.
* Zhu et al. [2021] Shijie Zhu, Hui Zhao, Pengjie Wang, Hongbo Deng, Jian Xu, and Bo Zheng. Gradient deconfiction via orthogonal projections onto subspaces for multi-task learning.

Gradient Manipulation Methods

In this section, we provide a brief overview of representative gradient manipulation methods in multitask/multiobjective optimization. Specifically, we will also discuss the connections among these methods.

Multiple Gradient Descent Algorithm (MGDA) [9, 35]The MGDA algorithm is one of the earliest gradient manipulation methods for multitask learning. In MGDA, the per step update \(d_{t}\) is found by solving

\[\max_{d\in\mathbb{R}^{m}}\min_{i\in[k]}\nabla\ell_{i,t}^{\top}d- \frac{1}{2}\|d\|^{2}.\]

As a result, the solution \(d^{*}\) of MGDA optimizes the "worst improvement" across all tasks or equivalently seeks an _equal_ descent across all task losses as much as possible. But in practice, MGDA suffers from slow convergence since the update \(d^{*}\) can be very small. For instance, if one task has a very small loss scale, the progress of all other tasks will be bounded by the progress on this task. Note that the original objective in (6) is similar to the MGDA objective in the sense that we can view optimizing (6) as optimizing the log of the task losses. Hence, when we compare FAMO against MGDA, one can regard FAMO as balancing the _rate_ of loss improvement while MGDA balances the absolute improvement across task losses.

Projecting Gradient Descent (PCGrad) [43]PCGrad initializes \(v_{\text{PC}}^{i}=\nabla\ell_{i,t}\), then for each task \(i\), PCGrad loops over all task \(j\neq i\) (in a random order, which is crucial as mentioned in [43]) and removes the "conflict"

\[v_{\text{PC}}^{i}\gets v_{\text{PC}}^{i}-\frac{{v_{\text{PC}}^{i}}^{ \top}\nabla\ell_{j,t}}{\left\|\ell_{j,t}\right\|^{2}}\nabla\ell_{j,t}\quad \text{if}\quad{v_{\text{PC}}^{i}}^{\top}\nabla\ell_{j,t}<0.\]

In the end, PCGrad produces \(d_{t}=\frac{1}{k}\sum_{i=1}^{k}v_{\text{PC}}^{i}\). Due to the construction, PCGrad will also help improve the "worst improvement" across all tasks since the "conflicts" have been removed. However, due to the stochastic iterative procedural of this algorithm, it is hard to understand PCGrad from a first principle approach.

Conflict-averse Gradient Descent (CAGrad) [24]\(d_{t}\) is found by solving

\[\max_{d\in\mathbb{R}^{m}}\min_{i\in[k]}\nabla\ell_{i,t}^{\top}d\quad\text{s.t. }\quad\|d-\nabla\ell_{0,t}\|\leq c\|\nabla\ell_{0,t}\|.\]

Here, \(\ell_{0,t}=\frac{1}{k}\sum_{i=1}^{k}\ell_{i,t}\). CAGrad seeks an update \(d_{t}\) that optimizes the "worst improvement" as much as possible, conditioned on that the update still decreases the average loss. By controlling the hyperparameter \(c\), CAGrad can recover MGDA (\(c\rightarrow\infty\)) and the vanilla averaged gradient descent (\(c\to 0\)). Due to the extra constraint, CAGrad provably converges to the stationary points of \(\ell_{0}\) when \(0\leq c<1\).

Impartial Multi-Task Learning (IMTL-G) [25]IMTL-G finds \(d_{t}\) such that it shares the same cosine similarity with any task gradients:

\[\forall i\neq j,\quad d_{t}^{\top}\frac{\nabla\ell_{i,t}}{\left\lvert\nabla \ell_{i,t}\right\rvert}=d_{t}^{\top}\frac{\nabla\ell_{j,t}}{\left\lvert\nabla \ell_{j,t}\right\rvert},\quad\text{and}\quad d_{t}=\sum_{i=1}^{k}w_{i,t} \nabla\ell_{i,t},\ \text{ for some }\ w_{t}\in\mathbb{S}_{k}.\]

The constraint that \(d_{t}=\sum_{i=1}^{k}w_{i,t}\nabla\ell_{i,t}\) is for preventing the problem from being under-determined. From the above equation, we can see that IMTL-G ignores the "size" of each task gradient and only cares about the "direction". As a result, **one can think of IMTL-G as a variant of MGDA that applies to the normalized gradients**. By doing so, IMTL-G does not suffer from the straggler effect due to slow objectives. Furthermore, one can **view IMTL-G as the equal angle descent**, which is also proposed in Katrutsa et al. [17], where the objective is to find \(d\) such that

\[\forall i\neq j,\qquad\cos(d,\nabla\ell_{i,t})=\cos(d,\nabla\ell_{j,t}).\]NashMTL[32]NashMTL finds \(d_{t}\) by solving a bargaining game treating the local improvement of each task loss as the utility for each task:

\[\max_{d\in\mathbb{R}^{m},\left\lvert d\right\rvert\leq 1}\sum_{i=1}^{k}\log \big{(}\nabla\ell_{i,t}^{\top}d\big{)}.\]

Note that the objective of NashMTL implicitly assumes that there exists \(d\) such that \(\forall\ i,\ \ \nabla\ell_{i,t}^{\top}d>0\) (otherwise we reach the Pareto front). It is easy to see that

\[\max_{\left\lvert d\right\rvert\leq 1}\sum_{i=1}^{k}\log\big{(}\nabla\ell_{i,t}^ {\top}d\big{)}=\max_{\left\lvert d\right\rvert\leq 1}\sum_{i=1}^{k}\log(\frac{ \nabla\ell_{i,t}}{\left\lVert\nabla\ell_{i,t}\right\rVert},d)=\max_{\left\lvert d \right\rvert\leq 1}\sum_{i=1}^{k}\log\cos\big{(}\nabla\ell_{i,t},d\big{)}.\]

Therefore, due to the \(\log\), NashMTL also ignores the "size" of task gradients and only cares about their "directions". Moreover, denote \(u_{i}=\frac{\nabla\ell_{i,t}}{\left\lVert\nabla\ell_{i,t}\right\rVert}\). Then, according to the KKT condition, we know:

\[\sum_{i}\frac{u_{i}}{u_{i}^{\top}d}-\alpha d=0,\quad\alpha\geq 0\qquad \Longrightarrow\qquad d=\frac{1}{\alpha}\sum_{i}\frac{1}{u_{i}^{\top}d}u_{i}.\]

Consider when \(k=2\), if we take the _equal angle descent_ direction: \(d_{\measuredangle}=\big{(}u_{1}+u_{2}\big{)}/2\) (note that as \(u_{1}\) and \(u_{2}\) are normalized, their bisector is just their average). Then it is easy to check that

\[d_{\measuredangle}=\frac{1}{\alpha}\Bigg{(}\frac{2}{u_{1}^{\top}\big{(}u_{1}+u_{2} \big{)}}u_{1}+\frac{2}{u_{2}^{\top}\big{(}u_{1}+u_{2}\big{)}}u_{2}\Bigg{)},\ \ \text{where}\ \ \alpha=\frac{u_{1}^{\top}\big{(}u_{1}+u_{2}\big{)}}{4}=\frac{u_{2}^{\top} \big{(}u_{1}+u_{2}\big{)}}{4}.\]

As a result, we can see that **when \(k=2\)**, **NashMTL is equivalent to IMTL-G (or the equal angle descent)**. However, when \(k>2\), this is not in general true.

**Remark** Note that all of these gradient manipulation methods require computing and storing \(K\) task gradients before applying \(f\) to compute \(d_{t}\), which often involves solving an additional optimization problem. Hence, these methods can be slow for large \(K\) and large model sizes.

## Appendix B Amortizing other Gradient Manipulation Methods

Although FAMO uses iterative update on \(w\), it is not immediately clear whether we can apply the same amortization easily on other existing gradient manipulation methods. In this section, we discuss such possibilities and point out the challenges.

Amortizing MgDAThis is almost the same as in FAMO, except that MGDA acts on the original task losses while FAMO acts on the log of task losses.

Amortizing PCGradFor PCGrad, finding the final update vector requires iteratively projecting one task gradient to the other, so there is no straightforward way of bypassing the computation of task gradients.

Amortizing IMTL-GThe task weighting in IMTL-G is computed by a series of matrix-matrix and matrix-vector products using task gradients [25]. Hence, it is also hard to amortize its computation over time.

Therefore, we focus on deriving the amortization for CAGrad and NashMTL.

Amortizing CAGradFor CAGrad, the dual objective is

\[\min_{w\in\mathbb{S}_{h}}F(w)=g_{w}^{\top}g_{0}+c\|g_{w}\|\|g_{0}\|,\] (15)

where \(g_{0}=\nabla\ell_{0,t}\) and \(g_{w}=\sum_{i=1}^{k}w_{i}\nabla\ell_{i}\). Denote

\[G=\begin{bmatrix}\nabla\ell_{1,t}^{\top}\\ \vdots\\ \nabla\ell_{k,t}^{\top}\end{bmatrix}.\]

[MISSING_PAGE_FAIL:16]

``` classFAMO: def__init__(self,num_tasks,min_losses,\(\alpha\)=0.025,\(\gamma\)=0.001): #min_losses(num_tasks,)thelosslowerboundforeachtask. self.min_losses=min_losses self.xi=torch.tensor([0.0]*num_tasks,requires_grad=True) self.xi_opt=torch.optim.Adam([self.xi],lr=\(\alpha\),weight_decay=\(\gamma\)) defget_weighted_loss(self,losses): #losses(num_tasks,) z=F.softmax(self.xi,-1) D=losses-self.min_losses+1e-8 c=1/(z/D).sum().detach() loss=(c*D.log()*z).sum() returnloss defupdate(self,prev_losses,curr_losses): #prev_losses(num_tasks,) #curr_losses(num_tasks,) delta=(prev_losses-self.min_losses+1e-8).log() - (curr_losses-self.min_losses+1e-8).log() withtorch.enable_grad(): d=torch.autograd.grad(F.softmax(self.xi,-1), self.xi, grad_outputs=delta.detach())[0] self.xi_opt.zero_grad() self.xi.grad=d self.xi_opt.step ```

**Algorithm 2** Implementation of FAMO in PyTorch-like Pseudocode

## Appendix E Experimental Results with Error Bars

We followed the exact experimental setup from NashMTL [32]. Therefore, the numbers for baseline methods are taken from their original paper. In the following, we provide FAMO's result with error bars.

\begin{table}
\begin{tabular}{l r r r r r r r r r r r r} \hline \hline  & \multicolumn{4}{c}{**CityScapes**} & \multicolumn{4}{c}{**CelebA**} \\ \cline{2-13} \multirow{2}{*}{**Method**} & \multicolumn{4}{c}{Segmentation} & \multicolumn{4}{c}{Depth} & \multicolumn{1}{c}{\(\bm{\Delta m}\%\downarrow\)} & \multicolumn{1}{c}{\(\bm{\Delta m}\%\downarrow\)} \\ \cline{2-13}  & \multicolumn{2}{c}{InloU \(\uparrow\)} & \multicolumn{2}{c}{Pix Acc \(\uparrow\)} & \multicolumn{2}{c}{Abs Err \(\downarrow\)} & \multicolumn{2}{c}{Rel Err \(\downarrow\)} & \multicolumn{1}{c}{} & \\ \hline FAMO (mean) & 74.54 & 93.29 & 0.0145 & 32.59 & 8.13 & 1.21 \\ FAMO (stderr) & \(\pm\)0.11 & \(\pm\)0.04 & \(\pm\)0.0009 & \(\pm\)1.06 & \(\pm\)1.98 & \(\pm\)0.24 \\ \hline \hline \end{tabular}
\end{table}
Table 7: Results on CityScapes (2 tasks) and CelebA (40 tasks) datasets. Each experiment is repeated over 3 random seeds and the mean is reported. The best average result is marked in bold. **MR** and \(\bm{\Delta m}\%\) are the main metrics for MTL performance.

\begin{table}
\begin{tabular}{l r r r r r r r r r r r} \hline \hline  & \multicolumn{4}{c}{Segmentation} & \multicolumn{4}{c}{Depth} & \multicolumn{4}{c}{Surface Normal} \\ \cline{2-13}  & \multicolumn{2}{c}{mloU \(\uparrow\)} & \multicolumn{2}{c}{Pix Acc \(\uparrow\)} & \multicolumn{2}{c}{Abs Err \(\downarrow\)} & \multicolumn{2}{c}{Rel Err \(\downarrow\)} & \multicolumn{2}{c}{Angle Dist \(\downarrow\)} & \multicolumn{2}{c}{Within \(t^{\circ}\uparrow\)} & \multicolumn{2}{c}{\(\bm{\Delta m}\%\downarrow\)} \\ \cline{2-13}  & \multicolumn{2}{c}{mloU \(\uparrow\)} & \multicolumn{2}{c}{Pix Acc \(\uparrow\)} & \multicolumn{2}{c}{Abs Err \(\downarrow\)} & \multicolumn{2}{c}{Rel Err \(\downarrow\)} & \multicolumn{2}{c}{Mean} & \multicolumn{2}{c}{Median} & \multicolumn{2}{c}{11.25} & \multicolumn{2}{c}{22.5} & \multicolumn{2}{c}{30} \\ \hline FAMO (mean) & 38.88 & 64.90 & 0.5474 & 0.2194 & 25.06 & 19.57 & 29.21 & 56.61 & 68.98 & -4.10 \\ FAMO (stderr) & \(\pm\)0.54 & \(\pm\)0.21 & \(\pm\)0.0016 & \(\pm\)0.0026 & \(\pm\)0.06 & \(\pm\)0.09 & \(\pm\)0.17 & \(\pm\)0.19 & \(\pm\)0.14 & \(\pm\)0.39 \\ \hline \hline \end{tabular}
\end{table}
Table 5: Results on NYU-v2 dataset (3 tasks). Each experiment is repeated over 3 random seeds and the mean is reported. The best average result is marked in bold. **MR** and \(\bm{\Delta m}\%\) are the main metrics for MTL performance.