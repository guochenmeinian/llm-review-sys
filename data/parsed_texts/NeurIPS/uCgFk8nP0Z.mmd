# DU-Shapley: A Shapley Value Proxy for

Efficient Dataset Valuation

 Felipe Garrido-Lucero*

Inria, Fairplay joint team

Palaiseau, France

felipe.garrido-lucero@irit.fr

* Equal contribution

Benjamin Heymann*

Criteo AI Lab

Paris, France

b.heymann@criteo.com

* Equal contribution

Maxime Vono*

Criteo AI Lab

Paris, France

m.vono@criteo.com

* Equal contribution

Patrick Loiseau

Inria, Fairplay joint team

Palaiseau, France

patrick.loiseau@inria.fr

Vianney Perchet

ENSAE, FairPlay joint team

Palaiseau, France

vianney@ensae.fr

###### Abstract

We consider the _dataset valuation problem_, that is, the problem of quantifying the incremental gain, to some relevant pre-defined utility of a machine learning task, of aggregating an individual dataset to others. The Shapley value is a natural tool to perform dataset valuation due to its formal axiomatic justification, which can be combined with Monte Carlo integration to overcome the computational tractability challenges. Such generic approximation methods, however, remain expensive in some cases. In this paper, we exploit the knowledge about the structure of the dataset valuation problem to devise more efficient Shapley value estimators. We propose a novel approximation, referred to as discrete uniform Shapley, which is expressed as an expectation under a discrete uniform distribution with support of reasonable size. We justify the relevancy of the proposed framework via asymptotic and non-asymptotic theoretical guarantees and illustrate its benefits via an extensive set of numerical experiments.

## 1 Introduction

One of the main challenges for training machine learning (ML) models with enough generalization capabilities is to access a sufficiently large set of labeled training data. These data often exist but are commonly spread across many parties, impairing their usage in a direct and simple way. Real world examples range from the advertising industry, where different retailers hold sets of observations with either similar or complementary features from consented data about browsing and shopping habits of individual users; to the medical sector where hospitals may improve their diagnostics accuracy by sharing their data. By collaborating with each other and pooling their individual datasets together, these _dataset owners_ could learn better ML models for their applications. Naturally, many questions raise from such collaborations. Federated learning [8; 9], for example, addresses the issues related to the practical ways that dataset owners can share their data. We consider a complementary problem to the one in federated learning: measuring the additional value each party would obtain by participating in the joint ML effort. In order to compute or estimate compensating rewards allowing to incentivize parties to share their data, a first stage that is commonly considered in the literature is to perform so-called _dataset valuation_[1; 42; 44].

Motivated by natural properties expected for fair valuation, different solution concepts from cooperative game theory [3] have been considered, the Shapley value [40] being arguably the most broadlystudied valuation scheme in ML due to its axiomatic justification. Agarwal et al. [1] designed a data marketplace and used the Shapley value to allocate the data among buyers. Tay et al. [44] considered a cooperative environment where agents can jointly train a generative model, from which synthetic data are drawn and distributed to the parties according to their Shapley values. Sim et al. [42] rewarded parties based on the Shapley value and information gain on model parameters. The critical challenge when using the Shapley value is its well-known computational intractability. To cope with it, [1; 44] considered Monte Carlo (MC) approximations, while [42] worked with a small set of three players. This approximation methods, however, remain expensive whenever computing the marginal contributions involve retraining. Moreover, they are generic and do not use the specific structure of the dataset valuation problem at stake, leaving open the possibility to find more adapted approximations for that problem.

The Shapley value was also used in the related problem of _data valuation_. Data valuation measures the contribution of a single data point within a dataset in the training of a given prediction model. Several solution concepts based on the Shapley value have been proposed for the data valuation problem including Data Shapley[10; 16], DShapley[11; 24], Beta Shapley[23] or CS-Shapley[38], together with different MC variants to cope with the computational intractability issue. For the data valuation problem, the structure was exploited to give easier-to-compute solutions in certain cases, in particular for the \(k\)-nearest neighbor problem [12; 15; 25; 26; 35; 41; 46]. Unlike data valuation, however, dataset valuation aims at quantifying the marginal contribution of a _whole dataset_ to a given ML task with respect to (w.r.t.) the datasets brought by other dataset owners. Although data and dataset valuation are related problems, they are different and the techniques developed for data valuation cannot be used for the dataset valuation problem that we study (we further develop this point in Section 2.3).

**Contributions.** We consider the dataset valuation problem. Following the ML literature, we model it as a cooperative game whose value function relates to the considered ML task, and aim at estimating the Shapley value to measure the dataset owners contribution. We propose a new way to address the computational intractability issue of the Shapley value. Instead of relying on generic MC approximation schemes, our approximation method leverages the structure of the dataset valuation problem as well as a convergence result for a key random variable of the problem. Our approximation behaves well in many cases, both theoretically and empirically. More specifically, our main contributions can be summarized as follows:

1. [leftmargin=*]
2. We propose DU-Shapley (Definition 3), a novel Shapley value approximation that exponentially reduces the number of utility function valuations required for the computation. This is the first dataset valuation approach leveraging the specific structure of the utility function.
3. Based on three different use-cases, we establish asymptotic and non-asymptotic theoretical guarantees for DU-Shapley, showing notably that it converges almost surely to the Shapley value as the number of dataset owners grows.
4. We assess the benefits of the proposed methodology using extensive numerical experiments on both Shapley value approximation and dataset valuation use-cases. We show, in particular, that DU-Shapley outperforms all considered MC approximations of the Shapley value.

**Additional Related Work.** Cooperative game theory has been applied to solve multi-agents ML problems beyond data and dataset valuation [6; 18; 29; 47]. In particular, the Shapley value has been used to solve several problems including variable selection [5], feature importance [7; 27; 28], or model interpretation [4]. In these problems, similarly to the data and dataset valuation problems, the computational intractability issue of the Shapley value is usually addressed via MC [2; 31; 32].

## 2 Problem Formulation and Main Concepts Involved

This section presents the dataset valuation problem we aim to solve, along with preliminaries including the definition and classical approximations of the Shapley value. For \(n\in\mathbb{N}\) and \(A\), we denote \([n]:=\{1,..,n\}\) and \(\mathrm{U}(A)\) the uniform distribution with support on \(A\).

### Generic Model

We consider a collaborative ML setting involving a set \(\mathcal{I}\) of \(I=|\mathcal{I}|\in\mathbb{N}^{*}\) dataset owners, also referred to as players in the sequel, who are willing to cooperate in order to solve a common ML problem. Each player \(i\in\mathcal{I}\) is assumed to possess an individual dataset \(\mathrm{D}_{i}=\{(x_{i}^{(j)},y_{i}^{(j)})\}_{j\in[n_{i}]}\)where \(x_{i}^{(j)}\in\mathcal{X}\subset\mathbb{R}^{d}\) stands for a feature vector, \(y_{i}^{(j)}\in\mathcal{Y}\) is a label, \(n_{i}=|\mathrm{D}_{i}|\) refers to the number of data points in \(\mathrm{D}_{i}\), and samples are drawn independently from a player-dependent distribution \(p_{i}\), i.e., \((x_{i}^{(j)},y_{i}^{(j)})\sim p_{i}\), for all \(j\in[n_{i}]\) and \(i\in\mathcal{I}\).

Our basic motivation is to quantify the incremental contribution that a given player \(i\in\mathcal{I}\) brings by sharing her dataset \(\mathrm{D}_{i}\) with other players towards solving some ML task. Hence, we are interested in scenarios in which, even though the data distribution might differ across players, they face a similar ML task, for instance the minimization of the expectation (with respect to \(p_{i}\)) of some loss function \(\ell(\hat{Y},Y)\), where \(\hat{Y}\) denotes a prediction of \(Y\). In such cases, players can usually learn from others' datasets, in the sense that given some \(X\), the optimal prediction \(\hat{Y}\) that minimizes \(\mathbb{E}[\ell(\hat{Y},Y)|X]\) is the same for all player. This holds, _e.g._, if the conditional distributions (or, in many cases, simply the conditional expectation) of \(y^{(j)}\) given \(x^{(j)}\) are the same but the marginal distributions of \(x^{(j)}\) differ.

To model this problem with full generality, we assume that the players \(i\in\mathcal{I}\) collaborate in solving an ML task whose success is measured through some abstract metric \(u\) that maps any dataset to a real number (say, the prediction accuracy in a classification problem). With a slight abuse of notation, for any coalition of players \(\mathcal{S}\subseteq\mathcal{I}\), we define \(u(\mathcal{S})=u(\mathrm{D}_{\mathcal{S}})\), where \(\mathrm{D}_{\mathcal{S}}:=\cup_{i\in\mathcal{S}}\mathrm{D}_{i}\). Hence, \(u:2^{\mathcal{I}}\rightarrow\mathbb{R}\) can be seen as a game-theoretical utility function that quantifies how well coalitions of players can solve the considered ML task based on the union of their datasets.

The following subsections provide three theoretical use-cases that instantiate the generic model and give specific utility functions \(u\) to illustrate the dataset valuation problem. Using different tools and techniques, Section 3 provides theoretical guarantees in each of them. These theoretical results are then complemented in Section 4 by numerical evidence of our proposed approach in more intricate practical problems on real data.

#### 2.1.1 Theoretical use-case 1: Non-parametric Regression

The first use case we shall investigate is quite generic and consists in non-parametric regression. We assume the existence of a function \(f^{*}\) such that \(y_{i}^{(j)}=f^{*}(x_{i}^{(j)})+\eta_{i}^{(j)}\) with \(\eta_{i}^{(j)}\) i.i.d., and a quadratic loss function. Without regularity assumption on \(f^{*}(\cdot)\), learning can be arbitrarily slow; hence it is usually assumed that this mapping is Lipschitz (or at least \(\beta\)-Holder [13; 45]).

The standard estimation method of \(f^{*}\) we shall consider is called the _regressogram_ or binning (also applied in [13] to study local differential privacy within regression) and consists in learning optimal piece-wise constant functions. More precisely, given some parameter \(B\in\mathbb{N}\)--chosen exogeneously as a function of the function regularity \(\beta\), the ambient dimension \(d\) and the total number \(n\) of datapoints, typically \(B\simeq n^{\nicefrac{{d}}{{(d+2\beta)}}}\)--, the feature space \(\mathcal{X}\) is partitioned into \(B\) cubic bins. The excess risk of learning \(f^{*}\) can then be decomposed into

\[\mathbb{E}\big{[}(\hat{f}(x)-f^{*}(x))^{2}\big{]}=\mathbb{E}\big{[}(\hat{f}(x )-\bar{f}(x))^{2}\big{]}+\mathbb{E}\big{[}(\bar{f}(x)-f^{*}(x))^{2}\big{]},\] (1)

where \(\hat{f}\) is the estimator of \(f^{*}\), \(\bar{f}(x):=\sum_{b\in[B]}\bar{f}_{b}\mathbbm{1}\{x\in b\}\), and \(\bar{f}_{b}\) is any value that \(f^{*}\) can take on the bin \(b\). The second term in (1) being agnostic to the players' datasets, the problem of measuring the contributions of the players to estimating \(f^{*}\) can be decomposed into measuring their contributions to estimating each \(\bar{f}_{b}\). In particular, the utility \(u(\mathcal{S})\) of a coalition \(\mathcal{S}\) can be defined, and split into the sum of \(B\) sub-utilities \(u_{b}(\mathcal{S})\) functions, as follows

\[u(\mathcal{S}):=-\mathbb{E}\big{[}(\hat{f}_{\mathcal{S}}(x)-\bar{f}(x))^{2} \big{]}=\sum\nolimits_{b\in[B]}-\mathbb{E}\big{[}(\hat{f}_{\mathcal{S},b}-\bar {f}_{b})^{2}\big{]}\mathbb{P}(x\in b)=:\sum\nolimits_{b\in[B]}u_{b}(\mathcal{S })\mathbb{P}(x\in b),\]

where \(\hat{f}_{\mathcal{S}}\) is the estimator of \(\bar{f}\) when using the datasets of all players in \(\mathcal{S}\) and \(\hat{f}_{\mathcal{S},b}\) is the estimator \(\bar{f}_{b}\) when using, for all players in \(\mathcal{S}\), the datasets of points in the bin \(b\). Interestingly, after this reduction, the problem is decomposed into \(B\) independent sub-problems--one per bin--, where the utility is a sole function of the number of data points used to estimate \(\bar{f}_{b}\), i.e., we can write \(u_{b}(\mathcal{S})=w_{b}(\sum_{i\in\mathcal{S}}n_{i,b})\) for some function \(w_{b}:\mathbb{N}\rightarrow\mathbb{R}\), where \(n_{i,b}\) is the number of data points that player \(i\) has in the bin \(b\). This last property motivates our second theoretical use-case.

#### 2.1.2 Theoretical use-case 2: Homogeneous case

The second theoretical setting considers a general learning problem (not necessarily restricted to regression) and supposes that all players have the same sampling distribution, i.e., it takes \(p_{i}=p\)for all \(i\in\mathcal{I}\). This homogeneity on the players allows to reduce the problem of measuring the contribution of the players to just counting the number of data points contributed by each of them. Formally, and similarly to the previous use-case, we suppose the existence of a function \(w:\mathbb{N}\rightarrow\mathbb{R}\) such that \(u(\mathcal{S})=w(\sum_{i\in\mathcal{S}}n_{i})\).

#### 2.1.3 Theoretical use-case 3: Heterogeneous Linear Regression - Local Differential Privacy

The third theoretical setting we consider is linear regression with random design and different variance of the features and labels per player. Although the setting is more general, one of the motivations behind it is standard linear regression with homogeneous data between players, but where players can purposely add noise when sharing their dataset (in order to provide Local Differential Privacy, for instance). Formally, for any \(i\in\mathcal{I}\), we consider the following linear model that generates the dataset \(\mathrm{D}_{i}\) of size \(n_{i}\):

\[y_{i}^{(j)}=x_{i}^{(j)}\theta+\eta_{i}^{(j)}\,,\text{ where }\eta_{i}^{(j)} \sim\mathrm{N}(0,\varepsilon_{i}^{2})\,,\text{ and }x_{i}^{(j)}\sim\mathrm{N}(0_{d}, \sigma_{i}^{2}\mathrm{I}_{d})\,,\text{ for any }j\in[n_{i}],\] (2)

with \(\theta\in\mathbb{R}^{d}\) a ground-truth parameter, \(\sigma_{i}\) positive and known, and \(\varepsilon_{i}\) the differential privacy level chosen by player \(i\). Under the linear regression framework defined in (2), and following [8], the utility function of a set \(\mathcal{S}\subseteq\mathcal{I}\) of players is defined by the negative expected mean square error over a hold-out dataset, i.e.,

\[u(\mathcal{S})=-\mathbb{E}\big{[}\big{(}x^{\top}\hat{\theta}_{\mathcal{S}}-x^{ \top}\theta\big{)}^{2}\big{]}\,,\] (3)

where the expectation is taken over the distribution \(p_{\mathrm{test}}\) of a hold-out testing datum \(x\in\mathbb{R}^{d}\), the sampling distributions \(\mathrm{N}(0,\sigma_{i}^{2}\mathrm{I}_{d})\) for all \(i\in\mathcal{S}\), and the linear regression error distributions \(\mathrm{N}(0,\varepsilon_{i}^{2}),\forall i\in\mathcal{S},j\in[n_{i}]\), and \(\hat{\theta}_{\mathcal{S}}\) stands for the generalized least square estimator defined by \(\hat{\theta}_{\mathcal{S}}=(X_{\mathcal{S}}^{\top}\Sigma_{\mathcal{S}}^{-1}X_ {\mathcal{S}})^{-1}X_{\mathcal{S}}^{\top}\Sigma_{\mathcal{S}}^{-1}Y_{\mathcal{ S}},\) where \(\Sigma_{\mathcal{S}}=\mathrm{diag}((\varepsilon_{i}^{2})_{i\in\mathcal{S}})\in \mathbb{R}^{|\mathcal{S}|\times|\mathcal{S}|}.\) The notations \(X_{\mathcal{S}}\) and \(Y_{\mathcal{S}}\) refer to the concatenation of \(\{X_{i}\}_{i\in\mathcal{S}}\) and \(\{Y_{i}\}_{i\in\mathcal{S}}\), respectively, and \(X_{i}\in\mathbb{R}^{n_{i}\times d}\) is defined by \(X_{i}=([x_{i}^{(1)}]^{\top},\dots,[x_{i}^{(n_{i})}]^{\top})^{\top}\) while \(Y_{i}\in\mathbb{R}^{n_{i}}\) is defined by \(Y_{i}=(y_{i}^{(1)},\dots,y_{i}^{(n_{i})})^{\top}\).

The following result provides a close-form expression for the utility function in this case:

**Proposition 1**.: _Let \(\mathcal{S}\) be a coalition of players and consider the value function as above. It follows,_

\[u(\mathcal{S})=\frac{-\mathrm{Tr}\big{[}\mathbb{E}\big{[}xx^{\top}\big{]} \big{]}}{q(\mathcal{S})-d-1},\text{ where }q(\mathcal{S}):=\left\lfloor\,\frac{\big{(}\sum_{i\in\mathcal{S}}\big{(} \nicefrac{{\sigma_{i}}}{{\varepsilon_{i}}}\big{)}n_{i}\big{)}^{2}}{\sum_{i \in\mathcal{S}}\big{(}\nicefrac{{\sigma_{i}}}{{\varepsilon_{i}}}\big{)}^{2}n _{i}}\,\right\rfloor\,,\text{ with the convention }q(\varnothing)=0.\]

_In particular, considering \(p_{\mathrm{test}}=\mathrm{N}(0,\mathrm{I}_{d})\), we get \(u(\mathcal{S})=\frac{d}{d+1-q(\mathcal{S})}\)._

Proposition 1 shows that, in this use-case, the utility function can be written as a function \(w(q(\mathcal{S}))\) of a scalar quantity \(q(\mathcal{S})\) that captures the datasets heterogeneity. Notice that in this use-case, if we add the homogeneity assumption that \(\sigma_{i}/\varepsilon_{i}=\sigma/\varepsilon\), for all \(i\in\mathcal{I}\), then the term \(q(\mathcal{S})\) becomes \(\sum_{i\in\mathcal{S}}n_{i}\) and, as a consequence, we get

\[u(\mathcal{S})=w(q(\mathcal{S}))=w\left(\sum\nolimits_{i\in\mathcal{S}}n_{i} \right)=\frac{d}{d+1-\sum_{i\in\mathcal{S}}n_{i}}.\]

Recall that, in the non-parametric regression use-case, it holds \(u(\mathcal{S})=\sum_{b\in[B]}\mathbb{P}(x\in b)w_{b}(q_{b}(\mathcal{S}))\) where \(q_{b}(\mathcal{S})=\sum_{i\in\mathcal{S}}n_{i,b}\). Therefore, in our three uses-cases, the utility of a coalition can be summarized as the function of some scalar quantity of interest. This observation will be useful to state later our theoretical results.

### Shapley Value

The Shapley value [40] is a classical solution concept in cooperative game theory to fairly allocate the total gains generated by a coalition of players. Given a utility function \(u\), the Shapley value of a player \(i\) is defined as the average marginal contribution of her dataset \(\mathrm{D}_{i}\) to all possible subsets of \(\{\mathrm{D}_{j}\}_{j\in\mathcal{I}\setminus\{i\}}\), built by aggregating the datasets of the other players. Formally, the Shapley value \(\varphi_{i}\) of player \(i\) writes

\[\varphi_{i}(u)=\frac{1}{|\Pi(\mathcal{I})|}\sum\nolimits_{\pi\in\Pi(\mathcal{I })}[u(\mathcal{P}_{i}^{\pi}\cup\{i\})-u(\mathcal{P}_{i}^{\pi})]\,,\] (4)where \(\Pi(\mathcal{I})\) refers to the set of permutations over \(\mathcal{I}\) and \(\mathcal{P}_{i}^{\pi}\) to the set of predecessors of player \(i\in\mathcal{I}\) in permutation \(\pi\in\Pi(\mathcal{I})\). The Shapley value of player \(i\) is equivalently expressed as

\[\varphi_{i}(u)=\frac{1}{I}\sum\nolimits_{\mathcal{S}\subseteq\mathcal{I}\setminus \{i\}}\left(\begin{matrix}I-1\\ |\mathcal{S}|\end{matrix}\right)^{-1}\left[u(\mathcal{S}\cup\{i\})-u( \mathcal{S})\right].\] (5)

The Shapley value has been commonly used in ML and cooperative game theory as it uniquely satisfies the following set of desirable properties.

1. _Efficiency._\(\sum_{i=1}^{I}\varphi_{i}(u)=u(\mathcal{I})\), i.e, the sum of all Shapley values is equal to the value of \(\mathcal{I}\).
2. _Symmetry._ If, for any \(\mathcal{S}\subseteq\mathcal{I}\setminus\{i_{1},i_{2}\}\), \(u(\mathcal{S}\cup\{i_{1}\})=u(\mathcal{S}\cup\{i_{2}\})\), then \(\varphi_{i_{1}}(u)=\varphi_{i_{2}}(u)\), i.e., whenever two players have the same marginal contributions, their Shapley values coincide.
3. _Dummy._ If, for any \(\mathcal{S}\subseteq\mathcal{I}\setminus\{i\}\), \(u(\mathcal{S}\cup\{i\})=u(\mathcal{S})\), then \(\varphi_{i}(u)=0\), i.e., whenever a player has null marginal contributions, her Shapley value is zero.
4. _Linearity._\(\varphi_{i}(u_{1}+u_{2})=\varphi_{i}(u_{1})+\varphi_{i}(u_{2})\), i.e., the Shapley value of sums of games is the sum of the Shapley values of the respective games.

**MC approximation of the Shapley Value.** Evaluating the Shapley value is unfortunately computationally expensive in general. As a consequence, many MC approximations have been considered by sampling with replacement \(T\) terms from the sum of either (4) or (5). Regarding (4), this boils down to considering the estimator

\[\hat{\varphi}_{i}(u)=\frac{1}{T}\sum\nolimits_{t=1}^{T}\left[u(\mathcal{P}_{i }^{\pi_{t}}\cup\{i\})-u(\mathcal{P}_{i}^{\pi_{t}})\right],\text{where }\pi_{t} \sim\mathrm{U}(\Pi(\mathcal{I})).\] (6)

### Data valuation vs Dataset valuation

A tentative, but naive, approach to solve the dataset valuation problem could be to run an auxiliary data-valuation algorithm on all the data and to assign to each dataset the sum of the values of its datapoints. We highlight the cons of this idea on a very simple, yet insightful example. Consider two datapoints \(x_{1}\) and \(x_{2}\), three datasets \(\mathrm{D}_{1}=\{x_{1}\}\), \(\mathrm{D}_{2}=\{x_{2}\}\), \(\mathrm{D}_{3}=\{x_{2},x_{2}\}\), and the following toy utility function \(u(\mathrm{D})=\mathbbm{1}\{x_{1},x_{2}\in\mathrm{D}\}\). In data valuation, any point \(x_{2}\) shall have the same value, as they are identical. In particular, a naive summation would value \(\mathrm{D}_{3}\) twice the value of \(\mathrm{D}_{2}\). In dataset valuation, and for this toy problem at hand, it is quite clear that both datasets should have the same value. Moreover, the Shapley values are \(1/6\) for \(\mathrm{D}_{2}\) and \(\mathrm{D}_{3}\) versus \(2/3\) for \(\mathrm{D}_{1}\).

The message here is twofold. Data valuation and dataset valuation are two fundamentally different concepts and one cannot directly reduce the latter to the former. This is actually true, and this is the second message, because the utility function \(u\) is highly non-linear (even for the regression task).

## 3 Discrete Uniform Shapley Value

This section introduces and studies our approximation scheme for the Shapley value. Section 3.1 shows an asymptotic property that gives the general intuition behind our approximation. The result holds for the three use-cases of Sections 2.1.1 to 2.1.3. Section 3.2 presents a general approximation methodology for dataset valuation and shows its almost surely convergence as the number of players grows for our three uses-cases. Section 3.3 studies the rate of convergence, first for the homogeneous setting (Section 2.1.2), and then leverages this result to obtain a similar one for the non-parametric regression setting (Section 2.1.1). All proofs are postponed to the supplementary material.

### Insights behind \(\mathtt{DU}\)-Shapley

The Shapley value, by re-arranging the coalitions \(\mathcal{S}\subseteq\mathcal{I}\setminus\{i\}\) by their cardinality in the sum in (5), can be equivalently expressed as

\[\varphi_{i}(u)=\mathbb{E}_{K\sim\mathrm{U}(\{0,\ldots,I-1\})}\mathbb{E}_{ \mathcal{S}\sim\mathrm{U}\left(2^{\mathcal{I}\setminus\{i\}}_{K})}\left[u( \mathcal{S}\cup\{i\})-u(\mathcal{S})\right]\,,\] (7)

where \(2^{\mathcal{I}\setminus\{i\}}_{K}\) denotes the subsets of \(\mathcal{I}\setminus\{i\}\) of cardinality \(K\). In our three uses-cases, it follows that

\[\varphi_{i}(u)=\varphi_{i}(w)=\mathbb{E}_{K\sim\mathrm{U}(\{0,\ldots,I-1\})} \mathbb{E}_{\mathcal{S}\sim\mathrm{U}\left(2^{\mathcal{I}\setminus\{i\}}_{K} \right)}\left[w(q(\mathcal{S}\cup\{i\}))-w(q(\mathcal{S}))\right]\,,\] (8)where \(w:\mathbb{R}_{+}\to\mathbb{R}\) is such that \(u(\mathcal{S})=w(q(\mathcal{S}))\) for any \(\mathcal{S}\subseteq\mathcal{I}\), and \(q(\mathcal{S})\) is the scalar quantity of interest identified in Sections 2.1.1 to 2.1.3 for each use-case:

\[q(\mathcal{S}):=\left\lfloor\frac{\left(\sum_{i\in\mathcal{S}}\gamma_{i}n_{i} \right)^{2}}{\sum_{i\in\mathcal{S}}\gamma_{i}^{2}n_{i}}\right\rfloor,\text{ where, for any }i\in\mathcal{I},\gamma_{i}=\left\{\begin{array}{cc}1&\text{ for the second use-case},\\ \sigma_{i}/\varepsilon_{i}&\text{ for the third use-case},\end{array}\right.\] (9)

and for the first use-case, \(q_{b}(\mathcal{S})\) is analogously defined at every bin, with \(\gamma_{i}^{b}=1\) for all players and all bins. We remark that the definition of \(q(\mathcal{S})\) in the first and second use-cases is not restricted to linear regression. Equation (8) explicitly reveals a key random variable, namely \(q(\mathcal{S})\). Interestingly, Figure 1 suggests that \(q(\mathcal{S})\) converges in distribution to a uniform random variable as the number of players increases (with i.i.d. datasets sizes). Theorem 2 proves this result formally for any \((\gamma_{i})_{i\in\mathcal{I}}\).

**Theorem 2**.: _Let \(\{n_{i},\gamma_{i}\}_{i\in[I]}\) be two sequences of positive numbers such that the following limits_

\[\lim_{I\to\infty}\frac{1}{I}\sum\nolimits_{i\in[I]}n_{i}\gamma_{i }=\mu_{A},\quad\lim_{I\to\infty}\frac{1}{I}\sum\nolimits_{i\in[I]}(n_{i} \gamma_{i}-\mu_{A})^{2}=\sigma_{A}^{2},\] \[\lim_{I\to\infty}\frac{1}{I}\sum\nolimits_{i\in[I]}n_{i}\gamma_{i }^{2}=\mu_{B},\quad\lim_{I\to\infty}\frac{1}{I}\sum\nolimits_{i\in[I]}(n_{i} \gamma_{i}^{2}-\mu_{B})^{2}=\sigma_{B}^{2}\,,\]

_all exist, for some constants \(\mu_{A},\mu_{B},\sigma_{A},\sigma_{B}>0\). Let \(K\sim\mathrm{U}(\{0,\ldots,I\})\), \(\mathcal{S}_{K}\sim\mathrm{U}([2^{\mathcal{I}}_{K}])\). Then, almost surely, \(\frac{q(\mathcal{S}_{K})}{q(\mathcal{I})}\xrightarrow{I\to\infty}\mathrm{U}([ 0,1])\)._

### Discrete Uniform Shapley value

The Shapley value re-arrangement in (7) exposes the main tool behind our approximation: it is enough to approximate the distribution of the random variable \(\mathrm{D}_{\mathcal{S}}\) that takes values on the subsets of \(\mathrm{D}_{-i}:=\cup_{j\in\mathcal{I}\setminus\{i\}}\mathrm{D}_{j}\) (recall that \(u(\mathcal{S})=u(\mathrm{D}_{\mathcal{S}})\)). Theorem 2, taking the example of the second use-case for intuition, indicates that these datasets have uniformly distributed numbers of points in the limit. Generalizing this intuition, we propose to approximate \(\mathrm{D}_{\mathcal{S}}\) by taking \(I\) samples of increasing size from the pool \(\mathrm{D}_{-i}\) by sampling data points uniformly. This leads to the following definition of DU-Shapley for our generic model:

**Definition 3**.: _[DU-Shapley] For any \(i\in\mathcal{I}\), the discrete uniform Shapley value (DU-Shapley) of the \(i\)-th player, denoted by \(\psi_{i}\), is given by_

\[\psi_{i}(u):=\frac{1}{I}\sum\nolimits_{k=0}^{I-1}u(\mathrm{D}^{(k)}\cup \mathrm{D}_{i})-u(\mathrm{D}^{(k)}),\]

_where \(\mathrm{D}^{(k)}\) is a set of data points uniformly sampled without replacement from \(\mathrm{D}_{-i}\) of size \(k\mu_{-i}\), with \(\mu_{-i}=\frac{1}{(I-1)}|\mathrm{D}_{-i}|\)._

Compared to the Shapley value defined in (5), which involves \(2^{I}\) terms to compute, note that DU-Shapley only involves \(I\) terms and hence it presents an exponential reduction of the number of utility function evaluations. Of course, these computational savings come at the cost of some bias. The latter is precisely quantified in Section 3.3 for our first two use-cases.

By definition, DU-Shapley is a random variable which depends on the sampled data points. However, whenever \(u(\mathcal{S})=w(q(\mathcal{S}))\), with \(q(\mathcal{S})\) some scalar quantify of interest, as in our use-cases, we

Figure 1: Distribution of \(q(\mathcal{S})/q(\mathcal{I})\) when \(\mathcal{S}\) is sampled as in (8) (i.e., first sample a size \(K\) uniformly, then sample a coalition \(\mathcal{S}\) of size \(K\) uniformly). (left) \(I=10\), (middle) \(I=50\), (right) \(I=500\). We considered \(10^{4}\) samples for each random variable, and the third use-case with \(n_{i}\sim\mathrm{U}([100])\) and \(\sigma_{i}/\varepsilon_{i}\sim\mathrm{U}([10])\) for each \(i\in\mathcal{I}\).

can get rid of the stochastic nature of DU-Shapley by considering \(I\) real values from well chosen intervals. In particular, in our uses-cases, DU-Shapley boils down to:

\[\psi_{i}(w)=\frac{1}{I}\sum\nolimits_{k=0}^{I-1}w(\bar{q}_{i}^{k})-w(\bar{q}_{-i }^{k}),\] (10)

where

\[\bar{q}_{i}^{k}:=\bigg{\lfloor}\frac{(\gamma_{i}n_{i}+\frac{k}{I-1}\sum_{j\in \mathcal{I}\setminus\{i\}}\gamma_{j}n_{j})^{2}}{\gamma_{i}^{2}n_{i}+\frac{k}{ I-1}\sum_{j\in\mathcal{I}\setminus\{i\}}\gamma_{j}^{2}n_{j}}\bigg{\rfloor}\text{ and }\bar{q}_{-i}^{k}:=\bigg{\lfloor}\frac{k}{I-1}\cdot\frac{(\sum_{j\in \mathcal{I}\setminus\{i\}}\gamma_{j}n_{j})^{2}}{\sum_{j\in\mathcal{I}\setminus \{i\}}\gamma_{j}^{2}n_{j}}\bigg{\rfloor}.\]

We remark the notation abuse as we should write \(\psi(w\circ q)\). For simplicity, we omit the composition and only write \(\psi(w)\). Equation (10) coincides exactly with Definition 3 in the first two use-cases, i.e., when \(\gamma_{j}=\gamma\) for all \(j\in\mathcal{I}\). Indeed, as the random datasets \(\mathrm{D}^{(k)}\) have a fixed size and the value function only looks at the number of data points within the coalition, we obtain,

\[\psi_{i}(u) =\frac{1}{I}\sum_{k=0}^{I-1}u(\mathrm{D}^{(k)}\cup\mathrm{D}_{i}) -u(\mathrm{D}^{(k)})=\frac{1}{I}\sum_{k=0}^{I-1}w(|\mathrm{D}^{(k)}\cup \mathrm{D}_{i}|)-w(|\mathrm{D}^{(k)}|)\] \[=\frac{1}{I}\sum_{k=0}^{I-1}w(k\mu_{-i}+n_{i})-w(k\mu_{-i})=\psi_ {i}(w).\]

For the third use-case, Equation (10) is an approximation that comes from assuming that, for any \(j\in\mathcal{I}\setminus\{i\}\), \(|\mathrm{D}_{j}\cap\mathrm{D}^{(k)}|=k\cdot\frac{n_{j}}{I-1}\), which holds with high probability for large values of \(I\), since

\[q(\mathrm{D}^{(k)}\cup\mathrm{D}_{i})=\bigg{\lfloor}\frac{\left(\gamma_{i}n_{ i}+\sum_{j\in\mathcal{I}\setminus\{i\}}\gamma_{j}\cdot|\mathrm{D}_{j}\cap \mathrm{D}^{(k)}|\right)^{2}}{\gamma_{i}^{2}n_{i}+\sum_{j\in\mathcal{I}\setminus \{i\}}\gamma_{j}^{2}\cdot|\mathrm{D}_{j}\cap\mathrm{D}^{(k)}|}\bigg{\rfloor}.\]

Theorem 2 implies the following result.

**Corollary 4**.: _Let \(\varphi_{i}\) and \(\psi_{i}\) be, respectively, the Shapley value (5) and the DU-Shapley (10) of player \(i\). Then, in our three uses-cases, it holds, \(\lim_{I\to\infty}|\varphi_{i}-\psi_{i}|=0\) almost surely._

While our theoretical results are based on Equation (10) for the cases where \(u(\mathcal{S})=w(q(\mathcal{S}))\), we will see through numerical experiments that Definition 3 gives good results in more general cases.

### Non-Asymptotic Theoretical Guarantees

Corollary 4 states asymptotic guarantees for DU-Shapley. In this section, we show non-asymptotic results that give the convergence rate for the first two uses-cases.1 Recall that in non-parametric estimation, the utility writes as \(u(\mathcal{S})=\sum_{b\in[B]}u_{b}(\mathcal{S})\mathbb{P}(x\in b)\), and therefore, by the linearity axiom of the Shapley value, for any \(i\in\mathcal{I},\varphi_{i}(u)=\sum_{b\in[B]}\varphi_{i}(u_{b})\mathbb{P}(x \in b)\). As a consequence, in order to estimate \(\varphi_{i}(u)\), it is enough to compute each \(\varphi_{i}(u_{b})\). In particular, the Shapley value approximation error over the whole feature space becomes a simple aggregation of the Shapley value approximation errors over the bins. We focus firstly on bounding the bias of our method in the homogeneous use-case to then extend it to the non-parametric regression case.

Footnote 1: A similar result, albeit more technical, can be shown with the same arguments for the third use-case.

As in the homogeneous use-case the utility function writes as \(u(\mathcal{S})=w(\sum_{i\in\mathcal{I}}n_{i})\), we consider the following regularity assumptions on \(w\).

**H1**.: _The function \(w:\mathbb{R}_{+}\to\mathbb{R}\) is increasing, twice continuously differentiable, and such that \(\lim_{n\to\infty}n^{2}|w^{(2)}(n)|<\infty\) (where \(w^{(2)}\) represents the second derivative)._

Monotonicity is a natural assumption in our framework as, the more data, the more precise the ML prediction is expected to be. The condition over the limit aims at controlling the growth behavior of the utility function and it is automatically satisfied whenever \(w\) is bounded and \(w^{(2)}\) is monotone, by the mean value theorem. Theorem 5 bounds the bias of DU-Shapley for the homogeneous use-case.

**Theorem 5**.: _Under Assumption **H1**, there exists a constant \(\kappa>0\), such that, for any \(i\in\mathcal{I}\), it holds,_

\[\big{|}\varphi_{i}-\psi_{i}\big{|}\leq\frac{\kappa}{(I-1)\mu_{-i}^{2}}\left( \sigma_{-i}^{2}(1+\ln(I-1))+\zeta_{-i}\right),\]_where \(\varphi_{i}\) and \(\psi_{i}\) are respectively the Shapley value and the DU-Shapley of player \(i\), \(\mu_{-i}=\frac{1}{(I-1)}|\mathtt{D}_{-i}|\) is the average dataset size of all players but \(i\), \(\sigma^{2}_{-i}=\frac{1}{I-1}\sum_{j\in\mathcal{I}\setminus\{i\}}(n_{j}-\mu_{- i})^{2}\) their empirical variance, and \(\zeta_{-i}\) measures the variability of the dataset sizes across players. Formally, it is defined as \(\zeta_{-i}:=R_{-i}^{2}\tau_{-i}^{2}/4n_{-i}^{\max}\) where \(R_{-i}:=\max_{j\in\mathcal{I}\setminus\{i\}}|n_{j}-\mu_{-i}|\), \(n_{-i}^{\max}:=\max_{j\in\mathcal{I}\setminus\{i\}}n_{j}\), and \(\tau_{-i}:=n_{-i}^{\max}/\min_{j\in\mathcal{I}\setminus\{i\}}n_{j}\)._

The full proof of Theorem 5 is included in Appendix C.3 and it relies on controlling the absolute value of \(\mathbb{E}[w(\mu_{-i}K)-w(\sum_{j\in\mathcal{S}}n_{j})]\), where \(K\sim\text{U}(\{0,...,I-1\})\) and \(\mathcal{S}\) is the random variable in (7). Using a second order Taylor expansion, the problem is reduced to controlling the term related to the second derivative of \(w\) by using the regularity assumptions in **H**1.

As advertised before, Theorem 5 can be directly generalized to the non-parametric use-case, since,

\[u(\mathcal{S})=\sum\nolimits_{b\in[B]}w_{b}\bigg{(}\sum\nolimits_{i\in \mathcal{S}}n_{i,b}\bigg{)}\mathbb{P}(x\in b),\ \ \text{for}\ n_{i,b}=|\{(x,y)\in\text{D}_{j},x\in b\}|.\]

**Corollary 6**.: _Under Assumption **H**1 for all functions \(w_{b}\), there exist constants \(\kappa_{b}>0\), such that, for any \(i\in\mathcal{I}\), it holds that_

\[\big{|}\varphi_{i}-\psi_{i}\big{|}\leq\sum\nolimits_{b\in[B]}\frac{\kappa_{b }\mathbb{P}(x\in b)}{(I-1)\mu_{-i,b}^{2}}\left(\sigma_{-i,b}^{2}(1+\ln(I-1))+2 \zeta_{-i,b}\right),\] (11)

_where \(\varphi_{i}\) and \(\psi_{i}\) are respectively the Shapley value and the DU-Shapley of player \(i\), and all terms are equivalently defined to Theorem 5 at each bin \(b\in[B]\)._

The upper bound in (11) depends on natural quantities related to the dataset valuation problem described in Section 2.1 at each bin, such as the first two moments \(\mu_{-i,b}\) and \(\sigma_{-i,b}\) of the datasets' size distribution. More precisely, the error increases when there are some outlier players with a very small or large dataset size. This behavior is expected since, in this particular setting, the random variable inside of the Shapley value differs from a uniform random variable. As showcased in Theorem 2, the error vanishes when the number of players \(I\) tends towards infinity.

## 4 Numerical Experiments

We illustrate the benefits of DU-Shapley by measuring numerically three properties: (1) how well DU-Shapley approximates the Shapley value in real data, (2) how many (theoretical) iterations need other methods to achieve the same accuracy level than DU-Shapley, and (3) how well DU-Shapley performs in classical dataset valuation tasks with real data. Appendix A.1 complements the results by a complexity comparison between our method and SVARM[22] and Appendix A.2 by experiments on synthetic data. The experiments strongly suggest that DU-Shapley performs well in all tasks.

### Approximating the Shapley Value in Real-World Data

We consider the real-world datasets in Mitchell et al. [32], whose details are provided in Table 3 in the appendix. To tackle these problems we consider logistic regression models and gradient-boosted decision trees (GBDT). For classification tasks, the utility function has been taken as the expected accuracy of the trained logistic regression model over a hold-out testing set while for regression tasks, the utility function corresponds to the averaged MSE over a hold-out testing set. In both cases we took a hold-out testing set with 10% of the size of the training dataset. For each dataset, we considered two worst-case scenarios for our method, namely \(I=10\) players and \(I=20\) players.

Starting from the datasets in Table 3, we heterogeneously allocate datasets to the players. We compare ourselves with two approaches, referred to as MC-Shapley, for the standard MC approximation defined in (6), and MC-anti-Shapley that considers, in addition, antithetic sampling [32]. We compute the averaged MSE across all players between the true Shapley value and each estimator.

Since computing the marginal contributions in this experiment requires re-training, which is clearly not feasible for a large number of epochs, we chose to restrict ourselves to 20 steps of stochastic gradient descent for logistic regression and 20 boosting iterations for GBDTs. For MC-based approaches, we considered \(I\) samples to compare those approximations with the proposed methodology on a fair basis, i.e., associated to the same computational budget.

Table 1 depicts the results. We clearly see that, even in the worst-case scenario where the number of players is small and far from the theoretical assumptions from Section 3.3, DU-Shapley competes favorably with the MC-based methods.

### Complexity of Computing the Shapley Values of all Players

We have looked at the number of iterations that DataShapley and the _Improved Group Testing-Based_ method [46] (IGTB) require to achieve DU-Shapley's accumulated bias, formally given by

\[\mathrm{DUbias}(I):=\frac{\kappa}{I-1}\biggl{(}\sum\nolimits_{i\in\mathcal{I }}\bigl{(}\frac{9\sigma_{-i}^{2}(1+\log(I-1))+\zeta_{-i}}{(\mu_{-i})^{4}} \bigr{)}^{1/2}.\]

To do so, we have replaced \(\varepsilon=\mathrm{DUbias}(I)\), respectively, in the formula in Section 4.1 in [16] and Equation 5 in [46], with a value function motivated from our third use-case under the homogeneity assumption \(\sigma_{i}/\varepsilon_{i}=\sigma/\varepsilon\) for all \(i\in\mathcal{I}\). The results are illustrated in Figure 2. Remark DU-Shapley requires \(I^{2}\) iterations to compute all Shapley values. We observe that in all tested instances, both methods require a higher number of iterations to achieve the same error than DU-Shapley.

### Applying DU-Shapley to dataset valuation problems

We considered non-tabular datasets used in [17], namely bbc-embedding, IMDB-embedding, both text datasets, and CIFAR10-embedding, an image dataset. Feature embedding have been generated

\begin{table}
\begin{tabular}{c|c c|c c|c c|c c} \hline \hline Dataset & \multicolumn{2}{c|}{adult} & \multicolumn{2}{c|}{breast-cancer} & \multicolumn{2}{c|}{bank} & \multicolumn{2}{c}{cal-housing} \\ \hline Players & 10 & 20 & 10 & 20 & 10 & 20 & 10 & 20 \\ \hline DU-Shapley & \(\mathbf{2.10^{-3}}\) & \(\mathbf{6.10^{-4}}\) & \(\mathbf{3.10^{-3}}\) & \(\mathbf{1.10^{-4}}\) & \(\mathbf{5.10^{-2}}\) & \(\mathbf{4.10^{-3}}\) & \(\mathbf{1.10^{-2}}\) & \(\mathbf{3.10^{-3}}\) \\ MC-Shapley & \(1.10^{-2}\) & \(4.10^{-3}\) & \(3.10^{-2}\) & \(1.10^{-3}\) & \(9.10^{-2}\) & \(6.10^{-2}\) & \(5.10^{-2}\) & \(2.10^{-2}\) \\ MC-anti-Shapley & \(8.10^{-3}\) & \(2.10^{-3}\) & \(1.10^{-2}\) & \(8.10^{-4}\) & \(8.10^{-2}\) & \(4.10^{-2}\) & \(3.10^{-2}\) & \(1.10^{-2}\) \\ \hline \hline \multicolumn{2}{c|}{Dataset} & \multicolumn{2}{c|}{make-regression} & \multicolumn{2}{c|}{year} \\ \hline Players & 10 & 20 & 10 & 20 & \\ \hline DU-Shapley & \(\mathbf{9.10^{-2}}\) & \(\mathbf{2.10^{-2}}\) & \(\mathbf{1.10^{-3}}\) & \(\mathbf{7.10^{-4}}\) \\ MC-Shapley & \(4.10^{-1}\) & \(3.10^{-1}\) & \(5.10^{-3}\) & \(1.10^{-3}\) \\ MC-anti-Shapley & \(4.10^{-1}\) & \(2.10^{-1}\) & \(5.10^{-3}\) & \(1.10^{-3}\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Worst-case comparison between DU-Shapley and competitors, for real-world datasets considered in Table 3. We report the averaged MSE across all players w.r.t. the exact Shapley value.

Figure 2: Iterations required by DataShapley and the Improved Group Testing-Based method to achieve DU-Shapley’s accumulated bias with function \(w(n_{S})=1-\frac{10^{k(\mathcal{I})}}{10^{k(\mathcal{I})}+n_{S}}\), where \(n_{S}\) is the number of data points of the coalition \(S\subseteq\mathcal{I}\), and \(k(\mathcal{I}):=\lfloor\log(\sum_{i\in\mathcal{I}}n_{i})\rfloor-1\) is a normalization factor. (top) \(\delta=0.01\), (bottom) \(\delta=0.1\), (left) \(n_{\max}=10\), (middle) \(n_{\max}=50\), (right) \(n_{\max}=100\).

using pretrained DistilBERT and ResNet50 models, respectively. In addition we have adapted three baselines from data valuation to our setting: Leave-One-Out (LOO), DataShapley, and KNN-Shapley. Appendix B.2 gives the implementations details. For these datasets associated to classification problems, we used a multi-layer perceptron classifier as prediction model.

We have considered three dataset valuation problems, none of them needing the real Shapley values, which allows us to increase the number of players w.r.t. the experiments in Section 4.1. We investigated noisy label detection (NLD), dataset removal (DR), and dataset addition (DA) [17]. For NLD, we used as a metric the F1-score (the larger the better). For DR, we used the testing accuracy (the lesser the better). For DA, we used the testing accuracy (the lesser the better).

We considered splitting the dataset across \(I=100\) players. The results are summarized in Table 2. We observe that DU-Shapley has competitive results compared to classical baselines despite of the fact that none of the considered cases verifies the structural assumptions from Section 3.3. In addition, we can see that DU-Shapley tends to have similar and even better results as DataShapley (which is a MC based method). This is in line with our theory as, for larger number of players, DU-Shapley tends to better estimate the true Shapley value.

## 5 Conclusion

We model the dataset valuation problem as a cooperative game and design a Shapley value approximation, named DU-Shapley, that exploits the underlying structure of the utility function and exponentially reduces the number of functions valuations required for the computation. In three different uses-cases, DU-Shapley is proved to almost surely converge to the real Shapley value as the number of players grows. Moreover, we find the rate of convergence, which depends only on natural parameters of dataset valuation. Numerical experiments showcase that DU-Shapley performs well in approximating the Shapley value and performing dataset valuation tasks, even when the assumptions needed for the theoretical guarantees do not hold, and it has a good complexity when computing the Shapley values of all players.

**Limitations of our method**. Our non-asymptotic bound for the non-parametric regression setting in Corollary 6 indicates that DU-Shapley works better when agents' datasets are _regular_ in the sense that they have similar sizes. Hence, a limitation of our approximation is that it may work poorly in settings where some players have large datasets compared to others, as the distribution of the random variable within the Shapley value drives apart from being uniform. Moreover, our convergence result in Theorem 2 (for all use-cases) assume the existence of limits, which roughly requires that heterogeneity between players--in terms of both dataset size and variance--can be bounded. This also indicates that convergence may be not be guaranteed if the heterogeneity is arbitrarily high.

\begin{table}
\begin{tabular}{l|c c c c c|c c c c c} \hline \hline Dataset & \multicolumn{5}{c|}{CIFAR 10} & \multicolumn{5}{c}{BBC} \\ \hline \multirow{2}{*}{Problem} & \multicolumn{3}{c}{NLD} & \multicolumn{3}{c}{DR} & \multicolumn{3}{c|}{DA} & \multicolumn{3}{c}{NLD} & \multicolumn{3}{c}{DR} & \multicolumn{3}{c}{DA} \\  & 5\% & 15\% & 5\% & 15\% & 5\% & 15\% & 5\% & 15\% & 5\% & 15\% & 5\% & 15\% \\ \hline Random & 0.11 & 0.19 & 0.61 & 0.60 & 0.25 & 0.41 & 0.11 & 0.19 & 0.90 & 0.88 & 0.68 & 0.81 \\ LOO & 0.13 & 0.18 & 0.62 & 0.60 & 0.15 & 0.32 & 0.11 & 0.17 & 0.90 & 0.88 & 0.61 & 0.77 \\ DataShapley & 0.13 & 0.25 & 0.61 & 0.59 & 0.12 & 0.18 & 0.12 & 0.20 & 0.89 & 0.87 & 0.08 & 0.12 \\ KNN-Shapley & **0.14** & 0.28 & **0.60** & 0.57 & 0.12 & 0.15 & **0.19** & 0.29 & **0.88** & 0.86 & 0.13 & 0.12 \\ DU-Shapley & **0.14** & **0.30** & 0.61 & **0.55** & **0.11** & **0.14** & 0.18 & **0.34** & 0.89 & **0.85** & **0.07** & **0.11** \\ \hline \hline \multicolumn{10}{l}{Dataset} & \multicolumn{5}{c}{IMBD} \\ \hline \multirow{2}{*}{Problem} & \multicolumn{3}{c}{NLD} & \multicolumn{3}{c}{DR} & \multicolumn{3}{c}{DA} \\  & 5\% & 15\% & 5\% & 15\% & 5\% & 5\% & 15\% \\ \hline Random & 0.10 & 0.16 & 0.77 & 0.75 & 0.62 & 0.68 \\ LOO & 0.11 & 0.18 & 0.77 & 0.74 & 0.53 & 0.59 \\ DataShapley & 0.17 & 0.28 & **0.75** & 0.69 & 0.36 & **0.33** \\ KNN-Shapley & **0.18** & 0.29 & 0.76 & 0.68 & 0.41 & 0.37 \\ DU-Shapley & **0.18** & **0.32** & 0.76 & **0.66** & **0.33** & 0.34 \\ \hline \hline \end{tabular}
\end{table}
Table 2: Comparison between DU-Shapley and competitors for real-world datasets considered in [17] in Noisy label detection, Dataset Removal and Dataset Addition.

## Acknowledgments

This research was supported in part by the French National Research Agency (ANR) in the framework of the PEPR IA FOUNDRY project (ANR-23-PEIA-0003) and through the grant DOOM ANR-23-CE23-0002. It was also funded by the European Union (ERC, Ocean, 101071601). Views and opinions expressed are however those of the author(s) only and do not necessarily reflect those of the European Union or the European Research Council Executive Agency. Neither the European Union nor the granting authority can be held responsible for them.

## References

* Agarwal et al. [2019] Anish Agarwal, Munther Dahleh, and Tuhin Sarkar. A Marketplace for Data: An Algorithmic Solution. In _Proceedings of the ACM Conference on Economics and Computation_, page 701-726, 2019.
* Castro et al. [2009] Javier Castro, Daniel Gomez, and Juan Tejada. Polynomial calculation of the Shapley value based on sampling. _Computers & Operations Research_, 36(5):1726-1730, 2009.
* Chalkiadakis et al. [2011] Georgios Chalkiadakis, Edith Elkind, and Michael J. Wooldridge. _Computational Aspects of Cooperative Game Theory_. Morgan & Claypool Publishers, 2011.
* Chen et al. [2019] Jianbo Chen, Le Song, Martin J. Wainwright, and Michael I. Jordan. L-Shapley and C-Shapley: Efficient Model Interpretation for Structured Data. In _International Conference on Learning Representations_, 2019.
* Cohen et al. [2005] Shay Cohen, Eytan Ruppin, and Gideon Dror. Feature Selection Based on the Shapley Value. In _International Joint Conference on Artificial Intelligence_, page 665-670, 2005.
* Cong et al. [2020] Mingshu Cong, Han Yu, Xi Weng, and Siu Ming Yiu. A game-theoretic framework for incentive mechanism design in federated learning. _Federated Learning: Privacy and Incentive_, pages 205-222, 2020.
* Covert et al. [2020] Ian C. Covert, Scott Lundberg, and Su-In Lee. Understanding Global Feature Contributions with Additive Importance Measures. In _Advances in Neural Information Processing Systems_, 2020.
* Donahue and Kleinberg [2021] Kate Donahue and Jon Kleinberg. Model-sharing games: Analyzing federated learning under voluntary participation. _Proceedings of the AAAI Conference on Artificial Intelligence_, 35(6):5303-5311, May 2021. URL https://ojs.aaai.org/index.php/AAAI/article/view/16669.
* Donahue and Kleinberg [2021] Kate Donahue and Jon Kleinberg. Optimality and Stability in Federated Learning: A Game-theoretic Approach. In _Advances in Neural Information Processing Systems_, volume 34, 2021.
* Ghorbani and Zou [2019] Amirata Ghorbani and James Zou. Data Shapley: Equitable Valuation of Data for Machine Learning. In _International Conference on Machine Learning_, 2019.
* Ghorbani et al. [2020] Amirata Ghorbani, Michael Kim, and James Zou. A Distributional Framework For Data Valuation. In _International Conference on Machine Learning_, 2020.
* Ghorbani et al. [2022] Amirata Ghorbani, James Zou, and Andre Esteva. Data shapley valuation for efficient batch active learning. In _2022 56th Asilomar Conference on Signals, Systems, and Computers_, pages 1456-1462. IEEE, 2022.
* Gyorfi and Kroll [2022] Laszlo Gyorfi and Martin Kroll. On rate optimal private regression under local differential privacy. _arXiv preprint arXiv:2206.00114_, 2022.
* Hoeffding [1963] Wassily Hoeffding. Probability Inequalities for Sums of Bounded Random Variables. _Journal of the American Statistical Association_, 58(301):13-30, 1963.
* Jia et al. [2019] Ruoxi Jia, David Dao, Boxin Wang, Frances Ann Hubis, Nezihe Merve Gurel, Bo Li, Ce Zhang, Costas Spanos, and Dawn Song. Efficient Task-Specific Data Valuation for Nearest Neighbor Algorithms. _Proc. VLDB Endow._, 12(11):1610-1623, 2019.

* [16] Ruoxi Jia, David Dao, Boxin Wang, Frances Ann Hubis, Nick Hynes, Nezihe Merve Gurel, Bo Li, Ce Zhang, Dawn Song, and Costas J. Spanos. Towards Efficient Data Valuation Based on the Shapley Value. In _International Conference on Artificial Intelligence and Statistics_, 2019.
* [17] Kevin Jiang, Weixin Liang, James Y Zou, and Yongchan Kwon. Opendataval: a unified benchmark for data valuation. _Advances in Neural Information Processing Systems_, 36, 2023.
* [18] Jiawen Kang, Zehui Xiong, Dusit Niyato, Shengli Xie, and Junshan Zhang. Incentive mechanism for reliable federated learning: A joint optimization approach to combining reputation and contract theory. _IEEE Internet of Things Journal_, 6(6):10700-10714, 2019.
* [19] R. Kelley Pace and Ronald Barry. Sparse spatial autoregressions. _Statistics & Probability Letters_, 33(3):291-297, 1997.
* [20] Andre I Khuri, Thomas Mathew, and Daan G Nel. A test to determine closeness of multivariate satterthwaite's approximation. _Journal of Multivariate Analysis_, 51(1):201-209, 1994.
* [21] Ron Kohavi. Scaling up the Accuracy of Naive-Bayes Classifiers: A Decision-Tree Hybrid. In _International Conference on Knowledge Discovery and Data Mining_, 1996.
* [22] Patrick Kolpaczki, Viktor Bengos, Maximilian Muschalik, and Eyke Hullermeier. Approximating the Shapley value without marginal contributions. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 38, pages 13246-13255, 2024.
* [23] Yongchan Kwon and James Zou. Beta Shapley: a Unified and Noise-reduced Data Valuation Framework for Machine Learning. In _International Conference on Artificial Intelligence and Statistics_, pages 8780-8802, 2022.
* [24] Yongchan Kwon, Manuel A. Rivas, and James Zou. Efficient Computation and Analysis of Distributional Shapley Values. In _International Conference on Artificial Intelligence and Statistics_, pages 793-801, 2021.
* [25] Weixin Liang, James Zou, and Zhou Yu. Beyond user self-reported likert scale ratings: A comparison model for automatic dialog evaluation. _arXiv preprint arXiv:2005.10716_, 2020.
* [26] Weixin Liang, Kai-Hui Liang, and Zhou Yu. Herald: an annotation efficient method to detect user disengagement in social conversations. _arXiv preprint arXiv:2106.00162_, 2021.
* [27] Scott M. Lundberg and Su-In Lee. A Unified Approach to Interpreting Model Predictions. In _Advances in Neural Information Processing Systems_, page 4768-4777, 2017.
* [28] Scott M. Lundberg, Gabriel Erion, Hugh Chen, Alex DeGrave, Jordan M. Prutkin, Bala Nair, Ronit Katz, Jonathan Himmelfarb, Nisha Bansal, and Su-In Lee. From local explanations to global understanding with explainable AI for trees. _Nature Machine Intelligence_, 2(1):56-67, 2020.
* [29] Lingjuan Lyu, Xinyi Xu, Qian Wang, and Han Yu. Collaborative fairness in federated learning. _Federated Learning: Privacy and Incentive_, pages 189-204, 2020.
* [30] Olvi L. Mangasarian, W. Nick Street, and William H. Wolberg. Breast Cancer Diagnosis and Prognosis Via Linear Programming. _Operations Research_, 43(4):570-577, 1995.
* [31] Irwin Mann and Lloyd S. Shapley. _Values of Large Games, IV: Evaluating the Electoral College by Montecarlo Techniques_. RAND Corporation, Santa Monica, CA, 1960.
* [32] Rory Mitchell, Joshua Cooper, Eibe Frank, and Geoffrey Holmes. Sampling Permutations for Shapley Value Estimation. _Journal of Machine Learning Research_, 23(43):1-46, 2022.
* [33] Sergio Moro, Paulo Cortez, and Paulo Rita. A data-driven approach to predict the success of bank telemarketing. _Decision Support Systems_, 62:22-31, 2014.
* [34] Guillermo Owen. Multilinear Extensions of Games. _Management Science_, 18(5):64-79, 1972.

* Pandl et al. [2021] Konstantin D Pandl, Fabian Feiland, Scott Thiebes, and Ali Sunyaev. Trustworthy machine learning for health care: scalable data valuation with the shapley value. In _Proceedings of the Conference on Health, Inference, and Learning_, pages 47-57, 2021.
* Pedregosa et al. [2011] Fabian Pedregosa, Gael Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, Jake Vanderplas, Alexandre Passos, David Cournapeau, Matthieu Brucher, Matthieu Perrot, and Edouard Duchesnay. Scikit-learn: Machine Learning in Python. _Journal of Machine Learning Research_, 12(85):2825-2830, 2011.
* Pivaro et al. [2017] Gabriel Fernando Pivaro, Santosh Kumar, Gustavo Fraidenraich, and Claudio Ferreira Dias. On the exact and approximate eigenvalue distribution for sum of wishart matrices. _IEEE Transactions on Vehicular Technology_, 66(11):10537-10541, 2017.
* Schoch et al. [2022] Stephanie Schoch, Haifeng Xu, and Yangfeng Ji. CS-shapley: Class-wise shapley values for data valuation in classification. In _Advances in Neural Information Processing Systems_, 2022.
* Serfling [1974] Robert J Serfling. Probability inequalities for the sum in sampling without replacement. _The Annals of Statistics_, pages 39-48, 1974.
* Shapley [1952] Lloyd S. Shapley. _A Value for N-Person Games_. RAND Corporation, Santa Monica, CA, 1952.
* Shim et al. [2021] Dongsub Shim, Zheda Mai, Jihwan Jeong, Scott Sanner, Hyunwoo Kim, and Jongseong Jang. Online class-incremental continual learning with adversarial shapley value. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 35, pages 9630-9638, 2021.
* Sim et al. [2020] Rachael Hwee Ling Sim, Yehong Zhang, Mun Choon Chan, and Bryan Kian Hsiang Low. Collaborative machine learning with incentive-aware model rewards. In _Proceedings of the 37th International Conference on Machine Learning_, ICML'20. JMLR.org, 2020.
* Tan and Gupta [1983] WY Tan and RP Gupta. On approximating a linear combination of central wishart matrices with positive coefficients. _Communications in Statistics-Theory and Methods_, 12(22):2589-2600, 1983.
* Tay et al. [2021] Sebastian Shenghong Tay, Xinyi Xu, Chuan-Sheng Foo, and Bryan Kian Hsiang Low. Incentivizing collaboration in machine learning via synthetic data rewards. In _AAAI Conference on Artificial Intelligence_, 2021.
* Tsybakov [2008] Alexandre B. Tsybakov. _Introduction to Nonparametric Estimation_. Springer Publishing Company, Incorporated, 1st edition, 2008. ISBN 0387790519.
* Wang et al. [2023] Jiachen T Wang, Yuqing Zhu, Yu-Xiang Wang, Ruoxi Jia, and Prateek Mittal. Threshold knn-shapley: A linear-time and privacy-friendly approach to data valuation. _arXiv preprint arXiv:2308.15709_, 2023.
* Yu et al. [2020] Han Yu, Zelei Liu, Yang Liu, Tianjian Chen, Mingshu Cong, Xi Weng, Dusit Niyato, and Qiang Yang. A fairness-aware incentive scheme for federated learning. In _Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society_, pages 393-399, 2020.

## Appendix A Complementary Numerical results

All experiments were executed on a laptop running macOS 13.3.1 and equipped with Apple M1 chip with 16GB of RAM. The minimum amount of compute was roughly 5 minutes while the maximum one roughly 10 hours.

### DU-Shapley vs SVARM

We have looked at the probability at which SVARM (Theorem 4 [22]) can ensure, after \(I^{2}\) iterations (without considering the warm up as part of the budget), an error equal to DU-Shapley's accumulated bias. We have considered the same value function than in Section 4.2 with \(n_{max}\in\{2\cdot 10^{3},3\cdot 10^{3},5\cdot 10^{3},10^{4}\}\) and 100 simulations of sets of players at each time. Figure 3 shows the results. We observe how SVARM cannot ensure, with high enough probability, an approximation error equal to the one of DU-Shapley.

### Approximating the Shapley value in Synthetic Data

We consider a toy dataset valuation problem associated to our heterogeneous linear regression with local differential privacy use-case (Section 2.1.3) and we measure the value of a coalition \(\mathcal{S}\) with the utility function in close-form from Proposition 1. We consider \(d=10\).

In order to benchmark the performances of DU-Shapley, we consider four competitive approaches, relying on Monte Carlo (MC) approximation strategies [32]. The first one, referred to as MC-Shapley is the standard MC approximation defined in (6). The second one, coined MC-anti-Shapley is a variance-reduced version of MC-Shapley that considers antithetic sampling. The third one coined Owen-Shapley stands for the multilinear extension of Owen [34] which represents the Shapley value as two nested expectations (further explained in Appendix B.3). Finally, the fourth approach, coined Orthogonal-Shapley, relies on efficient permutation sampling techniques on the hypersphere to draw permutations in (4) in a dependent way. To assess the performance of the aforementioned Shapley value estimators, we used the mean square error (MSE) averaged over all players. DU-Shapley is computed exactly by using (10) while, for each MC-based estimator, we performed 25 Shapley value estimations to compute the MSE, and did it 10 times to obtain confidence intervals for the MSE.

Figure 4 compares DU-Shapley (the horizontal line which does not depend on the sampling budget as we compute it exactly) and the MC-based methods, which are computed at several different budgets. The x-axis corresponds to the sampling budget allowed to the MC-bases methods w.r.t. DU-Shapley, i.e., \(10^{-1}\) means a budget equal to \(10\)% the one of DU-Shapley, \(10^{0}\) means same budget (indicated by the black vertical line), and \(10^{1}\) means 10 times the DU-Shapley budget. Remark that, even when the MC-methods use 10 times the budget of DU-Shapley, our method keeps approximating better the Shapley value.

Figure 3: Probability that SVARM guarantees an error equal to DU-Shapley’s bias

## Appendix B Further details about numerical implementations

### Datasets considered in Section 4.1.

Table 3 summarizes the real-world datasets considered in Section 4.1.

### OpenDataVal implementations

In this section we describe more in detail the implementations of DataShapley, Leave-One-Out (LOO), and KNN-Shapley for our numerical results in Section 4.3.

DataShapley, applied to the dataset valuation problem, simply corresponds to the method coined MC in Section 4.1. Therefore, we sample datasets and output the averaged marginal contribution.

Regarding LOO, notice that it corresponds to compute just one marginal contribution, usually computed on the big coalition, i.e.,

\[\mathrm{LOO}_{i}:=u(\mathcal{I})-u(\mathcal{I}\setminus\{i\}).\]

\begin{table}
\begin{tabular}{l c c c} \hline \hline Dataset & Size & \(d\) & Task \\ \hline adult [21] & 48,842 & 107 & classification \\ breast-cancer [30] & 699 & 30 & classification \\ bank [33] & 45,211 & 16 & classification \\ cal-housing [19] & 20,640 & 8 & regression \\ make-regression [36] & 1,000 & 10 & regression \\ year [36] & 515,345 & 90 & regression \\ \hline \hline \end{tabular}
\end{table}
Table 3: Datasets considered in Section 4.1.

As players' marginal contributions to large datasets tend to be small, we have preferred to sample one dataset \(\mathrm{D}\) from \(\mathrm{D}_{-i}\) and to output

\[\mathrm{LOO}_{i}:=u(\mathrm{D}\cup\mathrm{D}_{i})-u(\mathrm{D}).\]

Finally, regarding KNN-Shapley, we refer the reader to [15], Section E.3 of the appendix who explain how to adapt the method to dataset valuation.

### Owen's Shapley value approximation

In Section A.2, we considered the Shapley value approximation referred to as \(\mathtt{Owen-Shapley}\) as a state-of-the-art competitor to \(\mathtt{DU-Shapley}\). We provide in the following additional details regarding \(\mathtt{Owen-Shapley}\). For the other competitors, we directly refer the interested reader to Mitchell et al. [32]. Owen [34] studied the multilinear extension of a cooperative game and an alternative way to express the Shapley value. Formally, a cooperative game \(G=(\mathcal{I},u)\) consists on a set of \(I\) players \(\mathcal{I}=\{1,2,...,I\}\) and a value function \(u:2^{\mathcal{I}}\rightarrow\mathbb{R}\) such that, for any \(S\subseteq\mathcal{I}\), \(u(S)\) corresponds to the value generated by the coalition \(S\). The multilinear extension of \(G\), denoted \(\bar{G}=(\mathcal{I},\bar{u})\), is obtained when considering the value function \(\bar{u}:[0,1]^{\mathcal{I}}\rightarrow\mathbb{R}\) given by,

\[\bar{u}(x_{1},x_{2},...,x_{I})=\sum_{S\subseteq\mathcal{I}}\prod_{i\in S}x_{i }\prod_{j\notin S}(1-x_{i})u(S).\]

Intuitively, \(\bar{u}(x_{1},x_{2},...,x_{I})\) corresponds to the expected value of a coalition when each player \(i\in\mathcal{I}\) joins the coalition with probability \(x_{i}\). Theorem 5 in [34] gives an alternative way to compute the Shapley value \(\varphi_{i}(u)\) of player \(i\) in game \(G\), namely,

\[\varphi_{i}(u) =\int_{0}^{1}\frac{\partial\bar{u}}{\partial x_{i}}(\tau,...,\tau )\mathrm{d}\tau=\int_{0}^{1}\sum_{S\subseteq\mathcal{I}\setminus\{i\}}\tau^{|S |}(1-\tau)^{I-|S|-1}[u(S\cup\{i\})-u(S)]\mathrm{d}\tau\] \[=\int_{0}^{1}\mathbb{E}\big{[}u(\mathcal{E}_{i}(\tau)\cup i)-u( \mathcal{E}_{i}(\tau))\big{]}\mathrm{d}\tau=\mathbb{E}_{\tau\sim\mathrm{U}([0,1])}\bigg{[}\mathbb{E}\big{[}u(\mathcal{E}_{i}(\tau)\cup i)-u(\mathcal{E}_{i} (\tau))\big{]}\bigg{]},\]

where \(\mathcal{E}_{i}(\tau)\) is a random subset of \(\mathcal{I}\setminus\{i\}\), such that, \(\forall j\in\mathcal{I}\setminus\{i\}\), \(j\) is included in \(\mathcal{E}_{i}(\tau)\) with probability \(\tau\). In words, the Shapley value of player \(i\) corresponds to her expected marginal contribution to the random set \(\mathcal{E}_{i}(\tau)\), when \(\tau\) is uniformly distributed on \([0,1]\). This brings an alternative way to use Monte Carlo to approximate the Shapley value \(\varphi_{i}(u)\), coined Owen-Shapley, as,

\[\hat{\varphi}_{i}^{\text{Owen}}(u)=\frac{1}{T}\sum_{t=1}^{T}u(\mathcal{E}_{i} (\tau_{t})\cup i)-u(\mathcal{E}_{i}^{t}(\tau_{t})),\]

where for each \(t\in\{1,...,T\}\), we draw \(\tau_{t}\) independently and uniformly in \([0,1]\) and then, create a random set \(\mathcal{E}_{i}(\tau_{t})\) by adding each player \(j\in\mathcal{I}\setminus\{i\}\) to it with probability \(\tau_{t}\).

## Appendix C Missing proofs

### Proof of Proposition 1

**Proposition 1**.: _Let \(\mathcal{S}\subseteq\mathcal{I}\) be a coalition of players and consider the value function \(u\) as in (3). It follows,_

\[u(\mathcal{S})=\frac{-\mathrm{Tr}\big{[}\mathbb{E}\big{[}xx^{\top}\big{]} \big{]}}{q(\mathcal{S})-d-1},\text{ where }q(\mathcal{S}):=\left\lfloor\frac{\big{(} \sum\limits_{i\in\mathcal{S}}\frac{\sigma_{i}}{\varepsilon_{i}}n_{i}\big{)}^{ 2}}{\sum\limits_{i\in\mathcal{S}}\big{(}\frac{\sigma_{i}}{\varepsilon_{i}} \big{)}^{2}n_{i}}\right\rfloor,\text{with the convention }q(\varnothing)=0.\]

_In particular, considering \(p_{\mathrm{test}}=\mathrm{N}(0,\mathrm{I}_{d})\), we get,_

\[u(\mathcal{S})=\frac{d}{d+1-q(\mathcal{S})}.\]Proof.: Let \(\mathcal{S}\subseteq\mathcal{I}\) be a coalition of players and \(X_{\mathcal{S}},Y_{\mathcal{S}}\) be the concatenation of their datasets. The linear model can be rewritten in matrix form as

\[Y_{\mathcal{S}}=X_{\mathcal{S}}\theta+\eta_{\mathcal{S}},\]

where \(\eta_{\mathcal{S}}\) is the concatenation of \(\eta_{i}^{(j)}\) for all \(i\in\mathcal{S}\) and \(j\in[n_{i}]\). Take \(\hat{\theta}_{\mathcal{S}}=(X_{\mathcal{S}}^{\top}\Sigma_{\mathcal{S}}^{-1}X _{\mathcal{S}})^{-1}X_{\mathcal{S}}^{\top}\Sigma_{\mathcal{S}}^{-1}Y_{ \mathcal{S}}\) where \(\Sigma_{\mathcal{S}}=\mathrm{diag}((\varepsilon_{i}^{2})_{i\in\mathcal{S}})\), and let \(x\sim p_{\mathrm{test}}\) be a hold-out testing datum in \(\mathbb{R}^{d}\). It follows,

\[\big{(}x^{\top}(\theta-\hat{\theta}_{\mathcal{S}})\big{)}^{2}= \bigg{(}\sum_{i\in\mathcal{S}}\eta_{i}\varepsilon_{i}^{-2}X_{i}\bigg{)}\bigg{(} \sum_{i\in\mathcal{S}}X_{i}^{\top}\varepsilon_{i}^{-2}X_{i}\bigg{)}^{-1}xx^{ \top}\bigg{(}\sum_{i\in\mathcal{S}}X_{i}^{\top}\varepsilon_{i}^{-2}X_{i}\bigg{)} ^{-1}\bigg{(}\sum_{i\in\mathcal{S}}\eta_{i}\varepsilon_{i}^{-2}X_{i}\bigg{)}\] \[=\mathrm{Tr}\bigg{[}\bigg{(}\sum_{i\in\mathcal{S}}\eta_{i} \varepsilon_{i}^{-2}X_{i}\bigg{)}\bigg{(}\sum_{i\in\mathcal{S}}X_{i}^{\top} \varepsilon_{i}^{-2}X_{i}\bigg{)}^{-1}xx^{\top}\bigg{(}\sum_{i\in\mathcal{S}}X _{i}^{\top}\varepsilon_{i}^{-2}X_{i}\bigg{)}^{-1}\bigg{(}\sum_{i\in\mathcal{S} }\eta_{i}\varepsilon_{i}^{-2}X_{i}\bigg{)}\bigg{]}\] \[=\mathrm{Tr}\bigg{[}xx^{\top}\bigg{(}\sum_{i\in\mathcal{S}}X_{i}^{ \top}\varepsilon_{i}^{-2}X_{i}\bigg{)}^{-1}\bigg{(}\sum_{i\in\mathcal{S}}\sum_ {j\in\mathcal{S}}X_{i}^{\top}\varepsilon_{i}^{-2}\eta_{i}\eta_{j}^{\top} \varepsilon_{j}^{-2}X_{j}\bigg{)}\bigg{(}\sum_{i\in\mathcal{S}}X_{i}^{\top} \varepsilon_{i}^{-2}X_{i}\bigg{)}^{-1}\bigg{]}\]

We take expectation with respect to the different stochastic terms. Since \(\eta_{i}^{(k)}\sim\mathrm{N}(0,\varepsilon_{i}^{2})\) for any \(i\in\mathcal{S},k\in[n_{i}]\), it holds,

\[\mathbb{E}_{(\eta_{i}^{(k)}\sim\mathrm{N}(0,\varepsilon_{i}^{2}) _{i\in\mathcal{S}}^{k\in[n_{i}]}}\big{[} \big{(}x^{\top}(\theta-\hat{\theta}_{\mathcal{S}})\big{)}^{2} \big{]}\] \[=\mathrm{Tr}\bigg{[}xx^{\top}\bigg{(}\sum_{i\in\mathcal{S}}X_{i}^{ \top}\varepsilon_{i}^{-2}X_{i}\bigg{)}^{-1}\bigg{[}\sum_{i\in\mathcal{S}}X_{i} ^{\top}\varepsilon_{i}^{-2}X_{i}\bigg{)}\bigg{(}\sum_{i\in\mathcal{S}}X_{i}^{ \top}\varepsilon_{i}^{-2}X_{i}\bigg{)}^{-1}\bigg{]}\] \[=\mathrm{Tr}\bigg{[}xx^{\top}\bigg{(}\sum_{i\in\mathcal{S}}X_{i}^{ \top}\varepsilon_{i}^{-2}X_{i}\bigg{)}^{-1}\bigg{]}\]

Since players distributions differ on their variances, \(\sum_{i\in\mathcal{S}}X_{i}^{\top}\varepsilon_{i}^{-2}X_{i}\) corresponds to a semi-correlated Wishart random variable where each \(\frac{1}{\varepsilon_{i}}X_{i}\sim\mathrm{N}(0,(\frac{\sigma_{i}}{\varepsilon_{ i}})^{2}\mathrm{I}_{d})\). In particular, the semi-correlated Wishart random variable can be approximated by a central Wishart distribution [37, 43], whose precision depends on the homogeneity of the coefficients \(\sigma_{i}/\varepsilon_{i}\) over all \(i\in\mathcal{I}\), as showed in [20]. It follows,

\[\mathbb{E}_{(X_{i}\sim\mathrm{N}(0,\sigma_{i}^{2}\mathrm{I}_{d}))_{i\in \mathcal{S}}}\left[\bigg{(}\sum_{i\in\mathcal{S}}X_{i}^{\top}\varepsilon_{i}^{ -2}X_{i}\bigg{)}^{-1}\right]\approx\frac{\mathrm{I}_{d}}{(q(\mathcal{S})-d-1)}\,,\]

where

\[q(\mathcal{S}):=\left\lfloor\frac{\big{(}\sum\limits_{i\in\mathcal{S}}\frac{ \sigma_{i}}{\varepsilon_{i}}n_{i}\big{)}^{2}}{\sum\limits_{i\in\mathcal{S}} \big{(}\frac{\sigma_{i}}{\varepsilon_{i}}\big{)}^{2}n_{i}}\right\rfloor.\]

With all this in mind, it follows,

\[\mathbb{E}_{(\eta_{i}^{(j)}\sim\mathrm{N}(0,\varepsilon_{i}^{2}) _{i\in\mathcal{S}}^{j\in[n_{i}]}}\big{[}\big{(}x^{\top}(\theta-\hat{\theta}_{ \mathcal{S}})\big{)}^{2}\big{]}=\mathrm{Tr}\bigg{[}xx^{\top}\frac{\mathrm{I}_ {d}}{(q(\mathcal{S})-d-1)}\bigg{]}=\frac{1}{q(\mathcal{S})-d-1}\mathrm{Tr} \big{[}xx^{\top}\big{]}.\]

In particular, considering \(p_{\mathrm{test}}=\mathrm{N}(0,\mathrm{I}_{d})\), we get,

\[u(\mathcal{S})=\frac{d}{d+1-q(\mathcal{S})}.\]

### Proof of Theorem 2

**Theorem 2**.: _Let \(\{n_{i},\gamma_{i}\}_{i\in[I]}\) be two sequences of positive numbers such that the following limits_

\[\lim_{I\to\infty}\frac{1}{I}\sum\nolimits_{i\in[I]}n_{i}\gamma_{i} =\mu_{A},\quad\lim_{I\to\infty}\frac{1}{I}\sum\nolimits_{i\in[I]}(n_{i}\gamma_ {i}-\mu_{A})^{2}=\sigma_{A}^{2},\] \[\lim_{I\to\infty}\frac{1}{I}\sum\nolimits_{i\in[I]}n_{i}\gamma_{ i}^{2}=\mu_{B},\quad\lim_{I\to\infty}\frac{1}{I}\sum\nolimits_{i\in[I]}(n_{i} \gamma_{i}^{2}-\mu_{B})^{2}=\sigma_{B}^{2}\,,\]

_all exist, for some constants \(\mu_{A},\mu_{B},\sigma_{A},\sigma_{B}>0\). Let \(\mathbf{K}\sim\mathrm{U}(\{0,\dots,I\})\), \(\mathcal{S}_{\mathbf{K}}\sim\mathrm{U}([2_{\mathbf{K}}^{\mathcal{I}}])\), and define \(q(\mathcal{S}_{\mathbf{K}})\) as in (9) for the third use-case. Then, almost surely, \(\nicefrac{{q(\mathcal{S}_{\mathbf{K}})}}{{q(\mathcal{I})}}\xrightarrow{I\to \infty}\mathrm{U}([0,1])\)._

Proof.: Introduce, for any \(t,t_{0}\in(0,1)\) and any \(s>0\),

\[\mu_{A}(I)=\frac{1}{I}\sum_{i\in[I]}n_{i}\gamma_{i},\quad\mu_{B}( I)=\frac{1}{I}\sum_{i\in[I]}n_{i}\gamma_{i}^{2},\] \[Y_{A}(t,I)=\sum_{i\in\mathcal{S}_{\lfloor{It}\rfloor}}n_{i} \gamma_{i},\quad Y_{B}(t,I)=\sum_{i\in\mathcal{S}_{\lfloor{It}\rfloor}}n_{i} \gamma_{i}^{2}.\] \[R_{A}(I,t_{0},s)=\mathbb{P}\biggl{(}\sup_{t>t_{0}}\biggl{|}\frac {Y_{A}(t,I)}{\lfloor{It}\rfloor}-\mu_{A}(I)\biggr{|}>s\biggr{)},\] \[R_{B}(I,t_{0},s)=\mathbb{P}\biggl{(}\sup_{t>t_{0}}\biggl{|}\frac {Y_{B}(t,I)}{\lfloor{It}\rfloor}-\mu_{B}(I)\biggr{|}>s\biggr{)}.\]

By construction, \(Y_{A}(t,I)\) and \(Y_{B}(t,I)\) are sums of sampling without replacement of \(\lfloor{It}\rfloor\) elements. Therefore, by Corollary 1.3 in [39], for \(s\) fixed, there exists \(I_{0}^{A},I_{0}^{B}\in\mathbb{N}\) such that,

\[R_{A}(I,t_{0},s)\leq\frac{(1-t_{0})\sigma_{A}^{2}}{\lfloor{It_{0}}\rfloor s^ {2}},\forall I\geq I_{0}^{A}\text{ and }R_{B}(I,t_{0},s)\leq\frac{(1-t_{0})\sigma_{B}^{2}}{ \lfloor{It_{0}}\rfloor s^{2}},\forall I\geq I_{0}^{B}.\]

In other words, for any \(s>0\) and \(I\) large enough, almost surely, it holds,

\[\biggl{|}\frac{Y_{A}(t,I)}{\lfloor{It}\rfloor}-\mu_{A}(I)\biggr{|}\leq s\text { and }\biggl{|}\frac{Y_{B}(t,I)}{\lfloor{It}\rfloor}-\mu_{B}(I)\biggr{|}\leq s.\]

It follows,

\[\biggl{|}\frac{q(\mathcal{S}_{\lfloor{It}\rfloor})}{\lfloor{It} \rfloor}-\frac{\mu_{A}(I)^{2}}{\mu_{B}(I)}\biggr{|} =\biggl{|}\frac{1}{\lfloor{It}\rfloor}\cdot\frac{Y_{A}(t,I)^{2}} {Y_{B}(t,I)}-\frac{\mu_{A}(I)^{2}}{\mu_{B}(I)}\biggr{|}\] \[=\biggl{|}\biggl{(}\frac{Y_{A}(t,I)^{2}}{\lfloor{It}\rfloor^{2}}- \mu_{A}(I)^{2}+\mu_{A}(I)^{2}\biggr{)}\biggl{(}\frac{\lfloor{It}\rfloor}{Y_{B} (t,I)}-\frac{1}{\mu_{B}(I)}\biggr{)}\] \[\quad+\biggl{(}\frac{Y_{A}(t,I)^{2}}{\lfloor{It}\rfloor^{2}}- \mu_{A}(I)^{2}\biggr{)}\frac{1}{\mu_{B}(I)}\biggr{|}\] \[\leq s\biggl{(}s+\mu_{A}(I)+\frac{1}{\mu_{B}(I)}\biggr{)},\]

which is arbitrarily small as \(\mu_{A}(I),\mu_{B}(I)\to\mu_{A},\mu_{B}<\infty\). Therefore, almost surely,

\[\lim_{I\to\infty}\frac{q(\mathcal{S}_{\lfloor{It}\rfloor})}{I\mu_{A}(I)^{2}/ \mu_{B}(I)}=t.\]

The proof concludes noticing that

\[q(\mathcal{I})=\frac{I\mu_{A}(I)^{2}}{\mu_{B}(I)},\]

and that \(\mathbf{K}=\lfloor{IU}\rfloor\) with \(U\sim\mathrm{U}([0,1])\). 

### Proof of Theorem 5

To prove Theorem 5, we need two preliminary results: Lemma 3, which itself needs two supplementary results (Lemmas 1 and 2), and Lemma 4, which is directly proved.

#### c.3.1 Technical lemmata

**Lemma 1**.: _Consider a set of \(I\) values \(N=\{n_{1},\ldots,n_{I}\}\). Let \(X_{1},\ldots,X_{k}\) and \(Y_{1},\ldots,Y_{k}\) denote, respectively, \(k\) random samples with and without replacement from \(N\). For any continuous and convex function \(f\), it follows,_

\[\mathbb{E}\bigg{[}f\bigg{(}\sum_{i=1}^{k}Y_{i}\bigg{)}\bigg{]}\leq\mathbb{E} \bigg{[}f\bigg{(}\sum_{i=1}^{k}X_{i}\bigg{)}\bigg{]}\]

Proof.: The proof follows from [14]. 

**Lemma 2**.: _Let \(I\in\mathbb{N}\), \(N:=\{n_{1},\ldots,n_{I}\}\in\mathbb{R}_{+}^{I}\), \(\mu=\frac{1}{I}\sum_{i=1}^{I}n_{i}\) be their mean value and \(\sigma^{2}=\frac{1}{I}\sum_{i=1}^{I}(n_{i}-\mu)^{2}\) be their variance. For \(k\in\{0,\ldots,n\}\), let \(\mathcal{S}_{k}\sim\mathrm{U}(\{S_{k}\subseteq[I]:|S_{k}|=k\})\) be a uniform random variable on the subsets of \(\{1,\ldots,I\}\) of size \(k\), and \(n_{\mathcal{S}_{k}}=\sum_{i\in\mathcal{S}_{k}}n_{i}\) be the random variable defined by the sum of the elements of \(\mathcal{S}_{k}\). Let \(\mathbf{K}\sim\mathrm{U}(\{0,\ldots,I\})\) and define \(\mathbf{Y}=n_{S_{\mathbf{K}}}\). Then,_

\[\mathbb{E}[\mathbf{Y}-\mu\mathbf{K}\mid\mathbf{K}=k]=0,\] (12) \[\mathbb{E}\big{[}(\mathbf{Y}-\mu\mathbf{K})^{2}\mid\mathbf{K}=k \big{]}\leq k\sigma^{2}.\] (13)

Proof.: We prove (12) directly.

\[\mathbb{E}[\mathbf{Y}\mid\mathbf{K}=k] =\sum_{S_{k}\subseteq[I]:|S_{k}|=k}n_{S_{k}}\frac{1}{\binom{I}{ k}}=\frac{1}{\binom{I}{k}}\sum_{S_{k}\subseteq[I]:|S_{k}|=k}\sum_{i\in S_{k}}n_{i}\] \[=\frac{1}{\binom{I}{k}}\sum_{i\in[I]}n_{i}\binom{I-1}{k-1}\] \[=\frac{(I-k)!k!}{I!}\cdot\frac{(I-1)!}{(k-1)!(I-k)!}\sum_{i\in[I] }n_{i}=\mu k.\]

Thus, (12) follows as \(\mathbb{E}[\mu\mathbf{K}\mid\mathbf{K}=k]=\mu k\). To prove (13), let \((\mathbf{X}_{i})_{i=1}^{k}\) be \(k\) independent samples from the set \(N\). From Lemma 1 it holds,

\[\mathbb{E}\big{[}(\mathbf{Y}-\mu\mathbf{K})^{2}\mid\mathbf{K}=k \big{]}\leq\mathbb{E}\bigg{[}\big{(}\mu\mathbf{K}-\sum_{i=1}^{\mathbf{K}} \mathbf{X}_{i}\big{)}^{2}\mid\mathbf{K}=k\bigg{]}=\mathbb{E}\bigg{[}\bigg{(} \sum_{i=1}^{\mathbf{K}}\left(\mu-\mathbf{X}_{i}\right)\bigg{)}^{2}\mid\mathbf{ K}=k\bigg{]}.\]

Therefore,

\[\mathbb{E}\big{[}(\mathbf{Y}- \mu\mathbf{K})^{2}\mid\mathbf{K}=k\big{]}\leq\mathbb{E}\bigg{[} \bigg{(}\sum_{i=1}^{\mathbf{K}}\sum_{j=1}^{\mathbf{K}}\left(\mu-\mathbf{X}_{i} \right)\left(\mu-\mathbf{X}_{j}\right)\bigg{)}\mid\mathbf{K}=k\bigg{]}\] \[=\sum_{i=1}^{k}\sum_{j=1}^{k}\left(\mu^{2}-\mu(\mathbb{E}[ \mathbf{X}_{i}\mid\mathbf{K}=k]+\mathbb{E}[\mathbf{X}_{j}\mid\mathbf{K}=k])+ \mathbb{E}[\mathbf{X}_{i}\mathbf{X}_{j}\mid\mathbf{K}=k]\right)\] \[=\sum_{i=1}^{k}\sum_{j=1}^{k}\left(\mu^{2}-\mu(\mathbb{E}[ \mathbf{X}_{i}]+\mathbb{E}[\mathbf{X}_{j}])+\mathbb{E}[\mathbf{X}_{i}\mathbf{X }_{j}]\right)\] \[=\sum_{i=1}^{k}\left(\mu^{2}-2\mu\mathbb{E}[\mathbf{X}_{i}]+ \mathbb{E}[\mathbf{X}_{i}^{2}]\right)+\sum_{i=1}^{k}\sum_{\begin{subarray}{c}j=1 \\ j\neq i\end{subarray}}^{k}\left(\mu^{2}-\mu(\mathbb{E}[\mathbf{X}_{i}]+\mathbb{E }[\mathbf{X}_{j}])+\mathbb{E}[\mathbf{X}_{i}]\mathbb{E}[\mathbf{X}_{j}]\right)\]\[=\sum_{i=1}^{k}\mathbb{E}\big{[}(\mu-\mathbf{X}_{i})^{2}\big{]}+\sum_{ i=1}^{k}\sum_{\begin{subarray}{c}j=1\\ j\neq i\end{subarray}}^{k}\big{(}\mu^{2}-2\mu^{2}+\mu^{2}\big{)}\] \[=\sum_{i=1}^{k}\mathbb{E}\big{[}(\mu-\mathbf{X}_{i})^{2}\big{]}= \sum_{i=1}^{k}\text{Var}\,(\mu-\mathbf{X}_{i})=k\sigma^{2}.\]

The steps come from rearranging the terms, using the independence of \(\mathbf{X}_{i}\) with respect to \(\mathbf{K}\), the independence of \(\mathbf{X}_{i},\mathbf{X}_{j}\) for \(i\neq j\), and finally that \(\mathbb{E}[\mathbf{X}_{i}]=\mu\) and \(\text{Var}\,(\mathbf{X}_{i})=\sigma^{2}\). 

**Lemma 3**.: _Let \(I\in\mathbb{N}\), \(N:=\{n_{1},\ldots,n_{I}\}\in\mathbb{R}_{+}^{I}\), and define,_

\[\mu=\frac{1}{I}\sum_{i=1}^{I}n_{i},\quad\sigma^{2}=\frac{1}{I} \sum_{i=1}^{I}(n_{i}-\mu)^{2},\quad n^{\text{max}}=\max_{i\in\mathcal{I}}n_{i},\] \[R:=\max_{i\in[I]}|n_{i}-\mu|,\quad\tau=\max_{i\in[I]}n_{i}/\min _{i\in[I]}n_{i}.\]

_Consider \(\mathcal{S}_{\mathbf{K}}\), \(n_{\mathcal{S}_{\mathbf{K}}}\), \(\mathbf{K}\), and \(\mathbf{Y}\) as in Lemma 2. Let \(w:\mathbb{R}_{+}\to\mathbb{R}\) be a function in \(\mathcal{C}^{2}\), increasing, and suppose there exists \(\kappa\in\mathbb{R}_{+}\), such that,_

\[\big{|}w^{(2)}(n)\big{|}\leq\frac{\kappa}{n^{2}},\forall n\!>\!0,\]

_where \(w^{(k)}\) is the k-th derivative of \(w\). Then, it holds,_

\[\big{|}\mathbb{E}[w(\mu\mathbf{K})-w(\mathbf{Y})]\big{|}\leq\frac{\kappa}{2 \mu^{2}I}\left(9\sigma^{2}(1+\ln(I))+\frac{2R^{2}\tau^{2}}{n^{\text{max}}} \right).\]

Proof.: The proof considers a second-order Taylor extension of \(w\) at \(\mu k\) to recover the expected value of \(\mathbb{E}[w(\mu\mathbf{K})-w(\mathbf{Y})]\). Noticing that the first derivative has a null expected value, the upper bound stated on the Lemma comes from bounding the expected value of the second derivative.

The Taylor-Lagrange Theorem on \(w\) at \(\mu k>0\) provides,

\[w(y)=w(\mu k)+w^{(1)}(\mu k)(\mu k-y)+w^{(2)}(\tau)\frac{(\mu k-y)^{2}}{2},\]

for some \(\tau\) between \(y\) and \(\mu k\). Therefore, there exists a random variable \(\mathrm{T}\), almost surely between \(\mu\mathbf{K}_{+}\) and \(\mathbf{Y}\), such that,

\[\mathbb{E}[w(\mathbf{Y})-w(\mu\mathbf{K}_{+})]=\mathbb{E}\bigg{[}w^{(1)}(\mu \mathbf{K}_{+})(\mu\mathbf{K}_{+}-\mathbf{Y})+\frac{1}{2}w^{(2)}(\mathrm{T})( \mu\mathbf{K}_{+}-\mathbf{Y})^{2}\bigg{]},\]

where \(\mathbf{K}_{+}\) corresponds to \(\mathbf{K}\) conditioned to be positive. To avoid overcharging the notation, we drop the index from \(\mathbf{K}_{+}\). We observe that,

\[\mathbb{E}\bigg{[}w^{(1)}(\mu\mathbf{K})(\mu\mathbf{K}-\mathbf{Y })\bigg{]} =\mathbb{E}\bigg{[}\mathbb{E}\big{[}w^{(1)}(\mu\mathbf{K})(\mu \mathbf{K}-\mathbf{Y})\mid\mathbf{K}=k\big{]}\bigg{]}\] \[=\mathbb{E}\bigg{[}w^{(1)}(\mu k)\mathbb{E}\big{[}(\mu\mathbf{K}- \mathbf{Y})\mid\mathbf{K}=k\big{]}\bigg{]}=0,\]

by Lemma 2, Equation (12). Therefore,

\[\big{|}\mathbb{E}[w(\mathbf{Y})-w(\mu\mathbf{K})]\big{|} =\frac{1}{2}\big{|}\mathbb{E}\big{[}w^{(2)}(\mathrm{T})(\mu \mathbf{K}-\mathbf{Y})^{2}\big{]}\big{|}\] \[\leq\frac{1}{2}\mathbb{E}\big{[}|w^{(2)}(\mathrm{T})\big{|}(\mu \mathbf{K}-\mathbf{Y})^{2}\big{]}\] \[\leq\frac{1}{2}\mathbb{E}\bigg{[}\frac{\kappa}{\mathrm{T}^{2}}( \mu\mathbf{K}-\mathbf{Y})^{2}\bigg{]}=\frac{\kappa}{2}\mathbb{E}\bigg{[} \frac{1}{\mathrm{T}^{2}}(\mu\mathbf{K}-\mathbf{Y})^{2}\bigg{]}.\]

Setting \(\mathbf{I}:=\big{\{}|\mu\mathbf{K}-\mathbf{Y}|\leq\frac{1}{2}(\mu\mathbf{K}+ \mathbf{Y})\big{\}}\), the previous expected value can be expressed as,

\[\mathbb{E}\bigg{[}\frac{1}{\mathrm{T}^{2}}(\mu\mathbf{K}-\mathbf{Y})^{2} \bigg{]}=\mathbb{E}\bigg{[}\frac{1}{\mathrm{T}^{2}}(\mu\mathbf{K}-\mathbf{Y})^ {2}\cdot\mathbf{I}\bigg{]}+\mathbb{E}\bigg{[}\frac{1}{\mathrm{T}^{2}}(\mu \mathbf{K}-\mathbf{Y})^{2}\cdot\mathbf{I}^{c}\bigg{]}.\]We deal with each term separately. Notice that, as \(\mathrm{T}\) is almost surely between \(\mathbf{Y}\) and \(\mu\mathbf{K}\),

\[|\mu\mathbf{K}-\mathbf{Y}|\leq\frac{1}{2}(\mu\mathbf{K}+\mathbf{Y})\Longrightarrow \mathrm{T}\geq\frac{1}{3}\mu\mathbf{K}.\]

Thus,

\[\mathbb{E}\bigg{[}\frac{(\mu\mathbf{K}-\mathbf{Y})^{2}}{\mathrm{ T}^{2}}\cdot\mathbf{I}\bigg{]} \leq\mathbb{E}\left[\frac{(\mu\mathbf{K}-\mathbf{Y})^{2}}{(\frac {\mu\mathbf{K}}{3})^{2}}\cdot\mathbf{I}\right]=\frac{9}{\mu^{2}}\sum_{k=1}^{I} \frac{1}{I}\cdot\mathbb{E}\left[\frac{(\mu k-\mathbf{Y})^{2}}{k^{2}}\cdot \mathbf{I}\mid\mathbf{K}=k\right]\] \[\leq\frac{9}{I\mu^{2}}\sum_{k=1}^{I}\mathbb{E}\left[\frac{(\mu k- \mathbf{Y})^{2}}{k^{2}}\mid\mathbf{K}=k\right]\] \[\leq\frac{9\sigma^{2}}{I\mu^{2}}\cdot(1+\ln(I)).\]

Regarding the second term, denote \(\overline{n}:=\max_{i\in\mathcal{I}}n_{i}\) and \(\underline{n}:=\min_{i\in\mathcal{I}}n_{i}.\) As \(\mathbf{K}\underline{n}\leq\min\{\mu\mathbf{K},\mathbf{Y}\}\leq\mathrm{T}\), we have,

\[\mathbb{E}\bigg{[}\frac{(\mu\mathbf{K}-\mathbf{Y})^{2}}{\mathrm{ T}^{2}}\cdot\mathbf{I}^{c}\bigg{]} \leq\mathbb{E}\left[\frac{(R\mathbf{K})^{2}}{\mathrm{T}^{2}} \cdot\mathbf{I}^{c}\right]\leq\mathbb{E}\left[\frac{(R\mathbf{K})^{2}}{( \underline{n}\mathbf{K})^{2}}\cdot\mathbf{I}^{c}\right]\] \[=\frac{R^{2}}{\underline{n}^{2}}\sum_{k=1}^{I}\mathbb{E}\left[ \frac{1}{k^{2}}k^{2}\cdot\mathbf{I}^{c}\mid\mathbf{K}=k\right]\] \[=\frac{R^{2}}{\underline{n}^{2}}\sum_{k=1}^{I}\mathbb{P}\left(| \mu k-\mathbf{Y}|>\frac{1}{2}(\mu k+\mathbf{Y})\mid\mathbf{K}=k\right)\] \[\leq\frac{R^{2}}{\underline{n}^{2}}\sum_{k=1}^{I}\mathbb{P}\left( |\mu k-\mathbf{Y}|>\frac{\mu k}{2}\mid\mathbf{K}=k\right)\] \[\leq\frac{R^{2}}{\underline{n}^{2}}\sum_{k=1}^{I}\exp\!\left(- \frac{\mu^{2}k}{2\overline{n}}\right)\] \[=\frac{2R^{2}\tau^{2}}{I\mu^{2}\overline{n}}\sum_{k=1}^{I}\frac{ \mu^{2}}{2\overline{n}}\exp\!\left(-\frac{\mu^{2}k}{2\overline{n}}\right)\] \[\leq\frac{2R^{2}\tau^{2}}{I\mu^{2}\overline{n}}\int_{0}^{\infty} \frac{\mu^{2}}{2\overline{n}}\exp\!\left(-\frac{\mu^{2}k}{2\overline{n}}\right) dk=\frac{2R^{2}\tau^{2}}{I\mu^{2}\overline{n}},\]

as the integral corresponds to the cumulative distribution function of an exponential random variable of parameter \(\lambda=\mu^{2}/2\overline{n}\). The upper bound on the theorem's statement is obtained when gathering all together. 

**Lemma 4**.: _Let \(w:\mathbb{R}_{+}\rightarrow\mathbb{R}_{+}\) be a smooth and increasing function such that_

\[\lim_{n\rightarrow\infty}n^{2}|w^{(2)}(n)|<\infty.\]

_Then, there exists \(\kappa>0\) such that \(n^{2}|w^{(2)}(n)|\leq\kappa\)._

Proof.: Notice that the assumptions imply, in particular, that \(|w^{(2)}(n)|\) is bounded. We argue by contradiction. Suppose that for any \(m>0\), there exists \(n_{m}\) such that

\[n_{m}^{2}|w^{(2)}(n_{m})|>m.\]

Suppose the sequence \((n_{m})_{m}\) converges to a point \(n^{*}\). Then,

\[\lim_{m\rightarrow\infty}n_{m}^{2}|w^{(2)}(n_{m})|>\lim_{m\rightarrow\infty} m=\infty,\]which is a contradiction with \(|w^{(2)}(n)|\) being bounded. Therefore, necessarily \((n_{m})_{m}\) has to diverge. However, this implies,

\[\lim_{n\to\infty}n^{2}|w^{(2)}(n)|=\lim_{m\to\infty}n_{m}^{2}|w^{(2)}(n_{m})|> \lim_{m\to\infty}m=\infty,\]

obtaining again a contradiction. 

#### c.3.2 Proof of Theorem 5

We are ready to prove Theorem 5.

**Theorem 5**.: _Under Assumption **H**1, there exists a constant \(\kappa>0\), such that, for any \(i\in\mathcal{I}\), it holds,_

\[\left|\varphi_{i}-\psi_{i}\right|\leq\frac{\kappa}{(I-1)\mu_{-i}^{2}}\left( \sigma_{-i}^{2}(1+\ln(I-1))+\zeta_{-i}\right),\]

_where \(\varphi_{i}\) and \(\psi_{i}\) are respectively the Shapley value and the DV-Shapley of player \(i\), \(\mu_{-i}=\frac{1}{I-1}\sum_{j\in\mathcal{I}\setminus\{i\}}n_{j}\) is the average dataset size of other players, \(\sigma_{-i}^{2}=\frac{1}{I-1}\sum_{j\in\mathcal{I}\setminus\{i\}}(n_{j}-\mu_ {-i})^{2}\) its empirical variance, and \(\zeta_{-i}\) measures the variability of the dataset sizes across players. Formally, it is defined as_

\[\zeta_{-i}:=R_{-i}^{2}\frac{\tau_{-i}^{2}}{4n_{-i}^{\max}}\]

_where \(R_{-i}:=\max_{j\in\mathcal{I}\setminus\{i\}}|n_{j}-\mu_{-i}|\), \(n_{-i}^{\max}:=\max_{j\in\mathcal{I}\setminus\{i\}}n_{j}\), and \(\tau_{-i}:=\frac{n_{-i}^{\max}}{\min_{j\in\mathcal{I}\setminus\{i\}}n_{j}}\)._

Proof.: Under Assumption **H**1, Lemma 4 implies the existence of \(\kappa>0\) such that the value function \(w\) satisfies all assumptions from Lemma 3. Theorem 5 comes from (a) noticing that

\[\varphi_{i}=\mathbb{E}[w(\mathbf{Y}_{-i}+n_{i})-w(\mathbf{Y}_{-i})],\quad \psi_{i}=\mathbb{E}[w(\mathbf{K}\mu_{-i}+n_{i})-w(\mathbf{K}\mu_{-i})],\]

where \(\mathbf{K}\sim\mathrm{U}([I-1])\) and \(\mathbf{Y}_{-i}=n_{\mathcal{S}_{\mathbf{K}}^{(i)}}\) with \(\mathcal{S}_{\mathbf{K}}^{(i)}\) taking values on the subsets of \(\mathcal{I}\setminus\{i\}\) of size \(\mathbf{K}\), (b) writing

\[|\varphi_{i}-\psi_{i}|\leq|\mathbb{E}[w(\mathbf{Y}+n_{i})-w(\mathbf{K}\mu_{-i} +n_{i})]|+|\mathbb{E}[w(\mathbf{Y})-w(\mathbf{K}\mu_{-i})]|,\]

and (c) applying Lemma 3 to each of the expected values, as the function \(n\to w(n+n_{i})\) also satisfies **H**1.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: Both abstract and introduction states exactly what we proved in our article. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: A limitations section is included in Section 5. Moreover, all mathematical assumptions are clearly stated on the main article and discussed. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes]Justification: Assumptions are stated in each theoretical result and all proofs are included in the supplementary material. We have carefully checked the accuracy of all mathematical steps. Guidelines:

* The answer NA means that the paper does not include theoretical results.
* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: At the beginning of each numerical results section we explain the used methodology and the sources of the used data. Besides, Appendix B complements these descriptions. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.

5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: The codes are included and the implementations are detailed in the appendix. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: All details are explained in each numerical results section. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: The experiments in Section 4.2 and Appendices A.1 and A.2 present error bars. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).

* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: All details are included at the beginning of Appendix B. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: The research conducted conformes with all points in the NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: We discuss potential applications in Section 1. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.

* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).

11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: [NA] Guidelines:

* The answer NA means that the paper poses no such risks.
* Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.
* Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.
* We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.

12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: All creators are properly credited by citing their works. Guidelines:

* The answer NA means that the paper does not use existing assets.
* The authors should cite the original paper that produced the code package or dataset.
* The authors should state which version of the asset is used and, if possible, include a URL.
* The name of the license (e.g., CC-BY 4.0) should be included for each asset.
* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.

* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: [NA] Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: [NA] Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: [NA] Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.