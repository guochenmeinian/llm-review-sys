Wasserstein Distributionally Robust Optimization Through the Lens of Structural Causal Models and Individual Fairness

 Ahmad-Reza Ehyaei

Max Planck Institute for Intelligent Systems, Tubingen AI Center, Germany

ahmad.ehyaei@tuebingen.mpg.de

&Golnough Farnadi

Mila Quebec AI Institute ; McGill University, Montreal, Canada

farnadig@mila.quebec

&Samira Samadi

Max Planck Institute for Intelligent Systems, Tubingen AI Center, Germany

ssamadi@tuebingen.mpg.de

Lead scientific advisor on the project

###### Abstract

In recent years, Wasserstein Distributionally Robust Optimization (DRO) has garnered substantial interest for its efficacy in data-driven decision-making under distributional uncertainty. However, limited research has explored the application of DRO to address individual fairness concerns, particularly when considering causal structures and sensitive attributes in learning problems. To address this gap, we first formulate the DRO problem from causality and individual fairness perspectives. We then present the DRO dual formulation as an efficient tool to convert the DRO problem into a more tractable and computationally efficient form. Next, we characterize the closed form of the approximate worst-case loss quantity as a regularizer, eliminating the max-step in the min-max DRO problem. We further estimate the regularizer in more general cases and explore the relationship between DRO and classical robust optimization. Finally, by removing the assumption of a known structural causal model, we provide finite sample error bounds when designing DRO with empirical distributions and estimated causal structures to ensure efficiency and robust learning.

## 1 Introduction

Machine learning models must address discrimination because they often reflect and amplify biases present in their training datasets [31]. These biases can significantly influence decisions in domains such as healthcare [30], education [3], recruitment [18], and lending services [6]. Consequently, these decisions disproportionately affect individuals based on sensitive attributes like race or gender, perpetuating systemic discrimination.

To address and quantify unfairness, researchers have developed concepts like **group fairness** and **individual fairness**[45, 4]. Group fairness aims to achieve equitable outcomes across demographic groups, while individual fairness ensures that similar individuals receive similar treatment. Formally, with \(\mathcal{V}\) as the feature space and \(\mathcal{Y}\) as the label space, a model \(h:\mathcal{V}\rightarrow\mathcal{Y}\) ensures individual fairness if it satisfies the condition in [20]:

\[d_{\mathcal{Y}}(h(v),h(v^{\prime}))\leq Ld_{\mathcal{Y}}(v,v^{\prime})\quad\text{ for all }v,v^{\prime}\in\mathcal{V},\] (1)

where \(d_{\mathcal{Y}}\) and \(d_{\mathcal{Y}}\) are dissimilarity functions, often referred to as **fair metrics** on the input and output spaces. These functions capture the proximity of individuals and \(L\in\mathbb{R}^{+}\) is a Lipschitz constant. The metric \(d_{\mathcal{V}}\) reflects the intuition about which instances should be considered similar by the model.

Due to challenges in defining such metrics, group fairness is often prioritized in fairness literature because it more straightforwardly addresses observable disparities among distinct groups, making measurement and implementation easier in practice [8]. Therefore, it is crucial to study and formulate individual fairness under different assumptions in machine learning.

Individual fairness can be achieved through robust optimization methods such as **Wasserstein DRO**, which has gained significant attention for its applications in learning and decision-making [55; 43]. DRO incorporates a regularization term to mitigate overfitting [17; 26; 59]. By using a fair metric as the transportation cost function in computing the Wasserstein distance, models are designed to deliver consistent performance across varied data distributions, ensuring similar individuals receive comparable outcomes, thus satisfying individual fairness.

Incorporating causal structures and sensitive attributes into data models complicates using an individual fair metric as a cost function within the DRO framework. The fair metric must account for perturbations in sensitive attributes based on counterfactuals to ensure counterfactual fairness [22]. This can violate the positive-definite property, where \(d(v,v^{\prime})=0\) implies \(v=v^{\prime}\), a key assumption in many DRO theorems [55; 43].

Although previous works [42; 67; 70; 69; 57] have attempted to apply DRO to address individual fairness, they often do not explore the implications when causal structures and sensitive attributes are present in the learning problem. These studies are typically limited to linear **Structural Causal Model (SCM)** with specific metrics and do not discuss the form of the regularizer for other classical DRO theorems when using a fair metric. To accurately compare our work with related studies, we will postpone this discussion until after presenting our results in Section 4.1.

### Our Contributions

In this work, we adopt the definition of a _fair metric_ from [22] to define a **Causally Fair Dissimilarity Function (CFDF)**, which delineates how to establish a fair metric through causality and sensitive attributes. Using CFDF, we introduce **Causally Fair DRO** and present a strong duality theorem for our approach. Under mild assumptions about CFDF and causal structure, we demonstrate that the DRO regularizer can be estimated, or in some cases can be explicitly solved. This estimation often leads to being more practical and computationally efficient than solving the min-max problem in (4), as supported by advancements in algorithms from previous research such as [14; 15]. Finally, Our numerical analysis of both real and synthetic data demonstrates the practicality of our theoretical framework in real-world applications. (SS 5). In summary, the main contributions of this work are:

* Define a causally fair dissimilarity function, an individual fair metric incorporating causal structures and sensitive attributes (Def. 1), along with its representation form (Prop. 1).
* Define a causally fair DRO problem with a causally fair dissimilarity function cost (SS 4).
* Present the strong duality theorem for causally fair DRO (Thm. 1).
* Provide the exact regularizer for linear SCM under mild conditions for the loss function in regression and classification problems (Thm. 2 and Thm. 3).
* Estimate the first-order causally fair DRO regularizer for non-linear SCM (Thm. 4).
* Provide the relation between classical robust optimization and causally fair DRO (Prop. 2).
* Demonstrate that under unknown SCM assumptions, by estimating the SCM or cost function, we have finite sample guarantees for convergence of empirical DRO problems (Thm. 5).

## 2 Preliminaries & Notations

Data Model.Let \(\mathbf{V}\in\mathcal{V}\) denote a vector of feature space (predictor variables) and let \(\mathbf{Y}\in\mathcal{Y}\) represent the response variable, such that \(\mathbf{Z}=(\mathbf{V},\mathbf{Y})\) comprises the observation variables with an underlying probability \(\mathbb{P}_{*}\). Furthermore, assume that the feature vector \(\mathbf{V}=(\mathbf{A},\mathbf{X})\) comprises both sensitive attributes \(\mathbf{A}\in\mathcal{A}\) and non-sensitive attributes \(\mathbf{X}\in\mathcal{X}\). Let \(\{z^{i}=(v^{i},y^{i})\}_{i=1}^{N}\) represent the observations used to construct the empirical distribution \(\mathbb{P}_{N}\), defined as \(\mathbb{P}_{N}\coloneqq\frac{1}{N}\sum_{i=1}^{N}\delta_{z^{i}}\), where \(\delta_{z}\) is the Dirac delta function. Given a loss function \(\ell:\mathcal{Z}\times\Theta\rightarrow\mathbb{R}\), the risk function for a parameter \(\theta\in\Theta\) and a probability measure \(\mathbb{P}\) is \(\mathcal{R}(\mathbb{P},\theta)=\mathbb{E}_{\mathbb{P}}\left[\ell(Z,\theta)\right]\). This leads to the common **empirical risk minimization** approach. This method seeks to find the minimizer \(\theta_{N}^{\text{em}}\) within the set \(\theta_{N}^{\text{em}}\in\arg\min_{\theta\in\Theta}\mathcal{R}(\mathbb{P}_{N},\theta)\), as an empirical way to obtaining the optimal solution \(\theta_{*}\), which is given by \(\theta_{*}=\inf_{\theta\in\Theta}\mathcal{R}(\mathbb{P}_{*},\theta)\).

Assume the feature space is represented by a **structural causal model** (**SCM**) \(\mathcal{M}=\langle\mathcal{G},\mathbf{V},\mathbf{U},\mathbb{P}_{\mathbb{U}}\rangle\)[51]. This model includes **structural equations**\(\{\mathbf{V}_{i}:=f_{i}(\mathbf{V}_{\mathbf{P}u(i)},\mathbf{U}_{i})\}_{i=1}^ {n}\), which delineate the causal relations among an endogenous variable \(\mathbf{V}_{i}\), its causal predecessors \(\mathbf{V}_{\mathbf{P}u(i)}\), and an exogenous variable \(\mathbf{U}_{i}\) representing unobservable factors. The model's structure is encapsulated in a directed acyclic graph \(\mathcal{G}\). Exogenous variables are posited as mutually independent, enabling \(\mathbb{P}\mathbf{U}\) to be expressed as \(\prod_{i=1}^{n}\mathbb{P}_{\mathbf{U}_{i}}\), assuming causal sufficiency and excluding hidden confounders [53].

Counterfactuals.In causal structures, data perturbation is achieved through **counterfactuals**, which are derived from **interventions** in SCMs. These interventions, conducted using \(do\)-calculus, include both **hard** and **soft** types [51]. Hard interventions fix a subset \(\mathcal{I}\subseteq\{1,\ldots,n\}\) of features \(\mathbf{V}_{\mathcal{I}}\) to a constant \(\tau\), modifying their causal connections within the causal graph while maintaining the structural equations of other features [51]. This type of intervention is denoted as \(\mathcal{M}^{do(\mathbf{V}_{\mathcal{I}}:=\tau)}\) and its structural equations are obtained by:

\[\{\mathbf{V}_{i}:=\tau_{i},\quad\forall i\in\mathcal{I};\quad\mathbf{V}_{i}: =f_{i}(\mathbf{V}_{\mathbf{P}\mathbf{u}(i)},\mathbf{U}_{i}),\quad\forall i \notin\mathcal{I}\}.\]

Soft interventions, on the other hand, adjust the functions in the structural equations, such as through additive interventions, without disrupting existing causal links [53]. In an **additive (or shift)** intervention, a value \(\Delta\in\mathbb{R}^{n}\) is added to each feature within the SCM to enact manipulation:

\[\{\mathbf{V}_{i}:=f_{i}(\mathbf{V}_{\mathbf{P}\mathbf{u}(i)},\mathbf{U}_{i})+ \Delta_{i}\}_{i=1}^{n}.\]

In SCMs, counterfactuals are computed by modifying structural equations to reflect hard interventions on specific variables, thus exploring what would occur if the intervention was applied. Under the assumption of acyclicity, a unique function \(F:\mathcal{U}\rightarrow\mathcal{V}\) exists such that \(F(u)=v\). Acyclicity remains unchanged by either hard or shift interventions, allowing for the existence of modified functions \(F^{do(\mathbf{V}_{\mathcal{I}}:=\tau)}\) and \(F^{do(\mathbf{V}_{\mathcal{I}}:=\Delta)}\) corresponding to these interventions, respectively. The counterfactual outcome for a hard intervention can thus be calculated using \(\mathbf{CF}(v,\tau)=F^{do(\mathbf{V}_{\mathcal{I}}:=\tau)}(F^{-1}(v))\), and similarly, for a shift intervention, it is defined as \(\mathbf{CF}(v,\Delta)\). These interventions are frequently applied in this analysis.

Counterfactuals involving the modification of sensitive attributes (termed **twins**) are essential for addressing individual-level fairness [40, 64]. Twins are generated by altering the sensitive attribute from \(a\) to \(a^{\prime}\) across its domain \(\mathcal{A}\). For any instance, \(v\in\mathcal{V}\), a set of counterfactual twins is produced as \(\{\ddot{v}_{a}=\text{CF}(v,a):a\in\mathcal{A}\}\), facilitating the analysis of fairness by comparing outcomes under different sensitive attribute values.

Counterfactual Identifiability.To estimate the effects of interventions from observational data, counterfactuals must be **identifiable** within a causal framework. A notable example of such identifiable SCMs is the **additive noise models (ANMs)**, which suggest that structural equations can be represented as:

\[\{\mathbf{V}_{i}\coloneqq f_{i}(\mathbf{V}_{\mathbf{P}\mathbf{u}(i)})+\mathbf{ U}_{i}\}_{i=1}^{n}\implies\mathbf{U}=(I-f)(\mathbf{V})\implies\quad \mathbf{V}=(I-f)^{-1}(\mathbf{U})\] (2)

leading to a bijective mapping between \(U_{i}\) and \(V_{i}\), ensuring no loss of information from exogenous to endogenous variables[50]. This relationship implies that \(\mathbf{V}\) can be derived from \(\mathbf{U}\) through a bijective reduced-form mapping \(F=(I-f)^{-1}\), where \(I(x)=x\) is the identity function. Besides ANM, there are other counterfactually identifiable models such as LSNM [34] and PNL [71]. However, for the sake of simplicity, our focus remains on ANM. **Linear SCMs** is a specific instance of ANMs, characterized by linear functions \(f_{i}\).

Individual Fairness Through Robustness.In machine learning, individual fairness [20] is achieved through robustness by ensuring that similar individuals receive similar outcomes, regardless of variations in their inputs. This concept aligns with the notion of Lipschitz continuity in decision functions (Eq. 1), where small changes in input should not lead to excessively large changes in output.

Depending on how the uncertainty set is defined, various types of robust optimization can be employed. In **adversarially robust optimization**[44, 7], the uncertainty set is defined by introducing a slight perturbation \(\delta\) based on the metric \(d\) to the input data. The goal is to find the optimal \(\theta\) that minimizes risk even under the worst-case perturbation quantity:

\[\mathcal{R}_{\delta}^{\text{\emph{adv}}}(\mathbb{P},\theta)=\mathop{\mathbb{ E}}_{v\sim\mathbb{P}}\left[\sup_{d^{p}(v,v+\Delta)\leq\delta}\ell(v+\Delta,y, \theta)\right],\] (3)

where \(p\in[0,\infty]\). This formulation ensures that the optimization considers the maximum potential loss within the defined perturbation bounds.

In **counterfactually robust optimization**[40, 37, 64, 23, 24], the uncertainty set is generated by twins, which are obtained by creating counterfactuals concerning all levels of the sensitive attribute. In this scenario, the worst-case loss quantity is obtained by calculating the maximum loss over the twins of the input data:

\[\mathcal{R}_{\delta}^{\text{\emph{cf}}}(\mathbb{P},\theta)=\mathop{\mathbb{ E}}_{v\sim\mathbb{P}}\left[\sup_{\alpha\in\mathcal{A}}\ell(\tilde{v}_{a},y, \theta)\right].\]

**Distributionally Robust Optimization**[43, 55] is a data-driven approach designed to minimize the discrepancies between in-sample and out-of-sample expected losses, using ambiguity sets based on Wasserstein distances. Consider a lower semi-continuous cost function \(c(\cdot,\cdot):\mathcal{Z}\times\mathcal{Z}\rightarrow[0,\infty]\) that satisfies \(c(z,z)=0\) for all \(z\in\mathcal{Z}\), serving as a fair metric. The **optimal transport cost** between two distributions \(\mathbb{P},\mathbb{Q}\in\mathcal{P}(\mathcal{Z})\), is represented by:

\[W_{c,p}\left(\mathbb{P},\mathbb{Q}\right)\triangleq\min_{\pi\in\mathcal{P}( \mathcal{Z}\times\mathcal{Z})}\left\{\left(\mathop{\mathbb{E}}_{(z,z^{\prime })\sim\pi}[c^{p}(z,z^{\prime})]\right)^{\frac{1}{p}}:\pi_{1}=\mathbb{P},\pi_{2} =\mathbb{Q}\right\},\]

Here, \(\pi\in\mathcal{P}(\mathcal{Z}\times\mathcal{Z})\) denotes the set of all joint probability distributions, and \(\pi_{1}\) and \(\pi_{2}\) are the marginals of \(\pi\) under first and second coordinates [54, 63]. When \(c(z,z^{\prime})\) acts as a metric (in mathematics term) on \(\mathcal{Z}\), \(W_{c,p}\) is called the **Wasserstein distance**[63].

An important ingredient in the DRO formulation is the description of the distributional uncertainty region \(\mathbb{B}_{\delta}(\mathbb{P})\) that is defined by optimal transport cost:

\[\mathbb{B}_{\delta}(\mathbb{P})\coloneqq\{\mathbb{Q}\in\mathcal{P}(\mathcal{V }):W_{c,p}(\mathbb{Q},\mathbb{P})\leq\delta\}.\]

DRO problem minimizes worst-case loss quantity:

\[\mathcal{R}_{\delta}(\mathbb{P},\theta)\triangleq\sup_{\mathbb{Q}\ \in\ \mathbb{B}_{\delta}(\mathbb{P})}\big{\{}\mathbf{E}_{\mathbb{Q}}[\ell(\mathbf{Z},\theta)]\big{\}},\] (4)

and obtained the \(\theta_{N}^{\text{\emph{dro}}}\in\operatorname*{arg\,min}_{\theta\in\Theta} \mathcal{R}_{\delta}(\mathbb{P}_{N},\theta)\). The main tool in DRO is the **strong duality theorem**[26, 46], which converts an infinite-dimensional problem into a finite optimization problem. The theorem states that:

\[\sup_{\mathbb{Q}\in\mathbb{B}_{\delta}(\mathbb{P})}\left\{\mathop{\mathbb{E}} _{v\sim\mathbb{Q}}[\psi(v)]\right\}=\inf_{\lambda\geq 0}\left\{\lambda \delta^{p}+\mathop{\mathbb{E}}_{v\sim\mathbb{P}}[\psi_{\lambda}(v)]\right\},\] (5)

where \(\psi_{\lambda}(v)\) is defined as \(\psi_{\lambda}(v)\coloneqq\sup_{v^{\prime}\in\mathcal{V}}\left\{\psi(v^{ \prime})-\lambda d^{p}(v,v^{\prime})\right\}\).

## 3 Causally Fair Dissimilarity Function

The key to robust optimization and individual fairness is the metric that measures individual similarity. This section outlines the properties of such a metric in a causal framework to protect sensitive attributes. We begin with an illustrative example.

**Example 1**: _Let \(\mathcal{M}_{1}\) and \(\mathcal{M}_{2}\) represent two SCMs describing the relationships among the variables gender (\(\mathbf{G}\)), education (\(\mathbf{E}\)), and income (\(\mathbf{I}\)). \(\mathcal{M}_{1}\) models these variables as independent, whereas \(\mathcal{M}_{2}\) specifies a linear causal relationship:_

\[\mathcal{M}_{1}=\begin{cases}\mathbf{G}:=\mathbf{U}_{G},&\mathbf{U}_{G}\sim \mathcal{B}(0.5)\\ \mathbf{E}:=\mathbf{U}_{E},&\mathbf{U}_{E}\sim\mathcal{N}(0,1)\,\\ \mathbf{I}:=\mathbf{U}_{I},&\mathbf{U}_{I}\sim\mathcal{N}(0,1)\end{cases}, \mathcal{M}_{2}=\begin{cases}\mathbf{G}:=\mathbf{U}_{G},&\mathbf{U}_{G}\sim \mathcal{B}(0.5)\\ \mathbf{E}:=\mathbf{G}+\mathbf{U}_{E},&\mathbf{U}_{E}\sim\mathcal{N}(0,1)\\ \mathbf{I}:=\mathbf{G}+2\mathbf{E}+\mathbf{U}_{I},&\mathbf{U}_{I}\sim\mathcal{ N}(0,1)\end{cases},\]_Where \(\mathbf{U}_{G}\) represents the population distribution of gender, modeled by a Bernoulli distribution, while \(\mathbf{U}_{E}\) and \(\mathbf{U}_{I}\) are intrinsic talents for academic and income achievements, respectively, modeled by normal distributions. To compare individuals, let's consider the \(L_{1}\) norm on non-sensitive attributes (\(d(v,v^{\prime})=|e-e^{\prime}|+|i-i^{\prime}|\)). If two individuals have less than a 0.1 unit difference, they are deemed similar. Now, consider an individual with data \(v=(M,1,1)\). Based on experience, we expect that a perturbation in educational talent by 05 units will not significantly alter this individual's status. We model this perturbation with a shift intervention \(\Delta=(0,.05,0)\). In Model 1, the result \(\textbf{CF}(v,\Delta)=(M,1.05,1)\) is considered similar to \(v\). However, in Model 2, \(\textbf{CF}(v,\Delta)=(M,1.05,1.1)\) results in a distance of \(d(v,\textbf{CF}(v,\Delta))=0.15\), indicating dissimilarity. In the presence of causality, one attribute can be amplified multiple times in the final feature space. Therefore, we need to control our intuition of dissimilarity between the exogenous variables and the feature space._

_To protect against gender bias, we need to ensure that people with the same intrinsic characteristics but different genders behave similarly. This is modeled by a counterfactual change in gender. In Model 1, \(\textbf{CF}(v,F)=(F,1,1)\) shows no difference (\(d(v,\hat{v}_{F})=0\)). However, in Model 2, \(\textbf{CF}(v,F)=(F,0,-2)\) results \(d(v,\hat{v}_{F})=4\), which means that they are not similar._

The example 1 demonstrates that in the presence of causality and protected variables, the standard \(l_{p}\)-norm or any metric fails to accurately capture the intuition of similarity. In these scenarios, a dissimilarity function should incorporate counterfactuals and uniformly control for non-sensitive perturbations to effectively capture proximity. This approach is further elaborated in the following definition. Before proceeding, we introduce some notation. For a vector \(v\) or \(u\), we define \(P_{\mathcal{A}}(\cdot)\) and \(P_{\mathcal{X}}(\cdot)\) as the projections onto the sensitive and non-sensitive parts, respectively.

**Definition 1** (Causally Fair Dissimilarity Function): _Let \(d:\mathcal{V}\times\mathcal{V}\rightarrow[0,\infty]\) be a dissimilarity function defined on the feature space \(\mathcal{V}\), generated by a SCM \(\mathcal{M}\). Let \(\mathbf{A}\) denote a set of sensitive attributes, and \(\mathcal{I}\) represent their corresponding index within \(\{1,\ldots,n\}\). The metric is called a causally fair dissimilarity function or **CFDF** if it adheres to the following properties:_

* _Zero Dissimilarity for Twin Pairs:_ _For any_ \(v\in\mathcal{V}\) _and_ \(a\in\mathbf{A}\)_, the dissimilarity_ \(d(v,\hat{v}_{a})\) _between an instance and its twins is zero._
* _Guaranteed Similarity for Minor Perturbations:_ _For every_ \(v\in\mathcal{V}\) _and any_ \(\delta>0\)_, there exists an_ \(\epsilon\) _such that for any sufficiently small intervention_ \((\|\Delta\|\leq\epsilon)\) _on the non-sensitive attributes (_\(P_{\mathcal{A}}(\Delta)=0\)_), the distance_ \(d(v,\textbf{CF}(v,\Delta))\) _remains less than_ \(\delta\)_._

To understand the shape of \(d\) under the assumptions of Def. 1, we must first recognize that the CFDF needs to be defined on a larger space than \(\mathrm{Range}(\mathcal{M})\)[22]. This is because, generally, when \(\mathcal{M}\) is intervened upon by some sensitive attribute level \(a\), we have \(\mathrm{Range}(\mathcal{M})\subseteq\bigcup_{a\in\mathcal{A}}\mathrm{Range}( \mathcal{M}^{do(\mathbf{A}:=a)})\). The complete space encompassing all counterfactual values can be defined as follows.

**Definition 2** (Parent-Free Sensitive Attribute SCM): _Consider \(\mathcal{M}\) with sensitive attributes indexed by \(\mathcal{I}\). The parent-free sensitive attribute SCM denoted as \(\mathcal{M}_{0}\), is derived from \(\mathcal{M}\) by removing the causal effects of parents of sensitive attributes and replacing their exogenous variables with indigenous ones. The structural equations for \(\mathcal{M}_{0}\) are as follows:_

\[\mathbf{V}_{i}^{0}\coloneqq\begin{cases}\mathbf{U}_{i}&\mathbf{U}_{i}:= \mathbf{V}_{i}\sim\mathbb{P}_{\mathbf{V}_{i}},&i\in\mathcal{I}\\ f_{i}(\mathbf{V}_{\textbf{pa}(i)}^{0}+\mathbf{U}_{i}&\mathbf{U}_{i}\sim\mathbb{ P}_{\mathbf{U}_{i}},&i\notin\mathcal{I}\end{cases}\]

_The exogenous space corresponding to \(\mathcal{M}_{0}\), denoted by \(\mathcal{U}_{0}\), includes the sensitive attributes and the non-sensitive parts of the exogenous variables of \(\mathcal{M}\). This space called the semi-latent space, is constructed as \(\mathcal{U}_{0}=\mathcal{A}\times\mathcal{U}_{\mathcal{X}}\), where \(\mathcal{U}_{\mathcal{X}}\) is the non-sensitive part of the exogenous space in \(\mathcal{M}\)._

If we know the structural equations of \(\mathcal{M}\), we can first map the CFDF to the exogenous space. In this space, the exogenous variables are assumed to be independent. Therefore, we can design a dissimilarity function for each variable separately and then combine them using product topology (SS.2[48]). Following this intuition, we introduce the bijective map \(g:\mathcal{V}\rightarrow\mathcal{U}_{0}\) from the feature space to the semi-latent space, along with its inverse, defined as follows:

\[g_{i}(v)\coloneqq\begin{cases}v_{i}&i\in\mathcal{I}\\ F_{i}(v)&i\notin\mathcal{I}\end{cases},\;\;\;g_{i}^{-1}(u)\coloneqq\begin{cases}u_{ i}&i\in\mathcal{I}\\ f_{i}(g_{\textbf{pa}(i)}^{-1}(u))+u_{i}&i\notin\mathcal{I}\end{cases}\] (6)If all sensitive attributes have no parents, the semi-latent space is equivalent to the exogenous space, and \(g=F^{-1}\). The counterfactual with respect to \(\mathcal{M}_{0}\) is denoted by \(\textbf{CF}_{0}(v,\Delta)\). We can now present the following proposition to determine the shape of \(d\).

**Proposition 1**: _Let \(\mathcal{M}\) be an ANM, with \(g\) as its corresponding map to the semi-latent space 6, and \(P_{\mathcal{X}}(u)\) the projection of vector \(u\) to the non-sensitive part \(\mathcal{U}_{\mathcal{X}}\). Then:_

_(i) If_ \(d_{\mathcal{X}}\) _is a continuous dissimilarity function on diagonal_ \(\mathcal{U}_{\mathcal{X}}\times\mathcal{U}_{\mathcal{X}}\)_, then the function_ \(d\) _defined as:_

\[d(v,v^{\prime})=d_{\mathcal{X}}(P_{\mathcal{X}}(g(v)),P_{\mathcal{X}}(g(v^{ \prime})))\] (7)

_satisfies the definitions of a CFDF._

_(ii) If_ \(d:\mathcal{V}\times\mathcal{V}\rightarrow[0,\infty]\) _satisfies the CFDF definition and the triangle inequality property, then_ \(d\) _can be represented as a dissimilarity function_ \(d_{\mathcal{X}}\) _dependent solely on the non-sensitive components_ \(\mathcal{U}_{\mathcal{X}}\) _i.e.,_ \(d(v,v^{\prime})=d_{\mathcal{X}}(P_{\mathcal{X}}(g(v)),P_{\mathcal{X}}(g(v^{ \prime})))\)_._

Since \(d_{\mathcal{X}}\) is defined on independent coordinates, its relation to the components is less complex than the CFDF \(d\). We assume the dissimilarity function \(d_{\mathcal{X}}(x,x^{\prime})\) is translation-invariant. Therefore, for simplicity, we assume \(d_{\mathcal{X}}(x^{\prime},x)=\|x^{\prime}-x\|\). The dual of \(\|\cdot\|\) is defined as \(\|x\|_{*}=\sup_{x^{\prime}}\{x^{T}x^{\prime}\mid\|x^{\prime}\|\leq 1\}\). Now we establish our assumptions about the SCM and its CFDF.

**Assumption 1**:
* \(\mathcal{M}\) _is an ANM with known structural equations and a semi-latent map_ \(g\)_._
* _The CFDF is defined as_ \(d(v,v^{\prime})=\|P_{\mathcal{X}}(g(v))-P_{\mathcal{X}}(g(v^{\prime}))\|\)_, where_ \(\|.\|\) _is a some norm._
* _Cost function over_ \(\mathcal{Z}\) _has form_ \(c((v,y),(v^{\prime},y^{\prime}))=d(v,v^{\prime})+\infty\cdot|y-y^{\prime}|\)_._
* _The ambiguity set is defined as:_ \(\mathbb{B}_{\delta}(\mathbb{P})=\{\mathbb{Q}\in\mathcal{P}(\mathcal{V}):W_{c, p}(\mathbb{P},\mathbb{Q})\leq\delta\}\)_, for_ \(p\in[1,\infty)\)_._

**Remark 1**: _All results of this work apply to the **homogeneous dissimilarity function** (Def. 6), which includes a broad family of dissimilarity functions, such as norms._

## 4 Causally Fair Distributionally Robust Optimization

To find out the impact of the CFDF in DRO problems, we first consider the dual form of the worst-case loss quantity, which simplifies the infinite-dimensional primal problem into a more tractable and computationally manageable form.

**Theorem 1** (Causally Fair Strong Duality): _If Assumption 1 is satisfied, then for any reference probability distribution \(\mathbb{P}\) and any function \(\psi:\mathcal{V}\rightarrow\mathbb{R}\) that is both upper semi-continuous and \(L_{1}\)-integrable, the following duality holds:_

\[\sup_{\mathbb{Q}\in\mathbb{B}_{\delta}(\mathbb{P})}\left\{\underset{v\sim \mathbb{Q}}{\mathbb{E}}\left[\psi(v)\right]\right\}=\inf_{\lambda\geq 0} \left\{\lambda\delta^{p}+\underset{v\sim\mathbb{P}}{\mathbb{E}}\left[\sup_{a \in\mathcal{A}}\psi_{\lambda}(\ddot{v}_{a})\right]\right\},\] (8)

_where \(\psi_{\lambda}(v)\) is defined as_

\[\psi_{\lambda}(v)\coloneqq\sup_{\Delta\in\mathcal{X}}\left\{\psi(\textbf{CF}_ {0}(v,\Delta))-\lambda^{p}d(v,\textbf{CF}_{0}(v,\Delta))\right\},\] (9)

_and \(\textbf{CF}_{0}\) is counterfactual regarding parent-free SCM \(\mathcal{M}_{0}\)._

**Remark 2**: _The intuition behind the above formula is as follows: In the case where all features are independent, let \(v=(a,x)\). The CFDF should exhibit no difference between \((a,x)\) and \((a^{\prime},x)\) for each \(a,a^{\prime}\in\mathcal{A}\). Consequently, the distance metric satisfies \(d((a,x),(a^{\prime},x^{\prime}))=d_{\mathcal{X}}(x,x^{\prime})\). Under this condition, the classical strong duality theorem (Eq. 5) provides the following relationship:_

\[\psi_{\lambda}(v)=\sup_{(a^{\prime},x^{\prime})\in\mathcal{V}}\left\{\psi((a^{ \prime},x^{\prime}))-\lambda d^{p}_{\mathcal{X}}(x,x^{\prime})\right\}=\sup_{a \in\mathcal{A}}\left\{\sup_{\Delta\in\mathcal{X}}\psi((a,x+\Delta))-\lambda d ^{p}_{\mathcal{X}}(x,x+\Delta)\right\}\]

_When we incorporate causal structure instead of coordinating \(a\) and \(x\), the two dimensions \(\ddot{v}_{a}\) and \(\textbf{CF}_{0}(v,\Delta)\) are replaced accordingly._In the DRO formulation, the worst-case loss is expressed in a dual form and can act as a regularizer for parameter learning. Explicitly solving the dual problem eliminates the need to compute the worst-case distribution, resulting in faster, more efficient learning algorithms [14, 16, 62, 73]. Before presenting the general theorem, the next two theorems show that, under mild conditions, the dual formula for specific loss functions in classification and regression problems can be explicitly solved.

**Theorem 2** (Higher Order Linear Loss): _Given Assumptions 1, let \(\mathcal{M}\) be a linear SCM and the loss function \(\ell(z,\theta)^{p}\), where \(\ell(z,\theta)\) is of the form \(h(y-\langle\theta,v\rangle)\) or \(h(y\cdot\langle\theta,v\rangle)\) for functions \(h(t)\) such as \(|t|\), \(\max(0,t)\), \(|t-\tau|\), or \(\max(0,t-\tau)\) for some \(\tau\geq 0\), and \(p\in[1,\infty)\). Then the DRO problem 4 can be reduced to:_

\[\mathcal{R}_{\delta}(\mathbb{P}_{N},\theta)=\begin{cases}\left(\mathcal{R}_{ \delta}^{cf}(\mathbb{P}_{N},\theta)^{\frac{1}{p}}+\delta\left\|P_{\mathcal{X} }(M^{T}\theta)\right\|_{*}\right)^{p},&\text{diam}\left(\mathcal{A}\right)< \infty\\ \\ \left(\mathcal{R}(\mathbb{P}_{N},\theta)^{\frac{1}{p}}+\delta\left\|P_{ \mathcal{X}}(M^{T}\theta)\right\|_{*}\right)^{p},&\text{s.t.}\quad P_{ \mathcal{A}}(M^{T}\theta)=0;&\text{diam}\left(\mathcal{A}\right)=\infty\end{cases}\]

_where \(M\) is the corresponding matrix for the linear map \(g^{-1}\) (see Eq. 6)._

**Remark 3**: _In real-world datasets, the sensitive part always satisfies \(\text{diam}\left(\mathcal{A}\right)<\infty\). According to the above theorem, \(\mathcal{R}_{\delta}(\mathbb{P}_{N},\theta)\geq\mathcal{R}_{\delta}^{cf}( \mathbb{P}_{N},\theta)\). For practical applications, if the worst-case loss must not exceed a certain value, we can replace \(\infty\) with some constant in the above theorem._

**Example 2**: _Here are specific examples of the above theorem. We offer a framework to study the equivalence between the worst-case loss in the DRO problem, with the cost function derived from the CFDF, and the regularization scheme for classification and regression problems._

\begin{tabular}{|c c|} \hline Regression & Lower Partial Moments \\ \(\mathrm{E}_{\mathbb{P}}[|\mathbf{Y}-\langle\theta,\mathbf{V}\rangle|^{p}]\), \(p\geq 1\) & \(\mathrm{E}_{\mathbb{P}}[(\mathbf{Y}-\langle\theta,\mathbf{V}\rangle-\tau)_{+ }^{p}]\), \(p\geq 1\), \(\tau\in\mathbb{R}\) \\ \hline Ridge Linear Regression & \(\tau\)-Insensitive Regression \\ \(\mathrm{E}_{\mathbb{P}}[(\mathbf{Y}+\langle\theta,\mathbf{V}\rangle)^{2}]\) & \(\mathrm{E}_{\mathbb{P}}[(|\mathbf{Y}-\langle\theta,\mathbf{V}\rangle|-\tau)_{+ }^{p}]\), \(p\geq 1\), \(\tau\in\mathbb{R}\) \\ \hline Hinge Loss Binary Classification & Support Vector Machine Classification \\ \(\mathrm{E}_{\mathbb{P}}[(1-\mathbf{Y}\cdot\langle\theta,\mathbf{V}\rangle)_{+ }^{p}]\), \(p\geq 1\) & \(\mathrm{E}_{\mathbb{P}}[|1-\mathbf{Y}\cdot\langle\theta,\mathbf{V}\rangle|^{p}]\), \(p\geq 1\) \\ \hline \end{tabular}

The Thm. 2 can be extended to the non-linear regression loss function.

**Theorem 3** (Nonlinear Loss): _Let assumptions 1 be satisfied, with \(p=1\), \(\mathcal{M}\) linear with matrix \(M\) corresponding to map \(g^{-1}\), and a loss function \(\ell(z,\theta)\) of the form \(h(y-\langle\theta,v\rangle)\) for regression and \(h(y\cdot\langle\theta,v\rangle)\) for classification, where \(h\) has the following two properties:_

1. \(h\) _is Lipschitz on_ \(\mathbb{R}\) _with_ \(L_{h}\) _constant, i.e.,_ \(|h(t_{2})-h(t_{1})|\leq L_{h}|t_{2}-t_{1}|,\quad\forall t_{1},t_{2}\in\mathbb{R}\)_._
2. _There exists sequence of_ \(\{t_{k}\}_{k=1}^{\infty}\) _goes to_ \(\infty\) _such that for each_ \(t_{0}\in\mathbb{R}\) _we have_ \( lim_{k\to\infty}\frac{|h(t_{0}+t_{k})-h(t_{0})|}{|t_{k}|}=L_{h}\)_._

_By the above assumption, DRO problem 4 can be reduced as:_

\[\mathcal{R}_{\delta}(\mathbb{P}_{N},\theta)=\begin{cases}\mathcal{R}_{\delta}^{ cf}(\mathbb{P}_{N},\theta)+\delta L_{h}\left\|P_{\mathcal{X}}(M^{T}\theta) \right\|_{*},&\text{diam}(\mathcal{A})<\infty\\ \\ \mathcal{R}(\mathbb{P}_{N},\theta)+\delta L_{h}\left\|P_{\mathcal{X}}(M^{T} \theta)\right\|_{*},&\text{s.t.}\quad P_{\mathcal{A}}(M^{T}\theta)=0;&\text{ diam}(\mathcal{A})=\infty\end{cases}\]

**Example 3**: _The following forms of the loss function satisfy the conditions of \(h\) in Thm. 3:_

Now, the first-order estimation of the regularizer for non-linear SCM and loss function is ready to be stated.

**Theorem 4** (First-Order Estimation of DRO Regularizer): _Assume \(\mathcal{M}\) has structural equation \(f\), which \(f\) and loss function \(\ell\) are both twice continuously differentiable respect to non-sensitiveattributes, \(\operatorname{diam}\left(\mathcal{U}\right)<\infty\) and \(c\) satisfies the assumption 1 with \(p\in[2,\infty]\). The necessary condition for the existence of a finite DRO solution is that for each \(v\in\mathcal{V}\):_

\[\sup_{a\in\mathcal{A}}\left\{\ell(\tilde{v}_{a},y,\theta)\right\}<\infty.\]

_By these conditions, the worst-case loss quantity is equal to:_

\[\mathcal{R}_{\delta}(\mathbb{P}_{N},\theta)=\operatorname*{\mathbb{E}}_{v \sim\mathbb{P}_{N}}\left[\sup_{a\in\mathcal{A}}\ell(\tilde{v}_{a},y,\theta) \right]+\delta\cdot\left(\operatorname*{\mathbb{E}}_{v\sim\mathbb{P}_{N}} \left[\sup_{a\in\mathcal{A}}\lVert\nabla^{\sigma}\ell(\tilde{v}_{a},y,\theta) \rVert_{*}^{q}\right]\right)^{1/q}+O(\delta^{2}),\] (10)

_where the \(O(\delta^{2})\) term is uniform over all \(\theta\in\Theta\), \(q\) is p's conjugate, and the gradient \(\nabla^{\sigma}\ell\) equals to:_

\[\nabla^{\sigma}\ell(v,y,\theta)=\lim_{\Delta\to 0}\frac{\ell(\textbf{ CF}_{0}(v,\Delta),y,\theta)-\ell(v,y,\theta)}{\lVert\Delta\rVert}\]

_where \(\textbf{CF}_{0}\) is counterfactual regarding parent-free SCM \(\mathcal{M}_{0}\)._

By applying Prop. 2 from Gao's work [28], the next proposition presents the relationship between classical adversarial optimization 3 and DRO for CFDF.

**Proposition 2** (Approximation by Robust Optimization): _Suppose \(\mathcal{A}\) is a finite set and let \(\{(v^{i},y^{i})\}_{i=1}^{N}\) be observational data. Under Assumption 1, assume that for the loss function \(\ell\) there exist constants \(L,M\geq 0\) such that_

\[\lvert\ell(v,y,\theta)-\ell(v^{\prime},y,\theta)\rvert<\text{Ld}^{p}(v,v^{ \prime})+M\quad\text{for all}\quad v,v^{\prime}\in\mathcal{V}\text{ and }p\in[1,\infty).\]

_For an arbitrary \(K\in\mathbb{N}\), consider the adversarial loss within the setting:_

\[\tilde{\mathcal{R}}_{\delta}^{adv}(\mathbb{P}_{N}):=\sup_{(w^{ik})_{i,k}\in \tilde{B}_{\delta}}\left\{\frac{1}{NK}\sum_{i=1}^{N}\sum_{k=1}^{K}\sup_{a\in \mathcal{A}}\ell(\tilde{w}_{a}^{ik},y_{i},\theta)\right\},\]

_where the uncertainty set \(\tilde{B}_{\delta}\) is defined as:_

\[\tilde{B}_{\delta}:=\left\{(w^{ik})_{i,k}:\frac{1}{N}\sum_{i=1}^{N}\sum_{k=1} ^{K}d^{p}(v^{i},w^{ik})\leq\delta,\,w^{ik}\in\mathcal{V}\right\}.\]

_Then, the DRO can be approximated by adversarial optimization as follows:_

\[\tilde{\mathcal{R}}_{\delta}^{adv}(\mathbb{P}_{N})\leq\mathcal{R}_{\delta}( \mathbb{P}_{N})\leq\tilde{\mathcal{R}}_{\delta}^{adv}(\mathbb{P}_{N})+\frac{ LD+M}{NK},\]

_where \(D\) is independent of \(K\)._

One of the main challenges in designing DRO for SCMs is that the CFDF depends on the causal structure. When the functional structure is unknown, it must be estimated from data. This empirical estimation impacts the DRO learning process. Therefore, it is crucial to control the uniform convergence error of the DRO problem between the true metric and distribution and the DRO estimated from the data. The following theorem guarantees learning from sample data, but certain assumptions need to be established first.

**Assumption 2**:
1. \(\mathcal{M}\) _is an unknown ANM,_ \(\operatorname{diam}\left(\mathcal{V}\right)<\infty\)_, and_ \(\Theta\) _is a compact subset of_ \(\mathbb{R}^{d}\)_._
2. _The loss function_ \(\ell\) _is uniformly bounded: there exists a positive constant_ \(M\) _such that_ \(0\leq\ell(z,\theta)\leq M\) _for all_ \(\theta\in\Theta\)_. Moreover,_ \(\ell\) _is Lipschitz with respect to the counterfactual in_ \(\mathcal{M}_{0}\)_; that is, there exists a constant_ \(L\) _such that:_ \[|\ell(v,y,\theta)-\ell(\textbf{CF}_{0}(v,\Delta),y,\theta)|\leq\left\|\ell \right\|_{\operatorname{Lip}}).\]
3. \(\hat{d}\) _is an estimation of the CFDF such that, with probability_ \(1-\epsilon\)_, there exists_ \(M_{d}\) _such that, at a rate of_ \(N^{-\eta}\)_, the discrepancy is uniformly bounded by:_ \[\forall v,v^{\prime}\in\mathcal{V}:\quad|d(v,v^{\prime})-\hat{d}(v,v^{\prime} )|\leq M_{d}N^{-\eta},\quad\text{for some}\quad\eta>0.\]

The following theorem states that the efforts to estimate the metric or causal structures and the parameter \(\hat{\theta}_{N}^{\text{do}}\),

\[\hat{\theta}_{N}^{\text{do}}:=\inf_{\theta\in\Theta}\left\{\sup_{Q:W_{\varepsilon,p}(Q,\mathbb{P}_{N})\leq\delta}\ \ \mathbb{E}_{z\sim Q}[\ell(z,\theta)]\right\}\]

Where \(\hat{c}\) is the \(\hat{d}\) corresponding cost on \(\mathcal{Z}\), leading to the estimation of the true parameters of the DRO problem. To state our result, we need the Dudley entropy integral [61], which measures the complexity of the loss function class.

**Theorem 5** (Learning Finite Sample Guarantee): _With assumption 1 and 2, then for \(\hat{\theta}_{N}^{\text{do}}\) we have:_

\[\mathcal{R}_{\delta}(\mathbb{P}_{*},\hat{\theta}_{N}^{\text{do}})-\inf_{ \theta\in\Theta}\mathcal{R}_{\delta}(\mathbb{P}_{*},\theta)\leq N^{-1/2}\left[ c_{0}+c_{1}\delta^{1-p}+c_{2}\delta^{1-p}N^{-\eta+1/2}+c_{3}\sqrt{\log(2/ \epsilon)}\right],\]

_With probability at least \(1-2\epsilon\). With \(\mathfrak{C}(\mathcal{L})\) denoting the Dudley entropy integral for the function class \(\{\ell(\cdot,\theta):\theta\in\Theta\}\), the constants \(c_{0}\), \(c_{1}\) and \(c_{2}\) are identified as follows:_

\[c_{0}\coloneqq 96\mathfrak{C}(\mathcal{L}),\ c_{1}\coloneqq 96L\cdot \operatorname{diam}\left(\mathcal{V}\right)^{p},\ c_{2}\coloneqq 2 pL\cdot\operatorname{diam}\left(\mathcal{V}\right)^{p-1}\cdot M_{d},\ \text{ and }\ c_{3}\coloneqq 2\sqrt{2}\times M.\]

The final theorem completes our framework, enabling us to perform DRO on real-world datasets without knowing the SCM structures while providing performance bounds.

### Related Works

**Causally Fair Dissimilarity Function.** Various studies have addressed the specification and learning of individual fair metrics, such as [33, 68, 70, 47], but their construction based on causal structure and sensitive attributes remains unclear. Our work adopts and extends the concept of a causal fair metric, as discussed in the works [23, 24].

**DRO and Individual Fairness.** Previous works, such as [68, 70, 47], address the DRO problem with an individual fairness metric but are limited to linear SCMs and \(p=2\). These studies do not discuss the duality theorem or regularizers. Additionally, [42] studied DRO, but its connection to causality remains unclear.

**Strong Duality Theorem.** Various versions of the strong duality theorem have been explored in prior works. For instance, in [59, 46, 9, 14, 27, 28, 66], the cost function must be a metric or [74] has convex property. Additionally, in [72, 11, 58], the distance function \(d\) must be positive-definite, meaning \(d(v,v^{\prime})=0\) if and only if \(v=v^{\prime}\). However, these conditions are not met for CFDF, necessitating a new formulation of the duality theorem 1.

**DRO as Regularizer.** Previous works on using DRO as a regularizer, explicitly solved [60, 14, 16, 29] or through k-order estimation [5, 9, 6, 66, 27], only consider cases where the cost function is derived from a metric or a positive-definite dissimilarity function. Therefore, their theorems do not apply directly to our CFDF. We present new results in Theorems 2, 3, and 4 tailored for our cases.

**Finite Sample Guarantee.** Various works provide bounds on the performance of DRO solutions with finite samples [41, 25, 10, 12], but these do not apply to our CFDF due to previously mentioned reasons. The studies [68, 70, 47] offer performance bounds only for the case of linear SCMs with \(p=2\). Therefore, we present a general case in Theorem 5.

**Optimal Transport and Causality.** Recent works [39, 35, 13, 32, 21, 1, 2] on causal optimal transport focus on the causal structure of the transport map or plan, which differs from our problem. In our case, causality pertains to the transportation cost derived from SCMs.

[MISSING_PAGE_FAIL:10]

## References

* [1] Beatrice Acciaio, Julio Backhoff-Veraguas, and Anastasiia Zalashko. Causal optimal transport and its links to enlargement of filtrations and continuous-time stochastic optimization. _Stochastic Processes and their Applications_, 130(5):2918-2953, 2020.
* [2] Julio Backhoff, Mathias Beiglbock, Yiqing Lin, and Anastasiia Zalashko. Causal transport in discrete time and applications. _SIAM Journal on Optimization_, 27(4):2528-2562, 2017.
* [3] Ryan S Baker and Aaron Hawn. Algorithmic bias in education. _International Journal of Artificial Intelligence in Education_, pages 1-41, 2022.
* [4] Solon Barocas, Moritz Hardt, and Arvind Narayanan. _Fairness and machine learning: Limitations and opportunities_. MIT Press, 2023.
* [5] Daniel Bartl, Mathias Beiglbock, and Gudmund Pammer. The wasserstein space of stochastic processes. _arXiv preprint arXiv:2104.14245_, 2021.
* [6] Robert Bartlett, Adair Morse, Richard Stanton, and Nancy Wallace. Consumer-lending discrimination in the fintech era. _Journal of Financial Economics_, 143(1):30-56, 2022.
* [7] Dimitris Bertsimas, David B Brown, and Constantine Caramanis. Theory and applications of robust optimization. _SIAM review_, 53(3):464-501, 2011.
* [8] Reuben Binns. On the apparent conflict between individual and group fairness. In _Proceedings of the 2020 conference on fairness, accountability, and transparency_, pages 514-524, 2020.
* [9] Jose Blanchet, Yang Kang, and Karthyek Murthy. Robust wasserstein profile inference and applications to machine learning. _Journal of Applied Probability_, 56(3):830-857, 2019.
* [10] Jose Blanchet, Jiajin Li, Sirui Lin, and Xuhui Zhang. Distributionally robust optimization and robust statistics. _arXiv preprint arXiv:2401.14655_, 2024.
* [11] Jose Blanchet and Karthyek Murthy. Quantifying distributional model risk via optimal transport. _Mathematics of Operations Research_, 44(2):565-600, 2019.
* [12] Jose Blanchet, Karthyek Murthy, and Nian Si. Confidence regions in wasserstein distributionally robust estimation. _Biometrika_, 109(2):295-315, 2022.
* [13] Patrick Cheridito and Stephan Eckstein. Optimal transport and wasserstein distances for causal models. _arXiv preprint arXiv:2303.14085_, 2023.
* [14] Hong Chu, Meixia Lin, and Kim-Chuan Toh. Wasserstein distributionally robust optimization and its tractable regularization formulations. _arXiv preprint arXiv:2402.03942_, 2024.
* [15] Hong TM Chu, Kim-Chuan Toh, and Yangjing Zhang. On regularized square-root regression problems: distributionally robust interpretation and fast computations. _The Journal of Machine Learning Research_, 23(1):13885-13923, 2022.
* [16] Hong TM Chu, Kim-Chuan Toh, and Yangjing Zhang. On regularized square-root regression problems: distributionally robust interpretation and fast computations. _Journal of Machine Learning Research_, 23(308):1-39, 2022.
* [17] Zac Cranko, Zhan Shi, Xinhua Zhang, Richard Nock, and Simon Kornblith. Generalised Lipschitz regularisation equals distributional robustness. In _International Conference on Machine Learning_, pages 2178-2188. PMLR, 2021.
* [18] Jeffrey Dastin. Amazon scraps secret ai recruiting tool that showed bias against women. In _Ethics of data and analytics_, pages 296-299. Auerbach Publications, 2022.
* [19] Ricardo Dominguez-Olmedo, Amir H Karimi, and Bernhard Scholkopf. On the adversarial robustness of causal algorithmic recourse. In _International Conference on Machine Learning_, pages 5324-5342. PMLR, 2022.

* [20] Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard Zemel. Fairness through awareness. In _Proceedings of the 3rd innovations in theoretical computer science conference_, pages 214-226, 2012.
* [21] Stephan Eckstein and Gudmund Pammer. Computational methods for adapted optimal transport. _The Annals of Applied Probability_, 34(1A):675-713, 2024.
* [22] Ahmad-Reza Ehyaei, Golnoosh Farnadi, and Samira Samadi. Causal fair metric: Bridging causality, individual fairness, and adversarial robustness. _arXiv preprint arXiv:2310.19391_, 2023.
* [23] Ahmad-Reza Ehyaei, Amir-Hossein Karimi, Bernhard Scholkopf, and Setareh Maghsudi. Robustness implies fairness in causal algorithmic recourse. In _Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency_, pages 984-1001, 2023.
* [24] Ahmad-Reza Ehyaei, Kierash Mohammadi, Amir-Hossein Karimi, Samira Samadi, and Golnoosh Farnadi. Causal adversarial perturbations for individual fairness and robustness in heterogeneous data spaces. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 38, 10, pages 11847-11855, 2024.
* [25] Rui Gao. Finite-sample guarantees for wasserstein distributionally robust optimization: Breaking the curse of dimensionality. _Operations Research_, 71(6):2291-2306, 2023.
* [26] Rui Gao, Xi Chen, and Anton J Kleywegt. Wasserstein distributionally robust optimization and variation regularization. _arXiv preprint arXiv:1712.06050_, 2017.
* [27] Rui Gao, Xi Chen, and Anton J Kleywegt. Wasserstein distributionally robust optimization and variation regularization. _Operations Research_, 2022.
* [28] Rui Gao and Anton Kleywegt. Distributionally robust stochastic optimization with wasserstein distance. _Mathematics of Operations Research_, 48(2):603-655, 2023.
* [29] Camilo Andres Garcia Trillos and Nicolas Garcia Trillos. On the regularized risk of distributionally robust learning over deep neural networks. _Research in the Mathematical Sciences_, 9(3):54, 2022.
* [30] Milena A Gianfrancesco, Suzanne Tamang, Jinoos Yazdany, and Gabriela Schmajuk. Potential biases in machine learning algorithms using electronic health record data. _JAMA internal medicine_, 178(11):1544-1547, 2018.
* [31] Melissa Hall, Laurens van der Maaten, Laura Gustafson, Maxwell Jones, and Aaron Adcock. A systematic study of bias amplification. _arXiv preprint arXiv:2201.11706_, 2022.
* [32] Bingyan Han. Distributionally robust risk evaluation with a causality constraint and structural information. _arXiv preprint arXiv:2203.10571_, 2022.
* [33] Christina Ilvento. Metric learning for individual fairness. In _1st Symposium on Foundations of Responsible Computing (FORC 2020)_. Schloss-Dagstuhl-Leibniz Zentrum fur Informatik, 2020.
* [34] Alexander Immer, Christoph Schultheiss, Julia E Vogt, Bernhard Scholkopf, Peter Buhlmann, and Alexander Marx. On the identifiability and estimation of causal location-scale noise models. _arXiv preprint arXiv:2210.09054_, 2022.
* [35] Yifan Jiang. Duality of causal distributionally robust optimization: the discrete-time case. _arXiv preprint arXiv:2401.16556_, 2024.
* [36] Olav Kallenberg and Olav Kallenberg. _Foundations of modern probability_, volume 2. Springer, 1997.
* [37] Amir-Hossein Karimi, Bernhard Scholkopf, and Isabel Valera. Algorithmic recourse: from counterfactual explanations to interventions. In _Proceedings of the 2021 ACM conference on fairness, accountability, and transparency_, pages 353-362, 2021.

* [38] Ronny Kohavi and Barry Becker. Uci adult data set. _UCI Machine Learning Repository_, 5, 1996.
* [39] Daniel Krsek and Gudmund Pammer. General duality and dual attainment for adapted transport. _arXiv preprint arXiv:2401.11958_, 2024.
* [40] Matt J Kusner, Joshua Loftus, Chris Russell, and Ricardo Silva. Counterfactual fairness. In _Advances in Neural Information Processing Systems_, pages 4069-4079, 2017.
* [41] Jaeho Lee and Maxim Raginsky. Minimax statistical learning with wasserstein distances. _Advances in Neural Information Processing Systems_, 31, 2018.
* [42] Peizhao Li, Ethan Xia, and Hongfu Liu. Learning antidote data to individual unfairness. In _International Conference on Machine Learning_, pages 20168-20181. PMLR, 2023.
* [43] Fengming Lin, Xiaolei Fang, and Zheming Gao. Distributionally robust optimization: A review on theory and applications. _Numerical Algebra, Control and Optimization_, 12(1):159-212, 2022.
* [44] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. _arXiv preprint arXiv:1706.06083_, 2017.
* [45] Ninareh Mehrabi, Fred Morstatter, Nripsuta Saxena, Kristina Lerman, and Aram Galstyan. A survey on bias and fairness in machine learning. _ACM computing surveys (CSUR)_, 54(6):1-35, 2021.
* [46] Peyman Mohajerin Esfahani and Daniel Kuhn. Data-driven distributionally robust optimization using the wasserstein metric: Performance guarantees and tractable reformulations. _Mathematical Programming_, 171(1):115-166, 2018.
* [47] Debaghya Mukherjee, Mikhail Yurochkin, Moulinath Banerjee, and Yuekai Sun. Two simple ways to learn individual fairness metrics from data. In _International Conference on Machine Learning_, pages 7097-7107. PMLR, 2020.
* [48] James R Munkres. Topology, 2nd edn of [mr0464128], 2000.
* [49] Razieh Nabi and Ilya Shpitser. Fair inference on outcomes. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 32, 1, 2018.
* [50] Arash Nasr-Esfahany, Mohammad Alizadeh, and Devavrat Shah. Counterfactual identifiability of bijective causal models. In _International Conference on Machine Learning_, pages 25733-25754. PMLR, 2023.
* [51] Judea Pearl. _Causality: Models, Reasoning, and Inference_. Cambridge University Press, 2009.
* [52] Dana Pessach and Erez Shmueli. A review on fairness in machine learning. _ACM Computing Surveys (CSUR)_, 55(3):1-44, 2022.
* [53] Jonas Peters, Dominik Janzing, and Bernhard Scholkopf. _Elements of causal inference: foundations and learning algorithms_. The MIT Press, 2017.
* [54] Gabriel Peyre, Marco Cuturi, et al. Computational optimal transport. _Center for Research in Economics and Statistics Working Papers_, 2017-86, 2017.
* [55] Hamed Rahimian and Sanjay Mehrotra. Frameworks and results in distributionally robust optimization. _Open Journal of Mathematical Optimization_, 3:1-85, 2022.
* [56] Alexis Ross, Himabindu Lakkaraju, and Osbert Bastani. Learning models for actionable recourse. _Advances in Neural Information Processing Systems_, 34:18734-18746, 2021.
* [57] Anian Ruoss, Mislav Balunovic, Marc Fischer, and Martin Vechev. Learning certified individually fair representations. In _Advances in Neural Information Processing Systems_, 2020.

* [58] Soroosh Shafieezadeh-Abadeh, Liviu Aolaritei, Florian Dorfler, and Daniel Kuhn. New perspectives on regularization and computation in optimal transport-based distributionally robust optimization. _arXiv preprint arXiv:2303.03900_, 2023.
* [59] Soroosh Shafieezadeh-Abadeh, Daniel Kuhn, and Peyman Mohajerin Esfahani. Regularization via mass transportation. _Journal of Machine Learning Research_, 20(103):1-68, 2019.
* [60] Soroosh Shafieezadeh Abadeh, Peyman M Mohajerin Esfahani, and Daniel Kuhn. Distributionally robust logistic regression. _Advances in Neural Information Processing Systems_, 28, 2015.
* [61] Michel Talagrand. _Upper and lower bounds for stochastic processes_, volume 60. Springer, 2014.
* [62] Peipei Tang, Chengjing Wang, Defeng Sun, and Kim-Chuan Toh. A sparse semismooth newton based proximal majorization-minimization algorithm for nonconvex square-root-loss regression problem. _The Journal of Machine Learning Research_, 21(1):9253-9290, 2020.
* [63] Cedric Villani et al. _Optimal transport: old and new_, volume 338. Springer, 2009.
* [64] Julius Von Kugelgen, Amir-Hossein Karimi, Umang Bhatt, Isabel Valera, Adrian Weller, and Bernhard Scholkopf. On the fairness of causal algorithmic recourse. In _Proceedings of the AAAI conference on artificial intelligence_, volume 36, 9, pages 9584-9594, 2022.
* [65] Anne L Washington. How to argue with an algorithm: Lessons from the compas-propublica debate. _Colo. Tech. LJ_, 17:131, 2018.
* [66] Qinyu Wu, Jonathan Yu-Meng Li, and Tiantian Mao. On generalization and regularization via wasserstein distributionally robust optimization. _arXiv preprint arXiv:2212.05716_, 2022.
* [67] Samuel Yeom and Matt Fredrikson. Individual fairness revisited: transferring techniques from adversarial robustness. In _Proceedings of the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence_, 2021.
* [68] Mikhail Yurochkin, Amanda Bower, and Yuekai Sun. Training individually fair ml models with sensitive subspace robustness. _arXiv preprint arXiv:1907.00020_, 2019.
* [69] Mikhail Yurochkin, Amanda Bower, and Yuekai Sun. Training individually fair ml models with sensitive subspace robustness. In _International Conference on Learning Representations_, 2020.
* [70] Mikhail Yurochkin and Yuekai Sun. Sensei: Sensitive set invariance for enforcing individual fairness. In _International Conference on Learning Representations_, 2021.
* [71] K Zhang and A Hyvarinen. On the identifiability of the post-nonlinear causal model. In _25th Conference on Uncertainty in Artificial Intelligence (UAI 2009)_, pages 647-655. AUAI Press, 2009.
* [72] Luhao Zhang, Jincheng Yang, and Rui Gao. A simple and general duality proof for wasserstein distributionally robust optimization. _arXiv preprint arXiv:2205.00362_, 2022.
* [73] Yangjing Zhang, Ning Zhang, Defeng Sun, and Kim-Chuan Toh. An efficient hessian based algorithm for solving large-scale sparse group lasso problems. _Mathematical Programming_, 179:223-263, 2020.
* [74] Jianzhe Zhen, Daniel Kuhn, and Wolfram Wiesemann. A unified theory of robust and distributionally robust optimization via the primal-worst-equals-dual-best principle. _Operations Research_, 2023.

Supplementary Theoretical Details

Notations.In this work random variables are denoted by bold letters (e.g., \(\mathbf{V}\)), their corresponding probability spaces by calligraphic letters (e.g., \(\mathcal{V}\)), and instances by normal letters (e.g., \(v\)). The space of probability measures on \(\mathcal{V}\) is represented by \(\mathcal{P}(\mathcal{V})\) and probability measures by blackboard bold letters (e.g., \(\mathbb{P}\)).

Non-Sensitive Part.Let \(F:\mathcal{U}\to\mathcal{V}\) be the reduced-form map of \(\mathcal{M}\). The vector \(v\) decomposes into sensitive and non-sensitive parts, \(v=(a,x)\), and we have a corresponding decomposition in the exogenous space denoted by \(u=(u_{a},u_{x})\) and \(\mathcal{U}_{\mathcal{A}},\mathcal{U}_{\mathcal{X}}\) are corresponding spaces. Using the ANM model, we can assume that both \(\mathcal{V}\) and \(\mathcal{U}\) are equivalent, and therefore, the non-sensitive feature space is the same as the non-sensitive part of the exogenous space. If \(\mathbb{P}\) is a probability measure in \(\mathcal{P}(\mathcal{V})\), then \((\mathbb{P})_{\mathcal{X}}\) refers to the marginal probability over the non-sensitive part. We also refer to \(\mathbb{Q})_{\mathcal{X}}\)_ for marginal probability over the non-sensitive part of the exogenous space.

**Definition 3** (Push-forward Measure): _Let \(\mathbb{P}\in\mathcal{P}(\mathcal{V})\), \(\mathbb{Q}\in\mathcal{P}(\mathcal{U})\) be two probability measures and \(T:\mathcal{V}\to\mathcal{U}\) is map, the measure \(\mathbb{Q}\) is called the push-forward of \(\mathbb{P}\) through \(T\) is denoted by \(T_{\#}\mathbb{P}\) if:_

\[\mathbb{Q}(B)=\mathbb{P}(T^{-1}(B)),\quad\forall B\subset\mathcal{U}\]

**Definition 4** (Set of Couplings): _The set \(\Gamma(\mathbb{P},\mathbb{Q})\) represents the couplings of probability distributions \(\mathbb{P}\in\mathcal{P}(\mathcal{V})\), \(\mathbb{Q}\in\mathcal{P}(\mathcal{U})\), comprising distributions over \(\mathcal{V}\times\mathcal{U}\) with margins \(\mathbb{P}\) and \(\mathbb{Q}\). A measure \(\pi\) belongs to \(\Gamma(\mathbb{P},\mathbb{Q})\) if and only if_

\[\pi(A\times\mathcal{U})=\mathbb{P}(A)\quad\text{and}\quad\pi(\mathcal{V}\times B )=\mathbb{Q}(B)\quad\forall A\subset\mathcal{V},B\subset\mathcal{U}\]

_By extension, a random pair \((X,Y)\sim\pi\), where \(\pi\in\Gamma(\mathbb{P},\mathbb{Q})\), will also be called a coupling of \(\mathbb{P}\) and \(\mathbb{Q}\)._

**Definition 5** (Diameter of a Set): _Let \(A\) be a set in a metric space with a distance function \(d\). The diameter of \(A\), denoted \(\text{diam}(A)\), is defined as:_

\[\text{diam}(A)=\sup\{d(x,y):x,y\in A\}\]

_where \(\sup\) represents the supremum of the set of distances \(d(x,y)\) for all pairs \((x,y)\) in \(A\)._

**Definition 6** (Homogeneous dissimilarity function): _Let \(\Lambda\) be an extended-valued function \(\Lambda\colon\mathcal{X}\to[0,\infty]\) on a real vector space \(\mathcal{X}\) with absolutely homogeneous assumption i.e. \(\Lambda(tx)=|t|\,\Lambda(x)\) for any \(t\in\mathbb{R}\) and \(z\in\mathcal{X}\). In addition, \(\Lambda\) is proper it means there exists \(x_{0}\in\mathcal{X}\) such that \(\Lambda(x_{0})=1\). The cost function \(d:\mathcal{X}\times\mathcal{X}\to[0,\infty]\) is called Homogeneous dissimilarity function if is defined as \(d(x^{\prime},x)\coloneqq\Lambda(x^{\prime}-x)\) for any \(x^{\prime},x\in\mathcal{X}\)._

**Lemma 1**: _If \(\mathcal{M}\) is an additive noise model with mutually independent exogenous variables, then the parent-free sensitive \(\mathcal{M}_{0}\) attribute model retains both of these properties. Moreover, the map-reduced form mapping of \(\mathcal{M}_{0}\) is equivalent to \(g^{-1}\), where \(g\) represents the mapping to the semi-latent space._

Proof.First, \(M_{0}\) is an additive noise model because its structure is derived from the initial equations of \(\mathcal{M}\) by removing those equations related to the sensitive attributes and replacing the exogenous variable \(\mathbf{U}_{i}\) by \(\mathbf{V}_{i}\).

Regarding the mutual independence of the exogenous variables, in the original model \(\mathcal{M}\), the variables \(V_{i}\) for \(i\in\mathcal{I}\) are not independent of \(\mathbf{U}_{j}\) for \(j\notin\mathcal{I}\) if they have parents. However, assuming a hard intervention for each instance of \(V_{i}\) -- where a do-action is executed for this intervention -- it implies that the intervened variable \(V_{i}\) can be considered independent from the other variables. Therefore, since we apply hard interventions to all sensitive variables, we can assume that \(V_{i}\) for \(i\in\mathcal{I}\) are mutually independent, and also that \(V_{i}\) are independent of \(\mathbf{U}_{j}\) for all \(j\notin\mathcal{I}\).

Finally, by referencing equations 6, it is observable that the map-reduced form mapping of \(\mathcal{M}_{0}\) is equivalent to the inverse of the map to the semi-latent space.

**Lemma 2**: _Let \(\mathcal{M}\) be an additive noise model with a mapping \(g\) to semi-latent space \(\mathcal{U}_{0}\). Assume \(\mathcal{M}\) includes the sensitive attributes \(\mathbf{A}\) and other non-sensitive attributes \(\mathbf{X}\) that belong to the vector space \(\mathcal{X}\). Consider \(v=(a,x)\) as an instance in \(\mathcal{M}\), and let \(\Delta\in\mathcal{X}\) represent a shift intervention value. Then, the counterfactual corresponding to additive shit is obtained by:_

\[P_{\mathcal{X}}(\textbf{CF}(v,\Delta))=P_{\mathcal{X}}(g^{-1}(g(v)+(0,\Delta))).\]

_Moreover, if \(a^{\prime}\in\mathcal{A}\) represents another level of sensitive attributes, then the hard intervention concerning \(\textbf{A}:=a^{\prime}\) is achieved by:_

\[\ddot{v}_{a^{\prime}}=\textbf{CF}(v,do(\textbf{A:=}a^{\prime}))=g^{-1}((a^{ \prime},P_{\mathcal{X}}(g(v)))).\]

**Proof.** In additive noise models, an additive intervention can be conceptualized as adding a value \(\delta\) to the exogenous variables, while all structural equations remain unchanged. Consequently, during such an intervention, the reduced-form mapping \(F_{\mathcal{M}^{\Delta}}\) of the intervened SCM remains unchanged. Therefore by definition of intervention, it follows that:

\[\textbf{CF}(v,do(\textbf{X+=}\Delta))=F_{\mathcal{M}^{\Delta}}(F^{-1}(v)+(0, \Delta))=F(F^{-1}(v)+(0,\Delta)).\]

Since \(F\) and \(g^{-1}\) are coincide in non-sensitive coordinates then \(P_{\mathcal{X}}(\psi(F^{-1}(v)+(0,\Delta)))=P_{\mathcal{X}}(g^{-1}(g(v)+(0, \Delta)))\) and it completes the first part. In this case, we denote \(F\) as \(M\), which is an invertible matrix. Consequently, the counterfactual \(\textbf{CF}(v,do(\textbf{X+=}\Delta))\) can be expressed as \(v+M^{-1}(0,\Delta)\).

To prove the second part, when intervention is performed on sensitive attributes, \(\mathcal{M}\) transforms into a parent-free sensitive attribute model where the sensitive attribute \(a\) is replaced by \(a^{\prime}\). Given that the map-reduced form of the parent-free sensitive attribute model aligns with \(g^{-1}\), and since \(g\) and \(F^{-1}\) coincide on the non-sensitive parts, the counterfactual can be expressed as follows:

\[\ddot{v}_{a^{\prime}}=\textbf{CF}(v,do(\textbf{A:=}a^{\prime}))=g^{-1}((a^{ \prime},P_{\mathcal{X}}(g(v)))).\]

## Appendix B Proof Section

**Proof of Proposition 1.**

(i) If \(d\) adheres to Eq. 7, it means that for each \(v\in\mathcal{V}\), the mapping \(g(v)=(a,x)\). For its counterfactual \(\ddot{v}_{a}\), we have \(g(\ddot{v}_{a^{\prime}})=(a^{\prime},x)\). Using Eq. 7, we can express:

\[d(v,\ddot{v}_{a})=d_{\mathcal{X}}(P_{\mathcal{X}}(g(v)),P_{\mathcal{X}}(g( \ddot{v}_{a})))=d_{\mathcal{X}}(x,x)=0\]

This demonstrates that \(d\) retains the first property of Def. 1.

Additionally, since \(d_{\mathcal{X}}\) is continuous, for each \(x\) and any \(\epsilon>0\), there exists a \(\delta>0\) such that if \(\left\|\Delta\right\|<\delta\), then \(d_{\mathcal{X}}(x,x+\Delta)<\epsilon\). Referencing Lemma 2 and the formulation of \(d\), it follows that for \(\left\|\Delta\right\|<\delta\) and for each \(a\in\mathcal{A}\):

\[d(v,\textbf{CF}(v,\Delta))=d(P_{\mathcal{X}}(g(v)),P_{\mathcal{X}}(g(\textbf{ CF}(v,\Delta))))=d_{\mathcal{X}}(x,x+\Delta)<\epsilon\]

Thus, it satisfies property (ii) of the CFDF.

(ii) Let's consider a CFDF denoted as \(d:\mathcal{V}\times\mathcal{V}\rightarrow\mathbb{R}\), with an embedding \(\textbf{g}:\mathcal{V}\rightarrow\mathcal{Q}\) that maps from the feature space to a semi-latent space. We define \(d^{*}\) as the pull-back of \(d\) onto \(\mathcal{Q}\), \(d^{*}(q_{1},q_{2})=d(g^{-1}(q_{1}),g^{-1}(q_{2}))\) where \(d^{*}\) is a dissimilarity function, and we aim to clarify which properties it inherits from Def. 1. We utilize a decomposition of \(\mathcal{Q}\) into \(\mathcal{A}\times\mathcal{X}\), where \(q=g^{-1}(v)\) and \(v\in\mathcal{V}\), denoting \(q\) as \((a,x)\). Property (i) of the CFDF ensures:

\[d(v,\ddot{v}_{a^{\prime}})=d^{*}((a,x),(a^{\prime},x))=0\quad\forall a^{\prime }\in\mathcal{A}\]

This property confirms that \(d^{*}\) is insensitive to changes in the sensitive part \(\mathcal{A}\). To demonstrate, consider any two points \(q_{1}=(a_{1},x_{1})\) and \(q_{2}=(a_{2},x_{2})\), with an arbitrary \(a_{0}\in\mathcal{A}\). By triangle property of dissimilarity function \(d\) it can be seen:

\[d^{*}((a_{1},x_{1}),(a_{2},x_{2}))\leq d^{*}((a_{1},x_{1}),(a_{0},x_{1}))+d^{*} ((a_{0},x_{1}),(a_{2},x_{2}))\Longrightarrow\]

\[d^{*}((a_{1},x_{1}),(a_{2},x_{2}))\leq d^{*}((a_{0},x_{1}),(a_{2},x_{2}))\]

Here, \(d^{*}((s_{1},x_{1}),(s_{0},x_{1}))\) is zero due to property (i). Similarly, we can argue:

\[d^{*}((a_{0},x_{1}),(a_{2},x_{2}))\leq d^{*}((a_{0},x_{1}),(a_{1},x_{1}))+d^{* }((a_{1},x_{1}),(a_{2},x_{2}))\Longrightarrow\]

\[d^{*}((a_{0},x_{1}),(a_{2},x_{2}))\leq d^{*}((a_{1},x_{1}),(a_{2},x_{2}))\]This results in \(d^{*}((a_{1},x_{1}),(a_{2},x_{2}))=d^{*}((a_{0},x_{1}),(a_{2},x_{2}))\). With similar reasoning, we have:

\[d^{*}((a_{1},x_{1}),(a_{2},x_{2}))=d^{*}((a_{1},x_{1}),(a_{0},x_{2}))\Longrightarrow d ^{*}((a_{1},x_{1}),(a_{2},x_{2}))=d^{*}((a_{0},x_{1}),(a_{0},x_{2}))\]

Hence, \(d^{*}\) is invariant to the sensitive subspace. If \(d_{\mathcal{X}}\) is the dissimilarity function induced by \(d^{*}\) on \(\mathcal{X}\), then \(d^{*}((a_{1},x_{1}),(a_{2},x_{2}))=d_{\mathcal{X}}(x_{1},x_{2})\). In accordance with Lemma 2, the second property of Def. 1 states that for each \(\epsilon>0\), there exists a \(\delta\) such that if \(|\Delta|<\delta\), then \(d_{\mathcal{X}}(x,x+\Delta)<\epsilon\). This property demonstrates the continuity of \(d_{\mathcal{X}}\) along the diagonal.

Finally, \(d\) can be embedded in semi-latent space and described by another dissimilarity function on it that only depends on the non-sensitive part of exogenous space:

\[d(v,w)=d_{\mathcal{X}}(P_{\mathcal{X}}(g(v)),P_{\mathcal{X}}(g(w)))\]

This concludes the proof.

**Lemma 3** (Transformation by a Bijective Map): _Let \(g:\mathcal{V}\to\mathcal{U}\) be an invertible function and let the transportation cost function \(c\) be constructed by \(c(v,v^{\prime})=d(g(v),g(v^{\prime}))\) where \(d\) is a metric on the space \(\mathcal{U}\). For every \(\mathbb{P},\mathbb{Q}\in\mathcal{P}(\mathcal{V})\), the following equation holds:_

\[W_{c,p}(\mathbb{P},\mathbb{Q})=W_{d,p}(g_{\#}\mathbb{P},g_{\#}\mathbb{Q})\]

_where \(W_{c}\) and \(W_{d}\) represent the Wasserstein distances with respect to the metrics \(c\) and \(d\), respectively._

**Proof.** By the definition of the Wasserstein distance,

\[W_{c,p}(\mathbb{P},\mathbb{Q})=\inf_{\pi\in\Gamma(\mathbb{P},\mathbb{Q})}\int _{\mathcal{V}\times\mathcal{V}}c^{p}(v,v^{\prime})\,d\pi(v,v^{\prime}).\]

Substituting \(c(v,v^{\prime})=d(g(v),g(v^{\prime}))\) and \(u=g(v)\) gives:

\[W_{c,p}(\mathbb{P},\mathbb{Q})=\inf_{\pi\in\Gamma(\mathbb{P},\mathbb{Q})}\int _{\mathcal{V}\times\mathcal{V}}d^{p}(g(v),g(v^{\prime}))\,d\pi(v,v^{\prime}).\]

Consider a coupling \(\pi\) of \(\mathbb{P}\) and \(\mathbb{Q}\). Define a measure \(\tilde{\pi}\) on \(\mathcal{U}\times\mathcal{U}\) by \(\tilde{\pi}(A\times B)=\pi(g^{-1}(A)\times g^{-1}(B))\). \(\tilde{\pi}\) is a coupling of \(g_{\#}\mathbb{P}\) and \(g_{\#}\mathbb{Q}\) because:

\[\tilde{\pi}(A\times\mathcal{U})=\pi(g^{-1}(A)\times\mathcal{V})=g_{\#} \mathbb{P}(A);\quad\tilde{\pi}(\mathcal{U}\times B)=\pi(\mathcal{V}\times g^{ -1}(B))=g_{\#}\mathbb{Q}(B).\]

Therefore, the \(p\)-Wasserstein distance for the push-forward measures is

\[\inf_{\pi\in\Gamma(\mathbb{P},\mathbb{Q})}\int_{\mathcal{V}\times \mathcal{V}}d^{p}(g(v),g(v^{\prime}))\,d\pi(v,v^{\prime}) =\inf_{\tilde{\pi}\in\Gamma(g_{\#}\mathbb{P},g_{\#}\mathbb{Q})} \int_{\mathcal{U}\times\mathcal{U}}d^{p}(u,u^{\prime})\,d\tilde{\pi}(u,u^{ \prime})\] \[=W_{d,p}(g_{\#}\mathbb{P},g_{\#}\mathbb{Q}).\]

Since \(\tilde{\pi}\) arises from \(\pi\) via \(g\), and \(g\) is invertible and measure-preserving in this context, the values in the integrals of the definitions of \(W_{c,p}(\mathbb{P},\mathbb{Q})\) and \(W_{d,p}(g_{\#}\mathbb{P},g_{\#}\mathbb{Q})\) match. Thus, we have shown that \(W_{c,p}(\mathbb{P},\mathbb{Q})=W_{d,p}(g_{\#}\mathbb{P},g_{\#}\mathbb{Q})\).

**Lemma 4** (Optimal Transportation Cost on Subspace): _Let \(\mathcal{U}\subseteq\mathbb{R}^{n}\) and suppose \(\mathcal{U}\) is decomposed into two subspaces, \(\mathcal{U}=(\mathcal{A},\mathcal{X})\), where \(\mathcal{A}\) corresponds to the subset of some coordinates and \(\mathcal{X}\) to its complements. Let \(P_{\mathcal{X}}\) denote the projection function onto the \(\mathcal{X}\) space, i.e., \(P_{\mathcal{X}}(u)\) projects \(u\in\mathcal{U}\) onto \(\mathcal{X}\) components. Define a cost function \(c(u,u^{\prime})=d(P_{\mathcal{X}}(u),P_{\mathcal{X}}(u^{\prime}))\), where \(d\) is a cost function on the space \(\mathcal{X}\). Consider probability measures \(\mathbb{P},\mathbb{Q}\in\mathcal{P}(\mathcal{U})\), and define \(\mathbb{P}_{\mathcal{X}}=P_{\mathcal{X}\#}\mathbb{P}\) and \(\mathbb{Q}_{\mathcal{X}}=P_{\mathcal{X}\#}\mathbb{Q}\) as the pushforward measures of \(\mathbb{P}\) and \(\mathbb{Q}\) under the projection \(P_{\mathcal{X}}\), respectively, placing them in \(\mathcal{P}(\mathcal{X})\). Let \(\pi_{\mathcal{X}}^{*}\) be the optimal transport plan concerning the Wasserstein distance \(W_{d}(\mathbb{P}_{\mathcal{X}},\mathbb{Q}_{\mathcal{X}})\). Then, any transport plan \(\pi\in\mathcal{P}(\mathcal{U}\times\mathcal{U})\), whose marginal distribution over \(\mathcal{X}\times\mathcal{X}\) equals \(\pi_{\mathcal{X}}^{*}\), should also be an optimal solution for the Wasserstein distance \(W_{c}(\mathbb{P},\mathbb{Q})\) concerning the cost function \(c\)._

**Proof.** Given any coupling \(\pi\in\Gamma(\mathbb{P},\mathbb{Q})\), we consider elements \(u=(a,x)\) and \(u^{\prime}=(a^{\prime},x^{\prime})\) in \(\mathcal{U}=\mathcal{A}\times\mathcal{X}\). The cost function \(c\) is defined by \(c((a,x),(a^{\prime},x^{\prime}))=d(x,x^{\prime})\), where \(d\) is a metric on the space \(\mathcal{X}\). By definition of optimal transport cost \(W_{c}(\mathbb{P},\mathbb{Q})\) we have:

\[\sup_{\pi\in\Gamma(\mathbb{P},\mathbb{Q})}\left\{\int_{\mathcal{U} \times\mathcal{U}}c((a,x),(a^{\prime},x^{\prime}))\,d\pi\right\}=\sup_{\pi\in \Gamma(\mathbb{P},\mathbb{Q})}\left\{\int_{\mathcal{U}\times\mathcal{U}}d(x,x^ {\prime})\,d\pi\right\}=\] \[\sup_{\pi\in\Gamma(\mathbb{P},\mathbb{Q})}\left\{\int_{\mathcal{X }\times\mathcal{X}}\left(\int_{\mathcal{A}\times\mathcal{A}}d(x,x^{\prime}) \,d\pi((a,a^{\prime})|\mathbf{X}=x,\mathbf{X}^{\prime}=x^{\prime})\right)d(\pi )_{\mathcal{X}\times\mathcal{X}}\right\}=\] \[\sup_{\pi\in\Gamma(\mathbb{P},\mathbb{Q})}\left\{\int_{\mathcal{ X}\times\mathcal{X}}d(x,x^{\prime})d(\pi)_{\mathcal{X}\times\mathcal{X}}\right\}= \sup_{\pi\in\Gamma(\mathbb{P})_{\mathcal{X}},(\mathbb{Q})_{\mathcal{X}}} \left\{\int_{\mathcal{X}\times\mathcal{X}}d(x,x^{\prime})\pi\right\}\]

where \((\mathbb{P})_{\mathcal{X}}\) and \((\pi)_{\mathcal{X}\times\mathcal{X}}\) is marginal distribution over \(\mathcal{X}\) and \(\mathcal{X}\times\mathcal{X}\) respectively. \(\pi(.|\mathbf{X}=x,\mathbf{X}^{\prime}=x^{\prime})\) is conditional distribution of \(\pi\) condition to the first and second \(\mathcal{X}\) components equal to \(x\) and \(x^{\prime}\).

We observe that this integral effectively only depends on the \(\mathcal{X}\) component since the cost function \(c\) does not involve \(\mathcal{A}\). Hence, we reduce the expression to:

\[\sup_{\pi\in\Gamma(\mathbb{P})_{\mathcal{X}},(\mathbb{Q})_{\mathcal{X}}}\left\{ \int_{\mathcal{X}\times\mathcal{X}}d(x,x^{\prime})\pi\right\}\] (11)

The Eq. 11 shows that the optima cost function of \(W_{c}(\mathbb{P},\mathbb{Q})\) equals to \(W_{c}((\mathbb{P})_{\mathcal{X}},(\mathbb{Q})_{\mathcal{X}})\). Therefore if \(\pi_{\mathcal{X}}^{*}\) be the optimal transport plan for \(\mathbb{P}_{\mathcal{X}}\) to \(\mathbb{Q}_{\mathcal{X}}\) with respect to \(d\) on \(\mathcal{X}\), then any coupling \(\pi\) in \(\mathcal{U}\times\mathcal{U}\) that its marginal distribution \((\pi)_{\mathcal{X}\times\mathcal{X}}\) equals \(\pi_{\mathcal{X}}^{*}\) is the solution of optimal transport. It results that the conditional distribution \(\pi((.,.)|\mathbf{X}=x,\mathbf{X}^{\prime}=x^{\prime})\) could be any distribution. This completes the proof.

**Lemma 5**: _Let \(X\) and \(A\) be sets, and let \(f:X\times A\to\mathbb{R}\) be a function. Then_

\[\sup_{x\in X}\sup_{a\in A}f(x,a)=\sup_{a\in A}\sup_{x\in X}f(x,a).\]

**Proof.** Define:

\[L=\sup_{x\in X}\sup_{a\in A}f(x,a)\quad\text{and}\quad R=\sup_{a\in A}\sup_{ x\in X}f(x,a).\]

To show that \(L=R\), we need to prove that \(L\leq R\) and \(R\leq L\). Consider any \(x\in X\) and \(a\in A\). By definition, \(f(x,a)\leq\sup_{a\in A}f(x,a)\) for each fixed \(x\). Therefore,

\[f(x,a)\leq\sup_{a\in A}f(x,a)\leq\sup_{x\in X}\sup_{a\in A}f(x,a)=R.\]

Since \(f(x,a)\) was arbitrary, we have:

\[\sup_{a\in A}f(x,a)\leq R\quad\text{for all }x\in X,\]

and thus,

\[L=\sup_{x\in X}\sup_{a\in A}f(x,a)\leq R.\]

Similarly, for any fixed \(a\in A\), \(f(x,a)\leq\sup_{x\in X}f(x,a)\). Hence,

\[f(x,a)\leq\sup_{x\in X}f(x,a)\leq\sup_{a\in A}\sup_{x\in X}f(x,a)=L.\]

As before, since \(f(x,a)\) was arbitrary, we conclude:

\[\sup_{x\in X}f(x,a)\leq L\quad\text{for all }a\in A,\]

and thus,

\[R=\sup_{a\in A}\sup_{x\in X}f(x,a)\leq L.\]

Since \(L\leq R\) and \(R\leq L\), it follows that \(L=R\). Therefore, we have proven that:

\[\sup_{x\in X}\sup_{a\in A}f(x,a)=\sup_{a\in A}\sup_{x\in X}f(x,a).\]

This demonstrates the Principle of the Iterated Suprema.

### Proof of Theorem 1.

We prove the assertion in two steps: first, we assume that none of the sensitive attributes have parents, and second, we address and prove the general case. When all sensitive attributes do not have parents, in this case by definition2 semi-latent space equivalent with exogenous space and therefore \(g=F^{-1}\). First, we show that the worst-case loss quantity can be decomposed into sensitive and non-sensitive components like as below equation:

\[\sup_{\mathbb{Q}\in\mathbb{B}_{\delta}(\mathbb{P})}\left\{\underset{v\sim \mathbb{Q}}{\mathbb{E}}[\psi(v)]\right\}=\sup_{\mathbb{Q}\in\mathbb{B}_{\delta }((F^{-1}_{\#}\mathbb{P})_{\mathcal{X}})}\left\{\underset{u_{x}\sim\mathbb{Q} }{\mathbb{E}}\left[\sup_{u_{a}\in\mathcal{U}\mathcal{A}}\left\{\psi\left(F \left((u_{a},u_{x})\right)\right)\right\}\right]\right\}.\] (12)

By the assumption, the CFDF has a form \(d(v,v^{\prime})=d_{\mathcal{X}}(P_{\mathcal{X}}(F^{-1}(v)),P_{\mathcal{X}}(F^ {-1}(v^{\prime})))\). By Def. 2 in a case that sensitive attributes have no parents then the semi-latent space coincides with exogenous space and the map between feature space and semi-latent space equals \(g=F^{-1}\). Therefore in the following equations, we use \(g\) instead of \(F^{-1}\). Moreover since \(g\) is invertible by Lemma 3, we can write:

\[\sup_{\mathbb{Q}\in\mathbb{B}_{\delta}(\mathbb{P})}\left\{ \underset{v^{\prime}\sim\mathbb{Q}}{\mathbb{E}}[\psi(v^{\prime})]\right\}= \sup_{\mathbb{Q}\in\mathbb{B}_{\delta}(g\#\mathbb{P})}\left\{\underset{u^{ \prime}\sim\mathbb{Q}}{\mathbb{E}}[\psi(F(u^{\prime}))]\right\}=\] \[\sup_{\pi\in\mathcal{P}(\mathcal{U}\times\mathcal{U})}\left\{ \underset{u^{\prime}\sim\pi_{2}}{\mathbb{E}}[\psi(F(u^{\prime}_{a},u^{\prime} _{x}))]\;\Big{|}\;u\sim g_{\#}\mathbb{P},\underset{(u,u^{\prime})\sim\pi}{ \mathbb{E}}[\tilde{d}_{\mathcal{X}}(u,u^{\prime})]\leq\delta\right\}=\] \[\sup_{\pi\in\mathcal{P}(\mathcal{U}\times\mathcal{U})}\left\{ \underset{u^{\prime}\sim\pi_{2}}{\mathbb{E}}[\psi(F((u^{\prime}_{a},u^{\prime }_{x})))]\;\Big{|}\;u\sim g_{\#}\mathbb{P},\underset{(u,u^{\prime})\sim\pi}{ \mathbb{E}}[d_{\mathcal{X}}(u_{x},u^{\prime}_{x})]\leq\delta\right\}=\] \[\sup_{\pi\in\mathcal{P}(\mathcal{U}\times\mathcal{U})}\left\{ \underset{\mathcal{U}}{\int_{\mathcal{U}}\psi(F((u^{\prime}_{a},u^{\prime}_{x} )))\;d\pi_{2}(u^{\prime})}\;\Big{|}\;u\sim g_{\#}\mathbb{P},\underset{(u_{x}, u^{\prime}_{a})\sim\pi_{x\times\mathcal{X}}}{\mathbb{E}}[d_{\mathcal{X}}(u_{x},u^{ \prime}_{x})]\leq\delta\right\}=\ast,\]

where \(\tilde{d}\) be a cost function on \(\mathcal{U}\) defined as \(\tilde{d}(u,u^{\prime})=d_{\mathcal{X}}(P_{\mathcal{X}}(u),P_{\mathcal{X}}(u^ {\prime}))\), \(\pi_{2}\) denotes the marginal distribution on second part and \(\mathbb{B}_{\delta}(g_{\#}\mathbb{P})=\{\mathbb{Q}\in\mathcal{P}(\mathcal{U}) :W_{\tilde{d}}(\mathbb{Q},g_{\#}\mathbb{P})\leq\delta\}\). Using the disintegration theorem ([36] Chapter 3), the joint distribution \(\pi_{2}\) can be decomposed into the product of the conditional distribution of \(\mathcal{U}_{\mathcal{A}}\) given \(\mathcal{U}_{\mathcal{X}}\) and the marginal distribution on \(\mathcal{U}_{\mathcal{X}}\). Therefore we have

\[\int_{\mathcal{U}}\psi(F((u^{\prime}_{a},u^{\prime}_{x})))\;d\pi_{2}(u^{\prime })=\int_{\mathcal{U}_{\mathcal{X}}}\left(\int_{\mathcal{U}_{\mathcal{A}}}\psi(F ((u_{a},u_{x})))\;d\pi_{2}(u^{\prime}_{a}|\mathbf{U}_{\mathcal{X}}=u^{\prime} _{x})\right)d_{\mathcal{X}}(\pi_{2})_{\mathcal{X}}(u^{\prime}_{x}),\]

where \((\pi_{2})_{\mathcal{X}}\) is the marginal distribution of \(\pi_{2}\) over the non-sensitive part and \(\pi_{2}(u^{\prime}_{a}|\mathbf{U}_{\mathcal{X}}=u^{\prime}_{x})\) is a conditional distribution of the sensitive part of exogenous space condition by \(\mathbf{U}_{\mathcal{X}}=u^{\prime}_{x}\). By disintegration formula, (*) can be rewritten as:

\[\sup_{\pi_{2}\in\mathcal{P}(\mathcal{U})}\left\{\int_{\mathcal{U} }\psi(F((u^{\prime}_{a},u^{\prime}_{x})))d\pi_{2}(u^{\prime})\Big{|}\pi\in \mathcal{P}(\mathcal{U}\times\mathcal{U}),\pi_{1}=g_{\#}\mathbb{P},\underset{ (u_{x},u^{\prime}_{x})\sim(\pi)_{x\times\mathcal{X}}}{\mathbb{E}}[d(u_{x},u^{ \prime}_{x})]\leq\delta\right\}\] \[=\sup_{\pi_{2}\in\mathcal{P}(\mathcal{U})}\left\{\int_{\mathcal{U} _{\mathcal{X}}}\left(\int_{\mathcal{U}_{\mathcal{A}}}\psi(F(u_{a},u_{x}))\;d \pi_{2}(u^{\prime}_{a}|\mathbf{U}_{\mathcal{X}}=u^{\prime}_{x})\right)d_{ \mathcal{X}}(\pi_{2})_{\mathcal{X}}(u^{\prime}_{x})\;\Big{|}\] \[\pi\in\mathcal{P}(\mathcal{U}\times\mathcal{U}),\pi_{1}=g_{\#} \mathbb{P},\underset{(u_{x},u^{\prime}_{x})\sim(\pi)_{x\times\mathcal{X}}}{ \mathbb{E}}[d(u_{x},u^{\prime}_{x})]\leq\delta\right\}\] \[=\sup_{(\pi_{2})_{\mathcal{X}}\in\mathcal{P}(\mathcal{U}_{ \mathcal{X}})}\left\{\int_{\mathcal{U}_{\mathcal{X}}}\sup_{\pi_{2}(.|\mathbf{U} _{\mathcal{X}}=u^{\prime}_{x})}\Big{\{}\int_{\mathcal{U}_{\mathcal{A}}}\psi(F ((u^{\prime}_{a},u^{\prime}_{x})))d\pi_{2}(u^{\prime}_{a}|\mathbf{U}_{ \mathcal{X}}=u^{\prime}_{x})\Big{\}}d(\pi_{2})_{\mathcal{X}}(u^{\prime}_{x})\; \Big{|}\right.\] \[\pi\in\mathcal{P}(\mathcal{U}\times\mathcal{U}),\pi_{2\mathcal{X} }=(g_{\#}\mathbb{P})_{\mathcal{X}},\underset{(u_{x},u^{\prime}_{x})\sim(\pi)_{x \times\mathcal{X}}}{\mathbb{E}}[d_{\mathcal{X}}(u_{x},u^{\prime}_{x})]\leq\delta\right\}\] (13)

Since \(d_{\mathcal{X}}\) depends only on the non-sensitive components, it follows from Lemma 4 that \(\pi_{2}(.|\mathbf{U}_{\mathcal{X}}=u^{\prime}_{x})\) can achieve any distribution. Moreover, since it does not depend on the Wasserstein distance in each coupling, the marginal distribution of the sensitive attribute can be considered independent of the marginal distribution of the non-sensitive attributes. Therefore, the supremum over \(\pi_{2}(.|\mathbf{U}_{\mathcal{X}}=u^{\prime}_{x})\) of integral equals the supremum of \(\psi(F((u^{\prime}_{a},u^{\prime}_{x})))\) over all values of \(u^{\prime}_{a}\). Furthermore, the distribution\(\pi_{2}(u_{a}^{\prime}|u_{x}^{\prime})\) does not influence the value of the Wasserstein distance. Based on these points, the last equation can be rewritten as:

\[\sup_{\pi_{2}\in\mathcal{P}(\mathcal{U}_{\mathcal{X}})}\left\{\left. \int_{\mathcal{U}_{\mathcal{X}}}\sup_{u_{a}^{\prime}\in\mathcal{U}_{\mathcal{A} }}\psi(F((u_{a}^{\prime},u_{x}^{\prime})))\right]\pi_{2}\;\right|\pi\in\mathcal{ P}(\mathcal{U}_{\mathcal{X}}\times\mathcal{U}_{\mathcal{X}}),\] \[\pi_{1}=(g_{\#}\mathbb{P})_{\mathcal{X}},\mathop{\mathbb{E}}_{(u _{x},u_{x}^{\prime})\sim\pi}[d(u_{x},u_{x}^{\prime})]\leq\delta\right\}=\] \[\sup_{\pi_{2}\in\mathcal{P}(\mathcal{U}_{\mathcal{X}})}\left\{ \mathop{\mathbb{E}}_{u_{x}^{\prime}\sim\mathbb{Q}}\left[\sup_{u_{a}^{\prime} \in\mathcal{U}_{\mathcal{A}}}\left\{\psi\left(F\left((u_{a}^{\prime},u_{x}^{ \prime})\right)\right)\right\}\right]\;\right|\pi\in\mathcal{P}(\mathcal{U}_{ \mathcal{X}}\times\mathcal{U}_{\mathcal{X}}),\] \[\pi_{1}=(g_{\#}\mathbb{P})_{\mathcal{X}},\mathop{\mathbb{E}}_{(u _{x},u_{x}^{\prime})\sim\pi}[d(u_{x},u_{x}^{\prime})]\leq\delta\right\}=\] \[\sup_{\mathbb{Q}\in\mathbb{B}_{\delta}((g_{\#}\mathbb{P})_{ \mathcal{X}})}\left\{\mathop{\mathbb{E}}_{u_{x}^{\prime}\sim\mathbb{Q}}\left[ \sup_{u_{a}^{\prime}\in\mathcal{U}_{\mathcal{A}}}\left\{\psi\left(F\left((u_{a }^{\prime},u_{x}^{\prime})\right)\right)\right\}\right]\right\}.\]

The last equation concludes the proof of Eq. 12. Similarly, by altering the order of integration in Eq. 13, we arrive at the following equation:

\[\sup_{\mathbb{Q}\in\mathbb{B}_{\delta}(\varepsilon)}\left\{\mathop{\mathbb{E}} _{\upsilon\sim\mathbb{Q}}\left[\psi(v)\right]\right\}=\sup_{u_{a}\in\mathcal{U }_{\mathcal{A}}}\left\{\sup_{\mathbb{Q}\in\mathbb{B}_{\delta}((g_{\#} \mathbb{P})_{\mathcal{X}})}\left\{\mathop{\mathbb{E}}_{u_{x}\sim\mathbb{Q}} \left[\psi\left(F\left((u_{a},u_{x})\right)\right)\right]\right\}.\] (14)

To proceed with the proof, we utilize the strong duality theorem. There are various kinds of duality theorems for DRO, but we apply the one proposed by Blanchet et al. [11].

Strong duality [11].Suppose the transportation cost \(c:\mathcal{Z}\times\mathcal{Z}\rightarrow[0,\infty]\) satisfies \(c(z,z)=0\) for all \(z\in\mathcal{Z}\) and lower semi-continuous. Then for any reference probability distribution \(\mathbb{P}\) and upper semi-continuous \(\psi:\mathcal{Z}\rightarrow\mathbb{R}\) satisfying \(\mathop{\mathbb{E}}_{\mathbb{P}}[f(\mathbf{Z})]<\infty\), we have

\[\sup_{\mathbb{Q}\in\mathbb{B}_{\delta}(\mathbb{P})\;\mathbb{Q}}\left[\psi( \mathbf{Z})\right]=\inf_{\lambda\geq 0}\;\;\lambda\delta+\mathop{\mathbb{E}}_{ \mathbb{P}}\left[\psi_{\lambda}(\mathbf{Z})\right],\] (15)

where \(\psi_{\lambda}(z)\coloneqq\sup_{z^{\prime}\in\mathcal{Z}}\left\{\psi(z^{ \prime})-\lambda c(z,z^{\prime})\right\}\).

Based on the assumption about the CFDF, where only \(d(v,\breve{v}_{a^{\prime}})=0\), it follows that \(d(x,x^{\prime})=0\) only if \(x=x^{\prime}\). Therefore, we can apply the duality theorem to Eq. 12. According to the duality theorem, it can be expressed as follows:

\[\sup_{u_{a}\in\mathcal{U}_{\mathcal{A}}}\left\{\sup_{\mathbb{Q}\in\mathbb{B}_ {\delta}((g_{\#}\mathbb{P})_{\mathcal{X}})}\left\{\mathop{\mathbb{E}}_{u_{x} \sim\mathbb{Q}}\left[\psi\left(F\left((u_{a},u_{x})\right)\right)\right]\right\} \right\}=\inf_{\lambda\geq 0}\left\{\;\lambda\delta+\mathop{\mathbb{E}}_{u_{x}\sim(g_{ \#}\mathbb{P})_{\mathcal{X}}}\left[\eta_{\lambda}(u_{x})\right]\right\}\] (16)

where \(\eta_{\lambda}(u_{x})=\sup_{u_{x}^{\prime}\in\mathcal{U}_{\mathcal{X}}}\left\{ \sup_{u_{a}\in\mathcal{U}_{\mathcal{A}}}\left\{\psi(F((u_{a},u_{x}^{\prime})) )-\lambda d_{\mathcal{X}}(u_{x},u_{x}^{\prime})\right\}\). By using lemma 5\(\sup_{u_{x}^{\prime}\in\mathcal{U}_{\mathcal{X}}}\left\{\sup_{u_{a}\in \mathcal{U}_{\mathcal{A}}}\left\{\psi(F((u_{a},u_{x}^{\prime})))\right\}=\sup _{u_{a}\in\mathcal{U}_{\mathcal{A}}}\left\{\sup_{u_{a}^{\prime}\in\mathcal{U}_ {\mathcal{A}}}\left\{\psi(F((u_{a},u_{x}^{\prime})))\right\}\right\}\).

Now, since sensitive attributes don't have parents then two spaces \(\mathcal{U}_{\mathcal{A}}\) and \(\mathcal{A}\) are equal. By applying Lemma 2, we can replace the above equation with hard and soft interventions as follows:

\[\eta_{\lambda}(u_{x})=\sup_{a\in\mathcal{A}}\left\{\sup_{u_{x}^{ \prime}\in\mathcal{U}_{\mathcal{X}}}\left\{\psi(F((a,u_{x}^{\prime})))- \lambda d_{\mathcal{X}}(u_{x},u_{x}^{\prime})\right\}\right\}=\] \[\sup_{a\in\mathcal{A}}\left\{\sup_{\Delta\in\mathcal{U}_{ \mathcal{X}}}\left\{\psi(F((a,u_{x}+\Delta)))-\lambda d_{\mathcal{X}}(u_{x},u_{x }+\Delta)\right\}\right\}=\] \[\sup_{a\in\mathcal{A}}\left\{\sup_{\Delta\in\mathcal{U}_{ \mathcal{X}}}\psi(F((a,u_{x}+\Delta)))-\lambda d_{\mathcal{X}}(P_{\mathcal{X}}((a, u_{x})),P_{\mathcal{X}}((a,u_{x}+\Delta)))\right\}=\] \[\sup_{a\in\mathcal{A}}\left\{\sup_{\Delta\in\mathcal{U}_{ \mathcal{X}}}\psi(F((a,u_{x}+\Delta)))-\lambda d_{\mathcal{X}}(P_{\mathcal{X}}(g ^{-1}(g((a,u_{x})))),P_{\mathcal{X}}(g^{-1}(g((a,u_{x}+\Delta)))))\right\}=\] \[\sup_{a\in\mathcal{A}}\left\{\sup_{\Delta\in\mathcal{U}_{ \mathcal{X}}}\psi(\mathbf{CF}(\breve{v}^{a},\Delta))-\lambda c(\breve{v}^{a}, \mathbf{CF}(\breve{v}^{a},\Delta))\right\}=\sup_{a\in\mathcal{A}}\left\{ \breve{\psi}_{\lambda}(\breve{v}^{a})\right\}\] (17)Where \(\psi_{\lambda}(\tilde{v}^{a}):=\sup_{\Delta\in\mathcal{M}_{\mathcal{C}}}\psi( \textbf{CF}(\tilde{v}^{a},\Delta))-\lambda d(\tilde{v}^{a},\textbf{CF}(\tilde{v }^{a},\Delta)).\) The equations \(F((a,u_{x}+\Delta))=\textbf{CF}(v,\Delta)\) and \(F((a^{\prime},u_{x}+\Delta))=\textbf{CF}(\tilde{v}^{a^{\prime}},\Delta)\) hold true according to Lemma 2. Finally, by substituting \(\tilde{v}^{a}=\textbf{CF}(v,a)\) into the equation, we prove the equation:

\[\sup_{\mathbb{Q}\in\mathbb{B}_{d}(\mathbb{P})}\left\{\underset{v\sim\mathbb{Q}} {\mathbb{E}}\left[\psi(v)\right]\right\}=\inf_{\lambda\geq 0}\left\{\lambda \delta^{p}+\underset{v\sim\mathbb{P}}{\mathbb{E}}\left[\sup_{a\in\mathcal{A}} \psi_{\lambda}(\tilde{v}_{a})\right]\right\},\]

where \(\psi_{\lambda}(v)\) is defined as

\[\psi_{\lambda}(v)\coloneqq\sup_{\Delta\in\mathcal{X}}\left\{\psi(\textbf{CF} _{0}(v,\Delta))-\lambda^{p}d(v,\textbf{CF}_{0}(v,\Delta))\right\},\]

Since, in this case, \(CF\) is equivalent to \(\textbf{CF}_{0}\), this completes the proof for case one.

Now consider the scenario where sensitive attributes have parents. Eq. 17 shows in strong duality computation it needs to compute function in intervened \(\mathcal{M}\) concerning the sensitive attributes levels. In this case, instead of using the structural causal model \(\mathcal{M}\), it is sufficient to employ the parent-free sensitive attribute SCM (Def. 2), \(\mathcal{M}_{0}\). \(\mathcal{M}_{0}\) aligns with the semi-latent space and is compatible with the representation form outlined in Proposition 1. By adopting this strategy, we transform \(\mathcal{M}\) into a model where sensitive attributes do not have parents. The proof procedure for \(\mathcal{M}_{0}\) remains the same as for \(\mathcal{M}\) and completes the proof.

**Lemma 6**: _Let \((\mathcal{Z},c)\) be a space with cost function \(c\), \(\mathbb{P}_{N}\) an empirical probability measure based on observations \(\{z_{i}\}_{i=1}^{N}\), and define \(\mathbb{Q}\) as:_

\[\mathbb{Q}=\mathbb{P}_{N}-\frac{1}{N}\delta_{z_{1}}+\frac{1}{N}\delta_{z^{ \prime}_{1}}\]

_where \(\delta_{z_{1}}\) and \(\delta_{z^{\prime}_{1}}\) are Dirac measures at \(z_{1}\) and \(z^{\prime}_{1}\), respectively. Then, the \(p\)-Wasserstein distance between \(\mathbb{P}_{N}\) and \(\mathbb{Q}\) is given by:_

\[W_{c,p}(\mathbb{P}_{N},\mathbb{Q})=(\frac{1}{N})^{\frac{1}{p}}c(z_{1},z^{ \prime}_{1}).\]

Proof.The definition of the \(p\)-Wasserstein distance between two probability measures \(\mathbb{P}_{N}\) and \(\mathbb{Q}\) is:

\[W_{c,p}(\mathbb{P}_{N},\mathbb{Q})=\left(\inf_{\pi\in\Gamma(\mathbb{P}_{N}, \mathbb{Q})}\int_{\mathcal{Z}\times\mathcal{Z}}c(z,z^{\prime})^{p}\,d\pi(z,z^ {\prime})\right)^{\frac{1}{p}},\]

where \(\Gamma(\mathbb{P}_{N},\mathbb{Q})\) represents the set of all couplings of \(\mathbb{P}_{N}\) and \(\mathbb{Q}\). Since \(\mathbb{Q}\) is obtained by transferring a mass of \(\frac{1}{N}\) from \(z_{1}\) to \(z^{\prime}_{1}\), the optimal transport plan under the constraint that \(\mathbb{P}_{N}\) and \(\mathbb{Q}\) differ only at two points involves only moving the mass \(\frac{1}{N}\) from \(z_{1}\) to \(z^{\prime}_{1}\). The cost of this transportation is \(c(z_{1},z^{\prime}_{1})^{p}\), and because the entire mass \(\frac{1}{N}\) is being moved:

\[\int_{\mathcal{Z}\times\mathcal{Z}}c(z,z^{\prime})^{p}\,d\pi(z,z^{\prime})=c(z _{1},z^{\prime}_{1})^{p}\cdot\frac{1}{N}.\]

Therefore, substituting this into the formula for \(W_{p}\), we obtain:

\[W_{c,p}(\mathbb{P}_{N},\mathbb{Q})=\left(c(z_{1},z^{\prime}_{1})^{p}\cdot\frac {1}{N}\right)^{\frac{1}{p}}=\left(\frac{1}{N}\right)^{\frac{1}{p}}c(z_{1},z^{ \prime}_{1}),\]

thus proving the lemma.

**Lemma 7**: _Assume that \(f(a,x)\) is convex in \(x\) for each fixed \(a\) and continuous in both \(a\) and \(x\). Also, assume \(f\) is uniformly continuous in \(x\) across \(a\). If \(A\) is compact, then the function defined by_

\[g(x)=\sup_{a\in A}f(a,x)\]

_is convex and continuous in \(x\)._Proof.To show that \(g(x)\) is convex, consider any \(x_{1},x_{2}\) in the domain and \(\lambda\in[0,1]\). By the definition of supremum and the convexity of \(f(a,x)\) in \(x\),

\[f(a,\lambda x_{1}+(1-\lambda)x_{2})\leq\lambda f(a,x_{1})+(1-\lambda)f(a,x_{2}).\]

Taking the supremum over \(a\) in \(A\) on both sides, we get:

\[\sup_{a\in A}f(a,\lambda x_{1}+(1-\lambda)x_{2})\leq\sup_{a\in A}(\lambda f(a, x_{1})+(1-\lambda)f(a,x_{2})).\]

Using the properties of supremum,

\[\sup_{a\in A}f(a,\lambda x_{1}+(1-\lambda)x_{2})\leq\lambda\sup_{a\in A}f(a,x_ {1})+(1-\lambda)\sup_{a\in A}f(a,x_{2}).\]

Thus,

\[g(\lambda x_{1}+(1-\lambda)x_{2})\leq\lambda g(x_{1})+(1-\lambda)g(x_{2}),\]

proving that \(g(x)\) is convex.

To show continuity of \(g(x)\) at a point \(x_{0}\), consider any sequence \(\{x_{n}\}\) converging to \(x_{0}\). Since \(f\) is uniformly continuous in \(x\), given \(\epsilon>0\), there exists \(\delta>0\) such that for all \(x,y\) with \(|x-y|<\delta\),

\[|f(a,x)-f(a,y)|<\epsilon\quad\text{for all }a\in A.\]

Thus,

\[f(a,x_{n})<f(a,x_{0})+\epsilon\quad\text{and}\quad f(a,x_{0})<f(a,x_{n})+ \epsilon\quad\text{for all }a\in A\text{ and }|x_{n}-x_{0}|<\delta.\]

Taking the supremum over \(a\) in \(A\),

\[g(x_{n})\leq g(x_{0})+\epsilon\quad\text{and}\quad g(x_{0})\leq g(x_{n})+\epsilon.\]

This implies

\[|g(x_{n})-g(x_{0})|\leq\epsilon,\]

establishing the continuity of \(g(x)\) at \(x_{0}\).

Hence, we conclude that \(\sup_{a\in A}f(a,x)\) is convex and continuous in \(x\).

### Proof of Theorem 2.

Let's consider the \(\ell(v,y,\theta)=h(\mathbf{Y}-\langle\theta,\mathbf{V}\rangle)\) (or in abbreviation \(\ell(y)\)) or \(h(\mathbf{Y}\cdot\langle\theta,\mathbf{V}\rangle)\) where \(h:\mathbb{R}\to\mathbb{R}\) has one of the forms \(|t|\), \(\max(0,t)\), \(|t-\tau|\), or \(\max(0,t-\tau)\) for some \(\tau\geq 0\)

First consider the case \(\operatorname{diam}\left(\mathcal{A}\right)=\infty\). By property of CFDF for each Since the distance of \(v\) by its twins \(\breve{v}_{a}\) is zero.Let \(z\in\{z_{i}\}\) If \(\mathbb{P}_{N}\) the empirical distribution by lemma 6 it can be seen for observation \(Z=(v,y)\) the distribution

\[\mathbb{Q}_{a}=\mathbb{P}_{N}-\frac{1}{N}\delta_{z}+\frac{1}{N}\delta_{( \breve{v}_{a},y_{1})}\Longrightarrow W_{c,p}(\mathbb{Q}_{a},\mathbb{P}_{N})=0 \Longrightarrow\mathbb{Q}_{a}\in\mathbb{B}_{\delta}(\mathbb{P}_{N}).\]

This equation results that

\[\mathcal{R}_{\delta}(\mathbb{P}_{N},\theta)\geq\sup_{a\in\mathcal{A}}\mathcal{ R}_{\delta}(\mathbb{Q}_{a},\theta)\geq\mathcal{R}(\mathbb{P}_{N},\theta)-\frac{1}{N} \ell(v_{1})+\frac{1}{N}\sup_{a\in\mathcal{A}}\ell(\breve{v}_{a})|\geq\frac{1}{N }\sup_{a\in\mathcal{A}}\ell(\breve{v}_{a})|\]

Let \(u=(u_{\mathcal{A}},u_{\mathcal{X}})\) such that \(v=Mu\). By definition of hard intervention \(\breve{v}_{a}\) is obtained by the formula

\[\breve{v}_{a} =(M-M_{\mathbf{pa}})\times(u-(0,\ldots,\overbrace{a-u_{\mathcal{ A}},\ldots,0}^{\stackrel{{=\alpha}}{{\longrightarrow}}}^{\alpha}, \ldots,0)^{T})\] \[=M\times u-M\times(\overbrace{\alpha,\ldots,0}^{\stackrel{{ =}}{{\longrightarrow}}},\alpha,\ldots,0)-\underbrace{M_{\mathbf{pa}} \times\alpha}_{\stackrel{{=}}{{\longrightarrow}}}^{C}M_{ \mathbf{pa}}\times(\overbrace{0,\cdots,\alpha,0}^{\stackrel{{ =}}{{\longrightarrow}}},0)\] \[=v-M_{\mathcal{A}}\times\alpha-C\]where \(M_{\mathbf{pa}}\) refers to the effect of parents of sensitive variables and \(M_{\mathcal{A}}\) is the columns of matrix \(M\) related to sensitive attributes that show the effects of sensitive attributes on non-sensitive variables. By substituting that last equation in the loss function we have:

\[\mathcal{R}_{\delta}(\mathbb{P}_{N},\theta)\geq\frac{1}{N}\sup_{a\in\mathcal{A }}\ell(v-M_{\mathcal{A}}\times\alpha-C)\geq\frac{1}{N}\sup_{\alpha\to\infty}O (\theta^{T}M_{\mathcal{A}}\times\alpha)\]

where \(B\) is some constant value. with the assumptions about loss function, all of them by choosing proper \(z_{1}\), its behavior when \(\alpha\) is large enough is linear so can be approximated by its input value. Now Since the \(\operatorname{diam}\left(\mathcal{A}\right)=\infty\) therefore the value of \(\alpha\) goes to the infinity. Therefore to prevent the value of \(\mathcal{R}_{\delta}(\mathbb{P}_{N},\theta)\) it needs that the expression \(\theta^{T}M_{\mathcal{A}}\times\alpha=0\) in the other word the \(P_{\mathcal{A}}(M^{T}\theta)\) needs to be zero. This condition implies that for all \(a\in\mathcal{A}\) we have \(\ell(v)=\ell(\tilde{v}_{a})\). By using strong duality 8 we have:

\[\sup_{\mathbb{Q}\in\mathbb{B}_{\delta}(\mathbb{P})}\left\{\mathop{\mathbb{E}} \limits_{v\sim\mathbb{Q}}\left[\ell(v)\right]\right\}=\inf_{\lambda\geq 0} \left\{\lambda\delta+\mathop{\mathbb{E}}\limits_{v\sim\mathbb{P}}\left[\sup_{a \in\mathcal{A}}\ell_{\lambda}(\tilde{v}_{a})\right]\right\}=\inf_{\lambda \geq 0}\left\{\lambda\delta+\mathop{\mathbb{E}}\limits_{v\sim\mathbb{P}} \left[\ell_{\lambda}(v)\right]\right\},\] (18)

where

\[\ell_{\lambda}(v) =\sup_{\Delta\in\mathcal{X}}\left\{\ell(\mathbf{CF}(v,\Delta))- \lambda d(v,\mathbf{CF}(v,\Delta))\right\}=\] \[\sup_{\Delta\in\mathcal{X}}h(\theta^{T}M_{0}((u_{a},u_{x}+\Delta )))-\lambda d_{\mathcal{X}}(u_{x},u_{x}+\Delta)=\] \[\sup_{\Delta\in\mathcal{X}}h(P_{\mathcal{X}}(M^{T}\theta)^{T}(u_{ x}+\Delta))-\lambda d_{\mathcal{X}}(u_{x},u_{x}+\Delta)=\] \[\sup_{\Delta\in\mathcal{X}}h(\left\langle\theta_{0},u_{x}+\Delta \right\rangle)-\lambda\left\|\Delta\right\|=h_{\lambda}(u_{x})\] (19)

In the equation 19, \(M_{0}\) is reduced-form mapping of the parent-free sensitive attribute \(\mathcal{M}_{0}\). By the definition it is easy in both SCM, the effect of the sensitive attributes is equal therefore \(P_{\mathcal{A}}(M_{0}^{T}\theta)=P_{\mathcal{A}}(M^{T}\theta)=0\). Moreover since in \(\mathcal{M}_{0}\) the structure of non-sensitive attribute has not changed then \(P_{\mathcal{X}}(M_{0}^{T}\theta)=P_{\mathcal{X}}(M^{T}\theta)\). Finally, by substituting \(\theta_{0}=P_{\mathcal{X}}(M^{T}\theta)\), it can be seen that the problem of finding worst-case loss quantity converts to the regular problem in space \(\mathcal{X}\). This problem was solved previously in works of [14, 58, 25, 66]. By using Theorem 3.2 and 3.3 and Proposition 4.1 and 4.2, of work by Chu et al. [14], we can write

\[\inf_{\lambda\geq 0}\left\{\lambda\delta+\mathop{\mathbb{E}} \limits_{v\sim\mathbb{P}_{N}}\left[\ell_{\lambda}(v)\right]\right\}=\inf_{ \lambda\geq 0}\left\{\lambda\delta+\mathop{\mathbb{E}}\limits_{u_{x}\sim(g_{ \theta}\mathbb{P}_{N})_{\mathcal{X}}}\left[h_{\lambda}(u_{x})\right]\right\}=\] \[\left(\mathcal{R}((g_{\theta}\mathbb{P}_{N})_{\mathcal{X}},\theta _{0})^{\frac{1}{p}}+\delta\left\|\theta_{0}\right\|_{*}\right)^{p}=\left( \mathcal{R}(\mathbb{P}_{N},\theta)^{\frac{1}{p}}+\delta\left\|P_{\mathcal{X}} (M^{T}\theta)\right\|_{*}\right)^{p}\]

The equality \(\mathcal{R}((g_{\#}\mathbb{P}_{N})_{\mathcal{X}},\theta_{0})=\mathcal{R}( \mathbb{P}_{N},\theta)\) holds by definition and property \(P_{\mathcal{A}}(M^{T}\theta)=0\). The last equation completes the proof of the first case.

Now let's consider the case that \(\operatorname{diam}\left(\mathcal{A}\right)<\infty\).

Case:\(p\in(1,\infty)\).Lets consider Eq. 16 it implies that:

\[\mathcal{R}_{\delta}(\mathbb{P})=\inf_{\lambda\geq 0}\left\{\ \lambda\delta+\mathop{\mathbb{E}}\limits_{u_{x}\sim(g_{\#}\mathbb{P}_{N})_{ \mathcal{X}}}\left[\tilde{\ell}_{\lambda}(u_{x})\right]\right\}\] (20)

where, \(\tilde{\ell}(u_{x})=\sup_{u_{a}\in\mathcal{U}_{\mathcal{A}}}\left\{\ell(M(u_{a},u_{x}^{\prime})\right\}\) and \(\eta_{\lambda}(u_{x})=\sup_{\Delta\in\mathcal{U}_{\mathcal{X}}}\tilde{\ell}(u_{ x}+\Delta)-\lambda d_{\mathcal{X}}(u_{x},u_{x}+\Delta)\right\}\). By assumption, the whole type of loss functions are form \(h(\left\langle\theta,v\right\rangle)\) and convex and continuous. \(h\) can be written by \(M\) in the form \(h(\left\langle\theta,M(u_{a},u_{x})\right\rangle)\). Then \(\tilde{\ell}(u_{x})=\sup_{u_{a}\in\mathcal{U}_{\mathcal{A}}}h(P_{\mathcal{A}} (M^{T}\theta)u_{a}+P_{\mathcal{X}}(M^{T}\theta)u_{x})\). Since all forms of function are uniformly continuous concerning the \(u_{x}\) then lemma 7 implies that the \(\tilde{\ell}(u_{x})\) is still continuous and convex. Theorem 6 and 7 of work [66] states that:

**Theorem**.: [66] Let \(\ell:\mathbb{R}\to\mathbb{R}\) be a non-negative, Lipschitz continuous and convex function. For an integer \(p\in(1,\infty)\), suppose that for any \(\mathbb{P}\in\mathcal{P}(\mathcal{X})\), and \(\epsilon\geq 0\), we have:

\[\sup_{\mathbb{Q}\in\mathbb{B}_{\delta}(\mathbb{P})}\mathbb{E}_{x\sim\mathbb{Q}} [\ell^{p}(\theta^{T}x)]=\left(\left(\mathop{\mathbb{E}}\limits_{x\sim\mathbb{P}} [\ell^{p}(\theta^{T}x)]\right)^{1/p}+\delta\left\|\theta\right\|_{*}\right)^{p}.\]By applying above theorem in Eq. 20 it can be seen:

\[\mathcal{R}_{\delta}(\mathbb{P})= \left(\left(\left[\mathbb{E}_{u_{x}\sim\left\{g_{\mathcal{A}}\bar{ \theta}\right\}\!\bar{\nu}_{\mathcal{X}}}\big{[}\tilde{\ell}^{p}(u_{x})\big{]} \right)^{1/p}+\delta\left\|P_{\mathcal{X}}(M^{T}\theta)\right\|_{*}\right)^{p}=\] \[\left(\left(\mathbb{E}_{v\sim\mathbb{P}}[\sup_{a\in\mathcal{A}} \ell^{p}(\tilde{v}_{a})]\right)^{1/p}+\delta\left\|P_{\mathcal{X}}(M^{T}\theta )\right\|_{*}\right)^{p}\]

**Case: \(p=1\).** To complete the proof we use Theorem 2 and Corollary 2 of Gao et al. work [27].

**Theorem [27]** If \(\ell\) is Lipschitz \(|\ell(x_{1})-\ell(x_{2})|\leq L\left\|x_{k}-x_{0}\right\|\) and satisfies tightness at infinity, i.e. for every \(v_{0}\) there exists sequence \(\{v_{k}\}_{k=1}^{\infty}\in\mathcal{V}\) such that \(\left\|x_{k}-x_{0}\right\|\to\infty\) we have:

\[\lim_{\left\|x_{k}-x_{0}\right\|\to\infty}\frac{\left|\ell(x_{k})-\ell(x_{0}) \right|}{\left\|x_{k}-x_{0}\right\|}=L\] (21)

then we have \(\mathcal{R}_{\delta}(\mathbb{P})=\mathcal{R}(\mathbb{P})+\delta.L\).

Now back to the Eq. 20. It is necessary to show that \(\tilde{\ell}\) satisfies the Gao's theorem. Let \(h(t)\) be one of the functions \(\left|t\right|\), \((t-\tau)_{+}\), \((\left|t-\tau\right|_{+}\). All of loss function can be written as form \(h(y-\left\langle\theta,v\right\rangle)\) or \(h(y.\left\langle\theta,v\right\rangle)\). It is easy to check that \(h\) is Lipschitz with constant 1 and there exist \(t_{k}\) such that for each \(t_{0}\) we have \(lim_{k\to\infty}\frac{\left|h(t_{0}+t_{k})-h(t_{0})\right|}{\left|t_{k}\right| }=1\).

To use this theorem for \(\tilde{\ell}\), we need to prove that \(\tilde{\ell}\) is Lipschitz and has a tightness condition at infinity. Since the \(h\) is Lipschitz we know that:

\[\forall u_{a}\in\mathcal{U}_{\mathcal{A}}: \quad|h(P_{\mathcal{A}}(M^{T}\theta)u_{a}+P_{\mathcal{X}}(M^{T} \theta)u_{x})-h(P_{\mathcal{A}}(M^{T}\theta)u_{a}+P_{\mathcal{X}}(M^{T}\theta )u_{x}^{\prime})|\leq\] \[\quad|P_{\mathcal{X}}(M^{T}\theta)(u_{x}-u_{x}^{\prime})|\leq \left\|P_{\mathcal{X}}(M^{T}\theta)\right\|_{*}\left\|u_{x}-u_{x}^{\prime}\right\|\Rightarrow\] \[\quad\sup_{u_{a}\in\mathcal{U}_{\mathcal{A}}}\quad|h(P_{\mathcal{ A}}(M^{T}\theta)u_{a}+P_{\mathcal{X}}(M^{T}\theta)u_{x})-h(P_{\mathcal{A}}(M^{T} \theta)u_{a}+P_{\mathcal{X}}(M^{T}\theta)u_{x}^{\prime})|\] \[\quad\sup_{u_{a}\in\mathcal{U}_{\mathcal{A}}}|h(P_{\mathcal{A}}(M ^{T}\theta)u_{a}+P_{\mathcal{X}}(M^{T}\theta)u_{x})-h(P_{\mathcal{A}}(M^{T} \theta)u_{a}+P_{\mathcal{X}}(M^{T}\theta)u_{x}^{\prime})|=\] \[\quad|\tilde{\ell}(u_{x})-\tilde{\ell}(u_{x}^{\prime})|\leq \left\|P_{\mathcal{X}}(M^{T}\theta)\right\|_{*}\left\|u_{x}-u_{x}^{\prime} \right\|\Rightarrow\quad\tilde{\ell}\quad\text{is Lipbschitz}.\]

To satisfy the condition of Gao's theorem, it remains to show for \(u_{x}^{0}\) there exists sequence \(\{u_{x}^{k}\}_{k=1}^{\infty}\) such that \(\lim_{\left\|u_{x}^{k}-u_{x}^{0}\right\|\to\infty}\frac{\left|\tilde{\ell}(u_{x }^{k})-\tilde{\ell}(u_{x}^{0})\right|}{\left\|u_{x}^{k}-u_{x}^{0}\right\|}=L\) To prove it we consider that for function \(h\) there exists sequence such that \(lim_{k\to\infty}\frac{\left|h(t_{0}+t_{k})-h(t_{0})\right|}{\left|t_{k}\right| }=1\). consider the specific point \(u_{x}^{0}\) and \(u_{a}\in\mathcal{U}_{\mathcal{A}}\).

Lets define \(t_{0}=P_{\mathcal{A}}(M^{T}\theta)u_{a}+P_{\mathcal{X}}(M^{T}\theta)u_{x}^{0}\). Then there exists \(t_{k}\) that satisfies infinity tightness. Therefore there exist \(\Delta_{k}\in\mathcal{U}_{\mathcal{X}}\) such that \(P_{\mathcal{X}}(M^{T}\theta)u_{x}^{k}-P_{\mathcal{X}}(M^{T}\theta)u_{x}^{0}=t_ {k}\) therefore it can be written:

\[1= lim_{k\to\infty}\frac{\left|h(t_{0}+t_{k})-h(t_{0})\right|}{\left|t_{k} \right|}=\] \[\quad lim_{k\to\infty}\frac{\left|h(P_{\mathcal{A}}(M^{T}\theta)u _{a}+P_{\mathcal{X}}(M^{T}\theta)u_{x}^{k})-h(P_{\mathcal{A}}(M^{T}\theta)u_{a} +P_{\mathcal{X}}(M^{T}\theta)u_{x}^{0})\right|}{\left|P_{\mathcal{X}}(M^{T} \theta)u_{x}^{k}\right|}\Rightarrow\] \[\quad lim_{k\to\infty}\frac{\left|h(M^{T}\theta(u_{a},u_{x}^{k}) )-h(M^{T}\theta(u_{a},u_{x}^{0}))\right|}{\left\|u_{x}^{k}\right\|}=\left\|P_{ \mathcal{X}}(M^{T}\theta)\right\|_{*}\Rightarrow\] \[\quad\sup_{u_{a}\in\mathcal{U}_{\mathcal{A}}}\left\{lim_{k\to \infty}\frac{\left|h(M^{T}\theta(u_{a},u_{x}^{k}))-h(M^{T}\theta(u_{a},u_{x}^{0 }))\right|}{\left\|u_{x}^{k}\right\|}\right\}=\left\|P_{\mathcal{X}}(M^{T} \theta)\right\|_{*}\Rightarrow\] \[\quad lim_{k\to\infty}\frac{\left|\tilde{\ell}(u_{x}^{k})-\tilde{ \ell}(u_{x}^{0})\right|}{\left\|u_{x}^{k}\right\|}=\left\|P_{\mathcal{X}}(M^{T} \theta)\right\|_{*}\]In the above equation, since we have uniform convergence, we can change the limit and supremum. The last equation shows that there exits sequence of \(\{u_{x}^{k}\}_{k=1}^{\infty}\) satisfies tightness condition in \(\infty\). By applying Gao's theorem we have:

\[\mathcal{R}_{\delta}(\mathbb{P})= \mathbb{E}_{u_{x}\sim(g\pm\mathbb{P})_{\mathcal{X}}}[\tilde{\ell} (u_{x})]+\delta\left\|P_{\mathcal{X}}(M^{T}\theta)\right\|_{*}=\mathbb{E}_{v \sim\mathbb{P}}[\sup_{a\in\mathcal{A}}\ell^{p}(\tilde{v}_{a})]+\delta\left\|P_ {\mathcal{X}}(M^{T}\theta)\right\|_{*}\]

The last equation completes the proof.

### Proof of Theorem 3.

The case \(\operatorname{diam}\left(\mathcal{A}\right)=\infty\) corresponds exactly to the first part of the proof of Theorem 2, with the only difference being that in that theorem we have \(\left\|h\right\|_{\operatorname{Lip}}=1\), while in this case, we have \(\left\|h\right\|_{\operatorname{Lip}}=L_{h}\). Therefore we have the below equation:

\[\mathcal{R}_{\delta}(\mathbb{P})=\mathcal{R}(\mathbb{P})+L_{h}\left\|P_{ \mathcal{X}}(M^{T}\theta)\right\|_{*}\]

Now consider the case \(\operatorname{diam}\left(\mathcal{A}\right)<\infty\). To prove our assertion, we use Eq. 16 and it implies that:

\[\mathcal{R}_{\delta}(\mathbb{P})=\inf_{\lambda\geq 0}\left\{\begin{array}{c} \lambda\delta+\underset{u_{x}\sim(g\pm\mathbb{P})_{\mathcal{X}}}{\mathbb{E}} \left[\tilde{\ell}_{\lambda}(u_{x})\right]\right\}\]

where, \(\tilde{\ell}(u_{x})=\sup_{u_{a}\in\mathcal{U}_{\mathcal{A}}}\left\{\ell(M(u_{a},u_{x}^{\prime})\right\}\) and \(\eta_{\lambda}(u_{x})=\sup_{\Delta\in\mathcal{U}_{\mathcal{A}}}\tilde{\ell}(u _{x}+\Delta)-\lambda d_{\mathcal{X}}(u_{x},u_{x}+\Delta)\right\}\). To compute the right side of the above equation, we use the theorem 3.2 of work [14] that states.

Theorem [14] Let \(\mathcal{Z}_{N}\coloneqq\{z_{1},\ldots,z_{n}\}\subset\mathcal{Z}\) be a given dataset and \(\mathbb{P}_{N}\) be the corresponding empirical distribution. In addition, let \(c(\cdot,\cdot)\) be a cost function on \(\mathcal{Z}\times\mathcal{Z}\) and \(\delta\in(0,\infty)\) be a scalar. Suppose the loss function \(\ell:\mathcal{Z}\times\Theta\to\mathbb{R}\), where satisfies the following assumptions:

* \(\ell\) is Lipschitz respect to the cost function \(d\) at set \(\mathcal{Z}_{N}\) with \(L_{\theta}^{\mathcal{Z}_{N}}\in(0,\infty)\);
* for any \(\epsilon\in(0,L_{\theta}^{\mathcal{Z}_{N}})\) and each \(z_{i}\in\mathcal{Z}_{N}\), there exists \(\tilde{z}_{i}\in\mathcal{Z}\) such that \(\delta\leq d(\tilde{z}_{i},z_{i})<\infty\) and \[\ell(\tilde{z}_{i})-\ell(z_{i})\geq(L_{\theta}^{\mathcal{Z}_{N}}-\epsilon)c( \tilde{z}_{i},z_{i}).\]

Then we have:

\[\sup_{\mathbb{P}:\ W_{d,1}(\mathbb{Q},\mathbb{P}_{N})\leq\delta}\mathrm{E}_{ \mathbb{Q}}[\ell(\mathbf{Z},\theta)]=\mathrm{E}_{\mathbb{P}_{N}}[\ell( \mathbf{Z},\theta)]+L_{\theta}^{\mathcal{Z}_{N}}\delta.\]

To use the above theorem we need that \(\tilde{\ell}\) satisfies conditions (A1) and (A2).

* To prove the Lipschitz condition it can be seen: \[\forall u_{a}\in\mathcal{U}_{\mathcal{X}}: |h(y-\theta^{T}M(u_{a},u_{x}))-h(y-\theta^{T}M(u_{a},u_{x}^{ \prime}))|\] \[\leq L_{h}|P_{\mathcal{X}}(M^{T}\theta)(u_{a}-u_{a}^{\prime})| \leq L_{h}\left\|P_{\mathcal{X}}(M^{T}\theta)\right\|_{*}\left\|u_{a}-u_{a}^{ \prime}\right\|\Rightarrow\] \[\sup_{u_{a}\in\mathcal{U}_{\mathcal{X}}}|h(y-\theta^{T}M(u_{a},u _{x}))-h(y-\theta^{T}M(u_{a},u_{x}^{\prime}))|=\] \[|\tilde{\ell}(u_{a})-\tilde{\ell}(u_{x}^{\prime})|\leq L_{h} \left\|P_{\mathcal{X}}(M^{T}\theta)\right\|_{*}\left\|u_{x}-u_{x}^{\prime}\right\|\]

The case \(h(y.\left\langle\theta,y\right\rangle)\) is similar so we omit it.

* To check that \(\tilde{\ell}\) satisfies (A2) condition we show that there exists sequence \(\{\Delta^{k}\}\) such that the \(\left\|\Delta_{k}\right\|\to\infty\) for every \(v\in\mathcal{V}\) and sequence \(v_{k}=\mathbf{CF}(v,\Delta_{k})\) and we have: \[lim_{k\to\infty}\frac{|h(y-\left\langle\theta,v_{k}\right\rangle)-h(y-\left\langle \theta,v\right\rangle)|}{d(v_{k},v)}=L_{h}.\left\|P_{\mathcal{X}}(M^{T}\theta) \right\|_{*}\] (22) By assumption about \(h\) we have: For each \(t_{0}\in\mathbb{R}\) there exists sequence of \(\{t_{k}\}_{k=1}^{\infty}\) goes to \(\infty\) then \(lim_{k\to\infty}\frac{|h(t_{0}+t_{k})-h(t_{0})|}{|t_{k}|}=L_{h}\). By changing variable \(v=M(u_{a},u_{x})\). Let\(y-\theta^{T}M(u_{a},u_{x})\) and \(\Delta_{k}\in\mathcal{X}\) such that \(P_{\mathcal{X}}(M^{T}\theta)\Delta_{k}=t_{k}\) it is clear \(\Delta_{k}\) exist. No if we define \(v_{k}=\mathbf{CF}(v,\Delta_{k})\) we have:

\[L_{h}= lim_{k\rightarrow\infty}\frac{|h(t_{0}+t_{k})-h(t_{0})|}{|t_{k}|}=\] \[lim_{k\rightarrow\infty}\frac{|h(y-\theta^{T}M(u_{a},u_{x})+P_{ \mathcal{X}}(M^{T}\theta)\Delta_{k})-h(y-\theta^{T}M(u_{a},u_{x}))|}{|P_{ \mathcal{X}}(M^{T}\theta)\Delta_{k}|}=\] \[lim_{k\rightarrow\infty}\frac{|h(y-\theta^{T}M(u_{a},u_{x}+ \Delta_{k}))-h(y-\theta^{T}M(u_{a},u_{x}))|}{|P_{\mathcal{X}}(M^{T}\theta) \Delta_{k}|}=\] \[lim_{k\rightarrow\infty}\frac{|h(y-\theta^{T}v_{k})-h(y-\theta ^{T}v)|}{\|P_{\mathcal{X}}(M^{T}\theta)\|_{*}\left\|\Delta_{k}\right\|}=lim_ {k\rightarrow\infty}\frac{|h(y-\langle\theta,v_{k}\rangle)-h(y-\langle\theta,v\rangle)|}{\|P_{\mathcal{X}}(M^{T}\theta)\|_{*}\left\|d(v_{k},v\right)}\] \[\implies lim_{k\rightarrow\infty}\frac{|h(y-\langle\theta,v_{k} \rangle)-h(y-\langle\theta,v\rangle)|}{d(v_{k},v)}=\left\|P_{\mathcal{X}}(M^{ T}\theta)\right\|_{*}L_{h}\]

The Last equation is valid because, by Holder inequality, there exists \(\Delta\) such that for all \(\lambda\Delta\) the Holder inequality converts to equality. Now it is sufficient that find proper \(\lambda\) such that \(\lambda_{k}=t_{k}/\left\|P_{\mathcal{X}}(M^{T}\theta)\Delta\right\|_{*}\) so by define \(\Delta_{k}=\lambda_{k}\Delta\) we find sequence that holds the assertion.

The case of \(h(y.\langle\theta,v\rangle\) is similar. By discussion of the first part, we can find \(\Delta_{k}\). Now since we have binary classification, so \(y\in\{-1,1\}\) we define \(\tilde{\Delta}_{k}=\text{sign}(y)\Delta_{k}\) therefore for such \(\Delta_{k}\), we have \(y.P_{\mathcal{X}}(M^{T}\theta)\tilde{\Delta}_{k})=t_{k}\). By assumption, it can be written as:

\[L_{h}= lim_{k\rightarrow\infty}\frac{|h(t_{0}+t_{k})-h(t_{0})|}{|t_{k}|}=\] \[lim_{k\rightarrow\infty}\frac{|h(y.\theta^{T}M(u_{a},u_{x})+y.P_ {\mathcal{X}}(M^{T}\theta)\tilde{\Delta}_{k})-h(y.\theta^{T}M(u_{a},u_{x}))|}{ |P_{\mathcal{X}}(M^{T}\theta)\tilde{\Delta}_{k}|}=\] \[lim_{k\rightarrow\infty}\frac{|h(y.\theta^{T}M(u_{a},u_{x})+y.P_ {\mathcal{X}}(M^{T}\theta)\Delta_{k})-h(y.\theta^{T}M(u_{a},u_{x}))|}{|P_{ \mathcal{X}}(M^{T}\theta)\Delta_{k}|}=\] \[lim_{k\rightarrow\infty}\frac{|h(y.\theta^{T}M(u_{a},u_{x}+ \Delta_{k}))-h(y.\theta^{T}M(u_{a},u_{x}))|}{|P_{\mathcal{X}}(M^{T}\theta) \Delta_{k}|}=\] \[lim_{k\rightarrow\infty}\frac{|h(y.\theta^{T}v_{k})-h(y.\theta ^{T}v)|}{\|P_{\mathcal{X}}(M^{T}\theta)\|_{*}\left\|\Delta_{k}\right\|}=lim_{ k\rightarrow\infty}\frac{|h(y.\langle\theta,v_{k}\rangle)-h(y-\langle\theta,v \rangle)|}{\|P_{\mathcal{X}}(M^{T}\theta)\|_{*}\left\|d(v_{k},v\right)}\] \[\implies lim_{k\rightarrow\infty}\frac{|h(y-\langle\theta,v_{k} \rangle)-h(y-\langle\theta,v\rangle)|}{d(v_{k},v)}=\left\|P_{\mathcal{X}}(M^{T} \theta)\right\|_{*}.L_{h}\]

Let \(\ell(v)=h(y-P_{\mathcal{X}}(M^{T}\theta)u_{x}-P_{\mathcal{A}}(M^{T}\theta)u_{a})\). By assumption \(h\) is Lipschitz so it \(\ell\) is Lipschitz concerning each \(u_{x}\) and \(u_{a}\) and \(\mathcal{U}_{\mathcal{A}}\) is bounded so it is compact. Then these properties imply uniformly continuous so we have:

\[\sup_{u_{a}\in\mathcal{U}_{\mathcal{A}}}lim_{k\rightarrow\infty} \frac{|h(y-\langle\theta,v_{k}\rangle)-h(y-\langle\theta,v\rangle)|}{d(v_{k},v)}=\] \[lim_{k\rightarrow\infty}\sup_{u_{a}\in\mathcal{U}_{\mathcal{A}}} \frac{|h(y-\langle\theta,v_{k}\rangle)-h(y-\langle\theta,v\rangle)|}{d(v_{k},v)}= lim_{k\rightarrow\infty}\frac{|\tilde{\ell}(u_{x})-\tilde{\ell}(\Delta_{k})|}{ \left\|u_{x}-\Delta_{k}\right\|}=\left\|P_{\mathcal{X}}(M^{T}\theta)\right\|_{* }.L_{h}\]

The last equation satisfies the (\(A_{2}\)) condition because since \(lim_{k\rightarrow\infty}\frac{|\tilde{\ell}(u_{x})-\tilde{\ell}(\Delta_{k})|}{ \left\|u_{x}-\Delta_{k}\right\|}=\left\|P_{\mathcal{X}}(M^{T}\theta)\right\|_{*}.L_{h}\) in other hand we have \(\left|\tilde{\ell}(u_{a})-\tilde{\ell}(u_{x}^{\prime})\right|\leq L_{h}\left\|P_ {\mathcal{X}}(M^{T}\theta)\right\|_{*}\left\|u_{x}-u_{x}^{\prime}\right\|\), then for each \(\epsilon\) there exist \(\Delta_{k}\) such that \(\left|\tilde{\ell}(u_{x})-\tilde{\ell}(\Delta_{k})\right|>\left(L_{h}\left\|P_{ \mathcal{X}}(M^{T}\theta)\right\|_{*}\left\|u_{x}-\Delta_{k}\right\|-\epsilon \right)\left\|u_{x}-\Delta_{k}\right\|\)

Now we can use Chu's Theorem and it implies that: \(\mathcal{R}_{\delta}(\mathbb{P})=\mathcal{R}^{cf}(\mathbb{P})+L_{h}\left\|P_{ \mathcal{X}}(M^{T}\theta)\right\|_{*}\). The classification case is the same and it completes the proof.

### Proof of Theorem 4.

Let's prove the necessary condition. By first part proof of theorem 2, we have

\[\mathcal{R}_{\delta}(\mathbb{P}_{N},\theta)\geq\frac{1}{N}\sup_{a\in\mathcal{A}} \ell(\tilde{v}_{a},y,\theta).\]

Therefore, for a finite solution to exist for the DRO problem, it is necessary that:

\[\sup_{a\in\mathcal{A}}\ell(\tilde{v}_{a},y,\theta)<\infty.\]

To prove Eq. 10, we use again some parts of the proof of strong duality theorem1 and idea of proof of theorem 9.1 of Garcia's work [29]. It states:

\[\mathcal{R}_{\delta}(\mathbb{P}_{N})=\sup_{\mathbb{Q}\in\mathbb{B}_{\delta}( \mathbb{P}_{N})}\left\{\underset{v\sim\mathbb{Q}}{\mathbb{E}}\left[\ell(v^{ \prime},y,\theta)\right]\right\}=\sup_{\begin{subarray}{c}\mathbb{Q}_{x}\in \mathbb{B}_{\delta}((g_{\#}\mathbb{P}_{N})_{\mathcal{X}})\\ \mathbb{Q}_{x}\in\mathcal{P}(\mathcal{A})\end{subarray}}\left\{\underset{u_{x} ^{\prime}\sim\mathbb{Q}_{x}}{\mathbb{E}}\left[\ell\left(g^{-1}\left((u_{a}^{ \prime},u_{x}^{\prime})\right),y,\theta,\right)\right]\right\}\]

where by discussion in lemma 4, we can suppose that \(\mathbb{Q}_{x}\) and \(\mathbb{Q}_{a}\) are independent of each other. For simplicity, we define \(J(u_{x},u_{a},\theta)=\ell(g^{-1}((u_{a},u_{x})),y,\theta)\). By assumption, since the \(f\) is twice differentiable, then \((I-f)^{-1}\) is also twice differentiable. Because the function \(g\) is obtained by \(I-f\) by removing the functional structure of sensitive attributes and is a set identity function instead of them, there are two functions \(g\) and \(g^{-1}\). These results show that combination \(\ell(g^{-1})\) is also twice differentiable, so the gradient of \(J\) exists concerning the \(u_{x}\). By using Taylor's expansion theorem if \(f:\mathbb{R}\rightarrow\mathbb{R}\) is the function that has gradient then the first order estimation of \(f\) equals:

\[f(x+h)=f(x)+\nabla f(x)^{\top}h+\int_{0}^{1}(\nabla f(x+th)-\nabla f(x))^{\top }h\,dt.\]

Let \(u_{x}\sim(g_{\#}\mathbb{P})_{\mathcal{X}}\) by writing Taylor's expansion around \(u_{x}\) we have:

\[\mathbb{E}\left[J(u_{x}^{\prime},u_{a}^{\prime},\theta)\right]= \mathbb{E}\left[J(u_{x},u_{a}^{\prime},\theta)+\nabla_{x}J(u_{x},u_{a}^{\prime },\theta)\cdot(u_{x}^{\prime}-u_{x})\right.\] \[\left.+\,\int_{0}^{1}\{\nabla_{x}J(u_{x}+\lambda(u_{x}^{\prime}- u_{x}),u_{a}^{\prime},\theta)-\nabla_{x}J(u_{x},u_{a}^{\prime},\theta)\}\cdot(u_{x} ^{\prime}-u_{x})d\lambda\right].\]

Since the \(\text{diam}\left(\mathcal{U}\right)<\infty\) we can suppose that the space \(\mathcal{U}\) is compact. By assumption \(J\) is twice differentiable, so \(\nabla_{x}J(.,u_{a},\theta)\) is Lipschitz with constant \(\|\nabla_{x}J(.,u_{a},\theta)\|_{\text{Lip}}\) that there exist \(L<\infty\) such that \(\|\nabla_{x}J(.,u_{a},\theta)\|_{\text{Lip}}\leq L\). By these assumptions, it can be written:

\[\mathbb{E}\left[\int_{0}^{1}\{\nabla_{x}J(u_{x}+\lambda(u_{x}^{ \prime}-u_{x}),u_{a}^{\prime},\theta)-\nabla_{x}J(u_{x},u_{a}^{\prime},\theta) \}\cdot(u_{x}^{\prime}-u_{x})d\lambda\right]\leq\] \[\mathbb{E}\left[\int_{0}^{1}\|\nabla_{x}J(u_{x}+\lambda(u_{x}^{ \prime}-u_{x}),u_{a}^{\prime},\theta)-\nabla_{x}J(u_{x},u_{a}^{\prime},\theta) \|\|u_{x}^{\prime}-u_{x}\|_{e}^{2}d\lambda\right]=\] \[\mathbb{E}\left[\int_{0}^{1}\|\nabla_{x}J(.,u_{a}^{\prime}, \theta)\|_{\text{Lip}}\|u_{x}^{\prime}-u_{x}\|_{e}^{2}d\lambda\right]=\mathbb{ E}\left[\frac{1}{2}\|\nabla_{x}J(.,u_{a}^{\prime},\theta)\|_{\text{Lip}}\|u_{x}^{ \prime}-u_{x}\|_{e}^{2}\right]\leq\] \[\frac{C}{2}\|\nabla_{x}J(.,u_{a}^{\prime},\theta)\|_{\text{Lip}} \mathbb{E}\left[\|u_{x}^{\prime}-u_{x}\|^{2}\right]\leq\frac{CL}{2}\mathbb{E} \left[\|u_{x}^{\prime}-u_{x}\|^{2}\right]\leq O(\delta^{2}),\]

where \(C\) is a constant that arises from the equivalence of norms in \(\mathbb{R}^{d}\), it means that there exists \(C\,\|.\|_{e}\leq C\,\|.\|\). The inequality \(\mathbb{E}\left[\|u_{x}^{\prime}-u_{x}\|^{2}\right]\leq\delta^{2}\) is valid because by definition \(\mathbb{Q}_{x}\in\mathbb{B}_{\delta}((g_{\#}\mathbb{P}_{N})_{\mathcal{X}})\) and the cost function in the space \(\mathcal{U}_{\mathcal{X}}\) is expressed by the \(\|u_{x}^{\prime}-u_{x}\|\) so by definition of \(\mathbb{B}_{\delta}((g_{\#}\mathbb{P}_{N})_{\mathcal{X}})\), for \(p\geq 2\) by applying Jensen's inequality we have

\[\mathbb{Q}_{x}\in\mathbb{B}_{\delta}((g_{\#}\mathbb{P}_{N})_{ \mathcal{X}})\Rightarrow \mathbb{E}_{\mathbb{Q}_{x}}\left[\|u_{x}^{\prime}-u_{x}\|^{p} \right]^{\frac{1}{p}}\leq\delta\Rightarrow\] \[\mathbb{E}_{\mathbb{Q}_{x}}\left[\|u_{x}^{\prime}-u_{x}\|^{2} \right]\leq\mathbb{E}_{\mathbb{Q}_{x}}\left[\|u_{x}^{\prime}-u_{x}\|^{p} \right]^{\frac{2}{p}}\leq\delta^{2}.\]

Since by assumption \(\nabla J_{x}\) is uniformly Lipschitz for different value of \(\theta\) and \(u_{a}\) therefore we have:

\[\mathcal{R}_{\delta}(\mathbb{P}_{N})=\sup_{\begin{subarray}{c}\mathbb{Q}_{x}\in \mathbb{B}_{\delta}((g_{\#}\mathbb{P}_{N})_{\mathcal{X}})\\ \mathbb{Q}_{a}\in\mathcal{P}(\mathcal{A})\end{subarray}}\left\{\mathbb{E} \left[J(u_{x},u_{a}^{\prime},\theta)+\nabla_{x}J(u_{x},u_{a}^{\prime},\theta) \cdot(u_{x}^{\prime}-u_{x})\right]\right\}+O(\delta^{2}),\]for \(O(\delta^{2})\) independent of \(\theta\) and \(u_{a}\). The first expression of the above equation has the simple form:

\[\sup_{\mathbb{Q}_{a}\in\mathcal{P}(\mathcal{U}_{A})}\left\{\underset{ u_{a}\sim(g_{\theta}\mathbb{P}_{N})_{\mathcal{X}}}{\mathbb{E}}\left[J(u_{x},u_{a}^{ \prime},\theta)\right]\right\}=\underset{u_{a}\sim(g_{\theta}\mathbb{P}_{N})_{ \mathcal{X}}}{\mathbb{E}}\left[\sup_{\mathbb{Q}_{a}\in\mathcal{P}(\mathcal{U}_ {A})}\left\{\underset{u_{a}^{\prime}\sim\mathbb{Q}_{a}}{\mathbb{E}}\left[J(u_{ x},u_{a}^{\prime},\theta)\right]\right\}\right]=\] \[\underset{u_{x}\sim(g_{\theta}\mathbb{P}_{N})_{\mathcal{X}}}{ \mathbb{E}}\left[\sup_{u_{a}^{\prime}\in\mathcal{U}_{A}}\{J(u_{x},u_{a}^{ \prime},\theta)\}\right]=\underset{u_{x}\sim(g_{\theta}\mathbb{P}_{N})_{ \mathcal{X}}}{\mathbb{E}}\left[\sup_{u_{a}^{\prime}\in\mathcal{U}_{A}}\left\{ \ell(g^{-1}((u_{x},u_{a}^{\prime})),y,\theta)\right\}\right]=\] \[\underset{v\sim\mathbb{P}_{N}}{\mathbb{E}}\left[\sup_{a\in \mathcal{A}}\ell(\tilde{v}_{a},y,\theta)\right]\]

The only term in the above equation that still depends on the \(\mathbb{Q}\) is that term \(\nabla_{x}J(u_{x},u_{a}^{\prime},\theta)\cdot(u_{x}^{\prime}-u_{x})\). To remove this term we use extended Holder inequality with the expectation that can be expressed using the following formula:

\[\mathbb{E}[|XY|]\leq(\mathbb{E}[|X|^{p}])^{\frac{1}{p}}(\mathbb{E}[|Y|^{q}])^{ \frac{1}{q}},\]

and equality holds if and only if there exist constants \(c\geq 0\) such that:

\[|Y|=c|X|^{\frac{p}{q}}\quad\text{almost surely},\quad c\geq 0.\]

. By using Holder inequality, with the same reasoning we have:

\[\sup_{\begin{subarray}{c}\mathbb{Q}_{a}\in\mathbb{B}_{\delta}(g_{ \theta}\mathbb{P}_{N})_{\mathcal{X}})\\ \mathbb{Q}_{a}\in\mathcal{P}(\mathcal{A})\end{subarray}}\left\{\underset{u_{a} ^{\prime}\sim\mathbb{Q}_{a}}{\mathbb{E}}[\sup_{u_{a}^{\prime}\sim\mathbb{Q}_{ a}}[\nabla_{x}J(u_{x},u_{a}^{\prime},\theta)\cdot(u_{x}^{\prime}-u_{x})] \right\}=\] \[\left(\underset{u_{x}\sim(g_{\theta}\mathbb{P}_{N})_{\mathcal{X} }}{\mathbb{E}}[\sup_{u_{a}^{\prime}\in\mathcal{U}_{A}}\{\|\nabla_{x}J(u_{x},u_ {a}^{\prime},\theta)\|_{*}^{q}\}]\right)^{\frac{1}{q}}\sup_{\mathbb{Q}_{x}\in \mathbb{B}_{\delta}((g_{\theta}\mathbb{P}_{N})_{\mathcal{X}})}\left\{\underset{ u_{x}^{\prime}\sim\mathbb{Q}_{x}}{\mathbb{E}}\left[\|u_{x}^{\prime}-u_{x}\|^{p} \right]\right\}\] \[=\delta\left(\underset{u_{x}\sim(g_{\theta}\mathbb{P}_{N})_{ \mathcal{X}}}{\mathbb{E}}[\sup_{u_{a}^{\prime}\in\mathcal{U}_{A}}\{\|\nabla_{x }J(u_{x},u_{a}^{\prime},\theta)\|_{*}^{q}\}]\right)^{\frac{1}{p}}\]

where equality can be attained whenever \(1\leq p\leq\infty\) for a proper choice of \(u_{x}^{\prime}-u_{x}\) with \(\left(\mathbb{E}[\|u_{x}^{\prime}-u_{x}\|^{p}]\right)^{1/p}=\delta\). Therefore,

\[\mathcal{R}_{\delta}(\mathbb{P}_{N})=\underset{v\sim\mathbb{P}_{N}}{\mathbb{ E}}\left[\sup_{a\in\mathcal{A}}\ell(\tilde{v}_{a},y,\theta)\right]+\delta\left( \underset{v\sim\mathbb{P}_{N}}{\mathbb{E}}[\sup_{a\in\mathcal{A}}\{\|\nabla^{ \alpha}\ell(\tilde{v},y,\theta)\|_{*}^{q}\}]\right)^{1/q}+O(\delta^{2})\]

where

\[\nabla^{\alpha}\ell(v,y,\theta)=\underset{\Delta\to 0}{\lim}\frac{\ell( \mathbf{C}\mathbf{F}_{0}(v,\Delta))-f(v)}{\|\Delta\|}\]

the last equation completes the proofs.

### Proof of Proposition 2.

By Eq. 16 we have:

\[\mathcal{R}_{\delta}(\mathbb{P})=\underset{\lambda\geq 0}{\inf}\left\{ \lambda\delta+\underset{u_{x}\sim(g_{\theta}\mathbb{P}_{N})_{\mathcal{X}}}{ \mathbb{E}}\left[\tilde{\ell}_{\lambda}(u_{x})\right]\right\}\]

where, \(\tilde{\ell}(u_{x})=\sup_{u_{a}\in\mathcal{U}_{A}}\left\{\ell(M(u_{a},u_{x}^{ \prime}))\text{ and }\eta_{\lambda}(u_{x})=\sup_{\Delta\in\mathcal{U}_{A}}\tilde{\ell}(u_{x}+ \Delta)-\lambda d_{\mathcal{X}}(u_{x},u_{x}+\Delta)\right\}\). To prove we use Corollary 2[28] for the \(\tilde{\ell}\). First, we need to show that \(\tilde{\ell}\) satisfies the condition of Corollary 2[28]. By assumption for \(v^{\prime}\in\mathcal{V}\), \(L,M\geq 0\) such that \(\ell(v^{\prime},y,\theta)|<Ld^{p}(v,v^{\prime})+M\quad\text{for all}\quad v\in \mathcal{V}\) and \(p\in[1,\infty)\). By setting \(v=g^{-1}((u_{a},u_{a}))\) and \(v^{\prime}=g^{-1}((u^{\prime}_{a},u^{\prime}_{a}))\), and for simplicity \(\ell(v)=\ell(v,y,\theta)\) we have:

\[|\ell(g^{-1}((u_{a},u_{x})))-\ell(g^{-1}((u^{\prime}_{a},u^{\prime}_{x})))|< L\left\|u_{x}-u^{\prime}_{x}\right\|^{p}+M,\quad\forall u_{x}\in\mathcal{U}_{ \mathcal{X}},\;u_{a}\in\mathcal{U}_{\mathcal{A}}\Rightarrow\] \[|\tilde{\ell}(u_{x})-\tilde{\ell}(u^{\prime}_{x})|\leq L\left\|u _{x}-u^{\prime}_{x}\right\|^{p}+M\]

where \(\tilde{\ell}(u_{x})=sup_{u_{a}\in\mathcal{U}_{\mathcal{A}}}\ell(g^{-1}((u_{a}, u_{x})))\). This equation implies that \(\tilde{\ell}\) satisfies the condition of corollary 2. So in consequence of corollary 2 and the equation 16 if we define uncertainty set:

\[\tilde{B}_{\delta}=\left\{(\omega^{ik}_{x})_{i,k}:\frac{1}{N}\sum_{i=1}^{N} \sum_{k=1}^{K}\left\|u^{i}_{x}-\omega^{ik}\right\|\leq\delta,\,\omega^{ik}\in \mathcal{U}_{\mathcal{X}}\right\}\]

Since the casual fair metric \(d\) and loss function \(\tilde{\ell}\) do not depend on the sensitive part then \(\tilde{B}_{\delta}\) is equivalent to the below uncertainty set.

\[B_{\delta}=\left\{(w^{ik})_{i,k}:\frac{1}{N}\sum_{i=1}^{N}\sum_{k=1}^{K}d^{p}( v_{i},w^{ik})\leq\delta,\,w^{ik}\in\mathcal{V}\right\}.\]

By applying the uncertainty \(\tilde{B}_{\delta}\) for \(\tilde{\ell}\), the robust optimization problem has a form:

\[\tilde{\mathcal{R}}^{adv}_{\delta}(\mathbb{P}_{N})=\] \[\sup_{(\omega^{ik})_{i,k}\in\tilde{B}_{\delta}}\left\{\frac{1}{NK} \sum_{i=1}^{N}\sum_{k=1}^{K}\tilde{\ell}(\omega^{ik})\right\}=\sup_{(\omega^{ ik})_{i,k}\in\tilde{B}_{\delta}}\left\{\frac{1}{NK}\sum_{i=1}^{N}\sum_{k=1}^{K} \max_{u_{a}\in\mathcal{U}_{\mathcal{A}}}\ell(g^{-1}((u_{a},\omega^{ik}))) \right\}=\] \[\sup_{(w^{ik})_{i,k}\in B_{\delta}}\left\{\frac{1}{NK}\sum_{i=1}^{ N}\sum_{k=1}^{K}\max_{a\in\mathcal{A}}\ell(\tilde{w}^{ik}_{a}))\right\}\]

and finally for \(\tilde{\mathcal{R}}^{adv}_{\delta}(\mathbb{P}_{N})\) we have:

\[\tilde{\mathcal{R}}^{adv}_{\delta}(\mathbb{P}_{N})\leq\mathcal{R}^{n}_{\delta }(\mathbb{P}_{N})\leq\tilde{\mathcal{R}}^{adv}_{\delta}(\mathbb{P}_{N})+\frac {LD+M}{NK},\]

and it completes the proof.

**Lemma 8**: _Assume that the cost functions \(c\) and \(\hat{c}\) satisfy \(|c(x,x^{\prime})-\hat{c}(x,x^{\prime})|<\alpha\) for all \(x,x^{\prime}\in\mathbb{R}^{n}\) and for some \(\alpha\geq 0\). Then, for any \(\lambda\geq 0\), the difference between the \(\lambda\)-conjugates of \(f\) with respect to \(c\) and \(\hat{c}\) is bounded by \(\lambda\alpha\):_

\[|f_{\lambda}(x)-\hat{f}_{\lambda}(x)|\leq\lambda\alpha\quad\text{for all }x\in\mathbb{R}^{n}.\]

**Proof.** To prove the proposition, we consider any \(x\in\mathbb{R}^{n}\) and examine the definitions of \(f_{\lambda}(x)\) and \(\hat{f}_{\lambda}(x)\). Begin by expressing the bounds on \(\hat{c}\):

\[\hat{c}(x,x^{\prime})\leq c(x,x^{\prime})+\alpha\quad\text{and}\quad\hat{c}(x,x^{\prime})\geq c(x,x^{\prime})-\alpha.\]

From these inequalities, for any \(x^{\prime}\in\mathbb{R}^{n}\),

\[f(x^{\prime})-\lambda\hat{c}(x,x^{\prime})\geq f(x^{\prime})- \lambda(c(x,x^{\prime})+\alpha) =f(x^{\prime})-\lambda c(x,x^{\prime})-\lambda\alpha,\] \[f(x^{\prime})-\lambda\hat{c}(x,x^{\prime})\leq f(x^{\prime})- \lambda(c(x,x^{\prime})-\alpha) =f(x^{\prime})-\lambda c(x,x^{\prime})+\lambda\alpha.\]

Taking the supremum over all \(x^{\prime}\) in the above expressions, we obtain:

\[\hat{f}_{\lambda}(x)\geq f_{\lambda}(x)-\lambda\alpha\quad\text{and}\quad \hat{f}_{\lambda}(x)\leq f_{\lambda}(x)+\lambda\alpha.\]

These two bounds together imply:

\[|f_{\lambda}(x)-\hat{f}_{\lambda}(x)|\leq\lambda\alpha.\]

Thus, the proof is complete, showing that the difference between the \(\lambda\)-conjugates of \(f\) is indeed bounded by \(\lambda\alpha\).

**Lemma 9**: _Let \(c,\hat{c}:\mathbb{R}^{n}\times\mathbb{R}^{n}\rightarrow\mathbb{R}\) be two functions such that \(|c(x,y)-\hat{c}(x,y)|<\alpha\) for all \(x,y\in\mathbb{R}^{n}\) and some \(\alpha>0\). For any real number \(p\geq 1\), the following inequality holds:_

\[|c(x,y)^{p}-\hat{c}(x,y)^{p}|\leq p\cdot M^{p-1}\cdot\alpha,\]

_where \(M\geq\max\{|c(x,y)|,|\hat{c}(x,y)|\}\) for all \(x,y\)._

**Proof.** Consider the functions \(c\) and \(\hat{c}\) and any \(x,y\in\mathbb{R}^{n}\). By the hypothesis, we have \(|c(x,y)-\hat{c}(x,y)|<\alpha\). To find a bound on the difference of their powers, apply the mean value theorem to the function \(f(t)=t^{p}\), which is differentiable over \(\mathbb{R}\) (or over \(\mathbb{R}^{+}\) if \(p\) is not an integer). The derivative of \(f\) is \(f^{\prime}(t)=pt^{p-1}\).

Since \(f\) is continuously differentiable, there exists some \(\xi\) between \(c(x,y)\) and \(\hat{c}(x,y)\) such that

\[f(c(x,y))-f(\hat{c}(x,y))=f^{\prime}(\xi)\cdot(c(x,y)-\hat{c}(x,y)).\]

Therefore,

\[|c(x,y)^{p}-\hat{c}(x,y)^{p}|=|p\xi^{p-1}(c(x,y)-\hat{c}(x,y))|.\]

Using the bound \(|c(x,y)-\hat{c}(x,y)|<\alpha\) and noting that \(\xi\) must be within the range of values between \(c(x,y)\) and \(\hat{c}(x,y)\), we have \(\xi^{p-1}\leq M^{p-1}\). Thus,

\[|c(x,y)^{p}-\hat{c}(x,y)^{p}|\leq pM^{p-1}|c(x,y)-\hat{c}(x,y)|<pM^{p-1}\alpha.\]

**Lemma 10** ([41]): _Fix some \(\mathbb{P}\in\mathcal{P}(\mathcal{Z})\), \(\theta\in\Theta\) and \(\lambda^{*}\geq 0\) via_

\[\lambda^{*}:=\operatorname*{argmin}_{\lambda\geq 0}\left\{\lambda\delta^{p}+ \mathbb{E}_{\mathbb{P}}[\ell_{\lambda}(\mathbf{Z},\theta)]\right\}.\]

_Then under \(\ell\) Lipschitz and \(\operatorname*{diam}\left(\mathcal{Z}\right)<\infty\) assumptions, we have \(\lambda^{*}\leq L\delta^{-(p-1)}\)._

**Proof.** First, note that:

\[\lambda^{*}\delta^{p}\leq\lambda^{*}\delta^{p}+\mathbb{E}_{\mathbb{P}}\left[ \sup_{z^{\prime}\in\mathcal{Z}}\{\ell(z^{\prime},\theta)-\ell(z,\theta)- \lambda^{*}c^{p}(z,z^{\prime})\}\right]=*,\]

since the left-hand side, is greater than the case where \(z^{\prime}=z\) so it is positive. By the optimality of \(\lambda^{*}\) in \(\ell\), the right-hand side can be further upper-bounded as follows for any \(\lambda\geq 0\):

\[* \leq\lambda\delta^{p}+\mathbb{E}_{\mathbb{P}}\left[\sup_{z^{\prime }\in\mathcal{Z}}\{\ell(z^{\prime},\theta)-\ell(z,\theta)-\lambda c^{p}(z,z^{ \prime})\}\right]\] \[\leq\lambda\delta^{p}+\mathbb{E}_{\mathbb{P}}\left[\sup_{z^{\prime }\in\mathcal{Z}}\{Lc(z,z^{\prime})-\lambda c^{p}(z,z^{\prime})\}\right]\] \[\leq\lambda\delta^{p}+\sup_{t\geq 0}\{Lt-\lambda t^{p}\},\]

using the Lipschitz property for the second line and setting \(t=c(z,z^{\prime})\) in the third line. If \(p=1\), by setting \(\lambda=L\), we obtain:

\[\lambda^{*}\delta\leq L\delta+\sup_{t\geq 0}\{Lt-Lt\}=L\delta,\]

which implies \(\lambda^{*}\leq L\). For \(p>1\), using the optimal value \(t=(L/p\lambda)^{1/(p-1)}\), we derive:

\[\lambda^{*}\delta^{p}\leq\lambda\delta^{p}+L^{\frac{p}{p-1}}p^{-\frac{p}{p-1} }(p-1)\lambda^{-\frac{1}{p-1}}.\]

Minimizing the right-hand side with \(\lambda=L/p\delta^{p-1}\) yields:

\[\lambda^{*}\delta^{p}\leq L\delta\Rightarrow\lambda^{*}\leq L\delta^{-(p-1)},\]

resulting in the stated bound on \(\lambda^{*}\).

**Lemma 11**: _If \(f:A\times\mathbb{R}^{n}\to\mathbb{R}\) is Lipschitz with respect to \(x\) uniformly in \(a\), i.e., there exists a constant \(L\) such that for all \(a\in A\) and for all \(x,y\in\mathbb{R}^{n}\),_

\[|f(a,x)-f(a,y)|\leq L\|x-y\|,\]

_then \(\sup_{a\in A}f(a,x)\) is also Lipschitz in \(x\)._

[MISSING_PAGE_FAIL:31]

If define \(\lambda_{*}:=\operatorname*{argif}_{\lambda\geq 0}\left\{\lambda\delta^{p}+ \mathbb{E}_{\mathbb{P}}\big{[}\ell_{\lambda}^{c}(\mathbf{Z},\theta)\big{]}\right\}\), similarly,

\[\hat{\mathcal{R}}_{\delta}(\mathbb{P}_{N},\theta_{*})-\mathcal{R}_{ \delta}(\mathbb{P}_{*},\theta_{*})\leq\mathbb{E}_{\mathbb{P}_{N}}\big{[}\ell_{ \lambda_{*}}^{c}(\mathbf{Z},\theta)\big{]}-\mathbb{E}_{\mathbb{P}_{*}}\big{[} \ell_{\lambda_{*}}^{c}(\mathbf{Z},\theta)\big{]}\leq\] \[\mathbb{E}_{\mathbb{P}_{N}}\big{[}\ell_{\lambda_{*}}^{c}(\mathbf{ Z},\theta)\big{]}-\mathbb{E}_{\mathbb{P}_{*}}\big{[}\ell_{\lambda_{*}}^{c}( \mathbf{Z},\theta)\big{]}+\lambda_{*}\ p\operatorname*{diam}\left(\mathcal{V} \right)^{p-1}\ M_{d}\ N^{-\eta}.\]

We need to estimate the \(\lambda_{N}\) and \(\lambda_{*}\). By using strong duality theorem by Eq. 16 we have:

\[\mathcal{R}_{\delta}(\mathbb{P})=\inf_{\lambda\geq 0}\left\{\ \lambda\delta^{p}+\mathbb{E}_{\mathbb{P}}\big{[}\ell_{\lambda}^{c}(\mathbf{Z },\theta)\big{]}\right\}\] (24)

So instead the solve problem for \(\ell\) is it sufficient to prove our result for \(\tilde{\ell}(u_{x})=\sup_{u_{a}\in\mathcal{U}_{\mathcal{A}}}\ell(g^{-1}((u_{a},u_{x}))\). At first, we show that \(\tilde{\ell}\) is Lipschitz on the space \(\mathcal{U}_{\mathcal{X}}\) concerning the norm \(\left\|.\right\|\). By assumption, for each \(a\in\mathcal{A}\) the function \(\ell(g^{-1}((u_{a},u_{x}))\) is also Lipschitz:

\[\left\|\ell(g^{-1}((u_{a},u_{x})),y,\theta)-\ell(g^{-1}((u_{a},u_ {x}+\Delta)),y,\theta)\right\|=\] \[\left\|\ell(\textbf{CF}(v,a),y,\theta)-\ell(\textbf{CF}(v,a, \Delta),y,\theta)\right\|\leq Ld(\textbf{CF}(v,a),\textbf{CF}(v,a,\Delta))=L \left\|\Delta\right\|\]

Now by using lemma 11 it can be concluded that the function \(\tilde{\ell}(u_{x})=\sup_{u_{a}\in\mathcal{U}_{\mathcal{A}}}\ell(g^{-1}((u_{a},u_{x}))\) also has Lipschitz property with constant \(L\). By Applying lemma 10 for equation 24 and \((g_{\#}\mathbb{P}_{N})_{\mathcal{X}}\), it can be seen \(\lambda_{N},\lambda_{*}\leq L\delta^{-(p-1)}\). Therefore until now, we have two inequalities:

\[\mathcal{R}_{\delta}(\mathbb{P}_{*},\theta_{*})-\hat{\mathcal{R} }_{\delta}(\mathbb{P}_{N},\theta_{*})\leq\mathbb{E}_{P_{*}}\big{[}\ell_{ \lambda_{N}}^{c}(\mathbf{Z},\theta)\big{]}-\mathbb{E}_{P_{N}}\big{[}\ell_{ \lambda_{N}}^{c}(\mathbf{Z},\theta)\big{]}+\lambda_{N}\ p\operatorname*{diam} \left(\mathcal{V}\right)^{p-1}\ M_{d}\ N^{-\eta},\] \[\hat{\mathcal{R}}_{\delta}(\mathbb{P}_{N},\theta_{*})-\mathcal{R }_{\delta}(\mathbb{P}_{*},\theta_{*})\leq\mathbb{E}_{\mathbb{P}_{N}}\big{[} \ell_{\lambda_{*}}^{c}(\mathbf{Z},\theta)\big{]}-\mathbb{E}_{\mathbb{P}_{*}} \big{[}\ell_{\lambda_{*}}^{c}(\mathbf{Z},\theta)\big{]}+\lambda_{*}\ p \operatorname*{diam}\left(\mathcal{V}\right)^{p-1}\ M_{d}\ N^{-\eta}\Rightarrow\] \[\left|\mathcal{R}_{\delta}(\mathbb{P}_{*},\theta_{*})-\hat{ \mathcal{R}}_{\delta}(\mathbb{P}_{N},\theta_{*})\right|\leq\sup_{f\in\mathcal{ C}^{c}}\big{|}\int_{\mathcal{Z}}f(z)d(\mathbb{P}_{N}-\mathbb{P}_{*})(z) \big{|}+L\delta^{1-p}\ p\operatorname*{diam}\left(\mathcal{V}\right)^{p-1}\ M_{d}\ N^{-\eta},\]

where \(\mathcal{L}^{c}=\{\ell_{\lambda}^{c}(\cdot,\theta):\lambda\in[0,L\delta^{1-p} ],\theta\in\Theta\}\) is the DR loss class.

In the remaining part of the proof, we estimate \(\sup_{f\in\mathcal{L}^{c}}\left|\int_{\mathcal{Z}}f(z)d(\mathbb{P}_{*}- \mathbb{P}_{N})(z)\right|\) using conventional methods from statistical learning theory. According to assumption 2, the functions within \(\mathcal{F}\) are limited as shown:

\[0\leq\ell_{\lambda}^{c}(v,\theta)\leq\sup_{v^{\prime}\in\mathcal{V}}\ell(v^{ \prime},y,\theta)-\lambda d(v,v^{\prime})\leq\sup_{v^{\prime}\in\mathcal{V}} \ell(v^{\prime},y,\theta)\leq M.\]

similar to the proof Theorem 3 [41], by utilizing the bounded-differences inequality and symmetrization, we derive that:

\[\sup_{f\in\mathcal{L}^{c}}\big{|}\int_{\mathcal{Z}}f(z)d(\mathbb{P}_{N}-\mathbb{ P}_{*})(z)\big{|}\leq 2\mathfrak{R}_{N}(\mathcal{L}^{c})+M\sqrt{\frac{\log\frac{2}{ \epsilon}}{2N}}\]

holds with a probability of at least \(1-\epsilon\), where \(\mathfrak{R}_{n}(\mathcal{L}^{c})\) represents the Rademacher complexity of \(\mathcal{L}^{c}\):

\[\mathfrak{R}_{\mathfrak{N}}(\mathcal{L}^{c})=\mathbb{E}\bigg{[}\sup_{f\in \mathcal{L}^{c}}\frac{1}{N}\sum_{i=1}^{N}\sigma_{i}f(Z_{i})\bigg{]}.\]

In the proof of Theorem 2[41], the authors has proved:

\[\mathfrak{R}_{N}(\mathcal{L}^{c})\leq\frac{24\mathfrak{C}(\mathcal{L})}{\sqrt{N}}+ \frac{24L.\text{diam}\left(\mathcal{V}\right)^{p}}{\sqrt{N}\delta^{p-1}}.\]

where \(\mathfrak{C}(\mathcal{L})\), the entropy integral of of loss class. By applying this result it can be written:

\[|\mathcal{R}_{\delta}(\mathbb{P}_{*},\theta_{*})-\hat{\mathcal{R}}_{\delta}( \mathbb{P}_{N},\theta_{*})|\leq M\sqrt{\frac{\log\frac{2}{\epsilon}}{2N}}+\frac{ 48\mathfrak{C}(\mathcal{L})}{\sqrt{n}}+\frac{48L.\text{diam}\left(\mathcal{V} \right)^{p}}{\sqrt{n}\delta^{p-1}}+L\delta^{1-p}\ p\operatorname*{diam}\left( \mathcal{V}\right)^{p-1}\ M_{d}\ N^{-\eta}\]

Since the prove does depend on value of \(\theta\), then \(|\mathcal{R}_{\delta}(\mathbb{P}_{*},\hat{\theta}_{N}^{\text{do}})-\hat{\mathcal{R} }_{\delta}(\mathbb{P}_{N},\hat{\theta}_{N}^{\text{do}})|\) also satisfies in the above inequality. By combining two terms with probability \(1-\epsilon\) we have,

\[\mathcal{R}_{\delta}(\mathbb{P}_{*},\hat{\theta}_{N}^{\text{do}})-\mathcal{R}_{ \delta}(\mathbb{P}_{*},\theta_{*})\leq M\sqrt{\frac{2\log\frac{2}{\epsilon}}{N}}+ \frac{96\mathfrak{C}(\mathcal{L})}{\sqrt{n}}+\frac{96L.\text{diam}\left( \mathcal{V}\right)^{p}}{\sqrt{n}\delta^{p-1}}+2L\delta^{1-p}\ p\operatorname*{ diam}\left(\mathcal{V}\right)^{p-1}\ M_{d}\ N^{-\eta}\]

Since by assumption, with probability \(1-\epsilon\) we have inequality

\[\forall z,z^{\prime}\in|c(z,z^{\prime})-\hat{c}(z,z^{\prime})|\leq M_{d}N^{-\eta}, \quad\eta>0\]

therefore by probability \(1-2\epsilon\) the main inequality is true and it completes the proof.

Numerical Analysis Supplementary

### Synthetic Data Models

The structural equations used to generate the SCMs in SS 5 are listed below. For the LIN SCM, we generate the protected feature \(\mathbf{A}\) and variables \(\mathbf{X}_{i}\) according to the following structural equations:

* linear SCM (LIN): \[\begin{cases}\mathbf{A}:=\mathbf{U}_{\mathbf{A}},&\mathbf{U}_{\mathbf{A}}\sim \mathcal{B}(0.5)\\ \mathbf{X}_{1}:=2\mathbf{A}+U_{1},&U_{1}\sim\mathcal{N}(0,1)\\ \mathbf{X}_{2}:=\mathbf{A}-\mathbf{X}_{1}+\mathbf{U}_{2},&\mathbf{U}_{2}\sim \mathcal{N}(0,1)\\ \mathbf{Y}\sim\mathcal{B}((1+exp(-(\mathbf{X}_{1}+\mathbf{X}_{2}))^{-1})\\ \end{cases}\]

Here, \(\mathcal{B}(p)\) represents Bernoulli random variables with probability \(p\), and \(\mathcal{N}(\mu,\sigma^{2})\) represents normal random variables with mean \(\mu\) and variance \(\sigma^{2}\). To generate the ground truth \(h(\mathbf{A},X_{1},X_{2})\), we use a linear model for the LIN method. In all the synthetic models considered, we treat \(\mathbf{A}\) as a binary-sensitive attribute.

### Real-World Data

In our research, we have utilized the Adult dataset [38] and the COMPAS dataset [65] for our experimental analysis. To employ these datasets, we initially constructed an SCM based on the causal graph proposed by Nabi et al. [49]. For the Adult dataset, we incorporate features such as **sex**, **age**, **native-country**, **marital-status**, **education-num**, **hours-per-week**, and consider gender as a sensitive attribute. In the case of the COMPAS dataset, the utilized features comprise **age**, **race**, **sex**, and **priors count**, which function as variables. Additionally, sex is considered a sensitive attribute.

For classification purposes, we apply data standardization before the learning process.

\[\mathbf{Adult}=\begin{cases}\mathbf{A}=\mathbf{U}_{A}&\mathbf{ Sex}\\ \mathbf{X}_{2}=\mathbf{U}_{2}&\mathbf{Age}\\ \mathbf{X}_{3}=\mathbf{U}_{3}&\mathbf{Country}\\ \mathbf{X}_{4}=\mathbf{U}_{4}&\mathbf{Marital Status}\\ \mathbf{X}_{5}=\beta_{51}\mathbf{X}_{1}+\beta_{52}\mathbf{X}_{2}+\beta_{53} \mathbf{X}_{3}+\beta_{54}\mathbf{X}_{4}+\mathbf{U}_{5}&\text{\bf Education Level}\\ \mathbf{X}_{6}=\beta_{61}\mathbf{X}_{1}+\beta_{62}\mathbf{X}_{2}+\beta_{63} \mathbf{X}_{3}+\beta_{64}\mathbf{X}_{4}+\beta_{65}\mathbf{X}_{5}+\mathbf{U}_{6 }&\text{\bf Hours per Week}\\ \end{cases}\]

\[\mathbf{COMPAS}=\begin{cases}X_{1}=U_{1}&\mathbf{ Sex}\\ X_{2}=U_{2}&\mathbf{Age}\\ X_{3}=U_{3}&\mathbf{Race}\\ X_{4}=\beta_{41}X_{1}+\beta_{42}X_{2}+\beta_{43}X_{3}+U_{4}&\text{\bf Priors Count}\\ \end{cases}\]

In these above equations, \(\beta_{ij}\) are the coefficients for the linear combinations of the \(X\) variables, and \(U_{i}\) are the exogenous variables.

### Training Methods

In our study, we utilize various training objectives to train decision-making classifiers, with loss function \(\ell(v)\). The training objectives are as follows:

* **Empirical Risk Minimization (ERM)**: This approach minimizes the expected risk concerning the classifier parameters \(\psi\), represented by \[\inf_{\theta\in\Theta}\mathbb{E}_{z\sim\mathbb{P}}[\ell(z,\theta)]\]* **Adversarial Learning (AL)**: This method trains the model to withstand or defend against adversarial perturbations, represented by \[\inf_{\theta\in\Theta}\left\{\mathbb{E}_{z\sim\mathbb{P}}\left[\sup_{\|\Delta\| \leq\delta}\ell(z+\Delta,\theta)\right]\right\}\]
* **ROSS**: Based on the work of Ross et al. [56], this method minimizes the expected risk along with an adversarial perturbation term, represented by \[\inf_{\theta\in\Theta}\left\{\mathbb{E}_{(v,y)\sim\mathbb{P}}\left[\ell(v,y, \theta)+\inf_{\|\Delta\|\leq\delta}\ell(v+\Delta,1,\theta)\right]\right\}\]
* **CDRO**: Our approach, as described in this paper, is formulated as follows: \[\inf_{\theta\in\Theta}\left\{\sup_{\mathbb{Q}\in\mathbb{B}_{\delta}(\mathbb{P} )}\mathbf{E}_{z\sim\mathbb{Q}}[\ell(z,\theta)]\right\}\]

For our loss function \(\ell\), we use the binary cross-entropy loss.

### Hyperparameter Tuning

The majority of the experimental setup is based on the work of Ehyaei et al. [22]. For each dataset and its respective label, we use a generalized linear model (GLM). Each training objective is applied to four different datasets, using 100 different random seeds. The optimization process is performed using the Adam optimizer with a learning rate of \(10^{-3}\) and a batch size of 100. After optimizing the benchmark time and considering the training rate, we set the number of epochs to 10 to ensure comparability in benchmarking.

### Metrics

To assess the performance of various training methods concerning accuracy, unfair area, counterfactual fairness, and adversarial robustness, we employ seven distinct metrics as outlined below:

* **Acc**: The accuracy of the classifier, is represented as a percentage.
* \(U_{\delta}\): The proportion of data points within the unfair area with a radius of \(\delta\). \[U_{\delta}:=\mathbb{P}\big{(}\{v\in\mathcal{V}:\quad\exists v^{\prime}\in \mathcal{V}\quad\mathrm{s.t.}\quad d(v,v^{\prime})\leq\delta\quad\wedge\quad h (v)\neq h(v^{\prime})\}\big{)}.\]
* \(R_{\delta}\): The fraction of data points that are vulnerable to adversarial perturbations within a radius of \(\delta\). This metric coincides with the unfair area in cases where no sensitive attribute is considered. \[R_{\delta}:=\mathbb{P}\big{(}\{v\in\mathcal{V}:\quad\exists\Delta\in\mathcal{ V}\quad\mathrm{s.t.}\quad d(v,\textbf{CF}(v,\Delta))\leq\delta\quad\wedge\quad h (v)\neq h(\textbf{CF}(v,\Delta))\}\big{)}.\]
* \(CF\): The percentage of data points that exhibit counterfactual unfairness. This metric aligns with the unfair area when the perturbation radius is zero. \[\mathrm{CF}:=\mathbb{P}\big{(}\{v\in\mathcal{V}:\quad\exists a\in\mathcal{A} \quad\mathrm{s.t.}\quad h(v)\neq h(\tilde{v}_{a})\}\big{)}.\]

### Additional Results

In this section, we present additional simulation results. CDRO performs well across all datasets except for the \(R_{\delta}\) measure in the Adult dataset, likely because the linear model does not fit the SCM well. Nevertheless, CDRO demonstrates robustness and counterfactual fairness, as shown in Table 1, making it the preferred model when balancing both accuracy and fairness.

## Broader Impact Statement

Our theoretical framework bridges adversarial robustness, distributional robustness, individual fairness, and causality, aligning with the core pillars of responsible AI. By demonstrating the connection between these areas, we aim to inspire further research at their intersection and contribute to the development of safer, more equitable AI models for society. This approach holds the promise of improving decision-making under uncertainty while ensuring fairness and mitigating the impact of adversarial perturbations.

However, we acknowledge several limitations and ethical implications inherent in our approach. While our method produces fair and robust predictions under specific conditions, it is important to note that it fundamentally relies on a machine learning model, which may inherit the same vulnerabilities as the original model in areas not explicitly addressed in this work such as multiplicity,

\begin{table}
\begin{tabular}{l c c c c c|c c c c c c|c c c c c c c} \hline \hline  & \multicolumn{10}{c}{**Real-World Data**} & \multicolumn{10}{c}{**Synthetic Data**} \\ \cline{2-13}  & \multicolumn{10}{c}{**Adult**} & \multicolumn{10}{c}{**COMPAS**} & \multicolumn{10}{c}{**LIN**} \\ \cline{2-13}
**Trainer** & Acc & \(U_{0.5}\) & \(U_{0.1}\) & CF & \(R_{.05}\) & \(R_{.01}\) & Acc & \(U_{0.5}\) & \(U_{0.1}\) & CF & \(R_{.05}\) & \(R_{.01}\) & Acc & \(U_{0.5}\) & \(U_{0.1}\) & CF & \(R_{.05}\) & \(R_{.01}\) \\ \hline \hline AL & 0.79 & 0.06 & 0.04 & 0.04 & 0.03 & 0.01 & 0.67 & 0.2 & 0.17 & 0.16 & 0.04 & 0.01 & 0.67 & 0.2 & 0.19 & 0.19 & 0.11 & 0.1 \\ CDRO & 0.73 & **0.04** & **0.01** & **0** & 0.04 & 0.01 & 0.66 & **0.05** & **0.02** & **0.01** & 0.04 & **0.01** & 0.66 & **0.04** & **0.03** & **0.03** & **0.02** & **0.02** \\ ERM & **0.79** & 0.05 & 0.02 & 0.02 & **0.03** & **0** & **0.68** & 0.26 & 0.23 & 0.22 & **0.04** & 0.01 & **0.69** & 0.4 & 0.39 & 0.38 & 0.15 & 0.13 \\ ROS & 0.78 & 0.06 & 0.04 & 0.03 & 0.03 & 0.01 & 0.67 & 0.33 & 0.3 & 0.3 & 0.04 & 0.01 & 0.69 & 0.44 & 0.43 & 0.43 & 0.18 & 0.17 \\ \hline \hline \end{tabular}
\end{table}
Table 1: The table presents the results of our numerical experiment, comparing various trainers based on their input sets in terms of accuracy (Acc, higher values are better), unfairness areas (\(U_{.05}\), lower values are better), unfairness areas (\(U_{.01}\), lower values are better), Counterfactual Unfair area (CF, lower values are better), the non-robust percentage concerning adversarial perturbation with radii \(0.05\) (\(\mathcal{R}_{.05}\), lower values are better), and the non-robust percentage concerning adversarial perturbation with radii \(0.01\) (\(\mathcal{R}_{.01}\), lower values are better). The top-performing techniques for each trainer, dataset, and metric are highlighted in bold. The findings demonstrate that CDRO excels in reducing unfair areas. The average standard deviation for CDRO is \(.029\), while for the other methods, it is \(.031\).

Figure 3: Displays the findings from our numerical experiment, assessing the performance of DRO across different models and datasets. (Left) Bar plot showing the comparison of models based on the unfair area percentage \(U(\delta)\)(lower values are better) at \(\Delta=.01\). (Right) Bar plot showing the comparison of models based on the robustness area percentage \(R(\delta)\) (lower values are better) at \(\Delta=.01\).

Figure 2: Displays the findings from our numerical experiment, assessing the performance of DRO across different models and datasets. (left) Counterfactual unfair area percentage (lower values are better). (right) Non-robust area performance of classifier (higher values are better) for \(\Delta=.05\).

privacy breaches, lack of explainability, and safety/security concerns. Additionally, our work operates under simplifying assumptions regarding the fairness notion. In real-world applications, fairness is a complex and context-dependent concept. Therefore, it is essential to define fairness carefully and consider multiple dimensions of fairness when applying our approach. We emphasize that this work is a proof of concept, and we strongly recommend involving diverse stakeholders, including ethicists, domain experts, and affected communities, before applying our approach to high-risk application domains. It's important for users to be aware of these limitations and potential biases that might not be fully addressed by our framework.

## Appendix D NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: Our claims in the abstract and the introduction match the body of the text, the provided proofs, and the reported experimental results.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We discuss the main limitations of our work in the conclusion.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: We provide general assumptions in the beginning, and all the lemmas and theorems have either a full proof or a general intuition in the main body of the paper, with the remainder of proofs in the appendix.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We mention the used datasets and models, the random seeds, and details about the data splits in the main body of the paper and the appendix. Information about libraries and used algorithms is also provided.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes]
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We mention the used datasets and models, the random seeds, and details about the data splits in the main body of the paper and the appendix. Information about libraries and used algorithms is also provided.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: We give detailed results in the form of tables (with mean and std), while we only plot the mean values to not overcrowd the figures and just report the standard deviation in the table caption.
8. **Experiments Compute Resources**Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [No] Justification: The experiments are hardware agnostic and use fairly small models and datasets.
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes]
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: We include a broader impact statement in the appendix.
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: Our paper does not create new data or models.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We properly credit the used datasets.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: We documented the code.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA]
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA]