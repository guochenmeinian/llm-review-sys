# Label Noise: Ignorance Is Bliss

 Yilun Zhu

EECS

University of Michigan

allanzhu@umich.edu &Jianxin Zhang

EECS

University of Michigan

jianxinz@umich.edu &Aditya Gangrade

ECE

Boston University

gangrade@bu.edu &Clayton Scott

EECS, Statistics

University of Michigan

clayscot@umich.edu

###### Abstract

We establish a new theoretical framework for learning under multi-class, instance-dependent label noise. This framework casts learning with label noise as a form of domain adaptation, in particular, domain adaptation under posterior drift. We introduce the concept of _relative signal strength_ (RSS), a pointwise measure that quantifies the transferability from noisy to clean posterior. Using RSS, we establish nearly matching upper and lower bounds on the excess risk. Our theoretical findings support the simple _Noise Ignorant Empirical Risk Minimization (NI-ERM)_ principle, which minimizes empirical risk while ignoring label noise. Finally, we translate this theoretical insight into practice: by using NI-ERM to fit a linear classifier on top of a self-supervised feature extractor, we achieve state-of-the-art performance on the CIFAR-N data challenge.

## 1 Introduction

The problem of classification with label noise can be stated in terms of random variables \(X\), \(Y\), and \(\widetilde{Y}\), where \(X\) is the feature vector, \(Y\in\{1,\ldots,K\}\) is the true label associated to \(X\), and \(\widetilde{Y}\in\{1,\ldots,K\}\) is a noisy version of \(Y\). The learner has access to i.i.d. realizations of \((X,\widetilde{Y})\), and the objective is to learn a classifier that optimizes the risk associated with \((X,Y)\).

In recent years, there has been a surge of interest in the challenging setting of instance (i.e., feature) dependent label noise, in which \(\widetilde{Y}\) can depend on both \(Y\) and \(X\). While several algorithms have been developed, there remains relatively little theory regarding algorithm performance and the fundamental limits of this learning paradigm.

This work develops a theoretical framework for learning under multi-class, instance-dependent label noise. Our framework hinges on the concept of _relative signal strength_, which is a point-wise measure of "noisiness" in a label noise problem. Using relative signal strength to charachterize the difficulty of a label noise problem, we establish nearly matching upper and lower bounds for excess risk. We further identify distributional assumptions that ensure that the lower and upper bounds tend to zero as the sample size \(n\) grows, implying that consistent learning is possible.

Surprisingly, _Noise Ignorant Empirical Risk Minimization (NI-ERM)_ principle, which conducts empirical risk minimization as if no label noise exists, is (nearly) minimax optimal. To translate this insight into practice, we use NI-ERM to fit a linear classifier on top of a self-supervised feature extractor, achieving state-of-the-art performance on the CIFAR-N data challenge.

Literature review

Theory and algorithms for classification with label noise are often based on different probabilistic models. Such models may be categorized according on how \(\widetilde{Y}\) depends on \(Y\) and \(X\). The simplest model is symmetric noise, where the distribution of \(\widetilde{Y}\) is independent of \(Y\) and \(X\)(Angluin and Laird, 1988). In this case, the probability that \(\widetilde{Y}=k\) is the same for all \(k\neq Y\), regardless of \(Y\) and \(X\). In this setting, it is easy to show that minimizing the noisy excess risk (associated to the 0/1 loss) implies minimizing the clean excess risk, a property known as _immunity_. When immunity holds, there is no need to modify the learning algorithm on account of noisy labels. In other words, the learner may be _ignorant_ of the label noise and still learn consistently.

A more general model is classification with label dependent noise, in which the distribution of \(\widetilde{Y}\) depends on \(Y\), but not \(X\). Many practical algorithms have been developed over the years, based on principles including data re-weighting (Liu and Tao, 2015), robust training (Han et al., 2018; Liu et al., 2020; Hu et al., 2020; Foret et al., 2021; Liu et al., 2022) and data cleaning (Brodley and Friedl, 1999; Northcutt et al., 2021). Consistent learning algorithms still exist, such as those based on loss correction (Natarajan et al., 2013; Patrini et al., 2017; Van Rooyen and Williamson, 2018; Liu and Guo, 2020; Zhang et al., 2022). These approaches assume knowledge of the noise transition probabilities, which can be estimated under some identifiability assumptions (Scott et al., 2013; Zhang et al., 2021b).

In the most general setting, that of instance dependent label noise, the distribution of \(\widetilde{Y}\) depends on both \(Y\) and \(X\). While algorithms are emerging (Cheng et al., 2021; Zhu et al., 2021; Wang et al., 2022; Yang et al., 2023), theory has primarily focused on the binary setting. Scott (2019) establishes immunity for a Neyman-Pearson-like performance criterion under a _posterior drift_ model, discussed in more detail below. Cannings et al. (2020) establish an upper bound for excess risk under the strong assumption that the optimal classifiers for the clean and noisy distributions are the same.

Closest to our work, Im and Grigas (2023) derive excess risk upper and lower bounds, and reach a similar conclusion, that noise-ignorant ERM attains the lower bound up to a constant factor. Our results, based on the new concept of relative signal strength, provide a more refined analysis.

Additional connections between our contributions and prior work are made throughout the paper.

## 3 Problem statement

_Notation_. \(\mathcal{X}\) denotes the feature space and \(\mathcal{Y}=\{1,2,\ldots,K\}\) denotes the label space, with \(K\in\mathbb{N}\). The \(K\)-simplex is \(\Delta^{K}:=\{p\in\mathbb{R}^{K}:\forall i,p_{i}\geq 0,\sum p_{i}=1\}\). A \(K\times K\) matrix is _row stochastic_ if all of its rows are in \(\Delta^{K}\). Denote the \(i\)-th element of a vector \(\bm{v}\) as \([\bm{v}]_{i}\), and the \((i,j)\)-th element of a matrix \(\bm{M}\) as \([\bm{M}]_{i,j}\).

In conventional multiclass classification, we observe training data \((X_{1},Y_{1}),\ldots,(X_{n},Y_{n})\) drawn i.i.d. from a joint distribution \(P_{XY}\). The marginal distribution of \(X\) is denoted by \(P_{X}\), and the _class posterior_ probabilities \(P_{Y|X=x}\) are captured by a \(K\)-simplex-valued vector \(\bm{\eta}:\mathcal{X}\to\Delta^{K}\), where the \(j\)-th component of the vector is \([\bm{\eta}(x)]_{j}=\mathbb{P}\left(Y=j\mid X=x\right)\). A classifier \(f:\mathcal{X}\to\mathcal{Y}\) maps an instance \(x\) to a class \(f(x)\in\mathcal{Y}\). Denote the risk of a classifier \(f\) with respect to distribution \(P_{XY}\) as \(R(f)=\mathbb{E}_{(X,Y)\sim P_{XY}}\left[\mathbbm{1}_{\{f(X)\neq Y\}}\right]\). The Bayes optimal classifier for \(P_{XY}\) is \(f^{*}(x)\in\operatorname*{arg\,max}\bm{\eta}(x)\). The Bayes risk, which is the minimum achievable risk, is denoted as \(R^{*}=R(f^{*})=\inf_{f}R(f)\).

We consider the setting where, instead of the true class label \(Y\), a noisy label \(\widetilde{Y}\) is observed. The training data \((X_{1},\widetilde{Y}_{1}),\ldots,(X_{n},\widetilde{Y}_{n})\) can be viewed as an i.i.d. sample drawn from a "noisy" distribution \(P_{X\widetilde{Y}}\). We define \(P_{\widetilde{Y}|X=x}\), \(\widetilde{\bm{\eta}}\), \(\widetilde{R}\) and \(\widetilde{f}^{*}\) analogously to the "clean" distribution \(P_{XY}\).

The goal of learning from label noise is to find a classifier that is able to minimize the "clean test error," that is, the risk \(R\) defined w.r.t. \(P_{XY}\), even though the learner has access to only corrupted training data \((X_{i},\widetilde{Y}_{i})\stackrel{{\text{i.i.d.}}}{{\sim}}P_{X \widetilde{Y}}\).

Noise transition perspective.Traditionally, label noise is modeled through the joint distribution of \((X,Y,\widetilde{Y})\). This joint distribution is governed by \(P_{X}\), the clean class posterior \(P_{Y|X}\), and a matrix-valued function

\[\bm{E}:\mathcal{X}\rightarrow\{\bm{M}\in\mathbb{R}^{K\times K}:\bm{M}\text{ is row stochastic}\},\]

known as the _noise transition matrix_. The \((i,j)\)-th entry of the matrix is defined as:

\[[\bm{E}(x)]_{i,j}=\mathbb{P}\left(\widetilde{Y}=j\mid Y=i,X=x\right).\]

This implies that the noisy and clean class posteriors are related by \(\widetilde{\bm{\eta}}(x)=\bm{E}(x)^{\top}\bm{\eta}(x)\), where \({}^{\top}\) denotes the matrix transpose.

Domain adaptation perspective.Alternatively, label noise learning can be framed as a domain adaptation problem. In this view, \(P_{X\widetilde{Y}}\) represents the source domain, and \(P_{XY}\) represents the target domain. The relationship between the two domains is characterized by "posterior drift," meaning that while the source and target share the same \(X\)-marginal, the class posteriors (i.e., the distribution of labels given \(X\)) may differ (Scott, 2019; Cai and Wei, 2021; Maity et al., 2023; Liu et al., 2024). Thus, a label noise problem can also be described by a triple \((P_{X},\bm{\eta},\widetilde{\bm{\eta}})\).

The two perspectives are equivalent, as discussed in Appendix A.1. In this work, we emphasize the domain adaptation perspective for Sections 4 and 5, and the noise transition perspective for Section 6.

## 4 Relative signal strength

To study label noise, we introduce the concept of _relative signal strength_ (RSS). This is a pointwise measure of how much "signal" (certainty about the label) is contained in the noisy distribution relative to the clean distribution. Previous work (Cannings et al., 2020; Cai and Wei, 2021) has examined a related concept within the context of binary classification, under the restriction that clean and noisy Bayes classifiers are identical. Our definition incorporates multi-class classification and relaxes the requirement that the clean and noisy Bayes classifiers agree.

**Definition 1** (Relative Signal Strength): _For any class probability vectors \(\bm{\eta},\widetilde{\bm{\eta}}\), define the relative signal strength (RSS) at \(x\in\mathcal{X}\) as_

\[\mathcal{M}(x;\bm{\eta},\widetilde{\bm{\eta}})=\min_{j\in\mathcal{Y}}\ \frac{\max_{i}[\widetilde{\bm{\eta}}(x)]_{i}-[ \widetilde{\bm{\eta}}(x)]_{j}}{\max_{i}[\bm{\eta}(x)]_{i}-[\bm{\eta}(x)]_{j}},\] (1)

_where \(0/0:=+\infty\). Furthermore, for \(\kappa\in[0,\infty)\), denote the set of points whose RSS exceeds \(\kappa\) as_

\[\mathcal{A}_{\kappa}(\bm{\eta},\widetilde{\bm{\eta}})=\left\{x\in\mathcal{X}: \mathcal{M}(x;\bm{\eta},\widetilde{\bm{\eta}})>\kappa\right\}.\]

\(\mathcal{M}(x;\bm{\eta},\widetilde{\bm{\eta}})\) is a point-wise measure of how much "signal" the noisy posterior contains about the clean posterior. To gain some intuition, first notice that if the noisy Bayes classifier predicts a different class than the clean Bayes classifier, the RSS is 0 by taking \(j=\arg\max\widetilde{\bm{\eta}}\) (assuming for simplicity that the \(\arg\max\) is a singleton set). Now suppose the clean and noisy Bayes classifiers _do_ make the same prediction at \(x\), say \(i^{*}\), and consider a fixed \(j\). If

\[\frac{[\widetilde{\bm{\eta}}(x)]_{i^{*}}-[\widetilde{\bm{\eta}}(x)]_{j}}{[ \widetilde{\bm{\eta}}(x)]_{i^{*}}-[\widetilde{\bm{\eta}}(x)]_{j}}\]

is small, it means that the clean Bayes classifier is relatively certain that \(j\) is not the correct clean label, while the noisy Bayes classifier is less certain that \(j\) is not the correct noisy label. Taking the minimum over \(j\) gives the relative signal strength at \(x\). As we formalize in the next section, a large RSS at \(x\) ensures that a small (pointwise) _noisy_ excess risk at \(x\) implies a small (pointwise) _clean_ excess risk. To gain more intuition, consider the following examples.

**Example 1**: _When \(\bm{\eta}(x)=[0\ \ 1\ \ 0]^{\top}\) and \(\widetilde{\bm{\eta}}(x)=[0.3\ \ 0.6\ \ 0.1]^{\top}\),_

\[\mathcal{M}(x;\bm{\eta},\widetilde{\bm{\eta}})=\min_{j\in\mathcal{Y}}\ \frac{\max_{i}[ \widetilde{\bm{\eta}}(x)]_{i}-[\widetilde{\bm{\eta}}(x)]_{j}}{\max_{i}[\bm{ \eta}(x)]_{i}-[\bm{\eta}(x)]_{j}}=\frac{[\widetilde{\bm{\eta}}(x)]_{2}-[ \widetilde{\bm{\eta}}(x)]_{1}}{[\bm{\eta}(x)]_{2}-[\bm{\eta}(x)]_{1}}=\frac{0. 6-0.3}{1-0}=0.3.\]

_Here, first of all, \(\arg\max\bm{\eta}=\arg\max\widetilde{\bm{\eta}}=2\), i.e., the clean and noisy Bayes classifier give the same prediction. What's more, \(\mathcal{M}(x;\bm{\eta},\widetilde{\bm{\eta}})<1\) because the clean Bayes classifier is absolutely certain about its prediction, while the noisy Bayes classifier is much less certain._

**Example 2**: _When \(\bm{\eta}(x)=[0\;\;1\;\;0]^{\top}\) and \(\widetilde{\bm{\eta}}(x)=[0\;\;0\;\;1]^{\top}\),_

\[\mathcal{M}(x;\bm{\eta},\widetilde{\bm{\eta}})=\min_{j\in\mathcal{Y}}\;\;\frac{ \max_{i}[\widetilde{\bm{\eta}}(x)]_{i}-[\widetilde{\bm{\eta}}(x)]_{j}}{\max_{ i}[\bm{\eta}(x)]_{i}-[\bm{\eta}(x)]_{j}}=\frac{[\widetilde{\bm{\eta}}(x)]_{3}-[ \widetilde{\bm{\eta}}(x)]_{3}}{[\bm{\eta}(x)]_{2}-[\bm{\eta}(x)]_{3}}=\frac{1- 1}{1-0}=0.\]

_The zero signal strength results from \(\widetilde{\bm{\eta}}\) and \(\bm{\eta}\) leading to different predictions about \(\arg\max\)._

**Example 3** (Comparison to KL divergence): _When \(\bm{\eta}(x)=[0.05\;\;0.7\;\;0.25]^{\top}\), and \(\widetilde{\bm{\eta}}^{(1)}(x)=[0.25\;\;0.7\;\;0.05]^{\top}\), \(\widetilde{\bm{\eta}}^{(2)}(x)=[0.1\;\;0.6\;\;0.3]^{\top}\),_

\[\frac{1}{\mathcal{D}_{\mathrm{KL}}\left(\bm{\eta}\,\big{\|}\, \widetilde{\bm{\eta}}^{(1)}\right)}<\frac{1}{\mathcal{D}_{\mathrm{KL}}\left( \bm{\eta}\,\big{\|}\,\widetilde{\bm{\eta}}^{(2)}\right)}\quad\text{while}\quad \mathcal{M}\left(x;\bm{\eta},\widetilde{\bm{\eta}}^{(1)}\right)>\mathcal{M} \left(x;\bm{\eta},\widetilde{\bm{\eta}}^{(2)}\right).\]

_Here, \(\widetilde{\bm{\eta}}^{(2)}\) is "closer" to \(\bm{\eta}\) in terms of KL divergence, but \(\widetilde{\bm{\eta}}^{(1)}\) provides more information in terms of predicting the \(\arg\max\) of \(\bm{\eta}\). There is no conflict: KL divergence considers the similarity between two (whole) distributions, while the task of classification only focuses on predicting the \(\arg\max\)._

_This also illustrates why our notion of RSS is better suited for the label noise problem than other general-purpose distance measures between distributions._

A desirable learning scenario would be if \(\mathcal{A}_{\kappa}(\bm{\eta},\widetilde{\bm{\eta}})=\mathcal{X}\) for some large \(\kappa\), indicating that the signal strength is big across the entire space. Unfortunately, this ideal situation is generally not achievable. To gain some insight, consider the following result, proved in Appendix A.2.1.

**Proposition 1**: \(\mathcal{A}_{0}(\bm{\eta},\widetilde{\bm{\eta}})=\big{\{}x\in\mathcal{X}:\arg \max\,\widetilde{\bm{\eta}}(x)\subseteq\arg\max\,\bm{\eta}(x)\big{\}}\)_._

If we assume that both \(\arg\max\) sets are singletons, this result indicates that \(\mathcal{A}_{0}\), the region with positive RSS, is the region where the true and noisy Bayes classifiers agree. Accordingly, \(\mathcal{X}\setminus\mathcal{A}_{0}\), the zero signal region, is the region where the clean and noisy Bayes decision rules differ. The "region of strong signal," \(\mathcal{A}_{\kappa}\), is a subset of \(\mathcal{A}_{0}\). Since the clean and noisy Bayes classifiers will typically disagree for at least some \(x\), \(\mathcal{A}_{0}\neq\mathcal{X}\) in general. We note that the strong assumption that \(\mathcal{A}_{0}=\mathcal{X}\) has been made in prior studies (Cannings et al., 2020; Cai and Wei, 2021). Our notion of RSS relaxes this assumption and provides a unified view.

### RSS in binary classification

We can express relative signal strength more explicitly in the binary setup. Let \(\eta(x):=[\bm{\eta}(x)]_{1}=\mathbb{P}\left(Y=1\mid X=x\right)\) and \(\widetilde{\eta}(x):=[\widetilde{\bm{\eta}}(x)]_{1}=\mathbb{P}\big{(}\widetilde {Y}=1\mid X=x\big{)}\). In standard binary classification, the _margin_(Tsybakov, 2004; Massart and Nedelec, 2006), defined as \(\big{|}\eta(x)-\frac{1}{2}\big{|}\), serves as a pointwise measure of signal strength. Our notion of relative signal strength (RSS) can be interpreted as an extension of this concept in the context of label noise learning.

**Proposition 2**: _In the binary setting, for \(\kappa\geq 0\),_

\[\mathcal{M}(x;\eta,\widetilde{\eta})=\max\left\{\frac{\widetilde{\eta}(x)- \frac{1}{2}}{\eta(x)-\frac{1}{2}},\;0\right\},\qquad\text{and}\qquad\mathcal{ A}_{\kappa}(\eta,\widetilde{\eta})=\left\{x\in\mathcal{X}:\frac{\widetilde{\eta}(x)- \frac{1}{2}}{\eta(x)-\frac{1}{2}}>\kappa\right\}.\]

In other words, RSS can be viewed as a "relative" margin.

**Example 4**: _Illustration of relative signal strength in a binary classification setup (Figure 1)._

### Posterior Drift Model Class.

Now putting definitions together, we consider the posterior drift model \(\Pi\) defined over the triple \((P_{X},\bm{\eta},\widetilde{\bm{\eta}})\). Let \(\epsilon\in[0,1],\kappa\in[0,+\infty)\), and define

\[\Pi(\epsilon,\kappa):=\Big{\{}\left(P_{X},\bm{\eta},\widetilde{\bm{\eta}} \right):P_{X}\Big{(}\mathcal{A}_{\kappa}\left(\bm{\eta},\widetilde{\bm{\eta}} \right)\Big{)}\geq 1-\epsilon\Big{\}}.\]

This is a set of triples (label noise problems) such that \(\mathcal{A}_{\kappa}\), the region with RSS at least \(\kappa\), covers at least \(1-\epsilon\) of the probability mass. In the next section, we will demonstrate that the performance within \(\mathcal{A}_{\kappa}\) can be guaranteed, whereas learning outside the region \(\mathcal{A}_{\kappa}\) is provably challenging.

## 5 Upper and lower bounds

In this section, we establish both upper and lower bounds for excess risk under multi-class instance-dependent label noise.

### Minimax lower bound

Our first theorem reveals a fundamental limit: no classifier trained using noisy data can surpass the constraints imposed by relative signal strength in a minimax sense. To state the theorem, we employ the following notation and terminology. Denote the noisy training data by \(Z^{n}=\left\{(X_{i},\widetilde{Y}_{i})\right\}_{i=1}^{n}\overset{i.i.d.}{ \sim}P_{X\widetilde{Y}}\). A _learning rule_\(\hat{f}\) is an algorithm that takes \(Z^{n}\) and outputs a classifier. The risk \(R(\hat{f})\) of a learning rule is a random variable, where the randomness is due to the draw \(Z^{n}\).

**Theorem 1** (Minimax Lower Bound): _Let \(\epsilon\in[0,1],\kappa>0\). Then_

\[\inf_{\hat{f}}\sup_{(P_{X},\bm{\eta},\widetilde{\bm{\eta}})\in\Pi(\epsilon, \kappa)}\mathbb{E}_{Z^{n}}\left[R\left(\hat{f}\right)-R(f^{*})\right]\geq\frac {K-1}{K}\epsilon+\Omega\left(\frac{1}{\kappa}\,\sqrt{\frac{1}{n}}\right),\]

_where the \(\inf\) is over all learning rules._

The proof in Appendix A.2.3 offers insights into how label noise impacts the learning process: On the low RSS region \((\mathcal{X}\backslash\mathcal{A}_{\kappa})\), learning is difficult if not impossible, and the learner incurs an irreducible error of \((1-1/K)\epsilon\). On the high RSS region (\(\mathcal{A}_{\kappa}\)), the standard nonparametric rate [Devroye et al., 1996] is scaled by \(1/\kappa\). These aspects determine fundamental limits that no classifier trained only on noisy data can overcome without additional assumptions.

### Upper bound

This subsection establishes an upper bound for _Noise Ignorant Empirical Risk Minimizer (NI-ERM)_, the empirical risk minimizer trained on noisy data. This result implies that NI-ERM is (nearly) minimax optimal, a potentially surprising result given that NI-ERM is arguably the simplest approach one might consider. We begin by presenting a general result on the excess risk of any classifier, which is proved in Appendix A.2.4.

**Lemma 1** (Oracle Inequality): _For any label noise problem \((P_{X},\bm{\eta},\widetilde{\bm{\eta}})\) and any classifier \(f\),_

\[R(f)-R(f^{*})\leq\inf_{\kappa>0}\left\{P_{X}\Big{(}\mathcal{X}\setminus\mathcal{ A}_{\kappa}\left(\bm{\eta},\widetilde{\bm{\eta}}\right)\Big{)}+\frac{1}{\kappa} \left(\widetilde{R}(f)-\widetilde{R}\left(\tilde{f}^{*}\right)\right)\right\}.\]

For \((P_{X},\bm{\eta},\widetilde{\bm{\eta}})\in\Pi(\epsilon,\kappa)\), the first term is bounded by \(\epsilon\). When \(f\) is selected by ERM over the noisy training data, conventional learning theory implies a bound on the second term. This leads to the following upper bound for NI-ERM, whose proof is in Appendix A.2.5.

**Theorem 2** (Excess Risk Upper Bound of NI-ERM): _Let \(\epsilon\ \in\ [0,1],\kappa\ >0\). Consider any \((P_{X},\bm{\eta},\widetilde{\bm{\eta}})\in\Pi(\epsilon,\kappa)\), assume function class \(\mathcal{F}\) has Natarajan dimension \(V\), and the noisy Bayes classifier \(\tilde{f}^{*}\) belongs to \(\mathcal{F}\). Let \(\hat{f}\in\mathcal{F}\) be the ERM trained on \(Z^{n}=\left\{(X_{i},\widetilde{Y}_{i})\right\}_{i=1}^{n}\), i.e., \(\hat{f}=\operatorname*{arg\,min}\limits_{f\in\mathcal{F}}\frac{1}{n}\sum_{i =1}^{n}\mathbbm{1}_{\left\{f(X_{i})\neq\widetilde{Y}_{i}\right\}}\). Then_

\[\mathbb{E}_{Z^{n}}\left[R\left(\hat{f}\right)-R(f^{*})\right]\leq\epsilon+ \tilde{\mathcal{O}}\left(\frac{1}{\kappa}\sqrt{\frac{V}{n}}\right).\]

\(\tilde{\mathcal{O}}\) denotes big-\(\mathcal{O}\) notation ignoring logarithmic factors. The Natarajan dimension is a multiclass analogue of the VC dimension. The upper bound in Theorem 2 aligns with the minimax lower bound (Theorem 1) in both terms. For the irreducible error \(\epsilon\), there is a small gap of \(1/K\). This gap arises because, in the lower bound construction, the low signal region \(\mathcal{X}\setminus\mathcal{A}_{\kappa}\) is known to the learner, whereas knowledge of \(\mathcal{X}\setminus\mathcal{A}_{\kappa}\) is not provided to NI-ERM. If \(\mathcal{A}_{\kappa}\) were known to the learner (an unrealistic assumption), then a mixed strategy that preforms NI-ERM on \(\mathcal{A}_{\kappa}\) and randomly guesses on \(\mathcal{X}\setminus\mathcal{A}_{\kappa}\) would have an upper bound with first term of \((1-1/K)\epsilon\), exactly matching the lower bound. Regarding the second term, there is a universal constant and a logarithmic factor between the lower and upper bounds, which is a standard outcome in learning theory.

This result is surprising as it indicates that the simplest possible approach, which ignores the presence of noise, is nearly optimal. No learning rule could perform significantly better in this minimax sense.

### A smooth margin-condition on the relative signal strength

The previous sections have analyzed learning with label noise over the class \(\Pi(\epsilon,\kappa)=\{(P_{X},\bm{\eta},\tilde{\bm{\eta}}):P_{X}(\mathcal{A}_ {\kappa})\geq 1-\epsilon\}\). Now, the set \(\mathcal{X}\setminus\mathcal{A}_{\kappa}\) equals \((\mathcal{X}\setminus\mathcal{A}_{0})\cup(\mathcal{A}_{0}\setminus\mathcal{A}_ {\kappa})\). The first part of this decomposition is the region where the Bayes classifiers under the noisy and clean distributions differ, while the second is a region where these match, but the RSS is small. Naturally, while \(P_{X}(\mathcal{X}\setminus\mathcal{A}_{0})\) must be incurred as an irreducible error, one may question why the class \(\Pi\) also limits the mass of \(\mathcal{A}_{0}\setminus\mathcal{A}_{\kappa}\). After all, with enough data, the optimal prediction in this region can be learned.

This issue would be resolved if there existed a \(\kappa_{0}>0\) such that \(P_{X}(\mathcal{A}_{0})=P_{X}(\mathcal{A}_{\kappa_{0}})\), i.e., if \(P_{X}(\mathcal{A}_{0}\setminus\mathcal{A}_{\kappa_{0}})=0\). In fact, our lower bound from Theorem 1 uses precisely such a construction. An interesting point of comparison to this condition lies in Massart's hard-margin condition from standard supervised learning theory, which, for binary problems, demands that \(P_{X}(|[\bm{\eta}(x)]_{1}-[\bm{\eta}(x)]_{2}|<h)=0\) for some \(h>0\), under which one obtains minimax excess risk bounds of \(O^{(V/hh)}\)(Massart and Nedelec, 2006). With this lens, we can view the condition \(P_{X}(\mathcal{A}_{0}\setminus\mathcal{A}_{\kappa_{0}})=0\) as a type of _hard-margin condition on the relative signal strength \(\mathcal{M}\)_. This naturally motivates a smoothened version of this condition, inspired by Tsybakov's soft-margin condition (Tsybakov, 2004).

**Definition 2**: _A triple \((P_{X},\bm{\eta},\widetilde{\bm{\eta}})\) satisfies an \((\epsilon,\alpha,C_{\alpha})\)-smooth relative signal margin condition with \(\epsilon\in[0,1],\alpha>0,C_{\alpha}>0\) if_

\[\forall\kappa>0,\ P_{X}(\mathcal{M}(x;\bm{\eta},\widetilde{\bm{\eta}})\leq \kappa)\leq C_{\alpha}\kappa^{\alpha}+\epsilon.\]

_Further, we define \(\Pi^{\prime}(\epsilon,\alpha,C_{\alpha})\) as the set of triples \((P_{X},\bm{\eta},\widetilde{\bm{\eta}})\) that satisfy an \((\epsilon,\alpha,C_{\alpha})\)-smooth relative signal margin condition._

We show in Appendix A.2.7 that the techniques of Section 5.2 yield the following result for \(\Pi^{\prime}\).

**Theorem 3**: _Let \(\epsilon\in[0,1],\alpha>0,C_{\alpha}>0\). Consider any \((P_{X},\bm{\eta},\widetilde{\bm{\eta}})\in\Pi^{\prime}(\epsilon,\alpha,C_{\alpha})\), assume function class \(\mathcal{F}\) has Natarajan dimension \(V\), and the noisy Bayes classifier \(\tilde{f}^{*}\) belongs to \(\mathcal{F}\). Let \(\hat{f}\in\mathcal{F}\) be the ERM trained on \(Z^{n}=\left\{(X_{i},\tilde{Y}_{i})\right\}_{i=1}^{n}\). Then_

\[\mathbb{E}_{Z^{n}}\left[R\left(\hat{f}\right)-R\left(f^{*}\right)\right]\leq \epsilon+\inf_{\kappa>0}\left\{C_{\alpha}\kappa^{\alpha}+\tilde{\mathcal{O}} \left(\frac{1}{\kappa}\sqrt{\frac{V}{n}}\right)\right\}=\epsilon+\tilde{ \mathcal{O}}\left(n^{-\alpha/(2+2\alpha)}\right).\]

Compared to Theorem 2, we see that the rate of the second term is slightly slower, which is consistent with standard learning theory where Massart's hard margin assumption leads to faster rates than Tsybakov's. The advantage of the smooth relative margin is that the irreducible term in the above theorem is exactly \(P_{X}(\mathcal{X}^{{}^{\prime}}\setminus\mathcal{A}_{0})\), which has a clearer meaning as it measures the mismatch between clean and noisy Bayes classifiers. Further, notice that the NI-ERM algorithm does not need information about \(\alpha\), and thus the result is adaptive to both \(\alpha\), and to the optimal \(\kappa\) for each value of \(\alpha\), as a consequence of the oracle inequality of Lemma 1.

More broadly, Theorem 3 illustrates the flexibility of our conceptualization of label noise problems through RSS. The RSS \(\mathcal{M}\) characterizes the irreducible error in label noise learning, similar to how the regression function \(\bm{\eta}\) characterizes excess risk in standard learning. Thus, standard theoretical frameworks can be adapted to the noisy label problem via the _relative signal_.

## 6 Conditions that ensure noise immunity

The minimax lower bound in the previous section revealed a negative outcome, indicating that no method can do well in the low signal region. Nevertheless, numerous empirical successes have been observed even under significant label noise. This is not mere coincidence. In this section, we will illustrate that the high signal region \(\mathcal{A}_{\kappa}\) can indeed cover the entire input space \(\mathcal{X}\) even under massive label noise, albeit with the constraint \(\mathcal{A}_{\kappa}\subseteq\mathcal{A}_{0}\) as stated in Proposition 1. This not only explains past empirical successes, but also gives a rigorous condition on the consistency of NI-ERM.

This section will delve into the study of noise transition matrix \(\bm{E}\) and establish precise conditions that lead to \(\mathcal{A}_{0}=\mathcal{X}\). These conditions are linear algebraic conditions on \(\bm{E}\) that ensure \(\arg\max\widetilde{\bm{\eta}}(x)=\arg\max\bm{\eta}(x)\). As a result, we can infer that in a \(10\)-class classification problem, even with up to \(90\%\) of training labels being incorrect, the NI-ERM can still asymptotically achieve Bayes accuracy. In the upcoming definition, we introduce the concept of noise immunity, wherein the optimal classifiers remain unaffected by label noise [20, 17].

**Definition 3** (Immunity): _We say that a \(K\)-class classification problem \((P_{X},\bm{\eta},\widetilde{\bm{\eta}})\) is immune to label noise if \(\forall x\in\mathcal{X},\arg\max\widetilde{\bm{\eta}}(x)=\arg\max\bm{\eta}(x)\)._

Notice that due to Proposition 1, if a problem is immune, then \(\mathcal{A}_{0}=\mathcal{X}\). We now provide necessary and sufficient conditions on noise transition matrix \(\bm{E}\) that ensure noise immunity. We begin by considering distribution \(P_{XY}\) with zero Bayes risk, that is, where \(\bm{\eta}\) is one-hot almost surely. A matrix is defined as diagonally dominant if, for each row, the diagonal element is the unique maximum.

**Theorem 4** (Immunity for Zero-error Distribution): _If \(P_{XY}\) has Bayes risk of zero, then immunity holds if and only if for all \(x\), the noise transition matrix \(\bm{E}(x)\) is diagonally dominant._

**Remark** : _For a zero-error distribution \(P_{XY}\), even corrupted with instance-dependent label noise, achieving the Bayes risk is still feasible with a noise rate \(\mathbb{P}(\widetilde{Y}\neq Y)\) up to \(\frac{K-1}{K}\). This highlights that the task of classification itself is robust to label noise, specially when the clean \(\bm{\eta}\) is well-separated._

The above result relies on strong assumptions about the distribution \(P_{XY}\). Now, we present a result that applies to any distribution, which, as a trade-off, turns out to impose more requirements on \(\bm{E}\).

**Theorem 5** (Universal Immunity): _For any choice of \(P_{XY}\), immunity holds_

\[\Longleftrightarrow\quad\exists\;e(x)>0\;\text{s.t.}\;\forall x\in\mathcal{X },\;[\bm{E}(x)]_{i,j}=\begin{cases}\frac{1}{K}+e(x)&i=j\\ \frac{1}{K}-\frac{e(x)}{K-1}&i\neq j.\end{cases}\]Previous works (Ghosh and Kumar, 2017; Menon et al., 2018; Oyen et al., 2022) have established that symmetric label noise is sufficient for immunity. Our contribution advances this understanding by demonstrating that such noise conditions are not only sufficient but also necessary. Specifically, under symmetric label noise, learning towards the Bayes classifier is feasible as long as the proportion of wrong labels does not exceed \(\frac{K-1}{K}\). Furthermore, this transition is abrupt: when \(\mathbb{P}\big{(}\widetilde{Y}\neq Y\big{)}<\frac{K-1}{K}\), \(\mathcal{A}_{0}=\mathcal{X}\), but when \(\mathbb{P}\big{(}\widetilde{Y}\neq Y\big{)}\geq\frac{K-1}{K}\), \(\mathcal{A}_{0}=\emptyset\). Consequently, we expect to see a sudden drop in performance when noise rate passes the threshold.

The rationale behind the necessity of \(\bm{E}(x)\) taking this specific form is that it redistributes the probability mass of \(\bm{\eta}\) in a "uniform" manner. This constraint arises because \(\bm{E}(x)\) cannot favor any classes besides the true class. For instance, consider \(\bm{\eta}(x)=\big{[}\frac{1}{K}+\delta\ \frac{1}{K}-\delta\ \frac{1}{K}\cdots\frac{1}{K}\big{]}^{\top}\) for some small \(\delta>0\), a "non-uniform" \(\bm{E}(x)\) would alter the \(\arg\max\).

The above theorems demonstrate that signal strength at \(x\) can still be high even under massive label noise \(\mathbb{P}\big{(}\widetilde{Y}\neq Y\big{)}\), and, in essence, it is the discrete nature of the classification problem that allows robustness to label noise. When immunity holds, the irreducible error in Theorem 2 vanishes, therefore NI-ERM becomes a consistent learning rule. We validate this through data simulations presented in Figure 2, where we systematically flip labels uniformly and observe the corresponding changes in the testing accuracy of NI-ERM. The simulation results align closely with the theoretical expectations: NI-ERM achieves near-Bayes risk performance until a certain noise threshold is reached, beyond which the testing performance sharply deteriorates.

## 7 Practical implication

The modern practice of machine learning often involves training a deep neural network. In complex tasks involving noisy labels, the naive NI-ERM is often outperformed by state-of-the-art methods by a significant extent (Li et al., 2020; Xiao et al., 2023). This is consistent with the finding that directly training a large neural network on noisy data frequently leads to overfitting (Zhang et al., 2021).

Yet this is not grounds for abandoning NI-ERM altogether as a practical strategy. Instead of using NI-ERM for end-to-end training of a deep neural network, we instead propose the following simple, two-step procedure, termed 'feature extraction + NI-ERM'.

1. Perform feature extraction using any method (e.g., transfer learning or self-supervised learning) that does not require labels.
2. Learn a simple classifier (e.g., a linear classifier) on top of these extracted features, using the noisily labelled data, in a noise-ignorant way.

Figure 2: Data simulation that verifies noise immunity. For binary, the turning point is at noise rate \(\mathbb{P}\big{(}\widetilde{Y}\neq Y\big{)}=0.5\). For \(10\)-class, the turning point is at \(\mathbb{P}\big{(}\widetilde{Y}\neq Y\big{)}=0.9\).

This approach has three advantages over full network training. First, it avoids the potentially negative impact of the noisy labels on the extracted features. Second, it enjoys the inherent robustness of fitting a simple model (step 2) on noisy data, which we observed in Figure 2. Third, it avoids the need to tune hyperparameters of the feature extractor using noisy labels. We note that a "self-supervised + simple approach" to learning was previously studied by Bansal et al. (2021), although their focus was on generalisation properties without label noise. We also acknowledge that the practical idea of ignoring label noise is not new (Ghosh and Lan, 2021), but the full power of this approach has not been previously recognized. For example, prior works often combine this approach with additional steps or employ early stopping to mitigate the effects of noise (Zheltonozhskii et al., 2022; Xue et al., 2022).

Remarkably, this two-step approach attains extremely strong performance. We conducted experiments 1 on the CIFAR image data under two scenarios: synthetic label flipping (symmetric noise) and realistic human label errors (Wei et al., 2022), as shown in Figure 3. We examine three different feature extractors: the DINov2 foundation model (Oquab et al., 2023), ResNet-50 features extracted from training on ImageNet (He et al., 2016), and self-supervised ResNet-50 using contrastive loss (Chen et al., 2020). We also compared to a simple linear model trained on the raw pixel intensities, and a ResNet-50 trained end-to-end. We observed that ResNet-50 exhibits degrading performance with increasing noise, consistent with previous findings (Zhang et al., 2021; Mallinar et al., 2022). The linear model demonstrates robustness to noise, but suffers from significant approximation error.

Footnote 1: Code is available at: https://github.com/allan-z/label_noise_ignorance.

Conversely, the FE+NI-ERM approach enjoys the best of both worlds. Regardless of how the feature extraction is carried out, the resulting models exhibit robustness to label noise, while the overall accuracy depends entirely on the quality of the extracted features. This is illustrated in Figure 3, where the flatness of the accuracy curves as noise increases indicates the robustness, and the intercept at zero label noise is a measure of the feature quality. Importantly, this property holds true even under realistic label noise of CIFAR-N (Wei et al., 2022). In fact, we find that using the DINov2 (Oquab et al., 2023) extracted features in our FE+NI-ERM approach yields state of the art results on the CIFAR-10N and CIFAR-100N benchmarks, across the noise levels, as shown in Table 1.

We emphasize that the only hyperparameters of our model are the hyperparameters of the linear classifier, which are tuned automatically using standard cross-validation on the noisy labels. This contrasts to the implementation of many methods on the CIFAR-N leaderboard (http://noisylabels.com/)

Figure 3: A linear model trained on features obtained from either transfer learning (pretrained ResNet-50 on ImageNet (He et al., 2016) ), self-supervised learning (ResNet-50 trained on CIFAR-10 images with contrastive loss (Chen et al., 2020)), or a pretrained self-supervised foundation model DINov2 (Oquab et al., 2023) significantly boosts the performance of the original linear model. In contrast, full training of a ResNet-50 leads to overfitting.

2, where the hyperparameters are hard-coded. Furthermore, our approach does not rely on data augmentation. Additional experiments, detailed in Appendix A.4, include comparisons with the 'linear probing, then fine-tuning' approach (Kumar et al., 2022), the application of different robust learning strategies on DINov2 features, and results on synthetic instance-dependent label noise.

Footnote 2: If the link is inaccessible, see the May 23, 2024 archive captured by Wayback Machine: https://web.archive.org/web/20240523101740/http://noisylabels.com/.

Overall, the strong performance, the simplicity of the approach and the lack of any untunable hyperparameters highlights the effectiveness of FE+NI-ERM, and indicates the value of further investigation into its properties.

RSS for realistic human label error.To calculate the RSS under realistic human label error, we train two linear classifiers on DINov2 features under clean and noisy labels and use the models' predictions as estimates for the class probabilities \(\bm{\eta}\) and \(\widetilde{\bm{\eta}}\). Despite the high overall noise rate in CIFAR-10N "Worst" labels, with \(\mathbb{P}(Y\neq\widetilde{Y})=40.21\%\), we conjecture that the region where there is no signal, \(\mathcal{X}\setminus\mathcal{A}_{0}\), covers only a small portion of the probability mass (\(\epsilon\leq 4\%\)). Furthermore, the cumulative distribution of the estimated RSS can be upper-bounded by a polynomial \(C_{\alpha}\kappa^{\alpha}+\epsilon\), supporting the validity of the smooth relative signal margin condition introduced in Section 5.3.

## 8 Conclusions

This work presents a rigorous theory for learning under multi-class, instance-dependent label noise. We establish nearly matching upper and lower bounds for excess risk and identify precise conditions for classifier consistency. Our theory reveals the (nearly) minimax optimality of Noise Ignorant Empirical Risk Minimizer (NI-ERM). To make this theory practical, we provide a simple modification leveraging a feature extractor with NI-ERM, demonstrating significant performance enhancements. A limitation of this work is that our methodology warrants more extensive experimental evaluation.

\begin{table}
\begin{tabular}{c c c c c c c} \hline \hline \multirow{2}{*}{Leaderboard} & \multicolumn{5}{c}{CIFAR-10N} & CIFAR-100N \\ \cline{2-7}  & Aggre & Rand1 & Rand2 & Rand3 & Worst & Noisy \\ \hline ProMix & 97.65 \(\pm\) 0.19 & 97.39 \(\pm\) 0.16 & 97.55 \(\pm\) 0.12 & 97.52 \(\pm\) 0.09 & **96.34 \(\pm\) 0.23** & 73.79 \(\pm\) 0.28 \\ ILL & 96.40 \(\pm\) 0.03 & 96.06 \(\pm\) 0.07 & 95.98 \(\pm\) 0.12 & 96.10 \(\pm\) 0.05 & 93.55 \(\pm\) 0.14 & 68.07 \(\pm\) 0.33 \\ PLS & 96.09 \(\pm\) 0.09 & 95.86 \(\pm\) 0.26 & 95.96 \(\pm\) 0.16 & 96.10 \(\pm\) 0.07 & 93.78 \(\pm\) 0.30 & 73.25 \(\pm\) 0.12 \\ DivideMix & 95.01 \(\pm\) 0.71 & 95.16 \(\pm\) 0.19 & 95.23 \(\pm\) 0.07 & 95.21 \(\pm\) 0.14 & 92.56 \(\pm\) 0.42 & 71.13 \(\pm\) 0.48 \\ \hline FE + NI-ERM & **98.69 \(\pm\) 0.00** & **98.80 \(\pm\) 0.00** & **98.65 \(\pm\) 0.00** & **98.67 \(\pm\) 0.00** & 95.71 \(\pm\) 0.00 & **83.17 \(\pm\) 0.00** \\ \hline \hline \end{tabular}
\end{table}
Table 1: Performance comparison with CIFAR-N leaderboard (http://noisylabels.com/) in terms of testing accuracy. “Aggre”, “Rand1”,..., “Noisy” denote various types of human label noise. We compare with four methods that covers the top three performance for all noise categories: ProMix (Xiao et al., 2023), ILL (Chen et al., 2023), PLS (Albert et al., 2023) and DivideMix (Li et al., 2020). Our approach, a Noise Ignorant linear model trained on features extracted by the self-supervised foundation model DINov2 (Qquab et al., 2023) achieves new state-of-the-art results, highlighted in bold. We employed Python’s sklearn logistic regression and cross-validation functions without data augmentation; the results are deterministic and directly reproducible.

Figure 4: Empirical CDF of estimated RSS for CIFAR-10N, evaluated on test data.

## Acknowledgements

This work was supported in part by the National Science Foundation under award 2008074 and the Department of Defense, Defense Threat Reduction Agency under award HDTRA1-20-2-0002. The authors thank Zixuan Huang, Yihao Xue for helpful discussions and Raj Rao Nadakuditi for feedback during a course project in which some early experiments in this paper were conducted. We also thank the anonymous reviewers for their suggestions, especially the reviewer who provided Example 3.

## References

* Albert et al. (2023) Paul Albert, Eric Arazo, Tarun Krishna, Noel E O'Connor, and Kevin McGuinness. Is your noise correction noisy? pls: Robustness to label noise with two stage detection. In _Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision_, pages 118-127, 2023.
* Angluin and Laird (1988) Dana Angluin and Philip Laird. Learning from noisy examples. _Machine learning_, 2:343-370, 1988.
* Bansal et al. (2021) Yamini Bansal, Gal Kaplun, and Boaz Barak. For self-supervised learning, rationality implies generalization, provably. In _International Conference on Learning Representations_, 2021.
* Brodley and Friedl (1999) Carla E Brodley and Mark A Friedl. Identifying mislabeled training data. _Journal of artificial intelligence research_, 11:131-167, 1999.
* Cai and Wei (2021) T Tony Cai and Hongji Wei. Transfer learning for nonparametric classification: Minimax rate and adaptive classifier. _The Annals of Statistics_, 49(1):100-128, 2021.
* Cannings et al. (2020) Timothy I Cannings, Yingying Fan, and Richard J Samworth. Classification with imperfect training labels. _Biometrika_, 107(2):311-330, 2020.
* Chen et al. (2023) Hao Chen, Ankit Shah, Jindong Wang, Ran Tao, Yidong Wang, Xing Xie, Masashi Sugiyama, Rita Singh, and Bhiksha Raj. Imprecise label learning: A unified framework for learning with various imprecise label configurations. _arXiv preprint arXiv:2305.12715_, 2023.
* Chen et al. (2020) Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In _International conference on machine learning_, pages 1597-1607. PMLR, 2020.
* Cheng et al. (2021) Hao Cheng, Zhaowei Zhu, Xingyu Li, Yifei Gong, Xing Sun, and Yang Liu. Learning with instance-dependent label noise: A sample sieve approach. In _International Conference on Learning Representations_, 2021.
* Deng et al. (2009) Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In _2009 IEEE conference on computer vision and pattern recognition_, pages 248-255. IEEE, 2009.
* Devroye et al. (1996) Luc Devroye, Laszlo Gyorfi, and Gabor Lugosi. _A Probabilistic Theory of Pattern Recognition_, volume 31. Springer, 1996.
* Foret et al. (2021) Pierre Foret, Ariel Kleiner, Hossein Mobahi, and Behnam Neyshabur. Sharpness-aware minimization for efficiently improving generalization. In _International Conference on Learning Representations_, 2021. URL https://openreview.net/forum?id=6TmImposlrM.
* Ghosh and Kumar (2017) Aritra Ghosh and Himanshu Kumar. Robust loss functions under label noise for deep neural networks. In _Proceedings of the AAAI conference on artificial intelligence_, 2017.
* Ghosh and Lan (2021) Aritra Ghosh and Andrew Lan. Contrastive learning improves model robustness under label noise. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 2703-2708, 2021.
* Ghosh et al. (2015) Aritra Ghosh, Naresh Manwani, and PS Sastry. Making risk minimization tolerant to label noise. _Neurocomputing_, 160:93-107, 2015.
* Ghosh et al. (2016)Bo Han, Quanming Yao, Xingrui Yu, Gang Niu, Miao Xu, Weihua Hu, Ivor Tsang, and Masashi Sugiyama. Co-teaching: Robust training of deep neural networks with extremely noisy labels. _Advances in neural information processing systems_, 31, 2018.
* He et al. (2016) Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 770-778, 2016.
* Hu et al. (2020) Wei Hu, Zhiyuan Li, and Dingli Yu. Simple and effective regularization methods for training on noisily labeled data with generalization guarantee. In _International Conference on Learning Representations_, 2020.
* Im and Grigas (2023) Hyungki Im and Paul Grigas. Binary classification with instance and label dependent label noise. _arXiv preprint arXiv:2306.03402_, 2023.
* Kumar et al. (2022) Ananya Kumar, Aditi Raghunathan, Robbie Jones, Tengyu Ma, and Percy Liang. Fine-tuning can distort pretrained features and underperform out-of-distribution. In _International Conference on Learning Representations_, 2022.
* Li et al. (2020) Junnan Li, Richard Socher, and Steven CH Hoi. Dividemix: Learning with noisy labels as semi-supervised learning. In _International Conference on Learning Representations_, 2020.
* Liu et al. (2024) Jiashuo Liu, Tianyu Wang, Peng Cui, and Hongseok Namkoong. On the need for a language describing distribution shifts: Illustrations on tabular datasets. _Advances in Neural Information Processing Systems_, 36, 2024.
* Liu et al. (2020) Sheng Liu, Jonathan Niles-Weed, Narges Razavian, and Carlos Fernandez-Granda. Early-learning regularization prevents memorization of noisy labels. _Advances in neural information processing systems_, 33:20331-20342, 2020.
* Liu et al. (2022) Sheng Liu, Zhihui Zhu, Qing Qu, and Chong You. Robust training under label noise by over-parameterization. In _International Conference on Machine Learning_, pages 14153-14172. PMLR, 2022.
* Liu and Tao (2015) Tongliang Liu and Dacheng Tao. Classification with noisy labels by importance reweighting. _IEEE Transactions on pattern analysis and machine intelligence_, 38(3):447-461, 2015.
* Liu and Guo (2020) Yang Liu and Hongyi Guo. Peer loss functions: Learning from noisy labels without knowing noise rates. In _International conference on machine learning_, pages 6226-6236. PMLR, 2020.
* Ma and Fattahi (2022) Jianhao Ma and Salar Fattahi. Blessing of depth in linear regression: Deeper models have flatter landscape around the true solution. _Advances in Neural Information Processing Systems_, 35:3434-34346, 2022.
* Maity et al. (2023) Subha Maity, Diptavo Dutta, Jonathan Terhorst, Yuekai Sun, and Moulinath Banerjee. A linear adjustment based approach to posterior drift in transfer learning. _Biometrika_, 2023. URL https://doi.org/10.1093/biomet/asad029.
* Mallinar et al. (2022) Neil Mallinar, James Simon, Amirhesam Abedsoltan, Parthe Pandit, Misha Belkin, and Preetum Nakkiran. Benign, tempered, or catastrophic: Toward a refined taxonomy of overfitting. _Advances in Neural Information Processing Systems_, 35:1182-1195, 2022.
* Massart and Nedelec (2006) Pascal Massart and Elodie Nedelec. Risk bounds for statistical learning. _The Annals of Statistics_, 34(5):2326-2366, 2006.
* Menon et al. (2015) Aditya Menon, Brendan Van Rooyen, Cheng Soon Ong, and Bob Williamson. Learning from corrupted binary labels via class-probability estimation. In _International conference on machine learning_, pages 125-134. PMLR, 2015.
* Menon et al. (2018) Aditya Krishna Menon, Brendan Van Rooyen, and Nagarajan Natarajan. Learning from binary labels with instance-dependent noise. _Machine Learning_, 107:1561-1595, 2018.
* Natarajan (1989) Balas K Natarajan. On learning sets and functions. _Machine Learning_, 4:67-97, 1989.
* Nedelec et al. (2018)Nagarajan Natarajan, Inderjit S Dhillon, Pradeep K Ravikumar, and Ambuj Tewari. Learning with noisy labels. _Advances in neural information processing systems_, 26, 2013.
* Northcutt et al. (2021) Curtis Northcutt, Lu Jiang, and Isaac Chuang. Confident learning: Estimating uncertainty in dataset labels. _Journal of Artificial Intelligence Research_, 70:1373-1411, 2021.
* Oquab et al. (2023) Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy V Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel HAZIZA, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. _Transactions on Machine Learning Research_, 2023.
* Oyen et al. (2022) Diane Oyen, Michal Kucer, Nicolas Hengartner, and Har Simrat Singh. Robustness to label noise depends on the shape of the noise distribution. _Advances in Neural Information Processing Systems_, 35:35645-35656, 2022.
* Patrini et al. (2017) Giorgio Patrini, Alessandro Rozza, Aditya Krishna Menon, Richard Nock, and Lizhen Qu. Making deep neural networks robust to label noise: A loss correction approach. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 1944-1952, 2017.
* Scott (2019) Clayton Scott. A generalized Neyman-Pearson criterion for optimal domain adaptation. In _Algorithmic Learning Theory_, pages 738-761. PMLR, 2019.
* Scott et al. (2013) Clayton Scott, Gilles Blanchard, and Gregory Handy. Classification with asymmetric label noise: Consistency and maximal denoising. In _Conference on learning theory_, pages 489-511. PMLR, 2013.
* Tsybakov (2004) Alexander B Tsybakov. Optimal aggregation of classifiers in statistical learning. _The Annals of Statistics_, 32(1):135-166, 2004.
* Van Rooyen and Williamson (2018) Brendan Van Rooyen and Robert C Williamson. A theory of learning with corrupted labels. _Journal of Machine Learning Research_, 18(228):1-50, 2018.
* Vapnik and Chervonenkis (1971) V. N. Vapnik and A. Ya. Chervonenkis. On the uniform convergence of relative frequencies of events to their probabilities. _Theory of Probability & Its Applications_, 16(2):264-280, 1971. doi: 10.1137/1116025. URL https://doi.org/10.1137/1116025.
* Wang et al. (2022) Jialu Wang, Eric Xin Wang, and Yang Liu. Estimating instance-dependent label-noise transition matrix using a deep neural network. In _International Conference on Machine Learning_, 2022.
* Wei et al. (2022) Jiaheng Wei, Zhaowei Zhu, Hao Cheng, Tongliang Liu, Gang Niu, and Yang Liu. Learning with noisy labels revisited: A study using real-world human annotations. In _International Conference on Learning Representations_, 2022.
* Xia et al. (2020) Xiaobo Xia, Tongliang Liu, Bo Han, Nannan Wang, Mingming Gong, Haifeng Liu, Gang Niu, Dacheng Tao, and Masashi Sugiyama. Part-dependent label noise: Towards instance-dependent label noise. _Advances in Neural Information Processing Systems_, 33:7597-7610, 2020.
* Xiao et al. (2023) Ruixuan Xiao, Yiwen Dong, Haobo Wang, Lei Feng, Runze Wu, Gang Chen, and Junbo Zhao. Promix: Combating label noise via maximizing clean sample utility. In Edith Elkind, editor, _Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence, IJCAI-23_, pages 4442-4450. International Joint Conferences on Artificial Intelligence Organization, 8 2023. doi: 10.24963/ijcai.2023/494. URL https://doi.org/10.24963/ijcai.2023/494. Main Track.
* Xue et al. (2022) Yihao Xue, Kyle Whitecross, and Baharan Mirzasoleiman. Investigating why contrastive learning benefits robustness against label noise. In _International Conference on Machine Learning_, pages 24851-24871. PMLR, 2022.
* Yang et al. (2023) Shuo Yang, Songhua Wu, Erkun Yang, Bo Han, Yang Liu, Min Xu, Gang Niu, and Tongliang Liu. A parametrical model for instance-dependent label noise. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 2023.
* Zhang et al. (2021a) Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning (still) requires rethinking generalization. _Communications of the ACM_, 64(3):107-115, 2021a.

Jianxin Zhang, Yutong Wang, and Clayton Scott. Learning from label proportions by learning with label noise. _Advances in Neural Information Processing Systems_, 35:26933-26942, 2022.
* Zhang et al. (2021b) Mingyuan Zhang, Jane Lee, and Shivani Agarwal. Learning from noisy labels with no change to the training process. In _International Conference on Machine Learning_, pages 12468-12478. PMLR, 2021b.
* Zheltonozhskii et al. (2022) Evgenii Zheltonozhskii, Chaim Baskin, Avi Mendelson, Alex M Bronstein, and Or Litany. Contrast to divide: Self-supervised pre-training for learning with noisy labels. In _Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision_, pages 1657-1667, 2022.
* Zhu et al. (2021) Zhaowei Zhu, Tongliang Liu, and Yang Liu. A second-order approach to learning with instance-dependent label noise. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 10113-10123, 2021.

Appendix / supplemental material

### Equivalence of noise transition and domain adaptation perspectives

The noise transition perspective models the joint distribution of \((X,Y,\widetilde{Y})\), which can be characterized as:

\[P_{X,Y,\widetilde{Y}}=P_{X}\underbrace{P_{Y|X}}_{\boldsymbol{\eta}}\underbrace {P_{\widetilde{Y}|Y,X}}_{\boldsymbol{E}}\]

Thus, by specifying \(P_{X}\), \(\boldsymbol{\eta}\), and \(\boldsymbol{E}\), we obtain a triple \((P_{X},\boldsymbol{\eta},\widetilde{\boldsymbol{\eta}})\) with \(\widetilde{\boldsymbol{\eta}}(x)=\boldsymbol{E}(x)^{\top}\boldsymbol{\eta}(x)\).

In contrast, the domain adaptation perspective views label noise problems directly as a triple \((P_{X},\boldsymbol{\eta},\widetilde{\boldsymbol{\eta}})\), bypassing the explicit modeling of the noise transition matrix \(\boldsymbol{E}\).

If no assumptions are made about the form of \(\boldsymbol{E}\), the domain adaptation view remains fully expressive. Given a triple \((P_{X},\boldsymbol{\eta},\widetilde{\boldsymbol{\eta}})\), we can always define a noise transition matrix as:

\[\boldsymbol{E}(x)=\boldsymbol{1}\widetilde{\boldsymbol{\eta}}^{\top},\]

where \(\boldsymbol{1}=[1\ldots 1]^{\top}\). We can verify that \(\boldsymbol{E}\) is row-stochastic, and

\[\widetilde{\boldsymbol{\eta}}=\boldsymbol{E}(x)^{\top}\boldsymbol{\eta}=( \widetilde{\boldsymbol{\eta}}\boldsymbol{1}^{\top})\boldsymbol{\eta}= \widetilde{\boldsymbol{\eta}}(\boldsymbol{1}^{\top}\boldsymbol{\eta})= \widetilde{\boldsymbol{\eta}}.\]

Therefore, these two perspectives are equivalent.

### Proofs

#### a.2.1 Proof of Proposition 1

**Proposition**

\[\mathcal{A}_{0}(\boldsymbol{\eta},\widetilde{\boldsymbol{\eta}})=\big{\{}x\in \mathcal{X}:\arg\max\,\widetilde{\boldsymbol{\eta}}(x)\subseteq\arg\max\, \boldsymbol{\eta}(x)\big{\}}.\]

_Proof._ Notice that

\[\mathcal{M}(x;\boldsymbol{\eta},\widetilde{\boldsymbol{\eta}})=0\quad \Longleftrightarrow\quad\arg\max\,\widetilde{\boldsymbol{\eta}}(x)\not \subseteq\arg\max\,\boldsymbol{\eta}(x).\]

This is because \(\mathcal{M}(x;\boldsymbol{\eta},\widetilde{\boldsymbol{\eta}})=0\) when the numerator is zero and the denominator is non-zero, which happens when \(\arg\max\,\widetilde{\boldsymbol{\eta}}(x)\not\subseteq\arg\max\,\boldsymbol {\eta}(x)\).

An equivalent statement of this is

\[\mathcal{M}(x;\boldsymbol{\eta},\widetilde{\boldsymbol{\eta}})>0\quad \Longleftrightarrow\quad\arg\max\,\widetilde{\boldsymbol{\eta}}(x)\subseteq \arg\max\,\boldsymbol{\eta}(x).\]

#### a.2.2 Proof of Proposition 2

**Proposition** _In the binary setting, for \(\kappa\geq 0\),_

\[\mathcal{M}(x;\eta,\widetilde{\eta})=\max\left\{\frac{\widetilde{\eta}(x)- \frac{1}{2}}{\eta(x)-\frac{1}{2}},0\right\},\qquad\text{and}\qquad\mathcal{A} _{\kappa}(\eta,\widetilde{\eta})=\left\{x\in\mathcal{X}:\frac{\widetilde{\eta }(x)-\frac{1}{2}}{\eta(x)-\frac{1}{2}}>\kappa\right\}.\]

_Proof._ In a brute-force way, we can examine the nine cases where \(\eta(x),\widetilde{\eta}(x)\) is greater, equal, or smaller than \(1/2\).

If \(\widetilde{\eta}(x)>\frac{1}{2},\eta(x)>\frac{1}{2}\), then

\[\mathcal{M}(x;\boldsymbol{\eta},\widetilde{\boldsymbol{\eta}}) =\min_{j\in\mathcal{Y}}\ \frac{\max_{i}[\widetilde{\boldsymbol{\eta}}(x)]_{i}-[ \widetilde{\boldsymbol{\eta}}(x)]_{j}}{\max_{i}[\boldsymbol{\eta}(x)]_{i}-[ \boldsymbol{\eta}(x)]_{j}}\] \[=\frac{\widetilde{\eta}(x)-(1-\widetilde{\eta}(x))}{\eta(x)-(1- \eta(x))}\] \[=\frac{\widetilde{\eta}(x)-\frac{1}{2}}{\eta(x)-\frac{1}{2}}\] \[=\max\left\{\frac{\widetilde{\eta}(x)-\frac{1}{2}}{\eta(x)- \frac{1}{2}},0\right\}.\]

If \(\widetilde{\eta}(x)<\frac{1}{2},\eta(x)<\frac{1}{2}\), the same argument holds.

If \(\widetilde{\eta}(x)>\frac{1}{2},\eta(x)<\frac{1}{2}\) or \(\widetilde{\eta}(x)<\frac{1}{2},\eta(x)>\frac{1}{2}\), take \(j=\arg\max\widetilde{\boldsymbol{\eta}}(x)\), we have

\[\mathcal{M}(x;\boldsymbol{\eta},\widetilde{\boldsymbol{\eta}}) =\min_{j\in\mathcal{Y}}\ \frac{\max_{i}[\widetilde{\boldsymbol{\eta}}(x)]_{i}-[ \widetilde{\boldsymbol{\eta}}(x)]_{j}}{\max_{i}[\boldsymbol{\eta}(x)]_{i}-[ \boldsymbol{\eta}(x)]_{j}}\] \[=0\] \[=\max\left\{\frac{\widetilde{\eta}(x)-\frac{1}{2}}{\eta(x)- \frac{1}{2}},0\right\}.\]

If \(\widetilde{\eta}(x)=\frac{1}{2},\eta(x)<\frac{1}{2}\) or \(\widetilde{\eta}(x)=\frac{1}{2},\eta(x)<\frac{1}{2}\), take \(j\neq\arg\max\boldsymbol{\eta}(x)\), we have

\[\mathcal{M}(x;\boldsymbol{\eta},\widetilde{\boldsymbol{\eta}}) =\min_{j\in\mathcal{Y}}\ \frac{\max_{i}[\widetilde{\boldsymbol{\eta}}(x)]_{i}-[ \widetilde{\boldsymbol{\eta}}(x)]_{j}}{\max_{i}[\boldsymbol{\eta}(x)]_{i}-[ \boldsymbol{\eta}(x)]_{j}}\] \[=0\] \[=\max\left\{\frac{\widetilde{\eta}(x)-\frac{1}{2}}{\eta(x)- \frac{1}{2}},0\right\}.\]

If \(\eta(x)=\frac{1}{2}\), then

\[\mathcal{M}(x;\boldsymbol{\eta},\widetilde{\boldsymbol{\eta}}) =\min_{j\in\mathcal{Y}}\ \frac{\max_{i}[\widetilde{\boldsymbol{\eta}}(x)]_{i}-[ \widetilde{\boldsymbol{\eta}}(x)]_{j}}{\max_{i}[\boldsymbol{\eta}(x)]_{i}-[ \boldsymbol{\eta}(x)]_{j}}\] \[=\frac{\max_{i}[\widetilde{\boldsymbol{\eta}}(x)]_{i}-[ \widetilde{\boldsymbol{\eta}}(x)]_{j}}{0}\qquad\qquad\qquad\qquad\forall j\] \[=\max\left\{\frac{\widetilde{\eta}(x)-\frac{1}{2}}{\eta(x)-\frac {1}{2}},0\right\}.\]

Note that it makes sense for RSS to be \(+\infty\) when \(\eta(x)=\frac{1}{2}\), because in this case, clean excess risk \(R(f)-R(f^{*})\) is \(0\) at point \(x\) for any classifier.

Therefore, we can conclude that

\[\mathcal{M}(x;\eta,\widetilde{\eta})=\max\left\{\frac{\widetilde{\eta}(x)- \frac{1}{2}}{\eta(x)-\frac{1}{2}},0\right\},\]

by definition, we have, for \(\kappa\geq 0\)

\[\mathcal{A}_{\kappa}(\eta,\widetilde{\eta}) =\left\{x\in\mathcal{X}:\mathcal{M}(x;\eta,\widetilde{\eta})>\kappa\right\}\] \[=\left\{x\in\mathcal{X}:\frac{\widetilde{\eta}(x)-\frac{1}{2}}{ \eta(x)-\frac{1}{2}}>\kappa\right\}.\]

#### a.2.3 Proof of lower bound: Theorem 1

Now we provide a more formal statement of the minimax lower bound and its proof. We begin with the scenario where the noisy distribution \(P_{X\widetilde{Y}}\) has zero Bayes risk as an introductory example. The proof for the general case follows a similar strategy but involves more complex bounding techniques. We recommend that interested readers first review the proof of the zero-error version to build a solid understanding before tackling the general case.

Now consider a more restricted subset of \(\Pi(\epsilon,\kappa)\):

\[\Pi(\epsilon,\kappa,V,0):=\Big{\{}\left(P_{X},\bm{\eta},\widetilde{\bm{\eta}} \right):P_{X}\Big{(}\mathcal{A}_{\kappa}\left(\bm{\eta},\widetilde{\bm{\eta}} \right)\Big{)}\geq 1-\epsilon,P_{X}\text{ supported on }V+1\text{ points},\widetilde{R}^{*}=0\Big{\}}.\]

**Theorem (Minimax Lower Bound: when \(\widetilde{R}^{*}=0\))** _Let \(\epsilon\in[0,1],\kappa>0,V>1\). For any learning rule \(\hat{f}\) based upon \(Z^{n}=\left\{(X_{i},\widetilde{Y}_{i})\right\}_{i=1}^{n}\), and \(n>\max(V-1,2)\),_

\[\sup_{(P_{X},\bm{\eta},\widetilde{\bm{\eta}})\in\Pi(\epsilon, \kappa)}\mathbb{E}_{Z^{n}}\left[R\left(\hat{f}\right)-R(f^{*})\right] \geq\sup_{(P_{X},\bm{\eta},\widetilde{\bm{\eta}})\in\Pi(\epsilon,\kappa,V,0)}\mathbb{E}_{Z^{n}}\left[R\left(\hat{f}\right)-R(f^{*})\right]\] \[\geq\frac{K-1}{K}\epsilon+\frac{1}{\kappa}\frac{(V-1)(1-\epsilon )}{8en}\]

_Proof._

We will construct a triple \((P_{X},\bm{\eta},\widetilde{\bm{\eta}})\) that is parameterized by \(j,\bm{b}:=[b_{1}\ b_{2}\ \cdots\ b_{V-1}]^{\top}\), and \(\delta\).

First, we define \(P_{X}\). Pick any \(V+1\) distinct points \(x_{0},x_{1},\ldots,x_{V}\),

\[P_{X}(x)=\begin{cases}\epsilon&x=x_{0}\\ (1-\epsilon)\cdot\frac{1}{n}&x=x_{1},\ldots,x_{V-1}\,\\ (1-\epsilon)\cdot\big{(}1-\frac{V-1}{n}\big{)}&x=x_{V}.\end{cases}\]

this is where we need the condition that \(n>V-1\).

Then, define the clean and noisy class posteriors:

If \(x=x_{0}\), then \(\bm{\eta}(x)=\bm{e}_{j},\ \widetilde{\bm{\eta}}(x)=\bm{e}_{1},\quad j\in\{1,2, \ldots K\}\)

If \(x=x_{t}\), \(1\leq t\leq V-1\), then \(\bm{\eta}(x)=\begin{bmatrix}\frac{1}{2}+\frac{1}{2(\kappa+\delta)}\cdot(-1)^{b _{t}+1}\\ \frac{1}{2}-\frac{1}{2(\kappa+\delta)}\cdot(-1)^{b_{t}+1}\\ 0\\ \vdots\\ 0\end{bmatrix},\widetilde{\bm{\eta}}(x)=\bm{e}_{b_{t}},b_{t}\in\{1,2\},\delta>0\),

If \(x=x_{V}\), then \(\bm{\eta}(x)=\begin{bmatrix}\frac{1}{2}+\frac{1}{2(\kappa+\delta)}\cr\frac{1}{ 2}-\frac{1}{2(\kappa+\delta)}\cr 0\cr 0\cr\vdots\cr 0\end{bmatrix},\ \widetilde{\bm{\eta}}(x)=\bm{e}_{1},\)

where \(\bm{e}_{i}\) denotes the one-hot vector whose \(i\)-th element is one.

The triple \((P_{X},\bm{\eta},\widetilde{\bm{\eta}})\) is thus parameterized by \(j,\bm{b}:=[b_{1}\ b_{2}\ \cdots\ b_{V-1}]^{\top}\), and \(\delta\).

This construction ensures \((P_{X},\bm{\eta},\widetilde{\bm{\eta}})\in\Pi(\epsilon,\kappa,V,0)\). In particular,

\[\mathcal{A}_{\kappa}\supseteq\{x_{1},x_{2},\ldots,x_{V}\}, P_{X}(\mathcal{A}_{\kappa})\geq 1-\epsilon,\] \[\mathcal{X}\setminus\mathcal{A}_{\kappa}\subseteq\{x_{0}\}, P_{X}(\mathcal{X}\setminus\mathcal{A}_{\kappa})\leq\epsilon,\]

and \(\widetilde{R}^{*}=0\) because \(\widetilde{\bm{\eta}}(x)\) is one-hot for all \(x\).

For any classifier \(f\), by definition, its risk equals

\[R\left(f\right) =\mathbb{E}_{X,Y}\left[\mathbbm{1}_{f(X)\neq Y}\right]\] \[=\mathbb{E}_{X}\mathbb{E}_{Y|X}[\mathbbm{1}_{f(X)\neq Y}]\] \[=\mathbb{E}_{X}\mathbb{E}_{Y|X}[1-\mathbbm{1}_{f(X)=Y}]\] \[=\mathbb{E}_{X}\left[1-[\boldsymbol{\eta}(X)]_{f(X)}\right]\] \[=\int_{\mathcal{X}}\left(1-[\boldsymbol{\eta}(x)]_{f(x)}\right) dP_{X}(x).\]

Therefore, the Bayes risk and excess risk equal

\[R(f^{*}) =\inf_{f}\ \mathbb{E}_{X,Y}\left[\mathbbm{1}_{f(X)\neq Y}\right]\] \[=\int_{\mathcal{X}}\left(1-\max\boldsymbol{\eta}(x)\right)dP_{X}( x),\] \[R(f)-R(f^{*}) =\int_{\mathcal{X}}\Big{(}\max\boldsymbol{\eta}(x)-[\boldsymbol {\eta}(x)]_{f(x)}\Big{)}\;dP_{X}(x).\]

Under our construction of \(P_{X}\), \(R(f)\) can be decomposed into two parts

\[R\left(f\right) =\underbrace{\int_{\left\{x_{0}\right\}}\left(1-[\boldsymbol{ \eta}(x)]_{f(x)}\right)dP_{X}(x)}_{:=R_{0}(f)}+\underbrace{\int_{\left\{x_{1 },\ldots,x_{V}\right\}}\left(1-[\boldsymbol{\eta}(x)]_{f(x)}\right)dP_{X}(x)}_ {:=R_{V}(f)},\]

and so can the excess risk

\[R(f)-R(f^{*}) =\left(R_{0}\left(f\right)-R_{0}(f^{*})\right)+\left(R_{V}\left( f\right)-R_{V}(f^{*})\right)\] \[=\int_{\left\{x_{0}\right\}}\Big{(}\max\boldsymbol{\eta}(x)-[ \boldsymbol{\eta}(x)]_{f(x)}\Big{)}\;dP_{X}(x)\] \[\qquad+\int_{\left\{x_{1},\ldots,x_{V}\right\}}\Big{(}\max \boldsymbol{\eta}(x)-[\boldsymbol{\eta}(x)]_{f(x)}\Big{)}\;dP_{X}(x).\]

Recall that in our construction, \((P_{X},\boldsymbol{\eta},\widetilde{\boldsymbol{\eta}})\) is parameterized by \(j,\boldsymbol{b}\), and \(\delta\). Therefore

\[\sup_{(P_{X},\boldsymbol{\eta},\widetilde{\boldsymbol{\eta}}) \in\Pi(\epsilon,\kappa,V,0)}\mathbb{E}_{Z^{n}}\left[R\left(\hat{f}\right)-R(f ^{*})\right] \geq\sup_{j,\boldsymbol{b},\delta}\mathbb{E}_{Z^{n}}\left[R\left( \hat{f}\right)-R(f^{*})\right]\] \[=\sup_{j,\boldsymbol{b},\delta}\Big{\{}\mathbb{E}_{Z^{n}}\left[R_ {0}\left(\hat{f}\right)-R_{0}(f^{*})\right]\] \[\qquad+\mathbb{E}_{Z^{n}}\left[R_{V}\left(\hat{f}\right)-R_{V}(f ^{*})\right]\Big{\}}\] \[=\sup_{j}\ \mathbb{E}_{Z^{n}}\left[R_{0}\left(\hat{f}\right)-R_{0}(f ^{*})\right]\] \[\qquad+\sup_{\boldsymbol{b},\delta}\ \mathbb{E}_{Z^{n}}\left[R_{V}\left( \hat{f}\right)-R_{V}(f^{*})\right]\]

where the last equality holds because region \(\left\{x_{0}\right\}\) only depends on \(j\), while region \(\left\{x_{1},\ldots,x_{V}\right\}\) only depends on \(\boldsymbol{b},\delta\).

In the remaining part of the proof, we will examine

\[\sup_{j}\ \mathbb{E}_{Z^{n}}\left[R_{0}\left(\hat{f}\right)-R_{0}(f^{*})\right]\] (5)

and

\[\sup_{\boldsymbol{b},\delta}\ \mathbb{E}_{Z^{n}}\left[R_{V}\left( \hat{f}\right)-R_{V}(f^{*})\right]\] (6)separately.

Let's start with the first term (5), which reflects the excess risk over the "low signal strength" region \(\{x_{0}\}\). Since \(\boldsymbol{\eta}\) is one-hot on \(\{x_{0}\}\), its Bayes risk over that is zero

\[\sup_{j}\ \mathbb{E}_{Z^{n}}\left[R_{0}\left(\hat{f}\right)-R_{0}(f^{ *})\right] =\sup_{j}\ \mathbb{E}_{Z^{n}}\left[R_{0}\left(\hat{f}\right)\right]\] \[=\sup_{j}\ \mathbb{E}_{Z^{n}}\left[\int_{\{x_{0}\}}\mathbbm{1}_{f(x) \neq j}dP_{X}(x)\right].\]

To deal with \(\sup_{j}\), we use a technique called "the probabilistic method" (Devroye et al., 1996): replace \(j\) with a random variable \(J\sim\text{Uniform}\{1,2,\ldots,K\}\):

\[\sup_{j}\ \mathbb{E}_{Z^{n}}\left[\int_{\{x_{0}\}}\mathbbm{1}_{f(x) \neq j}dP_{X}(x)\right] \geq\mathbb{E}_{J}\left[\mathbb{E}_{Z^{n}|J}\left[\int_{\{x_{0}\} }\mathbbm{1}_{f(x)\neq J}dP_{X}(x)\right]\right]\] \[=\mathbb{E}_{J,\ Z^{n}}\left[\int_{\{x_{0}\}}\mathbbm{1}_{f(x) \neq J}dP_{X}(x)\right]\] \[=\mathbb{E}_{Z^{n}}\left[\mathbb{E}_{J|Z^{n}}\left[\int_{\{x_{0} \}}\mathbbm{1}_{\hat{f}(x)\neq J}dP_{X}(x)\right]\right].\]

Again, notice that \(J\) is an independent draw. Even if the point \(x_{0}\) is observed in \(Z^{n}\), the associated noisy label \(\widetilde{Y}=1\) does not give any information about the clean label \(Y=J\). Thus

\[\mathbb{E}_{Z^{n}}\left[\mathbb{E}_{J|Z^{n}}\left[\int_{\{x_{0} \}}\mathbbm{1}_{\hat{f}(x)\neq J}dP_{X}(x)\right]\right] =\mathbb{E}_{Z^{n}}\left[\mathbb{E}_{J}\left[\int_{\{x_{0}\}} \mathbbm{1}_{\hat{f}(x)\neq J}dP_{X}(x)\right]\right]\] \[=\mathbb{E}_{Z^{n}}\left[\int_{\{x_{0}\}}\mathbb{E}_{J}\left[ \mathbbm{1}_{\hat{f}(x)\neq J}\right]dP_{X}(x)\right]\] \[=\mathbb{E}_{Z^{n}}\left[\int_{\{x_{0}\}}\left(1-\frac{1}{K} \right)dP_{X}(x)\right]\] \[=\left(1-\frac{1}{K}\right)\epsilon.\]

Now we have the minimax lower bound for the first part (5):

\[\sup_{j}\ \mathbb{E}_{Z^{n}}\left[R_{\{x_{0}\}}\left(\hat{f}\right)-R_{\{x_{0} \}}(f^{*})\right]\geq\left(1-\frac{1}{K}\right)\epsilon.\]

For the second part (6), which is over \(\{x_{1},\ldots,x_{V}\}\), due to the relative signal strength condition, and from our explicit construction in Eqn. (3) and (4), the excess risks w.r.t. true and noisy distribution are related by

\[R_{V}(f)-R_{V}(f^{*}) =\int_{\{x_{1},\ldots,x_{V}\}}\Big{(}\max\boldsymbol{\eta}(x)- \left[\boldsymbol{\eta}(x)\right]_{f(x)}\Big{)}\,dP_{X}(x)\] \[=\int_{\{x_{1},\ldots,x_{V}\}}\frac{1}{\kappa+\delta}\Big{(}\max \widetilde{\boldsymbol{\eta}}(x)-\left[\widetilde{\boldsymbol{\eta}}(x) \right]_{f(x)}\Big{)}\,dP_{X}(x)\quad\text{by construction of }\boldsymbol{\eta},\widetilde{ \boldsymbol{\eta}}\] \[=\frac{1}{\kappa+\delta}\left(\widetilde{R}_{V}(f)-\widetilde{ R}_{V}(\tilde{f}^{*})\right),\]

where \(\widetilde{R}_{V}(f):=\int_{\{x_{1},\ldots,x_{V}\}}\Big{(}1-[\widetilde{ \boldsymbol{\eta}}(x)]_{f(x)}\Big{)}\ dP_{X}(x)\). Also note that \(f^{*}(x)=\tilde{f}^{*}(x)\) for \(x\in\{x_{1},\ldots,x_{V}\}\), which is a result of our construction of \(\boldsymbol{\eta},\widetilde{\boldsymbol{\eta}}\).

Then

\[\sup_{\boldsymbol{b},\delta}\ \mathbb{E}_{Z^{n}}\left[R_{V}\left(\hat{f}\right)-R_{V}(f^ {*})\right]=\sup_{\boldsymbol{b},\delta}\ \mathbb{E}_{Z^{n}}\left[\frac{1}{\kappa+\delta}\left( \widetilde{R}_{V}(f)-\widetilde{R}_{V}(\tilde{f}^{*})\right)\right].\]This allows us to reduce the label noise problem to a standard learning problem: we have an iid sample \(Z^{n}\) from \(P_{X\widetilde{Y}}\) and consider the risk evaluated on the same distribution \(P_{X\widetilde{Y}}\). The remainder of the proof is similar to the proof of Devroye et al. (1996, Theorem 14.1).

Notice that by our construction, \(\widetilde{Y}\) is a deterministic function of \(X\). To be specific, \(\widetilde{Y}=\tilde{f}^{*}(X)\), where

\[\tilde{f}^{*}(x)=\begin{cases}1&x=x_{0},\\ b_{t}&x=x_{t},\ 1\leq t\leq V-1\\ 1&x=x_{V}\end{cases}\]

is the noisy Bayes classifier.

We use the shorthand \(f_{\bm{b}}:=\tilde{f}^{*}\) to denote that the noisy Bayes classifier depends on \(\bm{b}\).

Since the noisy Bayes risk is zero,

\[\sup_{\bm{b},\delta}\ \mathbb{E}_{Z^{n}}\left[\frac{1}{\kappa+\delta}\left( \widetilde{R}_{V}(\hat{f})-\widetilde{R}_{V}(\tilde{f}^{*})\right)\right]= \sup_{\bm{b},\delta}\ \frac{1}{\kappa+\delta}\ \mathbb{E}_{Z^{n}}\left[\widetilde{R}_{V}(\hat{f}) \right].\]

Again, use the probabilistic method, replace \(\bm{b}\) with \(\bm{B}\sim\text{Uniform}\{1,2\}^{V-1}\),

\[\sup_{\bm{b},\delta}\ \frac{1}{\kappa+\delta}\ \mathbb{E}_{Z^{n}} \left[\widetilde{R}_{V}(\hat{f})\right] \geq\sup_{\delta}\ \frac{1}{\kappa+\delta}\ \mathbb{E}_{\bm{B},Z^{n}}\left[ \widetilde{R}_{V}(\hat{f})\right]\] \[=\sup_{\delta}\ \frac{1}{\kappa+\delta}\ \mathbb{E}_{Z^{n}}\left[ \mathbb{E}_{\bm{B}|Z^{n}}\left[\int_{\{x_{1},\ldots,x_{V}\}}\mathds{1}_{\hat{f }(x)\neq f_{\bm{B}}(x)}dP_{X}(x)\right]\right]\]

Since we have \(\bm{B}\sim\text{Uniform}\{1,2\}^{V-1}\) and also \(Z^{n}|\bm{B}\sim P_{X\widetilde{Y}}^{\otimes n}\), then by Bayes rule (or eye-balling, since \(\widetilde{\bm{\eta}}\) is one-hot), we get the posterior distribution of \(\bm{B}|Z^{n}\), to be specific: \(\forall x\in\{x_{1},\cdots,x_{V}\},\)

\[\text{If }x=X_{i},i\in\{1,2,\ldots,n\}, \text{then }\ \mathbb{P}\left(f_{\bm{B}}(x)=\widetilde{Y}_{i}|Z^{n}\right)=1,\ \mathbb{P}\left(f_{\bm{B}}(x)\neq\widetilde{Y}_{i}|Z^{n}\right)=0\] \[\text{If }x=x_{V}, \text{then }\ \mathbb{P}\left(f_{\bm{B}}(x)=1|Z^{n}\right)=1,\ \mathbb{P}\left(f_{\bm{B}}(x)=2|Z^{n}\right)=0\] \[\text{If }x\notin\{X_{1},\ldots,X_{n},x_{V}\}, \text{then }\ \mathbb{P}\left(f_{\bm{B}}(x)=1|Z^{n}\right)=\tfrac{1}{2},\ \mathbb{P}\left(f_{\bm{B}}(x)=2|Z^{n}\right)=\tfrac{1}{2},\]

where we overload the notation \(\mathbb{P}\) to denote conditional probability of \(\bm{B}|Z^{n}\).

Then the optimal decision rule for predicting \(\bm{B}\) based on sample \(Z^{n}\) is:

\[g(x;Z^{n})=\begin{cases}\widetilde{Y}_{i}&x=X_{i},i\in\{1,2,\ldots,n\}\\ 1&x=x_{V}\\ \text{random guess from }\{1,2\}&x\neq X_{1},\ldots,x\neq X_{n},x\neq x_{V}. \end{cases}\]Therefore, the error comes from the probability of \(X\in\{x_{1},\ldots,x_{V}\}\) not being one of the observed \(X_{i}\): for any \(\hat{f}\),

\[\mathbb{E}_{\bm{B},Z^{n}}\left[\widetilde{R}_{V}(\hat{f})\right] =\mathbb{E}_{Z^{n}}\left[\mathbb{E}_{\bm{B}|Z^{n}}\left[\int_{\{x_ {1},\ldots,x_{V}\}}\mathbbm{1}_{\hat{f}_{(x)\neq f_{\bm{B}}(x)}}dP_{X}(x) \right]\right]\] \[\geq\mathbb{P}\left(X\in\{x_{1},\ldots,x_{V}\},\;g(X;Z^{n})\neq f _{\bm{B}}(X)\right)\qquad\therefore\text{error of }\hat{f}\geq\text{error of }g\] \[=\left(1-\frac{1}{2}\right)\mathbb{P}\left(X\neq X_{1},\ldots,X \neq X_{n},X\neq x_{V},X\in\{x_{1},\ldots,x_{V}\}\right)\] \[=\frac{1}{2}\sum_{t=1}^{V}\mathbb{P}\left(X\neq X_{1},\ldots,X \neq X_{n},X\neq x_{V},X=x_{t}\right)\] \[=\frac{1}{2}\sum_{t=1}^{V}\mathbb{P}\left(X_{1}\neq x_{t},\ldots,X_{n}\neq x_{t},x_{V}\neq x_{t},X=x_{t}\right)\quad\therefore\text{replace all }X\text{ with }x_{t}\] \[=\frac{1}{2}\sum_{t=1}^{V-1}\mathbb{P}\left(X_{1}\neq x_{t}, \ldots,X_{n}\neq x_{t},X=x_{t}\right)\] \[=\frac{1}{2}\sum_{t=1}^{V-1}\mathbb{P}\left(X_{1}\neq x_{t}, \ldots,X_{n}\neq x_{t}|X=x_{t}\right)\mathbb{P}\left(X=x_{t}\right)\] \[=\frac{1}{2}\sum_{t=1}^{V-1}\left(1-\mathbb{P}\left(X=x_{t} \right)\right)^{n}\mathbb{P}\left(X=x_{t}\right)\] \[=\frac{1}{2}(V-1)\left(1-\frac{1-\epsilon}{n}\right)^{n}\left( \frac{1-\epsilon}{n}\right)\] \[=\frac{(V-1)(1-\epsilon)}{2n}\left(1-\frac{1-\epsilon}{n}\right) ^{n}\] \[=\frac{(V-1)(1-\epsilon)}{2n}\left(1-\frac{1-\epsilon}{n}\right) ^{1+\epsilon}\left(1-\frac{1-\epsilon}{n}\right)^{n-1-\epsilon}\] \[\geq\frac{(V-1)(1-\epsilon)}{2n}\left(1-\frac{1-\epsilon}{n} \right)^{1+\epsilon}e^{-1+\epsilon}\qquad\therefore\left(1-\frac{1-\epsilon}{n} \right)^{n-1-\epsilon}\downarrow e^{-1+\epsilon}\] \[\geq\frac{(V-1)(1-\epsilon)}{2n}\left(1-\frac{1}{n}\right)^{2}e^ {-1}\qquad\qquad\therefore\epsilon\in[0,1]\] \[\geq\frac{(V-1)(1-\epsilon)}{2n}\frac{e^{-1}}{4}=\frac{(V-1)(1- \epsilon)}{8en}\qquad\text{take }n>2.\]

Now we get the minimax risk for the second part (6)

\[\sup_{\bm{b},\delta}\;\mathbb{E}_{Z^{n}}\left[R_{\mathcal{A}_{n}} \left(\hat{f}\right)-R_{\mathcal{A}_{n}}(f^{*})\right] \geq\sup_{\delta}\;\frac{1}{\kappa+\delta}\frac{(V-1)(1-\epsilon) }{8en}\] \[\geq\frac{1}{\kappa}\frac{(V-1)(1-\epsilon)}{8en}\qquad\text{let } \delta\downarrow 0\]

Combine the two parts together, we get the final result, for \(n>\max(V-1,2)\)

\[\sup_{(P_{X},\bm{\eta},\widetilde{\bm{\eta}})\in\Pi(\epsilon,\kappa,V,0)} \mathbb{E}_{Z^{n}}\left[R\left(\hat{f}\right)-R(f^{*})\right]\geq\frac{K-1}{K }\epsilon+\frac{1}{\kappa}\frac{(V-1)(1-\epsilon)}{8en}.\]As for the general version of the lower bound, now consider the set of triples:

\[\Pi(\epsilon,\kappa,V,L):=\Big{\{}\left(P_{X},\bm{\eta},\widetilde{ \bm{\eta}}\right): P_{X}\Big{(}\mathcal{A}_{\kappa}\left(\bm{\eta},\widetilde{\bm{ \eta}}\right)\Big{)}\geq 1-\epsilon,\] \[P_{X}\text{ supported on }V+1\text{ points},\frac{\widetilde{R}_{ \mathcal{A}_{\kappa}}\left(\tilde{f}^{*}\right)}{P_{X}\Big{(}\mathcal{A}_{ \kappa}\left(\bm{\eta},\widetilde{\bm{\eta}}\right)\Big{)}}\leq L\Big{\}},\]

where \(\widetilde{R}_{C}(f)=\int_{C}\left(1-[\widetilde{\bm{\eta}}(x)]_{f(x)}\right) dP_{X}(x)\).

Theorem (Minimax Lower Bound (General Version)): _Let \(\epsilon\in[0,1],\kappa>0,V>1,L\in(0,1/2)\). For any learning rule \(\hat{f}\) based upon \(Z^{n}=\left\{(X_{i},\widetilde{Y}_{i})\right\}_{i=1}^{n}\), for \(n\geq\frac{V-1}{2L}\max\left\{16,\frac{1}{(1-2L)^{2}}\right\}\)_

\[\sup_{(P_{X},\bm{\eta},\widetilde{\bm{\eta}})\in\Pi(\epsilon, \kappa)}\mathbb{E}_{Z^{n}}\left[R\left(\hat{f}\right)-R(f^{*})\right] \geq\sup_{(P_{X},\bm{\eta},\widetilde{\bm{\eta}})\in\Pi(\epsilon,\kappa,V,L)}\mathbb{E}_{Z^{n}}\left[R\left(\hat{f}\right)-R(f^{*})\right]\] \[\geq\frac{K-1}{K}\epsilon+\frac{1-\epsilon}{\kappa}\sqrt{\frac{( V-1)L}{2n}}e^{-7}\] \[=\frac{K-1}{K}\epsilon+\Omega\left(\frac{1}{\kappa}\sqrt{\frac{1 }{n}}\right).\]

Proof.: Now we construct a triple \((P_{X},\bm{\eta},\widetilde{\bm{\eta}})\) that is parameterized by \(j,\bm{b}:=[b_{1}\;b_{2}\;\cdots\;b_{V-1}]^{\top}\), \(\delta\), \(c\) and \(p\). First, we define \(P_{X}\). Pick any \(V+1\) distinct points \(x_{0},x_{1},\ldots,x_{V}\),

\[P_{X}(x)=\begin{cases}\epsilon&x=x_{0}\\ (1-\epsilon)\cdot p&x=x_{1},\ldots,x_{V-1}\\ (1-\epsilon)\cdot(1-(V-1)p)&x=x_{V}.\end{cases}\]

This imposes the constraint \((V-1)p\leq 1\), which will be satisfied in the end. Notice the difference compared to the previous zero-error proof: we place probability mass \(p\), rather than \(1/n\), on \(x_{1},\ldots,x_{V-1}\).

As for the clean and noisy class probabilities, choose

\[\text{If }x=x_{0},\text{then }\bm{\eta}(x)=\bm{e}_{j},\; \widetilde{\bm{\eta}}(x)=\bm{e}_{1},\quad j\in\{1,2,\ldots k\}\] (7) \[\text{If }x=x_{t},1\leq t\leq V-1,\text{ then }\bm{\eta}(x)=\begin{bmatrix}\frac{1}{2}+\frac{c}{ \kappa+\delta}\cdot(-1)^{b_{t}+1}\\ \frac{1}{2}-\frac{c}{\kappa+\delta}\cdot(-1)^{b_{t}+1}\\ 0\\ \vdots\\ 0\end{bmatrix},\;\widetilde{\bm{\eta}}(x)=\begin{bmatrix}\frac{1}{2}+c\cdot( -1)^{b_{t}+1}\\ \frac{1}{2}-c\cdot(-1)^{b_{t}+1}\\ 0\\ \vdots\\ 0\end{bmatrix},\] \[\text{If }x=x_{V},\text{ then }\bm{\eta}(x)=\begin{bmatrix}\frac{1}{2}+\frac{1}{ 2(\kappa+\delta)}\\ \frac{1}{2}-\frac{1}{2(\kappa+\delta)}\\ 0\\ \vdots\\ 0\end{bmatrix},\;\widetilde{\bm{\eta}}(x)=\bm{e}_{1},\] (9)

where \(\bm{e}_{i}\) denotes the one-hot vector whose \(i\)-th element is one.

The construction for class posterior is also similar to the previous proof, except that for \(x=x_{t},t\in\{1,\ldots,V-1\}\), \(\widetilde{\bm{\eta}}\) is no longer a one-hot vector, rather has class probability separated by \(2c\): \(\Big{|}\left[\widetilde{\bm{\eta}}(x)\right]_{1}-\left[\widetilde{\bm{\eta}}(x) \right]_{2}\Big{|}=2c\).

Therefore, the triple \((P_{X},\bm{\eta},\widetilde{\bm{\eta}})\) can be parameterized by \(j,\bm{b}:=[b_{1}\ b_{2}\ \cdots\ b_{V-1}]^{\top}\), \(\delta\), \(c\) and \(p\).

Again, this construction ensures \((P_{X},\bm{\eta},\widetilde{\bm{\eta}})\in\Pi(\epsilon,\kappa)\), to be specific:

\[\mathcal{A}_{\kappa}\supseteq\{x_{1},x_{2},\ldots,x_{V}\}, P_{X}(\mathcal{A}_{\kappa})\geq 1-\epsilon,\] \[\mathcal{X}\setminus\mathcal{A}_{\kappa}\subseteq\{x_{0}\}, P_{X}(\mathcal{X}\setminus\mathcal{A}_{\kappa})\leq\epsilon.\]

For any classifier \(f\), its risk can be decomposed into two parts

\[R\left(f\right)=\underbrace{\int_{\{x_{0}\}}\left(1-[\bm{\eta}(x)]_{f(x)} \right)dP_{X}(x)}_{:=R_{0}(f)}+\underbrace{\int_{\{x_{1},\ldots,x_{V}\}}\left( 1-[\bm{\eta}(x)]_{f(x)}\right)dP_{X}(x)}_{:=R_{V}(f)},\]

as can its excess risk

\[R\left(f\right)-R(f^{*})=\left(R_{0}\left(f\right)-R_{0}(f^{*})\right)+\left( R_{V}\Big{(}f\Big{)}-R_{V}(f^{*})\right).\]

In our construction, \((P_{X},\bm{\eta},\widetilde{\bm{\eta}})\) is parameterized by \(j,\bm{b}:=[b_{1}\ b_{2}\ \cdots\ b_{V-1}]^{\top}\), \(\delta\), \(c\) and \(p\), therefore

\[\sup_{(P_{X},\bm{\eta},\widetilde{\bm{\eta}})\in\Pi(\epsilon, \kappa,V,L)}\mathbb{E}_{Z^{n}}\left[R\left(\hat{f}\right)-R(f^{*})\right] \geq\sup_{j}\ \mathbb{E}_{Z^{n}}\left[R_{0}\left(\hat{f}\right)-R_{0}(f^{*})\right]\] (10) \[\quad+\sup_{\bm{b},\delta,c,p}\ \mathbb{E}_{Z^{n}}\left[\frac{1}{ \kappa+\delta}\left(\widetilde{R}_{V}(f)-\widetilde{R}_{V}(\tilde{f}^{*}) \right)\right].\] (11)

Note that we have used the fact that

\[R_{V}(f)-R_{V}(f^{*})=\frac{1}{\kappa+\delta}\left(\widetilde{R}_{V}(f)- \widetilde{R}_{V}(\tilde{f}^{*})\right),\]

where \(\widetilde{R}_{V}(f):=\int_{\{x_{1},\ldots,x_{V}\}}\left(1-[\widetilde{\bm{ \eta}}(x)]_{f(x)}\right)dP_{X}(x)\).

The first part (10) is exactly the same as in the zero-error proof, and we have

\[\sup_{j}\ \mathbb{E}_{Z^{n}}\left[R_{0}\left(\hat{f}\right)-R_{0}(f^{*}) \right]\geq\left(1-\frac{1}{K}\right)\epsilon.\]

From this point forward, the procedure is similar to the proof of Devroye et al. (1996, Theorem 14.5). For the second part (11), the noisy Bayes classifier is still

\[\tilde{f}^{*}(x)=\begin{cases}j&x=x_{0},\\ b_{t}&x=x_{t},\ 1\leq t\leq V\\ 1&x=x_{V}.\end{cases}\]

We also use the shorthand \(f_{\bm{b}}:=\tilde{f}^{*}\) to denote that the noisy Bayes classifier depends on \(\bm{b}\).

Now the noisy Bayes risk is no longer zero. In fact

\[\widetilde{R}_{V}(\tilde{f}^{*})=\int_{\{x_{1},\ldots,x_{V}\}}\left(1-[ \widetilde{\bm{\eta}}(x)]_{f(x)}\right)dP_{X}(x)=(V-1)(1-\epsilon)p\left( \frac{1}{2}-c\right)\]

What's more,

\[\frac{\widetilde{R}_{\mathcal{A}_{\kappa}}\left(\tilde{f}^{*}\right)}{P_{X} \Big{(}\mathcal{A}_{\kappa}\left(\bm{\eta},\widetilde{\bm{\eta}}\right)\Big{)} }\leq\frac{\widetilde{R}_{V}(\tilde{f}^{*})}{P_{X}\Big{(}\left\{x_{1},\ldots,x _{V}\right\}\Big{)}}=(V-1)p\left(\frac{1}{2}-c\right),\] (12)

where the inequality holds from \(\widetilde{R}_{\mathcal{A}_{\kappa}}(\tilde{f}^{*})=\widetilde{R}_{V}(\tilde{f }^{*})\) (because \(\widetilde{\bm{\eta}}\) is one-hot at point \(x_{0}\)) and \(P_{X}\Big{(}\mathcal{A}_{\kappa}\left(\bm{\eta},\widetilde{\bm{\eta}}\right) \Big{)}\geq P_{X}\Big{(}\left\{x_{1},\ldots,x_{V}\right\}\Big{)}\).

Notice that in order to ensure that our construction \((P_{X},\bm{\eta},\widetilde{\bm{\eta}})\in\Pi(\epsilon,\kappa,V,L)\), by definition

\[\frac{\widetilde{R}_{\mathcal{A}_{\kappa}}\left(\tilde{f}^{*}\right)}{P_{X} \Big{(}\mathcal{A}_{\kappa}\left(\bm{\eta},\widetilde{\bm{\eta}}\right)\Big{)} }\leq L,\]

Due to the upper bound of (12), it suffices to require that

\[(V-1)p\left(\frac{1}{2}-c\right)=L,\] (13)

and this ensures that \((P_{X},\bm{\eta},\widetilde{\bm{\eta}})\in\Pi(\epsilon,\kappa,V,L)\) upon recalling that \((P_{X},\bm{\eta},\widetilde{\bm{\eta}})\in\Pi(\epsilon,\kappa),\) and that \(P_{X}\) is supported on \(V+1\) points.

It should be noted that since \((V-1)p\leq 1\) is required, and since \(c>0\), we have \(L<1\cdot 1/2\). This is the origin of our condition \(L<1/2\) in the statement of the theorem. Naturally, the statement can be adjusted to \(\min(L,1/2)\) instead. In any case, we are left with two nontrivial constraint on our parameters \((p,c)\): (13) and \((V-1)p\leq 1\), along with the boundary consraints \(p\in[0,1]\) and \(c\in[0,1/2]\).

For fixed \(\bm{b}\), plugging in the definition of \(\widetilde{\bm{\eta}}\), the excess risk over region \(\{x_{1},\ldots,x_{V}\}\) becomes

\[\widetilde{R}_{V}(\hat{f})-\widetilde{R}_{V}(\tilde{f}^{*}) =\int_{\{x_{1},\ldots,x_{V}\}}2c1_{\hat{f}(x)\neq f_{\bm{b}}(x)}dP _{X}(x)\] \[\geq 2c\sum_{t=1}^{V-1}(1-\epsilon)p1_{\hat{f}(x_{t})\neq f_{\bm{b }}(x_{t})},\]

where the inequality follows from the fact that we ignore the risk on point \(x_{V}\).

Using the probabilistic method, replace \(\bm{b}\) with \(\bm{B}\sim\text{Uniform}\{1,2\}^{V-1}\),

\[\sup_{\bm{b},\delta,c,p}\,\mathbb{E}_{Z^{n}}\left[\frac{1}{ \kappa+\delta}\left(\widetilde{R}_{V}(\hat{f})-\widetilde{R}_{V}(\tilde{f}^{* })\right)\right] \geq\sup_{\delta,c,p}\,\mathbb{E}_{\bm{B},Z^{n}}\left[\frac{1}{ \kappa+\delta}\left(\widetilde{R}_{V}(\hat{f})-\widetilde{R}_{V}(\tilde{f}^{* })\right)\right]\] \[=\sup_{\delta,c,p}\frac{1}{\kappa+\delta}\mathbb{E}_{Z^{n}} \left[\mathbb{E}_{\bm{B}|Z^{n}}\left[\left(\widetilde{R}_{V}(\hat{f})- \widetilde{R}_{V}(\tilde{f}^{*})\right)\right]\right]\]

Now, we need to calculate \(\bm{B}|Z^{n}\), which can be calculated using Bayes rule because we have \(\bm{B}\sim\text{Uniform}\{1,2\}^{V-1}\) and also \(Z^{n}|\bm{B}\sim P_{X\widetilde{Y}}^{\otimes n}\).

To be specific, for any \(x\in\{x_{0},x_{1},\ldots,x_{V-1}\}\), assume point \(x_{t}\) is observed \(k\) times in training sample \(Z^{n}\),

\[\mathbb{P}\left(f_{\bm{B}}(x)=1|Z^{n}\right)=\begin{cases}\frac{1}{2}&x\neq X_ {1},\ldots,x\neq X_{n},x\neq x_{V}\\ \mathbb{P}\left(B_{t}=1|Y_{t_{1}},\ldots,Y_{t_{k}}\right)&x=x_{t}=X_{t_{1}}= \cdots=X_{t_{k}},\ 1\leq t\leq V-1,\end{cases}\]

where \(B_{t}\) denotes the \(t\)-th element of vector \(\bm{B}\) (that associates with \(x_{t}\)).

Next we compute \(\mathbb{P}\left(B_{t}=1|Y_{t_{1}}=y_{1},\ldots,Y_{t_{k}}=y_{k}\right)\) for \(y_{1},\ldots,y_{k}\in\{1,2\}\). Denote the numbers of ones and twos by \(k_{1}=|\{j\leq k:y_{j}=1\}|\) and \(k_{2}=|\{j\leq k:y_{j}=2\}|\). Using Bayes rule, we get

\[\mathbb{P}\left(B_{t}=1|Y_{t_{1}},\ldots,Y_{t_{k}}\right) =\frac{\mathbb{P}\left(B_{t}=1\cap Y_{t_{1}},\ldots,Y_{t_{k}} \right)}{\mathbb{P}\left(Y_{t_{1}},\ldots,Y_{t_{k}}\right)}\] \[=\frac{\mathbb{P}\left(Y_{t_{1}},\ldots,Y_{t_{k}}|B_{t}=1\right) \mathbb{P}\left(B_{t}=1\right)}{\sum_{i=1}^{2}\mathbb{P}\left(Y_{t_{1}}, \ldots,Y_{t_{k}}|B_{t}=i\right)\mathbb{P}\left(B_{t}=i\right)}\] \[=\frac{(1/2+c)^{k_{1}}(1/2-c)^{k_{2}}(1/2)}{(1/2+c)^{k_{1}}(1/2-c )^{k_{2}}(1/2)+(1/2+c)^{k_{2}}(1/2-c)^{k_{1}}(1/2)}.\]After some calculation, following the proof of Devroye et al. (1996, Theorem 14.5), we get

\[\sup_{\boldsymbol{b},\delta,c,p}\,\mathbb{E}_{Z^{n}} \left[\frac{1}{\kappa+\delta}\left(\widetilde{R}_{\mathcal{A}_{ \kappa}}(f)-\widetilde{R}_{\mathcal{A}_{\kappa}}(\tilde{f}^{*})\right)\right]\] \[\geq\sup_{\delta,c,p}\,\frac{1}{\kappa+\delta}c(V-1)(1-\epsilon) pe^{-\frac{8n(1-1)pc^{2}}{1-2c}-\frac{4c\sqrt{n(1-c)p}}{1-2c}}\] \[\geq\frac{1-\epsilon}{\kappa}\sup_{c,p}c(V-1)pe^{-\frac{8np^{2}} {1-2c}-\frac{4c\sqrt{np}}{1-2c}}\quad\because\epsilon\geq 0,\text{ take }\delta\downarrow 0\] \[=\frac{1-\epsilon}{\kappa}\sup_{c,p}c\,\frac{L}{1/2-c}e^{-\frac{8 np^{2}}{1-2c}-\frac{4c\sqrt{np}}{1-2c}},\quad\because\eqref{eq:c-1}\] (14)

where the supremum is over \((p,c)\in[0,1]\times[0,1/2]\) such that

\[(V-1)p\leq 1,\text{ and }(V-1)p(1/2-c)=L.\]

Now, suppose \(n\) is so large that

\[n\geq\frac{(V-1)}{8L(1/2-L)^{2}}\iff L\leq\frac{1}{2}-\sqrt{\frac{(V-1)}{8nL}},\]

and further that

\[\sqrt{\frac{(V-1)}{8nL}}\leq\frac{1}{8}\iff n\geq\frac{8(V-1)}{L}.\]

We choose

\[c=\sqrt{\frac{(V-1)}{8nL}},\text{ and }p=\frac{L}{(V-1)(1/2-c)}.\]

By our choice of \(c\) and the first condition on \(n\) above, we can conclude that \(L\leq(1/2-c)\), and therefore,

\[(V-1)p=\frac{L}{1/2-c}\leq 1,\]

meaning that both the constraints required on \((p,c)\) are met by the above choice.

As a consequence of this choice of \(c,p\), we observe that

\[npc^{2}=\frac{nL}{(V-1)(1/2-c)}\cdot c^{2}=\frac{nL}{(V-1)(1/2-c)}\cdot\frac{( V-1)}{8nL}=\frac{1}{4-8c}\leq\frac{1}{3}.\]

Since \(c\leq 1/8\) further implies that \(\frac{1}{1-2c}\leq\frac{4}{3}\), this implies that

\[\frac{8npc^{2}}{1-2c}+\frac{4\sqrt{npc^{2}}}{1-2c}\leq\frac{8}{3}\cdot\frac{4 }{3}+4\cdot\frac{4}{3}\cdot\sqrt{\frac{1}{3}}\leq 7.\]

Thus, instantiating the bound (14), we conclude that

\[\sup_{\boldsymbol{b},\delta,c,p}\,\mathbb{E}_{Z^{n}}\left[\frac{ 1}{\kappa+\delta}\left(\widetilde{R}_{\mathcal{A}_{\kappa}}(f)-\widetilde{R}_ {\mathcal{A}_{\kappa}}(\tilde{f}^{*})\right)\right] \geq\frac{1-\epsilon}{\kappa}\cdot\sqrt{\frac{V-1}{8nL}}\cdot \frac{L}{1/2-c}\cdot e^{-7}\] \[\geq\frac{1-\epsilon}{\kappa}\sqrt{\frac{(V-1)L}{8n}}e^{-7}\cdot 2\] \[=\frac{1-\epsilon}{\kappa}\sqrt{\frac{(V-1)L}{2n}}e^{-7}.\]

Putting the two parts together

\[\sup_{(P_{X},\boldsymbol{\eta},\widetilde{\boldsymbol{\eta}})\in\Pi(\epsilon, \kappa)}\mathbb{E}_{Z^{n}}\left[R\left(\hat{f}\right)-R(f^{*})\right]\geq\frac {K-1}{K}\epsilon+\frac{1-\epsilon}{\kappa}\sqrt{\frac{(V-1)L}{2n}}e^{-7},\]

for \(n\geq\frac{V-1}{2L}\max\left\{16,\frac{1}{(1-2L)^{2}}\right\}\).

#### a.2.4 Proof of upper bound: Lemma 1

**Lemma (Oracle Inequality under Feature-dependent Label Noise)** _For any \(\left(P_{X},\bm{\eta},\widetilde{\bm{\eta}}\right)\) and any classifier \(f\), we have_

\[R(f)-R(f^{*})\leq\inf_{\kappa>0}\left\{P_{X}\Big{(}\mathcal{X}\setminus\mathcal{ A}_{\kappa}\left(\bm{\eta},\widetilde{\bm{\eta}}\right)\Big{)}+\frac{1}{\kappa} \left(\widetilde{R}(f)-\widetilde{R}\left(\tilde{f}^{*}\right)\right)\right\}.\]

_Proof._ For any \(\kappa\geq 0\), the input space \(\mathcal{X}\) can be divided into two regions: \(\mathcal{X}\setminus\mathcal{A}_{\kappa}\) and \(\mathcal{A}_{\kappa}\).

For any \(f\), its risk is

\[R\left(f\right) =\mathbb{E}_{X,Y}\left[\mathbbm{1}_{f(X)\neq Y}\right]\] \[=\mathbb{E}_{X}\mathbb{E}_{Y|X}[\mathbbm{1}_{f(X)\neq Y}]\] \[=\mathbb{E}_{X}\mathbb{E}_{Y|X}[1-\mathbbm{1}_{f(X)=Y}]\] \[=\mathbb{E}_{X}\left[1-[\bm{\eta}(X)]_{f(X)}\right]\] \[=\int_{\mathcal{X}}\left(1-[\bm{\eta}(x)]_{f(x)}\right)dP_{X}(x).\]

Therefore, its excess risk is

\[R(f)-R(f^{*}) =\int_{\mathcal{X}}\Big{(}\max\bm{\eta}(x)-[\bm{\eta}(x)]_{f(x)} \Big{)}\;dP_{X}(x)\] \[=\underbrace{\int_{\mathcal{X}\setminus\mathcal{A}_{\kappa}} \Big{(}\max\bm{\eta}(x)-[\bm{\eta}(x)]_{f(x)}\Big{)}\;dP_{X}(x)}_{\text{\text{ \textcircled{a}}}}\] \[\qquad+\underbrace{\int_{\mathcal{A}_{\kappa}}\Big{(}\max\bm{ \eta}(x)-[\bm{\eta}(x)]_{f(x)}\Big{)}\;dP_{X}(x)}_{\text{\textcircled{b}}}\]

Now examine the two terms separately,

\[\text{\textcircled{a}}\leq\int_{\mathcal{X}\setminus\mathcal{A}_{\kappa}}1\; dP_{X}(x)=P_{X}\Big{(}\mathcal{X}\setminus\mathcal{A}_{\kappa}\left(\bm{\eta}, \widetilde{\bm{\eta}}\right)\Big{)},\]

and

\[\text{\textcircled{b}} <\int_{\mathcal{A}_{\kappa}}\frac{1}{\kappa}\Big{(}\max\widetilde {\bm{\eta}}(x)-[\widetilde{\bm{\eta}}(x)]_{f(x)}\Big{)}\;dP_{X}(x)\quad\therefore \text{by definition of relative signal strength}\] \[\leq\int_{\mathcal{X}}\frac{1}{\kappa}\Big{(}\max\widetilde{\bm {\eta}}(x)-[\widetilde{\bm{\eta}}(x)]_{f(x)}\Big{)}\;dP_{X}(x)\] \[=\frac{1}{\kappa}\left(\widetilde{R}(f)-\widetilde{R}(\tilde{f}^ {*})\right)\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad \qquad\qquad\qquad\qquad\ddots\text{ by definition of }\widetilde{R}.\]

Since this works for any \(\kappa>0\), we then have

\[R(f)-R(f^{*})\leq\inf_{\kappa>0}\left\{P_{X}\Big{(}\mathcal{X}\setminus \mathcal{A}_{\kappa}\left(\bm{\eta},\widetilde{\bm{\eta}}\right)\Big{)}+\frac {1}{\kappa}\left(\widetilde{R}(f)-\widetilde{R}\left(\tilde{f}^{*}\right) \right)\right\}.\]

#### a.2.5 Proof of upper bound: Theorem 2

To set the stage for the rate of convergence proof, we first introduce the concept of shattering in the multiclass setting and the Natarajan dimension [Natarajan, 1989], which serves as a multiclass counterpart to the VC dimension [Vapnik and Chervonenkis, 1971].

**Definition 4** (Multiclass Shattering): _Let \(\mathcal{H}\) be a class of functions from \(\mathcal{X}\) to \(\mathcal{Y}=\{1,2,\ldots,K\}\). For any set containing \(n\) distinct elements \(C_{n}=\{x_{1},\ldots,x_{n}\}\subset\mathcal{X}\), denote_

\[\mathcal{H}_{C_{n}}=\left\{(h(x_{1}),\ldots,h(x_{n})):h\in\mathcal{H}\right\},\]

_and therefore \(|\mathcal{H}_{C_{n}}|\) is the number of distinct vectors of length \(n\) that can be realized by functions in \(\mathcal{H}\)._

_The \(n^{th}\) shatter coefficient is defined as_

\[S(\mathcal{H},n):=\max_{C_{n}}|\mathcal{H}_{C_{n}}|\,.\]

_We say that a set \(C_{n}\) is shattered by \(\mathcal{H}\) if there exists \(f,g:C_{n}\to\mathcal{Y}\) such that for every \(x\in C_{n}\), \(f(x)\neq g(x)\), and_

\[\mathcal{H}_{C}\supseteq\{f(x_{1}),g(x_{1})\}\times\{f(x_{2}),g(x_{2})\} \times\cdots\times\{f(x_{n}),g(x_{n})\}\]

If \(\mathcal{Y}=\{1,2\}\), this definition reduces to the binary notion of shattering which says all labeling of points can be realized by some function in the hypothesis class \(\mathcal{H}\), i.e., \(\mathcal{H}_{C}=\{1,2\}^{|C|}\). Note that multiclass shattering does not mean being able to realize all \(K\) possible labels for each point \(x\in C\). Instead, multiclass shattering is more like "embed the binary cube into multiclass", where every \(x\in C\) is allowed to pick from two of the \(K\) labels.

**Definition 5** (Natarajan Dimension): _The Natarajan dimension of \(\mathcal{H}\), denoted Ndim(\(\mathcal{H}\)), is the maximal size of a shattered set \(C\in\mathcal{X}\)._

**Theorem (Excess Risk Upper Bound of NI-ERM)** _Let \(\epsilon\in[0,1],\kappa\in(0,+\infty)\). Consider any \((P_{X},\boldsymbol{\eta},\widetilde{\boldsymbol{\eta}})\in\Pi(\epsilon,\kappa)\), assume function class \(\mathcal{F}\) has Natarajan dimension \(V\), and the noisy Bayes classifier \(\tilde{f}^{*}\) belongs to \(\mathcal{F}\). Let \(\hat{f}\in\mathcal{F}\) be the ERM trained on \(Z^{n}=\left\{(X_{i},\widetilde{Y}_{i})\right\}_{i=1}^{n}\), then_

\[\mathbb{E}_{Z^{n}}\left[R\left(\hat{f}\right)-R(f^{*})\right] \leq\epsilon+\frac{1}{\kappa}\cdot 16\sqrt{\frac{V\log n+2V\log k+4}{2 n}}\] \[=\epsilon+\mathcal{O}\left(\frac{1}{\kappa}\sqrt{\frac{V}{n}} \right)\quad\text{up to log factor}.\]

_Proof._ Following directly from Lemma 1, with \((P_{X},\boldsymbol{\eta},\widetilde{\boldsymbol{\eta}})\in\Pi(\epsilon,\kappa)\), we already have

\[R(f)-R(f^{*}) \leq P_{X}\Big{(}\mathcal{X}\setminus\mathcal{A}_{\kappa}\left( \boldsymbol{\eta},\widetilde{\boldsymbol{\eta}}\right)\Big{)}+\frac{1}{\kappa} \left(\widetilde{R}(f)-\widetilde{R}\left(\tilde{f}^{*}\right)\right)\] \[\leq\epsilon+\frac{1}{\kappa}\left(\widetilde{R}(f)-\widetilde{ R}\left(\tilde{f}^{*}\right)\right).\]

Now replace \(f\) with NI-ERM\(\hat{f}\). To bound the expected excess risk we employ a multiclass VC-style inequality.

**Lemma 2**: \[\mathbb{E}_{Z^{n}}\left[\widetilde{R}\left(\hat{f}\right)-\widetilde{R} \left(\tilde{f}^{*}\right)\right]\leq 16\sqrt{\frac{\log(8eS(\mathcal{H},n))}{2n}}\]

The binary version of this lemma is Corollary 12.1 in Devroye et al. [1996]. We prove the multiclass version below in Section A.2.6.

Next, we bound the multiclass shattering coefficient with Natarajan dimension, using the following lemma, which can be viewed as a multiclass version of Sauer's lemma.

**Lemma 3** (Natarajan [1989]): _Let \(C\) and \(\mathcal{Y}\) be two finite sets and let \(\mathcal{H}\) be a set of functions from \(C\) to \(\mathcal{Y}\). Then_

\[|\mathcal{H}|\leq|C|^{\text{Ndim}(\mathcal{H})}\cdot|\mathcal{Y}|^{2\text{ Ndim}(\mathcal{H})}\,.\]Letting \(V\) denote \(\text{Ndim}(\mathcal{H})\), we have that \(S(\mathcal{H},n)\leq n^{V}K^{2V}\), and therefore Lemma 2 can be upper bounded by

\[\mathbb{E}_{Z^{n}}\left[\widetilde{R}\left(\hat{f}\right)-\widetilde {R}\left(\tilde{f}^{*}\right)\right] \leq 16\sqrt{\frac{\log\left(8e(n)^{V}K^{2V}\right)}{2n}}\] \[=\ 16\sqrt{\frac{\log 8e+\log\left(n^{V}\right)+\log\left(K^{2V} \right)}{2n}}\] \[\leq 16\sqrt{\frac{V\log n+2V\log K+4}{2n}}\]

Putting things together,

\[\mathbb{E}_{Z^{n}}\left[R\left(\hat{f}\right)-R(f^{*})\right]\leq\epsilon+ \frac{1}{\kappa}\cdot 16\sqrt{\frac{V\log n+2V\log K+4}{2n}}.\]

#### a.2.6 Proof of Lemma 2

**Theorem 6**: _Consider any set of multiclass classifiers \(\mathcal{F}\). Let \((X_{1},Y_{1}),\ldots,(X_{n},Y_{n})\) be iid draws from \(P_{XY}\). For any \(n\), and any \(\epsilon>0\),_

\[\Pr\left\{\sup_{f\in\mathcal{F}}|R_{n}(f)-R(f)|>\epsilon\right\}\leq 8S( \mathcal{F},n)e^{-n\epsilon^{2}/32}\]

_where the probability is with respect to the draw of the data._

_Proof._ Apply Theorem 12.5 from Devroye et al. (1996), with the following identifications. In what follows, the left-hand side of each equation is a notation from Devroye et al. (1996), and the right-hand side is our notation.

\[\nu =P_{XY}\] \[Z =(X,Y)\] \[Z_{i} =(X_{i},Y_{i})\] \[\mathcal{A} =\{A_{f}\mid f\in\mathcal{F}\},\text{ where }A_{f}:=\{(x,y)\mid f(x)=y\}\]

With these identifications, we have

\[\nu(A_{f}) =1-R(f)\] \[\nu_{n}(A_{f}) =\frac{1}{n}\sum_{i}\mathbbm{1}_{\{Z_{i}\in A_{f}\}}=\frac{1}{n} \sum_{i}\mathbbm{1}_{\{f(X_{i})=Y_{i}\}}=1-R_{n}(f)\]

By Theorem 12.5 we conclude

\[\Pr\left\{\sup_{f\in\mathcal{F}}|R_{n}(f)-R(f)|>\epsilon\right\}\leq 8s( \mathcal{A},n)e^{-n\epsilon^{2}/32},\]

where \(s(\mathcal{A},n)\) (note the lowercase "s") is defined to be

\[\max_{z_{1},\ldots,z_{n}}\mathcal{N}_{\mathcal{A}}(z_{1},\ldots,z_{n})\]

where the max is over points \(z_{1},\ldots,z_{n}\), and \(\mathcal{N}_{\mathcal{A}}(z_{1},\ldots,z_{n})\) is the number of distinct subsets of the form

\[A_{f}\cap\{z_{1},\ldots,z_{n}\}\]

as \(f\) ranges over \(\mathcal{F}\).

To conclude the proof, it suffices to show that \(s(\mathcal{A},n)\leq S(\mathcal{F},n)\), where the latter expression is the multiclass shatter coefficient defined above. We show this as follows.

Consider fixed pairs \(z_{i}=(x_{i},y_{i})\), \(i=1,\ldots,n\). Supposed that there are \(N\) distinct subsets of the form \(A_{f}\cap\{z_{1},\ldots,z_{n}\}\), and let \(f_{1},\ldots,f_{N}\) be the classifiers in \(\mathcal{F}\) that realize these distinct subsets. Consider the map that sends \(f_{i}\) to the vector of its values at \(x_{1},\ldots,x_{n}\):

\[f_{i}\mapsto(f_{i}(x_{1}),\ldots,f_{i}(x_{n}))\in\mathcal{Y}^{n}.\]

We will show that this map is injective, from which the claim follows. To see injectivity, consider classifiers \(f_{i}\) and \(f_{j}\), where \(i\neq j\). Since \(f_{i}\) and \(f_{j}\) yield different subsets, it means there is some pair \((x_{k},y_{k})\) such that one of \(f_{i}\) and \(f_{j}\) classifies the pair correctly, while the other does not. This implies that \(f_{i}(x_{k})\neq f_{j}(x_{k})\), and therefore

\[(f_{i}(x_{1}),\ldots,f_{i}(x_{n}))\neq(f_{j}(x_{1}),\ldots,f_{j}(x_{n})).\]

This concludes the proof. 

Now, Lemma 2 follows from the above theorem (stated in terms of the noisy data/distribution/risk) in precisely the same way that Corollary 12.1 in Devroye et al. (1996) follows from Theorem 12.6 in the same book.

#### a.2.7 Proof of upper bound: Theorem 3

**Theorem (Excess Risk Upper Bound of NI-ERM under smooth relative margin condition)** _Let \(\epsilon\in[0,1],\alpha>0,C_{\alpha}>0\). Consider any \((P_{X},\boldsymbol{\eta},\widetilde{\boldsymbol{\eta}})\in\Pi^{\prime}( \epsilon,\alpha,C_{\alpha})\), assume function class \(\mathcal{F}\) has Natarajan dimension \(V\), and the noisy Bayes classifier \(\tilde{f}^{*}\) belongs to \(\mathcal{F}\). Let \(\hat{f}\in\mathcal{F}\) be the ERM trained on \(Z^{n}=\left\{(X_{i},\tilde{Y}_{i})\right\}_{i=1}^{n}\). Then_

\[\mathbb{E}_{Z^{n}}\left[R\left(\hat{f}\right)-R\left(f^{*}\right)\right] \leq\epsilon+\inf_{\kappa>0}\left\{C_{\alpha}\kappa^{\alpha}+ \tilde{\mathcal{O}}\left(\frac{1}{\kappa}\sqrt{\frac{V}{n}}\right)\right\}\] \[=\epsilon+\tilde{\mathcal{O}}\left(n^{-\alpha/(2+2\alpha)}\right).\]

Proof.: Again, using Lemma 1, and Theorem 2, we can conclude the following, where \(C\) is some large enough constant.

\[\mathbb{E}_{Z^{n}} [R(\hat{f})-R(f^{*})]\] \[\leq\inf_{\kappa>0}\left\{P_{X}\Big{(}\mathcal{X}\setminus \mathcal{A}_{\kappa}\left(\boldsymbol{\eta},\widetilde{\boldsymbol{\eta}} \right)\Big{)}+\frac{1}{\kappa}\sqrt{\frac{CV\log(nK)}{n}}\right\}.\]

Now, by definition of \(\Pi^{\prime}(\epsilon,\alpha,C_{\alpha}),\) it holds that

\[\forall\kappa>0,\ P_{X}(\mathcal{M}(x;\boldsymbol{\eta},\widetilde{\boldsymbol {\eta}})\leq\kappa)\leq C_{\alpha}\kappa^{\alpha}+\epsilon.\]

Thus, we can further conclude that

\[\mathbb{E}_{Z^{n}}[R(\hat{f})-R(f^{*})]\leq\inf_{\kappa>0}\left\{\epsilon+C_{ \alpha}\kappa^{\alpha}+\frac{1}{\kappa}\sqrt{\frac{CV\log(nK)}{n}}\right\}.\]

The final statement now comes from optimizing the above bound, which is attained by taking the derivative w.r.t. \(\kappa\) and set to zero, we have

\[\kappa_{*}=\left((\alpha C_{\alpha})^{-1}\sqrt{CV\log(nK)/n}\right)^{1/(\alpha +1)}.\]

This yields the bound

\[\mathbb{E}_{Z^{n}}[R(\hat{f})-R(f^{*})]\leq\epsilon+\mathcal{O}\left(\left( \sqrt{V\log(nK)/n}\right)^{\alpha/(\alpha+1)}\right)=\epsilon+\tilde{\mathcal{ O}}(n^{-\alpha/(2\alpha+2)}).\]

#### a.2.8 Proof of immunity results: Theorem 4 and 5

Here, we state the immunity theorems in an equivalent but different way, so that the proofs are easier to follow.

**Theorem (Immunity for one-hot vector)**_Denote \(\mathcal{B}=\{\boldsymbol{e}_{1},\ldots,\boldsymbol{e}_{K}\}\) to be the set of one-hot vectors._

\[\forall\ \boldsymbol{\eta}(x)\in\mathcal{B},\quad\arg\max\ \boldsymbol{\eta}(x)=\arg\max\ \boldsymbol{E}(x)^{\top}\boldsymbol{\eta}(x)\] \[\Longleftrightarrow\] _Diagonal elements of \[\boldsymbol{E}(x)\] maximizes its row._

_Proof._ Let \(\boldsymbol{\eta}(x)=\boldsymbol{e}_{y}\) for some \(y\), then

\[\widetilde{\boldsymbol{\eta}}(x)=\boldsymbol{E}^{\top}\boldsymbol{\eta}(x)= \begin{bmatrix}\mathbb{P}\left(\widetilde{Y}=1\mid Y=y,X=x\right)\\ \mathbb{P}\left(\widetilde{Y}=2\mid Y=y,X=x\right)\\ \vdots\\ \mathbb{P}\left(\widetilde{Y}=K\mid Y=y,X=x\right)\end{bmatrix}=\left[ \boldsymbol{E}(x)\right]_{y,:}^{\top}\]

To have

\[\arg\max\ \widetilde{\boldsymbol{\eta}}(x)=\arg\max\ \left[\boldsymbol{E}(x) \right]_{y,:}^{\top}=\arg\max\ \boldsymbol{\eta}(x)=y\]

for any choice of \(y\), it is equivalent to say that the diagonal elements of \(\boldsymbol{E}(x)\) maximizes its row.

**Theorem (Universal Immunity)**_Consider \(K\)-class classification,_

\[\forall\ \boldsymbol{\eta}(x),\quad\arg\max\ \boldsymbol{\eta}(x)=\arg\max\ \boldsymbol{E}(x)^{\top}\boldsymbol{\eta}(x)\] \[\Longleftrightarrow \exists\,e(x)\,\text{s.t.}\ \forall x,e(x)\in\left[0,\frac{1}{K}\right)\text{ and}\] \[\boldsymbol{E}(x)=\begin{bmatrix}1-(K-1)e(x)&e(x)&\cdots&e(x)\\ e(x)&1-(K-1)e(x)&\cdots&e(x)\\ \vdots&\vdots&\ddots&\vdots\\ e(x)&e(x)&\cdots&1-(K-1)e(x)\end{bmatrix}.\]

_Proof._

\(\Longleftarrow\): Plug \(\boldsymbol{E}(x)\) into the expression

\[\widetilde{\boldsymbol{\eta}}(x) =\boldsymbol{E}^{\top}\boldsymbol{\eta}(x)\] \[=\begin{bmatrix}1-(K-1)e(x)&e(x)&\cdots&e(x)\\ e(x)&1-(K-1)e(x)&\cdots&e(x)\\ \vdots&\vdots&\ddots&\vdots\\ e(x)&e(x)&\cdots&1-(K-1)e(x)\end{bmatrix}\boldsymbol{\eta}(x)\] \[=\left((1-K\cdot e(x))\cdot\begin{bmatrix}1&0&\cdots&0\\ 0&1&\cdots&0\\ \vdots&\vdots&\ddots&\vdots\\ 0&0&\cdots&1\end{bmatrix}+e(x)\cdot\begin{bmatrix}1\\ 1\\ \vdots\\ 1\end{bmatrix}\cdot\begin{bmatrix}1&\cdots&1\\ \cdots&1\end{bmatrix}\right)\boldsymbol{\eta}(x)\] \[=\left(1-K\cdot e(x)\right)\boldsymbol{\eta}(x)+\text{ constant vector}\]

When \(e(x)\in[0,\frac{1}{K})\), we have

\[\forall\boldsymbol{\eta}(x),\ \arg\max\ \widetilde{\boldsymbol{\eta}}(x)=\arg\max\ \boldsymbol{\eta}(x).\]\(\Longrightarrow\): Denote \(\bm{T}(x):=\bm{E}(x)^{\top}\), then

\[\bm{T}(x)=\begin{bmatrix}t_{11}(x)&t_{12}(x)&\cdots&t_{1K}(x)\\ t_{21}(x)&t_{22}(x)&\cdots&t_{2K}(x)\\ \vdots&\vdots&\ddots&\vdots\\ t_{K1}(x)&t_{K2}(x)&\cdots&t_{KK}(x)\end{bmatrix}\]

has each column sum to \(1\). Let us consider several choices of \(\bm{\eta}(x)\), which pose conditions on matrix \(\bm{T}(x)\).

1) If \(\bm{\eta}(x)=\left[\begin{smallmatrix}1&1\\ K&1\end{smallmatrix}\cdots\frac{1}{K}\right]^{\top}\), then

\[\widetilde{\bm{\eta}}(x)=\bm{T}(x)\bm{\eta}(x)=\frac{1}{K}\begin{bmatrix}t_{1 1}(x)+t_{12}(x)+\cdots+t_{1K}(x)\\ t_{21}(x)+t_{22}(x)+\cdots+t_{2K}(x)\\ \vdots\\ t_{K1}(x)+t_{K2}(x)+\cdots+t_{KK}(x)\end{bmatrix}.\]

To have

\[\arg\max\,\widetilde{\bm{\eta}}(x)=\arg\max\,\bm{\eta}(x)=\left\{1,2,\ldots,k \right\},\]

all elements of \(\widetilde{\bm{\eta}}(x)\) must be equal, i.e., each row of \(\bm{T}(x)\) should sum to the same value. The sum of all elements in \(\bm{T}(x)\) is \(K\), since all column sum to \(1\). Therefore, each row of \(\bm{T}(x)\) also sum to \(1\).

2) If \(\bm{\eta}(x)=\left[\frac{1}{K-1}\ \frac{1}{K-1}\cdots\frac{1}{K-1}\ 0 \right]^{\top}\), then

\[\widetilde{\bm{\eta}}(x)=\bm{T}(x)\bm{\eta}(x) =\frac{1}{K-1}\begin{bmatrix}t_{11}(x)+t_{12}(x)+\cdots+t_{1(K-1 )}(x)\\ t_{21}(x)+t_{22}(x)+\cdots+t_{2(K-1)}(x)\\ \vdots\\ t_{(K-1)1}(x)+t_{(K-1)2}(x)+\cdots+t_{(K-1)(K-1)}(x)\\ t_{K1}(x)+t_{K2}(x)+\cdots+t_{K(K-1)}(x)\end{bmatrix}\] \[=\frac{1}{K-1}\begin{bmatrix}1-t_{1K}(x)\\ 1-t_{2K}(x)\\ \vdots\\ 1-t_{(K-1)k}(x)\\ 1-t_{KK}(x)\end{bmatrix}.\qquad\because\text{each row of }\bm{T}(x)\text{ sum to }1\]

To have

\[\arg\max\,\widetilde{\bm{\eta}}(x)=\arg\max\,\bm{\eta}(x)=\left\{1,2,\ldots,K- 1\right\},\]

the first \(K-1\) elements of \(\widetilde{\bm{\eta}}(x)\) must be equal (and larger than \(t_{KK}(x)\)), then we have

\[t_{1K}(x)=t_{2K}(x)=\cdots=t_{(K-1)K}(x).\]

In other words, all elements of the \(K\)-th column of \(\bm{T}(x)\) are the same (except for the \((K,K)\)-th element). Similarly, consider \(\bm{\eta}(x)\) to be a vector that contains \(0\) in the \(i\)-th position and \(\frac{1}{K-1}\) in other positions, then the general condition for \(\bm{T}(x)\) is that: all elements of the \(i\)-th column are equal, except the \(i\)-th diagonal. Written explicitly,

\[\bm{T}(x)=\begin{bmatrix}t_{11}(x)&t_{12}(x)&t_{13}(x)&\cdots&t_{1K}(x)\\ t_{21}(x)&t_{22}(x)&t_{13}(x)&\cdots&t_{1K}(x)\\ t_{21}(x)&t_{12}(x)&t_{33}(x)&\cdots&t_{1K}(x)\\ \vdots&\vdots&\vdots&\ddots&\vdots\\ t_{21}(x)&t_{12}(x)&t_{13}(x)&\cdots&t_{KK}(x)\end{bmatrix}.\]

[MISSING_PAGE_FAIL:32]

[MISSING_PAGE_FAIL:33]

### Additional experiments

#### a.4.1 Linear probing, then fine tuning (LP-FT)

We study whether 'linear probing, then fine tuning' (LP-FT) [Kumar et al., 2022] works better than linear probing (LP) only, in label noise learning scenario.

#### a.4.2 Robust learning strategy over DINOv2 feature

This section examines how different robust learning strategy works over DINOv2 feature, compared with only training with cross entropy.

\begin{table}
\begin{tabular}{l c c c c c c c c c} \hline \hline \multirow{2}{*}{Feature} & \multirow{2}{*}{Method} & \multicolumn{5}{c}{CIFAR-10N} & \multicolumn{5}{c}{CIFAR-100N} \\ \cline{3-10}  & & Clean & Aggre & Rand1 & Rand2 & Rand3 & Worst & Clean & Noisy \\ \hline \multirow{2}{*}{ResNet-50 TL} & LP (ours) & 90.17 & 89.18 & **88.63** & **88.61** & **88.66** & **85.32** & 71.79 & 62.89 \\  & LP-FT & **95.94** & **92.03** & 88.55 & 87.78 & 87.82 & 71.88 & **82.3** & **63.85** \\ \hline \multirow{2}{*}{ResNet-50 SSL} & LP (ours) & 92.54 & **91.78** & **91.66** & **91.46** & **91.17** & **87.85** & 69.88 & **57.98** \\  & LP-FT & **94.11** & 89.11 & 84.49 & 83.75 & 84.15 & 65.00 & **74.41** & 54.49 \\ \hline \multirow{2}{*}{DINOv2 (small) SSL} & LP (ours) & 96.09 & **94.8** & **94.39** & **94.42** & **94.35** & **91.14** & 83.82 & **72.46** \\  & LP-FT & **98.23** & 93.29 & 88.03 & 87.27 & 86.94 & 67.42 & **89.97** & 64.81 \\ \hline \hline \end{tabular}
\end{table}
Table 6: Performance on CIFAR-N dataset (http://noisylabels.com/) in terms of testing accuracy. “Clean” refers to no label noise, “Aggre”, “Rand1”,..., “Noisy” denote various types of human label noise. We compare the testing accuracy of LP-FT versus LP only, over different feature extractors: “ResNet-50 TL” refers to using a pre-trained ResNet-50 on ImageNet [Deng et al., 2009] in a transfer learning fashion, “ResNet-50 SSL” refers to using a pre-trained ResNet-50 on unlabeled CIFAR data with contrastive loss [Chen et al., 2020] and “DINOv2 (small) SSL” refers to using a light version of the self-supervised foundation model DINOv2 [Qquab et al., 2023] as the feature extractor.

\begin{table}
\begin{tabular}{c c c c c c c} \hline \hline \multirow{2}{*}{Methods} & \multicolumn{5}{c}{CIFAR-10N} & CIFAR-100N \\ \cline{2-7}  & Aggre & Rand1 & Rand2 & Rand3 & Worst & Noisy \\ \hline Linear & 40.73 & 40.41 & 40.31 & 40.63 & 38.43 & 16.61 \\ Linear + ResNet-50 TL & 89.18 & 88.63 & 88.61 & 88.66 & 85.32 & 62.89 \\ Linear + ResNet-50 SSL & 91.78 & 91.66 & 91.39 & 91.28 & 87.84 & 57.95 \\ Linear + DINOv2 SSL & 98.69 & 98.80 & 98.65 & 98.67 & 95.71 & 83.17 \\ \hline \hline \end{tabular}
\end{table}
Table 5: Performance on CIFAR-N dataset (http://noisylabels.com/) in terms of testing accuracy. “Aggre”, “Rand1”,..., “Noisy” denote various types of human label noise. We apply linear model on top of different feature extractors: “ResNet-50 TL” refers to using a pre-trained ResNet-50 on ImageNet [Deng et al., 2009] in a transfer learning fashion, “ResNet-50 SSL” refers to using a pre-trained ResNet-50 on unlabeled CIFAR data with self-supervised loss [Chen et al., 2020] and “DINOv2 SSL” refers to using the self-supervised foundation model DINOv2 [Qquab et al., 2023] as the feature extractor. We employed Python’s sklearn logistic regression and cross-validation functions without data augmentation; the results are deterministic and directly reproducible.

#### a.4.3 Synthetic instance-dependent label noise

\begin{table}
\begin{tabular}{l c c c c c c c c c} \hline \hline \multirow{2}{*}{Feature} & \multirow{2}{*}{Method} & \multicolumn{5}{c}{CIFAR-10N} & \multicolumn{5}{c}{CIFAR-100N} \\ \cline{3-10}  & & Clean & Aggre & Rand1 & Rand2 & Rand3 & Worst & Clean & Noisy \\ \hline \multirow{4}{*}{DINOV2 SSL} & CE & 99.25 & 98.69 & 98.8 & 98.65 & 98.67 & 95.71 & **92.85** & **83.17** \\  & MAE & **99.27** & **99.04** & **99.01** & **99.09** & **99.11** & 95.55 & 90.68 & 82.55 \\ \cline{1-1}  & Sigmoid & 99.26 & 98.86 & 98.91 & 98.87 & 98.96 & **96.66** & 92.82 & 82.03 \\ \cline{1-1}  & ELR & 99.09 & 98.49 & 98.62 & 98.53 & 98.56 & 95.60 & 89.99 & 82.75 \\ \cline{1-1}  & SAM & 99.09 & 97.66 & 98.47 & 98.53 & 98.47 & 95.47 & 89.97 & 82.85 \\ \hline \hline \end{tabular}
\end{table}
Table 7: Comparison of different noise robust methods on DINOV2 features. Training a linear classifier with cross entropy (CE) loss is the baseline. We compare it with robust losses: mean absolute error (MAE) loss (Ghosh and Kumar, 2017; Ma and Fattahi, 2022), sigmoid loss (Ghosh et al., 2015), and regularized approaches: ‘Early-Learning Regularization’ (ELR) (Liu et al., 2020), ‘Sharpness Aware Minimization’ (SAM) (Foret et al., 2021).

\begin{table}
\begin{tabular}{c c c c c c} \hline \hline Method \(\backslash\) Noise rate & 10 \(\%\) & 20 \(\%\) & 30 \(\%\) & 40 \(\%\) & 50 \(\%\) \\ \hline PTD & 79.01 & 76.05 & 72.28 & 58.62 & 53.98 \\ NI-ERM (ours) & **99.11** & **98.94** & **98.20** & **93.35** & **74.67** \\ \hline \hline \end{tabular}
\end{table}
Table 8: We synthetically corrupt labels of CIFAR-10 according to Xia et al. (2020), and compare our NI-ERM with the ‘Part-dependent matrix estimation’ (PTD) method proposed in that same paper.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The claims are either supported by theory statements or by reproducible experiment results. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: Limitations about our practical method is described. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?Answer: [Yes] Justification: Assumptions are stated in the theorem statement. Full proofs are included in the appendix. Guidelines:

* The answer NA means that the paper does not include theoretical results.
* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: Important information about the experiments are in main text. Details on the experimental setup is described in the appendix. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.

5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: Code is provided, common benchmark dataset were used, instructions are given, the result is easily reproducible. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: See appendix and attached code. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: We have done repeated experiments for data simulation, e.g., different iid draw. For the result on CIFAR-N data challenge, our result is deterministic and thus no error bar. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.

* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: See appendix. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: The authors have read the NeurIPS Code of Ethics and confirm that this research follows the code of ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: This is a theory-oriented paper towards better understanding of label noise problem. Guidelines:* The answer NA means that there is no societal impact of the work performed.
* If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).

11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: The paper poses no such risks Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: Citations and urls are included. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset.

* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.

13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: The paper does not release new assets. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.

14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.

15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not involve with this matter. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.

* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.