# Linear Time Algorithms for \(k\)-means with Multi-Swap Local Search

Junyu Huang\({}^{1,2}\), Qilong Feng\({}^{1,2,}\), Ziyun Huang\({}^{3}\), Jinhui Xu\({}^{4}\), Jianxin Wang\({}^{1,2,5,}\)

\({}^{1}\)School of Computer Science and Engineering, Central South University,

Changsha 410083, China

\({}^{2}\)Xiangjiang Laboratory, Changsha 410205, China

\({}^{3}\)Department of Computer Science and Software Engineering, Penn State Erie,

The Behrend College

\({}^{4}\)Department of Computer Science and Engineering, State University of New York at Buffalo,

NY, USA

\({}^{5}\)The Hunan Provincial Key Lab of Bioinformatics, Central South University,

Changsha 410083, China

junyuhuangcsu@foxmail.com,csufeng@mail.csu.edu.cn,

zxh201@psu.edu,jinhui@buffalo.edu,jxwang@mail.csu.edu.cn

Corresponding Authors

###### Abstract

The local search methods have been widely used to solve the clustering problems. In practice, local search algorithms for clustering problems mainly adapt the single-swap strategy, which enables them to handle large-scale datasets and achieve linear running time in the data size. However, compared with multi-swap local search algorithms, there is a considerable gap on the approximation ratios of the single-swap local search algorithms. Although the current multi-swap local search algorithms provide small constant approximation, the proposed algorithms tend to have large polynomial running time, which cannot be used to handle large-scale datasets. In this paper, we propose a multi-swap local search algorithm for the \(k\)-means problem with linear running time in the data size. Given a swap size \(t\), our proposed algorithm can achieve a \((50(1+\frac{1}{t})+\epsilon)\)-approximation, which improves the current best result 509 (ICML 2019) with linear running time in the data size. Our proposed method, compared with previous multi-swap local search algorithms, is the first one to achieve linear running time in the data size. To obtain a more practical algorithm for the problem with better clustering quality and running time, we propose a sampling-based method which accelerates the process of clustering cost update during swaps. Besides, a recombination mechanism is proposed to find potentially better solutions. Empirical experiments show that our proposed algorithms achieve better performances compared with branch and bound solver (NeurIPS 2022) and other existing state-of-the-art local search algorithms on both small and large datasets.

## 1 Introduction

Clustering is a fundamental problem in the field of machine learning with many real-world applications. The goal of clustering is to partition a given set of data points into different clusters according to their similarity such that data points within the same cluster share high similarity as much as possible. Among different objective functions, the \(k\)-means clustering aims to minimize the sum of the squared distances between data points to their closest centers. More formally, given a setof data points in a \(d\)-dimensional Euclidean space, the goal of the \(k\)-means clustering is to find a set \(C\subset\mathbb{R}^{d}\) of size at most \(k\) with the following objective: \(\min_{C\subset\mathbb{R}^{d}}\sum_{p\in P}\min_{e\in C}\|p-c\|^{2}\).

For the \(k\)-means problem, Lloyd's algorithm [12] is one of the most widely used heuristic in practice. However, there is no theoretical guarantee for Lloyd-type method unless certain data distribution assumptions are introduced. It is known that there are several constant approximation schemes based on primal-dual and randomized rounding techniques [2; 5; 8]. The current best approximation ratio in polynomial time is 5.912 [5], which is based on primal-dual method and nested quasi-independent set. For fixed dimension \(d\) or the number of clusters \(k\), several \((1+\epsilon)\)-approximation algorithms were proposed [6; 7].

The \(k\)-means++ algorithm proposed by Arthur and Vassilvitskii [3] is a good seeding method that runs in linear time in the data size with \(O(\log k)\)-approximation. It is also known that the \(k\)-means++ algorithm gives a constant approximation by opening \(O(k)\) centers [1; 13; 16]. Lattanzi and Sohler [11] showed a combination of \(k\)-means++ seeding and the local search algorithm (named as LS++ algorithm), which yields a constant approximation with running time \(O(ndk^{2}\log\log k)\). In each round, LS++ algorithm samples a data point using \(k\)-means++ seeding and enumerates possible swap pairs to make improvements on clustering cost. They proved that after \(O(k\log\log k)\) rounds of sampling and swaps, one can obtain a \(509\)-approximate solution in expectation. Choo et al. [4] proved that one can achieve an \(O(1/\epsilon^{3})\)-approximation using reduced \(O(\epsilon k)\) rounds of LS++ algorithm. Under the assumption that each optimal cluster has size \(\Omega(n/k)\), Huang et al. [9] gave an improved approximation algorithm with ratio \((100+\epsilon)\) by random sampling methods.

However, there are still several issues for local search methods. Although current local search methods with multi-swap strategy can achieve good theoretical guarantee, the running time of them have polynomial dependence on the data size, which are hard to be used to handle large-scale datasets. Compared with the \((9+\epsilon)\)-approximation multi-swap local search algorithm given in [10], the approximation ratio 509 of LS++ is a large constant since it can only apply the single-swap strategy. Numerical experiments [9] showed that LS++ could easily fall into a poor local optimum when handling real-world datasets. An immediate idea is to apply the multi-swap strategy to LS++ algorithm for improvements. However, the swapping process of LS++ relies heavily on the one-to-one matched swap pairs defined in [11]. Thus, it is challenging to apply the multi-swap strategy to solve the \(k\)-means problem while maintaining a linear dependence of data size on the running time.

Secondly, in the process of local search swaps, the time and space complexities for clustering cost update during swaps have great impact on the efficiency of the algorithms. A direct way in [11] is to maintain the nearest and the second nearest centers for each data point such that picking the best swap pair and updating the clustering cost can be implemented in time \(O(nd)\) and \(O(ndk)\), respectively. Maintaining the distances from data points to their centers requires an extra space complexity of \(O(nd)\). To obtain faster implementation, as pointed out in [4], one can use binary search trees to store the distances from each data point to each of the clustering centers. By using this data structure, each local search step can be implemented in time \(O(nd\log k)\). However, the space complexity becomes \(O(ndk)\). To get much practical algorithms for the \(k\)-means problem, it is necessary to further improve the time and space complexities for updating the clustering cost during swaps.

### Our Contributions

In order to further narrow the gap between theory and practice, in this paper, we propose the first multi-swap local search algorithm for the \(k\)-means problem with linear running time in the data size. A common feature for the existing multi-swap local search methods is that \(\Theta((nk)^{t})\) candidate swaps should be enumerated for finding clustering cost improvements in a single local search step, which leads to at least quadratic running time. To overcome this challenge, our idea is to use a sampling-based strategy to construct a candidate set of centers that are close to the optimal clustering centers in linear time, which serves as the set of potentially good centers for swapping in. Based on the candidate set of centers constructed, enumerations on the current set of centers opened suffice to determine a good swap. Hence, the number of possible swaps can be reduced from \(\Theta((nk)^{t})\) to \(\Theta(k^{t})\), where each local search step can be conducted in linear-time in the data size.

Sampling-based strategy has been used in [11] for designing single-swap local search algorithms. However, the theoretical analysis relies heavily on the one-to-one matched swap pairs. Thus, the approximation ratio is a large constant since there may exist some optimal clusters that cannot be well approximated by performing matched swaps. In this paper, to obtain better approximation guarantee, we extend the notions of swap pairs and propose a new consecutive sampling method to construct candidate centers for swap such that data points close to a subset of optimal clustering centers can be swapped in simultaneously. A key challenge here is that there may exist some optimal clusters whose clustering costs only take a tiny fraction of the total clustering cost such that sampling methods may fail. To overcome this challenge, we propose new structures that divide optimal clusters into different groups for establishing a lower bound for the success probability of sampling.

By using the proposed multi-swap local search method (denoted as MLS algorithm for short), given a swap size \(t\), an improved \((50(1+\frac{1}{t})+\epsilon)\)-approximation can be obtained in time \(O(ndk^{2t+1}\log(\epsilon^{-1}\log k))\). To benefit more from the proposed multi-swap local search method when handling large-scale datasets, we propose a sampling-based method for accelerating the MLS algorithm. The proposed algorithm (denoted as MLSP algorithm) accelerates the updating of clustering cost during the swaps such that each local search iteration in MLS algorithm can be implemented in time \(O(ndk+poly(k)d)\) with extra space complexity of \(\tilde{O}(kd)\). In order to obtain better clustering quality, we develop a recombination mechanism in MLSP which combines sampling and scoring methods to help the local search algorithm find better solutions when the search falls into a poor local optimum. By picking the top-\(k\) data points with the highest scores as new initialization, the chance to get out of the local optimal solutions becomes large. Numerical experiments show that our proposed method achieves better performances compared with branch and bound solver and other local search algorithms. The main contributions of this paper are as follows.

* We propose the first multi-swap local search algorithm (MLS algorithm) with running time linearly dependent on the data size. Given a swap size \(t\) with \(t\geq 2\), our MLS algorithm achieves a \((50(1+\frac{1}{t})+\epsilon)\)-approximation in time \(O(ndk^{2t+1}\log(\epsilon^{-1}\log k))\), which improves the current best approximation ratio 509 with linear running time in the data size.
* We give a practical heuristic algorithm (MLSP algorithm) for better implementation of the proposed MLS algorithm, which accelerates the process of clustering cost update during swaps and provides better scalability to large-scale datasets. Besides, a recombination mechanism is proposed to prevent the local search algorithm falling into a poor local optimum too early.

## 2 Preliminaries

We use \(P\subset\mathbb{R}^{d}\) and \(k\) to denote the given dataset and the number of clusters, respectively. For any two points \(p\), \(q\in P\), we use \(d(p,q)=\|p-q\|^{2}\) to denote the squared distance between them. Given two sets \(A\), \(B\subseteq P\), let \(\Delta(A,B)=\sum_{p\in A}\min_{q\in B}d(p,q)\) denote the sum of the squared distances from data points in \(A\) to their closest points in \(B\). Let \(C^{*}=\{c_{1}^{*},c_{2}^{*},...,c_{k}^{*}\}\) be an optimal solution. We use \(\mathbb{P}(C^{*})=\{P_{1}^{*},P_{2}^{*},...,P_{k}^{*}\}\) to denote the corresponding optimal clusters by assigning data points in \(P\) to their closest centers in \(C^{*}\). Let \(Opt\) be the cost of the optimal solution. For a subset \(Q\subseteq\mathbb{P}(C^{*})\) of optimal clusters, we use \(Z(Q)=\cup_{P_{k}^{*}\in Q}P_{h}^{*}\) to denote the set of data points in clusters of \(Q\). Denote \(Z^{\prime}(Q)=\{c_{h}^{*}:P_{h}^{*}\in Q\}\) as the set of optimal centers of clusters in \(Q\). Given a subset \(S\subseteq C^{*}\), let \(J(S)=\{P_{h}^{*}:c_{h}^{*}\in S\}\) be the collection of optimal clusters whose clustering centers are in \(G\). For an integer \(t\), let \([t]=[1,2,...,t]\). The following lemma is a folklore for the \(k\)-means problem.

\begin{table}
\begin{tabular}{c c c c c} \hline \hline Result & Approximation Guarantee & Method & Assumption & Running Time \\ \hline
[3] & \(O(\log k)\) & \(k\)-means++ & - & \(O(ndk)\) \\
[10] & \((3+\frac{1}{t})^{2}\) & Multi-Swap Local Search & - & \(O(ndk^{t+1}k^{\prime}d\log\Delta)\) \\
[11] & \(509\) & Sampling + Single-Swap Local Search & - & \(O(ndk^{2}loglogk)\) \\
[4] & \(O(1)\) & Sampling + Single-Swap Local Search & - & \(O(ndk\log k)\) \\
[9] & \(100+\epsilon\) & Sampling + Single-Swap Local Search & \(|P_{k}^{*}|\geq\frac{\infty}{k}\) & \(O(ndk^{2}\log^{*}\)\(\epsilon^{-1})\) \\ \hline This Paper & \(50(1+\frac{1}{t})+\epsilon\) & Sampling + Multi-Swap Local Search & - & \(O(ndk^{2t+1}\log(\epsilon^{-1}\log k))\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Comparison with related results on \(k\)-means clustering, where \(n\) is the size of the given dataset, \(d\) is the dimension, \(k\) is the number of clusters opened, \(t\) is the parameter representing the swap size of local search methods, and \(\Delta\) is the aspect ratio (aspect ratio is defined as the maximum pairwise distance of the given instance divided by the minimum pairwise distance).

**Lemma 1**: [3] Let \(P\subseteq\mathbb{R}^{d}\) be a set of data points, and \(\mu(P)=\frac{1}{|F|}\sum_{p\in P}p\) denote the center of gravity. For any data point \(c\in\mathbb{R}^{d}\), we can get \(\Delta(P,\{c\})=|P|d(\mu(P),c)+\Delta(P,\{\mu(P)\})\).

**Theorem 1**: [3] Algorithm 1 returns an \(O(\log k)\)-approximate solution in time \(O(ndk)\).

**Input**: An instance \((P,k)\) of the \(k\)-means problem.

**Output**: A set \(C\subseteq\mathbb{R}^{d}\) of centers with size at most \(k\).

```
1:Randomly sample a point \(p\in P\) and set \(C=\{p\}\).
2:for\(i=1\) to \(k-1\)do
3: Pick a point \(p\in P\) with probability \(\Delta(\{p\},C)/\Delta(P,C)\), and add \(p\) to \(C\).
4:return\(C\). ```

**Algorithm 1**\(k\)-means++

## 3 Linear Time Local Search Algorithm with Multi-Swap Strategy

The general idea solving the \(k\)-means problem with multi-swap local search is that we propose a new consecutive sampling method to construct candidate centers for swap such that data points close to a subset of optimal clustering centers can be swapped in simultaneously. The multi-swap local search algorithm (denoted as MLS) is given in Algorithm 2. There are mainly two stages in each round of the MLS algorithm. Given a swap size \(t\), in the first stage (steps 3 to 5), a candidate set of centers with size \(t\) will be sampled using the \(k\)-means++ method. This avoids enumerating all the data points for constructing the candidate sets for swapping in. In the second stage (steps 6 to 7), the algorithm enumerates all subsets (with size at most \(t\)) of the candidate set of centers opened for swapping out. By extending the notions of swap pairs to swap set and carefully analyzing the structures of local optimal solutions, we prove that the clustering cost can be reduced significantly with certain probability in each iteration of the MLS Algorithm. The following is the main result of this paper.

**Theorem 2**: _In the \(i\)-th iteration of Algorithm 2, let \(C^{\prime}\) be the set of centers obtained in step 7. If the current clustering cost \(\Delta(P,C)\) is larger than \(50(1+\frac{1}{t})Opt\), then with probability at least \(\Omega(k^{-t})\), we have \(\Delta(P,C^{\prime})\leq(1-\frac{1}{100k})\Delta(P,C)\). After \(O(k^{O(t)}\log(\epsilon^{-1}\log k))\) iterations, we get an approximate solution with ratio \((50(1+\frac{1}{t})+\epsilon)\) in expectation2._

Footnote 2: Note that the authors did not try to optimize the constants

### Analysis

In this subsection, we analyze our proposed Algorithm 2, where a candidate set of centers for swap is constructed by \(t\) independent sampling steps in each iteration. Our objective is to show that the clustering cost can be reduced with certain probability in each iteration. Due to space limit, all the detailed proofs are given in Appendix A.

In the following, we consider a single iteration of the proposed MLS algorithm. Assume \(\Delta(P,C)\geq 50(1+\frac{1}{t})Opt\) holds within a single iteration in Algorithm 2. Otherwise, \(C\) is already a \(50(1+\frac{1}{t})\)-approximate solution for \(P\). Let \(C=\{c_{1},c_{2},...,c_{k}\}\) denote the set of centers before the swap (steps 6-7) of Algorithm 2. Let \(\mathbb{P}(C)=\{P_{1},P_{2},...,P_{k}\}\) be the corresponding partition of clusters induced by \(C\). For an optimal cluster \(P_{h}^{*}\), let \(c_{h}^{*}\) be its clustering center. For each cluster \(P_{h}\in\mathbb{P}(C)\), let \(c_{h}\in C\) denote its clustering center. Following the ones in [11], we extend the definition of good clusters with respect to \(C^{*}\) as follows.

**Definition 1**: _Good single cluster. A cluster \(P_{h}^{*}\in\mathbb{P}(C^{*})\) is called good with a pair of points \((c_{h}^{*},c_{j})\) such that \(c_{j}\in C\) and \(\Delta(P_{h}^{*},C)-\zeta(P,C_{h}^{*},c_{j})-9\Delta(P_{h}^{*},\{c_{h}^{*}\})> \frac{1}{100k}\Delta(P,C)\), where \(\zeta(P,C_{h}^{*},c_{j})=\Delta(P\backslash P_{h}^{*},C\backslash\{c_{j}\})- \Delta(P\backslash P_{h}^{*},C)\) is the reassignment cost by swapping \(c_{j}\) out. Otherwise we say that \(P_{h}^{*}\) is a bad single cluster with \((c_{h}^{*},c_{j})\)._

**Definition 2**: _Good -Clusters. Given an integer \(t\) with \(t\geq 2\), for a collection of optimal clusters \(Q\subseteq\mathbb{P}(C^{*})\) with \(|Q|\leq t\), \(Q\) is called good with a pair of sets \((Z^{\prime}(Q),V)\) such that \(V\subseteq C\), \(|V|=|Z^{\prime}(Q)|\) and \(\Delta(Z(Q),C)-\zeta(P,C,Z^{\prime}(Q),V)-9\Delta(Z(Q),C^{*})>\frac{1}{100k} \Delta(P,C)\), where \(\zeta(P,C,Z^{\prime}(Q),V)=\Delta(P\backslash Z(Q),C\backslash V)-\Delta(P \backslash Z(Q),C)\) is the reassignment cost by swapping the points in \(V\) out. Otherwise we say that \(Q\) is a set of bad \(t\)-clusters with \((Z^{\prime}(Q),V)\)._The above definitions estimate the changes of clustering cost by replacing a set of centers \(V\subseteq C\) with a set \(Q^{\prime}\) of data points that are close to a subset of optimal clustering centers, where a new clustering is constructed by reassigning data points in \(Z(Q)\) to \(Q^{\prime}\) and data points in \(P\backslash Z(Q)\) to \(C\backslash V\). The main idea behind Algorithm 2 is to iteratively find data points close enough to the optimal clustering centers for swap to make a reduction on clustering cost by at least \((1-\Theta(\frac{1}{k}))\). In the following, we will prove that with probability at least \(\Omega(k^{-t})\), the sampling and swap process induces a significant reduction on clustering cost in each iteration.

We start by dividing the optimal clusters into several groups to give an upper bound of reassignment costs. Given a swap size \(t\), for each optimal center \(c_{h}^{*}\in C^{*}\), we define \(\Psi(c_{h}^{*})\) as a mapping function that maps \(c_{h}^{*}\) to its closest center in \(C\). For simplicity, we say that \(c_{h}^{*}\) is captured by \(\Psi(c_{h}^{*})\). For a center \(c_{j}\in C\), let \(\Psi^{-1}(c_{j})\) be the set of optimal centers captured by \(c_{j}\). If \(|\Psi^{-1}(c_{j})|=0\), then \(c_{j}\) is called a lonely center. If \(|\Psi^{-1}(c_{j})|=1\), let \(c_{h}^{*}=\Psi^{-1}(c_{j})\). Then it is called that \((c_{h}^{*},c_{j})\) forms a type-1 matched swap pair. If \(1<|\Psi(c_{j})|\leq t\), let \(\sigma_{h}\) be an arbitrary set of unused lonely centers with \(|\sigma_{h}|=|\Psi^{-1}(c_{j})|-1\). Let \(A=\Psi^{-1}(c_{j})\) and \(A^{\prime}=\{c_{j}\}\cup\sigma_{h}\). Then, it is called that \((A,A^{\prime})\) forms a type-1 matched swap set. Let \(M_{1}=\{(c_{h}^{*},c_{j}):(c_{h}^{*},c_{j})\) is a type-1 matched swap pair\(\}\) and \(M_{2}=\{(A,A^{\prime}):(A,A^{\prime})\) is a type-1 matched swap set\(\}\) be the collections of type-1 matched swap pairs and type-1 matched swap sets, respectively. If \(|\Psi^{-1}(c_{j})|>t\), then find each lonely center \(c_{q}\in C\) that has not been used for constructing type-1 matched swap pairs or sets. For each lonely center \(c_{q}\) and each \(c_{h}^{*}\in\Psi^{-1}(c_{j})\), \((c_{h}^{*},c_{q})\) forms a type-2 matched swap pair. We use \(M_{3}=\{(c_{h}^{*},c_{q}):(c_{h}^{*},c_{q})\) is a type-2 matched swap pair\(\}\) to denote the set of type-2 matched swap pairs. For a set \(V\subseteq C\) of centers, let \(X(V)=\cup_{c_{h}\in V}P_{h}\) be the set of data points in \(P\) whose closest centers are in \(V\). Given a subset \(S\subseteq C\) of clustering centers, we also use \(J(S)=\{P_{h}:c_{h}\in G\}\) to denote the set of clusters whose centers are in \(S\). The following lemma gives upper bounds of reassignment cost by matched swap pairs or sets.

**Lemma 2**: _Given a type-1 or type-2 matched swap pair \((c_{h}^{*},c_{j})\), it holds that \(\zeta(P,C,c_{h}^{*},c_{j})\leq 24\Delta(P_{j},C^{*})+\frac{1}{5}\Delta(P_{j},C)\). Given a type-1 matched swap set \((Q,V)\), it holds that \(\zeta(P,C,Q,V)\leq 24\Delta(X(V),C^{*})+\frac{1}{5}\Delta(X(V),C)\)._

Let \(H_{1}=\{c_{h}^{*}:(c_{h}^{*},c_{j})\in M_{1}\}\) be the set of optimal centers that participate in constructing type-1 matched swap pair. Let \(H_{2}=\{A:(A,A^{\prime})\in M_{2}\}\) be the collection of the subsets of optimal centers that participate in constructing type-1 matched swap set. Let \(L=\{c_{h}^{*}:(c_{h}^{*},c_{q})\in M_{3}\}\) be the set of optimal centers that participate in constructing type-2 matched swap pair. Let \(H_{1}^{\prime}=\{c_{j}:(c_{h}^{*},c_{j})\in M_{1}\}\) be the set of centers in \(C\) that participate in constructing type-1 matched swap pair. Let \(H_{2}^{\prime}=\{A^{\prime}:(A,A^{\prime})\in M_{2}\}\) be the collection of the subsets of centers in \(C\) that participate in constructing type-1 matched swap pair.

During the sampling and swap process in steps 4-7 of Algorithm 2, there are two cases that may happen: (1) \(\exists\ c_{h}^{*}\in L\) such that \(P_{h}^{*}\) is a good single cluster with a type-2 matched swap pair \((c_{h}^{*},c_{q})\in M_{3}\); (2) \(\forall\ c_{h}^{*}\in L\), \({P_{h}^{*}}^{h}\) is a bad single cluster with any type-2 matched swap pair \((c_{h}^{*},c_{q})\in M_{3}\). We will discuss the two cases separately in the following. If case (1) happens, we first show that with probability at least \(\Omega(\frac{1}{k})\), the clustering cost can be reduced at least by \(1-\Theta(\frac{1}{k})\). Let \(c_{h}^{*}\in L\) be an optimal center such that \(P_{h}^{*}\) is a good single cluster with a type-2 matched swap pair \((c_{h}^{*},c_{q})\in M_{3}\). By the definition of good single cluster, we have \(\Delta(P_{h}^{*},C)\geq 9\Delta(P_{h}^{*},\{c_{h}^{*}\})\).

Define \(\chi(P_{h}^{*})=\{p\in P_{h}^{*}:d(p,c_{h}^{*})\leq\frac{1.5\Delta(P_{h}^{*},[c_{h }^{*}])}{|P_{h}^{*}|}\}\) as the set of data points in \(P_{h}^{*}\) that are close to the optimal center \(c_{h}^{*}\). Observe that \(|\chi(P_{h}^{*})|\geq\frac{1}{3}|P_{h}^{*}|\). Otherwise, the clustering cost of data points in \(P_{h}^{*}\setminus\chi(P_{h}^{*})\) is at least \(\Delta(P_{h}^{*},\{c_{h}^{*}\})\) using \(c_{h}^{*}\) as center, which contradicts with \(\Delta(P_{h}^{*}\setminus\chi(P_{h}^{*}),\{c_{h}^{*}\})<\Delta(P_{h}^{*},\{c_ {h}^{*}\})\).

Next, we will show that whenever an optimal cluster \(P_{h}^{*}\) has large clustering cost with respect to \(C\), i.e., \(\Delta(P_{h}^{*},C)=b\Delta(P_{h}^{*},\{c_{h}^{*}\})\) for a real number \(b\geq 3\), it suffices to use data points in \(\chi(P_{h}^{*})\) to approximate the clustering cost of \(P_{h}^{*}\).

**Lemma 3**: _Let \(P_{h}^{*}\) be an optimal cluster with \(\Delta(P_{h}^{*},C)=b\Delta(P_{h}^{*},\{c_{h}^{*}\})\) for a real number \(b\geq 3\). Then, \(\Delta(\chi(P_{h}^{*}),C)\geq\frac{1}{200}(b-1)\Delta(P_{h}^{*},\{c_{h}^{*}\})\)._

We now argue that the clustering cost of each good single cluster takes a certain fraction of the total clustering cost. Then, by sampling according to the squared distances, with good probability, data points close to the center of a good single cluster can be sampled. According to the definition of good single cluster, we can assume that \(\Delta(P_{h}^{*},C)=b\Delta(P_{h}^{*},\{c_{h}^{*}\})\) for a real number \(b\geq 9\). By Lemma 3, we know that \(\Delta(\chi(P_{h}^{*}),C)\geq\frac{b-1}{200}\Delta(P_{h}^{*},\{c_{h}^{*}\})= \frac{b-1}{200b}\Delta(P_{h}^{*},C)\geq\frac{1}{300}\Delta(P_{h}^{*},C)\). By the definition of good single cluster, we also have \(\Delta(P_{h}^{*},C)\geq\frac{1}{100b}\Delta(P,C)\), which implies that \(\Delta(\chi(P_{h}^{*}),C)\geq\frac{1}{3000b}\Delta(P,C)\). Thus, in each round of the sampling process in steps 4-5 of Algorithm 2, with probability at least \(\Omega(\frac{1}{k})\), we can sample a point \(q\in\chi(P_{h}^{*})\) for a good single cluster \(P_{h}^{*}\) with a type-2 matched swap pair \((c_{h}^{*},c_{q})\in M_{3}\). The following lemma shows that, by swapping \(q\) with \(c_{q}\) and assigning all the data points in \(P_{h}^{*}\) to \(q\), the clustering cost can be reduced at least by \(1-\Theta(\frac{1}{k})\).

**Lemma 4**: _By swapping \(q\) with \(c_{q}\), the clustering cost of \(\Delta(P,C)\) can be reduced at least by \(1-\Theta(\frac{1}{k})\)._

We have shown that if case (1) happens, we can sample a data point \(q\) close to the center of a good single cluster \(P_{h}^{*}\) to make the clustering cost reduced significantly. Next, we assume that case (1) never happens and case (2) happens. In case (2), the idea behind is to sample data points close to the optimal centers in \(H_{1}\) or sets of optimal centers in \(H_{2}\) to reduce the clustering cost. We first bound the clustering cost of optimal clusters in \(J(L)\).

**Lemma 5**: _If case (2) happens, then \(\Delta(X(L),C)\leq(1+\frac{1}{t})(9\Delta(X(L),C^{*})+24\Delta(X(L^{\prime}),C ^{*})+\frac{1}{5}\Delta(X(L^{\prime}),C)+\frac{1}{100}\Delta(P,C))\)._

Now, consider a set of centers \(Q\in H_{2}\). We will divide the optimal clusters in \(J(Q)\) into two groups. Let \(Q_{L}=\{P_{h}^{*}\in J(Q):\Delta(P_{h}^{*},C)\geq\frac{1}{300008^{2t-1}}\Delta (P,C)\}\) be the set of optimal clusters in \(J(Q)\) with large clustering cost with respect to \(C\) and \(Q_{S}=\{P_{h}^{*}\in J(Q):\Delta(P_{h}^{*},C)<\frac{1}{3000008^{2t-1}}\Delta(P,C)\}\) be the set of optimal clusters in \(J(Q)\) with small clustering cost with respect to \(C\), respectively. We define \(Q_{S}^{\prime}=\{P_{h}^{*}\in Q_{S}:\Delta(P_{h}^{*},C)<3\Delta(P_{h}^{*},\{c_ {h}^{*}\})\}\) as the set of optimal clusters in \(Q_{S}\) whose clustering centers are close to one of the centers in \(C\). Let \(Q_{S}^{\prime\prime}=Q_{S}\setminus Q_{S}^{\prime}\) and \(Q_{T}=Q_{L}\cup Q_{S}^{\prime\prime}\), respectively. We first show that, it suffices to only consider optimal clusters in \(Q_{T}\).

**Lemma 6**: _Let \(Q\in H_{2}\) be a set of centers in \(H_{2}\), where \(J(Q)\) is a set of good \(t\)-clusters with a type-1 matched swap set \((Q,A^{\prime})\in M_{2}\). Define \(V=A^{\prime}\backslash\{c_{j}\}\), where \(c_{j}\) is the center in \(A^{\prime}\) with \(|\Psi^{-1}(c_{j})|>1\). Let \(U\subseteq P\) be the set of data points with \(|U|=|V|\) such that \(U\cap\chi(P_{h}^{*})\neq\emptyset\) holds for each \(P_{h}^{*}\in Q_{T}\). Then, \(\Delta(P,C\backslash V\cup U)\leq(1-\frac{1}{100b})\Delta(P,C)\)._

Note that there are \(t\) sampling iterations in each step 4 of Algorithm 2. Let \(H_{2}^{*}=\{P_{h}^{*}:P_{h}^{*}\in Q_{T},Q\in H_{2}\}\) and \(H_{t}=J(H_{1})\cup H_{2}^{*}\). We will define a mapping function \(m\) which maps each \(P_{h}^{*}\in H_{t}\) to an integer \(m(P_{h}^{*})\in[t]\). For each \(P_{h}^{*}\in H_{t}\) such that \(c_{h}^{*}\in H_{1}\), we define \(m(P_{h}^{*})=1\). Then, consider each \(Q\in H_{2}\). For each \(P_{h}^{*}\in Q_{T}\), we define \(m(P_{h}^{*})=i\) such that \(i\in[|Q_{T}|]\) and \(m(P_{h}^{*})\neq m(P_{j}^{*})\) for any two optimal clusters \(P_{h}^{*}\), \(P_{j}^{*}\in Q_{T}\). For each \(Q\in H_{2}\), let \(p_{s}(Q)\) be the success probability that for each \(P_{h}^{*}\in Q_{T}\), a data point is sampled from \(\chi(P_{h}^{*})\) in the \(m(P_{h}^{*})\)-th iteration of Algorithm 2. For each \(c_{h}^{*}\in H_{1}\), let \(p_{s}(c_{h}^{*})\) be the success probability that a data point is sampled from \(\chi(P_{h}^{*})\) in the first iteration (note that in this case \(m(P_{h}^{*})=1\)) of step 4. Let \(H_{G}^{1}=\{c_{h}^{*}\in H_{1}:P_{h}^{*}\}\) is a good single cluster with the type-1 matched swap pair \((c_{h}^{*},c_{j})\in M_{1}\) by the set of centers in \(H_{1}\) whose optimal clusters are good single clusters with type-1 matched swap pairs in \(M_{1}\). Let \(H_{G}^{2}=\{Q\in H_{2}:J(Q)\) is a set of good \(t\)-clusters with the type-1 matched swap set \((Q,V)\in M_{2}\}\) be the collection of the sets of centers in \(H_{2}\) whose corresponding optimal clusters are good \(t\)-clusters with type-1 matched swap sets in \(M_{2}\). Define \(p_{s}^{f}=\sum_{c_{h}^{*}\in H_{G}^{1}}p_{s}(c_{h}^{*})+\sum_{Q\in H_{G}^{2}}p_{s}(Q)\) as the summation of the success probability. Since all the optimal clusters in \(J(L)\) belong to bad single cluster, and the events defined related to \(p_{s}(Q)\) or \(p_{s}(c_{h}^{*})\) for each \(Q\in H_{2}\) and \(c_{h}^{*}\in H_{1}\) are mutually exclusive, \(p_{s}^{f}\) gives a lower bound success probability to get a \((1-\frac{1}{100k})\) reduction on the clustering cost in each iteration of steps 2-7. In the following, we will show how to obtain a lower bound for \(p_{s}^{f}\).

Consider a set \(Q\in H_{2}\) of optimal centers such that \(J(Q)\) is a set of good \(t\)-clusters with a type-1 matched swap set \((Q,A^{\prime})\). There are two subcases that may happen: (1) \(\exists\ P_{h}^{*}\in J(Q)\) such that \(P_{h}^{*}\) is a good single cluster with a swap pair \((c_{h}^{*},c_{j})\), where \(c_{j}\) is a lonely center used for constructing type-1 matched swap set; (2) \(\forall\ P_{h}^{*}\in J(Q)\), \(P_{h}^{*}\) is a bad single cluster with any swap pair \((c_{h}^{*},c_{j})\), where \(c_{j}\) is a lonely center used to construct type-1 matched swap set. In subcase (1), since \(t\) is usually a constant and could be much smaller than \(k\), with probability at least \(\Omega(k^{-1})\), a data point \(q\in\chi(P_{h}^{*})\) can be sampled in the first iteration of step 4 in Algorithm 2 for swap to make the clustering cost reduced at least by \((1-\frac{1}{100k})\) according to Lemma 4. Next, we assume that subcase (1) never happens and subcase (2) happens. In subcase (2), our objective is to sample a set \(V\) of data points such that \(V\) contains at least one point from \(\chi(P_{h}^{*})\) for each \(P_{h}^{*}\in Q_{T}\). For each optimal cluster \(P_{h}^{*}\in Q_{L}\), with probability at least \(\Omega(k^{-1})\), we can sample a data point \(q\in\chi(P_{h}^{*})\) in the \(m(P_{h}^{*})\)-th iteration of step 4 in Algorithm 2. Thus, the probability can be bounded by \(\Omega(k^{-|Q_{L}|})\). Then, we only need to consider optimal clusters in \(Q_{S}\). By Lemma 6, it suffices to consider optimal clusters in \(Q_{S}^{f}\). The following lemma gives an upper bound of the failure probability of not sampling a data point from \(\chi(P_{h}^{*})\) for each \(P_{h}^{*}\in Q_{S}^{\prime\prime}\) in the \(m(P_{h}^{*})\)-th iteration in step 4 of Algorithm 2.

**Lemma 7**: _Given a set \(Q\in H_{2}\) of centers such that \(J(Q)\) is a set of good \(t\)-clusters, the probability that step 4 of Algorithm 2 fails to sample a data point \(q\) from \(\chi(P_{h}^{*})\) for each \(P_{h}^{*}\in Q_{S}^{\prime\prime}\) in the \(m(P_{h}^{*})\)-th iteration is at most \((1+\frac{1}{30000k})e^{-\Delta(Z(Q_{S}^{\prime\prime}),C)/(300\Delta(P,C))}\)._

Now we can bound the success probabilities of \(p_{s}(Q)\) and \(p_{s}(c_{h}^{*})\) for each \(Q\in H_{2}\) and \(c_{h}^{*}\in H_{1}\). We will divide the optimal clusters in \(\mathbb{P}(C^{*})\backslash J(L)\) into two different groups \(H_{G}\) and \(H_{B}\). Firstly, we consider the centers in \(H_{1}\). For a center \(c_{h}^{*}\in H_{1}\), if \(P_{h}^{*}\) is a good single cluster with the type-1 matched swap pair \((c_{h}^{*},c_{j})\in M_{1}\), then add \(P_{h}^{*}\) to \(H_{G}\). Otherwise add \(P_{h}^{*}\) to \(H_{B}\). For a set \(Q\in H_{2}\) of optimal clustering centers, if \(J(Q)\) is a set of good \(t\)-clusters with the type-1 matched swap set \((Q,V)\in M_{2}\), then add each \(P_{h}^{*}\in Q_{S}^{\prime\prime}\) to \(H_{G}\) and each \(P_{h}^{*}\in J(Q)\backslash Q_{S}^{\prime\prime}\) to \(H_{B}\). If \(J(Q)\) is a set of bad \(t\)-clusters with the type-1 matched swap set \((Q,V)\in M_{2}\), then add each \(P_{h}^{*}\in J(Q)\) to \(H_{B}\). The following lemma argues that the summation clustering cost of the optimal clusters in \(H_{G}\) is large.

**Lemma 8**: _For the optimal clusters in \(H_{G}\), we have \(\Delta(Z(H_{G}),C)\geq\frac{1}{100}\Delta(P,C)\)._

For a good single cluster \(P_{h}^{*}\) with the type-1 matched swap pair \((c_{h}^{*},c_{j})\in M_{1}\) where \(c_{h}^{*}\in H_{1}\), define \(p_{f}(c_{h}^{*})=1-p_{s}(c_{h}^{*})\) as the probability that the first iteration in step 4 of Algorithm 2 fails to sample a data point \(q\in\chi(P_{h}^{*})\). Then, we have \(p_{f}(c_{h}^{*})\leq e^{-p_{s}(c_{h}^{*})}\leq e^{-\Delta(P_{h}^{*},C)/300 \Delta(P,C)}\) by Lemma 3. For a set \(Q\in H_{G}^{2}\) of optimal centers, we have that \(p_{f}(Q_{S}^{\prime\prime})\) is the failure probability of not sampling a data point from \(\chi(P_{h}^{*})\) for each \(P_{h}^{*}\in Q_{S}^{\prime\prime}\) in the \(m(P_{h}^{*})\)-th iteration of step 4 in Algorithm 2. Recall that \(p_{s}^{f}=\sum_{c_{h}^{*}\in H_{G}^{2}}p_{s}(c_{h}^{*})+\sum_{Q\in H_{G}^{2}}p_ {s}(Q)\) is the summation of success probability. Then, we have \(p_{s}^{f}\geq\Omega(k^{-t})(\sum_{c_{h}^{*}\in H_{G}^{1}}p_{s}(c_{h}^{*})+ \sum_{Q\in H_{G}^{2}}p_{s}(Q_{S}^{\prime\prime}))\). Let \(p_{s}^{f^{\prime}}=\sum_{c_{h}^{*}\in H_{G}^{1}}p_{s}(c_{h}^{*})+\sum_{Q\in H_{G} ^{2}}p_{s}(Q_{S}^{\prime\prime})\). Observe that \(p_{s}^{f^{\prime}}\geq 1-\prod_{c_{h}^{*}\in H_{G}^{1}}(1-p_{s}(c_{h}^{*}))\prod_{Q\in H_{G} ^{2}}(1-p_{s}(Q_{S}^{\prime\prime}))\). Since there are at most \(\frac{k}{2}\) good \(t\)-clusters, by Lemma 7, we have \(p_{s}^{f^{\prime}}\geq 1-(1+\frac{1}{30000k})^{\frac{k}{2}}e^{-\frac{\Delta(Z(H_{G} ),C)}{300\Delta(P,C)}}\geq 1-e^{\frac{k}{2}\ln(1+\frac{1}{30000k})-\frac{\Delta(Z(H_{G} ),C)}{300\Delta(P,C)}}\geq 1-e^{\frac{1}{60000k}}\geq 1-e^{-\frac{1}{6000k0}}\geq\frac{1}{60001}\), where the third inequality follows from Lemma 8 and \(\ln(1+x)\leq x\). Then, it holds that \(p_{s}^{f}\geq\Omega(k^{-t})p_{s}^{f^{\prime}}=\Omega(k^{-t})\), which indicates that with probability at least \(\Omega(k^{-t})\), we can sample data points close to a set of good \(t\)-clusters or a good single cluster for swap to make the clustering cost reduced at least by \((1-\frac{1}{100k})\) according to Lemma 6. Putting all things together, Theorem 2 can be proved (Detailed proof of Theorem 2 is given in Appendix A).

**Running Time Analysis.** By Theorem 2, in order to obtain a \((50(1+\frac{1}{t})+\epsilon)\)-approximate solution, the iteration rounds for Algorithm 2 should be \(O(k^{t+1}\log(\epsilon^{-1}\log k))\). In each iteration, it takes \(O(ndk)\) time to update the distances between data points to their closest centers. During the sampling process, \(t\) data points are sampled according to the \(D^{2}\)-Sampling distribution to serve as the candidate set of centers for swapping in, which takes time \(O(nt)\) if the distances from data points to their centers are already known. It takes \(O(k^{t})\) time to enumerate each subset with size at most \(t\) of the set of current centers opened. It takes \(O(ndk^{t})\) time to recalculate the clustering cost after each swap if \(t\) nearest centers of each data point are maintained during the whole process. Thus, the total running time of Algorithm 2 is \(O(ndk^{2t+1}\log(\epsilon^{-1}\log k))\).

### Accelerating Multi-Swap Local Search for \(k\)-means

In this subsection, we provide a more practical algorithm for the \(k\)-means problem to accelerate the proposed multi-swap local search process. The algorithm is given in Algorithm 3. The main idea behind is to use sampling-based methods to obtain fast clustering cost updating during swaps. In step 7 of Algorithm 3, a small sample set \(S\) of size \(\frac{2k}{\epsilon}\log\frac{k}{\eta}\) is randomly taken from \(P\). For clustering cost updating, instead of calculating the clustering cost of all the data points in \(P\), we use the clustering cost of \(S\subseteq P\) as an estimation. This reduces the time for picking the best swap pair during a single local search iteration from \(O(nd)\) to \(O(poly(k)d)\). Then, in steps 16-18, we design a recombination method to find better initialization which prevents the local search algorithm from falling into a poor local optimum too early. In step \(16\) of Algorithm 3, we randomly take a set \(D\subseteq P\) of centers with size \(O(k\log k)\). Let \(C_{1}=C\cup D\) be the set of the new candidate centers. For each center \(c_{h}\in C_{1}\), we add a score of \(\Delta(P_{h},C)/\Delta(P,C)\) to it. Similarly, for each center \(c_{h}\in C\), we also add a score \(\Delta(P_{h},C)/\Delta(P,C)\) to it. Then, by giving each center in \(C_{1}\) and \(C\) a score weight of 0.75 and 0.25, respectively, we pick the top-\(k\) data points with the highest scores as a new initialization of clustering centers to find potentially better clustering costs until convergence.

**Input**: An instance \((P,k)\) of the \(k\)-means problem, parameters \(T\), \(t\), \(R^{\prime}\), \(\epsilon\) and \(\eta\).

**Output**: A set \(C\subseteq\mathbb{R}^{d}\) of centers with size at most \(k\).

```
1: Initialize \(C=k\)-means++\((P,k)\), \(r=0\), \(C_{f}=\emptyset\).
2:while\(r<R^{\prime}\)do
3:for\(i=1\) to \(T\)do
4:\(I=\emptyset\).
5:for\(j=1\) to \(t\)do
6: Pick a point \(p\in P\) with probability \(\Delta(\{p\},C)/\Delta(P,C)\), and add \(p\) to \(I\).
7: Randomly sample a set \(S\) from \(P\) of size \(\frac{2k}{\epsilon}\log\frac{k}{\eta}\).
8: Let \((U,V)\) be a swap set such that \(U\subseteq I\), \(V\subseteq C\), \(|U|=|V|\) and \(\Delta(S,C\backslash V\cup U)\) is minimized.
9:if\(\Delta(P,C\backslash V\cup U)<(1-\frac{1}{100k})\Delta(P,C)\)then
10:\(C=C\backslash V\cup U\).
11: For each center \(c\in C\), find the \(50\)-nearest neighbors in \(P\) to \(C\) for improvements on clustering cost by swapping \(c\) with one neighbor until convergence.
12:if\(\Delta(P,C)<\Delta(P,C_{f})\)then
13:\(C_{f}=C\).
14:else
15:\(r=r+1\), randomly sample a set \(D\) from \(P\) with size \(\frac{k}{\epsilon}\log\frac{k}{\eta}\), and set \(C_{1}=C\cup D\).
16: For each \(c_{h}\in C\), calculate \(S^{\prime}(c_{h})=\frac{\Delta(P_{h},C)}{\Delta(P,C)}\), and add a score of 0.25\(S^{\prime}(c_{h})\) to \(c_{h}\).
17: For each \(c_{h}\in C_{1}\), calculate \(S^{\prime}(c_{h})=\frac{\Delta(P_{h},C)}{\Delta(P,C)}\), and add a score of 0.75\(S^{\prime}(c_{h})\) to \(c_{h}\).
18: Reset \(C\) as data points in \(C_{1}\) with top \(k\) scores.
19:return\(C_{f}\). ```

**Algorithm 3** MLSP

## 4 Experiments

In this section, we compare our proposed algorithms with the branch and bound solver and other local search methods. For hardware, all the experiments are conducted on 72 Intel Xeon Gold 6230 CPUs with 500GB memory.

**Datasets** We evaluate the performance of our algorithms on 8 datasets used in [15] with sizes over 50,000, two datasets SUSY (5,000,000 \(\times\) 17) and HIGGS (11,000,000 \(\times\) 27) from the UCI Machine

[MISSING_PAGE_FAIL:9]

The experimental results on the performances with varying \(T\) and \(R^{\prime}\) (Appendix B.1) show that larger \(T\) and \(R^{\prime}\) will not influence the results too much. In general, larger sampling rounds and larger failure upper bound can result in potentially better solutions with higher running time. For parameters \(\epsilon\) and \(\eta\), the results (Appendix B.1) show that the performances of our proposed algorithms are almost the same for different choices of \(\epsilon\) and \(\eta\). Tables 3, 4 and 5 show the comparison results of different algorithms with varying iteration rounds on dataset rds, KEGG and Urban_10, respectively. The results show that, our proposed MLSP algorithm always achieves the best clustering cost compared with LS++ and MLS algorithms. Tables 6, 7, and 8 show the results of MLSP algorithm with varying number of failure upper bound \(R^{\prime}\) for fixed \(\epsilon=0.5\), \(\eta=0.5\) and \(T=400\). It can be seen that a larger failure upper bound will lead to smaller deviation, and the running time becomes higher. Tables 9, 10, and 11 show the results of MLSP algorithm with varying parameters \(\epsilon\) and \(\eta\) for fixed \(T=400\) and \(R^{\prime}=5\) on dataset rds, KEGG, and Urban_10, respectively. It can be seen that smaller values of \(\epsilon\) and \(\eta\) result in better performances on clustering cost with smaller deviation, and the running time becomes higher. The experimental results on small datasets (Appendix B.2) suggest that the proposed MLSP method not only outperforms other algorithms in terms of clustering quality but also runs much faster than the BB solver. The experimental results on the performances with different values of \(k\) (Appendix B.3) show that our proposed MLSP algorithm can still achieve the best clustering quality for smaller values of \(k\). The experimental results on the performances with fixed time limit (Appendix B.4) show that our proposed MLSP algorithm achieves the best clustering quality within any given time constraints.

Tables 12 and 13 show the results on 18 datasets with sizes smaller than \(50,000\) using \(k=10\). It can be seen that, for each dataset, the best clustering cost returned by our MLSP algorithm is smaller than BB method and other local search methods. For each dataset, the average clustering cost returned by our MLSP algorithm nearly matches the result of the BB method. As for FLS, although the performance of FLS is better than that of LS++, our proposed MLSP algorithm improves the performance of FLS on clustering cost with smaller deviation on most datasets. As for running time, there is no significant difference on running time among different local search algorithms for small datasets. Tables 14 and 15 present the performances of different local search algorithms with \(k=3\) on small datasets. Table 18 presents the performances of different local search algorithms with \(k=3\) on large datasets. Tables 16 and 17 present the performances of different local search algorithms with \(k=5\) on small datasets. Table 19 presents the performances of different local search algorithms with \(k=5\) on large datasets. It can be seen that, our proposed MLSP algorithm achieves the best clustering performance on most datasets with different values of \(k\). On each dataset, the best clustering cost returned by our MLSP algorithm matches the clustering cost of BB method. On each dataset, the average clustering cost returned by our MLSP algorithm nearly matches the result of the BB method. As for running time, there is no significant difference on running time among all local search algorithms on small datasets. However, as the data sizes grow, our proposed MLS algorithm becomes much faster than other local search algorithms.

## 5 Conclusion

In this paper, we propose fast local search algorithms for the \(k\)-means problem with multi-swap strategy, which runs in linear time in the data size. We develop new sampling techniques, which accelerate the process of clustering cost update during swaps. By proposing a recombination mechanism, the proposed algorithm can find potentially better solutions. Experimental results show that our algorithms achieve better performance on both small and large datasets compared with the state-of-the-art algorithms. An interesting future direction is how to design fast local search approximation algorithms for handling high dimensional clustering datasets.

## Acknowledgments

This work was supported by National Natural Science Foundation of China (62172446, 62350004, 62332020), Open Project of Xiangjiang Laboratory (22XJ02002, 22XJ03005), and Central South University Research Programme of Advanced Interdisciplinary Studies (2023QYJC023). This work was also carried out in part using computing resources at the High Performance Computing Center of Central South University.

## References

* [1] Ankit Aggarwal, Amit Deshpande, and Ravi Kannan. Adaptive sampling for \(k\)-means clustering. In _Approximation, Randomization, and Combinatorial Optimization. Algorithms and Techniques_, pages 15-28. 2009.
* [2] Sara Ahmadian, Ashkan Norouzi-Fard, Ola Svensson, and Justin Ward. Better guarantees for \(k\)-means and euclidean \(k\)-median by primal-dual algorithms. _SIAM Journal on Computing_, 49(4):17-97, 2019.
* [3] David Arthur and Sergei Vassilvitskii. \(k\)-means++: The advantages of careful seeding. In _Proceedings of the 18th Annual ACM-SIAM Symposium on Discrete Algorithms_, pages 1027-1035, 2007.
* [4] Davin Choo, Christoph Grunau, Julian Portmann, and Vaclav Rozhon. \(k\)-means++: Few more steps yield constant approximation. In _Proceedings of the 37th International Conference on Machine Learning_, volume 119, pages 1909-1917. PMLR, 2020.
* [5] Vincent Cohen-Addad, Hossein Esfandiari, Vahab S. Mirrokni, and Shyam Narayanan. Improved approximations for euclidean \(k\)-means and \(k\)-median, via nested quasi-independent sets. In _Proceedings of the 54th Annual ACM SIGACT Symposium on Theory of Computing_, pages 1621-1628, 2022.
* [6] Vincent Cohen-Addad, Philip N Klein, and Claire Mathieu. Local search yields approximation schemes for \(k\)-means and \(k\)-median in euclidean and minor-free metrics. _SIAM Journal on Computing_, 48(2):644-667, 2019.
* [7] Zachary Friggstad, Mohsen Rezapour, and Mohammad R Salavatipour. Local search yields a ptas for \(k\)-means in doubling metrics. _SIAM Journal on Computing_, 48(2):452-480, 2019.
* [8] Fabrizio Grandoni, Rafail Ostrovsky, Yuval Rabani, Leonard J Schulman, and Rakesh Venkat. A refined approximation for euclidean \(k\)-means. _Information Processing Letters_, 176:106251, 2022.
* [9] Junyu Huang, Qilong Feng, Ziyun Huang, Jinhui Xu, and Jianxin Wang. FLS: A new local search algorithm for K-means with smaller search space. In _Proceedings of the 31st International Joint Conference on Artificial Intelligence_, pages 3092-3098, 2022.
* [10] Tapas Kanungo, David M Mount, Nathan S Netanyahu, Christine D Piatko, Ruth Silverman, and Angela Y Wu. A local search approximation algorithm for \(k\)-means clustering. In _Proceedings of the 18th Annual Symposium on Computational Geometry_, pages 10-18, 2002.
* [11] Silvio Lattanzi and Christian Sohler. A better \(k\)-means++ algorithm via local search. In _Proceedings of the 36th International Conference on Machine Learning_, volume 97, pages 3662-3671. PMLR, 2019.
* [12] Stuart Lloyd. Least squares quantization in PCM. _IEEE Transactions on Information Theory_, 28(2):129-137, 1982.
* [13] Konstantin Makarychev, Aravind Reddy, and Liren Shan. Improved guarantees for \(k\)-means++ and \(k\)-means++ parallel. In _Proceedings of the 34th Conference on Neural Information Processing Systems_, pages 16142-16152, 2020.
* [14] Yusuke Matsui, Keisuke Ogaki, Toshihiko Yamasaki, and Kiyoharu Aizawa. Pqk-means: Billion-scale clustering for product-quantized codes. In _Proceedings of the 25th ACM International Conference on Multimedia_, pages 1725-1733, 2017.
* [15] Jiayang Ren, Kaixun Hua, and Yankai Cao. Global optimal K-medoids clustering of one million samples. In _Proceedings of the 36th Conference on Neural Information Processing Systems_, pages 982-994, 2022.
* [16] Dennis Wei. A constant-factor bi-criteria approximation guarantee for \(k\)-means++. In _Proceedings of the 30th Conference on Neural Information Processing Systems_, pages 604-612, 2016.

## Appendix A Missing Proofs in Section 3

**Lemma 2**.: _Given a type-1 or type-2 matched swap pair \((c_{h}^{*},c_{j})\), it holds that \(\zeta(P,C,c_{h}^{*},c_{j})\leq 24\Delta(P_{j},C^{*})+\frac{1}{5}\Delta(P_{j},C)\). Given a type-1 matched swap set \((Q,V)\), it holds that \(\zeta(P,C,Q,V)\leq 24\Delta(X(V),C^{*})+\frac{1}{5}\Delta(X(V),C)\)._

* **Proof** We first consider a type-1 matched swap set \((Q,V)\). For a data point \(p\in P\), we use \(s_{p}\) to denote its closest center in \(C\). Let \(o_{p}\) be its closest optimal center in \(C^{*}\). Observe that \[\zeta(P,C,Q,V)\leq\sum_{p\in X(V)\setminus Z(J(Q))}d(p,s_{o_{p}}) -d(p,s_{p})\] \[\leq\sum_{p\in X(V)\setminus Z(J(Q))}\left(\sqrt{d(p,o_{p})}+ \sqrt{d(o_{p},s_{o_{p}})}\right)^{2}-d(p,s_{p})\] \[\leq\sum_{p\in X(V)\setminus Z(J(Q))}\left(\sqrt{d(p,o_{p})}+ \sqrt{d(o_{p},s_{p})}\right)^{2}-d(p,s_{p})\] \[\leq\sum_{p\in X(V)\setminus Z(J(Q))}\left(2\sqrt{d(p,o_{p})}+ \sqrt{d(p,s_{p})}\right)^{2}-d(p,s_{p})\] \[\leq\sum_{p\in X(V)\setminus Z(J(Q))}4d(p,o_{p})+2\sqrt{\frac{2} {\lambda}}\sqrt{2\lambda d(p,o_{p})d(p,s_{p})}\] \[\leq\sum_{p\in X(V)\setminus Z(J(Q))}(4+\frac{2}{\lambda})d(p,o_{ p})+2\lambda d(p,s_{p}),\] where the first inequality follows from the fact that \(s_{o_{p}}\) is still in \(C\backslash V\) for each data point \(p\in X(V)\backslash Z(J(Q))\) according to the definition of the swap set defined, the second and fourth steps follow from the triangle inequality, the third step follows from the fact that \(s_{o_{p}}\) is the nearest point to \(o_{p}\) in \(C\), and the last step follows from Cauchy Inequality. Let \(\lambda=\frac{1}{10}\). Then \(\zeta(P,C,Q,V)\leq 24\Delta(X(V),C^{*})+\frac{1}{5}\Delta(X(V),C)\). The cases for matched swap pair are similar to the cases for matched swap set. For a matched swap pair \((c_{h}^{*},c_{j})\), we can get that \(\zeta(P,C,c_{h}^{*},c_{j})\leq 24\Delta(P_{j},C^{*})+\frac{1}{5}\Delta(P_{j},C)\). \(\square\)

**Lemma 3**.: _Let \(P_{h}^{*}\) be an optimal cluster with \(\Delta(P_{h}^{*},C)=b\Delta(P_{h}^{*},\{c_{h}^{*}\})\) for a real number \(b\geq 3\). Then, \(\Delta(\chi(P_{h}^{*}),C)\geq\frac{1}{200}(b-1)\Delta(P_{h}^{*},\{c_{h}^{*}\})\)._

* **Proof** Let \(s_{c_{h}^{*}}\) be the closest center in \(C\) to \(c_{h}^{*}\). Observe that \(d(c_{h}^{*},s_{c_{h}^{*}})\geq\frac{(b-1)\Delta(P_{h}^{*},\{c_{h}^{*}\})}{|P_ {h}^{*}|}\). Otherwise \(\Delta(P_{h}^{*},\{s_{c_{h}^{*}\}\})<b\Delta(P_{h}^{*},\{c_{h}^{*}\})\) holds according to Lemma 1. Consider an arbitrary data point \(p\in\chi(P_{h}^{*})\). By triangle inequality, we have \[\sqrt{\Delta(\{p\},C)}\geq\sqrt{d(c_{h}^{*},s_{c_{h}^{*}})}-\sqrt{d(p,c_{h}^{* })}\geq\sqrt{\frac{\Delta(P_{h}^{*},\{c_{h}^{*}\})}{|P_{h}^{*}|}}(\sqrt{b-1}- \sqrt{1.5}).\] Since \(\sqrt{b-1}\geq\sqrt{2}\), we have \[\Delta(\{p\},C)\geq\left(1-\sqrt{\frac{3}{4}}\right)^{2}\frac{(b-1)\Delta(P_{ h}^{*},\{c_{h}^{*}\})}{|P_{h}^{*}|}.\] Together with the fact that \(|\chi(P_{h}^{*})|\geq\frac{1}{3}|P_{h}^{*}|\), we have \(\Delta(\chi(P_{h}^{*}),C)\geq\frac{b-1}{200}\Delta(P_{h}^{*},\{c_{h}^{*}\})\), which proves the Lemma. \(\square\)

**Lemma 4**.: _By swapping \(q\) with \(c_{q}\), the clustering cost of \(\Delta(P,C)\) can be reduced at least by \(1-\Theta(\frac{1}{k})\)._

**Proof** Observe that \(\Delta(P_{h}^{*},\{q\})<9\Delta(P_{h}^{*},\{c_{h}^{*}\})\) holds by Lemma 1. Then, we can get that

\[\Delta(P,C\backslash\{c_{q}\}\cup\{q\}) =\Delta(P,C)-(\Delta(P,C)-\Delta(P,C\backslash\{c_{q}\}\cup\{q\}))\] \[=\Delta(P,C)-(\Delta(P_{h}^{*},C)+\Delta(P\backslash P_{h}^{*},C) -\Delta(P_{h}^{*},C\backslash\{c_{q}\}\cup\{q\})\] \[-\Delta(P\backslash P_{h}^{*},C\backslash\{c_{q}\}\cup\{q\}))\] \[\leq\Delta(P,C)-(\Delta(P_{h}^{*},C)-\zeta(P,C,c_{h}^{*},c_{q})- 9\Delta(P_{h}^{*},\{c_{h}^{*}\}))\] \[\leq(1-\frac{1}{100k})\Delta(P,C),\]

where the second to the last inequality follows from the fact that \(\Delta(P_{h}^{*},\{q\})<9\Delta(P_{h}^{*},\{c_{h}^{*}\})\), and the last inequality follows from the definition of good single cluster. \(\Box\)

**Lemma 5**.: _If case (2) happens, then \(\Delta(X(L),C)\leq(1+\frac{1}{t})(9\Delta(X(L),C^{*})+24\Delta(X(L^{\prime}), C^{*})+\frac{1}{5}\Delta(X(L^{\prime}),C)+\frac{1}{100}\Delta(P,C))\)._

**Proof** By the condition of case \((2)\), it holds that all the optimal clusters in \(J(L)\) belong to bad single cluster. For any bad single cluster \(P_{h}^{*}\in J(L)\), let \((c_{h}^{*},c_{h}^{m})\in M_{3}\) be the type-2 matched swap pair with minimum reassignment cost, i.e., \((c_{h}^{*},c_{h}^{m})=\arg\min_{(c_{h}^{*},c_{n})\in M_{3}}\zeta(P,C,c_{h}^{*},c_{n})\). According to the definition of bad single cluster, we have

\[\Delta(P_{h}^{*},C)\leq\frac{1}{100k}\Delta(P,C)+9\Delta(P_{h}^{*},\{c_{h}^{*} \})+\zeta(P,C,c_{h}^{*},c_{h}^{m}).\]

Let \(\kappa(M_{3})\) be the set of lonely centers used for constructing type-2 matched swap pair. For an optimal center \(c_{h}^{*}\in C^{*}\), denote \(s_{c_{h}^{*}}\) as the center in \(C\) closest to \(c_{h}^{*}\). Let \(L_{1}=\cup_{c_{h}^{*}\in L}s_{c_{h}^{*}}\). By the definitions of \(\kappa(M_{3})\) and \(L\), for each \(s_{c_{h}^{*}}\in L_{1}\), we can find a set \(z(s_{c_{h}^{*}})\subseteq\kappa(M_{3})\) with size \(|\Psi^{-1}(s_{c_{h}^{*}})|-1\) such that \(z(a)\cap z(b)=\emptyset\) for any \(a,b\in L_{1}\). For each \(s_{c_{h}^{*}}\in L_{1}\), since \(|\Psi^{-1}(s_{c_{h}^{*}})|>t\), it holds that \(|\Psi^{-1}(s_{c_{h}^{*}})|/|z(s_{c_{h}^{*}})|\leq 1+\frac{1}{t}\). By taking a summation over all the centers in \(L\), we have \(|L|=\sum_{c_{h}^{*}\in L}1=\sum_{s_{c_{h}^{*}}\in L_{1}}|\Psi^{-1}(s_{c_{h}^{* }})|\leq\sum_{s_{c_{h}^{*}}\in L_{1}}|z(s_{c_{h}^{*}})|(1+\frac{1}{t})\leq| \kappa(M_{3})|(1+\frac{1}{t})\). Then, by considering all the optimal centers in \(L\) and taking a summation, we have

\[\Delta(X(L),C) =\sum_{c_{h}^{*}\in L}\Delta(P_{h}^{*},C)\leq\sum_{c_{h}^{*}\in L }\frac{1}{100k}\Delta(P,C)+9\Delta(P_{h}^{*},\{c_{h}^{*}\})+\zeta(P,C,c_{h}^{* },c_{h}^{m})\] \[\leq\frac{1}{100}\Delta(P,C)+9\Delta(X(L),C^{*})+\sum_{c_{h}^{*} \in L}\frac{\sum_{c\in\kappa(M_{3})}\zeta(P,C,c_{h}^{*},c)}{|\kappa(M_{3})|}\] \[\leq(1+\frac{1}{t})(9\Delta(Z(J(L)),C^{*})+24\Delta(X(L^{\prime}),C^{*})\] \[+\frac{1}{5}\Delta(X(L^{\prime}),C)+\frac{1}{100}\Delta(P,C)),\]

where the second to the last inequality follows from the fact that \(c_{h}^{m}\) is the center with minimum reassignment cost, and the last inequality follows from Lemma 2 and the fact that \(|L|/|\kappa(M_{3})|\leq 1+t^{-1}\). \(\Box\)

**Lemma 6**.: _Let \(Q\in H_{2}\) be a set of centers in \(H_{2}\), where \(J(Q)\) is a set of good \(t\)-clusters with a type-1 matched swap set \((Q,A^{\prime})\in M_{2}\). Define \(V=A^{\prime}\backslash\{c_{j}\}\), where \(c_{j}\) is the center in \(A^{\prime}\) with \(|\Psi^{-1}(c_{j})|>1\). Let \(U\subseteq P\) be the set of data points with \(|U|=|V|\) such that \(U\cap\chi(P_{h}^{*})\neq\emptyset\) holds for each \(P_{h}^{*}\in Q_{T}\). Then, \(\Delta(P,C\backslash V\cup U)\leq(1-\frac{1}{100k})\Delta(P,C)\)._

**Proof** Consider an arbitrary optimal cluster \(P_{h}^{*}\in Q_{S}^{\prime}\). We know that \(\Delta(P_{h}^{*},C)<3\Delta(P_{h}^{*},\{c_{h}^{*}\})\) by the definition of \(Q_{S}^{\prime}\). Let \(s_{c_{h}^{*}}\) be the closest center in \(C\) to \(c_{h}^{*}\). We first argue that \(s_{c_{h}^{*}}\) is close to \(c_{h}^{*}\) with \(d(c_{h}^{*},s_{c_{h}^{*}})\leq\frac{8\Delta(P_{h}^{*},\{c_{h}^{*}\})}{|P_{h}^{*}|}\). For a data point \(p\in P_{h}^{*}\), let \(s_{p}\) be its closest center in \(C\). Accordingto the triangle inequality, we have

\[d(c_{h}^{*},s_{c_{h}^{*}}) \leq\frac{\sum_{p\in P_{h}^{*}}d(c_{h}^{*},s_{p})}{|P_{h}^{*}|}\leq \frac{1}{|P_{h}^{*}|}\sum_{p\in P_{h}^{*}}(1+\lambda)d(c_{h}^{*},p)+(1+\frac{1}{ \lambda})d(p,s_{p})\] \[\leq\frac{(1+\lambda)\Delta(P_{h}^{*},\{c_{h}^{*}\})+(1+\frac{1}{ \lambda})\Delta(P_{h}^{*},C)}{|P_{h}^{*}|}\] \[\leq\frac{8\Delta(P_{h}^{*},\{c_{h}^{*}\})}{|P_{h}^{*}|},\]

where the last inequality follows from \(\Delta(P_{h}^{*},C)<3\Delta(P_{h}^{*},\{c_{h}^{*}\})\) and feeding \(\lambda=\sqrt{3}\). By assigning all the data points in \(P_{h}^{*}\) to \(s_{c_{h}^{*}}\), we have \(\Delta(P_{h}^{*},\{s_{c_{h}^{*}}\})\leq 9\Delta(P_{h}^{*},\{c_{h}^{*}\})\) by Lemma 1. Then, by swapping \(U\) with \(V\), we have that

\[\Delta(P,C\backslash V\cup U) =\Delta(P,C)-(\Delta(P,C)-\Delta(P,C\backslash V\cup U))\] \[\leq\Delta(P,C)-(\Delta(Z(J(Q)),C)+\Delta(P\backslash Z(J(Q)),C) -\Delta(Z(Q_{T}),U)\] \[-\Delta(Z(Q_{S}^{\prime}),\{s_{c_{h}^{*}}\})-\Delta(P\backslash Z (J(Q)),C\backslash(V\cup\{s_{c_{h}^{*}}\}))\] \[\leq\Delta(P,C)-(\Delta(Z(J(Q)),C)-9\Delta(Z(J(Q)),C^{*})-\zeta( P,C,Q,V))\] \[\leq(1-\frac{1}{100k})\Delta(P,C),\]

where the first inequality follows from the fact that \(s_{c_{h}^{*}}\notin V\) for each \(P_{h}^{*}\in Q_{S}^{\prime}\), the second inequality follows from \(\Delta(P_{h}^{*},\{s_{c_{h}^{*}}\})\leq 9\Delta(P_{h}^{*},\{c_{h}^{*}\})\) for each \(P_{h}^{*}\in Q_{S}^{\prime}\), and the last inequality follows from the definition of good \(t\)-clusters and \(V\cup\{s_{c_{h}^{*}}\}=A^{\prime}\). 

**Lemma 7**.: _Given a set \(Q\in H_{2}\) of centers such that \(J(Q)\) is a set of good \(t\)-clusters, the probability that step 4 of Algorithm 2 fails to sample a data point \(q\) from \(\chi(P_{h}^{*})\) for each \(P_{h}^{*}\in Q_{S}^{\prime\prime}\) in the \(m(P_{h}^{*})\)-th iteration is at most \((1+\frac{1}{30000k})e^{-\Delta(Z(Q_{S}^{\prime\prime}),C)/(300\Delta(P,C))}\)._

**Proof** We first consider an optimal cluster \(P_{h}^{*}\in Q_{S}^{\prime\prime}\). Observe that \(\Delta(P_{h}^{*},C)<\frac{1}{30000k2^{t-1}}\Delta(P,C)\) by the definition of \(Q_{S}^{\prime\prime}\). Define \(p_{f}^{h}\) as the failure probability of not sampling data points from \(\chi(P_{h}^{*})\) in the \(m(P_{h}^{*})\)-th iteration of step 4 in Algorithm 2. Then, we have that

\[p_{f}^{h}=1-\frac{\Delta(\chi(P_{h}^{*}),C)}{\Delta(P,C)}\geq 1-\frac{\Delta(P_{h} ^{*},C)}{\Delta(P,C))}\geq\ 1-\frac{1}{30000k2^{t-1}}.\]

Let \(p_{s}^{h}=1-p_{f}^{h}\) be the success probability. Then, we have

\[p_{s}^{h}=1-p_{f}^{h}\leq(\frac{1}{1-\frac{1}{30000k2^{t-1}}}-1)p_{f}^{h}\leq \frac{\frac{1}{30000}}{k2^{t-1}-\frac{1}{30000}}p_{f}^{h}\leq\frac{1}{30000k(2 ^{t-1}-1)}p_{f}^{h},\]

where the last inequality follows from \(\frac{1}{30000}<k\). Let \(p_{f}(Q_{S}^{\prime\prime})\) denote the probability of not sampling a data point from \(\chi(P_{h}^{*})\) for each \(P_{h}^{*}\in Q_{S}^{\prime\prime}\) in the \(m(P_{h}^{*})\)-th iteration of step 4 in Algorithm 2. For each \(P_{h}^{*}\in Q_{S}^{\prime\prime}\), define \(X_{h}=1\) if the \(m(P_{h}^{*})\)-th iteration of step 4 in Algorithm 2 samples a data point \(q\in\chi(P_{h}^{*})\), and \(X_{h}=0\) if the \(m(P_{h}^{*})\)-th iteration fails to sample a data point \(q\in\chi(P_{h}^{*})\). Let \(E_{1}\) be the event that \(X_{h}=0\) for each \(P_{h}^{*}\in Q_{S}^{\prime\prime}\), and \(E_{2}\) be the event that there are at least two clusters \(P_{h}^{*}\), \(P_{j}^{*}\in Q_{S}^{\prime\prime}\) such that \(X_{h}\lor X_{j}=1\) and \(X_{h}\wedge X_{j}=0\). Observe that there are at most \(2^{t}-2\) subcases if even \(E_{2}\) happens. For each subcase \(s\), let \(\nu(s)=\{P_{h}^{*}:P_{h}^{*}\in Q_{S}^{\prime\prime},X_{h}=1\}\) and \(\nu^{\prime}(s)=\{P_{h}^{*}:P_{h}^{*}\in Q_{S}^{\prime\prime},X_{h}=0\}\), respectively. Define \(P_{r}(s)\) as the probability that subcase \(s\) happens. Since the sampling iterations in step 4 of Algorithm 2 are mutually independent, we have \(P_{r}(s)=\prod_{P_{h}^{*}\in\nu(s)}p_{h}^{h}\prod_{P_{h}^{*}\in\nu^{\prime}(s)}p_ {f}^{h}\). By applying the inequality that \(p_{s}^{h}\leq\frac{1}{30000k(2^{t-1}-1)}p_{f}^{h}\), we have

\[P_{r}(s)\leq(\frac{1}{30000k(2^{t-1}-1)})^{t}\prod_{P_{h}^{*}\in Q_{S}^{\prime \prime}}p_{f}^{h}\leq\frac{1}{30000k(2^{t-1}-1)}\prod_{P_{h}^{*}\in Q_{S}^{\prime \prime}}p_{f}^{h}\]

for each subcase \(s\) of \(E_{2}\), where the last inequality follows from \(\frac{1}{30000k(2^{t-1}-1)}<1\). Then, we have \(P_{r}(E_{2})\leq\frac{1}{30000k}\prod_{P_{h}^{*}\in Q_{S}^{\prime\prime}}p_{f}^{h}\) by taking a probability summation over all \(2^{t}-2\) subcases of event \(E_{2}\). Observe that the failure probability \(p_{f}(Q^{\prime\prime}_{S})=P_{r}(E_{1})+P_{r}(E_{2})\), where \(P_{r}(E_{1})=\prod_{P^{*}_{h}\in Q^{\prime\prime}_{S}}p^{h}_{f}\). Thus, we can get that

\[p_{f}(Q^{\prime\prime}_{S}) \leq(1+\frac{1}{30000k})\prod_{P^{*}_{h}\in Q^{\prime\prime}_{S}}p ^{h}_{f}=(1+\frac{1}{30000k})\prod_{P^{*}_{h}\in Q^{\prime\prime}_{S}}(1-p^{h}_ {s})\] \[=(1+\frac{1}{30000k})\prod_{P^{*}_{h}\in Q^{\prime\prime}_{S}}(1- \frac{\Delta(\chi(P^{*}_{h}),C)}{\Delta(P,C)})\leq(1+\frac{1}{30000k})\prod_{P^ {*}_{h}\in Q^{\prime\prime}_{S}}(1-\frac{\Delta(P^{*}_{h},C)}{300\Delta(P,C)})\] \[\leq(1+\frac{1}{30000k})e^{-\Delta(Z(Q^{\prime\prime}_{S}),C))/3 00\Delta(P,C)},\]

where the second to last inequality follows from Lemma 3, and the last inequality follows from the inequality \(1-x\leq e^{-x}\). 

**Lemma 8**.: _For the optimal clusters in \(H_{G}\), we have \(\Delta(Z(H_{G}),C)\geq\frac{1}{100}\Delta(P,C)\)._

**Proof** Given any type-1 matched swap set \((Q,V)\), if \(J(Q)\) is a set of good \(t\)-clusters with a type-1 matched swap set \((Q,V)\in M_{2}\), according to the assumption of subcase (2), we know that each optimal cluster \(P^{*}_{h}\) in \(Q_{L}\) is a bad single cluster with a swap pair \((c^{*}_{h},c_{l})\), where \(c_{l}\) is a lonely center in \(V\). Since \(J(Q)\) is a set of good \(t\)-clusters, not all optimal clusters in \(J(Q)\) are bad single clusters. Hence, for each optimal cluster \(P^{*}_{h}\in Q_{L}\), we can find a lonely center \(l(c^{*}_{h})\in V\) such that \(P^{*}_{h}\) is a bad single cluster with the swap pair \((c^{*}_{h},l(c^{*}_{h}))\). By the definition of bad single cluster, we have

\[\Delta(P^{*}_{h},C)\leq\frac{1}{100k}\Delta(P,C)+9\Delta(P^{*}_{h},\{c^{*}_{h} \})+\zeta(P,C,c^{*}_{h},l(c^{*}_{h})).\]

For each optimal cluster \(P^{*}_{h}\in Q^{\prime}_{S}\), by the definition of \(Q^{\prime}_{S}\), it holds that \(\Delta(P^{*}_{h},C)\leq 3\Delta(P^{*}_{h},\{c^{*}_{h}\})\). If \(J(Q)\) is a set of bad \(t\)-clusters, by the definition of bad \(t\)-clusters, we also have

\[\Delta(Z(J(Q)),C)\leq\frac{1}{100k}\Delta(P,C)+9\Delta(Z(J(Q)),C^{*})+\zeta(P, C,Q,V).\]

Define \(H^{1}_{B}=\{c^{*}_{h}\in H_{1}\,:\,P^{*}_{h}\text{ is a bad single cluster with the swap pair }(c^{*}_{h},c_{j})\,\in\,M_{1}\}\), \(H^{2}_{B}=\{Q\in H_{2}:J(Q)\text{ is a set of bad $t$-clusters with the swap set }(Q,V)\in M_{2}\}\) and \(H^{2}_{B^{\prime}}=\{Q^{\prime}_{S}\cup Q_{L}:Q\in H_{2},J(Q)\text{ is a set of good $t$-clusters with the swap set }(Q,V)\in M_{2}\}\), respectively. Putting all things together and taking a summation over all clusters in \(H_{B}\), we can get that

\[\Delta(Z(H_{B}),C) =\sum_{P^{*}_{h}\in H_{B}}\Delta(P^{*}_{h},C)\] \[=\sum_{c^{*}_{h}\in H^{1}_{B}}\Delta(P^{*}_{h},C)+\sum_{Q\in H^{ 2}_{B}}\Delta(Z(J(Q)),C)+\sum_{Q^{\prime}\in H^{2}_{B^{\prime}}}\Delta(Z(Q^{ \prime}),C)\] \[\leq\sum_{P^{*}_{h}\in H_{B}}\frac{1}{100k}\Delta(P,C)+9\Delta(P^ {*}_{h},\{c^{*}_{h}\})+24\Delta(X(H^{\prime}_{1}),C^{*})+\frac{1}{5}\Delta(X( H^{\prime}_{1}),C)\] \[+24\Delta(X(H^{\prime}_{2}),C^{*})+\frac{1}{5}\Delta(X(H^{\prime} _{2}),C)\] \[\leq\frac{1}{100}\Delta(P,C)+9\Delta(Z(H_{B}),C^{*})+24\Delta(X( H^{\prime}_{1}),C^{*})+\frac{1}{5}\Delta(X(H^{\prime}_{1}),C)\] \[+24\Delta(X(H^{\prime}_{2}),C^{*})+\frac{1}{5}\Delta(X(H^{\prime} _{2}),C),\]where the first inequality follows from Lemma 3. Then, together with Lemma 5, we can get that

\[\Delta(Z(H_{G}),C) \geq\Delta(P,C)-\Delta(Z(J(L)),C)-\Delta(Z(H_{B}),C)\] \[\geq\Delta(P,C)-(1+\frac{1}{t})(9\Delta(Z(J(L)),C^{*})+24\Delta(X( L^{\prime}),C^{*})+\frac{1}{5}\Delta(X(L^{\prime}),C)\] \[+\frac{1}{100}\Delta(P,C)+\frac{1}{100}\Delta(P,C)+9\Delta(Z(H_{B }),C^{*})+24\Delta(X(H_{1}^{\prime}),C^{*})\] \[+\frac{1}{5}\Delta(X(H_{1}^{\prime}),C))+24\Delta(X(H_{2}^{\prime }),C^{*})+\frac{1}{5}\Delta(X(H_{2}^{\prime}),C))\] \[\geq\Delta(P,C)-(1+\frac{1}{t})(\frac{1}{50}\Delta(P,C)+\frac{1}{ 5}\Delta(P,C)+33Opt)\] \[\geq\Delta(P,C)-\frac{33}{100}\Delta(P,C)-\frac{33}{50}\Delta(P,C)\] \[\geq\frac{1}{100}\Delta(P,C),\]

where the second inequality follows from Lemma 5, the third inequality follows from the fact that \(H_{1}^{\prime}\cap H_{2}^{\prime}\cap L^{\prime}=\emptyset\) and \(Z(H_{B})\cap J(L)=\emptyset\), and the fourth inequality follows from \(t\geq 2\) and the assumption that \(\Delta(P,C)\geq 50(1+\frac{1}{t})Opt\). 

**Theorem 2**.: _In the \(i\)-th iteration of Algorithm 2, let \(C^{\prime}\) be the set of centers obtained in step 7. If the current clustering cost \(\Delta(P,C)\) is larger than \(50(1+\frac{1}{t})Opt\), then with probability at least \(\Omega(k^{-t})\), we have \(\Delta(P,C^{\prime})\leq(1-\frac{1}{100k})\Delta(P,C)\). After \(O(k^{O(t)}\log(\epsilon^{-1}\log k))\) iterations, we get an approximate solution with ratio \((50(1+\frac{1}{t})+\epsilon)\) in expectation._

* Let \(T=\delta k^{t+1}\log(24\epsilon^{-1}\log k)\), where \(\delta\) is a sufficient large constant. Following the work in [11], we define another random process \(X\) with initial clustering cost \(\Delta(P,C^{\prime})\) for a set \(C^{\prime}\) returned by the \(k\)-means++ algorithm such that for \(T\) iterations of sampling and swaps, it reduces the value of \(\Delta(P,C^{\prime})\) by at least \((1-\frac{1}{100k})\) with probability \(\lambda k^{-t}\), and it increases the final value of \(\Delta(P,C^{\prime})\) by \(50(1+\frac{1}{t})OPT\), where \(\lambda\) is a constant with \(\lambda<\frac{\delta}{100}\). It is easy to see that \(E[\Delta(P,C)]<E[X]\). Then, we have \[E[X] =50(1+\frac{1}{t})Opt\] \[+\Delta(P,C^{\prime})\sum_{i=1}^{T}\binom{T}{i}(\frac{1}{\lambda k ^{t}})^{i}(1-\frac{1}{\lambda k^{t}})^{T-i}(1-\frac{1}{100k})^{i}\] \[=\Delta(P,C^{\prime})(1-\frac{1}{100\lambda k^{t+1}})^{T}+50(1+ \frac{1}{t})Opt\] \[\leq\frac{\epsilon\Delta(P,C^{\prime})}{24\log k}+50(1+\frac{1}{ t})Opt.\] This implies that \(E[\Delta(P,C)|C^{\prime}]\leq\frac{\epsilon\Delta(P,C^{\prime})}{24\log k}+50(1+\frac{1}{t})Opt\). Then, we can get that \[E[\Delta(P,C)] =\sum_{C^{\prime}}E[\Delta(P,C)|C^{\prime}]P_{r}(C^{\prime})\] \[\leq\sum_{C^{\prime}}P_{r}(C^{\prime})(\frac{\epsilon\Delta(P,C^ {\prime})}{24\log k}+50(1+\frac{1}{t})Opt)\] \[\leq\frac{\epsilon E[\Delta(P,C^{\prime})]}{24\log k}+50(1+\frac{1 }{t})Opt.\] Since the \(k\)-means++ algorithm provides an approximation ratio of \(8(\log k+2)\) in expectation by Theorem 1, we have \(E[\Delta(P,C)]\leq(50(1+\frac{1}{t})+\epsilon)Opt\). 

**Running Time Analysis.** By Theorem 2, in order to obtain a \((50(1+\frac{1}{t})+\epsilon)\)-approximate solution, the iteration rounds for Algorithm 2 should be \(O(k^{t+1}\log(\epsilon^{-1}\log k))\). In each iteration, it takes \(O(ndk)\) time to update the distances between data points to their closest centers. During the sampling 

[MISSING_PAGE_FAIL:17]

\begin{table}
\begin{tabular}{c c c c} \hline \hline Failure & Best Cost & Average Cost & Time(s) \\ \hline
3 & 131.7408 & 133.8048\(\pm\)2.5306 & 39.49 \\
4 & 131.7408 & 132.6853\(\pm\)1.5759 & 48.01 \\
5 & 131.7408 & 132.8304\(\pm\)1.6403 & 59.41 \\
6 & 131.7408 & 132.7752\(\pm\)1.1013 & 55.65 \\
7 & 131.7408 & 132.2285\(\pm\)0.8758 & 60.03 \\
8 & 131.7408 & 132.4954\(\pm\)0.6819 & 69.60 \\
9 & 131.7408 & 132.3064\(\pm\)0.6889 & 72.11 \\
10 & 131.7408 & 132.2246\(\pm\)0.6323 & 85.36 \\ \hline \hline \end{tabular}
\end{table}
Table 6: Results of MLSP Algorithm on dataset rds with varying number of failure upper bound \(R^{\prime}\)

\begin{table}
\begin{tabular}{c c c c c c c c c} \hline \hline Method & Rounds & Best Cost & Average Cost & Time(s) & Method & Rounds & Best Cost & Average Cost & Time(s) \\ \hline LS+ & & 6.1871E+07.6 & 6.212E+07.13E+06 & 2.13 & LS+ & 6.1619E+07.6 & 6.3341E+07.18E+06 & 5.64 \\ MLS & 100 & 6.1587E+07.7259E+07.11E+07 & **2.11** & MLS & 200 & 16.6884E+07.6 & 41542E+07.22E+06 & **4.00** \\ MLSP & **6.1534E+07** & **6.1559E+07.49E+04** & 22.22 & MLSP & **6.1534E+07** & **6.1534E+07** & **6.1534E+07** & 36.08 \\ \hline LS++ & & 6.1642E+07 & 6.2732E+07.17E+06 & 10.41 & LS+ & & 6.1547E+07.6 & 6.3276E+07.19E+06 & 16.58 \\ MLS & 300 & 6.1939E+07.6357E+07.27E+06 & **6.32** & MLS & 400 & 6.1547E+07.6 & 6.3255E+07.18E+06 & **7.98** \\ MLSP & **6.1534E+07** & **6.1535E+07** & **4.96** & **4.6** & MLSP & **6.1534E+07** & **6.1546E+07** & **6.1546E+07** & 5.13 \\ \hline LS++ & & 6.1547E+07 & 6.2891E+07.16E+06 & 24.15 & LS+ & & 6.1547E+07.6 & 2.8784E+07.16E+06 & 32.97 \\ MLS & 500 & 6.1661E+07 & 6.3595E+07.17E+06 & **10.54** & MLS & 600 & **6.1534E+07** & 6.4042E+07.29E+06 & **12.95** \\ MLSP & **6.1534E+07** & **6.1559E+07.49E+04** & 63.61 & MLSP & **6.1534E+07** & **6.1527E+07** & **6.1572E+07** & 7.74 \\ \hline LS++ & & **6.1544E+07** & 6.2824E+07.13E+06 & 43.03 & LS++ & & 6.1547E+07 & 6.261E+07.55E+05 & 54.28 \\ MLS & 700 & 6.1587E+07.6113E+07.34E+06 & **13.97** & MLS & 800 & 6.1953E+07 & 6.5338E+07+3.5E+06 & **16.22** \\ MLSP & **6.1534E+07** & **6.1547E+07** & **3.75E+04** & 86.74 & MLSP & **6.1534E+07** & **6.1559E+07** & **6.1559E+07** & **6.39E+04** & 96.73 \\ \hline \hline \end{tabular}
\end{table}
Table 4: Comparison results on dataset KEGG with varying iteration rounds

\begin{table}
\begin{tabular}{c c c c c} \hline \hline Failure & Best Cost & Average Cost & Time(s) \\ \hline
3 & 6.1534E+07 & 6.1559E+07\(\pm\)4.9E+04 & 44.52 \\
4 & 6.1534E+07 & 6.1559E+07\(\pm\)4.9E+04 & 59.61 \\
5 & 6.1534E+07 & 6.1559E+07\(\pm\)4.9E+04 & 64.45 \\
6 & 6.1534E+07 & 6.1559E+07\(\pm\)4.9E+04 & 70.69 \\
7 & 6.1534E+07 & 6.1534E+07 & 76.37 \\
8 & 6.1534E+07 & 6.1534E+07 & 76.37 \\
9 & 6.1534E+07 & 6.1534E+07 & 91.05 \\
10 & 6.1534E+07 & 6.1534E+07 & 100.82 \\
10 & 6.1534E+07 & 6.1534E+07 & 105.89 \\ \hline \hline \end{tabular}
\end{table}
Table 7: Results of MLSP Algorithm on dataset rds with varying number of failure upper bound \(R^{\prime}\)

\begin{table}
\begin{tabular}{c c c c} \hline \hline Failure & Best Cost & Average Cost & Time(s) \\ \hline
3 & 6.1534E+07 & 6.1559E+07\(\pm\)4.9E+04 & 44.52 \\
4 & 6.1534E+07 & 6.1559E+07\(\pm\)4.9E+04 & 59.61 \\
5 & 6.1534E+07 & 6.1559E+07\(\pm\)4.9E+04 & 64.45 \\
6 & 6.1534E+07 & 6.1559E+07\(\pm\)4.9E+04 & 70.69 \\
7 & 6.1534E+07 & 6.1534E+07 & 76.37 \\
8 & 6.1534E+07 & 6.1534E+07 & 91.05 \\
9 & 6.1534E+07 & 6.1534E+07 & 100.82 \\
10 & 6.1534E+07 & 6.1534E+07 & 105.89 \\ \hline \hline \end{tabular}
\end{table}
Table 5: Comparison results on dataset Urban_10 with varying iteration rounds

\begin{table}
\begin{tabular}{c c c c} \hline \hline eta/epsilon & Best Cost & Average Cost & Time(s) \\ \hline
0.25/0.25 & 6.1534E+07 & 6.1572E+07\(\pm\)5.7E+04 & 85.99 \\
0.5/0.25 & 6.1534E+07 & 6.1546E+07\(\pm\)3.7E+04 & 78.57 \\
0.75/0.25 & 6.1534E+07 & 6.1572E+07\(\pm\)5.7E+04 & 72.15 \\
0.25/0.5 & 6.1534E+07 & 6.1572E+07\(\pm\)5.7E+04 & 68.67 \\
0.5/0.5 & 6.1534E+07 & 6.1546E+07\(\pm\)3.7E+04 & 65.05 \\
0.75/0.5 & 6.1534E+07 & 6.1559E+07\(\pm\)4.9E+04 & 62.09 \\
0.25/0.75 & 6.1534E+07 & 6.1559E+07\(\pm\)4.9E+04 & 65.15 \\
0.5/0.75 & 6.1534E+07 & 6.1546E+07\(\pm\)3.7E+04 & 63.55 \\
0.75/0.75 & 6.1534E+07 & 6.1546E+07\(\pm\)3.7E+04 & 64.35 \\ \hline \hline \end{tabular}
\end{table}
Table 10: Results of MLSP algorithm on dataset KEGG with varying parameters \(\epsilon\) and \(\eta\)

\begin{table}
\begin{tabular}{c c c c} \hline \hline Failure & Best Cost & Average Cost & Time(s) \\ \hline
3 & 24659.0733 & 24824.3035\(\pm\)214.2367 & 41.44 \\
4 & 24659.0733 & 24819.0528\(\pm\)238.5684 & 56.65 \\
5 & 24659.0733 & 24755.9209\(\pm\)132.1597 & 59.97 \\
6 & 24659.0733 & 24806.0814\(\pm\)158.7277 & 66.19 \\
7 & 24659.0733 & 24829.4308\(\pm\)190.0205 & 80.24 \\
8 & 24659.0733 & 24769.4947\(\pm\)58.5737 & 88.09 \\
9 & 24659.0733 & 24703.7889\(\pm\)55.6132 & 92.57 \\
10 & 24659.0733 & 24684.7586\(\pm\)51.1046 & 95.37 \\ \hline \hline \end{tabular}
\end{table}
Table 8: Results of MLSP Algorithm on dataset Uran_10 with varying number of failure upper bound \(R^{\prime}\)

\begin{table}
\begin{tabular}{c c c c} \hline \hline eta/epsilon & Best Cost & Average Cost & Time(s) \\ \hline
0.25/0.25 & 24659.0733 & 24681.3945\(\pm\)240.9711 & 66.64 \\
0.5/0.25 & 24659.0733 & 24837.5858\(\pm\)203.3399 & 72.86 \\
0.75/0.25 & 24659.0733 & 24786.8942\(\pm\)168.9089 & 59.95 \\
0.25/0.5 & 24659.0733 & 24671.9425\(\pm\)38.3418 & 60.31 \\
0.5/0.5 & 24659.0733 & 24756.0949\(\pm\)134.5952 & 63.27 \\
0.75/0.5 & 24659.0733 & 24746.6868\(\pm\)146.2990 & 56.24 \\
0.25/0.75 & 24659.0733 & 24719.8526\(\pm\)61.3117 & 57.11 \\
0.5/0.75 & 24659.0733 & 24804.6754\(\pm\)173.0019 & 54.95 \\
0.75/0.75 & 24659.0733 & 24753.6144\(\pm\)204.2255 & 56.56 \\ \hline \hline \end{tabular}
\end{table}
Table 11: Results of MLSP algorithm on dataset Urban_10 with varying parameters \(\epsilon\) and \(\eta\)

\begin{table}
\begin{tabular}{c c c c} \hline \hline eta/epsilon & Best Cost & Average Cost & Time(s) \\ \hline
0.25/0.25 & 24659.0733 & 24681.3945\(\pm\)214.2367 & 41.44 \\
4 & 24659.0733 & 24819.0528\(\pm\)238.5684 & 56.65 \\
5 & 24659.0733 & 24755.9209\(\pm\)132.1597 & 59.97 \\
6 & 24659.0733 & 24806.0814\(\pm\)158.7277 & 66.19 \\
7 & 24659.0733 & 24829.4308\(\pm\)190.0205 & 80.24 \\
8 & 24659.0733 & 24679.4947\(\pm\)58.5737 & 88.09 \\
9 & 24659.0733 & 24703.7889\(\pm\)55.6132 & 92.57 \\
10 & 24659.0733 & 24684.7586\(\pm\)51.1046 & 95.37 \\ \hline \hline \end{tabular}
\end{table}
Table 9: Results of MLSP Algorithm on dataset rds with varying parameters \(\epsilon\) and \(\eta\)

### Experiments on Small Datasets

In this section, we present the experiments of the performances of our proposed MLSP algorithm on other datasets with sizes smaller than \(50,000\) used in [15].

Table 12 and Table 13 show the results on 18 datasets with sizes smaller than \(50,000\) using \(k=10\). It can be seen that, on each dataset, the best clustering cost returned by our MLSP algorithm is smaller than BB method (note that BB method already guarantees a gap smaller than 0.1% to the optimal solutions on small datasets) and other local search methods. On each dataset, the average clustering cost returned by our MLSP algorithm nearly matches the result of the BB method. As for FLS, although the performance of FLS is better than that of LS++, our proposed MLSP algorithm improves the performance of FLS on clustering cost with smaller deviation for most datasets. As for running time, there is no significant difference on the running time among different local search algorithms for small datasets.

\begin{table}
\begin{tabular}{c c c c c c} \hline Method & Size & BB(Cost) & Best & Mean & Time(s) \\ \hline LS++ & & & 30.01 & 30.45\(\pm\)0.53 & **0.04** \\ FLS & & & 29.79 & 29.91\(\pm\)0.14 & 0.15 \\ LLOYD & Iris(150*4) & 29.79(735s) & 30.32 & 30.90\(\pm\)0.4 & **0.01** \\ MLSP & & & **29.74** & **29.76\(\pm\)0.33** & 6.61 \\ MLS & & & 29.93 & 30.21\(\pm\)0.19 & 0.17 \\ \hline LS++ & & & 216.36 & 219.43\(\pm\)2.84 & **0.37** \\ FLS & & & **214.52** & 216.68\(\pm\)2.83 & 1.78 \\ LLOYD & SEEDS(210*7) & 218.49(448s) & 215.65 & 219.99\(\pm\)2.93 & **0.12** \\ MLSP & & & **214.52** & **215.08\(\pm\)0.49** & 8.07 \\ MLS & & & 214.95 & 219.29\(\pm\)2.82 & 0.46 \\ \hline LS++ & & & **251.86** & 254.17\(\pm\)1.71 & **0.37** \\ FLS & & & **251.86** & 252.05\(\pm\)0.56 & 1.61 \\ LLOYD & GLASS(214*9) & 251.86(2566s) & 253.25 & 259.94\(\pm\)3.94 & **0.11** \\ MLSP & & & **251.86** & **251.99\(\pm\)0.54** & 7.25 \\ MLS & & & 253.29 & 254.00\(\pm\)0.55 & 0.46 \\ \hline LS++ & & & 377753 & 392980\(\pm\)4437 & **0.37** \\ FLS & & & **375974** & 377496\(\pm\)1934 & 1.71 \\ LLOYD & BM(249*6) & 375974(1204s) & 376590 & 384475\(\pm\)5560 & **0.07** \\ MLSP & & & **375974** & **376276\(\pm\)265** & 6.81 \\ MLS & & & 378649 & 384982\(\pm\)4131 & 0.37 \\ \hline LS++ & & & 29.57 & 30.16\(\pm\)0.29 & **0.38** \\ FLS & & & **29.27** & 29.38\(\pm\)0.15 & 1.73 \\ LLOYD & UK(258*5) & 29.28(4h) & 29.94 & 30.33\(\pm\)0.26 & **0.08** \\ MLSP & & & **29.27** & **29.29\(\pm\)0.01** & 7.14 \\ MLS & & & 29.41 & 30.00\(\pm\)0.27 & 0.39 \\ \hline LS++ & & & **6.96E+10** & 7.01E+10\(\pm\)5.7E+08 & **0.39** \\ FLS & & & **6.96E+10** & 7.06E+10\(\pm\)3.1E+09 & 1.77 \\ LLOYD & HF(299*12) & 6.96E+10(4h) & **6.96E+10** & 6.97E+10\(\pm\)2.3E+08 & **0.09** \\ MLSP & & & **6.96E+10** & **6.96E+10\(\pm\)0** & 8.56 \\ MLS & & & **6.96E+10** & 7.01E+10\(\pm\)4.3E+08 & 0.4 \\ \hline LS++ & & & **3.36E+10** & 3.45E+10\(\pm\)6.0E+08 & **0.4** \\ FLS & & & **3.36E+10** & 3.41E+10\(\pm\)4.5E+08 & 2.01 \\ LLOYD & WHO(440*8) & 3.40E+10(4h) & 3.44E+10 & 3.49E+10\(\pm\)7.4E+08 & **0.11** \\ MLSP & & & **3.36E+10** & **3.37E+10\(\pm\)4.6E+07** & 9.17 \\ MLS & & & 3.42E+10 & 3.52E+10\(\pm\)5.6E+08 & **0.4** \\ \hline LS++ & & & 1.1315E+06 & 1.1454E+06\(\pm\)8.7E+03 & **0.45** \\ FLS & & & 1.1312E+06 & 1.1412E+06\(\pm\)2.4E+04 & 1.81 \\ LLOYD & HCV(572*12) & 1.1315E+06(4h) & 1.1329E+06 & 1.1538E+06\(\pm\)2.9E+04 & **0.15** \\ MLSP & & & **1.1311E+06** & **1.1410E+06\(\pm\)2.2E+04** & 11.05 \\ MLS & & & 1.1505E+06 & 1.2135E+06\(\pm\)4.2E+04 & 0.9 \\ \hline LS++ & & & **1.0786E+06** & 1.0983E+06\(\pm\)1.1E+04 & 0.51 \\ FLS & & & **1.0786E+06** & 1.0816E+06\(\pm\)5.3E+03 & 2.11 \\ LLOYD & Abs(740*21) & 1.0786E+06(4h) & 1.0824E+06 & 1.1209E+06\(\pm\)2.1E+04 & **0.12** \\ MLSP & & & **1.0786E+06** & **1.0786E+06\(\pm\)0** & 13.42 \\ MLS & & & **1.0789E+06** & 1.0979E+05\(\pm\)1.1E+04 & **0.47** \\ \hline \end{tabular}
\end{table}
Table 12: Comparison results on clustering costs and running time with \(k=10\) on datasets with sizes ranging from 150 to 740

\begin{table}
\begin{tabular}{c c c c c c} \hline \hline Method & Size & BB(Cost) & Best & Mean & Time(s) \\ \hline LS++ & & & 776.04 & 793.74\(\pm\)7.3 & **0.51** \\ FLS & & & **762.16** & 766.38\(\pm\)4.9 & 2.14 \\ LLOYD & TR(980*10) & 772.47(4h) & 770.72 & 776.48\(\pm\)5.0 & **0.12** \\ MLSP & & & **762.16** & **764.71\(\pm\)1.9** & 11.89 \\ MLS & & & 785.01 & 805.89\(\pm\)11.5 & 0.56 \\ \hline LS++ & & & **1.1734E+08** & 1.2124E+08\(\pm\)3.4E+05 & **0.58** \\ FLS & & & **1.1734E+08** & 1.2092E+08\(\pm\)3.9E+05 & 2.35 \\ LLOYD & SGC(1000*21) & 1.1742E+08(4h) & **1.1734E+08** & **1.1749E+08\(\pm\)1.7E+05** & **0.09** \\ MLSP & & & **1.1734E+08** & 1.1752E+08\(\pm\)5.1E+04 & 13.45 \\ MLS & & & 1.1735E+08 & 1.1846E+08\(\pm\)9.6E+04 & 0.62 \\ \hline LS++ & & & 2.7116E+06 & 2.7657E+06\(\pm\)4.4E+04 & **0.58** \\ FLS & & & 2.7073E+06 & 2.8519E+06\(\pm\)1.2E+04 & 2.51 \\ LLOYD & HEMI(1995*7) & 2.7421E+06(4h) & 2.7144E+06 & 2.7349E+06\(\pm\)6.7E+03 & **0.18** \\ MLSP & & & **2.7070E+06** & **2.7123E+06\(\pm\)6.7E+03** & 13.39 \\ MLS & & & 2.7292E+06 & 2.7886E+06\(\pm\)6.9E+04 & 0.59 \\ \hline LS++ & & & 5.3689E+09 & 5.4772E+09\(\pm\)1.0E+08 & **0.58** \\ FLS & & & 5.3610E+09 & 5.4256E+09\(\pm\)5.6E+07 & 2.65 \\ LLOYD & pr2392(2392*2) & 5.3578E+09(4h) & 5.3599E+09 & **5.3624E+09\(\pm\)2.3E+07** & **0.14** \\ MLSP & & & **5.3578E+09** & 5.3668E+09\(\pm\)2.3E+07 & 10.76 \\ MLS & & & 5.3629E+09 & 5.4203E+09\(\pm\)6.2E+07 & **0.58** \\ \hline LS++ & & & 1.3902E+05 & 1.4155E+05\(\pm\)1.5E+03 & **0.62** \\ FLS & & & 1.3829E+05 & 1.4018E+05\(\pm\)1.1E+03 & 3.43 \\ LLOYD & TRR(5456*24) & 1.3796E+05(4h) & 1.3868E+05 & 1.4177E+05\(\pm\)2.1E+03 & **0.18** \\ MLSP & & & **1.3796E+05** & **1.3829E+05\(\pm\)2.5E+02** & 25.48 \\ MLS & & & 1.4591E+05 & 1.4939E+05\(\pm\)2.0E+03 & 0.64 \\ \hline LS++ & & & 1167.6 & 1184.01\(\pm\)15.88 & **0.58** \\ FLS & & & **1163.7** & 1174.1\(\pm\)13.06 & 5.06 \\ LLOYD & AC(7195*22) & 1181.7(4h) & 1169 & 1172.1\(\pm\)6.05 & **0.36** \\ MLSP & & & **1163.7** & **1168.2\(\pm\)9.01** & 22.32 \\ MLS & & & 1165.5 & 1181.7\(\pm\)12.8 & 0.64 \\ \hline LS++ & & & 1.6104E+06 & 1.6508E+06\(\pm\)2.4E+05 & **0.24** \\ FLS & & & **1.6099E+06** & 1.6196E+06\(\pm\)7.3E+04 & 2.26 \\ LLOYD & rds\_cnt(10000*4) & 1.6119E+06(4h) & **1.6009E+06** & 1.6247E+06\(\pm\)1.4E+05 & **0.11** \\ MLSP & & & **1.6009E+06** & **1.6105E+06\(\pm\)7.2E+02** & 18.67 \\ MLS & & & 1.6146E+06 & 1.6520E+06\(\pm\)2.5E+05 & 0.42 \\ \hline LS++ & & & 1.8286E+07 & 1.8421E+07\(\pm\)1.1E+04 & **0.28** \\ FLS & & & 1.8269E+07 & 1.8404E+07\(\pm\)1.3E+04 & 0.94 \\ LLOYD & HTRU2(17898*8) & 1.8723E+07(4h) & **1.8266E+07** & 1.8380E+07\(\pm\)2.0E+05 & **0.15** \\ MLSP & & & **1.8266E+07** & **1.8312E+07\(\pm\)4.2E+03** & 24.43 \\ MLS & & & 1.8275E+07 & 1.8569E+07\(\pm\)1.7E+04 & 0.3 \\ \hline LS++ & & & 9.0239E+06 & 9.1784E+06\(\pm\)9.3E+04 & 7.06 \\ FLS & & & **8.9904E+06** & 9.0384E+06\(\pm\)5.4E+04 & 19.87 \\ LLOYD & GT(36733*11) & 8.9909E+06(4h) & 9.0001E+06 & 9.1229E+06\(\pm\)7.7E+04 & **0.33** \\ MLSP & & & **8.9904E+06** & **9.0131E+06\(\pm\)3.6E+04** & 47.51 \\ MLS & & & 9.0239E+06 & 9.1784e+06\(\pm\)9.3E+04 & **6.29** \\ \hline \hline \end{tabular}
\end{table}
Table 13: Comparison results on clustering costs and running time with \(k=10\) on datasets with sizes ranging from 980 to 36733

### Experiments with Different Values of \(k\)

In this section, we present the experiments of the performances of our proposed MLSP and MLS algorithms on different datasets with different values of \(k\). Tables 14 and 15 present the performances of different local search algorithms with \(k=3\) on small datasets. Table 18 presents the performances of different local search algorithms with \(k=3\) on large datasets. Tables 16 and 17 present the performances of different local search algorithms with \(k=5\) on small datasets. Table 19 presents the performances of different local search algorithms with \(k=5\) on large datasets.

It can be seen that, compared with other local search methods, our proposed MLSP algorithm achieves the best clustering performance on most datasets with different values of \(k\). On each dataset, the best clustering cost returned by our MLSP algorithm matches the clustering cost of BB method (note that BB method already guarantees a gap of \(0.1\%\) to the optimal solution). On each dataset, the average clustering cost returned by our MLSP algorithm nearly matches the result of the BB method. As for running time, there is no significant difference on running time among all local search algorithms on small datasets. However, as the data sizes grow, our proposed MLS algorithm becomes much faster than other local search algorithms.

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline Method & Size & BB(Cost) & Best & Mean & Time(s) \\ \hline LS++ & & & **83.96** & 84.82\(\pm\)1.05 & **0.26** \\ FLS & & & **83.96** & **83.96\(\pm\)0** & 0.5 \\ LLOYD & Iris(150*4) & 83.96(93s) & 84.68 & 84.68\(\pm\)0 & **0.09** \\ MLSP & & & **83.96** & **83.96\(\pm\)0** & 3.46 \\ MLS & & & **83.96** & 84.52\(\pm\)0.73 & 0.43 \\ \hline LS++ & & & **598.29** & 600.83\(\pm\)5.59 & **0.27** \\ FLS & & & **598.29** & **598.29\(\pm\)0** & 0.51 \\ LLOYD & SEEDS(210*7) & 598.29(84s) & **598.29** & 598.30\(\pm\)0.69 & **0.07** \\ MLSP & & & **598.29** & **598.29\(\pm\)0** & 3.34 \\ MLS & & & **598.29** & 600.70\(\pm\)5.63 & 0.67 \\ \hline LS++ & & & **629.02** & 629.51\(\pm\)0.73 & **0.27** \\ FLS & & & **629.02** & **629.02\(\pm\)0** & 0.62 \\ LLOYD & GLASS(214*9) & 692.02(107s) & **629.02** & **629.02\(\pm\)0** & **0.05** \\ MLSP & & & **629.02** & **629.02\(\pm\)0** & 3.37 \\ MLS & & & **629.02** & 630.72\(\pm\)5.08 & 0.68 \\ \hline LS++ & & & **8.63E+05** & 8.69E+05\(\pm\)6.4E+03 & **0.27** \\ FLS & & & **8.63E+05** & **8.63E+05\(\pm\)0** & 0.57 \\ LLOYD & BM(249*6) & 8.63E+05(86s) & 8.65E+05 & 8.65E+05\(\pm\)0 & **0.03** \\ MLSP & & & **8.63E+05** & **8.63E+05\(\pm\)0** & 3.69 \\ MLS & & & **8.63E+05** & 8.74E+05\(\pm\)7.3E+03 & 0.67 \\ \hline LS++ & & & 50.92 & 51.95\(\pm\)0.71 & **0.29** \\ FLS & & & **50.77** & 50.83\(\pm\)0.12 & 0.54 \\ LLOYD & UK(258*5) & 50.77(89s) & 51.29 & 51.31\(\pm\)0.03 & **0.04** \\ MLSP & & & **50.77** & **50.77\(\pm\)0** & 3.67 \\ MLS & & & **50.77** & 51.78\(\pm\)0.81 & 0.67 \\ \hline LS++ & & & **7.83E+11** & 7.90E+11\(\pm\)6.5E+10 & **0.27** \\ FLS & & & **7.83E+11** & 7.84E+11\(\pm\)2.0E+10 & 0.56 \\ LLOYD & HF(299*12) & 7.83E+11(107s) & **7.83E+11** & 7.83E+11\(\pm\)6.3E+07 & **0.05** \\ MLSP & & & **7.83E+11** & **7.83E+11\(\pm\)3.3E+07** & 4.02 \\ MLS & & & **7.83E+11** & 7.86E+11\(\pm\)3.4E+09 & 0.74 \\ \hline LS++ & & & **8.33E+10** & 8.43E+10\(\pm\)6.7E+08 & **0.29** \\ FLS & & & **8.33E+10** & **8.33E+10\(\pm\)0** & 0.58 \\ LLOYD & WHO(440*8) & 8.33E+10(117s) & 8.40E+10 & 8.40E+10\(\pm\)0 & **0.06** \\ MLSP & & & **8.33E+10** & **8.33E+10\(\pm\)0** & 3.74 \\ MLS & & & **8.33E+10** & 8.46E+10\(\pm\)8.1E+08 & 0.71 \\ \hline LS++ & & & **2.75E+06** & **2.75E+06\(\pm\)0** & **0.32** \\ FLS & & & **2.75E+06** & 2.85E+06\(\pm\)1.8E+05 & 0.61 \\ LLOYD & HCV(572*12) & 2.75E+06(215s) & **2.75E+06** & 2.79E+06\(\pm\)4.7E+04 & **0.08** \\ MLSP & & & **2.75E+06** & **2.75E+06\(\pm\)0** & 4.04 \\ MLS & & & **2.75E+06** & 2.79E+06\(\pm\)6.8E+04 & 0.78 \\ \hline LS++ & & & 2.63E+06 & 2.70E+06\(\pm\)5.8E+04 & **0.33** \\ FLS & & & **2.62E+06** & **2.62E+06\(\pm\)0** & 0.69 \\ LLOYD & Abs(740*21) & 2.62E+06(119s) & **2.62E+06** & **2.62E+06\(\pm\)0** & **0.07** \\ MLSP & & & **2.62E+06** & **2.62E+06\(\pm\)0** & 5.4 \\ MLS & & & **2.62E+06** & 2.66E+06\(\pm\)2.1E+04 & 0.81 \\ \hline \hline \end{tabular}
\end{table}
Table 14: Comparison results on clustering costs and running time with \(k=3\) on datasets with sizes ranging from 150 to 740

\begin{table}
\begin{tabular}{c c c c c c} \hline Method & Size & BB(Cost) & Best & Mean & Time(s) \\ \hline LS++ & & & 1145.77 & 1159.73\(\pm\)10.48 & **0.33** \\ FLS & & & **1134.45** & **1134.45\(\pm\)0** & 0.56 \\ LLOYD & TR(980*10) & 1134.45(126s) & 1136.93 & 1138.09\(\pm\)0.94 & **0.09** \\ MLSP & & & **1134.45** & **1134.45\(\pm\)0** & 4.75 \\ MLS & & & **1134.45** & 1154.49\(\pm\)14.61 & 0.78 \\ \hline LS++ & & & **1.28E+09** & 1.28E+09\(\pm\)1.79E+06 & **0.35** \\ FLS & & & **1.28E+09** & **1.28E+09\(\pm\)0** & 0.82 \\ LLOYD & SGC(1000*21) & 1.28E+09(140s) & **1.28E+09** & **1.28E+09\(\pm\)0** & **0.13** \\ MLSP & & & **1.28E+09** & **1.28E+09\(\pm\)0** & 5.18 \\ MLS & & & **1.28E+09** & 1.28E+09\(\pm\)1.1E+06 & 0.81 \\ \hline LS++ & & & **9.91E+06** & 9.91E+06\(\pm\)2.5E+03 & **0.53** \\ FLS & & & **9.91E+06** & **9.91E+06\(\pm\)0** & 0.92 \\ LLOYD & HEMI(1995*7) & 9.91E+06(97s) & **9.91E+06** & **9.91E+06\(\pm\)0** & **0.09** \\ MLSP & & & **9.91E+06** & **9.91E+06\(\pm\)0** & 5.44 \\ MLS & & & **9.91E+06** & 9.91E+06\(\pm\)2.3E+03 & 0.79 \\ \hline LS++ & & & **2.13E+09** & 2.15E+09\(\pm\)1.7E+08 & **0.48** \\ FLS & & & **2.13E+09** & 2.14E+09\(\pm\)1.4E+08 & 0.89 \\ LLOYD & pr2392(2392*2) & 2.13E+09(123s) & **2.13E+09** & **2.13E+09\(\pm\)0** & **0.09** \\ MLSP & & & **2.13E+09** & **2.13E+09\(\pm\)0** & 5.92 \\ MLS & & & **2.13E+09** & 2.15E+09\(\pm\)1.5E+08 & 0.76 \\ \hline LS++ & & & **1.96E+05** & 1.98E+05\(\pm\)1.7E+03 & **0.49** \\ FLS & & & **1.96E+05** & 1.97E+05\(\pm\)6.8E+02 & 0.97 \\ LLOYD & TRR(5456*24) & 1.96E+05(325s) & **1.96E+05** & 2.00E+05\(\pm\)3.3E+03 & **0.09** \\ MLSP & & & **1.96E+05** & **1.96E+05\(\pm\)3.8E+02** & 6.06 \\ MLS & & & **1.96E+05** & 1.98E+05\(\pm\)1.7E+03 & 0.78 \\ \hline LS++ & & & **2199.1** & 2234.68\(\pm\)41.91 & **0.92** \\ FLS & & & **2199.1** & **2201.90\(\pm\)8.42** & 2.02 \\ LLOYD & AC(7195*22) & 2199.10(222s) & 2227.18 & 2227.18\(\pm\)0 & **0.16** \\ MLSP & & & **2199.1** & 2205.75\(\pm\)19.95 & 6.43 \\ MLS & & & **2199.1** & 2212.69\(\pm\)26.49 & 1.06 \\ \hline LS++ & & & **1.49E+07** & 1.49E+07\(\pm\)269.4 & 1.1 \\ FLS & & & **1.49E+07** & **1.49E+07\(\pm\)0** & 1.91 \\ LLOYD & rds\_cnt(10000*4) & 1.49E+07(203s) & **1.49E+07** & 1.49E+07\(\pm\)331.2 & **0.03** \\ MLSP & & & **1.49E+07** & **1.49E+07\(\pm\)0** & 4.96 \\ MLS & & & **1.49E+07** & 1.49E+07\(\pm\)226.1 & **0.91** \\ \hline LS++ & & & **8.21E+07** & 8.22E+07\(\pm\)1.6E+05 & **1.65** \\ FLS & & & **8.21E+07** & **8.21E+07\(\pm\)0** & 3.71 \\ LLOYD & HTRU2(17898*8) & 8.21E+07(1555s) & **8.21E+07** & 8.21E+07\(\pm\)7.9E+03 & **0.97** \\ MLSP & & & **8.21E+07** & **8.21E+07\(\pm\)0** & 8.44 \\ MLS & & & **8.21E+07** & 8.22E+07\(\pm\)2.0E+05 & 1.21 \\ \hline LS++ & & & **1.95E+07** & 1.97E+07\(\pm\)4.9E+05 & 4.38 \\ FLS & & & **1.95E+07** & 1.95E+07\(\pm\)6.0E+04 & 7.28 \\ LLOYD & GT(36733*11) & 1.95E+07(1936s) & **1.95E+07** & 1.99E+07\(\pm\)5.4E+05 & **1.18** \\ MLSP & & & **1.95E+07** & **1.95E+07\(\pm\)7.3E+03** & 12.83 \\ MLS & & & **1.95E+07** & 1.97E+07\(\pm\)4.7E+05 & **3.82** \\ \hline \end{tabular}
\end{table}
Table 15: Comparison results on clustering costs and running time with \(k=3\) on datasets with sizes ranging from 980 to 36733

\begin{table}
\begin{tabular}{c c c c c c} \hline \hline Method & Size & BB(Cost) & Best & Mean & Time(s) \\ \hline LS++ & & & **50.97** & 52.75\(\pm\)1.74 & **0.37** \\ FLS & & & **50.97** & 51.70\(\pm\)1.10 & 0.78 \\ LLOYD & Iris(150*4) & 50.92(355s) & 51.19 & 51.19\(\pm\)0 & **0.1** \\ MLSP & & & **50.97** & **50.98\(\pm\)0.02** & 3.53 \\ MLS & & & **50.97** & 51.78\(\pm\)1.11 & 0.81 \\ \hline LS++ & & & **401.21** & 404.58\(\pm\)3.30 & **0.3** \\ FLS & & & **401.21** & 402.46\(\pm\)1.25 & 0.81 \\ LLOYD & SEEDS(210*7) & 401.21(376s) & **401.21** & 406.69\(\pm\)2.23 & **0.11** \\ MLSP & & & **401.21** & **401.46\(\pm\)0.75** & 3.62 \\ MLS & & & **401.21** & 409.65\(\pm\)9.40 & 0.83 \\ \hline LS++ & & & 437.88 & 443.63\(\pm\)7.54 & **0.29** \\ FLS & & & **437.73** & 439.82\(\pm\)6.29 & 0.89 \\ LLOYD & GLASS(214*9) & 437.73(592s) & 437.88 & 457.22\(\pm\)15.78 & **0.07** \\ MLSP & & & **437.73** & **437.73\(\pm\)0** & 3.43 \\ MLS & & & **437.73** & 439.37\(\pm\)2.49 & 0.86 \\ \hline LS++ & & & **6.0249E+05** & 6.0213E+05\(\pm\)1.8E+03 & **0.29** \\ FLS & & & **6.0249E+05** & 6.1293E+05\(\pm\)5.2E+03 & 0.83 \\ LLOYD & BM(249*6) & 6.0249E+05(389s) & **6.0249E+05** & 6.1293E+05\(\pm\)4.9E+03 & **0.04** \\ MLSP & & & **6.0249E+05** & **6.0249E+05** \(\pm\)**0** & 3.71 \\ MLS & & & **6.0249E+05** & 6.2224E+05\(\pm\)2.0E+04 & 0.83 \\ \hline LS++ & & & **40.17** & 40.97\(\pm\)0.63 & **0.29** \\ FLS & & & **40.17** & 40.28\(\pm\)0.25 & 0.82 \\ LLOYD & UK(258*5) & 40.17(457s) & 40.24 & 41.16\(\pm\)0.57 & **0.06** \\ MLSP & & & **40.17** & **40.25\(\pm\)0.25** & 3.72 \\ MLS & & & 40.34 & 41.55\(\pm\)0.85 & 0.83 \\ \hline LS++ & & & **3.0998E+11** & 3.1257E+11\(\pm\)2.5E+10 & **0.31** \\ FLS & & & **3.0998E+11** & 3.1083E+11\(\pm\)1.1E+10 & 0.89 \\ LLOYD & HF(299*12) & 3.0998E+11(1723s) & **3.0998E+11** & **3.1001E+11\(\pm\)1.2E+08** & **0.06** \\ MLSP & & & **3.0998E+11** & 3.1021E+11\(\pm\)7.1E+08 & 4.65 \\ MLS & & & **3.0998E+11** & 3.1499E+11\(\pm\)5.6E+09 & 0.92 \\ \hline LS++ & & & 5.5982E+10 & 5.6642E+10\(\pm\)3.6E+08 & **0.33** \\ FLS & & & **5.5914E+10** & 5.6333E+10\(\pm\)2.1E+08 & 0.89 \\ LLOYD & WHO(440*8) & 5.5914E+10(1840s) & **5.5914E+10** & 5.6219E+10\(\pm\)1.25E+08 & **0.09** \\ MLSP & & & **5.5914E+10** & **5.6034E+10\(\pm\)1.25E+08 & 4.84 \\ MLS & & & 5.5973E+10 & 5.6460E+10\(\pm\)3.5E+08 & 0.88 \\ \hline LS++ & & & **1.9716E+06** & 1.9722E+06\(\pm\)622.78 & **0.34** \\ FLS & & & **1.9716E+06** & 2.0044E+06\(\pm\)2.6E+04 & 0.91 \\ LLOYD & HCV(572*12) & 1.9716E+06(12768s) & **1.9716E+06** & 1.9995E+06\(\pm\)3.3E+04 & **0.13** \\ MLSP & & & **1.9716E+06** & **1.9716E+06** \(\pm\)**06** & 5.02 \\ MLS & & & **1.9716E+06** & 2.0113E+06\(\pm\)3.1E+04 & 0.93 \\ \hline LS++ & & & **1.7472E+06** & 1.7630E+06\(\pm\)3.8E+04 & **0.36** \\ FLS & & & **1.7472E+06** & **1.7472E+06** \(\pm\)**0** & 0.98 \\ LLOYD & Abs(740*21) & 1.7472E+06(410s) & **1.7472E+06** \(\pm\)**0** & 1.7502E+06\(\pm\)1.5E+03 & **0.09** \\ MLSP & & & **1.7472E+06** & **1.7472E+06** \(\pm\)**0** & 6.03 \\ MLS & & & **1.7472E+06** & 1.7676E+06\(\pm\)3.5E+04 & 1.16 \\ \hline \hline \end{tabular}
\end{table}
Table 16: Comparison results on clustering costs and running time with \(k=5\) on datasets with sizes ranging from 150 to 740

\begin{table}
\begin{tabular}{c c c c c c} \hline Method & Size & BB(Cost) & Best & Mean & Time(s) \\ \hline LS++ & & & 966.64 & 990.43\(\pm\)14.69 & **0.35** \\ FLS & & & **953.39** & 956.16\(\pm\)3.43 & 1.13 \\ LLOYD & TR(980*10) & 953.39(824s) & 965.74 & 969.10\(\pm\)5.65 & **0.14** \\ MLSP & & & **953.39** & **954.62\(\pm\)2.48** & 6.28 \\ MLS & & & 983.22 & 1000.86\(\pm\)10.34 & 0.93 \\ \hline LS++ & & & 4.6926E+08 & 4.7268E+08\(\pm\)2.8E+06 & **0.39** \\ FLS & & & **4.6919E+08** & 4.7197E+08\(\pm\)2.1E+06 & 1.06 \\ LLOYD & SGC(1000*21) & 4.6919E+08(3290s) & **4.6919E+08** & **4.6949E+08\(\pm\)2.3E+05** & **0.09** \\ MLSP & & & **4.6919E+08** & 4.6973E+08\(\pm\)1.0E+06 & 7.7 \\ MLS & & & 4.7204E+08 & 4.7529E+08\(\pm\)2.7E+06 & 1.22 \\ \hline LS++ & & & **5.3811E+06** & 5.4497E+06\(\pm\)5.6E+04 & **0.45** \\ FLS & & & **5.3811E+06** & 5.4090E+06\(\pm\)3.4E+04 & 1.24 \\ LLOYD & HEMI(1995*7) & 5.3811E+06(930s) & **5.3811E+06** & 5.4262E+06\(\pm\)1.5E+04 & **0.13** \\ MLSP & & & **5.3811E+06** & **5.3880E+06\(\pm\)2.1E+04** & 7.04 \\ MLS & & & **5.3811E+06** & 5.4781E+06\(\pm\)1.4E+04 & 0.86 \\ \hline LS++ & & & 1.1621E+09 & 1.1700E+09\(\pm\)6.3E+07 & **0.44** \\ FLS & & & **1.1619E+09** & 1.1636E+09\(\pm\)2.3E+07 & 1.31 \\ LLOYD & pr2392(2392*2) & 1.1619E+09(625s) & 1.1620E+09 & 1.1620E+09\(\pm\)2.9E+05 & **0.14** \\ MLSP & & & **1.1619E+09** & **1.1619E+09\(\pm\)0** & 7.46 \\ MLS & & & 1.1627E+09 & 1.1759E+09\(\pm\)1.4E+07 & 0.81 \\ \hline LS++ & & & 1.6920E+05 & 1.7055E+05\(\pm\)626.4 & **0.54** \\ FLS & & & 1.6894E+05 & 1.7012E+05\(\pm\)529.7 & 1.92 \\ LLOYD & TRR(5456*24) & 1.6870E+05(3094s) & 1.7086E+05 & 1.7250E+05\(\pm\)3.0E+03 & **0.15** \\ MLSP & & & **1.6870E+05** & **1.6903E+05\(\pm\)473.9** & 9.57 \\ MLS & & & 1.6914E+05 & 1.7027E+05\(\pm\)548.1 & 0.89 \\ \hline LS++ & & & **1636.1** & 1636.93\(\pm\)0.99 & **0.67** \\ FLS & & & **1636.1** & **1636.14**\(\pm\)**0 & 2.6 \\ LLOYD & AC(7195*22) & 1636.1(3552s) & 1542.6 & 1642.56\(\pm\)0 & **0.24** \\ MLSP & & & **1636.1** & **1636.14**\(\pm\)**0 & 7.68 \\ MLS & & & **1636.1** & 1637.25\(\pm\)1.26 & 1.57 \\ \hline LS++ & & & 5.3728E+06 & 5.3747E+06\(\pm\)2.2E+03 & **1.29** \\ FLS & & & **5.3725E+06** & **5.3725E+06\(\pm\)103** & 3.11 \\ LLOYD & rds\_cnt(10000*4) & 5.3725E+06(7171s) & 5.3744E+06 & 5.3751E+06\(\pm\)987 & **0.05** \\ MLSP & & & **5.3725E+06** & 5.3728E+06\(\pm\)554 & 7.53 \\ MLS & & & 5.3728E+06 & 5.3742E+06\(\pm\)2.1E+03 & 1.58 \\ \hline LS++ & & & 4.2176E+07 & 4.2260E+07\(\pm\)4.6E+04 & 2.31 \\ FLS & & & **4.2152E+07** & 4.2160E+07\(\pm\)9.3E+03 & 5.03 \\ LLOYD & HTRU2(17898*8) & 4.2154E+07(4h) & 4.2171E+07 & 4.2173E+07\(\pm\)2.9E+03 & **0.16** \\ MLSP & & & **4.2152E+07** & **4.2153E+07\(\pm\)2.9E+03** & 11.01 \\ MLS & & & 4.2268E+07 & 4.2413E+07\(\pm\)1.4E+04 & **1.56** \\ \hline LS++ & & & **1.3351E+07** & 1.3643E+07\(\pm\)3.8E+05 & 5.34 \\ FLS & & & **1.3351E+07** & 1.3438E+07\(\pm\)2.6E+05 & 10.67 \\ LLOYD & GT(36733*11) & 1.3351E+07(4h) & **1.3351E+07** & 1.3530E+07\(\pm\)3.4E+05 & **0.19** \\ MLSP & & & **1.3351E+07** & **1.3351E+07\(\pm\)0** & 16.61 \\ MLS & & & 1.3358E+07 & 1.3673E+07\(\pm\)3.9E+05 & **4.93** \\ \hline \end{tabular}
\end{table}
Table 17: Comparison results on clustering costs and running time with \(k=5\) on datasets with sizes ranging from 980 to 36733

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline Method & Size & BB(Cost) & Best & Mean & Time(s) \\ \hline LS++ & & & 476.88 & 488.32\(\pm\)22.61 & 2.93 \\ FLS & & & 476.88 & 476.88\(\pm\)0 & 10.44 \\ LLOYD & rds(50,000*3) & 476.79(811s) & 476.88 & 507.69\(\pm\)31.3 & **0.18** \\ MLSP & & & **476.79** & **476.8\(\pm\)0.07** & 15.52 \\ MLS & & & 476.88 & 482.77\(\pm\)16.93 & **2.26** \\ \hline LS++ & & & **4.9412E+08** & **4.9412E+08+0** & **1.04** \\ FLS & & & **4.9412E+08** & **4.9412E+08+0** & 7.27 \\ LLOYD & KEGG(53,413*23) & 4.9412E+08(3901s) & **4.9412E+08** & **4.9412E+08** & **0.26** \\ MLSP & & & **4.9412E+08** & **4.9412E+08** & 12.89 \\ MLS & & & **4.9412E+08** & **4.9412E+08** & 1.97 \\ \hline LS++ & & & **1.15E+05** & 1.16E+05\(\pm\)1.7E+03 & 3.26 \\ FLS & & & **1.15E+05** & 1.15E+05\(\pm\)229 & 14.73 \\ LLOYD & Urban\_10(100,000*2) & 1.15E+05(6834s) & **1.15E+05** & 1.23E+05\(\pm\)1.1E+04 & **0.18** \\ MLSP & & & **1.15E+05** & **1.15E+05** & **1.23E+05\(\pm\)1.27** & 26.14 \\ MLS & & & **1.15E+05** & 1.16E+05\(\pm\)1.6E+03 & **3.08** \\ \hline LS++ & & & **1.6396E+15** & 1.6398E+15\(\pm\)3.3E+11 & 8.27 \\ FLS & & & **1.6396E+15** & 1.6397E+15\(\pm\)1.3E+11 & 58.78 \\ LLOYD & RNG\_AGR(199,843*7) & 1.64E+15(4h) & **1.6396E+15** & 1.6398E+15\(\pm\)3.3E+11 & **0.42** \\ MLSP & & & **1.6396E+15** & **1.6396E+15** & 53.25 \\ MLS & & & **1.6396E+15** & 1.6398E+15\(\pm\)1.3E+11 & **7.73** \\ \hline LS++ & & & **414253** & 417302+4629 & 9.86 \\ FLS & & & **414253** & 414733+956 & 50.11 \\ LLOYD & Urban\_GB(360,177*2) & OOM & **414253** & 423509+12613 & **0.32** \\ MLSP & & & **414253** & **414642+314** & 71.46 \\ MLS & & & **414253** & 417063+6123 & **8.87** \\ \hline LS++ & & & **2.2779E+07** & 2.2781E+07\(\pm\)824 & 13.93 \\ FLS & & & **2.2779E+07** & 2.2780E+07\(\pm\)769 & 142.15 \\ LLOYD & SPNET\_3D(434,874*3) & OOM & 2.2781E+07 & 2.2782E+07\(\pm\)609 & **0.49** \\ MLSP & & & **2.2779E+07** & **2.2779E+07** & 121.03 \\ MLS & & & **2.2779E+07** & 2.2781E+07\(\pm\)1022 & **10.33** \\ \hline LS++ & & & **3.8720E+06** & 3.8722E+06\(\pm\)108.8 & 37.1 \\ FLS & & & **3.8720E+06** & 3.8721E+06\(\pm\)44.1 & 338.19 \\ LLOYD & syn(1,000,000*2) & OOM & **3.8720E+06** & 3.8724E+06\(\pm\)236.4 & **0.69** \\ MLSP & & & **3.8720E+06** & **3.8721E+06\(\pm\)32.9** & 223.97 \\ MLS & & & **3.8720E+06** & 3.8722E+06\(\pm\)119.2 & **2.93** \\ \hline LS++ & & & **6.9094E+08** & **6.9094E+08** & 286.41 \\ FLS & & & **6.9094E+08** & **6.9094E+08** & 3244.76 \\ LLOYD & USC\_1990(2,458,685*68) & OOM & **6.9094E+08** & **6.9094E+08** & **18.57** \\ MLSP & & & **6.9094E+08** & **6.9094E+08** & 2443.28 \\ MLS & & & **6.9094E+08** & **6.9094E+08** & **227.59** \\ \hline LS++ & & & **4.6299E+07** & 4.7891E+07\(\pm\)1.6E+06 & 305.27 \\ FLS & & & **4.6299E+07** & 4.8013E+07\(\pm\)1.5E+06 & 3894.45 \\ LLOYD & SUSY(5,000,000*17) & OOM & **4.6299E+07** & 4.8261E+07\(\pm\)1.3E+06 & **29.2** \\ MLSP & & & **4.6299E+07** & **4.7349E+07\(\pm\)7.7E+04** & 2610.89 \\ MLS & & & **4.6299E+07** & 4.7612E+07\(\pm\)1.6E+06 & **262.61** \\ \hline LS++ & & & **2.2614E+08** & 2.2726E+08\(\pm\)1.1E+06 & 614.07 \\ FLS & & & **2.2614E+08** & 2.27220E+08\(\pm\)9.2E+05 & 782.13 \\ LLOYD & HIGGS(11,000,000*27) & OOM & **2.2614E+08** & 2.2858E+08\(\pm\)2.6E+06 & **109.06** \\ MLSP & & & **2.2614E+08** & **2.2718E+08\(\pm\)8.8E+05** & 6492.59 \\ MLS & & & **2.2614E+08** & 2.2728E+08\(\pm\)1.2E+06 & **522.15** \\ \hline LS++ & & & 1.574E+13 & 1.6098E+13\(\pm\)1.6E+11 & 31994 \\ FLS & & & **1.5649E+13** & 1.5856E+13\(\pm\)1.1E+11 & 18306 \\ LLOYD & SIFT(100,000,000*128) & OOM & 1.5718E+13 & **1.5793E+13\(\pm\)1.5E+11** & **401** \\ MLSP & & & **- & - & - \\ MLS & & & 1.5718E+13 & 1.5848E+13\(\pm\)1.1E+11 & **12738** \\ \hline \hline \end{tabular}
\end{table}
Table 18: Comparison results on clustering costs and running time with \(k=3\) on datasets with sizes larger than \(50,000\), where OOM is short for out of memory

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline Method & Size & BB(Cost) & Best & Mean & Time(s) \\ \hline LS++ & & & 284.27 & 287.39\(\pm\)2.78 & 3.34 \\ FLS & & & 290.15 & 290.15\(\pm\)0 & 18.86 \\ LLOYD & rds(50,000*3) & 282.65(4h) & 284.22 & 287.50\(\pm\)8.67 & **0.86** \\ MLSP & & & **283.13** & **283.27\(\pm\)0.21** & 58.61 \\ MLS & & & 284.21 & 287.23\(\pm\)2.72 & **2.97** \\ \hline LS++ & & & **1.9201E+08** & 1.9205E+08\(\pm\)2.1E+04 & 2.17 \\ FLS & & & **1.9201E+08** & 1.9205E+08\(\pm\)1.6E+04 & 10.92 \\ LLOYD & KEGG(53,413*23) & 1.92E+08(3901s) & **1.9201E+08** & 1.9754E+08\(\pm\)1.6E+07 & **0.26** \\ MLSP & & & **1.9201E+08** & 1.9201E+08** & 24.83 \\ MLS & & & **1.9201E+08** & 1.9204E+08\(\pm\)3.2E+04 & **1.88** \\ \hline LS++ & & & **5.62E+04** & 56281\(\pm\)76 & 5.33 \\ FLS & & & **5.62E+04** & 56248\(\pm\)49 & 26.22 \\ LLOYD & Urban\_10(100,000*2) & 56231(4h) & **5.62E+04** & 57768\(\pm\)2762 & **0.14** \\ MLSP & & & **5.62E+04** & **56246\(\pm\)36** & 40.97 \\ MLS & & & **5.62E+04** & 56315\(\pm\)83 & **4.64** \\ \hline LS++ & & & **5.0721E+14** & 5.0721E+14\(\pm\)3.3E+11 & 15.65 \\ FLS & & & **5.0721E+14** & 5.0739E+14\(\pm\)3.0E+11 & 122.09 \\ LLOYD & RNG\_AGR(199,843*7) & 5.0721E+14(4h) & **5.0721E+14** & 5.0781E+14\(\pm\)3.2E+11 & **0.35** \\ MLSP & & & **5.0721E+14** & **5.0736E+14\(\pm\)3.1E+11** & 73.2 \\ MLS & & & **5.0721E+14** & 5.0795E+13\(\pm\)3.0E+11 & **14.24** \\ \hline LS++ & & & **201878** & 202554\(\pm\)954 & 16.48 \\ FLS & & & **201878** & 201879\(\pm\)2.5 & 97.19 \\ LLOYD & Urban\_GB(360,177*2) & OOM & **201878** & 204943\(\pm\)7816 & **0.53** \\ MLSP & & & **201873** & **201874\(\pm\)2.03** & 103.94 \\ MLS & & & **201878** & 202166\(\pm\)629 & **14.89** \\ \hline LS++ & & & 8.8280E+06 & 8.8309E+06\(\pm\)1.4E+03 & 26.85 \\ FLS & & & 8.8280E+06 & 8.8289E+06\(\pm\)1.1E+03 & 331.24 \\ LLOYD & SPNET\_3D(434,874*3) & OOM & 8.8280E+06 & 8.8315E+06\(\pm\)1.6E+03 & **0.83** \\ MLSP & & & **8.8273E+06** & **8.8273E+06\(\pm\)0.17** & 180.12 \\ MLS & & & 8.8277E+07 & 8.8313E+06\(\pm\)0.1E+03 & **22.94** \\ \hline LS++ & & & 1.8662E+06 & 1.8696E+06\(\pm\)3.2E+03 & 66.78 \\ FLS & & & 1.8765E+06 & 1.8674E+06\(\pm\)1.8E+03 & 527.22 \\ LLOYD & syn(1,000,000*2) & OOM & 1.8661E+06 & 1.9579E+06\(\pm\)1.1E+03 & **0.85** \\ MLSP & & & **1.8652E+06** & **1.8659E+06\(\pm\)1.1E+03 & 391.86 \\ MLS & & & 1.8658E+06 & 1.8694E+06\(\pm\)3.1E+03 & **31.62** \\ \hline LS++ & & & **3.9017E+08** & 3.9375E+08\(\pm\)1.1E+07 & 439.74 \\ FLS & & & **3.9017E+08** & **3.9017E+08** & 4802.6 \\ LLOYD & USC\_1990(2,458,685*68) & OOM & **3.9017E+08** & 3.9816E+08\(\pm\)3.6E+07 & **20.11** \\ MLSP & & & **3.9017E+08** & **3.9017E+08** & 3612.99 \\ MLS & & & **3.9017E+08** & **3.9372E+08\(\pm\)1.1E+07** & **324.97** \\ \hline LS++ & & & 4.0623E+07 & 4.1035E+07\(\pm\)1.5E+06 & 451.28 \\ FLS & & & 3.9880E+07 & 4.0997E+07\(\pm\)4.5E+04 & 5329.33 \\ LLOYD & SUSY(5,000,000*17) & OOM & 3.9848E+07 & 4.0648E+07\(\pm\)6.4E+05 & **31.67** \\ MLSP & & & **3.8720E+07** & **3.9316E+07\(\pm\)1.4E+05** & 4137.19 \\ MLS & & & 3.9904E+07 & 4.0882E+07\(\pm\)3.6E+06 & **297.43** \\ \hline LS++ & & & **2.0829E+08** & 2.1016E+08\(\pm\)1.3E+06 & 1316.59 \\ FLS & & & **2.0829E+08** & 2.1012E+08\(\pm\)1.2E+06 & 13489.3 \\ LLOYD & HIGGS(11,000,000*27) & OOM & **2.0829E+08** & 2.1046E+08\(\pm\)5.7E+06 & **114.18** \\ MLSP & & & **2.0829E+08** & **2.1016E+08\(\pm\)1.1E+06** & 12713.2 \\ MLS & & & **2.0829E+08** & 2.1018E+08\(\pm\)1.4E+06 & **1019.6** \\ \hline LS++ & & & 1.5035E+13 & 1.519E+13\(\pm\)3.8E+06 & 41376 \\ FLS & & & **1.4778E+13** & 1.4994E+13\(\pm\)9.1E+10 & 27908 \\ LLOYD & SIFT(100,000,000*128) & OOM & 1.4805E+13 & 1.5021E+13\(\pm\)8.0E+10 & **531** \\ MLSP & & & **-** & **-** & **-** \\ MLS & & & **1.4778E+13** & **1.4992E+13\(\pm\)8.9E+10** & **12931** \\ \hline \hline \end{tabular}
\end{table}
Table 19: Comparison results on clustering costs and running time with \(k=5\) on datasets with sizes larger than \(50,000\), where OOM is short for out of memory

### Experiments on Different Local Search Algorithms with Fixed Time Limits

In this section, we conduct some additional experiments on different datasets with fixed time limits. The experimental results in Table 20 demonstrate that our proposed MLSP algorithm achieves the best clustering quality with fixed time limits.

\begin{table}
\begin{tabular}{c c c c c c c c c} \hline \hline Dataset & Method & Time Point & Cost & Time Point & Cost & Time Point & Cost & Time Point & Cost \\ \hline \multirow{4}{*}{rts} & LS++ & & 137.96 & & 137.96 & & 137.93 & & 137.93 \\  & FLS & 138.98 & & 137.26 & & 134.68 & & 134.62 \\  & MLSP & 20s & 133.62 & 40s & 132.29 & 60s & 132.28 & 80s & 132.14 \\  & MLS & & 140.19 & & 138.23 & & 137.63 & & 137.63 \\ \hline \multirow{4}{*}{KEGG} & LS++ & & 62855194 & & 62855194 & & 62793065 & & 62793065 \\  & FLS & 61634349 & & 61633499 & & 61633499 & & 61608474 \\  & MLSP & 20s & 61534479 & 40s & 61534479 & 60s & 61534479 & 80s & 61534479 \\  & MLS & & 63419827 & & 63419827 & & 63419827 & & 63419827 \\ \hline \multirow{4}{*}{Urban\_10} & LS++ & & 25011.62 & & 25011.62 & & 25011.62 & & 25011.62 \\  & FLS & 25103.83 & & 25066.53 & & 25058.21 & & 24973.42 \\  & MLSP & 20s & **24767.85** & 40s & **24767.66** & 60s & **24751.62** & 80s & **24748.37** \\  & MLS & & 24920.38 & & 24885.80 & & 24885.80 & & 24885.80 \\ \hline \multirow{4}{*}{RNG\_AGR} & LS++ & & 1.3833+14 & & 1.3833+14 & & 1.3833+14 & & 1.3833+14 \\  & FLS & 1,3822+14 & & 1.3831+14 & & 1.3800+14 & & 1.33977E+14 \\  & MLSP & 60s & **1,3743E+14** & & **1.3703E+14** & & 180 & **1.3690E+14** \\  & MLS & & 1.3826E+14 & & 1.3826E+14 & & 1.3826E+14 & & 1.3826E+14 \\ \hline \multirow{4}{*}{Urban\_GB} & LS++ & & 89234.32 & & 98234.23 & & 89234.23 & & 89234.23 \\  & FLS & 8901.46 & & 8901.46 & & 89088.77 & & 890406.39 \\  & MLSP & 60s & 89314.14 & & 120s & 89311.78 & 180 & **89464.64** & 240s & **88407.99** \\  & MLS & & **89855.80** & & **89855.80** & & 88955.80 & & 89855.80 \\ \hline \multirow{4}{*}{SPNET\_3D} & LS++ & & 2576396 & & 2576396 & & 2575784 & & 2575784 \\  & FLS & 2573606 & & 2572637 & & 2571369 & & 2570776 \\  & MLSP & 20s & **2570513** & & **2570292** & & **2570160** & 800s & **2569484** \\  & MLS & & 2574266 & & 2574266 & & 2574266 & & 2574266 \\ \hline \multirow{4}{*}{syn} & LS++ & & 56144.2 & & 56144.2 & & 56144.2 & & 56144.2 \\  & FLS & 56191.99 & & 5619149 & & 561914.94 & & 561914.9 \\  & MLSP & 300s & **560502.5** & 600s & **560502.4** & 900s & **560502.4** & 1200s & **560502.4** \\  & MLS & & 562385.8 & & 562385.8 & & 562385.8 & & 562385.8 \\ \hline \multirow{4}{*}{USC\_1990} & LS++ & & 270910366 & & 270910366 & & 270910366 & & 270910366 \\  & FLS & 270910366 & & 270910366 & & 270910366 & & 270825616 \\  & MLSP & 2000s & **270935773** & & 4000s & **270357373** & 6000s & **270357373** \\  & MLS & & 270825616 & & 270825616 & & 270825616 & & 270825616 \\ \hline \multirow{4}{*}{SUSY} & LS++ & & 32437956 & & 32437956 & & 32437956 & & 32437956 \\  & MLS & & 31693128 & & 31691328 & & 31691328 & & 31691328 \\ \cline{1-1} \cline{2-1} \cline{2-11}  & MLSP & 2500s & 31756413 & 5000s & **31650548** & 7500s & **31629757** \\ \cline{1-1} \cline{2-11}  & MLS & & 32548680 & & 32548680 & & 32548680 & & 32548680 \\ \hline \multirow{4}{*}{HIGGS} & LS++ & & 189640888.6 & & 188373729.5 & 188373729.5 & 187089096.4 \\  & MLS & & 189186932.5 & & 189186932.5 & 189186932.5 & 189186932.5 \\ \cline{1-1} \cline{2-11}  & MLSP & 12000s & **184084129.5** & 24000s & **184084129.5** & 48000s & **184084129.5** \\ \cline{1-1} \cline{2-11}  & MLS & & 186890395.5 & & 18689395.5 & 186869395.5 & 186869395.5 \\ \hline \multirow{4}{*}{SIFT} & LS++ & 1.825+1.3 & & 1.77E+1.3 & & 1.67E+1.3 & & 1.58E+1.3 \\ \cline{1-1}  & FLS & 1.79E+13 & & 1.59E+13 & & 1.55E+13 & & 1.52E+13 \\ \cline{1-1} \cline{2-11}  & MLSP & 20000s & 1.85E+13 & 40000s & 1.85E+13 & 60000s & 1.85E+13 \\ \cline{1-1} \cline{2-11} \cline{2-11}  & MLS & **1.38E+13** & **1.38E+13** & **1.38E+13** & **1.38E+13** & **1.38E+13** \\ \hline \hline \end{tabular}
\end{table}
Table 20: Comparison results on clustering costs with \(k=10\) and fixed time limits