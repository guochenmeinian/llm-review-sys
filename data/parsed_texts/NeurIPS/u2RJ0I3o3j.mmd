# Plane: Representation Learning over Planar Graphs

 Radoslav Dimitro

Department of Computer Science

University of Oxford

contact@radoslav11.com

&Zeyang Zhao

Department of Computer Science

University of Oxford

zeyzang.zhao@cs.ox.ac.uk

&Ralph Abboud

Department of Computer Science

University of Oxford

ralph@ralphabb.ai

&Ismail likan Ceylan

Department of Computer Science

University of Oxford

ismail.ceylan@cs.ox.ac.uk

This work is largely conducted while these authors were still affiliated with the University of Oxford.Equal contribution.

###### Abstract

Graph neural networks iteratively compute representations of nodes of an input graph through a series of transformations in such a way that the learned graph function is isomorphism invariant on graphs, which makes the learned representations _graph invariants_. On the other hand, it is well-known that graph invariants learned by these class of models are _incomplete_: there are pairs of non-isomorphic graphs which cannot be distinguished by standard graph neural networks. This is unsurprising given the computational difficulty of graph isomorphism testing on general graphs, but the situation begs to differ for special graph classes, for which efficient graph isomorphism testing algorithms are known, such as planar graphs. The goal of this work is to design architectures for _efficiently_ learning _complete_ invariants of planar graphs. Inspired by the classical planar graph isomorphism algorithm of Hopcroft and Tarjan, we propose Plane as a framework for planar representation learning. PlanE includes architectures which can learn _complete_ invariants over planar graphs while remaining practically scalable. We validate the strong performance of PlanE architectures on various planar graph benchmarks.

## 1 Introduction

Graphs are used for representing relational data in a wide range of domains, including physical [53], chemical [17, 36], and biological [21, 68] systems, which led to increasing interest in machine learning (ML) over graphs. Graph neural networks (GNNs) [23, 51] have become prominent for graph ML for a wide range of tasks, owing to their capacity to explicitly encode desirable relational inductive biases [6]. GNNs iteratively compute representations of nodes of an input graph through a series of transformations in such a way that the learned graph-level function represents a _graph invariant_: a property of graphs which is preserved under all isomorphic transformations.

Learning functions on graphs is challenging for various reasons, particularly since the learning problem contains the infamous graph isomorphism problem, for which the best known algorithm, given in a breakthrough result by Babai [4], runs in quasi-polynomial time. A large class of GNNs can thus only learn _incomplete_ graph invariants for scalability purposes. In fact, standard GNNs are known to be at most as expressive as the 1-dimensional Weisfeiler-Leman algorithm (1-WL)[59] in terms of distinguishing power [45, 63]. There are simple non-isomorphic pairs of graphs, such as the pair shown in Figure 1, which cannot be distinguished by 1-WL and by a large class of GNNs. Thislimitation motivated a large body of work aiming to explain and overcome the expressiveness barrier of these architectures [1; 5; 7; 9; 11; 15; 37; 42; 43; 45; 50].

The expressiveness limitations of standard GNNs already apply on planar graphs, since, e.g., the graphs \(G_{1}\) and \(G_{2}\) from Figure 1 are planar. There are, however, efficient and complete graph isomorphism testing algorithms for planar graphs [28; 29; 41], which motivates an aligned design of dedicated architectures with better properties over planar graphs. Building on this idea, we propose architectures for _efficiently_ learning _complete_ invariants over planar graphs.

The contributions of this work can be summarized as follows:

* Building on the classical literature, we introduce PlanE as a framework for learning _isomorphism-complete_ invariant functions over planar graphs and derive the BasePlanE architecture (Section 5). We prove that BasePlanE is a _scalable_, _complete_ learning algorithm on planar graphs: BasePlanE can distinguish _any_ pair of non-isomorphic planar graphs (Section 6).
* We conduct an empirical analysis for BasePlanE evaluating its _expressive power_ on three tasks (Section 7.1), and validating its _performance_ on real-world graph classification (Section 7.2) and regression benchmarks (Section 7.3 and 7.4). The empirical results support the presented theory, and also yield multiple state-of-the-art results for BasePlanE on molecular datasets.

The proofs and additional experimental details are delegated to the appendix of this paper.

## 2 A primer on graphs, invariants, and graph neural networks

**Graphs.** Consider simple, undirected graphs \(G=(V,E,\zeta)\), where \(V\) is a set of nodes, \(E\subseteq V\times V\) is a set of edges, and \(\zeta:V\rightarrow\mathbb{C}\) is a (coloring) function. If the range of this map is \(\mathbb{C}=\mathbb{R}^{d}\), we refer to it as a \(d\)-dimensional _feature map_. A graph is _connected_ if there is a path between any two nodes and disconnected otherwise. A graph is _biconnected_ if it cannot become disconnected by removing any single node. A graph is _triconnected_ if the graph cannot become disconnected by removing any two nodes. A graph is _planar_ if it can be drawn on a plane such that no two edges intersect.

**Components.** A _biconnected component_ of a graph \(G\) is a maximal biconnected subgraph. Any connected graph \(G\) can be efficiently decomposed into a tree of biconnected components called the _Block-Cut tree_ of \(G\)[30], which we denote as BlockCut\((G)\). The blocks are attached to each other at shared nodes called _cut nodes_. A _triconnected component_ of a graph \(G\) is a maximal triconnected subgraph. Triconnected components of a graph \(G\) can also be compiled (very efficiently [27]) into a tree, known as the _SPQR tree_[16], which we denote as Spqr\((G)\). Given a graph \(G\), we denote by \(\sigma^{G}\) the set of all SPQR components of \(G\) (i.e., nodes of Spqr\((G)\)), and by \(\pi^{G}\) the set of all biconnected components of \(G\). Moreover, we denote by \(\sigma^{G}_{u}\) the set of all SPQR components of \(G\) where \(u\) appears as a node, and by \(\pi^{G}_{u}\) the set of all biconnected components of \(G\) where \(u\) appears as a node.

**Labeled trees.** We sometimes refer to rooted, undirected, labeled trees \(\Gamma=(V,E,\zeta)\), where the canonical root node is given as one of tree's _centroids_: a node with the property that none of its branches contains more than half of the other nodes. We denote by root\((\Gamma)\) the canonical root of \(\Gamma\), and define the _depth_\(d_{u}\) of a node \(u\) in the tree as the node's minimal distance from the canonical root. The _children_ of a node \(u\) is the set \(\chi(u)=\{v|(u,v)\in E,d_{v}=d_{u}+1\}\). The _descendants_ of a node

Figure 1: Two graphs indistinguishable by \(1\)-WL.

Figure 2: Acid anhydride as a planar graph.

\(u\) is given by set of all nodes reachable from \(u\) through a path of length \(k\geq 0\) such that the node at position \(j+1\) has one more depth than the node at position \(j\), for every \(0\leq j\leq k\). Given a rooted tree \(\Gamma\) and a node \(u\), the _subtree_ of \(\Gamma\) rooted at node \(u\) is the tree induced by the descendants of \(u\), which we donote by \(\Gamma_{u}\). For technical convenience, we allow the induced subtree \(\Gamma_{u}\) of a node \(u\) even if \(u\) does not appear in the tree \(\Gamma\), in which case \(\Gamma_{u}\) is the empty tree.

**Node and graph invariants.** An _isomorphism_ from a graph \(G=(V,E,\zeta)\) to a graph \(G^{\prime}=(V^{\prime},E^{\prime},\zeta^{\prime})\) is a bijection \(f:V\to V^{\prime}\) such that \(\zeta(u)=\zeta^{\prime}(f(u))\) for all \(u\in V\), and \((u,v)\in E\) if and only if \((f(u),f(v))\in E^{\prime}\), for all \(u,v\in V\). A _node invariant_ is a function \(\xi\) that associates with each graph \(G=(V,E,\zeta)\) a function \(\xi(G)\) defined on \(V\) such that for all graphs \(G\) and \(G^{\prime}\), all isomorphisms \(f\) from \(G\) to \(G^{\prime}\), and all nodes \(u\in V\), it holds that \(\xi(G)(u)=\xi(G^{\prime})(f(u))\). A _graph invariant_ is a function \(\xi\) defined on graphs such that \(\xi(G)=\xi(G^{\prime})\) for all isomorphic graphs \(G\) and \(G^{\prime}\). We can derive a graph invariant \(\xi\) from a node invariant \(\xi^{\prime}\) by mapping each graph \(G\) to the multiset \(\{\xi^{\prime}(G)(u)\mid u\in V\}\). We say that a graph invariant \(\xi\)_distinguishes_ two graphs \(G\) and \(G^{\prime}\) if \(\xi(G)\neq\xi(G^{\prime})\). If a graph invariant \(\xi\) distinguishes \(G\) and \(G^{\prime}\) then there is no isomorphism from \(G\) to \(G^{\prime}\). If the converse also holds, then \(\xi\) is a _complete_ graph invariant. We can speak of (in)completeness of invariants on special classes of graphs, e.g., 1-WL computes an incomplete invariant on general graphs, but it is well-known to compute a complete invariant on trees [24].

**Message passing neural networks.** A vast majority of GNNs are instances of _message passing neural networks (MPNNs)_[22]. Given an input graph \(G=(V,E,\zeta)\), an MPNN sets, for each node \(u\in V\), an initial node representation \(\zeta(u)=\boldsymbol{h}_{u}^{(0)}\), and iteratively computes representations \(\boldsymbol{h}_{u}^{(\ell)}\) for a fixed number of layers \(0\leq\ell\leq L\) as:

\[\boldsymbol{h}_{u}^{(\ell+1)}\coloneqq\phi\left(\boldsymbol{h}_{u}^{(\ell)}, \psi(\boldsymbol{h}_{u}^{(\ell)},\{\!\{\boldsymbol{h}_{v}^{(\ell)}|\;v\in N_{ u}\}\!\})\right),\]

where \(\phi\) and \(\psi\) are respectively _update_ and _aggregation_ functions, and \(N_{u}\) is the neighborhood of \(u\). Node representations can be _pooled_ to obtain graph-level embeddings by, e.g., summing all node embeddings. We denote by \(\boldsymbol{z}^{(L)}\) the resulting graph-level embeddings. In this case, an MPNN can be viewed as an encoder that maps each graph \(G\) to a representation \(\boldsymbol{z}_{G}^{(L)}\), computing a graph invariant.

## 3 Related work

The expressive power of MPNNs is upper bounded by 1-WL [45; 63] in terms of distinguishing graphs, and by the logic \(\mathbb{C}^{2}\) in terms of capturing functions over graphs [5], motivating a body of work to improve on these bounds. One notable direction has been to enrich node features with unique node identifiers [42; 65], random discrete colors [15], or random noisy dimensions [1; 50]. Another line of work proposes _higher-order_ architectures [37; 43; 44; 45] based on higher-order tensors [44], or a higher-order form of message passing [45], which typically align with a \(k\)-dimensional WL test4, for some \(k>1\). Higher-order architectures are not scalable, and most existing models are upper bounded by 2-WL (or, _oblivious_ 3-WL). Another body of work is based on sub-graph sampling [7; 9; 11; 56], with pre-set sub-graphs used within model computations. These approaches can yield substantial expressiveness improvements, but they rely on manual sub-graph selection, and require running expensive pre-computations. Finally, MPNNs have been extended to incorporate other graph kernels, i.e., shortest paths [2; 64], random walks [46; 47] and nested color refinement [67].

Footnote 4: We write \(k\)-WL to refer to the folklore (more standard) version of the algorithm following Grohe [25].

The bottleneck limiting the expressiveness of MPNNs is the implicit need to perform graph isomorphism checking, which is challenging in the general case. However, there are classes of graphs, such as planar graphs, with efficient and complete isomorphism algorithms, thus eliminating this bottleneck. For planar graphs, the first complete algorithm for isomorphism testing was presented by Hopcroft and Tarjan [29], and was followed up by a series of algorithms [10; 19; 28; 52]. Kukluk et al. [41] presented an algorithm, which we refer to as KHC, that is more suitable for practical applications, and that we align with in our work. As a result of this alignment, our approach is the first _efficient_ and _complete_ learning algorithm on planar graphs. Observe that 3-WL (or, _oblivious_ 4-WL) is also known to be complete on planar graphs [38], but this algorithm is far from being scalable [35] and as a result there is no neural, learnable version implemented. By contrast, our architecture learns representations of efficiently computed components and uses these to obtain refined representations. Our approach extends the inductive biases of MPNNs based on structures, such as biconnected components, which are recently noted to be beneficial in the literature [66].

A practical planar isomorphism algorithm

The idea behind the KHC algorithm is to compute a canonical code for planar graphs, allowing us to reduce the problem of isomorphism testing to checking whether the codes of the respective graphs are equal. Importantly, we do not use the codes generated by KHC in our model, and view codes as an abstraction through which alignment with KHC is later proven. Formally, we can define a code as a string over the alphabet \(\Sigma\cup\mathbb{N}\): for each graph, the KHC algorithm computes codes for various components resulting from decompositions, and gradually builds a code representation for the graph. For readability, we allow reserved symbols \("(",")"\) and \(","\) in the generated codes. We present an overview of KHC and refer to Kukluk et al. [41] for details. We can assume that the planar graphs are connected, as the algorithm can be extended to disconnected graphs by independently computing the codes for each of the components, and then concatenating them in their lexicographical order.

**Generating a code for the the graph.** Given a connected planar graph \(G=(V,E,\zeta)\), KHC decomposes \(G\) into a Block-Cut tree \(\delta=\textsc{BlockCut}(G)\). Every node \(u\in\delta\) is either a cut node of \(G\), or a virtual node associated with a biconnected component of \(G\). KHC iteratively removes the leaf nodes of \(\delta\) as follows: if \(u\) is a leaf node associated with a biconnected component \(B\), KHC uses a subprocedure BiCode to compute the canonical code \(\textsc{code}(\delta_{u})=\textsc{BiCode}(B)\) and removes \(u\) from the tree; otherwise, if the leaf node \(u\) is a cut node, KHC overrides the initial code \(\textsc{code}((\{u\},\{\}))\), using an aggregate code of the removed biconnected components in which \(u\) occurs as a node. This procedure continues until there is a single node left, and the code for this remaining node is taken as the code of the entire connected graph. This process yields a complete graph invariant. Conceptually, it is more convenient for our purposes to reformulate this procedure as follows: we first canonically root \(\delta\), and then code the subtrees in a _bottom-up procedure_. Specifically, we iteratively generate codes for subtrees and the final code for \(G\) is then simply the code of \(\delta_{\textsc{root}(\delta)}\).

**Generating a code for biconnected components.** KHC relies on a subprocedure BiCode to compute a code, given a biconnected planar graph \(B\). Specifically, it uses the SPQR tree \(\gamma=\textsc{Spqr}(B)\) which uniquely decomposes a biconnected component into a tree with virtual nodes of one of four types: \(S\) for _cycle_ graphs, \(P\) for _two-node dipole_ graphs, \(Q\) for a graph that has a _single edge_, and finally \(R\) for a triconnected component that is _not_ a dipole or a cycle. We first generate codes for the induced sub-graphs based on these virtual nodes. Then the SPQR tree is canonically rooted, and similarly to the procedure on the Block-Cut tree, we iteratively build codes for the subtrees of \(\gamma\) in a bottom-up fashion. Due to the simpler structure of the SPQR tree, instead of making overrides, the recursive code generation is done by prepending a number \(\theta(C,C^{\prime})\) for a parent SPQR tree node \(C\) and each \(C^{\prime}\in\chi(C)\). This number is generated based on the way \(C\) and \(C^{\prime}\) are connected in \(B\). Generating codes for the virtual \(P\) and \(Q\) nodes is trivial. For \(S\) nodes, we use the lexicographically smallest ordering of the cycle, and concatenate the individual node codes. However, for \(R\) nodes we require a more complex procedure and this is done using Weinberg's algorithm as a subroutine.

**Generating a code for triconnected components.** Whitney [60] has shown that triconnected graphs have only two planar embeddings, and Weinberg introduced an algorithm that computes a canonical code for triconnected planar graphs [58] which we call TriCode. This code is used as one of the building blocks of the KHC algorithm and can be extended to labeled triconnected planar graphs, which is essential for our purposes. Weinberg's algorithm [58] generates a canonical order for a triconnected component \(T\), by traversing all the edges in both directions via a walk. Let \(\omega\) be the sequence of visited nodes in this particular walk and write \(\omega[i]\) to denote \(i\)-th node in it. This walk is then used to generate a sequence \(\kappa\) of same length, that corresponds to the order in which we first visit the nodes: for each node \(u=\omega[i]\) that occurs in the walk, we set \(\kappa[i]=1+|\{\kappa[j]\mid j<i\}|\) if \(\omega[i]\) is the first occurrence of \(u\), or \(\kappa[i]=\kappa[\min\{j\mid\omega[i]=\omega[j]\}]\) otherwise. For example, the walk \(\omega=\langle v_{1},v_{3},v_{2},v_{3},v_{1}\rangle\) yields \(\kappa=\langle 1,2,3,2,1\rangle\). Given such a walk of length \(k\) and a corresponding sequence \(\kappa\), we compute the following canonical code: \(\textsc{TriCode}(T)=(\kappa[1],\textsc{code}((\{\omega[1]\},\{\}))),\ldots,( \kappa[k],\textsc{code}((\{\omega[k]\},\{\})))\).

## 5 PlanE: Representation learning over planar graphs

KHC generates a unique code for every planar graph in a hierarchical manner based on the decompositions. Our framework aligns with this algorithm: given an input graph \(G=(V,E,\zeta)\), PlanE first computes the BlockCut and Spqr trees (\(\mathbf{A}-\mathbf{C}\)), and then learns representations for the nodes, the components, and the whole graph (\(\mathbf{D}-\mathbf{G}\)), as illustrated in Figure 3.

Formally, PlanE sets the initial node representations \(\bm{h}_{u}^{(0)}=\zeta(u)\) for each node \(u\in V\), and computes, for every layer \(1\leq\ell\leq L\), the representations \(\bm{\widehat{h}}_{C}^{(\ell)}\) of Spqr components of \(\sigma^{G}\), the representations \(\bm{\widehat{h}}_{B}^{(\ell)}\) of biconnected components \(B\in\pi^{G}\), the representations \(\bm{\overline{h}}_{\delta_{u}}^{(\ell)}\) of subtrees in the Block-Cut tree \(\delta=\textsc{BlockCut}(G)\) for each node \(u\in V\), and the representations \(\bm{h}_{u}^{(\ell)}\) of nodes as:

\[\bm{\widehat{h}}_{C}^{(\ell)} =\textsc{TriEnc}\big{(}C,\{(\bm{h}_{u}^{(\ell-1)},u)|\;u\in C\} \big{)}\] \[\bm{\widetilde{h}}_{B}^{(\ell)} =\textsc{BiEnc}\big{(}B,\{(\bm{\widehat{h}}_{C}^{(\ell)},C)|\;C \in\sigma^{B}\}\big{)}\] \[\bm{\overline{h}}_{\delta_{u}}^{(\ell)} =\textsc{CutEnc}\big{(}\delta_{u},\{(\bm{h}_{v}^{(\ell-1)},v)|\;v \in V\},\{(\bm{\widetilde{h}}_{B}^{(\ell)},B)|\;B\in\pi^{G}\}\big{)}\] \[\bm{h}_{u}^{(\ell)} =\phi\big{(}\bm{h}_{u}^{(\ell-1)},\{\bm{\overline{h}}_{v}^{( \ell-1)}|\;v\in N_{u}\},\{\bm{\overline{h}}_{v}^{(\ell-1)}|\;v\in V\},\{\bm{ \overline{h}}_{T}^{(\ell)}|\;T\in\sigma_{u}^{G}\},\{\bm{\overline{h}}_{B}^{( \ell)}|\;B\in\pi_{u}^{G}\},\bm{\overline{h}}_{\delta_{u}}^{(\ell)}\big{)}\]

where TriEnc, BiEnc, and CutEnc are invariant encoders and \(\phi\) is an Update function. The encoders TriEnc and BiEnc are analogous to TriCode and BiCode of the KHC algorithm. In PlanE, we further simplify the final graph code generation, by learning embeddings for the cut nodes, which is implemented by the CutEnc encoder. For graph-level tasks, we apply a _pooling_ function on final node embeddings to obtain a graph-level embedding \(\bm{z}_{G}^{(L)}\).

There are many choices for deriving PlanE architectures, but we propose a simple model, BasePlanE, to clearly identify the virtue of the model architecture which aligns with KHC, as follows:

**TriEnc.** Given an Spqr component \(C\) and the node representations \(\bm{h}_{u}^{(\ell-1)}\) of each node \(u\), TriEnc encodes \(C\) based on the walk \(\omega\) given by Weinberg's algorithm, and its corresponding sequence \(\kappa\) as:

\[\bm{\widehat{h}}_{C}^{(\ell)}=\textsc{MLP}\left(\sum_{i=1}^{|\omega|}\textsc{ MLP}\left(\bm{h}_{\omega[i]}^{(\ell-1)}\|\bm{p}_{\kappa[i]}\|\bm{p}_{i}\right) \right)\,,\]

where \(\bm{p}_{x}\in\mathbb{R}^{d}\) is the positional embedding [57]. This is a simple sequence model with a positional encoding on the walk, and a second one based on the generated sequence \(\kappa\). Edge features can also be

Figure 3: Given an input graph (A) PlanE computes the BlockCut (B) and Spqr (C) trees and assigns initial embeddings to SPQR components using TriEnc (D). Then, BiEnc iteratively traverses each Spqr tree bottom-up (numbered), updates Spqr embeddings, and computes biconnected component embeddings (E). CutEnc then analogously traverses the BlockCut tree to compute a graph embedding (F). Finally, the node representation is updated (G).

concatenated while respecting the edge order given by the walk. The nodes of \(\textsc{Spqr}(G)\) are one of the types \(S\), \(P\), \(Q\), \(R\), where for \(S\), \(P\), \(Q\) types, we have a trivial ordering for the induced components, and Weinberg's algorithm also gives an ordering for \(R\) nodes that correspond to triconnected components.

**BiEnc.** Given a biconnected component \(B\) and the representations \(\bm{h}_{C}^{(\ell)}\) of each component induced by a node \(C\) in \(\gamma=\textsc{Spqr}(B)\), BiEnc uses the SPQR tree and the integers \(\theta(C,C^{\prime})\) corresponding to how we connect \(C\) and \(C^{\prime}\in\chi(C)\). BiEnc then computes a representation for each subtree \(\gamma_{C}\) induced by a node \(C\) in a bottom up fashion as:

\[\widetilde{\bm{h}}_{\gamma_{C}}^{(\ell)}=\textsc{MLP}\left(\widehat{\bm{h}}_{C }^{(\ell)}+\sum_{C^{\prime}\in\chi(C)}\textsc{MLP}\left(\widetilde{\bm{h}}_{ \gamma_{C^{\prime}}}^{(\ell)}\|\bm{p}_{\theta(C,C^{\prime})}\right)\right).\]

This encoder operates in a bottom up fashion to ensure that a subtree representation of the children of \(C\) exists before it encodes the subtree \(\gamma_{C}\). The representation of the canonical root node in \(\gamma\) is used as the representation of the biconnected component \(B\) by setting: \(\widetilde{\bm{h}}_{B}^{(\ell)}=\widetilde{\bm{h}}_{\gamma_{\text{root}( \gamma)}}^{(\ell)}\).

**CutEnc.** Given a subtree \(\delta_{u}\) of a Block-Cut tree \(\delta\), the representations \(\widetilde{\bm{h}}_{B}^{(\ell)}\) of each biconnected component \(B\), and the node representations \(\bm{h}_{u}^{(\ell-1)}\) of each node, CutEnc sets \(\widetilde{\bm{h}}_{\delta_{u}}^{(\ell)}=\bm{0}^{d(\ell)}\) if \(u\) is _not_ a cut node; otherwise, it computes the subtree representations as:

\[\overline{\bm{h}}_{\delta_{u}}^{(\ell)}=\textsc{MLP}\left(\bm{h}_{u}^{(\ell- 1)}+\sum_{B\in\chi(u)}\textsc{MLP}\left(\widetilde{\bm{h}}_{B}^{(\ell)}+\sum _{v\in\chi(B)}\overline{\bm{h}}_{\delta_{v}}^{(\ell)}\right)\right).\]

The CutEnc procedure is called in a bottom-up order to ensure that the representations of the grandchildren are already computed. We learn the cut node subtree representations instead of employing the hierarchical overrides that are present in the KHC algorithm, as the latter is not ideal in a learning algorithm. However, with sufficient layers, these representations are complete invariants.

**Update.** Putting these altogether, we update the node representations \(\bm{h}_{u}^{(\ell)}\) of each node \(u\) as:

\[\bm{h}_{u}^{(\ell)}=f^{(\ell)}\Big{(}g_{1}^{(\ell)}\big{(}\bm{h} _{u}^{(\ell-1)}+\sum_{v\in N_{u}}\bm{h}_{v}^{(\ell-1)}\big{)}\parallel g_{2}^{ (\ell)}\big{(}\sum_{v\in V}\bm{h}_{v}^{(\ell-1)}\big{)}\parallel\] \[g_{3}^{(\ell)}\big{(}\bm{h}_{u}^{(\ell-1)}+\sum_{C\in\sigma_{u} ^{G}}\widehat{\bm{h}}_{C}^{(\ell)}\big{)}\parallel g_{4}^{(\ell)}\big{(}\bm{h} _{u}^{(\ell-1)}+\sum_{B\in\pi_{u}^{G}}\widetilde{\bm{h}}_{B}^{(\ell)}\big{)} \parallel\widetilde{\bm{h}}_{\delta_{u}}^{(\ell)}\Big{)},\]

where \(f^{(\ell)}\) and \(g_{i}^{(\ell)}\) are either linear maps or two-layer MLPs. Finally, we pool as:

\[\bm{z}_{G}=\textsc{MLP}\left(\big{\|}_{\ell=1}^{L}\Big{(}\sum_{u\in V^{G}}\bm {h}_{u}^{(\ell)}\Big{)}\right).\]

## 6 Expressive power and efficiency of BasePlanE

We present the theoretical result of this paper, which states that BasePlanE can distinguish any pair of planar graphs, even when using only a logarithmic number of layers in the size of the input graphs:

**Theorem 6.1**.: _For any planar graphs \(G_{1}=(V_{1},E_{1},\zeta_{1})\) and \(G_{2}=(V_{2},E_{2},\zeta_{2})\), there exists a parametrization of BasePlanE with at most \(L=\lceil\log_{2}(\max\{|V_{1}|,|V_{2}|\})\rceil+1\) layers, which computes a complete graph invariant, that is, the final graph-level embeddings satisfy \(\bm{z}_{G_{1}}^{(L)}\neq\bm{z}_{G_{2}}^{(L)}\) if and only if \(G_{1}\) and \(G_{2}\) are not isomorphic._

The construction is non-uniform, since the number of layers needed depends on the size of the input graphs. In this respect, our result is similar to other results aligning GNNs with 1-WL with sufficiently many layers [45; 63]. There are, however, two key differences: (i) BasePlanE computes isomorphism-complete invariants over planar graphs and (ii) our construction requires only logarithmic number of layers in the size of the input graphs (as opposed to linear).

The theorem builds on the properties of each encoder being complete. We first show that a single application of TriEnc and BiEnc is sufficient to encode all relevant components of an input graph in an isomorphism-complete way:

**Lemma 6.2**.: _Let \(G=(V,E,\zeta)\) be a planar graph. Then, for any biconnected components \(B,B^{\prime}\) of \(G\), and for any SPQR components \(C\) and \(C^{\prime}\) of \(G\), there exists a parametrization of the functions TriEnc and BiEnc such that:_

1. \(\widetilde{\bm{h}}_{B}\neq\widetilde{\bm{h}}_{B^{\prime}}\) _if and only if_ \(B\) _and_ \(B^{\prime}\) _are not isomorphic, and_
2. \(\widehat{\bm{h}}_{C}\neq\widehat{\bm{h}}_{C^{\prime}}\) _if and only if_ \(C\) _and_ \(C^{\prime}\) _are not isomorphic._

Intuitively, this result follows from a natural alignment to the procedures of the KHC algorithm: the existence of unique codes for different components is proven for the algorithm and we lift this result to the embeddings of the respective graphs, using the universality of MLPs [14, 31, 32].

Our main result rests on a key result related to CutEnc, which states that BasePlanE computes complete graph invariants for all subtrees of the Block-Cut tree. We use an inductive proof, where the logarithmic bound stems from a single layer computing complete invariants for all subtrees induced by cut nodes that have at most one grandchild cut node, the induced subtree of which is incomplete.

**Lemma 6.3**.: _For a planar graph \(G=(V,E,\zeta)\) of order \(n\) and its associated Block-Cut tree \(\delta=\textsc{BlockCut}(G)\), there exists a \(L=\lceil\log_{2}(n)\rceil+1\) layer parametrization of BasePlanE that computes a complete graph invariant for each subtree \(\delta_{u}\) induced by each cut node \(u\)._

With Lemma 6.2 and Lemma 6.3 in place, Theorem 6.1 follows from the fact that every biconnected component and every cut node of the graph are encoded in an isomorphism-complete way, which is sufficient for distinguishing planar graphs.

**Runtime efficiency.** Theoretically, the runtime of one BasePlanE layer is \(\mathcal{O}(|V|d^{2})\) with a (one-off) pre-processing time \(\mathcal{O}(|V|^{2})\) as we elaborate in Appendix B. In practical terms, this makes BasePlanE linear in the number of graph nodes after preprocessing. This is very scalable as opposed to other complete algorithms based on 3-WL (or, oblivious 4-WL).

## 7 Experimental evaluation

In this section, we evaluate BasePlanE in three different settings. First, we conduct three experiments to evaluate the expressive power of BasePlanE. Second, we evaluate a BasePlanE variant using edge features on the MolHIV graph classification task from OGB [33, 61]. Finally, we evaluate this variant on graph regression over ZINC [18] and QM9 [12, 48]. We provide an additional experiment on the runtime of BasePlanE in Appendix B, and an ablation study on ZINC in Appendix C. All experimental details, including hyperparameters, can be found in Appendix D5.

Footnote 5: Across all experiments, we present tabular results, and follow a convention in which the **best** result is bold in **black**, and the second best result is shown in bold in gray.

### Expressiveness evaluation

#### 7.1.1 Planar satisfiability benchmark: EXP

**Experimental setup.** We evaluate BasePlanE on the planar EXP benchmark [1] and compare with standard MPNNs, MPNNs with random node initialization and (higher-order) 3-GCNs [45]. EXP consists of planar graphs which each represent a satisfiability problem (SAT) instance. These instances are grouped into pairs, such that these pairs cannot be distinguished by 1-WL, but lead to different SAT outcomes. The task in EXP is to predict the satisfiability of each instance. To obtain above-random performance on this dataset, a model must have a sufficiently strong expressive power (2-WL or more). To conduct this experiment, we use a 2-layer BasePlanE model with 64-dimensional node embeddings. We instantiate the triconnected component encoder with 16-dimensional positional encodings, each computed using a periodicity of 64.

\begin{table}
\begin{tabular}{l l} \hline \hline Model & Accuracy (\%) \\ \hline GCN & \(50.0_{\pm 0.00}\) \\ GCN-RNI(N) & \(98.0_{\pm 1.85}\) \\
3-GCN & \(99.7_{\pm 0.004}\) \\ \hline BasePlanE & \(\bm{100}_{\pm 0.00}\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Accuracy results on EXP. Baselines are from Abboud et al. [1].

**Results.** All results are reported in Table 1. As expected, BasePlanE perfectly solves the task, achieving a performance of 100% (despite not relying on any higher-order method). BasePlanE solely relies on classical algorithm component decompositions, and does not rely on explicitly selected and designed features, to achieve this performance gain. This experiment highlights that the general algorithmic decomposition effectively improves expressiveness in a practical setup, and leads to strong performance on EXP, where a standard MPNN would otherwise fail.

#### 7.1.2 Planar 3-regular graphs: P3R

**Experimental setup.** We propose a new synthetic dataset P3R based on 3-regular planar graphs, and experiment with BasePlanE, GIN and 2-WL-expressive PPGN [43]. For this experiment, we generated all 3-regular planar graphs of size 10, leading to exactly 9 non-isomorphic graphs. For each such graph, we generated 50 isomorphic graphs by permuting their nodes. The task is then to predict the correct class of an input graph, where the random accuracy is approximately \(11.1\%\). This task is challenging given the regularity of the graphs.

**Results.** We report all results in Table 2. As expected, GIN struggles to go beyond a random guess, whereas BasePlanE and PPGNs easily solve the task, achieving 100% accuracy.

#### 7.1.3 Clustering coefficients of QM9 graphs: QM9CC

In this experiment, we evaluate the ability of BasePlanE to detect structural graph signals _without_ an explicit reference to the target structure. To this end, we propose a simple, yet challenging, synthetic task: given a subset of graphs from QM9, we aim to predict the graph-level _clustering coefficient (CC)_. Computing CC requires counting triangles in the graph, which is impossible to solve with standard MPNNs [65].

**Data.** We select a subset QM9CC of graphs from QM9 to obtain a diverse distribution of CCs. As most QM9 graphs have a CC of 0, we consider graphs with a CC in the interval \([0.06,0.16]\), as this range has high variability. We then normalize the CCs to the unit interval \([0,1]\). We apply the earlier filtering on the original QM9 splits to obtain train/validation/test sets that are direct subsets of the full QM9 splits, and which consist of 44226, 3941 and 3921 graphs, respectively.

**Experimental setup.** Given the small size of QM9 and the locality of triangle counting, we use 32-dimensional node embeddings and 3 layers across all models. Moreover, we use a common 100 epoch training setup for fairness. For evaluation, we report mean absolute error (MAE) on the test set, averaged across 5 runs. For this experiment, our baselines are (i) an input-agnostic constant prediction that returns a minimal test MAE, (ii) the MPNNs GCNs [40] and GIN [63], (iii) ESAN [7], an MPNN that computes sub-structures through node and edge removals, but which does _not_ explicitly extract triangles, and (iv) BasePlanE, using 16-dimensional positional encoding vectors.

**Results.** Results on QM9CC are provided in Table 3. BasePlanE comfortably outperforms standard MPNNs. Indeed, GCN performance is only marginally better than the constant baseline and GIN's MAE is over an order of magnitude behind BasePlanE. This is a very substantial gap, and confirms that MPNNs are unable to accurately detect triangles to compute CCs. Moreover, BasePlanE achieves an MAE over 40% lower than ESAN. Overall, BasePlanE effectively detects triangle structures, despite these not being explicitly provided, and thus its underlying algorithmic decomposition effectively captures latent structural graph properties in this setting.

**Performance analysis.** To better understand our results, we visualize the predictions of BasePlanE, GIN, and GCN using scatter plots in Figure 4. As expected, BasePlanE follows the ideal regression line. By contrast, GIN and GCN are much less stable. Indeed, GIN struggles with CCs at the extremes of the \([0,1]\) range, but is better at intermediate values, whereas GCN is consistently unreliable, and rarely returns predictions above 0.7. This highlights the structural limitation of GCNs, namely its self-loop mechanism for representation updates, which causes ambiguity for detecting triangles.

\begin{table}
\begin{tabular}{l l} \hline \hline Model & Accuracy (\%) \\ \hline GIN & 11.1 \(\pm 0.00\) \\ PPGN & **100**\(\pm 0.00\) \\ \hline BasePlanE & **100**\(\pm 0.00\) \\ \hline \hline \end{tabular}
\end{table}
Table 2: Accuracy results on P3R.

\begin{table}
\begin{tabular}{l l} \hline \hline Model & MAE \\ \hline Constant & 0.1627 \(\pm 0.0000\) \\ GCN & 0.1275 \(\pm 0.0012\) \\ GIN & 0.0612 \(\pm 0.0018\) \\ ESAN & \(0.0038\pm 0.0010\) \\ BasePlanE & **0.0023**\(\pm 0.0004\) \\ \hline \hline \end{tabular}
\end{table}
Table 3: MAE of BasePlanE and baselines on the QM9CC dataset.

### Graph classification on MolHIV

**Model setup.** We use a BasePlanE variant that uses edge features, called E-BasePlane (defined in the appendix), on OGB [33] MolHIV and compare against baselines. We instantiate E-BasePlane with an embedding dimension of 64, 16-dimensional positional encodings, and report the average ROC-AUC across 10 independent runs.

**Results.** The results for E-BasePlanE and other baselines on MolHIV are shown in Table 4: Despite not explicitly extracting relevant cycles and/or molecular sub-structures, E-BasePlane outperforms standard MPNNs and the domain-specific HIMP model. It is also competitive with substructure-aware models CIN and GSN, which include dedicated structures for inference. Therefore, E-BasePlane performs strongly in practice with minimal design effort, and effectively uses its structural inductive bias to remain competitive with dedicated architectures.

### Graph regression on QM9

**Experimental setup.** We map QM9 [48] edge types into features by defining a learnable embedding per edge type, and subsequently apply E-BasePlane to the dataset. We evaluate E-BasePlane on all 13 QM9 properties following the same splits and protocol (with MAE results averaged over 5 test set reruns) of GNN-FiLM [12]. We compare R-SPN against GNN-FiLM models and their fully adjacent (FA) variants [3], as well as shortest path networks (SPNs) [2]. We report results with an 3-layer E-BasePlane using 128-dimensional node embeddings and 32-dimensional positional encodings.

**Results.** E-BasePlane results on QM9 are provided in Table 5. In this table, E-BasePlane outperforms high-hop SPNs, despite being simpler and more efficient, achieving state-of-the-art results on 9 of the 13 tasks. The gains are particularly prominent on the first five properties, where R-SPNs originally provided relatively little improvement over FA models, suggesting that E-BasePlane offers complementary structural advantages to SPNs. This was corroborated in our experimental tuning: E-BasePlane performance peaks around 3 layers, whereas SPN performance continues to improve up to 8 (and potentially more) layers, which suggests that E-BasePlane is more efficient at directly communicating information, making further message passing redundant.

Overall, E-BasePlane maintains the performance levels of R-SPN with a smaller computational footprint. Indeed, messages for component representations efficiently propagate over trees in E-BasePlane, and the number of added components is small (see appendix for more details). Therefore E-BasePlane and the PlanE framework offer a more scalable alternative to explicit higher-hop neighborhood message passing over planar graphs.

\begin{table}
\begin{tabular}{l c} \hline \hline GCN [40] & \(75.58_{\pm 0.97}\) \\ GIN [63] & \(77.07_{\pm 1.40}\) \\ PNA [13] & \(79.05_{\pm 1.32}\) \\ \hline ESAN [7] & \(78.00_{\pm 1.42}\) \\ GSN [11] & \(80.39_{\pm 0.90}\) \\ CIN [9] & \(\mathbf{80.94_{\pm 0.57}}\) \\ \hline HIMP [20] & \(78.80_{\pm 0.82}\) \\ \hline E-BasePlane & \(80.04_{\pm 0.50}\) \\ \hline \hline \end{tabular}
\end{table}
Table 4: ROC-AUC of BasePlane and baselines on MolHIV.

Figure 4: Scatter plots of BasePlane, GIN, and GCN predictions versus the true normalized CC. The red line represents ideal behavior where predictions match the true normalized CC.

### Graph regression on ZINC

**Experimental setup.** We (i) evaluate BasePlanE on the ZINC subset (12k graphs) without edge features, (ii) evaluate E-BasePlanE on this subset and on the full ZINC dataset (500k graphs). To this end, we run BasePlanE and E-BasePlanE with 64 and 128-dimensional embeddings, 16-dimensional positional embeddings, and 3 layers. For evaluation, we compute MAE on the respective test sets, and report the best average of 10 runs across all experiments.

**Results.** Results on ZINC are shown in Table 6: Both BasePlanE and E-BasePlanE perform strongly, with E-BasePlanE achieving state-of-the-art performance on ZINC12k with edge features and both models outperforming all but one baseline in the other two settings. These results are very promising, and highlight the robustness of (E-)BasePlanE.

## 8 Limitations, discussions, and outlook

Overall, both BasePlanE and E-BasePlanE perform strongly across all our experimental evaluation tasks, despite competing against specialized models in each setting. Moreover, both models are isomorphism-complete over planar graphs. This implies that these models benefit substantially from the structural inductive bias and expressiveness of classical planar algorithms, which in turn makes them a reliable, efficient, and robust solution for representation learning over planar graphs.

Though the PlanE framework is a highly effective and easy to use solution for planar graph representation learning, it is currently limited to planar graphs. Indeed, the classical algorithms underpinning PlanE do not naturally extend beyond the planar graph setting, which in turn limits the applicability of the approach. Thus, a very important avenue for future work is to explore alternative (potentially incomplete) graph decompositions that strike a balance between structural inductive bias, efficiency and expressiveness on more general classes of graphs.

\begin{table}
\begin{tabular}{l c c c} \hline \hline \multirow{2}{*}{Edge Features} & \multicolumn{1}{c}{ZINC(12k)} & \multicolumn{1}{c}{ZINC(12k)} & \multicolumn{1}{c}{ZINC(Full)} \\  & No & Yes & Yes \\ \hline GCN [40] & \(0.278\pm 0.003\) & - & - \\ GIN-(E) [34, 63] & \(0.387\pm 0.015\) & \(0.252\pm 0.014\) & \(0.088\pm 0.002\) \\ PNA [13] & \(0.320\pm 0.032\) & \(0.188\pm 0.004\) & \(0.320\pm 0.032\) \\ \hline GSN [11] & \(0.140\pm 0.006\) & \(0.101\pm 0.010\) & - \\ CIN [9] & \(\textbf{0.115}\pm 0.003\) & \(0.079\pm 0.006\) & \(\textbf{0.022}\pm 0.002\) \\ ESAN [7] & - & \(0.102\pm 0.003\) & - \\ \hline HIMP [20] & - & \(0.151\pm 0.006\) & \(0.036\pm 0.002\) \\ \hline (E-)BasePlanE & \(0.124\pm 0.004\) & \(\textbf{0.076}\pm 0.003\) & \(0.028\pm 0.002\) \\ \hline \hline \end{tabular}
\end{table}
Table 6: MAE of (E-)BasePlanE and baselines on ZINC.

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline \multirow{2}{*}{Property} & \multicolumn{1}{c}{R-GIN} & \multicolumn{1}{c}{R-GAT} & \multicolumn{1}{c}{R-SPN} & \multicolumn{1}{c}{BasePlanE} \\ \cline{2-7}  & base & +FA & base & +FA & \(k=5\) & \(k=10\) & \\ \hline mu & \(2.64\pm 0.11\) & \(2.54\pm 0.09\) & \(2.68\pm 0.11\) & \(2.73\pm 0.07\) & \(2.16\pm 0.08\) & \(2.21\pm 0.21\) & \(\textbf{1.97}\pm 0.03\) \\ alpha & \(4.67\pm 0.52\) & \(2.28\pm 0.04\) & \(4.65\pm 0.44\) & \(2.32\pm 0.16\) & \(1.74\pm 0.05\) & \(1.66\pm 0.06\) & \(\textbf{1.63}\pm 0.01\) \\ HOMO & \(1.42\pm 0.01\) & \(1.26\pm 0.02\) & \(1.48\pm 0.03\) & \(1.43\pm 0.02\) & \(1.19\pm 0.04\) & \(1.20\pm 0.08\) & \(\textbf{1.15}\pm 0.01\) \\ LUMO & \(1.50\pm 0.09\) & \(1.34\pm 0.04\) & \(1.53\pm 0.07\) & \(1.41\pm 0.03\) & \(1.13\pm 0.01\) & \(1.20\pm 0.06\) & \(\textbf{1.06}\pm 0.02\) \\ gap & \(2.27\pm 0.09\) & \(1.96\pm 0.04\) & \(2.31\pm 0.06\) & \(2.08\pm 0.05\) & \(1.76\pm 0.03\) & \(1.77\pm 0.06\) & \(\textbf{1.73}\pm 0.02\) \\ R2 & \(15.63\pm 1.40\) & \(12.61\pm 0.37\) & \(52.39\pm 42.5\) & \(15.76\pm 1.17\) & \(10.59\pm 0.35\) & \(10.63\pm 0.10\) & \(\textbf{10.53}\pm 0.55\) \\ ZPVE & \(12.93\pm 1.81\) & \(5.03\pm 0.36\) & \(14.87\pm 2.88\) & \(5.98\pm 0.43\) & \(3.16\pm 0.06\) & \(\textbf{2.58}\pm 0.13\) & \(2.81\pm 0.16\) \\ U0 & \(5.88\pm 1.01\) & \(2.21\pm 0.12\) & \(7.61\pm 0.46\) & \(2.19\pm 0.25\) & \(1.10\pm 0.03\) & \(\textbf{0.89}\pm 0.05\) & \(0.95\pm 0.04\) \\ U & \(18.71\pm 23.36\) & \(2.32\pm 0.18\) & \(6.86\pm 0.53\) & \(2.11\pm 0.10\) & \(1.09\pm 0.05\) & \(\textbf{0.93}\pm 0.03\) & \(0.94\pm 0.04\) \\ H & \(5.62\pm 0.81\) & \(2.26\pm 0.19\) & \(7.64\pm 0.92\) & \(2.27\pm 0.29\) & \(1.10\pm 0.03\) & \(\textbf{0.92}\pm 0.03\) & \(\textbf{0.92}\pm 0.04\) \\ G & \(5.38\pm 0.75\) & \(2.04\pm 0.24\) & \(6.54\pm 0.36\) & \(2.07\pm 0.07\) & \(1.04\pm 0.04\) & \(\textbf{0.83}\pm 0.05\) & \(0.88\pm 0.04\) \\ Cv & \(3.53\pm 0.37\) & \(1.86\pm 0.03\) & \(4.11\pm 0.27\) & \(2.03\pm 0.14\) & \(1.34\pm 0.03\) & \(1.23\pm 0.06\) & \(\textbf{1.20}\pm 0.06\) \\ Omega & \(1.05\pm 0.11\) & \(0.80\pm 0.04\) & \(1.48\pm 0.87\) & \(0.73\pm 0.04\) & \(0.53\pm 0.02\) & \(0.52\pm 0.02\) & \(\textbf{0.45}\pm 0.01\) \\ \hline \hline \end{tabular}
\end{table}
Table 5: MAE of E-BasePlanE and baselines on QM9. Other model results and their fully adjacent (FA) extensions are as previously reported [2; 3].

## Acknowledgments and Disclosure of Funding

The authors would like to thank the anonymous reviewers for their feedback which led to substantial improvements in the presentation of the paper. The authors would like to also acknowledge the use of the University of Oxford Advanced Research Computing (ARC) facility in carrying out this work (http://dx.doi.org/10.5281/zenodo.22558).

## References

* Abboud et al. [2021] Ralph Abboud, Ismail Ilkan Ceylan, Martin Grohe, and Thomas Lukasiewicz. The surprising power of graph neural networks with random node initialization. In _Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI_, 2021.
* Abboud et al. [2022] Ralph Abboud, Radoslav Dimitrov, and Ismail Ilkan Ceylan. Shortest path networks for graph property prediction. In _Proceedings of the First Annual Learning on Graphs Conference, LoG_, 2022.
* Alon and Yahav [2021] Uri Alon and Eran Yahav. On the bottleneck of graph neural networks and its practical implications. In _Proceedings of the Ninth International Conference on Learning Representations, ICLR_, 2021.
* Babai [2016] Laszlo Babai. Graph isomorphism in quasipolynomial time. In _Proceedings of the Forty-Eighth Annual ACM Symposium on Theory of Computing, STOC_, 2016.
* Barcelo et al. [2020] Pablo Barcelo, Egor V. Kostylev, Mikael Monet, Jorge Perez, Juan L. Reutter, and Juan Pablo Silva. The logical expressiveness of graph neural networks. In _Proceedings of the Eighth International Conference on Learning Representations, ICLR_, 2020.
* Battaglia et al. [2018] Peter W Battaglia, Jessica B Hamrick, Victor Bapst, Alvaro Sanchez-Gonzalez, Vinicius Zambaldi, Mateusz Malinowski, Andrea Tacchetti, David Raposo, Adam Santoro, Ryan Faulkner, et al. Relational inductive biases, deep learning, and graph networks. _arXiv preprint:1806.01261_, 2018.
* Bevilacqua et al. [2022] Beatrice Bevilacqua, Fabrizio Frasca, Derek Lim, Balasubramaniam Srinivasan, Chen Cai, Gopinath Balamurugan, Michael M. Bronstein, and Haggai Maron. Equivariant subgraph aggregation networks. In _Proceedings of the Tenth International Conference on Learning Representations, ICLR_, 2022.
* Bhatt and Leighton [1984] Sandeep N Bhatt and Frank Thomson Leighton. A framework for solving VLSI graph layout problems. _Journal of Computer and System Sciences_, 28(2):300-343, 1984.
* Bodnar et al. [2021] Cristian Bodnar, Fabrizio Frasca, Nina Otter, Yuguang Wang, Pietro Lio, Guido F. Montufar, and Michael M. Bronstein. Weisfeiler and lehman go cellular: CW networks. In _Proceedings of the Thirty-Fourth Annual Conference on Advances in Neural Information Processing Systems, NeurIPS_, 2021.
* Booth and Lueker [1976] Kellogg S Booth and George S Lueker. Testing for the consecutive ones property, interval graphs, and graph planarity using pq-tree algorithms. _Journal of Computer and System Sciences_, 13(3):335-379, 1976.
* Bouritsas et al. [2023] Giorgos Bouritsas, Fabrizio Frasca, Stefanos Zafeiriou, and Michael M. Bronstein. Improving graph neural network expressivity via subgraph isomorphism counting. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 45(1):657-668, 2023.
* Brockschmidt [2020] Marc Brockschmidt. GNN-FiLM: Graph neural networks with feature-wise linear modulation. In _Proceedings of the Thirty-Seventh International Conference on Machine Learning, ICML_, 2020.
* Corso et al. [2020] Gabriele Corso, Luca Cavalleri, Dominique Beaini, Pietro Lio, and Petar Velickovic. Principal neighbourhood aggregation for graph nets. In _Proceedings of the Thirty-Third Annual Conference on Advances in Neural Information Processing Systems, NeurIPS_, 2020.

* [14] George Cybenko. Approximation by superpositions of a sigmoidal function. _Mathematics of Control, Signals and Systems_, 2(4):303-314, 1989.
* [15] George Dasoulas, Ludovic Dos Santos, Kevin Scaman, and Aladin Virmaux. Coloring graph neural networks for node disambiguation. In _Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI_, 2020.
* [16] G. Di Battista and R. Tamassia. On-line maintenance of triconnected components with SPQR-trees. _Algorithmica_, 15(4):302-318, 1996.
* [17] David Duvenaud, Dougal Maclaurin, Jorge Aguilera-Iparraguirre, Rafael Gomez-Bombarelli, Timothy Hirzel, Alan Aspuru-Guzik, and Ryan P. Adams. Convolutional networks on graphs for learning molecular fingerprints. In _Proceedings of the Twenty-Eighth Annual Conference on Advances in Neural Information Processing Systems, NIPS_, 2015.
* [18] Vijay Prakash Dwivedi, Chaitanya K Joshi, Anh Tuan Luu, Thomas Laurent, Yoshua Bengio, and Xavier Bresson. Benchmarking graph neural networks. _Journal of Machine Learning Research_, 24(43):1-48, 2023.
* [19] David Eppstein. Subgraph isomorphism in planar graphs and related problems. _Journal of Graph Algorithms and Applications_, 3(3):1-27, 1999.
* [20] M. Fey, J. G. Yuen, and F. Weichert. Hierarchical inter-message passing for learning on molecular graphs. In _ICML Graph Representation Learning and Beyond (GRL+) Workshop_, 2020.
* [21] Alex Fout, Jonathon Byrd, Basir Shariat, and Asa Ben-Hur. Protein interface prediction using graph convolutional networks. In _Proceedings of the Thirtieth Annual Conference on Advances in Neural Information Processing Systems, NIPS_, 2017.
* [22] Justin Gilmer, Samuel S. Schoenholz, Patrick F. Riley, Oriol Vinyals, and George E. Dahl. Neural message passing for quantum chemistry. In _Proceedings of the Thirty-Fourth International Conference on Machine Learning, ICML_, 2017.
* [23] Marco Gori, Gabriele Monfardini, and Franco Scarselli. A new model for learning in graph domains. In _Proceedings of the 2005 IEEE International Joint Conference on Neural Networks, IJCNN_, 2005.
* [24] Martin Grohe. _Descriptive Complexity, Canonisation, and Definable Graph Structure Theory_. Lecture Notes in Logic. Cambridge University Press, 2017.
* [25] Martin Grohe. The logic of graph neural networks. In _Proceedings of the Thirty-Sixth Annual ACM/IEEE Symposium on Logic in Computer Science, LICS_, 2021.
* [26] Martin Grohe and Pascal Schweitzer. The graph isomorphism problem. _Communications of the ACM_, 63(11):128-134, 2020.
* [27] Carsten Gutwenger and Petra Mutzel. A linear time implementation of spqr-trees. In _Proceedings of the Eigth International Symposium on Graph Drawing, GD_, 2000.
* [28] J. E. Hopcroft and J. K. Wong. Linear time algorithm for isomorphism of planar graphs (preliminary report). In _Proceedings of the Sixth Annual ACM Symposium on Theory of Computing, STOC_, 1974.
* [29] John Hopcroft and Robert Tarjan. A v2 algorithm for determining isomorphism of planar graphs. _Information Processing Letters_, 1(1):32-34, 1971.
* [30] John Hopcroft and Robert Tarjan. Algorithm 447: Efficient algorithms for graph manipulation. _Communications of the ACM_, 16(6):372-378, 1973.
* [31] Kurt Hornik. Approximation capabilities of multilayer feedforward networks. _Neural Networks_, 4(2):251-257, 1991.
* [32] Kurt Hornik, Maxwell Stinchcombe, and Halbert White. Multilayer feedforward networks are universal approximators. _Neural Networks_, 2(5):359-366, 1989.

* [33] Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, and Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs. In _Proceedings of the Thirty-Third Annual Conference on Advances in Neural Information Processing Systems, NeurIPS_, 2020.
* [34] Weihua Hu, Bowen Liu, Joseph Gomes, Marinka Zitnik, Percy Liang, Vijay S. Pande, and Jure Leskovec. Strategies for pre-training graph neural networks. In _Proceedings of the Eighth International Conference on Learning Representations, ICLR_, 2020.
* [35] Neil Immerman and Eric Lander. Describing graphs: A first-order approach to graph canonization. _Complexity Theory Retrospective_, pages 59-81, 1990.
* [36] Steven M. Kearnes, Kevin McCloskey, Marc Berndl, Vijay S. Pande, and Patrick Riley. Molecular graph convolutions: moving beyond fingerprints. _Journal of Computer Aided Molecular Design_, 30(8):595-608, 2016.
* [37] Nicolas Keriven and Gabriel Peyre. Universal invariant and equivariant graph neural networks. In _Proceedings of the Thirty-Second Annual Conference on Advances in Neural Information Processing Systems, NeurIPS_, 2019.
* [38] Sandra Kiefer, Ilia Ponomarenko, and Pascal Schweitzer. The weisfeiler-leman dimension of planar graphs is at most 3. _Journal of the ACM_, 66(6):1-31, 2019.
* [39] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In _Proceedings of the Third International Conference on Learning Representations, ICLR_, 2015.
* [40] Thomas Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. In _Proceedings of the Fifth International Conference on Learning Representations, ICLR_, 2017.
* [41] Jacek Kukluk, Lawrence Holder, and Diane Cook. Algorithm and experiments in testing planar graphs for isomorphism. _Journal of Graph Algorithms and Applications_, 8(3):313-356, 2004.
* [42] Andreas Loukas. What graph neural networks cannot learn: depth vs width. In _Proceedings of the Eighth International Conference on Learning Representations, ICLR_, 2020.
* [43] Haggai Maron, Heli Ben-Hamu, Hadar Serviansky, and Yaron Lipman. Provably powerful graph networks. In _Proceedings of the Thirty-Second Annual Conference on Advances in Neural Information Processing Systems, NeurIPS_, 2019.
* [44] Haggai Maron, Ethan Fetaya, Nimrod Segol, and Yaron Lipman. On the universality of invariant networks. In _Proceedings of the Thirty-Sixth International Conference on Machine Learning, ICML_, 2019.
* [45] Christopher Morris, Martin Ritzert, Matthias Fey, William L. Hamilton, Jan Eric Lenssen, Gaurav Rattan, and Martin Grohe. Weisfeiler and Leman go neural: Higher-order graph neural networks. In _Proceedings of the Thirty-Third AAAI Conference on Artificial Intelligence, AAAI_, 2019.
* [46] Giannis Nikolentzos and Michalis Vazirgiannis. Random walk graph neural networks. In _Proceedings of the Thirty-Third Annual Conference on Advances in Neural Information Processing Systems, NeurIPS_, 2020.
* [47] Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. Deepwalk: online learning of social representations. In _Proceedings of the Twentieth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD_, 2014.
* [48] Raghunathan Ramakrishnan, Pavlo O Dral, Matthias Rupp, and O Anatole Von Lilienfeld. Quantum chemistry structures and properties of 134 kilo molecules. _Scientific data_, 1(1):1-7, 2014.
* [49] Louis C. Ray and Russell A. Kirsch. Finding chemical records by digital computers. _Science_, 126(3278):814-819, 1957.

* [50] Ryoma Sato, Makoto Yamada, and Hisashi Kashima. Random features strengthen graph neural networks. In _Proceedings of the 2021 SIAM International Conference on Data Mining, SDM_, 2021.
* [51] Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini. The graph neural network model. _IEEE Transactions on Neural Networks_, 20(1):61-80, 2009.
* [52] Wei-Kuan Shih and Wen-Lian Hsu. A new planarity test. _Theoretical Computer Science_, 223(1-2):179-191, 1999.
* [53] Jonathan Shlomi, Peter Battaglia, and Jean-Roch Vlimant. Graph neural networks in particle physics. _Machine Learning: Science and Technology_, 2(2):021001, 2021.
* [54] Howard E. Simmons and John E. Maggio. Synthesis of the first topologically non-planar molecule. _Tetrahedron Letters_, 22(4):287-290, 1981.
* [55] Daniel D. Sleator and Robert Endre Tarjan. A data structure for dynamic trees. In _Proceedings of the Thirteenth Annual ACM Symposium on Theory of Computing, STOC_, 1981.
* [56] Erik H. Thiede, Wenda Zhou, and Risi Kondor. Autobahn: Automorphism-based graph neural nets. In _Proceedings of the Thirty-Fourth Annual Conference on Advances in Neural Information Processing Systems, NeurIPS_, 2021.
* [57] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In _Proceedings of the Thirtieth Annual Conference on Advances in Neural Information Processing Systems, NIPS_, 2017.
* [58] Louis Weinberg. A simple and efficient algorithm for determining isomorphism of planar triply connected graphs. _IEEE Transactions on Circuit Theory_, 13(2):142-148, 1966.
* [59] Boris Weisfeiler and Andrei A Lehman. A reduction of a graph to a canonical form and an algebra arising during this reduction. _Nauchno-Technicheskaya Informatsia_, 2(9):12-16, 1968.
* [60] Hassler Whitney. A set of topological invariants for graphs. _American Journal of Mathematics_, 55(1):231-235, 1933.
* [61] Zhenqin Wu, Bharath Ramsundar, Evan N Feinberg, Joseph Gomes, Caleb Geniesse, Aneesh S Pappu, Karl Leswing, and Vijay Pande. Moleculenet: a benchmark for molecular machine learning. _Chemical science_, 9(2):513-530, 2018.
* [62] Feng Xie and David Levinson. Topological evolution of surface transportation networks. _Computers, Environment and Urban Systems_, 33(3):211-223, 2009.
* [63] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? In _Proceedings of the Seventh Annual Conference on Learning Representations, ICLR_, 2019.
* [64] Chengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin Zheng, Guolin Ke, Di He, Yanming Shen, and Tie-Yan Liu. Do transformers really perform badly for graph representation? In _Proceedings of the Thirty-Fourth Annual Conference on Advances in Neural Information Processing Systems, NeurIPS_, 2021.
* [65] Jiaxuan You, Jonathan M. Gomes-Selman, Rex Ying, and Jure Leskovec. Identity-aware graph neural networks. In _Proceedings of the Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI_, 2021.
* [66] Bohang Zhang, Shengjie Luo, Liwei Wang, and Di He. Rethinking the expressive power of gnns via graph biconnectivity. In _Proceedings of the Eleventh International Conference on Learning Representations, ICLR_, 2023.
* [67] Muhan Zhang and Pan Li. Nested graph neural networks. In _Proceedings of the Thirty-Fifth Annual Conference on Advanced in Neural Information Processing Systems, NeurIPS_, 2021.
* [68] Marinka Zitnik, Monica Agrawal, and Jure Leskovec. Modeling polypharmacy side effects with graph convolutional networks. _Bioinformatics_, 34(13):i457-i466, 2018.

Proofs of the statements

In this section, we provide the proofs of the statements from the main paper. Throughout the proofs, we make the standard assumption that the initial node features are from a compact space \(K\subseteq\mathbb{R}^{d}\), for some \(d\in\mathbb{N}^{+}\). We also often need to canonically map elements of finite sets to the integer domain: given a finite set \(S\), and any element \(x\in S\) of this set, the function \(\Psi(S,x):S\to\{1,\ldots,|S|\}\) maps \(x\) to a unique integer index given by some fixed order over the set \(S\). Furthermore, we also often need to injectively map sets into real numbers, which is given by the following lemma.

**Lemma A.1**.: _Given a finite set \(S\), there exists an injective map \(g:\mathbb{P}(S)\to[0,1]\)._

Proof.: For each subset \(M\subseteq S\), consider the mapping:

\[g(M)=\frac{1}{1+\sum_{x\in M}\Psi(S,x)|S|^{\Psi(M,x)}},\]

which clearly satisfies \(g(M_{1})\neq g(M_{2})\) for any \(M_{1}\neq M_{2}\subseteq S\). 

We now provide proofs for Lemma 6.2, Lemma 6.3 which are essential for Theorem 6.1. Let us first prove Lemma 6.2:

**Lemma 6.2**.: _Let \(G=(V,E,\zeta)\) be a planar graph. Then, for any biconnected components \(B,B^{\prime}\) of \(G\), and for any SPQR components \(C\) and \(C^{\prime}\) of \(G\), there exists a parameterization of the functions TriEnc and BiEnc such that:_

1. \(\widetilde{\bm{h}}_{B}\neq\widetilde{\bm{h}}_{B^{\prime}}\) _if and only if_ \(B\) _and_ \(B^{\prime}\) _are not isomorphic, and_
2. \(\widehat{\bm{h}}_{C}\neq\widehat{\bm{h}}_{C^{\prime}}\) _if and only if_ \(C\) _and_ \(C^{\prime}\) _are not isomorphic._

Proof.: We show this by first giving a parameterization of TriEnc to distinguish all SPQR components, and then use this construction to give a parameterization of BiEnc to distinguish all biconnected components. The proof aligns the encoders with the code generation procedure in the KHC algorithm. The parameterization needs only a single layer, so we will drop the superscripts in the node representations and write, e.g., \(\bm{h}_{u}\), for brevity. The construction yields single-dimensional real-valued vectors, and, for notational convenience, we view the resulting representations as reals.

Parameterizing TriEnc.We initialize the node features as \(\bm{h}_{u}=\zeta(u)\), where \(\zeta:V\to\mathbb{R}^{d}\). Given an SPQR component \(C\) and the initial node representations \(\bm{h}_{u}\) of each node \(u\), TriEnc encodes \(C\) based on the walk \(\omega\) given by Weinberg's algorithm, and its corresponding sequence \(\kappa\) as:

\[\widehat{\bm{h}}_{C}=\text{MLP}\left(\sum_{i=1}^{|\omega|}\text{MLP}\left( \bm{h}_{\omega[i]}\|\bm{p}_{\kappa[i]}\|\bm{p}_{i}\right)\right).\] (1)

In other words, we rely on the walks \(\omega\) generated by Weinberg's algorithm: we create a walk on the triconnected components that visits each edge exactly twice. Let us fix \(n=4(|V|+|E|+1)\), which serves as an upper bound for size of the walks \(\omega\), and the size of the walk-induced sequence \(\kappa\).

We show a bijection between the multiset of codes \(\mathcal{F}\) (given by the KHC algorithm), and the multiset of representations \(\mathcal{M}\) (given by TriEnc), where the respective multisets are defined, based on \(G\), as follows:

* \(\mathcal{F}=\{\!\text{code}(C)\mid C\in\sigma^{G}\!\}\): the multiset of all codes of the SPQR components of \(G\).
* \(\mathcal{M}=\{\!\widehat{\bm{h}}_{C}\mid C\in\sigma^{G}\!\}\): the multiset of the representations of the SPQR components of \(G\).

Specifically, we prove the following:

**Claim 1**.: _There exists a bijection \(\rho\) between \(\mathcal{F}\) and \(\mathcal{M}\) such that, for any SPQR component \(C\):_

\[\rho(\widehat{\bm{h}}_{C})=\text{code}(C)\text{ and }\rho^{-1}(\text{code}(C))= \widehat{\bm{h}}_{C}.\]Once Claim 1 established, the desired result is immediate, since, using the bijection \(\rho\), and the completeness of the codes generated by the KHC algorithm, we obtain:

\[\widehat{\bm{h}}_{C} \neq\widehat{\bm{h}}_{C^{\prime}}\] \[\Leftrightarrow\] \[\rho^{-1}(\textsc{code}(C)) \neq\rho^{-1}(\textsc{code}(C^{\prime}))\] \[\Leftrightarrow\] \[\textsc{code}(C) \neq\textsc{code}(C^{\prime})\] \[\Leftrightarrow\] \[C\text{ and }C^{\prime}\text{ are non-isomorphic}.\]

To prove the Claim 1, first note that the canonical code given by Weinberg's algorithm for any component \(C\) has the form:

\[\textsc{code}(C)=\textsc{TriCode}(C)= \left(\kappa[1],\textsc{code}((\{\omega[1]\},\{\}))\right), \ldots,\left(\kappa[k],\textsc{code}((\{\omega[k]\},\{\}))\right)\] \[= \left(\kappa[1],\zeta(\omega[1])\right),\ldots,\left(\kappa[k], \zeta(\omega[k])\right).\] (2)

There is a trivial bijection between the codes of the form (2) and sets of the following form:

\[S_{C}=\left\{(\zeta(\omega[i]),\kappa[i],i)\mid 1\leq i\leq|\omega|\right\},\] (3)

and, as a result, for each component \(C\), we can refer to the set \(S_{C}\) that represents this component \(C\) instead of \(\textsc{code}(C)\). Sets of this form are of bounded size (since the number of walks are bounded in \(C\)) and each of their elements is from a countable set. By Lemma A.1 there exists an injective map \(g\) between such sets and the interval \([0,1]\). Since the size of every such set is bounded, and every tuple is from a countable set, we can apply Lemma 5 of Xu et al. [63], and decompose the function \(g\) as:

\[g(S_{C})=\phi\left(\sum_{x\in S_{C}}f(x)\right),\]

for an appropriate choice of \(\phi:\mathbb{R}^{d^{\prime}}\to[0,1]\) and \(f(x)\in\mathbb{R}^{d^{\prime}}\). Based on the structure of the elements of \(S_{C}\), we can further rewrite this as follows:

\[g(S_{C})=\phi\left(\sum_{i=1}^{|\omega|}f\left((\zeta(\omega[i]),\kappa[i],i) \right)\right).\] (4)

Observe that this function closely resembles TriEnc (1). More concretely, for any \(\omega\) and \(i\), we have that \(\zeta(\omega[i])=\bm{h}_{\omega[i]}\), and, moreover, \(\bm{p}_{\kappa[i]}\) and \(\bm{p}_{i}\) are the positional encodings of \(\kappa[i]\), and \(i\), respectively. Hence, it is easy to see that there exists a function \(\mu\), which satisfies, for every \(i\):

\[((\zeta(\omega[i])),\kappa[i],i)=\mu(\bm{h}_{\omega[i]}\|\bm{p}_{\kappa[i]}\| \bm{p}_{i}).\]

This implies the following:

\[g(S_{C}) =\phi\left(\sum_{i=1}^{|\omega|}f\left((\zeta(\omega[i]),\kappa[i ],i)\right)\right)\] \[=\phi\left(\sum_{i=1}^{|\omega|}f\left(\mu(\bm{h}_{\omega[i]}\| \bm{p}_{\kappa[i]}\|\bm{p}_{i})\right)\right)\] \[=\phi\left(\sum_{i=1}^{|\omega|}(f\circ\mu)(\bm{h}_{\omega[i]}\| \bm{p}_{\kappa[i]}\|\bm{p}_{i})\right).\]

Observe that this function can be parameterized by TriEnc: we apply the universal approximation theorem [31, 14, 32], and encode \((f\circ\mu)\) with an MLP (i.e., the inner MLP) and similarly encode \(\phi\) with another MLP (i.e., the outer MLP).

This establishes a bijection \(\rho\) between \(\mathcal{F}\) and \(\mathcal{M}\): for any SPQR component \(C\), we can injectively map both the code \(\textsc{code}(C)\) (or, equivalently the corresponding set \(S_{C}\)) and the representation \(\widehat{\bm{h}}_{C}\) to the same unique value using the function \(g\) as \(\widehat{\bm{h}}_{C}=g(S_{C})\), and we have shown that there exists a parameterization of TriEnc for this target function \(g\).

Parameterizing BiEnc.In this case, we are given a biconnected component \(B\) and the representations \(\widehat{\bm{h}}_{C}\) of each component \(C\) from the SPQR tree \(\gamma=\textsc{Spr}(B)\). We consider the representations \(\widehat{\bm{h}}_{C}\) which are a result of the parameterization of TriEnc, described earlier.

BiEnc uses the SPQR tree and the integers \(\theta(C,C^{\prime})\) corresponding to how we connect \(C\) and \(C^{\prime}\in\chi(C)\). BiEnc then computes a representation for each subtree \(\gamma_{C}\) induced by a node \(C\) in a bottom up fashion as:

\[\widetilde{\bm{h}}_{\gamma_{C}}=\textsc{MLP}\left(\widehat{\bm{h}}_{C}+\sum_{ C^{\prime}\in\chi(C)}\textsc{MLP}\left(\widetilde{\bm{h}}_{\gamma_{C^{\prime}}} \|\bm{p}_{\theta(C,C^{\prime})}\right)\right).\] (5)

This encoder operates in a bottom up fashion to ensure that a subtree representation of the children of \(C\) exists before it encodes the subtree \(\gamma_{C}\). The representation of the canonical root node in \(\gamma\) is used as the representation of the biconnected component \(B\) by setting: \(\widetilde{\bm{h}}_{B}=\widetilde{\bm{h}}_{\gamma_{\textsc{root}(\gamma)}}\).

To show the result, we first note that the canonical code given by the KHC algorithm also operates in a bottom up fashion on the subtrees of \(\gamma\). We have two cases:

_Case 1._ For a _leaf node_\(C\) in \(\gamma\), the code for \(\gamma_{C}\) is given by \(\textsc{code}(\gamma_{C})=\textsc{TriCode}(C)\). This case can be seen as a special case of Case 2 (and we will treat it as such).

_Case 2._ For a _non-leaf node_\(C\), we concatenate the codes of the subtrees induced by the children of \(C\) in their lexicographical order, by first prepending the integer given by \(\theta\) to each child code. Then, we also prepend the code of the SPQR component \(C\) to this concatenation to get \(\textsc{code}(\gamma_{C})\). More precisely, if the lexicographical ordering of \(\chi(u)\), based on \(\textsc{code}(\gamma_{C^{\prime}})\) for a given \(C^{\prime}\in\chi(C)\) is \(x[1],\ldots,x[|\chi(C)|]\), then the code for \(\gamma_{C}\) is given by:

\[\textsc{code}(\gamma_{C})=\left(\textsc{TriCode}(C),(\theta(C,x[1]),\textsc{ code}(\gamma_{x[1]})),\ldots,(\theta(C,x[|x|]),\textsc{code}(\gamma_{x[|x|]}))\right)\] (6)

We show a bijection between the multiset of codes \(\mathcal{F}\) (given by the KHC algorithm), and the multiset of representations \(\mathcal{M}\) (given by BiEnc), where the respective multisets are defined, based on \(G\) and the SPQR tree \(\gamma\), as follows:

* \(\mathcal{F}=\{\textsc{code}(\gamma_{C})\mid C\in\gamma\}\): the multiset of all codes of all the induced SPQR subtrees.
* \(\mathcal{M}=\{\widetilde{\bm{h}}_{\gamma_{C}}\mid C\in\gamma\}\): the multiset of the representations of all the induced SPQR subtrees.

Analogously to the proof of TriEnc, we prove the following claim:

**Claim 2**.: _There exists a bijection \(\rho\) between \(\mathcal{F}\) and \(\mathcal{M}\) such that, for any node \(C\) in \(\gamma\):_

\[\rho(\widetilde{\bm{h}}_{\gamma_{C}})=\textsc{code}(\gamma_{C})\text{ and }\rho^{-1}(\textsc{code}(\gamma_{C}))=\widetilde{\bm{h}}_{\gamma_{C}}\]

Given Claim 2, the result follows, since, using the bijection \(\rho\), and the completeness of the codes generated by the KHC algorithm, we obtain:

\[\widetilde{\bm{h}}_{B} \neq\widetilde{\bm{h}}_{B^{\prime}}\] \[\Leftrightarrow\] \[\widetilde{\bm{h}}_{\gamma_{\textsc{root}}(B)} \neq\widetilde{\bm{h}}_{\gamma_{\textsc{root}}(B^{\prime})}\] \[\Leftrightarrow\] \[\rho^{-1}(\textsc{code}(\textsc{root}(B))) \neq\rho^{-1}(\textsc{code}(\textsc{root}(B^{\prime})))\] \[\Leftrightarrow\] \[\textsc{code}(\textsc{root}(B)) \neq\textsc{code}(\textsc{root}(B^{\prime}))\] \[\Leftrightarrow\] \[B\text{ and }B^{\prime}\text{ are non-isomorphic}.\]

To prove Claim 2, let us first consider how \(\textsc{code}(\gamma_{C})\) is generated. For any node \(C\) in \(\gamma\), there is a bijection between the codes of the form given in Equation (6) and sets of the following form:

\[S_{C}=\{\!\{\{\textsc{TriCode}(C)\}\}\!\}\cup\{\!\{\!(\theta(C,C^{\prime}), \textsc{code}(\gamma_{C^{\prime}}))\mid C^{\prime}\in\chi(C)\}\!\}\] (7)Observe that the sets of this form are of bounded size (since the number of children is bounded) and each of their elements is from a countable set (given the size of the graph, we can also bound the number of different codes that can be generated). By Lemma A.1 there exists an injective map \(g\) from such sets to the interval \([0,1]\). Since the size of every such set is bounded, and every tuple is from a countable set, we can apply Lemma 5 of Xu et al. [63], and decompose the function \(g\) as:

\[g(S_{C})=\phi\left(\sum_{x\in S}f(x)\right),\]

for an appropriate choice of \(\phi:\mathbb{R}^{d^{\prime}}\rightarrow[0,1]\) and \(f(x)\in\mathbb{R}^{d^{\prime}}\). Based on the structure of the elements of \(S_{C}\), we can further rewrite this as follows:

\[g(S_{C})=\phi\left(f(\textsc{TriCode}(C))+\sum_{C^{\prime}\in\chi(C)}f\left(( \theta(C,C^{\prime}),\textsc{code}(\gamma_{C^{\prime}}))\right)\right).\] (8)

Observe the connection between this function and BiEnc (5): for every \(C^{\prime}\in\chi(C)\), we have \(\widetilde{\boldsymbol{h}}_{\gamma_{C^{\prime}}}\) instead of \(\textsc{code}(\gamma_{C})\), and, moreover, \(\boldsymbol{p}_{\theta(C,C^{\prime})}\) is a positional encoding of \(\theta(C,C^{\prime})\). Then, there exists a function \(\mu\) such that:

\[(\theta(C,C^{\prime}),\textsc{code}(\gamma_{C^{\prime}}))=\mu(\boldsymbol{p}_ {\theta(C,C^{\prime})}\|\widetilde{\boldsymbol{h}}_{\gamma_{C^{\prime}}}),\]

provided that the following condition is met:

\[\widetilde{\boldsymbol{h}}_{\gamma_{C^{\prime}}}=\textsc{code}(\gamma_{C^{ \prime}})\text{ for any }C^{\prime}\in\chi(C).\] (9)

Importantly, the choice for \(\mu\) can be the same for all nodes \(C\). Hence, assuming the condition specified in Equation (9) is met, the function \(g\) can be further decomposed using some function \(f^{\prime}(x)\in\mathbb{R}^{d^{\prime}}\) which satisfies:

\[g(S_{C}) =\phi\left(f(\{\textsc{TriCode}(C)\})+\sum_{C^{\prime}\in\chi(C)} f\left((\theta(C,C^{\prime}),\textsc{code}(\gamma_{C^{\prime}}))\right)\right)\] \[=\phi\left(\widetilde{\boldsymbol{h}}_{C}+\sum_{C^{\prime}\in \chi(C)}f^{\prime}\left(\mu\left(\boldsymbol{p}_{\theta(C,C^{\prime})}\| \widetilde{\boldsymbol{h}}_{\gamma_{C^{\prime}}}\right)\right)\right)\] \[=\phi\left(\widetilde{\boldsymbol{h}}_{C}+\sum_{C^{\prime}\in \chi(C)}(f^{\prime}\circ\mu)\left(\boldsymbol{p}_{\theta(C,C^{\prime})}\| \widetilde{\boldsymbol{h}}_{\gamma_{C^{\prime}}}\right)\right).\]

Observe that this function can be parameterized by BiEnc6 (5): we apply the universal approximation theorem [14, 31, 32], and encode \((f^{\prime}\circ\mu)\) with an MLP (i.e., the inner MLP) and similarly encode \(\phi\) with another MLP (i.e., the outer MLP).

Footnote 6: Note that \(f(\textsc{TriCode}(C))\) can be omitted, because TriEnc has an outer MLP, which can incorporate \(f\).

To conclude the proof of Claim 2 (and thereby the proof of Lemma 6.2), we need to show the existence of bijection \(\rho\) between \(\mathcal{F}\) and \(\mathcal{M}\) such that, for any node \(C\) in \(\gamma\):

\[\rho(\widetilde{\boldsymbol{h}}_{\gamma_{C}})=\textsc{code}(\gamma_{C})\text{ and }\rho^{-1}(\textsc{code}(\gamma_{C}))=\widetilde{\boldsymbol{h}}_{\gamma_{C}}.\]

This can be shown by a straight-forward induction on the structure of the tree \(\gamma\). For the base case, it suffices to observe that \(C\) is a leaf node, which implies \(\widetilde{\boldsymbol{h}}_{\gamma_{C}}=\phi(\widehat{\boldsymbol{h}}_{C})\) and \(\textsc{code}(\gamma_{C})=\{\textsc{TriCode}(C)\}\). The existence of a bijection is then warranted by Claim 1. For the inductive case, assume that there is a bijection between the induced representations of the children of \(C\) and their codes to ensure that the condition given in Equation (9) is met (which holds since the algorithm operates in a bottom up manner). Using the injectivity of \(g\), and the fact that all subtree representations (of children) already admit a bijection, we can easily extend this to a bijection on all nodes \(C\) of \(\gamma\).

We have provided a parameterization of TriEnc and BiEnc and proven that they can compute representations which bijectively map to the codes of the KHC algorithm for the respective components, effectively aligning KHC algorithm with our encoders for these components. Given the completeness of the respective procedures in KHC, we conclude that the encoders are also complete in terms of distinguishing the respective components. 

Having showed that a single layer parameterization of BiEnc and TriEnc is sufficient for distinguishing the biconnected and triconnected components, we proceed with the main lemma.

**Lemma 6.3**.: _For a planar graph \(G=(V,E,\zeta)\) of order \(n\) and its associated Block-Cut tree \(\delta=\textsc{BlockCut}(G)\), there exists a \(L=\lceil\log_{2}(n)\rceil+1\) layer parameterization of BasePlanE that computes a complete graph invariant for each subtree \(\delta_{u}\) induced by each cut node \(u\)._

Proof.: CutEnc recursively computes the representation for induced subtrees \(\delta_{u}\) from cut nodes \(u\), where \(\delta=\textsc{BlockCut}(G)\). Recall that in Block-Cut trees, the children of a cut node always represent a biconnected component, and the children of a biconnected component always represent a cut node. Therefore, it is natural to give the update formula for a cut node \(u\) in terms of the biconnected component \(B\) represented by \(u\)'s children \(\chi(u)\) and \(B\)'s children \(\chi(B)\).

\[\bm{\bar{h}}_{\delta_{u}}^{(\ell)}=\textsc{MLP}\left(\bm{h}_{u}^{(\ell-1)}+ \sum_{B\in\chi(u)}\textsc{MLP}\left(\bm{\tilde{h}}_{B}^{(\ell)}+\sum_{v\in \chi(B)}\bm{\bar{h}}_{\delta_{v}}^{(\ell)}\right)\right).\] (10)

To show the result, we first note that the canonical code given by the KHC algorithm also operates in a bottom up fashion on the subtrees of \(\delta\). We have three cases:

Case 1.For a _leaf_\(B\) in \(\delta\), the code for \(\delta_{B}\) is given by \(\textsc{code}(\delta_{B})=\textsc{BiCode}(B)\). This is because the leafs of \(\delta\) are all biconnected components.

Case 2.For a _non-leaf_ biconnected component \(B\) in \(\delta\), we perform overrides for the codes associated with each child cut node, and then use BiEnc. More precisely, we override the associated \(\textsc{code}(\{\{u\},\{\}\}):=\textsc{code}(\delta_{u})\) for all \(u\in\chi(B)\), and then we compute \(\textsc{code}(\delta_{B})=\textsc{BiCode}(B)\).

Case 3.For a _non-leaf_ cut node \(u\) in \(\delta\), we encode in a similar way to BiEnc: we get the set of codes induced by the children of \(u\) in their lexicographical order. More precisely, if the lexicographical ordering of \(\chi(u)\), based on \(\textsc{code}(\delta_{B})\) for a given \(B\in\chi(u)\) is \(x[1],\ldots,x[|\chi(u)|]\), then the code for \(\delta_{u}\) is given by:

\[\textsc{code}(\delta_{u})=\big{(}\textsc{code}(\delta_{x[1]})),\ldots,\textsc {code}(\delta_{x[|x|]})\big{)}\] (11)

Instead of modelling the overrides (as in Case 2), BasePlanE learns the cut node representations. We first prove this result by giving a parameterization of BasePlanE which uses linearly many layers in \(n\) and then show how this construction can be improved to use logarithmic number of layers. Specifically, we will first show that BasePlanE can be parameterized to satisfy the following properties:

1. For every cut node \(u\), we reserve a dimension in \(\bm{h}_{u}^{L}\) that stores the number of cut nodes in \(\delta_{u}\). This is done in the Update part of the corresponding layer.
2. There is a bijection \(\lambda\) between the representations of the induced subtrees and subtree codes, such that \(\lambda(\bm{\bar{h}}_{\delta_{u}}^{(L)})=\textsc{code}(\delta_{u})\)_and_\(\lambda^{-1}(\textsc{code}(\delta_{u}^{(L)}))=\bm{\bar{h}}_{\delta_{u}}^{(L)}\), for cut nodes \(u\) that have strictly less than \(L\) cut nodes in \(\delta_{u}\).
3. There is a bijection \(\rho\) between the cut node representations and subtree codes, such that \(\rho(\bm{h}_{u}^{(L)})=\textsc{code}(\delta_{u})\)_and_\(\rho^{-1}(\textsc{code}(\delta_{u}^{(L)}))=\bm{h}_{u}^{(L)}\), for cut nodes \(u\) that have strictly less than \(L\) cut nodes in \(\delta_{u}\).

Observe that the property (2) directly gives us a complete graph invariant for each subtree \(\delta_{u}\) induced by each cut node \(u\), since the codes for every induced subtree are complete, and through the bijection, we obtain complete representations. The remaining properties are useful for later in order to obtain a more efficient construction.

The Update function in every layer is crucial for our constructions, and we recall its definition:

\[\bm{h}_{u}^{(\ell)} =f^{(\ell)}\Big{(}g_{1}^{(\ell)}\big{(}\bm{h}_{u}^{(\ell-1)}+\sum_{v \in N_{u}}\bm{h}_{v}^{(\ell-1)}\big{)}\ \|\ g_{2}^{(\ell)}\big{(}\sum_{v\in V}\bm{h}_{v}^{(\ell-1)}\big{)}\ \|\] \[g_{3}^{(\ell)}\big{(}\bm{h}_{u}^{(\ell-1)}+\sum_{C\in\sigma_{ \sigma}^{G}}\widehat{\bm{h}}_{C}^{(\ell)}\big{)}\ \|\ g_{4}^{(\ell)}\big{(}\bm{h}_{u}^{(\ell-1)}+\sum_{B\in\pi_{u}^{G}}\widehat{ \bm{h}}_{B}^{(\ell)}\big{)}\ \|\ \widehat{\bm{h}}_{\delta_{u}}^{(\ell)}\Big{)},\]

We prove that there exists a parameterization of BasePlane which satisfies the properties (1)-(3) by induction on the number of layers \(L\).

**Base case.**\(L=1\). In this case, there are no cut nodes satisfying the constraints, and the model trivially satisfies the properties (2)-(3). To satisfy (1), we can set the inner MLP in CutEnc as the identity function, and the outer MLP as a function which adds \(1\) to the first dimension of the input embedding. This ensures that the representations \(\overline{\bm{h}}_{\delta_{u}}^{(1)}\) of cut nodes \(u\) have their first components equal to the number of cut nodes in \(\delta_{u}\). We can encode the property (1) in \(\bm{h}_{u}^{(\ell)}\) using the representation \(\overline{\bm{h}}_{\delta_{u}}^{(1)}\), since the latter is a readout component in Update.

**Inductive step.**\(L>1\). By induction hypothesis, there is an \((L-1)\)-layer parameterization of BasePlane which satisfies the properties (1)-(3). We can define the \(L\)-th layer so that our \(L\) layer parameterization of BasePlane satisfies all the properties:

_Property (1):_ For every cut node \(u\), the function Update has a readout from \(\bm{h}_{u}^{(L-1)}\) and \(\overline{\bm{h}}_{\delta_{u}}^{(L)}\), which allows us to copy the first dimension of \(\bm{h}_{u}^{(L-1)}\) into the first dimension of \(\bm{h}_{u}^{(L)}\) using a linear transformation, which immediately gives us the property.

_Property (2):_ For this property, let us first consider the easier direction. Given the code of \(\delta_{u}\), we want to find the CutEnc representation for the induced subtree of \(u\). From the subtree code, we can reconstruct the induced subtree \(\delta_{u}\), and then run a \(L\)-layer BasePlanE on reconstructed Block-Cut Tree to find the CutEnc representation. As for the other direction, we want to find the subtree code of given the representation of the induced subgraph. The CutEnc encodes a multiset \(\{\!(\widetilde{\bm{h}}_{B},\{\!\!\{\bm{h}_{v}|v\in\chi(B)\}\!\})\ |\ B\in\chi(u)\}\!\}\). By induction hypothesis, we know that all grandchildren \(v\in\chi^{2}(u)\) already have properties (1)-(3) satisfied for them with the first \((L-1)\) layers. Hence, using the parameterization of TriEnc and BiEnc given in Lemma 6.2, as part of the \(L\)-th BasePlanE layer, we can get a bijection between BiCode and biconnected component representation. This way we can obtain BiCode\((B)\) for all the children biconnected components \(B\in\chi(u)\), with all the necessary overriding. Having all necessarily overrides is crucial, because to get the KHC code for the cut node \(u\), we need to concatenate the biconnected codes from 6 that already have the required overrides. Hence, we make the parameterization of CutEnc encode multisets of representations for biconnected components \(B\in\chi(u)\), and by similar arguments as in the proof of Lemma 6.2, this can be done using the MLPs in CutEnc.

_Property (3):_ Using the bijection from (2) as a bridge, we can easily show the property (3). In the update formula, we appended \(\overline{\bm{h}}_{\delta_{u}}^{(L)}\) using the MLP. If the MLP is bijective with the dimension taken by \(\overline{\bm{h}}_{\delta_{u}}^{(L)}\), we get a bijection between the node representation and the subtree representation. By transitivity, we get a bijection between node representations and subtree codes.

This concludes our construction using \(L=O(n)\) BasePlane layers.

Efficient construction.We now show that the presented construction can be made more efficient, using only \(\lceil\log_{2}(n)\rceil+1\) layers. This is achieved by treating the cut nodes \(u\) differently based on the number of cut nodes they include in their induced subtrees \(\delta_{u}\). In this construction, the property (1) remains the same, but the properties (2)-(3) are modified:

1. For every cut node \(u\), we reserve a component in \(\bm{h}_{u}^{(L)}\) that stored to the number of cut nodes in \(\delta_{u}\). This is done in the Update part of the corresponding layer.
2. There is a bijection \(\lambda\) between the representations of the induced subtrees and subtree codes, such that \(\lambda(\overline{\bm{h}}_{\delta_{u}}^{(L)})=\textsc{code}(\delta_{u})\)_and_\(\lambda^{-1}(\textsc{code}(\delta_{u}^{(L)}))=\overline{\bm{h}}_{\delta_{u}}^{(L)}\), for cut nodes \(u\) that have strictly less than \(2^{(L-1)}\) cut nodes in \(\delta_{u}\).

3. There is a bijection \(\rho\) between the cut node representations and subtree codes, such that \(\rho(\bm{h}_{u}^{(L)})=\textsc{code}(\delta_{u})\)_and_\(\rho^{-1}(\textsc{code}(\delta_{u}^{(L)}))=\bm{h}_{u}^{(L)}\), for cut nodes \(u\) that have strictly less than \(2^{(L-1)}\) cut nodes in \(\delta_{u}\).

These new modified properties allow us to reduce the depth of the induction to a logarithmic number of steps, by having a more carefully designed parameterization of CutEnc. The logarithmic depth comes immediately from properties (2) and (3), while the core observations for building the CutEnc parameterizations are motivated by standard literature on efficient tree data structures, and in particular by the Heavy Light Decomposition (HLD) [55]. In HLD, we build "chains" through the tree that always go to the child that has the most nodes in its subtree. This gives us the property that, the path between each two nodes visits at most a logarithmic number of different chains, and we will use this concept in our parametrization. More precisely, we can construct a single CutEnc layer, that will make the above properties hold for whole chains, and not only individual nodes as in the previous construction.

We will once again use induction to show that such parametrizations exist.

**Base case.**\(L=1\). The properties are equivalent to the ones in the less efficient construction: property (1) is the same, and \(2^{(L-1)}=2^{0}=L\), so we will satisfy them with the same parameterization.

**Inductive step.**\(L>1\). By induction hypothesis, there is an \((L-1)\)-layer parameterization of BasePlanE which satisfies the properties (1)-(3). We can define the \(L\)-th layer so that our \(L\) layer parameterization of BasePlanE satisfies all the properties:

_Property (1):_ This is satisfied in the same way as in the less efficient construction - we allocate one of the dimensions for the required count, and propagate it from the previous layer.

_Property (2):_ Let us consider an arbitrary cut node \(u\) that has strictly less than \(2^{(L-1)}\) cut nodes in its induced subtree \(\delta_{u}\), and still does _not_ satisfy property (2). We will call such nodes "improper" with respect to the current induction step, and show that there is a single layer CutEnc parameterization that makes all improper nodes satisfy these two properties, or become "proper".

First, observe that an arbitrary cut node \(u=v_{0}\) has at most one improper grandchild \(v_{1}\in\chi^{2}(u)\), because if there were more improper grandchildren then it would have had at least \(2^{(L-1)}\) cut nodes in it. Repeating a similar argument, we know that there is at most one improper \(v_{2}\in\chi^{2}(v_{1})\). Continuing this until there are no improper grandchildren, we form a sequence of improper cut nodes \(u=v_{0},v_{1},\ldots,v_{k}\), such that \(v_{i+1}\) is a grandchild of \(v_{i}\) for all \(0\leq i<k\). The chain \(u=v_{0},v_{1},\ldots,v_{k}\) is precisely a chain in HLD, and we will show that a single CutEnc layer can make an "improper chain" satisfy the properties. We prove this by applying inner induction on the chain:

**Induction hypothesis (for grandchildren):** By the induction hypothesis, we have an established \((L-1)\)-layer parameterization of BasePlanE that satisfies properties (1)-(3) for all but a single grandchild of _each_ node in the chain \(v_{0},...,v_{k}\). This assumption is critical as it provides the ground truth that our subsequent steps will build upon.

**Inner induction:** Having established the hypothesis for our node's children, we initiate an inner induction, starting from the farthest improper node in the chain, \(v_{k}\), and progressively moving towards the root of the chain, \(u=v_{0}\). This process is akin to traversing back up the chain, ensuring each node, starting from \(v_{k}\), satisfies property (2) as we ascend.

For each node \(v_{i}\) in our reverse traversal, we apply the special property: there exists at most one improper grandchild (potentially \(v_{i+1}\)) within its local structure. Our CutEnc architecture then identifies this improper grandchild, which, crucially, is "proper" in its representation due to our inner induction process. This _identification_ is facilitated by the inner MLP given in Equation (10), and property (1), as identifying is equivalent to selecting the grandchild with the largest number of cut nodes in the allocated dimension.

Having identified and isolated our improper grandchild \(v_{i+1}\) (now proper in \(\overline{\bm{h}}_{\delta_{v_{i+1}}}^{(L)}\)), CutEnc then integrates this node with the other children and grandchildren of \(v_{i}\). It is important to note that all these other nodes have representations bijective to their respective subtree _codes_ because of the induction hypothesis. This integration is not a mere collection; instead, the outer MLP in Equation (10) models the "code operation" that BiCode would have done, which is possible due to the mentioned bijections to codes. This concludes the inner induction.

Our choice for \(u\) was arbitrary and the required CutEnc parameterization is independent from the exact node representaton - it simply models a translation back to codes, and then injecting back to embeddings. Hence, all chains of improper nodes will satisfy property (2) after a single CutEnc layer.

_Property (3):_ We once again follow the same approach as in the less efficient construction - we use property (2) as a bridge, as we know we have already satisfied it. Property (3) is needed to ensure that all nodes that once became "proper" will always stay so, by propagating the bijection to codes.

This concludes our construction using \(\lceil\log_{2}(n)\rceil+1\) layers. 

**Theorem 6.1**.: _For any planar graphs \(G_{1}=(V_{1},E_{1},\zeta_{1})\) and \(G_{2}=(V_{2},E_{2},\zeta_{2})\), there exists a parameterization of BasePlanE with at most \(L=\lceil\log_{2}(\max\{|V_{1}|,|V_{2}|\})\rceil+1\) layers, which computes a complete graph invariant, that is, the final graph-level embeddings satisfy \(\bm{z}_{G_{1}}^{(L)}\neq\bm{z}_{G_{2}}^{(L)}\) if and only if \(G_{1}\) and \(G_{2}\) are not isomorphic._

Proof.: The "only if" direction is immediate because BasePlanE is an invariant model for planar graphs. To prove the "if" direction, we do a case analysis on the root of the two Block-Cut Trees. For each case, we provide a parameterization of BasePlanE such that \(\bm{z}_{G_{1}}^{(L)}\neq\bm{z}_{G_{2}}^{(L)}\) for any two non-isomorphic graphs \(G_{1}\) and \(G_{2}\). A complete BasePlanE model can be obtained by appropriately unifying the respective parameterizations.

Case 1.root\((\delta_{1})\) and root\((\delta_{2})\) represents two cut nodes.

Consider a parameterization of the final BasePlanE update formula, where only cut node representation is used, and a simplified readout that only aggregates from the last layer. We can rewrite the readout for a graph in terms of the cut node representation from the last BasePlanE layer:

\[\bm{z}_{G}^{(L)}=\text{MLP}\left(\sum_{u\in V^{G}}\text{MLP}(\overline{\bm{h} }_{\delta_{u}}^{(L)})\right).\]

Let \(\text{Cut}(G)=\{\overline{\bm{h}}_{\delta_{u}}^{(L)}|\;u\in V^{G}\}\). Intuitively, \(\text{Cut}(G)\) is a multiset of cut node representations from the last BasePlanE layer. We assume \(|V_{\delta_{1}}|\leq|V_{\delta_{2}}|\) without loss of generality. Consider the root node root\((\delta_{2})\) of the Block-Cut Tree \(\delta_{2}\). By Lemma 6.3, we have \(\bm{h}_{\text{ROOT}(\delta_{2})}\) as a complete graph invariant with \(L\) layers. Since \(\delta_{2}\) cannot appear as a subtree of \(\delta_{1}\), \(\bm{h}_{\text{ROOT}(\delta_{2})}\notin\text{Cut}(G_{1})\). Hence, \(\text{Cut}(G_{1})\neq\text{Cut}(G_{2})\). Since this model can define an injective mapping on the multiset Cut\((G)\) using similar arguments as before, we get that \(\bm{z}_{G_{1}}^{(L)}\neq\bm{z}_{G_{2}}^{(L)}\).

Case 2.root\((\delta_{1})\) and root\((\delta_{2})\) represents two biconnected components.

We use a similar strategy to prove Case 2. First, we consider a simplified BasePlanE model, where the update formula considers biconnected components only and the final readout aggregates from the last BasePlanE layer. We similarly give the final graph readout in terms of the biconnected component representation from the last BasePlanE layer.

\[\bm{z}_{G}=\text{MLP}\left(\sum_{u\in V^{G}}\text{MLP}((\bm{h}_{u}^{(L-1)}+ \sum_{B\in\pi_{u}^{G}}\widetilde{\bm{h}}_{B}^{(L)}))\right).\]

Let \(\text{Bc}(G)=\{(\bm{h}_{u}^{(L-1)},\{\overline{\bm{h}}_{B}^{(L)}\mid B\in\pi_ {u}^{G}\})|\;u\in V^{G}\}\). In Lemma 6.3, we prove that \(\widetilde{\bm{h}}_{B}^{(L)}\) is also a complete invariant for the subtree rooted at \(B\) in the Block-Cut Tree. First, we show \(\text{Bc}(G_{1})\neq\text{Bc}(G_{2})\). As before, we assume \(|V_{\delta_{1}}|\leq|V_{\delta_{2}}|\) without loss of generality. Consider how the biconnected component representation \(\bm{h}_{\textsc{ROOT}(\delta_{2})}\) appears in the two multisets of pairs. For \(G_{2}\), there exist at least one node \(u\) and pair:

\[(\bm{h}_{u}^{(L-1)},\{\bm{\widetilde{h}}_{B}^{(L)}\mid B\in\pi_{u}^{G}\bm{\}}) \in\textsc{Bc}(G_{2}),\]

such that \(\bm{h}_{\textsc{ROOT}(\delta_{2})}\in\{\bm{\widetilde{h}}_{B}^{(L)}\mid B\in \pi_{u}^{G}\bm{\}}\). However, because \(\bm{h}_{\textsc{ROOT}(\delta_{2})}\) is a complete invariant for \(\delta_{2}\) and \(\delta_{2}\) cannot appear as a subtree in \(\delta_{1}\), no such pair exists in \(\textsc{Bc}(G_{1})\). Given \(\textsc{Bc}(G_{1})\neq\textsc{Bc}(G_{2})\), we can parameterize the MLPs to define an injective mapping to get that \(\bm{z}_{G_{1}}^{(L)}\neq\bm{z}_{G_{2}}^{(L)}\).

Case 3.root\((\delta_{1})\) represents a cut node and root\((\delta_{2})\) represents a biconnected component.

We can distinguish \(G_{1}\) and \(G_{2}\) using a simple property of root\((\delta_{1})\). Recall that \(\pi_{u}^{G}\) represents the set of biconnected components that contains \(u\), and \(\chi^{\delta}(C)\) represents the C's children in the Block-Cut Tree \(\delta\). For root\((\delta_{1})\), we have \(|\pi_{u}^{G_{1}}|=|\chi^{\delta_{1}}(u)|\). However, for any other cut node \(u\), including the non-root cut node in \(\delta_{1}\) and all cut nodes in \(\delta_{2}\), we have \(|\pi_{u}^{G}|=|\chi^{\delta}(u)|+1\), because there must be a parent node \(v\) of \(u\) such that \(v\in\pi_{u}^{G_{1}}\) but \(v\notin\chi^{\delta}(u)\).

Therefore, we consider a parameterization of a one-layer BasePlanE model that exploits this property. In BiEnc, we have a constant vector \([1,0]^{\top}\) for all biconnected components. In CutEnc, we learn \([0,|\chi_{u}|]^{\top}\) for all cut nodes \(u\). In the update formula, we have \([|\pi_{u}^{G}|-|\chi^{\delta}(u)|-1,0]^{\top}\). All of the above specifications can be achieved using linear maps. For the final readout, we simply sum all node representations with no extra transformation.

Then, for root\((\delta_{1})\), we have \(\bm{h}_{\textsc{ROOT}(\delta_{u})}=[-1,0]^{\top}\). For any other cut node \(u\), we have \(\bm{h}_{u}=[0,0]^{\top}\). For all non-cut nodes \(u\), we also have \(\bm{h}_{u}=[0,0]^{\top}\) because \(|\pi_{u}^{G}|=1\) and \(\bm{\overline{h}}_{\delta_{u}}=[0,0]^{\top}\). Summing all the node representations yields \(\bm{z}_{G_{1}}=[-1,0]^{\top}\) but \(\bm{z}_{G_{2}}=[0,0]^{\top}\). Hence, we obtain \(\bm{z}_{G_{1}}^{(L)}\neq\bm{z}_{G_{2}}^{(L)}\), as required. 

## Appendix B Runtime analysis of BasePlanE

### Asymptotic analysis

In this section, we study the runtime complexity of the BasePlanE model.

**Computing components of a planar graph.** Given an input graph \(G=(V,E,\zeta)\), BasePlanE first computes the SPQR components/SPQR tree, and identifies cut nodes. For this step, we follow a simple \(O(|V|^{2})\) procedure, analogously to the computation in the KHC algorithm. Note that this computation only needs to run once, as a pre-processing step. Therefore, this pre-computation does not ultimately affect runtime for model predictions.

**Size and number of computed components.**

1. _Cut nodes:_ The number of cut nodes in \(G\) is at most \(|V|\), and this corresponds to the worst-case when \(G\) is a tree.
2. _Biconnected Components:_ The number of biconnected components \(|\pi^{G}|\) is at most \(|V|-1\), also obtained when \(G\) is a tree. This setting also yields a worst-case bound of \(2|V|-2\) on the total number of nodes across all individual biconnected components.
3. _SQPR components:_ As proved by Gutwenger and Mutzel [27], given a biconnected component \(B\), the number of corresponding SPQR components, as well as their size, is bounded by the number of nodes in \(B\). The input graph may have multiple biconnected components, whose total size is bounded by \(2|V|-2\) as described earlier. Thus, we can apply the earlier result from Gutwenger and Mutzel [27] to obtain an analogous bound of \(2|V|-2\) on the total number of SPQR components.
4. _SPQR trees_: As each SPQR component corresponds to exactly one node in a SPQR tree. The total size of all SPQR trees is upper-bounded by \(2|V|-2\).

**Complexity of TriEnc.** TriEnc computes a representation for each SPQR component edge. This number of edges, which we denote by \(e_{C}\), is in fact linear in \(V\): Indeed, following Euler's theorem for planar graphs (\(|E|\leq 3|V|-6\)), the number of edges per SPQR component is linear in its size.

Moreover, since the total number of nodes across all SPQR components is upper-bounded by \(2|V|-2\), the total number of edges is in \(O(V)\). Therefore, as each edge representation can be computed in a constant time with an MLP, TriEnc computes all edge representations across all SPQR components in \(O(e_{C}\cdot d)=O(|V|d^{2})\) time, where \(d\) denotes the embedding dimension of the model.

Using the earlier edge representations, TriEnc performs an aggregation into a triconnected component representation by summing the relevant edge representations, and this is done in \(O(e_{C}d)\). Then, the sum outputs across all SPQR components are transformed using an MLP, in \(O(|\sigma^{G}|d^{2})\). Therefore, the overall complexity of TriEnc is \(O((e_{c}+|\sigma^{G}|)d^{2})=O(|V|d^{2})\).

**Complexity of BiEnc.** BiEnc recursively encodes nodes in the SPQR tree. Each SPQR node is aggregated once by its parent and an MLP is applied to each node representation once. The total complexity of this call is therefore \(O(|\sigma^{G}|d^{2})=O(|V|d^{2})\).

**Complexity of CutEnc.** A similar complexity of \(O(|V|d^{2})\) applies for CutEnc, as CutEnc follows a similar computational pipeline.

**Complexity of node update.** As in standard MLPs, message aggregation runs in \(O(|E|d)\), and the combine function runs in \(O(|V|d^{2})\). Global readouts can be computed in \(O(|V|d)\). Aggregating from bi-connected components also involves at most \(O(|V|d)\) messages, as the number of messages corresponds to the total number of bi-connected component nodes, which itself does not exceed \(2|V|-2\). The same argument applies to messages from SPQR components: nodes within these components will message their original graph analogs, leading to the same bound. Finally, the cut node messages are linear, i.e., \(O(|V|)\), as each node receives exactly one message (no aggregation, no transformation). Overall, this leads to the step computation having a complexity of \(O(|V|d^{2})\).

**Overall complexity.** Each BasePlanE layer runs in \(O(|V|d^{2})\) time, as this is the asymptotic bound of each of its individual steps. The \(d^{2}\) term primarily stems from MLP computations, and is not a main hurdle to scalability, as the used embedding dimensionality in our experiments is usually small.

**Parallelization.** Parallelization can be used to speed up the pre-processing of large planar graphs. In particular, parallelization can reduce the runtime of KHC pre-processing, and more precisely the computation of the canonical walk in Weinberg's algorithm, which itself is the individual step with the highest complexity (\(O(|V|^{2})\)) in our approach. To this end, one can independently run Weinberg's algorithm across several triconnected components in parallel. Moreover, we can also parallelize the algorithm's operation within a single triconnected component: as the quadratic overhead comes from computing the lexicographically smallest TriCode, where the computation depends on the choice of the first walk edge, we can concurrently evaluate each first edge option and subsequently aggregate over all outputs to more efficiently find the smallest code.

### Empirical runtime evaluation

**Experimental setup.** To validate the runtime efficiency of BasePlanE, we conduct a runtime experiment on real-world planar graphs based on the geographic faces of Alaska from the dataset TIGER, provided by the U.S. Census Bureau. The original dataset is TIGER-Alaska-93K and has 93366 nodes. We also extract the smaller datasets TIGER-Alaska-2K and TIGER-Alaska-10K, which are subsets of the original dataset with 2000 and 10000 nodes, respectively. We compare the wallclock time of BasePlanE and PPGN which is as expressive as 2-WL.

**Results.** The runtimes reported in Table 7 confirm our expectation: BasePlanE is the only model which can scale up to all datasets. Indeed, both ESAN and PPGN fail on TIGER-Alaska-10K and TIGER-Alaska-93K. Somewhat surprisingly, ESAN training time is slower than PPGN training time

\begin{table}
\begin{tabular}{c c c c c c c c} \hline \hline  & & \multicolumn{2}{c}{BasePlanE} & \multicolumn{2}{c}{PPGN} & \multicolumn{2}{c}{ESAN} \\ \cline{3-8} Dataset & \#Nodes & Pre. & Train & Pre. & Train & Pre. & Train \\ \hline TIGER-Alaska-2K & 2000 & 9.8 sec & 0.1 sec & 3.4 sec & 5.9 sec & 4.6 sec & 87.73 sec \\ TIGER-Alaska-10K & 10000 & 50 sec & 0.33 sec & OOM & OOM & OOM & OOM \\ TIGER-Alaska-93K & 93366 & 3.7 hs & 2.2 sec & OOM & OOM & OOM & OOM \\ \hline \hline \end{tabular}
\end{table}
Table 7: Runtime experiments on planar graphs TIGER-Alaska-2K, TIGER-Alaska-10K, and TIGER-Alaska-93K. We report the pre-processing and training time (per epoch) for all models.

on TIGER-Alaska-2K which is likely due to the fact that the maximal degree of TIGER-Alaska-2K is 620, which the ESAN algorithm depends on. BasePlanE is very fast for training with the main bottleneck being _one-off_ pre-processing. This highlights that BasePlanE is a highly efficient option for inference on large-scale graphs compared to subgraph or higher-order alternatives.

## Appendix C An ablation study on ZINC

We additionally conduct extensive ablation studies on each component in our model update equation using the ZINC 12k dataset. We report the final results in Table 8. The setup and the main findings can be summarized as follows:

* BasePlanE (no readout): We removed the global readout term from the update formula and MAE worsened by \(0.003\).
* BasePlanE (no CutEnc): We removed the cut node term from the update formula and MAE worsened by \(0.003\).
* BasePlanE (no neighbors): We additionally removed the neighbor aggregation from the update formula and MAE worsened by \(0.023\).
* BasePlanE (only BiEnc): We only used the triconnected components for the update formula and MAE worsened by \(0.021\).
* BasePlanE (only TriEnc): We only used the triconnected components for the update formula and MAE worsened by \(0.016\).

Hence, BasePlanE significantly benefits from each of its components: the combination of standard message passing with component decompositions is essential to obtaining our reported results.

## Appendix D Further experimental details

### Link to code

The code for our experiments, as well as instructions to reproduce our results and set up dependencies, can be found at this GitHub repository: https://github.com/ZZYSonny/PlanE

### Computational resources

We run all experiments on 4 cores from Intel Xeon Platinum 8268 CPU @ 2.90GHz with 32GB RAM. In Table 9, we report the approximate time to train a BasePlanE model on each dataset and with each tuned hidden dimension value.

### E-BasePlanE architecture

E-BasePlanE builds on BasePlanE, and additionally processes edge features within the input graph. To this end, it supersedes the original BasePlanE update equations for SPQR components

\begin{table}
\begin{tabular}{l l} \hline \hline Models & ZINC MAE \\ \hline BasePlanE (original) & **0.076\(\pm\)** 0.003 \\ BasePlanE (no readout) & \(0.079\pm\) 0.002 \\ BasePlanE (no CutEnc) & \(0.079\pm\) 0.003 \\ BasePlanE (no neighbours) & 0.099\(\pm\) 0.004 \\ BasePlanE (only BiEnc) & 0.097\(\pm\) 0.002 \\ BasePlanE (only TriEnc) & 0.092\(\pm\) 0.003 \\ \hline \hline \end{tabular}
\end{table}
Table 8: BasePlanE ablation study on ZINC. Each component is essential to achieve the reported performance. The use of bi- and triconnected components yields substantial improvements. Aggregating over direct neighbours in the update equation remains important.

and nodes with the following analogs:

\[\widehat{\bm{h}}_{C}^{(\ell)} =\text{MLP}\Big{(}\sum_{i=1}^{|\omega|}\text{MLP}(\bm{h}_{u[i]}^{( \ell-1)}||\hat{\bm{h}}_{\omega[i],\omega[(i+1)\%|w|]}^{(\ell-1)}\|\bm{p}_{\kappa[ i]}\|\bm{p}_{i}))\Big{)},\text{ and}\] \[\bm{h}_{u}^{(\ell)} =f^{(\ell)}\Big{(}g_{1}^{(\ell)}\big{(}\bm{h}_{u}^{(\ell-1)}+\sum _{v\in N_{u}}g_{5}^{(\ell)}(\bm{h}_{v}^{(\ell-1)}\|\bm{h}_{v,u}^{(\ell-1)}) \big{)}\parallel g_{2}^{(\ell)}\big{(}\sum_{v\in V}\bm{h}_{v}^{(\ell-1)}\big{)}\] \[g_{3}^{(\ell)}\big{(}\bm{h}_{u}^{(\ell-1)}+\sum_{B\in\pi_{u}^{G}} \widetilde{\bm{h}}_{B}^{(\ell)}\big{)}\parallel g_{4}^{(\ell)}\big{(}\bm{h}_{ u}^{(\ell-1)}+\sum_{C\in\sigma_{u}^{G}}\widehat{\bm{h}}_{C}^{(\ell)}\big{)} \parallel\bar{\bm{h}}_{\delta_{u}}^{(\ell)}\Big{)},\]

where \(\hat{\bm{h}}_{i,j}=\bm{h}_{i,j}\) if \((i,j)\in E\) and is the ones vector (\(\bm{1}^{d}\)) otherwise

### Training setups

**Graph classification on EXP** We follow the same protocol as the original work: we use 10-fold cross validation on the dataset, train BasePlanE on each fold for 50 epochs using the Adam [39] optimizer with a learning rate of \(10^{-3}\), and binary cross entropy loss.

**Graph classification on P3R.** We follow a very similar protocol to the one in EXP: we use 10-fold cross validation, where we train BasePlanE for 100 epochs using the Adam [39] optimizer with a learning rate of \(10^{-3}\), and cross entropy loss.

**Clustering coefficient of QM9 graphs.** To train all baselines, we use the Adam optimizer with a learning rate from \(\{10^{-3};10^{-4}\}\), and train all models for 100 epochs using a batch size of 256 and L2 loss. We report the overall label distribution of normalized clustering coefficients on QM9 graphs in Figure 5.

**Graph classification on MolHIV.** We instantiate E-BasePlanE with an embedding dimension of 64 and a positional encoding dimensionality of 16. We further tune the number of layers within the

Figure 5: Normalized clustering coefficient distribution of QM9\({}_{\text{CC}}\).

\begin{table}
\begin{tabular}{c c c} \hline \hline Dataset Name & Hidden Dimension & Training Time (hours) \\ \hline QM9\({}_{\text{CC}}\) & 32 & 2.5 \\ MolHIV & 64 & 6 \\ QM9 & 128 & 25 \\ ZINC(12k) & 64 & 5 \\  & 128 & 8 \\ ZINC(Full) & 128 & 45 \\ EXP & 32 & 0.2 \\ \hline \hline \end{tabular}
\end{table}
Table 9: Approximate Training Time for BasePlanE.

set \(\{2,3\}\) and use a dropout probability from the set \(\{0,0.25,0.5,0.66\}\). Furthermore, we train our models with the Adam optimizer [39], with a constant learning rate of \(10^{-3}\). Finally, we perform training with a batch size of 256 and train for 300 epochs.

**Graph regression on QM9.** As standard, we train E-BasePlanE using mean squared error (MSE) and report mean absolute error (MAE) on the test set. For training E-BasePlanE, we tune the learning rate from the set \(\{10^{-3},5\times 10^{-4}\}\) with the Adam optimizer, and adopt a learning rate decay of 0.7 every 25 epochs. Furthermore, we use a batch size of 256, 128-dimensional node embeddings, and 32-dimensional positional encoding.

**Graph regression on ZINC.** In all experiments, we use a node embedding dimensionality from the set \(\{64,128\}\), use 3 message passing layers, and 16-dimensional positional encoding vectors. We train both BasePlanE and E-BasePlanE with the Adam optimizer [39] using a learning rate from the set \(\{10^{-3},5\times 10^{-4},10^{-4}\}\), and follow a decay strategy where the learning rate decays by a factor of 2 for every 25 epochs where validation loss does not improve. We train using a batch size of 256 in all experiments, and run training using L1 loss for 500 epochs on the ZINC subset, and for 200 epochs on the full ZINC dataset.