# Stochastic Approximation Approaches to

Group Distributionally Robust Optimization

Lijun Zhang\({}^{1,2}\), Peng Zhao\({}^{1}\), Zhen-Hua Zhuang\({}^{1}\), Tianbao Yang\({}^{3}\), Zhi-Hua Zhou\({}^{1}\)

\({}^{1}\)National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China

\({}^{2}\)Peng Cheng Laboratory, Shenzhen 518055, China

\({}^{3}\)Department of Computer Science and Engineering, Texas A&M University, College Station, USA {zhanglj, zhaop, zhuangzh, zhouzh}@lamda.nju.edu.cn, tianbao-yang@tamu.edu

###### Abstract

This paper investigates group distributionally robust optimization (GDRO), with the purpose to learn a model that performs well over \(m\) different distributions. First, we formulate GDRO as a stochastic convex-concave saddle-point problem, and demonstrate that stochastic mirror descent (SMD), using \(m\) samples in each iteration, achieves an \(O(m(\log m)/\epsilon^{2})\) sample complexity for finding an \(\epsilon\)-optimal solution, which matches the \(\Omega(m/\epsilon^{2})\) lower bound up to a logarithmic factor. Then, we make use of techniques from online learning to reduce the number of samples required in each round from \(m\) to \(1\), keeping the same sample complexity. Specifically, we cast GDRO as a two-players game where one player simply performs SMD and the other executes an online algorithm for non-oblivious multi-armed bandits. Next, we consider a more practical scenario where the number of samples that can be drawn from each distribution is different, and propose a novel formulation of weighted GDRO, which allows us to derive _distribution-dependent_ convergence rates. Denote by \(n_{i}\) the sample budget for the \(i\)-th distribution, and assume \(n_{1}\geq n_{2}\geq\cdots\geq n_{m}\). In the first approach, we incorporate non-uniform sampling into SMD such that the sample budget is satisfied in expectation, and prove that the excess risk of the \(i\)-th distribution decreases at an \(O(\sqrt{n_{1}\log m}/n_{i})\) rate. In the second approach, we use mini-batches to meet the budget exactly and also reduce the variance in stochastic gradients, and then leverage stochastic mirror-prox algorithm, which can exploit small variances, to optimize a carefully designed weighted GDRO problem. Under appropriate conditions, it attains an \(O((\log m)/\sqrt{n_{i}})\) convergence rate, which almost matches the optimal \(O(\sqrt{1/n_{i}})\) rate of only learning from the \(i\)-th distribution with \(n_{i}\) samples.

## 1 Introduction

In the classical statistical machine learning, our goal is to minimize the risk with respect to a _fixed_ distribution \(\mathcal{P}_{0}\)(Vapnik, 2000), i.e.,

\[\min_{\mathbf{w}\in\mathcal{W}}\ \left\{R_{0}(\mathbf{w})=\mathrm{E}_{ \mathbf{z}\sim\mathcal{P}_{0}}\big{[}\ell(\mathbf{w};\mathbf{z})\big{]} \right\},\] (1)

where \(\mathbf{z}\in\mathcal{Z}\) is a sample drawn from \(\mathcal{P}_{0}\), \(\mathcal{W}\) denotes a hypothesis class, and \(\ell(\mathbf{w};\mathbf{z})\) is a loss measuring the prediction error of model \(\mathbf{w}\) on \(\mathbf{z}\). During the past decades, various algorithms have been developed to optimize (1), and can be grouped in two categories: sample average approximation (SA) and stochastic approximation (SA) (Kushner and Yin, 2003). In SAA, we minimize an empirical risk defined as the average loss over a set of samples drawn from \(\mathcal{P}_{0}\), and in SA, we directly solve the original problem by using stochastic observations of the objective \(R_{0}(\cdot)\).

However, a model that trained over a single distribution may lack robustness in the sense that (i) it could suffer high error on minority subpopulations, though the average loss is small; (ii) its performance could degenerate dramatically when tested on a different distribution. Distributionally robust optimization (DRO) provides a principled way to address those limitations by minimizing the worst-case risk in a neighborhood of \(\mathcal{P}_{0}\)(Ben-Tal et al., 2013). Recently, it has attracted great interest in optimization (Shapiro, 2017), statistics (Duchi and Namkoong, 2021), operations research (Duchi et al., 2021), and machine learning (Hu et al., 2018; Jin et al., 2021; Agarwal and Zhang, 2022). In this paper, we consider an emerging class of DRO problems, named as Group DRO (GDRO) which optimizes the maximal risk over a finite number of distributions (Oren et al., 2019; Sagawa et al., 2020). Mathematically, GDRO can be formulated as a minimax stochastic problem:

\[\min_{\mathbf{w}\in\mathcal{W}}\max_{i\in[m]}\ \big{\{}R_{i}(\mathbf{w})= \mathrm{E}_{\mathbf{z}\sim\mathcal{P}_{i}}\big{[}\ell(\mathbf{w};\mathbf{z}) \big{]}\big{\}}\] (2)

where \(\mathcal{P}_{1},\ldots,\mathcal{P}_{m}\) denote \(m\) distributions. A motivating example is federated learning, where a centralized model is deployed at multiple clients, each of which faces a (possibly) different data distribution (Mohri et al., 2019).

Supposing that samples can be drawn from all distributions freely, we develop efficient SA approaches for (2), in favor of their light computations over SAA methods. As elaborated by Nemirovski et al. (2009, SS3.2), we can cast (2) as a stochastic convex-concave saddle-point problem:

\[\min_{\mathbf{w}\in\mathcal{W}}\max_{\mathbf{q}\in\Delta_{m}}\ \Bigg{\{}\phi( \mathbf{w},\mathbf{q})=\sum_{i=1}^{m}q_{i}R_{i}(\mathbf{w})\Bigg{\}}\] (3)

where \(\Delta_{m}=\{\mathbf{q}\in\mathbb{R}^{m}:\mathbf{q}\geq 0,\sum_{i=1}^{m}q_{i}=1\}\) is the \((m-1)\)-dimensional simplex, and then solve (3) by their mirror descent stochastic approximation method, namely stochastic mirror descent (SMD). In fact, several recent studies have adopted this (or similar) strategy to optimize (3). But, unfortunately, we found that existing results are unsatisfactory because they either deliver a loose sample complexity (Sagawa et al., 2020), suffer subtle dependency issues in their analysis (Haghtalab et al., 2022; Soma et al., 2022), or hold only in expectation (Carmon and Hausler, 2022).

As a starting point, we first provide a routine application of SMD to (3), and discuss the theoretical guarantees. In each iteration, we draw \(1\) sample from every distribution to construct unbiased estimators of \(R_{i}(\cdot)\) and its gradient, and then update both \(\mathbf{w}\) and \(\mathbf{q}\) by SMD. The proposed method achieves an \(O(\sqrt{(\log m)/T})\) convergence rate in expectation and with high probability, where \(T\) is the number of iterations. As a result, we obtain an \(O(m(\log m)/\epsilon^{2})\) sample complexity for finding an \(\epsilon\)-optimal solution of (3), which matches the \(\Omega(m/\epsilon^{2})\) lower bound (Soma et al., 2022, Theorem 5) up to a logarithmic factor, and tighter than the \(O(m^{2}(\log m)/\epsilon^{2})\) bound of Sagawa et al. (2020) by an \(m\) factor. While being straightforward, this result seems _new_ for GDRO.

Then, we proceed to reduce the number of samples used in each iteration from \(m\) to \(1\). We remark that a naive uniform sampling over \(m\) distributions does not work well, and yields a worse sample complexity (Sagawa et al., 2020). As an alternative, we borrow techniques from online learning with stochastic observations, and explicitly tackle the _non-oblivious_ nature of the online process, which distinguishes our method from that of Soma et al. (2022). Specifically, we use SMD to update \(\mathbf{w}\), and Exp3-IX, an algorithm for non-oblivious multi-armed bandits (MAB) (Neu, 2015), with stochastic rewards to update \(\mathbf{q}\). In this way, our algorithm only needs \(1\) sample in each round and attains an \(O(\sqrt{m(\log m)/T})\) convergence rate, implying the same \(O(m(\log m)/\epsilon^{2})\) sample complexity.

Next, we investigate a more practical and challenging scenario in which there are different budgets of samples that can be drawn from each distribution, a natural phenomenon encountered in learning with imbalanced data (Amodei et al., 2016). Let \(n_{i}\) be the sample budget of the \(i\)-th distribution, and without loss of generality, we assume that \(n_{1}\geq n_{2}\geq\cdots\geq n_{m}\). Now, the goal is not to attain the optimal sample complexity, but to reduce the risk on all distributions as much as possible, under the budget constraint. For GDRO with different budgets, we develop two SA approaches based on non-uniform sampling and mini-batches, respectively.

In each iteration of the first approach, we draw \(1\) sample from every \(\mathcal{P}_{i}\) with probability \(n_{i}/n_{1}\), and then construct stochastic gradients to perform mirror descent. In this way, the budget will be satisfied in expectation after \(n_{1}\) rounds. To analyze its performance, we propose a novel formulation of weighted GDRO, which weights each risk \(R_{i}(\cdot)\) in (3) by a scale factor \(p_{i}\). Then, our algorithm can be regarded as SMD for an instance of weighted GDRO. With the help of scale factors, we demonstrate that the proposed algorithm enjoys _distribution-dependent_ convergence in the sense that it converges faster for distributions with more samples. In particular, the excess risk on distribution \(\mathcal{P}_{i}\) reduces at an \(O(\sqrt{n_{1}\log m}/n_{i})\) rate, and for \(\mathcal{P}_{1}\), it becomes \(O(\sqrt{(\log m)/n_{1}})\), which almost matches the optimal \(O(\sqrt{1/n_{1}})\) rate of learning from a single distribution with \(n_{1}\) samples.

On the other hand, for distribution \(\mathcal{P}_{i}\) with budget \(n_{i}<n_{1}\), the above \(O(\sqrt{n_{1}\log m}/n_{i})\) rate is worse than the \(O(\sqrt{1/n_{i}})\) rate obtained by learning from \(\mathcal{P}_{i}\) alone. In shape contrast with this limitation, our second approach yields nearly optimal convergence rates for _multiple_ distributions across a large range of budgets. To meet the budget constraint, it runs for \(\bar{n}\leq n_{m}\) rounds, and in each iteration, draws a mini-batch of \(n_{i}/\bar{n}\) samples from every distribution \(\mathcal{P}_{i}\). As a result, (i) the budget constraint is satisfied _exactly_; (ii) for distributions with a larger budget, the associated risk function can be estimated more accurately, making the variance of the stochastic gradient smaller. To benefit from the small variance, we leverage stochastic mirror-prox algorithm (Juditsky et al., 2011), instead of SMD, to update our solutions, and again make use of the weighted GDRO formulation to obtain distribution-wise convergence rates. Theoretical analysis shows that the excess risk converges at an \(O((\frac{1}{n_{m}}+\frac{1}{\sqrt{n_{i}}})\log m)\) rate for each \(\mathcal{P}_{i}\). Thus, we obtain a nearly optimal \(O((\log m)/\sqrt{n_{i}})\) rate for distributions \(\mathcal{P}_{i}\) with \(n_{i}\leq n_{m}^{2}\), and an \(O((\log m)/n_{m})\) rate otherwise. Note that the latter rate is as expected since the algorithm only updates \(O(n_{m})\) times.

Related workWe briefly discuss the related works on the GDRO problem in (2)/(3) and will review the traditional DRO problem in Appendix A. Sagawa et al. (2020) have applied SMD (Nemirovski et al., 2009) to (3), but only obtain a sub-optimal sample complexity. In the sequel, Haghtalab et al. (2022) and Soma et al. (2022) have tried to improve the sample complexity by reusing samples and applying techniques from MAB respectively, but their analysis suffers dependency issues. Carmon and Hausler (2022, Proposition 2) successfully established an \(O(m(\log m)/\epsilon^{2})\) sample complexity by combining SMD and gradient clipping, but their result holds only in expectation. To deal with heterogeneous noise in different distributions, Agarwal and Zhang (2022) propose a variant of GDRO named as minimax regret optimization (MRO), which replaces the risk with "excess risk".

## 2 SA Approaches to GDRO

In this section, we provide two efficient SA approaches for GDRO, which are equipped with the same sample complexity, but with different number of samples used in each round (\(m\) versus \(1\)).

### Preliminaries

First, we state the general setup of mirror descent (Nemirovski et al., 2009). We equip the domain \(\mathcal{W}\) with a distance-generating function \(\nu_{w}(\cdot)\), which is \(1\)-strongly convex with respect to certain norm \(\|\cdot\|_{w}\). We define the Bregman distance associated with \(\nu_{w}(\cdot)\) as \(B_{w}(\mathbf{u},\mathbf{v})=\nu_{w}(\mathbf{u})-[\nu_{w}(\mathbf{v})+\langle \nabla\nu_{w}(\mathbf{v}),\mathbf{u}-\mathbf{v}\rangle]\). For the simplex \(\Delta_{m}\), we choose the entropy function \(\nu_{q}(\mathbf{q})=\sum_{i=1}^{m}q_{i}\ln q_{i}\), which is \(1\)-strongly convex with respect to the vector \(\ell_{1}\)-norm \(\|\cdot\|_{1}\), as the distance-generating function. Similarly, \(B_{q}(\cdot,\cdot)\) is the Bregman distance associated with \(\nu_{q}(\cdot)\).

Then, we introduce the standard assumptions about the domain, and the loss function.

**Assumption 1**: _The domain \(\mathcal{W}\) is convex and its diameter measured by \(\nu_{w}(\cdot)\) is bounded by \(D\), i.e.,_

\[\max_{\mathbf{w}\in\mathcal{W}}\nu_{w}(\mathbf{w})-\min_{\mathbf{w}\in \mathcal{W}}\nu_{w}(\mathbf{w})\leq D^{2}.\] (4)

For \(\Delta_{m}\), it is easy to verify that its diameter measured by the entropy function is bounded by \(\sqrt{\ln m}\).

**Assumption 2**: _For all \(i\in[m]\), the risk function \(R_{i}(\mathbf{w})=\mathrm{E}_{\mathbf{z}\sim\mathcal{P}_{i}}[\ell(\mathbf{w} ;\mathbf{z})]\) is convex._

**Assumption 3**: _For all \(i\in[m]\), we have_

\[0\leq\ell(\mathbf{w};\mathbf{z})\leq 1,\ \forall\mathbf{w}\in\mathcal{W},\ \mathbf{z} \sim\mathcal{P}_{i}.\] (5)

**Assumption 4**: _For all \(i\in[m]\), we have_

\[\|\nabla\ell(\mathbf{w};\mathbf{z})\|_{w,*}\leq G,\ \forall\mathbf{w}\in \mathcal{W},\ \mathbf{z}\sim\mathcal{P}_{i}\] (6)

_where \(\|\cdot\|_{w,*}\) is the dual norm of \(\|\cdot\|_{w}\)._Last, we discuss the performance measure. To analyze the convergence property, we measure the quality of an approximate solution \((\bar{\mathbf{w}},\bar{\mathbf{q}})\) to (3) by the error

\[\epsilon_{\phi}(\bar{\mathbf{w}},\bar{\mathbf{q}})=\max_{\mathbf{q}\in\Delta_{m} }\phi(\bar{\mathbf{w}},\mathbf{q})-\min_{\mathbf{w}\in\mathcal{W}}\phi(\mathbf{ w},\bar{\mathbf{q}})\] (7)

which directly controls the optimality of \(\bar{\mathbf{w}}\) to the original problem (2), since

\[\max_{i\in[m]}R_{i}(\bar{\mathbf{w}})-\min_{\mathbf{w}\in\mathcal{W}}\max_{i \in[m]}R_{i}(\mathbf{w})\leq\max_{\mathbf{q}\in\Delta_{m}}\sum_{i=1}^{m}q_{i} R_{i}(\bar{\mathbf{w}})-\min_{\mathbf{w}\in\mathcal{W}}\sum_{i=1}^{m}\bar{q}_{i} R_{i}(\mathbf{w})=\epsilon_{\phi}(\bar{\mathbf{w}},\bar{\mathbf{q}}).\] (8)

### Stochastic Mirror Descent for GDRO

To apply SMD, the key is to construct stochastic gradients of the function \(\phi(\mathbf{w},\mathbf{q})\) in (3). In each round \(t\), denote by \(\mathbf{w}_{t}\) and \(\mathbf{q}_{t}\) the current solutions. We draw one sample \(\mathbf{z}_{t}^{(i)}\) from every distribution \(\mathcal{P}_{i}\), and define stochastic gradients as

\[\mathbf{g}_{w}(\mathbf{w}_{t},\mathbf{q}_{t})=\sum_{i=1}^{m}q_{t,i}\nabla \ell(\mathbf{w}_{t};\mathbf{z}_{t}^{(i)}),\text{ and }\mathbf{g}_{q}(\mathbf{w}_{t}, \mathbf{q}_{t})=[\ell(\mathbf{w}_{t};\mathbf{z}_{t}^{(1)}),\ldots,\ell(\mathbf{ w}_{t};\mathbf{z}_{t}^{(m)})]^{\top}.\] (9)

It is worth mentioning that the construction of \(\mathbf{g}_{w}(\mathbf{w}_{t},\mathbf{q}_{t})\) can be further simplified to

\[\tilde{\mathbf{g}}_{w}(\mathbf{w}_{t},\mathbf{q}_{t})=\nabla\ell(\mathbf{w}_{ t};\mathbf{z}_{t}^{(i_{t})})\] (10)

where \(i_{t}\in[m]\) is drawn randomly according to the probability \(\mathbf{q}_{t}\).

Then, we utilize SMD to update \(\mathbf{w}_{t}\) and \(\mathbf{q}_{t}\):

\[\mathbf{w}_{t+1}= \operatorname*{argmin}_{\mathbf{w}\in\mathcal{W}}\big{\{}\eta_{q }\langle\mathbf{g}_{w}(\mathbf{w}_{t},\mathbf{q}_{t}),\mathbf{w}-\mathbf{w}_{ t}\rangle+B_{w}(\mathbf{w},\mathbf{w}_{t})\big{\}},\] (11) \[\mathbf{q}_{t+1}= \operatorname*{argmin}_{\mathbf{q}\in\Delta_{m}}\big{\{}\eta_{q }\langle-\mathbf{g}_{q}(\mathbf{w}_{t},\mathbf{q}_{t}),\mathbf{q}-\mathbf{q}_ {t}\rangle+B_{q}(\mathbf{q},\mathbf{q}_{t})\big{\}}\] (12)

where \(\eta_{w}>0\) and \(\eta_{q}>0\) are two step sizes that will be determined later. The updating rule of \(\mathbf{w}_{t}\) depends on the choice of the distance-generating function \(\nu_{w}(\cdot)\). Since \(B_{q}(\mathbf{q},\mathbf{q}_{t})\) is defined in terms of the negative entropy, (12) is equivalent to

\[q_{t+1,i}=\frac{q_{t,i}\exp\big{(}\eta_{q}\ell(\mathbf{w}_{t};\mathbf{z}_{t}^{( i)})\big{)}}{\sum_{j=1}^{m}q_{t,j}\exp\big{(}\eta_{q}\ell(\mathbf{w}_{t}; \mathbf{z}_{t}^{(j)})\big{)}},\;\forall i\in[m]\] (13)

which is the Hedge algorithm (Freund and Schapire, 1997) applied to a maximization problem. In the beginning, we set \(\mathbf{w}_{1}=\operatorname*{argmin}_{\mathbf{w}\in\mathcal{W}}\nu_{w}( \mathbf{w})\), and \(\mathbf{q}_{1}=\frac{1}{m}\mathbf{1}_{m}\), where \(\mathbf{1}_{m}\) is the \(m\)-dimensional vector consisting of \(1\)'s. In the last step, we return the averaged iterates \(\bar{\mathbf{w}}=\frac{1}{T}\sum_{t=1}^{T}\mathbf{w}_{t}\) and \(\bar{\mathbf{q}}=\frac{1}{T}\sum_{t=1}^{T}\mathbf{q}_{t}\) as final solutions. The completed procedure is summarized in Algorithm 1.

```
0: Two step sizes: \(\eta_{w}\) and \(\eta_{q}\)
1: Initialize \(\mathbf{w}_{1}=\operatorname*{argmin}_{\mathbf{w}\in\mathcal{W}}\nu_{w}( \mathbf{w})\), and \(\mathbf{q}_{1}=[1/m,\ldots,1/m]^{\top}\in\mathbb{R}^{m}\)
2:for\(t=1\) to \(T\)do
3: For each \(i\in[m]\), draw a sample \(\mathbf{z}_{t}^{(i)}\) from distribution \(\mathcal{P}_{i}\)
4: Construct the stochastic gradients defined in (9)
5: Update \(\mathbf{w}_{t}\) and \(\mathbf{q}_{t}\) according to (11) and (12), respectively
6:endfor
7:return\(\bar{\mathbf{w}}=\frac{1}{T}\sum_{t=1}^{T}\mathbf{w}_{t}\) and \(\bar{\mathbf{q}}=\frac{1}{T}\sum_{t=1}^{T}\mathbf{q}_{t}\) ```

**Algorithm 1** Stochastic Mirror Descent for GDRO

Based on the theoretical guarantee of SMD (Nemirovski et al., 2009, SS3.1), we have the following theorem.

**Theorem 1**: _Under Assumptions 1, 2, 3 and 4, and setting \(\eta_{w}=D^{2}\sqrt{\frac{8}{5T(D^{2}G^{2}+\ln m)}}\) and \(\eta_{q}=(\ln m)\sqrt{\frac{8}{5T(D^{2}G^{2}+\ln m)}}\) in Algorithm 1, with probability at least \(1-\delta\), we have_

\[\epsilon_{\phi}(\bar{\mathbf{w}},\bar{\mathbf{q}})\leq\left(8+2\ln\frac{2}{ \delta}\right)\sqrt{\frac{10(D^{2}G^{2}+\ln m)}{T}}.\]

**Remark 1**Theorem 1 shows that Algorithm 1 achieves an \(O(\sqrt{(\log m)/T})\) convergence rate. Since it consumes \(m\) samples per iteration, the sample complexity is \(O(m(\log m)/\epsilon^{2})\), which nearly matches the \(\Omega(m/\epsilon^{2})\) lower bound (Soma et al., 2022, Theorem 5). Due to space limitations, we defer all the proofs to Appendix B, and omit expectation bounds.

Comparisons with Sagawa et al. (2020)In their stochastic algorithm, Sagawa et al. (2020) generate a random index \(i_{t}\in[m]\) uniformly in each round \(t\), and draw \(1\) sample \(\mathbf{z}_{t}^{(i_{t})}\) from \(\mathcal{P}_{i_{t}}\). The stochastic gradients are constructed as follows:

\[\hat{\mathbf{g}}_{w}(\mathbf{w}_{t},\mathbf{q}_{t})=q_{t,i_{t}}m\nabla\ell( \mathbf{w}_{t};\mathbf{z}_{t}^{(i_{t})}),\text{ and }\hat{\mathbf{g}}_{q}(\mathbf{w}_{t}, \mathbf{q}_{t})=[0,\ldots,m\ell(\mathbf{w}_{t};\mathbf{z}_{t}^{(i_{t})}), \ldots,0]^{\top}\] (14)

where \(\hat{\mathbf{g}}_{g}(\mathbf{w}_{t},\mathbf{q}_{t})\) is a vector with \(m\ell(\mathbf{w}_{t};\mathbf{z}_{t}^{(i_{t})})\) in position \(i_{t}\) and \(0\) elsewhere. Then, the two stochastic gradients are used to update \(\mathbf{w}_{t}\) and \(\mathbf{q}_{t}\), in the same way as (11) and (12). However, it only attains a slow convergence rate: \(O(m\sqrt{(\log m)/T})\), leading to an \(O(m^{2}(\log m)/\epsilon^{2})\) sample complexity, which is higher than that of Algorithm 1 by a factor of \(m\). The slow convergence is due to the fact that the optimization error depends on the dual norm of the stochastic gradients in (14), which blows up by a factor of \(m\), compared with the gradients in (9).

Comparisons with Haghtalab et al. (2022)To reduce the number of samples required in each round, Haghtalab et al. (2022) propose to reuse samples for multiple iterations. To approximate \(\nabla_{\mathbf{w}}\phi(\mathbf{w}_{t},\mathbf{q}_{t})\), they construct the stochastic gradient \(\tilde{\mathbf{g}}_{w}(\mathbf{w}_{t},\mathbf{q}_{t})\) in the same way as (10), which needs \(1\) sample. To approximate \(\nabla_{\mathbf{q}}\phi(\mathbf{w}_{t},\mathbf{q}_{t})\), they draw \(m\) samples \(\mathbf{z}_{\tau}^{(1)},\ldots,\mathbf{z}_{\tau}^{(m)}\), one from each distribution, at round \(\tau=mk+1\), \(k\in\mathbb{Z}\), and reuse them for \(m\) iterations to construct

\[\mathbf{g}_{q}^{\prime}(\mathbf{w}_{t},\mathbf{q}_{t})=[\ell(\mathbf{w}_{t}; \mathbf{z}_{\tau}^{(1)}),\ldots,\ell(\mathbf{w}_{t};\mathbf{z}_{\tau}^{(m)})] ^{\top},\ t=\tau,\ldots,\tau+m-1.\] (15)

Then, they treat \(\tilde{\mathbf{g}}_{w}(\mathbf{w}_{t},\mathbf{q}_{t})\) and \(\mathbf{g}_{q}^{\prime}(\mathbf{w}_{t},\mathbf{q}_{t})\) as stochastic gradients, and update \(\mathbf{w}_{t}\) and \(\mathbf{q}_{t}\) by SMD. In this way, their algorithm uses \(2\) samples on average in each iteration. However, the gradient in (15) is no longer an unbiased estimator of the true gradient \(\mathbf{g}_{q}(\mathbf{w}_{t},\mathbf{q}_{t})\) at rounds \(t=\tau+2,\ldots,\tau+m-1\), making their analysis ungrounded. To see this, from the updating rule of SMD, we know that \(\mathbf{w}_{\tau+2}\) depends on \(\mathbf{q}_{\tau+1}\), which in turn depends on the \(m\) samples drawn at round \(\tau\), and thus

\[\mathrm{E}\left[\ell(\mathbf{w}_{\tau+2};\mathbf{z}_{\tau}^{(i)})\right]\neq R _{i}(\mathbf{w}_{\tau+2}),\ i=1,\ldots,m.\]

**Remark 2** As shown in (10), we can use \(1\) sample to construct a stochastic gradient for \(\mathbf{w}_{t}\) with small norm, since \(\|\tilde{\mathbf{g}}_{w}(\mathbf{w}_{t},\mathbf{q}_{t})\|_{w,*}\leq G\) under Assumption 4. Thus, it is relatively easy to control the error related to \(\mathbf{w}_{t}\). However, we do not have such guarantees for the stochastic gradient of \(\mathbf{q}_{t}\). Recall that the infinity norm of \(\hat{\mathbf{g}}_{q}(\mathbf{w}_{t},\mathbf{q}_{t})\) in (14) is upper bounded by \(m\). The reason is that we insist on the unbiasedness of the stochastic gradient, which leads to a large variance. In the next section, we borrow techniques from online learning to better balance the bias and the variance.

### Non-oblivious Online Learning for GDRO

In the studies of convex-concave saddle-point problems, it is now well-known that they can be solved by playing two online learning algorithms against each other (Freund and Schapire, 1999, Rakhlin and Sridharan, 2013, Syrgkanis et al., 2015). With this transformation, we can exploit no-regret algorithms developed in online learning to bound the optimization error. To solve problem (3), we ask the 1st player to minimize a sequence of convex functions

\[\phi(\mathbf{w},\mathbf{q}_{1})=\sum_{i=1}^{m}q_{1,i}R_{i}(\mathbf{w}),\ \ \phi(\mathbf{w},\mathbf{q}_{2})=\sum_{i=1}^{m}q_{2,i}R_{i}(\mathbf{w}),\ \cdots,\ \phi(\mathbf{w},\mathbf{q}_{T})=\sum_{i=1}^{m}q_{T,i}R_{i}(\mathbf{w})\]

subject to the constraint \(\mathbf{w}\in\mathcal{W}\), and the 2nd player to maximize a sequence of linear functions

\[\phi(\mathbf{w}_{1},\mathbf{q})=\sum_{i=1}^{m}q_{i}R_{i}(\mathbf{w}_{1}),\ \ \phi( \mathbf{w}_{2},\mathbf{q})=\sum_{i=1}^{m}q_{i}R_{i}(\mathbf{w}_{2}),\ \cdots,\ \phi(\mathbf{w}_{T},\mathbf{q})=\sum_{i=1}^{m}q_{i}R_{i}( \mathbf{w}_{T})\]

subject to the constraint \(\mathbf{q}\in\Delta_{m}\). We highlight that there exists an important difference between our stochastic convex-concave problem and its deterministic counterpart. Here, the two players cannotdirectly observe the loss function, and can only approximate \(R_{i}(\mathbf{w})=\mathrm{E}_{\mathbf{z}\sim\mathcal{P}_{i}}\big{[}\ell(\mathbf{w };\mathbf{z})\big{]}\) by drawing samples from \(\mathcal{P}_{i}\). The stochastic setting makes the problem more challenging, and in particular, we need to take care of the _non-oblivious_ nature of the learning process. Here, "non-oblivious" refers to the fact that the online functions depend on the past decisions of the learner.

Next, we discuss the online algorithms that will be used by the two players. As shown in Section 2.2, the 1st player can easily obtain a stochastic gradient with small norm by using \(1\) sample. So, we model the problem faced by the 1st player as "non-oblivious online convex optimization (OCO) with stochastic gradients", and still use SMD to update its solution. In each round \(t\), with \(1\) sample drawn from \(\mathcal{P}_{i}\), the 2nd player can estimate the value of \(R_{i}(\mathbf{w}_{t})\) which is the coefficient of \(q_{i}\). Since the 2nd player is maximizing a linear function over the simplex, the problem can be modeled as "non-oblivious multi-armed bandits (MAB) with stochastic rewards". And fortunately, we have powerful online algorithms for non-oblivious MAB (Auer et al., 2002; Lattimore and Szepesvari, 2020), whose regret has a sublinear dependence on \(m\). In this paper, we choose the Exp3-IX algorithm (Neu, 2015), and generalize its theoretical guarantee to stochastic rewards.

The complete procedure is presented in Algorithm 2, and we explain key steps below. In each round \(t\), we generate an index \(i_{t}\in[m]\) from the probability distribution \(\mathbf{q}_{t}\), and then draw a sample \(\mathbf{z}_{t}^{(i_{t})}\) from the distribution \(\mathbf{q}_{i_{t}}\). With the stochastic gradient in (10), we use SMD to update \(\mathbf{w}_{t}\):

\[\mathbf{w}_{t+1}=\operatorname*{argmin}_{\mathbf{w}\in\mathcal{W}}\big{\{} \eta_{w}\langle\tilde{\mathbf{g}}_{w}(\mathbf{w}_{t},\mathbf{q}_{t}),\mathbf{ w}-\mathbf{w}_{t}\rangle+B_{w}(\mathbf{w},\mathbf{w}_{t})\big{\}}\] (16)

Then, we reuse the sample \(\mathbf{z}_{t}^{(i_{t})}\) to update \(\mathbf{q}_{t}\) according to Exp3-IX, which first constructs the Implicit-eXploration (IX) loss estimator (Kocak et al., 2014):

\[\tilde{s}_{t,i}=\frac{1-\ell(\mathbf{w}_{t},\mathbf{z}_{t}^{(i_{t})})}{q_{t,i }+\gamma}\cdot\mathbb{I}[i_{t}=i],\;\forall i\in[m],\] (17)

where \(\gamma>0\) is the IX coefficient and \(\mathbb{I}[A]\) equals to \(1\) when the event \(A\) is true and \(0\) otherwise, and then performs a mirror descent update:

\[\mathbf{q}_{t+1}=\operatorname*{argmin}_{\mathbf{q}\in\Delta_{m}}\big{\{} \eta_{q}\left\langle\tilde{\mathbf{s}}_{t},\mathbf{q}-\mathbf{q}_{t}\right\rangle +B_{q}(\mathbf{q},\mathbf{q}_{t})\big{\}}.\] (18)

We present the theoretical guarantee of Algorithm 2.

**Theorem 2**: _Under Assumptions 1, 2, 3 and 4, and setting \(\eta_{w}=\frac{2D}{G\sqrt{5T}}\), \(\eta_{q}=\sqrt{\frac{\ln m}{mT}}\) and \(\gamma=\frac{\eta_{q}}{2}\) in Algorithm 2, with probability at least \(1-\delta\), we have_

\[\epsilon_{\phi}(\bar{\mathbf{w}},\bar{\mathbf{q}})\] (19) \[\leq DG\sqrt{\frac{1}{T}}\left(2\sqrt{5}+8\sqrt{\ln\frac{2}{\delta}} \right)+3\sqrt{\frac{m\ln m}{T}}+\sqrt{\frac{1}{2T}}+\left(\sqrt{\frac{m}{T \ln m}}+\sqrt{\frac{1}{2T}}+\frac{1}{T}\right)\ln\frac{6}{\delta}.\]

**Remark 3**: The above theorem shows that with \(1\) sample per iteration, Algorithm 2 is able to achieve an \(O(\sqrt{m(\log m)/T})\) convergence rate, thus maintaining the \(O(m(\log m)/\epsilon^{2})\) sample complexity.

Comparisons with Soma et al. (2022)In a recent work, Soma et al. (2022) have deployed online algorithms to optimize \(\mathbf{w}\) and \(\mathbf{q}\), but did not consider the non-oblivious property. As a result, their theoretical guarantees, which build upon the analysis for oblivious online learning (Orabona, 2019), cannot justify the optimality of their algorithm for (3). Specifically, their results imply that for any _fixed_\(\mathbf{w}\) and \(\mathbf{q}\) that are independent from \(\bar{\mathbf{w}}\) and \(\bar{\mathbf{q}}\)(Soma et al., 2022, Theorem 3),

\[\mathrm{E}\left[\phi(\bar{\mathbf{w}},\mathbf{q})-\phi(\mathbf{w},\bar{ \mathbf{q}})\right]=O\left(\sqrt{\frac{m}{T}}\right).\] (20)

However, (20) cannot be used to bound \(\epsilon_{\phi}(\bar{\mathbf{w}},\bar{\mathbf{q}})\) in (7), because of the dependency issue. To be more clear, we have \(\epsilon_{\phi}(\bar{\mathbf{w}},\bar{\mathbf{q}})=\max_{\mathbf{q}\in\Delta_ {m}}\phi(\bar{\mathbf{w}},\mathbf{q})-\min_{\mathbf{w}\in\mathcal{W}}\phi( \mathbf{w},\bar{\mathbf{q}})=\phi(\bar{\mathbf{w}},\bar{\mathbf{q}})-\phi( \bar{\mathbf{w}},\bar{\mathbf{q}})\), where \(\bar{\mathbf{w}}=\operatorname*{argmin}_{\mathbf{w}\in\mathcal{W}}\phi( \bar{\mathbf{w}},\bar{\mathbf{q}})\) and \(\bar{\mathbf{q}}=\operatorname*{argmax}_{\mathbf{q}\in\Delta_{m}}\phi(\bar{ \mathbf{w}},\mathbf{q})\)_depend_ on \(\bar{\mathbf{w}}\) and \(\bar{\mathbf{q}}\).

**Remark 4**: After we pointed out the issue of reusing samples, Haghtalab et al. (2023) modified their method by incorporating bandits algorithms to optimize \(\mathbf{q}\). From our understanding, the idea of applying bandits to GDRO is _firstly_ proposed by Soma et al. (2022), and subsequently refined by us.

## 3 Weighted GDRO and SA Approaches

When designing SA approaches for GDRO, it is common to assume that the algorithms are free to draw samples from every distribution (Sagawa et al., 2020), as we do in Section 2. However, this assumption may not hold in practice. For example, data collection costs can vary widely among distributions (Radivojac et al., 2004), and data collected from various channels can have different throughputs (Zhou, 2023). In this section, we investigate the scenario where the number of samples can be drawn from each distribution could be different. Denote by \(n_{i}\) the number of samples that can be drawn from \(\mathcal{P}_{i}\). Without loss of generality, we assume that \(n_{1}\geq n_{2}\geq\cdots\geq n_{m}\). Note that we have a straightforward **Baseline** which just runs Algorithm 1 for \(n_{m}\) iterations, and the optimization error \(\epsilon_{\phi}(\bar{\mathbf{w}},\bar{\mathbf{q}})=O(\sqrt{(\log m)/n_{m}})\).

### Stochastic Mirror Descent with Non-uniform Sampling

To meet the budget, we propose to incorporate non-uniform sampling into SMD. Specifically, in round \(t\), we first generate a set of Bernoulli random variables \(\{b_{t}^{(1)},\ldots,b_{t}^{(m)}\}\) with \(\Pr[b_{t}^{(i)}=1]=p_{i}\) to determine whether to sample from each distribution. If \(b_{t}^{(i)}=1\), we draw a sample \(\mathbf{z}_{t}^{(i)}\) from \(\mathcal{P}_{i}\). Now, the question is how to construct stochastic gradients from those samples. Let \(\mathcal{C}_{t}=\{i|b_{t}^{(i)}=1\}\) be the indices of selected distributions. If we stick to the original problem in (3), then the stochastic gradients should be constructed in the following way

\[\mathbf{g}_{w}(\mathbf{w}_{t},\mathbf{q}_{t})=\sum_{i\in C_{t}}\frac{q_{t,i}}{ p_{i}}\nabla\ell(\mathbf{w}_{t};\mathbf{z}_{t}^{(i)}),\text{ and }[\mathbf{g}_{q}(\mathbf{w}_{t},\mathbf{q}_{t})]_{i}=\left\{\begin{array}{ll} \ell(\mathbf{w}_{t};\mathbf{z}_{t}^{(i)})/p_{i},&i\in\mathcal{C}_{t}\\ 0,&\text{otherwise}\end{array}\right.\] (21)

to ensure unbiasedness. Then, they can be used by SMD to update \(\mathbf{w}_{t}\) and \(\mathbf{q}_{t}\). After \(n_{1}\) iterations, the _expected_ number of samples drawn from \(\mathcal{P}_{i}\) will be \(n_{1}p_{i}=n_{i}\), and thus the budget is satisfied in expectation. To analyze the optimization error, we need to bound the norm of stochastic gradients in (21). To this end, we have \(\|\mathbf{g}_{w}(\mathbf{w}_{t},\mathbf{q}_{t})\|_{w,*}\leq Gn_{1}/n_{m}\) and \(\|\mathbf{g}_{q}(\mathbf{w}_{t},\mathbf{q}_{t})\|_{\infty}\leq n_{1}/n_{m}\). Following the arguments of Theorem 1, we can prove that the error \(\epsilon_{\phi}(\bar{\mathbf{w}},\bar{\mathbf{q}})=O(\sqrt{(\log m)/n_{1}} \cdot n_{1}/n_{m})=O(\sqrt{n_{1}\,\log m}/n_{m})\), which is even larger than the \(O(\sqrt{(\log m)/n_{m}})\) error of the Baseline.

In the following, we demonstrate that a simple twist of the above procedure can still yield meaningful results that are complementary to the Baseline. We observe that the large norm of the stochastic gradients in (21) is caused by the inverse probability \(1/p_{i}\). A natural idea is to ignore \(1/p_{i}\), and define the following stochastic gradients:

\[\mathbf{g}_{w}(\mathbf{w}_{t},\mathbf{q}_{t})=\sum_{i\in C_{t}}q_{t,i}\nabla \ell(\mathbf{w}_{t};\mathbf{z}_{t}^{(i)}),\text{ and }[\mathbf{g}_{q}(\mathbf{w}_{t},\mathbf{q}_{t})]_{i}=\left\{ \begin{array}{ll}\ell(\mathbf{w}_{t};\mathbf{z}_{t}^{(i)}),&i\in\mathcal{C}_ {t}\\ 0,&\text{otherwise}.\end{array}\right.\] (22)

In this way, they are no longer stochastic gradients of (3), but can be treated as stochastic gradients of a weighted GDRO problem:

\[\min_{\mathbf{w}\in\mathcal{W}}\max_{\mathbf{q}\in\Delta_{m}}\ \left\{\varphi(\mathbf{w},\mathbf{q})=\sum_{i=1}^{m}q_{i}p_{i}R_{i}( \mathbf{w})\right\}\] (23)where each risk \(R_{i}(\cdot)\) is scaled by a factor \(p_{i}\). Based on the gradients in (22), we still use (11) and (12) to update \(\mathbf{w}_{t}\) and \(\mathbf{q}_{t}\). We summarize the complete procedure in Algorithm 3.

```
1:Initialize \(\mathbf{w}_{1}=\operatorname*{argmin}_{\mathbf{w}\in\mathcal{W}}\nu_{w}( \mathbf{w})\), and \(\mathbf{q}_{1}=[1/m,\dots,1/m]^{\top}\in\mathbb{R}^{m}\)
2:for\(t=1\) to \(n_{1}\)do
3: For each \(i\in[m]\), generate a Bernoulli random variable \(b_{t}^{(i)}\) with \(\Pr[b_{t}^{(i)}=1]=p_{i}\), and if \(b_{t}^{(i)}=1\), draw a sample \(\mathbf{z}_{t}^{(i)}\) from distribution \(\mathcal{P}_{i}\)
4: Construct the stochastic gradients defined in (22)
5: Update \(\mathbf{w}_{t}\) and \(\mathbf{q}_{t}\) according to (11) and (12), respectively
6:endfor
7:return\(\bar{\mathbf{w}}=\frac{1}{n_{1}}\sum_{t=1}^{n_{1}}\mathbf{w}_{t}\) and \(\bar{\mathbf{q}}=\frac{1}{n_{1}}\sum_{t=1}^{n_{1}}\mathbf{q}_{t}\) ```

**Algorithm 3** Stochastic Mirror Descent for Weighted GDRO

We omit the optimization error of Algorithm 3 for (23), since it has exactly the same form as Theorem 1. What we are really interested in is the theoretical guarantee of its solution on multiple distributions. To this end, we have the following theorem.

**Theorem 3**: _Under Assumptions 1, 2, 3 and 4, and setting \(\eta_{w}=D^{2}\sqrt{\frac{8}{5n_{1}(D^{2}G^{2}+\ln m)}}\) and \(\eta_{q}=(\ln m)\sqrt{\frac{8}{5n_{1}(D^{2}G^{2}+\ln m)}}\) in Algorithm 3, with probability at least \(1-\delta\), we have_

\[R_{i}(\bar{\mathbf{w}})-\frac{n_{1}}{n_{i}}p_{\varphi}^{*}\leq\frac{1}{p_{i}} \mu(\delta)\sqrt{\frac{10(D^{2}G^{2}+\ln m)}{n_{1}}}=\mu(\delta)\frac{\sqrt{1 0(D^{2}G^{2}+\ln m)n_{1}}}{n_{i}},\;\forall i\in[m]\]

_where \(p_{\varphi}^{*}\) is the optimal value of (23) and \(\mu(\delta)=8+2\ln\frac{2}{\delta}\)._

**Remark 5**: We see that Algorithm 3 exhibits a _distribution-dependent_ convergence behavior: The larger the number of samples \(n_{i}\), the smaller the target risk \(n_{1}p_{\varphi}^{*}/n_{i}\), and the faster the convergence rate \(O(\sqrt{n_{1}\log m}/n_{i})\). Note that its rate is always better than the \(O(\sqrt{n_{1}\log m}/n_{m})\) rate of SMD with (21) as gradients. Furthermore, it converges faster than the Baseline when \(n_{i}\geq\sqrt{n_{1}n_{m}}\). In particular, for distribution \(\mathcal{P}_{1}\), Algorithm 3 attains an \(O(\sqrt{(\log m)/n_{1}})\) rate, which almost matches the optimal \(O(\sqrt{1/n_{1}})\) rate of learning from a single distribution. Finally, we would like to emphasize that a similar idea of introducing "scale factors" has been used by Juditsky et al. (2011, SS4.3.1) for stochastic semidefinite feasibility problems and Agarwal and Zhang (2022) for empirical MRO.

### Stochastic Mirror-Prox Algorithm with Mini-batches

In Algorithm 3, distributions with more samples take their advantage by appearing more frequently in the stochastic gradients. In this section, we propose a different way, which lets them reduce the variance in the elements of stochastic gradients by mini-batches (Roux et al., 2008).

The basic idea is as follows. We run our algorithm for a small number of iterations \(\bar{n}\) that is no larger than \(n_{m}\). Then, in each iteration, we draw a mini-batch of \(n_{i}/\bar{n}\) samples from every distribution \(\mathcal{P}_{i}\). For \(\mathcal{P}_{i}\) with more samples, we can estimate the associated risk \(R_{i}(\cdot)\) and its gradient more accurately, i.e., with a smaller variance. However, to make this idea work, we need to tackle two obstacles: (i) the performance of the SA algorithm should depend on the variance of gradients instead of the norm, and for this reason SMD is unsuitable; (ii) even some elements of the stochastic gradient have small variances, the entire gradient may still have a large variance. To address the first challenge, we resort to a more advanced SA approach--stochastic mirror-prox algorithm (SMPA), whose convergence rate depends on the variance (Juditsky et al., 2011). To overcome the second challenge, we again introduce scale factors into the optimization problem and the stochastic gradients.

In SMPA, we need to maintain two sets of solutions: \((\mathbf{w}_{t},\mathbf{q}_{t})\) and \((\mathbf{w}_{t}^{\prime},\mathbf{q}_{t}^{\prime})\). In each round \(t\), we first draw \(n_{i}/n_{m}\) samples from every distribution \(\mathcal{P}_{i}\), denoted by \(\mathbf{z}_{t}^{(i,j)}\), \(j=1,\dots,n_{i}/n_{m}\). Then, we use them to construct stochastic gradients at \((\mathbf{w}_{t}^{\prime},\mathbf{q}_{t}^{\prime})\) of a weighted GDRO problem (23), where the value of \(p_{i}\) will be determined later. Specifically, we define

\[\begin{split}\mathbf{g}_{w}(\mathbf{w}_{t}^{\prime},\mathbf{q}_{t}^{ \prime})=&\sum_{i=1}^{m}q_{t,i}^{\prime}p_{i}\left(\frac{n_{m}}{n_{ i}}\sum_{j=1}^{n_{i}/n_{m}}\nabla\ell(\mathbf{w}_{t}^{\prime};\mathbf{z}_{t}^{(i,j)}) \right),\\ \mathbf{g}_{q}(\mathbf{w}_{t}^{\prime},\mathbf{q}_{t}^{\prime})=& \left[p_{1}\frac{n_{m}}{n_{1}}\sum_{j=1}^{n_{1}/n_{m}}\ell(\mathbf{w}_{t}^{ \prime};\mathbf{z}_{t}^{(1,j)}),\ldots,p_{m}\ell(\mathbf{w}_{t}^{\prime}; \mathbf{z}_{t}^{(m)})\right]^{\top}.\end{split}\] (24)

Let's take the stochastic gradient \(\mathbf{g}_{q}(\mathbf{w}_{t}^{\prime},\mathbf{q}_{t}^{\prime})\), whose variance will be measured in terms of \(\|\cdot\|_{\infty}\), as an example to explain the intuition of inserting \(p_{i}\). Define \(u_{i}=\frac{n_{m}}{n_{i}}\sum_{j=1}^{n_{i}/n_{m}}\ell(\mathbf{w}_{t}^{\prime} ;\mathbf{z}_{t}^{(i,j)})\). With a larger mini-batch size \(n_{i}/n_{m}\), \(u_{i}\) will approximate \(R_{i}(\mathbf{w}_{t}^{\prime})\) more accurately, and thus have a smaller variance. Then, it allows us to insert a larger value of \(p_{i}\), without affecting the variance of \(\|\mathbf{g}_{q}(\mathbf{w}_{t}^{\prime},\mathbf{q}_{t}^{\prime})\|_{\infty}\), since \(\|\cdot\|_{\infty}\) is insensitive to perturbations of small elements. Similar to the case in Theorem 3, the convergence rate of \(R_{i}(\cdot)\) depends on \(1/p_{i}\), and becomes faster if \(p_{i}\) is larger.

Based on (24), we use SMD to update \((\mathbf{w}_{t}^{\prime},\mathbf{q}_{t}^{\prime})\), and denote the solution by \((\mathbf{w}_{t+1},\mathbf{q}_{t+1})\):

\[\mathbf{w}_{t+1}= \operatorname*{argmin}_{\mathbf{w}\in\mathcal{W}}\big{\{}\eta_{ w}\langle\mathbf{g}_{w}(\mathbf{w}_{t}^{\prime},\mathbf{q}_{t}^{\prime}), \mathbf{w}-\mathbf{w}_{t}^{\prime}\rangle+B_{w}(\mathbf{w},\mathbf{w}_{t}^{ \prime})\big{\}},\] (25) \[\mathbf{q}_{t+1}= \operatorname*{argmin}_{\mathbf{q}\in\Delta_{m}}\big{\{}\eta_{q} \langle-\mathbf{g}_{q}(\mathbf{w}_{t}^{\prime},\mathbf{q}_{t}^{\prime}), \mathbf{q}-\mathbf{q}_{t}^{\prime}\rangle+B_{q}(\mathbf{q},\mathbf{q}_{t}^{ \prime})\big{\}}.\] (26)

Next, we draw another \(n_{i}/n_{m}\) samples from each distribution \(\mathcal{P}_{i}\), denoted by \(\hat{\mathbf{z}}_{t}^{(i,j)}\), \(j=1,\ldots,n_{i}/n_{m}\), to construct stochastic gradients at \((\mathbf{w}_{t+1},\mathbf{q}_{t+1})\):

\[\begin{split}\mathbf{g}_{w}(\mathbf{w}_{t+1},\mathbf{q}_{t+1})=& \sum_{i=1}^{m}q_{t+1,i}p_{i}\left(\frac{n_{m}}{n_{i}}\sum_{j=1}^{n_ {i}/n_{m}}\nabla\ell(\mathbf{w}_{t+1};\hat{\mathbf{z}}_{t}^{(i,j)})\right), \\ \mathbf{g}_{q}(\mathbf{w}_{t+1},\mathbf{q}_{t+1})=& \left[p_{1}\frac{n_{m}}{n_{1}}\sum_{j=1}^{n_{1}/n_{m}}\ell( \mathbf{w}_{t+1};\hat{\mathbf{z}}_{t}^{(1,j)}),\ldots,p_{m}\ell(\mathbf{w}_{t +1};\hat{\mathbf{z}}_{t}^{(m)})\right]^{\top}.\end{split}\] (27)

Then, we use them to update \((\mathbf{w}_{t}^{\prime},\mathbf{q}_{t}^{\prime})\) again, and denote the result by \((\mathbf{w}_{t+1}^{\prime},\mathbf{q}_{t+1}^{\prime})\):

\[\mathbf{w}_{t+1}^{\prime}= \operatorname*{argmin}_{\mathbf{w}\in\mathcal{W}}\big{\{}\eta_{ w}\langle\mathbf{g}_{w}(\mathbf{w}_{w}(\mathbf{w}_{t+1},\mathbf{q}_{t+1}), \mathbf{w}-\mathbf{w}_{t}^{\prime}\rangle+B_{w}(\mathbf{w},\mathbf{w}_{t}^{ \prime})\big{\}},\] (28) \[\mathbf{q}_{t+1}^{\prime}= \operatorname*{argmin}_{\mathbf{q}\in\Delta_{m}}\big{\{}\eta_{q} \langle-\mathbf{g}_{q}(\mathbf{w}_{t+1},\mathbf{q}_{t+1}),\mathbf{q}-\mathbf{ q}_{t}^{\prime}\rangle+B_{q}(\mathbf{q},\mathbf{q}_{t}^{\prime})\big{\}}.\] (29)

To meet the budget constraints, we repeat the above process for \(n_{m}/2\) iterations. Finally, we return \(\bar{\mathbf{w}}=\frac{2}{n_{m}}\sum_{t=2}^{1+n_{m}/2}\mathbf{w}_{t}\) and \(\bar{\mathbf{q}}=\frac{2}{n_{m}}\sum_{t=2}^{1+n_{m}/2}\mathbf{q}_{t}\) as solutions. The completed procedure is summarized in Algorithm 4.

```
1:Initialize \(\mathbf{w}_{t}^{\prime}=\operatorname*{argmin}_{\mathbf{w}\in\mathcal{W}} \nu_{w}(\mathbf{w})\), and \(\mathbf{q}_{t}^{\prime}=[1/m,\ldots,1/m]^{\top}\in\mathbb{R}^{m}\)
2:for\(t=1\) to \(n_{m}/2\)do
3: For each \(i\in[m]\), draw \(n_{i}/n_{m}\) samples \(\{\mathbf{z}_{t}^{(i,j)}:j=1,\ldots,n_{i}/n_{m}\}\) from distribution \(\mathcal{P}_{i}\)
4: Construct the stochastic gradients defined in (24)
5: Calculate \(\mathbf{w}_{t+1}\) and \(\mathbf{q}_{t+1}\) according to (25) and (26), respectively
6: For each \(i\in[m]\), draw \(n_{i}/n_{m}\) samples \(\{\hat{\mathbf{z}}_{t}^{(i,j)}:j=1,\ldots,n_{i}/n_{m}\}\) from distribution \(\mathcal{P}_{i}\)
7: Construct the stochastic gradients defined in (27)
8: Calculate \(\mathbf{w}_{t+1}^{\prime}\) and \(\mathbf{q}_{t+1}^{\prime}\) according to (28) and (29), respectively
9:endfor
10:return\(\bar{\mathbf{w}}=\frac{2}{n_{m}}\sum_{t=2}^{1+n_{m}/2}\mathbf{w}_{t}\) and \(\bar{\mathbf{q}}=\frac{2}{n_{m}}\sum_{t=2}^{1+n_{m}/2}\mathbf{q}_{t}\) ```

**Algorithm 4** Stochastic Mirror-Prox Algorithm for Weighted GDRO

To analysis the performance of Algorithm 4, we further assume the risk function \(R_{i}(\cdot)\) is smooth, and the dual norm \(\|\cdot\|_{w,*}\) satisfies a regularity condition.

**Assumption 5**: _All the risk functions are \(L\)-smooth, i.e.,_

\[\|\nabla R_{i}(\mathbf{w})-\nabla R_{i}(\mathbf{w}^{\prime})\|_{w,*}\leq L\| \mathbf{w}-\mathbf{w}^{\prime}\|_{w},\ \forall\mathbf{w},\mathbf{w}^{\prime}\in\mathcal{W},i\in[m].\] (30)

Note that even in the studies of stochastic convex optimization (SCO), smoothness is necessary to obtain a variance-based convergence rate (Lan, 2012).

**Assumption 6**: _The dual norm \(\|\cdot\|_{w,*}\) is \(\kappa\)-regular for some small constant \(\kappa\geq 1\)._

The regularity condition is used when analyzing the effect of mini-batches on stochastic gradients. For a formal definition, please refer to Juditsky and Nemirovski (2008). Assumption 6 is satisfied by most of papular norms considered in the literature, such as the vector \(\ell_{p}\)-norm and the infinity norm.

Then, we have the following theorem for Algorithm 4.

**Theorem 4**: _Define_

\[\begin{split}& p_{\max}=\max_{i\in[m]}p_{i},\quad\omega_{\max}= \max_{i\in[m]}\frac{p_{i}^{2}n_{m}}{n_{i}},\\ &\widetilde{L}=2\sqrt{2}p_{\max}(D^{2}L+D^{2}G\sqrt{\ln m}),\text { and }\sigma^{2}=2c\omega_{\max}(\kappa D^{2}G^{2}+\ln^{2}m)\end{split}\] (31)

_where \(c>0\) is an absolute constant. Under Assumptions 1, 2, 3, 4, 5 and 6, and setting_

\[\eta_{w}=2D^{2}\min\left(\frac{1}{\sqrt{3}\widetilde{L}},\frac{2}{\sqrt{7 \sigma^{2}n_{m}}}\right),\text{ and }\eta_{q}=2\min\left(\frac{1}{\sqrt{3} \widetilde{L}},\frac{2}{\sqrt{7\sigma^{2}n_{m}}}\right)\ln m\]

_in Algorithm 4, with probability at least \(1-\delta\), we have_

\[R_{i}(\bar{\mathbf{w}})-\frac{1}{p_{i}}p_{\varphi}^{*}=\frac{1}{p_{i}}\left( \frac{7\widetilde{L}}{n_{m}}+\sqrt{\frac{\sigma^{2}}{n_{m}}}\left(14\sqrt{ \frac{2}{3}}+7\sqrt{3\log\frac{2}{\delta}}+\frac{14}{n_{m}}\log\frac{2}{\delta }\right)\right)\]

_where \(p_{\varphi}^{*}\) is the optimal value of (23). Furthermore, by setting \(p_{i}\) as_

\[p_{i}=\frac{1/\sqrt{n_{m}}+1}{1/\sqrt{n_{m}}+\sqrt{n_{m}/n_{i}}},\] (32)

_with high probability, we have_

\[R_{i}(\bar{\mathbf{w}})-\frac{1}{p_{i}}p_{\varphi}^{*}=O\left(\left(\frac{1}{ n_{m}}+\frac{1}{\sqrt{n_{i}}}\right)\sqrt{\kappa+\ln^{2}m}\right).\]

**Remark 6**: Compared with Algorithm 3, Algorithm 4 has two advantages: (i) the budget constraint is satisfied exactly; (ii) we obtain a faster \(O((\log m)/\sqrt{n_{i}})\) rate for all distributions \(\mathcal{P}_{i}\) such that \(n_{i}\leq n_{m}^{2}\), which is much better than the \(O(\sqrt{n_{1}\ln m}/n_{i})\) rate of Algorithm 3, and the \(O(\sqrt{(\log m)/n_{m}})\) rate of the Baseline. For distributions with a larger number of budget, i.e., \(n_{i}>n_{m}^{2}\), it maintains a fast \(O((\log m)/n_{m})\) rate. Since it only updates \(n_{m}\) times, and the best we can expect is the \(O(1/n_{m})\) rate of deterministic settings (Nemirovski, 2004). So, there is a performance limit for mini-batch based methods, and after that increasing the batch-size cannot reduce the rate, which consists with the usage of mini-batches in SCO (Cotter et al., 2011; Zhang et al., 2013).

## 4 Conclusion

For the GDRO problem, we develop two SA approaches based on SMD and non-oblivious MAB. Both of them attain the nearly optimal \(O(m(\log m)/\epsilon^{2})\) sample complexity, but with different number of samples used in each round, which are \(m\) and \(1\) respectively. Then, we formulate a weighted GDRO problem to handle the scenario in which different distributions have different sample budgets. We first incorporate non-uniform sampling into SMD to satisfy the sample budget in expectation, and deliver distribution-dependent convergence rates. Then, we propose to use mini-batches to meet the budget exactly, deploy SMPA to exploit the small variances, and establish nearly optimal rates for multiple distributions. We have conducted experiments to evaluate our proposed algorithms. The empirical results are presented in Appendix C, and align closely with our theories.

## Acknowledgments and Disclosure of Funding

This work was partially supported by the National Key R&D Program of China (2022ZD0114801), NSFC (62122037, 61921006), National Postdoctoral Program for Innovative Talent, China Postdoctoral Science Foundation (2023M731597), and the major key project of PCL (PCL2021A12).

## References

* Agarwal and Zhang (2022) A. Agarwal and T. Zhang. Minimax regret optimization for robust machine learning under distribution shift. In _Proceedings of 35th Conference on Learning Theory_, pages 2704-2729, 2022.
* Amodei et al. (2016) D. Amodei, S. Ananthanarayanan, R. Anubhai, and et al. Deep speech 2 : End-to-end speech recognition in english and mandarin. In _Proceedings of the 33rd International Conference on Machine Learning_, pages 173-182, 2016.
* Auer et al. (2002) P. Auer, N. Cesa-Bianchi, Y. Freund, and R. E. Schapire. The nonstochastic multiarmed bandit problem. _SIAM Journal on Computing_, 32(1):48-77, 2002.
* Becker and Kohavi (1996) B. Becker and R. Kohavi. Adult. UCI Machine Learning Repository, 1996. DOI: https://doi.org/10.24432/C5XW20.
* Ben-Tal et al. (2009) A. Ben-Tal, L. E. Ghaoui, and A. Nemirovski. _Robust Optimization_. Princeton University Press, 2009.
* Ben-Tal et al. (2013) A. Ben-Tal, D. den Hertog, A. D. Waegenaere, B. Melenberg, and G. Rennen. Robust solutions of optimization problems affected by uncertain probabilities. _Management Science_, 59(2):341-357, 2013.
* Ben-Tal et al. (2015) A. Ben-Tal, E. Hazan, T. Koren, and S. Mannor. Oracle-based robust optimization via online learning. _Operations Research_, 63(3):628-638, 2015.
* Bertsimas et al. (2018) D. Bertsimas, V. Gupta, and N. Kallus. Robust sample average approximation. _Mathematical Programming_, 171:217-282, 2018.
* Blum et al. (2017) A. Blum, N. Haghtalab, A. D. Procaccia, and M. Qiao. Collaborative PAC learning. In _Advances in Neural Information Processing Systems 30_, pages 2389-2398, 2017.
* Bubeck and Cesa-Bianchi (2012) S. Bubeck and N. Cesa-Bianchi. Regret analysis of stochastic and nonstochastic multi-armed bandit problems. _Foundations and Trends in Machine Learning_, 5(1):1-122, 2012.
* Carmon and Hausler (2022) Y. Carmon and D. Hausler. Distributionally robust optimization via ball oracle acceleration. In _Advances in Neural Information Processing Systems 35_, pages 35866-35879, 2022.
* Cesa-Bianchi and Lugosi (2006) N. Cesa-Bianchi and G. Lugosi. _Prediction, Learning, and Games_. Cambridge University Press, 2006.
* Cotter et al. (2011) A. Cotter, O. Shamir, N. Srebro, and K. Sridharan. Better mini-batch algorithms via accelerated gradient methods. In _Advances in Neural Information Processing Systems 24_, pages 1647-1655, 2011.
* Delage and Ye (2010) E. Delage and Y. Ye. Distributionally robust optimization under moment uncertainty with application to data-driven problems. _Operations Research_, 58(3):595-612, 2010.
* 1406, 2021.
* Duchi et al. (2021) J. C. Duchi, P. W. Glynn, and H. Namkoong. Statistics of robust optimization: A generalized empirical likelihood approach. _Mathematics of Operations Research_, 46(3):946-969, 2021.
* Esfahani and Kuhn (2018) P. M. Esfahani and D. Kuhn. Data-driven distributionally robust optimization using the Wasserstein metric: performance guarantees and tractable reformulations. _Mathematical Programming_, 171:115-166, 2018.
* Esfahani et al. (2018)A. D. Flaxman, A. T. Kalai, and H. B. McMahan. Online convex optimization in the bandit setting: Gradient descent without a gradient. In _Proceedings of the 16th Annual ACM-SIAM Symposium on Discrete Algorithms_, pages 385-394, 2005.
* Freund and Schapire (1997) Y. Freund and R. E. Schapire. A decision-theoretic generalization of on-line learning and an application to boosting. _Journal of Computer and System Sciences_, 55(1):119-139, 1997.
* Freund and Schapire (1999) Y. Freund and R. E. Schapire. Adaptive game playing using multiplicative weights. _Games and Economic Behavior_, 29(1):79-103, 1999.
* Haghtalab et al. (2022) N. Haghtalab, M. I. Jordan, and E. Zhao. On-demand sampling: Learning optimally from multiple distributions. In _Advances in Neural Information Processing Systems 35_, pages 406-419, 2022.
* Haghtalab et al. (2023) N. Haghtalab, M. I. Jordan, and E. Zhao. On-demand sampling: Learning optimally from multiple distributions. _ArXiv e-prints_, arXiv:2210.12529v2, 2023.
* Hashimoto et al. (2018) T. Hashimoto, M. Srivastava, H. Namkoong, and P. Liang. Fairness without demographics in repeated loss minimization. In _Proceedings of the 35th International Conference on Machine Learning_, pages 1929-1938, 2018.
* Hu et al. (2018) W. Hu, G. Niu, I. Sato, and M. Sugiyama. Does distributionally robust supervised learning give robust classifiers? In _Proceedings of the 35th International Conference on Machine Learning_, volume 80, pages 2029-2037, 2018.
* Jin et al. (2021) J. Jin, B. Zhang, H. Wang, and L. Wang. Non-convex distributionally robust optimization: Non-asymptotic analysis. In _Advances in Neural Information Processing Systems 34_, pages 2771-2782, 2021.
* Juditsky et al. (2011) A. Juditsky, A. Nemirovski, and C. Tauvel. Solving variational inequalities with stochastic mirror-prox algorithm. _Stochastic Systems_, 1(1):17-58, 2011.
* Juditsky and Nemirovski (2008) A. B. Juditsky and A. S. Nemirovski. Large deviations of vector-valued martingales in 2-smooth normed spaces. _ArXiv e-prints_, arXiv:0809.0813, 2008.
* Kocak et al. (2014) T. Kocak, G. Neu, M. Valko, and R. Munos. Efficient learning by implicit exploration in bandit problems with side observations. In _Advances in Neural Information Processing Systems 27_, pages 613-621, 2014.
* Kuhn et al. (2019) D. Kuhn, P. M. Esfahani, V. A. Nguyen, and S. Shafieezadeh-Abadeh. Wasserstein distributionally robust optimization: Theory and applications in machine learning. _Operations Research & Management Science in the Age of Analytics_, pages 130-166, 2019.
* Kushner and Yin (2003) H. J. Kushner and G. G. Yin. _Stochastic Approximation and Recursive Algorithms and Applications_. Springer, second edition, 2003.
* Lan (2012) G. Lan. An optimal method for stochastic composite optimization. _Mathematical Programming_, 133:365-397, 2012.
* Lattimore and Szepesvari (2020) T. Lattimore and C. Szepesvari. _Bandit Algorithms_. Cambridge University Press, 2020.
* Levy et al. (2020) D. Levy, Y. Carmon, J. C. Duchi, and A. Sidford. Large-scale methods for distributionally robust optimization. In _Advances in Neural Information Processing Systems 33_, pages 8847-8860, 2020.
* Mohri et al. (2019) M. Mohri, G. Sivek, and A. T. Suresh. Agnostic federated learning. In _Proceedings of the 36th International Conference on Machine Learning_, pages 4615-4625, 2019.
* Namkoong and Duchi (2016) H. Namkoong and J. C. Duchi. Stochastic gradient methods for distributionally robust optimization with \(f\)-divergences. In _Advances in Neural Information Processing Systems 29_, pages 2216-2224, 2016.
* Namkoong and Duchi (2017) H. Namkoong and J. C. Duchi. Variance-based regularization with convex objectives. In _Advances in Neural Information Processing Systems 30_, pages 2971-2980, 2017.
* Namkoong et al. (2018)* Nemirovski (2004) A. Nemirovski. Prox-method with rate of convergence \(O(1/t)\) for variational inequalities with lipschitz continuous monotone operators and smooth convex-concave saddle point problems. _SIAM Journal on Optimization_, 15(1):229-251, 2004.
* Nemirovski et al. (2009) A. Nemirovski, A. Juditsky, G. Lan, and A. Shapiro. Robust stochastic approximation approach to stochastic programming. _SIAM Journal on Optimization_, 19(4):1574-1609, 2009.
* Neu (2015) G. Neu. Explore no more: Improved high-probability regret bounds for non-stochastic bandits. In _Advances in Neural Information Processing Systems 28_, pages 3168-3176, 2015.
* Nguyen and Zakynthinou (2018) H. L. Nguyen and L. Zakynthinou. Improved algorithms for collaborative PAC learning. In _Advances in Neural Information Processing Systems 31_, pages 7642-7650, 2018.
* Orabona (2019) F. Orabona. A modern introduction to online learning. _ArXiv e-prints_, arXiv:1912.13213(v5), 2019.
* Oren et al. (2019) Y. Oren, S. Sagawa, T. B. Hashimoto, and P. Liang. Distributionally robust language modeling. In _Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing_, pages 4227-4237, 2019.
* Qi et al. (2021) Q. Qi, Z. Guo, Y. Xu, R. Jin, and T. Yang. An online method for a class of distributionally robust optimization with non-convex objectives. In _Advances in Neural Information Processing Systems 34_, pages 10067-10080, 2021.
* Radivojac et al. (2004) P. Radivojac, N. V. Chawla, A. K. Dunker, and Z. Obradovic. Classification and knowledge discovery in protein databases. _Journal of Biomedical Informatics_, 37(4):224-239, 2004.
* Rafique et al. (2022) H. Rafique, M. Liu, Q. Lin, and T. Yang. Weakly-convex-concave min-max optimization: Provable algorithms and applications in machine learning. _Optimization Methods and Software_, 37(3):1087-1121, 2022.
* Rakhlin and Sridharan (2013) S. Rakhlin and K. Sridharan. Optimization, learning, and games with predictable sequences. In _Advances in Neural Information Processing Systems 26_, pages 3066-3074, 2013.
* Rothblum and Yona (2021) G. N. Rothblum and G. Yona. Multi-group agnostic PAC learnability. In _Proceedings of the 38th International Conference on Machine Learning_, pages 9107-9115, 2021.
* Roux et al. (2008) N. L. Roux, P.-A. Manzagol, and Y. Bengio. Topmoumoute online natural gradient algorithm. In _Advances in Neural Information Processing Systems 20_, pages 849-856, 2008.
* Sagawa et al. (2020) S. Sagawa, P. W. Koh, T. B. Hashimoto, and P. Liang. Distributionally robust neural networks for group shifts: On the importance of regularization for worst-case generalization. In _International Conference on Learning Representations_, 2020.
* Samuel and Chechik (2021) D. Samuel and G. Chechik. Distributional robustness loss for long-tail learning. In _Proceedings of the 2021 IEEE/CVF International Conference on Computer Vision_, pages 9475-9484, 2021.
* Scarf (1958) H. Scarf. A min-max solution of an inventory problem. _Studies in the Mathematical Theory of Inventory and Production_, pages 201-209, 1958.
* Shapiro (2017) A. Shapiro. Distributionally robust stochastic programming. _SIAM Journal on Optimization_, 27(4):2258-2275, 2017.
* Sinha et al. (2018) A. Sinha, H. Namkoong, and J. Duchi. Certifying some distributional robustness with principled adversarial training. In _International Conference on Learning Representations_, 2018.
* Slowik and Bottou (2022) A. Slowik and L. Bottou. On distributionally robust optimization and data rebalancing. In _Proceedings of the 25th International Conference on Artificial Intelligence and Statistics_, pages 1283-1297, 2022.
* Soma et al. (2022) T. Soma, K. Gatmiry, and S. Jegelka. Optimal algorithms for group distributionally robust optimization and beyond. _ArXiv e-prints_, arXiv:2212.13669, 2022.
* Syrgkanis et al. (2015) V. Syrgkanis, A. Agarwal, H. Luo, and R. E. Schapire. Fast convergence of regularized learning in games. In _Advances in Neural Information Processing Systems 28_, pages 2989-2997, 2015.
* Soma et al. (2018)V. N. Vapnik. _The Nature of Statistical Learning Theory_. Springer, second edition, 2000.
* Vershynin [2018] R. Vershynin. _High-Dimensional Probability: An Introduction with Applications in Data Science_. Cambridge University Press, 2018.
* Wang et al. [2021] J. Wang, R. Gao, and Y. Xie. Sinkhorn distributionally robust optimization. _ArXiv e-prints_, arXiv:2109.11926, 2021.
* Xu et al. [2020] Z. Xu, C. Dan, J. Khim, and P. Ravikumar. Class-weighted classification: Trade-offs and robust approaches. In _Proceedings of the 37th International Conference on Machine Learning_, pages 10544-10554, 2020.
* Zhang et al. [2021] J. Zhang, A. K. Menon, A. Veit, S. Bhojanapalli, S. Kumar, and S. Sra. Coping with label shift via distributionally robust optimisation. In _International Conference on Learning Representations_, 2021.
* Zhang et al. [2013] L. Zhang, T. Yang, R. Jin, and X. He. \(O(\log T)\) projections for stochastic optimization of smooth and strongly convex functions. In _Proceedings of the 30th International Conference on Machine Learning_, pages 1121-1129, 2013.
* Zhou [2023] Z.-H. Zhou. A theoretical perspective of machine learning with computational resource concerns. _ArXiv e-prints_, arXiv:2305.02217, 2023.

Related Work

Distributionally robust optimization (DRO) stems from the pioneering work of Scarf (1958), and has gained a lot of interest with the advancement of robust optimization (Ben-Tal et al., 2009, 2015). It has been successfully applied to a variety of machine learning tasks, including adversarial training (Sinha et al., 2018), algorithmic fairness (Hashimoto et al., 2018), class imbalance (Xu et al., 2020), long-tail learning (Samuel and Chechik, 2021), label shift (Zhang et al., 2021), etc.

In general, DRO is formulated to reflect our uncertainty about the target distribution. To ensure good performance under distribution perturbations, it minimizes the risk w.r.t. the worst distribution in an uncertainty set, i.e.,

\[\min_{\mathbf{w}\in\mathcal{W}}\ \sup_{\mathcal{P}\in\mathcal{S}( \mathcal{P}_{0})}\left\{\mathrm{E}_{\mathbf{z}\sim\mathcal{P}}\big{[}\ell( \mathbf{w};\mathbf{z})\big{]}\right\}\] (33)

where \(\mathcal{S}(\mathcal{P}_{0})\) denotes a set of probability distributions around \(\mathcal{P}_{0}\). In the literature, there mainly exist three ways to construct \(\mathcal{S}(\mathcal{P}_{0})\): (i) enforcing moment constraints (Delage and Ye, 2010), (ii) defining a neighborhood around \(\mathcal{P}_{0}\) by a distance function such as the \(f\)-divergence (Ben-Tal et al., 2013), the Wasserstein distance (Kuhn et al., 2019), and the Sinkhorn distance (Wang et al., 2021), and (iii) hypothesis testing of goodness-of-fit (Bertsimas et al., 2018).

By drawing a set of samples from \(\mathcal{P}_{0}\), we can also define an empirical DRO problem, which can be regarded as an SAA approach for solving (33). When the uncertainty set is defined in terms of the Cressie-Read family of \(f\)-divergences, Duchi and Namkoong (2021) have studied finite sample and asymptotic properties of the empirical solution. Besides, it has been proved that empirical DRO can also benefit the risk minimization problem in (1). Namkoong and Duchi (2017) show that empirical DRO with the \(\chi^{2}\)-divergence has the effect of variance regularization, leading to better generalization w.r.t. distribution \(\mathcal{P}_{0}\). Later, Duchi et al. (2021) demonstrate similar behaviors for the \(f\)-divergence constrained neighborhood, and provide one- and two-sided confidence intervals for the minimal risk in (1). Based on the Wasserstein distance, Esfahani and Kuhn (2018) establish an upper confidence bound on the risk of the empirical solution.

Since (33) is more complex than (1), considerable research efforts were devoted to develop efficient algorithms for DRO and its empirical version. For \(\mathcal{P}_{0}\) with finite support, Ben-Tal et al. (2013, Corollary 3) have demonstrated that (33) with \(f\)-divergences is equivalent to a convex optimization problem, provided that the loss \(\ell(\mathbf{w};\mathbf{z})\) is convex in \(\mathbf{w}\). Actually, this conclusion is true even when \(\mathcal{P}_{0}\) is continuous (Shapiro, 2017, SS3.2). Under mild assumptions, Esfahani and Kuhn (2018) show that DRO problems over Wasserstein balls can be reformulated as finite convex programs--in some cases even as linear programs. Besides the constrained formulation in (33), there also exists a penalized (or regularized) form of DRO (Sinha et al., 2018), which makes the optimization problem more tractable. In the past years, a series of SA methods have been proposed for empirical DRO with convex losses (Namkoong and Duchi, 2016), and DRO with convex loss (Levy et al., 2020) and non-convex losses (Jin et al., 2021, Qi et al., 2021, Rafique et al., 2022).

The main focus of this paper is the GDRO problem in (2)/(3), instead of the traditional DRO in (33). Sagawa et al. (2020) have applied SMD (Nemirovski et al., 2009) to (3), but only obtain a sub-optimal sample complexity of \(O(m^{2}(\log m)/\epsilon^{2})\), because of the large variance in their gradients. In the sequel, Haghtalab et al. (2022) and Soma et al. (2022) have tried to improve the sample complexity by reusing samples and applying techniques from MAB respectively, but their analysis suffers dependency issues. Carmon and Hausler (2022, Proposition 2) successfully established an \(O(m(\log m)/\epsilon^{2})\) sample complexity by combining SMD and gradient clipping, but their result holds only in expectation. To deal with heterogeneous noise in different distributions, Agarwal and Zhang (2022) propose a variant of GDRO named as minimax regret optimization (MRO), which replaces the risk \(R_{i}(\mathbf{w})\) with "excess risk" \(R_{i}(\mathbf{w})-\min_{\mathbf{w}\in\mathcal{W}}R_{i}(\mathbf{w})\). More generally, we can introduce calibration terms in DRO to prevent any single distribution to dominate the maximum (Slowik and Bottou, 2022).

Finally, we note that GDRO has a similar spirit with collaborative PAC learning (Blum et al., 2017, Nguyen and Zakynthinou, 2018, Rothblum and Yona, 2021) in the sense that both aim to find a single model that performs well on multiple distributions.

Analysis

In this section, we present proofs of main theorems.

### Proof of Theorem 1

The proof is based on Lemma 3.1 and Proposition 3.2 of Nemirovski et al. (2009). To apply them, we show that their preconditions are satisfied under our assumptions.

Although two instances of SMD are invoked to update \(\mathbf{w}\) and \(\mathbf{q}\) separately, they can be merged as \(1\) instance by concatenating \(\mathbf{w}\) and \(\mathbf{q}\) as a single variable \([\mathbf{w};\mathbf{q}]\in\mathcal{W}\times\Delta_{m}\), and redefine the norm and the distance-generating function (Nemirovski et al., 2009, SS3.1). Let \(\mathcal{E}\) be the space that \(\mathcal{W}\) lies in. We equip the Cartesian product \(\mathcal{E}\times\mathbb{R}^{m}\) with the following norm and dual norm:

\[\big{\|}[\mathbf{w};\mathbf{q}]\big{\|}=\sqrt{\frac{1}{2D^{2}}\|\mathbf{w}\|_{w }^{2}+\frac{1}{2\ln m}\|\mathbf{q}\|_{1}^{2}},\text{ and }\big{\|}[\mathbf{u};\mathbf{v}] \big{\|}_{*}=\sqrt{2D^{2}\|\mathbf{u}\|_{w,*}^{2}+2\|\mathbf{v}\|_{\infty}^{2 }\ln m}.\] (34)

We use the notation \(\mathbf{x}=[\mathbf{w};\mathbf{q}]\), and equip the set \(\mathcal{W}\times\Delta_{m}\) with the distance-generating function

\[\nu(\mathbf{x})=\nu([\mathbf{w};\mathbf{q}])=\frac{1}{2D^{2}}\nu_{w}(\mathbf{ w})+\frac{1}{2\ln m}\nu_{q}(\mathbf{q}).\] (35)

It is easy to verify that \(\nu(\mathbf{x})\) is \(1\)-strongly convex w.r.t. the norm \(\|\cdot\|\). Let \(B(\cdot,\cdot)\) be the Bregman distance associated with \(\nu(\cdot)\):

\[\begin{split} B(\mathbf{x},\mathbf{x}^{\prime})=& \nu(\mathbf{x})-\big{[}\nu(\mathbf{x}^{\prime})+\langle\nabla\nu( \mathbf{x}^{\prime}),\mathbf{x}-\mathbf{x}^{\prime}\rangle\big{]}\\ =&\frac{1}{2D^{2}}\left(\nu_{w}(\mathbf{w})-\big{[} \nu_{w}(\mathbf{w}^{\prime})+\langle\nabla\nu_{w}(\mathbf{w}^{\prime}), \mathbf{w}-\mathbf{w}^{\prime}\rangle\big{]}\right)\\ &+\frac{1}{2\ln m}\left(\nu_{q}(\mathbf{q})-\big{[}\nu_{q}( \mathbf{q}^{\prime})+\langle\nabla\nu_{q}(\mathbf{q}^{\prime}),\mathbf{q}- \mathbf{q}^{\prime}\rangle\big{]}\right)\\ =&\frac{1}{2D^{2}}B_{w}(\mathbf{w},\mathbf{w}^{ \prime})+\frac{1}{2\ln m}B_{q}(\mathbf{q},\mathbf{q}^{\prime})\end{split}\] (36)

where \(\mathbf{x}^{\prime}=[\mathbf{w}^{\prime};\mathbf{q}^{\prime}]\).

Then, we consider the following version of SMD for updating \(\mathbf{x}_{t}\):

\[\mathbf{x}_{t+1}=\operatorname*{argmin}_{\mathbf{x}\in\mathcal{W}\times \Delta_{m}}\left\{\eta/\big{[}\mathbf{g}_{w}(\mathbf{w}_{t},\mathbf{q}_{t});- \mathbf{g}_{q}(\mathbf{w}_{t},\mathbf{q}_{t})],\mathbf{x}-\mathbf{x}_{t} \big{\rangle}+B(\mathbf{x},\mathbf{x}_{t})\right\}\] (37)

where \(\eta>0\) is the step size. In the beginning, we set \(\mathbf{x}_{1}=\operatorname*{argmin}_{\mathbf{x}\in\mathcal{W}\times\Delta_{ m}}\nu(\mathbf{x})=[\mathbf{w}_{1};\mathbf{q}_{1}]\). From the decomposition of the Bregman distance in (36), we observe that (37) is equivalent to (11) and (12) by setting

\[\eta_{w}=2\eta D^{2},\text{ and }\eta_{q}=2\eta\ln m.\]

Next, we show that the stochastic gradients are well-bounded. Under our assumptions, we have

\[\|\mathbf{g}_{w}(\mathbf{w}_{t},\mathbf{q}_{t})\|_{w,*}=\left\| \sum_{i=1}^{m}q_{t,i}\nabla\ell(\mathbf{w}_{t};\mathbf{z}_{t}^{(i)})\right\|_{w,*}\leq\sum_{i=1}^{m}q_{t,i}\left\|\nabla\ell(\mathbf{w}_{t};\mathbf{z}_{t}^{ (i)})\right\|_{w,*}\stackrel{{(\ref{eq:Bregman_update_eq})}}{{ \leq}}\sum_{i=1}^{m}q_{t,i}G=G,\] \[\|\mathbf{g}_{q}(\mathbf{w}_{t},\mathbf{q}_{t})\|_{\infty}=\big{\|} [\ell(\mathbf{w}_{t};\mathbf{z}_{t}^{(1)}),\ldots,\ell(\mathbf{w}_{t};\mathbf{ z}_{t}^{(m)})]^{\top}\big{\|}_{\infty}^{\eqref{eq:Bregman_update_eq}}.\]

As a result, the concatenated gradients used in (37) is also bounded in term of the dual norm \(\|\cdot\|_{*}\):

\[\big{\|}[\mathbf{g}_{w}(\mathbf{w}_{t},\mathbf{q}_{t});-\mathbf{g }_{q}(\mathbf{w}_{t},\mathbf{q}_{t})]\big{\|}_{*}= \sqrt{2D^{2}\|\mathbf{g}_{w}(\mathbf{w}_{t},\mathbf{q}_{t})\|_{ w,*}^{2}+2\|\mathbf{g}_{q}(\mathbf{w}_{t},\mathbf{q}_{t})\|_{\infty}^{2}\ln m}\] \[\leq \underbrace{\sqrt{2D^{2}G^{2}+2\ln m}}_{:=M}.\]

Now, we are ready to state our theoretical guarantees. By setting

\[\eta=\frac{2}{M\sqrt{5T}}=\sqrt{\frac{2}{5T(D^{2}G^{2}+\ln m)}},\]Lemma 3.1 and (3.13) of Nemirovski et al. (2009) imply that

\[\mathrm{E}\big{[}\epsilon_{\phi}(\bar{\mathbf{w}},\bar{\mathbf{q}})\big{]}\leq 2 M\sqrt{\frac{5}{T}}=2\sqrt{\frac{10(D^{2}G^{2}+\ln m)}{T}}\]

Furthermore, from Proposition 3.2 of Nemirovski et al. (2009), we have, for any \(\Omega>1\)

\[\mathrm{Pr}\left[\epsilon_{\phi}(\bar{\mathbf{w}},\bar{\mathbf{q}})\geq(8+2 \Omega)M\sqrt{\frac{5}{T}}=(8+2\Omega)\sqrt{\frac{10(D^{2}G^{2}+\ln m)}{T}} \right]\leq 2\exp(-\Omega).\]

We complete the proof by setting \(\delta=2\exp(-\Omega)\).

### Proof of Theorem 2

We first bound the regret of the 1st player. In the analysis, we address the non-obliviousness by the "ghost iterate" technique of Nemirovski et al. (2009).

**Theorem 5**: _Under Assumptions 1, 2 and 4, by setting \(\eta_{w}=\frac{2D}{G\sqrt{5T}}\), we have_

\[\mathrm{E}\left[\sum_{t=1}^{T}\phi(\mathbf{w}_{t},\mathbf{q}_{t})-\min_{ \mathbf{w}\in\mathcal{W}}\sum_{t=1}^{T}\phi(\mathbf{w},\mathbf{q}_{t})\right] \leq 2DG\sqrt{5T}\]

_and with probability at least \(1-\delta\),_

\[\sum_{t=1}^{T}\phi(\mathbf{w}_{t},\mathbf{q}_{t})-\min_{\mathbf{w}\in \mathcal{W}}\sum_{t=1}^{T}\phi(\mathbf{w},\mathbf{q}_{t})\leq DG\sqrt{T}\left( 2\sqrt{5}+8\sqrt{\ln\frac{1}{\delta}}\right).\]

By extending Exp3-IX to stochastic rewards, we have the following bound for the 2nd player.

**Theorem 6**: _Under Assumption 3, by setting \(\eta_{q}=\sqrt{\frac{\ln m}{mT}}\) and the IX coefficient \(\gamma=\frac{\eta_{q}}{2}\), we have_

\[\mathrm{E}\left[\max_{q\in\Delta_{m}}\sum_{t=1}^{T}\phi(\mathbf{w}_{t}, \mathbf{q})-\sum_{t=1}^{T}\phi(\mathbf{w}_{t},\mathbf{q}_{t})\right]\leq 3 \sqrt{mT\ln m}+\sqrt{\frac{T}{2}}+3\left(\sqrt{\frac{mT}{\ln m}}+\sqrt{\frac{T }{2}}+1\right)\]

_and with probability at least \(1-\delta\),_

\[\max_{q\in\Delta_{m}}\sum_{t=1}^{T}\phi(\mathbf{w}_{t},\mathbf{q})-\sum_{t=1} ^{T}\phi(\mathbf{w}_{t},\mathbf{q}_{t})\leq 3\sqrt{mT\ln m}+\sqrt{\frac{T}{2}}+ \left(\sqrt{\frac{mT}{\ln m}}+\sqrt{\frac{T}{2}}+1\right)\ln\frac{3}{\delta}.\]

From Jensen's inequality and the outputs \(\bar{\mathbf{w}}=\frac{1}{T}\sum_{t=1}^{T}\mathbf{w}_{t}\) and \(\bar{\mathbf{q}}=\frac{1}{T}\sum_{t=1}^{T}\mathbf{q}_{t}\), we have

\[\begin{split}&\epsilon_{\phi}(\bar{\mathbf{w}},\bar{\mathbf{q}})= \max_{\mathbf{q}\in\Delta_{m}}\phi(\bar{\mathbf{w}},\mathbf{q})-\min_{\mathbf{ w}\in\mathcal{W}}\phi(\mathbf{w},\bar{\mathbf{q}})\\ \leq&\frac{1}{T}\left(\max_{\mathbf{q}\in\Delta_{m} }\sum_{t=1}^{T}\phi(\mathbf{w}_{t},\mathbf{q})-\min_{\mathbf{w}\in\mathcal{W}} \sum_{t=1}^{T}\phi(\mathbf{w},\mathbf{q}_{t})\right)\\ =&\frac{1}{T}\left(\max_{\mathbf{q}\in\Delta_{m} }\sum_{t=1}^{T}\phi(\mathbf{w}_{t},\mathbf{q})-\sum_{t=1}^{T}\phi(\mathbf{w}_ {t},\mathbf{q}_{t})\right)+\frac{1}{T}\left(\sum_{t=1}^{T}\phi(\mathbf{w}_{t},\mathbf{q}_{t})-\min_{\mathbf{w}\in\mathcal{W}}\sum_{t=1}^{T}\phi(\mathbf{w}, \mathbf{q}_{t})\right).\end{split}\] (38)

We obtain (19) by substituting the high probability bound in Theorems 5 and 6 into (38), and taking the union bound.

### Proof of Theorem 5

Our goal is to analyze SMD for non-oblivious OCO with stochastic gradients. In the literature, we did not find a convenient reference for it. A very close one is the Lemma 3.2 of Flaxman et al. (2005), which bounds the expected regret of SGD for non-oblivious OCO. But it is insufficient for our purpose, so we provide our proof by following the analysis of SMD for stochastic convex-concave

[MISSING_PAGE_FAIL:18]

[MISSING_PAGE_EMPTY:19]

[MISSING_PAGE_FAIL:20]

[MISSING_PAGE_FAIL:21]

[MISSING_PAGE_FAIL:22]

From Assumption 4, we can prove that each risk function \(R_{i}(\cdot)\) is \(G\)-Lipschitz continuous. To see this, we have

\[\|\nabla R_{i}(\mathbf{w})\|_{w,*}=\|\mathrm{E}_{\mathbf{z}\sim\mathcal{P}_{i}} \nabla\ell(\mathbf{w};\mathbf{z})\|_{w,*}\leq\mathrm{E}_{\mathbf{z}\sim \mathcal{P}_{i}}\|\nabla\ell(\mathbf{w};\mathbf{z})\big{\|}_{w,*}\overset{\eqref {eq:R_1}}{\leq}G,\ \forall\mathbf{w}\in\mathcal{W},i\in[m].\] (60)

As a result, we have

\[|R_{i}(\mathbf{w})-R_{i}(w^{\prime})|\leq G\|\mathbf{w}-\mathbf{w}^{\prime}\| _{w},\ \forall\mathbf{w},\mathbf{w}^{\prime}\in\mathcal{W},i\in[m].\] (61)

Furthermore, the difference between the gradient of \(R_{i}(\cdot)\) and its estimator is also well-bounded, i.e., for all \(i\in[m]\),

\[\|\nabla R_{i}(\mathbf{w})-\nabla\ell(\mathbf{w};\mathbf{z})\|_{w,*}\leq\| \nabla R_{i}(\mathbf{w})\|_{w,*}+\|\nabla\ell(\mathbf{w};\mathbf{z})\|_{w,*} \overset{\eqref{eq:R_1},\eqref{eq:R_1}}{\leq}2G,\ \forall\mathbf{w}\in\mathcal{W}, \mathbf{z}\sim\mathcal{P}_{i}.\] (62)

Recall the definition of the norm \(\|\cdot\|\) and dual norm \(\|\cdot\|_{*}\) for the space \(\mathcal{E}\times\mathbb{R}^{m}\) in (34), and the distance-generating function \(\nu(\cdot)\) in (35). Following the arguments in Section B.1, the two updating rules in (25) and (26) can be merged as

\[[\mathbf{w}_{t+1};\mathbf{q}_{t+1}]=\operatorname*{argmin}_{\mathbf{x}\in \mathcal{W}\times\Delta_{m}}\Big{\{}\eta\big{\langle}[\mathbf{g}_{w}(\mathbf{ w}_{t}^{\prime},\mathbf{q}_{t}^{\prime});-\mathbf{g}_{q}(\mathbf{w}_{t}^{\prime}, \mathbf{q}_{t}^{\prime})],\mathbf{x}-[\mathbf{w}_{t}^{\prime};\mathbf{q}_{t}^ {\prime}]\big{\rangle}+B(\mathbf{x},[\mathbf{w}_{t}^{\prime};\mathbf{q}_{t}^{ \prime}])\Big{\}}\] (63)

where \(\eta_{w}=2\eta D^{2}\) and \(\eta_{q}=2\eta\ln m\). Similarly, (28) and (29) are equivalent to

\[[\mathbf{w}_{t+1}^{\prime};\mathbf{q}_{t+1}^{\prime}]=\operatorname*{argmin}_{ \mathbf{x}\in\mathcal{W}\times\Delta_{m}}\Big{\{}\eta\big{\langle}[\mathbf{g}_ {w}(\mathbf{w}_{t+1},\mathbf{q}_{t+1});-\mathbf{g}_{q}(\mathbf{w}_{t+1}, \mathbf{q}_{t+1})],\mathbf{x}-[\mathbf{w}_{t}^{\prime};\mathbf{q}_{t}^{\prime} ]\big{\rangle}+B(\mathbf{x},[\mathbf{w}_{t}^{\prime};\mathbf{q}_{t}^{\prime}] )\Big{\}}.\] (64)

Let \(F([\mathbf{w};\mathbf{q}])\) be the monotone operator associated with the weighted GDRO problem in (23), i.e.,

\[\begin{split}& F([\mathbf{w};\mathbf{q}])=[\nabla_{\mathbf{w}} \varphi(\mathbf{w},\mathbf{q});-\nabla_{\mathbf{q}}\varphi(\mathbf{w}, \mathbf{q})]\\ =&\left[\sum_{i=1}^{m}q_{i}p_{i}\nabla R_{i}(\mathbf{ w});-\big{[}p_{1}R_{1}(\mathbf{w}),\ldots,p_{m}R_{m}(\mathbf{w})\big{]}^{\top} \right].\end{split}\] (65)

From our constructions of stochastic gradients in (24) and (27), we clearly have

\[\begin{split}&\mathrm{E}_{t-1}\left\{[\mathbf{g}_{w}(\mathbf{w}_{t}^ {\prime},\mathbf{q}_{t}^{\prime});-\mathbf{g}_{q}(\mathbf{w}_{t}^{\prime}, \mathbf{q}_{t}^{\prime})]\right\}=F([\mathbf{w}_{t}^{\prime};\mathbf{q}_{t}^{ \prime}]),\\ &\mathrm{E}_{t-1}\left\{[\mathbf{g}_{w}(\mathbf{w}_{t+1},\mathbf{ q}_{t+1});-\mathbf{g}_{q}(\mathbf{w}_{t+1},\mathbf{q}_{t+1})]\right\}=F([\mathbf{w}_{t+1}; \mathbf{q}_{t+1}]).\end{split}\]

Thus, Algorithm 4 is indeed an instance of SMPA (Juditsky et al., 2011, Algorithm 1), and we can use their Theorem 1 and Corollary 1 to bound the optimization error.

Before applying their results, we show that all the preconditions are satisfied. The parameter \(\Omega\) defined in (16) of Juditsky et al. (2011) can be upper bounded by

\[\begin{split}\Omega=&\sqrt{2\max_{\mathbf{x}\in \mathcal{W}\times\Delta_{m}}B(\mathbf{x},[\mathbf{w}_{1}^{\prime};\mathbf{q}_{1 }^{\prime}])}\overset{\eqref{eq:R_1}}{=}\sqrt{\frac{1}{D^{2}}\max_{\mathbf{w} \in\mathcal{W}}B_{w}(\mathbf{w}_{1},\mathbf{w}_{1}^{\prime})+\max_{\mathbf{q} \in\Delta_{m}}\frac{1}{\ln m}B_{q}(\mathbf{q},\mathbf{q}_{1}^{\prime})}\\ \overset{\eqref{eq:R_1}}{\leq}&\sqrt{\frac{1}{D^{2}} \left(\max_{\mathbf{w}\in\mathcal{W}}\nu_{w}(\mathbf{w})-\min_{\mathbf{w}\in \mathcal{W}}\nu_{w}(\mathbf{w})\right)+\frac{1}{\ln m}\left(\max_{\mathbf{q} \in\Delta_{m}}\nu_{q}(\mathbf{q})-\min_{\mathbf{q}\in\Delta_{m}}\nu_{q}( \mathbf{q})\right)}\overset{\eqref{eq:R_1}}{=}\sqrt{2}.\end{split}\] (66)

Next, we need to demonstrate that \(F([\mathbf{w};\mathbf{q}])\) is continuous.

**Lemma 3**: _For the monotone operator \(F([\mathbf{w};\mathbf{q}])\), we have_

\[\|F([\mathbf{w};\mathbf{q}])-F([\mathbf{w}^{\prime};\mathbf{q}^{\prime}])\|_{* }\leq\widetilde{L}\big{\|}[\mathbf{w}-\mathbf{w}^{\prime};\mathbf{q}-\mathbf{q} ^{\prime}]\big{\|}\]

_where \(\widetilde{L}\) is defined in (31)._

We proceed to show the variance of the stochastic gradients satisfies the light tail condition. To this end, we introduce the stochastic oracle used in Algorithm 4:

\[\mathbf{g}([\mathbf{w};\mathbf{q}])=[\mathbf{g}_{w}(\mathbf{w},\mathbf{q});- \mathbf{g}_{q}(\mathbf{w},\mathbf{q})]\] (67)where

\[\mathbf{g}_{w}(\mathbf{w},\mathbf{q}) =\sum_{i=1}^{m}q_{i}p_{i}\left(\frac{n_{m}}{n_{i}}\sum_{j=1}^{n_{i}/ n_{m}}\nabla\ell(\mathbf{w};\mathbf{z}^{(i,j)})\right),\] \[\mathbf{g}_{q}(\mathbf{w},\mathbf{q}) =\left[p_{1}\frac{n_{m}}{n_{1}}\sum_{j=1}^{n_{1}/n_{m}}\ell( \mathbf{w};\mathbf{z}^{(1,j)}),\ldots,p_{m}\ell(\mathbf{w};\mathbf{z}^{(m)}) \right]^{\top}\]

and \(\mathbf{z}^{(i,j)}\) is the \(j\)-th sample drawn from distribution \(\mathcal{P}_{i}\). The following lemma shows that the variance is indeed sub-Gaussian.

**Lemma 4**: _For the stochastic oracle \(\mathbf{g}([\mathbf{w};\mathbf{q}])\), we have_

\[\mathrm{E}\left[\exp\left(\frac{\|F([\mathbf{w};\mathbf{q}])-\mathbf{g}([ \mathbf{w};\mathbf{q}])\|_{*}^{2}}{\sigma^{2}}\right)\right]\leq 2\]

_where \(\sigma^{2}\) is defined in (31)._

Based on (66), Lemma 3, and Lemma 4, we can apply the theoretical guarantee of SMPA. Recall that the total number of iterations is \(n_{m}/2\) in Algorithm 4. From Corollary 1 of Juditsky et al. (2011), by setting

\[\eta=\min\left(\frac{1}{\sqrt{3}L},\frac{2}{\sqrt{7\sigma^{2}n_{m}}}\right)\]

we have

\[\Pr\left[\epsilon_{\varphi}(\bar{\mathbf{w}},\bar{\mathbf{q}})\geq\frac{7 \widetilde{L}}{n_{m}}+14\sqrt{\frac{2\sigma^{2}}{3n_{m}}}+7\Lambda\sqrt{\frac{ \sigma^{2}}{n_{m}}}\right]\leq\exp\left(-\frac{\Lambda^{2}}{3}\right)+\exp \left(-\frac{\Lambda n_{m}}{2}\right)\]

for all \(\Lambda>0\). Choosing \(\Lambda\) such that \(\exp(-\Lambda^{2}/3)\leq\delta/2\) and \(\exp(-\Lambda n_{m}/2)\leq\delta/2\), we have with probability at least \(1-\delta\)

\[\epsilon_{\varphi}(\bar{\mathbf{w}},\bar{\mathbf{q}})\leq\frac{7\widetilde{L} }{n_{m}}+14\sqrt{\frac{2\sigma^{2}}{3n_{m}}}+7\left(\sqrt{3\log\frac{2}{ \delta}}+\frac{2}{n_{m}}\log\frac{2}{\delta}\right)\sqrt{\frac{\sigma^{2}}{n_ {m}}}.\]

Following the derivation of (58), we have

\[R_{i}(\bar{\mathbf{w}})-\frac{1}{p_{i}}\min_{\mathbf{w}\in \mathcal{W}}\max_{i\in[m]}p_{i}R_{i}(\mathbf{w})\] (68) \[\leq \frac{1}{p_{i}}\left(\frac{7\widetilde{L}}{n_{m}}+\sqrt{\frac{ \sigma^{2}}{n_{m}}}\left(14\sqrt{\frac{2}{3}}+7\sqrt{3\log\frac{2}{\delta}}+ \frac{14}{n_{m}}\log\frac{2}{\delta}\right)\right).\]

Inspired by Juditsky et al. (2011, SS4.3.1), we use the value of \(p_{i}\) in (32) to simplify (68). It is easy to verify that

\[\frac{p_{\max}}{p_{i}}=\frac{1/\sqrt{n_{m}}+\sqrt{n_{m}/n_{i}}}{ 1/\sqrt{n_{m}}+\sqrt{n_{m}/n_{1}}}\leq\left(1+\frac{n_{m}}{\sqrt{n_{i}}}\right),\] \[\frac{1}{p_{i}}\frac{\widetilde{L}}{n_{m}}=O\left(\frac{p_{\max}} {p_{i}}\frac{\sqrt{\ln m}}{n_{m}}\right)=O\left(\left(\frac{1}{n_{m}}+\frac{ 1}{\sqrt{n_{i}}}\right)\sqrt{\ln m}\right),\] (69) \[p_{i}\leq\left(\frac{1}{\sqrt{n_{m}}}+1\right)\sqrt{\frac{n_{i}} {n_{m}}},\quad\omega_{\max}=\max_{i\in[m]}\frac{p_{i}^{2}n_{m}}{n_{i}}\leq \left(\frac{1}{\sqrt{n_{m}}}+1\right)^{2},\] \[\frac{1}{p_{i}}\sqrt{\omega_{\max}}=\frac{1/\sqrt{n_{m}}+\sqrt{ n_{m}/n_{i}}}{1/\sqrt{n_{m}}+1}\sqrt{\omega_{\max}}\leq\frac{1}{\sqrt{n_{m}}}+ \sqrt{\frac{n_{m}}{n_{i}}},\] \[\frac{1}{p_{i}}\sqrt{\frac{\sigma^{2}}{n_{m}}}=O\left(\frac{1}{p _{i}}\sqrt{\frac{\omega_{\max}(\kappa+\ln^{2}m)}{n_{m}}}\right)=O\left(\left( \frac{1}{n_{m}}+\frac{1}{\sqrt{n_{i}}}\right)\sqrt{\kappa+\ln^{2}m}\right).\] (70)Substituting (69) and (70) into (68), we have

\[R_{i}(\bar{\mathbf{w}})-\frac{1}{p_{i}}\min_{\mathbf{w}\in\mathcal{W}}\max_{i\in[ m]}p_{i}R_{i}(\mathbf{w})=O\left(\left(\frac{1}{n_{m}}+\frac{1}{\sqrt{n_{i}}} \right)\sqrt{\kappa+\ln^{2}m}\right).\]

## Appendix C Experiments

In this section, we conduct empirical studies to evaluate our proposed algorithms.

### Datasets

Following the setup in previous work (Namkoong and Duchi, 2016; Soma et al., 2022), we use both synthetic and real-world datasets. First, we construct a synthetic data with group number \(m=20\). For each group \(i\in[m]\), we draw a model \(\mathbf{w}_{i}^{*}\in\mathbb{R}^{1000}\) from the uniform distribution over the unit sphere. For distribution \(\mathcal{P}_{i}\), the sample \((\mathbf{x},y)\) is generated with \(\mathbf{x}\sim\mathcal{N}(0,I)\) and \(y=\mathrm{sign}(\mathbf{x}^{\top}\mathbf{w}_{i}^{*})\) with probability \(0.9\) and \(y=-\,\mathrm{sign}(\mathbf{x}^{\top}\mathbf{w}_{i}^{*})\) with probability \(0.1\).

We also use the Adult dataset (Becker and Kohavi, 1996), which includes attributes such as age, gender, race, and educational background of \(48842\) individuals. The objective is to determine whether an individual's income exceeds \(50000\) USD. We set up \(m=6\) groups based on the race and gender attributes, where each group represents a combination of {black, white, others} with {female, male}.

Figure 1: Balanced settings: max risk of different methods versus the number of iterations

Figure 2: Balanced settings: max risk of different methods versus the number of samples

### GDRO with Balanced Data

For experiments on the synthetic dataset, we will generate the random sample on the fly, according to the protocol above. For those on the Adult dataset, we will randomly select samples from ech group. In other words, \(\mathcal{P}_{i}\) is defined as the empirical distribution over the data in the \(i\)-th group.

We refer to the method of Sagawa et al. (2020) and our Algorithm 1 by SMD(\(1\)) and SMD(\(m\)) to underscore that they are instances of SMD with \(1\) sample and \(m\) samples in each iteration, respectively. We denote our Algorithm 2 as Online(\(1\)) to emphasize that it is based on techniques from online learning and uses \(1\) sample per iteration. We set \(\ell(\cdot;\cdot)\) to be the logistic loss and utilize different methods to train a linear model. In the balanced setting, we use the max risk, i.e., \(\max_{i\in[m]}R_{i}(\mathbf{w})\), as the performance measure. To estimate the risk value, we will draw a substantial number of samples, and use the empirical average to approximate the expectation.

We first report the max risk with respect to the number of iterations in Fig. 1. We observe that SMD(\(m\)) is faster than Online(\(1\)), which in turn outperforms SMD(\(1\)). This observation is consistent with our theories, since their convergence rates are \(O(\sqrt{(\log m)/T})\), \(O(\sqrt{m(\log m)/T})\), and \(O(m\sqrt{(\log m)/T})\), respectively. Next, we plot the max risk against the number of samples consumed by each algorithm in Fig. 2. As can be seen, the curves of SMD(\(m\)) and Online(\(1\)) are very close, indicating that they share the same sample complexity, i.e., \(O(m(\log m)/\epsilon^{2})\). By contrast, SMD(\(1\)) has a much higher sample complexity, i.e., \(O(m^{2}(\log m)/\epsilon^{2})\).

Figure 3: Imbalanced settings with the synthetic dataset: individual risk versus the number of iterations

### GDRO with Imbalanced Data

For experiments on the synthetic dataset, we set the number of samples as \(n_{i}=800\times(21-i)\), and generate each sample as before. For those on the Adult dataset, we first select \(364\) samples randomly from each group, reserving them for later use in estimating the risk of each group. Then, we visit the remaining samples in each group _once_ to simulate the imbalanced setting, where the numbers of samples in \(6\) groups are \(26656\), \(11518\), \(1780\), \(1720\), \(998\), and \(364\).

Similarly, we label the Baseline mentioned in the first paragraph of Section 3 as \(\text{SMD}(m)\). We designate our Algorithms 3 and 4 as \(\text{SMD}_{\text{r}}\) and \(\text{SMPA}_{\text{m}}\) to highlight that the former combines SMD with random sampling and the latter one integrates SMPA and mini-batches. In the imbalanced setting, we will examine how the risk on each individual distribution decreases with respect to the number of iterations. Recall that the total number of iterations of \(\text{SMD}(m)\), \(\text{SMD}_{\text{r}}\) and \(\text{SMPA}_{\text{m}}\) are \(n_{m}\), \(n_{1}\) and \(n_{m}/2\), respectively.

We present the risk on the individual distribution in Fig. 3 and Fig. 4. First, we observe that our \(\text{SMPA}_{\text{m}}\) is faster than both \(\text{SMD}(m)\) and \(\text{SMD}_{\text{r}}\) across all distributions, and finally attains the lower risk is most cases. This behavior aligns with our Theorem 4, which reveals that \(\text{SMPA}_{\text{m}}\) achieves a nearly optimal rate of \(O((\log m)/\sqrt{n_{i}})\) for all distributions \(\mathcal{P}_{i}\), after \(n_{m}/2\) iterations. We also note that on distribution \(\mathcal{P}_{1}\), although \(\text{SMD}_{\text{r}}\) converges slowly, its final risk is the lowest, as illustrated in Fig. 3(a) and Fig. 4(a). This phenomenon is again in accordance with our Theorem 3, which shows that the risk of \(\text{SMD}_{\text{r}}\) on \(\mathcal{P}_{1}\) reduces at a nearly optimal \(O(\sqrt{(\log m)/n_{1}})\) rate, after \(n_{1}\) iterations. From Fig. 3(i) and Fig. 4(f), we can see that the final risk of \(\text{SMD}(m)\) on the last distribution \(\mathcal{P}_{m}\) matches that of \(\text{SMPA}_{\text{m}}\). This outcome is anticipated, as they exhibit similar convergence rates of \(O(\sqrt{(\log m)/n_{m}})\) and \(O((\log m)/\sqrt{n_{m}})\), respectively.

## Appendix D Supporting Lemmas

### Proof of Lemma 2

The proof follows the argument of Neu (2015, Proof of Lemma 1), and we generalize it to the setting with stochastic rewards. First, observe that for any \(i\in[m]\) and \(t\in[T]\),

Figure 4: Imbalanced settings with the Adult dataset: individual risk versus the number of iterations\[\tilde{\xi}_{t,i} =\frac{\hat{\xi}_{t,i}}{p_{t,i}+\gamma_{t}}\cdot\mathbb{I}[i_{t}=i]\] \[\leq\frac{\hat{\xi}_{t,i}}{p_{t,i}+\gamma_{t}\hat{\xi}_{t,i}}\cdot \mathbb{I}[i_{t}=i] (\hat{\xi}_{t,i}\in[0,1])\] \[=\frac{1}{2\gamma_{t}}\frac{2\gamma_{t}\cdot\hat{\xi}_{t,i}/p_{t,i }}{1+\gamma_{t}\cdot\hat{\xi}_{t,i}/p_{t,i}}\cdot\mathbb{I}[i_{t}=i]\] \[\leq\frac{1}{\beta_{t}}\log\big{(}1+\beta_{t}\bar{\xi}_{t,i}\big{)}\] (71)

where the last step is due to the inequality \(\frac{z}{1+z/2}\leq\log(1+z)\) for \(z\geq 0\) and we introduce the notations \(\beta_{t}=2\gamma_{t}\) and \(\bar{\xi}_{t,i}=(\hat{\xi}_{t,i}/p_{t,i})\cdot\mathbb{I}[i_{t}=i]\) to simplify the presentation.

Define the notation \(\tilde{\lambda}_{t}=\sum_{i=1}^{m}\alpha_{t,i}\tilde{\xi}_{t,i}\) and \(\lambda_{t}=\sum_{i=1}^{m}\alpha_{t,i}\xi_{t,i}\). Then, we have

\[\mathrm{E}_{t-1}\left[\exp(\tilde{\lambda}_{t})\right] =\mathrm{E}_{t-1}\left[\exp\Big{(}\sum_{i=1}^{m}\alpha_{t,i}\tilde {\xi}_{t,i}\Big{)}\right]\] \[\stackrel{{\eqref{eq:E_t-1}}}{{\leq}}\mathrm{E}_{t- 1}\left[\exp\left(\sum_{i=1}^{m}\frac{\alpha_{t,i}}{\beta_{t}}\log\Big{(}1+ \beta_{t}\bar{\xi}_{t,i}\Big{)}\right)\right]\] \[\leq\mathrm{E}_{t-1}\left[\exp\left(\sum_{i=1}^{m}\log\Big{(}1+ \alpha_{t,i}\bar{\xi}_{t,i}\Big{)}\right)\right] (\tfrac{\alpha_{t,i}}{\beta_{t}}\leq 1\text{ by assumption})\] \[=\mathrm{E}_{t-1}\left[1+\sum_{i=1}^{m}\alpha_{t,i}\bar{\xi}_{t,i}\right]\] \[=1+\sum_{i=1}^{m}\alpha_{t,i}\xi_{t,i}\leq\exp\left(\sum_{i=1}^{ m}\alpha_{t,i}\xi_{t,i}\right)=\exp(\lambda_{t})\] (72)

where the second inequality is by the inequality \(x\log(1+y)\leq\log(1+xy)\) that holds for all \(y\geq-1\) and \(x\in[0,1]\), and the equality \(\mathrm{E}_{t-1}\left[\Pi_{i=1}^{m}\big{(}1+\alpha_{t,i}\bar{\xi}_{t,i}\big{)} \right]=\mathrm{E}_{t-1}\left[1+\sum_{i=1}^{m}\alpha_{t,i}\bar{\xi}_{t,i}\right]\) follows from the fact that \(\bar{\xi}_{t,i}\cdot\bar{\xi}_{t,j}=0\) holds whenever \(i\neq j\). The last line is due to \(\mathrm{E}_{t-1}[\bar{\xi}_{t,i}]=\mathrm{E}_{t-1}[(\hat{\xi}_{t,i}/p_{t,i}) \cdot\mathbb{I}[i_{t}=i]]=\xi_{t,i}\) and the inequality \(1+z\leq e^{z}\) for all \(z\in\mathbb{R}\).

As a result, from (72) we conclude that the process \(Z_{t}=\exp\left(\sum_{s=1}^{t}(\tilde{\lambda}_{s}-\lambda_{s})\right)\) is a supermartingale. Indeed, \(\mathrm{E}_{t-1}[Z_{t}]=\mathrm{E}_{t-1}\big{[}\exp\big{(}\sum_{s=1}^{t-1}( \tilde{\lambda}_{s}-\lambda_{s})\big{)}\cdot\exp(\tilde{\lambda}_{t}-\lambda _{t})\big{]}\leq Z_{t-1}\). Thus, we have \(\mathrm{E}[Z_{T}]\leq\mathrm{E}[Z_{T-1}\leq\ldots\leq\mathrm{E}[Z_{0}]=1\). By Markov's inequality,

\[\Pr\left[\sum_{t=1}^{T}(\tilde{\lambda}_{t}-\lambda_{t})>\epsilon\right]\leq \mathrm{E}\left[\exp\left(\sum_{t=1}^{T}(\tilde{\lambda}_{t}-\lambda_{t}) \right)\right]\cdot\exp(-\epsilon)\leq\exp(-\epsilon)\]

holds for any \(\epsilon>0\). By setting \(\exp(-\epsilon)=\delta\) and solving the value, we complete the proof for (54). And the inequality (55) for the scenario \(\gamma_{t}=\gamma\) can be immediately obtained by setting \(\alpha_{t,i}=2\gamma\) and taking the union bound over all \(i\in[m]\).

### Proof of Lemma 3

From the definition of norms in (34), we have

\[\|F([\mathbf{w};\mathbf{q}])-F([\mathbf{w}^{\prime};\mathbf{q}^{ \prime}])\|_{*}^{2}\] \[= \Bigg{\|}\left[\sum_{i=1}^{m}q_{i}p_{i}\nabla R_{i}(\mathbf{w})- \sum_{i=1}^{m}q_{i}^{\prime}p_{i}\nabla R_{i}(\mathbf{w}^{\prime});\left[p_{1} R_{1}(\mathbf{w}^{\prime})-p_{1}R_{1}(\mathbf{w}),\dots,p_{m}R_{m}(\mathbf{w}^{ \prime})-p_{m}R_{m}(\mathbf{w})\right]^{\top}\right]\right\|_{*}^{2}\] \[= 2D^{2}\left\|\sum_{i=1}^{m}q_{i}p_{i}\nabla R_{i}(\mathbf{w})- \sum_{i=1}^{m}q_{i}^{\prime}p_{i}\nabla R_{i}(\mathbf{w}^{\prime})\right\|_{w,* }^{2}\] \[+2\left\|\left[p_{1}R_{1}(\mathbf{w}^{\prime})-p_{1}R_{1}( \mathbf{w}),\dots,p_{m}R_{m}(\mathbf{w}^{\prime})-p_{m}R_{m}(\mathbf{w}) \right]^{\top}\right\|_{\infty}^{2}\ln m\] \[= 2D^{2}\left\|\sum_{i=1}^{m}q_{i}p_{i}\nabla R_{i}(\mathbf{w})- \sum_{i=1}^{m}q_{i}^{\prime}p_{i}\nabla R_{i}(\mathbf{w})+\sum_{i=1}^{m}q_{i}^ {\prime}p_{i}\nabla R_{i}(\mathbf{w})-\sum_{i=1}^{m}q_{i}^{\prime}p_{i}\nabla R _{i}(\mathbf{w}^{\prime})\right\|_{w,*}^{2}\] \[+2\left\|\left[p_{1}R_{1}(\mathbf{w}^{\prime})-p_{1}R_{1}( \mathbf{w}),\dots,p_{m}R_{m}(\mathbf{w}^{\prime})-p_{m}R_{m}(\mathbf{w}) \right]^{\top}\right\|_{\infty}^{2}\ln m\] \[\leq \underbrace{4D^{2}\left\|\sum_{i=1}^{m}q_{i}p_{i}\nabla R_{i}( \mathbf{w})-\sum_{i=1}^{m}q_{i}^{\prime}p_{i}\nabla R_{i}(\mathbf{w})\right\|_ {w,*}^{2}}_{:=A}+\underbrace{4D^{2}\left\|\sum_{i=1}^{m}q_{i}^{\prime}p_{i} \nabla R_{i}(\mathbf{w})-\sum_{i=1}^{m}q_{i}^{\prime}p_{i}\nabla R_{i}( \mathbf{w}^{\prime})\right\|_{w,*}^{2}}_{:=B}\] \[+\underbrace{2\max_{i\in[m]}\left|p_{i}\left[R_{i}(\mathbf{w})-R_ {i}(\mathbf{w}^{\prime})\right]\right|^{2}\ln m}_{:=C}.\]

To bound term \(A\), we have

\[4D^{2}\left\|\sum_{i=1}^{m}q_{i}p_{i}\nabla R_{i}(\mathbf{w})- \sum_{i=1}^{m}q_{i}^{\prime}p_{i}\nabla R_{i}(\mathbf{w})\right\|_{w,*}^{2}\] \[\leq 4D^{2}\left(\sum_{i=1}^{m}|q_{i}-q_{i}^{\prime}\|p_{i}\nabla R_{ i}(\mathbf{w})\|_{w,*}\right)^{2}\stackrel{{\eqref{eq:bound}}}{{ \leq}}4D^{2}\left(\sum_{i=1}^{m}|q_{i}-q_{i}^{\prime}|p_{i}G\right)^{2}\leq 4D^{2}G^{2}p_{ \max}^{2}\|\mathbf{q}-\mathbf{q}^{\prime}\|_{1}^{2}.\]

where \(p_{\max}\) is defined in (31). To bound \(B\), we have

\[4D^{2}\left\|\sum_{i=1}^{m}q_{i}^{\prime}p_{i}\nabla R_{i}(\mathbf{ w})-\sum_{i=1}^{m}q_{i}^{\prime}p_{i}\nabla R_{i}(\mathbf{w}^{\prime})\right\|_{w,*}^{2}\] \[\leq 4D^{2}\left(\sum_{i=1}^{m}q_{i}^{\prime}p_{i}\left\|\nabla R_{i }(\mathbf{w})-\nabla R_{i}(\mathbf{w}^{\prime})\right\|_{w,*}\right)^{2} \stackrel{{\eqref{eq:bound}}}{{\leq}}4D^{2}\left(\sum_{i=1}^{m}q_{i }^{\prime}p_{i}L\|\mathbf{w}-\mathbf{w}^{\prime}\|_{w}\right)^{2}\] \[\leq 4D^{2}L^{2}p_{\max}^{2}\|\mathbf{w}-\mathbf{w}^{\prime}\|_{w}^{ 2}\left(\sum_{i=1}^{m}q_{i}^{\prime}\right)^{2}=4D^{2}L^{2}p_{\max}^{2}\| \mathbf{w}-\mathbf{w}^{\prime}\|_{w}^{2}.\]

To bound \(C\), we have

\[2\max_{i\in[m]}\left|p_{i}\left[R_{i}(\mathbf{w})-R_{i}(\mathbf{ w}^{\prime})\right]\right|^{2}\ln m\] \[\stackrel{{\eqref{eq:bound}}}{{\leq}} 2\max_{i\in[m]}\left|p_{i}G\|\mathbf{w}-\mathbf{w}^{\prime}\|_{w} \right|^{2}\ln m\leq 2G^{2}p_{\max}^{2}\|\mathbf{w}-\mathbf{w}^{\prime}\|_{w}^{2}\ln m.\]

Putting everything together, we have

\[\|F([\mathbf{w};\mathbf{q}])-F([\mathbf{w}^{\prime};\mathbf{q}^{ \prime}])\|_{*}^{2}\leq(4D^{2}L^{2}+2G^{2}\ln m)p_{\max}^{2}\|\mathbf{w}- \mathbf{w}^{\prime}\|_{w}^{2}+4D^{2}G^{2}p_{\max}^{2}\|\mathbf{q}-\mathbf{q}^{ \prime}\|_{1}^{2}\] \[\leq p_{\max}^{2}(8D^{4}L^{2}+8D^{2}G^{2}\ln m)\left(\frac{1}{2D^{2}}\| \mathbf{w}-\mathbf{w}^{\prime}\|_{w}^{2}+\frac{1}{2\ln m}\|\mathbf{q}-\mathbf{q} ^{\prime}\|_{1}^{2}\right)\] \[= p_{\max}^{2}(8D^{4}L^{2}+8D^{2}G^{2}\ln m)\big{\|}[\mathbf{w}- \mathbf{w}^{\prime};\mathbf{q}-\mathbf{q}^{\prime}]\big{\|}^{2}\]which implies

\[\|F([\mathbf{w};\mathbf{q}])-F([\mathbf{w}^{\prime};\mathbf{q}^{\prime}])\|_{*} \leq p_{\max}\sqrt{8D^{4}L^{2}+8D^{2}G^{2}\ln m}\big{\|}[\mathbf{w}-\mathbf{w}^{ \prime};\mathbf{q}-\mathbf{q}^{\prime}]\big{\|}\leq\widetilde{L}\big{\|}[ \mathbf{w}-\mathbf{w}^{\prime};\mathbf{q}-\mathbf{q}^{\prime}]\big{\|}\]

where \(\widetilde{L}\) is defined in (31).

### Proof of Lemma 4

The light tail condition, required by Juditsky et al. (2011), is essentially the sub-Gaussian condition. To this end, we introduce the following sub-gaussian properties (Vershynin, 2018, Proposition 2.5.2).

**Proposition 1** (Sub-gaussian properties): _Let \(X\) be a random variable. Then the following properties are equivalent; the parameters \(K_{i}>0\) appearing in these properties differ from each other by at most an absolute constant factor._

1. _The tails of_ \(X\) _satisfy_ \[\Pr[|X|\geq t]\leq 2\exp(-t^{2}/K_{1}^{2}),\;\forall t\geq 0.\]
2. _The moments of_ \(X\) _satisfy_ \[\|X\|_{L_{p}}=(\mathrm{E}|X|^{p})^{1/p}\leq K_{2}\sqrt{p},\;\forall p\geq 1.\]
3. _The moment generating function (MGF) of_ \(X^{2}\) _satisfies_ \[\mathrm{E}\big{[}\exp(\lambda^{2}X^{2})\big{]}\leq\exp(K_{3}^{2}\lambda^{2}), \;\forall\lambda\text{ such that }|\lambda|\leq 1/K_{3}.\]
4. _The MGF of_ \(X^{2}\) _is bounded at some point, namely_ \[\mathrm{E}\big{[}\exp(X^{2}/K_{4}^{2})\big{]}\leq 2.\] _Moreover, if_ \(\mathrm{E}[X]=0\) _then properties (i)-(iv) are also equivalent to the following property._
5. _The MGF of X satisfies_ \[\mathrm{E}\big{[}\exp(\lambda X)\big{]}\leq\exp(K_{5}^{2}\lambda^{2}),\; \forall\lambda\in\mathbb{R}.\]

From the above proposition, we observe that the exact value of those constant \(K_{1},\ldots,K_{5}\) is not important, and it is very tedious to calculate them. So, in the following, we only focus on the order of those constants. To simplify presentations, we use \(c\) to denote an absolute constant that is independent of all the essential parameters, and its value may change from line to line.

Since

\[\|F([\mathbf{w};\mathbf{q}])-\mathbf{g}([\mathbf{w};\mathbf{q}]) \|_{*}^{2}\] \[= 2D^{2}\|\nabla_{\mathbf{w}}\varphi(\mathbf{w},\mathbf{q})- \mathbf{g}_{w}(\mathbf{w},\mathbf{q})\|_{w,*}^{2}+2\|\nabla_{\mathbf{q}} \varphi(\mathbf{w},\mathbf{q})-\mathbf{g}_{q}(\mathbf{w},\mathbf{q})\|_{\infty }^{2}\ln m,\]

we proceed to analyze the behavior of \(\|\nabla_{\mathbf{w}}\varphi(\mathbf{w},\mathbf{q})-\mathbf{g}_{w}(\mathbf{w},\mathbf{q})\|_{w,*}^{2}\) and \(\|\nabla_{\mathbf{q}}\varphi(\mathbf{w},\mathbf{q})-\mathbf{g}_{q}(\mathbf{w},\mathbf{q})\|_{\infty}^{2}\). To this end, we have the following lemma.

**Lemma 5**: _We have_

\[\mathrm{E}\bigg{[}\exp\bigg{(}\frac{1}{c\kappa G^{2}\omega_{\max} }\|\nabla_{\mathbf{w}}\varphi(\mathbf{w},\mathbf{q})-\mathbf{g}_{w}(\mathbf{w },\mathbf{q})\|_{w,*}^{2}\bigg{)}\,\bigg{]}\leq 2,\] (73) \[\mathrm{E}\left[\exp\bigg{(}\frac{1}{c\omega_{\max}\ln m}\|\nabla_ {\mathbf{q}}\varphi(\mathbf{w},\mathbf{q})-\mathbf{g}_{q}(\mathbf{w},\mathbf{q })\|_{\infty}^{2}\bigg{)}\right]\leq 2\]

_where \(\omega_{\max}\) is defined in (31) and \(c>0\) is an absolute constant._

[MISSING_PAGE_EMPTY:31]

[MISSING_PAGE_FAIL:32]

### Proof of Lemma 6

When \(\gamma\in[0,2\alpha]\), we have

\[\Pr\left[X\geq\gamma\right]\leq 1\leq 2\exp(-2/3)\leq 2\exp(-\gamma^{2}/6\alpha^{2}).\]

When \(\gamma\geq 2\alpha\), we have

\[\Pr\left[X\geq\gamma\right]=\Pr\left[X\geq\alpha+\gamma-\alpha\right]\leq\exp (-(\gamma-\alpha)^{2}/2)\leq\exp(-\gamma^{2}/8)\]

where we use the fact

\[\gamma-\alpha\geq\frac{\gamma}{2}.\]

Thus, we always have

\[\Pr\left[X\geq\gamma\right]\leq 2\exp\left(-\gamma^{2}/\max(6\alpha^{2},8) \right),\;\forall\gamma>0.\]

### Proof of Lemma 7

From (77), and the equivalence between Proposition 1.(i) and Proposition 1.(iv), we have

\[\Pr\left[\left|X_{j}\right|\geq t\right]\leq 2\exp\left(-t^{2}/cK_{j}^{2}\right),\;\forall t\geq 0,\forall j\in[m].\]

As a result,

\[\Pr\left[\max_{j\in[m]}\left|X_{j}\right|\geq t\right]=\Pr\left[ \exists j,\left|X_{j}\right|\geq t\right]\leq\sum_{j=1}^{m}\Pr\left[\left|X_{j }\right|\geq t\right]\leq 2\sum_{j=1}^{m}\exp\left(-t^{2}/cK_{j}^{2}\right)\] \[\leq 2m\exp\left(-t^{2}/cK_{\max}^{2}\right)=\exp\left(-t^{2}/cK_{ \max}^{2}+\ln[2m]\right).\]

Choosing \(t=\sqrt{cK_{\max}^{2}(\ln[2m]+\gamma^{2}/2)}\), we have

\[\Pr\left[\max_{j\in[m]}\left|X_{j}\right|\geq\sqrt{cK_{\max}^{2}(\ln[2m]+ \gamma^{2}/2)}\right]\leq\exp\left(-\gamma^{2}/2\right).\]

Thus

\[\Pr\left[\max_{j\in[m]}\left|X_{j}\right|\geq\sqrt{cK_{\max}^{2}} \left(\sqrt{\ln[2m]}+\gamma/\sqrt{2}\right)\right]\leq\exp\left(-\gamma^{2}/2\right)\] \[\Leftrightarrow \Pr\left[\sqrt{\frac{2}{cK_{\max}^{2}}}\max_{j\in[m]}\left|X_{j} \right|\geq\sqrt{2\ln[2m]}+\gamma\right]\leq\exp\left(-\gamma^{2}/2\right).\]

By Lemma 6, we have

\[\Pr\left[\sqrt{\frac{2}{cK_{\max}^{2}}}\max_{j\in[m]}\left|X_{j}\right|\geq \gamma\right]\leq 2\exp\left(-\gamma^{2}/\max\left(12\cdot\ln[2m],8\right) \right),\;\forall\gamma>0.\]

From the equivalence between Proposition 1.(i) and Proposition 1.(iv), we have

\[\mathrm{E}\left[\exp\left(\max_{j\in[m]}\left|X_{j}\right|^{2}\middlemiddlemiddle/ \left[cK_{\max}^{2}\ln m\right]\right)\right]\leq 2.\]