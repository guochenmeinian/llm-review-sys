WITRAN: Water-wave Information Transmission and Recurrent Acceleration Network for Long-range Time Series Forecasting

 Yuxin Jia   Youfang Lin   Xinyan Hao   Yan Lin   Shengnan Guo   Huaiyu Wan

School of Computer and Information Technology, Beijing Jiaotong University, China

Beijing Key Laboratory of Traffic Data Analysis and Mining, Beijing, China

{yuxinjia, yflin, xinyanhao, ylincs, guoshn, hywan}@bjtu.edu.cn

Corresponding author

###### Abstract

Capturing semantic information is crucial for accurate long-range time series forecasting, which involves modeling global and local correlations, as well as discovering long- and short-term repetitive patterns. Previous works have partially addressed these issues separately, but have not been able to address all of them simultaneously. Meanwhile, their time and memory complexities are still not sufficiently low for long-range forecasting. To address the challenge of capturing different types of semantic information, we propose a novel Water-wave Information Transmission (WIT) framework. This framework captures both long- and short-term repetitive patterns through bi-granular information transmission. It also models global and local correlations by recursively fusing and selecting information using Horizontal Vertical Gated Selective Unit (HVGSU). In addition, to improve the computing efficiency, we propose a generic Recurrent Acceleration Network (RAN) which reduces the time complexity to \(\mathcal{O}(\sqrt{L})\) while maintaining the memory complexity at \(\mathcal{O}(L)\). Our proposed method, called Water-wave Information Transmission and Recurrent Acceleration Network (WITRAN), outperforms the state-of-the-art methods by 5.80% and 14.28% on long-range and ultra-long-range time series forecasting tasks respectively, as demonstrated by experiments on four benchmark datasets. The code is available at: https://github.com/Water2sea/WITRAN.

## 1 Introduction

Time series forecasting is a valuable tool across diverse fields, such as energy, traffic, weather and so on. Compared with short-range time series forecasting, long-range forecasting offers the advantage of providing individuals with ample time to prepare and make informed decisions. For instance, accurate long-range traffic and weather forecasts enable individuals to plan their travel arrangements and attire accordingly. To improve the accuracy of such forecasting, it is essential to utilize longer historical sequences as input for the forecasting models (Liu et al., 2021; Zeng et al., 2023).

Previous studies (Wu et al., 2023; Nie et al., 2023) have highlighted the importance of capturing semantic information in long-range time series to achieve accurate predictions. However, the semantic information in long-range time series is diverse, so how to analyze and capture it becomes a major challenge. Specifically, semantic information includes two main aspects: (1) Global and local correlations. The local semantic information usually contains short-range changes within the data, while the global semantic information reflects the long-range trends present in the time series (Wang et al., 2023), which can be referred to as global-local semantic information. (2) Long- and short-term repetitive patterns. Time series often exhibit repetitive patterns at different timescales (Laiet al., 2018; Liu et al., 2021), such as hourly or daily cycles, which can be identified as periodic semantic information. Furthermore, it is crucial to model both aspects of semantic information simultaneously and ensure that the modeling approach remains computationally efficient, avoiding excessive complexity.

Unfortunately, although the existing state-of-the-art methods have shown great performance, they encounter difficulties in addressing the aforementioned challenges simultaneously. Specifically, **CNN-based methods**(Bai et al., 2018; Franceschi et al., 2019; Sen et al., 2019; Wu et al., 2023; Wang et al., 2023) have linear complexity with respect to the sequence length \(L\), but they are either limited by the size of the convolution receptive field (Wang et al., 2023) or constrained by the 1D input sequence (Wu et al., 2023), making it difficult to capture both important semantic information simultaneously. **Transformer-based methods** can be broadly classified into two categories based on whether the point-wise attention is used or not. The used ones (Vaswani et al., 2017; Zhou et al., 2021; Zhou et al., 2022a) capture correlations between points in the sequence, yet face challenges in capturing hidden semantic information directly from point-wise input tokens (Nie et al., 2023; Wu et al., 2023). The others (Li et al., 2019; Wu et al., 2021; Liu et al., 2021) struggles either with achieving sufficiently low computational complexity or with effectively capturing periodic semantic information. **Other methods**(Zhou et al., 2022b; Zeng et al., 2023) also exhibit limitations in capturing semantic information mentioned above. Further details can be found in Section 2.

**RNN-based methods**(Hochreiter and Schmidhuber, 1997; Chung et al., 2014; Rangapuram et al., 2018; Salinas et al., 2020) have significant advantages in capturing global and local semantic information through their recurrent structure, as shown in Figure 1(a), while maintaining linear complexity. However, they suffer from gradient vanishing/exploding (Pascanu et al., 2013) and information forgetting issues (refer to Appendix B for more details), making them less suitable for direct application in long-range forecasting tasks. Fortunately, it has been proven that by splitting the information transmissions into patches between a few adjacent time steps and processing them individually, it is possible to maintain both global and local semantic information (Nie et al., 2023; Wu et al., 2023). This insight is highly inspiring, as it suggests that by dividing the inputs of RNNs into numerous subseries and processing them separately, we can effectively address the significant limitation mentioned earlier, without compromising the efficiency of long-range forecasting.

Figure 1: Information transmission process diagram of different forecasting models.

Based on the aforementioned insights, we propose a novel framework called **W**ater-wave **I**nformation **T**ransmission and **R**ecurrent **A**cceleration **N**etwork (**WITRAN**) which comprises two key components: The **W**ater-wave **I**nformation **T**ransmission (**WIT**) framework and the **R**ecurrent **A**cceleration **N**etwork (**RAN**). The overall information transmission process of WITRAN is illustrated in Figure 1(i). Firstly, to capture the periodic semantic information of long- and short-term, we rearrange the input sequences according to their natural period, as shown in Figure 2(a). This rearrangement allows for information transmission in two directions, resembling the propagation of water-wave energy, as shown in Figure 2(b). The horizontal red arrows indicate the intra-periodic information transmission along the time steps, while the vertical blue arrows indicate the inter-periodic information transmission. Secondly, to preserve the characteristics of long- and short-term periodic semantic information, we propose a novel **I**nforizontal **V**ertical **G**ated **S**elective **U**nit (**HVGSU**) which incorporates **G**ated **S**elective **C**ells (**GSC**s) separately in both directions. To capture the correlation between periodic semantic information of both directions at each time step, we design fusion and selection operations in GSC. With a recurrent structure, HVGSU progressively captures more local semantic information until it encompasses the global semantic information. Thirdly, to improve efficiency, we propose the Recurrent Acceleration Network (RAN), which enables parallel processing of information transmission in both directions. Notably, RAN maintains a memory complexity of \(\mathcal{O}(L)\) while reducing the time complexity to \(\mathcal{O}(\sqrt{L})\). In addition, RAN could serve as a universal framework for integrating other models to facilitate information fusion and transmission. In summary, our main contributions are as follows:

* We propose a **W**ater-wave **I**nformation **T**ransmission and **R**ecurrent **A**cceleration **N**etwork (**WITRAN**), which represents a novel paradigm in information transmission by enabling bi-granular flows. We provide a comprehensive comparison of WITRAN with previous methods in Figure 1 to highlight its uniqueness. Furthermore, in order to compare the differences between WITRAN and the model (a)-(h) in Figure 1 more clearly, we have prepared Table 1 to highlight the advantages of WITRAN.
* We propose a novel **I**nforizontal **V**ertical **G**ated **S**elective **U**nit (**HVGSU**) which captures long- and short-term periodic semantic information by using **G**ated **S**elective **C**ell (**GSC**) independently in both directions, preserving the characteristics of periodic semantic information. The fusion and selection in GSC can model the correlations of long- and short-term periodic semantic information. Furthermore, utilizing a recurrent structure with HVGSU facilitates the gradual capture of semantic information from local to global within a sequence.
* We present a **R**ecurrent **A**cceleration **N**etwork (**RAN**) which is a generic acceleration framework that significantly reduces the time complexity to \(\mathcal{O}(\sqrt{L})\) while maintaining the memory complexity of \(\mathcal{O}(L)\). We summarize the complexities of different methods in Table 2, demonstrating the superior efficiency of our method.
* We conduct extensive experiments on four benchmark datasets across various fields (energy, traffic and weather). The empirical studies demonstrate the remarkable performance of WITRAN, which achieves relative improvements of \(5.80\%\) and \(14.28\%\) in long-range and ultra-long-range forecasting respectively. In addition, the introduction of the generic RAN framework greatly improves the computational efficiency.

\begin{table}
\begin{tabular}{c c c c c c c c c c c c} \hline \hline Advantages & (\(\alpha\)D Rank) & (\(\alpha\)D Rank) & (\(\alpha\)D Rank) & (\(\alpha\)D Rank) & (\(\alpha\)D Rank) & (\(\alpha\)D Rank) & (\(\alpha\)D Rank) & (\(\alpha\)D Rank) & (\(\alpha\)D Rank) & (\(\alpha\)D Rank) & (\(\alpha\)D Rank) & (\(\alpha\)D Rank) \\ \hline Non-price semantic information capture & ✓ & ✓ & ✗ & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ \\ \hline Special design \(\alpha\) capture long-term prediction patterns & ✗ & ✗ & ✗ & ✗ & ✓ & ✗ & ✗ & ✗ & ✓ & ✓ & ✓ \\ \hline Enhanced (\(\alpha\) + \(2\) largest model global consensus) & ✓(1) & ✗ & ✓(1) & ✗ & ✗ & ✓(2) & ✓(2) & ✗ & ✓(1) \\ \hline WiI study the gradient using/residuals prediction patterns \(\alpha\)D Rank & ✗ & - & - & - & - & - & - & - & ✓ \\ \hline \hline \end{tabular}
\end{table}
Table 1: Advantages of **WITRAN** compared to other methods.

\begin{table}
\begin{tabular}{c c c c c c c c c c c c} \hline \hline Methods & RNN & CNN & Transformer & LogTrans & Transformer & Autoformer & Transformer & FEDformer & FEDformer & FEDformer & FEDformer & FEDM & **F**nIo**TST & **MCN** & **WITRAN (ours)** \\ \hline Time & \(\mathcal{O}(L)\) & \(\mathcal{O}(L)\) & \(\mathcal{O}(L^{2})\) & \(\mathcal{O}(L\log L)\) & \(\mathcal{O}(L\log L)\) & \(\mathcal{O}(L\log L)\) & \(\mathcal{O}(L)\) & \(\mathcal{O}(L)\) & \(\mathcal{O}(L)\) & \(\mathcal{O}(L/S)^{3}\) & \(\mathcal{O}(L)\) & \(\mathcal{O}(\sqrt{L})\) \\ \hline Memory & \(\mathcal{O}(L)\) & \(\mathcal{O}(L)\) & \(\mathcal{O}(L)\) & \(\mathcal{O}(L^{2})\) & \(\mathcal{O}(L\log

## 2 Related Work

Time series forecasting methods can be broadly categorized into statistical methods and neural network methods. Neural network methods, in turn, can be further classified into several subcategories based on the specific techniques employed, including RNN-based, CNN-based, Transformer-based, and other Deep Neural Networks (DNNs) by the methods used.

**Statistical Methods** with major representatives include ARIMA (Box and Jenkins, 1968), Prophet (Taylor and Letham, 2018), and Holt Winter (Athanasopoulos and Hyndman, 2020). However, the inherent variability and complexity of real-world time series often render these methods inadequate, as they are built on hypotheses that may not align well with the characteristics of such data. As a result, the performance of these statistical methods tends to be limited.

**RNN-based Methods**(Hochreiter and Schmidhuber, 1997; Chung et al., 2014; Rangapuram et al., 2018; Salinas et al., 2020) possess the inherent advantage of capturing semantic information in time series data through their recurrent structure, which mimics the natural information transmission process of time series. This allows them to progressively capture semantic information from local to global contexts. These methods exhibit linear time and memory complexity, enabling efficient processing of time series data with a sequence length of \(L\). However, when dealing with longer sequences, the problems of gradient vanishing/exploding (Pascanu et al., 2013) and information forgetting (see Appendix B for more details) further limit it.

**CNN-based methods**(Bai et al., 2018; Franceschi et al., 2019; Sen et al., 2019; Wu et al., 2023; Wang et al., 2023) are adept at capturing local semantic information through the application of convolutional kernels, while maintaining linear time and memory complexity. However, on the one hand, most of these methods face challenges in capturing comprehensive global information due to the limited receptive field of individual convolutional layers, which make the training process more difficult and overhead (Wang et al., 2023). On the other hand, it is difficult to directly capture long- and short-term repetitive patterns on 1D inputs. MICN (Wang et al., 2023) adopted downsampling 1D convolutions and isometric convolutions combined with a multi-branch framework, which can effectively solve the former problem, but still suffers from the latter one. TimesNet (Wu et al., 2023) transforms 1D inputs into 2D space, leveraging a parameter-efficient inception block to capture intra- and inter-period variations. However, it solves the latter problem while still facing the former one.

**Transformer-based methods** have shown advancements in time series forecasting, with two main categories. The first category uses point-wise attention mechanisms, such as Vanilla Transformer (Vaswani et al., 2017), Informer (Zhou et al., 2021), and FEDformer (Zhou et al., 2022), while facing challenges in extracting sufficient semantic information from individual time points. The second category employs non point-wise dot product techniques, including LogTrans (Li et al., 2019), Autoformer (Wu et al., 2021), Pyraformer (Liu et al., 2021) and PatchTST (Nie et al., 2023). They reduce computational complexities to a certain degree, yet face difficulties in incorporating the long-term correlations in time-series. For a detailed description of these methods, please refer to Appendix A. Furthermore, it is worth noting that the majority of the aforementioned methods fail to achieve lower complexity than RNN-based methods. For a comprehensive comparison of the theoretical time and memory complexity, as well as experimental results, please refer to Appendix C.

**Other DNN methods** have also demonstrated promising performance. For example, FiLM (Zhou et al., 2022) utilizes Legendre Polynomials and Fourier projection methods to capture historical in

Figure 2: Input rearrangement and water-wave information transmission.

formation, eliminate noise, and expedite computations using low-rank approximation. DLinear (Zeng et al., 2023) has showcased impressive outcomes by employing simple linear operations. Nevertheless, both approaches encounter difficulties in capturing various repeating patterns present in the sequence.

## 3 The WITRAN Model

The time series forecasting task involves predicting future values \(Y\in\mathbb{R}^{P\times c_{\rm out}}\) for \(P\) time steps based on the historical input sequences \(X=\{x_{1},x_{2},\ldots,x_{H}\}\in\mathbb{R}^{H\times c_{\rm in}}\) of \(H\) time steps, where \(c_{\rm in}\) and \(c_{\rm out}\) represent the number of input and output features respectively. In order to integrate enough historical information for analysis, it is necessary for \(H\) to have a sufficient size (Liu et al., 2021; Zeng et al., 2023). Furthermore, capturing semantic information from the historical input is crucial for accurate forecasting, which includes modeling global and local correlations, as well as discovering long- and short-term repetitive patterns. However, how to address them simultaneously is a major challenge. With these in mind, we propose WITRAN, a novel information transmission framework akin to the propagation of water waves. WITRAN captures both long- and short-term periodic semantic information, as well as global-local semantic information simultaneously during information transmission. Moreover, WITRAN reduces time complexity while maintaining linear memory complexity. The overall structure of WITRAN is depicted in Figure 3.

### Input Module

To facilitate the analysis of long- and short-term repetitive patterns, inspired by TimesNet (Wu et al., 2023), we first rearrange the sequence from 1D to 2D based on its natural period, as illustrated by Figure 2(a). Importantly, our approach involves analyzing the natural period of time series and setting appropriate hyperparameters to determine the input rearrangement, rather than using Fast Fourier Transform (FFT) to learn multiple adaptive periods of inputs in TimesNet. Consequently, our method is much simpler. Additionally, in order to minimize the distribution shift in datasets, we draw inspiration from NLinear (Zeng et al., 2023) and employ an adaptive learning approach to determine whether to perform simple normalization. The input module can be described as follows:

\[\begin{split} X_{\rm 1D}&=\begin{cases}X&,norm=0\\ X-x_{H}&,norm=1\end{cases}\\ X_{\rm 2D}&=\mathrm{Rearrange}([X_{\rm 1D},TF_{\rm en}]),\end{split}\] (1)

here, \(X_{\rm 1D}\in\mathbb{R}^{H\times c_{\rm in}}\) represents the original input sequences, \(x_{H}\in\mathbb{R}^{c_{\rm in}}\) represents the input at the last time step of the original sequence, \(TF_{\rm en}\in\mathbb{R}^{H\times c_{\rm time}}\) represents the temporal contextual features of original input sequence (e.g., HourOfDay, DayOfWeek, DayOfMonth and DayOfYear), where \(c_{\rm time}\) is the dimension of time features. \(X_{\rm 2D}\in\mathbb{R}^{R\times C\times(c_{\rm in}+c_{\rm time})}\) represents the inputs after rearrangement, where \(R\) denotes the total number of horizontal rows and \(C\) denotes the vertical columns. \(norm\) is an adaptive parameter for different tasks. \([\cdot]\) represents the concat operation and \(\mathrm{Rearrange}\) represents the rearrange operation, with reference to Figure 2(a).

Figure 3: Overall structure of WITRAN.

### Horizontal Vertical Gated Selective Unit

To capture long- and short-term periodic semantic information and reserve their characteristics, we propose a novel **H**orizontal **V**ertical **G**ated **S**elective **U**nit (**HVGSU**) which consists of **G**ated **S**elective **C**ells (**GSCs**) in two directions. To capture the correlation at each time step between periodic semantic information of both directions, we design the specific operations in GSC. Furthermore, HVGSU is capable of capturing both global and local information via a recurrent structure. In this subsection, we will provide a detailed introduction to them.

HvguAs depicted in Figure 3, the process of HVGSU via a recurrent structure is:

\[H_{\rm hor},H_{\rm ver},Out=\text{HVGSU}(X_{\rm 2D}),\] (2)

where \(H_{\rm hor}\in\mathbb{R}^{L\times R\times d_{\rm model}}\) and \(H_{\rm ver}\in\mathbb{R}^{L\times C\times d_{\rm model}}\) represent the horizontal and the vertical output hidden state of HVGSU separately. \(L\) is the depth of the model, and \(Out\in\mathbb{R}^{R\times C\times d_{\rm model}}\) denotes the output information of the last layer.

In greater detail, the cellular structure of HVGSU is shown in Figure 4(b), which consists of GSCs in two directions to capture the periodic semantic information of long- and short-term. The cell operations for row \(r\)\((1\leq r\leq R)\) and column \(c\)\((1\leq c\leq C)\) in layer \(l\)\((1\leq l\leq L)\) can be formalized as:

\[\begin{array}{l}h_{r,\;c,\;l}^{\rm hor}=\text{GSC}_{\rm hor}(input_{r,\;c, \;l},\;h_{r,\;c,\;l,\;l}^{\rm hor},\;h_{r-1,\;c,\;l}^{\rm ver},\;h_{r,\;c,\;l }^{\rm ver})\\ h_{r,\;c,\;l}^{\rm ver}=\text{GSC}_{\rm ver}(input_{r,\;c,\;l,\;h_{r-1,\;c,\;l }^{\rm ver},\;h_{r,\;c-1,\;l}^{\rm hor})}\\ o_{r,\;c,\;l}=[h_{r,\;c,\;l}^{\rm hor},\;h_{r,\;c,\;l}^{\rm ver}],\end{array}\] (3)

here, \(input_{r,\;c,\;l}\in\mathbb{R}^{d_{\rm in}}\). When \(l=1,\;input_{r,\;c,\;l}=x_{r,\;c}\in\mathbb{R}^{c_{\rm in}+c_{\rm time}}\) represents the input for the first layer, and when \(l>1,\;input_{r,\;c,\;l}=o_{r,\;c,\;l-1}\in\mathbb{R}^{2\times d_{\rm model}}\) represents the input for subsequent layers. \(h_{r,\;c-1,\;l}^{\rm hor}\) and \(h_{r-1,\;c,\;l}^{\rm ver}\in\mathbb{R}^{d_{\rm model}}\) represent the horizontal and vertical hidden state inputs of the current cell. Note that when \(r=1,\;h_{r-1,\;c,\;l}^{\rm ver}\) is replaced by an all-zero tensor of the same size, and the same is true for \(h_{r,\;c-1,\;l}^{\rm hor}\) when \(c=1\). \([\cdot]\) represents the concatenation operation and \(o_{r,\;c,\;l}\in\mathbb{R}^{2\times d_{\rm model}}\) represents the output of the current cell.

GscInspired by the two popular RNN-based models, LSTM [Hochreiter and Schmidhuber, 1997] and GRU [Chung et al., 2014] (for more details, see Appendix A), we propose a Gated Selective Cell (GSC) to fuse and select information. Its structure is shown in Figure 4(a), which comprises two gates: the selection gate, and the output gate. The fused information consists of input and principal-subordinate hidden states, and the selection gate determines the retention of the original principal information and the addition of the fused information. Finally, the output gate determines the final output information of the cell. The different colored arrows in Figure 4(a) represent different semantic information transfer processes. The black arrow represents the input, the red arrows represent the process of transmitting principal hidden state information, the blue arrow represents the subordinate hidden state, and the purple arrows represent the process by which fused information of

Figure 4: The structure of our method.

principal-subordinate hidden states is transmitted. The formulations are given as follows:

\[\begin{split} S_{t}&=\sigma(W_{s}[h_{t-1}^{\mathrm{pri} },\;h_{t-1}^{\mathrm{sub}},\;x]+b_{s})\\ O_{t}&=\sigma(W_{o}[h_{t-1}^{\mathrm{pri}},\;h_{t-1}^ {\mathrm{sub}},\;x]+b_{o})\\ h_{f}&=\text{tanh}(W_{f}[h_{t-1}^{\mathrm{pri}},\;h_{t- 1}^{\mathrm{pri}},\;x]+b_{f})\\ \widetilde{h_{t-1}^{\mathrm{pri}}}&=(1-S_{t}) \odot h_{t-1}^{\mathrm{pri}}+S_{t}\odot h_{f}\\ h_{t}^{\mathrm{pri}}&=\text{tanh}(\widetilde{h_{t-1 }^{\mathrm{pri}}})\odot O_{t},\end{split}\] (4)

where \(h_{t-1}^{\mathrm{pri}}\) and \(h_{t-1}^{\mathrm{sub}}\in\mathbb{R}^{d_{\mathrm{model}}}\) represent the input principal and subordinate hidden state, \(x\in\mathbb{R}^{d_{in}}\) represents the input. \(W_{*}\in\mathbb{R}^{d_{\mathrm{model}}\times(2d_{\mathrm{model}+d_{in}})}\) are weight matrices and \(b_{*}\in\mathbb{R}^{d_{\mathrm{model}}}\) are bias vectors. \(S_{t}\) and \(O_{t}\) denote the selection gate and the output gate, \(\odot\) denotes an element-wise product, \(\sigma(\cdot)\) and \(\text{tanh}(\cdot)\) denote the sigmoid and tanh activation function. \(h_{f}\) and \(\widetilde{h_{t-1}^{\mathrm{pri}}}\in\mathbb{R}^{d_{\mathrm{model}}}\) represent the intermediate variables of the calculation. \(h_{t}^{\mathrm{pri}}\) represents the output hidden state.

### Recurrent Acceleration Network

In traditional recurrent structure, for two adjacent time steps in series, the latter one always waits for the former one until the information computation of the former one is completed. When the sequence becomes longer, this becomes slower. Fortunately, in the WIT framework we designed, some of the time step information can be computed in parallel. As shown in Figure 2(b), after a point is calculated, the right point in its horizontal direction and the point below it in its vertical direction can start calculation without waiting for each other. Therefore, we propose the Recurrent Acceleration Network (RAN) as our accelerated framework, which enables parallel computation of data points without waiting for each other, greatly improving the efficiency of information transmission in HVGSU. We place parallelizable points in a slice, and the updated information transfer process is shown in Figure 4(c). Each green box in Figure 4(c) represents a slice, and the number of green boxes is the number of times we need to recurrently compute. The meanings of the remaining markers are the same as those in Figure 2. Under the RAN framework, the recurrent length has changed from the sequence length \(L=R\times C\) to \(R+C-1\), while the complexity of \(R\) and \(C\) is \(\mathcal{O}(\sqrt{L})\). Thus, we have reduced the time complexity to \(\mathcal{O}(\sqrt{L})\) via the RAN framework. It should be noted that although we parallelly compute some data points, which may increase some memory, the complexity of parallel computation, \(\mathcal{O}(\sqrt{L})\), is far less than the complexity of saving the output variables, \(\mathcal{O}(L)\), because we need to save the output information of each point in the sequence. Implementation source code for RAN is given in Appendix D.

### Forecasting Module

In the forecasting module, we address the issue of error accumulation in the auto-regressive structure by drawing inspiration from Informer (Zhou et al., 2021) and Pyraformer (Liu et al., 2021), combining both horizontal and vertical hidden states, and then making predictions through a fully connected layer, as illustrated in Figure 3.

We utilize the last row of the horizontal hidden states as it contains sufficient global and latest short-term semantic information from the historical sequence. In contrast, all columns of the vertical hidden states, which capture different long-term semantic information, are all preserved. The combined operation not only maximizes the retention of the various semantic information needed for predicting the points, but also avoids excessive redundancy in order to obtain accurate predictions. This module can be formalized as follows:

\[\begin{split} H_{\mathrm{rep}}^{\mathrm{hor}}&=\text{ Repeat}(h_{\mathrm{hor}})\\ H_{\mathrm{h-v}}&=\text{Reshape}([H_{\mathrm{rep}}^{ \mathrm{hor}},H_{\mathrm{ver}}])\\ \widehat{Y}&=\text{FC1}(H_{\mathrm{h-v}})\\ Y&=\text{FC2}(\text{Reshape}(\widehat{Y})+ TFE_{ \mathrm{de}}),\end{split}\] (5)

where \( TFE_{\mathrm{de}}\in\mathbb{R}^{P\times d_{\mathrm{model}}}\) represents time features encoding of the forecasting points separately. \(h_{\mathrm{hor}}\in\mathbb{R}^{L\times 1\times d_{\mathrm{model}}}\) represents the last row hidden state in \(H_{\mathrm{hor}}\). Repeat\((\cdot)\) is for repeat operation and \(H_{rep}^{\rm hor}\in\mathbb{R}^{L\times C\times d_{\rm model}}\) represents the hidden state after the repeat operation. \(H_{\rm h-v}\in\mathbb{R}^{C\times(L*2d_{\rm model})}\) is the vector of horizontal and vertical combination after reshape operation. FC1 and FC2 represent two fully connected layers respectively. \(\widehat{Y}\in\mathbb{R}^{C\times(R_{\rm de}*d_{\rm model})}\) represents the intermediate variables of the calculation and \(R_{\rm de}\times C=P\). \(Y\) represents the output of this module, and it is worth noting that we need to utilize the adaptive parameter \(norm\) for denormalization, when \(norm=1,Y=Y+x_{H}\).

## 4 Experiment

DatasetsTo evaluate the proposed WITRAN, we conducted extensive experiments on four widely recognized real-world benchmark datasets for long-range and ultra-long-range forecasting. These datasets cover energy, traffic, and weather domains. We split all datasets into training, validation and test set in chronological order by the ratio of 6:2:2. More details about the datasets and implementation are described in Appendix E and Appendix F.

BaselinesIn light of the underperformance produced by classical models such as ARIMA and simple RNN/CNN models, as evidenced by (Zhou et al., 2021) and (Wu et al., 2021), and the subpar performance exhibited by certain Transformer-based models like LogTrans (Li et al., 2019) and Reformer (Kitaev et al., 2020), as shown in (Wu et al., 2023) and (Wang et al., 2023), our study primarily incorporates six transformer-based models: PatchTST (Nie et al., 2023), FEDformer (Zhou et al., 2022), Pyraformer (Liu et al., 2021), Autoformer(Zhou et al., 2022), Informer (Zhou et al., 2021), Vanilla Transformer (Vaswani et al., 2017), and four non-transformer-based methods: MICN (Wang et al., 2023), TimesNet (Wu et al., 2023), DLinear (Zhou et al., 2022), FiLM (Zhou et al., 2022). Please refer to Appendix A and Appendix H for more details of the baselines and implementation.

### Experimental Results

For a better comparison, we adopted the experimental setup of Pyraformer (Liu et al., 2021) for long-range and ultra-long-range series forecasting. In addition, channel-independence is crucial for multivariate time series prediction (Nie et al., 2023), so it is necessary to verify the performance of models on a single channel to ensure their effectiveness across all channels in multivariate time series prediction. In this paper, experiments were conducted on a single channel. Note that in order to fairly compare the performance of each model, we set up the search space so that each model can perform optimally on each task. For further details, please refer to Appendix F.

Long-range Forecasting ResultsWe conducted five tasks on each dataset for long-range prediction, and the results are shown in Table 3. Taking the task setting 168-336 on the left side of the Table 3 as an example, it indicates that the input length is 168 and the prediction length is 336. Notably, our proposed WITRAN achieves state-of-the-art performance, surpassing the previous best method with an average MSE reduction of 5.803%. Specifically, WITRAN exhibits an average MSE reduction of 10.246% for ECL, 3.879% for traffic, 2.519% for ETTh1, 4.431% for ETTh2, and 7.939% for weather. Additionally, we note that the competition among the baselines is intense due to their best performance on each task, but none of them consistently performs well across all tasks. In contrast, WITRAN demonstrates its robust competitiveness across various tasks and datasets. In addition, we can find that in most cases, a longer input length yields greater improvement with the same prediction length. All findings above collectively highlight WITRAN's efficacy in addressing diverse time-series forecasting tasks in real-world applications. Further results and showcases are presented in Appendix H and Appendix J.

Ultra-long-range Forecasting ResultsWe also show the three tasks for ultra-long-range prediction results on each dataset in Table 4. The tasks on the left side of the table hold the same interpretation as above. Notably, WITRAN achieves an 14.275% averaged MSE reduction. More specifically, WITRAN demonstrates an average MSE reduction of 39.916% for ECL, 3.122% for traffic, 14.837% for ETTh1, 2.441% for ETTh2, and 11.062% for weather. In particular, our method showcases substantial improvements of over 10% in ECL, ETTh1, and weather, reinforcing our ability to predict ultra-long-range time series. And by comparing the results of ultra-long-range forecasting and long-range Forecasting, we can find that our method is more competitive for ultra-long-range prediction,

[MISSING_PAGE_FAIL:9]

can be found in Appendix G. Here, we provide a concise summary of the key findings: 1) The fusion and selection design of GSC enables both long- and short-term semantic information to be captured for each historical input data points. 2) Setting up independent cells in both directions enables to extract semantic information in the long- and short-term respectively. 3) The specially designed combined operation can make fuller use of the captured local and global semantic information while ensuring that the information is not redundant. 4) RAN offers advantages in terms of speed and space complexity. The utilization of RAN eliminates the need to store excessive intermediate variables, as shown in the previous section.

### Robustness Analysis

We have followed MICN (Wang et al., 2023) and introduced a simple white noise injection to demonstrate the robustness of our model. Specifically, we randomly select a proportion \(\varepsilon\) of data from the original input sequence and apply random perturbations within the range \([-2X_{i},2X_{i}]\) to the selected data, where \(X_{i}\) denotes the original data. After the noise injection, the data is then used for training, and the MSE and MAE metrics are recorded in Table 5.

It can be found that as the perturbation proportion increases, there is a slight increase in the MSE and MAE metrics in terms of forecasting. It indicates that WITRAN demonstrates good robustness when dealing with less noisy data (up to 10\(\%\)), and it possesses a significant advantage in effectively handling various abnormal data fluctuations.

## 5 Conclusions

In this paper, we propose WITRAN, a novel Water-wave Information Transmission framework and a universal acceleration framework. WITRAN effectively captures both long- and short-term repetitive patterns and global and local correlations with \(\mathcal{O}(\sqrt{L})\) time complexity and \(\mathcal{O}(L)\) memory complexity. The experimental results demonstrate the remarkable performance and efficiency of WITRAN. However, this recurrent structure is still not optimally efficient for Python-based implementations because of the information waiting between slices. Therefore, we plan to explore the integration of WITRAN into an interface using C++, similar to the implementation of nn.GRU/nn.LSTM in PyTorch in the future, to further improve its efficiency.

## Acknowledgments

This work was supported by the Fundamental Research Funds for the Central Universities under Grant No. 2022YJS142. Furthermore, we would like to express our gratitude to Shuo Wang and Shuohao Lin for their valuable discussions and assistance.

\begin{table}
\begin{tabular}{c c c c c c c c c c c c c c} \hline \hline  & Tasks & 168-168 & 168-336 & 336-336 & 336-720 & 720-720 & 720-1440 & 1440-1440 & 1440-2880 \\  & Metric & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE \\ \hline \multirow{4}{*}{\(\varepsilon\)} & \(\varepsilon=0\%\) & 0.2397 & 0.3519 & 0.2607 & 0.3721 & 0.25417 & 0.3627 & 0.3084 & 0.4055 & 0.2478 & 0.3651 & 0.2499 & 0.3727 & 0.2348 & 0.3680 & 0.3359 & 0.4383 \\  & \(\varepsilon=1\%\) & 0.2430 & 0.3534 & 0.2630 & 0.3738 & 0.2516 & 0.3626 & 0.3078 & 0.4060 & 0.2345 & 0.3960 & 0.2502 & 0.3739 & 0.2395 & 0.3672 & 0.3327 & 0.4767 \\  & \(\varepsilon=5\%\) & 0.2463 & 0.3573 & 0.2692 & 0.3788 & 0.2567 & 0.3652 & 0.3039 & 0.4046 & 0.2255 & 0.3720 & 0.2568 & 0.3786 & 0.2410 & 0.3687 & 0.3315 & 0.4408 \\  & \(\varepsilon=10\%\) & 0.2543 & 0.3621 & 0.2726 & 0.3811 & 0.2703 & 0.3763 & 0.3098 & 0.4061 & 0.2596 & 0.3766 & 0.2648 & 0.3880 & 0.2444 & 0.3716 & 0.3366 & 0.4445 \\ \hline \multirow{4}{*}{\(\varepsilon\)} & \(\varepsilon=0\%\) & 0.3177 & 0.2051 & 0.1321 & 0.2969 & 0.1336 & 0.2954 & 0.1391 & 0.2715 & 0.1408 & 0.2919 & 0.1672 & 0.2449 & 0.1543 & 0.2325 & 0.1425 & 0.2333 \\  & \(\varepsilon=1\%\) & 0.1376 & 0.2063 & 0.1329 & 0.2893 & 0.1316 & 0.2704 & 0.1422 & 0.2218 & 0.1676 & 0.2466 & 0.1551 & 0.2372 & 0.1466 & 0.2360 \\  & \(\varepsilon=5\%\) & 0.1370 & 0.2115 & 0.1362 & 0.2148 & 0.1322 & 0.2106 & 0.1442 & 0.2260 & 0.1416 & 0.2247 & 0.1699 & 0.2519 & 0.1572 & 0.2373 & 0.1519 & 0.2480 \\  & \(\varepsilon=10\%\) & 0.1409 & 0.2164 & 0.1355 & 0.2193 & 0.1372 & 0.2200 & 0.1467 & 0.2325 & 0.1459 & 0.2291 & 0.1652 & 0.2475 & 0.1561 & 0.2338 & 0.1575 & 0.2580 \\ \hline \multirow{4}{*}{\(\varepsilon\)} & \(\varepsilon=0\%\) & 0.105 & 0.2589 & 0.1192 & 0.2114 & 0.11263 & 0.1498 & 0.1494 & 0.1926 & 0.2086 & 0.1319 & 0.2943 & 0.1304 & 0.2920 & 0.1580 & 0.3452 \\  & \(\varepsilon=1\%\) & 0.1112 & 0.2596 & 0.1208 & 0.2726 & 0.1111 & 0.2637 & 0.1463 & 0.3053

## References

* Liu et al. (2021) Shizhan Liu, Hang Yu, Cong Liao, Jianguo Li, Weiyao Lin, Alex X Liu, and Schahram Dustdar. Pyraformer: Low-complexity pyramidal attention for long-range time series modeling and forecasting. In _International conference on learning representations_, 2021.
* Zeng et al. (2023) Ailing Zeng, Muxi Chen, Lei Zhang, and Qiang Xu. Are transformers effective for time series forecasting? In _Proceedings of the AAAI conference on artificial intelligence_, volume 37, pages 11121-11128, 2023.
* Wu et al. (2023) Haixu Wu, Tengge Hu, Yong Liu, Hang Zhou, Jianmin Wang, and Mingsheng Long. Timesnet: Temporal 2d-variation modeling for general time series analysis. In _The Eleventh International Conference on Learning Representations_, 2023.
* Nie et al. (2023) Yuqi Nie, Nam H Nguyen, Phanwadee Sinthong, and Jayant Kalagnanam. A time series is worth 64 words: Long-term forecasting with transformers. In _The Eleventh International Conference on Learning Representations_, 2023.
* Wang et al. (2023) Huiqiang Wang, Jian Peng, Feihu Huang, Jince Wang, Junhui Chen, and Yifei Xiao. Micn: Multi-scale local and global context modeling for long-term series forecasting. In _The Eleventh International Conference on Learning Representations_, 2023.
* Lai et al. (2018) Guokun Lai, Wei-Cheng Chang, Yiming Yang, and Hanxiao Liu. Modeling long-and short-term temporal patterns with deep neural networks. In _The 41st international ACM SIGIR conference on research & development in information retrieval_, pages 95-104, 2018.
* Bai et al. (2018) Shaojie Bai, J Zico Kolter, and Vladlen Koltun. An empirical evaluation of generic convolutional and recurrent networks for sequence modeling. _arXiv preprint arXiv:1803.01271_, 2018.
* Franceschi et al. (2019) Jean-Yves Franceschi, Aymeric Dieuleveut, and Martin Jaggi. Unsupervised scalable representation learning for multivariate time series. _Advances in neural information processing systems_, 32, 2019.
* Sen et al. (2019) Rajat Sen, Hsiang-Fu Yu, and Inderjit S Dhillon. Think globally, act locally: A deep neural network approach to high-dimensional time series forecasting. _Advances in neural information processing systems_, 32, 2019.
* Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. _Advances in neural information processing systems_, 30, 2017.
* Zhou et al. (2021) Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, and Wancai Zhang. Informer: Beyond efficient transformer for long sequence time-series forecasting. In _Proceedings of the AAAI conference on artificial intelligence_, volume 35, pages 11106-11115, 2021.
* Zhou et al. (2022a) Tian Zhou, Ziqing Ma, Gingsong Wen, Xue Wang, Liang Sun, and Rong Jin. Fedformer: Frequency enhanced decomposed transformer for long-term series forecasting. In _International Conference on Machine Learning_, pages 27268-27286. PMLR, 2022a.
* Li et al. (2019) Shiyang Li, Xiaoyong Jin, Yao Xuan, Xiyou Zhou, Wenhu Chen, Yu-Xiang Wang, and Xifeng Yan. Enhancing the locality and breaking the memory bottleneck of transformer on time series forecasting. _Advances in neural information processing systems_, 32, 2019.
* Wu et al. (2021) Haixu Wu, Jiehui Xu, Jianmin Wang, and Mingsheng Long. Autoformer: Decomposition transformers with auto-correlation for long-term series forecasting. _Advances in Neural Information Processing Systems_, 34:22419-22430, 2021.
* Zhou et al. (2022b) Tian Zhou, Ziqing Ma, Qingsong Wen, Liang Sun, Tao Yao, Wotao Yin, Rong Jin, et al. Film: Frequency improved legendre memory model for long-term time series forecasting. _Advances in Neural Information Processing Systems_, 35:12677-12690, 2022b.
* Hochreiter and Schmidhuber (1997) Sepp Hochreiter and Jurgen Schmidhuber. Long short-term memory. _Neural computation_, 9(8):1735-1780, 1997.
* Chung et al. (2014) Junyoung Chung, Caglar Gulcehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation of gated recurrent neural networks on sequence modeling. In _NIPS 2014 Workshop on Deep Learning, December 2014_, 2014.
* Rangapuram et al. (2018) Syama Sundar Rangapuram, Matthias W Seeger, Jan Gasthaus, Lorenzo Stella, Yuyang Wang, and Tim Januschowski. Deep state space models for time series forecasting. _Advances in neural information processing systems_, 31, 2018.
* Salinas et al. (2020) David Salinas, Valentin Flunkert, Jan Gasthaus, and Tim Januschowski. Deepar: Probabilistic forecasting with autoregressive recurrent networks. _International Journal of Forecasting_, 36(3):1181-1191, 2020.
* Yang et al. (2019)Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. On the difficulty of training recurrent neural networks. In _International conference on machine learning_, pages 1310-1318. Pmlr, 2013.
* Box and Jenkins (1968) George EP Box and Gwilym M Jenkins. Some recent advances in forecasting and control. _Journal of the Royal Statistical Society. Series C (Applied Statistics)_, 17(2):91-109, 1968.
* Taylor and Letham (2018) Sean J Taylor and Benjamin Letham. Forecasting at scale. _The American Statistician_, 72(1):37-45, 2018.
* Athanasopoulos and Hyndman (2020) G Athanasopoulos and RJ Hyndman. Forecasting: principle and practice: O texts; 2018, 2020.
* Kitaev et al. (2020) Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. _arXiv preprint arXiv:2001.04451_, 2020.
* Chang et al. (2017) Shiyu Chang, Yang Zhang, Wei Han, Mo Yu, Xiaoxiao Guo, Wei Tan, Xiaodong Cui, Michael Witbrock, Mark A Hasegawa-Johnson, and Thomas S Huang. Dilated recurrent neural networks. _Advances in neural information processing systems_, 30, 2017.
* Yu and Liu (2018) Zeping Yu and Gongshen Liu. Sliced recurrent neural networks. In _Proceedings of the 27th International Conference on Computational Linguistics_, pages 2953-2964, 2018.
* Shen et al. (2018) Lifeng Shen, Qianli Ma, and Sen Li. End-to-end time series imputation via residual short paths. In _Asian conference on machine learning_, pages 248-263. PMLR, 2018.
* Kingma and Ba (2014) Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. _arXiv preprint arXiv:1412.6980_, 2014.
* Paszke et al. (2019) Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. _Advances in neural information processing systems_, 32, 2019.

A Brief Overview of RNN-Based and Transformer-Based Models

In this section, we will provide a brief overview of representative variants of RNN-based methods (Appendix A.1) and Transformer-based methods (Appendix A.2). Detailed descriptions of other methods can be found in Section 2.

### RNN-based methods

LSTM (Hochreiter and Schmidhuber, 1997) and GRU (Chung et al., 2014) are two prominent variants of RNN-based models. They have gained popularity in diverse domains, such as natural language processing, speech recognition, and video analysis due to their ability to capture important information. This subsection, we will provide a detailed introduction to LSTM and GRU.

LstmThe structure of LSTM, shown in Figure 6(a), comprises three gates: the input gate, the forget gate and the output gate. In addition, LSTM has two vectors: the hidden state and the cell state, for recursively processing the information flow between different time steps. LSTM can be formalized as follows:

\[\begin{split} f_{t}^{\text{lstm}}&=\sigma(W_{f}^{ \text{lstm}}[h_{t-1},\;x]+b_{f}^{\text{lstm}})\\ i_{t}^{\text{lstm}}&=\sigma(W_{i}^{\text{lstm}}[h_{t -1},\;x]+b_{i}^{\text{lstm}})\\ \widetilde{C}_{t}&=\text{tanh}(W_{c}^{\text{lstm}}[h_ {t-1},\;x]+b_{c}^{\text{lstm}})\\ o_{t}^{\text{lstm}}&=\sigma(W_{o}^{\text{lstm}}[h_ {t-1},\;x]+b_{o}^{\text{lstm}})\\ C_{t}&=f_{t}^{\text{lstm}}\odot C_{t-1}+i_{t}^{ \text{lstm}}\odot\widetilde{C_{t}}\\ h_{t}&=o_{t}^{\text{lstm}}\odot\text{tanh}(C_{t}), \end{split}\] (6)

where \(f_{t}^{\text{lstm}}\), \(i_{t}^{\text{lstm}}\) and \(o_{t}^{\text{lstm}}\) represent the input gate, the forget gate, and the output gate. \(x\in\mathbb{R}^{d_{\text{in}}}\) represents the input, and \(C_{t-1}\), \(h_{t-1}\in\mathbb{R}^{d_{\text{model}}}\) represents the input cell state and the input hidden state. \(W_{*}^{\text{lstm}}\in\mathbb{R}^{d_{\text{model}}\times(d_{\text{model}}+d_{ \text{in}})}\) are weight matrices and \(b_{*}^{\text{lstm}}\in\mathbb{R}^{d_{\text{model}}}\) are bias vectors. \(C_{t}\) and \(h_{t}\) denote the output cell state and the output hidden state; \(\widetilde{C_{t}}\) represents the intermediate variables of the calculation; \(\odot\) denotes an element-wise product; \(\sigma(\cdot)\) and \(\text{tanh}(\cdot)\) denote the sigmoid and tanh activation function.

GruThe structure of GRU, depicted in Figure 6(b), consists of two gates: the reset gate and the update gate. Besides, GRU has one vector: the hidden state. It can be mathematically formulated as follows:

\[\begin{split} r_{t}^{\text{gru}}&=\sigma(W_{r}^{ \text{gru}}[h_{t-1},\;x]+b_{r}^{\text{gru}})\\ u_{t}^{\text{gru}}&=\sigma(W_{u}^{\text{gru}}[h_ {t-1},\;x]+b_{u}^{\text{gru}})\\ \tilde{h}_{t}^{\text{gru}}&=\text{tanh}(W_{h}^{ \text{gru}}[r_{t}^{\text{gru}}\odot h_{t-1},\;x]+b_{h}^{\text{gru}})\\ h_{t}&=(1-u_{t}^{\text{gru}})\odot h_{t-1}+u_{t}^{ \text{gru}}\odot\tilde{h}_{t}^{\text{gru}},\end{split}\] (7)

Figure 6: The structure of LSTM and GRU.

where \(r_{t}^{\text{gru}}\) and \(u_{t}^{\text{gru}}\) represent the reset gate and the update gate. \(x\in\mathbb{R}^{d_{\text{in}}}\) represents the input, and \(h_{t-1}\in\mathbb{R}^{d_{\text{model}}}\) represent the input hidden state. \(W_{*}^{\text{gru}}\in\mathbb{R}^{d_{\text{model}}\times(d_{\text{model}}+d_{ \text{in}})}\) are weight matrices and \(b_{\theta}^{\text{gru}}\in\mathbb{R}^{d_{\text{model}}}\) are bias vectors. \(h_{t}\) denotes the output hidden state; \(\widetilde{h}_{t}^{\text{gru}}\) represents the intermediate variables of the calculation; \(\odot\) denotes an element-wise product; \(\sigma(\cdot)\) and \(\text{tanh}(\cdot)\) denote the sigmoid and \(\tanh\) activation function.

### Transformer-based methods

In recent years, variants of Transformer have made significant progress in time series forecasting and can be divided into two categories.

The first category captures the correlation between different time steps through point-wise attention mechanisms. Vanilla Transformer (Vaswani et al., 2017) is able to effectively extract correlations between any two-time steps through its self-attention mechanism, but its complexity is too high, reaching \(\mathcal{O}(L^{2})\). Informer (Zhou et al., 2021) proposes a ProbSparse self-attention with distilling techniques, reducing the complexity to \(\mathcal{O}(L\log L)\). FEDformer (Zhou et al., 2022) takes a different approach by applying attention modules in the frequency domain through Fourier or wavelet transformations, achieving a complexity of \(\mathcal{O}(L)\). Despite the performance and efficiency improvements achieved by these representative methods, they face difficulties in extracting sufficient semantic information from a single time point, which has been raised in previous works (Nie et al., 2023; Wu et al., 2023).

The second category adopts non-dot-product techniques to extract correlation information. LogTrans (Li et al., 2019) effectively captures local information by leveraging LogSparse and convolutional self-attention layers. However, it overlooks the long-term repetitive pattern of points among subseries and exhibits a high complexity of \(\mathcal{O}(L\log L)\). Drawing from the traditional ideas of time-series analysis, Autoformer (Wu et al., 2021) is able to capture long-range trends and short-range changes in sequences through decomposition and auto-correlation mechanisms. However, its complexity remains high at \(\mathcal{O}(L\log L)\). Pyraformer (Liu et al., 2021) initializes coarser-scale node information in the pyramidal graph by capturing local and global correlations through convolution. It then employs pyramidal attention to effectively capture long- and short-term repetitive patterns, leading to significant performance improvements while reducing complexity to \(\mathcal{O}(L)\). However, Pyraformer still suffers from the limitation of the receptive field of the convolution kernel as discussed earlier. PatchTST (Nie et al., 2023) captures local semantic information through patch, and further reduces the complexity to \(\mathcal{O}((L/S)^{2})\), where \(S\) represents the stride length. Nevertheless, it does not consider the long-term repeated pattern among subseries.

Based on the above analysis, we find that most Transformer-based methods struggle to simultaneously effectively model local and global correlations (global-local semantic information) and capture long- and short-term repetitive patterns (periodic semantic information) in sequences. Furthermore, it is worth noting that the majority of the aforementioned methods fail to achieve lower complexity than RNN-based methods. For a comprehensive comparison of the theoretical time and memory complexity, as well as experimental results, please refer to Appendix C.

## Appendix B Inspiration and Our Approach

RNNs, such as LSTM and GRU, have a significant advantage in capturing semantic information due to their recurrent structure, as discussed in Appendix A.1. Yet RNNs still have some drawbacks. In addition to the well-known issues of gradient vanishing/exploding (Pascanu et al., 2013). This paper focuses on analyzing the information forgetting problem (Appendix B.2) and the difficulty in parallelizing RNN (Appendix C). In this section, we will analyze RNNs using LSTM and GRU as representatives, and introduce the inspiration we draw from during the process of designing our model.

### LSTM vs GRU

LSTM incorporates three gates (input gate, output gate, and forget gate), while GRU utilizes two gates (reset gate and update gate). The complex gating mechanism of LSTM enables it to capture and learn intricate patterns, making it effective for processing longer sequences. On the other hand, GRUperforms well on shorter sequences and requires less computational and memory resources compared to LSTM. It should be noted that, as shown in Equation 7, the computation of \(\widetilde{h_{t}}\) in GRU is dependent on the completion of \(r_{t}\) computation. This undoubtedly slows down the computation that involves the input information \(x\). Therefore, in terms of actual running time, GRU may not necessarily have an advantage over LSTM, as detailed in Appendix C.2.

### The information forgetting problem of RNNs

When Recurrent Neural Networks (RNNs) process sufficiently long sequences, the information contained in the earlier parts of the sequence may be forgotten within the recurrent structure as it passes through multiple cells. To verify this phenomenon, we conducted a concrete experiment. We conducted a benchmark experiment using the ECL dataset, with a historical sequence length of 720 and a prediction horizon of 720, and the description of the ECL dataset can be found in Appendix E.

To eliminate the influence caused by the split of training, validation and test sets, we partitioned the dataset with a fixed input sequence length of 720 and a prediction horizon of 720. Furthermore, we conducted experiments with varying input sequence lengths using LSTM and GRU models. For the experiments, we used the last \(m=\{24,48,72,\ldots,696,720\}\) points near the prediction time as input, and the results of these experiments are shown in Figure 7.

Observing Figure 7, on the one hand, we can see that when the input sequence length is relatively short (not within the red box), the prediction performance of LSTM and GRU gradually improves as sequence length increases. This indicates that longer input sequences can provide more contextual information, which can enhance the model's prediction performance and is consistent with previous research findings [Liu et al., 2021, Zeng et al., 2023]. On the other hand, the part within the red box indicates that when longer historical sequences are fed into LSTM and GRU, their prediction performance does not continue to improve. This suggests that introducing earlier information does not provide significant benefits for the models, which in turn confirms the problem of information forgetting in RNNs.

### Inspiration and WITRAN

Our proposed Water-wave Information Transmission (WIT) framework utilizes input rearrangement to capture both long- and short-term repeating patterns in the sequence more directly and efficiently, while avoiding the processing of excessively long sequences, as described in Section 3.1. This successfully addresses the limitations of RNNs, such as the problems of exploding and vanishing gradients, as well as information forgetting. Furthermore, through the design of Recurrent Acceleration Network (RAN), we enable parallel computation of the entire sequence, as described in Section 3.3. For a comprehensive understanding of RAN, we provide detailed code and analysis of RAN in Appendix D. In this subsection, we focus on introducing the inspiration behind the important components of WIT.

The specific structure of WIT has been detailed in Section 3.2. The inspiration behind the design of Horizontal Vertical Gated Selective Unit (HVGSU) and Gated Selective Cell (GSC) comes from the advantages of LSTM and GRU. HVGSU serves two major purposes: 1) capturing both long- and short-term periodic semantic information in the sequence, and 2) modeling these two types

Figure 7: The issue of information forgetting in LSTM and GRU.

of periodic semantic information simultaneously for each time step. To achieve these objectives, two separate GSCs are established in the HVGSU, each responsible for the fusion and selection of long-term and short-term information in both directions.

For each GSC cell, to selectively incorporate fused information into the periodic semantic information in each direction, we draw inspiration from the reset gate of GRU and design a selection gate to control the retention of original information and the addition of new information. Furthermore, we are inspired by the output gate of LSTM and design our output gate to output the updated information. To reduce computational costs while maintaining sufficient learning capacity, we draw inspiration from GRU and only utilize these two gates, resulting in smaller computational and memory overheads. However, we address the problem of waiting for information to be computed between gate calculations in GRU by parallelizing the gate calculation vectors with the input \(x\), which greatly accelerated the computation speed of the cell. In summary, we analyze and draw inspiration from the strengths of LSTM and GRU, and incorporated them into the design of our GSC.

### WITRAN VS other RNN-based models

There are also several other RNN models, such as DilatedRNN (Chang et al., 2017), SlicedRNN (Yu and Liu, 2018), and RIMP-LSTM (Shen et al., 2018), which also have certain advantages in handling sequences. To further compare the differences between **WITRAN** and these methods, we provide the comparison in Table 6:

**(1)**Efficiently (1 layer) model global correlations (a) When the dilations of DilatedRNN does not include the value 1, multiple layers need to be constructed to extract global correlations. (b) SlicedRNN improves efficiency to some extent by parallel processing of minimum subsequences, but it still requires the introduction of multiple layers to capture the global correlations of the sequence.

**(2)**Special design to capture long-term repetitive patterns (a) SlicedRNN is unable to capture long-term repetitive patterns among elements of sub-sequences. (b) Although RIMP-LSTM incorporates Residual Paths and Residual Sum Unit designs, it still cannot effectively extract long-term repetitive patterns.

**(3)**Using 1 layer to capture long- and short-term repetitive patterns simultaneously (a) Dilate-dRNN can capture long- and short-term repetitive patterns, but it requires the use of multiple layers to achieve this. (b) SlicedRNN and RIMP-LSTM are not particularly adept at handling long-term repetitive patterns, as mentioned in **(2)**.

**(4)**Well solve the gradient vanishing/exploding problem of RNN (a) In DilatedRNN (Chang et al., 2017), it states and provides formal proof that reducing the length of information paths between time steps can prevent the issues of gradient vanishing/exploding. DilatedRNN, SlicedRNN, and WITRAN tackle this problem by reducing the length of information transmission paths. (b) RIMP-LSTM addresses this issue by the designs of Residual Paths and Residual Sum Units.

## Appendix C Time and Memory Consumption of RNNs and Transformer

In this section, we derive the complexities of LSTM, GRU and Vanilla Transformer respectively, and analyze their actual running speeds and memory usage through experiments.

### The theoretical computational complexity

The structure diagrams of LSTM and GRU are shown in Figure 6, while the structure of Vanilla Transformer can be found in paper (Vaswani et al., 2017). Assuming the sequence length is \(L\), the

\begin{table}
\begin{tabular}{c c c c c} \hline \hline Advantages & DilatedRNN & SlicedRNN & RIMP-LSTM & WITRAN (ours) \\ \hline Efficiently (1 layer) model global correlations & ✗ & ✗ & ✓\((1)\) & ✓\((1)\) \\ \hline Special design to capture long-term repetitive patterns & ✓ & ✗ & ✗ & ✓ \\ \hline Using 1 layer to capture long- and short-term repetitive patterns simultaneously & ✗ & ✗ & ✗ & ✓ \\ \hline Well solve the gradient vanishing/exploding problem of RNN & ✓ & ✓ & ✓ & ✓ \\ \hline \hline \end{tabular}
\end{table}
Table 6: Advantages of **WITRAN** compared to **other RNN methods**.

input dimension is \(d_{\rm in}\) and the hidden layer dimension is \(d_{\rm model}\), and considering the case where each model has only one layer, the derivation process is as follows:

GruGRU includes two gates and three matrix multiplication operations, as mentioned in Appendix A.1. Each matrix multiplication operation has a complexity of \(d_{\rm model}\times(d_{\rm model}+d_{\rm in})\), although there are also \(\odot\) operations in GRU, and the complexity of element-wise multiplication is \(d_{\rm model}\), which is much smaller than the complexity of matrix multiplication operation. Therefore, the complexity of one GRU unit can be simplified as \(3\times d_{\rm model}\times(d_{\rm model}+d_{\rm in})\). When GRU processes the entire sequence, it needs to perform \(L\) sequential computations, therefore, the overall computational complexity can be summarized as: \(3\times L\times d_{\rm model}\times(d_{\rm model}+d_{\rm in})\).

LstmLSTM includes three gates and four matrix multiplication operations, as mentioned in Appendix A.1. Similar to GRU, the computational complexity of LSTM can also be summarized as: \(4\times L\times d_{\rm model}\times(d_{\rm model}+d_{\rm in})\).

TransformerTaking the single-head attention mechanism in Transformer as an example, in the calculation of the attention mechanism, the each linear transformations of \(Query(Q)\), \(Key(K)\) and \(Value(V)\) require matrix multiplication operation of size \(d_{\rm model}\times d_{\rm model}\). In the calculation of the attention weights, the \(Q\) and transpose of \(K\) are multiplied together, and since the size of \(Q\) and \(K\) are \(L\times d_{\rm model}\), the complexity of computing the attention weights is \(L\times d_{\rm model}\times L\), and the size of attention weights is \(L\times L\). Similarly, when we multiply the attention weights with \(V\) in the computation, the complexity is \(L\times L\times d_{\rm model}\). The output from the attention matrix multiplication is further linearly transformed, and thus requires an additional \(d_{\rm model}\times d_{\rm model}\) complexity. To summarize, the overall complexity of the single-headed attention computation in Transformer can be expressed as: 1) Linear transformations of \(Q\), \(K\) and \(V\): \(3\times L\times d_{\rm model}\times d_{\rm model}\). 2) Calculation of attention weights: \(L\times d_{\rm model}\times L\). 3) Matrix multiplication of attention weights and \(V\): \(L\times L\times d_{\rm model}\). 4) Linear transformation of the output: \(L\times d_{\rm model}\times d_{\rm model}\). The total complexity of the single-headed attention computation in Transformer is \(3\times L\times d_{\rm model}\times d_{\rm model}+L\times d_{\rm model} \times L+L\times d_{\rm model}+L\times d_{\rm model}+d_{\rm model} \times d_{\rm model}=4\times L\times{d_{\rm model}}^{2}+2\times L^{2}\times d_{ \rm model}\). It should be noted that the complexity improvements in Transformer-based methods only apply to the \(L^{2}\) regularization term, as the linear transformations are necessary for any Transformer-based model. In addition, assuming the dimension of the Feed-Forward Networks (FFN) in Transformer is \(d_{\rm ff}\), the first layer of the fully connected network has a complexity of \(L\times d_{\rm model}\times d_{\rm ff}\), while the second layer has a complexity of \(L\times d_{\rm ff}\times d_{\rm ff}\), and the total complexity of FFN is \(L\times d_{\rm model}\times d_{\rm ff}+L\times{d_{\rm ff}}^{2}\). It should be noted that the input and output transformations in Transformer still require a certain level of complexity. On the hand, as it needs to transform the input data from dimension \(d_{\rm in}\) to dimension \(d_{\rm model}\), the input transformation in Transformer requires at least a \(L\times d_{\rm in}\times d_{\rm model}\) complexity. On the other hand, as it needs to transform the input data from dimension \(d_{\rm ff}\) to dimension \(d_{\rm model}\), the output transformation requires a \(L\times d_{\rm ff}\times d_{\rm model}\) complexity. Therefore, the overall complexity of Transformer can be summarized as: \(L\times d_{\rm in}\times d_{\rm model}+4\times L\times{d_{\rm model}}^{2}+2 \times L^{2}\times d_{\rm model}+L\times{d_{\rm model}}\times{d_{\rm ff}}+L \times{d_{\rm ff}}^{2}+L\times{d_{\rm ff}}\times{d_{\rm model}}\). It should be noted that in most cases for Transformer, \(d_{\rm ff}\) is larger than \(d_{\rm model}\).

SummaryIn time series prediction tasks, the input dimension \(d_{\rm in}\) is usually much smaller than the model dimension \(d_{\rm model}\). Based on the analysis conducted earlier, we can observe that the computational complexity of the encoder part of most Transformer-based methods is generally higher than that of LSTM and GRU. This is because Transformer includes many complex operations such as matrix multiplication and linear transformation. Although there have been improvements made to the attention mechanism, these modifications only affect a small portion of the overall complexity which remains high. Therefore, compared to LSTM and GRU, the computational complexity of Transformer is higher.

### The practical time and memory consumption

In practical situations, LSTM can perform computations in parallel without the need to wait for information from all four matrix operations. On the other hand, in GRU, the first two matrix operations can be parallelized, but the third matrix operation has a dependency on the completion of the previous operations, as mentioned in Appendix A.1. Therefore, the actual runtime of GRU may be longer than that of LSTM. To verify this, we compared the processing time and memory usage of a single-layer LSTM and a single-layer GRU on sequences of the same length using Python implementation, as shown in Figure 8.

Transformer has the advantage of parallelizing the computation of correlations for a sequence of length L, which greatly improves its speed compared to GRU/LSTM. It is important to note that this comparison is not entirely fair, as Transformer can parallelize at the sequence level, whereas RNNs can parallelize at the batch level. Each approach has its own strengths and is suited for different scenarios.

To ensure a fair comparison of runtime and memory usage, it is important to consider the conditions under which the models are compared. Following the experimental setup in FiLM [22], we can ensure that the hidden state dimension and batch size of the models being compared are the same. This allows us to evaluate whether the models can be trained on devices with low memory capacity. For time evaluation, we need to ensure that the hidden state dimension is the same and the memory utilization is similar, so that memory resources are not wasted. In terms of time comparison between Transformer and GRU/LSTM, although GRU/LSTM are slower than

Figure 8: Time and memory consumption of RNNs and Transformer.

Transformer under the same batch conditions in Python implementation, they can still speed up their parallel processing by increasing batch size while ensuring similar memory usage to Transformer. At the same time, because the computation process of GRU and LSTM is much simpler than that of Transformer, they can be more easily implemented in C++ and integrated into PyTorch's library as nn.GRU/nn.LSTM, which is faster. For comparison, we compared the memory and time costs of these five methods, as shown in Figure 8. The Python implementation of GRU and LSTM code is shown in Algorithm 1 and Algorithm 2.

``` importtorch importtorch.nn as nn importtorch.nn.functional as F classManual_LSTM(nn.Module): def_init_(self, input_size, hidden_size, dropout): super(Manual_LSTM, self)_init_() self.hidden_size = hidden_size self.dropout = dropout self.gates = nn.Linear(input_size+hidden_size, hidden_size * 4) self.igmoid = nn.Sigmoid() self.tanh = nn.Tanh() forparaminself.parameters(): ifparam.dim()>1: nn.init.xavier_uniform_(param) defforward(self, x): batch_size = x.size(0) seq_len = x.size(1) h, c = (torch.zeros(batch_size, self.hidden_size).to(x.device) for_inrange(2)) y_list = [] fori inrange(seq_len): forget_gate, input_gate, output_gate, candidate_cell = self.gates(torch.cat([x[:, i, :], h], dim=-1)).chunk(4, -1) forget_gate, input_gate, output_gate = (self.sigmoid(g) forg in (forget_gate, input_gate, output_gate)) c = forget_gate * c + input_gate * self.tanh(candidate_cell) h = output_gate * self.tanh(c) y_list.append(h) output = F.dropout(torch.stack(y_list, dim=1), self.dropout, self.training) returnoutput, h, c ```

**Algorithm 2** LSTM based on Python

## Appendix D The Design Process and Implementation Code of Recurrent Acceleration Network (RAN)

In this section, we will provide a detailed explanation of the implementation approach (Appendix D.1) and specific code of RAN (Appendix D.2).

### Implementation approach of RAN

In Section 3.3, we introduced the Recurrent Acceleration Network (RAN) as part of our overall approach. However, we encountered a challenge when the number of rows processed by RAN is smaller than the number of columns. In such cases, if the number of slices processed exceeds the number of rows, the intermediate variables of the slices become fully occupied. To ensure the continuity of operations, it becomes necessary to continuously shift and reset these variables. However, we found that this reset operation was time-consuming and hindered the expected acceleration benefits of RAN. To overcome this issue, we made a simple adjustment by setting the size of the slice to the maximum value of the rows and columns. While this may introduce additional space complexity, it maintains the length of the slice and eliminates the need for a reset operation. As a result, the processing time is significantly reduced. The comparison diagram between RAN-min and RAN-max is shown in Figure 9.

### Specific code of RAN

The specific code of RAN can be found in Algorithm 3. Due to its long content, we place it at the end of the Appendix.

## Appendix E Dataset Details

In this section, we will provide a detailed introduction of the datasets used in this paper. (1) _Electricity2_ (_ECL_) contains the hourly electricity consumption of 321 customers from 2012 to 2014. (2) _ETT_[21] contains the load and oil temperature, which was recorded every 15 minutes between July 2016 and July 2018 from electricity transformers. (3) _Traffic3_ contains the hourly data the road occupancy rates measured by different sensors on San Francisco Bay area freeways, collected from California Department of Transportation. (4) _Weather4_ contains 21 meteorological indicators (such as air temperature, humidity, etc.) and was recorded every 10 minutes for 2020 whole year. They are all split into the training set, validation set and test set by the ratio of 6:2:2 during modeling.

Footnote 2: https://archive.ics.uci.edu/ml/datasets/ElectricityLoadDiagrams20112014

Footnote 3: http://pems.dot.ca.gov

Footnote 4: https://www.bgc-jena.mpg.de/weitter/

Due to the different original acquisition granularities of each dataset, in order to ensure that they contain the same semantic information on the same task, in this paper, our experiments are conducted by aggregating them into one hour. Table 7 summarizes all the features of the four benchmark datasets. The target value of ETT is 'oil temperature (OT)', the target value of ECL is 'MT_320', the target value of Traffic is 'Node_862' and the target value of Weather is 'wet_bul'.

\begin{table}
\begin{tabular}{c c c c c} \hline \hline Dataset & Sample Length & Dimension & Usage Frequency & Recorded Frequency \\ \hline ETT & 17420 & 8 & 1h & 15min \\ ECL & 26304 & 322 & 1h & 1h \\ Traffic & 17544 & 863 & 1h & 1h \\ Weather & 35064 & 22 & 1h & 10min \\ \hline \hline \end{tabular}
\end{table}
Table 7: The details of datasets.

Figure 9: The figure shows two designs of RANs side by side. The left one represents the case where the number of slices is the minimum value between the number of rows and columns, while the right one represents the case where the number of slices is the maximum value between the number of rows and columns.

Experiment Setup Details

Our method and all the baseline methods are trained with the L2 loss, using the ADAM [Kingma and Ba, 2014] optimizer with an initial learning rate of 10-3. Batch size is set to 32. The training process is early stopped after 5 epochs if there is no loss degradation on the validation set and the max epochs is 25. We save all the models with the lowest loss on the validation set for the final testing.

To ensure a fair comparison of each model's performance, we set the same search space for the common parameters included in each model. This ensures that each model can achieve its best performance under the same conditions for comparison, with the seed set to 2023.

Regarding the private parameters of each model, we set the corresponding search space according to the specific descriptions given in their respective papers. In addition, it should be emphasized that the performance of Pyraformer shown in this paper is the best between the two prediction designs of the model, the performance of FEDformer shown here is the best between FEDformer-w and FEDformer-f, the performance of DLinear shown here is the best among DLinear, NLinear and Linear, and the performance of MICN is the best between MICN-regre and MICN-mean.

The mean square error (MSE) and mean absolute error (MAE) are used as metrics. All experiments are repeated 5 times and the mean of the metrics is reported as the final results in Table 3 and Table 4.

All the deep learning networks are implemented using PyTorch [Paszke et al., 2019] and conducted on NVIDIA RTX A4000 16GB GPUs.

The search space for the common parameters is as follows: (1) \(d_{\mathrm{model}}\) is set to {32, 64, 128, 256, 512, 1024}. (2) \(e_{\mathrm{layer}}\) and \(d_{\mathrm{layer}}\) are set to {1, 2, 3}. (3) \(n_{\mathrm{head}}\) is set to {1, 2, 4, 8} and \(d_{\mathrm{ff}}\) is four times as large as \(d_{\mathrm{model}}\). Regarding the private parameters of our model, \(norm\) is set to {0, 1} and \(C\) represents the period is set to {12, 24, 48} and it is required that the length \(H\) of the input sequence is divisible by \(C\). Furthermore, it is important to note that adjustments should be made to \(C\) for different tasks to ensure its ability to adequately represent the period.

## Appendix G Ablation Study

In Section 4.3, we presented the results of our ablation experiments to demonstrate the effectiveness of each component in the Water-wave Information Transmission (WIT) framework. We design five variant methods and evaluate their performance on four datasets. The experimental results are summarized in Table 8, and we will provide a detailed analysis of these results in this section. It is important to note that the role of the Recurrent Acceleration Network (RAN) is primarily to improve computational efficiency, as demonstrated in Section 4.2, and it does not significantly impact the model's accuracy. Therefore, our focus in this section will be on discussing the design and role of the WIT framework.

### Impact of forecasting module

Our proposed forecasting module is also a part of the model design, and its goal is to effectively capture periodic semantic information and global-local semantic information obtained through the Water-wave Information Transmission (WIT) framework. Our specific design has been described in detail in Section 3.4. To verify that we can fully utilize the relevant information by combining operations for pairwise prediction, we changed the combination prediction method of this module to a simple way of first concatenating the horizontal hidden state and the vertical hidden state, and then using the fully connected prediction module for prediction. We named this variant WITRAN-FC.

The comparison of results between WITRAN and WITRAN-FC in Table 8 shows that when we remove the combining operations for pairwise prediction and replace it with ordinary concatenation and fully connected operations, even though the same information is used, WITRAN-FC has difficulty discovering the most relevant long-term periodic semantic information for each prediction point. This demonstrates the effectiveness of the combining operations for pairwise prediction in the forecasting module.

[MISSING_PAGE_FAIL:22]

hidden state are different. Therefore, WITRAN-2DLSTM and WITRAN-2DGRU are not heavily influenced by redundant information. In addition, the comparison of results between WITRAN and WITRAN-2DLSTM/WITRAN-2DGRU shows that information fusion and selection can effectively extract relevant information, which demonstrates the necessity of the GSC design in WIT.

### Impact of bi-granular information transmission

In the WITRAN we designed, information can be propagated in two directions, and because the time granularity of sequence propagation in these two directions is different, their periodic semantic information is different. In order to verify the effectiveness of the two independent GSCs set up in HVGSU to capture long- and short-term periodic semantic information, we replaced the cells in the two directions of WITRAN-2DLSTM/WITRAN-2DGRU with a single LSTM/GRU cell, and uniformly used the hidden state information of one point to transmit information in both directions. It should be noted that at this time, the information propagated horizontally and vertically by the model is completely the same. Sure, we can name the current models as WITRAN-LSTM/WITRAN-GRU.

The comparison of results between WITRAN-2DLSTM/WITRAN-2DGRU and WITRAN-LSTM/WITRAN-GRU in Table 8 demonstrates that failing to distinguish between long- and short-term periodic semantic information can have a significant negative impact on the performance of prediction tasks.

### Summary

In this section, we focused on the importance of each building module of WIT. During the experiments, we gradually removed the building blocks we designed, and it was clear that the performance decreased significantly. This fully demonstrates the rationality and effectiveness of the WIT framework we designed. Specifically, the summary of WIT is as follows: (1) The fusion and selection design of GSC enables both long- and short-term semantic information to be captured for each historical input data point. (2) Setting up independent cells in both directions enables to extract semantic information in the long- and short-term respectively. (3) The specially designed combined operation is able to make fuller use of the captured local and global semantic information while ensuring that the information is not redundant.

## Appendix H Experiment Error Bars

We saved all models and used the model with the lowest validation loss for the final testing. We repeated this process 5 times and calculated the error bars for all models to compare their robustness on long-range and ultra-long-range tasks, as shown in Table 9. It can be seen from Table 9 that the overall performance of the proposed WITRAN is better than other state-of-the-art baseline models, indicating the effectiveness of our approach.

## Appendix I Model Analysis

In this section, we will analyze the parameter sensitivity (Appendix I.1) of our proposed model and provide a detailed explanation (Appendix I.2) of the time and memory consumption in Section 4.2.

### Parameter sensitivity

Regarding the private parameters of our model, \(norm\) is set to {0, 1} and \(C\) represents the period is set to {12, 24, 48} and it is required that the length \(H\) of the input sequence is divisible by \(C\). In this subsection, we will explain the choices of these parameters and their impact on the model's predictions.

The impact of \(norm\)For different prediction tasks on different datasets, we determined the value of \(norm\) through validation set. To verify that the value of \(norm\) conforms to the distribution of the datasets, we conducted experiments on the distribution of the training and validation sets for different prediction tasks, the results and the value of \(norm\) are shown in Table 10. We noticed that although there may be a significant difference in the mean values of the data between the training and validation 

[MISSING_PAGE_EMPTY:24]

The impact of \(C\)The choice of \(C\) for the model represents the determination of the sequence period. Due to the different original acquisition granularities of each dataset, in order to ensure that they contain the same semantic information on the same task, in this paper, our experiments are conducted by aggregating them into one hour. To facilitate the discovery of long-term repetitive patterns in the sequence, we set the \(C\) to 12, 24, 48, and the specific experimental results are shown in Table 11. From Table 11, we can clearly see that for sequences with time steps in hours, using a period of 24 hours (1 day) for division yields better results. This is because in WIT, information is transmitted horizontally at the granularity of hours, while information is transmitted vertically at the granularity of days. This approach can more fully capture the long- and short-term repetitive patterns and global/local correlations hidden in the time series data.

### A detailed explanation of time and memory consumption

The efficiency of the model is crucial, as even if the model's efficiency is high, it is limited if it does not have good predictive performance. Therefore, in this paper, we only compared our proposed method with methods that have good or the latest results. It should be noted that, for example, Transformer (Vaswani et al., 2017), Informer (Zhou et al., 2021) and Autoformer (Wu et al., 2021) have been shown in previous works to have higher complexity than FiLM (Zhou et al., 2022), so we only compared our proposed method with FiLM, rather than comparing with Transformer, Informer and Autoformer again.

Memory UsageAs shown in Figure 5(a) and (c), WITRAN has good memory usage with the prolonging the input length and output length. For a fair comparison, we fix the experimental settings of all methods, where we fix the input length as 720 and prolong the output length. Moreover, we fix the output length as 720 and prolong the input length. And for each model, to achieve good performance, it is important to have a sufficiently large search space to select the most suitable parameters for the model, in order to achieve the best possible results. Therefore, we set all public parameters to their upper limits to test the memory usage of each model. To allow as many models as possible to be trained, we uniformly set the batch size to 8 for testing in this section. It should be noted that if a point is not shown on the graph, it indicates that the model encountered an "out of memory" (OOM) error at that particular configuration of input sequence length and output length.

\begin{table}
\begin{tabular}{c c c c c c c c c c c} \hline \hline \multicolumn{2}{c}{Settings for \(C\)} & \multicolumn{3}{c}{12} & \multicolumn{3}{c}{24} & \multicolumn{3}{c}{48} \\ \cline{2-11} Datasets & Tasks & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE \\ \hline \multirow{10}{*}{\begin{tabular}{c} \(\text{\text{\text{\text{\text{\text{\text{\text{\text{\text{\text{\text{\text{\text{\text{\text{\text{\text{\text{ \texttext{ }}}}}}} }{}}{}}}}}}}}\) \\ \end{tabular} } & 168-168 & 0.2461 & 0.3648 & 0.2397 & 0.3519 & - & - \\  & 168-336 & 0.3166 & 0.4230 & 0.2607 & 0.3721 & - & - \\  & 336-336 & 0.3217 & 0.4099 & 0.2517 & 0.3627 & 0.2634 & 0.3847 \\  & 336-720 & 0.3397 & 0.4310 & 0.3084 & 0.4055 & 0.3233 & 0.4130 \\  & 720-720 & 0.2802 & 0.3951 & 0.2478 & 0.3651 & 0.3175 & 0.4063 \\  & 720-1440 & 0.2907 & 0.4051 & 0.2499 & 0.3727 & 0.2985 & 0.3946 \\  & 1440-1440 & 0.3007 & 0.4110 & 0.2408 & 0.3680 & 0.2748 & 0.3768 \\  & 1440-2880 & 0.4301 & 0.5150 & 0.3359 & 0.4383 & 0.3464 & 0.4347 \\ \hline \multirow{10}{*}{
\begin{tabular}{c} \(\text{\text{\text{\text{\text{\text{\text{\text{\text{\text{\text{\text{\text{\text{\texttexttexttext{ \texttexttext{ \texttexttext  \text{  \texttexttexttext{ \texttexttexttexttexttexttexttexttexttexttexttexttexttexttext {    \As for TimesNet, due to its large memory usage, it cannot be trained even with a batch size of 1. Therefore, we did not include TimesNet in the Figure 5. From Figure 5(a), we can observe that when the input sequence is fixed, the memory usage of WITRAN and WIT is not significantly different since they both have the same prediction module. Additionally, we found that as the output length increases, the memory usage of WITRAN and WIT also increases linearly, but our proposed method still has the lowest memory usage. From Figure 5(c), we can observe that the memory usage of WITRAN increases very slowly with the increase in sequence length, and the memory usage of WITRAN is even lower than that of WIT. This is because WITRAN does not need to retain a large number of intermediate storage variables in the computation of each slice, while WIT needs to store the intermediate variables for the entire sequence. Through comparison with other methods, we can clearly see the advantages of WITRAN on memory consumption.

Training SpeedAs shown in Figure 5(b) and (d), WITRAN has a faster training speed than others with the prolonging the input length and output length. To ensure a fair comparison of the performance of each model, we fixed the experimental settings, which were the same as the settings used for testing memory usage. The experiment is performed on ECL dataset. Due to our extremely low space complexity, we followed the experimental settings used in FiLM (Zhou et al., 2022b) for comparing the training Speed. To fully utilize the GPU memory, our model can increase the batch size from 8 to 32 under the experimental settings mentioned above. Similarly, FiLM can also increase the batch size = 32. However, we still have the fastest training speed, and due to the role of RAN mainly being on the input sequence, our advantage can be clearly seen from Figure 5(d), which is related to the square root of the sequence length. Through comparison with other methods, we can clearly see the advantages of WITRAN on time consumption.

## Appendix J Showcases of Main Results

As shown in Figure 10 to Figure 49, we plot the forecasting results from the test set of all datasets for comparison. For the long-range forecasting task, we chose two suboptimal models, FiLM and Pyraformer. As for the ultra-long-range forecasting task, we also chose two suboptimal models, Pyraformer and TimesNet. Our model WITRAN gives the best performance among different models. Moreover, WITRAN is significantly better at modeling global and local correlations, as well as discovering long- and short-term repetitive patterns in the time series than other models.

Figure 10: Forecasting cases comparison of WITRAN, FiLM and Pyraformer on the 168-168 task of the ECL dataset.

Figure 11: Forecasting cases comparison of WITRAN, FiLM and Pyraformer on the 168-336 task of the ECL dataset.

Figure 12: Forecasting cases comparison of WITRAN, FiLM and Pyraformer on the 336-336 task of the ECL dataset.

Figure 13: Forecasting cases comparison of WITRAN, FiLM and Pyraformer on the 336-720 task of the ECL dataset.

Figure 14: Forecasting cases comparison of WITRAN, FiLM and Pyraformer on the 720-720 task of the ECL dataset.

Figure 15: Forecasting cases comparison of WITRAN, Pyraformer and TimesNet on the 720-1440 task of the ECL dataset.

Figure 16: Forecasting cases comparison of WITRAN, Pyraformer and TimesNet on the 1440-1440 task of the ECL dataset.

Figure 17: Forecasting cases comparison of WITRAN, Pyraformer and TimesNet on the 1440-2880 task of the ECL dataset.

Figure 18: Forecasting cases comparison of WITRAN, FiLM and Pyraformer on the 168-168 task of the Traffic dataset.

Figure 19: Forecasting cases comparison of WITRAN, FiLM and Pyraformer on the 168-336 task of the Traffic dataset.

Figure 20: Forecasting cases comparison of WITRAN, FiLM and Pyraformer on the 336-336 task of the Traffic dataset.

Figure 21: Forecasting cases comparison of WITRAN, FiLM and Pyraformer on the 336-720 task of the Traffic dataset.

Figure 22: Forecasting cases comparison of WITRAN, FiLM and Pyraformer on the 720-720 task of the Traffic dataset.

Figure 23: Forecasting cases comparison of WITRAN, Pyraformer and TimesNet on the 720-1440 task of the Traffic dataset.

Figure 24: Forecasting cases comparison of WITRAN, Pyraformer and TimesNet on the 1440-1440 task of the Traffic dataset.

Figure 25: Forecasting cases comparison of WITRAN, Pyraformer and TimesNet on the 1440-2880 task of the Traffic dataset.

Figure 26: Forecasting cases comparison of WITRAN, FiLM and Pyraformer on the 168-168 task of the ETTh1 dataset.

Figure 27: Forecasting cases comparison of WITRAN, FiLM and Pyraformer on the 168-336 task of the ETTh1 dataset.

Figure 28: Forecasting cases comparison of WITRAN, FiLM and Pyraformer on the 336-336 task of the ETTh1 dataset.

Figure 29: Forecasting cases comparison of WITRAN, FiLM and Pyraformer on the 336-720 task of the ETTh1 dataset.

Figure 30: Forecasting cases comparison of WITRAN, FiLM and Pyraformer on the 720-720 task of the ETTh1 dataset.

Figure 31: Forecasting cases comparison of WITRAN, Pyraformer and TimesNet on the 720-1440 task of the ETTh1 dataset.

Figure 32: Forecasting cases comparison of WITRAN, Pyraformer and TimesNet on the 1440-1440 task of the ETTh1 dataset.

Figure 33: Forecasting cases comparison of WITRAN, Pyraformer and TimesNet on the 1440-2880 task of the ETTh1 dataset.

Figure 34: Forecasting cases comparison of WITRAN, FiLM and Pyraformer on the 168-168 task of the ETTh2 dataset.

Figure 35: Forecasting cases comparison of WITRAN, FiLM and Pyraformer on the 168-336 task of the ETTh2 dataset.

Figure 36: Forecasting cases comparison of WITRAN, FiLM and Pyraformer on the 336-336 task of the ETTh2 dataset.

Figure 37: Forecasting cases comparison of WITRAN, FiLM and Pyraformer on the 336-720 task of the ETTh2 dataset.

Figure 38: Forecasting cases comparison of WITRAN, FiLM and Pyraformer on the 720-720 task of the ETTh2 dataset.

Figure 39: Forecasting cases comparison of WITRAN, Pyraformer and TimesNet on the 720-1440 task of the ETTh2 dataset.

Figure 40: Forecasting cases comparison of WITRAN, Pyraformer and TimesNet on the 1440-1440 task of the ETTh2 dataset.

Figure 41: Forecasting cases comparison of WITRAN, Pyraformer and TimesNet on the 1440-2880 task of the ETTh2 dataset.

Figure 42: Forecasting cases comparison of WITRAN, FiLM and Pyraformer on the 168-168 task of the Weather dataset.

Figure 43: Forecasting cases comparison of WITRAN, FiLM and Pyraformer on the 168-336 task of the Weather dataset.

Figure 44: Forecasting cases comparison of WITRAN, FiLM and Pyraformer on the 336-336 task of the Weather dataset.

Figure 45: Forecasting cases comparison of WITRAN, FiLM and Pyraformer on the 336-720 task of the Weather dataset.

Figure 46: Forecasting cases comparison of WITRAN, FiLM and Pyraformer on the 720-720 task of the Weather dataset.

Figure 47: Forecasting cases comparison of WITRAN, Pyraformer and TimesNet on the 720-1440 task of the Weather dataset.

Figure 48: Forecasting cases comparison of WITRAN, Pyraformer and TimesNet on the 1440-1440 task of the Weather dataset.

Figure 49: Forecasting cases comparison of WITRAN, Pyraformer and TimesNet on the 1440-2880 task of the Weather dataset.

``` importtorch.nn as nn importmath importtorch.nn.functional as F classWITRAN_HVGSU_Encoder(torch.nn.Module): def_init_(self, input_size, hidden_size, num_layers, dropout, water_rows, water_cols, res_mode=' none"): super(WITRAN_HVGSU_Encoder, self)_init_() self.input_size = input_size self.hidden_size = hidden_size self.num_layers = num_layers self.dropout = dropout self.water_rows = water_rows self.water_cols = water_cols self.res_mode = res_mode #parameter of now cell  self.W_first_layer = torch.nn.Parameter(torch.empty(6 * hidden_size, input_size + 2 * hidden_size))  self.W_other_layer = torch.nn.Parameter(torch.empty(num_layers - 1, 6 * hidden_size, 4 * hidden_size))  self.B = torch.nn.Parameter(torch.empty(num_layers, 6 * hidden_size))  self.reset_parameters() defreset_parameters(self):  stdv = 1.0 / math.sqrt(self.hidden_size) forweightinself.parameters():  weight.data.uniform_(-stdv, +stdv) deflinear(self,input, weight, bias, batch_size, **slice**, Water2sea_slice_num):  a = F.linear(**input**, weight) if**slice < Water2sea_slice_num:  a[:batch_size * (**slice** + 1), :] = a[:batch_size * (**slice** + 1), :] + bias return a defforward(self,input,batch_size, input_size, flag): if flag==1:#cols>rows input=input.permute(2, 0, 1, 3) else: input=input.permute(1, 0, 2, 3)  Water2sea_slice_num, _ Original_slice_len, _= input.shape Water2sea_slice_len = Water2sea_slice_num + Original_slice_len - 1  hidden_slice_row = torch.zeros(Water2sea_slice_num * batch_size, self.hidden_size).to(**input**. device) hidden_slice_col = torch.zeros(Water2sea_slice_num * batch_size, self.hidden_size).to(**input**.device input_transfer = torch.zeros(Water2sea_slice_num, batch_size, Water2sea_slice_len, input_size).to( input_device) forr inrange(Water2sea_slice_num):  input_transfer[r, :,r+Original_slice_len, :] = input[r, :, :, :]  hidden_row_all_list = []  hidden_col_all_list = [] ```

**Algorithm 3** Recurrent Acceleration Network (RAN)``` forlayerinrange(self.num_layers): iflayer==0: a=input_transfer.reshape(Water2sea_slice_num*batch_size, Water2sea_slice_len, input_size) W=self.W_first_layer else: a=F.dropout(output_all_slice, self.dropout, self.training) iflayer==1: layer0_output=a W=self.W_other_layer[]layer-1,:.:] hidden_slice_row=hidden_slice_row*0 hidden_slice_col=hidden_slice_col*0 B=self.B[layer:.] #starteveryforallslice output_all_slice_list=[] forsliceinrange(Water2sea_slice_len): #gategenerate #=self.linear(torch.cat([hidden_slice_row, hidden_slice_col, a[:, **slice**, :]), dim=-1), W, B, batch_size, **slice**, Water2sea_slice_num) #gate  sigmod_gate, tanh_gate=torch.split(gate, 4*self.hidden_size, dim=-1) sigmod_gate=torch.sigmoid(sigmod_gate) tanh_gate=torch.tanh(tanh_gate) update_gate_row, output_gate_row, update_gate_col, output_gate_col=sigmoid_gate.chunk (4, dim=-1) input_gate_row, input_gate_col=tanh_gate.chunk(2, dim=-1) #gateeffect hidden_slice_row=torch.tanh( (1-update_gate_row)*hidden_slice_row+update_gate_row+input_gate_row)* output_gate_row hidden_slice_col=torch.tanh( (1-update_gate_col)*hidden_slice_col+update_gate_col+input_gate_col)* output_gate_col output_slice=torch.cat([hidden_slice_row, hidden_slice_col], dim=-1) #saveoutput_all_slice_list.append(output_slice) #saverowhidden ifslice>=Original_slice_len-1: need_save_row_loc=**slice**-Original_slice_len+1 hidden_row_all_list.append( hidden_slice_row|need_save_row_loc*batch_size:(need_save_row_loc+1)* batch_size,:)) #savecolhidden ifslice>=Water2sea_slice_num-1: hidden_col_all_list.append( hidden_slice_col[(Water2sea_slice_num-1)*batch_size:.]) #hidden_transfer= hidden_slice_col=torch.(hidden_slice_col, shifts=batch_size, dims=0) ifself.res_mode=='layer_res'andlayer>=1:#layer-res output_all_slice=torch.stack(output_all_slice_list, dim=1)  else: output_all_slice=torch.stack(output_all_slice_list, dim=1) hidden_row_all=torch.stack(hidden_row_all_list, dim=1) hidden_col_all=torch.stack(hidden_col_all_list, dim=1) hidden_row_all_=hidden_row_all.reshape(batch_size, self.num_layers, Water2sea_slice_num, hidden_row_all.shape[-1]) hidden_col_all=hidden_col_all.reshape(batch_size, self.num_layers, Original_slice_len, hidden_col_all.shape[-1]) iflag==1: returnoutput_all_slice, hidden_col_all, hidden_row_all else: returnoutput_all_slice, hidden_row_all, hidden_col_all