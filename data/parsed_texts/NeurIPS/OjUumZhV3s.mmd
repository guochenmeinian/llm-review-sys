# MMed-RAG: Versatile Multimodal RAG System for Medical Vision Language Models

 Peng Xia\({}^{1}\), Kangyu Zhu\({}^{5}\), Haoran Li\({}^{6}\), Tianze Wang\({}^{3}\), Weijia Shi\({}^{4}\),

Sheng Wang\({}^{4}\), Linjun Zhang\({}^{3}\), James Zou\({}^{2}\), Huaxiu Yao\({}^{1}\)

\({}^{1}\)UNC-Chapel Hill, \({}^{2}\)Stanford University, \({}^{3}\)Rutgers University,

\({}^{4}\)University of Washington, \({}^{5}\)Brown University, \({}^{6}\)PloyU

{pxia,huaxiu}@cs.unc.edu

###### Abstract

Artificial Intelligence (AI) has demonstrated significant potential in healthcare, particularly in disease diagnosis and treatment planning. Recent progress in Medical Large Vision-Language Models (Med-LVLMs) has opened up new possibilities for interactive diagnostic tools. However, these models often suffer from factual hallucination, which can lead to incorrect diagnoses. Fine-tuning and retrieval-augmented generation (RAG) have emerged as methods to address these issues. However, the amount of high-quality data and distribution shifts between training data and deployment data limit the application of fine-tuning methods. Although RAG is lightweight and effective, existing RAG-based approaches are not sufficiently general to different medical domains and can potentially cause misalignment issues, both between modalities and between the model and the ground truth. In this paper, we propose a versatile multimodal RAG system, MMed-RAG, designed to enhance the factuality of Med-LVLMs. Our approach introduces a domain-aware retrieval mechanism, an adaptive retrieved contexts selection, and a provable RAG-based preference fine-tuning strategy. These innovations make the RAG process sufficiently general and reliable, significantly improving alignment when introducing retrieved contexts. Experimental results across five medical datasets (involving radiology, ophthalmology, pathology) on medical VQA and report generation demonstrate that MMed-RAG can achieve an average improvement of 43.8% in the factual accuracy of Med-LVLMs.

## 1 Introduction

Artificial Intelligence (AI) has already transformed healthcare and still has a lot of potential for further advancements (Tautan et al., 2021; Wang et al., 2019; Ye et al., 2021; Tu et al., 2024). Recently, Medical Large Vision-Language Models (Med-LVLMs) have shown great promise for advancing interactive and intelligent diagnosis (Li et al., 2023; Moor et al., 2023; Zhang et al., 2023; Wu et al., 2023). Despite this potential (Li et al., 2023; Wu et al., 2023; Shi et al., 2024), current Med-LVLMs still face significant reliability issues, particularly their tendency to generate non-factual medical responses (Xia et al., 2024; Royer et al., 2024; Chen et al., 2024; Jiang et al., 2024), making them unreliable in critical medical applications. These factuality issues raise serious concerns when deploying such models in clinical settings, where even small diagnostic errors could lead to severe consequences for patient care.

Recently, researchers have begun to focus on improving the factuality of Med-LVLMs through various techniques, including fine-tuning (Li et al., 2023; Moor et al., 2023; Thawkar et al., 2023; Zhang et al., 2023; Chen et al., 2024) and retrieval-augmented generation (RAG) (Xia et al., 2024; He et al., 2024; Sun et al., 2024). Fine-tuning is a direct method to improve model performance, but it faces several limitations in the medical field. First, there is a lack of sufficient high-quality labeled data for fine-tuning in the medical domain. Additionally, a distribution gap often exists between the training data and the real-world deployment data (Schrouff et al., 2022), leading to significantly worse model performance during deployment. Hence, RAG has emerged as a viable alternative by providing external references during the inference stage, enhancing the factuality of Med-LVLMs (Wu et al., 2023; Gao et al., 2023). However, despite its advantages, current RAG implementations in Med-LVLMs have significant limitations. First, these methods tend tobe _dataset-specific_, reducing their generalizability across various medical domains. Second, these models are still facing _misalignment issues_ that lead to factuality problems. This misalignment may arise from the impact of adding RAG on the original Med-LVLMs' _cross-modality alignment_, as well as on the _overall alignment_ between the model and ground truth.

To address these challenges, we propose a versatile factual **M**ultimodal **M**edical **RAG** system called **MMed-RAG**. Specifically, MMed-RAG first introduces a domain-aware retrieval mechanism, designed to handle different domains of medical images more effectively. Here, we design a domain identification module to adaptively select a corresponding retrieval model given the input medical image. Secondly, we include a adaptive calibration approach for selecting the number of retrieved contexts. Lastly, MMed-RAG incorporates RAG-based preference fine-tuning to enhance cross-modality alignment and overall alignment with ground truth. The preference pairs are designed to achieve two goals: first, to improve cross-modality alignment by encouraging the model to avoid generating responses without utilizing input medical images, even the responses are correct; second, to improve overall alignment by encouraging the model to understand retrieved contexts when unsure, while avoiding interference from irrelevant retrieved information.

The primary contribution of this paper is MMed-RAG, a versatile multimodal RAG system designed specifically for Med-LVLMs to generate more factual responses. Under mild assumptions, our theoretical analysis demonstrates that MMed-RAG mitigates both cross-modality misalignment and overall misalignment with ground truth. Furthermore, empirical results on five medical multimodal datasets, covering three medical image modalities (radiology, pathology, and ophthalmology), show that MMed-RAG significantly improves the factual accuracy of Med-LVLMs, achieving improvements of 18.5% and 69.1% on Medical VQA and report generation tasks, respectively, compared to the original Med-LVLM. These empirical findings further demonstrate the effectiveness of our proposed components and support the theoretical analysis in addressing misalignment issues.

## 2 Preliminaries

In this section, we will provide a brief overview of Med-LVLMs and preference optimization.

**Medical Large Vision Language Models**. Med-LVLMs bridge LLMs with medical visual modules, allowing the model to take medical image \(x_{v}\) and clinical query \(x_{t}\) as input \(x\), and autoregressively predict the probability distribution of the next token. The text output is denoted as \(y\).

**Preference Optimization**. Preference optimization has achieved remarkable results in LLM alignment. Give an input \(x\), a language model policy \(\pi_{\theta}\) can produce a conditional distribution \(\pi_{\theta}(y\mid x)\) with \(y\) as the output text response. The recently popular DPO (Rafailov et al., 2023) utilizes preference data achieve objective alignment in LLMs. The preference data is defined as \(\mathcal{D}=\{x^{(i)},y_{v}^{(i)},y_{v}^{(i)}\}_{i=1}^{N}\), where \(y_{v}^{(i)}\) and \(y_{v}^{(i)}\) represent preferred and dispreferred responses given an input prompt \(x\). The probably of obtaining each preference pair is \(p(y_{w}\succ y_{l})=\sigma(r(x,y_{w})-r(x,y_{l}))\), where \(\sigma(\cdot)\) is the sigmoid function. In DPO, the optimization can be formulated as classification loss over the preference data as:

\[\mathcal{L}_{\textit{DPO}}(\pi_{\theta};\pi_{\textit{ref}})=-\mathbb{E}_{(x, y_{w},y_{l})\sim\mathcal{D}}\left[\log\sigma\left(\alpha\log\frac{\pi_{\theta}(y_{w} \mid x)}{\pi_{\textit{ref}}(y_{w}\mid x)}-\alpha\log\frac{\pi_{\theta}(y_{w} \mid x)}{\pi_{\textit{ref}}(y_{w}\mid x)}\right)\right].\] (1)

where \(\pi_{\theta}\) represents the reference policy, which is the LLM fine-tuned through supervised learning.

## 3 MMed-RAG: A Versatile Medical RAG System

In this section, as illustrated in Figure 1, we will propose MMed-RAG, a versatile RAG system for improving the factuality of Med-LVLMs. Specifically, MMed-RAG consists of three complementary modules. First, we design a domain-aware retrieval mechanism to select the optimal retriever by feeding each given medical image to the domain identification module. Second, to select an optimal number of retrieved contexts and filter out low-quality information, MMed-RAG adopts a adaptive method by filtering out low-quality information using the similarity scores during the RAG phase. Lastly, we use a RAG-based preference fine-tuning approach to improve the cross-modality alignment and the overall alignment between groundtruth. We detail these steps as follows:

### Domain-Aware Retrieval Mechanism

In MMed-RAG, we introduce a domain-aware retrieval mechanism to efficiently handle medical images from different sources (e.g., radiology, pathology, ophthalmology). Specifically, we first employ a domain identification module that assigns a domain label to each input medical image. To achieve this, we create a small dataset with medical images as inputs and their corresponding domain labels as outputs, using this dataset to fine-tune the BiomedCLIP model (Zhang et al., 2023) to improve its domain awareness. Formally, for a given medical image \(x_{v}\), we predict its domain \(d=\mathcal{F}(x_{v})\). Based on the assigned domain label \(d\), the image \(x_{v}\) is fed into the corresponding multimodal retriever \(\mathcal{R}_{d}(\cdot)\) for knowledge retrieval.

Here, each multimodal retriever \(\mathcal{R}_{d}(\cdot)\) for each domain \(d\) is trained through contrastive learning (Radford et al., 2021). Specifically, the visual and textual information \(X_{img},X_{txt}\) are processed by their corresponding encoders \(\mathcal{E}_{img}(\cdot),\mathcal{E}_{txt}(\cdot)\) to generate textual and visual embeddings \(V_{txt}=\mathcal{E}_{txt}(X_{txt}),V_{img}=\mathcal{E}_{img}(X_{img})\). Contrastive learning loss is then applied to maximize the similarity between text and image embeddings representing the same example, while minimizing the similarity between embeddings representing different examples, as defined below:

\[\mathcal{L}=\frac{\mathcal{L}_{img}+\mathcal{L}_{txt}}{2},\text{where }\ \mathcal{L}_{img}=-\frac{1}{N}\sum_{i=1}^{N}\log\frac{\exp(S_{i,i})}{\sum_{j=1 }^{N}\exp(S_{i,j})},\mathcal{L}_{txt}=-\frac{1}{N}\sum_{i=1}^{N}\log\frac{ \exp(S_{i,i})}{\sum_{j=1}^{N}\exp(S_{j,i})},\] (2)

where \(S\in\mathbb{R}^{N\times N}\) represents the similarity matrix between image and text modalities, calculated as: \(S=\frac{V_{img}}{|V_{img}|}\cdot(\frac{V_{txt}}{|V_{txt}|})^{T}\), where each element \(S_{i,j}\) represents the similarity between the image representation of example \(i\) and the text representation of example \(j\).

Finally, for the input image \(x_{t}\), after feeding into the corresponding multimodal retriever \(\mathcal{R}_{d}(\cdot)\), the multimodal retriever will retrieves the top-\(k\) most similar reports for the image. These retrieved reports \(x_{r}=\mathcal{R}_{d}(x_{v})\) are then provided to the Med-LVLM \(\mathcal{M}(\cdot)\) as references to guide the generation.

### Adaptive Retrieved Context Selection

Following the domain-aware retrieval mechanism, the next step is to determine the optimal amount of context to retrieve.

Figure 1: Overview of MMed-RAG, a versatile factual multimodal RAG system designed to enhance the reliability of Med-LVLMs. It introduces a domain-aware retrieval mechanism that effectively handles different domains of medical images by selecting suitable retrieval models. Additionally, it uses an adaptive context selection approach to determine the optimal number of retrieved contexts and employs preference fine-tuning to improve both cross-modality and overall alignment.

Figure 2: Relations between selected contexts and similarity score.

Retrieving too much or too little information can result in hallucinations (Xia et al., 2024b). Current RAG methods applied to Med-LVLMs generally rely on empirical results or fixed values based on validation sets to select the optimal value of the number of retrieved contexts \(k\)(Xia et al., 2024b; He et al., 2024; Sun et al., 2024b). However, the distribution of similarity scores varies depending on the complexity of the image and its alignment with the textual information from the data source. These fixed-\(k\) methods do not guarantee optimal performance on target data, as they overlook the similarity scores generated during the retrieval process. To address this, we propose an adaptive method that dynamically selects \(k\) based on the similarity scores of the retrieved contexts. Specifically, during the domain-aware retrieval mechanism phase, the retrieved information is denoted as \(x_{r}(k)=\mathcal{R}_{d}(x_{v};k)\), where \(k\) represents the number of retrieved contexts, and the corresponding similarity scores are denoted as \(S_{k}\). For simplicity, when there is no ambiguity, we will refer to \(x_{r}(k)\) as \(x_{r}\).

As illustrated in Figure 2, our method is based on a key observation: the similarity scores (CLIP score in this case) between retrieved contexts often exhibit a sharp decline after a certain number of results (nearly top-9 in this case). This suggests that lower-quality information can still be included among the top-\(k\) retrieved contexts when using a fixed-\(k\) strategy, especially in cases where the fixed value of \(k\) is too large. These lower-quality retrievals introduce noise and irrelevant information, which can significantly impair the model's ability to generate factual and coherent responses. To mitigate this issue, we draw inspiration from the Gap statistic method used in clustering (Tibshirani et al., 2001) and extend this concept to RAG for Med-LVLMs. Specifically, after retrieving the top-\(k\) contexts, we perform an additional round of \(k\) optimization by analyzing the similarity ratios between consecutive retrievals. These similarity ratios are denoted as \(u_{i}=\log(S_{i}/S_{i+1})\) for \(0<i\leq k\), where \(S_{i}\) represents the similarity score of the \(i\)-th retrieved context. When \(u_{i}\) exceeds a predefined threshold \(\gamma\), this indicates a substantial drop in relevance, suggesting that the remaining retrievals are less likely to contribute preferredly to the model's output. At this point \(i\), we truncate \(k\), effectively discarding the less relevant retrievals that follow. This adaptive truncation mechanism ensures that only the most relevant contexts are retained for generating the final response, reducing the risk of hallucination and improving the factual accuracy of the outputs.

Although the threshold \(\gamma\) is fixed, this approach provides a adaptive way to balance the bias and variance in retrieved contexts. By adapting to the characteristics of each input \(x_{v}\), our method enhances the robustness of the retrieval process and ensures that the selection of \(k\) is tailored to the specific data at hand, thereby improving overall performance across diverse contexts and tasks.

### RAG-based Preference Fine-Tuning

After context selection, MMed-RAG supplies Med-LVLM with reliable retrieved information as external knowledge to aid in generating factual responses. However, incorporating this retrieved knowledge may potentially disrupt the original alignment within the existing Med-LVLM, a concern we will elaborate on below:

**Alignment Analysis.** In the alignment analysis, we aim to explore how incorporating retrieved context impacts the original alignment in Med-LVLMs, focusing on two key aspects: (1) cross-modality alignment and (2) overall alignment with the ground truth. To evaluate cross-modality alignment, we conduct two tests on LLaVA-Med-1.5 (Li et al., 2023a) using the Harvard-FairVLMed (Luo et al., 2024) dataset. First, when replacing the original image with a highly noisy image associated with a different ground truth, the original model gives incorrect answers (the ground truth being the response for the original image). After incorporating RAG, where context is retrieved based on the original image, 55.08% of these cases return correct answers. This indicates that the model _directly references the retrieved knowledge_ without considering the input image, highlighting significant _cross-modal misalignment issues_. Furthermore, 43.31% of the questions that were originally answered correctly are answered incorrectly after incorporating RAG, suggesting _interference from incorrect retrieval information_, which leads to,_overall misalignment with the ground truth_. To address cross-modality misalignment and the overall misalignment introduced by incorporating retrieved knowledge, as shown in Algorithm 1, we propose a RAG-based preference fine-tuning (RAG-PT) approach to fine-tune the target Med-LVLM \(\mathcal{M}(\cdot)\). Specifically, RAG-PT constructs two types of preference pairs designed to mitigate both categories of misalignment.

**Preference Pairs for Cross-Modality Alignment.** We first construct preference pairs aimed at improving cross-modality alignment. In this dataset, we select samples from \(\mathcal{D}=\{x_{v}^{(i)},x_{t}^{(i)},y^{(i)}\}_{i=1}^{N}\)where \(x_{v}\), \(x_{t}\), and \(y\) represent the input medical image, clinical query, and ground-truth answer, respectively. For simplicity, we omit the sample index \((i)\) in the following sections. A model's correct response using retrieved knowledge, i.e., \(\mathcal{M}(x_{v},x_{t}+x_{r})=y\), is considered a preferred response \(p_{i}\), where \(x_{r}\) is the retrieved information. A dispreferred response \(n_{i}\) is selected from cases where the model makes a correct inference based on an unrelated image, i.e., \(\mathcal{M}(x_{v}^{*},x_{t})\neq y\), but \(\mathcal{M}(x_{v}^{*},x_{t}+x_{r})=y\), reflecting the model's reliance on the retrieved knowledge. The unrelated images \(x_{v}^{*}\) are generated through a two-step process: first, we use the retriever to select an image \(x_{v}^{\prime}\) with the lowest similarity to the target image; then, we introduce diffusion noise into the selected unrelated image. We define the noise step as \(s\), and the noised image at step \(s\) is expressed as:

\[x_{v}^{*}=\sqrt{\xi_{s}}\cdot x_{v}^{\prime}+\sqrt{1-\xi_{s}}\cdot\epsilon,\] (3)

where \(\bar{\xi_{s}}=\prod_{i=0}^{s}\xi_{i}\) and \(\xi_{s}\in(0,1)\) is a hyperparameter. The preference pairs constructed in this stage are denoted as \(\mathcal{D}_{cm}\). By comparing the preferred and dispreferred responses in \(\mathcal{D}_{cm}\), we encourage the model to prioritize the input medical image when generating responses.

**Preference Pairs for Overall Alignment.** Second, we construct preference pairs to improve overall alignment, focusing on enhancing the model's ability to effectively leverage retrieved knowledge when generating responses. The preference pairs in this stage are constructed from two subsets. The first subset, \(\mathcal{D}_{oa}^{1}\), is designed to strengthen the model's comprehension and reasoning abilities regarding the retrieved knowledge. Preferred responses are selected where the model correctly answers based on both the original image and the retrieved information, i.e., \(\mathcal{M}(x_{v},x_{t}+x_{r})=y\), while dispreferred responses represent cases where the model answers incorrectly based on the image without using retrieval, i.e., \(\mathcal{M}(x_{v},x_{t})\neq y\). Comparing these preferred and dispreferred responses enhances the model's understanding of the retrieved information and improves the overall effectiveness of RAG. In the second subset, \(\mathcal{D}_{oa}^{2}\), the goal is to mitigate interference from the retrieved knowledge. Preferred responses are selected where the model correctly answers based solely on the original image without using retrieved knowledge, i.e., \(\mathcal{M}(x_{v},x_{t})=y\), while dispreferred responses occur when the model answers incorrectly using both the image and retrieved information, i.e., \(\mathcal{M}(x_{v},x_{t}+x_{r})\neq y\). This helps the model learn when to rely on its internal knowledge versus retrieved knowledge. Finally, we combine the first and second subsets to form the second set of preference pairs, \(\mathcal{D}_{oa}=\mathcal{D}^{1}_{oa}\cup\mathcal{D}^{2}_{oa}\).

Finally, we merge the first and second preference set and denote the preference dataset as \(\mathcal{D}_{pt}=\mathcal{D}_{em}\cup\mathcal{D}_{oa}=\{x^{(i)},y^{(i)}_{w,o},y^{ (i)}_{h,o}\}_{i=1}^{N}\), where \(y^{(i)}_{w,o}\), \(y^{(i)}_{h,o}\) are represented as preferred and dispreferred responses, respectively. Based on the curated preferences, we fine-tune Med-LVLM using direct preference optimization (Rafailov et al., 2023) with the following loss:

\[\mathcal{L}_{pt}=-\mathbb{E}_{(x,y_{w,o},y_{l,o})\sim\mathcal{D}}\left[\log \sigma\left(\alpha\log\frac{\pi_{a}(y_{w,o}|x)}{\pi_{a}(y_{w,o}|x)}-\alpha\log \frac{\pi_{a}(y_{w,o}|x)}{\pi_{a}(y_{w,o}|x)}\right)\right].\] (4)

## 4 Experiment

In this section, we evaluate the performance of MMed-RAG, aiming to answer the following questions: (1) Can MMed-RAG effectively improve the factuality of Med-LVLMs compared to decoding-based and RAG-based baselines? (2) How effective is each proposed component on performance? (3) What is the effect of preference data for different alignment goals? and (4) Does MMed-RAG actually improve cross-modality alignment and overall alignment?

### Experimental Setups

**Implementation Details**. We use LLaVA-Med-1.5 7B (Li et al., 2023a) as the backbone model. During the preference fine-tuning process, we adapt LoRA fine-tuning (Hu et al., 2021). For the training of retriever, the vision encoder is a ResNet-50 (He et al., 2016), and the text encoder is a bio-BioClinicalBERT (Alsentzer et al., 2019). We use the AdamW optimizer with a learning rate of \(10^{-3}\), weight decay of \(10^{-2}\) and a batch size of 32. The model is trained for 360 epochs. For more detailed information on training hyperparameters and training data, please see Appendix A.1.1.

**Baseline Methods**. We compare MMed-RAG with two types of LVLM hallucination mitigation methods that show promising results in natural image understanding. 1) Decoding-based methods, including Greedy Decoding, Beam Search (Sutskever et al., 2014), DoLa (Chuang et al., 2023), OPERA (Huang et al., 2023), VCD (Leng et al., 2023). These methods manipulate the logits of the model's output tokens to enhance factual accuracy. 2) Multimodal RAG-based methods, including MedDr (He et al., 2024), FactMM-RAG (Sun et al., 2024b), RULE (Xia et al., 2024b). Furthermore, we compare the performance with other open-source Med-LVLMs, including Med-Flamingo (Moor et al., 2023), MedVInT (Zhang et al., 2023b), RadFM (Wu et al., 2023b).

**Evaluation Datasets**. We utilize five medical vision-language datasets for medical VQA and report generation tasks, i.e., MIMIC-CXR (Johnson et al., 2019), IU-Xray (Demner-Fushman et al., 2016), Harvard-FairVLMed (Luo et al., 2024), PMC-OA (Lin et al., 2023a) (we only select the pathology part) and Quilt-1M (Ikezogwoo et al., 2024). These datasets cover radiology, ophthalmology, and pathology. To construct the VQA benchmarks, following (Xia et al., 2024a), we generate question-answer pairs from medical reports using GPT-4 (OpenAI, 2023), with answers formatted as _yes_ or _no_. Pathology images are excluded from the report generation task due to their brief and insufficient descriptions. The detailed dataset descriptions are provided in the Appendix A.2.

**Evaluation Metrics**. Following (Jing et al., 2017; Lin et al., 2023b), we use Accuracy, F1 Score and AUROC for evaluating medical VQA task, and BLEU Score (Papineni et al., 2002), ROUGE-L (Lin, 2004) and METEOR (Banerjee & Lavie, 2005) for evaluating report generation task.

### Main Results

In this section, we provide a comprehensive comparison with various baseline methods and other open-source Med-LVLMs on medical VQA and report generation tasks.

**Comparison with Baselines.** We compare MMed-RAG with baseline methods on medical VQA and report generation tasks, with the results presented in Table 1 and Table 2, respectively. Overall, MMed-RAG outperforms all baselines across nearly all metrics and datasets. Specifically, MMed-RAG demonstrates a significant performance boost, improving by 18.5% and 69.1% over the original Med-LVLM in medical VQA and report generation tasks, respectively. When compared to baseline methods, MMed-RAG surpasses decoding-based approaches, achieving improvements of 11.5% and 44.2% in the two tasks. Furthermore, recent RAG-based methods show substantial improvements over earlier techniques, yet our approach still outperforms RAG-based baselines by 2.8% and 16.1% in the medical VQA and report generation tasks, respectively. This indicates that MMed-RAG effectively mitigates misalignment issues introduced by RAG. Notably, MMed-RAG achieves more pronounced gains in report generation, likely due to the higher complexity of the task and the greater influence of retrieved contexts in guiding open-ended generation.

**Comparison with Other Med-LVLMs.** To provide a comprehensive comparison, we evaluate MMed-RAG against other open-source Med-LVLMs to demonstrate the superiority of our approach. We assess the performance of these models across different medical image modalities, reporting the average results for medical VQA and report generation tasks in Table 3 (see Appendix A.6 for detailed results). Our findings show that MMed-RAG significantly outperforms Med-LVLMs pre-trained on large-scale datasets across various domains. This reinforces the generalizability and effectiveness of our approach across diverse image domains and medical multimodal tasks.

### Analysis

In this section, we provide a detailed analysis of each module's performance, along with a series of analytical experiments, to better understand the performance gains of MMed-RAG. Additionally, we demonstrate the compatibility of our approach, achieving a consistent 40.3% performance improvement on a different backbone, i.e., LLAVA-Med-1.0 (see details in Appendix A.6).

\begin{table}
\begin{tabular}{l|c c c c c|c c c|c c c c} \hline \hline \multirow{2}{*}{Models} & \multicolumn{4}{c|}{**Radiology**} & \multicolumn{4}{c|}{**Ophthalmology**} & \multicolumn{4}{c}{**Pathology**} \\ \cline{2-13}  & \multicolumn{3}{c|}{IU-Xray} & \multicolumn{3}{c|}{MIMIC-CSR} & \multicolumn{3}{c|}{Harvard-FairVLMed} & \multicolumn{3}{c}{Quil-1M} & \multicolumn{3}{c}{PMC-OA (Pathology)} \\ \cline{2-13}  & Acc & F1 & AUC & Acc & F1 & AUC & Acc & F1 & AUC & Acc & F1 & AUC & Acc & F1 & AUC \\ \hline LLAVA-Med-1.5 & 75.47 & 64.04 & 67.46 & 75.79 & 80.49 & 68.84 & 63.03 & 74.11 & 63.05 & 62.80 & 72.90 & 60.03 & 59.28 & 71.98 & 54.19 \\ \hline \hline Greedy & 76.88 & 65.9 & 68.74 & 78.32 & 86.76 & 71.13 & 82.54 & 85.98 & 90.09 & 64.72 & 70.12 & 58.75 & 68.14 & 70.42 & 51.10 \\ \hline \multicolumn{1}{l}{* Beam Search Search } & 69.61 & 66.06 & 68.77 & 81.56 & 86.73 & 73.99 & 80.88 & 68.94 & 65.32 & 69.33 & 57.65 & 56.29 & 69.84 & 52.89 \\ \multicolumn{1}{l}{* Della 2.06} & 78.00 & 66.75 & 72.19 & 81.35 & 85.73 & 72.73 & 76.87 & 85.53 & 61.07 & 64.91 & 57.58 & 57.71 & 70.27 & 52.95 \\ \multicolumn{1}{l}{* OPERA} & 70.99 & 69.51 & 64.32 & 69.34 & 66.26 & 71.41 & 81.37 & 65.59 & 60.51 & 63.24 & 57.95 & 52.83 & 50.31 & 51.86 \\ \multicolumn{1}{l}{* VCD} & 68.99 & 54.35 & 61.08 & 70.89 & 75.57 & 64.61 & 65.88 & 77.20 & 64.16 & 61.43 & 67.39 & 55.72 & 55.10 & 67.94 & 51.62 \\ \hline \multicolumn{1}{l}{* MMed-RAG} & 83.13 & 67.70 & 71.55 & 51.66 & 58.54 & 70.17 & 80.72 & 61.15 & 61.53 & 73.23 & 67.01 & 59.97 & 69.19 & 57.01 \\ \multicolumn{1}{l}{* FachML-RAG} & 84.16 & 68.51 & 77.07 & 77.58 & 81.86 & 70.09 & 83.67 & 72.21 & 70.09 & 29.73 & 76.82 & 68.15 & 69.49 & 69.38 & 57.31 \\ \multicolumn{1}{l}{* RULE} & 87.84 & 78.00 & 85.78 & 83.92 & 87.49 & 83.44 & 87.12 & 92.29 & 70.08 & 69.77 & 73.80 & 68.13 & 64.41 & 70.36 & 58.91 \\ \hline MMed-RAG & **89.54** & 80.72 & 87.13 & 83.57 & 88.49 & 85.08 & 87.94 & 92.78 & 80.81 & 72.95 & 76.35 & 72.25 & 66.45 & 73.09 & 61.42 \\ \hline \hline \end{tabular}
\end{table}
Table 1: Model performance (%) of different methods based on LLAVA-Med-1.5 on medical VQA task. Notably, we report the accuracy, F1 score and AUROC. The best results and second best results are highlighted in \(\tt|\tt|\tt|\) and \(\tt|\tt|\), respectively.

\begin{table}
\begin{tabular}{l|c c c c} \hline \hline \multirow{2}{*}{Models} & \multicolumn{4}{c|}{**Radiology**} & \multicolumn{4}{c}{**Ophthalmology**} \\ \cline{2-5}  & \multicolumn{3}{c|}{IU-Xray} & \multicolumn{3}{c}{MIMIC-CSR} & \multicolumn{3}{c}{Harvard-FairVLMed} \\ \cline{2-5}  & BLEU & ROUGE-L & METEOR & BLEU & ROUGE-L & METEOR & BLEU & ROUGE-L & METEOR \\ \hline LLAVA-Med-1.5 & 9.64 & 12.26 & 8.21 & 12.11 & 13.05 & 11.16 & 18.11 & 11.36 & 10.75 \\ \hline + Greedy & 11.47 & 15.38 & 12.69 & 16.63 & 14.26 & 14.19 & 17.98 & 11.49 & 13.77 \\ + Beam Search & 12.10 & 16.21 & 13.17 & 16.97 & 14.74 & 14.43 & 18.37 & 12.62 & 14.50 \\ + Dela & 11.79 & 15.82 & 12.72 & 17.11 & 14.89 & 14.81 & 18.26 & 12.51 & 14.51 \\ + OPERA & 10.66 & 14.70 & 12.01 & 15.40 & 12.52 & 13.72 & 16.59 & 11.47 & 13.63 \\ + VCD & 10.42 & 14.14 & 11.59 & 15.18 & 12.30 & 13.38 & 16.73 & 11.38 & 13.89 \\ \hline + MedDrr & 12.37 & 16.45 & 13.50 & 18.59 & 15.72 & 16.77 & 19.82 & 13.72 & 15.40 \\ + FactM-RAG & 14.70 & 18.05 & 15.92 & 18.71 & 15.84 & 16.82 & 20.82 & 14.17 & 15.31 \\ + RULE & 27.53 & 23.16 & 27.99 & 18.61 & 15.96 & 17.42 & 22.35 & 14.93 & 17.74 \\ \hline MMed-RAG & **31.38** & 25.59 & 32.43 & 23.25 & 12.34 & & 20.47 & 24.82 & 16.59 & 19.85 \\ \hline \hline \end{tabular}
\end{table}
Table 2: Model performance (%) of different methods based on LLAVA-Med-1.5 on report generation task. Notably, we report the average BLEU, ROUGE-L, METEOR.

\begin{table}
\begin{tabular}{l|c c c} \hline \hline Model & \multicolumn{3}{c}{IU-Xray} & \multicolumn{3}{c}{FairVLMed} \\  & VQA & RG & VQA & RG \\ \hline LLAVA-Med-1.5 & 68.99 & 10.04 & 66.63 & 13.41 \\ +DR & 77.12 & 13.32 & 72.69 & 15.89 \\ +RCS & 79.56 & 17.92 & 75.74 & 17.22 \\ +RAG-PT (Oars) & **85.80** & **29.80** & **87.18** & **20.42** \\ \hline \hline \hline \end{tabular}
\end{table}
Table 4: Ablation results on two datasets covering different domains. RG: report generation, FairVLMed: Harvard-FairVLMed.

\begin{table}
\begin{tabular}{l|c c c c} \hline \hline Model & \multicolumn{3}{c}{**Rid**} & \multicolumn{3}{c}{**OphthalmAblation Studies.We conduct a series of ablation experiments to evaluate the impact of each component in MMed-RAG. The results for both medical VQA and report generation tasks on the IU-Xray and Harvard-FairVLMed datasets are summarized in Table 4. According to the results, we can see that: (1) The domain-aware retrieval mechanism (DR) significantly improves the factuality of Med-LVLM, with an average performance increase of 17.9% and 16.1% on the IU-Xray and FairVLMed datasets, respectively. Here, the retrieved knowledge aids the model in generating more factual responses. (2) Building on this, the introduction of adaptive retrieval context selection (RCS) further filters out unreliable retrieved contexts, yielding an additional performance boost of 19.3% and 6.3% on the IU-Xray and FairVLMed datasets. (3) The inclusion of RAG-based preference fine-tuning (RAG-PT) enhances the model's understanding of the retrieved knowledge, leading to substantial performance gains of 37.1% and 16.9% on the respective datasets. This demonstrates that RAG-PT effectively addresses misalignment issues.

Impact of the Preference Data in RAG-PT.To better understand how RAG-PT mitigates the misalignment issue and improves performance, we conducted a detailed study on the training preference data composition of RAG-PT. As described in Section 3.3, the RAG-PT data is designed to address both cross-modality alignment and overall alignment objectives, with the latter focusing on enhanced understanding of retrieved knowledge and minimizing retrieval interference. The detailed experimental results in Table 5 demonstrate that the preference data tailored for different alignment objectives positively impacts the model's performance, showing the effectiveness of RAG-PT.

How Effective is MMed-RAG in Mitigating Misalignment Issues?To gain a more intuitive understanding of the effectiveness of MMed-RAG in addressing misalignment issues: 1) we calculate the proportion of errors caused by RAG and compare it to the proportion after incorporating MMed-RAG. 2) We visualize the attention maps of image and text tokens with and without RAG-PT. First, as mentioned in Section 3.3, the model may directly copy reference information, referred to as Copy-Reference (CR) rate. After applying MMed-RAG, as shown in Figure 3, the CR rate drops to 28.19%. Additionally, the proportion of errors affected by RAG interference, referred to as Over-Reliance (OR) rate, which is initially 43.31%, decreased to 8.38% after incorporating MMed-RAG. Furthermore, as shown in Figure 4, the original Med-LVLM tends to rely more heavily on text while ignoring visual information. When retrieval information is introduced, the original Med-LVLM focused more on the retrieved answers, even if the content is incorrect. After RAG-PT, the model significantly increases its attention to visual information and reduces the interference of RAG, thus better aligning the model's knowledge with the fundamental facts.

## 5 Conclusion

This paper introduces MMed-RAG, a versatile multimodal RAG system designed to address the critical issue of factual hallucination in Med-LVLMs. MMed-RAG employs a domain-aware retrieval mechanism, adaptive calibration for selecting the optimal number of retrieved contexts, and RAG-based preference fine-tuning to improve both cross-modality alignment and overall alignment with the ground truth. These enhancements significantly boost the factual accuracy of Med-LVLMs. Experimental results demonstrate MMed-RAG' effectiveness in enhancing factual accuracy across various imaging domains, underscoring its potential for reliable use in healthcare.

Figure 4: Visualization of attention map. The red box region is labeled with the attentions that can be enhanced by MMed-RAG.

Figure 3: Alignment analysis with and without RAG. OR: Over-Reliance; CR: Copy-Reference.

\begin{table}
\begin{tabular}{l|c c c c} \hline \hline Model & \multicolumn{3}{c}{IU-Xray} & \multicolumn{2}{c}{FairVLMed} \\  & VQA & RG & VQA & RG \\ \hline LLAvAv-Med-1.5 & 68.99 & 10.04 & 66.63 & 13.41 \\
*+RAG-PT 1 & 80.19 & 19.38 & 79.42 & 18.77 \\
*+RAG-PT 2 & 80.27 & **20.16** & 79.35 & 18.66 \\
*+RAG-PT 3 & **81.30** & 19.43 & **80.07** & **18.52** \\ \hline \hline \end{tabular}
\end{table}
Table 5: Performance using RAG-PT based on subsets of preference data.

## References

* Alkhaldi et al. (2024) Asma Alkhaldi, Raneem Alnajim, Layan Alabdullatef, Rawan Alyahya, Jun Chen, Deyao Zhu, Ahmed Alsian, and Mohamed Elhoseiny. Minigpt-med: Large language model as a general interface for radiology diagnosis. _arXiv preprint arXiv:2407.04106_, 2024.
* Alsentzer et al. (2019) Emily Alsentzer, John R Murphy, Willie Boag, Wei-Hung Weng, Di Jin, Tristan Naumann, and Matthew McDermott. Publicly available clinical bert embeddings. _arXiv preprint arXiv:1904.03323_, 2019.
* Banerjee and Lavie (2005) Satanjeev Banerjee and Alon Lavie. Meteor: An automatic metric for mt evaluation with improved correlation with human judgments. In _Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization_, pp. 65-72, 2005.
* Chen et al. (2024a) Jiawei Chen, Dingkang Yang, Tong Wu, Yue Jiang, Xiaolu Hou, Mingcheng Li, Shunli Wang, Dongling Xiao, Ke Li, and Lihua Zhang. Detecting and evaluating medical hallucinations in large vision language models. _arXiv preprint arXiv:2406.10185_, 2024a.
* Chen et al. (2024b) Junying Chen, Ruyi Ouyang, Anningzhe Gao, Shunian Chen, Guiming Hardy Chen, Xidong Wang, Ruifei Zhang, Zhenyang Cai, Ke Ji, Guangjun Yu, et al. Huatuogpt-vision, towards injecting medical visual knowledge into multimodal lms at scale. _arXiv preprint arXiv:2406.19280_, 2024b.
* Chen et al. (2024c) Zhanpeng Chen, Chengjin Xu, Yiyan Qi, and Jian Guo. Mllm is a strong reranker: Advancing multimodal retrieval-augmented generation via knowledge-enhanced reranking and noise-injected training. _arXiv preprint arXiv:2407.21439_, 2024c.
* Chen et al. (2024d) Zixiang Chen, Yihe Deng, Huizhuo Yuan, Kaixuan Ji, and Quanquan Gu. Self-play fine-tuning converts weak language models to strong language models, 2024d. URL https://arxiv.org/abs/2401.01335.
* Chuang et al. (2023) Yung-Sung Chuang, Yujia Xie, Hongyun Luo, Yoon Kim, James Glass, and Pengcheng He. Dola: Decoding by contrasting layers improves factuality in large language models. _arXiv preprint arXiv:2309.03883_, 2023.
* Demner-Fushman et al. (2016) Dina Demner-Fushman, Marc D Kohli, Marc B Rosenman, Sonya E Shooshan, Laritza Rodriguez, Sameer Antani, George R Thoma, and Clement J McDonald. Preparing a collection of radiology examinations for distribution and retrieval. _Journal of the American Medical Informatics Association_, 23(2):304-310, 2016.
* Gao et al. (2023) Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, and Haofen Wang. Retrieval-augmented generation for large language models: A survey. _arXiv preprint arXiv:2312.10997_, 2023.
* He et al. (2016) Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition_, pp. 770-778, 2016.
* He et al. (2024) Sunan He, Yuxiang Nie, Zhixuan Chen, Zhiyuan Cai, Hongmei Wang, Shu Yang, and Hao Chen. Meddr: Diagnosis-guided bootstrapping for large-scale medical vision-language learning. _arXiv preprint arXiv:2404.15127_, 2024.
* Hu et al. (2021) Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. _arXiv preprint arXiv:2106.09685_, 2021.
* Huang et al. (2023) Qidong Huang, Xiaoyi Dong, Pan Zhang, Bin Wang, Conghui He, Jiaqi Wang, Dahua Lin, Weiming Zhang, and Nenghai Yu. Opera: Alleviating hallucination in multi-modal large language models via over-trust penalty and retrospection-allocation. _arXiv preprint arXiv:2311.17911_, 2023.
* Ikezogwo et al. (2024) Wisdom Ikezogwo, Saygin Seyfioglu, Fatemeh Ghezloo, Dylan Geva, Fatwir Sheikh Mohammed, Pavan Kumar Anand, Ranjay Krishna, and Linda Shapiro. Quilt-1m: One million image-text pairs for histopathology. _Advances in neural information processing systems_, 36, 2024.

* Jiang et al. (2024) Yue Jiang, Jiawei Chen, Dingkang Yang, Mingcheng Li, Shunli Wang, Tong Wu, Ke Li, and Lihua Zhang. Medthink: Inducing medical large-scale visual language models to hallucinate less by thinking more. _arXiv preprint arXiv:2406.11451_, 2024.
* Jing et al. (2017) Baoyu Jing, Pengtao Xie, and Eric Xing. On the automatic generation of medical imaging reports. _arXiv preprint arXiv:1711.08195_, 2017.
* Johnson et al. (2019) Alistair EW Johnson, Tom J Pollard, Nathaniel R Greenbaum, Matthew P Lungren, Chih-ying Deng, Yifan Peng, Zhiyong Lu, Roger G Mark, Seth J Berkowitz, and Steven Horng. Mimic-cxr-jpg, a large publicly available database of labeled chest radiographs. _arXiv preprint arXiv:1901.07042_, 2019.
* Kumar and Marttinen (2024) Yogesh Kumar and Pekka Marttinen. Improving medical multi-modal contrastive learning with expert annotations. _arXiv preprint arXiv:2403.10153_, 2024.
* Leng et al. (2023) Sicong Leng, Hang Zhang, Guanzheng Chen, Xin Li, Shijian Lu, Chunyan Miao, and Lidong Bing. Mitigating object hallucinations in large vision-language models through visual contrastive decoding. _arXiv preprint arXiv:2311.16922_, 2023.
* Li et al. (2023a) Chunyuan Li, Cliff Wong, Sheng Zhang, Naoto Usuyama, Haotian Liu, Jianwei Yang, Tristan Naumann, Hoifung Poon, and Jianfeng Gao. Llava-med: Training a large language-and-vision assistant for biomedicine in one day. In _Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track_, 2023a.
* Li et al. (2023b) Yingshu Li, Yunyi Liu, Zhanyu Wang, Xinyu Liang, Lingqiao Liu, Lei Wang, Leyang Cui, Zhaopeng Tu, Longyue Wang, and Luping Zhou. A comprehensive study of gpt-4v's multimodal capabilities in medical imaging. _arXiv preprint arXiv:2310.20381_, 2023b.
* Lin (2004) Chin-Yew Lin. Rouge: A package for automatic evaluation of summaries. In _Text summarization branches out_, pp. 74-81, 2004.
* Lin et al. (2023a) Weixiong Lin, Ziheng Zhao, Xiaoman Zhang, Chaoyi Wu, Ya Zhang, Yanfeng Wang, and Weidi Xie. Pmc-clip: Contrastive language-image pre-training using biomedical documents. In _International Conference on Medical Image Computing and Computer-Assisted Intervention_, pp. 525-536. Springer, 2023a.
* Lin et al. (2023b) Zhihong Lin, Donghao Zhang, Qingyi Tao, Danil Shi, Gholamreza Haffari, Qi Wu, Mingguang He, and Zongyuan Ge. Medical visual question answering: A survey. _Artificial Intelligence in Medicine_, 143:102611, 2023b.
* Liu et al. (2024a) Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 26296-26306, 2024a.
* Liu et al. (2024b) Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. _Advances in neural information processing systems_, 36, 2024b.
* Luo et al. (2024) Yan Luo, Min Shi, Muhammad Osama Khan, Muhammad Muneeb Afzal, Hao Huang, Shuaihang Yuan, Yu Tian, Luo Song, Ava Kouhana, Tobias Elze, et al. Fairclip: Harnessing fairness in vision-language learning. _arXiv preprint arXiv:2403.19949_, 2024.
* Moor et al. (2023) Michael Moor, Qian Huang, Shirley Wu, Michihiro Yasunaga, Yash Dalmia, Jure Leskovec, Cyril Zakka, Eduardo Pontes Reis, and Pranav Rajpurkar. Med-flamingo: a multimodal medical few-shot learner. In _Machine Learning for Health (ML4H)_, pp. 353-367. PMLR, 2023.
* (2023) OpenAI. Gpt-4 technical report, 2023. https://arxiv.org/abs/2303.08774.
* Papineni et al. (2002) Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation of machine translation. In _Proceedings of the 40th annual meeting of the Association for Computational Linguistics_, pp. 311-318, 2002.
* Qu et al. (2024) Xiaoye Qu, Qiyuan Chen, Wei Wei, Jishuo Sun, and Jianfeng Dong. Alleviating hallucination in large vision-language models with active retrieval augmentation. _arXiv preprint arXiv:2408.00555_, 2024.

* Radford et al. (2021) Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision, 2021.
* Rafailov et al. (2023) Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model. In _Thirty-seventh Conference on Neural Information Processing Systems_, 2023.
* Royer et al. (2024) Corentin Royer, Bjoern Menze, and Anjany Sekuboyina. Multimedeval: A benchmark and a toolkit for evaluating medical vision-language models. _arXiv preprint arXiv:2402.09262_, 2024.
* Schrouff et al. (2022) Jessica Schrouff, Natalie Harris, Sanmi Koyejo, Ibrahim M Alabdulmohsin, Eva Schnider, Krista Opsahl-Ong, Alexander Brown, Subhrajit Roy, Diana Mincu, Christina Chen, et al. Diagnosing failures of fairness transfer across distribution shift in real-world medical settings. _Advances in Neural Information Processing Systems_, 35:19304-19318, 2022.
* Shi et al. (2024) Congzhen Shi, Ryan Rezai, Jiaxi Yang, Qi Dou, and Xiaoxiao Li. A survey on trustworthiness in foundation models for medical image analysis. _arXiv preprint arXiv:2407.15851_, 2024.
* Sun et al. (2024a) Jiashuo Sun, Jihai Zhang, Yucheng Zhou, Zhaochen Su, Xiaoye Qu, and Yu Cheng. Surf: Teaching large vision-language models to selectively utilize retrieved information. _arXiv preprint arXiv:2409.14083_, 2024a.
* Sun et al. (2024b) Liwen Sun, James Zhao, Megan Han, and Chenyan Xiong. Fact-aware multimodal retrieval augmentation for accurate medical radiology report generation. _arXiv preprint arXiv:2407.15268_, 2024b.
* Sutskever et al. (2014) Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks. In _Advances in neural information processing systems_, pp. 3104-3112, 2014.
* Tao et al. (2024) Yitian Tao, Liyan Ma, Jing Yu, and Han Zhang. Memory-based cross-modal semantic alignment network for radiology report generation. _IEEE Journal of Biomedical and Health Informatics_, 2024.
* Tautan et al. (2021) Alexandra-Maria Tautan, Bogdan Ionescu, and Emiliano Santarnecchi. Artificial intelligence in neurodegenerative diseases: A review of available tools with a focus on machine learning techniques. _Artificial Intelligence in Medicine_, 117:102081, 2021.
* Thawkar et al. (2023) Omkar Thawkar, Abdelrahman Shaker, Sahal Shaji Mullappilly, Hisham Cholakkal, Rao Muhammad Anwer, Salman Khan, Jorma Laaksonen, and Fahad Shahbaz Khan. Xraygpt: Chest radiographs summarization using medical vision-language models. _arXiv preprint arXiv:2306.07971_, 2023.
* Tibshirani et al. (2001) Robert Tibshirani, Guenther Walther, and Trevor Hastie. Estimating the number of clusters in a data set via the gap statistic. _Journal of the Royal Statistical Society: Series B (Statistical Methodology)_, 63(2):411-423, 2001.
* Tu et al. (2024) Tao Tu, Shekoofeh Azizi, Danny Driess, Mike Schaekermann, Mohamed Amin, Pi-Chuan Chang, Andrew Carroll, Charles Lau, Ryutaro Tanno, Ira Ktena, et al. Towards generalist biomedical ai. _NEJM AI_, 1(3):Aloa2300138, 2024.
* Wang et al. (2019) Chunhao Wang, Xiaofeng Zhu, Julian C Hong, and Dandan Zheng. Artificial intelligence in radiotherapy treatment planning: present and future. _Technology in cancer research & treatment_, 18:1533033819873922, 2019.
* Wu et al. (2023a) Chaoyi Wu, Jiayu Lei, Qiaoyu Zheng, Weike Zhao, Weixiong Lin, Xiaoman Zhang, Xiao Zhou, Ziheng Zhao, Ya Zhang, Yanfeng Wang, et al. Can gpt-4v (ision) serve medical applications? case studies on gpt-4v for multimodal medical diagnosis. _arXiv preprint arXiv:2310.09909_, 2023a.
* Wu et al. (2023b) Chaoyi Wu, Xiaoman Zhang, Ya Zhang, Yanfeng Wang, and Weidi Xie. Towards generalist foundation model for radiology. _arXiv preprint arXiv:2308.02463_, 2023b.

[MISSING_PAGE_FAIL:12]

Training for 20 hours on one A100 80G GPU. For the RAG-PT phase, we adjust the diffusion noise level, symbolized by \(\xi\) through a specific formula: \(\xi=\text{Sigmoid}(l_{t})\times(0.5\times 10^{-2}-10^{-5})+10^{-5}\), where \(\epsilon\) is drawn from a normal distribution. The reports available for retrieval are from the training set of the corresponding dataset. In our experiments, we apply cross-validation to tune all hyperparameters with grid search. All the experiments are implemented on PyTorch 2.1.2 using four NVIDIA RTX A6000 GPUs. It takes roughly 3 and 4 hours for fine-tuning CLIP and LLaVA-Med-1.5 7B, respectively.

### Evaluated Datasets

We utilize five open-source medical vision-language datasets, i.e., MIMIC-CXR (Johnson et al., 2019), IU-Xray (Demner-Fushman et al., 2016), Harvard-FairVLMed (Luo et al., 2024), PMC-OA (Lin et al., 2023a) and Quilt-1M (Ikezogwo et al., 2024).

* **MIMIC-CXR** (Johnson et al., 2019) is a large publicly available dataset of chest X-ray images in DICOM format with associated radiology reports.
* **IU-Xray** (Demner-Fushman et al., 2016) is a dataset that includes chest X-ray images and corresponding diagnostic reports.
* **Harvard-FairVLMed**(Luo et al., 2024) focuses on fairness in multimodal fundus images, containing image and text data from various sources. It aims to evaluate bias in AI models on this multimodal data comprising different demographics.
* **PMC-OA**(Lin et al., 2023a) is a large-scale dataset comprising figure-caption pairs extracted from PubMed Central. It covers 2,478,267 papers and includes a total of 12,211,907 figure-caption pairs. We only use the pathology subset filtered by GPT-4 based on the captions.
* **Quilt-1M**(Ikezogwo et al., 2024) is the largest vision-language dataset in histopathology, containing 1 million image-text pairs sourced from platforms such as YouTube, Twitter, research papers, and other parts of the internet.

### Evaluated Models

We evaluate five open-source Med-LVLMs, _i.e._, LLaVA-Med (Li et al., 2023a), Med-Flamingo (Moor et al., 2023), MedVInT (Zhang et al., 2023b), RadFM (Wu et al., 2023b), miniGPT-Med (Alkhaldi et al., 2024). The selected models are all at the 7B level.

* **LLaVA-Med**(Li et al., 2023a) is a vision-language conversational assistant, adapting the general-domain LLaVA (Liu et al., 2024b) model for the biomedical field. The model is fine-tuned using a novel curriculum learning method, which includes two stages: aligning biomedical vocabulary with figure-caption pairs and mastering open-ended conversational semantics. It demonstrates excellent multimodal conversational capabilities.
* **Med-Flamingo**(Moor et al., 2023) is a multimodal few-shot learner designed for the medical domain. It builds upon the OpenFlamingo, continuing pre-training with medical image-text data

\begin{table}
\begin{tabular}{l|c c c c c} \hline \hline  & Harvard-FairVLMed & IU-Xray & MIMIC-CXR & PMC-OA & Quilt-1M \\ \hline \# Images & 713 & 589 & 700 & 530 & 559 \\ \# QA Items & 4285 & 2573 & 3470 & 3124 & 1994 \\ \hline \hline \end{tabular}
\end{table}
Table 8: Data statistics for various datasets. The rows represent the number of images and QA pairs for each dataset.

\begin{table}
\begin{tabular}{l|c c c c c} \hline \hline Dataset & Train (R) & All (RAG-PT) & Train (RAG-PT)-a & Train (RAG-PT)-b & Train (RAG-PT)-c \\ \hline Ophthalmology & 7000 & 3247 & 142 & 78 & 207 \\ Radiology & 4034 & 4836 & 233 & 126 & 342 \\ \hline \hline \end{tabular}
\end{table}
Table 7: Data statistics for report generation. “Train (DR)” refers to the number of image-text pairs for retriever training, “All (RAG-PT)” refers to the total data for RAG-PT, and “Train (RAG-PT)-a/b/c” refer to the respective sample categories for RAG-PT training.

from publications and textbooks. This model aims to facilitate few-shot generative medical visual question answering, enhancing clinical applications by generating relevant responses and rationales from minimal data inputs.
* **RadFM**(Wu et al., 2023b) serve as a versatile generalist model in radiology, distinguished by its capability to adeptly process both 2D and 3D medical scans for a wide array of clinical tasks. It integrates ViT as visual encoder and a perceiver module, alongside the MedLLaMA language model, to generate sophisticated medical insights for a variety of tasks. This design allows RadFM to not just recognize images but also to understand and generate human-like explanations.
* **MedVInT**(Zhang et al., 2023b), which stands for Medical Visual Instruction Tuning, is designed to interpret medical images by answering clinically relevant questions. This model features two variants to align visual and language understanding: MedVInT-TE and MedVInT-TD. Both MedVInT variants connect a pre-trained vision encoder ResNet-50 adopted from PMC-CLIP (Lin et al., 2023a), which processes visual information from images. It is an advanced model that leverages a novel approach to align visual and language understanding.
* **miniGPT-Med**(Alkhaldi et al., 2024) is a vision-language model derived from large-scale language models and tailored for radiology diagnosis applications. It handles various medical vision-language task using distinct task identifiers, demonstrating advanced performance in disease grounding, medical report generation, and medical VQA.

### Overview of the Baselines

We compare MMed-RAG with two types of LVLM hallucination mitigation methods that show promising results in natural image understanding. 1) Decoding-based methods, including Greedy Decoding, Beam Search (Sutskever et al., 2014), DoLa (Chuang et al., 2023), OPERA (Huang et al., 2023), VCD (Leng et al., 2023). These methods manipulate the logits of the model's output tokens to enhance factual accuracy. 2) Multimodal RAG-based methods, including MedDr (He et al., 2024), FactMM-RAG (Sun et al., 2024b), RULE (Xia et al., 2024b).

* **Greedy decoding** involves selecting the most probable next token at each step of generation. While it is efficient and straightforward, it can lead to suboptimal outcomes by getting stuck in repetitive or less creative patterns.
* **Beam search**(Sutskever et al., 2014) expands on greedy decoding by maintaining multiple candidate sequences (or "beams") at each step, allowing for a broader exploration of possible outputs. This approach balances quality and diversity by selecting the top-k sequences based on their probabilities, resulting in more coherent and creative text generation compared to greedy decoding.
* **DoLa**(Chuang et al., 2023) derives the next-token distribution by contrasting the logits projected from later layers against those from earlier layers, leveraging the fact that factual knowledge in LLMs is typically localized within specific transformer layers.
* **OPERA**(Huang et al., 2023) is a LVLMs decoding method based on an Over-trust Penalty and a Retrospection-Allocation strategy The key insight is that hallucinations are closely tied to knowledge aggregation patterns in the self-attention matrix, where MLLMs tend to focus on summary tokens, neglecting image tokens and resulting in content hallucination.
* **VCD**(Leng et al., 2023) is a decoding method that tackles the object hallucination issue in LVLMs. It contrasts output distributions derived from original and distorted visual inputs to calibrate the model's output without the usage of external tools, reducing the the over-reliance on statistical bias and unimodal priors.
* **MedDr**(He et al., 2024) is a healthcare foundation model built upon generated diagnosis-based datasets, demonstrating advanced capabilities in various data modalities. Meddr also integrates a retrieval-augmented medical diagnosis strategy during inferencing to enhance factual accuracy.
* **FactMM-RAG**(Sun et al., 2024b) is a fact-aware multimodal retrieval-augmented pipeline for radiology report generation. It utilize RadGraph to annotate chest radiograph reports and mine clinically relevant pairs to train a universal multimodal retriever.
* **RULE**(Xia et al., 2024b) is an advanced medical retrieval-augmented generation strategy designed to enhance the factuality of Med-LVLMs. First, it introduces a robust strategy for controlling factuality risk through the calibrated selection of retrieved contexts. Second, RULE developsa preference optimization strategy to balance Med-LVLMs' intrinsic knowledge and the retrieved information.

### Prompts

We convert the medical reports into a series of closed-ended questions with _yes_ or _no_ answers. To ensure the quality of the VQA data, we perform a round of self-checks using GPT-4 (OpenAI, 2023). Finally, we conduct an round of manual filtering to remove questions with obvious issues or those related to multiple images or patient histories. The prompt templates used are shown in Table 9.

### Additional Results

**Generalization on Different Backbones.** To demonstrate the compatibility of our approach across different backbone models, we apply it to LLaVA-Med-1.0. As shown in Table 10, our method delivers an average improvement of 40.3% over the original LLaVA-Med-1.0, further highlighting its effectiveness in enhancing RAG performance and its adaptability to various backbones. MMed-RAG can be transferred to different Med-LVLMs, yielding consistent improvements across various domains, demonstrating the compatibility of our method.

**Detailed Results of Other Med-LVLMs.** As shown in Table 11, we illustrate the detailed performance simply shown in Table 3.

## Appendix B Related Work

**Factuality in Med-LVLMs.** The rapid advancements in Large Vision-Language Models (LVLMs) (Liu et al., 2024;b) are beginning to influence the field of medical image analysis. Several Med-LVLMs (Li et al., 2023; Moor et al., 2023; Zhang et al., 2023; Wu et al., 2023b), have emerged, showing remarkable performance across different medical imaging modalities. Despite these advances, Med-LVLMs continue to present notable factual hallucination (Xia et al., 2024; Royer et al., 2024), generating textual outputs that contradict medical visual information. This raises concerns about potential misdiagnoses or overlooked conditions. Recently, benchmarks have been developed to assess the accuracy of Med-LVLMs in tasks such as visual question answering (VQA) and report generation (Xia et al., 2024; Royer et al., 2024). However, research aimed at enhancing the factual accuracy of Med-LVLMs remains relatively unexplored.

**Retrieval Augmented Generation in Med-LVLMs.** Retrieval-Augmented Generation (RAG) has proven to be a powerful technique for enhancing factual accuracy in language modeling (Gao et al.,

\begin{table}
\begin{tabular}{l|c c c c} \hline \hline Model & \multicolumn{2}{c}{IU-Xray} & \multicolumn{2}{c}{FairVLMed} \\  & VQA & RG & VQA & RG \\ \hline LLaVA-Med-1.0 & 61.73 & 8.74 & 59.54 & 10.59 \\ +MMed-RAG & **80.32** & **22.63** & **78.49** & **15.58** \\ \hline \hline \end{tabular}
\end{table}
Table 10: Performance on different backbones.

\begin{table}
\begin{tabular}{|l|} \hline
**Instruction [Round1]** \\ You are a professional medical expert. I will provide you with some medical reports. Please generate some questions with answers (the answer should be yes or no) based on the provided report. The subject of the questions should be the medical image or patient, not the report. \\ Below are the given report: \\ [REPORT] \\
**Instruction [Round2]** \\ Please double-check the questions and answers, including how the questions are asked and whether the answers are correct. You should only generate the questions with answers and no other unnecessary information. \\ Below are the given report and QA pairs in round1: \\ [REPORT] \\ [QA PAIRS R1] \\ \hline \end{tabular}
\end{table}
Table 9: The instruction to GPT-4 for generating QA pairs.

2023; Wu et al., 2023; Chen et al., 2024; Qu et al., 2024; Sun et al., 2024a). In the biomedical domain, RAG leverages external knowledge to guide the generation of Med-LVLMs, offering clear advantages in tasks such as medical VQA and report generation (Yuan et al., 2023; Kumar and Martinen, 2024; Tao et al., 2024; He et al., 2024; Sun et al., 2024b). However, these works mainly focus on enhancing the relevance of the retrieved contexts without considering the model's understanding of retrieved knowledge. Recently, RULE (Xia et al., 2024b) is proposed to use preference fine-tuning to reduce the model's over-reliance on retrieved contexts. However, it still overlooks misalignment issues caused by RAG, as well as the generalizability of the retriever given the diverse domains of input images. In response, we propose MMed-RAG to mitigate these risks, enhancing the factuality of Med-LVLMs by addressing these overlooked factors. This can lead to a better cross-modality and overall alignment to enhance the understanding of retrieved knowledge and visual information, ensuring more consistent and reliable performance across tasks.

## Appendix C Theoretical Analysis

In this section, we provide a theoretical analysis of the model obtained from equation 4 and examine how the image input and retrieved context influences the model. Recall that \(x_{v},y,x_{t},x_{r}\) denotes input medical image, groundtruth answer, question, and retrieved information, respectively.

### The Improvement on Cross-Modality Alignment

We first consider the loss for cross-modality alignment,

\[\mathcal{L}_{cm}=-\mathbb{E}_{(x,y_{w,o},y_{t,o})\sim\mathcal{D}_{cm}}\left[ \log\sigma\left(\alpha\log\frac{\pi_{\theta}(y_{w,o}|x)}{\pi_{\theta}(y_{w,o }|x)}-\alpha\log\frac{\pi_{\theta}(y_{l,o}|x)}{\pi_{\theta}(y_{l,o}|x)}\right) \right].\] (5)

where \((x_{v},y_{w,o})\sim q_{w}(x_{w},y_{w,o}|x_{t},x_{r})\) and \((x_{l},y_{l,o})\sim q_{l}(x_{l},y_{l,o}|x_{t},x_{r})\) represent distributions of the preferred responses and dispreferred responses on \(\mathcal{D}_{cm}\), respectively. Let \(x\) denote \((x_{v},x_{r},x_{t})\)

**Definition C.1**: _Define the weight of \(x_{v}\) with respect to \(\log\pi_{\theta}(y|x)\) as_

\[\text{wt}(x_{v},\pi_{\theta}):=\mathbb{E}_{y\sim\pi_{\theta}(\cdot|x)}\left[ \frac{\partial}{\partial x_{v}}\log\pi_{\theta}(y|x)\right]^{2}\] (6)

Definition C.1 describes how \(\log\pi_{\theta}(y|x)\) changes with respect to \(x_{v}\), and the weight is always non-dispreferred. We demonstrate that this is a reasonable definition through Lemma C.1.

**Lemma C.1**: _For linear model \(y=\theta_{1}x_{v}+\theta_{2}x_{t}+\epsilon\) such that \(\epsilon\sim N(0,1)\), \(\text{wt}(x_{v},\pi_{\theta})=\theta_{1}^{2}\)_

**Assumption C.1**: _Let \(h(x,y)\), abbreviate as \(h\), be_

\[h:=\left[\sum_{y}\pi_{o}(y|x)\left(\frac{q_{w}(y|x)}{q_{l}(y|x)}\right)^{\frac {1}{\alpha}}\right]^{-1}\left(\frac{q_{w}(y|x)}{q_{l}(y|x)}\right)^{\frac{1}{ \alpha}}\] (7)

_Assume that \(\text{wt}(x_{v},\pi_{o})<c^{2}\), where_

\[c=\sqrt{\left\|\sqrt{\pi_{o}(y|x)}\cdot\frac{\partial}{\partial x_{v}}h \right\|_{2}^{2}+\int\left(\frac{\partial}{\partial x_{v}}h\right)^{2}\frac{ \pi_{o}(y|x)}{h}dy}-\left\|\sqrt{\pi_{o}(y|x)}\cdot\frac{\partial}{\partial x _{v}}h\right\|_{2}\] (8)

\begin{table}
\begin{tabular}{l|c c|c c c} \hline \hline \multirow{2}{*}{Models} & \multicolumn{2}{c|}{**Radiology**} & \multicolumn{2}{c|}{**Ophthalmology**} & \multicolumn{2}{c}{**Pathology**} \\  & IU-Xray & MIMIC-CXR & Harvard-FairVLMed & Quilt-1M & PMC-OA (Pathology) \\ \hline LLaVA-Med-1.5 & 75.47 & 75.79 & 63.03 & 62.80 & 59.28 \\ MMed-RAG & **89.54** & **83.57** & **87.94** & **72.95** & **64.54** \\ \hline Med-Flamingo & 26.74 & 61.27 & 42.06 & 27.11 & 32.62 \\ MedVInT & 73.34 & 66.06 & 35.92 & 26.81 & 27.77 \\ RadFM & 26.67 & 69.30 & 52.47 & 27.02 & 25.12 \\ miniGPT-Med & 54.87 & 53.92 & 66.73 & 26.82 & 27.03 \\ \hline \hline \end{tabular}
\end{table}
Table 11: Model performance (%) of different Med-LVLMs based on LLaVA-Med-1.5 on medical VQA task.

Assumption C.1 requires that \(x_{v}\) has a small weight in \(\log\pi_{o}(y|x)\). A model \(\pi_{o}(y|x)\) independent of \(x_{v}\) could satisfy Assumption C.1. In this case, the reference model generates answers without using information from the image.

**Theorem C.1**: _Suppose that Assumption C.1 holds, cross-modality loss increase the weight of \(x_{v}\)._

\[\text{wt}(x_{v},\pi_{\theta})>\text{wt}(x_{v},\pi_{o})\] (9)

Theorem C.1 indicates that when the weight of \(x_{v}\) is too small in the initial model \(\pi_{o}(y|x)\), the cross-modality loss function adjusts the model to place greater emphasis on images, informed by the retrieved data. Intuitively, for any sample \((x,y)\), generating unrelated images causes the policy to rely less on images. By using samples from this distribution as negative samples, the new model diverges from the initial model, increasing its reliance on images.

### The Improvement on Overall Alignment

In this section, we analyze the improvement on overall alignment. Let \(q^{1}_{w}(x_{v},y_{w,o}|x_{t},x_{r})\) and \(q^{1}_{l}(x_{v},y_{l,o}|x_{t})\) represent distributions of the preferred responses and dispreferred responses on \(\mathcal{D}^{1}_{oa}\), respectively; \(q^{2}_{w}(x_{v},y_{w,o}|x_{t})\) and \(q^{2}_{l}(x_{v},y_{l,o}|x_{t},x_{r})\) represent distributions of the preferred responses and dispreferred responses on \(\mathcal{D}^{2}_{oa}\), respectively. Overall loss is defined by

\[\mathcal{L}_{oa}=-\mathbb{E}_{(x,y_{w,o},y_{l,o})\sim\mathcal{D}_{oa}}\left[ \log\sigma\left(\alpha\log\frac{\pi_{o}(y_{w,o}|x)}{\pi_{o}(y_{w,o}|x)}-\alpha \log\frac{\pi_{o}(y_{w,o}|x)}{\pi_{o}(y_{w,o}|x)}\right)\right].\] (10)

Consider \(\pi\) as the generative distribution underlying \(\mathcal{M}\), construction of \(\mathcal{D}^{1}_{oa}\) and \(\mathcal{D}^{2}_{oa}\) indicate that there is a significant gap between \(\pi(y|x_{v},x_{t},x_{r})\) and \(\pi(y|x_{v},x_{t},\tilde{x}_{r})\) for \(x_{r}\) generates true answer while \(\tilde{x}_{r}\) generate a false one.

**Assumption C.2**: _Assume that \(\pi(y|x_{x},x_{r},x_{t}):x\to y\) is L-lipschitz continuous on \(x_{r}\) for all \((x_{v},x_{t},y)\) such that \(|\pi(y|x_{v},x_{t},x_{r})-\pi(y|x_{v},x_{t},\tilde{x}_{r})|\leq L\cdot d_{x}(x _{r},\tilde{x}_{r})\), where \(d_{x}\) is any distance metric on the text space._

Based on Assumption C.2, \(\tilde{x}_{r}\) can be viewed as being far from the meaningful retrieved information \(x_{r}\), resulting in different weight in the model. Then, we claim in the following theorem that the overall loss in equation 10 can effectively leverage retrieved knowledge while training.

**Assumption C.3**: _Let \(h_{1}(x_{v},x_{t},x_{r}),\) abbreviate as \(h_{1}\), be_

\[h_{1}:=\left[\sum_{y}\pi_{o}(y|x)\left(\frac{q^{1}_{w}(y|x_{v},x_{t},x_{r})+ q^{2}_{w}(y|x_{v},x_{t})}{q^{1}_{l}(y|x_{v},x_{t})+q^{2}_{l}(y|x_{v},x_{t},x_{r})} \right)^{\frac{1}{\alpha}}\right]^{-1}\left(\frac{q^{1}_{w}(y|x_{v},x_{t},x_{ r})+q^{2}_{w}(y|x_{v},x_{t})}{q^{1}_{l}(y|x_{v},x_{t})+q^{2}_{l}(y|x_{v},x_{t},x_{ r})}\right)^{\frac{1}{\alpha}}\] (11)

_Assume that \(\text{wt}(x_{r},\pi_{o})<c_{1}^{2}\) and \(\text{wt}(\tilde{x}_{r},\pi_{o})>c_{2}^{2}\) where_

\[\begin{split}& c_{1}=\sqrt{\sqrt{\pi_{o}}\cdot\frac{\partial h_{1} }{\partial x_{r}}}\bigg{\|}^{2}_{2}+\int\left(\frac{\partial h_{1}}{\partial x _{r}}\right)^{2}\frac{\pi_{o}}{h_{1}}dy-\left\|\sqrt{\pi_{o}}\cdot\frac{ \partial h_{1}}{\partial x_{r}}\right\|_{2}\\ & c_{2}=\sqrt{\sqrt{\pi_{o}}\cdot\frac{\partial h_{1}}{\partial \tilde{x}_{r}}}\bigg{\|}^{2}_{2}+\int\left(\frac{\partial h_{1}}{\partial \tilde{x}_{r}}\right)^{2}\frac{\pi_{o}}{h_{1}}+\left(\frac{\partial\pi_{o}}{ \partial\tilde{x}_{r}}\right)^{2}\frac{h_{1}}{\pi_{o}}dy+\left\|\sqrt{\pi_{o}} \cdot\frac{\partial h_{1}}{\partial\tilde{x}_{r}}\right\|_{2}\end{split}\] (12)

**Theorem C.2**: _Suppose that Assumption C.3 holds, then overall loss 10 increase the weight of \(x_{r}\) and decrease the weight of \(\tilde{x}_{r}\)._

\[\text{wt}(x_{r},\pi_{\theta})>\text{wt}(x_{r},\pi_{o}),\ \ \text{wt}(\tilde{x}_{r},\pi_{ \theta})<\text{wt}(\tilde{x}_{r},\pi_{o})\] (13)

Theorem C.2 suggests that the model tend to improve the overall alignment. When \(\tilde{x}_{r}\) generates a false answer, the training procedure tends to reduce the reliance on \(\tilde{x}_{r}\), resulting in a decrease in the weight assigned to \(\tilde{x}_{r}\). Conversely, if \(x_{r}\) is helpful for generating the true answer, \(\pi_{\theta}(y|x)\) tend to enhance its use of \(x_{r}\).

## Appendix D Proofs for Theoretical Results in Section C

Here we provide proofs for the results in Section C.

### Notations

Let \(x_{v},y,x_{t},x_{r}\) be input medical image, ground-truth answer, question, and retrieved information, respectively. Denote \((x_{w},y_{w,o})\sim q_{p}(x_{w},y_{w,o}|x_{t},x_{r})\) and \((x_{l},y_{l,o})\sim q_{l}(x_{l},y_{l,o}|x_{t},x_{r})\) as distributions of the preferred responses and dispreferred responses. Let \(x\) denote \((x_{v},x_{r},x_{t})\). We aim to a fine-tune a generative model \(\pi_{\theta}(y|x,x_{t})\) through DPO loss (Rafailov et al., 2023):

\[\operatorname*{arg\,min}_{\pi_{\theta}}\mathbb{E}_{(x_{w},x_{l},y_{w,o},y_{l,o })\sim\mathcal{D}}U\left(\alpha\log\frac{\pi_{\theta}(y_{w,o}|x)}{\pi_{o}(y_{w,o}|x)}-\alpha\log\frac{\pi_{\theta}(y_{l,o}|x)}{\pi_{o}(y_{l,o}|x)}\right).\] (14)

where \(U(t)=\log(1+\exp(-t))\). Define the weight of \(x_{v}\) with respect to \(\log\pi_{\theta}(y|x)\) as

\[\text{wt}(x_{v},\pi_{\theta}):=\mathbb{E}_{y\sim\pi_{\theta}(\cdot|x)}\left[ \frac{\partial}{\partial x_{v}}\log\pi_{\theta}(y|x)\right]^{2}\] (15)

### Assumptions

**Assumption D.1**: _(Large parameter space) Assume that \(\pi(x_{v},y|x_{t},x_{r})\) lies in the optimization space \(\{\pi_{\theta},\theta\in\Theta\}\) such that \(\pi(x_{v},y|x_{t},x_{r})\propto\pi_{o}(x_{v},y|x_{t},x_{r})\left(\frac{q_{w}( x_{v},y|x_{t},x_{r})}{q_{l}(x_{v},y|x_{t},x_{r})}\right)^{\frac{1}{\alpha}}\)_

Assumption D.1 requires that the parameter space sufficiently large to ensure that \(\pi_{\theta}\) can achieve its global optimum, allowing us to represent the optimizer with a closed form.

**Assumption D.2**: _Let \(h(x,y)\), abbreviate as \(h\), be_

\[h:=\left[\sum_{y}\pi_{o}(y|x)\left(\frac{q_{w}(y|x)}{q_{l}(y|x)}\right)^{ \frac{1}{\alpha}}\right]^{-1}\left(\frac{q_{w}(y|x)}{q_{l}(y|x)}\right)^{\frac {1}{\alpha}}\] (16)

_Assume that \(\text{wt}(x_{v},\pi_{o})<c^{2}\), where_

\[c=\sqrt{\left\|\sqrt{\pi_{o}(y|x)}\cdot\frac{\partial}{\partial x_{v}}h \right\|_{2}^{2}+\int\left(\frac{\partial}{\partial x_{v}}h\right)^{2}\frac{ \pi_{o}(y|x)}{h}dy}-\left\|\sqrt{\pi_{o}(y|x)}\cdot\frac{\partial}{\partial x_ {v}}h\right\|_{2}\] (17)

**Assumption D.3**: _Let \(h_{1}(x_{v},x_{t},x_{r},y)\), abbreviate as \(h_{1}\), be_

\[h_{1}:=\left[\sum_{y}\pi_{o}(y|x)\left(\frac{q_{w}^{1}(y|x_{v},x_{t},x_{r})+q_{ w}^{2}(y|x_{v},x_{t})}{q_{l}^{2}(y|x_{v},x_{t})+q_{l}^{2}(y|x_{v},x_{t})} \right)^{\frac{1}{\alpha}}\right]^{-1}\left(\frac{q_{w}^{1}(y|x_{v},x_{t},x_{ r})+q_{w}^{2}(y|x_{v},x_{t})}{q_{l}^{2}(y|x_{v},x_{t})+q_{l}^{2}(y|x_{v},x_{t},x_{ r})}\right)^{\frac{1}{\alpha}}\] (18)

_Assume that \(\text{wt}(x_{r},\pi_{o})<c_{1}^{2}\) and \(\text{wt}(\tilde{x}_{r},\pi_{o})>c_{2}^{2}\), where_

\[\begin{split} c_{1}&=\sqrt{\sqrt{\pi_{o}}\cdot \frac{\partial h_{1}}{\partial x_{r}}\Big{\|}_{2}^{2}+\int\left(\frac{\partial h _{1}}{\partial x_{r}}\right)^{2}\frac{\pi_{o}}{h_{1}}dy}-\left\|\sqrt{\pi_{o}} \cdot\frac{\partial h_{1}}{\partial x_{r}}\right\|_{2}\\ c_{2}&=\sqrt{\left\|\sqrt{\pi_{o}}\cdot\frac{\partial h _{1}}{\partial\tilde{x}_{r}}\right\|_{2}^{2}+\int\left(\frac{\partial h_{1}}{ \partial\tilde{x}_{r}}\right)^{2}\frac{\pi_{o}}{h_{1}}+\left(\frac{\partial\pi _{o}}{\partial\tilde{x}_{r}}\right)^{2}\frac{h_{1}}{\pi_{o}}dy}+\left\|\sqrt{ \pi_{o}}\cdot\frac{\partial h_{1}}{\partial\tilde{x}_{r}}\right\|_{2}\end{split}\] (19)

### Proofs

**Lemma D.1**: _Suppose that Assumption D.1 hold, optimizing equation 14 gives_

\[\pi_{\theta}(y|x)\propto\pi_{o}(y|x)\left(\frac{q_{w}(y|x)}{q_{l}(y|x)}\right)^ {\frac{1}{\alpha}}\] (20)

Lemma D.1 indicates that the model tends to increase \(\pi_{o}(y|x)\) if \(q_{w}(y|x)>q_{l}(y|x)\), which is more likely to occur when \((x_{v},y)\) represents a preferred sample given \(x_{t}\) and \(x_{r}\). Below, we provide an application of Lemma D.1 using a linear regression example. Lemma D.1 is proved with Lemma D.2 and Lemma D.3.

**Lemma D.2**: _(Lemma C.1 in Chen et al. (2024d)) For \(a,b>0\), the following inequality holds_

\[a\cdot U(t)+b\cdot U(-t)\geq a\log(1+b/a)+b\log(1+a/b)\]

_and equality holds if and only if \(t=\log(a/b)\)_

**Lemma D.3**: _Denote_

\[\left\{\begin{array}{ll}p_{1}(x_{w},y_{w,o},x_{l},y_{l,o}|x_{t},x_{r})&=q_{w }(x_{w},y_{w,o}|x_{t},x_{r})\cdot q_{l}(x_{l},y_{l,o}|x_{t},x_{r})\\ p_{2}(x_{w},y_{w,o},x_{l},y_{l,o}|x_{t},x_{r})&=q_{l}(x_{w},y_{w,o}|x_{t},x_{r })\cdot q_{w}(x_{l},y_{l,o}|x_{t},x_{r})\end{array}\right.\]

_and abbreviated as \(p_{1}\) and \(p_{2}\) for notational convenience. Then,_

\[\begin{split} 2\mathbb{E}_{\mathcal{D}}\left[U\left(f(x_{w},y_{ w,o},x_{t},x_{r})-f(x_{l},y_{l,o},x_{t},x_{r})\right)\right]\\ \geq& 2\log 2-D_{\mathrm{KL}}\left(p_{1}\|\frac{p_{1}+p_{2}}{ 2}\right)-D_{\mathrm{KL}}\left(p_{2}\|\frac{p_{1}+p_{2}}{2}\right)\end{split}\] (21)

_Equality holds if and only if_

\[f(x,y)=g(x)+\log\frac{q_{w}(x_{v},y|x_{t},x_{r})}{q_{l}(x_{v},y|x_{t},x_{r})}\] (22)

_where \(g(x)\) is any function that is possibly dependent on \(x_{v}\), \(x_{t}\) and \(x_{r}\)._

**Proof D.1**: \[\begin{split}& 2\mathbb{E}_{\mathcal{D}}\left[U\left(f(x_{w},y_{ w,o},x_{t},x_{r})-f(x_{l},y_{l,o},x_{t},x_{r})\right)\right]\\ =&\int q(x_{t},x_{r})\cdot p_{1}\cdot U\left(f(x_{ w},y_{w,o},x_{t},x_{r})-f(x_{l},y_{l,o},x_{t},x_{r})\right)dxdy\\ &+\int q(x_{t},x_{r})\cdot p_{2}\cdot U\left(f(x_{l},y_{l,o},x_{t },x_{r})-f(x_{w},y_{w,o},x_{t},x_{r})\right)dxdy\\ \geq&\int q(x_{t},x_{r})\left[p_{1}\cdot\log\left(1 +\frac{p_{2}}{p_{1}}\right)+p_{2}\cdot\log\left(1+\frac{p_{1}}{p_{2}}\right) \right]dxdy\\ =& 2\log 2+\int q(x_{t},x_{r})\left[p_{1}\cdot\log \left(\frac{p_{1}+p_{2}}{2p_{1}}\right)+p_{2}\cdot\log\left(\frac{p_{1}+p_{2} }{2p_{2}}\right)\right]dxdy\\ =& 2\log 2-KL\left(p_{1}\|\frac{p_{1}+p_{2}}{2} \right)-KL\left(p_{2}\|\frac{p_{1}+p_{2}}{2}\right)\end{split}\] (23)

_where the first inequality follows from Lemma D.2. For equivalence,_

\[f(x,y_{w,o},x_{t},x_{r})-f(x_{l},y_{l,o},x_{t},x_{r})=\log\frac{q_{w}(x_{w},y _{w,o}|x_{t},x_{r})\cdot q_{l}(x_{l},y_{l,o}|x_{t},x_{r})}{q_{l}(x_{w},y_{w,o} |x_{t},x_{r})\cdot q_{w}(x_{l},y_{l,o}|x_{t},x_{r})}\] (24)

_Thus, for any \(x_{w},y_{w,o},x_{l},y_{l,o},x_{t},x_{r}\),_

\[f(x_{w},y_{w,o},x_{t},x_{r})-\log\frac{q_{w}(x_{w},y_{w,o}|x_{t},x_{r})}{q_{l} (x_{w},y_{w,o}|x_{t},x_{r})}=f(x_{l},y_{l,o},x_{t},x_{r})-\log\frac{q_{w}(x_{l },y_{l,o}|x_{t},x_{r})}{q_{l}(x_{l},y_{l,o}|x_{t},x_{r})}\] (25)

_Therefore, equation 25 holds if and only if there exists some \(g(x_{v},x_{t},x_{r})\) such that_

\[f(x_{v},x_{t},x_{r},y)=g(x_{t},x_{r})+\log\frac{q_{w}(x_{v},y|x_{t},x_{r})}{ q_{l}(x_{v},y|x_{t},x_{r})}\] (26)

Lemma D.3 provides a closed-form solution to equation 14 if the parameter space is sufficiently large. This lemma is crucial for the proof Lemma D.1, which follows below

**Proof D.2**: _According to the Assumption D.1, we have_

\[\pi(x_{v},y|x_{t},x_{r})=\hat{g}(x_{t},x_{r})\pi_{o}(x_{v},y|x_{t},x_{r}) \left(\frac{q_{w}(x_{v},y|x_{t},x_{r})}{q_{l}(x_{v},y|x_{t},x_{r})}\right)^{ \frac{1}{\alpha}}\] (27)_After reparameterization,_

\[\alpha\log\left(\frac{\pi(x_{v},y|x_{t},x_{r})}{\pi_{o}(x_{v},y|x_{t},x_{r})} \right)=\alpha\log[\hat{g}(x_{t},x_{r})]+\log\frac{q_{w}(x_{v},y|x_{t},x_{r})}{ q_{l}(x_{v},y|x_{t},x_{r})}\] (28)

_which is the global minimum of_

\[\operatorname*{arg\,min}_{f}\mathbb{E}_{\mathcal{D}}\left[U\left(f(x_{w},y_{w,o},x_{t},x_{r})-f(x_{l},y_{l,o},x_{t},x_{r})\right)\right]\] (29)

_by Lemma D.3. Since \(\pi(x_{v},y|x_{t},x_{r})\in\{\pi_{\theta},\theta\in\Theta\}\) lies in the optimization space, we have_

\[\begin{split}&\min_{f}\mathbb{E}_{\mathcal{D}}U\left(f(x_{w},y_{w,o},x_{t},x_{r})-f(x_{l},y_{l,o},x_{t},x_{r})\right)\\ =&\min_{\pi_{\theta}}\mathbb{E}_{\mathcal{D}}U\left( \alpha\log\frac{\pi_{\theta}(y_{w,o}|x_{w},x_{t},x_{r})}{\pi_{o}(y_{w,o}|x_{ w},x_{t},x_{r})}-\alpha\log\frac{\pi_{\theta}(y_{l,o}|x_{l},x_{t},x_{r})}{\pi_{o}(y_{l,o}|x_{l},x_{t},x_{r})}\right)\end{split}\] (30)

_and \(\pi_{\theta}(x_{v},y|x_{t},x_{r})\) is the optimizer of equation 30, which gives_

\[\begin{split}&\alpha\log\left(\frac{\pi_{\theta}(x_{v},y|x_{t},x_{r})}{ \pi_{o}(x_{v},y|x_{t},x_{r})}\right)=g(x_{t},x_{r})+\log\frac{q_{w}(x_{v},y|x_ {t},x_{r})}{q_{l}(x_{v},y|x_{t},x_{r})}\\ \Longrightarrow&\pi_{\theta}(x_{v},y|x_{t},x_{r})= \pi_{o}(x_{v},y|x_{t},x_{r})\left(\frac{q_{w}(x_{v},y|x_{t},x_{r})}{q_{l}(x_ {v},y|x_{t},x_{r})}\right)^{\frac{1}{\alpha}}\exp\left(\frac{1}{\alpha}g(x_{t},x_{r})\right)\end{split}\] (31)

_Then_

\[\begin{split}\pi_{\theta}(y|x)&=\frac{\pi_{\theta}(x _{v},y|x_{t},x_{r})}{\pi_{\theta}(x|x_{t},x_{r})}=\frac{\pi_{o}(x_{v},y|x_{t},x _{r})\left(\frac{q_{v}(x_{v},y|x_{t},x_{r})}{q_{l}(x_{v},y|x_{t},x_{r})}\right) ^{\frac{1}{\alpha}}\exp\left(\frac{1}{\alpha}(g(x_{t},x_{r})\right)}{\sum_{y} \pi_{o}(x_{v},y|x_{t},x_{r})\left(\frac{q_{w}(x_{v},y|x_{t},x_{r})}{q_{l}(x_{ v},y|x_{t},x_{r})}\right)^{\frac{1}{\alpha}}\exp\left(\frac{1}{\alpha}(g(x_{t},x_{r})\right)\right.}\\ &=\frac{\pi_{o}(y|x)\left(\frac{q_{w}(x_{v},y|x_{t},x_{r})}{q_{l} (x_{v},y|x_{t},x_{r})}\right)^{\frac{1}{\alpha}}}{\sum_{y}\pi_{o}(y|x)\left( \frac{q_{w}(x_{v},y|x_{t},x_{r})}{q_{l}(x_{v},y|x_{t},x_{r})}\right)^{\frac{1} {\alpha}}}=\frac{\pi_{o}(y|x)\left(\frac{q_{w}(y|x_{v},x_{t},x_{r})}{q_{l}(y |x_{v},x_{t},x_{r})}\right)^{\frac{1}{\alpha}}}{\sum_{y}\pi_{o}(y|x)\left( \frac{q_{w}(y|x_{v},x_{t},x_{r})}{q_{l}(y|x_{v},x_{t},x_{r})}\right)^{\frac{1} {\alpha}}}\end{split}\] (32)

**Corollary D.1**: _Suppose that preferred responses \((x_{w},y_{w})\) and dispreferred responses \((x_{l},y_{l})\) satisfy \(y_{w}=\beta x_{w}+\epsilon_{1}\) and \(y_{l}=\tilde{\beta}x_{l}+\epsilon_{2}\) respectively. DPO for \(y=\theta x_{v}+\epsilon_{3}\) is based on reference model \(y=\theta_{o}x_{v}+\epsilon_{4}\), where \(\epsilon_{i}\)'s are independent and follow standard normal distribution. Then,_

\[\theta=\theta_{o}+\frac{1}{\alpha}(\beta-\tilde{\beta})\] (33)

Corollary D.1 is a direct application of Lemma D.1, indicating that the model updates coefficient \(\theta_{o}\) towards the direction of \(\beta\) for preferred responses and away from \(\tilde{\beta}\) for dispreferred responses.

**Proof D.3**: _Let \(\phi(\cdot)\) denote the probability density function of standard normal, by Lemma D.1,_

\[\begin{split}&\phi(y-\theta x)\propto\phi(y-\theta_{o}x)\left( \frac{\phi(y-\beta x)}{\phi(y-\tilde{\beta}x)}\right)^{\frac{1}{\alpha}}\\ \Longrightarrow&\exp\left(\frac{1}{2}y^{2}-\theta_{1} xy\right)\propto\exp\left(\frac{1}{2}y^{2}-\theta_{o}xy\right)\cdot\exp\left(- \frac{1}{\alpha}(\beta-\tilde{\beta})xy\right)\\ \Longrightarrow&\exp\left(\theta_{1}xy\right)\propto \exp\left(\theta_{o}xy\right)\cdot\exp\left(\frac{1}{\alpha}(\beta-\tilde{ \beta})xy\right)\\ \Longrightarrow&\theta=\theta_{o}+\frac{1}{\alpha}( \beta-\tilde{\beta})\end{split}\] (34)

**Lemma D.4**: _For linear model \(y=\theta_{1}x_{v}+\theta_{2}x_{t}+\epsilon\) such that \(\epsilon\sim N(0,1)\), wt\((x_{v},\pi_{\theta})=\theta_{1}^{2}\)_

**Proof D.4**: _Let \(\phi(\cdot)\) denote the probability density function of standard normal,_

\[\begin{split}\text{wt}(x_{v},\pi_{\theta})&=\int\left( -\frac{1}{2}\frac{\partial}{\partial x_{v}}\left(y-\theta_{1}x_{v}-\theta_{2}x _{t}\right)^{2}\right)^{2}\phi(y-\theta_{1}x_{v}-\theta_{2}x_{t})dy\\ &=\theta_{1}^{2}\int\left(y-\theta_{1}x_{v}-\theta_{2}x_{t} \right)^{2}\phi(y-\theta_{1}x_{v}-\theta_{2}x_{t})dy\\ &=\theta_{1}^{2}\int\left(\theta_{1}x_{v}+\theta_{2}x_{t}-y \right)\frac{d\phi(y-\theta_{1}x_{v}-\theta_{2}x_{t})}{dy}dy\\ &=\theta_{1}^{2}\int\phi(y-\theta_{1}x_{v}-\theta_{2}x_{t})dy= \theta_{1}^{2}\end{split}\] (35)

**Theorem D.2**: _Suppose that Assumption D.2 holds, then cross-modality increase the weight of \(x_{v}\)._

\[\text{wt}(x_{v},\pi_{\theta})>\text{wt}(x_{v},\pi_{o})\] (36)

**Proof D.5**: _By Lemma D.1, we have_

\[\pi_{\theta}(y|x)=\pi_{o}(y|x)\cdot h(x,y),\ \int\pi_{o}(y|x)\cdot h(x,y)dy=1\] (37)

_Abbreviate \(h(x,y)\) and \(\pi_{o}(y|x_{v},x_{t})\) as \(h\) and \(\pi_{o}\) respectively, we have_

\[\begin{split}\text{wt}(x_{v},\pi_{\theta})-\text{wt}(x_{v},\pi_{o })&\geq\int\left(\frac{\frac{\partial}{\partial x_{v}}\pi_{o}}{ \pi_{o}}+\frac{\frac{\partial}{\partial x_{v}}h}{h}\right)^{2}\pi_{o}h\ dy- \text{wt}(x_{v},\pi_{o})\\ &\geq\int\left[\frac{\partial}{\partial x_{v}}h\right]^{2}\frac{ \pi_{o}}{h}dy-2\sqrt{\text{wt}(x_{v},\pi_{o})}\cdot\left\|\sqrt{\pi_{o}}\cdot \frac{\partial}{\partial x_{v}}h\right\|_{2}-\text{wt}(x_{v},\pi_{o})\end{split}\] (38)

_the second inequality follows from Cauchy-Schwarz inequality_

\[\int\frac{\partial}{\partial x_{v}}\pi_{o}\cdot\frac{\partial}{\partial x_{v} }h\ dy=\int\frac{\partial}{\partial x_{v}}\pi_{o}\cdot\frac{\sqrt{\pi_{o}}}{ \sqrt{\pi_{o}}}\cdot\frac{\partial}{\partial x_{v}}h\ dy\leq\sqrt{\text{wt}( x_{v},\pi_{o})}\cdot\left\|\sqrt{\pi_{o}}\cdot\frac{\partial}{\partial x_{v}}h \right\|_{2}\] (39)

_Denote \(c\) as_

\[c:=\sqrt{\left\|\sqrt{\pi_{o}}\cdot\frac{\partial}{\partial x_{v}}h\right\|_{2 }^{2}+\int\left(\frac{\partial}{\partial x_{v}}h\right)^{2}\frac{\pi_{o}}{h} dy}-\left\|\sqrt{\pi_{o}}\cdot\frac{\partial}{\partial x_{v}}h\right\|_{2}\] (40)

_the last term in equation 38 is equivalent to_

\[\left(c-\sqrt{\text{wt}(x_{v},\pi_{o})}\right)\cdot\left(\sqrt{\text{wt}(x_{v },\pi_{o})}+c+2\left\|\sqrt{\pi_{o}}\cdot\frac{\partial}{\partial x_{v}}h \right\|_{2}\right)\] (41)

_Thus, \(\text{wt}(x_{v},\pi_{\theta})>\text{wt}(x_{v},\pi_{o})\) if \(\sqrt{\text{wt}(x_{v},\pi_{o})}<c\)._

**Theorem D.3**: _Suppose that Assumption D.3 holds, the overall loss increase the weight of \(x_{r}\) and decrease the weight of \(\tilde{x}_{r}\)._

\[\text{wt}(x_{r},\pi_{\theta})>\text{wt}(x_{r},\pi_{o}),\ \ \ \text{wt}(\tilde{x}_{r},\pi_{\theta})<\text{wt}(\tilde{x}_{r},\pi_{o})\] (42)

**Proof D.6**: _The distribution of preferred responses can be considered as a mixture distribution: \(q_{w}^{1}(x_{v},y_{w,o}|x_{t},x_{r})+q_{w}^{2}(x_{v},y_{w,o}|x_{t})\). Similarly, for dispreferred responses, the distribution is represented as \(q_{l}^{1}(x_{v},y_{o,}|x_{t})+q_{l}^{2}(x_{v},y_{l,o}|x_{t},x_{r})\). By Lemma D.1,_

\[\pi_{\theta}(y|x)=\pi_{o}(y|x)\cdot h_{1}(x,y),\ \ \int\pi_{o}(y|x)\cdot h_{1}(x,y)dy=1\] (43)

_Abbreviate \(h_{1}(x,y)\) as \(h_{1}\). Follow the same procedure in the proof of Theorem D.2,_

\[\begin{split}\text{wt}(x_{r},\pi_{\theta})-\text{wt}(x_{r},\pi_{o })&\geq\int\left[\frac{\partial}{\partial x_{r}}h_{1}\right]^{2} \frac{\pi_{o}}{h_{1}}dy-2\sqrt{\text{wt}(x_{r},\pi_{o})}\cdot\left\|\sqrt{ \pi_{o}}\cdot\frac{\partial}{\partial x_{r}}h_{1}\right\|_{2}-\text{wt}(x_{r}, \pi_{o})\\ &=\left(c_{1}-\sqrt{\text{wt}(x_{r},\pi_{o})}\right)\cdot\left( \sqrt{\text{wt}(x_{r},\pi_{o})}+c_{1}+2\left\|\sqrt{\pi_{o}}\cdot\frac{ \partial}{\partial x_{r}}h_{1}\right\|_{2}\right)\end{split}\] (44)_where we apply Cauchy-Schwarz inequality in equation 44._

\[c_{1}=\sqrt{\left\|\sqrt{\pi_{o}(y|x)}\cdot\frac{\partial}{\partial x_{r}}h_{1} \right\|_{2}^{2}+\int\left(\frac{\partial}{\partial x_{r}}h_{1}\right)^{2}\frac {\pi_{o}(y|x)}{h_{1}}dy}-\left\|\sqrt{\pi_{o}(y|x)}\cdot\frac{\partial}{\partial x _{r}}h_{1}\right\|_{2}\] (45)

_Thus, \(\text{wt}(x_{r},\pi_{\theta})>\text{wt}(x_{r},\pi_{o})\) if \(\sqrt{\text{wt}(x_{r},\pi_{o})}<c_{1}\). Again, by Cauchy-Schwarz inequality_

\[\begin{split}&\text{wt}(\tilde{x}_{r},\pi_{\theta})-\text{wt}( \tilde{x}_{r},\pi_{o})\\ \leq&\int\left(\frac{\partial h_{1}}{\partial\tilde{ x}_{r}}\right)^{2}\frac{\pi_{o}}{h_{1}}+\left(\frac{\partial\pi_{o}}{\partial \tilde{x}_{r}}\right)^{2}\frac{h_{1}}{\pi_{o}}dy+2\sqrt{\text{wt}(\tilde{x}_{ r},\pi_{o})}\cdot\left\|\sqrt{\pi_{o}}\cdot\frac{\partial h_{1}}{\partial\tilde{x}_{r}} \right\|_{2}-\text{wt}(\tilde{x}_{r},\pi_{o})\\ =&-\left(\sqrt{\text{wt}(\tilde{x}_{r},\pi_{o})}-c_{ 2}\right)\cdot\left(\sqrt{\text{wt}(\tilde{x}_{r},\pi_{o})}-c_{2}+2\left\|\sqrt {\pi_{o}}\cdot\frac{\partial}{\partial\tilde{x}_{r}}h_{1}\right\|_{2}\right) \end{split}\] (46)

_where_

\[c_{2}=\sqrt{\left\|\sqrt{\pi_{o}}\cdot\frac{\partial}{\partial\tilde{x}_{r}}h _{1}\right\|_{2}^{2}+\int\left(\frac{\partial}{\partial\tilde{x}_{r}}h_{1} \right)^{2}\frac{\pi_{o}}{h_{1}}+\left(\frac{\partial}{\partial\tilde{x}_{r}} \pi_{o}\right)^{2}\frac{h_{1}}{\pi_{o}}dy+\left\|\sqrt{\pi_{o}}\cdot\frac{ \partial}{\partial\tilde{x}_{r}}h_{1}\right\|_{2}}\] (47)

_Thus, \(\text{wt}(x_{r},\pi_{\theta})<\text{wt}(x_{r},\pi_{o})\) if \(\sqrt{\text{wt}(x_{r},\pi_{o})}>c_{2}\)._