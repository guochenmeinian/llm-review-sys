# Overcoming the Sim-to-Real Gap: Leveraging Simulation to Learn to Explore for Real-World RL

 Andrew Wagenmaker

University of California, Berkeley

&Kevin Huang

University of Washington

&Liyiming Ke

University of Washington

&Kevin Jamieson

University of Washington

&Abhishek Gupta

University of Washington

Correspondance to: ajwagen@berkeley.edu

###### Abstract

In order to mitigate the sample complexity of real-world reinforcement learning, common practice is to first train a policy in a simulator where samples are cheap, and then deploy this policy in the real world, with the hope that it generalizes effectively. Such _direct sim2real_ transfer is not guaranteed to succeed, however, and in cases where it fails, it is unclear how to best utilize the simulator. In this work, we show that in many regimes, while direct sim2real transfer may fail, we can utilize the simulator to learn a set of _exploratory_ policies which enable efficient exploration in the real world. In particular, in the setting of low-rank MDPs, we show that coupling these exploratory policies with simple, practical approaches--least-squares regression oracles and naive randomized exploration--yields a polynomial sample complexity in the real world, an exponential improvement over direct sim2real transfer, or learning without access to a simulator. To the best of our knowledge, this is the first evidence that simulation transfer yields a provable gain in reinforcement learning in settings where direct sim2real transfer fails. We validate our theoretical results on several realistic robotic simulators and a real-world robotic sim2real task, demonstrating that transferring exploratory policies can yield substantial gains in practice as well.

## 1 Introduction

Over the last decade, reinforcement learning (RL) techniques have been deployed to solve a variety of real-world problems, with applications in robotics, the natural sciences, and beyond [27, 54, 52, 26, 46, 23]. While promising, the broad application of RL methods has been severely limited by its large sample complexity--the number of interactions with the environment required for the algorithm to learn to solve the desired task. In applications of interest, it is often the case that collecting samples is very costly, and the number of samples required by RL algorithms is prohibitively expensive.

In many domains, while collecting samples in the desired deployment environment may be very costly, we have access to a _simulator_ where the cost of samples is virtually nonexistent. As a concrete example, in robotic applications where the goal is real-world deployment, directly training in the real world typically requires an infeasibly large number of samples. However, it is often possible to obtain a simulator--derived from first principles or knowledge of the robot's actuation--which provides an approximate model of the real-world deployment environment. Given such a simulator, common practice is to first train a policy to accomplish the desired task in the simulator, and then deploy it in the real world, with the hope that the policy generalizes effectively from the simulator to the goal deployment environment. Indeed, such "sim2real" transfer has become a key piece in theapplication of RL to robotic settings, as well as many other domains of interest such as the natural sciences [12; 15], and is a promising approach towards reducing the sample complexity of RL in real-world deployment [19; 4; 18].

Effective sim2real transfer can be challenging, however, as there is often a non-trivial mismatch between the simulated and real environments. The real world is difficult to model perfectly, and some discrepancy is inevitable. As such, directly transferring the policy trained in the simulator to the real world often fails, the mismatch between sim and real causing the policy--which may perfectly solve the task in sim--to never solve the task in real. While some attempts have been made to address this--for example, utilizing domain randomization to extend the space of environments covered by simulator [60; 49], or finetuning the policy learned in sim in the real world [50; 73]--these approaches are not guaranteed to succeed. In settings where such methods fail, can we still utilize a simulator to speed up real-world RL?

In this work we take steps towards developing principled approaches to sim2real transfer that addresses this question. Our key intuition is that it is often _easier to learn to explore than to learn to solve the goal task_. While solving the goal task may require very precise actions, collecting high-quality exploratory data can require significantly less precision. For example, successfully solving a complex robotic manipulation task requires a particular sequence of motions, but obtaining a policy that will interact with the object of interest in some way, providing useful exploratory data on its behavior, would require significantly less precision.

Formally, we show that, in the setting of low-rank MDPs where there is a mismatch in the dynamics between the "sim" and "real" environments, even when this mismatch is such that direct sim2real transfer fails, under certain conditions we can still effectively transfer a set of _exploratory_ policies from sim to real. In particular, we demonstrate that access to such exploratory policies, coupled with random exploration and a least-squares regression oracle--which are insufficient for efficient learning on their own, but often still favored in practice due to their simplicity--enable provably efficient learning in real. Our results therefore demonstrate that simulators, when carefully applied, can yield a provable--exponential--gain over both naive sim2real transfer and learning without a simulator, and enable algorithms commonly used in practice to learn efficiently.

Furthermore, our results motivate a simple, easy-to-implement algorithmic principle: rather than training and transferring a policy that solves the task in the simulator, utilize the simulator to train a set of exploratory policies, and transfer these, coupled with random exploration, to generate high

Figure 1: **Left: Overview of our approach compared to standard sim2real transfer on puck pushing task. Standard sim2real transfer first trains a policy to solve the goal task in sim and then transfers this policy to real. This policy may fail to solve the task in real due to the sim2real gap, and furthermore may not provide sufficient data coverage to successfully learn a policy that does solve the goal task in real. In contrast, our approach trains a set of exploratory policies in sim which achieve high-coverage data when deployed in real, even if they are unable to solve the task 0-shot. This high-coverage data can then be used to successfully learn a policy that solves the goal task in real. Right: Quantitative results running our approach on the puck pushing task illustrated on left, compared to standard sim2real transfer. Over 6 real-world trials, our approach solves the task 6/6 times while standard sim2real transfer solves the task 0/6 times.**

quality exploratory data in real. We show experimentally--through a realistic robotic simulator and real-world sim2real transfer problem on the Franka robot platform--that this principle of transferring exploratory policies from sim to real yields a significant practical gain in sample efficiency, often enabling efficient learning in settings where naive transfer fails completely (see Figure 1).

## 2 Related Work

**Provable Transfer in RL.** Perhaps the first theoretical result on transfer in RL is the "simulation lemma", which transforms a bound on the total-variation distance between the dynamics to a bound on policy value [24; 25; 6; 22]--we argue that we can do significantly better with exploration transfer. More recent work has considered transfer in the setting of block MDPs [34], but requires relatively strong assumptions on the similarity between source and target MDPs, or the meta-RL setting [69], but only consider tabular MDPs, and assume the target MDP is covered by the training distribution. Perhaps most relevant to this work is the work of [36], which presents several lower bounds showing that efficient transfer in RL is not feasible in general. In relation to this work, our work can be seen as providing a set of sufficient conditions that do enable efficient transfer; the lower bounds presented in [36] do not hold in the low-rank MDP setting we consider. Several other works exist, but either consider different types of transfer than what we consider (e.g., observation space mismatch), or only learn a policy that has suboptimality bounded by the sim2real mismatch [37; 56; 58]. Another somewhat tangential line of work considers representation transfer in RL, where it is assumed the source and target tasks share a common representation [35; 10; 2]. We remark as well that the formal sim2real setting we consider is a special case of the MF-MDP setting of [53].

**Simulators and Low-Rank MDPs.** Several existing works show that there are provable benefits to training a policy in "simulation" due to the ability to reset on command [67; 33; 5; 68; 70; 42]. These works do not consider the transfer problem, however. The setting of linear and low-rank MDPs which we consider has seen a significant amount of attention over the last several years, and many provably efficient algorithms exist [21; 1; 62; 63; 43; 41]. These works typically assume access to powerful oracles which enable efficient learning; we only consider access to a simple regression oracle. Beyond the theory literature, recent work has also shown that low-rank MDPs can effectively model a variety of standard RL settings in practice [72].

**Sim2Real Transfer in Practice.** The sim2real literature is vast and we only highlight particularly relevant works here; see [74] for a full survey. To mitigate the inconsistency between the simulator and real world's physical parameters and modeling, domain randomization creates a variety of simulated environments with randomized properties to develop a robust policy [60; 49; 44; 8; 39]. Domain adaptation instead constructs encoding of deployment conditions (e.g., physical condition or past histories) and adapts to the deployment environment by matching the encoding [29; 9; 66; 55; 38; 40]. In contrast, our work assumes a fundamental sim2real mismatch where we do not expect the real system to match the simulator for any parameter settings. A related line of work shows that policies trained with robust exploration strategies generalize better to disturbed or unseen environments [13; 20]. Our work is complimentary to this work in that our goal is not to transfer a policy that solves the task in new environment, but rather explores the environment.

## 3 Preliminaries

We let \(\triangle_{\mathcal{X}}\) denote the set of distributions over set \(\mathcal{X}\), \([H]:=\{1,2,\ldots,H\}\), and \(\|P-Q\|_{\mathrm{TV}}\) the total-variation distance between distributions \(P\) and \(Q\).

Markov Decision Processes.We consider the setting of episodic Markov Decision Processes (MDPs). An MDP is denoted by a tuple \(\mathcal{M}=(\mathcal{S},\mathcal{A},\{P_{h}\}_{h=1}^{H},\{r_{h}\}_{h=1}^{H},s _{1},H)\), where \(\mathcal{S}\) denotes the set of states, \(\mathcal{A}\) the set of actions, \(P_{h}:\mathcal{S}\times\mathcal{A}\rightarrow\triangle_{\mathcal{S}}\) the transition function, \(r_{h}:\mathcal{S}\times\mathcal{A}\rightarrow[0,1]\) the reward (which we assume is deterministic and known), \(s_{1}\) the initial state, and \(H\) the horizon. We assume \(\mathcal{A}\) is finite and denote \(A:=|\mathcal{A}|\). Interaction with an MDP starts from state \(s_{1}\), the agent takes some action \(a_{1}\), transitions to state \(s_{2}\sim P_{1}(\cdot\mid s_{1},a_{1})\), and receives reward \(r_{1}(s_{1},a_{1})\). This process continues for \(H\) steps at which points the episode terminates, and the process resets.

The goal of the learner is to find a policy \(\pi=\{\pi_{h}\}_{h=1}^{H}\), \(\pi_{h}:\mathcal{S}\rightarrow\triangle_{\mathcal{A}}\), that achieves maximum reward. We can quantify the reward received by some policy \(\pi\) in terms of the value and \(Q\)-valuefunctions. The \(Q\)-value function is defined as \(Q_{h}^{\pi}(s,a):=\mathbb{E}^{\pi}[\sum_{a^{\prime}=h^{\prime}}^{H}r_{h^{\prime}} (s_{h^{\prime}},a_{h^{\prime}})\mid s_{h}=s,a_{h}=a]\), and value function is defined in terms of the \(Q\)-value function as \(V_{h}^{\pi}(s):=\mathbb{E}_{a\sim\pi_{h}(\cdot|s)}[Q_{h}^{\pi}(s,a)]\). The value of policy \(\pi\), its expected reward, is denoted by \(V_{0}^{\pi}:=V_{1}^{\pi}(s_{1})\), and the value of the optimal policy, the maximum achievable reward, by \(V_{0}^{\pi}:=\sup_{\pi}V_{0}^{\pi}\).

In this work we are interested in the setting where we wish to solve some task in the "real" environment, represented as an MDP, and we have access to a simulator which approximates the real environment in some sense. We denote the real MDP as \(\mathcal{M}^{\mathsf{real}}\), and the simulator as \(\mathcal{M}^{\mathsf{sim}}\). We assume that \(\mathcal{M}^{\mathsf{real}}\) and \(\mathcal{M}^{\mathsf{sim}}\) have the same state and actions spaces, reward function, and initial state, but different transition functions, \(P^{\mathsf{real}}\) and \(P^{\mathsf{sim}}\). We denote value functions in \(\mathcal{M}^{\mathsf{real}}\) and \(\mathcal{M}^{\mathsf{sim}}\) as \(V_{h}^{\mathsf{real},\pi}(s)\) and \(V_{h}^{\mathsf{sim},\pi}(s)\), respectively. We make the following assumption.

**Assumption 1**.: _For all \((s,a,h)\in\mathcal{S}\times\mathcal{A}\times[H]\) and some \(\epsilon_{\mathrm{sim}}>0\), we have:_

\[\|P_{h}^{\mathsf{real}}(\cdot\mid s,a)-P_{h}^{\mathsf{sim}}(\cdot\mid s,a)\|_{ \mathrm{TV}}\leq\epsilon_{\mathrm{sim}}.\]

We do not assume that the value of \(\epsilon_{\mathrm{sim}}\) is known, simply that there exists some such \(\epsilon_{\mathrm{sim}}\).

Function Approximation.In order to enable efficient learning, some structure on the MDPs of interest is required. We will assume that \(\mathcal{M}^{\mathsf{real}}\) and \(\mathcal{M}^{\mathsf{sim}}\) are low-rank MDPs, as defined below.

**Definition 3.1** (Low-Rank MDP).: We say an MDP is a _low-rank_ MDP with dimension \(d\) if there exists some featurization \(\bm{\phi}:\mathcal{S}\times\mathcal{A}\to\mathbb{R}^{d}\) and measure \(\bm{\mu}:[H]\times\mathcal{S}\to\mathbb{R}^{d}\) such that:

\[P_{h}(\cdot\mid s,a)=\langle\bm{\phi}(s,a),\bm{\mu}_{h}(\cdot)\rangle,\quad \forall s,a,h.\]

We assume that \(\|\bm{\phi}(s,a)\|_{2}\leq 1\) for all \((s,a)\), and for all \(h\), \(\|\bm{\mu}_{h}(\mathcal{S})\|_{2}=\|\int_{s\in\mathcal{S}}|\mathrm{d}\bm{\mu}_ {h}(s)|\|_{2}\leq\sqrt{d}\).

Formally, we make the following assumption on the structure of \(\mathcal{M}^{\mathsf{sim}}\) and \(\mathcal{M}^{\mathsf{real}}\).

**Assumption 2**.: _Both \(\mathcal{M}^{\mathsf{sim}}\) and \(\mathcal{M}^{\mathsf{real}}\) satisfy Definition 3.1 with feature maps and measures \((\bm{\phi}^{\mathsf{s}},\bm{\mu}^{\mathsf{s}})\) and \((\bm{\phi}^{\mathsf{r}},\bm{\mu}^{\mathsf{r}})\), respectively. Furthermore, \(\bm{\phi}^{\mathsf{s}}\) is known, but all of \(\bm{\mu}^{\mathsf{s}},\bm{\phi}^{\mathsf{r}}\), and \(\bm{\mu}^{\mathsf{r}}\) are unknown._

In the literature, MDPs satisfying Definition 3.1 but where \(\bm{\phi}\) is known are typically referred to as "linear" MDPs, while MDPs satisfying Definition 3.1 but with \(\bm{\phi}\) unknown are typically referred to as "low-rank" MDPs. Given this terminology, we have that \(\mathcal{M}^{\mathsf{sim}}\) is a linear MDP2, while \(\mathcal{M}^{\mathsf{real}}\) is a low-rank MDP. We assume the following reachability condition on \(\mathcal{M}^{\mathsf{sim}}\).

Footnote 2: The assumption that \(\bm{\phi}^{\mathsf{s}}\) is known is for simplicity only—similar results could be obtained were \(\bm{\phi}^{\mathsf{s}}\) also unknown using more complex algorithmic tools in \(\mathcal{M}^{\mathsf{sim}}\).

**Assumption 3**.: _There \(\exists\lambda_{\min}^{\star}>0\) with \(\min_{h}\sup_{\pi}\lambda_{\min}(\mathbb{E}^{\mathcal{M}^{\mathsf{sim}},\pi}[ \bm{\phi}^{\mathsf{s}}(s_{h},a_{h})\bm{\phi}^{\mathsf{s}}(s_{h},a_{h})^{ \top}])\geq\lambda_{\min}^{\star}\)._

Assumption 3 posits that each direction in the feature space in our simulator can be activated by some policy, and can be thought of as a measure of how easily each direction can be reached. Similar assumptions have appeared before in the literature on linear and low-rank MDPs [71, 3, 2]. Note that we only require this reachability assumption in \(\mathcal{M}^{\mathsf{sim}}\). We also assume we are given access to function classes \(\mathcal{F}_{h}:\mathcal{S}\times\mathcal{A}\to[0,H]\) and let \(\mathcal{F}:=\mathcal{F}_{1}\times\mathcal{F}_{2}\times\ldots\times\mathcal{F }_{H}\). Since no reward is collected in the \((H+1)\)th step we take \(f_{H+1}=0\). For any \(f:\mathcal{S}\times\mathcal{A}\to\mathbb{R}\), we let \(\pi_{h}^{f}(s):=\arg\max_{a\in\mathcal{A}}f_{h}(s,a)\). We define the _Bellman operator_ on some function \(f_{h+1}:\mathcal{S}\times\mathcal{A}\to\mathbb{R}\) as:

\[\mathcal{T}f_{h+1}(s,a):=r_{h}(s,a)+\mathbb{E}_{s^{\prime}\sim P_{h}(\cdot|s,a)}[ \max_{a^{\prime}}f_{h+1}(s^{\prime},a^{\prime})].\]

We make the following standard assumption on \(\mathcal{F}\).

**Assumption 4** (Bellman Completeness).: _For all \(f_{h+1}\in\mathcal{F}_{h+1}\), we have \(\mathcal{T}^{\mathsf{real}}f_{h+1},\mathcal{T}^{\mathsf{sim}}f_{h+1}\in\mathcal{F }_{h}\), where \(\mathcal{T}^{\mathsf{real}}\) and \(\mathcal{T}^{\mathsf{sim}}\) denote the Bellman operators on \(\mathcal{M}^{\mathsf{real}}\) and \(\mathcal{M}^{\mathsf{sim}}\), respectively._

PAC Reinforcement Learning.Our goal is to find a policy \(\widehat{\pi}\) that achieves maximum reward in \(\mathcal{M}^{\mathsf{real}}\). Formally, we consider the PAC (Probably-Approximately-Correct) RL setting.

**Definition 3.2** (PAC Reinforcement Learning).: Given some \(\epsilon>0\) and \(\delta>0\), with probability at least \(1-\delta\) identify some policy \(\widehat{\pi}\) such that: \(V_{0}^{\mathsf{real},\widehat{\pi}}\geq V_{0}^{\mathsf{real},\star}-\epsilon\).

We will be particularly interested in solving the PAC RL problem with the aid of a simulator, using the minimum number of samples from \(\mathcal{M}^{\mathsf{real}}\) possible, as we will formalize in the following. As we will see, while it is straightforward to achieve this objective using \(\mathcal{M}^{\mathsf{sim}}\) if \(\epsilon=\mathcal{O}(\epsilon_{\mathrm{sim}})\), naive transfer methods can fail to achieve this completely if \(\epsilon\ll\epsilon_{\mathrm{sim}}\). As such, our primary focus will be on developing efficient \(\mathsf{sim}\)2real methods in this regime.

Theoretical Results

In this section we provide our main theoretical results. We first present two negative results: in Section 4.1 showing that "naive exploration"--utilizing only a least-squares regression oracle and random exploration approaches such as \(\zeta\)-greedy3--is provably inefficient, and in Section 4.2 showing that directly transferring the optimal policy from \(\mathcal{M}^{\mathsf{sim}}\) to \(\mathcal{M}^{\mathsf{real}}\) is unable to efficiently obtain a policy with suboptimality better than \(\mathcal{O}(\epsilon_{\mathrm{sim}})\) in real. Then in Section 4.3 we present our main positive result, showing that by utilizing the same oracles as in Sections 4.1 and 4.2--a least-squares regression oracle, simulator access, and the ability to take actions randomly--we _can_ efficiently learn an \(\epsilon\)-optimal policy for \(\epsilon\ll\epsilon_{\mathrm{sim}}\) in \(\mathcal{M}^{\mathsf{real}}\) by carefully utilizing the simulator to learn exploration policies.

Footnote 3: Throughout this paper, we use “\(\zeta\)-greedy” to refer to the method more commonly known as “\(\epsilon\)-greedy” in the literature, to avoid ambiguity between this \(\epsilon\) and the \(\epsilon\) in our definition of PAC RL, Definition 3.2.

### Naive Exploration is Provably Inefficient

While a variety of works have developed provably efficient methods for solving PAC RL in low-rank MDPs [1, 62, 43, 41], these works typically either rely on complex computation oracles or carefully directed exploration strategies which are rarely utilized in practice. In contrast, RL methods utilized in practice typically rely on "simple" computation oracles and exploration strategies. Before considering the sim2real setting, we first show that such "simple" strategies are insufficient for efficient PAC RL. To instantiate such strategies, we consider a least-squares regression oracle, often available in practice.

**Oracle 4.1** (Least-Squares Regression Oracle).: We assume access to a least-squares regression oracle such that, for any \(h\) and dataset \(\mathfrak{D}=\{(s^{t},a^{t},y^{t})\}_{t=1}^{T}\), we can compute \(\arg\min_{f\in\mathcal{F}_{h}}\sum_{t=1}^{T}(f(s^{t},a^{t})-y^{t})^{2}\).

We couple this oracle with "naive exploration", which here we use to refer to any method that explores by randomly perturbing the action recommended by the current estimate of the optimal policy. While a variety of instantiations of naive exploration exist (see e.g. [11]), we consider a particularly common formulation, \(\zeta\)-greedy exploration.

**Protocol 4.1** (\(\zeta\)-Greedy Exploration).: Given access to a regression oracle, any \(\zeta\in[0,1]\), and time horizon \(T\), consider the following protocol:

1. Interact with \(\mathcal{M}^{\mathsf{real}}\) for \(T\) episodes. At every step of episode \(t+1\), play \(\pi_{h}^{f^{t}}(s)\) with probability \(1-\zeta\), and \(a\sim\mathrm{unif}(\mathcal{A})\) otherwise, where: \[f_{h}^{t}=\arg\min_{f\in\mathcal{F}_{h}}\sum_{t^{\prime}=1}^{t}(f(s_{h}^{t^{ \prime}},a_{h}^{t^{\prime}})-r_{h}^{t^{\prime}}-\max_{a^{\prime}}f_{h+1}^{t}( s_{h+1}^{t^{\prime}},a^{\prime}))^{2}.\]
2. Using collected data in any way desired, propose a policy \(\widehat{\pi}\).

Protocol 4.1 forms the backbone of many algorithms used in practice. Despite its common application, as existing work [11] and the following result show, it is provably inefficient.

**Proposition 1**.: _For any \(H>1\), \(\zeta\in[0,1]\), and \(c\leq 1/6\), there exist some \(\mathcal{M}^{\mathsf{real},1}\) and \(\mathcal{M}^{\mathsf{real},2}\) such that both \(\mathcal{M}^{\mathsf{real},1}\) and \(\mathcal{M}^{\mathsf{real},2}\) satisfy Assumptions 2 and 4, and unless \(T\geq\Omega(2^{H/2})\), when running Protocol 4.1 we have:_

\[\sup_{\mathcal{M}^{\mathsf{real}}\in\{\mathcal{M}^{\mathsf{real},1},\mathcal{M }^{\mathsf{real},2}\}}\mathbb{E}^{\mathcal{M}^{\mathsf{real}}}[V_{0}^{\mathcal{ M}^{\mathsf{real}},\star}-V_{0}^{\mathcal{M}^{\mathsf{real}},\widehat{\pi}}]\geq c/32.\]

Proposition 1 shows that, in a minimax sense, \(\zeta\)-greedy exploration is insufficient for provably efficient reinforcement learning: on one of \(\mathcal{M}^{\mathsf{real},1}\) and \(\mathcal{M}^{\mathsf{real},2}\), \(\zeta\)-greedy exploration will only be able to find a policy that is suboptimal by a constant factor, unless we take an exponentially large number of samples. While we focus on \(\zeta\)-greedy exploration in Proposition 1, this result extends to other types of naive exploration, for example, those given in [11]. See Section 5.2 for further discussion of the construction for Proposition 1.

### Understanding the Limits of Direct sim2real Transfer

Proposition 1 shows that in general utilizing a least-squares regression oracle with \(\zeta\)-greedy exploration is insufficient for provably efficient RL. Can this be made efficient with access to a simulator \(\mathcal{M}^{\mathsf{sim}}\)? In practice, standard \(\mathsf{sim2real}\) methodology typically trains a policy to accomplish the goal task in \(\mathcal{M}^{\mathsf{sim}}\), and then transfers this policy to \(\mathcal{M}^{\mathsf{real}}\). We refer to this methodology as _direct_\(\mathsf{sim2real}\)_transfer_. The following canonical result, usually referred to as the "simulation lemma" [24; 25; 6; 22], provides a sufficient guarantee for direct \(\mathsf{sim2real}\) transfer to succeed under Assumption 1.

**Proposition 2** (Simulation Lemma).: _Let \(\pi^{\mathsf{sim},\star}\) denote an optimal policy in \(\mathcal{M}^{\mathsf{sim}}\). Then under Assumption 1 we have \(V_{0}^{\mathsf{real},\mathsf{v}^{\mathsf{sim},\star}}\geq V_{0}^{\mathsf{real}, \star}-2H^{2}\epsilon_{\mathrm{sim}}\)._

Proposition 2 shows that, as long as \(\epsilon\geq 2H^{2}\epsilon_{\mathrm{sim}}\), direct \(\mathsf{sim2real}\) transfer succeeds in obtaining an \(\epsilon\)-optimal policy in \(\mathcal{M}^{\mathsf{real}}\). While this justifies direct \(\mathsf{sim2real}\) transfer in settings where \(\mathcal{M}^{\mathsf{sim}}\) and \(\mathcal{M}^{\mathsf{real}}\) are sufficiently close, we next show that given access only to \(\pi^{\mathsf{sim},\star}\) and a least-squares regression oracle--even when coupled with random exploration--we cannot hope to efficiently obtain a policy with suboptimality less than \(\mathcal{O}(\epsilon_{\mathrm{sim}})\) on \(\mathcal{M}^{\mathsf{real}}\) using naive exploration. To formalize this, we consider the following interaction protocol.

**Protocol 4.2** (Direct \(\mathsf{sim2real}\) Transfer with Naive Exploration).: Given access to \(\pi^{\mathsf{sim},\star}\), an optimal policy in \(\mathcal{M}^{\mathsf{sim}}\), any \(\zeta\in[0,1]\), and time horizon \(T\), consider the following protocol:

1. [noitemsep,topsep=0pt]
2. Interact with \(\mathcal{M}^{\mathsf{real}}\) for \(T\) episodes, and at each step \(h\) and state \(s\) play \(\pi^{\mathsf{sim},\star}_{h}(\cdot\mid s)\) with probability \(1-\zeta\), and \(a\sim\mathrm{unif}(\mathcal{A})\) with probability \(\zeta\).
3. Using collected data in any way desired, propose a policy \(\widehat{\pi}\).

Protocol 4.2 is a standard instantiation of direct \(\mathsf{sim2real}\) transfer commonly found in the literature, and couples playing the optimal policy from \(\mathcal{M}^{\mathsf{sim}}\) with naive exploration. We have the following.

**Proposition 3**.: _With the same choice of \(\mathcal{M}^{\mathsf{real},1}\) and \(\mathcal{M}^{\mathsf{real},2}\) as in Proposition 1, there exists some \(\mathcal{M}^{\mathsf{sim}}\) such that both \(\mathcal{M}^{\mathsf{real},1}\) and \(\mathcal{M}^{\mathsf{real},2}\) satisfy Assumption 1 with \(\mathcal{M}^{\mathsf{sim}}\) for \(\epsilon_{\mathrm{sim}}\gets c\), Assumptions 2 to 4 hold, and unless \(T\geq\Omega(2^{H})\) when running Protocol 4.2, we have:_

\[\sup_{\mathcal{M}^{\mathsf{real}}\in\{\mathcal{M}^{\mathsf{real},1},\mathcal{M }^{\mathsf{real},2}\}}\mathbb{E}^{\mathcal{M}^{\mathsf{real}}}[V_{0}^{\mathcal{ M}^{\mathsf{real}},\star}-V_{0}^{\mathcal{M}^{\mathsf{real}},\widehat{\pi}}]\geq \epsilon_{\mathrm{sim}}/32.\]

Proposition 3 shows that there exists a setting where there are two possible \(\mathcal{M}^{\mathsf{real}}\) satisfying Assumption 1 with \(\mathcal{M}^{\mathsf{sim}}\), and where, using direct policy transfer, unless we interact with \(\mathcal{M}^{\mathsf{real}}\) for exponentially many episodes (in \(H\)), we cannot determine a better than \(\Omega(\epsilon_{\mathrm{sim}})\)-optimal policy for the worst-case \(\mathcal{M}^{\mathsf{real}}\). Together, Propositions 2 and 3 show that, while we can utilize direct \(\mathsf{sim2real}\) transfer to learn a policy that is \(\mathcal{O}(\epsilon_{\mathrm{sim}})\)-optimal in \(\mathcal{M}^{\mathsf{real}}\), if our goal is to learn an \(\epsilon\)-optimal policy for \(\epsilon\ll\epsilon_{\mathrm{sim}}\), direct \(\mathsf{sim2real}\) transfer is unable to efficiently achieve this.

### Efficient \(\mathsf{sim2real}\) Transfer via Exploration Policy Transfer

Does there exist _some_ way to utilize \(\mathcal{M}^{\mathsf{sim}}\) and a least-squares regression oracle to enable efficient learning in \(\mathcal{M}^{\mathsf{real}}\), even when \(\epsilon\ll\epsilon_{\mathrm{sim}}\)? Our key insight is that, rather than transferring the policy that optimally solves the task in \(\mathcal{M}^{\mathsf{sim}}\), we should instead transfer policies that _explore_ effectively in \(\mathcal{M}^{\mathsf{sim}}\). While learning to solve a task may require very precise actions, we can often obtain sufficiently rich data with relatively imprecise actions--it is easier to learn to explore than learn to solve a task. In such settings, directly transferring a policy to solve the task will likely fail due to imprecision in the simulator, but it may be possible to still transfer a policy that generates exploratory data. To formalize this, we consider the following access model to \(\mathcal{M}^{\mathsf{sim}}\).

**Oracle 4.2** (\(\mathcal{M}^{\mathsf{sim}}\) Access).: We may interact with \(\mathcal{M}^{\mathsf{sim}}\) by either:

1. [noitemsep,topsep=0pt]
2. **(Trajectory Sampling)** For any policy \(\pi\), sampling a trajectory \(\{(s_{h},a_{h},r_{h},s_{h+1})\}_{h=1}^{H}\) generated by playing \(\pi\) on \(\mathcal{M}^{\mathsf{sim}}\).
3. **(Policy Optimization)** For any reward \(\widetilde{r}\), computing a policy \(\pi^{\mathsf{sim}}(\widetilde{r})\) maximizing \(\widetilde{r}\) on \(\mathcal{M}^{\mathsf{sim}}\).

While access to such a policy optimization oracle is unrealistic in \(\mathcal{M}^{\mathsf{real}}\), where we want to minimize the number of samples collected, given cheap access to samples in \(\mathcal{M}^{\mathsf{sim}}\), such an oracle can often be (approximately) implemented in practice4. Note that under Oracle 4.2 we only assume _black-box_access to our simulator--rather than allowing the behavior of the simulator to be queried at arbitrary states, we are simply allowed to roll out policies on \(\mathcal{M}^{\mathsf{sim}}\), and compute optimal policies. Given Oracle 4.2, as well as our least-squares regression oracle, Oracle 4.1, we propose the following algorithm.

```
1:input: budget \(T\), confidence \(\delta\), simulator \(\mathcal{M}^{\mathsf{sim}}\)// Learn policies \(\Pi^{h}_{\exp}\) which cover features space in \(\mathcal{M}^{\mathsf{sim}}\)
2:\(\Pi^{h}_{\exp}\leftarrow\textsc{LearnExpPolicies}(\mathcal{M}^{\mathsf{sim}}, \delta,\frac{4A^{\sharp}_{\mathsf{c}}}{H},h)\) (Algorithm 5) for all \(h\in[H]\)
3:\(\tilde{\Pi}^{h}_{\exp}\leftarrow\{\tilde{\pi}_{\exp}:\tilde{\pi}_{\exp}\text{ plays }\pi_{\exp}\text{ up to step }h\), then plays actions randomly, \(\forall\pi_{\exp}\in\Pi^{h}_{\exp}\}\)// Explore in \(\mathcal{M}^{\mathsf{real}}\) via \(\tilde{\Pi}^{\mathsf{c}}_{\exp}\)
4: Play \(\pi_{\exp}\sim\text{unif}(\{\inf(\tilde{\Pi}^{h}_{\exp})\}_{h=1}^{H})\) for \(T/2\) episodes in \(\mathcal{M}^{\mathsf{real}}\), add data to \(\mathfrak{D}\)// Estimate optimal policy on collected data
5:for\(h=H,H-1,\ldots,1\)do
6:\(\widehat{f}_{h}\leftarrow\arg\min_{f\in F}\sum_{(s,a,r,s^{\prime})\in \mathfrak{D}}(f_{h}(s,a)-r-\max_{a^{\prime}}\widehat{f}_{h+1}(s^{\prime},a^{ \prime}))^{2}\)
7: Compute \(\pi^{\mathsf{sim},*}\) via Oracle 4.2
8: Play \(\pi^{\mathsf{sim},*}\) for \(T/4\) episodes in real, compute average return \(\widehat{V}^{\mathsf{real},\pi^{\mathsf{sim},*}}_{0}\)
9: Play \(\pi^{\widehat{f}}\) for \(T/4\) episodes in real, compute average return \(\widehat{V}^{\mathsf{real},\pi^{\widehat{f}}}_{0}\)
10:return\(\widehat{\pi}\leftarrow\arg\max_{\pi\in\{\pi^{\widehat{f}},\pi^{\mathsf{sim},*} \}}\widehat{V}^{\mathsf{real},\pi}_{0}\) ```

**Algorithm 1**\(\mathsf{sim}\)2real Exploration Policy Transfer

Algorithm 1 first calls a subroutine LearnExpPolicies, which learns a set of policies that provide rich data coverage on \(\mathcal{M}^{\mathsf{sim}}\)--precisely, LearnExpPolicies returns policies \(\{\Pi^{h}_{\exp}\}_{h\in[H]}\) which induce covariates with lower-bounded minimum eigenvalue on \(\mathcal{M}^{\mathsf{sim}}\) and relies only on Oracle 4.2 (as well as knowledge of the featurization of \(\mathcal{M}^{\mathsf{sim}}\), \(\bm{\phi}^{\mathsf{s}}\)) to find such policies. Algorithm 1 then plays these exploration policies in \(\mathcal{M}^{\mathsf{real}}\), coupled with random exploration, and applies the regression oracle to the data they collect. Finally, it estimates the value of the policy learned by the regression oracle and \(\pi^{\mathsf{sim},*}\), and returns whichever is best. We have the following.

**Theorem 1**.: _If Assumptions 1 to 4 hold and_

\[\epsilon_{\mathrm{sim}}\leq\frac{\lambda^{\mathsf{sim}}_{\mathrm{sim}}}{644HA^ {3}},\] (4.1)

_then as long as_

\[T\geq c\cdot\frac{d^{2}H^{16}}{\epsilon^{8}}\cdot\log\frac{H|\mathcal{F}|}{ \delta},\]

_with probability at least \(1-\delta\), Algorithm 1 returns a policy \(\widehat{\pi}\) such that \(V^{\mathsf{real},*}_{0}-V^{\mathsf{real},\widehat{\pi}}_{0}\leq\epsilon\), and Oracles 4.1 and 4.2 are invoked at most \(\mathrm{poly}(d,H,\epsilon^{-1},\log\frac{1}{\delta})\) times._

Theorem 1 shows that, as long as \(\epsilon_{\mathrm{sim}}\) satisfies (4.1), utilizing a simulator and least-squares regression oracle, Oracles 4.1 and 4.2, allows for efficient learning in \(\mathcal{M}^{\mathsf{real}}\), achieving a complexity scaling polynomially in problem parameters. This yields an _exponential improvement_ over learning without a simulator using naive exploration or direct \(\mathsf{sim}\)2real transfer--which Propositions 1 and 3 show have complexity scaling exponentially in the horizon--despite utilizing the same practical computation oracles. To the best of our knowledge, this result provides the first theoretical evidence that \(\mathsf{sim}\)2real transfer can yield provable gains in RL beyond trivial settings where direct transfer succeeds.

Note that the condition in (4.1) is independent of \(\epsilon\)--unlike direct \(\mathsf{sim}\)2real transfer, which requires \(\epsilon=\mathcal{O}(\epsilon_{\mathrm{sim}})\), we simply must assume \(\epsilon_{\mathrm{sim}}\) is small enough that (4.1) holds, and Theorem 1 shows that we can efficiently learn an \(\epsilon\)-optimal policy in \(\mathcal{M}^{\mathsf{real}}\) for any \(\epsilon>0\). In Appendix B.4, we also present an extended version of Theorem 1, Theorem 3, which utilizes data from \(\mathcal{M}^{\mathsf{sim}}\) to reduce the dependence on \(\log|\mathcal{F}|\). In particular, instead of scaling with \(\log|\mathcal{F}|\), it only scales with the log-cardinality of functions that are (approximately) Bellman-consistent on \(\mathcal{M}^{\mathsf{sim}}\). To illustrate the effectiveness of Theorem 1, we return to the instance of Propositions 1 and 3, where naive exploration and direct \(\mathsf{sim}\)2real transfer fails. We have the following.

**Proposition 4**.: _In the setting of Propositions 1 and 3 and assuming that \(\epsilon_{\mathrm{sim}}\leq\frac{1}{8192}\cdot\frac{1}{H}\), running Algorithm 1 will require \(\mathrm{poly}(H,\epsilon^{-1})\cdot\log\frac{1}{\delta}\) samples from \(\mathcal{M}^{\mathsf{real}}\) in order to identify an \(\epsilon\)-optimal policy in \(\mathcal{M}^{\mathsf{real}}\) with probability at least \(1-\delta\), for any \(\epsilon>0\)._Note that the condition required by Proposition 4 is simply that \(\epsilon_{\text{sim}}\lesssim 1/H\)--as long as our simulator satisfies this condition, we can efficiently transfer exploration policies to learn an \(\epsilon\)-optimal policy, for any \(\epsilon>0\), while naive methods would be limited to only obtaining an \(\Omega(1/H)\)-optimal policy.

**Remark 4.1** (Necessity of Random Exploration).: Algorithm 1 achieves efficient exploration in \(\mathcal{M}^{\text{real}}\) by learning policies \(\Pi_{\exp}^{h}\) in \(\mathcal{M}^{\text{sim}}\) that span the feature space of \(\mathcal{M}^{\text{sim}}\) (Line 2), and then playing these policies in \(\mathcal{M}^{\text{real}}\), coupled with random exploration (Line 4). This use of random exploration is critical to obtaining Theorem 1. As we show in Proposition 5, if we omit the random exploration, Assumption 1 is not sufficient to guarantee \(\Pi_{\exp}^{h}\) explores effectively in \(\mathcal{M}^{\text{real}}\), even when (4.1) holds.

**Remark 4.2** (Computational Efficiency).: Algorithm 1, as well as its main subroutine LearnExpPolicies, relies only on calls to Oracle 4.1 and Oracle 4.2. Thus, assuming we can efficiently implement these oracles, which is often the case in problem settings of interest, Algorithm 1 can be run in a computationally efficient manner.

## 5 Practical Algorithm and Experiments

We next validate the effectiveness of our proposal in practice: can a set of diverse exploration policies obtained from simulation improve the efficiency of real-world reinforcement learning? We start by showing that this holds for a simple, didactic, tabular environment in Section 5.2. From here, we consider several more realistic task domains: simulators inspired by real-world robotic manipulation tasks (sim2sim transfer, Section 5.3); and an actual real-world sim2real experiment on a Franka robotic platform (sim2real transfer, Section 5.4). Further details on all experiments, including additional baselines, can be found in Appendix E. Before stating our experimental results, we first provide a practical instantiation of Algorithm 1 that we can apply with real robotic systems and neural network function approximators.

### Practical Instantiation of Exploration Policy Transfer

The key idea behind Algorithm 1 is quite simple: learn a set of exploratory policies in \(\mathcal{M}^{\text{sim}}\)--policies which provide rich data coverage in \(\mathcal{M}^{\text{sim}}\)--and transfer these policies to \(\mathcal{M}^{\text{real}}\), coupled with random exploration, using the collected data to determine a near-optimal policy for \(\mathcal{M}^{\text{real}}\). Algorithm 1 provides a particular instantiation of this principle, learning exploratory policies in \(\mathcal{M}^{\text{sim}}\) via the LearnExpPolicies subroutine, which aims to cover the feature space of \(\mathcal{M}^{\text{sim}}\), and utilizing a least-squares regression oracle to compute an optimal policy given the data collected in \(\mathcal{M}^{\text{real}}\). In practice, however, other instantiations of this principle are possible by replacing LearnExpPolicies with any procedure which generates exploratory policies in \(\mathcal{M}^{\text{sim}}\), and replacing the regression oracle with any RL algorithm able to learn from off-policy data. We consider a general meta-algorithm instantiating this in Algorithm 2.

```
1:Input: Simulator \(\mathcal{M}^{\text{sim}}\), real environment \(\mathcal{M}^{\text{real}}\), simulator budget \(T_{\text{sim}}\), real budget \(T\), algorithm to generate exploratory policies in sim \(\mathfrak{A}_{\exp}\), algorithm to solve policy optimization in real \(\mathfrak{A}_{\mathrm{po}}\)// Learn exploratory policies in \(\mathcal{M}^{\text{sim}}\)
2:Run \(\mathfrak{A}_{\exp}\) for \(T_{\text{sim}}\) steps in \(\mathcal{M}^{\text{sim}}\) to generate set of exploratory policies \(\Pi_{\exp}\)// Deploy exploratory policies in \(\mathcal{M}^{\text{real}}\)
3:for\(t=1,2,\dots,T/2\)do
4:Draw \(\pi_{\exp}\sim\mathrm{unif}(\Pi_{\exp})\), play in \(\mathcal{M}^{\text{real}}\) for one episode, add data to replay buffer of \(\mathfrak{A}_{\mathrm{po}}\)
5:Run \(\mathfrak{A}_{\mathrm{po}}\) for one episode // optional if\(\mathfrak{A}_{\mathrm{po}}\)learns fully offline ```

**Algorithm 2** Practical sim2real Exploration Policy Transfer Meta-Algorithm

In practice, \(\mathfrak{A}_{\exp}\) and \(\mathfrak{A}_{\mathrm{po}}\) can be instantiated with a variety of algorithms. For example, we might take \(\mathfrak{A}_{\exp}\) to be an RND [7] or bootstrapped Q-learning-style [45, 31] algorithm, or any unsupervised RL procedure [48, 14, 32, 47], and \(\mathfrak{A}_{\mathrm{po}}\) to be an off-policy policy optimization algorithm such as soft actor-critic (SAC) [16] or implicit \(Q\)-learning (IQL) [28].

For the following experiments, we instantiate Algorithm 2 by setting \(\mathfrak{A}_{\exp}\) to an algorithm inspired by recent work on inducing diverse behaviors in RL [14, 30], and \(\mathfrak{A}_{\mathrm{po}}\) to SAC. In particular, \(\mathfrak{A}_{\exp}\) simultaneously trains an ensemble of policies \(\Pi_{\exp}=\{\pi_{\exp}^{i}\}_{i=1}^{n}\) and a discriminator \(d_{\theta}:\mathcal{S}\times[n]\to\mathbb{R}\), where \(d_{\theta}\) is trained to discriminate between the behaviors of each policy \(\pi_{\exp}^{i}\), and is optimized on a weighting of the true task reward and the exploration reward induced by the discriminator, \(r_{e}(s,i):=\log\frac{\exp(d_{\theta}(s,i))}{\sum_{j\in[n]}\exp(d_{\theta}(s,j))}\). As shown in existing work [14; 30], this simple training objective effectively induces diverse behavior with temporally correlated exploration while remaining within the vicinity of the optimal policy, using standard optimization techniques. Note that the particular choice of algorithm is less critical here than abiding by the recipes laid out in the meta-algorithm (Algorithm 2). The particular instantiation that we run for our experiments is detailed in Algorithm 6, along with further details in Appendix E.2.

### Didactic Combination Lock Experiment

We first consider a variant of the construction used to prove Propositions 1 and 3, itself a variant of the classic combination lock instance. We illustrate this instance in Figure 2. Unless noted, all transitions occur with probability 1, and rewards are 0. Here, in \(\mathcal{M}^{\text{sim}}\) the optimal policy, \(\pi^{\text{sim},*}\), plays action \(a_{2}\) for all steps \(h<H-1\), while in \(\mathcal{M}^{\text{real}}\), the optimal policy plays action \(a_{1}\) at every step. Which policy is optimal is determined by the outgoing transition from \(s_{1}\) at the \((H-1)\)th step and, as such, to identify the optimal policy, any algorithm must reach \(s_{1}\) at the \((H-1)\)th step. As \(s_{1}\) will only be reached at step \(H-1\) by playing \(a_{1}\) for \(H-1\) consecutive times, any algorithm relying on naive exploration will take exponentially long to identify the optimal policy. Furthermore, playing \(\pi^{\text{sim},*}\) coupled with random exploration will similarly take an exponential number of episodes, since \(\pi^{\text{sim},*}\) always plays \(a_{2}\). As such, both direct sim2real policy transfer as well as \(Q\)-learning with naive exploration (Protocol 4.1) will fail to find the optimal policy in \(\mathcal{M}^{\text{real}}\). However, if we transfer exploratory policies from \(\mathcal{M}^{\text{sim}}\), since \(\mathcal{M}^{\text{sim}}\) and \(\mathcal{M}^{\text{real}}\) behave identically up to step \(H-1\), these policies can efficiently traverse \(\mathcal{M}^{\text{real}}\), reach \(s_{1}\) at step \(H-1\), and identify the optimal policy. We compare our approach of exploration policy transfer to these baselines methods and illustrate the performance of each in Figure 2. As this is a simple tabular instance, we implement Algorithm 1 directly here. As Figure 2 shows, the intuition described above leads to real gains in practice--exploration policy transfer quickly identifies the optimal policy, while more naive approach fail completely over the time horizon we considered.

### Realistic Robotics \(\mathsf{sim2sim}\) Experiment

To test the ability of our proposed method to scale to more complex problems, we next experiment on a \(\mathsf{sim2sim}\) transfer setting with a realistic robotic simulator. We consider TychoEnv, a simulator of the 7DOF Tycho robotics platform introduced by [73], and shown in Figure 3. We test \(\mathsf{sim2sim}\) transfer on a reaching task where the goal is to touch a small ball hanging in the air with the tip of the chopstick end effector. The agent perceives the ball and its own end effector pose and outputs a delta in its desired end effector pose as a command. We set \(\mathcal{M}^{\text{sim}}\) and \(\mathcal{M}^{\text{real}}\) to be two instances of TychoEnv with slightly different parameters to model real-world \(\mathsf{sim2real}\) transfer. Precisely, we change the action bounds and control frequency from \(\mathcal{M}^{\text{sim}}\) to \(\mathcal{M}^{\text{real}}\).

We aim to compare our approach of exploration policy transfer with direct \(\mathsf{sim2real}\) policy transfer. To this end, we first train a policy in \(\mathcal{M}^{\text{sim}}\) that solves the task in \(\mathcal{M}^{\text{sim}}\), \(\pi^{\text{sim},*}\), and then utilize this policy in place of \(\Pi_{\exp}\) in Algorithm 2. We instantiate our approach of exploration policy transfer as outlined above. Our aim in this experiment is to illustrate how the quality of the data provided by direct policy transfer vs. exploration policy transfer affects learning. As such, for both approaches we simply initialize our SAC agent in \(\mathcal{M}^{\text{real}}\), \(\mathfrak{A}_{\mathrm{po}}\), from scratch, and set the reward in \(\mathcal{M}^{\text{real}}\) to be sparse: the agent only receives a non-zero reward if it successfully touches the ball. For each approach, we repeat the process of training in \(\mathcal{M}^{\text{sim}}\) four times, and for each of these run them for two trials in \(\mathcal{M}^{\text{real}}\).

Figure 3: TychoEnv Reach Task Setup

Figure 2: **Left: Illustration ofCombination Lock Example. Right: Results on Combination Lock.**

We illustrate our results in Figure 4. As this figure illustrates, direct policy transfer fails to learn completely, while exploration policy transfer successfully solves the task. Investigating the behavior of each method, we find that the policies transferred via exploration policy transfer, while failing to solve the task with perfect accuracy, when coupled with naive exploration are able to successfully make contact with the ball on occasion. This provides sufficiently rich data for SAC to ultimately learn to solve the task. In contrast, direct policy transfer fails to collect any reward when run in \(\mathcal{M}^{\text{real}}\), and, given the sparse reward nature of the task, SAC is unable to locate any reward and learn.We include an additional sim2sim experiment on the Franka Emika Panda Robot Arm in Appendix E.4.

### Real-World Robotic sim2real Experiment

Finally, we demonstrate our algorithm for actual sim2real policy transfer for a manipulation task on a real-world Franka Emika Panda robot arm [17] with a parallel gripper. Our task is to push a 75mm diameter cylindrical "puck" from the center to the edge of the surface, as shown in Figure 1, with the arm initialized at random locations. The observed state \(s=[\mathbf{p}_{\mathrm{ee}},\mathbf{p}_{\mathrm{obj}}]\in\mathbb{R}^{4}\) consists of the planar Cartesian coordinate of the end effector \(\mathbf{p}_{\mathrm{ee}}\) along with the center of mass of the puck \(\mathbf{p}_{\mathrm{obj}}\). Our policy outputs planar end effector position deltas \(a=\Delta\mathbf{p}_{\mathrm{ee}}\in\mathbb{R}^{2}\), evaluated at 8 Hz, which are passed into a lower-level joint position PID controller. We use an Intel Realsense D435 depth camera to track the location of the puck. Our reward function is a sum of a success indicator (indicating when the puck has been pushed to the edge of the surface) and terms which give negative reward if the distance from the end effector to the puck, or puck to the goal, are too large (see (E.1)); in particular, a reward greater than 0 indicates success.

We run the instantiation of Algorithm 2 outlined above. In particular, we train an ensemble of \(n=15\) exploration policies, training for 20 million steps in \(\mathcal{M}^{\text{sim}}\). In addition, we train a policy that solves the task in \(\mathcal{M}^{\text{sim}}\), \(\pi^{\text{sim},\star}\). We use a custom simulator of the arm, where during training the friction of the table is randomized and noise is added to the observations.

We observe a substantial sim2real gap between our simulator and the real robot, with policies trained in simulation failing to complete the pushing task zero shot in real, even when trained with domain randomization. We compare direct sim2real policy transfer against our method of transferring exploration policies. For direct policy transfer, we simply run SAC to finetune \(\pi^{\text{sim},\star}\) in the real world, using the current policy to collect data. For exploration policy transfer, we instead utilize \(\Pi_{\mathrm{exp}}\), our ensemble of exploration policies, to collect data in the real world. We run this in tandem with an SAC agent, feeding the data from the exploration policies into the SAC agent's replay buffer. Unlike in Section 5.3, rather than initializing the SAC policy from scratch, we set the initial policy as \(\pi^{\text{sim},\star}\), and fine-tune from this on the data collected from playing \(\Pi_{\mathrm{exp}}\). See Appendix E.5 for additional details.

Our results are shown on the right side of Figure 1. Statistics are computed over 6 runs for each method. Direct policy transfer with finetuning is unable to solve the task in real in each of the 6 runs, and converges to a suboptimal solution. However, our method is able to solve the task successfully each time and achieve a substantially higher reward.

## 6 Discussion

In this work, we have demonstrated that simulators can make naive exploration efficient even in settings where direct sim2real transfer fails, if they are used to train a set of exploration policies. We highlight several limitations of this work, which we believe are interesting future research questions:

* Our focus is purely on dynamics shift--where the dynamics of sim and real differ, but the environments are otherwise the same. While dynamics shift is common in many scenarios, other types of shift can exist as well, for example perceptual shift. How can we best handle these types of shift?
* How can we utilize a simulator in sim2real transfer if we can reset it arbitrarily, rather than just allowing for black-box access? Does the ability to reset allow us to improve sample efficiency further?
* Is the reachability condition, Assumption 3, necessary for successful exploration transfer?

Figure 4: Results on sim2sim Transfer in TychoEnv Simulator

## Acknowledgements

The work of AW and KJ was partially supported by the NSF through the University of Washington Materials Research Science and Engineering Center, DMR-2308979, and awards CCF 2007036 and CAREER 2141511. The work of LK was partially supported by Toyota Research Institute URP.

## References

* Agarwal et al. [2020] Alekh Agarwal, Sham Kakade, Akshay Krishnamurthy, and Wen Sun. Flambe: Structural complexity and representation learning of low rank mdps. _Advances in neural information processing systems_, 33:20095-20107, 2020.
* Agarwal et al. [2023] Alekh Agarwal, Yuda Song, Wen Sun, Kaiwen Wang, Mengdi Wang, and Xuezhou Zhang. Provable benefits of representational transfer in reinforcement learning. In _The Thirty Sixth Annual Conference on Learning Theory_, pages 2114-2187. PMLR, 2023.
* Agarwal et al. [2021] Naman Agarwal, Syomantak Chaudhuri, Prateek Jain, Dheeraj Nagaraj, and Praneeth Netrapalli. Online target q-learning with reverse experience replay: Efficiently finding the optimal policy for linear mdps. _arXiv preprint arXiv:2110.08440_, 2021.
* Akkaya et al. [2019] Ilge Akkaya, Marcin Andrychowicz, Maciek Chociej, Mateusz Litwin, Bob McGrew, Arthur Petron, Alex Paino, Matthias Plappert, Glenn Powell, Raphael Ribas, et al. Solving rubik's cube with a robot hand. _arXiv preprint arXiv:1910.07113_, 2019.
* Amortila et al. [2022] Philip Amortila, Nan Jiang, Dhruv Madeka, and Dean P Foster. A few expert queries suffices for sample-efficient rl with resets and linear value approximation. _Advances in Neural Information Processing Systems_, 35:29637-29648, 2022.
* Brafman and Tennenholtz [2002] Ronen I Brafman and Moshe Tennenholtz. R-max-a general polynomial time algorithm for near-optimal reinforcement learning. _Journal of Machine Learning Research_, 3(Oct):213-231, 2002.
* Burda et al. [2018] Yuri Burda, Harrison Edwards, Amos Storkey, and Oleg Klimov. Exploration by random network distillation. _arXiv preprint arXiv:1810.12894_, 2018.
* Chebotar et al. [2019] Yevgen Chebotar, Ankur Handa, Viktor Makoviychuk, Miles Macklin, Jan Issac, Nathan Ratliff, and Dieter Fox. Closing the sim-to-real loop: Adapting simulation randomization with real world experience. In _ICRA_, 2019.
* Chen et al. [2023] Tao Chen, Megha Tippur, Siyang Wu, Vikash Kumar, Edward Adelson, and Pulkit Agrawal. Visual dexterity: In-hand reorientation of novel and complex object shapes. _Science Robotics_, 8(84):eadc9244, 2023.
* Cheng et al. [2022] Yuan Cheng, Songtao Feng, Jing Yang, Hong Zhang, and Yingbin Liang. Provable benefit of multitask representation learning in reinforcement learning. _Advances in Neural Information Processing Systems_, 35:31741-31754, 2022.
* Dann et al. [2022] Chris Dann, Yishay Mansour, Mehryar Mohri, Ayush Sekhari, and Karthik Sridharan. Guarantees for epsilon-greedy reinforcement learning with function approximation. In _International conference on machine learning_, pages 4666-4689. PMLR, 2022.
* Degrave et al. [2022] Jonas Degrave, Federico Felici, Jonas Buchli, Michael Neunert, Brendan Tracey, Francesco Carpanese, Timo Ewalds, Roland Hafner, Abbas Abdolmaleki, Diego de Las Casas, et al. Magnetic control of tokamak plasmas through deep reinforcement learning. _Nature_, 602(7897):414-419, 2022.
* Eysenbach and Levine [2021] Benjamin Eysenbach and Sergey Levine. Maximum entropy rl (provably) solves some robust rl problems. _arXiv preprint arXiv:2103.06257_, 2021.
* Eysenbach et al. [2018] Benjamin Eysenbach, Abhishek Gupta, Julian Ibarz, and Sergey Levine. Diversity is all you need: Learning skills without a reward function. _arXiv preprint arXiv:1802.06070_, 2018.
* Ghugare et al. [2023] Raj Ghugare, Santiago Miret, Adriana Hugessen, Mariano Phielipp, and Glen Berseth. Searching for high-value molecules using reinforcement learning and transformers. _arXiv preprint arXiv:2310.02902_, 2023.
* Haarnoja et al. [2018] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. In _International conference on machine learning_, pages 1861-1870. PMLR, 2018.

* [17] Sami Haddadin, Sven Parusel, Lars Johannsmeier, Saskia Golz, Simon Gabl, Florian Walch, Mohamadreza Sabaghian, Christoph Jahne, Lukas Hausperger, and Simon Haddadin. The franka emika robot: A reference platform for robotics research and education. _IEEE Robotics & Automation Magazine_, 29(2):46-64, 2022. doi: 10.1109/MRA.2021.3138382.
* [18] Sebastian Hofer, Kostas Bekris, Ankur Handa, Juan Camilo Gamboa, Melissa Mozifian, Florian Golemo, Chris Atkeson, Dieter Fox, Ken Goldberg, John Leonard, et al. Sim2real in robotics and automation: Applications and challenges. _IEEE transactions on automation science and engineering_, 18(2):398-400, 2021.
* [19] Stephen James, Paul Wohlhart, Mrinal Kalakrishnan, Dmitry Kalashnikov, Alex Irpan, Julian Ibarz, Sergey Levine, Raia Hadsell, and Konstantinos Bousmalis. Sim-to-real via sim-to-sim: Data-efficient robotic grasping via randomized-to-canonical adaptation networks. arxiv e-prints, page. _arXiv preprint arXiv:1812.07252_, 2018.
* [20] Yiding Jiang, J Zico Kolter, and Roberta Raileanu. On the importance of exploration for generalization in reinforcement learning. _arXiv preprint arXiv:2306.05483_, 2023.
* [21] Chi Jin, Zhuoran Yang, Zhaoran Wang, and Michael I Jordan. Provably efficient reinforcement learning with linear function approximation. In _Conference on Learning Theory_, pages 2137-2143. PMLR, 2020.
* [22] Sham Kakade, Michael J Kearns, and John Langford. Exploration in metric state spaces. In _Proceedings of the 20th International Conference on Machine Learning (ICML-03)_, pages 306-312, 2003.
* [23] Elia Kaufmann, Leonard Bauersfeld, Antonio Loquercio, Matthias Muller, Vladlen Koltun, and Davide Scaramuzza. Champion-level drone racing using deep reinforcement learning. _Nature_, 620(7976):982-987, 2023.
* [24] Michael Kearns and Daphne Koller. Efficient reinforcement learning in factored mdps. In _IJCAI_, volume 16, pages 740-747, 1999.
* [25] Michael Kearns and Satinder Singh. Near-optimal reinforcement learning in polynomial time. _Machine learning_, 49:209-232, 2002.
* [26] B Ravi Kiran, Ibrahim Sobh, Victor Talpaert, Patrick Mannion, Ahmad A Al Sallab, Senthil Yogamani, and Patrick Perez. Deep reinforcement learning for autonomous driving: A survey. _IEEE Transactions on Intelligent Transportation Systems_, 23(6):4909-4926, 2021.
* [27] Jens Kober, J Andrew Bagnell, and Jan Peters. Reinforcement learning in robotics: A survey. _The International Journal of Robotics Research_, 32(11):1238-1274, 2013.
* [28] Ilya Kostrikov, Ashvin Nair, and Sergey Levine. Offline reinforcement learning with implicit q-learning. _arXiv preprint arXiv:2110.06169_, 2021.
* [29] Ashish Kumar, Zipeng Fu, Deepak Pathak, and Jitendra Malik. Rma: Rapid motor adaptation for legged robots. _arXiv preprint arXiv:2107.04034_, 2021.
* [30] Saurabh Kumar, Aviral Kumar, Sergey Levine, and Chelsea Finn. One solution is not all you need: Few-shot extrapolation via structured maxent rl. _Advances in Neural Information Processing Systems_, 33:8198-8210, 2020.
* [31] Kimin Lee, Michael Laskin, Aravind Srinivas, and Pieter Abbeel. Sunrise: A simple unified framework for ensemble learning in deep reinforcement learning. In _International Conference on Machine Learning_, pages 6131-6141. PMLR, 2021.
* [32] Lisa Lee, Benjamin Eysenbach, Emilio Parisotto, Eric Xing, Sergey Levine, and Ruslan Salakhutdinov. Efficient exploration via state marginal matching. _arXiv preprint arXiv:1906.05274_, 2019.
* [33] Gen Li, Yuxin Chen, Yuejie Chi, Yuantao Gu, and Yuting Wei. Sample-efficient reinforcement learning is feasible for linearly realizable mdps with limited revisiting. _Advances in Neural Information Processing Systems_, 34:16671-16685, 2021.
* [34] Yao Liu, Dipendra Misra, Miro Dudik, and Robert E Schapire. Provably sample-efficient rl with side information about latent dynamics. _Advances in Neural Information Processing Systems_, 35:33482-33493, 2022.
* [35] Rui Lu, Gao Huang, and Simon S Du. On the power of multitask representation learning in linear mdp. _arXiv preprint arXiv:2106.08053_, 2021.

* Malik et al. [2021] Dhruv Malik, Yuanzhi Li, and Pradeep Ravikumar. When is generalizable reinforcement learning tractable? _Advances in Neural Information Processing Systems_, 34:8032-8045, 2021.
* Mann and Choe [2013] Timothy A Mann and Yoonsuck Choe. Directed exploration in reinforcement learning with transferred knowledge. In _European Workshop on Reinforcement Learning_, pages 59-76. PMLR, 2013.
* Margolis et al. [2023] Gabriel B Margolis, Xiang Fu, Yandong Ji, and Pulkit Agrawal. Learning physically grounded robot vision with active sensing motor policies. In _CoRL_, 2023.
* Mehta et al. [2020] Bhairav Mehta, Manfred Diaz, Florian Golemo, Christopher J Pal, and Liam Paull. Active domain randomization. In _CoRL_, 2020.
* Memmel et al. [2024] Marius Memmel, Andrew Wagenmaker, Chuning Zhu, Patrick Yin, Dieter Fox, and Abhishek Gupta. Asid: Active exploration for system identification in robotic manipulation. _arXiv preprint arXiv:2404.12308_, 2024.
* Mhammedi et al. [2024] Zak Mhammedi, Adam Block, Dylan J Foster, and Alexander Rakhlin. Efficient model-free exploration in low-rank mdps. _Advances in Neural Information Processing Systems_, 36, 2024.
* Mhammedi et al. [2024] Zakaria Mhammedi, Dylan J Foster, and Alexander Rakhlin. The power of resets in online reinforcement learning. _arXiv preprint arXiv:2404.15417_, 2024.
* Modi et al. [2024] Aditya Modi, Jinglin Chen, Akshay Krishnamurthy, Nan Jiang, and Alekh Agarwal. Model-free representation learning and exploration in low-rank mdps. _Journal of Machine Learning Research_, 25(6):1-76, 2024.
* Muratore et al. [2019] Fabio Muratore, Michael Gienger, and Jan Peters. Assessing transferability from simulation to reality for reinforcement learning. _IEEE transactions on pattern analysis and machine intelligence_, 2019.
* Osband et al. [2016] Ian Osband, Charles Blundell, Alexander Pritzel, and Benjamin Van Roy. Deep exploration via bootstrapped dqn. _Advances in neural information processing systems_, 29, 2016.
* Ouyang et al. [2022] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. _Advances in neural information processing systems_, 35:27730-27744, 2022.
* Park et al. [2023] Seohong Park, Oleh Rybkin, and Sergey Levine. Metra: Scalable unsupervised rl with metric-aware abstraction. _arXiv preprint arXiv:2310.08887_, 2023.
* Pathak et al. [2017] Deepak Pathak, Pulkit Agrawal, Alexei A Efros, and Trevor Darrell. Curiosity-driven exploration by self-supervised prediction. In _International conference on machine learning_, pages 2778-2787. PMLR, 2017.
* Peng et al. [2018] Xue Bin Peng, Marcin Andrychowicz, Wojciech Zaremba, and Pieter Abbeel. Sim-to-real transfer of robotic control with dynamics randomization. In _2018 IEEE international conference on robotics and automation (ICRA)_, pages 3803-3810. IEEE, 2018.
* Peng et al. [2020] Xue Bin Peng, Erwin Coumans, Tingnan Zhang, Tsang-Wei Lee, Jie Tan, and Sergey Levine. Learning agile robotic locomotion skills by imitating animals. _arXiv preprint arXiv:2004.00784_, 2020.
* Raffin et al. [2021] Antonin Raffin, Ashley Hill, Adam Gleave, Anssi Kanervisto, Maximilian Ernestus, and Noah Dormann. Stable-baselines3: Reliable reinforcement learning implementations. _Journal of Machine Learning Research_, 22(268):1-8, 2021.
* Rajeswaran et al. [2017] Aravind Rajeswaran, Vikash Kumar, Abhishek Gupta, Giulia Vezzani, John Schulman, Emanuel Todorov, and Sergey Levine. Learning complex dexterous manipulation with deep reinforcement learning and demonstrations. _arXiv preprint arXiv:1709.10087_, 2017.
* Silva et al. [2023] FL Silva, Jiachen Yang, Mikel Landajuela, Andre Goncalves, Alexander Ladd, Daniel Faissol, and Brenden Petersen. Toward multi-fidelity reinforcement learning for symbolic optimization. Technical report, Lawrence Livermore National Laboratory (LLNL), Livermore, CA (United States), 2023.
* Silver et al. [2016] David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go with deep neural networks and tree search. _nature_, 529(7587):484-489, 2016.

* Sinha et al. [2022] Rohan Sinha, James Harrison, Spencer M Richards, and Marco Pavone. Adaptive robust model predictive control with matched and unmatched uncertainty. In _2022 American Control Conference (ACC)_, 2022.
* Song et al. [2020] Yuda Song, Aditi Mavalankar, Wen Sun, and Sicun Gao. Provably efficient model-based policy adaptation. _arXiv preprint arXiv:2006.08051_, 2020.
* Song et al. [2022] Yuda Song, Yifei Zhou, Ayush Sekhari, J Andrew Bagnell, Akshay Krishnamurthy, and Wen Sun. Hybrid rl: Using both offline and online data can make rl efficient. _arXiv preprint arXiv:2210.06718_, 2022.
* Sun et al. [2022] Yanchao Sun, Ruijie Zheng, Xiyao Wang, Andrew Cohen, and Furong Huang. Transfer rl across observation feature spaces via model-based regularization. _arXiv preprint arXiv:2201.00248_, 2022.
* Tirinzoni et al. [2021] Andrea Tirinzoni, Matteo Pirotta, and Alessandro Lazaric. A fully problem-dependent regret lower bound for finite-horizon mdps. _arXiv preprint arXiv:2106.13013_, 2021.
* Tobin et al. [2017] Josh Tobin, Rachel Fong, Alex Ray, Jonas Schneider, Wojciech Zaremba, and Pieter Abbeel. Domain randomization for transferring deep neural networks from simulation to the real world. In _2017 IEEE/RSJ international conference on intelligent robots and systems (IROS)_, pages 23-30. IEEE, 2017.
* Todorov et al. [2012] Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. In _2012 IEEE/RSJ International Conference on Intelligent Robots and Systems_, pages 5026-5033, 2012. doi: 10.1109/IROS.2012.6386109.
* Uehara et al. [2021] Masatoshi Uehara, Xuezhou Zhang, and Wen Sun. Representation learning for online and offline rl in low-rank mdps. _arXiv preprint arXiv:2110.04652_, 2021.
* Wagenmaker and Jamieson [2022] Andrew Wagenmaker and Kevin G Jamieson. Instance-dependent near-optimal policy identification in linear mdps via online experiment design. _Advances in Neural Information Processing Systems_, 35:5968-5981, 2022.
* Wagenmaker et al. [2022] Andrew Wagenmaker, Guanya Shi, and Kevin Jamieson. Optimal exploration for model-based rl in nonlinear systems. _arXiv preprint arXiv:2306.09210_, 2023.
* Wagenmaker et al. [2022] Andrew J Wagenmaker, Yifang Chen, Max Simchowitz, Simon Du, and Kevin Jamieson. Reward-free rl is no harder than reward-aware rl in linear markov decision processes. In _International Conference on Machine Learning_, pages 22430-22456. PMLR, 2022.
* Wang et al. [2016] Jane X Wang, Zeb Kurth-Nelson, Dhruva Tirumala, Hubert Soyer, Joel Z Leibo, Remi Munos, Charles Blundell, Dharshan Kumaran, and Matt Botvinick. Learning to reinforcement learn. _arXiv preprint arXiv:1611.05763_, 2016.
* Weisz et al. [2021] Gellert Weisz, Philip Amortila, Barnabas Janzer, Yasin Abbasi-Yadkori, Nan Jiang, and Csaba Szepesvari. On query-efficient planning in mdps under linear realizability of the optimal state-value function. In _Conference on Learning Theory_, pages 4355-4385. PMLR, 2021.
* Weisz et al. [2022] Gellert Weisz, Andras Gyorgy, Tadashi Kozuno, and Csaba Szepesvari. Confident approximate policy iteration for efficient local planning in \(q^{\pi}\)-realizable mdps. _Advances in Neural Information Processing Systems_, 35:25547-25559, 2022.
* Ye et al. [2023] Haotian Ye, Xiaoyu Chen, Liwei Wang, and Simon Shaolei Du. On the power of pre-training for generalization in rl: provable benefits and hardness. In _International Conference on Machine Learning_, pages 39770-39800. PMLR, 2023.
* Yin et al. [2022] Dong Yin, Botao Hao, Yasin Abbasi-Yadkori, Nevena Lazic, and Csaba Szepesvari. Efficient local planning with linear function approximation. In _International Conference on Algorithmic Learning Theory_, pages 1165-1192. PMLR, 2022.
* Zanette et al. [2020] Andrea Zanette, Alessandro Lazaric, Mykel J Kochenderfer, and Emma Brunskill. Provably efficient reward-agnostic navigation with linear value iteration. _Advances in Neural Information Processing Systems_, 33:11756-11766, 2020.
* Zhang et al. [2022] Tianjun Zhang, Tongzheng Ren, Mengjiao Yang, Joseph Gonzalez, Dale Schuurmans, and Bo Dai. Making linear mdps practical via contrastive representation learning. In _International Conference on Machine Learning_, pages 26447-26466. PMLR, 2022.
* Zhang et al. [2023] Yunchu Zhang, Liyiming Ke, Abhay Deshpande, Abhishek Gupta, and Siddhartha Srinivasa. Cherry-picking with reinforcement learning. _arXiv preprint arXiv:2303.05508_, 15, 2023.

* [74] Wenshuai Zhao, Jorge Pena Queralta, and Tomi Westerlund. Sim-to-real transfer in deep reinforcement learning for robotics: a survey. In _2020 IEEE symposium series on computational intelligence (SSCI)_, pages 737-744. IEEE, 2020.

## Appendix A Technical Results

We denote the state-visitations for some policy \(\pi\) as \(w_{h}^{\pi}(s,a):=\mathbb{P}^{\pi}[(s_{h},a_{h})=(s,a)]\), \(w_{h}^{\pi}(\mathcal{Z}):=\mathbb{P}^{\pi}[(s_{h},a_{h})\in\mathcal{Z}]\), for \(\mathcal{Z}\subseteq\mathcal{S}\times\mathcal{A}\). For \(\mathcal{X}\subseteq\mathbb{R}^{d}\), we denote \(w_{h}^{\pi}(\mathcal{X}):=\mathbb{P}^{\pi}[\boldsymbol{\phi}(s_{h},a_{h}) \in\mathcal{X}]\), for \(\boldsymbol{\phi}\) the featurization of the environment.

**Lemma A.1**.: _Consider MDPs \(M\) and \(\widetilde{M}\) with transition kernels \(P\) and \(\widetilde{P}\). Assume that both \(M\) and \(\widetilde{M}\) start in the same state \(s_{0}\) and that, for each \((s,a,h)\):_

\[\|P_{h}(\cdot\mid s,a)-\widetilde{P}_{h}(\cdot\mid s,a)\|_{\mathrm{TV}}\leq \epsilon_{\mathrm{sim}}.\] (A.1)

_Consider some reward function \(r\) such that \(\sum_{h=1}^{H}r_{h}(s_{h},a_{h})\leq R\) for all possible sequences \(\{(s_{h},a_{h})\}_{h=1}^{H}\). Then it follows that, for any \(\pi\) and \((s,a,h)\),_

\[|Q_{h}^{M,\pi}(s,a)-Q_{h}^{\widetilde{M},\pi}(s,a)|\leq HR\cdot\epsilon_{ \mathrm{sim}}.\]

Proof.: We prove this by induction. First, assume that for some \(h\) and all \(s,a\), we have \(|Q_{h+1}^{M,\pi}(s,a)-Q_{h+1}^{\widetilde{M},\pi}(s,a)|\leq\epsilon_{h+1}\). By definition we have

\[Q_{h}^{M,\pi}(s,a)=r_{h}(s,a)+\mathbb{E}^{M,\pi}[Q_{h+1}^{M,\pi}(s_{h+1},a_{h+ 1})\mid s_{h}=s,a_{h}=a]\]

and similarly for \(Q_{h+1}^{\widetilde{M},\pi}(s,a)\). Thus:

\[|Q_{h}^{M,\pi}(s,a)-Q_{h}^{\widetilde{M},\pi}(s,a)|\] \[\stackrel{{(a)}}{{\leq}}|\mathbb{E}^{M,\pi}[Q_{h+1}^ {M,\pi}(s_{h+1},a_{h+1})\mid s_{h}=s,a_{h}=a]-\mathbb{E}^{\widetilde{M},\pi}[ Q_{h+1}^{M,\pi}(s_{h+1},a_{h+1})\mid s_{h}=s,a_{h}=a]|\] \[\qquad\quad+\mathbb{E}^{\widetilde{M},\pi}[|Q_{h+1}^{M,\pi}(s_{h+ 1},a_{h+1})-Q_{h+1}^{\widetilde{M},\pi}(s_{h+1},a_{h+1})|\mid s_{h}=s,a_{h}=a]\] \[\stackrel{{(b)}}{{\leq}}|\mathbb{E}^{M,\pi}[Q_{h+1}^ {M,\pi}(s_{h+1},a_{h+1})\mid s_{h}=s,a_{h}=a]-\mathbb{E}^{\widetilde{M},\pi}[ Q_{h+1}^{M,\pi}(s_{h+1},a_{h+1})\mid s_{h}=s,a_{h}=a]|+\epsilon_{h+1}\]

where \((a)\) follows from the triangle inequality and \((b)\) follows from the inductive hypothesis. Under (A.1), we can bound

\[|\mathbb{E}^{M,\pi}[Q_{h+1}^{M,\pi}(s_{h+1},a_{h+1})\mid s_{h}=s,a_{h}=a]- \mathbb{E}^{\widetilde{M},\pi}[Q_{h+1}^{M,\pi}(s_{h+1},a_{h+1})\mid s_{h}=s,a_ {h}=a]|\leq\epsilon_{\mathrm{sim}}\!\cdot\!R.\]

It follows that for any \((s,a)\), \(|Q_{h}^{M,\pi}(s,a)-Q_{h}^{\widetilde{M},\pi}(s,a)|\leq\epsilon_{h}=: \epsilon_{\mathrm{sim}}R+\epsilon_{h+1}\).

The base case follows trivially with \(\epsilon_{H}=0\) since for any MDP we have that \(Q_{H}^{M,\pi}(s,a)=r_{H}(s,a)=Q_{H}^{\widetilde{M},\pi}(s,a)\). 

**Lemma A.2**.: _Under the same setting as Lemma A.1 and for any \(h\), \(\pi\), and \(\mathcal{Z}\subseteq\mathcal{S}\times\mathcal{A}\), we have_

\[|w_{h}^{M,\pi}(\mathcal{Z})-w_{h}^{\widetilde{M},\pi}(\mathcal{Z})|\leq H \epsilon_{\mathrm{sim}}.\]

Proof.: This is an immediate consequence of Lemma A.1 since, setting the reward \(r_{h^{\prime}}(s,a)=\mathbb{I}\{(s,a)\in\mathcal{Z},h^{\prime}=h\}\), we can set \(R=1\) and have \(V_{0}^{M,\pi}=w_{h}^{M,\pi}(\mathcal{Z})\). 

**Lemma A.3** (Proposition 2).: _Under Assumption 1, we have that_

\[V_{0}^{\mathsf{real},\star}-V_{0}^{\mathsf{real},\pi^{\mathsf{sim},\star}}\leq 2H^ {2}\epsilon_{\mathrm{sim}}\quad\text{and}\quad V_{0}^{\mathsf{sim},\star}-V_{0}^ {\mathsf{sim},\pi^{\mathsf{real},\star}}\leq 2H^{2}\epsilon_{\mathrm{sim}}.\]

Proof.: We prove the result for \(\mathsf{real}\)--the result for \(\mathsf{sim}\) follows analogously. We have

\[V_{0}^{\mathsf{real},\star}-V_{0}^{\mathsf{real},\pi^{\mathsf{ sim},\star}} =V_{0}^{\mathsf{real},\pi^{\mathsf{eval},\star}}-V_{0}^{\mathsf{sim},\pi^{\mathsf{eval },\star}}+\underbrace{V_{0}^{\mathsf{sim},\pi^{\mathsf{eval},\star}}-V_{0}^ {\mathsf{sim},\pi^{\mathsf{sim},\star}}}_{\leq 0}+V_{0}^{\mathsf{sim},\pi^{\mathsf{ sim},\star}}-V_{0}^{\mathsf{real},\pi^{\mathsf{in},\star}}-V_{0}^{\mathsf{real},\pi^{ \mathsf{in},\star}}\] \[\leq|V_{0}^{\mathsf{real},\pi^{\mathsf{eval},\star}}-V_{0}^{ \mathsf{sim},\pi^{\mathsf{eval},\star}}|+|V_{0}^{\mathsf{sim},\pi^{\mathsf{ sim},\star}}-V_{0}^{\mathsf{real},\pi^{\mathsf{sim},\star}}|.\]

The result then follows by applying Lemma A.1 to bound each of these terms by \(H^{2}\epsilon_{\mathrm{sim}}\).

**Lemma A.4**.: _For any \(f\in\mathcal{F}\),_

\[V_{0}^{\star}-{V_{0}^{\pi}}^{f}\leq\max_{\pi\in\{\pi^{f},\pi^{\star} \}}\sum_{h=0}^{H-1}2\left|\mathbb{E}^{\pi}\left[f_{h}(s_{h},a_{h})-\mathcal{T}f_ {h+1}(s_{h},a_{h})\right]\right|.\]

Proof.: We write

\[V_{0}^{\star}-{V_{0}^{\pi}}^{f}=\underbrace{V_{0}^{\star}-\max_{a}f_{0}(s_{0}, a)}_{(a)}+\underbrace{\max_{a}f_{0}(s_{0},a)-{V_{0}^{\pi}}^{f}}_{(b)}\]

and then bound each of these terms separately. By Lemma 5 of [57] we have

\[(a) \leq\sum_{h=0}^{H}\left|\mathbb{E}^{\pi^{\star}}\left[f_{h}(s_{h},a_{h})-r_{h}-\max_{a^{\prime}}f_{h+1}(s_{h+1},a^{\prime})\right]\right|\] \[=\sum_{h=0}^{H-1}\left|\mathbb{E}^{\pi^{\star}}\left[f_{h}(s_{h},a_{h})-\mathbb{E}[r_{h}+\max_{a^{\prime}}f_{h+1}(s_{h+1},a^{\prime})\mid s_ {h},a_{h}]\right]\right|.\]

Similarly, by Lemma 4 of [57] we have

\[(b) \leq\sum_{h=0}^{H-1}\left|\mathbb{E}^{\pi^{\prime}}\left[f_{h}(s_{ h},a_{h})-r_{h}-\max_{a^{\prime}}f_{h+1}(s_{h+1},a^{\prime})\right]\right|\] \[=\sum_{h=0}^{H-1}\left|\mathbb{E}^{\pi^{\prime}}\left[f_{h}(s_{h},a_{h})-\mathbb{E}[r_{h}+\max_{a^{\prime}}f_{h+1}(s_{h+1},a^{\prime})\mid s_ {h},a_{h}]\right]\right|.\]

## Appendix B Proof of Main Results

In Appendix B.1 we first provide a general result on learning in real when collecting data via a fixed set of exploration policies, given a particular coverage assumption. Then in Appendix B.2, we show that by playing a set of policies which induce full-rank covariates in sim, these policies provide sufficient coverage for learning in real. Finally in Appendices B.3 and B.4, we use these results to prove Theorems 1 and 3. Throughout the appendix we develop the supporting lemmas for our more general result, Theorem 3, which utilizes the simulator to restrict the version space (i.e. the dependence on \(|\mathcal{F}|\)) in addition to utilizing the simulator to aid in exploration.

Throughout this and the following section we assume that Assumption 4 holds. We also assume that \(f_{h}\in[0,V_{\max}]\) instead of \(f_{h}\in[0,H]\), for some \(V_{\max}>0\). For any \(f\in\mathcal{F}\), we denote the Bellman residual as

\[\mathcal{E}_{h}(f)(s,a):=\mathcal{T}f_{h+1}(s,a)-f_{h}(s,a).\]

Note that by assumption on \(\mathcal{F}\), we have \(\mathcal{E}_{h}(f)(s,a)\in[-V_{\max},V_{\max}]\).

For any policy \(\pi\), we denote \(\mathbf{\Lambda}_{\pi,h}^{\star}:=\mathbb{E}^{\text{sim},\pi}[\bm{\phi}^{ \text{s}}(s_{h},a_{h})\bm{\phi}^{\text{s}}(s_{h},a_{h})^{\top}]\) and \(\mathbf{\Lambda}_{\pi,h}^{\star}:=\mathbb{E}^{\text{real},\pi}[\bm{\phi}^{\text {r}}(s_{h},a_{h})\bm{\phi}^{\text{r}}(s_{h},a_{h})^{\top}]\).

Necessity of Random Exploration.Algorithm 1 achieves efficient exploration in \(\mathcal{M}^{\text{real}}\) by first learning a set of policies \(\Pi_{\text{exp}}^{h}\) in \(\mathcal{M}^{\text{sim}}\) that span the feature space of \(\mathcal{M}^{\text{sim}}\) (Line 2), achieving

\[\lambda_{\min}\Big{(}\frac{1}{\Pi_{\text{exp}}^{h}}\sum_{\pi\in \Pi_{\text{exp}}^{h}}\mathbb{E}^{\mathcal{M}^{\text{sim}},\pi}[\bm{\phi}^{ \text{s}}(s_{h},a_{h})\bm{\phi}^{\text{s}}(s_{h},a_{h})^{\top}]\Big{)}\gtrsim \lambda_{\min}^{\star},\] (B.1)

and then playing these policies in \(\mathcal{M}^{\text{real}}\), coupled with random exploration (Line 4). In particular, Algorithm 1 plays policies from \(\widetilde{\Pi}_{\text{exp}}^{h}\), where each \(\widetilde{\pi}_{\text{exp}}\in\widetilde{\Pi}_{\text{exp}}^{h}\) is defined as the policy which plays some \(\pi_{\text{exp}}\in\Pi_{\text{exp}}^{h}\) up to step \(h\), and then for steps \(h^{\prime}=h+1,\ldots,H\) chooses actions uniformly at random. This use of random exploration is critical to obtaining Theorem 1. Indeed, under ourtransfer model, condition (4.1) of Theorem 1 is not strong enough to ensure that policies satisfying (B.1) collect rich enough data in \(\mathcal{M}^{\mathsf{real}}\) to allow for learning a near-optimal policy. While (4.1) is sufficient to guarantee that playing \(\Pi^{h}_{\exp}\) on \(\mathcal{M}^{\mathsf{real}}\) collects data which spans the feature space of \(\mathcal{M}^{\mathsf{sim}}\)--that is, satisfying (B.1) but with the expectation over \(\mathcal{M}^{\mathsf{sim}}\) replaced by an expectation of \(\mathcal{M}^{\mathsf{real}}\)-- this is insufficient for learning, as the following result shows.

**Proposition 5**.: _For any \(\epsilon_{\mathrm{sim}}\leq 1/2\), there exist some \(\mathcal{M}^{\mathsf{sim}}\), \(\mathcal{M}^{\mathsf{real},1}\), and \(\mathcal{M}^{\mathsf{real},2}\) such that:_

1. _Both_ \(\mathcal{M}^{\mathsf{real},1}\) _and_ \(\mathcal{M}^{\mathsf{real},2}\) _satisfy Assumption_ 1 _with_ \(\mathcal{M}^{\mathsf{sim}}\) _and Assumptions_ 2 _to_ 4 _hold._
2. _There exists some policy_ \(\pi_{\exp}\) _such that_ \(\lambda_{\min}(\mathbb{E}^{\mathcal{M}^{\mathsf{real}},\pi_{\exp}}[\bm{\phi}^{ \mathsf{s}}(s_{h},a_{h})\bm{\phi}^{\mathsf{s}}(s_{h},a_{h})^{\top}])=1/2\)_,_ \(\forall h\in[H]\)_, and for any_ \(T\geq 0\)_, if we play_ \(\pi_{\exp}\) _on_ \(\mathcal{M}^{\mathsf{real}}\) _for_ \(T\) _steps, we have:_ \[\inf_{\widehat{\mathsf{s}}}\sup_{\mathcal{M}^{\mathsf{real}}\in\{\mathcal{M}^{ \mathsf{real},1},\mathcal{M}^{\mathsf{real},2}\}}\mathbb{E}^{\mathcal{M}^{ \mathsf{real}},\pi_{\exp}}[V_{0}^{\mathcal{M}^{\mathsf{real},\pi}}-V_{0}^{ \mathcal{M}^{\mathsf{real},\widehat{\pi}}}]\geq\epsilon_{\mathrm{sim}}.\]

Proposition 5 holds because two MDPs may be "close" in the sense of Assumption 1 but admit very different feature representations. As a result, transferring a policy that covers the feature space of \(\mathcal{M}^{\mathsf{sim}}\) is not necessarily sufficient for covering the feature space of \(\mathcal{M}^{\mathsf{real}}\), which ultimately means that data collected from \(\pi_{\exp}\) is unable to identify the optimal policy in \(\mathcal{M}^{\mathsf{real}}\). Our key technical result, Lemma B.4, shows, however, that under Assumption 1 and (4.1), policies which achieve high coverage in \(\mathcal{M}^{\mathsf{sim}}\) (i.e. satisfy (B.1)) are able to reach within a _logarithmic_ number of steps of relevant states in \(\mathcal{M}^{\mathsf{real}}\). While the sample complexity of random exploration typically scales exponentially in the horizon, if the horizon over which we must explore is only logarithmic, the total complexity is then only polynomial. Theorem 1 critically relies on these facts--by playing policies in \(\Pi^{h}_{\exp}\) up to step \(h\) and then exploring randomly, and repeating this for each \(h\in[H]\), we show that sufficiently rich data is collected in \(\mathcal{M}^{\mathsf{real}}\) for learning an \(\epsilon\)-optimal policy.

### Learning in \(\mathsf{real}\) with Fixed Exploration Policies

```
1:input: exploration policies \(\{\pi^{h}_{\exp}\}_{h=1}^{H}\), budget \(T\), \(\mathsf{sim}\) date \(\mathfrak{D}_{\mathsf{sim}}\), \(\mathsf{sim}\) regularization \(\gamma\)
2: Play \(\pi_{\exp}=\min(\{\pi^{h}_{\exp}\}_{h=1}^{H})\) for \(T\) episodes in \(\mathsf{real}\), add data to \(\mathfrak{D}\)
3:for\(h=H,H-1,\dots,1\)do
4: \[\begin{split}\widetilde{f}_{h}&\leftarrow\operatorname {arg\,min}_{f\in\mathcal{F}}\sum_{(s,a,r,s^{\prime})\in\mathfrak{D}^{h}_{ \mathsf{sim}}}(f_{h}(s,a)-r-\max_{a^{\prime}}\widehat{f}_{h+1}(s^{\prime},a^ {\prime}))^{2}\\ \widehat{f}_{h}&\leftarrow\operatorname{arg\,min}_{f \in\mathcal{F}}\sum_{(s,a,r,s^{\prime})\in\mathfrak{D}_{h}}(f_{h}(s,a)-r- \max_{a^{\prime}}\widehat{f}_{h+1}(s^{\prime},a^{\prime}))^{2}\\ &\text{s.t.}\quad\frac{1}{|\mathfrak{D}_{\mathsf{sim}}|}\sum_{(s,a )\in\mathfrak{D}^{h}_{\mathsf{sim}}}(f_{h}(s,a)-\widetilde{f}_{h}(s,a))^{2} \leq\gamma\end{split}\] (B.2)
5:return\(\pi^{\widehat{f}}\) ```

**Algorithm 3**\(\mathsf{sim2real}\) transfer with fixed exploration policies (ExploreReal)

**Lemma B.1**.: _Consider running Algorithm 3. Assume that \(\mathfrak{D}_{\mathsf{sim}}\) was generated as in Assumption 5, via the procedure of Lemma C.3 run with some parameter \(\beta\), and \(\gamma\) satisfies_

\[2V_{\max}^{2}\epsilon_{\mathrm{sim}}^{2}+\frac{43V_{\max}^{2} \beta^{2}}{dH}\cdot\log\frac{8H|\mathcal{F}_{h}|}{\delta}+6V_{\max}^{2}\beta \sqrt{\frac{\log\frac{8H|\mathcal{F}_{h}|}{\delta}}{dH}}\leq\gamma.\]

_Furthermore, assume that there exists some \(\mathfrak{C},\epsilon>0\) such that, for any \(\pi\), \(h\in[H]\), and \(\mathcal{Z}^{\prime}\subseteq\mathcal{S}\times\mathcal{A}\), we have:_

\[w_{h}^{\mathsf{real},\pi}(\mathcal{Z}^{\prime})\leq\mathfrak{C} \cdot w_{h}^{\mathsf{real},\pi_{\exp}}(\mathcal{Z}^{\prime})+\epsilon.\] (B.3)_Then with probability at least \(1-2\delta\), the policy \(\pi^{\widehat{f}}\) generated by Algorithm 3 satisfies_

\[V_{0}^{\star}-{V_{0}^{\pi}}^{\widehat{f}}\leq 4\mathfrak{C}H\sqrt{\frac{256V_{ \max}^{2}\log(4H|\widetilde{\mathcal{F}}(\pi_{\exp}^{\mathsf{sim}})|/\delta)}{ T}}+4HV_{\max}\epsilon\]

_for_

\[\widetilde{\mathcal{F}}(\pi_{\exp}^{\mathsf{sim}}):=\{f\in\mathcal{F}\;:\; \mathbb{E}^{\mathsf{sim},\pi_{\exp}^{\mathsf{sim}}}[(f_{h}(s_{h},a_{h})- \mathcal{T}^{\mathsf{sim}}f_{h+1}(s_{h},a_{h}))^{2}]\leq 2\gamma,\forall h \in[H]\}.\]

Proof.: Let \(\mathcal{E}\) denote the good event of Lemma B.2, which holds with probability at least \(1-2\delta\). By Lemma A.4 we have

\[V_{0}^{\mathsf{real},\star}-V_{0}^{\mathsf{real},\pi^{\widehat{f}}} \leq\max_{\pi\in\{\pi^{\widehat{f}},\pi^{\mathsf{real},\star}\}} \sum_{h=0}^{H-1}2\left|\mathbb{E}^{\mathsf{real},\pi}[\widehat{f}_{h}(s_{h},a_ {h})-\mathcal{T}^{\mathsf{real}}\widehat{f}_{h+1}(s_{h},a_{h})]\right|\] \[\leq\max_{\pi}\sum_{h=0}^{H-1}2\mathbb{E}^{\mathsf{real},\pi}[| \mathcal{E}_{h}^{\mathsf{real}}(\widehat{f})(s_{h},a_{h})|].\]

Let

\[\mathcal{Z}_{h,i}:=\{(s,a)\;:\;|\mathcal{E}_{h}^{\mathsf{real}}(\widehat{f})(s,a)|\in[V_{\max}\cdot 2^{-i},V_{\max}\cdot 2^{-i+1})\}.\]

Then we have, for any \(\pi\),

\[\mathbb{E}^{\mathsf{real},\pi}[|\mathcal{E}_{h}^{\mathsf{real}}( \widehat{f})(s_{h},a_{h})|] \leq\sum_{i=1}^{\infty}w_{h}^{\mathsf{real},\pi}(\mathcal{Z}_{h,i })\cdot V_{\max}2^{-i+1}\] \[\leq\mathfrak{C}\cdot\sum_{i=1}^{\infty}w_{h}^{\mathsf{real},\pi _{\exp}}(\mathcal{Z}_{h,i})\cdot V_{\max}2^{-i+1}+2V_{\max}\epsilon\] \[\leq 2\mathfrak{C}\cdot\mathbb{E}^{\mathsf{real},\pi_{\exp}}[| \mathcal{E}_{h}^{\mathsf{real}}(\widehat{f})(s_{h},a_{h})|]+2V_{\max}\epsilon\]

where the second inequality follows from (B.3). On \(\mathcal{E}\), by Lemma B.2 and Jensen's inequality, we have

\[\mathbb{E}^{\mathsf{real},\pi_{\exp}}[|\mathcal{E}_{h}^{\mathsf{real}}( \widehat{f})(s_{h},a_{h})|] \leq\sqrt{\mathbb{E}^{\mathsf{real},\pi_{\exp}}[\mathcal{E}_{h}^{ \mathsf{real}}(\widehat{f})(s_{h},a_{h})^{2}]}\leq\sqrt{\frac{1}{T}\cdot 256V_{ \max}^{2}\log\frac{2H|\widetilde{\mathcal{F}}_{h}(\pi_{\exp}^{\mathsf{sim}})|} {\delta}}.\]

As this holds for each \(h\) and \(\pi\), we have therefore shown that

\[V_{0}^{\mathsf{real},\star}-{V_{0}^{\mathsf{real},\pi^{\widehat{f}}}} \leq 4\mathfrak{C}\cdot\sum_{h=0}^{H-1}\sqrt{\frac{1}{T}\cdot 256V_{ \max}^{2}\log\frac{2H|\widetilde{\mathcal{F}}_{h}(\pi_{\exp}^{\mathsf{sim}})|} {\delta}}+4HV_{\max}\epsilon\] \[\leq 4\mathfrak{C}H\sqrt{\frac{1}{T}\cdot 256V_{\max}^{2}\log \frac{2H|\widetilde{\mathcal{F}}(\pi_{\exp}^{\mathsf{sim}})|}{\delta}}+4HV_{ \max}\epsilon.\]

This proves the result. 

**Lemma B.2**.: _With probability at least \(1-2\delta\), for each \(h\in[H]\) simultaneously, as long as the conditions on \(\gamma\) given in Lemma B.3 hold, we have_

\[\mathbb{E}^{\mathsf{real},\pi_{\exp}}[(\widehat{f}_{h}(s_{h},a_{h})-\mathcal{T }^{\mathsf{real}}\widehat{f}_{h+1}(s_{h},a_{h}))^{2}]\leq\frac{1}{T}\cdot 256V_{ \max}^{2}\log(2H|\widetilde{\mathcal{F}}_{h}(\pi_{\exp}^{\mathsf{sim}})|/\delta),\]

_and \(\widehat{f}_{h}\in\widetilde{\mathcal{F}}_{h}(\pi_{\exp}^{\mathsf{sim}})\) for all \(h\in[H]\), where_

\[\widetilde{\mathcal{F}}_{h}(\pi_{\exp}^{\mathsf{sim}}):=\{f_{h}\in\mathcal{F}_ {h}\;:\;\exists f_{h+1}\in\mathcal{F}_{h+1}\text{ s.t. }\mathbb{E}^{\mathsf{sim},\pi_{\exp}^{\mathsf{sim}}}[(f_{h}(s_{h},a_{h})- \mathcal{T}^{\mathsf{sim}}f_{h+1}(s_{h},a_{h}))^{2}]\leq 2\gamma\}.\]

Proof.: Let \(\widehat{\mathcal{F}}_{h}\) denote the feasible set of (B.2) at step \(h\). By Lemma B.3, with probability at least \(1-\delta\), \(\widehat{\mathcal{F}}_{h}^{\mathsf{t}}\subseteq\widetilde{\mathcal{F}}_{h}\), and, furthermore, that \(\mathcal{T}^{\mathsf{real}}\widehat{f}_{h+1}\) is feasible. The result then follows from Lemma 3 of [57], since the constraint on the regression problem restricts the version space.

**Lemma B.3**.: _Assume that data in \(\mathfrak{D}_{\mathsf{sim}}\) is generated as in Assumption 5 via the procedure of Lemma C.3 run with some parameter \(\beta\), and \(\gamma\) satisfies_

\[2V_{\max}^{2}\epsilon_{\mathrm{sim}}^{2}+\frac{43V_{\max}^{2}\beta^{2}}{dH} \cdot\log\frac{8H|\mathcal{F}_{h}|}{\delta}+6V_{\max}^{2}\beta\sqrt{\frac{\log \frac{8H|\mathcal{F}_{h}|}{\delta}}{dH}}\leq\gamma.\]

_Then with probability at least \(1-\delta\) we have, for each \(h\in[H]\):_

1. \(\mathcal{T}^{\mathsf{real}}\widehat{f}_{h+1}\) _is feasible for_ (B.2)_._
2. _The set of feasible_ \(f\) _for (_B.2_) is a subset of_ \[\{f\in\mathcal{F}\ :\ \mathbb{E}^{\mathsf{sim},\pi_{\mathrm{exp}}^{\mathsf{ sim}}}[(f_{h}(s_{h},a_{h})-\mathcal{T}^{\mathsf{sim}}\widehat{f}_{h+1}(s_{h},a_{h})) ^{2}]\leq 2\gamma\}.\]

Proof.: By Lemma C.1, we have that with probability at least \(1-\delta/2H\),

\[\frac{1}{T_{\mathsf{sim}}}\sum_{t=1}^{T_{\mathsf{sim}}}(\mathcal{T}^{\mathsf{ real}}\widehat{f}_{h+1}(\widetilde{s}_{h}^{t},\widetilde{a}_{h}^{t})- \widetilde{f}_{h}(\widetilde{s}_{h}^{t},\widetilde{a}_{h}^{t}))^{2}\leq 2V_{ \max}^{2}\epsilon_{\mathrm{sim}}^{2}+\frac{512V_{\max}^{2}}{T_{\mathsf{sim}}} \cdot\log\frac{8H|\mathcal{F}_{h}|}{\delta}+V_{\max}^{2}\sqrt{\frac{2\log \frac{4H|\mathcal{F}_{h}|}{\delta}}{T_{\mathsf{sim}}}}.\]

By Lemma C.3, we have \(\frac{12dH}{\beta^{2}}\leq T_{\mathsf{sim}}\), which implies

\[\frac{1}{T_{\mathsf{sim}}}\sum_{t=1}^{T_{\mathsf{sim}}}(\mathcal{T}^{\mathsf{ real}}\widehat{f}_{h+1}(\widetilde{s}_{h}^{t},\widetilde{a}_{h}^{t})- \widetilde{f}_{h}(\widetilde{s}_{h}^{t},\widetilde{a}_{h}^{t}))^{2}\leq 2V_{ \max}^{2}\epsilon_{\mathrm{sim}}^{2}+\frac{43V_{\max}^{2}\beta^{2}}{dH}\cdot \log\frac{8H|\mathcal{F}_{h}|}{\delta}+V_{\max}^{2}\beta\sqrt{\frac{\log\frac{ 4H|\mathcal{F}_{h}|}{\delta}}{6dH}}.\]

Part 1 then follows given our assumption on \(\gamma\).

To bound the feasible set for (B.2) we appeal to Lemma C.2 which states that with probability at least \(1-\delta/2H\) we have that the feasible set of (B.2) is a subset of

\[\left\{f_{h}\in\mathcal{F}_{h}\ :\ \mathbb{E}^{\mathsf{sim},\pi_{\mathrm{exp}}^{ \mathsf{sim}}}[(f_{h}(s_{h},a_{h})-\mathcal{T}^{\mathsf{sim}}\widehat{f}_{h+ 1}(s_{h},a_{h}))^{2}]\leq\gamma+18V_{\max}^{2}\sqrt{\frac{\log\frac{8H| \mathcal{F}_{h}|}{\delta}}{T_{\mathsf{sim}}}}\right\}.\]

Again using that \(\frac{12dH}{\beta^{2}}\leq T_{\mathsf{sim}}\), we have have that this is a subset of

\[\left\{f_{h}\in\mathcal{F}_{h}\ :\ \mathbb{E}^{\mathsf{sim},\pi_{ \mathrm{exp}}^{\mathsf{sim}}}[(f_{h}(s_{h},a_{h})-\mathcal{T}^{\mathsf{sim}} \widehat{f}_{h+1}(s_{h},a_{h}))^{2}]\leq\gamma+18V_{\max}^{2}\beta\sqrt{\frac {\log\frac{8H|\mathcal{F}_{h}|}{\delta}}{12dH}}\right\}\] \[\subseteq\left\{f_{h}\in\mathcal{F}_{h}\ :\ \mathbb{E}^{\mathsf{sim},\pi_{ \mathrm{exp}}^{\mathsf{sim}}}[(f_{h}(s_{h},a_{h})-\mathcal{T}^{\mathsf{sim}} \widehat{f}_{h+1}(s_{h},a_{h}))^{2}]\leq 2\gamma\right\}\]

where the inclusion follows from our assumption on \(\gamma\). The result then follows from a union bound. 

### Performance of Full-Rank \(\mathsf{sim}\) Policies in \(\mathsf{real}\)

**Lemma B.4**.: _Consider policies \(\{\pi_{\mathrm{exp}}^{h}\}_{h=1}^{H}\), and assume that_

\[\lambda_{\min}\left(\mathbf{A}_{\pi_{\mathrm{exp}}^{h},h}^{*}\right)\geq\bar{ \lambda}_{\min},\quad\forall h\in[H]\] (B.4)

_and that \(\pi_{\mathrm{exp}}^{h}\) plays actions uniformly at random for \(h^{\prime}>h\). Let \(\pi_{\mathrm{exp}}=\mathrm{unif}(\{\pi_{\mathrm{exp}}^{h}\}_{h=1}^{H})\). Then, for any \(\pi\), \(\kappa>0\), \(\gamma>0\), \(h\in[H]\), and \(\mathcal{Z}^{\prime}\subseteq\mathcal{S}\times\mathcal{A}\), we have_

\[w_{h}^{\mathsf{real},\pi}(\mathcal{Z}^{\prime})\leq\frac{4H\gamma A^{k^{\star}- 2}}{\kappa}\cdot w_{h}^{\mathsf{real},\pi_{\mathrm{exp}}}(\mathcal{Z}^{\prime}) +4\kappa,\]

_where_

\[\xi:=2\sqrt{\frac{A}{\lambda_{\min}}\left(\frac{d}{\gamma}+H\epsilon_{\mathrm{ sim}}\right)}\quad\text{and}\quad k^{\star}:=\lceil\frac{\log 1/\kappa}{\log 1/\xi}\rceil.\]Proof.: Denote

\[\widetilde{\mathcal{Z}}_{h+1}:=\{(s,a)\;:\;\bm{\phi}^{\prime}(s,a)^{\top}(\bm{ \Lambda}^{\prime}_{\pi^{h}_{\mathrm{exp}},h+1})^{-1}\bm{\phi}^{\prime}(s,a)>\gamma\}\]

for some \(\gamma>0\). We have

\[w^{\mathsf{real},\pi^{h}_{\mathrm{exp}}}_{h+1}(\widetilde{ \mathcal{Z}}_{h+1}) =\mathbb{E}^{\mathsf{real},\pi^{h}_{\mathrm{exp}}}[\mathbb{I}\{(s _{h+1},a_{h+1})\in\widetilde{\mathcal{Z}}_{h+1}\}]\] \[\overset{(a)}{\leq}\mathbb{E}^{\mathsf{real},\pi^{h}_{\mathrm{ exp}}}\left[\frac{\bm{\phi}^{\prime}(s_{h+1},a_{h+1})^{\top}(\bm{\Lambda}^{ \prime}_{\pi^{h}_{\mathrm{exp}},h+1})^{-1}\bm{\phi}^{\prime}(s_{h+1},a_{h+1}) }{\gamma}\cdot\mathbb{I}\{(s_{h+1},a_{h+1})\in\widetilde{\mathcal{Z}}_{h+1} \}\right]\] \[\leq\mathbb{E}^{\mathsf{real},\pi^{h}_{\mathrm{exp}}}\left[\frac{ \bm{\phi}^{\prime}(s_{h+1},a_{h+1})^{\top}(\bm{\Lambda}^{\prime}_{\pi^{h}_{ \mathrm{exp}},h+1})^{-1}\bm{\phi}^{\prime}(s_{h+1},a_{h+1})}{\gamma}\right]\] \[=\frac{1}{\gamma}\cdot\mathrm{tr}\left(\mathbb{E}^{\mathsf{real}, \pi^{h}_{\mathrm{exp}}}[\bm{\phi}^{\prime}(s_{h+1},a_{h+1})\bm{\phi}^{\prime}( s_{h+1},a_{h+1})^{\top}](\bm{\Lambda}^{\prime}_{\pi^{h}_{\mathrm{exp}},h+1})^{-1}\right)\] \[=\frac{d}{\gamma}\]

where \((a)\) follows since for all \((s,a)\in\widetilde{\mathcal{Z}}_{h+1}\), we have \(1<\bm{\phi}^{\prime}(s,a)^{\top}(\bm{\Lambda}^{\prime}_{\pi^{h}_{\mathrm{exp} },h+1})^{-1}\bm{\phi}^{\prime}(s,a)/\gamma\). By Lemma A.2, we then have that

\[w^{\mathsf{sim},\pi^{h}_{\mathrm{exp}}}_{h+1}(\widetilde{\mathcal{Z}}_{h+1}) \leq\frac{d}{\gamma}+H\epsilon_{\mathrm{sim}}.\] (B.5)

Let \(\widetilde{\mathcal{S}}_{h+1}:=\{s\;:\;\exists a\text{ s.t. }(s,a)\in\widetilde{\mathcal{Z}}_{h+1}\}\) and note that

\[w^{\mathsf{sim},\pi^{h}_{\mathrm{exp}}}_{h+1}(\widetilde{\mathcal{ Z}}_{h+1}) =\mathbb{E}^{\mathsf{sim},\pi^{h}_{\mathrm{exp}}}\left[\int_{ \widetilde{\mathcal{S}}_{h+1}}\sum_{a:(s,a)\in\widetilde{\mathcal{Z}}_{h+1}} \pi^{h}_{\mathrm{exp}}(a\mid s,h+1)\mathrm{d}P^{\mathsf{sim}}_{h}(s\mid s_{h},a_{h})\right]\] \[\geq\frac{1}{A}\mathbb{E}^{\mathsf{sim},\pi^{h}_{\mathrm{exp}}} \left[\int_{\widetilde{\mathcal{S}}_{h+1}}\mathrm{d}\bm{\mu}^{\mathsf{s}}_{h} (s)^{\top}\bm{\phi}^{\mathsf{s}}(s_{h},a_{h})\right]\] \[=\frac{1}{A}\mathbb{E}^{\mathsf{sim},\pi^{h}_{\mathrm{exp}}}[P^{ \mathsf{sim}}_{h}(\widetilde{\mathcal{S}}_{h+1}\mid s_{h},a_{h})]\] \[\geq\frac{1}{A}\mathbb{E}^{\mathsf{sim},\pi^{h}_{\mathrm{exp}}}[P^ {\mathsf{sim}}_{h}(\widetilde{\mathcal{S}}_{h+1}\mid s_{h},a_{h})^{2}]\]

where we have used the fact that \(\pi^{h}_{\mathrm{exp}}(a\mid s,h+1)=1/A\) for all \((s,a)\) by assumption, and define \(P^{\mathsf{sim}}_{h}(\widetilde{\mathcal{S}}_{h+1}\mid s,a):=\mathbb{P}^{ \mathsf{sim}}[s_{h+1}\in\widetilde{\mathcal{S}}_{h+1}\mid s_{h}=s,a_{h}=a]= \int_{\widetilde{\mathcal{S}}_{h+1}}\mathrm{d}\bm{\mu}^{\mathsf{s}}_{h}(s)^{ \top}\bm{\phi}^{\mathsf{s}}(s,a)\), where the last equality follows from the definition of a linear MDP. Letting \(\bm{\mu}^{\mathsf{s}}_{h}(\widetilde{\mathcal{S}}_{h+1}):=\int_{\widetilde{ \mathcal{S}}_{h+1}}\mathrm{d}\bm{\mu}^{\mathsf{s}}_{h}(s)\), note that:

\[\frac{1}{A}\mathbb{E}^{\mathsf{sim},\pi^{h}_{\mathrm{exp}}}[P^{ \mathsf{sim}}_{h}(\widetilde{\mathcal{S}}_{h+1}\mid s_{h},a_{h})^{2}] =\frac{1}{A}\bm{\mu}^{\mathsf{s}}_{h}(\widetilde{\mathcal{S}}_{h+1 })^{\top}\mathbb{E}^{\mathsf{sim},\pi^{h}_{\mathrm{exp}}}[\bm{\phi}^{\mathsf{s} }(s_{h},a_{h})\bm{\phi}^{\mathsf{s}}(s_{h},a_{h})^{\top}]\bm{\mu}^{\mathsf{s}}_{h }(\widetilde{\mathcal{S}}_{h+1})\] \[=\frac{1}{A}\bm{\mu}^{\mathsf{s}}_{h}(\widetilde{\mathcal{S}}_{h+1 })^{\top}\bm{\Lambda}^{\mathsf{s}}_{\pi^{h}_{\mathrm{exp}},h}\bm{\mu}^{\mathsf{s }}_{h}(\widetilde{\mathcal{S}}_{h+1})\] \[\geq\frac{\lambda_{\min}}{A}\|\bm{\mu}^{\mathsf{s}}_{h}(\widetilde{ \mathcal{S}}_{h+1})\|_{2}^{2},\]

where the last inequality follows from (B.4). Combining this with (B.5), we have

\[\frac{d}{\gamma}+H\epsilon_{\mathrm{sim}}\geq\frac{\bar{\lambda}_{\min}}{A}\|\bm {\mu}^{\mathsf{s}}_{h}(\widetilde{\mathcal{S}}_{h+1})\|_{2}^{2}.\]

Now note that, for any \(z\in\mathcal{S}\times\mathcal{A}\):

\[P^{\mathsf{sim}}_{h}(\widetilde{\mathcal{S}}_{h+1}\mid z)=\int_{\widetilde{ \mathcal{S}}_{h+1}}\mathrm{d}P^{\mathsf{sim}}_{h}(s\mid z)=\left(\int_{ \widetilde{\mathcal{S}}_{h+1}}\mathrm{d}\bm{\mu}^{\mathsf{s}}_{h}(s)\right)^{ \top}\bm{\phi}^{\mathsf{s}}(z)\leq\|\bm{\mu}^{\mathsf{s}}_{h}(\widetilde{ \mathcal{S}}_{h+1})\|_{2}\]and we also have that \(P_{h}^{\text{sim}}(\widetilde{\mathcal{S}}_{h+1}\mid z)\geq P_{h}^{\text{real}}( \widetilde{\mathcal{S}}_{h+1}\mid z)-\epsilon_{\text{sim}}\) under Assumption 1. Putting this together we have that for all \(z\in\mathcal{S}\times\mathcal{A}\):

\[P_{h}^{\text{real}}(\widetilde{\mathcal{S}}_{h+1}\mid z)\leq\sqrt{\frac{A}{ \bar{\lambda}_{\min}}\left(\frac{d}{\gamma}+H\epsilon_{\text{sim}}\right)}+ \epsilon_{\text{sim}}.\]

Note that we can always take \(\epsilon_{\text{sim}}\leq 1\), and will always have \(\bar{\lambda}_{\min}\leq 1\). This implies that \(\epsilon_{\text{sim}}\leq\sqrt{\frac{A}{\bar{\lambda}_{\min}}\left(\frac{d}{ \gamma}+H\epsilon_{\text{sim}}\right)}\). Thus,

\[P_{h}^{\text{real}}(\widetilde{\mathcal{S}}_{h+1}\mid z)\leq 2\sqrt{\frac{A}{ \bar{\lambda}_{\min}}\left(\frac{d}{\gamma}+H\epsilon_{\text{sim}}\right)}=:\xi.\]

Coverage of \(\pi_{\text{exp}}\) in real.Let \(k^{\star}:=\lceil\frac{\log 1/\kappa}{\log 1/\xi}\rceil\), so that \(\xi^{k^{\star}}\leq\kappa\). Let \(\bar{\mathcal{Z}}_{h}:=(\mathcal{S}\times\mathcal{A})\backslash\widetilde{ \mathcal{Z}}_{h}\). Fix some \(\mathcal{Z}^{\prime}\subseteq(\mathcal{S}\times\mathcal{A})\), \(h\in[H]\), and policy \(\pi\).

Consider some \(z\in\bar{\mathcal{Z}}_{h}\), and some \(\mathcal{S}^{\prime}\subseteq\mathcal{S}\). Then note that5

Footnote 5: If \(\boldsymbol{\Lambda}_{\pi_{\text{exp}}^{h-1},h}^{\prime}\) is not invertible, we can repeat this argument with \(\boldsymbol{\Lambda}_{\pi_{\text{exp}}^{h-1},h}^{\prime}+\lambda I\) and take \(\lambda\to 0\).

\[P_{h}^{\text{real}}(\mathcal{S}^{\prime}\mid z)=\boldsymbol{ \mu}_{h}^{\prime}(\mathcal{S}^{\prime})^{\top}\boldsymbol{\phi}^{\prime}(z) =\boldsymbol{\mu}_{h}^{\prime}(\mathcal{S}^{\prime})^{\top}( \boldsymbol{\Lambda}_{\pi_{\text{exp}}^{h-1},h}^{\prime})^{1/2}( \boldsymbol{\Lambda}_{\pi_{\text{exp}}^{h-1},h}^{\prime})^{-1/2}\boldsymbol{ \phi}^{\prime}(z)\] \[\leq\|\boldsymbol{\mu}_{h}^{\prime}(\mathcal{S}^{\prime})\|_{ \boldsymbol{\Lambda}_{\pi_{\text{exp}}^{h-1},h}^{\prime}}\|\boldsymbol{\phi}^ {\prime}(z)\|(\boldsymbol{\Lambda}_{\pi_{\text{exp}}^{h-1},h}^{\prime})^{-1}\] \[\leq\sqrt{\gamma}\|\boldsymbol{\mu}_{h}^{\prime}(\mathcal{S}^{ \prime})\|_{\boldsymbol{\Lambda}_{\pi_{\text{exp}}^{h-1},h}^{\prime}}\]

where the last inequality follows from the definition of \(\bar{\mathcal{Z}}_{h}\). Note, though, that

\[\|\boldsymbol{\mu}_{h}^{\prime}(\mathcal{S}^{\prime})\|_{ \boldsymbol{\Lambda}_{\pi_{\text{exp}}^{h-1},h}^{\prime}}^{2}=\mathbb{E}^{ \text{real},\pi_{\text{exp}}^{h-1}}[(\boldsymbol{\mu}_{h}^{\prime}(\mathcal{S }^{\prime})^{\top}\boldsymbol{\phi}^{\prime}(z_{h}))^{2}]=\mathbb{E}^{\text{ real},\pi_{\text{exp}}^{h-1}}[P_{h}^{\text{real}}(\mathcal{S}^{\prime}\mid z_{h})^{2}].\]

This implies that for all \(z\in\bar{\mathcal{Z}}_{h}\),

\[\mathbb{E}^{\text{real},\pi_{\text{exp}}^{h-1}}[P_{h}^{\text{real}}(\mathcal{ S}^{\prime}\mid z_{h})^{2}]\geq\frac{1}{\gamma}\cdot P_{h}^{\text{real}}( \mathcal{S}^{\prime}\mid z)^{2}.\]

For \(h^{\prime}<h\), define

\[\mathcal{S}_{h^{\prime},i}:=\{s\;:\;w_{h}^{\text{real},\pi}(\mathcal{Z}^{ \prime}\mid s_{h^{\prime}}=s)\in[2^{-i+1},2^{-i})\}\]

for \(w_{h}^{\text{real},\pi}(\mathcal{Z}\mid s_{h^{\prime}}=s):=\mathbb{P}^{\text{ real},\pi}[z_{h}\in\mathcal{Z}\mid s_{h^{\prime}}=s]\). Note that we then have \(w_{h}^{\text{real},\pi}(\mathcal{Z}^{\prime}\mid\mathcal{S}_{h^{\prime},i})\in[ 2^{-i+1},2^{-i})\). By what we have just shown, we have that for \(z\in\bar{\mathcal{Z}}_{h^{\prime}}\)

\[\mathbb{E}^{\text{real},\pi_{\text{exp}}^{\pi_{\text{exp}}^{h^{ \prime}-1}}}[P_{h^{\prime}}^{\text{real}}(\mathcal{S}_{h^{\prime}+1,i}\mid z_{h ^{\prime}})^{2}]\geq\frac{1}{\gamma}\cdot P_{h^{\prime}}^{\text{real}}( \mathcal{S}_{h^{\prime}+1,i}\mid z)^{2}\]

which implies that

\[\mathbb{E}^{\text{real},\pi_{\text{exp}}^{h^{\prime}-1}}[P_{h^{\prime}}^{\text{ real}}(\mathcal{S}_{h^{\prime}+1,i}\mid z_{h^{\prime}})]\geq\frac{1}{\gamma}\cdot P_{h^{ \prime}}^{\text{real}}(\mathcal{S}_{h^{\prime}+1,i}\mid z)^{2}.\] (B.6)Fix \(z\in\bar{\mathcal{Z}}_{h^{\prime}}\). Note that

\[w_{h}^{\mathsf{real},\pi}(\mathcal{Z}^{\prime}\mid z_{h^{\prime}}=z) =\mathbb{E}_{s\sim P_{h^{\prime}}^{\mathsf{real}}(\cdot\mid z)}[w_{ h}^{\mathsf{real},\pi}(\mathcal{Z}^{\prime}\mid s_{h^{\prime}+1}=s)]\] \[=\sum_{i=1}^{\infty}\mathbb{E}_{s\sim P_{h^{\prime}}^{\mathsf{ real}}(\cdot\mid z)}[w_{h}^{\mathsf{real},\pi}(\mathcal{Z}^{\prime}\mid s _{h^{\prime}+1}=s)\cdot\mathbb{I}\{s\in\mathcal{S}_{h^{\prime}+1,i}\}]\] \[\leq\sum_{i=1}^{\infty}2^{-i+1}P_{h^{\prime}}^{\mathsf{real}}( \mathcal{S}_{h^{\prime}+1,i}\mid z)\] \[=\sum_{i=1}^{\lfloor\log 4/\kappa\rfloor}2^{-i+1}P_{h^{\prime}}^{ \mathsf{real}}(\mathcal{S}_{h^{\prime}+1,i}\mid z)+\kappa\] \[\leq\sum_{i=1}^{\lfloor\log 4/\kappa\rfloor}2^{-i+1}P_{h^{ \prime}}^{\mathsf{real}}(\mathcal{S}_{h^{\prime}+1,i}\mid z)\cdot\mathbb{I} \{P_{h^{\prime}}^{\mathsf{real}}(\mathcal{S}_{h^{\prime}+1,i}\mid z)\geq\kappa \}+3\kappa\] \[\leq 2\sum_{i=1}^{\lfloor\log 4/\kappa\rfloor}\mathbb{E}_{s\sim \lambda_{i}}[w_{h}^{\mathsf{real},\pi}(\mathcal{Z}^{\prime}\mid s_{h^{\prime} +1}=s)]P_{h^{\prime}}^{\mathsf{real}}(\mathcal{S}_{h^{\prime}+1,i}\mid z) \cdot\mathbb{I}\{P_{h^{\prime}}^{\mathsf{real}}(\mathcal{S}_{h^{\prime}+1,i} \mid z)\geq\kappa\}+3\kappa\]

for any \(\lambda_{i}\in\triangle_{\mathcal{S}_{h^{\prime}+1,i}}\). Note also that, since \(\pi_{\exp}^{h^{\prime}-1}\) plays randomly for all \(h^{\prime\prime}\geq h^{\prime}\), we have:

\[w_{h}^{\mathsf{real},\pi_{\exp}^{h^{\prime}-1}}(\mathcal{Z}^{\prime}\mid s_{ h^{\prime}+1}=s)\geq\frac{1}{A^{h-h^{\prime}}}\cdot w_{h}^{\mathsf{real},\pi}( \mathcal{Z}^{\prime}\mid s_{h^{\prime}+1}=s),\]

since with probability \(1/A^{h-h^{\prime}}\) on any given episode, \(\pi_{\exp}^{h^{\prime}-1}\) will play the same sequence of actions as \(\pi\) from steps \(h^{\prime}\) to \(h\). It follows that we can bound the above as:

\[\leq 2A^{h-h^{\prime}}\cdot\sum_{i=1}^{\lfloor\log 4/\kappa \rfloor}\mathbb{E}_{s\sim\lambda_{i}}[w_{h}^{\mathsf{real},\pi_{\exp}^{h^{ \prime}-1}}(\mathcal{Z}^{\prime}\mid s_{h^{\prime}+1}=s)]P_{h^{\prime}}^{ \mathsf{real}}(\mathcal{S}_{h^{\prime}+1,i}\mid z)\cdot\mathbb{I}\{P_{h^{ \prime}}^{\mathsf{real}}(\mathcal{S}_{h^{\prime}+1,i}\mid z)\geq\kappa\}+3\kappa\] \[\stackrel{{(a)}}{{\leq}}\frac{2A^{h-h^{\prime}}\gamma} {\kappa}\cdot\sum_{i=1}^{\lfloor\log 4/\kappa\rfloor}\mathbb{E}_{s\sim \lambda_{i}}[w_{h}^{\mathsf{real},\pi_{\exp}^{h^{\prime}-1}}(\mathcal{Z}^{ \prime}\mid s_{h^{\prime}+1}=s)]\mathbb{E}^{\mathsf{real},\pi_{\exp}^{h^{ \prime}-1}}[P_{h^{\prime}}^{\mathsf{real}}(\mathcal{S}_{h^{\prime}+1,i}\mid z )\geq\kappa\}+3\kappa\] \[\leq\frac{2\gamma A^{h-h^{\prime}}}{\kappa}\cdot\sum_{i=1}^{ \lfloor\log 4/\kappa\rfloor}\mathbb{E}_{s\sim\lambda_{i}}[w_{h}^{\mathsf{real}, \pi_{\exp}^{h^{\prime}-1}}(\mathcal{Z}^{\prime}\mid s_{h^{\prime}+1}=s)] \cdot w_{h^{\prime}+1}^{\mathsf{real},\pi_{\exp}^{h^{\prime}-1}}(\mathcal{S}_{h ^{\prime}+1,i})+3\kappa\] \[\stackrel{{(b)}}{{=}}\frac{2\gamma A^{h-h^{\prime}}}{ \kappa}\cdot\sum_{i=1}^{\lfloor\log 4/\kappa\rfloor}\sum_{s\in\mathcal{S}_{h^{\prime}+1,i}}w_{h}^{ \mathsf{real},\pi_{\exp}^{h^{\prime}-1}}(\mathcal{Z}^{\prime}\mid s_{h^{\prime} +1}=s)w_{h^{\prime}+1}^{\mathsf{real},\pi_{\exp}^{h^{\prime}-1}}(s)+3\kappa\] \[\leq\frac{2\gamma A^{h-h^{\prime}}}{\kappa}\cdot w_{h}^{\mathsf{ real},\pi_{\exp}^{h^{\prime}-1}}(\mathcal{Z}^{\prime})+3\kappa\]

where \((a)\) follows from (B.6) and since \(P_{h^{\prime}}^{\mathsf{real}}(\mathcal{S}_{h^{\prime},i}\mid z)\geq\kappa\), and \((b)\) follows choosing \(\lambda_{i}(s)=w_{h^{\prime}+1}^{\mathsf{real},\pi_{\exp}^{h^{\prime}-1}}(s)/w_{h ^{\prime}+1}^{\mathsf{real},\pi_{\exp}^{h^{\prime}-1}}(\mathcal{S}_{h^{\prime}+1,i})\cdot\mathbb{I}\{s\in\mathcal{S}_{h^{\prime}+1,i}\}\). We therefore have that, for all \(z\in\bar{\mathcal{Z}}_{h^{\prime}}\):

\[w_{h}^{\mathsf{real},\pi}(\mathcal{Z}^{\prime}\mid z_{h^{\prime}}=z)\leq\frac{2 \gamma A^{h-h^{-h}}}{\kappa}\cdot w_{h}^{\mathsf{real},\pi_{\exp}^{h^{\prime}-1}}( \mathcal{Z}^{\prime})+3\kappa.\] (B.7)

Controlling events.Consider events \(\mathcal{E}:=\{z_{h}\in\mathcal{Z}^{\prime}\}\) and \(\mathcal{E}_{h^{\prime}}:=\{z_{h^{\prime}}\in\bar{\mathcal{Z}}_{h^{\prime}}\}\). We then have

\[w_{h}^{\mathsf{real},\pi}(\mathcal{Z}^{\prime}) =\mathbb{P}^{\mathsf{real},\pi}[\mathcal{E}]\] \[=\mathbb{P}^{\mathsf{real},\pi}[\mathcal{E}\cap\mathcal{E}_{h-1}]+ \mathbb{P}^{\mathsf{real},\pi}[\mathcal{E}\cap\mathcal{E}_{h-1}^{c}]\] \[=\sum_{h^{\prime}=h-k^{*}+1}^{h}\mathbb{P}^{\mathsf{real},\pi}[ \mathcal{E}\cap\mathcal{E}_{h^{\prime}-1}\cap\bigcap_{i=h^{\prime}}^{h-1} \mathcal{E}_{i}^{c}]+\mathbb{P}^{\mathsf{real},\pi}[\mathcal{E}\cap\mathcal{E}_{h-k^{* }-1}\cap\bigcap_{i=h-k^{*}}^{h-1}\mathcal{E}_{i}^{c}]\] \[\leq\sum_{h^{\prime}=h-k^{*}+1}^{h}\mathbb{P}^{\mathsf{real},\pi}[ \mathcal{E}\cap\mathcal{E}_{h^{\prime}-1}]+\mathbb{P}^{\mathsf{real},\pi}[ \mathcal{E}\cap\mathcal{E}_{h-k^{*}-1}\cap\bigcap_{i=h-k^{*}}^{h-1}\mathcal{E }_{i}^{c}].\]We now analyze each of these terms. First, note that

\[\mathbb{P}^{\mathsf{real},\pi}[\mathcal{E}\cap\mathcal{E}_{h^{\prime}-1}]=\mathbb{ P}^{\mathsf{real},\pi}[\mathcal{E}\ |\ \mathcal{E}_{h^{\prime}-1}]\mathbb{P}^{\mathsf{real},\pi}[\mathcal{E}_{h^{\prime} -1}]\leq\mathbb{P}^{\mathsf{real},\pi}[\mathcal{E}\ |\ \mathcal{E}_{h^{\prime}-1}]=w_{h}^{\mathsf{real},\pi}( \mathcal{Z}^{\prime}\ |\ z_{h^{\prime}-1}\in\bar{\mathcal{Z}}_{h^{\prime}-1}).\]

We can then bound

\[w_{h}^{\mathsf{real},\pi}(\mathcal{Z}^{\prime}\ |\ z_{h^{\prime}-1}\in\bar{ \mathcal{Z}}_{h^{\prime}-1})\leq\frac{2\gamma A^{h-h^{\prime}-1}}{\kappa} \cdot w_{h}^{\mathsf{real},\pi_{\mathrm{exp}}^{h^{\prime}-2}}(\mathcal{Z}^{ \prime})+3\kappa\]

where the inequality follows from (B.7). For the second term, we have

\[\mathbb{P}^{\mathsf{real},\pi}[\mathcal{E}\cap\mathcal{E}_{h-k^{ \star}-1}\cap\bigcap_{i=h-k^{\star}}^{h-1}\mathcal{E}_{i}^{c}] \leq\mathbb{P}^{\mathsf{real},\pi}[\mathcal{E}\cap\bigcap_{i=h- k^{\star}}^{h-1}\mathcal{E}_{i}^{c}]\] \[=\mathbb{P}^{\mathsf{real},\pi}[\mathcal{E}\ |\ \bigcap_{i=h-k^{\star}}^{h-1} \mathcal{E}_{i}^{c}]\cdot\prod_{j=1}^{k^{\star}}\mathbb{P}^{\mathsf{real},\pi}[ \mathcal{E}_{h-j}^{c}\mid\bigcap_{i=h-k^{\star}}^{h-j-1}\mathcal{E}_{i}^{c}].\]

Note, however, that \(\mathbb{P}^{\mathsf{real},\pi}[\mathcal{E}\ |\ \bigcap_{i=h-k}^{h-1},\mathcal{E}_{i}^{c}]\leq\xi\) and \(\mathbb{P}^{\mathsf{real},\pi}[\mathcal{E}_{h-j}^{c}\mid\bigcap_{i=h-k}^{h-j- 1}\mathcal{E}_{i}^{c}]\leq\xi\) for all \(j\). We therefore can bound the above as

\[\xi^{k^{\star}+1}\leq\kappa.\]

Altogether, then, we have that

\[w_{h}^{\mathsf{real},\pi}(\mathcal{Z}^{\prime})\leq\sum_{h^{\prime}=h-k^{ \star}+1}^{h}\frac{2\gamma A^{h-h^{\prime}-1}}{\kappa}\cdot w_{h}^{\mathsf{ real},\pi_{\mathrm{exp}}^{h^{\prime}-2}}(\mathcal{Z}^{\prime})+4\kappa.\]

Furthermore, since \(\pi_{\mathrm{exp}}=\mathrm{unif}(\{\pi_{\mathrm{exp}}^{h}\}_{h=1}^{H})\), we have \(w_{h}^{\mathsf{real},\pi_{\mathrm{exp}}^{h^{\prime}-2}}(\mathcal{Z}^{\prime}) \leq Hw_{h}^{\mathsf{real},\pi_{\mathrm{exp}}}(\mathcal{Z}^{\prime})\), so we conclude that

\[w_{h}^{\mathsf{real},\pi}(\mathcal{Z}^{\prime})\leq\frac{4H\gamma A^{k^{ \star}-2}}{\kappa}\cdot w_{h}^{\mathsf{real},\pi_{\mathrm{exp}}}(\mathcal{Z}^{ \prime})+4\kappa.\]

### Proof of Unconstrained Upper Bound

**Theorem 2**.: _Assume that one of the two conditions is met:_

1. _For each_ \(h\)_,_ \(\pi_{\mathrm{exp}}^{h}\) _plays actions uniformly at random for_ \(h^{\prime}>h\)_,_ \[\lambda_{\min}\left(\mathbf{\Lambda}_{\pi_{\mathrm{exp}}^{h},h}^{s}\right) \geq\bar{\lambda}_{\min},\] (B.8) _and_ \[T\geq c\cdot\frac{V_{\max}^{4}H^{4}d^{2}A^{2(k^{\star}-2)}\log(2H| \mathcal{F}|/\delta)}{\epsilon^{4}\epsilon_{\mathrm{sim}}^{2}},\] _for_ \[k^{\star}=\lceil\frac{\log_{A}\frac{64HV_{\max}}{\epsilon}}{\log_{A}1/\xi} \rceil,\quad\xi=2\sqrt{\frac{2HA}{\bar{\lambda}_{\min}}\cdot\epsilon_{\mathrm{ sim}}}.\]
2. \(\epsilon_{\mathrm{sim}}\leq\epsilon/4H^{2}\) _and_ \[T\geq\frac{16H^{2}\log\frac{4}{\delta}}{\epsilon^{2}}.\]

_Then with probability at least \(1-\delta\), Algorithm 1 returns a \(\widehat{\pi}\) such that \(V_{0}^{\mathsf{real},\pi^{\mathsf{real},\pi}}-V_{0}^{\mathsf{real},\widehat{\pi }}\leq\epsilon\)._

Proof.: We consider each of the conditions above.

Condition 1.First, note that by our assumption on \(\pi_{\mathrm{exp}}\) and applying Lemma B.4 with \(\kappa=\frac{\epsilon}{64HV_{\mathrm{max}}}\) and \(\gamma=\frac{d}{H\epsilon_{\mathrm{sim}}}\), for any \(\pi\) and \(\mathcal{Z}^{\prime}\subseteq\mathcal{S}\times\mathcal{A}\), we have

\[w_{h}^{\mathsf{real},\pi}(\mathcal{Z}^{\prime})\leq\frac{256dW_{ \mathrm{max}}A^{k^{*}-2}}{\epsilon\epsilon_{\mathrm{sim}}}\cdot w_{h}^{ \mathsf{real},\pi_{\mathrm{exp}}}(\mathcal{Z}^{\prime})+\frac{\epsilon}{16HV_{ \mathrm{max}}}\]

for

\[k^{\star}=\lceil\frac{\log_{A}\frac{64HV_{\mathrm{max}}}{\epsilon} }{\log_{A}1/\xi}\rceil,\quad\xi=2\sqrt{\frac{2HA}{\lambda_{\mathrm{min}}} \cdot\epsilon_{\mathrm{sim}}}.\]

By Lemma B.1 we then have that, with probability at least \(1-\delta\)6,

Footnote 6: Note that, while Lemma B.1 applies to the constrained regression setting, this is equivalent to the unconstrained regression setting considered here if we choose \(\gamma\) large enough so that the constraint is vacuous.

\[V_{0}^{\mathsf{real},\pi^{\mathsf{rad},\star}}-V_{0}^{\mathsf{ real},\widetilde{\pi}} \leq\frac{256dW_{\mathrm{max}}A^{k^{*}-2}}{\epsilon\epsilon_{ \mathrm{sim}}}\cdot 4H\sqrt{\frac{256V_{\mathrm{max}}^{2}\log(2H|\mathcal{F}|/ \delta)}{T}}+\epsilon/4\] \[\leq\epsilon/2\]

where the last inequality follows under our condition on \(T\).

Condition 2.By Lemma A.3, we have that \(V_{0}^{\mathsf{real},\star}-V_{0}^{\mathsf{real},\pi^{\mathsf{sim},\star}}\leq 2 H^{2}\epsilon_{\mathrm{sim}}\). Thus, if \(\epsilon_{\mathrm{sim}}\leq\epsilon/4H^{2}\), we have \(V_{0}^{\mathsf{real},\star}-V_{0}^{\mathsf{real},\pi^{\mathsf{sim},\star}} \leq\epsilon/2\).

Concluding the Proof.By what we have shown, as long as one of our conditions is met, we will have that with probability at least \(1-\delta/2\), there exists \(\pi\in\{\pi^{\widetilde{f}},\pi^{\mathsf{sim},\star}\}\) such that \(V_{0}^{\mathsf{real},\star}-V_{0}^{\mathsf{real},\pi}\leq\epsilon/2\). Denote this policy as \(\widetilde{\pi}\).

Note that \(V_{0}^{\mathsf{real},\pi}=\mathbb{E}^{\mathsf{real},\pi}[\sum_{h=0}^{H-1}r_{h}]\) and that \(\sum_{h=0}^{H-1}r_{h}\in[0,H]\) almost surely. Consider playing \(\pi\) for \(T/4\) episodes in real and let \(R^{i}\) denote the total return of the \(i\)th episode. Let

\[\widehat{V}_{0}^{\pi}:=\frac{4}{T}\sum_{i=1}^{T/4}R^{i}.\]

By Hoeffding's inequality we have that, with probability at least \(1-\delta/4\):

\[|\widehat{V}_{0}^{\pi}-V_{0}^{\mathsf{real},\pi}|\leq H\sqrt{ \frac{4\log\frac{4}{\delta}}{T}}.\]

Thus, if

\[T\geq\frac{16H^{2}\log\frac{4}{\delta}}{\epsilon^{2}},\] (B.9)

we have that \(|\widehat{V}_{0}^{\pi}-V_{0}^{\mathsf{real},\pi}|\leq\epsilon/2\). Union bounding over this for both \(\pi\in\{\pi^{\widetilde{f}},\pi^{\mathsf{sim},\star}\}\), we have that with probability at least \(1-\delta/2\):

\[V_{0}^{\mathsf{real},\widehat{\pi}}\geq\widehat{V}_{0}^{\widehat{\pi}}- \epsilon/4\geq\widehat{V}_{0}^{\widetilde{\pi}}-\epsilon/4\geq V_{0}^{ \mathsf{real},\widetilde{\pi}}-\epsilon/2.\]

It follows that

\[V^{\mathsf{real},\star}-V_{0}^{\mathsf{real},\widetilde{\pi}}\leq V^{\mathsf{ real},\star}-V_{0}^{\mathsf{real},\widetilde{\pi}}+\epsilon/2\leq\epsilon.\]

The proof follows from a union bound and our condition on \(T\) (note that (B.9) is satisfied in both cases).

Proof of Theorem 1.: We first assume that \(\zeta\leq\frac{\lambda_{\min}^{*}}{4d}\), for \(\zeta\) the input regularization value given to Algorithm 5 by Algorithm 1, and Condition 1 of Theorem 2, and show that in this case \(A^{k^{*}-2}\) is at most polynomial in problem parameters.

First, by Lemma C.7 we have that, under the assumption that \(\zeta\leq\frac{\lambda_{\min}^{*}}{4d}\), the policy \(\pi_{\exp}^{h}\) given by the uniform mixture of policies returned by Algorithm 5 will, with probability at least \(1-\delta\), satisfy \(\lambda_{\min}(\bm{\Lambda}_{\pi_{\exp}^{h},h}^{*})\geq\frac{\lambda_{\min}^{ *}}{8d}\) under Assumption 3. Plugging \(\bar{\lambda}_{\min}\leftarrow\frac{\lambda_{\min}^{*}}{8d}\) into Theorem 2, we have that \(\xi=2\sqrt{\frac{16dHA}{\lambda_{\min}^{*}}\cdot\epsilon_{\mathrm{sim}}}\). Now note that

\[A^{k^{*}-2}\leq A^{\frac{\log 4dHV_{\max}/\epsilon}{\log A^{1/ \xi}}}=\left(\frac{64HV_{\max}}{\epsilon}\right)^{1/\log A\,1/\xi}.\]

It then suffices that we show \(1/\log_{A}1/\xi\leq 1\iff 1/A\geq\xi\). However, this is clearly met by our condition on \(\epsilon_{\mathrm{sim}}\). Thus, as long as

\[T\geq c\cdot\frac{V_{\max}^{6}H^{6}d^{2}\log(2HT|\mathcal{F}|/ \delta)}{\epsilon^{6}\epsilon_{\mathrm{sim}}^{2}},\]

by Theorem 2 we have that \(\widehat{\pi}\) is \(\epsilon\)-optimal.

Now, if \(\epsilon_{\mathrm{sim}}\leq\epsilon/4H^{2}\) and \(T\geq\frac{16H^{2}\log 4/\delta}{\epsilon}\), we also have that \(\widehat{\pi}\) is \(\epsilon\)-optimal, by Theorem 2. Thus, in the first case, we at most will require

\[T\geq c\cdot\frac{V_{\max}^{6}H^{10}d^{2}\log(2HT|\mathcal{F}| /\delta)}{\epsilon^{8}}\]

to produce a policy that is \(\epsilon\)-optimal, since otherwise we will be in the second case.

It remains to justify the assumption that \(\zeta\leq\frac{\lambda_{\min}^{*}}{4d}\). Note that the condition of (4.1) is only required in the first case. Furthermore, if \(\epsilon_{\mathrm{sim}}\leq\epsilon/4H^{2}\) we will be in the second case. Thus, in the first case, we will have

\[\frac{\epsilon}{4H^{2}}\leq\epsilon_{\mathrm{sim}}\leq\frac{ \lambda_{\min}^{*}}{64dHA^{3}}.\]

Rearranging this we obtain that, to be in the first case, we have

\[\frac{16dA^{3}\epsilon}{H}\leq\lambda_{\min}^{*}\]

By our choice of \(\zeta=\frac{4A^{3}\epsilon}{H}\), we then have that \(\zeta\leq\frac{\lambda_{\min}^{*}}{4d}\). By Lemma C.7 and our choice of \(\zeta\), we have that Oracle 4.2 is called at most \(\mathrm{poly}(d,H,\epsilon^{-1},\log\frac{1}{\delta})\) times, and we call the oracle of Oracle 4.1 only \(H\) times. The result follows from a union bound and rescaling \(\delta\). 

### Reducing the Version Space

As we noted, in general, given that we do not assume that \(\bm{\phi}^{t}\) is unknown, \(\log|\mathcal{F}|\) could be significantly greater than the dimension. One might hope that, given access to \(\mathcal{M}^{\mathsf{sim}}\), we can reduce this dependence somewhat. We next show that this is possible given access to the following _constrained_ regression oracle.

**Oracle B.1** (Constrained Regression Oracle).: We assume access to a regression oracle such that, for any \(h\) and datasets \(\{(s^{t},a^{t},y^{t})\}_{t=1}^{T}\) and \(\{(\widetilde{s}^{t},\widetilde{a}^{t},\widetilde{y}^{t})\}_{t=1}^{\widetilde {T}}\), we can compute:

\[\widehat{f}_{h}=\operatorname*{arg\,min}_{f\in\mathcal{F}_{h}} \sum_{t=1}^{T}(f(s^{t},a^{t})-y^{t})^{2}\quad\text{s.t.}\quad\sum_{t=1}^{ \widetilde{T}}(f(\widetilde{s}^{t},\widetilde{a}^{t})-\widetilde{y}^{t})^{2} \leq\gamma.\]

While in general the oracle of Oracle B.1 cannot be reduced to the oracle of Oracle 4.1, under certain conditions on \(\mathcal{F}\) this is possible. Given this oracle, we have the following result.

**Theorem 3**.: _Assume that \(\epsilon_{\rm sim}\leq\frac{\lambda_{\rm min}^{\epsilon}}{64dH^{3}}\). Then if_

\[T\geq\widetilde{\mathcal{O}}\left(\frac{d^{2}H^{16}}{\epsilon^{8}}\cdot\log \frac{H|\widetilde{\mathcal{F}}|}{\delta}\right),\]

_with probability at least \(1-\delta\), Algorithm 4 returns policy \(\widehat{\pi}\) such that \(V_{0}^{\mathsf{real},\pi^{\mathsf{sm},*}}-V_{0}^{\mathsf{real},\widehat{\pi}}\leq\epsilon\), where_

\[\widetilde{\mathcal{F}}:=\bigg{\{}f\in\mathcal{F}\;:\;\sup_{\pi} \left(\mathbb{E}^{\mathsf{sim},\pi}[f_{h}(s_{h},a_{h})-\mathcal{T}^{\mathsf{ sim}}f_{h+1}(s_{h},a_{h})]\right)^{2}\leq\alpha\cdot\epsilon_{\rm sim}^{2}\bigg{\}}\]

_for \(\alpha=\widetilde{\mathcal{O}}(AdH^{3}\cdot\log^{2}\frac{\log|\mathcal{F}|/ \delta}{\epsilon_{\rm sim}})\). Furthermore, the computation oracles of Oracle 4.2 and Oracle B.1 are called at most \(\mathrm{poly}(d,A,H,\epsilon^{-1},\log\frac{|\mathcal{F}|}{\delta})\) times._

Theorem 3 shows that, rather than paying for the full complexity of \(\mathcal{F}\), we can pay only for the subset of \(\mathcal{F}\) that is Bellman-consistent on \(\mathcal{M}^{\mathsf{sim}}\).

#### b.4.1 Algorithm and Proof

```
1:input: tolerance \(\epsilon\), confidence \(\delta\), budget \(T\), \(Q\)-value function class \(\mathcal{F}\)
2:\(\Pi_{\rm exp}^{h}\leftarrow\textsc{LearnExpPolicies}(\mathcal{M}^{\mathsf{sim}}, \delta,\frac{4A^{3}\epsilon}{H},h)\) for all \(h\in[H]\)
3:\(\iota\leftarrow\mathcal{O}(\log_{2}\frac{V_{\rm max}AdH}{\epsilon})\)
4:for\(\ell=1,2,\ldots,t\)do
5:\(\bar{\epsilon}^{\ell}\gets 2^{-\ell}\), \(T^{\ell}\gets T/2\iota\), \(\gamma^{\ell}\gets 10V_{\rm max}^{2}(\bar{\epsilon}^{\ell})^{2}\)
6: Run exploration procedure of Lemma C.3 with \(\beta_{\ell}\leftarrow\frac{\gamma^{\ell}}{20V_{\rm max}^{2}\log\frac{8H| \mathcal{F}|}{\delta}}\) to obtain \(\mathfrak{D}_{\mathsf{sim}}^{\ell}\)
7:\(\widehat{\pi}^{\ell}\leftarrow\textsc{ExploreReal}\left(\{\mathrm{unif}(\Pi_{ \rm exp}^{h})\}_{h\in[H]},T^{\ell},\mathfrak{D}_{\mathsf{sim}}^{\ell},\gamma^ {\ell}\right)\) (Algorithm 3)
8:\(\widehat{V}_{0}^{\bar{\epsilon}^{\ell}}\leftarrow\) average return running \(\widehat{\pi}^{\ell}\) in real \(T^{\ell}/2\) times
9:return\(\widehat{\pi}\leftarrow\arg\max_{\ell\in[\ell]}\widehat{V}_{0}^{\bar{ \epsilon}^{\ell}}\) ```

**Algorithm 4**\(\mathsf{sim}\)-to-real transfer via simulated exploration (Sim2Explore)

**Theorem 4**.: _Assume that one of the two conditions is met:_

1. _For each_ \(h\)_,_ \(\pi_{\rm exp}^{h}\) _plays actions uniformly at random for_ \(h^{\prime}>h\)_,_ \[\lambda_{\rm min}\left(\mathbf{\Lambda}_{\pi_{\rm exp}^{h},h}^{\mathsf{s}} \right)\geq\bar{\lambda}_{\rm min},\] (B.10) _and_ \[T\geq c\cdot\frac{V_{\rm max}^{4}H^{4}d^{2}A^{2(k^{*}-2)}\iota\log(16H| \widetilde{\mathcal{F}}|/\delta)}{\epsilon^{4}\epsilon_{\rm sim}^{2}},\] _for_ \[k^{*}=\lceil\frac{\log_{A}\frac{64HV_{\rm max}}{\epsilon}}{\log_{A}1/\xi} \rceil,\quad\xi=2\sqrt{\frac{2HA}{\bar{\lambda}_{\rm min}}\cdot\epsilon_{\rm sim}}\] _and_ \[\widetilde{\mathcal{F}}:=\bigg{\{}f\in\mathcal{F}\;:\;\sup_{\pi} \left(\mathbb{E}^{\mathsf{sim},\pi}[f_{h}(s_{h},a_{h})-\mathcal{T}^{\mathsf{ sim}}f_{h+1}(s_{h},a_{h})]\right)^{2}\] \[\leq c\left(\log\frac{\log\frac{32H|\mathcal{F}|}{\delta}}{V_{\rm max }\epsilon_{\rm sim}^{2}}+1\right)AdHV_{\rm max}^{2}\log\frac{48d\log\frac{32H| \mathcal{F}|}{\delta}}{V_{\rm max}\epsilon_{\rm sim}^{2}}\cdot\epsilon_{\rm sim }^{2}\bigg{\}}.\]
2. \(\epsilon_{\rm sim}\leq\epsilon/16H^{2}\) _and_ \[T\geq c\cdot\frac{H^{2}\iota\log\frac{16\iota}{\delta}}{\epsilon^{2}}.\]

_Then with probability at least \(1-\delta\), Algorithm 4 returns a policy \(\widehat{\pi}\) such that \(V_{0}^{\mathsf{real},\pi^{\mathsf{sm},*}}-V_{0}^{\mathsf{real},\widehat{\pi}}\leq\epsilon\)._

Proof.: We break the proof into two cases.

Case 1: \(\epsilon_{\rm sim}\geq\epsilon/16H^{2}\).Let \(\bar{\ell}=\lfloor\log_{2}\epsilon_{\rm sim}^{-1}\rfloor\) and note that \(\bar{\ell}\leq\iota\) in this case and that this is a deterministic quantity. Further, note that \(\gamma^{\bar{\ell}}\in[10V_{\rm max}^{2}\epsilon_{\rm sim}^{2},40V_{\rm max}^{2 }\epsilon_{\rm sim}^{2}]\) and \(\bar{\epsilon}^{\bar{\ell}}\in[\epsilon_{\rm sim},2\epsilon_{\rm sim}]\). Note that by our assumption on \(\pi_{\rm exp}\) and applying Lemma B.4 with \(\kappa=\frac{\epsilon}{64HV_{\rm max}}\) and \(\gamma=\frac{d}{H\epsilon_{\rm sim}}\), for any \(\pi\) and \(\mathcal{Z}^{\prime}\subseteq\mathcal{S}\times\mathcal{A}\), we have

\[w_{h}^{\mathsf{real},\pi}(\mathcal{Z}^{\prime})\leq\frac{256dWV_{\rm max}A^{k^ {*}-2}}{\epsilon\epsilon_{\rm sim}}\cdot w_{h}^{\mathsf{real},\pi_{\rm exp}}( \mathcal{Z}^{\prime})+\frac{\epsilon}{16HV_{\rm max}}\]

for

\[k^{*}=\lceil\frac{\log_{A}\frac{64HV_{\rm max}}{\epsilon}}{ \log_{A}1/\xi}\rceil,\quad\xi=2\sqrt{\frac{2HA}{\bar{\lambda}_{\rm min}}\cdot \epsilon_{\rm sim}}.\]

By Lemma B.1, as long as \(\beta^{\bar{\ell}}\) and \(\gamma^{\bar{\ell}}\) satisfy

\[2V_{\rm max}^{2}\epsilon_{\rm sim}^{2}+\frac{43V_{\rm max}^{2 }\beta_{\bar{\ell}}^{2}}{dH}\cdot\log\frac{8H|\mathcal{F}_{h}|}{\delta}+6V_{ \rm max}^{2}\beta_{\bar{\ell}}\sqrt{\frac{\log\frac{8H|\mathcal{F}_{h}|}{ \delta}}{dH}}\leq\gamma^{\bar{\ell}},\] (B.11)

we have that with probability at least \(1-2\delta\),

\[V_{0}^{\mathsf{real},\pi^{\mathsf{real},\pi^{\mathsf{real},\pi}} }-V_{0}^{\mathsf{real},\bar{\pi}^{\bar{\ell}}}\leq\frac{256dWV_{\rm max}A^{k^ {*}-2}}{\epsilon\epsilon_{\rm sim}}\cdot 4H\sqrt{\frac{256V_{\rm max}^{2}\log(4H| \widetilde{\mathcal{F}^{\bar{\ell}}}|/\delta)}{T^{\bar{\ell}}}}+\epsilon/4\]

where

\[\widetilde{\mathcal{F}^{\bar{\ell}}}:=\{f\in\mathcal{F}\;:\;\mathbb{E}^{ \mathsf{sim},\pi_{\rm exp}^{\mathsf{sim}}}[(f_{h}(s_{h},a_{h})-\mathcal{T}^{ \mathsf{sim}}f_{h+1}(s_{h},a_{h}))^{2}]\leq 2\gamma^{\bar{\ell}},\forall h\in[H]\}.\]

However, since \(V_{\rm max}^{2}\epsilon_{\rm sim}\leq\frac{1}{10}\gamma^{\bar{\ell}}\), and by our choice of \(\beta^{\bar{\ell}}=\frac{\gamma^{\bar{\ell}}}{20V_{\rm max}^{2}\log\frac{8H| \mathcal{F}|}{\delta}}\), we see that (B.11) is met, so the conclusion holds. Note that, by Lemma C.5, we have that with probability at least \(1-\delta\):

\[\widetilde{\mathcal{F}^{\bar{\ell}}}\subseteq\left\{f\in\mathcal{ F}\;:\;\sup_{\pi}\;(\mathbb{E}^{\mathsf{sim},\pi}[f_{h}(s_{h},a_{h})-\mathcal{T}^{ \mathsf{sim}}f_{h+1}(s_{h},a_{h})])^{2}\right.\] \[\qquad\qquad\leq\left(4\log\frac{1}{\beta_{\bar{\ell}}}+6\right) A\cdot\left[48dH\log\frac{48d}{\beta_{\bar{\ell}}^{2}}\cdot 2\gamma^{\bar{\ell}}+V_{\rm max}^{2} \sqrt{96dH\log\frac{48d}{\beta_{\bar{\ell}}^{2}}\log\frac{1}{\delta}}\cdot \beta_{\bar{\ell}}\right]\right\}\] \[\subseteq\left\{f\in\mathcal{F}\;:\;\sup_{\pi}\;(\mathbb{E}^{ \mathsf{sim},\pi}[f_{h}(s_{h},a_{h})-\mathcal{T}^{\mathsf{sim}}f_{h+1}(s_{h}, a_{h})])^{2}\right.\] \[\qquad\qquad\leq c\left(\log\frac{\log\frac{8H|\mathcal{F}|}{ \delta}}{V_{\rm max}\epsilon_{\rm sim}^{2}}+1\right)AdHV_{\rm max}^{2}\log \frac{48d\log\frac{8H|\mathcal{F}|}{\delta}}{V_{\rm max}\epsilon_{\rm sim}^{2}} \cdot\epsilon_{\rm sim}^{2}\right\}\] \[\quad=:\widetilde{\mathcal{F}}\]

where the second inclusion follows from our setting of \(\beta_{\bar{\ell}}\), and bounds on \(\gamma^{\bar{\ell}}\).

Since \(T^{\ell}\gets T/2\iota\), it follows that if

\[T\geq c\cdot\frac{d^{2}H^{4}V_{\rm max}^{4}A^{2(k^{*}-2)}\iota \log(4H|\widetilde{\mathcal{F}}|/\delta)}{\epsilon^{4}\epsilon_{\rm sim}^{2}},\]

then we have that \(V_{0}^{\mathsf{real},\pi^{\mathsf{real},\pi^{\mathsf{real},*}}}-V_{0}^{\mathsf{ real},\bar{\pi}^{\bar{\ell}}}\leq\epsilon/2\).

Case 2: \(\epsilon_{\rm sim}\leq\epsilon/16H^{2}\).By Lemma B.5 and our choice of \(T_{\rm sim}^{\ell}\), we have that with probability at least \(1-\delta\),

\[V_{0}^{\mathsf{real},\star}-V_{0}^{\mathsf{real},\bar{\pi}^{ \bar{\iota}}}\leq 6H\left(2\log\frac{20V_{\rm max}^{2}\log\frac{8H| \mathcal{F}|}{\delta}}{\gamma^{\iota}}+3\right)\cdot\sqrt{192AdH\log\frac{960 dV_{\rm max}^{2}\log\frac{8H|\mathcal{F}|}{\delta}}{\gamma^{\iota}}}\cdot \gamma^{\iota}+4H^{2}\epsilon_{\rm sim}.\]

By our choice of \(\iota=\mathcal{O}(\log_{2}\frac{V_{\rm max}AdH}{\epsilon})\) and since \(\gamma^{\iota}=10V_{\rm max}^{2}(\bar{\epsilon}^{\iota})^{2}=10V_{\rm max}^{2} \cdot 2^{-2\iota}\), we can bound \(V_{0}^{\mathsf{real},\star}-V_{0}^{\mathsf{real},\bar{\pi}^{\bar{\iota}}}\leq \epsilon/2\).

Completing the Proof.In either case, we have that with probability at least \(1-\delta\), there exists some \(\widehat{i}\in[\iota]\) such that \(V_{0}^{\mathsf{real},\star}-V_{0}^{\mathsf{real},\widehat{\pi}^{\widehat{i}}} \leq\epsilon/2\).

Note that \(V_{0}^{\mathsf{real},\star}=\mathbb{E}^{\mathsf{real},\pi}[\sum_{h=0}^{H-1}r_{h}]\) and that \(\sum_{h=0}^{H-1}r_{h}\in[0,H]\) almost surely. Consider playing \(\pi\) for \(n\) episodes in real and let \(R^{i}\) denote the total return of the \(i\)th episode. Let

\[\widehat{V}_{0}^{\pi}:=\frac{1}{n}\sum_{i=1}^{n}R^{i}.\]

By Hoeffding's inequality we have that, with probability at least \(1-\delta/\iota\):

\[|\widehat{V}_{0}^{\pi}-V_{0}^{\mathsf{real},\pi}|\leq H\sqrt{\frac{\log\frac{ 2\iota}{\delta}}{n}}.\]

Thus, if

\[n\geq\frac{16H^{2}\log\frac{2\iota}{\delta}}{\epsilon^{2}},\]

we have that \(|\widehat{V}_{0}^{\pi}-V_{0}^{\mathsf{real},\pi}|\leq\epsilon/2\). However, as we run each \(\pi\in\widehat{\Pi}^{\ell}\ T_{\ell}/2=T/2\iota\) times, and in either case we assume \(T\geq\frac{\epsilon H^{2}}{\epsilon^{2}}\cdot\log\frac{4\iota}{\delta}\), this will be met. Union bounding over this for all \(\widehat{\pi}^{\ell}\), we have that with probability at least \(1-\delta\):

\[V_{0}^{\mathsf{real},\widehat{\pi}}\geq\widehat{V}_{0}^{\widehat{\pi}}- \epsilon/4\geq\widehat{V}_{0}^{\widehat{\pi}^{\widehat{i}}}-\epsilon/4\geq V _{0}^{\mathsf{real},\widehat{\pi}^{\widehat{i}}}-\epsilon/2.\]

It follows that

\[V^{\mathsf{real},\star}-V_{0}^{\mathsf{real},\widehat{\pi}}\leq V^{\mathsf{ real},\star}-V_{0}^{\mathsf{real},\widehat{\pi}^{\widehat{i}}}+\epsilon/2\leq\epsilon.\]

The result then follows from a union bound and rescaling \(\delta\). 

Proof of Theorem 3.: The argument follows analogously to the proof of Theorem 1, but using Theorem 4 in place of Theorem 2. The bound on the number of oracle calls follows from Lemma C.3 and our choice of \(\beta_{\ell}\). 

**Lemma B.5**.: _With probability at least \(1-\delta\), for some \(\ell\), we have_

\[V_{0}^{\mathsf{sim},\star}-V_{0}^{\mathsf{sim},\widehat{\pi}^{ \ell}} \leq 6H\left(2\log\frac{20V_{\max}^{2}\log\frac{8H|\mathcal{F}|}{ \delta}}{\gamma^{\ell}}+3\right)\cdot\sqrt{192AdH\log\frac{960dV_{\max}^{2} \log\frac{8H|\mathcal{F}|}{\delta}}{\gamma^{\ell}}}\cdot\gamma^{\ell},\] \[V_{0}^{\mathsf{real},\star}-V_{0}^{\mathsf{real},\widehat{\pi}^{ \ell}} \leq 6H\left(2\log\frac{20V_{\max}^{2}\log\frac{8H|\mathcal{F}|}{ \delta}}{\gamma^{\ell}}+3\right)\cdot\sqrt{192AdH\log\frac{960dV_{\max}^{2} \log\frac{8H|\mathcal{F}|}{\delta}}{\gamma^{\ell}}}\cdot\gamma^{\ell}+4H^{2} \epsilon_{\mathrm{sim}}.\]

Proof.: By Lemma C.4 we have, with probability at least \(1-\delta\),

\[V_{0}^{\mathsf{sim},\star}-V_{0}^{\mathsf{sim},\widehat{\pi}^{ \ell}} \leq 2H\left(2\log\frac{1}{\beta_{\ell}}+3\right)\cdot\left[\beta_{ \ell}\sqrt{512V_{\max}^{2}A\log\frac{8H|\mathcal{F}|}{\delta}}+\sqrt{96AdH\log \frac{48d}{\beta_{\ell}^{2}}\cdot\gamma^{\ell}}\right.\] \[\qquad+\left.\sqrt{2AV_{\max}^{2}\sqrt{96dH\log\frac{48d}{\beta_{ \ell}^{2}}\log\frac{2}{\delta}}\cdot\beta_{\ell}}\right]\] \[\leq 6H\left(2\log\frac{20V_{\max}^{2}\log\frac{8H|\mathcal{F}|}{ \delta}}{\gamma^{\ell}}+3\right)\cdot\sqrt{192AdH\log\frac{960dV_{\max}^{2} \log\frac{8H|\mathcal{F}|}{\delta}}{\gamma^{\ell}}}\cdot\gamma^{\ell}\]

where the second inequality holds by our setting of \(\beta_{\ell}\).

We have

\[V_{0}^{\mathsf{real},\star}-V_{0}^{\mathsf{real},\widehat{\pi}^{\ell}}=V_{0}^{ \mathsf{real},\star}-V_{0}^{\mathsf{real},\pi^{\mathsf{sim},\star}}+V_{0}^{ \mathsf{real},\pi^{\mathsf{sim},\star}}-V_{0}^{\mathsf{sim},\pi^{\mathsf{sim}, \star}}+V_{0}^{\mathsf{sim},\pi^{\mathsf{sim},\star}}-V_{0}^{\mathsf{sim}, \widehat{\pi}^{\ell}}+V_{0}^{\mathsf{sim},\widehat{\pi}^{\ell}}-V_{0}^{\mathsf{ real},\widehat{\pi}^{\ell}}.\]By Lemma A.3, we can bound

\[V_{0}^{\mathsf{real},\star}-V_{0}^{\mathsf{real},\pi^{\mathsf{sim},\star}}\leq 2 H^{2}\epsilon_{\text{sim}}\]

and by Lemma A.1 we can bound

\[V_{0}^{\mathsf{real},\pi^{\mathsf{sim},\star}}-V_{0}^{\mathsf{sim},\pi^{ \mathsf{sim},\star}}\leq H^{2}\epsilon_{\text{sim}},\quad V_{0}^{\mathsf{ sim},\widehat{\pi}^{\ell}}-V_{0}^{\mathsf{real},\widehat{\pi}^{\ell}}\leq H^{2} \epsilon_{\text{sim}}.\]

Combining this with our bound on \(V_{0}^{\mathsf{sim},\star}-V_{0}^{\mathsf{sim},\widehat{\pi}^{\ell}}\) gives the result. 

## Appendix C Learning in \(\mathsf{sim}\)

In this section we provide additional supporting lemmas for our main results and in particular, we focus on linear in \(\mathsf{sim}\). In Appendix C.1 we provide several technical results critical to showing that \(\mathsf{sim}\) can be utilized to restrict the version space, as is done in Theorem 4. In order to restrict the version space using \(\mathsf{sim}\), sufficiently rich data must be collected from \(\mathsf{sim}\), and in Appendix C.2 we provide results on this data collection. Finally, in Appendix C.3 we provide a procedure to compute the exploration policies in \(\mathsf{sim}\) which we ultimately transfer to \(\mathsf{real}\).

In Appendices C.1 and C.2, we let hypothesis \(\widetilde{f}\) and \(\widehat{f}\) be defined recursively as:

\[\widetilde{f}_{h}:=\operatorname*{arg\,min}_{f_{h}\in\mathcal{F}_{h}}\frac{1 }{T_{\mathsf{sim}}}\sum_{t=1}^{T_{\mathsf{sim}}}(f_{h}(\widetilde{s}_{h}^{t},\widetilde{a}_{h}^{t})-\widetilde{r}_{h}^{t}-\max_{a^{\prime}}\widehat{f}_{h+ 1}(\widetilde{s}_{h+1}^{t},a^{\prime}))^{2}.\]

and \(\widehat{f}_{h}\in\mathcal{F}_{h}\) some hypothesis satisfying

\[\frac{1}{T_{\mathsf{sim}}}\sum_{t=1}^{T_{\mathsf{sim}}}(\widehat{f}_{h}( \widetilde{s}_{h}^{t},\widetilde{a}_{h}^{t})-\widetilde{f}_{h}(\widetilde{s} _{h}^{t},\widetilde{a}_{h}^{t}))^{2}\leq\gamma\]

for parameter \(\gamma>0\).

In Appendix C.1 we make the following assumption on the data generating process.

**Assumption 5**.: _Consider the dataset \(\mathfrak{D}_{\mathsf{sim}}=\{(\widetilde{s}_{0}^{t},\widetilde{a}_{0}^{t}, \widetilde{r}_{0}^{t},\ldots,\widetilde{s}_{H-1}^{t},\widetilde{a}_{H-1}^{t}, \widetilde{r}_{H-1}^{t})\}_{t=1}^{T_{\mathsf{sim}}}\). We assume that episode \(t\) in \(\mathfrak{D}_{\mathsf{sim}}\) was generated by playing an \(\mathcal{F}_{t-1}\)-measurable policy \(\widetilde{\pi}_{\text{exp}}^{t}\), and denote \(\pi_{\text{exp}}^{\mathsf{sim}}=\operatorname*{unif}(\{\widetilde{\pi}_{\text {exp}}^{t}\}_{t=1}^{T_{\mathsf{sim}}})\)._

We provide a specific instantiation of \(\pi_{\text{exp}}^{\mathsf{sim}}\) in Appendix C.2. In Appendix C.3, we provide a procedure for learning a set of policies which induce full-rank covariates in \(\mathsf{sim}\), a crucial piece in obtaining good exploration performance in \(\mathsf{real}\).

### Regularizing with Data from \(\mathsf{sim}\)

**Lemma C.1**.: _With probability at least \(1-\delta\):_

\[\frac{1}{T_{\mathsf{sim}}}\sum_{t=1}^{T_{\mathsf{sim}}}(\mathcal{T}^{\mathsf{ real}}\widehat{f}_{h+1}(\widetilde{s}_{h}^{t},\widetilde{a}_{h}^{t})- \widetilde{f}_{h}(\widetilde{s}_{h}^{t},\widetilde{a}_{h}^{t}))^{2}\leq 2V_{ \max}^{2}\epsilon_{\text{sim}}^{2}+\frac{512V_{\max}^{2}}{T_{\mathsf{sim}}} \cdot\log\frac{4|\mathcal{F}_{h}|}{\delta}+V_{\max}^{2}\sqrt{\frac{2\log\frac{ 2|\mathcal{F}_{h}|}{\delta}}{T_{\mathsf{sim}}}}.\]

Proof.: First, note that \(\mathcal{T}^{\mathsf{real}}\widehat{f}_{h+1}\in\mathcal{F}_{h}\) by Assumption 4.

By Azuma-Hoeffding and a union bound, we have that, with probability at least \(1-\delta\), for each \(f,f^{\prime}\in\mathcal{F}_{h}\),

\[\frac{1}{T_{\mathsf{sim}}}\sum_{t=1}^{T_{\mathsf{sim}}}(f_{h}( \widetilde{s}_{h}^{t},\widetilde{a}_{h}^{t})-f_{h}^{\prime}(\widetilde{s}_{h} ^{t},\widetilde{a}_{h}^{t}))^{2} \leq\frac{1}{T_{\mathsf{sim}}}\sum_{t=1}^{T_{\mathsf{sim}}}\mathbb{ E}^{\mathsf{sim},\widetilde{\pi}_{\text{exp}}^{t}}[(f_{h}(\widetilde{s}_{h}, \widetilde{a}_{h})-f_{h}^{\prime}(\widetilde{s}_{h},\widetilde{a}_{h}))^{2}]+V_ {\max}^{2}\sqrt{\frac{2\log|\mathcal{F}_{h}|/\delta}{T_{\mathsf{sim}}}}\] \[=\mathbb{E}^{\mathsf{sim},\pi_{\text{exp}}^{\mathsf{sim}}}[(f_{h}(s _{h},a_{h})-f_{h}^{\prime}(s_{h},a_{h}))^{2}]+V_{\max}^{2}\sqrt{\frac{2\log| \mathcal{F}_{h}|/\delta}{T_{\mathsf{sim}}}}.\]

[MISSING_PAGE_FAIL:31]

By Lemma 3 of [57], with probability at least \(1-\delta\),

\[\mathbb{E}^{\mathsf{sim},\pi^{\mathsf{sim}}_{\mathrm{exp}}}[(\widetilde{f}_{h}(s _{h},a_{h})-\mathcal{T}^{\mathsf{sim}}\widehat{f}_{h+1}(s_{h},a_{h}))^{2}]\leq \frac{256V_{\max}^{2}}{T_{\mathsf{sim}}}\cdot\log\frac{2|\mathcal{F}_{h}|}{\delta}.\]

We can therefore bound the final term as

\[\mathbb{E}^{\mathsf{sim},\pi^{\mathsf{sim}}_{\mathrm{exp}}}[( \widetilde{f}_{h}(s_{h},a_{h})-\mathcal{T}^{\mathsf{sim}}\widehat{f}_{h+1}(s_ {h},a_{h}))(f_{h}(s_{h},a_{h})-\mathcal{T}^{\mathsf{sim}}\widehat{f}_{h+1}(s_{ h},a_{h}))]\] \[\qquad\leq V_{\max}\cdot\mathbb{E}^{\mathsf{sim},\pi^{\mathsf{ sim}}_{\mathrm{exp}}}[|\widetilde{f}_{h}(s_{h},a_{h})-\mathcal{T}^{\mathsf{sim}} \widehat{f}_{h+1}(s_{h},a_{h})|]\] \[\qquad\leq V_{\max}\cdot\sqrt{\mathbb{E}^{\mathsf{sim},\pi^{ \mathsf{sim}}_{\mathrm{exp}}}[(\widetilde{f}_{h}(s_{h},a_{h})-\mathcal{T}^{ \mathsf{sim}}\widehat{f}_{h+1}(s_{h},a_{h}))^{2}]}\] \[\qquad\leq V_{\max}\cdot\sqrt{\frac{256V_{\max}^{2}}{T_{\mathsf{ sim}}}\cdot\log\frac{2|\mathcal{F}_{h}|}{\delta}}.\]

Altogether then we have shown that, for any \(f_{h}\in\mathcal{F}_{h}\), with probability at least \(1-2\delta\):

\[\frac{1}{T_{\mathsf{sim}}}\sum_{t=1}^{T_{\mathsf{sim}}}(f_{h}(\widetilde{s}_{ h}^{t},\widetilde{a}_{h}^{t})-\widetilde{f}_{h}(\widetilde{s}_{h}^{t}, \widetilde{a}_{h}^{t}))^{2}\geq\mathbb{E}^{\mathsf{sim},\pi^{\mathsf{sim}}_{ \mathrm{exp}}}[(f_{h}(s_{h},a_{h})-\mathcal{T}^{\mathsf{sim}}\widehat{f}_{h+1 }(s_{h},a_{h}))^{2}]-18V_{\max}^{2}\sqrt{\frac{\log 2|\mathcal{F}_{h}|/\delta}{T_{ \mathsf{sim}}}}.\]

Thus, if

\[\frac{1}{T_{\mathsf{sim}}}\sum_{t=1}^{T_{\mathsf{sim}}}(f_{h}(\widetilde{s}_{ h}^{t},\widetilde{a}_{h}^{t})-\widetilde{f}_{h}(\widetilde{s}_{h}^{t}, \widetilde{a}_{h}^{t}))^{2}\leq\gamma,\]

then

\[\mathbb{E}^{\mathsf{sim},\pi^{\mathsf{sim}}_{\mathrm{exp}}}[(f_{h}(s_{h},a_{h })-\mathcal{T}^{\mathsf{sim}}\widehat{f}_{h+1}(s_{h},a_{h}))^{2}]\leq\gamma+ 18V_{\max}^{2}\sqrt{\frac{\log 2|\mathcal{F}_{h}|/\delta}{T_{ \mathsf{sim}}}}.\]

The result follows from a union bound. 

### Data Collection with CoverTraj

**Lemma C.3**.: _Consider running the CoverTraj algorithm of [65] for each \(h\in[H]\) with parameters \(m\leftarrow\lceil\log_{2}1/\beta\rceil\) and \(\gamma_{i}\gets 2^{i}\cdot\beta\) for some \(\beta\in[0,1]\), and with RegMin set to the policy optimization oracle of Oracle 4.2. Then this procedure collects_

\[T_{\mathsf{sim}}:=H\cdot\sum_{i=1}^{m}\left\lceil\frac{24d}{2^{i}\cdot\beta^{ 2}}\log\frac{48d}{2^{i}\cdot\beta^{2}}\right\rceil\]

_episodes, calls the policy optimization oracle at most \(T_{\mathsf{sim}}\) times, and produces covariates \(\boldsymbol{\Lambda}_{h,i}\) and sets \(\mathcal{X}_{h,i}\) such that, for each \(i\in[m]\),_

\[\sup_{\pi}w_{h}^{\mathsf{sim},\pi}(\mathcal{X}_{h,i})\leq 2^{-i+1}\quad\text{and} \quad\boldsymbol{\phi}^{\top}\boldsymbol{\Lambda}_{h,i}^{-1}\boldsymbol{\phi} \leq 2^{2i}\cdot\beta^{2},\forall\boldsymbol{\phi}\in\mathcal{X}_{h,i},\]

_and \(\sup_{\pi}w_{h}^{\mathsf{sim},\pi}(\mathcal{B}^{d}\backslash\cup_{i=1}^{m} \mathcal{X}_{h,i})\leq\beta\). Furthermore, we have_

\[\frac{12dH}{\beta^{2}}\leq T_{\mathsf{sim}}\leq\frac{48dH}{\beta^{2}}\log \frac{48d}{\beta^{2}}.\]

Proof.: Instantiating RegMin with the oracle of Oracle 4.2, we have that Definition 5.1 of [65] is met with \(\mathcal{C}_{1}=\mathcal{C}_{2}=0\). Therefore, we have that at each stage \(i\) we collect exactly (using the precise form for \(K_{i}\) given in the appendix of [65])

\[K_{i}=\lceil 2^{i}\cdot\frac{24d}{\gamma_{i}^{2}}\log\frac{48\cdot 2^{i}d}{ \gamma_{i}^{2}}\rceil\]

episodes. The result then follows by Theorem 3 of [65].

**Lemma C.4**.: _Consider running the procedure of Lemma C.3 to collect data. Then with probability at least \(1-2\delta\), we have_

\[V_{0}^{\mathsf{sim},\star}-V_{0}^{\mathsf{sim},\pi^{\widehat{f}}} \leq 2H\left(2\log\frac{1}{\beta}+3\right)\cdot\bigg{[}\beta\sqrt{512 V_{\max}^{2}A\log(4H|\mathcal{F}|/\delta)}+\sqrt{96AdH\log\frac{48d}{\beta^{2}} \cdot\gamma}\] \[\quad+\sqrt{2AV_{\max}^{2}\sqrt{96dH\log\frac{48d}{\beta^{2}}\log \frac{1}{\delta}\cdot\beta}}\bigg{]}.\]

Proof.: By Lemma A.4:

\[V_{0}^{\mathsf{sim},\star}-V_{0}^{\mathsf{sim},\pi^{\widehat{f}}} \leq\max_{\pi\in\{\pi^{\widehat{f}},\pi^{\widehat{\mathsf{sim}}, \pi^{\widehat{f}}}\}}\sum_{h=0}^{H-1}2\left|\mathbb{E}^{\mathsf{sim},\pi}[ \widehat{f}_{h}(s_{h},a_{h})-\mathcal{T}^{\mathsf{sim}}\widehat{f}_{h+1}(s_{h },a_{h})]\right|\] \[\leq\max_{\pi\in\{\pi^{\widehat{f}},\pi^{\widehat{\mathsf{sim}}, \pi^{\widehat{\mathsf{sim}}}}\}}\sum_{h=0}^{H-1}2\mathbb{E}^{\mathsf{sim},\pi}[ \lvert\widehat{f}_{h}(s_{h},a_{h})-\mathcal{T}^{\mathsf{sim}}\widehat{f}_{h+1} (s_{h},a_{h})\rvert].\]

Denote \(g(z_{h}):=\lvert\widehat{f}_{h}(s_{h},a_{h})-\mathcal{T}^{\mathsf{sim}} \widehat{f}_{h+1}(s_{h},a_{h})\rvert\) and \(\boldsymbol{\Lambda}_{h-1}=\sum_{i=1}^{m}\boldsymbol{\Lambda}_{h,i}+I\), for \(\boldsymbol{\Lambda}_{h,i}\) collected as in Lemma C.3, and note that

\[\mathbb{E}^{\mathsf{sim},\pi}[g(z_{h})] =\mathbb{E}^{\mathsf{sim},\pi}[\int g(z)\mathrm{d}P_{h}^{\pi}(z \mid z_{h-1})]\] \[=\mathbb{E}^{\mathsf{sim},\pi}[\int\int g(z)\pi(a\mid s) \mathrm{d}\mathrm{d}\boldsymbol{\mu}_{h-1}^{\mathsf{s}}(s)^{\top}\boldsymbol {\phi}^{\mathsf{s}}(z_{h-1})]\] (C.1) \[=\mathbb{E}^{\mathsf{sim},\pi}[\int\int g(z)\pi(a\mid s) \mathrm{d}\mathrm{d}\boldsymbol{\mu}_{h-1}^{\mathsf{s}}(s)^{\top}\boldsymbol {\Lambda}_{h-1}^{1/2}\boldsymbol{\Lambda}_{h-1}^{-1/2}\boldsymbol{\phi}^{ \mathsf{s}}(z_{h-1})]\] \[\leq\mathbb{E}^{\mathsf{sim},\pi}[\lvert\lvert\int\int g(z)\pi (a\mid s)\mathrm{d}\mathrm{d}\boldsymbol{\mu}_{h-1}^{\mathsf{s}}(s)\rvert _{\boldsymbol{\Lambda}_{h-1}}\cdot\lVert\boldsymbol{\phi}^{\mathsf{s}}(z_{h-1 })\rvert_{\boldsymbol{\Lambda}_{h-1}^{-1}}]\] \[=\lVert\int\int g(z)\pi(a\mid s)\mathrm{d}\mathrm{d}\boldsymbol{ \mu}_{h-1}^{\mathsf{s}}(s)\rVert_{\boldsymbol{\Lambda}_{h-1}}\cdot\mathbb{E}^ {\mathsf{sim},\pi}[\lVert\boldsymbol{\phi}^{\mathsf{s}}(z_{h-1})\rVert_{ \boldsymbol{\Lambda}_{h-1}^{-1}}].\]

We bound each of these terms separately. First, we have

\[\mathbb{E}^{\mathsf{sim},\pi}[\lVert\boldsymbol{\phi}^{\mathsf{ s}}(z_{h-1})\rVert_{\boldsymbol{\Lambda}_{h-1}^{-1}}] \leq\sum_{i=1}^{m}\max_{\Phi\in\mathcal{X}_{h-1,i}}\lVert \boldsymbol{\phi}\rVert_{\boldsymbol{\Lambda}_{h-1}^{-1}}\cdot\sup_{\pi} \mathbb{E}^{\mathsf{sim},\pi}[\mathbb{I}\{\boldsymbol{\phi}^{\mathsf{s}}(z_{h-1 })\in\mathcal{X}_{h-1,i}\}]\] \[\quad+\max_{\phi\in\mathcal{B}^{\mathsf{s}}\cup_{i=1}^{m} \mathcal{X}_{h-1,i}}\lVert\boldsymbol{\phi}\rVert_{\boldsymbol{\Lambda}_{h-1}^ {-1}}\cdot\sup_{\pi}\mathbb{E}^{\mathsf{sim},\pi}[\mathbb{I}\{\boldsymbol{\phi }^{\mathsf{s}}(z_{h-1})\in\mathcal{X}_{h-1,i}\}]\] \[\stackrel{{(a)}}{{\leq}}\sum_{i=1}^{t}\gamma_{i} \cdot 2^{-i+1}+\beta\] \[\leq(2m+1)\beta\]

where \((a)\) follows from Lemma C.3 and since \(\lVert\boldsymbol{\phi}\rVert_{\boldsymbol{\Lambda}_{h-1}^{-1}}\leq 1\) always.

We turn now to bounding the first term. Note that

\[\|\int\int g(z)\pi(a\mid s)\mathrm{d}a\mathrm{d}\bm{\mu}_{h-1}^{ \bm{s}}(s)\|_{\bm{\Lambda}_{h-1}}\] \[=\sqrt{\sum_{t=1}^{T_{\text{sim}}}(\int\int g(z)\pi(a\mid s) \mathrm{d}a\mathrm{d}\bm{\mu}_{h-1}^{\bm{s}}(s)^{\top}\bm{\phi}_{h-1}^{t})^{2}}\] \[=\sqrt{\sum_{t=1}^{T_{\text{sim}}}\mathbb{E}^{\pi}[g(z_{h})\mid z_ {h-1}^{t}]^{2}}\] \[\leq\sqrt{\sum_{t=1}^{T_{\text{sim}}}\mathbb{E}^{\pi}[g(z_{h})^{2 }\mid z_{h-1}^{t}]}\] \[\overset{(a)}{\leq}\sqrt{A\cdot\sum_{t=1}^{T_{\text{sim}}} \mathbb{E}^{\pi_{\text{exp}}^{h-1,t}}[g(z_{h})^{2}\mid z_{h-1}^{t}]}\] \[=\sqrt{A\cdot\sum_{t=1}^{T_{\text{sim}}}\mathbb{E}^{\pi_{\text{ exp}}^{h-1,t}}[(\widehat{f}_{h}(s_{h},a_{h})-\mathcal{T}^{\text{sim}} \widehat{f}_{h+1}(s_{h},a_{h}))^{2}\mid z_{h-1}^{t}]}\] \[\leq\sqrt{2A\cdot\sum_{t=1}^{T_{\text{sim}}}\mathbb{E}^{\pi_{ \text{exp}}^{h-1,t}}[(\widetilde{f}_{h}(s_{h},a_{h})-\mathcal{T}^{\text{sim}} \widehat{f}_{h+1}(s_{h},a_{h}))^{2}\mid z_{h-1}^{t}]+2A\cdot\sum_{t=1}^{T_{ \text{sim}}}\mathbb{E}^{\pi_{\text{exp}}^{h-1,t}}[(\widetilde{f}_{h}(s_{h},a_ {h})-\widehat{f}_{h}(s_{h},a_{h}))^{2}\mid z_{h-1}^{t}]}\] \[\overset{(b)}{\leq}\sqrt{512V_{\max}^{2}A\log(4H|\mathcal{F}|/ \delta)+2A\cdot\sum_{t=1}^{T_{\text{sim}}}\mathbb{E}^{\pi_{\text{exp}}^{h-1,t }}[(\widetilde{f}_{h}(s_{h},a_{h})-\widehat{f}_{h}(s_{h},a_{h}))^{2}\mid z_{h- 1}^{t}]}\]

where \((a)\) uses the fact that \(\pi_{\text{exp}}^{h-1,t}\) plays actions randomly at step \(h\) and \((b)\) holds with probability at least \(1-\delta\) by Lemma C.6. By Azuma-Hoeffding, we have with probability \(1-\delta\):

\[\sum_{t=1}^{T_{\text{sim}}}\mathbb{E}^{\pi_{\text{exp}}^{h-1,t}}[( \widetilde{f}_{h}(s_{h},a_{h})-\widehat{f}_{h}(s_{h},a_{h}))^{2}\mid z_{h-1}^{ t}] \leq\sum_{t=1}^{T_{\text{sim}}}(\widetilde{f}_{h}(s_{h}^{t},a_{h }^{t})-\widehat{f}_{h}(s_{h}^{t},a_{h}^{t}))^{2}+\sqrt{2V_{\max}^{4}T_{\text{ sim}}\log 1/\delta}\] \[\leq T_{\text{sim}}\gamma+\sqrt{2V_{\max}^{4}T_{\text{sim}}\log 1 /\delta}\]

where the last inequality follows from the definition of \(\widehat{f}_{h}\).

Altogether then we have shown that, with probability at least \(1-2\delta\):

\[V_{0}^{\text{sim},\star}-V_{0}^{\text{sim},\pi^{\tilde{f}}}\leq 2H(2m+1)\beta \cdot\sqrt{512V_{\max}^{2}A\log(4H|\mathcal{F}|/\delta)+2AT_{\text{sim}} \gamma+2AV_{\max}^{2}\sqrt{2T_{\text{sim}}\log 1/\delta}}.\]

Using that \(T_{\text{sim}}\leq\frac{48dH}{\beta^{2}}\log\frac{48d}{\beta^{2}}\) as given in Lemma C.3, we can bound this as

\[\leq 2H(2m+1)\bigg{[}\beta\sqrt{512V_{\max}^{2}A\log(4H|\mathcal{F }|/\delta)}+\sqrt{96AdH\log\frac{48d}{\beta^{2}}\cdot\gamma}\] \[\qquad+\sqrt{2AV_{\max}^{2}\sqrt{96dH\log\frac{48d}{\beta^{2}} \log\frac{1}{\delta}\cdot\beta}}\bigg{]}.\]

The result follows. 

**Lemma C.5**.: _Assume that_

\[\mathbb{E}^{\text{sim},\pi_{\text{exp}}^{\text{sim}}}[(f_{h}(s_{h},a_{h})- \mathcal{T}^{\text{sim}}f_{h+1}(s_{h},a_{h}))^{2}]\leq\gamma.\]_Then this implies that, with probability at least \(1-\delta\),_

\[\sup_{\pi} \left(\mathbb{E}^{\mathsf{sim},\pi}[f_{h}(s_{h},a_{h})-\mathcal{T}^ {\mathsf{sim}}f_{h+1}(s_{h},a_{h})]\right)^{2}\] \[\leq\left(4\log\frac{1}{\beta}+6\right)A\cdot\left[48d\log\frac{48 d}{\beta^{2}}\cdot\gamma+V_{\max}\sqrt{96d\log\frac{48d}{\beta^{2}}\log\frac{1}{ \delta}}\cdot\beta\right].\]

_Therefore,_

\[\{f\in\mathcal{F}\,:\,\,\mathbb{E}^{\mathsf{sim},\pi^{\mathsf{ sim}}}[(f_{h}(s_{h},a_{h})-\mathcal{T}^{\mathsf{sim}}f_{h+1}(s_{h},a_{h}))^{2}] \leq\gamma\}\] \[\leq\bigg{\{}f\in\mathcal{F}\,:\,\sup_{\pi}\left(\mathbb{E}^{ \mathsf{sim},\pi}[f_{h}(s_{h},a_{h})-\mathcal{T}^{\mathsf{sim}}f_{h+1}(s_{h}, a_{h})]\right)^{2}\] \[\leq\left(4\log\frac{1}{\beta}+6\right)A\cdot\left[48dH\log\frac{ 48d}{\beta^{2}}\cdot\gamma+V_{\max}^{2}\sqrt{96dH\log\frac{48d}{\beta^{2}}\log \frac{1}{\delta}}\cdot\beta\right]\bigg{\}}.\]

Proof.: We follow a similar argument as the proof of Lemma C.4. Denoting \(g(z_{h}):=f_{h}(s_{h},a_{h})-\mathcal{T}^{\mathsf{sim}}f_{h+1}(s_{h},a_{h})\), by the same calculation as (C.1) we have

\[\mathbb{E}^{\mathsf{sim},\pi}[g(z_{h})]\leq\|\int\int g(z)\pi(a\mid s)\mathrm{ d}a\mathrm{d}\bm{\mu}_{h-1}^{\mathsf{s}}(s)\|_{\mathbf{\Lambda}_{h-1}}\cdot \mathbb{E}^{\mathsf{sim},\pi}[\|\bm{\phi}^{\mathsf{s}}(z_{h-1})\|_{\mathbf{ \Lambda}_{h-1}^{-1}}]\]

and as in the proof of Lemma C.4, we can bound

\[\mathbb{E}^{\mathsf{sim},\pi}[\|\bm{\phi}^{\mathsf{s}}(z_{h-1})\|_{\mathbf{ \Lambda}_{h-1}^{-1}}]\leq(2m+1)\beta\]

and

\[\|\int\int g(z)\pi(a\mid s)\mathrm{d}a\mathrm{d}\bm{\mu}_{h-1}^{ \mathsf{s}}(s)\|_{\mathbf{\Lambda}_{h-1}}\leq\sqrt{A\cdot\sum_{t=1}^{T_{ \mathsf{sim}}}\mathbb{E}^{\pi_{\exp}^{h-1,t}}[(f_{h}(s_{h},a_{h})-\mathcal{T} ^{\mathsf{sim}}f_{h+1}(s_{h},a_{h}))^{2}\mid z_{h-1}^{t}]}\]

By Azuma-Hoeffding, with probability at least \(1-\delta\) we can then bound

\[\sum_{t=1}^{T_{\mathsf{sim}}}\mathbb{E}^{\pi_{\exp}^{h-1,t}}[(f_ {h}(s_{h},a_{h})-\mathcal{T}^{\mathsf{sim}}f_{h+1}(s_{h},a_{h}))^{2}\mid z_{h -1}^{t}] \leq T_{\mathsf{sim}}\cdot\mathbb{E}^{\pi_{\exp}^{\mathsf{sim}}}[( f_{h}(s_{h},a_{h})-\mathcal{T}^{\mathsf{sim}}f_{h+1}(s_{h},a_{h}))^{2}]\] \[\qquad+\sqrt{2V_{\max}^{4}T_{\mathsf{sim}}\log 1/\delta}\] \[\leq T_{\mathsf{sim}}\gamma+\sqrt{2V_{\max}^{4}T_{\mathsf{sim}} \log 1/\delta}\]

where the last inequality follows by assumption, and where \(\pi_{\exp}^{\mathsf{sim}}=\mathrm{unif}(\{\pi_{\exp}^{h-1,t}\}_{t=1}^{T_{ \mathsf{sim}}})\). Altogether then, for all \(\pi\), we have

\[\mathbb{E}^{\mathsf{sim},\pi}[f_{h}(s_{h},a_{h})-\mathcal{T}^{\mathsf{sim}}f_{ h+1}(s_{h},a_{h})]\leq(2m+1)\beta\cdot\sqrt{AT_{\mathsf{sim}}\gamma+AV_{\max}^{2} \sqrt{2T_{\mathsf{sim}}\log 1/\delta}}.\]

Using that \(T_{\mathsf{sim}}\leq\frac{48dH}{\beta^{2}}\log\frac{48d}{\beta^{2}}\) as given in Lemma C.3, we can bound this as

\[\leq(2m+1)\sqrt{48AdH\log\frac{48d}{\beta^{2}}\cdot\gamma}+(2m+1)\sqrt{AV_{ \max}^{2}\sqrt{96dH\log\frac{48d}{\beta^{2}}\log\frac{1}{\delta}}\cdot\beta}.\]

The result follows from some algebra. 

**Lemma C.6**.: _With probability at least \(1-\delta\), for each \(h\in[H]\) simultaneously, we have_

\[\sum_{t=1}^{T_{\mathsf{sim}}}\mathbb{E}^{\mathsf{sim},\pi_{\exp}^{h-1,t}}[( \widetilde{f}_{h}(s_{h},a_{h})-\mathcal{T}^{\mathsf{sim}}\widehat{f}_{h+1}(s_{h },a_{h}))^{2}\mid s_{h-1}^{t},a_{h-1}^{t}]\leq 256V_{\max}^{2}\log(4H| \mathcal{F}|/\delta).\]

Proof.: This follows from Lemma 3 of [57].

### Learning Full-Rank Policies

We consider running the MinEig algorithm (Algorithm 6) of [64] in sim. For a fixed \(h\), we instantiate the setting of Appendix C of [64] with \(\bm{\psi}(\bm{\tau})=\bm{\phi}(s_{h},a_{h})\bm{\phi}(s_{h},a_{h})^{\top}\), \(D=1\), and \(\mathbb{A}_{\mathcal{R}}\) the policy optimization oracle of Oracle 4.2 (and so \(C_{\mathcal{R}}=0\)), and set \(N=1\) for MinEig. We note that this algorithm is computationally efficient, given a policy optimization oracle.

**Lemma C.7**.: _For \(\mathcal{M}\leftarrow\mathcal{M}^{\text{sim}}\), Algorithm 5 will call Oracle 4.2 at most \(\widetilde{\mathcal{O}}(\frac{d}{\xi}\cdot\log\frac{1}{\delta}+\zeta^{-9}\cdot \log^{3/2}\frac{1}{\delta})\) times, and with probability at least \(1-\delta\), under Assumption 3 and if \(\zeta\leq\frac{\lambda_{\min}^{\star}}{4d}\), will return policies \(\Pi\) such that_

\[\lambda_{\min}\left(\frac{1}{|\Pi|}\sum_{\pi\in\Pi}\bm{\Lambda}_{ \pi,h}^{\text{s}}\right)\geq\frac{\lambda_{\min}^{\star}}{8d}\] (C.2)

_and each \(\pi\in\Pi\) plays actions randomly for \(h^{\prime}>h\)._

Proof.: We first argue that, if \(\zeta\leq\frac{\lambda_{\min}^{\star}}{4d}\), then with probability at least \(1-\delta\), (C.2) holds. Let \(\mathcal{E}\) denote the success event of each call to DynamicOED, and note that by our choice of \(\delta_{j}\), we have \(\mathbb{P}[\mathcal{E}]\geq 1-\delta/2\). Let \(j^{\star}\) denote the minimal value of \(j\) such that

\[\frac{\lambda_{\min}^{\star}}{4d}T_{j}\geq 12544d\log\frac{4+64T_{j}}{\delta} \quad\text{ and }\quad T_{j}\geq c\cdot\zeta^{-9}\cdot\log^{3/2}\frac{jT_{j}}{\delta}.\] (C.3)

By Lemma C.4 of [64] and if \(\zeta\leq\frac{\lambda_{\min}^{\star}}{4d}\), we then have that, on \(\mathcal{E}\), \(\lambda_{\min}(\bm{\Sigma}_{j^{\star}})\geq\frac{\lambda_{\min}^{\star}}{4d}T _{j^{\star}}\), which implies that the termination criteria of Algorithm 5 will be met. By Lemma C.5 of [64], it follows that with probability at least \(1-\delta/2\), we have \(\lambda_{\min}(\frac{1}{|\Pi_{j^{\star}}|}\sum_{\pi\in\Pi_{j}}\bm{\Lambda}_{ \pi,h}^{\text{s}})\geq\frac{\lambda_{\min}^{\star}}{8d}\) (since \(T_{j^{\star}}=|\Pi_{j^{\star}}|\)), the desired conclusion.

Assume that Algorithm 5 terminates for some \(j<j^{\star}\). This implies that \(\frac{\lambda_{\min}^{\star}}{4d}T_{j}<12544d\log\frac{4+64T_{j}}{\delta}\). However, in this case, we then have that

\[\lambda_{\min}(\bm{\Sigma}_{j})\geq 12544d\log\frac{4+64T_{j}}{\delta}\geq \frac{\lambda_{\min}^{\star}}{4d}T_{j}.\]

From Lemma C.5 of [64], it then follows that with probability at least \(1-\delta/2\), we have \(\lambda_{\min}(\frac{1}{|\Pi_{j}|}\sum_{\pi\in\Pi_{j}}\bm{\Lambda}_{\pi,h}^{ \text{s}})\geq\frac{\lambda_{\min}^{\star}}{8d}\).

It follows that, assuming \(T_{j}\) is large enough that (C.3) is met, and we are in the case when \(\zeta\leq\frac{\lambda_{\min}^{\star}}{4d}\) holds, then Algorithm 5 will terminate and return a set of policies satisfying (C.2), with probability at least \(1-\delta\). Note that \(T_{j}=\mathcal{O}(2^{j})\). Given that Algorithm 5 does not terminate until \(j=\mathcal{O}(\log_{2}(\frac{d}{\xi}\cdot\log\frac{1}{\delta}+\zeta^{-9}\cdot \log^{3/2}\frac{1}{\delta})\geq\mathcal{O}(\log_{2}(\frac{d^{2}}{\lambda_{\min }^{\star}}\cdot\log\frac{1}{\delta}+\zeta^{-9}\cdot\log^{3/2}\frac{1}{\delta}))\), we will have that \(T_{j}\) will be large enough that (C.3) is met, if \(\zeta\leq\frac{\lambda_{\min}^{\star}}{4d}\). The proof then follows since DynamicOED calls Oracle 4.2 at most \(T_{j}\) times at round \(j\), and the total sum of \(T_{j}\) is bounded as \(\widetilde{\mathcal{O}}(\frac{d}{\xi}\cdot\log\frac{1}{\delta}+\zeta^{-9} \cdot\log^{3/2}\frac{1}{\delta})\) by the maximum of \(j\), and since the actions chosen by \(\pi\in\Pi\) for \(h^{\prime}>h\) are irrelevant for the operation of DynamicOED, so they can be set to random.

Lower Bound Proofs

### Proof of Propositions 1, 3 and 4

**Construction.** Consider the following variation of the combination lock. We let the action space \(\mathcal{A}=\{1,2\}\), and assume there are two states, \(\mathcal{S}=\{s_{1},s_{2}\}\), and horizon \(H\). We start in state \(s_{1}\). The sim dynamics are given as:

\[\forall h<H-1:\quad P_{h}^{\mathsf{sim}}(s_{1}\mid s_{1},a_{1})=1,\quad P_{h}^{\mathsf{sim}}(s_{2}\mid,s_{1},a_{2})=1\] \[P_{H-1}^{\mathsf{sim}}(s_{1}\mid s_{1},a_{1})=P_{H-1}^{\mathsf{ sim}}(s_{2}\mid s_{1},a_{1})=P_{H-1}^{\mathsf{sim}}(s_{1}\mid s_{1},a_{2})=P_{H-1}^ {\mathsf{sim}}(s_{2}\mid s_{1},a_{2})=1/2\] \[\forall h\in[H]:\quad P_{h}^{\mathsf{sim}}(s_{2}\mid s_{2},a)=1,a \in\{a_{1},a_{2}\}.\]

We define two real instances, \(\mathcal{M}_{1}:=\mathcal{M}^{\mathsf{real},1}\) and \(\mathcal{M}_{2}:=\mathcal{M}^{\mathsf{real},2}\), where for both we have:

\[\forall h<H-1:\quad P_{h}^{\mathsf{real}}(s_{1}\mid s_{1},a_{1})= 1,\quad P_{h}^{\mathsf{real}}(s_{2}\mid,s_{1},a_{2})=1\] \[\forall h\in[H]:\quad P_{h}^{\mathsf{real}}(s_{2}\mid s_{2},a)=1,a\in\{a_{1},a_{2}\}\]

for \(\mathcal{M}_{1}\):

\[P_{H-1}^{\mathsf{real}}(s_{1}\mid s_{1},a_{1}) =1/2+\epsilon_{\mathrm{sim}},P_{H-1}^{\mathsf{real}}(s_{2}\mid s_ {1},a_{1})=1/2-\epsilon_{\mathrm{sim}},\] \[P_{H-1}^{\mathsf{real}}(s_{1}\mid s_{1},a_{2}) =1/2-\epsilon_{\mathrm{sim}},P_{H-1}^{\mathsf{real}}(s_{2}\mid s_ {1},a_{2})=1/2+\epsilon_{\mathrm{sim}},\]

and for \(\mathcal{M}_{2}\):

\[P_{H-1}^{\mathsf{real}}(s_{1}\mid s_{1},a_{1}) =1/2-\epsilon_{\mathrm{sim}},P_{H-1}^{\mathsf{real}}(s_{2}\mid s_ {1},a_{1})=1/2+\epsilon_{\mathrm{sim}},\] \[P_{H-1}^{\mathsf{real}}(s_{1}\mid s_{1},a_{2}) =1/2+\epsilon_{\mathrm{sim}},P_{H-1}^{\mathsf{real}}(s_{2}\mid s_ {1},a_{2})=1/2-\epsilon_{\mathrm{sim}}.\]

Note then that \(\mathcal{M}_{1},\mathcal{M}_{2}\), and sim only differ at step \(H-1\) in state \(s_{1}\). Furthermore, it is easy to see that both \(\mathcal{M}_{1}\) and \(\mathcal{M}_{2}\) satisfy Assumption 1 with misspecification \(\epsilon_{\mathrm{sim}}\). It is easy to see that Assumption 2 holds as well with \(d=4\) since this is a tabular MDP, and furthermore Assumption 3 also holds with \(\lambda_{\mathrm{min}}^{\star}=1/4\). We define the reward function as (note that this is deterministic, and the same for all instances):

\[\forall h\in[H]:\quad r_{h}(s_{1},a_{2})=1/2+\epsilon_{\mathrm{ sim}}(1/2-h/4H)\] \[r_{H}(s_{1},a)=1,a\in\{a_{1},a_{2}\},\]

and all other rewards are taken to be 0.

In sim, we see that the optimal policy always plays \(a_{2}\). In both \(\mathcal{M}_{1}\) and \(\mathcal{M}_{2}\), the optimal policy plays \(a_{1}\) for all \(h<H-1\), for \(\mathcal{M}_{1}\) plays \(a_{1}\) at \(H-1\), and for \(\mathcal{M}_{2}\) plays \(a_{2}\) at \(H-1\). Note that for both \(\mathcal{M}_{1}\) and \(\mathcal{M}_{2}\), we have \(V_{0}^{\star}=1/2+\epsilon_{\mathrm{sim}}\).

The most natural choice of \(\mathcal{F}\) would be the set of all tabular \(Q\)-value functions, however, this set is infinite, and would require a covering argument to incorporate. For simplicity, consider \(\mathcal{F}_{H}\) the set of functions mapping to \(\{0,1\}\), and \(\mathcal{F}_{h}\) the set of functions mapping to a finite set containing \(\{0,1/2-\epsilon_{\mathrm{sim}},1/2+\epsilon_{\mathrm{sim}}\}\cup\{1/2+ \epsilon_{\mathrm{sim}}(1/2-h^{\prime}/4H)\}_{h^{\prime}=0}^{H}\). Note that such a set satisfies Assumption 4 and we can construct it such that \(\log|\mathcal{F}|\leq\mathcal{O}(H)\).

Lower Bound for Direct Policy Transfer (Proposition 3).We consider direct sim2real transfer with randomized exploration. In particular, as noted, the optimal policy in sim always plays \(a_{2}\), so we consider the \(\zeta\)-greedy policy that at every state plays \(a_{2}\) with probability \(1-\zeta\), and plays \(\mathrm{unif}(\{a_{1},a_{2}\})\) with probability \(\zeta\). Denote this policy as \(\widetilde{\pi}\). We then wish to lower bound:

\[\inf_{\widetilde{\pi}}\sup_{i\in\{1,2\}}\mathbb{E}^{\mathcal{M}_{i}, \widetilde{\pi}}[V_{0}^{\mathcal{M}_{i},\star}-V_{0}^{\mathcal{M}_{i},\widetilde {\pi}}]\]

after running our procedure for \(T\) episodes. Note that on \(\mathcal{M}_{1}\), regardless of the actions \(\widehat{\pi}\) chooses in other states, we have

\[V_{0}^{\mathcal{M}_{1},\star}-V_{0}^{\mathcal{M}_{1},\widehat{\pi}}\geq\frac{ \epsilon_{\mathrm{sim}}}{2}(1-\widehat{\pi}_{H-1}(a_{1}\mid s_{1})),\]

[MISSING_PAGE_FAIL:38]

Lower Bound for \(\zeta\)-Greedy Without \(\mathsf{sim}\) (Proposition 1).In order to quantify the performance of a \(\zeta\)-greedy algorithm, we must specify how it chooses \(\widehat{f}\) when it has not yet observed any samples from a given \((s,a,h)\). Following the lead of Theorem 2 of [11], to avoid an overly optimistic or pessimistic initialization, we assume that the replay buffer is initialized with a single sample from each \((s,a,h)\). Note that the conclusion would hold with other initializations, however, e.g. initializing \(\widehat{f}_{h}(s,a)=0\) or randomly if we have no observations from \((s,a,h)\).

Assume that the observation from \((s_{1},a_{1},H-1)\) transitions to \(s_{2}\), which occurs with probability at least \(1/4\). In this case, we then have that, for each \(h\), \(\widehat{f}_{h}^{0}(s_{1},a_{2})\geq\widehat{f}_{h}^{0}(s_{1},a_{1})\). Thus, following the \(\zeta\)-greedy policy, we have that \(\pi_{h}^{0}(a_{1}\mid s_{1})\leq 1/2\). Denote this event on \(\mathcal{E}_{0}\). Furthermore, the only way we will have \(\widehat{f}_{h}^{0}(s_{1},a_{2})<\widehat{f}_{h}^{0}(s_{1},a_{1})\) is if we visit \((s_{1},a_{1},H-1)\) again and observe a transition to \(s_{1}\). For this to occur, however, we must play action \(a_{1}\,H-1\) times consecutively which, in this case, will occur with probability at most \(\max\{1/2,\zeta/2\}^{H-1}\leq 1/2^{H-1}\).

Following the argument in the direct policy transfer case, we have

\[\inf_{\widetilde{\pi}}\sup_{i\in\{1,2\}}\mathbb{E}^{\mathcal{M}_ {i},\widetilde{\pi}}[V_{0}^{\mathcal{M}_{i},\star}-V_{0}^{\mathcal{M}_{i}, \widetilde{\pi}}] \geq\inf_{\widetilde{\pi}}\sup_{i\in\{1,2\}}\frac{1}{4}\mathbb{E }^{\mathcal{M}_{i},\widetilde{\pi}}[V_{0}^{\mathcal{M}_{i},\star}-V_{0}^{ \mathcal{M}_{i},\widetilde{\pi}}\mid\mathcal{E}_{0}]\] \[\geq\frac{\epsilon_{\mathrm{sim}}}{16}\left(1-\sqrt{\frac{3}{10} \mathbb{E}^{\mathcal{M}_{1}}[T_{H-1}(s_{1})\mid\mathcal{E}_{0}]}\right)\]

where \(\mathbb{E}^{\mathcal{M}_{1}}[T_{H-1}(s_{1})\mid\mathcal{E}_{0}]\) is the expected number of visitations to \((s_{1},H-1)\) after \(T\) episodes of running the \(\zeta\)-greedy policy. We can rewrite

\[\mathbb{E}^{\mathcal{M}_{1}}[T_{H-1}(s_{1})\mid\mathcal{E}_{0}]=\sum_{t=1}^{ T}\mathbb{E}^{\mathcal{M}_{1}}[\mathbb{I}\{s_{H-1}=s_{1}\}\mid\mathcal{E}_{0}].\]

Let \(\mathcal{E}\) be the event that we have reached \((s_{1},H-1)\) in the first \(T\) rounds. Then,

\[\mathbb{E}^{\mathcal{M}_{1}}[\mathbb{I}\{s_{H-1}=s_{1}\}\mid \mathcal{E}_{0}] =\mathbb{E}^{\mathcal{M}_{1}}[\mathbb{I}\{s_{H-1}=s_{1}\}\mid \mathcal{E},\mathcal{E}_{0}]\mathbb{P}^{\mathcal{M}_{1}}[\mathcal{E}\mid \mathcal{E}_{0}]+\mathbb{E}^{\mathcal{M}_{1}}[\mathbb{I}\{s_{H-1}=s_{1}\}\mid \mathcal{E}^{c},\mathcal{E}_{0}]\mathbb{P}^{\mathcal{M}_{1}}[\mathcal{E}^{c} \mid\mathcal{E}_{0}]\] \[\leq\mathbb{P}^{\mathcal{M}_{1}}[\mathcal{E}\mid\mathcal{E}_{0}]+ \mathbb{E}^{\mathcal{M}_{1}}[\mathbb{I}\{s_{H-1}=s_{1}\}\mid\mathcal{E}^{c}, \mathcal{E}_{0}].\]

By what we have just argued, we have \(\mathbb{P}^{\mathcal{M}_{1}}[\mathcal{E}\mid\mathcal{E}_{0}]\leq T\cdot\frac{ 1}{2^{H-1}}\), and \(\mathbb{E}^{\mathcal{M}_{1}}[\mathbb{I}\{s_{H-1}=s_{1}\}\mid\mathcal{E}^{c}, \mathcal{E}_{0}]\leq\frac{1}{2^{H-1}}\). Thus, \(\mathbb{E}^{\mathcal{M}_{1}}[T_{H-1}(s_{1})\mid\mathcal{E}_{0}]\leq\frac{2T^ {2}}{2^{H-1}}\). It follows that,

\[\inf_{\widetilde{\pi}}\sup_{i\in\{1,2\}}\mathbb{E}^{\mathcal{M}_{i}, \widetilde{\pi}}[V_{0}^{\mathcal{M}_{i},\star}-V_{0}^{\mathcal{M}_{i}, \widetilde{\pi}}]\geq\frac{\epsilon_{\mathrm{sim}}}{16}\left(1-\sqrt{\frac{3}{ 10}\frac{2T^{2}}{2^{H-1}}}\right)\]

and we therefore have \(\inf_{\widetilde{\pi}}\sup_{i\in\{1,2\}}\mathbb{E}^{\mathcal{M}_{i}, \widetilde{\pi}}[V_{0}^{\mathcal{M}_{i},\star}-V_{0}^{\mathcal{M}_{i}, \widetilde{\pi}}]\geq\epsilon_{\mathrm{sim}}/32\) unless

\[T\geq\sqrt{\frac{5}{8}\cdot 2^{H-1}}.\]

Upper Bound for Exploration Policy Transfer (Proposition 4).To obtain an upper bound for Algorithm 1, we can apply Theorem 1, so long as

\[\epsilon_{\mathrm{sim}}\leq\frac{\lambda_{\min}^{\star}}{64dHA^{3}}.\]

Note that in our setting we have \(d=4\), \(A=2,\lambda_{\min}^{\star}=1/4\), so this condition reduces to \(\epsilon_{\mathrm{sim}}\leq\frac{1}{8192H}\). Taking \(\mathcal{F}\) to simply be the set of \(Q\)-functions defined above (so \(V_{\max}=H\)), Theorem 1 then gives that with probability at least \(1-\delta\), Algorithm 1 learns an \(\epsilon\)-optimal policy as long as \(T\geq c\cdot\frac{H^{17}}{\epsilon^{8}}\cdot\log\frac{H}{\delta}\).

### Proof of Proposition 5

We define three MDPs: \(\mathcal{M}^{\mathsf{sim}}\), and two possible real MDPs, \(\mathcal{M}_{1}:=\mathcal{M}^{\mathsf{real},1}\) and \(\mathcal{M}_{2}:=\mathcal{M}^{\mathsf{real},2}\). In all cases we have states \(\mathcal{S}=\{s_{1},s_{2}\}\), actions \(\mathcal{A}=\{a_{1},a_{2},a_{3},a_{4}\}\), and \(H=2\), and set the starting state to \(s_{1}\). We define

\[P_{1}^{\mathsf{sim}}(s_{1}\mid s_{1},a_{1})=1,\quad P_{1}^{\mathsf{sim}}(s_{1} \mid s_{1},a)=P_{1}^{\mathsf{sim}}(s_{2}\mid s_{1},a)=1/2,a\in\{a_{2},a_{3},a_{4}\}.\]For both \(\mathcal{M}_{1}\) and \(\mathcal{M}_{2}\), we have:

\[P_{1}^{\mathsf{real}}(s_{1}\mid s_{1},a_{1})=1,P_{1}^{\mathsf{real}}(s_{1}\mid s _{1},a_{4})=P_{1}^{\mathsf{real}}(s_{2}\mid s_{1},a_{4})=1/2\]

for \(\mathcal{M}_{1}\), we have

\[P_{1}^{\mathsf{real}}(s_{2}\mid s_{1},a_{2})=1+\epsilon_{\text{sim}},P_{1}^{ \mathsf{real}}(s_{1}\mid s_{1},a_{2})=1-\epsilon_{\text{sim}},P_{1}^{\mathsf{ real}}(s_{1}\mid s_{1},a_{3})=P_{1}^{\mathsf{real}}(s_{2}\mid s_{1},a_{3})=1/2\]

and for \(\mathcal{M}_{2}\),

\[P_{1}^{\mathsf{real}}(s_{2}\mid s_{1},a_{3})=1+\epsilon_{\text{sim}},P_{1}^{ \mathsf{real}}(s_{1}\mid s_{1},a_{3})=1-\epsilon_{\text{sim}},P_{1}^{\mathsf{ real}}(s_{1}\mid s_{1},a_{2})=P_{1}^{\mathsf{real}}(s_{2}\mid s_{1},a_{2})=1/2.\]

We take the reward to be 0 everywhere, except \(r_{2}(s_{2},a)=1\) for all \(a\).

Note that each of these can be represented as a linear MDP in \(d=2\) dimensions, so Assumption 2 holds. In particular, for \(\mathcal{M}^{\mathsf{sim}}\) we can take:

\[\bm{\phi}^{\sharp}(s,a_{1})=e_{1},\bm{\phi}^{\sharp}(s,a)=e_{2},a \in\{a_{2},a_{3},a_{4}\},s\in\mathcal{S},\] \[\bm{\mu}_{1}^{\sharp}(s_{1})=[1,1/2],\bm{\mu}_{1}^{\sharp}(s_{2}) =[0,1/2].\]

For \(\mathcal{M}_{1}\) we can instead take:

\[\bm{\phi}^{\prime}(s,a_{1})=e_{1},\bm{\phi}^{\prime}(s,a)=[1/2,1/ 2],a\in\{a_{3},a_{4}\},s\in\mathcal{S},\] \[\bm{\phi}^{\prime}(s,a_{2})=[1/2-\epsilon_{\text{sim}},1/2+ \epsilon_{\text{sim}}],s\in\mathcal{S},\] \[\bm{\mu}_{1}^{\star}(s_{1})=[1,0],\bm{\mu}_{1}^{\star}(s_{2})=[0,1].\]

\(\mathcal{M}_{2}\) follows similarly with the role of \(a_{2}\) and \(a_{3}\) flipped.

It is easy to see that Assumption 1 is met on this instance for both choices of \(\mathcal{M}^{\mathsf{real}}\). On \(\mathcal{M}^{\mathsf{sim}}\), the policy \(\pi_{\mathrm{exp}}\) which in every states plays action \(a_{1}\) with probability 1/2 and action \(a_{4}\) with probability 1/2 satisfies \(\lambda_{\min}(\mathbb{E}^{\mathsf{sim},\pi_{\mathrm{exp}}}[\bm{\phi}^{\sharp} (s_{h},a_{h})\bm{\phi}^{\sharp}(s_{h},a_{h})^{\top}])\geq 1/2\) (which shows that Assumption 3 holds).

Note, however, that \(\pi_{\mathrm{exp}}\) does not play action \(a_{2}\) or \(a_{3}\). As \(\mathcal{M}_{1}\) and \(\mathcal{M}_{2}\) differ only on \(a_{2}\) and \(a_{3}\), playing \(\pi_{\mathrm{exp}}\) will not allow for \(\mathcal{M}_{1}\) and \(\mathcal{M}_{2}\) to be distinguished. As \(a_{2}\) is the optimal action on \(\mathcal{M}_{1}\) and \(a_{3}\) the optimal action on \(\mathcal{M}_{2}\), it follows that playing \(\pi_{\mathrm{exp}}\) will not allow for the identification of the optimal policy on \(\mathcal{M}_{1}\) and \(\mathcal{M}_{2}\). This can be formalized identically to Appendix D.1, yielding the stated result.

## Appendix E Experimental Details

### Didactic Tabular Example

Consider the following variation of the combination lock. We let the action space \(\mathcal{A}=\{1,2\}\), and assume there are two states, \(\mathcal{S}=\{s_{1},s_{2}\}\), and horizon \(H\). We start in state \(s_{1}\). The \(\mathsf{sim}\) dynamics are given as:

\[\forall h<H-1:\quad P_{h}^{\mathsf{sim}}(s_{1}\mid s_{1},a_{1})=1,\quad P_{h}^{\mathsf{sim}}(s_{2}\mid,s_{1},a_{2})=1\] \[P_{H-1}^{\mathsf{sim}}(s_{1}\mid s_{1},a_{1})=1/4,P_{H-1}^{ \mathsf{sim}}(s_{2}\mid s_{1},a_{1})=3/4,P_{H-1}^{\mathsf{sim}}(s_{2}\mid s_{1 },a_{2})=1\] \[\forall h\in[H]:\quad P_{h}^{\mathsf{sim}}(s_{2}\mid s_{2},a)=1,a \in\{a_{1},a_{2}\},\]

and the \(\mathsf{real}\) dynamics are given as:

\[\forall h<H-1:\quad P_{h}^{\mathsf{real}}(s_{1}\mid s_{1},a_{1})=1,\quad P_{h}^{\mathsf{real}}(s_{2}\mid,s_{1},a_{2})=1\] \[P_{H-1}^{\mathsf{real}}(s_{1}\mid s_{1},a_{1})=3/4,P_{H-1}^{ \mathsf{real}}(s_{2}\mid s_{1},a_{1})=1/4,P_{H-1}^{\mathsf{real}}(s_{2}\mid s_{1 },a_{2})=1\] \[\forall h\in[H]:\quad P_{h}^{\mathsf{real}}(s_{2}\mid s_{2},a)=1,a\in\{a_{1},a_{2}\}.\]

Note that these only differ on \((s_{1},a_{1})\) at \(h=H-1\), and we have \(\epsilon_{\text{sim}}=1/2\). We define the reward function as (note that this is deterministic, and the same for both \(\mathsf{sim}\) and \(\mathsf{real}\)):

\[\forall h\in[H]:\quad r_{h}(s_{1},a_{2})=1/8-h/8H\] \[r_{H}(s_{1},a)=1/5,a\in\{a_{1},a_{2}\},\]

and all other rewards are taken to be 0.

The intuition for this example is as follows. In both sim and real, the only way the agent can get reward is to either end up in state \(s_{1}\) at step \(H\), or to take action \(a_{2}\) in state \(s_{1}\) at any point. In sim, the probability of ending up in state \(s_{1}\) at step \(H\), even if the optimal sequence of actions to do this is taken, is only \(1/4\), due to the final transition, and thus the average reward obtained by the policy which aims to end up in \(s_{1}\) is only 1/4. In contrast, if we take action \(a_{2}\) in \(s_{1}\), we will always collect reward of at least 3/8 (and the earlier we take action \(a_{2}\) the more reward we collect, up to 1/2). Thus, in sim the optimal thing to do in \(s_{1}\) is always to play \(a_{2}\). However, if we play \(a_{2}\) even once, we will transition out of \(s_{1}\) and never return, so there is no chance we will reach \(s_{1}\) at step \(H\).

In real, the transitions at the final step are flipped, so that now the probability of finishing in \(s_{1}\), if we take the optimal sequences of actions to do this, is 3/4, and the expected reward for this is then also 3/4. Since the reward for taking \(a_{2}\) in \(s_{1}\) does not change, and is bounded as 1/2, then in real the optimal policy is to seek to end up in \(s_{1}\) at the final step.

The challenge with ending up in \(s_{1}\) at the end is that it requires playing action \(a_{1}\) at every step. In this sense it is then a classic combination lock instance, and randomized exploration will fail, requiring \(\Omega(2^{H})\) episodes to reach the final state (since the probability of randomly taking \(a_{1}\) at every state decreases exponentially with the horizon). Similarly, if we transfer the optimal policy from sim to real, it will never take action \(a_{1}\), so will never reach \(s_{1}\) at the end, and if we transfer the optimal policy from sim with some random exploration, it will fail for the same reason random exploration from scratch fails.

However, note that we can transfer a policy from sim that is able to reach \(s_{1}\) at the second-to-last step with probability 1, i.e. the policy that takes action \(a_{1}\) at every step. Thus, if in sim we aim to learn exploration policies that can traverse the MDP, and we transfer these exploration policies, they will transfer, and will allow us to easily reach \(s_{1}\) at the final step, and quickly determine that it is indeed the optimal thing in real.

We provide additional experimental results on this instance in Appendix E.1.

### Practical Algorithm Details

The core of our work is to decouple the optimal policy training from exploration strategies in reinforcement learning fine-tuning. Specifically, we propose a framework that uses a set of diverse exploration policies to collect samples from the environment. These exploration policies are fixed while we run off policy RL updates on the collected samples to extract an optimal policy. Our theoretical derivation suggests that this decoupling can improve sample efficiency and overall learning performance.

Figure 5: Performance of Exploration Policy Transfer on instance from Section 5.2, varying number of states, actions, and horizon. We plot the number of samples required to achieve a reward of 0.35, which is approximately solving the task. All results are averaged across 20 trials. When increasing the number of states, we add additional 0-reward states (i.e. states given in yellow in Figure 2), and when adding additional actions we add additional low-reward actions (i.e. actions that have the same behavior as action \(a_{2}\) in Figure 2). We observe that increasing the number of states and horizon increases the number of samples needed, while increasing the number of actions does not substantially. We emphasize, however, that this is for a _particular_ example, and this scaling may not be the same for all examples—Theorem 1, however, gives an upper bound on _all_ examples.

Our framework is complementary to (a) RL works on diversity or exploration that generate diverse policies and (b) off policy RL algorithms that optimize for policies. One can plug in (a) to extract a set of exploration policy from a simulator and use them for data collect in the real world but use (b) to optimize for the final policy. The design choice to use simulator to extract a set of exploration policies where each policy is not necessarily optimizing for the task at hand marks our distinction from previous works in (a) and (b).

We provide a practical instantiation of our framework using an approach inspired by One Solution is Not All You Need (OS) [30] to extract exploration policies and Soft Actor Critic (SAC) [30] to optimize for the optimal policy. We details the instantiation in Algorithm 6. OS trains a set of policy to optimize not only the task reward but also a discriminator reward where the discriminator encourages each policy to achieve different state. Unlike OS which carefully balances the task and exploration rewards to ensure all policies have a chance at solving the desired task, we emphasize only on having diverse policies. With a known sim2real gap, we posit that some sub-optimal policies that are not solving the task in the simulator is actually helpful for exploration in the real world, which allows us to simplify the balance between task and exploration. We uses standard off-shelf SAC update to optimize for the policy.

```
1:Input: Simulator \(\mathcal{M}^{\mathsf{sim}}\), real environment \(\mathcal{M}^{\mathsf{real}}\), simulator training budget \(N\), exploration reward balancing \(\alpha\), reward threshold \(\epsilon\), exploration set size \(n\).
2:Pre-train Exploration Policies in \(\mathcal{M}^{\mathsf{sim}}\):
3:Initialize \(\Pi_{exp}=\{\pi_{\theta}(\cdot|z)|z\in\{1\dots n\}\}\)
4:Initialize discriminator \(D_{\phi}\)
5:for\(i=1\) to \(N\)do\(\triangleright\) Learn diverse exploration policies
6: Sample latent \(z\sim\text{unif}(1,n)\) and initial state \(s_{0}\).
7:for\(t=1\) to max_steps_per_episode do
8: Sample action \(a_{t}\sim\pi_{\theta}(a_{t}|s_{t},z)\).
9: Step environment: \(s_{t+1}\sim p(s_{t+1}|s_{t},a_{t})\).
10: Compute discriminator score \(d_{t}=D(s_{t+1},z)\)
11: Compute exploration reward \(r_{e}(s_{t+1},z)=\log\frac{\exp(d_{t})}{\sum_{s^{\prime}}\exp(d(s_{t+1},z^{ \prime}))}\).
12:if\(R_{\pi}\geq\epsilon\)then
13: Compute reward \(r_{t}=r(s_{t},a_{t})+\alpha\cdot r_{e}(s_{t+1},z)\).
14:else
15: Compute reward \(r_{t}=r(s_{t},a_{t})\)
16: Let \(\mathcal{D}\leftarrow\mathcal{D}\cup\{(s_{t},a_{t},r_{t},s_{t+1},z)\}\).
17: Update \(\pi_{\theta}\) to maximize \(J_{\pi}\) with SAC.
18: Update \(\phi\) to maximize \(J_{u}\), \(\phi\leftarrow\phi+\eta\nabla_{\phi}\mathbb{E}_{s,z\sim\mathcal{D}}\left[\log D _{\phi}(s,z)\right]\)
19: Compute \(R_{\pi}=\sum_{t}r_{t}\)
20:Explore in \(\mathcal{M}^{\mathsf{real}}\) and Estimate Optimal Policy :
21:Initialize SAC agent (either from scratch or to weights of optimal sim policy).
22:while not converged do
23: Sample \(z\sim\text{unif}(1,n)\), play \(\pi_{\theta}(\cdot\mid z)\) in \(\mathcal{M}^{\mathsf{real}}\), add data to replay buffer of SAC.
24: Roll out SAC policy for one step, perform standard SAC update. ```

**Algorithm 6** sim2real transfer using OS for exploration and SAC for optimization

### TychoEnv \(\mathsf{sim2sim}\) Experiment Details

For the TychoEnv experiment we run a variant of Algorithm 6. We set \(n=20\), and set the reward to \(r_{t}^{i}=(1-\alpha_{i})r+\alpha_{i}r_{e}\) where we vary \(\alpha_{i}\) from 0 to 0.5. While we use a sparse reward in \(\mathcal{M}^{\mathsf{real}}\), to speed up training in \(\mathcal{M}^{\mathsf{sim}}\) we use a dense reward that penalizes the agent for its distance to the target. We train in \(\mathcal{M}^{\mathsf{sim}}\) for \(\mathcal{M}\) steps to obtain exploration policies. Rather than simply transferring the converged version of the exploration policies trained in \(\mathcal{M}^{\mathsf{sim}}\), we found it most effective to save the weights of the policies throughout training, and transfer all of these policies. As the majority of these policies do not collect any reward in \(\mathcal{M}^{\mathsf{sim}}\), we run an initial filtering stage where we identify several policies from this set that find reward (this can be seen in Figure 4 with the initial region of 0 reward). We then run SAC in \(\mathcal{M}^{\mathsf{real}}\), initialized from scratch, feeding in the data collected by these refined exploration policies into the replay buffer. We found it most effective to only inject data from the exploration policies in the replay buffer on episodes where they observe reward. We run vanilla SAC with UTD = 3 and target entropy of -3. We rely on the implementation of SAC from stable-baselines3 [51].

For direct policy transfer, we train a policy to convergence in \(\mathcal{M}^{\text{sim}}\) that solves the task (using SAC), and then transfer this single policy, otherwise following the same procedure as above.

In \(\mathcal{M}^{\text{real}}\), our reward is chosen to have a value of 50 if the end effector makes contact with the ball, and otherwise 0. If the robot successfully makes contact with the ball the episode terminates. To generate a realistic transfer environment, we change the control frequency (doubling it in \(\mathcal{M}^{\text{real}}\)) and the action bounds.

For both methods, we run the \(\mathcal{M}^{\text{sim}}\) training procedure 4 times, and then with each of these run it in \(\mathcal{M}^{\text{real}}\) twice. Error bars in our plot denote one standard error.

All experiments were run on two Nvidia V100 GPUs, and 32 Intel(R) Xeon(R) CPU E5-2620 v4 @ 2.10GHz CPUs. Additional hyperparameters in given in Table 1.

We provide results on several additional baselines for the Tycho setup in Figure 6.

\begin{table}
\begin{tabular}{c|c c} \hline Hyperarameter & & Value \\ \hline \hline reward balance \(\alpha\) (OS) & \{\(\frac{1}{38}i-\frac{1}{38}\) & : \(i=1,2,\dots,20\}\) \\ learning rate & & 0.0003 \\ Q update magnitude \(\tau\) & & 0.005 \\ discount \(\gamma\) & & 0.99 \\ batch size & & 2048 \\ steps per episode & & 45 \\ replay buffer size & & \(5\times 10^{6}\) \\ training steps \(N\) (in \(\mathcal{M}^{\text{real}}\)) & \(7\times 10^{7}\) \\ \end{tabular}
\end{table}
Table 1: Hyperparameters used in Tycho training and finetuning

Figure 6: Additional results on Tycho, including baselines training from scratch in \(\mathcal{M}^{\text{real}}\), and training exploration policies in \(\mathcal{M}^{\text{sim}}\) with reward as stated above but with \(\alpha_{i}=1\) (which is equivalent to simply training exploration policies with DIAYN [14]). As can be seen, while training from scratch in \(\mathcal{M}^{\text{real}}\) is able to learn, it learns at a much slower rate than exploration policy transfer, and achieves a much lower final value. Furthermore, training the exploration policies to maximize a mix of the task and diversity reward yields a substantial gain over simply training them to be diverse.

Figure 7: Results on Franka sim2real experiment, comparing to training from scratch in real.

### sim2sim Transfer on Franka Emika Panda Robot Arm

We next turn to the Franka Emika Panda robot arm [17], for which we use a realistic custom simulator built using the MuJoCo simulation engine [61]. We consider a hammering task, where the Franka arm holds a hammer, and the goal is to hammer a nail into the board (see Figure 8). Success is obtained when the nail is fully inserted. We simulate sim2real transfer by setting \(\mathcal{M}^{\text{real}}\) to be a version of the simulator with nail location and stiffness significantly beyond the range seen during training in \(\mathcal{M}^{\text{sim}}\).

We compare exploration policy transfer with direct sim2real policy transfer. Unlike the Tycho experiment, where we trained policies from scratch in \(\mathcal{M}^{\text{real}}\) and simply used the policies trained in \(\mathcal{M}^{\text{sim}}\) to explore, here we initialize the task policy in \(\mathcal{M}^{\text{real}}\) to \(\pi^{\text{sim},*}\), which we then finetune on the data collected in \(\mathcal{M}^{\text{real}}\) by running SAC. For direct sim2real transfer, we collect data in \(\mathcal{M}^{\text{real}}\) by simply rolling out \(\pi^{\text{sim},*}\) and feeding this data to the replay buffer of SAC. For exploration policy transfer, we train an ensemble of \(n=10\) exploration policies in \(\mathcal{M}^{\text{sim}}\) and run these policies in \(\mathcal{M}^{\text{real}}\), again feeding this data to the replay buffer of SAC to finetune \(\pi^{\text{sim},*}\). During training in \(\mathcal{M}^{\text{sim}}\), we utilize domain randomization for both methods, randomizing nail stiffness, location, radius, mass, board size, and damping.

The results of this experiment are shown in Figure 9. We see that, while direct policy transfer is able to learn, it learns at a significantly slower rate than our exploration policy transfer approach, and achieves a much smaller final success rate.

### Franka sim2real Experiment Details

We use Algorithm 6 to train a policy on the Franka robot with \(n=15\).

The reward of the pushing task is given by:

\[r(s_{t},a_{t})=-\|\mathbf{p}_{\text{ee}}-\mathbf{p}_{\text{obj}}\|^{2}-\| \mathbf{p}_{\text{obj}}-\mathbf{p}_{\text{goal}}\|^{2}+\mathbb{I}_{\mathbf{p} _{\text{obj}}-\mathbf{p}_{\text{goal}}\leq 0.025}-\mathbb{I}_{\mathbf{p}_{ \text{obj}}\text{offtable}}\] (E.1)

where \(\mathbf{p}_{\text{goal}}\) is the desired position of the puck by the edge of the surface.

The network architecture of the actor and critic networks are identical, consisting of a 2-layer MLP, each of size 256 and ReLU activations.

We use stable-baselines3 [51] for our SAC implementation, using all of their default hyperparameters. The implemention of OS is built on top of this SAC implementation. Values of hyperparameters

\begin{table}
\begin{tabular}{c|c} \hline Hyperarameter & Value \\ \hline \hline reward balance \(\alpha\) (OS) & 0.5 \\ reward threshold \(\epsilon\) (OS) & -16 \\ learning rate & 0.0003 \\ Q update magnitude \(\tau\) & 0.005 \\ discount \(\gamma\) & 0.99 \\ batch size & 256 \\ steps per episode & 45 \\ replay buffer size & \(1\times 10^{6}\) \\ training steps \(N\) & \(2\times 10^{7}\) \\ \end{tabular}
\end{table}
Table 2: Hyperparameters used in Franka training and finetuningare shown in Table 2. Gaussian noise with mean \(0\) and standard deviation \(0.005\) meters is added in simulation to the position of the puck. Hyperparameters are identical between exploration policy transfer and direct transfer methods.

For finetuning in real, we start off by sampling exclusively from the buffer used during simulation. Then, as finetuning proceeds, we gradually start taking more samples from the real buffer, with the proportion of samples taken from sim equal to \(1-s/3000\), where \(s\) is the current number of steps. After \(3000\) steps, all samples are taken from the real buffer.

Experiments were run using a standard Nvidia RTX 4090 GPU. Training in simulation takes about 3 hours, while finetuning was ran for about 90 minutes.

In Figure 7, we provide results on this setup running the additional baseline of training a policy from scratch in real. As can be seen, this is significantly worse than either transfer method.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: We validate all our claims with theoretical results and experiments. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: Please see Discussion section. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes]Justification: All results are precisely proved in the supplemental, and all assumptions clearly stated. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: To the extent possible, given that we are working with real-world systems, we have described our setup and implementation details. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code**Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [No] Justification: We have not currently released our code but hope to in the future. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We have stated all parameters and algorithm details to the best of our knowledge (please see Experimental Details section in supplemental). Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: For all our experimental results, we provide error bars corresponding to 1 standard error. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).

* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: Please see Experimental Details section in appendix. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: This paper does not violate any ethical guidelines. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: This is work is related to the advancement of our fundamental understanding of machine learning. As such, we do not believe there are any direct societal impacts from this work. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.

* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: We are not releasing high-risk models. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We have cited the creators of the code used in this project. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.

* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: This work does not involve human subjects research. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: This work does not involve human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. * We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. * For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.