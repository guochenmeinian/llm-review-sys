# Approximation-Aware Bayesian Optimization

 Natalie Maus

University of Pennsylvania

nmaus@seas.upenn.edu

&Kyurae Kim

University of Pennsylvania

**Geoff Pleiss**

University of British Columbia

Vector Institute

&David Eriksson

Meta&John P. Cunningham

Columbia University&Jacob R. Gardner

University of Pennsylvania

###### Abstract

High-dimensional Bayesian optimization (BO) tasks such as molecular design often require >10,000 function evaluations before obtaining meaningful results. While methods like sparse variational Gaussian processes (SVGPs) reduce computational requirements in these settings, the underlying approximations result in suboptimal data acquisitions that slow the progress of optimization. In this paper we modify SVGPs to better align with the goals of BO: targeting informed data acquisition rather than global posterior fidelity. Using the framework of utility-calibrated variational inference, we unify GP approximation and data acquisition into a joint optimization problem, thereby ensuring optimal decisions under a limited computational budget. Our approach can be used with any decision-theoretic acquisition function and is readily compatible with trust region methods like TuRBO. We derive efficient joint objectives for the expected improvement and knowledge gradient acquisition functions for standard and batch BO. Our approach outperforms standard SVGPs on high-dimensional benchmark tasks in control and molecular design.

## 1 Introduction

Bayesian optimization (BO; Frazier, 2018; Garnett, 2023; Jones et al., 1998; Mockus, 1982; Shahriari et al., 2015) casts optimization as a sequential decision-making problem. Many recent successes of BO have involved complex and high-dimensional problems. In contrast to "classic" low-dimensional BO problems--where expensive black-box function evaluations far exceeded computational costs--these modern problems necessitate tens of thousands of function evaluations, and it is often the complexity and dimensionality of the search space that makes optimization challenging, rather than a limited evaluation budget (Eriksson et al., 2019; Griffiths and Hernandez-Lobato, 2020; Maus et al., 2022, 2023; Stanton et al., 2022). Because of these scenarios, BO is entering a regime where computational costs are becoming a primary bottleneck (Maddox et al., 2021; Maus et al., 2023; Moss et al., 2023; Vakili et al., 2021), as the Gaussian process (GP; Rasmussen and Williams, 2005) surrogate models that underpin most of Bayesian optimization scale cubically with the number of observations.

In this new regime, we require scalable GP approximations, an area that has made tremendous progress over the last decade. In particular, sparse variational Gaussian processes (SVGP; Hensman et al., 2013; Quinonero-Candela and Rasmussen, 2005; Titsias, 2009) have seen an increase in use (Griffiths and Hernandez-Lobato, 2020; Maddox et al., 2021; Maus et al., 2022, 2023; Stanton et al., 2022; Tripp et al., 2020; Vakili et al., 2021), but many challenges remain to effectively deploy SVGPs for large-budget BO. In particular, the standard SVGP training objective is not aligned with the goals of black-box optimization. SVGPs construct an inducing point approximation that maximizes the standard variational evidence lower bound (ELBO; Jordan et al., 1999), yielding a posterior approximation \(q^{*}(f)\) that models all observed data (Matthews et al., 2016; Moss et al., 2023). However, the optimal posterior approximation \(q^{*}\) is suboptimal for the decision-making tasks involvedin BO (Lacoste-Julien et al., 2011). In BO, we do not care about posterior fidelity at the majority of prior observations; rather, we only care about the fidelity of downstream functions involving the posterior, such as the expected utility. To illustrate this point intuitively, consider using the common expected improvement (EI; Jones et al., 1998) acquisition function for selecting new observations. Maximizing the ELBO might result in a posterior approximation that maintains fidelity for training examples in regions of virtually zero EI, thus wasting "approximation budget."

To solve this problem, we focus on the deep connections between statistical decision theory (Robert, 2001; Wasserman, 2013, SS12) and Bayesian optimization (Garnett, 2023, SS6-7), where acquisition maximization can be viewed as maximizing posterior-expected utility. Following this perspective, we leverage the utility-calibrated approximate inference framework (Jaiswal et al., 2020, 2023; Lacoste-Julien et al., 2011), and solve the aforementioned problem through a variational bound (Blei et al., 2017; Jordan et al., 1999)-the (log) **expected utility lower bound (EULBO)**--a joint function of the decision (the BO query) and the posterior approximation (the SVGP). When optimized jointly, the EULBO automatically yields the approximately optimal decision through the minorize-maximize principle (Lange, 2016). The EULBO is reminiscent of the standard variational ELBO (Jordan et al., 1999), and can indeed be viewed as a standard ELBO for a generalized Bayesian inference problem (Bissiri et al., 2016; Knoblauch et al., 2022), where we seek to approximate the _utility-weighted_ posterior. This work represents the first application of utility-calibrated approximate inference towards BO despite its inherent connection with utility maximization.

The benefits of our proposed approach are visualized in Fig. 1. Furthermore, it can be applied to acquisition function that admits a decision-theoretic interpretation, which includes the popular expected improvement (EI; Jones et al., 1998) and knowledge gradient (KG; Wu et al., 2017) acquisition functions, and is trivially compatible with local optimization techniques like TuRBO (Eriksson et al., 2019) for high-dimensional problems. We demonstrate that our joint SVGP/acquisition optimization approach yields significant improvements across numerous Bayesian optimization benchmarks. As an added benefit, our approach can simplify the implementation and reduce the computational burden of complex (decision-theoretic) acquisition functions like KG. We demonstrate a novel algorithm derived from our joint optimization approach for computing and optimizing the KG that expands recent work on one-shot KG (Balandat et al., 2020) and variational GP posterior refinement (Maddox et al., 2021).

Overall, our contributions are summarized as follows:

* We propose utility-calibrated variational inference of SVGPs in the context of large-budget BO.
* We study this framework in two special cases using the utility functions of two common acquisition functions: EI and KG. For each, we derive tractable EULBO expressions that can be optimized.
* For KG, we demonstrate that the computation of the EULBO takes only negligible additional work over computing the standard ELBO by leveraging an online variational update. Thus, as a byproduct of optimizing the EULBO, optimizing KG becomes comparable to the cost of the EI.
* We extend this framework to be capable of running in batch mode, by introducing q-EULBO analogs of q-KG and q-EI as commonly used in practice (Wilson et al., 2018).
* We demonstrate the effectiveness of our proposed method against standard SVGPs trained with ELBO maximization on high-dimensional benchmark tasks in control and molecular design, where the dimensionality and evaluation budget go up to 256 and 80k, respectively.

## 2 Background

Noisy Black-Box Optimization.Noisy black-box optimization refers to problems of the form: \(\text{maximize}_{\bm{x}\in\mathcal{X}}\,F\left(\bm{x}\right),\) where \(\mathcal{X}\subset\mathbb{R}^{d}\) is some compact domain, \(F:\mathcal{X}\rightarrow\mathcal{Y}\) is some objective function, and we assume that only zeroth-order information of \(F\) is available. More formally, for some \(i\in\mathbb{N}_{>0}\), we assume that observations of the objective function \(\left(\bm{x}_{i},\bm{y}_{i}=\widehat{F}\left(\bm{x}_{i}\right)\right)\) have been corrupted by independently and identically distributed (i.i.d.) Gaussian noise \(\widehat{F}\left(\bm{x}_{i}\right)\triangleq F(\bm{x}_{i})+\epsilon\), where \(\epsilon\sim\mathcal{N}(0,\sigma_{\text{n}}^{2})\). The noise variance \(\sigma_{\text{n}}^{2}\) is also unknown.

Bayesian optimization.Bayesian Optimization (BO) is and iterative approach to noisy black-box optimization that iterates the following steps: \(\spadesuit\) At each step \(t\geq 0\), we use a set of observations \(\mathcal{D}_{t}=\left\{\left(\bm{x}_{i},\bm{y}_{i}=\widehat{F}(\bm{x}_{i}) \right)\right\}_{i=1}^{n_{t}}\) of \(\widehat{F}\) to fit a surrogate supervised model \(f\in\mathcal{F}\). Typically, \(\mathcal{F}\) is taken to be the sample space of a Gaussian process such that the function-valued posterior distribution \(\pi\left(f\mid\mathcal{D}\right)\) forms a distribution over surrogate models at step \(t\).

The posterior is then used to form a decision problem where we choose which point we should evaluate next, \(\bm{x}_{t+1}=\delta_{\alpha}\left(\mathcal{D}_{t}\right)\), by maximizing an acquisition function \(\alpha:\mathcal{X}\rightarrow\mathbb{R}\) as

\[\delta_{\alpha}\left(\mathcal{D}_{t}\right)\triangleq\operatorname*{arg\,max} _{\bm{x}\in\mathcal{X}}\;\;\alpha\left(\bm{x};\mathcal{D}_{t}\right).\] (1)

After selecting \(\bm{x}_{t+1}\), \(\widehat{F}\) is evaluated to obtain the new datapoint \((\bm{x}_{t+1},y_{t+1}=\widehat{F}\left(\bm{x}_{t+1}\right))\). This is then added to the dataset, forming \(\mathcal{D}_{t+1}=\mathcal{D}_{t}\cup\left(\bm{x}_{t+1},y_{t+1}\right)\) to be used in the next iteration.

Utility-Based Acquisition Functions.Many commonly used acquisition functions, including EI and KG, can be expressed as posterior-expected utility functions

\[\alpha\left(\bm{x};\mathcal{D}\right)\triangleq\int u\left(\bm{x},f;\mathcal{ D}\right)\pi\left(f\mid\mathcal{D}\right)\mathrm{d}f,\] (2)

where \(\bm{u}\left(\bm{x},f;\mathcal{D}\right):\mathcal{X}\times\mathcal{F}\to \mathbb{R}\) is some utility function associated with \(\alpha\)(Garnett, 2023, SS6-7). In statistical decision theory, posterior-expected utility maximization policies such as \(\delta_{\alpha}\) are known as _Bayes policies_. These are important because, for a given utility function, they attain certain notions of statistical optimality such as Bayes optimality and admissibility (Robert, 2001, SS2.4; Wasserman, 2013, SS12). However, this only holds true if we can exactly compute Eq. (2) over the posterior. Once approximate inference is involved, making optimal Bayes decisions becomes challenging.

Sparse Variational Gaussian Processes.While the \(\mathcal{O}(n^{3})\) complexity of exact Gaussian process model selection and inference is not necessarily a roadblock in the traditional regression setting with 10,000-50,000 training examples, BO amplifies the scalability challenge by requiring us to sequentially train or update _many_ large scale GPs as we iteratively acquire more data.

To address this, sparse variational GPs (SVGP; Hensman et al., 2013; Titsias, 2009) have become commonly used in high-throughput Bayesian optimization. SVGPs modify the original GP prior from \(p(f)\) to \(p(f\mid\bm{u})p(\bm{u})\), where we assume the latent function \(f\) is "induced" by a finite set of _inducing values_\(\bm{u}=(\bm{u}_{1},...,\bm{u}_{m})\in\mathbb{R}^{m}\) located at _inducing points_\(\bm{z}_{l}\in\mathcal{X}\) for \(i=1,...,m\). Inference is done through variational inference (Blei et al., 2017; Jordan et al., 1999), where the posterior of the inducing points is approximated using \(q_{\lambda}\left(\bm{u}\right)=\mathcal{N}\left(\bm{u};\bm{\lambda}=\left(\bm{ m},\bm{S}\right)\right)\) and that of the latent functions with \(q\left(f\mid\bm{u}\right)=p\left(f\mid\bm{u}\right)\). Here, the variational parameters \(\bm{m}\) and \(\bm{S}\) are defined as the learned mean and covariance of the variational distribution \(q_{\lambda}\left(\bm{u}\right)\). It is standard practice to define \(\bm{\lambda}=\left(\bm{m},\bm{S}\right)\) so that \(\bm{\lambda}\) can be used as shorthand to represent all of the trainable variational parameters. As is typical in the BO literature, we use the subscript \(\bm{\lambda}\in\Lambda\) to denote that the distribution denoted as \(q\) contains trainable parameters in \(\bm{\lambda}\).

For a positive definite kernel function \(k:\mathcal{X}\times\mathcal{X}\rightarrow\mathbb{R}_{>0}\), the resulting ELBO objective, which can be computed in a closed form (Hensman et al., 2013), is then

\[\mathcal{L}_{\text{ELBO}}\left(\bm{\lambda};\mathcal{D}_{t}\right)\triangleq \mathbb{E}_{q_{\lambda}\left(f\right)}\left[\sum_{i=1}^{n_{t}}\log\ell\left(y_ {i}\mid f\left(\bm{x}_{i}\right)\right)\right]-\operatorname*{D}_{\text{KL}} \left(q_{\lambda}\left(\bm{u}\right),p\left(\bm{u}\right)\right),\] (3)

where \(\ell(y_{i}\mid f\left(\bm{x}_{i}\right))=\mathcal{N}\left(y_{i}\mid f\left( \bm{x}_{i}\right),\sigma_{\epsilon}\right)\) is a Gaussian likelihood. The marginal variational approximation can be computed as

\[q_{\lambda}(f)=\int q_{\lambda}(f,\bm{u})\,\mathrm{d}\bm{u}=\int p\left(f\mid \bm{u}\right)\,q_{\lambda}(\bm{u})\,\mathrm{d}\bm{u}\]

Figure 1: **(Left.) Fitting an SVGP model with only \(m=4\) inducing points sacrifices modeling areas of high EI (few data points at right) because the ELBO focuses only on global data approximation (left data) and is ignorant of the downstream decision making task. (Middle.) Because of this, (normalized) EI with the SVGP model peaks in an incorrect location relative to the exact posterior. (Right.) Updating the GP fit and selecting a candidate jointly using the EULBO (our method) results in candidate selection much closer to the exact model.**

such that the point-wise function evaluation on some \(\bm{x}\in\mathcal{X}\) is

\[q_{\lambda}(f(\bm{x}))=\mathcal{N}\left(f(\bm{x});\quad\mu_{f}(\bm{x})\triangleq \bm{K}_{\bm{x}\bm{Z}}\bm{K}_{\bm{Z}\bm{Z}}^{-1}\bm{m},\quad\sigma_{f}^{2}\left( \bm{x}\right)\triangleq\widetilde{k}_{\bm{x}\bm{x}}+\bm{k}_{\bm{x}\bm{Z}}^{ \top}\bm{K}_{\bm{Z}\bm{Z}}^{-1}\bm{S}\bm{K}_{\bm{Z}\bm{Z}}^{-1}\bm{k}_{\bm{Z} \bm{x}}\right),\] (4)

with \(\widetilde{k}_{\bm{x}\bm{x}}\triangleq k\left(\bm{x},\bm{x}\right)-\bm{k}_{ \bm{x}\bm{Z}}\bm{K}_{\bm{Z}\bm{Z}}^{-1}\bm{k}_{\bm{Z}\bm{x}}^{\top}\), the vector \(\bm{k}_{\bm{Z}\bm{x}}\in\mathbb{R}^{m}\) is formed as \([\bm{k}_{\bm{Z}\bm{x}}]_{i}=k\left(\bm{z}_{i},\bm{x}\right)\), and the matrix \(\bm{K}_{\bm{Z}\bm{Z}}\in\mathbb{R}^{m\times m}\) is formed as \([\bm{K}_{\bm{Z}\bm{z}}]_{i}=k(\bm{z}_{i},\bm{z}_{j})\). Additionally, the GP likelihood and kernel contain hyperparameters, which we denote as \(\theta\in\Theta\), and we collectively denote the set of inducing point locations as \(\bm{Z}=(\bm{z}_{1},...,\bm{z}_{m})\in\mathcal{X}^{m}\). We therefore denote the ELBO as \(\mathcal{L}_{\text{ELBO}}\left(\bm{\lambda},\bm{Z},\theta;\mathcal{D}_{t}\right)\).

## 3 Approximation-Aware Bayesian Optimization

When SVGPs are used in conjunction with BO (Maddox et al., 2021; Moss et al., 2023) at iteration \(t\geq 0\), acquisition functions of the form of Eq.2 are naively approximated as

\[\alpha\left(\bm{x};\mathcal{D}\right)\approx\int u\left(\bm{x},f;\mathcal{D}_ {t}\right)q_{\bm{\lambda}}\left(f\right)\mathrm{d}f,\]

where \(q_{\bm{\lambda}}(f)\) is the approximate SVGP posterior given by Eq.4. The acquisition policy implied by this approximation contains two separate optimization problems:

\[\bm{x}_{t+1}=\operatorname*{arg\,max}_{\bm{x}\in\mathcal{X}}\int u\left(\bm{ x},f;\mathcal{D}_{t}\right)q_{\bm{\lambda}_{\text{ELBO}}^{*}}\left(f \right)\mathrm{d}f\quad\text{and}\quad\bm{\lambda}_{\text{ELBO}}^{*}= \operatorname*{arg\,max}_{\bm{\lambda}\in\Lambda}\mathcal{L}_{\text{ELBO}} \left(\bm{\lambda};\mathcal{D}_{t}\right).\] (5)

Treating these optimization problems separately creates an artificial bottleneck that results in suboptimal data acquisition decisions. Intuitively, \(\bm{\lambda}_{\text{ELBO}}^{*}\) is chosen to faithfully model all observed data (Matthews et al., 2016; Moss et al., 2023), without regard for how the resulting model performs at selecting the next function evaluation in the BO loop. For an illustration of this, see Figure1. Instead, we propose a modification to SVGPs that couples the posterior approximation and data acquisition through a joint problem of the form:

\[\left(\bm{x}_{t+1},\,\bm{\lambda}^{*}\,\right)=\operatorname*{arg\,max}_{\bm {\lambda}\in\Lambda,\bm{x}\in\mathcal{X}}\mathcal{L}_{\text{EULBO}}\left(\bm{ \lambda},\bm{x};\mathcal{D}_{t}\right).\] (6)

This results in \(\bm{x}_{t+1}\) directly approximating a solution to Eq.2, where the **expected utility lowerbound** (EULBO) is an ELBO-like objective function derived below.

### Expected Utility Lower-Bound

Consider an acquisition function of the form of Eq.2, where the utility \(\bm{u}\,:\,\mathcal{X}\times\mathcal{F}\to\mathbb{R}_{>0}\) is strictly positive. We can derive a similar variational formulation of the acquisition function maximization problem following Lacoste-Julien et al. (2011). That is, given any distribution \(q_{\bm{\lambda}}\) indexed by \(\bm{\lambda}\in\Lambda\) and considering the SVGP prior augmentation \(p(f)\to p(f\mid\bm{u})p(\bm{u})\), the acquisition function can be lower-bounded through Jensen's inequality as

\[\log\alpha\left(\bm{x};\mathcal{D}_{t}\right) =\log\int u\left(\bm{x},f;\mathcal{D}_{t}\right)\pi\left(f\mid \mathcal{D}_{t}\right)\mathrm{d}f\] \[=\log\int u\left(\bm{x},f;\mathcal{D}_{t}\right)\pi\left(f,\bm{u }\mid\mathcal{D}_{t}\right)\frac{q_{\bm{\lambda}}\left(f,\bm{u}\right)}{q_{ \bm{\lambda}}\left(f,\bm{u}\right)}\mathrm{d}f\,\mathrm{d}\bm{u}\] \[=\log\int u\left(\bm{x},f;\mathcal{D}_{t}\right)\,\ell\left( \mathcal{D}_{t}\mid f\right)p\left(f\mid\bm{u}\right)p\left(\bm{u}\right)\frac{ q_{\bm{\lambda}}\left(\bm{u}\right)p\left(f\mid\bm{u}\right)}{q_{\bm{ \lambda}}\left(\bm{u}\right)p\left(f\mid\bm{u}\right)}\mathrm{d}f\,\mathrm{d} \bm{u}-\log Z\] \[\geq\int\log\left(\frac{u\left(\bm{x},f;\mathcal{D}_{t}\right) \ell\left(\mathcal{D}_{t}\mid f\right)p\left(\bm{u}\right)}{q_{\bm{\lambda}} \left(\bm{u}\right)}\right)p\left(f\mid\bm{u}\right)q_{\bm{\lambda}}\left(\bm {u}\right)\,\mathrm{d}f\,\mathrm{d}\bm{u}-\log Z,\] (7)

where \(Z\) is a normalizing constant. A restriction on \(u\) comes from the inequality in Eq.7, where the utility needs to be strictly positive. This means that non-strictly positive utilities need to be modified to be incorporated into this framework. (See the examples by Kusmierczyk et al., 2019.) Also, notice that the derivation is reminiscent of expectation-maximization (Dempster et al., 1977) and variational lower bounds (Jordan et al., 1999). That is, through the minorize-maximize principle (Lange, 2016), maximizing the lower bound with respect to \(\bm{x}\) and \(\bm{\lambda}\) approximately solves the original problem of maximizing the posterior-expected utility.

Expected Utility Lower-Bound.Up to a constant and rearranging terms, maximizing Eq.7 is equivalent to maximizing

\[\mathcal{L}_{\text{EULBO}}\left(\bm{\lambda},\bm{x};\mathcal{D}_{t}\right) \triangleq\mathbb{E}_{p(f|\bm{u})q_{\lambda}\left(\bm{u}\right)} \left[\log\ell(\mathcal{D}_{t}\mid f)+\log p\left(\bm{u}\right)-\log q_{ \lambda}(\bm{u})+\log u\left(\bm{x},f;\mathcal{D}_{t}\right)\right]\] \[=\mathbb{E}_{q_{\lambda}(f)}\left[\sum_{i=1}^{n_{t}}\log\ell(y_{i }\mid f)\right]-\text{D}_{\text{KL}}\left(q_{\lambda}(\bm{u}),p(\bm{u})\right)+ \mathbb{E}_{q_{\lambda}(f)}\log u\left(\bm{x},f;\mathcal{D}_{t}\right)\] \[=\mathcal{L}_{\text{ELBO}}\left(\bm{\lambda};\mathcal{D}_{t}\right) +\mathbb{E}_{q_{\lambda}(f)}\log u\left(\bm{x},f;\mathcal{D}_{t}\right),\] (8)

which is the joint objective function alluded to in Eq.6. We maximize \(\text{EULBO}\) to obtain \(\left(\bm{x}_{t+1},\bm{\lambda}^{*}\right)=\arg\max_{\bm{x}\in\mathcal{X}, \bm{\lambda}\in\mathcal{L}}\ \ \mathcal{L}_{\text{EULBO}}\left(\bm{x},\bm{\lambda}\right)\), where \(\bm{x}_{t+1}\) corresponds our next BO "query".

From Eq.8, the connection between the \(\text{EULBO}\) and \(\text{ELBO}\) is obvious: the \(\text{EULBO}\) is now "nudging" the ELBO solution toward high utility regions. An alternative perspective is that we are approximating a _generalized posterior_ weighted by the utility (Table. 1 by Knoblauch et al., 2022; Bissiri et al., 2016). Furthermore, Jaiswal et al. (2020, 2023) prove that the resulting actions satisfy consistency guarantees under assumptions typical in such results for variational inference (Wang and Blei, 2019).

Hyperparameters and Inducing Point Locations.For the hyperparameters \(\bm{\theta}\) and inducing point locations \(\bm{Z}\), we use the marginal likelihood to perform model selection, which is common practice in BO (Shahriari et al., 2015, SSV.A). (Optimizing over \(\bm{Z}\) was popularized by Snelson and Ghahramani, 2005.) Following suit, we also optimize the \(\text{EULBO}\) as a function of \(\bm{\theta}\) and \(\bm{Z}\) as

\[\underset{\bm{\lambda},\bm{x},\bm{\theta},\bm{Z}}{\text{maximize}}\ \left\{\ \mathcal{L}_{\text{EULBO}}\left(\bm{\lambda},\bm{x},\bm{\theta},\bm{Z}; \mathcal{D}_{t}\right)\triangleq\mathcal{L}_{\text{ELBO}}\left(\bm{\lambda}, \bm{Z},\bm{\theta};\mathcal{D}_{t}\right)+\mathbb{E}_{q_{\lambda}(f)}\log u \left(\bm{x},f;\mathcal{D}_{t}\right)\right\}.\]

We emphasize here that the SVGP-associated parameters \(\bm{\lambda},\bm{\theta},\bm{Z}\) have gradients that are determined by _both_ terms above. Thus, the expected log-utility term \(\mathbb{E}_{f\sim q_{\lambda}(f)}\log u\left(\bm{x},f;\mathcal{D}_{t}\right)\) simultaneously results in acquisition of \(\bm{x}_{t+1}\) and directly influences the underlying SVGP regression model.

### \(\text{EULBO}\) for Expected Improvement (EI)

The EI acquisition function can be expressed as a posterior-expected utility, where the underlying "improvement" utility function is given by the difference between the objective value of the query, \(f(\bm{x})\), and the current best objective value \(y_{t}^{*}=\max_{i=1,\ldots,t}\left\{y_{i}\mid y_{i}\in\mathcal{D}_{t}\right\}\):

\[u_{\text{EI}}\left(\bm{x},f;\mathcal{D}_{t}\right)\triangleq\text{ReLU}\left( f\left(\bm{x}\right)-y_{t}^{*}\right),\qquad\text{(EI; Jones et al., 1998)}\] (9)

where \(\text{ReLU}\left(x\right)\triangleq\max\left(x,0\right)\). Unfortunately, this utility is not strictly positive whenever \(f\left(\bm{x}\right)\leq y^{*}\). Thus, we cannot immediately plug \(u_{\text{EI}}\) into the \(\text{EULBO}\). While it is possible to add a small positive constant to \(u_{\text{EI}}\) and make it strictly positive as done by Kusmierczyk et al. (2019), this results in a looser Jensen gap in Eq.7, which could be detrimental. This also introduces the need for tuning the constant, which is not straightforward. Instead, we define the following "soft" EI utility:

\[u_{\text{SEI}}\left(\bm{x},f;\mathcal{D}_{t}\right)\triangleq\text{softplus} \left(f\left(\bm{x}\right)-y_{t}^{*}\right),\]

where the ReLU in Eq.9 is replaced with \(\text{softplus}\left(x\right)\triangleq\log\left(1+\exp(x)\right)\). \(\text{softplus}(x)\) converges to the ReLU in both extremes of \(x\to\pm\infty\). Thus, \(u_{\text{SEI}}\) will behave closely to \(u_{\text{EI}}\), while being slightly more explorative due to positivity.

Computing the \(\text{EULBO}\) and its derivatives now requires the computation of \(\mathbb{E}_{f\sim q_{\lambda}(f)}\log u_{\text{SEI}}\left(\bm{x},f;\mathcal{D} _{t}\right)\), which, unlike EI, does not have a closed-form. However, since the utility function only depends on the function values of \(f\), the expectation can be efficiently computed to high precision through one-dimensional Gauss-Hermite quadrature. Crucially, the expensive \(K_{zz}^{-1}m\) and \(K_{zz}^{-1}SK_{zz}^{-1}\) solves that dominate both the asymptotic and practical running time of both the ELBO and the \(\text{EULBO}\) are fixed across the log utility evaluations needed by quadrature. Because quadrature only depends on these precomputed moments, the additional work necessary due to lacking a closed form solution is negligible: Gauss-Hermite quadrature converges extremely quickly in the number of quadrature sites, and only requires on the order of 10 or so of these post-solve evaluations to achieve near machine precision.

### \(\text{EULBO}\) for Knowledge Gradient (KG)

Although non-trivial, the KG acquisition is also a posterior-expected utility, where the underlying utility function is given by the maximum predictive mean value anywhere in the input domain _after_conditioning on a new observation \((\bm{x},y)\in\mathcal{X}\times\mathcal{Y}\):

\[u_{\text{KG}}\left(\bm{x},y;\mathcal{D}_{t}\right)\triangleq\max_{\bm{x}^{ \prime}\in\mathcal{X}}\,\mathbb{E}\left[f(\bm{x}^{\prime})\mid\mathcal{D}_{t} \cup\{(\bm{x},y)\}\right].\qquad\text{(KG; Frazier, 2009; Garnett, 2023)}\]

Note that the utility function as defined above is not non-negative: the maximum predictive mean of a Gaussian process can be negative. For this reason, the utility function is commonly (and originally, _e.g._Frazier, 2009, Eq. 4.11) written in the literature as the _difference_ between the new maximum mean after conditioning on \((\bm{x},y)\) and the maximum mean beforehand:

\[u_{\text{KG}}\left(\bm{x},y;\mathcal{D}_{t}\right)\triangleq\max_{\bm{x}^{ \prime}\in\mathcal{X}}\,\mathbb{E}\left[f(\bm{x}^{\prime})\mid\mathcal{D}_{t }\cup\{(\bm{x},y)\}\right]-\mu_{t}^{+},\]

where \(\mu_{t}^{+}\triangleq\max_{\bm{x}^{\prime\prime}\in\mathcal{X}}\mathbb{E} \left[f(\bm{x}^{\prime\prime})\mid\mathcal{D}_{t}\right]\). Note that \(\mu_{t}^{+}\) plays the role of a simple constant as it depends on neither \(\bm{x}\) nor \(y\). Similarly to the EI acquisition, this utility is still not strictly positive, and we thus define its "softplus-ed" variant:

\[u_{\text{SKG}}\left(\bm{x},y;\mathcal{D}_{t}\right)\triangleq\text{softplus} \left(u_{\text{KG}}\left(\bm{x},y;\mathcal{D}_{t}\right)-c^{+}\right).\]

Here, \(c^{+}\) acts as \(\mu_{t}^{+}\) by making \(u_{\text{KG}}\) positive as often as possible. This is particularly important when the GP predictive mean is negative as a consequence of the objective values being negative. One natural choice of constant is using \(\mu_{t}^{+}\); however, we find that simply choosing \(c^{+}=y_{t}^{+}\) works well and is more computationally efficient. Here, \(y_{t}^{+}\) is the highest value of \(y_{t}\) (the highest objective value observed so far).

One-Shot KG Eulbo.The Eulbo using \(u_{\text{SKG}}\) results in an expensive nested optimization problem. To address this, we use an approach similar to the one-shot knowledge gradient method of Balandat et al. (2020). For clarity, we will define the parameterization function

\[y_{\bm{\lambda}}(\bm{x};\bm{\varepsilon}_{i})\triangleq\mu_{q_{\bm{\lambda}}} (\bm{x})+\sigma_{q_{\bm{\lambda}}}(\bm{x})\,\bm{\varepsilon}_{i},\]

where, for an i.i.d. sample \(\bm{\varepsilon}_{i}\sim\mathcal{N}\left(0,1\right)\), computing \(y_{\bm{\lambda}}\left(\bm{x},\bm{\varepsilon}_{i}\right)\) is equivalent to sampling \(y_{i}\sim\mathcal{N}\left(\mu_{q_{\bm{\lambda}}}(\bm{x}),\sigma_{q_{\bm{ \lambda}}}(\bm{x})\right)\). This enables the use of the reparameterization gradient estimator (Kingma and Welling, 2014; Rezende et al., 2014; Titsias and Lazaro-Gredilla, 2014). Now, notice that the KG acquisition function can be approximated through Monte Carlo as

\[\alpha_{\text{KG}}(\bm{x};\mathcal{D})\approx\frac{1}{S}\sum_{i=1}^{S}u_{\text {KG}}(\bm{x},y_{\bm{\lambda}}\left(\bm{x};\bm{\varepsilon}_{i}\right);\mathcal{ D}_{t})=\frac{1}{S}\sum_{i=1}^{S}\max_{\bm{x}^{\prime}}\mathbb{E}\left[f(\bm{x}^{ \prime})\mid\mathcal{D}_{t}\cup\{\bm{x},y_{\bm{\lambda}}\left(\bm{x};\bm{ \varepsilon}_{i}\right)\}\right],\]

where, for \(i=1,...,S\), \(\bm{\varepsilon}_{i}\sim\mathcal{N}(0,1)\) are i.i.d. The one-shot KG approach absorbs the nested optimization over \(\bm{x}^{\prime}\) into a simultaneous joint optimization over \(\bm{x}\) and a mean maximizer for each of the S samples, \(\bm{x}^{\prime}_{1},...,\bm{x}^{\prime}_{S}\) such that \(\max_{\bm{x}}\alpha_{\text{KG}}(\bm{x};\mathcal{D}_{t})\approx\max_{\bm{x}, \bm{x}^{\prime}_{1},...,\bm{x}^{\prime}_{S}}\alpha_{\text{1-KG}}(\bm{x}; \mathcal{D})\), where

\[\alpha_{\text{1-KG}}(\bm{x};\mathcal{D}_{t})\triangleq\frac{1}{S}\sum_{i=1}^{S }\bm{u}_{\text{1-KG}}(\bm{x},\bm{x}^{\prime}_{i},y_{\bm{\lambda}}\left(\bm{x} ;\bm{\varepsilon}_{i}\right);\mathcal{D}_{t})=\frac{1}{S}\sum_{i=1}^{S}\mathbb{ E}\left[f(\bm{x}^{\prime}_{i})\mid\mathcal{D}_{t}\cup\{\bm{x},y_{\bm{ \lambda}}\left(\bm{x};\bm{\varepsilon}_{i}\right)\}\right],\]

Evidently, there is no longer an inner optimization problem over \(\bm{x}^{\prime}\). To estimate the \(i\)th term of this sum, we draw a sample of the objective value of \(\bm{x}\), \(y_{\bm{\lambda}}(\bm{x};\bm{\varepsilon}_{i})\), and condition the model on this sample. We then compute the new posterior predictive mean at \(\bm{x}^{\prime}_{i}\). After summing, we compute gradients with respect to both the candidate \(\bm{x}\) and the mean maximizers \(\bm{x}^{\prime}_{1},...,\bm{x}^{\prime}_{S}\). Again, we use the "soft" version of one-shot KG in our Eulbo optimization problem:

\[u_{\text{1-SKG}}\left(\bm{x},\bm{x}^{\prime},y;\mathcal{D}_{t}\right)=\text{ softplus}\left(\mathbb{E}\left[f(\bm{x}^{\prime})\mid\mathcal{D}_{t}\cup\{(\bm{x},y)\} \right]-c^{+}\right),\]

where this utility function is crucially a function of both \(\bm{x}\) and a free parameter \(\bm{x}^{\prime}\). As with \(\alpha_{\text{1-KG}}\), maximizing the Eulbo can be set up as a joint optimization problem:

\[\max_{\bm{x},\bm{x}^{\prime}_{1},...,\bm{x}^{\prime}_{S},\bm{\lambda},\bm{ \mathcal{Z}},\bm{\theta}}\quad\mathcal{L}_{\text{ELBO}}(\bm{\lambda},\bm{Z}, \bm{\theta})+\frac{1}{S}\sum_{i=1}^{S}\log u_{\text{1-SKG}}\left(\bm{x},\bm{x}^{ \prime}_{i},y_{\bm{\lambda}}\left(\bm{x};\bm{\varepsilon}_{i}\right);\mathcal{D} _{t}\right)\] (10)

Efficient KG-Eulbo Computation.The computation time of the non-ELBO term in Eq. (10) is dominated by having to compute \(\mathbb{E}\left[f(\bm{x}^{\prime}_{i})\mid\mathcal{D}_{t}\cup\{(\bm{x},y_{\bm{ \lambda}}\left(\bm{x};\bm{\varepsilon}_{i}\right))\}\right]\)\(S\)-timestimes. Notice that we only need to compute an updated posterior predictive mean, and can ignore predictive variances. For this, we can leverage the online updating strategy of Maddox et al. (2021). In particular, the predictive mean can be updated in \(\mathcal{O}(m^{2})\) time using a simple Cholesky update. The additional \(\mathcal{O}(Sm^{2})\) cost of computing the Eulbo is therefore amortized by the original \(\mathcal{O}(m^{3})\) cost of computing the ELBO.

### Extension to q-Eulbo for Batch Bayesian Optimization

The EULBO can be extended to support batch Bayesian optimization by using the Monte Carlo batch mode analogs of utility functions as discussed _e.g._ by Balandat et al. (2020); Wilson et al. (2018). Given a set of candidates \(\bm{X}=\left(\bm{x}_{1},...,\bm{x}_{q}\right)\in\mathcal{X}^{q}\), the \(q\)-EI utility function is given by:

\[\bm{u}_{q\text{-EI}}\left(\bm{X},\bm{f};\mathcal{D}_{t}\right)\triangleq\max_{j =1...q}\text{ReLU}\left(f\left(\bm{x}_{j}\right)-y_{t}^{*}\right)\quad\text{(q -EI; Balandat et al., 2020; Wilson et al., 2018)}\]

This utility can again be softened as:

\[\bm{u}_{q\text{-SEI}}\left(\bm{X},\bm{f};\mathcal{D}_{t}\right)\triangleq\max_ {j=1...q}\text{softplus}\left(f\left(\bm{x}_{j}\right)-y_{t}^{*}\right)\]

Because this is now a \(q\)-dimensional integral, Gauss-Hermite quadrature is no longer applicable. However, we can apply Monte Carlo as

\[\mathsf{E}_{q_{\bm{\lambda}}(f)}\log\bm{u}_{q\text{-SEI}}\left(\bm{X},\bm{f}; \mathcal{D}_{t}\right)\approx\frac{1}{S}\sum_{i=1}^{S}\max_{j=1...q}\text{ softplus}\left(\,y_{\bm{\lambda}}\left(\bm{x};\epsilon_{i}\right)-y_{t}^{*}\, \right).\]

As done in the BoTorch software package (Balandat et al., 2020), we observe that fixing the set of base samples \(\epsilon_{1},...,\epsilon_{S}\) during each BO iteration results in better optimization performance at the cost of negligible q-EULBO bias. Now, optimizing the q-EULBO is done over the full set of \(q\) candidates \(\left(\bm{x}_{1},...,\bm{x}_{q}\right)\) jointly, as well as the GP hyperparameters, inducing points, and variational parameters.

Knowledge Gradient.The KG version of the EULBO can be similarly extended. The expected log utility term in the maximization problem Eq.10 becomes:

\[\underset{\bm{x}_{1},...,\bm{x}_{q},\bm{x}^{\prime}_{1},...,\bm{x}^{\prime}_{ S},\bm{\lambda},\bm{Z},\bm{\theta}}{\text{maximize}}\ \ \mathcal{L}_{\text{ELBO}}(\bm{\lambda},\bm{Z},\bm{\theta})+\frac{1}{S}\sum_{i=1 }^{S}\max_{j=1..q}\log\bm{u}_{1\text{-SKG}}(\bm{x}_{j},\bm{x}^{\prime}_{i},y_ {\bm{\lambda}}\left(\bm{x};\epsilon_{i}\right);\mathcal{D}_{t}),\]

resulting in a similar analog to q-KG as described by Balandat et al. (2020).

### Optimizing the Eulbo

Optimizing the EULBO for SVGPs is known to be challenging (Galy-Fajou and Opper, 2021; Tereinin et al., 2024) as the optimization landscape for the inducing points is non-convex, multi-modal, and non-smooth. Naturally, these are also challenges for EULBO; we found that care must be taken when implementing and initializing the EULBO maximization problem. In this subsection, we outline some key ideas, while a detailed description with pseudocode is presented in Appendix A.

Initialization and Warm-Starting.We warm-start the EULBO maximization procedure by solving the conventional two-step scheme in Eq.5: At each BO iteration, we obtain the "warm" initial values for \((\bm{\lambda},\bm{Z},\bm{\theta})\) by optimizing the standard ELBO. Then, we use this to maximize the conventional acquisition function corresponding to the chosen utility function \(\bm{u}\) (the expectation of \(\bm{u}\) over \(q_{\bm{\lambda}}(f)\)), which provides the warm-start initialization for \(\bm{x}\).

Alternating Maximization Scheme.To optimize \(\mathcal{L}_{\text{EULBO}}\left(\bm{x},\bm{\lambda},\bm{Z},\bm{\theta}\right)\), we alternate between optimizing over the query \(\bm{x}\) and the SVGP parameters \(\bm{\lambda},\bm{Z},\bm{\theta}\). We find this block-coordinate descent scheme to be more stable and robust than jointly updating all parameters, though the reason why this is more stable than jointly optimizing all parameters requires further investigation.

## 4 Experiments

We evaluate EULBO-based SVGPs on a number of benchmark BO tasks, described in detail in Section4.1. These tasks include standard low-dimensional BO problems, e.g., the 6D Hartmann function, as well as 7 high-dimensional and high-throughput optimization tasks.

Baselines.We compare EULBO to several baselines with the main goal of achieving a high reward using as few function evaluations as possible. Our primary point of comparison is ELBO-based SVGPs. We consider two approaches for inducing point locations: 1. optimizing inducing point locations via the ELBO (denoted as **ELBO**), 2. placing the inducing points using the strategy proposed by Moss et al. (2023) at each stage of ELBO optimization (denoted as **Moss et al.**). The latter offers improved BO performance over standard ELBO-SVGP in BO settings, yet--unlike our method--it exclusivelytargets inducing point placement and does not affect variational parameters or hyperparameters of the model. In addition, we compare to BO using exact GPs using \(2,000\) function evaluations as the use of exact GP is intractable beyond this point due to the need to _repeatedly_ fit models.

Acquisition Functions and BO algorithms.For EULBO, we test the versions based on both the Expected Improvement (EI) and Knowledge Gradient (KG) acquisition functions as well as the batch variant. We test the baseline methods using EI only. On high-dimensional tasks (tasks with dimensionality above \(10\)), we run EULBO and baseline methods with standard BO and with trust region Bayesian optimization (TuRB0) (Eriksson et al., 2019). For the largest tasks (Lasso, Molecules) we use acquisition batch size of \(20\) (\(q=20\)), and batch size \(1\) (\(q=1\)) for all others.

Implementation Details and Hyperparameters.Code to reproduce all results in the paper is available at https://github.com/nataliemaus/aabo. We implement EULBO and baseline methods using the GPTorch (Gardner et al., 2018) and BoTorch (Balandat et al., 2020) packages. For all methods, we initialize using a set of \(100\) data points sampled uniformly at random in the search space. We use the same trust region hyperparameters as in (Eriksson et al., 2019). In Appendix B.1, we also evaluate an additional initialization strategy for the molecular design tasks. This alternative initialization matches prior work in using \(10,000\) molecules from the GuacaMol dataset Brown et al. (2019) rather than the details we used above for consistency across tasks, but does achieve higher overall performance.

### Tasks

Hartmann 6D.The widely used Hartmann benchmark function (Surjanovic and Bingham, 2013).

Lunar Lander.The goal of this task is to find an optimal \(12\)-dimensional control policy that allows an autonomous lunar lander to consistently land without crashing. The final objective value we optimize is the reward obtained by the policy averaged over a set of \(50\) random landing terrains. For this task, we use the same controller setup used by Eriksson et al. (2019).

Rover.The rover trajectory optimization task introduced by Wang et al. (2018) consists of finding a \(60\)-dimensional policy that allows a rover to move along some trajectory while avoiding a set of obstacles. We use the same obstacle set up as in Maus et al. (2023).

Figure 2: **Optimization results on the 8 considered tasks.** We compare all methods for both standard BO and TuRB0-based BO (on all tasks except Hartmann). Each line/shaded region represents the mean/standard error over \(20\) runs See subsection B.1 for additional molecule results.

Lasso DNA.We optimize the 180--dimensional DNA task from the LassoBench library (Sehic et al., 2022) of benchmarks based on weighted LASSO regression (Gasso et al., 2009).

Molecular design tasks (x4).We select four challenging tasks from the Guacamol benchmark suite of molecular design tasks (Brown et al., 2019): Osimertinib MPO, Fexofenadine MPO, Median Molecules 1, and Median Molecules 2. We use the SELFIES-VAE introduced by Maus et al. (2022) to enable continuous 256 dimensional optimization.

### Optimization Results

In Figure 2, we plot the reward of the best point found by the optimizer after a given number of function evaluations. Error bars show the standard error of the mean over 20 replicate runs. EULB0 with TuRB0 outperforms the other baselines with TuRB0. Similarly, EULB0 with standard BO outperforms the other standard BO baselines. One noteworthy observation is that neither acquisition function appears to consistently outperform the other. However, EULB0-SVGP almost always dominates ELBO-SVGP and often requires a small fraction of the number of oracle calls to achieve comparable performance. These results suggest that coupling data acquisition with approximate inference/model selection results in significantly more sample-efficient optimization.

### Ablation Study

While the results in Fig. 2 demonstrate that EULB0-SVGP improves the BO performance it is not immediately clear to what extent joint optimization modifies the posterior approximation beyond what is obtained by standard ELBO optimization. To that end, in Fig. 3 we refine an ELBO-SVGP model with varying degrees of additional EULB0 optimization. At every BO iteration we begin by obtaining a SVGP model (where the variational parameters, inducing point locations, and GP hyperparameters are all obtained by optimizing the standard ELBO objective). We then refine some subset of parameters (either the inducing points, the variational parameters, the GP hyperparameters, or all of the above) through additional optimization with respect to the EULB0 objective. Interestingly, we find that tasks respond differently to the varying levels of EULB0 refinement. In the case of Lasso DNA, there is not much of a difference between EULB0 refinement on all parameters versus refinement on the variational parameters alone. On the other hand, the performance on Median Molecules 2 is clearly dominated by refinement on all parameters. Nevertheless, we see that EULB0 is always beneficial, whether applied to all parameters or some subset.

## 5 Related Work

Scaling Bayesian Optimization to the Large-Budget Regime.BO has traditionally been confined to the small-budget optimization regime with a few hundred objective evaluations at most. However, recent interest in high-dimensional optimization problems has demonstrated the need to scale BO to large data acquisition budgets. For problems with \(\sim\)10\({}^{3}\) data acquisitions, Hernandez-Lobato et al. (2017); Snoek et al. (2015); Springenberg et al. (2016) consider Bayesian neural networks (BNN; Neal, 1996), McIntire et al. (2016) use SVGP, and Wang et al. (2018) turn to ensembles of

Figure 3: **Ablation study measuring the impact of EULB0 optimization on various SVGP parameters.** At each BO iteration, we use the standard ELBO objective to optimize the SVGP hyperparameters, variational parameters, and inducing point locations. We then refine some subset of these parameters by further optimizing them with respect to the EULB0 objective.

subsampled GPs. For problems with \(\gg 10^{3}\) acquisitions, SVGP has become the _de facto_ approach to alleviate computational complexity (Griffiths and Hernandez-Lobato, 2020; Maus et al., 2022, 2023; Stanton et al., 2022; Tripp et al., 2020; Vakili et al., 2021). As in this paper, many works have proposed modifications to SVGP to improve its performance in BO applications. Moss et al. (2023) proposed an inducing point placement based on a heuristic modification of determinantal point processes (Kulesza and Taskar, 2012), which we used for initialization, while Maddox et al. (2021) proposed a method for a fast online update strategy for SVGPs, which we utilize for the KG acquisition strategy.

Utility-Calibrated Approximate Inference.The utility-calibrated VI objective was first proposed by Lacoste-Julien et al. (2011), where they used a coordinate ascent algorithm to maximize it. Since then, various extensions have been proposed: Kusmierczyk et al. (2019) leverage black-box variational inference (Ranganath et al., 2014; Titsias and Lazaro-Gredilla, 2014; Morais and Pillow (2022) use expectation-propagation (EP; Minka, 2001); Abbasnejad et al. (2015) and Rainforth et al. (2020) employ importance sampling; Cobb et al. (2018) and Li and Zhang (2023) derive a specific variant for BNNs; and (Wei et al., 2021) derive a specific variant for GP classification. Closest to our work is the GP-based recommendation model learning algorithm by Abbasnejad et al. (2013), which sparsifies an EP-based GP approximation by maximizing a utility similar to those used in BO.

## 6 Limitations and Discussion

The main limitation of our proposed approach is increased computational cost. While EULBO-SVGP still retains the \(O(m^{3})\) computational complexity of standard SVGP, our practical implementation requires a warm-start: first fitting SVGP with the ELBO loss and then maximizing the acquisition function before jointly optimizing with the EULBO loss. Furthermore, EULBO optimization currently requires multiple tricks such as clipping and block-coordinate updates. In future work, we aim to develop a better understanding of the EULBO geometry in order to develop developing more stable, efficient, and easy-to-use EULBO optimization schemes. Nevertheless, our results in Section4 demonstrate that the additional computation of EULBO yields substantial improvements in BO data-efficiency, a desirable trade-off in many applications. Moreover, EULBO-SVGP is modular, and our experiments capture a fraction of its potential use. It can be applied to any decision-theoretic acquisition function, and it is likely compatible with non-standard Bayesian optimization problems such as cost-constrained BO (Snoek et al., 2012), causal BO (Aglietti et al., 2020), and many more.

More importantly, our paper highlights a new avenue for research in BO, where surrogate modeling, approximate inference, and data selection are jointly determined from a unified objective. Extending this idea to GP approximations beyond SVGP and acquisition functions beyond EI/KG may yield further improvements, especially in the increasingly popular high-throughput BO setting.

## Acknowledgments and Disclosure of Funding

The authors thank the anonymous reviewers for suggestions that improved the quality of the work.

N. Maus was supported by the National Science Foundation Graduate Research Fellowship; K. Kim was supported by a gift from AWS AI to Penn Engineering's ASSET Center for Trustworthy AI; G. Pleiss was supported by NSERC and the Canada CIFAR AI Chair program; J. P. Cunningham was supported by the Gatsby Charitable Foundation (GAT3708), the Simons Foundation (542963), the NSF AI Institute for Artificial and Natural Intelligence (ARNI: NSF DBI 2229929), and the Kavli Foundation; J. R. Gardner was supported by NSF awards IIS-2145644 and DBI-2400135.

## References

* Abbasnejad et al. (2015) Ehsan Abbasnejad, Justin Domke, and Scott Sanner. Loss-calibrated Monte Carlo action selection. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 29 of _AAAI_. AAAI Press, March 2015. (page 10)
* Abbasnejad et al. (2013) M. Ehsan Abbasnejad, Edwin V. Bonilla, and Scott Sanner. Decision-theoretic sparsification for Gaussian process preference learning. In _Machine Learning and Knowledge Discovery in Databases_, volume 13717 of _LNCS_, pages 515-530, Berlin, Heidelberg, 2013. Springer. (page 10)
* Aglietti et al. (2020) Virginia Aglietti, Xiaoyu Lu, Andrei Paleyes, and Javier Gonzalez. Causal Bayesian optimization. In _Proceedings of the International Conference on Artificial Intelligence and Statistics_, volume 108 of _PMLR_, pages 3155-3164. JMLR, June 2020. (page 10)
* Balandat et al. (2020) Maximilian Balandat, Brian Karrer, Daniel R. Jiang, Samuel Daulton, Benjamin Letham, Andrew Gordon Wilson, and Eytan Bakshy. BoTorch: A framework for efficient Monte-Carlo Bayesian optimization. In _Advances in Neural Information Processing Systems_, volume 33, pages 21524-21538. Curran Associates, Inc., 2020. (pages 2, 6, 7, 8, 16)
* Bissiri et al. (2016) P. G. Bissiri, C. C. Holmes, and S. G. Walker. A general framework for updating belief distributions. _Journal of the Royal Statistical Society Series B: Statistical Methodology_, 78(5):1103-1130, 2016. (pages 2, 5)
* Blei et al. (2017) David M. Blei, Alp Kucukelbir, and Jon D. McAuliffe. Variational inference: A review for statisticians. _Journal of the American Statistical Association_, 112(518):859-877, April 2017. (pages 2, 3)
* Brown et al. (2019) Nathan Brown, Marco Fiscato, Marwin H.S. Segler, and Alain C. Vaucher. Guacamol: Benchmarking models for de novo molecular design. _Journal of Chemical Information and Modeling_, 59(3):1096-1108, Mar 2019. (pages 8, 9)
* Cobb et al. (2018) Adam D. Cobb, Stephen J. Roberts, and Yarin Gal. Loss-Calibrated Approximate Inference in Bayesian Neural Networks. arXiv Preprint arXiv:1805.03901, arXiv, May 2018. (page 10)
* Dempster et al. (1977) A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum likelihood from incomplete data via the EM algorithm. _Journal of the Royal Statistical Society: Series B (Methodological)_, 39(1):1-22, September 1977. (page 4)
* Eriksson et al. (2019) David Eriksson, Michael Pearce, Jacob Gardner, Ryan D Turner, and Matthias Poloczek. Scalable global optimization via local Bayesian optimization. In _Advances in Neural Information Processing Systems_, volume 32, pages 5496-5507. Curran Associates, Inc., 2019. (pages 1, 2, 8)
* Frazier (2009) Peter I Frazier. _Knowledge-gradient methods for statistical learning_. PhD thesis, Princeton University Princeton, 2009. (page 6)
* Frazier (2018) Peter I Frazier. A tutorial on Bayesian optimization. arXiv Preprint arXiv:1807.02811, ArXiv, 2018. (page 1)
* Galy-Fajou and Opper (2021) Theo Galy-Fajou and Manfred Opper. Adaptive inducing points selection for Gaussian processes. arXiv Preprint arXiv:2107.10066, arXiv, 2021. (page 7)
* Gardner et al. (2018) Jacob Gardner, Geoff Pleiss, Kilian Q. Weinberger, David Bindel, and Andrew G. Wilson. GPyTorch: Blackbox matrix-matrix Gaussian process inference with GPU acceleration. In _Advances in Neural Information Processing Systems_, volume 31, pages 7576-7586. Curran Associates, Inc., 2018. (pages 8, 16)
* Garnett (2023) Roman Garnett. _Bayesian Optimization_. Cambridge University Press, Cambridge, United Kingdom ; New York, NY, 2023. (pages 1, 2, 3, 6)
* Galy-Fajou and Gatsby (2018)Gilles Gasso, Alain Rakotomamonjy, and Stephane Canu. Recovering sparse signals with a certain family of nonconvex penalties and DC programming. _IEEE Transactions on Signal Processing_, 57(12):4686-4698, 2009.
* Griffiths and Hernandez-Lobato (2020) Ryan-Rhys Griffiths and Jose Miguel Hernandez-Lobato. Constrained Bayesian optimization for automatic chemical design using variational autoencoders. _Chemical Science_, 11(2):577-586, 2020.
* Hensman et al. (2013) James Hensman, Nicolo Fusi, and Neil D. Lawrence. Gaussian processes for big data. In _Proceedings of the Conference on Uncertainty in Artificial Intelligence_, pages 282-290. AUAI Press, 2013.
* Hernandez-Lobato et al. (2017) Jose Miguel Hernandez-Lobato, James Requeima, Edward O. Pyzer-Knapp, and Alan Aspuru-Guzik. Parallel and distributed Thompson sampling for large-scale accelerated exploration of chemical space. In _Proceedings of the International Conference on Machine Learning_, volume 70 of _PMLR_, pages 1470-1479. JMLR, July 2017.
* Jaiswal et al. (2020) Prateek Jaiswal, Harsha Honnappa, and Vinayak A. Rao. Asymptotic consistency of loss-calibrated variational Bayes. _Stat_, 9(1):e258, 2020.
* Jaiswal et al. (2023) Prateek Jaiswal, Harsha Honnappa, and Vinayak Rao. On the statistical consistency of risk-sensitive bayesian decision-making. In _Advances in Neural Information Processing Systems_, volume 36, pages 53158-53200. Curran Associates, Inc., December 2023.
* Jankowiak et al. (2020) Martin Jankowiak, Geoff Pleiss, and Jacob R. Gardner. Parametric gaussian process regressors. In _Proceedings of the 37th International Conference on Machine Learning_, ICML'20. JMLR.org, 2020.
* Jones et al. (1998) Donald R. Jones, Matthias Schonlau, and William J. Welch. Efficient global optimization of expensive black-box functions. _Journal of Global Optimization_, 13(4):455-492, 1998.
* Jordan et al. (1999) Michael I. Jordan, Zoubin Ghahramani, Tommi S. Jaakkola, and Lawrence K. Saul. An introduction to variational methods for graphical models. _Machine Learning_, 37(2):183-233, 1999.
* Kingma and Ba (2015) Diederik P. Kingma and Jimmy Ba. Adam: A Method for Stochastic Optimization. In _Proceedings of the International Conference on Learning Representations_, San Diego, California, USA, 2015.
* Kingma and Welling (2014) Diederik P. Kingma and Max Welling. Auto-encoding variational Bayes. In _Proceedings of the International Conference on Learning Representations_, Banff, AB, Canada, April 2014.
* Knoblauch et al. (2022) Jeremias Knoblauch, Jack Jewson, and Theodoros Damoulas. An optimization-centric view on Bayes' rule: Reviewing and generalizing variational inference. _Journal of Machine Learning Research_, 23(132):1-109, 2022.
* Kulesza and Taskar (2012) Alex Kulesza and Ben Taskar. Determinantal point processes for machine learning. _Foundations and Trends(r) in Machine Learning_, 5(2-3):123-286, 2012.
* Kusmierczyk et al. (2019) Tomasz Kusmierczyk, Joseph Sakaya, and Arto Klami. Variational Bayesian decision-making for continuous utilities. In _Advances in Neural Information Processing Systems_, volume 32, pages 6395-6405. Curran Associates, Inc., 2019.
* Lacoste-Julien et al. (2011) Simon Lacoste-Julien, Ferenc Huszar, and Zoubin Ghahramani. Approximate inference for the loss-calibrated Bayesian. In _Proceedings of the International Conference on Artificial Intelligence and Statistics_, volume 15 of _PMLR_, pages 416-424. JMLR, June 2011.
* Lange (2016) Kenneth Lange. _MM Optimization Algorithms_. Society for Industrial and Applied Mathematics, Philadelphia, 2016.
* Li and Zhang (2023) Bolian Li and Ruqi Zhang. Long-tailed Classification from a Bayesian-decision-theory Perspective. arXiv Preprint arXiv:2303.06075, arXiv, 2023.
* Maddox et al. (2021) Wesley J Maddox, Samuel Stanton, and Andrew G Wilson. Conditioning sparse variational Gaussian processes for online decision-making. In _Advances in Neural Information Processing Systems_, volume 34, pages 6365-6379. Curran Associates, Inc., 2021.
* de G. Matthews et al. (2016) Alexander G. de G. Matthews, James Hensman, Richard Turner, and Zoubin Ghahramani. On sparse variational methods and the Kullback-Leibler divergence between stochastic processes. In _Proceedings of the International Conference on Artificial Intelligence and Statistics_, volume 51 of _PMLR_, pages 231-239. JMLR, May 2016.

Natalie Maus, Haydn Jones, Juston Moore, Matt J. Kusner, John Bradshaw, and Jacob Gardner. Local latent space Bayesian optimization over structured inputs. In _Advances in Neural Information Processing Systems_, volume 35, pages 34505-34518, December 2022.
* Maus et al. (2023) Natalie Maus, Kaiwen Wu, David Eriksson, and Jacob Gardner. Discovering many diverse solutions with Bayesian optimization. In _Proceedings of the International Conference on Artificial Intelligence and Statistics_, volume 206, pages 1779-1798. PMLR, April 2023.
* McIntire et al. (2016) Mitchell McIntire, Daniel Ratner, and Stefano Ermon. Sparse Gaussian Processes for Bayesian Optimization. In _Proceedings of the Conference on Uncertainty in Artificial Intelligence_, Jersey City, New Jersey, USA, 2016. AUAI Press.
* Minka (2001) Thomas P. Minka. Expectation propagation for approximate bayesian inference. In _Proceedings of the Conference on Uncertainty in Artificial Intelligence_, pages 362-369, San Francisco, CA, USA, 2001. Morgan Kaufmann Publishers Inc.
* Mockus (1982) Jonas Mockus. The Bayesian approach to global optimization. In _System Modeling and Optimization_, pages 473-481. Springer, 1982.
* Morais and Pillow (2022) Michael J. Morais and Jonathan W. Pillow. Loss-calibrated expectation propagation for approximate Bayesian decision-making. Technical Report arXiv:2201.03128, arXiv, January 2022.
* Moss et al. (2023) Henry B. Moss, Sebastian W. Ober, and Victor Picheny. Inducing point allocation for sparse Gaussian processes in high-throughput Bayesian optimisation. In _Proceedings of the International Conference on Artificial Intelligence and Statistics_, volume 206 of _PMLR_, pages 5213-5230. JMLR, April 2023.
* Neal (1996) Radford M. Neal. _Bayesian Learning for Neural Networks_, volume 118 of _Lecture Notes in Statistics_. Springer New York, New York, NY, 1996.
* Quinonero-Candela and Rasmussen (2005) Joaquin Quinonero-Candela and Carl Edward Rasmussen. A unifying view of sparse approximate Gaussian process regression. _Journal of Machine Learning Research_, 6(65):1939-1959, 2005.
* Rainforth et al. (2020) Tom Rainforth, Adam Golinski, Frank Wood, and Sheheryar Zaidi. TargetA"aware bayesian inference: How to beat optimal conventional estimators. _Journal of Machine Learning Research_, 21(88):1-54, 2020. URL http://jmlr.org/papers/v21/19-102.html.
* Ranganath et al. (2014) Rajesh Ranganath, Sean Gerrish, and David Blei. Black box variational inference. In _Proceedings of the International Conference on Artificial Intelligence and Statistics_, volume 33 of _PMLR_, pages 814-822. JMLR, April 2014.
* Rasmussen and Williams (2005) Carl Edward Rasmussen and Christopher K. I. Williams. _Gaussian Processes for Machine Learning_. The MIT Press, November 2005.
* Rezende et al. (2014) Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and approximate inference in deep generative models. In _Proceedings of the International Conference on Machine Learning_, volume 32 of _PMLR_, pages 1278-1286. JMLR, June 2014.
* Robert (2001) Christian P. Robert. _The Bayesian Choice: From Decision-Theoretic Foundations to Computational Implementation_. Springer Texts in Statistics. Springer, New York Berlin Heidelberg, 2. ed edition, 2001.
* Shahriari et al. (2015) Bobak Shahriari, Kevin Swersky, Ziyu Wang, Ryan P Adams, and Nando De Freitas. Taking the human out of the loop: A review of Bayesian optimization. _Proceedings of the IEEE_, 104(1):148-175, 2015.
* Snelson and Ghahramani (2005) Edward Snelson and Zoubin Ghahramani. Sparse Gaussian processes using pseudo-inputs. In _Advances in Neural Information Processing Systems_, volume 18, pages 1257-1264. MIT Press, 2005.
* Snoek et al. (2012) Jasper Snoek, Hugo Larochelle, and Ryan P Adams. Practical Bayesian optimization of machine learning algorithms. _Advances in neural information processing systems_, 25:2951-2959, 2012.
* Snoek et al. (2015) Jasper Snoek, Oren Rippel, Kevin Swersky, Ryan Kiros, Nadathur Satish, Narayanan Sundaram, Mostofa Patwary, Mr Prabhat, and Ryan Adams. Scalable Bayesian optimization using deep neural networks. In _Proceedings of the International Conference on Machine Learning_, volume 37 of _PMLR_, pages 2171-2180. JMLR, June 2015.

Jost Tobias Springenberg, Aaron Klein, Stefan Falkner, and Frank Hutter. Bayesian Optimization with Robust Bayesian Neural Networks. In _Advances in Neural Information Processing Systems_, volume 29, pages 4134-4142. Curran Associates, Inc., 2016.
* Stanton et al. (2022) Samuel Stanton, Wesley Maddox, Nate Gruver, Phillip Maffettone, Emily Delaney, Peyton Greenside, and Andrew Gordon Wilson. Accelerating Bayesian optimization for biological sequence design with denoising autoencoders. In _Proceedings of the International Conference on Machine Learning_, volume 162 of _PMLR_, pages 20459-20478. JMLR, June 2022.
* Surjanovic and Bingham (2013) Sonja Surjanovic and Derek Bingham. Virtual library of simulation experiments: Test functions and datasets, 2013.
* Terenin et al. (2024) Alexander Terenin, David R. Burt, Artem Artemev, Seth Flaxman, Mark van der Wilk, Carl Edward Rasmussen, and Hong Ge. Numerically stable sparse Gaussian processes via minimum separation using cover trees. _Journal of Machine Learning Research_, 25(26):1-36, 2024.
* Titsias (2009) Michalis Titsias. Variational learning of inducing variables in sparse gaussian processes. In _Proceedings of the International Conference on Artificial Intelligence and Statistics_, volume 5 of _PMLR_, pages 567-574. JMLR, April 2009.
* Titsias and Lazaro-Gredilla (2014) Michalis Titsias and Miguel Lazaro-Gredilla. Doubly stochastic variational Bayes for non-conjugate inference. In _Proceedings of the International Conference on Machine Learning_, volume 32 of _PMLR_, pages 1971-1979. JMLR, June 2014.
* Tripp et al. (2020) Austin Tripp, Erik Daxberger, and Jose Miguel Hernandez-Lobato. Sample-efficient optimization in the latent space of deep generative models via weighted retraining. In _Advances in Neural Information Processing Systems_, volume 33, pages 11259-11272. Curran Associates, Inc., 2020.
* Vakili et al. (2021) Sattar Vakili, Henry Moss, Artem Artemev, Vincent Dutordoir, and Victor Picheny. Scalable Thompson sampling using sparse Gaussian process models. In _Advances in Neural Information Processing Systems_, volume 34, pages 5631-5643, 2021.
* Sehic et al. (2022) Kenan Sehic, Alexandre Gramfort, Joseph Salmon, and Luigi Nardi. Lassobench: A high-dimensional hyperparameter optimization benchmark suite for LASSO. In _Proceedings of the International Conference on Automated Machine Learning_, volume 188 of _PMLR_, pages 2/1-24. JMLR, 25-27 Jul 2022.
* Wang and Blei (2019) Yixin Wang and David M. Blei. Frequentist consistency of variational Bayes. _Journal of the American Statistical Association_, 114(527):1147-1161, July 2019.
* Wang et al. (2018) Zi Wang, Clement Gehring, Pushmeet Kohli, and Stefanie Jegelka. Batched large-scale bayesian optimization in high-dimensional spaces. In _Proceedings of the International Conference on Artificial Intelligence and Statistics_, volume 84 of _PMLR_, pages 745-754. JMLR, March 2018.
* Wasserman (2013) Larry Wasserman. _All of statistics: a concise course in statistical inference_. Springer Science & Business Media, 2013.
* Wei et al. (2021) Yadi Wei, Rishit Sheth, and Roni Khardon. Direct loss minimization for sparse Gaussian processes. In _Proceedings of the International Conference on Artificial Intelligence and Statistics_, volume 130 of _PMLR_, pages 2566-2574. JMLR, March 2021.
* Wilson et al. (2018) James Wilson, Frank Hutter, and Marc Deisenroth. Maximizing acquisition functions for Bayesian optimization. In _Advances in Neural Information Processing Systems_, pages 9884-9895. Curran Associates, Inc., 2018.
* Wu et al. (2017) Jian Wu, Matthias Poloczek, Andrew G Wilson, and Peter Frazier. Bayesian optimization with gradients. In _Advances in Neural Information Processing Systems_, volume 30, pages 5267-5278. Curran Associates, Inc., 2017.

Implementation Details

We will now provide additional details on the implementation. For the implementation, we treat the SVGP parameters, such as the variational parameters \(\lambda\), inducing point locations \(Z\), and hyperparameters \(\theta\), equally. Therefore, for clarity, we will collectively denote them as \(\bm{w}=(\lambda,Z,\theta)\) such that \(\bm{w}\in\mathcal{W}\triangleq\Lambda\times\mathcal{X}^{m}\times\Theta\), and the resulting SVGP variational approximation as \(q_{\bm{w}}\). Then, the ELBO and EULBO are equivalently denoted as follows:

\[\mathcal{L}_{\text{ELBO}}\left(\bm{w};\mathcal{D}\right) \triangleq\mathcal{L}_{\text{ELBO}}\left(\lambda,Z,\theta; \mathcal{D}\right)\] \[\mathcal{L}_{\text{EULBO}}\left(\bm{x},\bm{w};\mathcal{D}_{\bm{x }},\mathcal{D}_{\bm{w}}\right) \triangleq\operatorname*{\mathrm{E}}_{f\sim q_{\bm{w}}\left(f \right)}\log u\left(\bm{x},f;\mathcal{D}_{\bm{x}}\right)+\mathcal{L}_{\text{ ELBO}}\left(\bm{w};\mathcal{D}_{\bm{w}}\right).\]

Also, notice that the \(\mathcal{L}_{\text{EULBO}}\) separately denote the dataset to be passed to the utility and the ELBO. (Setting \(\mathcal{D}_{t}=\mathcal{D}_{\bm{w}}=\mathcal{D}_{\bm{x}}\) retrieves the original formulation in Eq. (8).)

Alternating UpdatesWe perform block-coordinate ascent on the EULBO by alternating between maximizing over \(\bm{x}\) as \(\bm{w}\). Using vanilla gradient descent, the \(\bm{x}\)-update is equivalent to

\[\bm{x}\leftarrow\bm{x}+\gamma_{\bm{x}}\nabla_{\bm{x}}\mathcal{L}_{\text{ELBO}} \left(\bm{x},\bm{w};\mathcal{D}\right)=\bm{x}+\gamma_{\bm{x}}\nabla_{\bm{x}} \mathbb{E}_{f\sim q_{\bm{w}}\left(f\right)}\log u\left(\bm{x},f;\mathcal{D} \right),\]

where \(\gamma_{\bm{x}}\) is the stepsize. On the other hand, for the \(\bm{w}\)-update, we subsample the data such that we optimize the ELBO over a minibatch \(S\subset\mathcal{D}\) of size \(B=|S|\) as

\[\bm{w}\leftarrow\bm{w}+\gamma_{\bm{w}}\nabla_{\bm{w}}\mathcal{L}_{\text{EULBO }}\left(\bm{x},\bm{w};S,\mathcal{D}\right)=\bm{w}+\gamma_{\bm{w}}\nabla_{\bm{w }}\left(\mathbb{E}_{f\sim q_{\bm{w}}\left(f\right)}\log u\left(\bm{x},f; \mathcal{D}\right)+\mathcal{L}_{\text{ELBO}}\left(\bm{w};S\right)\right),\]

where \(\gamma_{\bm{w}}\) is the stepsize. Naturally, the \(\bm{w}\)-update is stochastic due to minibatching, while the \(\bm{x}\)-update is deterministic. In practice, we leverage the Adam update rule (Kingma and Ba, 2015) instead of simple gradient descent. Together with gradient clipping, this alternating update scheme is much more robust than jointly updating \(\left(\bm{x},\bm{w}\right)\).

``` Input: SVGP parameters \(\bm{w}_{0}=(\lambda_{0},Z_{0},\theta_{0})\), Dataset \(\mathcal{D}_{t}\), BO utility function \(\bm{u}\), Output: BO query \(\bm{x}_{t+1}\)
1\(\triangleright\) Compute Warm-Start Initializations
2\(\bm{w}\leftarrow\arg\max_{\bm{w}\in\mathcal{W}}\mathcal{L}_{\text{ELBO}} \left(\bm{w};\mathcal{D}_{t}\right)\) with \(\bm{w}_{0}\) as initialization.
3\(\bm{x}\leftarrow\arg\max_{\bm{x}\in\mathcal{X}}f\,u\left(\bm{x},f;\mathcal{D} _{t}\right)q_{\bm{w}}\left(f\right)\mathrm{d}f\)
4\(\triangleright\) Maximize EULBO
5repeat
6\(\triangleright\) Update posterior approximation \(q_{\bm{w}}\)
7\(\text{Fetch minibatch }S\) from \(\mathcal{D}_{t}\)
8 Compute \(\bm{g}_{\bm{w}}\leftarrow\nabla_{\bm{w}}\mathcal{L}_{\text{EULBO}}\left(\bm{x },\bm{w};S,\mathcal{D}_{t}\right)\)
9\(\text{Clip }\bm{g}_{\bm{w}}\) with threshold \(G_{\text{clip}}\)
10\(\bm{w}\leftarrow\text{AdamStep}_{\gamma_{\bm{w}}}\left(\bm{w},\bm{g}_{\bm{w}}\right)\)
11\(\triangleright\) Update BO query \(\bm{x}\leftarrow\nabla_{\bm{x}}\mathcal{L}_{\text{EULBO}}\left(\bm{x},\bm{w}; S,\mathcal{D}_{t}\right)\)
12\(\text{Clip }\bm{g}_{\bm{x}}\) with threshold \(G_{\text{clip}}\)
13\(\bm{x}\leftarrow\text{AdamStep}_{\gamma_{\bm{x}}}\left(\bm{x},\bm{g}_{\bm{x}}\right)\)
14\(\bm{x}\leftarrow\text{proj}_{\mathcal{X}}\left(\bm{x}\right)\)
15untiluntil converged
16\(\bm{x}_{t+1}\leftarrow\bm{x}\) ```

**Algorithm 1**EULBO Maximization Policy

Overview of Pseudocode.The complete high-level view of the algorithm is presented in Algorithm 1, except for the acquisition-specific details. \(\text{AdamStep}_{\gamma_{\bm{y}}}\left(\bm{x},\bm{g}\right)\) applies the Adam stepsize rule (Kingma and Ba, 2015) to the current location \(\bm{x}\) with the gradient estimate \(\bm{g}\) and the stepsize \(\gamma\). In practice, Adam is a "stateful" optimizer, which maintains two scalar-valued states for each scalar parameter. For this, we re-initialize the Adam states at the beginning of each BO step.

Initialization.In the initial BO step \(t=0\), we initialize \(\bm{Z}_{0}\) with the DPP-based inducing point selection strategy of Moss et al. (2023). For the remaining SVGP parameters \(\bm{\lambda}_{0}\) and \(\theta_{0}\), we used the default initialization of GPyTorch (Gardner et al., 2018). For the remaining BO steps \(t>0\), we use \(\bm{w}\) from the previous BO step as the initialization \(\bm{w}_{0}\) of the current BO step.

Warm-Starting.Due to the non-convexity and multi-modality of both the ELBO and the acquisition function, it is critical to appropriately initialize the EULBO maximization procedure. As mentioned in Section 3.5, to warm-start the EULBO maximization procedure, we use the conventional 2-step scheme Eq. (5), where we maximize the ELBO and then maximize the acquisition function. For ELBO maximization, we apply Adam (Kingma and Ba, 2015) with the stepsize set as \(\gamma_{w}\) until the convergence criteria (described below) are met. For acquisition function maximization, we invoke the highly optimized BoTorch.optimize.optimize_acqf function (Balandat et al., 2020).

Minibatch Subsampling Strategy.As commonly done, we use the reshuffling subsampling strategy where the dataset \(\mathcal{D}_{t}\) is shuffled and partitioned into minibatches of size \(B\). The number of minibatches constitutes an "epoch." The dataset is reshuffled/repartitioned after going through a full epoch.

Convergence Determination.For both maximizing the ELBO during warm-starting and maximizing the EULBO, we continue optimization until we stop making progress or exceed \(k_{\text{epochs}}\) number of epochs. That is if the ELBO/EULBO function value fails to make progress for \(n_{\text{fail}}\) number of steps.

Hyperparameters.The hyperparameters used in our experiments are organized in Table 1. For the full-extent of the implementation details and experimental configuration, please refer to the supplementary code.

\begin{table}
\begin{tabular}{c c c} \hline \hline
**Hyperparameter** & **Value** & **Description** \\ \hline \(\gamma_{\bm{x}}\) & 0.001 & ADAM stepsize for the query \(\bm{x}\) \\ \(\gamma_{\bm{w}}\) & 0.01 & ADAM stepsize for the SVGP parameters \(\bm{w}\) \\ \(B\) & 32 & Minibatch size \\ \(G_{\text{clip}}\) & 2.0 & Gradient clipping threshold \\ \(k_{\text{epochs}}\) & 30 & Maximum number of epochs \\ \(n_{\text{fail}}\) & 3 & Maximum number of failure to improve \\ \(m\) & 100 & Number of inducing points \\ \(n_{0}=|\mathcal{D}_{0}|\) & 100 & Number of observations for initializing BO \\ \# quad. & 20 & Number of Gauss-Hermite quadrature points \\ optimize\_acqf: restarts & 10 & \\ optimize\_acqf: raw\_samples & 256 & \\ optimize\_acqf: batch\_size & 1/20 & Depends on task; see details in Section 4 \\ \hline \hline \end{tabular}
\end{table}
Table 1: Configurations of Hyperparameters used for the ExperimentsAdditional Plots

We provide additional results and plots that were omitted from the main text.

### Additional Results on Molecule Tasks

In Fig. 4, we provide plots on additional results that are similar to those in Fig. 2. On three of the molecule tasks, we use 10,000 random molecules from the GuacaMol dataset as initialization. This is more consistent with what has been done in previous works and achieves better overall optimization performance.

### Separate Plots for BO and TuRB0 Results

In this section, we provide additional plots separating out BO and TuRB0 results to make visualization easier.

Figure 4: **Additional optimization results on three molecule tasks using 10,000 random molecules from the GuacaMol dataset as initialization**. Each line/shaded region represents the mean/standard error over 20 runs. We count oracle calls starting _after_ these initialization evaluations for all methods.

Figure 5: **BO-only optimization results of Fig. 2. We compare EULBO-SVGP, ELBO-SVGP, ELBO-SVGP with DPP inducing point placement (Moss et al., 2023), and exact GPs. These are a subset of the same results shown in Fig. 2. Each line/shaded region represents the mean/standard error over 20 runs.**

### Effect of Number of Inducing Points

For the results with approximate-GPs in Section 4, we used \(m=100\) inducing points. In Fig. 7, we evaluate the effect of using a larger number of inducing points (\(m=1024\)) for EULB0-SVGP and ELBO-SVGP.

Fig. 7 shows that the number of inducing points has limited impact on the overall performance of TuRB0, and EULB0-SVGP outperforms ELBO-SVGP regardless of the number of inducing points used.

Figure 6: **TuRB0-only optimization results of Fig. 2. We compare EULB0-SVGP, ELBO-SVGP, ELBO-SVGP with DPP inducing point placement (Moss et al., 2023), and exact GPs. These are a subset of the same results shown in Fig. 2. Each line/shaded region represents the mean/standard error over 20 runs.**

Figure 7: **Ablating the number of inducing points used by EULB0-SVGP and ELBO-SVGP. As in Fig. 2, we compare running TuRB0 with EULB0-SVGP and with ELBO-SVGP using \(m=100\) inducing points used for both methods. We add two additional curves for TuRB0 with EULB0-SVGP and TuRB0 with ELBO-SVGP using \(m=1024\) inducing points. Each line/shaded region represents the mean/standard error over 20 runs.**

### Effect of GP Objective

The results in Section 4 used a standard SVGP objective. In this section, we evaluate the effect of using an alternative objective: the parametric Gaussian process regressor (PPGPR; Jankowiak et al., 2020) objective. PPGPR differs from the standard SVGP objective in that the variational approximation is optimized to maximize the predictive accuracy instead of matching the posterior.

We compare the choice of objective (PPGPR vs SVGP) in Fig. 8 and observe that the objective has limited impact on the overall performance of TuRB0. In particular, EULB0-EI outperforms ELBO-EI regardless of the GP objective.

Figure 8: **Effect of using the PPGPR objective instead of the SVGP objective for EULB0-EI and ELBO-EI**. As in Fig. 2, we compare running TuRB0 with EULB0-EI and with ELBO-EI using an SVGP model for both methods. We add two additional curves for TuRB0 with EULB0-EI with a PPGPR model, and TuRB0 with ELBO-EI using a PPGPR model. Each line/shaded region represents the mean/standard error over 20 runs.

## Appendix C Compute Resources

Type of Compute and Memory.All results in the paper required the use of GPU workers (one GPU per run of each method on each task). The majority of runs were executed on an internal cluster, where details are shown in Table 2, where each node was equipped with an NVIDIA RTX A5000 GPU. In addition, we used cloud compute resources for a short period leading up to the submission of the paper. We used 40 RTX 4090 GPU workers from runpod.io, where each GPU had approximately 24 GB of GPU memory. While we used 24 GB GPUs for our experiments, each run of our experiments only requires approximately 15 GB of GPU memory.

Execution Time.Each optimization run for non-molecule tasks takes approximately one day to finish. Since we run the molecule tasks out to a much larger number of function evaluations than other tasks (80000 total function evaluations for each molecule optimization task), each molecule optimization task run takes approximately 2 days of execution time. With all eight tasks, ten methods run, and 20 runs completed per method, results in Fig. 2 include 1600 total optimization runs (800 for molecule tasks and 800 for non-molecule tasks). Additionally, the two added curves in each plot in Fig. 3 required 160 additional runs (120 for molecule tasks and 40 for non-molecule task). Completing all of the runs needed to produce all of the results in this paper therefore required roughly 2680 total GPU hours.

Compute Resources Used During Preliminary Investigations.In addition to the computational resources required to produce experimental results in the paper discussed above, we spent approximately 500 hours of GPU time on preliminary investigations. This was done on the aforementioned internal cluster shown in Table 2.

## Appendix D Wall-clock Run Times

In Table 3, we provide average wall-clock run times of different methods on the Lasso DNA optimization task.

\begin{table}
\begin{tabular}{l l} \hline \hline
**Type** & **Model and Specifications** \\ \hline System Topology & 20 nodes with 2 sockets each with 24 logical threads (total 48 threads) \\ Processor & 1 Intel Xeon Silver 4310, 2.1 GHz (maximum 3.3 GHz) per socket \\ Cache & 1.1 MiB L1, 30 MiB L2, and 36 MiB L3 \\ Memory & 250 GiB RAM \\ Accelerator & 1 NVIDIA RTX A5000 per node, 2 GHZ, 24GB RAM \\ \hline \hline \end{tabular}
\end{table}
Table 2: Internal Cluster Setup

\begin{table}
\begin{tabular}{l l} \hline \hline
**Method** & **Wall-clock Run Time in Minutes** \\ \hline EULBO EI & 267.30 \(\pm\) 2.53 \\ EULBO KG & 296.95 \(\pm\) 1.31 \\ ELBO EI & 184.40 \(\pm\) 0.59 \\ Moss et al. 20203 EI & 194.32 \(\pm\) 0.77 \\ \hline \hline \end{tabular}
\end{table}
Table 3: Average wall-clock run times for one full run of TuRBO on the Lasso DNA task. We compare the average wall-clock run time of TuRBO on all TuRBO methods from Figure 2. Note that we do not include the wall clock run time for TuRBO with Exact EI here because we only ran this method out to 2k oracle calls (rather than the full budget of 20k oracle calls).

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: All stated claims are backed-up with results in Section4 and the stated focus/scope of the paper accurately reflects what is discussed throughout the rest of the paper. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: See Section6. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [NA].

Justification: This work does not contain a formal theoretical analysis. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes]. Justification: We provide detailed explanation of how our method works in Section3 and all additional required details to reproduce results in Section4 and AppendixA. Additionally, we have included a link to a public GitHub repository containing all of the source code used in the work in Section4. This source code allows any reader to run our code to reproduce all results in the paper. Additionally, the README in the repository provides detailed instructions to make setting up the proper environment and running the code easy for users. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.

5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Replace by [Yes], [No], or [NA]. Justification: We have included a link to a public GitHub repository containing all of the source code used in the work in Section4. This source code allows any reader to run our code to reproduce all results in the paper. Additionally, the README in the repository provides detailed instructions to make setting up the proper environment and running the code easy for users. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: All chosen hyper-parameters and implementation details are stated in section4 and AppendixA. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: On all plots, we plot the mean taken over multiple random runs and include error bars to show the standard error over the runs. Guidelines: * The answer NA means that the paper does not include experiments.

* The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.
* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: See Appendix C. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We have read the NeurIPS Code of Ethics and made sure to adhere to them in all aspects. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [No]. Justification: The paper is methodological, where the considered algorithm does not immediately pose societal risks.

Guidelines:

* The answer NA means that there is no societal impact of the work performed.
* If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).

11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA]. Justification: The paper does not use data with potential societal concerns. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: All creators of assets used to produce our results are cited in Section4. All assets used are open source software or models. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset.

* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.

13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA]. Justification: The paper does not introduce new assets. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.

14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA]. Justification: The paper does not involve human participants. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.

15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA]. Justification: The paper does not involve live participants. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.

* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.