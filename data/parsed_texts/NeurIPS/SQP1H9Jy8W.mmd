# Leveraging Locality and Robustness to Achieve

Massively Scalable Gaussian Process Regression

Robert Allison

Department of Mathematics

Bristol University

marfa@bristol.ac.uk Anthony Stephenson

Department of Mathematics

Bristol University

**Samuel F**

Alan Turing Institute

**Edward Pyzer-Knapp**

IBM Research

###### Abstract

The accurate predictions and principled uncertainty measures provided by GP regression incur \(\mathcal{O}\big{(}n^{3}\big{)}\) cost which is prohibitive for modern-day large-scale applications. This has motivated extensive work on computationally efficient approximations. We introduce a new perspective by exploring robustness properties and limiting behaviour of GP nearest neighbour (GPnn) prediction. We demonstrate through theory and simulation that as the data-size \(n\) increases, accuracy of estimated parameters and GP model assumptions become increasingly irrelevant to GPnn predictive accuracy. Consequently, it is sufficient to spend small amounts of work on parameter estimation in order to achieve high MSE accuracy, even in the presence of gross misspecification. In contrast, as \(n\rightarrow\infty\), uncertainty calibration and NLL are shown to remain sensitive to just one parameter, the additive noise-variance; but we show that this source of inaccuracy can be corrected for, thereby achieving both well-calibrated uncertainty measures and accurate predictions at remarkably low computational cost. We exhibit a very simple GPnn regression algorithm with stand-out performance compared to other state-of-the-art GP approximations as measured on large UCI datasets. It operates at a small fraction of those other methods' training costs, for example on a basic laptop taking about 30 seconds to train on a dataset of size \(n=1.6\times 10^{6}\).

## 1 Introduction

We first briefly review the computational cost of exact GP regression and the motivation for this paper: Given \(n\) training samples \(X,\bm{y}\), where \(X\in\mathbb{R}^{n\times d}\) has feature vector \(\bm{x}_{i}\in\mathbb{R}^{d}\) in its \(i\)'th row and \(\bm{y}\in\mathbb{R}^{n}\), exact GP regression [36] makes use of an \(n\times n\) gram matrix \(K=K_{X,\bm{\theta}}\) constructed from a pre-specified positive definite covariance function \(c(\cdot,\cdot):\mathbb{R}^{d}\times\mathbb{R}^{d}\rightarrow\mathbb{R}_{+}\) together with length-scale, additive-noise variance and kernel-scale "hyperparameters" \(\bm{\theta}=(l,\sigma_{\xi}^{2},\sigma_{f}^{2})\). In the training phase estimates of the hyperparameters, \(\hat{\bm{\theta}}=(\hat{l},\hat{\sigma}_{\xi}^{2},\hat{\sigma}_{f}^{2})\), are obtained by minimising the loss function

\[\text{loss}(\bm{\theta})=-\log p(\bm{y}|X,\bm{\theta})=\frac{1}{2}\{\bm{y}^{T }K_{\bm{\theta}}^{-1}\bm{y}+\log|K_{\bm{\theta}}|+n\log(2\pi)\}.\] (1)

Then for subsequent predictions the predictive distribution at a point \(\bm{x}^{*}\in\mathbb{R}^{d}\) is defined by

\[y^{*}\mid X,\bm{y} \sim\mathcal{N}(\mu^{*},\sigma^{*\,2})\] (2) \[\mu^{*} =\bm{k}^{*\;\;T}K^{-1}\bm{y}\] (3) \[\sigma^{*\,2} =\hat{\sigma}_{f}^{2}-\bm{k}^{*\;\;T}K^{-1}\bm{k}^{*}\,+\hat{ \sigma}_{\xi}^{2}\] (4)where \(K=K_{\hat{\bm{\theta}}}\) with components \([K]_{ij}=k_{\hat{\bm{\theta}}}(\bm{x}_{i},\bm{x}_{j})\); the vector \(\bm{k}^{*}\) has components \(k^{*}_{i}=k_{\hat{\bm{\theta}}}(\bm{x}_{i},\bm{x}^{*})\), and \(k_{\bm{\theta}}(\bm{x},\bm{x}^{\prime})=\sigma_{f}^{2}c(\bm{x}/l,\bm{x}^{ \prime}/l)+\delta_{\bm{x},\bm{x}^{\prime}}\sigma_{\xi}^{2}\) with a "normalised" covariance function \(c(\cdot,\cdot)\) such that \(c(\bm{x},\bm{x})=1\). The derivation of these steps is based on the assumption that the underlying random field is Gaussian, as is the additive noise with variance \(\sigma_{\xi}^{2}\).

The single cost-\(\mathcal{O}\big{(}n^{3}\big{)}\) step of inverting \(K\) is needed repeatedly to compute the loss. Sophisticated implementations reduce this toward \(\mathcal{O}\big{(}n^{2}\big{)}\) ([35]), but even that cost is generally impractical for \(n>10^{6}\). For a survey of numerous GP approximations and their reduced costs see [19].

Machine learning methods must tackle massive data problems to handle many modern day applications. Revolutionary developments in neural network methodologies achieve this, but Bayesian predictive methodologies, in particular GP regression with its major advantages of robustness and uncertainty measures, are somewhat behind the curve. This motivates development of fast implementations retaining the accuracy and well-principled uncertainty of exact GPs.

## 2 Background and Paper Outline

A feature common to all mainstream GP approximations is that training and prediction processes make joint use of the same underlying mathematical constructions. In the "subset-of-data" method the _same_ subset of data is used both for parameter estimation and prediction. Similarly, in the various Bayesian committee methods ([18]) hyperparameters are estimated using a collection of subsets of data and then the _same_ subsets are used in combination to make predictions. In the variational ([14; 31]) and other inducing point methods parameters are estimated using a low rank approximation to the kernel gram matrix and then the _same_ low-rank matrix approximation is used to make predictions. Despite being almost universally adopted there is no obvious reason why constraining algorithms to use the same constructions for estimation and prediction will help rather than hinder the end goal of high performance at low cost. Whilst there are some passing mentions of decoupling prediction and estimation in the literature - e.g. [28; 1; 3] - it has not been adopted as a mainstream approach.

Our first observation is that allowing parameter-estimation and prediction processes to become decoupled may provide the flexibility to greatly improve cost-accuracy trade-off. As shown in Figure 1, GP approximations first obtain a point estimate of the kernel hyperparameters \(\hat{\bm{\theta}}\) from training data and then feed \(\hat{\bm{\theta}}\) into a predictive process. Our end-goal is only to obtain accurate and well-calibrated _predictive distributions_ of \(y^{*}\) at each target point \(\bm{x}^{*}\); obtaining accurate parameter estimates is _not_ a goal in itself. It follows that the computational budget devoted to parameter estimation need only be sufficient to provide parameters capable of delivering accurate and well-calibrated predictions.

This need not mean that \(\hat{\bm{\theta}}\) is an accurate estimate of the parameters which leads us to the second observation and main theoretical component of this paper: In section 5, theory and simulations reveal that under widely applicable circumstances, as \(n\) increases the mean squared error (MSE) predictive accuracy obtained from GP nearest neighbour prediction becomes increasingly insensitive to model misspecification, i.e. insensitive to the wrong choice of covariance function, to the choice of \(\hat{l}\), \(\hat{\sigma}_{f}^{2}\) and \(\hat{\sigma}_{\xi}^{2}\) and even insensitive to departures from Gaussian model assumptions made for the underlying stochastic process and additive noise. Similarly the negative log likelihood (NLL) predictive-accuracy becomes insensitive to all of those factors _apart from_ the variance of the additive noise \(\hat{\sigma}_{\xi}^{2}\). In 6.1

Figure 1: Flowchart of the GP regression procedure. The dashed box indicates the usual approach of combining the parameter estimation and prediction tasks under one strategy.

we describe a simple calibration step that corrects for the latter inaccuracy thereby achieving near optimal limiting NLL values in addition to well-calibrated uncertainty measures whilst leaving the well-behaved MSE values completely unaltered. We apply these overall observations to construct a highly efficient, accurate and well calibrated regression algorithm in section 6.

**Our key contributions:** Demonstration of GPnn robustness against model and parameter misspecification through theory and simulation (5); derivation of explicit formulae for the limiting MSE, NLL and calibration performance of GPnn as \(n\rightarrow\infty\) (5.1); translation of this theory into a new GP approximation framework with stand-out performance relative to other state-of-the-art GP approximations (6,7.1); a simple generic method for re-calibrating uncertainty measures in GP regression with immediate applications to improving calibration of other GP approximations such as SVGP (6.1); achievement of massive scalability for GPs, for example a \(100\times\) speed-up over state-of-the-art methods on a \(1.6\times 10^{6}\) training set whilst also improving upon their performance (7.1); demonstrating that provably best possible MSE, NLL and calibration performance can be closely approached on data that is grossly misspecified relative to GP model assumptions (7.2).

## 3 Performance Measures, Weak and Strong Calibration

Along with many other GP publications we use mean squared error MSE (or its square root RMSE) and negative log likelihood (NLL), both computed from held-aside test data, to assess predictive performance. These are simply the mean values of \(e_{i}^{*}=(y_{i}^{*}-\mu_{i}^{*})^{2}\) and \(l_{i}^{*}=0.5\cdot(\log{\sigma_{i}^{*}}^{2}+(y_{i}^{*}-\mu_{i}^{*})^{2}/{\sigma _{i}^{*}}^{2}+\log 2\pi)\) respectively. However, we find those measures alone inadequate for determining how _well calibrated_ a predictive distribution is. We define "weakly calibrated" prediction to mean that \(\mathrm{E}_{y^{*}}\mid_{X,y}\big{\{}(y^{*}-\mu^{*})^{2}/{\sigma^{*}}^{2}\big{\}} =1\) and accordingly use "calibration" to be a measure of how well the average value of \(z_{i}^{*}=(y_{i}^{*}-\mu_{i}^{*})^{2}/{\sigma_{i}^{*}}^{2}\) over test-data agrees with 1. This choice of metric (also made use of in [16]) can be motivated as follows: For a well-calibrated GP the expected squared deviation of \(y_{i}^{*}\) from \(\mu_{i}^{*}\) should match the corresponding predictive variance \({\sigma_{i}^{*}}^{2}\) (2), i.e. \(\mathrm{E}_{y^{*}}\mid_{X,y}\big{\{}z^{*}\big{\}}=1\). Hence, observing an average of \(z^{*}\) values close to 1 is consistent with a _necessary_ condition for effective calibration. In practice, we find that GP approximation methods can fall well short of this condition (e.g. see the LHS plot in Figure 4, and Table 3 for those results in tabular form) whilst this is not evident from their MSE and NLL values alone. A better measure of calibration ("strong-calibration") would have been to see how well percentiles of the predictive distributions agree with those observed in test data, e.g. see [24], but we defer such a refinement to future work.

## 4 Prediction Method and Sources of Misspecification

### GP Nearest Neighbour Prediction

We now describe what we mean by "GP nearest neighbour (GPnn) prediction". Assume that we are given parameters \(\hat{\bm{\theta}}=(\hat{l},\hat{\sigma}_{\xi}^{2},\hat{\sigma}_{f}^{2})\) obtained from the parameter estimation phase of Figure 1. Then to compute the estimated pointwise-distribution of \(y^{*}\) at \(\bm{x}^{*}\) indicated in Figure 1 we find the \(m\) nearest training-set neighbours \(N=N(\bm{x}^{*})\) to \(\bm{x}^{*}\) and apply exactly the same GP prediction formulae as in (2), (3) and (4) but with \(X\in\mathbb{R}^{n\times d}\) replaced by \(N\in\mathbb{R}^{m\times d}\) and \(\bm{y}\in\mathbb{R}^{n}\) replaced by \(\bm{y}_{N}\in\mathbb{R}^{m}\). Note that in this setup conditioning on \(N(\bm{x}^{*})\) is equivalent to conditioning on the full input matrix \(X\). We obtain:

\[y^{*}\mid N(\bm{x}^{*}),\bm{y}_{N} \sim\mathcal{N}(\mu_{N}^{*},\sigma_{N}^{*2})\] (5) \[\mu_{N}^{*} =\hat{\bm{k}}_{N}^{*T}\hat{K}_{N}^{-1}\bm{y}_{N}\] (6) \[{\sigma_{N}^{*}}^{2} =\hat{\sigma}_{f}^{2}-\hat{\bm{k}}_{N}^{*T}\hat{K}_{N}^{-1}\hat{ \bm{k}}_{N}^{*}+\hat{\sigma}_{\xi}^{2}\] (7)

where we have used hatted notation in a generic manner to cover all the potential sources of misspecification (4.2) that might arise when we carry out these predictions. The \(\mu_{N}^{*},{\sigma_{N}^{*}}^{2}\) parameters are substituted for \(\mu^{*},{\sigma^{*}}^{2}\) when computing the performance measures described in section 3.

**Note:** In this paper we use Euclidean distance for nearest neighbour assignment but more generally could employ a metric defined by the covariance function - see A. For the covariance functions used in this paper these metrics are "equivalent" because one is a monotonic function of the other.

In our algorithmic implementations we replace an exact \(m\) nearest neighbour algorithm with a much more efficient _approximate_ nearest neighbour algorithm as discussed in section 6. However for the purpose of the theoretical analysis of robustness in section 5 this distinction can be ignored.

### Sources of Misspecification

For the remainder of the paper we extend our theory and notation to encompass several (possibly simultaneous) sources of misspecification: standard GP theory assumes that data comes from a latent Gaussian random field \(\mathcal{GRF}[\sigma_{f}^{2}c(./l,./l)]\) specified by covariance function \(c(\cdot,\cdot)\) and parameters \(l,\sigma_{f}^{2}\). The construction of the matrix \(K_{\bm{\theta}}\) in section 1 assumes data to have arisen from this \(\mathcal{GRF}\) with i.i.d \(\mathcal{N}(0,\sigma_{\xi}^{2})\) additive noise. Henceforth, we limit covariance functions \(c(\bm{x},\bm{x}^{\prime})\) to be stationary, i.e. to vary only with \((\bm{x}-\bm{x}^{\prime})\). The forms of (possibly simultaneous) misspecifications to be accounted for in the theoretical treatment of 5.1 are: (a) parameter \(\sigma_{\xi}^{2}\) wrongly specified as \(\hat{\sigma}_{\xi}^{2}\), (b) (normalised) covariance function \(c(\cdot,\cdot)\) wrongly specified as \(\hat{c}(\cdot,\cdot)\), (c) parameters \(l,\sigma_{f}^{2}\) misspecified as \(\hat{l},\hat{\sigma}_{f}^{2}\) (relevant only if \(c(\cdot,\cdot)\)_not_ misspecified), (d) true additive noise is _not_ Gaussian and (e) the data is generated by a non-Gaussian weakly stationary random field \(\mathcal{WSRF}\) rather than a \(\mathcal{GRF}\).

## 5 GP nearest neighbour Limits and Robustness

In this section we investigate the behaviour of MSE, NLL and calibration for GPnn prediction as \(n\to\infty\), showing how all of these performance measures become increasingly less sensitive to hyperparameter accuracy, kernel choice and the above departures from the GP model assumptions.

### Theory

**Assumptions:** The true generative model from which the data arises is \(y_{i}=f(\bm{x}_{i})+\xi_{i}\) with \(\xi_{i}\stackrel{{\text{iid}}}{{\sim}}P_{\xi}\), \(f(\bm{x})\sim\mathcal{WSRF}[\sigma_{f}^{2}c(./l,./l))]\) and \(y_{i}\mid f(\bm{x}_{i})\sim P_{\xi}\) where the variance of the distribution \(P_{\xi}\) is \(\sigma_{\xi}^{2}\). Neither the \(\mathcal{WSRF}\) nor the additive noise distribution \(P_{\xi}\) need be Gaussian. The training \(\bm{x}\) values are i.i.d. The MSE, NLL and calibration statistics on the test set are derived according to the nearest neighbour GP prediction process (4.1) and subject to any/all forms of misspecification in 4.2. Additionally, we assume that if and only if the \(m^{th}\) nearest neighbour converges to the test point under \(c\), then it also converges under \(\hat{c}\) (A:Definition 10).

**Result:** Given a size-\(n\) training set \(X\) and test point \(\bm{x}^{*}\) in the support (A:Definition 9) of the measure of \(\bm{x}\), let \(f_{n}^{\text{MSE}}(\hat{\bm{\theta}})=\mathrm{E}_{\bm{y},\bm{y}^{*}}\) {\(e_{N}^{*}\)}, \(f_{n}^{\text{NLL}}(\hat{\bm{\theta}})=\mathrm{E}_{\bm{y},\bm{y}^{*}}\) {\(l_{N}^{*}\)}, \(f^{\text{CAL}}(\hat{\bm{\theta}})=\mathrm{E}_{\bm{y},\bm{y}^{*}}\) {\(z_{N}^{*}\)}; where expectations are w.r.t. the true generative process for \(\bm{y}\) and \(y^{*}\) and the performance measures \(e_{N}^{*},l_{N}^{*},z_{N}^{*}\) (section 3) are for the nearest neighbour prediction process. Note that these are the expected (rather than mean) values of the performance measures described in section 3 and the dependence on \(n\) is implicit in the construction of the nearest neighbour sets \(N=N_{m}(\bm{x}^{*})\) used for prediction. Then we have:

**Theorem 1** (GPnn limits).: _As \(n\to\infty\), \(f_{n}^{\text{MSE}},f_{n}^{\text{CAL}},f_{n}^{\text{NLL}}\to f_{\infty}^{\text {MSE}},f_{\infty}^{\text{CAL}},f_{\infty}^{\text{NLL}}\) a.e w.r.t. the (i.i.d.) measure on \(\bm{x}\in X\) and \(\bm{x}^{*}\), and pointwise as functions of \(\hat{\bm{\theta}}\) where:_

1. \(f_{\infty}^{\text{MSE}}(\hat{\bm{\theta}})=\sigma_{\xi}^{2}(1+m^{-1})\pm \mathcal{O}\big{(}m^{-2}\big{)}\)__
2. \(f_{\infty}^{\text{CAL}}(\hat{\bm{\theta}})=\frac{\sigma_{\xi}^{2}}{\hat{\sigma} _{\xi}^{2}}\pm\mathcal{O}\big{(}m^{-2}\big{)}\)__
3. \(f_{\infty}^{\text{NLL}}(\hat{\bm{\theta}})=\frac{1}{2}\left\{\log\left(\hat{ \sigma}_{\xi}^{2}(1+m^{-1})\right)+\frac{\sigma_{\xi}^{2}}{\hat{\sigma}_{\xi}^{ 2}}+\log 2\pi\right\}\pm\mathcal{O}\big{(}m^{-2}\big{)}\)_._

_Setting \(\hat{\bm{\theta}}=\bm{\theta}\) (and in particular \(\hat{\sigma}_{\xi}^{2}=\sigma_{\xi}^{2}\)) in the above provides matched-parameter limiting results._

Proof sketch.: It is quite straightfoward to derive expressions for each of the expectations \(f_{n}^{\text{MSE}},f_{n}^{\text{CAL}},f_{n}^{\text{NLL}}\) since these only depend on the known marginal covariance matrices of the (misspecified) \(\mathcal{WSRF}\). We then use results concerning asymptotic convergence of Euclidean nearest neighbours, in combination with some standard linear algebra results and continuity properties, to obtain the stated limits. Note that in the expression for \(f_{\infty}^{\mathrm{MSE}}(\hat{\bm{\theta}})\) above the right hand side must always exceed \(\sigma_{\xi}^{2}\) since this is an absolute lower bound on MSE performance; likewise \(f_{\infty}^{\mathrm{CAL}}(\hat{\bm{\theta}})\) is constrained to be non-negative. See _full proof (A)._. 

**Interpretation:** The MSE results of Theorem 1 show that to within a small factor (e.g. \(m^{-1}=0.0025\) when \(m=400\) as for all reported runs of our algorithm) the _best possible_ MSE will be achieved in the limit. The NLL results also tell us (by setting \(\sigma_{\xi}^{2}=\hat{\sigma}_{\xi}^{2}\)) what the best possible limiting NLL value is, but only according to the _possibly misspecified_ Gaussian model. The corrupting influence of an incorrect value of \(\hat{\sigma}_{\xi}^{2}\) on the limiting NLL value is clearly evident from the expression for \(f_{\infty}^{\mathrm{NLL}}\) and the picture is similar for calibration.

**Remark 2**.: _Theorem 1 shows that isotropic (e.g. RBF and Matern) kernels converge to the best possible MSE as \(n\to\infty\) even on data generated with independent lengthscales on each \(\bm{x}\) coordinate._

Note that 1 refers to pointwise convergence whereas we believe uniform convergence results should also be obtainable, e.g. perhaps of the form (or similar):

**Conjecture 3**.: \(\mathrm{E}_{X,\mathbf{x}^{*}}\left\{f_{\mathrm{n}}^{\mathrm{MSE}}(\hat{\bm{ \theta}})\right\}\to f_{\infty}^{\mathrm{MSE}}=\sigma_{\xi}^{2}(1+m^{-1})\pm \mathcal{O}\!\left(m^{-2}\right)\) _uniformly as a function of \(\hat{\bm{\theta}}\) as \(n\to\infty\)._

This particular conjecture would hold, for example, if the l.h.s. were shown to be a continuous function of \(\hat{\bm{\theta}}\) reducing monotonically and pointwise to the limit with \(n\) (by Dini's theorem). We also have initial results on rate of convergence in Theorem 1 which we defer to a later publication once more fully extended.

### Simulation of Limits and Robustness at Scale

At first sight it seems infeasible to demonstrate the above robustness and limit properties empirically on GP data-sets of size \(10^{6}\) or above. One major obstacle being the generation of GP synthetic datasets at this size which is computationally prohibitive even allowing for the speedups described in [29]. Fortunately we can avoid the need for large-scale data-generation, in addition to achieving other major efficiencies, by adopting the approach described in Algorithm 1.

The simulation algorithm gains its efficiency by exploiting the locality of the GPnn prediction process at \(\bm{x}^{*}\) whereby the predictive distribution only makes use of a size-\((m+1)\) marginal distribution of the full distribution of \((\bm{y},y^{*})\) over \(\mathbb{R}^{n+1}\). (By the definition of a Gaussian process this marginal is a (low dimensional) multivariate Gaussian distribution from which samples can cheaply be generated). The following lemma is proved in Appendix B:

Figure 2: Behaviour of performance metrics as functions of kernel hyperparameters for increasing training set sizes \(n\). The black dashed line denotes the true parameter value; the red dashed line shows the limiting behaviour as \(n\to\infty\) and the green dashed line shows the limiting behaviour when the hyperparameters are correct (the red and green dashed lines coincide for MSE). True \(l\) is shown in the title; additionally \(\sigma_{\xi}^{2}=0.1\), \(\sigma_{f}^{2}=0.9\), \(d=20\). When not varied, the assumed parameters are \(\hat{\sigma}_{\xi}^{2}=0.2\), \(\hat{\sigma}_{f}^{2}=0.8\), \(\hat{l}=l\). Finally we generate the input data from the measure \(P_{\mathbf{x}}=\mathcal{N}\left(0,\frac{1}{d}I_{d}\right)\).

**Lemma 4** (Algorithm 1 validity).: _The MSE, NLL and calibration estimates returned by Algorithm 1 are equally valid to those that would be obtained by applying the full GPnn predictive process (exactly as described in subsection 4.1) to synthetic data sets of size \(n\)._

Figure 2 shows how the observed performance metrics approach the limiting behaviour as \(n\) increases. In particular, from the RHS plot we see that as \(n\) increases, MSE becomes increasingly insensitive to departure of \(\hat{l}\) from the true value \(l=1\). This is a consequence of (what appears to be uniform) convergence of MSE toward constant value \(\sigma_{\xi}^{2}=0.1\) (the best achievable MSE) as predicted by Theorem 1 (i). In practical terms: once a practitioner selects a particular kernel family, the accuracy of the hyperparameter \(\hat{l}\) becomes less and less critical to MSE predictive accuracy with increasing \(n\), so that expenditure on estimating it accurately provides diminishing returns.

The interpretation of the leftmost two plots is similar albeit somewhat more involved: The dotted red lines show the asymptotic dependence on the misspecification of the noise-variance as predicted by Theorem 1 (ii) and (iii), i.e. plots of \(y=\frac{0.1}{\hat{\sigma}_{\xi}^{2}}\) and \(y=\frac{1}{2}\{\log\hat{\sigma}_{\xi}^{2}+\frac{0.1}{\hat{\sigma}_{\xi}^{2}}+ \log 2\pi\}\) where \(0.1=\sigma_{\xi}^{2}\) is the true noise value used to generate the synthetic GP data. We again use evidence of uniform convergence toward this limiting behaviour with increasing \(n\). The green horizontal dotted lines show the limiting values of NLL and calibration (\(y=\frac{1}{2}\{\log\sigma_{\xi}^{2}+1+\log 2\pi\}\) and \(y=1\) respectively) that can be achieved if the incorrect \(\hat{\sigma}_{\xi}^{2}\) value is replaced by the correct value \(\sigma_{\xi}^{2}\). This underlines the importance of estimating this particular parameter more accurately in order to obtain improved NLL and uncertainty calibration for large \(n\). Further plots from Algorithm 1, showing dependence of each metric on all of the parameters, are given in Figure 6.

## 6 A Highly Scalable GP Nearest Neighbour Regression Algorithm

### Parameter Estimation

**Parameter Estimation Phase 1** The first step of parameter estimation (Figure 1) involves randomly selecting a small subset \(E\) of the training data to obtain a first-pass estimate \(\hat{\bm{\theta}}=(\hat{l},\hat{\sigma}_{\xi}^{2},\hat{\sigma}_{f}^{2})\). Small subsets yield sub-optimal \(\hat{\bm{\theta}}\) values, yet as shown in 7.1, these are capable of yielding strong MSE performance due to the robustness properties of section 5. We use the method in section 3.1 of [7] to estimate parameters from \(E\), randomly partitioning \(E\) into \(w\) size-\(s\) subsets (\(ws=e\)) and using a block diagonal approximation (with \(w\) blocks of size \(s\times s\) ) to the full \(e\times e\) gram matrix. For Table 1 we set \(e=3000,s=300,m=10\). For strong computational efficiency we set \(e=|E|\) to a small _constant_ value no matter the size of \(n\). Thus as \(n\) grows, an increasingly small portion of the data is used for this phase of parameter estimation and the associated cost does not increase with \(n\). Note that other choices of cheap parameter estimation could be substituted here.

**Parameter Estimation Phase 2 (calibration)** As shown in section 5, NLL and calibration performance derived from \(\hat{\bm{\theta}}=(\hat{l},\hat{\sigma}_{\xi}^{2},\hat{\sigma}_{f}^{2})\) remain very sensitive to inaccuracies in \(\hat{\sigma}_{\xi}^{2}\). An additional "calibration step" is used to refine those parameters: We randomly select a size \(c\) calibration set \(C\) (which is _otherwise unused_) from the training data and proceed according to Algorithm 2.

**Input:** A size \(c\) subset \(C\) of \((\bm{x}^{*},y^{*})\) pairs from the training data, parameters \(\hat{\bm{\theta}}=(\hat{l},\hat{\sigma}_{\xi}^{2},\hat{\sigma}_{f}^{2})\).

1. For each \((\bm{x}^{*}_{i},y^{*}_{i})\in C\) use the efficient GPnn predictive algorithm of 6.2, with covariance function \(\hat{c}(.,.)\) and parameters \(\hat{\bm{\theta}}=(\hat{l},\hat{\sigma}_{\xi}^{2},\hat{\sigma}_{f}^{2})\), to obtain an estimate of the mean and variance, \(\mu_{i}^{*},{\sigma_{i}^{*}}^{2}\) of the predictive distribution of \(y^{*}_{i}\) at \(\bm{x}^{*}_{i}\).
2. Compute \(\alpha=\frac{1}{c}\sum_{i=1}^{c}\frac{(y^{*}_{i}-\mu^{*}_{i})^{2}}{{\sigma_{i} ^{*}}^{2}}\).

**Output:** Calibrated parameters \(\hat{\bm{\theta}}^{\prime}=(\hat{l},\alpha\cdot\hat{\sigma}_{\xi}^{2},\alpha \cdot\hat{\sigma}_{f}^{2})\).

**Algorithm 2** Calibration of Predictive Distribution

Note that this process not only adjusts the noise variance estimate \(\sigma_{\xi}^{2}\) but also the kernel scale parameter \(\sigma_{f}^{2}\). In so doing it simultaneously calibrates the predictive distribution and improves NLL performance whilst leaving unchanged the MSE performance obtained from the original parameter estimates \(\hat{\bm{\theta}}=(\hat{l},\hat{\sigma}_{\xi}^{2},\hat{\sigma}_{f}^{2})\). The lemma below is straightforward to prove (Appendix C):

**Lemma 5** (Calibration).: _The parameters \(\hat{\bm{\theta}}^{\prime}\) output from Algorithm 2 produce GPnn predictions that (a) achieve perfect (weak) calibration on \(C\), (b) minimise NLL on \(C\) over all choices of \(\alpha\) and (c) produce the same MSE as \(\hat{\bm{\theta}}\) does on any choice of test set._

**Remark 6**.: _Algorithm 2 can be applied to other GP methods, such as SVGP, to improve calibration._

Table 1 uses \(c=1000\); a simple refinement would be to select \(c\) automatically (e.g. using a bootstrap) with optional manual override. Where accurate uncertainty calibration is paramount practitioners could devote much larger CPU resources to this phase (which is also easily distributed); when no uncertainty measures are to be used this calibration step can be bypassed altogether.

### Efficient Nearest Neighbour Prediction

In order to implement GPnn prediction described in 4.1 we use the scikit-learn NearestNeighbors package ([22]). This implements an efficient _approximate_ nearest neighbour algorithm whereby one-time work is carried out to construct a table (at \(\mathcal{O}(dn\log n)\) (see e.g. [9; 21]) and counted within the total _training_ times quoted) which subsequent calibration/test predictions then make use of. Query compute-costs are described in the associated documentation, e.g. \(\mathcal{O}(d\log n)\) for the Ball-tree algorithm which the default automated algorithm selection in SciKit-Learn should at least match. In contrast, exact kNN costs are listed in the documentation as \(\mathcal{O}(dn)\). As is evident in Table 2, Figure 3 and the quoted query and table-setup complexities, the nearest neighbour work increases with \(\bm{x}\)-dimension \(d\). Alternative nearest neighbour algorithms and/or dimension-reduction techniques to help address this are yet to be investigated. We set the number of nearest neighbours to be \(m=400\) for all usages in this paper having observed minimal sensitivity to this parameter on independent synthetic datasets. Although a simple cross-validation procedure could be followed if tuning of \(m\) is desired (at an increase in computational overhead), we wished to minimise such fine-tuning to emphasise the simplicity and robustness of the method we present, noting the strong performance we obtain despite this. At first glance, prediction complexity might appear restrictive, but some empirical tests on a laptop reveal comparable performance to SVGP prediction (Table 5).

## 7 Experimental Performance of GPnn Regression

### Performance on Real World Datasets

**Implementation Details1:** Comparisons are made between our method and the state-of-the-art approaches of SVGP [14] and five distributed methods ([15; 2; 33; 7] and [18] following the recommendation in [4]). We have chosen not to include other highly-performant approximations(e.g. structured kernel interpolation (SKI) methods and their extensions ([37; 39; 11])), since, to our knowledge, none have supplanted these methods in the community as ubiquitous benchmarks on general datasets. Parameters for our method are given in 6.1 and 6.2. SVGP used 1024 inducing points; the distributed methods all used randomly selected subsets of sizes as close as possible to 625. The learning rate for the Adam optimiser was 0.01 for SVGP and 0.1 for our method and distributed methods. All runs in Table 1 used the the squared exponential ("RBF") covariance function. A "pre-whitening" process (E.1) was applied to x values for all methods and the y values normalised (using training data-derived means and sds) to have mean zero and variance 1. More complete details are given in E. SVGP was run on a single Tesla V100 GPU with 16GB memory; all distributed methods run on eight Intel Xeon Platinum 8000 CPUs sharing 32GB of memory. Our method was run on a Macbook Pro with 2.4 GHz Intel core i5. See D for a full explanation of our selection and pre-processing of datasets which, apart from Protein, are taken from the UCI repository.

**Results** Runs were made on three randomly selected \(7/9,2/9\) splits into training and test sets. Table 1 shows MSE and NLL results for our method alongside SVGP and distributed method (note: \(n=training\)_set_ size). The table shows only the best of the five distributed methods' results (w.r.t. MSE) but full results and details of all methods and all three performance measures are given in E. Complete calibration results are also plotted in Figure 4. With the exception of the Bike dataset our method is found to outperform all methods simultaneously for both MSE and NLL, and calibration likewise bar a narrow second place on the Song dataset. Table 2 and Figure 3 show that this is achieved whilst undercutting the training costs of the other methods, an effect that is very pronounced for large training sets (e.g. approximately \(100\times\) faster than the other methods at \(n=1.6\times 10^{6}\) on House Electric). Figure 3 shows that a significant portion of time involves calibration; this can be parallelised (or eliminated if uncertainty is not required). Note also that larger timings observed for higher dimensional datasets are due to slower performance of the approximate nearest neighbour algorithm (6.2) in that regime, both for nn table construction and calibration. As discussed in 6.2, future improvements may reduce this effect. It is very interesting that "curse-of-dimensionality" has not impacted on the method's MSE, NLL or calibration competitiveness at large \(d\). This was despite the fact that a PCA analysis of the training \(\bm{x}\) values showed no concentration within a low dimensional space (as to be expected given the prewhitening that has been applied (subsection E.1).

**Conjecture 7**.: _Robustness to "curse-of-dimensionality" is at least partially explained by the increase in the intrinsic data-length-scale by a factor of order \(\sqrt{d}\) that must arise in order for GP methods to be effective._

The heuristic reasoning behind this conjecture is as follows: Unless length scale increases with \(d\) the kernel gram matrix will exhibit an abundance of exceptionally small off-diagonal entries and hence be unable able to gain significant predictive power. A \(\sqrt{d}\) increase serves to counterbalance this effect and seems consistent with length scales recovered from real data in practice.

### Performance on Massive Synthetic Datasets

We generated size \(5\times 10^{7}\) datasets using the 15-variable deterministic Oakley and O'Hagan function [20; 30] with i.i.d. variance-\(\sigma_{\xi}^{2}\) additive noise sampled from a zero-mean Laplacian distribution (with much wider tails than \(\mathcal{N}(0,\sigma_{\xi}^{2})\)). This function has 5 inputs contributing significantly to output

\begin{table}
\begin{tabular}{l l l l l l l l l} \hline \hline  & & \multicolumn{2}{c}{**NLL**} & \multicolumn{2}{c}{**RMSE**} \\ Dataset & \(n\) & \(d\) & \multicolumn{2}{c}{Distributed} & \multicolumn{2}{c}{OURS} & \multicolumn{2}{c}{SSVGP} \\ \hline Poletle & 4.6e+03 & 19 & 0.0091 \(\pm\) 0.015 & **-0.214 \(\pm\) 0.019** & -0.0667 \(\pm\) 0.017 & 0.241 \(\pm\) 0.0033 & **0.195 \(\pm\) 0.0042** & 0.226 \(\pm\) 0.0059 \\ Bike & 1.4e+04 & 13 & 0.977 \(\pm\) 0.0057 & 0.953 \(\pm\) 0.013 & **0.93 \(\pm\) 0.0043** & 0.634 \(\pm\) 0.004 & 0.624 \(\pm\) 0.0079 & **0.606 \(\pm\) 0.0033** \\ Protein & 3.6e+04 & 9 & 1.11 \(\pm\) 0.0051 & **1.012 \(\pm\) 0.0016** & 1.05 \(\pm\) 0.0059 & 0.733 \(\pm\) 0.0038 & **0.666 \(\pm\) 0.0014** & 0.688 \(\pm\) 0.0043 \\ Cslice & 4.2e+04 & 378 & 0.159 \(\pm\) 0.052 & **-1.26 \(\pm\) 0.01** & 0.467 \(\pm\) 0.016 & 0.237 \(\pm\) 0.012 & **0.132 \(\pm\) 0.0006** & 0.384 \(\pm\) 0.0064 \\ Road3D & 3.4e+05 & 2 & 0.685 \(\pm\) 0.0041 & **0.371 \(\pm\) 0.004** & 0.608 \(\pm\) 0.018 & 0.478 \(\pm\) 0.0023 & **0.351 \(\pm\) 0.0014** & 0.443 \(\pm\) 0.008 \\ Song & 4.6e+05 & 90 & 1.32 \(\pm\) 0.0012 & **1.18 \(\pm\) 0.0045** & 1.24 \(\pm\) 0.0012 & 0.851 \(\pm\) 6.7e-05 & **0.787 \(\pm\) 0.0045** & 0.834 \(\pm\) 0.0011 \\ HouseE & 1.6e+06 & 8 & -1.34 \(\pm\) 0.0013 & **-1.56 \(\pm\) 0.0065** & -1.46 \(\pm\) 0.0046 & 0.0626 \(\pm\) 5.2e-05 & **0.050 \(\pm\) 0.00072** & 0.0566 \(\pm\) 0.00011 \\ \hline \hline \end{tabular}
\end{table}
Table 1: RMSE and NLL results (mean and standard deviation over 3 runs) for the best distributed method (w.r.t. MSE), SVGP and our method.

variance, 5 with smaller effect, and 5 with almost no effect. These properties are poorly matched by the isotropic covariance functions being applied, resulting in gross misspecification of the assumed \(\mathcal{GRF}\) model and the additive noise. Figure 5 shows performance achieved with both the squared exponential ("RBF") covariance function and the exponential (Matern 1/2) covariance function. It is very interesting to note the improvement in convergence rate achieved by the exponential kernel. (see Remark 2 for a potential explanation of why isotropic covariance functions are so effective at large \(n\)).

**Remark 8**.: _We checked to see whether this strong exponential kernel performance extended to UCI datasets. Surprisingly, given that it is not recommended for use in GP regression (e.g. [36] page 85), it produced best RMSE performance across the board when compared with RBF and Matern 3/2 kernels (Table 4), with Road3D RMSE reducing from 0.351 to 0.098. Exponential-kernel NLL performance was everywhere best apart from Ctslice and calibration was also better in most cases._

## 8 Discussion

**Related work**: The basic "subset-of-data" approximation ([3]) also achieves training efficiency by using a small portion of training data and can achieve surprisingly goods results ([3], [35] section 5.1, [13]). But it typically would need a much greater proportion of training data than we are using for large \(n\) due to its failure to leverage the power of large training sets for prediction; this explains why it is not consistently competitive with other methods. Passing references to the decoupling of prediction and estimation have been made, e.g. [28, 1, 3], but not shown to be as consistently powerful as we have found, nor justified in terms of robustness theory or explored as a mainstream

\begin{table}
\begin{tabular}{l l l l l l} \hline \hline  & & \multicolumn{3}{c}{**Train time/s**} \\  & & \multicolumn{3}{c}{Distributed} & \multicolumn{1}{c}{OURS} & \multicolumn{1}{c}{SVGP} \\ Dataset & \(n\) & \(d\) & & & \\ \hline Poleele & 4.6e+03 & 19 & \(17.1\pm 0.66\) & \(28.8\pm 0.22\) & \(\mathbf{11.9\pm 0.081}\) \\ Bike & 1.4e+04 & 13 & \(43.5\pm 0.64\) & \(\mathbf{28.4\pm 0.12}\) & \(32.3\pm 0.15\) \\ Protein & 3.6e+04 & 9 & \(98.9\pm 1.7\) & \(\mathbf{27.7\pm 0.19}\) & \(81.1\pm 1.1\) \\ Cslice & 4.2e+04 & 378 & \(86.9\pm 1.7\) & \(\mathbf{76.1\pm 4.6}\) & \(98.2\pm 1.8\) \\ Road3D & 3.4e+05 & 2 & \(1200.0\pm 110.0\) & \(\mathbf{27.9\pm 1.3}\) & \(760.0\pm 8.0\) \\ Song & 4.6e+05 & 90 & \(1050.0\pm 110.0\) & \(\mathbf{138.0\pm 5.8}\) & \(1080.0\pm 14.0\) \\ HouseE & 1.6e+06 & 8 & \(3110.0\pm 250.0\) & \(\mathbf{32.0\pm 0.34}\) & \(3720.0\pm 17.0\) \\ \hline \hline \end{tabular}
\end{table}
Table 2: Corresponding recorded training times (with mean and standard deviation from 3 runs) associated to the metrics in Table 1, i.e. recorded at the same time and with the time given for the “distributed” method relating to the best performing model in terms of MSE. Mean times are rounded to 3 s.f. and standard deviation to 2.

approach. Various works (primarily from the geospatial community) make use of nearest neighbour (NN) techniques for GPs (e.g. [34, 25, 28, 6, 5, 8, 32, 38]). Vecchia ([34]) uses NNs to approximate likelihoods for parameter estimation, whilst Stein ([28]) adapts this work to REML using more distant points as well as NNs, again for parameter estimation purposes. In contrast, our focus is on using NNs for _prediction_, and whilst we have found passing references to its explicit use for this purpose (e.g. [27]) we have found little or no discussion of effectiveness in comparison to other methods on large datasets and no detailed accompanying analysis of its robustness properties and how these can achieve very high efficiency at scale. [5] gives a construction of a hierarchical fully Bayesian model ('NNGP') derived using collections of NN sets. This approach adds considerable computational overhead and code-complexity, e.g. use of Gibbs sampling. Whilst fully Bayesian treatment of hyperparameters is explored in the ML literature, e.g. [17], it has not been adopted by the ML community for use at scale due to its high computational cost relative to empirical Bayes methods ([17], section 5). Bearing this in mind, we consider the extension given in [8] for improved scalability (Algorithm 5 - 'conjugate NNGP') to be more relevant. In this hybrid method some hyperparameters are recovered as 'empirical-Bayes' point-estimates via grid-based search and the remainder treated in a fully Bayesian fashion using a conjugate prior with some choice of hyper-hyperparameters. This results in a Student-\(t\) predictive distribution, rather than Gaussian, but with equal first moment to ours and variance differing only by a (hyper-hyperparameter dependent) multiplicative factor; an effect that our recalibration step would render redundant (see F.1). Recent work ([32, 38]) extends local geospatial GP methods into sparse variational ML applications. 'VNNGP' is shown to be competitive with other methods in [38] despite adding further approximations into the model. We note that when using all observations as the inducing points their predictive mechanism matches ours, up to choice of parameter estimation. We believe these pre-existing works, which differ significantly in approach and perspective, complement our own, which emphasizes the benefits of decoupling parameter estimation from prediction, the robustness properties that can be achieved at large scales, the efficacy of a simple recalibration step and can be run at high scale with a simple algorithm on an off-the-shelf laptop.

**Limitations and Future Research:** Our results exhibit a leap in speed and performance for GP regression at scale, but there remains more to be done to fully explain and extend performance (as evidenced by our remarks and conjectures). This is particularly so for high dimensional problems where (a) a faster nearest neighbour algorithm would have a particularly big pay-off and (b) there is a need to explain why "curse-of-dimensionality" appears not to have damaged the method's competitiveness (see Conjecture 7). Extensions of theory to broader aspects of GP robustness, rates of convergence and "strong calibration" (section 3) are current areas of some the authors' ongoing work.

## Acknowledgments

We would like to thank IBM Research and EPSRC for supplying iCase funding for Anthony Stephenson and the UK National Cyber Security Centre for contributing toward Robert Allison's funding. We also thank the anonymous reviewers of this paper for their constructive comments and suggestions.

Figure 5: Behaviour of performance metrics on the Oakley and O’Hagan function-derived dataset ([20, 30]) as a function of data size, \(n\). The horizontal green line indicates the limiting behaviour if the predictive and generative were to match. Shaded regions indicate 95% confidence intervals for the fitted curves.

## References

* Bachoc et al. [2022] F. Bachoc, N. Durrande, D. Rulliere, and C. Chevalier. Properties and Comparison of Some Kriging Sub-model Aggregation Methods. _Mathematical Geosciences_, 2022.
* Cao and Fleet [2014] Y. Cao and D. J. Fleet. Generalized product of experts for automatic and principled fusion of gaussian process predictions. _arXiv preprint arXiv:1410.7827_, 2014.
* Chalupka et al. [2013] K. Chalupka, C. K. Williams, and I. Murray. A framework for evaluating approximation methods for gaussian process regression. _Journal of Machine Learning Research_, 14:333-350, 2013.
* Cohen et al. [2020] S. Cohen, R. Mbuvha, T. Marwala, and M. Deisenroth. Healing products of Gaussian process experts. In H. D. III and A. Singh, editors, _Proceedings of the 37th International Conference on Machine Learning_, volume 119 of _Proceedings of Machine Learning Research_, pages 2068-2077. PMLR, 2020-13.
* Datta et al. [2016] A. Datta, S. Banerjee, A. O. Finley, and A. E. Gelfand. Hierarchical Nearest-Neighbor Gaussian Process Models for Large Geostatistical Datasets. _Journal of the American Statistical Association_, 111(514):800-812, 2016.
* Datta et al. [2016] A. Datta, S. Banerjee, A. O. Finley, and A. E. Gelfand. On nearest-neighbor Gaussian process models for massive spatial data. _Wiley Interdisciplinary Reviews: Computational Statistics_, 8(5):162-171, 2016.
* Deisenroth and Ng [2015] M. Deisenroth and J. W. Ng. Distributed gaussian processes. In _International Conference on Machine Learning_, pages 1481-1490. PMLR, 2015.
* Finley et al. [2019] A. O. Finley, A. Datta, B. D. Cook, D. C. Morton, H. E. Andersen, and S. Banerjee. Efficient algorithms for bayesian nearest neighbor gaussian processes. _Journal of Computational and Graphical Statistics_, 28(2):401-414, 2019. PMID: 31543693.
* Friedman et al. [1977] J. Friedman, J. Bentley, and R. Finkel. An Algorithm for Finding Best Matches in Logarithmic Expected Time. _ACM Transactions on Mathematical Software (TOMS)_, 3:209-226, 1977.
* Gardner et al. [2018] J. Gardner, G. Pleiss, K. Q. Weinberger, D. Bindel, and A. G. Wilson. Gpytorch: Blackbox matrix-matrix gaussian process inference with gpu acceleration. _Advances in neural information processing systems_, 31, 2018.
* Gardner et al. [2018] J. R. Gardner, G. Pleiss, R. Wu, K. Q. Weinberger, and A. G. Wilson. Product kernel interpolation for scalable gaussian processes. _International Conference on Artificial Intelligence and Statistics, AISTATS 2018_, 84:1407-1416, 2018.
* Gyorfi et al. [2010] L. Gyorfi, M. Kohler, A. Krzyzak, and H. Walk. _A Distribution-Free Theory of Nonparametric Regression_. Springer Series in Statistics, 2010.
* Hayashi et al. [2020] K. Hayashi, M. Imaizumi, and Y. Yoshida. On random subsampling of gaussian process regression: A graphon-based analysis. In _International Conference on Artificial Intelligence and Statistics_, pages 2055-2065. PMLR, 2020.
* Hensman et al. [2013] J. Hensman, N. Fusi, and N. D. Lawrence. Gaussian processes for big data. _arXiv preprint arXiv:1309.6835_, 2013.
* Hinton [2002] G. E. Hinton. Training products of experts by minimizing contrastive divergence. _Neural computation_, 14(8):1771-1800, 2002.
* Jankowiak et al. [2020] M. Jankowiak, G. Pleiss, and J. Gardner. Parametric Gaussian process regressors. In H. D. III and A. Singh, editors, _Proceedings of the 37th International Conference on Machine Learning_, volume 119 of _Proceedings of Machine Learning Research_, pages 4702-4712. PMLR, 13-18 Jul 2020.
* Lalchand and Rasmussen [2020] V. Lalchand and C. E. Rasmussen. Approximate Inference for Fully Bayesian Gaussian Process Regression, Apr. 2020. arXiv:1912.13440 [cs, stat].

* [18] H. Liu, J. Cai, Y. Wang, and Y. S. Ong. Generalized robust bayesian committee machine for large-scale gaussian process regression. In _International Conference on Machine Learning_, pages 3131-3140. PMLR, 2018.
* [19] H. Liu, Y.-S. Ong, X. Shen, and J. Cai. When gaussian process meets big data: A review of scalable gps. _IEEE transactions on neural networks and learning systems_, 31(11):4405-4423, 2020.
* [20] J. E. Oakley and A. O'Hagan. Probabilistic sensitivity analysis of complex models: A bayesian approach. _Journal of the Royal Statistical Society. Series B (Statistical Methodology)_, 66(3):751-769, 2004.
* [21] S. Omohundro. Five Balltree Construction Algorithms. Technical report, International Computer Science Institute, 1989.
* [22] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. _Journal of Machine Learning Research_, 12:2825-2830, 2011.
* [23] B. Scholkopf. The Kernel Trick for Distances. In T. Leen, T. Dietterich, and V. Tresp, editors, _Advances in Neural Information Processing Systems_, volume 13. MIT Press, 2000.
* [24] H. Song, T. Diethe, M. Kull, and P. Flach. Distribution calibration for regression. _36th International Conference on Machine Learning, ICML 2019_, 2019-June:10347-10356, 2019.
* [25] M. L. Stein. The screening effect in kriging. _Annals of Statistics_, 30(1):298-323, 2002.
* [26] M. L. Stein. 2010 Rietz lecture when does the screening effect hold? _Annals of Statistics_, 39(6):2795-2819, 2011. arXiv: 1203.1801v1.
* [27] M. L. Stein. Limitations on low rank approximations for covariance matrices of spatial data. _Spatial Statistics_, 8:1-19, 2014. Spatial Statistics Miami.
* [28] M. L. Stein, Z. Chi, and L. J. Welty. Approximating likelihoods for large spatial data sets. _Journal of the Royal Statistical Society. Series B: Statistical Methodology_, 66(2):275-296, 2004.
* [29] A. Stephenson, R. Allison, and E. Pyzer-Knapp. Provably reliable large-scale sampling from gaussian processes. _arXiv preprint arXiv:2211.08036_, 2022.
* [30] S. Surjanovic and D. Bingham. Virtual library of simulation experiments: Test functions and datasets. Retrieved May 5, 2023, from http://www.sfu.ca/~ssurjano.
* [31] M. Titsias. Variational learning of inducing variables in sparse gaussian processes. In _Artificial intelligence and statistics_, pages 567-574. PMLR, 2009.
* [32] G.-L. Tran, D. Milios, P. Michiardi, and M. Filippone. Sparse within sparse gaussian processes using neighbor information. In M. Meila and T. Zhang, editors, _Proceedings of the 38th International Conference on Machine Learning_, volume 139 of _Proceedings of Machine Learning Research_, pages 10369-10378. PMLR, 2021-18.
* [33] V. Tresp. A bayesian committee machine. _Neural computation_, 12(11):2719-2741, 2000.
* [34] A. V. Vecchia. Estimation and Model Identification for Continuous Spatial Processes. _Journal of the Royal Statistical Society. Series B (Methodological)_, 50(2):297-312, 1988. Publisher: [Royal Statistical Society, Wiley].
* [35] K. Wang, G. Pleiss, J. Gardner, S. Tyree, K. Q. Weinberger, and A. G. Wilson. Exact gaussian processes on a million data points. _Advances in Neural Information Processing Systems_, 32, 2019.
* [36] C. K. Williams and C. E. Rasmussen. _Gaussian processes for machine learning_, volume 2. MIT press Cambridge, MA, 2006.

* [37] A. G. Wilson and H. Nickisch. Kernel interpolation for scalable structured Gaussian processes (KISS-GP). _32nd International Conference on Machine Learning, ICML 2015_, 3:1775-1784, 2015.
* [38] L. Wu, G. Pleiss, and J. Cunningham. Variational Nearest Neighbor Gaussian Process. _Proceedings of the 39th International Conference on Machine Learning_, 2022.
* [39] M. Yadav, D. Sheldon, and C. Musco. Faster kernel interpolation for gaussian processes. In A. Banerjee and K. Fukumizu, editors, _Proceedings of The 24th International Conference on Artificial Intelligence and Statistics_, volume 130 of _Proceedings of Machine Learning Research_, pages 2971-2979. PMLR, 13-15 Apr 2021.

Theoretical GPnn Results

### Preliminary results

Let \(\rho(\bm{x},\bm{x}^{\prime})=\sigma_{f}\sqrt{1-c(\bm{x}/l,\bm{x}^{\prime}/l)}\) be the kernel-induced distance function over \(\mathbb{R}^{d}\) ([23]). We define \(\mathbf{x}_{(j,n)}(\bm{x}^{*})\) as the \(j^{th}\) nearest neighbour random variable to a test point \(\bm{x}^{*}\) under \(\rho\), which we abbreviate to \(\mathbf{x}_{(j)}\) when the context is clear, and \(\bm{x}_{(j)}(\bm{x}^{*})\in N_{m}(\bm{x}^{*})\) as the realised \(j^{th}\) nearest neighbour of the test point \(\bm{x}^{*}\) from a training set \(X\). From this we define \(\epsilon_{i}=\rho^{2}(\mathbf{x}_{(i)},\bm{x}^{*})\) and \(\epsilon_{ij}=\rho^{2}(\mathbf{x}_{(i)},\mathbf{x}_{(j)})\).

**Definition 9** (Support).: _Let \(P_{\mathbf{x}}\) be the probability measure of \(\mathbf{x}\) and \(S^{\rho}_{\bm{x},\epsilon}\) the closed ball of radius \(\epsilon>0\) under the metric \(\rho\) centred at \(\bm{x}\). Then we define \(\mathrm{support}(P_{\mathbf{x}})=\{\bm{x}:P_{\mathbf{x}}(S^{\rho}_{\bm{x}, \epsilon})>0\,\forall\,\epsilon>0\}\)._

**Definition 10** (Weakly-faithful).: _We define a pair of metrics \(\rho(\cdot,\cdot),\hat{\rho}(\cdot,\cdot)\) to be weakly-faithful w.r.t. each other if the following condition holds: The \(m^{th}\) nearest neighbour under \(\hat{\rho}\) converges to the test point as \(n\to\infty\) if and only if the \(m^{th}\) nearest neighbour under \(\rho\) converges to the test point in the limit._

Assumptions 1. \(\mathbf{x}\stackrel{{\text{\rm iid}}}{{\sim}}P_{\mathbf{x}}\) and \(\mathbf{x}^{*}\in\mathrm{support}(P_{\mathbf{x}})\) under the generative metric defined by \(c(\cdot,\cdot)\). 2. \(c(\cdot,\cdot),\hat{c}(\cdot,\cdot)\) are stationary kernels whose induced distance functions are _weakly faithful_ metrics (Definition 10). 3. \(y_{i}=f(\bm{x}_{i})+\xi_{i}\) with \(\xi_{i}\stackrel{{\text{\rm iid}}}{{\sim}}P_{\xi}\), \(f(\bm{x})\sim\mathcal{WSRF}(\sigma_{f}^{2}c(./l,./l))\) and \(y_{i}\,|\,f(\bm{x}_{i})\sim P_{\xi}\) and \(\mathrm{E}[\xi]=0\), \(\mathrm{E}[\xi^{2}]=\sigma_{\xi}^{2}\).

**Note:** Assumption (A2) is not overly restrictive and encompasses commonly used kernels such as all those mentioned in this paper.

**Lemma 11**.: \(\epsilon_{i}\to 0\) _and \(\epsilon_{ij}\to 0\) as \(n\to\infty\) a.e. with respect to the measure over \(\mathbf{x}\in\mathbb{R}^{d}\), \(P_{\mathbf{x}}\), for \(i,j\leq m\), \(\frac{m}{n}\to 0\) and under (A1-2)._

Proof.: Lemma 6.1 of [12] states that \(\left\|\mathbf{x}_{(m,n)}(\bm{x})-\bm{x}\right\|\xrightarrow{n\to\infty}0\) with probability one (with respect to \(P_{\mathbf{x}}\)). Their proof can be generalised immediately to state that \(\rho(\mathbf{x}_{(m,n)}(\bm{x}),\bm{x})\xrightarrow{n\to\infty}0\) by using our definition of support, 9, that directly invokes the metric \(\rho\). Hence \(\epsilon_{i}\to 0\) for all \(i\leq m\) (since \(\bm{x}^{*}\) is in \(\mathrm{support}(P_{\mathbf{x}})\)). Since \(\rho\) is a metric it satisfies the triangle inequality; hence \(\rho(\mathbf{x}_{(i)},\mathbf{x}_{(j)})\leq\rho(\mathbf{x}_{(i)},\bm{x}^{*}) +\rho(\mathbf{x}_{(j)},\bm{x}^{*})\xrightarrow{n\to\infty}0\) for all \(i,j\leq m\). 

**Lemma 12**.: _For an \(m\)-GPnn under the assumptions (A1-3),_

\[\lim_{n\to\infty}\bm{k}_{N}^{\star T}K_{N}^{-1}\bm{k}_{N}^{\star}=\sigma_{f}^ {2}-\sigma_{\xi}^{2}m^{-1}+\mathcal{O}\!\left(m^{-2}\right).\]

Proof.: From Lemma 11 we have that \(\lim_{n\to\infty}k(\mathbf{x}_{(j)}(\bm{x}^{*}),\bm{x}^{*})=\lim_{n\to\infty}( \sigma_{f}^{2}-\epsilon_{i})=\sigma_{f}^{2}\) and \(\lim_{n\to\infty}k(\mathbf{x}_{(i)}(\bm{x}^{*}),\mathbf{x}_{(j)}(\bm{x}^{*}))= \lim_{n\to\infty}(\sigma_{f}^{2}-\epsilon_{ij})=\sigma_{f}^{2}\). As a result, \(\bm{k}_{N}^{\star}\to\sigma_{f}^{2}\mathbf{1}\) and

\[K^{\infty}:=\lim_{n\to\infty}K_{N}=\sigma_{\xi}^{2}I+\sigma_{f}^{2}\mathbf{1} \mathbf{1}^{T}.\] (8)Now using Sherman-Morrison and the continuity of matrix inverse and matrix-matrix products:

\[(A+\bm{bc}^{T})^{-1} =A^{-1}-\frac{A^{-1}\bm{bc}^{T}A^{-1}}{1+\bm{c}^{T}A^{-1}\bm{b}}\] (9) \[(K^{\infty})^{-1} =(\sigma_{\xi}^{2}I+\sigma_{f}^{2}\bm{1}\bm{1}^{T})^{-1} =\frac{1}{\sigma_{\xi}^{2}}\left(I-\sigma_{f}^{2}\frac{\bm{1}\bm{1 }^{T}}{\sigma_{\xi}^{2}+\sigma_{f}^{2}\bm{1}^{T}\bm{1}}\right)\] (10) \[\bm{1}^{T}(K^{\infty})^{-1}\bm{1} =\frac{m}{\sigma_{\xi}^{2}}\left(1-\frac{m\sigma_{f}^{2}}{\sigma_ {\xi}^{2}+m\sigma_{f}^{2}}\right)\] \[=\frac{m}{\sigma_{\xi}^{2}}\left(1-m\sigma_{f}^{2}\frac{1}{m \sigma_{f}^{2}}\left(1-\frac{\sigma_{\xi}^{2}}{m\sigma_{f}^{2}}+\frac{\sigma_{ \xi}^{4}}{m^{2}\sigma_{f}^{4}}-\mathcal{O}\big{(}m^{-3}\big{)}\right)\right)\] \[=\frac{1}{\sigma_{f}^{2}}-\frac{\sigma_{\xi}^{2}}{m\sigma_{f}^{4 }}+\mathcal{O}\big{(}m^{-2}\big{)}\,.\] (11)

Thus,

\[\lim_{n\to\infty}\bm{k}_{N}^{*T}K_{N}^{-1}\bm{k}_{N}^{*}=\sigma_{f}^{4}\bm{1}^ {T}(K^{\infty})^{-1}\bm{1}=\sigma_{f}^{2}-\sigma_{\xi}^{2}m^{-1}+\mathcal{O} \big{(}m^{-2}\big{)}\,.\] (12)

**Lemma 13** (\(\mathcal{WSRF}\) expectations).: _Under (A3), \(\mathrm{E}_{\bm{y},y^{*}}\) and \(\mathrm{E}_{\bm{y}}\{\bm{y}\bm{y}^{T}\}=K\)._

Proof.: By assumption on the covariance properties of \(y\) and the independence and zero-mean of the additive noise, \(\mathrm{E}_{\bm{y}}\{y_{i}y_{j}\}=k(\bm{x}_{i},\bm{x}_{j})\). Extending this to the joint distribution over \(\bm{y},y^{*}\) is straightforward and gives the results stated. 

Lemma 13 is subsequently assumed to be in use throughout A.2.

### Limit proofs

In the following statements only misspecification of type (d) and/or (e) (subsection 4.2) is considered to be at work.

**Lemma 14** (MSE limit).: _Under the assumptions (A1-3), for fixed \(m<\infty\), the predictive GPnn given in subsection 4.1 converges pointwise in the sense of MSE wrt \(P_{\bm{\mathsf{x}}}\)-a.e. as_

\[\lim_{n\to\infty}f_{n}^{\mathrm{MSE}}(\bm{\theta})=\sigma_{\xi}^{2}(1+m^{-1}) -\mathcal{O}\big{(}m^{-2}\big{)}\,.\]

Proof.: This follows from Lemma 12 by expanding the definition of \(\mathrm{MSE}\):

\[\lim_{n\to\infty}f_{n}^{\mathrm{MSE}}(\bm{\theta}) =\lim_{n\to\infty}\mathrm{E}_{\bm{y},y^{*}}\left\{|y^{*}-\mu_{N}^{ *}|^{2}\right\}\] \[=\lim_{n\to\infty}\left[\mathrm{E}_{y^{*}}\{y^{*}{}^{2}\}+ \mathrm{E}_{\bm{y}}\{\mu_{N}^{*}{}^{2}\}-2\,\mathrm{E}_{\bm{y},y^{*}}\{\bm{k}_ {N}^{*T}K_{N}^{-1}\bm{y}_{N}y^{*}\}\right]\] \[=\sigma_{f}^{2}+\sigma_{\xi}^{2}-\lim_{n\to\infty}\mathrm{E}_{\bm {y}}\{\mu_{N}^{*}{}^{2}\}\] \[=\sigma_{\xi}^{2}(1+m^{-1})-\mathcal{O}\big{(}m^{-2}\big{)}\,.\]

Since \(\mathrm{E}_{\bm{y}}\{\mu_{N}^{*}{}^{2}\}=\mathrm{E}_{\bm{y}}\{\bm{k}_{N}^{*T}K_ {N}^{-1}\bm{y}_{N}\bm{y}_{N}^{T}K_{N}^{-1}\bm{k}_{N}^{*}\}=\bm{k}_{N}^{*T}K_{N}^ {-1}\bm{k}_{N}^{*}\) and by assumption \(\mathrm{E}_{\bm{y},y^{*}}\{\bm{y}_{N}y^{*}\}=\bm{k}_{N}^{*}\) even under a \(\mathcal{WSRF}\) generative model (Lemma 13). 

**Corollary 15** (NLL limit).: \[\lim_{n\to\infty}f_{n}^{\mathrm{NLL}}(\bm{\theta})=\frac{1}{2}\log\!\big{(} \sigma_{\xi}^{2}(1+m^{-1})\big{)}+\frac{1}{2}+\frac{1}{2}\log 2\pi-\mathcal{O} \big{(}m^{-2}\big{)}\,.\]Proof.: The proof follows straightforwardly from Lemma 12 and because \(\sigma_{N}^{*\,2}=\sigma_{f}^{2}+\sigma_{\xi}^{2}-\boldsymbol{k}_{N}^{*^{T}}K_{N} ^{-1}\boldsymbol{k}_{N}^{*}\).

\[2\,\mathrm{E}_{\mathbf{y},\mathbf{y}^{*}}\left\{l_{N}^{*}\right\} =\mathrm{E}_{\mathbf{y},\mathbf{y}^{*}}\left\{\log\sigma_{N}^{*\,2 }+\frac{(y^{*}-\mu_{N}^{*})^{2}}{\sigma_{N}^{*\,2}}+\log 2\pi\right\}\] \[=\log\sigma_{N}^{*\,2}+1+\log 2\pi\] \[\lim_{n\to\infty}2\,\mathrm{E}_{\mathbf{y},\mathbf{y}^{*}}\left\{l _{N}^{*}\right\} =\log\bigl{(}\sigma_{f}^{2}+\sigma_{\xi}^{2}-(\sigma_{f}^{2}- \sigma_{\xi}^{2}m^{-1}+\mathcal{O}\bigl{(}m^{-2}\bigr{)})\bigr{)}+1+\log 2\pi\] \[=\log\bigl{(}\sigma_{\xi}^{2}(1+m^{-1})-\mathcal{O}\bigl{(}m^{-2} \bigr{)}\bigr{)}+1+\log 2\pi\] \[=\log\sigma_{\xi}^{2}+m^{-1}+1+\log 2\pi-\mathcal{O}\bigl{(}m^{-2} \bigr{)}\,.\]

#### a.2.1 Full misspecification

For the remainder of A.2 we assume that the full range of possible misspecifications ((a)-(e)) outlined in subsection 4.2 are in action. We refer to this case as "fully-misspecified" and introduce the notation \(\hat{\mu}_{N}^{*},\hat{\sigma}_{N}^{*\,2}\) to be understood to mean the predictive mean and variance under these misspecifications.

**Lemma 16** (Fully misspecified MSE limit).: _For a fully misspecified model, asymptotically_

\[\lim_{n\to\infty}f_{n}^{\mathrm{MSE}}(\hat{\boldsymbol{\theta}})=\sigma_{\xi} ^{2}(1+m^{-1})\pm\mathcal{O}\bigl{(}m^{-2}\bigr{)}\,.\]

_provided the misspecified kernel distance metric is weakly faithful in the sense that the \(m^{th}\) nearest neighbour converges under both the true and misspecified metrics (Definition 10)._

Proof.: \[\mathrm{E}_{\mathbf{y}}\left\{\,\mathrm{E}_{\mathbf{y}^{*}}\left[ (y^{*}-\hat{\mu}_{N}^{*})^{2}\,|\,\mathbf{y}\right]\right\} =\mathrm{E}_{\mathbf{y}}\left\{\,\mathrm{E}_{\mathbf{y}^{*}}\left[ y^{*\,2}-2y^{*}\hat{\mu}_{N}^{*}+(\hat{\mu}_{N}^{*})^{2}\,|\,\mathbf{y}\right]\,\right\}\] \[=\mathrm{E}_{\mathbf{y}}\left\{\sigma_{N}^{*\,2}+\boldsymbol{k}_{ N}^{*\,T}K_{N}^{-1}\boldsymbol{k}_{N}^{*}-2\,\boldsymbol{k}_{N}^{*\,T}\hat{K}_{N} ^{-1}\hat{\boldsymbol{k}}_{N}^{*}+\,\hat{\boldsymbol{k}}_{N}^{*\,T}\hat{K}_{N} ^{-1}K_{N}\hat{K}_{N}^{-1}\hat{\boldsymbol{k}}_{N}^{*}\right\}\!\!.\]

We can use standard results to state that \((a)+(b)=\sigma_{f}^{2}+\sigma_{\xi}^{2}\). Then we define \(\hat{\gamma}=\frac{\hat{\sigma}_{f}^{2}}{\hat{\sigma}_{\xi}^{2}+m\hat{\sigma} _{f}^{2}}\) and expand it in terms of \(m^{-1}\):

\[1-m\hat{\gamma}=\frac{\hat{\sigma}_{\xi}^{2}}{m\hat{\sigma}_{f}^{2}}-\frac{\hat {\sigma}_{\xi}^{4}}{m^{2}\hat{\sigma}_{f}^{4}}+\mathcal{O}\bigl{(}m^{-3}\bigr{)}\,.\]

In a manner similar to Lemma 12 we use this result to compute:

\[\lim_{n\to\infty}(c) =\sigma_{f}^{2}\boldsymbol{1}^{T}\hat{\sigma}_{\xi}^{-2}(I-\hat{ \gamma}\boldsymbol{1}\boldsymbol{1}^{T})\boldsymbol{1}\hat{\sigma}_{f}^{2}\] \[=\frac{\sigma_{f}^{2}\hat{\sigma}_{f}^{2}}{\hat{\sigma}_{\xi}^{2} }m(1-m\hat{\gamma})\] \[=\frac{\sigma_{f}^{2}\hat{\sigma}_{f}^{2}}{\hat{\sigma}_{\xi}^{2} }\left(\frac{\hat{\sigma}_{\xi}^{2}}{\hat{\sigma}_{f}^{2}}-\frac{\hat{\sigma}_{ \xi}^{4}}{m\hat{\sigma}_{f}^{4}}\right)+\mathcal{O}\bigl{(}m^{-2}\bigr{)}\] \[=\sigma_{f}^{2}-\frac{\sigma_{f}^{2}\hat{\sigma}_{f}^{2}}{m\hat{ \sigma}_{f}^{2}}+\mathcal{O}\bigl{(}m^{-2}\bigr{)}\]and

\[\lim_{n\to\infty}(d) =\frac{\hat{\sigma}_{f}^{4}}{\hat{\sigma}_{\xi}^{4}}\mathbf{1}^{T} (I-\hat{\gamma}\mathbf{1}\mathbf{1}^{T})(\sigma_{\xi}^{2}I+\sigma_{f}^{2} \mathbf{1}\mathbf{1}^{T})(I-\hat{\gamma}\mathbf{1}\mathbf{1}^{T})\mathbf{1}\] \[=\frac{\hat{\sigma}_{f}^{4}}{\hat{\sigma}_{\xi}^{4}}\mathbf{1}^{T }\left[\sigma_{\xi}^{2}I+\sigma_{f}^{2}\mathbf{1}\mathbf{1}^{T}-2\sigma_{\xi}^{ 2}\hat{\gamma}\mathbf{1}\mathbf{1}^{T}+\hat{\gamma}^{2}\sigma_{\xi}^{2}m \mathbf{1}\mathbf{1}^{T}-2\sigma_{f}^{2}\hat{\gamma}m\mathbf{1}\mathbf{1}^{T}+ \sigma_{f}^{2}\hat{\gamma}^{2}m^{2}\mathbf{1}\mathbf{1}^{T}\right]\mathbf{1}\] \[=\frac{\hat{\sigma}_{f}^{4}}{\hat{\sigma}_{\xi}^{4}}m(\sigma_{ \xi}^{2}+m\sigma_{f}^{2})\left[1-2m\hat{\gamma}+m^{2}\hat{\gamma}^{2}\right]\] \[=\frac{\hat{\sigma}_{f}^{4}}{\hat{\sigma}_{\xi}^{4}}m(\sigma_{ \xi}^{2}+m\sigma_{f}^{2})(1-m\hat{\gamma})^{2}\] \[=\frac{\hat{\sigma}_{f}^{4}}{\hat{\sigma}_{\xi}^{4}}m(\sigma_{ \xi}^{2}+m\sigma_{f}^{2})\left(\frac{\hat{\sigma}_{\xi}^{4}}{m^{2}\hat{\sigma} _{f}^{4}}-2\frac{\hat{\sigma}_{\xi}^{6}}{m^{3}\hat{\sigma}_{f}^{6}}+\mathcal{O }\big{(}m^{-4}\big{)}\right)\] \[=\sigma_{f}^{2}+\frac{\sigma_{\xi}^{2}}{\hat{\sigma}_{f}^{2}}-2 \frac{\sigma_{f}^{2}}{\hat{\sigma}_{f}^{2}}\frac{\hat{\sigma}_{\xi}^{2}}{m} \pm\mathcal{O}\big{(}m^{-2}\big{)}\,,\]

where we have used the expansion of \(1-m\hat{\gamma}\) given earlier. Putting these results together gives

\[\lim_{n\to\infty}f_{n}^{\mathrm{MSE}}(\hat{\boldsymbol{\theta}}) =\lim_{n\to\infty}[(a)+(b)-2(c)+(d)]\] \[=\sigma_{f}^{2}+\sigma_{\xi}^{2}-2\left(\sigma_{f}^{2}-\frac{ \sigma_{f}^{2}\hat{\sigma}_{\xi}^{2}}{m\hat{\sigma}_{f}^{2}}\right)+\sigma_{ f}^{2}+\frac{\sigma_{\xi}^{2}}{m}-2\frac{\sigma_{f}^{2}}{\hat{\sigma}_{f}^{2}} \frac{\hat{\sigma}_{\xi}^{2}}{m}\pm\mathcal{O}\big{(}m^{-2}\big{)}\] \[=\sigma_{\xi}^{2}(1+m^{-1})\pm\mathcal{O}\big{(}m^{-2}\big{)}\,.\qed\]

**Lemma 17** (Calibration limit under full misspecification).: \[\lim_{n\to\infty}f_{n}^{\mathrm{CAL}}(\hat{\boldsymbol{\theta}})=\frac{ \sigma_{\xi}^{2}}{\hat{\sigma}_{\xi}^{2}}\pm\mathcal{O}\big{(}m^{-2}\big{)}\,.\]

Proof.: We use continuity to write

\[\lim_{n\to\infty}\mathrm{E}_{\mathbf{y},{\color[rgb]{0,0,0}\definecolor[ named]{pgfstrokecolor}{rgb}{0,0,0}\pgfsys@color@gray@stroke{0}\pgfsys@color@gray@fill{0}y^{*}}}\left\{ \frac{(y^{*}-\hat{\mu}_{N}^{*})^{2}}{\hat{\sigma}_{N}^{*}2}\right\}=\left(\lim_ {n\to\infty}\frac{1}{\hat{\sigma}_{N}^{*}2}\right)\left(\lim_{n\to\infty}f_{ n}^{\mathrm{MSE}}(\hat{\boldsymbol{\theta}})\right).\]

By direct application of Lemma 12\(\hat{\sigma}_{N}^{*}2\xrightarrow{n\to\infty}\hat{\sigma}_{\xi}^{2}(1+m^{-1})- \mathcal{O}\big{(}m^{-2}\big{)}\) and thus

\[\lim_{n\to\infty}f_{n}^{\mathrm{CAL}}(\hat{\boldsymbol{\theta}})=\frac{ \sigma_{\xi}^{2}}{\hat{\sigma}_{\xi}^{2}}\pm\mathcal{O}\big{(}m^{-2}\big{)}\,.\]

**Corollary 18** (NLL limit under full misspecification).: \[\lim_{n\to\infty}f_{n}^{\mathrm{NLL}}(\hat{\boldsymbol{\theta}})=\frac{1}{2} \log\big{(}\hat{\sigma}_{\xi}^{2}(1+m^{-1})\big{)}+\frac{1}{2}\frac{\sigma_{ \xi}^{2}}{\hat{\sigma}_{\xi}^{2}}+\frac{1}{2}\log 2\pi\pm\mathcal{O}\big{(}m^{-2}\big{)}\,.\]

Proof.: We start with

\[2f_{n}^{\mathrm{NLL}}(\hat{\boldsymbol{\theta}})=\mathrm{E}_{\mathbf{y},{ \color[rgb]{0,0,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0} \pgfsys@color@gray@stroke{0}\pgfsys@color@gray@fill{0}y^{*}}}\left\{\log\hat{ \sigma}_{N}^{*}2+\frac{(y^{*}-\hat{\mu}_{N}^{*})^{2}}{\hat{\sigma}_{N}^{*}2} +\log 2\pi\right\}.\]

For the second term we use Lemma 17 so that we have

\[\lim_{n\to\infty}2f_{n}^{\mathrm{NLL}}(\hat{\boldsymbol{\theta}})=\log\hat{ \sigma}_{\xi}^{2}+m^{-1}+\frac{\sigma_{\xi}^{2}}{\hat{\sigma}_{\xi}^{2}}+\log 2\pi\pm \mathcal{O}\big{(}m^{-2}\big{)}\,.\]

Proof of Theorem 1.: We construct the proof using all of the intermediate results given above. In particular item (i) follows from Lemma 16, item (ii) from Lemma 17 and item (iii) from Corollary 18.

Validity of Algorithm 1 (Proof of Lemma 4)

Proof of Lemma 4.: We prove the result for MSE, the proofs for NLL and calibration being essentially identical.

The proof involves showing that Algorithm 1 is exactly equivalent to Algorithm 1b, an alternative scheme which self-evidently provides valid estimates of MSE on size-\(n\) synthetic datasets:

1. Generate a size-\(n\) set of i.i.d. training \(\bm{x}\)-values \(X=\{\bm{x}_{i}\}_{i=1}^{n}\) with \(\bm{x}_{i}\sim P_{\mathbf{x}}\) (\(X\) is held constant henceforth).
2. Generate \(n^{*}\) i.i.d. test points \(\bm{x}_{i}^{*}\sim P_{\mathbf{x}}\).
3. Generate \(n^{*}\) separate independent synthetic GP samples \(\bm{y}_{i}\in\mathbb{R}^{(n+1)}\) corresponding to each of the values \((\bm{x}_{i}^{*},X)\).
4. From within \(X\) take the \(m\) nearest-neighbours \(N(\bm{x}_{i}^{*})\) to the test point \(\bm{x}_{i}^{*}\).
5. Corresponding to each test point \(\bm{x}_{i}^{*}\) evaluate the function \({e_{i}^{*}}^{\prime}=f_{i}(\bm{y}_{i})=(y_{i}^{*}-\mu_{N}^{*}(\bm{y}_{i}^{ \prime}))^{2}\). Where, as defined in Equation 6, \(\mu_{N}^{*}=\hat{\bm{k}}_{N}^{*}\hat{K}_{N}^{-1}\bm{y}_{i}^{\prime}\) with \(N=N(\bm{x}_{i})\) and where \(\bm{y}_{i}^{\prime}\in\mathbb{R}^{m}\) is formed from the components of \(\bm{y}_{i}\) corresponding to \(N=N(\bm{x}_{i})\).
6. Compute the average \(\frac{1}{n^{*}}\sum_{i=1}^{n^{*}}{e_{i}^{*}}^{\prime}\) to obtain the MSE statistic.

Algorithm 1b clearly provides a valid evaluation of the MSE arising from the full GPnn prediction process described in section 3 (albeit at prohibitive expense for large \(n\)). Hence it is sufficient to prove that Algorithm 1 is equivalent to Algorithm 1b. We do so by applying two minor alterations to Algorithm 1b in succession whose combined effect is to convert it to Algorithm 1. We also show that throughout this process equivalence is maintained with Algorithm 1b thus completing the proof:

Change 1: Let \(\bm{y}_{i}=(y_{i}^{*},\bm{y}_{i}^{\prime},\bm{y}_{i}^{\prime\prime})\) where \((y_{i}^{*},\bm{y}_{i}^{\prime})\in\mathbb{R}^{(m+1)}\) with \(\bm{y}_{i}^{\prime}\) corresponding to the \(\bm{x}\)-values in \(N(\bm{x}_{i}^{*})\). Since each function \(f_{i}(\bm{y}_{i})\) in line 5 above only depends on the components \((y_{i}^{*},\bm{y}_{i}^{\prime})\) of the \((n+1)\)-long vector \(\bm{y}_{i}\) we can equally write \({e_{i}^{*}}^{\prime}=f_{i}(y_{i}^{*},\bm{y}_{i}^{\prime})\) and hence truncate each of the vectors \(\bm{y}_{i}\) to \((y_{i}^{*},\bm{y}_{i}^{\prime})\) before evaluating \({e_{i}^{*}}^{\prime}\) in 5 and this clearly leaves the output of Algorithm 1b completely unchanged.

Change 2: Note (by a standard property of multivariate normal distributions) that each of the truncated vectors \((y_{i}^{*},\bm{y}_{i}^{\prime})\) are i.i.d. from the multivariate normal distribution whose covariance \(\Sigma\) is the kernel gram matrix corresponding to just \((\bm{x}_{i}^{*},N(\bm{x}_{i}^{*}))\). Hence it is legitimate to change the (inefficient) way of sampling \((y_{i}^{*},\bm{y}_{i}^{\prime})\) (via the large sample \(\bm{y}\)) to direct sampling from \(\mathcal{N}(\bm{0},\Sigma)\) whilst still maintaining equivalence with Algorithm 1b.

In this way we arrive at Algorithm 1. 

## Appendix C Parameter Calibration (Proof of Lemma 5)

Proof of Lemma 5.: (a) Replacing parameters \(\hat{\bm{\theta}}=(\hat{l},\hat{\sigma}_{\xi}^{2},\hat{\sigma}_{f}^{2})\) with \(\hat{\bm{\theta}}^{\prime}=(\hat{l},\alpha\hat{\sigma}_{\xi}^{2},\alpha\hat{ \sigma}_{f}^{2})\) changes all of the \({\sigma_{i}^{*}}^{2}\) values to \(\alpha{\sigma_{i}^{*}}^{2}\) and therefore changes the calibration value on \(C\) from \(\alpha=\frac{1}{c}\sum_{i=1}^{c}\frac{(y_{i}^{*}-\mu_{i}^{*})^{2}}{{\sigma_{i}^ {*}}^{2}}\) to \(\alpha/\alpha=1\). (b) The NLL on \(C\) arising from parameters \((\hat{l},\alpha\hat{\sigma}_{\xi}^{2},\alpha\hat{\sigma}_{f}^{2})\) is \(\frac{1}{2c}\sum_{i=1}^{c}\{\log\Bigl{(}\alpha\hat{\sigma}_{\xi}^{2}\Bigr{)}+(y _{i}^{*}-\mu_{i}^{*})^{2}/(\alpha{\sigma_{i}^{*}}^{2})+\log 2\pi)\}\) which, on taking first and second derivatives w.r.t. \(\alpha\), is found to be uniquely minimised by \(\alpha=\frac{1}{c}\sum_{i=1}^{c}\frac{(y_{i}^{*}-\mu_{i}^{*})^{2}}{{\sigma_{i}^ {*}}^{2}}\). (c) It is easily shown that replacing parameters \((\hat{\sigma}_{\xi}^{2},\hat{\sigma}_{f}^{2})\) by \((k\hat{\sigma}_{\xi}^{2},k\hat{\sigma}_{f}^{2})\) (for any \(k>0\)) in the formula for \(\mu^{*}\) (Equation 3 and Equation 6) does not alter \(\mu^{*}\). Hence the value of \(\text{MSE}=\frac{1}{n^{*}}\sum_{i=1}^{n^{*}}(y_{i}^{*}-\mu_{i}^{*})^{2}\) on any size-\(n^{*}\) test set is unchanged when parameters \(\hat{\bm{\theta}}^{\prime}\) are used in place of \(\hat{\bm{\theta}}\). 

## Appendix D Real world datasets

We consider a variety of datasets from the standard UCI machine learning repository2. These datasets are commonly used in the GP literature (see [10] for instance) and are, in principle, easily available online. In practice, we encountered some difficulties: the dataset documentation is often limited; the dataset names commonly used in other published papers do not always match the UCI database naming and important details about data pre-processing, which features to use etc, are often omitted. There are numerous attempts on GitHub and elsewhere at cataloguing these datasets along with any pre-processing, however we had limited success using them, with many appearing unmaintained. Our focus in this work is on testing our methods on a variety of real world datasets and in a way that is, as far as possible, consistent with other papers. We therefore rejected datasets about which there is ambiguity over the correct features to use, or even which column to regress on or for which outlier rejection is required but undocumented elsewhere.

Footnote 2: https://archive-beta.ics.uci.edu, accessed April 2023.

Referring to the datasets used in [10], we were able to locate the following:

* Song (https://archive.ics.uci.edu/ml/machine-learning-databases/00203/YearPredictionMSD.txt.zip)
* Bike (https://archive.ics.uci.edu/ml/machine-learning-databases/00275/Bike-Sharing-Dataset.zip)
* Poletele (https://archive.ics.uci.edu/ml/machine-learning-databases/parkinsons/telemonitoring/parkinsons_updrs.data)
* Kegdirected (https://archive.ics.uci.edu/ml/machine-learning-databases/00220/Relation%20Network%20(Directed).data)
* Keggundirected machine-learning-databases/00221/Reaction%20Network%20(Undirected).data)
* CTSlice (https://archive.ics.uci.edu/ml/machine-learning-databases/00206/slice_localization_data.zip)
* Road3d (https://archive.ics.uci.edu/ml/machine-learning-databases/00246/3D_spatial_network.txt)
* Protein (https://archive.ics.uci.edu/ml/machine-learning-databases/00265/CASP.csv)
* Buzz (https://archive.ics.uci.edu/ml/machine-learning-databases/00248/regression.tar.gz)
* HouseElectric (HouseE) (https://archive-beta.ics.uci.edu/dataset/235/individual+household+electric+power+consumption)

We were unable to find any documentation on the Kegg datasets to indicate which of the columns should be used as the independent variable (the regressor) and neither is this mentioned in any literature of which we are aware. Initial runs of standard exact GP training and prediction produced RMSEs much higher that reported in [10]. Combining these two observations, we chose to exclude both Kegg datasets. Likewise we faced problems with Buzz. An analysis of the \(y\) values revealed a small proportion of extremely large outliers that we found could unduly distort performance results (e.g. depending on whether these outliers appeared in the test set for some of the random splits). With the lack of documentation we were unable to identify an outlier rejection scheme that we were confident would be consistent with results quoted in other papers. For this reason we have excluded Buzz.

The choice of \((\bm{x},y)\) value that we applied for each of the used datasets is as follows:* Song. The first column is \(y\), all remaining columns are \(\bm{x}\).
* Bike. We use hour.csv. The \(y\) value is cnt. dteday (the date) is transformed to just be the integer representation of the day. instant is just an index so is dropped. registered and casual are dropped as registered \(+\) casual \(=\) cnt.
* Poletele. The \(y\) value is total_UPDRS. The columns subject# and test_time are not relevant to the problem so are dropped.
* CTSlice. \(y\) value is the final column. The first column is dropped as it is just an index. We additionally drop six columns which are constant over the majority of the dataset, namely columns 59, 69, 179, 189, 279 and 351.
* Road3d. \(y\) value is the final column. The first column is dropped as it is just an index.
* Protein. This dataset was processed as per https://github.com/hughsalimbeni/bayesian_benchmarks, whereafter we used our own random (seeded) train/test split.
* HouseElectric. \(y\) value is the column labelled "Global active power", rescaled by \(1000/60\) and with "Sub metering 1,2,3" columns subtracted. We convert the date column into day-of-year/365 and the time column into time of day in minutes. Further, we remove any rows with null entries.

We note that although we are using a standard set of real-world datasets, it is not always clear exactly how others in the field have carried out their own preprocessing, limiting the ability to make direct comparisons to other results reported in the literature.

## Appendix E Additional Implementation Details

### Pre-whitening of Data

For all datasets covered in subsection 7.1 the following "whitening" preprocessing step is adopted: Let \(\bm{y}\) be the vector of all regressor values in the _training_ dataset only, and \(X\) the matrix of all regressands in the _training_ dataset only, where each row of \(X\) is a feature. Let \(\mu_{y},\sigma_{y}^{2}\) be the sample mean and variance of \(\bm{y}\) respectively in the training dataset, then the whitened \(y\) values used in both the training and test set are simply \(\sigma_{y}^{-1}(y-\mu_{y})\). Let \(\bm{\mu}_{X},\Sigma_{X}\) be the sample mean and covariance matrix of \(X\) respectively. Let \(\Sigma_{X}=MM^{T}\), then the whitened \(x\) values in both the training and test data are \(\frac{1}{\sqrt{d}}M^{-1}(\bm{x}-\bm{\mu}_{X})\), where \(d\) is the feature dimension of \(X\). **Note**: the performance metrics given in subsection 7.1 are expressed in terms of the whitened \(y\) values rather than the \(y\) values in their original form. This appears to be common practice in the literature and has no bearing on the _comparative_ performance of the different methods within this paper.

### Test-Set Batching

To prevent excessive memory consumption, we perform all predictions for the distributed and variational methods in batches of 1000 points at a time. Where this is not possible (e.g. for especially large datasets), we use smaller batches of 500 or 250 points, as appropriate.

### Additional Implementation Details for SVGP

We use the sparse variational inducing point approach of [14], following the implementation provided by GPyTorch, which in particular uses a Choleksy decomposition to parameterise the covariance matrix of the variational prior. We broadly follow the SVGP implementation example provided by https://docs.gpytorch.ai/en/stable/examples/04_Variational_and_Approximate_GPs/SVGP_Regression_CUDA.html. In particular, we follow their example in using the Adam optimiser to train our model over 100 epochs with a minibatch size of 1024 and a learning rate of 0.01. We opt to use 1024 inducing points. All experiments under this method are run on a SageMaker ml.p3.2xlarge instance, consisting of a single Tesla V100 GPU with 16GB of memory.

### Additional Implementation Details for Distributed methods

A good introduction to distributed methods for Gaussian process inference is [7]. Here we run the product-of-experts (PoE) [15], generalised product-of-experts (gPoE) [2], Bayesian committee machine (BCM) [33], robust Bayesian committee machine (rBCM) [7] and generalised robust Bayesian committee machine (GrBCM) [18] following the recommendation in [4] to aggregate in \(f\)-space. There are three components to any distributed method: the hyperparameter inference, the _partitioner_ and the _aggregator_. Hyperparameter estimation is the same for all of the methods: we use the method in section 3.1 of [7], randomly partitioning the entire training set into subsets of size 625 (or as close as possible with equal-sized experts given that in general \(n\) is not a multiple of 625). A block diagonal approximation (with \(n/625\) blocks) is then used to approximate to the full \(n\times n\) gram kernel matrix. To recover hyperparameters with this we use Gaussian Process models with a zero prior mean and a scaled square-exponential kernel. Training is conducted using the Adam optimiser with a learning rate of 0.1 over 100 optimiser iterations. Once the hyperparameters are trained, we run our distributed prediction mechanism to evaluate performance against the test-set. The 625-sized partitioned blocks are referred to as "experts" and the shared hyperparameter values are distributed to each expert and held fixed thereafter. In the _aggregator_, or distributed prediction phase, each expert produces an individual predictive distribution and these are then aggregated to a final predictive mean and variance for each of our test points. GRBCM prediction is a little more complex than this as it makes use of an additional "communications" expert as explained in [18], aggregating in \(f\)-space as recommended in [4]. We provide timing statistics for training these models.

We use our own GPyTorch-based implementation of distributed GP approximations. All exact GP calculations are performed using GPyTorch using the default settings (so 20 Lanczos iterations throughout and a CG tolerance of 1 for hyperparameter inference, and \(10^{-3}\) for posterior predictions). For all of our experiments, we utilise an AWS t3.2xlarge instance (consisting of 8 Intel Skylake Processors and 32 GB of RAM).

### Reproducibility

All code used to generate tables and figures in this document can be found at https://github.com/ant-stephenson/gpnn-experiments/.

## Appendix F Related Work

### Nngp

An hierarchical Bayesian approach to nearest neighbour GPs is derived in [5], who construct a full stochastic process allowing an end-to-end probabilistic approach they term 'NNGP' derived from a 'parent' GP using collections of nearest neighbour sets forming a'reference' set. This work is modified in various ways in [8] including by adaption to a hybrid empirical-Bayes/fully-Bayesian approach to improve scalability. This is described in Algorithm 5 which presents an MCMC-free approach ('conjugate NNGP') where some parameters are estimated via K-fold cross-validation and the remainder are given conjugate priors to allow exact posterior inference. Under this model, the marginal predictive distribution is Student-\(t\) with mean and variance expressions that match that given by GPnn up to a scaling factor on the variance. Here we will go into more detail to make this connection explicit. First of all, note that they use an alternative parameterisation to us, specifying an inverse lengthscale (\(\phi\)), a ratio (\(\alpha\)) of noise variance (\(\tau^{2}\)) to kernelscale (\(\sigma^{2}\)) and a vector of coefficients for their linear mean function (\(\bm{\beta}\), which we can neglect since we focus exclusively on mean-zero GPs).

In Algorithm 5 they go on to give the following expressions for the predictive mean and variance:

\[z =M(s,N(s,k))\] \[w =\operatorname{solve}(M[N(s,k),N(s,k)],z)\] \[m_{0} =\widehat{y(s)}=\text{dot}(x(s),g(k))+\text{dot}(w,(y[N(s,k)]- \text{dot}(X[N(s,k),],g(k))))\] \[u =x(s)-\text{dot}(X[N(s,k),],w)\] \[v_{0} =\text{dot}(u,\text{gemv}(V(k),u))+1+\alpha-\text{dot}(w,z)\] \[\widehat{\operatorname{Var}(y(s))} =b_{\sigma}^{*}(k)v_{0}/(a_{\sigma}^{*}(k)-1)\]

We can see that the predictive mean \(m_{0}\) matches our own in the mean-zero setting. In particular, in this case the only non-zero term is \(\text{dot}(w,y[N(s,k)])\) which can be re-written in our notation as \(\bm{k}_{N}^{*T}K_{N}^{-1}\bm{y}_{N}\). Note that this expression is, for fixed \(\alpha\), independent of choice of kernelscale. This can be seen using the notation given in section 1: \(\bm{k}_{N}^{*T}K_{N}^{-1}\bm{y}_{N}=\sigma_{f}^{2}\bm{c}_{N}^{*T}(\sigma_{f}^ {2}C_{N}+\sigma_{\xi}^{2}I)^{-1}\bm{y}_{N}=\bm{c}_{N}^{*T}(C_{N}+\alpha I)^{- 1}\bm{y}_{N}\).

In addition to this, the predictive variance is equivalent to ours up to multiplicative scaling \(S=b_{\sigma}^{*}(k)/[\sigma_{f}^{2}(a_{\sigma}^{*}(k)-1)]\); i.e. \(\widehat{\operatorname{Var}(y(s))}=S\sigma_{N}^{*\,2}\), which be seen as follows: In the mean-zero case we have \(v_{0}=1+\alpha-\text{dot}(w,z)\) which can again be re-written in our notation as \(1+\sigma_{\xi}^{2}/\sigma_{f}^{2}-\bm{c}_{N}^{*T}C_{\alpha,N}^{-1}\bm{c}_{N}^ {*}\) (where \(C_{\alpha,N}=C_{N}+\alpha I\)). Hence, if we rescale this by a factor of \(\sigma_{f}^{2}\) we exactly obtain the GPnn predictive variance, so that \(\widehat{\operatorname{Var}(y(s))}=b_{\sigma}^{*}(k)v_{0}/(a_{\sigma}^{*}(k)- 1)=b_{\sigma}^{*}(k)\sigma_{N}^{*2}/[\sigma_{f}^{2}(a_{\sigma}^{*}(k)-1)]\) as claimed. Since GPnn uses an additional recalibration step to rescale all predictive variances by a single shared multiplicative factor, the factor \(S\) between the two methods can be made effectively redundant: i.e. if our recalibration step (Algorithm 2) were applied to their method, the two methods would become equivalent _with respect to RMSE and weak-calibration_ (setting aside differences in parameter estimation and their associated costs).

We reemphasise that whilst GPnn and Conjugate NNGP pointwise predictions can be related as above, the latter is given in the form of a \(t\)-distribution which, even with matched first two moments, will have a different shape to that of a Normal distribution. As such, the (Gaussian) NLL performance measure is no longer a valid choice to assess this model. RMSE and weak-calibration remain distribution-agnostic however.

[MISSING_PAGE_EMPTY:23]

[MISSING_PAGE_EMPTY:24]

[MISSING_PAGE_EMPTY:25]

Overall Computational Expenditure

Our distributed and variational method experiments were conducted using cloud computing resources. Experiments using our own method have been carried out on an author's laptop. SVGP experiments were run using a SageMaker virtual machine on a single Nvidia Tesla V100 GPU with 16GB memory. Distributed method experiments were run using eight Intel Xeon Platinum 8000 CPU cores (t3.2xlarge EC2 instances).

Below we will attempt to give reasonable indications of the amount of computational work expended to obtain the results in this paper, though note that we are neglecting the work expended in the development and research stages that did not directly contribute to the runs in the paper. As such, the costs presented are representative of the costs of replicating our paper, not repeating the research from scratch. Instead of reporting costs in dollars, we will report approximate computing hours for each instance type. The reader can then estimate their own costs using the current instance costs in the region of their choice, or under other cloud providers or even using on-premise compute.

\begin{tabular}{l c c} \hline \hline
**Dataset** & **Billed hours (1 GPU, 3 runs)** & **Billed hours (8 CPUs, 3 runs of 5 methods)** \\ \hline bike & 0.027 & 0.222 \\ ctslice & 0.082 & 0.546 \\ houseelectric & 3.713 & 256.776 \\ potelele & 0.010 & 0.079 \\ protein & 0.068 & 0.680 \\ road3d & 0.635 & 31.674 \\ song & 0.904 & 12.237 \\ \hline \hline \end{tabular}

This gives a total of around 5.4 hours of compute time on a 1 GPU VM and 302.2 hours on an 8 CPU VM.