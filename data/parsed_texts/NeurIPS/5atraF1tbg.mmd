# Panoramia: Privacy Auditing of Machine Learning Models without Retraining

 Mishaal Kazmi

University of British Columbia

&Hadrien Lautraite

University du Quebec a Montreal

&Alireza Akbari

Simon Fraser University

&Qiaoyue Tang

University of British Columbia

&Mauricio Soroco

University of British Columbia

&Tao Wang

Simon Fraser University

&Sebastien Gambs

University du Quebec a Montreal

&Mathias Lecuyer

University of British Columbia

equal contribution

###### Abstract

We present PANORAMIA, a privacy leakage measurement framework for machine learning models that relies on membership inference attacks using generated data as non-members. By relying on generated non-member data, PANORAMIA eliminates the common dependency of privacy measurement tools on in-distribution non-member data. As a result, PANORAMIA does not modify the model, training data, or training process, and only requires access to a subset of the training data. We evaluate PANORAMIA on ML models for image and tabular data classification, as well as on large-scale language models.

## 1 Introduction

Training Machine Learning (ML) models with Differential Privacy (DP) Dwork et al. (2006), such as with DP-SGD Abadi et al. (2016), upper-bounds the worst-case privacy loss incurred by the training data. In contrast, privacy auditing aims to empirically lower-bound the privacy loss of a target ML model or algorithm. In practice, privacy audits usually rely on the link between DP and the performance of membership inference attacks (MIA) Wasserman and Zhou (2010); Kairouz et al. (2015); Dong et al. (2019). At a high level, DP implies an upper-bound on the performance of MIAs, thus creating a high-performance MIA implies a lower-bound on the privacy loss. Auditing schemes have proven valuable in many settings, such as to audit DP implementations Nasr et al. (2023), or to study the tightness of DP algorithms Nasr et al. (2021); Lu et al. (2023); Steinke et al. (2023). Typical privacy audits rely on retraining the model several times, each time guessing the membership of one sample Jagielski et al. (2020); Carlini et al. (2022); Zanella-Beguelin et al. (2022), which is computationally prohibitive, requires access to the target model (entire) training data as well as control over the training pipeline.

To circumvent these concerns, Steinke et al. (2023) proposed an auditing recipe (called O(1)) requiring only one training run (which could be the same as the actual training) by randomly including/excluding several samples (called auditing examples) into the training dataset of the target model. Later, the membership of the auditing examples are guessed for privacy audit. However, O(1) faces a few challenges in certain setups. First, canaries, which are datapoints specially crafted to be easy to detect when added to the training set Nasr et al. (2023); Lu et al. (2023); Steinke et al. (2023), cannot be employed as auditing examples when measuring the privacy leakage for data that a contributor actually puts into the model, and not a _worst case data point_.

This matches a setting in which individual data contributors (_e.g._, a hospital in a cross-site Federated Learning (FL) setting or a user of a service that trains ML models on users' data) measure the leakage of their own (_i.e._, known) partial training data in the final trained model. Second, O(1) also relies on the withdrawal of real data from the model to construct non-member in-distribution data. This is problematic in situations in which ML model owners need to conduct post-hoc audits, in which case it is too late for removal Negoscu et al. (2023). Moreover, in-distribution audits require much more data, thus withholding many data points (typically more than the test set size) and reducing model utility. This brings us to the question: _Given an instance of a machine learning model as a target, can we perform post-hoc estimation of the privacy loss with regards to a known member subset of the target model training dataset?_

**Our contributions.** We propose PANORAMIA, a new scheme for _Privacy Auditing with NO Retraining by using Artificial data for Membership Inference Attacks_. More precisely, we consider an auditor with access to a subset of the training data and introduce a new alternative for accessing non-members: using synthetic datapoints from a generative model trained on the member data, unlocking the limit on non-member data. PANORAMIA uses this generated data, together with known members, to train and evaluate a MIA attack on the target model to audit (SS3). We also adapt the theory of privacy audits, and show how PANORAMIA can estimate the privacy loss (though not a lower-bound) of the target model with regards to the known member subset (SS4). An important benefit of PANORAMIA is to perform privacy loss measurements with (1) no retraining the target ML model (_i.e._, we audit the end-model, not the training algorithm), (2) no alteration of the model, dataset, or training procedure, and (3) only partial knowledge of the training set. We evaluate PANORAMIA on CIFAR10 models and observe that overfitted models, larger models, and models with larger DP parameters have higher measured privacy leakage. We also demonstrate the applicability of our approach on the GPT-2 based model (_i.e._, WikiText dataset) and CelebA models.

## 2 Background and Related Work

DP is the established privacy definition in the context of ML models, as well as for data analysis in general. We focus on the pure DP definition to quantify privacy loss with well-understood semantics. In a nutshell, DP is a property of a randomized mechanism (or computation) from datasets to an output space \(\mathcal{O}\), noted \(M:\mathcal{D}\rightarrow\mathcal{O}\). It is defined over neighboring datasets \(D,D^{\prime}\), differing by one element \(x\in\mathcal{X}\) (we use the add/remove neighboring definition), which is \(D^{\prime}=D\cup\{x\}\). Formally:

**Definition 1** (Differential Privacy Dwork et al. (2006)).: _A mechanism \(M:\mathcal{D}\rightarrow\mathcal{O}\) is \(\epsilon\)-DP if for any two neighbouring datasets \(D\), \(D^{\prime}\in\mathcal{D}\), and for any measurable output subset \(O\subseteq\mathcal{O}\) it holds that:_

\[P[M(D)\in O]\leq e^{\epsilon}P[M(D^{\prime})\in O].\]

Since the neighbouring definition is symmetric, so is the DP definition, and we also have that \(P[M(D^{\prime})\in O]\leq e^{\epsilon}P[M(D)\in O]\). Intuitively, \(\epsilon\) upper-bounds the worst-case contribution of any individual example to the distribution over outputs of the computation (_i.e._, the ML model learned). More formally, \(\epsilon\) is an upper-bound on the _privacy loss_ incurred by observing an output \(o\), defined as \(\big{|}\ln\Big{(}\frac{\mathbb{P}[M(D)=o]}{\mathbb{P}[M(D^{\prime})=o]}\Big{)} \big{|}\), which quantifies how much an adversary can learn to distinguish \(D\) and \(D^{\prime}\) based on observing output \(o\) from \(M\). A smaller \(\epsilon\) hence means higher privacy.

**DP, MIA and privacy audits.** To audit a DP training algorithm \(M\) that outputs a model \(f\), one can perform a MIA on datapoint \(x\), trying to distinguish between a neighboring training sets \(D\) and \(D^{\prime}=D\cup\{x\}\). The MIA can be formalized as a hypothesis test to distinguish between \(\mathcal{H}_{0}=D\) and \(\mathcal{H}_{1}=D^{\prime}\) using the output of the computation \(f\). Wasserman and Zhou (2010); Kairouz et al. (2015); Dong et al. (2019) show that any such test at significance level \(\alpha\) (False Positive Rate or FPR) has power (True Positive Rate or TPR) bounded by \(e^{\epsilon}\alpha\). In practice, one repeats the process of training model \(f\) with and without \(x\) in the training set, and uses a MIA to guess whether \(x\) was included. If the MIA has TPR \(>e^{\epsilon}\)FPR, the training procedure that outputs \(f\) is not \(\epsilon\)-DP. This is the building block of most privacy audits Jagielski et al. (2020); Nasr et al. (2021); Zanella-Beguelin et al. (2022); Lu et al. (2023); Nasr et al. (2023). Appendix F provides further context, and discusses other related work that is not directly relevant to our contribution.

**Averaging over data instead of models with O(1).** The above result bounds the success rate of MIAs when performed over several _retrained models_, on two alternative datasets \(D\) and \(D^{\prime}\). Steinke et al. (2023) show that it is possible to average _over data_ when several data points independently differ between \(D\) and \(D^{\prime}\). Let \(x_{1,m}\) be the \(m\) data points independently included in the training set, and \(s_{1,m}\in\{0,1\}^{m}\) be the vector encoding inclusion. \(T_{0,m}\in\mathbb{R}^{m}\) represents any vector of guesses, with positive values for inclusion in the training set (member), negative values for non-member, and zero for abstaining. Then, if the training procedure is \(\epsilon\)-DP, Proposition 5.1 in Steinke et al. (2023) bounds the performance of guesses from \(T\) with:

\[\mathbb{P}\Big{[}\sum_{i=1}^{m}\max\{0,T_{i}\cdot S_{i}\}\geq v\mid T=t\Big{]} \leq\underset{S^{\prime}\sim\text{Bernoulli}(\frac{\epsilon^{\epsilon}}{1+ \epsilon^{\epsilon}})^{m}}{\mathbb{P}\Big{[}\sum_{i=1}^{m}|t_{i}|\cdot S^{ \prime}_{i}\geq v\Big{]}}.\]

In other words, an audit (MIA) \(T\) that can guess membership better than a Bernoulli random variable with probability \(\frac{\epsilon^{\epsilon}}{1+\epsilon^{\epsilon}}\) refutes an \(\epsilon\)-DP claim. In this work we build on this result, extending the algorithm (SS3) and theoretical analysis (SS4) to enable the use of generated data for non-members, which breaks the independence between data points.

## 3 Panoramia

Figure 1 summarizes the end-to-end PANORAMIA privacy measurement. The measurement starts with a target model \(f\), and a subset of its training data \(D_{f}\) from distribution \(\mathcal{D}\). For instance, \(\mathcal{D}\) and \(D_{f}\) could be the distribution and dataset of one participant in an FL training procedure that outputs a final model \(f\). The privacy measurement then proceeds in two phases.

**Phase 1:** In the first phase, PANORAMIA uses a subset of the known training data \(D_{G}\subset D_{f}\) to train a generative model \(\mathcal{G}\). The goal of the generative model \(\mathcal{G}\) is to match the training data distribution \(\mathcal{D}\) as closely as possible, which is formalized in Definition 3 (SS4). Using the generative model \(\mathcal{G}\), we can synthesize non-member data, which corresponds to data that was not used in the training of target model \(f\). Hence, we now have access to an independent dataset of member data \(D_{\text{in}}=D_{f}\backslash D_{G}\), and a synthesized dataset of non-member data \(D_{\text{out}}\sim\mathcal{G}\), of size \(m=|D_{\text{in}}|\).

**Phase 2:** In the second phase, we leverage \(D_{\text{in}}\) and \(D_{\text{out}}\) to audit the privacy leakage of \(f\) using a MIA. To this end, we split \(D_{\text{in}}\), \(D_{\text{out}}\) into training and testing sets, respectively called \(D_{\text{in}}^{\text{tr}}\), \(D_{\text{out}}^{\text{tr}}\) and \(D_{\text{in}}^{\text{te}}\), \(D_{\text{out}}^{\text{te}}\). We use the training set to train a MIA (called PANORAMIA in Figure 1), a binary classifier that predicts whether a given datapoint is a member of \(D_{f}\), the training set of the target model \(f\). This MIA classifier makes its prediction based on both a training example \(x\), as well as information from applying the target model \(f\) to the input, such as the loss of the target model when applied to this example \(\text{loss}(f(x))\) (see SS5, Appendix C for details). We use the test set to measure the MIA performance, using the precision at different recall values.

Previous results linking the performance of a MIA on several data-points to \(\epsilon\)-DP bounds rely on independence between members and non-members. This intuitively means that there is no information about membership in \(x\) itself. When the auditor controls the training process this independence is enforced by construction, by adding data points to the training set based on an independent coin flip. In PANORAMIA, we do not have independence between membership and \(x\), as all non-members

Figure 1: PANORAMIA’s two-phase privacy audit. Phase 1 trains generative model \(\mathcal{G}\) on member data. Phase 2 trains a MIA on a subset of member data and generated non-member data, using the loss of \(f\) on these data points. The performance of the MIA is compared to a baseline classifier that does not have access to \(f\). Notations are summarized in Table 4 in Appendix A.

come from the generator \(\mathcal{G}\neq\mathcal{D}\). As a result, there are two ways to guess membership and have high MIA precision: either by using \(f\) to detect membership (_i.e._, symptomatic of privacy leakage) or by detecting generated data (_i.e._, not a symptom of privacy leakage). To measure the privacy leakage, we compare the results of the MIA to that of a baseline classifier \(b\) that guesses membership based exclusively on \(x\), without access to \(f\). The stronger this baseline, the better the removal of the effect of synthesized data detection. Algorithm 1 summarizes the entire procedure. In the next section, we demonstrate how to relate the difference between the baseline \(b\) and the MIA performance to the privacy loss \(\epsilon\).

``` Input: Target ML model \(f\), audit set size \(m\), confidence \(1-\beta\) Phase 1:
1:Split \(D_{f}\) in \(D_{G},D_{\text{in}}^{\text{tr}}\), \(D_{\text{in}}^{\text{te}}\), with \(|D_{\text{in}}^{\text{te}}|=m\)
2:Train generator \(\mathcal{G}\) on \(D_{G}\)
3:Generate \(D_{\text{out}}^{\text{tr}}\), \(D_{\text{out}}^{\text{te}}\) of size \(|D_{\text{in}}^{\text{tr}}|,|D_{\text{in}}^{\text{te}}|\)
4:Phase 2:
5:Train the baseline and MIA:
6:Label \(D_{\text{in}}^{\text{tr}}\) as members, and \(D_{\text{out}}^{\text{tr}}\) as non-members
7:Train \(b\) to predict labels using \(x\in D_{\text{in}}^{\text{tr}}\cup D_{\text{out}}^{\text{tr}}\)
8:Train MIA to predict labels using \(x\in D_{\text{in}}^{\text{tr}}\cup D_{\text{out}}^{\text{tr}}\) and \(f(x)\) Measure privacy leakage (see SS4):
9:Sample \(s\sim\text{Bernoulli}(\frac{1}{2})^{m}\)\(\triangleright\) Def.2
10:Create audit set \(X=s\cdot D_{\text{in}}^{\text{te}}+(1-s)D_{\text{out}}^{\text{te}}\)
11:Score each audit point for membership, creating \(t^{b}\triangleq b(X)\in\mathbb{R}_{+}^{m}\) and \(t^{a}\triangleq\text{MIA}(X)\in\mathbb{R}_{+}^{m}\)
12:Set \(v_{\text{ub}}^{b}(c,t)\triangleq\sup\left\{v:\beta^{b}(m,c,v,t)\leq\frac{ \beta}{2}\right\}\)\(\triangleright\) Prop.1
13:\(c_{\text{lb}}=\max_{t,c}\mathbbm{1}\{t^{b}\geq t\}\cdot s\leq v_{\text{ub}}^{b} (c,\mathbbm{1}\{t^{b}\geq t\})\)\(\triangleright\) Prop.2
14:Set \(v_{\text{ub}}^{a}(c,\epsilon,t)\triangleq\sup\left\{v:\beta^{a}(m,c,\epsilon,v, t)\leq\frac{\beta}{2}\right\}\)\(\triangleright\) Prop.2
15:\(\{c+\epsilon\}_{\text{lb}}=\max_{t,c,\epsilon}\mathbbm{1}\{t^{a}\geq t\}\cdot s \leq v_{\text{ub}}^{a}(c,\epsilon,\mathbbm{1}\{t^{a}\geq t\})\)
16:Return\(\tilde{\epsilon}\triangleq\{c+\epsilon\}_{\text{lb}}-c_{\text{lb}}\) ```

**Algorithm 1**Panovamia

## 4 Quantifying Privacy Leakage with Panoramia

We formalize a privacy game as follows: construct a sequence of auditing samples, in which \(s_{i}\) is sampled independently to choose either the real (\(x^{\text{in}}\in\mathcal{X}^{m}\) training points from \(\mathcal{D}\) if \(s_{i}=1\)) or generated (\(x^{\text{gen}}\in\mathcal{X}^{m}\) generated points from \(\mathcal{G}\) if \(s_{1}=0\)) data point at each index \(i\).

**Definition 2** (Privacy game).:

\[s\sim\text{Bernoulli}(\frac{1}{2})^{m},\text{ with }s_{i}\in\{0,1\},x_{i}=(1-s_{i})x _{i}^{\text{gen}}+s_{i}x_{i}^{in},\;\forall i\in\{1,\dots,m\}.\]

The key difference between Definition 2 and the privacy game of Steinke et al. (2023) lies in how we create the audit set. Instead of sampling non-members independently and removing them from the training set (which requires modifyig the training set and the model), Definition 2 starts from a set of known members (after the fact), and pairs each point with a non-member (generated i.i.d. from the generator distribution). For each pair, we then flip a coin to decide which point in the pair will be shown to the auditor for testing, thereby creating the test task of our privacy measurement.

The level of achievable success in this game quantifies the privacy leakage of the target model \(f\). More precisely, we follow an analysis inspired by that of Steinke et al. (2023), but require several key technical changes to support auditing with no retraining using generated non-member data.

### Formalizing the Privacy Measurements as a Hypothesis Test

We first need a notion of quality for our generator:

**Definition 3** (\(c\)-closeness).: _For all \(c>0\), we say that a generative model \(\mathcal{G}\) is \(c\)-close for data distribution \(\mathcal{D}\) if:_

\[\forall x\in\mathcal{X},\;e^{-c}\mathbb{P}_{\mathcal{D}}\big{[}x\big{]}\leq \mathbb{P}_{\mathcal{G}}\big{[}x\big{]}.\]The smaller \(c\), the better the generator, as \(\mathcal{G}\) assigns a probability to real data that cannot be much smaller than that of the real data distribution. We make three important remarks.

**Remark 1:**\(c\)-closeness is a strong requirement but it is achievable, at least for a large \(c\). For instance, a uniform random generator is \(c\)-close to any distribution \(\mathcal{D}\), with \(c<\infty\). Of course \(c\) would be very large, and the challenge in PANORAMIA is to create a generator that empirically has a small enough \(c\).

**Remark 2:** We show in Appendix E that we can relax this definition to hold only with high probability (following the \((\epsilon,\delta)\)-DP relaxation). Empirical results under this relaxation are very similar. We also show how to measure \((\epsilon,\delta)\)-DP, instead of \(\epsilon\)-DP on which we focus here.

**Remark 3:** Our definition of closeness is very similar to that of DP. This is not a coincidence as we will use this definition to be able to reject claims of both \(c\)-closeness for the generator and \(\epsilon\)-DP for the algorithm that gave the target model. We further note that, contrary to the DP definition, \(c\)-closeness is one-sided as we only bound \(\mathbb{P}_{\mathcal{G}}\) from below. Intuitively, this means that the generator has to produce high-quality samples (_i.e._, samples likely under \(\mathcal{D}\)) with high enough probability but it does not require that all samples are good though. This one-sided measure of closeness is enabled by our focus on detecting members (_i.e._, true positives) as opposed to members and non-members, and puts less stringent requirements on the generator.

Using Definition 3, we can formulate the hypothesis test on the privacy game of Definition 2 that underpins our approach:

\[\mathcal{H}:\text{generator }\mathcal{G}\text{ is }c\text{-close, and target model }f\text{ is }\epsilon\text{-DP}.\]

Note that, in a small abuse of language, we will often refer to \(f\) as \(\epsilon\)-DP to say that \(f\) is the output of an \(\epsilon\)-DP training mechanism. That is, \(B(s,x)=\{b(x_{1}),b(x_{2}),\ldots,b(x_{m})\}\).

We define two key mechanisms: \(B(s,x):\{0,1\}^{m}\times\mathcal{X}^{m}\rightarrow\mathbb{R}_{+}^{m}\) outputs a (potentially randomized) non-negative score for the membership of each datapoint \(x_{i}\), based on \(x_{i}\) only; \(A(s,x,f):\{0,1\}^{m}\times\mathcal{X}^{m}\times\mathcal{F}\rightarrow\mathbb{R }_{+}^{m}\) outputs a (potentially randomized) non-negative score for the membership of each datapoint, with the guess for index \(i\) depending on \(x_{\leq i}\) and target model \(f\). We can construct a statistical test for each part of the hypothesis separately.

**Proposition 1**.: _Let \(\mathcal{G}\) be \(c\)-close, \(S\) and \(X\) be the random variables for \(s\) and \(x\) from Definition 2, and \(T^{b}\triangleq B(S,X)\) be the vector of guesses from the baseline. Then, for all \(v\in\mathbb{R}\) and all \(t\) in the support of \(T\):_

\[\mathbb{P}_{S,X,T^{b}}\Big{[}\sum_{i=1}^{m}T_{i}^{b}\cdot S_{i}\geq v\mid T^{ b}=t^{b}\Big{]}\leq\underset{S^{\prime}\sim\text{Bernoulli}(\frac{c^{c}}{1+c^{c}})^{m}}{ \mathbb{P}}\Big{[}\sum_{i=1}^{m}t_{i}^{b}\cdot S_{i}^{\prime}\geq v\Big{]} \triangleq\beta^{b}(m,c,v,t^{b})\]

Proof.: In Appendix B.1. 

Now that we have a test to reject a claim that the generator \(\mathcal{G}\) is \(c\)-close for the data distribution \(\mathcal{D}\), we turn our attention to the second part of \(\mathcal{H}\) which claims that the target model \(f\) is the result of an \(\epsilon\)-DP mechanism.

**Proposition 2**.: _Let \(\mathcal{G}\) be \(c\)-close, \(S\) and \(X\) be the random variables for \(s\) and \(x\) from Definition 2, \(f\) be \(\epsilon\)-DP, and \(T^{a}\triangleq A(S,X,f)\) be the guesses from the membership audit. Then, for all \(v\in\mathbb{R}\) and all \(t\) in the support of \(T\):_

\[\mathbb{P}_{S,X,T^{a}}\Big{[}\sum_{i=1}^{m}T_{i}^{a}\cdot S_{i}\geq v\mid T^ {a}=t^{a}\Big{]}\leq\underset{S^{\prime}\sim\text{Bernoulli}(\frac{c^{c}+t }{1+c^{c}+t})^{m}}{\mathbb{P}}\Big{[}\sum_{i=1}^{m}t_{i}^{a}\cdot S_{i}^{ \prime}\geq v\Big{]}\triangleq\beta^{a}(m,c,\epsilon,v,t^{a})\]

Proof.: In Appendix B.2. 

We can now provide a test for hypothesis \(\mathcal{H}\), by applying a union bound over Propositions 1 and 2:

**Corollary 1**.: _Let \(\mathcal{H}\) be true, \(T^{b}\triangleq B(S,X)\), and \(T^{a}\triangleq A(S,X,f)\). Then:_

\[\mathbb{P}\Big{[}\sum_{i=1}^{m}T_{i}^{a}\cdot S_{i}\geq v^{a},\;\sum_{i=1}^{m }T_{i}^{b}\cdot S_{i}\geq v^{b}\mid T^{a}=t^{a},T^{b}=t^{b}\Big{]}\leq\beta^{ a}(m,c,\epsilon,v^{a},t^{a})+\beta^{b}(m,c,v^{b},t^{b})\]To make things more concrete, let us instantiate Corollary1 as we do in \(\mathtt{PANORAMIA}\). Our baseline (\(B\) above) and MIA (\(A\) above) classifiers return a membership guessess \(T^{a,b}\in\{0,1\}^{m}\), with \(1\) corresponding to membership. Let us call \(r^{a,b}\triangleq\sum_{i}t_{i}^{a,b}\) the total number of predictions, and \(\text{tp}^{a,b}\triangleq\sum_{i}t_{i}^{a,b}\cdot s_{i}\) the number of correct membership guesses (true positives). We also call the precision \(\text{prec}^{a,b}\triangleq\frac{\text{tp}^{a,b}}{r^{a,b}}\). Using the following tail bound on the sum of Bernoulli random variables for simplicity and clarity (we use a tighter bound in practice, but this one is easier to read),

\[\mathbb{P}_{S^{\prime}\sim\text{Bernoulli}(p)^{r}}\Big{[}\sum_{i=1}^{r}\frac{S _{i}^{\prime}}{r}\geq p+\sqrt{\frac{\log(1/\beta)}{2r}}\Big{]}\leq\beta,\] (1)

we can reject \(\mathcal{H}\) at confidence level \(\beta\) by setting \(\beta^{a}=\beta^{b}=\frac{\beta}{2}\) and if either \(\text{prec}^{b}\geq\frac{e^{c}}{1+e^{c}}+\sqrt{\frac{\log(2/\beta)}{2r^{b}}}\) or \(\text{prec}^{a}\geq\frac{e^{c+\epsilon}}{1+e^{c+\epsilon}}+\sqrt{\frac{\log (2/\beta)}{2r^{a}}}\).

### Quantifying Privacy Leakage and Interpretation

In an privacy measurement, we would like to quantify \(\epsilon\), not just reject a given \(\epsilon\) claim. We use the hypothesis test from Corollary1 to compute a confidence interval on \(c\) and \(\epsilon\). To do this, we first define an ordering between \((c,\epsilon)\) pairs, such that if \((c_{1},\epsilon_{1})\leq(c_{2},\epsilon_{2})\), the event (_i.e._, set of observations for \(T^{a,b}\), \(S\)) for which we can reject \(\mathcal{H}(c_{2},\epsilon_{2})\) is included in the event for which we can reject \(\mathcal{H}(c_{1},\epsilon_{1})\). That is, if we reject \(\mathcal{H}\) for values \((c_{2},\epsilon_{2})\) based on audit observations, we also reject values \((c_{1},\epsilon_{1})\) based on the same observations.

We define the following order to fit this assumption, based on the hypothesis test from Corollary1:

\[(c_{1},\epsilon_{1})\leq(c_{2},\epsilon_{2})\text{ if either }\begin{cases}c_{1}<c_{2}, \text{ or }\\ c_{1}=c_{2}\text{ and }\epsilon_{1}\leq\epsilon_{2}\end{cases}\] (2)

This lexicographic order over hypotheses ensures that, when using the joint hypothesis test from Corollary1 to construct confidence intervals, we always reject hypotheses with a low value of \(c\) incompatible with the baseline performance. Formally, this yields the following confidence intervals:

**Corollary 2**.: _For all \(\beta\in]0,1]\), \(m\), and observed \(t^{a},t^{b}\), call \(v^{a}_{\text{ub}}(c,\epsilon)\triangleq\sup\,\{v:\beta^{a}(m,c,\epsilon,v,t^ {a})\leq\frac{\beta}{2}\}\) and \(v^{b}_{\text{ub}}(c)\triangleq\sup\,\{v:\beta^{b}(m,c,v,t^{b})\leq\frac{ \beta}{2}\}\). Then:_

\[\mathbb{P}\Big{[}(c,\epsilon)\geq\sup\big{\{}(c^{\prime},\epsilon^{\prime}):t ^{a}\cdot s\leq v^{a}_{\text{ub}}(c^{\prime},\epsilon^{\prime})\text{ and }t^{b}\cdot s\leq v^{b}_{\text{ub}}(c^{\prime})\big{\}}\Big{]}\geq 1-\beta\]

Proof.: Apply Lemma4.7 from Steinke et al. (2023) with the ordering from Equation2 and the test from Corollary1. 

The lower confidence interval for \((c,\epsilon)\) at confidence \(1-\beta\) is the largest \((c,\epsilon)\) pair that cannot be rejected using Corollary1 with false rejection probability at most \(\beta\). Hence, for a given confidence level \(1-\beta\), \(\mathtt{PANORAMIA}\) computes \((c_{\text{lb}},\tilde{\epsilon})\), the largest value for \((c,\epsilon)\) that it cannot reject. Note that Corollaries1 and 2 rely on a union bound between two tests, one for \(c\) and one for \(c+\epsilon\). We can thus consider each test separately (at level \(\beta/2\)). In practice, we use the whole precision/recall curve to achieve tighter bounds: each level of recall (_i.e._, threshold on \(t^{a,b}\) to predict membership) corresponds to a bound on the precision, which we can compare to the empirical value. Separately for each test, we pick the level of recall yielding the highest lower-bound (Lines 4-7 in the last section of Algorithm1). For all bounds to hold together, we apply a union bound over all tests at each level of recall. Specially, if we have \(m\) test datapoints, we have at most \(m\) different values of recall, and the union bound corresponds to \(\beta\leftarrow\beta/m\) (notice in Equation1 that the bounds depend on \(\sqrt{\log(1/\beta)}\), so this is not too costly). We next discuss the semantics of returned measurement values \((c_{\text{lb}},\tilde{\epsilon})\).

**Measurement semantics.** Corollary2 gives us a lower-bound for \((c,\epsilon)\), based on the ordering from Equation2. To understand the value \(\tilde{\epsilon}\) returned by \(\mathtt{PANORAMIA}\), we need to understand what the hypothesis test rejects. Rejecting \(\mathcal{H}\) means either rejecting the claim about \(c\), or the claim about \(c+\epsilon\) (which is the reason for the ordering in Equation2). With Corollary2, we hence get both a lower-bound \(c_{\text{lb}}\) on \(c\), and \(\{c+\epsilon\}_{\text{lb}}\) on \(c+\epsilon\). Unfortunately, \(\tilde{\epsilon}\triangleq\max\{0,\{c+\epsilon\}_{\text{lb}}-c_{\text{lb}}\}\), which is the value \(\mathtt{PANORAMIA}\) returns, does not imply a lower-bound on \(\epsilon\). Instead, we can claim that "PANORAMIA could not reject a claim of \(c\)-closeness for \(\mathcal{G}\), and if this claim is tight, then \(f\) cannot be the output of a mechanism that is \(\tilde{\epsilon}\)-DP".

While this is not as strong a claim as typical lower-bounds on \(\epsilon\)-DP from prior privacy auditing works, we believe that this measure is useful in practice. Indeed, the \(\tilde{\epsilon}\) measured by PANORAMIA is a quantitative privacy measurement, that is accurate (close to a lower-bound on \(\epsilon\)-DP) when the baseline performs well (and hence \(c_{\text{lb}}\) is tight). Thus, when the baseline is good, \(\tilde{\epsilon}\) can be interpreted as (close to) a lower bound on (pure) DP. PANORAMIA opens a new capability, measuring privacy leakage of a trained model \(f\) without access or control of the training pipeline, with a meaningful and practically useful measurement, as we empirically show next.

## 5 Evaluation

We instantiate PANORAMIA on target models for four tasks from three data modalities1. For **image classification**, we consider the CIFAR10Krizhevsky (2009), and CelebA Liu et al. (2015) datasets, with varied target models: a four-layers CNN O'Shea and Nash (2015), a ResNet101He et al. (2015), a Wide ResNet-28-10 Zagoruyko and Komodakis (2017), a Vision Transformer (ViT)-small Dosovitskiy et al. (2021), and a DP ResNet18He et al. (2015) trained with DP-SGD (Abadi et al., 2016) using Opacus (Yousefpour et al., 2021) at different values of \(\epsilon\). We use StyleGAN2Karras et al. (2020) for \(\mathcal{G}\). For **language models**, we fine-tune small GPT-2 Radford et al. (2019) on the WikiText train dataset Merity et al. (2016) (we take a subset of the documents in WikiText-103). \(\mathcal{G}\) is again based on small GPT-2, and then fine-tuned on \(D_{G}\). We generate samples using top-\(k\) sampling Holtzman et al. (2019) and a held-out prompt dataset \(D_{G}^{\text{prompt}}\subset D_{G}\). Finally, we conduct experiments on **classification on tabular data**. However for this data modality, PANORAMIA did not detect any meaningful leakage. More precisely, we have observed significant variance in the audit values returned by PANORAMIA which make the results inconclusive. Nonetheless, we present the results obtained for this task and discuss this issue further in Appendix D.6. Table 1 summarizes the tasks, models, and performance. More details on the settings, the models used, and implementation details are provided in Appendix C.

Footnote 1: Code available here: https://github.com/ubc-systopia/panoramia-privacy-measurement.

### Baseline Design and Evaluation

Our MIA is a loss-based attack, which uses an ML model taking as input both a datapoint \(x\) and the value of the loss of target model \(f\) on point \(x\). Appendix C details the architectures used for the attack model for each data modality. Recall from SS4.2 the importance of having a tight \(c_{\text{lb}}\) for our measure \(\tilde{\epsilon}\) to be close to a lower-bound on \(\epsilon\)-DP. To increase the performance of our baseline \(b\), we mimic the role of the target model \(f\)'s loss in the MIA closing a helper model \(h\) trained on synthetic data, which adds a loss-based feature to \(b\).

This new feature can be viewed as side information about the data distribution. Table 2 shows the \(c_{\text{lb}}\) value for different designs for \(h\). The best performance is consistently when \(h\) is trained on synthetic data before being used as a feature to train the \(b\). Indeed, such a design reaches a \(c_{\text{lb}}\) up to \(1.19\) larger that without any helper (CIFAR10) and \(0.23\) higher than when training on real non-member data, _without requiring access to real non-member data_, a key requirement in PANORAMIA. We adopt this design in all the following experiments. In Figure 2 we show that the baseline has enough training data (vertical dashed line) to reach its best performance. Appendix D.1, shows similar results on GPT-2 as well as different architectures that we tried for the helper model. All these pieces of evidence confirm the strength of our baseline.

### Main Privacy Measurement Results

We run PANORAMIA on models with different degrees of over-fitting by varying the number of epochs, (see the final accuracy on Table 1) for each data modality. More over-fitted models are known to leak more information about their training data due to memorization (Yeom et al., 2018; Carlini et al., 2019, 2022a). To show the privacy loss measurement power of PANORAMIA, we compare it with two strong approaches to lower-bounding privacy loss.

First, we compare PANORAMIA with the O(1) audit (Steinke et al., 2023) (in input space, without canaries in a black box setting), which uses a loss threshold MIA to detect both members and non-members. Second, we use a hybrid between O(1) and PANORAMIA, in which we use real non-member data instead of generated data with our ML-based MIA (called RM;RN for Real Members; Real Non-members). In both cases \(c_{\text{lb}}=0\), but it requires control over training data inclusion and a large set of non-member data. The privacy loss measured by these techniques gives a target that we hope PANORAMIA to detect.

Figure 3 shows the precision of \(b\) and PANORAMIA at different levels of recall, and Figure 4 the corresponding value of \(\{c+\epsilon\}_{\text{lb}}\) (or \(c_{\text{lb}}\) for \(b\)). Dashed lines show the maximum value of \(\{c+\epsilon\}_{\text{lb}}\) (or \(c_{\text{lb}}\)) achieved (Fig. 4) (returned by PANORAMIA), and the precision implying these values at different recalls (Fig. 3). Table 3 summarizes those \(\{c+\epsilon\}_{\text{lb}}\) (or \(c_{\text{lb}}\)) values, as well as the \(\epsilon\) measured by existing approaches. We make two key observations.

Figure 4: \(\{c+\epsilon\}_{\text{lb}}\) (or \(c_{\text{lb}}\)) vs recall, for our target models, reported over \(5\) independent experiment runs.

Figure 3: Precision vs. recall comparison between PANORAMIA and the baseline \(b\) for our target models, based on one experiment run. The maximum \(c_{\text{lb}}\) or \(\{c+\epsilon\}_{\text{lb}}\) values set an upper bound on the empirical precision values across different recall levels, indicated by the dashed line.

First, the best prior method (whether RM;RN or O(1)) measures a larger privacy loss (\(\tilde{\epsilon}\leq\epsilon\)). Overall, these results empirically confirm the strength of \(b\), as we do not seem to spuriously assign differences between \(\mathcal{G}\) and \(\mathcal{D}\) to our privacy loss proxy \(\tilde{\epsilon}\). We also note that O(1) tends to perform better, due to its ability to rely on non-member detection, which improves the power of the statistical test at equal data sizes. Such tests are not available in PANORAMIA given our one-sided closeness definition for \(\mathcal{G}\) (see SS4), and we keep that same one-sided design for RM;RN for the sake of comparison.

Second, the values of \(\tilde{\epsilon}\) measured by PANORAMIA are close to those of the methods against which we compared. In particular, despite a more restrictive adversary model (_i.e._, no non-member data, no control over the training process, and no shadow model training), PANORAMIA is able to detect meaningful amounts of privacy loss, comparable to that of state-of-the-art methods.

For instance, on a non-overfitted CIFAR-10 ResNet101 model (E20), PANORAMIA detects a privacy loss of \(0.33\), while using real non-member (RM;RN) data yields \(0.42\), and controlling the training process \(O(1)\) gets \(0.45\). The relative gap gets even closer on models that reveal more about their training data. Indeed, for the most over-fitted model (E100), \(\tilde{\epsilon}=1.16\) is very close to RM;RN (\(\epsilon=1.22\)) and O(1) (\(\epsilon=1.41\)). This also confirms that the leakage detected by PANORAMIA on increasingly overfitted models does augment, which is confirmed by prior state-of-the-art methods. For instance, NLP models' \(\tilde{\epsilon}\) goes from \(0.22\) to \(2.40\) (\(2.82\) to \(5.73\) for O(1)). Appendix D.6 details results on tabular data, in which PANORAMIA is not able to detect significant privacy loss.

#### Privacy Measurement of \(\epsilon\)-DP Models:

Another avenue to varying privacy leakage is by training Differentially-Private (DP) models with different values of \(\epsilon\). We evaluate PANORAMIA on DP ResNet-18 models on CIFAR10, with \(\epsilon\) values shown on Table 3 and Figure 5. The hyper-parameters were tuned independently for the highest train accuracy (see Appendix D.9 for more results and discussion). As the results show, neither PANORAMIA nor O(1) detect privacy loss on the most private models (\(\epsilon=1,2\)). At higher values of \(\epsilon=10,15\) (_i.e._, less private models) and \(\epsilon=\infty\) (_i.e._, non-private model) PANORAMIA does detect an increasing level of privacy leakage with \(\tilde{\epsilon}_{\epsilon=10}<\tilde{\epsilon}_{\epsilon=15}<\tilde{ \epsilon}_{\epsilon=\infty}\). In this regime, the O(1) approach detects a larger, though comparable, amount of privacy loss.

We also evaluate PANORAMIA on DP (fine-tuned) large language models (see Appendix D.9 and Table 9 for details). We fine-tune GPT2-Small target models with DP-SGD on the WikiText dataset, with various values of \(\epsilon\). Neither PANORAMIA nor O(1) are able to measure a positive privacy loss.

Figure 5: ResNet18, CIFAR-10, \(\epsilon\)-DP for various \(\epsilon\) values.

\begin{table}
\begin{tabular}{|c|c|c|c|c|} \hline
**Target model /** & Adult & \(\epsilon\)-D & \(\tilde{\epsilon}\)-D & \(\tilde{\epsilon}\)-D \\ \hline \multirow{2}{*}{ResNet11,15} & \multirow{2}{*}{\begin{tabular}{} \end{tabular} } & 2.41\(\pm\)0.10 & 2.5\(\pm\)0.20 & 3.0\(\pm\)0.21 & \(\epsilon\)-D \\  & & \(\epsilon\)-D & 0.42\(\pm\)0.19 & - & 0.40\(\pm\)0.10 \\  & & \(\epsilon\)-D & RM & - & - & 0.40\(\pm\)0.10 \\ \hline \multirow{2}{*}{ResNet11,150} & \multirow{2}{*}{\begin{tabular}{} \end{tabular} } & 2.41\(\pm\)0.10 & 1.7\(\pm\)0.11 & 1.0\(\pm\)0.11 & 0.0\(\pm\)0.11 \\  & & \(\epsilon\)-D & RM & - & - & 0.71\(\pm\)0.21 \\  & & \(\epsilon\)-D & RM & - & - & - & 0.79\(\pm\)0.21 \\ \hline \multirow{2}{*}{ResNet11,150} & \multirow{2}{*}{\begin{tabular}{} \end{tabular} } & 2.41\(\pm\)0.10 & 1.4\(\pm\)0.10 & 1.4\(\pm\)0.13 & - & 1.27\(\pm\)0.19 \\  & & \(\epsilon\)-D & RM & - & - & - & 1.11\(\pm\)0.180 \\ \hline \multirow{2}{*}{WR-24,10,150} & \multirow{2}{*}{\begin{tabular}{} \end{tabular} } & 2.44\(\pm\)0.10 & 2.5\(\pm\)0.21 & 2.5\(\pm\)0.22 & 2.5\(\pm\)0.22 \\  & & \(\epsilon\)-D & RM & - & - & - & 2.87\(\pm\)0.20 \\  & & \(\epsilon\)-D & RM & - & - & - & 2.86\(\pm\)0.19 \\ \hline \multirow{2}{*}{WR-24,10,200} & \multirow{2}{*}{\begin{tabular}{} \end{tabular} } & 2.41\(\pm\)0.10 & 3.5\(\pm\)0.21 & 3.3\(\pm\)0.22 & 2.5\(\pm\)0.22 \\  & & \(\epsilon\)-D & RM & - & - & - & 2.86\(\pm\)0.19 \\ \hline \multirow{2}{*}{WR-24,10,200} & \multirow{2}{*}{\begin{tabular}{} \end{tabular} } & 2.41\(\pm\)0.10 & 3.5\(\pm\)0.21 & 2.3\(\pm\)0.22 & 2.3\(\pm\)0.22 \\  & & \(\epsilon\)-D & RM & - & - & - & 2.87\(\pm\)0.20 \\  & & \(\epsilon\)-D & RM & - & - & - & 2.86\(\pm\)0.19 \\ \hline \multirow{2}{*}{WR-24,10,200} & \multirow{2}{*}{\begin{tabular}{} \end{tabular} } & 2.41\(\pm\)0.10 & 3.5\(\pm\)0.21 & 2.3\(\pm\)0.22 & 2.3\(\pm\)0.22 \\  & & \(\epsilon\)-D & RM & - & - & - & 2.87\(\pm\)0.20 \\  & & \(\epsilon\)-D & RM & - & - & - & 2.86\(\pm\)0.19 \\ \hline \multirow{2}{*}{WR-24,10,200} & \multirow{2}{*}{\begin{tabular}{} \end{tabular} } & 2.41\(\pm\)0.10 & 3.5\(\pm\)0.21 & 2.3\(\pm\)0.22 & 2.3\(\pm\)0.22 \\  & & \(\epsilon\)-D & RM & - & - & - & 2.77\(\pm\)0.21 \\  & & \(\epsilon\)-D & RM & - & - & - & 2.86\(\pm\)0.19 \\ \hline \multirow{2}{*}{WR-24,10,200} & \multirow{2}{*}{\begin{tabular}{} \end{tabular} } & 2.41\(\pm\)0.10 & 3.5\(\pm\)0.21 & 2.3\(\pm\)0.22 & - & - & - & 2.86\(\pm\)0.19 \\  & & \(\epsilon\)-D & RM & - & - & - & - & - \\ \hline \multirow{2}{*}{WR-24,10,200} & \multirow{2}{*}{\begin{tabular}{} \end{tabular} } & 2.41\(\pm\)0.10 & 3.5\(\pm\)0.21 & 2.3\(\pm\)0.22 & - & - & - & 2.86\(\pm\)0.19 \\  & & \(\epsilon\)-D & RM & - & - & - & - & - \\ \hline \multirow{2}{*}{WR-24,10,200} & \multirow{2}{*}{\begin{tabular}{} \end{tabular} } & 2.41\(\pm\)0.10 & 3.5\(\pm\)0.21 & 2.3\(\pm\)0.22 & - & - & - & - & - \\ \hline \multirow{2}{*}{WR-24,10,200} & \multirow{2}{*}{\begin{tabular}{} \end{tabular} } & 2.41\(\pm\)0.10 & 3.5\(\pm\)0.21 & 2.3\(\pm\)0.21 & 2.3\(\pm\)0.22 \\  & & \(\epsilon\)-D & RM & - & - & - & - & - \\ \hline \multirow{2}{*}{WR-24,10,200} & \multirow{2}{*}{\begin{tabular}{} \end{tabular} } & 2.41\(\pm\)0.10 & 3.5\(\pm\)0.21 & 2.3\(\pm\)0.21 & 2.3\(\pm\)0.21 \\  & & \(\epsilon\)-D & RM & - & - & - & - & - & - \\ \hline \multirow{2}{*}{WR-24,10,200} & \multirow{2}{*}{
\begin{tabular}{

### Leveraging More Data to Improve Privacy Measurements

PANORAMIA can leverage much more data for its measurement (up to the whole training set size for training and testing the MIA), while in our setting O(1) is limited by the size of the test set (for non-members). As we have seen in Figure 6(b), PANORAMIA can leverage larger test set size to measure higher privacy loss values than O(1), up to \(2.62\) on the ResNet101 trained for 100 epochs, versus \(2.23\) for O(1). It is also important to note that at small amounts of data O(1) measures a larger privacy loss (\(\tilde{\epsilon}\leq\epsilon\)). These findings provide additional empirical evidence for the robustness of \(b\), indicating that we are not erroneously attributing differences between \(\mathcal{G}\) and \(\mathcal{D}\) to our privacy loss proxy \(\tilde{\epsilon}\). However, in the case of text data (on the GPT-2 trained for 150 epochs), Figure 6(d), we hit an upper-bound on the power of the hypothesis test which estimates \(\{c+\epsilon\}_{\text{IB}}\) using \(20k\) test samples. The maximum measurement for \(\tilde{\epsilon}\) we can achieve is around \(2.59\) with a \(20k\) test set, as opposed to O(1), which reaches \(5.73\) with \(10k\) test set size.

**Increasing Test Set Size for DP models:** With an increase in the test set size \(m\), we demonstrate that we can achieve tighter bounds on the privacy measurement \(\tilde{\epsilon}\). Figure 6(c) shows that as \(m\) increases, PANORAMIA (solid lines) can measure higher privacy leakage for various \(\epsilon\)-DP ResNet18 models. The reason for this is that PANORAMIA can leverage a much higher test set size (up to the whole training set size for training and testing the MIA), in comparison to O(1) in our setting (due to being limited by the number of real non-member data points available).

## 6 Impact, Limitations and Future Directions

We have introduced a new approach to quantify the privacy leakage from ML models, in settings in which individual data contributors (such as a hospital in a cross-site FL setting or a user of a text auto-complete service) measure the leakage of their own, known partial training data in the final trained model. Our approach does not introduce new attack capabilities (it would likely benefit from future progress on MIA performance though), but can help model providers and data owners assess the privacy leakage incurred by their data, due to inclusion in ML models training datasets. Consequently, we believe that any impact from our proposed work is likely to be positive.

However, PANORAMIA suffers from a major limitation: its privacy measurement is not a lower-bound on the privacy loss (see details in Section 4.2). Providing a proper lower-bound \(\epsilon_{lb}\) on privacy leakage in this setting is an important avenue for future work. A promising direction is to devise a way to measure or enforce an upper-bound on \(c\), thereby yielding \(\epsilon_{lb}\) via \(\{c+\epsilon\}_{lb}-c_{ub}\). Despite this shortcoming, we believe that the new measurement setting we introduce in this work is important. Indeed, recent work has shown that MIA evaluation on foundation models suffers from a similar lack of non-member data (since all in distribution data is included in the model) Das et al. (2024); Duan et al. (2024); Meeus et al. (2024). The theory we develop in this paper provides a meaningful step towards addressing privacy measurements in this setting, and provides a more rigorous approach to privacy benchmarks for such models. We also demonstrate that PANORAMIA's privacy measurements can also be empirically valuable for instance for providing improved measurements with more data (Figure 6).

Figure 6: Effect of increasing test set size on PANORAMIA’s privacy measurement for our target models. In the case of the image dataset, increasing the number of auditing examples allows us to achieve tighter empirical measurements for privacy leakage despite a restricted adversary. For the case of the language modeling task, when increasing test set size \(m\), we hit an upper bound on the power of the hypothesis test due to test dataset size, and do not see significant improvement in privacy measurement in this case.

## Acknowledgments

Mathias Lecuyer is grateful for the support of the Natural Sciences and Engineering Research Council of Canada (NSERC) [reference number RGPIN-2022-04469], as well as a Google Research Scholar award. Sebastien Gambs is supported by the Canada Research Chair program, a Discovery Grant from the Natural Sciences and Engineering Research Council of Canada (NSERC) as well as the DEEL Project CRDPJ 537462-18 funded by the NSERC and the Consortium for Research and Innovation in Aerospace in Quebec (CRIAQ). This research was enabled by computational support provided by the Digital Research Alliance of Canada (alliancecan.ca). Hadrien Lautraite is supported by National Bank of Canada and a bursary from the Canada Research Chair in Privacy-preserving and Ethical Analysis of Big Data..

## References

* Abadi et al. (2016) Abadi, M., Chu, A., Goodfellow, I., McMahan, H. B., Mironov, I., Talwar, K., and Zhang, L. Deep learning with differential privacy. In _Proceedings of the 2016 ACM SIGSAC conference on computer and communications security_, pp. 308-318, 2016.
* Andrew et al. (2023) Andrew, G., Kairouz, P., Oh, S., Oprea, A., McMahan, H. B., and Suriyakumar, V. One-shot empirical privacy estimation for federated learning, 2023.
* Carlini et al. (2019) Carlini, N., Liu, C., Erlingsson, U., Kos, J., and Song, D. The secret sharer: Evaluating and testing unintended memorization in neural networks. In _28th USENIX Security Symposium (USENIX Security 19)_, pp. 267-284, 2019.
* Carlini et al. (2021) Carlini, N., Tramer, F., Wallace, E., Jagielski, M., Herbert-Voss, A., Lee, K., Roberts, A., Brown, T., Song, D., Erlingsson, U., et al. Extracting training data from large language models. In _30th USENIX Security Symposium (USENIX Security 21)_, 2021.
* Carlini et al. (2022) Carlini, N., Chien, S., Nasr, M., Song, S., Terzis, A., and Tramer, F. Membership inference attacks from first principles. In _2022 2022 IEEE Symposium on Security and Privacy (SP) (SP)_, pp. 1519-1519, Los Alamitos, CA, USA, may 2022a. IEEE Computer Society. doi: 10.1109/SP46214.2022.00090. URL https://doi.ieeecomputersociety.org/10.1109/SP46214.2022.00090.
* Carlini et al. (2022b) Carlini, N., Chien, S., Nasr, M., Song, S., Terzis, A., and Tramer, F. Membership inference attacks from first principles. In _2022 IEEE Symposium on Security and Privacy (SP)_, pp. 1897-1914. IEEE, 2022b.
* Das et al. (2024) Das, D., Zhang, J., and Tramer, F. Blind baselines beat membership inference attacks for foundation models. _arXiv preprint arXiv:2406.16201_, 2024.
* Dong et al. (2019) Dong, J., Roth, A., and Su, W. J. Gaussian differential privacy. _arXiv preprint arXiv:1905.02383_, 2019.
* Dosovitskiy et al. (2021) Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., and Houlsby, N. An image is worth 16x16 words: Transformers for image recognition at scale, 2021. URL https://arxiv.org/abs/2010.11929.
* Duan et al. (2024) Duan, M., Suri, A., Mireshghallah, N., Min, S., Shi, W., Zettlemoyer, L., Tsvetkov, Y., Choi, Y., Evans, D., and Hajishirzi, H. Do membership inference attacks work on large language models? _arXiv preprint arXiv:2402.07841_, 2024.
* Dwork et al. (2006) Dwork, C., McSherry, F., Nissim, K., and Smith, A. Calibrating noise to sensitivity in private data analysis. In _Theory of cryptography conference_. Springer, 2006.
* He et al. (2015) He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learning for image recognition, 2015.
* Holtzman et al. (2019) Holtzman, A., Buys, J., Du, L., Forbes, M., and Choi, Y. The curious case of neural text degeneration. _arXiv preprint arXiv:1904.09751_, 2019.
* Jagielski et al. (2020) Jagielski, M., Ullman, J., and Oprea, A. Auditing differentially private machine learning: How private is private sgd? _Advances in Neural Information Processing Systems_, 33:22205-22216, 2020.
* Jagielski et al. (2020)Jayaraman, B. and Evans, D. E. Evaluating differentially private machine learning in practice. In _USENIX Security Symposium_, 2019.
* Kairouz et al. (2015) Kairouz, P., Oh, S., and Viswanath, P. The composition theorem for differential privacy. In _International conference on machine learning_. PMLR, 2015.
* Karras et al. (2020) Karras, T., Aittala, M., Hellsten, J., Laine, S., Lehtinen, J., and Aila, T. Training generative adversarial networks with limited data, 2020.
* Krizhevsky (2009) Krizhevsky, A. Learning multiple layers of features from tiny images. 2009. URL https://api.semanticscholar.org/CorpusID:18268744.
* Li et al. (2021) Li, X., Tramer, F., Liang, P., and Hashimoto, T. Large language models can be strong differentially private learners. _arXiv preprint arXiv:2110.05679_, 2021.
* Liu et al. (2015) Liu, Z., Luo, P., Wang, X., and Tang, X. Deep learning face attributes in the wild. In _Proceedings of International Conference on Computer Vision (ICCV)_, December 2015.
* Lu et al. (2023) Lu, F., Munoz, J., Fuchs, M., LeBlond, T., Zaresky-Williams, E., Raff, E., Ferraro, F., and Testa, B. A general framework for auditing differentially private machine learning, 2023.
* Maddock et al. (2023) Maddock, S., Sablayrolles, A., and Stock, P. Canife: Crafting canaries for empirical privacy measurement in federated learning, 2023.
* McKenna et al. (2021) McKenna, R., Miklau, G., and Sheldon, D. Winning the nist contest: A scalable and general approach to differentially private synthetic data. _arXiv preprint arXiv:2108.04978_, 2021.
* Meeus et al. (2024) Meeus, M., Jain, S., Rei, M., and de Montjoye, Y.-A. Inherent challenges of post-hoc membership inference for large language models. _arXiv preprint arXiv:2406.17975_, 2024.
* Merity et al. (2016) Merity, S., Xiong, C., Bradbury, J., and Socher, R. Pointer sentinel mixture models. _arXiv preprint arXiv:1609.07843_, 2016.
* Nasr et al. (2019) Nasr, M., Shokri, R., and Houmansadr, A. Comprehensive privacy analysis of deep learning: Passive and active white-box inference attacks against centralized and federated learning. In _2019 IEEE symposium on security and privacy (SP)_, pp. 739-753. IEEE, 2019.
* Nasr et al. (2021) Nasr, M., Songi, S., Thakurta, A., Papernot, N., and Carlin, N. Adversary instantiation: Lower bounds for differentially private machine learning. In _2021 IEEE Symposium on Security and Privacy (SP)_, pp. 866-882, 2021. doi: 10.1109/SP40001.2021.00069.
* Nasr et al. (2023) Nasr, M., Hayes, J., Steinke, T., Balle, B., Tramer, F., Jagielski, M., Carlini, N., and Terzis, A. Tight auditing of differentially private machine learning, 2023.
* Negoescu et al. (2023) Negoescu, D. M., Gonzalez, H., Orjany, S. E. A., Yang, J., Lut, Y., Tandra, R., Zhang, X., Zheng, X., Douglas, Z., Nolkha, V., et al. Epsilon*: Privacy metric for machine learning models. _arXiv preprint arXiv:2307.11280_, 2023.
* O'Shea & Nash (2015) O'Shea, K. and Nash, R. An introduction to convolutional neural networks, 2015.
* Radford et al. (2019) Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et al. Language models are unsupervised multitask learners. _OpenAI blog_, 1(8):9, 2019.
* Steinke et al. (2023) Steinke, T., Nasr, M., and Jagielski, M. Privacy auditing with one (1) training run. In _Thirty-seventh Conference on Neural Information Processing Systems_, 2023.
* Suri et al. (2018) Suri, A., Zhang, X., and Evans, D. Do parameters reveal more than loss for membership inference? In _High-dimensional Learning Dynamics 2024: The Emergence of Structure and Reasoning_.
* Wasserman & Zhou (2010) Wasserman, L. and Zhou, S. A statistical framework for differential privacy. _Journal of the American Statistical Association_, 2010.
* Yeom et al. (2018) Yeom, S., Giacomelli, I., Fredrikson, M., and Jha, S. Privacy risk in machine learning: Analyzing the connection to overfitting. In _2018 IEEE 31st computer security foundations symposium (CSF)_, pp. 268-282. IEEE, 2018.
* Zhang et al. (2019)Yousefpour, A., Shilov, I., Sablayrolles, A., Testuggine, D., Prasad, K., Malek, M., Nguyen, J., Ghosh, S., Bharadwaj, A., Zhao, J., Cormode, G., and Mironov, I. Opacus: User-friendly differential privacy library in PyTorch. _arXiv preprint arXiv:2109.12298_, 2021.
* Zagoruyko and Komodakis (2017a) Zagoruyko, S. and Komodakis, N. Wide residual networks, 2017a.
* Zagoruyko and Komodakis (2017b) Zagoruyko, S. and Komodakis, N. Wide residual networks, 2017b. URL https://arxiv.org/abs/1605.07146.
* Zanella-Beguelin et al. (2022) Zanella-Beguelin, S., Wutschitz, L., Tople, S., Salem, A., Ruhle, V., Paverd, A., Naseri, M., Kopf, B., and Jones, D. Bayesian estimation of differential privacy, 2022.
* Zarifzadeh et al. (2023) Zarifzadeh, S., Liu, P. C.-J. M., and Shokri, R. Low-cost high-power membership inference by boosting relativity. 2023.

## Appendix A Notations

Table 4 summarizes the main notations used in the paper.

## Appendix B Proofs

For both Proposition 1 and Proposition 2, we state the proposition again for convenience before proving it.

### Proof of Proposition 1

**Proposition 1**.: _Let \(\mathcal{G}\) be \(c\)-close, and \(T^{b}\triangleq B(S,X)\) be the guess from the baseline. Then, for all \(v\in\mathbb{R}\) and all \(t\) in the support of \(T\):_

\[\mathbb{P}_{S,X,T^{b}}\Big{[}\sum_{i=1}^{m}T^{b}_{i}\cdot S_{i}\geq v\mid T^{ b}=t^{b}\Big{]}\leq\underset{S^{\prime}\sim\text{Bernoulli}(\frac{c^{c}}{1+e^{c}})^{m}}{ \mathbb{P}}\Big{[}\sum_{i=1}^{m}t^{b}_{i}\cdot S^{\prime}_{i}\geq v\Big{]} \triangleq\beta^{b}(m,c,v,t^{b})\]

Proof.: Notice that under our baseline model \(B(s,x)=\{b(x_{1}),b(x_{2}),\ldots,b(x_{m})\}\), and given that the \(X_{i}\) are i.i.d., we have that: \(S_{<i}\perp T^{b}_{<i}\mid X_{<i}\), since \(T^{b}_{i}=B(S,X)_{i}\)'s distribution is entirely determined by \(X_{i}\); and \(S_{\leq i}\perp T^{b}_{<i}\mid X_{<i}\) since the \(X_{i}\) are sampled independently from the past.

We study the distribution of \(S\) given a fixed prediction vector \(t^{b}\), one element \(i\in[m]\) at a time:

\[\mathbb{P}\big{[}S_{i}=1\mid T^{b}=t^{b},S_{<i}=s_{<i},X_{\leq i }=x_{\leq i}\big{]}\] \[=\mathbb{P}\big{[}S_{i}=1\mid S_{<i}=s_{<i},X_{\leq i}=x_{\leq i} \big{]}\] \[=\mathbb{P}\big{[}X_{i}\mid S_{i}=1,S_{<i}=s_{<i},X_{<i}=x_{<i} \big{]}\frac{\mathbb{P}\big{[}S_{i}=1\mid S_{<i}=s_{<i},X_{<i}=x_{<i}\big{]}}{ \mathbb{P}\big{[}X_{i}\mid S_{<i}=s_{<i},X_{<i}=x_{<i}\big{]}}\] \[=\frac{\mathbb{P}\big{[}X_{i}\mid S_{i}=1,S_{<i}=s_{<i},X_{<i}=x_ {<i}\big{]}\mathbb{P}\big{[}S_{i}=1\big{]}}{\mathbb{P}\big{[}X_{i}\mid S_{<i} =s_{<i},X_{<i}=x_{<i}\big{]}}\] \[=\frac{\mathbb{P}\big{[}X_{i}\mid S_{i}=1\big{]}\frac{1}{2}}{1+ \frac{\mathbb{P}\big{[}X_{i}\mid S_{i}=0\big{]}}{\mathbb{P}\big{[}X_{i}\mid S _{i}=1\big{]}}}=\frac{1}{1+\frac{\mathbb{P}_{\mathcal{G}}\big{[}X_{i}\mid}{ \mathbb{P}_{\mathcal{D}}\big{[}X_{i}\big{]}}}\leq\frac{1}{1+e^{-c}}=\frac{e^{ c}}{1+e^{c}}\]

The first equality uses the independence remarks at the beginning of the proof, the second relies Bayes' rule, while the third and fourth that \(S_{i}\) is sampled i.i.d from a Bernoulli with probability half, and \(X_{i}\) i.i.d. conditioned on \(S_{i}\). The last inequality uses Definition 3 for \(c\)-closeness.

\begin{table}
\begin{tabular}{l l} \hline \hline
**Notation** & **Description** \\ \hline \(f\) & the target model to be audited. \\ \(\mathcal{G}\) & the generative model \\ \(\mathcal{D}\) & distribution over auditor samples \\ \(D_{f}\) & (subset of the) training set of the target model \(f\) from \(\mathcal{D}\) \\ \(D_{G}\) & training set of the generative model \(\mathcal{G}\), with \(D_{G}\subset D_{f}\) \\ \(D_{\text{in}}\) & member auditing set, with \(D_{\text{in}}\subset D_{f}\) and \(D_{\text{in}}\cap D_{G}=\{\}\) \\ \(D_{\text{out}}\) & non-member auditing set, with \(D_{\text{out}}\sim\mathcal{G}\) \\ \(D_{\text{in,out}}^{\text{(tr,te)}}\) & training and testing splits of \(D_{\text{in}}\) and \(D_{\text{out}}\) \\ \(m\) & \(|D_{\text{in}}|=|D_{\text{out}}|\triangleq m\) \\ \(b\) & baseline classifier for \(D_{\text{in}}\) vs. \(D_{\text{out}}\) \\ \hline \hline \end{tabular}
\end{table}
Table 4: Summary of notationsUsing this result and the law of total probability to introduce conditioning on \(X_{\leq i}\), we get that:

\[\mathbb{P}\big{[}S_{i}=1\mid T^{b}=t^{b},S_{<i}=s_{<i}\big{]}\] \[=\sum_{x_{\leq i}}\mathbb{P}\big{[}S_{i}=1\mid T^{b}=t^{b},S_{<i}=s _{<i},X_{\leq i}=x_{\leq i}\big{]}\mathbb{P}\big{[}X_{\leq i}=x_{\leq i}\mid T^{ b}=t^{b},S_{<i}=s_{<i}\big{]}\] \[\leq\sum_{x_{\leq i}}\frac{e^{c}}{1+e^{c}}\mathbb{P}\big{[}X_{\leq i }=x_{\leq i}\mid T^{b}=t^{b},S_{<i}=s_{<i}\big{]},\]

and hence that:

\[\mathbb{P}\big{[}S_{i}=1\mid T^{b}=t^{b},S_{<i}=s_{<i}\big{]}\leq\frac{e^{c}}{ 1+e^{c}}\] (3)

We can now proceed by induction: assume inductively that \(W_{m-1}\triangleq\sum_{i=1}^{m-1}T_{i}^{b}\cdot S_{i}\) is stochastically dominated (see Definition 4.8 in Steinke et al. (2023)) by \(W^{\prime}_{m-1}\triangleq\sum_{i=1}^{m-1}T_{i}^{b}\cdot S_{i}^{\prime}\), in which \(S^{\prime}\sim\text{Bernoulli}(\frac{e^{c}}{1+e^{c}})^{m-1}\). Setting \(W_{1}=W^{\prime}_{1}=0\) makes it true for \(m=1\). Then, conditioned on \(W_{m-1}\) and using Eq. 3, \(T_{m}^{b}\cdot S_{m}=T_{m}\cdot\mathds{1}\{S_{m}=1\}\) is stochastically dominated by \(T_{m}^{b}\cdot\text{Bernoulli}(\frac{e^{c}}{1+e^{c}})\). Applying Lemma 4.9 from Steinke et al. (2023) shows that \(W_{m}\) is stochastically dominated by \(W^{\prime}_{m}\), which proves the induction and implies the proposition's statement. 

### Proof of Proposition 2

**Proposition 2**.: _Let \(\mathcal{G}\) be \(c\)-close, \(f\) be \(\epsilon\)-DP, and \(T^{a}\triangleq A(S,X,f)\) be the guess from the membership audit. Then, for all \(v\in\mathbb{R}\) and all \(t\) in the support of \(T\):_

\[\mathbb{P}_{S,X,T^{a}}\Big{[}\sum_{i=1}^{m}T_{i}^{a}\cdot S_{i}\geq v\mid T^{ a}=t^{a}\Big{]}\leq\mathbb{P}_{S^{\prime}\sim\text{Bernoulli}(\frac{e^{c+\epsilon}}{1+e^{c+ \epsilon}})^{m}}\Big{[}\sum_{i=1}^{m}t_{i}^{a}\cdot S_{i}^{\prime}\geq v\Big{]} \triangleq\beta^{a}(m,c,\epsilon,v,t^{a})\]

Proof.: Fix some \(t^{a}\in\mathbb{R}_{+}^{m}\). We study the distribution of \(S\) one element \(i\in[m]\) at a time:

\[\mathbb{P}\big{[}S_{i}=1\mid T^{a}=t^{a},S_{<i}=s_{<i},X_{\leq i} =x_{\leq i}\big{]}\] \[=\mathbb{P}\big{[}T^{a}=t^{a}\mid S_{i}=1,S_{<i}=s_{<i},X_{\leq i }=x_{\leq i}\big{]}\frac{\mathbb{P}\big{[}S_{i}=1\mid S_{<i}=s_{<i},X_{\leq i} =x_{\leq i}\big{]}}{\mathbb{P}\big{[}T^{a}=t^{a}\mid S_{<i}=s_{<i},X_{\leq i} =x_{\leq i}\big{]}}\] \[\leq\frac{1}{1+e^{-\epsilon}e^{-c}}=\frac{e^{c+\epsilon}}{1+e^{ c+\epsilon}}\]

The first equality uses Bayes' rule. The first inequality uses the decomposition:

\[\mathbb{P}\big{[}T^{a}=t^{a}\mid S_{<i}=s_{<i},X_{\leq i}=x_{ \leq i}\big{]}\] \[=\mathbb{P}\big{[}T^{a}=t^{a}\mid S_{i}=1,S_{<i}=s_{<i},X_{\leq i }=x_{\leq i}\big{]}\cdot\mathbb{P}\big{[}S_{i}=1\mid S_{<i}=s_{<i},X_{\leq i }=x_{\leq i}\big{]}\] \[+\mathbb{P}\big{[}T^{a}=t^{a}\mid S_{i}=0,S_{<i}=s_{<i},X_{\leq i }=x_{\leq i}\big{]}\cdot\mathbb{P}\big{[}S_{i}=0\mid S_{<i}=s_{<i},X_{\leq i }=x_{\leq i}\big{]},\]

and the fact that \(A(s,x,f)\) is \(\epsilon\)-DP w.r.t. \(s\) and hence that:

\[\frac{\mathbb{P}\big{[}T^{a}=t^{a}\mid S_{i}=0,S_{<i}=s_{<i},X_{ \leq i}=x_{\leq i}\big{]}}{\mathbb{P}\big{[}T^{a}=t^{a}\mid S_{i}=1,S_{<i}=s_{ <i},X_{\leq i}=x_{\leq i}\big{]}}\geq e^{-\epsilon}.\] (4)

This inequality might seem surprising at first, as we know from the privacy game (Definition 2) that \(S\) is independent of \(f\), that is \(S\perp\!\!\!\perp f\). However, these two quantities are not conditionally independent when conditioning on \(X\) (where \(X\) is the test set for the MIA/baseline), (i.e., \(S\perp\!\!\!\perp f|X\)). This is because \(X_{i}\) is either a member or non-member based on \(S_{i}\). So when \(S_{i}=1\), then \(f\) was trained on \(X_{i}\), which makes \(f\) and \(S_{i}\) dependent if membership can be detected through \(f\). If \(f\) is \(\epsilon\)-DP, we can bound this dependency using DP properties, which is what we do in Equation (4).

The second inequality uses that:

\[\frac{\mathbb{P}\big{[}S_{i}=0\mid S_{<i}=s_{<i},X_{\leq i}=x_{\leq i }\big{]}}{\mathbb{P}\big{[}S_{i}=1\mid S_{<i}=s_{<i},X_{\leq i}=x_{\leq i}\big{]}}\] \[=\frac{\mathbb{P}\big{[}X_{i}=x_{i}\mid S_{i}=0,S_{<i}=s_{<i},X_{<i }=x_{<i}\big{]}}{\mathbb{P}\big{[}X_{i}=x_{i}\mid S_{i}=1,S_{<i}=s_{<i},X_{<i}=x _{<i}\big{]}}\cdot\frac{\mathbb{P}\big{[}S_{i}=0\mid S_{<i}=s_{<i},X_{<i}=x_{<i }\big{]}}{\mathbb{P}\big{[}S_{i}=1\mid S_{<i}=s_{<i},X_{<i}=x_{<i}\big{]}}\] \[=\frac{\mathbb{P}\big{[}X_{i}=x_{i}\mid S_{i}=0,S_{<i}=s_{<i},X_{< i}=x_{<i}\big{]}}{\mathbb{P}\big{[}X_{i}=x_{i}\mid S_{i}=1,S_{<i}=s_{<i},X_{<i}=x _{<i}\big{]}}\cdot\frac{1/2}{1/2}\] \[=\frac{\mathbb{P}_{\hat{\mathcal{G}}}\big{[}X_{i}\big{]}}{\mathbb{ P}_{\hat{\mathcal{D}}}\big{[}X_{i}\big{]}}\geq e^{-c}\]

As in Proposition 1, applying the law of total probability to introduce conditioning on \(X_{\leq i}\) yields:

\[\mathbb{P}\big{[}S_{i}=1\mid T^{a}=t^{a},S_{<i}=s_{<i}\big{]}\leq\frac{e^{c+ \epsilon}}{1+e^{c+\epsilon}},\] (5)

and we can proceed by induction. Assume inductively that \(W_{m-1}\triangleq\sum_{i=1}^{m-1}T_{i}^{a}\cdot S_{i}\) is stochastically dominated (see Definition 4.8 in Steinke et al. (2023)) by \(W^{\prime}_{m-1}\triangleq\sum_{i=1}^{m-1}T_{i}^{a}\cdot S^{\prime}_{i}\), in which \(S^{\prime}\sim\text{Bernoulli}(\frac{e^{c+\epsilon}}{1+e^{c+\epsilon}})^{m-1}\). Setting \(W_{1}=W^{\prime}_{1}=0\) makes it true for \(m=1\). Then, conditioned on \(W_{m-1}\) and using Eq. \(5,T_{m}^{a}\cdot S_{m}=T_{m}^{a}\cdot\mathds{1}\{S_{m}=1\}\) is stochastically dominated by \(T_{m}^{a}\cdot\text{Bernoulli}(\frac{e^{c+\epsilon}}{1+e^{c+\epsilon}})\). Applying Lemma 4.9 from Steinke et al. (2023) shows that \(W_{m}\) is stochastically dominated by \(W^{\prime}_{m}\), which proves the induction and implies the proposition's statement. 

## Appendix C Experimental Details

Hereafter, we provide the details about the datasets and models trained in our experiments.

### Image data

**Target models.** We audit target models with the following architectures: a Multi-Label Convolutional Neural Network (CNN) with four layers O'Shea and Nash (2015), and the ResNet101 He et al. (2015) as well as a Wide ResNet-28-10 Zagoruyko and Komodakis (2017), and a Vision Transformer (ViT)-small Dosovitskiy et al. (2021). We also include in our analysis, differentially-private models for ResNet18 He et al. (2015) and WideResNet-16-4 Zagoruyko and Komodakis (2017) models as targets, with \(\epsilon=1,2,4,6,10,15,20\). The ResNet-based models are trained on CIFAR10 using \(50k\) images Krizhevsky (2009) of \(32\mathrm{x}32\) resolution. For ResNet101 CIFAR10-based classification models, we use a training batch size of 32; for WideResNet-28-10, we use batch size 128. The details for training DP models are given in D.9. The associated test accuracies and epochs are mentioned in Table 1. The Multi-Label CNN is trained on \(200k\) images of CelebA Liu et al. (2015) of \(128\mathrm{x}128\) resolution, training batch-size 32, to predict \(40\) attributes associated with each image.

**Generator.** For both image datasets, we use StyleGAN2 Karras et al. (2020) to train the generative model \(\mathcal{G}\) from scratch on \(D_{G}\), and produce non-member images. For CIFAR10 dataset, we use a \(10,000\) out of \(50,000\) images from the training data of the target model to train the generative model. For the CelebA dataset, we select \(35,000\) out of \(200,000\) images from the training data of the target model to train the generative model. Generated images will in turn serve as non-members for performing the MIAs. Figure 7 shows examples of member and non-member images used in our experiments. In the case of CelebA, we also introduce a vanilla CNN as a classifier or filter to distinguish between fake and real images, and remove any poor-quality images that the classifier detects with high confidence. The data used to train this classifier was the same data used to train StyleGAN2, which ensures that the generated high-resolution images are of high quality.

**MIA and Baseline training.** For the MIA, we follow a loss-based attack approach: PANORAMIA takes as input raw member and non-member data points for training along with the loss values the target model \(f\) attributes to these data points. More precisely, the training set of PANORAMIA is:

\[(D_{in}^{tr},f(D_{in}^{tr}))\cup(D_{out}^{tr},f(D_{out}^{tr}))\]In SS4.2, we discussed the importance of having a tight \(c_{lb}\) so that our measure, \(\tilde{\epsilon}\), becomes close to a lower-bound on \(\epsilon\)-DP, which requires a strong baseline. To strengthen our baseline, we introduce the helper model \(h\), which helps the baseline model \(b\) by supplying additional features (_i.e._, embeddings) that can be viewed as side information about the data distribution. The motivation is that \(h\)'s features might differ between samples from \(\mathcal{D}\) and \(\mathcal{D}^{\prime}\), enhancing the performance of the baseline classifier. This embedding model \(h\) is similar in design to \(f\) (same task and architecture) but is trained on synthetic data that is close in distribution to the real member data distribution.

The helper model has the same classification task and architecture as the target model. To train it, we generate separate sets of training and validation data using our generator. For image data, the synthetic samples do not have a label. We thus train a labeler model, also a classifier with the same task, on the same dataset used to train the generator. We use the labeler model to provide labels for our synthetic samples (training and validation sets above). We train the helper model on the resulting training set, and select hyperparameters on the validation set.

Whether for the baseline or MIA, we use side information models (\(h\) and \(f\), respectively) by concatenating the loss of \(h(x)\) and \(f(x)\) to the final feature representation (more details are provided later) before a last layer of the MIA/Baseline makes the membership the prediction. Since we need labels to compute the loss, we label synthetic images with a Wide ResNet-28-2 in the case of CIFAR10, and a Multi-Label CNN of similar architecture as the target model in the case of CelebA labeling. For both instances, we used a subset of the data, that was used to train the respective generative models, to train the "labeler" classifiers as well. The labeler used to train the helper model for image data has 80.4% test accuracy on the CIFAR10 real data test set. The rationale for this approach is to augment the baseline with a model providing good features (here for generated data) to balance the good features provided to the MIA by \(f\) outside of the membership information. In practice, the labeled generated data seems enough to provide such good features, despite the fact that the labeler is not extremely accurate. We studied alternative designs (e.g., a model trained on non-member data, no helper model) in Appendix D.1, Table 5, and the helper model trained on the synthetic data task performs best (while not requiring non-member data, which is a key point).

We use two different modules for each MIA and Baseline training. More precisely, the first module optimizes image classification using a built-in Pytorch ResNet101 classifier. The second module, in the form of a multi-layer perceptron, focuses on classifying member and non-member labels via loss values attributed to these data points by \(f\) as input for the loss module of MIA and losses of \(e\) to the baseline \(b\) respectively. We then stack the scores of both image and loss modules into a logistic regression task (as a form of meta-learning) to get the final outputs for member and non-member data points by MIA and baseline \(b\). When training the baseline and MIA models, we use a validation set to select hyper-parameters and training stopping time that maximize the lower bounds (effectively maximizing the \(c_{lb}\) or \(\tilde{\epsilon}_{lb}\) for the Baseline and MIA respectively) on the validation set. The MIA and baseline are trained on \(4500\) data samples (half members and half generated non-members). The test dataset consists of \(10000\) samples, again half members and half generated non-members. The actual and final number of members and non-members that ended up in the test set depends on the Bernoulli samples in our auditing game. We repeat the training process over 5 times, each time independently resampling the training dataset for the MIA and baseline (keeping the train dataset size fixed). We report all our results over a \(95\%\) confidence interval and report the standard deviation for our results in Table 3. The **compute resources** used for MIA and Baseline training were mainly running all experiments on cloud-hosted VMs, using 1 v100l GPU, 4 nodes per task, and 32G memory requested for each job on the cloud cluster. The time to run the MIA and baseline attack pipeline was around 10 hours including hyperparameter tuning on \(\epsilon_{lb}\) and \(c_{lb}\) respectively.

### Language Modeling

**Target model.** The target model (a small GPT-2 Radford et al. (2019)) task is causal language modeling (CLM), a self-supervised task in which the model predicts the next word in a given sequence of words (_i.e._, context), which is done on a subset of WikiText-103 dataset Merity et al. (2016), a collection of Wikipedia articles. While the GPT-2 training dataset is undisclosed, Radford et al. (2019) do state that they did not include any Wikipedia document.

One common standard pre-processing in CLM is to break the tokenized training dataset into chunks of equal sizes to avoid padding of the sequences. To achieve this, the entire training dataset is split into token sequences of fixed length, specifically 64 tokens per sequence, refer hereafter as "chunks".

Each of these chunks serves as an individual sample within our training dataset. Therefore, in the experiments, the membership inference will be performed for a chunk. In other words, we adopt an example-level differential privacy (DP) viewpoint, where an example is considered a sequence of tokens that constitute a row in a batch of samples. This chunk-based strategy offers additional benefits in synthetic sample generation. By maintaining uniformity in the length of these synthetic samples, which also consists of 64 tokens, it is possible to effectively mitigate any distinguishability between synthetic and real samples based solely on their length. We also consider the possibility of weak correlations between chunks (e.g., being from the same article). To mitigate this, we perform a form of stratified sampling as a pre-processing step. From each document, we include only the first 50 chunks and discard the rest. We select 2,113 documents in total, resulting in 105,650 chunks in our dataset.

We split the dataset into train and test with a 90:10 ratio for the target model (We insist on following the typical training pipelines in machine learning, since we plan to conduct post-hoc audits with PANORAMIA). The test dataset provides real non-member samples, allowing comparison to the O(1) auditor and another version of our approach which uses real non-member data. The training dataset (\(D_{f}\)) contains 95,085 samples, while the test dataset consists of 10,565 samples.

We train the target model for 200 epochs in total. Figure 8 displays how the training and validation cross-entropy loss changes throughout the training, when we audit models from epochs 37 (best generalization), 75, and 150. To check how well the target model generalizes, we look at the cross-entropy loss (on a validation set), which is the only metric in a causal language model task to report (or the perplexity Radford et al. (2019) which conveys the same information).

The **compute resources** used for training the target models are 4 V100-32gb GPUs, running on cloud-hosted VMs. Using this setup, fine-tuning the target model for 200 epochs required about 13 hours.

**Generator.** The \(\mathcal{G}\) is a GPT-2 fine-tuned using a CLM task on dataset \(D_{G}\), a subset of \(D_{f}\). To create synthetic samples with \(\mathcal{G}\), we use top-\(k\) sampling Holtzman et al. (2019) method in an auto-regressive manner while keeping the generated sequence length fixed at 64 tokens. To make \(\mathcal{G}\) generate samples less like the real members it learned from (and hopefully, more like real non-members), we introduce more randomness in the generation process in the following manner. We choose a top_k value of

Figure 8: Target model (GPT-2) loss during overtraining on WikiText dataset. We pick GPT-2_E37, GPT-2_E75, GPT-2_E150 as target models to audit corresponding to the gray dashed lines.

Figure 7: Member and Non-Member datasets used in our experiments for CelebA at \(128\) x \(128\) resolution (\(7(\texttt{a})\), \(7(\texttt{b})\)) and CIFAR10 (\(7(\texttt{c})\), \(7(\texttt{d})\)) image data at \(32\) x \(32\) resolution.

200, controlling the number of available tokens for sampling. In addition, we balance the value of top_k, making it not too small to avoid repetitive generated texts but also not too large to maintain the quality. Finally, we fix the temperature parameter that controls the randomness in the softmax function at one. Indeed, a larger value increases the entropy of the distribution over tokens resulting in a more diverse generated text.

However, the quality of synthetic text depends on the prompts used. To mitigate this issue, we split the \(D_{G}\) dataset into two parts: \(D_{G}^{train}\) and \(D_{G}^{prompt}\). \(\mathcal{G}\) is fine-tuned on \(D_{G}^{train}\), while \(D_{G}^{prompt}\) is used for generating prompts. Numerically, the size of the \(D_{G}^{train}\) is \(35\%\) of the \(D_{f}\) and the size of \(D_{G}^{prompt}\) is \(15\%\) of \(D_{f}\). During generation, we sample from \(D_{G}^{prompt}\), which is a sequence of length 64. We feed the prompt into the generator and request the generator to generate a suffix of equivalent length. In our experiments, for each prompt, we make 8 synthetic samples, supplying a sufficient number of synthetic non-member samples for different parts of the pipeline of PANORAMIA, including training the helper model, and constructing \(D_{\text{out}}^{\text{tr}}\), \(D_{\text{out}}^{\text{te}}\). Overall, we generate 92276 synthetic samples out of which we use 53502 of them for training the helper model and keep the rest for audit purposes. Note that this generation approach does not inherently favor our audit scheme, and shows how PANORAMIA can leverage existing, public generative models with fine tuning to access high-quality generators at reasonable training cost.

We also perform a sanity check on the quality of the generated data. We visualize the loss distribution of in-distribution members, in-distribution non-members, and synthetic data under both the target and helper models, aiming for synthetic data that behaves similarly to in-distribution non-member data. Figure 14 visualizes these distributions for our target and helper models.

The **compute resources** used for fine-tuning the generator model, and synthetic text generation are 4 V100-32gb GPUs, running on cloud-hosted VMs. Using this setup, fine-tuning the GPT2-small model took approximately 1.5 hours, and generating the synthetic samples required about 1 hour.

**Baseline & MIA.** For both the baseline and PANORAMIA classifiers, we employ a GPT-2 based sequence classification model. In this setup, GPT-2 extracts features (_i.e._, hidden vectors) directly from the samples. We concatenate a vector of features extracted using the helper \(h\) (for the baseline) or target model \(f\) (for the MIA) to this representation. This vector of features is the loss sequence produced in a CLM task for the respective model. These two sets of extracted features are then concatenated before being processed through a feed-forward layer to generate the logit values required for binary classification (distinguishing between members and non-members).

In the main results in Table 3, for both the baseline and PANORAMIA classifiers, the training and validation sets consist of 20000 and 2000 samples, respectively, with equal member non-member distribution. The test set consists of 10000 samples, in which the actual number of members and non-members that ended up in the test set depends on the Bernoulli samples in the auditing game. The helper model is fine-tuned for \(60\) epochs on synthetic samples and we pick the model that has the lowest validation loss throughout training, generalizing well.

As the GPT-2 component of the classifiers can effectively minimize training loss without achieving strong generalization, regularization is applied to the classifier using weight decay. Additionally, the optimization process is broken into two phases. In the first phase, we exclusively update the parameters associated with the target (in PANORAMIA) or helper model (in the baseline) to ensure that this classifier has the opportunity to focus on these specific features. In the second phase, we optimize the entire model as usual. We train both the baseline and Membership Inference Attack (MIA) models with 5 different seeds. The randomness is over: 1. The data split into train/validation/test sets for both the baseline and MIA classifiers. Each classifier is retrained for every seed. 2. The coin flips in our auditing game (see Definition 2). For a fixed seed mentioned above, we employ an additional set of 5 seeds to account for the randomness in the training algorithm (such as initialization of the model, and batch sampling). We then train a classifier for each of these seeds. From the 5 models generated for each latter seed, we select the one with the largest \(c_{\text{lb}}\) value (for the baseline) or \(\{c+\epsilon\}_{\text{lb}}\) (for MIA), determined over the validation set. We calculate the 95% confidence interval of our measurements using the t-score.

The **compute resources** used for MIA and Baseline training were 1 GPU with 32gb memory, running on cloud-hosted VMs. The compute time depends on the training set size, for the largest training size, it took about 3 hours to train the baseline or MIA.

The **compute resources** used for fine-tuning the helper model are 4 V100-32gb GPUs, running on cloud-hosted VMs. Using this setup, it took about 4 hours to fine-tune the helper model.

### Tabular data

**Target \(f\) and the helper model \(\mathbf{h}\)**. The Target \(f\) and the helper \(h\) model are both Multi-Layer Perceptron with four hidden layers containing 150, 100, 50, and 10 neurons respectively. Both models are trained using a learning rate of 0.0005 over 100 training epochs. In the generalized case and to obtain the embedding, we retain the model's parameters that yield the lowest loss on a validation set, which typically occurs at epoch 10 (MLPE_10). For the overfitted scenario, we keep the model's state after the 100 training epochs (MLP_E100). The non overfitted model and the overfitted one achieved an accuracy of 86% and 82% respectively.

**Generator.** We use the MST method McKenna et al. (2021) which is a differential-private method to generate synthetic data. However, as we do not need Differential Privacy for data generation we simply set the value for \(\varepsilon\) to \(1000\). Synthetic data generators can sometimes produce bad-quality samples. Those out-of-distribution samples can affect our audit process (the detection between real and synthetic data being due to bad quality samples rather than privacy leakage). To circumvent this issue, we train an additional classifier to distinguish between real data from \(D_{G}\) and additional synthetic data (not used in the audit). We use this classifier to remove from the audit data synthetic data synthetic samples predicted as synthetic with high confidence.

**Baseline and MIA.** To distinguish between real and synthetic data, we use the Gradient Boosting model and conduct a grid search to find the best hyperparameters.

### Comparison with Privacy Auditing with One (1) Training Run: Experimental Details

We implement the black-box auditor version of O(1) approach Steinke et al. (2023). This method assigns a membership score to a sample based on its loss value under the target model. They also subtract the sample's loss under the initial state (or generally, a randomly initialized model) of the target model, helping to distinguish members from non-members even more. In our instantiation of the O(1) approach, we only consider the loss of samples on the final state of the target model. Moreover, in their audit, they choose not to guess the membership of every sample. This abstention has an advantage over making wrong predictions as it does not increase their baseline. Roughly speaking, their baseline is the total number of correct guesses achieved by employing a randomized response \((\epsilon,0)\) mechanism, for those samples that O(1) auditor opts to predict. We incorporate this abstention approach in our implementation by using two thresholds, \(t_{+}\) and \(t_{-}\). More precisely, samples with scores below \(t_{+}\) are predicted as members, those above \(t_{-}\) as non-members, and the rest are abstained from prediction. We check for different combinations of \(t_{+}\) and \(t_{-}\) and report the highest \(\epsilon\) among them. This involves performing multiple tests on the same evidence, each with a separate confidence level of 95%. To ensure that these tests hold collectively, we apply a union bound and adjust the significance level accordingly. The total number of separate tests depends on the different number of guesses that O(1) can make. We also set \(\delta\) to 0 and use a confidence interval of 0.05 in their test. In PANORAMIA, for each hypothesis test (whether for \(c_{\text{lb}}\) or \(\{c+\epsilon\}_{\text{lb}}\)), we stick to a 0.025 confidence interval for each one, adding up to an overall confidence level of 0.05.

## Appendix D Results: Additional Experiments and Detailed Discussion

### Baseline Strength Evaluation on Text and Tabular datasets

The basis of our privacy measurement directly depends on how well our baseline classifier distinguishes between real member data samples and synthetic non-member data samples generated by our generative model. As mentioned in Section 5.1 and Appendix C.1, in order to increase the performance of our baseline \(b\), we mimic the role of the target model \(f\)'s loss in the MIA using a helper model \(h\), which adds a loss-based feature to \(b\). To assess the strength of the baseline classifier, we train the baseline's helper model \(h\) on two datasets, synthetic data, and real non-members. We also train a baseline without any helper model. This enables us to identify which setup allows the baseline classifier to distinguish the member and non-member data the best in terms of the highest \(c_{lb}\) value (\(c_{lb}\) is reported for each case in table 5). For initial analysis, we keep a sub-set of real data as non-members to train a helper model with them, for conducting this ablation study on the baseline.

Our experiments confirm that the baseline with loss values from a helper model trained on synthetic data performs better compared to a helper model trained on real non-member data. This confirms the strength of our framework in terms of not having a dependency of using real non-members. Therefore, we choose this setting for our baseline helper model. In addition, we also experiment with different helper model architectures in the case of CIFAR10, looking for the best baseline performance among them. For the WikiText dataset, the \(c_{\text{lb}}\) reported in Table 5 differs from that in Table 3 because we had less data available for this experiment. We withheld half of the real samples to train the helper model with real samples in its training set.

For each data modality setting, we also train the baseline classifier on an increasing dataset size of real member and synthetic non-member data points to gauge the trend of the baseline performance as the training dataset size increases (see Figure 2 and Figure 9). This experiment allows us to aim for a worst-case empirical lower bound on how close the generated non-member data is to the real member data distribution, _i.e._ what would be the largest \(c_{\text{lb}}\) we can reach as we increase the training set size. Subsequently, we can compare the \(c_{\text{lb}}\) reported in table 3 (corresponding to the vertical dashed lines in Figure 2 and Figure 9) to the largest \(c_{\text{lb}}\) we can achieve in this experiment. In the following, we explain why we chose that number of training samples for our baseline in the main results.

On the WikiText dataset, we increase the baseline's training set size up to 50k, while fixing the validation and test sets sizes to 2000 and 10000, respectively). Figure 9 shows the changes of \(c_{\text{lb}}\) while varying the training set size. The vertical dashed line specified the number of training samples we chose for our main auditing results reported in Table 3.

Now, let us discuss why we don't choose the largest training set size available for the baseline. The measurement for \(c_{\text{lb}}\) depends not only on the training set size but also on the test set size. Therefore, we need to balance the sizes of the train and test sets. We conducted another experiment where we varied the train set size and, instead of fixing the test set size, allocated the remaining available data to the test set (keeping in mind that our data for the audit is limited by the number of real members allocated for audit purposes).Figure 9 depicts the trend of changes. As expected, we observed an initial increase in \(c_{\text{lb}}\) as the training set size increased (and the number of test data is sufficient to capture that). However, it decreased toward the end because of the smaller test set size. For a train set size of 40k, we found the best trade-off between train and test set sizes, which justifies our choice of 40k for the training set size in the main results. The same argument applies for other data modalities.

Let us make one last point. It is also important to note that the MIA is expected to show similar behavior if we use a larger train set size or test set size. Hence, the gap between the performance of MIA and baseline (which is our privacy measurement) is not expected to be affected (see Appendix D.4).

The experiments are repeated five times, and we provide a 95% confidence interval. For the image data modality, the randomness lies in the training data provided to the classifiers, while the test set remains fixed throughout. For the text data modality, both the train and test sets change across the five runs (the test set size is fixed to 10k).

### Further Analysis on General Privacy Measurement Results

The value of \(\tilde{\epsilon}\) for each target model is the gap between its corresponding dashed line and the baseline one in Figure 4. This allows us to compute values of \(\tilde{\epsilon}\) reported in Table 3. It is also interesting to note

\begin{table}
\begin{tabular}{|l|l|} \hline Baseline model & \(c_{\text{lb}}\) \\ \hline CIFAR-10 Baseline\(D_{h}^{\prime}=\text{gen}\), WRN & \(\mathbf{2.44\pm 0.19}\) \\ CIFAR-10 Baseline\(D_{h}^{\prime}=\text{real}\), WRN & \(2.21\pm 0.17\) \\ CIFAR-10 Baseline\(D_{h}^{\prime}=\text{real}\), resnet101 & \(2.01\pm 0.15\) \\ CIFAR-10 Baseline\(D_{h}^{\prime}=\text{input}\) & \(1.12\pm 0.19\) \\ CIFAR-10 Baseline\(\text{helper}\) & \(1.25\pm 0.24\) \\ \hline WikiText Baseline\(D_{\text{ll}}=\text{gen}\) & \(\mathbf{3.31\pm 0.15}\) \\ WikiText Baseline\(D_{h}^{\prime}=\text{real}\) & \(3.26\pm 0.14\) \\ WikiText Baseline\({}_{\text{nn helper}}\) & \(3.11\pm 0.15\) \\ \hline \end{tabular}
\end{table}
Table 5: Baseline evaluation with different helper model scenarios, where WRN is Wide-ResNet-28-2 helper model architecture (in the case of CIFAR10 baseline).

that the maximum value of \(\{c+\epsilon\}_{\text{lb}}\) typically occurs at low recall. Even if we do not use the same metric (precision-recall as opposed to TPR at low FPR) this is coherent with the findings from Carlini et al. (2022b). We can detect more privacy leakage (higher \(\{c+\epsilon\}_{\text{lb}}\) which leads to higher \(\bar{\epsilon}\)) when making a few confident guesses about membership rather than trying to maximize the number of members found (_i.e._, recall). Figure 10, decomposes the precision in terms of the number of true positives and the number of predictions for a direct mapping to the propositions 1 and 2. These are non-negligible values of privacy leakage, even though the true value is likely much higher.

### Panoraima performance with increasing Test and Train Size \(\mathbf{m}\)

PANORAMIA: **Increasing Test Set Size:** We compare PANORAMIA with the O(1) audit (Steinke et al., 2023) (in input space, without canaries), which uses a loss threshold MIA. As shown in Figure 6(b) and Figure 6(c) in the CIFAR10 image dataset case, both for non-private and Differentially Private (DP) models, PANORAMIA can leverage much more data for its measurement (up to the whole training set size for training and testing the MIA), while in our setting O(1) is limited by the size of the test set (for non-members). We vary test size from \(m=500\), up to \(35k\). The experiment is repeated \(10\) times with resampling of the test set each time to produce the average. Unless otherwise stated, all results are produced at a \(95\%\) confidence.

Figure 10: Comparison of the number of true positives and predictions on different datasets

Figure 9: Baseline Strength Evaluation on WikiText dataset as the train set size increases. In Figure 9(a), the test set size is fixed, while in Figure 9(b) it varies as well. See Appendix D.1 for more details.

### Panoramia with Increasing Train Dataset Size

We also conduct experiments to show the effect of increasing training dataset size on the performance of PANORAMIA (as well as the baseline). In the CIFAR10 image dataset case, Figure 11, we see PANORAMIA performance slowly increase as the train dataset size increases. It reaches its max performance at dataset sizes \(4500-5000,10k,20k-30k\) and slowly begins to stabilize at the higher dataset sizes. We vary train size from \(m=500\), up to \(30k\). The experiment is repeated \(5\) times with resampling of the train set each time to produce the average. Unless otherwise stated, all results are produced at a \(95\%\) confidence.

### Panoramia in cases of no privacy leakage

As illustrated in Figure 4(c) and Table 3, there exist instances in which the \(c_{\text{lb}}\) is as large as \(\{c+\epsilon\}_{\text{lb}}\). For instance, for GPT-2_E37, the upper bound in the confidence interval measured for \(c_{\text{lb}}\) is larger than the lower bound of the measurement for \(\{c+\epsilon\}_{\text{lb}}\). Although the measured \(\tilde{\epsilon}\) is positive on average, but the lower bound of the interval is negative. For such cases, we don't claim any detection of privacy leakage. This implies that, under certain conditions, the baseline could outperform the MIA in PANORAMIA. We can see three possible reasons for this behavior. (1) The synthetic data might not be of high enough quality. If there is an evident distinction between real and synthetic samples, the baseline cannot be fooled reducing the power of our audit. (2) The MIA in PANORAMIA is not powerful enough. (3) The target model's privacy leakage is indeed low. For GPT-2_E150 and GPT-2_E75 in Figure 4(c), we can claim that given the target model's leakage level, the synthetic data and the MIA can perform the audit to a non-negligible power. For GPT-2_37, we need to examine the possible reasons more closely. Focusing on generated data, in the ideal case we would like the synthetic data to behave just like real non-member data on Figure 14(a). If this was the case, \(\{c+\epsilon\}_{\text{lb}}\) would go up (since the MIA uses this loss to separate members and non-members). Nonetheless, predicting changes in \(c_{\text{lb}}\) remains challenging, as analogous loss distributions do not guarantee indistinguishability between data points themselves (also, we cannot necessarily predict how the loss distribution changes under the helper model \(h\) in Figure 14(d)). Hence, reason (1) which corresponds to low generative data quality, seems to be a factor in the under-performance of PANORAMIA. Moreover, in the case GPT-2_37, we observe that real members and non-members have very similar loss distributions. Hence, factor (2) also seems to be at play, and a stronger MIA that does not rely exclusively on the plot may increase auditing power. Finally, the helper model \(h\) seems to have a more distinct loss distribution when compared to GPT-2_37 (see Figure 14(d)).

### Panoramia with Tabular data

As we can see in Table 6, our proposed method PANORAMIA cannot detect any meaningful privacy leakage in the case of a small MLP classifier trained on tabular data.

Figure 11: Effect on PANORAMIA and baseline with increasing train set size for CIFAR10, ResNet101 target model (trained for 100 epochs) (with union bound correction).

As shown in Figure 12(b) there is no significant difference between the values\((c+\varepsilon)_{lb}\)\(c_{lb}\) returned by PANORAMIA. As a result we get \(\tilde{\varepsilon}=0\)

The high precision at low recall in the baseline as can be seen in Figure 12(a) illustrates the fact that some real data can easily be distinguished from the synthetic ones. Instead of poor quality synthetic data, we are dealing with real observation that are actually "looking to real". Investigating those observations reveals that they have extreme or rare values in one of their features. Indeed, the Adult dataset contains some extreme values (_e.g._, in the capital_gain column), such extreme values are rarely reproduced by the generative model. This leads to the baseline accurately predicting such observations as real with high confidence that translates into a high \(c_{lb}\). Moreover, the audit values maximum values \((c+\varepsilon)_{lb}\) and \(c_{lb}\) being achieved for a relatively small number of guesses corresponding to extreme values, the audits results vary a lot depending on the sampling or not of such extreme values in the audit test set. We believe this is part of the reason why we observe an high variance in the audit results leading to no significant privacy leakage detected by PANORAMIA.

This is a potential drawback of our auditing scheme, while the presence of extreme or rare values in the dataset usually allows for good auditing results it can have a negative impact on PANORAMIA's success.

### Privacy Auditing of Overfitted ML Models

**Methodology.** Varying the number of training epochs for the target model to induce overfitting is known to be a factor in privacy loss Yeom et al. (2018); Carlini et al. (2022a). As discussed in Section 5.2, since these different variants of target models share the same dataset and task, PANORAMIA can compare them in terms of privacy leaking.

To verify PANORAMIA will indeed attribute a higher value of \(\tilde{\epsilon}\) to more overfitted models, we train our target models for varying numbers of training epochs. The final train and test accuracies are reported in Table 1. For GPT-2 (as a target model), we report the train and validation loss as a measure of overfitting in Figure 8. Figures 13 and 14 show how the gap between member data points (_i.e._, data used to train the target models) and non-member data points (both real as well as

\begin{table}
\begin{tabular}{|l|l|l|} \hline  & PANORAMIA \(\tilde{\varepsilon}\) & O(1) \(\varepsilon\) \\ \hline MLP\_E10 & 0 \(\pm\) 0.142 & 0.655 \(\pm\) 0.014 \\ MLP\_E100 & 0.062 \(\pm\) 0.156 & 0.648 \(\pm\) 0.015 \\ \hline \end{tabular}
\end{table}
Table 6: Privacy measurement with PANORAMIA and O(1) for models trained on tabular Adult dataset.

Figure 12: PANORAMIA on tabular Adult dataset, MLP target model.

generated non-members) increases as the degree of overfitting increases, in terms of loss distributions. We study the distribution of losses since these are the features extracted from the target model \(f\) or helper model \(h\), to pass respectively to PANORAMIA and the baseline classifier. The fact that the loss distributions of member data become more separable from non-member data for more overfitted models is a sign that the corresponding target model could leak more information about its training data. We thus run PANORAMIA on each model, hereafter presenting the results obtained.

**Results.** In Figure 3, we observe that more training epochs (_i.e._, more overfitting) lead to better precision-recall trade-offs and higher maximum precision values. Our results are further confirmed by Figure 10 with PANORAMIA being able to capture the number of member data points (positive predictions) better than the baseline \(b\).

In Table 3, we further demonstrate that our audit output \(\tilde{\epsilon}\) orders the target models in terms of privacy leakage: higher the degree of overfitting, more memorization and hence a higher \(\tilde{\epsilon}\) returned by PANORAMIA. From our experiments, we consistently found that as the number of epochs increased, the value of \(\tilde{\epsilon}\) also increased. For instance, in the WikiText dataset, since we observe \(\{c+\epsilon\}_{\text{lb}}^{\text{GPT-2\_E150}}>\{c+\epsilon\}_{\text{lb}}^{ \text{GPT-2\_E75}}>\{c+\epsilon\}_{\text{lb}}^{\text{GPT-2\_E150}}\), we would have \(\{c+\epsilon\}_{\text{lb}}^{\text{GPT-2\_E150}}-c_{\text{lb}}>\{c+\epsilon\}_ {\text{lb}}^{\text{GPT-2\_E75}}-c_{\text{lb}}>\{c+\epsilon\}_{\text{lb}}^{ \text{GPT-2\_E37}}-c_{\text{lb}}\), which leads to \(\tilde{\epsilon}_{\text{GPT-2\_E150}}>\tilde{\epsilon}_{\text{GPT-2\_E75}}> \tilde{\epsilon}_{\text{GPT-2\_E37}}\). Our experiment is coherent with the intuition that more training epochs lead to more over-fitting, leading to more privacy leakage measured with a higher value of \(\tilde{\epsilon}\).

Figure 15: CIFAR-10, \(\{c+\epsilon\}_{lb}\) when varying privacy leakage with increasing model parameters over one experiment run.

Figure 13: CelebA Multi-Label CNN Loss Comparisons for a generalized vs overfitted model.

### Models of varying complexity:

Carlini et al. (2021) have shown that larger models tend to have bigger privacy losses. To confirm this, we also audit ML models with varying numbers of parameters, from a \(\approx 4M\) parameters Wide ResNet-28-2, to a \(25.5M\) parameters ResNet-50, and a \(44.5M\) parameters ResNet-101. Figure 15 shows that PANORAMIA does detect increasing privacy leakage, with \(\tilde{\epsilon}_{wide-resnet}\leq\tilde{\epsilon}_{resnet50}\leq\tilde{ \epsilon}_{resnet101}\).

### Privacy Auditing of Differentially-Private ML Models

#### Methodology.

For the image data modality, we evaluate the performance of PANORAMIA on differentially-private ResNet-18 and Wide-ResNet-16-4 models on the CIFAR10 dataset under different target privacy budgets (\(\epsilon\)) with \(\delta=10^{-5}\) and the non-private (\(\epsilon=\infty\)) cases. The models are trained using the DP-SGD algorithm (Abadi et al., 2016) using Opacus (Yousefpour et al., 2021), which we tune for the highest train accuracy on learning rate \(lr\), number of epochs \(e\), batch size \(bs\) and maximum \(\ell_{2}\) clipping norm (C) for the largest final accuracy.

For the text data modality, we evaluate the performance of PANORAMIA on the differentially private GPT-2-Small model, which has been fine-tuned on the WikiText dataset (the preprocessing steps are identical to those in Appendix C.2). As with the image modality, we use various target privacy budgets (\(\epsilon\)) with \(\delta=10^{-5}\), as well as non-private settings (\(\epsilon=\infty\)). We train the models using the DP-SGD algorithm Abadi et al. (2016), incorporating Ghost clipping Li et al. (2021) for calculating

Figure 14: Comparison of the loss distributions of real members, real non-members, and synthetic non-members under three target models while varying the degree of over-training on the WikiText dataset. Figure 14 compares the loss distributions under the helper model, the model providing side information to our baseline. We train the helper with some other synthetic samples, which effectively mimic real non-members’ loss distributions under the target models. However, they are distinguishable to some extent from real non-members under the helper model, thus increasing our \(c_{\text{b}}\).

[MISSING_PAGE_FAIL:27]

Discussion.In the image data modality, we observe that the auditing outcome (\(\tilde{\epsilon}\) values for PANORAMIA and \(\epsilon\) for O(1)) can be different for DP models with the same \(\epsilon\) values (Table 10). We hypothesize that the auditing results may relate more to level of overfitting than the target \(\epsilon\) values in trained DP models. The difference between train and test accuracies could be a possible indicator that has a stronger relationship with the auditing outcome. We also observe that O(1) shows a faster increase in \(\epsilon\) for DP models with higher targeted \(\epsilon\) values. We believe it depends on the actual ratio of the correct and total number of predicted samples, since O(1) considers both true positives and true negatives while PANORAMIA considers true positives only. We leave these questions for future work.

In the text data modality, the membership inference attacks (MIAs) relying on loss thresholds in privacy audits, without retraining or access to canaries, may not be sufficiently strong to effectively classify members and non-members.

## Appendix E Generator Closeness \(\gamma\)-relaxation

**Definition 4** (\((c,\gamma)\)-closeness).: _Let the \(\gamma\)-approximate maximum divergence between generative model \(\mathcal{G}\) and data distribution \(\mathcal{D}\) be,_

\[D^{\gamma}_{\infty}(\mathcal{D}||\mathcal{G})\coloneqq\max_{S\subseteq Supp( \mathcal{D}):\mathbb{P}(\mathcal{D}\in S)\geq\gamma}\bigg{[}\ln\frac{\mathbb{ P}(\mathcal{D}\in S)-\gamma}{\mathbb{P}(\mathcal{G}\in S)}\bigg{]}.\]

_For all \(c>0\) we say \(\mathcal{G}\) is \((c,\gamma)\)-close to \(\mathcal{D}\) if \(D^{\gamma}_{\infty}(\mathcal{D}||\mathcal{G})\leq c\), i.e._

\[\forall x\in\mathcal{X},\mathbb{P}_{\mathcal{D}}[x]\leq e^{c}\mathbb{P}_{ \mathcal{G}}[x]+\gamma\text{ or }e^{-c}(\mathbb{P}_{\mathcal{D}}[x]-\gamma)\leq \mathbb{P}_{\mathcal{G}}[x].\]

### Pure DP case

First we prove a similar result as Lemma 5.5(Steinke et al., 2023) but with the one-sided inequality of \((c,\gamma)\)-closeness and the case when we additionally observe output from a pure-DP mechanism.

**Lemma 1**.: _Let \(\mathcal{G},\mathcal{D}\) be probability distributions over \(\mathcal{Y}\), fix \(c,\gamma\geq 0\). Suppose for all measurable \(S\subset\mathcal{Y}\) we have \(e^{-c}(\mathcal{D}(S)-\gamma)\leq\mathcal{G}(S)\). Then \(\exists\gamma^{\prime}\in[0,\gamma]\) and \(\mathcal{D}^{\prime},\mathcal{D}^{\prime\prime},\mathcal{G}^{\prime},\mathcal{ G}^{\prime\prime}\) such that the following properties are satisfied._

1. \(\mathcal{D},\mathcal{G}\) _can be expressed as a convex combination_ \(\mathcal{D}=(1-\gamma^{\prime})\mathcal{D}^{\prime}+\gamma^{\prime}\mathcal{D} ^{\prime\prime},\mathcal{G}=(1-\gamma^{\prime})\mathcal{G}^{\prime}+\gamma^{ \prime}\mathcal{G}^{\prime\prime}\)_;_
2. _for all measurable_ \(S\subset\mathcal{Y}\)_, we have_ \(e^{-c}\mathcal{D}^{\prime}(S)\leq\mathcal{G}^{\prime}(S)\)_;_
3. _there exists measurable_ \(S\subset\mathcal{Y}\) _such that_ \(\mathcal{D}^{\prime\prime}(S)=1\)_,_ \(\forall S^{\prime}\subset S,\mathcal{D}(S^{\prime})\geq\mathcal{G}(S^{\prime})\)_._

\begin{table}
\begin{tabular}{|c|c|c|c|c|c|c|c|} \hline
**Target model** & **Audit** & \(\epsilon_{\text{D}}\) & \(\varepsilon+\epsilon_{\text{D}}\) & \(\tilde{\varepsilon}\) & \(\varepsilon\) & **Train Acc** & **Test Acc** & **Diff Train-Test Acc** \\ \hline \multirow{3}{*}{ResNet18-eps-20} & PANORAMIA RM-GN & 2.508 & 3.63 & 1.06 & - & 71.82 & 67.12 & 4.70 \\  & O (1) RM:RN & - & - & - & 1.22 & - & - & - \\  & PANORAMIA & 2.508 & 2.28 & 0 & - & 71.78 & 68.08 & 3.70 \\  & R-GN & - & - & - & 0.09 & - & - & - \\  & O (1) RM:RN & 2.508 & 3.63 & 1.13 & - & 69.01 & 65.7 & 3.31 \\ \hline \multirow{3}{*}{ResNet18-eps-15} & PANORAMIA RM-GN & 2.508 & 3.63 & 1.13 & - & 69.01 & 65.7 & 3.31 \\  & O (1) RM:RN & - & - & - & 1.34 & - & - \\ \cline{1-1}  & PANORAMIA RM-GN & 2.508 & 1.61 & 0 & 66.68 & 69.30 & 2.62 \\ \cline{1-1}  & O (1) RM:RN & - & - & 0.08 & - & - & - \\ \hline \end{tabular}
\end{table}
Table 10: DP models with the same \(\epsilon\) values can have different auditing outcomes (the above numbers are reported here without union-bound correction).

\begin{table}
\begin{tabular}{|l|c|c|c|c|} \hline
**Target model /** & Audit & **Generator \(\mathcal{G}\)** & \(\tilde{\epsilon}\) \\ \hline \(\infty\)-DP GPT-Small (GPT2-Small) & PANORAMIA & GPT2-Small & \(0.22\pm 0.37\) & - \\  & O (1) RM:RN & - & \(2.82\pm 0.31\) \\ \hline
1-DP GPT-Small & PANORAMIA & GPT2-Small & 0 & - \\  & O (1) RM:RN & - & 0 \\ \hline
3-DP GPT2-Small & PANORAMIA & GPT2-Small & 0 & - \\  & O (1) RM:RN & - & 0 \\ \hline \end{tabular}
\end{table}
Table 9: Privacy audits on GPT-2-Small target models with varying privacy guarantees.

_Let \(M\) denote an arbitrary \(\epsilon\)-DP mechanism which takes input \(Y\) sampled from distribution \(\mathcal{G}\) or \(\mathcal{D}\) and outputs \(f\). Let \(\mathcal{P},\mathcal{Q}\) be probability distributions over \(\mathcal{Z}\) which are distributions of \(M(Y)\) when \(Y\) is sampled from \(\mathcal{D}\) and \(\mathcal{G}\) respectively. Fix \(\epsilon\geq 0\), suppose for all measurable \(T\subset\mathcal{Z}\) we have \(\mathcal{P}(T)\leq e^{\epsilon}\mathcal{Q}(T)\) and \(\mathcal{Q}(T)\leq e^{\epsilon}\mathcal{P}(T)\). When additionally observing the output from a DP mechanism we have the following properties. Let \((\mathcal{D},\mathcal{P}),(\mathcal{G},\mathcal{Q})\) be the joint distribution over \((\mathcal{Y},\mathcal{Z})\) which we assume \(\mathcal{D}\perp\mathcal{P}\), \(\mathcal{G}\perp\mathcal{Q}\)._

1. \((\mathcal{D},\mathcal{P}),(\mathcal{G},\mathcal{Q})\) _can be expressed as a convex combination_ \(\mathcal{DP}=((1-\gamma^{\prime})\mathcal{D}^{\prime}+\gamma^{\prime}\mathcal{ D}^{\prime\prime})\mathcal{P},\mathcal{G}\mathcal{Q}=((1-\gamma^{\prime}) \mathcal{G}^{\prime}+\gamma^{\prime}\mathcal{G}^{\prime\prime})\mathcal{Q}\)_;_
2. _for all measurable_ \(S\subset\mathcal{Y}\)_,_ \(T\subset\mathcal{Z}\)_, we have_ \(e^{-c}e^{-\epsilon}\mathcal{D}^{\prime}(S)\mathcal{P}(T)\leq\mathcal{G}^{ \prime}(S)\mathcal{Q}(T)\)_;_
3. _there exists measurable_ \(S\subset\mathcal{Y}\)_,_ \(T\subset\mathcal{Z}\) _such that_ \(\mathcal{D}^{\prime\prime}(S)\mathcal{P}(T)=1\)_,_ \(\forall S^{\prime}\subset S,T^{\prime}\subset T,\mathcal{D}(S^{\prime}) \mathcal{P}(T)\geq\mathcal{G}(S^{\prime})\mathcal{P}(T)\)_._

Proof.: For the edge cases of \(\gamma=0\), the results hold with \(\gamma^{\prime}=0,\mathcal{D}^{\prime}=\mathcal{D},\mathcal{G}^{\prime}= \mathcal{G}\); similarly when \(\gamma=1\), the results hold with \(\gamma^{\prime}=1,\mathcal{D}^{\prime\prime}=\mathcal{D},\mathcal{G}^{\prime \prime}=\mathcal{G},\mathcal{D}^{\prime}=\mathcal{G}^{\prime}\). Let \(c^{\prime}\in[0,c]\), \(\gamma_{1},\gamma_{2}\in(0,1)\), define distribution \(\mathcal{G}^{\prime},\mathcal{G}^{\prime\prime},\mathcal{D}^{\prime},\mathcal{ D}^{\prime\prime}\) as follows. For all points \(y\in\mathcal{Y}\),

\[\mathcal{D}^{\prime}(y) =\frac{\min\{\mathcal{D}(y),e^{c^{\prime}}\mathcal{G}(y)\}}{1- \gamma_{1}},\] \[\mathcal{D}^{\prime\prime}(y) =\frac{\mathcal{D}(y)-(1-\gamma_{1})\mathcal{D}^{\prime}(y)}{ \gamma_{1}}=\frac{\max\{0,\mathcal{D}(y)-e^{c^{\prime}}\mathcal{G}(y)\}}{ \gamma_{1}},\] \[\mathcal{G}^{\prime}(y) =\frac{\mathcal{G}(y)}{1-\gamma_{2}},\] \[\mathcal{G}^{\prime\prime}(y) =\frac{\mathcal{G}(y)-(1-\gamma_{2})\mathcal{G}^{\prime}(y)}{ \gamma_{2}}=0.\]

By construction \((1-\gamma_{1})\mathcal{D}^{\prime}+\gamma_{1}\mathcal{D}^{\prime\prime}= \mathcal{D}\), \((1-\gamma_{2})\mathcal{G}^{\prime}+\gamma_{2}\mathcal{G}^{\prime\prime}= \mathcal{G}\), so the first property is satisfied, and \(\mathcal{D}^{\prime\prime}(y)\) is supported on \(S=\{y\in\mathcal{Y}:\mathcal{D}(y)>e^{c^{\prime}}\mathcal{G}(y)\}\) so the third property is implied. If \(0<\gamma_{1}=\gamma_{2}=\gamma^{\prime}\leq\gamma\), for all \(y\in\mathcal{Y}\) we have,

\[\frac{\mathcal{G}^{\prime}(y)}{\mathcal{D}^{\prime}(y)}=\frac{\mathcal{G}(y)} {\min\{\mathcal{D}(y),e^{c^{\prime}}\mathcal{G}(y)\}}\geq e^{-c^{\prime}}\geq e ^{-c},\]

as required for the second property. Following the same as in Lemma 5.5(Steinke et al., 2023), we can ensure \(0<\gamma_{1}=\gamma_{2}\leq\gamma\) by appropriately setting \(\epsilon_{1},\epsilon_{2}\in[0,\epsilon]\). We can use the same decomposition of \(\mathcal{D},\mathcal{G}\) to prove the second part of the Lemma. If assuming \(\mathcal{D}\perp\mathcal{P}\) and \(\mathcal{G}\perp\mathcal{Q}\), the joint distributions can be decomposed as \(f_{\mathcal{D},\mathcal{P}}=f_{\mathcal{P}}\cdot f_{\mathcal{D}}\) and \(f_{\mathcal{G},\mathcal{Q}}=f_{\mathcal{Q}}\cdot f_{\mathcal{Q}}\). Therefore by construction, \(\mathcal{DP}=((1-\gamma^{\prime})\mathcal{D}^{\prime}+\gamma^{\prime} \mathcal{D}^{\prime\prime})\mathcal{P},\mathcal{G}\mathcal{Q}=((1-\gamma^{ \prime})\mathcal{G}^{\prime}+\gamma^{\prime}\mathcal{G}^{\prime\prime}) \mathcal{Q}\). \(\mathcal{D}^{\prime\prime}\mathcal{P}\) is supported on \((S,T)=\{y\in\mathcal{Y},M(y)\in\mathcal{Z}:\mathcal{D}(y)\mathcal{P}(M(y))>e^{c^ {\prime}}\mathcal{G}(y)\mathcal{P}(M(y))\}\). For all \(y\in\mathcal{Y}\), \(M(y)\in\mathcal{Z}\) we have,

\[\frac{\mathcal{G}^{\prime}(y)}{\mathcal{D}^{\prime}(y)}\cdot\frac{\mathcal{Q} (M(y))}{\mathcal{P}(M(y))}=\frac{\mathcal{G}(y)}{\min\{\mathcal{D}(y),e^{c^{ \prime}}\mathcal{G}(y)\}}\cdot\frac{\mathcal{Q}(M(y))}{\mathcal{P}(M(y))}\geq e ^{-c^{\prime}}e^{-\epsilon}\geq e^{-c}e^{-\epsilon},\]

where \(\frac{\mathcal{Q}(M(y))}{\mathcal{P}(M(y))}\geq e^{-\epsilon}\) since \(M\) is an \(\epsilon\)-DP mechanism. 

Similar to Lemma 5.6(Steinke et al., 2023), next we prove a Bayesian version of Lemma 1 which is used to prove the main result afterwards.

**Lemma 2**.: _Let \(\mathcal{G},\mathcal{D}\) be probability distributions over \(\mathcal{Y}\), fix \(c,\gamma\geq 0\), suppose for all measurable \(S\subset\mathcal{Y}\) we have \(e^{-c}(\mathcal{D}(S)-\gamma)\leq\mathcal{G}(S)\). Then there exists a randomized function \(E_{\mathcal{D},\mathcal{G}}:\mathcal{Y}\rightarrow\{0,1\}\) with the following properties. Suppose \(X\sim\mathrm{Bernoulli}(\frac{1}{2})\), if \(X=1\) sample \(Y\sim\mathcal{D}\), and if \(X=0\) sample \(Y\sim\mathcal{G}\). Then for all \(y\in\mathcal{Y}\) we have,_

\[\operatorname*{\mathbb{P}}_{\begin{subarray}{c}X\sim\mathcal{B} \mathrm{Bernoulli}(\frac{1}{2})\\ Y\vdash X\mathcal{D}+(1-\bar{X})\mathcal{G}\end{subarray}}\left[X=1,E_{ \mathcal{D},\mathcal{G}}(Y)=1|Y=y\right] \leq\frac{e^{c}}{1+e^{c}},\] \[\operatorname*{\mathbb{E}}_{Y\sim\mathcal{D}}[E_{\mathcal{D}, \mathcal{G}}(Y)] \geq 1-\gamma,\ \operatorname*{\mathbb{E}}_{Y\sim\mathcal{G}}[E_{\mathcal{D}, \mathcal{G}}(Y)] \geq 1-\gamma.\]_Let \(M\) denote an arbitrary \(\epsilon\)-DP mechanism which takes input \(Y\) sampled from distribution \(\mathcal{G}\) or \(\mathcal{D}\) and outputs \(f\). Let \(\mathcal{P},\mathcal{Q}\) be probability distributions over \(\mathcal{Z}\) which are distributions of \(M(Y)\) when \(Y\) is sampled from \(\mathcal{D}\) and \(\mathcal{G}\) respectively. Fix \(\epsilon\geq 0\), suppose for all measurable \(T\subset\mathcal{Z}\) we have \(\mathcal{P}(T)\leq e^{\epsilon}\mathcal{Q}(T)\) and \(\mathcal{Q}(T)\leq e^{\epsilon}\mathcal{P}(T)\). Let \((\mathcal{D},\mathcal{P}),(\mathcal{G},\mathcal{Q})\) denote the joint distributions over \((\mathcal{Y},\mathcal{Z})\) which we assume \(\mathcal{D}\perp\mathcal{P}\), \(\mathcal{G}\perp\mathcal{Q}\). Then there exists a randomized function \(E_{(\mathcal{D},\mathcal{P}),(\mathcal{G},\mathcal{Q})}:(\mathcal{Y},\mathcal{ Z})\to\{0,1\}\) such that for all \(y\in\mathcal{Y}\), \(M(y)\in\mathcal{Z}\) we have,_

\[\begin{split}\mathbb{P}_{\begin{subarray}{c}X\sim\text{ Bernoulli}(\frac{1}{2})\\ Y\gets X\mathcal{D}+(1-X)\mathcal{G}\\ M(Y)\gets M(X\mathcal{D}+(1-X)\mathcal{G})\end{subarray}}&[X=1,E_{(\mathcal{D},\mathcal{P}),(\mathcal{G},\mathcal{Q})}(Y,M(Y))=1|Y=y,M(Y)=f ]\leq\frac{e^{c+\epsilon}}{1+e^{c+\epsilon}},\\ \mathbb{E}_{Y\sim\mathcal{D},M(Y)\sim\mathcal{P}}&[E_{( \mathcal{D},\mathcal{P}),(\mathcal{G},\mathcal{Q})}(Y,M(Y))]\geq 1-\gamma,\\ \mathbb{E}_{Y\sim\mathcal{G},M(Y)\sim\mathcal{Q}}&[E_{( \mathcal{D},\mathcal{P}),(\mathcal{G},\mathcal{Q})}(Y,M(Y))]\geq 1-\gamma.\end{split}\]

Proof.: We apply the decomposition from Lemma 1 and denote \(\mathcal{D}^{\prime},\mathcal{D}^{\prime\prime},\mathcal{G}^{\prime},\mathcal{ G}^{\prime\prime},\gamma^{\prime}\) as is. We define \(E_{\mathcal{D},\mathcal{G}}:\mathcal{Y}\to\{0,1\}\) by,

\[\mathbb{P}[E_{\mathcal{D},\mathcal{G}}(y)=1]=(1-\gamma^{\prime})\frac{ \mathcal{D}^{\prime}(y)}{\mathcal{D}(y)}=1-\frac{\gamma^{\prime}\mathcal{D}^{ \prime\prime}(y)}{\mathcal{D}(y)}.\]

For any \(y\in\mathcal{Y}\) we have,

\[\begin{split}&\mathbb{P}[X=1,E_{\mathcal{D},\mathcal{G}}(Y)=1|Y=y] \\ &=\mathbb{P}[X=1|Y=y,E_{\mathcal{D},\mathcal{G}}(Y)=1]\mathbb{P}[ E_{\mathcal{D},\mathcal{G}}(Y)=1|Y=y]\\ &=\mathbb{P}[X=1|Y=y]\mathbb{P}[E_{\mathcal{D},\mathcal{G}}(y)=1] \\ &=\frac{\mathbb{P}[Y=y|X=1]\mathbb{P}[X=1]}{\mathbb{P}[Y=y]} \mathbb{P}[E_{\mathcal{D},\mathcal{G}}(y)=1]\\ &=\frac{\mathcal{D}(y)}{\mathcal{D}(y)+\mathcal{G}(y)}\mathbb{P}[ E_{\mathcal{D},\mathcal{G}}(y)=1]\\ &=\frac{(1-\gamma^{\prime})\mathcal{D}^{\prime}(y)+\gamma^{ \prime}\mathcal{D}^{\prime\prime}(y)}{(1-\gamma^{\prime})\mathcal{D}^{\prime} (y)+\gamma^{\prime}\mathcal{D}^{\prime\prime}(y)+(1-\gamma^{\prime})\mathcal{G }^{\prime}(y)}\mathbb{P}[E_{\mathcal{G},\mathcal{D}}(y)=1]\\ &=\frac{1+\frac{\gamma^{\prime}\mathcal{D}^{\prime\prime}(y)}{(1 -\gamma^{\prime})\mathcal{D}^{\prime}(y)}}{1+\frac{\gamma^{\prime}\mathcal{D} ^{\prime\prime}(y)}{(1-\gamma^{\prime})\mathcal{D}^{\prime}(y)}+\frac{\mathcal{ G}^{\prime}(y)}{\mathcal{D}^{\prime}(y)}}\mathbb{P}[E_{\mathcal{G},\mathcal{D}}(y)=1]\\ &\leq\frac{1+\frac{\gamma^{\prime}\mathcal{D}^{\prime\prime}(y)} {1+0+e^{-c}}\mathbb{P}[E_{\mathcal{G},\mathcal{D}}(y)=1]\\ &=\frac{1}{1+e^{-c}}\cdot\left(\frac{(1-\gamma^{\prime})\mathcal{ D}^{\prime}(y)+\gamma^{\prime}\mathcal{D}^{\prime\prime}(y)}{(1-\gamma^{\prime}) \mathcal{D}^{\prime}(y)}\right)\cdot\mathbb{P}[E_{\mathcal{G},\mathcal{D}}(y )=1]\\ &=\frac{1}{1+e^{-c}}\cdot\left(\frac{\mathcal{D}(y)}{(1-\gamma^{ \prime})\mathcal{D}^{\prime}(y)}\right)\cdot\mathbb{P}[E_{\mathcal{G}, \mathcal{D}}(y)=1]\\ &=\frac{1}{1+e^{-c}}=\frac{e^{c}}{1+e^{c}}.\end{split}\]

The first equality uses that \(\mathbb{P}[A,B|C]=\mathbb{P}[A|B,C]\mathbb{P}[B|C]\). The second equality holds since we assume the internal randomness of \(E\) is independent of everything else. The third equality uses the Bayes' rule. The fourth equality uses the definition of the randomized function \(E\). The fifth equality uses the decomposition of Lemma 1. After rearranging and use the properties from Lemma 1 we obtain the first inequality, then the rest follows from simplifications. Furthermore we have the following,

\[\mathbb{E}_{Y\sim\mathcal{D}}[E_{\mathcal{D},\mathcal{G}}] =\int_{\mathcal{Y}}\mathcal{D}(y)\mathbb{P}[E_{\mathcal{D},\mathcal{G }}=1]dy\] \[=\int_{\mathcal{Y}}(1-\gamma^{\prime})\mathcal{D}^{\prime}(y)dy=1- \gamma^{\prime}\geq 1-\gamma,\] \[\mathbb{E}_{Y\sim\mathcal{G}}[E_{\mathcal{D},\mathcal{G}}] =1-\gamma^{\prime}\mathbb{E}_{Y\sim\mathcal{G}}\bigg{[}\frac{ \mathcal{D}^{\prime\prime}(y)}{\mathcal{D}(y)}\bigg{]}\] \[=1-\gamma^{\prime}\int_{\mathcal{Y}}\frac{\mathcal{G}(y)}{ \mathcal{D}(y)}\cdot\mathcal{D}^{\prime\prime}(y)dy\] \[\geq 1-\gamma^{\prime}\int_{\mathcal{Y}}\mathcal{D}^{\prime\prime }(y)dy\] \[=1-\gamma^{\prime}\geq 1-\gamma.\]

We follow a similar procedure to prove the second part of the Lemma. We define \(E_{(\mathcal{D},\mathcal{P}),(\mathcal{G},\mathcal{Q})}:(\mathcal{Y}, \mathcal{Z})\to\{0,1\}\) by,

\[\mathbb{P}[E_{(\mathcal{D},\mathcal{P}),(\mathcal{G},\mathcal{Q})}(y,M(y))=1] =\mathbb{P}[E_{\mathcal{D},\mathcal{G}}(y)=1]\]

With similar reasoning, for any \(y\in\mathcal{Y}\), \(M(y)\in\mathcal{Z}\) we have,

\[\mathbb{P}[X=1,E_{(\mathcal{D},\mathcal{P}),(\mathcal{G},\mathcal{ Q})}(Y,M(Y))=1|Y=y,M(Y)=f]\] \[=\frac{\mathbb{P}[Y=y,M(Y)=f|X=1]\mathbb{P}[X=1]}{\mathbb{P}[Y=y,M (Y)=f]}\mathbb{P}[E_{(\mathcal{D},\mathcal{P}),(\mathcal{G},\mathcal{Q})}(y,M (y))=1]\] \[=\frac{\mathcal{D}(y)\mathcal{P}(M(y))}{\mathcal{D}(y)\mathcal{P} (M(y))+\mathcal{G}(y)\mathcal{Q}(M(y))}\mathbb{P}[E_{(\mathcal{D},\mathcal{P}),(\mathcal{G},\mathcal{Q})}(y,M(y))=1]\] \[=\frac{((1-\gamma^{\prime})\mathcal{D}^{\prime}(y)+\gamma^{ \prime}\mathcal{D}^{\prime\prime}(y))\mathcal{P}(M(y))}{((1-\gamma^{\prime}) \mathcal{D}^{\prime}(y)+\gamma^{\prime}\mathcal{D}^{\prime\prime}(y))\mathcal{ P}(M(y))+(1-\gamma^{\prime})\mathcal{G}^{\prime}(y)\mathcal{Q}(M(y))}\mathbb{P}[E_{( \mathcal{D},\mathcal{P}),(\mathcal{G},\mathcal{Q})}(y,M(y))=1]\] \[=\frac{1+\frac{\gamma^{\prime}\mathcal{D}^{\prime\prime}(y)}{1+ \gamma^{\prime}\mathcal{D}^{\prime\prime}(y)}}{1+0+e^{-c}e^{-\epsilon}} \mathbb{P}[E_{(\mathcal{D},\mathcal{P}),(\mathcal{G},\mathcal{Q})}(y,M(y))=1]\] \[=\frac{1}{1+e^{-c}e^{-\epsilon}}\cdot\bigg{(}\frac{\mathcal{D}(y )}{(1-\gamma^{\prime})\mathcal{D}^{\prime}(y)}\bigg{)}\cdot\mathbb{P}[E_{( \mathcal{D},\mathcal{P}),(\mathcal{G},\mathcal{Q})}(y,M(y))=1]\] \[=\frac{1}{1+e^{-c}e^{-\epsilon}}=\frac{c^{c+\epsilon}}{1+e^{c+ \epsilon}}.\]

Since we define the success probability with the same expression, the expected success of the randomized function directly follows,

\[\mathbb{E}_{Y\sim\mathcal{D},M(Y)\sim\mathcal{P}}[E_{(\mathcal{D},\mathcal{P}),(\mathcal{G},\mathcal{Q})}(Y,M(Y))] =\mathbb{E}_{Y\sim\mathcal{D}}[E_{\mathcal{D},\mathcal{G}}(Y)] \geq 1-\gamma,\] \[\mathbb{E}_{Y\sim\mathcal{G},M(Y)\sim\mathcal{Q}}[E_{(\mathcal{D},\mathcal{P}),(\mathcal{G},\mathcal{Q})}(Y,M(Y))] =\mathbb{E}_{Y\sim\mathcal{G}}[E_{\mathcal{D},\mathcal{G}}(Y)] \geq 1-\gamma.\]

Next we use Lemma 2 to prove the following Proposition which is used to form statistical tests on hypothesis \(\mathcal{H}\).

**Proposition 3**.: _Let \(\mathcal{G}\) be \((c,\gamma)\)-close, \(f\) be output of an \(\epsilon\)-DP mechanism, \(T^{b}\triangleq B(S,X)\) be the guess from the baseline, \(T^{a}\triangleq A(S,X,f)\) be the guess from the membership audit. Let \(T_{b},T_{a}\in[0,1]^{m}\) be bounded. Then, for all \(v\in\mathbb{R}\) and all \(t\) in the support of \(T\):_

\[\mathbb{P}_{S,X,T^{a}}\bigg{[}\sum_{i=1}^{m}T_{i}^{b}\cdot S_{i} \geq v\mid T^{b}=t^{b}\bigg{]} \leq\underset{S^{\prime}\sim\text{Bernoulli}(\frac{c^{c}}{1+c^{c} })^{m},F}{\mathbb{P}}\Big{[}F(t^{b})+\sum_{i=1}^{m}t_{i}^{b}\cdot S_{i}^{\prime} \geq v\Big{]},\] \[\mathbb{P}_{S,X,T^{a}}\bigg{[}\sum_{i=1}^{m}T_{i}^{a}\cdot S_{i} \geq v\mid T^{a}=t^{a}\bigg{]} \leq\underset{S^{\prime}\sim\text{Bernoulli}(\frac{c^{c+\epsilon} }{1+c^{c}\epsilon})^{m},F}{\mathbb{P}}\Big{[}F(t^{a})+\sum_{i=1}^{m}t_{i}^{a} \cdot S_{i}^{\prime}\geq v\Big{]},\]_where \(F\) is independent from \(S^{\prime}\), \(F(T^{b}),F(T^{a})\) is supported on \(\{0,1,\ldots,m\}\) and \(\mathbb{E}_{T^{b},F}[F(T^{b})]=\mathbb{E}_{T^{a},F}[F(T^{a})]\leq 2m\gamma\)._

Proof.: Let \(B(s_{\leq i})\) denote the distribution on \([0,1]^{m}\) obtained by conditioning \(B\) on past \(S_{<i}=s_{<i}\) and \(X_{\leq i}=x_{\leq i}\). By Lemma 2, for all \(t^{b}\in[0,1]^{m}\) we have

\[\mathbb{P}[S_{i}=1,E_{B(s_{<i,1}),B(s_{<i,0})}(T^{b})=1|T^{b}=t^{b},S_{<i}=s_{<i},X_{\leq i}=x_{\leq i}]\leq\frac{e^{c}}{1+e^{c}},\] \[\mathbb{E}[E_{B(s_{\leq i,1}),B(s_{\leq i,0})}(T^{b})=1|S_{<i}=s_ {<i},S_{i}=0]\geq 1-\gamma,\] \[\mathbb{E}[E_{B(s_{\leq i,1}),B(s_{\leq i,0})}(T^{b})=1|S_{<i}=s_ {<i},S_{i}=1]\geq 1-\gamma.\]

Using the law of total probability we get,

\[\mathbb{P}[S_{i}=1,E_{B(s_{<i,1}),B(s_{<i,0})}(T^{b})=1|T^{b}=t^{ b},S_{<i}=s_{<i}]\] \[=\sum_{x\leq i}\mathbb{P}[S_{i}=1,E_{B(s_{<i,1}),B(s_{<i,0})}(T^{ b})=1|T^{b}=t^{b},S_{<i}=s_{<i},X_{\leq i}=x_{\leq i}]\] \[\quad\mathbb{P}[X_{\leq i}=x_{\leq i}|T^{b}=t^{b},S_{<i}=s_{<i}]\] \[\leq\frac{e^{c}}{1+e^{c}}\sum_{x\leq i}\mathbb{P}[X_{\leq i}=x_{ \leq i}|T^{b}=t^{b},S_{<i}=s_{<i}]=\frac{e^{c}}{1+e^{c}}.\]

Applying the union bound on \(S_{i}\) we can bound the expectation of success events by,

\[\mathbb{E}[E_{B(s_{\leq i,1}),B(s_{\leq i,0})}(T^{b})|S_{<i}=s_{<i},S_{i}=s_{ i}]\geq 1-2\gamma.\]

By induction, for \(k\in[m]\) define \(\tilde{W}_{k}(s,t^{b})\coloneqq\sum_{i=1}^{k}t_{i}^{b}\cdot s_{i}\cdot E_{B(s_ {<i,1}),B(s_{<i,0})}(t^{b})\), then \(\tilde{W}_{k}(S,t^{b})\) is stochastically dominated by \(\tilde{W}_{k}(t^{b})\coloneqq\sum_{i=1}^{k}t_{i}^{b}\cdot\tilde{S}_{i}(t)\) where \(\tilde{S}_{i}(t)\sim\mathrm{Bernoulli}(\frac{e^{c}}{1+e^{c}})\). Let the failing events \(F\) be,

\[F(s,t^{b})\coloneqq\sum_{i=1}^{m}\mathds{1}\{E_{B(s_{\leq i,1}),B(s_{\leq i,0 })}(t^{b})=0\},\]

then we have \(W_{m}(s,t^{b})\coloneqq\sum_{i=1}^{m}t_{i}^{b}\cdot s_{i}=\tilde{W}_{m}(s,t^{b })+F(s,t^{b})\) is stochastically dominated by \(\tilde{W}_{m}(T^{b})+F(S,T^{b})\), where the expectation of the failing events is bounded by,

\[\mathbb{E}[F(S,T^{b})]=\sum_{i=1}^{m}\mathbb{P}[E_{B(s_{\leq i,1}),B(s_{\leq i,0})}(t^{b})=0]\leq m(2\gamma).\]

Similar to Lemma 5.5 in Steinke et al. (2023) since \(\mathbb{E}[F(S,T^{b})]\) is independent of \(S\) we could have \(F(T^{b})=F(S,T^{b})\) for \(S\) drawn from an appropriate distribution. The proof of the second part of the Proposition is essentially the same except with the following result from Lemma 2,

\[\mathbb{P}[S_{i}=1,E_{A(s_{<i,1}),A(s_{<i,0})}(T^{a})=1|T^{a}=t^{ a},S_{<i}=s_{<i},X_{\leq i}=x_{\leq i}]\leq\frac{e^{c+\epsilon}}{1+e^{c+ \epsilon}},\] \[\mathbb{E}[E_{A(s_{\leq i,1}),A(s_{\leq i,0})}(T^{a})=1|S_{<i}=s_ {<i},S_{i}=0]\geq 1-\gamma,\] \[\mathbb{E}[E_{A(s_{\leq i,1}),A(s_{\leq i,0})}(T^{a})=1|S_{<i}=s_ {<i},S_{i}=1]\geq 1-\gamma.\]

Similar to Theorem 5.2 (Steinke et al., 2023) we could compute for the optimal distribution \(F(t^{b})\) and \(F(t^{a})\) by formulating it into a linear programming problem. Similar to Corollary 1, to test Hypothesis \(\mathcal{H}\) together we would apply Union bound on the two parts of the hypothesis as in Proposition 3.

### Approximate DP case

In this section we consider the case where we have a \((c,\gamma)\)-close generator and audit for an approximate DP mechanism. We modify the results to account for the failing events from these two sources of randomness. We start by proving a similar version of Lemma 2 where we use both the decomposition of \(\mathcal{G},\mathcal{D}\) from Lemma 1 and the decomposition of \(\mathcal{P},\mathcal{Q}\) from Lemma 5.5(Steinke et al., 2023).

[MISSING_PAGE_EMPTY:33]

Note that we can rewrite the success probability as,

\[\mathbb{P}[E(y,f)=1]=1-\frac{\gamma^{\prime}(1-\delta^{\prime})\mathcal{D}^{ \prime\prime}\mathcal{P}^{\prime}}{\mathcal{D}\mathcal{P}}-\frac{(1-\gamma^{ \prime})\delta^{\prime}\mathcal{D}^{\prime}\mathcal{P}^{\prime\prime}}{ \mathcal{D}\mathcal{P}}-\frac{\gamma^{\prime}\delta^{\prime}\mathcal{D}^{ \prime\prime}\mathcal{P}^{\prime\prime}}{\mathcal{D}\mathcal{P}}.\]

Then we have,

\[\mathbb{E}_{Y\sim\mathcal{G},M(Y)\sim\mathcal{Q}}[E(Y,F)]\] \[=1-\gamma^{\prime}(1-\delta^{\prime})\mathbb{E}\bigg{[}\frac{ \mathcal{D}^{\prime\prime}(y)\mathcal{P}^{\prime}(f)}{\mathcal{D}(y)\mathcal{ P}(f)}\bigg{]}-(1-\gamma^{\prime})\delta^{\prime}\mathbb{E}\bigg{[}\frac{ \mathcal{D}^{\prime}(y)\mathcal{P}^{\prime\prime}(f)}{\mathcal{D}(y)\mathcal{ P}(f)}\bigg{]}-\gamma^{\prime}\delta^{\prime}\mathbb{E}\bigg{[}\frac{ \mathcal{D}^{\prime\prime}(y)\mathcal{P}^{\prime\prime}(f)}{\mathcal{D}(y) \mathcal{P}(f)}\bigg{]}\] \[=1-\gamma^{\prime}(1-\delta^{\prime})\int\mathcal{G}(y)\mathcal{ Q}(f)\bigg{[}\frac{\mathcal{D}^{\prime\prime}(y)\mathcal{P}^{\prime}(f)}{ \mathcal{D}(y)\mathcal{P}(f)}\bigg{]}dydf\] \[\geq 1-\gamma^{\prime}(1-\delta^{\prime})\int\mathcal{D}^{ \prime\prime}(y)\mathcal{P}^{\prime}(f)dydf-(1-\gamma^{\prime})\delta^{\prime }\int\mathcal{D}^{\prime}(y)\mathcal{P}^{\prime\prime}(f)dydf\] \[\quad-\gamma^{\prime}\delta^{\prime}\int\mathcal{D}^{\prime \prime}(y)\mathcal{P}^{\prime\prime}(f)dydf\] \[=1-\gamma^{\prime}(1-\delta^{\prime})-(1-\gamma^{\prime}) \delta^{\prime}-\gamma^{\prime}\delta^{\prime}\] \[=1-\gamma^{\prime}-\delta^{\prime}+\gamma^{\prime}\delta^{\prime}\] \[=(1-\gamma^{\prime})(1-\delta^{\prime})\geq(1-\gamma)(1-\delta).\]

Next we use Lemma 3 to prove the following Proposition to form the statistical tests on hypothesis \(\mathcal{H}\) for the \((\epsilon,\delta)\)-DP version.

**Proposition 4**.: _Let \(\mathcal{G}\) be \((c,\gamma)\)-close, \(f\) be output of an \((\epsilon,\delta)\)-DP mechanism, \(T^{b}\triangleq B(S,X)\) be the guess from the baseline, \(T^{a}\triangleq A(S,X,f)\) be the guess from the membership audit. Let \(T_{b},T_{a}\in[0,1]^{m}\) be bounded. Then, for all \(v\in\mathbb{R}\) and all \(t\) in the support of \(T\):_

\[\mathbb{P}_{S,X,T^{b}}\Big{[}\sum_{i=1}^{m}T_{i}^{b}\cdot S_{i} \geq v\mid T^{b}=t^{b}\Big{]}\leq\underset{S^{\prime}\sim\text{Bernoulli}( \frac{c^{\epsilon}}{1+c^{\epsilon}})^{m},F}{\mathbb{P}}\Big{[}F(t^{b})+\sum_{i =1}^{m}t_{i}^{b}\cdot S_{i}^{\prime}\geq v\Big{]},\] \[\mathbb{P}_{S,X,T^{a}}\Big{[}\sum_{i=1}^{m}T_{i}^{a}\cdot S_{i} \geq v\mid T^{a}=t^{a}\Big{]}\leq\underset{S^{\prime}\sim\text{Bernoulli}( \frac{c^{\epsilon}+c^{\epsilon}}{1+c^{\epsilon}+c^{\epsilon}})^{m},F}{\mathbb{P }}\Big{[}F(t^{a})+\sum_{i=1}^{m}t_{i}^{a}\cdot S_{i}^{\prime}\geq v\Big{]},\]

_where \(F\) is independent from \(S^{\prime}\), \(F(T^{b})\), \(F(T^{a})\) is supported on \(\{0,1,\ldots,m\}\) and \(\mathbb{E}_{T^{b},F}[F(T^{b})]\leq 2m\delta\), \(\mathbb{E}_{T^{a},F}[F(T^{a})]\leq 2m(\gamma+\delta-\gamma\delta)\)._

Proof.: The first part of the result (baseline test) exactly follows the proof of Proposition 3. For the approximate DP case, the proof of the auditor test follows similar steps except for the bound on the expectation of the failing events,

\[\mathbb{E}[F(S,T^{b})]=\sum_{i=1}^{m}\mathbb{P}[E_{B(s_{\leq i,1} ),B(s_{\leq i,0})}(t^{b})=0] \leq m(2(1-(1-\gamma)(1-\delta)))\] \[=2m(\gamma+\delta-\gamma\delta).\]

Similar to the pure DP case we could optimize for the optimal distribution of \(F(t^{b})\) and \(F(t^{a})\) by solving a linear program (Theorem 5.2 in Steinke et al. (2023)) and apply Union bound as in Corollary 1 to test for Hypothesis \(\mathcal{H}\).

### Evaluations

We empirically evaluate the auditing performance of PANORAMIA under different relaxed level of the generator for both the pure and approximate DP cases. We observe that the overall performance is similar to that of no relaxation. We observe smaller \(\tilde{\varepsilon}\) (which is analogous to a looser lower bound) as we increase the relaxation level, and in the more extreme cases where we allow many failing events (e.g. when relaxation \(>1\)e-3) we would fail to detect meaningful privacy leakage as \(\tilde{\varepsilon}=0\).

## Appendix F Extended Related Work

Most related privacy auditing works measure the privacy of an ML model by lower-bounding its privacy loss. This usually requires altering the training pipeline of the ML model, either by injecting canaries that act as outliers Carlini et al. (2019) or by using data poisoning attack mechanisms to search for worst-case memorization Jagielski et al. (2020); Nasr et al. (2021). MIAs are also increasingly used in privacy auditing, to estimate the degree of memorization of member data by an ML algorithm by resampling the target algorithm \(\mathcal{M}\) to bound \(\frac{P(M|in)}{P(M|out)}\)Jayaraman and Evans (2019). The auditing procedure usually involves searching for optimal neighboring datasets \(D,D^{\prime}\) and sampling the DP outputs \(\mathcal{M}(D),\mathcal{M}(D^{\prime})\), to get a Monte Carlo estimate of \(\epsilon\). This approach raises important challenges. First, existing search methods for neighboring inputs, involving enumeration or symbolic search, are impossible to scale to large datasets, making it difficult to find optimal dataset pairs. In addition, Monte Carlo estimation requires up to thousands of costly model retrainings to bound \(\epsilon\) with high confidence. Consequently, existing approaches for auditing ML models predominantly require the re-training of ML models for every (batch of) audit queries, which is computationally expensive in large-scale systems Jagielski et al. (2020); Zanella-Beguelin et al. (2022); Lu et al. (2023). This makes privacy auditing computationally expensive and gives an estimate by averaging over models, which might not reflect the true guarantee of a specific pipeline deployed in practice.

Nonetheless, improvements to auditing have been made in a variety of directions. For example, Nasr et al. (2023) and Maddock et al. (2023) have taken advantage of the iterative nature of DP-SGD, auditing individual steps to understand the privacy of the end-to-end algorithm. Andrew et al. (2023) leverage the fact that analyzing MIAs for non-member data does not require re-running the algorithm. Instead, it is possible to re-sample the non-member data point: if the data points are i.i.d. from an asymptotically Gaussian distribution with mean zero and variance \(1/d\), this enables a closed-form analysis of the non-member case.

Recently, Steinke et al. (2023) proposed a novel scheme for auditing differential privacy with \(O(1)\) training rounds. This approach enables privacy audits using multiple training examples from the

\begin{table}
\begin{tabular}{c c c c c} \hline \hline
**Model** & **Generator Relaxation (\(\gamma\))** & \(\mathbf{c_{b}}\) & \(\{\varepsilon+\mathbf{c}\}_{n}\) & \(\tilde{\varepsilon}\) \\ \hline \multirow{4}{*}{ResNet18 \(\epsilon=2\)} & 0 (no relaxation) & 2.508 & 2.066 & 0 \\  & 1e-5 & 2.507 & 2.065 & 0 \\  & 1e-4 & 2.499 & 2.059 & 0 \\  & 1e-3 & 2.365 & 1.989 & 0 \\ \hline \multirow{4}{*}{ResNet18 \(\epsilon=10\)} & 0 (no relaxation) & 2.508 & 2.833 & 0.325 \\  & 1e-5 & 2.507 & 2.801 & 0.294 \\  & 1e-4 & 2.499 & 2.570 & 0.071 \\  & 1e-3 & 2.365 & 1.280 & 0 \\ \hline \multirow{4}{*}{ResNet18 \(\epsilon=15\)} & 0 (no relaxation) & 2.508 & 3.661 & 1.153 \\  & 1e-5 & 2.507 & 3.658 & 1.151 \\  & 1e-4 & 2.499 & 3.628 & 1.129 \\  & 1e-3 & 2.365 & 3.312 & 0.947 \\ \hline \hline \multirow{2}{*}{ResNet18 \(\epsilon=2,\delta=\)1e-5} & 1e-5 & 2.035 & 2.064 & 0.029 \\  & 1e-4 & 2.029 & 2.069 & 0.030 \\  & 1e-3 & 1.904 & 1.988 & 0.084 \\ \hline \multirow{2}{*}{ResNet18 \(\epsilon=10,\delta=\)1e-5} & 1e-5 & 2.035 & 2.765 & 0.730 \\  & 1e-4 & 2.029 & 2.541 & 0.512 \\  & 1e-3 & 1.904 & 1.271 & 0 \\ \hline \multirow{2}{*}{ResNet18 \(\epsilon=15,\delta=\)1e-5} & 1e-5 & 2.035 & 3.654 & 1.619 \\  & 1e-4 & 2.029 & 3.625 & 1.596 \\ \cline{1-1}  & 1e-3 & 1.904 & 3.308 & 1.404 \\ \hline \hline \end{tabular}
\end{table}
Table 11: Privacy audit of ResNet18 under different values of \(\epsilon\)-DP and \((\epsilon,\delta)\)-DP, using PANORAMIA with different level of generator relaxations (\(\gamma\)), testing over range of recall from 0 to 0.5 without applying a union bound.

same model training, if examples are included in training independently (which requires control over the training phase, and altering the target model). They demonstrate the effectiveness of this new approach on DP-SGD, in which they achieve meaningful empirical privacy lower bounds by training only one model (the strongest results are achieved by including canaries in the training set though, which is not possible in our setting), whereas standard methods would require training hundreds of models. Our work builds closely on the theory from Steinke et al. (2023), but introduces a baseline model to account for distribution shifts. The key difference that enables our work to account for member/non-member distribution shifts lies in how we create the audit set in our privacy game (Definition 2). In Steinke et al. (2023), the audit set is fixed, and data points are randomly assigned to member or non-member by a Bernoulli random variable \(S\). Members are actually used in training the target model \(f\), while non-members are not (so assignment happens before training). In our framework, we take a set of known members (after the fact), and pair each point with a non-member(generated i.i.d. from the generator distribution). We then flip \(S\) to sample which data point of each pair will be shown to the "auditor" (MIA/baseline) for testing, thereby creating the test task of our privacy measurement. When we replace our generated data with in-distribution independent non-members (PANORAMIA RM;RN), we exactly enforce the same independence as Steinke et al. (2023) (and as a result have \(c=0\), and \(\tilde{c}\) is a lower-bound on \(\epsilon\)), except that we "waste" some member data by drawing our auditing game after the fact. The difference in analysis accounts for distribution shifts when our non-members are generated.

The field of MIA without direct connections to privacy leakage measurement has also seen a lot of recent activity, with new proposals to improve the strength of MIAs in various settings (Carlini et al., 2022; Nasr et al., 2019; Suri et al.; Zarifzadeh et al., 2023). While it is not the focus of this paper, an interesting avenue for future work is to port ideas from these new MIAs to improve PANORAMIA's MIA and baseline. Such transfer of MIA progress could lead to more powerful measurements with PANORAMIA. More closely related to our proposal, a recent set of works on Das et al. (2024); Duan et al. (2024); Meeus et al. (2024) MIAs on foundation models has observed that current evaluations are limited due a lack of availability of non-member data. Indeed, foundation models typical include all known data at the time of their training, and there is no well known public set of in-distribution non-member data. Evaluation tasks typical rely on more recent data, which is known to consist in non-members, but suffers from distribution shifts. Das et al. (2024); Duan et al. (2024); Meeus et al. (2024) all show that such a shift invalidates MIA evaluations by showing that "blind attacks" (i.e., our baselines) perform better than proposed MIAs. One could apply our proposed framework to such a setting: when the baseline in our framework performs better than the MIA, PANORAMIA returns a privacy loss measurement of zero instead of misleading measurements of MIA performance. We thus believe that PANORAMIA is an important step towards a theory for rigorous evaluations of membership inference for MIA models.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The abstract and Introduction sections highlight the motivation and contributions we make; We provide, to the best of our ability, comprehensive experimental results in Section 5 to justify our claims and contributions. Section 4 provides the theory behind our experiments, whilst Appendix B details proofs.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We discuss these in Section 4.2 and Section 6 of the paper.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: Yes, we present sections assumptions, theory, and formal proofs in 3, 4 and Appendix B, E provides respectively.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We attach the code as supplementary material in a zip file and also include the main experimental details in Section 5 as well as a detailed experimental setup, hyperparameters, compute details, and model architecture details along with other full necessary information in Appendix C to ensure reproducibility. Section 3 also includes the algorithm as well as the system design diagram detailing our privacy measurement framework. We also updated the paper and experiments according to the reviewers' helpful feedback.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: We specify the details of the code we release in the zip file we attach. The code is primarily the PANORAMIA privacy measurement pipeline. We include structured files specifying run instructions, licenses etc. Training details are within the code itself and also highlighted in Appendix C. Now we also updated the paper with an open source github code link given under the experimental section of the main paper.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: Section 5 and Table 1 give key experimental details, whilst the full experimental setup is highlighted in Appendix C, as well as the zip file of code attached.
7. **Experiment Statistical Significance**Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: We repeated all experiments that involved training data resampling 5 times, and repeated experiments that involved test data resampling over 10 times. All our figures and plots in the core body of the paper report results with error bars, with all results reported at a 95% confidence interval. We also report the standard deviation in the tables in Section 5.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We provide information on compute resources for each modality in Appendix C. We also updated compute times to run the PANORAMIA pipeline in C for each data modality.
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We have reviewed the NeurIPS Code of Ethics and conform to them in every respect.
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes]. Justification: We discuss these in Section 6; overall, our approach can only help model providers and owners to assess the privacy of risk of their data and is not applicable as a privacy attack. As a consequence, we believe that any impact from our proposed work is likely to be positive.
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer:[NA]. Justification: Our paper poses no such risks.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes]. Justification: All papers, code, and asset owners and authors have been properly cited to the best of our knowledge.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes]. Justification: We specify the details of the code we release in the zip file we attach. The code is primarily the PANORAMIA privacy measurement pipeline. We include structured files specifying run instructions, licenses, etc. Training details are within the code itself and also highlighted in Appendix C.

14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA]. Justification: The paper does not involve crowdsourcing, nor research with human subjects.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer:[NA]. Justification: The paper does not involve crowdsourcing nor research with human subjects.