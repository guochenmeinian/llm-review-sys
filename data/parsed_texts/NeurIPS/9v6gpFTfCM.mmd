# Unbiased Compression Saves Communication in Distributed Optimization: When and How Much?

 Yutong He

Peking University

yutonghe@pku.edu.cn

&Xinmeng Huang

University of Pennsylvania

xinmengh@sas.upenn.edu

Equal Contribution. Corresponding Author: Kun Yuan. Kun Yuan is also affiliated with National Engineering Laboratory for Big Data Analytics and Applications, and AI for Science Institute, Beijing, China.

Kun Yuan

Peking University

kunyuan@pku.edu.cn

Equal Contribution. Corresponding Author: Kun Yuan. Kun Yuan is also affiliated with National Engineering Laboratory for Big Data Analytics and Applications, and AI for Science Institute, Beijing, China.

###### Abstract

Communication compression is a common technique in distributed optimization that can alleviate communication overhead by transmitting compressed gradients and model parameters. However, compression can introduce information distortion, which slows down convergence and incurs more communication rounds to achieve desired solutions. Given the trade-off between lower per-round communication costs and additional rounds of communication, it is unclear whether communication compression reduces the total communication cost.

This paper explores the conditions under which unbiased compression, a widely used form of compression, can reduce the total communication cost, as well as the extent to which it can do so. To this end, we present the first theoretical formulation for characterizing the total communication cost in distributed optimization with unbiased compressors. We demonstrate that unbiased compression alone does not necessarily save the total communication cost, but this outcome can be achieved if the compressors used by all workers are further assumed independent. We establish lower bounds on the communication rounds required by algorithms using independent unbiased compressors to minimize smooth convex functions and show that these lower bounds are tight by refining the analysis for ADIANA. Our results reveal that using independent unbiased compression can reduce the total communication cost by a factor of up to \(\Theta(\sqrt{\min\{n,\kappa\}})\) when all local smoothness constants are constrained by a common upper bound, where \(n\) is the number of workers and \(\kappa\) is the condition number of the functions being minimized. These theoretical findings are supported by experimental results.

## 1 Introduction

Distributed optimization is a widely used technique in large-scale machine learning, where data is distributed across multiple workers and training is carried out through worker communication. However, dealing with a vast number of data samples and model parameters across workers poses a significant challenge in terms of communication overhead, which ultimately limits the scalability of distributed machine learning systems. To tackle this issue, communication compression strategies [3, 8, 52, 55, 49] have emerged, aiming to reduce overhead by enabling efficient yet imprecise message transmission. Instead of transmitting full-size gradients or models, these strategies exchange compressed gradients or model vectors of much smaller sizes in communication.

There are two common approaches to compression: quantization and sparsification. Quantization [3, 20, 39, 52] maps input vectors from a large, potentially infinite, set to a smaller set of discrete values. In contrast, sparsification [57, 55, 50] drops a certain amount of entries to obtain a sparse vector for communication. In literature [3, 27, 21], these compression techniques are often modeledas a random operator \(C\), which satisfies the properties of unbiasedness \(\mathbb{E}[C(x)]=x\) and \(\omega\)-bounded variance \(\mathbb{E}\|C(x)-x\|^{2}\leq\omega\|x\|^{2}\). Here, \(x\) represents the input vector to be compressed, and \(\omega\) is a fixed parameter that characterizes the degree of information distortion. Besides, part of the compressors can also be modeled as biased yet contractive operators [19, 48, 49].

While communication compression efficiently reduces the volume of vectors sent by workers, it suffers substantial information distortion. As a result, algorithms utilizing communication compression require additional rounds of communication to converge satisfactorily compared to algorithms without compression. This adverse effect of communication compression has been extensively observed both empirically [57, 25, 8] and theoretically [21, 49]. Since the extra rounds of communication needed to compensate for the information loss may outweigh the saving in the per-round communication cost from compression, this naturally motivates the following fundamental question:

_Q1. Can unbiased compression alone reduce the total communication cost?_

By "unbiased compression alone", we refer to the compression that solely satisfies the assumptions of unbiasedness and \(\omega\)-bounded variance without any additional advanced properties. To address this open question, we formulate the total communication cost as the product of the per-round communication cost and the number of rounds needed to reach an \(\epsilon\)-accurate solution to distributed optimization problems. Using this formulation, we demonstrate the decrease in the per-round communication cost from unbiased compression is completely offset by additional rounds of communication. Therefore, we answer Q1 by showing unbiased compression alone _cannot_ ensure a lower total communication cost, even with an optimal algorithmic design, see Sec. 3 for more details. This negative conclusion drives us to explore the next fundamental open question:

_Q2. Under what additional conditions and how much can unbiased compression provably save the total communication cost?_

Fortunately, some pioneering works [40, 32, 33] have shed light on this question. They impose _independence_ on unbiased compressors, _i.e._, the compressed vectors \(\{C_{i}(x_{i})\}_{i=1}^{n}\) sent by workers are mutually independent regardless of the inputs \(\{x_{i}\}_{i=1}^{n}\). This independence assumption enables an "error cancellation" effect, producing a more accurate compressed vector \(n^{-1}\sum_{i=1}^{n}C_{i}(x_{i})\) and hence incurring fewer additional rounds of communication compared to dependent compressors. Consequently, the decrease in the per-round communication cost outweighs the extra communication rounds, reducing the total communication cost.

However, it remains unclear how much the total communication cost can be reduced _at most_ by independent unbiased compression and whether we can develop algorithms to achieve this optimal reduction. Addressing this question poses significant challenges as it necessitates a study of the optimal convergence rate for algorithms using independent unbiased compression.

This paper provides the _first_ affirmative answer to this question for convex problems by: (i) establishing lower bounds on convergence rates of distributed algorithms employing independent unbiased compression, and (ii) demonstrating the tightness of these lower bounds by revisiting ADIANA [32] and presenting novel and refined convergence rates nearly attaining the lower bounds. Our results reveal that compared to non-compression algorithms, independent unbiased compression can save the total communication cost by up to \(\Theta(\sqrt{\min\{n,\kappa\}})\)-fold, where \(n\) is the number of workers and \(\kappa\in[1,+\infty]\) is the function condition number. Figure 1 provides a simple empirical justification. It shows independent compression (ADIANA i.d.) reduces communication costs compared to no compression (Nesterov's Accelerated algorithm), while dependent compression (ADIANA s.d.) does not, which validates our theory.

### Contributions

Specifically, our contributions are as follows:

Figure 1: Performance of ADIANA using random-\(s\) sparsification compressors with shared (s.d.) or independent (i.d.) randomness against distributed Nesterovâ€™s accelerated algorithm with no compression in communication. Experimental descriptions are in Appendix F.1

* We present a theoretical formalization of the total communication cost in distributed optimization with unbiased compression. With this formulation, we demonstrate that unbiased compression alone is insufficient to save the total communication cost, even with an optimal algorithmic design. This is because any reduction in the per-round communication cost is fully offset by the additional rounds of communication required due to the presence of compression errors.
* We prove lower bounds on the convergence complexity of distributed algorithms using independent unbiased compression to minimize smooth convex functions. Compared to lower bounds when using unbiased compression without independence [19], our lower bounds demonstrate significant improvements when \(n\) and \(\kappa\) are large, see the first two lines in Table 1. This improvement highlights the importance of independence in unbiased compression.
* We revisit ADIANA [32] by deriving an improved rate for strongly-convex functions and proving a novel convergence result for generally-convex functions. Our rates nearly match the lower bounds, suggesting their tightness and optimality. Our optimal complexities reveal that, compared to non-compression algorithms, independent unbiased compression can decrease total communication costs by up to \(\mathcal{O}(\sqrt{\min\{n,\kappa\}})\)-fold when all local smoothness constants are constrained by a common upper bound.
* We support our theoretical findings with experiments on both synthetic data and real datasets.

We present the lower bounds, upper bounds, and complexities of state-of-the-art distributed algorithms using independent unbiased compressors in Table 1. With our new and refined analysis, ADIANA nearly matches the lower bounds for both strongly-convex and generally-convex functions.

### Related work

**Communication compression.** Two main approaches to compression are extensively explored in literature: quantization and sparsification. Quantization coarsely encodes input vectors into fewer discrete values, \(e\)._g_., from 32-bit to 8-bit integers [37, 18]. Schemes like Sign-SGD [52, 8] use 1 bit per entry, introducing unbiased random information distortion. Other variants such as Q-SGD [3], TurnGrad [58], and natural compression [20] quantize each entry with more effective bits. In contrast, sparsification either randomly zeros out entries to yield sparse vectors [57], or transmits only the largest model/gradient entries [55].

\begin{table}
\begin{tabular}{c c c} \hline \hline
**Method** & **GC** & **SC** \\ \hline
**Lower Bound** & \(\bar{\Omega}\left(\omega\ln(\frac{1}{\epsilon})+\left(1+\frac{\omega}{\sqrt{n} }\right)\frac{\sqrt{L\Delta}}{\sqrt{\epsilon}}\right)\) & \(\bar{\Omega}\left(\left(\omega+\left(1+\frac{\omega}{\sqrt{n}}\right)\sqrt{ \kappa}\right)\ln\left(\frac{1}{\epsilon}\right)\right)\) \\ Lower Bound [19]\({}^{\ddagger}\) & \(\Omega\left((1+\omega)\frac{\sqrt{L\Delta}}{\sqrt{\epsilon}}\right)\) & \(\bar{\Omega}\left((1+\omega)\sqrt{\kappa}\ln\left(\frac{1}{\epsilon}\right)\right)\) \\ \hline CGD [27]\({}^{\Diamond}\) & \(\mathcal{O}\left((1+\omega)\frac{L\Delta}{\sqrt{\epsilon}}\right)\) & \(\bar{\mathcal{O}}\left((1+\omega)\kappa\ln\left(\frac{1}{\epsilon}\right)\right)\) \\ ACGD [32]\({}^{\Diamond}\) & \(\mathcal{O}\left((1+\omega)\frac{\sqrt{L\Delta}}{\sqrt{\epsilon}}\right)\) & \(\bar{\mathcal{O}}\left((1+\omega)\sqrt{\kappa}\ln\left(\frac{1}{\epsilon}\right)\right)\) \\ DIANA [40] & \(\mathcal{O}\left(\left(1+\frac{\omega^{2}+\omega}{n+\omega}\right)\frac{L \Delta}{\epsilon}\right)\) & \(\bar{\mathcal{O}}\left(\left(\omega+\left(1+\frac{\omega}{\sqrt{n}}\right) \kappa\ln\left(\frac{1}{\epsilon}\right)\right)\right.\) \\ EF21 [49]\({}^{\ddagger}\) & \(\bar{\mathcal{O}}\left((1+\omega)\frac{L\Delta}{\epsilon}\right)\) & \(\bar{\mathcal{O}}\left((1+\omega)\kappa\ln\left(\frac{1}{\epsilon}\right)\right)\) \\ ADIANA [32] & â€” & \(\bar{\mathcal{O}}\left(\left(\omega+\left(1+\frac{\omega^{3/4}}{n^{1/4}}+ \frac{\omega}{\sqrt{n}}\right)\sqrt{\kappa}\right)\ln\left(\frac{1}{\epsilon }\right)\right)\) \\ CANITA [33]\({}^{\ddagger}\) & \(\mathcal{O}\left(\omega\frac{\bar{\mathcal{D}}\Delta}{\sqrt{\epsilon}}+\left(1+ \frac{\omega^{3/4}}{n^{1/4}}+\frac{\omega}{\sqrt{n}}\right)\frac{\sqrt{L\Delta }}{\sqrt{\epsilon}}\right)\) & â€” \\ NEOLITHIC [19]\({}^{\ddagger}\) & \(\bar{\mathcal{O}}\left((1+\omega)\frac{\sqrt{L\Delta}}{\sqrt{\epsilon}}\right)\) & \(\bar{\mathcal{O}}\left((1+\omega)\sqrt{\kappa}\ln\left(\frac{1}{\epsilon}\right)\right)\) \\ \hline
**ADIANA (Thm. 3)** & \(\mathcal{O}\left(\omega\frac{\sqrt{L\Delta}}{\sqrt{\epsilon}}+\left(1+\frac{ \omega}{\sqrt{n}}\right)\frac{\sqrt{L\Delta}}{\sqrt{\epsilon}}\right)\) & \(\bar{\mathcal{O}}\left(\left(\omega+\left(1+\frac{\omega}{\sqrt{n}}\right)\sqrt{ \kappa}\right)\ln\left(\frac{1}{\epsilon}\right)\right)\) \\ \hline \hline \end{tabular} \({}^{\Diamond}\) Results obtained in the single-worker setting and cannot be extended to the distributed setting.

\({}^{\ddagger}\) The rate is obtained by correcting mistakes in the derivations of [33]. See details in Appendix E.

\({}^{\ddagger}\) Results hold without assuming independence across compressors.

\end{table}
Table 1: Lower and upper bounds on the number of communication rounds for distributed algorithms using unbiased compression to achieve an \(\epsilon\)-accurate solution. Notations \(\Delta,n,L,\mu\) (\(\kappa\triangleq L/\mu\geq 1\)) are defined in Section 2. \(\omega\) is a parameter for unbiased compressors (Assumption 2). \(\bar{\mathcal{O}}\) and \(\bar{\Omega}\) hides logarithmic factors independent of \(\epsilon\). GC and SC denote generally-convex and strongly-convex functions respectively.

**Error compensation.** Recent works [52; 59; 55; 4; 49] propose error compensation or feedback to relieve the effects of compression errors. These techniques propagate information loss backward during compression, thus preserving more useful information. Reference [52] uses error compensation for 1-bit quantization, while the work [59] proposes error-compensated quantization for quadratic problems. Error compensation also reduces sparsification-induced errors [55] and is studied for convergence in non-convex scenarios [4]. Recently, the work [49] introduces EF21, an error feedback scheme that compresses only local gradient increments with improved theoretical guarantees.

**Lower bounds.** Lower bounds in optimization set a limit for the performance of a single or a class of algorithms. Prior works have established numerous lower bounds for optimization algorithms [1; 15; 6; 43; 7; 21; 64; 17; 44]. In the field of distributed optimization with communication compression, reference [46] provides an algorithm-specific lower bound for strongly-convex functions, while the work [53] establishes the bit-wise complexities for PL-type problems, which reflect the influence of the number of agents \(n\) and dimension \(d\), but not the condition number \(\kappa\) and the compression \(\omega\). In particular, [21] characterizes the optimal convergence rate for all first-order and linear-spanning algorithms, in the stochastic non-convex case, which is later extended by [19] to convex cases.

**Accelerated algorithms with communication compression.** There is a scarcity of academic research on compression algorithms incorporating acceleration, as evident in a limited number of studies [32; 33; 48]. References [32; 33] develop accelerated algorithms with compression in the strongly-convex and generally-convex cases, respectively. For distributed finite-sum problems, accelerated algorithms with compression can further leverage variance-reduction techniques to expedite convergence [48].

**Other communication-efficient strategies.** Other than communication compression studied in this paper, there are a few different techniques to mitigate the communication overhead in distributed systems, including decentralized communication and lazy communication. Notable examples of decentralized algorithms encompass decentralized SGD [12; 34; 30; 64], D2/Exact-Diffusion [56; 62; 61], gradient tracking [47; 60; 29; 2], and their momentum variants [35; 63]. Lazy communication allows each worker to either perform multiple local updates as opposed to a single communication round [38; 54; 41; 24; 14; 22], or by adaptively skipping communication [13; 36].

## 2 Problem setup

This section introduces the problem formulation and assumptions used throughout the paper. We consider the following distributed stochastic optimization problem

\[\min_{x\in\mathbb{R}^{d}}\quad f(x)=\frac{1}{n}\sum_{i=1}^{n}f_{i}(x), \tag{1}\]

where the global objective function \(f(x)\) is decomposed into \(n\) local objective functions \(\{f_{i}(x)\}_{i=1}^{n}\), and each local \(f_{i}(x)\) is maintained by node \(i\). Next, we introduce the setup and assumptions.

### Function class

We let \(\mathcal{F}^{\Delta}_{L,\mu}\) (\(0\leq\mu\leq L\)) denote the class of convex and smooth functions satisfying Assumption 1. We define \(\kappa\triangleq L/\mu\in[1,+\infty]\) as the condition number of the functions to be optimized. When \(\mu>0\), \(\mathcal{F}^{\Delta}_{L,\mu}\) represents strongly-convex functions. Conversely, when \(\mu=0\), \(\mathcal{F}^{\Delta}_{L,\mu}\) represents generally-convex functions with \(\kappa=\infty\).

**Assumption 1** (Convex and smooth function).: _We assume each \(f_{i}(x)\) is \(L\)-smooth and \(\mu\)-strongly convex, i.e., there exists constants \(L\geq\mu\geq 0\) such that_

\[\frac{\mu}{2}\|y-x\|^{2}\leq f_{i}(y)-f_{i}(x)-\langle\nabla f_{i}(x),y-x \rangle\leq\frac{L}{2}\|y-x\|^{2}\]

_for any \(x,y\in\mathbb{R}^{d}\) and \(1\leq i\leq n\). We further assume \(\|x^{0}-x^{\star}\|^{2}\leq\Delta\) where \(x^{\star}\) is one of the global minimizers of \(f(x)=\frac{1}{n}\sum_{i=1}^{n}f_{i}(x)\)._

### Compressor class

Each worker \(i\in\{1,\cdots,n\}\) is equipped with a potentially random compressor \(C_{i}:\mathbb{R}^{d}\rightarrow\mathbb{R}^{d}\). We let \(\mathcal{U}_{\omega}\) denote the set of all \(\omega\)-unbiased compressors satisfying Assumption 2, and \(\mathcal{U}^{\text{ind}}_{\omega}\) denote the set of all _independent_\(\omega\)-unbiased compressors satisfying both Assumption 2 and Assumption 3.

**Assumption 2** (Unbiased compressor).: _We assume all compressors \(\{C_{i}\}_{i=1}^{n}\) satisfy_

\[\mathbb{E}[C_{i}(x)]=x,\quad\mathbb{E}[\|C_{i}(x)-x\|^{2}]\leq\omega\|x\|^{2}, \quad\forall\,x\in\mathbb{R}^{d} \tag{2}\]

_for constant \(\omega\geq 0\) and any input \(x\in\mathbb{R}^{d}\), where the expectation is taken over the randomness of the compression operator \(C_{i}\).2_

Footnote 2: Compression is typically employed for input \(x\) that is bounded [3, 59] or encoded with finite bits (_e.g._, float64 numbers) [20]. In these practical scenarios, \(\omega\)-unbiased compression can be employed with finite bits. For instance, random-\(s\) sparsification for \(r\)-bit \(d\)-dimensional vectors costs (nearly) \(rs=rd/(1+\omega)\) bits per communication where \(\omega=d/s-1\).

**Assumption 3** (Independent compressor).: _We assume all compressors \(\{C_{i}\}_{i=1}^{n}\) are mutually independent, i.e., outputs \(\{C_{i}(x_{i})\}_{i=1}^{n}\) are mutually independent random variables for any \(\{x_{i}\}_{i=1}^{n}\)._

### Algorithm class

Similar to [19], we consider centralized and synchronous algorithms in which first, every worker is allowed to communicate only directly with a central server but not between one another; second, all iterations/communications are synchronized, meaning that all workers start each of their iterations simultaneously. We further require algorithms to satisfy the so-called "linear-spanning" property, which appears in [9, 10, 21, 19] (see formal definition in Appendix C). Intuitively, this property requires each local model \(x_{i}^{k}\) to lie in the linear manifold spanned by the local gradients and the received messages at worker \(i\). The linear-spanning property is satisfied by all algorithms in Table 1 as well as most first-order methods [42, 28, 23, 65].

Formally, this paper considers a class of algorithms specified by Definition 1.

**Definition 1** (Algorithm class).: _Given compressors \(\{C_{i}\}_{i=1}^{n}\), we let \(\mathcal{A}_{\{C_{i}\}_{i=1}^{n}}\) denote the set of all centralized, synchronous, linear-spanning algorithms admitting compression in which compressor \(C_{i}\), \(\forall\,1\leq i\leq n\), is applied for the messages sent by worker \(i\) to the server._

For any algorithm \(A\in\mathcal{A}_{\{C_{i}\}_{i=1}^{n}}\), we define \(\hat{x}^{k}\) and \(x_{i}^{k}\) as the output of the server and worker \(i\) respectively, after \(k\) communication rounds.

### Convergence complexity

With all the interested classes introduced above, we are ready to define our complexity metric for convergence analysis. Given a set of local functions \(\{f_{i}\}_{i=1}^{n}\in\mathcal{F}_{L,\mu}^{\text{A}}\), a set of compressors \(\{C_{i}\}_{i=1}^{n}\in\mathcal{C}\) (\(\mathcal{C}=\mathcal{U}_{\omega}^{\text{ind}}\) or \(\mathcal{U}_{\omega}\)), and an algorithm \(A\in\mathcal{A}_{\{C_{i}\}_{i=1}^{n}}\), we let \(\hat{x}_{A}^{t}\) denote the output of algorithm \(A\) after \(t\) communication rounds. The convergence complexity of \(A\) solving \(f(x)=\frac{1}{n}\sum_{i=1}^{n}f_{i}(x)\) under \(\{(f_{i},C_{i})\}_{i=1}^{n}\) is defined as

\[T_{\epsilon}(A,\{(f_{i},C_{i})\}_{i=1}^{n})=\min\left\{t\in\mathbb{N}:\mathbb{ E}[f(\hat{x}_{A}^{t})]-\min_{x}f(x)\leq\epsilon\right\}. \tag{3}\]

This measure corresponds to the number of communication rounds required by algorithm \(A\) to achieve an \(\epsilon\)-accurate optimum of \(f(x)\) in expectation.

**Remark 1**.: _The measure in (3) is commonly referred to as the communication complexity in literature [51, 21, 31, 32]. However, we refer to it as the convergence complexity here to avoid potential confusion with the notion of "communication complexity" and "total communication cost". This complexity metric has been traditionally used to compare communication rounds used by distributed algorithms [32, 33]. However, it cannot capture the total communication costs of multiple algorithms with different per-round communication costs, e.g., algorithms with or without communication compression. Therefore, it is unable to address the motivating questions Q1 and Q2._

**Remark 2**.: _The definition of \(T_{\epsilon}\) can be independent of the per-round communication cost, which is specified only through the degree of compression \(\omega\) (i.e., choice of compressor class). However, to be precise, we may further assume these compressors are non-adaptive with the same fixed per-round communication cost. Namely, the compressors output compressed vectors that can be represented by a fixed and common number of bits. Notably, such hypothesis of non-adaptive cost is widely adopted for practical comparison of communication costs and is valid when input \(x\) is bounded or can be encoded with finite bits [3, 59, 20, 22]._

## 3 Total communication cost

### Formulation of total communication cost

This section introduces the concept of Total Communication Cost (TCC). TCC can be calculated at both the level of an individual worker and of the overall distributed machine learning system comprising all \(n\) workers. In a centralized and synchronized algorithm where each worker communicates compressed vectors of the same dimension, the TCC of the entire system is directly proportional to the TCC of a single worker. Therefore, it is sufficient to use the TCC of a single worker as the metric for comparing different algorithms. In this paper, we let TCC denote the total communication cost incurred by each worker in achieving a desired solution when no ambiguity is present.

Let each worker to be equipped with a non-adaptive compressor with the same fixed per-round communication cost, _i.e._, the compressor outputs compressed vectors of the same length (size), the TCC of an algorithm \(A\) to solve problem (1) using a set of \(\omega\)-unbiased compressors \(\{C_{i}\}_{i=1}^{n}\) in achieving an \(\epsilon\)-accurate optimum can be characterized as

\[\text{TCC}_{\epsilon}(A,\{(f_{i},C_{i})\}_{i=1}^{n}):=\text{per-round cost}(\{C_{i}\}_{i=1}^{n})\times T_{\epsilon}(A,\{(f_{i},C_{i})\}_{i=1}^{n}). \tag{4}\]

### A tight lower bound for per-round cost

The per-round communication cost incurred by \(\{C_{i}\}_{i=1}^{n}\) in (4) will vary with different \(\omega\) values. Typically, compressors that induce less information distortion, _i.e._, associated with a smaller \(\omega\), incur higher per-round costs. To illustrate this, we consider random-\(s\) sparsification compressors, whose per-round cost corresponds to the transmission of \(s\) entries, which depends on parameter \(\omega\) through \(s=d/(1+\omega)\) (see Example 1 in Appendix A). Specifically, if each entry of the input \(x\) is numerically represented with \(r\) bits, then the random-\(s\) sparsification incurs a per-round cost of \(rd/(1+\omega)\) bits up to a logarithm factor.

The following proposition, motivated by the inspiring work [50], establishes a lower bound of TCC when using any compressor satisfying Assumption 2.

**Proposition 1**.: _Let \(x\in\mathbb{R}^{d}\) be the input to a compressor \(C\) and \(b\) be the number of bits needed to compress \(x\). Suppose each entry of input \(x\) is numerically represented with \(r\) bits, i.e., errors smaller than \(2^{-r}\) are ignored. Then for any compressor \(C\) satisfying Assumption 2, the per-round communication cost of \(C(x)\) is lower bounded by \(b=\Omega_{r}(d/(1+\omega))\) where \(r\) is viewed as an absolute number in \(\Omega_{r}(\cdot)\) (See the proof in Appendix B)._

Proposition 1 presents a lower bound on the per-round cost of an arbitrary compressor satisfying Assumption 2. This lower bound is tight since the random-\(s\) compressor discussed above can achieve this lower bound up to a logarithm factor. Since \(d\) only relates to the problem instance itself and \(r\) is often a constant absolute number in practice, _e.g._, \(r=32\) or \(64\), both of which are independent of the choices of compressors and algorithm designs, they can be omitted from the lower bound order. As a result, the TCC in (4) can be lower bounded by

\[\text{TCC}_{\epsilon}=\Omega((1+\omega)^{-1})\times T_{\epsilon}(A,\{(f_{i},C _{i})\}_{i=1}^{n}). \tag{5}\]

Notably, when no compression is employed (i.e., \(\omega=0\)), \(\text{TCC}_{\epsilon}=\Omega(1)\times T_{\epsilon}(A,\{(f_{i},C_{i})\}_{i=1}^{ n})\) is consistent with the convergence complexity.

## 4 Unbiased compressor alone cannot save total communication cost

With formulation (5), given the number of communication rounds \(T_{\epsilon}\), the total communication cost can be readily characterized. A recent pioneer work [19] characterizes a tight lower bound for \(T_{\epsilon}(A,\{(f_{i},C_{i})\}_{i=1}^{n})\) when each \(C_{i}\) satisfies Assumption 2.

**Lemma 1** ([19], Theorem 1, Informal).: _Relying on unbiased compressibility alone, i.e., \(\{C_{i}\}_{i=1}^{n}\in\mathcal{U}_{\omega}\), without leveraging additional property of compressors such as mutual independence, the fewest rounds of communication needed by algorithms with compressed communication to achieve an \(\epsilon\)-accurate solution to distributed strongly-convex and generally-convex optimization problems are lower bounded by \(T_{\epsilon}=\Omega((1+\omega)\sqrt{\kappa}\ln{(\mu\Delta/\epsilon)})\) and \(T_{\epsilon}=\Omega((1+\omega)\sqrt{L\Delta/\epsilon})\), respectively._

Substituting Lemma 1 into our TCC lower bound in (5), we obtain \(\text{TCC}_{\epsilon}=\tilde{\Omega}(\sqrt{\kappa}\ln(1/\epsilon))\) or \(\Omega(\sqrt{L\Delta/\epsilon})\) in the strongly-convex or generally-convex case, respectively, by relying solely onunbiased compression. These results do not depend on the compression parameter \(\omega\), indicating that _the lower per-round cost is fully compensated by the additional rounds of communication incurred by compressor errors_. Notably, these lower bounds are of the same order as optimal algorithms without compression such as Nesterov's accelerated gradient descent [43; 44], leading to the conclusion:

**Theorem 1**.: _When solving convex optimization problems following Assumption 1, any algorithm \(A\in\mathcal{A}_{\{C_{i}\}_{i=1}^{n}}\) that relies solely on unbiased compression satisfying Assumption 2 cannot reduce the total communication cost compared to not using compression. The best achievable total communication cost with unbiased compression alone is of the same order as without compression._

Theorem 1 presents a _negative_ finding that unbiased compression alone is insufficient to reduce the total communication cost, even with an optimal algorithmic design.3 Meanwhile, it also implies that to develop algorithms that provably reduce the total communication cost, one must leverage compressor properties beyond \(\omega\)-unbiasedness as defined in (2). Fortunately, mutual independence is one such property which we discuss in depth in later sections.

Footnote 3: The theoretical results can vary from practical observations due to the particularities of real datasets with which compressed algorithms can enjoy faster convergence, compared to the minimax optimal rates (_e.g._, ours and [19]) justified without resorting any additional condition.

## 5 Independent unbiased compressor provably saves communication

### An intuition on why independence can help

A series of works [40; 32; 33] have shown theoretical improvements in the total communication cost by imposing independence across compressors, _i.e._, \(\{C_{i}\}_{i=1}^{n}\in\mathcal{U}_{\omega}^{\mathrm{ind}}\). The intuition behind the role of independence among worker compressors can be illustrated by a simple example where workers intend to transmit the same vector \(x\) to the server. Each worker \(i\) sends a compressed message \(C_{i}(x)\) that adheres to Assumption 2. Consequently, the aggregated vector \(n^{-1}\sum_{i=1}^{n}C_{i}(x)\) is an unbiased estimate of \(x\) with variance

\[\mathbb{E}\left[\left\|\frac{1}{n}\sum_{i=1}^{n}C_{i}(x)-x\right\|^{2}\right]= \frac{1}{n^{2}}\left(\sum_{i=1}^{n}\mathbb{E}[\|C_{i}(x)-x\|^{2}]+\sum_{i\neq j }\mathbb{E}[\langle C_{i}(x)-x,C_{j}(x)-x\rangle]\right) \tag{6}\]

If the compressed vectors \(\{C_{i}(x)\}_{i=1}^{n}\) are further assumed to be independent, _i.e._, \(\{C_{i}\}_{i=1}^{n}\in\mathcal{U}_{\omega}^{\mathrm{ind}}\), then the cancellation of cross error terms leads to the following equation:

\[\mathbb{E}\left[\left\|\frac{1}{n}\sum_{i=1}^{n}C_{i}(x_{i})-x\right\|^{2} \right]=\frac{1}{n^{2}}\sum_{i=1}^{n}\mathbb{E}[\|C_{i}(x)-x\|^{2}]\leq\frac{ \omega}{n}\|x\|^{2}. \tag{7}\]

We observe that the mutual independence among unbiased compressors leads to a decreased variance, which corresponds to the information distortion, of the aggregated message. Remarkably, this reduction is achieved by a factor of \(n\) compared to the transmission of a single compressor. Therefore, the independence among the compressors plays a pivotal role in enhancing the accuracy of the aggregated vector, consequently reducing the number of required communication rounds.

On the contrary, in cases where independence is not assumed and no other properties of compressors can be leveraged, the use of Cauchy's inequality only allows us to bound variance (6) as follows:

\[\mathbb{E}\left[\left\|\frac{1}{n}\sum_{i=1}^{n}C_{i}(x)-x\right\|^{2}\right] \leq\frac{1}{n}\sum_{i=1}^{n}\mathbb{E}[\|C_{i}(x)-x\|^{2}]\leq\omega\|x\|^{2}. \tag{8}\]

It is important to note that the upper bound \(\omega\|x\|^{2}\) can only be attained when the compressors \(\{C_{i}\}_{i=1}^{n}\) are identical, indicating that this bound cannot be generally improved further. By comparing (7) and (8), we can observe that the variance of the aggregated vector achieved through unbiased compression with independence can be \(n\) times smaller than the variance achieved without independence.

### Convergence lower bounds with independent unbiased compressors

While mutual independence can boost the unbiased worker compressors, it remains unclear how much the total communication cost can be reduced _at most_ by independent unbiased compression and how to develop algorithms to achieve this optimal reduction. The following subsections aim to address these open questions.

Following the formulation in (5), to establish the best achievable total communication cost using _independent unbiased compression_, we shall study tight lower bounds on the number of communication rounds \(T_{\epsilon}\) to achieve an \(\epsilon\)-accurate solution, which is characterized by the following theorem.

**Theorem 2**.: _For any \(L\geq\mu\geq 0\), \(n\geq 2\), the following results hold. See the proof in Appendix C._

* _Strongly-convex:_ _For any_ \(\Delta>0\)_, there exists a constant_ \(c_{\kappa}\) _only depends on_ \(\kappa\triangleq L/\mu\)_, a set of local loss functions_ \(\{f_{i}\}_{i=1}^{n}\in\mathcal{F}^{\Delta}_{L,\mu>0}\)_, independent unbiased compressors_ \(\{C_{i}\}_{i=1}^{n}\in\mathcal{U}^{\mathrm{ind}}_{\omega}\)_, such that the output_ \(\hat{x}\) _of any_ \(A\in\mathcal{A}_{\{C_{i}\}_{i=1}^{n}}\) _starting from_ \(x^{0}\) _requires_ \[T_{\epsilon}(A,\{(f_{i},C_{i})\}_{i=1}^{n})=\Omega\left(\left(\omega+\left(1+ \frac{\omega}{\sqrt{n}}\right)\sqrt{\kappa}\right)\ln\left(\frac{\mu\Delta}{ \epsilon}\right)\right)\] _rounds of communication to reach_ \(\mathbb{E}[f(\hat{x})]-\min_{x}f(x)\leq\epsilon\) _for any_ \(0<\epsilon\leq c_{\kappa}\mu\Delta\)_._
* _Generally-convex:_ _For any_ \(\Delta>0\)_, there exists a constant_ \(c=\Theta(1)\)_, a set of local loss functions_ \(\{f_{i}\}_{i=1}^{n}\in\mathcal{F}^{\Delta}_{L,0}\)_, independent unbiased compressors_ \(\{C_{i}\}_{i=1}^{n}\in\mathcal{U}^{\mathrm{ind}}_{\omega}\)_, such that the output_ \(\hat{x}\) _of any_ \(A\in\mathcal{A}_{\{C_{i}\}_{i=1}^{n}}\) _starting from_ \(x^{0}\) _requires at least_ \[T_{\epsilon}(A,\{(f_{i},C_{i})\}_{i=1}^{n})=\Omega\left(\omega\ln\left(\frac{L \Delta}{\epsilon}\right)+\left(1+\frac{\omega}{\sqrt{n}}\right)\left(\frac{L \Delta}{\epsilon}\right)^{\frac{1}{2}}\right)\] _rounds of communication to reach_ \(\mathbb{E}[f(\hat{x})]-\min_{x}f(x)\leq\epsilon\) _for any_ \(0<\epsilon\leq cL\Delta\)_._

**Consistency with prior works.** The lower bounds established in Theorem 2 are consistent with the best-known lower bounds in previous literature. When \(\omega=0\), our result reduces to the lower bound for distributed first-order algorithms established by Y. Nesterov in [43]. When \(n=1\), our result reduces to the lower bound established in [19] for the single-node case.

**Independence improves lower bounds.** A recent work [19] establishes lower bounds for unbiased compression without the independence assumption, listed in the second row of Table 1. Compared to these results, our lower bound in Theorem 2 replaces \(\omega\) with \(\omega/\sqrt{n}\), showing a reduction in order. This reduction highlights the role of independence in unbiased compression. To better illustrate the reduction, we take the strongly-convex case as an example. The ratio of the number of communication rounds \(T_{\epsilon}\) under unbiased compression with independence to the one without independence is:

\[\frac{\omega+(1+\omega/\sqrt{n})\sqrt{\kappa}}{(1+\omega)\sqrt{\kappa}}=\frac{ 1}{1+\omega}+\frac{\omega}{1+\omega}\left(\frac{1}{\sqrt{n}}+\frac{1}{\sqrt{ \kappa}}\right)=\Theta\left(\frac{1}{\min\{1+\omega,\sqrt{n},\sqrt{\kappa}\} }\right). \tag{9}\]

Clearly, using independent unbiased compression can allow algorithms to converge faster, by up to a factor of \(\Theta(\sqrt{\min\{n,\kappa\}})\) (attained at \(\omega\gtrsim\sqrt{\min\{n,\kappa\}}\)), in terms of the number of communication rounds, compared to the best algorithms with unbiased compressors but without independence.

**Total communication cost.** Substituting Theorem 2 into the TCC formulation in (5), we can obtain the TCC of algorithms using independent unbiased compression. Comparing this with algorithms without compression, such as Nesterov's accelerated algorithm, and using the relations in (9), we can demonstrate that independent unbiased compression can reduce the total communication cost. Such reduction can be up to \(\Theta(\sqrt{\min\{n,\kappa\}})\) by using compressors with \(\omega\gtrsim\sqrt{\min\{n,\kappa\}}\), _e.g._, random-\(s\) sparsification with \(s\lesssim d/\sqrt{\min\{n,\kappa\}}\).

### ADIANA: a unified optimal algorithm

By comparing existing algorithms using independent unbiased compression, such as DIANA, ADIANA, and CANITA, to our established lower bounds in Table 1, it becomes clear that there is a noticeable gap between their convergence complexities and our established lower bounds. This gap could indicate that these algorithms are suboptimal, but it could also mean that our lower bounds are loose. As a result, our claim that using independent unbiased compression reduces the total communication cost by up to \(\Theta(\sqrt{\min\{n,\kappa\}})\) times is not well-grounded yet. In this section, we address this issue by revisiting ADIANA [32] (Algorithm 1) and providing novel and refined convergence results in both strongly- and generally-convex cases.

**Algorithm 1**: ADIANA

**Input:** Scalars \(\{\theta_{1,k}\}_{k=0}^{T-1}\), \(\theta_{2}\), \(\alpha\), \(\beta\), \(\{\gamma_{k}\}_{k=0}^{T-1}\), \(\{\eta_{k}\}_{k=0}^{T-1}\), \(p\).

Initialize \(w^{0}=x^{0}=y^{0}=z^{0}=h^{0}=h_{i}^{0}\), \(\forall\,1\leq i\leq n\).

**for**\(k=0,\cdots,T-1\)do

**On server:**

 Update \(x\): \(x^{k}=\theta_{1,k}z^{k}+\theta_{2}w^{k}+(1-\theta_{1,k}-\theta_{2})y^{k}\) and broadcast to all workers;

**On all workers in parallel:**

 Compress the increment of local gradient \(m_{i}^{k}=C_{i}(\nabla f_{i}(x^{k})-h_{i}^{k})\) and send to the server;

 Compress the increment of local gradient \(c_{i}^{k}=C_{i}(\nabla f_{i}(w^{k})-h_{i}^{k})\) and send to the server;

 Update local shift \(h_{i}^{k+1}=h_{i}^{k}+\alpha c_{i}^{k}\);

**On server:**

 Aggregate received compressed message \(g^{k}=h^{k}+\frac{1}{n}\sum_{i=1}^{n}m_{i}^{k}\);

 Update shift \(h^{k+1}=h^{k}+\alpha\frac{1}{n}\sum_{i=1}^{n}c_{i}^{k}\);

 Apply gradient descent \(y^{k+1}=x^{k}-\eta_{k}g^{k}\);

 Update \(z\): \(z^{k+1}=\beta z^{k}+(1-\beta)x^{k}+\frac{\gamma_{k}}{\eta_{k}}(y^{k+1}-x^{k})\);

 Update \(w\): \(w^{k+1}=\begin{cases}y^{k},&\text{with probability }p,\\ w^{k},&\text{with probability }1-p;\end{cases}\)

**Output:**\(\hat{x}=w^{T}\) if \(f(w^{T})\leq f(y^{T})\) else \(\hat{x}=y^{T}\).

In the strongly-convex case, we refine the analysis of [32] by: (i) adopting new parameter choices where the initial scalar \(\theta_{2}\) is delicately chosen instead of being fixed as \(\theta_{2}=1/2\) in [32], (ii) balancing different terms in the construction of the Lyapunov function. While we do not modify the algorithm design, our technical ingredients are necessary to obtain an improved convergence rate. In the generally-convex case, we provide the _first_ convergence result for ADIANA, which is missing in literature to our knowledge. In both strongly- and generally-convex cases, our convergence results (nearly) match the lower bounds in Theorem 2. This verifies the tightness of our lower bounds for both the convergence complexity and the total communication cost. In particular, our results are:

**Theorem 3**.: _For any \(L\geq\mu\geq 0\), \(\Delta\geq 0\), \(n\geq 1\), and precision \(\epsilon>0\), the following results hold. See the proof in Appendix D._

* _Strongly-convex:_ _If_ \(\mu>0\)_, by setting parameters_ \(\eta_{k}\equiv\eta=n\theta_{2}/(120\omega L)\)_,_ \(\theta_{1,k}\equiv\theta_{1}=1/(3\sqrt{\kappa})\)_,_ \(\alpha=p=1/(1+\omega)\)_,_ \(\gamma_{k}\equiv\gamma=\eta/(2\theta_{1}+\eta\mu)\)_,_ \(\beta=2\theta_{1}/(2\theta_{1}+\eta\mu)\)_, and_ \(\theta_{2}=1/(3\sqrt{n}+3n/\omega)\)_, ADIANA requires_ \[\mathcal{O}\left(\left(\omega+\left(1+\frac{\omega}{\sqrt{n}}\right)\sqrt{ \kappa}\right)\ln\left(\frac{L\Delta}{\epsilon}\right)\right)\] _rounds of communication to reach_ \(\mathbb{E}[f(\hat{x})]-\min_{x}f(x)\leq\epsilon\)_._
* _Generally-convex:_ _If_ \(\mu=0\)_, by setting parameters_ \(\alpha=1/(1+\omega)\)_,_ \(\beta=1\)_,_ \(p=\theta_{2}=1/(3(1+\omega))\)_,_ \(\theta_{1,k}=9/(k+27(1+\omega))\)_,_ \(\gamma_{k}=\eta_{k}/(2\theta_{1,k})\)_, and_ \[\eta_{k}=\min\left\{\frac{k+1+27(1+\omega)}{9(1+\omega)^{2}(1+27(1+\omega))L},\frac{3n}{200high-precision regime \(\epsilon<L\Delta\left(\frac{1+\omega/\sqrt{n}}{1+\omega}\right)^{6}\). Our refined rates for ADIANA are state-of-the-art among existing algorithms using independent unbiased compression.

## 6 Experiments

In this section, we empirically compare ADIANA with DIANA [32], EF21 [49], and CANITA [33] using unbiased compression, as well as Nesterov's accelerated algorithm [43] which is an optimal algorithm when no compression is employed. We conduct experiments on least-square problems (strongly-convex) with synthetic datasets as well as logistic regression problems (generally-convex) with real datasets. In all experiments, we measure the total communicated bits sent by a single worker, which is calculated through _communication rounds to acheive an \(\epsilon\)-accurate solutions \(\times\) per-round communicated bits_. All curves are averaged over \(20\) trials with the region of standard deviations depicted. Due to the space limit, we only provide results with random-\(s\) compressors here. More experimental results can be found in Appendix F.2.

**Least squares.** Consider a distributed least-square problem (1) with \(f_{i}(x):=\frac{1}{2}\|A_{i}x-b_{i}\|^{2}\), where \(A_{i}\in\mathbb{R}^{M\times d}\) and \(b_{i}\in\mathbb{R}^{M}\) are randomly generated. We set \(d=20\), \(n=400\), and \(M=25\), and generate \(A_{i}\)'s by randomly generating a Gaussian matrix in \(\mathbb{R}^{nM\times d}\), then modify its condition number to \(10^{4}\) through the SVD decomposition, and finally distribute its rows to all \(A_{i}\). We use independent random-\(1\) compressors for communication compression. The results are depicted in Fig. 2 (left) where we observe ADIANA beats all baselines in terms of the total communication cost. We do not compare with CANITA since it does not have theoretical guarantees for strongly-convex problems.

**Logistic regression.** Consider a distributed logistic regression problem (1) with \(f_{i}(x):=\frac{1}{M}\sum_{m=1}^{M}\ln(1+\exp(-b_{i,m}a_{i,m}^{\top}x))\), where \(\{(a_{i,m},b_{i,m})\}_{1\leq i\leq n,1\leq m\leq M}\) are datapoints in a9a and w8a datasets from LIBSVM [11]. We set \(n=400\) and choose independent random-\(\lfloor d/20\rfloor\) compressors for algorithms with compressed communication. The results are as shown in Fig. 2 (middle and right). Again, we observe that ADIANA outperforms all baselines.

**Influence of independence in unbiased compression.** We also construct a delicate quadratic problem to validate the role of independence in unbiased compression to save communication, see Fig. 1. Experimental details are in Appendix F.1. We observe that ADIANA with independent random-\(s\) compressors saves more bits than Nesterov's accelerated algorithm while random-\(s\) compressors of shared randomness do not. Furthermore, more aggresive compression, _i.e._, a larger \(\omega\), saves more communication costs in total. These observations are consistent with our theories implied in (9).

## 7 Conclusion

This paper clarifies that unbiased compression alone cannot save communication, but this goal can be achieved by further assuming mutual independence between compressors. We also demonstrate the saving can be up to \(\Theta(\sqrt{\min\{n,\kappa\}})\). Future research can explore when and how much biased compressors can save communication in non-convex and stochastic scenarios.

## 8 Acknowledgment

This work is supported by NSFC Grant 12301392, 92370121, and 12288101.

Figure 2: Convergence results of various distributed algorithms on a synthetic least squares problem (left), logistic regression problems with dataset a9a (middle) and w8a (right). The \(y\)-axis represents \(f(\hat{x})-f^{\star}\) and the \(x\)-axis indicates the total communicated bits sent by per worker.

## References

* [1] A. Agarwal and L. Bottou. A lower bound for the optimization of finite sums. In _International Conference on Machine Learning_, 2015.
* [2] S. A. Alghunaim and K. Yuan. A unified and refined convergence analysis for non-convex decentralized learning. _arXiv preprint arXiv:2110.09993_, 2021.
* [3] D. Alistarh, D. Grubic, J. Li, R. Tomioka, and M. Vojnovic. QSGD: Communication-efficient SGD via gradient quantization and encoding. In _Advances in Neural Information Processing Systems_, 2017.
* [4] D. Alistarh, T. Hoefler, M. Johansson, S. Khirirat, N. Konstantinov, and C. Renggli. The convergence of sparsified gradient methods. In _Advances in Neural Information Processing Systems_, 2018.
* [5] Y. Arjevani, Y. Carmon, J. C. Duchi, D. J. Foster, N. Srebro, and B. E. Woodworth. Lower bounds for non-convex stochastic optimization. _ArXiv_, abs/1912.02365, 2019.
* [6] Y. Arjevani and O. Shamir. Communication complexity of distributed convex learning and optimization. In _Advances in Neural Information Processing Systems_, 2015.
* [7] E. Balkanski and Y. Singer. Parallelization does not accelerate convex optimization: Adaptivity lower bounds for non-smooth convex minimization. _ArXiv_, abs/1808.03880, 2018.
* [8] J. Bernstein, Y.-X. Wang, K. Azizzadenesheli, and A. Anandkumar. Signsgd: compressed optimisation for non-convex problems. In _International Conference on Machine Learning_, 2018.
* [9] Y. Carmon, J. C. Duchi, O. Hinder, and A. Sidford. Lower bounds for finding stationary points i. _Mathematical Programming_, 184(1):71-120, 2020.
* [10] Y. Carmon, J. C. Duchi, O. Hinder, and A. Sidford. Lower bounds for finding stationary points ii: first-order methods. _Mathematical Programming_, 185:315-355, 2021.
* [11] C.-C. Chang and C.-J. Lin. Libsvm: a library for support vector machines. _ACM transactions on intelligent systems and technology (TIST)_, 2(3):1-27, 2011.
* [12] J. Chen and A. H. Sayed. Diffusion adaptation strategies for distributed optimization and learning over networks. _IEEE Transactions on Signal Processing_, 60(8):4289-4305, 2012.
* [13] T. Chen, G. Giannakis, T. Sun, and W. Yin. LAG: Lazily aggregated gradient for communication-efficient distributed learning. In _Advances in Neural Information Processing Systems_, pages 5050-5060, 2018.
* [14] Z. Cheng, X. Huang, and K. Yuan. Momentum benefits non-iid federated learning simply and provably. _arXiv preprint arXiv:2306.16504_, 2023.
* [15] J. Diakonikolas and C. Guzman. Lower bounds for parallel and randomized convex optimization. In _Conference on Learning Theory_, 2019.
* [16] P. Elias. Universal codeword sets and representations of the integers. _IEEE transactions on information theory_, 21(2):194-203, 1975.
* [17] D. J. Foster, A. Sekhari, O. Shamir, N. Srebro, K. Sridharan, and B. E. Woodworth. The complexity of making the gradient small in stochastic convex optimization. In _Conference on Learning Theory_, 2019.
* [18] V. Gandikota, D. Kane, R. K. Maity, and A. Mazumdar. vagsd: Vector quantized stochastic gradient descent. In _International Conference on Artificial Intelligence and Statistics_, pages 2197-2205. PMLR, 2021.
* [19] Y. He, X. Huang, Y. Chen, W. Yin, and K. Yuan. Lower bounds and accelerated algorithms in distributed stochastic optimization with communication compression. _arXiv preprint arXiv:2305.07612_, 2023.
* [20] S. Horvath, C.-Y. Ho, L. Horvath, A. N. Sahu, M. Canini, and P. Richtarik. Natural compression for distributed deep learning. _ArXiv_, abs/1905.10988, 2019.
* [21] X. Huang, Y. Chen, W. Yin, and K. Yuan. Lower bounds and nearly optimal algorithms in distributed learning with communication compression. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2022.
* [22] X. Huang, P. Li, and X. Li. Stochastic controlled averaging for federated learning with communication compression. _arXiv preprint arXiv:2308.08165_, 2023.

* [23] X. Huang, K. Yuan, X. Mao, and W. Yin. Improved analysis and rates for variance reduction under without-replacement sampling orders. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2021.
* [24] S. P. Karimireddy, S. Kale, M. Mohri, S. Reddi, S. Stich, and A. T. Suresh. Scaffold: Stochastic controlled averaging for federated learning. In _International Conference on Machine Learning_, 2020.
* [25] S. P. Karimireddy, Q. Rebjock, S. U. Stich, and M. Jaggi. Error feedback fixes signsgd and other gradient compression schemes. In _International Conference on Machine Learning_, 2019.
* [26] A. Khaled, O. Sebbouh, N. Loizou, R. M. Gower, and P. Richtarik. Unified analysis of stochastic gradient methods for composite convex and smooth optimization. _arXiv preprint arXiv:2006.11573_, 2020.
* [27] S. Khirirat, H. R. Feyzmahdavian, and M. Johansson. Distributed learning with compressed gradients. _arXiv preprint arXiv:1806.06573_, 2018.
* [28] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. _CoRR_, abs/1412.6980, 2015.
* [29] A. Koloskova, T. Lin, and S. U. Stich. An improved analysis of gradient tracking for decentralized machine learning. In _Advances in Neural Information Processing Systems_, 2021.
* [30] A. Koloskova, N. Loizou, S. Boreiri, M. Jaggi, and S. Stich. A unified theory of decentralized sgd with changing topology and local updates. In _International Conference on Machine Learning_, pages 5381-5393. PMLR, 2020.
* [31] D. Kovalev, E. Shulgin, P. Richtarik, A. V. Rogozin, and A. Gasnikov. Adom: Accelerated decentralized optimization method for time-varying networks. In M. Meila and T. Zhang, editors, _Proceedings of the 38th International Conference on Machine Learning_, volume 139 of _Proceedings of Machine Learning Research_, pages 5784-5793. PMLR, 18-24 Jul 2021.
* [32] Z. Li, D. Kovalev, X. Qian, and P. Richtarik. Acceleration for compressed gradient descent in distributed and federated optimization. _arXiv preprint arXiv:2002.11364_, 2020.
* [33] Z. Li and P. Richtarik. Canita: Faster rates for distributed convex optimization with communication compression. _Advances in Neural Information Processing Systems_, 34:13770-13781, 2021.
* [34] X. Lian, C. Zhang, H. Zhang, C.-J. Hsieh, W. Zhang, and J. Liu. Can decentralized algorithms outperform centralized algorithms? a case study for decentralized parallel stochastic gradient descent. In _Advances in Neural Information Processing Systems_, 2017.
* [35] T. Lin, S. P. Karimireddy, S. U. Stich, and M. Jaggi. Quasi-global momentum: Accelerating decentralized deep learning on heterogeneous data. In _International Conference on Machine Learning_, 2021.
* [36] Y. Liu, W. Xu, G. Wu, Z. Tian, and Q. Ling. Communication-censored admm for decentralized consensus optimization. _IEEE Transactions on Signal Processing_, 67(10):2565-2579, 2019.
* [37] P. Mayekar and H. Tyagi. Ratq: A universal fixed-length quantizer for stochastic optimization. In _International Conference on Artificial Intelligence and Statistics_, pages 1399-1409. PMLR, 2020.
* [38] B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y Arcas. Communication-efficient learning of deep networks from decentralized data. In _Artificial Intelligence and Statistics_, 2017.
* [39] P. Micikevicius, S. Narang, J. Alben, G. Diamos, E. Elsen, D. Garcia, B. Ginsburg, M. Houston, O. Kuchaiev, G. Venkatesh, et al. Mixed precision training. In _International Conference on Learning Representations_, 2018.
* [40] K. Mishchenko, E. A. Gorbunov, M. Takac, and P. Richtarik. Distributed learning with compressed gradient differences. _ArXiv_, abs/1901.09269, 2019.
* [41] K. Mishchenko, G. Malinovsky, S. Stich, and P. Richtarik. Proxskip: Yes! local gradient steps provably lead to communication acceleration! finally! In _International Conference on Machine Learning_, pages 15750-15769. PMLR, 2022.
* [42] Y. Nesterov. A method for unconstrained convex minimization problem with the rate of convergence o(1/k2). In _Doklady an ussr_, volume 29, 1983.
* [43] Y. Nesterov. _Introductory lectures on convex optimization: A basic course_, volume 87. Springer Science & Business Media, 2003.

* [44] Y. E. Nesterov. A method of solving a convex programming problem with convergence rate \(o(1/k^{2})\). In _Doklady Akademii Nauk_, volume 269, pages 543-547. Russian Academy of Sciences, 1983.
* [45] F. Nogueira et al. Bayesian optimization: Open source constrained global optimization tool for python. 2014.
* [46] C. Philippenko and A. Dieuleveut. Bidirectional compression in heterogeneous settings for distributed or federated learning with partial participation: tight convergence guarantees. _ArXiv_, 2022.
* [47] S. Pu and A. Nedic. Distributed stochastic gradient tracking methods. _Mathematical Programming_, 187:409-457, 2021.
* [48] X. Qian, P. Richtarik, and T. Zhang. Error compensated distributed sgd can be accelerated. _Advances in Neural Information Processing Systems_, 34:30401-30413, 2021.
* [49] P. Richtarik, I. Sokolov, and I. Fatkhullin. Ef21: A new, simpler, theoretically better, and practically faster error feedback. _ArXiv_, abs/2106.05203, 2021.
* [50] M. H. Safaryan, E. Shulgin, and P. Richtarik. Uncertainty principle for communication compression in distributed and federated learning and the search for an optimal compressor. _Information and Inference: A Journal of the IMA_, 2021.
* [51] K. Scaman, F. R. Bach, S. Bubeck, Y. T. Lee, and L. Massoulie. Optimal algorithms for smooth and strongly convex distributed optimization in networks. In _International Conference on Machine Learning_, 2017.
* [52] F. Seide, H. Fu, J. Droppo, G. Li, and D. Yu. 1-bit stochastic gradient descent and its application to data-parallel distributed training of speech dnns. In _INTERSPEECH_, 2014.
* [53] B. Song, I. Tsaknakis, C.-Y. Yau, H.-T. Wai, and M. Hong. Distributed optimization for overparameterized problems: Achieving optimal dimension independent communication complexity. In _Advances in Neural Information Processing Systems_, volume 35, pages 6147-6160, 2022.
* [54] S. U. Stich. Local sgd converges fast and communicates little. In _International Conference on Learning Representations_, 2019.
* [55] S. U. Stich, J.-B. Cordonnier, and M. Jaggi. Sparsified sgd with memory. In _Advances in Neural Information Processing Systems_, 2018.
* [56] H. Tang, X. Lian, M. Yan, C. Zhang, and J. Liu. \(d^{2}\): Decentralized training over decentralized data. In _International Conference on Machine Learning_, pages 4848-4856, 2018.
* [57] J. Wangni, J. Wang, J. Liu, and T. Zhang. Gradient sparsification for communication-efficient distributed optimization. In _Advances in Neural Information Processing Systems_, 2018.
* [58] W. Wen, C. Xu, F. Yan, C. Wu, Y. Wang, Y. Chen, and H. H. Li. Terngrad: Ternary gradients to reduce communication in distributed deep learning. In _Advances in Neural Information Processing Systems_, 2017.
* [59] J. Wu, W. Huang, J. Huang, and T. Zhang. Error compensated quantized sgd and its applications to large-scale distributed optimization. In _International Conference on Machine Learning_, 2018.
* [60] R. Xin, U. A. Khan, and S. Kar. An improved convergence analysis for decentralized online stochastic non-convex optimization. _IEEE Transactions on Signal Processing_, 2020.
* [61] K. Yuan, S. A. Alghunaim, and X. Huang. Removing data heterogeneity influence enhances network topology dependence of decentralized sgd. _Journal of Machine Learning Research_, 24(280):1-53, 2023.
* [62] K. Yuan, S. A. Alghunaim, B. Ying, and A. H. Sayed. On the influence of bias-correction on distributed stochastic optimization. _IEEE Transactions on Signal Processing_, 2020.
* [63] K. Yuan, Y. Chen, X. Huang, Y. Zhang, P. Pan, Y. Xu, and W. Yin. Decentlam: Decentralized momentum sgd for large-batch deep training. In _2021 IEEE/CVF International Conference on Computer Vision (ICCV)_, pages 3009-3019, 2021.
* [64] K. Yuan, X. Huang, Y. Chen, X. Zhang, Y. Zhang, and P. Pan. Revisiting optimal convergence rate for smooth and non-convex stochastic decentralized optimization. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2022.
* [65] M. D. Zeiler. Adadelta: An adaptive learning rate method. _ArXiv_, abs/1212.5701, 2012.

Random sparsification

We illustrate the random-\(s\) sparsification here. More examples of unbiased compressors can be found in literature [50].

**Example 1** (Random-\(s\) sparsification).: _For any \(x\in\mathbb{R}^{d}\), the random-\(s\) sparsification is defined by \(C(x):=\frac{d}{s}(\xi\odot x)\) where \(\odot\) denotes the entry-wise product and \(\xi\in\{0,1\}^{d}\) is a uniformly random binary vector with \(s\) non-zero entries. This random-\(s\) sparsification operator \(C\) satisfies Assumption 2 with \(\omega=d/s-1\). When each entry of the input \(x\) is represented with \(r\) bits, random-\(s\) sparsification compressor takes \(rs\) bits to transmit \(s\) entries and \(\log_{2}\binom{d}{s}\) bits to transmit the indices of \(s\) transmitted entries, resulting in a total \(\frac{rd}{1+\omega}+\log_{2}\binom{d}{s}\) bits in each communication round, see [50, Table 1]._

## Appendix B Proof of Proposition 1

We first recall a result proved by [50].

**Lemma 2** ([50], Theorem 2).: _Let \(C:\mathbb{R}^{d}\rightarrow\mathbb{R}^{d}\) be any unbiased compressors satisfying 2 and \(b\) be the total number of bits needed to encode the compressed vector \(C(x)\) for any \(x\in\mathbb{R}^{d}\). If each entry of the input \(x\) is represented with \(r\) bits, it holds that \(\max\{\frac{\omega}{1+\omega},4^{-r}\}4^{b/d}\geq 1\)._

Using Lemma 2, when \(\omega/(1+\omega)\leq 4^{-r}\), _i.e._, \(\omega\leq(4^{r}-1)^{-1}\leq 1/3\), we have \((1+\omega)=\Theta(1)\) and \(b\geq rd=\Omega_{r}(d/(1+\omega))\), where \(r\) is regarded as a constant in \(\Omega_{r}(\cdot)\). When \(\omega/(1+\omega)\geq 4^{-r}\), we have

\[b\geq d\log_{4}(1+\omega^{-1})=d\ln(1+\omega^{-1})/\ln(4)\geq d\frac{\omega^{- 1}}{\ln(4)(1+\omega^{-1})}=\Omega_{r}\left(\frac{d}{1+\omega}\right),\]

where we use the inequality \(\ln(1+t)\geq t/(1+t)\) with \(t=\omega^{-1}\geq 0\).

## Appendix C Proof of Theorem 2

Following [5, 9], we denote the \(k\)-th coordinate of a vector \(x\in\mathbb{R}^{d}\) by \([x]_{k}\) for \(k=1,\ldots,d\), and let \(\operatorname{prog}(x)\) be

\[\operatorname{prog}(x):=\begin{cases}0,&\text{if }x=0,\\ \max_{1\leq k\leq d}\{k:[x]_{k}\neq 0\},&\text{otherwise}.\end{cases}\]

Similarly, for a set of multiple points \(\mathcal{X}=\{x_{1},x_{2},\ldots\}\), we define \(\operatorname{prog}(\mathcal{X}):=\max_{x\in\mathcal{X}}\operatorname{prog}(x)\). We call a function \(f\) zero-chain if it satisfies

\[\operatorname{prog}(\nabla f(x))\leq\operatorname{prog}(x)+1,\quad\forall\,x \in\mathbb{R}^{d},\]

which implies that starting from \(x^{0}=0\), a single gradient evaluation can only earn at most one more non-zero coordinate for the model parameters.

Let us now illustrate the setup of distributed optimization with communication compression. For any \(t\geq 1\), we consider the \(t\)-th communication round, which begins with the server broadcasting a vector denoted as \(u^{t}\) to all workers. We initialize \(u^{1}\) as \(x^{0}\). Upon receiving the vector \(u^{t}\) from the server, each worker performs necessary algorithmic operations, and the round concludes with each worker sending a compressed message back to the server.

We denote \(v^{t}_{i}\) as the vector that worker \(i\) aims to send in the \(t\)-th communication round before compression, and \(\hat{v}^{t}_{i}\) as the compressed vector that will be received by the server, _i.e._, \(\hat{v}^{t}_{i}=C_{i}(v^{t}_{i})\). While we require communication to be synchronous among workers, we do not impose restrictions on the number of gradient queries made by each worker within a communication round. We use \(\mathcal{Y}^{t}_{i}\) to represent the set of vectors at which worker \(i\) makes gradient queries in the \(t\)-th communication round, after receiving \(u^{t}\) but before sending \(\hat{v}^{t}_{i}\).

Following the above description, we now formally state the linear spanning property in the setting of centralized distributed optimization with communication compression.

**Definition 2** (Linear-spanning algorithms).: _We say a distributed algorithm \(A\) is linear-spanning if, for any \(t\geq 1\), the following conditions hold:_1. _The server can only send a vector in the linear manifold spanned by all the past received messages, sent messages,_ i.e._,_ \(u^{t}\in\operatorname{span}\left(\{u^{t}\}_{r=1}^{t-1}\cup\{\hat{v}_{i}^{r}:1\leq i \leq n\}_{r=1}^{t-1}\right)\)_._
2. _Worker_ \(i\) _can only query at vectors in the linear manifold spanned by its past received messages, compressed messages, and gradient queries,_ i.e._,_ \(\mathcal{Y}_{i}^{t}\subseteq\operatorname{span}\left(\{u^{r}\}_{r=1}^{t}\cup \{\nabla f_{i}(y):y\in\mathcal{Y}_{i}^{r}\}_{r=1}^{t-1}\cup\{\hat{v}_{i}^{r}\} _{r=1}^{t-1}\right)\)_._
3. _Worker_ \(i\) _can only send a vector in the linear manifold spanned by its past received messages, compressed messages, and local gradient queries,_ i.e._,_ \(v_{i}^{t}\in\operatorname{span}\left(\{u^{r}\}_{r=1}^{t}\cup\{\nabla f_{i}(y): y\in\mathcal{Y}_{i}^{r}\}_{r=1}^{t}\cup\{\hat{v}_{i}^{r}\}_{r=1}^{t-1}\right)\)_._
4. _After_ \(t\) _communication rounds, the server can only output a model in the linear manifold spanned by all the past received messages, sent messages,_ i.e._,_ \(\hat{x}^{t}\in\operatorname{span}\left(\{u^{r}\}_{r=1}^{t}\cup\{\hat{v}_{i}^{r }:1\leq i\leq n\}_{r=1}^{t}\right)\)_._

In essence, when starting from \(x^{0}=0\), the above linear-spanning property requires that any expansion of non-zero coordinates in vectors held by worker \(i\) (_e._g._, \(\mathcal{Y}_{i}^{t},v_{i}^{t}\)) are attributed to its past local gradient updates, local compression, or synchronization with the server. Meanwhile, it also requires that any expansion of non-zero coordinate in vectors held, including the final algorithmic output, in the server is due to the received compressed messages from workers.

Without loss of generality, we assume algorithms to start from \(x^{0}=0\) throughout the proofs. When \(\{f_{i}\}_{i=1}^{n}\) are further assumed to be zero-chain, following Definition 2, one can easily establish by induction that for any \(t\geq 1\),

\[\max_{1\leq r\leq t}\operatorname{prog}(u^{r}) \leq\max_{1\leq r<t}\max_{1\leq i\leq n}\operatorname{prog}(\hat{ v}_{i}^{r}) \tag{10}\] \[\max_{1\leq r\leq t}\operatorname{prog}(v_{i}^{t}) \leq\max_{1\leq r<t}\max\left\{\max_{1\leq i\leq n}\operatorname{ prog}(\hat{v}_{i}^{r}),\operatorname{prog}(\mathcal{Y}_{i}^{r})\right\}\leq\max_{1 \leq r<t}\max_{1\leq i\leq n}\operatorname{prog}(\hat{v}_{i}^{r})+1\] \[\operatorname{prog}(\hat{x}^{t}) \leq\max_{1\leq r\leq t}\max_{1\leq i\leq n}\operatorname{prog}( \hat{v}_{i}^{r})\]

Next, we outline the proofs for the lower bounds presented in Theorem 2. For each case, we provide separate proofs for terms in the lower bound by constructing different hard-to-optimize examples, respectively. The construction of these proofs follows four steps:

* Constructing a set of zero-chain local functions \(\{f_{i}\}_{i=1}^{n}\).
* Constructing a set of independent unbiased compressors \(\{C_{i}\}_{i=1}^{n}\subseteq\mathcal{U}_{\omega}^{\mathrm{ind}}\). These compressors are delicately designed to impede algorithms from expanding the non-zero coordinates of model parameters.
* Establishing a limitation on zero-respecting algorithms that utilize the predefined compressor with \(t\) rounds of compressed communication on each worker. This limitation is based on the non-zero coordinates of model parameters.
* Translating the above limitation into the lower bound of the complexity measure defined in equation (3).

While the overall proof structure is similar to that of [19], our novel construction of functions and compressors enable us to derive lower bounds for independent compressors. These lower bounds clarify the unique properties and benefits of independent compressors.

We will use the following lemma in the analysis of the third step.

**Lemma 3** ([19], Lemma 3).: _Given a constant \(p\in[0,1]\) and random variables \(\{B^{t}\}_{t=0}^{\infty}\) such that \(B^{t}\leq B^{(t-1)}+1\) and \(\mathbb{P}(B^{t}\leq B^{t-1}\mid\{B^{r}\}_{r=0}^{t-1})\geq 1-p\) for any \(t\geq 1\), it holds for \(t\geq 1/p\), with probability at least \(1-e^{-1}\), that \(B^{t}\leq B^{0}+\text{ept}\)._

### Strongly-convex case

Below, we present two examples, each of which corresponding to a lower bound \(LB_{m}\) for \(T_{\epsilon}\). We integrate the two lower bounds together and use the inequality

\[T_{\epsilon}\geq\max_{1\leq m\leq 2}\{LB_{m}\}=\Omega\left(LB_{1}+LB_{2}\right)\]

[MISSING_PAGE_FAIL:16]

Therefore, for any given \(\Delta>0\), letting \(\lambda=\sqrt{((1-q^{2})\Delta)/q^{2}}\) results in \(\|x^{0}-x^{\star}\|^{2}=\Delta\). Consequently, our construction ensures \(\{f_{i}\}_{i=1}^{n}\in\mathcal{F}_{L,\mu}^{\Delta}\).

(Step 2.) For the construction of \(\omega\)-unbiased compressors, we consider \(\{C_{i}\}_{i=1}^{n}\) to be independent random sparsification compressors. Building upon Example 1, we make a slight modification: during a round of communication on any worker, each coordinate is independetly chosen with a probability of \((1+\omega)^{-1}\) to be transmitted, and if selected, its value is scaled by \((1+\omega)\) and then the scaled value is transmitted. Notably, the indices of chosen coordinates are not identical across all workers due to the independence of compressors. It can be easily verified that this construction ensures that \(\{C_{i}\}_{i=1}^{n}\subseteq\mathcal{U}_{\omega}^{\text{ind}}\).

(Step 3.) Since the algorithmic output \(\hat{x}^{t}\) calculated by the server lies in the linear manifold spanned by received messages, we can use (10) to obtain the following expression:

\[\operatorname{prog}(\hat{x}^{t})\leq\max_{1\leq r\leq t}\max_{1\leq i\leq n} \max\{\operatorname{prog}(u^{r}),\operatorname{prog}(\hat{v}^{r}_{i})\}=\max_ {1\leq r\leq t}\max_{1\leq i\leq n}\operatorname{prog}(\hat{v}^{r}_{i})\triangleq B ^{t}. \tag{13}\]

We next bound \(B^{t}\) with \(B^{0}:=0\) by showing that \(\{B^{t}\}_{t=0}^{\infty}\) satisfies Lemma 3 with \(p=(1+\omega)^{-1}\).

For any linear-spanning algorithm \(A\), according to (11), the worker \(i\) can only attain one additional non-zero coordinate through local gradient-based updates when \(\operatorname{prog}(\mathcal{Y}_{i}^{t})\equiv i\mod n\). In other words, upon receiving messages \(\{u^{r}_{i}\}_{t=1}^{t}\) from the server, we have

\[\operatorname{prog}(v^{t}_{i})\leq\begin{cases}\max_{1\leq r\leq t} \operatorname{prog}(u^{r}_{i})+1\leq B^{t-1}+1,&\text{if }\operatorname{prog}( \mathcal{Y}_{i}^{t})\equiv i\mod n,\\ \max_{1\leq r\leq t}\operatorname{prog}(u^{r}_{i})\leq B^{t-1},&\text{otherwise.}\end{cases}\]

Consequently, we have

\[\max_{1\leq r\leq t}\operatorname{prog}(v^{r}_{i})\leq\max_{1\leq r\leq t}B^{ r-1}+1=B^{t-1}+1.\]

It then follows from the definition of the constructed \(C_{i}\) in Step 2 that \(\max_{1\leq i\leq n}\operatorname{prog}(\hat{v}^{t}_{i})\leq\max_{1\leq i\leq n }\operatorname{prog}(v^{t}_{i})\), and therefore we have:

\[B^{t}\leq\max_{1\leq r\leq t}\max_{1\leq i\leq n}\operatorname{prog}(v^{r}_{i })\leq B^{t-1}+1.\]

Next, we aim to prove that \(B^{t}\leq B^{t-1}+1\) with a probability of at least \(\omega/(1+\omega)\). For any \(t\geq 1\), let \(i\in\{1,\dots,n\}\) be such that \(B^{t-1}\equiv i\mod n\). Due to the property in equation (11), during the \(t\)-th communication round, if \(\operatorname{prog}(\mathcal{Y}_{i}^{t})=B^{t-1}\), worker \(i\) can push the number of non-zero entries forward by \(1\), resulting in \(\operatorname{prog}(v^{t}_{i})=B^{t-1}+1\), using local gradient updates. Note that any other worker \(j\) cannot achieve this even if \(\operatorname{prog}(\mathcal{Y}_{j}^{t})=B^{t-1}\) due to equation (11).

Therefore, to achieve \(B^{t}=B^{t-1}+1\), it is necessary for worker \(i\) to transmit a non-zero value at the \((B^{t-1}+1)\)-th entry to the server. Otherwise, we have \(B^{t}\leq B^{t-1}\). However, since the compressor \(C_{i}\) associated with worker \(i\) has a probability \(\omega/(1+\omega)\) to zero out the \((B^{t-1}+1)\)-th entry in the \(t\)-th communication round, we have

\[\mathbb{P}\left(B^{t}\leq B^{t-1}\mid\{B^{r}\}_{r=0}^{t-1}\right)\geq\omega/(1+ \omega).\]

In summary, we have shown that \(B^{t}\leq B^{t-1}+1\) and \(\mathbb{P}(B^{t}\leq B^{t-1}\mid\{B^{r}\}_{r=0}^{t-1})\geq\omega/(1+\omega)\).

By applying Lemma 3, we can conclude that for any \(t\geq(1+\omega)^{-1}\), with a probability of at least \(1-e^{-1}\), it holds that \(B^{t}\leq et/(1+\omega)\) and hence \(\operatorname{prog}(\hat{x}^{t})\leq et/(1+\omega)\) due to (13).

(Step 4.) Using Lemma 4 and that \(\operatorname{prog}(\hat{x}^{t})\leq et/(1+\omega)\) with probability at least \(1-e^{-1}\), we obtain

\[\begin{split}\mathbb{E}[f(\hat{x}^{t})]-\min_{x}f(x)\geq& \frac{(1-e^{-1})\mu\Delta}{2}\left(1-2\left(1+\sqrt{1+\frac{2( \kappa-1)}{n}}\right)^{-1}\right)^{2et/(1+\omega)}\\ =&\Omega\left(\mu\Delta\exp\left(-\frac{4et}{(\sqrt {\kappa/n}+1)(1+\omega)}\right)\right).\end{split} \tag{14}\]Therefore, to ensure \(\mathbb{E}[f(\hat{x}^{t})]-\min_{x}f(x)\leq\epsilon\), relation (14) implies the lower bound \(T_{\epsilon}=\Omega((1+\omega)(1+\sqrt{\kappa/n})\ln(\mu\Delta/\epsilon))\).

**Example 2**.: Considering \(f_{1}=f\) to be homogeneous and \(C_{i}=I\) to be a loss-less compressor for all \(1\leq i\leq n\), the problem reduces to single-node convex optimization. In this case, the lower bound of \(\Omega(\sqrt{\kappa}\ln\left(\mu\Delta/\epsilon\right))\) is well-known in the literature, as shown in [44, 43].

With the two lower bounds achieved in Examples 1 and 2, we have

\[T_{\epsilon} =\Omega\Big{(}(1+\omega)(1+\sqrt{\kappa/n})\ln(\mu\Delta/\epsilon )+\sqrt{\kappa}\ln(\mu\Delta/\epsilon)\Big{)}\] \[=\Omega\Big{(}(1+\omega+\sqrt{\kappa/n}+\omega\sqrt{\kappa/n}+ \sqrt{\kappa})\ln(\mu\Delta/\epsilon)\Big{)}\] \[=\Omega\Big{(}(\omega+\omega\sqrt{\kappa/n}+\sqrt{\kappa})\ln(\mu \Delta/\epsilon)\Big{)}\]

which is the result for the strongly-convex case in Theorem 2.

### Generally-convex case

Below, we present three examples, each of which corresponding to a lower bound \(LB_{m}\) for \(T_{\epsilon}\). We integrate the three lower bounds together and use the inequality

\[T_{\epsilon}\geq\max_{1\leq m\leq 3}\{LB_{m}\}=\Omega\left(LB_{1}+LB_{2}+LB_{3}\right)\]

to accomplish the lower bound for the generally-convex case in Theorem 2.

**Example 1**.: In this example, we prove the lower bound \(\Omega((1+\omega)(L\Delta/\epsilon)^{1/2})\).

(Step 1.) We assume variable \(x\in\mathbb{R}^{d}\), where \(d\) can be sufficiently large and will be determined later. Let \(M\) denote

\[M=\left[\begin{array}{ccccc}2&-1&&&\\ -1&2&-1&&\\ &\ddots&\ddots&\ddots&\\ &&-1&2&-1\\ &&&-1&2\end{array}\right]\in\mathbb{R}^{d\times d},\]

it is easy to verify \(0\preceq M\preceq 4I\). Similar to example 1 of the strongly-convex case, we consider

\[f_{i}(x)=\left\{\begin{array}{ll}\frac{L}{4}\sum_{r\geq 0}([x]_{nr+i}-[x]_{nr +i+1})^{2},&\text{if }1\leq i\leq n-1,\\ \frac{L}{4}\left([x]_{1}^{2}+\sum_{r\geq 1}([x]_{nr}-[x]_{nr+1})^{2}-2\lambda[x]_ {1}\right),&\text{if }i=n.\end{array}\right.\]

where \(\lambda\in\mathbb{R}\backslash\{0\}\) is to be specified. It is easy to see that all \(f_{i}\)s are \(L\)-smooth. We further have \(f(x)=\frac{1}{n}\sum_{i=1}^{n}f_{i}(x)=\frac{L}{4n}\left(x^{\top}Mx-2\lambda[x ]_{1}\right)\). The \(f_{i}\) functions defined above are also zero-chain functions satisfying (11).

Following [43], it is easy to verify that the optimum of \(f\) satisfies

\[x^{\star}=\left(\lambda\left(1-\frac{k}{d+1}\right)\right)_{1\leq k\leq d} \quad\text{and}\quad f(x^{\star})=\min_{x}f(x)=-\frac{\lambda^{2}Ld}{4n(d+1)}.\]

More generally, it holds for any \(0\leq k\leq d\) that

\[\min_{x:\,\text{pro}(x)\leq k}f(x)=-\frac{\lambda^{2}Lk}{4n(k+1)}. \tag{15}\]

Since \(\|x^{0}-x^{\star}\|^{2}=\frac{\lambda^{2}}{(d+1)^{2}}\sum_{k=1}^{d}k^{2}=\frac {\lambda^{2}d(2d+1)}{6(d+1)}\leq\frac{\lambda^{2}d}{3}\), letting \(\lambda=\sqrt{3\Delta/d}\), we have \(\{f_{i}\}_{i=1}^{n}\in\mathcal{F}_{L,0}^{\Delta}\).

(Step 2.) Same as Step 2 of Example 1 of the strongly-convex case, we consider \(\{C_{i}\}_{i=1}^{n}\) to be independent random sparsification operators.

(Step 3.) Following the same argument as step 3 of example 1 of the strongly-convex case, we have that for any \(t\geq(1+\omega)^{-1}\), it holds with probability at least \(1-e^{-1}\) that \(\operatorname{prog}(\hat{x}^{t})\leq et/(1+\omega)\).

(Step 4.) Thus, combining (15), we have

\[\mathbb{E}[f(\hat{x}^{t})]-\min_{x}f(x)\geq (1-e^{-1})\frac{\lambda^{2}L}{4n}\left(\frac{d}{d+1}-\frac{et/(1+ \omega)}{1+et/(1+\omega)}\right)\] \[= (1-e^{-1})\frac{3L\Delta}{4nd}\left(\frac{d}{d+1}-\frac{et/(1+ \omega)}{1+et/(1+\omega)}\right)\]

Letting \(d=1+et/(1+\omega)\), we further have

\[\mathbb{E}[f(\hat{x}^{t})]-\min_{x}f(x)\geq\frac{3(1-e^{-1})L\Delta}{8net(1+ \omega)^{-1}(1+2et(1+\omega)^{-1})}=\Omega\left(\frac{(1+\omega)^{2}L\Delta}{ nt^{2}}\right).\]

Therefore, to ensure \(\mathbb{E}[f(\hat{x}^{t})]-\min_{x}f(x)\leq\epsilon\), the above inequality implies the lower bound to be \(T=\Omega((1+\omega)(L\Delta/(n\epsilon))^{\frac{1}{2}})\).

**Example 2**.: Considering \(f_{1}=f\) to be homogeneous and \(C_{i}=I\) to be a loss-less compressor for all \(1\leq i\leq n\). The problem reduces to the single-node convex optimization. The lower bound \(\Omega(\sqrt{L\Delta/\epsilon})\) is well-known in literature, see, \(e\)., [44, 43].

**Example 3**.: In this example, we prove the lower bound \(\Omega(\omega\ln{(L\Delta/\epsilon)})\).

(Step 1.) We consider \(f_{1}=\cdots=f_{n-1}=L\|x\|^{2}/2\) and \(f_{n}=L\|x\|^{2}/2+n\lambda\langle\mathds{1}_{d},x\rangle\) where \(\mathds{1}_{d}\in\mathbb{R}^{d}\) is the vector with all entries being \(1\) and \(\lambda\in\mathbb{R}\) is to be determined. By definition, \(\{f_{i}\}_{i=1}^{n}\) are \(\mu\)-strongly-convex and \(L\)-smooth and the solution \(x^{\star}=-\frac{\lambda}{L}\mathds{1}_{d}\). Letting \(\lambda=L\sqrt{\Delta}/\sqrt{n}\), we have \(\|x^{\star}-x^{0}\|^{2}=\Delta\). Thus, the construction ensures \(\{f_{i}\}_{i=1}^{n}\in\mathcal{F}_{L,\mu}^{\Delta}\).

(Step 2.) Same as in Example 1, we consider \(\{C_{i}\}_{i=1}^{n}\) to be independent random sparsification operators.

(Step 3.) By the construction of \(\{f_{i}\}_{i=1}^{n}\), we observe that the optimization process relies solely on transmitting the information of \(\mathds{1}_{d}\) from worker \(n\) to the server. Let \(E^{t}\) denote the set of entries at which the server has received a non-zero value from worker \(n\) in the first \(t\) communication rounds. Note that for each entry, due to the construction of \(\{C_{i}\}_{i=1}^{n}\), the server has a probability of at least \((\omega/(1+\omega))^{t}\) of not receiving a non-zero value at that entry from worker \(n\). Consequently, \(|(E^{t})^{c}|\) is lower bounded by the sum of \(n\) independent \(\operatorname{Bernoulli}(\omega^{t}/(1+\omega)^{t})\) random variables. Therefore, we have \(\mathbb{E}[|(E^{t})^{c}|]\geq d\omega^{t}/(1+\omega)^{t}\).

(Step 4.) Given \(|E^{t}|\), due to the linear-spanning property, we have \(\hat{x}^{t}\in\operatorname{span}\{e_{j}:j\in E^{t}\}\) where \(e_{j}\) is the \(j\)-th canonical vector. As a result, we have

\[\mathbb{E}[f(\hat{x}^{t})]-\min_{x}f(x)\] \[\geq \mathbb{E}[\min_{x\in\operatorname{span}\{e_{j}:j\in E^{t}\}}f(x )]-\min_{x}f(x)=\frac{L\Delta}{2}\frac{\mathbb{E}[|(E^{t})^{c}|]}{d}\geq\frac {L\Delta}{2}\frac{\omega^{t}}{(1+\omega)^{t}}. \tag{16}\]

Therefore, to ensure \(\mathbb{E}[f(\hat{x}^{t})]-\min_{x}f(x)\leq\epsilon\), (16) implies the lower bound \(T_{\epsilon}=\Omega(\omega\ln(L\Delta/\epsilon))\).

With the three lower bounds achieved in Examples 1, 2, and 3, we have

\[T_{\epsilon} =\Omega\Big{(}\sqrt{\frac{L\Delta}{\epsilon}}+(1+\omega)\sqrt{ \frac{L\Delta}{n\epsilon}}+\omega\ln(L\Delta/\epsilon)\Big{)}\] \[=\Omega\Big{(}\sqrt{\frac{L\Delta}{\epsilon}}+\omega\sqrt{\frac {L\Delta}{n\epsilon}}+\omega\ln(L\Delta/\epsilon)\Big{)}\]

which is the result for the generally-convex case in Theorem 2.

[MISSING_PAGE_FAIL:20]

By \(L\)-smoothness and \(\mu\)-strongly convexity, we have for \(\forall u\in\mathbb{R}^{d}\) that

\[f(u)\geq f(x^{k})+\langle\nabla f(x^{k}),u-x^{k}\rangle+\frac{\mu}{2}\|u-x^{k}\|^ {2},\]

and that

\[f_{i}(u)\geq f_{i}(x^{k})+\langle\nabla f_{i}(x^{k}),u-x^{k}\rangle+\frac{1}{2L }\|\nabla f_{i}(u)-\nabla f_{i}(x^{k})\|^{2},\]

thus we obtain for \(\forall u\in\mathbb{R}^{d}\),

\[f(x^{k})\leq f(u)-\langle\nabla f(x^{k}),u-x^{k}\rangle\] \[-\max\left\{\frac{\mu}{2}\|u-x^{k}\|^{2},\frac{1}{2Ln}\sum_{i=1}^ {n}\|\nabla f_{i}(u)-\nabla f_{i}(x^{k})\|^{2}\right\}. \tag{22}\]

Applying Young's inequality to (21) and using \(\eta_{k}\leq 1/(2L)\), we reach

\[f(y^{k+1})\leq f(x^{k})+\frac{\eta_{k}}{2}\mathcal{G}^{k}-\frac{\eta_{k}}{2}(1- L\eta_{k})\|g^{k}\|^{2}\] \[\leq f(x^{k})+\frac{\eta_{k}}{2}\mathcal{G}^{k}-\frac{\eta_{k}}{4}\|g ^{k}\|^{2}. \tag{23}\]

Adding (17) in Lemma 5 to \(\left(\frac{2\gamma_{k}\beta}{\theta_{1,k}}+2\gamma_{k}(1-\beta)\right)\times\)(23) + \(2\gamma_{k}\times\)(22) (where \(u=x^{\star}\)) + \(\frac{2\gamma_{k}\beta\theta_{2}}{\theta_{1,k}}\times\)(22) (where \(u=w^{k}\)) + \(\frac{2\gamma_{k}\beta(1-\theta_{1,k}-\theta_{2})}{\theta_{1,k}}\times\)(22) (where \(u=y^{k}\)) and using the unbiasedness of \(g^{k}\), we obtain

\[\frac{2\gamma_{k}\beta}{\theta_{1,k}}\mathbb{E}_{k}[\mathcal{Y}^{ k+1}]+\mathbb{E}_{k}[\mathcal{Z}^{k+1}]\] \[\leq \beta\mathcal{Z}^{k}+(1-\beta-\mu\gamma_{k})\|x^{k}-x^{\star}\|^ {2}+\left(\gamma_{k}^{2}-\frac{\eta_{k}\gamma_{k}\beta}{2\theta_{1,k}}\right) \mathbb{E}_{k}[\|g^{k}\|^{2}]+\eta_{k}\left(\frac{\gamma_{k}\beta}{\theta_{1,k }}+\gamma_{k}(1-\beta)\right)\mathcal{G}^{k}\] \[-\frac{\gamma_{k}\beta\theta_{2}}{L\theta_{1,k}}\mathcal{G}^{k}_{ w}-\frac{\gamma_{k}\beta(1-\theta_{1,k}-\theta_{2})}{L\theta_{1,k}}\mathcal{G}^{k}_{ y}+\frac{2\gamma_{k}\beta\theta_{2}}{\theta_{1,k}}\mathcal{W}^{k}+\frac{2\gamma_{k} \beta(1-\theta_{1,k}-\theta_{2})}{\theta_{1,k}}\mathcal{Y}^{k}\] \[-2\gamma_{k}(1-\beta)\mathbb{E}_{k}[\mathcal{Y}^{k+1}]-\frac{\eta _{k}\gamma_{k}(1-\beta)}{2}\mathbb{E}_{k}[\|g^{k}\|^{2}]\]

On top of that, by applying our choice of the parameters, it can be easily verified that \(1-\beta-\mu\gamma_{k}=0\), \(\gamma_{k}^{2}-\frac{\eta_{k}\gamma_{k}\beta}{2\theta_{1,k}}=0\), \(1-\beta\leq\frac{\beta}{4\theta_{1,k}}\), which leads to (20). 

**Lemma 7** ([32], Lemma 3, 4, 5).: _Under Assumptions 1, 2, and 3, the iterates of Algorithm 1 satisfy the following inequalities:_

\[\mathbb{E}[\mathcal{W}^{k+1}]= (1-p)\mathbb{E}[\mathcal{W}^{k}]+p\mathbb{E}[\mathcal{Y}^{k}], \tag{24}\] \[\mathbb{E}[\mathcal{G}^{k}]\leq \frac{2\omega}{n}\mathbb{E}[\mathcal{G}^{k}_{w}]+\frac{2\omega}{n }\mathbb{E}[\mathcal{H}^{k}],\] (25) \[\mathbb{E}[\mathcal{H}^{k+1}]\leq \left(1-\frac{\alpha}{2}\right)\mathbb{E}[\mathcal{H}^{k}]+2p \left(1+\frac{2p}{\alpha}\right)(\mathbb{E}[\mathcal{G}^{k}_{w}]+\mathbb{E}[ \mathcal{G}^{k}_{y}]). \tag{26}\]

Now we define a Lyapunov function \(\Psi^{k}\) for \(k\geq 1\) as

\[\Psi^{k}=\lambda_{k-1}\mathcal{W}^{k}+\frac{2\gamma_{k-1}\beta}{\theta_{1,k-1}} \mathcal{Y}^{k}+\mathcal{Z}^{k}+\frac{10\eta_{k-1}\omega(1+\omega)\gamma_{k-1} \beta}{\theta_{1,k-1}n}\mathcal{H}^{k},\quad\forall k\geq 1, \tag{27}\]

where \(\lambda_{k}=\frac{\gamma_{k}\beta}{p\theta_{1,k}}(\theta_{1,k}+\theta_{2}-p+ \sqrt{(p-\theta_{1,k}-\theta_{2})^{2}+4p\theta_{2}})\). Furthermore, it is straightforward to verify that

\[\frac{2\gamma_{k}\beta\theta_{2}}{p\theta_{1,k}}\leq\lambda_{k}\leq\frac{2 \gamma_{k}\beta(\theta_{1,k}+\theta_{2})}{p\theta_{1,k}}.\]

Now we restate the convergence result in the strongly-convex case in Theorem 3 and prove it using Lemma 6, 7 and the Lyapunov function.

**Theorem 4**.: _If \(\mu>0\) and parameters satisfy \(\eta_{k}\equiv\eta=n\theta_{2}/(120\omega L)\), \(\theta_{1,k}\equiv\theta_{1}=1/(3\sqrt{\kappa})\), \(\alpha=p=1/(1+\omega)\), \(\gamma_{k}\equiv\gamma=\eta/(2\theta_{1}+\eta\mu)\), \(\beta=2\theta_{1}/(2\theta_{1}+\eta\mu)\), and \(\theta_{2}=1/(3\sqrt{n}+3n/\omega)\), then the number of communication rounds performed by ADIANA to find an \(\epsilon\)-accurate solution such that \(\mathbb{E}[f(\hat{x})]-\min_{x}f(x)\leq\epsilon\) is at most \(\mathcal{O}((\omega+(1+\omega/\sqrt{n})\sqrt{\kappa})\ln(L\Delta/\epsilon))\)._

Proof.: In the strongly-convex case, parameters \(\{\gamma_{k}\}_{k\geq 1}\) and \(\{\theta_{1,k}\}_{k\geq 1}\) are constants, then so is \(\lambda_{k}\). Thus, we simply write \(\gamma\triangleq\gamma_{k}\), \(\theta_{1}\triangleq\theta_{1,k}\), and \(\lambda\triangleq\lambda_{k}\) for all \(k\geq 1\). Considering (20)+\(\lambda(24)+\frac{5\gamma\beta\eta}{4\theta_{1}}(25)+\frac{10\eta\omega(1+ \omega)\gamma\beta}{n\theta_{1}}(26)\), we have

\[\mathbb{E}[\Psi^{k+1}]\leq \left(\frac{2\gamma\beta\theta_{2}}{\theta_{1}}+(1-p)\lambda \right)\mathcal{W}^{k}+\left(\frac{2\gamma\beta(1-\theta_{1}-\theta_{2})}{ \theta_{1}}+p\lambda\right)\mathcal{Y}^{k}+\beta\mathcal{Z}^{k}\] \[+\left(1-\frac{1}{4(1+\omega)}\right)\frac{10\eta\omega(1+\omega )\gamma\beta}{\theta_{1}n}\mathcal{H}^{k}-\left(\frac{\gamma\beta\theta_{2}}{ L\theta_{1}}-\frac{125\gamma\beta\eta\omega}{2n\theta_{1}}\right)\mathcal{G}^{k}_{w}\] \[-\left(\frac{\gamma\beta(1-\theta_{1}-\theta_{2})}{L\theta_{1}}- \frac{60\eta\omega\gamma\beta}{n\theta_{1}}\right)\mathcal{G}^{k}_{y}. \tag{28}\]

By the definition of \(\lambda\), we have

\[\frac{2\gamma\beta\theta_{2}}{\theta_{1}}+(1-p)\lambda= \lambda\left(1-p+\frac{2p\theta_{2}}{\sqrt{(p-\theta_{1}-\theta_{ 2})^{2}+4p\theta_{2}}+\theta_{1}+\theta_{2}-p}\right)\] \[= \lambda\left(1-p+\frac{2p\theta_{2}}{2\theta_{2}+\frac{4\theta_{ 1}\theta_{2}}{\sqrt{(p-\theta_{1}-\theta_{2})^{2}+4p\theta_{2}-\theta_{1}+ \theta_{2}+p}}}\right)\] \[\leq \lambda\left(1-p+\frac{p}{1+\frac{2\theta_{1}}{(p+\theta_{1}+ \theta_{2})-\theta_{1}+\theta_{2}+p}}\right)=\left(1-\frac{p\theta_{1}}{p+ \theta_{1}+\theta_{2}}\right)\lambda, \tag{29}\]

and

\[\frac{2\gamma\beta(1-\theta_{1}-\theta_{2})}{\theta_{1}}+p\lambda= \frac{2\gamma\beta}{\theta_{1}}\left[1-\theta_{1}-\theta_{2}+ \frac{1}{2}\left(\theta_{1}+\theta_{2}-p+\sqrt{(p-\theta_{1}-\theta_{2})^{2}+ 4p\theta_{2}}\right)\right]\] \[= \frac{2\gamma\beta}{\theta_{1}}\left(1-\frac{2p\theta_{1}}{p+ \theta_{1}+\theta_{2}+\sqrt{(p-\theta_{1}-\theta_{2})^{2}+4p\theta_{2}}}\right)\] \[\leq \left(1-\frac{p\theta_{1}}{p+\theta_{1}+\theta_{2}}\right)\frac{ 2\gamma\beta}{\theta_{1}}. \tag{30}\]

From the choice of \(\eta\), it is easy to verify that

\[\frac{\gamma\beta\theta_{2}}{L\theta_{1}}-\frac{5\gamma\beta\eta\omega}{2n \theta_{1}}-\frac{60\eta\omega\gamma\beta}{n\theta_{1}}\geq 0, \tag{31}\]

and further noting \(1-\theta_{1}-\theta_{2}\geq\theta_{2}\),

\[\frac{\gamma\beta(1-\theta_{1}-\theta_{2})}{L\theta_{1}}-\frac{60\eta\omega \gamma\beta}{n\theta_{1}}\geq 0. \tag{32}\]

Plugging (29), (30), (31), and (32) into (28), we obtain

\[\mathbb{E}[\Psi^{k+1}]\leq \left(1-\min\left\{\frac{p\theta_{1}}{p+\theta_{1}+\theta_{2}}, \frac{\eta\mu}{2\theta_{1}+\eta\mu},\frac{1}{4(1+\omega)}\right\}\right)\Psi^{k}\] \[\leq \left(1-\frac{1}{\frac{p+\theta_{1}+\theta_{2}}{p\theta_{1}}+ \frac{2\theta_{1}+\eta\mu}{\eta\mu}+4(1+\omega)}\right)\Psi^{k}\] \[\leq \left(1-\frac{1}{250\left(\omega+\left(1+\frac{\omega}{\sqrt{n}} \right)\sqrt{\kappa}\right)}\right)\Psi^{k},\quad\forall k\geq 0, \tag{33}\]where \(\Psi^{0}:=\lambda\mathcal{W}^{0}+\frac{2\gamma_{2}\beta\theta_{2}}{\theta_{1}p} \mathcal{V}^{0}+\mathcal{Z}^{0}+\frac{10\eta\omega(1+\omega)\gamma\beta}{\theta _{1}n}\mathcal{H}^{0}\). Note that since we use initialization \(y^{0}=z^{0}=w^{0}=h_{i}^{0}=h^{0},\,\forall 1\leq i\leq n\), we have \(\mathcal{W}^{0}=\mathcal{Y}^{0}\leq(L\Delta)/2\), \(\mathcal{Z}^{0}\leq\Delta\), \(\mathcal{H}^{0}\leq L^{2}\Delta\), which indicates that

\[\Psi^{0}\leq\frac{L}{2}\cdot(\lambda_{W}+\lambda_{Y}+\lambda_{Z}+\lambda_{H}) \Delta,\]

where \(\lambda_{W}=\lambda\geq\frac{2\gamma\beta\theta_{2}}{\theta_{1}p}\), \(\lambda_{Y}=\frac{2\gamma\beta}{\theta_{1}}\), \(\lambda_{Z}=\frac{2}{L}\), \(\lambda_{H}=\frac{20\eta\omega(1+\omega)\gamma\beta L}{\theta_{1}n}\). These coefficients have the following inequalities:

\[\lambda_{W}+\lambda_{Y}\geq \frac{4\eta(\theta_{2}+p)}{p(2\theta_{1}+\eta\mu)^{2}}=\frac{n \theta_{2}(\theta_{2}+p)}{30\omega Lp(2/3\sqrt{\kappa}+n\theta_{2}/120\omega \kappa)^{2}}\geq\frac{n\theta_{2}(\theta_{2}+p)\kappa}{15\omega Lp}\] \[\geq \frac{\kappa}{135L}\geq\frac{1}{270}\lambda_{Z},\]

and

\[\frac{3}{32}(\lambda_{W}+\lambda_{Y})\geq\frac{\kappa}{1440L}\geq\frac{(1+ \omega)n\theta_{2}^{2}\kappa}{160\omega L}\geq\frac{40\eta^{2}\omega(1+\omega) L}{(2\theta_{1}+\eta\mu)^{2}n}=\lambda_{H}.\]

Consequently, the initial value of the Lyapunov function can be bounded as

\[\Psi^{0}\leq 136L(\lambda_{W}+\lambda_{Y})\Delta,\]

which together with (33) further implies that

\[\min\{\mathbb{E}[f(w^{T})],\mathbb{E}[f(y^{T})]\}-f^{\star}\] \[\leq \min\left\{\frac{1}{\lambda_{W}},\frac{1}{\lambda_{Y}}\right\} \left(1-\frac{1}{250\left(\omega+\left(1+\frac{\omega}{\sqrt{n}}\right)\sqrt{ \kappa}\right)}\right)^{T}\Psi^{0}\] \[\leq 272L\Delta\left(1-\frac{1}{250\left(\omega+\left(1+\frac{\omega }{\sqrt{n}}\right)\sqrt{\kappa}\right)}\right)^{T}.\]

Thus, \(\mathcal{O}\left(\left(\omega+\left(1+\frac{\omega}{\sqrt{n}}\right)\sqrt{ \kappa}\right)\ln\left(\frac{L\Delta}{e}\right)\right)\) iterations are sufficient to guarantee an \(\epsilon\)-solution. 

### Generally-convex case

In this subsection, we restate the convergence result in the generally-convex case as in Theorem 3 and prove it using Lemma 6, 7 and the Lyapunov function defined in (27).

**Theorem 5**.: _If \(\mu=0\) and parameters satisfy \(\alpha=1/(1+\omega)\), \(\beta=1\), \(p=\theta_{2}=1/(3(1+\omega))\), \(\theta_{1,k}=9/(k+27(1+\omega))\), \(\gamma_{k}=\eta_{k}/(2\theta_{1,k})\), and_

\[\eta_{k}=\min\left\{\frac{k+1+27(1+\omega)}{9(1+\omega)^{2}(1+27(1+\omega))L}, \frac{3n}{200\omega(1+\omega)L},\frac{1}{2L}\right\},\]

_then the number of communication rounds performed by ADIANA to find an \(\epsilon\)-accurate solution such that \(\mathbb{E}[f(\hat{x})]-\min_{x}f(x)\leq\epsilon\) is provided by \(\mathcal{O}((1+\omega/\sqrt{n})\sqrt{L\Delta/\epsilon}+(1+\omega)\sqrt[3]{L \Delta/\epsilon})\)._

Proof.: Considering (20) \(+\lambda_{k}(24)+\frac{5\gamma_{k}\beta\eta_{k}}{4\theta_{1,k}}(25)+\frac{10 \eta_{k}\omega(1+\omega)\gamma_{k}\beta}{n\theta_{1,k}}(26)\) and applying the choice of \(\theta_{2}\), \(p\) and \(\alpha\), we have

\[\mathbb{E}_{k}[\Psi^{k+1}]\] \[\leq \left(\frac{2\gamma_{k}\beta\theta_{2}}{\theta_{1,k}}+(1-p) \lambda_{k}\right)\mathcal{W}^{k}+\left(\frac{2\gamma_{k}\beta(1-\theta_{1,k }-\theta_{2})}{\theta_{1,k}}+p\lambda_{k}\right)\mathcal{Y}^{k}+\beta\mathcal{ Z}^{k}\] \[+\left(1-\frac{1}{4(1+\omega)}\right)\frac{10\eta_{k}\omega(1+ \omega)\gamma_{k}\beta}{n\theta_{1,k}}\mathcal{H}^{k}-\left(\frac{\gamma_{k} \beta\theta_{2}}{L\theta_{1,k}}-\frac{5\omega\gamma_{k}\beta\eta_{k}}{2n\theta _{1,k}}-\frac{100\eta_{k}\omega\gamma_{k}\beta}{9n\theta_{1,k}}\right)\mathcal{ G}^{k}_{w}\] \[-\left(\frac{\gamma_{k}\beta(1-\theta_{1,k}-\theta_{2})}{L\theta _{1,k}}-\frac{100\eta_{k}\omega\gamma_{k}\beta}{9n\theta_{1,k}}\right) \mathcal{G}^{k}_{y}. \tag{34}\]Similar to the proof of Theorem 4, we can simplify (34) by validating

\[\begin{cases}\frac{2\gamma_{k}\beta\theta_{2}}{\theta_{1,k}}+(1-p) \lambda_{k}\leq\left(1-\frac{p\theta_{1,k}}{p+\theta_{1,k}+\theta_{2}}\right) \lambda_{k}\leq\left(1-\frac{\theta_{1,k}}{3}\right)\lambda_{k},\\ \frac{2\gamma_{k}\beta(1-\theta_{1,k}-\theta_{2})}{\theta_{1,k}}+p\lambda_{k} \leq\left(1-\frac{p\theta_{1,k}}{p+\theta_{1,k}+\theta_{2}}\right)\frac{2 \gamma_{k}\beta}{\theta_{1,k}}\leq\left(1-\frac{\theta_{1,k}}{3}\right)\frac{2 \gamma_{k}\beta}{\theta_{1,k}},\\ \frac{\gamma_{k}\beta\theta_{2}}{L\theta_{1,k}}\frac{-5\gamma_{k}\beta\eta_{2 }}{2n\theta_{1,k}}-\frac{100n\omega\gamma_{k}\beta}{9n\theta_{1,k}}\geq 0,\\ \frac{\gamma_{k}\beta(1-\theta_{1,k}-\theta_{2})}{L\theta_{1,k}}-\frac{100n \omega\gamma_{k}\beta}{9n\theta_{1,k}}\geq 0,\end{cases}\]

and then obtain

\[\mathbb{E}_{k}[\Psi^{k+1}]\leq \left(1-\frac{\theta_{1,k}}{3}\right)\lambda_{k}\mathcal{W}^{k}+ \left(1-\frac{\theta_{1,k}}{3}\right)\frac{2\gamma_{k}}{\theta_{1,k}}\mathcal{ Y}^{k}+\mathcal{Z}^{k}\] \[+\left(1-\frac{1}{4(1+\omega)}\right)\frac{10\eta_{k}\omega(1+ \omega)\gamma_{k}}{\theta_{1,k}n}\mathcal{H}^{k}. \tag{35}\]

For \(\forall k\geq 1\), we have \(\theta_{1,k}\leq\theta_{1,k-1}\) and thus

\[\left(1-\frac{\theta_{1,k}}{3}\right)\lambda_{k}= \left(1-\frac{3}{k+27(1+\omega)}\right)\frac{\eta_{k}}{2p\theta_ {1,k}^{2}}\left(\theta_{1,k}+\sqrt{\theta_{1,k}^{2}+4p\theta_{2}}\right)\] \[\leq \left(1-\frac{3}{k+27(1+\omega)}\right)\frac{\eta_{k}}{2p\theta_ {1,k}^{2}}(\theta_{1,k-1}+\sqrt{\theta_{1,k-1}^{2}+4p\theta_{2}})\] \[= \left(1-\frac{3}{k+27(1+\omega)}\right)\left(\frac{k+27(1+\omega )}{k-1+27(1+\omega)}\right)^{2}\frac{\eta_{k}}{\eta_{k-1}}\lambda_{k-1}.\]

Further noting \(\frac{\eta_{k}}{\eta_{k-1}}\leq 1+\frac{1}{k+27(1+\omega)}\), we obtain

\[\left(1-\frac{\theta_{1,k}}{3}\right)\lambda_{k}\leq \left(1-\frac{3}{k+27(1+\omega)}\right)\left(1-\frac{1}{k+27(1+ \omega)}\right)^{-2}\left(1+\frac{1}{k+27(1+\omega)}\right)\lambda_{k-1}\] \[\leq \lambda_{k-1}. \tag{36}\]

Similarly,

\[\left(1-\frac{\theta_{1,k}}{3}\right)\frac{2\gamma_{k}}{\theta_{1,k}}\] \[= \left(1-\frac{3}{k+27(1+\omega)}\right)\left(\frac{k+27(1+\omega )}{k-1+27(1+\omega)}\right)^{2}\frac{\eta_{k}}{\eta_{k-1}}\frac{2\gamma_{k-1 }}{\theta_{1,k-1}}\] \[\leq \left(1-\frac{3}{k+27(1+\omega)}\right)\left(1-\frac{1}{k+27(1+ \omega)}\right)^{-2}\left(1+\frac{1}{k+27(1+\omega)}\right)\frac{2\gamma_{k- 1}}{\theta_{1,k-1}}\] \[\leq \frac{2\gamma_{k-1}}{\theta_{1,k-1}}, \tag{37}\]

and

\[\left(1-\frac{1}{4(1+\omega)}\right)\frac{10\eta_{k}\omega(1+ \omega)\gamma_{k}\beta}{n\theta_{1,k}}\] \[= \left(1-\frac{1}{4(1+\omega)}\right)\left(\frac{k+27(1+\omega)}{k -1+27(1+\omega)}\right)^{2}\left(\frac{\eta_{k}}{\eta_{k-1}}\right)^{2}\frac{1 0\eta_{k-1}\omega(1+\omega)\gamma_{k-1}\beta}{n\theta_{1,k-1}}\] \[\leq \frac{\left(1-\frac{5}{k+27(1+\omega)}\right)\left(1+\frac{3}{k +27(1+\omega)}\right)}{\left(1-\frac{1}{k+27(1+\omega)}\right)^{2}}\frac{10 \eta_{k-1}\omega(1+\omega)\gamma_{k-1}\beta}{n\theta_{1,k-1}}\] \[\leq \frac{10\eta_{k-1}\omega(1+\omega)\gamma_{k-1}\beta}{\theta_{1,k- 1}n}. \tag{38}\]Combining (35),(36),(37), and (38), we have for \(\forall\,k\geq 1\) that

\[\mathbb{E}_{k}[\Psi^{k+1}]\leq\Psi^{k}. \tag{39}\]

By applying (39) with \(k=T-1,T-2,\cdots,1\) and (35) with \(k=0\), we obtain

\[\mathbb{E}[\Psi^{T}]\leq \left(1-\frac{\theta_{1,0}}{3}\right)\frac{2\gamma_{0}(\theta_{1, 0}+\theta_{2})}{p\theta_{1,0}}\mathcal{W}^{0}+\left(1-\frac{\theta_{1,0}}{3} \right)\frac{2\gamma_{0}}{\theta_{1,0}}\mathcal{Y}^{0}+\mathcal{Z}^{0}\] \[+\left(1-\frac{1}{4(1+\omega)}\right)\frac{10\eta_{0}\omega(1+ \omega)\gamma_{0}}{\theta_{1,0}n}\mathcal{H}^{0}\] \[\leq \frac{2}{L}\mathcal{W}^{0}+\frac{1}{L}\mathcal{Y}^{0}+\mathcal{Z }^{0}+\frac{3}{40L^{2}}\mathcal{H}^{0}\leq\left(1+\frac{1}{2}+1+\frac{3}{40} \right)\Delta\leq 3\Delta.\]

Note that

\[\Psi^{T}\geq \lambda_{T-1}\mathcal{W}^{T}+\frac{2\gamma_{T-1}\beta}{\theta_{1, T-1}}\mathcal{Y}^{T}\geq\frac{2\gamma_{T-1}\beta\theta_{2}}{\theta_{1,T-1}p} \mathcal{W}^{T}+\frac{2\gamma_{T-1}\beta}{\theta_{1,T-1}}\mathcal{Y}^{T}=\frac {\eta_{T-1}}{\theta_{1,T-1}^{2}}(\mathcal{W}^{T}+\mathcal{Y}^{T}),\]

thus

\[\max\{\mathbb{E}[f(w^{T})],\mathbb{E}[f(y^{T})]\}-f^{\star}\] \[\leq \frac{\theta_{1,T-1}^{2}}{\eta_{T-1}}\mathbb{E}[\Psi^{T}]\] \[\leq \frac{243\Delta}{(T-1+27(1+\omega))^{2}}\cdot\max\left\{\frac{9(1 +\omega)^{2}(1+27(1+\omega))L}{T+27(1+\omega)},\frac{200\omega(1+\omega)L}{3n},2L\right\}\] \[= \mathcal{O}\left(\frac{(1+\omega^{2}/n)L\Delta}{T^{2}}+\frac{(1+ \omega^{3})L\Delta}{T^{3}}\right),\]

thus it suffices to achieve an \(\epsilon\)-solution with \(\mathcal{O}\left(\left(1+\frac{\omega}{\sqrt{n}}\right)\sqrt{\frac{L\Delta}{ \epsilon}}+(1+\omega)\sqrt[3]{\frac{L\Delta}{\epsilon}}\right)\) iterations. 

## Appendix E Correction on CANITA [33]

We observe that when \(\omega\gg n\), the original convergence rate of CANITA [33] contradicts the lower bounds presented in our Theorem 2. This discrepancy may stem from errors in the derivation of equations (35) and (36) in [33], or from the omission of certain conditions such as \(\omega=\Omega(n)\). To address this issue, we provide a corrected proof and the corresponding convergence rate. Here we modify the choice of \(\beta_{0}\) in ([33], Theorem 2) to \(9(1+b+\omega)^{2}/(2(1+b))\), while keeping all other choices consistent with the original proof, _i.e._, \(b=\min\{\omega,\,\sqrt{\omega(1+\omega)^{2}/n}\}\), \(p_{t}\equiv 1/(1+b)\), \(\alpha_{t}\equiv 1/(1+\omega)\), \(\theta_{t}=3(1+b)/(t+9(1+b+\omega))\), \(\beta=48\omega(1+\omega)(1+b+2(1+\omega))/(n(1+b)^{2})\) and

\[\eta_{t}=\begin{cases}\frac{1}{L(\beta_{0}+3/2)},&\text{for }t=0,\\ \min\left\{\left(1+\frac{1}{t+9(1+b+\omega)}\right)\eta_{t-1},\frac{1}{L(\beta +3/2)}\right\},&\text{for }t\geq 1.\end{cases}\]

By definition we have

\[\eta_{T}= \min\left\{\frac{T+1+9(1+b+\omega)}{1+9(1+b+\omega)}\eta_{0},\frac {1}{L(\beta+3/2)}\right\}\] \[= \min\left\{\frac{T+1+9(1+b+\omega)}{1+9(1+b+\omega)}\frac{1}{L( \beta_{0}+3/2)},\frac{1}{L(\beta+3/2)}\right\}\] \[\geq \min\left\{\frac{(T+9(1+b+\omega))(1+b)}{60L(1+b+\omega)^{3}}, \frac{1}{L(\beta+3/2)}\right\} \tag{40}\]

Plugging (40) and ([33],34) into ([33],33), we obtain

\[\mathbb{E}[F^{T+1}]= \mathcal{O}\left(\frac{(1+b+\omega)^{3}L\Delta}{(T+9(1+b+\omega))^ {3}}+\frac{(1+b)(\beta+3/2)L\Delta}{(T+9(1+b+\omega))^{2}}\right)\] \[= \mathcal{O}\left(\frac{(1+b+\omega)^{3}L\Delta}{T^{3}}+\frac{(1+b )(\beta+3/2)L\Delta}{T^{2}}\right). \tag{41}\]Using \(b=\min\{\omega,\sqrt{\omega(1+\omega)^{2}/n}\}\), we have

\[(1+b+\omega)^{3}=\Theta\left((1+\omega)^{3}\right),\]

and

\[(1+b)(\beta+3/2)= \Theta\left((1+b)+\frac{\omega(1+\omega)(1+b+\omega)}{n(1+b)}\right)\] \[= \Theta\left(1+\frac{\omega^{3/2}}{n^{1/2}}+\frac{\omega^{2}}{n} \right),\]

thus (41) can be simplified as

\[\mathbb{E}[F^{T+1}]=\mathcal{O}\left(\frac{(1+\omega)^{3}L\Delta}{T^{3}}+\frac {(1+\omega^{3/2}/n^{1/2}+\omega^{2}/n)L\Delta}{T^{2}}\right).\]

Consequently, for \(\epsilon<L\Delta/2\) (_i.e._, a precision that the initial point does not satisfy), the communication rounds to achieve precision \(\epsilon\) is given by \(\mathcal{O}\left(\omega\frac{\sqrt{L\Delta}}{\sqrt{\epsilon}}+\left(1+\frac{ \omega^{3/4}}{n^{1/4}}+\frac{\omega}{\sqrt{n}}\right)\frac{\sqrt{L\Delta}}{ \sqrt{\epsilon}}\right)\).

## Appendix F Experimental details and additional results

This section provides more details of the experiments listed in Sec. 6, as well as a few new experiments to validate our theories.

### Experimental details

This section offers a comprehensive and detailed description of the experiments listed in Sec. 6, including problem formulation, data generation, cost calculation, and algorithm implementation.

**Least squares.** The local objective function of node \(i\) is defined as \(f_{i}(x):=\frac{1}{2}\|A_{i}x-b_{i}\|^{2}\), where \(A_{i}\in\mathbb{R}^{M\times d}\), \(b_{i}\in\mathbb{R}^{M}\). We set \(d=20\), \(M=25\), and the number of nodes \(n=400\). To generate \(A_{i}\)'s, we first randomly generate a Gaussian matrix \(G\in\mathbb{R}^{nM\times d}\); we then apply the SVD decomposition \(G=U\Sigma V^{\top}\) and replace the singular values in \(\Sigma\) by an arithmetic sequence starting from 1 and ending at 100 to get \(\tilde{\Sigma}\) and the resulted data matrix \(\tilde{G}=U\tilde{\Sigma}V^{\top}\); we finally allocate the submatrix of \(\tilde{G}\) composed of the \(((i-1)M+1)\)-th row to the \((iM)\)-th row to be \(A_{i}\) for all \(1\leq i\leq n\).

**Logistic regression.** The local objective function of node \(i\) is defined as \(f_{i}(x):=\frac{1}{M}\sum_{m=1}^{M}\ln(1+\exp(-b_{i,m}a_{i,m}^{\top}x)\), where number of nodes \(n=400\), \(a_{i,m}\) stands for the feature of the \(m\)-th datapoint in the node \(i\)'s dataset, and \(b_{i,m}\) stands for the corresponding label. In a9a dataset, node \(i\) owns the \((81(i-1)+1)\)-th to the \((81i)\)-th datapoint with feature dimension \(d=123\). In w8a dataset, node \(i\) owns the \((120(i-1)+1)\)-th to the \((120i)\)-th datapoint with feature dimension \(d=300\).

**Constructed problem.** The local objective function of node \(i\) is defined as

\[f_{i}(x):=\begin{cases}\frac{\mu}{2}\|x\|^{2}+\frac{L-\mu}{4}([x]_{1}^{2}+\sum _{1\leq r\leq d/2-1}([x]_{2r}-[x]_{2r+1})^{2}+[x]_{d}^{2}-2[x]_{1}),&\text{if $i \leq n/2$},\\ \frac{\mu}{2}\|x\|^{2}+\frac{L-\mu}{4}(\sum_{1\leq r\leq d/2}([x]_{2r-1}-[x]_ {2r})^{2}),&\text{if $i>n/2$},\end{cases}\]

where \([x]_{l}\) denotes the \(l\)-th entry of vector \(x\in\mathbb{R}^{d}\). We set \(\mu=1\), \(L=10^{4}\), \(d=20\) and number of nodes \(n=400\).

**Compressors.** We apply various compressors to the algorithms with communication compression through our experiments. In the constructed quadratic problem, we consider ADIANA algorithm with random-\(s\) compressors (see Example 1 in Appendix A) in six different settings, \(i\)., three choices of \(s\) (\(s=1,2,4\)), with two different (shared or independent) randomness settings. In the least squares and logistic regression problems, we apply the independent random-\(\lfloor d/20\rfloor\) compressor to ADIANA, CANITA and DIANA algorithm. In particular, we use the unscaled version of the independent random-\(\lfloor d/20\rfloor\) compressor for EF21 to guarantee convergence, where the values of selected entries are transmitted directly to the server without being scaled by \(d/s\) times. In Appendix F.2, we further apply independent _natural compression_[20] and _random quantization_[3] with \(s=\lceil\sqrt{d}\rceil\) in the above algorithms.

**Total communicated bits.** For non-compression algorithms and algorithms with a fixed-length compressor, such as random-\(s\) and natural compression, the total communication bits can be calculated using the following formula: _total communication bits = number of iterations \(\times\) communication rounds per iteration \(\times\) communicated bits per round._ Among the algorithms we compare, ADIANA and CANITA communicate twice per iteration, while the other algorithms communicate only once. The communicated bits per round for non-compression algorithms amount to \(64d\) for \(d\) float64 entries. In the case of the random-\(s\) compressor, the communicated bits per communication are calculated as \(64s+\lceil\log_{2}\binom{d}{s}\rceil\). Similarly, for natural compression, the communicated bits per round are fixed at \(12d\), with 1 sign bit and 11 exponential bits allocated for each entry. In the case of adaptive-length random quantization, the communication cost is evaluated using _Elias_ integer encoding [16]. This cost is then averaged among \(n\) nodes, providing a more representative estimate.

**Algorithm implementation.** We implement ADIANA, CANITA, DIANA, EF21 algorithms following the formulation in Algorithm 1, [33, 26], and [49], respectively. We implement Nesterov's accelerated algorithm with the following recursions:

\[\begin{cases}y^{k}=(1-\theta_{t})x^{k}+\theta_{t}z^{k},\\ x^{k+1}=y^{k}-\eta_{k}\nabla f(y^{k}),\\ z^{k+1}=x^{k}+\frac{1}{\theta_{k}}(x^{k+1}-x^{k}).\end{cases}\]

The value of \(\alpha\) in ADIANA, CANITA and DIANA are all set to \(1/(1+\omega)\), and we set \(\gamma_{k},\beta\) of ADIANA as in Theorem 3. Other parameters are all selected through running Bayesian Optimization [45] for the first \(20\%\) iterations with 5 initial points and 20 trials. The exact value of the selected parameters are listed in Appendix F.3. Each curve (except for Nesterov's accelerated algorithm which does not involve randomness) is averaged through 20 trials, with the range of standard deviation depicted.

**Computational resource.** All experiments are run on an NVIDIA A100 server. Each trial consumes up to 10 minutes of running time.

### Additional experiments

**Additional compressors.** In addition to the experiments in Sec. 6, we consider applying different compressors in the algorithms with communication compression. Fig. 3 and Fig. 4 show results of using natural compression and random quantization, respectively. These results are consistent with the results in Sec. 6.

**CIFAR-10 dataset.** We also consider binomial logistic regression with CIFAR-10 dataset, where labels of each datum are categorized by whether they equal to 3, \(i\).e., the corresponding figures belong to the cat category. The full training set with \(50000\) images, are devided equally to \(n=250\) nodes. The compressor choices follow the same strategies as in Appendix F.1, where dimension \(d=3072\). Fig. 5 compares convergence results between Nesterov method and ADIANA with different compressors. It can be observed that ADIANA equipped with more aggressive compressors, \(i\).e., those with bigger \(\omega\), benefits more from the compression, which is consistent with our theoretical results.

Figure 3: Convergence results of various distributed algorithms on a synthetic least squares problem (left), logistic regression problems with dataset a9a (middle) and w8a (right). The \(y\)-axis represents \(f(\hat{x})-f^{*}\) and the \(x\)-axis indicates the total communicated bits sent by per worker. All compressors used are independent natural compression.

### Parameter values

In this subsection, we list all the parameter values that are selected by applying Bayesian Optimization. Table 2, 3, 4, 5, 6 list the parameters chosen in the least squares problem, logistic regression using a9a dataset, logistic regression using w8a dataset, the constructed problem, and logistic regression using CIFAR-10 dataset, respectively.

Figure 4: Convergence results of various distributed algorithms on a synthetic least squares problem (left), logistic regression problems with dataset a9a (middle) and w8a (right). The \(y\)-axis represents \(f(\hat{x})-f^{*}\) and the \(x\)-axis indicates the total communicated bits sent by per worker. All compressors used are independent random quantization.

Figure 5: Experimental results of logistic regression problem on the CIFAR-10 dataset. The objective function is constructed by relabeling the 10 classes into 2 classes, namely cat (corresponding to the original cat class) and non-cat (corresponding to the rest classes). ADIANA w. R.S. / R.Q. / N.C. represents ADIANA algorithm with random-\(\lfloor d/20\rfloor\) compressor / random quantization compressor with \(s=\lceil\sqrt{d}\rceil\) / natural compression compressor, where \(d=3072\) is the dimension of gradient vectors as well as the number of features in CIFAR-10 dataset. The experiments are conducted under the same setting as in the "Algorithm implementation" part in Appendix F.1.

\begin{table}
\begin{tabular}{r c} \hline Algorithm & Parameters \\ \hline \hline Nesterov & \(\eta=9.4\times 10^{-1}\), \(\theta=1.7\times 10^{-1}\). \\ ADIANA R.S. & \(\eta=2.1\), \(\theta_{1}=\frac{1.3\times 10^{1}}{k+5.2\times 10^{2}}\), \(\theta_{2}=2.1\times 10^{-1}\), \(p=7.7\times 10^{-1}\). \\ ADIANA N.C. & \(\eta=2.1\), \(\theta_{1}=\frac{1.0}{k+4.3}\), \(\theta_{2}=8.0\times 10^{-3}\), \(p=8.0\times 10^{-1}\). \\ ADIANA R.Q. & \(\eta=2.2\), \(\theta_{1}=\frac{1.3}{k+1.3}\), \(\theta_{2}=1.5\times 10^{-1}\), \(p=8.5\times 10^{-1}\). \\ CANITA R.S. & \(\eta=\min\{\frac{k+2.1\times 10^{2}}{2.1\times 10^{2}},1.4\}\), \(\theta=\frac{2.0\times 10^{1}}{k+2.3\times 10^{2}}\), \(p=7.8\times 10^{-1}\). \\ CANITA N.C. & \(\eta=1.2\), \(\theta=\frac{2.1}{k+1.2\times 10^{4}}\), \(p=5.2\times 10^{-1}\). \\ CANITA R.Q. & \(\eta=2.0\), \(\theta=\frac{3.0}{k+3.0}\), \(p=7.2\times 10^{-1}\). \\ DIANA N.C. & \(\gamma=2.6\). \\ DIANA R.S. & \(\gamma=9.4\times 10^{-1}\). \\ DIANA R.Q. & \(\gamma=4.7\times 10^{-1}\) \\ EF21 R.S. & \(\gamma=1.3\). \\ EF21 N.C. & \(\gamma=1.6\). \\ EF21 R.Q. & \(\gamma=2.7\). \\ \hline \hline \end{tabular}
\end{table}
Table 3: Parameters for algorithms in the logistic regression problem with a9a dataset. Notation \(k\) stands for the index of iteration. Other notations are as in Table 2.

\begin{table}
\begin{tabular}{r c} \hline Algorithm & Parameters \\ \hline \hline Nesterov & \(\eta=3.0\times 10^{-2}\), \(\theta=1.4\times 10^{-2}\). \\ ADIANA R.S. & \(\eta=4.8\times 10^{-2}\), \(\theta_{1}=2.2\times 10^{-2}\), \(\theta_{2}=7.6\times 10^{-2}\), \(p=4.1\times 10^{-2}\). \\ ADIANA N.C. & \(\eta=3.9\times 10^{-2}\), \(\theta_{1}=1.0\times 10^{-2}\), \(\theta_{2}=2.9\times 10^{-1}\), \(p=9.9\times 10^{-1}\). \\ ADIANA R.Q. & \(\eta=6.5\times 10^{-2}\), \(\theta_{1}=1.4\times 10^{-2}\), \(\theta_{2}=2.7\times 10^{-1}\), \(p=5.5\times 10^{-1}\). \\ DIANA R.S. & \(\gamma=7.9\times 10^{-2}\). \\ DIANA N.C. & \(\gamma=7.4\times 10^{-2}\). \\ DIANA R.Q. & \(\gamma=7.6\times 10^{-2}\). \\ EF21 R.S. & \(\gamma=6.2\times 10^{-2}\). \\ EF21 N.C. & \(\gamma=6.8\times 10^{-2}\). \\ EF21 R.Q. & \(\gamma=7.4\times 10^{-2}\). \\ \hline \hline \end{tabular}
\end{table}
Table 2: Parameters for algorithms in the least squares problem. Notation R.S. stands for independent random sparsification, N.C. stands for independent natural compression, R.Q. stands for independent random quantization.

\begin{table}
\begin{tabular}{c c} \hline \hline Algorithm & Parameters \\ \hline Nesterov & \(\eta=1.1\times 10^{-1}\), \(\theta=1.5\times 10^{-1}\). \\ ADIANA R.S. & \(\eta=1.4\times 10^{-1}\), \(\theta_{1}=\frac{12}{k+3.3\times 10^{2}}\), \(\theta_{2}=9.0\times 10^{-2}\), \(p=4.3\times 10^{-1}\). \\ ADIANA N.C. & \(\eta=1.4\times 10^{-1}\), \(\theta_{1}=\frac{1.0\times 10^{-2}}{k+7.0}\), \(\theta_{2}=7.0\times 10^{-1}\), \(p=8.5\times 10^{-1}\). \\ ADIANA R.Q. & \(\eta=1.2\times 10^{1}\), \(\theta_{1}=\frac{8.2}{k+59}\), \(\theta_{2}=8.0\times 10^{-1}\), \(p=6.0\times 10^{-1}\). \\ \hline \hline \end{tabular}
\end{table}
Table 4: Parameters for algorithms in logistic regression with w8a dataset. Notations are as in Table 3.

\begin{table}
\begin{tabular}{c c} \hline \hline Algorithm & Parameters \\ \hline Nesterov & \(\eta=1.5\times 10^{1}\), \(\theta=9.4\times 10^{-1}\). \\ ADIANA R.S. & \(\eta=\min\{\frac{k+4.1\times 10^{2}}{1.2\times 10^{2}},15\}\), \(\theta_{1}=\frac{8.8}{k+4.8\times 10^{2}}\), \(\theta_{2}=2.4\times 10^{-2}\), \(p=3.6\times 10^{-1}\). \\ ADIANA N.C. & \(\eta=1.5\times 10^{1}\), \(\theta_{1}=\frac{2.5}{k+1.1\times 10^{1}}\), \(\theta_{2}=6.7\times 10^{-1}\), \(p=8.3\times 10^{-1}\). \\ ADIANA R.Q. & \(\eta=1.5\times 10^{1}\), \(\theta_{1}=\frac{1.9}{k+7.4}\), \(\theta_{2}=4.2\times 10^{-1}\), \(p=9.9\times 10^{-1}\). \\ CANITA R.S. & \(\eta=\min\{\frac{k+2.0\times 10^{2}}{2.2\times 10^{2}},7.7\}\), \(\theta=\frac{1.1\times 10^{1}}{k+2.3\times 10^{2}}\), \(p=4.3\times 10^{-1}\). \\ CANITA N.C. & \(\eta=\min\{\frac{k+1.1\times 10^{1}}{2.7},1.1\times 10^{1}\}\), \(\theta=\frac{5.4}{k+7.4\times 10^{1}}\), \(p=4.9\times 10^{1}\). \\ CANITA R.Q. & \(\eta=\min\{\frac{k+1.6\times 10^{1}}{7.3},1.5\times 10^{1}\}\), \(\theta=\frac{2.2}{k+2.2\times 10^{2}}\), \(p=4.6\times 10^{-1}\). \\ DIANA R.S. & \(\gamma=1.5\times 10^{1}\). \\ DIANA N.C. & \(\gamma=1.6\times 10^{1}\). \\ DIANA R.Q. & \(\gamma=1.5\times 10^{1}\). \\ EF21 R.S. & \(\gamma=2.0\times 10^{1}\). \\ EF21 N.C. & \(\gamma=1.5\times 10^{1}\). \\ EF21 R.Q. & \(\gamma=1.5\times 10^{1}\). \\ \hline \hline \end{tabular}
\end{table}
Table 5: Parameters for algorithms in the constructed problem. Notation i.d.rand-\(s\) denotes independent random-\(s\) compressor, s.d.rand-\(s\) denotes random-\(s\) compressor with shared randomness.