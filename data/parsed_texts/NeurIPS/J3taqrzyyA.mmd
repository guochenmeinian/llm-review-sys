# Stability-penalty-adaptive

follow-the-regularized-leader:

Sparsity, game-dependency, and best-of-both-worlds

 Taira Tsuchiya

The University of Tokyo

tsuchiya@mist.i.u-tokyo.ac.jp

&Shinji Ito

NEC Corporation / RIKEN

i-shinji@nec.com

&Junya Honda

Kyoto University / RIKEN

honda@i.kyoto-u.ac.jp

This work was done when the author was with Kyoto University and RIKEN.

###### Abstract

Adaptivity to the difficulties of a problem is a key property in sequential decision-making problems to broaden the applicability of algorithms. Follow-the-regularized-leader (FTRL) has recently emerged as one of the most promising approaches for obtaining various types of adaptivity in bandit problems. Aiming to further generalize this adaptivity, we develop a generic adaptive learning rate, called _stability-penalty-adaptive (SPA) learning rate_ for FTRL. This learning rate yields a regret bound jointly depending on stability and penalty of the algorithm, into which the regret of FTRL is typically decomposed. With this result, we establish several algorithms with three types of adaptivity: _sparsity_, _game-dependency_, and _best-of-both-worlds_ (BOBW). Despite the fact that sparsity appears frequently in real problems, existing sparse multi-armed bandit algorithms with \(k\)-arms assume that the sparsity level \(s\leq k\) is known in advance, which is often not the case in real-world scenarios. To address this issue, we first establish \(s\)-agnostic algorithms with regret bounds of \(\widetilde{O}(\sqrt{sT})\) in the adversarial regime for \(T\) rounds, which matches the existing lower bound up to a logarithmic factor. Meanwhile, BOBW algorithms aim to achieve a near-optimal regret in both the stochastic and adversarial regimes. Leveraging the SPA learning rate and the technique for \(s\)-agnostic algorithms combined with a new analysis to bound the variation in FTRL output in response to changes in a regularizer, we establish the first BOBW algorithm with a sparsity-dependent bound. Additionally, we explore partial monitoring and demonstrate that the proposed SPA learning rate framework allows us to achieve a game-dependent bound and the BOBW simultaneously.

## 1 Introduction

This study considers the multi-armed bandits (MAB) and partial monitoring (PM). In the MAB problem, the learner selects one of \(k\) arms, and the adversary simultaneously determines the loss of each arm, \(\ell_{t}=(\ell_{t1},\ldots,\ell_{tk})^{\top}\) in \([0,1]^{k}\) or \([-1,1]^{k}\). After that, the learner observes only the loss for the chosen arm. The learner's goal is to minimize the regret, which is the difference between the learner's total loss and the total loss of an optimal arm fixed in hindsight. PM is a generalization of MAB, and the learner observes feedback symbols instead of the losses.

One of the most promising frameworks for MABs and PM is follow-the-regularized-leader (FTRL) [5; 12], which determines the arm selection probability at each round by minimizing the sum of the cumulative (estimated) losses so far plus a convex regularizer. Note that the well-known Exp3 algorithm developed in [5] is equivalent to FTRL with the (negative) Shannon entropy regularizer. FTRL is also known to perform well for the classic expert problem [17] and reinforcement learning [52]. Furthermore, when the problem is "benign", it is known that FTRL can exploit the underlying structure to adaptively improve its performance. Typical examples of such adaptive improvements are (i) _data-dependent bounds_ and (ii) _best-of-both-worlds_ (BOBW).

Data-dependent bounds have been investigated to enhance the adaptivity of algorithms to a given structure of losses in the _adversarial regime_, where feedback (_e.g.,_ losses in MABs) is decided in an arbitrary manner. There are various examples of data-dependent bounds, and this study considers _sparsity-dependent bounds_ and _game-dependent bounds_.

A sparsity-dependent bound is an important example of data-dependent bounds, as sparsity frequently appears in real-world problems. For example, in online advertisement allocation, it is often the case that only a fraction of the ads is clicked. Although there are some studies for sparse MABs [10; 27; 51], all of them assume that (an upper bound of) sparsity level \(s\geq\|\ell_{t}\|_{0}=|\{i\in[k]\colon\ell_{ti}\neq 0\}|\) is known beforehand, which in many practical scenarios does not hold.

The concept of a game-dependent bound was recently introduced by Lattimore and Szepesvari [32] to derive a regret upper bound that depends on the game the learner is facing. As the authors suggest, one of the motivations for the game-dependent bound is that previous PM algorithms are "quite conservative and not practical for normal problems". For example, whereas the Bernoulli MAB is expressed as a PM, algorithms for PM do not always achieve the minimax regret of MAB [5]. The game-dependent bound enables the learner to automatically adapt to the essential difficulty of the game the algorithm is actually facing.

The BOBW algorithm aims to achieve near-optimal regret bounds in stochastic and adversarial regimes, where the feedback is stochastically generated in the stochastic regime. Since we often do not know the underlying regime, it is desirable for an algorithm to _simultaneously_ obtain a near-optimal performance both for the stochastic and adversarial regimes. For multi-armed bandits, Bubeck and Slivkins [9] developed the first BOBW algorithm, and Zimmert and Seldin [53] proposed the well-known Tsallis-INF algorithm, which achieves the optimal regret for both regimes. The Tsallis-INF algorithm also achieves favorable regret guarantees in the _adversarial regime with a self-bounding constraint_, which interpolates between the stochastic and adversarial regimes.

To realize the aforementioned adaptivity in FTRL, the _adaptive learning rate_ (a.k.a. time-varying learning rate) is one of the most representative approaches. This approach adjusts the learning rate based on previous observations. In the literature, adaptive learning rates have been designed to depend on _stability_ or _penalty_, which are components of a regret upper bound of FTRL. The stability term increases if the variation of FTRL outputs in the adjacent rounds is large, and stability-dependent learning rates have been used in a considerable number of algorithms available in the literature, _e.g.,_[32; 37; 38] and references therein. In contrast, the penalty term comes from the strength of the regularization, and recently penalty-dependent learning rates were considered to achieve BOBW guarantees [22; 46]. However, existing stability-dependent (resp. penalty-dependent) learning rates are designed with the worst-case penalty (resp. stability), which could potentially limit the adaptivity and performance of FTRL. (There are numerous studies related to this paper and we include additional related work in Appendix C.)

### Contribution of this study

In this paper, in order to further broaden the applicability of FTRL, we establish a generic framework for designing an adaptive learning rate that depends on both the stability and penalty components simultaneously, which we call a _stability-penalty-adaptive (SPA) learning rate_ (Definition 2). This enables us to bound the regret approximately by \(\widetilde{O}\big{(}\sqrt{\sum_{t=1}^{T}z_{t}h_{t+1}}\big{)}\) for stability component \((z_{t})_{t}\) and a penalty component \((h_{t})_{t}\), which we call a _SPA regret bound_ (Theorem 1). With appropriate selections of \(z_{t}\) and \(h_{t}\), this result yields the three important adaptive bounds mentioned earlier, namely sparsity, game-dependency, and BOBW. In particular, our contributions are as follows (see also Tables 1 and 2):* (Section 5.1) We initially provide new algorithms for sparse MABs as preliminaries for establishing a BOBW algorithm with a sparsity-dependent bound. In Section 5.1.1, we propose a novel estimator of the sparsity level, which is linked to a stability component and induces \(L_{2}=\sum_{t=1}^{T}\lVert\ell_{t}\rVert^{2}\leq sT\). We demonstrate that a learning rate using this estimator with the Shannon entropy regularizer and \(\widetilde{\Theta}((kT)^{-2/3})\) uniform exploration immediately results in an \(O(\sqrt{L_{2}\log k})\) regret bound for \(\ell_{t}\in[0,1]^{k}\). In Section 5.1.2, we investigate possibly negative losses \(\ell_{t}\in[-1,1]^{k}\). We employ the time-invariant log-barrier proposed in [10] to control the stability term. This allows us to achieve an \(O(\sqrt{L_{2}\log k})\) regret bound for losses in \([-1,1]^{k}\) even _without_ the \(\widetilde{\Theta}((kT)^{-2/3})\) uniform exploration. This is a key component for developing the BOBW guarantee that we discuss next. Note that Section 5.1 serves as preliminary findings for the subsequent section.
* (Section 5.2) We establish a BOBW algorithm with a sparsity-dependent bound. In order to achieve this goal, we make another major technical development: we analyze the variation in the FTRL output when the regularizer changes (Lemma 7), which holds thanks to the time-invariant log-barrier and may be of independent interest. This analysis is necessary since we use a time-varying learning rate to obtain a BOBW guarantee, whereas Bubeck et al. [10] uses a constant learning rate. This technical development successfully allows us to achieve the goal (Theorem 4) in combination with the SPA learning rate developed in Section 4 and a technique for exploiting sparsity in Section 5.1.2.
* (Section 6) We show that the SPA learning rate established in Section 4 can also be used to achieve a game-dependent bound and a BOBW guarantee simultaneously, which further highlights the usefulness of the SPA learning rate.

## 2 Setup

This section introduces the preliminaries of this study. Sections 2.1 and 2.2 formulate the MAB and PM problems, respectively, and Section 2.3 defines regimes considered in this paper.

\begin{table}
\begin{tabular}{l l l l l} \hline \hline Reference & \(s\)-agnostic? & Range of \(\ell_{ti}\) & Regime & Regret bound \\ \hline Kwon and Perchet [27] & – & \([0,1]\) & Adv. & \(\Omega(\sqrt{sT})\) \\ \hline Kwon and Perchet [27] & No & \([0,1]\) & Adv. & \(2\sqrt{e}\sqrt{sT\log(k/s)}\) \\
**Ours (Sec. 5.1.1, Cor. 2)** & Yes & \([0,1]\) & Adv. & \(2\sqrt{2}\sqrt{L_{2}\log k}+O((kT\log k)^{1/3})\) \\ \hline Bubeck et al. [10] & No & \([-1,1]\) & Adv. & \(10\sqrt{L_{2}\log k}+20k\log T\) \\
**Ours (Sec. 5.1.2, Cor. 3)** & Yes & \([-1,1]\) & Adv. & \(4\sqrt{2}\sqrt{L_{2}\log k}+2k\log T\) \\
**Ours (Sec. 5.2, Thm. 4)** & Yes & \([-1,1]\) & Adv. & \(4\sqrt{L_{2}\log k\log T}+O(k\log T)\) \\  & & & Stoc. & \(O(s\log(T)\log(kT)/\Delta_{\min})\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Regret upper bounds with sparsity-dependent bounds in multi-armed bandits. \(T\) is the time horizon. \(s\leq k\) is the level of sparsity in losses. Let \(L_{2}=\sum_{t=1}^{T}\lVert\ell_{t}\rVert^{2}\), and \(\lVert\ell_{t}\rVert_{0}\leq s\) implies \(L_{2}=\sum_{t=1}^{T}\lVert\ell_{t}\rVert^{2}\leq sT\) since \(\lVert\ell_{t}\rVert_{\infty}\leq 1\). \(\Delta_{\min}\) is the minimum suboptimality gap. Adv. and Stoc. are the abbreviations of the adversarial and stochastic regime, respectively.

\begin{table}
\begin{tabular}{l l l l} \hline \hline Reference & Game-dependent? & BOBW? & Order of regret bound \\ \hline Many existing studies on PM & No & No & – \\ Lattimore and Szepesvari [32] & Yes & No & \(\sqrt{\sum_{t=1}^{T}V_{t}\log k}\) \\ Tsuchiya et al. [46] & No (only game-class-dependent) & Yes & \(\sqrt{\bar{V}\sum_{t=1}^{T}H(q_{t+1})}\) \\
**Ours (Sec. 6, Cor. 5)** & Yes & Yes & \(\sqrt{\sum_{t=1}^{T}V_{t}^{\prime}H(q_{t+1})\log T}\) \\ \hline \hline \end{tabular}
\end{table}
Table 2: Regret bounds for non-degenerate local PM games. \(V_{t}\), \(V_{t}^{\prime}\), and \(\bar{V}^{\prime}\) are game-dependent quantities satisfying \(V_{t}\leq V_{t}^{\prime}\leq\bar{V}\) (see Section 6 and Appendix B for definitions). \(H(q_{t})\) is the Shannon entropy for FTRL output \(q_{t}\).

NotationLet \(\|x\|\), \(\|x\|_{1}\), and \(\|x\|_{\infty}\) be the Euclidian, \(\ell_{1}\)-, and \(\ell_{\infty}\)-norms for a vector \(x\), respectively. Let \(\|x\|_{0}\) be the number of non-zero elements for a vector \(x\). Let \(\mathcal{P}_{k}=\{p\in[0,1]^{k}:\|p\|_{1}=1\}\) be the \((k-1)\)-dimensional probability simplex. A vector \(e_{i}\in\{0,1\}^{k}\) is the \(i\)-th standard basis of \(\mathbb{R}^{k}\), and \(\mathbf{1}\) is the all-one vector. Let \(D_{\Phi}\) be the _Bregman divergence_ induced by differentiable convex function \(\Phi\), _i.e.,_\(D_{\Phi}(p,q)=\Phi(p)-\Phi(q)-\langle\nabla\Phi(q),p-q\rangle\). Table 3 in Appendix A summarizes the notation used in this paper.

### Multi-armed bandits

In MAB with \(k\)-arms, at each round \(t\in[T]\coloneqq\{1,2,\ldots,T\}\), the environment determines the loss vector \(\ell_{t}=(\ell_{t1},\ell_{t2},\ldots,\ell_{tk})^{\top}\) in \([0,1]^{k}\) or \([-1,1]^{k}\), and the learner simultaneously chooses an arm \(A_{t}\in[k]\) without knowing \(\ell_{t}\). After that, the learner observes only the loss \(\ell_{tA_{t}}\) for the chosen arm. The performance of the learner is evaluated by the regret \(\text{Reg}_{T}\), which is the difference between the cumulative loss of the learner and of the single optimal arm, that is, \(a^{*}=\arg\min_{a\in[k]}\mathbb{E}\big{[}\sum_{t=1}^{T}\ell_{ta}\big{]}\) and \(\text{Reg}_{T}=\mathbb{E}\big{[}\sum_{t=1}^{T}(\ell_{tA_{t}}-\ell_{ta^{*}}) \big{]},\) where the expectation is taken with respect to the internal randomness of the algorithm and the randomness of the loss vectors \((\ell_{t})_{t=1}^{T}\).

### Partial monitoring

FormulationA PM game \(\mathcal{G}=(\mathcal{L},\Phi)\) with \(k\)-actions and \(d\)-outcomes is defined by a pair of a loss matrix \(\mathcal{L}\in[0,1]^{k\times d}\) and feedback matrix \(\Phi\in\Sigma^{k\times d}\), where \(\Sigma\) is a set of feedback symbols. The game is played in a sequential manner by a learner and an opponent across \(T\) rounds. The learner begins the game with knowledge of \(\mathcal{L}\) and \(\Phi\). For every round \(t\in[T]\), the opponent selects an outcome \(x_{t}\in[d]\), and the learner simultaneously chooses an action \(A_{t}\in[k]\). Then the learner suffers an unobserved loss \(\mathcal{L}_{A_{t}x_{t}}\) and receives only a feedback symbol \(\sigma_{t}=\Phi_{A_{t}x_{t}}\), where \(\mathcal{L}_{ax}\) is the \((a,x)\)-th element of \(\mathcal{L}\). The learner's performance in the game is evaluated by the regret \(\text{Reg}_{T}\) as in the MAB case: \(a^{*}=\arg\min_{a\in[k]}\mathbb{E}\big{[}\sum_{t=1}^{T}\mathcal{L}_{ax_{t}} \big{]}\) and \(\text{Reg}_{T}=\mathbb{E}\big{[}\sum_{t=1}^{T}(\mathcal{L}_{A_{t}x_{t}}- \mathcal{L}_{a^{*}x_{t}})\big{]}=\mathbb{E}\big{[}\sum_{t=1}^{T}\langle\ell_{ A_{t}}-\ell_{a^{*}},e_{x_{t}}\rangle\big{]},\) where \(\ell_{a}\in\mathbb{R}^{d}\) is the \(a\)-th row of \(\mathcal{L}\).

Several concepts in PMLet \(m\leq|\Sigma|\) be the maximum number of distinct symbols in a single row of \(\Phi\in\Sigma^{k\times d}\). Different actions \(a\) and \(b\) are duplicate if \(\ell_{a}=\ell_{b}\). We can decompose possible distributions of \(d\) outcomes in \(\mathcal{P}_{d}\) based on the loss matrix. For every action \(a\in[k]\), cell \(\mathcal{C}_{a}=\{u\in\mathcal{P}_{d}:\max_{b\in[k]}(\ell_{a}-\ell_{b})^{\top} u\leq 0\}\) is the set of probability vectors in \(\mathcal{P}_{d}\) for which action \(a\) is optimal. Each cell is a closed convex polytope.

Define \(\dim(\mathcal{C}_{a})\) as the dimension of the affine hull of \(\mathcal{C}_{a}\). Action \(a\) is said to be dominated if \(\mathcal{C}_{a}=\emptyset\). For non-dominated actions, action \(a\) is said to be Pareto optimal if \(\dim(\mathcal{C}_{a})=d-1\), and degenerate if \(\dim(\mathcal{C}_{a})<d-1\). Let \(\Pi\) be the set of Pareto optimal actions. Two Pareto optimal actions \(a,b\in\Pi\) are called neighbors if \(\dim(\mathcal{C}_{a}\cap\mathcal{C}_{b})=d-2\), which is used to define the difficulty of PM games. A PM game is said to be non-degenerate if it has no degenerate actions. We assume that PM game \(\mathcal{G}\) is non-degenerate and contains no duplicate actions.

The difficulty of PM games is characterized by the following observability conditions. Neighbouring actions \(a\) and \(b\) are locally observable if there exists \(w_{ab}:[k]\times\Sigma\rightarrow\mathbb{R}\) such that \(w_{ab}(c,\sigma)=0\) for \(c\not\in\{a,b\}\) and \(\sum_{c=1}^{k}w_{ab}(c,\Phi_{cx})=\mathcal{L}_{ax}-\mathcal{L}_{bx}\) for all \(x\in[d]\). A PM game is locally observable if all neighboring actions are locally observable, and this study considers locally observable games.

Loss difference estimationLet \(\mathcal{H}\) be the set of all functions from \([k]\times\Sigma\) to \(\mathbb{R}^{d}\). For any locally observable games, there exists \(G\in\mathcal{H}\) such that for any \(b,c\in\Pi\), \(\sum_{a=1}^{k}(G(a,\Phi_{ax})_{b}-G(a,\Phi_{ax})_{c})=\mathcal{L}_{bx}- \mathcal{L}_{cx}\) for all \(x\in[d]\)[32]. For example, we can take \(G=G_{0}\) defined by \(G_{0}(a,\sigma)_{b}=\sum_{e\in\operatorname{path}_{\mathcal{H}}(b)}w_{e}(a,\sigma)\) for \(a\in\Pi\), where \(\mathscr{T}\) is a tree over \(\Pi\) induced by neighborhood relations and \(\operatorname{path}_{\mathscr{T}}(b)\) is the set of edges from \(b\in\Pi\) to an arbitrarily chosen root \(c\in\Pi\) on \(\mathscr{T}\)[32]. See Appendix C and [31, Chapter 37] for a more detailed explanation and background of PM.

### Considered regimes

We consider three regimes on the assumptions for losses in MABs and outcomes in PM. In the _stochastic regime_, a sequence of loss vector \((\ell_{t})\) in MAB and that of outcome vector \((x_{t})\) in PM follow an unknown distribution \(\nu^{*}\) in an i.i.d. manner. Define the minimum suboptimality gap in \(\Delta_{\min}=\min_{a\neq a^{*}}\Delta_{a}\) for \(\Delta_{a}=\mathbb{E}_{\ell_{t}\sim\nu^{*}}\big{[}(\ell_{ta}-\ell_{ta^{*}}) \big{]}\) in MAB and \(\Delta_{a}=\mathbb{E}_{x_{t}\sim\nu^{*}}\big{[}(\ell_{a}-\ell_{a^{*}})^{\top}e _{x_{t}}\big{]}\) in PM. Note that the definitions of \(\ell\) in MAB and PM are different.

In contrast, the _adversarial regime_ does not assume any stochastic structure for the losses or outcomes, and they can be chosen in an arbitrary manner. In this regime, the environment can choose \(\ell_{t}\) for MAB and \(x_{t}\) for PM depending on the past history until the \((t-1)\)-th round, \((A_{s})_{s=1}^{t-1}\).

We also consider, the _adversarial regime with a self-bounding constraint_[53], an intermediate regime between the stochastic and adversarial regimes.

**Definition 1**.: _Let \(\Delta\in[0,2]^{k}\) and \(C\geq 0\). The environment is in an adversarial regime with a \((\Delta,C,T)\) self-bounding constraint if it holds for any algorithm that \(\mathsf{Reg}_{T}\geq\mathbb{E}\big{[}\sum_{t=1}^{T}\Delta_{A_{t}}-C\big{]}\)._

One can see that the stochastic and adversarial regimes are indeed instances of this regime, and that well-known _stochastic regimes with adversarial corruptions_[36] are also in this regime (see [53] and [46] for definitions in MAB and PM, respectively).

We assume that there exists a unique optimal arm (or action) \(a^{*}\), which was employed by many studies aiming at developing BOBW algorithms [18; 34; 49; 53].

## 3 Preliminaries

This section provides preliminaries for developing and analyzing algorithms. We first introduce FTRL, upon which we develop our algorithms, and then describe the self-bounding technique, which is a common technique for proving a BOBW guarantee.

Follow-the-regularized-leaderIn the FTRL framework, an arm selection probability \(p_{t}\in\mathcal{P}_{k}\) at round \(t\) is given by

\[q_{t}=\operatorname*{arg\,min}_{q\in\mathcal{P}_{k}}\left\langle\sum_{s=1}^{t -1}\widehat{y}_{s},\,q\right\rangle+\Phi_{t}(q)\quad\text{and}\quad p_{t}= \mathcal{T}_{t}(q_{t})\,,\] (1)

where \(\widehat{y}_{t}\in\mathbb{R}^{k}\) is an estimator of loss \(\ell_{t}\) at round \(t\), \(\Phi_{t}\colon\mathcal{P}_{k}\to\mathbb{R}\) is a strongly-convex regularizer, and \(\mathcal{T}_{t}\colon\mathcal{P}_{k}\to\mathcal{P}_{k}\) is a map from the output of FTRL \(q_{t}\) to an arm selection probability vector \(p_{t}\).

In the analysis of FTRL, it is common to evaluate \(\sum_{t=1}^{T}\left\langle\widehat{y}_{t},p_{t}-p\right\rangle=\sum_{t=1}^{T} \left\langle\widehat{y}_{t},q_{t}-p\right\rangle+\sum_{t=1}^{T}\left\langle \widehat{y}_{t},p_{t}-q_{t}\right\rangle\) for some \(p\in\mathcal{P}_{k}\). It is known (see _e.g.,_[31; Exercise 28.12]) that quantity \(\sum_{t=1}^{T}\left\langle\widehat{y}_{t},q_{t}-p\right\rangle\) is bounded from above by

\[\sum_{t=1}^{T}(\underbrace{\Phi_{t}(q_{t+1})-\Phi_{t+1}(q_{t+1})}_{\text{ penalty term}})+\Phi_{T+1}(p)-\Phi_{1}(q_{1})+\sum_{t=1}^{T}(\underbrace{\left\langle q_{t}-q_{t+1}, \widehat{y}_{t}\right\rangle-D_{\Phi_{t}}(q_{t+1},q_{t})}_{\text{stability term}})\,.\] (2)

We refer to the terms in (2) as a _penalty_ and _stability_ terms, and to the quantity \(\left\langle\widehat{y}_{t},p_{t}-q_{t}\right\rangle\) as a _transformation_ term. Note that, though this study focuses on examples in which \(\Phi_{T+1}(p)\) is not dominant, this term may be dominant dependent on the choice of regularizers.

Self-bounding techniqueA self-bounding technique is a common method for proving a BOBW guarantee [18; 49; 53]. In the self-bounding technique, we first derive regret upper and lower bounds in terms of a variable dependent on the arm selection probabilities \((p_{t})_{t}\) or the FTRL outputs \((q_{t})_{t}\), and then derive a regret bound by combining the upper and lower bounds. We use a version proposed in [22]. We consider \(Q(i)\), \(\bar{Q}(i)\), \(P(i)\), and \(\bar{P}(i)\) for \(i\in[k]\) defined by \(Q(i)=\sum_{t=1}^{T}(1-q_{ti}),\)\(\bar{Q}(i)=\mathbb{E}\left[Q(i)\right],\)\(P(i)=\sum_{t=1}^{T}(1-p_{ti}),\) and \(\bar{P}(i)=\mathbb{E}\left[P(i)\right].\) Note that \(\bar{Q}(i),\bar{P}(i)\in[0,T]\) for any \(i\in[k]\). In terms of \(\bar{Q}(i)\) or \(\bar{P}(i)\), we can obtain the lower bound of the regret for the adversarial regime with a self-bounding constraint as follows:

**Lemma 1** ([46, Lemma 4]).: _In the adversarial regime with a self-bounding constraint (Definition 1), if there exists \(c^{\prime}\in(0,1]\) such that \(p_{ti}\geq c^{\prime}\,q_{ti}\) for all \(t\in[T]\) and \(i\in[k]\), then \(\mathsf{Reg}_{T}\geq\Delta_{\min}\bar{P}(a^{*})-C\geq c^{\prime}\,\Delta_{\min} \bar{Q}(a^{*})-C\,.\)_

It is known that the sums of the entropy \(H(\cdot)\) of \((p_{t})_{t}\) is bounded by \(P(i)\) as follows:

**Lemma 2** ([22, Lemma 4]).: _Let \((q_{t})_{t=1}^{T}\) be any sequence of probability vectors and define \(Q(i)=\sum_{t=1}^{T}(1-q_{ti})\). Then for any \(i\in[k]\), \(\sum_{t=1}^{T}H(q_{t})\leq Q(i)\log(\mathrm{e}kT/Q(i))\)._

Based on Lemmas 1 and 2, it suffices to show \(\mathsf{Reg}_{T}\lesssim\mathbb{E}\Big{[}\sqrt{\sum_{t=1}^{T}H(q_{t})\, \mathrm{polylog}(T)}\Big{]}\) to prove a BOBW guarantee in MAB. This is because, for the adversarial regime, using \(H(q_{t})\leq\log k\) immediately implies a \(\widetilde{O}(\sqrt{T})\) bound, and for the stochastic regime, using Lemmas 1 and 2 roughly bounds the regret as \(\mathsf{Reg}_{T}=2\mathsf{Reg}_{T}-\mathsf{Reg}_{T}\lesssim\sqrt{Q(a^{*})\, \mathrm{polylog}(T)}-\Delta_{\min}\bar{Q}(a^{*})\lesssim\mathrm{polylog}(T)/ \Delta_{\min}\).

## 4 Stability-penalty-adaptive (SPA) learning rate and regret bound

This section proposes a new adaptive learning rate, which yields a regret upper bound dependent on both the stability component \(z_{t}\) and penalty component \(h_{t}\) for various choices of \(z_{t}\) and \(h_{t}\). When we use a learning rate \(\eta_{t}\), the analysis of FTRL boils down to the evaluation of

\[\widehat{\mathsf{Reg}}_{T}^{\mathsf{SP}}=\sum_{t=1}^{T}\left(\frac{1}{\eta_{ t+1}}-\frac{1}{\eta_{t}}\right)h_{t+1}+\lambda\sum_{t=1}^{T}\eta_{t}z_{t}\quad \text{for some}\quad\lambda>0\,.\] (3)

In particular, when we use the Exp3 algorithm, \(h_{t}\) is the Shannon entropy of the FTRL output at round \(t\). This can be confirmed by checking the existing studies (_e.g.,_[22, 46]) or the proofs in Appendices F, G, H.2, and I. To favorably bound \(\widehat{\mathsf{Reg}}_{T}^{\mathsf{SP}}\), we develop a new learning rate framework, which we call the jointly stability- and penalty-adaptive learning rate, or the _stability-penalty-adaptive (SPA) learning rate_ for short:

**Definition 2** (Stability-penalty-adaptive learning rate).: _Let \(((h_{t},z_{t},\bar{z}_{t}))_{t=1}^{T}\) be non-negative reals such that \(h_{1}\geq h_{t}\) for all \(t\in[T]\), \((\bar{z}_{t}h_{1}+\sum_{s=1}^{t}z_{s}h_{s+1})_{t=1}^{T}\) is non-decreasing, and \(\bar{z}_{t}h_{1}\geq z_{t}h_{t+1}\) for all \(t\in[T]\). Let \(c_{1},c_{2}>0\). Then, a sequence of \((\eta_{t})_{t=1}^{T}\) is a SPA learning rate if it has a form of_

\[\eta_{t}=\frac{1}{\beta_{t}}\,,\quad\beta_{1}>0\,,\quad\text{and}\quad\beta_{ t+1}=\beta_{t}+\frac{c_{1}z_{t}}{\sqrt{c_{2}+\bar{z}_{t}h_{1}+\sum_{s=1}^{t-1}z_{s}h _{s+1}}}\,.\] (4)

**Remark**.: To the best of our knowledge, this is the first learning rate that depends on both the stability and penalty components. Note that when we set the penalties to their worst-case value, that is, \(h_{t}=h_{1}\) for all \(t\in[T]\) (recalling \(h_{t}\leq h_{1}\)), the SPA learning rate in (4) becomes equivalent to the standard type of the learning rate, which depends only on the stability and has the form of \(\beta_{t}=1/\eta_{t}\simeq\frac{c_{1}}{\sqrt{h_{1}}}\sqrt{\bar{z}_{1}+\sum_{s= 1}^{t-1}z_{s}}\). On the other hand, when we set the stabilities to be their worst-case value, that is, \(z\geq\max_{t\in[T]}z_{t}\), the SPA learning rate in (4) corresponds to the learning rate dependent only on the penalty in [22, 46].

Using learning rate \((\eta_{t})\) in (4), we can bound \(\widehat{\mathsf{Reg}}_{T}^{\mathsf{SP}}\) as follows.

**Theorem 1** (Stability-penalty-adaptive regret bound).: _Let \((\eta_{t})_{t=1}^{T}\) be a SPA learning rate in Definition 2. Then \(\widehat{\mathsf{Reg}}_{T}^{\mathsf{SP}}\) in (3) is bounded as follows:_

**(I)** _If \(((h_{t},z_{t},\bar{z}_{t}))_{t=1}^{T}\) in \((\eta_{t})\) satisfies \(\frac{\sqrt{c_{2}+\bar{z}_{t}h_{1}}}{c_{1}}(\beta_{1}+\beta_{t})\geq\varepsilon+ z_{t}\) for all \(t\in[T]\) for some \(\varepsilon>0\) (stability condition (S1)), then_

\[\widehat{\mathsf{Reg}}_{T}^{\mathsf{SP}}\leq 2\left(c_{1}+\frac{\lambda}{c_{1}} \log\left(1+\sum_{u=1}^{T}\frac{z_{u}}{\varepsilon}\right)\right)\sqrt{c_{2}+ \bar{z}_{T}h_{1}+\sum_{t=1}^{T}z_{t}h_{t+1}}\,.\] (5)

**(II)** _If \(h_{t}=h_{1}\) for all \(t\in[T]\), \(c_{2}=0\), and \(((h_{t},z_{t},\bar{z}_{t}))_{t=1}^{T}\) in \((\eta_{t})\) satisfies \(\beta_{t}\geq\frac{ac_{1}}{\sqrt{h_{1}}}\sqrt{\sum_{s=1}^{t}z_{s}}\) for some \(a>0\) (stability condition (S2)), then \(\widehat{\mathsf{Reg}}_{T}^{\mathsf{SP}}\leq 2\left(c_{1}+\frac{\lambda}{ac_{1}} \right)\sqrt{h_{1}\sum_{t=1}^{T}z_{t}}\)._The proof of Theorem 1 can be found in Appendix D. In Part (I) of Theorem 1, we can see that \(\widetilde{\mathsf{Reg}}_{T}^{\mathsf{SP}}\) is bounded by \(\sqrt{\sum_{t=1}^{T}z_{t}h_{t+1}}\), which will enable us to obtain BOBW and data-dependent bounds simultaneously. Note that \(\widetilde{\mathsf{Reg}}_{T}^{\mathsf{SP}}\), which is the component of the regret, often becomes dominant in particular when we use the Shannon entropy regularizer. Thus, checking the stability condition (S1) and applying Theorem 1 to bound \(\widetilde{\mathsf{Reg}}_{T}^{\mathsf{SP}}\) almost complete the regret analysis. In Section 6, we will see that applying Theorem 1 immediately provides a BOBW bound with a game-dependent bound for PM. In contrast, when deriving BOBW with a sparsity-dependent bound for MAB in Section 5, we will develop additional techniques and conduct further analysis, for example, to satisfy the stability condition (S1), making use of the time-invariant log-barrier regularizer.

## 5 Sparsity-dependent bounds in multi-armed bandits

This section establishes several sparsity-dependent bounds. We use the FTRL framework in (1) with the inverse weighted estimator \(\widehat{y}_{t}\in\mathbb{R}^{k}\) given by \(\widehat{y}_{t_{i}}=\ell_{t_{i}}\mathbb{I}\left[A_{t}=i\right]/p_{t_{i}}\). This estimator is common in the literature and is useful for its unbiasedness, _i.e._, \(\mathbb{E}_{A_{t}\sim p_{t}}[\widehat{y}_{t}\,|\,p_{t}]=\ell_{t}\). We first propose algorithms that achieve sparsity-dependent bounds using substibility-dependent learning rates in Section 5.1 as preliminaries for the subsequent section. Following that, in Section 5.2, we establish a BOBW algorithm with a sparsity-dependent bound based on the SPA learning rate. More specific steps are summarized as follows.

* Section 5.1.1 discusses the case of \(\ell_{t}\in[0,1]^{k}\) and shows that appropriately choosing \(z_{t}\) in the SPA learning rate (4) with the Shannon entropy regularizer and \(\widetilde{\Theta}((kT)^{-2/3})\) uniform exploration achieves a \(O(\sqrt{L_{2}\log k})\) regret for \(\ell_{t}\in[0,1]^{k}\) without knowing \(L_{2}\).
* Section 5.1.2 considers the case of \(\ell_{t}\in[-1,1]^{k}\), which is known to be more challenging than \(\ell_{t}\in[0,1]^{k}\). We show that the time-invariant log-barrier enables us to choose a "tighter" \(z_{t}\) in (4), which removes the uniform exploration used in Section 5.1.1. This not only results in the bound of \(O(\sqrt{L_{2}\log k})\) for \(\ell_{t}\in[-1,1]^{k}\) but also becomes one of the key properties to achieve BOBW.
* Section 5.2 presents a BOBW algorithm with a sparsity-dependent bound using the technique developed in Section 5.1 and Theorem 1. While Theorem 1 itself is a strong tool leading directly to the result for PM (Section 6), its application does not lead to the desired bounds. In particular, in this setting the \(\widetilde{O}\big{(}\sqrt{\sum_{t=1}^{T}z_{t}h_{t+1}}\big{)}\) term derived through Theorem 1 does not immediately imply a BOBW guarantee with a sparsity-dependent bound. To solve this problem, we develop a novel technique to analyze _the variation in FTRL outputs \(q_{t}\) in response to the change in a regularizer (Lemma 7)_, and prove a BOBW bound with a sparsity-dependent bound of \(O(\sqrt{L_{2}\log k\log T})\).

### Parameter-agnostic sparsity-dependent bounds

This section establishes \(s\)-agnostic algorithms to achieve sparsity-dependent bounds for the adversarial regime, which are preliminaries for Section 5.2.

5.1.1 \(L_{2}\)-agnostic algorithm with \(O(\sqrt{L_{2}\log k})\) bound for \(\ell_{t}\in[0,1]^{k}\)

Here, we use \(p_{t}=\mathcal{T}_{t}(q_{t})\) for \(\mathcal{T}_{t}(q)=(1-\gamma)q+\frac{\gamma}{k}\mathbf{1}\) and \(\gamma=\frac{k^{1/3}(\log k)^{1/3}}{T^{2/3}}\) and assume \(\gamma\in[0,1/2]\) (this holds when \(T\geq\sqrt{8k\log k}\)), which implies \(2p_{t}\geq q_{t}\). We use the Shannon entropy regularizer \(\Phi_{t}(p)=-\frac{1}{\eta_{t}}H(p)=\frac{1}{\eta_{t}}\psi^{\mathsf{AS}}(p)= \frac{1}{\eta_{t}}\sum_{i=1}^{k}p_{i}\log p_{i}\) with learning rate \(\eta_{t}=1/\beta_{t}\) and

\[\beta_{1}=\frac{2c_{1}}{\sqrt{h_{1}}}\sqrt{\frac{k}{\gamma}}\,,\quad\beta_{t+1 }=\beta_{t}+\frac{c_{1}\omega_{t}}{\sqrt{\log k}\sqrt{\frac{k}{\gamma}+\sum_{s =1}^{t-1}\omega_{s}}}\quad\text{for}\quad\omega_{t}\coloneqq\frac{\ell_{tA_{t} }^{2}}{p_{tA_{t}}}\,,\] (6)

which corresponds to the learning rate in Definition 2 with \(h_{t}\gets H(q_{1})=\log k\), \(z_{t}\leftarrow\omega_{t}\), \(\bar{z}_{t}\gets k/\gamma\), and \(c_{2}\gets 0\). The uniform exploration is used to satisfy stability condition (S2) in Theorem 1, the amount of which is determined by balancing the regret coming from the uniform exploration and stability condition (S2). Theorem 1 immediately gives the following bound.

**Corollary 2**.: _When \(T\geq\sqrt{8k\log k}\), the above algorithm with \(c_{1}=1/\sqrt{2}\) achieves \(\mathsf{Reg}_{T}\leq 2\sqrt{2}\sqrt{L_{2}\log k}+(2\sqrt{2}+1)(kT\log k)^{1/3}\) without knowing \(L_{2}\). In particular, when \(T\geq 7k^{2}/s^{3}\), \(\mathsf{Reg}_{T}\leq(4\sqrt{2}+1)\sqrt{sT\log k}\)._

The proof is given in Appendix F. The most striking feature of the algorithm is its \(L_{2}\) (or \(s\))-agnostic property. This is essentially made possible by the learning rate using the data-dependent quantity \(\omega_{t}\) in (6), which satisfies \(\mathbb{E}\bigg{[}\sqrt{\sum_{t=1}^{T}\omega_{t}}\bigg{]}\leq\sqrt{\sum_{t=1} ^{T}\mathbb{E}[\omega_{t}]}=\sqrt{\sum_{t=1}^{T}\sum_{i=1}^{k}\ell_{ti}^{2}}= \sqrt{L_{2}}.\) The leading constant of the bound is better than the existing bounds, as shown in Table 1, despite its agnostic property. Note that while the first-order bound in [49] implies the above sparsity-dependent bound when \(\ell_{t}\in[0,1]^{k}\), this does not hold when \(\ell_{t}\in[-1,1]\), which we investigate in the following (see Appendix K for details). We will see in Section 5.1.2 that this assumption can be totally removed by adding a time-invariant log-barrier regularization.

1.2 \(L_{2}\)-agnostic algorithm with \(O(\sqrt{L_{2}\log k}+k\log T)\) bound for \(\ell_{t}\in[-1,1]^{k}\)

Here, we consider the case of \(\ell_{t}\in[-1,1]^{k}\). It is worth noting that the negative loss cannot be handled by simply shifting the loss since it removes the sparsity from the losses \((\ell_{t})\); see [10; 27] and Appendix K for further details. We directly use the output \(q_{t}\) as \(p_{t}\), that is, \(p_{t}=q_{t}.\) We use the hybrid regularizer consisting of the negative Shannon entropy and the log-barrier function, \(\Phi_{t}(p)=\frac{1}{\eta_{t}}\psi^{\mathsf{nS}}(p)+2\psi^{\mathsf{LB}}(p),\) where \(\psi^{\mathsf{LB}}(p)=\sum_{i=1}^{k}\log(1/x_{i})\). We use learning rate \(\eta_{t}=1/\beta_{t}\) and

\[\beta_{1}=\frac{c_{1}^{2}}{8h_{1}}\,,\quad\beta_{t+1}=\beta_{t}+\frac{c_{1} \nu_{t}}{\sqrt{\log k}\sqrt{\nu_{t}+\sum_{s=1}^{t-1}\nu_{s}}}\quad\text{for} \quad\nu_{t}\coloneqq\omega_{t}\min\left\{1,\frac{p_{tA_{t}}}{2\eta_{t}} \right\}\,,\] (7)

where \(\omega_{t}\) is defined in (6). Learning rate (7) corresponds to that in Definition 2 with \(h_{t}\gets H(q_{1})=\log k\), \(z_{t}\leftarrow\nu_{t}\), \(\bar{z}_{t}\leftarrow\nu_{t}\), and \(c_{2}\gets 0\). We then have the following bound:

**Corollary 3**.: _If we run the above algorithm with \(c_{1}=\sqrt{2}\), \(\mathsf{Reg}_{T}\leq 4\sqrt{2}\sqrt{L_{2}\log k}+2k\log T+k+1/4,\) which implies that \(\mathsf{Reg}_{T}\leq 4\sqrt{2}\sqrt{sT\log k}+2k\log T+k+1/4.\)_

The proof is given in Appendix G. Corollary 3 removes the assumption of \(T\geq 7k^{2}/s^{3}\) in Corollary 2, and it also improves the leading constant of the regret in [10]. Note that one can prove a bound of the same order, but with a worse leading constant, by setting \(\beta_{1}\geq 15k\) and combining the analysis similar to that in Section 5.1.1 and the stability bound in [10]. We successfully remove the assumption of \(T\geq 7k^{2}/s^{3}\) thanks to the following lemma, which serves as one of the key elements in achieving a BOBW guarantee with a sparsity-dependent bound (The proof is given in Appendix G.):

**Lemma 3** (Stability bound for negative losses).: _Let \(\ell_{t}\in[-1,1]^{k}\) and \(\widehat{y}_{ti}=\ell_{ti}\mathbbm{1}[A_{t}=i]/p_{ti}\) be the inverse weighted estimator. Assume that \(q_{t}\leq\delta p_{t}\) for some \(\delta\geq 1\). Then the stability term of FTRL with the hybrid regularizer \(\Phi_{t}=\frac{1}{\eta_{t}}\psi^{\mathsf{nS}}+2\delta\,\psi^{\mathsf{LB}}\) is bounded as_

\[\langle q_{t}-q_{t+1},\widehat{y}_{t}\rangle-D_{\Phi_{t}}(q_{t+1},q_{t})\leq \delta\eta_{t}\frac{\ell_{tA_{t}}^{2}}{p_{tA_{t}}}\min\left\{1,\frac{p_{tA_{t }}}{2\eta_{t}}\right\}=\delta\eta_{t}\nu_{t}\,.\]

**Remark.** We can observe from Lemma 3 that the stability term is bounded in terms of \(\nu_{t}\), and the most important observation is that this \(\nu_{t}\) is bounded by the inverse of the learning rate \(1/(2\eta_{t})=\beta_{t}/2\), _i.e.,_\(\nu_{t}\leq\beta_{t}/2\). This enables us to guarantee the stability condition (S2) in Theorem 1 without needing to mix the \(\widetilde{\Theta}((kT)^{-2/3})\) uniform exploration used in Section 5.1.1. Moreover, this will be a key property to prove a BOBW with a sparsity-dependent bound in the next section.

As a minor contribution, by directly bounding the stability component, the RHS of Lemma 3 has a smaller leading constant than the bound obtained by using the bound in [10].

### Best-of-both-worlds guarantee with sparsity-dependent bound

Finally, we are ready to establish a BOBW algorithm with a sparsity-dependent bound and derive its regret bound. We use \(p_{t}=\mathcal{T}_{t}(q_{t})=(1-\gamma)q_{t}+\frac{\gamma}{k}\mathbf{1}\) with \(\gamma=\frac{k}{T}\) (_i.e.,_\(\Theta(1/T)\) uniform exploration)and assume \(\gamma\in[0,1/2]\), which implies \(2p_{t}\geq q_{t}\) and \(\nu_{t}\leq T\). We use the hybrid regularizer \(\Phi_{t}=\frac{1}{\eta_{t}}\psi^{\mathsf{s}}+4\psi^{\mathsf{LB}}\) with learning rate \(\eta_{t}=1/\beta_{t}\) and

\[\beta_{1}=15k\,,\ \ \beta_{t+1}=\beta_{t}+\frac{c_{1}\nu_{t}}{\sqrt{81c_{1}^{2} +\nu_{t}h_{t+1}+\sum_{s=1}^{t-1}\nu_{s}h_{s+1}}}\ \ \text{with}\ \ h_{t}=\frac{1}{1-\frac{k}{T}}H(p_{t})\,,\] (8)

where \(\nu_{t}\) is defined in (7). Note that this learning depends on both the stability and penalty components. This corresponds to the SPA learning rate in Definition 2 with \(z_{t}\leftarrow\nu_{t}\), \(\bar{z}_{t}\leftarrow\nu_{t}h_{t+1}/h_{1}\), and \(c_{2}\gets 81c_{1}^{2}\). One can see that stability assumption (S1) in Part (I) of Theorem 1 are satisfied thanks to \(\nu_{t}\leq\beta_{t}\) (see Appendix H for the proof). We then have the following bounds.

**Theorem 4** (BOBW with sparsity-dependent bound).: _Suppose that \(T\geq 2k\). Then the above algorithm with \(c_{1}=\sqrt{2\log{(1+T/\beta_{1})}}\) (Algorithm 2 in Appendix H) achieves_

\[\mathsf{Reg}_{T}\leq 4\sqrt{L_{2}\log(k)\log(1+T)}+O(k\log{T})\]

_in the adversarial regime,_

\[\mathsf{Reg}_{T}=O\Bigg{(}\frac{s\log(T)\log(kT)}{\Delta_{\min}}+\sqrt{\frac{ Cs\log(T)\log(kT)}{\Delta_{\min}}}\Bigg{)}\]

_in the adversarial regime with a \((\Delta,C,T)\) self-bounding constraint, and \(\mathsf{Reg}_{T}=O(\mathbb{E}[\sum_{i=1}^{k}\ell_{ti}^{2}]\log(T)\log(kT)/ \Delta_{\min})\) in the stochastic regime._

The proof is given in Appendix H. This is the first BOBW bound with the sparsity-dependent (\(L_{2}\)-dependent) bound. Note that the bounds in the stochastic regime and the adversarial regime with a self-bounding constraint can also exploit the propoerty of the underlying losses. The bound in the stochastic regime is suboptimal in two respects: its dependence on \(\Delta_{\min}\) and \((\log T)^{2}\). Concurrently, two separate studies improve each suboptimality ([25] for \(\Delta_{\min}\) and [13] for \((\log T)^{2}\)), but it is highly uncertain if we can prove a BOBW with _a sparsity-dependent bound_ based on their approach, and it is an important future work to investigate this problem.

Key elements of the proofIn the following, we describe some key elements of the proof of Theorem 4. We need to solve one remaining technical issue. Using Part (I) of Theorem 1, we can show that the regret is roughly bounded by \(\mathbb{E}\bigg{[}\sqrt{\sum_{t=1}^{T}\nu_{t}h_{t+1}}\bigg{]}\leq\sqrt{\sum_{ t=1}^{T}\mathbb{E}[\nu_{t}h_{t+1}]}\). However, this quantity cannot be straightforwardly bounded since \(h_{t+1}\) depends on \(\nu_{t}\).

To address this issue, we analyze the behavior of arm selection probabilities when the regularizer changes. In particular, we first prove in Lemma 7 that \(h_{t+1}\leq h_{t}+k\left(\beta_{t+1}/\beta_{t}-1\right)h_{t+1}\). This lemma can be proven by a novel analysis evaluating the changes of the FTRL outputs when the learning rate varies (given in Appendices H.1 and H.2) which is not considered and required when we use a time-invariant learning rate (_e.g._, [10]). Using the last inequality, we have \(\sqrt{\sum_{t=1}^{T}\mathbb{E}[\nu_{t}h_{t+1}]}\lesssim\sqrt{\sum_{t=1}^{T} \mathbb{E}[\nu_{t}h_{t}]}+k\sum_{t=1}^{T}\mathbb{E}[\nu_{t}\left(\beta_{t+1}/ \beta_{t}-1\right)h_{t+1}]\lesssim\sqrt{\sum_{t=1}^{T}\mathbb{E}[\nu_{t}h_{t} ]}+k\sum_{t=1}^{T}\mathbb{E}\bigg{[}\sqrt{\sum_{t=1}^{T}\nu_{t}h_{t+1}}\bigg{]}\lesssim \sqrt{\sum_{t=1}^{T}\mathbb{E}[\nu_{t}h_{t}]}+k\sum_{t=1}^{T}\mathbb{E}\bigg{[} \sqrt{\sum_{t=1}^{T}\nu_{t}h_{t+1}}\bigg{]}\lesssim\sqrt{\sum_{t=1}^{T} \mathbb{E}[\nu_{t}h_{t}]}+k,\) which holds thanks again to \(\nu_{t}\leq\beta_{t}\) and based on the fact that \(x\leq\sqrt{a+bx}\) for \(a,b,x>0\) implies \(x\lesssim\sqrt{b}+a\) (here we ignore some logarithmic factors). This combined with the self-bounding technique leads to a BOBW guarantee in the stochastic regime.

ImplementationOne may wonder how to compute \(\beta_{t+1}\) satisfying (8) since \(h_{t+1}=h_{t+1}(\beta_{t+1})\) depends on \(\beta_{t+1}\). In fact, this can be computed by defining \(F_{t}:[\beta_{t},\beta_{t}+T]\rightarrow\mathbb{R}\) as \(F_{t}(\alpha)=\alpha-\Big{(}\beta_{t}+c_{1}\nu_{t}/\sqrt{81c_{1}^{2}+\nu_{t}h_{t +1}(\alpha)+\sum_{s=1}^{t-1}\nu_{s}h_{s+1}}\Big{)}\) and setting \(\beta_{t+1}\) to be a root of \(F_{t}(\alpha)=0\). Such \(\alpha\) can be computed using the bisection method because \(F_{t}\) is continuous (proved in Proposition 2 in Appendix H.3). The detailed discussion can be found in Appendix H.3.

## 6 Best-of-both-worlds with game-dependent bound for partial monitoring

This section discusses the result of a BOBW guarantee with a game-dependent bound for PM. The desired bound is obtained by direct application of the SPA learning rate and Theorem 1, which highlights the usefulness of the SPA learning rate. Due to the space constraints, the background and algorithm are given in Appendix B.

We rely on the Exploration by Optimization (EbO) by Lattimore and Szepesvari [32]. Define

\[\mathsf{ST}(p,G;q_{t},\eta_{t})=\max_{x\in[d]}\left[\frac{(p-q_{t})^{\top} \mathcal{L}e_{x}}{\eta_{t}}+\frac{\mathrm{bias}_{q_{t}}(G;x)}{\eta_{t}}+ \frac{1}{\eta_{t}^{2}}\sum_{a=1}^{k}p_{a}\Psi_{q_{t}}\left(\frac{\eta_{t}G(a, \Phi_{ax})}{p_{a}}\right)\,\right],\] (9)

where \(\eta_{t}\) is a learning rate, \(\Psi_{q}(z)=\langle q,\exp(-z)+z-1\rangle\) and \(\mathrm{bias}_{q}(G;x)=\langle q,\,\mathcal{L}e_{x}-\sum_{a=1}^{k}G(a,\Phi_{ax })\rangle+\max_{c\in\Pi}(\sum_{a=1}^{k}G(a,\Phi_{ax})_{c}-\mathcal{L}_{cx})\) is the bias of the estimator. Note that the first and third terms in (9) correspond to the stability and transformation terms. Then in EbO we choose \((p_{t},G_{t})\in\mathcal{P}_{k}\times\mathcal{H}\) by

\[(p_{t},G_{t})=\operatorname*{arg\,min}_{p\in\mathcal{P}_{k}^{\prime}(q_{t}), \,G\in\mathcal{H}}\mathsf{ST}(p,G;q_{t},\eta_{t})\quad\text{for}\quad\mathcal{ P}_{k}^{\prime}(q)=\{p\in\mathcal{P}_{k}\colon p\geq q/(2k)\}\,\] (10)

where \(\mathcal{P}_{k}^{\prime}(q)\) is the feasible set proposed in [46]. Let \(\mathrm{opt}_{q_{t}}^{\prime}(\eta_{t})\) be the optimal value of the optimization problem and \(V_{t}^{\prime}=\max\{0,\mathrm{opt}_{q_{t}}^{\prime}(\eta_{t})\}\) be its truncation.

**Remark.** It is worth noting that \(V_{t}^{\prime}=\max\{0,\mathrm{opt}_{q_{t}}^{\prime}(\eta_{t})\}\) captures game difficulty the learner is facing as discussed in [32]. As discussed above, \(\mathsf{ST}(p,G)\) in (9), the objective function that determines \(\mathrm{opt}_{q_{t}}^{\prime}(\eta_{t})\), correspond to the components of the regret upper bound of FTRL. Hence, the smaller \(\mathsf{ST}(p,G)\) can become by optimizing \(p\) and \(G\), the smaller the regret can become, and thus \(\mathrm{opt}_{q_{t}}^{\prime}(\eta_{t})\) captures the game difficulty.

Now we are ready to state our result.

**Corollary 5**.: _For any non-degenerate PM games, there exists an algorithm based on a SPA learning rate achieving \(\mathsf{Reg}_{T}=O\big{(}\mathbb{E}\Big{[}\sqrt{\sum_{t=1}^{T}V_{t}^{\prime} \log(k)\log(1+T)}\Big{]}+mk^{2}\sqrt{\log(k)\log(T)}\big{)}\) in the adversarial regime, and \(\mathsf{Reg}_{T}=O(m^{2}k^{4}\log(T)\log(kT)/\Delta_{\min}+\sqrt{Cm^{2}k^{4} \log(T)\log(kT)/\Delta_{\min}}+mk^{2}\sqrt{\log(k)\log(T)})\) in the adversarial regime with a \((\Delta,C,T)\) self-bounding constraint._

An extended result and the proof are given in Appendices B and I, respectively. Recall that \(V_{t}^{\prime}=\max\{0,\mathrm{opt}_{q_{t}}^{\prime}(\eta_{t})\}\) in the bound reflects the difficulty of the game the learner is facing, rather than the worst-case difficulty of the class of the game. The bounds in both regimes are optimal up to logarithmic factors, and further detailed comparisons are given in Table 2 and Appendix B.

## 7 Conclusion and future work

In this paper, we established the stability-penalty-adaptive (SPA) learning rate (Definition 2), which provides the regret upper bound that jointly depends on the stability and penalty components of FTRL (Theorem 1). This learning rate combined with the technique and analysis for bounding stability terms allows us to achieve BOBW and data-dependent bounds (sparsity- and game-dependent bounds) simultaneously in MAB and PM.

There are some remaining questions. First of all, it would be important future direction to apply the SPA learning rate to other online decision-making problems or regularizers. For example, it is as to investigate online learning with feedback graphs [3], in which the Shannon entropy regularizer (or the Tsallis entropy with the exponent larger than \(1-1/\log k\)) is necessary to achieve nearly optimal regret bounds. Another interesting example is to employ the Tsallis entropy as a dominant regularizer in the SPA learning rate, for instance, to improve logarithmic dependences on the regret bounds, while in this paper we only focused on the Shannon entropy. Second, it is an open question whether we can achieve the bound of \(O(\sqrt{sT\log(k/s)})\) in the sparsity-dependent bound without knowing the sparsity level \(s\). Finally, while we only considered PM games with local observability, investigating if the game-dependent bound with the BOBW guarantee is possible for PM with global observability is important future work.

## Acknowledgments and Disclosure of Funding

TT was supported by JST, ACT-X Grant Number JPMJAX210E, Japan and JSPS, KAKENHI Grant Number JP21J21272, Japan. JH was supported by JSPS, KAKENHI Grant Number JP21K11747, Japan.

## References

* Abernethy et al. [2015] Jacob D Abernethy, Chansoo Lee, and Ambuj Tewari. Fighting bandits with a new kind of smoothness. In _Advances in Neural Information Processing Systems_, volume 28, pages 2197-2205, 2015.
* Allenberg et al. [2006] Chamy Allenberg, Peter Auer, Laszlo Gyorfi, and Gyorgy Ottucsak. Hannan consistency in online learning in case of unbounded losses under partial monitoring. In _International Conference on Algorithmic Learning Theory_, pages 229-243, 2006.
* Alon et al. [2015] Noga Alon, Nicolo Cesa-Bianchi, Ofer Dekel, and Tomer Koren. Online learning with feedback graphs: Beyond bandits. In _Proceedings of The 28th Conference on Learning Theory_, volume 40, pages 23-35, 2015.
* Audibert and Bubeck [2009] Jean-Yves Audibert and Sebastien Bubeck. Minimax policies for adversarial and stochastic bandits. In _Conference on Learning Theory_, volume 7, pages 1-122, 2009.
* Auer et al. [2002] Peter Auer, Nicolo Cesa-Bianchi, Yoav Freund, and Robert E Schapire. The nonstochastic multiarmed bandit problem. _SIAM Journal on Computing_, 32(1):48-77, 2002.
* Bartok [2013] Gabor Bartok. A near-optimal algorithm for finite partial-monitoring games against adversarial opponents. In _Proceedings of the 26th Annual Conference on Learning Theory_, volume 30, pages 696-710, 2013.
* Bartok et al. [2011] Gabor Bartok, David Pal, and Csaba Szepesvari. Minimax regret of finite partial-monitoring games in stochastic environments. In _Proceedings of the 24th Annual Conference on Learning Theory_, volume 19, pages 133-154, 2011.
* Bartok et al. [2012] Gabor Bartok, Navid Zolghadr, and Csaba Szepesvari. An adaptive algorithm for finite stochastic partial monitoring. In _the 29th International Conference on Machine Learning_, pages 1-20, 2012.
* Bubeck and Slivkins [2012] Sebastien Bubeck and Aleksandrs Slivkins. The best of both worlds: Stochastic and adversarial bandits. In _Proceedings of the 25th Annual Conference on Learning Theory_, volume 23, pages 42.1-42.23, 2012.
* Bubeck et al. [2018] Sebastien Bubeck, Michael Cohen, and Yuanzhi Li. Sparsity, variance and curvature in multi-armed bandits. In _Proceedings of Algorithmic Learning Theory_, volume 83 of _Proceedings of Machine Learning Research_, pages 111-127, 2018.
* Bubeck et al. [2019] Sebastien Bubeck, Yuanzhi Li, Haipeng Luo, and Chen-Yu Wei. Improved path-length regret bounds for bandits. In _Conference on Learning Theory_, pages 508-528, 2019.
* Cesa-Bianchi et al. [2006] Nicolo Cesa-Bianchi, Gabor Lugosi, and Gilles Stoltz. Regret minimization under partial monitoring. _Mathematics of Operations Research_, 31(3):562-580, 2006.
* Dann et al. [2023] Christoph Dann, Chen-Yu Wei, and Julian Zimmert. A blackbox approach to best of both worlds in bandits and beyond. _arXiv preprint arXiv:2302.09739_, 2023.
* Erez and Koren [2021] Liad Erez and Tomer Koren. Towards best-of-all-worlds online learning with feedback graphs. In _Advances in Neural Information Processing Systems_, volume 34, pages 28511-28521, 2021.
* Foster and Rakhlin [2012] Dean Foster and Alexander Rakhlin. No internal regret via neighborhood watch. In _Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics_, volume 22, pages 382-390, 2012.

* [16] Dylan J Foster, Zhiyuan Li, Thodoris Lykouris, Karthik Sridharan, and Eva Tardos. Learning in games: Robustness of fast convergence. In _Advances in Neural Information Processing Systems_, volume 29, pages 4734-4742, 2016.
* [17] Yoav Freund and Robert E Schapire. A decision-theoretic generalization of on-line learning and an application to boosting. _Journal of Computer and System Sciences_, 55(1):119-139, 1997.
* [18] Pierre Gaillard, Gilles Stoltz, and Tim van Erven. A second-order bound with excess losses. In _Proceedings of The 27th Conference on Learning Theory_, volume 35, pages 176-196, 2014.
* [19] Elad Hazan and Satyen Kale. Better algorithms for benign bandits. _Journal of Machine Learning Research_, 12(4), 2011.
* [20] William W Hogan. Point-to-set maps in mathematical programming. _SIAM review_, 15(3):591-603, 1973.
* [21] Shinji Ito. Parameter-free multi-armed bandit algorithms with hybrid data-dependent regret bounds. In _Proceedings of Thirty Fourth Conference on Learning Theory_, volume 134, pages 2552-2583, 2021.
* [22] Shinji Ito, Taira Tsuchiya, and Junya Honda. Nearly optimal best-of-both-worlds algorithms for online learning with feedback graphs. In _Advances in Neural Information Processing Systems_, volume 35, 2022.
* [23] Shinji Ito, Taira Tsuchiya, and Junya Honda. Adversarially robust multi-armed bandit algorithm with variance-dependent regret bounds. In _Proceedings of Thirty Fifth Conference on Learning Theory_, volume 178, pages 1421-1422, 2022.
* [24] Tiancheng Jin, Longbo Huang, and Haipeng Luo. The best of both worlds: stochastic and adversarial episodic MDPs with unknown transition. In _Advances in Neural Information Processing Systems_, volume 34, pages 20491-20502, 2021.
* [25] Tiancheng Jin, Junyan Liu, and Haipeng Luo. Improved best-of-both-worlds guarantees for multi-armed bandits: Frtl with general regularizers and multiple optimal arms. _arXiv preprint arXiv:2302.13534_, 2023.
* [26] Junpei Komiyama, Junya Honda, and Hiroshi Nakagawa. Regret lower bound and optimal algorithm in finite stochastic partial monitoring. In _Advances in Neural Information Processing Systems_, volume 28, pages 1792-1800, 2015.
* [27] Joon Kwon and Vianney Perchet. Gains and losses are fundamentally different in regret minimization: The sparse case. _Journal of Machine Learning Research_, 17(227):1-32, 2016.
* [28] Joon Kwon, Vianney Perchet, and Claire Vernade. Sparse stochastic bandits. In _Proceedings of the 2017 Conference on Learning Theory_, volume 65 of _Proceedings of Machine Learning Research_, pages 1269-1270, 2017.
* [29] T. L. Lai and Herbert Robbins. Asymptotically efficient adaptive allocation rules. _Advances in Applied Mathematics_, 6(1):4-22, 1985.
* [30] Tor Lattimore and Csaba Szepesvari. Cleaning up the neighborhood: A full classification for adversarial partial monitoring. In _Proceedings of the 30th International Conference on Algorithmic Learning Theory_, volume 98, pages 529-556, 2019.
* [31] Tor Lattimore and Csaba Szepesvari. _Bandit algorithms_. Cambridge University Press, 2020.
* [32] Tor Lattimore and Csaba Szepesvari. Exploration by optimisation in partial monitoring. In _Proceedings of Thirty Third Conference on Learning Theory_, volume 125, pages 2488-2515, 2020.
* [33] Chung-Wei Lee, Haipeng Luo, and Mengxiao Zhang. A closer look at small-loss bounds for bandits with graph feedback. In _Proceedings of Thirty Third Conference on Learning Theory_, volume 125, pages 2516-2564, 2020.

* [34] Haipeng Luo and Robert E. Schapire. Achieving all with no parameters: AdaNormalHedge. In _Proceedings of The 28th Conference on Learning Theory_, volume 40, pages 1286-1304, 2015.
* [35] Haipeng Luo, Chen-Yu Wei, and Kai Zheng. Efficient online portfolio with logarithmic regret. In _Advances in Neural Information Processing Systems_, volume 31, pages 8235-8245, 2018.
* [36] Thodoris Lykouris, Vahab Mirrokni, and Renato Paes Leme. Stochastic bandits robust to adversarial corruptions. In _Proceedings of the 50th Annual ACM SIGACT Symposium on Theory of Computing_, pages 114-122, 2018.
* [37] Brendan McMahan. Follow-the-regularized-leader and mirror descent: Equivalence theorems and L1 regularization. In _Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics_, volume 15, pages 525-533, 2011.
* [38] Francesco Orabona. A modern introduction to online learning. _CoRR_, abs/1912.13213, 2019. URL http://arxiv.org/abs/1912.13213.
* [39] Antonio Piccolboni and Christian Schindelhauer. Discrete prediction games with arbitrary feedback and loss (extended abstract). In _Computational Learning Theory_, pages 208-223, 2001.
* [40] Roman Pogodin and Tor Lattimore. On first-order bounds, variance and gap-dependent bounds for adversarial bandits. In _Proceedings of The 35th Uncertainty in Artificial Intelligence Conference_, volume 115 of _Proceedings of Machine Learning Research_, pages 894-904, 2020.
* [41] Aldo Rustichini. Minimizing regret: The general case. _Games and Economic Behavior_, 29(1):224-243, 1999.
* [42] Aadirupa Saha and Pierre Gaillard. Versatile dueling bandits: Best-of-both world analyses for learning from relative preferences. In _Proceedings of the 39th International Conference on Machine Learning_, volume 162, pages 19011-19026, 2022.
* [43] Yevgeny Seldin and Gabor Lugosi. An improved parametrization and analysis of the EXP3++ algorithm for stochastic and adversarial bandits. In _Proceedings of the 2017 Conference on Learning Theory_, volume 65, pages 1743-1759, 2017.
* [44] Yevgeny Seldin and Aleksandrs Slivkins. One practical algorithm for both stochastic and adversarial bandits. In _Proceedings of the 31st International Conference on Machine Learning_, volume 32 of _Proceedings of Machine Learning Research_, pages 1287-1295, 2014.
* [45] Taira Tsuchiya, Junya Honda, and Masashi Sugiyama. Analysis and design of Thompson sampling for stochastic partial monitoring. In _Advances in Neural Information Processing Systems_, volume 33, pages 8861-8871, 2020.
* [46] Taira Tsuchiya, Shinji Ito, and Junya Honda. Best-of-both-worlds algorithms for partial monitoring. In _Proceedings of The 34th International Conference on Algorithmic Learning Theory_, 2023.
* [47] Taira Tsuchiya, Shinji Ito, and Junya Honda. Further adaptive best-of-both-worlds algorithm for combinatorial semi-bandits. In _Proceedings of The 26th International Conference on Artificial Intelligence and Statistics_, volume 206 of _Proceedings of Machine Learning Research_, pages 8117-8144, 2023.
* [48] Hastagiri P Vanchinathan, Gabor Bartok, and Andreas Krause. Efficient partial monitoring with prior information. In _Advances in Neural Information Processing Systems_, volume 27, pages 1691-1699, 2014.
* [49] Chen-Yu Wei and Haipeng Luo. More adaptive algorithms for adversarial bandits. In _Proceedings of the 31st Conference On Learning Theory_, volume 75, pages 1263-1291, 2018.
* [50] Chen-Yu Wei, Yi-Te Hong, and Chi-Jen Lu. Tracking the best expert in non-stationary stochastic environments. In _Advances in Neural Information Processing Systems_, volume 29, pages 3972-3980, 2016.

* [51] Kai Zheng, Haipeng Luo, Ilias Diakonikolas, and Liwei Wang. Equipping experts/bandits with long-term memory. In _Advances in Neural Information Processing Systems_, volume 32, pages 5929-5939, 2019.
* [52] Alexander Zimin and Gergely Neu. Online learning in episodic Markovian decision processes by relative entropy policy search. In _Advances in Neural Information Processing Systems_, volume 26, pages 1583-1591, 2013.
* [53] Julian Zimmert and Yevgeny Seldin. Tsallis-INF: An optimal algorithm for stochastic and adversarial bandits. _Journal of Machine Learning Research_, 22(28):1-49, 2021.

## Appendix A Notation

Table 3 summarizes the symbols used in this paper.

## Appendix B Detailed background and algorithm description omitted from Section 6

This section supplements the material that was briefly explained in Section 6. We also consider full information (FI) and MAB as well as non-degenerate locally observable PM (PM-local), and let \(\mathcal{M}\) be a such underlying model.

### Exploration by optimization and its extension

Exploration-by-optimizationWe first describe the approach of EbO by Lattimore and Szepesvari [32], which is a strong technique to bound the regret in PM with local observability. The key idea behind EbO is to minimize a part of a regret upper bound of the FTRL with the Shannon entropy. Recall that \(\mathcal{H}\) is the set of all functions from \([k]\times\Sigma\) to \(\mathbb{R}^{d}\). Then in EbO we consider the choice of \((p_{t},G_{t})\in\mathcal{P}_{k}\times\mathcal{H}\) to minimize the sum of the stability and transformation terms for the worst-case outcome given as follows (also defined in (9)):

\[\mathsf{ST}(p,G;q_{t},\eta_{t})=\max_{x\in[d]}\left[\frac{(p-q_{t})^{\top} \mathcal{L}e_{x}}{\eta_{t}}+\frac{\mathrm{bias}_{q_{t}}(G;x)}{\eta_{t}}+\frac{ 1}{\eta_{t}^{2}}\sum_{a=1}^{k}p_{a}\Psi_{q_{t}}\left(\frac{\eta_{t}G(a,\Phi_{ ax})}{p_{a}}\right)\right].\] (11)

Note that the first and third terms in (11) correspond to the stability and transformation terms (divided by the learning rate \(\eta_{t}\)), respectively. We define the optimal value of the optimization problem by

\begin{table}
\begin{tabular}{l l} \hline \hline Symbol & Meaning \\ \hline \(\mathcal{P}_{k}\) & \((k-1)\)-dimensional probability simplex \\ \(T\in\mathbb{N}\) & time horizon \\ \(k\in\mathbb{N}\) & number of arms (or actions) \\ \(A_{t}\in[k]\) & arm (or action) chosen by learner at round \(t\) \\ \hline \(s\leq k\) & \(\max_{k}\lVert\ell_{t}\rVert_{0}\), sparsity level of losses \\ \(L_{2}\) & \(\sum_{t=1}^{T}\lVert\ell_{t}\rVert^{2}\) \\ \hline \(\mathcal{L}\in[0,1]^{k\times d}\) & loss matrix \\ \(\Sigma\) & set of feedback symbols \\ \(\Phi\in\Sigma^{k\times d}\) & feedback matrix \\ \(d\in\mathbb{N}\) & number of outcomes \\ \(m\in\mathbb{N}\) & maximum number of distinct symbols in a single row of \(\Phi\) \\ \(x_{t}\in[d]\) & outcome chosen by opponent at round \(t\) \\ \hline \(q_{t}\in\mathcal{P}_{k}\) & output of FTRL at round \(t\) \\ \(p_{t}\in\mathcal{P}_{k}\) & arm selection probability at round \(t\) \\ \(\Phi_{t}\colon\mathcal{P}_{k}\to\mathbb{R}\) & regularizer of FTRL at round \(t\) \\ \(\eta_{t}=1/\beta_{t}>0\) & learning rate of FTRL at round \(t\) \\ \(\psi^{\mathsf{AS}}\colon\mathbb{R}_{+}^{k}\to\mathbb{R}\) & \(\sum_{i=1}^{k}x_{i}\log x_{i}\), negative Shannon entropy \\ \(\psi^{\mathsf{LB}}\colon\mathbb{R}_{+}^{k}\to\mathbb{R}\) & \(\sum_{i=1}^{k}\log(1/x_{i})\), log-barrier \\ \(\phi^{\mathsf{AS}}\colon\mathbb{R}_{+}\to\mathbb{R}\) & \(x\log x\) \\ \(\phi^{\mathsf{LB}}\colon\mathbb{R}_{+}\to\mathbb{R}\) & \(\log(1/x)\) \\ \hline \(h_{t}\) & penalty component at round \(t\) \\ \(z_{t}\) & stability component at round \(t\) \\ \hline \(\omega_{t}\) & stability component \(z_{t}\) introduced in (6) (Section 5.1.1) \\ \(\nu_{t}\) & stability component \(z_{t}\) introduced in (7) (Section 5.1.2) \\ \(V_{t}^{\prime}\) & stability component \(z_{t}\) introduced in (13) (Appendix B) \\ \hline \(C\geq 0\) & corruption level \\ \hline \hline \end{tabular}
\end{table}
Table 3: Notation\(\operatorname{opt}_{q}(\eta)=\min_{(p,G)\in\mathcal{P}_{k}\times\mathcal{H}} \mathsf{ST}(p,G;q_{t},\eta)\) and its truncation at round \(t\) by \(V_{t}=\max\{0,\operatorname{opt}_{q_{t}}(\eta_{t})\}\) (appeared in Table 2). Note that this optimization problem is convex and can be solved numerically by using standard solvers [32].

Extending exploration-by-optimizationWhile the vanilla EbO is a strong tool to derive a regret bound in PM-local, it only has a guarantee for the adversarial regime. Recall that in the self-bounding technique, we require a lower bound of the regret expressed in terms of \(q_{t}\) (see Lemma 1). However, when we use the vanilla EbO, it may make a certain action selection probability \(p_{ta}\) for some action \(a\) become extremely small even when the output of FTRL \(q_{ta}\) is far from zero [32], which makes it impossible for us to use the self-bounding technique.

To solve this problem, the vanilla EbO was recently extended so that it is applicable to the stochastic regime (and the adversarial regime with a self-bounding constraint) for PM-local [46]. We define \(\mathcal{P}^{\prime}_{k}(q,\mathcal{M})\) for a class of games \(\mathcal{M}\), which is the extended version of \(\mathcal{P}^{\prime}_{k}(q)\) in (10), by

\[\mathcal{P}^{\prime}_{k}(q,\mathcal{M})=\{p\in\mathcal{P}_{k}:\mathsf{cond}(q,\mathcal{M})\}\quad\text{with}\quad\mathsf{cond}(q,\mathcal{M})=\begin{cases} p=q&\text{if $\mathcal{M}$ is FI or MAB}\,\\ p\geq q/(2k)&\text{if $\mathcal{M}$ is PM-local}\,.\end{cases}\]

We then consider the following optimization problem, which can be seen as a slight generalization of the approach developed in [46]:

\[(p_{t},G_{t})=\operatorname*{arg\,min}_{p\in\mathcal{P}^{\prime}_{k}(q_{t}, \mathcal{M}),\,G\in\mathcal{H}}\mathsf{ST}(p,G;q_{t},\eta_{t})\,,\] (12)

where the feasible region \(\mathcal{P}_{k}\) of \(p\) is replaced with \(\mathcal{P}^{\prime}_{k}(q,\mathcal{M})\). We define the optimal value of (12) by \(\operatorname{opt}^{\prime}_{q}(\eta,\mathcal{M})\) and its truncation at round \(t\) by \(V^{\prime}_{t}(\mathcal{M})=\max\{0,\operatorname{opt}^{\prime}_{q}(\eta_{t}, \mathcal{M})\}\). We will abbreviate \(\mathcal{M}\) when it is clear from a context. The following proposition shows that the component of the regret in (9) can be made small enough even if the feasible region is restricted to \(\mathcal{P}^{\prime}_{k}(q,\mathcal{M})\subset\mathcal{P}_{k}\).

**Proposition 1**.: _Let \(\mathcal{M}\) be an underlying model. If \(\mathcal{M}\) is FI, MAB, or PM-local with \(\eta\leq 1/(2mk^{2})\),_

\[\operatorname{opt}^{\prime}_{*}(\eta)\coloneqq\sup_{q\in\mathcal{P}_{k}} \operatorname{opt}^{\prime}_{q}(\eta)\leq\bar{V}(\mathcal{M})\coloneqq \begin{cases}\nicefrac{{1}}{{2}}&\text{if $\mathcal{M}$ is FI}\\ \nicefrac{{k}}{{2}}&\text{if $\mathcal{M}$ is MAB}\\ 3m^{2}k^{3}&\text{if $\mathcal{M}$ is PM-local}\,.\end{cases}\]

One can immediately obtain this result by following the same lines as [32, Propositions 11 and 12] and [46, Lemma 5].

### Algorithm

We use the negative Shanon entropy regularizer \(\Phi_{t}=\frac{1}{\eta_{t}}\psi^{\mathsf{nS}}\) for (1) with learning rate \(\eta_{t}=1/\beta_{t}\) with

\[\beta_{1}=B\sqrt{\frac{\log(1+T)}{\log k}}\quad\text{and}\quad\beta_{t+1}= \beta_{t}+\frac{c_{1}V^{\prime}_{t}}{\sqrt{\bar{V}h_{1}+\sum_{s=1}^{t-1}V^{ \prime}_{s}h_{s+1}}}\,,\] (13)

where \(B=1/2\) for FI, \(B=k/2\) for MAB, and \(B=2mk^{2}\) for PM-local, which corresponds to the learning rate in Definition 2 with \(h_{t}\gets H(q_{t})\), \(z_{t}\gets V^{\prime}_{t}\), \(\bar{z}_{t}\gets 0\), and \(c_{2}\gets 0\). Algorithm 1 summarizes the proposed algorithm.

### Main result

Let \(r_{\mathcal{M}}\) be \(1\) if \(\mathcal{M}\) is FI or MAB, and \(2k\) if \(\mathcal{M}\) is PM-local. Then we have the following bound.

**Corollary 6** (Extended version of Corollary 5).: _Let \(\mathcal{M}\) be FI, MAB, or PM-local. Then the above algorithm with \(c_{1}=\sqrt{\log(1+T)/2}\) (Algorithm 1) achieves_

\[\mathsf{Reg}_{T}\leq\mathbb{E}\Bigg{[}\sqrt{2\sum_{t=1}^{T}V_{t}^{\prime} \log(k)\log(1+T)}\Bigg{]}+O(B\sqrt{\log(k)\log(T)})\]

_in the adversarial regime, and_

\[\mathsf{Reg}_{T}=O\Bigg{(}\frac{r_{\mathcal{M}}\bar{V}\log(T)\log(kT)}{\Delta _{\min}}+\sqrt{\frac{Cr_{\mathcal{M}}\bar{V}\log(T)\log(kT)}{\Delta_{\min}}}+B \sqrt{\log(k)\log(T)}\Bigg{)}\]

_in the adversarial regime with a \((\Delta,C,T)\) self-bounding constraint._

The bound for the adversarial regime with a self-bounding constraint with \(C=0\) yields the bound in the stochastic regime, which is optimal up to logarithmic factors in FI and MAB, and has the same order as the bounds in (46, Theorem 6).

The bound for the adversarial regime has a form similar to [32] and is game-dependent in the sense that it can be bounded by the empirical difficulty \(V_{t}^{\prime}\) of the current game. In addition, we can also obtain the worst-case bound by replacing \(V_{t}^{\prime}\) with its upper bound \(\bar{V}\). This bound is optimal up to \(\log(T)\) factor in FI and \(\log(k)\log(T)\) factor in MAB, and is a factor of \(\sqrt{\log T}\) worse than the best known bound in [32], which can be seen as the cost for the BOBW guarantee (see also Table 2).

## Appendix C Additional related work

This appendix provides additional related work that could not be included in the main body due to the numerous studies related to this paper.

Multi-armed banditsIn the stochastic regime, it is known that the optimal regret is approximately expressed as \(\mathsf{Reg}_{T}=O(k\log T/\Delta_{\min})\)[29]. In the adversarial regime (a.k.a. non-stochastic regime), it is known that the Online Mirror Descent (OMD) framework with the (negative) Tsallis entropy regularizer achieves \(O(\sqrt{kT})\) regret bounds [1, 4], which match the lower bound of \(\Omega(\sqrt{kT})\)[5].

Data-dependent boundIn the adversarial MAB, algorithms with various data-dependent regret bounds have been developed. Typical examples of such bounds are first-order bounds dependent on the cumulative loss and second-order bounds depending on sample variances in losses. Allenberg et al. [2] provided an algorithm with a first-order regret bound of \(O(\sqrt{kL^{*}\log k})\) for \(L^{*}=\min_{a\in A}\sum_{t=1}^{T}\left\langle\ell_{t},a\right\rangle\). Second-order regret bounds have been shown in some studies, _e.g.,_[10, 19, 49], In particular, Bubeck et al. [10] provided the regret bound of \(O(\sqrt{Q_{2}\log k})\) for \(Q_{2}=\sum_{t=1}^{T}\lVert\ell_{t}-\bar{\lVert}\rVert^{2}\). Other examples of data-dependent bounds include path-length bounds in the form of \(O(\sqrt{kV_{1}\log T})\) for \(V_{1}=\sum_{t=1}^{T-1}\lVert\ell_{t}-\ell_{t+1}\rVert_{1}\) as well as a sparsity-dependent bound, which have been investigated [10, 11, 27, 49, 51].

Sparsity-dependent boundThe study on a sparsity-dependent bound was initiated by Kwon and Perchet [27], who showed that when \(\ell_{t}\in[0,1]^{k}\), the OMD with Tsallis entropy can achieve the bound of \(\mathsf{Reg}_{T}\leq 2\sqrt{\mathrm{e}}\sqrt{sT\log(k/s)})\) and prove the matching (up to logarithmic factor) lower bound of \(\mathsf{Reg}_{T}=\Omega(\sqrt{sT})\) when \(T\geq k^{3}/(4s^{2})\). Bubeck et al. [10] also showed that OMD with a hybrid-regularizer consisting of the Shannon entropy and a log-barrier can achieve \(\mathsf{Reg}_{T}\leq 10\sqrt{L_{2}\log k}+20k\log T\) when \(\ell_{t}\in[-1,1]^{k}\). Zheng et al. [51] investigated the sparse MAB problem in the context of the switching regret. Although their result is not directly related to our study, they show that sparsity is useful in some cases. Note that all of these algorithms assume the knowledge of the sparsity level and do not have a BOBW guarantee. The study to exploit the sparsity was investigated also in the stochastic regime by Kwon et al. [28]. However, they define the sparsity level \(s\) by the number of arms with rewards larger than or equal to \(0\) (_i.e.,_ losses smaller than or equal to \(0\)), and hence the definition of sparsity is different from that in our paper.

Adaptive learning rateThe stability-dependent learning rate is quite ubiquitous (see [38] and the references therein). To our knowledge, the literature on the penalty-dependent bound is quite scarce in bandits and considered in the context of BOBW algorithms [22, 46], both of which consider the Shannon entropy regularizer.

Best-of-both-worldsSince the seminal study by Bubeck and Slivkins [9], BOBW algorithms have been developed for many online decision-making problems. Although they have been investigated mainly in the context of an MAB [43, 53], other settings have also been investigated, [18, 24, 49], to name a few.

FTRL and OMD are now one of the most common approaches to achieving a BOBW guarantee owing to the usefulness of the self-bounding technique [18, 49, 53], while the first [9] and earlier work [43, 44] on BOBW do not rely on the technique. Most of the recent algorithms beyond the MAB are also based on FTRL (to name a few, [24, 42, 49]).

Our BOBW algorithm with the sparsity-dependent bound can be seen as one of the studies that aim to achieve BOBW and data-dependent bound simultaneously. There is not so much existing research, and we are only aware of [21, 23, 47, 49]. They consider first-, second-order, and path-length bound, and we are the first to investigate the sparsity-dependent bound in this line of work.

Log-barrier regularizer and hybrid regularizerThe log-barrier regularizer has been used in various studies (to name a few, [14, 16, 35, 40, 49]). The time-invariant log-barrier (a.k.a. constant amount of log-barrier [33]), whose properties are extensively exploited in this paper, was invented by Bubeck et al. [10] and has been used in several subsequent studies [33, 51].

Partial monitoringStarting from the work by Rustichini [41], PM has been investigated in many works in the literature [7, 12, 39]. It is known that all PM games can be classified into four classes based on their minimax regrets [7, 30]. In particular, all PM games fall into trivial, easy, hard, and hopeless games, for which its minimax regrets is \(0\), \(\Theta(\sqrt{T})\), \(\Theta(T^{2/3})\), and \(\Theta(T)\), respectively. PM has also been investigated in both the adversarial and stochastic regimes as for MAB. In the stochastic regime, there are relatively small amount of works [8, 26, 45, 48], some of which are proven to achieve an instance-dependent \(O(\log T)\) regrets for locally or globally observable games. In the adversarial regime, since the development of the FeedExp3 algorithm [12, 39], many algorithms achieving the minimax optimal regret have been developed [6, 7, 15, 31].

## Appendix D Proof of Theorem 1

Proof.: We first prove (5) in Part (I).

Penalty termFirst, we consider the penalty term \(\sum_{t=1}^{T}\left(\frac{1}{\eta_{t+1}}-\frac{1}{\eta_{t}}\right)h_{t+1}\). By the definition of \(\beta_{t}\) in (4),

\[\sum_{t=1}^{T}\left(\frac{1}{\eta_{t+1}}-\frac{1}{\eta_{t}}\right) h_{t+1}=\sum_{t=1}^{T}\left(\beta_{t+1}-\beta_{t}\right)h_{t+1}=\sum_{t=1}^{T} \frac{c_{1}z_{t}h_{t+1}}{\sqrt{c_{2}+\bar{z}_{t}h_{1}+\sum_{s=1}^{t-1}z_{s}h_{ s+1}}}\] \[\leq c_{1}\sum_{t=1}^{T}\frac{z_{t}h_{t+1}}{\sqrt{\sum_{s=1}^{t} z_{s}h_{s+1}}}\leq c_{1}\int_{0}^{\sum_{t=1}^{T}z_{t}h_{t+1}}\frac{1}{\sqrt{x}} \,\mathrm{d}x=2c_{1}\sqrt{\sum_{t=1}^{T}z_{t}h_{t+1}}\,,\] (14)

where the first inequality follows from \(\bar{z}_{t}h_{1}\geq z_{t}h_{t+1}\) and the second inequality follows by Lemma 8 given in Appendix J.

Stability termNext, we consider the stability term \(\sum_{t=1}^{T}\eta_{t}z_{t}\). Using the definition of \(\beta_{t}\) in (4) and defining \(U_{t}=\sqrt{c_{2}+\bar{z}_{t}h_{1}+\sum_{s=1}^{t-1}z_{s}h_{s+1}}\) for \(t\in\{0\}\cup[T]\), we bound \(\beta_{t}\) from below as

\[\beta_{t}=\beta_{1}+\sum_{u=1}^{t-1}\frac{c_{1}z_{u}}{\sqrt{c_{2}+\bar{z}_{u} h_{1}+\sum_{s=1}^{u-1}z_{s}h_{s+1}}}=\beta_{1}+\sum_{u=1}^{t-1}\frac{c_{1}z_{u}}{U _{u}}\geq\beta_{1}+\frac{c_{1}}{U_{T}}\sum_{u=1}^{t-1}z_{u}\,,\] (15)where the inequality follows since \((U_{t})\) is non-decreasing. Using the last inequality, we can bound \(\sum_{t=1}^{T}\eta_{t}z_{t}\) as

\[\sum_{t=1}^{T}\eta_{t}z_{t}=2\sum_{t=1}^{T}\frac{z_{t}}{\beta_{t}+ \beta_{t}}\leq 2\sum_{t=1}^{T}\frac{z_{t}}{\beta_{t}+\beta_{1}+\frac{c_{1}}{U_{T}} \sum_{s=1}^{t-1}z_{s}}=\frac{2U_{T}}{c_{1}}\sum_{t=1}^{T}\frac{z_{t}}{\frac{U_ {T}}{c_{1}}\left(\beta_{t}+\beta_{1}\right)+\sum_{s=1}^{t-1}z_{s}}\,.\] (16)

Since we have \(\frac{U_{T}}{c_{1}}(\beta_{1}+\beta_{t})\geq\frac{\sqrt{c_{2}+z_{t}h_{1}}}{c_ {1}}(\beta_{1}+\beta_{t})\geq\varepsilon+z_{t}\) by the assumption, a part of the last inequality is further bounded as

\[\sum_{t=1}^{T}\frac{z_{t}}{\frac{U_{T}}{c_{1}}\left(\beta_{t}+ \beta_{1}\right)+\sum_{s=1}^{t-1}z_{s}}\leq\sum_{t=1}^{T}\frac{z_{t}}{ \varepsilon+\sum_{s=1}^{t}z_{s}}\leq\int_{\varepsilon}^{\varepsilon+\sum_{t=1} ^{T}z_{t}}\frac{1}{x}\,\mathrm{d}x\leq\log\left(1+\sum_{t=1}^{T}\frac{z_{t}}{ \varepsilon}\right)\,,\] (17)

where the second inequality follows by Lemma 8. Combining (16) and (17) yields

\[\sum_{t=1}^{T}\eta_{t}z_{t}\leq\frac{2U_{T}}{c_{1}}\log\left(1+ \sum_{t=1}^{T}\frac{z_{t}}{\varepsilon}\right)=\frac{2}{c_{1}}\log\left(1+\sum _{t=1}^{T}\frac{z_{t}}{\varepsilon}\right)\sqrt{c_{2}+\bar{z}_{T}h_{1}+\sum_{ t=1}^{T}z_{t}h_{t+1}}\,.\] (18)

Combining (14) and (18) completes the proof of (5) in Part (I).

We next prove Part (II). For the penalty term, setting \(h_{t}=h_{1}\) for all \(t\in[T]\) in (14) gives

\[\sum_{t=1}^{T}\left(\frac{1}{\eta_{t+1}}-\frac{1}{\eta_{t}}\right)h_{t+1} \leq 2c_{1}\sqrt{h_{1}\sum_{t=1}^{T}z_{t}}\,.\]

For the stability term, since there exists \(a>0\) such that \(\beta_{t}\geq\frac{ac_{1}}{\sqrt{h_{1}}}\sqrt{\sum_{s=1}^{t}z_{s}}\) for any \(t\in[T]\) by the assumption,

\[\sum_{t=1}^{T}\eta_{t}z_{t}=\sum_{t=1}^{T}\frac{z_{t}}{\beta_{t}} \leq\frac{\sqrt{h_{1}}}{ac_{1}}\sum_{t=1}^{T}\frac{z_{t}}{\sqrt{\sum_{s=1}^{t} z_{s}}}\leq\frac{2}{ac_{1}}\sqrt{h_{1}\sum_{t=1}^{T}z_{t}}\,.\]

Summing up the above arguments completes the proof of Part (II). 

## Appendix E Basic facts to bound stability terms

Here, we introduce basic facts, which are useful to bound the stability term. We have

\[\xi(x) \coloneqq\exp(-x)+x-1\leq\begin{cases}\frac{1}{2}x^{2}&\text{ for }x\geq 0\\ x^{2}&\text{ for }x\geq-1\,,\end{cases}\] (19) \[\zeta(x) \coloneqq x-\log(1+x)\leq x^{2}\quad\text{for }x\in\left[- \frac{1}{2},\frac{1}{2}\right]\,.\] (20)

We also have the following inequalities for \(\phi^{\text{\tiny H5}}(x)=x\log x\) and \(\phi^{\text{\tiny L8}}(x)=\log(1/x)\), which are components of the negative Shannon entropy and log-barrier function:

\[\max_{y\in\mathbb{R}}\big{\{}a(x-y)-D_{\phi^{\text{\tiny H5}}}(y,x)\big{\}} =x\xi(a) \text{for }a\in\mathbb{R}\,,\] (21) \[\max_{y\in\mathbb{R}}\big{\{}a(x-y)-D_{\phi^{\text{\tiny L8}}}(y,x)\big{\}} =\zeta(ax) \text{for }a\geq-\frac{1}{x}\,.\] (22)

It is easy to prove these facts by the standard calculus and you can find the proofs of (21) and (22) in Lemma 15 of Tsuchiya et al. [46] and Lemma 5 of Ito et al. [23], respectively.

Proof of Corollary 2

Let \(\mathsf{Reg}_{T}(a)=\mathbb{E}\big{[}\sum_{t=1}^{T}(\ell_{tA_{t}}-\ell_{ta}) \big{]}\) for \(a\in[k]\). Here we provide the complete proof of Corollary 2.

Proof of Corollary 2.: Fix \(i^{*}\in[k]\). Since \(p_{t}=(1-\gamma)q_{t}+\frac{\gamma}{k}\mathbf{1}\), it holds that

\[\mathsf{Reg}_{T}(i^{*}) =\mathbb{E}\Bigg{[}\sum_{t=1}^{T}\ell_{tA_{t}}-\sum_{t=1}^{T}\ell _{ti^{*}}\Bigg{]}=\mathbb{E}\Bigg{[}\sum_{t=1}^{T}\left\langle\ell_{t},p_{t}-e_ {i^{*}}\right\rangle\Bigg{]}\]

where the last inequality follows by \(\mathbb{E}[\widehat{y}_{t}\mid q_{t}]=\ell_{t}\) and the Cauchy-Schwarz inequality. Then, using the standard analysis of the FTRL described in Section 3, the first term in the last inequality is bounded as

\[\sum_{t=1}^{T}\left\langle\widehat{y}_{t},q_{t}-e_{i^{*}}\right\rangle\leq \sum_{t=1}^{T}\left(\frac{1}{\eta_{t+1}}-\frac{1}{\eta_{t}}\right)H(q_{t+1})+ \frac{H(q_{1})}{\eta_{1}}+\sum_{t=1}^{T}\left(\left\langle q_{t}-q_{t+1}, \widehat{y}_{t}\right\rangle-D_{\Phi_{t}}(q_{t+1},q_{t})\right)\,.\]

By (19) and (21) given in Appendix E, the stability term \(\left\langle q_{t}-q_{t+1},\widehat{y}_{t}\right\rangle-D_{\Phi_{t}}(q_{t+1}, q_{t})\) in the last inequality is bounded as

\[\left\langle q_{t}-q_{t+1},\widehat{y}_{t}\right\rangle-D_{\Phi_{ t}}(q_{t+1},q_{t})=\left\langle q_{t}-q_{t+1},\widehat{y}_{t}\right\rangle- \frac{1}{\eta_{t}}D_{\psi^{\mathrm{dS}}}(q_{t+1},q_{t})\] \[=\sum_{i=1}^{k}\left(\widehat{y}_{ti}(q_{ti}-q_{t+1,i})-\frac{1}{ \eta_{t}}D_{\phi^{\mathrm{dS}}}(q_{t+1,i},q_{ti})\right)\] \[\leq\sum_{i=1}^{k}\frac{1}{\eta_{t}}q_{ti}\,\xi\left(\eta_{t} \widehat{y}_{ti}\right)\leq\frac{1}{2}\eta_{t}\sum_{i=1}^{k}q_{ti}\widehat{y}_ {ti}^{2}\leq\eta_{t}\omega_{t}\,,\]

where the first inequality follows from (21), the second inequality follows by (19) with \(\widehat{y}_{t}\geq 0\), and the last inequality holds since \(\sum_{i=1}^{k}q_{ti}\widehat{y}_{ti}^{2}\leq\sum_{i=1}^{k}2p_{ti}\widehat{y}_ {ti}^{2}=2\omega_{t}\).

We will confirm that the assumptions for Part (I) of Theorem 1 are indeed satisfied. Using the definition of \(\beta_{t}\) in (6), We have

\[\beta_{t} =\beta_{1}+\frac{1}{\sqrt{h_{1}}}\sum_{u=1}^{t-1}\frac{c_{1} \omega_{u}}{\sqrt{\frac{k}{\gamma}+\sum_{s=1}^{u-1}\omega_{s}}}=\beta_{1}+ \frac{2c_{1}}{\sqrt{h_{1}}}\sum_{u=1}^{t-1}\frac{\omega_{u}}{\sqrt{\frac{k}{ \gamma}+\sum_{s=1}^{u-1}\omega_{s}}+\sqrt{\frac{k}{\gamma}+\sum_{s=1}^{u-1} \omega_{s}}}\] \[\geq\beta_{1}+\frac{2c_{1}}{\sqrt{h_{1}}}\sum_{u=1}^{t-1}\frac{ \omega_{u}}{\sqrt{\frac{k}{\gamma}+\sum_{s=1}^{u}\omega_{s}}+\sqrt{\frac{k}{ \gamma}+\sum_{s=1}^{u-1}\omega_{s}}}\] \[=\beta_{1}+\frac{2c_{1}}{\sqrt{h_{1}}}\sum_{u=1}^{t-1}\left( \sqrt{\frac{k}{\gamma}+\sum_{s=1}^{u}\omega_{s}}-\sqrt{\frac{k}{\gamma}+\sum_ {s=1}^{u-1}\omega_{s}}\right)\] \[=\beta_{1}+\frac{2c_{1}}{\sqrt{h_{1}}}\left(\sqrt{\frac{k}{ \gamma}+\sum_{s=1}^{t-1}\omega_{s}}-\sqrt{\frac{k}{\gamma}}\right)\geq\frac{2 c_{1}}{\sqrt{h_{1}}}\sqrt{\sum_{s=1}^{t}\omega_{s}}\,,\]

where the last inequality follows since \(\beta_{1}=\frac{2c_{1}}{\sqrt{h_{1}}}\sqrt{\frac{k}{\gamma}}\) and \(\frac{k}{\gamma}\geq\omega_{t}\). Hence, stability condition (S2) in Theorem 1 is satisfied with \(a=2\), and one can see that the other assumptions are trivially satisfied. Hence, by Part (I) of Theorem 1,

\[\sum_{t=1}^{T}\left\langle\widehat{y}_{t},q_{t}-e_{i^{*}}\right\rangle\leq\sum_ {t=1}^{T}\left(\frac{1}{\eta_{t+1}}-\frac{1}{\eta_{t}}\right)H(q_{t+1})+\sum_ {t=1}^{T}\eta_{t}\omega_{t}+\frac{H(q_{1})}{\eta_{1}}\leq 2\left(c_{1}+\frac{1}{2c_{1}} \right)\sqrt{h_{1}\sum_{t=1}^{T}\omega_{t}}+\frac{\log k}{\eta_{1}}\,,\]where in the last inequality we used \(h_{1}\leq\log k\). Now,

\[\mathbb{E}\left[\sqrt{\sum_{t=1}^{T}\omega_{t}}\right]\leq\sqrt{\sum_{t=1}^{T} \mathbb{E}[\omega_{t}]}=\sqrt{\sum_{t=1}^{T}\mathbb{E}\bigg{[}\frac{\ell_{tA_{t} }^{2}}{p_{tA_{t}}}\bigg{]}}=\sqrt{\sum_{t=1}^{T}\sum_{i=1}^{k}\ell_{ti}^{2}}= \sqrt{\sum_{t=1}^{T}\lVert\ell_{t}\rVert_{2}^{2}}=\sqrt{L_{2}}\,.\]

Summing up the above arguments and setting \(c_{1}=1/\sqrt{2}\), we have

\[\mathsf{Reg}_{T}\leq 2\sqrt{2}\sqrt{L_{2}\log k}+\frac{\log k}{\eta_{1}}+ \gamma T=2\sqrt{2}\sqrt{L_{2}\log k}+(\sqrt{2}+1)(kT\log k)^{1/3}\,,\]

which completes the proof of Corollary 2. 

## Appendix G Proof of Corollary 3

We first prove Lemma 3.

Proof of Lemma 3.: Recall that \(\Phi_{t}(p)=\frac{1}{\eta_{t}}\psi^{\mathsf{nS}}(p)+2\delta\psi^{\mathsf{LB}}(p)\). Since \(D_{\Phi_{t}}=\frac{1}{\eta_{t}}D_{\psi^{\mathsf{nS}}}+2\delta D_{\psi^{ \mathsf{nS}}}\) and \(D_{\psi^{\mathsf{nS}}}(x,y)=\sum_{i=1}^{k}D_{\phi^{\mathsf{nS}}}(x_{i},y_{i})\) and \(D_{\psi^{\mathsf{LB}}}(x,y)=\sum_{i=1}^{k}D_{\phi^{\mathsf{LB}}}(x_{i},y_{i})\), we can bound the stability term as

\[\langle q_{t}-q_{t+1},\widehat{y}_{t}\rangle-D_{\Phi_{t}}(q_{t+1 },q_{t})\leq\langle q_{t}-q_{t+1},\widehat{y}_{t}\rangle-\max\left\{\frac{1}{ \eta_{t}}D_{\psi^{\mathsf{nS}}}(q_{t+1},q_{t}),\,2\delta D_{\psi^{\mathsf{LB}} }(q_{t+1},q_{t})\right\}\] \[\leq\sum_{i=1}^{k}\left(\widehat{y}_{ti}(q_{ti}-q_{t+1,i})-\max \left\{\frac{1}{\eta_{t}}D_{\phi^{\mathsf{nS}}}(q_{t+1,i},q_{ti}),\,2\delta D_ {\phi^{\mathsf{LB}}}(q_{t+1,i},q_{ti})\right\}\right)\] \[\leq\sum_{i=1}^{k}\min\left\{\frac{1}{\eta_{t}}q_{ti}\,\xi\left( \eta_{t}\widehat{y}_{ti}\right),\,2\delta\,\zeta\left(\frac{1}{2\delta}q_{ti} \widehat{y}_{ti}\right)\right\}\,,\] (23)

where in the last inequality we used (21) and (22) with

\[\frac{\widehat{y}_{ti}}{2\delta}\geq-\frac{1}{2\delta p_{ti}}\geq-\frac{1}{2 \delta(q_{ti}/\delta)}\geq-\frac{1}{q_{ti}}\,,\]

where the first inequality follows by the definition of \(\widehat{y}_{t}\) and the second inequality follows by \(p_{ti}\geq q_{ti}/\delta\).

Next, we will prove that for any \(i\in[k]\),

\[\min\left\{\frac{1}{\eta_{t}}q_{ti}\,\xi\left(\eta_{t}\widehat{y}_{ti}\right),\,2\delta\,\zeta\left(\frac{1}{2\delta}q_{ti}\widehat{y}_{ti}\right)\right\} \leq\delta\eta_{t}\frac{\ell_{ti}^{2}}{p_{ti}}\min\left\{1,\frac{p_{ti}}{2\eta _{t}}\right\}\mathbbm{1}[A_{t}=i]\,.\] (24)

Fix \(i\in[k]\). By \(q_{ti}\leq\delta p_{ti}\),

\[\frac{1}{2\delta}q_{ti}\widehat{y}_{ti}=\frac{1}{2}p_{ti}\widehat{y}_{ti}\leq \frac{1}{2}\,.\]

Using this and \(\zeta(x)\leq x^{2}\) for \(x\in[-\frac{1}{2},\frac{1}{2}]\) in (20), we have for any \(p_{ti}\in[0,1]\) that

\[2\delta\,\zeta\left(\frac{1}{2\delta}q_{ti}\widehat{y}_{ti}\right)\leq 2\delta \left(\frac{1}{2\delta}q_{ti}\widehat{y}_{ti}\right)^{2}\leq\frac{\delta}{2} \ell_{ti}^{2}\mathbbm{1}[A_{t}=i]\,,\] (25)

where in the last inequality we used \(q_{ti}\leq\delta p_{ti}\). In particular, when \(p_{ti}\leq\eta_{t}\), _i.e._, the probability of selecting arm \(i\) is small to some extent, the last inequality can be further bounded as

\[2\delta\,\zeta\left(\frac{1}{2\delta}q_{ti}\widehat{y}_{ti}\right)\leq\frac{ \eta_{t}}{p_{ti}}\frac{\delta}{2}\ell_{ti}^{2}\mathbbm{1}[A_{t}=i]\leq\delta \eta_{t}\frac{\ell_{ti}^{2}}{p_{ti}}\mathbbm{1}[A_{t}=i]\,.\] (26)

On the other hand when \(p_{ti}>\eta_{t}\), we have \(\eta_{t}\widehat{y}_{ti}\geq-1\). Hence, by the inequality \(\xi(x)\leq x^{2}\) for \(x\geq-1\) in (19),

\[\frac{1}{\eta_{t}}q_{ti}\xi\left(\eta_{t}\widehat{y}_{ti}\right)\leq\frac{1}{ \eta_{t}}\delta p_{ti}(\eta_{t}\widehat{y}_{ti})^{2}=\delta\eta_{t}\frac{\ell_ {ti}^{2}}{p_{ti}}\mathbbm{1}[A_{t}=i]\,.\] (27)

Hence, combining (25), (26), and (27) completes the proof of (24). Finally, by combining (23) and (24) we completes the proof of Lemma 3.

**Remark**.: When \(\ell_{t}\) can be negative, the Shannon entropy regularizer alone cannot bound the stability term if the arm selection probability is small, _i.e._, \(p_{t}\leq\eta_{t}\). Introducing a time-invariant log-barrier regularizer enables us to bound the stability term even when the arm selection probability is small. This idea was proposed by Bubeck et al. [10], who analyzed the variation of arm selection probability for the change of cumulative losses. Unlike their analysis, our proof directly analyses the stability term, enabling us to obtain the tighter regret bound. More importantly, we will utilize the property \(\nu_{t}\leq O(1/\eta_{t})\) many times, which directly follows from Lemma 3, in the subsequent sections to prove the BOBW guarantee with the sparsity-dependent bound.

Now, we are ready to prove Corollary 3.

Proof of Corollary 3.: Fix \(i^{*}\in[k]\). Define \(p^{*}\in\mathcal{P}_{k}\) by

\[p^{*}=\left(1-\frac{k}{T}\right)e_{i^{*}}+\frac{1}{T}\mathbf{1}\,.\]

Then, using the definition of the algorithm,

\[\mathsf{Reg}_{T}(i^{*}) =\mathbb{E}\Bigg{[}\sum_{t=1}^{T}\ell_{tA_{t}}-\sum_{t=1}^{T} \ell_{ti^{*}}\Bigg{]}=\mathbb{E}\Bigg{[}\sum_{t=1}^{T}\left\langle\ell_{t},p_{ t}-e_{i^{*}}\right\rangle\Bigg{]}\] \[=\mathbb{E}\Bigg{[}\sum_{t=1}^{T}\left\langle\ell_{t},p_{t}-p^{* }\right\rangle\Bigg{]}+\mathbb{E}\Bigg{[}\sum_{t=1}^{T}\left\langle\ell_{t},p ^{*}-e_{i^{*}}\right\rangle\Bigg{]}\] \[\leq\mathbb{E}\Bigg{[}\sum_{t=1}^{T}\left\langle\widehat{y}_{t}, p_{t}-p^{*}\right\rangle\Bigg{]}+k\,,\]

where the inequality follows from the definition of \(p^{*}\) and the Cauchy-Schwarz inequality. By the standard analysis of the FTRL, described in Section 3,

\[\sum_{t=1}^{T}\left\langle\widehat{y}_{t},p_{t}-p^{*}\right\rangle \leq\sum_{t=1}^{T}\Bigl{(}\Phi_{t}(p_{t+1})-\Phi_{t+1}(p_{t+1}) \Bigr{)}+\Phi_{t+1}(p^{*})-\Phi_{1}(p_{1})+\sum_{t=1}^{T}\Bigl{(}\langle p_{t} -p_{t+1},\widehat{y}_{t}\rangle-D_{\Phi_{t}}(p_{t+1},p_{t})\Bigr{)}\,.\]

For the penalty term, since \(\Phi_{t}(p)=\frac{1}{\eta_{t}}\psi^{\mathsf{nS}}(p)+2\psi^{\mathsf{LB}}(p)\),

\[\sum_{t=1}^{T}\Bigl{(}\Phi_{t}(p_{t+1})-\Phi_{t+1}(p_{t+1})\Bigr{)} +\Phi_{t+1}(p^{*})-\Phi_{1}(p_{1})\] \[\leq\sum_{t=1}^{T}\left(\frac{1}{\eta_{t+1}}-\frac{1}{\eta_{t}} \right)H(p_{t+1})+\frac{H(p_{1})}{\eta_{1}}+2\sum_{i=1}^{k}\log\left(\frac{1}{ p_{i}^{*}}\right)\] \[\leq\sum_{t=1}^{T}\left(\frac{1}{\eta_{t+1}}-\frac{1}{\eta_{t}} \right)H(p_{t+1})+\frac{\log k}{\eta_{1}}+2k\log T\,,\]

where in the last inequality we used the fact that \(p_{i}^{*}\geq 1/T\) for all \(i\in[k]\).

For the stability term, by Lemma 3 with \(\delta=1\) (since \(p_{t}=q_{t}\)),

\[\sum_{t=1}^{T}\Bigl{(}\langle p_{t}-p_{t+1},\widehat{y}_{t}\rangle-D_{\Phi_{t }}(p_{t+1},p_{t})\Bigr{)}\leq\sum_{t=1}^{T}\eta_{t}\nu_{t}\,.\]

We will confirm that the assumptions for Part (II) of Theorem 1 are indeed satisfied. By the definition of the learning rate in (7),

\[\beta_{t} =\beta_{1}+\sum_{u=1}^{t-1}\frac{c_{1}\nu_{u}}{\sqrt{h_{1}}\sqrt {\sum_{s=1}^{u}\nu_{s}}}\geq\beta_{1}+\frac{c_{1}}{\sqrt{h_{1}}}\sum_{u=1}^{ t-1}\frac{\nu_{u}}{\sqrt{\sum_{s=1}^{u}\nu_{s}}+\sqrt{\sum_{s=1}^{u-1}\nu_{s}}}\] \[\geq\beta_{1}+\frac{c_{1}}{\sqrt{h_{1}}}\sum_{u=1}^{t-1}\left( \sqrt{\sum_{s=1}^{u}\nu_{s}}-\sqrt{\sum_{s=1}^{u-1}\nu_{s}}\right)=\beta_{1}+ \frac{c_{1}}{\sqrt{h_{1}}}\sqrt{\sum_{s=1}^{t-1}\nu_{s}}\,.\]Using this inequality, \(\beta_{t}\) is bounded from below as

\[2\beta_{t}=\beta_{t}+\beta_{t}\geq 2\nu_{t}+\beta_{1}+\frac{c_{1}}{\sqrt{h_{1}}} \sqrt{\sum_{s=1}^{t-1}\nu_{s}}\geq 2\sqrt{2\beta_{1}\nu_{t}}+\frac{c_{1}}{ \sqrt{h_{1}}}\sqrt{\sum_{s=1}^{t-1}\nu_{s}}\geq\frac{c_{1}}{\sqrt{h_{1}}}\sqrt {\sum_{s=1}^{t}\nu_{s}}\,,\]

where the first inequality follows by \(\nu_{t}\leq\beta_{t}/2\) and the above inequality, the second inequality follows by the AM-GM inequality, and the last inequality follows from \(2\sqrt{2\beta_{1}}\geq\frac{c_{1}}{\sqrt{h_{1}}}\) and \(\sqrt{x}+\sqrt{y}\geq\sqrt{x+y}\) for \(x,y\geq 0\). Dividing the both sides by \(2\), we can see stability condition (S2) in Theorem 1 is satisfied with \(a=1/2\). One can also see that the other assumptions are trivially satisfied. Hence, by Part (II) of Theorem 1,

\[\sum_{t=1}^{T}\left(\frac{1}{\eta_{t+1}}-\frac{1}{\eta_{t}}\right)H(p_{t+1})+ \sum_{t=1}^{T}\eta_{t}\nu_{t}\leq 2\left(c_{1}+\frac{2}{c_{1}}\right)\sqrt{h_{1} \sum_{t=1}^{T}\nu_{t}}\,.\]

Using the last inequality with \(\mathbb{E}\bigg{[}\sqrt{\sum_{t=1}^{T}\nu_{t}}\bigg{]}\leq\mathbb{E}\bigg{[} \sqrt{\sum_{t=1}^{T}\omega_{t}}\bigg{]}\leq\sqrt{L_{2}}\), and setting \(c_{1}=\sqrt{2}\), we have

\[\mathsf{Reg}_{T}(i^{*}) \leq\mathbb{E}\Bigg{[}2\left(c_{1}+\frac{2}{c_{1}}\right)\sqrt{h _{1}\sum_{t=1}^{T}\nu_{t}}+\frac{\log k}{\eta_{1}}+2k\log T+k\Bigg{]}\] \[\leq 4\sqrt{2}\sqrt{L_{2}\log k}+2k\log T+k+\frac{1}{4}\,,\]

which completes the proof of Corollary 3. 

## Appendix H Proof of results in Section 5.2

Appendix H.1 provides preliminary results, which will be used to quantify the difference between \(q_{t}\) and \(q_{t+1}\) in Appendix H.2 and will be used to prove the continuity of \(F_{t}\) in Appendix H.3. Appendix H.2 proves Theorem 4 and Appendix H.3 discusses the bisection method to compute \(\beta_{t+1}\).

### Some stability results

Before proving Theorem 4, we prove several important lemmas. Consider the following three optimization problems:

\[\begin{split}& p\in\operatorname*{arg\,min}_{p^{\prime}\in \mathcal{P}_{k}}\left\langle L-\xi e_{1},p^{\prime}\right\rangle+\beta\psi(p^ {\prime})\,,\\ & q\in\operatorname*{arg\,min}_{q^{\prime}\in\mathcal{P}_{k}} \left\langle L,q^{\prime}\right\rangle+\beta\psi(q^{\prime})\,,\\ & r\in\operatorname*{arg\,min}_{r^{\prime}\in\mathcal{P}_{k}} \left\langle L,r^{\prime}\right\rangle+\beta^{\prime}\psi^{\prime}(r^{\prime}) \end{split}\] (28)

with \(L\in\mathbb{R}_{+}^{k}\), \(0\leq\xi\leq\min_{i\in[k]}L_{i}\), and \(\beta,\beta^{\prime}>0\) satisfying \(\beta^{\prime}\geq\beta\),

\[\psi(q)=\sum_{i=1}^{k}(q_{i}\log q_{i}-q_{i})-\frac{c}{\beta}\sum_{i=1}^{k} \log q_{i}\quad\text{and}\quad\psi^{\prime}(q)=\sum_{i=1}^{k}(q_{i}\log q_{i}- q_{i})-\frac{c}{\beta^{\prime}}\sum_{i=1}^{k}\log q_{i}\]

for \(c>0\). Note that the outputs of FTRL with \(\psi(q)\) and with \(-H(q)-(c/\beta)\sum_{i=1}^{k}\log q_{i}\) are identical since adding a constant to \(\psi\) does not change the output of the above optimization problems.

In the following lemma, we investigate the relation between \(q\) and \(r\) in (28).

**Lemma 4**.: _Consider \(q\) and \(r\) in (28). Then,_

\[r_{i}\leq q_{i}^{\beta/\beta^{\prime}}\,.\] (29)Proof.: From the KKT conditions there exist \(\mu,\mu^{\prime}\in\mathbb{R}\) such that

\[L+\beta\nabla\psi(q)+\mu\mathbf{1}=0\quad\text{ and }\quad L+\beta^{\prime}\psi^{ \prime}(r)+\mu^{\prime}\mathbf{1}=0\,,\]

which implies, by \((\nabla\psi(q))_{i}=\log q_{i}-\frac{c}{\beta q_{i}}\), that

\[L_{i}+\beta\log q_{i}-\frac{c}{q_{i}}+\mu=0\quad\text{and}\quad\eta^{\prime}L _{i}+\beta\log r_{i}-\frac{c}{r_{i}}+\mu^{\prime}=0\] (30)

for all \(i\in[k]\). This is equivalent to

\[q_{i}=\exp\left(-\frac{1}{\beta}\left(L_{i}-\frac{c}{q_{i}}+\mu\right)\right) \quad\text{and}\quad r_{i}=\exp\left(-\frac{1}{\beta^{\prime}}\left(L_{i}- \frac{c}{r_{i}}+\mu^{\prime}\right)\right)\,.\]

Removing \(L_{i}\) from these equalities yields that

\[r_{i}=q_{i}^{\beta/\beta^{\prime}}\exp\left(\frac{c}{\beta^{\prime}}\left( \frac{1}{r_{i}}-\frac{1}{q_{i}}\right)\right)\exp\left(\frac{1}{\beta^{\prime }}(\mu-\mu^{\prime})\right)\,.\] (31)

We will prove \(\frac{\mathrm{d}\mu}{\mathrm{d}\beta}>0\). Taking derivative with respect to \(\beta\) of (30), we have

\[\log q_{i}+\left(\frac{1}{q_{i}}+\frac{c}{q_{i}^{2}}\right)\frac{\mathrm{d}q_ {i}}{\mathrm{d}\beta}+\frac{\mathrm{d}\mu}{\mathrm{d}\beta}=0\,.\]

Multiplying \(\left(\frac{1}{q_{i}}+\frac{c}{q_{i}^{2}}\right)^{-1}\) and summing over \(i\in[k]\) in the last equality, we have

\[-\left(\frac{1}{q_{i}}+\frac{c}{q_{i}^{2}}\right)^{-1}\log(1/q_{i})+\sum_{i=1 }^{k}\frac{\mathrm{d}q_{i}}{\mathrm{d}\beta}+\left(\frac{1}{q_{i}}+\frac{c}{q _{i}^{2}}\right)^{-1}\frac{\mathrm{d}\mu}{\mathrm{d}\beta}=0\,,\]

which with the fact \(\sum_{i=1}^{k}\frac{\mathrm{d}q_{i}}{\mathrm{d}\beta}=0\) implies \(\frac{\mathrm{d}\mu}{\mathrm{d}\beta}>0\). Hence, since \(\beta\leq\beta^{\prime}\) we have \(\mu\leq\mu^{\prime}\).

When \(r_{i}\leq q_{i}\), it is obvious that we get \(r_{i}\leq q_{i}^{\beta/\beta^{\prime}}\).

When \(r_{i}>q_{i}\), using (31) with the inequalities \(\beta\leq\beta^{\prime}\) and \(\mu\leq\mu^{\prime}\),

\[r_{i}=q_{i}^{\beta/\beta^{\prime}}\exp\left(\frac{c}{\beta^{\prime}}\left( \frac{1}{r_{i}}-\frac{1}{q_{i}}\right)\right)\exp\left(\frac{1}{\beta^{\prime }}(\mu-\mu^{\prime})\right)\leq q_{i}^{\beta/\beta^{\prime}}\,,\]

which is the desired bound. 

**Lemma 5**.: _Consider \(p\), \(q\), and \(r\) in (28). Then, under \(\eta\coloneqq 1/\beta\leq\frac{1}{15k}\), we have_

\[r_{i}\leq 3p_{i}^{\beta/\beta^{\prime}}\,.\] (32)

Proof.: By Lemma 8 of Bubeck et al. [10] we have \(q_{i}\leq 3p_{i}\) for all \(i\in[k]\). Using this with Lemma 4, we have

\[r_{i}\leq q_{i}^{\beta/\beta^{\prime}}\leq 3q_{i}^{\beta/\beta^{\prime}}\,.\]

### Proof of Theorem 4

In this section, we will provide the proof of Theorem 4. We first see that the ratio \(\beta_{t}/\beta_{t+1}\) is close to one to some extent.

**Lemma 6**.: _The learning rate \(\beta_{t}\) in (8) satisfies_

\[1-\frac{\beta_{t}}{\beta_{t+1}}\in(0,1/10]\,.\]Proof.: Recall that \(\beta_{t}=\beta_{1}+\sum_{u=1}^{t-1}b_{u}\) with \(b_{u}=\frac{c_{1}\nu_{u}}{U_{u}}\) and \(U_{t}=\sqrt{c_{2}+\bar{z}_{t}h_{1}+\sum_{s=1}^{t-1}z_{s}h_{s+1}}\) for \(t\in\{0\}\cup[T]\). It suffices to show

\[\frac{\beta_{t}}{\beta_{t+1}}=\frac{\beta_{t}}{\beta_{t}+b_{t}}\geq\frac{9}{10 }\,\Leftrightarrow\,\beta_{t}\geq 9b_{t}\,.\]

This indeed follows since using \(\nu_{t}\leq\beta_{t}/2\) we have

\[b_{t}=\frac{c_{1}\nu_{t}}{\sqrt{81c_{1}^{2}+\sum_{s=1}^{t-1}z_{s}h_{s}+z_{t}h_ {t+1}}}\leq\frac{c_{1}\nu_{t}}{\sqrt{81c_{1}^{2}}}=\frac{\nu_{t}}{9}\leq\frac {\beta_{t}}{9}\,.\]

Finally, we are ready to prove one of the key lemmas for proving the BOBW regret bound with the sparsity-dependent bound. Recall that we have \(p_{t}=\left(1-\frac{k}{T}\right)q_{t}+\frac{1}{T}\mathbf{1}\) and \(h_{t}=\frac{1}{1-\frac{k}{T}}H(p_{t})\). Using the result in Appendix H.1, we will show that \(h_{t+1}\) is bounded in terms of \(h_{t}\).

**Lemma 7**.: _Suppose that \(\beta_{t}\) is defined as (8). Then,_

\[h_{t+1}\leq 3h_{t}+\frac{20k}{9}\left(\frac{\beta_{t+1}}{\beta_{t}}-1\right) \log\left(\frac{T}{k}\right)\,h_{t+1}\,.\]

Proof.: Let us recall that \(q_{t}\) and \(q_{t+1}\) are defined as

\[q_{t}\in\operatorname*{arg\,min}_{q\in\mathcal{P}_{k}}\left\langle\sum_{s=1} ^{t-1}\widehat{y}_{s},\,q\right\rangle+\Phi_{t}(q)\quad\text{and}\quad q_{t+1} \in\operatorname*{arg\,min}_{q\in\mathcal{P}_{k}}\left\langle\sum_{s=1}^{t} \widehat{y}_{s},\,q\right\rangle+\Phi_{t+1}(q)\,,\]

which corresponds to optimization problems (28) with \(p=q_{t}\), \(L=\sum_{s=1}^{t}\widehat{y}_{s}\), \(\xi=\widehat{y}_{tA_{t}}\), \(\psi=\Phi_{t}/\beta_{t}\), \(\eta=1/\beta_{t}\), \(r=q_{t+1}\), \(\psi^{\prime}=\Phi_{t+1}/\beta_{t+1}\), and \(\eta^{\prime}=1/\beta_{t+1}\).

Since \(H\) is concave, by \(p_{ti}=(1-\frac{k}{T})q_{ti}+\frac{1}{T}\) and Jensen's inequality we have

\[\left(1-\frac{k}{T}\right)h_{t}=H(p_{t})\geq\left(1-\frac{k}{T}\right)H(q_{t}) +\frac{k}{T}H\left(\frac{1}{k}\mathbf{1}\right)\geq\left(1-\frac{k}{T}\right) H(q_{t})\,,\]

which implies \(h_{t}\geq H(q_{t})\). By Lemma 5 we also have \(q_{t+1,i}\leq 3q_{ti}^{\beta_{t}/\beta_{t+1}},\) which implies that

\[p_{t+1,i}=\left(1-\frac{k}{T}\right)q_{t+1,i}+\frac{1}{T}\leq\left(1-\frac{k} {T}\right)\,3q_{ti}^{\beta_{t}/\beta_{t+1}}+\frac{1}{T}\leq 6p_{ti}^{\beta_{t}/ \beta_{t+1}}\,.\]

The last inequality follows since when \(\left(1-\frac{k}{T}\right)\,3q_{ti}^{\beta_{t}/\beta_{t+1}}\leq\frac{1}{T}\),

\[\left(1-\frac{k}{T}\right)\,3q_{ti}^{\beta_{t}/\beta_{t+1}}+\frac{1}{T}\leq \frac{2}{T}\leq 2\left(\frac{1}{T}\right)^{\beta_{t}/\beta_{t+1}}\leq 2\left( \left(1-\frac{k}{T}\right)q_{ti}+\frac{1}{T}\right)^{\beta_{t}/\beta_{t+1}}=2p _{ti}^{\beta/\beta_{t+1}}\,,\]

and otherwise

\[\left(1-\frac{k}{T}\right)\,3q_{ti}^{\beta_{t}/\beta_{t+1}}+\frac{1}{T}\leq 6 \left(1-\frac{k}{T}\right)^{\beta_{t}/\beta_{t+1}}q_{ti}^{\beta_{t}/\beta_{t+1 }}\leq 6p_{ti}^{\beta/\beta_{t+1}}\,.\]

Using these inequalities, we have

\[h_{t+1}-3h_{t} =\frac{1}{1-\frac{k}{T}}\left(H(p_{t+1})-3H(p_{t})\right)\] \[\leq\frac{1}{1-\frac{k}{T}}\left(H(p_{t})+\langle\nabla H(p_{t}),p _{t+1}-p_{t}\rangle-3H(p_{t})\right)\] \[=\frac{1}{1-\frac{k}{T}}\sum_{i=1}^{k}\left(p_{t+1,i}-3p_{ti} \right)\log\left(\frac{1}{p_{ti}}\right)\] \[\leq\sum_{i=1}^{k}\left(q_{t+1,i}-3q_{ti}\right)\log\left(\frac{1} {p_{ti}}\right)\,,\] (33)where the first inequality follows by the concavity of \(H\), the second inequality follows since \(p_{t+1,i}-3p_{ti}\leq\left(1-\frac{k}{T}\right)(q_{t+1,i}-q_{ti})\). Defining \(\Omega_{t}=\{i\in[k]:q_{t+1,i}-3q_{ti}\geq 0\}\), (33) is further bounded as

\[\sum_{i=1}^{k}\left(q_{t+1,i}-3q_{ti}\right)\log\left(\frac{1}{p_ {ti}}\right) =\sum_{i\in\mathcal{Q}_{t}}\left(q_{t+1,i}-3q_{ti}\right)\log \left(\frac{1}{p_{ti}}\right)+\sum_{i\not\in\mathcal{Q}_{t}}\left(q_{t+1,i}-3q _{ti}\right)\log\left(\frac{1}{p_{ti}}\right)\] \[\leq\frac{\beta_{t+1}}{\beta_{t}}\sum_{i\in\mathcal{Q}_{t}}\left( q_{t+1,i}-3q_{ti}\right)\log\left(\frac{1}{p_{t+1,i}}\right)+0\] \[\leq\frac{10}{9}\sum_{i\in\mathcal{Q}_{t}}\left(q_{t+1,i}-q_{t+1,i}^{\beta_{t+1}/\beta_{t}}\right)\log\left(\frac{1}{p_{t+1,i}}\right)\] \[=\frac{10}{9}\sum_{i\in\mathcal{Q}_{t}}q_{t+1,i}\left(1-q_{t+1,i }^{\frac{\beta_{t+1}}{\beta_{t}}-1}\right)\log\left(\frac{1}{p_{t+1,i}}\right)\,,\] (34)

where the first inequality follows by \(p_{t+1,i}\leq 6p_{t}^{\beta_{t}/\beta_{t+1}}\), the second follows by Lemma 6, and the last inequality follows by \(q_{t+1,i}\leq 3q_{ti}^{\beta_{t}/\beta_{t+1}}\). Since for any \(\varepsilon>0\), \(x\in[0,1]\), and \(\gamma\in[0,1]\), it holds that

\[x(1-x^{\varepsilon}) \leq x\log\left(\frac{1}{x^{\varepsilon}}\right)=\varepsilon x \log\left(\frac{1}{x}\right)\] \[\leq\varepsilon\left(\left(\log\frac{1}{\gamma}-1\right)(x-r)+ \gamma\log\frac{1}{\gamma}\right)\leq\varepsilon\log\left(\frac{1}{\gamma} \right)\left(\gamma+(1-\gamma)x\right),\] (35)

setting \(\gamma=k/T\) in (35) implies that the RHS of (34) is further bounded as

\[h_{t+1}-3h_{t}\] \[\leq\frac{10}{9}\sum_{i\in\mathcal{Q}_{t}}\left(\frac{\beta_{t+1 }}{\beta_{t}}-1\right)\log(T/k)\left(\frac{k}{T}+\left(1-\frac{k}{T}\right)q_{ t+1,i}\right)\log\left(\frac{1}{p_{t+1,i}}\right)\] \[\leq\frac{10k}{9}\left(\frac{\beta_{t+1}}{\beta_{t}}-1\right)\log (T/k)\sum_{i=1}^{k}\left(\frac{1}{T}+\left(1-\frac{k}{T}\right)q_{t+1,i} \right)\log\left(\frac{1}{p_{t+1,i}}\right)\] \[\leq\frac{20k}{9}\left(\frac{\beta_{t+1}}{\beta_{t}}-1\right) \log(T/k)h_{t+1}\,,\]

where the second inequality follows by Lemma 6 and the last inequality follows by the definition of \(h_{t+1}\). 

Finally we are ready to prove Theorem 4.

Proof of Theorem 4.: Fix \(i^{*}\in[k]\) and define \(p^{*}\in\mathcal{P}_{k}\) by

\[p^{*}=\left(1-\frac{k}{T}\right)e_{i^{*}}+\frac{1}{T}\mathbf{1}\,.\]Then, using the definition of the algorithm,

\[\mathsf{Reg}_{T}(i^{*}) =\mathbb{E}\Bigg{[}\sum_{t=1}^{T}\ell_{tA_{t}}-\sum_{t=1}^{T}\ell_{ ti^{*}}\Bigg{]}=\mathbb{E}\Bigg{[}\sum_{t=1}^{T}\left\langle\ell_{t},p_{t}-e_{i^{*}} \right\rangle\Bigg{]}\] \[=\mathbb{E}\Bigg{[}\sum_{t=1}^{T}\left\langle\ell_{t},q_{t}-e_{i^ {*}}\right\rangle\Bigg{]}+\mathbb{E}\Bigg{[}\gamma\sum_{t=1}^{T}\left\langle \ell_{t},\frac{1}{k}\mathbf{1}-q_{t}\right\rangle\Bigg{]}\] \[\leq\mathbb{E}\Bigg{[}\sum_{t=1}^{T}\left\langle\ell_{t},q_{t}-p^ {*}\right\rangle\Bigg{]}+\mathbb{E}\Bigg{[}\sum_{t=1}^{T}\left\langle\ell_{t},p ^{*}-e_{i^{*}}\right\rangle\Bigg{]}+\gamma T\] \[\leq\mathbb{E}\Bigg{[}\sum_{t=1}^{T}\left\langle\widehat{y}_{t}, q_{t}-p^{*}\right\rangle\Bigg{]}+2k\,,\]

where the first inequality follows since \(p_{t}=(1-\gamma)q_{t}+\frac{\gamma}{k}\mathbf{1}\) and the last inequality follows by the definition of \(p^{*}\) and \(\gamma=\frac{k}{T}\). By the standard analysis of the FTRL described in Section 3,

\[\sum_{t=1}^{T}\left\langle\widehat{y}_{t},q_{t}-p^{*}\right\rangle \leq\sum_{t=1}^{T}\Bigl{(}\Phi_{t}(q_{t+1})-\Phi_{t+1}(q_{t+1}) \Bigr{)}+\Phi_{t+1}(p^{*})-\Phi_{1}(q_{1})\] \[\qquad+\sum_{t=1}^{T}\Bigl{(}\left\langle q_{t}-q_{t+1},\widehat {y}_{t}\right\rangle-D_{\Phi_{t}}(q_{t+1},q_{t})\Bigr{)}\,.\]

We first consider the penalty term. Since \(\Phi_{t}=\frac{1}{\eta_{t}}\psi^{\mathsf{nS}}+4\psi^{\mathsf{LB}}\),

\[\sum_{t=1}^{T}\Bigl{(}\Phi_{t}(q_{t+1})-\Phi_{t+1}(q_{t+1}) \Bigr{)}+\Phi_{t+1}(p^{*})-\Phi_{1}(q_{1})\] \[\leq\sum_{t=1}^{T}\left(\frac{1}{\eta_{t+1}}-\frac{1}{\eta_{t}} \right)H(q_{t+1})+\frac{H(q_{1})}{\eta_{1}}+4\sum_{i=1}^{k}\log\left(\frac{1} {p_{i}^{*}}\right)\] \[\leq\sum_{t=1}^{T}\left(\frac{1}{\eta_{t+1}}-\frac{1}{\eta_{t}} \right)H(q_{t+1})+\frac{\log k}{\eta_{1}}+4k\log T\,,\]

where in the last inequality we used the fact that \(p_{i}^{*}\geq 1/T\) for all \(i\in[k]\).

For the stability term, by Lemma 3 with \(\delta=2\),

\[\sum_{t=1}^{T}\Bigl{(}\left\langle q_{t}-q_{t+1},\widehat{y}_{t}\right\rangle- D_{\Phi_{t}}(q_{t+1},q_{t})\Bigr{)}\leq 2\sum_{t=1}^{T}\eta_{t}\nu_{t}\,.\]

We will confirm that the assumptions for Part (I) of Theorem 1 are indeed satisfied. By the definition of the learning rate in (8) and \(\nu_{t}\leq\beta_{t}/2\),

\[\frac{\sqrt{c_{2}}}{c_{1}}(\beta_{1}+\beta_{t})\geq 9(\beta_{1}+\nu_{t})\geq \beta_{1}+\nu_{t}\,.\]

Hence stability condition (S1) of Theorem 1 is satisfied and one can also see that the other assumptions are trivially satisfied. Hence, by Part (I) of Theorem 1,

\[\sum_{t=1}^{T}\left(\frac{1}{\eta_{t+1}}-\frac{1}{\eta_{t}}\right) +2\sum_{t=1}^{T}\eta_{t}\nu_{t} \leq 2\left(c_{1}+\frac{2}{c_{1}}\log\left(1+\sum_{s=1}^{T}\frac{ \nu_{s}}{\beta_{1}}\right)\right)\sqrt{c_{2}+\sum_{t=1}^{T+1}\nu_{t}h_{t+1}}\] \[\leq 2\left(c_{1}+\frac{2}{c_{1}}\log\left(1+\frac{T^{2}}{\beta_{1 }}\right)\right)\sqrt{c_{2}+\sum_{t=1}^{T+1}\nu_{t}h_{t+1}}\,,\] \[=2\sqrt{2\log\left(1+\frac{T^{2}}{\beta_{1}}\right)}\sqrt{c_{2}+ \sum_{t=1}^{T+1}\nu_{t}h_{t+1}}\,,\]where in the last inequality we used \(\nu_{t}\leq T\) and in the equality we set \(c_{1}=\sqrt{2\log\left(1+\frac{T^{2}}{\beta_{1}}\right)}\).

By summing up the above arguments and Jensen's inequality, we have

\[\mathsf{Reg}_{T}(i^{*}) \leq\mathbb{E}\Bigg{[}2\sqrt{2\log\left(1+\frac{T^{2}}{\beta_{1}} \right)}\sqrt{c_{2}+\sum_{t=1}^{T+1}\nu_{t}h_{t+1}}\Bigg{]}+2k+4k\log T+\frac{ \log k}{\eta_{1}}\] \[\leq 2\sqrt{2\log\left(1+\frac{T^{2}}{\beta_{1}}\right)}\sqrt{c_{ 2}+\mathbb{E}\left[\sum_{t=1}^{T+1}\nu_{t}h_{t+1}\right]}+2k+4k\log T+15k\log k\] \[\leq 2\sqrt{2\log\left(1+\frac{T^{2}}{\beta_{1}}\right)}\sqrt{ \mathbb{E}\left[\sum_{t=1}^{T}\nu_{t}h_{t+1}\right]}+O(k\log T)\,.\] (36)

Adversarial regimeWe first consider the adversarial regime. Recall that \(\mathbb{E}\bigg{[}\sqrt{\sum_{t=1}^{T}\nu_{t}}\bigg{]}\leq\mathbb{E}\bigg{[} \sqrt{\sum_{t=1}^{T}\omega_{t}}\bigg{]}\leq\sqrt{L_{2}}\) as was done in the proof of Corollary 3. Hence (36) with \(h_{t}\leq 2\log k\) (since \(T\geq 2k\)) yields that

\[\mathsf{Reg}_{T}\leq 4\sqrt{L_{2}\log(k)\log\left(1+\frac{T^{2}}{\beta_{1}} \right)}+O(k\log T)\,.\]

Adversarial regime with a self-bounding constraintNext we consider the adversarial regime with a self-bounding constraint. We will bound a component of (36). By Lemma 7, \(\sqrt{\mathbb{E}\left[\sum_{t=1}^{T}\nu_{t}h_{t+1}\right]}\) is bounded as

\[X_{t} \coloneqq\sqrt{\mathbb{E}\bigg{[}\sum_{t=1}^{T}\nu_{t}h_{t+1} \bigg{]}}\] \[\leq\sqrt{3\,\mathbb{E}\bigg{[}\sum_{t=1}^{T}\nu_{t}h_{t}\bigg{]} +\frac{20k}{9}\log\left(\frac{T}{k}\right)\mathbb{E}\bigg{[}\sum_{t=1}^{T}\nu _{t}\left(\frac{\beta_{t+1}}{\beta_{t}}-1\right)h_{t+1}\bigg{]}}\] \[\leq\sqrt{3\,\mathbb{E}\bigg{[}\sum_{t=1}^{T}\nu_{t}h_{t}\bigg{]} +\frac{10k}{9}\log\left(\frac{T}{k}\right)\mathbb{E}\bigg{[}\sum_{t=1}^{T} \left(\beta_{t+1}-\beta_{t}\right)h_{t+1}\bigg{]}}\] \[=\sqrt{3\,\mathbb{E}\bigg{[}\sum_{t=1}^{T}\nu_{t}h_{t}\bigg{]} +\frac{20k}{9}\log\left(\frac{T}{k}\right)X_{t}}\,,\]

where the first inequality follows by Lemma 7, the second inequality follows by \(\nu_{t}\leq\beta_{t}/2\), the last inequality follows by Lemma 8. Since \(x\leq\sqrt{a+bx}\) for \(x>0\) implies \(x\leq 2\sqrt{a}+b\),

\[X_{t}\leq 2\sqrt{3\,\mathbb{E}\bigg{[}\sum_{t=1}^{T}\nu_{t}h_{t} \bigg{]}}+\frac{20k}{9}\log\left(\frac{T}{k}\right)=2\sqrt{3\,\mathbb{E} \bigg{[}\sum_{t=1}^{T}\mathbb{E}[\nu_{t}\,|\,p_{t}]h_{t}\bigg{]}}+\frac{20k}{9 }\log\left(\frac{T}{k}\right)\]\[\leq 2\sqrt{6s\,\mathbb{E}\!\left[\sum_{t=1}^{T}H(p_{t})\right]}+\frac{20k}{9}\log \left(\frac{T}{k}\right)\,,\] (37)

where we used \(\mathbb{E}[\nu_{t}\,|\,h_{t}]\leq\mathbb{E}[\sum_{i=1}^{k}p_{ti}(\ell_{ti}^{2}/p_{ ti})]=\mathbb{E}[\sum_{i\in[k]:\ell_{ti}\neq 0}p_{ti}(\ell_{ti}^{2}/p_{ti})]\leq s\).

We consider the case of \(P(a^{*})\geq\mathrm{e}\), since otherwise Lemma 2 implies \(\sum_{t=1}^{T}H(p_{t})\leq\mathrm{e}\log(kT)\) and thus the desired bound is trivially obtained. When \(P(a^{*})\geq\mathrm{e}\), Lemma 2 implies that \(\sum_{t=1}^{T}H(p_{t})\leq P(a^{*})\log(kT)\). Then from the self-bounding technique, for any \(\lambda\in(0,1]\) it holds that

\[\mathsf{Reg}_{T} =(1+\lambda)\mathsf{Reg}_{T}-\lambda\mathsf{Reg}_{T}\] \[\leq\mathbb{E}\!\left[(1+\lambda)O\left(\sqrt{s\log(T)\log(kT)P( a^{*})}\right)-\lambda\Delta_{\min}P(a^{*})\right]+\lambda C+O(k\log T)\] \[\leq O\!\left(\frac{(1+\lambda)^{2}s\log(T)\log(kT)}{\lambda \Delta_{\min}}+\lambda C\right)\] \[=O\!\left(\frac{s\log(T)\log(kT)}{\Delta_{\min}}+\lambda\!\left( \frac{s\log(T)\log(kT)}{\Delta_{\min}}+C\right)+\frac{1}{\lambda}\frac{s\log (T)\log(kT)}{\Delta_{\min}}\right),\]

where the first inequality follows by Lemma 1 and the second inequality follows from \(a\sqrt{x}-bx/2\leq a^{2}/(2b)\) for \(a,b,x\geq 0\). Setting \(\lambda\in(0,1]\) to

\[\lambda=\sqrt{\frac{s\log(T)\log(kT)}{\Delta_{\min}}}\Big{/}\left(\frac{s\log (T)\log(kT)}{\Delta_{\min}}+C\right)\]

gives the desired regret bound for the adversarial regime with a self-bounding constraint.

Stochastic regimeUsing \(\mathbb{E}[\nu_{t}\,|\,h_{t}]\leq\mathbb{E}[\omega_{t}\,|\,h_{t}]\leq\mathbb{ E}[\sum_{i=1}^{k}p_{ti}(\ell_{ti}^{2}/p_{ti})]=\mathbb{E}[\sum_{i=1}^{k}\ell_{ti}^{2}]\) in the second inequality of (37) and following the same arguments as the analysis for the adversarial regime with a self-bounding constraint, one can obtain the regret bound for the stochastic regime.

### Discussion on bisection method for computing \(\beta_{t+1}\)

This section describes the bisection method to compute \(\beta_{t+1}\) described in Section 5.2. Recall that \(F_{t}:[\beta_{t},\beta_{t}+T]\to\mathbb{R}\) is defined by the difference of the both sides of the update rule of \((\beta_{t})\) in (8):

\[F_{t}(\alpha)=\alpha-\left(\beta_{t}+\frac{c_{1}\nu_{t}}{\sqrt{c_{2}+\nu_{t}h_ {t+1}(\alpha)+\sum_{s=1}^{t-1}\nu_{s}h_{s+1}}}\right)\,,\] (38)

where \(h_{t+1}(\alpha)=\frac{1}{1-\frac{T}{T}}H(p_{t+1}(\alpha))\), and \(p_{t+1}(\alpha)\) is the FTRL output with the regularizer \(\Phi_{t}=\alpha\psi^{\mathsf{nS}}+4\psi^{\mathsf{LB}}\). Note that \(c_{1}\nu_{t}/\sqrt{c_{2}+\nu_{t}h_{t+1}(\alpha)+\sum_{s=1}^{t-1}\nu_{s}h_{s+1 }}\leq c_{1}\nu_{t}/c_{2}\leq T/9\) since \(\nu_{t}\leq T\).

Assume that \(F_{t}\) is continuous. Then we can see that there exists \(\alpha\in[\beta_{t},\beta_{t}+T]\) such that \(F_{t}(\alpha)=0\). In fact, if \(p_{tA_{t}}=0\) then \(\beta_{t+1}=\beta_{t}\), and otherwise, we have \(F_{t}(\beta_{t})\leq 0\) and \(F_{t}(\beta_{t}+T)>0\). Using the intermediate value theorem with the assumption that \(F_{t}\) is continuous, there indeed exists \(\alpha\in[\beta_{t},\beta_{t}+T]\) satisfying \(F_{t}(\alpha)=0\). We can compute such \(\alpha\) by the bisection method. In particular, we first set the range of \(\alpha\) to \([\beta_{t},\beta_{t}+T]\), and then iteratively halve it by evaluating the value of \(F_{t}\) at the middle point. Such a bisection method (binary search) is also used in [50], although the computed target is different. The whole BOBW algorithm with the sparsity-dependent bound in Section 5.2 is given in Algorithm 2, and the concrete procedure of the bisection given in Algorithm 3.

Now, all that remains is to show that \(F_{t}\) is continuous. To prove this, it suffices to prove that \(h_{t+1}(\alpha)=\frac{1}{1-\frac{T}{T}}H(p_{t+1}(\alpha))\) is continuous with respect to \(\alpha\).

**Proposition 2**.: \(F_{t}\) _in (38) is continuous with respect to \(\alpha\)._

Proof of Proposition 2.: Take any \(\alpha\in[\beta_{t},\beta_{t}+T]\) and then consider the following optimization problem:

\[q_{t+1}(\alpha)=\operatorname*{arg\,min}_{q\in\mathcal{P}_{k}}\left\langle \sum_{s=1}^{t}\tilde{y}_{s},q\right\rangle+\Phi_{t+1}(q)\,,\]

where \(\Phi_{t+1}=\alpha\psi^{\mathsf{nS}}+4\psi^{\mathsf{LB}}\). Now using Corollary 8.1 of Hogan [20] with the fact that the solution of the above optimization problem is unique, \(q_{t+1}(\alpha)\) is continuous with respect to \(\alpha\). This completes the proof since \(p_{t+1}\) is continuous with respect to \(q_{t+1}\), \(1/T\leq p_{t+1,i}(\alpha)\leq 1-k/T\), and \(H(p)\) is continuous in a neighborhood of \(p=p_{t+1}(\alpha)\). 

## Appendix I Proof of Corollary 6

This section proves Corollary 6, which is the extended result of Corollary 5. Recall that \(B=1/2\) for FI, \(B=k/2\) for MAB, and \(B=2mk^{2}\) for PM-local, and \(r_{\mathsf{M}}\) is \(1\) if \(\mathsf{M}\) is FI or MAB, and \(2k\) if \(\mathsf{M}\) is PM-local, which are appeared in Appendix B. Let \(\mathsf{Reg}_{T}(a)=\mathbb{E}\big{[}\sum_{t=1}^{T}\bigl{(}\mathcal{L}_{A_{t} x_{t}}-\mathcal{L}_{ax_{t}}\bigr{)}\big{]}=\mathbb{E}\big{[}\sum_{t=1}^{T} \left\langle\ell_{A_{t}}-\ell_{a},e_{x_{t}}\right\rangle\big{]}\) for \(a\in[k]\).

Proof.: Fix \(i^{*}\in[k]\). From Lemma 7 in Tsuchiya et al. [46], if \(\eta_{t}>0\), we have

\[\mathsf{Reg}_{T}(i^{*})\leq\mathbb{E}\Biggl{[}\sum_{t=1}^{T}\left(\frac{1}{ \eta_{t+1}}-\frac{1}{\eta_{t}}\right)H(q_{t+1})+\frac{H(q_{1})}{\eta_{1}}+\sum_ {t=1}^{T}\eta_{t}V_{t}^{\prime}\Biggr{]}\,.\] (39)

We will confirm that the assumptions for Part (I) of Theorem 1 are indeed satisfied. Since

\[\frac{\sqrt{c_{2}+\bar{z}_{t}h_{1}}}{c_{1}}(\beta_{t}+\beta_{1})\geq\sqrt{ \frac{2\bar{V}\log k}{\log(1+T)}}\cdot 2B\sqrt{\frac{\log(1+T)}{\log k}}\geq \sqrt{2}\left(\bar{V}+\bar{V}_{t}\right)\,,\]stability condition (S1) is satisfied. One can also see that the other conditions are trivially satisfied. Hence, using Part (I) of Theorem 1, we can bound the RHS of (39) as

\[\mathsf{Reg}_{T}(i^{*}) \leq\mathbb{E}\Bigg{[}\Bigg{(}2c_{1}+\frac{1}{c_{1}}\log\left(1+ \sum_{u=1}^{T}\frac{V_{u}^{\prime}}{\bar{V}}\right)\Bigg{)}\sqrt{\bar{V}H(q_{1} )+\sum_{t=1}^{T}V_{t}^{\prime}H(q_{t+1})}\Bigg{]}+\frac{H(q_{1})}{\eta_{1}}\] \[\leq\left(2c_{1}+\frac{1}{c_{1}}\log\left(1+T\right)\right)\sqrt{ \mathbb{E}\bigg{[}\sum_{t=1}^{T}V_{t}^{\prime}H(q_{t+1})\bigg{]}}+O\left(\sqrt {\bar{V}\log(k)\log(T)}+B\sqrt{\log(k)\log T}\right)\] \[=\sqrt{2\log(1+T)}\sqrt{\mathbb{E}\left[\sum_{t=1}^{T}V_{t}^{ \prime}H(q_{t+1})\right]}+O\left(B\sqrt{\log(k)\log(T)}\right)\,,\] (40)

where the second inequality follows from \(V_{u}^{\prime}/\bar{V}\leq 1\) and in the equality we set \(c_{1}=\sqrt{\frac{\log(1+T)}{2}}\) and used \(\sqrt{\bar{V}}\leq B\).

Adversarial regimeFor the adversarial regime, since \(H(q_{t})\leq\log k\), (40) immediately implies

\[\mathsf{Reg}_{T}\leq\mathbb{E}\Bigg{[}\sqrt{2\sum_{t=1}^{T}V_{t}^{\prime}\log( k)\log(1+T)}+O\left(B\sqrt{\log(k)\log(T)}\right)\Bigg{]}\,,\]

which is the desired bound.

Adversarial regime with a self-bounding constraintNext, we consider the adversarial regime with a self-bounding constraint. We consider the case of \(Q(a^{*})\geq\mathrm{e}\), since otherwise Lemma 2 implies \(\sum_{t=1}^{T}H(p_{t})\leq\mathrm{e}\log(kT)\) and thus the desired bound is trivially obtained. When \(Q(a^{*})\geq\mathrm{e}\), Lemma 2 implies that \(\sum_{t=1}^{T}H(q_{t})\leq Q(a^{*})\log(kT)\). Then from the self-bounding technique, for any \(\lambda\in(0,1]\)

\[\mathsf{Reg}_{T} =(1+\lambda)\mathsf{Reg}_{T}-\lambda\mathsf{Reg}_{T}\] \[\leq\mathbb{E}\Big{[}(1+\lambda)O\left(\sqrt{\bar{V}\log(T)\log( kT)Q(a^{*})}\right)-\frac{\lambda\Delta_{\min}Q(a^{*})}{r_{\mathcal{M}}} \Big{]}+\lambda C\] \[\leq(1+\lambda)O\left(\sqrt{\bar{V}\log(T)\log(kT)}\right)-\frac {\lambda\Delta_{\min}\bar{Q}(a^{*})}{r_{\mathcal{M}}}+\lambda C\] \[\leq O\left(\frac{(1+\lambda)^{2}r_{\mathcal{M}}\log(T)\log(kT)}{ \lambda\Delta_{\min}}+\lambda C\right)\] \[=O\left(\frac{r_{\mathcal{M}}\bar{V}\log(T)\log(kT)}{\Delta_{ \min}}+\lambda\left(\frac{r_{\mathcal{M}}\bar{V}\log(T)\log(kT)}{\Delta_{\min} }+C\right)+\frac{1}{\lambda}\frac{r_{\mathcal{M}}\bar{V}\log(T)\log(kT)}{ \Delta_{\min}}\right)\,,\]

where the first inequality follows by (40) and Lemma 1 with \(c^{\prime}=r_{\mathcal{M}}\) and the second inequality follows from \(a\sqrt{x}-bx/2\leq a^{2}/(2b)\) for \(a,b,x\geq 0\). Setting \(\lambda\in(0,1]\) to

\[\lambda=\sqrt{\frac{r_{\mathcal{M}}\bar{V}\log(T)\log(kT)}{\Delta_{\min}}} \Big{/}\left(\frac{r_{\mathcal{M}}\bar{V}\log(T)\log(kT)}{\Delta_{\min}}+C\right)\]

gives the desired bound for the adversarial regime with a self-bounding constraint. 

## Appendix J Basic lemma

**Lemma 8** ([38, Lemma 4.13]).: _Let \(a_{0}\geq 0\), \((a_{t})_{t=1}^{T}\) be non-negative reals and \(f:\mathbb{R}_{+}\to\mathbb{R}_{+}\) be a non-increasing function. Then,_

\[\sum_{t=1}^{T}a_{t}f\left(a_{0}+\sum_{s=1}^{t}a_{s}\right)\leq\int_{a_{0}}^{ \sum_{t=0}^{T}a_{t}}f(x)\mathrm{d}x\,.\]We include the proof for the completeness.

Proof.: Let \(A_{t}=\sum_{s=0}^{t}a_{s}\). Then summing the following inequality over \(t\) completes the proof:

\[a_{t}f\left(a_{0}+\sum_{s=1}^{t}a_{s}\right)=a_{t}f(A_{t})=\int_{A_{t-1}}^{A_{t} }f(A_{t})\mathrm{d}x\leq\int_{A_{t-1}}^{A_{t}}f(x)\mathrm{d}x\,.\]

## Appendix K Comparison of the sparsity-dependent bound and the first-order bound for negative losses

If \(\ell_{t}\in[0,1]^{k}\), the first-order bound by Wei and Luo [49] implies sparsity bounds. This, however, does not hold when \(\ell_{t}\in[-1,1]^{k}\). In fact, let us consider the case where \(\ell_{t}\) is a zero vector except that only one arm's loss is \(-1\) for some \(t\in[T]\). Then the sparsity-dependent bound becomes \(O(\sqrt{T})\). On the other hand, the first-order bound in [49] is not directly applicable, and we need to transform losses to range \([0,1]\). This implies that the first-order bound becomes \(O(\sqrt{kT})\), which is worse than the sparsity-dependent bound.