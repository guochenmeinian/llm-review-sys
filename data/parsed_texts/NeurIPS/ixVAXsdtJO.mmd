# Open Visual Knowledge Extraction via

Relation-Oriented Multimodality Model Prompting

 Hejie Cui\({}^{1}\)1 & Xinyu Fang\({}^{2}\)2 & Zihan Zhang\({}^{2}\) & Ran Xu\({}^{1}\) & Xuan Kan\({}^{1}\) & Xin Liu\({}^{3}\) &Yue Yu\({}^{4}\) & Manling Li\({}^{5}\) & Yangqi Song\({}^{3}\) & Carl Yang\({}^{1}\)2

\({}^{1}\)Emory University \({}^{2}\)Tongji University \({}^{3}\) The Hong Kong University of Science and Technology

\({}^{4}\) Georgia Institute of Technology \({}^{5}\) Northwestern University

These authors contributed equally to this work.Correspondence to: j.carlyang@emory.edu

Footnote 1: footnotemark:

###### Abstract

Images contain rich relational knowledge that can help machines understand the world. Existing methods on visual knowledge extraction often rely on the pre-defined format (e.g., sub-verb-obj tuples) or vocabulary (e.g., relation types), restricting the expressiveness of the extracted knowledge. In this work, we take a first exploration to a new paradigm of open visual knowledge extraction. To achieve this, we present OpenVik which consists of an open relational region detector to detect regions potentially containing relational knowledge and a visual knowledge generator that generates format-free knowledge by prompting the large multimodality model with the detected region of interest. We also explore two data enhancement techniques for diversifying the generated format-free visual knowledge. Extensive knowledge quality evaluations highlight the correctness and uniqueness of the extracted open visual knowledge by OpenVik. Moreover, integrating our extracted knowledge across various visual reasoning applications shows consistent improvements, indicating the real-world applicability of OpenVik.

## 1 Introduction

Knowledge extraction has been widely studied on texts [8; 1; 13; 9] for enhancing logical reasoning [45; 14; 6] and explainable AI [18; 57; 5; 55], and recent studies have explored _open_ knowledge extraction through categorizing seed relations [64; 40] and eliciting from language models [47]. Visual knowledge extraction, on the other hand, captures intricate details like tools, sizes, and positional relationships, which are often difficult to express exhaustively in texts [39; 28; 48; 7]. Yet existing approaches of visual knowledge extraction are either restricted by a fixed knowledge format [52; 63; 20; 22] or the predefined sets of objects/relations [52; 63; 21]. While efficient at capturing interactions between objects, the produced visual knowledge is often limited in richness and confined to a single format, falling short in representing the diverse real-world information that can be complemented by visual data.

In this endeavor, we propose to further explore a new paradigm of open visual knowledge extraction (OpenVik). Specifically, we propose to generate relation-oriented, but format-free knowledge that includes a wider variety of elements, such as descriptions, insertions, and attributes, among others. Drawing inspiration from the wealth of knowledge encapsulated in large models [49; 61; 46], we propose to leverage pre-trained large multimodality models by eliciting open visual knowledge through relation-oriented visual prompting. This approach allows for a more nuanced understanding of visual data, mirroring how humans naturally emphasize certain aspects of visual scenes when perceiving and describing visual information, leading to more flexible visual knowledge extraction.

Our proposed OpenVik framework consists of two modules, an open relational region detector and a format-free visual knowledge generator. It is a unique challenge to detect the regions potentially containing relational knowledge, since traditional region detectors primarily focus on learning predefined object classes. To learn the regression of relational regions, we propose to use free-form knowledge descriptions as supervision and leverage knowledge generation as a training objective. With the detected regions, the remaining question is how to interpret these regions into free-form knowledge. We propose a visual knowledge generator by harnessing the power of language variety enhancement in large pre-trained multimodality models. Specifically, we prompt them to generate knowledge descriptions of any formats and condition the generation on the detected relational regions.

However, establishing a new paradigm of open visual knowledge extraction is challenging due to the absence of comprehensive and diverse training data. Existing datasets sources such as scene graphs [51; 24], dense captions [20], and dense relational subsets [22] often exhibit a long-tail distribution biased to more prevalent relations and entities [44]. Brute-force merging of these datasets could exacerbate the distribution bias inherent in the data. To alleviate the bias, we propose two diversity-driven data enhancement strategies based on an adapted TF-IDF+ score, involving random dropping and data augmentation with external knowledge resources. These strategies optimize data distributions and richness, thus fostering diverse open visual knowledge extraction.

We implement extensive evaluations to assess the quality and utility of the open visual knowledge extracted by OpenVik, encompassing: 1) directly evaluating the performance of knowledge generation; 2) engaging human evaluators for a multi-faceted assessment of in-depth knowledge quality; and 3) comparing the open visual knowledge extracted with OpenVik with existing knowledge sources, such as non-parametric knowledge from the ConceptNet knowledge graph, and parametric knowledge from the GPT-3.5 large language model. Furthermore, the utility of the extracted open visual knowledge is validated through its integration with several common applications that require visual understanding, including text-to-image retrieval, grounded situation recognition, and visual commonsense reasoning. These applications demonstrate consistent improvements, affirming the practical utility of OpenVik.

## 2 Related Work

**Visual knowledge extraction.** Recent advancements in knowledge extraction have extended from being purely text-driven to incorporating images [11; 29]. VisKE [39] is designed to verify relations between pairs of entities, e.g., _eat(horse_, _hav_). Scene graphs, which locate objects in the image and identify visual predicates between subjects and objects in a triple format, e.g., (_man, on, chair_), are extensively studied for vision understanding [52; 63; 60]. A recent work OpenSGG [17] extends SGG to open-vocabulary objects, enabling the relation prediction for unseen objects. Other studies have explored caption-like formats, like dense captioning [20] with a set of object-centric descriptions across regions, and relational captioning [22] focusing on relational information between objects. Despite these advancements, existing methods either adhere to a pre-defined format and vocabulary or are constrained by the biased distribution of training sets. This highlights the pressing need for a format-free approach in visual knowledge extraction with knowledge diversity.

**Large model prompting.** Recently, large language and multimodality models have exhibited remarkable successes in capturing commonsense knowledge across various tasks, especially facilitating few-shot [15; 53; 25; 58] and zero-shot learning [23; 66; 59]. The potential of prompt-based learning for pre-trained vision-language models [2; 37; 42] has been explored for handling diverse data types across multiple modalities, such as images and texts, with improved performance in tasks including image classification [33; 67], segmentation [32] and visual question answering [16]. Leveraging the substantial information encapsulated within these pre-trained multimodality models to extract explicit knowledge can enrich existing resources, potentially laying the groundwork for advances in interpretability research and mitigating the hallucination issue associated with large models [19; 10].

## 3 Method

In this section, we introduce our new paradigm and two key model design novelty featuring OpenVik, relation-oriented multimodality model prompting and diversity-driven data enhancement.

### Open Visual Knowledge Extraction

Given a dataset \(\mathcal{D}=\{(\mathcal{I}_{i},\mathbf{T}_{i},\mathbf{U}_{i})\}_{i=1}^{M}\) consisting of \(M\) samples, \(\mathcal{I}_{i}\) is the \(i\)-th image (such as the input image in Figure 1), \(\mathbf{T}_{i}=\{\mathcal{T}_{j}\}_{j=1}^{n_{i}}\) is a set of \(n_{i}\) region descriptions (such as "_the boat on water_" in Figure 1), \(\mathbf{U}_{i}=\{\mathcal{U}_{j}\}_{j=1}^{n_{i}}\) is the set of \(n_{i}\) relation-oriented visual regions, where each \(\mathcal{T}_{j}\) corresponds to a visual region \(\mathcal{U}_{j}\in\mathbf{U}_{i}\) in image \(\mathcal{I}_{i}\). The goal of our open visual knowledge discovery is to train a model \(\mathcal{M}\) capable of producing a set of format-free knowledge descriptions (such as "_large boat docked at pier_" in Figure 1) given any image \(\mathcal{I}_{k}\) during the inference stage.

### Relation-Oriented Multimodality Model Prompting

The overall architecture of OpenVik is shown in Figure 1. It comprises two modules: an open relational region detector \(\mathcal{M}_{v}\) and a format-free visual knowledge generator \(\mathcal{M}_{t}\). The two modules are learned separately during training with our diversity-enhanced data (Section 3.3) and combined to produce format-free visual knowledge at inference. Specifically, the relational region detector \(\mathcal{M}_{v}\) takes an image \(\mathcal{I}_{i}\) as the input and learns to select a flexible number of relational regions \(\mathbf{U}_{i}=\{(\mathcal{U}_{j})\}_{j=1}^{n_{i}}\) that captures object interactions, each corresponding to a description \(\mathcal{T}_{j}\) in \(\mathbf{T}_{i}\); the visual knowledge generator \(\mathcal{M}_{t}\) generates format-free knowledge descriptions by prompting and fine-tuning the multimodality model with the guidance of detected visual region \(\mathcal{U}_{j}\). All notations for the region detector and knowledge generator are detailed in Table 9 and Table 10, respectively.

**Open relational region detector.** Although existing object detection algorithms have been widely recognized for their efficiency in object detection, they are usually restricted to object-centric visual regions in a predefined set, and thus cannot directly capture open relational information with a single box. Detecting regions containing relational knowledge remains to be a challenge. We make two adaptions on the object detection FasterRCNN [38] to train the open relational region detector:

* _Region Regression_: we change the original object-centric region labels to our newly created relation-centric box labels, denoted as \(\mathbf{U}_{j}\). The foreground of each relation-centric region label \(\mathcal{U}_{j}\) is created by taking the union of the object-level bounding boxes of the entities, i.e., _boat, water_, contained in a ground truth region knowledge description \(\mathcal{T}_{j}\). This forms the region regression loss \(\mathcal{L}_{\text{RD}}\).

\begin{table}
\begin{tabular}{c|c} \hline \hline
**Notation** & **M Meaning** \\ \hline \(\mathcal{I}_{i}\) & input image of the relational region detector \\ \(\mathcal{U}_{i}\) & relation-centric box label \\ \(\mathcal{U}_{i}\) & set of relation-centric boxes of an image \\ \(\mathcal{T}_{i}\) & region description of the new \\ \(\mathcal{T}_{i}\) & set of region descriptions of the new image \\ \(\mathcal{L}_{\text{RD}}\) & region representation that supervised by Open regional boxes \\ \(\mathcal{L}_{\text{RC}}\) & knowledge generation loss supervised by Open relational knowledge \\ \(\mathcal{L}_{i}\) & the overall objective of the relational region detector \\ \hline \hline \end{tabular}
\end{table}
Table 1: Notations for open region detector.

Figure 1: The overview of OpenVik. The left orange and purple panels illustrate key components of relation-oriented multimodality model prompting: open relational region detector and format-free visual knowledge generator. The right green one depicts diversity-driven data enhancement strategy. OpenVik is designed to extract relation-oriented format-free open visual knowledge with novel entities, diverse relations, and nuanced descriptive details.

\begin{table}
\begin{tabular}{c|c} \hline \hline
**Notation** & **M Meaning** \\ \hline \(\mathcal{I}_{i}\) & input image of the relational region detector \\ \(\mathcal{U}_{i}\) & relation-centric box label \\ \(\mathcal{U}_{i}\) & set of relation-centric boxes of an image \\ \(\mathcal{T}_{i}\) & region description of the new \\ \(\mathcal{T}_{i}\) & set of region descriptions of the new \\ \(\mathcal{L}_{\text{RD}}\) & region representation of the new image \\ \(\mathcal{L}_{\text{RD}}\) & region representation that supervised by Open regional boxes \\ \(\mathcal{L}_{\text{RC}}\) & knowledge generation loss supervised by Open relational knowledge \\ \(\mathcal{L}_{\text{RC}}\) & the overall objective of the relational region detector \\ \hline \hline \end{tabular}
\end{table}
Table 2: Notations for knowledge generator.

* _Knowledge Supervision_: To assist with the refinement of the bounding box, we replaced the object-centric label classification in traditional object detectors with knowledge supervision. A pre-trained generator is finetuned to create the regional description grounded to the given region. This is supervised by the cross-entropy loss \(\mathcal{L}_{\text{K}}\) with region description \(\mathbf{T}_{j}\).

The training objective \(\mathcal{L}_{l}\) of the relational region detector is formulated as below, where \(\mathcal{L}_{\text{RD}}\) is the regional regression loss and \(\mathcal{L}_{\text{K}}\) is the knowledge supervision loss,

\[\mathcal{L}_{v}=\mathcal{L}_{\text{RD}}+\mathcal{L}_{\text{K}}.\] (1)

**Format-free visual knowledge generator.** OpenVik provides better knowledge grounding by conditioning the generator on the detected relational region, leading to a reasoning-driven generation. Specifically, the detected bounding box (such as the box containing "_boat_" and "_pier_" on the far left) is utilized as a visual prompt when fine-tuning the visual knowledge generator. The model architecture of the knowledge generator is built upon a combined large multimodality model, which composes a pre-trained vision transformer ViT-B [12] and the image-grounded text decoder of BLIP [27]. The two modules are jointly trained on a generic image-text paired dataset comprising over 14 million entries and fine-tuned on the image captioning task, which delivered state-of-the-art performance.

In our visual knowledge generator, the decoder takes the ViT visual representation of the entire image as input and leverages the detected regional mask as a binary visual prompt. This prompt aids in filtering out the background and directing attention toward the relational foreground. The generation of format-free knowledge from the decoder is supervised by the language modeling loss \(\mathcal{L}_{\text{MLE}}\), which further refines visual attention during the knowledge generation process. As a result, our approach facilitates the production of format-free outcomes that extend beyond the conventional sub-verb-obj form. Besides, to improve information variety, we introduce an amplifying penalty factor for highly similar knowledge generation. For any two generated sequences \(T_{a}\) and \(T_{b}\) describing image \(\mathcal{I}_{i}\),

\[\mathcal{L}_{\text{V}}=\frac{1}{N_{i}}\sum_{N_{i}}\mathrm{ReLU}\left(-\log \left(1-\left(s\left(T_{a},T_{b}\right)-\phi\right)\right)\right),\] (2)

where \(N_{i}\) is the number of generated knowledge of image \(\mathcal{I}_{i}\), \(s\left(T_{a},T_{b}\right)\) indicates the semantic cosine similarity, and \(\phi\) is a hyper-parameter set as 0.01 controlling the penalty on sequences with only slight difference (e.g. "_dog chasing the man_" and "_dog likching the man_") to be relatively small.

The training objective \(\mathcal{L}_{l}\) of the format-free visual knowledge generator is formulated as

\[\mathcal{L}_{l}=\alpha\times\mathcal{L}_{\text{MLE}}+(1-\alpha)\times\mathcal{ L}_{\text{V}},\] (3)

where \(\alpha\) is a weighting hyper-parameter we set as 0.7. The trained relational region detector and visual knowledge generator are combined during inference. Given any image \(\mathcal{I}\), the open relational region detector first detects a flexible number of open relations regions of interest, then each detected region \(\mathcal{R}\) is passed to the format-free visual knowledge generator, where a relation-oriented format-free knowledge phrase (such as "_flying jet leaving behind smoke_" in Figure 1) is generated to describe the given visual focus subarea \(\mathcal{R}\) of the image. To further encourage within-sequence language variety during inference, we leverage the contrastive decoding strategy from [43], which improves over nucleus sampling and beam search.

### Diversity-driven Data Enhancement

The training data for relational knowledge extraction usually exhibits a long-tail distribution, where more prevalent but simple relations such as _in_, _on_, and _wear_ dominate the training set [44]. Consequently, the model trained with such a biased dataset may render limited, and repetitive knowledge. As a remedy, we propose two data enhancement techniques to optimize the data distribution. As the foundational measure for given relation \(r\)'s importance, we design a grid TF-IDF+ score \(\mathcal{S}_{r}\)[34, 54]:

\[\mathcal{S}_{r}=(\log(\frac{N}{1+f_{r}*\alpha_{1}}))^{\alpha_{2}},\] (4)

where \(N\) is the total number of knowledge phrases in the datasets, \(f_{r}\) is the number of occurrences of the relation \(r\), \(\alpha_{1}\) and \(\alpha_{2}\) are the grid scales whose values are selected based on \(f_{r}\).

**Random dropping on low-quality data.** We first remove repeated knowledge descriptions in the same image and then randomly drop descriptions that contain frequently occurring yet meaningless 

[MISSING_PAGE_FAIL:5]

limitation of training data, we incorporate four additional metrics [31], which delve into an in-depth quality evaluation of the extracted visual knowledge from four distinct perspectives:

* _Validity_ (\(\uparrow\)): whether the generated visual knowledge is valid to human.
* _Conformity_ (\(\uparrow\)): whether the generated knowledge faithfully depicts the scenarios in the images.
* _Freshness_ (\(\uparrow\)): the novelty of the knowledge, i.e., the proportion not present in the training set.
* _Diversity_ (\(\uparrow\)): the language variance between a randomly sampled pair of knowledge pieces.

Among the four metrics, both the validity and conformity metrics involve human annotators. We randomly selected 100 images as the evaluative subset. Details regarding the scoring guidance and the interface provided to the annotators can be found in Appendix D. The remaining metrics, i.e., freshness and diversity, are calculated automatically. The in-depth knowledge quality evaluation results are displayed in the right part of Table 3, where the average pairwise Cohen's \(\kappa\) on human evaluation results is 0.76 (good agreement). The findings demonstrate that trained with the diversity-enhanced datasets, the format-free visual knowledge extracted by OpenVik significantly outperforms other types of baselines in terms of all four metrics. The improvement of diversity, in particular, reaches 14% relatively compared with the inference results from the second runner DenseCap, indicating the advantage of OpenVik in generating rich and comprehensive visual knowledge.

### Comparison with Existing Knowledge Sources

We compare the extracted visual knowledge with the non-parametric knowledge in the existing knowledge graph (KG) and the parametric knowledge from the large language model (LLM). The comparison insights from the three knowledge resources are shown in the Venn Diagram in Figure 2.

**Compare with non-parametric knowledge.** We take ConceptNet [41] as the representative in the comparison with non-parametric knowledge. To map the knowledge generated by OpenVik to ConceptNet, we parse the knowledge into triplets and associate the endpoints of these triplets with

\begin{table}
\begin{tabular}{l|c c c|c c c c} \hline \hline \multirow{2}{*}{**Method**} & \multicolumn{3}{c|}{**Generation Performance**} & \multicolumn{3}{c}{**In-Depth Knowledge Quality**} \\ \cline{2-9}  & BLEU\(\uparrow\) & ROUGE-L\(\uparrow\) & METEOR\(\uparrow\) & Validity\(\uparrow\) & Conformity\(\uparrow\) & Freshness\(\uparrow\) & Diversity\(\uparrow\) \\ \hline \multicolumn{9}{l}{_Closed/Open Scene Graph Generation_} \\ IMP [52] & 0.075 & 0.123 & 0.118 & 0.800 & 0.823 & 0.676 & 0.316 \\ Neural Motifs [63] & 0.229 & 0.283 & **0.273** & 0.822 & 0.767 & 0.667 & 0.349 \\ UnbiasSGG [44] & 0.217 & 0.258 & 0.194 & 0.739 & 0.733 & 0.666 & 0.357 \\ Ov-SGG [17] & 0.167 & 0.210 & 0.183 & 0.712 & 0.633 & 0.693 & 0.413 \\ \hline \multicolumn{9}{l}{_Dense Relational Captioning_} \\ MTTSNet+REM [22] & 0.240 & 0.226 & 0.228 & 0.897 & 0.852 & 0.754 & 0.375 \\ \hline \multicolumn{9}{l}{_Region Captioning_} \\ DensCap [20] & 0.248 & 0.245 & 0.196 & 0.883 & 0.843 & 0.790 & 0.543 \\ Sub-GC [65] & 0.272 & 0.263 & 0.221 & 0.892 & 0.871 & 0.795 & 0.547 \\ BLIP [27] & 0.264 & 0.266 & 0.252 & 0.886 & 0.855 & 0.760 & 0.531 \\ BLIP2 [26] & 0.275 & **0.285** & 0.257 & 0.892 & 0.871 & 0.766 & 0.535 \\ \hline \multicolumn{9}{l}{_Open Visual Knowledge Extraction_} \\ OpenVik & **0.280** & 0.283 & 0.250 & **0.907** & **0.883** & **0.809** & **0.619** \\ \hline \hline \end{tabular}
\end{table}
Table 3: Knowledge comparison of OpenVik and baselines on performance and in-depth quality (%).

Figure 2: The Venn diagram of knowledge comparison between the open visual knowledge from OpenVik with the non-parametric knowledge from existing knowledge graph (i.e., ConceptNet) and parametric knowledge from large language model (i.e., COMET).

nodes in ConceptNet. Then we calculate the similarity of embeddings3 between the parsed relation and all the edge relations among the mapped nodes in ConceptNet. If the similarity score exceeds a predetermined threshold, i.e., 0.75, we consider the mapping successful. As illustrated in Figure 2, we observe that compared with the non-parametric knowledge in KG, the extracted visual knowledge captures richer and more meaningful spatial details, e.g., "_three layer cake on table_", and motion dynamics, e.g., "_baby elephants walking around adventurous wood_".

Footnote 3: Embeddings are produced by ConceptNet API: https://github.com/commonsense/conceptnet-numberbatch.

**Compare with parametric knowledge.** We compare with parametric knowledge contained in LLM by prompting the gpt-3.5-turbo4 model with the object information in the image. The prompt template used is detailed in Appendix E. The mapping process follows the approach mentioned earlier. It is found that compared with the parametric knowledge in LLM, the extracted visual knowledge exhibits unique fine-grained visual details, e.g., "_red sticker on fence_", and provides precise scene information, e.g., "_the light shining from bright black background_".

Footnote 4: https://platform.openai.com/docs/models/gpt-3-5

### Ablation Study

**The influence on knowledge quality with information variety regularization and data strategies.** We conducted ablation studies to evaluate the effectiveness of the information variety regularizer, \(\mathcal{L}_{\text{V}}\), and our diversity-driven data enhancement strategies. This involves an in-depth assessment of knowledge quality on the same evaluation subset. The results are presented in Figure 3. It is evident from the results that our proposed information variety design primarily impacts freshness and diversity, without compromising validity and conformity. For the freshness, the omission of data augmentation for entities and relations results in the most significant performance degradation. This implies the crucial role these strategies play in infusing novel knowledge into the generation process. As for diversity, the most notable changes in metrics are observed when the \(\mathcal{L}_{\text{V}}\) and random dropping are removed. The strategy for augmenting entities and relations also plays a valuable role in enriching diversity.

**Ablation of the pre-training for the open relational region detector.** We conducted a comparison of the outcomes when loading a pre-trained detector backbone versus training the detector from scratch, as shown by the yellow bar in Figure 3. Results demonstrate a noticeable decrease in both knowledge diversity and freshness, which indicates the importance of loading the pre-trained model for region detection. This may be because omitting the pre-training step of the FasterRCNN model tends to result in the detection of more overlapping regions, which in turn causes the drop.

**The influence on dataset diversity with data strategies.** We conduct a direct analysis of the knowledge diversity of the existing datasets and our diversity-enhanced one, compared with the visual knowledge generated from OpenVik. The findings, presented in Table 4, show that the diversity-driven data enhancement strategies significantly boost knowledge diversity. Trained with this enhanced data, OpenVik can extract visual knowledge that exhibits greater diversity than that found in the _Visual Genome_ and _Relational Caps_, indicating the advantage of OpenVik to format-free visual knowledge generation and its ability to yield richer knowledge diversity.

### Case Study

We present two case studies in Figure 4 (See Appendix F for more) to showcase the format-free visual knowledge generated by OpenVik, in comparison to Visual Genome (Scene Graph and Region Description) and Relational Caps. Contrary to the rigidity of scene graphs, which strictly adhere to a

\begin{table}
\begin{tabular}{l|c|c|c|c} \hline \hline \multirow{2}{*}{**Metrics**} & \multicolumn{3}{c|}{**Training Dataset**} & **Generate Knowledge** \\  & _Visual Genome [24]_ & _Relational Caps [22]_ & _Diversity Enhanced (Ours)_ & OpenVik (_Ours_) \\ \hline
**Diversity** & 0.589 & 0.604 & 0.632 & 0.619 \\ \hline \hline \end{tabular}
\end{table}
Table 4: Diversity of existing and enhanced datasets and generated knowledge from OpenVik.

Figure 3: The influence of information variety regularization and diversity-driven data enhancement strategies.

predefined format, OpenVik can generate knowledge with a flexible semantic structure, not strictly bound to the sub-verb-obj format (e.g., "_blue post attached to wall with white letter_"). Examples of this adaptability are highlighted in red. When compared to dense region descriptions, the relational knowledge extracted by OpenVik offers a deeper understanding of the multiple entity interactions within an image. In comparison to Relational Caps, which mainly focus on interactions between two objects, OpenVik significantly broadens the diversity of relation with vivid verbs (e.g., "_attached to_", "_adorning_"). Moreover, it introduces novel entities (e.g., "_post_", "_mane_") and enriches the knowledge representation with nuanced details (e.g., "_full of_", "_striped_") that are missed by Relational Caps.

Note that we observe the unbalanced and noisy distributions within the training data can lead to errors in the knowledge produced. Viewing hallucinations as erroneous inferences based on input, the inaccuracies observed in OpenVik and similar baselines often stem from detection errors. These errors are typically caused by data biases that incorrectly associate features with a specific class or label. We further two illustrative failure cases in Figure 5. For example, a "_black speaker by flat tr_" is generated, although the speaker is not present in the image--possibly reflecting common co-occurrences within the dataset. Similarly, a ladder in the right figure has been misidentified as a towel, leading to the erroneous description of a "_blue towel hanging from dry shower_". The key to mitigating such incorrect inference is identifying the cofounder feature of class labeling.

## 5 Application

This section explores whether the extracted open visual knowledge from OpenVik can bolster reasoning and inference capabilities in multimodality downstream tasks by augmenting a baseline in the challenging zero-shot setting.

### Text-to-Image Retrieval

**Task Setting.** In the text-to-image retrieval task, a given caption is matched to a large set of candidate images, with the most relevant image returned as the result. Adopting the challenging zero-shot

Figure 4: Case study on the extracted open visual knowledge from OpenVik. Examples of format-free knowledge are highlighted in red. Compared with VG and Relational Caps, OpenVik performs better at capturing novel entities, broadening object interactions with diverse relations, and enriching the knowledge representation with nuanced descriptive details.

Figure 5: Examples of incorrectly knowledge resulting from distribution bias are highlighted.

[MISSING_PAGE_FAIL:9]

that can be accurately mapped to extracted visual knowledge, as well as 138 verbs that have a fuzzy match through ConceptNet embedding comparison. The full lists of the exact and fuzzy-matched verbs are detailed in Appendix H. The evaluated metrics include Accuracy, Precision, Recall, and \(F_{1}\). The results are presented in Table 6. It can be observed that knowledge enrichment significantly outperforms the zero-shot and BLIP baselines. This suggests that the verb-related contexts introduced by OpenVik-generated knowledge are intuitive and greatly assist in understanding the semantics of event verbs, bolstered by related visual information.

### Visual Commonsense Reasoning

**Task setting.** The goal of visual commonsense reasoning is to predict an answer from four given option candidates for a given image and question. For the baseline approach, we compare the backbone model R2C from the VCR paper [62] and BLIP [27]. In the visual knowledge-enhanced OpenVik Enriched approach, we perform two-level context augmentation, incorporating both entities and relations: (1) we parse the question and options to obtain all (S, 0) pairs and, for each entity pair, apply the same relation augmentation as in the image retrieval task; (2) for the V in each option, we enrich the visual context using the same method as illustrated in grounded situation recognition.

**Qualitative examples.** Figure 8 presents an example before and after applying the two-level visual knowledge-based enrichment for visual commonsense reasoning. The results indicate that visual knowledge enhances the correspondence between the correct answer and the image itself.

**Quantitative results.** We assembled a test set of 939 samples from the validation set of the VCR dataset [62]. Each sample in this test set contains questions and answers with a minimum of five nouns and two relations, guaranteeing an adequate level of information complexity for meaningful engagement with open visual knowledge. The results can be found in Table 7. We observe that the enriched visual knowledge helps especially when solving reasoning questions on humans and their interactions with visually impressive entities, such as "_game_" in Figure 8. This enhancement results in a performance improvement above 3.0% over the zero-shot baseline.

## 6 Conclusion, Limitations, and Future Work

This work is the first exploration of a new paradigm of open visual knowledge extraction, which combines an open relational region detector to flexibly pinpoint relational regions and a format-free visual knowledge generator that generates visual knowledge by prompting a multimodality model conditioned on the region of interest. To further enhance the diversity of the generated knowledge, we explore two distinct data enhancement techniques. Extensive knowledge evaluations underscore the correctness and uniqueness of our extracted open visual knowledge, and the consistent improvements observed across various visual reasoning tasks highlight the real-world applicability of OpenVik.

While our approach has been shown effective in various scenarios, its performance at larger scales or on more diverse datasets remains to be studied. Future work could investigate its effectiveness across a broader range of tasks and contexts. Also, the current model requires fine-tuning for the visual knowledge extractor. Developing a model that can generalize well with prompt tuning or demonstration augmentation could be another interesting direction for future work.

## 7 Acknowledgments

Carl Yang was supported by the National Institute Of Diabetes And Digestive And Kidney Diseases of the National Institutes of Health under Award Number K25DK135913.

\begin{table}
\begin{tabular}{l|c c c c} \hline \hline
**Method** & **Accuracy** & **Precision** & **Recall** & \(\mathbf{F}_{i}\) \\ \hline R2C & 56.66 & 56.73 & 56.72 & 56.72 \\ \hline
**OpenVik\(\mathbf{\approx}\)R2C** & 59.96 & 64.01 & 60.03 & 60.02 \\ BLIP & 62.50 & 62.45 & 62.47 \\
**OpenVik\(\mathbf{\approx}\)BLIP** & 67.04 & 67.54 & 67.43 & 67.48 \\ \hline \hline \end{tabular}
\end{table}
Table 7: Visual commonsense reasoning results (%) of OpenVik context enrichment compared with zero-shot baselines.

Figure 8: An example of OpenVik context enrichment on the VCR task (See Appendix G.3 for more).

[MISSING_PAGE_FAIL:11]

* [18] Andreas Holzinger, Peter Kieseberg, Edgar Weippl, and A Min Tjoa. Current advances, trends and challenges of machine learning and knowledge extraction: from machine learning to explainable ai. In _CD-MAKE_, 2018.
* [19] Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung. Survey of hallucination in natural language generation. _ACM Comput Surv_, 55:1-38, 2023.
* [20] Justin Johnson, Andrej Karpathy, and Li Fei-Fei. Densecap: Fully convolutional localization networks for dense captioning. In _CVPR_, 2016.
* [21] Xuan Kan, Hejie Cui, and Carl Yang. Zero-shot scene graph relation prediction through commonsense knowledge integration. In _European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML-PKDD)_, 2021.
* [22] Dong-Jin Kim, Jinsoo Choi, Tae-Hyun Oh, and In So Kweon. Dense relational captioning: Triple-stream networks for relationship-based captioning. In _CVPR_, 2019.
* [23] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. In _NeurIPS_, 2022.
* [24] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, et al. Visual genome: Connecting language and vision using crowdsourced dense image annotations. _IJCV_, 123:32-73, 2017.
* [25] Hunter Lang, Monica N Agrawal, Yoon Kim, and David Sontag. Co-training improves prompt-based learning for large language models. In _ICML_, 2022.
* [26] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. _arXiv preprint arXiv:2301.12597_, 2023.
* [27] Junnan Li, Dongxu Li, Caiming Xiong, and Steven C. H. Hoi. BLIP: bootstrapping language-image pre-training for unified vision-language understanding and generation. In _ICML_, 2022.
* [28] Manling Li, Alireza Zareian, Ying Lin, Xiaoman Pan, Spencer Whitehead, Brian Chen, Bo Wu, Heng Ji, Shih-Fu Chang, Clare Voss, et al. Gaia: A fine-grained multimedia knowledge extraction system. In _ACL_, 2020.
* [29] Ye Liu, Hui Li, Alberto Garcia-Duran, Mathias Niepert, Daniel Onoro-Rubio, and David S Rosenblum. Mmkg: multi-modal knowledge graphs. In _ESWC_, 2019.
* [30] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In _ICLR_, 2019.
* [31] Ximing Lu, Sean Welleck, Jack Hessel, Liwei Jiang, Lianhui Qin, Peter West, Prithviraj Ammanabrolu, and Yejin Choi. Quark: Controllable text generation with reinforced unlearning. _NeurIPS_, 2022.
* [32] Timo Luddecke and Alexander Ecker. Image segmentation using text and image prompts. In _CVPR_, 2022.
* [33] Sachit Menon and Carl Vondrick. Visual classification via description from large language models. In _ICLR_, 2023.
* [34] Jiaul H Paik. A novel tf-idf weighting scheme for effective ranking. In _SIGIR_, 2013.
* [35] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. _NeurIPS_, 2019.
* [36] Sarah Pratt, Mark Yatskar, Luca Weihs, Ali Farhadi, and Aniruddha Kembhavi. Grounded situation recognition. In _ECCV_, 2020.

* [37] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In _ICML_, 2021.
* [38] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. _NeurIPS_, 2015.
* [39] Fereshteh Sadeghi, Santosh K Kumar Divvala, and Ali Farhadi. Viske: Visual knowledge extraction and question answering by visual verification of relation phrases. In _CVPR_, 2015.
* [40] Maarten Sap, Ronan Le Bras, Emily Allaway, Chandra Bhagavatula, Nicholas Lourie, Hannah Rashkin, Brendan Roof, Noah A Smith, and Yejin Choi. Atomic: An atlas of machine commonsense for if-then reasoning. In _AAAI_, 2019.
* [41] Robyn Speer, Joshua Chin, and Catherine Havasi. Conceptnet 5.5: An open multilingual graph of general knowledge. In _AAAI_, 2017.
* [42] Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu Wei, and Jifeng Dai. Vl-bert: Pre-training of generic visual-linguistic representations. In _ICLR_, 2020.
* [43] Yixuan Su, Tian Lan, Yan Wang, Dani Yogatama, Lingpeng Kong, and Nigel Collier. A contrastive framework for neural text generation. In _NeurIPS_, 2022.
* [44] Kaihua Tang, Yulei Niu, Jianqiang Huang, Jiaxin Shi, and Hanwang Zhang. Unbiased scene graph generation from biased training. In _CVPR_, 2020.
* [45] Komal Teru, Etienne Denis, and Will Hamilton. Inductive relation prediction by subgraph reasoning. In _ICML_, 2020.
* [46] Maria Tsimpoukelli, Jacob L Menick, Serkan Cabi, SM Eslami, Oriol Vinyals, and Felix Hill. Multimodal few-shot learning with frozen language models. _NeurIPS_, 2021.
* [47] Chenguang Wang, Xiao Liu, and Dawn Song. Language models are open knowledge graphs. _arXiv preprint arXiv:2010.11967_, 2020.
* [48] Meng Wang, Sen Wang, Han Yang, Zheng Zhang, Xi Chen, and Guilin Qi. Is visual context really helpful for knowledge graph? a representation learning perspective. In _ACM MM_, 2021.
* [49] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models. In _NeurIPS_, 2022.
* [50] Sean Welleck, Ilia Kulikov, Stephen Roller, Emily Dinan, Kyunghyun Cho, and Jason Weston. Neural text generation with unlikelihood training, 2019.
* [51] Danfei Xu, Yuke Zhu, Christopher Choy, and Li Fei-Fei. Scene graph generation by iterative message passing. In _CVPR_, 2017.
* [52] Danfei Xu, Yuke Zhu, Christopher B Choy, and Li Fei-Fei. Scene graph generation by iterative message passing. In _CVPR_, 2017.
* [53] Ran Xu, Yue Yu, Hejie Cui, Xuan Kan, Yanqiao Zhu, Joyce Ho, Chao Zhang, and Carl Yang. Neighborhood-regularized self-training for learning with few labels. In _AAAI_, 2023.
* [54] Ran Xu, Yue Yu, Joyce Ho, and Carl Yang. Weakly-supervised scientific document classification via retrieval-augmented multi-stage training. In _SIGIR_, pages 2501-2505, 2023.
* [55] Ran Xu, Yue Yu, Chao Zhang, Mohammed K Ali, Joyce C Ho, and Carl Yang. Counterfactual and factual reasoning over hypergraphs for interpretable clinical predictions on ehr. In _Machine Learning for Health_, 2022.
* [56] Yiben Yang, Chaitanya Malaviya, Jared Fernandez, Swabha Swayamdipta, Ronan Le Bras, Ji Ping Wang, Chandra Bhagavatula, Yejin Choi, and Doug Downey. Generative data augmentation for commonsense reasoning. In _Findings of EMNLP_, 2020.

* [57] Yue Yu, Kexin Huang, Chao Zhang, Lucas M Glass, Jimeng Sun, and Cao Xiao. Sumgnn: multi-typed drug interaction prediction via efficient knowledge graph summarization. _Bioinformatics_, 2021.
* [58] Yue Yu, Rongzhi Zhang, Ran Xu, Jieyu Zhang, Jiaming Shen, and Chao Zhang. Cold-start data selection for few-shot language model fine-tuning: A prompt-based uncertainty propagation approach. _arXiv preprint arXiv:2209.06995_, 2022.
* [59] Yue Yu, Yuchen Zhuang, Jieyu Zhang, Yu Meng, Alexander Ratner, Ranjay Krishna, Jiaming Shen, and Chao Zhang. Large language model as attributed training data generator: A tale of diversity and bias. _arXiv preprint arXiv:2306.15895_, 2023.
* [60] Alireza Zareian, Swebor Karaman, and Shih-Fu Chang. Bridging knowledge graphs to generate scene graphs. In _ECCV_, 2020.
* [61] Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. Star: Bootstrapping reasoning with reasoning. _NeurIPS_, 2022.
* [62] Rowan Zellers, Yonatan Bisk, Ali Farhadi, and Yejin Choi. From recognition to cognition: Visual commonsense reasoning. In _CVPR_, 2019.
* [63] Rowan Zellers, Mark Yatskar, Sam Thomson, and Yejin Choi. Neural motifs: Scene graph parsing with global context. In _CVPR_, 2018.
* [64] Hongming Zhang, Xin Liu, Haojie Pan, Yangqiu Song, and Cane Wing-Ki Leung. Aser: A large-scale eventuality knowledge graph. In _WWW_, 2020.
* [65] Yiwu Zhong, Liwei Wang, Jianshu Chen, Dong Yu, and Yin Li. Comprehensive image captioning via scene graph decomposition. In _ECCV_, 2020.
* [66] Chunting Zhou, Junxian He, Xuezhe Ma, Taylor Berg-Kirkpatrick, and Graham Neubig. Prompt consistency for zero-shot task generalization. In _Findings of EMNLP_, 2022.
* [67] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Conditional prompt learning for vision-language models. In _CVPR_, 2022.

Details of Data Augmentation with External Knowledge Resources

_Enhance Relation Recognition_: We enriched the relationships between objects parsed from the original knowledge descriptions by leveraging the external resource of ConceptNet. ConceptNet comprises commonly observed entities and their connections, where edge weights signify the reliability and frequency of these relationships. The typical value of edge weights in ConceptNet is 1. To prevent the redundancy of common information and to maintain the validity of the enriched relations, we categorized the relationships based on their weights. Relationships with weights less than 1 were deemed "weak" and those with a weight of 1 were labeled "average". We refrained from using these categories for relation enhancement. Instead, only relationships with weights greater than 1, indicative of high reliability, were employed for augmenting the relations.

_Boost Entity Perception_: On the entity side, we augment complement entities and descriptive information with two external knowledge resources. On one hand, for descriptions with a high TF-IDF+ score, we enrich related entities of the object from ConceptNet to create additional knowledge descriptions. The relatedness is based on the between-word relatedness score provided by ConceptNet and we take the threshold as 0.85. On the other hand, we employ the Commonsense Transformers (COMET) [4] model to enrich related new objects and descriptive information. The COMET model is a language model designed to generate commonsense knowledge and understand causal relationships between descriptions. It is pretrained using the atomic dataset, which consists of structured, crowdsourced knowledge about everyday events and their associated causes and effects. The COMET model can provide neighbor descriptions of the given input of nine different categories of relation. We take the xAttr and oEffect relation categories and augmented the COMET model by formulating the existing knowledge description texts as the input and choose the corresponding category branch during generation for enriching objects and descriptions respectively.

## Appendix B Dataset Information

The statistic information of our augmented dataset is summarized in Table 8, where **split** specifies the dataset split, **#image** indicates the number of images in the split, **#descriptor** indicates the total number of relational descriptors of the images, **#relation** is the total number of unique relations in the relational descriptors after deduplication, and **#subject & object** is the total number of subjects and objects contained in the description text.

## Appendix C Implementation Details

**Open relational region detector.** The visual feature extraction backbone is constructed upon a pre-trained ResNet50-FPN. The detector head incorporates a BLIP\({}_{\text{base}}\) equipped with the essential

\begin{table}
\begin{tabular}{c|c} \hline \hline \multicolumn{1}{c|}{**Hyperparameter**} & \multicolumn{1}{c}{**Assignment**} \\ \hline batch size & 4 \\ learning rate optimizer & Adam \\ Adam epsilon & 1e-8 \\ Adam initial learning rate & 1e-5 \\ learning rate scheduler & cosine scheduler \\ Adam decay weight & 0.05 \\ \hline \hline \end{tabular} 
\begin{tabular}{c|c} \hline \hline \multicolumn{1}{c|}{**Hyperparameter**} & \multicolumn{1}{c}{**Assignment**} \\ \hline batch size & 4 \\ learning rate optimizer & Adam \\ Adam epsilon & 1e-8 \\ Adam initial learning rate & 1e-5 \\ learning rate scheduler & cosine scheduler \\ Adam decay weight & 0.05 \\ \hline \hline \end{tabular}
\end{table}
Table 9: Hyperparameters for training open relational region detector.

\begin{table}
\begin{tabular}{c c c c c} \hline \hline \multicolumn{1}{c}{**split**} & **\#image** & **\#descriptor** & **\#relation** & **\#subject \& object** \\ \hline Train & 75,456 & 832,351 & 30,241 & 302,735 \\ Validation & 4,871 & 64,137 & 5,164 & 34,177 \\ Test & 4,873 & 62,579 & 5,031 & 32,384 \\ \hline \hline \end{tabular}
\end{table}
Table 8: Dataset statistics.

ViT-B/16 for text supervision, using multiple fully connected layers to derive region features. For each candidate region, we engage a regressor to conduct boundary regression on these features. The detector undergoes fine-tuning for 20 epochs using the relational region bounding box dataset and an Adam optimizer [30]. The hyperparameters for training are detailed in Table 9.

**Format-free visual knowledge generator.** The format-free visual knowledge generator is initialized from BLIP\({}_{\text{base}}\), which incorporates the basic ViT-B/16. We fine-tune the generator model for 20 epochs using the same optimizer as the one employed for the region detector. Detailed hyperparameters for the visual knowledge generator can be found in Table 10.

## Appendix D Human Evaluation Guidance and Interface

We perform the human evaluation on two of the four in-depth knowledge quality assessment metrics. We build an interface by referring to [50], where raters are presented with a given image and the corresponding knowledge descriptions and are required to choose one from the multiple choice for two questions on whether the knowledge is valid to humans and whether the knowledge description depicts the image. The detailed scoring criteria for _Validity_ and _Conformity_ are provided below:

* _Validity_ (\(\uparrow\)): _whether the generated visual knowledge is valid to humans_.
* 0 (Invalid): The knowledge description does not conform to human cognition, rendering it unreliable or misleading to humans.
* 1 (Valid): The knowledge description is valid and accurately conforms to human cognition, providing reliable and meaningful knowledge to humans.
* _Conformity_ (\(\uparrow\)): _whether the generated knowledge faithfully depicts the scenarios in the images_.
* 0 (Inconsistent): The knowledge description does not faithfully depict the scenarios in the images, showing significant deviations or discrepancies, making it difficult for users to relate the textual information to the visual context.
* 1 (Partially Conforming): The knowledge description partially conforms to the scenarios in the images, but there might be minor inconsistencies or missing relevant details.
* 2 (Moderately Conforming): The knowledge description exhibits a moderate level of conformity with the scenarios in the images, capturing the key aspects and providing coherent descriptions.
* 3 (Highly Conforming): The knowledge description highly conforms to the scenarios in the images, accurately capturing the details and faithfully representing the visual context.

**Agreement/validation** We use Cohen's \(\kappa\) as the agreement score to measure potential subjectivity involved in ratings of knowledge quality. Cohen's \(\kappa\) is a statistic that is used to measure inter-rater reliability for qualitative items and is scaled from -1 (perfect systematic disagreement) to 1 (perfect agreement), where values \(\leq\) 0 as indicating _no agreement_ and 0.01-0.20 as _none to slight_, 0.21-0.40 as _fair_, 0.41-0.60 as _moderate_, 0.61-0.80 as _substantial_, and 0.81-1.00 as _almost perfect_ agreement. Our calculated average pairwise Cohen's \(\kappa\) on human evaluation results from three different raters is 0.76, which indicates a good agreement.

Figure 9: The human evaluation interface for in-depth knowledge quality evaluation.

Parametric Knowledge Prompting Template

Given an image \(\mathcal{I}\) and the corresponding extracted visual knowledge from it based on OpenVik, we perform knowledge comparison with parametric knowledge contained in LLM by prompting the gpt-3.5-turbo model with the object information contained in the \(\mathcal{I}\). The prompt format is shown in the followings:

Suppose you are looking at an image that contains the following subject and object entities:

Subject list: [Insert the subject names here]

Object list: [Insert the object names here]

Please extract 5-10 condensed descriptions that describe the interactions and/or relations among those entities in the image. Try to elucidate the associations and relationships with diverse language formats instead of being restricted to sub-verb-obj tuples.

## Appendix F More Case Studies of Open Visual Knowledge from OpenVik

Figure 10 shows some other cases on the extracted open visual knowledge from OpenVik. In comparison to VG and Relational Caps, OpenVik exhibits superior performance at capturing novel entities, expanding object interactions through diverse relations, and enriching knowledge representation with nuanced descriptive details. For example for the bottom right image, OpenVik can extract novel entities such as "_tracks_", "_shoe_", diverse relations such as "_sticking out of_", and nuanced descriptive details such as "_cold thick_", "_with man feet on it_", "_braze_". The generated knowledge with a more format-free semantic structure is highlighted in red.

Figure 10: Case studies of open visual knowledge from OpenVik.

More Qualitative Examples on Applications

### Text-to-Image Retrieval

Figure 11 presents more qualitative examples of OpenVik-based visual knowledge enrichment on captions. The enriched text is based on the objects present in the images themselves, supplemented with additional relationships from our generated visual knowledge in OpenVik. It is shown that the introduced relationships often provide new context information that aligns with the visual content of the images. For example, in the image of an old woman sitting on a bench in a park, the enriched context information includes the positional relationship between the "_bench_", "_fence_", and "_park_", which provides a more comprehensive description of the original image.

### Grounded Situation Recognition

Figure 12 presents more qualitative examples of OpenVik-based context enrichment in the grounded situation recognition (GSR) task. Our context enrichment setting for the GSR task is to perform enrichment based on verbs like "_shopping_" and "_carrying_". We further restrict the enriched context with the objects contained in the image to avoid noisy enrichment. For example, for the image showing people shopping at a market, the enriched knowledge contexts could be "_the people shopping at market_", "_standing person shopping for fruit_". The idea is to enrich the original description \(\mathcal{T}\): "_An image of \(<\)verb\(>\)_" with relevant actions and relations with the extracted visual knowledge from OpenVik, which can potentially help in drawing-in the matched candidates.

### Visual Commonsense Reasoning

Figure 13 presents more qualitative examples of OpenVik-based context enrichment in the visual commonsense reasoning (VCR) task. The context enrichment on VCR is performed at two-level,

Figure 11: Qualitative examples of OpenVik context enrichment on text-to-image retrieval.

incorporating both entities and relations: (1) we parse the question and options to obtain all (S, 0) pairs and, for each entity pair, apply the same relation augmentation as in the image retrieval task: (2) for the V in each option, we enrich the visual context using the same method as illustrated in GSR. It is shown that unrelated answers are usually enriched with contexts that are not relevant to the image, thus enlarging the distance between incorrect answers and the question, e.g., the enriched contexts "_squating person fixing handy bathroom_" for example 3 in Figure 13. At the same time, the knowledge description of the correct answer is enhanced by incorporating information that aligns with the image contents, e.g., the enriched knowledge contexts "_sitting people on red ground_" for example 1 in Figure 13.

## Appendix H Full List of Filtered Verbs for GSR

We provide the full list of verbs out of the predefined 504 candidates of GSR [36] that can be accurate-matched or fuzzy-matched to extracted visual knowledge in Table 11, based on which we compose the testing subset for our evaluation on GSR application in Section 5.2.

Figure 12: Qualitative examples of OpenVik context enrichment on task GSR.

Figure 13: Qualitative examples of OpenVik context enrichment on task VCR.

\begin{table}
\begin{tabular}{p{34.1pt} p{34.1pt}} \hline \hline
**Matching Type** & **The Word List of Event Types** \\ \hline  & putting, butting, bathing, dusting, rearing, turning, skating, placing, carting, staring, biting, mashing, folding, wetting, sprinkling, branching, drying, standing, flaming, taxiing, performing, circling, molding, parachuting, glowing, fishing, drinking, speaking, pawing, blocking, milking, racing, stripping, potting, spinning, eating, making, kicking, catching, lacing, urinating, sleeping, pressing, buttering, shearing, sliding, hiking, glaring, dipping, swimming, shopping, slicing, shelling, wagging, grilling, crafting, raining, clawing, splashing, rubbing, snowing, breaking, guarding, clipping, sewing, braiding, telephoning, buttoning, waiting, serving, picking, camping, leaning, working, kissing, wrapping, trimming, tripping, pasting, soaring, driving, kneeling, pumping, coloring, lighting, training, ducking, bowing, arching, cooking, checking, pushing, flipping, rocking, cresting, cleaning, reading, nailing, stitching, building, climbing, covering, shelving, attaching, calming, selling, gluing, dyeing, lapping, photographing, peeling, sprouting, licking, displaying, combing, stacking, planting, fastening, buying, mopping, burning, erasing, measuring, dining, tattooing, gardening, decorating, clearing, fixing, weeding, pulling, feeding, watering, crowning, shaking, dripping, emptying, typing, chasing, poking, leaping, pouring, hanging, sniffing, piloting, falling, overflowing, resting, crashing, carving, ballooning, wading, loading, shaving, boarding, pinning, rowing, juggling, shoveling, hugging, throwing, calling, singing, carrying, walking, writing, crouching, floating, painting, opening, tying, riding, strapping, dialing, saying, bubbling, signing, camouflaging, operating, leading, laughing, parading, sking, drawing, gnawing, celebrating, spreading, filling, giving, running, smelling, plowing, helping, brushing, scopping, adjusting, wrinkling, steering, biking, smiling, spraying, boating, paying, chewing, stuffing, clinging, landing, wheeling, talking, scoring, teaching, jogging, pitching, flapping, tipping, scrubbing, sitting, surfing, stirring, competing, drumming, jumping, filming, dancing, waxing, hitting, recording, baking, waving, washing, signaling, chopping, stretching, rafting, microwaving, phoning, lifting, swinging, releasing, ramming, towing, packing, hauling, frying (_244 words_) \\ \hline  & educating, marching, spanking, descending, smearing, heaving, cramming, inflating, stooping, inserting, squeezing, tugging, tilting, moistening, swarming, subduling, wahrling, flexing, punching, attacking, puzzling, sprinting, sucking, puckering, sketching, rotting, videotaping, complaining, tuning, locking, hurling, pricking, arranging, constructing, slapping, sweeping, restraining, dousing, frisking, twisting, wringing, hoisting, immersing, shredding, blossoming, igniting, spying, offering, pouting, confronting, docking, assembling, prying, grinning, sharpening, pruning, disciplining, nipping, coaching, nagging, storming, handcuffing, apprehending, bouncing, clenching, taping, distributing, striking, studying, plunging, curling, aiming, sowing, grinding, rinsing, punting, moving, hitchhiking, skipping, leaking, providing, hunching, spoiling, kneading, burying, foraging, lathering, vaulting, ejecting, mending, pinching, deflecting, ascending, peering, bothering, repairing, pedaling, ailing, fueling, skidding, scraping, soaking, grimancing, scolding, spitting, knocking, crushing, bandaging, saluting, fording, stumbling, discussing, raking, launching, whirling, fetching, brawling, retrieving, snuggling, exercising, colliding, stroking, whipping, tilling, betting, farming, browsing, examining, dropping, barbecuing, ignoring, asking, flinging, perspiring, embracing, slipping, flicking, smashing, arresting, lecturing, tearing, gasping, applying, counting, spilling, dragging, recovering, practicing, scratching, shooting, packaging, hunting, stinging (_154 words_) \\ \hline \hline \end{tabular}
\end{table}
Table 11: The full list of filtered verbs for GSR.