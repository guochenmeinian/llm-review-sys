# Dynamic Conditional Optimal Transport through Simulation-Free Flows

Gavin Kerrigan

Department of Computer Science

University of California, Irvine

gavin.k@uci.edu &Giosue Migliorini

Department of Statistics

University of California, Irvine

gmiglior@uci.edu &Padhraic Smyth

Department of Computer Science

University of California, Irvine

smyth@ics.uci.edu

###### Abstract

We study the geometry of conditional optimal transport (COT) and prove a dynamic formulation which generalizes the Benamou-Brenier Theorem. Equipped with these tools, we propose a simulation-free flow-based method for conditional generative modeling. Our method couples an arbitrary source distribution to a specified target distribution through a triangular COT plan, and a conditional generative model is obtained by approximating the geodesic path of measures induced by this COT plan. Our theory and methods are applicable in infinite-dimensional settings, making them well suited for a wide class of Bayesian inverse problems. Empirically, we demonstrate that our method is competitive on several challenging conditional generation tasks, including an infinite-dimensional inverse problem.

## 1 Introduction

Many fundamental tasks in machine learning and statistics may be posed as modeling a conditional distribution \(\nu(u\mid y)\) where obtaining an analytical representation of \(\nu(u\mid y)\) is impractical. While sampling-based approaches such as Markov Chain Monte Carlo (MCMC) methods are useful, they suffer from several limitations. First, MCMC requires numerous likelihood evaluations, rendering it prohibitively expensive in scientific and engineering applications where the likelihood is determined by an expensive numerical simulator. Second, MCMC must be run away for every observation \(y\), which is impractical in applications such as Bayesian inverse problems (Dashti and Stuart, 2013) and generative modeling (Mirza and Osindero, 2014). These limitations motivate the need for a _likelihood-free_(Cranmer et al., 2020) and _amortized_(Amos et al., 2023) approach. While methods like ABC (Beaumont, 2010) and variational inference (Blei et al., 2017) partially address these challenges, they are difficult to scale to high dimensions or have limited flexibility.

Recently, generative models such as normalizing flows (Papamakarios et al., 2019, 2021), GANs (Ramesh et al., 2022), and diffusion models (Sharrock et al., 2022) have shown promise in amortized and likelihood-free inference. These models may be viewed in the framework of _measure transport_(Baptista et al., 2020), where samples \(u\sim\eta(u)\) from a tractable source distribution are transformed by a mapping \(T(y,u)\) such that the transformed samples are approximately distributed as \(\nu(u\mid y)\). One way to achieve this is through _triangular_ mappings (Baptista et al., 2020; Spantini et al., 2022), where a joint source distribution \(\eta(y,u)\) is transformed by a mapping of the form \(T:(y,u)\mapsto(T_{Y}(y),T_{U}(y,u))\). Under suitable assumptions, if \(T\) transforms the source \(\eta(y,u)\) into the target \(\nu(y,u)\), then \(T_{U}(y,-)\) couples the conditionals \(\eta(u\mid y)\) and \(\nu(u\mid y)\).

Typically, such a map \(T\) is not unique (Wang et al., 2023), and a natural idea is thus to regularize the transport and search for an admissible mapping that is in some sense optimal. In other words, learning a conditional sampler may be phrased as finding a conditional optimal transport (COT) map. While there exists some work on learning COT maps, these approaches often rely on a difficult adversarial optimization problem (Baptista et al., 2020; Hosseini et al., 2023; Bunne et al., 2022; Ray et al., 2022) or simulating from the model during training (Baptista et al., 2023; Wang et al., 2023). In this work, we propose a conditional generative model for likelihood-free inference based on a dynamic formulation of conditional optimal transport. Specifically, our contributions are as follows:

1. We develop a general theoretical framework for dynamic conditional optimal transport in separable Hilbert spaces. Our framework is applicable in infinite-dimensional spaces, enabling applications in function space Bayesian inference. In Section 4, we study the _conditional Wasserstein space_\(\mathbb{P}_{p}^{\mu}(Y\times U)\) and show that this space admits constant speed geodesics between any two measures. In Section 5, we characterize the absolutely continuous curves of measures in \(\mathbb{P}_{p}^{\mu}(Y\times U)\) via the continuity equation and _triangular_ vector fields. As a consequence, we obtain conditional generalizations of the McCann interpolants (McCann, 1997) and the Benamou-Brenier Theorem (Benamou and Brenier, 2000).
2. In Section 6, we propose COT flow matching (COT-FM), a simulation-free flow-based model for conditional generation. This model directly leverages our theoretical framework, where we learn to model a path of measures interpolating between an arbitrary source and target distribution via a geodesic in the conditional Wasserstein space.
3. In Section 7, we demonstrate our method on several challenging conditional generation tasks. We apply our method to two Bayesian inverse problems - one arising from the Lotka-Volterra dynamical system, and an infinite-dimensional problem arising from the Darcy Flow PDE. Our method shows competitive performance against recent COT methods.

## 2 Related Work

Conditional Optimal Transport.Conditional Optimal Transport (COT) remains relatively under-explored in both machine learning and related fields. Recent approaches learn static COT maps via input convex networks (Bunne et al., 2022; Wang et al., 2023) or normalizing flows (Wang et al., 2023). In addition, there have been a number of heuristic approaches to conditional simulation through W-GANs (Sajiadi et al., 2017; Adler and Oktem, 2018; Kim et al., 2022, 2023), for which Chemseddine et al. (2023) provide a rigorous basis. Closely related to our work are those which employ triangular plans (Carlier et al., 2016; Trigila and Tabak, 2016), which have been modeled through GANs in Euclidean spaces (Baptista et al., 2020) and function spaces (Hosseini et al., 2023). In contrast, we use a novel _dynamic_ formulation of COT, which we model through a generalization of flow matching (Lipman et al., 2022; Albergo et al., 2023; Liu et al., 2022). This allows us to use flexible architectures while avoiding the difficulties of training GANs (Arora et al., 2018).

Simulation-Free Continuous Normalizing Flows.Flow matching (Lipman et al., 2022) (and the closely related stochastic interpolants (Albergo et al., 2023) and rectified flows (Liu et al., 2022)) are a class of methods for building continuous-time normalizing flows in a simulation-free manner. Notably, these works do not approximate an optimal transport between the source and target measures. Pooladian et al. (2023) and Tong et al. (2023) propose instead to couple the source and target distributions via optimal transport, leading to marginally optimal paths. In this work, we study an extension of these techniques for conditional generation.

While some works (Davtyan et al., 2023; Gebhard et al., 2023; Isobe et al., 2024; Wildberger et al., 2024) have applied flow matching for conditional generation, these approaches do not employ COT. Notably, these approaches are limited to the finite-dimensional setting, whereas our method adds to the growing literature on function-space generative models (Hosseini et al., 2023; Kerrigan et al., 2023; Kerrigan et al., 2024; Lim et al., 2023; Franzese et al., 2024). Concurrent work by Chemseddine et al. (2024) develops the foundation of dynamic COT with applications to flow matching, and concurrent work by Barboni et al. (2024) develop the theory of dynamic COT for the purposes of studying infinitely deep ResNets. However, these works focus on the Euclidean setting, whereas our methods are applicable in general separable Hilbert spaces.

## 3 Background and Notation

Let \(X,X^{\prime}\) represent arbitrary separable Hilbert spaces, equipped with the Borel \(\sigma\)-algebra. We use \(\mathbb{P}(X)\) to represent the space of Borel probability measures on \(X\), and \(\mathbb{P}_{p}(X)\subseteq\mathbb{P}(X)\) to represent the subspace of measures having finite \(p\)th moment. If \(\eta\in\mathbb{P}(X)\) is a probability measure on \(X\) and \(T:X\to X^{\prime}\) is measurable, then the pushforward measure \(T_{\#}\eta(-)=\eta(T^{-1}(-))\) is a probability measure on \(X^{\prime}\). Maps of the form e.g. \(\pi^{X}:X\times X^{\prime}\to X\) represent the canonical projection.

We assume that we have two separable Hilbert spaces of interest. The first, \(Y\), is a space of observations, and the second, \(U\), is a space of unknowns. These spaces may be of infinite dimensions, but a case of practical interest is when \(Y\) and \(U\) are finite dimensional Euclidean spaces. We will consider the product space \(Y\times U\), equipped with the canonical inner product obtained via the sum of the inner products on \(Y\) and \(U\), under which the space \(Y\times U\) is also a separable Hilbert space. Let \(\eta\in\mathbb{P}(Y\times U)\) be a joint probability measure. The measures \(\pi^{Y}_{\#}\eta\in\mathbb{P}(Y)\) and \(\pi^{U}_{\#}\eta\in\mathbb{P}(U)\) obtained via projection are the _marginals_ of \(\eta\). We use \(\eta^{y}\in\mathbb{P}(U)\) to represent the measure obtained by conditioning \(\eta\) on the value \(y\in Y\). By the disintegration theorem (Bogachev and Raus, 2007, Chapter 10), such conditional measures exist and are essentially unique, in the sense that there exists a Borel set \(E\subseteq Y\) with \(\pi^{Y}_{\#}\eta(E)=0\), and the \(\eta^{y}\) are unique for \(y\notin E\).

### Static Conditional Optimal Transport

In conditional optimal transport, we are given a target measure \(\nu\in\mathbb{P}(Y\times U)\) and some source measure \(\eta\in\mathbb{P}(U)\). We seek a transport map \(T:Y\times U\to U\) such that, for any given \(y\in Y\), the mapping \(T(y,-):U\to U\) transforms the source distribution \(\eta\) into the conditional distribution \(\nu^{y}\), i.e., \(T(y,-)_{\#}\eta=\nu^{y}\). In a sense, \(T\) can be thought of as a collection of transport maps indexed by \(y\in Y\). If such a \(T\) were available, by drawing samples \(u_{0}\sim\eta\) and transforming them, one would obtain samples \(T(y,u_{0})\sim\nu^{y}\). Solving this transport problem for each fixed \(y\) is expensive at best, or impossible when only has a single (or no) samples \((y,u)\sim\nu\) for any given \(y\). Thus, one must leverage information across different observations \(y\). To that end, recent work has focused on the notion of _triangular mappings_\(T:Y\times U\to Y\times U\)(Hosseini et al., 2023, Baptista et al., 2020) of the form \(T(y,u)=(T_{Y}(y),T_{U}(T_{Y}(y),u))\) for some \(T_{Y}:Y\to Y\) and \(T_{U}:Y\times U\to U\). Triangular mappings are of interest as they allow us to obtain conditional couplings from joint couplings.

**Proposition 1** (Theorem 2.4(Baptista et al., 2020), Prop. 2.3(Hosseini et al., 2023)): _Suppose \(\eta,\nu\in\mathbb{P}(Y\times U)\) and \(T:Y\times U\to Y\times U\) is triangular. If \(T_{\#}\eta=\nu\), then \(T_{U}(T_{Y}(y),-)_{\#}\eta^{y}=\nu^{T_{Y}(y)}\) for \(\pi^{Y}_{\#}\eta\)-almost every \(y\)._

In many scenarios of practical interest, the source measure \(\eta\) and the target measure \(\nu\) have the same \(Y\)-marginals. We will henceforth make this assumption, and use \(\mu=\pi^{Y}_{\#}\eta=\pi^{Y}_{\#}\nu\) to represent this marginal. In this case, we may take \(T_{Y}\) to be the identity mapping, so that the conclusion of Proposition 1 simplifies to \(T_{U}(y,-)_{\#}\eta^{y}=\nu^{y}\) for \(\mu\)-almost every \(y\). We note that in situations where such an assumption does not hold, one may simply preprocess the source measure \(\eta\) via an invertible mapping \(T_{Y}\) satisfying \([T_{Y}]_{\#}[\pi^{Y}_{\#}\eta]=\pi^{Y}_{\#}\nu\)(Hosseini et al., 2023, Prop 3.2).

Given a source and target measures \(\eta,\nu\in\mathbb{P}^{\mu}(Y\times U)\) and a cost function \(c:(Y\times U)^{2}\to\mathbb{R}\), the _conditional Monge problem_ seeks to find a triangular mapping solving

\[\inf_{T}\left\{\int_{Y\times U}c(y,u,T(y,u))\,\mathrm{d}\eta(y,u)\mid T_{\#} \eta=\nu,T:(y,u)\mapsto(y,T_{U}(y,u))\right\}.\] (1)

The conditional Monge problem also admits a relaxation under which one only considers couplings whose \(Y\)-components are almost surely equal. To that end, for \(\eta,\nu\in\mathbb{P}^{\mu}_{p}(Y\times U)\) we define the set of _triangular couplings_\(\Pi_{Y}(\eta,\nu)\) to be the couplings of \(\eta\) and \(\nu\) that almost surely fix the \(Y\)-components,

\[\Pi_{Y}(\eta,\nu)=\left\{\gamma\in\mathbb{P}\left(\left(Y\times U\right)^{2} \right)\mid\pi^{1,2}_{\#}\gamma=\eta,\pi^{3,4}_{\#}\gamma=\nu,\pi^{1,3}_{\#}=( I,I)_{\#}\mu\right\}.\] (2)

In other words, a triangular coupling \(\gamma\in\Pi_{Y}(\eta,\nu)\) has samples \((y_{0},u_{0},y_{1},u_{1})\sim\gamma\) such that \(y_{0}=y_{1}\) almost surely. The _conditional Kantorovich problem_ seeks a triangular coupling solving

\[\inf_{\gamma}\left\{\int_{(Y\times U)^{2}}c(y_{0},u_{0},y_{1},u_{1})\,\mathrm{d} \gamma(y_{0},u_{0},y_{1},u_{1})\mid\gamma\in\Pi_{Y}(\eta,\nu)\right\}.\] (3)Hosseini et al. (2023) prove the existence of minimizers to the conditional Kantorovich and Monge problems under very general assumptions. Moreover, optimal couplings to the conditional Kantorovich problem induce optimal couplings for \(\mu\)-almost every conditional measure. We refer to Appendix B and Hosseini et al. (2023) for further details.

## 4 Conditional Wasserstein Space

Motivated by our discussion on triangular transport maps, we introduce the conditional Wasserstein spaces, consisting of joint measures with finite \(p\)th moments and having fixed \(Y\)-marginals \(\mu\). Interestingly, Gigli (2008, Chapter 4) studies the same space for the purposes of constructing geometric tangent spaces in the usual Wasserstein space.

**Definition 1** (Conditional Wasserstein Space):

_Suppose \(\mu\in\mathbb{P}(Y)\) is given and \(1\leq p<\infty\). The conditional \(p\)-Wasserstein space is_

\[\mathbb{P}_{p}^{\mu}(Y\times U)=\left\{\gamma\in\mathbb{P}_{p}(Y\times U)\mid \pi_{\#}^{Y}\gamma=\mu\right\}.\] (4)

We now equip \(\mathbb{P}_{p}^{\mu}(Y\times U)\) with a metric \(W_{p}^{\mu}\), the conditional Wasserstein distance. Intuitively, the conditional Wasserstein distance measures the usual Wasserstein distance between all of the conditional distributions in expectation under the fixed \(Y\)-marginal \(\mu\).

**Definition 2** (Conditional \(p\)-Wasserstein Distance):

_Suppose \(\eta,\nu\in\mathbb{P}_{p}^{\mu}(Y\times U)\) and \(1\leq p<\infty\). The function \(W_{p}^{\mu}:\mathbb{P}_{p}^{\mu}(Y\times U)\times\mathbb{P}_{p}^{\mu}(Y\times U )\rightarrow\mathbb{R}\),_

\[W_{p}^{\mu}(\eta,\nu)=\left(\mathbb{E}_{y\sim\mu}\left[W_{p}^{p}(\eta^{y},\nu ^{y})\right]\right)^{1/p}=\left(\int_{Y}W_{p}^{p}(\eta^{y},\nu^{y})\,\mathrm{d }\mu(y)\right)^{1/p}\] (5)

_is the conditional \(p\)-Wasserstein distance. \(W_{p}\) is the usual Wasserstein distance for measures on \(U\)._

By Jensen's inequality we have \(W_{p}^{\mu}(\eta,\nu)\geq\mathbb{E}_{y\sim\mu}\left[W_{p}(\eta^{y},\nu^{y})\right]\). In the following, we show that the conditional Wasserstein distance is a well-defined metric as well as a few other metric properties.

**Proposition 2** (Some Properties of \(W_{p}^{\mu}\)):

_Let \(1\leq p<\infty\)._

* \(W_{p}^{\mu}\) _is well-defined, finite, and equals the minimal conditional Kantorovich cost._
* \(W_{p}^{\mu}\) _is a metric on the space_ \(\mathbb{P}_{p}^{\mu}(Y\times U)\)_._
* _There does not exist_ \(C>0\) _such that_ \(W_{p}^{\mu}(\eta,\nu)\leq CW_{p}(\eta,\nu)\) _for all_ \(\eta,\nu\in\mathbb{P}_{p}^{\mu}(Y\times U)\)_._
* _For all_ \(\eta,\nu\in\mathbb{P}_{p}^{\mu}(Y\times U)\)_,_ \(W_{p}\left(\pi_{\#}^{U}\eta,\pi_{\#}^{U}\nu\right)\leq W_{p}^{\mu}(\eta,\nu)\) _and_ \(W_{p}(\eta,\nu)\leq W_{p}^{\mu}(\eta,\nu)\)_._

Proposition 2(c, d) together shows that one should expect the topology generated by \(W_{p}^{\mu}\) to be stronger than the unconditional distance \(W_{p}\). Here, we note that Gigli (2008) and Chemseddine et al. (2023) previously showed that \(W_{p}^{\mu}\) is a metric through an equivalence with restricted couplings. Our approach builds on the results of Hosseini et al. (2023) and is somewhat more direct, and hence our proofs may be of independent interest.

For the sake of concreteness, we include an example where the conditional \(2\)-Wasserstein distance may be explicitly computed. Here, the necessary calculations follows from the fact that the conditional distributions of a multivariate are again Gaussian, and Gaussian distributions admit a closed-form expression for the usual unconditional \(2\)-Wasserstein distance.

Example: Gaussian Measures.Suppose \(Y=U=\mathbb{R}\) and that \(\eta,\nu\in\mathbb{P}_{p}^{\mu}(Y\times U)\) are Gaussians

\[\eta=\mathcal{N}(0,I)\qquad\nu=\mathcal{N}\left(0,\begin{bmatrix}1&\rho\\ \rho&1\end{bmatrix}\right)\qquad|\rho|<1.\] (6)

It follows that \(\mu=\pi_{\#}^{Y}\eta=\pi_{\#}^{Y}\nu=\mathcal{N}(0,1)\). As \(\eta^{y},\nu^{y}\) are Gaussians, their \(W_{2}\) distance admits a closed form and we can directly compute the expectation in Equation (5) to obtain \(W_{2}^{\mu,2}(\eta,\nu)=2(1-\sqrt{1-\rho^{2}})\). This is zero if and only if \(\rho=0\), i.e. \(\eta=\nu\). However, \(\pi_{\#}^{U}\eta=\pi_{\#}^{U}\nu=\mathcal{N}(0,1)\) and \(W_{2}(\pi_{\#}^{U}\eta,\pi_{\#}^{U}\nu)=0\) regardless of \(\rho\). Moreover, the unconditional distance is \(W_{2}^{2}(\eta,\nu)=2\left(2-\sqrt{1-\rho}-\sqrt{1+\rho}\right)\), from which it is easy to verify that \(W_{2}(\eta,\nu)\leq W_{2}^{\mu}(\eta,\nu)\). See Appendix C for a similar derivation which applies to arbitrary Gaussians.

Conditional Wasserstein Space as a Geodesic Space.We now turn our attention to the geodesics in \(\mathbb{P}_{p}^{\mu}(Y\times U)\). In particular, we show that there exists a constant speed geodesic between any two measures in \(\mathbb{P}_{p}^{\mu}(Y\times U)\), generalizing a similar result in the unconditional setting (Santambrogio, 2015, Theorem 5.27). Moreover, we show that under suitable regularity assumptions, solutions to the conditional Monge problem (1) induce constant speed geodesics. Our motivation for studying geodesics in \(\mathbb{P}_{p}^{\mu}(Y\times U)\) is practical - in Section 6, we show how one can model geodesics in \(\mathbb{P}_{p}^{\mu}(Y\times U)\) in order to obtain a conditional flow-based model whose paths are easy to integrate.

A _curve_ is a continuous function \(\gamma_{\star}:I\to\mathbb{P}_{p}^{\mu}(Y\times U)\) where \(I=(a,b)\subseteq\mathbb{R}\) is any open interval of finite length. A curve is _absolutely continuous_ if there exists \(m\in L^{1}((a,b))\) such that

\[W_{p}^{\mu}(\gamma_{s},\gamma_{t})\leq\int_{s}^{t}m(\tau)\,\mathrm{d}\tau \qquad\forall a<s\leq t<b.\] (7)

If \((\gamma_{t})\) is an absolutely continuous curve, then its metric derivative

\[|\gamma^{\prime}|(t)=\lim_{s\to t}\frac{W_{p}^{\mu}(\gamma_{s},\gamma_{t})}{| s-t|}\] (8)

exists for almost every \(t\in(a,b)\), and, moreover, we almost surely have \(|\gamma^{\prime}|(t)\leq m(t)\) pointwise for any \(m\) satisfying Equation (7) (Ambrosio et al., 2005, Theorem 1.1.2). A curve \((\gamma_{t})\) is called a _constant speed geodesic_ if for all \(a<s\leq t<b\), we have \(W_{p}^{\mu}(\gamma_{s},\gamma_{t})=|t-s|W_{p}^{\mu}(\gamma_{a},\gamma_{b})\). It is straightforward to show that every constant speed geodesic is absolutely continuous.

**Theorem 1** (\(\mathbb{P}_{p}^{\mu}(Y\times U)\) is a Geodesic Space)

_For any \(\eta,\nu\in\mathbb{P}_{p}^{\mu}(Y\times U)\), there exists a constant speed geodesic between \(\eta\) and \(\nu\)._

When an optimal triangular coupling \(\gamma^{*}\in\Pi_{Y}(\eta,\nu)\) is induced by an injective triangular map \(T^{\star}\), we may recover a constant speed geodesic in \(\mathbb{P}_{p}^{\mu}(Y\times U)\), generalizing the McCann interpolant (McCann, 1997) to the conditional setting. We refer to Proposition 4 for sufficient conditions on \(\eta,\nu\) under which such a \(T^{\star}\) exists. Informally, samples from \((y_{0},u_{0})\sim\eta\) flow in a straight path at a constant speed to their destination \(T^{\star}(y_{0},u_{0})\).

**Theorem 2** (Conditional McCann Interpolants)

_Fix \(\eta,\nu\in\mathbb{P}_{p}^{\mu}(Y\times U)\). Suppose \(T^{\star}(y,u)=(y,T^{\star}_{\mathcal{U}}(y,u))\) is an injective triangular map solving the conditional Monge problem (1). Define the maps \(T_{t}:Y\times U\to Y\times U\) for \(0\leq t\leq 1\) via \(T_{t}=(1-t)I+tT^{\star}\), and define the curve of measures \(\gamma_{t}=[T_{t}]_{\#}\eta\in\mathbb{P}_{p}^{\gamma}(Y\times U)\). Then,_

* \((\gamma_{t})\) _is absolutely continuous and a constant speed geodesic between_ \(\eta,\nu\)__
* _The vector field_ \(v_{t}(T^{\star}_{t}(y,u))=(0,T^{\star}_{\mathcal{U}}(y,u)-u)\) _generates the path_ \(\gamma_{t}\)_, in the sense that_ \((\gamma_{t},v_{t})\) _solve the continuity equation (_9_)._

## 5 Conditional Benamou-Brenier Theorem

In this section, we prove a characterization of the absolutely continuous curves in \(\mathbb{P}_{p}^{\mu}(Y\times U)\). As a corollary, we obtain a conditional generalization of the Benamou-Brenier Theorem (Benamou and Brenier, 2000), giving us a dynamic characterization of the conditional Wasserstein distance. Roughly speaking, all such curves are generated by a vector field on \(Y\times U\) which has zero velocity in the \(Y\) component. This is natural, as all measures in \(\mathbb{P}_{p}^{\mu}(Y\times U)\) have a fixed \(Y\)-marginal \(\mu\). Such a vector field can be informally seen as tangent to a curve of measures, and is the dynamic analogue of the triangular maps discussed in Section 3. More formally, given an open interval \(I\subseteq\mathbb{R}\), a time-dependent Borel vector field \(v:I\times Y\times U\to Y\times U\) is said to be _triangular_ if there exists a Borel vector field \(v^{U}:I\times Y\times U\to U\) such that \(v_{t}(y,u)=\big{(}0,v^{U}_{t}(y,u)\big{)}\).

Continuity Equation.We introduce some necessary background which allows us to link vector fields to curves of measures. The _continuity equation_\(\partial_{t}\gamma_{t}+\text{div}(v_{t}\gamma_{t})=0\) describes the evolution of a measure \(\gamma_{t}\) which flows along a given vector field \(v_{t}\)(Ambrosio et al., 2005, Chapter 8). This equation must be understood in the sense of distributions, i.e. for every \(\varphi\) in a space of test functions,

\[\int_{I}\int_{Y\times U}\left(\partial_{t}\varphi(y,u,t)+\langle v_{t}(y,u), \nabla_{y,u}\varphi(y,u,t)\rangle\right)\,\mathrm{d}\gamma_{t}(y,u)\,\mathrm{d }t=0.\] (9)We consider cylindrical test functions \(\varphi\in\text{Cyl}(Y\times U\times I)\), i.e. of the form \(\varphi(y,u,t)=\psi(\pi^{d}(y,u),t)\) where \(\pi^{d}:Y\times U\to\mathbb{R}^{d}\) maps \((y,u)\mapsto(\langle(y,u),e_{1}\rangle,\ldots,\langle(y,u),e_{d}\rangle)\) and \(\{e_{1},e_{2},\ldots,e_{d}\}\) is any orthonormal family in \(Y\times U\). In the finite dimensional setting, one may take \(\varphi\in C_{c}^{\infty}(Y\times U)\) to be smooth and compactly supported (Ambrosio et al., 2005, Remark 8.1.1).

In Appendix E, we prove Lemma 1, which is key in proving Theorem 4 below. Informally, Lemma 1 states that if the weak continuity equation (9) is satisfied for a joint distribution and triangular vector field, then the continuity equation is also satisfied for the corresponding conditional distributions and \(U\) components of the vector field.

**Lemma 1** (Triangular Vector Fields Preserve Conditionals): _Suppose \(v_{t}(y,u)=(0,v_{t}^{U}(y,u))\) is triangular and that \((\gamma_{t})\subset\mathbb{P}_{p}^{\mu}(Y\times U)\) is a path of measures such that \((v_{t},\gamma_{t})\) satisfy the continuity equation in the sense of distributions. Then, it follows that for \(\mu\)-almost every \(y\in Y\), we have \(\partial_{t}\gamma_{t}^{y}+\nabla\cdot(v_{t}^{U}(y,-)\gamma_{t}^{y})=0\)._

We note that having \(v_{t}\) be triangular is sufficient, but certainly not necessary, for the conditional continuity equation to almost surely hold. For instance, the vector field in \(\mathbb{R}^{d}\) that rotates \(\mathcal{N}(0,I)\) about the origin is not triangular yet preserves all conditional distributions.

Absolutely Continuous Curves.In this section, we state our characterization of absolutely continuous curves in \(\mathbb{P}_{p}^{\mu}(Y\times U)\). Informally, given such a curve, Theorem 3 provides us with a triangular vector field which generates the curve, in the sense that the pair solve the continuity equation.

**Theorem 3** (Absolutely Continuous Curves in \(\mathbb{P}_{p}^{\mu}(Y\times U)\)): _Let \(I\subset\mathbb{R}\) be an open interval, and suppose \(\gamma_{t}:I\to\mathbb{P}_{p}^{\mu}(Y\times U)\) is an absolutely continuous in the \(W_{p}^{\mu}\) metric with \(|\gamma^{\prime}|(t)\in L^{1}(I)\). Then, there exists a Borel vector field \(v_{t}(y,u)\) such that_

* \(v_{t}\) _is triangular_
* \(v_{t}\in L^{p}(\gamma_{t},Y\times U)\) _and_ \(\|v_{t}\|_{L^{p}(\gamma_{t},Y\times U)}\leq|\gamma^{\prime}|(t)\) _for a.e._ \(t\)__
* \((v_{t},\gamma_{t})\) _solve the continuity equation in the sense of distributions._

Conversely, we show in Theorem 4 that if the pair \((\gamma_{t},v_{t})\) solve the continuity equation and \(v_{t}\) is triangular, then the curve \((\gamma_{t})\) is absolutely continuous and \(|\gamma^{\prime}|(t)\leq\|v_{t}\|_{L^{p}(\gamma_{t},Y\times U)}\). The main technique of this result is to study the collection of _conditional_ continuity equations (which is feasible by Lemma 1) and to apply the converse of Ambrosio et al. (2005, Theorem 8.3.1). In this setting, the infinite-dimensional result is obtained via a finite-dimensional approximation argument.

**Theorem 4** (Continuous Curves Generated by Triangular Vector Fields): _Suppose that \(\gamma_{t}:I\to\mathbb{P}_{p}^{\mu}(Y\times U)\) is narrowly continuous and \((v_{t})\) is a triangular vector field such that \((\gamma_{t},v_{t})\) solve the continuity equation with \(\|v_{t}\|_{L^{p}(\gamma_{t},Y\times U)}\in L^{1}(I)\). Then, \(\gamma_{t}:I\to\mathbb{P}_{p}^{\mu}(Y\times U)\) is absolutely continuous in the \(W_{p}^{\mu}\) metric and \(|\gamma^{\prime}|(t)\leq\|v_{t}\|_{L^{p}(\mu,Y\times U)}\) for almost every \(t\)._

As a corollary of Theorem 3 and Theorem 4, we obtain a conditional version of the Benamou-Brenier theorem (Benamou and Brenier, 2000). Once we have our characterization of absolutely continuous curves provided by these theorems, the proof of Theorem 5 largely follows the unconditional case (see e.g. Ambrosio et al. (2005, Chapter 8)), but we include it for the sake of completeness.

**Theorem 5** (Conditional Benamou-Brenier): _Let \(1<p<\infty\). For any \(\eta,\nu\in\mathbb{P}_{p}^{\mu}(Y\times U)\), we have_

\[W_{p}^{p,\mu}(\eta,\nu)=\min_{(\gamma_{t},v_{t})}\left\{\int_{0}^{1}\|v_{t}\| _{L^{p}(\mu_{t})}^{p}\,\,\mathrm{d}t\,|\,(v_{t},\gamma_{t})\text{ solve (\ref{eq:brenier}), $\gamma_{0}=\eta,\gamma_{1}=\nu$, and $v_{t}$ is triangular}\right\}.\]

## 6 COT Flow Matching

We have thus far seen that the COT problem (3) admits a dynamic formulation by Theorem 5, where one may take the underlying vector fields to be triangular. We use these results to design a principled model for conditional generation based on flow matching (Lipman et al., 2022, Albergo et al., 2023, Liu et al., 2022, Tong et al., 2023, Pooladian et al., 2023). We hereafter use the squared-distance cost (i.e. \(p=2\)).

Flow Matching.We assume that we have access to samples \(z_{0}=(y_{0},u_{0})\sim\eta(y_{0},u_{0})\in\mathbb{P}_{p}^{\mu}(Y\times U)\) from a source measure, and samples \(z_{1}=(y_{1},u_{1})\sim\nu(y_{1},u_{1})\in\mathbb{P}_{p}^{\mu}(Y\times U)\) from a target measure. Let \(z=(z_{0},z_{1})\sim\rho(z_{0},z_{1})\in\Pi(\eta,\nu)\) be any coupling of the source and target measure. We specify a collection of measures and vector fields on \(Y\times U\) via

\[\gamma_{t}(y,u\mid z)=\mathcal{N}\left(y,u\mid tz_{1}+(1-t)\,z_{0},C\right) \qquad v_{t}(y,u\mid z)=z_{1}-z_{0}\] (10)

where \(C\) is any trace-class covariance operator (Da Prato and Zabczyk, 2014). As is standard in flow matching (Lipman et al., 2022; Kerrigan et al., 2024), we obtain from Equations (10) a marginal measure \(\gamma_{t}(y,u)\) and vector field \(v_{t}(y,u)\) satisfying the continuity equation via

\[\gamma_{t}(y,u)=\int_{(Y\times U)^{2}}\gamma_{t}(y,u\mid z)\,\mathrm{d}\rho( z)\qquad v_{t}(y,u)=\int_{(Y\times U)^{2}}v_{t}(y,u\mid z)\frac{\mathrm{d} \gamma_{t}(y,u\mid z)}{\mathrm{d}\gamma_{t}(y,u)}\,\mathrm{d}\rho(z).\] (11)

This marginal path \((\gamma_{t})_{t=0}^{1}\) interpolates between the source measure (\(t=0\)) and a smoothed version of the target measure (\(t=1\)). To transform source samples from \(\eta\) into target samples from \(\nu\), we seek to learn the intractable vector field \(v_{t}(y,u)\) with a model \(v^{\theta}(t,y,u)\) by minimizing the loss1

Footnote 1: Previous work has referred to this as the _conditional flow matching loss_(Tong et al., 2023), which is not to be confused with the notion of conditioning that we focus on in this work.

\[\mathcal{L}(\theta)=\mathbb{E}_{t,\rho(z),\gamma_{t}(y,u|z)}\left\|v^{\theta}(t,y,u)-v_{t}(y,u\mid z)\right\|^{2}\] (12)

which has the same \(\theta\)-gradient as the MSE loss to the true vector field \(u_{t}(y,u)\)(Tong et al., 2023).

COT Flow Matching.In the preceding section, \(\rho(z)\) may be an arbitrary coupling between \(\eta\) and \(\nu\). Motivated by Proposition 1, we will choose \(\rho\) to be a COT coupling. Under sufficient regularity conditions (see Appendix B), this COT plan will be induced by a triangular map. In turn, Theorem 2 gives us that this triangular map is generated by a triangular vector field of the form (10). Thus, we parametrize our model \(u^{\theta}\) to also be triangular. Moreover, we recover the optimal dynamic transport given in Theorem 5 as \(\text{Tr}(C)\to 0\) by a pointwise application of (Tong et al., 2023, Proposition 3.4).

Given a collection of samples \(\{z_{0}^{i},z_{1}^{i}\}_{i=1}^{n}\) drawn from \(\eta\) and \(\nu\), we approximate a conditional optimal coupling \(\rho\) using standard numerical techniques with the cost function \(c_{e}(y_{0},u_{0},y_{1},u_{1})=|y_{1}-y_{0}|^{2}+\epsilon|u_{1}-u_{0}|^{2}\) for some \(0<\epsilon\ll 1\). Intuitively, such a cost penalizes mass transfer along the \(Y\) dimension, which is precisely the constraint sought in the COT problem (3). As \(\epsilon\downarrow 0\), we recover the true optimal triangular map (Carlier et al., 2010; Hosseini et al., 2023). The COT coupling

Figure 1: Samples from the ground-truth joint target distribution and the various models. Samples from COT-FM more closely match the ground-truth distribution than the baselines. In the final column, we plot conditional KDEs for samples drawn conditioned on the \(y\) value indicated by the dashed horizontal line. See Appendix F for a larger figure and additional results.

can either be precomputed for small datasets or computed on each minibatch drawn during training. While the use of minibatches is a computational necessity, we find that surprisingly small batch sizes still yields accurate approximations of the true COT mapping using our COT-FM method. See Appendix G.

After training, we obtain a learned triangular vector field \(v^{\theta}(t,y,u)\). Given an arbitrary fixed \(y\in Y\), we may approximately sample from the target \(\nu(u\mid y)\) by sampling \(u_{0}\sim\eta(u_{0}\mid y)\) and numerically solving the corresponding flow equation \(\partial_{t}(y,u_{t})=v^{\theta}(t,y,u_{t})\) with initial condition \((y,u_{0})\).

Source Measure.Our framework is agnostic to the choice of source measure \(\eta\), allowing for flexibility in the modeling process. The main requirement is that the \(Y\)-marginals of the source \(\eta\) and target \(\eta\) must match. In some scenarios, this is trivially satisfied. If one is interested in using a source distribution which is simply random noise, one may take \(\eta(y_{0},u_{0})=\pi_{\#}^{Y}\nu(y_{0})\otimes\eta_{U}(u_{0})\) to be the product of two independent distributions where \(\eta_{U}\) is arbitrary, e.g. Gaussian noise.

## 7 Experiments

We now illustrate our methodology (COT-FM) on a variety of conditional simulation tasks. We compare our method against several competitive baselines, namely PCP-Map (Wang et al., 2023), COT-Flow (Wang et al., 2023), and WaMGAN (Hosseini et al., 2023). These baselines are chosen as they reflect current state-of-the-art approaches to learning COT maps. We additionally compare against flow matching (Lipman et al., 2022; Wildberger et al., 2024) without COT, i.e. where the coupling between the source and target measures is the independent coupling \(\rho(z_{0},z_{1})=\eta\otimes\nu\). This baseline serves as an ablation for the COT component of our model.

Overall, our method (COT-FM) typically outperforms these baselines across the diverse and challenging set of tasks we consider. We find that PCP-Map (Wang et al., 2023) is a strong baseline, but we emphasize that this model relies on the use of an input-convex neural network (Amos et al., 2017) and it is hence unclear how to adapt this method to e.g. images. Appendix F contains further details and results for all of our experiments.2

Footnote 2: Code for all of our experiments is available at https://github.com/GavinKerrigan/cot_fm

2D Synthetic Data.We first consider synthetic distributions where \(Y=U=\mathbb{R}\). Our source measure is taken to be the independent product \(\eta(y,u)=\pi_{\#}^{Y}\nu\otimes\mathcal{N}(0,1)\). We plot ground-truth joint distributions and samples for two datasets in Figure 1. See Appendix F for additional results. Samples from our method (COT-FM) closely match those from the ground-truth distribution, whereas samples from PCP-Map and COT-Flow (Wang et al., 2023) can produce samples in regions of zero support under the ground-truth distribution. In Table 1, we provide a quantitative analysis, where we measure the \(W_{2}\) and MMD distances between the generated and ground-truth joint distributions. This is motivated by Proposition 1, as triangular maps which couple the joint distributions necessarily couple the conditional distributions. Our method outperforms the baselines across all metrics.

\begin{table}
\begin{tabular}{l|c c c c c c c} \hline \hline  & \multicolumn{2}{c}{Checkerboard} & \multicolumn{2}{c}{Moons} & \multicolumn{2}{c}{Circles} & \multicolumn{2}{c}{Swissroll} \\  & \(W_{2}\) (1e-2) & MMD (1e-3) & \(W_{2}\) (1e-2) & MMD (1e-3) & \(W_{2}\) (1e-2) & MMD (1e-3) & \(W_{2}\) (1e-2) & MMD (1e-3) \\ \hline PCP-Map & \(6.27\pm 0.81\) & \(0.21\pm 0.13\) & \(8.44\pm 1.09\) & \(0.22\pm 0.10\) & \(6.19\pm 0.43\) & \(\mathbf{0.20}\pm 0.17\) & \(5.35\pm 0.93\) & \(0.16\pm 0.13\) \\ COT-Flow & \(8.20\pm 0.49\) & \(0.26\pm 0.16\) & \(18.49\pm 2.22\) & \(1.32\pm 0.79\) & \(10.04\pm 1.69\) & \(0.24\pm 0.22\) & \(6.47\pm 0.69\) & \(0.19\pm 0.19\) \\ FM & \(8.81\pm 0.58\) & \(0.24\pm 0.20\) & \(15.55\pm 0.77\) & \(1.85\pm 0.22\) & \(7.03\pm 0.17\) & \(0.45\pm 0.11\) & \(8.18\pm 0.34\) & \(0.58\pm 0.09\) \\ COT-FM (Ours) & \(\mathbf{4.69}\pm 1.00\) & \(\mathbf{0.17}\pm 0.13\) & \(\mathbf{6.50}\pm 1.41\) & \(\mathbf{0.13}\pm 0.10\) & \(\mathbf{5.56}\pm 0.43\) & \(\mathbf{0.20}\pm 0.04\) & \(\mathbf{4.64}\pm 1.26\) & \(\mathbf{0.15}\pm 0.19\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Distances between the ground-truth and generated joint distributions for the 2D datasets. Our method (COT-FM) obtains lower distances than the considered baselines. Average results \(\pm\) one standard deviation are reported across five test sets, with the lowest average distance in bold.

\begin{table}
\begin{tabular}{l|c c} \hline \hline  & \(W_{2}\) (1e-2) & MMD (1e-3) \\ \hline PCP-Map & \(5.04\pm 0.05\) & \(2.67\pm 2.1\) \\ COT-Flow & \(4.86\pm 1.1\) & \(\mathbf{0.83}\pm 0.50\) \\ FM & \(11.41\pm 0.26\) & \(2.65\pm 0.14\) \\ COT-FM (Ours) & \(\mathbf{4.02}\pm 0.06\) & \(0.95\pm 0.03\) \\ \hline \hline \end{tabular}
\end{table}
Table 2: Statistical distances between MCMC and posterior samples \(u\sim\nu(u\mid y)\) for each method on the LV dataset. Average results \(\pm\) one standard deviation reported across five test sets.

Lotka-Volterra (LV) Dynamical System.Here we estimate parameters of the Lotka-Volterra (LV) model given only noisy observations of its solution. The LV model has parameters \(u=(\alpha,\beta,\gamma,\delta)\in\mathbb{R}^{4}_{\geq 0}\) and a pair of coupled nonlinear ODEs of the form

\[\frac{\mathrm{d}p_{1}(t)}{\mathrm{d}t}=\alpha p_{1}-\beta p_{1}p_{2}\quad\frac{ \mathrm{d}p_{2}(t)}{\mathrm{d}t}=-\gamma p_{2}+\delta p_{1}p_{2}\] (13)

whose solution \(p(t)=(p_{1}(t),p_{2}(t))\in\mathbb{R}^{2}_{\geq 0}\) represents the number of prey and predator species at time \(t\in[0,T]\). Following Alfonso et al. (2023), we assume \(p(0)=(30,1)\) and that \(\log(u)\sim\mathcal{N}(m,0.5I)\) with \(m=(-0.125,-3,-0.125,-3)\). Given parameters \(u\in\mathbb{R}^{4}_{\geq 0}\), we simulate Equation (13) for \(t\in\{0,2,\ldots,20\}\) to obtain a solution \(z(u)\in\mathbb{R}^{22}_{\geq 0}\). An observation \(y\in\mathbb{R}^{22}_{\geq 0}\) is obtained by the addition of log-normal noise, i.e. \(\log(y)\sim\mathcal{N}(\log(z(u),0.1I)\). We thus may simulate many \((y,u)\) pairs from the target measure for training.

As a benchmark, we follow the settings of Alfonso et al. (2023) and choose parameters \(u=(0.83,0.041,1.08,0.04)\) to generate a single observation \(y\) as described above. In Figure 2, we plot a histogram of \(10,000\) samples from the posterior \(\nu(u\mid y)\) of COT-FM.

Since the ground-truth posterior is intractable, we compare against differential evolution Metropolis MCMC (Braak, 2006). Samples from our method qualitatively resemble those from MCMC, and the posterior mode is typically close to the true unknown \(u\) (shown in red). Our method is quantitatively closest to the MCMC samples in the \(W_{2}\) metric, and competitive in the MMD metric (Table 2).

Darcy Flow Inverse Problem.Here we consider an infinite-dimensional Bayesian inverse problem from the 2D Darcy flow PDE. The setting is adapted from Hosseini et al. (2023). We opt to compare against WaMGAN (Hosseini et al., 2023), as this is currently the only other extant amortized function-space COT method, and FFM (Kerrigan et al., 2023) as a function-space flow matching ablation.

The Darcy flow PDE is an elliptic equation on a smooth domain \(\Omega\subseteq\mathbb{R}^{d}\) which relates a permeability field \(\exp(u)\), a pressure field \(\rho\), and a source term \(f\) via \(-\text{div}\exp(u)\nabla\rho=f\) on \(\Omega\) subject to \(\rho=0\) on \(\partial\Omega\). Our goal is to recover the permeability \(u\) from noisy measurements \(y\) of the pressure \(\rho\). Both the unknown \(u\) and observations \(y\) are functions and thus infinite-dimensional. To define our target measure, we specify a prior \(\nu(u)=\mathcal{N}(0,C)\) with a Matern kernel \(C\) of lengthscale \(\ell=1/2\) and \(\nu=3/2\). Given \(u\sim\eta(u)\), the Darcy flow PDE is solved numerically \(\partial\Omega\). Our goal is to recover the permeability \(u\) from noisy measurements \(y\) of the pressure \(\rho\). Both the unknown \(u\) and observations \(y\) are functions and thus infinite-dimensional. To define our target measure, we specify a prior \(\nu(u)=\mathcal{N}(0,C)\) with a Matern kernel \(C\) of lengthscale \(\ell=1/2\) and \(\nu=3/2\). Given \(u\sim\eta(u)\), the Darcy flow PDE is solved numerically (Alnaes et al., 2015) to obtain a solution \(\rho(u)\) observed at some finite but arbitrary number of points \(\{x_{1},\ldots,x_{n}\}\subset\mathbb{R}^{2}\). An observation \(y(u)\) is obtained by adding Gaussian noise to each observation, i.e. \(y(u)\sim\mathcal{N}(\rho(u),\sigma^{2}I)\) where \(\sigma=2.5\times 10^{-2}\). We implement all models via a Fourier Neural Operator (Li et al., 2020), allowing us to work with arbitrary discretizations, as required by the functional nature of this problem.

We provide an illustration in Figure 3. As the true posterior is intractable, we compare against preconditioned Crank-Nicolson (pCN) (Cotter et al., 2013), a function-space MCMC method. In Figure 3, we plot the mean posteriors obtained from the various methods. Qualitatively, both COT-FM and FFM are good approximations to pCN, while WaMGAN has visual artifacts. However, the MSE

\begin{table}
\begin{tabular}{l|l l} \hline \hline  & MSE (1e-2) & CRPS (1e-2) \\ \hline WaMGAN & \(6.55\pm 0.07\) & \(18.75\pm 0.10\) \\ FFM & \(7.30\pm 0.07\) & \(\mathbf{15.47}\pm 0.06\) \\ COT-FFM (Ours) & \(\mathbf{5.40}\pm 0.08\) & \(15.56\pm 0.08\) \\ \hline \hline \end{tabular}
\end{table}
Table 3: Predictive performance of the generated samples on the Darcy flow inverse problem. Average result \(\pm\) one standard deviation obtained on 5 test sets of 5,000 samples each.

Figure 2: Sample KDEs on the Lotka-Volterra inverse problem. The red lines denote the true parameter values.

between our method and the pCN mean is lower than that of FFM. Table 3 provides a quantitative comparison between the methods on a test set of 5,000 samples, where we measure MSE and CRPS (Hersbach, 2000). We compare the ensemble mean of 10 samples against the true \(u\) value as running pCN for each observation is prohibitively expensive. COT-FM outperforms FFM and WaMGAN in terms of MSE and is on-par with FFM in terms of CRPS. See Appendix F for further details.

## 8 Conclusion

We analyze conditional optimal transport from a geometric and dynamical point of view. Our analysis culminates in the characterization of absolutely continuous curves of measures in a conditional Wasserstein space, resulting in a conditional analog of the Benamou-Brenier Theorem.

We use these result to build on the framework of triangular transport and flow matching to develop simulation-free methods for conditional generative models. Our methods are applicable across a wide class of problems, and we demonstrate our methodology on several challenging inverse problems.

Limitations and Broader Impacts.A limitation COT-FM is that computing the full COT plan can be expensive for large datasets, necessitating the use of minibatch approximations potentially resulting in sub-optimal plans. While this approximation does not limit the practical applicability of our method, an interesting challenge is to characterize the precise relationship between this minibatch approximation and the full COT plan. Moreover, computing the COT plan incurs a small additional computational cost compared to standard flow matching. As with all generative models, a potential negative impact is the potential for disinformation through generated samples being purported as real.

## Acknowledgments and Disclosure of Funding

This research was supported by the Hasso Plattner Institute (HPI) Research Center in Machine Learning and Data Science at the University of California, Irvine, by the National Science Foundation under award 1900644, and by the National Institutes of Health under award R01-LM013344.

Figure 3: Darcy flow illustration. Several true permeability fields \(u\) are shown, as well as the pressure field \(\rho\) and its observed, noisy version \(y\). We compare an ensemble average of posterior samples from the various methods against MCMC (pCN) (Cotter et al., 2013). COT-FM achieves the lowest MSE to pCN. We note here that WaMGAN has clear visual artifacts despite achieving reasonable MSE and CRPS scores.

## References

* Abril-Pla et al. [2023] Oriol Abril-Pla, Virgile Andreani, Colin Carroll, Larry Dong, Christopher J Fonnesbeck, Maxim Kochurov, Ravin Kumar, Junpeng Lao, Christian C Luhmann, Osvaldo A Martin, et al. PyMC: A modern, and comprehensive probabilistic programming framework in python. _PeerJ Computer Science_, 9:e1516, 2023.
* Adler and Oktem [2018] Jonas Adler and Ozan Oktem. Deep Bayesian inversion. _arXiv preprint arXiv:1811.05910_, 2018.
* Albergo et al. [2023a] Michael S Albergo, Nicholas M Boffi, and Eric Vanden-Eijnden. Stochastic interpolants: A unifying framework for flows and diffusions. _arXiv preprint arXiv:2303.08797_, 2023a.
* Albergo et al. [2023b] Michael S Albergo, Mark Goldstein, Nicholas M Boffi, Rajesh Ranganath, and Eric Vanden-Eijnden. Stochastic interpolants with data-dependent couplings. _arXiv preprint arXiv:2310.03725_, 2023b.
* Alfonso et al. [2023] Jason Alfonso, Ricardo Baptista, Anupam Bhakta, Noam Gal, Alfin Hou, Isa Lyubimova, Daniel Pocklington, Josef Sajonz, Giulio Trigila, and Ryan Tsai. A generative flow for conditional sampling via optimal transport. _arXiv preprint arXiv:2307.04102_, 2023.
* Alnaes et al. [2015] Martin Alnaes, Jan Blechta, Johan Hake, August Johansson, Benjamin Kehlet, Anders Logg, Chris Richardson, Johannes Ring, Marie E Rognes, and Garth N Wells. The FEniCS project version 1.5. _Archive of numerical software_, 3(100), 2015.
* Ambrosio et al. [2005] Luigi Ambrosio, Nicola Gigli, and Giuseppe Savare. _Gradient Flows: In Metric Spaces and in the Space of Probability Measures_. Springer Science & Business Media, 2005.
* Ambrosio et al. [2013] Luigi Ambrosio, Alberto Bressan, Dirk Helbing, Axel Klar, Enrique Zuazua, Luigi Ambrosio, and Nicola Gigli. A user's guide to optimal transport. _Modelling and Optimisation of Flows on Networks: Cetraro, Italy 2009, Editors: Benedetto Piccoli, Michel Rascle_, pages 1-155, 2013.
* Amos et al. [2017] Brandon Amos, Lei Xu, and J Zico Kolter. Input convex neural networks. In _International Conference on Machine Learning_, pages 146-155. PMLR, 2017.
* Amos et al. [2023] Brandon Amos et al. Tutorial on amortized optimization. _Foundations and Trends in Machine Learning_, 16(5):592-732, 2023.
* Arora et al. [2018] Sanjeev Arora, Andrej Risteski, and Yi Zhang. Do GANs learn the distribution? Some theory and empirics. In _International Conference on Learning Representations_, 2018.
* Baptista et al. [2020] Ricardo Baptista, Bamdad Hosseini, Nikola B Kovachki, and Youssef Marzouk. Conditional sampling with monotone GANs: from generative models to likelihood-free inference. _arXiv preprint arXiv:2006.06755_, 2020.
* Baptista et al. [2023] Ricardo Baptista, Youssef Marzouk, and Olivier Zahm. On the representation and learning of monotone triangular transport maps. _Foundations of Computational Mathematics_, pages 1-46, 2023.
* Barboni et al. [2024] Raphael Barboni, Gabriel Peyre, and Francois-Xavier Vialard. Understanding the training of infinitely deep and wide ResNets with conditional optimal transport. _arXiv preprint arXiv:2403.12887_, 2024.
* Beaumont [2010] Mark A Beaumont. Approximate Bayesian computation in evolution and ecology. _Annual review of ecology, evolution, and systematics_, 41:379-406, 2010.
* Benamou and Brenier [2000] Jean-David Benamou and Yann Brenier. A computational fluid mechanics solution to the Monge-Kantorovich mass transfer problem. _Numerische Mathematik_, 84(3):375-393, 2000.
* Blei et al. [2017] David M Blei, Alp Kucukelbir, and Jon D McAuliffe. Variational inference: A review for statisticians. _Journal of the American statistical Association_, 112(518):859-877, 2017.
* Bogachev and Ruas [2007] Vladimir Igorevich Bogachev and Maria Aparecida Soares Ruas. _Measure Theory_, volume 2. Springer, 2007.
* Braak [2006] Cajo JF Ter Braak. A Markov chain Monte Carlo version of the genetic algorithm differential evolution: easy bayesian computing for real parameter spaces. _Statistics and Computing_, 16:239-249, 2006.
* Bunne et al. [2022] Charlotte Bunne, Andreas Krause, and Marco Cuturi. Supervised training of conditional monge maps. _Advances in Neural Information Processing Systems_, 35:6859-6872, 2022.
* Carlier et al. [2010] Guillaume Carlier, Alfred Galichon, and Filippo Santambrogio. From knothe's transport to Brenier's map and a continuation method for optimal transport. _SIAM Journal on Mathematical Analysis_, 41(6):2554-2576, 2010.
* 1192, 2016. doi: 10.1214/15-AOS1401. URL https://doi.org/10.1214/15-AOS1401.
Jannis Chemseddine, Paul Hagemann, and Christian Wald. Y-Diagonal couplings: Approximating posteriors with conditional Wasserstein distances. _arXiv preprint arXiv:2310.13433_, 2023.
* Chemseddine et al. [2024] Jannis Chemseddine, Paul Hagemann, Christian Wald, and Gabriele Steidl. Conditional Wasserstein distances with applications in Bayesian OT flow matching. _arXiv preprint arXiv:2403.18705_, 2024.
* 446, 2013.
* Cranmer et al. [2020] Kyle Cranmer, Johann Brehmer, and Gilles Louppe. The frontier of simulation-based inference. _Proceedings of the National Academy of Sciences_, 117(48):30055-30062, 2020.
* Da Prato and Zabczyk [2014] Giuseppe Da Prato and Jerzy Zabczyk. _Stochastic Equations in Infinite Dimensions_. Cambridge University Press, 2014.
* Dashti and Stuart [2013] Masoumeh Dashti and Andrew M Stuart. The Bayesian approach to inverse problems. _arXiv preprint arXiv:1302.6989_, 2013.
* Davtyan et al. [2023] Aram Davtyan, Sepehr Sameni, and Paolo Favaro. Efficient video prediction via sparsely conditioned flow matching. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 23263-23274, 2023.
* Flamary et al. [2021] Remi Flamary, Nicolas Courty, Alexandre Gramfort, Mokhtar Z. Alaya, Aurelie Boisbunon, Stanislas Chambon, Laetitia Chapel, Adrien Corenlfos, Kilian Fatras, Nemo Fournier, Leo Gautheron, Nathalie T.H. Gayraud, Hicham Janati, Alain Rakotomamonjev, Ievgen Redko, Antoine Rolet, Antony Schutz, Vivien Seguy, Dancia J. Sutherland, Romain Tavenard, Alexander Tong, and Titouan Vayer. POT: Python optimal transport. _Journal of Machine Learning Research_, 22(78):1-8, 2021. URL http://jmlr.org/papers/v22/20-451.html.
* Franzese et al. [2024] Giulio Franzese, Giulio Corallo, Simone Rossi, Markus Heinonen, Maurizio Filippone, and Pietro Michiardi. Continuous-time functional diffusion processes. _Advances in Neural Information Processing Systems_, 36, 2024.
* Gebhard et al. [2023] Timothy D Gebhard, Jonas Wildberger, Maximilian Dax, Daniel Angerhausen, Sascha P Quanz, and Bernhard Scholkopf. Inferring atmospheric properties of exoplanets with flow matching and neural importance sampling. _arXiv preprint arXiv:2312.08295_, 2023.
* Gigli [2008] Nicola Gigli. _On the geometry of the space of probability measures in Rn endowed with the quadratic optimal transport distance_. PhD thesis, Scuola Normale Superiore, 2008.
* Hersbach [2000] Hans Hersbach. Decomposition of the continuous ranked probability score for ensemble prediction systems. _Weather and Forecasting_, 15(5):559-570, 2000.
* Hosseini et al. [2023] Bamdad Hosseini, Alexander W Hsu, and Amirhossein Taghvaei. Conditional optimal transport on function spaces. _arXiv preprint arXiv:2311.05672_, 2023.
* Isobe et al. [2024] Noboru Isobe, Masanori Koyama, Kohei Hayashi, and Kenji Fukumizu. Extended flow matching: a method of conditional generation with generalized continuity equation. _arXiv preprint arXiv:2402.18839_, 2024.
* Kerrigan et al. [2023] Gavin Kerrigan, Justin Ley, and Padhraic Smyth. Diffusion generative models in infinite dimensions. In _Proceedings of The 26th International Conference on Artificial Intelligence and Statistics_, volume 206 of _Proceedings of Machine Learning Research_, pages 9538-9563. PMLR, 2023.
* Kerrigan et al. [2024] Gavin Kerrigan, Giosue Migliorini, and Padhraic Smyth. Functional flow matching. In _Proceedings of The 27th International Conference on Artificial Intelligence and Statistics_, volume 238 of _Proceedings of Machine Learning Research_, pages 3934-3942, 2024.
* Kim et al. [2022] Young-geun Kim, Kyungbok Lee, and Myunghee Cho Paik. Conditional wasserstein generator. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 2022.
* Kim et al. [2023] Young-geun Kim, Kyungbok Lee, Youngwon Choi, Joong-Ho Won, and Myunghee Cho Paik. Wasserstein geodesic generator for conditional distributions. _arXiv preprint arXiv:2308.10145_, 2023.
* Klambauer et al. [2017] Gunter Klambauer, Thomas Unterthiner, Andreas Mayr, and Sepp Hochreiter. Self-normalizing neural networks. _Advances in neural information processing systems_, 30, 2017.
* Kovachki et al. [2021] Nikola B. Kovachki, Zongyi Li, Burigede Liu, Kamyar Azizzadenesheli, Kaushik Bhattacharya, Andrew M. Stuart, and Anima Anandkumar. Neural operator: Learning maps between function spaces. _CoRR_, abs/2108.08481, 2021.
* Klambauer et al. [2021]Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya, Andrew Stuart, and Anima Anandkumar. Fourier neural operator for parametric partial differential equations. _arXiv preprint arXiv:2010.08895_, 2020.
* Lim et al. [2023] Jae Hyun Lim, Nikola B Kovachki, Ricardo Baptista, Christopher Beckham, Kamyar Azizzadenesheli, Jean Kossaifi, Vikram Voleti, Jiaming Song, Karsten Kreis, Jan Kautz, et al. Score-based diffusion models in function space. _arXiv preprint arXiv:2302.07400_, 2023.
* Lipman et al. [2022] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew Le. Flow matching for generative modeling. In _The Eleventh International Conference on Learning Representations_, 2022.
* Liu et al. [2022] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. _arXiv preprint arXiv:2209.03003_, 2022.
* McCann [1997] Robert J McCann. A convexity principle for interacting gases. _Advances in Mathematics_, 128(1):153-179, 1997.
* Mirza and Osindero [2014] Mehdi Mirza and Simon Osindero. Conditional generative adversarial nets. _arXiv preprint arXiv:1411.1784_, 2014.
* Papamakarios et al. [2019] George Papamakarios, David Sterratt, and Iain Murray. Sequential neural likelihood: Fast likelihood-free inference with autoregressive flows. In _The 22nd international conference on artificial intelligence and statistics_, pages 837-848. PMLR, 2019.
* Papamakarios et al. [2021] George Papamakarios, Eric Nalisnick, Danilo Jimenez Rezende, Shakir Mohamed, and Balaji Lakshminarayanan. Normalizing flows for probabilistic modeling and inference. _Journal of Machine Learning Research_, 22(57):1-64, 2021.
* Pedregosa et al. [2011] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. _Journal of Machine Learning Research_, 12:2825-2830, 2011.
* Pooladian et al. [2023] Aram-Alexandre Pooladian, Heli Ben-Hamu, Carles Domingo-Enrich, Brandon Amos, Yaron Lipman, and Ricky Chen. Multisample flow matching: Straightening flows with minibatch couplings. _arXiv preprint arXiv:2304.14772_, 2023.
* Ramesh et al. [2022] Poornima Ramesh, Jan-Matthuis Lueckmann, Jan Boelts, Alvaro Tejero-Cantero, David S Greenberg, Pedro J Goncalves, and Jakob H Macke. Gatsbi: Generative adversarial training for simulation-based inference. _arXiv preprint arXiv:2203.06481_, 2022.
* Ray et al. [2022] Deep Ray, Harisankar Ramaswamy, Dhruv V Patel, and Assad A Oberai. The efficacy and generalizability of conditional gans for posterior inference in physics-based inverse problems. _arXiv preprint arXiv:2202.07773_, 2022.
* Sajjadi et al. [2017] Mehdi SM Sajjadi, Bernhard Scholkopf, and Michael Hirsch. Enhancenet: Single image super-resolution through automated texture synthesis. In _Proceedings of the IEEE international conference on computer vision_, pages 4491-4500, 2017.
* Santambrogio [2015] Filippo Santambrogio. Optimal transport for applied mathematicians. _Birkauser, NY_, 55(58-63):94, 2015.
* Sharrock et al. [2022] Louis Sharrock, Jack Simons, Song Liu, and Mark Beaumont. Sequential neural score estimation: Likelihood-free inference with conditional score based diffusion models. _arXiv preprint arXiv:2210.04872_, 2022.
* Spantini et al. [2022] Alessio Spantini, Ricardo Baptista, and Youssef Marzouk. Coupling techniques for nonlinear ensemble filtering. _SIAM Review_, 64(4):921-953, 2022.
* Tong et al. [2023] Alexander Tong, Nikolay Malkin, Guillaume Huguet, Yanlei Zhang, Jarrid Rector-Brooks, Kilian Fatras, Guy Wolf, and Yoshua Bengio. Improving and generalizing flow-based generative models with minibatch optimal transport. In _ICML Workshop on New Frontiers in Learning, Control, and Dynamical Systems_, 2023.
* Trigila and Tabak [2016] Giulio Trigila and Esteban G Tabak. Data-driven optimal transport. _Communications on Pure and Applied Mathematics_, 69(4):613-648, 2016.
* Villani et al. [2009] Cedric Villani et al. _Optimal Transport: Old and New_, volume 338. Springer, 2009.
* Wang et al. [2023] Zheyu Oliver Wang, Ricardo Baptista, Youssef Marzouk, Lars Ruthotto, and Deepanshu Verma. Efficient neural network approaches for conditional optimal transport with applications in Bayesian inference. _arXiv preprint arXiv:2310.16975_, 2023.
* Wildberger et al. [2024] Jonas Wildberger, Maximilian Dax, Simon Buchholz, Stephen Green, Jakob H Macke, and Bernhard Scholkopf. Flow matching for scalable simulation-based inference. _Advances in Neural Information Processing Systems_, 36, 2024.
* Wang et al. [2020]

## Appendix: Table of Contents

* A Optimal Transport
* B Conditional Optimal Transport
* C Closed-Form Conditional Wasserstein Distance for Gaussian Measures
* D Proofs: Section 4
* D.1 Metric Properties
* D.2 Geodesics
* E Proofs: Section 5
* E.1 Continuity Equation
* E.2 Absolutely Continuous Curves
* F Experiment Details
* F.1 2D Synthetic Data
* F.2 Lotka-Volterra Dynamical System
* F.3 Inverse Darcy Flow
* G Minibatch COT

## Appendix A Optimal Transport

We provide here a brief and informal overview of optimal transport in the standard unconditional setting. For more details, we refer to the standard references of Villani et al. (2009); Santambrogio (2015); Ambrosio et al. (2005). Let \(X\) be a separable metric space and fix a cost function \(c:X\times X\to\mathbb{R}\cup\{+\infty\}\). Suppose we have two Borel measures \(\eta,\nu\in\mathbb{P}(X)\). The _Monge problem_ seeks to find a measurable transport map \(T:X\to X\) minimizing the expected cost of transport, i.e. corresponding to the optimization problem

\[\inf_{T}\left\{\int_{X}c(x,T(x))\,\mathrm{d}\eta(x)\mid T_{\#}\eta=\nu\right\}.\] (14)

This optimization problem is challenging, though, as it involves a nonlinear constraint and the set of feasible maps may be empty. In contrast, the _Kantorovich problem_ is a relaxation which seeks to find an optimal coupling \(\gamma\in\Pi(\eta,\nu)\), i.e. a probability distribution over \(X\times X\) with marginals \(\eta,\nu\), which solves

\[\inf_{\gamma}\left\{\int_{X\times X}c(x_{0},x_{1})\,\mathrm{d}\gamma(x_{0},x_ {1})\mid\gamma\in\Pi(\eta,\nu)\right\}.\] (15)

Under fairly weak conditions (e.g., the cost is lower semicontinuous and bounded from below (Ambrosio et al., 2013, Theorem 2.5)), minimizers to the Kantorovich problem are guaranteed to exist. If the cost function is \(c(x_{0},x_{1})=|x_{0}-x_{1}|^{p}\) for some \(1<p<\infty\), under sufficient regularity conditions on \(\eta\) a solution \(T^{\star}\) to the Monge problem is guaranteed to exist and, moreover, the coupling \(\gamma^{\star}=(I,T^{\star})_{\#}\eta\) is optimal for the Kantorovich problem. See Ambrosio et al. (2013, Chapter 2) and Ambrosio et al. (2005, Theorem 6.2.10).

Wasserstein Space.In the special case that \(c(x_{0},x_{1})=|x_{0}-x_{1}|^{p}\) for \(1\leq p<\infty\), and \(\eta,\nu\in\mathbb{P}_{p}(X)\), the Kantorovich problem admits a finite-cost solution. The cost of such an optimal coupling is the \(p\)_-Wasserstein distance_

\[W_{p}^{p}(\eta,\nu)=\min_{\gamma}\left\{\int_{X\times X}|x_{0}-x_{1}|^{p}\, \mathrm{d}\gamma(x_{0},x_{1})\mid\gamma\in\Pi(\eta,\nu)\right\}\] (16)

which, as the name suggests, is a metric on the space \(\mathbb{P}_{p}(X)\)(Ambrosio et al., 2005, Section 7.1)(Santambrogio, 2015, Section 5.1). The Wasserstein distance admits a _dynamical_ formulation via the Benamou-Brenier theorem (Benamou and Brenier, 2000). Namely, the \(p\)-Wasserstein distance can be obtained by finding a time-dependent vector field transforming \(\eta\) to \(\nu\) across time \(t\in[0,1]\) with minimal energy:

\[W_{p}(\eta,\nu)=\min_{(\gamma_{t},v_{t})}\left\{\int_{0}^{1}\int_{X}|v_{t}(x) |^{p}\,\mathrm{d}\gamma_{t}(x)\,\mathrm{d}t\mid\gamma_{0}=\eta,\gamma_{1}=\nu,\partial_{t}\gamma_{t}+\text{div}(v_{t}\gamma_{t})=0\right\}.\] (17)

Here, we constrain our minimization problem over the set of measures and vector fields \((\gamma_{t},v_{t})\) interpolating between \(\eta\) and \(\nu\), satisfying a continuity equation (see Section 5). In Section 4, we study a generalization of the Wasserstein distances for conditional optimal transport problems. In particular, Theorem 5 provides a generalization of the Benamou-Brenier theorem to the conditional setting which recovers a conditional Wasserstein distance.

## Appendix B Conditional Optimal Transport

This section contains additional discussion regarding the static COT problem, supplementing Section 3.1. We refer to Hosseini et al. (2023); Baptista et al. (2020), and Chemseddine et al. (2023) for further results and details.

Given a source and target measures \(\eta,\nu\in\mathbb{P}_{p}^{\mu}(Y\times U)\), the _conditional Monge problem_ seeks to find a triangular mapping solving

\[\inf_{T}\left\{\int_{Y\times U}c(y,u,T(y,u))\,\mathrm{d}\eta(y,u)\mid T_{\#} \eta=\nu,T:(y,u)\mapsto(y,T_{U}(y,u))\right\}.\] (18)

The conditional Monge problem also admits a relaxation under which one only considers couplings whose \(Y\)-components are almost surely equal. To that end, we consider the subset \(\mathscr{C}\subset(Y\times U)^{2}\) whose \(Y\) components are identical, i.e.,

\[\mathscr{C}:=\left\{(y_{0},u_{0},y_{1},u_{1})\in(Y\times U)^{2}\mid y_{0}=y_{ 1}\right\}\] (19)

and we define the set of \((Y)\)_-restricted probability measures_\(\mathcal{R}_{Y}\subset\mathbb{P}\left((Y\times U)^{2}\right)\) such that every \(\gamma\in\mathcal{R}_{Y}\) is concentrated on \(\mathscr{C}\). In other words, if \(\gamma\in\mathcal{R}_{Y}\), then samples \((y_{0},u_{0},y_{1},u_{1})\sim\gamma\) have \(y_{0}=y_{1}\) almost surely. In addition, for any \(\eta,\nu\in\mathbb{P}(Y\times U)\), we define the set of _triangular couplings_\(\Pi_{Y}(\eta,\nu)\) to be the probability measures in \(\mathcal{R}_{Y}\) whose marginals are \(\eta\) and \(\nu\), i.e.

\[\Pi_{Y}(\eta,\nu)=\left\{\gamma\in\mathcal{R}_{Y}\mid\pi_{\#}^{1,2}\gamma= \eta,\pi_{\#}^{3,4}\gamma=\nu\right\}.\] (20)

The _conditional Kantorovich problem_ seeks a triangular coupling \(\gamma^{\star}\) solving

\[\inf_{\gamma}\left\{\int_{(Y\times U)^{2}}c(y_{0},u_{0},y_{1},u_{1})\,\mathrm{ d}\gamma(y_{0},u_{0},y_{1},u_{1})\mid\gamma\in\Pi_{Y}(\eta,\nu)\right\}.\] (21)

Hosseini et al. (2023) prove the existence of minimizers to the conditional Kantorovich and Monge problems under very general assumptions. Moreover, optimal couplings to the conditional Kantorovich problem induce optimal couplings for \(\mu\)-almost every conditional measure. Assuming sufficient regularity assumptions on the conditional measures, unique solutions to the conditional Monge problem exist. We restate these results here for the sake of completeness.

**Proposition 3** (Prop 3.3 (Hosseini et al., 2023)): _Fix \(\eta,\nu\in\mathbb{P}^{\mu}(Y\times U)\). Suppose the cost function \(c\) is continuous, \(\inf c>-\infty\), and there exists a finite cost coupling \(\gamma\in\Pi_{Y}(\eta,\nu)\). Then, the conditional Kantorovich problem admits a minimizer \(\gamma^{\star}\). Moreover, \(\gamma^{\star,y_{0}}(y_{1},u_{0},u_{1})=\hat{\gamma}^{\star,y_{0}}(u_{0},u_{1} )\delta(y_{1}-y_{0})\) where for \(\mu\)-almost every \(y\) the measure \(\gamma^{\star,y}\) is an optimal coupling for \(\eta^{y},\nu^{y}\) under the cost \(c^{y}(u_{0},u_{1})=c(y,u_{0},y,u_{1})\)_

**Proposition 4** (Prop 3.8 (Hosseini et al., 2023)): _Fix \(1<p<\infty\) and \(\eta,\nu\in\mathbb{P}^{\mu}_{p}(Y\times U)\). Suppose \(c(y_{0},u_{0},y_{1},u_{1})=|u_{0}-u_{1}|^{p}\). If \(\eta^{y}\) assign zero measure to Gaussian null sets for \(\mu\)-almost every \(y\), then there is a unique solution \(T^{\star}\) to the conditional Monge problem, and \(\gamma^{\star}=(I,T^{\star})_{\#}\eta\) is the unique solution to the conditional Kantorovich problem. If \(\nu^{y}\) also assign zero measure to Gaussian null sets for \(\mu\)-almost every \(y\), then \(T^{\star}\) is injective \(\eta\)-almost everywhere._

## Appendix C Closed-Form Conditional Wasserstein Distance for Gaussian Measures

In this section, we provide additional details and results regarding the closed-form conditional Wasserstein distance for Gaussian distributions. See Section 4 in the main paper.

Suppose \(Y=\mathbb{R}^{d}\) and \(U=\mathbb{R}^{d^{\prime}}\) are Euclidean spaces (of possibly different dimensions), and that \(\eta,\nu\in\mathbb{P}^{\mu}_{p}(Y\times U)\) are Gaussians of the form

\[\eta=\mathcal{N}\left(\begin{bmatrix}m\\ m_{u}^{\eta}\end{bmatrix},\begin{bmatrix}\Sigma&\Lambda^{\eta}\\ \Lambda^{\eta^{T}}&\Sigma_{u}^{\eta}\end{bmatrix}\right)\qquad\nu=\mathcal{N} \left(\begin{bmatrix}m\\ m_{u}^{\nu}\end{bmatrix},\begin{bmatrix}\Sigma&\Lambda^{\nu}\\ \Lambda^{\nu^{T}}&\Sigma_{u}^{\nu}\end{bmatrix}\right)\] (22)

where \(m\in\mathbb{R}^{d}\), \(m_{u}^{\eta},m_{u}^{\nu}\in\mathbb{R}^{d^{\prime}}\), and the (block) covariance matrices are \(\Sigma\in\mathbb{R}^{d\times d}\), \(\Lambda^{\eta},\Lambda^{\nu}\in\mathbb{R}^{d\times d^{\prime}}\), and \(\Sigma_{u}^{\eta},\Sigma_{u}^{\nu}\in\mathbb{R}^{d^{\prime}\times\,d^{\prime}}\).

This form is chosen to ensure that \(\eta\) and \(\nu\) have equal \(Y\)-marginals. It follows that \(\mu=\pi_{\#}^{Y}\eta=\pi_{\#}^{Y}\nu=\mathcal{N}(m,\Sigma)\). Let

\[Q^{\eta}=\Sigma_{u}^{\eta}-\Lambda^{\eta^{T}}\Sigma^{-1}\Lambda^{\eta}\qquad Q ^{\nu}=\Sigma_{u}^{\nu}-\Lambda^{\nu^{T}}\Sigma^{-1}\Lambda^{\nu}\qquad R=( \Lambda^{\eta}-\Lambda^{\nu})^{T}\Sigma^{-1}.\] (23)

We have that the conditionals \(\eta^{y},\nu^{y}\) are available in closed-form:

\[\eta^{y}=\mathcal{N}\left(m_{u}^{\eta}+\Lambda^{\eta^{T}}\Sigma^{-1}(y-m),Q^{ \eta}\right)\qquad\nu^{y}=\mathcal{N}\left(m_{u}^{\nu}+\Lambda^{\nu^{T}} \Sigma^{-1}(y-m),Q^{\nu}\right).\] (24)

Thus, for any fixed \(y\), we use the known closed-form unconditional Wasserstein distance to obtain

\[W_{2}^{2}(\eta^{y},\nu^{y})= \big{|}m_{u}^{\eta}-m_{u}^{\nu}+R(y-m)\big{|}^{2}+\text{Tr}\left( Q^{\eta}+Q^{\nu}-2\left((Q^{\eta})^{1/2}Q^{\nu}(Q^{\eta})^{1/2}\right)^{1/2} \right).\] (25)

We now take an expectation over \(y\sim\mu=\mathcal{N}(m,\Sigma)\) to compute \(W_{2}^{\mu,2}\). Observe that \(R(y-m)\sim\mathcal{N}(0,R\Sigma R^{\text{T}})\) and that \(\mathbb{E}_{y\sim\mu}[|R(y-m)|^{2}]=\text{Tr}(R\Sigma R^{\text{T}})\). Thus,

\[W_{2}^{\mu,2}(\eta,\nu) =\mathbb{E}_{y\sim\mu}\left[W_{2}^{2}(\eta^{y},\nu^{y})\right]\] (26) \[=\mathbb{E}_{y\sim\mu}\left[|m_{u}^{\eta}-m_{u}^{\nu}|^{2}+2 \langle m_{u}^{\eta}-m_{u}^{\nu},R(y-m)\rangle+|R(y-m)|^{2}\right]\] (27) \[\qquad+\text{Tr}\left(Q^{\eta}+Q^{\nu}-2\left((Q^{\eta})^{1/2}Q ^{\nu}(Q^{\eta})^{1/2}\right)^{1/2}\right)\] \[=|m_{u}^{\eta}-m_{u}^{\nu}|^{2}+\text{Tr}\left(Q^{\eta}+Q^{\nu} -2\left((Q^{\eta})^{1/2}Q^{\nu}(Q^{\eta})^{1/2}\right)^{1/2}+R\Sigma R^{T} \right).\] (28)

This form, perhaps unsurprisingly, closely resembles the unconditional Wasserstein distance between two Gaussians, except for the presence of an additional \(\text{Tr}(R\Sigma R^{\text{T}})\) term. Note that when \(\eta,\nu\) have uncorrelated \(Y,U\) components, we precisely recover \(W_{2}^{2}(\pi_{\#}^{\nu}\eta,\pi_{\#}^{U}\nu)\) as one may expect. As a special case of interest, if \(Y=U=\mathbb{R}\) and

\[\eta=\mathcal{N}(0,I)\qquad\nu=\mathcal{N}\left(0,\begin{bmatrix}1&\rho\\ \rho&1\end{bmatrix}\right)\quad|\rho|<1\] (29)then we obtain as a special case of Equation (26) that \(W_{2}^{\mu,2}(\eta,\nu)=2(1-\sqrt{1-\rho^{2}})\). This is zero if and only if \(\rho=0\), i.e. \(\eta=\nu\).

## Appendix D Proofs: Section 4

In this section, we provide detailed proofs of our claims in Section 4, regarding the metric properties of the conditional Wasserstein space.

### Metric Properties

We first note that \(W_{p}^{\mu}(\eta,\nu)\) may be viewed as the minimal value of the constrained Kantorovich problem in Equation (3) when one takes the cost to be the metric on the space \(Y\times U\). Similar results, relating the conditional Wasserstein distance to triangular couplings, have appeared previously, but our proof is independent of these prior works (Chemseddine et al., 2023; Gigli, 2008).

**Proposition 5** (Equivalent Formulation of the Conditional Wasserstein Distance):

_Fix \(\eta,\nu\in\mathbb{P}_{p}^{\mu}(Y\times U)\) and \(1\leq p<\infty\). Then, \(W_{p}^{\mu}(\eta,\nu)\) is well-defined, finite, and_

\[W_{p}^{\mu,p}(\eta,\nu)=\min_{\gamma}\left\{\int_{(Y\times U)^{2}}d^{p}(y_{0},u_{0},y_{1},u_{1})\,\mathrm{d}\gamma\mid\gamma\in\Pi_{Y}(\eta,\nu)\right\}\] (30)

_where \(W_{p}^{\mu,p}(\eta,\nu)\) represents the \(p\)-th power of the conditional \(p\)-Wasserstein distance._

Proof.: The cost function \(d^{p}\) is clearly continuous and non-negative, and hence by Proposition 3 it suffices to exhibit a finite-cost coupling \(\gamma\in\Pi_{Y}(\eta,\nu)\) between \(\eta\) and \(\nu\). Indeed, take the conditionally independent coupling

\[\gamma(y_{0},u_{0},y_{1},u_{1})=\eta(u_{0}\mid y_{1})\nu(u_{1}\mid y_{1}) \delta(y_{1}-y_{0})\mu(y_{1})\] (31)

which is clearly in \(\Pi_{Y}(\eta,\nu)\). We then have that

\[\int_{(Y\times U)^{2}}d^{p}(y_{0},u_{0},y_{1},u_{1})\,\mathrm{d} \gamma(y_{0},u_{0},y_{1},u_{1})=\int_{(Y\times U)^{2}}\|(y_{0},u_{0})-(y_{1},u_ {1})\|_{Y\times U}^{p}\,\,\mathrm{d}\gamma(y_{0},u_{0},y_{1},u_{1})\] \[\leq 2^{p}\int_{(Y\times U)^{2}}\left(\|(y_{0},u_{0})\|_{Y\times U }^{p}+\|(y_{1},u_{1})\|_{Y\times U}^{p}\right)\,\mathrm{d}\gamma(y_{0},u_{0},y _{1},u_{1})\] \[=2^{p}\left(\int_{Y\times U}\|(y_{0},u_{0})\|_{Y\times U}^{p}\, \,\mathrm{d}\eta(y_{0},u_{0})+\int_{Y\times U}\|(y_{1},u_{1})\|_{Y\times U}^{ p}\,\,\mathrm{d}\nu(y_{1},u_{1})\right)<+\infty.\]

Hence, Equation (30) admits a minimizer \(\gamma^{\star}\in\Pi_{Y}(\eta,\nu)\). By Proposition 3, this minimizer may be taken to have the form \(\gamma^{\star}=\gamma^{\star,y_{1}}(u_{0},u_{1})\delta(y_{1}-y_{0})\mu(y_{1})\) where \(\gamma^{\star,y_{1}}(u_{0},u_{1})\) is \(\mu(y_{1})\)-almost surely an optimal coupling between \(\eta^{y_{1}},\nu^{y_{1}}\) for the cost \(|u_{1}-u_{0}|^{p}\). Thus,

\[\int_{(Y\times U)^{2}}d^{p}\,\mathrm{d}\gamma^{\star} =\int_{Y}\int_{U^{2}}|u_{1}-u_{0}|^{p}\,\mathrm{d}\gamma^{\star,y} (u_{0},u_{1})\,\mathrm{d}\mu(y)\] (32) \[=\int_{Y}W_{p}^{p}(\eta^{y},\nu^{y})\,\mathrm{d}\mu(y)=W_{p}^{p, \mu}(\eta,\nu).\] (33)

Here, we emphasize that the \(\mu\)-almost sure uniqueness of the disintegrations of \(\eta,\nu\) along \(Y\) result in a well-defined expression.

Moreover, if \(\eta\in\mathbb{P}_{p}^{\mu}(Y\times U)\) it follows that \(\eta^{y}\in\mathbb{P}_{p}(U)\) for \(\mu\)-a.e. \(y\), because

\[\int_{Y}\int_{U}|u|^{p}\,\mathrm{d}\eta^{y}(u)\,\mathrm{d}\mu(y) \leq\int_{Y}\int_{U}|(y,u)|^{p}\,\mathrm{d}\eta^{y}(u)\,\mathrm{d} \mu(y)\] (34) \[=\int_{Y\times U}|(y,u)|^{p}\,\mathrm{d}\eta(y,u)<+\infty.\] (35)

Thus all considered \(p\)-Wasserstein distances on \(U\) are finite.

[MISSING_PAGE_FAIL:18]

Thus \(\pi^{U}_{\#}\gamma=\pi^{U}_{\#}\eta\). A similar argument shows that for the map \(\pi^{1}:(u_{0},u_{1})\mapsto u_{1}\) we have \(\pi^{1}_{\#}\gamma=\pi^{U}_{\#}\nu\), so that \(\gamma\in\Pi(\pi^{U}_{\#}\eta,\pi^{U}_{\#}\nu)\).

Now, as \(\gamma^{\star,y_{1}}(u_{0},u_{1})\in\Pi(\eta^{y_{1}},\nu^{y_{1}})\) is \(\mu\)-almost surely optimal in the usual Wasserstein sense,

\[W^{p,\mu}_{p}(\eta,\nu) =\int_{Y}\int_{U^{2}}|u_{0}-u_{1}|^{p}\,\mathrm{d}\gamma^{\star,y} (u_{0},u_{1})\,\mathrm{d}\mu(y)\] (46) \[=\int_{U^{2}}|u_{0}-u_{1}|^{p}\,\mathrm{d}\gamma(u_{0},u_{1})\] (47) \[\geq W^{p}_{p}(\pi^{U}_{\#}\eta,\pi^{U}_{\#}\nu)\] (48)

since \(\gamma\in\Pi(\pi^{U}_{\#}\eta,\pi^{U}_{\#}\nu)\) is a coupling but potentially sub-optimal.

### Geodesics

We now study the geodesics in the space \(\mathbb{P}^{\mu}_{p}(Y\times U)\).

**Theorem 1** (\(\mathbb{P}^{\mu}_{p}(Y\times U)\) is a Geodesic Space): _For any \(\eta,\nu\in\mathbb{P}^{\mu}_{p}(Y\times U)\), there exists a constant speed geodesic between \(\eta\) and \(\nu\)._

Proof.: Write \(\lambda_{t}:(Y\times U)^{2}\to Y\times U\) for the linear interpolant

\[\lambda_{t}(y_{0},u_{0},y_{1},u_{1})=(ty_{0}+(1-t)y_{1},tu_{0}+(1-t)u_{1}) \qquad 0\leq t\leq 1.\] (49)

Let \(\gamma^{\star}\in\Pi_{Y}(\eta,\nu)\) be an optimal restricted coupling, and consider the path of measures in \(\mathbb{P}_{p}(Y\times U)\) given by

\[\gamma_{t}=[\lambda_{t}]_{\#}\gamma^{\star}\qquad 0\leq t\leq 1.\] (50)

**Step one:** We check that for each \(0\leq t\leq 1\), we have \(\gamma_{t}\in\mathbb{P}^{\mu}_{p}(Y\times U)\). That is, we need to check that for all Borel \(A\subseteq Y\), we have \(\gamma_{t}(A\times U)=\mu(A)\). Indeed, recall that restricted measures are concentrated on the set \(\mathscr{C}\) (see Equation (19)). Thus,

\[\gamma_{t}(A\times U) =\gamma^{\star}\left\{\lambda_{t}^{-1}(A\times U)\right\}\] \[=\gamma^{\star}\left\{(y,u_{0},y,u_{1})\mid y\in A\right\}\] \[=\pi^{1}_{\#}\gamma^{\star}(A)=(\pi^{1}\circ\pi^{1,2})_{\#}\gamma^ {\star}(A)\] \[=\pi^{1}_{\#}\eta(A)=\mu(A)\]

i.e. \(\gamma_{t}(A\times Y)=\mu(A)\) as claimed.

**Step two:** We show that \(W^{\mu}_{p}(\gamma_{t},\gamma_{s})=|t-s|W^{\mu}_{p}(\eta,\nu)\). Set \(\gamma^{s}_{t}:=(\lambda_{t},\lambda_{s})_{\#}\gamma^{\star}\) for \(0\leq s<t\leq 1\). We claim \(\gamma^{s}_{t}\in\Pi_{Y}(\gamma_{t},\gamma_{s})\). Indeed, we have \(\pi^{1,2}_{\#}\gamma^{s}_{t}=\gamma_{t}\) because for all Borel \(A\subseteq Y\times U\),

\[(\lambda_{t},\lambda_{s})_{\#}\gamma^{\star}\left(A\times Y\times U\right)= \gamma^{\star}\left(\lambda_{t}^{-1}(A)\right)=(\lambda_{t})_{\#}\gamma^{\star }(A).\] (51)

Figure 4: The counterexample in Proposition 2. The measure \(\eta_{k}\) is shown in black and the measure \(\nu_{k}\) is shown in white.

An analogous calculation shows that \(\pi^{3,4}_{\mathscr{H}}\gamma^{*}_{t}=\gamma_{s}\), so that \(\gamma^{*}_{t}\in\Pi(\gamma_{t},\gamma_{s})\). We now check that \(\gamma^{*}_{t}\in\mathcal{R}_{Y}(Y\times U)\). Indeed, suppose \(E\subseteq Y\times U\) is a Borel set such that \(E\cap\mathscr{C}\varnothing\). In other words, for every \((y_{0},u_{0},y_{1},u_{1})\in E\) we have \(y_{0}\neq y_{1}\). Set \(D:=(\lambda_{t},\lambda_{s})^{-1}(E)\). We claim \(D\cap\mathscr{C}=\varnothing\), so that

\[\gamma^{*}_{t}(E)=(\lambda_{t},\lambda_{s})_{\#}\gamma^{*}(E) =\gamma^{*}((\lambda_{t},\lambda_{s})^{-1}(E))\] (52) \[=\gamma^{*}(D\cap\mathscr{C})=0.\] (53)

Indeed, if \(c=(y,u_{0},y,u_{1})\in\mathscr{C}\), then

\[(\lambda_{t},\lambda_{s})(c) =(y,tu_{0}+(1-t)u_{1},y,su_{0}+(1-s)u_{1})\notin E\] (54) \[\implies c\notin(\pi_{t},\pi_{s})^{-1}(E).\] (55)

Thus \(\gamma^{s}_{t}\in\Pi_{Y}(\eta,\nu)\) as claimed. Now, we have

\[W^{\mu,p}_{p}(\gamma_{t},\gamma_{s}) \leq\int_{(Y\times U)^{2}}d^{p}\left(y_{0},u_{0},y_{1},u_{1} \right)\,\mathrm{d}\lambda^{s}_{t}(y_{0},u_{0},y_{1},u_{1})\] \[=\int_{(Y\times U)^{2}}d^{p}\left(\lambda_{t}(y_{0},u_{0},y_{1},u _{1}),\lambda_{s}(y_{0},u_{0},y_{1},u_{1})\right)\,\mathrm{d}\gamma^{\star}(y_ {0},u_{0},y_{1},u_{1})\] \[=\int_{(Y\times U)^{2}}\left(|(t-s)(y_{0}-y_{1})|^{2}+|(t-s)(u_{0 }-u_{1})|^{2}\right)^{p/2}\,\mathrm{d}\gamma^{\star}(y_{0},u_{0},y_{1},u_{1})\] \[=|t-s|^{p}\int_{(Y\times U)^{2}}d^{p}(y_{0},u_{0},y_{1},u_{1})\, \mathrm{d}\gamma^{\star}(y_{0},u_{0},y_{1},u_{1})\] \[=|t-s|^{p}W^{\mu,p}_{p}(\eta,\nu).\]

Conversely, an application of the previous inequality and the triangle inequality show that for \(0\leq s\leq t\leq 1\),

\[W^{\mu}_{p}(\eta,\nu) \leq W^{\mu}_{p}(\eta,\gamma_{s})+W^{\mu}_{p}(\gamma_{s},\gamma_ {t})+W^{\mu}_{p}(\gamma_{t},\nu)\] (56) \[\leq sW^{\mu}_{p}(\eta,\nu)+W^{\mu}_{p}(\gamma_{s},\gamma_{t})+(1 -t)W^{\mu}_{p}(\eta,\nu).\] (57)

Rearranging the previous inequality implies \(|t-s|W^{\mu}_{p}(\eta,\nu)\leq W^{\mu}_{p}(\gamma_{s},\gamma_{t})\) for all \(s,t\in[0,1]\), and hence \(W^{\mu}_{p}(\gamma_{t},\gamma_{s})=|t-s|^{p}W^{\mu}_{p}(\eta,\nu)\). 

**Theorem 2** (Conditional McCann Interpolants): _Fix \(\eta,\nu\in\mathbb{P}^{\mu}_{p}(Y\times U)\). Suppose \(T^{*}(y,u)=(y,T^{*}_{\mathscr{U}}(y,u))\) is an injective triangular map solving the conditional Monge problem (1). Define the maps \(T_{t}:Y\times U\to Y\times U\) for \(0\leq t\leq 1\) via \(T_{t}=(1-t)I+tT^{*}\), and define the curve of measures \(\gamma_{t}=[T_{t}]_{\#}\eta\in\mathbb{P}^{\gamma}_{p}(Y\times U)\). Then,_

1. \((\gamma_{t})\) _is absolutely continuous and a constant speed geodesic between_ \(\eta,\nu\)__
2. _The vector field_ \(v_{t}(T^{*}_{t}(y,u))=(0,T^{*}_{U}(y,u)-u)\) _generates the path_ \(\gamma_{t}\)_, in the sense that_ \((\gamma_{t},v_{t})\) _solve the continuity equation (_9_)._

Proof.: Consider the function \(w_{t}:Y\times U\to U\) given by

\[w_{t}(y,u)=(0,T^{*}_{U}(y,u)-u)=(0,w_{t,U}(y,u))\] (58)

and note this is precisely \(w_{t}(y,u)=\partial_{t}T^{*}_{t}(y,u)\). Define the vector field

\[v_{t}(y,u)=\left(w_{t}\circ T^{*,-1}_{t}\right)(y,u)=\left(0,(w_{t,\mathscr{U} }\circ T^{*,-1}_{t,\mathscr{U}})(y,u)\right).\] (59)

For any \(\varphi\in\text{Cyl}(Y\times U)\), we have

\[\frac{\mathrm{d}}{\mathrm{d}t}\int_{Y\times U}\varphi(y,u)\, \mathrm{d}\gamma_{t}(y,u) =\frac{\mathrm{d}}{\mathrm{d}t}\int_{Y\times U}\varphi(y,u)\, \mathrm{d}[T_{t}]_{\#}\eta(y,u)\] (60) \[=\frac{\mathrm{d}}{\mathrm{d}t}\int_{Y\times U}\varphi(y,T^{*}_{t,U}(y,u))\,\mathrm{d}\eta(y,u)\] (61) \[=\int_{Y\times U}\langle\nabla\varphi(y,T^{*}_{t,U}(y,u),w_{t}(y, u))\rangle\,\mathrm{d}\eta(y,u)\] (62) \[=\int_{Y\times U}\langle\nabla\varphi(y,u),v_{t}(y,u)\rangle\, \mathrm{d}\gamma_{t}(y,u)\] (63)which shows that \((\gamma_{t},v_{t})\) solve the continuity equation.

Now, note that for \(0\leq a\leq b\leq 1\), we have

\[\int_{a}^{b}\left\|v_{t}\right\|_{L^{p}(\gamma_{t},Y\times U)}\, \mathrm{d}t =\int_{a}^{b}\left(\int_{Y\times U}\left|w_{t}\circ T_{t}^{\star,- 1}\right|^{p}(y,u)\,\mathrm{d}\gamma_{t}(y,u)\right)^{1/p}\,\mathrm{d}t\] (64) \[=\int_{a}^{b}\left(\int_{Y\times U}|w_{t}|^{p}(y,u)\,\mathrm{d} \eta(y,u)\right)^{1/p}\,\mathrm{d}t\] (65) \[=\int_{a}^{b}\left(\int_{Y\times U}|u-T_{U}^{\star}(y,u)|^{p}(y,u )\,\mathrm{d}\eta(y,u)\right)^{1/p}\,\mathrm{d}t\] (66) \[=(b-a)W_{p}^{\mu}(\eta,\nu).\] (67)

In particular, \(\int_{0}^{1}\left\|v_{t}\right\|_{L^{p}(\gamma_{t},Y\times U)}\,\mathrm{d}t<\infty\) and so by Theorem 4\((\gamma_{t})\) is absolutely continuous. A similar calculation shows that \((b-a)W_{p}^{\mu}(\eta,\nu)=W_{p}^{\mu}(\gamma_{b},\gamma_{a})=\int_{a}^{b}| \gamma^{\prime}(t)|\), where the last line follows from the absolute continuity of \(\gamma_{t}\). Thus, \(\left\|v_{t}\right\|_{L^{p}(\gamma_{t},Y\times U)}=|\gamma^{\prime}|(t)\) for almost every \(t\in[0,1]\) by Lebesgue differentiation.

## Appendix E Proofs: Section 5

In this section, we provide proofs of all claims made in Section 5.

### Continuity Equation

We begin with a lemma that is used in the proof of Theorem 4. Informally, a solution to the continuity equation with a triangular vector field will result in the conditional measures almost surely satisfying the continuity equation as well.

**Lemma 1** (Triangular Vector Fields Preserve Conditionals): _Suppose \(v_{t}(y,u)=(0,v_{t}^{U}(y,u))\) is triangular and that \((\gamma_{t})\subset\mathbb{P}_{p}^{\mu}(Y\times U)\) is a path of measures such that \((v_{t},\gamma_{t})\) satisfy the continuity equation in the sense of distributions. Then, it follows that for \(\mu\)-almost every \(y\in Y\), we have \(\partial_{t}\gamma_{t}^{y}+\nabla\cdot(v_{t}^{U}(y,-)\gamma_{t}^{y})=0\)._

Proof.: Fix any \(\varphi\in\text{Cyl}(U\times I)\). Suppose \(\psi\in\text{Cyl}(Y)\) is given, and note that \(\psi(y)\varphi(u,t)\in\text{Cyl}(Y\times U\times I)\). As \((v_{t},\gamma_{t})\) solve the continuity equation, it follows from the triangular structure of \(v_{t}\) that upon testing against \(\psi\varphi\) we have

\[\int_{I}\int_{Y}\psi(y)\int_{U}\left(\partial_{t}\varphi(u,t)+\langle v_{t}^{ U}(y,u),\nabla_{u}\varphi(u,t)\right)\,\mathrm{d}\gamma_{t}^{y}(u)\,\mathrm{d} \mu(y)\,\mathrm{d}t=0.\] (68)

Because \(\psi(y)\in\text{Cyl}(Y)\), it is of the form \(\rho(\pi(y))\) where \(\pi:Y\to\mathbb{R}^{k}\) for some \(k\geq 1\) and \(\rho\in C_{c}^{\infty}(\mathbb{R}^{k})\). Taking \(\rho\) to be a sequence of smooth approximations to the indicator function of an arbitrary rectangle \(E=E_{1}\times E_{2}\times\cdots\times E_{k}\subseteq\mathbb{R}^{k}\), we see

\[\int_{\pi^{-1}(E)}\int_{I}\int_{U}\left(\partial_{t}\varphi(u,t)+\langle v_{t} ^{U}(y,u),\nabla_{u}\varphi(u,t)\right)\,\mathrm{d}\gamma_{t}^{y}(u)\, \mathrm{d}t\,\mathrm{d}\mu(y)=0.\] (69)

As \(Y\) is separable, the Borel \(\sigma\)-algebra on \(Y\) is generated by the cylinder sets, i.e. those which are precisely of the form \(\pi^{-1}(E)\) for some finite-dimensional rectangle \(E\). We have thus shown that for an arbitrary Borel measurable set \(E\subseteq Y\),

\[\int_{E}\int_{I}\int_{U}\left(\partial_{t}\varphi(u,t)+\langle v_{t}^{U}(y,u),\nabla_{u}\varphi(u,t)\right)\,\mathrm{d}\gamma_{t}^{y}(u)\,\mathrm{d}t\, \mathrm{d}\mu(y)=0.\] (70)

From this, it follows that

\[\int_{I}\int_{U}\left(\partial_{t}\varphi(u,t)+\langle v_{t}^{U}(y,u),\nabla_ {u}\varphi(u,t)\right)\,\mathrm{d}\gamma_{t}^{y}(u)\,\mathrm{d}\mu(y)\, \mathrm{d}t=0\qquad\text{$\mu$-almost every $y$}.\] (71)

### Absolutely Continuous Curves

We now proceed to prove the main results of this section. First, we introduce some preliminary notions. We define the map \(j_{q}:L^{q}(\gamma,Y\times U)\to L^{p}(\gamma,Y\times U)\) for \(1/p+1/q=1\) via

\[j_{q}(w)=\begin{cases}|w|^{q-2}w&w\neq 0\\ 0&w=0\end{cases}\] (72)

which is the Frechet differential of the convex functional \(\frac{1}{q}\left\|w\right\|_{L^{q}(\gamma,Y\times U)}^{q}\). A straightforward calculation shows that this map satisfies

\[\left\|j_{q}(w)\right\|_{L^{p}(\gamma,Y\times U)}^{p}=\left\|w\right\|_{L^{q} (\gamma,Y\times U)}^{q}=\int_{Y\times U}\langle j_{q}(w),w\rangle\,\mathrm{d} \gamma(y,u).\] (73)

See also Ambrosio et al. (2005, Chapter 8).

**Theorem 3** (Absolutely Continuous Curves in \(\mathbb{P}_{p}^{\mu}(Y\times U)\)): _Let \(I\subset\mathbb{R}\) be an open interval, and suppose \(\gamma_{t}:I\to\mathbb{P}_{p}^{n}(Y\times U)\) is an absolutely continuous in the \(W_{p}^{\mu}\) metric with \(|\gamma^{\prime}|(t)\in L^{1}(I)\). Then, there exists a Borel vector field \(v_{t}(y,u)\) such that_

* \(v_{t}\) _is triangular_
* \(v_{t}\in L^{p}(\gamma_{t},Y\times U)\) _and_ \(\left\|v_{t}\right\|_{L^{p}(\gamma_{t},Y\times U)}\leq|\gamma^{\prime}|(t)\) _for a.e._ \(t\)__
* \((v_{t},\gamma_{t})\) _solve the continuity equation in the sense of distributions._

Proof.: Assume without loss of generality that \(|\gamma^{\prime}|(t)\in L^{\infty}(I)\) and that \(I=(0,1)\)(Ambrosio et al., 2005, Lemma 1.1.4, Lemma 8.1.3). Fix any \(\varphi\in\text{Cyl}(Y\times U)\). For \(s,t\in I\) there exists an optimal triangular coupling \(\gamma_{st}\in\Pi_{Y}(\gamma_{s},\gamma_{t})\). By Holder's inequality,

\[|\gamma_{t}(\varphi)-\gamma_{s}(\varphi)|\leq\text{Lip}(\varphi)W_{p}^{\mu}( \gamma_{s},\gamma_{t}).\] (74)

It follows that \(t\mapsto\gamma_{t}(\varphi)\) is absolutely continuous. We can introduce the upper semicontinuous and bounded map

\[H(y_{0},u_{0},y_{1},u_{1})=\begin{cases}|\nabla\varphi(y_{0},u_{0})|&(y_{0},u _{0})=(y_{1},u_{1})\\ \frac{|\varphi(y_{0},u_{0})-\varphi(y_{1},u_{1})|}{|(y_{0},u_{0})-(y_{1},u_{1} )|}&(y_{0},u_{0})\neq(y_{1},u_{1})\end{cases}.\] (75)

For \(|h|\) sufficiently small, choose any optimal coupling \(\gamma_{(s+h)h}\in\Pi_{Y}(\gamma_{s+h},\gamma_{s})\) and note that

\[\frac{|\gamma_{s+h}(\varphi)-\gamma_{s}(\varphi)|}{|h|} \leq\frac{1}{|h|}\int_{(Y\times U)^{2}}|(y_{0},u_{0})-(y_{1},u_{1} )|H(y_{0},u_{0},y_{1},u_{1})\,\mathrm{d}\gamma_{(s+h)s}\] (76) \[\leq\frac{W_{p}^{\mu}(\gamma_{s+h},\gamma_{s})}{|h|}\left(\int_{( Y\times U)^{2}}H^{q}(y_{0},u_{0},y_{1},u_{1})\,\mathrm{d}\gamma_{(s+h)h,s} \right)^{1/q}.\] (77)

If \(t\) is a point of metric differentiability for \(t\mapsto\gamma_{t}\), note that \(\gamma_{(t+h)t}\to(I,I)_{\#}\gamma_{t}\) narrowly, where \(I\) is the identity map on \(Y\times U\). Moreover, since \(\gamma_{t}\in\mathbb{P}_{p}^{\mu}(Y\times U)\), it follows that on the diagonal we have that almost surely \(H(y_{0},u_{0},y_{0},u_{1})=\iota(|\nabla_{u}\varphi(y_{0},u_{0})|\). Thus,

\[\limsup_{h\to 0}\frac{|\gamma_{t+h}(\varphi)-\gamma_{t}( \varphi)|}{|h|} \leq|\gamma^{\prime}|(t)\left(\int_{Y\times U}|H|^{q}(y_{0},u_{0},y _{0},u_{0})\,\mathrm{d}\gamma_{t}(y_{0},u_{0})\right)^{1/q}\] (78) \[=|\gamma^{\prime}|(t)\left\|\iota(\nabla_{u}\varphi)\right\|_{L^{ q}(\gamma_{t},Y\times U)}=|\gamma^{\prime}|(t)\left\|\nabla_{u}\varphi \right\|_{L^{q}(\gamma_{t},U)}.\] (79)

Taking \(Q=Y\times U\times I\) and \(\gamma=\int\gamma_{t}\,\mathrm{d}t\), fix any \(\varphi\in\text{Cyl}(Q)\). We have that

\[\begin{split}&\int_{Q}\partial_{s}\varphi(y,u,s)\,\mathrm{d} \gamma(y,u,s)\\ &=\lim_{h\downarrow 0}\int_{I}\frac{1}{h}\left(\int_{Y\times U} \varphi(y,u,s)\,\mathrm{d}\gamma_{s}(y,u)-\int_{(Y\times U)}\varphi(y,u,s)\, \mathrm{d}\gamma_{s+h}(y,u)\right)\,\mathrm{d}s.\end{split}\] (80)An application of Fatou's Lemma, Equation (78), and Holder's inequality gives us

\[\left|\int_{Q}\partial_{s}\varphi(y,u,s)\,\mathrm{d}\gamma(y,u,s)\right|\leq \left(\int_{J}|\gamma^{\prime}|(s)\,\mathrm{d}s\right)^{1/p}\left(\int_{Q}| \nabla_{u}\varphi(y,u,s)|^{q}\,\mathrm{d}\mu(y,u,s)\right)^{1/q}\] (81)

for any interval \(J\subset I\) with supp \(\varphi\subset J\times Y\times U\).

Fix the subspace

\[V=\{\iota(\nabla_{u}\varphi(y,u,s)):\varphi\in\text{Cyl}(Q)\}\subseteq Y\times U\] (82)

and denote by \(\overline{V}\) its \(L^{q}(\gamma,Y\times U\times I)\) closure. Define the linear functional \(L:V\to\mathbb{R}\) via

\[L(\nabla_{u}\varphi)=-\int_{Q}\partial_{s}\varphi(y,u,s)\,\mathrm{d}\gamma(y, u,s)\] (83)

and note that Equation (81) implies that \(L\) is a bounded linear functional on \(V\). Thus (by Hahn-Banach and the fact that \(V\subseteq\overline{V}\) is dense) we may uniquely extend \(L\) to \(\overline{V}\). We thus have a convex minimization problem

\[\min_{w\in\overline{V}}\frac{1}{q}\int_{Q}|w(y,u,s)|^{q}\,\mathrm{d}\gamma(y, u,s)-L(w)\] (84)

which admits the unique solution \(w\) such that \(j_{q}(w)-L=0\). In particular, the estimate (81) shows that the above functional is coercive and hence admits a minimizer which we may obtain via its differential as a consequence of convexity. Thus, we obtain a triangular vector field \(v=j_{q}(w)\) such that for all \(\varphi\in\text{Cyl}(Q)\),

\[\langle v,\nabla\varphi\rangle=\int_{Q}\langle v(y,u,s),\nabla \varphi(y,u,s)\rangle\,\mathrm{d}\gamma(y,u,s)=\langle L,\nabla\varphi\rangle =-\int_{Q}\partial_{s}\varphi(y,u,s)\,\mathrm{d}\gamma(y,u,s).\] (85)

This precisely shows that \((v_{t},\gamma_{t})\) is a triangular distributional solution to the continuity equation.

Now, choose any interval \(J\subset I\) and choose a sequence \(\eta^{k}\in C^{\infty}_{c}(J)\), with \(0\leq\eta^{k}\leq 1\) and \(\eta_{k}\to\mathbbm{1}_{J}\) as \(k\to\infty\). Moreover choose a sequence \((\nabla_{u}\varphi_{n})\subset V\) converging to \(w=j_{p}(v)\) in \(L^{q}(\gamma,Q)\). Our previous calculations give

\[\int_{Q}\eta^{k}(s)|v(y,u,s)|^{p}\,\mathrm{d}\gamma(y,u,s)=\int_{ Q}\eta^{k}(s)\langle v,w\rangle\,\mathrm{d}\gamma=\lim_{n\to\infty}\int_{Q} \eta^{k}\langle v,\nabla_{u}\varphi_{n}\rangle\,\mathrm{d}\gamma\] (86) \[=\lim_{n\to\infty}\langle L,\nabla_{u}(\eta^{k}\varphi_{n}) \rangle\leq\left(\int_{J}|\gamma^{\prime}|^{p}(s)\,\mathrm{d}s\right)^{1/p} \left(\int_{J\times Y\times U}|v|^{p}\,\mathrm{d}\gamma\right)^{1/p}.\] (87)

Taking \(k\to\infty\) we see that

\[\int_{J}\int_{Y\times U}|v_{t}(y,u)|^{p}\,\mathrm{d}\gamma_{t}(y,u)\,\mathrm{ d}t\leq\int_{J}|\gamma^{\prime}|^{p}(s)\,\mathrm{d}s\] (88)

and since \(J\subset I\) was arbitrary, we conclude

\[\|v_{t}\|_{L^{p}(\gamma_{t},Y\times U)}\leq|\gamma^{\prime}|(t)\qquad\text{ a.e.-}t.\] (89)

We now prove, in some sense, a converse of the previous theorem.

**Theorem 4** (Continuous Curves Generated by Triangular Vector Fields)

_Suppose that \(\gamma_{t}:I\to\mathbb{P}^{\mu}_{p}(Y\times U)\) is narrowly continuous and \((v_{t})\) is a triangular vector field such that \((\gamma_{t},v_{t})\) solve the continuity equation with \(\|v_{t}\|_{L^{p}(\gamma_{t},Y\times U)}\in L^{1}(I)\). Then, \(\gamma_{t}:I\to\mathbb{P}^{\mu}_{p}(Y\times U)\) is absolutely continuous in the \(W^{\mu}_{p}\) metric and \(|\gamma^{\prime}|(t)\leq\|v_{t}\|_{L^{p}(\mu,Y\times U)}\) for almost every \(t\)._Proof.: We first assume that \(U\) is finite dimensional. Our strategy is to check the hypotheses necessary for Ambrosio et al. (2005, Theorem 8.3.1) to hold for \(\mu\)-almost every \(y\), followed by an application of this theorem. By Lemma 1, for \(\mu\)-almost every \(y\) we have that \((\gamma_{t}^{y},v_{t}^{U}(y,-))\) solve the continuity equation distributionally on \(I\times U\).

By Jensen's inequality (and the assumption \(p\geq 1\)) we see

\[\int_{I}\|v_{t}\|_{L^{p}(\gamma_{t},Y\times U)}\;\mathrm{d}t =\int_{I}\mathbb{E}_{y\sim\mu}\left[\left\|v_{t}^{U}(y,-)\right\| _{L^{p}(\gamma_{t}^{y},U)}^{p}\right]^{1/p}\,\mathrm{d}t\] (90) \[\geq\int_{I}\mathbb{E}_{y\sim\mu}\left[\left\|v_{t}^{U}(y,-) \right\|_{L^{p}(\gamma_{t}^{y},U)}\right]\,\mathrm{d}t\] (91) \[=\mathbb{E}_{y\sim\mu}\left[\int_{I}\left\|v_{t}^{U}(y,-)\right\| _{L^{p}(\gamma_{t}^{y},U)}\,\mathrm{d}t\right].\] (92)

Since the first term is finite, it follows that

\[\left\|v_{t}^{U}(y,-)\right\|_{L^{p}(\gamma_{t}^{y},U)}\in L^{1}(I)\qquad \text{$\mu$-almost every $y$}.\] (93)

Now Ambrosio et al. (2005, Lemma 8.1.2) shows that for \(\mu\)-almost every \(y\) we have that \((\gamma_{t}^{y})\) admits a narrowly continuous representative \((\tilde{\gamma}_{t}^{y})\) with \(\tilde{\gamma}_{t}^{y}=\gamma_{t}^{y}\) for almost every \(t\). It follows from Ambrosio et al. (2005, Theorem 8.3.1) that for any \(t_{1}\leq t_{2}\) in \(I\), we have

\[W_{p}^{p}(\tilde{\gamma}_{t_{1}}^{y},\tilde{\gamma}_{t_{2}}^{y}) \leq(t_{2}-t_{1})^{p-1}\int_{t_{1}}^{t_{2}}|v_{t}^{U}(y,u)|^{p}\, \mathrm{d}\tilde{\gamma}_{t}^{y}(u)\,\mathrm{d}t\] (94) \[=(t_{2}-t_{1})^{p-1}\int_{t_{1}}^{t_{2}}|v_{t}^{U}(y,u)|^{p}\, \mathrm{d}\gamma_{t}^{y}(u)\,\mathrm{d}t\] (95)

where the second line follows as \(\tilde{\gamma}_{t}^{y}=\gamma_{t}^{y}\) for almost every \(t\).

Let \(\tilde{\gamma}_{t}=\int_{Y}\tilde{\gamma}_{t}^{y}\,\mathrm{d}\mu(y)\) be the measure obtained via marginalizing over the \(Y\)-variables. Taking an expectation over \(y\sim\mu\), the previous inequality shows us that

\[\frac{W_{p}^{\mu,p}(\tilde{\gamma}_{t_{1}},\tilde{\gamma}_{t_{2}})}{(t_{2}-t _{1})^{p}}\leq\frac{1}{t_{2}-t_{1}}\int_{t_{1}}^{t_{2}}\left\|v_{t}\right\|_{L ^{p}(\gamma_{t},Y\times U)}^{p}\,\mathrm{d}t.\] (96)

Now, note that \(t_{1}\) is almost surely a Lebesgue point of the right-hand side and \(\tilde{\gamma}_{t_{1}}=\gamma_{t_{1}}\). Taking \(t_{2}\to t_{1}\) along a sequence where \(\tilde{\gamma}_{t_{2}}=\gamma_{t_{2}}\) shows us that

\[|\gamma^{\prime}|(t)\leq\left\|v_{t}\right\|_{L^{p}(\gamma_{t}),Y\times U}\] (97)

for almost every \(t\in I\).

In the case that \(U\) is infinite dimensional, fix any \(y\in Y\) such that Lemma 1 holds (which is of full measure) and fix a countable orthonormal basis \((e_{k})\) for \(U\). Set \(\pi^{d}:U\to\mathbb{R}^{d}\) to be the projection operator for this basis, i.e. \(u\mapsto(\langle u,e_{1}\rangle,\ldots,\langle u,e_{d}\rangle)\). We consider the collection of finite dimensional conditional measures \(\gamma_{t}^{d,y}=\pi_{\#}^{d}\gamma_{t}^{y}\). By the same argument in Ambrosio et al. (2005, Theorem 8.3.1), there exists a vector field \(v_{t}^{d,y}\) on \(\mathbb{R}^{d}\) such that \((\gamma_{t}^{d,y},v_{t}^{d,y})\) solve the continuity equation and

\[\left\|v_{t}^{d,y}\right\|_{L^{p}(\gamma_{t}^{d,y},\mathbb{R}^{d})}\leq\left\| v_{t}^{U}(y,-)\right\|_{L^{p}(\gamma_{t}^{y},U)}.\] (98)

It follows from the finite-dimensional case above that for almost every \(t_{1}\leq t_{2}\), we have

\[W_{p}^{p}(\gamma_{t_{1}}^{d,y},\gamma_{t_{2}}^{d,y})\leq(t_{2}-t_{1})^{p-1} \int_{t_{1}}^{t_{2}}\left\|v_{t}^{U}(y,-)\right\|_{L^{p}(\gamma_{t}^{y},U)}^{p} \,\mathrm{d}t.\] (99)

Let \(\hat{\gamma}_{t}^{y,d}=(\pi^{d})_{\#}^{\star}\gamma_{t}^{y,d}\) where \((\pi^{d})^{\star}:\mathbb{R}^{d}\to U\) maps \(z\mapsto\sum_{k=1}^{d}z_{k}e_{k}\). As \(d\to\infty\) we have \(\hat{\gamma}_{t}^{d,y}\to\gamma_{t}^{y}\) narrowly for all \(t\in I\). Since \((\pi^{d})^{\star}\) is an isometry, Ambrosio et al. (2005, Lemma 7.1.4) shows that

\[W_{p}^{p}(\gamma_{t_{1}}^{y},\gamma_{t_{2}}^{y})\leq\liminf_{d\to\infty}W_{p}^{ p}(\gamma_{t_{1}}^{d,y},\gamma_{t_{2}}^{d,y})\leq(t_{2}-t_{1})^{p-1}\int_{t_{1}}^{t_{2}} \left\|v_{t}^{U}(y,-)\right\|_{L^{p}(\gamma_{t}^{y},U)}^{p}\,\mathrm{d}t.\] (100)Now, integration with respect to \(\,\mathrm{d}\mu(y)\) yields

\[W_{p}^{p,\mu}(\gamma_{t_{1}},\gamma_{t_{2}})\leq(t_{2}-t_{1})^{p-1}\int_{t_{1}}^ {t_{2}}\|v_{t}\|_{L^{p}(\gamma_{t},Y\times U)}^{p}\,\,\mathrm{d}t.\] (101)

Taking \(t_{2}\to t_{1}\) shows that for almost every \(t\) we have

\[|\gamma^{\prime}|(t)\leq\|v_{t}\|_{L^{p}(\gamma_{t},Y\times U)}\,.\] (102)

Together, Theorem 3 and Theorem 4 give us a dynamical interpretation of the conditional Wasserstein distance. The following result is a conditional analogue of the well-known Benamou-Brenier Theorem (Benamou and Brenier, 2000). Here, we note that the following proof follows the standard proof closely - the main legwork in obtaining this conditional generalization is through the previous two theorems.

**Theorem 5** (Conditional Benamou-Brenier): _Let \(1<p<\infty\). For any \(\eta,\nu\in\mathbb{P}_{p}^{\mu}(Y\times U)\), we have_

\[W_{p}^{p,\mu}(\eta,\nu)=\min_{(\gamma_{t},v_{t})}\left\{\int_{0}^{1}\|v_{t}\| _{L^{p}(\mu_{t})}^{p}\,\,\mathrm{d}t\,|\,(v_{t},\gamma_{t})\text{ solve (\ref{eq:1}), }\gamma_{0}=\eta,\gamma_{1}=\nu\text{, and }v_{t}\text{ is triangular}\right\}.\]

Proof.: Write \(M\) for the infimum on the right-hand side.

First, suppose that \((v_{t},\mu_{t})\) are admissible and \(\int_{0}^{1}\|v_{t}\|_{L^{p}(\mu_{t})}<\infty\). It follows from Theorem 4 that \((\gamma_{t})\) is an absolutely continuous curve in \(\mathbb{P}_{p}^{\mu}(Y\times U)\) and \(\|v_{t}\|_{L^{p}(\mu_{t},Y\times U)}\geq|\gamma^{\prime}|(t)\). Thus,

\[W_{p}^{\mu,p}(\eta,\nu)\leq\left(\int_{0}^{1}|\gamma^{\prime}|(t)\,\mathrm{d}t \right)^{p}\leq\int_{0}^{1}\|v_{t}\|_{L^{p}(\mu_{t},Y\times U)}^{p}\,\,\mathrm{ d}t\leq M.\] (103)

Conversely, by Theorem 1 there exists a constant speed geodesic \((\gamma_{t})\subset\mathbb{P}_{p}^{\mu}(Y\times U)\) connecting \(\eta\) and \(\nu\). Recall that constant speed geodesics are absolutely continuous. By Theorem 3, there exists a Borel triangular vector field \(v_{t}\) such that \((v_{t},\gamma_{t})\) solve the continuity equation, and moreover \(\|v_{t}\|_{L^{p}(\mu_{t},Y\times U)}\leq|\gamma^{\prime}|(t)\). In fact, because \((v_{t},\gamma_{t})\) solve the continuity equation, Theorem 4 yields that \(\|v_{t}\|_{L^{p}(\mu_{t},Y\times U)}=|\gamma^{\prime}|(t)\).

Since \(\gamma_{t}\) is a constant speed geodesic in \(\mathbb{P}_{p}^{\mu}(Y\times U)\), it follows that \(|\mu^{\prime}|(t)=W_{p}^{\mu}(\eta,\nu)\) for almost every \(t\in(0,1)\). Hence,

\[W_{p}^{\mu,p}(\eta,\nu)=\int_{0}^{1}|\gamma^{\prime}|(t)^{p}\,\mathrm{d}t= \int_{0}^{1}\|v_{t}\|_{L^{p}(\gamma_{t},Y\times U)}^{p}\geq M.\] (104)

Thus, \(W_{p}^{\mu,p}(\eta,\nu)=M\) as desired. 

## Appendix F Experiment Details

In this section, we provide additional details regarding all of our experiments, as well as additional results not contained within the main paper. All models can be trained on a single GPU with less than 24 GB of memory, and our experiments were parallelized over 8 such GPUs on a local server. We first describe our setting for the 2D and Lotka-Volterra experiments, as these share a similar setup. Details for the Darcy flow inverse problem are described in the corresponding section.

Models.For FM and COT-FM, our model architecture is an MLP with SeLU activations (Klambauer et al., 2017). Time conditioning is achieved by concatenating the time variable as an input to the network. The covariance operator \(C\) chosen in the path of measures in Equation (10) is taken to be \(C=\sigma^{2}I\) where \(\sigma\) is a hyperparameter.

Our implementation of FM is adapted from the torchcfm package Tong et al. (2023), available under the MIT License. For PCP-Map and COT-Flow, we adapt the open-source implementations from Wang et al. (2023), available under the MIT License.

Training and Model Selection.Hyperparameter tuning of the PCP-Map and COT-Flow models was performed directly using the code of Wang et al. (2023), essentially implementing grid-search with an early stopping procedure. We refer to the paper and codebase of Wang et al. (2023) for further details. For COT-FM and FM, we perform a random grid search over 100 hyperparameter settings using the grid described in Table 4. For all model types, we select the best model used to generate the results in the paper as the training checkpoint that resulted in the lowest \(W_{2}\) error to the joint target distribution on a held-out validation set. For training, we use the Adam optimizer where we only tune the learning rate, leaving all other settings as their defaults in pytorch.

### 2D Synthetic Data

Data Generation.This experiment consists of four 2D synthetic datasets, where \(Y=U=\mathbb{R}\). The datasets moons, circles, swissroll are available through scikit-learnPedregosa et al. (2011). The moons dataset is generated with noise\(=\)\(0.05\) followed by standard scaling with a mean of \(m=(0.5,0.25)\) and standard deviation of \(\sigma=(0.75,0.25)\). The circles dataset is generated with factor=0.5 and noise\(=\)0.05. The swissroll dataset is generated with noise\(=\)0.75, followed by projection to the first two coordinates and re-scaling by a factor of \(12\). All other unstated parameters are left as their default values. We use the code available from Hosseini et al. (2023) to generate the checkerboard dataset. For all datasets, we generate a training set (i.e., samples from the target distribution) of \(20,000\) samples and \(1,000\) held-out validation samples for model selection. Means and standard deviations in Table 1 are reported across five independent testing sets of 5,000 samples for the best representative of each model type.

In COT-FM, to generate samples from the source distribution, we sample an additional \(20,000\) points from the target distribution and keep only the \(Y\) coordinates. This ensures that the source and target have equal \(Y\) marginals. During training, standard Gaussian noise \(\mathcal{N}(0,1)\) is sampled for the \(U\) coordinate of these source points at each minibatch.

We use minibatch COT couplingsTong et al. (2023) in this experiment as computing the full COT plan was prohibitively expensive in terms of memory usage. However, we note that we use large batch sizes, meaning that the COT plan we find in this way should not be too far from optimal. All couplings are computed using the POT Python package (Flamary et al., 2021).

### Lotka-Volterra Dynamical System

Data Generation.We adopt the settings of Alfonso et al. (2023) for this experiment. As described in the main paper, we assume \(p(0)=(30,1)\) and that \(\log(u)\sim\mathcal{N}(m,0.5I)\) with \(m=(-0.125,-3,-0.125,-3)\). Given parameters \(u\in\mathbb{R}^{4}_{\geq 0}\), we simulate Equation (13) for \(t\in\{0,2,\dots,20\}\) to obtain a solution \(z(u)\in\mathbb{R}^{22}_{\geq 0}\). An observation \(y\in\mathbb{R}^{22}_{\geq 0}\) is obtained by the addition of log-normal noise, i.e. \(\log(y)\sim\mathcal{N}(\log(z(u),0.1I)\). We thus may simulate many \((y,u)\) pairs from the target measure for training.

We generate a training set of \(10,000\)\((y,u)\) pairs using the procedure described above and a held-out validation set of \(10,000\)\((y,u)\) pairs for model selection. Means and standard deviations in Table 2 are reported across five independent testing sets of 5,000 samples for the best representative of each model type. Figure 2 and Figures 9, 8, 7, 10 show \(10,000\) samples from each model, as well as \(10,000\) samples from the differential evolution Metropolis MCMC sampler (Braak, 2006) after a

\begin{table}
\begin{tabular}{l l|c} \hline \hline Hyperparameter & Description & Values \\ \hline \(\epsilon\) & COT coupling strength & [1e-6, 1e-4, 1e-2, 1e-1] \\ \(\sigma\) & Variance for \(C=\sigma^{2}I\) in (10) & [1e-3, 1e-2, 1e-1, 5e-1] \\ Batch Size & Training batch size & [256, 512, 1024] \\ Width & Layer width in MLP & [256, 512, 1024, 2048] \\ LR & Learning rate & [1e-4, 3e-4, 7e-4, 1e-3] \\ Layers & Number of MLP layers & [4, 6, 8] \\ \hline \hline \end{tabular}
\end{table}
Table 4: Hyperparameter grid used for random search of the FM and COT-FM models on the 2D and Lotka-Volterra datasets.

burn-in of \(50,000\) samples. This is implemented through the PyMC Python package (Abril-Pla et al., 2023).

For COT-FM we use the full COT couplings, i.e. without minibatches. This is available to use due to the smaller size of the training set used in this experiment. The COT couplings are computed in the same way as the previous section, and as described in Section 7.

Figure 5: Samples from the ground-truth joint target distribution and the various models for the 2D datasets. Samples from COT-FM more closely match the ground-truth distribution than the baselines. A common failure mode for the baselines is to generate samples from regions with zero support under the true data distributions. Table 1 contains a quantitative evaluation.

Figure 6: Conditional KDEs shown for each of the methods on the 2D datasets. The conditioning variable \(y\) is fixed at the horizontal dashed line shown in Figure 5. In all plots, the orange solid line indicates the CKDE of the ground-truth joint samples. In each column, the dashed blue line indicates the CKDE of samples generated from the respective method.

Figure 7: KDE plots of the samples on the Lotka-Volterra system, using the settings described in Section 7. Plots include one-dimensional KDEs on the diagonal, as well as all two-dimensional pairs. In all plots, samples from MCMC are drawn in orange, and samples from our method (COT-FM) are indicated in blue. The true unknown parameters are indicated by the red vertical line in the diagonal plots, or the black x in the off-diagonal plots.

Figure 8: KDE plots of the samples on the Lotka-Volterra system, using the settings described in Section 7. Plots include one-dimensional KDEs on the diagonal, as well as all two-dimensional pairs. In all plots, samples from MCMC are drawn in orange, and samples from PCP-Map are indicated in blue. The true unknown parameters are indicated by the red vertical line in the diagonal plots, or the black x in the off-diagonal plots.

Figure 9: KDE plots of the samples on the Lotka-Volterra system, using the settings described in Section 7. Plots include one-dimensional KDEs on the diagonal, as well as all two-dimensional pairs. In all plots, samples from MCMC are drawn in orange, and samples from COT-Flow are indicated in blue. The true unknown parameters are indicated by the red vertical line in the diagonal plots, or the black x in the off-diagonal plots.

Figure 10: KDE plots of the samples on the Lotka-Volterra system, using the settings described in Section 7. Plots include one-dimensional KDEs on the diagonal, as well as all two-dimensional pairs. In all plots, samples from MCMC are drawn in orange, and samples from flow matching (FM) are indicated in blue. The true unknown parameters are indicated by the red vertical line in the diagonal plots, or the black x in the off-diagonal plots.

### Inverse Darcy Flow

Dataset.The training and test datasets are generated following the same procedure as Hosseini et al. (2023): pressure fields \(u\) are sampled from a Gaussian process with Matern kernel having \(\nu=3/2\) and lengthscale \(\ell=1/2\), on a regular \(40\times 40\) grid. The parameters are then exponentiated and used to simulate the permeability fields \(p\) from the forward model \(\mathfrak{F}\) solving the Darcy flow PDE, using FEniCS (Alnaes et al., 2015). Stochasticity arises from adding Gaussian noise to the permeability fields, obtaining \(y=\mathfrak{F}(u)+\epsilon\), \(\epsilon\sim\mathcal{N}(0,\sigma^{2}I)\). For our experiments we observe \(y\) on a \(100\times 100\) grid, and we use \(\sigma=2.5\times 10^{-2}\). We note that this level of noise is quite considerable, as it accounts for roughly \(60\%\) of the variability in the \(y\). Figure 11 showcases a data point for reference. Our source and target training sets contain \(1\times 10^{4}\) samples each, and our test set comprises \(5\times 10^{3}\) samples. We remark that although \(y\) and \(u\) are observed on a grid their resolution does not need to be fixed, allowing for training at different resolutions.

Models.In order to make learning feasible in infinite-dimensional Hilbert spaces, we adapt the architecture of a Fourier Neural Operator (FNO) (Li et al., 2020) from the neuraloperator package (Kovachki et al., 2021) to accommodate for conditioning information observed at an arbitrary resolution. We do so by introducing a projection layer mapping the conditioning information to match the hidden channels of the input lifting block, and a pooling operation to project to the input dimensions. The two are then concatenated and passed through an FNOBlock mapping from \((2\times\texttt{hidden\_channels})\times\texttt{input\_dim}\) to hidden_channels\(\times\texttt{input\_dim}\), before following the original architecture. For all of the models in consideration, we fix the architecture to be have hidden_channels\(=64\), \(\texttt{projection\_channels}=256\), and \(32\) Fourier modes. We train each model for 1500 epochs, and hyperparameters for each architecture are selected as follows:

* WaMGAN (Hosseini et al., 2023): using an adaptation to the FNO architecture of the original code3, we perform a grid search as detailed in Table 5. We found the training procedure to be rather unstable, and for this reason we checkpoint the model every 100 epochs and report the results for the best performing model at its best checkpoint. We found this to be a model with learning rate \(1\times 10^{-4}\), 2 full critic iterations, and monotone penalty of \(1\times 10^{-3}\). The gradient penalty parameter did not seem to significantly affect performance on the test set, and was set to 5. FFM (Kerrigan et al., 2024): the learning rate is fixed to \(5\times 10^{-4}\), and the covariance operator \(C\) is set to match that of the prior, but rescaled by a factor of \(\sigma=1\times 10^{-3}\). We use the code from the original repository4. Footnote 3: https://github.com/TADSGroup/ConditionalOT2023
* COT-FFM: we set \(\epsilon=1\times 10^{-5}\) in the cost function used to build the COT plan. The learning rate and \(C\) are chosen to be the same as FFM. In order to build COT couplings, we take the source measure to be the product measure \(\pi_{\#}^{Y}\eta\times\mathcal{N}(0,C)\). Approximate couplings are obtained on minibatches of size \(256\).

It should be noted that in any scenario where the source and the target \(U-\)marginals are identical, using the OT coupling would yield the identity mapping as the optimal vector field minimizing (12). Hence, the OT-CFM model (Tong et al., 2023) is inapplicable here.

Figure 11: Example of one random data point from the Darcy flow dataset.

Sampling.The resulting amortized sampler, denoted for simplification by the mapping \((y,u_{0})\mapsto u_{1}=\tilde{T}_{U}(y,u_{0})\), will parameterize an approximate posterior measure. Notice that, in contrast to classical variational inference techniques, no distributional assumptions are made about the approximate posterior. In turn, integrals are obtained numerically by Monte Carlo sampling \(K\) samples from the prior, resulting in the approximation

\[\nu^{y}(f)\approx\int f\,\mathrm{d}\delta_{\tilde{T}_{U}(y,u_{0})}\,\mathrm{d} \mathcal{N}(0,C)\approx\frac{1}{K}\sum_{k=1}^{K}f(\tilde{T}_{U}(y_{k},u_{0,k}) ),\quad\{u_{0,k}\}_{k=1}^{K}\stackrel{{\text{i.i.d.}}}{{\sim}} \mathcal{N}(0,C).\] (105)

## Appendix G Minibatch COT

In this section, we perform an additional experiment demonstrating that our COT-FM is able to obtain good approximations of the true COT map even when trained on relatively small minibatches.

We work on \(Y=U=\mathbb{R}\) and use a standard Gaussian source and a Gaussian target distribution with covariance \(\rho=0.75\). These are chosen so that we may evaluate, in closed-form, the true conditional \(2\)-Wasserstein distance, as detailed in Section C. We then train our COT-FM method using various batch sizes, and measure the resulting model's conditional \(2\)-Wasserstein distance by sampling \(10,000\) points from the source distribution, flowing each source point along the model's learned vector field, and computing the resulting squared distance to the corresponding terminal point.

In Figure 12, we plot the resulting deviation from the true value of \(W_{2}^{\mu,2}\) as a function of batch size. We see that even at a relatively small batch size of \(16\), the resulting error in the (squared) distances is less than 10% of the true value (\(\approx 0.678\)). While this experiment is only feasible on synthetic data (as we must compute the true distance), it nonetheless demonstrates that even with small batches we may recover the true distance.

Figure 12: Error in the squared transportation distance as a function of batch size. Values closer to zero indicate a better approximation of the COT cost. For fairly small batch sizes (\(>16\)) the magnitude of the error is stable and relatively small. Shading indicates one standard deviation computed over ten random evaluation sets.

\begin{table}
\begin{tabular}{c|c} \hline \hline Parameter & Search Space \\ \hline Learning rate & \(\{1\times 10^{-3},\,5\times 10^{-4},\,1\times 10^{-4}\}\) \\ Full critic iter. & \(\{\)2, 5, 10\(\}\) \\ Monotone penalty & \(\{1\times 10^{-3},\,5\times 10^{-2},\,1\times 10^{-1}\}\) \\ Gradient penalty & \(\{\)1, 5, 10\(\}\) \\ \hline \hline \end{tabular}
\end{table}
Table 5: Hyperparameter search space for WaMGAN

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The claims and contributions of our work are clearly stated in the introduction and abstract. In addition, the introduction clearly points to the sections in the paper where the claims are discussed in more detail. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: The limitations of the work are discussed in Section 8. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?Answer: [Yes] Justification: All formal claims in the paper have their assumptions clearly stated. Proofs of all theoretical claims are available in the Appendix. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: Our experimental settings are clearly described in Section 7 and further details, including hyperparameter choices, are provided in Appendix F. We will release our code upon publication of the paper. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.

5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: We will release our code as open-source upon acceptance of the paper for publication. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: These details are provided in Appendix F. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: Error bars are provided for all quantitative results. Explicit descriptions of how these are calculated are included as well. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.

* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: These details are provided in Appendix F. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We have reviewed the ethics guidelines and conform to all requirements. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: The potential societal impacts are discussed in Section 8. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.

* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: We believe that our paper does not pose any such risks. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licensees for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: Any assets used are properly cited throughout the paper. We provide addiitonal details in Appendix F. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.

* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: Our paper does not release new assets. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: Our paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: Our paper does not involve crowdsourcing nor research with Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.