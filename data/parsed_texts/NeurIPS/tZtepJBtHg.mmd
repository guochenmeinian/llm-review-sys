# Transductive Active Learning:

Theory and Applications

 Jonas Hubotter

Department of Computer Science

ETH Zurich, Switzerland

&Bhavya Sukhija

Department of Computer Science

ETH Zurich, Switzerland

&Lenart Treven

Department of Computer Science

ETH Zurich, Switzerland

&Yarden As

Department of Computer Science

ETH Zurich, Switzerland

&Andreas Krause

Department of Computer Science

ETH Zurich, Switzerland

Correspondence to jonas.huebotter@inf.ethz.ch

###### Abstract

We study a generalization of classical active learning to real-world settings with concrete prediction targets where sampling is restricted to an accessible region of the domain, while prediction targets may lie outside this region. We analyze a family of decision rules that sample adaptively to minimize uncertainty about prediction targets. We are the first to show, under general regularity assumptions, that such decision rules converge uniformly to the smallest possible uncertainty obtainable from the accessible data. We demonstrate their strong sample efficiency in two key applications: active fine-tuning of large neural networks and safe Bayesian optimization, where they achieve state-of-the-art performance.

## 1 Introduction

Machine learning, at its core, is about designing systems that can extract knowledge or patterns from data. One part of this challenge is determining not just how to learn given observed data but deciding what data to obtain next, given the information already available. More formally, given an unknown and sufficiently regular function \(f\) over a domain \(\mathcal{X}\): _How can we learn \(f\) sample-efficiently from (noisy) observations?_ This problem is widely studied in _active learning_ and _experimental design_(Chaloner & Verdinelli, 1995; Settles, 2009).

Active learning methods commonly aim to learn \(f\) globally, i.e., across the entire domain \(\mathcal{X}\). However, in many real-world problems, **(i)** the domain is so large that learning \(f\) globally is hopeless or **(ii)** agents have limited information and cannot access the entire domain (e.g., due to restricted access or to act safely). Thus, global learning is often not desirable or even possible. Instead, intelligent systems are typically required to act in a more _directed_ manner and _extrapolate_ beyond their limited information. This work formalizes the above two aspects of active learning, which have remained largely unaddressed by prior work. We provide a comprehensive overview of related work in Section 6.

"Directed" transductive active learningWe consider the generalized problem of _transductive active learning_, where given two arbitrary subsets of the domain \(\mathcal{X}\); a _target space_\(\mathcal{A}\subseteq\mathcal{X}\), and a _sample space_\(\mathcal{S}\subseteq\mathcal{X}\), we study the question:

_How can we learn \(f\) within \(\mathcal{A}\) by actively sampling observations within \(\mathcal{S}\)?_This problem is ubiquitous in real-world applications such as safe Bayesian optimization, where \(\mathcal{S}\) is a set of safe parameters and \(\mathcal{A}\) might represent parameters outside \(\mathcal{S}\) whose safety we want to infer. Active fine-tuning of neural networks is another example, where the target space \(\mathcal{A}\) represents the test set over which we want to minimize risk, and the sample space \(\mathcal{S}\) represents the dataset from which we can retrieve data points to fine-tune our model to \(\mathcal{A}\). Figure 1 visualizes some instances of transductive active learning.

Whereas most prior work has focused on the "global" inductive instance \(\mathcal{X}=\mathcal{A}=\mathcal{S}\), MacKay (1992) was the first to consider specific target spaces \(\mathcal{A}\) and proposed the principle of selecting points in \(\mathcal{S}\) to minimize the "posterior uncertainty" about points in \(\mathcal{A}\). Since then, several works have studied this principle empirically (e.g., Seo et al., 2000; Yu et al., 2006; Bogunovic et al., 2016; Wang et al., 2021; Kothawade et al., 2021; Bickford Smith et al., 2023). In this work, we model \(f\) as a Gaussian process or (equivalently) as a function in a reproducing kernel Hilbert space, for which the above principle is analytically and computationally tractable. Our contributions are:

* **Theory (Section 3):** We are the first to give rates for the uniform convergence of uncertainty over the target space \(\mathcal{A}\) to the smallest attainable value, given samples from the sample space \(\mathcal{S}\) (Theorems 3.2 and 3.3), Our results provide a theoretical justification for the principle of minimizing posterior uncertainty in transductive active learning, and indicate that transductive active learning can be more sample efficient than inductive active learning.
* **Applications:** We show that transductive active learning improves upon the state-of-the-art in the batch-wise _active fine-tuning_ of neural networks for image classification (Section 4) and in _safe Bayesian optimization_ (Section 5).

## 2 Problem Setting

We assume for now that the target space \(\mathcal{A}\) and sample space \(\mathcal{S}\) are finite, and relax these assumptions in the appendices. We model \(f\) as a stochastic process and denote the marginal random variables \(f(\bm{x})\) by \(f_{\bm{x}}\), and joint random vectors \(\{f_{\bm{x}}\}_{\bm{x}\in X}\) for some \(X\subseteq\mathcal{X},|X|<\infty\) by \(\bm{f}_{X}\). Let \(\bm{y}_{X}\) denote the noisy observations of \(\bm{f}_{\mathcal{X}}\), \(\{y_{\bm{x}}=f_{\bm{x}}+\varepsilon_{\bm{x}}\}_{\bm{x}\in X}\), where \(\varepsilon_{\bm{x}}\) is independent noise.2 We study the "adaptive" setting, where in round \(n\) the agent selects a point \(\bm{x}_{n}\in\mathcal{S}\) and observes \(y_{n}=y_{\bm{x}_{n}}\). The agent's choice of \(\bm{x}_{n}\) may depend on the outcome of prior observations \(\mathcal{D}_{n-1}\stackrel{{\mathrm{def}}}{{=}}\{(\bm{x}_{i},y_{i })\}_{i<n}\).

Footnote 2: \(X\) may be a multiset in which case repeated occurrence of \(\bm{x}\) corresponds to independent observations of \(y_{\bm{x}}\).

Background on information theoryWe briefly recap several important concepts from information theory of which we provide formal definitions in Appendix B. The (differential) entropy \(\mathrm{H}[\bm{f}]\) is one possible measure of uncertainty about \(\bm{f}\) and the conditional entropy \(\mathrm{H}[\bm{f}\mid\bm{y}]\) is the (expected) posterior uncertainty about \(\bm{f}\) after observing \(\bm{y}\). The information gain \(\mathrm{I}(\bm{f};\bm{y})=\mathrm{H}[\bm{f}]-\mathrm{H}[\bm{f}\mid\bm{y}]\) measures the (expected) reduction in uncertainty about \(\bm{f}\) due to \(\bm{y}\). We denote the information gain about \(\mathcal{A}\) from observing \(X\) by \(\mathrm{I}(\bm{f}_{\mathcal{A}};\bm{y}_{X})\). The maximum information gain about \(\mathcal{A}\) from \(n\) observations within \(\mathcal{S}\) is

\[\gamma_{\mathcal{A},\mathcal{S}}(n)\stackrel{{\mathrm{def}}}{{=}} \max_{\begin{subarray}{c}X\subset\mathcal{S}\\ |X|\leq n\end{subarray}}\mathrm{I}(\bm{f}_{\mathcal{A}};\bm{y}_{X}).\]

This "information capacity" measures the information about \(\bm{f}_{\mathcal{A}}\) that is accessible from within \(\mathcal{S}\), and has been used previously (e.g., by Srinivas et al., 2009; Chowdhury and Gopalan, 2017; Vakili et al., 2021) in the setting where \(\mathcal{X}=\mathcal{A}=\mathcal{S}\), taking the form of \(\gamma_{n}\stackrel{{\mathrm{def}}}{{=}}\gamma_{\mathcal{X}}(n) \stackrel{{\mathrm{def}}}{{=}}\gamma_{\mathcal{X},\mathcal{X}}(n)\). We remark that \(\gamma_{\mathcal{A},\mathcal{S}}(n)\leq\gamma_{\mathcal{S}}(n)\) holds uniformly for all \(\mathcal{A}\), \(\mathcal{S}\), and \(n\) due to the data processing inequality. Generally, \(\gamma_{\mathcal{A},\mathcal{S}}(n)\) can be substantially smaller if the target space is a sparse subset of the sample space.

Figure 1: Instances of transductive active learning with target space \(\mathcal{A}\) shown in blue and sample space \(\mathcal{S}\) shown in gray. The points denote plausible observations within \(\mathcal{S}\) to “learn” \(\mathcal{A}\). In **(A)**, the target space contains “everything” within \(\mathcal{S}\) as well as points _outside_\(\mathcal{S}\). In **(B, C, D)**, one makes observations _directed_ towards learning about a particular target. Prior work on inductive active learning has focused on the instance \(\mathcal{A}=\mathcal{S}\).

Main Results

We analyze the following principle for transductive active learning:

_Select samples to minimize the posterior "uncertainty" about \(f\) within \(\mathcal{A}\)._ (\(\dagger\))

This principle yields a family of simple and natural decision rules which depend on the chosen measure of "uncertainty". Two natural measures of uncertainty are (1) the entropy of prediction targets, \(\mathrm{H}[\bm{f_{\mathcal{A}}}]\), and (2) their total variance, \(\sum_{\bm{x}^{\prime}\in\mathcal{A}}\mathrm{Var}[f_{\bm{x}^{\prime}}]\). The corresponding decision rules are

\[\text{(1)}\] \[\text{(2)}\] \[\text{(VTL)}\]

with an implicit expectation over the feedback \(y_{\bm{x}}\). That is, \(\mathrm{ITL}\) (short for _Information-based Transductive Learning_) and \(\mathrm{VTL}\) (_Variance-based TL_) select \(\bm{x}_{n}\) so as to minimize the uncertainty about the prediction targets \(\bm{f_{\mathcal{A}}}\) (in expectation) after having received the feedback \(y_{n}\). Unlike \(\mathrm{VTL}\), \(\mathrm{ITL}\) takes into account the mutual dependence between points in \(\mathcal{A}\). These decision rules were suggested previously (MacKay, 1992; Seo et al., 2000; Yu et al., 2006) without deriving theoretical guarantees; and they generalize several widely used algorithms which we discuss in more detail in Section 6. Most prominently, in the inductive setting where \(\mathcal{S}\subseteq\mathcal{A}\), \(\mathrm{ITL}\) reduces to \(\bm{x}_{n}=\arg\max_{\bm{x}\in\mathcal{S}}\mathrm{I}(f_{\bm{x}};y_{\bm{x}}\mid \mathcal{D}_{n-1})\), i.e., is "undirected" and reduces to standard uncertainty-based active learning strategies (cf. Appendix C.1). The convergence properties for the special instance of \(\mathrm{ITL}\) with \(\mathcal{S}=\mathcal{A}\) have been studied extensively. To the best of our knowledge, we are the first to extend these guarantees to the more general setting of transductive active learning.

In our presented results, we make the following assumption.

**Assumption 3.1**.: In the case of \(\mathrm{ITL}\), the information gain \(\psi_{\mathcal{A}}(X)=\mathrm{I}(\bm{f_{\mathcal{A}}};\bm{y_{X}})\) is submodular. In the case of \(\mathrm{VTL}\), the variance reduction \(\psi_{\mathcal{A}}(X)=\mathrm{tr}\ \mathrm{Var}[\bm{f_{\mathcal{A}}}]- \mathrm{tr}\ \mathrm{Var}[\bm{f_{\mathcal{A}}}\mid\bm{y_{X}}]\) is submodular.

Under this assumption, \(\psi_{\mathcal{A}}(\bm{x}_{1:n})\) is a constant factor approximation of \(\max_{X\subseteq\mathcal{S},|X|\leq n}\psi_{\mathcal{A}}(X)\) due to the seminal result on submodular function maximization by Nemhauser et al. (1978). Similar assumptions have been made, e.g., by Bogunovic et al. (2016) and Kothawade et al. (2021). Assumption 3.1 is satisfied exactly for \(\mathrm{ITL}\) when \(\mathcal{S}\subseteq\mathcal{A}\) and \(f\) is a Gaussian process (cf. Lemma C.9), and we provide an extensive discussion of our results in Appendix C.4 for instances where Assumption 3.1 is satisfied approximately, relying on the notion of weak submodularity (Das and Kempe, 2018).

### Gaussian Process Setting

When \(f\sim\mathcal{GP}(\mu,k)\) is a Gaussian process (GP, Williams and Rasmussen, 2006) with known mean function \(\mu\) and kernel \(k\), and the noise \(\varepsilon_{\bm{x}}\) is mutually independent and zero-mean Gaussian with known variance, the \(\mathrm{ITL}\) and \(\mathrm{VTL}\) objectives have a closed form expression (cf. Appendix F) and can be optimized efficiently. Further, the information capacity \(\gamma_{n}\) is sublinear in \(n\) for a rich class of GPs (Srinivas et al., 2009; Vakili et al., 2021), with rates summarized in Table 3 of the appendix.

**Convergence to irreducible uncertainty** So far, our discussion was centered around the role of the target space \(\mathcal{A}\) in facilitating _directed_ learning. An orthogonal contribution of this work is to study _extrapolation_ from the sample space \(\mathcal{S}\) to points \(\bm{x}\in\mathcal{A}\setminus\mathcal{S}\). To this end, we derive bounds on the marginal posterior variance \(\sigma_{n}^{2}(\bm{x})\stackrel{{\mathrm{def}}}{{=}}\mathrm{Var}[f _{\bm{x}}\mid\mathcal{D}_{n}]\) for points in \(\mathcal{A}\). These bounds depend on the instance of transductive active learning (i.e., \(\mathcal{A}\) and \(\mathcal{S}\)) and might be of independent interest for active learning. For \(\mathrm{ITL}\) and \(\mathrm{VTL}\), they imply uniform convergence of the variance for a rich class of GPs. To the best of our knowledge, this work is the first to present such bounds.

We define the _irreducible uncertainty_ as the variance of \(f(\bm{x})\) provided complete knowledge of \(f\) in \(\mathcal{S}\):

\[\eta_{\mathcal{S}}^{2}(\bm{x})\stackrel{{\mathrm{def}}}{{=}} \mathrm{Var}[f_{\bm{x}}\mid\bm{f_{\mathcal{S}}}].\]

As the name suggests, \(\eta_{\mathcal{S}}^{2}(\bm{x})\) represents the smallest uncertainty one can hope to achieve from observing only within \(\mathcal{S}\). For all \(\bm{x}\in\mathcal{S}\), it is easy to see that \(\eta_{\mathcal{S}}^{2}(\bm{x})=0\). However, the irreducible uncertainty of \(\bm{x}\not\in\mathcal{S}\) may be (and typically is!) strictly positive.

**Theorem 3.2** (Bound on marginal variance for \(\mathrm{ITL}\) and \(\mathrm{VTL}\)).: _Let Assumption 3.1 hold and the data be selected by either \(\mathrm{ITL}\) or \(\mathrm{VTL}\).. Assume that \(f\sim\mathcal{GP}(\mu,k)\) with known mean function \(\mu\)_and kernel \(k\), the noise \(\varepsilon_{\bm{x}}\) is mutually independent and zero-mean Gaussian with known variance, and \(\gamma_{n}\) is sublinear in \(n\). Then there exists a constant \(C\) such that for any \(n\geq 1\) and \(\bm{x}\in\mathcal{A}\),_

\[\sigma_{n}^{2}(\bm{x})\leq\underbrace{\eta_{\mathcal{S}}^{2}(\bm{x})}_{\text{ irreducible}}+\underbrace{C\frac{\gamma_{\mathcal{A},\mathcal{S}}(n)}{\sqrt{n}}}_{\text{ reducible}}.\] (1)

_Moreover, if \(\bm{x}\in\mathcal{A}\cap\mathcal{S}\), there exists a constant \(C^{\prime}\) such that_

\[\sigma_{n}^{2}(\bm{x})\leq C^{\prime}\frac{\gamma_{\mathcal{A},\mathcal{S}}( n)}{n}.\] (2)

Intuitively, Equation (1) of Theorem 3.2 can be understood as bounding an epistemic "generalization gap" (Wainwright, 2019) of the learner. The reducible uncertainty converges to zero at all prediction targets \(\bm{x}\in\mathcal{A}\), e.g., for linear, Gaussian, and smooth Matern kernels. As to be expected, a smaller target space (i.e., more targeted sampling) leads to faster convergence due to a smaller information capacity \(\gamma_{\mathcal{A},\mathcal{S}}(n)\ll\gamma_{n}\). Equation (2) matches prior results for the setting \(\mathcal{S}=\mathcal{A}\). We provide a formal proof of Theorem 3.2 in Appendix C.6.

### Agnostic Setting

The result from the GP setting translates also to the agnostic setting, where the "ground truth" \(f^{\star}\) may be any sufficiently regular fixed function on \(\mathcal{X}\).3 In this case, we use the model \(f\) from Section 3.1 as a (misspecified) model of \(f^{\star}\), with some kernel \(k\) and zero mean function \(\mu(\cdot)=0\). We denote by \(\mu_{n}(\bm{x})=\mathbb{E}[f(\bm{x})\mid\mathcal{D}_{n}]\) the posterior mean of \(f\). W.l.o.g. we assume in the following result that the prior variance is bounded, i.e., \(\operatorname{Var}[f(\bm{x})]\leq 1\).

Footnote 3: Here \(f^{\star}(\bm{x})\) denotes the mean observation \(y_{\bm{x}}=f^{\star}(\bm{x})+\epsilon_{\bm{x}}\)

**Theorem 3.3** (Bound on approximation error for ITL and VTL, following Abbasi-Yadkori (2013); Chowdhury and Gopalan (2017)).: _Let Assumption 3.1 hold and the data be selected by either ITL or VTL. Pick any \(\delta\in(0,1)\). Assume that \(f^{\star}\) lies in the reproducing kernel Hilbert space \(\mathcal{H}_{k}(\mathcal{X})\) of the kernel \(k\) with norm \(\left\lVert f^{\star}\right\rVert_{k}<\infty\), the noise \(\varepsilon_{n}\) is conditionally \(\rho\)-sub-Gaussian, and \(\gamma_{n}\) is sublinear in \(n\). Let \(\beta_{n}(\delta)=\left\lVert f^{\star}\right\rVert_{k}+\rho\sqrt{2(\gamma_{n }+1+\log(1/\delta))}\). Then for any \(n\geq 1\) and \(\bm{x}\in\mathcal{A}\), jointly with probability at least \(1-\delta\),_

\[\left\lvert f^{\star}(\bm{x})-\mu_{n}(\bm{x})\right\rvert\leq\beta_{n}(\delta) \Big{[}\underbrace{\eta_{\mathcal{S}}(\bm{x})}_{\text{irreducible}}+ \underbrace{\nu_{\mathcal{A},\mathcal{S}}(n)}_{\text{reducible}}\Big{]}\]

_where \(\nu_{\mathcal{A},\mathcal{S}}^{2}(n)\) denotes the reducible part of Equation (1)._

We provide a formal proof of Theorem 3.3 in Appendix C.7. Theorem 3.3 generalizes approximation error bounds of prior works to the extrapolation setting, where some prediction targets \(\bm{x}\in\mathcal{A}\) lie outside the sample space \(\mathcal{S}\). For prediction targets \(\bm{x}\in\mathcal{A}\cap\mathcal{S}\), the irreducible uncertainty vanishes, and we recover previous results from the setting \(\mathcal{S}=\mathcal{A}\).

Theorems 3.2 and 3.3 show that ITL and VTL efficiently learn \(f\) at the prediction targets \(\mathcal{A}\) for large classes of "sufficiently regular" functions \(f\). In the following, we validate these results experimentally by showing that ITL and VTL exhibit strong empirical performance in a broad range of applications.

### Experiments in the Gaussian Process Setting

Before demonstrating ITL and VTL on GPs to develop more intuition, we introduce a natural correlation-based baseline, which will later uncover connections to existing approaches:

\[\bm{x}_{n}=\operatorname*{arg\,max}_{\bm{x}\in\mathcal{S}}\sum_{\bm{x}^{ \prime}\in\mathcal{A}}\operatorname{Cor}[f_{\bm{x}},f_{\bm{x}^{\prime}}\mid \mathcal{D}_{n-1}].\] (CTL)

How does the smoothness of \(f\) affect ITL?We contrast two "extreme" kernels: the _Gaussian kernel_\(k(\bm{x},\bm{x}^{\prime})=\exp(-\left\lVert\bm{x}-\bm{x}^{\prime}\right\rVert _{2}^{2}/2)\) and the _Laplace kernel_\(k(\bm{x},\bm{x}^{\prime})=\exp(-\left\lVert\bm{x}-\bm{x}^{\prime}\right\rVert _{1})\). In the mean-squared sense, the Gaussian kernel yields a smooth process \(f\) whereas the Laplace kernel yields a continuous but non-differentiable \(f\)(Williams and Rasmussen, 2006). Figure 2 showshow ITL adapts to the smoothness of \(f\): Under the "smooth" Gaussian kernel, points outside \(\mathcal{A}\) provide higher-order information. In contrast, under the "rough" Laplace kernel and if \(\mathcal{A}\subseteq\mathcal{S}\), points outside \(\mathcal{A}\) do not provide any additional information, and therefore are not sampled by ITL. If, however, \(\mathcal{A}\not\subseteq\mathcal{S}\), information "leaks" \(\mathcal{A}\) even under a Laplace kernel prior. That is, even for non-smooth functions, the point with most information need not be in \(\mathcal{A}\).

Does ITL outperform uncertainty sampling?Uncertainty sampling (UnSa, Lewis and Catlett, 1994) is one of the most popular active learning methods. UnSa selects points \(\bm{x}\) with high _prior_ uncertainty: \(\bm{x}_{n}=\operatorname*{arg\,max}_{\bm{x}\in\mathcal{S}}\sigma_{n-1}^{2}( \bm{x})\). This is in stark contrast to ITL and VTL which select points \(\bm{x}\) that minimize _posterior_ (epistemic) uncertainty about \(\mathcal{A}\). It can be seen that UnSA is the special "undirected" case of ITL when \(\mathcal{S}\subseteq\mathcal{A}\) and observation noise is homoscedastic (cf. Appendix C.1).

We compare UnSa to ITL, VTL, and CTL in Figure 3. We observe that ITL and VTL outperform UnSa which also samples points that are not informative about \(\mathcal{A}\). Further, ITL and VTL outperform "local" UnSa (i.e., UnSa constrained to \(\mathcal{A}\cap\mathcal{S}\)) which neglects all information provided by points outside \(\mathcal{A}\).4 As one would expect, VTL has an advantage with respect to reducing the total variance of \(\bm{f}_{\mathcal{A}}\), whereas ITL reduces the entropy of \(\bm{f}_{\mathcal{A}}\) faster. We include ablations in Appendix H where we, in particular, observe that the advantage of ITL and VTL over UnSA increases as the volume of prediction targets shrinks in comparison to the size of domain.

Footnote 4: If \(\mathcal{A}\not\subseteq\mathcal{S}\) then “local” UnSa does _not even_ converge to the irreducible uncertainty.

## 4 Active Fine-Tuning of Neural Networks

Fine-tuning a large pre-trained model is a cost- and computation-effective approach to improve performance on a given target domain (Lee et al., 2022). While previous work has studied the effectiveness of various training procedures for fine-tuning (e.g., Eustratiadis et al., 2024), we ask: _How can we select the right data for fine-tuning to a specific task?_ This _active_ fine-tuning problem is an instance of the introduced "directed" transductive learning problem: Concretely, consider a supervised setting, where the function \(f\) maps inputs \(\bm{x}\in\mathcal{X}\) to outputs \(y\in\mathcal{Y}\). We have access

Figure 3: Entropy of \(\bm{f}_{\mathcal{A}}\) ranging from \(-3850\) to \(-3725\) and the mean marginal standard deviations of \(\bm{f}_{\mathcal{A}}\) ranging from \(0\) to \(0.15\). Experiment is using the Gaussian kernel of the left instance (\(\mathcal{A}\subset\mathcal{S}\)) from Figure 2. It can be seen that ITL and VTL outperform UnSa and Random. Uncertainty bands correspond to one standard error over \(10\) random seeds.

Figure 2: Initial \(25\) samples of ITL under a Gaussian kernel with lengthscale \(1\) (left) and a Laplace kernel with lengthscale \(10\) (right). Shown in gray is the sample space \(\mathcal{S}\) and shown in blue is the target space \(\mathcal{A}\). In three of the four examples, points outside the target space provide useful information.

to noisy samples from a training set \(\mathcal{S}\) on \(\mathcal{X}\), and we would like to learn \(f\) such that our estimate minimizes a given risk measure, such as classification error, with respect to a test distribution \(\mathcal{P}_{\mathcal{A}}\) on \(\mathcal{X}\). The goal is to actively and efficiently sample from \(\mathcal{S}\) to minimize risk with respect to \(\mathcal{P}_{\mathcal{A}}\).5 We show in this section that ITL and VTL can learn \(f\) from only _few examples_ from \(\mathcal{S}\).

Footnote 5: The setting with target distributions \(\mathcal{P}_{\mathcal{A}}\) can be reduced to considering target sets \(\mathcal{A}\) (cf. Appendix E).

How can we leverage the latent structure learned by the pre-trained model?As common in related work, we approximate the (pre-trained) neural network (NN) \(f(\cdot;\bm{\theta})\) as a linear function in a latent embedding space, \(f(\bm{x};\bm{\theta})\approx\bm{\beta}^{\top}\bm{\phi}_{\bm{\theta}}(\bm{x})\), with weights \(\bm{\beta}\in\mathbb{R}^{p}\) and embeddings \(\bm{\phi}_{\bm{\theta}}:\mathcal{X}\rightarrow\mathbb{R}^{p}\). Common choices of embeddings include last-layer embeddings (Devlin et al., 2019; Holzmuller et al., 2023), neural tangent embeddings arising from neural tangent kernels (Jaco et al., 2018) which are motivated by their relationship to the training and fine-tuning of ultra-wide NNs (Arora et al., 2019; Lee et al., 2019; Khan et al., 2019; He et al., 2020; Malladi et al., 2023), and loss gradient embeddings (Ash et al., 2020). We provide a comprehensive overview of embeddings in Appendix J.2. Now, supposing the prior \(\bm{\beta}\sim\mathcal{N}(\bm{0},\bm{\Sigma})\), often with \(\bm{\Sigma}=\bm{I}\)(Khan et al., 2019; He et al., 2020; Antoran et al., 2022; Wei et al., 2022), this approximation of \(f\) is a Gaussian process with kernel \(k(\bm{x},\bm{x^{\prime}})=\bm{\phi}_{\bm{\theta}}(\bm{x})^{\top}\bm{\Sigma} \bm{\phi}_{\bm{\theta}}(\bm{x^{\prime}})\) which quantifies the similarity between points in terms of their alignment in the learned latent space. Note that the correlation \(k(\bm{x},\bm{x^{\prime}})/\sqrt{k(\bm{x},\bm{x})k(\bm{x^{\prime}},\bm{x^{ \prime}})}\) between two points \(\bm{x},\bm{x^{\prime}}\) is equal to the cosine similarity of their embeddings.

In this context, Theorem 3.2 bounds the epistemic posterior uncertainty about a prediction using the approximation \(\bm{\beta}^{\top}\bm{\phi}_{\bm{\theta}}(\bm{x})\), given that the model is trained using data selected by ITL or VTL. Theorem 3.3 bounds the generalization error when using the posterior mean of \(\bm{\beta}\) for prediction. This extends recent work which has studied estimators of this generalization error (Wei et al., 2022).

Batch selection: Diversity via conditional embeddingsEfficient labeling and training necessitates a batch-wise selection of inputs. The selection of a batch of size \(b>1\) can be seen as an individual _non-adaptive_ active learning problem, and significant recent work has shown that batch diversity is crucial in this setting (Ash et al., 2020; Zanette et al., 2021; Holzmuller et al., 2023; Pacchiano et al., 2024). An information-based batch-wise selection strategy is formalized by the following non-adaptive transductive active learning problem (Chen and Krause, 2013) and the greedy approximation of \(B_{n}\) by ITL which selects elements \(\bm{x}_{n,i}\) of the \(n\)-th batch iteratively based on \(\bm{x}_{n,1:i-1}\):

\[B_{n}=\operatorname*{arg\,max}_{B\subseteq\mathcal{S},|B|=b}\mathrm{I}(\bm{f} _{\mathcal{A}};\bm{y}_{B}\mid\mathcal{D}_{n-1});\qquad\bm{x}_{n,i}=\operatorname* {arg\,max}_{\bm{x}\in\mathcal{S}}\mathrm{I}(\bm{f}_{\mathcal{A}};\bm{y}_{\bm{x }}\mid\mathcal{D}_{n-1},\bm{y}_{\bm{x}_{n,1:i-1}}).\] (3)

The batch \(B_{n}\) is diverse and informative by design. We show that under Assumption 3.1, \(B_{n}^{\prime}=\bm{x}_{n,1:b}\) yields a constant-factor approximation of \(B_{n}\) (cf. Appendix C.3).

### Experiments on Active Fine-Tuning

Our empirical evaluation is motivated by the following practical example: We deploy a pre-trained image classifier to user's phones who use it within their local environment. We would like to locally fine-tune a user's model to their environment. Since the users' images \(\mathcal{A}\) are unlabeled, this requires selecting a small number of relevant and diverse images from the set of labeled images \(\mathcal{S}\). As such, we will focus here on the setting where the points in our test set do not lie in our training set (i.e., \(\mathcal{A}\cap\mathcal{S}=\emptyset\)), and discuss alternative instances such as active domain adaptation in Appendix I.

Testbeds & architecturesWe use the MNIST (LeCun et al., 1998) and CIFAR-100 (Krizhevsky et al., 2009) datasets as testbeds. In both cases, we take \(\mathcal{S}\) to be the training set, and we consider the task of learning the digits \(3\), \(6\), and \(9\) (MNIST) or the first \(10\) categories of CIFAR-100.6 For MNIST, we train a simple convolutional neural network with ReLU activations, three convolutional layers with max-pooling, and two fully-connected layers. For CIFAR-100, we fine-tune an EfficientNet-B0 (Tan and Le, 2019) pre-trained on ImageNet (Deng et al., 2009), augmented by a final fully-connected layer. We train the NNs using the cross-entropy loss and the ADAM optimizer (Kingma and Ba, 2014).

Footnote 6: That is, we restrict \(\mathcal{P}_{\mathcal{A}}\) to the support of points with labels \(\{3,6,9\}\) (MNIST) or labels \(\{0,\dots,9\}\) (CIFAR-100) and train a neural network using few examples drawn from the training set \(\mathcal{S}\).

ResultsIn Figure 4, We compare against **(i)** active learning methods which largely aim for sample diversity but which are not directed towards the target distribution \(\mathcal{P}_{\mathcal{A}}\) (e.g., BADGE; Ash et al., 2020), and **(ii)** search methods that aim to retrieve the most relevant samples from \(\mathcal{S}\) with respect to the targets \(\mathcal{P}_{\mathcal{A}}\) (e.g., maximizing cosine similarity to target embeddings as is common in vector databases;Settles & Craven, 2008; Johnson et al., 2019). InformationDensity (ID, Settles & Craven, 2008) is a heuristic approach aiming to combine **(i)** diversity and **(ii)** relevance. In Appendix J.5, we also compare against a wide range of additional baselines (e.g., CoreSet(Sener & Savarese, 2017), TypiClust(Hacohen et al., 2022), ProbCover(Yehuda et al., 2022), etc.) that fall into one of the categories **(i)** and **(ii)**, and which perform similar to the baselines listed here.

We observe that ITL, VTL, and CTL consistently and significantly outperform random sampling from \(\mathcal{S}\) as well as all baselines. We see that relevance-based methods such as CosineSimilarity have an initial advantage over Random but for batch sizes larger than \(1\) they quickly fall behind due to diminishing informativeness of the selected data. In contrast, diversity-based methods such as BADGE are more competitive with Random but do not explicitly aim to retrieve relevant samples.

Remarkably, transductive active learning outperforms random data selection even in the MNIST experiment where the model is randomly initialized. This suggests that the learned embeddings can be informative for data selection even in the early stages of training, bootstrapping the learning progress.

Balancing sample relevance and diversityOur proposed methods unify approaches to coverage (promoting _diverse_ samples) and search (aiming for _relevant_ samples with respect to a given query \(\mathcal{A}\)) which leads to the significant improvement upon the state-of-the-art in Figure 4. Notably, for a batch size and query size of \(1\) and if correlations are non-negative, ITL, VTL, CTL, and the canonical cosine similarity are equivalent. CTL can be seen as a direct generalization of cosine similarity-based retrieval to batch and query sizes larger than one. In contrast to CTL, ITL and VTL may also sample points which exhibit a strong negative correlation (which is also informative).

We observe empirically that ITL obtains samples from \(\mathcal{P}_{\mathcal{A}}\) at more than twice the rate of CosineSimilarity, which translates to a significant improvement in accuracy in more difficult learning tasks, while requiring fewer (labeled) samples from \(\mathcal{S}\). This phenomenon manifests for both MNIST and CIFAR-100, as well as imbalanced datasets \(\mathcal{S}\) or imbalanced reference samples from \(\mathcal{P}_{\mathcal{A}}\) (cf. Appendix J.6). The improvement in accuracy appears to increase in the large-data regime, where the learning tasks become more difficult. Akin to a previously identified scaling trend with size of the pre-training dataset (Tamkin et al., 2022), this suggests a potential scaling trend where the improvement of ITL over random batch selection grows as models are fine-tuned on a larger pool of data.

Figure 4: Active fine-tuning on MNIST (left) and CIFAR-100 (right). Random selects each observation uniformly at random from \(\mathcal{S}\). The batch size is \(1\) for MNIST and \(10\) for CIFAR-100. Uncertainty bands correspond to one standard error over \(10\) random seeds. We see that transductive active learning with ITL and VTL significantly outperforms competing methods, and in particular, retrieves substantially more samples from the support of \(\mathcal{P}_{\mathcal{A}}\). See Appendix J for details and ablations.

Towards task-driven few-shot learningBeing able to efficiently and automatically select data may allow dynamic few-shot fine-tuning to individual tasks (Vinyals et al., 2016; Hardt and Sun, 2024), e.g., fine-tuning the model to each test point / query / prompt. Such task-driven few-shot learning can be seen as a form of "memory recall" akin to associative memory (Hopfield, 1982). Our results are a first indication that task-driven learning can lead to substantial performance gains, and we believe that this is a promising direction for future studies.

## 5 Safe Bayesian Optimization

Another practical problem that can be cast as "directed" learning is safe Bayesian optimization (Safe BO, Sui et al., 2015; Berkenkamp et al., 2021) which has applications in natural science (Cooper and Netoff, 2022) and robotics (Wischnewski et al., 2019; Sukhija et al., 2023; Widmer et al., 2023). Safe BO solves the following optimization problem

\[\max_{\bm{x}\in\mathcal{S}^{\star}}f^{\star}(\bm{x})\quad\text{ where}\quad\mathcal{S}^{\star}=\{\bm{x}\in\mathcal{X}\mid g^{\star}(\bm{x})\geq 0\}\] (4)

which can be generalized to multiple constraints. The functions \(f^{\star}\) and \(g^{\star}\), and hence also the "safe set" \(\mathcal{S}^{\star}\), are unknown and have to be actively learned from data. However, it is crucial that the data collection does not violate the constraint, i.e., \(\bm{x}_{n}\in\mathcal{S}^{\star},\forall n\geq 1\).

Safe Bayesian optimization as Transductive Active LearningIn the agnostic setting from Section 3.2, GPs \(f\) and \(g\) can be used as well-calibrated models of the ground truths \(f^{\star}\) and \(g^{\star}\), and we denote lower- and upper-confidence bounds by \(l_{n}^{f}(\underline{x}),l_{n}^{g}(\bm{x})\) and \(u_{n}^{f}(\bm{x}),u_{n}^{g}(\bm{x})\), respectively. These confidence bounds induce a _pessimistic_ safe set \(\mathcal{S}_{n}=\{\bm{x}\mid l_{n}^{g}(\bm{x})\geq 0\}\) and an _optimistic_ safe set \(\widehat{\mathcal{S}}_{n}=\{\bm{x}\mid u_{n}^{g}(\bm{x})\geq 0\}\) which satisfy \(\mathcal{S}_{n}\subseteq\mathcal{S}^{\star}\subseteq\widehat{\mathcal{S}}_{n}\) with high probability at all times. Similarly, the set of _potential maximizers_

\[\mathcal{A}_{n}\overset{\mathrm{def}}{=}\{\bm{x}\in\widehat{\mathcal{S}}_{n} \mid u_{n}^{f}(\bm{x})\geq\max_{\bm{x}^{\prime}\in\mathcal{S}_{n}}l_{n}^{f}( \bm{x}^{\prime})\}\] (5)

contains the solution to Equation (4) at all times with high probability.

The (simple) regret \(r_{n}(\mathcal{S})\overset{\mathrm{def}}{=}\max_{\bm{x}\in\mathcal{S}}f^{\star }(\bm{x})-f^{\star}(\widehat{\bm{x}}_{n})\) with \(\widehat{\bm{x}}_{n}\overset{\mathrm{def}}{=}\operatorname*{arg\,max}_{\bm{x} \in\mathcal{S}_{n}}l_{n}^{f}(\bm{x})\) measures the worst-case performance of a decision rule. To achieve small regret, one faces an _exploration-expansion_ dilemma wherein one needs to explore points that are known-to-be-safe, i.e., lie in the estimated safe set \(\mathcal{S}_{n}\), and might be optimal, while at the same time discovering new safe points by "expanding" \(\mathcal{S}_{n}\). Accordingly, a natural choice for the target space of Safe BO is \(\mathcal{A}_{n}\) since it captures both exploration and expansion _simultaneously_.7 To prevent constraint violation, the sample space is restricted to the pessimistic safe set \(\mathcal{S}_{n}\). In Safe BO, both the target space and sample space change with each round \(n\), and we generalize our theoretical results from Section 3 in Appendix C to this setting.

Footnote 7: An alternative possibility is to weigh each point in \(\mathcal{A}_{n}\) according to how likely it is to be the safe optimum. Which approach performs better is task-dependent, and we include a detailed discussion in Appendix K.1.

**Theorem 5.1** (Convergence to safe optimum).: _Pick any \(\epsilon>0\), \(\delta\in(0,1)\). Assume that \(f^{\star}\), \(g^{\star}\) lie in the reproducing kernel Hilbert space \(\mathcal{H}_{k}(\mathcal{X})\) of the kernel \(k\), and that the noise \(\varepsilon_{n}\) is conditionally \(\rho\)-sub-Gaussian. Then, we have with probability at least \(1-\delta\),_

\[\text{Safety: for all }n\geq 1\text{,}\quad\bm{x}_{n}\in\mathcal{S}^{\star}.\]

_Moreover, assume \(\mathcal{S}_{0}\neq\emptyset\) and denote with \(\mathcal{R}\) the largest reachable safe set starting from \(\mathcal{S}_{0}\). Then, the convergence of reducible uncertainty implies that there exists \(n^{\star}>0\) such that with probability at least \(1-\delta\),_

\[\text{Optimality: for all }n\geq n^{\star}\text{,}\quad r_{n}(\mathcal{R})\leq\epsilon.\]

We provide a formal proof in Appendix C.8. Central to the proof is the application of Theorem 3.3 to show that the safety of parameters _outside_ the safe set \(\mathcal{S}_{n}\) can be inferred efficiently. In Section 3, we outline settings where the reducible uncertainty converges which is the case for a very general class of functions, and for such instances Theorem 5.1 guarantees optimality in the largest reachable safe set \(\mathcal{R}\). \(\mathcal{R}\) represents the largest set any safe learning algorithm can explore without violating the safety constraints (with high probability) during learning (cf. Definition C.29). Our guarantees are similar to those of other Safe BO algorithms (Berkenkamp et al., 2021) but require fewer assumptions and generalize to continuous domains. We obtain Theorem 5.1 from a more general result (Theorem C.34) which can be specialized to yield "free" novel convergence guarantees for problems other than Bayesian optimization, such as level set estimation, by choosing an appropriate target space.

### Experiments on Safe Bayesian Optimization

We evaluate two synthetic experiments for a 1d and 2d parameter space, respectively (cf. Appendix K.4 for details), which demonstrate the various shortcomings of existing Safe BO baselines. Additionally, as third experiment, we safely tune the controller of a quadcopter.

Safe controller tuning for a quadcopterWe consider a quadcopter with unknown dynamics; \(\bm{s}_{t+1}=\bm{T}(\bm{s}_{t},\bm{u}_{t})\) where \(\bm{u}_{t}\in\mathbb{R}^{d_{u}}\) is the control signal and \(\bm{s}_{t}\in\mathbb{R}^{d_{s}}\) is the state at time \(t\). The inputs \(\bm{u}_{t}\) are calculated through a deterministic function of the state \(\bm{\pi}:\mathcal{S}\to\mathcal{U}\) which we call the policy. The policy is parameterized via parameters \(\bm{x}\in\mathcal{X}\), e.g., PID controller gains, such that \(\bm{u}_{t}=\bm{\pi}_{\bm{x}}(\bm{s}_{t})\). The goal is to find the optimal parameters with respect to an unknown objective \(f^{\star}\) while satisfying some unknown constraint(s) \(g^{\star}(\bm{x})\geq 0\), e.g., the quadcopter does not fall on the ground. This is a typical Safe BO problem which is widely applied for safe controller learning in robotics (Berkenkamp et al., 2021; Baumann et al., 2021; Widmer et al., 2023).

ResultsWe compare ITL and VTL to SafeOpt(Berkenkamp et al., 2021), which is undirected, i.e., expands in all directions including ones that are known-to-be suboptimal, and ISE (Bottero et al., 2022), which is solely expansionist -- does not trade-off expansion-exploration. We provide a detailed discussion of baselines in Appendix K.2. In all our experiments, summarized in Figure 5, we observe that ITL and VTL systematically perform well, i.e., better or on par with the state-of-the-art. We attribute this to its directed exploration and less conservative expansion over Safeopt (cf. 1d task and quadcopter experiment), and natural trade-off between expansion and exploration as opposed to ISE (see 2d task). Generally, VTL has a slight advantage over ITL, which is because VTL minimizes marginal variances (as opposed to entropy), which are decisive for expanding the safe set. While ITL and VTL do not violate constraints, we observe that other methods that do not explicitly enforce safety such as EIC (Gardner et al., 2014) lead to constraint violation (cf. Appendix K.4.2).

## 6 Related Work

(Inductive) active learningThe special case of transductive active learning where \(\mathcal{A}=\mathcal{S}=\mathcal{X}\) has been widely studied. We refer to this special instance as _inductive_ active learning, since the goal is to extract as much information as possible as opposed to making predictions on a specific target set.

Several works have previously found entropy-based decision rules to be useful for inductive active learning (Krause and Guestrin, 2007; Guo and Greiner, 2007; Krause et al., 2008) and semi-supervised learning (Grandvalet and Bengio, 2004). The variance-based VTL has previously been proposed by Cohn (1993) in the special case of inductive active learning without proving theoretical guarantees. VTL was then recently re-derived by Shoham and Avron (2023) along other experimental design

Figure 5: We compare ITL and VTL to Oracle SafeOpt, which has oracle knowledge of the Lipschitz constants, SafeOpt, where the Lipschitz constants are estimated from the GP, as well as Heuristic SafeOpt and ISE, and observe that ITL and VTL systematically perform well. We compare against additional baselines in Appendix K.1. The regret is evaluated with respect to the ground truth objective \(f^{\star}\) and constraint \(g^{\star}\), and averaged over 10 (in synthetic experiments) and 25 (in the quadcopter experiment) random seeds. Additional details can be found in Appendix K.4.

criteria under the lens of minimizing risk for inductive one-shot learning in overparameterized models. Substantial work on active learning has studied entropy-based criteria in _parameter-space_, most notably BALD (MacKay, 1992; Houlsby et al., 2011; Gal et al., 2017; Kirsch et al., 2019), which selects \(\bm{x}_{n}=\arg\max_{\bm{x}\in\mathcal{X}}\bm{I}(\bm{\theta};\bm{y}_{\bm{x}}\mid \mathcal{D}_{n-1})\), where \(\bm{\theta}\) is the random parameter vector of a parametric model (e.g., obtained via Bayesian deep learning). Such methods are inherently inductive in the sense that they do not facilitate learning on specific prediction targets.

Transductive active learningIn contrast, ITL operates in _output-space_ where it is straightforward to specify prediction targets, and which is computationally easier. Special cases of ITL when \(\mathcal{S}=\mathcal{X}\) and \(|\mathcal{A}|=1\) have been proposed in the foundational work of MacKay (1992) on "directed" output-space active learning. As generalization to larger target spaces, MacKay (1992) proposed mean-marginal ITL,

\[\bm{x}_{n}=\operatorname*{arg\,max}_{\bm{x}\in\mathcal{S}}\sum_{\bm{x}^{ \prime}\in\mathcal{A}}\mathrm{I}(f_{\bm{x}^{\prime}};y_{\bm{x}}\mid\mathcal{D }_{n-1})\,,\] (MM-ITL)

for which we derive analogous versions of Theorems 3.2 and 3.3 in Appendix D.3. We note that similarly to VTL, MM-ITL disregards the mutual dependence of points in the target space \(\mathcal{A}\) and differs from VTL only in a different weighting of the posterior marginal variances of the prediction targets (cf. Appendix D.3). Recently, Bickford Smith et al. (2023) generalized MM-ITL by treating the prediction target as a random variable, and Kothawade et al. (2021) and Bickford Smith et al. (2024) demonstrated the use of output-space decision rules for image classification tasks in a pre-training context.

Influence functionsmeasure the change in a model's prediction when a single data point is removed from the training data (Cook, 1977; Koh and Liang, 2017; Pruthi et al., 2019). Influence functions have been used for data selection in settings closely related to the transductive active fine-tuning of neural networks proposed in this work (Xia et al., 2024). They select data that reduces a first-order Taylor approximation to the test loss after fine-tuning a neural network, which corresponds to maximizing cosine similarity to the prediction targets in a loss-gradient embedding space. We show in our experiments that transductive active learning can substantially outperform CosineSimilarity. We attribute this primarily to influence functions implicitly assuming that the influence of selected data adds linearly (i.e., two equally scored data points are expected to doubly improve the model performance, Xu and Kazantsev, 2019, Section 3.2). This assumption does not hold in practice as seen, e.g., by simply duplicating data. The same limitation applies to the related approach of datamodels (Ilyas et al., 2022).

Other work on directed active learningDirected active learning methods have been proposed for the problem of determining the optimum of an unknown function, also known as best-arm identification (Audibert et al., 2010) or pure exploration bandits (Bubeck et al., 2009). Entropy search methods (Hennig and Schuler, 2012; Hernandez-Lobato et al., 2014) are widely used and select \(\bm{x}_{n}=\arg\max_{\bm{x}\in\mathcal{X}}\bm{I}(\bm{x}^{*};y_{\bm{x}}\mid \mathcal{D}_{n-1})\) in _input-space_ where \(\bm{x}^{*}=\arg\max_{\bm{x}}f_{\bm{x}}\). Similarly to ITL, _output-space_ entropy search methods (Hoffman and Ghahramani, 2015; Wang and Jegelka, 2017), which select \(\bm{x}_{n}=\arg\max_{\bm{x}\in\mathcal{X}}\mathrm{I}(f^{*};y_{\bm{x}}\mid \mathcal{D}_{n-1})\) with \(f^{*}=\max_{\bm{x}}f_{\bm{x}}\), are more computationally tractable. In fact, output-space entropy search is a special case of ITL with a stochastic target space (cf. Equation (47) in Appendix K.1). Bogunovic et al. (2016) analyze TruVar in the context of Bayesian optimization and level set estimation. TruVar is akin to VTL with a similar notion of "target space", but their algorithm and analysis rely on a threshold scheme which requires that \(\mathcal{A}\subseteq\mathcal{S}\). Fiez et al. (2019) introduce the _transductive linear bandit_ problem, which is a special case of transductive active learning limited to a linear function class and with the objective of determining the maximum within an initial candidate set.8 We mention additional more loosely related works in Appendix A.

Footnote 8: The transductive bandit problem can be solved analogously to Safe BO, by maintaining the set \(\mathcal{A}_{n}\).

## 7 Conclusion

We investigated the generalization of active learning to settings with concrete prediction targets and/or with limited information due to constrained sample spaces. This provides a flexible framework, applicable also to other domains than were discussed (such as recommender systems, molecular design, robotics, etc.) by varying the choice of target space and sample space. Further, we proved novel generalization bounds which may be of independent interest for active learning. Finally, we demonstrated across broad applications that sampling _relevant and diverse_ points (as opposed to only one of the two) leads to a substantial improvement upon the state-of-the-art.

## Acknowledgements

Many thanks to Armin Lederer, Johannes Kirschner, Jonas Rothfuss, Lars Lorch, Manish Prajapat, Nicolas Emmenegger, Parnian Kassraie, and Scott Sussex for their insightful feedback on different versions of this manuscript, as well as Anton Baumann for helpful discussions. We further thank Freddie Bickford Smith for a constructive discussion regarding the relationship between our work and prior work.

This project was supported in part by the European Research Council (ERC) under the European Union's Horizon 2020 research and Innovation Program Grant agreement no. 815943, the Swiss National Science Foundation under NCCR Automation, grant agreement 51NF40 180545, and by a grant of the Hasler foundation (grant no. 21039).

## References

* Abbasi-Yadkori (2013) Abbasi-Yadkori, Y. _Online learning for linearly parametrized control problems_. PhD thesis, University of Alberta, 2013.
* Antoran et al. (2022) Antoran, J., Janz, D., Allingham, J. U., Daxberger, E., Barbano, R. R., Nalisnick, E., and Hernandez-Lobato, J. M. Adapting the linearised laplace model evidence for modern deep learning. In _ICML_, 2022.
* Arora et al. (2019) Arora, S., Du, S. S., Hu, W., Li, Z., Salakhutdinov, R. R., and Wang, R. On exact computation with an infinitely wide neural net. _NeurIPS_, 32, 2019.
* Arthur et al. (2007) Arthur, D., Vassilvitskii, S., et al. k-means++: The advantages of careful seeding. In _SODA_, volume 7, 2007.
* Ash et al. (2021) Ash, J., Goel, S., Krishnamurthy, A., and Kakade, S. Gone fishing: Neural active learning with fisher embeddings. _NeurIPS_, 34, 2021.
* Ash et al. (2020) Ash, J. T., Zhang, C., Krishnamurthy, A., Langford, J., and Agarwal, A. Deep batch active learning by diverse, uncertain gradient lower bounds. _ICLR_, 2020.
* Audibert et al. (2010) Audibert, J.-Y., Bubeck, S., and Munos, R. Best arm identification in multi-armed bandits. In _COLT_, 2010.
* Balestriero et al. (2023) Balestriero, R., Ibrahim, M., Sobal, V., Morcos, A., Shekhar, S., Goldstein, T., Bordes, F., Bardes, A., Mialon, G., Tian, Y., et al. A cookbook of self-supervised learning. _arXiv preprint arXiv:2304.12210_, 2023.
* Barrett (2015) Barrett, A. B. Exploration of synergistic and redundant information sharing in static and dynamical gaussian systems. _Physical Review E_, 91(5), 2015.
* Baumann et al. (2021) Baumann, D., Marco, A., Turchetta, M., and Trimpe, S. Gosafe: Globally optimal safe robot learning. In _ICRA_, 2021.
* Bengio et al. (2009) Bengio, Y., Louradour, J., Collobert, R., and Weston, J. Curriculum learning. In _ICML_, volume 26, 2009.
* Beraha et al. (2019) Beraha, M., Metelli, A. M., Papini, M., Tirinzoni, A., and Restelli, M. Feature selection via mutual information: New theoretical insights. In _IJCNN_, 2019.
* Berkenkamp et al. (2016) Berkenkamp, F., Schoellig, A. P., and Krause, A. Safe controller optimization for quadrotors with gaussian processes. In _ICRA_, 2016.
* Berkenkamp et al. (2021) Berkenkamp, F., Krause, A., and Schoellig, A. P. Bayesian optimization with safety constraints: safe and automatic parameter tuning in robotics. _Machine Learning_, 2021.
* Berlind & Urner (2015) Berlind, C. and Urner, R. Active nearest neighbors in changing environments. In _ICML_, 2015.
* Bickford Smith et al. (2023) Bickford Smith, F., Kirsch, A., Farquhar, S., Gal, Y., Foster, A., and Rainforth, T. Prediction-oriented bayesian active learning. In _AISTATS_, 2023.
* Bickford et al. (2015)Bickford Smith, F., Foster, A., and Rainforth, T. Making better use of unlabelled data in bayesian active learning. In _AISTATS_, 2024.
* Blundell et al. (2015) Blundell, C., Cornebise, J., Kavukcuoglu, K., and Wierstra, D. Weight uncertainty in neural network. In _ICML_, 2015.
* Bogunovic et al. (2016) Bogunovic, I., Scarlett, J., Krause, A., and Cevher, V. Truncated variance reduction: A unified approach to bayesian optimization and level-set estimation. _NeurIPS_, 29, 2016.
* Bottero et al. (2022) Bottero, A., Luis, C., Vinogradska, J., Berkenkamp, F., and Peters, J. R. Information-theoretic safe exploration with gaussian processes. _NeurIPS_, 35, 2022.
* Bottero et al. (2024) Bottero, A. G., Luis, C. E., Vinogradska, J., Berkenkamp, F., and Peters, J. Information-theoretic safe bayesian optimization. _arXiv preprint arXiv:2402.15347_, 2024.
* Bubeck et al. (2009) Bubeck, S., Munos, R., and Stoltz, G. Pure exploration in multi-armed bandits problems. In _ALT_, volume 20, 2009.
* Chaloner & Verdinelli (1995) Chaloner, K. and Verdinelli, I. Bayesian experimental design: A review. _Statistical Science_, 1995.
* Chandra (2023) Chandra, B. Quadrotor simulation, 2023. URL https://github.com/Bharath2/Quadrotor-Simulation.
* Chen & Krause (2013) Chen, Y. and Krause, A. Near-optimal batch mode active learning and adaptive submodular optimization. In _ICML_, 2013.
* Chowdhury & Gopalan (2017) Chowdhury, S. R. and Gopalan, A. On kernelized multi-armed bandits. In _ICML_, 2017.
* Cohn (1993) Cohn, D. Neural network exploration using optimal experiment design. _NeurIPS_, 6, 1993.
* Coleman et al. (2022) Coleman, C., Chou, E., Katz-Samuels, J., Culatana, S., Bailis, P., Berg, A. C., Nowak, R., Sumbaly, R., Zaharia, M., and Yalniz, I. Z. Similarity search for efficient active learning and search of rare concepts. In _AAAI_, volume 36, 2022.
* Cook (1977) Cook, R. D. Detection of influential observation in linear regression. _Technometrics_, 19(1), 1977.
* Cooper & Netoff (2022) Cooper, S. E. and Netoff, T. I. Multidimensional bayesian estimation for deep brain stimulation using the safeopt algorithm. _medRxiv_, 2022.
* Cover (1999) Cover, T. M. _Elements of information theory_. John Wiley & Sons, 1999.
* Das & Kempe (2008) Das, A. and Kempe, D. Algorithms for subset selection in linear regression. In _STOC_, volume 40, 2008.
* Das & Kempe (2018) Das, A. and Kempe, D. Approximate submodularity and its applications: Subset selection, sparse approximation and dictionary selection. _JMLR_, 19(1), 2018.
* Daxberger et al. (2021) Daxberger, E., Kristiadi, A., Immer, A., Eschenhagen, R., Bauer, M., and Hennig, P. Laplace redux-effortless bayesian deep learning. _NeurIPS_, 34, 2021.
* Deng et al. (2009) Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. Imagenet: A large-scale hierarchical image database. In _CVPR_, 2009.
* Devlin et al. (2019) Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. BERT: Pre-training of deep bidirectional transformers for language understanding. In _NAACL_, 2019.
* Emmenegger et al. (2023) Emmenegger, N., Mutny, M., and Krause, A. Likelihood ratio confidence sets for sequential decision making. _NeurIPS_, 37, 2023.
* Esfandiari et al. (2021) Esfandiari, H., Karbasi, A., and Mirrokni, V. Adaptivity in adaptive submodularity. In _COLT_, 2021.
* Eustratiadis et al. (2024) Eustratiadis, P., Dudziak, L., Li, D., and Hospedales, T. Neural fine-tuning search for few-shot learning. _ICLR_, 2024.
* Fiez et al. (2019) Fiez, T., Jain, L., Jamieson, K. G., and Ratliff, L. Sequential experimental design for transductive linear bandits. _NeurIPS_, 32, 2019.
* Fiez et al. (2019)Fu, B., Cao, Z., Wang, J., and Long, M. Transferable query selection for active domain adaptation. In _CVPR_, 2021.
* Gal et al. (2017) Gal, Y., Islam, R., and Ghahramani, Z. Deep bayesian active learning with image data. In _ICML_, 2017.
* Gao et al. (2020) Gao, M., Zhang, Z., Yu, G., Arik, S. O., Davis, L. S., and Pfister, T. Consistency-based semi-supervised active learning: Towards minimizing labeling cost. In _ECCV_, 2020.
* Gardner et al. (2014) Gardner, J. R., Kusner, M. J., Xu, Z. E., Weinberger, K. Q., and Cunningham, J. P. Bayesian optimization with inequality constraints. In _ICML_, volume 2014, 2014.
* Geifman & El-Yaniv (2017) Geifman, Y. and El-Yaniv, R. Deep active learning over the long tail. _arXiv preprint arXiv:1711.00941_, 2017.
* Grandvalet & Bengio (2004) Grandvalet, Y. and Bengio, Y. Semi-supervised learning by entropy minimization. _NeurIPS_, 17, 2004.
* Graves et al. (2017) Graves, A., Bellemare, M. G., Menick, J., Munos, R., and Kavukcuoglu, K. Automated curriculum learning for neural networks. In _ICML_, 2017.
* Graybill (1961) Graybill, F. A. _An introduction to linear statistical models_. Literary Licensing, LLC, 1961.
* Guo & Greiner (2007) Guo, Y. and Greiner, R. Optimistic active-learning using mutual information. In _IJCAI_, volume 7, 2007.
* Hacohen et al. (2022) Hacohen, G., Dekel, A., and Weinshall, D. Active learning on a budget: Opposite strategies suit high and low budgets. _ICML_, 2022.
* Hardt & Sun (2024) Hardt, M. and Sun, Y. Test-time training on nearest neighbors for large language models. _ICLR_, 2024.
* He et al. (2020) He, B., Lakshminarayanan, B., and Teh, Y. W. Bayesian deep ensembles via the neural tangent kernel. _NeurIPS_, 33, 2020.
* Hendrycks & Gimpel (2017) Hendrycks, D. and Gimpel, K. A baseline for detecting misclassified and out-of-distribution examples in neural networks. _ICLR_, 2017.
* Hennig & Schuler (2012) Hennig, P. and Schuler, C. J. Entropy search for information-efficient global optimization. _JMLR_, 13(6), 2012.
* Hernandez-Lobato et al. (2014) Hernandez-Lobato, J. M., Hoffman, M. W., and Ghahramani, Z. Predictive entropy search for efficient global optimization of black-box functions. _NeurIPS_, 27, 2014.
* Hoffman & Ghahramani (2015) Hoffman, M. W. and Ghahramani, Z. Output-space predictive entropy search for flexible global optimization. In _NeurIPS workshop on Bayesian Optimization_, 2015.
* Holzmuller et al. (2023) Holzmuller, D., Zaverkin, V., Kastner, J., and Steinwart, I. A framework and benchmark for deep batch active learning for regression. _JMLR_, 24(164), 2023.
* Hopfield (1982) Hopfield, J. J. Neural networks and physical systems with emergent collective computational abilities. _Proceedings of the national academy of sciences_, 79(8), 1982.
* Houlsby et al. (2011) Houlsby, N., Huszar, F., Ghahramani, Z., and Lengyel, M. Bayesian active learning for classification and preference learning. _CoRR_, 2011.
* Hubotter et al. (2024) Hubotter, J., Sukhija, B., Treven, L., As, Y., and Krause, A. Active few-shot fine-tuning. _ICLR workshop on Bridging the Gap Between Practice and Theory in Deep Learning_, 2024.
* Ilyas et al. (2022) Ilyas, A., Park, S. M., Engstrom, L., Leclerc, G., and Madry, A. Datamodels: Predicting predictions from training data. _arXiv preprint arXiv:2202.00622_, 2022.
* Jacot et al. (2018) Jacot, A., Gabriel, F., and Hongler, C. Neural tangent kernel: Convergence and generalization in neural networks. _NeurIPS_, 31, 2018.
* Jiao et al. (2018)Johnson, J., Douze, M., and Jegou, H. Billion-scale similarity search with gpus. _IEEE Transactions on Big Data_, 7(3), 2019.
* Kaddour et al. (2020) Kaddour, J., Saemundsson, S., et al. Probabilistic active meta-learning. _NeurIPS_, 33, 2020.
* Kassraie & Krause (2022) Kassraie, P. and Krause, A. Neural contextual bandits without regret. In _AISTATS_, 2022.
* Khan et al. (2019) Khan, M. E. E., Immer, A., Abedi, E., and Korzepa, M. Approximate inference turns deep networks into gaussian processes. _NeurIPS_, 32, 2019.
* Khanna et al. (2017) Khanna, R., Elenberg, E., Dimakis, A., Negahban, S., and Ghosh, J. Scalable greedy feature selection via weak submodularity. In _AISTATS_, 2017.
* Kingma & Ba (2014) Kingma, D. P. and Ba, J. Adam: A method for stochastic optimization. In _ICLR_, 2014.
* Kirsch (2023) Kirsch, A. Black-box batch active learning for regression. _arXiv preprint arXiv:2302.08981_, 2023.
* Kirsch et al. (2019) Kirsch, A., Van Amersfoort, J., and Gal, Y. Batchbald: Efficient and diverse batch acquisition for deep bayesian active learning. _NeurIPS_, 32, 2019.
* Kirschner et al. (2019) Kirschner, J., Mutny, M., Hiller, N., Ischebeck, R., and Krause, A. Adaptive and safe bayesian optimization in high dimensions via one-dimensional subspaces. In _ICML_, 2019.
* Koh & Liang (2017) Koh, P. W. and Liang, P. Understanding black-box predictions via influence functions. In _ICML_, 2017.
* Kothawade et al. (2021) Kothawade, S., Beck, N., Killamsetty, K., and Iyer, R. Similar: Submodular information measures based active learning in realistic scenarios. _NeurIPS_, 34, 2021.
* Krause & Golovin (2014) Krause, A. and Golovin, D. Submodular function maximization. _Tractability_, 3, 2014.
* Krause & Guestrin (2007) Krause, A. and Guestrin, C. Nonmyopic active learning of gaussian processes: an exploration-exploitation approach. In _ICML_, volume 24, 2007.
* Krause et al. (2008) Krause, A., Singh, A., and Guestrin, C. Near-optimal sensor placements in gaussian processes: Theory, efficient algorithms and empirical studies. _JMLR_, 9(2), 2008.
* Krizhevsky et al. (2009) Krizhevsky, A., Hinton, G., et al. Learning multiple layers of features from tiny images. Technical report, University of Toronto, 2009.
* Kumari et al. (2024) Kumari, L., Wang, S., Das, A., Zhou, T., and Bilmes, J. An end-to-end submodular framework for data-efficient in-context learning. In _NAACL_, 2024.
* Lakshminarayanan et al. (2017) Lakshminarayanan, B., Pritzel, A., and Blundell, C. Simple and scalable predictive uncertainty estimation using deep ensembles. _NeurIPS_, 30, 2017.
* LeCun et al. (1998) LeCun, Y., Cortes, C., and Burges, C. J. The mnist database of handwritten digits. _http://yann.lecun.com/exdb/mnist/_, 1998.
* Lee et al. (2018) Lee, J., Bahri, Y., Novak, R., Schoenholz, S. S., Pennington, J., and Sohl-Dickstein, J. Deep neural networks as gaussian processes. _ICLR_, 2018.
* Lee et al. (2019) Lee, J., Xiao, L., Schoenholz, S., Bahri, Y., Novak, R., Sohl-Dickstein, J., and Pennington, J. Wide neural networks of any depth evolve as linear models under gradient descent. _NeurIPS_, 32, 2019.
* Lee et al. (2022) Lee, Y., Chen, A. S., Tajwar, F., Kumar, A., Yao, H., Liang, P., and Finn, C. Surgical fine-tuning improves adaptation to distribution shifts. _NeurIPS workshop on Distribution Shifts_, 2022.
* Lewis & Gale (1994) Lewis, D. and Gale, W. A sequential algorithm for training text classifiers. In _SIGIR_, 1994.
* Lewis & Catlett (1994) Lewis, D. D. and Catlett, J. Heterogeneous uncertainty sampling for supervised learning. In _Machine learning proceedings_. 1994.
* MacKay (1992) MacKay, D. J. Information-based objective functions for active data selection. _Neural computation_, 4(4), 1992.
* MacKay (1994)Maddox, W. J., Izmailov, P., Garipov, T., Vetrov, D. P., and Wilson, A. G. A simple baseline for bayesian uncertainty in deep learning. _NeurIPS_, 32, 2019.
* Malladi et al. (2023) Malladi, S., Wettig, A., Yu, D., Chen, D., and Arora, S. A kernel-based view of language model fine-tuning. In _ICML_, 2023.
* Martens & Grosse (2015) Martens, J. and Grosse, R. Optimizing neural networks with kronecker-factored approximate curvature. In _ICML_, 2015.
* Mehta et al. (2022) Mehta, R., Shui, C., Nichyporuk, B., and Arbel, T. Information gain sampling for active learning in medical image classification. In _UNSURE_, 2022.
* Murphy (2023) Murphy, K. P. _Probabilistic machine learning: Advanced topics_. MIT Press, 2023.
* Mutny & Krause (2022) Mutny, M. and Krause, A. Experimental design for linear functionals in reproducing kernel hilbert spaces. _NeurIPS_, 35, 2022.
* Nemhauser et al. (1978) Nemhauser, G. L., Wolsey, L. A., and Fisher, M. L. An analysis of approximations for maximizing submodular set functions--i. _Mathematical programming_, 14, 1978.
* Ostrovsky et al. (2013) Ostrovsky, R., Rabani, Y., Schulman, L. J., and Swamy, C. The effectiveness of lloyd-type methods for the k-means problem. _JACM_, 2013.
* Pacchiano et al. (2024) Pacchiano, A., Lee, J. N., and Brunskill, E. Experiment planning with function approximation. _NeurIPS_, 37, 2024.
* Peng et al. (2005) Peng, H., Long, F., and Ding, C. Feature selection based on mutual information criteria of max-dependency, max-relevance, and min-redundancy. _IEEE Transactions on pattern analysis and machine intelligence_, 27(8), 2005.
* Prabhu et al. (2021) Prabhu, V., Chandrasekaran, A., Saenko, K., and Hoffman, J. Active domain adaptation via clustering uncertainty-weighted embeddings. In _ICCV_, 2021.
* Pruthi et al. (2019) Pruthi, G., Liu, F., Kale, S., and Sundararajan, M. Estimating training data influence by tracing gradient descent. In _NeurIPS_, 2019.
* Rahimi & Recht (2007) Rahimi, A. and Recht, B. Random features for large-scale kernel machines. _NeurIPS_, 20, 2007.
* Rai et al. (2010) Rai, P., Saha, A., Daume III, H., and Venkatasubramanian, S. Domain adaptation meets active learning. In _NAACL HLT workshop on Active Learning for Natural Language Processing_, 2010.
* Rothfuss et al. (2023) Rothfuss, J., Koenig, C., Rupenyan, A., and Krause, A. Meta-learning priors for safe bayesian optimization. In _COLT_, 2023.
* Russo et al. (2018) Russo, D. J., Van Roy, B., Kazerouni, A., Osband, I., Wen, Z., et al. A tutorial on thompson sampling. _Foundations and Trends(r) in Machine Learning_, 11(1), 2018.
* Saha et al. (2011) Saha, A., Rai, P., Daume, H., Venkatasubramanian, S., and DuVall, S. L. Active supervised domain adaptation. In _Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD_, 2011.
* Scheffer et al. (2001) Scheffer, T., Decomain, C., and Wrobel, S. Active hidden markov models for information extraction. In _IDA_, 2001.
* Schreiter et al. (2015) Schreiter, J., Nguyen-Tuong, D., Eberts, M., Bischoff, B., Markert, H., and Toussaint, M. Safe exploration for active learning with gaussian processes. In _ECML PKDD_, 2015.
* Sener & Savarese (2017) Sener, O. and Savarese, S. Active learning for convolutional neural networks: A core-set approach. _ICLR_, 2017.
* Seo et al. (2000) Seo, S., Wallat, M., Graepel, T., and Obermayer, K. Gaussian process regression: Active data selection and test point rejection. In _Mustererkennung 2000_. Springer, 2000.
* Settles (2009) Settles, B. Active learning literature survey. Technical report, University of Wisconsin-Madison Department of Computer Sciences, 2009.
* Sermanerman & Kempper (2009)Settles, B. and Craven, M. An analysis of active learning strategies for sequence labeling tasks. In _EMNLP_, 2008.
* Shoham and Avron (2023) Shoham, N. and Avron, H. Experimental design for overparameterized learning with application to single shot deep active learning. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 2023.
* Shwartz-Ziv and LeCun (2023) Shwartz-Ziv, R. and LeCun, Y. To compress or not to compress-self-supervised learning and information theory: A review. _arXiv preprint arXiv:2304.09355_, 2023.
* Soviany et al. (2022) Soviany, P., Ionescu, R. T., Rota, P., and Sebe, N. Curriculum learning: A survey. _IJCV_, 2022.
* Srinivas et al. (2009) Srinivas, N., Krause, A., Kakade, S. M., and Seeger, M. Gaussian process optimization in the bandit setting: No regret and experimental design. In _ICML_, volume 27, 2009.
* Strang (2016) Strang, G. _Introduction to linear algebra_. SIAM, 5 edition, 2016.
* Su et al. (2020) Su, J.-C., Tsai, Y.-H., Sohn, K., Liu, B., Maji, S., and Chandraker, M. Active adversarial domain adaptation. In _WACV_, 2020.
* Sui et al. (2015) Sui, Y., Gotovos, A., Burdick, J., and Krause, A. Safe exploration for optimization with gaussian processes. In _ICML_, 2015.
* Sukhija et al. (2023) Sukhija, B., Turchetta, M., Lindner, D., Krause, A., Trimpe, S., and Baumann, D. Gosafeopt: Scalable safe exploration for global optimization of dynamical systems. _Artificial Intelligence_, 2023.
* Tamkin et al. (2022) Tamkin, A., Nguyen, D., Deshpande, S., Mu, J., and Goodman, N. Active learning helps pretrained models learn the intended task. _NeurIPS_, 35, 2022.
* Tan and Le (2019) Tan, M. and Le, Q. Efficientnet: Rethinking model scaling for convolutional neural networks. In _ICML_, 2019.
* Thompson (1933) Thompson, W. R. On the likelihood that one unknown probability exceeds another in view of the evidence of two samples. _Biometrika_, 1933.
* Tu et al. (2023) Tu, S., Frostig, R., Singh, S., and Sindhwani, V. JAX: A python library for differentiable optimal control on accelerators, 2023. URL http://github.com/google/trajax.
* Turchetta et al. (2019) Turchetta, M., Berkenkamp, F., and Krause, A. Safe exploration for interactive machine learning. _NeurIPS_, 32, 2019.
* Vakili et al. (2021) Vakili, S., Khezeli, K., and Picheny, V. On information gain and regret bounds in gaussian process bandits. In _AISTATS_, 2021.
* Vapnik (1982) Vapnik, V. _Estimation of dependences based on empirical data_. Springer Science & Business Media, 1982.
* Vergara and Estevez (2014) Vergara, J. R. and Estevez, P. A. A review of feature selection methods based on mutual information. _Neural computing and applications_, 24, 2014.
* Vinyals et al. (2016) Vinyals, O., Blundell, C., Lillicrap, T., Wierstra, D., et al. Matching networks for one shot learning. _NeurIPS_, 29, 2016.
* Wainwright (2019) Wainwright, M. J. _High-dimensional statistics: A non-asymptotic viewpoint_, volume 48. Cambridge university press, 2019.
* Wang et al. (2021) Wang, C., Sun, S., and Grosse, R. Beyond marginal uncertainty: How accurately can bayesian regression models estimate posterior predictive correlations? In _AISTATS_, 2021.
* Wang and Jegelka (2017) Wang, Z. and Jegelka, S. Max-value entropy search for efficient bayesian optimization. In _ICML_, 2017.
* Wei et al. (2022) Wei, A., Hu, W., and Steinhardt, J. More than a toy: Random matrix models predict how real-world neural representations generalize. In _ICML_, 2022.
* Wang et al. (2021)Widmer, D., Kang, D., Sukhija, B., Hubotter, J., Krause, A., and Coros, S. Tuning legged locomotion controllers via safe bayesian optimization. _CORL_, 2023.
* Wilks (1932) Wilks, S. S. Certain generalizations in the analysis of variance. _Biometrika_, 1932.
* Williams and Rasmussen (2006) Williams, C. K. and Rasmussen, C. E. _Gaussian processes for machine learning_, volume 2. MIT press Cambridge, MA, 2006.
* Wischnewski et al. (2019) Wischnewski, A., Betz, J., and Lohmann, B. A model-free algorithm to safely approach the handling limit of an autonomous racecar. In _ICCVE_, 2019.
* Xia et al. (2024) Xia, M., Malladi, S., Gururangan, S., Arora, S., and Chen, D. Less: Selecting influential data for targeted instruction tuning. In _ICML_, 2024.
* Xu and Kazantsev (2019) Xu, M. and Kazantsev, G. Understanding goal-oriented active learning via influence functions. In _NeurIPS Workshop on Machine Learning with Guarantees_, 2019.
* Ye et al. (2023) Ye, J., Wu, Z., Feng, J., Yu, T., and Kong, L. Compositional exemplars for in-context learning. In _ICML_, 2023.
* Yehuda et al. (2022) Yehuda, O., Dekel, A., Hacohen, G., and Weinshall, D. Active learning through a covering lens. _NeurIPS_, 35, 2022.
* Yu and Kim (2010) Yu, H. and Kim, S. Passive sampling for regression. In _ICDM_, 2010.
* Yu et al. (2006) Yu, K., Bi, J., and Tresp, V. Active learning via transductive experimental design. In _ICML_, volume 23, 2006.
* Zanette et al. (2021) Zanette, A., Dong, K., Lee, J. N., and Brunskill, E. Design of experiments for stochastic contextual linear bandits. _NeurIPS_, 34, 2021.
* Zheng et al. (2023) Zheng, H., Liu, R., Lai, F., and Prakash, A. Coverage-centric coreset selection for high pruning rates. _ICLR_, 2023.

## Appendices

A general principle of "transductive learning" was already formulated by the famous computer scientist Vladimir Vapnik in the 20th century. Vapnik proposes the following "imperative for a complex world":

_When solving a problem of interest, do not solve a more general problem as an intermediate step. Try to get the answer that you really need but not a more general one._

- Vapnik (1982)

These appendices provide additional background, proofs, experiment details, and ablation studies.

###### Contents

* A Additional Related Work
* B Background
* B.1 Information Theory
* B.2 Gaussian Processes
* C Proofs
* C.1 Undirected Case of ITL
* C.2 Non-adaptive Data Selection & Submodularity
* C.3 Batch Diversity: Batch Selection as Non-adaptive Data Selection
* C.4 Measures of Synergies & Approximate Submodularity
* C.5 Convergence of Marginal Gain
* C.6 Proof of Theorem 3.2
* C.7 Proof of Theorem 3.3
* C.8 Proof of Theorem 5.1
* C.9 Useful Facts and Inequalities
* D Interpretations & Approximations of Principle (\(\dagger\))
* D.1 Interpretations of ITL
* D.2 Interpretations of VTL
* D.3 Mean Marginal ITL
* D.4 Correlation-based Transductive Learning
* D.5 Summary
* E Stochastic Target Spaces
* F Closed-form Decision Rules
* G Computational Complexity
* H Additional GP Experiments & Details* 1 Alternative Settings for Active Fine-Tuning
	* 1.1 Prediction Targets are Contained in Sample Space: \(\mathcal{A}\subseteq\mathcal{S}\)
	* 1.2 Active Domain Adaptation
* 2 Additional NN Experiments & Details
	* 2.1 Experiment Details
	* 2.2 Embeddings and Kernels
	* 2.3 Towards Uncertainty Quantification in Latent Space
	* 2.4 Batch Selection via Conditional Embeddings
	* 2.5 Baselines
	* 2.6 Additional experiments
	* 2.7 Ablation study of noise standard deviation \(\rho\)
* 3 Additional Safe BO Experiments & Details
	* 3.1 A More Exploitative Stochastic Target Space
	* 3.2 Detailed Comparison with Prior Works
	* 3.3 Jumping Past Local Barriers
	* 3.4 Experiment Details
Additional Related Work

The general principle of non-active "transductive learning" was introduced by Vapnik (1982). The notion of "target" from transductive active learning is akin to the notion of "task" in curriculum learning (Bengio et al., 2009; Graves et al., 2017; Soviany et al., 2022). The study of settings where the irreducible uncertainty is zero is related to the study of estimability in experimental design (Grayhill, 1961; Mutny and Krause, 2022). In feature selection, selecting features that maximize information gain with respect to a to-be-predicted label is a standard approach (Peng et al., 2005; Vergara and Estevez, 2014; Beraha et al., 2019) which is akin to ITL (cf. Appendix D). The themes of relevance and diversity are also important for efficient in-context learning (e.g., Ye et al., 2023; Kumari et al., 2024) and data pruning (Zheng et al., 2023). Transductive active learning is complimentary to other learning methodologies, such as semi-supervised learning (Gao et al., 2020), self-supervised learning (Shwartz-Ziv and LeCun, 2023; Balestriero et al., 2023), and meta-learning (Kaddour et al., 2020; Rothfuss et al., 2023).

## Appendix B Background

### Information Theory

Throughout this work, \(\log\) denotes the natural logarithm. Given random vectors \(\bm{x}\) and \(\bm{y}\), we denote by

\[\mathrm{H}[\bm{x}] \stackrel{{\mathrm{def}}}{{=}}\mathbb{E}_{p(\bm{x} )}[-\log p(\bm{x})],\] \[\mathrm{H}[\bm{x}\mid\bm{y}] \stackrel{{\mathrm{def}}}{{=}}\mathbb{E}_{p(\bm{x},\bm{y})}[-\log p(\bm{x}\mid\bm{y})],\quad\text{and}\] \[\mathrm{I}(\bm{x};\bm{y}) \stackrel{{\mathrm{def}}}{{=}}\mathrm{H}[\bm{x}]- \mathrm{H}[\bm{x}\mid\bm{y}]\]

the (differential) entropy, conditional entropy, and information gain, respectively (Cover, 1999).9

Footnote 9: One has to be careful to ensure that \(\mathrm{I}(\bm{x};\bm{y})\) exists, i.e., \(|\mathrm{I}(\bm{x};\bm{y})|<\infty\). We will assume that this is the case throughout this work. When \(\bm{x}\) and \(\bm{y}\) are jointly Gaussian, this is satisfied when the noise variance \(\rho^{2}\) is positive.

The _multivariate information gain_(Murphy, 2023) between random vectors \(\bm{x},\bm{y},\bm{z}\) is given by

\[\mathrm{I}(\bm{x};\bm{y};\bm{z}) \stackrel{{\mathrm{def}}}{{=}}\mathrm{I}(\bm{x};\bm{y} )-\mathrm{I}(\bm{x};\bm{y}\mid\bm{z})\] (6) \[=\mathrm{I}(\bm{x};\bm{y})+\mathrm{I}(\bm{x};\bm{z})-\mathrm{I}( \bm{x};\bm{y},\bm{z}).\] (7)

When \(\mathrm{I}(\bm{x};\bm{y};\bm{z})\neq 0\) it is said that \(\bm{y}\) and \(\bm{z}\)_interact_ regarding their information about \(\bm{x}\). If the interaction is positive, it is said that the information of \(\bm{z}\) about \(\bm{x}\) is _redundant_ given \(\bm{y}\). Conversely, if the interaction is negative, it is said that the information of \(\bm{z}\) about \(\bm{x}\) is _synergistic_ with \(\bm{y}\). The notion of synergy is akin to the frequentist notion of "suppressor variables" in linear regression (Das and Kempe, 2008).

### Gaussian Processes

The stochastic process \(f\) is a Gaussian process (GP, Williams and Rasmussen (2006)), denoted \(f\sim\mathcal{GP}(\mu,k)\), with mean function \(\mu\) and kernel \(k\) if for any finite subset \(X=\{\bm{x}_{1},\dots,\bm{x}_{n}\}\subseteq\mathcal{X}\), \(\bm{f}_{X}\sim\mathcal{N}(\bm{\mu}_{X},\bm{K}_{XX})\) is jointly Gaussian with mean vector \(\bm{\mu}_{X}(i)=\mu(\bm{x}_{i})\) and covariance matrix \(\bm{K}_{XX}(i,j)=k(\bm{x}_{i},\bm{x}_{j})\).

In the following, we formalize the assumptions from the GP setting (cf. Section 3.1).

**Assumption B.1** (Gaussian prior).: We assume that \(f\sim\mathcal{GP}(\mu,k)\) with known mean function \(\mu\) and kernel \(k\).

**Assumption B.2** (Gaussian noise).: We assume that the noise \(\varepsilon_{\bm{x}}\) is mutually independent and zero-mean Gaussian with known variance \(\rho^{2}(\bm{x})>0\). We write \(\bm{P}_{X}=\operatorname{diag}\rho^{2}(\bm{x}_{1}),\dots,\rho^{2}(\bm{x}_{n})\).

Under Assumptions B.1 and B.2, the posterior distribution of \(f\) after observing points \(X\) is \(\mathcal{GP}(\mu_{n},k_{n})\) with

\[\mu_{n}(\bm{x}) =\mu(\bm{x})+\bm{K}_{\bm{x}X}(\bm{K}_{XX}+\bm{P}_{X})^{-1}(\bm{y }_{X}-\bm{\mu}_{X}),\] \[k_{n}(\bm{x},\bm{x}^{\prime}) =k(\bm{x},\bm{x}^{\prime})-\bm{K}_{\bm{x}X}(\bm{K}_{XX}+\bm{P}_{X })^{-1}\bm{K}_{X\bm{x}^{\prime}},\] \[\sigma_{n}^{2}(\bm{x}) =k_{n}(\bm{x},\bm{x}).\]For Gaussian random vectors \(\bm{f}\) and \(\bm{y}\), the entropy is \(\mathrm{H}[\bm{f}]=\frac{n}{2}\log(2\pi e)+\frac{1}{2}\log|\mathrm{Var}[\bm{f}]|\), the information gain is \(\mathrm{I}(\bm{f};\bm{y})=\frac{1}{2}(\log|\mathrm{Var}[\bm{y}]|-\log|\mathrm{ Var}[\bm{y}\mid\bm{f}]|)\), and

\[\gamma_{n}=\max_{\begin{subarray}{c}X\in\mathcal{X}\\ |X|\leq n\end{subarray}}\frac{1}{2}\log\left|\bm{I}+\bm{P}_{X}^{-1}\bm{K}_{XX} \right|.\]

## Appendix C Proofs

We will write

* \(\sigma^{2}\operatorname*{\stackrel{{\mathrm{def}}}{{=}}}\max_{ \bm{x}\in\mathcal{X}}\sigma_{0}^{2}(\bm{x})\), and
* \(\tilde{\sigma}^{2}\operatorname*{\stackrel{{\mathrm{def}}}{{=}}} \max_{\bm{x}\in\mathcal{X}}\sigma_{0}^{2}(\bm{x})+\rho^{2}(\bm{x})\).

The following is a brief overview of the structure of this section:

1. Appendix C.1 relates ITL in the inductive learning setting (\(\mathcal{S}\subseteq\mathcal{A}\)) to prior work.
2. Appendix C.2 relates the designs selected by ITL and VTL to the optimal designs for corresponding non-adaptive objectives.
3. Appendix C.3 shows that batch selection via ITL or VTL leads to informative and diverse batches, utilizing the results from Appendix C.2.
4. Appendix C.4 introduces measures of synergies that generalize the submodularity assumption (cf. Assumption 3.1).
5. Appendix C.5 proves key results on the convergence of the ITL and VTL objectives.
6. Appendix C.6 proves Theorem 3.2 (convergence in GP setting).
7. Appendix C.7 proves Theorem 3.3 (convergence in agnostic setting).
8. Appendix C.8 proves Theorem 5.1 (convergence in safe BO application).
9. Appendix C.9 includes useful facts.

### Undirected Case of ITL

We briefly examine the important special case of ITL where \(\mathcal{S}\subseteq\mathcal{A}\). In this setting, for all \(\bm{x}\in\mathcal{S}\), the decision rule of ITL simplifies to

\[\mathrm{I}(\bm{f}_{\mathcal{A}};y_{\bm{x}}\mid\mathcal{D}_{n}) \stackrel{{(i)}}{{=}}\mathrm{I}(\bm{f}_{\mathcal{A} \setminus\{\bm{x}\}};y_{\bm{x}}\mid f_{\bm{x}},\mathcal{D}_{n})+\mathrm{I}(f_{ \bm{x}};y_{\bm{x}}\mid\mathcal{D}_{n})\] \[\stackrel{{(ii)}}{{=}}\mathrm{I}(f_{\bm{x}};y_{\bm{x }}\mid\mathcal{D}_{n})\] \[=\mathrm{H}[y_{\bm{x}}\mid\mathcal{D}_{n}]-\mathrm{H}[\varepsilon_ {\bm{x}}]\]

where \((i)\) follows from the chain rule of information gain and \(\bm{x}\in\mathcal{S}\subseteq\mathcal{A}\); and \((ii)\) follows from the conditional independence \(\bm{f}_{\mathcal{A}}\perp y_{\bm{x}}\mid f_{\bm{x}}\).

If additionally \(f\) is a GP then

\[\mathrm{H}[y_{\bm{x}}\mid\mathcal{D}_{n}]-\mathrm{H}[\varepsilon_{\bm{x}}]= \frac{1}{2}\log\biggl{(}1+\frac{\mathrm{Var}[f_{\bm{x}}\mid\mathcal{D}_{n}]}{ \mathrm{Var}[\varepsilon_{\bm{x}}]}\biggr{)}.\]

This decision rule has also been termed _total information gain_(MacKay, 1992). When \(\mathcal{S}\subseteq\mathcal{A}\) and observation noise is homoscedastic, this decision rule is equivalent to uncertainty sampling.

### Non-adaptive Data Selection & Submodularity

Recall the non-myopic information gain \(\psi_{\mathcal{A}}(X)=\mathrm{I}(\bm{f}_{\mathcal{A}};\bm{y}_{X})\) (ITL) and variance reduction \(\psi_{\mathcal{A}}(X)=\mathrm{tr}\ \mathrm{Var}[\bm{f}_{\mathcal{A}}]-\mathrm{tr}\ \mathrm{Var}[\bm{f}_{ \mathcal{A}}\mid\bm{y}_{X}]\) (VTL) objective functions from Assumption 3.1. In this section, we will relate the designs selected by ITL and VTL to the optimal designs for these objectives. To this end, consider the non-adaptive optimization problem

\[X^{\star}=\operatorname*{arg\,max}_{\begin{subarray}{c}X\subseteq\mathcal{S} \\ |X|=k\end{subarray}}\psi_{\mathcal{A}}(X).\]

**Lemma C.1**.: _For both_ ITL _and_ VTL_, \(\psi_{\mathcal{A}}\) is non-negative and monotone._

Proof.: For ITL, \(\psi_{\mathcal{A}}(X)\geq 0\) follows from the non-negativity of mutual information. To conclude monotonicity, note that for any \(X^{\prime}\subseteq X\subseteq\mathcal{S}\),

\[\mathrm{I}(\bm{f}_{\mathcal{A}};\bm{y}_{X^{\prime}})=\mathrm{H}[\bm{f}_{ \mathcal{A}}]-\mathrm{H}[\bm{f}_{\mathcal{A}}\mid\bm{y}_{X^{\prime}}]\leq \mathrm{H}[\bm{f}_{\mathcal{A}}]-\mathrm{H}[\bm{f}_{\mathcal{A}}\mid\bm{y}_{X }]=\mathrm{I}(\bm{f}_{\mathcal{A}};\bm{y}_{X})\]

due to monotonicity of conditional entropy (which is also called the "information never hurts" principle).

For VTL, recall that \(\mathrm{tr}\;\mathrm{Var}[\bm{f}_{\mathcal{A}}\mid\bm{y}_{X}]\leq\mathrm{tr}\; \mathrm{Var}[\bm{f}_{\mathcal{A}}\mid\bm{y}_{X^{\prime}}]\) for any \(X^{\prime}\subseteq X\subseteq\mathcal{S}\) (with an implicit expectation over \(\bm{y}_{X},\bm{y}_{X^{\prime}}\)). Non-negativity and monotonicity of \(\psi_{\mathcal{A}}\) then follow analogously to ITL. 

**Lemma C.2**.: _The marginal gain \(\Delta_{\mathcal{A}}(\bm{x}\mid X)\stackrel{{\mathrm{def}}}{{=}} \psi_{\mathcal{A}}(X\cup\{\bm{x}\})-\psi_{\mathcal{A}}(X)\) of \(\bm{x}\in\mathcal{S}\) given \(X\subseteq\mathcal{S}\) is the_ ITL _and_ VTL _objective, respectively._

Proof.: For ITL,

\[\Delta_{\mathcal{A}}(\bm{x}\mid X) =\mathrm{I}(\bm{f}_{\mathcal{A}};\bm{y}_{X},y_{\bm{x}})-\mathrm{I }(\bm{f}_{\mathcal{A}};\bm{y}_{X})\] \[=\mathrm{H}[\bm{f}_{\mathcal{A}}\mid\bm{y}_{X}]-\mathrm{H}[\bm{f} _{\mathcal{A}}\mid\bm{y}_{X},y_{\bm{x}}]\] \[=\mathrm{I}(\bm{f}_{\mathcal{A}};\bm{y}_{\bm{x}}\mid\bm{y}_{X})\]

which is precisely the ITL objective.

For VTL,

\[\Delta_{\mathcal{A}}(\bm{x}\mid X) =\mathrm{tr}\;\mathrm{Var}[\bm{f}_{\mathcal{A}}\mid\bm{y}_{X}]- \mathrm{tr}\;\mathrm{Var}[\bm{f}_{\mathcal{A}}\mid\bm{y}_{X},y_{\bm{x}}]\] \[=-\mathrm{tr}\;\mathrm{Var}[\bm{f}_{\mathcal{A}}\mid\bm{y}_{X},y_ {\bm{x}}]+\mathrm{const}\]

which is precisely the VTL objective. 

**Definition C.3** (Submodularity).: \(\psi_{\mathcal{A}}\) is submodular if and only if for all \(\bm{x}\in\mathcal{S}\) and \(X^{\prime}\subseteq X\subseteq\mathcal{S}\),

\[\Delta_{\mathcal{A}}(\bm{x}\mid X^{\prime})\geq\Delta_{\mathcal{A}}(\bm{x}\mid X).\]

**Theorem C.4** (Nemhauser et al. (1978)).: _Let Assumption 3.1 hold. For any \(n\geq 1\), if_ ITL _or_ VTL _selected \(\bm{x}_{1:n}\), respectively, then_

\[\psi_{\mathcal{A}}(\bm{x}_{1:n})\geq(1-1/e)\max_{\begin{subarray}{c}X\subseteq \mathcal{S}\\ \mid X\mid\leq n\end{subarray}}\psi_{\mathcal{A}}(X).\]

Proof.: This is a special case of a canonical result from non-negative monotone submodular function maximization (Nemhauser et al., 1978; Krause and Golovin, 2014). 

### Batch Diversity: Batch Selection as Non-adaptive Data Selection

Recall the non-adaptive optimization problem

\[B_{n,k}=\operatorname*{arg\,max}_{\begin{subarray}{c}B\subseteq\mathcal{S} \\ \mid B\mid=k\end{subarray}}\mathrm{I}(\bm{f}_{\mathcal{A}};\bm{y}_{B}\mid \mathcal{D}_{n-1})\]

from Equation (3) with batch size \(k>0\), and denote by \(B^{\prime}_{n,k}=\bm{x}_{n,1:k}\) the greedy approximation from Equation (3). The selection of an individual batch can be seen as a single non-adaptive optimization problem with marginal gain

\[\Delta_{n}(\bm{x}\mid B) =\mathrm{I}(\bm{f}_{\mathcal{A}};\bm{y}_{B},y_{\bm{x}}\mid\mathcal{ D}_{n-1})-\mathrm{I}(\bm{f}_{\mathcal{A}};\bm{y}_{B}\mid\mathcal{D}_{n-1})\] \[=\mathrm{H}[\bm{f}_{\mathcal{A}}\mid\mathcal{D}_{n-1},\bm{y}_{B}] -\mathrm{H}[\bm{f}_{\mathcal{A}}\mid\mathcal{D}_{n-1},\bm{y}_{B},y_{\bm{x}}]\] \[=\mathrm{I}(\bm{f}_{\mathcal{A}};\bm{y}_{\bm{x}}\mid\mathcal{D}_{n -1},\bm{y}_{B})\]

and which is precisely the objective function of ITL from Equation (3). Hence, the approximation guarantees from Theorems C.4 and C.11 apply. The derivation is analogous for VTL.

Prior work has shown that the greedy solution \(B^{\prime}_{n}\) is also competitive with a fully sequential "batchless" decision rule (Chen and Krause, 2013; Esfandiari et al., 2021).

### Measures of Synergies & Approximate Submodularity

We will now show that "downstream synergies", if present, can be seen as a source of learning complexity, which is orthogonal to the information capacity \(\gamma_{n}\).

**Example C.5**.: Consider the example where \(f\) is a stochastic process of three random variables \(X,Y,Z\) where \(X\) and \(Y\) are Bernoulli (\(p=\frac{1}{2}\)), and \(Z\) is the XOR of \(X\) and \(Y\). Suppose that observations are exact (i.e., \(\varepsilon_{n}=0\)), that the target space \(\mathcal{A}\) comprises the output variable \(Z\) while the sample space \(\mathcal{S}\) comprises the input variables \(X\) and \(Y\). Observing any single \(X\) or \(Y\) yields no information about \(Z\): \(\mathrm{I}(Z;X)=\mathrm{I}(Z;Y)=0\), however, observing both inputs jointly perfectly determines \(Z\): \(\mathrm{I}(Z;X,Y)=1\). Thus, \(\gamma_{n}(\mathcal{A};\mathcal{S})=1\) if \(n\geq 2\) and \(\gamma_{n}(\mathcal{A};\mathcal{S})=0\) else.

Learning about \(Z\) in examples of this kind is difficult for agents that make decisions greedily, since the next action (observing \(X\) or \(Y\)) yields no signal about its long-term usefulness. We call a sequence of observations, such as \(\{X,Y\}\), _synergistic_ since its combined information value is larger than the individual values. The prevalence of synergies is not captured by the information capacity \(\gamma_{n}(\mathcal{A};\mathcal{S})\) since it measures only the joint information gain of \(n\) samples within \(\mathcal{S}\). Instead, the prevalence of synergies is captured by the sequence \(\Gamma_{n}\stackrel{{\mathrm{def}}}{{=}}\max_{\bm{x}\in\mathcal{S }}\Delta_{\mathcal{A}}(\bm{x}\mid\bm{x}_{1:n})\), which measures the maximum information gain of \(y_{n+1}\). If \(\Gamma_{n}>\Gamma_{n-1}\) at any round \(n\), this indicates a synergy. The following key object measures the additional complexity due to synergies.

**Definition C.6** (Task complexity).: For \(n\geq 1\), assuming \(\Gamma_{i}>0\) for all \(1\leq i\leq n\), we define the _task complexity_ as

\[\alpha_{\mathcal{A},\mathcal{S}}(n)\stackrel{{\mathrm{def}}}{{= }}\max_{i\in\{0,\ldots,n-1\}}\frac{\Gamma_{n-1}}{\Gamma_{i}}.\]

Note that \(\alpha_{\mathcal{A},\mathcal{S}}(n)\) is large only if the information gain of \(y_{n}\) is larger than that of a previous observation \(y_{i}\). Intuitively, if \(\alpha_{\mathcal{A},\mathcal{S}}(n)\) is large, the agent had to discover the _implicit_ intermediate observations \(y_{1},\ldots,y_{n-1}\) that lead to downstream synergies. We will subsequently formalize the intimate connections of the task complexity to synergies and submodularity. Note that in the GP setting, \(\alpha_{\mathcal{A},\mathcal{S}}(n)\) can be computed online by keeping track of the smallest \(\Gamma_{i}\) during previous rounds \(i\). Further, note that \(\alpha_{\mathcal{A},\mathcal{S}}(n)\leq 1\) if \(\psi_{\mathcal{A}}\) is submodular.

#### c.4.1 The Information Ratio

Another object will prove useful in our analysis of synergies.

Consider an alternative multiplicative interpretation of the multivariate information gain (cf. Equation (7)), which we call the _information ratio_ of \(X\subseteq\mathcal{S}\) given \(D\subseteq\mathcal{S}\), \(|X|,|D|<\infty\):

\[\bar{\kappa}(X\mid D)\stackrel{{\mathrm{def}}}{{= }}\frac{\sum_{\bm{x}\in X}\Delta_{\mathcal{A}}(\bm{x}\mid D)}{\Delta_{ \mathcal{A}}(X\mid D)}\in[0,\infty).\] (8)

Observe that \(\bar{\kappa}(X\mid D)\) measures the synergy properties of \(\bm{y}_{X}\) with respect to \(\bm{f}_{\mathcal{A}}\) given \(\bm{y}_{D}\) in a multiplicative sense. That is, if \(\bar{\kappa}(X\mid D)>1\) then information in \(\bm{y}_{X}\) is redundant, whereas if \(\bar{\kappa}(X\mid D)<1\) then information in \(\bm{y}_{X}\) is synergistic, and if \(\bar{\kappa}(X\mid D)=1\) then \(\bm{y}_{X}\) do not mutually interact with respect to \(\bm{f}_{\mathcal{A}}\) (all given \(\bm{y}_{D}\)). In the degenerate case where \(\Delta_{\mathcal{A}}(X\mid D)=0\) (which implies \(\sum_{\bm{x}\in X}\Delta_{\mathcal{A}}(\bm{x}\mid D)=0\)) we therefore let \(\bar{\kappa}(X\mid D)=1\).

The information ratio of ITL is strictly positive in the Gaussian caseWe prove the following straightforward lower bound to the information ratio of ITL.

**Lemma C.7**.: _Let \(X,D\subseteq\mathcal{S},|X|,|D|<\infty\). If \(\bm{f}_{\mathcal{A}}\) and \(\bm{y}_{X\cup D}\) are jointly Gaussian then \(\bar{\kappa}(X\mid D)>0\)._

Proof.: W.l.o.g. assume \(D=\emptyset\). We let \(X=\{\bm{x}_{1},\ldots,\bm{x}_{k}\}\) and prove lower and upper bound separately. We assume w.l.o.g. that \(\mathrm{I}(\bm{f}_{\mathcal{A}};\bm{y}_{X})>0\) which implies \(|\mathrm{Var}[\bm{f}_{\mathcal{A}}\mid\bm{y}_{X}]|<|\mathrm{Var}[\bm{f}_{ \mathcal{A}}]|\). Thus, there exists some \(i\) such that \(\bm{f}_{\mathcal{A}}\) and \(\bm{y}_{\bm{x}_{i}}\) are dependent, so \(|\mathrm{Var}[\bm{f}_{\mathcal{A}}\mid\bm{y}_{\bm{x}_{i}}]|<|\mathrm{Var}[\bm{f} _{\mathcal{A}}]|\) which implies \(\mathrm{I}(\bm{f}_{\mathcal{A}};y_{\bm{x}_{i}})>0\). We therefore conclude that \(\bar{\kappa}(X)>0\). 

The following example shows that this lower bound is tight.

**Example C.8** (Synergies of Gaussian random variables, inspired by Section 3 of Barrett (2015)).: Consider the three random variables \(X\), \(Y\), and \(Z\) (think \(\mathcal{A}=\{X\}\) and \(\mathcal{S}=\{Y,Z\}\)) which are jointly Gaussian with mean vector \(\mathbf{0}\) and covariance matrix

\[\mathbf{\Sigma}=\begin{bmatrix}1&a&a\\ a&1&0\\ a&0&1\end{bmatrix},\qquad\text{for }2a^{2}<1\]

where the constraint on \(a\) is to ensure that \(\mathbf{\Sigma}\) is positive definite. Computing the mutual information, we have

\[\mathrm{I}(X;Y)=\mathrm{I}(X;Z)=-\frac{1}{2}\log(1-a^{2})\]

and \(\mathrm{I}(X;Y,Z)=-\frac{1}{2}\log(1-2a^{2})\). Therefore,

\[\frac{\mathrm{I}(X;Y)+\mathrm{I}(X;Z)}{\mathrm{I}(X;Y,Z)}=\frac{\log(1-2a^{2}+ a^{4})}{\log(1-2a^{2})}<1.\]

Note that

\[\lim_{a\rightarrow\frac{1}{\sqrt{2}}}\frac{\log(1-2a^{2}+a^{4})}{\log(1-2a^{2 })}=0,\]

and hence -- perhaps unintuitively -- even if \(Y\) and \(Z\) are uncorrelated, their information about \(X\) may be arbitrarily synergistic.

#### c.4.2 The Submodularity of the Special "Undirected" Case of ITL

In the inductive active learning problem considered in most prior works, where \(\mathcal{S}\subseteq\mathcal{A}\) and \(f\) is a Gaussian process, it holds for ITL that \(\alpha_{\mathcal{A},\mathcal{S}}(n)=1\) since all learning targets appear _explicitly_ in \(\mathcal{S}\):

**Lemma C.9**.: _Let \(\mathcal{S}\subseteq\mathcal{A}\). Then \(\psi_{\mathcal{A}}\) of_ ITL _is submodular._

Proof.: Fix any \(\boldsymbol{x}\in\mathcal{S}\) and \(X^{\prime}\subseteq X\subseteq\mathcal{S}\). Let \(\bar{X}\stackrel{{\mathrm{def}}}{{=}}X\setminus X^{\prime}\). By the definition of conditional information gain, we have

\[\Delta_{\mathcal{A}}(\boldsymbol{x}\mid X)=\mathrm{I}(y_{\boldsymbol{x}}; \boldsymbol{f}_{\mathcal{A}}\mid\boldsymbol{y}_{X})=\mathrm{I}(y_{\boldsymbol {x}};\boldsymbol{f}_{\mathcal{A}},\boldsymbol{y}_{X^{\prime}}\mid\boldsymbol{ y}_{\bar{X}})-\mathrm{I}(y_{\boldsymbol{x}};\boldsymbol{y}_{X^{\prime}}\mid \boldsymbol{y}_{\bar{X}}).\]

Since for any \(\boldsymbol{x}\in\mathcal{S}\) and \(X\subseteq\mathcal{S}\), \(y_{\boldsymbol{x}}\perp\boldsymbol{y}_{X}\mid\boldsymbol{f}_{\mathcal{A}}\), this simplifies to

\[\mathrm{I}(y_{\boldsymbol{x}};\boldsymbol{f}_{\mathcal{A}}\mid\boldsymbol{y}_ {X})=\mathrm{I}(y_{\boldsymbol{x}};\boldsymbol{f}_{\mathcal{A}}\mid \boldsymbol{y}_{\bar{X}})-\mathrm{I}(y_{\boldsymbol{x}};\boldsymbol{y}_{X^{ \prime}}\mid\boldsymbol{y}_{\bar{X}}).\]

It then follows from \(\mathrm{I}(y_{\boldsymbol{x}};\boldsymbol{y}_{X^{\prime}}\mid\boldsymbol{y}_ {\bar{X}})\geq 0\) that

\[\Delta_{\mathcal{A}}(\boldsymbol{x}\mid X)=\mathrm{I}(y_{\boldsymbol{x}}; \boldsymbol{f}_{\mathcal{A}}\mid\boldsymbol{y}_{X})\leq\mathrm{I}(y_{ \boldsymbol{x}};\boldsymbol{f}_{\mathcal{A}}\mid\boldsymbol{y}_{\bar{X}})= \Delta_{\mathcal{A}}(\boldsymbol{x}\mid X^{\prime}).\]

This implies that \(\alpha_{\mathcal{A},\mathcal{S}}(n)\leq 1\) for any \(n\) and \(\bar{\kappa}(X\mid D)\geq 1\) for any \(X,D\subseteq\mathcal{S}\) when \(\mathcal{S}\subseteq\mathcal{A}\).

#### c.4.3 The Submodularity Ratio

Building upon the theory of maximizing non-negative monotone submodular functions Nemhauser et al. (1978); Krause and Golovin (2014), Das and Kempe (2018) define the following notion of "approximate" submodularity:

**Definition C.10** (Submodularity ratio).: The _submodularity ratio_ of \(\psi_{\mathcal{A}}\) up to cardinality \(n\geq 1\) is

\[\kappa_{\mathcal{A}}(n)\stackrel{{\mathrm{def}}}{{=}}\min_{ \begin{subarray}{c}D\subseteq\mathcal{A}_{1:n}\\ X\subseteq\mathcal{S}:|X|\leq n\\ D\cap X=\emptyset\end{subarray}}\bar{\kappa}(X\mid D),\] (9)

where they define \(\frac{0}{0}\equiv 1\). \(\psi_{\mathcal{A}}\) is said to be \(\kappa\)_-weakly submodular_ for some \(\kappa>0\) if \(\inf_{n\in\mathbb{N}}\kappa_{\mathcal{A}}(n)\geq\kappa\).

As a special case of Theorem 6 from Das and Kempe (2018), applying that \(\psi_{\mathcal{A}}\) is non-negative and monotone, we obtain the following result.

**Theorem C.11** (Das and Kempe (2018)).: _For any \(n\geq 1\), if_ ITL _or_ VTL _selected \(\boldsymbol{x}_{1:n}\), respectively, then_

\[\psi_{\mathcal{A}}(\boldsymbol{x}_{1:n})\geq(1-e^{-\kappa_{\mathcal{A}}(n)}) \max_{\begin{subarray}{c}X\subseteq\mathcal{S}\\ |X|\leq n\end{subarray}}\psi_{\mathcal{A}}(X).\]

If \(\psi_{\mathcal{A}}\) is submodular, it is implied that \(\kappa_{\mathcal{A}}(n)\geq 1\) for all \(n\geq 1\) in which case Theorem C.11 recovers Theorem C.4.

### Convergence of Marginal Gain

Our following analysis allows for changing target spaces \(\mathcal{A}_{n}\) and sample spaces \(\mathcal{S}_{n}\) (cf. Section 5), and to this end, we redefine \(\Gamma_{n}\stackrel{{\mathrm{def}}}{{=}}\max_{\mathbf{x}\in \mathcal{S}_{n}}\Delta_{\mathcal{A}_{n}}(\mathbf{x}\mid\mathbf{x}_{1:n})\). The following theorems show that the marginal gains of ITL and VTL converge to zero, and will serve as the main tool for establishing Theorems 3.2 and 3.3. We will abbreviate \(\alpha_{\mathcal{A},\mathcal{S}}(n)\) by \(\alpha_{n}\).

**Theorem C.12** (Convergence of Marginal Gain for ITL).: _Assume that Assumptions B.1 and B.2 are satisfied. Fix any integers \(n_{1}>n_{0}\geq 0\), \(\Delta=n_{1}-n_{0}+1\) such that for all \(i\in\{n_{0},\ldots,n_{1}-1\}\), \(\mathcal{A}_{i+1}\subseteq\mathcal{A}_{i}\) and \(\mathcal{S}\stackrel{{\mathrm{def}}}{{=}}\mathcal{S}_{i+1}= \mathcal{S}_{i}\). Further, assume \(|\mathcal{A}_{n_{0}}|<\infty\). Then, if the sequence \(\{\mathbf{x}_{i+1}\}_{i=n_{0}}^{n_{1}}\) was generated by ITL,_

\[\Gamma_{n_{1}}\leq\alpha_{n_{1}}\frac{\gamma_{\Delta}}{\Delta}.\] (10)

_Moreover, if \(n_{0}=0\),_

\[\Gamma_{n_{1}}\leq\alpha_{n_{1}}\frac{\gamma_{\mathcal{A}_{0}, \mathcal{S}}(\Delta)}{\Delta}.\] (11)

Proof.: We have

\[\Gamma_{n_{1}} =\frac{1}{\Delta}\sum_{i=n_{0}}^{n_{1}}\Gamma_{n_{1}}\] \[\stackrel{{(i)}}{{\leq}}\frac{\alpha_{n_{1}}}{ \Delta}\sum_{i=n_{0}}^{n_{1}}\Gamma_{i}\] \[=\frac{\alpha_{n_{1}}}{\Delta}\sum_{i=n_{0}}^{n_{1}}\max_{ \mathbf{x}\in\mathcal{S}}\mathrm{I}(\boldsymbol{f}_{\mathcal{A}_{i}};y_{ \mathbf{x}}\mid\boldsymbol{y}_{1:i})\] \[\stackrel{{(ii)}}{{=}}\frac{\alpha_{n_{1}}}{\Delta} \sum_{i=n_{0}}^{n_{1}}\mathrm{I}(\boldsymbol{f}_{\mathcal{A}_{i}};y_{ \mathbf{x}_{i+1}}\mid\mathcal{D}_{i})\] \[\stackrel{{(iii)}}{{\leq}}\frac{\alpha_{n_{1}}}{ \Delta}\sum_{i=n_{0}}^{n_{1}}\mathrm{I}(\boldsymbol{f}_{\mathcal{A}_{n_{0}}};y _{\mathbf{x}_{i+1}}\mid\mathcal{D}_{i})\] \[\stackrel{{(iv)}}{{=}}\frac{\alpha_{n_{1}}}{\Delta} \sum_{i=n_{0}}^{n_{1}}\mathrm{I}(\boldsymbol{f}_{\mathcal{A}_{n_{0}}};y_{ \mathbf{x}_{i+1}}\mid\boldsymbol{y}_{\mathbf{x}_{n_{0}+1:i}},\mathcal{D}_{n_{ 0}})\] \[\stackrel{{(v)}}{{=}}\frac{\alpha_{n_{1}}}{\Delta} \mathrm{I}(\boldsymbol{f}_{\mathcal{A}_{n_{0}}};\boldsymbol{y}_{\mathbf{x}_{n _{0}+1:n_{1}+1}}\mid\mathcal{D}_{n_{0}})\] \[\leq\frac{\alpha_{n_{1}}}{\Delta}\max_{\begin{subarray}{c}X \subseteq\mathcal{S}\\ |X|=\Delta\end{subarray}}\mathrm{I}(\boldsymbol{f}_{\mathcal{A}_{n_{0}}}; \boldsymbol{y}_{X}\mid\mathcal{D}_{n_{0}})\] \[\stackrel{{(vi)}}{{\leq}}\frac{\alpha_{n_{1}}}{\Delta} \max_{\begin{subarray}{c}X\subseteq\mathcal{S}\\ |X|=\Delta\end{subarray}}\mathrm{I}(\boldsymbol{f}_{X};\boldsymbol{y}_{X})\] \[=\alpha_{n_{1}}\frac{\gamma_{\Delta}}{\Delta}\]

where \((i)\) follows from the definition of the task complexity \(\alpha_{n_{1}}\) (cf. Definition C.6); \((ii)\) uses the objective of ITL and that the posterior variance of Gaussians is independent of the realization and only depends on the _location_ of observations; \((iii)\) uses \(\mathcal{A}_{i+1}\subseteq\mathcal{A}_{i}\) and monotonicity of information gain; \((iv)\) uses that the posterior variance of Gaussians is independent of the realization and only depends on the _location_ of observations; \((v)\) uses the chain rule of information gain; \((vi)\) uses \(\boldsymbol{y}_{X}\perp\boldsymbol{f}_{\mathcal{A}_{n_{0}}}\mid\boldsymbol{f} _{X}\) and the data processing inequality. The conditional independence follows from the assumption that the observation noise is independent. Similarly, \(\boldsymbol{y}_{X}\perp\mathcal{D}_{n_{0}}\mid\boldsymbol{f}_{X}\) which implies \((vii)\). If \(n_{0}=0\), then the bound before line \((vi)\) simplifies to \(\alpha_{n_{1}}\gamma_{\mathcal{A}_{0},\mathcal{S}}(\Delta)/\Delta\).

The result for VTL is stated, for simplicity, only for the case where the target space and sample space are fixed.

**Theorem C.13** (Convergence of Marginal Gain for VTL).: _Assume that Assumptions B.1 and B.2 are satisfied. Then for any \(n\geq 1\), if the sequence \(\{\bm{x}_{i}\}_{i=1}^{n}\) is generated by VTL,_

\[\Gamma_{n-1}\leq\frac{2\sigma^{2}\alpha_{n}}{n}\sum_{\bm{x}^{\prime}\in\mathcal{ A}}\gamma_{\{\bm{x}^{\prime}\},\mathcal{S}}(n).\] (12)

We remark that \(\sum_{\bm{x}^{\prime}\in\mathcal{A}}\gamma_{\{\bm{x}^{\prime}\},\mathcal{S}}(n) \leq|\mathcal{A}|\gamma_{\mathcal{A},\mathcal{S}}(n)\).

Proof.: We have

\[\Gamma_{n-1} =\frac{1}{n}\sum_{i=0}^{n-1}\Gamma_{n-1}\] \[\overset{(i)}{\leq}\frac{\alpha_{n}}{n}\sum_{i=0}^{n-1}\Gamma_{i}\] \[=\frac{\alpha_{n}}{n}\sum_{i=0}^{n-1}\left[\operatorname{tr} \operatorname{Var}[\bm{f}_{\mathcal{A}}\mid\bm{y}_{1:i}]-\min_{\bm{x}\in \mathcal{S}}\operatorname{tr}\operatorname{Var}[\bm{f}_{\mathcal{A}}\mid\bm{y }_{1:i},y_{\bm{x}}]\right]\] \[\overset{(ii)}{=}\frac{\alpha_{n}}{n}\sum_{i=0}^{n-1}\left[ \operatorname{tr}\operatorname{Var}[\bm{f}_{\mathcal{A}}\mid\mathcal{D}_{i}]- \operatorname{tr}\operatorname{Var}[\bm{f}_{\mathcal{A}}\mid\mathcal{D}_{i+1} ]\right]\] \[\overset{(iii)}{\leq}\frac{\sigma^{2}\alpha_{n}}{n}\sum_{\bm{x}^{ \prime}\in\mathcal{A}}\sum_{i=0}^{n-1}\log\biggl{(}\frac{\operatorname{Var}[f _{\bm{x}^{\prime}}\mid\mathcal{D}_{n}]}{\operatorname{Var}[f_{\bm{x}^{\prime} }\mid\mathcal{D}_{n+1}]}\biggr{)}\] \[=\frac{2\sigma^{2}\alpha_{n}}{n}\sum_{\bm{x}^{\prime}\in\mathcal{ A}}\sum_{i=0}^{n-1}\mathrm{I}\bigl{(}f_{\bm{x}^{\prime}};y_{\bm{x}_{n+1}}\bigm{|} \mathcal{D}_{n}\bigr{)}\] \[\overset{(iv)}{=}\frac{2\sigma^{2}\alpha_{n}}{n}\sum_{\bm{x}^{ \prime}\in\mathcal{A}}\sum_{i=0}^{n-1}\mathrm{I}\bigl{(}f_{\bm{x}^{\prime}};y_{ \bm{x}_{n+1}}\bigm{|}\bm{y}_{\bm{x}_{1:n}}\bigr{)}\] \[\overset{(v)}{=}\frac{2\sigma^{2}\alpha_{n}}{n}\sum_{\bm{x}^{ \prime}\in\mathcal{A}}\mathrm{I}\bigl{(}f_{\bm{x}^{\prime}};\bm{y}_{\bm{x}_{1:n }}\bigr{)}\] \[\leq\frac{2\sigma^{2}\alpha_{n}}{n}\sum_{\bm{x}^{\prime}\in \mathcal{A}}\max_{\begin{subarray}{c}X\subset\mathcal{S}\\ |X|=n\end{subarray}}\mathrm{I}\bigl{(}f_{\bm{x}^{\prime}};\bm{y}_{X}\bigr{)}\] \[=\frac{2\sigma^{2}\alpha_{n}}{n}\sum_{\bm{x}^{\prime}\in\mathcal{A} }\gamma_{\{\bm{x}^{\prime}\},\mathcal{S}}(n)\]

where \((i)\) follows from the definition of the task complexity \(\alpha_{n_{1}}\) (cf. Definition C.6); \((ii)\) follows from the VTL decision rule and that the posterior variance of Gaussians is independent of the realization and only depends on the _location_ of observations; \((iii)\) follows from Lemma C.38 and monotonicity of variance; \((iv)\) uses that the posterior variance of Gaussians is independent of the realization and only depends on the _location_ of observations; and \((v)\) uses the chain rule of mutual information. The remainder of the proof is analogous to the proof of Theorem C.12 (cf. Appendix C.5). 

Keeping track of the task complexity onlineIn general, the task complexity \(\alpha_{n}\) may be larger than one in the "directed" setting (i.e., when \(\mathcal{S}\not\subseteq\mathcal{A}\)). However, note that \(\alpha_{n}\) can easily be evaluated online by keeping track of the smallest \(\Gamma_{i}\) during previous rounds \(i\).

### Proof of Theorem 3.2

We will now prove Theorem 3.2. We first prove the convergence of marginal variance within \(\mathcal{S}\) for ITL, before proving the convergence outside \(\mathcal{S}\) in Appendix C.6.1.

**Lemma C.14** (Uniform convergence of marginal variance within \(\mathcal{S}\) for ITL).: _Assume that Assumptions B.1 and B.2 are satisfied. For any \(n\geq 0\) and \(\bm{x}\in\mathcal{A}\cap\mathcal{S}\),_

\[\sigma_{n}^{2}(\bm{x})\leq 2\tilde{\sigma}^{2}\cdot\Gamma_{n}.\] (13)

Proof.: We have

\[\sigma_{n}^{2}(\bm{x}) =\mathrm{Var}[f_{\bm{x}}\mid\mathcal{D}_{n}]-\underbrace{ \mathrm{Var}[f_{\bm{x}}\mid f_{\bm{x}},\mathcal{D}_{n}]}_{0}\] \[\overset{(i)}{=}\mathrm{Var}[y_{\bm{x}}\mid\mathcal{D}_{n}]- \rho^{2}(\bm{x})-(\mathrm{Var}[y_{\bm{x}}\mid f_{\bm{x}},\mathcal{D}_{n}]-\rho ^{2}(\bm{x}))\] \[=\mathrm{Var}[y_{\bm{x}}\mid\mathcal{D}_{n}]-\mathrm{Var}[y_{\bm{ x}}\mid f_{\bm{x}},\mathcal{D}_{n}]\] \[\overset{(ii)}{\leq}\tilde{\sigma}^{2}\log\biggl{(}\frac{\mathrm{ Var}[y_{\bm{x}}\mid\mathcal{D}_{n}]}{\mathrm{Var}[y_{\bm{x}}\mid f_{\bm{x}}, \mathcal{D}_{n}]}\biggr{)}\] \[=2\tilde{\sigma}^{2}\cdot\mathrm{I}(f_{\bm{x}};y_{\bm{x}}\mid \mathcal{D}_{n})\] \[\overset{(iii)}{\leq}2\tilde{\sigma}^{2}\cdot\max_{\bm{x}^{\prime }\in\mathcal{S}}\mathrm{I}(f_{\bm{\mathcal{A}}};y_{\bm{x}^{\prime}}\mid \mathcal{D}_{n})\] \[\overset{(v)}{=}2\tilde{\sigma}^{2}\cdot\max_{\bm{x}^{\prime}\in \mathcal{S}}\mathrm{I}(f_{\bm{\mathcal{A}}};y_{\bm{x}^{\prime}}\mid\bm{y}_{1:n})\] \[=2\tilde{\sigma}^{2}\cdot\Gamma_{n}\]

where \((i)\) follows from the noise assumption (cf. Assumption B.2); \((ii)\) follows from Lemma C.38 and using monotonicity of variance; \((iii)\) follows from \(\bm{x}\in\mathcal{A}\) and monotonicity of information gain; \((iv)\) follows from \(\bm{x}\in\mathcal{S}\); and \((v)\) uses that the posterior variance of Gaussians is independent of the realization and only depends on the _location_ of observations. 

#### c.6.1 Convergence outside \(\mathcal{S}\) for ITL

We will now show convergence of marginal variance to the irreducible uncertainty for points outside the sample space.

Our proof roughly proceeds as follows: We construct an "approximate Markov boundary" of \(\bm{x}\) in \(\mathcal{S}\), and show (1) that the size of this Markov boundary is independent of \(n\), and (2) that a small uncertainty reduction within the Markov boundary implies that the marginal variances at the Markov boundary and(!) \(\bm{x}\) are small.

**Definition C.15** (Approximate Markov boundary).: For any \(\epsilon>0\), \(n\geq 0\), and \(\bm{x}\in\mathcal{X}\), we denote by \(B_{n,\epsilon}(\bm{x})\) the smallest (multi-)subset of \(\mathcal{S}\) such that

\[\mathrm{Var}[f_{\bm{x}}\mid\mathcal{D}_{n},\bm{y}_{B_{n,\epsilon}(\bm{x})}] \leq\eta_{\mathcal{S}}^{2}(\bm{x})+\epsilon.\] (14)

We call \(B_{n,\epsilon}(\bm{x})\) an \(\epsilon\)_-approximate Markov boundary_ of \(\bm{x}\) in \(\mathcal{S}\).

Equation (14) is akin to the notion of the smallest Markov blanket in \(\mathcal{S}\) of some \(\bm{x}\in\mathcal{X}\) (called a _Markov boundary_) which is the smallest set \(\mathcal{B}\subseteq\mathcal{S}\) such that \(f_{\bm{x}}\perp f_{\mathcal{S}}\mid f_{\mathcal{B}}\).

**Lemma C.16** (Existence of an approximate Markov boundary).: _For any \(\epsilon>0\), let \(k\) be the smallest integer satisfying_

\[\frac{\gamma_{k}}{k}\leq\frac{\epsilon\lambda_{\min}(\bm{K}_{\mathcal{S}\mathcal{ S}})}{2|\mathcal{S}|\sigma^{2}\tilde{\sigma}^{2}}.\] (15)

_Then, for any \(n\geq 0\) and \(\bm{x}\in\mathcal{X}\), there exists an \(\epsilon\)-approximate Markov boundary \(B_{n,\epsilon}(\bm{x})\) of \(\bm{x}\) in \(\mathcal{S}\) with size at most \(k\)._

Lemma C.16 shows that for any \(\epsilon>0\) there exists a universal constant \(b_{\epsilon}\) (with respect to \(n\) and \(\bm{x}\)) such that

\[|B_{n,\epsilon}(\bm{x})|\leq b_{\epsilon}\qquad\forall n\geq 0,\bm{x}\in \mathcal{X}.\] (16)

We defer the proof of Lemma C.16 to Appendix C.6.3 where we also provide an algorithm to compute \(B_{n,\epsilon}(\bm{x})\).

**Lemma C.17**.: _For any \(\epsilon>0\), \(n\geq 0\), and \(\bm{x}\in\mathcal{X}\),_

\[\sigma_{n}^{2}(\bm{x})\leq 2\sigma^{2}\cdot\mathrm{I}(f_{\bm{x}};\bm{y}_{B_{n, \epsilon}(\bm{x})}\mid\mathcal{D}_{n})+\eta_{\mathcal{S}}^{2}(\bm{x})+\epsilon\] (17)

_where \(B_{n,\epsilon}(\bm{x})\) is an \(\epsilon\)-approximate Markov boundary of \(\bm{x}\) in \(\mathcal{S}\)._

Proof.: We have

\[\sigma_{n}^{2}(\bm{x}) =\mathrm{Var}[f_{\bm{x}}\mid\mathcal{D}_{n}]-\eta_{\mathcal{S}}^ {2}(\bm{x})+\eta_{\mathcal{S}}^{2}(\bm{x})\] \[\overset{(i)}{\leq}\mathrm{Var}[f_{\bm{x}}\mid\mathcal{D}_{n}]- \mathrm{Var}[f_{\bm{x}}\mid\bm{y}_{B_{n,\epsilon}(\bm{x})},\mathcal{D}_{n}]+ \eta_{\mathcal{S}}^{2}(\bm{x})+\epsilon\] \[\overset{(ii)}{\leq}\sigma^{2}\log\Biggl{(}\frac{\mathrm{Var}[f_ {\bm{x}}\mid\mathcal{D}_{n}]}{\mathrm{Var}[f_{\bm{x}}\mid\bm{y}_{B_{n,\epsilon }(\bm{x})},\mathcal{D}_{n}]}\Biggr{)}+\eta_{\mathcal{S}}^{2}(\bm{x})+\epsilon\] \[=2\sigma^{2}\cdot\mathrm{I}(f_{\bm{x}};\bm{y}_{B_{n,\epsilon}(\bm {x})}\mid\mathcal{D}_{n})+\eta_{\mathcal{S}}^{2}(\bm{x})+\epsilon\]

where \((i)\) follows from the defining property of an \(\epsilon\)-approximate Markov boundary (cf. Equation (14)); and \((ii)\) follows from Lemma C.38 and using monotonicity of variance. 

**Lemma C.18**.: _For any \(\epsilon>0,n\geq 0\), and \(\bm{x}\in\mathcal{A}\),_

\[\mathrm{I}(f_{\bm{x}};\bm{y}_{B_{n,\epsilon}(\bm{x})}\mid\mathcal{D}_{n})\leq \frac{b_{\epsilon}}{\bar{\kappa}_{n}(B_{n,\epsilon}(\bm{x}))}\Gamma_{n}\] (18)

_where \(B_{n,\epsilon}(\bm{x})\) is an \(\epsilon\)-approximate Markov boundary of \(\bm{x}\) in \(\mathcal{S}\), \(|B_{n,\epsilon}(\bm{x})|\leq b_{\epsilon}\), and where \(\bar{\kappa}_{n}(\cdot)\overset{\mathrm{def}}{=}\bar{\kappa}(\cdot\mid\bm{x}_ {1:n})\) denotes the information ratio from Equation (8)._

We remark that \(\bar{\kappa}_{n}(\cdot)>0\) as is shown in Lemma C.7, and hence, the right-hand side of the inequality is well-defined.

Proof.: We use the abbreviated notation \(B=B_{n,\epsilon}(\bm{x})\). We have

\[\mathrm{I}(f_{\bm{x}};\bm{y}_{B}\mid\mathcal{D}_{n}) \overset{(i)}{\leq}\mathrm{I}(\bm{f}_{\mathcal{A}};\bm{y}_{B} \mid\mathcal{D}_{n})\] \[\overset{(ii)}{\leq}\frac{1}{\bar{\kappa}_{n,b_{\epsilon}}}\sum_{ \tilde{\bm{x}}\in B}\mathrm{I}(\bm{f}_{\mathcal{A}};y_{\tilde{\bm{x}}}\mid \mathcal{D}_{n})\] \[\overset{(iii)}{\leq}\frac{b_{\epsilon}}{\bar{\kappa}_{n,b_{ \epsilon}}}\max_{\tilde{\bm{x}}\in B}\mathrm{I}(\bm{f}_{\mathcal{A}};y_{ \tilde{\bm{x}}}\mid\mathcal{D}_{n})\] \[\overset{(iv)}{=}\frac{b_{\epsilon}}{\bar{\kappa}_{n,b_{\epsilon }}}\max_{\tilde{\bm{x}}\in S}\mathrm{I}(\bm{f}_{\mathcal{A}};y_{\tilde{\bm{x}}} \mid\mathcal{y}_{1:n})\] \[=\frac{b_{\epsilon}}{\bar{\kappa}_{n,b_{\epsilon}}}\Gamma_{n}\]

where \((i)\) follows from monotonicity of mutual information; \((ii)\) follows from the definition of the information ratio \(\bar{\kappa}_{n,b_{\epsilon}}\) (cf. Equation (8)); \((iii)\) follows from \(b\leq b_{\epsilon}\); \((iv)\) follows from \(B\subseteq\mathcal{S}\); and \((v)\) uses that the posterior variance of Gaussians is independent of the realization and only depends on the _location_ of observations. 

Proof of Theorem 3.2 for \(\mathrm{ITL}\).: The case where \(\bm{x}\in\mathcal{A}\cap\mathcal{S}\) is shown by Lemma C.14 with \(C=2\tilde{\sigma}^{2}\).

To prove the more general result, fix any \(\bm{x}\in\mathcal{A}\) and \(\epsilon>0\). By Lemma C.16, there exists an \(\epsilon\)-approximate Markov boundary \(B_{n,\epsilon}(\bm{x})\) of \(\bm{x}\) in \(\mathcal{S}\) such that \(|B_{n,\epsilon}(\bm{x})|\leq b_{\epsilon}\). We have

\[\sigma_{n}^{2}(\bm{x}) \overset{(i)}{\leq}2\sigma^{2}\cdot\mathrm{I}(f_{\bm{x}};\bm{y}_ {B_{n,\epsilon}(\bm{x})}\mid\mathcal{D}_{n})+\eta_{\mathcal{S}}^{2}(\bm{x})+\epsilon\] \[\overset{(ii)}{\leq}\frac{2\sigma^{2}b_{\epsilon}}{\bar{\kappa}_ {n}(B_{n,\epsilon}(\bm{x}))}\Gamma_{n}+\eta_{\mathcal{S}}^{2}(\bm{x})+\epsilon\]where \((i)\) follows from Lemma C.17; and \((ii)\) follows from Lemma C.18.

Let \(\epsilon=c\frac{\gamma_{\sqrt{n}}}{\sqrt{n}}\) with \(c=2|\mathcal{S}|\sigma^{2}\tilde{\sigma}^{2}/\lambda_{\min}(\bm{K}_{\mathcal{S }\mathcal{S}})\). Then, by Equation (15), \(b_{\epsilon}\) can be bounded for instance by \(\sqrt{n}\). Together with Theorem C.12 this implies for ITL that

\[\sigma_{n}^{2}(\bm{x}) \leq\eta_{\mathcal{S}}^{2}(\bm{x})+2\sigma^{2}\sqrt{n}\,\Gamma_{ n}+c\gamma_{\sqrt{n}}/\sqrt{n}\] \[\leq\eta_{\mathcal{S}}^{2}(\bm{x})+c^{\prime}\gamma_{n}/\sqrt{n}\]

for a constant \(c^{\prime}\), e.g., \(c^{\prime}=2\sigma^{2}+c\). 

#### c.6.2 Convergence outside \(\mathcal{S}\) for VTL

Proof of Theorem 3.2 for VTL.: Analogously to Lemma C.17, we have

\[\sigma_{n}^{2}(\bm{x}) =\mathrm{Var}[f_{\bm{x}}\mid\mathcal{D}_{n}]-\eta_{\mathcal{S}}^{ 2}(\bm{x})+\eta_{\mathcal{S}}^{2}(\bm{x})\] \[\overset{(i)}{\leq}\mathrm{Var}[f_{\bm{x}}\mid\mathcal{D}_{n}]- \mathrm{Var}[f_{\bm{x}}\mid\bm{y}_{B_{n,\epsilon}(\bm{x})},\mathcal{D}_{n}]+ \eta_{\mathcal{S}}^{2}(\bm{x})+\epsilon\]

where \((i)\) follows from the defining property of an \(\epsilon\)-approximate Markov boundary (cf. Equation (14)). Further, we have

\[\mathrm{Var}[f_{\bm{x}}\mid\mathcal{D}_{n}]-\mathrm{Var}[f_{\bm{x }}\mid\bm{y}_{B_{n,\epsilon}(\bm{x})},\mathcal{D}_{n}]\] \[\overset{(i)}{\leq}\sum_{\tilde{\bm{x}}\in B_{n,\epsilon}(\bm{x} )}(\mathrm{Var}[f_{\bm{x}}\mid\mathcal{D}_{n}]-\mathrm{Var}[f_{\bm{x}}\mid y_{ \tilde{\bm{x}}},\mathcal{D}_{n}])\] \[\overset{(ii)}{\leq}\sum_{\tilde{\bm{x}}\in B_{n,\epsilon}(\bm{x} )}(\mathrm{tr}\;\mathrm{Var}[\bm{f_{\mathcal{A}}}\mid\bm{y}_{1:n}]-\mathrm{tr }\;\mathrm{Var}[\bm{f_{\mathcal{A}}}\mid y_{\tilde{\bm{x}}},\bm{y}_{1:n}])\] \[\overset{(iii)}{\leq}b_{\epsilon}\Gamma_{n}\]

where \((i)\) follows from the submodularity of \(\psi_{\mathcal{A}}\); \((ii)\) uses that the posterior variance of Gaussians is independent of the realization and only depends on the _location_ of observations; and \((iii)\) follows from the definition of \(\Gamma_{n}\) and Lemma C.16.

The remainder of the proof is analogous to the result for ITL, using Theorem C.13 to bound \(\Gamma_{n}\). 

#### c.6.3 Existence of an Approximate Markov Boundary

We now derive Lemma C.16 which shows the existence of an approximate Markov boundary of \(\bm{x}\) in \(\mathcal{S}\).

**Lemma C.19**.: _For any \(S\subseteq\mathcal{S}\) and \(k\geq 0\), there exists \(B\subseteq S\) with \(|B|=k\) such that for all \(\bm{x^{\prime}}\in S\),_

\[\mathrm{Var}[f_{\bm{x^{\prime}}}\mid\bm{y}_{B}]\leq 2\tilde{\sigma}^{2} \frac{\gamma_{k}}{k}.\] (19)

Proof.: We choose \(B\subseteq S\) greedily using the acquisition function

\[\tilde{\bm{x}}_{k}\overset{\mathrm{def}}{=}\operatorname*{arg\,max}_{\tilde{ \bm{x}}\in S}\mathrm{I}(\bm{f}_{\mathcal{S}};y_{\tilde{\bm{x}}}\mid\bm{y}_{B_{ k-1}})\]

where \(B_{k}=\tilde{\bm{x}}_{1:k}\). Note that this is the "undirected" special case of ITL, and hence, we have

\[\mathrm{Var}\big{[}f_{\bm{x^{\prime}}}\mid\bm{y}_{B_{k}}\big{]} \overset{(i)}{\leq}2\tilde{\sigma}^{2}\Gamma_{k}\] \[\overset{(ii)}{\leq}2\tilde{\sigma}^{2}\frac{\gamma_{k}}{k}\]

where \((i)\) is due to Lemma C.14; and \((ii)\) is due to Theorem C.12 and \(\alpha_{S,S}(k)\leq 1\). 

**Lemma C.20**.: _Given any \(\epsilon>0\) and \(B\subseteq S\subseteq\mathcal{S}\) with \(|S|<\infty\), such that for any \(\bm{x^{\prime}}\in S\),_

\[\mathrm{Var}[f_{\bm{x^{\prime}}}\mid\bm{y}_{B}]\leq\frac{\epsilon\lambda_{\min} (\bm{K}_{SS})}{|S|\sigma^{2}}.\] (20)

_Then for any \(\bm{x}\in\mathcal{X}\),_

\[\mathrm{Var}[f_{\bm{x}}\mid\bm{y}_{B}]\leq\mathrm{Var}[f_{\bm{x}}\mid\bm{f}_{ \mathcal{S}}]+\epsilon.\] (21)Proof.: We will denote the right-hand side of Equation (20) by \(\epsilon^{\prime}\). We have

\[\mathrm{Var}[f_{\bm{x}}\mid\bm{y}_{B}]\] \[\stackrel{{(i)}}{{=}}\mathbb{E}_{\bm{f}_{S}}[\mathrm{ Var}_{f_{\bm{x}}}[f_{\bm{x}}\mid\bm{f}_{\bm{S}},\bm{y}_{B}]\mid\bm{y}_{B}]\] \[\quad\quad+\mathrm{Var}_{\bm{f}_{\bm{S}}}[\mathbb{E}_{f_{\bm{x}}}[ f_{\bm{x}}\mid\bm{f}_{\bm{S}},\bm{y}_{B}]\mid\bm{y}_{B}]\] \[\stackrel{{(ii)}}{{=}}\mathrm{Var}_{f_{\bm{x}}}[ \bm{f}_{\bm{x}}\mid\bm{f}_{\bm{S}},\bm{y}_{B}]+\mathrm{Var}_{\bm{f}_{\bm{S}}}[ \mathbb{E}_{f_{\bm{x}}}[f_{\bm{x}}\mid\bm{f}_{\bm{S}},\bm{y}_{B}]\mid\bm{y}_{B}]\] \[\stackrel{{(iii)}}{{=}}\underbrace{\mathrm{Var}_{f_{ \bm{x}}}[f_{\bm{x}}\mid\bm{f}_{\bm{S}}]}_{\text{reducible uncertainty}}+ \underbrace{\mathrm{Var}_{\bm{f}_{\bm{S}}}[\mathbb{E}_{f_{\bm{x}}}[f_{\bm{x}} \mid\bm{f}_{\bm{S}}]\mid\bm{y}_{B}]}_{\text{reducible (epistemic) uncertainty}}\]

where \((i)\) follows from the law of total variance; \((ii)\) uses that the conditional variance of a Gaussian depends only on the location of observations and not on their value; and \((iii)\) follows from \(f_{\bm{x}}\perp\bm{y}_{B}\mid\bm{f}_{\bm{S}}\) since \(B\subseteq S\). It remains to bound the reducible uncertainty.

Let \(h_{\bm{x}}:\mathbb{R}^{d}\to\mathbb{R},\ \bm{f}_{\bm{S}}\mapsto\mathbb{E}[f_{\bm{x}} \mid\bm{f}_{\bm{S}}]\) where we write \(d\stackrel{{\mathrm{def}}}{{=}}|S|\). Using the formula for the GP posterior mean, we have

\[h_{\bm{x}}(\bm{f}_{\bm{S}})=\mathbb{E}[f_{\bm{x}}]+\bm{z}^{\top}(\bm{f}_{\bm{ S}}-\mathbb{E}[\bm{f}_{\bm{S}}])\]

where \(\bm{z}\stackrel{{\mathrm{def}}}{{=}}\bm{K}_{SS}^{-1}\bm{K}_{S\bm {x}}\). Because \(h\) is a linear function in \(\bm{f}_{\bm{S}}\) we have for the reducible uncertainty that

\[\mathrm{Var}_{\bm{f}_{\bm{S}}}[h_{\bm{x}}(\bm{f}_{\bm{S}})\mid\bm {y}_{B}] =\bm{z}^{\top}\mathrm{Var}[\bm{f}_{\bm{S}}\mid\bm{y}_{B}]\bm{z}\] \[\stackrel{{(i)}}{{\leq}}d\cdot\bm{z}^{\top}\mathrm{ diag}\,\mathrm{Var}[\bm{f}_{\bm{S}}\mid\bm{y}_{B}]\bm{z}\] \[\stackrel{{(ii)}}{{\leq}}\epsilon^{\prime}d\ \bm{z}^{\top}\bm{z}\] \[=\epsilon^{\prime}d\ \bm{K}_{\bm{x}S}\bm{K}_{S\bm{S}}^{-1}\bm{K}_{S\bm{S}}^{-1}\bm{K}_{S\bm {x}}\] \[\leq\frac{\epsilon^{\prime}d}{\lambda_{\min}(\bm{K}_{SS})}\bm{K}_ {\bm{x}S}\bm{K}_{SS}^{-1}\bm{K}_{S\bm{x}}\] \[\stackrel{{(iii)}}{{\leq}}\frac{\epsilon^{\prime}d \sigma^{2}}{\lambda_{\min}(\bm{K}_{SS})}\]

where \((i)\) follows from Lemma C.37; \((ii)\) follows from the assumption that \(\mathrm{Var}[f_{\bm{x}^{\prime}}\mid\bm{y}_{B}]\leq\epsilon^{\prime}\) for all \(\bm{x}^{\prime}\in S\); and \((iii)\) follows from

\[\bm{K}_{\bm{x}S}\bm{K}_{SS}^{-1}\bm{K}_{S\bm{x}}\leq\bm{K}_{\bm{x}\bm{x}}= \sigma^{2}\]

since \(\bm{K}_{\bm{x}\bm{x}}-\bm{K}_{\bm{x}S}\bm{K}_{SS}^{-1}\bm{K}_{S\bm{x}}\geq 0\). 

Proof of Lemma c.16.: Let \(B\subseteq\mathcal{S}\) be the set of size \(k\) generated by Lemma C.19 to satisfy \(\mathrm{Var}[f_{\bm{x}^{\prime}}\mid\bm{y}_{B}]\leq 2\bar{\sigma}^{2}\gamma_{k}/k\) for all \(\bm{x}^{\prime}\in\mathcal{S}\). We have for any \(\bm{x}\in\mathcal{X}\),

\[\mathrm{Var}[f_{\bm{x}}\mid\mathcal{D}_{n},\bm{y}_{B}] \stackrel{{(i)}}{{\leq}}\mathrm{Var}[f_{\bm{x}}\mid \bm{y}_{B}]\] \[\stackrel{{(ii)}}{{\leq}}\mathrm{Var}[f_{\bm{x}}\mid \bm{f}_{\bm{S}}]+\epsilon\]

where \((i)\) follows from monotonicity of variance; and \((ii)\) follows from Lemma C.20; using \(|\mathcal{S}|<\infty\) and the condition on \(k\). 

We remark that Lemma C.19 provides an algorithm (just "undirected" ITL!) to compute an approximate Markov boundary, and the set \(B\) returned by this algorithm is a valid approximate Markov boundary for all \(\bm{x}\in\mathcal{X}\). One can simply swap-in ITL with target space \(\{\bm{x}\}\) for "undirected" ITL to obtain tighter (but instance-dependent) bounds on the size of the approximate Markov boundary.

#### c.6.4 Generalization to Continuous \(\mathcal{S}\) for Finite Dimensional RKHSs

In this subsection we generalize Theorem 3.2 to continuous sample spaces \(\mathcal{S}\). We will make the following assumption:

**Assumption C.21**.: The RKHS of the kernel \(k\) is finite dimensional. In other words, the kernel \(k\) can be expressed as \(k(\bm{x},\bm{x}^{\prime})=\bm{\phi}(\bm{x})^{\top}\bm{\phi}(\bm{x}^{\prime})\) for some feature map \(\bm{\phi}:\mathcal{X}\to\mathbb{R}^{d}\) with \(d<\infty\).

In the following, we will denote the design matrix of the sample space \(\mathcal{S}\) by \(\bm{\Phi}\mathop{=}\limits^{\mathrm{def}}[\bm{\phi}(\bm{x}):\bm{x}\in\mathcal{ S}]^{\top}\in\mathbb{R}^{|\mathcal{S}|\times d}\), and we denote by \(\bm{\Pi}_{\bm{\Phi}}\) its orthogonal projection onto the orthogonal complement of the span of \(\bm{\Phi}\). In particular, it holds that

1. \(\bm{\Pi}_{\bm{\Phi}}\bm{v}=\bm{0}\) for all \(\bm{v}\in\operatorname{span}\bm{\Phi}\), and
2. \(\bm{\Pi}_{\bm{\Phi}}\bm{v}=\bm{v}\) for all \(\bm{v}\in(\operatorname{span}\bm{\Phi})^{\perp}\).

Especially, \(\bm{v}\in\ker\bm{\Pi}_{\bm{\Phi}}\) if and only if \(\bm{v}\in\operatorname{span}\bm{\Phi}\). This projection can be computed as follows:

**Lemma C.22**.: _It holds that_

\[\bm{\Pi}_{\bm{\Phi}}=\bm{I}-\bm{\Phi}^{\top}(\bm{\Phi}\bm{\Phi}^{\top})^{-1} \bm{\Phi}.\] (22)

Proof.: \(\bm{\Phi}^{\top}(\bm{\Phi}\bm{\Phi}^{\top})^{-1}\bm{\Phi}\) is the orthogonal projection onto the span of \(\bm{\Phi}\)(see, e.g., Strang, 2016, page 211). 

**Lemma C.23**.: _Under Assumption C.21, the irreducible uncertainty \(\eta^{2}_{\mathcal{S}}(\bm{x})\) of \(\bm{x}\in\mathcal{X}\) is_

\[\eta^{2}_{\mathcal{S}}(\bm{x})=\left\|\bm{\phi}(\bm{x})\right\|^{2}_{\bm{\Pi} _{\bm{\Phi}}}\] (23)

_where \(\left\|\bm{v}\right\|_{\bm{A}}=\sqrt{\bm{v}^{\top}\bm{A}\bm{v}}\) denotes the Mahalanobis distance._

Proof.: This is an immediate consequence of the formula for the conditional variance of multivariate Gaussians (cf. Appendix B.2), applied to the linear kernel. 

Lemmas C.22 and C.23 imply that \(\eta^{2}_{\mathcal{S}}(\bm{x}^{\parallel})=0\) for all \(\bm{x}^{\parallel}\in\mathcal{X}\) with \(\bm{\phi}(\bm{x}^{\parallel})\in\operatorname{span}\bm{\Phi}\). That is, the irreducible uncertainty is zero for points in the span of the sample space. In contrast, for points \(\bm{x}^{\perp}\) with \(\bm{\phi}(\bm{x}^{\perp})\in(\operatorname{span}\bm{\Phi})^{\perp}\), the irreducible uncertainty equals the initial uncertainty: \(\eta^{2}_{\mathcal{S}}(\bm{x}^{\perp})=\sigma^{2}_{0}(\bm{x}^{\perp})\). The irreducible uncertainty of any other point \(\bm{x}\) can be computed by simple decomposition of \(\bm{\phi}(\bm{x})\) into parallel and orthogonal components.

Assuming that Assumption C.21 holds and given any (non-finite) \(\mathcal{S}\subseteq\mathcal{X}\), there exists a basis \(\Omega_{\mathcal{S}}\subseteq\mathcal{X}\) in the space of embeddings \(\bm{\phi}(\cdot)\) such that \(\operatorname{span}\mathcal{S}=\operatorname{span}\Omega_{\mathcal{S}}\) and \(\left|\Omega_{\mathcal{S}}\right|\leq d\). The generalized existence of an approximate Markov boundary for continuous domains can then be shown analogously to Lemma C.16:

**Lemma C.24** (Existence of an approximate Markov boundary for a continuous domain).: _Let \(\mathcal{S}\) be any (continuous) subset of \(\mathcal{X}\) and let Assumption C.21 hold with \(d<\infty\). Further, for any \(\epsilon>0\), let \(k\) be the smallest integer satisfying_

\[\frac{\gamma_{k}}{k}\leq\frac{\epsilon\lambda_{\min}(\bm{K}_{\Omega_{\mathcal{ S}}\Omega_{\mathcal{S}}})}{2d\sigma^{2}\tilde{\sigma}^{2}}.\] (24)

_Then, for any \(n\geq 0\) and \(\bm{x}\in\mathcal{X}\), there exists an \(\epsilon\)-approximate Markov boundary \(B_{n,\epsilon}(\bm{x})\) of \(\bm{x}\) in \(\mathcal{S}\) with size at most \(k\)._

Proof sketch.: The proof follows analogously to Lemma C.16 by conditioning on the finite set \(\Omega_{\mathcal{S}}\) as opposed to \(\mathcal{S}\). 

### Proof of Theorem 3.3

We first formalize the assumptions of Theorem 3.3:

**Assumption C.25** (Regularity of \(f^{\star}\)).: We assume that \(f^{\star}\) is in a reproducing kernel Hilbert space \(\mathcal{H}_{k}(\mathcal{X})\) associated with a kernel \(k\) and has bounded norm, that is, \(\left\|f^{\star}\right\|_{k}\leq B\) for some finite \(B\in\mathbb{R}\).

**Assumption C.26** (Sub-Gaussian noise).: We further assume that each \(\varepsilon_{n}\) from the noise sequence \(\{\varepsilon_{n}\}_{n=1}^{\infty}\) is conditionally zero-mean \(\rho(\bm{x}_{n})\)-sub-Gaussian with known constants \(\rho(\bm{x})>0\) for all \(\bm{x}\in\mathcal{X}\). Concretely,

\[\forall n\geq 1,\lambda\in\mathbb{R}:\quad\mathbb{E}\big{[}e^{\lambda\epsilon_{n}} \bigm{|}\mathcal{D}_{n-1}\big{]}\leq\exp\biggl{(}\frac{\lambda^{2}\rho^{2}( \bm{x}_{n})}{2}\biggr{)}\]

where \(\mathcal{D}_{n-1}\) corresponds to the \(\sigma\)-algebra generated by the random variables \(\{\bm{x}_{i},\epsilon_{i}\}_{i=1}^{n-1}\) and \(\bm{x}_{n}\).

We make use of the following foundational result, showing that under the above two assumptions the (misspecified) Gaussian process model from Section 3.1 is an all-time well-calibrated model of \(f^{\star}\):

**Lemma C.27** (Well-calibrated confidence intervals; Abbasi-Yadkori (2013); Chowdhury & Gopalan (2017)).: _Pick \(\delta\in(0,1)\) and let Assumptions C.25 and C.26 hold. Let_

\[\beta_{n}(\delta)=\left\|f^{\star}\right\|_{k}+\rho\sqrt{2(\gamma_{n}+1+\log( 1/\delta))}\]

_where \(\rho=\max_{\bm{x}\in\mathcal{X}}\rho(\bm{x})\).10 Then, for all \(\bm{x}\in\mathcal{X}\) and \(n\geq 0\) jointly with probability at least \(1-\delta\),_

Footnote 10: \(\beta_{n}(\delta)\) can be tightened adaptively (Emmenegger et al., 2023).

\[|f^{\star}(\bm{x})-\mu_{n}(\bm{x})|\leq\beta_{n}(\delta)\cdot\sigma_{n}(\bm{x})\]

_where \(\mu_{n}(\bm{x})\) and \(\sigma_{n}^{2}(\bm{x})\) are mean and variance (as defined in Appendix B.2) of the GP posterior of \(f(\bm{x})\) conditional on the observations \(\mathcal{D}_{n}\), pretending that \(\varepsilon_{i}\) is Gaussian with variance \(\rho^{2}(\bm{x}_{i})\)._

The proof of Theorem 3.3 is a straightforward application of Lemma C.27 and Theorem 3.2:

Proof of Theorem 3.3.: By Theorem 3.2, we have that for all \(\bm{x}\in\mathcal{A}\),

\[\sigma_{n}(\bm{x})\leq\sqrt{\eta_{\mathcal{S}}^{2}(\bm{x})+\nu_{\mathcal{A}, \mathcal{S}}^{2}(n)}\leq\eta_{\mathcal{S}}(\bm{x})+\nu_{\mathcal{A},\mathcal{ S}}(n).\]

The result then follows by application of Lemma C.27. 

### Proof of Theorem 5.1

In this section, we derive our main result on Safe BO. In Appendix C.8.1, we give the definition of the reachable safe set \(\mathcal{R}\) and derive the conditions under which convergence to the reachable safe set is guaranteed. Then, in Appendix C.8.2, we prove Theorem 5.1.

NotationIn the agnostic setting from Section 3.2 (i.e., under Assumptions C.25 and C.26), Lemma C.27 provides us with the following \((1-\delta)\)-confidence intervals (CIs)

\[\mathcal{C}_{n}(\bm{x})\operatorname{\stackrel{{\mathrm{def}}}{{= }}}\mathcal{C}_{n-1}(\bm{x})\cap[\mu_{n}(\bm{x})\pm\beta_{n}(\delta)\cdot\sigma _{n}(\bm{x})]\] (25)

where \(\mathcal{C}_{-1}(\bm{x})=\mathbb{R}\). We write \(u_{n}(\bm{x})\operatorname{\stackrel{{\mathrm{def}}}{{=}}}\max \mathcal{C}_{n}(\bm{x})\), \(l_{n}(\bm{x})\operatorname{\stackrel{{\mathrm{def}}}{{=}}}\min \mathcal{C}_{n}(\bm{x})\), and \(w_{n}(\bm{x})\operatorname{\stackrel{{\mathrm{def}}}{{=}}}u_{n}( \bm{x})-l_{n}(\bm{x})\) for its upper bound, lower bound, and width, respectively.

We learn separate statistical models \(f\) and \(\{g_{1},\dots\operatorname{\stackrel{{\mathrm{def}}}{{=}}}g_{q}\}\) for the ground truth objective \(f^{\star}\) and ground truth constraints \(\{g_{1}^{\star},\dots,g_{q}^{\star}\}\). We write \(\mathcal{I}=\{f,1,\dots,q\}\) and collect the constraints in \(\mathcal{I}_{s}\operatorname{\stackrel{{\mathrm{def}}}{{=}}}\{1, \dots,q\}\). Without loss of generality, we assume that the confidence intervals include the ground truths with probability at least \(1-\delta\) jointly for all \(i\in\mathcal{I}\).11 For \(i\in\mathcal{I}\), denote by \(u_{n,i},l_{n,i},w_{n,i},\eta_{n,i}\) the respective quantities. In the following, we do not explicitly denote the dependence of \(\beta_{n}\) on \(\delta\).

Footnote 11: This can be achieved by taking a union bound and rescaling \(\delta\).

To improve clarity, we will refer to the set of potential maximizers defined in Equation (5) as \(\mathcal{M}_{n}\) and denote by \(\mathcal{A}_{n}\) an arbitrary target space.

We point out the following corollary:

**Corollary C.28** (Safety).: _With high probability, jointly for any \(n\geq 0\) and any \(i\in\mathcal{I}_{s}\),_

\[\forall\bm{x}\in\mathcal{S}_{n}:g_{i}^{\star}(\bm{x})\geq 0.\] (26)

#### c.8.1 Convergence to Reachable Safe Set

**Definition C.29** (Reachable safe set).: Given any pessimistic safe set \(\mathcal{S}\subseteq\mathcal{X}\) and any \(\epsilon\geq 0\) and \(\beta\geq 0\), we define the _reachable safe set_ up to \((\epsilon,\beta)\)-slack and its closure as

\[\mathcal{R}_{\epsilon,\beta}(\mathcal{S}) \operatorname{\stackrel{{\mathrm{def}}}{{=}}} \mathcal{S}\cup\{\bm{x}\in\mathcal{X}\setminus\mathcal{S}\mid\] \[g_{i}^{\star}(\bm{x})-\beta(\eta_{i}(\bm{x};\mathcal{S})+ \epsilon)\geq 0\text{ for all }i\in\mathcal{I}_{s}\}\] \[\bar{\mathcal{R}}_{\epsilon,\beta}(\mathcal{S}) \operatorname{\stackrel{{\mathrm{def}}}{{=}}} \lim_{n\to\infty}(\mathcal{R}_{\epsilon,\beta})^{n}(\mathcal{S})\]

where \((\mathcal{R}_{\epsilon,\beta})^{n}\) denotes the \(n\)-th composition of \(\mathcal{R}_{\epsilon,\beta}\) with itself.

_Remark C.30_.: Convergence of the safe set to the closure of the reachability operator can only be guaranteed for finite safe sets (\(|\mathcal{S}^{\star}|<\infty\)). The following proofs readily generalize to continuous domains by considering convergence within the \(k\)-th composition of the reachability operator with itself for some \(k<\infty\). In this case the sample complexity grows with \(k\) rather than \(|\mathcal{S}^{\star}|\). The only required modification is to lift the assumption of Theorem C.12 that information is gained only while safe sets remain constant (i.e., \(\mathcal{S}_{i+1}=\mathcal{S}_{i}\) for all \(i\)). This assumption is straightforward to lift since for any \(n\geq 0\) and \(T\geq 1\),

\[\max_{\boldsymbol{x}\in\mathcal{S}_{n}}\Delta_{\mathcal{A}}(\boldsymbol{x} \mid\boldsymbol{x}_{1:n+T})\leq\frac{1}{T}\sum_{t=1}^{T}\max_{\boldsymbol{x} \in\mathcal{S}_{n}}\Delta_{\mathcal{A}}(\boldsymbol{x}\mid\boldsymbol{x}_{1: n+t})\leq\frac{1}{T}\sum_{t=1}^{T}\max_{\boldsymbol{x}\in\mathcal{S}_{n+t}} \Delta_{\mathcal{A}}(\boldsymbol{x}\mid\boldsymbol{x}_{1:n+t})\leq\frac{ \gamma_{T}}{T},\]

using submodularity for the first inequality and the monotonicity of the safe set for the second inequality. In particular, this shows that one continues learning about points in the original safe set -- even as the safe set grows.

We denote by \(\mathcal{S}_{0}\) the initial pessimistic safe set induced by the (prior) statistical model \(g\) (cf. Section 5) and write \(\bar{\mathcal{R}}_{\epsilon,\beta}\equiv\bar{\mathcal{R}}_{\epsilon,\beta}( \mathcal{S}_{0})\).

**Lemma C.31** (Properties of the reachable safe set).: _For all \(\mathcal{S},\mathcal{S}^{\prime}\subseteq\mathcal{X}\), \(\epsilon\geq 0\), and \(\beta\geq 0\):_

1. \(\mathcal{S}^{\prime}\subseteq\mathcal{S}\implies\mathcal{R}_{\epsilon,\beta}( \mathcal{S}^{\prime})\subseteq\mathcal{R}_{\epsilon,\beta}(\mathcal{S})\)_,_
2. \(\mathcal{R}_{\epsilon,\beta}(\mathcal{S})\subseteq\mathcal{S}\implies\bar{ \mathcal{R}}_{\epsilon,\beta}(\mathcal{S})\subseteq\mathcal{S}\)_, and_
3. \(\mathcal{R}_{0,0}(\emptyset)=\bar{\mathcal{R}}_{0,0}=\mathcal{S}^{\star}\)_._

Proof (adapted from lemma 7.1 of Berkenkamp et al. (2021)).:
1. Let \(\boldsymbol{x}\in\mathcal{R}_{\epsilon,\beta}(\mathcal{S}^{\prime})\). If \(\boldsymbol{x}\in\mathcal{S}\) then \(\boldsymbol{x}\in\mathcal{R}_{\epsilon,\beta}(\mathcal{S})\), so let \(\boldsymbol{x}\not\in\mathcal{S}\). Then, by definition, for all \(i\in\mathcal{I}_{s}\), \(f_{i}^{\star}(\boldsymbol{x})-\beta\eta_{i}(\boldsymbol{x};\mathcal{S}^{\prime })-\epsilon\geq 0\). By the monotonicity of variance, \(\eta_{i}(\boldsymbol{x};\mathcal{S}^{\prime})\geq\eta_{i}(\boldsymbol{x}; \mathcal{S})\) for all \(i\in\mathcal{I}\), and hence \(f_{i}^{\star}(\boldsymbol{x})-\beta\eta_{i}(\boldsymbol{x};\mathcal{S})- \epsilon\geq 0\) for all \(i\in\mathcal{I}_{s}\). It follows that \(\boldsymbol{x}\in\mathcal{R}_{\epsilon,\beta}(\mathcal{S})\).
2. By the monotonicity of variance, \(\eta_{i}(\boldsymbol{x};\mathcal{R}_{\epsilon,\beta}(\mathcal{S}))\geq\eta_{ i}(\boldsymbol{x};\mathcal{S})\) for all \(\boldsymbol{x}\in\mathcal{X}\) and \(i\in\mathcal{I}\). Thus, by definition of the safe region, we have that \(\mathcal{R}_{\epsilon,\beta}(\mathcal{R}_{\epsilon,\beta}(\mathcal{S}))\subseteq \mathcal{S}\). The result follows by taking the limit.
3. The result follows directly from the definition of the true safe set \(\mathcal{S}^{\star}\) (cf. Equation (4)). 

Clearly, we cannot expand the safe set beyond \(\bar{\mathcal{R}}_{0,0}\). The following is our main intermediate result, showing that either we expand the safe set at some point or the uncertainty converges to the irreducible uncertainty.

**Lemma C.32**.: _Given any \(n_{0}\geq 0\), \(\epsilon>0\), let \(n^{\prime}\) be the smallest integer such that \(\nu_{n^{\prime},\tilde{\epsilon}^{2}}\leq\tilde{\epsilon}\) where \(\tilde{\epsilon}=\epsilon/2\). Let \(\beta_{n_{0}+n^{\prime}}=\max_{i\in\mathcal{I}_{s}}\beta_{n_{0}+n^{\prime},i}\). Assume that the sequence of target spaces is monotonically decreasing, i.e., \(\mathcal{A}_{n+1}\subseteq\mathcal{A}_{n}\). Then, we have with high probability (at least) one of_

\[\Big{(}\forall\boldsymbol{x} \in\mathcal{A}_{n_{0}+n^{\prime}},\;\forall i\in\mathcal{I}:\] \[w_{n_{0}+n^{\prime},i}(\boldsymbol{x}) \leq\beta_{n_{0}+n^{\prime}}[\eta_{i}(\boldsymbol{x};\mathcal{S}_{n_{0} +n^{\prime}})+\epsilon]\] \[\text{and}\quad\mathcal{A}_{n_{0}+n^{\prime}}\cap\mathcal{R}_{ \epsilon,\beta_{n_{0}+n^{\prime}}}(\mathcal{S}_{n_{0}+n^{\prime}})\subseteq \mathcal{S}_{n_{0}+n^{\prime}}\Big{)}\]

_or \(|\mathcal{S}_{n_{0}+n^{\prime}+1}|>|\mathcal{S}_{n_{0}}|\)._

Proof.: Suppose that \(|\mathcal{S}_{n_{0}+n^{\prime}+1}|=|\mathcal{S}_{n_{0}}|\). Then, by Theorem 3.3 (using that the sequence of target spaces is monotonically decreasing), for any \(\boldsymbol{x}\in\mathcal{A}_{n_{0}+n^{\prime}}\) and \(i\in\mathcal{I}\),

\[w_{n_{0}+n^{\prime},i}(\boldsymbol{x})\leq\beta_{n_{0}+n^{\prime}}[\eta_{i}( \boldsymbol{x};\mathcal{S}_{n_{0}+n^{\prime}})+\epsilon].\]

As \(\mathcal{S}_{n_{0}+n^{\prime}+1}=\mathcal{S}_{n_{0}+n^{\prime}}\) we have for all \(\boldsymbol{x}\in\mathcal{A}_{n_{0}+n^{\prime}}\setminus\mathcal{S}_{n_{0}+n^{ \prime}}\) and \(i\in\mathcal{I}_{s}\), with high probability that

\[0>l_{n_{0}+n^{\prime},i}(\boldsymbol{x}) \geq g_{i}^{\star}(\boldsymbol{x})-w_{n_{0}+n^{\prime},i}( \boldsymbol{x})\] \[\geq g_{i}^{\star}(\boldsymbol{x})-\beta_{n_{0}+n^{\prime}}[\eta_{i} (\boldsymbol{x};\mathcal{S}_{n_{0}+n^{\prime}})+\epsilon].\]

It follows that \(\mathcal{A}_{n_{0}+n^{\prime}}\cap\mathcal{R}_{\epsilon,\beta_{n_{0}+n^{\prime}}}( \mathcal{S}_{n_{0}+n^{\prime}})\subseteq\mathcal{S}_{n_{0}+n^{\prime}}\).

To gather more intuition about the above lemma, consider the target space

\[\mathcal{E}_{n}\stackrel{{\mathrm{def}}}{{=}}\widehat{\mathcal{S}}_{n} \setminus\mathcal{S}_{n}.\] (27)

We call \(\mathcal{E}_{n}\) the _potential expanders_ since it contains all points which might be safe, but are not yet known to be safe. Under this target space, the above lemma simplifies slightly:

**Lemma C.33**.: _For any \(n\geq 0\) and \(\epsilon,\beta\geq 0\), if \(\mathcal{E}_{n}\subseteq\mathcal{A}_{n}\) then with high probability,_

\[\mathcal{S}_{n}\cup(\mathcal{A}_{n}\cap\mathcal{R}_{\epsilon,\beta}(\mathcal{ S}_{n}))=\mathcal{R}_{\epsilon,\beta}(\mathcal{S}_{n}).\]

Proof.: With high probability, \(\mathcal{R}_{\epsilon,\beta}(\mathcal{S}_{n})\subseteq\widehat{\mathcal{S}}_ {n}=\mathcal{S}_{n}\cup\mathcal{E}_{n}\). The lemma is a direct consequence. 

The above lemmas can be combined to yield our main result of this subsection, establishing the convergence of ITL to the reachable safe set.

**Theorem C.34** (Convergence to reachable safe set).: _For any \(\epsilon>0\), let \(n^{\prime}\) be the smallest integer satisfying the condition of Lemma C.32, and define \(n^{\star}\stackrel{{\mathrm{def}}}{{=}}(|\mathcal{S}^{\star}|+1)n ^{\prime}\). Let \(\bar{\beta}_{n^{\star}}\geq\beta_{n,i}\) for all \(n\leq n^{\star},i\in\mathcal{I}_{s}\). Assume that the sequence of target spaces is monotonically decreasing, i.e., \(\mathcal{A}_{n+1}\subseteq\mathcal{A}_{n}\). Then, the following inequalities hold jointly with probability at least \(1-\delta\):_

1. \(\forall n\geq 0,\ \forall i\in\mathcal{I}_{s}:g_{i}^{\star}(\bm{x}_{n})\geq 0\)_,_ _safety_
2. \(\mathcal{A}_{n^{\star}}\cap\bar{\mathcal{R}}_{\epsilon,\bar{\beta}_{n^{\star} }}\subseteq\mathcal{S}_{n^{\star}}\subseteq\bar{\mathcal{R}}_{0,0}=\mathcal{S}^ {\star}\)_,_ _convergence to safe region_
3. \(\forall\bm{x}\in\mathcal{A}_{n^{\star}},\ \forall i\in\mathcal{I}:w_{n^{\star},i}(\bm{x}) \leq\bar{\beta}_{n^{\star}}\eta_{i}(\bm{x};\bar{\mathcal{R}}_{\epsilon,\bar{ \beta}_{n^{\star}}})+\epsilon\)_,_ _convergence of width_
4. \(\forall\bm{x}\in\bar{\mathcal{R}}_{\epsilon,\bar{\beta}_{n^{\star}}},\ \forall i\in \mathcal{I}:\eta_{i}(\bm{x};\bar{\mathcal{R}}_{\epsilon,\bar{\beta}_{n^{\star} }})=0\)_._ _convergence of width within safe region_

Proof.: \((i)\) is a direct consequence of Corollary C.28. \(\mathcal{S}_{n^{\star}}\subseteq\mathcal{S}^{\star}\) follows directly from the pessimistic safe set \(\mathcal{S}_{n^{\star}}\) from \((ii)\) being a subset of the true safe set \(\mathcal{S}^{\star}\). \((iv)\) follows directly from the definition of irreducible uncertainty. Thus, it remains to establish \(\mathcal{A}_{n^{\star}}\cap\bar{\mathcal{R}}_{\epsilon,\bar{\beta}_{n^{\star} }}\subseteq\mathcal{S}_{n^{\star}}\) and \((iii)\).

Recall that with high probability \(|\mathcal{S}_{n}|\in[0,|\mathcal{S}^{\star}|]\) for all \(n\geq 0\). Thus, the size of the pessimistic safe set can increase at most \(|\mathcal{S}^{\star}|\) many times. By Lemma C.32, using the assumption on \(n^{\prime}\), the size of the pessimistic safe set increases at least once every \(n^{\prime}\) iterations, or else:

\[\begin{split}&\forall\bm{x}\in\mathcal{A}_{n_{0}+n^{\prime}},\ \forall i\in\mathcal{I}:w_{n_{0}+n^{\prime},i}(\bm{x})\leq\beta_{n_{0}+n^{ \prime}}[\eta_{i}(\bm{x};\mathcal{S}_{n_{0}+n^{\prime}})+\epsilon]\\ &\text{and}\quad\mathcal{A}_{n_{0}+n^{\prime}}\cap\mathcal{R}_{ \epsilon,\beta_{n_{0}+n^{\prime}}}(\mathcal{S}_{n_{0}+n^{\prime}})\subseteq \mathcal{S}_{n_{0}+n^{\prime}}.\end{split}\] (28)

Because the safe set can expand at most \(|\mathcal{S}^{\star}|\) many times, Equation (28) occurs eventually for some \(n_{0}\leq|\mathcal{S}^{\star}|n^{\prime}\). In this case, since \(\bar{\beta}_{n^{\star}}\ \geq\ \beta_{n_{0}+n^{\prime}}\) and \(\mathcal{A}_{n^{\star}}\subseteq\mathcal{A}_{n_{0}+n^{\prime}}\) (as \(n_{0}+n^{\prime}\leq n^{\star}\)) we have that

\[\begin{split}\mathcal{A}_{n^{\star}}\cap\mathcal{R}_{\epsilon,\bar {\beta}_{n^{\star}}}(\mathcal{S}_{n_{0}+n^{\prime}})&\subseteq \mathcal{A}_{n_{0}+n^{\prime}}\cap\mathcal{R}_{\epsilon,\beta_{n_{0}+n^{\prime} }}(\mathcal{S}_{n_{0}+n^{\prime}})\\ &\subseteq\mathcal{S}_{n_{0}+n^{\prime}}.\end{split}\]

By Lemma C.31 (ii), this implies

\[\mathcal{A}_{n^{\star}}\cap\bar{\mathcal{R}}_{\epsilon,\bar{\beta}_{n^{\star} }}\subseteq\mathcal{S}_{n_{0}+n^{\prime}}\subseteq\mathcal{S}_{n^{\star}}.\]

We emphasize that Theorem C.34 holds for arbitrary target spaces \(\mathcal{A}_{n}\). If additionally, \(\mathcal{E}_{n}\subseteq\mathcal{A}_{n}\) for all \(n\geq 0\) then by Lemma C.33, Theorem C.34 strengthens to \(\bar{\mathcal{R}}_{\epsilon,\bar{\beta}_{n^{\star}}}\subseteq\mathcal{S}_{n^{\star}}\). Intuitively, \(\mathcal{E}_{n}\subseteq\mathcal{A}_{n}\) ensures that one aims to expand the safe set in _all_ directions. Conversely, if \(\mathcal{E}_{n}\not\subseteq\mathcal{A}_{n}\) then one aims only to expand the safe set in the direction of \(\mathcal{A}_{n}\) (or not at all if \(\mathcal{A}_{n}\subseteq\mathcal{S}_{n}\)).

"Free" convergence guarantees in many applicationsTheorem C.34 can be specialized to yield convergence guarantees in various settings by choosing an appropriate target space \(\mathcal{A}_{n}\). Straightforward application of Theorem C.34 (informally) requires that the sequence of target spaces is monotonically decreasing (i.e., \(\mathcal{A}_{n+1}\subseteq\mathcal{A}_{n}\)), and that each target space \(\mathcal{A}_{n}\) is an "over-approximation" of the actual set of targeted points (such as the set of optimas in the Bayesian optimization setting). We discuss two such applications in the following.

1. _Pure expansion:_ For example, for the target space \(\mathcal{E}_{n}\), Theorem C.34 bounds the convergence of the safe set to the reachable safe set. In this case, the transductive active learning problem corresponds to the "pure expansion" setting, also addressed by the ISE baseline discussed in Section 5. The ISE baseline, however, does not establish convergence guarantees of the kind of Theorem C.34. Note that \(\mathcal{E}_{n}\) satisfies the (informal) requirements laid out previously, since it is monotonically decreasing by definition, and with high probability, any point \(\bm{x}\in\mathcal{S}^{\star}\) that is not in \(\mathcal{S}_{n}\) is contained within \(\mathcal{E}_{n}\).
2. _Level set estimation:_ Given any \(\tau\in\mathbb{R}\), we denote the (safe) \(\tau\)-level set of \(f^{\star}\) by \(\mathcal{L}^{\tau}\mathop{\stackrel{{\mathrm{def}}}{{=}}}\{\bm{x} \in\mathcal{S}^{\star}\mid f^{\star}(\bm{x})=\tau\}\). We define the _potential level set_ as \[\mathcal{L}^{\tau}_{n}\mathop{\stackrel{{\mathrm{def}}}{{=}}}\{ \bm{x}\in\widehat{\mathcal{S}}_{n}\mid l^{f}_{n}(\bm{x})\leq\tau\leq u^{f}_{n} (\bm{x})\}.\] (29) That is, \(\mathcal{L}^{\tau}_{n}\) is the subset of the optimistic safe set \(\widehat{\mathcal{S}}_{n}\) where the \(\tau\)-level set of \(f^{\star}\) may be located. Analogously to the potential expanders, it is straightforward to show that \(\mathcal{L}^{\tau}_{n}\) over-approximates the true \(\tau\)-level set and is monotonically decreasing.

We remark that our guarantees from this section also apply to the standard ("unsafe") setting where \(\mathcal{S}^{\star}=\mathcal{S}_{0}=\mathcal{X}\).

#### c.8.2 Convergence to Safe Optimum

In this section, we specialize Theorem C.34 for the case that the target space contains the potential maximizers \(\mathcal{M}_{n}\) (cf. Equation (5)). It is straightforward to see that the sequence \(\mathcal{M}_{n}\) is monotonically decreasing (i.e., \(\mathcal{M}_{n+1}\subseteq\mathcal{M}_{n}\)). The following lemma shows that the potential maximizers over-approximate the set of safe maxima \(\mathcal{X}^{*}\mathop{\stackrel{{\mathrm{def}}}{{=}}}\arg\max_{ \bm{x}\in\mathcal{S}^{\star}}f^{\star}(\bm{x})\).

**Lemma C.35** (Potential maximizers over-approximate safe maxima).: _For all \(n\geq 0\) and with probability at least \(1-\delta\),_

1. \(\bm{x}\in\mathcal{X}^{*}\) _implies_ \(\bm{x}\in\mathcal{M}_{n}\) _and_
2. \(\bm{x}\not\in\mathcal{M}_{n}\) _implies_ \(\bm{x}\not\in\mathcal{X}^{*}\)_._

Proof.: If \(\bm{x}\not\in\mathcal{M}_{n}\) then

\[u_{n,f}(\bm{x})<\max_{\bm{x}^{\prime}\in\mathcal{S}_{n}}l_{n,f}(\bm{x}^{\prime })\leq\max_{\bm{x}^{\prime}\in\mathcal{S}^{*}}l_{n,f}(\bm{x}^{\prime})\]

where we used \(\mathcal{S}_{n}\subseteq\mathcal{S}^{\star}\) with high probability, which directly implies with high probability that \(\bm{x}\not\in\mathcal{X}^{*}\).

For the other direction, if \(\bm{x}\in\mathcal{X}^{*}\) then

\[u_{n,f}(\bm{x})\geq\max_{\bm{x}^{\prime}\in\mathcal{S}^{*}}l_{n,f}(\bm{x}^{ \prime})\geq\max_{\bm{x}^{\prime}\in\mathcal{S}_{n}}l_{n,f}(\bm{x}^{\prime})\]

with high probability. 

We denote the set of optimal actions which are safe up to \((\epsilon,\beta)\)-slack by

\[\mathcal{X}^{*}_{\epsilon,\beta}\mathop{\stackrel{{ \mathrm{def}}}{{=}}}\mathop{\stackrel{{\mathrm{def}}}{{=}}} \mathop{\stackrel{{\mathrm{grad}}}{{=}}}\mathop{\stackrel{{ \mathrm{grad}}}{{=}}}f^{\star}(\bm{x}),\]

and by \(f^{*}_{\epsilon,\beta}\) the maximum value attained by \(f^{\star}\) at any of the points in \(\mathcal{X}^{*}_{\epsilon,\beta}\). The regret can be expressed as

\[r_{n}(\bar{\mathcal{R}}_{\epsilon,\beta})=f^{*}_{\epsilon,\beta}-f^{\star}(\widehat {\bm{x}}_{n})\]

The following theorem formalizes Theorem 5.1 and establishes convergence to the safe optimum.

**Theorem C.36** (Convergence to safe optimum).: _For any \(\epsilon>0\), let \(n^{\prime}\) be the smallest integer satisfying the condition of Lemma C.32, and define \(n^{*}\stackrel{{\mathrm{def}}}{{=}}(|\mathcal{S}^{\star}|+1)n^{\prime}\). Let \(\bar{\beta}_{n^{*}}\geq\beta_{n,i}\) for all \(n\leq n^{*},i\in\mathcal{I}_{s}\). Then, the following inequalities hold jointly with probability at least \(1-\delta\):_

1. \(\forall n\geq 0,\;\forall i\in\mathcal{I}_{s}:g_{i}^{*}(\bm{x}_{n})\geq 0\)_,_ _safety_
2. \(\forall n\geq n^{\star}:r_{n}(\bar{\mathcal{R}}_{\epsilon,\bar{\beta}_{n^{*} }})\leq\epsilon\)_._ _convergence to safe optimum_

Proof.: Fix any \(\bm{x}^{*}\in\mathcal{X}^{*}_{\epsilon,\bar{\beta}_{n^{*}}}\subseteq\bar{ \mathcal{R}}_{\epsilon,\bar{\beta}_{n^{*}}}\). Assume w.l.o.g. that \(\bm{x}^{*}\in\mathcal{M}_{n^{*}}\).12 Then, with high probability,

Footnote 12: Otherwise, with high probability, \(f^{*}(\widehat{\bm{x}}_{n^{*}})>f^{*}_{\epsilon,\bar{\beta}_{n^{*}}}\).

\[f^{*}_{\epsilon,\bar{\beta}_{n^{*}}}=f^{\star}(\bm{x}^{*}) \leq u_{n^{*},f}(\bm{x}^{*})\] \[=l_{n^{*},f}(\bm{x}^{*})+w_{n^{*},f}(\bm{x}^{*})\] \[\stackrel{{(i)}}{{\leq}}l_{n^{*},f}(\widehat{\bm{x} }_{n^{*}})+w_{n^{*},f}(\bm{x}^{*})\] \[\leq f^{\star}(\widehat{\bm{x}}_{n^{*}})+w_{n^{*},f}(\bm{x}^{*})\] \[\stackrel{{(ii)}}{{\leq}}f^{\star}(\widehat{\bm{x} }_{n^{*}})+\epsilon\]

where \((i)\) follows from the definition of \(\widehat{\bm{x}}_{n}\); and \((ii)\) follows from Theorem C.34 and noting that \(\bm{x}^{*}\in\mathcal{M}_{n^{*}}\cap\bar{\mathcal{R}}_{\epsilon,\bar{\beta}_{n ^{*}}}\).

We have shown that \(f^{\star}(\widehat{\bm{x}}_{n^{*}})\geq f^{*}_{\epsilon,\bar{\beta}_{n^{*}}}-\epsilon\), which implies \(r_{n^{*}}(\bar{\mathcal{R}}_{\epsilon,\bar{\beta}_{n^{*}}})\leq\epsilon\). Since the upper- and lower-confidence bounds are monotonically decreasing / increasing, respectively, we have that for all \(n\geq n^{\star}\), \(r_{n}(\bar{\mathcal{R}}_{\epsilon,\bar{\beta}_{n^{*}}})\leq\epsilon\). 

### Useful Facts and Inequalities

We denote by \(\preceq\) the Loewner partial ordering of symmetric matrices.

**Lemma C.37**.: _Let \(\bm{A}\in\mathbb{R}^{n\times n}\) be a positive definite matrix with diagonal \(\bm{D}\). Then, \(\bm{A}\preceq n\bm{D}\)._

Proof.: Equivalently, one can show \(n\bm{D}-\bm{A}\succeq\bm{0}\). We write \(\bm{A}\stackrel{{\mathrm{def}}}{{=}}\bm{D}^{\nicefrac{{1}}{{2}}} \bm{Q}\bm{D}^{\nicefrac{{1}}{{2}}}\), and thus, \(\bm{Q}=\bm{D}^{-\nicefrac{{1}}{{2}}}\bm{A}\bm{D}^{-\nicefrac{{1}}{{2}}}\) is a positive definite symmetric matrix with all diagonal elements equal to \(1\). It remains to show that

\[n\bm{D}-\bm{A}=\bm{D}^{\nicefrac{{1}}{{2}}}(n\bm{I}-\bm{Q})\bm{D}^{\nicefrac{{ 1}}{{2}}}\succeq\bm{0}.\]

Note that \(\sum_{i=1}^{n}\lambda_{i}(\bm{Q})=\mathrm{tr}\ \bm{Q}=n\), and hence, all eigenvalues of \(\bm{Q}\) belong to \((0,n)\). 

**Lemma C.38**.: _If \(a,b\in(0,M]\) for some \(M>0\) and \(b\geq a\) then_

\[b-a\leq M\cdot\log\biggl{(}\frac{b}{a}\biggr{)}.\] (30)

_If additionally, \(a\geq M^{\prime}\) for some \(M^{\prime}>0\) then_

\[b-a\geq M^{\prime}\cdot\log\biggl{(}\frac{b}{a}\biggr{)}.\] (31)

Proof.: Let \(f(x)\stackrel{{\mathrm{def}}}{{=}}\log x\). By the mean value theorem, there exists \(c\in(a,b)\) such that

\[\frac{1}{c}=f^{\prime}(c)=\frac{f(b)-f(a)}{b-a}=\frac{\log b-\log a}{b-a}= \frac{\log(\frac{b}{a})}{b-a}.\]

Thus,

\[b-a=c\cdot\log\biggl{(}\frac{b}{a}\biggr{)}<M\cdot\log\biggl{(}\frac{b}{a} \biggr{)}.\]Under the additional condition that \(a\geq M^{\prime}\), we obtain

\[b-a=c\cdot\log\biggl{(}\frac{b}{a}\biggr{)}>M^{\prime}\cdot\log\biggl{(}\frac{b}{a }\biggr{)}.\]

## Appendix D Interpretations & Approximations of Principle (\(\dagger\))

We give a brief overview of interpretations and approximations of ITL, as well as alternative decision rules adhering to the fundamental principle (\(\dagger\)).

The discussed interpretations of (\(\dagger\)) differ mainly in how they quantify the "uncertainty" about \(\mathcal{A}\). In the GP setting, this "uncertainty" is captured by the covariance matrix \(\bm{\Sigma}\) of \(\bm{f}_{\mathcal{A}}\), and we consider two main ways of "scalarizing" \(\bm{\Sigma}\):

1. the total (marginal) variance \(\operatorname{tr}\,\bm{\Sigma}\), and
2. the "generalized variance" \(|\bm{\Sigma}|\).

The generalized variance -- which was originally suggested by Wilks (1932) as a generalization of variance to multiple dimensions -- takes into account correlations. In contrast, the total variance discards all correlations between points in \(\mathcal{A}\).

All discussed decision rules following principle (\(\dagger\)) (i.e., ITL, VTL, MM-ITL) differ only in their weighting of the points in \(\mathcal{A}\), and they coincide when \(|\mathcal{A}|=1\).

### Interpretations of ITL

We briefly discuss three interpretations of ITL.

Minimizing generalized varianceIn the GP setting, ITL can be equivalently characterized as minimizing generalized posterior variance:

\[\bm{x}_{n} =\operatorname*{arg\,max}_{\bm{x}\in\mathcal{S}}\mathrm{I}(\bm{ f}_{\mathcal{A}};y_{\bm{x}}\mid\mathcal{D}_{n})\] \[=\operatorname*{arg\,max}_{\bm{x}\in\mathcal{S}}\frac{1}{2}\log \biggl{(}\frac{|\mathrm{Var}[\bm{f}_{\mathcal{A}}\mid\mathcal{D}_{n-1}]|}{| \mathrm{Var}[\bm{f}_{\mathcal{A}}\mid\mathcal{D}_{n-1},y_{\bm{x}}]|}\biggr{)}\] \[=\operatorname*{arg\,min}_{\bm{x}\in\mathcal{S}}|\mathrm{Var}[ \bm{f}_{\mathcal{A}}\mid\mathcal{D}_{n-1},y_{\bm{x}}]|\,.\] (32)

Maximizing relevance and minimizing redundancyAn alternative interpretation of ITL is

\[\mathrm{I}(\bm{f}_{\mathcal{A}};y_{\bm{x}}\mid\mathcal{D}_{n})=\underbrace{ \mathrm{I}(\bm{f}_{\mathcal{A}};y_{\bm{x}})}_{\text{relevance}}-\underbrace{ \mathrm{I}(\bm{f}_{\mathcal{A}};y_{\bm{x}};\mathcal{D}_{n})}_{\text{redundancy}}\] (33)

where \(\mathrm{I}(\bm{f}_{\mathcal{A}};y_{\bm{x}};\mathcal{D}_{n})=\mathrm{I}(\bm{f}_ {\mathcal{A}};y_{\bm{x}})-\mathrm{I}(\bm{f}_{\mathcal{A}};y_{\bm{x}}\mid \mathcal{D}_{n})\) denotes the _multivariate information gain_ (cf. Appendix B). In this way, ITL can be seen as maximizing observation relevance while minimizing observation redundancy. This interpretation is common in the literature on feature selection (Peng et al., 2005; Vergara and Estevez, 2014; Beraha et al., 2019).

Steepest descent in measure spacesITL can be seen as performing steepest descent in the space of probability measures over \(\bm{f}_{\mathcal{A}}\), with the KL divergence as metric:

\[\mathrm{I}(\bm{f}_{\mathcal{A}};y_{\bm{x}}\mid\mathcal{D}_{n})=\mathbb{E}_{y_ {\bm{x}}}[\mathrm{KL}(p(\bm{f}_{\mathcal{A}}\mid\mathcal{D}_{n},y_{\bm{x}}) \|p(\bm{f}_{\mathcal{A}}\mid\mathcal{D}_{n}))].\]

That is, ITL finds the observation yielding the "largest update" to the current density.

### Interpretations of VTL

Quantifying the uncertainty about \(\bm{f}_{\mathcal{A}}\) by the marginal variance of points in \(\mathcal{A}\) rather than entropy (or generalized variance), the principle (\(\dagger\)) leads to VTL. Note that if \(|\mathcal{A}|=1\), then VTL is equivalent to ITL. Unlike the similar, but more technical, TruVar algorithm proposed by Bogunovic et al. (2016), VTL does not require truncated variances, and hence, VTL can be applied to constrained settings (where \(\mathcal{A}\not\subseteq\mathcal{S}\)) as well.

Relationship to ITLNote that the ITL criterion in the GP setting can be expressed as

\[\bm{x}_{n}=\operatorname*{arg\,min}_{\bm{x}\in\mathcal{S}}\operatorname*{tr} \,\log\operatorname*{Var}[\bm{f}_{\bm{A}}\mid\mathcal{D}_{n-1},y_{\bm{x}}]\] (34)

where for a positive semi-definite matrix \(\bm{A}\) with spectral decomposition \(\bm{A}=\bm{V}\bm{\Lambda}\bm{V}^{\top}\) we write \(\log\bm{A}=\bm{V}\log\bm{\Lambda}\bm{V}^{\top}\) for the logarithmic matrix function. To derive Equation (34) we use that \(\log|\bm{A}|=\sum_{i}\log\lambda_{i}(\bm{A})=\operatorname*{tr}\,\log\bm{A}\). Hence, ITL and VTL are identical up to a different weighting of the eigenvalues of the posterior covariance matrix.

Minimizing a bound to the approximation errorChowdhury and Gopalan (2017) (page 19) bound the approximation error \(|f^{\star}(\bm{x})-\mu_{n}(\bm{x})|\) by

\[\underbrace{|\bm{k}_{t}(\bm{x})^{\top}(\bm{K}_{t}+\bm{P}_{t})^{-1}\bm{\varepsilon }_{1:t}}_{\text{variance}}+\underbrace{|f^{\star}(\bm{x})-\bm{k}_{t}(\bm{x})^{ \top}(\bm{K}_{t}+\bm{P}_{t})^{-1}\bm{f}_{1:t}|}_{\text{bias}}\]

where \(\bm{k}_{t}(\bm{x})\mathop{=}^{\mathrm{def}}\bm{K}_{\bm{x}_{1:t}}\), \(\bm{K}_{t}\mathop{=}^{\mathrm{def}}\bm{K}_{\bm{x}_{1:t}}\), and \(\bm{P}_{t}\mathop{=}^{\mathrm{def}}\bm{P}_{\bm{x}_{1:t}}\). Similar to a standard bias-variance decomposition, the first term measures variance and the second term measures bias. Following Lemma C.27, VTL can be seen as greedily minimizing this bound to the approximation error (i.e., both bias and variance).

Maximizing correlation to prediction targets weighted by their varianceIt can be shown (see the proof below) that the VTL decision rule is equivalent to

\[\bm{x}_{n}=\operatorname*{arg\,max}_{\bm{x}\in\mathcal{S}}\sum_{\bm{x}^{ \prime}\in\mathcal{A}}\operatorname*{Var}[f_{\bm{x}^{\prime}}\mid\mathcal{D}_ {n-1}]\cdot\operatorname*{Cor}[f_{\bm{x}^{\prime}},y_{\bm{x}}\mid\mathcal{D}_ {n-1}]^{2}.\] (35)

That is, VTL maximizes the squared correlation between the next observation and the prediction targets, weighted by their variance. Intuitively, prediction targets are weighted by their variance since more can be learned about a prediction target with higher variance. This is precisely what leads to the "diverse" sample selection, and is akin to "uncertainty sampling" among the prediction targets and then selecting the observation which is most correlated with the selected prediction target.

Proof.: Starting with the VTL objective, we have

\[\operatorname*{arg\,min}_{\bm{x}\in\mathcal{S}}\sum_{\bm{x}^{ \prime}\in\mathcal{A}}\operatorname*{Var}[f_{\bm{x}^{\prime}}\mid\mathcal{D}_ {n},y_{\bm{x}}] =\operatorname*{arg\,min}_{\bm{x}\in\mathcal{S}}\sum_{\bm{x}^{ \prime}\in\mathcal{A}}\Biggl{(}\operatorname*{Var}[f_{\bm{x}^{\prime}}\mid \mathcal{D}_{n}]-\frac{\operatorname*{Cov}[f_{\bm{x}^{\prime}},y_{\bm{x}}\mid \mathcal{D}_{n}]^{2}}{\operatorname*{Var}[y_{\bm{x}}\mid\mathcal{D}_{n}]} \Biggr{)}\] \[=\operatorname*{arg\,max}_{\bm{x}\in\mathcal{S}}\sum_{\bm{x}^{ \prime}\in\mathcal{A}}\frac{\operatorname*{Var}[f_{\bm{x}^{\prime}}\mid \mathcal{D}_{n}]\cdot\operatorname*{Cov}[f_{\bm{x}^{\prime}},y_{\bm{x}}\mid \mathcal{D}_{n}]^{2}}{\operatorname*{Var}[f_{\bm{x}^{\prime}}\mid\mathcal{D}_ {n}]\cdot\operatorname*{Var}[y_{\bm{x}}\mid\mathcal{D}_{n}]}+\text{const}\] \[=\operatorname*{arg\,max}_{\bm{x}\in\mathcal{S}}\sum_{\bm{x}^{ \prime}\in\mathcal{A}}\operatorname*{Var}[f_{\bm{x}^{\prime}}\mid\mathcal{D}_ {n}]\cdot\operatorname*{Cor}[f_{\bm{x}^{\prime}},y_{\bm{x}}\mid\mathcal{D}_{n} ]^{2}+\text{const}.\]

### Mean Marginal ITL

MacKay (1992) previously proposed "mean-marginal" ITL (MM-ITL) in the setting where \(\mathcal{S}=\mathcal{X}\), which selects

\[\bm{x}_{n}=\operatorname*{arg\,max}_{\bm{x}\in\mathcal{S}}\sum_{\bm{x}^{ \prime}\in\mathcal{A}}\operatorname*{I}(f_{\bm{x}^{\prime}};y_{\bm{x}}\mid \mathcal{D}_{n-1})\] (36)

and which simplifies in the GP setting to

\[\bm{x}_{n} =\operatorname*{arg\,max}_{\bm{x}\in\mathcal{S}}\frac{1}{2}\sum_{ \bm{x}^{\prime}\in\mathcal{A}}\log\biggl{(}\frac{\operatorname*{Var}[f_{\bm{x}^ {\prime}}\mid\mathcal{D}_{n-1}]}{\operatorname*{Var}[f_{\bm{x}^{\prime}}\mid \mathcal{D}_{n-1},y_{\bm{x}}]}\biggr{)}\] \[=\operatorname*{arg\,min}_{\bm{x}\in\mathcal{S}}\sum_{\bm{x}^{ \prime}\in\mathcal{A}}\log\operatorname*{Var}[f_{\bm{x}^{\prime}}\mid \mathcal{D}_{n-1},y_{\bm{x}}]\] \[=\operatorname*{arg\,min}_{\bm{x}\in\mathcal{S}}\operatorname*{tr} \Analogously to the derivation of Equation (34), this can also be expressed as

\[\bm{x}_{n}=\operatorname*{arg\,min}_{\bm{x}\in\mathcal{S}}\left| \operatorname{diag}\operatorname{Var}[\bm{f}_{\mathcal{A}}\mid\mathcal{D}_{n-1},y_{\bm{x}}]\right|.\] (38)

Effectively, MM-ITL ignores the mutual interaction between points in \(\mathcal{A}\). As can be seen from Equation (37) and as is also mentioned by MacKay (1992), MM-ITL is equivalent to VTL up to a different weighting of the points in \(\mathcal{A}\): instead of minimizing the average posterior variance (as in VTL), MM-ITL minimizes the average posterior log-variance. Under the lens of principle (\(\dagger\)), this can be seen as minimizing the average marginal entropy of predictions within the target space:

\[\bm{x}_{n}=\operatorname*{arg\,min}_{\bm{x}\in\mathcal{S}}\sum_{ \bm{x}^{\prime}\in\mathcal{A}}\operatorname{H}[f_{\bm{x}^{\prime}}\mid \mathcal{D}_{n-1},y_{\bm{x}}]\,.\]

We remark that MM-ITL is a special case of EPIG (Bickford Smith et al., 2023, Appendix E.2).

Not a generalization of uncertainty samplingUnlike ITL, MM-ITL is not a generalization of uncertainty sampling. The reason is precisely that MM-ITL ignores the mutual interaction between points in \(\mathcal{A}\). Consider the example where \(\mathcal{X}=\mathcal{S}=\mathcal{A}=\{1,\dots,10\}\) where \(\bm{f}_{1:9}\) are highly correlated while \(f_{10}\) is mostly independent of the other points. Visually, imagine a smooth function (i.e., under a Gaussian kernel) with points \(1\) through \(9\) close to each other and point \(10\) far away. Further, suppose that point \(10\) has a slightly larger marginal variance than the others. Then, MM-ITL would select one of the points \(1:9\) since this leads to the largest reduction in the marginal (log-)variances (i.e., to a small posterior "uncertainty").13 In contrast, ITL selects the point with the largest prior marginal variance (cf. Appendix C.1), point \(10\), since this leads to the largest reduction in entropy.14

Footnote 13: This is because the observation reduces uncertainty not just about the observed point itself.

Footnote 14: Because points \(\bm{f}_{1:9}\) are highly correlated, \(\operatorname{H}[\bm{f}_{1:9}]\) is already “small”.

Similarity to VTLObserve that the following decision rule is equivalent to VTL:

\[\bm{x}_{n}=\operatorname*{arg\,max}_{\bm{x}\in\mathcal{S}} \operatorname{tr}\operatorname{Var}[\bm{f}_{\mathcal{A}}\mid\mathcal{D}_{n-1 }]-\operatorname{tr}\operatorname{Var}[\bm{f}_{\mathcal{A}}\mid\mathcal{D}_{n -1},y_{\bm{x}}].\]

By Lemma C.38, for any \(\bm{x}\in\mathcal{S}\), this objective value can be tightly lower- and upper-bounded (up to constant-factors) by

\[\sum_{\bm{x}^{\prime}\in\mathcal{A}}\log\biggl{(}\frac{ \operatorname{Var}[f_{\bm{x}^{\prime}}\mid\mathcal{D}_{n-1}]}{\operatorname{ Var}[f_{\bm{x}^{\prime}}\mid\mathcal{D}_{n-1},y_{\bm{x}}]}\biggr{)}\] \[=2\sum_{\bm{x}^{\prime}\in\mathcal{A}}\operatorname{I}(f_{\bm{x} ^{\prime}};y_{\bm{x}}\mid\mathcal{D}_{n-1})\] (see MM-ITL) \[\stackrel{{(i)}}{{=}}-\sum_{\bm{x}^{\prime}\in \mathcal{A}}\log\Bigl{(}1-\operatorname{Cor}[f_{\bm{x}^{\prime}},y_{\bm{x}} \mid\mathcal{D}_{n-1}]^{2}\Bigr{)}\] (39)

where \((i)\) is detailed in example 8.5.1 of Cover (1999). Thus, VTL and MM-ITL are closely related.

ExperimentsIn our experiments with Gaussian processes from Figures 2 and 6, we observe that MM-ITL performs similarly to VTL and CTL.

Convergence of uncertaintyWe derive a convergence guarantee for MM-ITL which is analogous to the guarantees for ITL from Theorem C.12 and for VTL from Theorem C.13. We will assume for simplicity that \(\Gamma_{n}\) is monotonically decreasing in \(n\) (i.e., \(\alpha_{n}\leq 1\)).

**Theorem D.1** (Convergence of uncertainty reduction of MM-ITL).: _Assume that Assumptions B.1 and B.2 are satisfied. Then for any \(n\geq 1\), if \(\Gamma_{0}\geq\dots\geq\Gamma_{n-1}\) and the sequence \(\{\bm{x}_{i}\}_{i=1}^{n}\) is generated by MM-ITL, then_

\[\Gamma_{n-1}\leq\frac{1}{n}\sum_{\bm{x}^{\prime}\in\mathcal{A}} \gamma_{n}(\{\bm{x}^{\prime}\};\mathcal{S}).\] (40)

Proof.: We have

\[\Gamma_{n-1}=\frac{1}{n}\sum_{i=0}^{n-1}\Gamma_{n-1}\]\[\overset{(i)}{\leq}\frac{1}{n}\sum_{i=0}^{n-1}\Gamma_{i}\] \[=\frac{1}{n}\sum_{i=0}^{n-1}\max_{\bm{x}\in\mathcal{S}}\sum_{\bm{x} ^{\prime}\in\mathcal{A}}\mathrm{I}(f_{\bm{x}^{\prime}};y_{\bm{x}}\mid\mathcal{ D}_{n})\] \[\overset{(ii)}{=}\frac{1}{n}\sum_{i=0}^{n-1}\sum_{\bm{x}^{\prime} \in\mathcal{A}}\mathrm{I}(f_{\bm{x}^{\prime}};y_{\bm{x}_{n+1}}\mid\mathcal{ D}_{n})\] \[\overset{(iii)}{=}\frac{1}{n}\sum_{\bm{x}^{\prime}\in\mathcal{A}} \sum_{i=0}^{n-1}\mathrm{I}(f_{\bm{x}^{\prime}};y_{\bm{x}_{n+1}}\mid y_{\bm{x}_{ 1:n}})\] \[\overset{(iv)}{=}\frac{1}{n}\sum_{\bm{x}^{\prime}\in\mathcal{A}} \mathrm{I}\big{(}f_{\bm{x}^{\prime}};y_{\bm{x}_{1:n}}\big{)}\] \[\leq\frac{1}{n}\sum_{\bm{x}^{\prime}\in\mathcal{A}}\max_{ \begin{subarray}{c}X\subseteq\mathcal{S}\\ |X|=n\end{subarray}}\mathrm{I}(f_{\bm{x}^{\prime}};y_{X})\] \[=\frac{1}{n}\sum_{\bm{x}^{\prime}\in\mathcal{A}}\gamma_{n}(\{\bm{ x}^{\prime}\};\mathcal{S})\]

where \((i)\) follows by assumption; \((ii)\) follows from the MM-ITL decision rule; \((iii)\) uses that the posterior variance of Gaussians is independent of the realization and only depends on the _location_ of observations; and \((iv)\) uses the chain rule of mutual information. The remainder of the proof is analogous to the proof of Theorem C.12 (cf. Appendix C.5). 

Noting that

\[\mathrm{I}(f_{\bm{x}^{\prime}};y_{\bm{x}}\mid\mathcal{D}_{n-1}) \leq\sum_{\bm{x}^{\prime}\in\mathcal{A}}\mathrm{I}(f_{\bm{x}^{\prime}};y_{\bm {x}}\mid\mathcal{D}_{n-1})\]

for any \(n\geq 1\), \(\bm{x}\in\mathcal{X}\), and \(\bm{x}^{\prime}\in\mathcal{A}\), Theorem 3.2 can be readily rederived for MM-ITL (cf. Lemmas C.14 and C.18). Hence, the posterior marginal variances of MM-ITL can be bounded uniformly in terms of \(\Gamma_{n}\) analogously to ITL.

### Correlation-based Transductive Learning

We will briefly look at the CTL (_Correlation-based TL_) decision rule

\[\bm{x}_{n}=\operatorname*{arg\,max}_{\bm{x}\in\mathcal{S}}\sum_{ \bm{x}^{\prime}\in\mathcal{A}}\mathrm{Cor}[f_{\bm{x}},f_{\bm{x}^{\prime}}\mid \mathcal{D}_{n-1}]\] (41)

which permits no interpretation under principle (\(\dagger\)). However, if all correlations are non-negative (such as for the standard Gaussian and Matern kernels), CTL is closely related to ITL, VTL, and MM-ITL (cf. Equations (35) and (39)). In this case, if \(|\mathcal{A}|=1\), then all decision rules coincide.

If, on the other hand, correlations may be negative then there is a crucial difference between CTL and the decision rules motivated from principle (\(\dagger\)). Namely, decision rules following (\(\dagger\)) exhibit a preference for points with high _absolute_ correlation to prediction targets as opposed to CTL which prefers points with high _positive_ correlation. This stems from the intuitive fact that points with a strong negative correlation are equally informative as points with a strong positive correlation. Nevertheless, we observe in our experiments that (even for a linear kernel which does not ensure non-negative correlations) points selected by ITL and VTL are typically positively correlated with prediction targets.

### Summary

We have seen that ITL, VTL, and MM-ITL can be seen as different interpretations of the same fundamental principle (\(\dagger\)), with the approximations CTL. If \(|\mathcal{A}|=1\) and correlations are non-negative, then all four decision rules are equivalent. CTL prefers points with high positive correlation whereas the other decision rules prefer points with high absolute correlation. ITL is the only decision rule that takes into account the mutual dependence between points in \(\mathcal{A}\), and VTL and MM-ITL differ only in their weighting of the posterior marginal variances of points in \(\mathcal{A}\).

## Appendix E Stochastic Target Spaces

When the target space \(\mathcal{A}\) is large, it may be computationally infeasible to compute the exact objective. A natural approach to address this issue is to approximate the target space by a smaller set of size \(K\).

Discretizing the target spaceOne possibility is to discretize the target space \(\mathcal{A}\). Compact target spaces can be addressed, e.g., via discretization arguments which are common in the Bayesian optimization literature (see, e.g., appendix C.1 of Srinivas et al. (2009)). That is, if the target space can be covered approximately using a finite (possibly large) set of points, the guarantees of Theorem 3.2 extend directly. This, however, can be impractical when the required size of discretization for sufficiently small approximation error is large. In the following, we briefly discuss a natural alternative approach based on sampling points from \(\mathcal{A}\).

Target distributionsLet \(\mathcal{A}\subseteq\mathcal{X}\) be a (possibly continuous) target space, and let \(\mathcal{P}_{\mathcal{A}}\) be a probability distribution supported on \(\mathcal{A}\). In iteration \(n\), a subset \(A_{n}\) of \(K\) points is sampled independently from \(\mathcal{A}\) according to the distribution \(\mathcal{P}_{\mathcal{A}}\) and the objective is computed on this subset. Formally, this amounts to a single-sample Monte Carlo approximation of

\[\bm{x}_{n}\in\operatorname*{arg\,max}_{\bm{x}\in\mathcal{S}}\mathbb{E}_{A} \mathop{\sim}\limits^{\operatorname*{id}}\mathcal{P}_{\mathcal{A}}[\mathbb{I }(\bm{f}_{\bm{k}};y_{\bm{x}}\mid\mathcal{D}_{n-1})].\] (42)

The convergence guarantees from Appendix C can be generalized to the setting of stochastic target spaces by estimating how often points "near" a specified prediction target \(\bm{x}\in\mathcal{A}\) are sampled.

**Definition E.1** (\(\gamma\)-ball at \(\bm{x}\)).: Given \(\bm{x}\in\mathcal{A}\) and any \(\gamma\geq 0\), we call the set

\[B_{\gamma}(\bm{x})\mathop{=}\limits^{\operatorname*{def}}\{\bm{x}^{\prime} \in\mathcal{X}\mid\|\bm{x}-\bm{x}^{\prime}\|\leq\gamma\}\]

the \(\gamma\)-ball at \(\bm{x}\). Further, we call \(\mathcal{P}_{\mathcal{A}}(B_{\gamma}(\bm{x}))\) the weight of that ball.

**Proposition E.2** (sketch).: _Given any \(n\geq 1,K\geq 1,\gamma>0\), and \(\bm{x}\in\mathcal{A}\), suppose that \(B_{\gamma}(\bm{x})\) has weight \(p>0\). Assume that the ITL objective is \(L_{I}\)-Lipschitz continuous. Then, with probability at least \(1-\exp(-(1-p)n/(8K))\),_

\[\sigma_{n}^{2}(\bm{x})\lesssim\eta_{\mathcal{S}}^{2}(\bm{x})+CL_{I}\gamma \frac{\gamma_{k(n)}}{\sqrt{k(n)}}\]

_where \(k(n)\mathop{=}\limits^{\operatorname*{def}}Kpn/2\)._

Proof sketch.: Let \(Y_{i}\sim\operatorname*{Binom}(K,p)\) denote the random variable counting the number of occurrences of a point from \(B_{\gamma}(\bm{x})\) in \(A_{i}\). Moreover, we write \(X_{i}\mathop{=}\limits^{\operatorname*{def}}1\{B_{\gamma}(\bm{x})\cap A_{i}\neq\emptyset\}\). Note that

\[\nu\mathop{=}\limits^{\operatorname*{def}}\mathbb{E}X_{i}=\mathbb{P}(B_{ \gamma}(\bm{x})\cap A_{i}\neq\emptyset)=1-\mathbb{P}(Y_{i}=0)=1-(1-p)^{K} \approx Kp\]

where the approximation stems from a first-order truncation of the Bernoulli series. Let \(X\mathop{=}\limits^{\operatorname*{def}}\sum_{i=1}^{n}X_{i}\) with \(\mathbb{E}X=n\nu\approx Kpn\).

Using the assumed Lipschitz-continuity of the objective, we know that \(\operatorname{I}(\bm{f}_{\mathcal{A}^{\prime}};y_{\bm{x}}\mid\mathcal{D}_{n-1 })\leq L_{I}\gamma\mathbb{I}(\bm{f}_{\mathcal{A}};y_{\bm{x}}\mid\mathcal{D}_{ n-1})\) where \(A^{\prime}\mathop{=}\limits^{\operatorname*{def}}(A\setminus\{\bm{x}_{\gamma}\}) \cup\{\bm{x}\}\) and \(\bm{x}_{\gamma}\) is the point from the \(\gamma\)-ball at \(\bm{x}\). The bound then follows analogously to Theorem 3.2.

Finally, by Chernoff's bound, at least \(Kpn/2\) iterations contain a point from \(B_{\gamma}(\bm{x})\) with probability at least \(1-\exp(-Kpn/8)\). 

This strategy can also be used to generalize the VTL, CTL, and MM-ITL objectives to stochastic target spaces.

## Appendix F Closed-form Decision Rules

Below, we list the closed-form expressions for the ITL and VTL objectives. In the following, \(k_{n}\) denotes the kernel conditional on \(\mathcal{D}_{n}\).

**ITL**

\[\mathrm{I}(\bm{f}_{\mathcal{A}};y_{\bm{x}}\mid\mathcal{D}_{n-1}) =\frac{1}{2}\log\biggl{(}\frac{\mathrm{Var}[y_{\bm{x}}\mid\mathcal{ D}_{n-1}]}{\mathrm{Var}[y_{\bm{x}}\mid\bm{f}_{\mathcal{A}},\mathcal{D}_{n-1}]} \biggr{)}\] (43) \[=\frac{1}{2}\log\Biggl{(}\frac{k_{n-1}(\bm{x},\bm{x})+\rho^{2}}{ \hat{k}_{n-1}(\bm{x},\bm{x})+\rho^{2}}\Biggr{)}\]

where \(\hat{k}_{n}(\bm{x},\bm{x})=k_{n}(\bm{x},\bm{x})-\bm{k}_{n}(\bm{x},\mathcal{A}) \bm{K}_{n}(\mathcal{A},\mathcal{A})^{-1}\bm{k}_{n}(\mathcal{A},\bm{x})\).

**VTL**

\[\mathrm{tr}\ \mathrm{Var}[\bm{f}_{\mathcal{A}}\mid\mathcal{D}_{n-1},y_{ \bm{x}}]=\sum_{\bm{x}^{\prime}\in\mathcal{A}}\biggl{(}k_{n-1}(\bm{x}^{\prime},\bm{x}^{\prime})-\frac{k_{n-1}(\bm{x},\bm{x}^{\prime})^{2}}{k_{n-1}(\bm{x}, \bm{x})+\rho^{2}}\biggr{)}.\]

## Appendix G Computational Complexity

Evaluating the acquisition function of ITL in round \(n\) requires computing for each \(\bm{x}\in\mathcal{S}\),

\[\mathrm{I}(\bm{f}_{\mathcal{A}};y_{\bm{x}}\mid\mathcal{D}_{n})\] \[=\frac{1}{2}\log\biggl{(}\frac{|\mathrm{Var}[\bm{f}_{\mathcal{A}} \mid\mathcal{D}_{n}]|}{|\mathrm{Var}[\bm{f}_{\mathcal{A}}\mid y_{\bm{x}}, \mathcal{D}_{n}]|}\biggr{)} \text{(forward)}\] \[=\frac{1}{2}\log\biggl{(}\frac{\mathrm{Var}[y_{\bm{x}}\mid \mathcal{D}_{n}]}{\mathrm{Var}[y_{\bm{x}}\mid\bm{f}_{\mathcal{A}},\mathcal{D}_ {n}]}\biggr{)} \text{(backward)}.\]

Let \(|\mathcal{S}|=m\) and \(|\mathcal{A}|=k\). Then, the forward method has complexity \(O\bigl{(}m\cdot k^{3}\bigr{)}\). For the backward method, observe that the variances are scalar and the covariance matrix \(\mathrm{Var}[\bm{f}_{\mathcal{A}}\mid\mathcal{D}_{n}]\) only has to be inverted once for all points \(\bm{x}\). Thus, the backward method has complexity \(O\bigl{(}k^{3}+m\bigr{)}\).

When the size \(m\) of \(\mathcal{S}\) is relatively small (and hence, all points in \(\mathcal{S}\) can be considered during each iteration of the algorithm), GP inference corresponds simply to computing conditional distributions of a multivariate Gaussian. The performance can therefore be improved by keeping track of the full posterior distribution over \(\bm{f}_{\mathcal{S}}\) of size \(O\bigl{(}m^{2}\bigr{)}\) and conditioning on the latest observation during each iteration of the algorithm. In this case, after each observation the posterior can be updated at a cost of \(O\bigl{(}m^{2}\bigr{)}\) which does not grow with the time \(n\), unlike classical GP inference.

Overall, when \(m\) is small, the computational complexity of ITL is \(O\bigl{(}k^{3}+m^{2}\bigr{)}\). When \(m\) is large (or possibly infinite) and a subset of \(\tilde{m}\) points is considered in a given iteration, the computational complexity of ITL is \(O\bigl{(}k^{3}+\tilde{m}\cdot n^{3}\bigr{)}\), neglecting the complexity of selecting the \(\tilde{m}\) candidate points. In the latter case, the computational cost of ITL is dominated by the cost of GP inference.

Khanna et al. (2017) discuss distributed and stochastic approximations of greedy algorithms to (weakly) submodular problems that are also applicable to ITL.

## Appendix H Additional GP Experiments & Details

We use homoscedastic Gaussian noise with standard deviation \(\rho=0.1\) and a discretization of \(\mathcal{X}=[-3,3]^{2}\) of size \(2\,500\). Uncertainty bands correspond to one standard error over \(10\) random seeds.

**Additional experiments** Figure 6 includes the following additional experiments:

1. _Extrapolation Setting (\(\mathcal{A}\cap\mathcal{S}=\emptyset\)):_ Right experiment from Figure 2 under the Gaussian kernel. ITL has a similar advantage as in the setting shown in Figure 3.
2. _Heteroscedastic Noise_: Left experiment from Figure 2 under the Gaussian kernel with heteroscedastic Gaussian noise \[\rho(\bm{x})=\begin{cases}1&\text{if }\bm{x}\in[-\frac{1}{2},\frac{1}{2}]^{2}\\ 0.1&\text{otherwise}\end{cases}.\] If observation noise is heteroscedastic, in considering _posterior_ rather than _prior_ uncertainty, ITL avoids points with high aleatoric uncertainty, which accelerates learning.

3. _Effect of Smoothness_: Experiment from Figure 3 under the Laplace kernel. All algorithms except for US and Random perform equally well. This validates our claims from Section 3.3: in the extreme non-smooth case of a Laplace kernel and \(\mathcal{A}\subseteq\mathcal{S}\), points outside \(\mathcal{A}\) do not provide any additional information, and ITL and "local" UnSa coincide.
4. _Sparser Target_: Experiment from Figure 3 under the Gaussian kernel, but with domain extended to \(\mathcal{X}=[-10,10]^{2}\).

Hyperparameters of TruVarAs suggested by Bogunovic et al. (2016), we use \(\tilde{\eta}^{2}_{\{1\}}=1\), \(r=0.1\), and \(\delta=0\) (even though the theory only holds for \(\delta>0\)). The TruVar baseline only applies when \(\mathcal{A}\subseteq\mathcal{S}\) (cf. Section 6).

Smoothing to reduce numerical noiseApplied running average with window \(5\) to entropy curves of Figures 2 and 6 to smoothen out numerical noise.

## Appendix I Alternative Settings for Active Fine-Tuning

In our main experiments, we consider the setting \(\mathcal{A}\cap\mathcal{S}=\emptyset\), i.e., the prediction targets cannot be used for fine-tuning since their labels are not known. This setting is particularly relevant for practical applications where the model is fine-tuned dynamically at test time to each prediction target (or a small set of prediction target). Put differently, in this "transductive" setting, extrapolation to new prediction targets happens at _test-time_ with knowledge of the prediction target(s). This is in contrast to a more traditional "inductive" setting, where extrapolation happens at _train-time_ without knowledge of the concrete prediction targets, but under the assumption of samples from (or knowledge of) the target distribution. In the following, we briefly survey two settings motivated from an "inductive" perspective.

### Prediction Targets are Contained in Sample Space: \(\mathcal{A}\subseteq\mathcal{S}\)

If labels can be obtained cheaply, one can also fine-tune on the prediction targets directly, i.e., \(\mathcal{A}\subseteq\mathcal{S}\). Note, however, that the set \(\mathcal{A}\) is still assumed to be small (e.g., \(|\mathcal{A}|=100\) in the CIFAR-100 experiment). We perform an experiment in this setting and report the results in Figure 7. The experiment shows that -- similarly to the GP experiment from Figure 2 -- there can be _additional value_ in fine-tuning the model on relevant data selected from \(\mathcal{S}\) beyond simply fine-tuning the model on \(\mathcal{A}\).

Figure 6: Additional GP experiments

### Active Domain Adaptation

Active DA (Rai et al., 2010; Saha et al., 2011; Berlind and Urner, 2015) studies the problem of selecting the most informative samples from a (large) target domain \(\mathcal{A}\), given a model trained on a source domain \(\mathcal{S}\). This problem can be cast as an instance of transductive active learning with target space \(\mathcal{A}\) and sample space \(\mathcal{S}^{\prime}=\mathcal{S}\cup\mathcal{A}\) where the model is already conditioned on all of \(\mathcal{S}\). This is slightly different from the setting considered in Section 4 where \(\mathcal{A}\) is small and not necessarily part of the sample space. We hypothesize that ITL behaves similarly to recent work on active DA (Su et al., 2020; Prabhu et al., 2021; Fu et al., 2021): querying informative and diverse samples from \(\mathcal{A}\) that are dissimilar to \(\mathcal{S}\). Evaluating ITL and VTL empirically in this setting is a promising direction for future work.

## Appendix J Additional NN Experiments & Details

We outline the active fine-tuning of NNs in Algorithm 1.

``` Given: initialized or pre-trained model \(f\), small sample \(A\sim\mathcal{P}_{\mathcal{A}}\)  initialize dataset \(\mathcal{D}=\emptyset\) repeat  sample \(S\sim\mathcal{P}_{\mathcal{S}}\)  subsample target space \(A^{\prime}\stackrel{{\text{u.a.r.}}}{{\sim}}A\)  initialize batch \(B=\emptyset\)  compute kernel matrix \(\bm{K}\) over domain \([S,A^{\prime}]\) repeat\(b\) times  compute acquisition function w.r.t. \(A^{\prime}\), based on \(\bm{K}\)  add maximizer \(\bm{x}\in S\) of acquisition function to \(B\)  update conditional kernel matrix \(\bm{K}\)  obtain labels for \(B\) and add to dataset \(\mathcal{D}\)  update \(f\) using data \(\mathcal{D}\) ```

**Algorithm 1** Active Fine-Tuning of NNs

In Appendix J.1, we detail metrics and hyperparameters. We describe in Appendices J.2 and J.3 how to compute the (initial) conditional kernel matrix \(\bm{K}\), and in Appendix J.4 how to update this matrix \(\bm{K}\) to obtain conditional embeddings for batch selection.

In Appendix J.5, we show that ITL and CTL significantly outperform a wide selection of commonly used heuristics. In Appendices J.6 and J.7, we conduct additional experiments and ablations.

Figure 7: Evaluation of CIFAR-100 experiment in the setting \(\mathcal{A}\subseteq\mathcal{S}\), i.e., one can also sample from the \(100\) prediction targets \(\mathcal{A}\). The solid black line denotes the performance of the model fine-tuned on all of \(\mathcal{A}\). This experiment shows that there is _additional value_ in fine-tuning the model on relevant data from \(\mathcal{S}\) beyond simply fine-tuning the model on \(\mathcal{A}\). The baselines are summarized in Appendix J.5Hubotter et al. (2024) discusses additional motivation and related work that has previously studied active fine-tuning, but which has largely focused on the training algorithm rather than data selection.

### Experiment Details

We evaluate the accuracy with respect to \(\mathcal{P}_{\mathcal{A}}\) using a Monte Carlo approximation with out-of-sample data:

\[\text{accuracy}(\widehat{\boldsymbol{\theta}})\approx\mathbb{E}_{(\boldsymbol{x },y)\sim\mathcal{P}_{\mathcal{A}}}\mathbbm{1}\{y=\operatorname*{arg\,max}_{i }f_{i}(\boldsymbol{x};\widehat{\boldsymbol{\theta}})\}.\]

We provide an overview of the hyperparameters used in our NN experiments in Table 1. The effect of noise standard deviation \(\rho\) is small for all tested \(\rho\in[1,100]\) (cf. ablation study in Table 2).15\(M\) denotes the size of the sample \(A\sim\mathcal{P}_{\mathcal{A}}\). In each iteration, we select the target space \(\mathcal{A}\gets A^{\prime}\) as a random subset of \(m\) points from \(A\).16 We provide an ablation over \(m\) in Appendix J.6.

Footnote 15: We use a larger noise standard deviation \(\rho\) in CIFAR-100 to stabilize the numerics of batch selection via conditional embeddings (cf. Table 2).

Footnote 16: This appears to improve the training, likely because it prevents overfitting to peculiarities in the finite sample \(A\) (cf. Figure 16).

During each iteration, we select the batch \(B\) according to the decision rule from a random sample from \(\widehat{\mathcal{P}}_{\mathcal{S}}\) of size \(k\).17

Footnote 17: In large-scale problems, the work of Coleman et al. (2022) suggests to use an (approximate) nearest neighbor search to select the (large) candidate set rather than sampling u.a.r. from \(\mathcal{P}_{\mathcal{S}}\). This can be a viable alternative to simply increasing \(k\) and suggests future work.

Since we train the MNIST model from scratch, we train from random initialization until convergence on oracle validation accuracy.18 We do this to stabilize the learning curves, and provide the least biased (due to the training algorithm) results. For CIFAR-100, we train for \(5\) epochs (starting from the previous iterations' model) which we found to be sufficient to obtain good performance.

Footnote 18: That is, to stop training as soon as accuracy on a validation set from \(\mathcal{P}_{\mathcal{A}}\) decreases in an epoch.

We use the ADAM optimizer (Kingma and Ba, 2014). In our CIFAR-100 experiments, we use a pre-trained EfficientNet-B0 (Tan and Le, 2019), and fine-tune the final and penultimate layers. We freeze earlier layers to prevent overfitting to the "few-shot" training data.

To prevent numerical inaccuracies when computing the ITL objective, we optimize

\[\text{I}(\boldsymbol{y}_{\mathcal{A}};y_{\boldsymbol{x}}\mid\mathcal{D}_{n-1} )=\frac{1}{2}\log\biggl{(}\frac{\operatorname*{Var}[y_{\boldsymbol{x}}\mid \mathcal{D}_{n-1}]}{\operatorname*{Var}[y_{\boldsymbol{x}}\mid\boldsymbol{y}_ {\mathcal{A}},\mathcal{D}_{n-1}]}\biggr{)}\] (44)

instead of Equation (43), which amounts to adding \(\rho^{2}\) to the diagonal of the covariance matrix before inversion. This appears to improve numerical stability, especially when using gradient embeddings.19

Footnote 19: In our experiments, we observe that the effect of various choices of \(\rho\) on this slight adaptation of the ITL decision rule has negligible impact on performance. The more prominent effect of \(\rho\) appears to arise from the batch selection via conditional embeddings (cf. Table 2).

\begin{table}
\begin{tabular}{l l l} \hline \hline  & MNIST & CIFAR-100 \\ \hline \(\rho\) & \(0.01\) & \(1\) \\ \(M\) & \(30\) & \(100\) \\ \(m\) & \(3\) & \(10\) \\ \(k\) & \(1\,000\) & \(1\,000\) \\ batch size \(b\) & \(1\) & \(10\) \\ \# of epochs & (*) & \(5\) \\ learning rate & \(0.001\) & \(0.001\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Hyperparameter summary of NN experiments. (*) we train until convergence on oracle validation accuracy.

In our experiments, we use last-layer neural tangent embeddings20 and \(\bm{\Sigma}=\bm{I}\) to evaluate ITL and VTL, and select inputs for labeling and training \(f\). Notably, we use this linear Gaussian approximation of \(f\) only to guide the active data selection and not for inference.

Footnote 20: We observe essentially the same performance with loss gradient embeddings, cf. Appendix J.2.

### Embeddings and Kernels

Using a neural network to parameterize \(f\), we evaluate the canonical approximations of \(f\) by a stochastic process in the following.

An embedding \(\bm{\phi}(\bm{x})\) is a latent representation of an input \(\bm{x}\). Collecting the embeddings as rows in the design matrix \(\bm{\Phi}\) of a set of inputs \(X\), one can approximate the network by the linear function \(\bm{f}_{X}=\bm{\Phi}\bm{\beta}\) with weights \(\bm{\beta}\). Approximating the weights by \(\bm{\beta}\sim\mathcal{N}(\bm{\mu},\bm{\Sigma})\) implies that \(\bm{f}_{X}\sim\mathcal{N}(\bm{\Phi}\bm{\mu},\bm{\Phi}\bm{\Sigma}\bm{\Phi}^{\top})\). The covariance matrix \(\bm{K}_{XX}=\bm{\Phi}\bm{\Sigma}\bm{\Phi}^{\top}\) can be succinctly represented in terms of its associated kernel \(k(\bm{x},\bm{x}^{\prime})=\bm{\phi}(\bm{x})^{\top}\bm{\Sigma}\bm{\phi}(\bm{x} ^{\prime})\). Here,

* \(\bm{\phi}(\bm{x})\) is the latent representation of \(\bm{x}\), and
* \(\bm{\Sigma}\) captures the dependencies in the latent space.

While any choice of embedding \(\bm{\phi}\) is possible, the following are common choices:

1. _Last-Layer_: A common choice for \(\bm{\phi}(\bm{x})\) is the representation of \(\bm{x}\) from the penultimate layer of the neural network (Holzmuller et al., 2023). Interpreting the early layers as a feature encoder, this uses the low-dimensional feature map akin to random feature methods (Rahimi and Recht, 2007).
2. _Output Gradients (eNTK)_: Another common choice is \(\bm{\phi}(\bm{x})=\bm{\nabla}_{\bm{\theta}}\,\bm{f}(\bm{x};\bm{\theta})\) where \(\bm{\theta}\) are the network parameters (Holzmuller et al., 2023). Its associated kernel is known as the _empirical neural tangent kernel_ (eNTK) and the posterior mean of this GP approximates ultra-wide NNs trained with gradient descent (Jacot et al., 2018; Arora et al., 2019; Lee et al., 2019; Khan et al., 2019; He et al., 2020; Malladi et al., 2023). Kassraie and Krause (2022) derive bounds of \(\gamma_{n}\) under this kernel. If \(\bm{\theta}\) is restricted to the weights of the final linear layer, then this embedding is simply the last-layer embedding.
3. _Loss Gradients_: Another possible choice is \[\bm{\phi}(\bm{x})=\bm{\nabla}_{\bm{\theta}}\left.\left\langle\bm{f}(\bm{x}; \bm{\theta}),\hat{y}(\bm{x})\right\rangle\right|_{\bm{\theta}=\widehat{\bm{ \theta}}}\] where \(\ell\) is a loss function, \(\hat{y}(\bm{x})\) is the predicted label, and \(\widehat{\bm{\theta}}\) are the current parameter estimates (Ash et al., 2020).
4. _Outputs (eNNGP)_: Another possible choice is \(\bm{\phi}(\bm{x})=\bm{f}(\bm{x})\), i.e., the output of the network. Its associated kernel is known as the _empirical neural network Gaussian process_ (eNNGP) kernel (Lee et al., 2018).
5. _Predictive_(Kirsch, 2023): Given a Bayesian neural network (Blundell et al., 2015) or probabilistic (deep) ensemble (Lakshminarayanan et al., 2017), which induce samples \(\bm{\theta}_{1},\dots,\bm{\theta}_{K}\sim p(\bm{\theta})\) from the distribution over network parameters, one can approximate the predictive covariance \(k(\bm{x},\bm{x}^{\prime})=\mathrm{Cov}_{\bm{\theta}}[f(\bm{x};\bm{\theta}),f( \bm{x}^{\prime};\bm{\theta})]\). This kernel measures proximity in the prediction space rather than parameter space and as such does not require gradient information. The corresponding feature map is \(\bm{\phi}(\bm{x})=\frac{1}{\sqrt{K}}[\bar{f}(\bm{x};\bm{\theta}_{1})\ \cdots\ \bar{f}(\bm{x};\bm{\theta}_{K})]^{\top}\) where \(f(\bm{x};\bm{\theta}_{k})\stackrel{{\mathrm{def}}}{{=}}f(\bm{x}; \bm{\theta}_{k})-\frac{1}{K}\sum_{l=1}^{K}f(\bm{x};\bm{\theta}_{l})\).

In the additional experiments from this appendix we use last-layer embeddings unless noted otherwise. We compare the performance of last-layer and the loss gradient embedding

\[\bm{\phi}(\bm{x})=\left.\bm{\nabla}_{\bm{\theta}^{\prime}}\,\ell_{\mathrm{CE} }(\bm{f}(\bm{x};\bm{\theta}),\hat{y}(\bm{x}))\right|_{\bm{\theta}=\widehat{\bm {\theta}}}\] (45)

where \(\bm{\theta}^{\prime}\) are the parameters of the final output layer, \(\widehat{\bm{\theta}}\) are the current parameter estimates, \(\hat{y}(\bm{x})=\arg\max_{i}f_{i}(\bm{x};\widehat{\bm{\theta}})\) are the associated predicted labels, and \(\ell_{\mathrm{CE}}\) denotes the cross-entropy loss. This gradient embedding captures the potential update direction upon observing a new point (Ash et al., 2020). Moreover, Ash et al. (2020) show that for most neural networks, the norm of these gradient embeddings are a conservative lower bound to the norm assumed by taking any other proxy label \(\hat{y}(\bm{x})\). In Figure 8, we observe only negligible differences in performance between this and the last-layer embedding.

### Towards Uncertainty Quantification in Latent Space

A straightforward and common approximation of the uncertainty about NN weights is given by \(\bm{\Sigma}=\bm{I}\), and we use this approximation throughout our experiments.

The poor performance of UnSa (cf. Appendix J.5) with this approximation suggests that with more sophisticated approximations, the performance of ITL, VTL, and CTL can be further improved. Further research is needed to study the effect of more sophisticated approximations of "uncertainty" in the latent space. For example, with parameter gradient embeddings, the latent space is the network parameter space where various approximations of \(\bm{\Sigma}\) based on Laplace approximation (Daxberger et al., 2021; Antoran et al., 2022), variational inference (Blundell et al., 2015), or Markov chain Monte Carlo (Maddox et al., 2019) have been studied. We also evaluate Laplace approximation (LA, Daxberger et al. (2021)) for estimating \(\bm{\Sigma}\) but see no improvement (cf. Figure 9). Nevertheless, we believe that uncertainty quantification is a promising direction for future work, with the potential to improve performance of ITL and its variations substantially.

### Batch Selection via Conditional Embeddings

We will refer to the greedy decision rule from Equation (3) as BACE, short for _Batch selection via Conditional Embeddings_. BACE can be implemented efficiently using the Gaussian approximation of \(\bm{f}_{X}\) from Appendix J.2 by iteratively conditioning on the previously selected points \(\bm{x}_{n,1:i-1}\), and updating the kernel matrix \(\bm{K}_{XX}\) using the closed-form formula for the variance of conditional

Figure 8: Comparison of loss gradient (“G-”) and last-layer embeddings (“L-”).

Figure 9: Uncertainty quantification (i.e., estimation of \(\bm{\Sigma}\)) via a Laplace approximation (LA, Daxberger et al. (2021)) over last-layer weights using a Kronecker factored log-likelihood Hessian approximation (Martens and Grosse, 2015) and the loss gradient embeddings from Equation (45). The results are shown for the MNIST experiment. We do not observe a performance improvement beyond the trivial approximation \(\bm{\Sigma}=\bm{I}\).

Gaussians:

\[\bm{K}_{XX}\leftarrow\bm{K}_{XX}-\frac{1}{\bm{K}_{\bm{x}_{j}\bm{x}_{j}}+\rho^{2}} \bm{K}_{X\bm{x}_{j}}\bm{K}_{\bm{x}_{j}X}\] (46)

where \(j\) denotes the index of the selected \(\bm{x}_{n,i}\) within \(X\) and \(\rho^{2}\) is the noise variance. Note that \(\bm{K}_{\bm{x}_{j}\bm{x}_{j}}\) is a scalar and \(\bm{K}_{X\bm{x}_{j}}\) is a row vector, and hence, this iterative update can be implemented efficiently.

We remark that Equation (3) is a natural extension of previous non-adaptive active learning methods, which typically maximize some notion of "distance" between points in the batch, to the "directed" setting (Ash et al., 2020; Zanette et al., 2021; Holzmuller et al., 2023; Pacchiano et al., 2024). BACE simultaneously maximizes "distance" between points in a batch and minimizes "distance" to points in \(\mathcal{A}\).

The sample efficiency of BACE\(B_{n}\), and therefore also the greedily constructed \(B_{n}^{\prime}\) (which gives a constant-factor approximation with respect to the objective), yields diverse batches by design. In Figure 10, we compare BACE to selecting the top-\(b\) points according to the decision rule (which does _not_ yield diverse batches). We observe a significant improvement in accuracy and data retrieval when using BACE. We expect the gap between both approaches to widen further with larger batch sizes.

Computational complexity of BACEAs derived in Appendix G, a single batch selection step of BACE has complexity \(O\big{(}b(k^{3}+m^{2})\big{)}\) where \(b\) is the size of the batch, \(k=|\mathcal{A}|\) is the size of the target space, and \(m=|\mathcal{S}|\) is the size of the candidate set. In the case of large \(m\), an alternative implementation whose runtime does not depend on \(m\) is described in Appendix G.

### Baselines

In Figure 11, we compare against additional baselines:

* Both TypiClust(Hacohen et al., 2022) and ProbCover(Yehuda et al., 2022) are recent methods to select points that "cover" the data distribution well. To maintain comparability between algorithms, we use the same embeddings as for ITL which are re-computed before every new batch selection. ITL significantly outperforms TypiClust & ProbCover, which only attempt to cover \(\mathcal{S}\) well without taking \(\mathcal{A}\) into account (i.e., are "undirected").
* Mehta et al. (2022) introduced EIG for training neural classification models, which uses the same decision rule as ITL, but approximates the conditional entropy based on the networks' softmax output rather than using a GP approximation. We approximate the conditional entropy using a single gradient step of the hallucinated updates on the parameters of the final layer, as mentioned by Mehta et al. (2022). We observe that EIG is not competitive for batch-wise selection (CIFAR-100) since it does not encourage batch diversity. Moreover, we observe that EIG is orders of magnitude slower than ITL (since it has to compute \(|\mathcal{S}|\cdot C\) individual gradient steps where \(C\) is the number of classes). We note that since our datasets are balanced, the AEIG algorithm from Mehta et al. (2022) coincides with EIG.

Figure 10: Advantage of batch selection via conditional embeddings over top-\(b\) selection in the CIFAR-100 experiment.

Since, EIG does not have an open-source implementation, we implemented it ourselves following Mehta et al. (2022). For TypiClust & ProbCover, we use the author's implementation. In the figure, we show that ITL & VTL substantially outperform all baselines.

In the following, we briefly describe other commonly used "undirected" decision rules.

Denote the softmax distribution over labels \(i\) at inputs \(\bm{x}\) by

\[p_{i}(\bm{x};\widehat{\bm{\theta}})\propto\exp(f_{i}(\bm{x};\widehat{\bm{ \theta}})).\]

The following heuristics computed based on the softmax distribution aim to quantify the "uncertainty" about a particular input \(\bm{x}\):

* MaxEntropy(Settles & Craven, 2008): \[\bm{x}_{n}=\operatorname*{arg\,max}_{\bm{x}\in\mathcal{S}}\operatorname{H}[p (\bm{x};\widehat{\bm{\theta}}_{n-1})].\]
* MaxMargin(Scheffer et al., 2001; Settles & Craven, 2008): \[\bm{x}_{n}=\operatorname*{arg\,min}_{\bm{x}\in\mathcal{S}}p_{1}(\bm{x}; \widehat{\bm{\theta}}_{n-1})-p_{2}(\bm{x};\widehat{\bm{\theta}}_{n-1})\] where \(p_{1}\) and \(p_{2}\) are the two largest class probabilities.
* LeastConfidence(Lewis & Gale, 1994; Settles & Craven, 2008; Hendrycks & Gimpel, 2017; Tamkin et al., 2022): \[\bm{x}_{n}=\operatorname*{arg\,min}_{\bm{x}\in\mathcal{S}}p_{1}(\bm{x}; \widehat{\bm{\theta}}_{n-1})\] where \(p_{1}\) is the largest class probability.

An alternative class of decision rules aims to select diverse batches by maximizing the distances between points. Embeddings \(\bm{\phi}(\bm{x})\) induce the (Euclidean) embedding distance

\[d_{\bm{\phi}}(\bm{x},\bm{x^{\prime}})\operatorname{\stackrel{{ \mathrm{def}}}{{=}}}\left\|\bm{\phi}(\bm{x})-\bm{\phi}(\bm{x^{\prime}}) \right\|_{2}.\]

Similarly, a kernel \(k\) induces the kernel distance

\[d_{k}(\bm{x},\bm{x^{\prime}})\operatorname{\stackrel{{ \mathrm{def}}}{{=}}}\sqrt{k(\bm{x},\bm{x})+k(\bm{x^{\prime}},\bm{x^{ \prime}})-2k(\bm{x},\bm{x^{\prime}})}.\]

It is straightforward to see that if \(k(\bm{x},\bm{x^{\prime}})=\bm{\phi}(\bm{x})^{\top}\bm{\phi}(\bm{x^{\prime}})\), then embedding and kernel distances coincide, i.e., \(d_{\bm{\phi}}(\bm{x},\bm{x^{\prime}})=d_{k}(\bm{x},\bm{x^{\prime}})\).

Figure 11: Comparison to baselines for the experiment of Figure 4.

* MaxDist(Holzmuller et al., 2023; Yu and Kim, 2010; Sener and Savarese, 2017; Geifman and El-Yaniv, 2017) constructs the batch by choosing the point with the maximum distance to the nearest previously selected point: \[\bm{x}_{n}=\operatorname*{arg\,max}_{\bm{x}\in\mathcal{S}}\min_{i<n}d(\bm{x}, \bm{x}_{i})\]
* Similarly, k-means++(Holzmuller et al., 2023) selects the batch via k-means++ seeding (Arthur et al., 2007; Ostrovsky et al., 2013). That is, the first centroid \(\bm{x}_{1}\) is chosen uniformly at random and the subsequent centroids are chosen with a probability proportional to the square of the distance to the nearest previously selected centroid: \[\mathbb{P}(\bm{x}_{n}=\bm{x})\propto\min_{i<n}d(\bm{x},\bm{x}_{i})^{2}.\] When using the loss gradient embeddings from Equation (45), this decision rule is known as BADGE (Ash et al., 2020).

Finally, we summarize common kernel-based decision rules.

* Undirected ITL chooses \[\bm{x}_{n} =\operatorname*{arg\,max}_{\bm{x}\in\mathcal{S}}\operatorname{I }(\bm{f}_{\mathcal{S}};y_{\bm{x}}\mid\mathcal{D}_{n-1})\] \[=\operatorname*{arg\,max}_{\bm{x}\in\mathcal{S}}\operatorname{I }(f_{\bm{x}};y_{\bm{x}}\mid\mathcal{D}_{n-1})\,.\] This can be shown to be equivalent to MaxDet(Holzmuller et al., 2023) which selects \[\bm{x}_{n}=\operatorname*{arg\,max}_{\bm{x}\in\mathcal{S}}\big{|}\bm{K}_{\bm {x}}+\sigma^{2}\bm{I}\big{|}\] where \(\bm{K}_{\bm{x}}\) denotes the kernel matrix over \(\bm{x}_{1:n-1}\cup\{\bm{x}\}\), conditioned on the prior observations \(\mathcal{D}_{n-1}\).
* UnSA(Lewis and Catlett, 1994) which with embeddings \(\bm{\phi}_{n-1}\) after round \(n-1\) corresponds to: \[\bm{x}_{n}=\operatorname*{arg\,max}_{\bm{x}\in\mathcal{S}}\sigma_{n-1}^{2}(\bm {x})=\operatorname*{arg\,max}_{\bm{x}\in\mathcal{S}}\big{\|}\bm{\phi}_{n-1}( \bm{x})\big{\|}_{2}^{2}\,.\] With batch size \(b=1\), UnSA coincides with Undirected ITL. When evaluated with gradient embeddings, this acquisition function is similar to previously used "embedding length" or "gradient length" heuristics (Settles and Craven, 2008).
* Undirected VTL(Cohn, 1993) is the special case of VTL without specified prediction targets (i.e., \(\mathcal{A}=\mathcal{S}\)). In the literature, this decision rule is also known as Bair(Holzmuller et al., 2023; Ash et al., 2021).

We compare to the abovementioned decision rules and summarize the results in Figure 12. We observe that most "undirected" decision rules perform worse (and often significantly so) than Random. This is likely due to frequently selecting points from the support of \(\mathcal{P}_{\mathcal{S}}\) which are not in the support of \(\mathcal{P}_{\mathcal{A}}\) since the points are "adversarial examples" that the model \(\widehat{\bm{\theta}}\) is not trained to perform well on. In the case of MNIST, the poor performance can also partially be attributed to the well-known "cold-start problem" (Gao et al., 2020).

In Figure 4, we also compare to the following "directed" decision rules:

* CosineSimilarity(Settles and Craven, 2008) selects \(\bm{x}_{n}=\operatorname*{arg\,max}_{\bm{x}\in\mathcal{S}}\angle_{\bm{\phi}_{ n-1}}(\bm{x},\mathcal{A})\) where \[\angle_{\bm{\phi}}(\bm{x},\mathcal{A})\stackrel{{\mathrm{def}}}{{=}} \frac{1}{\left|\mathcal{A}\right|}\sum_{\bm{x}^{\prime}\in\mathcal{A}}\frac{ \bm{\phi}(\bm{x})^{\top}\bm{\phi}(\bm{x}^{\prime})}{\left\|\bm{\phi}(\bm{x}) \right\|_{2}\left\|\bm{\phi}(\bm{x}^{\prime})\right\|_{2}}.\]
* InformationDensity(Settles and Craven, 2008) is defined as the multiplicative combination of MaxEntropy and CosineSimilarity: \[\bm{x}_{n}=\operatorname*{arg\,max}_{\bm{x}\in\mathcal{S}}\operatorname{H}[p( \bm{x};\widehat{\bm{\theta}}_{n-1})]\cdot\left(\angle_{\bm{\phi}_{n-1}}(\bm{x},\mathcal{A})\right)^{\beta}\] where \(\beta>0\) controls the relative importance of both terms. We set \(\beta=1\) in our experiments.

Figure 12: Comparison of “undirected” baselines for the experiment of Figure 4. In the MNIST experiment, UnSa and Undirected ITL coincide, and we therefore only plot the latter.

Figure 13: Imbalanced \(\mathcal{P}_{\mathcal{S}}\) experiment.

### Additional experiments

We conduct the following additional experiments:

1. _Imbalanced_\(\mathcal{P}_{\mathcal{S}}\) (Figure 13): We artificially remove \(80\%\) of the support of \(\mathcal{P}_{\mathcal{A}}\) from \(\mathcal{P}_{\mathcal{S}}\). For example, in case of MNIST, we remove \(80\%\) of the images with labels \(3\), \(6\), and \(9\) from \(\mathcal{P}_{\mathcal{S}}\). This makes the learning task more difficult, as \(\mathcal{P}_{\mathcal{A}}\) is less represented in \(\mathcal{P}_{\mathcal{S}}\), meaning that the "targets" are more sparse. The trend of ITL outperforming CTL which outperforms Random is even more pronounced in this setting.
2. _Imbalanced_\(A\sim\mathcal{P}_{\mathcal{A}}\) (Figure 14): We artificially remove \(50\%\) of part of the support of \(\mathcal{P}_{\mathcal{A}}\) while generating \(A\sim\mathcal{P}_{\mathcal{A}}\) to evaluate the robustness of ITL and CTL in presence of an imbalanced target space \(\mathcal{A}\). Concretely, in case of MNIST, we remove \(50\%\) of the images with labels \(3\) and \(6\) from \(A\). In case of CIFAR-100, we remove \(50\%\) of the images with labels \(\{0,\dots,4\}\) from \(A\). We still observe the same trends as in the other experiments.
3. VTL _& choice of \(k\)_ (Figure 15): We observe that VTL performs almost as well as ITL. Additionally, we evaluate the effect of the number of points \(k\) at which the decision rule is evaluated. Not surprisingly, we observe that the performance of ITL, VTL, and CTL improves with larger \(k\).
4. _Choice of \(m\)_ (Figure 16): Next, we evaluate the choice of \(m\), i.e., the size of the target space \(\mathcal{A}\) relative to the number \(M\) of candidate points \(A\sim\mathcal{P}_{\mathcal{A}}\). We write \(p=m/M\). We generally observe that a larger \(p\) leads to better performance (with \(p=1\) being the best choice). However, it appears that a smaller \(p\) can be beneficial with respect to accuracy when a large number of batches are selected. We believe that this may be because a smaller \(p\) improves the diversity between selected batches.
5. _Choice of \(M\)_ (Figure 17): Finally, we evaluate the choice of \(M\), i.e., the size of \(A\sim\mathcal{P}_{\mathcal{A}}\). Not surprisingly, we observe that the performance of ITL improves with larger \(M\).

### Ablation study of noise standard deviation \(\rho\)

In Table 2, we evaluate the CIFAR-100 experiment with different noise standard deviations \(\rho\). We observe that the performance of batch selection via conditional embeddings drops (mostly for the less numerically stable gradient embeddings) if \(\rho\) is too small, since this leads to numerical inaccuracies when computing the conditional embeddings. Apart from this, the effect of \(\rho\) is negligible.

Figure 15: Performance of VTL & choice of \(k\) in the CIFAR-100 experiment.

\begin{table}
\begin{tabular}{l l l l l} \hline \hline \(\rho\) & \(0.0001\) & \(0.01\) & \(1\) & \(100\) \\ \hline G-ITL & _78.26 \(\pm\) 1.40_ & _79.12 \(\pm\) 1.19_ & \(\mathbf{87.16\pm 0.29}\) & \(\mathbf{87.18\pm 0.28}\) \\ L-ITL & \(\mathbf{87.52\pm 0.48}\) & \(\mathbf{87.52\pm 0.41}\) & \(\mathbf{87.53\pm 0.35}\) & \(86.47\pm 0.22\) \\ G-CTL & _58.68 \(\pm\) 2.11_ & _81.44 \(\pm\) 1.04_ & \(86.52\pm 0.44\) & \(\mathbf{86.92\pm 0.56}\) \\ L-CTL & \(\mathbf{86.40\pm 0.71}\) & \(\mathbf{86.38\pm 0.75}\) & \(86.00\pm 0.69\) & \(84.78\pm 0.39\) \\ G-ITL (top-\(b\)) & \(85.84\pm 0.54\) & \(85.92\pm 0.52\) & \(85.84\pm 0.54\) & \(85.55\pm 0.46\) \\ L-ITL (top-\(b\)) & \(85.44\pm 0.58\) & \(85.46\pm 0.54\) & \(85.44\pm 0.59\) & \(85.29\pm 0.36\) \\ G-CTL (top-\(b\)) & \(82.27\pm 0.67\) & \(82.27\pm 0.67\) & \(82.27\pm 0.67\) & \(82.27\pm 0.67\) \\ L-CTL (top-\(b\)) & \(80.73\pm 0.68\) & \(80.73\pm 0.68\) & \(80.73\pm 0.68\) & \(80.73\pm 0.68\) \\ BADGE & \(83.24\pm 0.60\) & \(83.24\pm 0.60\) & \(83.24\pm 0.60\) & \(83.24\pm 0.60\) \\ InformationDensity & \(79.24\pm 0.51\) & \(79.24\pm 0.51\) & \(79.24\pm 0.51\) & \(79.24\pm 0.51\) \\ Random & \(82.49\pm 0.66\) & \(82.49\pm 0.66\) & \(82.49\pm 0.66\) & \(82.49\pm 0.66\) \\ \hline \hline \end{tabular}
\end{table}
Table 2: Ablation study of noise standard deviation \(\rho\) in the CIFAR-100 experiment. We list the accuracy after \(100\) rounds per decision rule, with its standard error over \(10\) random seeds. “(top-\(b\))” denotes variants where batches are selected by taking the top-\(b\) points according to the decision rule rather than using batch selection via conditional embeddings. Shown in **bold** are the best performing decision rules, and shown in _italics_ are results due to numerical instability.

Figure 16: Evaluation of the choice of \(m\) relative to the size \(M\) of \(A\sim\mathcal{P}_{\mathcal{A}}\). Here, \(p=m/M\).

Figure 17: Evaluation of the choice of \(M\), i.e., the size of \(A\sim\mathcal{P}_{\mathcal{A}}\), in the CIFAR-100 experiment.

## Appendix K Additional Safe BO Experiments & Details

In Appendix K.1, we discuss the use of stochastic target spaces in the safe BO setting. We provide a comprehensive overview of prior works in Appendix K.2 and an additional experiment highlighting that ITL, unlike SafeOpt, is able to "jump past local barriers" in Appendix K.3. In Appendix K.4, we provide details on the experiments from Figure 5.

### A More Exploitative Stochastic Target Space

Alternatively to the target space \(\mathcal{A}_{n}\) which comprises all potentially optimal points, we evaluate the stochastic target space

\[\mathcal{P}_{\!A_{n}}(\cdot)=\mathbb{P}(\operatorname*{arg\,max}_{\bm{x}\in \mathcal{X}:g(\bm{x})\geq 0}f(\bm{x})=\cdot\mid\mathcal{D}_{n})\] (47)

which effectively weights points in \(\mathcal{A}_{n}\) according to how likely they are to be the safe optimum, and is therefore more exploitative than the uniformly-weighted target space discussed so far. Samples from \(\mathcal{P}_{\!A_{n}}\) can be obtained efficiently via Thompson sampling Thompson (1933); Russo et al. (2018). Observe that \(\mathcal{P}_{\!A_{n}}\) is supported precisely on the set of potential maximizers \(\mathcal{A}_{n}\). We provide a formal analysis of stochastic target spaces in Appendix E. Whether transductive active learning with \(\mathcal{A}_{n}\) or \(\mathcal{P}_{\!A_{n}}\) performs better is task-dependent, as we will see in the following.

Note that performing ITL with this target space is analogous to output-space entropy search Wang and Jegelka (2017). Samples from \(\mathcal{P}_{\!A_{n}}\) can be obtained via Thompson sampling Thompson (1933); Russo et al. (2018). That is, in iteration \(n+1\), we sample \(K\in\mathbb{N}\) independent functions \(f^{(j)}\sim f\mid\mathcal{D}_{n}\) from the posterior distribution and select \(K\) points \(\bm{x}^{(1)},\dots,\bm{x}^{(K)}\) which are a safe maximum of \(f^{(1)},\dots,f^{(K)}\), respectively.

ExperimentsIn Figure 18, we contrast the performance of ITL with \(\mathcal{P}_{\!A_{n}}\) to the performance of ITL with the exact target space \(\mathcal{A}_{n}\). We observe that their relative performance is instance dependent: in tasks that require more difficult expansion, ITL with \(\mathcal{A}_{n}\) converges faster, whereas in simpler tasks (such as the 2d experiment), ITL with \(\mathcal{P}_{\!A_{n}}\) converges faster. We compare against the GoOSE algorithm Turchetta et al. (2019) which is a heuristic extension of SafeOpt that explores more greedily in directions of (assumed) high reward (cf. Appendix K.2.3). GoOSE suffers from the same limitations as SafeOpt, which were highlighted in Section 5, and additionally is limited by its heuristic approach to expansion which fails in the 1d task and safe controller tuning task. Analogously to our experiments with SafeOpt, we also compare against Oracle GoOSE which has oracle knowledge of the true Lipschitz constants.

The different behaviors of ITL with \(\mathcal{A}_{n}\) and \(\mathcal{P}_{\!A_{n}}\), respectively, as well as SafeOpt and GoOSE are illustrated in Figure 19. We observe that ITL with \(\mathcal{A}_{n}\) and SafeOpt expand the safe set more "uniformly" since the set of potential maximizers encircles the true safe set.21 Intuitively, this is because the set of potential maximizers _conservatively_ captures mi

Figure 18: We perform the tasks of Figure 5 using Thompson sampling to evaluate the stochastic target space \(\mathcal{P}_{\!A_{n}}\). We additionally compare to GoOSE (cf. Appendix K.2.3) and ISE-BO (cf. Appendix K.2.4).

optimal. In contrast, ITL with \(\mathcal{P}_{\mathcal{A}_{n}}\) and GoOSE focus exploration and expansion in those regions where the objective is likely to be high.

### Detailed Comparison with Prior Works

The most widely used method for Safe BO is SafeOpt (Sui et al., 2015; Berkenkamp et al., 2021) which keeps track of separate candidate sets for expansion and exploration and uses UnSa to pick one of the candidates in each round. Treating expansion and exploration separately, sampling is directed towards expansion in _all_ directions -- even those that are known to be suboptimal. The safe set is expanded based on a Lipschitz constant of \(g^{\star}\), which is assumed to be known. In most real-world settings, this constant is unknown and has to be estimated using the GP. This estimate is generally conservative and results in suboptimal performance. To this end, Berkenkamp et al. (2016) proposed Heuristic SafeOpt which relies solely on the confidence intervals of \(g\) to expand the safe set, but lacks convergence guarantees. More recently, Bottero et al. (2022) proposed ISE which queries parameters from \(\mathcal{S}_{n}\) that yield the most "information" about the safety of another parameter in \(\mathcal{X}\). Hence, ISE focuses solely on the expansion of the safe set \(\mathcal{S}_{n}\) and does not take into account the objective \(f\). In practice, this can lead to significantly worse performance on the simplest of problems (cf. Figure 5). In contrast, ITL balances expansion of and exploration within the safe set. Furthermore, ISE does not have known convergence guarantees of the kind of Theorem 5.1. In parallel independent work, Bottero et al. (2024) proposed a combination of ISE and max-value entropy search (Wang and Jegelka, 2017) for which they derive a similar guarantee to Theorem 5.1.22 Similar to SafeOpt, their method aims to expand the safe set in all directions including those that are known to be suboptimal. In contrast, ITL directs expansion only towards potentially optimal regions.

Footnote 22: We provide an empirical evaluation in Appendix K.2.4.

In the 1d task and quadcopter experiment (cf. Figure 5), we observe that SafeOpt and even Oracle SafeOpt converge significantly slower than ITL to the safe optima. We believe this is due to their conservative Lipschitz-continuity/global smoothness-based expansion, as opposed to ITL's expansion,

Figure 19: The first \(100\) samples of **(A)** ITL with \(\mathcal{A}_{n}\), **(B)** SafeOpt, **(C)** Oracle SafeOpt, **(D)** ITL with \(\mathcal{P}_{\mathcal{A}n}\), **(E)** GoOSE, **(F)** Oracle GoOSE. The white region denotes the pessimistic safe set \(\mathcal{S}_{100}\), the light gray region denotes the true safe set \(\mathcal{S}^{\star}\) (i.e., the “island”), and the darker gray regions denotes unsafe points (i.e., the “ocean”).

which adapts to the local smoothness of the constraints. Heuristic SafeOpt, which does not rely on the Lipschitz constant for expansion, does not efficiently expand the safe set due to its heuristic that only considers single-step expansion. This is especially the case for the 1d task. Furthermore, in the 2d task, we notice the suboptimality of ISE since it does not take into account the objective, and purely aims to expand the safe set. ITL, on the other hand, balances expansion and exploration.

#### k.2.1 SafeOpt

SafeOpt(Sui et al., 2015; Berkenkamp et al., 2021) is a well-known algorithm for Safe BO.

Lipschitz-based expansionSafeOpt expands the set of known-to-be safe points by assuming knowledge of an upper bound \(L_{i}\) to the Lipschitz constant of the unknown constraints \(g_{i}^{*}\).23 In each iteration, the (pessimistic) safe set \(\mathcal{S}_{n}\) is updated to include all points which can be reached safely (with respect to the Lipschitz continuity) from a known-to-be-safe point \(\bm{x}\in\mathcal{S}_{n}\). Formally,

Footnote 23: Recall that due to the assumption that \(\|g_{i}^{*}\|_{k_{s}}<\infty\), \(g_{i}^{*}\) is indeed Lipschitz continuous.

\[\mathcal{S}_{n}^{\textsc{SafeOpt}} \stackrel{{\mathrm{def}}}{{=}}\bigcup_{\bm{x}\in \mathcal{S}_{n}^{\textsc{SafeOpt}}}\{\bm{x}^{\prime}\in\mathcal{X}\mid\] (48) \[l_{n,i}(\bm{x})-L_{i}\|\bm{x}-\bm{x}^{\prime}\|_{2}\geq 0 \text{ for all }i\in\mathcal{I}_{s}\}.\]

The expansion of the safe set is illustrated in Figure 20.

We remark two main limitations of this approach. First, the Lipschitz constant is an additional safety critical hyperparameter of the algorithm, which is typically not known. The RKHS assumption (cf. Assumption C.25) induces an assumption on the Lipschitz continuity, however, the worst-case a-priori Lipschitz constant is typically very large, and prohibitive for expansion. Second, the Lipschitz constant is global property of the unknown function, meaning that it does not adapt to the local smoothness. For example, a constraint may be "flat" in one direction (permitting straightforward expansion) and "steep" in another direction (requiring slow expansion). Furthermore, the Lipschitz constant is constant over time, whereas ITL is able to adapt to the local smoothness and reduce the (induced) Lipschitz constant over time.

Undirected expansionSafeOpt addresses the trade-off between expansion and exploration by focusing learning on two different sets. First, the set of _maximizers_

\[\mathcal{M}_{n}^{\textsc{SafeOpt}}\stackrel{{ \mathrm{def}}}{{=}}\{\bm{x}\in\mathcal{S}_{n}^{\textsc{SafeOpt}} \mid\] \[u_{n,f}(\bm{x})\geq\max_{\bm{x}^{\prime}\in\mathcal{S}_{n}^{ \textsc{SafeOpt}}}l_{n,f}(\bm{x})\}\]

which contains all _known-to-be-safe_ points which are potentially optimal. Note that if \(\mathcal{S}_{n}^{\textsc{SafeOpt}}=\mathcal{S}_{n}\) then \(\mathcal{M}_{n}^{\textsc{SafeOpt}}\subseteq\mathcal{A}_{n}\) since \(\mathcal{A}_{n}\) contains points which are potentially optimal and potentially safe _but possibly unsafe_.

To facilitate expansion, for each point \(\bm{x}\in\mathcal{S}_{n}\), the algorithm considers a set of _expanding points_

\[\mathcal{F}_{n}^{\textsc{SafeOpt}}(\bm{x})\stackrel{{ \mathrm{def}}}{{=}}\{\bm{x}^{\prime}\in\mathcal{X}\setminus\mathcal{S}_{n}^{ \textsc{SafeOpt}}\mid\] \[u_{n,i}(\bm{x})-L_{i}\|\bm{x}-\bm{x}^{\prime}\|_{2}\geq 0\text{ for all }i\in \mathcal{I}_{s}\}\]

A point is expanding if it is unsafe initially and can be (optimistically) deduced as safe by observing \(\bm{x}\). The set of _expanders_ corresponds to all known-to-be-safe points which optimistically lead to expansion of the safe set:

\[\mathcal{G}_{n}^{\textsc{SafeOpt}}\stackrel{{\mathrm{def}}}{{=}} \{\bm{x}\in\mathcal{S}_{n}\mid|\mathcal{F}_{n}(\bm{x})|>0\}.\]

That is, an expander is a safe point \(\bm{x}\) which is "close" to at least one expanding point \(\bm{x}^{\prime}\). Observe that here, we start with a safe \(\bm{x}\) and then find a close and potentially safe \(\bm{x}^{\prime}\) using the Lipschitz-property of the constraint function. Thus, the set of expanding points is inherently limited by the assumed Lipschitzness (cf. Figure 20), and generally a subset of the potential expanders \(\mathcal{E}_{n}\) (cf. Equation (27)):

**Lemma K.1**.: _For any \(n\geq 0\), if \(\mathcal{S}_{n}^{\textsc{SafeOpt}}=\mathcal{S}_{n}\) then_

\[\bigcup_{\bm{x}\in\mathcal{S}_{n}}\mathcal{F}_{n}^{\textsc{SafeOpt}}(\bm{x}) \subseteq\mathcal{E}_{n}.\]Proof.: Without loss of generality, we consider the case where \(\mathcal{I}_{s}=\{i\}\). We have

\[\mathcal{E}_{n}=\widehat{\mathcal{S}}_{n}\setminus\mathcal{S}_{n}=\{\bm{x}\in \mathcal{X}\setminus\mathcal{S}_{n}\mid u_{n,i}(\bm{x})\geq 0\}.\]

The result follows directly by observing that \(L_{i}\|\bm{x}-\bm{x}^{\prime}\|_{2}\geq 0\). 

SafeOpt then selects \(\bm{x}_{n+1}\) according to uncertainty sampling _within_ the maximizers and expanders: \(\mathcal{M}_{n}^{\text{SafeOpt}}\;\cup\;\mathcal{G}_{n}^{\text{SafeOpt}}\). We remark that due to the separate handling of expansion and exploration, SafeOpt expands the safe set in _all_ directions -- even those that are known to be suboptimal. In contrast, ITL only expands the safe set in directions that are potentially optimal by balancing expansion and exploration through the single set of potential maximizers \(\mathcal{A}_{n}\).

Based on uncertainty samplingAs mentioned in the previous paragraph, SafeOpt selects as next point the maximizer/expander with the largest prior uncertainty.24 In contrast, ITL selects the point within \(\mathcal{S}_{n}\) which minimizes the posterior uncertainty within \(\mathcal{A}_{n}\). Note that the two approaches are not identical as typically \(\mathcal{M}_{n}^{\text{SafeOpt}}\;\cup\;\mathcal{G}_{n}^{\text{SafeOpt}}\subset \mathcal{S}_{n}^{\text{SafeOpt}}\) and \(\mathcal{A}_{n}\not\supseteq\mathcal{S}_{n}\).

Footnote 24: The use of uncertainty sampling for safe sequential decision-making goes back to Schreiter et al. (2015) and Sui et al. (2015).

We show empirically in Section 3.3 that depending on the kernel choice (i.e., the smoothness assumptions), uncertainty sampling within a given target space neglects higher-order information that can be attained by sampling outside the set. This can be seen even more clearly when considering linear functions, in which case points outside the maximizers and expanders can be equally informative as points inside.

Finally, note that the set of expanders is constructed "greedily", i.e., only considering _single-step_ expansion. This is necessitated as the inference of safety is based on single reference points. Instead, ITL directly quantifies the information gained towards the points of interest without considering intermediate reference points.

Requires homoscedastic noiseSafeOpt imposes a homoscedasticity assumption on the noise which is an artifact of the analysis of uncertainty sampling. It is well known that in the presence of heteroscedastic noise, one has to distinguish epistemic and aleatoric uncertainty. Uncertainty sampling fails because it may continuously sample a high variance point where the variance is dominated by aleatoric uncertainty, potentially missing out on reducing epistemic uncertainty at points with small aleatoric uncertainty. In contrast, maximizing mutual information naturally takes

Figure 20: Illustration of the expansion of the safe set à la SafeOpt. Here, the blue region denotes the pessimistic safe set \(\mathcal{S}\), the red region denotes the true safe set \(\mathcal{S}^{\star}\), and the orange region denotes the optimistic safe set \(\widehat{\mathcal{S}}\). Whereas ITL learns about the point \(\bm{x}^{\prime}\)_directly_, SafeOpt expands the safe set using the reduction of uncertainty at \(\bm{x}\), and then extrapolating using the Lipschitz constant (cf. Equation (48)). The dashed orange line denotes the expanding points of SafeOpt which under-approximate the optimistic safe set of ITL (cf. Lemma 1). Thus, ITL may even learn about points in \(\widehat{\mathcal{S}}\) which are “out of reach” for SafeOpt.

into account the two sources of uncertainty, preferring those points where epistemic uncertainty is large and aleatoric uncertainty is small (cf. Appendix C.1).

Suboptimal reachable safe setSui et al. (2015) and Berkenkamp et al. (2021) show that SafeOpt converges to the optimum within the closure of

\[\mathcal{R}^{\textsc{SafeOpt}}_{\epsilon}(\mathcal{S})\stackrel{{ \mathrm{def}}}{{=}}\mathcal{S}\cup\{\bm{x}\in\mathcal{X}\mid\exists\bm{x}^{ \prime}\in\mathcal{S}\text{ such that }\] \[f^{*}_{i}(\bm{x}^{\prime})-(L_{i}\|\bm{x}-\bm{x}^{\prime}\|_{2}+ \epsilon)\geq 0\text{ for all }i\in\mathcal{I}_{s}\}.\]

Note that analogously to the expansion of the safe set, the "expansion" of the reachable safe set is based on "inferring safety" through a reference point in \(\mathcal{S}\) and using Lipschitz continuity. This is opposed to the reachable safe set of ITL (cf. Definition C.29).

We remark that under the additional assumption that a Lipschitz constant is known, ITL can easily be extended to expand its safe set based on the kernel _and_ the Lipschitz constant, resulting in a strictly larger reachable safe set than SafeOpt. We leave the concrete formalization of this extension to future work. Moreover, we do not evaluate this extension in our experiments, as we observe that even without the additional assumption of a Lipschitz constant, ITL outperforms SafeOpt in practice.

#### k.2.2 Heuristic SafeOpt

Berkenkamp et al. (2016) also implement a heuristic variant of SafeOpt which does not assume a known Lipschitz constant. This heuristic variant uses the same (pessimistic) safe sets \(\mathcal{S}_{n}\) as ITL. The set of maximizers is identical to SafeOpt. As expanders, the heuristic variant considers all safe points \(\bm{x}\in\mathcal{S}_{n}\) that if \(\bm{x}\) were to be observed next with value \(\bm{u}_{n}(\bm{x})\) lead to \(|\mathcal{S}_{n+1}|>|\mathcal{S}_{n}|\). We refer to this set as \(\mathcal{G}_{n}^{\textsc{H-SafeOpt}}\). The next point is then selected by uncertainty sampling within \(\mathcal{M}_{n}^{\textsc{SafeOpt}}\cup\mathcal{G}_{n}^{\textsc{H-SafeOpt}}\).

The heuristic variant shares some properties with SafeOpt, such that it is based on uncertainty sampling, not adapting to heteroscedastic noise, and separate notions of maximizers and expanders (leading to an "undirected" expansion of the safe set). Note that there are no known convergence guarantees for heuristic SafeOpt. Importantly, note that similar to SafeOpt the set of expanders is constructed "greedily", and in particular, does only take into account _single-step_ expansion. In contrast, an objective such as ITL which quantifies the "information gained towards expansion" also actively seeks out _multi-step_ expansion.

#### k.2.3 Goose

To address the "undirected" expansion of SafeOpt discussed in the previous section, Turchetta et al. (2019) proposed _goal-oriented safe exploration_ (GoOSE). GoOSE extends any unsafe BO algorithm (which we subsequently call an oracle) to the safe setting. In our experiments, we evaluate GoOSE-UCB which uses UCB as oracle and which is also the variant studied by Turchetta et al. (2019). In the following, we assume for ease of notation that \(\mathcal{I}_{s}=\{c\}\).

Given the oracle proposal \(\bm{x}^{\star}\), GoOSE first determines whether \(\bm{x}^{\star}\) is safe. If \(\bm{x}^{\star}\) is safe, \(\bm{x}^{\star}\) is queried next. Otherwise, GoOSE first learns about the safety of \(\bm{x}^{\star}\) by querying "expansionist" points until the oracle's proposal is determined to be either safe or unsafe.

GoOSE expands the safe set identically to SafeOpt according to Equation (48). In the context of GoOSE, \(\mathcal{S}_{n}^{\textsc{SafeOpt}}\) is called the _pessimistic safe set_. To determine that a point cannot be deduced as safe, GoOSE also keeps track of a Lipschitz-based _optimistic safe set_:

\[\widehat{\mathcal{S}}_{n,\epsilon}^{\textsc{GoOSE}} \stackrel{{\mathrm{def}}}{{=}} \bigcup_{\bm{x}\in\mathcal{S}_{n-1}^{\textsc{SafeOpt}}}\{\bm{x}^{ \prime}\in\mathcal{X}\mid\] \[u_{n,c}(\bm{x})-L_{c}\|\bm{x}-\bm{x}^{\prime}\|_{2}-\epsilon\geq 0\}.\]

We summarize the algorithm in Algorithm 2 where we denote by \(\mathcal{O}(\mathcal{X})\) the oracle proposal over the domain \(\mathcal{X}\).

It remains to discuss the heuristic used to select the "expansionist" points. GoOSE considers all points \(\bm{x}\in\mathcal{S}_{n}^{\textsc{SafeOpt}}\) with confidence bands of size larger than the accuracy \(\epsilon\), i.e.,

\[\mathcal{W}_{n,\epsilon}^{\textsc{GoOSE}}\stackrel{{\mathrm{ def}}}{{=}}\{\bm{x}\in\mathcal{S}_{n}^{\textsc{SafeOpt}}\mid u_{n,c}(\bm{x})-l_{n,c}(\bm{x})>\epsilon\}.\]Which of the points in this set is evaluated depends on a set of learning targets \(\mathcal{A}_{n,\epsilon}^{\text{GoOSE}}\stackrel{{\text{def}}}{{=}} \widehat{\mathcal{S}}_{n,\epsilon}^{\text{GoOSE}}\setminus\mathcal{S}_{n}^{ \text{AgFEOpt}}\) akin to the "potential expanders" \(\mathcal{E}_{n}\) (cf. Equation (27)), to each of which we assign a priority \(h(\bm{x})\). When \(h(\bm{x})\) is large, this indicates that the algorithm is prioritizing to determine whether \(\bm{x}\) is safe. We use as heuristic the negative \(\ell_{1}\)-distance between \(\bm{x}\) and \(\bm{x}^{\star}\). GoOSE then considers the set of _potential immediate expanders_

\[\mathcal{G}_{n,\epsilon}^{\text{GoOSE}}(\alpha)\stackrel{{ \text{def}}}{{=}}\{\bm{x}\in\mathcal{W}_{n,\epsilon}^{\text{GoOSE}} \mid\exists\bm{x}^{\prime}\in\mathcal{A}_{n,\epsilon}^{\text{GoOSE}}\text{ with }\] \[\text{priority }\alpha\text{ such that }u_{n,c}(\bm{x})-L_{c}\|\bm{x}-\bm{x}^{ \prime}\|_{2}\geq 0\}.\]

The "expansionist" point selected by GoOSE is then any point in \(\mathcal{G}_{n,\epsilon}^{\text{GoOSE}}(\alpha^{\star})\) where \(\alpha^{\star}\) denotes the largest priority such that \(|\mathcal{G}_{n,\epsilon}^{\text{GoOSE}}(\alpha^{\star})|>0\).

We observe empirically that the sample complexity of GoOSE is not always better than that of SafeOpt. Notably, the expansion of the safe set is based on a "greedy" heuristic. Moreover, determining whether a single oracle proposal \(\bm{x}^{\star}\) is safe may take significant time. Consider the (realistic) example where the prior is uniform, and UCB proposes a point which is far away from the safe set and suboptimal. GoOSE will typically attempt to derive the safety of the proposed point until the uncertainty at _all_ points within \(\mathcal{S}_{0}^{\text{AgFEOpt}}\) is reduced to \(\epsilon\).25 Thus, GoOSE can "waste" a significant number of samples, aiming to expand the safe set towards a known-to-be suboptimal point. In larger state spaces, due to the greedy nature of the expansion strategy, this can lead to GoOSE being effectively stuck at a suboptimal point for a significant number of rounds.

Footnote 25: This is because the proposed point typically remains in the optimistic safe set when it is sufficiently far away from the pessimistic safe set.

#### k.2.4 ISE and ISE-BO

Recently, Bottero et al. (2022) proposed an information-theoretic approach to efficiently expand the safe set which they call _information-theoretic safe exploration_ (ISE). Specifically, they choose the next action \(\bm{x}_{n}\) by approximating

\[\operatorname*{arg\,max}_{\bm{x}\in\mathcal{S}_{n-1}}\underbrace{\max}_{\bm{x }^{\prime}\in\mathcal{X}}\operatorname{I}(\mathbbm{1}\{g_{\bm{x}^{\prime}} \geq 0\};y_{\bm{x}}\mid\mathcal{D}_{n-1})\,.\] (ISE)

In a parallel independent work, Bottero et al. (2024) extended ISE to the Safe BO problem where they propose to choose \(\bm{x}_{n}\) according to

\[\operatorname*{arg\,max}_{\bm{x}\in\mathcal{S}_{n-1}}\max\{\alpha^{\text{ISE} }(\bm{x}),\alpha^{\text{MES}}(\bm{x})\}\] (ISE-BO)

where \(\alpha^{\text{MES}}\) denotes the acquisition function of max-value entropy search (Wang and Jegelka, 2017). Similarly to SafeOpt, ISE-BO treats expansion and exploration separately, which leads to "undirected" expansion of the safe set. That is, the safe set is expanded in all directions, even those that are known to be suboptimal. In contrast, ITL balances expansion and exploration through the single set of potential maximizers \(\mathcal{A}_{n}\). With a stochastic target space, ITL generalizes max-value entropy search (cf. Appendix K.1).

We evaluate ISE-BO in Figure 18 and observe that it does not outperform ITL and VTL in any of the tasks, while performing poorly in the 1d task and suboptimally in the 2d task.

### Jumping Past Local Barriers

In this additional experiment we demonstrate that ITL is able to extrapolate safety beyond local unsafe "barriers", which is a fundamental limitation of Lipschitz-based methods such as SafeOpt. We consider the ground truth function and prior statistical model shown in Figure 21. Note that initially, there are three disjoint safe "regions" known to the algorithm corresponding to two of the three safe "bumps" of the ground truth function. In this experiment, the main challenge is to "jump past" the local barrier separating the leftmost and initially unknown safe "bump".

Figure 22 shows the sampled points during the first \(100\) iterations of SafeOpt and ITL. Clearly, SafeOpt does not discover the third safe "bump" while ITL does. Indeed, it is a fundamental limitation of Lipschitz-based methods that they can never "jump past local barriers", even if the oracle Lipschitz constant were to be known and tight (i.e., locally accurate) around the barrier. This is because Lipschitz-based methods expand to the point \(\bm{x}\) based on a reference point \(\bm{x^{\prime}}\), and by definition, if \(\bm{x}\) is added to the safe set so are all points on the line segment between \(\bm{x}\) and \(\bm{x^{\prime}}\). Hence, if there is a single point on this line segment which is unsafe (i.e., a "barrier"), the algorithm will _never_ expand past it. This limitation does not exist for kernel-based algorithms as expansion occurs in function space.

Moreover, note that for a non-stationary kernel such as in this example, ITL samples the "closest points" in function space rather than Euclidean space. We observe that SafeOpt still samples "locally at the boundary" whereas ITL samples the most informative point which in this case is

Figure 21: The ground truth \(f^{\star}\) is shown as the dashed black line. The solid black line denotes the constraint boundary. The GP prior is given by a linear kernel with \(\sin\)-transform and mean \(0.1x\). The light gray region denotes the initial optimistic safe set \(\widehat{\mathcal{S}}_{0}\) and the dark gray region denotes the initial pessimistic safe set \(\mathcal{S}_{0}\).

Figure 22: First \(100\) samples of ITL using the potential expanders \(\mathcal{E}_{n}\) (cf. Equation (27)) as target space (left) and SafeOpt sampling only from the set of expanders \(\mathcal{G}_{n}^{\text{SafeOpt}}\) (right).

the local maximum of the sinusoidal function. In other words, ITL adapts to the geometry of the function. This generally leads us to believe that ITL is more capable to exploit (non-stationary) prior knowledge than distance-based methods such as SafeOpt.

### Experiment Details

#### k.4.1 Synthetic Experiments

1d taskFigure 23 shows the objective and constraint function, as well as the prior. We discretize using \(500\) points. The main difficulty in this experiment lies in sufficiently expanding the safe set to discover the global maximum. Figure 24 plots the size of the safe set \(\mathcal{S}_{n}\) for the compared algorithms, which in this experiment matches the achieved regret closely.

2d taskWe model our constraint in the form of a spherical "island" where the goal is to get a good view of the coral reef located to the north-east of the island while staying in the interior of the island during exploration (cf. Figure 25). The precise objective and constraint functions are unknown to the agent. Hence, the agent has to gradually and safely update its belief about boundaries of the "island" and the location of the coral reef. The prior is obtained by a single observation within the center of the island \([-0.5,0.5]^{2}\). We discretize using \(2\,500\) points.

Figure 23: Ground truth and prior well-calibrated model in 1d synthetic experiment. The function serves simultaneously as objective and as constraint. The light gray region denotes the initial safe set \(\mathcal{S}_{0}\).

Figure 24: Size of \(\mathcal{S}_{n}\) in 1d synthetic experiment. The dashed black line denotes the size of \(\mathcal{S}^{\star}\). In this task, “discovering” the optimum is closely linked to expansion of the safe set, and Heuristic SafeOpt fails since it does not expand the safe set sufficiently.

#### k.4.2 Safe Controller Tuning for Quadcopter

Modeling the real-world dynamicsWe learn a feedback policy (i.e., "control gains") to compensate for inaccuracies in the initial controller. In our experiment, we model the real world dynamics and the adjusted model using the PD control feedback (Widmer et al., 2023),

\[\boldsymbol{\delta}_{t}(\boldsymbol{x})\mathop{=}^{\mathrm{def}}(\boldsymbol{x }^{\star}-\boldsymbol{x})[(\boldsymbol{s}^{\star}-\boldsymbol{s}_{t})\;( \dot{\boldsymbol{s}}^{\star}-\dot{\boldsymbol{s}}_{t})],\] (49)

where \(\boldsymbol{x}^{\star}\) are the _unknown_ ground truth disturbance parameters, and \(\boldsymbol{s}^{\star}\) and \(\dot{\boldsymbol{s}}^{\star}\) are the desired state and state derivative, respectively. This yields the following ground truth dynamics:

\[\boldsymbol{s}_{t+1}(\boldsymbol{x})=\boldsymbol{T}(\boldsymbol{s}_{t}, \boldsymbol{u}_{t}+\boldsymbol{\delta}_{t}(\boldsymbol{x})).\] (50)

The feedback parameters \(\boldsymbol{x}=[\boldsymbol{x}_{p}\;\boldsymbol{x}_{d}]^{\top}\) can be split into \(\boldsymbol{x}_{p}\) tuning the state difference which are called _proportional parameters_ and \(\boldsymbol{x}_{d}\) tuning the state derivative difference which are called _derivative parameters_. We use the "critical damping" heuristic to relate the proportional and derivative parameters: \(\boldsymbol{x}_{d}=2\sqrt{\boldsymbol{x}_{p}}\). We thus consider the restricted domain \(\mathcal{X}=[0,20]^{4}\) where each dimension corresponds to the proportional feedback to one of the four rotors.

Ground truth disturbance parameters are sampled from a chi-squared distribution with one degree of freedom (i.e., the square of a standard normal distribution), \(\boldsymbol{x}_{p}^{\star}\sim\chi_{1}^{2}\), and \(\boldsymbol{x}_{d}^{\star}\) is determined according to the critical damping heuristic.

The learning problemThe goal of our learning problem is to move the quadcopter from its initial position \(\boldsymbol{s}(0)=[1\;1\;1]^{\top}\) (in Euclidean space with meter as unit) to position \(\boldsymbol{s}^{\star}=[0\;0\;2]^{\top}\). Moreover, we aim to stabilize the quadcopter at the goal position, and therefore regularize the control signal towards an action \(\boldsymbol{u}^{\star}\) which results in hovering (approximately) without any disturbances. We formalize these goals with the following objective function:

\[f^{\star}(\boldsymbol{x})\mathop{=}^{\mathrm{def}}-\sigma\!\left(\sum_{t=0}^{ T}\|\boldsymbol{s}^{\star}-\boldsymbol{s}_{t}(\boldsymbol{x})\|_{\boldsymbol{Q}}^{2 }+\|\boldsymbol{u}^{\star}-\boldsymbol{u}_{t}(\boldsymbol{x})\|_{\boldsymbol{ R}}^{2}\right)\] (51)

where \(\sigma(v)\mathop{=}^{\mathrm{def}}\tanh((v-100)/100)\) is used to smoothen the objective function and ensure that its range is \([-1,1]\). The non-smoothed control objective in Equation (51) is known as a _linear-quadratic regulator_ (LQR) which we solve exactly for the undisturbed system using ILQR (Tu et al., 2023). Finally, we want to ensure at all times that the quadcopter is at least \(0.5\) meter above the ground, that is,

\[g^{\star}(\boldsymbol{x})\mathop{=}^{\mathrm{def}}\min_{t\in[T]}\boldsymbol{ s}_{t}^{z}(\boldsymbol{x})-0.5\] (52)

where we denote by \(\boldsymbol{s}_{t}^{z}\) the z-coordinate of state \(\boldsymbol{s}_{t}\).

We use a time horizon of \(T=3\) seconds which we discretize using \(100\) steps. The objective is modeled by a zero-mean GP with a \(\text{Mat\'{e}rn}(\nu=5/2)\) kernel with lengthscale \(0.1\), and the constraint is modeled by a GP with mean \(-0.5\) and a \(\text{Mat\'{e}rn}(\nu=5/2)\) kernel with lengthscale \(0.1\). The prior is obtained by a single observation of the "safe seed" \([0\;0\;0\;10]^{\top}\).

Figure 25: Ground truth in 2d synthetic experiment.

Adaptive discretizationWe discretize the domain \(\mathcal{X}\) adaptively using coordinate LineBO(Kirschner et al., 2019). That is, in each iteration, one of the four control dimensions is selected uniformly at random, and the active learning oracle is executed on the corresponding one-dimensional subspace.

SafetyUsing the (unsafe) constrained BO algorithm EIC (Gardner et al., 2014) leads constraint violation,26 while ITL and VTL do not violate the constraints during learning for any of the random seeds.

HyperparametersThe observation noise is Gaussian with standard deviation \(\rho=0.1\). We let \(\beta=10\). The control target is \(\bm{u}^{\star}=[1.766\ 0\ 0\ 0]^{\top}\).

Footnote 26: On average, \(1.6\) iterations of the first \(50\) violate the constraints.

The state space is \(12\)-dimensional where the first three states correspond to the velocity of the quadcopter, the next three states correspond to its acceleration, the following three states correspond to its angular velocity, and the last three states correspond to its angular velocity in local frame. The LQR parameters are given by

\[\bm{Q} =\operatorname{diag}\left\{1,1,1,1,1,1,0.1,0.1,0.1,0.1,0.1,0.1 \right\}\quad\text{and}\] \[\bm{R} =0.01\cdot\operatorname{diag}\left\{5,0.8,0.8,0.3\right\}.\]

The quadcopter simulation was adapted from Chandra (2023).

Each one-dimensional subspace is discretized using \(2\,000\) points.

Random seedsWe repeat the experiment for \(25\) different seeds where the randomness is over the ground truth disturbance, observation noise, and the randomness in the algorithm.

\begin{table}
\begin{tabular}{l l l} \hline \hline Kernel & \(k(\bm{x},\bm{x^{\prime}})\) & \(\gamma_{n}\) \\ \hline Linear & \(\bm{x^{\top}x^{\prime}}\) & \(O(d\log(n))\) \\ Gaussian & \(\exp\!\left(-\frac{\left\|\bm{x}-\bm{x^{\prime}}\right\|_{2}^{2}}{2h^{2}}\right)\) & \(\widetilde{O}\!\left(\log^{d+1}(n)\right)\) \\ Laplace & \(\exp\!\left(-\frac{\left\|\bm{x}-\bm{x^{\prime}}\right\|_{1}}{h}\right)\) & \(\widetilde{O}\!\left(n^{\frac{d}{1+d}}\log^{\frac{1}{1+d}}(n)\right)\) \\ Matérn & \(\frac{2^{1-\nu}}{\Gamma(\nu)}\!\left(\frac{\sqrt{2\nu}\left\|\bm{x}-\bm{x^{ \prime}}\right\|_{2}}{h}\right)^{\nu}B_{\nu}\!\left(\frac{\sqrt{2\nu}\left\| \bm{x}-\bm{x^{\prime}}\right\|_{2}}{h}\right)\) & \(\widetilde{O}\!\left(n^{\frac{d}{2\nu+d}}\log^{\frac{2\nu}{2\nu+d}}(n)\right)\) \\ \hline any & & \(O(\left|\mathcal{X}\right|\log(n))\) \\ \hline \hline \end{tabular}
\end{table}
Table 3: Magnitudes of \(\gamma_{n}\) for common kernels. The magnitudes hold under the assumption that \(\mathcal{X}\) is compact. Here, \(B_{\nu}\) is the modified Bessel function. We take the magnitudes from Theorem 5 of Srinivas et al. (2009) and Remark 2 of Vakili et al. (2021). The notation \(\widetilde{O}(\cdot)\) subsumes log-factors. For \(\nu=1/2\), the Matérn kernel is equivalent to the Laplace kernel. For \(\nu\to\infty\), the Matérn kernel is equivalent to the Gaussian kernel. The functions sampled from a Matérn kernel are \(\lceil\nu\rceil-1\) mean square differentiable. The kernel-agnostic bound follows by simple reduction to a linear kernel in \(\left|\mathcal{X}\right|\) dimensions.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes]Justification:

Guidelines:

* The answer NA means that the paper does not include theoretical results.
* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: The code is publicly available. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?Answer: [Yes] Justification: Guidelines:

* The answer NA means that paper does not include experiments requiring code.
* Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
* The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: See the extensive discussions in the appendices. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors). * It should be clear whether the error bar is the standard deviation or the standard error of the mean.

* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: All individual experiments require only small compute resources. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.

* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licensees for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.

* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.