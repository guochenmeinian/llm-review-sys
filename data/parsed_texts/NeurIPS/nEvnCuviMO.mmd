# Metric Distortion Under Probabilistic Voting

Anonymous Author(s)

###### Abstract

Metric distortion in social choice provides a framework for assessing how well voting rules minimize social cost in scenarios where voters and candidates exist in a shared metric space, with voters submitting rankings and the rule outputting a single winner. We expand this framework to include probabilistic voting. Our extension encompasses a broad range of probability functions, including widely studied models like Plackett-Luce (PL) and Bradley-Terry, and a novel "pairwise quantal voting" model inspired by quantal response theory. We demonstrate that distortion results under probabilistic voting better correspond with conventional intuitions regarding popular voting rules such as Plurality, Copeland, and Random Dictator (RD) than those under deterministic voting. For example, in the PL model with candidate strength inversely proportional to the square of their metric distance, we show that Copeland's distortion is at most 2, whereas that of RD is \(\Omega(\sqrt{m})\) in large elections, where \(m\) is the number of candidates. This contrasts sharply with the classical model, where RD beats Copeland with a distortion of 3 versus 5 [1].

## 1 Introduction

Societies must make decisions collectively; different agents often have conflicting interests, and the choice of the mechanism used for combining everyone's opinions often makes a big difference to the outcome. The machine learning community has applied social choice principles for AI alignment [2; 3], algorithmic fairness [4; 5], and preference modelling [6; 7]. Over the last century, there has been increasing interest in using computational tools to analyse and design voting rules [8; 9; 10; 11]. One prominent framework for evaluating voting rules is that of _distortion_[12], where the voting rule has access to only the _ordinal_ preferences of the voters. However, the figure of merit is the sum of all voters' _cardinal_ utilities (or costs). The distortion of a voting rule is the worst-case ratio of the cost of the alternative it selects and the cost of the truly optimal alternative.

An additional assumption is imposed in _metric distortion_[1] - that the voters and candidates all lie in a shared (unknown) metric space, and costs are given by distances (thus satisfying non-negativity and triangular inequality). This model is a generalization of a commonly studied _spatial model of voting_ in the Economics literature [13; 14], and has a natural interpretation of voters liking candidates with a similar ideological position across many dimensions. While metric distortion is a powerful framework and has led to the discovery and re-discovery of interesting voting rules (e.g. Plurality Veto [15] and the study of Maximal Lotteries [16] for metric distortion by Charikar et al. [17]), its outcomes sometimes do not correspond with traditional wisdom around popular voting rules. For example, the overly simple _Random Dictator (RD)_ rule (where the winner is the top choice of a uniform randomly selected voter) beats the _Copeland_ rule (which satisfies the Condorcet Criterion [10] and other desirable properties) with a metric distortion of 3 versus 5 [1].

While not yet adopted in the metric distortion framework, there is a mature line of work on _Probabilistic voting_ (PV) [18; 19; 20]. Here, the focus is on the behavioural modelling of voters and accounting for the randomness of their votes. Two sources of this randomness often cited in the literature are the boundedness of the voters' rationality and the noise in their estimates of candidates' positions. A popular model for this behaviour is based on the _Quantal Response Theory_[20]. Another closely related line of work is on Random Utility Models (RUMs) [21; 22; 23] in social choice wherethe hypothesis is that the candidates have ground-truth strengths. Voters make noisy observations of these strengths and vote accordingly. We adopt these models of voting behaviour and study it within the metric distortion framework. The questions we ask are:

_Given a model of probabilistic voting, what is the metric distortion of popular voting rules? How does this differ (qualitatively and quantitatively) from the deterministic model?_

### Preliminaries and Notation

Let \(\mathcal{N}\) be a set of \(n\) voters and \(\mathcal{A}\) be the set of \(m\) candidates. Let \(\mathcal{S}\) be the set of total orders on \(\mathcal{A}\). Each voter \(i\in\mathcal{N}\) has a preference ranking \(\sigma_{i}\in\mathcal{S}\). A vote profile is a set of preference rankings \(\sigma_{\mathcal{N}}=(\sigma_{1},...,\sigma_{n})\in\mathcal{S}^{n}\) for all voters. The tuple \((\mathcal{N},\mathcal{A},\sigma_{\mathcal{N}})\) defines an instance of an election. Let \(\Delta(\mathcal{A})\) denote the set of all probability distributions over the set of candidates.

**Definition 1** (Voting Rule).: _A voting rule \(f:S^{n}\rightarrow\Delta(\mathcal{A})\) takes a vote profile \(\sigma_{N}\) and outputs a probability distribution \(p\) over the alternatives._

For deterministic voting rules, we overload notation by saying that the rule's output is a candidate and not a distribution. We now define some voting rules [10]. Let \(\mathbb{I}\) denote the indicator function.

**Random Dictator Rule:** Select a voter uniformly at random and output their top choice, i.e., \(\mathrm{RD}(\sigma_{\mathcal{N}})=p\) such that \(p_{j}=\frac{1}{n}\sum_{i\in\mathcal{N}}\mathbb{I}(\sigma_{i,1}=j)\).

**Plurality Rule:** Choose the candidate who is the top choice of the most voters, i.e., \(\mathrm{\textsc{plu}}(\sigma_{\mathcal{N}})=\mathrm{arg}\max_{j\in\mathcal{A} }\sum_{i\in\mathcal{N}}\mathbb{I}(\sigma_{i,1}=j)\). Ties are broken arbitrarily.

**Copeland Rule:** Choose the candidate who wins the most pairwise comparisons, i.e., \(\mathrm{COP}(\sigma_{\mathcal{N}})=\mathrm{arg}\max_{j\in\mathcal{A}}\sum_{j ^{\prime}\in\mathcal{A}\setminus\{j\}}\mathbb{I}\left(\sum_{i\in\mathcal{N}} \mathbb{I}(j\succ_{\sigma_{i}}j^{\prime})>\frac{n}{2}\right).\) Ties are broken arbitrarily.

Distance function \(d:(\mathcal{N}\cup\mathcal{A})^{2}\rightarrow\mathbb{R}_{\geq 0}\) satisfies triangular inequality (\(d(x,y)\leq d(x,z)+d(z,y)\)) and symmetry (\(d(x,y)=d(y,x)\)). The distance between voter \(i\in\mathcal{N}\) and candidate \(j\in\mathcal{A}\) is also referred to as the _cost_ of \(j\) for \(i\). We consider the most commonly studied social cost function, which is the sum of the costs of all voters. \(SC(j,d):=\sum_{i\in\mathcal{N}}d(i,j)\).

In deterministic voting, the preference ranking \(\sigma_{i}\) of voter \(i\) is consistent with the distances. That is, \(d(i,j)>d(i,j^{\prime})\implies j^{\prime}\succ_{\sigma_{i}}\), \(j\) for all voters \(i\in\mathcal{N}\) and candidates \(j,j^{\prime}\in\mathcal{A}\). Let \(\rho(\sigma_{\mathcal{N}})\) be the set of distance functions \(d\) consistent with vote profile \(\sigma_{\mathcal{N}}\). The metric distortion of a voting rule is:

**Definition 2** (Metric Distortion).: \(\textsc{dist}(f)=\sup_{\mathcal{N},\mathcal{A},\sigma_{\mathcal{N}}}\;\sup \limits_{d\in\rho(\sigma_{\mathcal{N}})}\frac{\mathbb{I}[SC(f(\sigma_{\mathcal{ N}}),d)]}{\min\limits_{j\in\mathcal{A}}SC(j,d)}.\)__

### Our Contributions

We extend the study of metric distortion to probabilistic voting (Definition 4). This extension is useful since voters, in practice, have been shown to vote randomly [20]. We define axiomatic properties of models of probabilistic voting which are suitable for studying metric distortion. These are scale-freeness with distances (Axiom 1), pairwise order probabilities being independent of other candidates (Axiom 2), and strict monotonicity of pairwise order probabilities in distances (Axiom 3).

All our results apply to a broad class of models of probabilistic models, as explained in SS 2. We provide distortion bounds for all \(n\geq 3\) and \(m\geq 2\), which are most salient in the limit \(n\rightarrow\infty\). For large elections (\(m\) fixed, \(n\rightarrow\infty\)), we provide matching upper and lower bounds on the distortion of Plurality, an upper bound for Copeland, and a lower bound for RD. The distortion of plurality grows linearly in \(m\). The distortion upper bound of Copeland is constant. The distortion lower bound for RD increases sublinearly in \(m\) where this rate depends on the probabilistic model. Crucially, our results match those in deterministic voting in the limit where the randomness goes to zero.

The technique is as follows. For the problem of maximizing the distortion, we establish a critical threshold of the expected fraction of votes on pairwise comparisons on all edges on a directed path from a winner to the "true optimal" candidate for Copeland and Plurality. This path is one or two hops for Copeland and one for Plurality. We then formulate a linear-fractional program which incorporates this critical threshold. We linearize this program via the sub-level sets technique [24], and find a feasible solution of the dual problem. Concentration inequalities on this solution provide an upper bound on the distortion. We find a matching lower bound for Plurality by construction.

### Related Work

Metric distortionAnshelevich et al. [1] initiated the study of metric distortion and showed that any deterministic voting rule has a distortion of at least 3 and that Copeland has a distortion of 5. The Plurality Veto Rule attains the optimal distortion of 3 [15]. Charikar and Ramakrishnan [25] showed that any randomized voting rule has a distortion of at least \(2.112\). Charikar et al. [17] gave a randomized voting rule with a distortion of at most 2.753. Anshelevich et al. [26] gave a useful survey on distortion in social choice.

**Distortion with Additional Information** Abramowitz et al. [27] showed that deterministic voting rules achieve a distortion of 2 when voters provide preference strengths as ratios of distances. Amanatidis et al. [28] demonstrated that even a few queries from each voter can significantly improve distortion in non-metric settings. Anshelevich et al. [29] examined threshold approval voting, where voters approve candidates with utilities above a threshold. Our work relates to these studies since in probabilistic voting, the likelihood of a voter switching the order of two candidates depends on the relative strength of their preference, often resulting in lower distortion than deterministic methods.

**Probabilisit voting and random utility models (RUMs)** Hinich [30] showed that the celebrated Median Voter Theorem of [31] does not hold under probabilistic voting. Classical work has focused on studying the equilibrium positions of voters and/or candidates in game-theoretic models of probabilistic voting [20; 32; 33; 34; 35]. McKelvey and Patty [20] adopt the quantal response model, a popular way to model agents' bounded rationality.

RUMs have mostly been studied in social choice [21; 23; 36] with the hypothesis that candidates have _universal_ ground-truth strengths, which voters make noisy observations of. Our model is the same as RUM regarding the voters' behaviour; however, voters have _independent_ costs from candidates. The Plackett-Luce (PL) model [37; 38] has been widely studied in social choice [39; 40; 41]. For probabilities on pairwise orders, PL reduces to the Bradley-Terry (BT) model [42]. These probabilities are proportional to candidates' strengths (which we define as the inverse of powers of costs).

The widely studied Mallows model [43], based on Condorcet [44], flips the order of each candidate pair (relative to a ground truth ranking) with a constant probability \(p\in(0,\frac{1}{2})\)[45; 46]. The process is repeated if a linear order is not attained. In the context of metric distortion, a limitation of this model is that it doesn't account for the relative distance of candidates to the voter. For a comprehensive review of RUM models, see Marden [47]. Critchlow et al. [48] does an axiomatic study of RUM models; our axioms are grounded in metric distortion and are distinct from theirs.

Recently, there has been significant interest in smoothed analysis [49] of social choice. Here a small amount of randomness is added to problem instances and its effect is studied on the satisfiability of axioms [50; 51; 52; 53] and the computational complexity of voting rules [54; 55; 56]. Baumeister et al. [50] term this model as being 'towards reality,' highlighting the need to study the randomness in the election instance generation processes. Unlike smoothed analysis where the voter and candidate positions are randomized, we consider these positions fixed, but the submitted votes are random given these positions. The technical difference appears in the benchmark (the "optimal" outcome in the denominator of the distortion is unchanged in our framework and changes in smoothed analysis).

## 2 Axioms and Model

Under probabilistic voting, the submitted preferences may no longer be consistent with the underlying distances. For a distribution \(\mathcal{P}(d)\) over \(\sigma_{\mathcal{N}}\), let \(q^{\mathcal{P}(d)}(i,j,j^{\prime})\) denote the induced marginal probability that voter \(i\) ranks candidate \(j\) higher than \(j^{\prime}\). We focus on these marginal probabilities on pairwise orders and provide axioms for classifying which \(q^{\mathcal{P}(d)}(\cdot)\) are suitable for studying distortion.

**Axiom 1** (Scale-Freeness (SF)).: _The probability \(q^{\mathcal{P}(d)}(\cdot)\) must be invariant to scaling of \(d\). That is, for any tuple \((i,j,j^{\prime})\) and any constant \(\kappa>0,\) we must have \(q^{\mathcal{P}(d)}(i,j,j^{\prime})=q^{\mathcal{P}(\kappa d)}(i,j,j^{\prime}).\)_

Note that the metric distortion (Definition 2) for deterministic voting is scale-free. We want to retain the same property in the probabilistic model as well. Conceptually, one may think of the voter's preferences as being a function of the relative (and not absolute) distances to the candidates.

**Axiom 2** (Independence of Other Candidates (IOC)).: _The probability \(q^{\mathcal{P}(d)}(i,j,j^{\prime})\) must be independent of the distance of voter \(i\) to all 'other' candidates, i.e., those in \(\mathcal{A}\setminus\{j,j^{\prime}\}\)._This axiom extends Luce's choice axioms [38], defined for selecting the top choice, to entire rankings. IOC is reminiscent of the _independence of irrelevant alternatives_ axiom for voting rules.

**Axiom 3** (Strict Monotonicity (SM)).: _For every tuple \((i,j,j^{\prime})\), for fixed distance \(d(i,j)>0,\) the probability \(q^{\mathcal{P}(d)}(i,j,j^{\prime})\) must be strictly increasing in \(d(i,j^{\prime})\) at all but at most finitely many points._

The monotonicity in \(d(i,j)\) follows since \(q^{\mathcal{P}(d)}(i,j^{\prime},j)=1-q^{\mathcal{P}(d)}(i,j,j^{\prime}).\) This axiom is natural.

In the Mallows model [43], \(q^{\mathcal{P}(d)}(\cdot)\) was derived by Busa-Fekete et al. [57] and is as follows:

\[\text{Mallows:}\qquad q^{\mathcal{P}(d)}(i,j,j^{\prime})=h(r_{j^{\prime}}-r_{j }+1,\phi)-h(r_{j^{\prime}}-r_{j},\phi).\] (1)

Here \(h(k,\phi)=\frac{k}{(1-\phi^{k})}.\) Whereas \(r_{j}\) and \(r_{j^{\prime}}\) are the positions of \(j\) and \(j^{\prime}\) in the ground-truth (noiseless) ranking, and the constant \(\phi\) is a dispersion parameter. Observe that this model fails Axiom 2 since it depends on the number of candidates between \(j\) and \(j^{\prime}\) in the noiseless ranking. It also fails Axiom 3 since it does not depend on the exact distances but only on the order of the distances.

**Plackett-Luce Model:** The PL model [37, 38] is'sequential' in the following way. For each voter \(i\in\mathcal{N}\), each candidate \(j\in\mathcal{A}\) has a'strength' \(s_{i,j}\). In most of the literature on RUMs, a common assumption is that \(s_{i,j}\) is the same for all voters \(i\). However, we choose this more general model to make it useful in the context of metric distortion. The voter chooses their top choice with probability proportional to the strengths. Similarly, for every subsequent rank, they choose a candidate from among the _remaining_ ones with probabilities proportional to their strengths. In terms of the pairwise order probabilities, the PL model reduces to the Bradley-Terry (BT) model [42], that is:

\[\text{PL/BT:}\qquad q^{\mathcal{P}(d)}(i,j,j^{\prime})=\frac{s_{i,j}}{s_{i,j}+s _{i,j^{\prime}}}\] (2)

Prima facie, in the metric distortion framework, any decreasing function of distance \(d(i,j)\) would be a natural choice for \(s_{i,j}\). However, not all such functions satisfy Axiom 1. The exponential function is a popular choice in the literature employing BT or PL models. However, in general, \(\frac{e^{-d(i,j)}}{e^{-d(i,j)}+e^{-d(i,j^{\prime})}}\neq\frac{e^{-2d(i,j)}}{e ^{-2d(i,j)}+e^{-2d(i,j^{\prime})}},\) thus failing the Scale-Freeness Axiom 1.

On the other hand, observe that all functions \(s=d^{-\theta}\) for any \(\theta\in(0,\infty)\) satisfy our axioms. We use the regime \(\theta\in(1,\infty)\) for technical simplicity in this work.

We also define the following class of functions "PQV" for \(q^{\mathcal{P}(d)}(\cdot)\) motivated by Quantal Response Theory [58] and its use in probabilistic voting [20]. Observe that PQV satisfies all our axioms.

**Definition 3** (Pairwise Quantal Voting (PQV)).: _Let the relative preference \(r(i,j,j^{\prime})\) be the ratio of distances, \(\frac{d(i,j^{\prime})}{d(i,j)}.\) For constant \(\lambda>0,\) PQV is as follows: \(q^{\mathcal{P}(d)}(i,j,j^{\prime})=\frac{e^{-\lambda/r(i,j,j^{\prime})}}{e^{- \lambda r(i,j,j^{\prime})}+e^{-\lambda/r(i,j,j^{\prime})}}.\)_

We now define a general class of functions for pairwise order probabilities in terms of the relative preference (ratio of distances) \(r\). Let \(\mathbf{G}\) be a class of functions such that any \(\mathbf{G}\ni g:[0,\infty)\cup\{\infty\}\rightarrow[0,1]\) has the following properties.

1. \(g\) is continuous and twice-differentiable.
2. \(g(0)=0.\) Further, \(g^{\prime}(r)>0\ \forall r\in(0,\infty)\) i.e. \(g(r)\) is strictly increasing in \([0,\infty).\)
3. Define \(\frac{1}{r}\) as \(+\infty\) when \(r=0\). Then we must have \(g(r)+g(\frac{1}{r})=1\ \ \forall r\geq 0.\)
4. There \(\exists c\in[0,\infty)\ \ \text{s.t.}\ \ q^{\prime r}(r)>0\ \ \forall r\in(0,c)\) i.e. \(g\) is convex in the open interval \((0,c).\)

Observe that PL (with \(g(r)=\frac{r^{\theta}}{1+r^{\theta}},\theta>1\)) and PQV (with \(g(r)=\frac{e^{-\lambda/r}}{e^{-\lambda r}+e^{-\lambda/r}},\lambda>0\)) are in \(\mathbf{G}\). Construction of distributions (if any exists) on rankings \(\sigma_{\mathcal{N}}\) which generate pairwise order

\begin{table}
\begin{tabular}{l c c c} \hline \hline  & Axiom 1: SF & Axiom 2: IOC & Axiom 3: Strict Monotonocity \\ \hline Mallows & ✓ & \(\times\) & \(\times\) \\ PL/BT with exponential in d & \(\times\) & ✓ & ✓ \\ PL/BT with powers of d & ✓ & ✓ & ✓ \\ PQV & ✓ & ✓ & ✓ \\ \hline \hline \end{tabular}
\end{table}
Table 1: Axioms satisfied by commonly studied models of probabilistic voting probabilities \(q^{\mathcal{P}(d)}(i,j,j^{\prime})=g(\frac{d(i,j^{\prime})}{d(i,j)})\) according to PQV is left for future work. We do not need it for our technical derivations. For PL, these distributions are known from prior work [40].

We assume \(g\in\mathbf{G}\) in the rest of the paper. Let \(\mathcal{M}(\mathcal{N}\cup\mathcal{A})\) denote the set of valid distance functions on \((\mathcal{N},\mathcal{A})\). For any \(g\) and \(d\in\mathcal{M}(\mathcal{N}\cup\mathcal{A})\) let \(\hat{\mathcal{P}}^{(g)}(d)\) denote the set of probability distributions on \(\sigma_{\mathcal{N}}\) for which the marginal pairwise order probabilities are \(g(\frac{d(i,j^{\prime})}{d(i,j)})\). That is,

\[\forall\mathcal{P}\in\hat{\mathcal{P}}^{(g)}(d),\sigma_{\mathcal{N}}\sim \mathcal{P}\implies\mathbb{P}[A\succ_{i}B]=g\left(\frac{d(i,B)}{d(i,A)} \right).\] (3)

We assume that all voters vote independently of each other. We now define metric distortion under probabilistic voting as a function of \(g\) for a given \(m\) and \(n\).

**Definition 4** (Metric Distortion under Probabilistic Voting).: \[\textsc{Dist}^{(g)}(f,n,m):=\sup_{\begin{subarray}{c}\mathcal{N}:|\mathcal{N }|=n\\ \mathcal{A}:|\mathcal{A}|=m\end{subarray}}\sup_{d\in\mathcal{M}(\mathcal{N}\cup \mathcal{A})}\sup_{\mathcal{P}\in\hat{\mathcal{P}}^{(g)}(d)}\frac{\mathbb{E}_{ \sigma_{\mathcal{N}}\sim\mathcal{P}}[SC(f(\sigma_{\mathcal{N}}),d)]}{\min_{ \begin{subarray}{c}\mathcal{A}\in\mathcal{A}\end{subarray}}SC(A,d)}.\] (4)

\(\textsc{Dist}^{(g)}(f)=\sup_{n,m}\textsc{dist}^{(g)}(f,n,m)\) _by supremizing over all possible \(n\) and \(m\)._

_The expectation is both over the randomness in the votes and the voting rule \(f\)._

Observe that the distortion is a supremum over all distributions in \(\hat{\mathcal{P}}^{(g)}(d)\). Since we focus on large elections (with large \(n\) and relatively small \(m\)), we define \(\textsc{Dist}^{(g)}\) as a function of \(m\) and \(n\).

As in Fig. 1, consider the 1-d Euclidean space with candidate \(X\) at the origin and \(Y\) at \(1\). Observe that \(g\left(\frac{x}{1-x}\right)\) and \(g\left(\frac{x}{1+x}\right)\) denote the probability that a voter located at a distance \(x\) from \(X\) votes for \(Y\) when the voter is to the left and right of \(X\) respectively. Interestingly, this 1-d intuition extends well for general metric spaces. Towards this, we define the following functions.

\[g_{\textsc{MID}}(x):=g\left(\frac{x}{1-x}\right)\forall x\in(0,1)\text{ and }g_{\textsc{OUT}}(x):=g\left(\frac{x}{1+x}\right)\forall x\in[0,\infty).\] (5)

**Lemma 1**.: \(\frac{g_{\textsc{MID}}(x)}{x}\) _and \(\frac{g_{\textsc{OUT}}(x)}{x}\) have unique local maxima in \((0,1)\) and \((0,\infty)\) respectively._

Denote the unique maximisers of \(\frac{g_{\textsc{MID}}(x)}{x}\) and \(\frac{g_{\textsc{OUT}}(x)}{x}\) by \(x^{*}_{\textsc{MID}}\) and \(x^{*}_{\textsc{OUT}}\) respectively.

For simplifying notation, in the rest of the work, we use \(\hat{g}_{\textsc{MID}}\) for \(\frac{g_{\textsc{MID}}(x^{*}_{\textsc{MID}})}{x^{*}_{\textsc{MID}}}\) and \(\hat{g}_{\textsc{OUT}}\) for \(\frac{g_{\textsc{OUT}}(x^{*}_{\textsc{MID}})}{x^{*}_{\textsc{OUT}}}\).

In the analysis in the rest of the paper, we will see \(\hat{g}_{\textsc{MID}}\) and \(\hat{g}_{\textsc{OUT}}\) appear many times, so we note these quantities for the PL and PQV models here. For the PL model with \(\theta=2\), \(\hat{g}_{\textsc{MID}}=\frac{\sqrt{2}+1}{2}\approx 1.21\) and \(\hat{g}_{\textsc{OUT}}=\frac{\sqrt{2}-1}{2}\approx 0.21\). When \(\theta=4\), \(\hat{g}_{\textsc{MID}}\approx 1.42\) and \(\hat{g}_{\textsc{OUT}}\approx 0.06\). When \(\theta\rightarrow\infty\), \(\hat{g}_{\textsc{MID}}\to 2\) and \(\hat{g}_{\textsc{OUT}}\to 0\). This limit is where PL resembles deterministic voting.

For PQV with \(\lambda=1\), \(\hat{g}_{\textsc{MID}}\approx 1.25\) and \(\hat{g}_{\textsc{OUT}}=0.18\). When \(\lambda\rightarrow\infty\), \(\hat{g}_{\textsc{MID}}\to 2\) and \(\hat{g}_{\textsc{OUT}}\to 0\).

Figure 1: A 1-d Euclidean example of voting probabilities. There are two candidates at 0 and 1. The figure on the left shows the voter position between 0 and 1. In the right figure, the voter is in positions to the left of 0. As the distance grows, both candidates look similar to the voter in the probabilistic model but not in deterministic voting. The case of voter positions to the right of 1 is symmetric.

## 3 Distortion of Plurality Rule Under Probabilistic Voting

In this section, we give upper and lower bounds on the distortion of the Plurality rule [59] (plu).In the limit the number of voters \(n\rightarrow\infty\) ("large election"), our upper and lower bounds match and are linear in the number of candidates \(m\). Let \(B\) represent the candidate that minimizes the social cost (referred to as 'best'), and let \(\{A_{j}\}_{j\in[m-1]}\) denote the set of other candidates.

### Upper bound on the distortion of Plurality(PLU)

**Theorem 1**.: _For every \(\epsilon>0\) and \(m\geq 2\) and \(n\geq m^{2}\) we have_

\[\textsc{dist}^{(g)}(\textsc{plu},n,m) \leq m(m-1)\left(\hat{g}_{\textsc{mid}}+\hat{g}_{\textsc{out}} \right)\exp\left(\frac{-n^{(\frac{1}{2}+\epsilon)}+2m}{(2n^{(\frac{1}{2}- \epsilon)}-1)m}\right)\] (6) \[+\max\left(\frac{m\hat{g}_{\textsc{mid}}}{(1-n^{-(\frac{1}{2}- \epsilon)})}-1,\frac{m\hat{g}_{\textsc{out}}}{(1-n^{-(\frac{1}{2}-\epsilon)}) }+1\right).\]

_Further, \(\lim\limits_{n\rightarrow\infty}\textsc{dist}^{(g)}(\textsc{plu},n,m)\leq \max\left(m\hat{g}_{\textsc{mid}}-1,m\hat{g}_{\textsc{out}}+1\right).\)_

To prove this theorem, we first give a lemma which upper bounds \(\frac{SC(W,d)}{SC(B,d)}\) under the constraint that the expected number of voters that rank candidate \(W\) over \(B\) is given by \(\alpha\). This ratio will be useful to bound the contribution of non-optimal candidate \(W\) to the distortion of plu. We state an optimization problem (7) below, which would be required to bound the ratio as a function of \(\alpha\).

\[\mathcal{E}_{\alpha} \,=\,\left\{\begin{aligned} \min\limits_{\mathbf{b},\mathbf{w}\in \mathbb{R}_{\geq 0}^{n}}&\frac{\sum\limits_{i=1}^{n}b_{i}}{\sum\limits_{i=1}^{n}w_ {i}}\\ \text{s.t.}&\sum\limits_{i=1}^{n}g\left(\frac{b_{i}} {w_{i}}\right)\geq\alpha&\forall\alpha\geq 0\\ &\max\limits_{i}|w_{i}-b_{i}|\leq\min\limits_{i}(w_{i}+b_{i})\end{aligned}\right.\] (7)

**Lemma 2**.: _For any two candidates \(W,B\in\mathcal{A}\) which satisfy \(\sum\limits_{i=1}^{n}\mathbb{P}\left[W\succ_{i}B\right]=\alpha\), we have_

\[\frac{SC(W,d)}{SC(B,d)}\leq\frac{1}{\text{opt}(\mathcal{E}_{\alpha})}\leq \max\left(\frac{n}{\alpha}\hat{g}_{\textsc{mid}}-1,\frac{n}{\alpha}\hat{g}_{ \textsc{out}}+1\right).\] (8)

Our proof is via Lemmas 3 and 4. Lemma 3 shows that we can bound the ratio of social costs by the inverse of the optimum value of \(\mathcal{E}_{\alpha}\) and Lemma 4 gives a lower bound on the optimum value of \(\mathcal{E}_{\alpha}\).

**Lemma 3**.: _For any two candidates \(W,B\in\mathcal{A}\) satisfying \(\sum\nolimits_{i=1}^{n}\mathbb{P}\left[W\succ_{i}B\right]=\alpha\), we have_

\[\frac{SC(W,d)}{SC(B,d)}\leq\frac{1}{\text{opt}(\mathcal{E}_{\alpha})}.\] (9)

Proof.: \(b_{i}\) and \(w_{i}\) in (7) represent the distances \(d(i,B)\) and \(d(i,W)\). The last constraint is the triangle inequality i.e. \(|d(i,B)-d(i,W)|\leq d(B,W)\leq|d(i,B)+d(i,W)|\) for every voter \(i\in\mathcal{N}\). 

Consider the following linearized version of (7).

\[\mathcal{E}_{\mu,\alpha} \,=\,\left\{\begin{aligned} \min\limits_{\mathbf{w},\mathbf{b}\in \mathbb{R}_{\geq 0}^{n}}&\left(\sum\limits_{i=1}^{n}b_{i}\right)-\mu \left(\sum\limits_{i=1}^{n}w_{i}\right)\\ \text{s.t.}&\sum\limits_{i=1}^{n}g\left(\frac{b_{i}} {w_{i}}\right)\geq\alpha&\forall 0\leq\mu\leq 1,\alpha\geq 0.\\ &|b_{i}-w_{i}|\leq 1\;\forall i\in[n]\\ & b_{i}+w_{i}\geq 1\;\forall i\in[n]\end{aligned}\right.\] (10)

**Lemma 4**.: \(\text{opt}(\mathcal{E}_{\alpha})\geq\min\left(\left(\frac{n}{\alpha}\hat{g}_{ \textsc{MID}}-1\right)^{-1},\left(\frac{n}{\alpha}\hat{g}_{\textsc{OUT}}+1 \right)^{-1}\right).\)__

Our proof uses Lemma 5 and is by solving a linearized version of (7) in (10). This is done by introducing an extra non-negative parameter \(\mu\leq 1\). Note that it is sufficient to consider \(\mu\leq 1\) since \(\text{opt}(\mathcal{E}_{\alpha})\leq 1\) because \(B\) minimises the social cost by definition. We find the smallest \(\mu\in(0,1)\) such that its objective is non-negative.

**Lemma 5**.: _If \(\text{opt}(\mathcal{E}_{\mu,\alpha})\geq 0\), then \(\text{opt}(\mathcal{E}_{\alpha})\geq\mu\)._

_Further, \(\text{opt}(\mathcal{E}_{\mu,\alpha})\geq 0\) if \(\mu=\min\left(\left(\frac{n}{\alpha}\hat{g}_{\textsc{MID}}-1\right)^{-1}, \left(\frac{n}{\alpha}\hat{g}_{\textsc{OUT}}+1\right)^{-1}\right)\)._

The first part follows since scaling each term by a constant \(r\) satisfies the constraints and also yields the same objective. And thus we may replace the constraints by \(\max_{i}|w_{i}-b_{i}|\leq 1\) and \(\min_{i}(w_{i}+b_{i})\geq 1\) in equation (10). Further, the objective function is linearized as \(\left(\sum_{i=1}^{n}b_{i}\right)-\mu\left(\sum_{i=1}^{n}w_{i}\right)\).

The proof of the second part is technical and has been moved to Appendix B. It involves introducing a Lagrangian multiplier \(\lambda\) and demonstrating that the objective function is non-negative for a suitably chosen \(\lambda\). To establish this, we show that minimising the Lagrangian over the boundaries of the constraint set given by \(|b_{i}-w_{i}|=1\) and \(b_{i}+w_{i}=1\) is sufficient. This requires a careful analysis.

The main technique used in proving Theorem 1 involves considering two cases for every non-optimal candidate \(A_{j}\): one where the expected number of voters ranking candidate \(A_{j}\) above \(B\) (call it \(\alpha_{j}\)) exceeds a threshold of \(\frac{n}{m}-\frac{n^{+1/2}}{m}\) and one where it does not. In the first case, the ratio of social costs of \(A_{j}\) and \(B\) is bounded using Lemma 2 that naturally gives a bound on contribution of candidate \(A_{j}\) to the distortion. In the later case, we use Chernoff bound to bound the probability of \(A_{j}\) being the winner and multiply it with the ratio of social costs of \(A_{j}\) and \(B\) to bound the distortion. The proof of Theorem 1 is in Appendix C.

### Lower bound on the distortion of Plurality

We now present a lower bound on the distortion of plu for any \(m\) in the limit \(n\) tends to infinity. This lower bound matches the upper bound of Theorem 1 in the limit. A full proof is in Appendix D. Note that the proof has an adversarially chosen distribution over the rankings subject to the marginals on pairwise relationships satisfying \(g\) (as in the definition of distortion under probabilistic voting 4). This lower bound does not apply to the PL model, which has a specific distribution over rankings.

**Theorem 2**.: _For every \(m\geq 2,\ \ \lim_{n\to\infty}\textsc{dist}^{(g)}(\textsc{plu},n,m)\geq\max \left(m\hat{g}_{\textsc{MID}}-1,m\hat{g}_{\textsc{OUT}}+1\right).\)_

Proof Sketch.: The proof is by an example in an Euclidean metric space in \(\mathbb{R}^{3}\). One candidate "C" is at \((1,0,0)\). The other \(m-1\) candidates are "good" and are equidistantly placed on a circle of radius \(\epsilon\) on the \(y-z\) plane centred at \((0,0,0)\). We call them \(\mathcal{G}:=\{G_{1},G_{2},\ldots,G_{m-1}\}\).

We present sketches of two constructions below for every \(\epsilon,\zeta>0\).

_Construction 1_: Let \(q_{\textsc{MID}}:=g\left(\frac{\sqrt{(x_{\textsc{MID}})^{2}+\epsilon^{2}}}{1 -x_{\textsc{MID}}^{2}}\right)\) and \(a_{\textsc{MID}}:=\frac{1}{m-1}\left(1-\frac{1+\zeta}{mq_{\textsc{MID}}}\right)\). Each of the \(m-1\) candidates in \(\mathcal{G}\) has \(\lfloor a_{\textsc{MID}}n\rfloor\) voters overlapping with it. The remaining voters (we call them "ambivalent") are placed at \((x_{\textsc{MID}}^{\star},0,0)\). Clearly, each voter overlapping with a candidate votes for it as the most preferred candidate with probability one. Each of the ambivalent voters votes as follows.

- With probability \(q_{\textsc{MID}}\), vote for candidate \(C\) as the top choice and uniformly randomly permute the other candidates in the rest of the vote.

- With probability \(1-q_{\textsc{MID}}\), vote for candidate \(C\) as the last choice and uniformly randomly permute the other candidates in the rest of the vote.

We show that the probability that \(C\) wins tends to 1 as \(n\to\infty\) and the distortion is \(m\hat{g}_{\textsc{MID}}-1\).

_Construction 2_: We give a construction where the locations of the candidates are identical as in Construction 1, and some voters are located with the "good" candidates. The ambivalent voters are at \((-x_{\textsc{OUT}}^{\star},0,0)\). We show that \(\mathbb{P}[C\text{ wins}]\) tends to 1 as \(n\to\infty\) and the distortion is \(m\hat{g}_{\textsc{OUT}}+1\). 

This result establishes that the distortion of Plurality is bound to increase linearly with \(m\) even under probabilistic voting, and is therefore not a good choice when \(m\) is even moderately large.

## 4 Distortion of Copeland Rule Under Probabilistic Voting

We now bound the distortion of the Copeland voting rule. We say that candidate \(W\) defeats candidate \(Y\) if more than half of the voters rank \(W\) above \(Y\).

**Theorem 3**.: _For every \(\epsilon>0,m\geq 2\) and \(n\geq 4\), we have_

\[\textsc{dist}^{(g)}(\textsc{cop},n,m) \leq 4m(m-1)\exp\Big{(}\frac{-n^{(\frac{1}{2}+\epsilon)}+8}{2(2n^{ (\frac{1}{2}-\epsilon)}-1)}\Big{)}\left(\hat{g}_{\textsc{mid}}+\hat{g}_{ \textsc{out}}\right)^{2}\] \[+\max\Bigl{(}\Bigl{(}\frac{2\hat{g}_{\textsc{mid}}}{1-n^{-(\frac {1}{2}-\epsilon)}}-1\Bigr{)}^{2},\Bigl{(}\frac{2\hat{g}_{\textsc{out}}}{1-n^{-( \frac{1}{2}-\epsilon)}}+1\Bigr{)}^{2}\Bigr{)}.\]

_For every \(m\geq 2\), we have \(\lim_{n\to\infty}\textsc{dist}^{(g)}(\textsc{cop},n,m)\leq\max\big{(}\left(2 \hat{g}_{\textsc{mid}}-1\right)^{2},\left(2\hat{g}_{\textsc{out}}+1\right)^{2} \big{)}.\)_

Proof Sketch.: A Copeland winner belongs to the uncovered set in the tournament graph, as demonstrated in [1, Theorem 15]. Recall that \(B\) denotes the candidate with the least social cost. For a Copeland winner \(W\), either \(W\) defeats \(B\) or it defeats a candidate \(Y\) who defeats \(B\).

We now consider two exhaustive cases on candidate \(A_{j}\) and define event \(E_{j}\) for every \(j\in[m-1]\) by computing the expected fraction of votes on pairwise comparisons. The event \(E_{j}\) denotes the existence of an at-most two hop directed path from a candidate \(A_{j}\) to candidate \(B\) for Copeland such that the expected fraction of votes on all edges along that path exceed \(\frac{n}{2}-\frac{n^{(1/2+\epsilon)}}{2}\).

If \(E_{j}\) holds true, we upper bound the ratio of social cost of candidate \(A_{j}\) and social cost of candidate \(B\) using Lemma 2 which in-turn would give a bound on the distortion. Otherwise, we use union bound and Chernoff's bound to upper bound the probability of \(A_{j}\) being the winner. Multiplying the probability bound with the ratio of social costs (one obtained from Lemma 2) leads to a bound on the distortion. A detailed proof is in Appendix E. 

## 5 Distortion of Random Dictator Rule Under Probabilistic Voting

We first give an upper bound on the distortion of RD; the proof is in Appendix F.

**Theorem 4**.: \(\textsc{dist}^{(g)}(\textit{RD},m,n)\leq(m-1)\hat{g}_{\textsc{mid}}+1.\)__

We now give a lower bound on the distortion of RD. We do this by constructing an example.

**Theorem 5**.: _For \(m\geq 3\) and \(n\geq 2,\textsc{dist}^{(g)}(\textit{RD},m,n)\geq 2+\frac{1}{g^{-1}(\frac{1}{m-1 })}-\frac{2}{n}.\)_

Proof.: We have a 1-D Euclidean construction. Let \(B\) be at \(0\) and all other candidates \(\mathcal{A}\setminus\{B\}\) be at 1. \(m-1\) voters are at \(0\) and one voter \(V\) is at \(\tilde{x}=g^{-1}(\frac{1}{m-1})/(1+g^{-1}(\frac{1}{m-1})).\)

The ranking for \(V\) is generated as follows: pick a candidate from \(\mathcal{A}\setminus\{B\}\) as the top rank uniformly at random. Keep \(B\) on the second rank. Permute the remaining candidates uniformly at random for the remaining ranks. Observe that the marginal pairwise order probabilities are consistent with the distance of \(V\) from \(B\) and each candidate in \(\mathcal{A}\setminus\{B\}\). In particular \(g(\frac{\tilde{x}}{1-\tilde{x}})=\frac{1}{m-1}.\) The distortion for this instance is \(\mathbb{P}[B\text{ wins}]\cdot 1+\mathbb{P}[B\text{ loses}]\cdot\frac{n-\tilde{x}}{\tilde{x}}= \frac{n-1}{n}+\frac{1}{n}\frac{n-\tilde{x}}{\tilde{x}}=1+\frac{1}{\tilde{x}}- \frac{2}{n}=2+\frac{1}{g^{-1}(\frac{1}{m-1})}-\frac{2}{n}.\) 

For \(g(r)=\frac{r^{\theta}}{1+r^{\theta}},\) we have \(g^{-1}(t)=(\frac{t}{1-t})^{\frac{1}{\theta}}.\) Then \(g^{-1}(\frac{1}{m-1})=(m-2)^{-\frac{1}{\theta}},\) and the distortion lower bound is \(\textsc{dist}^{(g)}(\textit{RD},m,n)\geq 2+(m-2)^{\frac{1}{\theta}}-\frac{2}{n},\) and \(\lim_{n\to\infty}\textsc{dist}^{(g)}(\textit{RD},m,n)\geq 2+(m-2)^{\frac{1}{ \theta}}.\)

However, note that this result does not apply to the PL model! This is because the PL model has a specific distribution on the rankings. In contrast, the above result is obtained by choosing an adversarial distribution on rankings subject to the constraint that its marginals on pairwise relations are given by \(g\). In the PL model, \(\mathbb{P}[A_{j}\text{ is top-ranked in }\sigma_{i}]=\frac{d(i,A_{j})^{-\theta}}{ \sum_{A_{k}\in\mathcal{A}}d(i,A_{k})^{-\theta}}\)[45]. We have the following result for the PL model. A proof via a similar construction as Theorem 5 is in Appendix G.

**Theorem 6**.: _Let \(\textsc{dist}_{PL}^{\theta}(\textit{RD},m,n)\) denote the distortion when the voters' rankings are generated per the PL model with parameter \(\theta\). We have \(\lim_{n\to\infty}\textsc{dist}_{PL}^{\theta}(\textit{RD},m,n)\geq 1+\frac{(m-1)^{1/ \theta}}{2}.\)_

## 6 Numerical Evaluations

Recall that higher values of \(\theta\) and \(\lambda\) correspond to lower randomness. From Figure 2, we observe that under sufficient randomness, the more intricate voting rule Copeland outshines the simpler rule RD, which only looks at a voter's top choice. Moreover, its distortion is independent of \(m\) in the limit \(n\to\infty\). This is in sharp contrast to RD, where the distortion is \(\Omega(m^{1/\theta})\) in the PL model, a sharp rate of increase in \(m\) for low values of \(\theta\). The distortion of Plurality increases linearly in \(m\).

An important observation is regarding the asymptotics when \(\theta\) or \(\lambda\) increases. The distortion of RD converges to its value under deterministic voting, i.e., 3. The distortion of Plurality also converges to \(2m-1\), the same as in deterministic voting. Since our bound on Copeland is not tight, it converges to \(9\) rather than \(5\). So far, in the study of metric distortion, the social choice community has looked only at these asymptotic; here, we present insights available from looking at the 'complete' picture. Interestingly, the distortion of RD increases with randomness, whereas that of Copeland decreases up to a certain point and then increases again. The reason for the increases in the high randomness regime is that the votes become too noisy to reveal the best candidate any more.

Since these plots have no abrupt transitions, this figure hints that _smoothened analysis_[52] (typically done with small amounts of noise) is unlikely to give any new insights regarding metric distortion.

## 7 Discussion and Future Work

We extend the metric distortion framework in social choice in an important way - by capturing the bounded rationality and randomness in voters' behaviour. Consideration of this randomness shows that, in general, the original metric distortion framework is too pessimistic on important voting rules, most notably on Copeland. On the other hand, the simplistic voting rule Random Dictator, which attains a distortion of 3 (at least as good as _any_ deterministic rule [1]), is not so good when we look at the full picture - its distortion increases with the number of candidates in our model. Our framework opens up opportunities to revisit the metric distortion problem with a closer-to-reality view of voters. It may hopefully lead to the development of new voting rules that consider the randomness of voters' behaviour. For example, Liu and Moitra [46] take a learning theory approach to design voting rules under the assumption of random voting per the Mallows model. However, technical analysis in our framework may be challenging because of the interplay of the _geometric_ structure of voters' positions and the _probabilistic_ nature of their votes.

Future WorkAn interesting extension would be to other tournament graph-based voting rules (weighted or unweighted). Our techniques are well-suited for this class of rules since it is based on the expected weights of the edges of the tournament graph. Closing the gap for the distortion of Copeland would be useful for getting deeper insights. Another open problem is the characterization of the set of distributions on rankings that induce the pairwise probabilities per PQV.

Figure 2: Here, we illustrate how the distortion bounds on different voting rules vary with \(m\) and with the randomness parameters of the two models, PL and PQV, in the limit \(n\to\infty\). Both the x and y axes are on the log scale. We plot the upper bound for Copeland (Theorem 3), the lower bound for RD (Theorem 5), and the matching bounds for Plurality (Theorem 1).

## References

* [1] Elliot Anshelevich, Onkar Bhardwaj, and John Postl. Approximating optimal social choice under metric preferences. In _Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence_, pages 777-783, 2015.
* [2] Jessica Dai and Eve Fleisig. Mapping social choice theory to RLHF. _arXiv preprint arXiv:2404.13038_, 2024.
* [3] Vincent Conitzer, Rachel Freedman, Jobst Heitzig, Wesley H Holliday, Bob M Jacobs, Nathan Lambert, Milan Mosse, Eric Pacuit, Stuart Russell, Hailey Schoelkopf, et al. Social choice for AI alignment: Dealing with diverse human feedback. _arXiv preprint arXiv:2404.10271_, 2024.
* [4] Seth D Baum. Social choice ethics in artificial intelligence. _AI & Society_, 35(1):165-176, 2020.
* [5] Jessie Finocchiaro, Roland Maio, Faidra Monachou, Gourab K Patro, Manish Raghavan, Ana-Andreea Stoica, and Stratis Tsirtsis. Bridging machine learning and mechanism design towards algorithmic fairness. In _Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency_, pages 489-503, 2021.
* [6] Francesca Rossi, Kristen Brent Venable, and Toby Walsh. _A Short Introduction to Preferences: Between AI and Social Choice_. Morgan & Claypool Publishers, 2011.
* [7] Meltem Ozturk, Alexis Tsoukias, and Philippe Vincke. Preference modelling. _Multiple criteria decision analysis: State of the art surveys_, 78:27-59, 2005.
* [8] Kenneth J Arrow. A difficulty in the concept of social welfare. _Journal of political economy_, 58(4):328-346, 1950.
* [9] Amartya Sen. Social choice theory. _Handbook of mathematical economics_, 3:1073-1181, 1986.
* [10] Kenneth J Arrow, Amartya Sen, and Kotaro Suzumura. _Handbook of social choice and welfare_, volume 2. Elsevier, 2010.
* [11] Felix Brandt, Vincent Conitzer, Ulle Endriss, Jerome Lang, and Ariel D Procaccia. _Handbook of computational social choice_. Cambridge University Press, 2016.
* [12] Ariel D Procaccia and Jeffrey S Rosenschein. The distortion of cardinal preferences in voting. In _International Workshop on Cooperative Information Agents_, pages 317-331. Springer, 2006.
* [13] James M Enelow and Melvin J Hinich. _The spatial theory of voting: An introduction_. CUP Archive, 1984.
* [14] Samuel Merrill and Bernard Grofman. _A unified theory of voting: Directional and proximity spatial models_. Cambridge University Press, 1999.
* [15] Fatih Erdem Kizilkaya and David Kempe. Plurality veto: A simple voting rule achieving optimal metric distortion. _Proceedings of the 31st International Joint Conference on Artificial Intelligence (IJCAI)_, pages 349-355, 2022.
* [16] Germain Kreweras. Aggregation of preference orderings. In _Mathematics and Social Sciences I: Proceedings of the seminars of Menthon-Saint-Bernard, France (1-27 July 1960) and of Gosing, Austria (3-27 July 1962)_, pages 73-79, 1965.
* [17] Moses Charikar, Prasanna Ramakrishnan, Kangning Wang, and Hongxun Wu. Breaking the metric voting distortion barrier. In _Proceedings of the 2024 Annual ACM-SIAM Symposium on Discrete Algorithms (SODA)_, pages 1621-1640. SIAM, 2024.
* [18] Peter J Coughlin. _Probabilistic voting theory_. Cambridge University Press, 1992.
* [19] Kevin M Quinn, Andrew D Martin, and Andrew B Whitford. Voter choice in multi-party democracies: a test of competing theories and models. _American Journal of Political Science_, pages 1231-1247, 1999.
* [20] Richard D McKelvey and John W Patty. A theory of voting in large elections. _Games and Economic Behavior_, 57(1):155-180, 2006.

* Pfeiffer et al. [2012] Thomas Pfeiffer, Xi Gao, Yiling Chen, Andrew Mao, and David Rand. Adaptive polling for information aggregation. In _Proceedings of the AAAI conference on artificial intelligence_, volume 26, pages 122-128, 2012.
* Parkes et al. [2012] David C Parkes, Houssein Azari Soufiani, and Lirong Xia. Random utility theory for social choice. In _Proceedings of the 25th Annual Conference on Neural Information Processing Systems_. Curran Associates, Inc., 2012.
* Soufiani et al. [2013] Hossein Azari Soufiani, David C Parkes, and Lirong Xia. Preference elicitation for general random utility models. In _Proceedings of the Twenty-Ninth Conference on Uncertainty in Artificial Intelligence_, pages 596-605, 2013.
* Boyd and Vandenberghe [2004] Stephen P Boyd and Lieven Vandenberghe. _Convex optimization_. Cambridge university press, 2004.
* Charikar and Ramakrishnan [2022] Moses Charikar and Prasanna Ramakrishnan. Metric distortion bounds for randomized social choice. In _Proceedings of the 2022 Annual ACM-SIAM Symposium on Discrete Algorithms (SODA)_, pages 2986-3004. SIAM, 2022.
* Anshelevich et al. [2021] Elliot Anshelevich, Aris Filos-Ratsikas, Nisarg Shah, and Alexandros A Voudouris. Distortion in social choice problems: The first 15 years and beyond. In _30th International Joint Conference on Artificial Intelligence_, pages 4294-4301, 2021.
* Abramowitz et al. [2019] Ben Abramowitz, Elliot Anshelevich, and Wennan Zhu. Awareness of voter passion greatly improves the distortion of metric social choice. In _International Conference on Web and Internet Economics_, pages 3-16. Springer, 2019.
* Amanatidis et al. [2021] Georgios Amanatidis, Georgios Birmpas, Aris Filos-Ratsikas, and Alexandros A Voudouris. Peeking behind the ordinal curtain: Improving distortion via cardinal queries. _Artificial Intelligence_, 296:103488, 2021.
* Anshelevich et al. [2024] Elliot Anshelevich, Aris Filos-Ratsikas, Christopher Jerrett, and Alexandros A Voudouris. Improved metric distortion via threshold approvals. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 38, pages 9460-9468, 2024.
* Hinich [1977] Melvin J Hinich. Equilibrium in spatial voting: The median voter result is an artifact. _Journal of Economic Theory_, 16(2):208-219, 1977.
* Black [1948] Duncan Black. On the rationale of group decision-making. _Journal of political economy_, 56(1):23-34, 1948.
* Banks and Duggan [2005] Jeffrey S Banks and John Duggan. Probabilistic voting in the spatial model of elections: The theory of office-motivated candidates. In _Social Choice and Strategic Decisions: Essays in Honor of Jeffrey S. Banks_, pages 15-56. Springer, 2005.
* Patty [2005] John Wiggs Patty. Local equilibrium equivalence in probabilistic voting models. _Games and Economic Behavior_, 51(2):523-536, 2005.
* Coughlin and Nitzan [1981] Peter Coughlin and Shmuel Nitzan. Electoral outcomes with probabilistic voting and nash social welfare maxima. _Journal of Public Economics_, 15(1):113-121, 1981.
* Coughlin and Nitzan [1981] Peter Coughlin and Shmuel Nitzan. Directional and local electoral equilibria with probabilistic voting. _Journal of Economic Theory_, 24(2):226-239, 1981.
* Xia [2013] Lirong Xia. Designing social choice mechanisms using machine learning. In _Proceedings of the international conference on Autonomous agents and multi-agent systems_, pages 471-474, 2013.
* Plackett [1975] Robin L Plackett. The analysis of permutations. _Journal of the Royal Statistical Society Series C: Applied Statistics_, 24(2):193-202, 1975.
* Luce [2005] R Duncan Luce. _Individual choice behavior: A theoretical analysis_. Courier Corporation, 2005.
* Gormley and Murphy [2006] Isobel Claire Gormley and Thomas Brendan Murphy. Analysis of Irish third-level college applications data. _Journal of the Royal Statistical Society Series A: Statistics in Society_, 169(2):361-379, 2006.

* Azari et al. [2012] Hossein Azari, David Parks, and Lirong Xia. Random utility theory for social choice. _Advances in Neural Information Processing Systems_, 25, 2012.
* Gormley and Murphy [2004] Isobel Claire Gormley and Thomas Brendan Murphy. A grade of membership model for rank data. _Bayesian Analysis_, 1(1):1-32, 2004.
* Bradley and Terry [1952] Ralph Allan Bradley and Milton E Terry. Rank analysis of incomplete block designs: I. The method of paired comparisons. _Biometrika_, 39(3/4):324-345, 1952.
* Mallows [1957] Colin L Mallows. Non-null ranking models. i. _Biometrika_, 44(1/2):114-130, 1957.
* de Condorcet [1785] Marquis de Condorcet. Essay on the application of analysis to the probability of majority decisions. _Paris: Imprimerie Royale_, page 1785, 1785.
* Caragiannis et al. [2016] Ioannis Caragiannis, Ariel D Procaccia, and Nisarg Shah. When do noisy votes reveal the truth? _ACM Transactions on Economics and Computation (TEAC)_, 4(3):1-30, 2016.
* Liu and Moitra [2023] Allen Liu and Ankur Moitra. Robust voting rules from algorithmic robust statistics. In _Proceedings of the Annual ACM-SIAM Symposium on Discrete Algorithms (SODA)_, pages 3471-3512. SIAM, 2023.
* Marden [1996] John I Marden. _Analyzing and modeling rank data_. CRC Press, 1996.
* Critchlow et al. [1991] Douglas E Critchlow, Michael A Fligner, and Joseph S Verducci. Probability models on rankings. _Journal of mathematical psychology_, 35(3):294-318, 1991.
* Spielman and Teng [2004] Daniel A Spielman and Shang-Hua Teng. Smoothed analysis of algorithms: Why the simplex algorithm usually takes polynomial time. _Journal of the ACM (JACM)_, 51(3):385-463, 2004.
* Baumeister et al. [2020] Dorothea Baumeister, Tobias Hogrebe, and Jorg Rothe. Towards reality: smoothed analysis in computational social choice. In _Proceedings of the 19th International Conference on Autonomous Agents and Multiagent Systems_, pages 1691-1695, 2020.
* Flanigan et al. [2023] Bailey Flanigan, Daniel Halpern, and Alexandros Psomas. Smoothed analysis of social choice revisited. In _International Conference on Web and Internet Economics_, pages 290-309. Springer, 2023.
* Xia [2020] Lirong Xia. The smoothed possibility of social choice. _Advances in Neural Information Processing Systems_, 33:11044-11055, 2020.
* Xia [2023] Lirong Xia. Semi-random impossibilities of condorcet criterion. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 37, pages 5867-5875, 2023.
* Liu and Xia [2022] Ao Liu and Lirong Xia. The semi-random likelihood of doctrinal paradoxes. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 36, pages 5124-5132, 2022.
* Xia and Zheng [2021] Lirong Xia and Weiqiang Zheng. The smoothed complexity of computing kemeny and slater rankings. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 35, pages 5742-5750, 2021.
* Xia and Zheng [2022] Lirong Xia and Weiqiang Zheng. Beyond the worst case: Semi-random complexity analysis of winner determination. In _International Conference on Web and Internet Economics_, pages 330-347. Springer, 2022.
* Busa-Fekete et al. [2014] Robert Busa-Fekete, Eyke Hullermeier, and Balazs Szorenyi. Preference-based rank elicitation using statistical models: The case of mallows. In _International conference on machine learning_, pages 1071-1079. PMLR, 2014.
* McKelvey and Palfrey [1995] Richard D McKelvey and Thomas R Palfrey. Quantal response equilibria for normal form games. _Games and economic behavior_, 10(1):6-38, 1995.
* Arrow [1963] Kenneth J. Arrow. _Social Choice and Individual Values_. Yale University Press, New Haven, 2 edition, 1963.
* Canny [2017] John Canny. Chernoff bounds. URL https://people.eecs.berkeley.edu/~jfc/cs174/lecs/lec10/lec10.pdf.

## Appendix A Proof of Lemma 1

_Lemma_ (Restatement of Lemma 1).: \(\frac{g_{\textsc{MID}}(x)}{x}\) and \(\frac{g_{\textsc{MID}}(x)}{x}\) have unique local maxima in \((0,1)\) and \((0,\infty)\) respectively.

To prove Lemma 1, we first state and prove Lemma 6 which shows that \(g_{\textsc{MID}}(x)\) and \(g_{\textsc{OUT}}(x)\) change from convex to concave in intervals \((0,1)\) and \((0,\infty)\) respectively.

**Lemma 6**.: \(\bullet\) _There \(\exists c_{1}\in[0,1]\) s.t. \(g_{\textsc{MID}}(x)\) is convex in \([0,c_{1}]\) and concave in \([c_{1},1]\)._

\(\bullet\) _There \(\exists c_{2}\in[0,\infty)\) s.t. \(g_{\textsc{OUT}}(x)\) is convex in \([0,c_{2}]\) and concave in \([c_{2},\infty)\)._

Proof.: Observe that \(g^{\prime\prime}(x)<0\) for \(x\geq 1\).

Recall that \(g_{\textsc{MID}}(x)=g\left(\frac{x}{1-x}\right)\) thus, \(g^{\prime}_{\textsc{MID}}(x)=g^{\prime}\left(\frac{x}{1-x}\right)\frac{1}{(1- x)^{2}}\) and \(g_{\textsc{MID}}(x)+g_{\textsc{MID}}(1-x)=1\) Thus, \(g^{\prime\prime}_{\textsc{MID}}(x)=g^{\prime}\left(\frac{x}{1-x}\right)\frac {2}{(1-x)^{3}}+g^{\prime\prime}\left(\frac{x}{1-x}\right)\frac{1}{(1-x)^{4}}\). Observe that \(g^{\prime\prime}_{\textsc{MID}}(0)>0\) which implies \(\lim_{x\to 1}g^{\prime\prime}_{\textsc{MID}}(x)<0\) and thus, there must exist a \(c\in(0,1)\) such that \(g^{\prime\prime}_{\textsc{MID}}(c)=0\).

Now we show that there cannot exist two distinct \(c_{1},c_{2}\in(0,1)\) such that \(g^{\prime\prime}_{\textsc{MID}}(c_{1})=0\) and \(g^{\prime\prime}_{\textsc{MID}}(c_{2})=0\). We prove this statement by contradiction assuming the contrary which implies that \(g_{\textsc{MID}}(x)\) must have changed its sign twice. However, since \(g^{\prime}\left(\frac{x}{1-x}\right)>0\) we must have \(g^{\prime\prime}(\frac{x}{1-x})\) changing its sign twice which is a contradiction since \(g^{\prime\prime}(r)>0\) for \(r\in(0,c)\) and \(g^{\prime\prime}(r)<0\) for \(r\in(c,\infty)\).

Now consider \(g_{\textsc{OUT}}(x)=g\left(\frac{x}{1+x}\right)\) we have \(g^{\prime}_{\textsc{OUT}}(x)=g^{\prime}\left(\frac{x}{1+x}\right)\frac{1}{(1+ x)^{2}}\). Thus, \(g^{\prime\prime}_{\textsc{OUT}}(x)=-g^{\prime}\left(\frac{x}{1+x}\right)\frac {2}{(1+x)^{3}}+g^{\prime\prime}\left(\frac{x}{1+x}\right)\frac{1}{(1+x)^{4}}\). Using a similar approach, we can also prove the second point in the Lemma. 

Using Lemma 6, we now prove Lemma 1 showing the existence of unique maxims of \(\frac{g_{\textsc{MID}}(x)}{x}\) and \(\frac{g_{\textsc{MID}}(x)}{x}\).

Proof of Lemma 1.: Recall from Lemma 6 that \(g_{\textsc{MID}}(x)\) is convex in \([0,c_{1}]\) and concave in \([c_{1},1]\).

Since the first derivative equals zero at every local maxima, we must have \(xg^{\prime}_{\textsc{MID}}(x)-g(x)=0\) for any local maxima \(x\). We now argue that such a maxima cannot exist in \([0,c_{1}]\). Suppose such a maxima exists in that case, we must have \(g^{\prime}_{\textsc{MID}}(x)=\frac{g_{\textsc{MID}}(x)-g_{\textsc{MID}}(0)}{x-0}\) for some \(x\in(0,c_{1})\). Applying LMVT in the interval \([0,x]\)1, we must have some \(t\in(0,x)\) s.t. \(g^{\prime}_{\textsc{MID}}(t)=\frac{g_{\textsc{MID}}(x)-g_{\textsc{MID}}(0)}{x-0}\), thus implying \(g^{\prime}_{\textsc{MID}}(x)=g^{\prime}(t)\) contradicting the fact that \(g^{\prime}_{\textsc{MID}}(r)\) is strictly increasing in \([0,c_{1}]\).

Footnote 1: Observe that \(g(x)/x\) has a removable discontinuity at \(0\) since the limit is defined.

Observe that \(g_{\textsc{MID}}(t)-t\frac{g_{\textsc{MID}}(c_{1})}{c_{1}}\) is zero at \(t=0\) and \(t=c_{1}\) and thus, by Rolle's theorem, we have \(g^{\prime}_{\textsc{MID}}(x)=\frac{g_{\textsc{MID}}(c_{1})}{c_{1}}\) for some \(x\in(0,c_{1})\). Since, \(g^{\prime}_{\textsc{MID}}(x)\) is increasing in \([0,c_{1}]\), we must have \(g^{\prime}_{\textsc{MID}}(c_{1})>\frac{g_{\textsc{MID}}(c_{1})}{c_{1}}\). Observe \(\lim_{t\to 1}\frac{g_{\textsc{MID}}(t)}{t}=1\) and \(\frac{g_{\textsc{MID}}(c_{1})}{c_{1}}>1\). Also, we have \(\frac{d}{dt}\left(\frac{g_{\textsc{MID}}(t)}{t}\right)\Big{|}_{t=c_{1}}>0\) since \(c_{1}g^{\prime}_{\textsc{MID}}(c_{1})>g_{\textsc{MID}}(c_{1})\) implying \(g_{\textsc{MID}}(t)/t\) is increasing at \(t=c_{1}\). Thus, \(g_{\textsc{MID}}(t)/t\) must have at least one local maxima \(x^{*}\) in the open interval \((c_{1},\infty)\) and no local maxima elsewhere.

We now argue that this local maxima \(x^{*}\) is unique. Suppose we have two distinct local maxims at \(x_{1},x_{2}\in(c_{1},\infty)\) and thus, we have \(x_{1}g^{\prime}_{\textsc{MID}}(x_{1})-g_{\textsc{MID}}(x_{1})=0\) and \(x_{2}g^{\prime}_{\textsc{MID}}(x_{2})-g_{\textsc{MID}}(x_{2})=0\). Rolle's theorem would imply that there exists \(t\in(x_{1},x_{2})^{2}\) s.t. \(tg^{\prime\prime}_{\textsc{MID}}(t)=0\) which is a contradiction since \(g^{\prime\prime}_{\textsc{MID}}(x)<0\) in \((c_{1},\infty)\).

Similarly, we can prove the result on the existence and uniqueness of maxima of the function \(\frac{g\left(\frac{x}{x+1}\right)}{x}\). 

## Appendix B Proof of Lemma 5

_Lemma_ (Restatement of Lemma 5).: If \(\text{opt}(\mathcal{E}_{\mu,\alpha})\geq 0\), then \(\text{opt}(\mathcal{E}_{\alpha})\geq\mu\).

Further, \(\text{opt}(\mathcal{E}_{\mu,\alpha})\geq 0\) if \(\mu=\min\Big{(}\big{(}\frac{n}{\alpha}\hat{g}_{\textsc{MID}}-1\big{)}^{-1}, \big{(}\frac{n}{\alpha}\hat{g}_{\textsc{OUT}}+1\big{)}^{-1}\Big{)}\).

Proof.: To lower bound the optimal value of \(\mathcal{E}_{\mu,\alpha}\), we first pre-multiply the first constraint by \(\lambda\) (and substitute \(\frac{b_{i}}{w_{i}}=r_{i}\ \forall i\in[n]\)) and thus define,

\[F(\mathbf{r},\mathbf{b},\lambda)=\left(\sum_{i=1}^{n}b_{i}\right)-\mu\left( \sum_{i=1}^{n}\frac{b_{i}}{r_{i}}\right)-\lambda\left(\sum_{i=1}^{n}g(r_{i})- \alpha\right).\] (11)

Further, we define the set which satisfies the last two constraints in \(\mathcal{E}_{\mu,\alpha}\) by \(\mathcal{C}\) as

\[\mathcal{C}:=\{(\mathbf{r},\mathbf{b})\in(\mathbb{R}_{\geq 0}^{n},\mathbb{R}_{ \geq 0}^{n}):b_{i}(1+1/r_{i})\geq 1;|b_{i}(1/r_{i}-1)|\leq 1\ \forall i\in[n]\}.\] (12)

From the theory of Lagrangian, we have the following

\[\text{opt}(\mathcal{E}_{\mu,\alpha})\geq\min_{(\mathbf{r},\mathbf{b})\in \mathcal{C}}\max_{\lambda\geq 0}F(\mathbf{r},\mathbf{b},\lambda)\geq\max_{ \lambda\geq 0}\min_{(\mathbf{r},\mathbf{b})\in\mathcal{C}}F(\mathbf{r},\mathbf{b}, \lambda).\] (13)

Now for a fixed \(\lambda>0\), we minimise \(F(\mathbf{r},\mathbf{b},\lambda)\) over \((\mathbf{r},\mathbf{b})\in\mathcal{C}\). Observe that for every \(i\in[n]\), it is sufficient to minimise \(h(r_{i},b_{i})\) defined as follows.

\[h(r_{i},b_{i}):=b_{i}(1-\mu/r_{i})-\lambda\left(g(r_{i})-\frac{\alpha}{n} \right).\] (14)

Observe that the constraints in \(\mathcal{C}\) can be written as \(b_{i}\geq\frac{r_{i}}{1+r_{i}}\) and \(b_{i}\leq\frac{r_{i}}{|1-r_{i}|}\).

Observe that for a given \(r_{i}\), the function \(h(r_{i},b_{i})\) is monotonic in \(b_{i}\) and thus the optimum point must lie on the boundary and first optimize over \(b_{i}(1+1/r_{i})=1\) (call it \(\mathcal{C}_{i}^{\text{\tiny MID}}\)) and \(|b_{i}(1-1/r_{i})|=1\) (call it \(\mathcal{C}_{i}^{\text{\tiny OUT}}\)) respectively.

Recall from Lemma 6 that there exists \(c_{1},c_{2}\) s.t. \(g_{\text{\tiny MID}}(x)\) is convex in \((0,c_{1})\) and concave in \((c_{1},1)\) and \(g_{\text{\tiny OUT}}(x)\) is convex in \((0,c_{2})\) and concave in \((c_{2},\infty)\).

* _Minimisation of \(h(r_{i},b_{i})\) over \(b_{i}(1+1/r_{i})=1\)._ We first substitute \(1/r_{i}=1/b_{i}-1\) in the function and thus, can write the function \(h(b_{i})=b_{i}(\mu+1)-\mu-\lambda\left(g\left(\frac{b_{i}}{1-b_{i}}\right)- \frac{\alpha}{n}\right)=b_{i}(\mu+1)-\mu-\lambda\left(g_{\text{\tiny MID}}(b_ {i})-\frac{\alpha}{n}\right)\). Observe that on optimizing over \(b_{i}\), we obtain two local minima, one at \(b_{i}=0\) and the other at \(b_{i}=\tilde{x}^{\text{\tiny MID}}(\lambda)\in(c_{1},\infty)\) where \(\tilde{x}^{\text{\tiny MID}}(\lambda)\) satisfies the following equations if \(\lambda\geq\frac{1+\mu}{g_{\text{\tiny MID}}(c_{1})}\). Otherwise, we have a unique minima at \(b_{i}=0\). 3 Footnote 3: This follows from the fact that \(g_{\text{\tiny MID}}(x)\) is monotonically decreasing in \([c_{1},1)\) and monotonically increasing in \([0,c_{1})\). \[g^{\prime}_{\text{\tiny MID}}(\lambda)>c_{1}\] since \[g_{\text{\tiny MID}}\] is concave only in \[[c_{1},1]\]. Also observe that since \[g^{\prime}_{\text{\tiny MID}}(x)\] is monotonically increasing, \[\tilde{x}^{\text{\tiny MID}}(\lambda)\] is monotonically increasing in \[\lambda\].
* _Minimisation of \(h(r_{i},b_{i})\) over \(b_{i}|(1-1/r_{i})|=1\)._ On substituting, we write the function \[h(b_{i})=\begin{cases}(1-\mu)b_{i}-\mu-\lambda\left(g\left(\frac{b_{i}}{1+b_{i}} \right)-\frac{\alpha}{n}\right)&=(1-\mu)b_{i}-\mu-\lambda\left(g_{\text{\tiny OUT }}(b_{i})-\frac{\alpha}{n}\right)&\text{if }r_{i}\geq 1\\ (1-\mu)b_{i}+\mu-\lambda\left(g\left(\frac{b_{i}}{b_{i}-1}\right)-\frac{\alpha }{n}\right)&\stackrel{{(a)}}{{=}}(1-\mu)b_{i}+\mu-\lambda+ \lambda\left(g_{\text{\tiny OUT}}(b_{i}-1)+\frac{\alpha}{n}\right)&\text{ otherwise}\end{cases}\] (16) \((a)\) follows from the fact that \(g(r)+g(1/r)=1\). Since the second function has only a single minima at \(b_{i}=1\), it is sufficient to consider only the first function in the case \(r_{i}\geq 1\). Observe that on optimizing over \(b_{i}\), we obtain two local minima one at \(b_{i}=0\) and one at \(b_{i}=\tilde{x}^{\text{\tiny OUT}}(\lambda)\in(c_{2},\infty)\) where \(\tilde{x}^{\text{\tiny OUT}}(\lambda)\) satisfies the equations if \(\lambda\geq\frac{1-\mu}{g_{\text{\tiny OUT}}^{\prime}(c_{2})}\). Otherwise, we have a unique minima at \(b_{i}=0\). 4 Footnote 4: This follows from the fact that \(g_{\text{\tiny OUT}}(x)\) is monotonically decreasing in \([c_{2},\infty)\) and monotonically increasing in \([0,c_{2})\). Since \(g^{\prime}_{\text{\tiny OUT}}(\infty)=0\), the solution to (17) exists for every \(\lambda\in\left(\frac{1-\mu}{g_{\text{\tiny OUT}}(c_{2})},\infty\right)\).

\[g^{\prime}_{\text{\tiny OUT}}(\tilde{x}^{\text{\tiny OUT}}(\lambda))=\left( \frac{1-\mu}{\lambda}\right)\text{ and }g^{\prime\prime}_{\text{\tiny OUT}}(\tilde{x}^{\text{\tiny OUT}}(\lambda))<0.\] (17)

Thus, we have \(\tilde{x}^{\text{\tiny OUT}}(\lambda)>c_{2}\) since \(g_{\text{\tiny OUT}}\) is concave only in \([c_{2},\infty)\). Also observe that since \(g^{\prime}_{\text{\tiny OUT}}(x)\) is monotonic, \(\tilde{x}^{\text{\tiny OUT}}(\lambda)\) is monotonic in \(\lambda\).

Since, this argument is true for every \(i\in[n]\), we obtain

\[\min_{(\mathbf{r},\mathbf{b})}F(\mathbf{r},\mathbf{b},\lambda)=n \cdot\min\Biggl{(}-\mu+\lambda\frac{\alpha}{n}, (\mu+1)\tilde{x}^{\textsc{mdp}}(\lambda)-\mu-\lambda\left(g_{\textsc{mdp}}( \tilde{x}^{\textsc{mdp}}(\lambda))-\frac{\alpha}{n}\right),\] \[(1-\mu)\tilde{x}^{\textsc{mdp}}(\lambda)-\mu-\lambda\left(g_{ \textsc{mdp}}(\tilde{x}^{\textsc{mdp}}(\lambda))-\frac{\alpha}{n}\right) \Biggr{)}.\] (18)

Since \(x^{*}_{\textsc{mdp}}\) is the local maximiser of \(\frac{g_{\textsc{mdp}}(x)}{x}\), we have

\[g_{\textsc{mdp}}(x^{*}_{\textsc{mdp}})=x^{*}_{\textsc{mdp}}\hat{g}_{\textsc{ mdp}}\text{ and }x^{*}_{\textsc{mdp}}>c_{1}.\] (19)

Similarly,

\[g_{\textsc{mdp}}(x^{*}_{\textsc{mdp}})=x^{*}_{\textsc{mdp}}\hat{g}_{\textsc{ mdp}}\text{ and }x^{*}_{\textsc{mdp}}>c_{2}.\] (20)

For the purpose of this analysis, we define two functions \(\delta^{\textsc{mdp}}(\lambda)\) and \(\delta^{\textsc{mdp}}(\lambda)\) below.

\[\delta^{\textsc{mdp}}(\lambda)=(\mu+1)\tilde{x}^{\textsc{mdp}}(\lambda)-\mu- \lambda\left(g_{\textsc{mdp}}(\tilde{x}^{\textsc{mdp}}(\lambda))-\frac{\alpha }{n}\right).\] (21)

\[\delta^{\textsc{mdp}}(\lambda)=(1-\mu)\tilde{x}^{\textsc{mdp}}(\lambda)-\mu- \lambda\left(g_{\textsc{mdp}}(\tilde{x}^{\textsc{mdp}}(\lambda))-\frac{\alpha }{n}\right).\] (22)

We also define

\[\mu^{*}:=\min\left(\left(\frac{n}{\alpha}\hat{g}_{\textsc{mdp}}-1\right)^{-1}, \left(\frac{n}{\alpha}\hat{g}_{\textsc{mdp}}+1\right)^{-1}\right),\quad\text{ and }\quad\quad\lambda^{*}:=\mu^{*}\frac{n}{\alpha}.\] (23)

Recall that we aim to show \(\text{opt}(\mathcal{E}_{\mu,\alpha})\geq 0\) when \(\mu=\mu^{*}\) and thus substitute \(\mu=\mu^{*}\) in every subsequent equation. Observe that it is sufficient to show \(\delta^{\textsc{mdp}}(\lambda^{*})\) and \(\delta^{\textsc{mdp}}(\lambda^{*})\) are non-negative since this would imply that \(\max_{\lambda\geq 0}\min_{(\mathbf{r},\mathbf{b})\in\mathcal{C}}F(\mathbf{r}, \mathbf{b},\lambda)\) is non-negative.

We now consider the following two exhaustive cases.

* Case 1: \(\hat{g}_{\textsc{mdp}}-\frac{\alpha}{n}>\hat{g}_{\textsc{mdp}}+\frac{\alpha}{n}\). Observe from Equation (15), \[g^{\prime}_{\textsc{mdp}}(\tilde{x}^{\textsc{mdp}}(\lambda^{*}))=\max\left( \frac{n}{\alpha}\left(\frac{1}{\mu^{*}}+1\right),g^{\prime}_{\textsc{mdp}}(1^ {-})\right)=g^{\prime}_{\textsc{mdp}}(x^{*}_{\textsc{mdp}})\ \stackrel{{(d)}}{{ \Longrightarrow}}\ \tilde{x}^{\textsc{mdp}}(\lambda^{*})=x^{*}_{\textsc{mdp}}.\] (24)

\((d)\) follows from the fact that both \(\tilde{x}^{\textsc{mdp}}(\lambda^{*})\) and \(x^{*}_{\textsc{mdp}}\) exceed \(c_{1}\) and \(g^{\prime}_{\textsc{mdp}}(x)\) is monotonically decreasing for \(x\geq c_{1}\).

\[\delta^{\textsc{mdp}}(\lambda^{*})=(\mu^{*}+1)\tilde{x}^{\textsc{ mdp}}(\lambda^{*})-\mu^{*}-\lambda^{*}\left(g_{\textsc{mdp}}(\tilde{x}^{ \textsc{mdp}}(\lambda^{*}))-\frac{\alpha}{n}\right)\] \[\stackrel{{(b)}}{{\geq}}(-\mu^{*}+\lambda^{*}\alpha/n)+ \lambda^{*}\left(\tilde{x}^{\textsc{mdp}}(\lambda^{*})g^{\prime}_{\textsc{mdp }}(\tilde{x}^{\textsc{mdp}}(\lambda^{*}))-g_{\textsc{mdp}}(\tilde{x}^{ \textsc{mdp}}(\lambda^{*}))\right)\stackrel{{(c)}}{{\geq}}0.\] (25)

\((b)\) follows from \(g^{\prime}_{\textsc{mdp}}(\tilde{x}^{\textsc{mdp}}(\lambda^{*}))=\frac{1+\mu^{* }}{\lambda^{*}}\)5 as stated in Equation (15).

\((c)\) follows from \(\tilde{x}^{\textsc{mdp}}(\lambda^{*})=x^{*}_{\textsc{mdp}}\) (in Equation (24)) and \(g_{\textsc{mdp}}(x^{*}_{\textsc{mdp}})=x^{*}_{\textsc{mdp}}\hat{g}_{\textsc{ mdp}}\) (in Equation (19)) and the fact that \(\mu^{*}=\lambda^{*}\frac{\alpha}{n}\). Now consider,

Footnote 5: This follows from the fact that \(\frac{1+\mu^{*}}{\lambda^{*}}=\hat{g}_{\textsc{mdp}}=g^{\prime}(x^{*}_{\textsc{ mdp}})\geq g^{\prime}_{\textsc{mdp}}(1^{-})\)

\((g)\) follows from the definition of \(\lambda^{*}\) and that \(\mu=\lambda^{*}\frac{\alpha}{n}\).

\((h)\) follows from the constraint in (17).

\((i)\) follows from the fact that \(g^{\prime}_{\textsc{out}}(x)\) is monotonically decreasing in \(x\) in \([c_{2},\infty)\).

\[\delta^{\textsc{out}}(\lambda^{*}) =(1-\mu)\tilde{x}^{\textsc{out}}(\lambda^{*})-\mu-\lambda^{*} \left(g_{\textsc{out}}(\tilde{x}^{\textsc{out}}(\lambda^{*}))-\frac{\alpha}{n}\right)\] \[\overset{(j)}{=}\left(-\mu+\lambda^{*}\frac{\alpha}{n}\right)+ \lambda^{*}(\tilde{x}^{\textsc{out}}(\lambda^{*})g^{\prime}_{\textsc{out}}( \tilde{x}^{\textsc{out}}(\lambda^{*}))-g_{\textsc{out}}(\tilde{x}^{\textsc{out} }(\lambda^{*})))\] \[\overset{(k)}{\geq}0+0\geq 0.\] (26)

\((j)\) follows from \(g^{\prime}_{\textsc{out}}(\tilde{x}^{\textsc{out}}(\lambda))=\frac{1-\mu}{\lambda}\) as stated in Equation (17), and

\((k)\) follows from the following reasons:

* Observe that \(xg^{\prime}_{\textsc{out}}(x)-g_{\textsc{out}}(x)\) is monotonically decreasing in \([c_{2},\infty)\) as \(g_{\textsc{out}}\) is concave in this region. However, since \(x^{*}_{\textsc{out}}\geq\tilde{x}^{\textsc{out}}(\lambda^{*})\geq c_{2}\), we have \[(\tilde{x}^{\textsc{out}}(\lambda^{*})g^{\prime}_{\textsc{out}}(\tilde{x}^{ \textsc{out}}(\lambda^{*}))-g_{\textsc{out}}(\tilde{x}^{\textsc{out}}(\lambda^ {*})))\geq x^{*}_{\textsc{out}}\hat{g}_{\textsc{out}}-g_{\textsc{out}}(x^{*}_{ \textsc{out}})=0\]
* \(\lambda^{*}=\mu^{*}\frac{n}{\alpha}\) follows from the definition of \(\lambda^{*}\).

Thus, using (25) and (26) we show that for the chosen value of \(\lambda^{*}=\mu^{*}\frac{n}{\alpha}\), we have

\(\min_{(\textbf{r},\textbf{w})\in\mathcal{C}}F(\textbf{r},\textbf{w},\lambda^{ *})\geq 0\) implying from (13) that \(\text{opt}(\mathcal{E}_{\mu,\alpha})\geq 0\).

* Case 2: \(\hat{g}_{\textsc{mid}}-\frac{\alpha}{n}\leq\hat{g}_{\textsc{out}}+\frac{\alpha} {n}\)

Choosing \(\lambda^{*}=\mu^{*}\frac{n}{\alpha}\), we can prove \(\text{opt}(\mathcal{E}_{\mu,\alpha})\geq 0\) in a very similar manner whenever \(\mu=\mu^{*}\). 

## Appendix C Proof of Theorem 1

_Theorem_ (Restatement of Theorem 1).: For every \(\epsilon>0\) and \(m\geq 2\) and \(n\geq m^{2}\) we have

\[\textsc{dist}^{(g)}(\textsc{plu},n,m) \leq m(m-1)\left(\hat{g}_{\textsc{mid}}+\hat{g}_{\textsc{out}} \right)\exp\Big{(}\frac{-n^{(\frac{1}{2}+\epsilon)}+2m}{(2n^{(\frac{1}{2}- \epsilon)}-1)m}\Big{)}\] (27) \[+\max\bigg{(}\frac{m\hat{g}_{\textsc{mid}}}{(1-n^{-(\frac{1}{2}- \epsilon)})}-1,\frac{m\hat{g}_{\textsc{out}}}{(1-n^{-(\frac{1}{2}-\epsilon)})} +1\bigg{)}.\]

Further, \(\lim_{n\rightarrow\infty}\textsc{dist}^{(g)}(\textsc{plu},n,m)\leq\max \left(m\hat{g}_{\textsc{mid}}-1,m\hat{g}_{\textsc{out}}+1\right).\)

Proof.: Recall that candidate \(B\in\mathcal{A}\) minimises the social cost. The other candidates are denoted by \(\{A_{j}\}_{j\in[m-1]}\).

\[\textsc{dist}^{(g)}(\textsc{PLU},n,m)=\sup_{d\in\mathcal{M}(\mathcal{N}\cup \mathcal{A})}\left(\sum_{j=1}^{m-1}\mathbb{P}[A_{j}\text{ wins}]\frac{\text{SC}(A_{j},d)}{\text{SC}(B,d)}+ \mathbb{P}[B\text{ wins}]\right)\] (28)

For every \(j\in[m-1]\), we now bound the probability of \(A_{j}\) being the winner. This event implies that at least \(\frac{n}{m}\) voters choose \(A_{j}\) as the top preference, implying that the same voters rank \(A_{j}\) over \(B\). Further, we now define Bernoulli random variables \(\{Y_{i,j}\}_{i=1}^{n}\) each denoting the event that voter \(i\) ranks candidate \(A_{j}\) over \(B\). Recall from Equation 3, \(Y_{i,j}\sim\text{Bern}\left(g\left(\frac{d(i,B)}{d(i,A_{j})}\right)\right)\). Therefore,

\[\mathbb{P}[A_{j}\text{ wins}]\leq\mathbb{P}\left(\sum_{i=1}^{n}Y_{i,j}\geq \frac{n}{m}\right).\] (29)

Let \(\alpha_{j}\) be the expectation of the random variable \(\sum_{i=1}^{n}Y_{i,j}\) i.e. the expected number of voters ranking \(A_{j}\) over \(B\).

\[\alpha_{j}:=\sum_{i=1}^{n}\mathbb{E}[Y_{i,j}]=\sum_{i=1}^{n}g\left(\frac{d(i,B) }{d(i,A_{j})}\right)\text{ for every }j\in[m-1].\] (30)Now we use Chernoff bounds on the sum of Bernoulli random variable for every \(j\in[m-1]\) when \(\alpha_{j}\leq\frac{n}{m}-\frac{n^{(1/2+\epsilon)}}{m}\) to bound the probability of \(A_{j}\) being the winner.

If \(\alpha_{j}\leq\frac{n}{m}-\frac{n^{(1/2+\epsilon)}}{m}\) we have,

\[\mathbb{P}[A_{j}\text{ wins}]\leq\mathbb{P}\left(\sum_{i=1}^{n}Y_ {i,j}\geq\frac{n}{m}\right) =\mathbb{P}\left(\sum_{i=1}^{n}Y_{i,j}\geq\alpha_{j}\left(1+\frac{ n}{m\alpha_{j}}-1\right)\right)\] (31) \[\stackrel{{(a)}}{{\leq}}\left(\frac{e^{(\frac{n}{m \alpha_{j}}-1)}}{(\frac{n}{m\alpha_{j}})^{n/m\alpha_{j}}}\right)^{\alpha_{j}}\] (32) \[=\left(\frac{m\alpha_{j}}{n}\right)^{\frac{n}{m}}e^{\frac{n}{m}- \alpha_{j}}\] (33) \[\leq\frac{m\alpha_{j}}{n}\left(\frac{m\alpha_{j}}{n}\exp\left(- \frac{\alpha_{j}}{n/m-1}\right)\right)^{(\frac{n}{m}-1)}e^{\frac{n}{m}}\] (34) \[\stackrel{{(c)}}{{\leq}}\frac{m\alpha_{j}}{n}e^{ \frac{n}{m}}\left(\left(1-n^{-(\frac{1}{2}-\epsilon)}\right)\exp\left(-\frac{ \frac{n}{m}-\frac{n^{(\frac{1}{2}+\epsilon)}}{m}}{n/m-1}\right)\right)^{\frac{ n}{m}-1}\] (35) \[=\frac{m\alpha_{j}}{n}\left(1-n^{-(\frac{1}{2}-\epsilon)}\right)^ {(n/m-1)}\exp\left(\frac{n^{(\frac{1}{2}+\epsilon)}}{m}\right)\] (36) \[\stackrel{{(d)}}{{\leq}}\frac{m\alpha_{j}}{n}\exp \left(\frac{-2n^{-(\frac{1}{2}-\epsilon)}(n/m-1)}{2-n^{-(\frac{1}{2}-\epsilon) }}+\frac{n^{(\frac{1}{2}+\epsilon)}}{m}\right)\] (37) \[=\frac{m\alpha_{j}}{n}\exp\left(\frac{-n^{(\frac{1}{2}+\epsilon)} +2m}{(2n^{(\frac{1}{2}-\epsilon)}-1)m}\right).\] (38)

\((a)\) follows from applying the Chernoff bound. We restate the bound from [60] below.

Suppose \(X_{1},X_{2},\)\(\ldots,X_{n}\) be independent Bernoulli random variables with \(\mathbb{P}(X_{i})=\mu_{i}\) for every \(i\in[n]\) and \(\mu:=\sum_{i=1}^{n}\mu_{i}\), then we have

\[\mathbb{P}(\sum_{i}X_{i}\geq(1+\delta)\mu)\leq\left(\frac{e^{\delta}}{(1+ \delta)^{1+\delta}}\right)^{\mu}\] (39)

\((c)\) holds since \(xe^{-x}\) is increasing in \((0,1)\) and because \(\frac{\alpha}{n/m-1}\leq 1\) and \(\alpha\leq\frac{n}{m}-\frac{n^{(\frac{1}{2}+\epsilon)}}{m}\), the maxima is attained at \(\alpha=\frac{n}{m}-\frac{n^{(\frac{1}{2}+\epsilon)}}{m}\). \((d)\) holds since \(\log(1+x)\leq\frac{2x}{2+x}\) for \(-1<x\leq 0\).

Let \(S:=\{j\in[m-1]:\alpha_{j}<\frac{n}{m}-\frac{n^{(1/2+\epsilon)}}{m}\}\) i.e. \(S\) denotes the indices of candidates with \(\alpha_{j}\) less than \(\frac{n}{m}-\frac{n^{(1/2+\epsilon)}}{m}\). Now using Lemma 2 and \(\alpha_{j}\geq\frac{n}{m}-\frac{n^{(1/2+\epsilon)}}{m}\) for every \(j\in[m-1]\setminus S\), we have

\[\frac{SC(A_{j},d)}{SC(B,d)}\leq\max\left(\frac{m\hat{g}_{\textsc{MID}}}{(1-n^ {-(1/2-\epsilon)})}-1,\frac{m\hat{g}_{\textsc{OUT}}}{(1-n^{-(1/2-\epsilon)}) }+1\right)\] (40)

We now have

\[=\sup_{d\in\mathcal{M}(\mathcal{N}\cup\mathcal{A})}\Biggl{(}\sum_{ j\in[m-1]\setminus S}\left(\mathbb{P}[A_{j}\text{ wins}]\frac{\text{SC}(A_{j},d)}{\text{SC}(B,d)}\right)+\mathbb{P}[B\text{ wins}]+\sum_{j\in S}\left(\mathbb{P}[A_{j}\text{ wins}]\frac{\text{SC}(A_{j},d)}{\text{SC}(B,d)}\right)\Biggr{)}\] \[\stackrel{{(a)}}{{\leq}}\max\left(\max_{j\in[m-1] \setminus S}\frac{SC(A_{j},d)}{SC(B,d)},1\right)+\sum_{j\in S}\Biggl{(}\max \left(\frac{n}{\alpha_{j}}\hat{g}_{\textsc{MID}}-1,\frac{n}{\alpha_{j}}\hat{g} _{\textsc{OUT}}+1\right)\frac{m\alpha_{j}}{n}\exp\left(\frac{-n^{(\frac{1}{2}+ \epsilon)}+2m}{(2n^{(\frac{1}{2}-\epsilon)}-1)m}\right)\Biggr{)}\] \[\stackrel{{(b)}}{{\leq}}m(m-1)\left(\hat{g}_{ \textsc{MID}}+\hat{g}_{\textsc{OUT}}\right)\exp\left(\frac{-n^{(\frac{1}{2}+ \epsilon)}+2m}{(2n^{(\frac{1}{2}-\epsilon)}-1)m}\right)+\max\left(\frac{m\hat{g} _{\textsc{MID}}}{(1-n^{-(1/2-\epsilon)})}-1,\frac{m\hat{g}_{\textsc{OUT}}}{ (1-n^{-(1/2-\epsilon)})}+1\right).\]\((a)\) follows from the following observations.

* Apply Lemma 2 to bound \(\frac{SC(A_{j},d)}{SC(B,d)}\). Since \(\alpha_{j}\leq\frac{n}{m}-\frac{n^{(1/2+\epsilon)}}{m}\ \forall j\in S\), apply Equation (31) to bound \(\mathbb{P}\left[A_{j}\text{ wins}\right]\).
* \(\sum\limits_{j\in[m-1]\setminus S}\left(\mathbb{P}[A_{j}\text{ wins}]\frac{ \text{SC}(A_{j},d)}{\text{SC}(B,d)}\right)+\mathbb{P}[B\text{ wins}]\leq\max \left(\max\limits_{j\in[m-1]\setminus S}\frac{SC(A_{j},d)}{SC(B,d)},1\right).\)

\((b)\) follows from the fact that \(|S|\leq m-1\), \(\max(a,b)\leq a+b\), and applying Equation (40). 

## Appendix D Proof of Theorem 2

_Theorem_ (Restatement of Theorem 2).: For every \(m\geq 2,\)\(\lim_{n\to\infty}\textsc{dist}^{(g)}(\textsc{plu},n,m)\geq\max\left(m\hat{g}_ {\textsc{mhd}}-1,m\hat{g}_{\textsc{out}}+1\right).\)

Proof.: The proof is by an example in an Euclidean metric space in \(\mathbb{R}^{3}\). One candidate "C" is at \((1,0,0)\). The other \(m-1\) candidates are "good" and are equidistantly placed on a circle of radius \(\epsilon\) on the \(y-z\) plane centred at \((0,0,0)\). We call them \(\mathcal{G}:=\{G_{1},G_{2},\ldots,G_{m-1}\}\).

We present two constructions below for every \(\epsilon,\zeta>0\).

_Construction 1_: Let \(q_{\textsc{mhd}}:=g\Big{(}\frac{\sqrt{(x_{\textsc{mhd}}^{*})^{2}+\epsilon^{ 2}}}{1-x_{\textsc{mhd}}^{*}}\Big{)}\) and \(a_{\textsc{mhd}}:=\frac{1}{m-1}\Big{(}1-\frac{1+\zeta}{m_{\textsc{mhd}}}\Big{)}\). Each of the \(m-1\) candidates in \(\mathcal{G}\) has \(\lfloor a_{\textsc{mhd}}n\rfloor\) voters overlapping with it. The remaining voters (we call them "ambivalent") are placed at \((x_{\textsc{mhd}}^{*},0,0)\). Clearly, each voter overlapping with a candidate votes for it as the most preferred candidate with probability one. Each of the ambivalent voters votes as follows.

- With probability \(q_{\textsc{mhd}}\), vote for candidate \(C\) as the top choice and uniformly randomly permute the other candidates in the rest of the vote.

- With probability \(1-q_{\textsc{mhd}}\), vote for candidate \(C\) as the last choice and uniformly randomly permute the other candidates in the rest of the vote.

Observe that this satisfies the pairwise probability criterion in Equation 3. Since \(\lim_{n\to\infty}\lfloor an\rfloor/n=a\) and that the distance of a candidate in \(\mathcal{G}\) from any non-ambivalent voter is at most \(2\epsilon\), we have that for every \(j\in[m-1]\),

\[\lim_{n\to\infty}\frac{SC(C,d)}{SC(G_{j},d)}\geq \frac{(1-x_{\textsc{mhd}}^{*})(1-(m-1)a_{\textsc{mhd}})+(m-1)a_{ \textsc{mhd}}\sqrt{1+\epsilon^{2}}}{(1-(m-1)a_{\textsc{mhd}})\sqrt{(x_{ \textsc{mhd}}^{*})^{2}+\epsilon^{2}}+2(m-2)a_{\textsc{mhd}}\epsilon}\] (41) \[=\frac{(mq_{\textsc{mhd}}-(1+\zeta))\sqrt{1+\epsilon^{2}}+(1+ \zeta)(1-x_{\textsc{mhd}}^{*})}{(1+\zeta)\sqrt{(x_{\textsc{mhd}}^{*})^{2}+ \epsilon^{2}}+2(m-2)a_{\textsc{mhd}}\epsilon}.\] (42)

Clearly every candidate in \(\mathcal{G}\) minimises the social cost and now we show that \(\lim_{n\to\infty}\mathbb{P}[C\text{ wins}]=1\).

Let Bernoulli random variables \(\{Y_{i}\}_{i=1}^{n}\) denote the events that voter \(i\in\mathcal{N}\) ranks candidate \(C\) at the top. Here, \(\sum_{i=1}^{n}\mathbb{P}[Y_{i}=1]=q_{\textsc{mhd}}(n-(m-1)\lfloor a_{\textsc{ mhd}}n\rfloor)\) and thus

\[\lim_{n\to\infty}\frac{\sum\limits_{i=1}^{n}\mathbb{P}[Y_{i}=1]}{n}=\frac{1+ \zeta}{m}.\]

By the law of large numbers, we have that \(\mathbb{P}[\sum_{i}Y_{i}\geq\frac{n}{m}]=1\) as \(n\to\infty\). Since every candidate in \(\mathcal{G}\) is equally likely to win, the event \(\sum_{i}Y_{i}\geq\frac{n}{m}\) implies the event that \(C\) is the winner and thus, \(\lim_{n\to\infty}\mathbb{P}[C\text{ wins}]=1\). Thus,

\[\lim_{n\to\infty}\textsc{dist}^{(g)}(\textsc{PLU},n,m)\geq\frac{(mq_{\textsc{ mhd}}-(1+\zeta))\sqrt{1+\epsilon^{2}}+(1+\zeta)(1-x_{\textsc{mhd}}^{*})}{(1+ \zeta)\sqrt{(x_{\textsc{mhd}}^{*})^{2}+\epsilon^{2}}+2(m-2)a_{\textsc{mhd}} \epsilon}.\] (43)

_Construction 2_: Let \(q_{\textsc{out}}:=g\left(\frac{\sqrt{(x_{\textsc{out}}^{*})^{2}+\epsilon^{2}} }{1+x_{\textsc{out}}^{*}}\right)\) and \(a_{\textsc{out}}:=\frac{1}{m-1}\left(1-\frac{1+\zeta}{m_{\textsc{mhd}}\epsilon}\right)\). Each candidate in \(\mathcal{G}\) has \(\lfloor a_{\textsc{out}}n\rfloor\) voters overlapping with it, and the remaining "ambivalent" voters are at \((-x_{\textsc{out}}^{*},0,0)\).

Clearly, each voter overlapping with a candidate votes for it as the most preferred candidate with probability one. Each of the ambivalent voters votes as follows.

* With probability \(q_{\textsc{out}}\), vote for candidate \(C\) as the top choice and uniformly randomly permute the other candidates in the rest of the vote.
* With probability \(1-q_{\textsc{out}}\), vote for candidate \(C\) as the last choice and uniformly randomly permute the other candidates in the rest of the vote.

This satisfies the pairwise probability criterion in Equation 3. For every \(j\in[m-1]\),

\[\lim_{n\to\infty}\frac{SC(C,d)}{SC(G_{j},d)}\geq \frac{(1+x_{\textsc{out}}^{*})(1-(m-1)a_{\textsc{out}})+(m-1)a_{ \textsc{out}}\sqrt{1+\epsilon^{2}}}{(1-(m-1)a_{\textsc{out}})\sqrt{(x_{ \textsc{out}}^{*})^{2}+\epsilon^{2}}+2(m-2)a_{\textsc{out}}\epsilon}\] (44) \[=\frac{(1+\zeta)(1+x_{\textsc{out}}^{*})+(mq_{\textsc{out}}-(1+ \zeta))\sqrt{1+\epsilon^{2}}}{(1+\zeta)\sqrt{(x_{\textsc{out}}^{*})^{2}+ \epsilon^{2}}+2(m-2)a_{\textsc{out}}\epsilon}.\] (45)

Clearly, every candidate in \(\mathcal{G}\) minimises the social cost. Now, we show that \(\lim_{n\to\infty}\mathbb{P}[C\text{ wins}]=1\).

LEt Bernoulli random variables \(\{Y_{i}\}_{i=1}^{n}\) denote the events that voter \(i\in\mathcal{N}\) ranks candidate \(C\) at the top. We have \(\sum_{i=1}^{n}\mathbb{P}[Y_{i}=1]=q_{\textsc{mid}}(n-(m-1)\lfloor an\rfloor)\) and thus, \(\lim_{n\to\infty}\frac{\sum_{i=1}^{n}\mathbb{P}[Y_{i}=1]}{n}=\frac{1+\zeta}{m}\). Applying the law of large numbers, we get that \(\mathbb{P}[\sum\limits_{i}Y_{i}\geq\frac{n}{m}]=1\) as \(n\) tends to \(\infty\). However since every candidate in \(\mathcal{G}\) is equally likely to win, the event \(\sum_{i}Y_{i}\geq\frac{n}{m}\) corresponds to the event that \(C\) is the winner and thus, \(\lim_{n\to\infty}\mathbb{P}[C\text{ wins}]=1\). Therefore we have,

\[\lim_{n\to\infty}\textsc{dist}^{(g)}(\text{PLU},n,m)\geq\frac{(mq_{\textsc{ out}}-(1+\zeta))\sqrt{1+\epsilon^{2}}+(1+\zeta)(1+x_{\textsc{out}}^{*})}{(1+ \zeta)\sqrt{(x_{\textsc{out}}^{*})^{2}+\epsilon^{2}}+2(m-2)a_{\textsc{out}} \epsilon}.\] (46)

On applying the limit \(\epsilon,\zeta\to 0\) and substituting for \(q_{\textsc{mid}}\) and \(q_{\textsc{out}}\), we get the desired lower bound by combining the results from the two constructions. 

## Appendix E Proof of Theorem 3

**Theorem 7**.: _Restatement of Theorem 3 For every \(\epsilon>0,m\geq 2\) and \(n\geq 4\), we have_

\[\textsc{dist}^{(g)}(\textsc{cop},n,m) \leq 4m(m-1)\exp\Big{(}\frac{-n^{(\frac{1}{2}+\epsilon)}+8}{2(2n^{ (\frac{1}{2}-\epsilon)}-1)}\Big{)}\left(\hat{g}_{\textsc{mid}}+\hat{g}_{ \textsc{out}}\right)^{2}\] \[+\max\Bigl{(}\Bigl{(}\frac{2\hat{g}_{\textsc{mid}}}{1-n^{-(\frac{ 1}{2}-\epsilon)}}-1\Bigr{)}^{2},\Bigl{(}\frac{2\hat{g}_{\textsc{out}}}{1-n^{-( \frac{1}{2}-\epsilon)}}+1\Bigr{)}^{2}\Bigr{)}.\]

_For every \(m\geq 2\), we have \(\lim_{n\to\infty}\textsc{dist}^{(g)}(\textsc{cop},n,m)\leq\max\big{(}\left(2 \hat{g}_{\textsc{mid}}-1\right)^{2},\left(2\hat{g}_{\textsc{out}}+1\right)^{2 }\big{)}.\)_

Proof.: Recall that \(B\in\mathcal{A}\) minimises the social cost, and \(\{A_{j}\}_{j\in[m-1]}\) denotes the set \(\mathcal{A}\setminus B\).

\[\textsc{dist}^{(g)}(\textsc{cop},n,m)=\sup_{d\in\mathcal{M}(\mathcal{N} \cup\mathcal{A})}\left(\sum_{j=1}^{m-1}\mathbb{P}[A_{j}\text{ wins}]\frac{\text{SC}(A_{j},d)}{\text{SC}(B,d)}+\mathbb{P}[B \text{ wins}]\right)\] (47)

Consider a Copeland winner \(W\). As noted by prior work [1], \(W\) must be in the uncovered set of the tournament graph, and one of the following two cases must be true.

* \(W\) defeats \(B\).
* There exists a candidate \(Y\in\mathcal{A}\) s.t. \(W\) defeats \(Y\) and \(Y\) defeats \(B\).

For every \(j\in[m-1]\), we now bound the probability of \(A_{j}\) being the winner. For every \(j\in[m-1]\), we define Bernoulli random variables \(\{Y_{i,j}\}_{i=1}^{n}\) denoting the event that voter \(i\) ranks candidate \(A_{j}\) over candidate \(B\). From Equation 3, we have that \(Y_{i,j}\sim\text{Bern}\left(g\left(\frac{d(i,A_{j})}{d(i,B)}\right)\right)\). For every distinct \(j,k\in[m-1]\), we define Bernoulli random variables \(\{Z_{i,j,k}\}_{i=1}^{n}\) denoting the event that voter \(i\) ranks candidate \(A_{j}\) over \(A_{k}\). \(Z_{i,j,k}\sim\text{Bern}(g\left(\frac{d(i,A_{k})}{d(i,A_{j})}\right))\).

Observe that

\[\mathbb{P}[A_{j}\text{ wins}]\leq\mathbb{P}\left(\sum_{i=1}^{n}Y_{i,j}\geq\frac{n }{2}\bigcup_{k\in[m-1]\setminus\{j\}}\left(\sum_{i=1}^{n}Z_{i,j,k}\geq\frac{n} {2}\cap\sum_{i=1}^{n}Y_{i,k}\geq\frac{n}{2}\right)\right).\] (48)

Let \(\alpha_{j}\) denote the expected value of the random variable \(\sum_{i=1}^{n}Y_{i,j}\), i.e., the expected number of voters who rank candidate \(A_{j}\) over \(B\).

\[\alpha_{j}:=\sum_{i=1}^{n}\mathbb{E}[Y_{i,j}]=\sum_{i=1}^{n}g\left(\frac{d(i,B )}{d(i,A_{j})}\right)\text{ for every }j\in[m-1].\] (49)

Let \(\beta_{j,k}\) denote the expected value of the random variable \(\sum_{i=1}^{n}Z_{i,j,k}\), i.e., the expected number of voters who rank candidate \(A_{j}\) over \(A_{k}\).

\[\beta_{j,k}:=\sum_{i=1}^{n}\mathbb{E}[Z_{i,j,k}]=\sum_{i=1}^{n}g\left(\frac{d( i,A_{k})}{d(i,A_{j})}\right)\text{ for every }j\in[m-1].\] (50)

Similar to Equation (31), we have the following bound:

\[\text{If }\alpha_{j}\leq\frac{n}{2}-\frac{n^{(1/2+\epsilon)}}{2}, \text{ we have}\] \[\mathbb{P}\left(\sum_{i=1}^{n}Y_{i,j}\geq\frac{n}{2}\right) =\mathbb{P}\left(\sum_{i=1}^{n}Y_{i,j}\geq\alpha_{j}\left(1+\frac {n}{2\alpha_{j}}-1\right)\right)\] (51) \[\overset{(a)}{\leq}\left(\frac{e^{(\frac{n}{2\alpha_{j}}-1)}}{( \frac{n}{2\alpha_{j}})^{n/2\alpha_{j}}}\right)^{\alpha_{j}}\] (52) \[\leq\left(\frac{2\alpha_{j}}{n}\right)^{2}\left(\frac{m\alpha_{j} }{n}\exp\left(-\frac{\alpha_{j}}{n/2-2}\right)\right)^{(\frac{n}{2}-2)}e^{ \frac{n}{m}}\] (53) \[\overset{(c)}{\leq}\left(\frac{2\alpha_{j}}{n}\right)^{2}e^{ \frac{n}{2}}\left(\left(1-n^{-(\frac{1}{2}-\epsilon)}\right)\exp\left(-\frac{ \frac{n}{2}-\frac{n^{(\frac{1}{2}+\epsilon)}}{2}}{n/2-2}\right)\right)^{\frac{ n}{2}-2}\] (54) \[=\left(\frac{2\alpha_{j}}{n}\right)^{2}\left(1-n^{-(\frac{1}{2}- \epsilon)}\right)^{(n/2-2)}\exp\left(\frac{n^{(\frac{1}{2}+\epsilon)}}{2}\right)\] (55) \[\overset{(d)}{\leq}\left(\frac{2\alpha_{j}}{n}\right)^{2}\exp \left(\frac{-2n^{-(\frac{1}{2}-\epsilon)}(n/2-2)}{2-n^{-(\frac{1}{2}-\epsilon) }}+\frac{n^{(\frac{1}{2}+\epsilon)}}{2}\right)\] (56) \[=\left(\frac{2\alpha_{j}}{n}\right)^{2}\exp\left(\frac{-n^{(\frac {1}{2}+\epsilon)}+8}{(2n^{(\frac{1}{2}-\epsilon)}-1)2}\right)\] (57)

From Equation (31) in the proof of Theorem 1, we have

\[\mathbb{P}\left(\sum_{i=1}^{n}Y_{i,j}\geq\frac{n}{2}\right)\leq \left(\frac{2\alpha_{j}}{n}\right)^{2}\exp\left(\frac{-n^{(\frac{1}{2}+ \epsilon)}+8}{2(2n^{(\frac{1}{2}-\epsilon)}-1)}\right)\text{ if }\alpha_{j}\leq\frac{n}{2}-\frac{n^{(1/2+ \epsilon)}}{2}.\] (58) \[\text{Similarly, }\mathbb{P}\left(\sum_{i=1}^{n}Z_{i,j,k}\geq\frac{n }{2}\right)\leq\left(\frac{2\beta_{j,k}}{n}\right)^{2}\exp\left(\frac{-n^{( \frac{1}{2}+\epsilon)}+8}{2(2n^{(\frac{1}{2}-\epsilon)}-1)}\right)\text{ if }\beta_{j,k}\leq\frac{n}{2}-\frac{n^{(1/2+ \epsilon)}}{2}.\] (59)

Consider two exhaustive cases on candidate \(A_{j}\) and define an event \(E_{j}\) for every \(j\in[m-1]\). We compute the expected fraction of votes on pairwise comparisons. The event \(E_{j}\) denotes the existence of an at-most two hop directed path from a candidate \(A_{j}\) to candidate \(B\) for Copeland such that the expected fraction of votes on all edges along that path exceed \(\frac{n}{2}-\frac{n^{(1/2+\epsilon)}}{2}\). Recall that we only considered one hop path for the case of plu in the proof of Theorem 1.

[MISSING_PAGE_FAIL:21]

Recall that for every \(j\in[m-1]\setminus S\), \(E_{j}\) is satisfied. Let us further denote

\[\hat{E}_{j}:=\alpha_{j}\geq\frac{n}{2}-\frac{n^{(1/2+\epsilon)}}{2}\text{ and }\hat{D}_{j,k}:=\left(\beta_{j,k}\geq\frac{n}{2}-\frac{n^{(1/2+\epsilon)}}{2} \right).\]

Observe that \(E_{j}\) being satisfied implies either a) \(\hat{E}_{j}\) is satisfied or b) \(\exists k\in[m-1]\setminus\{j\}\) s.t \(\hat{E}_{k}\) and \(\hat{D}_{j,k}\) are satisfied. We consider both cases separately.

Suppose \(\hat{E}_{j}\) is satisfied for some \(j\in[m-1]\setminus S\). Then we have from Lemma 2,

\[\frac{SC(A_{j},d)}{SC(B,d)}\leq\max\left(\frac{2\hat{g}_{\text{ MID}}}{(1-n^{-(1/2-\epsilon)})}-1,\frac{2\hat{g}_{\text{OUT}}}{(1-n^{-(1/2- \epsilon)})}+1\right).\] (64)

Now we consider case (b) where \(\hat{E}_{k}\) and \(\hat{D}_{j,k}\) are both satisfied for some \(k\in[m-1]\setminus\{j\}\). From Lemma 2 we have,

\[\frac{SC(A_{j},d)}{SC(B,d)}\leq\max\left(\left(\frac{2\hat{g}_{ \text{MID}}}{(1-n^{-(1/2-\epsilon)})}-1\right)^{2},\left(\frac{2\hat{g}_{\text {OUT}}}{(1-n^{-(1/2-\epsilon)})}+1\right)^{2}\right).\] (65)

Now combining Equations (63), (64), and (65), we have for any metric space \(d\in\mathcal{M}(\mathcal{N}\cup\mathcal{A})\),

\[\text{dist}^{(g)}(\text{cop},n,m)\leq\left(\sum_{j\in S}\left( \mathbb{P}[A_{j}\text{ wins}]\frac{\text{SC}(A_{j},d)}{\text{SC}(B,d)}\right)+ \mathbb{P}[B\text{ wins}]+\sum_{j\in[m-1]\setminus S}\left(\mathbb{P}[A_{j} \text{ wins}]\frac{\text{SC}(A_{j},d)}{\text{SC}(B,d)}\right)\right)\] \[\overset{(a)}{\leq}4(m-1)m\exp\left(\frac{-n^{(\frac{1}{2}+ \epsilon)}+8}{2(2n^{(\frac{1}{2}-\epsilon)}-1)}\right)(\hat{g}_{\text{MID}}+ \hat{g}_{\text{OUT}})+\max\left(\max_{j\in[m-1]\setminus S}\frac{SC(A_{j},d)}{ SC(B,d)},1\right)\] \[\overset{(b)}{\leq}4(m-1)m\exp\left(\frac{-n^{(\frac{1}{2}+ \epsilon)}+8}{2(2n^{(\frac{1}{2}-\epsilon)}-1)}\right)(\hat{g}_{\text{MID}}+ \hat{g}_{\text{OUT}})+\max\Big{(}\Big{(}\frac{2\hat{g}_{\text{MID}}}{(1-n^{-( 1/2-\epsilon)})}-1\Big{)}^{2},\Big{(}\frac{2\hat{g}_{\text{OUT}}}{(1-n^{-(1/2- \epsilon)})}+1\Big{)}^{2}\Big{)}.\]

\((a)\) follows from Equation (61) and the fact that \(\sum_{j\in S}\left(\mathbb{P}[A_{j}\text{ wins}]\frac{\text{SC}(A_{j},d)}{ \text{SC}(B,d)}\right)+\mathbb{P}[B\text{ wins}]\leq\max\left(\max_{j\in S} \frac{SC(A_{j},d)}{SC(B,d)},1\right)\). \((b)\) follows from combining Equations (63), (64), and (65). 

## Appendix F Proof of Theorem 4

_Theorem_ (Restatement of Theorem 4).: \(\text{dist}^{(g)}(\text{RD},m,n)\leq(m-1)\hat{g}_{\text{MID}}+1\).

Proof.: The probability of voter \(i\) voting for candidate \(W\) as its top candidate is upper bounded by \(g\left(\frac{d(i,B)}{d(i,W)}\right)\) which is the probability that \(W\) is ranked over \(B\). Therefore, under RD, the probability of \(W\) winning satisfies:

\[\mathbb{P}[W\text{ wins}]\leq\frac{1}{n}\left(\sum_{i=1}^{n}g\left(\frac{d(i,B )}{d(i,W)}\right)\right).\] (66)

Recall that we define the set of candidates in \(\mathcal{A}\setminus B\) as \(\{A_{1},A_{2},\ldots,A_{m-1}\}\). In the rest of the analysis we denote \(d(i,A_{j})\) by \(y_{i,j}\) (for all \(j\in[m-1]\)) and \(d(i,B)\) by \(b_{i}\) for every \(i\in[n]\). We also denote \(d(B,A_{j})\) by \(z_{j}\) for every \(j\in[m-1]\). Now for every metric \(d\), we bound the distortion as follows.

\[\text{dist}^{(g)}(\text{RD},m,n) \leq\sum_{j=1}^{m-1}\left(\mathbb{P}[A_{j}\text{ wins}]\frac{\sum_{i=1}^{n}y_{i,j}}{\sum_{i=1}^{n}b_{i}}\right)+(1-\sum_{j=1}^{m-1 }\mathbb{P}[A_{j}\text{ wins}])\] (67) \[=\sum_{j=1}^{m-1}\mathbb{P}[A_{j}\text{ wins}]\left(\frac{\sum_{i= 1}^{n}y_{i,j}}{\sum_{i=1}^{n}b_{i}}-1\right)+1\] (68) \[\overset{(a)}{\leq}\sum_{j=1}^{m-1}\frac{1}{n}\left(\sum_{i=1}^{n }g\left(\frac{b_{i}}{y_{i,j}}\right)\right)\frac{\sum_{i=1}^{n}(y_{i,j}-b_{i}) }{\sum_{i=1}^{n}b_{i}}+1\] (69) \[\leq\sum_{j=1}^{m-1}\frac{1}{n}\left(\sum_{i=1}^{n}g\left(\frac{b _{i}/z_{j}}{y_{i,j}/z_{j}}\right)\right)\frac{\sum_{i=1}^{n}(y_{i,j}/z_{j}-b_{ i}/z_{j})}{\sum_{i=1}^{n}b_{i}/z_{j}}+1\] (70) \[\overset{(d)}{\leq}\sum_{j=1}^{m-1}\frac{\left(\sum_{i=1}^{n}g \left(\frac{b_{i}/z_{j}}{y_{i,j}/z_{j}}\right)\right)}{\sum_{i=1}^{n}b_{i}/z_{ j}}+1\] (71) \[\overset{(e)}{\leq}(m-1)\frac{g\left(\frac{x_{\text{min}}^{*}}{ -x_{\text{min}}^{*}}\right)}{x_{\text{MD}}^{*}}+1=(m-1)\hat{g}_{\text{MD}}+1.\] (72)

\((a)\) follows from Equation (66).

\((d)\) follows from the fact that \(y_{i,j}-b_{i}\leq z_{j}\) which follows from triangle inequality.

\((e)\) follows from the following arguments by considering two cases namely \(\frac{b_{i}}{z_{j}}\leq 1\) and \(\frac{b_{i}}{z_{j}}\geq 1\).

When \(\frac{b_{i}}{z_{j}}\leq 1\) and thus, \(\frac{y_{i,j}}{z_{j}}\geq 1-\frac{b_{i}}{z_{j}}\) from triangle inequality. Similarly, we have \(\frac{y_{i,j}}{z_{j}}\geq\frac{b_{i}}{z_{j}}-1\) when \(\frac{b_{i}}{z_{j}}\geq 1\). Thus,

\[\frac{g\left(\frac{b_{i}/z_{j}}{y_{i,j}/z_{j}}\right)}{b_{i}/z_{j }}\leq\max\left(\sup_{x\in(0,1)}\frac{g(\frac{x}{1-x})}{x},\sup_{x\in(1,\infty )}\frac{g(\frac{x}{x-1})}{x}\right)\text{ for every }i\in[n]\] (73) \[\implies\frac{\sum_{i=1}^{n}g\left(\frac{b_{i}/z_{j}}{y_{i,j}/z_{ j}}\right)}{\sum_{i=1}^{n}b_{i}/z_{j}}\leq\max\left(\frac{g\left(\frac{x_{\text{ min}}^{*}}{1-x_{\text{min}}^{*}}\right)}{x_{\text{MD}}^{*}},1\right).\] (74)

The last inequality follows from the fact that \(\frac{g(\frac{x}{x-1})}{x}\leq 1\) when \(x\geq 1\). Further, we have \(\hat{g}_{\text{MD}}\geq 1\) for all valid \(g\). 

## Appendix G Proof of Theorem 6

_Theorem_ (Restatement of Theorem 6).: Let \(\text{dist}_{PL}^{\theta}(\text{RD},m,n)\) denote the distortion when the voters' rankings are generated per the PL model with parameter \(\theta\). We have \(\lim_{n\to\infty}\text{dist}_{PL}^{\theta}(\text{RD},m,n)\geq 1+\frac{(m-1)^{1/ \theta}}{2}\).

Proof.: We have a 1-D Euclidean construction. Let \(B\) be at \(0\) and all other candidates \(\mathcal{A}\setminus\{B\}\) be at 1. \(m-1\) voters are at \(0\), and one voter is at \(t\). We will set \(t\) later by optimizing for the distortion.

The distortion for this instance is \(\mathbb{P}[B\text{ wins}]\cdot 1+\mathbb{P}[B\text{ loses}]\cdot\frac{n-t}{t}=\frac{n-1}{n}+\frac{1}{n} \frac{t^{-\theta}}{t^{-\theta}+(m-1)(1-t)^{-\theta}}+\frac{1}{t^{-\theta}+(m-1 )(1-t)^{-\theta}}\frac{n-t}{t}\). We drop the terms which are \(O(1/n)\) to obtain \(1+\frac{(m-1)(1-t)^{-\theta}}{t(t^{-\theta}+(m-1)(1-t)^{-\theta})}\). This simplifies to \(1+\frac{(m-1)t^{\theta-1}}{(1-t)^{\theta}+(m-1)t^{\theta}}\). This is lower bounded by \(1+\frac{(m-1)t^{\theta-1}}{1+(m-1)t^{\theta}}\). Setting \(t=(m-1)^{-1/\theta},\) we obtain a distortion lower bound of \(1+\frac{(m-1)^{1/\theta}}{2}\).

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: We took care to make sure the claims made in the abstract and introduction accurately reflect the paper's contributions and scope. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We have added a future work section which lays the open questions and limitations. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: All assumptions are mentioned clearly. All the proofs are provided, and we took care to make them correct to the best of our understanding. Guidelines:* The answer NA means that the paper does not include theoretical results.
* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.

4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [NA] Justification: [NA] Guidelines:

* The answer NA means that the paper does not include experiments.
* If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.
* If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.
* Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.
* While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.

5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [NA] Justification: [NA] Guidelines:

* The answer NA means that paper does not include experiments requiring code.
* Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).

* The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [NA] Justification: [NA] Guidelines:

* The answer NA means that the paper does not include experiments.
* The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.
* The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [NA] Justification: [NA] Guidelines:

* The answer NA means that the paper does not include experiments.
* The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.
* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [NA] Justification: [NA] Guidelines:

* The answer NA means that the paper does not include experiments.

* The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.
* The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.
* The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: [NA] Guidelines:
* The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
* If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.
* The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: We have discussed the positive social impact of the design of voting rules and ways in which our paper can be instrumental towards it. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. * The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. * The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. * If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: [NA] Guidelines: * The answer NA means that the paper poses no such risks.

* Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.
* Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.
* We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
* **Licensees for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [NA] Justification: [NA] Guidelines:
* The answer NA means that the paper does not use existing assets.
* The authors should cite the original paper that produced the code package or dataset.
* The authors should state which version of the asset is used and, if possible, include a URL.
* The name of the license (e.g., CC-BY 4.0) should be included for each asset.
* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: [NA] Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: [NA] Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects**Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained?

Answer:[NA]

Justification: [NA]

Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.