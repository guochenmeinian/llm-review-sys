Semi-Truths: A Large-Scale Dataset of AI-Augmented Images for Evaluating Robustness of AI-Generated Image detectors

Anisha Pal\({}^{1}\)1 **Julia Kruk\({}^{1}\)**1 **Mansi Phute\({}^{1}\)**Manognya Bhattaram\({}^{1}\)**

**Diyi Yang\({}^{2}\)**Duen Horng Chau\({}^{1}\)**Judy Hoffman\({}^{1}\)**

Footnote 1: Equal contribution.

\({}^{1}\)Georgia Institute of Technology, \({}^{2}\)Stanford University

{apa172, jkruk3, mphute6, polo, judy}@gatech.edu

{manognya.work}@gmail.com, {diyiy}@stanford.edu

https://huggingface.co/datasets/semi-truths/Semi-Truths

## 1 Abstract

Text-to-image diffusion models have impactful applications in art, design, and entertainment, yet these technologies also pose significant risks by enabling the creation and dissemination of misinformation. Although recent advancements have produced AI-generated image detectors that claim robustness against various augmentations, their true effectiveness remains uncertain. Do these detectors reliably identify images with different levels of augmentation? Are they biased toward specific scenes or data distributions? To investigate, we introduce Semi-Truths, featuring \(27,600\) real images, \(223,400\) masks, and \(1,329,155\) AI-augmented images that feature targeted and localized perturbations produced using diverse augmentation techniques, diffusion models, and data distributions. Each augmented image is accompanied by metadata for standardized and targeted evaluation of detector robustness. Our findings suggest that state-of-the-art detectors exhibit varying sensitivities to the types and degrees of perturbations, data distributions, and augmentation methods used, offering new insights into their performance and limitations. The code for the augmentation and evaluation pipeline is available at https://github.com/J-Kruk/SemiTruths.

Figure 1: **Semi-Truths image augmentations that are measured by the size of the augmented region (Area Ratio) and the semantic change achieved (Semantic Magnitude), categorized into \(3\) levels - small (col1), medium (col2), and large (col3).**

[MISSING_PAGE_FAIL:2]

Finally, we demonstrate how the knowledge abstractions in Semi-Truths can be used to identify the sensitivities of existing detectors. By stress-testing \(6\) models, we reveal unique sensitivities to different data distributions, diffusion models, and perturbation degrees. Our goal is to offer a resource for targeted, interpretable, and standardized evaluation of AI-generated image detection systems, and to provide a customizable evaluation pipeline for the community.

## 2 Related Work

AI Augmented Image datasetThe field of AI-based image generation and perturbation has rapidly evolved from autoencoders [20] and graphics-based techniques [81] to GANs [101; 58; 2; 49; 6] and, more recently, diffusion models [57; 70; 61; 24]. These advancements have heightened ethical concerns regarding identity theft and misinformation, [3; 27; 31] necessitating robust datasets for AI-generated image detection. While most research has focused on GAN-generated human faces [16; 71; 47; 39; 14], there is a growing emphasis on diffusion-based techniques for detection of deepfakes [76], digital forgery [75] and generic AI-generated content [102; 5; 83; 90]. However, existing datasets face several limitations that restrict their applicability as a benchmark for developing robust detection systems. They often come from a single model [83; 90] or source data distribution [102; 5], lack detailed generation and image metadata [10], and provide limited control over degree and quality of perturbations [83; 90; 102; 5; 76; 10; 66]. Furthermore, they do not offer scalable pipelines for integrating future image generation and perturbation techniques and are limited in their analysis of detection methods. Recognizing these gaps, we introduce Semi-Truths that incorporates multiple model variations, perturbing techniques, and source data distributions, provides comprehensive metadata, and offers fine-grained control over the quality and degree of perturbations (Tab. 1 summarizes Semi-Truths's contributions).

Image editing pipelinesWith the advent of diffusion models, the field of image editing has seen tremendous advancements [33]. Recent developments in image inpainting, both in text-conditioned [92; 93; 87; 97] and unconditioned [51] settings, have enabled fine-grained control over image editing significantly enhancing precision and quality. While image inpainting requires the use of masks, prompt-based image editing [28; 54] performs targeted perturbations conditioned solely on text prompts. Existing frameworks like LANCE [62] and InstructPix2Pix [7] leverage this capability to develop automated image perturbation pipelines. LANCE [62], leveraging large language models (LLMs)[82] and image captioning[45], enables human-supervision-free image editing across diverse perturbations. Building on this, we extend LANCE [62] to handle a broader range of perturbation magnitudes, guided by semantic change definitions [8; 36]. Our approach integrates LlaVA [50] and LLAMA [82] models, combining inpainting and prompt-based techniques for precise, contextually informed perturbations.

Stress Testing PipelinesStress testing pipelines, crucial in software engineering, remain under-utilized in machine learning. While various metrics exist for performance assessment and model comparison [67], they often lack the depth to fully capture model robustness and explain failure cases adequately. While initiatives like Stress Test NLI [56] focus on generating adversarial examples to evaluate models' inferential capabilities across six tasks, DynaBench [40] and CheckList [68] take a different approach by employing human-in-the-loop systems to dynamically benchmark and assess the robustness of natural language models in real-world scenarios. Simultaneously, in the vision community, Li et al. [46] utilize diffusion models to create ImageNet-E, honing in on assessing classifier robustness through object attributes, while Luo et al. [52] explore model sensitivity to user-defined text attributes using StyleGAN [2]. Building upon these endeavors, LANCE [62] advances the field by extracting insights from failures via a targeted perturbation algorithm, enabling stress testing across diverse attributes. Our work extends this paradigm to AI-generated image detection, presenting a versatile pipeline capable of performing image augmentation with varying magnitudes of perturbations across any diffusion model for a given set of image data points, facilitating evaluation and bias discovery in detector architectures through a comprehensive range of stress tests.

## 3 Semi-Truths

To precisely evaluate a detector's ability to distinguish between AI-generated and real images, we curate Semi-Truths, consisting of over \(27,600\) real images and \(1,329,155\) fake images. Weconsider several crucial factors: (1) strategies for targeted augmentation at varying magnitudes, (2) diversification of scene distributions, (3) caption perturbation methods, (4) image augmentation techniques, and (5) the saliency of augmented images. The imbalance in our Semi-Truths dataset arises from pairing each real image with multiple augmented variations, a vital requirement for the benchmarking scheme as it enables a comprehensive exploration of model sensitivities across various dimensions (such magnitude of augmentation, change in subject matter, and augmentation technique). This section details methods to quantify augmentation magnitudes, followed by our generation and saliency check pipeline, and an overview of key dataset attributes.

### Magnitudes of Augmentation

The alteration made to an image can be quantified in two ways: (1) the proportion of the image area that has been altered (_area ratio of change_), and (2) the degree to which the semantics of the image were altered (_semantic change_). To control the degree of alteration along these axioms, we start with an initial description of the image. This description is obtained by either selecting a segmentation mask and the corresponding class label or, in the absence of mask information, by generating a caption for the image using BLIP [45].

Introducing PerturbationsMotivated by the categorization of semantic and abstract content from visual semantics research [8], we create a taxonomy for small, medium, and large semantic changes (see Tab. 2). This taxonomy is used to guide the perturbation of an image caption or mask label using LLaVA-Mistral-7B [50] or LLAMA-7B [82] (see Sec. E). As shown in Fig.3, the model is provided with a semantic magnitude category, its definition, a caption to perturb, and the image (if using LLaVA-Mistral-7b). For prompt-based-editing, a diffusion model augments images based on perturbed captions, introducing semantic changes. In conditional inpainting, the perturbed mask label enables precise control over changes in the masked image region.

Measuring Surface Area ChangeWhile segmentation masks help localize augmentations to an image, providing an area ratio of change, diffusion model imprecision can compromise their accuracy.

\begin{table}
\begin{tabular}{c c c} \hline
**Smail Changes** & **Medium Changes** & **Large Changes** \\ Do not significantly alter the overall meaning or context of the image. This could include changing the color of a specific object, adding or removing a minor detail, adjusting the composition or perspective of the image, or slightly adjusting the color distribution of the image. & **Shigthly alter the viewer’s perception of the image and its subject. They could involve minor changes to an object or in setting, like altering a back-ground element, moving an object or prone to another location within the frame, or changing the emotions of the people in the frame.** & **Involve substantial modifications to the image that fundamentally transform its interpretation or message. It may even appear surprising or change to an audience. This could include altering, adding or removing major elements of the image background and making changes to the subject of the image. \\ \end{tabular}
\end{table}
Table 2: **Semantic Taxonomy. Definitions of the magnitudes of semantic change, used to guide the perturbation of image captions (for prompt-based-editing) and mask labels(for inpainting) using LLMs for targeted image perturbation.**

Figure 2: **End-to-end pipeline for Semi-Truths curation and detector stress testing. The Semi-Truths pipeline sources data from \(6\) benchmarks and uses \(2\) perturbation techniques to perturb images. These images undergo saliency checks, metric computation, and stress testing of detectors across our curated tests based on the computed change metrics.**

Dong et al. [17] demonstrate diffusion models can "color outside the box" during inpainting. Furthermore, the lack of mask guidance in prompt-based-editing necessitates the use of post-augmentation metrics to capture the size of alteration. Therefore we employ SSIM [89], MSE, and a custom metric that assesses the extent to which the structural components differ between the original and augmented images in pixel space. Our custom metric, derived from MSE, uses thresholding to remove noisy components followed by connected component analysis to generate masks indicating areas of change. Similar to the area ratio computed using the mask and the image, we compute a ratio using the generated mask to quantify the surface area of change. Each of these metrics are normalized between 0 and 1 and categorized into small, medium, and large changes based on percentiles: the bottom \(25^{th}\) percentile for small, the \(25^{th}\) to \(75^{th}\) percentile for medium and anything beyond the \(75^{th}\) percentile for large.

Measuring Semantic ChangeAs mentioned previously, large language models (LLMs) are used to perturb image captions and mask labels with respect to the taxonomy of semantic change shown in Tab. 2. However, the stochasticity of LLMs and diffusion models necessitates the implementation of post-augmentation metrics that provide a quantitative measure of semantic change achieved. We use three different scores for this task: LPIPS [98], DreamSim [22] (both computed between the original and augmented images), and Sentence Similarity [78] (calculated between the original and perturbed captions/mask labels; see Sec. C.2). These metrics are normalized and categorized like Surface Area Change metrics, indicating small, medium, and large augmentations.

Additional MetricsIn addition to surface area and semantic change, we incorporate metrics that provide a richer description of the underlying distribution, such as _scene diversity_ and _scene complexity_. Scene diversity is defined by the number of unique elements within the original image, while scene complexity measures the quantity of each unique element. Both of these metrics are derived from instance segmentation maps (see Sec. C.2). Additionally, we distinguish changes by their spatial context - diffused changes are dispersed throughout the augmented image, whereas localized changes are concentrated in a specific area (see Algo. 2).

### Image Augmentation Pipeline

Our image augmentation pipeline, delineated in Fig. 3, expands upon the work of LANCE [62] by integrating two distinct image augmentation techniques: (1) conditional inpainting and (2) prompt-based-editing. Both approaches leverage linguistic signal as guidance in image augmentation: prompt-based-editing requires a perturbation to the image caption using LLAMA-7B [82], and conditional inpainting relies on zero-shot mask label perturbation produced by LlaVA-Mistral-7B [50]. Furthermore, the complexity of this pipeline demands comprehensive saliency checks at various stages to ensure that augmented images maintain structural integrity and align with the specified directions of change. To this end, we implement two rounds of saliency evaluation within our image augmentation pipeline to identify instances of high-quality text and image augmentations.

Caption FilteringThe first saliency check protocol evaluates LLM-perturbed captions to ensure two key aspects: (1) accuracy of generated BLIP [45] captions for prompt-based-editing in representing relevant image information, and (2) coherence and desirability of image edits produced

Figure 3: **Image Augmentation Pipeline.** Components of the image augmentation process for Semi-Truth’s curation using inpainting and prompt-based-editing methods.

by perturbed captions/labels, ensuring semantic alignment with original content. For the former, CLIPScore [29] measures the difference between embeddings of the original image and its generated caption, filtering out the lowest 5th percentile values. For the latter, cosine similarity between CLIP [65] text embeddings of the perturbed caption/mask label and the original is calculated, removing values above the \(95^{th}\) percentile (negligible change) and below the \(5^{th}\) percentile (semantic incoherence). Additional details are mentioned in Sec.B

Image Saliency CheckIn the second stage of the saliency check pipeline, we aim to evaluate the (1) structural integrity of augmentations in the image while retaining resemblance to the original, and (2) semantic alignment of image augmentations with the perturbed captions/labels that were used to produce them. Since we lack reference images for direct comparison in image augmentations, conventional metrics like PSNR and SSIM are not suitable. Instead, we explored metrics that assess the structural integrity of each image individually. We use BRISQUE [53], a reference-free metric which quantifies the perceptual quality of an image, labeling images with a score under 70 as highly salient2. Similarly, we use CLIP similarity [65] between original and augmented images to ensure the diffusion model performed substantial enough augmentations on the original. We also employ CLIP directional similarity [23] to confirm that changes in images align with the changes in captions/labels. Images between the \(20^{th}\) and \(80^{th}\) percentile are considered highly salient. Additional details are mentioned in Sec. B

Footnote 2: High BRISQUE scores are indicative of low perceptual quality.

### Semi-Truths Details

Data DistributionWe collect data from \(6\) semantic segmentation benchmarks representing various data distributions: CityScapes [12] for outdoor urban scenes, SUN RGBD [77] for indoor scenes, CelebA HQ [37] for human faces, Human Parsing [48] for full-body human images, and ADE20K [99] and OpenImages [43] for diverse themes. This combined dataset comprises \(27,600\) real images and \(223,400\) masks. Using conditional inpainting and prompt-based-editing techniques across \(5\)[61, 63, 74, 70] diffusion models for inpainting and \(3\)[63, 70] diffusion models for prompt-based-editing, with LlaVA-Mistral-7B [50] and LLAMA-7B [82] for prompt perturbation, we create \(325,718\) prompt-based-editing datapoints and \(1,003,437\) inpainting datapoints. After post-perturbation saliency checks, \(\sim 74\)% of images from inpainting and \(\sim 55\)% from prompt-based-editing techniques are labeled as highly salient, totaling \(\sim 915,445\) images.

MetadataSemi-Truths encompasses extensive metadata accompanying both real and fake image pairs and masks, offering insights into every facet of the perturbation process (see Fig. 4). This metadata includes details about the source data distribution, such as the original benchmark from which the image was sourced, scene complexity and diversity (defined by the number and variety of scene elements), a list of unique entities present in each image, and the ratio of mask-occupied area. Additionally, it provides information about the diffusion model, perturbation technique, and language model utilized for each perturbation, alongside the original and perturbed caption/label. Furthermore,

Figure 4: **Semi-Truths details and metadata. Each augmented image in Semi-Truths is accompanied by metadata detailing properties related to the native data distribution, change magnitude (both area and semantics), and directional semantic edits. Attributes highlighted in yellow are novel contributions presented in this work.**

each perturbed image is accompanied by quantitative and qualitative measures of change categorized across semantic and surface area-based metrics, as outlined in Sec. 3.1. The metadata also indicates whether the change is categorized as diffused or localized, determined using a custom algorithm (detailed in Algo. 2). All of this information is crucial for testing the effectiveness of detectors across various axes, as demonstrated in Sec. 4.

## 4 Experiments

We conduct extensive experiments with Semi-Truths to evaluate the effectiveness of AI-generated image detectors in distinguishing real images from AI-augmented content (see Tab. 3). In the following sections, we show how knowledge abstraction over image augmentations in the dataset helps identify nuanced biases in various detectors. All evaluations are conducted on a 10% sample of Semi-Truths, totaling 87,000 images (27,000 real and 60,000 augmented). The evaluation dataset is available at: https://huggingface.co/datasets/semi-truths/Semi-Truths-Evalset. Since the real class (Real) is unaffected in the distribution-specific analysis, the key metric to observe is Recall on the augmented (Fake) class. A dip in Recall for a specific group indicates the detector's sensitivity to that augmentation. Detector default settings (provided in their respective codebases) have been used for conducting evaluations.

Overall Detector PerformanceWe select a diverse set of open-source AI-generated image detectors for stress testing. As demonstrated in Tab. 3, each model has a unique architecture and training distribution. We first evaluate these detectors in a zero-shot setting using metrics like Precision, Recall, and F1-Score to identify top performers for further analysis. Of the \(6\) models 3chosen, only half demonstrated adequate performance for continued evaluation. The underperforming models include (1) DinoV2, a foundation vision model leveraged for zero-shot AI-generated image prediction, (2) CNNSpot, a ResNet-50 trained solely on GAN-generated content, and (3) DIRE, a ResNet-50 trained on diffusion-generated content.

Footnote 3: These models represent a range of state-of-the-art AI image detectors, showcasing Semi-Truths’s versatility. The evaluation pipeline enables easy benchmarking of new detectors across standardized tests. See: https://github.com/J-Kruk/SemiTruths.

Sensitivity to Data DistributionTo assess potential biases toward specific data distributions, we inspect detector performances on various semantic segmentation benchmarks represented in Semi-Truths. Fig. 5 shows that detector performance varies significantly across data sources. Notably, CrossEfficientViT [11], which is trained on GAN-generated images of human faces, exhibits a significant performance drop on human faces sourced from benchmarks ADE20K, CityScapes [12], and SUN-RGBD [77] (CrossEfficientViT pre-emptively filters any images that do not contain a human face). In contrast, DE-FAKE [73], trained on general scene images, exhibits the worst performance on CelebA-HQ [44] and HumanParsing [48] due to limited focus on humans and portrait-like images in its training distribution. On the other hand, UniversalFakeDetect [59], trained on indoor bedroom images and other generic scenes, fails to perform well with SUN RGBD and shows a significant performance drop on CityScapes.

Furthermore, we investigate the detectors' ability to handle highly complex and diverse multi-instance scenes. Their performance is evaluated across varying degrees of scene diversity (number of unique class instances in the images) and scene complexity (number of instances in total), categorized into small, medium, and large bins (additional details in Sec. C.2). We find that UniversalFakeDetect's [73] performance drops with increasing scene diversity and complexity. In contrast, DE-FAKE [73]

\begin{table}
\begin{tabular}{l c c c c c|c c c|c c c} \hline \hline \multirow{2}{*}{**Detector**} & \multirow{2}{*}{**Backbone**} & \multicolumn{2}{c}{**Training Data Distribution**} & \multicolumn{4}{c}{**Precision(\(\uparrow\))**} & \multicolumn{2}{c}{**Recall(\(\uparrow\))**} \\ \cline{3-12}  & & Scene & GANs & Diffusion & All & Real & Fake & All & Real & Fake \\ \hline
1 DINOv2 [60] & ViT [18] + ResNet-50 [25] & General & ✗ & \(\times\) & \(29.30\) & \(37.17\) & \(21.43\) & \(49.99\) & \(99.96\) & \(00.01\) \\
2 CNNSpot [86] & ResNet-50 [25] & General & ✓ & \(\times\) & \(30.13\) & \(35.27\) & \(25.00\) & \(49.99\) & \(99.99\) & \(00.00\) \\
3 DIRE [88] & ResNet-50 [25] & General & \(\times\) & ✓ & \(31.09\) & \(37.18\) & \(25.00\) & \(49.99\) & \(99.99\) & \(00.00\) \\
4 CrossEfficientViT [11] & EfficientNet-B0 [80] + ViT [18] & Face & ✓ & \(\times\) & \(46.37\) & \(34.89\) & \(57.85\) & \(46.58\) & \(28.87\) & \(30.28\) \\
5 UniversalFakeDetect [59] & CLIP [65]-ViT [18] & General & ✓ & ✓ & \(64.84\) & \(58.89\) & \(70.79\) & \(60.57\) & \(34.11\) & \(87.03\) \\
6 DE-FAKE [73] & CLIP [65] & General & ✓ & ✓ & \(61.65\) & \(49.97\) & \(73.33\) & \(61.88\) & \(52.28\) & \(71.48\) \\ \hline \hline \end{tabular}
\end{table}
Table 3: **AI-generated Image Detectors evaluated with Semi-Truths. We evaluated \(6\) detectors with varying backbones and training data distributions. Models that performed satisfactorily, highlighted in green, were selected for additional testing.**remains fairly robust across different scene variations. CrossEfficientViT [11] shows improved performance with increasing scene complexity and diversity, which can be attributed to human-centered benchmarks like CelebA-HQ [44] and HumanParsing [48] segmenting distinct facial features and body parts which results in lower scene complexity.

These results highlight that detectors are highly sensitive to the semantic attributes of data distributions, emphasizing the importance of stress tests to identify and address distributional weaknesses.

Evaluation across Augmentation Techniques and ModelsSemi-Truths contains images generated using two different augmentation approaches - conditional inpainting and prompt-based-editing - as well as five different diffusion algorithms: StableDiffusion v1.4, StableDiffusion v1.5, StableDiffusion XL [61], OpenJourney [63], and Kandinsky 2.2 [74]. This diversity in generated content enables investigation of detector sensitivities to different augmentation procedures.4 As shown in Fig.6, UniversalFakeDetect [59] performs best on images augmented with Kandinsky 2.2 [74] and worst on those augmented with StableDiffusion v1.5 [70], with a 10% difference in Recall score. The inverse is true for DE-FAKE [73]. CrossEfficientVit [11] performs best on images augmented with StableDiffusion v1.4 and worst with Kandinsky 2.2 [74], with a 12% drop in performance. Furthermore, CrossEfficientViT [11] and DE-FAKE [73] are more sensitive to inpainted images, whereas UniversalFakeDetect [59] performs worst on content augmented with prompt-based-editing.

Footnote 4: Limitations of [28], [54] restrict prompt-based-editing to OpenJourney, StableDiffusion v1.4 & v1.5

Evaluation across Varying Magnitudes of AugmentationAs detailed in Sec. 3.1, each image in Semi-Truths is fitted with an array of descriptive attributes that capture the magnitude of change. In Fig.7 we examine the impact of varying degrees of augmentation on detector performance, focusing on both surface area and semantic changes. Note that CrossEfficientViT [11] performs better on smaller values of Area Ratio, where as UniversalFakeDetect [59] performs better on larger changes.

Figure 5: **Detectors are sensitive to semantic aspects of data distribution. The \(3\) detectors, CrossEfficientViT, DE-FAKE and UniversalFakeDetect were evaluated across varying (a) data distribution, (b) scene complexity and (c) scene diversity.**

Figure 6: **Performance variation across image augmentation methods and diffusion algorithms. Semi-Truths offers data generated using various diffusion algorithms and augmentation methods facilitating detector evaluation on these aspects.**

UniversalFakeDetect's [59] performance also drops as DreamSim [22] scores increase. Even though DE-FAKE [73] is not the best performing model, it appears to be the most robust against various magnitudes of change across the board.

Directional Semantic EditsQuantitative metrics can be reductive when describing how the semantics or narrative of an image changes. Transitioning to an embedded space to assess similarity, for example, often results in significant information loss. To address this issue, we introduce "Directional Semantic Edits", which groups augmented images from Semi-Truths by distinct pairs of original and perturbed caption/mask labels. In the evaluation set, certain directional semantic edits occurred as frequently as \(445\) times. Each detector is evaluated on these groups, and metrics are sorted by Recall, as shown in Tab.4. Each model exhibits distinct performance variations based on specific semantic changes. Notably, UniversalFakeDetect [59] performs best on augmentations to facial features but worst on augmentations to vegetation. Conversely, DE-FAKE [73] excels at detecting augmentations to cars and vegetation but struggles with augmentations to human faces. CrossEfficientViT [11] shows varied performance with augmentations to human faces, appearing in both its highest and lowest ranks, indicating sensitivity to the magnitude of the change.

Further analysis of these augmentations can maximize the potential of these algorithms by informing decisions about the most suitable ensemble techniques. For example, while UniversalFakeDetect [59] struggles with vegetation-to-tree augmentations, DE-FAKE [73] excels, suggesting a suitable combination for ensemble approaches. Such analysis reveals the most challenging directional augmentations, offering insights into detector model limitations.

Surveying Human Perception of Magnitudes of ChangeTo build intuition about the algorithms we use to quantify the degree of visual and semantic change achieved during image augmentation, we conduct a user study to evaluate if any metrics align with human perception. Annotators are asked to categorize changes between original and augmented images as "not much," "some," or "a lot," corresponding to our "small," "medium," and "large" change bins. We then compute correlation coefficients (Pearson [41], Kendall Tau [64], and Spearman [1]) between human scores and quantitative measures in Semi-Truths. The results in Tab.5 show that Area Ratio, a novel metric presented in this work, demonstrates the highest correlation with human perception, whereas other metrics demonstrate little to no correlation. It is important to note, however, that some changes may be imperceptible to the human eye but appear drastic in pixel space (additional discussion in Sec. D).

\begin{table}

\end{table}
Table 4: **Directional Semantic Edits for investigating detector biases. Directional Semantic Edits provide insights on which edits to a certain entity has a higher chance of fooling detectors, we notice that patterns vary significantly across detectors.**

Figure 7: **Performance variation of select detectors across various magnitudes of augmentation. DE-FAKE [73] is robust across the board, Area Ratio captures the sensitivity exhibited in UniversalFakeDetect [59] and CrossEfficientViT [11].**

## 5 Discussion

Limitations and Future WorkOur inpainting pipeline currently relies on manual semantic mask input from existing semantic segmentation benchmarks, limiting usability. To improve, automatic mask generation methods like SAM [42] can be embedded into the augmentation pipeline, similar to InstructEdit [85]. Additionally, using LLAMA-7B [82] and LlaVA [50] models for zero-shot perturbation has led to many poor-quality outputs, requiring filtering. Future iterations will involve fine-tuning these models. We are also aware of potential biases in metrics like LPIPS [98], Sentence Similarity [78], and DreamSim [22], which may impact evaluations. To mitigate this issue, we will incorporate a combination of multiple open-source LLMs to compute semantic change metrics, thereby reducing the inherent biases associated with any single model, a process facilitated by our modular pipeline which enables easy switching between different LLMs of the user's choosing.

Ethical Issues and Bias MitigationWhile our project aims to create a test suite for evaluating and improving detector robustness, it can also be used to create fake images capable of deceiving AI-generated image detectors, potentially facilitating the spread of misinformation. Hence, we curated Semi-Truths by sourcing images from publicly available datasets with minimal potential for harm, and any manipulations on such images should not serve as a potential threat to society as per our knowledge. Additionally, despite our efforts at diversification of data and models, inherent biases from these modules may persist, potentially perpetuating or exacerbating existing inequalities, resulting in uneven performance across different contexts and types of images. To minimize additional bias, we employ a diverse range of perturbation techniques, diffusion models, LLMs, and source benchmarks. This diversity, along with comprehensive metadata--including original and perturbed captions/labels and images--enables users to analyze perturbation styles and identify existing biases in the generative models.

## 6 Conclusion

To address the rising threat of misinformation from AI-augmented images, we introduce Semi-Truths: a comprehensive dataset of \(1,329,155\) AI-augmented images, with detailed metadata on source distribution, augmentation techniques, change magnitudes, diffusion models, and directional edits (paired original and perturbed captions). Our plug-and-play image perturbation pipeline enables easy generation of additional augmentations and offers a standardized platform to test detector robustness across curated scenarios. Our analysis reveals that state-of-the-art detectors exhibit varying sensitivity to perturbation levels, data distributions, and augmentation methods, providing valuable insights into detector functionality. With a semantic taxonomy for defining change types and a quality-check pipeline, Semi-Truths also serves as a robust training and testing resource, enhancing the resilience of AI-generated image detectors. Furthermore, its diverse metadata enables bias analysis, supporting research into model fairness. We believe the user-friendly design of Semi-Truths will facilitate ongoing research into robustness against evolving generative models, helping combat misinformation effectively.

## References

* [1]_Spearman Rank Correlation Coefficient_, pages 502-505. Springer New York, New York, NY,

\begin{table}
\begin{tabular}{l c c c} \hline \hline \multirow{2}{*}{**Correlation Coeff.**} & \multicolumn{3}{c}{**Change Metrics\((\dagger)\)**} \\ \cline{2-4}  & Area Ratio & LPIPS Score & SSIM \\ \hline
1 Pearson & \(0.46\) & \(0.14\) & \(-0.16\) \\
2 Kendall-Tau & \(0.40\) & \(0.15\) & \(-0.14\) \\
3 Spearman & \(0.50\) & \(0.19\) & \(-0.17\) \\ \hline \hline \end{tabular}
\end{table}
Table 5: **Correlation between quantitative measures of change and Human Perception.** Correlation coefficients computed between human-annotated magnitudes of change and quantitative metrics available in the dataset. Quantitative metrics not displayed here had coefficients \(<\) 0.10.

Figure 8: **Relationship between quantitative change metrics and Human Perception of change (small, medium, large) in Semi-Truths.** Each violin plot shows the distribution of metric values for a change category.

2008.
* [2] Rameen Abdal, Yipeng Qin, and Peter Wonka. Image2stylegan: How to embed images into the stylegan latent space?, 2019.
* [3] C. Avey. Ethical pros and cons of ai image generation. _IEEE Computer Society/Tech News/Community Voices_, December 27 2023.
* [4] Jianmin Bao, Dong Chen, Fang Wen, Houqiang Li, and Gang Hua. Cvae-gan: fine-grained image generation through asymmetric training. In _Proceedings of the IEEE international conference on computer vision_, pages 2745-2754, 2017.
* [5] Jordan J. Bird and Ahmad Lotfi. Cifake: Image classification and explainable identification of ai-generated synthetic images, 2023.
* [6] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale gan training for high fidelity natural image synthesis, 2019.
* [7] Tim Brooks, Aleksander Holynski, and Alexei A. Efros. Instructpix2pix: Learning to follow image editing instructions, 2023.
* [8] B. Burford, P. Briggs, and J. P. Eakins. A taxonomy of the image: On the classification of content for image retrieval. _Visual Communication_, 2(2):123-161, 2003.
* [9] Wayne Chang. Ai is the final blow for an id system whose time has passed. _Forbes_, August 2024.
* [10] Yiqun Chen and James Zou. Twigma: A dataset of ai-generated images with metadata from twitter, 2023.
* [11] Davide Alessandro Coccomini, Nicola Messina, Claudio Gennaro, and Fabrizio Falchi. _Combining EfficientNet and Vision Transformers for Video Deepfake Detection_, page 219-229. Springer International Publishing, 2022.
* [12] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding, 2016.
* [13] Florinel-Alin Croitoru, Vlad Hondru, Radu Tudor Ionescu, and Mubarak Shah. Diffusion models in vision: A survey. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 45(9):10850-10869, 2023.
* [14] Hao Dang, Feng Liu, Joel Stehouwer, Xiaoming Liu, and Anil Jain. On the detection of digital face manipulation, 2020.
* [15] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. _Advances in neural information processing systems_, 34:8780-8794, 2021.
* [16] Brian Dolhansky, Ben Pflaum Joanna Bitton, Jikuo Lu, Russ Howes, Menglin Wang, and Cristian Canton Ferrer. The deepfake detection challenge dataset, 2020.
* [17] Wenkai Dong, Song Xue, Xiaoyue Duan, and Shumin Han. Prompt tuning inversion for text-driven image editing using diffusion models, 2023.
* [18] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale, 2021.
* [19] Simon Ellery. Fake photos of pope francis in a puffer jacket go viral, highlighting the power and peril of ai, March 2023.
* faceswap, March 2024.
- los angeles times. _Los Angeles Times_, April 2024.
* [22] Stephanie Fu, Netanel Tamir, Shobhita Sundaram, Lucy Chai, Richard Zhang, Tali Dekel, and Phillip Isola. Dreamsim: Learning new dimensions of human visual similarity using synthetic data. _arXiv preprint arXiv:2306.09344_, 2023.
* [23] Rinon Gal, Or Patashnik, Haggai Maron, Gal Chechik, and Daniel Cohen-Or. Stylegan-nada: Clip-guided domain adaptation of image generators, 2021.
* [24] Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo Zhang, Dongdong Chen, Lu Yuan, and Baining Guo. Vector quantized diffusion model for text-to-image synthesis, 2022.
* [25] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition, 2015.
* [26] Kathleen Magramo Heather Chen. Finance worker pays out $25 million after video call with deepfake 'chief financial officer' | CNN -- cnn.com. https://www.cnn.com/2024/02/04/asia/deepfake-cfo-scam-hong-kong-intl-hnk/index.html.
* [27] M. Heikkila. The algorithm: Ai-generated art raises tricky questions about ethics, copyright, and security. _MIT Technology Review_, September 20 2022.
* [28] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt image editing with cross attention control, 2022.
* [29] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. Clipscore: A reference-free evaluation metric for image captioning, 2022.
* [30] R Devon Hjelm, Athul Paul Jacob, Tong Che, Adam Trischler, Kyunghyun Cho, and Yoshua Bengio. Boundary-seeking generative adversarial networks, 2017.
* [31] Tiffany Hsu and Stuart A Thompson. The new york times company. _A.I. Muddies Israel-Hamas War in Unexpected Way_, Oct 2023.
* [32] Huaibo Huang, Ran He, Zhenan Sun, Tieniu Tan, et al. Introvae: Introspective variational autoencoders for photographic image synthesis. _Advances in neural information processing systems_, 31, 2018.
* [33] Yi Huang, Jiancheng Huang, Yifan Liu, Mingfu Yan, Jiaxi Lv, Jianzhuang Liu, Wei Xiong, He Zhang, Shifeng Chen, and Liangliang Cao. Diffusion model-based image editing: A survey, 2024.
* [34] Rowan T Hughes, Liming Zhu, and Tomasz Bednarz. Generative adversarial networks-enabled human-artificial intelligence collaborative applications for creative and design industries: A systematic review of current approaches and trends. _Frontiers in artificial intelligence_, 4:604234, 2021.
* [35] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A. Efros. Image-to-image translation with conditional adversarial networks. In _2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_. IEEE, July 2017.
* [36] Alejandro Jaimes and Shih-Fu Chang. Conceptual framework for indexing visual information at multiple levels. In _Electronic imaging_, 1999.
* [37] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of gans for improved quality, stability, and variation. 2018.
* [38] Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial networks. In _2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_. IEEE, June 2019.
* [39] Hasam Khalid, Shahroz Tariq, Minha Kim, and Simon S. Woo. Fakeavceleb: A novel audio-video multimodal deepfake dataset, 2022.

* [40] Douwe Kiela, Max Bartolo, Yixin Nie, Divyansh Kaushik, Atticus Geiger, Zhengxuan Wu, Bertie Vidgen, Grusha Prasad, Amanpreet Singh, Pratik Ringshia, Zhiyi Ma, Tristan Thrush, Sebastian Riedel, Zeerak Waseem, Pontus Stenetorp, Robin Jia, Mohit Bansal, Christopher Potts, and Adina Williams. Dynabench: Rethinking benchmarking in nlp, 2021.
* [41] Wilhelm Kirch, editor. _Pearson's Correlation Coefficient_, pages 1090-1091. Springer Netherlands, Dordrecht, 2008.
* [42] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollar, and Ross Girshick. Segment anything, 2023.
* [43] Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Stefan Popov, Matteo Malloci, Alexander Kolesnikov, Tom Duerig, and Vittorio Ferrari. The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale. _IJCV_, 2020.
* [44] Cheng-Han Lee, Ziwei Liu, Lingyun Wu, and Ping Luo. Maskgan: Towards diverse and interactive facial image manipulation. In _IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2020.
* [45] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation, 2022.
* [46] Xiaodan Li, Yuefeng Chen, Yao Zhu, Shuhui Wang, Rong Zhang, and Hui Xue. Imagenet-e: Benchmarking neural network robustness via attribute editing, 2023.
* [47] Yuezun Li, Xin Yang, Pu Sun, Honggang Qi, and Siwei Lyu. Celeb-df: A large-scale challenging dataset for deepfake forensics. In _IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2020.
* [48] Xiaodan Liang, Si Liu, Xiaohui Shen, Jianchao Yang, Luoqi Liu, Jian Dong, Liang Lin, and Shuicheng Yan. Deep human parsing with active template regression. _Pattern Analysis and Machine Intelligence, IEEE Transactions on_, 37(12):2402-2414, Dec 2015.
* [49] Huan Ling, Karsten Kreis, Daiqing Li, Seung Wook Kim, Antonio Torralba, and Sanja Fidler. Editgan: High-precision semantic image editing, 2021.
* [50] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning, 2023.
* [51] Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher Yu, Radu Timofte, and Luc Van Gool. Repaint: Inpainting using denoising diffusion probabilistic models, 2022.
* [52] Jinqi Luo, Zhaoning Wang, Chen Henry Wu, Dong Huang, and Fernando De la Torre. Zero-shot model diagnosis, 2023.
* [53] Anish Mittal, Anush K. Moorthy, and Alan C. Bovik. Blind/referenceless image spatial quality evaluator. In _2011 Conference Record of the Forty Fifth Asilomar Conference on Signals, Systems and Computers (ASILOMAR)_, pages 723-727, 2011.
* [54] Ron Mokady, Amir Hertz, Kfir Aherman, Yael Pritch, and Daniel Cohen-Or. Null-text inversion for editing real images using guided diffusion models, 2022.
* [55] Scott Monteith, Tasha Glenn, John R Geddes, Peter C Whybrow, Eric Achtyes, and Michael Bauer. Artificial intelligence and increasing misinformation. _The British Journal of Psychiatry_, 224(2):33-35, 2024.
* [56] Aakanksha Naik, Abhilasha Ravichander, Norman Sadeh, Carolyn Rose, and Graham Neubig. Stress test evaluation for natural language inference, 2018.
* [57] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models, 2022.

* [58] Yuval Nirkin, Yosi Keller, and Tal Hassner. FSGAN: Subject agnostic face swapping and reenactment. In _Proceedings of the IEEE International Conference on Computer Vision_, pages 7184-7193, 2019.
* [59] Utkarsh Ojha, Yuheng Li, and Yong Jae Lee. Towards universal fake image detectors that generalize across generative models, 2023.
* [60] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Mahmoud Assran, Nicolas Ballas, Wojciech Galuba, Russell Howes, Po-Yao Huang, Shang-Wen Li, Ishan Misra, Michael Rabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, Herve Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. Dinov2: Learning robust visual features without supervision, 2024.
* [61] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis, 2023.
* [62] Viraj Prabhu, Sriram Yenamandra, Prithvijit Chattopadhyay, and Judy Hoffman. Lance: Stress-testing visual models by generating language-guided counterfactual images, 2023.
* [63] prompthero. Openjourney.
* [64] Llukan Puka. _Kendall's Tau_, pages 713-715. Springer Berlin Heidelberg, Berlin, Heidelberg, 2011.
* [65] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision, 2021.
* [66] Md Awasafur Rahman, Bishmoy Paul, Najibul Haque Sarker, Zaber Ibn Abdul Hakim, and Shaikh Anowarul Fattah. Artifact: A large-scale dataset with artificial and factual images for generalizable and robust synthetic image detection, 2023.
* [67] O. Rainio, J. Teuho, and R. Klen. Evaluation metrics and statistical tests for machine learning. _Scientific Reports_, 14:6086, 2024.
* [68] Marco Tulio Ribeiro, Tongshuang Wu, Carlos Guestrin, and Sameer Singh. Beyond accuracy: Behavioral testing of NLP models with CheckList. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault, editors, _Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics_, pages 4902-4912, Online, July 2020. Association for Computational Linguistics.
* [69] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 10684-10695, 2022.
* [70] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models, 2022.
* [71] Andreas Rossler, Davide Cozzolino, Luisa Verdoliva, Christian Riess, Justus Thies, and Matthias Niessner. FaceForensics++: Learning to detect manipulated facial images. In _International Conference on Computer Vision (ICCV)_, 2019.
* [72] Elyse Samuels. White House social media director tweets manipulated video of Biden.

9 2020.
* [73] Zeyang Sha, Zheng Li, Ning Yu, and Yang Zhang. De-fake: Detection and attribution of fake images generated by text-to-image generation models, 2023.
* [74] Arseniy Shakhmatov, Anton Razhizgaev, Aleksandr Nikolich, Vladimir Arkhipkin, Igor Pavlov, Andrey Kuznetsov, and Denis Dimitrov. Ikandinsky 2.2, 2023.

* [75] Gowthami Somepalli, Vasu Singla, Micah Goldblum, Jonas Geiping, and Tom Goldstein. Diffusion art or digital forgery? investigating data replication in diffusion models, 2022.
* [76] Haixu Song, Shiyu Huang, Yinpeng Dong, and Wei-Wei Tu. Robustness and generalizability of deepfake detection: A study with diffusion models, 2023.
* [77] Shuran Song, Samuel P Lichtenberg, and Jianxiong Xiao. Sun rgb-d: A rgb-d scene understanding benchmark suite. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 567-576, 2015.
* [78] Xiaofei Sun, Yuxian Meng, Xiang Ao, Fei Wu, Tianwei Zhang, Jiwei Li, and Chun Fan. Sentence similarity based on contexts, 2022.
* [79] Natnicha Surasit. Criminal exploitation of deepfakes in South East Asia -- globalinitiative.net. https://globalinitiative.net/analysis/deepfakes-ai-cyber-scam-south-east-asia-organized-crime/.
* [80] Mingxing Tan and Quoc V. Le. Efficientnet: Rethinking model scaling for convolutional neural networks. _ArXiv_, abs/1905.11946, 2019.
* [81] J. Thies, M. Zollhofer, M. Stamminger, C. Theobalt, and M. Niessner. Face2face: Real-time face capture and reenactment of rgb videos. In _Proc. Computer Vision and Pattern Recognition (CVPR), IEEE_, 2016.
* [82] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothe Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models, 2023.
* [83] Iulia Turc and Gaurav Nemade. Midjourney user prompts &amp; generated images (250k), 2022.
* [84] Arash Vahdat and Jan Kautz. Nvae: A deep hierarchical variational autoencoder. _Advances in neural information processing systems_, 33:19667-19679, 2020.
* [85] Qian Wang, Biao Zhang, Michael Birsak, and Peter Wonka. Instructedit: Improving automatic masks for diffusion-based image editing with user instructions, 2023.
* [86] Sheng-Yu Wang, Oliver Wang, Richard Zhang, Andrew Owens, and Alexei A Efros. Cnn-generated images are surprisingly easy to spot...for now. In _CVPR_, 2020.
* [87] Su Wang, Chitwan Saharia, Ceslee Montgomery, Jordi Pont-Tuset, Shai Noy, Stefano Pellegrini, Yasumasa Onoe, Sarah Laszlo, David J. Fleet, Radu Soricut, Jason Baldridge, Mohammad Norouzi, Peter Anderson, and William Chan. Imagen editor and editbench: Advancing and evaluating text-guided image inpainting, 2023.
* [88] Zhendong Wang, Jianmin Bao, Wengang Zhou, Weilun Wang, Hezhen Hu, Hong Chen, and Houqiang Li. Dire for diffusion-generated image detection. _arXiv preprint arXiv:2303.09295_, 2023.
* [89] Zhou Wang, A.C. Bovik, H.R. Sheikh, and E.P. Simoncelli. Image quality assessment: from error visibility to structural similarity. _IEEE Transactions on Image Processing_, 13(4):600-612, 2004.
* [90] Zijie J. Wang, Evan Montoya, David Munechika, Haoyang Yang, Benjamin Hoover, and Duen Horng Chau. DiffusionDB: A large-scale prompt gallery dataset for text-to-image generative models. _arXiv:2210.14896 [cs]_, 2022.
* report, March 2024.
* [92] Shaoan Xie, Zhifei Zhang, Zhe Lin, Tobias Hinz, and Kun Zhang. Smartbrush: Text and shape guided object inpainting with diffusion model, 2022.

* [93] Shaoan Xie, Yang Zhao, Zhisheng Xiao, Kelvin C. K. Chan, Yandong Li, Yanwu Xu, Kun Zhang, and Tingbo Hou. Dreaminpainter: Text-guided subject-driven image inpainting with diffusion models, 2023.
* [94] Danni Xu, Shaojing Fan, and Mohan Kankanhalli. Combating misinformation in the era of generative ai models. In _Proceedings of the 31st ACM International Conference on Multimedia_, MM '23, page 9291-9298, New York, NY, USA, 2023. Association for Computing Machinery.
* [95] Angela Yang. Even katy perry's mom was fooled by what appeared to be ai-generated met gala pics, May 2024.
* [96] Ling Yang, Zhilong Zhang, Yang Song, Shenda Hong, Runsheng Xu, Yue Zhao, Wentao Zhang, Bin Cui, and Ming-Hsuan Yang. Diffusion models: A comprehensive survey of methods and applications. _ACM Comput. Surv._, 56(4), nov 2023.
* [97] Tao Yu, Runseng Feng, Ruoyu Feng, Jinming Liu, Xin Jin, Wenjun Zeng, and Zhibo Chen. Inpaint anything: Segment anything meets image inpainting, 2023.
* [98] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric. In _CVPR_, 2018.
* [99] Bolei Zhou, Hang Zhao, Xavier Puig, Tete Xiao, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Semantic understanding of scenes through the ade20k dataset. _International Journal of Computer Vision_, 127(3):302-321, 2019.
* [100] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A. Efros. Unpaired image-to-image translation using cycle-consistent adversarial networks. In _2017 IEEE International Conference on Computer Vision (ICCV)_. IEEE, October 2017.
* [101] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Unpaired image-to-image translation using cycle-consistent adversarial networks. In _Computer Vision (ICCV), 2017 IEEE International Conference on_, 2017.
* [102] Mingjian Zhu, Hanting Chen, Qiangyu Yan, Xudong Huang, Guanyu Lin, Wei Li, Zhijun Tu, Hailin Hu, Jie Hu, and Yunhe Wang. Genimage: A million-scale benchmark for detecting ai-generated image, 2023.

## Checklist

1. For all authors... 1. Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? [Yes] See Sec. 3, 4. 2. Did you describe the limitations of your work? [Yes] See Sec. 5. 3. Did you discuss any potential negative societal impacts of your work? [Yes] See Sec. 5. 4. Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes] We have discussed it further in Sec. 5 and Sec. D.
2. If you are including theoretical results... 1. Did you state the full set of assumptions of all theoretical results? [NA] 2. Did you include complete proofs of all theoretical results? [NA]
3. If you ran experiments (e.g. for benchmarks)... 1. Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? [Yes] The URLs are mentioned in the appendix and in main text: (code) https://github.com/J-Kruk/SemiTruths, (data) https://huggingface.co/datasets/semi-truths/Semi-Truths.
4. Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [Yes] We have described the split of the main dataset used for evaluation in Sec. 4. We have also mentioned that we have performed out of the box evaluation of the detectors using the default settings provided in their respective codebase. 5. Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? We use the default codebase provided by the detectors as mentioned in Sec. 4 hence we do not report any custom seeds. 6. Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes] These details have been provided in the appendix.
5. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets... 1. If your work uses existing assets, did you cite the creators? We have cited all the existing work we have built up on, such as LANCE (Sec. 2), diffusion models (Sec. 4), and AI-generated image detectors evaluated (Sec. 4). 2. Did you mention the license of the assets? [Yes] See Sec. H. 3. Did you include any new assets either in the supplemental material or as a URL? [No] 4. Did you discuss whether and how consent was obtained from people whose data you're using/curating? [NA] All the existing assets we have used are open source, publicly available for use. 5. Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [Yes] We have included a discussion in Sec. 5.
6. If you used crowdsourcing or conducted research with human subjects... 1. Did you include the full text of instructions given to participants and screenshots, if applicable? [Yes] see Sec. D. 2. Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [Yes] A discussion on participant risks and the study's IRB Exempt status can be found in Sec.D. 3. Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [Yes] Annotator compensation can also be found in Sec.D.

Image Augmentation

### Data Preprocessing

We preprocess and condense all benchmarks into one by standardizing segmentation masks, sizes of visual media, and metadata. Area ratios are computed for every mask, this is a value that denotes the percentage of the original image highlighted by the segmentation. Masks that are too small, i.e. denote a region that is less than 5% of the image area, are excluded. We process all masks by applying two Max Filters (size 9), one Mode Filter (size 9) to smooth edges, followed by Gaussian blur (radius 16), as demonstrated by Fig.9. This procedure is optimized by inspecting the quality of image augmentations produced. Fig.10 highlights how diffusion inpainting outputs are affected by various magnitudes of Gaussian blur and Max filtering.

Figure 10: This figure demonstrates how each mask is preprocessed before image augmentation through diffusion inpainting. Two Max Filters are applied with width 9, a Mode Filter to smooth edges, followed by Gaussian blur.

Figure 9: This figure demonstrates how each mask is preprocessed before it is used to augment images through diffusion inpainting. Two Max Filters are applied with width 9, following a mode filter to smooth edges, and finally Gaussian blur.

### Visualizing Capabilities of Various Diffusion Models

To select the diffusion models used for image augmentation in Semi-Truths, a qualitative investigation is conducted to evaluate the quality of the generated content. Fig.11 displays one such exploration, in which we compare the inpainting capabilities of each diffusion model on a number of image, mask pairs.

## Appendix B Quality Verification

### Caption Filtering

The first step in ensuring high quality generations is to select high quality prompts that will be provided as to diffusion models. In addition to the steps mentioned in the paper, we also filter the generated captions based on the following criteria:

1. Length of perturbed text: To ensure high quality generation, we prune all generated text. In the case of inpainting, we discard any perturbed label larger than 5 words. For prompt-tuning we discard any perturbed captions larger than 30 words.
2. Special characters: We observe that models used for image augmentation do not respond well to the inclusion of special characters in the prompts. Hence, any caption or label containing a special character is discarded.

### Image Saliency Check

Additional details regarding quality check metrics mentioned in Sec.3.2, as well as metrics used after the image perturbing phase to determine the quality of images, are given below:

1. _Directional Similarity score:_ This metric ensures the change in the images is reflective of the change in the captions/label, and the generated image aligns with the perturbed caption/label. Directional similarity is calculated as follows:

Figure 11: Exploring capabilities of diffusion models for inpainting image augmentation.

\[Cosine(CLIP_{\text{original image}}-CLIP_{\text{perturbed image}},\] \[CLIP_{\text{original text}}-CLIP_{\text{perturbed text}})\] We retain the images that correspond to the values lying above the \(23^{rd}\) percentile of the distribution.
2. _Remove completely black images:_ In certain cases, if the prompt is complex or considered potentially harmful, the diffusion models mentioned in Sec. 3.3 generate completely black images with no information. We remove all such generations from the dataset.

Examples of the images corresponding to varying distribution of metrics are shown in Fig.13.

Through our quality check process, we assign a True or False label depending on whether they pass the saliency check. For an image to pass the saliency check it must simultaneously satisfy all

Figure 12: Overview of the quality check pipeline: Low-quality captions or label perturbations are rejected before image generation, and low-quality images are rejected at the end of the pipeline. Only generations that maintain high quality throughout the process are accepted into the final dataset.

Figure 13: Examples of original and perturbed images throughout the spectrum of each quality check metric, from minimum to maximum

the following conditions: (1) have a Brisbane score below 70 (2) have the CLIP image similarity the original and augmented image lie between the 20th an 80th percentile values, and (3) have the directional similarity score lie above the 23rd percentile value. Examples of filtered out images are shown in **??**

## Appendix C Experimental Design

### Cropping of Detector Input Images

Most AI-generated image detection models evaluated in our experiments prepare input images by resizing and taking a 224x224 center-crop. For our experimental setup, we acknowledge that this method of pre-processing may lead to cases where the perturbations made to the image lie outside the cropped area. To address this concern, we implemented a method of cropping the input image in a way that does not exclude the augmentation using the image perturbation masks. For inpainting the generation masks are used, and for prompt tuning we use the masks generated after image perturbing.

The part of the image to be perturbed is masked using white pixels. This original mask is center-cropped, and the surface area of white pixels in the cropped mask is compared with the surface area of white pixels in the original mask. If this surface area ratio passes a specified threshold, it is

Figure 14: Distribution of quality check metrics across prompt based perturbations. The images lying in the grayed out areas are discarded during quality check.

determined that the cropped area includes a sufficient amount of the perturbed area for the detectors to identify, and this transformed input is passed in through the detector.

### Metric Binning Strategy

Scene DiversityWe use scene diversity of the original image to determine how the diversity of the scene plays a role in misleading AI-generated image detectors. To calculate scene diversity, we use class labels provided by the segmentation masks to understand the composition of the scene by finding how many unique classes are present in each scene. We then divide the values into three bins of small, medium and large where the values below \(1\) standard deviation from the mean are assigned the bin small, the values above \(1\) standard deviation from the mean are assigned the bin large and the

Figure 15: Scene diversity visualized across the datasets with cutoffs for binning

rest are assigned the bin medium. Fig. 15 illustrates the scene diversity distributions for the entire source dataset as well as for each of the individual source datasets.

Scene ComplexityWe use scene complexity of the original image to determine how the complexity of the scene plays a role in misleading AI-generated image detectors. To calculate scene complexity, we count the number of instances present in each image using the provided instance segmentation masks. We then divide the values into three bins of small, medium and large where the values below \(1\) standard deviation from the mean are assigned the bin small, the values above \(1\) standard deviation from the mean are assigned the bin large and the rest are assigned the bin medium. Fig. 16 illustrates the scene complexity distributions for the entire source dataset as well as for each of the individual source datasets.

Semantic and Surface Change MetricsFor semantic and surface area change, these values are categorized into 3 bins (small, medium and large), where the bottom \(25^{th}\) percentile was assigned to small changes, the middle \(25^{th}\) to \(75^{th}\) percentile for medium and above \(75^{th}\) for large. The distribution of changes across the different bins for the entire dataset along with the cutoffs for binning is illustrated in Fig. 17.

### Post Perturbation Surface Area Change Metric

The Algo. 2 indicates the generation of post-perturbation surface area change metric and corresponding mask. We also use these metrics to calculate additional metrics that categorize a change as

Figure 16: Scene complexity visualized across the datasets with cutoffs for binning

Figure 17: Distribution across different semantic change metrics with their binning cutoffs

localized or diffused. Here localized means that the changes are concentrated in one area whereas diffused indicates that the change is spread out throughout the image. The algorithm also highlights how we use information gathered from the connected components to classify a change as diffused or localized. Fig. 18 and Fig. 19 illustrate a few examples of diffused and localized changes.

## Appendix D Human Evaluation

Figure 19: **Examples of diffused changesSteps in the calculation of post-perturbation mask and corresponding diffused change identification**

Figure 20: The instructive figure provided to annotators during the user study. The visualization was design to guide the participant on what kind of inputs are expected, without priming them on explicit definitions that we used in designing our semantic change categories.

Figure 18: **Examples of localized changesSteps in the calculation of post-perturbation mask and corresponding localized change identification**

The annotated dataset contains 800 image pairs, 100 for each augmentation method and diffusion model combination. A total of 145 individuals contributed to this survey through Amazon Mechanical Turk, with an approximate compensation of $ \(15\) per hour. Quality was maintained through 25 pre-annotated image pairs that served as attention checks, as well as qualifications that were granted to a community of vetted annotators. Three unique annotations were collected per image pair, for which the Interclass Correlation (ICC2k) score across all image pairs is 0.835. The final human perception score was the mode of the set.

The motivation behind this experiment is to evaluate how various means of quantifying magnitude of change in an image relate to human perception. The results show that the Area Ratio metric, a contribution of this work, had the highest correlation with human annotation. However, its important to note that specific metrics, such as MSE RGB, are incredibly sensitive to the slightest of changes. Whereas high-performing generative models can alter images in a way that would not be perceived by the human eye. Therefore, some metrics used in Semi-Truths capture important information about image augmentation even if they do not correlate with human perception.

This user study was IRB Exempt, as it did not obtain/access any private, personally identifiable, or demographic data about the participants. The only information provided was a statement on perceived change between two images.

Perturbation Model Experiments

We considered different types of models to perform the perturbations in both the generated captions, and the mask label. We used both unimodal (LlaMA 2) and multimodal (LLaVA Mistral 7B, LLaVA Hermes 34B) models to perform these perturbations and compared their outputs to settle on our final pipeline.

We find that the performance of these models varied significantly across different perturbation methods. Initially, we used multimodal models to suggest perturbations, as they could process images as input and thereby capture contextual information that might be missed when only captions are provided. However, we observed that the LlaVA Mistral 7B model performed poorly in caption perturbation in the prompt-perturbation method. To address this, we scaled up the model size by incorporating LlaVA Hermes 34B into our pipeline. Surprisingly, we discovered that despite scaling up the best caption perturbations for prompt-perturbation were achieved using the LlaMA model instead. A comparison of the perturbed captions and the related generated images can be found in Fig.21. However we maintain LlaVA Mistral 7B as part of the inpainting pipeline as it shows best performance and also retains image context during suggesting perturbed labels for masks.

## Appendix F Evaluation Metrics

The choice of metrics plays an essential role when evaluating on an imbalanced dataset. Therefore, we report individual class Precision, Recall and F1, along with overall Precision, Recall and F1, to provide a more interpretable overview. The majority of our experiments emphasize Recall on the fake class, as the impact of various augmentations primarily affects this class while leaving real images largely unaffected. However, we recognize that additional metrics addressing class imbalance would offer a more comprehensive evaluation. Consequently, we have included AUC-ROC and AUC-PR curves for both the original and a balanced evaluation set (containing 27,000 real and fake images) for the experiments mentioned in Tab. 3.

Figure 21: Examples of perturbations made by LlaMA and LlaVA, and their resulting images.

## Appendix G Compute Requirements

We used A40 GPUs from internal university cluster to run the augmentation techniques. Each dataset and diffusion model variation for each augmentation technique used 1 A40 GPU to run. Each image augmentation took \(\tilde{2}\) minutes to generate for both prompt-based-edit and inpainting techniques.

## Appendix H License of Assets

1. LANCE [62]: Apache 2.0 license
2. Diffusers library (used for setting up inpainting pipeline and for all diffusion model inference) - Apache 2.0 license
3. LlaVa [50] - Apache 2.0 license
4. Llama [82] - Apache 2.0 license
5. UniversalFakeDetect [59] - No license
6. DIRE [88] - No license
7. DE-FAKE [73] - No license
8. DinoV2 [60] - Apache 2.0 license
9. CrossEfficientViT [11] - MIT License
10. CNNSpot [86] - Creative Commons Public License

Figure 23: Precision-Recall curve with AP and AUC-ROC for each detector evaluated on a balanced Semi-Truths Eval Dataset

Figure 22: Precision-Recall curve with AP and AUC-ROC for each detector evaluated on Semi-Truths Eval Dataset