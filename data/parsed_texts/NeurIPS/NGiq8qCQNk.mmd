# Zero-sum Polymatrix Markov Games: Equilibrium Collapse and Efficient Computation of Nash Equilibria

**Fivos Kalogiannis**

Department of Computer Science

University of California, Irvine

Irvine, CA

fkalogia@uci.edu &**Ioannis Panageas**

Department of Computer Science

University of California, Irvine

Irvine, CA

ipanagea@ics.uci.edu

###### Abstract

The works of (Daskalakis et al., 2009, 2022; Jin et al., 2022; Deng et al., 2023) indicate that computing Nash equilibria in multi-player Markov games is a computationally hard task. This fact raises the question of whether or not computational intractability can be circumvented if one focuses on specific classes of Markov games. One such example is two-player zero-sum Markov games, in which efficient ways to compute a Nash equilibrium are known. Inspired by zero-sum polymatrix normal-form games (Cai et al., 2016), we define a class of zero-sum multi-agent Markov games in which there are only pairwise interactions described by a graph that changes per state. For this class of Markov games, we show that an \(\epsilon\)-approximate Nash equilibrium can be found efficiently. To do so, we generalize the techniques of (Cai et al., 2016), by showing that the set of coarse-correlated equilibria collapses to the set of Nash equilibria. Afterwards, it is possible to use any algorithm in the literature that computes approximate coarse-correlated equilibria Markovian policies to get an approximate Nash equilibrium.

## 1 Introduction

Multi-agent reinforcement learning (MARL) is the discipline that is concerned with strategic interactions between agents who find themselves in a dynamically changing environment. Early aspects of MARL can be traced back to, as early as, the initial text on two-player zero-sum stochastic/Markov games (Shapley, 1953). Today, Markov games have been established as the theoretical framework for MARL (Littman, 1994). The connection between game theory and MARL has lead to several recent cornerstone results in benchmark domains in AI (Bowling et al., 2015; Brown and Sandholm, 2019, 2018; Brown et al., 2020; Silver et al., 2017; Moravcik et al., 2017; Perolat et al., 2022; Vinyals et al., 2019). The majority of the aforementioned breakthroughs relied on computing _Nash equilibria_(Nash, 1951) in a scalable and often decentralized manner. Although the theory of single agent reinforcement learning (RL) has witnessed an outstanding progress (_e.g._, see (Agarwal et al., 2020; Bertsekas, 2000; Jin et al., 2018; Li et al., 2021; Luo et al., 2019; Panait and Luke, 2005; Sidford et al., 2018; Sutton and Barto, 2018), and references therein, the landscape of multi-agent settings eludes a thorough understanding. In fact, guarantees for provably efficient computation of Nash equilibria remain limited to either environments in which agents strive to coordinate towards a shared goal (Chen et al., 2022; Claus and Boutilier, 1998; Ding et al., 2022; Fox et al., 2022; Leonardos et al., 2021; Maheshwari et al., 2022; Wang and Sandholm, 2002; Zhang et al., 2021) or fully competitive such as two-player zero-sum games (Cen et al., 2021; Condon, 1993; Daskalakis et al., 2020; Sayin et al., 2021, 2020; Wei et al., 2021) to name a few. Part of the lack of efficient algorithmic results in MARL is the fact that computing approximate Nash equilibria in (general-sum) games is computationally intractable (Daskalakis et al., 2009; Rubinstein, 2017; Chen et al., 2009; Etessami and Yannakakis, 2010) even when the games have a single state, _i.e._, normal-form two-player games.

We aim at providing a theoretical framework that captures an array of real-world applications with multiple agents -- which admittedly correspond to a big portion of all modern applications. A recent contribution that computes NE efficiently in a setting that combines both collaboration and competition, (Kalogiannis et al., 2022), concerns adversarial team Markov games, or competition between an adversary and a group of uncoordinated agents with common rewards. Efficient algorithms for computing Nash equilibria in settings that include both cooperation and competition are far fewer and tend to impose assumptions that are restrictive and difficult to meet in most applications (Bowling, 2000; Hu and Wellman, 2003). The focus of our work is centered around the following question:

_Are there any other settings of Markov games that encompass both competition and coordination while mantaining the tractability of Nash equilibrium computation?_

Inspired by contemporary works in algorithmic game theory and specifically zero-sum _polymatrix_ normal-form games (Cai et al., 2016), we focus on the problem of computing Nash equilibria in zero-sum _polymatrix Markov_ games. Informally, a polymatrix Markov game is a multi-agent Markov decision process with \(n\) agents, state-space \(\mathcal{S}\), action space \(\mathcal{A}_{k}\) for agent \(k\), a transition probability model \(\mathbb{P}\) and is characterized by a graph \(\mathcal{G}_{s}(\mathcal{V},\mathcal{E}_{s})\) which is potentially different in every state \(s\). For a fixed state \(s\), the nodes of the graph \(\mathcal{V}\) correspond to the agents, and the edges \(\mathcal{E}_{s}\) of the graph are two-player normal-form games (different per state). Every node/agent \(k\) has a fixed set of actions \(\mathcal{A}_{k}\), and chooses a strategy from this set to play in all games corresponding to adjacent edges. Given an action profile of all the players, the node's reward is the sum of its rewards in all games on the edges adjacent to it. The game is globally zero-sum if, for all strategy profiles, the rewards of all players add up to zero. Afterwards, the process transitions to a state \(s^{\prime}\) according to \(\mathbb{P}\). In a more high-level description, the agents interact over a network whose connections change at every state.

Our results.We consider a zero-sum polymatrix Markov game with the additional property that a single agent (not necessarily the same) controls the transition at each state, _i.e._, the transition model is affected by a single agent's actions for each state \(s\). These games are known as _switching controller_ Markov games. We show that we can compute in time \(\text{poly}(|\mathcal{S}|,n,\max_{i\in[n]}|\mathcal{A}_{i}|,1/\epsilon)\) an \(\epsilon\)-approximate Nash equilibrium. The proof relies on the fact that zero-sum polymatrix Markov games with a switching controller have the following important property: the marginals of a coarse-correlated equilibrium constitute a Nash equilibrium (see Section 3.2). We refer to this phenomenon as _equilibrium collapse_. This property was already known for zero-sum polymatrix normal-form games by Cai et al. (2016) and our results generalize the aforementioned work for Markov games. As a corollary, we get that any algorithm in the literature that guarantees convergence to approximate coarse-correlated equilibria Markovian policies--_e.g._, (Daskalakis et al., 2022)--can be used to get approximate Nash equilibria. Our contribution also unifies previous results that where otherwise only applicable to the settings of _single_ and _switching-control two-player zero-sum games_, or _zero-sum polymatrix normal-form games_. Finally, we show that the equilibrium collapsing phenomenon does not carry over if there are two or more controllers per state (see Section 3.3).

Technical overview.In order to prove our results, we rely on nonlinear programming and, in particular, nonlinear programs whose optima coincide with the Nash equilibria for a particular Markov game (Filar et al., 1991; Filar and Vrieze, 2012). Our approach is analogous to the one used by (Cai et al., 2016) which uses linear programming to prove the collapse of the set of CCE to the set of NE. Nevertheless, using the duality of linear programming in our case is not possible since a Markov game introduces nonlinear terms in the program. It is noteworthy that we do not need to invoke (Lagrangian) duality or an argument that relies on stationary points of a Lagrangian function. Rather, we use the structure of the zero-sum polymatrix Markov games with a switching controller to conclude the relation between a correlated policy and the individual policies formed by its marginals in terms of the individual utilities of the game.

### Importance of zero-sum polymatrix Markov games

Strategic interactions of agents over a network is a topic of research in multiple disciplines that span computer science (Easley and Kleinberg, 2010), economics (Schweitzer et al., 2009), control theory (Tipsuwan and Chow, 2003), and biology (Szabo and Fath, 2007) to name a few.

In many environments where multiple agents interact with each other, they do so in a localized manner. That is, every agent is affected by the set of agents that belong to their immediate "neighborhood". Further, it is quite common that these agents will interact independently with each one of their neighbors; meaning that the outcome of their total interactions is a sum of pairwise interactions rather than interactions that depend on joint actions. Finally, players might remain indifferent to actions of players are not their neighbors.

To illustrate this phenomenon we can think of multiplayer e-games (_e.g._, CS:GO, Fortnite, League of Legends, etc) where each player interacts through the same move only with players that are present on their premises and, in general, the neighbors cannot combine their actions into something that is not a mere sum of their individual actions (_i.e._, they rarely can "multiply" the effect of the individual actions). In other scenarios, such as strategic games played on social networks (_e.g._, opinion dynamics) agents clearly interact in a pairwise manner with agents that belong to their neighborhood and are somewhat oblivious to the actions of agents who they do not share a connection with.

With the proposed model we provide the theoretical framework needed to reason about such strategic interactions over dynamically changing networks.

### Related work

From the literature of Markov games, we recognize the settings of _single controller_(Filar and Raghavan, 1984; Sayin et al., 2022; Guan et al., 2016; Qiu et al., 2021) and _switching controller_(Vrieze et al., 1983) Markov games to be one of the most related to ours. In these settings, all agents' actions affect individual rewards, but in every state one particular player (_single controller_), or respectively a potentially different one (_switching controller_), controls the transition of the environment to a new state. To the best of our knowledge, prior to our work, the only Markov games that have been examined under this assumption are either zero-sum or potential games.

Further, we manage to go beyond the dichotomy of absolute competition or absolute collaboration by generalizing zero-sum polymatrix games to their Markovian counterpart. In this sense, our work is related to previous works of Cai et al. (2016); Anagnostides et al. (2022); Ao et al. (2022) which show fast convergence to Nash equilibria in zero-sum polymatrix normal-form games for various no-regret learning algorithms including optimistic gradient descent.

## 2 Preliminaries

Notation.We define \([n]\coloneqq\{1,\cdots,n\}\). Scalars are denoted using lightface variables, while, we use boldface for vectors and matrices. For simplicity in the exposition, we use \(O(\cdot)\) to suppress dependencies that are polynomial in the parameters of the game. Additionally, given a collection \(\bm{x}\) of policies or strategies for players \([n]\), \(\bm{x}_{-k}\) denotes the policies of every player excluding \(k\).

### Markov games

In its most general form, a Markov game (MG) with a finite number of \(n\) players is defined as a tuple \(\Gamma(H,\mathcal{S},\{\mathcal{A}_{k}\}_{k\in[n]},\mathbb{P},\{r_{k}\}_{k\in[ n]},\gamma,\bm{\rho})\). Namely,

* \(H\in\mathbb{N}_{+}\) denotes the _time horizon_, or the length of each episode,
* \(\mathcal{S}\), with cardinality \(S\coloneqq|\mathcal{S}|\), stands for the state space,
* \(\{\mathcal{A}_{k}\}_{k\in[n]}\) is the collection of every player's action space, while \(\mathcal{A}\coloneqq\mathcal{A}_{1}\times\cdots\times\mathcal{A}_{n}\) denotes the _joint action space_; further, an element of that set --a joint action-- is generally noted as \(\bm{a}=(a_{1},\ldots,a_{n})\in\mathcal{A}\),
* \(\mathbb{P}\coloneqq\{\mathbb{P}_{k}\}_{h\in[H]}\) is the set of all _transition matrices_, with \(\mathbb{P}_{h}:\mathcal{S}\times\mathcal{A}\rightarrow\Delta(\mathcal{S})\); further, \(\mathbb{P}_{h}(\cdot|s,\bm{a})\) marks the probability of transitioning to every state given that the joint action \(\bm{a}\) is selected at time \(h\) and state \(s\) -- in infinite-horizon games \(\mathbb{P}\) does not depend on \(h\) and the index is dropped,
* \(r_{k}\coloneqq\{r_{k,h}\}\) is the reward function of player \(k\) at time \(h\); \(r_{k,h}:\mathcal{S},\mathcal{A}\rightarrow[-1,1]\) yields the reward of player \(k\) at a given state and joint action -- in infinite-horizon games, \(r_{k,h}\) is the same for every \(h\) and the index is dropped,
* a discount factor \(\gamma>0\), which is generally set to \(1\) when \(H<\infty\), and \(\gamma<1\) when \(H\rightarrow\infty\),
* an initial state distribution \(\bm{\rho}\in\Delta(\mathcal{S})\).

Policies and value functions.We will define stationary and nonstationary Markov policies. When the horizon \(H\) is finite, a stationary policy equilibrium need not necessarily exist even for a single-agent MG, _i.e._, a Markov decision process; in this case, we seek nonstationary policies. For the case of infinite-horizon games, it is folklore that a stationary Markov policy Nash equilibrium always exists.

We note that a policy is _Markovian_ when it depends on the present state only. A _nonstationary_ Markov policy \(\bm{\pi}_{k}\) for player \(k\) is defined as \(\bm{\pi}_{k}\coloneqq\{\bm{\pi}_{k,h}:\mathcal{S}\rightarrow\Delta(\mathcal{A }_{k}),\ \forall h\in[H]\}\). It is a sequence of mappings of states \(s\) to a distribution over actions \(\Delta(\mathcal{A}_{k})\) for every timestep \(h\). By \(\bm{\pi}_{k,h}(a|s)\) we will denote the probability of player \(k\) taking action \(a\) in timestep \(h\) and state \(s\). A Markov policy is said to be _stationary_ in the case that it outputs an identical probability distribution over actions whenever a particular state is visited regardless of the corresponding timestep \(h\).

Further, we define a nonstationary Markov _joint policy_\(\bm{\sigma}\coloneqq\{\bm{\pi}_{h},\ \forall h\in[H]\}\) to be a sequence of mappings from states to distributions over joint actions \(\Delta(\mathcal{A})\equiv\Delta(\mathcal{A}_{1}\times\cdots\times\mathcal{A }_{n})\) for all times steps \(h\) in the time horizon. In this case, the players can be said to share a common source of randomness, or that the joint policy is correlated.

A joint policy \(\bm{\pi}\) will be said to be a _product policy_ if there exist policies \(\bm{\pi}_{k}:[H]\times\mathcal{S}\rightarrow\Delta(\mathcal{A}_{k}),\ \forall k \in[n]\) such that \(\bm{\pi}_{h}=\bm{\pi}_{1,h}\times\cdots\times\bm{\pi}_{n,h},\ \forall h\in[H]\). Moreover, given a joint policy \(\bm{\pi}\) we let a joint policy \(\bm{\pi}_{-k}\) stand for the _marginal joint policy_ excluding player \(k\), _i.e._,

\[\pi_{-k,h}(\bm{a}|s)=\sum_{a^{\prime}\in\mathcal{A}_{k}}\pi_{h}(a^{\prime},\bm {a}|s),\ \forall h\in[H],\forall s\in\mathcal{S},\forall\bm{a}\in\mathcal{A}_{-k}.\]

By fixing a joint policy \(\bm{\pi}\) we can define the value function of any given state \(s\) and timestep \(h\) for every player \(k\) as the expected cumulative reward they get from that state and timestep \(h\) onward,

\[V^{\bm{\pi}}_{k,h}(s_{1})=\mathbb{E}_{\bm{\pi}}\left[\sum_{\tau=h}^{H}\gamma^{ \tau-1}r_{k,\tau}(s_{\tau},\bm{a}_{\tau})\big{|}s_{1}\right]=\bm{e}_{s_{1}}^{ \top}\sum_{\tau=h}^{H}\left(\gamma^{\tau-1}\prod_{\tau=h}^{h}\mathbb{P}_{\tau }(\bm{\pi}_{\tau})\right)\bm{r}_{k,\tau}(\bm{\pi}_{\tau}).\]

Depending on whether the game is of finite or infinite horizon we get the followin displays,

* In finite-horizon games, \(\gamma=1\), the value function reads, \[V^{\bm{\pi}}_{k,h}(s_{1})=\bm{e}_{s_{1}}^{\top}\sum_{\tau=h}^{H}\left(\prod_{ \tau^{\prime}=h}^{\tau}\mathbb{P}_{\tau^{\prime}}(\bm{\pi}_{\tau^{\prime}}) \right)\bm{r}_{k,\tau}(\bm{\pi}_{\tau}), V^{\bm{\pi}}_{k}(s_{1})=\bm{e}_{s_{1}}^{\top}\left(\mathbf{I}- \gamma\,\mathbb{P}(\bm{\pi})\right)^{-1}\bm{r}(\bm{\pi}).\]

Where \(\mathbb{P}_{h}(\bm{\pi}_{h}),\mathbb{P}(\bm{\pi})\) and \(\bm{r}_{h}(\bm{\pi}_{h}),\bm{r}(\bm{\pi})\) denote the state-to-state transition probability matrix and expected per-state reward vector for a given policy \(\bm{\pi}_{h}\) or \(\bm{\pi}\) accordingly. Additionally, \(\bm{e}_{s_{1}}\) is an all-zero vector apart of a value of \(1\) in its \(s_{1}\)-th position. Also, we denote \(V^{\bm{\pi}}_{k,h}(\bm{\rho})=\sum_{s\in\mathcal{S}}\rho(s)V^{\bm{\pi}}_{k,h}(s)\).

Best-response policies.Given an arbitrary joint policy \(\bm{\sigma}\), we define the _best-response policy_ of a player \(k\) to be a policy \(\bm{\pi}_{k}^{\dagger}\coloneqq\{\bm{\pi}_{k,h}^{\dagger},\ \forall h\in[H]\}\), such that it is a maximizer of \(\max_{\bm{\pi}_{k}^{\prime}}V^{\bm{\pi}_{k}\times\bm{\sigma}_{-k}}_{k,1}(s_{1})\). Additionally, we will use the following notation \(V^{\dagger,\bm{\sigma}_{-k}}_{k,h}(s)\coloneqq\max_{\bm{\pi}_{k}^{\prime}}V^{ \bm{\pi}_{k}^{\prime}\times\bm{\sigma}_{-k}}_{k,h}(s)\).

Equilibrium notions.Having defined what a best-response is, it is then quite direct to define different notions of equilibria for Markov games.

**Definition 2.1** (Cc).: _We will say that a joint (potentially correlated) policy \(\bm{\sigma}\in\Delta(\mathcal{A})^{H\times S}\) is an \(\epsilon\)-approximate coarse-correlated equilibrium if it holds that, for an \(\epsilon>0\),_

\[V^{\dagger,\bm{\sigma}_{-k}}_{k,1}(s_{1})-V^{\bm{\sigma}}_{k,1}(s_{1})\leq \epsilon,\ \forall k\in[n].\] (CCE)

Further, we will define a Nash equilibrium policy,

**Definition 2.2** (Ne).: _A joint, product policy \(\bm{\pi}\in\prod_{k\in[n]}\Delta(\mathcal{A}_{k})^{H\times S}\) is an \(\epsilon\)-approximate Nash equilibrium if it holds that, for an \(\epsilon>0\),_

\[V^{\dagger,\bm{\pi}_{-k}}_{k,1}(s_{1})-V^{\bm{\pi}}_{k,1}(s_{1})\leq\epsilon,\ \forall k\in[n].\] (NE)

It is quite evident that an approximate Nash equilibrium is also an approximate coarse-correlated equilibrium while the converse is not generally true. For infinite-horizon games the definitions are analogous and are deferred to the appendix.

### Our setting

We focus on the setting of zero-sum polymatrix switching-control Markov games. This setting encompasses two major assumptions related to the reward functions in every state \(\{r_{k}\}_{k\in[n]}\) and the transition kernel \(\mathbb{P}\). The first assumption imposes a zero-sum, polymatrix structure on \(\{r_{k}\}_{k\in[n]}\) for every state and directly generalizes zero-sum polymatrix games for games with multiple states.

**Assumption 1** (Zero-sum polymatrix games).: The reward functions of every player in any state \(s\) are characterized by a _zero-sum_, _polymatrix_ structure.

Polymatrix structure.For every state \(s\) there exists an undirected graph \(\mathcal{G}_{s}(\mathcal{V},\mathcal{E}_{s})\) where,

* the set of nodes \(\mathcal{V}\) coincides with the set of agents \([n]\); the \(k\)-th node is the \(k\)-th agent,
* the set of edges \(\mathcal{E}_{s}\) stands for the set of pair-wise interactions; each edge \(e=(k,j),k,j\in[n],k\neq j\) stands for a general-sum normal-form game played between players \(k,j\) and which we note as \(\left(r_{kj}(s,\cdot,\cdot),r_{jk}(s,\cdot,\cdot)\right)\) with \(r_{kj},r_{jk}:\mathcal{S}\times\mathcal{A}_{k}\times\mathcal{A}_{j}\to[-1,1]\).

Moreover, we define \(\operatorname{adj}(s,k)\coloneqq\{j\in[n]\mid(k,j)\in\mathcal{E}_{s}\} \subseteq[n]\) to be the set of all neighbors of an arbitrary agent \(k\) in state \(s\). The reward of agent \(k\) at state \(s\) given a joint action \(\bm{a}\) depends solely on interactions with their neighbors,

\[r_{k,h}(s,\bm{a})=\sum_{j\in\operatorname{adj}(k)}r_{kj,h}(s,a_{k},a_{j}),\; \forall h\in[H],\forall s\in\mathcal{S},\forall\bm{a}\in\mathcal{A}.\]

Further, the _zero-sum_ assumption implies that,

\[\sum_{k}r_{k,h}(s,\bm{a})=0,\quad\forall h\in[H],\forall s\in\mathcal{S}, \forall\bm{a}\in\mathcal{A}.\] (1)

In the infinite-horizon setting, the subscript \(h\) can be dropped.

A further assumption (_switching-control_) is necessary in order to ensure the desirable property of equilibrium collapse.

**Assumption 2** (Switching-control).: In every state \(s\in\mathcal{S}\), there exists a single player (not necessarily the same), or _controller_, whose actions determine the probability of transitioning to a new state.

The function \(\operatorname{argctrl}:\mathcal{S}\to[n]\) returns the index of the player who controls the transition probability at a given state \(s\). On the other hand, the function \(\operatorname{ctrl}:\mathcal{S}\times\mathcal{A}\to\mathcal{A}_{ \operatorname{argctrl}(s)}\) gets an input of a joint action \(\bm{a}\), for a particular state \(s\), and returns the action of the controller of that state, \(a_{\operatorname{argctrl}(s)}\).

**Remark 1**.: _It is direct to see that Markov games with a single controller and turn-based Markov games (Daskalakis et al., 2022), are special case of Markov games with switching controller._

## 3 Main results

In this section we provide the main results of this paper. We shall show the collapsing phenomenon of coarse-correlated equilibria to Nash equilibria in the case of zero-sum, single switching controller polymatrix Markov games. Before we proceed, we provide a formal definition of the notion of collapsing.

**Definition 3.1** (CCE collapse to NE).: _Let \(\bm{\sigma}\) be any \(\epsilon\)-CCE policy of a Markov game. Moreover, let the marginal policy \(\bm{\pi}^{\bm{\sigma}}:=(\bm{\pi}_{1}^{\bm{\sigma}},...,\bm{\pi}_{n}^{\bm{ \sigma}})\) be defined as:_

\[\pi_{k}^{\bm{\sigma}}(a|s)=\sum_{\bm{a}_{-k}\in\mathcal{A}_{-k}}\sigma(a,\bm{a }_{-k}|s),\;\forall k,\forall s\in\mathcal{S},\forall a\in\mathcal{A}_{k}.\]

_If \(\bm{\pi}^{\bm{\sigma}}\) is a \(O(\epsilon)\)-NE equilibrium for every \(\bm{\sigma}\) then we say the set of approximate CCE's collapses to that of approximate NE's._

We start with the warm-up result that the set of CCE's collapses to the set of NE's for two-player zero-sum Markov games.

### Warm-up: equilibrium collapse in two-player zero-sum MG's

Since we focus on two-player zero-sum Markov games, we simplify the notation by using \(V_{h=1}^{\cdot}(s):=V_{2,1}^{\cdot}(s)\)--_i.e._, player \(1\) is the minimizing player and player \(2\) is the maximizer. We show the following theorem:

**Theorem 3.1** (Collapse in two-player zero-sum MG's).: _Let a two-player zero-sum Markov game \(\Gamma^{\prime}\) and an \(\epsilon\)-approximate CCE policy of that game \(\bm{\sigma}\). Then, the marginalized product policies \(\bm{\pi}_{1}^{\bm{\sigma}},\bm{\pi}_{2}^{\bm{\sigma}}\) form a \(2\epsilon\)-approximate NE._

Proof.: Since \(\bm{\sigma}\) is an \(\epsilon\)-approximate CCE joint policy, by definition it holds that for any \(\bm{\pi}_{1}\) and any \(\bm{\pi}_{2}\),

\[V_{h=1}^{\bm{\sigma}_{-2}\times\bm{\pi}_{2}}(s_{1})-\epsilon\leq V _{h=1}^{\bm{\sigma}}(s_{1})\leq V_{h=1}^{\bm{\pi}_{1}\times\bm{\sigma}_{-1}}( s_{1})+\epsilon.\]

Due to Claim A.1, the latter is equivalent to the following inequality,

\[V_{h=1}^{\bm{\pi}_{1}^{\bm{\sigma}_{1}}\times\bm{\pi}_{2}}(s_{1})-\epsilon \leq V_{h=1}^{\bm{\sigma}}(s_{1})\leq V_{h=1}^{\bm{\pi}_{1}\times\bm{\pi}_{2} ^{\bm{\sigma}}}(s_{1})+\epsilon.\]

Plugging in \(\bm{\pi}_{1}^{\sigma},\bm{\pi}_{2}^{\sigma}\) alternatingly, we get the inequalities:

\[\begin{cases}V_{h=1}^{\bm{\pi}_{1}^{\bm{\sigma}_{1}}\times\bm{\pi}_{2}}(s_{1}) -\epsilon\leq V_{h=1}^{\bm{\sigma}}(s_{1})\leq V_{h=1}^{\bm{\pi}_{1}^{\bm{ \sigma}_{1}}\times\bm{\pi}_{2}^{\bm{\sigma}}}(s_{1})+\epsilon\\ V_{h=1}^{\bm{\pi}_{1}^{\bm{\sigma}_{1}}\times\bm{\pi}_{2}^{\bm{\sigma}}}(s_{1}) -\epsilon\leq V_{h=1}^{\bm{\sigma}_{1}}(s_{1})\leq V_{h=1}^{\bm{\pi}_{1}\times \bm{\pi}_{2}^{\bm{\sigma}}}(s_{1})+\epsilon\end{cases}\]

The latter leads us to conclude that for any \(\bm{\pi}_{1}\) and any \(\bm{\pi}_{2}\),

\[V_{h=1}^{\bm{\pi}_{1}^{\bm{\sigma}_{1}}\times\bm{\pi}_{2}}(s_{1})-2\epsilon \leq V_{h=1}^{\bm{\pi}_{1}^{\bm{\sigma}_{1}}\times\bm{\pi}_{2}^{\bm{\sigma}}} (s_{1})\leq V_{h=1}^{\bm{\pi}_{1}\times\bm{\pi}_{2}^{\bm{\sigma}}}(s_{1})+2\epsilon,\]

which is the definition of a NE in a zero-sum game. 

### Equilibrium collapse in finite-horizon polymatrix Markov games

In this section, we focus on the more challenging case of polymatrix Markov games which is the main focus of this paper. For any finite horizon Markov game, we define (\(\mathrm{P}_{\mathrm{NE}}\)) to be the following nonlinear program with variables \(\bm{\pi},\bm{w}\):

(P \[{}_{\mathrm{NE}}\] ) \[\begin{split}\min&\sum\limits_{k\in[n]}\left(w_{k,1} (s_{1})-\bm{e}_{s_{1}}^{\top}\sum\limits_{h=1}^{H}\left(\prod\limits_{\tau=1}^ {h}\mathbb{P}_{\tau}(\bm{\pi}_{\tau})\right)\bm{r}_{k,h}(\bm{\pi}_{h})\right) \\ \mathrm{s.t.}& w_{k,h}(s)\geq r_{k,h}(s,a,\bm{\pi}_{ -k,h})+\mathbb{P}_{h}(s,a,\bm{\pi}_{-k,h})\bm{w}_{k,h+1},\\ &\qquad\forall s\in\mathcal{S},\forall h\in[H],\forall k\in[n], \forall a\in\mathcal{A}_{k};\\ &\qquad w_{k,H}(s)=0,\quad\forall k\in[n],\forall s\in\mathcal{S}; \\ &\qquad\bm{\pi}_{k,h}(s)\in\Delta(\mathcal{A}_{k}),\\ &\qquad\forall s\in\mathcal{S},\forall h\in[H],\forall k\in[n], \forall a\in\mathcal{A}_{k}.\end{split}\]

Using the following theorem, we are able to use (\(\mathrm{P}_{\mathrm{NE}}\)) to argue about equilibrium collapse.

**Theorem 3.2** (NE and global optima of (\(\mathrm{P}_{\mathrm{NE}}\))).: _If \((\bm{\pi}^{\star},\bm{w}^{\star})\) yields an \(\epsilon\)-approximate global minimum of (\(\mathrm{P}_{\mathrm{NE}}\)), then \(\bm{\pi}^{\star}\) is an \(n\epsilon\)-approximate NE of the zero-sum polymatrix switching controller MG, \(\Gamma\). Conversely, if \(\bm{\pi}^{\star}\) is an \(\epsilon\)-approximate NE of the MG \(\Gamma\) with corresponding value function vector \(\bm{w}^{\star}\) such that \(w_{k,h}^{\star}(s)=V_{k,h}^{\bm{\pi}_{h}^{\star}}(s)\forall(k,h,s)\in[n]\times[H] \times\mathcal{S}\), then \((\bm{\pi}^{\star},\bm{w}^{\star})\) attains an \(\epsilon\)-approximate global minimum of (\(\mathrm{P}_{\mathrm{NE}}\))._

Following, we are going to use (\(\mathrm{P}_{\mathrm{NE}}\)) in proving the collapse of CCE's to NE's. We observe that the latter program is nonlinear and in general nonconvex. Hence, duality cannot be used in the way it was used in (Cai et al., 2016) to prove equilibrium collapse. Nevertheless, we can prove that given a CCE policy \(\bm{\sigma}\), the marginalized, product policy \(\bigtimes_{k\in[n]}\bm{\pi}_{k}^{\bm{\sigma}}\) along with an appropriate vector \(\bm{w}^{\sigma}\) achieves a global minimum in the nonlinear program (\(\mathrm{P}_{\mathrm{NE}}\)). More precisely, our main result reads as the following statement.

**Theorem 3.3** (CCE collapse to NE in polymatrix MG).: _Let a zero-sum polymatrix switching-control Markov game, i.e., a Markov game for which Assumptions 1 and 2 hold. Further, let an \(\epsilon\)-approximate CCE of that game \(\bm{\sigma}\). Then, the marginal product policy \(\bm{\pi}^{\sigma}\), with \(\bm{\pi}^{\sigma}_{k,h}(a|s)=\sum_{\bm{a}_{-k}\in\mathcal{A}_{-k}}\bm{\sigma}_ {h}(a,\bm{a}_{-k}),\ \forall k\in[n],\forall h\in[H]\) is an \(n\epsilon\)-approximate NE._

**Proof.** Let an \(\epsilon\)-approximate CCE policy, \(\bm{\sigma}\), of game \(\Gamma\). Moreover, let the best-response value-vectors of each agent \(k\) to joint policy \(\bm{\sigma}_{-k},\bm{w}^{\dagger}_{k}\).

Now, we observe that due to Assumption 1,

\[w^{\dagger}_{k,h}(s) \geq r_{k,h}(s,a,\bm{\sigma}_{-k,h})+\mathbb{P}_{h}(s,a,\bm{ \sigma}_{-k,h})\bm{w}^{\dagger}_{k,h+1}\] \[=\sum_{j\in\mathrm{adj}(k)}r_{(k,j),h}(s,a,\bm{\pi}^{\bm{\sigma} }_{j})+\mathbb{P}_{h}(s,a,\bm{\sigma}_{-k,h})\bm{w}^{\dagger}_{k,h+1}.\]

Further, due to Assumption 2,

\[\mathbb{P}_{h}(s,a,\bm{\sigma}_{-k,h})\bm{w}^{\dagger}_{k,h+1}= \mathbb{P}_{h}(s,a,\bm{\pi}^{\bm{\sigma}}_{\mathrm{arctrl}(s),h})\bm{w}^{ \dagger}_{k,h+1},\]

or,

\[\mathbb{P}_{h}(s,a,\bm{\sigma}_{-k,h})\bm{w}^{\dagger}_{k,h+1}= \mathbb{P}_{h}(s,a,\bm{\pi}^{\bm{\sigma}})\bm{w}^{\dagger}_{k,h+1}.\]

Putting these pieces together, we reach the conclusion that \((\bm{\pi}^{\sigma},\bm{w}^{\dagger})\) is feasible for the nonlinear program (\(\mathrm{P}_{\mathrm{NE}}\)).

What is left is to prove that it is also an \(\epsilon\)-approximate global minimum. Indeed, if \(\sum_{k}\bm{w}^{\dagger}_{k,h}(s_{1}){\leq}\epsilon\) (by assumption of an \(\epsilon\)-approximate CCE), then the objective function of (\(\mathrm{P}_{\mathrm{NE}}\)) will attain an \(\epsilon\)-approximate global minimum. In turn, due to Theorem 3.2 the latter implies that \(\bm{\pi}^{\bm{\sigma}}\) is an \(n\epsilon\)-approximate NE. \(\square\)

We can now conclude that due to the algorithm introduced in (Daskalakis et al., 2022) for CCE computation in general-sum MG's, the next statement holds true.

**Corollary 3.1** (Computing a NE--finite-horizon).: Given a finite-horizon switching control zero-sum polymatrix Markov game, we can compute an \(\epsilon\)-approximate Nash equilibrium policy that is Markovian with probability at least \(1-\delta\) in time \(\mathrm{poly}\left(n,H,S,\max_{k}|\mathcal{A}_{k}|,\frac{1}{\epsilon},\log(1/ \delta)\right)\).

In the next section, we discuss the necessity of the assumption of switching control using a counter-example of non-collapsing equilibria.

### No equilibrium collapse with more than one controllers per-state

Although Assumption 1 is sufficient for the collapse of any CCE to a NE in single-state (_i.e._, normal-form) games, we will prove that Assumption 2 is indispensable in guaranteeing such a collapse in zero-sum polymatrix Markov games. That is, if more than one players affect the transition probability from one state to another, a CCE is not guaranteed to collapse to a NE.

**Example 1**.: _We consider the following \(3\)-player Markov game that takes place for a time horizon \(H=3\). There exist three states, \(s_{1},s_{2},\) and \(s_{3}\) and the game starts at state \(s_{1}\). Player \(3\) has a single action in every state, while players \(1\) and \(2\) have two available actions \(\{a_{1},a_{2}\}\) and \(\{b_{1},b_{2}\}\) respectively in every state._

Reward functions._If player \(1\) (respectively, player \(2\)) takes action \(a_{1}\) (resp., \(b_{1}\)), in either of the states \(s_{1}\) or \(s_{2}\), they get a reward equal to \(\frac{1}{20}\). In state \(s_{3}\), both players get a reward equal to \(-\frac{1}{2}\) regardless of the action they select. Player \(3\) always gets a reward that is equal to the negative sum of the reward of the other two players. This way, the zero-sum polymatrix property of the game is ensured (Assumption 1)._Transition probabilities._If players \(1\) and \(2\) select the joint action \((a_{1},b_{1})\) in state \(s_{1}\), the game will transition to state \(s_{2}\). In any other case, it will transition to state \(s_{3}\). The converse happens if in state \(s_{2}\) they take joint action \((a_{1},b_{1})\); the game will transition to state \(s_{3}\). For any other joint action, it will transition to state \(s_{1}\). From state \(s_{3}\), the game transitions to state \(s_{1}\) or \(s_{2}\) uniformly at random._

_At this point, it is important to notice that two players control the transition probability from one state to another. In other words, Assumption 2 does not hold._

_Next, we consider the joint policy \(\bm{\sigma}\),_

\[\bm{\sigma}(s_{1})=\bm{\sigma}(s_{2})=\begin{smallmatrix}&b_{1}&b_{2}\\ &a_{1}&0&1/2\\ &a_{2}&\left(1/2&0\right).\end{smallmatrix}\]

**Claim 3.1**.: The joint policy \(\bm{\sigma}\) that assigns probability \(\frac{1}{2}\) to the joint actions \((a_{1},b_{2})\) and \((a_{2},b_{1})\) in both states \(s_{1},s_{2}\) is a CCE and \(V^{\bm{\sigma}}_{1,1}(s_{1})=V^{\bm{\sigma}}_{2,1}(s_{1})=\frac{1}{20}\).

_Yet, the marginalized product policy of \(\bm{\sigma}\) which we note as \(\bm{\pi}_{1}^{\bm{\sigma}}\times\bm{\pi}_{2}^{\bm{\sigma}}\) does not constitute a NE. The components of this policy are,_

\[\begin{cases}\bm{\pi}_{1}^{\bm{\sigma}}(s_{1})=\bm{\pi}_{1}^{\bm{\sigma}}(s_{ 2})=\begin{smallmatrix}a_{1}&a_{2}\\ \left(1/2&1/2\right),\\ &\\ \bm{\pi}_{2}^{\bm{\sigma}}(s_{1})=\bm{\pi}_{2}^{\bm{\sigma}}(s_{2})=\begin{smallmatrix} b_{1}&b_{2}\\ \left(1/2&1/2\right).\end{smallmatrix}\end{cases}\end{cases}\]

_I.e., the product policy \(\bm{\pi}_{1}^{\bm{\sigma}}\times\bm{\pi}_{2}^{\bm{\sigma}}\) selects any of the two actions of each player in states \(s_{1},s_{2}\) independently and uniformly at random. With the following claim, it can be concluded that in general when more than one player control the transition the set of equilibria do not collapse._

**Claim 3.2**.: The product policy \(\bm{\pi}_{1}^{\bm{\sigma}}\times\bm{\pi}_{2}^{\bm{\sigma}}\) is not a NE.

_In conclusion, Assumption 1 does not suffice to ensure equilibrium collapse._

**Theorem 3.4**.: _There exists a zero-sum polymatrix Markov game (Assumption 2 is not satisfied) that has a CCE which does not collapse to a NE._

### Equilibrium collapse in infinite-horizon polymatrix Markov games

In proving equilibrium collapse for infinite-horizon polymatrix Markov games, we use similar arguments and the following nonlinear program with variables \(\bm{\pi},\bm{w}\),

Figure 1: A graph of the state space with transition probabilities parametrized with respect to the policy of each player.

\[\min \sum_{k\in[n]}\boldsymbol{\rho}^{\top}\left(\boldsymbol{w}_{k}-( \mathbf{I}-\gamma\,\mathbb{P}(\boldsymbol{\pi}))^{-1}\boldsymbol{r}_{k}( \boldsymbol{\pi})\right)\] \[\mathrm{s.t.} w_{k}(s)\geq r_{k}(s,a,\boldsymbol{\pi}_{-k})+\gamma\,\mathbb{P}(s,a, \boldsymbol{\pi}_{-k})\boldsymbol{w}_{k},\] \[\forall s\in\mathcal{S},\forall k\in[n],\forall a\in\mathcal{A}_ {k};\] \[\boldsymbol{\pi}_{k}(s)\in\Delta(\mathcal{A}_{k}),\] \[\forall s\in\mathcal{S},\forall k\in[n],\forall a\in\mathcal{A}_ {k}.\]

We note that Example 1 can be properly adjusted to show that the switching-control assumption is necessary for equilibrium collapse in infinite-horizon games as well. Compared to finite-horizon games, infinite-horizon games cannot be possibly solved using backward induction. They pose a genuine computational challenge and, in that sense, the importance of the property of equilibrium collapse gets highlighted.

Computational implications.Equilibrium collapse in infinite-horizon MG's allows us to use the CCE computation technique found in (Daskalakis et al., 2022) in order to compute an \(\epsilon\)-approximate NE. Namely, given an accuracy threshold \(\epsilon\), we truncate the infinite-horizon game to its _effective horizon_\(H\coloneqq\frac{\log(1/\epsilon)}{1-\gamma}\). Then, we define reward functions that depend on the time-step \(h\), _i.e._, \(r_{k,h}=\gamma^{h-1}r_{k}\). Finally,

**Corollary 3.2**.: (Computing a NE--infinite-horizon) Given an infinite-horizon switching control zero-sum polymatrix game \(\Gamma\), it is possible to compute a Nash equilibrium policy that is Markovian and nonstationary with probability at least \(1-\delta\) in time \(\mathrm{poly}\left(n,\frac{1}{1-\gamma},S,\max_{k}|\mathcal{A}_{k}|,\frac{1} {\epsilon},\log(1/\delta)\right)\).

## 4 Conclusion and open problems

In this paper, we unified switching-control Markov games and zero-sum polymatrix normal-form games. We highlighted how numerous applications can be modeled using this framework and we focused on the phenomenon of equilibrium collapse from the set of coarse-correlated equilibria to that of Nash equilibria. This property holds implications for computing approximate Nash equilibria in switching control zero-sum polymatrix Markov games; it ensures that it can be done efficiently.

Open problems.In light of the proposed problem and our results there are multiple interesting open questions:

* Is it possible to use a policy optimization algorithm similar to those of (Erez et al., 2022; Zhang et al., 2022) in order to converge to an approximate Nash equilibrium? We note that the question can be settled in one of two ways; _either_ extend the current result of equilibrium collapse to policies that are non-Markovian _or_ guarantee convergence to Markovian policies. The notion of _regret_ in (Erez et al., 2022) gives rise to the computation of a CCE that is a non-Markovian policy in the sense that the policy at every timestep depends on the policy sampled from the history of no-regret play and not only the given state.
* We conjecture that a convergence rate of \(O(\frac{1}{T})\) to a NE is possible, _i.e._, there exists an algorithm with running time \(O(1/\epsilon)\) that computes an \(\epsilon\)-approximate NE.
* Are there more classes of Markov games in which computing Nash equilibria is computationally tractable?

## References

* Agarwal et al. (2020) Alekh Agarwal, Sham M. Kakade, Jason D. Lee, and Gaurav Mahajan. Optimality and approximation with policy gradient methods in markov decision processes. In _Conference on Learning Theory, COLT 2020, 9-12 July 2020_, volume 125 of _Proceedings of Machine Learning Research_, pages 64-66. PMLR, 2020.
* Anagnostides et al. (2022) Ioannis Anagnostides, Ioannis Panageas, Gabriele Farina, and Tuomas Sandholm. On last-iterate convergence beyond zero-sum games. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, _International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA_, volume 162 of _Proceedings of Machine Learning Research_, pages 536-581. PMLR, 2022. URL https://proceedings.mlr.press/v162/anagnostides22a.html.
* Ao et al. (2022) Ruicheng Ao, Shicong Cen, and Yuejie Chi. Asynchronous gradient play in zero-sum multi-agent games, 2022.
* Bertsekas (2000) D. P. Bertsekas. _Dynamic Programming and Optimal Control_. Athena Scientific, 2nd edition, 2000. ISBN 1886529094.
* Bowling (2000) Michael Bowling. Convergence problems of general-sum multiagent reinforcement learning. In _In Proceedings of the Seventeenth International Conference on Machine Learning_, pages 89-94. Morgan Kaufmann, 2000.
* Bowling et al. (2015) Michael Bowling, Neil Burch, Michael Johanson, and Oskari Tammelin. Heads-up limit hold'em poker is solved. _Science_, 347(6218):145-149, 2015. doi: 10.1126/science.1259433.
* Brown and Sandholm (2018) Noam Brown and Tuomas Sandholm. Superhuman ai for heads-up no-limit poker: Libratus beats top professionals. _Science_, 359(6374):418-424, 2018. doi: 10.1126/science.aao1733.
* Brown and Sandholm (2019) Noam Brown and Tuomas Sandholm. Superhuman ai for multiplayer poker. _Science_, 365(6456):885-890, 2019. doi: 10.1126/science.aay2400.
* Brown et al. (2020) Noam Brown, Anton Bakhtin, Adam Lerer, and Qucheng Gong. Combining deep reinforcement learning and search for imperfect-information games. In _Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020_, 2020.
* Cai et al. (2016) Yang Cai, Ozan Candogan, Constantinos Daskalakis, and Christos Papadimitriou. Zero-sum polymatrix games: A generalization of minmax. _Mathematics of Operations Research_, 41(2):648-655, 2016.
* Cen et al. (2021) Shicong Cen, Yuting Wei, and Yuejie Chi. Fast policy extragradient methods for competitive games with entropy regularization. In _Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021_, pages 27952-27964, 2021.
* Chen et al. (2022) Dingyang Chen, Qi Zhang, and Thinh T. Doan. Convergence and price of anarchy guarantees of the softmax policy gradient in markov potential games. In _Decision Awareness in Reinforcement Learning Workshop at ICML 2022_, 2022.
* Chen et al. (2009) Xi Chen, Xiaotie Deng, and Shang-Hua Teng. Settling the complexity of computing two-player nash equilibria. _J. ACM_, 56(3):14:1-14:57, 2009. doi: 10.1145/1516512.1516516.
* Claus and Boutilier (1998) Caroline Claus and Craig Boutilier. The dynamics of reinforcement learning in cooperative multiagent systems. In _Proceedings of the Fifteenth National Conference on Artificial Intelligence and Tenth Innovative Applications of Artificial Intelligence Conference, AAAI 98_, pages 746-752. AAAI Press / The MIT Press, 1998.
* Condon (1993) Anne Condon. On algorithms for simple stochastic games. In _Advances in Computational Complexity Theory, volume 13 of DIMACS Series in Discrete Mathematics and Theoretical Computer Science_, pages 51-73. American Mathematical Society, 1993.
* Condon et al. (1993)Qiwen Cui, Kaiqing Zhang, and Simon Du. Breaking the curse of multiagents in a large state space: Rl in markov games with independent linear function approximation. In _The Thirty Sixth Annual Conference on Learning Theory_, pages 2651-2652. PMLR, 2023.
* Daskalakis et al. (2009) Constantinos Daskalakis, Paul W Goldberg, and Christos H Papadimitriou. The complexity of computing a nash equilibrium. _SIAM Journal on Computing_, 39(1):195-259, 2009.
* Daskalakis et al. (2020) Constantinos Daskalakis, Dylan J Foster, and Noah Golowich. Independent policy gradient methods for competitive reinforcement learning. _Advances in neural information processing systems_, 33:5527-5540, 2020.
* Daskalakis et al. (2022) Constantinos Daskalakis, Noah Golowich, and Kaiqing Zhang. The complexity of markov equilibrium in stochastic games. _arXiv preprint arXiv:2204.03991_, 2022.
* Deng et al. (2023) Xiaotie Deng, Ningyuan Li, David Mguni, Jun Wang, and Yaodong Yang. On the complexity of computing markov perfect equilibrium in general-sum stochastic games. _National Science Review_, 10(1):nwac256, 2023.
* Ding et al. (2022) Dongsheng Ding, Chen-Yu Wei, Kaiqing Zhang, and Mihailo R Jovanovic. Independent policy gradient for large-scale markov potential games: Sharper rates, function approximation, and game-agnostic convergence. _arXiv preprint arXiv:2202.04129_, 2022.
* Easley and Kleinberg (2010) David Easley and Jon Kleinberg. _Networks, crowds, and markets: Reasoning about a highly connected world_. Cambridge university press, 2010.
* Erez et al. (2022) Liad Erez, Tal Lancewicki, Uri Sherman, Tomer Koren, and Yishay Mansour. Regret minimization and convergence to equilibria in general-sum markov games, 2022.
* Etessami and Yannakakis (2010) Kousha Etessami and Mihalis Yannakakis. On the complexity of nash equilibria and other fixed points. _SIAM J. Comput._, 39(6):2531-2597, 2010. doi: 10.1137/080720826.
* Filar and Vrieze (2012) Jerzy Filar and Koos Vrieze. _Competitive Markov decision processes_. Springer Science & Business Media, 2012.
* Filar and Raghavan (1984) Jerzy A Filar and TES Raghavan. A matrix game solution of the single-controller stochastic game. _Mathematics of Operations Research_, 9(3):356-362, 1984.
* Filar et al. (1991) Jerzy A Filar, Todd A Schultz, Frank Thuijsman, and OJ Vrieze. Nonlinear programming and stationary equilibria in stochastic games. _Mathematical Programming_, 50(1-3):227-237, 1991.
* Fox et al. (2022) Roy Fox, Stephen M. McAleer, Will Overman, and Ioannis Panageas. Independent natural policy gradient always converges in markov potential games. In _International Conference on Artificial Intelligence and Statistics, AISTATS 2022_, volume 151 of _Proceedings of Machine Learning Research_, pages 4414-4425. PMLR, 2022.
* Guan et al. (2016) Peng Guan, Maxim Raginsky, Rebecca Willett, and Daphney-Stavroula Zois. Regret minimization algorithms for single-controller zero-sum stochastic games. In _2016 IEEE 55th Conference on Decision and Control (CDC)_, pages 7075-7080. IEEE, 2016.
* Hu and Wellman (2003) Junling Hu and Michael P. Wellman. Nash q-learning for general-sum stochastic games. _J. Mach. Learn. Res._, 4:1039-1069, 2003.
* Jin et al. (2018) Chi Jin, Zeyuan Allen-Zhu, Sebastien Bubeck, and Michael I. Jordan. Is q-learning provably efficient? In _Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018_, pages 4868-4878, 2018.
* Jin et al. (2022) Yujia Jin, Vidya Muthukumar, and Aaron Sidford. The complexity of infinite-horizon general-sum stochastic games. _CoRR_, abs/2204.04186, 2022. doi: 10.48550/arXiv.2204.04186. URL https://doi.org/10.48550/arXiv.2204.04186.
* Kalogiannis et al. (2022) Fivos Kalogiannis, Ioannis Anagnostides, Ioannis Panageas, Emmanouil-Vasileios Vlatakis-Gkaragkounis, Vaggos Chatziafratis, and Stelios Stavroulakis. Efficiently computing nash equilibria in adversarial team markov games. _arXiv preprint arXiv:2208.02204_, 2022.
* Krizhevsky et al. (2014)Stefanos Leonardos, Will Overman, Ioannis Panageas, and Georgios Piliouras. Global convergence of multi-agent policy gradient in markov potential games. _arXiv preprint arXiv:2106.01969_, 2021.
* Li et al. (2021) Yuanzhi Li, Ruosong Wang, and Lin F. Yang. Settling the horizon-dependence of sample complexity in reinforcement learning. In _62nd IEEE Annual Symposium on Foundations of Computer Science, FOCS 2021_, pages 965-976. IEEE, 2021. doi: 10.1109/FOCS52979.2021.00097.
* Littman (1994) Michael L Littman. Markov games as a framework for multi-agent reinforcement learning. In _Machine learning proceedings 1994_, pages 157-163. Elsevier, 1994.
* Luo et al. (2019) Yuping Luo, Huazhe Xu, Yuanzhi Li, Yuandong Tian, Trevor Darrell, and Tengyu Ma. Algorithmic framework for model-based deep reinforcement learning with theoretical guarantees. In _7th International Conference on Learning Representations, ICLR 2019_. OpenReview.net, 2019.
* Maheshwari et al. (2022) Chinmay Maheshwari, Manxi Wu, Druv Pai, and Shankar Sastry. Independent and decentralized learning in markov potential games, 2022.
* Moravcik et al. (2017) Matej Moravcik, Martin Schmid, Neil Burch, Viliam Lisy, Dustin Morrill, Nolan Bard, Trevor Davis, Kevin Waugh, Michael Johanson, and Michael Bowling. Deepstack: Expert-level artificial intelligence in heads-up no-limit poker. _Science_, 356(6337):508-513, 2017. doi: 10.1126/science.aam6960.
* Nash (1951) John Nash. Non-cooperative games. _Annals of mathematics_, pages 286-295, 1951.
* Neu and Pike-Burke (2020) Gergely Neu and Ciara Pike-Burke. A unifying view of optimism in episodic reinforcement learning. _Advances in Neural Information Processing Systems_, 33:1392-1403, 2020.
* Panait and Luke (2005) L. Panait and S. Luke. Cooperative Multi-Agent Learning: The State of the Art. _Autonomous Agents and Multi-Agent Systems_, 11(3):387-434, Nov 2005. doi: 10.1007/s10458-005-2631-2.
* Perolat et al. (2022) Julien Perolat, Bart de Vylder, Daniel Hennes, Eugene Tarassov, Florian Strub, Vincent de Boer, Paul Muller, Jerome T. Connor, Neil Burch, Thomas Anthony, Stephen McAleer, Romuald Elie, Sarah H. Cen, Zhe Wang, Audrunas Gruslys, Aleksandra Malysheva, Mina Khan, Sherjil Ozair, Finbarr Timbers, Toby Pohlen, Tom Eccles, Mark Rowland, Marc Lanctot, Jean-Baptiste Lespiau, Bilal Piot, Shayegan Omidshafiei, Edward Lockhart, Laurent Sifre, Nathalie Beauguerlange, Remi Munos, David Silver, Satinder Singh, Demis Hassabis, and Karl Tuyls. Mastering the game of strategy with model-free multiagent reinforcement learning, 2022.
* Qiu et al. (2021) Shuang Qiu, Xiaohan Wei, Jieping Ye, Zhaoran Wang, and Zhuoran Yang. Provably efficient fictitious play policy optimization for zero-sum markov games with structured transitions. In _International Conference on Machine Learning_, pages 8715-8725. PMLR, 2021.
* Rubinstein (2017) Aviad Rubinstein. Settling the complexity of computing approximate two-player nash equilibria. _SIGecom Exch._, 15(2):45-49, 2017. doi: 10.1145/3055589.3055596.
* Sayin et al. (2021) Muhammed Sayin, Kaiqing Zhang, David Leslie, Tamer Basar, and Asuman Ozdaglar. Decentralized q-learning in zero-sum markov games. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan, editors, _Advances in Neural Information Processing Systems_, volume 34, pages 18320-18334. Curran Associates, Inc., 2021.
* Sayin et al. (2020) Muhammed O Sayin, Francesca Parise, and Asuman Ozdaglar. Fictitious play in zero-sum stochastic games. _arXiv preprint arXiv:2010.04223_, 2020.
* Sayin et al. (2022) Muhammed O Sayin, Kaiqing Zhang, and Asuman Ozdaglar. Fictitious play in markov games with single controller. In _Proceedings of the 23rd ACM Conference on Economics and Computation_, pages 919-936, 2022.
* Schweitzer et al. (2009) Frank Schweitzer, Giorgio Fagiolo, Didier Sornette, Fernando Vega-Redondo, Alessandro Vespignani, and Douglas R White. Economic networks: The new challenges. _science_, 325(5939):422-425, 2009.
* Shapley (1953) Lloyd S Shapley. Stochastic games. _Proceedings of the national academy of sciences_, 39(10):1095-1100, 1953.
* Shapley et al. (2018)Aaron Sidford, Mengdi Wang, Xian Wu, Lin Yang, and Yinyu Ye. Near-optimal time and sample complexities for solving markov decision processes with a generative model. In _Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018_, pages 5192-5202, 2018.
* Silver et al. (2017) David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, Yutian Chen, Timothy P. Lillicrap, Fan Hui, Laurent Sifre, George van den Driessche, Thore Graepel, and Demis Hassabis. Mastering the game of go without human knowledge. _Nat._, 550(7676):354-359, 2017. doi: 10.1038/nature24270.
* Sutton and Barto (2018) R. S. Sutton and A. G. Barto. _Reinforcement Learning: An Introduction_. A Bradford Book, Cambridge, MA, USA, 2018. ISBN 0262039249.
* Szabo and Fath (2007) Gyorgy Szabo and Gabor Fath. Evolutionary games on graphs. _Physics reports_, 446(4-6):97-216, 2007.
* Tipsuwan and Chow (2003) Yodyium Tipsuwan and Mo-Yuen Chow. Control methodologies in networked control systems. _Control engineering practice_, 11(10):1099-1111, 2003.
* Vinyals et al. (2019) Oriol Vinyals, Igor Babuschkin, Wojciech M. Czarnecki, Michael Mathieu, Andrew Dudzik, Junyoung Chung, David H. Choi, Richard Powell, Timo Ewalds, Petko Georgiev, Junhyuk Oh, Dan Horgan, Manuel Kroiss, Ivo Danihelka, Aja Huang, Laurent Sifre, Trevor Cai, John P. Agapiou, Max Jaderberg, Alexander Sasha Vezhnevets, Remi Leblond, Tobias Pohlen, Valentin Dalibard, David Budden, Yury Sulsky, James Molloy, Tom Le Paine, Caglar Gulcehre, Ziyu Wang, Tobias Pfaff, Yuhuai Wu, Roman Ring, Dani Yogatama, Dario Wunsch, Katrina McKinney, Oliver Smith, Tom Schaul, Timothy P. Lillicrap, Koray Kavukcuoglu, Demis Hassabis, Chris Apps, and David Silver. Grandmaster level in starcraft II using multi-agent reinforcement learning. _Nat._, 575(7782):350-354, 2019. doi: 10.1038/s41586-019-1724-z.
* Vrieze et al. (1983) OJ Vrieze, SH Tijs, TES Raghavan, and JA Filar. A finite algorithm for the switching control stochastic game. _Or Spektrum_, 5(1):15-24, 1983.
* Wang and Sandholm (2002) Xiaofeng Wang and Tuomas Sandholm. Reinforcement learning to play an optimal nash equilibrium in team markov games. In _Advances in Neural Information Processing Systems 15 [Neural Information Processing Systems, NIPS 2002, December 9-14, 2002_, pages 1571-1578. MIT Press, 2002.
* Wang et al. (2023) Yuanhao Wang, Qinghua Liu, Yu Bai, and Chi Jin. Breaking the curse of multiagency: Provably efficient decentralized multi-agent rl with function approximation. _arXiv preprint arXiv:2302.06606_, 2023.
* Wei et al. (2021) Chen-Yu Wei, Chung-Wei Lee, Mengxiao Zhang, and Haipeng Luo. Last-iterate convergence of decentralized optimistic gradient descent/ascent in infinite-horizon competitive markov games. In Mikhail Belkin and Samory Kpotufe, editors, _Proceedings of Thirty Fourth Conference on Learning Theory_, volume 134 of _Proceedings of Machine Learning Research_, pages 4259-4299. PMLR, 15-19 Aug 2021.
* Zhang et al. (2021) Runyu Zhang, Zhaolin Ren, and Na Li. Gradient play in stochastic games: stationary points, convergence, and sample complexity. _arXiv preprint arXiv:2106.00198_, 2021.
* Zhang et al. (2022) Runyu Zhang, Qinghua Liu, Huan Wang, Caiming Xiong, Na Li, and Yu Bai. Policy optimization for markov games: Unified framework and faster convergence, 2022.

Missing statements and proofs

### Statements for Section 3.1

**Claim A.1**.: Let a two-player Markov game where both players affect the transition. Further, consider a correlated policy \(\bm{\sigma}\) and its corresponding marginalized product policy \(\bm{\pi}^{\bm{\sigma}}=\bm{\pi}_{1}^{\bm{\sigma}}\times\bm{\pi}_{2}^{\bm{\sigma}}\). Then, for any \(\bm{\pi}_{1}^{\prime},\bm{\pi}_{2}^{\prime}\),

\[V_{k,1}^{\bm{\pi}_{1}^{\prime},\bm{\sigma}_{-1}}(s_{1}) =V_{k,1}^{\bm{\pi}_{1}^{\prime},\bm{\pi}_{2}^{\bm{\sigma}}}(s_{1}),\] \[V_{k,2}^{\bm{\sigma}_{-2},\bm{\pi}_{2}^{\prime}}(s_{1}) =V_{k,2}^{\bm{\pi}_{1}^{\prime},\bm{\pi}_{2}^{\prime}}(s_{1}).\]

Proof.: We will effectively show that the problem of best-responding to a correlated policy \(\bm{\sigma}\) is equivalent to best-responding to the marginal policy of \(\bm{\sigma}\) for the opponent. The proof follows from the equivalence of the two MDPs.

As a reminder,

\[\pi_{1,h}(a|s) =\sum_{b\in\mathcal{A}_{2}}\bm{\sigma}_{h}(a,b|s)\] \[\pi_{2,h}(b|s) =\sum_{a\in\mathcal{A}_{1}}\bm{\sigma}_{h}(a,b|s)\]

As we have seen in Section 2.1, in the case of unilateral deviation from joint policy \(\bm{\sigma}\), an agent faces a single agent MDP. More specifically, agent \(2\), best-responds by optimizing a reward function \(\bar{r}_{2,h}(s,b)\) under a transition kernel \(\bar{\mathbb{P}}_{2}\) for which,

\[\bar{r}_{2,h}(s,b)=\mathbb{E}_{b\sim\bm{\sigma}}\left[r_{2,h}(s,a,b)\right]= \mathbb{E}_{b\sim\bm{\pi}_{1}^{\bm{\sigma}}}\left[r_{2,h}(s,a,b)\right]=r_{2,h }(s,\bm{\pi}_{1}^{\bm{\sigma}},b).\]

Similarly,

\[\bar{r}_{1,h}(s,b)=r_{1,h}(s,a,\bm{\pi}_{2}^{\bm{\sigma}}).\]

Analogously, for each of the transition kernels,

\[\bar{\mathbb{P}}_{2,h}(s^{\prime}|s,b)=\mathbb{E}_{a\sim\bm{\sigma}}\left[ \mathbb{P}_{2,h}(s^{\prime}|s,a,b)\right]=\mathbb{E}_{a\sim\bm{\pi}_{2}^{\bm{ \sigma}}}\left[\mathbb{P}_{2,h}(s^{\prime}|s,a,b)\right]=\mathbb{P}_{2,h}(s^{ \prime}|s,\bm{\pi}_{1}^{\bm{\sigma}},b),\]

as for agent \(1\),

\[\bar{\mathbb{P}}_{1,h}(s^{\prime}|s,a)=\mathbb{P}_{1,h}(s^{\prime}|s,a,\bm{ \pi}_{2}^{\bm{\sigma}}).\]

Hence, it follows that, \(V_{2,1}^{\bm{\sigma}_{-2}\times\bm{\pi}_{2}^{\prime}}(s_{1})=V_{2,1}^{\bm{ \pi}_{1}^{\bm{\sigma}}\times\bm{\pi}_{2}^{\bm{\sigma}}}(s_{1}),\ \ \forall\bm{\pi}_{2}^{\prime}\) and \(V_{1,1}^{\bm{\pi}_{1}^{\prime}\times\bm{\sigma}_{-1}}(s_{1})=V_{1,1}^{\bm{\pi} _{1}^{\prime}\times\bm{\pi}_{2}^{\bm{\sigma}}}(s_{1}),\ \forall\bm{\pi}_{2}^{\prime}\).

### Proof of Theorem 3.2

The best-response program.First, we state the following lemma that will prove useful for several of our arguments,

**Lemma A.1** (Best-response LP).: Let a (possibly correlated) joint policy \(\hat{\bm{\sigma}}\). Consider the following linear program with variables \(\bm{w}\in\mathbb{R}^{n\times H\times S}\),

( \[\begin{array}{cc}\min&\sum\limits_{k\in[n]}w_{k,s}(s_{1})-\bm{e}_{s_{1}}^{ \top}\sum\limits_{h=1}^{H}\left(\prod\limits_{r=1}^{h}\mathbb{P}_{\tau}(\hat{ \bm{\sigma}}_{\tau})\right)\bm{r}_{k,h}(\hat{\bm{\sigma}}_{h})\\ \mathrm{s.t.}&w_{k,h}(s)\geq r_{k,h}(s,a,\hat{\bm{\sigma}}_{-k,h})+\mathbb{P}_ {h}(s,a,\hat{\bm{\sigma}}_{-k,h})\bm{w}_{k,h+1},\\ &\forall s\in\mathcal{S},\forall h\in[H],\forall k\in[n],\forall a\in \mathcal{A}_{k};\\ &w_{k,H}(s)=0,\ \forall k\in[n],\forall s\in\mathcal{S}.\end{array}\]

The optimal solution \(\bm{w}^{\dagger}\) of the program is unique and corresponds to the value function of each player \(k\in[n]\) when player \(k\) best-responds to \(\hat{\bm{\sigma}}\).

**Proof.** We observe that the program is separable to \(n\) independent linear programs, each with variables \(\bm{w}_{k}\in\mathbb{R}^{n\times H}\),

\[\min w_{k,1}(s_{1})\] \[\mathrm{s.t.} w_{k,h}(s)\geq r_{k,h}(s,a,\hat{\bm{\sigma}}_{-k,h})+\mathbb{P}_{h} (s,a,\hat{\bm{\sigma}}_{-k,h})\bm{w}_{k,h+1},\] \[\forall s\in\mathcal{S},\forall h\in[H],\forall a\in\mathcal{A}_{k};\] \[w_{k,H}(s)=0,\ \forall k\in[n],\forall s\in\mathcal{S}.\]

Each of these linear programs describes the problem of a single agent MDP (Neu and Pike-Burke, 2020, Section 2) --that agent being \(k\)-- which, as we have seen in Best-response policies, is equivalent to the problem of finding a best-response to \(\hat{\bm{\sigma}}_{-k}\). It follows that the optimal \(\bm{w}_{k}^{\dagger}\) for every program is unique (each program corresponds to a set of Bellman optimality equations). \(\square\)

Properties of the NE program.Second, we need to prove that the minimum value of the objective function of the program is nonnegative.

**Lemma A.2** (Feasibility of (P\({}_{\mathrm{NE}}^{\prime}\)) and global optimum).: The nonlinear program (P\({}_{\mathrm{NE}}^{\prime}\)) is feasible, has a nonnegative objective value, and its global minimum is equal to \(0\).

**Proof.** Analogously to the finite-horizon case, for the feasibility of the nonlinear program, we invoke the theorem of the existence of a Nash equilibrium. We let a NE product policy, \(\bm{\pi}^{\star}\), and a vector \(\bm{w}^{\star}\in\mathbb{R}^{n\times S}\) such that \(w_{k}^{\star}(s)=V_{k}^{\dagger,\bm{\pi}_{-k}^{\star}}(s),\ \forall k\in[n]\times \mathcal{S}\).

By Lemma A.1, we know that \((\bm{\pi}^{\star},\bm{w}^{\star})\) satisfies all the constraints of (\(\mathrm{P}_{\mathrm{NE}}\)). Additionaly, because \(\bm{\pi}^{\star}\) is a NE, \(V_{k,h}^{\bm{\pi}^{\star}}(s_{1})=V_{k,h}^{\dagger,\bm{\pi}^{\star}_{-k}}(s_{ 1})\) for all \(k\in[n]\). Observing that,

\[w_{k,1}^{\star}(s_{1})-\bm{e}_{s_{1}}^{\top}\sum_{h=1}^{H}\left(\prod_{\tau=1 }^{h}\mathbb{P}_{\tau}(\bm{\pi}_{\tau}^{\star})\right)\bm{r}_{k,h}(\bm{\pi}_{h }^{\star})=V_{k,h}^{\dagger,\bm{\pi}^{\star}_{-k}}(s_{1})-V_{k,h}^{\bm{\pi}^{ \star}}(s_{1})=0,\]

concludes the argument that a NE attains an objective value equal to \(0\).

Continuing, we observe that due to (1) the objective function can be equivalently rewritten as,

\[\sum_{k\in[n]}\left(w_{k,1}(s_{1})-\bm{e}_{s_{1}}^{\top}\sum_{h=1} ^{H}\left(\prod_{\tau=1}^{h}\mathbb{P}_{\tau}(\bm{\pi}_{\tau})\right)\bm{r}_{k,h}(\bm{\pi}_{h})\right)\] \[\qquad=\sum_{k\in[n]}w_{k,1}(s_{1})-\bm{e}_{s_{1}}^{\top}\sum_{h=1 }^{H}\left(\prod_{\tau=1}^{h}\mathbb{P}_{\tau}(\bm{\pi}_{\tau})\right)\sum_{k \in[n]}\bm{r}_{k,h}(\bm{\pi}_{h})\] \[\qquad=\sum_{k\in[n]}w_{k,1}(s_{1}).\]

Next, we focus on the inequality constraint

\[w_{k,h}(s)\geq r_{k,h}(s,a,\bm{\pi}_{-k,h})+\mathbb{P}_{h}(s,a,\bm{\pi}_{-k,h })\bm{w}_{k,h+1}\]

which holds for all \(s\in\mathcal{S}\), all players \(k\in[n]\), all \(a\in\mathcal{A}_{k}\), and all timesteps \(h\in[H-1]\).

By summing over \(a\in\mathcal{A}_{k}\) while multiplying each term with a corresponding coefficient \(\pi_{k,h}(a|s)\), the display written in an equivalent element-wise vector inequality reads:

\[\bm{w}_{k,h}\geq\bm{r}_{k,h}(\bm{\pi}_{h})+\mathbb{P}_{h}(\bm{\pi}_{h})\bm{w}_{ k,h+1}.\]

Finally, after consecutively substituting \(\bm{w}_{k,h+1}\) with the element-wise lesser term \(\bm{r}_{k,h+1}(\bm{\pi}_{h+1})+\mathbb{P}_{h+1}(\bm{\pi}_{h+1})\bm{w}_{k,h+2}\), we end up with the inequality:

\[\bm{w}_{k,1}\geq\sum_{h=1}^{H}\left(\prod_{\tau=1}^{h}\mathbb{P}_{\tau}(\bm{ \pi}_{\tau})\right)\bm{r}_{k,h}(\bm{\pi}_{h}).\] (5)

Summing over \(k\), it holds for the \(s_{1}\)-th entry of the inequality,

\[\sum_{k\in[n]}w_{k,1}\geq\sum_{k\in[n]}\sum_{h=1}^{H}\left(\prod_{\tau=1}^{h} \mathbb{P}_{\tau}(\bm{\pi}_{\tau})\right)\bm{r}_{k,h}(\bm{\pi}_{h})=0.\]

Where the equality holds due to the zero-sum property, (1). \(\square\)An approximate NE is an approximate global minimum.We show that an \(\epsilon\)-approximate NE, \(\bm{\pi}^{\star}\), achieves an \(n\epsilon\)-approximate global minimum of the program. Utilizing Lemma A.1, setting \(w^{\star}_{k}(s_{1})=V^{\dagger,\bm{\pi}^{\star}}_{k,1}(s_{1})\), and the definition of an \(\epsilon\)-approximate NE we see that,

\[\sum_{k\in[n]}\left(w^{\star}_{k,1}(s_{1})-\bm{e}^{\top}_{s_{1}} \sum_{h=1}^{H}\left(\prod_{\tau=1}^{h}\mathbb{P}_{\tau}(\bm{\pi}^{\star}_{\tau} )\right)\bm{r}_{k,h}(\bm{\pi}^{\star}_{h})\right) =\sum_{k\in[n]}\left(w^{\star}_{k,1}(s_{1})-V^{\bm{\pi}^{\star}}_{ k,1}(s_{1})\right)\] \[\leq\sum_{k\in[n]}\epsilon=n\epsilon.\]

Indeed, this means that \(\bm{\pi}^{\star},\bm{w}^{\star}\) is an \(n\epsilon\)-approximate global minimizer of (\(\mathrm{P}_{\mathrm{NE}}\)).

An approximate global minimum is an approximate NE.For the opposite direction, we let a feasible \(\epsilon\)-approximate global minimizer of the program (\(\mathrm{P}_{\mathrm{NE}}\)), \((\bm{\pi}^{\star},\bm{w}^{\star})\). Because a global minimum of the program is equal to \(0\), an \(\epsilon\)-approximate global optimum must be at most \(\epsilon>0\). We observe that for every \(k\in[n]\),

\[w^{\star}_{k,1}(s_{1})\geq\bm{e}^{\top}_{s_{1}}\sum_{h=1}^{H}\left(\prod_{ \tau=1}^{h}\mathbb{P}_{\tau}(\bm{\pi}^{\star}_{\tau})\right)\bm{r}_{k,h}(\bm{ \pi}^{\star}_{h}),\] (6)

which follows from induction on the inequality constraint over all \(h\) similar to (5).

Consequently, the assumption that

\[\epsilon \geq\sum_{k\in[n]}\left(w^{\star}_{k,1}(s_{1})-\bm{e}^{\top}_{s_{ 1}}\sum_{h=1}^{H}\left(\prod_{\tau=1}^{h}\mathbb{P}_{\tau}(\bm{\pi}^{\star}_{ \tau})\right)\bm{r}_{k,h}(\bm{\pi}^{\star}_{h})\right),\]

and Equation (6), yields the fact that

\[\epsilon \geq w^{\star}_{k,1}(s_{1})-\bm{e}^{\top}_{s_{1}}\sum_{h=1}^{H} \left(\prod_{\tau=1}^{h}\mathbb{P}_{\tau}(\bm{\pi}^{\star}_{\tau})\right)\bm{r }_{k,h}(\bm{\pi}^{\star}_{h})\] \[\geq V^{\dagger,\bm{\pi}^{\star}_{-k}}_{k,1}(s_{1})-V^{\bm{\pi}^{ \star}}_{k,1}(s_{1}),\]

where the second inequality holds from the fact that \(\bm{w}^{\star}\) is feasible for (\(\mathrm{P}_{\mathrm{BR}}\)). The latter concludes the proof, as the display coincides with the definition of an \(\epsilon\)-approximate NE.

### Proof of Claim 3.1

Proof.: The value function of \(s_{1}\) for \(h=1\) of players \(1\) and \(2\) read:

\[V^{\bm{\sigma}}_{1,1}(s_{1}) =\bm{e}^{\top}_{s_{1}}\left(\bm{r}_{1}(\bm{\sigma})+\mathbb{P}( \bm{\sigma})\bm{r}_{1}(\bm{\sigma})\right)\] \[=-\frac{9\sigma(a_{1},b_{1}|s_{1})}{20}+\frac{\sigma(a_{1},b_{2}| s_{1})}{20}+\frac{\left(1-\sigma(a_{1},b_{1}|s_{1})\right)\left(\sigma(a_{1},b_{1}| s_{2})+\sigma(a_{1},b_{2}|s_{2})\right)}{20},\]

and,

\[V^{\bm{\sigma}}_{2,1}(s_{1}) =\bm{e}^{\top}_{s_{1}}\left(\bm{r}_{2}(\bm{\sigma})+\mathbb{P}( \bm{\sigma})\bm{r}_{2}(\bm{\sigma})\right)\] \[=-\frac{9\sigma(a_{1},b_{1}|s_{1})}{20}+\frac{\sigma(a_{2},b_{2}| s_{1})}{20}+\frac{\left(1-\sigma(a_{1},b_{1}|s_{1})\right)\left(\sigma(a_{1},b_{1}| s_{2})+\sigma(a_{2},b_{1}|s_{2})\right)}{20}.\]

We are indifferent to the corresponding value function of player \(3\) as they only have one available action per state and hence, cannot affect their rewards. For the joint policy \(\bm{\sigma}\), the corresponding value functions of both players \(1\) and \(2\) are \(V^{\bm{\sigma}}_{1,1}(s_{1})=V^{\bm{\sigma}}_{2,1}(s_{1})=\frac{1}{20}\).

Deviations.We will now prove that no deviation of player \(1\) manages to accumulate a reward greater than \(\frac{1}{20}\). The same follows for player \(2\) due to symmetry.

When a player deviates unilaterally from a joint policy, they experience a single agent Markov decision process (MDP). It is well-known that MDPs always have a deterministic optimal policy. As such, it suffices to check whether \(V^{\bm{\pi}_{1},\bm{\sigma}_{-1}}_{1,1}(s_{1})\) is greater than \(\frac{1}{20}\) for any of the four possible deterministic policies:* \(\boldsymbol{\pi}_{1}(s_{1})=\boldsymbol{\pi}_{1}(s_{2})=(1\quad 0)\),
* \(\boldsymbol{\pi}_{1}(s_{1})=\boldsymbol{\pi}_{1}(s_{2})=(0\quad 1)\),
* \(\boldsymbol{\pi}_{1}(s_{1})=(0\quad 1)\),
* \(\boldsymbol{\pi}_{1}(s_{1})=(0\quad 1)\),
* \(\boldsymbol{\pi}_{1}(s_{1})=(0\quad 1)\), \(\boldsymbol{\pi}_{1}(s_{2})=(1\quad 0)\).

Finally, the value function of any deviation \(\boldsymbol{\pi}_{1}^{\prime}\) writes,

\[V_{1,1}^{\boldsymbol{\pi}_{1}^{\prime}\times\boldsymbol{\sigma}_{-1}}(s_{1})=- \frac{\pi_{1}^{\prime}(a_{1}|s_{1})}{5}-\frac{\pi_{1}^{\prime}(a_{1}|s_{2}) \left(\pi_{1}^{\prime}(a_{1}|s_{1})-2\right)}{40}.\]

We can now check that for all deterministic policies \(V_{1,1}^{\boldsymbol{\pi}_{1}^{\prime}\times\boldsymbol{\sigma}_{-1}}(s_{1}) \leq\frac{1}{20}\). By symmetry, it follows that \(V_{2,1}^{\boldsymbol{\pi}_{2}^{\prime}\times\boldsymbol{\sigma}_{-2}}(s_{1}) \leq\frac{1}{20}\) and as such \(\boldsymbol{\sigma}\) is indeed a CCE. 

### Proof of Claim 3.2

**Proof.** In general, the value functions of each player \(1\) and \(2\) are:

\[V_{1,1}^{\boldsymbol{\pi}_{1}\times\boldsymbol{\pi}_{2}}(s_{1})= -\frac{\pi_{1}(a_{1}|s_{1})\pi_{2}(b_{1}|s_{1})}{2}+\frac{\pi_{1} (a_{1}|s_{1})}{20}-\frac{\pi_{1}(a_{1}|s_{2})\left(\pi_{1}(a_{1}|s_{1})\pi_{2} (b_{1}|s_{1})-1\right)}{20},\]

and

\[V_{2,1}^{\boldsymbol{\pi}_{1}\times\boldsymbol{\pi}_{2}}(s_{1})= -\frac{\pi_{1}(a_{1}|s_{1})\pi_{2}(b_{1}|s_{1})}{2}+\frac{\pi_{1} (b_{1}|s_{1})}{20}-\frac{\pi_{1}(b_{1}|s_{2})\left(\pi_{1}(a_{1}|s_{1})\pi_{2 }(b_{1}|s_{1})-1\right)}{20}.\]

Plugging in \(\boldsymbol{\pi}_{1}^{\sigma},\boldsymbol{\pi}_{2}^{\sigma}\) yields \(V_{1,1}^{\boldsymbol{\pi}_{1}^{\sigma}\times\boldsymbol{\pi}_{2}^{\sigma}}(s_ {1})=V_{2,1}^{\boldsymbol{\pi}_{1}^{\sigma}\times\boldsymbol{\pi}_{2}^{\sigma} }(s_{1})=-\frac{13}{160}\). But, if player \(1\) deviates to say \(\pi_{1}^{\prime}(s_{1})=\pi_{1}^{\prime}(s_{2})=(0\quad 1)\), they get a value equal to \(0\) which is clearly greater than \(-\frac{13}{160}\). Hence, \(\boldsymbol{\pi}_{1}^{\sigma}\times\boldsymbol{\pi}_{2}^{\sigma}\) is not a NE. 

### Proof of Theorem 3.4

**Proof.** The proof follows from the game of Example 1, and Claims 3.1 and 3.2.

Proofs for infinite-horizon Zero-Sum Polymatrix Markov Games

In this section we will explicitly state definitions, theorems and proofs relating to the infinite-horizon discounted zero-sum polymatrix Markov games.

### Definitions of equilibria for the infinite-horizon

Let us restate the definition specifically for infinite-horizon Markov games. They are defined as a tuple \(\Gamma(H,\mathcal{S},\{\mathcal{A}_{k}\}_{k\in[n]},\mathbb{P},\{r_{k}\}_{k\in[n] },\gamma,\boldsymbol{\rho})\).

* \(H=\infty\) denotes the _time horizon_
* \(\mathcal{S}\), with cardinality \(S\coloneqq|\mathcal{S}|\), stands for the state space,
* \(\{\mathcal{A}_{k}\}_{k\in[n]}\) is the collection of every player's action space, while \(\mathcal{A}\coloneqq\mathcal{A}_{1}\times\cdots\times\mathcal{A}_{n}\) denotes the _joint action space_; further, an element of that set --a joint action-- is generally noted as \(\boldsymbol{a}=(a_{1},\ldots,a_{n})\in\mathcal{A}\),
* \(\mathbb{P}:\mathcal{S}\times\mathcal{A}\to\Delta(\mathcal{S})\) is the transition probability function,
* \(r_{k}:\mathcal{S},\mathcal{A}\to[-1,1]\) yields the reward of player \(k\) at a given state and joint action,
* a discount factor \(0<\gamma<1\),
* an initial state distribution \(\boldsymbol{\rho}\in\Delta(\mathcal{S})\).

Policies and value functions.In infinite-horizon Markov games policies can still be distinguished in two main ways, _Markovian/non-Markovian_ and _stationary/nonstationary_. Moreover, a joint policy can be a _correlated_ policy or a _product_ policy.

_Markovian_ policies attribute a probability over the simplex of actions solely depending on the running state \(s\) of the game. On the other hand, _non-Markovian_ policies attribute a probability over the simplex of actions that depends on any subset of the history of the game. _I.e._, they can depend on any sub-sequence of actions and states up until the running timestep of the horizon.

_Stationary_ policies are those that will attribute the same probability distribution over the simplex of actions for every timestep of the horizon. _Nonstationary_ policies, on the contrary can change depending on the timestep of the horizon.

A joint Markovian stationary policy \(\boldsymbol{\sigma}\) is said to be _correlated_ when for every state \(s\in\mathcal{S}\), attributes a probability distribution over the simplex of joint actions \(\mathcal{A}\) for all players, _i.e._, \(\boldsymbol{\sigma}(s)\in\Delta(\mathcal{A})\). A Markovian stationary policy \(\boldsymbol{\pi}\) is said to be a _product_ policy when for every \(s\in\mathcal{S}\), \(\boldsymbol{\pi}(s)\in\prod_{k=1}^{n}\Delta(\mathcal{A}_{k})\). It is rather easy to define _correlated/product_ policies for the case of non-Markovian and nonstationary policies.

Given a Markovian stationary policy \(\boldsymbol{\pi}\), the value function for an infinite-horizon discounted game is defined as,

\[V_{k}^{\boldsymbol{\pi}}(s_{1})=\mathbb{E}_{\boldsymbol{\pi}}\left[\sum_{h=1} ^{H}\gamma^{h-1}r_{k,h}(s_{h},\boldsymbol{a}_{h})\big{|}s_{1}\right]= \boldsymbol{e}_{s_{1}}^{\top}\sum_{h=1}^{H}\left(\gamma^{h-1}\prod_{\tau=1}^{ h}\mathbb{P}_{\tau}(\boldsymbol{\pi}_{\tau})\right)\boldsymbol{r}_{k,h}( \boldsymbol{\pi}_{h}).\]

It is possible to express the value function of each player \(k\) in the following way,

\[V_{k}^{\boldsymbol{\pi}}(s_{1})=\boldsymbol{e}_{s_{1}}^{\top}\left(\mathbf{I}- \gamma\,\mathbb{P}(\boldsymbol{\pi})\right)^{-1}\boldsymbol{r}(\boldsymbol{ \pi}).\]

Where \(\mathbf{I}\) is the identity matrix of appropriate dimensions. Also, when the initial state is drawn from the initial state distribution, we denote, the value function reads \(V_{k}^{\boldsymbol{\pi}}(\boldsymbol{\rho})=\boldsymbol{\rho}^{\top}\left( \mathbf{I}-\gamma\,\mathbb{P}(\boldsymbol{\pi})\right)^{-1}\boldsymbol{r}( \boldsymbol{\pi})\).

Best-response policies.Given an arbitrary joint policy \(\boldsymbol{\sigma}\) (which can be either a correlated or product policy), a best-response policy of a player \(k\) is defined to be \(\boldsymbol{\pi}_{k}^{\dagger}\in\Delta(\mathcal{A}_{k})^{S}\) such that \(\boldsymbol{\pi}_{k}^{\dagger}\in\arg\max_{\boldsymbol{\pi}_{k}^{\prime}}V_{k }^{\boldsymbol{\pi}_{k}^{\prime}\times\boldsymbol{\sigma}_{-k}}(s)\). Also, we will denote \(V_{k}^{\dagger,\boldsymbol{\sigma}-k}(s)=\max_{\boldsymbol{\pi}_{k}^{\prime}}V_ {k}^{\boldsymbol{\pi}_{k}^{\prime},\boldsymbol{\sigma}_{-k}}(s)\). It is rather straightforward to see that the problem of computing a best-response to a given policy is equivalent to solving a single-agent MDP problem.

Notions of equilibria.Now that best-response policies have been defined, it is straightforward to define the different notions of equilibria. First, we define the notion of a coarse-correlated equilibrium.

**Definition B.1** (CCE--infinite-horizon).: _A joint (potentially correlated) policy \(\boldsymbol{\sigma}\in\Delta(\mathcal{A})^{S}\) is an \(\epsilon\)-approximate coarse-correlated equilibrium if it holds that for an \(\epsilon\),_

\[V_{k}^{\dagger,\boldsymbol{\sigma}_{-k}}(\boldsymbol{\rho})-V_{k}^{\boldsymbol {\sigma}}(\boldsymbol{\rho})\leq\epsilon,\;\forall k\in[n].\]

Second, we define the notion of a Nash equilibrium. The main difference of the definition of the coarse-correlated equilibrium, is the fact that a NE Markovian stationary policy is a _product policy_.

**Definition B.2** (NE--infinite-horizon).: _A joint (potentially correlated) policy \(\boldsymbol{\pi}\in\prod_{k\in[n]}\Delta(\mathcal{A}_{k})^{S}\) is an \(\epsilon\)-approximate coarse-correlated equilibrium if it holds that for an \(\epsilon\),_

\[V_{k}^{\dagger,\boldsymbol{\pi}-k}(\boldsymbol{\rho})-V_{k}^{\boldsymbol{\pi} }(\boldsymbol{\rho})\leq\epsilon,\;\forall k\in[n].\]

As it is folklore by now, infinite-horizon discounted Markov games have a stationary Markovian Nash equilibrium.

## Appendix C Main results for infinite-horizon games

The workhorse of our arguments in the following results is still the following nonlinear program with variables \(\boldsymbol{\pi},\boldsymbol{w}\),

(P\({}_{\rm NE}^{\prime}\))

As we will prove, approximate NE's correspond to approximate global minima of (P\({}_{\rm NE}^{\prime}\)) and vice-versa. Before that, we need some intermediate lemmas. The first lemma we prove is about the best-response program.

The best-response program.Even for the infinite-horizon, we can define a linear program for the best-responses of all players. That program is the following, with variables \(\boldsymbol{w}\),

(P\({}_{\rm BR}^{\prime}\))

**Lemma C.1** (Best-response LP--infinite-horizon).: Let a (possibly correlated) joint policy \(\hat{\boldsymbol{\sigma}}\). Consider the linear program (P\({}_{\rm BR}^{\prime}\)). The optimal solution \(\boldsymbol{w}^{\dagger}\) of the program is unique and corresponds to the value function of each player \(k\in[n]\) when player \(k\) best-responds to \(\hat{\boldsymbol{\sigma}}\).

Proof.: We observe that the program is separable to \(n\) independent linear programs, each with variables \(\boldsymbol{w}_{k}\in\mathbb{R}^{n}\),

\[\min \boldsymbol{\rho}^{\top}\boldsymbol{w}_{k}\] \[\mathrm{s.t.} w_{k}(s)\geq r_{k}(s,a,\hat{\boldsymbol{\sigma}}_{-k})+\gamma \,\mathbb{P}(s,a,\hat{\boldsymbol{\sigma}}_{-k})\boldsymbol{w}_{k},\] \[\forall s\in\mathcal{S},\forall a\in\mathcal{A}_{k}.\]

Each of these linear programs describes the problem of a single agent MDP --that agent being \(k\). It follows that the optimal \(\boldsymbol{w}_{k}^{\dagger}\) for every program is unique (each program corresponds to a set of Bellman optimality equations).

Properties of the NE program.Second, we need to prove that the minimum value of the objective function of the program is nonnegative.

**Lemma C.2** (Feasibility of (\(\mathrm{P}^{\prime}_{\mathrm{NE}}\)) and global optimum).: The nonlinear program (\(\mathrm{P}^{\prime}_{\mathrm{NE}}\)) is feasible, has a nonnegative objective value, and its global minimum is equal to \(0\).

Proof.: For the feasibility of the nonlinear program, we invoke the theorem of the existence of a Nash equilibrium. _i.e._, let a NE product policy, \(\boldsymbol{\pi}^{\star}\), and a vector \(\boldsymbol{w}^{\star}\in\mathbb{R}^{n\times H\times S}\) such that \(w^{\star}_{k,s}(s)=V^{\dagger,\boldsymbol{\pi}^{\star}_{-k}}_{k}(s),\;\forall k \in[n]\times\mathcal{S}\).

By Lemma C.1, we know that \((\boldsymbol{\pi}^{\star},\boldsymbol{w}^{\star})\) satisfies all the constraints of (\(\mathrm{P}^{\prime}_{\mathrm{NE}}\)). Additionally, because \(\boldsymbol{\pi}^{\star}\) is a NE, \(V^{\boldsymbol{\pi}^{\star}}_{k}(\boldsymbol{\rho})=V^{\dagger,\boldsymbol{ \pi}^{\star}_{-k}}_{k}(\boldsymbol{\rho})\) for all \(k\in[n]\). Observing that,

\[\boldsymbol{\rho}^{\top}\left(\boldsymbol{w}^{\star}_{k}-(\mathbf{I}-\gamma \operatorname{\mathbb{P}}(\boldsymbol{\pi}^{\star}))^{-1}\boldsymbol{r}_{k}( \boldsymbol{\pi}^{\star})\right)=V^{\dagger,\boldsymbol{\pi}^{\star}_{-k}}_{k }(\boldsymbol{\rho})-V^{\boldsymbol{\pi}^{\star}}_{k}(\boldsymbol{\rho})=0,\]

concludes the argument that a NE attains an objective value equal to \(0\).

Continuing, we observe that due to (1) the objective function can be equivalently rewritten as,

\[\sum_{k\in[n]}\left(\boldsymbol{\rho}^{\top}\boldsymbol{w}_{k}- \boldsymbol{\rho}^{\top}(\mathbf{I}-\gamma\operatorname{\mathbb{P}}( \boldsymbol{\pi}))^{-1}\boldsymbol{r}_{k}(\boldsymbol{\pi})\right)\] \[\qquad=\sum_{k\in[n]}\boldsymbol{\rho}^{\top}\boldsymbol{w}_{k}- \boldsymbol{\rho}^{\top}(\mathbf{I}-\gamma\operatorname{\mathbb{P}}( \boldsymbol{\pi}))^{-1}\sum_{k\in[n]}\boldsymbol{r}_{k}(\boldsymbol{\pi}_{h})\] \[\qquad=\sum_{k\in[n]}\boldsymbol{\rho}^{\top}\boldsymbol{w}_{k}.\]

Next, we focus on the inequality constraint

\[w_{k}(s)\geq r_{k}(s,a,\boldsymbol{\pi}_{-k})+\gamma\operatorname{\mathbb{P}}( s,a,\boldsymbol{\pi}_{-k})\boldsymbol{w}_{k}\]

which holds for all \(s\in\mathcal{S}\), all players \(k\in[n]\), and all \(a\in\mathcal{A}_{k}\).

By summing over \(a\in\mathcal{A}_{k}\) while multiplying each term with a corresponding coefficient \(\pi_{k}(a|s)\), the display written in an equivalent element-wise vector inequality reads:

\[\boldsymbol{w}_{k}\geq\boldsymbol{r}_{k,h}(\boldsymbol{\pi})+\gamma \operatorname{\mathbb{P}}(\boldsymbol{\pi})\boldsymbol{w}_{k}.\]

Finally, after consecutively substituting \(\boldsymbol{w}_{k}\) with the element-wise lesser term \(\boldsymbol{r}_{k}(\boldsymbol{\pi})+\gamma\operatorname{\mathbb{P}}_{(} \boldsymbol{\pi})\boldsymbol{w}_{k}\), we end up with the inequality:

\[\boldsymbol{w}_{k}\geq\left(\mathbf{I}-\gamma\operatorname{\mathbb{P}}( \boldsymbol{\pi})\right)^{-1}\boldsymbol{r}_{k}(\boldsymbol{\pi}).\] (9)

We note that \(\mathbf{I}+\gamma\operatorname{\mathbb{P}}(\boldsymbol{\pi})+\gamma^{2} \operatorname{\mathbb{P}}^{2}(\boldsymbol{\pi})+\cdots=\left(\mathbf{I}- \gamma\operatorname{\mathbb{P}}(\boldsymbol{\pi})\right)^{-1}\).

Summing over \(k\), it holds for the \(s_{1}\)-th entry of the inequality,

\[\sum_{k\in[n]}\boldsymbol{w}_{k}\geq\sum_{k\in[n]}\left(\mathbf{I}-\gamma \operatorname{\mathbb{P}}(\boldsymbol{\pi})\right)^{-1}\boldsymbol{r}_{k}( \boldsymbol{\pi})=\left(\mathbf{I}-\gamma\operatorname{\mathbb{P}}(\boldsymbol {\pi})\right)^{-1}\sum_{k\in[n]}\boldsymbol{r}_{k}(\boldsymbol{\pi})=0.\]

Where the equality holds due to the zero-sum property, (1). 

**Theorem C.1** (NE and global optima of (\(\mathrm{P}^{\prime}_{\mathrm{NE}}\))--infinite-horizon).: _If \((\boldsymbol{\pi}^{\star},\boldsymbol{w}^{\star})\) yields an \(\epsilon\)-approximate global minimum of (\(\mathrm{P}^{\prime}_{\mathrm{NE}}\)), then \(\boldsymbol{\pi}^{\star}\) is an \(n\epsilon\)-approximate NE of the infinite-horizon zero-sum polymatrix switching controller MG, \(\Gamma\). Conversely, if \(\boldsymbol{\pi}^{\star}\) is an \(\epsilon\)-approximate NE of the MG \(\Gamma\) with corresponding value function vector \(\boldsymbol{w}^{\star}\) such that \(w^{\star}_{k}(s)=V^{\boldsymbol{\pi}^{\star}}_{k}(s)\forall(k,s)\in[n]\times \mathcal{S}\), then \((\boldsymbol{\pi}^{\star},\boldsymbol{w}^{\star})\) attains an \(\epsilon\)-approximate global minimum of (\(\mathrm{P}^{\prime}_{\mathrm{NE}}\))._

Proof.: **An approximate NE is an approximate global minimum.** We show that an \(\epsilon\)-approximate NE, \(\boldsymbol{\pi}^{\star}\), achieves an \(n\epsilon\)-approximate global minimum of the program. Utilizing Lemma C.1 by setting \(\boldsymbol{w}^{\star}_{k}=\mathbf{V}^{\dagger,\boldsymbol{\pi}^{\star}_{-k}}( \boldsymbol{\rho})\), feasibility, and the definition of an \(\epsilon\)-approximate NE we see that,\[\sum_{k\in[n]}\left(\bm{\rho}^{\top}\bm{w}_{k}^{\star}-\bm{\rho}^{ \top}\left(\mathbf{I}-\gamma\,\mathbb{P}(\bm{\pi}^{\star})\right)^{-1}\bm{r}_{k} (\bm{\pi}^{\star})\right) =\sum_{k\in[n]}\left(\bm{\rho}^{\top}\bm{w}_{k}^{\star}-V_{k}^{\bm {\pi}^{\star}}(\bm{\rho})\right)\] \[\leq\sum_{k\in[n]}\epsilon=n\epsilon.\]

Indeed, this means that \(\bm{\pi}^{\star},\bm{w}^{\star}\) is an \(n\epsilon\)-approximate global minimizer of (\(\mathrm{P}_{\mathrm{NE}}^{\prime}\)).

**An approximate global minimum is an approximate NE.** For this direction, we let a feasible \(\epsilon\)-approximate global minimizer of the program (\(\mathrm{P}_{\mathrm{NE}}^{\prime}\)), \((\bm{\pi}^{\star},\bm{w}^{\star})\). Because a global minimum of the program is equal to \(0\), an \(\epsilon\)-approximate global optimum must be at most \(\epsilon>0\). We observe that for every \(k\in[n]\),

\[\bm{\rho}^{\top}\bm{w}_{k}^{\star}\geq\bm{\rho}^{\top}\left(\mathbf{I}-\gamma \,\mathbb{P}(\bm{\pi}^{\star})\right)^{-1}\bm{r}_{k}(\bm{\pi}^{\star}),\] (10)

which follows from induction on the inequality constraint (9).

Consequently, the assumption that

\[\epsilon{\geq}\bm{\rho}^{\top}\bm{w}_{k}^{\star}-\bm{\rho}^{\top} \left(\mathbf{I}-\gamma\,\mathbb{P}(\bm{\pi}^{\star})\right)^{-1}\bm{r}_{k}( \bm{\pi}^{\star})\]

and Equation (10), yields the fact that

\[\epsilon \geq\bm{\rho}^{\top}\bm{w}_{k}^{\star}-\bm{\rho}^{\top}\left( \mathbf{I}-\gamma\,\mathbb{P}(\bm{\pi}^{\star})\right)^{-1}\bm{r}_{k}(\bm{\pi} ^{\star})\] \[\geq V_{k}^{\dagger,\bm{\pi}^{\star}_{-k}}(\bm{\rho})-V_{k}^{\bm {\pi}^{\star}}(\bm{\rho}),\]

where the second inequality holds from the fact that \(\bm{w}^{\star}\) is also feasible for (\(\mathrm{P}_{\mathrm{BR}}^{\prime}\)). The latter concludes the proof, as the display coincides with the definition of an \(\epsilon\)-approximate NE. 

**Theorem C.2** (CCE collapse to NE in polymatrix MG--infinite-horizon).: _Let a zero-sum polymatrix switching-control Markov game, i.e., a Markov game for which Assumptions 1 and 2 hold. Further, let an \(\epsilon\)-approximate CCE of that game \(\bm{\sigma}\). Then, the marginal product policy \(\bm{\pi}^{\sigma}\), with \(\bm{\pi}_{k}^{\sigma}(a|s)=\sum_{\bm{a}_{-k}\in\mathcal{A}_{-k}}\bm{\sigma}(a,\bm{a}_{-k}),\ \forall k\in[n]\) is an \(n\epsilon\)-approximate NE._

Proof.: Let an \(\epsilon\)-approximate CCE policy, \(\bm{\sigma}\), of game \(\Gamma\). Moreover, let the best-response value-vectors of each agent \(k\) to joint policy \(\bm{\sigma}_{-k}\), \(\bm{w}_{k}^{\dagger}\).

Now, we observe that due to Assumption 1,

\[w_{k}^{\dagger}(s) \geq r_{k}(s,a,\bm{\sigma}_{-k})+\gamma\,\mathbb{P}_{h}(s,a,\bm{ \sigma}_{-k})\bm{w}_{k}^{\dagger}\] \[=\sum_{j\in\mathrm{adj}(k)}r_{(k,j),h}(s,a,\bm{\pi}_{j}^{\bm{ \sigma}})+\gamma\,\mathbb{P}(s,a,\bm{\sigma}_{-k})\bm{w}_{k}^{\dagger}.\]

Further, due to Assumption 2,

\[\mathbb{P}(s,a,\bm{\sigma}_{-k})\bm{w}_{k}^{\dagger}=\mathbb{P}(s,a,\bm{\pi}_{ \mathrm{argctrl}(s)}^{\bm{\sigma}})\bm{w}_{k}^{\dagger},\]

or,

\[\mathbb{P}(s,a,\bm{\sigma}_{-k})\bm{w}_{k}^{\dagger}=\mathbb{P}(s,a,\bm{\pi}^{ \sigma})\bm{w}_{k}^{\dagger}.\]

Putting these pieces together, we reach the conclusion that \((\bm{\pi}^{\sigma},\bm{w}^{\dagger})\) is feasible for the nonlinear program (\(\mathrm{P}_{\mathrm{NE}}^{\prime}\)).

What is left is to prove that it is also an \(\epsilon\)-approximate global minimum. Indeed, if \(\sum_{k}\bm{\rho}^{\top}\bm{w}_{k}^{\dagger}{\leq}\epsilon\) (by assumption of an \(\epsilon\)-approximate CCE), then the objective function of (\(\mathrm{P}_{\mathrm{NE}}^{\prime}\)) will attain an \(\epsilon\)-approximate global minimum. In turn, due to Theorem C.1 the latter implies that \(\bm{\pi}^{\sigma}\) is an \(n\epsilon\)-approximate NE. 

### No equilibrium collapse with more than one controllers per-state

**Example 2**.: _We consider the following \(3\)-player Markov game that takes place for a time horizon \(H=3\). There exist three states, \(s_{1},s_{2},\) and \(s_{3}\) and the game starts at state \(s_{1}\). Player \(3\) has a single action in every state, while players \(1\) and \(2\) have two available actions \(\{a_{1},a_{2}\}\) and \(\{b_{1},b_{2}\}\) respectively in every state. The initial state distribution \(\bm{\rho}\) is the uniform probability distribution over \(\mathcal{S}\)._Reward functions._If player \(1\) (respectively, player \(2\)) takes action \(a_{1}\) (resp., \(b_{1}\)), in either of the states \(s_{1}\) or \(s_{2}\), they get a reward equal to \(\frac{1}{20}\). In state \(s_{3}\), both players get a reward equal to \(-\frac{1}{2}\) regardless of the action they select. Player \(3\) always gets a reward that is equal to the negative sum of the reward of the other two players. This way, the zero-sum polymatrix property of the game is ensured (Assumption 1)._

Transition probabilities._If players \(1\) and \(2\) select the joint action \((a_{1},b_{1})\) in state \(s_{1}\), the game will transition to state \(s_{2}\). In any other case, it will transition to state \(s_{3}\). The converse happens if in state \(s_{2}\) they take joint action \((a_{1},b_{1})\); the game will transition to state \(s_{3}\). For any other joint action, it will transition to state \(s_{1}\). From state \(s_{3}\), the game transition to state \(s_{1}\) or \(s_{2}\) uniformally at random._

_At this point, it is important to notice that two players control the transition probability from one state to another. In other words, Assumption 2 does not hold._

_Next, we consider the joint policy \(\boldsymbol{\sigma}\),_

\[\boldsymbol{\sigma}(s_{1})=\boldsymbol{\sigma}(s_{2})=\begin{smallmatrix}&b_{ 1}&b_{2}\\ &a_{1}&\begin{pmatrix}0&1/2\\ 1/2&0\end{pmatrix}.\end{smallmatrix}\]

**Claim C.1**.: The joint policy \(\boldsymbol{\sigma}\) that assigns probability \(\frac{1}{2}\) to the joint actions \((a_{1},b_{2})\) and \((a_{2},b_{1})\) in both states \(s_{1},s_{2}\) is a CCE and \(V_{1}^{\boldsymbol{\sigma}}(\boldsymbol{\rho})=V_{2}^{\boldsymbol{\sigma}}( \boldsymbol{\rho})=-\frac{1}{10}\).

**Proof.**

\[V_{1}^{\boldsymbol{\sigma}}(\boldsymbol{\rho}) =\boldsymbol{\rho}^{\top}\left(\mathbf{I}-\gamma\,\mathbb{P}( \boldsymbol{\sigma})\right)^{-1}\boldsymbol{r}_{1}(\boldsymbol{\sigma})\] \[=-\frac{1}{10}.\]

We check every deviation,

* \(\boldsymbol{\pi}_{1}(s_{1})=\boldsymbol{\pi}_{1}(s_{2})=(1\quad 0)\,,V^{ \boldsymbol{\pi}_{1}\times\boldsymbol{\sigma}_{-1}}(\boldsymbol{\rho})=-\frac{ 5}{5}\),
* \(\boldsymbol{\pi}_{1}(s_{1})=\boldsymbol{\pi}_{1}(s_{2})=(0\quad 1)\,,V^{ \boldsymbol{\pi}_{1}\times\boldsymbol{\sigma}_{-1}}(\boldsymbol{\rho})=-\frac{ 1}{6}\),
* \(\boldsymbol{\pi}_{1}(s_{1})=(1\quad 0)\,,\,\boldsymbol{\pi}_{1}(s_{2})=(0\quad 1)\,,V^{ \boldsymbol{\pi}_{1}\times\boldsymbol{\sigma}_{-1}}(\boldsymbol{\rho})=-\frac{ 5}{16}\),
* \(\boldsymbol{\pi}_{1}(s_{1})=(0\quad 1)\,,\,\boldsymbol{\pi}_{1}(s_{2})=(1\quad 0)\,,V^{ \boldsymbol{\pi}_{1}\times\boldsymbol{\sigma}_{-1}}(\boldsymbol{\rho})=-\frac{ 5}{16}\).

Figure 2: A graph of the state space with transition probabilities parametrized with respect to the policy of each player.

For every such deviation the value of player \(1\) is smaller than \(-\frac{1}{10}.\) For player \(2\), the same follows by symmetry. Hence, \(\bm{\sigma}\) is indeed a CCE.

_Yet, the marginalized product policy of \(\bm{\sigma}\) which we note as \(\bm{\pi}_{1}^{\bm{\sigma}}\times\bm{\pi}_{2}^{\bm{\sigma}}\) does not constitute a NE. The components of this policy are,_

\[\begin{cases}\bm{\pi}_{1}^{\bm{\sigma}}(s_{1})=\bm{\pi}_{1}^{\bm{\sigma}}(s_{2 })=\begin{pmatrix}a_{1}&a_{2}\\ 1/2&1/2\end{pmatrix},\\ \bm{\pi}_{2}^{\bm{\sigma}}(s_{1})=\bm{\pi}_{2}^{\bm{\sigma}}(s_{2})=\begin{pmatrix} b_{1}&b_{2}\\ 1/2&1/2\end{pmatrix}.\end{cases}\]

_I.e., the product policy \(\bm{\pi}_{1}^{\bm{\sigma}}\times\bm{\pi}_{2}^{\bm{\sigma}}\) selects any of the two actions of each player in states \(s_{1},s_{2}\) independently and unformally at random. With the following claim, it can be concluded that in general when more than one player control the transition the set of equilibria do not collapse._

**Claim C.2**.: The product policy \(\bm{\pi}_{1}^{\bm{\sigma}}\times\bm{\pi}_{2}^{\bm{\sigma}}\) is not a NE.

Proof.: For \(\bm{\pi}^{\sigma}=\bm{\pi}_{1}^{\bm{\sigma}}\times\bm{\pi}_{2}^{\bm{\sigma}}\) we get,

\[V_{1}^{\bm{\pi}^{\sigma}} =\bm{\rho}^{\top}\left(\mathbf{I}-\gamma\,\mathbb{P}(\bm{\pi}^{ \bm{\sigma}})\right)^{-1}\bm{r}_{1}(\bm{\pi}^{\bm{\sigma}})\] \[=\begin{pmatrix}\frac{1}{3}&\frac{1}{3}&\frac{1}{3}\end{pmatrix} \begin{pmatrix}\frac{34}{20}&\frac{20}{34}&\frac{3}{3}\\ \frac{21}{7}&\frac{21}{6}&\frac{2}{7}\\ \frac{21}{7}&\frac{21}{7}&\frac{21}{7}\end{pmatrix}\begin{pmatrix}\frac{1}{40} \\ \frac{40}{42}\\ -\frac{1}{2}\end{pmatrix}\] \[=-\frac{3}{10}.\]

But, for the deviation \(\bm{\pi}_{1}(a_{1}|s_{1})=\bm{\pi}_{1}(a_{1}|s_{2})=0\), the value funciton of player \(1\), is equal to \(-\frac{1}{6}\), Hence, \(\bm{\pi}^{\sigma}\) is not a NE. 

_In conclusion, Assumption 1 does not suffice to ensure equilibrium collapse._

**Theorem C.3** (No collapse--infinite-horizon).: _There exists a zero-sum polymatrix Markov game (Assumption 2 is not satisfied) that has a CCE which does not collapse to a NE._

Proof.: The proof follows from the game of Example 2, and Claims C.1 and C.2. 

## Appendix D An algorithm for approximating Markovian CCE

In this section we describe the algorithm in (Daskalakis et al., 2022) used for the computation a of Markovian CCE. We note that \(M,N_{\mathrm{visit}},p\) are parameters that affect the accuracy of the approximation and we kindly ask the reader to refer to (Daskalakis et al., 2022) for further details. This algorithm computes an \(\epsilon\)-approximate CCE in time \(\tilde{O}(\epsilon^{-3})\). Newer works have improved the dependence on \(\epsilon\) to \(\tilde{O}(\epsilon^{-2})\), see (Wang et al., 2023; Cui et al., 2023).

As soon as an \(\epsilon\)-approximate CCE is computed, what is left to do is to compute the marginal policy of every player, \(\bm{\pi}_{k,h}^{\bm{\sigma}}(a|s)=\sum_{\bm{a}_{-k}\in\mathcal{A}_{-k}}\sigma (a,\bm{a}_{-k}|s)\).

```
1:procedureSPoCMAR(\(n,\mathcal{S},\mathcal{A},H,M,N_{\mathrm{visit}},p\))
2: Set \(\mathcal{V}=\emptyset\).
3: For each \(h\in[H],\;s\in\mathcal{S}\), set \(\boldsymbol{\sigma}^{\mathrm{cover}}_{h,s}=\perp\).
4:for\(q\geq 1\) and while \(\tau=0\)do #Terminate flag.
5: Set \(\tau=1\)
6: Set \(\Pi^{q}_{h}:=\{\boldsymbol{\sigma}^{\mathrm{cover}}_{h,s}:s\in\mathcal{S}\}\) for each \(h\in[H]\).
7:for\(h=H,H-1,\ldots,1\)do
8: Set \(k=0\), and \(\tilde{V}^{q}_{i,H+1}(s)=0\) for all \(s\in\mathcal{S}\) and \(i\in[m]\).
9: Each player \(i\) initializes an adversarial bandit for all \((s,h)\in\mathcal{S}\times[H]\).
10:for each \(\boldsymbol{\sigma}\in\Pi^{q}_{h}\cup\boldsymbol{\sigma}^{\mathcal{U}}\)do #\(\boldsymbol{\sigma}^{\mathcal{U}}\) chooses actions uniformly at random
11:for a total of \(M\) times do
12:\(k\gets k+1\).
13: Let \(\bar{\boldsymbol{\sigma}}\) be the policy which follows \(\boldsymbol{\sigma}\) for the first \(h-1\) steps and plays according to the bandit algorithm for the state visited at step \(h\) (and acts arbitrarily for steps \(h^{\prime}>h\)).
14: Draw a joint trajectory \((s_{1,m},\boldsymbol{a}_{1,m},\boldsymbol{r}_{1,m},\ldots,s_{H,m},\boldsymbol{ a}_{H,m},\boldsymbol{r}_{H,m})\) from \(\bar{\boldsymbol{\sigma}}\).
15:if\((h,s_{h,m})\in\mathcal{V}\)then
16: Each \(i\) updates its bandit alg. at \((h,s_{h,m})\) with \((a_{i,h,m},\frac{H-r_{i,h,m}-\tilde{V}^{q}_{i,h+1}(s_{h+1,m})}{H})\).
17:else
18: Each \(i\) updates its bandit alg. at \((h,s_{h,m})\) with \((a_{i,h,m},\frac{H-(H+1-h)}{H})\).
19:endif
20:endfor
21:endfor
22: For each \(s\in\mathcal{S}\), and \(j\geq 1\), let \(m_{j,h,s}\in[M+1]\) denote the \(j\)th smallest value of \(k\) so that \(s_{h,m}=s\), or \(M+1\) if such a \(j\)th smallest value does not exist.
23: For each \(s\in\mathcal{S}\), let \(J_{h,s}\) denote the largest integer \(j\) so that \(k_{j,h,s}\leq M\).
24: Define \(\bar{\boldsymbol{\sigma}}^{q}_{h}\in\Delta(\mathcal{A})^{\mathcal{S}}\) to be the 1-step policy: \(\bar{\boldsymbol{\sigma}}^{q}_{h}(\boldsymbol{a}|s)=\frac{1}{J_{h,s}}\sum_{j= 1}^{J_{h,s}}\mathbbm{1}[\boldsymbol{a}=\boldsymbol{a}_{h,m_{j,h,s}}]\).
25: Set \[\bar{V}^{q}_{i,h}(s):=\begin{cases}\frac{1}{J_{h,s}}\sum_{j=1}^{J_{h,s}}\left( r_{i,h,k_{j,h,s}}+\bar{V}^{q}_{i,h+1}(s_{h+1,k_{j,h,s}})\right)&:(h,s)\in \mathcal{V}\\ (H+1-h)&:(h,s)\not\in\mathcal{V}.\end{cases}\]
26:endfor
27: Define the joint policy \(\tilde{\boldsymbol{\sigma}}^{q}\), which follows \(\tilde{\boldsymbol{\sigma}}^{q}_{h}\) at each step \(h^{\prime}\in[H]\).
28: Call \(\mathtt{EstVisitation}(\tilde{\boldsymbol{\sigma}}^{q},N_{\mathrm{visit}})\) (Alg. 2) to obtain estimates \(\hat{d}^{q}_{h^{\prime}}\in\Delta(\mathcal{S})\) for each \(h^{\prime}\in[H]\).
29:for each \(s\in\mathcal{S}\) and \(h^{\prime}\in[H]\)do
30:if\(\hat{d}^{q}_{h^{\prime}}(s)\geq p\) and \((h^{\prime},s)\not\in\mathcal{V}\)then
31: Set \(\boldsymbol{\sigma}^{\mathrm{cover}}_{h^{\prime},s}\leftarrow\tilde{\boldsymbol {\sigma}}^{q}\).
32: Add \((h^{\prime},s)\) to \(\mathcal{V}\).
33: Set \(\tau\gets 0\).
34:endif
35:endfor
36:endfor
37:return the policy \(\hat{\boldsymbol{\sigma}}:=\tilde{\boldsymbol{\sigma}}^{q}\).
38:endprocedure ```

**Algorithm 1**SPoCMAR(Daskalakis et al., 2022)

```
1:procedureEstVisitation(\(\boldsymbol{\sigma},N\))
2:for\(1\leq n\leq N\)do
3: Draw a trajectory from \(\boldsymbol{\sigma}\), and let \((s_{1}^{n},\ldots,s_{H}^{n})\) denote the sequence of states observed.
4:endfor
5:for\(h\in[H]\)do
6: Let \(\hat{d}_{h}\in\Delta(\mathcal{S})\) denote the empirical distribution over \((s_{h}^{1},\ldots,s_{h}^{N})\).
7:endfor
8:return\((\hat{d}_{1},\ldots,\hat{d}_{H})\).
9:endprocedure ```

**Algorithm 2**EstVisitation