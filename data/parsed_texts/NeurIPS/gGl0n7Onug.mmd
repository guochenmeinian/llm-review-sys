# Theoretical and Practical Perspectives on what Influence Functions Do

 Andrea Schioppa\({}^{1}\) Katja Filippova\({}^{1}\) Ivan Titov\({}^{2,3}\) Polina Zablotskaia\({}^{1}\)

\({}^{1}\)Google DeepMind \({}^{2}\)University of Edinburgh \({}^{3}\)University of Amsterdam

{arischioppa, katjaf, polinaz}@google.com, ititov@inf.ed.ac.uk

###### Abstract

Influence functions (IF) have been seen as a technique for explaining model predictions through the lens of the training data. Their utility is assumed to be in identifying training examples "responsible" for a prediction so that, for example, correcting a prediction is possible by intervening on those examples (removing or editing them) and retraining the model. However, recent empirical studies have shown that the existing methods of estimating IF predict the leave-one-out-and-retrain effect poorly. In order to understand the mismatch between the theoretical promise and the practical results, we analyse five assumptions made by IF methods which are problematic for modern-scale deep neural networks and which concern convexity, numeric stability, training trajectory and parameter divergence. This allows us to clarify what can be expected theoretically from IF. We show that while most assumptions can be addressed successfully, the parameter divergence poses a clear limitation on the predictive power of IF: influence fades over training time even with deterministic training. We illustrate this theoretical result with BERT and ResNet models. Another conclusion from the theoretical analysis is that IF are still useful for model debugging and correcting even though some of the assumptions made in prior work do not hold: using natural language processing and computer vision tasks, we verify that mis-predictions can be successfully corrected by taking only a few fine-tuning steps on influential examples.

## 1 Introduction and related work

Influence Functions (IF) [10] have been regarded as a tool that can trace model behavior on any example to the training examples [1, 2, 1]. Their theoretical justification lies in the ability to predict loss changes on a specific test point when training on a perturbed loss obtained by removing or down-sampling a given training point. In the case of an undesired model behavior on a test-point, the influential training examples for that test point have been assumed to be the ones "responsible" for the prediction so that intervening on those - e.g., by removing them and then _retraining_ the model, or by taking additional fine-tuning steps [1] - would result in a change in the loss or prediction. It has been confirmed that for linear models it is indeed the case [1].

Recently, in their extensive experiments, [2] and [10] could not find empirical support for the claim that IF approximate the Leave-Some-Out Retraining (LSOR) effect on the loss in deep neural networks. In particular, they show that the correlation between the ranking of training examples produced by LSOR and the IF-based ranking is low and considerably affected by choice of hyperparameters. How can this discrepancy be explained? And, given that the theoretical justification for IF lacks empirical support, does it mean that IF should be abandoned as an explainability and debugging tool altogether?

In this work we clarify what question Influence Functions (IF) actually answer. We first identify assumptions which are either implicit or not investigated in prior work: these concern convexity,numeric stability, training trajectory, and the the parameter divergence when retraining on a new loss. _In principle, any of these assumptions might be problematic and be the reason why the original theoretical justification for IF is not supported empirically_. However, we show how to address most of them successfully so that they cannot be the reason for the aforementioned discrepancy. Unfortunately, we also show that the parameter divergence is indeed problematic and requires to revise both theoretical and practical expectations about IF. This analysis paves the way to clarifying the question that IF can answer. As a first step in this direction, we need to distinguish two approaches to computing influence. The _Hessian-based Influence Functions (HIF)_[12, 13] have a rigorous theoretical justification in statistics, but rely on strict convexity assumptions, which are not met in Deep Learning and which previous analyses of HIF in the Deep Learning literature still rely on [13, 14]. Our first contribution concerns HIF and is twofold: we prove (Theorem 1) that this unsatisfied convexity assumption is not as problematic for HIF as one may think-given a stationary point for the original loss, there is a nearby stationary point for the perturbed loss which can be approximated by using HIF. However, we point out a more serious problem with HIF: _there is no guarantee that by retraining on the perturbed loss one would get to that stationary point_. This observation provides an additional support to the second popular approach to IF, TracIn [15], which explicitly models the training dynamics and which additionally does not require to compute the expensive inverse Hessian vector product as it only uses gradient information.1 Despite TracIn being grounded in the training dynamics, we unveil a hidden additive modeling assumption in [15] that _prevents it from correctly modeling the (re)-training dynamics_. Our second contribution is thus to provide a theoretical analysis (Theorem 2) of how training trajectories change when perturbing the loss. This is a key result as it suggests that two training trajectories differing by a small loss perturbation _could diverge over time to the point of violating a first-order expansion assumption that IF make_, thus making IF's predictions unreliable. Our further theoretical and empirical investigation confirms this conjecture. Therefore, _what IF can do is to predict parameter changes when fine-tuning for a limited number of steps on the perturbed loss. This requires to adjust how IF are evaluated (Section 5) and applied (Section 6)_. In order to confirm the conjecture and re-adjust expectations regarding IF, we need a deeper analysis of the trajectory divergence.

Footnote 1: To incorporate training dynamics, TracIn exploits multiple checkpoints which introduces a substantial overhead, hence only the latest checkpoint is often used in practice. Other gradient-based methods have been proposed in [16, 2], but they lack a justification from the training dynamics perspective.

Our third contribution is thus to validate the conjecture on trajectory divergence and its consequences for IF. We first prove (Theorem 3), using a discrete version of Gronwall's Lemma [1], an upper bound on the parameter divergence when (re)-training on a perturbed loss. We then empirically verify that the bound is sharp in Section 5 and that IF can approximate parameter changes along the perturbed trajectory _only for a limited amount of time_. We then empirically verify that this leads to a _fading (of accuracy)_ of IF predictions over time. Therefore, we theoretically demonstrate and empirically verify that IF can in general answer only what happens _when fine-tuning on a perturbed loss for a limited amount of time_, instead of a general retraining setting.

Therefore, our new theory indicates that IF have been used incorrectly as the emphasis has been on their LSOR potential. On the positive side, it suggests an alternative way of using IF that indeed yields substantial empirical improvement. To demonstrate that, as our final contribution, in Section 6 we propose and verify a very simple method for correcting mis-predictions by taking only a few gradient steps on influential examples. Our proposal is related to model editing [1], inter alia] in that the latter also aims at changing model predictions. However, there the model is given and the modifications are done on its parameters whereas IF aim at understanding how specific training examples are responsible for the current model behavior and editing predictions through the data. We leave for future work building connections between model editing and IF.

For the case of Hessian-based influence functions, the recent work of [14] has proposed to resolve the discrepancy between the theory and the empirical evidence by evaluating them using the proximal Bregman response function. However, [14] relies crucially on the convexity assumption: while under this assumption there is agreement between our work and their findings (e.g. our parameter divergence drives the linearization error [14, Sec. 4.4]), our theory applies also the the case where the Hessian is not positive definite. Moreover, our work covers also the scope of Gradient-based influence functions including TracIn [15].

Definition of Influence Functions and Notation

The different methods proposed to define Influence Functions share a common goal: _forecasting the change in the prediction on a test example when up (or down-) weighting a training example_. This is achieved by tracing the effect that re-weighting a training example has on the model parameters.

Removing or adding a training point can be modeled by a _perturbation_ of the loss function; such a perturbation can be made smooth by modeling the weighting of a training point by a continuous parameter \(\varepsilon\). More generally, let \(L(\theta)\) denote the loss function where \(\theta\in\textbf{R}^{N}\) are the model parameters. We model loss perturbations by introducing _a variation of the loss_, which is a smooth function \(\mathcal{L}(\theta,\varepsilon)\) depending on an additional vector parameter \(\varepsilon\in\textbf{R}^{Q}\) and coinciding with the original loss for \(\varepsilon=0\). For example if \(l_{x}\) denotes the loss on a given training point \(x\), we set \(\mathcal{L}(\theta,\varepsilon)=L(\theta)+\varepsilon l_{x}\). While the scalar case \(Q=1\) is the one commonly considered, e.g. [10, 11], _we introduce the vector one_\(Q>1\) which arises naturally when considering the effect of re-weighting multiple points differently, e.g. when modifying the weights in a mixture of different data-sets.

The IF method of choice then predicts _what would happen if training on \(\mathcal{L}(\theta,\varepsilon)\) instead of \(L(\theta)\)_. The final parameters are modeled as a function \(\theta_{\varepsilon}\) of the perturbation parameter; _assuming that such a function is well-defined and sufficiently smooth_, one then makes a first order expansion \(\theta_{\varepsilon}\simeq\theta_{0}+\varepsilon^{T}\nabla_{\varepsilon|0} \theta_{\varepsilon}\), where \(\nabla_{\varepsilon|0}\theta_{\varepsilon}\) is the \(Q\times N\)-dimensional Jacobian at \(\varepsilon=0\) (see also Section A).

Under this first order assumption it is then straightforward to measure the change in the loss \(l_{z}\) corresponding to a test point \(z\):

\[l_{z}(\theta_{\varepsilon})-l_{z}(\theta_{0})\simeq\varepsilon^{T}\nabla_{ \varepsilon|0}\theta_{\varepsilon}\nabla_{\theta|\theta_{0}}l_{z}.\] (1)

We emphasize that (1) is _general to different IF methods_, which _differ in the specific derivation_ of \(\nabla_{\varepsilon|0}\theta_{\varepsilon}\).

## 3 Problematic assumptions made by Influence Functions

Problematic Assumption #1: Convexity can be used to show that \(\theta_{\varepsilon}\) is a function of \(\varepsilon\)

In order to show that for each value of \(\varepsilon\) there is a single value of \(\theta_{\varepsilon}\) (so that one can model the final parameters as a function of the perturbation parameter), HIF relies on strict convexity of \(L\). While this assumption is realistic for the statistical models considered in [10], this is _not the case for neural networks_. Even when introducing a regularization term, the loss of a neural network is not even weakly convex, and optimization methods usually converge to saddle points [2]. To the best of our knowledge, previous analyses of HIF in the Machine Learning literature, e.g. [10, 1], _have relied on some form of strict convexity_. In Section 4.1 we will revisit HIF and prove (Theorem 1), roughly speaking, that near a given stationary point \(\theta_{0}\) for the original loss \(L\), there is a stationary point \(\theta_{\varepsilon}\) of the perturbed loss \(\mathcal{L}(\theta,\varepsilon)\) which can be modeled as a function of \(\varepsilon\) and such that \(\nabla_{\varepsilon|0}\theta_{\varepsilon}\) is given by \(-H_{\theta_{0}}^{-1}\nabla_{(\varepsilon,\theta)|(0,\theta_{0})}^{2}\mathcal{L}\) as in [10] (see Section A regarding the usage of \(\nabla^{2}\)).

### Problematic Assumption #2: The model Hessian is not degenerate

As HIF requires to apply the inverse model Hessian to \(\nabla_{(\varepsilon,\theta)|(0,\theta_{0})}^{2}\mathcal{L}\), one needs to _ensure that inverting the Hessian is numerically stable_. It has been empirically demonstrated [1] that most eigenvalues of the Hessian tend to cluster near \(0\). Numerically, this results in a considerable source of errors and instabilities when estimating HIF; while regularization can alleviate this problem, it introduces a hyper-parameter in the definition of HIF; the minimal value of such a hyper-parameter ensuring numerical stability depends on the _smallest negative eigenvalue of the Hessian_. Unfortunately, in realistic settings, this can be _larger in absolute value than reasonable values for the regularization parameter_. For example in our ResNet experiments regularization is of the order \(10^{-4}\), while the smallest negative eigenvalue is \(\simeq-5\). In Section 4.2 we discuss how the Arnoldi-based Influence Functions (abbr. ABIF) (which were introduced by [13] for computational efficiency) can be used to address such instability issues.

### Problematic Assumption #3: Training trajectory can be ignored in Hessian-based Influence

Even if we solve the Problematic Assumption #1 for HIF, there is no guarantee that when actually re-training from scratch on \(\mathcal{L}(\theta,\varepsilon)\) one would converge to the \(\theta_{\varepsilon}\) given by Theorem 1 because the training trajectory is disregarded in HIF. As optimization is performed via some form of stochastic gradient descent, [10] propose _TracIn_ which averages gradient dot-products across checkpoints in order to take into account the path taken by the training process. Importantly, for a single checkpoint \(\theta_{0}\) TracIn estimates \(\nabla_{\varepsilon}\theta_{\varepsilon}\) as \(-\nabla^{2}_{(\varepsilon,\theta)|(0,\theta_{0})}\mathcal{L}\), so the inverse Hessian vector product does not need to be computed. While it seems that TracIn takes into account the training trajectory, in the next Assumption we identify an issue with the way it models the training trajectory.

### Problematic Assumption #4: The training trajectory can be modelled additively

The analysis of TracIn in [10] is based on a first-order expansion of the final change of the loss of a test point in terms of the gradient steps across the training trajectory. While this argument seems mathematically convincing, it overlooks that if one point is removed, or slightly up-sampled / down-sampled, _the subsequent training trajectory is modified_. Let \(\theta_{\varepsilon,t}\) denote the value of the parameters after \(t\) steps when doing gradient descent on \(\mathcal{L}(\theta,\varepsilon)\). Denoting by \(T\) the end-time, TracIn derives

\[\nabla_{\varepsilon|0}\theta_{\varepsilon,T}=-\sum_{t=0}^{T-1} \eta_{t}\nabla^{2}_{(\varepsilon,\theta)|(0,\theta_{0,t})}\mathcal{L},\] (2)

where \(\eta_{t}\) is the learning rate at time step \(t\). Now, the right-hand side in formula (2) is purely additive in the time steps; and addition is commutative, so _the order of the time steps does not matter_. One way to see that this is problematic is by making \(\mathcal{L}\) time-dependent so that it differs from \(L\) only at a specific time step \(t\). In this case \(\nabla^{2}_{(\varepsilon,\theta)}\mathcal{L}\) would be non-zero only at \(t\) and formula (2) would consist of a single term. However, we would expect the perturbation at \(t\)_to affect the following time steps_, so we should have at least \(T-t-1\) terms on the RHS for (2). In Section 4.3 we compute \(\nabla_{\varepsilon}\theta_{\varepsilon,T}\) looking at the whole training trajectory and _discover an additional first-order term that is missing from TracIn_: _this term models the dependency of a time step on the earlier ones_. Concurrent work [12] also criticizes the additive assumption in TracIn on empirical grounds and proposes to build (re)-training simulators which are unfortunately computationally expensive as a new simulator must be fitted on the training set for each test point.

Problematic Assumption #5: \(\theta_{\varepsilon}\) can be expanded to first order in \(\varepsilon\)

After we derive a formula for \(\theta_{\varepsilon,T}\) and \(\nabla_{\varepsilon}\theta_{\varepsilon,T}\) we realize that the latter can grow in norm in \(T\). However, if \(\theta_{\varepsilon,T}\) can be Taylor-expanded in \(\varepsilon\), we _need \(\theta_{\varepsilon,T}-\theta_{0,T}\) to be \(O(\varepsilon)\) and \(\nabla_{\varepsilon}\theta_{\varepsilon,T}\) to be \(O(1)\)_. If this is not the case, the whole IF approach described in Section 2 breaks down because IF approximate the parameter change \(\theta_{\varepsilon,T}-\theta_{0,T}\) using the Taylor expansion \(\varepsilon^{T}\nabla_{\varepsilon}\theta_{\varepsilon,T}\), but _the conditions to apply such a Taylor expansion are not satisfied_.

In Section 4 we show how assumptions #1-#4 can be successfully addressed which makes them not as problematic as they may first appear. However, for assumption #5 we will see that it puts a substantial limitation on the predictive power of IF. At the same time it allows for a new, locally bound perspective on IF - that influence holds for a limited number of fine-tuning steps (Section 5). Based on this finding, in Section 6 we propose a simple approach to use IF to correct mis-predictions which is theoretically grounded and is in addition much less compute intensive than those that involve re-training (e.g. [10]).

## 4 Addressing the problematic assumptions

_Proofs of all results are in the Appendix and we provide some motivation for the proof in the main text._

### HIF does not need Assumption #1

Previous work [13, 14, 15, 16] on Hessian-based Influence Functions (HIF) has assumed that \(L\)_is strictly convex_ in order to claim that 1) _the minimum is unique_ so that \(\theta_{\varepsilon}\) can be modeled as a function, and 2) to use the Implicit Function Theorem to differentiate through the optimality condition.

Here we _will just assume that the Hessian is not singular at \(\theta_{0}\)_; by requiring that the final gradients do not change as we change \(\varepsilon\) we prove:

**Theorem 1**.: _Assume that \(\nabla\mathcal{L}\) is \(C^{k}\) (\(k\geq 1\)) and let \(\theta_{0}\in\textbf{R}^{N}\); assume that the Hessian \(H_{\theta_{0}}=\nabla^{2}_{\theta|\theta_{0}}L\) is non-singular; then there exist neighborhoods \(U\) of \(\theta_{0}\) and \(V\) of \(0\in\textbf{R}^{Q}\), and a \(C^{k}\)-function \(\Theta:V\to U\) such that \(\Theta(0)=\theta_{0}\) and \(\Theta(\varepsilon)\in U\) is the unique solution in \(U\) of the equation_

\[\nabla_{\theta}\mathcal{L}(\Theta(\varepsilon),\varepsilon)=\nabla_{\theta} \mathcal{L}(\theta_{0},0).\] (3)

_Moreover, the gradient of \(\Theta\) at the origin is given by:_

\[\nabla_{\varepsilon|0}\Theta=-H_{\theta_{0}}^{-1}\nabla^{2}_{(\varepsilon, \theta)|(0,\theta_{0})}\mathcal{L}.\] (4)

Idea of the proof.: Existence of \(\Theta(\varepsilon)\) is formulated as a solution to the local problem eq. (3); building a local solution does not require convexity if one uses the Implicit Function Theorem. 

The requirement that \(\nabla_{\theta}L(\Theta(\varepsilon),\varepsilon)\) is constant in \(\varepsilon\) has allowed us to establish a link between the training under different losses \(\{\theta\rightarrow\mathcal{L}(\theta,\varepsilon)\}_{\varepsilon}\). If we assume that \(\theta_{0}\) is a _stationary point_, i.e. \(\nabla_{\theta}L(\theta_{0})=0\), we can strengthen the conclusions:

**Corollary 1**.: _Under the assumptions of Theorem 1:_

1. _If_ \(\theta_{0}\) _is a stationary point of_ \(L\)_, then each_ \(\Theta(\varepsilon)\) _is a stationary point of the loss_ \(\theta\mapsto\mathcal{L}(\theta,\varepsilon)\)_._
2. _If_ \(\theta_{0}\) _is a local (strict) minimum of_ \(L\)_, for_ \(\varepsilon\) _sufficiently small,_ \(\Theta(\varepsilon)\) _is a local (strict) minimum of the loss_ \(\theta\mapsto\mathcal{L}(\theta,\varepsilon)\)_._
3. _Let_ \(Q=1\) _(hence epsilon is a scalar) with_ \(\mathcal{L}(\theta,\varepsilon)=L(\theta)+\varepsilon l_{x}(\theta)\)_, where_ \(l_{x}\) _is the loss corresponding to a specific training point_ \(x\)_. We then obtain the classical result_ _[_13_]__:_ \[\frac{d\Theta}{d\varepsilon}\Big{|}_{\varepsilon=0}=-(\nabla^{2}_{\theta}L( \theta_{0}))^{-1}\nabla_{\theta}l_{x}(\theta_{0}).\] (5)

### If Assumption #2 is not satisfied, use Arnoldi-based Influence Functions

Theorem 1 requires that \(H_{\theta_{0}}\) is non-singular. If the Hessian is singular, _we just need to keep fixed those parameters that are responsible for the degeneracy_. More precisely, we diagonalize \(H_{\theta_{0}}\); we let \(P_{1}\) be the subspace spanned by the eigenvectors corresponding to the non-zero eigenvalues and let \(P_{0}\) be its orthogonal complement, that is, the kernel of \(H_{\theta_{0}}\). Up to an orthogonal transformation of the parameters, we can assume that \(P_{1}\) is spanned by the first \(N_{1}\)-coordinates and decompose \(\theta=(\vartheta,\varphi)\in\textbf{R}^{N1}\times\textbf{R}^{N2}\) so that \(H_{\theta_{0}}=\nabla^{2}_{\vartheta}L((\vartheta_{0},\varphi_{0}))\) is non-singular. We then apply Theorem 1 to the restricted variation \((\vartheta,\epsilon)\mapsto L((\vartheta,\varphi_{0}),\epsilon)\). In terms of the original parameters \(\theta\), this means that the function \(\Theta(\varepsilon)\) is constrained to lie in \(\theta_{0}+P_{1}\), keeping the \(\varphi\)-component constantly equal to \(\varphi_{0}\). Concretely, we can approximate \(P_{1}\) using the Arnoldi iteration; therefore, _we can address the failure of Assumption #2 by using Arnoldi-based Influence Functions_ (ABIF) [17], which approximate \(P_{1}\) using the subspace spanned by the eigenvectors corresponding to the top-k (in absolute value) eigenvalues of the Hessian. The fact that ABIF stabilizes the estimation of influence scores has been observed empirically in previous work: in [17, Fig. 2] where using ABIF instead of the full Hessian improves the retrieval of mislabeled examples, and in [15, Fig. 3] where ABIF is the best solver on the QA task and is comparable to the other solvers on the text-completion task.

### The training trajectory can be traced to address Assumptions #3-#4

We need to improve our notation to correctly trace the training trajectory. The first issue is to keep track of the parameters across the time steps; the second issue are _sources of non-determinism_,e.g. batch selection or random state for dropout. When comparing training trajectories for different values of \(\varepsilon\) we want our notation to account for sources of non-determinism, as they might increase the difference between the training trajectories.

To address the first issue, we let \(\theta_{\varepsilon,t}\) be the value of the parameters after training on \(\mathcal{L}(\theta,\varepsilon)\) for \(t\) steps. In particular, \(\theta_{\varepsilon,0}\)_denotes_ the initial value condition that we assume held fixed at \(\theta_{\mathrm{init}}\) for different values of \(\varepsilon\). As random state is a function of the training step (e.g. the batch to use at step \(t\)), to address the second issue, we just need to allow both the loss and the variation to depend on the train step, denoting them by \(L_{t}\) and \(\mathcal{L}_{t}\).

To simplify the exposition and for consistency with [20] we assume that models are trained with stochastic gradient descent. Letting \(\eta_{t}\) be the learning rate at step \(t\) we prove:

**Theorem 2**.: _Assume that the model is trained for \(T\) time-steps with stochastic gradient descent. Denoting by \(H_{t}\) the Hessian \(\nabla^{2}_{\theta|\theta_{0,t}}L_{t}\), then the final parameters \(\theta_{\varepsilon,T}\) satisfy:_

\[\nabla_{\varepsilon|0}\theta_{\varepsilon,T}=-\sum_{t=0}^{T-1}\eta_{t}\nabla^ {2}_{(\varepsilon,\theta)|(0,\theta_{0,t})}\mathcal{L}_{t}-\sum_{t=0}^{T-1} \eta_{t}H_{t}\nabla_{\varepsilon|0}\theta_{\varepsilon,t}.\] (6)

_Idea of the proof._ One can explicitly write a system of equations for \(\theta_{\varepsilon,T}\) and apply the operator \(\nabla_{\varepsilon|0}\) to the solution of this system. 

Note that the second term on the RHS of (6), which is missing from (2), takes into account the _contribution of the earlier time steps_ that is missing from the analysis of [20]. A practical consequence of this second term is that the norm of \(\nabla_{\varepsilon|0}\theta_{\varepsilon,T}\) might grow (in \(T\)) more quickly than (2) would suggest: this is closely related to Assumption #5 which requires \(\nabla_{\varepsilon|0}\theta_{\varepsilon,T}\) to be \(O(1)\). In particular, for a constant learning rate, while (2) suggests a linear growth in the time step \(T\), we will empirically verify in Section 5.1 that the growth is super-linear.

In Section K we illustrate the additional modeling error introduced by TracIn when computing the Jacobian \(\nabla_{\varepsilon|0}\theta_{\varepsilon,T}\) in the case of retraining BERT with SGD.

### Assumption #5 becomes problematic over time

To address Assumption #5 we need to look into the _parameter divergence_\(\|\theta_{\varepsilon,T}-\theta_{0,T}\|\) between training on \(L\) and \(\mathcal{L}\). The training dynamics is a discrete version of an ODE for which uniqueness and differentiability of the solutions with respects to the initial conditions can be established by Gronwall's Lemma [14]. The parameter \(\varepsilon\) can itself be considered an initial condition and we can prove a discrete version of Gronwall's Lemma to bound the parameter divergence. For simplicity of notation we prove the result for stochastic gradient descent, but we also sketch in the Appendix how to modify the argument to deal with optimizers.

**Theorem 3**.: _In the setting of Theorem 2 assume that, for \(t\leq T\), \(\theta_{\varepsilon,t}\) lies in a bounded region \(R\) such that_

\[\sup_{t,\theta\in R}\|\nabla\mathcal{L}_{t}(\theta,\varepsilon)-\nabla \mathcal{L}(\theta,0)\|\leq C\varepsilon\] (7)

_and that for \(\theta\in R\) each loss \(\mathcal{L}_{t}(\theta,\varepsilon)\) and its gradient wrt. \(\theta\) are \(A\)-Lipschitz in \(\theta\). Then_

\[\|\theta_{\varepsilon,T}-\theta_{0,T}\|\leq C\varepsilon\sum_{s<T}\eta_{s}(1+ \exp(2A\sum_{s<T}\eta_{s})).\] (8)

_Idea of the proof._ If the time steps were infinitesimal, i.e. if time was made continuous, the evolution of \(\theta_{\varepsilon,t}\) would be governed by an ODE; then the bound (8) would be straightforward by applying the classical Gronwall's Lemma [14]. In our case we just need to modify the ODE arguments to work with discrete time. 

Note that the bound (8) is quite pessimistic as it involves an _exponential of the integrated learning rate_\(\sum_{s<T}\eta_{s}\). This means that as \(\sum_{s<T}\eta_{s}\) increases, the parameter divergence is no longer \(O(\varepsilon)\) and the crucial Assumption #5 is no longer satisfied. This observation leads to a few crucial conclusions:

1. An IF method can predict \(\theta_{\varepsilon,t}\) only for a limited amount of time-steps: it is therefore incorrect to evaluate IF methods on LSOR or retraining from scratch.
2. IF methods need to be evaluated on what they can potentially do; therefore the evaluation setup should consist of fine-tuning on the perturbed loss only a limited amount of steps with evaluation metrics being reported as a function of the step.
3. Applying IF for correcting mis-predictions should also involve a time-bound scenario: we propose such a method in Section 6.
4. Sources of non-determinism between two training runs will likely increase the parameter divergence. So one should try to reduce this with _deterministic training_. For example, in the case of re-weighting a point \(x\), i.e. setting \(\mathcal{L}=L+\varepsilon l_{x}\), one should make sure to use the same batch \(B_{t}\) for the loss \(L\) at time step \(t\) across training runs for different values of \(\varepsilon\).

## 5 Illustrating the Theory

In this section we first demonstrate Theorem 3 empirically and then verify that the predictive power of influence scores degrades over time. Full details of our experimental setup are reported in the Appendix. We consider binary classification for nlp, where we fine-tune BERT on SST2; for computer vision we consider multi-class classification where we train from scratch ResNet on CIFAR10. _All our experiments use deterministic training_: the order of the training batches for the loss \(L\) is held fixed across different runs, as well are the random generators when dropout is used.

### Illustrating Parameter Divergence (Theorem 3)

Theorem 3 provides an upper bound when re-training on a perturbed loss. Such a bound is rather pessimistic as it involves the integrated learning rate. We therefore investigate empirically what happens with some typical Deep Learning setups. We take an intermediate checkpoint and keep training on a new loss obtained by up-sampling 16 training points with a weight \(\varepsilon\), that is: \(\mathcal{L}_{t}=L_{B_{t}}+\varepsilon\cdot L_{B}\), where \(B_{t}\) is the training batch for step \(t\) and \(B\) is the batch of 16 points selected for up-sampling. For each time step \(t\) we then compute \(\|\theta_{\varepsilon,T}-\theta_{0,T}\|\) and then plot it against the integrated learning rate, see Figure 1.

Unfortunately, we observe that in these experiments _the upper bound in Theorem 3 is matched by a lower bound with the same exponential divergence_. We observe a first phase of quick divergence and then a second one in which the divergence is slower. For the second phase a linear fit of \(\log\|\theta_{\varepsilon,T}-\theta_{0,T}\|\) against the integrated learning rate appears to be strong: for example for BERT we obtain an \(R^{2}\) of at least \(0.9\) across the different values of \(\varepsilon\). The fitted slope, corresponding to \(A\) in Theorem 3 depends on \(\varepsilon\) and varies between \(30\) and \(130\).

Figure 1: Divergence of parameters (log-scale) as a function of the integrated learning rate. For each value of \(\epsilon\) the divergence is exponential (corresponding to a line in log-scale) with two different divergence rates, one more steep at the beginning of (re)-training. (a) BERT, (b) ResNet

### Illustrating the fading of influence

We now verify that the predictive power of influence scores fades over time. We again fix a model checkpoint \(\theta_{\mathrm{init}}\) and select 32 training points and 16 test points. For each training point \(x\) we retrain on \(\mathcal{L}_{t}^{x}=L_{B_{t}}-\frac{1}{100}l_{x}\), i.e. \(x\) has been down-sampled; for each test point \(z\) and time step \(t\) we then compute the loss difference \(\delta(z,x,t)=l_{z,x,t}-l_{z,t}\) where \(l_{z,x,t}\) is obtained when (re)-training on \(\mathcal{L}_{t}^{x}\) and \(l_{z,t}\) is obtained when training on the vanilla loss \(L_{t}=L_{B_{t}}\). Again, we have kept the order of the batches \(B_{t}\) the same when re-training. Now, at the original checkpoint \(\theta_{\mathrm{init}}\) we can compute the influence scores \(IF(z,x)\) for different methods, e.g. TracIn or HIF (using the the Conjugate Residual method2). For each time step we thus have \(32\times 16\) values of \(\delta(z,x,t)\) that can be linearly regressed against \(IF(z,x)\); _the corresponding Pearson correlation \(R(t)\) then measures the predictive power of influence scores on the loss shifts when re-training_. We repeat the experiments for ResNet \(9\) times and for BERT \(25\) times, with a different selection of train and test points, so that we obtain confidence intervals for the resulting time-series \(R(t)\). Ideally, the theory behind an influence method predicts \(R(t)\sim 1\). However, as discussed above, Assumption #5 is indeed problematic and, because of the parameter divergence illustrated in 5.1 we expect \(R(t)\) to degrade over time.

Footnote 2: Further details in the Appendix.

As Figure 2 demonstrates, this is indeed the case. The predictive power is high for BERT after a few re-training steps and then degrades quickly oscillating around 0 (which is covered by the confidence intervals). For ResNet, the predictive power is never as high, _but it still degrades monotonically over time, as predicted by the theory_. As we see in Section 6, the proponents retrieved for ResNet are less effective at correcting mis-predictions than those retrieved for BERT: we conjecture that this is related to the worse predictive power of IF in the case of ResNet. As BERT was trained with the Adam Optimizer, we also considered a variant which takes into account the optimizer's pre-conditioner by multiplying the gradients by the square root of the pre-conditioning matrix. In this case the predictive power is worse than for the vanilla version of TracIn. For BERT, more plots and a further discussion about TracIn are included in the Appendix (Section G.1). In the Appendix (Section G.2), we also illustrate the fading of influence for another NLP pre-trained model, T5. While we have illustrated the fading of influence for both BERT and ResNet, in the latter case the peak of correlation is quite low; we repeated the same experiments on the ViT (Section G.3), where we find a high correlation peak as in the case of BERT.

## 6 Using Influential Examples for Error Correction

[1] propose to correct model mis-predictions by first using influence scores to retrieve the examples most responsible for a given prediction, and, after correcting them, _retraining the model_. Computational considerations aside, a key conclusion from the theory in Section 3 and the empirical verification in Section 5.2 is that IF only predict influence over a limited number of training steps. In this section we demonstrate that IF can still be used to correct model mis-predictions by _taking a few fine-tuning steps on influential examples_.

Figure 2: The predictive power of influence scores on the loss shifts degrades over time. (a) BERT, (b) ResNet. The line represents the average of the correlation \(R(t)\) across runs, with the shaded area the corresponding \(95\%\) confidence region.

Concretely, we propose to correct mis-predictions at a given test point \(z\) by first identifying a batch of influential examples \(B\) and then taking a few fine-tuning steps on the perturbed loss \(\mathcal{L}=L+\varepsilon\cdot l_{B}\). We propose two methods: (1) _Proponents-correction_: we identify the set \(B\) of top-k proponents for the current \(x\) and _relabel_ them to what should be the correct prediction on \(x\); (2) _Opponents-tuning_: as opponents _oppose the current prediction_, we take \(B\) to be the set of top-k opponents of \(x\).

We investigate how well these error-correction techniques work on SST2 (BERT) and CIFAR10 (ResNet). As a baseline, we randomly sample a set \(B\) of training points with the same label as the prediction on \(x\) and then set their label equal to the correct one for \(x\). We take a maximum of \(50\) fine-tuning steps and take the top-50 proponents or opponents to build \(B\). The main metric we compute is the _success rate_, i.e. the ratio of mis-predictions successfully corrected within the limit of \(50\) steps. Additionally, we report _prediction retention_[DCAT21] on a fixed held-out set of \(50\) test examples, i.e. the ratio of examples predictions which have not changed after a correction - ideally, a correction does not cause too many changes in model predictions otherwise. We experiment with different values of \(\varepsilon\), starting from no up-sampling and gradually increasing it to when influential examples account for slightly more than half of the batch.

From Figure 3 we see that Proponents-correction and Opponents-tuning strongly outperform the baseline in binary classification (SST2). For multi-class classification (CIFAR10) Opponents-tuning is not effective, as we verified that only 54% of the retrieved opponents have the desired label; Proponents-correction still outperforms the baseline increasing on average the success rate by 2% and reducing the number of steps to take by 6%. We conjecture that for ResNet the improvement over the baseline is less than for BERT because of the worse predictive power of IF (Figure 2 (b)). In Appendix H we include additional plots showing the number of steps to correct mis-predictions as a function of the up-sampling parameter \(\varepsilon\).

The primary goal of the experiments in this section is to verify that tuning on influential examples results in a correction more reliably and faster than on other classes of examples. Since the SST2 and CIFAR10 datasets are largely clean, for the proponents method, the training example label is flipped to an incorrect one. In Appendix I we give examples from a noisy text classification dataset illustrating the scenario that the correction-with-proponents method is supposed to address.

## 7 Limitations

We derive the perturbed training trajectory (Theorem 2) and the divergence of trajectories (Theorem 3) for stochastic gradient descent and, while we sketch in the Appendix the modifications needed when using other optimizers, we do not pursue this topic in detail. In Section 6 we measure prediction retention after correcting mis-predictions. While this metric is intuitive and has been used previously (e.g., [DCAT21]), we do not distinguish between semantically similar and unrelated examples and thus do not check the consistency and generalization properties of the update [MBAB22], leaving a thorough study of the correction-retention tradeoff to future work.

Figure 3: Success rate and retention for error correction. (a) BERT, (b) ResNet

Conclusions

IF have been regarded as a tool that promises to trace model behavior to the training data. Unfortunately, recent studies have found no empirical support for such a claim as IF fail to predict the LSOR effect. This finding gives rise to the question of what IF methods really predict and whether they could be useful for model debugging. In this work we clarified which questions IF can be expected to answer. We first identified problematic assumptions made by IF methods - a priori any of these assumptions could be a reason for the observed empirical failure of IF. Thus, for each assumption we studied if it is indeed problematic. While most have turned out to be addressable in one way or another, we demonstrated that the one about parameter divergence puts a severe limitation on IF. With a deeper analysis of this assumption, we revised what can be theoretically expected from IF: IF methods are time-bound, that is, they can at most predict what happens when fine-tuning on a perturbed loss for a limited amount of time. With that, a practical usage of IF for model debugging is still possible - we proposed and empirically validated a theoretically-grounded procedure to apply IF to correct model mis-predictions.

## References

* [BNL\({}^{+}\)22] Juhan Bae, Nathan Ng, Alston Lo, Marzyeh Ghassemi, and Roger Grosse. If influence functions are the answer, then what is the question?, 2022.
* [BPF21] Samyadeep Basu, Phil Pope, and Soheil Feizi. Influence functions in deep learning are fragile. In _International Conference on Learning Representations_, 2021.
* [CGFT19] Guillaume Charpiat, Nicolas Girard, Loris Felardos, and Yuliya Tarabalka. Input similarity from the neural network perspective. In H. Wallach, H. Larochelle, A. Beygelzimer, F. dAlche Buc, E. Fox, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 32. Curran Associates, Inc., 2019.
* [CS82] R. Cook and Weisberg S. _Residuals and influence in regression_. Chapman and Hall, New York, 1982.
* [DCAT21] Nicola De Cao, Wilker Aziz, and Ivan Titov. Editing factual knowledge in language models. In _Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing_, pages 6491-6506, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics.
* [DPG\({}^{+}\)14] Yann N Dauphin, Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho, Surya Ganguli, and Yoshua Bengio. Identifying and attacking the saddle point problem in high-dimensional non-convex optimization. In Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence, and K.Q. Weinberger, editors, _Advances in Neural Information Processing Systems_, volume 27. Curran Associates, Inc., 2014.
* [FLP\({}^{+}\)23] Jillian Fisher, Lang Liu, Krishna Pillutla, Yejin Choi, and Zaid Harchaoui. Influence diagnostics under self-concordance. In Francisco Ruiz, Jennifer Dy, and Jan-Willem van de Meent, editors, _Proceedings of The 26th International Conference on Artificial Intelligence and Statistics_, volume 206 of _Proceedings of Machine Learning Research_, pages 10028-10076. PMLR, 2023.
* [GKX19] Behrooz Ghorbani, Shankar Krishnan, and Ying Xiao. An investigation into neural net optimization via hessian eigenvalue density. In _ICML_, 2019.
* [GRH\({}^{+}\)21] Han Guo, Nazneen Rajani, Peter Hase, Mohit Bansal, and Caiming Xiong. FastIF: Scalable influence functions for efficient model interpretation and debugging. In _Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing_, pages 10333-10350, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics.
* [Gro19] T. H. Gronwall. Note on the derivatives with respect to a parameter of the solutions of a system of differential equations. _Annals of Mathematics_, 20(4):292-296, 1919.
* [GWP\({}^{+}\)23] Kelvin Guu, Albert Webson, Ellie Pavlick, Lucas Dixon, Ian Tenney, and Tolga Bolukbasi. Simfluence: Modeling the influence of individual training examples by simulating training runs, 2023.

* [HYH120] Kazuaki Hanawa, Sho Yokoi, Satoshi Hara, and Kentaro Inui. Evaluation of similarity-based explanations. In _ICLR-21_, 2020.
* [KATL19] Pang Wei Koh, Kai-Siang Ang, Hubert H. K. Teo, and Percy Liang. On the accuracy of influence functions for measuring group effects. In _Proceedings of the 33rd International Conference on Neural Information Processing Systems_, Red Hook, NY, USA, 2019. Curran Associates Inc.
* [KL17] Pang Wei Koh and Percy Liang. Understanding black-box predictions via influence functions. In Doina Precup and Yee Whye Teh, editors, _Proceedings of the 34th International Conference on Machine Learning_, volume 70 of _Proceedings of Machine Learning Research_, pages 1885-1894. PMLR, 06-11 Aug 2017.
* [KS21] Karthikeyan K and Anders Sogaard. Revisiting methods for finding influential examples, 2021.
* [MBAB22] Kevin Meng, David Bau, Alex J Andonian, and Yonatan Belinkov. Locating and editing factual associations in GPT. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, _Advances in Neural Information Processing Systems_, 2022.
* [PLKS20] Garima Pruthi, Frederick Liu, Satyen Kale, and Mukund Sundararajan. Estimating training data influence by tracing gradient descent. In Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, _Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual_, 2020.
* [SZTS22] Andrea Schioppa, Polina Zablotskaia, David Vilar Torres, and Artem Sokolov. Scaling up influence functions. In _AAAI-22_, 2022.
* [WTD17] Ellery Wulczyn, Nithum Thain, and Lucas Dixon. Ex machina: Personal attacks seen at scale. In _Proceedings of the 26th International Conference on World Wide Web_, WWW '17, pages 1391-1399, Republic and Canton of Geneva, CHE, 2017. International World Wide Web Conferences Steering Committee.

Clarifications regarding the operator \(\nabla\)

We spell out in further detail the way we employ the \(\nabla\) operator. For example, we say that \(\nabla_{\varepsilon|0}\theta_{\varepsilon}\) is the \(Q\times N\)-dimensional Jacobian at \(\varepsilon=0\): concretely, \(\nabla_{\varepsilon|\varepsilon_{0}}f\) means computing the Jacobian of \(f\) wrt. \(\varepsilon\) and evaluating it at the point \(\varepsilon_{0}\); this Jacobian is a matrix whose \((i,j)\)-entry is given by:

\[(\nabla_{\varepsilon|\varepsilon_{0}}f)_{i,j}=\frac{\partial f_{j}}{\partial \varepsilon_{i}}(\varepsilon_{0}).\] (9)

This is easily extended to higher order derivatives denote by \(\nabla^{k}\); specifically we use \(\nabla^{2}_{(\varepsilon,\theta)|(0,\theta_{0})}\mathcal{L}\) to denote the matrix whose \(i,j\)-entry is given by:

\[(\nabla^{2}_{(\varepsilon,\theta)|(0,\theta_{0})}\mathcal{L})_{i,j}=\frac{ \partial^{2}\mathcal{L}}{\partial\varepsilon_{i}\partial\theta_{j}}(0, \theta_{0});\] (10)

as the matrix \(H^{-1}_{\theta_{0}}\) acts on the space of the parameters \(\theta\), the notation \(H^{-1}_{\theta_{0}}\nabla^{2}_{(\varepsilon,\theta)|(0,\theta_{0})}\mathcal{L}\) will denote a contraction on \(j\):

\[(H^{-1}_{\theta_{0}}\nabla^{2}_{(\varepsilon,\theta)|(0,\theta_{0})}\mathcal{ L})_{i,k}=\sum_{j}(H^{-1}_{\theta_{0}})_{k,j}\frac{\partial^{2}\mathcal{L}}{ \partial\varepsilon_{i}\partial\theta_{j}}(0,\theta_{0}).\] (11)

## Appendix B Proof of Theorem 1

For convenience, we first recall the statement of Theorem 1.

**Theorem**.: _Assume that \(\nabla\mathcal{L}\) is \(C^{k}\) (\(k\geq 1\)) and let \(\theta_{0}\in\textbf{R}^{N}\); assume that the Hessian \(H_{\theta_{0}}=\nabla^{2}_{\theta|\theta_{0}}L\) is non-singular; then there exist neighborhoods \(U\) of \(\theta_{0}\) and \(V\) of \(0\in\textbf{R}^{Q}\), and a \(C^{k}\)-function \(\Theta:V\to U\) such that \(\Theta(0)=\theta_{0}\) and \(\Theta(\varepsilon)\in U\) is the unique solution in \(U\) of the equation_

\[\nabla_{\theta}\mathcal{L}(\Theta(\varepsilon),\varepsilon)=\nabla_{\theta} \mathcal{L}(\theta_{0},0).\] (12)

_Moreover, the gradient of \(\Theta\) at the origin is given by:_

\[\nabla_{\varepsilon|0}\Theta=-H^{-1}_{\theta_{0}}\nabla^{2}_{(\varepsilon, \theta)|(0,\theta_{0})}\mathcal{L}.\] (13)

Proof.: Equation (12) gives us \(N\) constraints on the \(N+Q\) variables \((\theta,\varepsilon)\) and we want to solve them for the first \(N\) variables \(\theta\); to obtain the function \(\Theta\) we invoke the Implicit Function Theorem, using that the Jacobian wrt. the variables we want to solve for is \(H_{\theta_{0}}\) and is therefore non-singular. Finally (13) is obtained by taking the gradient of (12) wrt. \(\varepsilon\) and setting \(\varepsilon=0\):

\[\nabla^{2}_{\theta|\theta_{0}}L\cdot\nabla_{\varepsilon|0}\Theta+\nabla^{2}_{ (\varepsilon,\theta)|(0,\theta_{0})}\mathcal{L}=0.\]

## Appendix C Proof of Corollary 1

For convenience, we first recall the statement of Corollary 1.

**Corollary**.: _Under the assumptions of Theorem 1:_

1. _If_ \(\theta_{0}\) _is a stationary point of_ \(L\)_, then each_ \(\Theta(\varepsilon)\) _is a stationary point of the loss_ \(\theta\mapsto\mathcal{L}(\theta,\varepsilon)\)_._
2. _If_ \(\theta_{0}\) _is a local (strict) minimum of_ \(L\)_, for_ \(\varepsilon\) _sufficiently small,_ \(\Theta(\varepsilon)\) _is a local (strict) minimum of the loss_ \(\theta\mapsto\mathcal{L}(\theta,\varepsilon)\)_._
3. _Let_ \(Q=1\) _(hence epsilon is a scalar) with_ \(\mathcal{L}(\theta,\varepsilon)=L(\theta)+\varepsilon l_{x}(\theta)\)_, where_ \(l_{x}\) _is the loss corresponding to a specific training point_ \(x\)_. We then obtain the classical result_ _[_58_]_ _about the influence of down/up-sampling_ \(x\) _on the training parameters:_ \[\frac{d\Theta}{d\varepsilon}\Big{|}_{\varepsilon=0}=-(\nabla^{2}_{\theta}L( \theta_{0}))^{-1}\nabla_{\theta}l_{x}(\theta_{0}).\] (14)Proof.: If \(\theta_{0}\) is a stationary point of \(L\), then \(\nabla_{\theta}\mathcal{L}(\theta_{0},0)=0\) in (3). This implies that \(\nabla_{\theta}\mathcal{L}(\Theta(\varepsilon),\varepsilon)=0\) in (3), which is exactly the statement that \(\Theta(\varepsilon)\) is a stationary point of \(\mathcal{L}\). If \(\theta_{0}\) is a local (strict) minimum of \(L\), it is not just a stationary point, but the Hessian at \(\theta_{0}\) is also positive definite. By continuity, for sufficiently small \(\varepsilon\), also the Hessian \(\nabla^{2}_{\theta|\Theta(\varepsilon)}\mathcal{L}\) will be positive definite so that \(\Theta(\varepsilon)\) will be a local (strict) minimum. Finally, (14) follows from applying (3) to the variation \(\mathcal{L}(\theta,\varepsilon)=L(\theta)+\varepsilon l_{x}(\theta)\). 

## Appendix D Proof of Theorem 2

For convenience, we first recall the statement of Theorem 2.

**Theorem**.: _Assume that the model is trained for \(T\) time-steps with stochastic gradient descent. Denoting by \(H_{t}\) the Hessian \(\nabla^{2}_{\theta|\theta_{0,t}}L_{t}\), then the final parameters \(\theta_{\varepsilon,T}\) satisfy:_

\[\nabla_{\varepsilon|0}\theta_{\varepsilon,T}=-\sum_{t=0}^{T-1} \eta_{t}\nabla^{2}_{(\varepsilon,\theta)|(0,\theta_{0,t})}\mathcal{L}_{t}- \sum_{t=0}^{T-1}\eta_{t}H_{t}\nabla_{\varepsilon|0}\theta_{\varepsilon,t}.\] (15)

Proof.: The parameters \(\theta_{\varepsilon,t}\) obey a recurrence relation:

\[\theta_{\varepsilon,t}-\theta_{\varepsilon,t-1}=-\eta_{t-1}\nabla_{\theta} \mathcal{L}_{t-1}(\theta_{\varepsilon,t-1},\varepsilon),\] (16)

which can be solved to give

\[\theta_{\varepsilon,T}=-\sum_{t=0}^{T-1}\eta_{t}\nabla_{\theta} \mathcal{L}_{t}(\theta_{\varepsilon,t},\varepsilon);\] (17)

then (15) follows immediately by applying \(\nabla_{\varepsilon|0}\), i.e. computing the Jacobian wrt. \(\varepsilon\) at the origin. 

## Appendix E Proof of Theorem 3

The first step in the proof of Theorem 3 is a discrete version of Gronwall's Lemma [1].

**Lemma 1**.: _Let \(\{u_{t}\}\), \(\{\alpha_{t}\}\) and \(\{\beta_{t}\}\) be sequences such that \(\beta_{t}\geq 0\) and_

\[u_{t}\leq\alpha_{t}+\sum_{s=0}^{t-1}\beta_{s}u_{s}.\] (18)

_Note that we assume that (18) holds for \(t\geq 1\) and we consider \(u_{0}\) an initial condition. Then:_

\[u_{T}\leq\alpha_{T}+\beta_{0}\prod_{s=1}^{T-1}(1+\beta_{s})u_{0}+ \sum_{s=1}^{T-1}\alpha_{s}\beta_{s}\prod_{k=s+1}^{T-1}(1+\beta_{k}).\] (19)

_If \(\{\alpha_{t}\}\) is non-decreasing in \(t\) we then get:_

\[u_{T}\leq\beta_{0}\prod_{s=1}^{T-1}(1+\beta_{s})u_{0}+\alpha_{T}(1+\sum_{s=1}^ {T-1}\beta_{s}\prod_{k=s+1}^{T-1}(1+\beta_{k})).\] (20)

_Moreover, defining \(u_{t}\) by setting (18) to be an equality, shows that (19) is sharp._

Proof.: We first observe that if we set \(v_{0}=u_{0}\) and build \(v_{t}\) by declaring (18) to be an equality, then \(v_{t}\geq u_{t}\) for any \(t\). This is true by induction as:

\[u_{t+1}-\alpha_{t+1}\leq\sum_{s=0}^{t}\beta_{s}u_{s}\leq\sum_{s=0}^{t}\beta_{ s}v_{s}=v_{t+1}-\alpha_{t+1}.\] (21)

We thus focus on bounding \(v_{t+1}\); we note that

\[(v_{t+1}-\alpha_{t+1})-(v_{t}-\alpha_{t})=\beta_{t}(v_{t}-\alpha_{t})+\beta_{ t}\alpha_{t};\] (22)thus

\[v_{t+1}-\alpha_{t+1} =(1+\beta_{t})(v_{t}-\alpha_{t})+\beta_{t}\alpha_{t}\] (23) \[=(1+\beta_{t})(1+\beta_{t-1})(v_{t-1}-\alpha_{t-1})+(1+\beta_{t}) \beta_{t-1}\alpha_{t-1}+\beta_{t}\alpha_{t};\]

then (19) follows by induction and letting \(t+1=T\). Finally, if \(\{\alpha_{t}\}\) is non-decreasing in \(t\) we can simply replace \(\alpha_{s}\) with \(\alpha_{T}\) obtaining (20). The sharpness follows by considering the sequence \(\{v_{t}\}\) we defined in the proof. 

For convenience, we first recall the statement of Theorem 3.

**Theorem**.: _In the setting of Theorem 2 assume that, for \(t\leq T\), \(\theta_{\varepsilon,t}\) lies in a bounded region \(R\) such that_

\[\sup_{\varepsilon,\theta\in R}\|\nabla\mathcal{L}_{t}(\theta,\varepsilon)- \nabla\mathcal{L}(\theta,0)\|\leq C\varepsilon\] (24)

_and that for \(\theta\in R\) each loss \(\mathcal{L}_{t}(\theta,\varepsilon)\) and its gradient wrt. \(\theta\) are \(A\)-Lipschitz in \(\theta\). Then_

\[\|\theta_{\varepsilon,T}-\theta_{0,T}\|\leq C\varepsilon\sum_{s<T}\eta_{s}(1 +\exp(2A\sum_{s<T}\eta_{s})).\] (25)

Proof.: The idea is to apply Lemma 1 as we would in the case of a standard ODE. Let us compare the evolution of \(\theta_{\varepsilon,t}\) and \(\theta_{0,t}\):

\[\theta_{\varepsilon,t} =\theta_{\mathrm{init}}-\sum_{s=0}^{t-1}\eta_{s}\nabla_{\theta} \mathcal{L}(\theta_{\varepsilon,s},\varepsilon)\] (26) \[\theta_{0,t} =\theta_{\mathrm{init}}-\sum_{s=0}^{t-1}\eta_{s}\nabla_{\theta} \mathcal{L}(\theta_{0,s},0);\] (27)

which leads to

\[\|\theta_{\varepsilon,t}-\theta_{0,t}\| \leq\sum_{s=0}^{t-1}\eta_{s}\|\nabla_{\theta}\mathcal{L}(\theta_{ \varepsilon,s},\varepsilon)-\nabla_{\theta}\mathcal{L}(\theta_{0,s},0)\|\] (28) \[\leq C\varepsilon\sum_{s=0}^{t-1}\eta_{s}+\sum_{s=0}^{t-1}\eta_{s }A\|\theta_{\varepsilon,s}-\theta_{0,s}\|;\]

we now just need to rephrase this inequality in terms of Lemma 1:

\[\underbrace{\|\theta_{\varepsilon,t}-\theta_{0,t}\|}_{u_{t}}\leq\underbrace {C\varepsilon\sum_{s=0}^{t-1}\eta_{s}+\sum_{s=0}^{t-1}\underbrace{\eta_{s}A} _{\beta_{s}}\underbrace{\|\theta_{\varepsilon,s}-\theta_{0,s}\|}_{u_{s}}}_{u_ {s}};\] (29)

we note that \(u_{0}=0\) as both dynamics start at \(\theta_{\mathrm{init}}\) and that \(\alpha_{t}\) is non-decreasing in \(t\). We then have that

\[u_{T} \leq\alpha_{T}(1+\sum_{s=1}^{T-1}\beta_{s}\prod_{k=s+1}^{T-1}(1+ \beta_{k}))\] (30) \[\leq\alpha_{T}(1+\sum_{s=1}^{T-1}\beta_{s}\prod_{k=s+1}^{T-1}\exp (\beta_{k}))\] \[\leq\alpha_{T}(1+\exp(\sum_{s=1}^{T-1}\beta_{k})\sum_{s=1}^{T-1} \beta_{s})\] \[\leq\alpha_{T}(1+\exp(\sum_{s=1}^{T-1}\beta_{k})\times\exp(\sum_{ s=1}^{T-1}\beta_{k}))\] \[\leq\alpha_{T}(1+\exp(\sum_{s=1}^{T-1}2\beta_{k})),\]

which is (8).

### Sketching modifications needed in the case of optimizers

The proofs of Theorems 2 and 3 were given for SGD. The argument in the case of using an optimizer would be more involved. Here we sketch the modifications needed when dealing with optimizers. An optimizer is characterized by an _optimizer state_, \(\sigma_{\varepsilon,t}\), which will also evolve in time. While for SGD we just considered the update rule for \(\theta_{\varepsilon,t}\), in the case of optimizers one needs to study a joint system of update rules for the parameters and the optimizer state:

\[\theta_{\varepsilon,t}-\theta_{\varepsilon,t-1} =-\eta_{t-1}F(\nabla_{\theta}\mathcal{L}_{t-1}(\theta_{ \varepsilon,t-1},\varepsilon),\sigma_{\varepsilon,t})\] (31) \[\sigma_{\varepsilon,t}-\sigma_{\varepsilon,t-1} =-\rho_{t-1}G(\nabla_{\theta}\mathcal{L}_{t-1}(\theta_{ \varepsilon,t-1},\varepsilon),\sigma_{\varepsilon,t-1}).\] (32)

In the case of Theorem 2 one would then apply \(\nabla_{\varepsilon|0}\) to the joint system to obtain the update rule. For Theorem 3 one should add additional continuity in \(\varepsilon\) and Lipschitz conditions on \(F\) and \(G\); one should then check that these are indeed satisfied for common optimizers like Adam or Adafactor.

## Appendix F Why do we use the Conjugate Residual method?

Regarding HIF, note that usually the Conjugate Gradient method is used [17] for computing inverse Hessian vector products. However, in our experiments the Conjugate Gradient method always yielded a time-series of Pearson Correlations \(R(t)\) oscillating around \(0\), _thus without any predictive power_. The reason is that this method assumes the Hessian to be positive-definite, which is not the case for most Neural Networks. A simple fix to the problem is to use the Conjugate Residual method which does not require the Hessian to be positive definite. We recommend to use the Conjugate Residual when applying HIF in Deep Learning; _this might look like a minor technical point, but it can avoid reporting that HIF has no predictive power, when instead the issue lies in the numerical method used to compute inverse Hessian vector products_.

For more discussion on the Conjugate Residual method see its Wikipedia article.

## Appendix G Further empirical results on Fading of Influence scores

### Fading of Influence for BERT

In the setting of Figure 2(a) we plot the distance from the confidence interval of the Pearson's r to \(0\) (Figure 4); once the distance becomes \(0\), the predictive power of influence scores has become null.

In Figure 5 we zoom in Figure 2 (a: BERT). The fading phenomenon was non affected by checkpoint selection: in Figure 6 we consider a later checkpoint and we see the same qualitative behavior. Moreover, in Figure 6 we also consider TracIn with 3 checkpoints selected using the advice in [17] - we do not see improvements and the peak is even slightly lower than for TracIn using one checkpoint. Thus, in our further experiments we have used TracIn with a single checkpoint.

In Figure 7 we zoomed in Figure 2 (b: ResNet). In this case the predictive power was never particularly high, e.g. for TracIn it quickly peaked at \(0.3\) but it takes more steps than in the case of BERT to reach \(0\). We conjecture that this is in part due to the slower divergence of parameters in

Figure 4: For BERT the predictive power of influence scores becomes 0 over time as the distance from 0 to the confidence interval of the Pearson’s r becomes 0, i.e. the confidence interval contains 0.

Figure 5: For BERT the predictive power of influence scores on the loss shifts degrades over time very quickly.

Figure 6: For BERT the predictive power of influence scores on the loss shifts degrades over time very quickly. Using multiple checkpoints did not improve performance of TracIn.

Figure 8: For BERT the predictive power of influence scores faded more slowly when using SGD. The “Gradient” method is TracIn.

Figure 7: For ResNet the predictive power of influence scores faded more slowly and was never particularly high.

ResNet (compare the x-axis of (a) and (b) in Figure 1) and in part due to the use of SGD. In particular, we verified that a slower fading also takes place when fine-tuning BERT with SGD (Figure 8).

### Fading of Influence for T5

We consider another NLP pre- trained model fine-tuned on SST2, T5. In Figure 9 we illustrate the same effect of fading of influence for the T5 model.

### Fading of Influence for the ViT

In the case of ResNet, in our fading of influence experiments (Fig. 2 (b)) the peak is particularly low. On the same task, we trained a ViT with SGD and observe that the peak is higher, similarly to what happens with BERT, see Figure 10. We conjecture that this difference is due to the different model architecture. Note that in both cases, we observe the fading of influence as predicted by the theory.

## Appendix H How many steps are needed to correct mis-predictions?

In Figure 11 we plot the average and median number of steps to correct a mis-prediction as a function of \(\varepsilon\): for any fixed \(\varepsilon\), the more effective a correction method is, the fewer steps are needed to correct the prediction. For BERT we see that both Proponent-correction and Opponent-tuning result in a large decrease in the number of steps to take, compared with the Baseline. For ResNet, we do not plot Opponent-tuning as it performed poorly on success-rate. For ResNet the gains of Proponent-correction are smaller but consistent as \(\varepsilon\) varies.

Figure 10: The predictive power of influence scores on the loss shifts degrades over time for the Vision Transformer (ViT). (a) Full range of re-training steps, (b) zoom-in in the peak and decay phase.

Figure 9: The predictive power of influence scores on the loss shifts degrades over time for T5. (a) Correlation with the ground truth, with the shaded area the corresponding \(95\%\) confidence region. (b) Distance from 0 to the confidence interval, which measures when the confidence interval covers 0.

## Appendix A

\begin{table}
\begin{tabular}{c l|l} \hline \hline  & \multicolumn{1}{c}{Test example} & \multicolumn{1}{c}{Top proponent} \\ \hline \multicolumn{3}{c}{False positives} \\ \hline
1 & Agreed. This is dumb. & Yeah i no it sucks. \\
2 & == Iowa-class battleships == Stupid me... I & By the way, sorry about that stupid edit. \\  & didn’t even look... sorry about that... ( & \\
3 & It’s so silly of me to ask this question but I’m still not getting what you’re trying to say. & By the way, sorry about that stupid edit. \\  & Sorry for being such an idiot! ;) & \\
4 & hahaha this whale is huge and ugly and funky & A cheeseburger for you! Fatty \\  & doodle...lol... & \\
5 & Cool you can change the content! love sex!! & Sex in space Sweet. Thanks for the education! \\
6 & ====: A lot of poeple got killed that day & Yes they were Hitler Stated that Slavs were lesser races and should be salves or killed \\
7 & == A question regarding Rodriguez’s sexual orientation == Would the statement, ”I’ve gone both ways. I do as I please. I am too & Ravenkills97 I didn’t change the genres on In This Moment, all i did was put commas in between the genres. U gave me a final warning for no reason. All because I put commas in between something, like it fucking erases the genre. \\ \hline \multicolumn{3}{c}{False negatives} \\ \hline
1 & your mama is a man & grow up Get a life \\
2 & (...) It’s nice to see you finally admit it with your edit summary of, ”what a terrible idiot I am”. (...) & (...) you’re a nerd living with your mom and you have no friends. You see? (...) \\
3 & U r worse than obamas presidency & Are you known as a sock puppet? \\
4 & I don’t want to say sorry,so get a life before i & Then grow up and stop spamming on my page. \\  & ruin yours buthead & I hear your mommy calling. Run along. \\
5 & (...) it’s hard for me to not take the kind of & (...) thats 2 against 1 buddy you lose \\  & stupidity evident from your edits personally & (...) \\
6 & Your spelling seems to be at about the 3rd grade level, I hope you’re not having sex. & Okay then. I’ll just have to agree to disagree. \\ \hline \hline \end{tabular}
\end{table}
Table 1: Pairs of misclassified test examples with their mislabeled training proponents

Figure 11: Average and median steps to correct mis-predictions. (a) BERT, (b) ResNet

## Appendix I Selecting mislabeled proponents for correction

For the experiments in Section 6 we use comparatively clean datasets, so the proponents we retrieve and correct are not mislabeled. In this section we use the notoriously noisy Wikipedia Toxicity Subtypes dataset [22] where for many misclassified test examples we can find mislabeled proponents used during training. Table 1 presents false positives and negatives (that is, test examples incorrectly predicted to be toxic, resp. non-toxic) with a high-scoring proponent which we consider mislabeled. It is worth noting that some examples lack context necessary to confidently verify whether they are toxic or not. In total, we collected 14 such pairs, seven for mispredictions of each kind.3

Footnote 3: We omitted the most upsetting example from the false negatives.

Table 2 presents the mean and the median number of steps needed to be taken on the corrected proponents to achieve a change in prediction, as well as retention once there is a change. As a point of comparison, we permute the proponents withing the false positive and false negative sets: this is a more challenging baseline than selecting examples for correction randomly. Note that the influence scores for the presented examples range from \(-0.90\) to \(0.92\), so some of the permuted proponents still have some quite influence on the test examples they get assigned to. Still, the results confirm that correcting mislabeled top-scoring proponents is the fastest way of achieving a change in model predictions, with as few as three steps required on average and with the smallest damage to predictions on other examples (measured with retention).

## Appendix J Hyper-parameters

### Training

ResNet was trained on a single V100 with the hyper-parameters in Table 3; training data was augmented using torchvision using the transformations transforms.RandomCrop(32, padding=4), and transforms.RandomHorizontalFlip().

\begin{table}
\begin{tabular}{l l} \hline \hline Hyper-parameter & value \\ Batch-size & 128 \\ Steps & 35000 \\ Learning rate & \(10^{-5}\) \\ Learning rate scheduler & None \\ Optimizer & Adam \\ \(l_{2}\)-regularization & \(5\times 10^{-6}\) \\ \hline \hline \end{tabular}
\end{table}
Table 4: Training hyper-parameters for BERT on SST2

\begin{table}
\begin{tabular}{l l|c c c} \hline \hline proponents & mean influence & mean step & median step & retention \\ True & 0.91 (\(\pm 0.03\)) & 3.0 & 2.5 & 98.1\% \\ Permuted & 0.50 (\(\pm 0.21\)) & 5.1 & 3.5 & 96.3\% \\ \hline \hline \end{tabular}
\end{table}
Table 2: Mean influence with variance and metrics for the experiments with the example in Table 1

\begin{table}
\begin{tabular}{l l} \hline \hline Hyper-parameter & value \\ Batch-size & 128 \\ Epochs & 200 \\ Learning rate & \(10^{-1}\) \\ Learning rate scheduler & cosine decay \\ Optimizer & SGD \\ \(l_{2}\)-regularization & \(10^{-4}\) \\ \hline \hline \end{tabular}
\end{table}
Table 3: Training hyper-parameters for ResNet on CIFAR10The ViT was trained on a single V100 with the hyper-parameters in Table 6; we used the same data augmentation as in the case of ResNet.

BERT was trained on 8 TPUv3 cores using the hyper-parameters in Table 4. The best checkpoint was selected on validation-set accuracy evaluated every 500 steps; it corresponded to 6000 steps of training.

T5 was trained on a GPU V100 using the hyper-parameters in Table 5. We selected a checkpoint based on the validation set accuracy at epoch 4, where the accuracy was \(92.9\%\). Note that T5 is a generative model, so at prediction time the sequence to classify is encoded with the encoder and then the decoder is used to generate a sequence that represents the desired class label. When computing influence functions, one uses the cross-entropy loss for sequences.

### Correction Experiments

For the correction experiments we used ABIF [20] because of their computational efficiency as IF scores need to be computed against the whole training set; we used 32 projectors obtained with 64 Amoldi iterations. The Arnoldi iteration can take up to 2 hours; scoring the data takes a few minutes. We used a learning rate of \(10^{-3}\) and for BERT we loaded the state of the Adam optimizer from the selected checkpoint.

For BERT we selected the checkpoint at 6000 steps which was the best on validation performance. For ResNet we selected the checkpoint after 10 epochs at which about 150 test points are incorrectly classified.

## Appendix K Illustrating TracIn's modeling error

In Theorem 2 we have derived the correct formula for the Jacobian \(\nabla_{\varepsilon|0}\theta_{\varepsilon,T}\) and we expect TracIn (Eq. 2) to introduce a modeling error in calculating \(\nabla_{\varepsilon|0}\theta_{\varepsilon,T}\). We can compute the difference between the ground truth \(\theta_{\varepsilon,T}\) and the first order expansion \(\theta_{0,T}+\varepsilon^{T}\nabla_{\varepsilon|0}\theta_{\varepsilon,T}\) when the Jacobian is computed with TracIn (Eq. 2) or the corrected formula (Eq. 6). We can then compute the relative excess error introduced by TracIn for different values of \(\varepsilon\). In Figure 12 we observe that except for the extremely small perturbation \(\varepsilon=10^{-4}\), TracIn does introduce an additional relative error in modeling the change of parameters when retraining BERT with SGD. The setup of this experiment is the same as those of our parameter divergence experiments.

\begin{table}
\begin{tabular}{l l} \hline \hline Hyper-parameter & value \\ Batch-size & 128 \\ Epochs & 300 \\ Peak learning rate & \(10^{-3}\) \\ Learning rate scheduler & 1 epoch warm-up and then cosine decay \\ Optimizer & SGD \\ \hline \hline \end{tabular}
\end{table}
Table 6: Training hyper-parameters for ViT on CIFAR10

\begin{table}
\begin{tabular}{l l} \hline \hline Hyper-parameter & value \\ Batch-size & 16 \\ Maximal sequence Length & 128 \\ Epochs & 4 \\ Learning rate & \(10^{-4}\) \\ Learning rate scheduler & None \\ Optimizer & Adam \\ \hline \hline \end{tabular}
\end{table}
Table 5: Training hyper-parameters for T5 on SST2

## Appendix L Broader Impact Statement

We believe that our work can benefit model developers who want to debug and correct mis-predictions made by models. Our suggested error-correction procedure is much less compute intensive than previous ones using IF as it does not require model retraining. However, this debugging procedure could introduce new errors in the systems, for example if on a specific problem IF are not effective at predicting loss-shifts or if examples retrieved by IF lead to over-fitting against spurious artifacts present in such examples. Since this could have a negative impact on the users of such systems, mitigation strategies should be put in place regarding the trade-offs between the success rate of fixing specific mis-predictions and the overall retention of the system performance.

Figure 12: Illustrating the additional modeling error introduced by TracIn for different values of the perturbation \(\epsilon\). The error is the ratio between the error of TracIn (Eq. 2) and the error of the exact formula (Eq. 6) when retraining BERT on a perturbed loss with SGD.