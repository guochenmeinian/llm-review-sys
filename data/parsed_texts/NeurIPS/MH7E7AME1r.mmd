# Debiasing Conditional Stochastic Optimization

Lie He

EPFL

lie.he@epfl.ch

Shiva Prasad Kasiviswanathan

Amazon

kasivisw@gmail.com

Work initiated during internship at Amazon.

###### Abstract

In this paper, we study the conditional stochastic optimization (CSO) problem which covers a variety of applications including portfolio selection, reinforcement learning, robust learning, causal inference, etc. The sample-averaged gradient of the CSO objective is biased due to its nested structure, and therefore requires a high sample complexity for convergence. We introduce a general _stochastic extrapolation_ technique that effectively reduces the bias. We show that for non-convex smooth objectives, combining this extrapolation with variance reduction techniques can achieve a significantly better sample complexity than the existing bounds. Additionally, we develop new algorithms for the finite-sum variant of the CSO problem that also significantly improve upon existing results. Finally, we believe that our debiasing technique has the potential to be a useful tool for addressing similar challenges in other stochastic optimization problems.

## 1 Introduction

In this paper, we investigate the _conditional stochastic optimization_ (CSO) problem as presented by Hu et al. [16], which is formulated as follows:

\[\min_{\bm{x}\in\mathbb{R}^{d}}F(\bm{x})=\mathbb{E}_{\xi}[f_{\xi}(\mathbb{E}_{ \eta|\xi}[g_{\eta}(\bm{x};\xi)])],\] (CSO)

where \(\xi\) and \(\eta\) represent two random variables, with \(\eta\) conditioned on \(\xi\). The \(f_{\xi}:\mathbb{R}^{p}\rightarrow\mathbb{R}\) and \(g_{\eta}:\mathbb{R}^{d}\rightarrow\mathbb{R}^{p}\) denote a stochastic function and a mapping respectively. The inner expectation is calculated with respect to the conditional distribution of \(\eta|\xi\). In line with the established CSO framework [16; 15], throughout this paper, we assume access to samples from the distribution \(\mathbb{P}(\xi)\) and the conditional distribution \(\mathbb{P}(\eta|\xi)\).

Many machine learning tasks can be formulated as a CSO problem, such as policy evaluation and control in reinforcement learning [6; 24], and linearly-solvable Markov decision process [5]. Other examples of the CSO problem include instrumental variable regression [23] and invariant learning [16]. Moreover, the widely-used Model-Agnostic Meta-Learning (MAML) framework, which seeks to determine a meta-initialization parameter using metadata for related learning tasks that are trained through gradient-based algorithms, is another example of a CSO problem. In this context, tasks \(\xi\) are drawn randomly, followed by the drawing of samples \(\eta|\xi\) from the specified task [11]. It is noteworthy that the standard stochastic optimization problem \(\min_{\bm{x}}\mathbb{E}_{\xi}[f_{\xi}(\bm{x})]\) represents a degenerate case of the CSO problem, achieved by setting \(g_{\eta}\) as an identity function.

In numerous prevalent CSO problems, such as first-order MAML (FO-MAML) [11], the outer random variable \(\xi\) only takes value in a finite set (say in \(\{1,\dots,n\}\)). These problems can be reformulated to have a finite-sum structure in the outer loop and referred to as _Finite-sum Coupled Compositional Optimization_ (_FCCO_) problem in [33; 19]. In this paper, we also study this problem, formulated as:

\[\min_{\bm{x}\in\mathbb{R}^{d}}F_{n}(\bm{x})=\tfrac{1}{n}\sum_{i=1}^{n}f_{i}( \mathbb{E}_{\eta|i}[g_{\eta}(\bm{x};i)]).\] (FCCO)

The FCCO problem also has broad applications in machine learning for optimizing average precision, listwise ranking losses, neighborhood component analysis, deep survival analysis, deep latent variable models [33; 19].

Although the CSO and FCCO problems are widespread, they present challenges for optimization algorithms. Based on the special composition structure of CSO, using chain rule, under mild conditions, the full gradient of CSO is given by

\[\nabla F(\bm{x})=\mathbb{E}_{\xi}\big{[}\left(\mathbb{E}_{\eta|\xi}[\nabla g_{ \eta}(\bm{x};\xi)]\right)^{\top}\nabla f_{\xi}(\mathbb{E}_{\eta|\xi}[g_{\eta}( \bm{x};\xi)])\big{]}.\]

Constructing an unbiased stochastic estimator for the gradient is generally computationally expensive (and even impossible). A straightforward estimation of \(\nabla F(\bm{x})\) is to estimate \(\mathbb{E}_{\xi}\) with 1 sample of \(\xi\), estimate \(\mathbb{E}_{\eta|\xi}[g_{\eta}(\cdot)]\) with a set \(H_{\xi}\) of \(m\) independent and identically distributed (i.i.d.) samples drawn from the conditional distribution \(\mathbb{P}(\eta|\xi)\), and \(\mathbb{E}_{\eta|\xi}[\nabla g_{\eta}(\cdot)]\) with a different set \(\tilde{H}_{\xi}\) of \(m\) i.i.d. samples drawn from the same conditional distribution, i.e.,

\[\nabla\hat{F}_{m}(\bm{x}):=\big{(}\tfrac{1}{m}\sum_{\tilde{\eta}\in H_{\xi}} \nabla g_{\tilde{\eta}}(\bm{x};\xi)\big{)}^{\top}\nabla f_{\xi}(\tfrac{1}{m} \sum_{\eta\in H_{\xi}}g_{\eta}(\bm{x};\xi)).\] (1)

Note that \(\nabla\hat{F}_{m}(\bm{x})\) consists of two terms. The first term, \((1/m)\sum_{\tilde{\eta}\in\tilde{H}_{\xi}}\nabla g_{\tilde{\eta}}(\bm{x};\xi)\), is an unbiased estimate of \(\mathbb{E}_{\eta|\xi}[\nabla g_{\eta}(\bm{x};\xi)]\). However, the second term is generally biased, i.e.,

\[\mathbb{E}_{\eta|\xi}[\nabla f_{\xi}(\tfrac{1}{m}\sum_{\eta\in H_{\xi}}g_{ \eta}(\bm{x};\xi))]\neq\nabla f_{\xi}(\mathbb{E}_{\eta|\xi}[g_{\eta}(\bm{x}; \xi)]).\]

Consequently, \(\nabla\hat{F}_{m}(\bm{x})\) is a biased estimator of \(\nabla F(\bm{x})\). To reach the \(\varepsilon\)-stationary point of \(F(\bm{x})\) (Definition 1), the bias has to be sufficiently small.

Optimization with biased gradients converges only to a neighborhood of the stationary point. While the bias diminishes with increasing batch size, it also introduces additional sample complexity. For nonconvex objectives, Biased Stochastic Gradient Descent (BSGD) requires a total sample complexity of \(\mathcal{O}(\varepsilon^{-6})\) to reach an \(\varepsilon\)-stationary point [16]. This contrasts with standard stochastic optimization, where sample-averaged gradients are unbiased with a sample complexity of \(\mathcal{O}(\varepsilon^{-4})\)[12, 3]. This discrepancy has spurred a multitude of proposals aimed at reducing the sample complexities of both CSO and FCCO problems. Hu et al. [16] introduced Biased SpiderBoost (BSpiderBoost), which, based on the variance reduction technique SpiderBoost from Wang et al. [38], reduces the variance of \(\xi\) to achieve a sample complexity of \(\mathcal{O}(\varepsilon^{-5})\) for the CSO problem. Hu et al. [17] proposed multi-level Monte Carlo (MLMC) gradient methods V-MLMC and RT-MLMC to further enhance the sample complexity to \(\mathcal{O}(\varepsilon^{-4})\). The SOX [33] and MSVR-V2 [19] algorithms concentrated on the FCCO problem and improved the sample complexity to \(\mathcal{O}(n\varepsilon^{-4})\) and \(\mathcal{O}(n\varepsilon^{-3})\), respectively.

**Our Contributions.** In this paper, we improve the sample complexities for both the CSO and FCCO problems (see Table 1). To facilitate a clear and concise presentation, we will suppress the dependence on specific problem parameters throughout the ensuing discussion.

1. [leftmargin=*]
2. Our main technical tool in this paper is an extrapolation-based scheme that mitigates. bias in gradient estimations. Considering a suitably differentiable function \(q(\cdot)\) and a random variable \(\delta\sim\mathcal{D}\), we show that we can approximate the value of \(q(\mathbb{E}[\delta])\) via extrapolation from a limited number of evaluations of \(q(\delta)\), while maintaining a minimal bias. In the context of CSO and FCCO problems, this scheme is used in gradient estimation, where the function \(q\) corresponds to \(\nabla f_{\xi}\) and the random variable \(\delta\) corresponds to \(g_{\eta}\).
3. For the CSO problem, we present novel algorithms that integrate the above extrapolation-based scheme with BSGD and BSpiderBoost algorithms of Hu et al. [16]. Our algorithms, referred to as E-BSGD and E-BSpiderBoost, achieve a sample complexity of \(\mathcal{O}(\varepsilon^{-4.5})\) and \(\mathcal{O}(\varepsilon^{-3.5})\) respectively, in order to attain an \(\varepsilon\)-stationary point for nonconvex smooth objectives. Notably, the sample complexity of E-BSpiderBoost improves the best-known sample complexity of \(\mathcal{O}(n\varepsilon^{-4})\) for the CSO problem from Hu et al. [17].
4. For the FCCO problem2 we propose a new algorithm that again combines the extrapolation-based scheme with a multi-level variance reduction applied to both inner and outer parts of the problem. Our algorithm, referred to as E-NestedVR, achieves a sample complexity of \(\mathcal{O}(n\varepsilon^{-3})\) if \(n\leq\varepsilon^{-2/3}\) and \(\mathcal{O}(\max\{\sqrt{n}\varepsilon^{-2.5},\varepsilon^{-4}/\sqrt{n}\})\) if \(n>\varepsilon^{-2/3}\) for nonconvex smooth objectives and second-order extrapolation scheme. Our bound is never worse than the \(\mathcal{O}(n\varepsilon^{-3})\) bound of MSVR-V2 algorithm of Jiang et al. [19] and is in fact better if \(n=\Omega(\varepsilon^{-2/3})\). As an illustration, when \(n=\Theta(\varepsilon^{-1.5})\), our bound of \(\mathcal{O}(\varepsilon^{-3.25})\) is significantly better than the MSVR-V2 bound of \(\mathcal{O}(\varepsilon^{-4.5})\).

In terms of proof techniques, our approach diverges from conventional analyses for the CSO and FCCO problems in that we focus on explicitly bounding the bias and variance terms of the gradient estimator to establish the convergence guarantee. Compared to previous results, our improvements do require an additional mild regularity assumption on \(f_{\xi}\) and \(g_{\eta}\) mainly that \(\nabla f_{\xi}\) is \(4\)th order differentiable. Firstly, as we discuss in Remark 2 most common instantiations of CSO/FCCO framework such as: 1) invariant logistic regression [16], 2) instrumental variable regression [23], 3) first-order MAML for sine-wave few shot regression [11] and other problems, 4) deep average precision maximization [26; 34], tend to satisfy this assumption. Secondly, we highlight that the bounds derived from previous studies do not improve when incorporating this additional regularity assumption. Thirdly, \(\Omega(\varepsilon^{-3})\) remains the lower bound for stochastic optimization even under the arbitrary smoothness constraint [2], demonstrating that our improvement is non-trivial. Our results show that, this regularity assumption, which seems to practically valid, can be exploited through a novel extrapolation-based bias reduction technique to provide substantial improvements in sample complexity.3

Footnote 3: Higher-order smoothness conditions have also been exploited in standard stochastic optimization for performance gains [4].

We defer some additional related work to Appendix B and conclude with some preliminaries.

**Notation.** Vectors are denoted by boldface letters. For a vector \(\bm{x}\), \(\|\bm{x}\|_{2}\) denotes its \(\ell_{2}\)-norm. A function with \(k\) continuous derivatives is called a \(\mathcal{C}^{k}\) function. We use \(a\lesssim b\) to denote that \(a\leq Cb\) for some constant \(C>0\). We consider expectation over various randomness: \(\mathbb{E}_{\xi}[\cdot]\) denotes expectation over the random variable \(\xi\), \(\mathbb{E}_{\eta|\xi}[\cdot]\) denotes expectation over the conditional distribution of \(\eta|\xi\). Unless otherwise specified, for a random variable \(X\), \(\mathbb{E}[X]\) denotes expectation over the randomness in \(X\). We focus on nonconvex objectives in this paper and use the following standard convergence criterion for nonconvex optimization [18].

**Definition 1** (\(\varepsilon\)-stationary point): _For a differentiable function \(F(\cdot)\), we say that \(\bm{x}\) is a first-order \(\varepsilon\)-stationary point if \(\|\nabla F(\bm{x})\|^{2}\leq\varepsilon^{2}\)._

For notational convenience, in the rest of this paper, we omit the dependence on \(\xi\) (or \(i\) in the FCCO context) in the function \(g\) and use \(g_{\eta}(\bm{x})\) to represent \(g_{\eta}(\bm{x};\xi)\).

## 2 Stochastic Extrapolation as a Tool for Bias Correction

In this section, we present an approach for tackling the bias problem as appears in optimization procedures such as BSGD, BSpiderBoost, etc. Importantly, our approach addresses a general problem appearing in optimization settings and could be of independent interest. All missing details from this section are presented in Appendix C.

For ease of presentation, we start by considering the 1-dimensional case and assume a function \(q:\mathbb{R}\to\mathbb{R}\), a constant \(s\in\mathbb{R}\). Let \(\delta\) be a random variable drawn from an arbitrary distribution \(\mathcal{D}\) over \(\mathbb{R}\). In Sections 3 and 4, we apply these ideas to the CSO and FCCO problems where the random variable \(\delta\) is played by \(g_{\eta}(\cdot)\) and function \(q\) is played by \(\nabla f_{\xi}\). Informally stated, our goal in this section will be to

\[\boxed{\text{Efficiently approximate }q(s+\mathbb{E}[\delta])\text{ with few evaluations of }\{q(s+\delta)\}_{\delta\sim\mathcal{D}}.}\]

\begin{table}
\begin{tabular}{l c c c c} \hline \hline Problem & \multicolumn{2}{c}{Old Bounds} & \multicolumn{2}{c}{Our Bounds} \\ \cline{2-5}  & Algorithm & Bound & Algorithm & Bound \\ \hline CSO & BSGD [16] & \(\mathcal{O}(\varepsilon^{-6})\) & E-BSGD & \(\mathcal{O}(\varepsilon^{-4.5})\) \\ CSO & BSpiderBoost [16] & \(\mathcal{O}(\varepsilon^{-5})\) & E-BSpiderBoost & \(\mathcal{O}(\varepsilon^{-3.5})\) \\ CSO & RT-MLMC [17] & \(\mathcal{O}(\varepsilon^{-4})\) & & \\ FCCO & MSVR-V2 [19] & \(\mathcal{O}(n\varepsilon^{-3})\) & E-NestedVR & \(\begin{cases}\mathcal{O}(n\varepsilon^{-3})&\text{if }n\leq\varepsilon^{-2/3}\\ \mathcal{O}(\max\{\frac{\sqrt{n}}{\varepsilon^{2/3}},\frac{1}{\sqrt{n} \varepsilon^{4}}\}),&\text{if }n>\varepsilon^{-2/3}\end{cases}\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Sample complexities needed to reach \(\varepsilon\)-stationary point for FCCO and CSO problems with nonconvex smooth objectives. Assumptions are comparable, but our results require an additional mild regularity on \(f_{\xi}\) and \(g_{\eta}\). For FCCO also see Footnote 2. Note that \(\Omega(\varepsilon^{-3})\) is a sample complexity lower bound for standard stochastic nonconvex optimization [3], and hence, also for the problems considered in this paper.

An interesting case is when \(s=0\), where we are approximating \(q(\mathbb{E}[\delta])\) with evaluations of \(\{q(\delta)\}_{\delta\sim\mathcal{D}}\). Now, if \(q\) is an affine function, then \(q(s+\mathbb{E}[\delta])=\mathbb{E}[q(s+\delta)]\). However, the equality does not hold true for general \(q\), and there exists a bias, i.e., \(|q(s+\mathbb{E}[\delta])-\mathbb{E}[q(s+\delta)]|>0\). In this section, we introduce a stochastic _extrapolation_-based method, where we use an affine combination of biased stochastic estimates, to achieve better approximation.

Suppose \(q\in\mathcal{C}^{2k}\) is a continuous differentiable up to \(2k\)-th derivative and let \(h=\mathbb{E}[\delta]\). We expand \(q(s+\delta)\), the most straightforward approximation of \(q(s+\mathbb{E}[\delta])\), using Taylor series at \(s+h\), and take expectation,

\[\begin{split}\mathbb{E}[q(s+\delta)]=& q(s+h)+q^{ \prime}(s+h)\,\mathbb{E}[\delta-h]+\frac{q^{\prime\prime}(s+h)}{2}\,\mathbb{E} [(\delta-h)^{2}]+\frac{q^{(3)}(s+h)}{6}\,\mathbb{E}[(\delta-h)^{3}]\\ +&\ldots+\frac{q^{(2k-1)}(s+h)}{(2k-1)!}\,\mathbb{E} [(\delta-h)^{(2k-1)}]+\frac{1}{(2k)!}\,\mathbb{E}[q^{(2k)}(\phi_{\delta})( \delta-h)^{2k}],\end{split}\] (2)

where \(\phi_{\delta}\) between \(s+\delta\) and \(s+h\). While \(\mathbb{E}[q(s+\delta)]\) matches \(q(s+h)\) in the first 2 terms, the third term is no longer zero. The approximation error (bias) is

\[|\,\mathbb{E}[q(s+\delta)]-q(s+h)|=|\tfrac{q^{\prime\prime}(s+h)}{2}\,\mathbb{ E}[(\delta-h)^{2}]+\ldots+\tfrac{1}{(2k)!}\,\mathbb{E}[q^{(2k)}(\phi_{\delta})( \delta-h)^{2k}]|.\]

In order to analyze the upper bound, we make the following assumption on \(\mathcal{D}\) and \(q\).

**Assumption 1** (Bounded moments): _For all \(\delta\sim\mathcal{D}\) has bounded higher-order moments: \(\sigma_{l}:=|\,\mathbb{E}[(\delta-\mathbb{E}[\delta])^{l}]|<\infty\) for \(l=2,3,\ldots 2k\)._

**Assumption 2** (Bounded derivatives): _The \(q\in\mathcal{C}^{2k}\) and has bounded derivatives, i.e., \(a_{l}:=\sup_{s\in dom(q)}|q^{(l)}(s)|<\infty\) for \(l=1,2,\ldots,2k\)._

In addition, we consider a sample averaged distribution \(\mathcal{D}_{m}\) derived from \(\mathcal{D}\) as follows.

**Definition 2**: _Given a distribution \(\mathcal{D}\) satisfying Assumption 1 and \(m\in\mathbb{N}^{+}\), we define the distribution \(\mathcal{D}_{m}\) that outputs \(\delta\) where \(\delta=\frac{1}{m}\sum_{i=1}^{m}\delta_{i}\) with \(\delta_{i}\stackrel{{ i.i.d.}}{{\sim}}\mathcal{D}\)._

The moments of such distribution \(\mathcal{D}_{m}\) decrease with batch size \(m\) as \(k\geq 2\), \(|\,\mathbb{E}[(\delta-\mathbb{E}[\delta])^{k}]|=\mathcal{O}(m^{-\lceil k/2 \rceil})\) (see Lemma 1). Our desiderata would be to construct a scheme that uses some samples from the distribution \(\mathcal{D}_{m}\) to construct an approximation of \(q(s+\mathbb{E}[\delta])\) that satisfies the following requirement.

**Definition 3** (\(k\)**th-order Extrapolation Operator**): _Given a function \(q:\mathbb{R}\to\mathbb{R}\) satisfying Assumption 2 and distribution \(\mathcal{D}_{m}\) satisfying Assumption 1, we define a \(k\)th-order extrapolation operator \(\mathcal{T}_{\mathcal{D}_{m}}^{(k)}\) as an operator from \(\mathcal{C}^{2k}\to\mathcal{C}^{2k}\) that given \(N=N(k)\) i.i.d. samples \(\delta_{1},\ldots,\delta_{N}\) from \(\mathcal{D}_{m}\) satisfies \(\forall s\in\mathbb{R}\colon|\,\mathbb{E}[\mathcal{T}_{m}^{(k)}q(s)]-q(s+ \mathbb{E}[\delta])|=\mathcal{O}(m^{-k})\)._

We now propose a sequence of operators \(\mathcal{L}_{\mathcal{D}_{m}}^{(1)},\mathcal{L}_{\mathcal{D}_{m}}^{(2)}, \mathcal{L}_{\mathcal{D}_{m}}^{(3)},\ldots\) that satisfy the above definition. The \(\mathcal{L}_{\mathcal{D}_{m}}^{(k)}q(s)\) is designed to ensure its Taylor expansion at \(s+h\) has a form of \(q(s+h)+\mathcal{O}(\mathbb{E}[(\delta-h)^{2k}])\). The remainder \(\mathcal{O}(\mathbb{E}[(\delta-h)^{2k}])\) is bounded by \(\mathcal{O}(m^{-k})\) due to Lemma 1.

**A First-order Extrapolation Operator.** We define the simplest operator

\[\mathcal{L}_{\mathcal{D}_{m}}^{(1)}q:s\mapsto[q(s+\delta)]\qquad\text{ where }\delta\stackrel{{ i.i.d.}}{{\sim}}\mathcal{D}_{m}.\]

In Proposition 2 (Appendix C), we show that \(\mathcal{L}_{\mathcal{D}_{m}}^{(1)}\) is a first-order extrapolation operator.4

Footnote 4: Note that if the function \(q\) is only \(L_{q}\)-Lipschitz continuous, then \(|\mathbb{E}\left[q(s+\delta)\right]-q(s+\mathbb{E}[\delta])|\,\,\,\leq\,\sqrt{L _{q}^{2}\,\mathbb{E}[\|\delta-\mathbb{E}[\delta]\|^{2}}\leq\frac{L_{q}\sqrt{ \sigma_{2}}}{m!^{2}}\). Therefore, in this case, \(q(s+\delta)\) does not satisfy the first-order guarantee.

**A Second-order Extrapolation Operator.** We define the following linear operator \(\mathcal{L}_{\mathcal{D}_{m}}^{(2)}\) which transforms \(q\in\mathcal{C}^{4}\) into \(\mathcal{L}_{\mathcal{D}_{m}}^{(2)}q\) which has lesser bias (but similar variance, as shown later).

**Definition 4** (\(\mathcal{L}_{\mathcal{D}_{m}}^{(2)}\) Operator): _Given \(\mathcal{D}_{m}\) and \(q\), define the following operator,_

\[\mathcal{L}_{\mathcal{D}_{m}}^{(2)}q:s\mapsto\left[2\cdot q(s+\tfrac{\delta_{1} +\delta_{2}}{2})-\tfrac{q(s+\delta_{1})+q(s+\delta_{2})}{2}\right]\qquad\text{ where }\delta_{1},\delta_{2}\stackrel{{ i.i.d.}}{{\sim}}\mathcal{D}_{m}.\]Note that \(\frac{\delta_{1}+\delta_{2}}{2}\) is same as sampling from \(\mathcal{D}_{2m}\). The absolute difference in the Taylor expansion of \(\mathcal{L}_{\mathcal{D}_{m}}^{(2)}q\) at \(s+h\) differs from \(q(s+h)\) as,

\[\mathcal{O}\left(\left|\mathbb{E}\left[2(\tfrac{\delta_{1}+\delta_{2}}{2}-h)^{ 3}-\tfrac{1}{2}((\delta_{1}-h)^{3}+(\delta_{2}-h)^{3})\right]\right|\right)= \mathcal{O}(|(\mathbb{E}[(\delta-h)^{3}]|)\text{ for }\delta\stackrel{{ i.i.d.}}{{\sim}} \mathcal{D}_{m}.\] (3)

The bias error of this scheme can be bounded through the following proposition.

**Proposition 1** (**Second-order Guarantee**): _Assume that distribution \(\mathcal{D}_{m}\) and \(q(\cdot)\) satisfies Assumption 1 and 2 respectively with \(k=2\). Then, for all \(s\in\mathbb{R}\), \(\left|\mathbb{E}\left[\mathcal{L}_{\mathcal{D}_{m}}^{(2)}q(s)\right]-q(s+ \mathbb{E}[\delta])\right|\leq\frac{4a_{3}\sigma_{3}+9a_{4}\sigma_{2}^{2}}{48 m^{2}}+\frac{5a_{4}}{96}\frac{\sigma_{4}-3\sigma_{2}^{2}}{m^{3}}\)._

**Remark 1**: _While extrapolation is motivated by Taylor expansion which requires smoothness, higher order derivatives are not explicitly computed. Appendix F.3 empirically shows that applying extrapolation to non-smooth functions achieves similar bias correction. Relaxing the smoothness conditions is a direction for future work._

The above proposition shows that \(\mathcal{L}_{\mathcal{D}_{m}}^{(2)}\) is in fact a second-order extrapolation operator with \(k=2\) under Definition 3. We will use this operator when we consider the CSO and FCCO problems later. Now, focusing on variance, we can relate the variance of \(\mathcal{L}_{\mathcal{D}_{m}}^{(2)}q(s)\) in terms of the variance of \(q(s+\delta)\). In particular, a consequence of Lemma 2 is that

\[\mathbb{E}\left[\left(\mathcal{L}_{\mathcal{D}_{m}}^{(2)}q(s)-\mathbb{E}[ \mathcal{L}_{\mathcal{D}_{m}}^{(2)}q(s)]\right)^{2}\right]=\mathcal{O}( \mathbb{E}[(q(s+\delta)-\mathbb{E}[q(s+\delta)])^{2}]).\]

**Extension of \(\mathcal{L}_{\mathcal{D}_{m}}^{(2)}\) to Higher-dimensional Case.** If \(q:\mathbb{R}^{p}\rightarrow\mathbb{R}^{\ell}\) is a vector-valued function, then there is a straightforward extension of Definition 4. Now, for distribution \(\mathcal{D}\) over \(\mathbb{R}^{p}\) and corresponding sampled averaged distribution \(\mathcal{D}_{m}\), and \(\bm{s}\in\mathbb{R}^{p}\)

\[\mathcal{L}_{\mathcal{D}_{m}}^{(2)}q:\bm{s}\mapsto\left[2\cdot q(\bm{s}+ \tfrac{\bm{\delta}_{1}+\bm{\delta}_{2}}{2})-\tfrac{q(\bm{s}+\bm{\delta}_{1})+ q(\bm{s}+\bm{\delta}_{2})}{2}\right]\qquad\text{ where }\bm{\delta}_{1},\bm{\delta}_{2}\stackrel{{ \text{i.i.d.}}}{{\sim}}\mathcal{D}_{m}.\] (4)

**Higher-order Extrapolation Operators.** The idea behind the construction of \(\mathcal{L}_{\mathcal{D}_{m}}^{(2)}\) can be generalized to higher \(k\)'s. For example, in Proposition 3, we construct a third-order extrapolation operator \(\mathcal{L}_{\mathcal{D}_{m}}^{(3)}\) through higher degree Taylor series approximation

\[\mathcal{L}_{\mathcal{D}_{m}}^{(3)}q:s\mapsto(-\tfrac{1}{36}\mathcal{L}_{ \mathcal{D}_{m}}^{(2)}+\tfrac{5}{9}\mathcal{L}_{\mathcal{D}_{2m}}^{(2)}- \tfrac{3}{4}\mathcal{L}_{\mathcal{D}_{3m}}^{(2)}-\tfrac{16}{9}\mathcal{L}_{ \mathcal{D}_{4m}}^{(2)}+3\mathcal{L}_{\mathcal{D}_{6m}}^{(2)})q(s).\]

While this idea of expressing the \(k\)-th order operator as an affine combination of lower-order operators works for every \(k\), explicit constructions soon become tedious.

In Fig. 1, we empirically demonstrate the effectiveness of extrapolation in stochastic estimation. 5 In Fig. 0(a), we choose \(q(s)=s^{2}/2\), \(\delta\sim\mathcal{N}(10,100)\). For both \(\mathcal{L}_{\mathcal{D}_{6m}}^{(2)}q(s)\) and \(\mathcal{L}_{\mathcal{D}_{m}}^{(3)}q(s)\), their estimation errors converge to 0 with increasing number of estimates. This coincides with Proposition 1 as \(a_{3}=0\)and \(a_{4}=0\) for quadratic \(q\). In contrast, biased first order method only converges to a neighborhood. In Fig. 0(b), we consider \(q(s)=s^{4}\) and \(p(\delta)=\delta/2\) where \(\delta\in[0,2]\). All three methods are biased and their biases decrease with \(m\), i.e. \(\mathcal{O}(m^{-k})\) for \(k\)th order method. Depending on the constants (e.g. \(a_{i}\), \(\sigma_{i}\)), a higher-order extrapolation method may need decently large \(m\) (burn-in phase) to outperform lower-order methods.

## 3 Applying Stochastic Extrapolation in the CSO Problem

In this section, we apply the extrapolation-based scheme from the previous section to reduce the bias in the CSO problem. We focus on variants of BSGD and their accelerated version BSpiderBoost based on our second-order approximation operator (Definition 4). Let \(H_{\xi}\), \(\tilde{H}_{\xi}\), and \(H_{\xi}^{\prime}\) indicate different sets, each of which contains \(m\) i.i.d. random variables/samples drawn from the conditional distribution \(\mathbb{P}(\eta|\xi)\). Remember that, as mentioned earlier, we use \(g_{\eta}(\bm{x})\) to represent \(g_{\eta}(\bm{x};\xi)\).

**Extrapolated BSGD.** At time \(t\), BSGD constructs a biased estimator of \(\nabla F(\bm{x}^{t})\) using one sample \(\xi\) and \(2m\) i.i.d. samples from the conditional distribution as in (1)

\[G_{\text{BSGD}}^{t+1}=\big{(}\tfrac{1}{m}\sum\nolimits_{\tilde{\eta}\in \tilde{H}_{\xi}}\nabla g_{\tilde{\eta}}(\bm{x}^{t})\big{)}^{\top}\nabla f_{\xi }\big{(}\tfrac{1}{m}\sum\nolimits_{\eta\in H_{\xi}}g_{\eta}(\bm{x}^{t})\big{)}.\] (5)

To reduce this bias, we apply the second-order extrapolation operator from (4). At time \(t\), we define \(\mathcal{D}_{\bm{g},\xi}^{t+1}\) to be the distribution of the random variable \(\frac{1}{m}\sum\nolimits_{\eta\in H_{\xi}}g_{\eta}(\bm{x}^{t})\). Then we apply \(\mathcal{L}_{\mathcal{D}_{\bm{g},\xi}^{t+1}}^{(2)}\) by setting \(q\) to \(\nabla f_{\xi}\) and \(\bm{s}=0\), i.e.

\[\mathcal{L}_{\mathcal{D}_{\bm{g},\xi}^{t+1}}^{(2)}\nabla f_{\xi} (0):=2\nabla f_{\xi}\left(\tfrac{1}{2m}\sum\nolimits_{\eta\in H_{\xi}}g_{\eta }(\bm{x}^{t})+\tfrac{1}{2m}\sum\nolimits_{\eta^{\prime}\in H_{\xi}^{\prime}} g_{\eta^{\prime}}(\bm{x}^{t})\right)\] \[\qquad\qquad\qquad\qquad\qquad-\tfrac{1}{2}\left(\nabla f_{\xi}( \tfrac{1}{m}\sum\nolimits_{\eta\in H_{\xi}}g_{\eta}(\bm{x}^{t}))+\nabla f_{\xi }(\tfrac{1}{m}\sum\nolimits_{\eta^{\prime}\in H_{\xi}^{\prime}}g_{\eta^{\prime }}(\bm{x}^{t}))\right),\] (6)

where \(\frac{1}{m}\sum\nolimits_{\eta\in H_{\xi}}g_{\eta}(\bm{x}^{t})\) and \(\frac{1}{m}\sum\nolimits_{\eta^{\prime}\in H_{\xi}^{\prime}}g_{\eta^{\prime}}( \bm{x}^{t})\) are i.i.d. drawn from \(\mathcal{D}_{\bm{g},\xi}^{t+1}\). In Algorithm 2 (Appendix A), we present our extrapolated BSGD (E-BSGD) scheme, where we replace \(\nabla f_{\xi}(\frac{1}{m}\sum\nolimits_{\eta\in H_{\xi}}g_{\eta}(\bm{x}^{t}))\) in (5) by \(\mathcal{L}_{\mathcal{D}_{\bm{g},\xi}^{t+1}}^{(2)}\nabla f_{\xi}(0)\) resulting in this following gradient estimate:

\[G_{\text{E-BSGD}}^{t+1}=\left(\tfrac{1}{m}\sum\nolimits_{\tilde{\eta}\in \tilde{H}_{\xi}}\nabla g_{\tilde{\eta}}(\bm{x}^{t})\right)^{\top}\mathcal{L}_{ \mathcal{D}_{\bm{g},\xi}^{t+1}}^{(2)}\nabla f_{\xi}(0).\] (7)

**Extrapolated BSpiderBoost.** BSpiderBoost, proposed by Hu et al. [16], uses the variance reduction methods for nonconvex smooth stochastic optimization developed by Fang et al. [10], Wang et al. [38]. BSpiderBoost builds upon BSGD and has two kinds of updates: a large batch and a small batch update. In each step, it decides which update to apply based on a random coin. With probability \(p_{\text{out}}\), it selects a large batch update with \(B_{1}\) outer samples of \(\xi\). With remaining probability \(1-p_{\text{out}}\), it selects a small batch update where the gradient estimated will be updated with gradient information in the current iteration generated with \(B_{2}\) outer samples of \(\xi\) and the information from the last iteration. Formally, it constructs a gradient estimate as follows,

\[G_{\text{BSB}}^{t+1}=\begin{cases}G_{\text{BSB}}^{t}+\frac{1}{B_{2}}\sum \nolimits_{\xi\in\mathcal{B}_{2},|\mathcal{B}_{2}|=B_{2}}(G_{\text{BSGD}}^{t+1} -G_{\text{BSGD}}^{t})&\text{with prob. }1-p_{\text{out}}\\ \frac{1}{B_{1}}\sum\nolimits_{\xi\in\mathcal{B}_{1},|\mathcal{B}_{1}|=B_{1}}G_{ \text{BSGD}}^{t+1}&\text{with prob. }p_{\text{out}}.\end{cases}\] (8)

We propose our extrapolated BSpiderBoost scheme (formally defined in Algorithm 3, Appendix A) by replacing the BSGD gradient estimates in (8) with E-BSGD.

\[G_{\text{E-BSB}}^{t+1}=\begin{cases}G_{\text{E-BSB}}^{t}+\frac{1}{B_{2}}\sum \nolimits_{\xi\in\mathcal{B}_{2},|\mathcal{B}_{2}|=B_{2}}(G_{\text{E-BSGD}}^{t+ 1}-G_{\text{E-BSGD}}^{t})&\text{with prob. }1-p_{\text{out}}\\ \frac{1}{B_{1}}\sum\nolimits_{\xi\in\mathcal{B}_{1},|\mathcal{B}_{1}|=B_{1}}G_{ \text{E-BSGD}}^{t+1}&\text{with prob. }p_{\text{out}}.\end{cases}\] (9)

**Sample Complexity Analyses of E-BSGD and E-BSpiderBoost.** We adopt the standard assumptions used in the literature [27; 35; 33; 41]. All proofs are deferred to Appendix D.

**Assumption 3** (Lower bound): \(F\) _is lower bounded by \(F^{\star}\)._

**Assumption 4** (Bounded variance): _Assume that \(g_{\eta}\) and \(\nabla g_{\eta}\) have bounded variances, i.e., for all \(\xi\) in the support of \(\mathbb{P}(\xi)\) and \(\bm{x}\in\mathbb{R}^{p}\), \(\sigma_{g}^{2}:=\mathbb{E}_{\eta\in[}\big{\|}g_{\eta}(\bm{x};\xi)-\mathbb{E}_{ \eta|\xi}[g_{\eta}(\bm{x};\xi)]\big{\|}_{2}^{2}]<\infty\) and \(\zeta_{g}^{2}:=\mathbb{E}_{\eta|\xi}[\big{\|}\nabla g_{\eta}(\bm{x};\xi)- \mathbb{E}_{\eta|\xi}[\nabla g_{\eta}(\bm{x};\xi)]\big{\|}_{2}^{2}]<\infty\)._

**Assumption 5** (Lipschitz continuity/smoothness of \(f_{\xi}\) and \(g_{\eta}\)): _For all \(\xi\) in the support of \(\mathbb{P}(\xi)\), \(f_{\xi}(\cdot)\) is \(C_{f}\)-Lipschitz continuous (i.e., \(\|f_{\xi}(\bm{x})-f_{\xi}(\bm{x}^{\prime})\|_{2}\leq C_{f}\left\|\bm{x}-\bm{x}^ {\prime}\right\|_{2}\,\forall\bm{x},\bm{x}^{\prime}\in\mathbb{R}^{p}\)) and \(L_{f}\)-Lipschitz smooth (i.e., \(\|\nabla f_{\xi}(\bm{x})-\nabla f_{\xi}(\bm{x}^{\prime})\|_{2}\leq L_{f}\left\| \bm{x}-\bm{x}^{\prime}\right\|_{2}\), \(\forall\bm{x},\bm{x}^{\prime}\in\mathbb{R}^{p}\)) for any \(\xi\). Similarly, for all \(\xi\) in the support of \(\mathbb{P}(\xi)\) and \(\eta\) in the support of \(\mathbb{P}(\eta|\xi)\), \(g_{\eta}(\cdot;\xi)\) is \(C_{g}\)-Lipschitz continuous and \(L_{g}\)-Lipschitz smooth._

The smoothness of \(f_{\xi}\) and \(g_{\eta}\) naturally implies the smoothness of \(F\). Zhang and Xiao [41, Lemma 4.2] show that Assumption 5 ensures \(F\) is: 1) \(C_{F}\)-Lipschitz continuous with \(C_{F}=C_{f}C_{g}\); and 2) \(L_{F}\)-Lipschitz smooth with \(L_{F}=L_{g}C_{f}+C_{g}^{2}L_{f}\). We denote \(\tilde{L}_{F}=\zeta_{g}C_{f}+\sigma_{g}C_{g}L_{f}\). Moreover, Assumption 5 also guarantees that \(f_{\xi}\) and \(g_{\eta}\) have bounded gradients. In addition, \(f_{\xi}\) and \(g_{\eta}\) are assumed to satisfy the following regularity condition in order to apply our extrapolation-based scheme from Section 2.

**Assumption 6** (Regularity): _For all \(\xi\) in the support of \(\mathbb{P}(\xi)\), \(\nabla f_{\xi}\) is 4th-order differentiable with bounded derivatives (i.e., \(a_{l}:=\sup_{\bm{g}\in\mathbb{R}^{p}}\big{\|}\nabla^{(l)}f_{\xi}(\bm{g})\big{\|} _{2}<\infty\) for \(l=1,2,3,4\), \(\forall\bm{x}\in\mathbb{R}^{p}\)) and \(g_{\eta}\) has bounded moments upto 4th-order (i.e., \(\sigma_{k}=\sup_{\bm{x}\in\mathbb{R}^{d}}\sup_{\xi}\mathbb{E}_{\eta|\xi} \left[\sum_{i=1}^{p}\big{[}g_{\eta}(\bm{x})-\mathbb{E}_{\eta|\xi}[g_{\eta}( \bm{x})]\big{]}_{i}^{k}\right]<\infty,k=1,2,3,4\))._

**Remark 2**: _The core piece of Assumption 6 is the 4th order differentiability of \(\nabla f_{\xi}\) as other parts can be easily satisfied through appropriate boundedness assumptions. This condition though is satisfied by common instantiations of CSO/FCCO. We discuss some examples including invariant logistic regression, instrumental variable regression, first-order MAML for sine-wave few-shot regression task, deep average precision maximization in Section 5. Therefore, our improvements in sample complexity apply to all these problems._

Consider some time \(t>0\). Let \(G^{t+1}\) be a stochastic estimate of \(\nabla F(\bm{x}^{t})\) where \(\bm{x}^{t}\) is the current iterate. The next iterate \(\bm{x}^{t+1}:=\bm{x}^{t}-\gamma G^{t}\). Let \(\mathbb{E}[\cdot]\) denote the conditional expectation, where we condition on all the randomness until time \(t\). We consider the bias and variance terms coming from our gradient estimate. Formally, we define the following two quantities

\[\mathcal{E}_{\text{bias}}^{t+1}=\big{\|}\nabla F(\bm{x}^{t})-\mathbb{E}[G^{t+ 1}]\big{\|}_{2}^{2},\quad\mathcal{E}_{\text{var}}^{t+1}=\mathbb{E}[\big{\|}G^ {t+1}-\mathbb{E}[G^{t+1}]\big{\|}_{2}^{2}].\]

Our idea of getting to an \(\varepsilon\)-stationary point (Definition 1) will be to ensure that \(\mathcal{E}_{\text{bias}}^{t+1}\) and \(\mathcal{E}_{\text{var}}^{t+1}\) are bounded. The main technical component of our analyses is in fact analyzing these bias and variance terms for the various gradient estimates considered. For this purpose, we first analyze the bias and variance terms for the (original) BSGD (Lemma 5) and BSpiderBoost (Lemma 7) algorithms, which are then used to get the corresponding bounds for our E-BSGD (Lemma 6) and E-BSpiderBoost (Lemma 8) algorithms. Through these bias and variance bounds, we establish the following main results of this section.

**Theorem 3**: _[E-BSGD Convergence] Consider the (CSO) problem. Suppose Assumptions 3, 4, 5, 6 hold true and \(L_{F},C_{F},\tilde{L}_{F},C_{g},F^{\star}\) are constants and \(C_{e}(f;g):=\frac{8a_{9}\sigma_{3}+18a_{9}\sigma_{2}^{2}+5a_{9}\sigma_{4}}{96}\) defined in Corollary 1 are associated with second order extrapolation in the CSO problem. Let step size \(\gamma\leq 1/(2L_{F})\). Then the output \(\bm{x}^{s}\) of E-BSGD (Algorithm 2) satisfies: \(\mathbb{E}[\|\nabla F(\bm{x}^{s})\|_{2}^{2}]\leq\varepsilon^{2}\), for nonconvex \(F\), if the inner batch size \(m=\Omega(C_{e}C_{g}\varepsilon^{-1/2})\), and the number of iterations_

\[T=\Omega(L_{F}(F(\bm{x}^{0})-F^{\star})(\tilde{L}_{F}^{2}/m+C_{F}^{2}) \varepsilon^{-4}).\]

The E-BSGD takes \(\mathcal{O}(\varepsilon^{-4})\) iterations to converge and compute \(\mathcal{O}(\varepsilon^{-0.5})\) gradients per iteration. Therefore, its resulting sample complexity is \(\mathcal{O}(\varepsilon^{-4.5})\) which is more efficient than \(\mathcal{O}(\varepsilon^{-6})\) of BSGD. Similar improvements can be observed for E-BSpiderBoost in Theorem 4.

**Theorem 4**: _[E-BSpiderBoost Convergence] Consider the (CSO) problem under the same assumptions as Theorem 3. Let step size \(\gamma\leq 1/(13L_{F})\). Then the output \(\bm{x}^{s}\) of E-BSpiderBoost (Algorithm 3)_satisfies: \(\mathbb{E}[||\nabla F(\bm{x}^{s})||_{2}^{2}]\leq\varepsilon^{2}\), for nonconvex \(F\), if the inner batch size \(m=\mathcal{O}(C_{e}C_{g}\varepsilon^{-0.5})\), the hyperparameters of the outer loop of E-BSpiderBoost \(B_{1}=(\tilde{L}_{F}^{2}/m+C_{F}^{2})\varepsilon^{-2},\quad B_{2}=\sqrt{B_{1}},\quad p_{\text{out}}=1/B_{2}\), and the number of iterations_

\[T=\Omega(L_{F}(F(\bm{x}^{0})-F^{\star})\varepsilon^{-2}).\]

The resulting sample complexity of E-BSpiderBoost is \(\mathcal{O}(\varepsilon^{-3.5})\), which improves \(\mathcal{O}(\varepsilon^{-5})\) bound of BSpiderBoost [16] and \(\mathcal{O}(\varepsilon^{-4})\) bound of V-MLMC/RT-MLMC [17].

## 4 Applying Stochastic Extrapolation in the FCCO Problem

In this section, we apply the extrapolation-based scheme from Section 2 to the FCCO problem. We focus on case where \(n=O(\varepsilon^{-2})\). For larger \(n\), we can treat the FCCO problem as a CSO problem and get an \(\mathcal{O}(\varepsilon^{-3.5})\) bound from Theorem 4. All missing details are presented in Appendix E.

Now, a straightforward algorithm for FCCO is to use the finite-sum variant of SpiderBoost (or SPIDER) [10; 38] in Algorithm 3. In this case, if we choose the outer batch sizes to be \(B_{1}=n\), \(B_{2}=\sqrt{n}\) and the inner batch size to be \(m=\max\{\varepsilon^{-2}/n,\varepsilon^{-1/2}\}\). The resulting sample complexity of E-BSpiderBoost now becomes, \(\mathcal{O}(\max\{\sqrt{n}/\varepsilon^{2.5},1/\sqrt{n}\varepsilon^{4}\})\), which recovers \(\mathcal{O}(\varepsilon^{-3.5})\) bound as in Theorem 4 for \(n=\Theta(\varepsilon^{-2})\). However, when \(n\) is small, such as \(n=\mathcal{O}(1)\), the sample complexity degenerates to \(\mathcal{O}(\varepsilon^{-4})\) which is worse than the \(\Omega(\varepsilon^{-3})\) lower bound of stochastic optimization [3]. We leave the details to Theorem 8. We still use Assumptions 3, 4, 5, 6 for the analysis of FCCO problem, replacing the role of \(\xi\) with \(i\).

```
1:Input:\(\bm{x}^{0}\in\mathbb{R}^{d}\), step-size \(\gamma\), batch sizes \(S_{1}\), \(S_{2}\), \(B_{1}\), \(B_{2}\), Probability \(p_{\text{in}},p_{\text{out}}\)
2:for\(t=0,1,\ldots,T-1\)do
3:if (\(t=0\)) or (with prob. \(p_{\text{out}}\)) then\(\triangleright\) Large outer batch
4:for\(i\in\mathcal{B}_{1}\sim[n]\) with \(|\mathcal{B}_{1}|=B_{1}\)do
5: draw \(\bm{y}_{i}^{t+1}\) from distribution \(\mathcal{D}_{\bm{y},i}^{t+1}\) defined in (10)
6: compute \(\bm{z}_{i}^{t+1}\) using (11) and define \(\phi_{i}^{t}=\bm{x}^{t}\)
7:endfor
8:\(G_{\text{E-NVR}}^{t+1}=\frac{1}{B_{1}}\sum_{i\in\mathcal{B}_{2}}(\bm{z}_{i}^{t +1})^{\top}\mathcal{L}_{\mathcal{D}_{\bm{y},i}^{t+1}}^{(2)}\nabla f_{i}(0)\)
9:else
10:for\(i\in\mathcal{B}_{2}\) with \(|\mathcal{B}_{2}|=B_{2}\)do
11: draw \(\bm{y}_{i}^{t+1}\) and \(\bm{y}_{i}^{t}\) from distribution \(\mathcal{D}_{\bm{y},i}^{t+1}\) and \(\mathcal{D}_{\bm{y},i}^{t}\) defined in (10)
12: compute \(\bm{z}_{i}^{t+1}\) using (11) and define \(\phi_{i}^{t}=\bm{x}^{t}\)
13:endfor
14:\(G_{\text{E-NVR}}^{t+1}=G_{\text{E-NVR}}^{t}+\frac{1}{B_{2}}\sum_{i\in\mathcal{ B}_{2}}(\bm{z}_{i}^{t+1})^{\top}(\mathcal{L}_{\mathcal{D}_{\bm{y},i}^{(2)}}^{(2)} \nabla f_{i}(0)-\mathcal{L}_{\mathcal{D}_{\bm{y},i}^{(2)}}^{(2)}\nabla f_{i}(0))\)
15:endif
16:\(\bm{x}^{t+1}=\bm{x}^{t}-\gamma G_{\text{E-NVR}}^{t+1}\)
17:endfor
18:Output:\(\bm{x}^{s}\) picked uniformly at random from \(\{\bm{x}^{t}\}_{t=0}^{T-1}\) ```

**Extrapolated NestedVR.** We now introduce a nested variance reduction algorithm E-NestedVR which reaches low sample complexity for all choices of \(n\). Missing proofs from this section are presented in Appendix E. For the stochasticities in the FCCO problem, our idea is to use two nested SpiderBoost variance reduction components: one for the outer random variable \(i\) and the other for the inner random variable \(\eta|i\). In each outer (resp. inner) SpiderBoost step, we choose large batch \(B_{1}\) (resp. \(S_{1}\)) with probability \(p_{\text{out}}\) (resp. \(p_{\text{in}}\)); otherwise we choose small batch. Let \(H_{i}\) denote a set of \(m\) i.i.d. samples drawn from the conditional distribution \(\mathbb{P}(\eta|i)\). Similarly, let \(\tilde{H}_{i}\) denote another set of \(m\) i.i.d. samples drawn from the same conditional distribution. For each given \(i\), we approximate \(\mathbb{E}_{\eta|i}[g_{\eta}(\bm{x}^{t})]\) with \(\bm{y}_{i}^{t+1}\) from distribution \(\mathcal{D}_{\bm{y},i}^{t+1}\) where,

\[\bm{y}_{i}^{t+1}=\begin{cases}\frac{1}{S_{1}}\sum_{\eta\in H_{i}}g_{\eta}(\bm{x}^ {t})&\text{with prob. $p_{\text{in}}$ or $t=0$}\\ \bm{y}_{i}^{t}+\frac{1}{S_{2}}\sum_{\eta\in H_{i}}(g_{\eta}(\bm{x}^{t})-g_{\eta}( \bm{\phi}_{i}^{t}))&\text{with prob. $1-p_{\text{in}}$.}\end{cases}\] (10)

Similarly, we approximate \(\mathbb{E}_{\eta|i}[\nabla g_{\tilde{\eta}}(\bm{x}^{t})]\) with \(\bm{z}_{i}^{t+1}\) defined as follows

\[\bm{z}_{i}^{t+1}=\begin{cases}\frac{1}{S_{1}}\sum_{\eta\in\tilde{H}_{i}}\nabla g _{\tilde{\eta}}(\bm{x}^{t})&\text{with prob. $p_{\text{in}}$ or $t=0$}\\ \bm{z}_{i}^{t}+\frac{1}{S_{2}}\sum_{\tilde{\eta}\in\tilde{H}_{i}}(\nabla g_{\tilde{\eta}}(\bm{x}^{t})- \nabla g_{\tilde{\eta}}(\bm{\phi}_{i}^{t}))&\text{with prob. $1-p_{\text{in}}$,}\end{cases}\] (11)where \(\bm{\phi}_{i}^{t}\) is the last time \(i\) is visited before time \(t\). If \(i\) is not selected at time \(t\), then \(\bm{y}_{i}^{t+1}=\bm{y}_{i}^{t}\) and \(\bm{z}_{i}^{t+1}=\bm{z}_{i}^{t}\). Note that we use independent samples for \(\bm{y}_{i}^{t+1}\) and \(\bm{z}_{i}^{t+1}\).

Finally, we present E-NestedVR in Algorithm 1 where second-order extrapolation operator \(\mathcal{L}^{(2)}\) is applied to each occurrence of \(\nabla f_{i}\). We now analyze its convergence guarantee. Our analysis works by first looking at the effect of multi-level variance reduction without the extrapolation (that we refer to as NestedVR, Theorem 10, Appendix E.2), and then showing how extrapolation could further help to drive down the sample complexity.

**Theorem 5**: _[E-NestedVR Convergence] Consider the (FCCO) problem. Under the same assumptions as Theorem 3._

* _If_ \(n=\mathcal{O}(\varepsilon^{-2/3})\)_, then we choose the hyperparameters of E-NestedVR (Algorithm_ 1_) as_ \(B_{1}=B_{2}=n,p_{out}=1,S_{1}=\tilde{L}_{P}^{2}\varepsilon^{-2},S_{2}=\tilde{ L}_{F}\varepsilon^{-1},p_{in}=\tilde{L}_{F}^{-1}\varepsilon,\gamma=\mathcal{O}( \frac{1}{L_{F}}).\)__
* _If_ \(n=\Omega(\varepsilon^{-2/3})\)_, then we choose the hyperparameters of E-NestedVR as_ \(B_{1}=n,B_{2}=\sqrt{n},p_{out}=1/\sqrt{n},S_{1}=S_{2}=\max\left\{C_{e}C_{g} \varepsilon^{-1/2},\tilde{L}_{F}^{2}/(n\varepsilon^{2})\right\},p_{in}=1, \gamma=\mathcal{O}(\frac{1}{L_{F}}).\)__

_Then the output \(\bm{x}^{s}\) of E-NestedVR satisfies: \(\mathbb{E}[\left\|\nabla F(\bm{x}^{s})\right\|_{2}^{2}]\leq\varepsilon^{2}\), for nonconvex \(F\) with iterations_

\[T=\Omega\left(L_{F}(F(\bm{x}^{0})-F^{\star})\varepsilon^{-2}\right).\]

From Theorem 5, E-NestedVR has a sample complexity of \(\mathcal{O}(n\varepsilon^{-3})\) in the small \(n\) regime (\(n=\mathcal{O}(\varepsilon^{-2/3})\)) and \(\mathcal{O}(\max\{\sqrt{n}/\varepsilon^{2.5},1/\sqrt{n}\varepsilon^{4}\})\) in the large \(n\) regime (\(n=\Omega(\varepsilon^{-2/3})\)). Therefore, in the large \(n\) regime, this improves the \(\mathcal{O}(n\varepsilon^{-3})\) sample complexity of MSVR-V2 [19].

## 5 Applications

In this section, we demonstrate the numerical performance of our proposed algorithms. We focus on the application of invariant logistic regression here. In Appendix F, we discuss performance of our proposed algorithms on other common CSO/FCCO applications.

### Application of Invariant Risk Minimization

Invariant learning has wide applications in machine learning and related areas [22; 1]. Invariant logistic regression [16] is formulated as follows:

\[\min_{\bm{x}}\mathbb{E}_{\xi=(\bm{a},b)}[\log(1+\exp(-b\mathbb{E}_{\eta|\xi}[ \eta]^{\top}\bm{x})],\]

where \(\bm{a}\) and \(b\) represent a sample and its corresponding label, and \(\eta\) is a noisy observation of the sample \(\bm{a}\). This first part can be considered as a CSO objective, with \(f_{\xi}(y):=\log(1+\exp(-by))\) and \(g_{\eta}(\bm{x};\xi):=\eta^{\top}\bm{x}\). As the loss \(f_{\xi}\in\mathcal{C}^{\infty}\) is smooth, our results from Sections 3 and 4 are applicable.

An \(\ell_{2}\)-regularizer is added to ensure the existence of an unique minimizer. Since the gradient of the penalization term is unbiased, we only have to consider the bias of the data-dependent term. We generate a synthetic dataset with \(d=10\) dimensions. The minimizer is drawn from Gaussian distribution \(\bm{x}^{\star}\sim\mathcal{N}(0,1)\in\mathbb{R}^{d}\). We draw invariant samples \(\{(\bm{a}_{i},b_{i})\}_{i}\) where \(\bm{a}_{i}\sim\mathcal{N}(0,1)\in\mathbb{R}^{d}\) and compute \(b_{i}=\text{sgn}(\bm{a}_{i}^{\top}\bm{x}^{\star})\) and its perturbed observation \(\eta\sim\mathcal{N}(\bm{a}_{i},100)\in\mathbb{R}^{d}\).

We consider drawing \(\xi\) from a large set (\(n=50000\)) and a small set (\(n=50\)) as CSO and FCCO problems respectively. As baselines, we implemented the BSGD and BSpiderBoost methods

Figure 2: Performances of algorithms and their extrapolated versions on the invariant logistic regression task. Algorithms in each subplot use the same amount of inner batch size in each iteration. The shaded region represents the 95%-confidence interval computed over 10 runs.

from [16], V-MLMC approach from [17], and NestedVR approach from Appendix E.2 which achieves the same complexity as MSVR-V2 [19] for the FCCO problem. RT-MLMC shares the same sample complexity as V-MLMC, and is thus omitted from the experiment [17]. The results are shown in Fig. 2. In the CSO setting, we compare biased gradient methods with their extrapolated variants (BSGD vs. E-BSGD, BSpiderBoost vs. E-BSpiderBoost, and NestedVR vs. E-NestedVR). The extrapolated versions of BSGD, BSpiderBoost, and NestedVR consistently reach lower error than their non-extrapolated counterparts, as is evident in Figure 1(a). In this case, the performance of BSpiderBoost is similar to BSGD as also noted by the authors of these techniques [16], and a drawback of BSpiderBoost seems to be that it is much harder to tune in practice. However, it is clear that E-BSGD outperforms BSGD, and E-BSpiderBoost outperforms BSpiderBoost, respectively. In the FCCO setting, we compare extrapolation based methods and MLMC based methods. Figure 1(a), shows that E-NestedVR outperforms all other extrapolated algorithms, including the V-MLMC approach of [17], matching our theoretical findings.

### Application of Instrumental Variable Regression

Instrumental variable regression is a popular technique in econometrics that aims to estimate the causal effect of input \(X\) on a confounded outcome \(Y\) with an instrument variable \(Z\), which, for a given \(X\), is conditionally independent of \(Y\). As noted by [23], the instrumental variable regression is a special case of the CSO problem. The instrumental variable regression problem is phrased as:

\[\min_{w}\mathbb{E}_{\xi=(Y,Z)}\left[\ell(Y,\mathbb{E}_{X|Z}[g_{X}(w)])\right]\]

where \(\xi=(Y,Z),\eta=X\). This can be viewed in the CSO framework with \(f_{\xi}(\cdot)=\ell(Y,\cdot)\). We choose \(\ell(y,\hat{y})=\log\cosh(y-\hat{y})\) as regression loss function and \(g_{X}(w)=w^{\top}X\) to be a linear regressor. In this case, \(f_{\xi}\in\mathcal{C}^{\infty}\) with \(\nabla f_{\xi}(\hat{y})=\tanh(\hat{y}-Y)\), and our results from Sections 3 and 4 apply. We generate the data similar to [31]

\[Z\sim\text{Uniform}([-3,3]^{2}),\quad e\sim\mathcal{N}(0,1), \delta\sim\mathcal{N}(0,0.1),\quad\gamma\sim\text{Exponential}(10)\] \[X=\tfrac{1}{2}z_{1}+\tfrac{1}{2}e+\gamma, Y=X+e+\delta\]

where \(z_{1}\) is the first dimension of \(Z\), \(e\) is the confounding variable and \(\delta\), \(\gamma\) are noises. In this experiment, we solve the instrumental variable regression using BSGD, BSpiderBoost, NestedVR and their extrapolated variants described in Sections 3 and 4. In each pair of experiments, the samples used per iteration are fixed same, i.e.: 1) BSGD uses \(m=2\) and E-BSGD uses \(m=1\); 2) For BSpiderBoost and E-BSpiderBoost, we use cycle length of 10, small batch and large batch in Spider to be 10 and 100 respectively, and we choose inner batch sizes \(m=2\) for BSpiderBoost and \(m=1\) for E-BSpiderBoost; 3) For NestedVR and E-NestedVR, we fix the outer batch size to 10 and 5 respectively, and choose fix the inner Spider Cycle to be 10 with large batch 100 and small batch 10. The results are presented in Figure 3. As is quite evident, the extrapolation variants achieve faster convergence in all 3 cases, confirming our theoretical findings.

## 6 Concluding Remarks

In this paper, we consider the conditional stochastic optimization CSO problem and its finite-sum variant FCCO. Due to the interplay between nested structure and stochasticity, most of the existing gradient estimates suffer from large biases and have large sample complexity of \(\mathcal{O}(\varepsilon^{-5})\). We propose stochastic extrapolation-based algorithms that tackle this bias problem and improve the sample complexities for both these problems. While we focus on nonconvex objectives, our proposed algorithms can also be beneficial when used with strongly convex, convex objectives. We also believe that similar ideas could also prove helpful for multi-level stochastic optimization problems [41] with nested dependency.

Figure 3: Performances of algorithms and their extrapolated versions on the instrumental variable regression task. The shaded region represents the 95%-confidence interval.

## Acknowledgements

We would like to thank Caner Turkmen, Sai Praneeth Karimireddy, and Martin Jaggi for helpful initial discussions surrounding this project.

## References

* A. Defazio, F. R. Bach, and S. Lacoste-Julien (2014)SAGA: A fast incremental gradient method with support for non-strongly convex composite objectives. In Zoubin Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger (Eds.), Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems 2014, December 8-13 2014, Montreal, Quebec, Canada, pp. 1646-1654. External Links: Link, Document Cited by: SS1.
* S. Bubeck, Q. Jiang, Y. T. Lee, Y. Li, and A. Sidford (2019)Near-optimal method for highly smooth convex optimization. In Proceedings of Machine Learning Research, OLT 2019, Phoenix, AZ, USA, pp. 492-507. External Links: Link, Document Cited by: SS1.
* S. Bubeck, Q. Jiang, Y. T. Lee, Y. Li, and A. Sidford (2019)Near-optimal method for highly smooth convex optimization. In Proceedings of Machine Learning Research, OLT 2019, Phoenix, AZ, USA, pp. 492-507. External Links: Link, Document Cited by: SS1.
* B. Dai, N. He, Y. Pan, B. Boots, and L. Song (2017)Learning from conditional distributions via dual embeddings. In Proceedings of the 20th International Conference on Artificial Intelligence and Statistics, AISTATS 2017, 20-22 April 2017, Fort Lauderdale, FL, USA, pp. 54 of Proceedings of Machine Learning Research, Vol. 1458-1467. External Links: Link, Document Cited by: SS1.
* B. Dai, A. Shaw, L. Li, L. Xiao, N. He, Z. Liu, J. Chen, and L. Song (2018)SBEED: convergent reinforcement learning with nonlinear function approximation. In Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsmassan, Stockholm, Sweden, July 10-15, 2018, Vol. 80, Proceedings of Machine Learning Research, Vol. 80, pp. 1133-1142. External Links: Link, Document Cited by: SS1.
* A. Defazio, F. R. Bach, and S. Lacoste-Julien (2014)SAGA: A fast incremental gradient method with support for non-strongly convex composite objectives. In Advances in Neural Information Processing Systems 2014, December 8-13 2014, Montreal, Quebec, Canada, pp. 1646-1654. External Links: Link, Document Cited by: SS1.
* A. Defazio, F. R. Bach, and S. Lacoste-Julien (2018)SAGA: A fast incremental gradient method with support for non-strongly convex composite objectives. In Advances in Neural Information Processing Systems 2018, December 8-13 2014, Montreal, Quebec, Canada, pp. 1646-1654. External Links: Link, Document Cited by: SS1.
* A. Defazio, F. R. Bach, and S. Lacoste-Julien (2018)SAGA: A fast incremental gradient method with support for non-strongly convex composite objectives. In Advances in Neural Information Processing Systems 2018, December 8-13 2014, Montreal, Quebec, Canada, pp. 1646-1654. External Links: Link, Document Cited by: SS1.
* A. Defazio, F. R. Bach, and S. Lacoste-Julien (2018)SAGA: A fast incremental gradient method with support for non-strongly convex composite objectives. In Advances in Neural Information Processing Systems 2018, December 8-13 2014, Montreal, Quebec, Canada, pp. 1646-1654. External Links: Link, Document Cited by: SS1.
* A. Defazio, F. R. Bach, and S. Lacoste-Julien (2018)SAGA: A fast incremental gradient method with support for non-strongly convex composite objectives. In Advances in Neural Information Processing Systems 2018, December 8-13 2014, Montreal, Quebec, Canada, pp. 1646-1654. External Links: Link, Document Cited by: SS1.
* A. Defazio, F. R. Bach, and S. Lacoste-Julien (2018)SAGA: A fast incremental gradient method with support for non-strongly convex composite objectives. In Advances in Neural Information Processing Systems 2018, December 8-13 2014, Montreal, Quebec, Canada, pp. 1646-1654. External Links: Link, Document Cited by: SS1.
* A. Defazio, F. R. Bach, and S. Lacoste-Julien (2018)SAGA: A fast incremental gradient method with support for non-strongly convex composite objectives. In Advances in Neural Information Processing Systems 2018, December 8-13 2014, Montreal, Quebec, Canada, pp. 1646-1654. External Links: Link, Document Cited by: SS1.
* A. Defazio, F. R. Bach, and S. Lacoste-Julien (2018)SAGA: A fast incremental gradient method with support for non-strongly convex composite objectives. In Advances in Neural Information Processing Systems 2018, December 8-13 2014, Montreal, Quebec, Canada, pp. 1646-1654. External Links: Link, Document Cited by: SS1.
* A. Defazio, F. R. Bach, and S. Lacoste-Julien (2018)SAGA: A fast incremental gradient method with support for non-strongly convex composite objectives. In Advances in Neural Information Processing Systems 2018, December 8-13 2014, Montreal, Quebec, Canada, pp. 1646-1654. External Links: Link, Document Cited by: SS1.
* A. Defazio, F. R. Bach, and S. Lacoste-Julien (2018)SAGA: A fast incremental gradient method with support for non-strongly convex composite objectives. In Advances in Neural Information Processing Systems 2018, December 8-13 2014, Montreal, Quebec, Canada, pp. 1646-1654. External Links: Link, Document Cited by: SS1.
* A. Defazio, F. R. Bach, and S. Lacoste-Julien (2018)SAGA: A fast incremental gradient method with support for non-strongly convex composite objectives. In Advances in Neural Information Processing Systems 2018, December 8-13 2014, Montreal, Quebec, Canada, pp. 1646-1654. External Links: Link, Document Cited by: SS1.
* A. Defazio, F. R. Bach, and S. Lacoste-Julien (2018)SAGA: A fast incremental gradient method with support for non-strongly convex composite objectives. In Advances in Neural Information Processing Systems 2018, December 8-13 2014, Montreal, Quebec, Canada, pp. 1646-1654. External Links: Link, Document Cited by: SS1.
* A. Defazio, F. R. Bach, and S. Lacoste-Julien (2018)SAGA: A fast incremental gradient method with support for non-strongly convex composite objectives. In Advances in Neural Information Processing Systems 2018, December 8-13 2014, Montreal, Quebec, Canada, pp. 1646-1654. External Links: Link, Document Cited by: SS1.
* A. Defazio, F. R. Bach, and S. Lacoste-Julien (2018)SAGA: A fast incremental gradient method with support for non-strongly convex composite objectives. In Advances in Neural Information Processing Systems 2018, December 8-13 2014, Montreal, Quebec, Canada, pp. 1646-1654. External Links: Link, Document Cited by: SS1.
* A. Defazio, F. R. Bach, and S. Lacoste-Julien (2018)SAGA: A fast incremental gradient method with support for non-strongly convex composite objectives. In Advances in Neural Information Processing Systems 2018, December 8-13 2014, Montreal, Quebec, Canada, pp. 1646-1654. External Links: Link, Document Cited by: SS1.
* A. Defazio, F. R. Bach, and S. Lacoste-Julien (2018)SAGA: A fast incremental gradient method with support for non-strongly convex composite objectives. In Advances in Neural Information Processing Systems 2018, December 8-13 2014, Montreal, Quebec, Canada, pp. 1646-1654. External Links: Link, Document Cited by: SS1.
* A. Defazio, F. R. Bach, and S. Lacoste-Julien (2018)SAGA: A fast incremental gradient method with support for non-strongly convex composite objectives. In Advances in Neural Information Processing Systems 2018, December 8-13 2014, Montreal, Quebec, Canada, pp. 1646-1654. External Links: Link, Document Cited by: SS1.
* A. Defazio, F. R. Bach, and S. Lacoste-Julien (2018)SAGA: A fast incremental gradient method with support for non-strongly convex composite objectives. In Advances in Neural Information Processing Systems 2018, December 8-13 2014, Montreal, Quebec, Canada, pp. 1646-1654. External Links: Link, Document Cited by: SS1.
* A. Defazio, F. R. Bach, and S. Lacoste-Julien (2018)SAGA: A fast incremental gradient method with support for non-strongly convex composite objectives. In Advances in Neural Information Processing Systems 2018, December 8-13 2014, Montreal, Quebec, Canada, pp. 1646-1654. External Links: Link, Document Cited by: SS1.
* A. Defazio, F. R. Bach, and S. Lacoste-Julien (2018)SAGA: A fast incremental gradient method with support for non-strongly convex composite objectives. In Advances in Neural Information Processing Systems 2018, December 20-22 2018, Montreal, Quebec, Canada, pp. 1646-1654. External Links: Link, Document Cited by: SS1.
* A. Defazio, F. R. Bach, and S. Lacoste-Julien (2018)SAGA: A fast incremental gradient method with support for non-strongly convex composite objectives.

* Finn et al. [2017] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In Doina Precup and Yee Whye Teh, editors, _Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017_, volume 70 of _Proceedings of Machine Learning Research_, pages 1126-1135. PMLR, 2017. URL http://proceedings.mlr.press/v70/finn17a.html.
* Ghadimi and Lan [2013] Saeed Ghadimi and Guanghui Lan. Stochastic first-and zeroth-order methods for nonconvex stochastic programming. _SIAM Journal on Optimization_, 23(4):2341-2368, 2013.
* Goda and Kitade [2022] Takashi Goda and Wataru Kitade. Constructing unbiased gradient estimators with finite variance for conditional stochastic optimization. _ArXiv preprint_, abs/2206.01991, 2022. URL https://arxiv.org/abs/2206.01991.
* Han et al. [2020] Yanjun Han, Jiantao Jiao, and Tsachy Weissman. Minimax estimation of divergences between discrete distributions. _IEEE Journal on Selected Areas in Information Theory_, 1(3):814-823, 2020.
* Hu et al. [2020] Yifan Hu, Xin Chen, and Niao He. Sample complexity of sample average approximation for conditional stochastic optimization. _SIAM Journal on Optimization_, 30(3):2103-2133, 2020.
* Hu et al. [2020] Yifan Hu, Siqi Zhang, Xin Chen, and Niao He. Biased stochastic first-order methods for conditional stochastic optimization and applications in meta learning. In Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, _Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual_, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/1cdf14d1e3699d61d237cf76ce1c2dca-Abstract.html.
* Hu et al. [2021] Yifan Hu, Xin Chen, and Niao He. On the bias-variance-cost tradeoff of stochastic optimization. _Advances in Neural Information Processing Systems_, 34:22119-22131, 2021.
* Jain et al. [2017] Prateek Jain, Purushottam Kar, et al. Non-convex optimization for machine learning. _Foundations and Trends(r) in Machine Learning_, 10(3-4):142-363, 2017.
* Jiang et al. [2022] Wei Jiang, Gang Li, Yibo Wang, Lijun Zhang, and Tianbao Yang. Multi-block-single-probe variance reduced estimator for coupled compositional optimization. _ArXiv preprint_, abs/2207.08540, 2022. URL https://arxiv.org/abs/2207.08540.
* Jiao and Han [2020] Jiantao Jiao and Yanjun Han. Bias correction with jackknife, bootstrap, and taylor series. _IEEE Transactions on Information Theory_, 66(7):4392-4418, 2020.
* Johnson and Zhang [2013] Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance reduction. In Christopher J. C. Burges, Leon Bottou, Zoubin Ghahramani, and Kilian Q. Weinberger, editors, _Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems 2013. Proceedings of a meeting held December 5-8, 2013, Lake Tahoe, Nevada, United States_, pages 315-323, 2013. URL https://proceedings.neurips.cc/paper/2013/hash/ac1dd209cbc5e5d1c6e28598e8cbcbe8-Abstract.html.
* Mroueh et al. [2015] Youssef Mroueh, Stephen Voinea, and Tomaso A. Poggio. Learning with group invariant features: A kernel perspective. In Corinna Cortes, Neil D. Lawrence, Daniel D. Lee, Masashi Sugiyama, and Roman Garnett, editors, _Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada_, pages 1558-1566, 2015. URL https://proceedings.neurips.cc/paper/2015/hash/6602294be910b1e3c4571bd98c4d5484-Abstract.html.
* Muandet et al. [2020] Krikamol Muandet, Arash Mehrjou, Si Kai Lee, and Anant Raj. Dual instrumental variable regression. In Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, _Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual_, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/1c383cd30b7c298ab50293adfecb7b18-Abstract.html.
* Nachum and Dai [2020] Ofir Nachum and Bo Dai. Reinforcement learning via fenchel-rockafellar duality. _ArXiv preprint_, abs/2001.01866, 2020. URL https://arxiv.org/abs/2001.01866.

* Nguyen et al. [2017] Lam M. Nguyen, Jie Liu, Katya Scheinberg, and Martin Takac. SARAH: A novel method for machine learning problems using stochastic recursive gradient. In Doina Precup and Yee Whye Teh, editors, _Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017_, volume 70 of _Proceedings of Machine Learning Research_, pages 2613-2621. PMLR, 2017. URL http://proceedings.mlr.press/v70/nguyen17b.html.
* Qi et al. [2021] Qi Qi, Youzhi Luo, Zhao Xu, Shuiwang Ji, and Tianbao Yang. Stochastic optimization of areas under precision-recall curves with provable convergence. In Marc'Aurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan, editors, _Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual_, 1752-1765, 2021. URL https://proceedings.neurips.cc/paper/2021/hash/0dd1bc593a91620daecf7723d2235624-Abstract.html.
* Qi et al. [2021] Qi Qi, Youzhi Luo, Zhao Xu, Shuiwang Ji, and Tianbao Yang. Stochastic optimization of areas under precision-recall curves with provable convergence. _Advances in Neural Information Processing Systems_, 34:1752-1765, 2021.
* Reddi et al. [2016] Sashank J. Reddi, Ahmed Hefny, Suvrit Sra, Barnabas Poczos, and Alexander J. Smola. Stochastic variance reduction for nonconvex optimization. In Maria-Florina Balcan and Kilian Q. Weinberger, editors, _Proceedings of the 33nd International Conference on Machine Learning, ICML 2016, New York City, NY, USA, June 19-24, 2016_, volume 48 of _JMLR Workshop and Conference Proceedings_, pages 314-323. JMLR.org, 2016. URL http://proceedings.mlr.press/v48/reddi16.html.
* Reddi et al. [2016] Sashank J Reddi, Suvrit Sra, Barnabas Poczos, and Alex Smola. Fast incremental method for nonconvex optimization. _ArXiv preprint_, abs/1603.06159, 2016. URL https://arxiv.org/abs/1603.06159.
* Schmidt et al. [2017] Mark Schmidt, Nicolas Le Roux, and Francis Bach. Minimizing finite sums with the stochastic average gradient. _Mathematical Programming_, 162(1):83-112, 2017.
* Singh et al. [2019] Rahul Singh, Maneesh Sahani, and Arthur Gretton. Kernel instrumental variable regression. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d'Alche-Buc, Emily B. Fox, and Roman Garnett, editors, _Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada_, pages 4595-4607, 2019. URL https://proceedings.neurips.cc/paper/2019/hash/17b3c7061788dbe82de5abe9f6fe22b3-Abstract.html.
* Tukey [1958] John Tukey. Bias and confidence in not quite large samples. _Ann. Math. Statist._, 29:614, 1958.
* Wang and Yang [2022] Bokun Wang and Tianbao Yang. Finite-sum coupled compositional stochastic optimization: Theory and applications. In _International Conference on Machine Learning_, pages 23292-23317. PMLR, 2022.
* Wang et al. [2022] Guanghui Wang, Ming Yang, Lijun Zhang, and Tianbao Yang. Momentum accelerates the convergence of stochastic AUPRC maximization. In Gustau Camps-Valls, Francisco J. R. Ruiz, and Isabel Valera, editors, _International Conference on Artificial Intelligence and Statistics, AISTATS 2022, 28-30 March 2022, Virtual Event_, volume 151 of _Proceedings of Machine Learning Research_, pages 3753-3771. PMLR, 2022. URL https://proceedings.mlr.press/v151/wang22b.html.
* Wang et al. [2022] Guanghui Wang, Ming Yang, Lijun Zhang, and Tianbao Yang. Momentum accelerates the convergence of stochastic auprc maximization. In _International Conference on Artificial Intelligence and Statistics_, pages 3753-3771. PMLR, 2022.
* Wang et al. [2016] Mengdi Wang, Ji Liu, and Ethan X. Fang. Accelerating stochastic composition optimization. In Daniel D. Lee, Masashi Sugiyama, Ulrike von Luxburg, Isabelle Guyon, and Roman Garnett, editors, _Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain_, pages 1714-1722, 2016. URL https://proceedings.neurips.cc/paper/2016/hash/92262bf907af914b95a0fc33c3f33bf6-Abstract.html.

* Wang et al. [2017] Mengdi Wang, Ethan X Fang, and Han Liu. Stochastic compositional gradient descent: algorithms for minimizing compositions of expected-value functions. _Mathematical Programming_, 161:419-449, 2017.
* Wang et al. [2019] Zhe Wang, Kaiyi Ji, Yi Zhou, Yingbin Liang, and Vahid Tarokh. Spiderboost and momentum: Faster variance reduction algorithms. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d'Alche-Buc, Emily B. Fox, and Roman Garnett, editors, _Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada,_ pages 2403-2413, 2019. URL https://proceedings.neurips.cc/paper/2019/hash/512c5cad6c37edb98ae91c8a76c3a291-Abstract.html.
* Withers [1987] Christopher Stroude Withers. Bias reduction by taylor series. _Communications in Statistics-Theory and Methods_, 16(8):2369-2383, 1987.
* Yermol'yev [1971] Yu M Yermol'yev. A general stochastic programming problem. 1971.
* Zhang and Xiao [2021] Junyu Zhang and Lin Xiao. Multilevel composite stochastic optimization via nested variance reduction. _SIAM Journal on Optimization_, 31(2):1131-1157, 2021.

Appendix

[title=A] Missing Pseudocodes

## Appendix B Missing Details from Section 1

### Other Related Work

C Missing Details from Section 2

### Stationary Point Convergence Proofs from Section 3 (CSO)

### Helpful Lemmas

D.2 Convergence of BSGD

D.3 Convergence of E-BSGD

D.4 Convergence of BSpiderBoost

D.5 Convergence of E-BSpiderBoost

### Stationary Point Convergence Proofs from Section 4 (FCCO)

E.1 E-BSpiderBoost for FCCO problem

E.2 Convergence of NestedVR

E.3 Convergence of E-NestedVR

### Missing Details from Section 5

F.1 Application of First-order MAML

F.2 Application of Deep Average Precision Maximization

F.3 Necessity of Additional Smoothness Conditions

### Missing Pseudocodes

We present pseudocodes of E-BSGD and E-BSpiderBoost scheme in Algorithms 2 and 3 respectively.

```
1:Input:\(\bm{x}^{0}\in\mathbb{R}^{d}\), step-size \(\gamma\), batch sizes \(m\)
2:for\(t=0,1,\ldots,T-1\)do
3: Draw one sample \(\xi\) and compute extrapolated gradient \(G_{\text{E-BSGD}}^{t+1}\) from (7)
4:\(\bm{x}^{t+1}\leftarrow\bm{x}^{t}-\gamma G_{\text{E-BSGD}}^{t+1}\)
5:endfor
6:Output:\(\bm{x}^{s}\) picked uniformly at random from \(\{\bm{x}^{t}\}_{t=0}^{T-1}\) ```

**Algorithm 2** E-BSGD

## Appendix C Missing Details from Section 1

### Other Related Work

**CSO.** Dai et al. [5] proposed a primal-dual stochastic approximation algorithm to solve a min-max reformulation of CSO, employing the kernel embedding techniques. However, this method requires convexity of \(f_{\xi}\) and linearity of \(g_{\eta}\), which are not satisfied by general applications when neural networks are involved. Goda and Kitade [13] showed that a special class of CSO problems can be unbiased, e.g., when \(f_{\xi}\) measures the squared error between some \(u(\xi)\) and \(\mathbb{E}_{\eta|\xi}[g_{\eta}(\bm{x};\xi]]\), giving rise to this objective function \(\mathbb{E}_{\xi}[(u(\xi)-\mathbb{E}_{\eta|\xi}[g_{\eta}(\bm{x};\xi])^{2}]\). However, they did not show any improvement over the sample complexity of BSGD (i.e., \(\mathcal{O}(\varepsilon^{-6})\)). Hu et al. [16] also analyzed lower bounds on the minimax error for the CSO problem and showed that for a specific class of biased gradients with \(\mathcal{O}(\varepsilon)\) bias (same bias as BSGD) and variance \(\mathcal{O}(1)\) the bound achieved by BSpiderBoost is tight. However, these lower bounds are not applicable in settings such as ours (and also to [17]) where the bias is smaller than the BSGD bias.

**Variance Reduction.** The reduction of variance in stochastic optimization is a crucial approach to decrease sample complexity, particularly when dealing with finite-sum formulations of the form \(\min_{\bm{x}}\frac{1}{n}\sum_{i=1}^{n}f_{i}(\bm{x})\). Pioneering works such as Stochastic Average Gradient (SAG) [30], Stochastic Variance Reduced Gradient (SVRG) [21, 28], and SAGA [7, 29] improved the iteration complexity from \(\mathcal{O}(\varepsilon^{-4})\) in Stochastic Gradient Descent (SGD) to \(\mathcal{O}(\varepsilon^{-2})\). Subsequent research, including Stochastic Path-Integrated Differential Estimator (SPIDER) [10] and Stochastic Recursive Gradient Algorithm (SARAH) [25], expanded the application of these techniques to both finite-sum and online scenarios, where \(n\) is large or possibly infinite. These methods boast an improved sample complexity of \(\min(\sqrt{n}\varepsilon^{-2},\varepsilon^{-3})\). SpiderBoost [38], achieves the same near-optimal complexity performance as SPIDER, but allows a much larger step size and hence runs faster in practice than SPIDER. In this paper, we use a probabilistic variant of SpiderBoost as the variance reduction module for CSO and FCCO problems. We highlight that alternative techniques, such as SARAH, can also be applied and offer similar guarantees.

**Bias Correction.** One of the classic problems in statistics is to design procedures to reduce the bias of estimators. Well-established general bias correction techniques, such as the jackknife [32], bootstrap [8], Taylor series [39, 14], have been extensively studied and applied in various contexts [20]. However, these methods are predominantly examined in relation to standard statistical distributions, with limited emphasis on their adaptability to optimization problems. Our proposed extrapolation-based approach is derived from sample-splitting methods [14], specifically tailored and analyzed for optimization problems involving unknown distributions.

**Stochastic Composition Optimization.** Finally, a closely related class of problems, called stochastic composition optimization, has been extensively studied (e.g., [40, 9, 36, 37]) in the literature where the goal is:

\[\min_{\bm{x}\in\mathbb{R}^{d}}\mathbb{E}_{\xi}[f_{\xi}(\mathbb{E}_{\eta}[g_{ \eta}(\bm{x})])].\] (12)

Despite having nested expectations in their formulations (CSO) and (12) are fundamentally different: a) in stochastic composite optimization the inner randomness \(\eta\) is conditionally dependent on the outer randomness \(\xi\) and b) in CSO the inner random function \(g_{\eta}(\bm{x},\xi)\) depends on both \(\xi\) and \(\eta\). These differences lead to quite different sample complexity bounds for these problems, as explored in Hu et al. [15]. In fact, Zhang and Xiao [41] presented a near optimal complexity of \(\mathcal{O}(\min(\varepsilon^{-3},\sqrt{n}\varepsilon^{-2}))\)for stochastic composite optimization problems using nested variance reduction. While Wang et al. [36] also use the "extrapolation" technique, their motivation and formula are significantly different from ours and cannot reduce the bias in the CSO problem.

## Appendix C Missing Details from Section 2

**Lemma 1** (Moments of \(\mathcal{D}_{m}\)): _The moments of \(\delta\in\mathcal{D}_{m}\) are bounded as follows_

\[\mathbb{E}[(\delta-\mathbb{E}[\delta])^{2}]=\tfrac{\sigma_{2}}{m},\quad|\, \mathbb{E}[(\delta-\mathbb{E}[\delta])^{3}]|=\tfrac{\sigma_{3}}{m^{2}},\quad \mathbb{E}[(\delta-\mathbb{E}[\delta])^{4}]=\tfrac{\sigma_{4}}{m^{3}}+\tfrac{3 (m-1)\sigma_{2}^{2}}{m^{3}}.\]

_More generally, for \(k\geq 2\), \(|\,\mathbb{E}[(\delta-\mathbb{E}[\delta])^{k}]|=\mathcal{O}(m^{-\lceil k/2 \rceil})\)._

**Proof:** Define \(\hat{\delta}=\delta-\mathbb{E}[\delta]\) as the centered random variable. Now

\[\mathbb{E}[(\delta-\mathbb{E}[\delta])^{k}]=\mathbb{E}[\hat{\delta}^{k}].\]

So we focus on \(\mathbb{E}[\hat{\delta}^{k}]\) in the remainder of the proof. For \(k=2\),

\[|\,\mathbb{E}[\hat{\delta}^{2}]|=\tfrac{1}{m^{2}}|\,\mathbb{E}[\sum_{i=1}^{m} \hat{\delta}_{i}]^{2}|=\tfrac{1}{m^{2}}\left|\mathbb{E}\left[\sum_{i}\hat{ \delta}_{i}^{2}+2\sum_{i<j}\hat{\delta}_{i}\hat{\delta}_{j}\right]\right|= \tfrac{\sigma_{2}}{m}.\]

For \(k=3\),

\[|\,\mathbb{E}[\hat{\delta}^{3}]| =\tfrac{1}{m^{3}}|\,\mathbb{E}[\sum_{i=1}^{m}\hat{\delta}_{i}]^{3}|\] \[=\tfrac{1}{m^{3}}\left|\mathbb{E}\left[\sum_{i}\hat{\delta}_{i}^{ 3}+3\sum_{i\neq j}\hat{\delta}_{i}^{2}\hat{\delta}_{j}+6\sum_{i<j<k}\hat{ \delta}_{i}\hat{\delta}_{j}\hat{\delta}_{k}\right]\right|\] \[=\tfrac{\sigma_{3}}{m^{2}}.\]

For \(k=4\),

\[|\,\mathbb{E}[\hat{\delta}^{4}]| =\tfrac{1}{m^{4}}|\,\mathbb{E}[\sum_{i=1}^{m}\hat{\delta}_{i}]^{4}\] \[=\tfrac{1}{m^{4}}\left|\mathbb{E}\left[\sum_{i}\hat{\delta}_{i}^{ 4}+4\sum_{i\neq j}\hat{\delta}_{i}^{3}\hat{\delta}_{j}+6\sum_{i<j}\hat{\delta }_{i}^{2}\hat{\delta}_{j}^{2}+24\sum_{i<j<k<l}\hat{\delta}_{i}\hat{\delta}_{j }\hat{\delta}_{k}\hat{\delta}_{l}\right]\right|\] \[=\tfrac{1}{m^{3}}\left|m\,\mathbb{E}[\hat{\delta}_{i}^{4}]+6 \tfrac{m(m-1)}{2}\,\mathbb{E}[\hat{\delta}_{i}^{2}]\,\mathbb{E}[\hat{\delta }_{j}^{2}]\right|\] \[=\tfrac{\sigma_{4}}{m^{3}}+\tfrac{3(m-1)\sigma_{2}^{2}}{m^{3}}.\]

For \(k=5\),

\[|\,\mathbb{E}[\hat{\delta}^{5}]| =\tfrac{1}{m^{5}}|\,\mathbb{E}[\sum_{i=1}^{m}\hat{\delta}_{i}]^{5}\] \[=\tfrac{1}{m^{5}}\left|\mathbb{E}\left[\sum_{i}\hat{\delta}_{i}^{ 5}+10\sum_{i\neq j}\hat{\delta}_{i}^{3}\hat{\delta}_{j}^{2}\right]\right|\] \[=\tfrac{1}{m^{5}}\left|m\,\mathbb{E}[\hat{\delta}_{i}^{5}]+10m(m-1 )\,\mathbb{E}[\hat{\delta}_{i}^{3}]\,\mathbb{E}[\hat{\delta}_{j}^{2}]\right|\] \[=\tfrac{\sigma_{5}}{m^{4}}+\tfrac{10(m-1)\sigma_{3}\sigma_{2}}{m^ {4}}.\]

For general \(k>0\), we expand the following term as a function of \(m\)

\[|\,\mathbb{E}[\hat{\delta}^{k}]|=\tfrac{1}{m^{k}}|\,\mathbb{E}[\sum_{i=1}^{m} \hat{\delta}_{i}]^{k}|.\]

As \(\mathbb{E}[\hat{\delta}_{i}]=0\) and \(\hat{\delta}_{i}\) and \(\hat{\delta}_{j}\) are independent for different \(i\) and \(j\), the outcome has the following form

\[|\,\mathbb{E}[\hat{\delta}^{k}]|=\frac{1}{m^{k}}\mathcal{O}\left(\sum_{2a_{2}+ \begin{subarray}{c}3a_{3}+\cdots+ka_{k}=k\\ a_{i}\geq 0\ \forall i\end{subarray}=k}m^{\sum_{i=2}^{k}a_{i}}\sigma_{2}^{a_{2}} \sigma_{3}^{a_{3}}\cdots\sigma_{k}^{a_{k}}\right)\] (13)

where \(\sum_{i=2}^{k}a_{i}\) is the count of independent \(\{\hat{\delta}_{i}\}\) used in \(\sigma_{2}^{a_{2}}\sigma_{3}^{a_{3}}\cdots\sigma_{4}^{a_{4}}\). Among the terms in (13), the dominating one in terms of \(m\) is one with largest \(\sum_{i=2}^{k}a_{i}\), i.e.

\[|\,\mathbb{E}[\hat{\delta}^{k}]|=\begin{cases}\frac{1}{m^{k}}\mathcal{O}(m^{ k/2})\sigma_{2}^{k/2}&\text{if k even,}\\ \frac{1}{m^{k}}\mathcal{O}(m^{\lfloor k/2\rfloor})\sigma_{2}^{\lfloor k/2 \rfloor-1}\sigma_{3}&\text{if k odd.}\end{cases}\]

Then, we can simplify the upper right-hand side with

\[|\,\mathbb{E}[\hat{\delta}^{k}]|=\mathcal{O}(m^{-k+\lfloor k/2\rfloor}),\]

which gives all the desired results.

**Proposition 2** (First-order Guarantee): _Assume that \(\mathcal{D}_{m}\) and \(q(\cdot)\) satisfy Assumption 1 and 2 respectively with \(k=1\). Then, \(\forall s\in\mathbb{R}\), \(\left|\mathbb{E}\left[\mathcal{L}_{\mathcal{D}_{m}}^{(1)}q(s)\right]-q(s+ \mathbb{E}[\delta])\right|\leq a_{2}\sigma_{2}/(2m)\)._

**Proof:** Let \(h=\mathbb{E}[\delta]\). If the function \(q\in\mathcal{C}^{2}\), then the Taylor expansion at \(s+h\) with remainders leads to

\[\mathbb{E}[q(s+\delta)]=q(s+h)+q^{\prime}(s+h)\,\mathbb{E}[\delta-h]+\tfrac{1} {2}\,\mathbb{E}[q^{\prime\prime}(\phi_{1})(\delta-h)^{2}]\]

where \(\phi_{1}\) between \(s+h\) and \(s+\delta\). Then the error of extrapolation becomes

\[|\,\mathbb{E}[q(s+\delta)]-q(s+h)|=\left|\tfrac{1}{2}\,\mathbb{E}[q^{\prime \prime}(\phi_{1})(\delta-h)^{2}]\right|\leq\tfrac{a_{2}}{2}\,\mathbb{E}[( \delta-h)^{2}].\]

By Assumption 2 and Lemma 1, we have that

\[|\,\mathbb{E}[q(s+\delta)]-q(s+h)|\leq\tfrac{a_{2}}{2}\,\mathbb{E}[(\delta-h) ^{2}]=\tfrac{a_{2}}{2}\,\mathbb{E}[(\delta-h)^{2}]=\tfrac{a_{2}\sigma_{2}}{2m}.\]

This completes the proof. \(\square\)

**Proposition 1** (Second-order Guarantee): _Assume that distribution \(\mathcal{D}_{m}\) and \(q(\cdot)\) satisfies Assumption 1 and 2 respectively with \(k=2\). Then, for all \(s\in\mathbb{R}\), \(\left|\mathbb{E}\left[\mathcal{L}_{\mathcal{D}_{m}}^{(2)}q(s)\right]-q(s+ \mathbb{E}[\delta])\right|\leq\tfrac{4a_{3}\sigma_{3}+9a_{4}\sigma_{2}^{2}}{48 m^{2}}+\tfrac{5a_{4}}{96}\tfrac{\sigma_{4}-3\sigma_{2}^{2}}{m^{3}}\)._

**Proof:** Let \(h=\mathbb{E}[\delta]\). If the function \(q\in\mathcal{C}^{4}\), then the Taylor expansion at \(s+h\) with remainders leads to

\[\mathbb{E}[q(s+\delta_{1})]= q(s+h)+q^{\prime}(s+h)\,\mathbb{E}[\delta_{1}-h]+\tfrac{q^{\prime \prime}(s+h)}{2}\,\mathbb{E}[(\delta_{1}-h)^{2}]+\tfrac{q^{(3)}(s+h)}{6}\, \mathbb{E}[(\delta_{1}-h)^{3}]\] \[+\tfrac{1}{24}\,\mathbb{E}[q^{(4)}(\phi_{1})(\delta_{1}-h)^{4}]\] \[\mathbb{E}[q(s+\delta_{2})]= q(s+h)+q^{\prime}(s+h)\,\mathbb{E}[\delta_{2}-h]+\tfrac{q^{\prime \prime}(s+h)}{2}\,\mathbb{E}[(\delta_{2}-h)^{2}]+\tfrac{q^{(3)}(s+h)}{6}\, \mathbb{E}[(\delta_{2}-h)^{3}]\] \[+\tfrac{1}{24}\,\mathbb{E}[q^{(4)}(\phi_{2})(\delta_{2}-h)^{4}]\] \[\mathbb{E}[q(s+\tfrac{\delta_{1}+\delta_{2}}{2})]= q(s+h)+q^{\prime}(s+h)\,\mathbb{E}[\tfrac{\delta_{1}+\delta_{2}}{2}-h]+\tfrac{q^{ \prime\prime}(s+h)}{2}\,\mathbb{E}[(\tfrac{\delta_{1}+\delta_{2}}{2}-h)^{2}]\] \[+\tfrac{q^{(3)}(s+h)}{6}\,\mathbb{E}[\left(\tfrac{\delta_{1}+ \delta_{2}}{2}-h\right)^{3}]+\tfrac{1}{24}\,\mathbb{E}[q^{(4)}(\phi_{3})\left( \tfrac{\delta_{1}+\delta_{2}}{2}-h\right)^{4}]\]

where \(\phi_{1},\phi_{2},\phi_{3}\) between \(s+h\) and \(s+\delta_{1}\), \(s+\delta_{2}\), \(s+\delta_{3}\) respectively.

As \(\mathbb{E}[\delta-h]=0\), the error of extrapolation becomes

\[|\,\mathbb{E}[\mathcal{L}_{\mathcal{D}_{m}}^{2}q(s)]-q(s+h)|\] \[\leq \left|2\,\mathbb{E}\left[\tfrac{q^{(3)}(s+h)}{6}\left(\tfrac{ \delta_{1}+\delta_{2}}{2}-h\right)^{3}\right]-\tfrac{1}{2}\left(\mathbb{E}[ \tfrac{q^{(3)}(s+h)}{6}(\delta_{1}-h)^{3}]+\mathbb{E}[\tfrac{q^{(3)}(s+h)}{6} (\delta_{2}-h)^{3}]\right)\right|\] \[+\left|2\,\mathbb{E}\left[\tfrac{q^{(4)}(\phi_{3})}{24}\left( \tfrac{\delta_{1}+\delta_{2}}{2}-h\right)^{4}\right]-\tfrac{1}{2}\left( \mathbb{E}[\tfrac{q^{(4)}(\phi_{1})}{24}(\delta_{1}-h)^{4}]+\mathbb{E}[ \tfrac{q^{(4)}(\phi_{2})}{24}(\delta_{2}-h)^{4}]\right)\right|\] \[\leq \tfrac{a_{3}}{6}\left|2\,\mathbb{E}\left[\left(\tfrac{\delta_{1}+ \delta_{2}}{2}-h\right)^{3}\right]-\tfrac{1}{2}\left(\mathbb{E}[(\delta_{1}-h)^{ 3}]+\mathbb{E}[(\delta_{2}-h)^{3}]\right)\right|\] \[+\left|2\,\mathbb{E}\left[\tfrac{q^{(4)}(\phi_{3})}{24}\left( \tfrac{\delta_{1}+\delta_{2}}{2}-h\right)^{4}\right]-\tfrac{1}{2}\left( \mathbb{E}[\tfrac{q^{(4)}(\phi_{1})}{24}(\delta_{1}-h)^{4}]+\mathbb{E}[ \tfrac{q^{(4)}(\phi_{2})}{24}(\delta_{2}-h)^{4}]\right)\right|\] \[\leq \tfrac{a_{3}}{6}\left|2\,\mathbb{E}\left[\left(\tfrac{\delta_{1}+ \delta_{2}}{2}-h\right)^{3}\right]-\tfrac{1}{2}\left(\mathbb{E}[(\delta_{1}-h)^{ 3}]+\mathbb{E}[(\delta_{2}-h)^{3}]\right)\right|\] \[+\tfrac{a_{4}}{24}\left|2\,\mathbb{E}\left[\left(\tfrac{\delta_{1}+ \delta_{2}}{2}-h\right)^{4}\right]+\tfrac{1}{2}\left(\mathbb{E}[(\delta_{1}-h)^{ 4}]+\mathbb{E}[(\delta_{2}-h)^{4}]\right)\right|.\]

where the second inequality uses the upper bound on \(q^{(3)}(\cdot)\) (Assumption 2) and the third inequality uses \((\delta-h)^{4}\) is non-negative and the last inequality uses the uniform bound on \(q^{(4)}(\cdot)\) (Assumption 2).

Then

\[|\,\mathbb{E}[\mathcal{L}_{\mathcal{D}_{m}}^{2}q(s)]-q(s+h)|\] \[\leq \tfrac{a_{3}}{12}|\,\mathbb{E}(\delta_{1}-h)^{3}|+\tfrac{a_{4}}{24} \left(2\,\mathbb{E}\left(\tfrac{\delta_{1}+\delta_{2}}{2}-h\right)^{4}+\mathbb{ E}(\delta_{1}-h)^{4}\right)\] \[\leq \tfrac{a_{3}\sigma_{3}}{12m^{2}}+\tfrac{a_{4}}{24}\left(\tfrac{ \sigma_{4}}{4m^{3}}+\tfrac{3(2m-1)\sigma_{2}^{2}}{4m^{3}}+\tfrac{\sigma_{4}}{ m^{3}}+\tfrac{3(m-1)\sigma_{2}^{2}}{m^{3}}\right)\] \[\leq \tfrac{a_{3}\sigma_{3}}{12m^{2}}+\tfrac{a_{4}}{24}\left(\tfrac{9 \sigma_{2}^{2}}{2m^{2}}+\tfrac{5(\sigma_{4}-3\sigma_{2}^{2})}{4m^{3}}\right)\] \[\leq \tfrac{4a_{3}\sigma_{3}+9a_{4}\sigma_{2}^{2}}{48m^{2}}+\tfrac{5a _{4}}{96}\tfrac{\sigma_{4}-3\sigma_{2}^{2}}{m^{3}}.\]

we first use that \(\mathbb{E}[(\delta_{1}-h)^{3}]=\mathbb{E}[(\delta_{2}-h)^{3}]=4\,\mathbb{E}[( \tfrac{\delta_{1}+\delta_{2}}{2}-h)^{3}]\) and the uses the bound on moments in Lemma 1. Note that \(\mathbb{E}\left(\tfrac{\delta_{1}+\delta_{2}}{2}-h\right)^{4}\) can be seen as the 4th order moments of a batch size of \(2m\).

**Proposition 3**: _Assume \(q\in\mathcal{C}^{6}\). Then \(\mathcal{L}_{\mathcal{D}_{m}}^{(3)}\) as defined below is a third-order extrapolation operator._

\[\mathcal{L}_{\mathcal{D}_{m}}^{(3)}q:s\mapsto(-\tfrac{1}{36}\mathcal{L}_{ \mathcal{D}_{m}}^{(2)}+\tfrac{5}{9}\mathcal{L}_{\mathcal{D}_{2m}}^{(2)}- \tfrac{3}{4}\mathcal{L}_{\mathcal{D}_{3m}}^{(2)}-\tfrac{16}{9}\mathcal{L}_{ \mathcal{D}_{4m}}^{(2)}+3\mathcal{L}_{\mathcal{D}_{6m}}^{(2)})q(s).\]

**Proof:** Let \(h=\mathbb{E}[\delta]\). If \(q\in\mathcal{C}^{2k}\), then \(q\) has the following Taylor expansion

\[\mathbb{E}[q(s+\delta)]= \underbrace{q(s+h)}_{\text{zero order term}}+q^{\prime}(s+h)\, \mathbb{E}[\delta-h]+\underbrace{\tfrac{q^{\prime\prime}(s+h)}{2}\,\mathbb{E} [(\delta-h)^{2}]}_{\text{second order term}}+\ldots\] \[+\tfrac{q^{(2k-1)}(s+h)}{(2k-1)!}\,\mathbb{E}[(\delta-h)^{2k-1}] +\tfrac{1}{2k!}\,\mathbb{E}[q^{(2k)}(\phi)(\delta-h)^{2k}].\]

**Eliminate the third order term in the Taylor expansion.** Consider the following affine combination which

\[\mathcal{F}_{\mathcal{D}_{m}}^{(3)}q:s\mapsto\alpha_{1}\mathcal{L}_{\mathcal{ D}_{m}}^{(2)}q(s)+\alpha_{2}\mathcal{L}_{\mathcal{D}_{2m}}^{(2)}q\left(s \right).\]

We determine \(\alpha_{1}\) and \(\alpha_{2}\) by expanding \(\mathcal{L}_{\mathcal{D}_{m}}^{(2)}q(s)\) and \(\mathcal{L}_{\mathcal{D}_{2m}}^{(2)}q(s)\) and analyze the coefficients of terms:

* **(Affine).** Taylor expansion of \(\mathcal{F}_{\mathcal{D}_{m}}^{(3)}q(s)\) at \(s+h\) should have zero order term \(q(s+h)\), i.e. \[\alpha_{1}q(s+h)+\alpha_{2}q(s+h)=q(s+h).\]
* **(Eliminate third term).** Taylor expansion of \(\mathcal{F}_{\mathcal{D}_{m}}^{(3)}q(s)\) at \(s+h\) should have third order term \(\mathbb{E}[(\delta-h)^{3}]\). That is, \[\alpha_{1}\,\mathbb{E}[(\delta_{1}-h)^{3}]+\alpha_{2}\,\mathbb{E}\left[\left( \tfrac{\delta_{1}+\delta_{2}}{2}-h\right)^{3}\right]=0.\] This is equivalent to \[\alpha_{1}\,\mathbb{E}[(\delta_{1}-h)^{3}]+\tfrac{\alpha_{2}}{4}\,\mathbb{E} \left[(\delta_{1}-h)^{3}\right]=0.\]

Therefore, \(\alpha_{1}\) and \(\alpha_{2}\) can be determined through the following linear system

\[\alpha_{1}+\alpha_{2}=1\] \[\alpha_{1}+\tfrac{1}{4}\alpha_{2}=0.\]

The solution is \(\alpha_{1}=-\tfrac{1}{3}\) and \(\alpha_{2}=\tfrac{4}{3}\).

**For \(k=3\) order extrapolation,** consider the following

\[\mathcal{L}_{\mathcal{D}_{m}}^{(3)}q:s\mapsto\alpha_{1}^{\prime}\mathcal{F}_{ \mathcal{D}_{m}}^{(3)}q(s)+\alpha_{2}^{\prime}\mathcal{F}_{\mathcal{D}_{2m}}^{(3 )}q\left(s\right)+\alpha_{3}^{\prime}\mathcal{F}_{\mathcal{D}_{3m}}^{(3)}q \left(s\right).\]

We determine \(\alpha_{1}^{\prime}\), \(\alpha_{2}^{\prime}\) and \(\alpha_{3}^{\prime}\) by satisfying the following two conditions

* **(Affine).** Taylor expansion of \(\mathcal{L}_{\mathcal{D}_{m}}^{(3)}q(s)\) at \(s+h\) should have zero order term \(q(x+h)\), i.e. \[(\alpha_{1}^{\prime}+\alpha_{2}^{\prime}+\alpha_{3}^{\prime})q(x+h)=q(x+h).\]* Taylor expansion of \(\mathcal{L}^{(3)}_{\mathcal{D}_{m}}q(s)\) at \(s+h\) should have 4th order term \(\mathbb{E}[(\delta-h)^{4}]\). That is \[\alpha_{1}^{\prime}\,\mathbb{E}[(\delta_{1}-h)^{4}]+\alpha_{2}^{\prime}\, \mathbb{E}\left[\left(\tfrac{\delta_{1}+\delta_{2}}{2}-h\right)^{4}\right]+ \alpha_{3}^{\prime}\,\mathbb{E}\left[\left(\tfrac{\delta_{1}+\delta_{2}+\delta _{3}}{3}-h\right)^{4}\right]=0.\] This is equivalent to \[\left(\alpha_{1}^{\prime}+\tfrac{\alpha_{2}^{\prime}}{8}+\tfrac{ \alpha_{3}^{\prime}}{27}\right)\mathbb{E}[(\delta_{1}-h)^{4}] =0\] \[\left(\tfrac{3}{8}\alpha_{2}^{\prime}+\tfrac{2}{9}\alpha_{2}^{ \prime}\right)\left(\mathbb{E}[(\delta_{1}-h)^{2}]\right)^{2} =0.\] Therefore, \(\alpha_{1}^{\prime}\), \(\alpha_{2}^{\prime}\) and \(\alpha_{3}^{\prime}\) can be determined through the following linear system \[\alpha_{1}^{\prime}+\alpha_{2}^{\prime}+\alpha_{3}^{\prime} =1\] \[\alpha_{1}^{\prime}+\tfrac{1}{8}\alpha_{2}^{\prime}+\tfrac{1}{27 }\alpha_{3}^{\prime} =0\] \[\alpha_{1}^{\prime}+\tfrac{3}{8}\alpha_{2}^{\prime}+\tfrac{2}{9} \alpha_{3}^{\prime} =0.\]

The solution is \(\alpha_{1}^{\prime}=\tfrac{1}{12}\), \(\alpha_{2}^{\prime}=-\tfrac{4}{3}\) and \(\alpha_{3}^{\prime}=\tfrac{9}{4}\). Then consider the Taylor expansion of \(\mathcal{L}^{(3)}_{\mathcal{D}_{m}}q(s)\) at \(s+h\) with (2), we can

\[|\,\mathbb{E}[\mathcal{L}^{(3)}_{\mathcal{D}_{m}}q(s)]-q(s+h)|\lesssim\left|q^ {(5)}(s+h)\,\mathbb{E}[(\delta-h)^{5}]\right|+\left|\mathbb{E}[q^{(6)}(\phi_{ \delta})(\delta-h)^{6}]\right|\lesssim\mathcal{O}((a_{5}+a_{6})m^{-3})\]

where the first inequality uses the fact that \(\mathcal{L}^{(3)}_{\mathcal{D}_{m}}\) is an affine mapping and the last inequality uses Lemma 1. Therefore, \(\mathcal{L}^{(3)}_{\mathcal{D}_{m}}\) is a 3rd-order extrapolation operator. We can expand it into

\[\mathcal{L}^{(3)}_{\mathcal{D}_{m}}q:s \mapsto\tfrac{1}{12}\left(-\tfrac{1}{3}\mathcal{L}^{(2)}_{ \mathcal{D}_{m}}q(s)+\tfrac{4}{3}\mathcal{L}^{(2)}_{\mathcal{D}_{2m}}q\left(s \right)\right)-\tfrac{4}{3}\left(-\tfrac{1}{3}\mathcal{L}^{(2)}_{\mathcal{D}_{2 m}}q(s)+\tfrac{4}{3}\mathcal{L}^{(2)}_{\mathcal{D}_{4m}}q\left(s\right)\right)\] \[\qquad+\tfrac{9}{4}\left(-\tfrac{1}{3}\mathcal{L}^{(2)}_{ \mathcal{D}_{3m}}q(s)+\tfrac{4}{3}\mathcal{L}^{(2)}_{\mathcal{D}_{6m}}q\left(s \right)\right)\] \[=(-\tfrac{1}{36}\mathcal{L}^{(2)}_{\mathcal{D}_{m}}+\tfrac{5}{9} \mathcal{L}^{(2)}_{\mathcal{D}_{2m}}-\tfrac{3}{4}\mathcal{L}^{(2)}_{\mathcal{ D}_{3m}}-\tfrac{16}{9}\mathcal{L}^{(2)}_{\mathcal{D}_{4m}}+3\mathcal{L}^{(2)}_{ \mathcal{D}_{6m}})q(s).\]

\(\square\)

**Lemma 2** (Variance Bound): _Assume that \(q:\mathbb{R}^{p}\to\mathbb{R}^{\ell}\) is in \(\mathcal{C}^{4}\) and \(\mathcal{D}_{m}\) is the distribution in Assumption 1. Suppose that the variance of \(q(\boldsymbol{s}+\boldsymbol{\delta})\) is bounded as_

\[\mathbb{E}[\left\|q(\boldsymbol{s}+\boldsymbol{\delta})-\mathbb{E}[q( \boldsymbol{s}+\boldsymbol{\delta})]\right\|_{2}^{2}]\leq\tfrac{V^{2}}{m}+C.\]

_Then the variance of extrapolation \(\mathcal{L}^{(2)}_{\mathcal{D}_{m}}q(\boldsymbol{s})\) is upper bounded by_

\[\mathbb{E}\left[\left\|\mathcal{L}^{(2)}_{\mathcal{D}_{m}}q(\boldsymbol{s})- \mathbb{E}[\mathcal{L}^{(2)}_{\mathcal{D}_{m}}q(\boldsymbol{s})]\right\|_{2} ^{2}\right]\leq 14(\tfrac{V^{2}}{m}+C).\]

**Proof:** Let us use the definition of \(\mathcal{L}^{(2)}_{\mathcal{D}_{m}}q(\boldsymbol{s})\):

\[\mathbb{E}\left[\left\|\mathcal{L}^{(2)}_{\mathcal{D}_{m}}q( \boldsymbol{s})-\mathbb{E}[\mathcal{L}^{(2)}_{\mathcal{D}_{m}}q(\boldsymbol{s})] \right\|_{2}^{2}\right]\] \[\leq\mathbb{E}\left[\left\|2q(\boldsymbol{s}+\tfrac{\boldsymbol{ \delta}_{1}+\boldsymbol{\delta}_{2}}{2})-\tfrac{q(\boldsymbol{s}+\boldsymbol{ \delta}_{1})+q(\boldsymbol{s}+\boldsymbol{\delta}_{2})}{2}-\mathbb{E}\left[2q( \boldsymbol{s}+\tfrac{\boldsymbol{\delta}_{1}+\boldsymbol{\delta}_{2}}{2})- \tfrac{q(\boldsymbol{s}+\boldsymbol{\delta}_{1})+q(\boldsymbol{s}+\boldsymbol{ \delta}_{2})}{2}\right]\right\|_{2}^{2}\right]\] \[\leq 3\,\mathbb{E}\left[\left\|2q(\boldsymbol{s}+\tfrac{\boldsymbol{ \delta}_{1}+\boldsymbol{\delta}_{2}}{2})-\mathbb{E}\left[2q(\boldsymbol{s}+ \tfrac{\boldsymbol{\delta}_{1}+\boldsymbol{\delta}_{2}}{2})\right]\right\|_{2}^ {2}\right]+3\,\mathbb{E}\left[\left\|\tfrac{q(\boldsymbol{s}+\boldsymbol{ \delta}_{1})}{2}-\mathbb{E}\left[\tfrac{q(\boldsymbol{s}+\boldsymbol{\delta}_{1}) }{2}\right]\right\|_{2}^{2}\right]\] \[+3\,\mathbb{E}\big{[}\Big{\|}\tfrac{q(\boldsymbol{s}+\boldsymbol{ \delta}_{2})}{2}-\mathbb{E}\left[\tfrac{q(\boldsymbol{s}+\boldsymbol{\delta}_{2}) }{2}\right]\Big{\|}_{2}^{2}\big{]}\] \[\leq 12(\tfrac{V^{2}}{2m}+C)+\tfrac{3}{4}(\tfrac{V^{2}}{m}+C)+ \tfrac{3}{4}(\tfrac{V^{2}}{m}+C)\] \[=\tfrac{15V^{2}}{2m}+\tfrac{27C}{2}.\]

This completes the proof. \(\square\)Stationary Point Convergence Proofs from Section 3 (CSO)

In this section, we provide the convergence proofs for the CSO problem. We start by establishing some helpful lemmas in Appendix D.1. In Appendix D.2, we reanalyze the BSGD algorithm to obtain explicit bias and variance bounds, which are then useful when we analyze E-BSGD in Appendix D.3. Similarly, we reanalyze BSpiderBoost in Appendix D.4 and use the resulting bias and variance bounds for the analysis of E-BSpiderBoost in Appendix D.5.

Note that throughout our analyses, we define \(\mathbb{E}^{t+1}[\cdot|t]\) as the expectation of randomness at time \(t+1\) conditioning on the randomness until time \(t\). When there is no ambiguity, we use \(\mathbb{E}[\cdot]\) instead of \(\mathbb{E}^{t+1}[\cdot|t]\).

### Helpful Lemmas

**Lemma 3** (Sufficient Decrease): _Suppose Assumption 5 holds true and \(\gamma\leq\frac{1}{2L_{F}}\) then_

\[\left\|\nabla F(\bm{x}^{t})\right\|_{2}^{2}\leq\frac{2(\mathbb{E}[F(\bm{x}^{t +1})]-F(\bm{x}^{t}))}{\gamma}+L_{F}\gamma\mathcal{E}_{\text{var}}^{t+1}+ \mathcal{E}_{\text{bias}}^{t+1},\]

_where \(\mathbb{E}[\cdot]\) denote conditional expectation over the randomness at time \(t\) conditioned on all of the past randomness until time \(t\)._

**Proof:** In this proof, we use \(\mathbb{E}[\cdot]\) to denote conditional expectation over the randomness at time \(t\) conditioned on all the past randomness until time \(t\).

Let us expand \(F(\bm{x}^{t+1})\) and apply the \(L_{F}\)-smoothness of \(F\)

\[\mathbb{E}[F(\bm{x}^{t+1})]\leq F(\bm{x}^{t})-\gamma\,\mathbb{E}[\langle \nabla F(\bm{x}^{t}),G^{t+1}\rangle]+\tfrac{L_{F}\gamma^{2}}{2}\,\mathbb{E}[ \left\|G^{t+1}\right\|_{2}^{2}].\]

Since \(\mathbb{E}[\left\|G^{t+1}\right\|_{2}^{2}]=\mathbb{E}[\left\|G^{t+1}-\mathbb{ E}[G^{t+1}]\right\|_{2}^{2}]+\left\|\mathbb{E}[G^{t+1}]\right\|_{2}^{2}= \mathcal{E}_{\text{var}}^{t+1}+\left\|\mathbb{E}[G^{t+1}]\right\|_{2}^{2}\), then

\[\mathbb{E}[F(\bm{x}^{t+1})]\leq F(\bm{x}^{t})-\gamma\,\mathbb{E}[\langle \nabla F(\bm{x}^{t}),G^{t+1}\rangle]+\tfrac{L_{F}\gamma^{2}}{2}(\mathcal{E}_{ \text{var}}^{t+1}+\left\|\mathbb{E}[G^{t+1}]\right\|_{2}^{2}).\]

Expand the middle term with

\[-\gamma\,\mathbb{E}[\langle\nabla F(\bm{x}^{t}),G^{t+1}\rangle] =-\tfrac{\gamma}{2}\left\|\nabla F(\bm{x}^{t})\right\|_{2}^{2}- \tfrac{\gamma}{2}\left\|\mathbb{E}[G^{t+1}]\right\|_{2}^{2}+\tfrac{\gamma}{2} \left\|\nabla F(\bm{x}^{t})-\mathbb{E}[G^{t+1}]\right\|_{2}^{2}\] \[=-\tfrac{\gamma}{2}\left\|\nabla F(\bm{x}^{t})\right\|_{2}^{2}- \tfrac{\gamma}{2}\left\|\mathbb{E}[G^{t+1}]\right\|_{2}^{2}+\tfrac{\gamma}{2} \mathcal{E}_{\text{bias}}^{t+1}.\]

Combine with the inequality

\[\mathbb{E}[F(\bm{x}^{t+1})]\leq F(\bm{x}^{t})-\tfrac{\gamma}{2}\left\|\nabla F (\bm{x}^{t})\right\|_{2}^{2}-\tfrac{\gamma}{2}(1-L_{F}\gamma)\left\|\mathbb{E }[G^{t+1}]\right\|_{2}^{2}+\tfrac{\gamma}{2}\mathcal{E}_{\text{bias}}^{t+1}+ \tfrac{L_{F}\gamma^{2}}{2}\mathcal{E}_{\text{var}}^{t+1}.\]

By taking \(\gamma\leq\frac{1}{2L_{F}}\), we have that

\[\mathbb{E}[F(\bm{x}^{t+1})]\leq F(\bm{x}^{t})-\tfrac{\gamma}{2}\left\|\nabla F (\bm{x}^{t})\right\|_{2}^{2}-\tfrac{\gamma}{4}\left\|\mathbb{E}[G^{t+1}] \right\|_{2}^{2}+\tfrac{\gamma}{2}\mathcal{E}_{\text{bias}}^{t+1}+\tfrac{L_{F} \gamma^{2}}{2}\mathcal{E}_{\text{var}}^{t+1}.\]

Re-arranging the terms we get the desired inequality. 

A consequence of Lemma 3 is the following result.

**Lemma 4** (Descent Lemma): _Suppose Assumption 5 holds true. By taking \(\gamma\leq\frac{1}{2L_{F}}\), we have,_

\[\tfrac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}\left[\left\|\nabla F(\bm{ x}^{t})\right\|_{2}^{2}\right]+\tfrac{1}{2}\tfrac{1}{T}\sum_{t=0}^{T-1} \mathbb{E}\left[\left\|\mathbb{E}^{t}[G^{t+1}|t]\right\|_{2}^{2}\right]\\ \leq\tfrac{2(F(\bm{x}^{0})-F^{*})}{\gamma T}+\tfrac{1}{T}\sum_{t =0}^{T-1}\mathbb{E}[\mathcal{E}_{\text{bias}}^{t+1}]+\tfrac{L_{F}\gamma}{T} \sum_{t=0}^{T-1}\mathbb{E}[\mathcal{E}_{\text{var}}^{t+1}]\]

_where the expectation is taken over all randomness from \(t=0\) to \(T\)._

**Proof:** We denote the conditional expectation at time \(t\) in the descent lemma (Lemma 3) as \(\mathbb{E}^{t+1}[\cdot|t]\) which conditions on all past randomness until time \(t\). Then the descent lemma can be written as

\[\mathbb{E}^{t+1}[F(\bm{x}^{t+1})|t]\leq F(\bm{x}^{t})-\tfrac{\gamma}{2}\left\| \nabla F(\bm{x}^{t})\right\|_{2}^{2}-\tfrac{\gamma}{4}\left\|\mathbb{E}^{t+1 }[G^{t+1}|t]\right\|_{2}^{2}+\tfrac{\gamma}{2}\,\mathbb{E}^{t+1}[\mathcal{E}_{ \text{bias}}^{t+1}|t]+\tfrac{L_{F}\gamma^{2}}{2}\,\mathbb{E}^{t+1}[\mathcal{E }_{\text{var}}^{t+1}|t].\]If we additionally consider the randomness at time \(t-1\), and apply \(\mathbb{E}^{t}[\cdot|t-1]\) to both sides

\[\mathbb{E}^{t}\left[\mathbb{E}^{t+1}[F(\bm{x}^{t+1})|t|]t-1\right]\leq \,\mathbb{E}^{t}[F(\bm{x}^{t})|t-1]-\tfrac{\gamma}{2}\,\mathbb{E} [\big{\|}\nabla F(\bm{x}^{t})\big{\|}_{2}^{2}\,|t-1]\] \[-\mathbb{E}^{t}\left[\tfrac{\gamma}{4}\,\big{\|}\mathbb{E}^{t+1}[ G^{t+1}|t|\big{\|}_{2}^{2}\,|t-1\right]+\tfrac{\gamma}{2}\,\mathbb{E}^{t}\left[ \mathbb{E}^{t+1}[\mathcal{E}^{t+1}_{\text{bias}}|t|]t-1\right]\] \[+\tfrac{L_{F}\gamma^{2}}{2}\,\mathbb{E}^{t-1}\left[\mathbb{E}^{t+ 1}[\mathcal{E}^{t+1}_{\text{var}}]|t|-1\right].\]

By the law of iterative expectations, we have \(\mathbb{E}^{t}\left[\mathbb{E}^{t+1}[\cdot|t]|t-1\right]=\mathbb{E}^{t}\, \mathbb{E}^{t+1}\left[\cdot|t-1\right]\)

\[\mathbb{E}^{t}[\mathbb{E}^{t+1}\left[F(\bm{x}^{t+1})|t-1\right]]\leq \,\mathbb{E}^{t}[F(\bm{x}^{t})|t-1]-\tfrac{\gamma}{2}\,\mathbb{E} [\big{\|}\nabla F(\bm{x}^{t})\big{\|}_{2}^{2}\,|t-1]\] \[-\mathbb{E}^{t}\left[\tfrac{\gamma}{4}\,\big{\|}\mathbb{E}^{t+1}[ G^{t+1}|t|\big{\|}_{2}^{2}\,|t-1\right]+\tfrac{\gamma}{2}\,\mathbb{E}^{t}\left[ \mathbb{E}^{t+1}\left[\mathcal{E}^{t+1}_{\text{bias}}|t-1\right]\right]\] \[+\tfrac{L_{F}\gamma^{2}}{2}\,\mathbb{E}^{t}[\mathbb{E}^{t}\, \big{[}\mathcal{E}^{t+1}_{\text{var}}|t-1\big{]}].\]

Similarly, we can apply \(\mathbb{E}^{t-1}[\cdot|t-2]\), \(\mathbb{E}^{t-2}[\cdot|t-3]\), \(\ldots\), \(\mathbb{E}^{2}[\cdot|1]\) and finally \(\mathbb{E}^{1}[\cdot]\)

\[\mathbb{E}^{1}\ldots[\mathbb{E}^{t+1}\left[F(\bm{x}^{t+1})\right]]\leq \mathbb{E}^{1}\ldots[\mathbb{E}^{t}[F(\bm{x}^{t})]]-\tfrac{\gamma}{2}\, \mathbb{E}^{1}\ldots[\mathbb{E}^{t}[\big{\|}\nabla F(\bm{x}^{t})\big{\|}_{2}^ {2}]]\] \[-\mathbb{E}^{1}\ldots[\mathbb{E}^{t}[\tfrac{\gamma}{4}\,\big{\|} \mathbb{E}^{t+1}[G^{t+1}|t|]\big{\|}_{2}^{2}]]+\tfrac{\gamma}{2}\,\mathbb{E} ^{1}\ldots[\mathbb{E}^{t+1}\left[\mathcal{E}^{t+1}_{\text{bias}}\right]]\] \[+\tfrac{L_{F}\gamma^{2}}{2}\,\mathbb{E}^{1}\ldots[\mathbb{E}^{t} \left[\mathcal{E}^{t+1}_{\text{var}}\right]].\]

Now that both sides of the inequality have no randomness, we can simplify the notation by applying \(\mathbb{E}^{t+1}\ldots[\mathbb{E}^{t}[\cdot]]\) to both sides and by denoting

\[\mathbb{E}[\cdot]=\mathbb{E}^{1}\ldots[\mathbb{E}^{+1}[\cdot]].\]

Then the descent lemma becomes

Now we can sum the descent lemmas from \(t=0\) to \(T-1\)

\(\sum_{t=0}^{T-1}\mathbb{E}[F(\bm{x}^{t+1})]\leq\sum_{t=0}^{T-1}\mathbb{E}[F(\bm {x}^{t})]-\tfrac{\gamma}{2}\sum_{t=0}^{T-1}\mathbb{E}\left[\big{\|}\nabla F(\bm {x}^{t})\big{\|}_{2}^{2}\right]\)

\[-\tfrac{\gamma}{4}\sum_{t=0}^{T-1}\mathbb{E}\left[\big{\|}\mathbb{E}^{t+1}[G^{ t+1}|t|]\big{\|}_{2}^{2}\right]+\tfrac{\gamma}{2}\sum_{t=0}^{T-1}\mathbb{E}[ \mathcal{E}^{t+1}_{\text{bias}}]+\tfrac{L_{F}\gamma^{2}}{2}\sum_{t=0}^{T-1} \mathbb{E}[\mathcal{E}^{t+1}_{\text{var}}].\]

After simplification and division by \(T\), we get

\[\tfrac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}\left[\big{\|}\nabla F(\bm {x}^{t})\big{\|}_{2}^{2}\right]+\tfrac{1}{2}\tfrac{1}{T}\sum_{t=0}^{T-1} \mathbb{E}\left[\big{\|}\mathbb{E}^{t+1}[G^{t+1}|t]\big{\|}_{2}^{2}\right]\] \[\leq\tfrac{2(\mathbb{E}[F(\bm{x}^{T})]-\mathbb{E}[F(\bm{x}^{0}) ])}{\gamma T}+\tfrac{\gamma}{2}\tfrac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}[ \mathcal{E}^{t+1}_{\text{bias}}]+\tfrac{L_{F}\gamma^{2}}{2}\tfrac{1}{T}\sum_{t =0}^{T-1}\mathbb{E}[\mathcal{E}^{t+1}_{\text{var}}]\] \[\leq\tfrac{2(\mathbb{E}[F(\bm{x}^{T})]-F^{*})}{\gamma T}+\tfrac{1} {T}\sum_{t=0}^{T-1}\mathbb{E}[\mathcal{E}^{t+1}_{\text{bias}}]+\tfrac{L_{F} \gamma}{T}\sum_{t=0}^{T-1}\mathbb{E}[\mathcal{E}^{t+1}_{\text{var}}].\]

The following corollary is a consequence of Proposition 1.

**Corollary 1**: _Assume \(\nabla f_{\xi}\) in CSO satisfies_

\[a_{l}:=\sup_{\bm{x}}\sup_{\xi}\left\|\nabla^{l+1}f_{\xi}(\bm{x})\right\|_{2}< \infty,\qquad l=1,2,3,4.\]

_Let's further assume that the higher order moments of \(g_{\eta}(\cdot)\) are bounded,_

\[\sigma_{k}=\sup_{\bm{x}}\sup_{\xi}\mathbb{E}_{\eta|\xi}\left[\sum_{i=1}^{p} \left[g_{\eta}(\bm{x})-\mathbb{E}_{\eta|\xi}[g_{\eta}(\bm{x})]\right]_{i}^{k} \right]<\infty,\qquad k=1,2,3,4\]

_where \([\cdot]_{i}\) refers to the \(i\)-th coordinate of a vector. Consider the \(\mathcal{L}^{(2)}_{\mathcal{D}^{t+1}_{\bm{g},\xi}}\nabla f_{\xi}(0)\) defined in (6), then_

\[\left\|\mathbb{E}\left[\mathcal{L}^{(2)}_{\mathcal{D}^{t+1}_{\bm{g},\xi}}\nabla f _{\xi}(0)\right]-\nabla f_{\xi}(\mathbb{E}_{\eta|\xi}[g_{\eta}(\bm{x}^{t})]) \right\|_{2}^{2}\leq\tfrac{C_{e}^{2}}{m^{4}}\qquad\forall\xi,\]

_where \(C_{e}^{2}(f;g):=\left(\tfrac{8a_{3}\sigma_{3}+18a_{4}\sigma_{2}^{2}+5a_{4} \sigma_{4}}{96}\right)^{2}\)._

**Proof:** The Proposition 1 gives the following upper bound

\[\left\|\mathbb{E}\left[\mathcal{L}^{(2)}_{\mathcal{D}^{t+1}_{g,\xi}}\nabla f_{\xi} (0)\right]-\nabla f_{\xi}(\mathbb{E}_{\eta|\xi}[g_{\eta}(\bm{x}^{t})])\right\| _{2}^{2}\leq\left(\frac{4a_{3}\sigma_{3}+9a_{4}\sigma_{2}^{2}}{48m^{2}}+\frac{5a _{4}}{96}\frac{\sigma_{4}-3\sigma_{2}^{2}}{m^{3}}\right)^{2}.\]

For simplicity, we can relax the upper bound to

\[\left\|\mathbb{E}\left[\mathcal{L}^{(2)}_{\mathcal{D}^{t+1}_{g,\xi}}\nabla f_{ \xi}(0)\right]-\nabla f_{\xi}(\mathbb{E}_{\eta|\xi}[g_{\eta}(\bm{x}^{t})]) \right\|_{2}^{2}\leq\tfrac{1}{m^{4}}\left(\frac{8a_{3}\sigma_{3}+18a_{4}\sigma _{2}^{2}+5a_{4}\sigma_{4}}{96}\right)^{2}.\]

\(\square\)

### Convergence of BSGD

In this section, we reanalyze the BSGD algorithm of [16] to obtain bounds on bias and variance of its gradient estimates. Theorem 6 shows that BSGD achieves an \(\mathcal{O}(\varepsilon^{-6})\) sample complexity.

**Lemma 5** (Bias and Variance of BSGD): _The bias and variance of BSGD are_

\[\mathcal{E}^{t+1}_{\text{bias}}\leq\tfrac{\sigma_{\text{max}}^{2}}{m},\quad \mathcal{E}^{t+1}_{\text{var}}\leq\tfrac{\sigma_{\text{m}}^{2}}{m}+\sigma_{ \text{out}}^{2}\]

_where \(\sigma_{\text{in}}^{2}=\zeta_{g}^{2}C_{f}^{2}+\sigma_{g}^{2}C_{g}^{2}L_{f}^{2}\), \(\sigma_{\text{out}}^{2}=C_{F}^{2}\), and \(\sigma_{\text{bias}}^{2}=\sigma_{g}^{2}C_{g}^{2}L_{f}^{2}\)._

**Proof:** Denote \(G^{t+1}=G^{t+1}_{\text{BSGD}}\) (5) and denote \(\mathbb{E}[\cdot]\) as the conditional expectation \(\mathbb{E}^{t+1}[\cdot|t]\) which conditions on all past randomness until time \(t\). Note that the \(\nabla g_{\bar{\eta}}\) can be estimated without bias, i.e.

\[\mathbb{E}_{\bar{\eta}|\xi}\left[\tfrac{1}{m}\sum_{\bar{\eta}\in\bar{H}_{\xi} }\nabla g_{\bar{\eta}}(\bm{x})\right]=\mathbb{E}_{\bar{\eta}|\xi}\left[\nabla g _{\bar{\eta}}(\bm{x})\right],\]

Then let's first look at the bias of BSGD

\[\mathcal{E}^{t+1}_{\text{bias}} =\left\|\nabla F(\bm{x}^{t+1})-\mathbb{E}[G^{t+1}]\right\|_{2}^{2}\] \[=\left\|\mathbb{E}_{\xi}\left[(\mathbb{E}_{\bar{\eta}|\xi}[\nabla g _{\bar{\eta}}(\bm{x}^{t})])^{\top}\left(\nabla f_{\xi}(\mathbb{E}_{\eta|\xi}[ g_{\eta}(\bm{x}^{t})])-\mathbb{E}_{\eta|\xi}[\nabla f_{\xi}(\tfrac{1}{m}\sum_{ \eta\in H_{\xi}}g_{\eta}(\bm{x}^{t}))]\right)\right]\right\|_{2}^{2}\] \[\leq C_{g}^{2}\,\mathbb{E}_{\xi}\left[\left\|\nabla f_{\xi}( \mathbb{E}_{\eta|\xi}[g_{\eta}(\bm{x}^{t})])-\mathbb{E}_{\eta|\xi}[\nabla f_{ \xi}(\tfrac{1}{m}\sum_{\eta\in H_{\xi}}g_{\eta}(\bm{x}^{t}))]\right\|_{2}^{2}\right]\] \[\leq C_{g}^{2}L_{f}^{2}\,\mathbb{E}_{\xi}\left[\mathbb{E}_{\eta| \xi}\left[\left\|\mathbb{E}_{\eta|\xi}[g_{\eta}(\bm{x}^{t})]-\tfrac{1}{m} \sum_{\eta\in H_{\xi}}g_{\eta}(\bm{x}^{t})\right\|_{2}^{2}\right]\right]\] \[\leq\tfrac{C_{g}^{2}L_{f}^{2}}{m}\,\mathbb{E}_{\xi}\left[\mathbb{ E}_{\bar{\eta}|\xi}\left[\left\|\mathbb{E}_{\eta|\xi}[g_{\eta}(\bm{x}^{t})]-g_{\eta}( \bm{x}^{t})\right\|_{2}^{2}\right]\right]\] \[=\tfrac{\sigma_{g}^{2}C_{g}^{2}L_{f}^{2}}{m}=\tfrac{\sigma_{\text{ in}}^{2}}{m}.\]

For the first inequality, we take the expectation outside the norm and bound \(\nabla g_{\bar{\eta}}\) with \(C_{g}\).

On the other hand, the variance of BSGD can be decomposed into inner variance and outer variance

\[\mathcal{E}^{t+1}_{\text{var}} =\mathbb{E}_{\xi}[\mathbb{E}_{\eta|\xi,\bar{\eta}|\xi}[\left\|G^{ t+1}-\mathbb{E}_{\xi}[\mathbb{E}_{\eta|\xi,\bar{\eta}|\xi}[G^{t+1}]]\right\|_{2}^{2}]]\] \[=\mathbb{E}_{\xi}[\mathbb{E}_{\eta|\xi,\bar{\eta}|\xi}[\left\|(G^ {t+1}-\mathbb{E}_{\eta|\xi,\bar{\eta}|\xi}[G^{t+1}])+(\mathbb{E}_{\eta|\xi,\bar {\eta}|\xi}[G^{t+1}]-\mathbb{E}_{\xi}[\mathbb{E}_{\eta|\xi,\bar{\eta}|\xi}[G^{ t+1}]])\right\|_{2}^{2}]]\] \[=\underbrace{\mathbb{E}_{\xi}[\mathbb{E}_{\eta|\xi,\bar{\eta}|\xi}[ \left\|G^{t+1}-\mathbb{E}_{\eta|\xi,\bar{\eta}|\xi}[G^{t+1}]\right\|_{2}^{2}]]}_{ \text{Inner variance}}+\underbrace{\mathbb{E}_{\xi}[\left\|\mathbb{E}_{\eta|\xi, \bar{\eta}|\xi}[G^{t+1}]-\mathbb{E}_{\xi}[\mathbb{E}_{\eta|\xi,\bar{\eta}|\xi}[ G^{t+1}]]\right\|_{2}^{2}]}_{\text{Outer variance}}.\]The inner variance is bounded as follows

\[\mathbb{E}_{\xi}[\mathbb{E}_{\eta|\xi,\bar{\eta}|\xi}[G^{t+1}]- \mathbb{E}_{\xi}[\mathbb{E}_{\eta|\xi,\bar{\eta}|\xi}[G^{t+1}]]\big{\|}_{2}^{2}]]\] \[=\mathbb{E}_{\xi}\left[\mathbb{E}_{\eta|\xi,\bar{\eta}|\xi}\left[ \left\|(\tfrac{1}{m}\sum_{\bar{\eta}\in\bar{H}_{\xi}}\nabla g_{\bar{\eta}}( \boldsymbol{x}^{t})-\mathbb{E}_{\bar{\eta}|\xi}[\nabla g_{\bar{\eta}}( \boldsymbol{x}^{t})])^{\top}\nabla f_{\xi}(\tfrac{1}{m}\sum_{\eta\in H_{\xi}}g _{\eta}(\boldsymbol{x}^{t}))\right\|_{2}^{2}]\right]\] \[\qquad+\mathbb{E}_{\xi}\left[\mathbb{E}_{\eta|\xi}\left[\left\|( \mathbb{E}_{\bar{\eta}|\xi}[\nabla g_{\bar{\eta}}(\boldsymbol{x}^{t})])^{\top }(\nabla f_{\xi}(\tfrac{1}{m}\sum_{\eta\in H_{\xi}}g_{\eta}(\boldsymbol{x}^{t }))-\mathbb{E}_{\eta|\xi}[\nabla f_{\xi}(\tfrac{1}{m}\sum_{\eta\in H_{\xi}}g_{ \eta}(\boldsymbol{x}^{t}))])\right\|_{2}^{2}\right]\right]\] \[\leq C_{f}^{2}\,\mathbb{E}_{\xi}\left[\mathbb{E}_{\bar{\eta}|\xi} \left[\left\|\tfrac{1}{m}\sum_{\bar{\eta}\in\bar{H}_{\xi}}\nabla g_{\bar{\eta} }(\boldsymbol{x}^{t})-\mathbb{E}_{\bar{\eta}|\xi}[\nabla g_{\bar{\eta}}( \boldsymbol{x}^{t})]\right\|_{2}^{2}]\right]\] \[\qquad+C_{g}^{2}\,\mathbb{E}_{\xi}\left[\mathbb{E}_{\eta|\xi} \left[\left\|\nabla f_{\xi}(\tfrac{1}{m}\sum_{\eta\in H_{\xi}}g_{\eta}( \boldsymbol{x}^{t}))-\mathbb{E}_{\eta|\xi}[\nabla f_{\xi}(\tfrac{1}{m}\sum_{ \eta\in H_{\xi}}g_{\eta}(\boldsymbol{x}^{t}))]\right\|_{2}^{2}]\right]\] \[=C_{f}^{2}\,\mathbb{E}_{\xi}\left[\mathbb{E}_{\bar{\eta}|\xi} \left[\left\|\tfrac{1}{m}\sum_{\bar{\eta}\in\bar{H}_{\xi}}\nabla g_{\bar{\eta} }(\boldsymbol{x}^{t})-\mathbb{E}_{\bar{\eta}|\xi}[\nabla g_{\bar{\eta}}( \boldsymbol{x}^{t})]\right\|_{2}^{2}]\right]\] \[\qquad+C_{g}^{2}\,\mathbb{E}_{\xi}\left[\mathbb{E}_{\eta|\xi} \left[\left\|\nabla f_{\xi}(\tfrac{1}{m}\sum_{\eta\in H_{\xi}}g_{\eta}( \boldsymbol{x}^{t}))-\nabla f_{\xi}(\mathbb{E}_{\eta}[g_{\eta}(\boldsymbol{x} ^{t})])\right\|_{2}^{2}]\right]\] \[\qquad-C_{g}^{2}\,\mathbb{E}_{\xi}\left[\left\|\nabla f_{\xi}( \mathbb{E}_{\eta}[g_{\eta}(\boldsymbol{x}^{t})])-\mathbb{E}_{\eta|\xi}[ \nabla f_{\xi}(\tfrac{1}{m}\sum_{\eta\in H_{\xi}}g_{\eta}(\boldsymbol{x}^{t}) )]\right\|_{2}^{2}\right]\] \[\leq\tfrac{\zeta_{g}^{2}C_{f}^{2}}{m}+C_{g}^{2}L_{f}\,\mathbb{E} _{\xi}\left[\mathbb{E}_{\eta|\xi}\left[\left\|\tfrac{1}{m}\sum_{\eta\in H_{\xi }}g_{\eta}(\boldsymbol{x}^{t})-\mathbb{E}_{\eta|\xi}[g_{\eta}(\boldsymbol{x}^{ t})]\right\|_{2}^{2}\right]\right]\] \[\leq\tfrac{C_{f}^{2}\zeta_{g}^{2}}{m}+\tfrac{C_{g}^{2}L_{f}}{m} \,\mathbb{E}_{\xi}\left[\mathbb{E}_{\eta|\xi}\left[\left\|g_{\eta}(\boldsymbol {x}^{t})-\mathbb{E}_{\eta|\xi}[g_{\eta}(\boldsymbol{x}^{t})]\right\|_{2}^{2} \right]\right]\] \[\leq\tfrac{\zeta_{g}^{2}C_{f}^{2}+\sigma_{g}^{2}C_{g}^{2}L_{f}^{ 2}}{m}=\tfrac{\sigma_{\text{in}}^{2}}{m}.\]

The outer variance is independent of the inner batch size and can be bounded by

\[\mathbb{E}_{\xi}[\left\|\mathbb{E}_{\eta|\xi,\bar{\eta}|\xi}[G^{t+1}]- \mathbb{E}_{\xi}[\mathbb{E}_{\eta|\xi,\bar{\eta}|\xi}[G^{t+1}]]\right\|_{2}^{ 2}]\leq\mathbb{E}_{\xi}[\left\|\mathbb{E}_{\eta|\xi,\bar{\eta}|\xi}[G^{t+1}] \right\|_{2}^{2}]\leq C_{f}^{2}C_{g}^{2}=C_{F}^{2}=\sigma_{\text{out}}^{2}\]

Therefore, the variance is bounded as follows

\[\mathcal{E}_{\text{var}}^{t+1}\leq\tfrac{\sigma_{\text{in}}^{2}}{m}+\sigma_{ \text{out}}^{2}.\]

This completes the proof. 

**Theorem 6** (**BSGD Convergence**): _Consider the (CSO) problem. Suppose Assumptions 3, 4, 5 holds true. Let step size \(\gamma\leq 1/(2L_{F})\). Then for BSGD, \(\boldsymbol{x}^{s}\) picked uniformly at random among \(\{\boldsymbol{x}^{t}\}_{t=0}^{T-1}\) satisfies: \(\mathbb{E}[\|\nabla F(\boldsymbol{x}^{s})\|_{2}^{2}]\leq\varepsilon^{2}\), for nonconvex \(F\), if the inner batch size \(m=\Omega\left(\sigma_{\text{max}}^{2}\varepsilon^{-2}\right)\) and the number of iterations \(T=\Omega\left((F(\boldsymbol{x}^{0})-F^{\star})L_{F}(\sigma_{\text{max}}^{2}/m+ \sigma_{\text{out}}^{2})\varepsilon^{-4}\right)\), where \(\sigma_{\text{in}}^{2}=\zeta_{g}^{2}C_{f}^{2}+\sigma_{g}^{2}C_{g}^{2}L_{f}^{2}\), \(\sigma_{\text{out}}^{2}=C_{F}^{2}\), and \(\sigma_{\text{bias}}^{2}=\sigma_{g}^{2}C_{g}^{2}L_{f}^{2}\)._

**Proof:** Denote \(G^{t+1}=G_{\text{BSGD}}^{t+1}\) (1). Using descent lemma (Lemma 4) and bias-variance bounds of BSGD (Lemma 5)

\[\tfrac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}\left[\|\nabla F( \boldsymbol{x}^{t})\|_{2}^{2}\right]+\tfrac{1}{2}\tfrac{1}{T}\sum_{t=0}^{T-1} \mathbb{E}\left[\left\|\mathbb{E}^{t+1}[G^{t+1}[t]]\right\|_{2}^{2}\right]\\ \leq\tfrac{2(\mathbb{E}[F(\boldsymbol{x}^{T})]-F^{\star})}{\gamma T }+L_{F}\gamma(\tfrac{\sigma_{\text{in}}^{2}}{m}+\sigma_{\text{out}}^{2})+ \tfrac{\sigma_{\text{bias}}^{2}}{m}\]

Then we can minimize the right-hand size by optimizing \(\gamma\) to

\[\gamma=\sqrt{\tfrac{2(F(\boldsymbol{x}^{0})-F^{\star})}{L_{F}(\sigma_{\text{in}}^{2}/m +\sigma_{\text{out}}^{2})T}}\]

which is smaller than the bound of step size \(\gamma\leq\tfrac{1}{2L_{F}}\) if \(T\) is greater than the following constant which does not rely on the target precision \(\varepsilon\)

\[T\geq\tfrac{8L_{F}(F(\boldsymbol{x}^{0})-F^{\star})}{\sigma_{\text{in}}^{2}/m+ \sigma_{\text{out}}^{2}}.\]Then the upper bound of gradient becomes

\[\frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}\left[\left\|\nabla F(\bm{x}^{t}) \right\|_{2}^{2}\right]\leq\sqrt{\frac{2(F(\bm{x}^{0})-F^{\star})L_{F}(\sigma_{ \text{in}}^{2}/m+\sigma_{\text{out}}^{2})}{T}}+\frac{\sigma_{\text{in}}^{2}}{m}.\]

By taking inner batch size of at least

\[m\geq\frac{\sigma_{\text{in}}^{2}}{\varepsilon^{2}},\]

and iteration \(T\) greater than

\[T\geq\frac{2(F(\bm{x}^{0})-F^{\star})L_{F}(\sigma_{\text{in}}^{2} /m+\sigma_{\text{out}}^{2})}{\varepsilon^{4}},\]

we have that

\[\frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}\left[\left\|\nabla F(\bm{x} ^{t})\right\|_{2}^{2}\right]\leq 2\varepsilon^{2}.\]

By picking \(\bm{x}^{s}\) uniformly at random among \(\{\bm{x}^{t}\}_{t=0}^{T-1}\), we get the desired guarantee. \(\Box\)

The resulting sample complexity of BSGD to get to an \(\varepsilon\)-stationary point is \(\mathcal{O}(\varepsilon^{-6})\).

### Convergence of E-BSGD

In this section, we analyze the sample complexity of Algorithm 2 (E-BSGD) for the CSO problem.

**Lemma 6** (Bias and Variance of E-BSGD): _The bias and variance of E-BSGD are_

\[\mathcal{E}_{\text{bias}}^{t+1} \leq\frac{\tilde{\sigma}_{\text{in}}^{2}}{m^{\star}},\quad \mathcal{E}_{\text{var}}^{t+1}\leq 14(\frac{\sigma_{\text{in}}^{2}}{m}+\sigma_{ \text{out}}^{2})\]

_where \(\sigma_{\text{in}}^{2}=\zeta_{g}^{2}C_{f}^{2}+\sigma_{g}^{2}C_{g}^{2}L_{f}^{2}\), \(\sigma_{\text{out}}^{2}=C_{F}^{2}\), and \(\tilde{\sigma}_{\text{bias}}^{2}=C_{g}^{2}C_{e}^{2}\) with \(C_{e}^{2}\) defined in Corollary 1._

**Proof:** Denote \(G^{t+1}=G_{\text{E-BSGD}}^{t+1}\) (7). Like previously (Lemma 5), let \(\mathbb{E}[\cdot]\) denote the conditional expectation \(\mathbb{E}^{t+1}[\cdot]\) which conditions on all past randomness until time \(t\). In E-BSGD, we apply extrapolation to \(\nabla f_{\xi}(\cdot)\). The bias can be estimated with the help of Corollary 1 as

\[\mathcal{E}_{\text{bias}}^{t+1} =\left\|\nabla F(\bm{x}^{t+1})-\mathbb{E}[G^{t+1}]\right\|_{2}^{2}\] \[\leq C_{g}^{2}\mathbb{E}_{\xi}\left[\left\|\nabla f_{\xi}( \mathbb{E}_{\eta|\xi}[g_{\eta}(\bm{x}^{t})])-\mathbb{E}\left[\mathcal{L}_{ \mathcal{D}_{\bm{\theta},\xi}}^{(2)}\nabla f_{\xi}(0)\right]\right\|_{2}^{2}\right]\] \[\leq\frac{C_{g}^{2}C_{e}^{2}}{m^{\star}}.\]

Since the variance of BSGD in Lemma 5 is upper bounded by \(\frac{\sigma_{\text{in}}^{2}}{m}+\sigma_{\text{out}}^{2}\), then Lemma 2 gives

\[\mathcal{E}_{\text{var}}^{t+1}\leq 14(\nicefrac{{\sigma_{\text{in}}^{2}}} {m}+\sigma_{\text{out}}^{2}).\]

This proves the claimed bounds. \(\Box\)

**Theorem 3**: _[E-BSGD Convergence] Consider the (CSO) problem. Suppose Assumptions 3, 4, 5, 6 hold true and \(L_{F},C_{F},\tilde{L}_{F},C_{g},F^{\star}\) are constants and \(C_{e}(f;g):=\frac{8a_{1}\sigma_{3}+18a_{4}\sigma_{2}^{2}+5a_{4}\sigma_{4}}{96}\) defined in Corollary 1 are associated with second order extrapolation in the CSO problem. Let step size \(\gamma\leq 1/(2L_{F})\). Then the output \(\bm{x}^{s}\) of E-BSGD (Algorithm 2) satisfies: \(\mathbb{E}[\left\|\nabla F(\bm{x}^{s})\right\|_{2}^{2}]\leq\varepsilon^{2}\), for nonconvex \(F\), if the inner batch size \(m=\Omega(C_{e}C_{g}\varepsilon^{-1/2})\), and the number of iterations_

\[T=\Omega(L_{F}(F(\bm{x}^{0})-F^{\star})(L_{F}^{2}/m+C_{F}^{2}) \varepsilon^{-4}).\]

**Proof:** The proof is very similar to Theorem 6. Denote \(G^{t+1}=G_{\text{E-BSGD}}^{t+1}\) (7). Using descent lemma (Lemma 4) and bias-variance bounds of E-BSGD (Lemma 6)

\[\frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}\left[\left\|\nabla F(\bm{x} ^{t})\right\|_{2}^{2}\right]+\frac{1}{2}\frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E} \left[\left\|\mathbb{E}^{t+1}[G^{t+1}|t]\right\|_{2}^{2}\right]\] \[\leq\frac{2(\mathbb{E}[(\bm{x}^{T})]-F^{\star})}{\gamma T}+14L_{F }\gamma(\frac{\sigma_{\text{in}}^{2}}{m}+\sigma_{\text{out}}^{2})+\frac{C_{g}^ {2}C_{e}^{2}}{m^{\star}}.\]Then we optimize \(\gamma\) to

\[\gamma=\sqrt{\frac{(F(\bm{x}^{0})-F^{*})}{\tau_{L_{F}(\sigma_{\text{in}}^{2}/m+ \sigma_{\text{out}}^{2})T}}}\]

which is smaller than the bound of step size \(\gamma\leq\frac{1}{2L_{F}}\) if \(T\) is greater than the following constant which does not rely on the target precision \(\varepsilon\)

\[T\geq\frac{4L_{F}(F(\bm{x}^{0})-F^{*})}{\tau(\sigma_{\text{in}}^{2}/m+\sigma_ {\text{out}}^{2})}.\]

Then the gradient norm has the following upper bound.

\[\frac{1}{T}\sum_{t=0}^{T-1}\left\|\nabla F(\bm{x}^{t})\right\|_{2}^{2}\leq 4 \sqrt{\frac{\gamma(F(\bm{x}^{0})-F^{*})L_{F}(\sigma_{\text{in}}^{2}+\sigma_{ \text{out}}^{2})}{T}}+\frac{\hat{\sigma}_{\text{out}}^{2}}{m^{4}}.\]

In order to reach \(\varepsilon\)-stationary point, i.e.

\[\frac{1}{T}\sum_{t=0}^{T-1}\left\|\nabla F(\bm{x}^{t})\right\|_{2}^{2}\leq \varepsilon^{2},\]

we can enforce

\[4\sqrt{\frac{\gamma(F(\bm{x}^{0})-F^{*})L_{F}(\sigma_{\text{in}}^{2}/m+\sigma _{\text{out}}^{2})}{T}}\leq\varepsilon^{2},\quad\frac{C_{g}^{2}C_{g}^{2}}{m^{ 4}}\leq\varepsilon^{2}.\]

By taking inner batch size of at least

\[m=\Omega(\hat{\sigma}_{\text{bias}}^{1/2}\varepsilon^{-1/2}),\]

and iteration \(T\) greater than

\[T\geq\frac{112(F(\bm{x}^{0})-F^{*})L_{F}(\frac{\sigma_{\text{in}}^{2}}{m}+ \sigma_{\text{out}}^{2})}{\varepsilon^{4}},\]

we have that

\[\frac{1}{T}\sum_{t=0}^{T-1}\left\|\nabla F(\bm{x}^{t})\right\|_{2}^{2}\leq 3 \varepsilon^{2}.\]

By picking \(\bm{x}^{s}\) uniformly at random among \(\left\{\bm{x}^{t}\right\}_{t=0}^{T-1}\), we get the desired guarantee. 

### Convergence of BSpiderBoost

In this section, we reanalyze the BSpiderBoost algorithm of [16] to obtain bounds on bias and variance of its gradient estimates. Theorem 6 shows that BSpiderBoost achieves an \(\mathcal{O}(\varepsilon^{-5})\) sample complexity.

Let \(G_{\text{BSB}}^{t+1}\) as the BSpiderBoost gradient estimate

\[G_{\text{BSB}}^{t+1}=\begin{cases}G_{\text{BSB}}^{t}+\frac{1}{B_{2}}\sum_{ \xi\in\mathcal{E}_{2}}(G_{\text{BSGD}}^{t+1}-G_{\text{BSGD}}^{t})&\text{with prob. }1-p_{\text{out}}\\ \frac{1}{B_{1}}\sum_{\xi\in\mathcal{E}_{1}}G_{\text{BSGD}}^{t+1}&\text{with prob.}p_{\text{out}}.\end{cases}\]

**Lemma 7** (**Bias and Variance of BSpiderBoost**): _If \(\gamma\leq\min\{\frac{1}{2L_{F}},\frac{\sqrt{B_{2}}}{6L_{F}}\}\), then the bias and variance of BSpiderBoost are_

\[\frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}[\mathcal{E}_{\text{bias}}^{t+1}]\leq \frac{2\sigma_{\text{out}}^{2}}{m}+\frac{(1-p_{\text{out}})^{3}}{p_{\text{out}} B_{2}}\frac{56L_{F}^{2}\gamma^{2}}{T}\sum_{t=0}^{T-1}\mathbb{E}[\big{\|}\mathbb{E}^{t+ 1}[G^{t+1}[t]\big{\|}_{2}^{2}]+(\frac{1}{T_{\text{Pout}}}+1)\frac{4(1-p_{\text {out}})^{2}}{B_{1}}(\frac{\sigma_{\text{out}}^{2}}{m}+\sigma_{\text{out}}^{2})\]

_where \(\sigma_{\text{in}}^{2}=\zeta_{g}^{2}C_{f}^{2}+\sigma_{g}^{2}C_{g}^{2}L_{f}^{2}\), \(\sigma_{\text{out}}=C_{F}^{2}\), and \(\sigma_{\text{bias}}^{2}=\sigma_{g}^{2}C_{g}^{2}L_{f}^{2}\)._

**Proof:** Denote \(G^{t+1}=G_{\text{BSB}}^{t+1}\) (8). Like previously (Lemma 5), let \(\mathbb{E}[\cdot]\) denote the conditional expectation \(\mathbb{E}^{t+1}[\cdot|t]\) which conditions on all past randomness until time \(t\). Denote \(G_{L}^{t+1}\) and \(G_{S}^{t+1}\) as the large batch and small batch in BSpiderBoost separately, i.e.,

\[\begin{cases}G_{L}^{t+1}=\frac{1}{B_{1}}\sum_{\xi\in\mathcal{E}_{1}}G_{\text{ BSGD}}^{t+1}&\text{with prob. }p_{\text{out}}\\ G_{S}^{t+1}=G^{t}+\frac{1}{B_{2}}\sum_{\xi\in\mathcal{E}_{2}}(G_{\text{BSGD}}^{t+ 1}-G_{\text{BSGD}}^{t})&\text{with prob. }1-p_{\text{out}}.\end{cases}\]The bias of BSpiderBoost can be decomposed to its distance to BSGD and the distance from BSGD to the full gradient, i.e.,

\[\begin{split}\mathcal{E}_{\text{bias}}^{t+1}&=\left\| \nabla F(\bm{x}^{t+1})-\mathbb{E}[G^{t+1}]\right\|_{2}^{2}\\ &\leq 2\left\|\nabla F(\bm{x}^{t+1})-\mathbb{E}[G_{\text{BSGD}}^{t+1} ]\right\|_{2}^{2}+2\left\|\mathbb{E}[G_{\text{BSGD}}^{t+1}]-\mathbb{E}[G^{t+ 1}]\right\|_{2}^{2}\\ &\leq\frac{2\sigma_{\text{bias}}^{2}}{m}+2\left\|\mathbb{E}[G_{ \text{BSGD}}^{t+1}]-\mathbb{E}[G^{t+1}]\right\|_{2}^{2}.\end{split}\] (14)

where the last inequality uses the bias of BSGD from Lemma 5. Then the second term can be bounded as follows

\[\left\|\mathbb{E}[G_{\text{BSGD}}^{t+1}]-\mathbb{E}[G^{t+1}] \right\|_{2}^{2} =(1-p_{\text{out}})^{2}\left\|\mathbb{E}[G_{\text{BSGD}}^{t+1}] -\mathbb{E}[G_{S}^{t+1}]\right\|_{2}^{2}\] \[=(1-p_{\text{out}})^{2}\left\|\mathbb{E}[G_{\text{BSGD}}^{t}]-G^{ t}\right\|_{2}^{2}.\]

By taking the expectation of randomness of \(G^{t}\)

\[\left\|\mathbb{E}[G_{\text{BSGD}}^{t+1}]-\mathbb{E}[G^{t+1}] \right\|_{2}^{2} =(1-p_{\text{out}})^{2}\left(\left\|\mathbb{E}[G_{\text{BSGD}}^{ t}]-\mathbb{E}[G^{t}]\right\|_{2}^{2}+\mathbb{E}\left\|G^{t}-\mathbb{E}[G^{t}] \right\|_{2}^{2}\right)\] \[=(1-p_{\text{out}})^{2}\left(\left\|\mathbb{E}[G_{\text{BSGD}}^{ t}]-\mathbb{E}[G^{t}]\right\|_{2}^{2}+\mathcal{E}_{\text{var}}^{t}\right)\]

Note that \(\left\|\mathbb{E}[G_{\text{BSGD}}^{1}]-\mathbb{E}[G^{1}]\right\|_{2}^{2}=0\) as the first iteration always chooses the large batch. Then as we always use large batch at \(t=0\) we know that

\[\tfrac{1}{T}\sum_{t=0}^{T-1}\left\|\mathbb{E}[G_{\text{BSGD}}^{t+1}]-\mathbb{E }[G^{t+1}]\right\|_{2}^{2}\leq\tfrac{(1-p_{\text{out}})^{2}}{p_{\text{out}}} \tfrac{1}{T}\sum_{t=0}^{T-1}\mathcal{E}_{\text{var}}^{t+1}.\] (15)

Therefore combine (14) and (15) we can upper bound the bias

\[\tfrac{1}{T}\sum_{t=0}^{T-1}\mathcal{E}_{\text{bias}}^{t+1}\leq\tfrac{2\sigma _{\text{bias}}^{2}}{m}+\tfrac{2(1-p_{\text{out}})^{2}}{p_{\text{out}}}\tfrac{1 }{T}\sum_{t=0}^{T-1}\mathcal{E}_{\text{var}}^{t+1}.\] (16)

**Variance.** Now we consider the variance,

\[\begin{split}\mathcal{E}_{\text{var}}^{t+1}&=\mathbb{ E}\left[\left\|G^{t+1}-\mathbb{E}[G^{t+1}]\right\|_{2}^{2}\right]\\ &\leq(1-p_{\text{out}})\,\mathbb{E}\left[\left\|G_{S}^{t+1}- \mathbb{E}[G_{S}^{t+1}]\right\|_{2}^{2}\right]+p_{\text{out}}\,\mathbb{E}\left[ \left\|G_{L}^{t+1}-\mathbb{E}[G_{L}^{t+1}]\right\|_{2}^{2}\right]\\ &=\tfrac{(1-p_{\text{out}})}{B_{2}}\,\mathbb{E}\left[\left\|G_{ \text{BSGD}}^{t+1}-G_{\text{BSGD}}^{t}-\mathbb{E}[G_{\text{BSGD}}^{t+1}]-G_{ \text{BSGD}}^{t}]\right\|_{2}^{2}\right]+\tfrac{p_{\text{out}}}{B_{1}}\, \mathbb{E}\left[\left\|G_{\text{BSGD}}^{t+1}-\mathbb{E}[G_{\text{BSGD}}^{t+1} ]\right\|_{2}^{2}\right]\\ &\leq\tfrac{1-p_{\text{out}}}{B_{2}}\,\mathbb{E}\left[\left\|G_{ \text{BSGD}}^{t+1}-G_{\text{BSGD}}^{t}-\mathbb{E}[G_{\text{BSGD}}^{t+1}-G_{ \text{BSGD}}^{t}]\right\|_{2}^{2}\right]+\tfrac{p_{\text{out}}}{B_{1}}\,( \tfrac{\sigma_{\text{in}}^{2}}{m}+\sigma_{\text{out}}^{2})\end{split}\] (17)

where the last equality is because the large batch in BSpiderBoost is similar to BSGD.

\[\mathcal{E}_{\text{var}}^{1}=\mathbb{E}\left[\left\|G^{1}-\mathbb{E}[G^{1}] \right\|_{2}^{2}\right]=\mathbb{E}\left[\left\|G_{L}^{1}-\mathbb{E}[G_{L}^{1} ]\right\|_{2}^{2}\right]=\tfrac{1}{B_{1}}\,\mathbb{E}\left[\left\|G_{\text{ BSGD}}^{1}-\mathbb{E}[G_{\text{BSGD}}^{1}]\right\|_{2}^{2}\right]\leq\tfrac{1}{B_{1}}( \tfrac{\sigma_{\text{in}}^{2}}{m}+\sigma_{\text{out}}^{2}).\] (18)

Finally, we expand the variance at small batch size epoch

\[\mathbb{E}\left[\left\|G_{\text{BSGD}}^{t+1}-G_{\text{BSGD}}^{t} -\mathbb{E}[G_{\text{BSGD}}^{t+1}-G_{\text{BSGD}}^{t}]\right\|_{2}^{2}\right]\] \[=\underbrace{\mathbb{E}\left[\left\|G_{\text{BSGD}}^{t+1}-G_{ \text{BSGD}}^{t}-G_{\text{BSGD}}^{t}-\mathbb{E}_{\eta[\xi,\bar{\eta}]\xi}[G_{ \text{BSGD}}^{t+1}-G_{\text{BSGD}}^{t}]\right\|_{2}^{2}\right]}_{\text{Inter variance }\mathcal{T}_{\text{in}}}\] \[\qquad\qquad+\underbrace{\mathbb{E}_{\xi}\left[\left\|\mathbb{E}_{ \eta[\xi,\bar{\eta}]\xi}[G_{\text{BSGD}}^{t+1}-G_{\text{BSGD}}^{t}]- \mathbb{E}[G_{\text{BSGD}}^{t+1}-G_{\text{BSGD}}^{t}]\right\|_{2}^{2}\right]}_{ \text{Outer variance }\mathcal{T}_{\text{out}}}.\]

[MISSING_PAGE_EMPTY:28]

[MISSING_PAGE_FAIL:29]

### Convergence of E-BSpiderBoost

In this section, we analyze the sample complexity of Algorithm 3 (E-BSpiderBoost) for the CSO problem.

**Lemma 8** (Bias and Variance of E-BSpiderBoost): _The bias and variance of E-BSpiderBoost are_

\[\frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}[\mathcal{E}_{\text{var}}^{t+1}] \leq\frac{28(1-p_{\text{out}})L_{F}^{2}\gamma^{2}}{B_{2}}\frac{1}{T}\sum_{t=0}^ {T-1}\mathbb{E}[\big{\|}\mathbb{E}^{t}[G^{t+1}|t]\big{\|}_{2}^{2}]+(\frac{1}{T_{ \text{Pout}}}+1)\frac{28p_{\text{out}}}{B_{1}}(\frac{\sigma_{\text{in}}^{2}}{m} +\sigma_{\text{out}}^{2})\] \[\frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}[\mathcal{E}_{\text{bias}}^{t+1 }]\leq\frac{2\sigma_{\text{out}}^{2}}{m^{4}}+\frac{2}{p_{\text{out}}}\frac{(1- p_{\text{out}})^{2}}{T}\sum_{t=0}^{T-1}\mathbb{E}[\mathcal{E}_{\text{var}}^{t+1}],\]

_where \(\sigma_{\text{in}}^{2}:=\zeta_{g}^{2}C_{f}^{2}+\sigma_{g}^{2}C_{g}^{2}L_{f}^{2}\), \(\sigma_{\text{out}}=C_{F}^{2}\), and \(\tilde{\sigma}_{\text{bias}}^{2}=C_{g}^{2}C_{e}^{2}\) with \(C_{e}^{2}\) defined in Corollary 1._

**Proof:** Denote \(G^{t+1}=G_{\text{E-BSB}}^{t+1}\) (9). Like previously (Lemma 5), let \(\mathbb{E}[\cdot]\) denote the conditional expectation \(\mathbb{E}^{t}[\cdot|t]\) which conditions on all past randomness until time \(t\). Let \(G^{t+1}=G_{\text{E-BSB}}^{t+1}\) be the E-BSpiderBoost update. We expand the bias as follows

\[\mathcal{E}_{\text{bias}}^{t+1} =\big{\|}\nabla F(\bm{x}^{t+1})-\mathbb{E}[G^{t+1}]\big{\|}_{2}^{2}\] \[\leq 2\left\|\nabla F(\bm{x}^{t+1})-\mathbb{E}[G_{\text{E-BSGD}}^{ t+1}]\right\|_{2}^{2}]+2\left\|\mathbb{E}[G_{\text{E-BSGD}}^{t+1}]- \mathbb{E}[G^{t+1}]\right\|_{2}^{2}.\]

From Lemma 6, we know that

\[\big{\|}\nabla F(\bm{x}^{t+1})-\mathbb{E}[G_{\text{E-BSGD}}^{t+1}]\big{\|}_{2 }^{2}\leq\frac{\tilde{\sigma}_{\text{in}}^{2}}{m^{4}}.\]

The distance between \(\mathbb{E}[G_{\text{E-BSGD}}^{t+1}]\) and \(\mathbb{E}[G^{t+1}]\) can be bounded as follows.

\[\big{\|}\mathbb{E}[G_{\text{E-BSGD}}^{t+1}]-\mathbb{E}[G^{t+1}] \big{\|}_{2}^{2} =(1-p_{\text{out}})^{2}\left\|\mathbb{E}[G_{\text{E-BSGD}}^{t+1}] -(G^{t}+\mathbb{E}[G_{\text{E-BSGD}}^{t+1}-G_{\text{E-BSGD}}^{t}])\right\|_{2} ^{2}\] \[=(1-p_{\text{out}})^{2}\left\|\mathbb{E}[G_{\text{E-BSGD}}^{t}]-G^ {t}\right\|_{2}^{2}\]

Taking expectation with respect to \(G^{t}\)

\[\big{\|}\mathbb{E}[G_{\text{E-BSGD}}^{t+1}]-\mathbb{E}[G^{t+1}]\big{\|}_{2}^{2} \leq(1-p_{\text{out}})^{2}(\big{\|}\mathbb{E}[G_{\text{E-BSGD}}^{t}]- \mathbb{E}[G^{t}]\big{\|}_{2}^{2}+\big{\|}G^{t}-\mathbb{E}[G^{t}]\big{\|}_{2}^ {2}).\]

where \(\big{\|}\mathbb{E}[G_{\text{E-BSGD}}^{1}]-\mathbb{E}[G^{1}]\big{\|}_{2}^{2}=0\). By averaging over time we have

\[\frac{1}{T}\sum_{t=0}^{T-1}\big{\|}\mathbb{E}[G_{\text{E-BSGD}}^{t+1}]- \mathbb{E}[G^{t+1}]\big{\|}_{2}^{2}\leq\frac{1}{p_{\text{out}}}\frac{(1-p_{ \text{out}})^{2}}{T}\sum_{t=0}^{T-1}\mathbb{E}[\mathcal{E}_{\text{var}}^{t+1}].\]

Then the bias is bounded by

\[\frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}[\mathcal{E}_{\text{bias}}^{t+1}]\leq \frac{2\tilde{\sigma}_{\text{in}}^{2}}{m^{4}}+\frac{2}{p_{\text{out}}}\frac{( 1-p_{\text{out}})^{2}}{T}\sum_{t=0}^{T-1}\mathbb{E}[\mathcal{E}_{\text{var}}^{t+ 1}].\]

**Variance.** Since the extrapolation only gives a constant overhead given Lemma 2

\[\frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}\left[\big{\|}G_{\text{BSB}}^{t+1}- \mathbb{E}^{t}[G_{\text{BSB}}^{t+1}|t]\big{\|}_{2}^{2}\right]\leq\frac{28(1-p_ {\text{out}})L_{F}^{2}\gamma^{2}}{B_{2}}\frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}[ \big{\|}\mathbb{E}^{t}[G_{\text{BSB}}^{t+1}|t]\big{\|}_{2}^{2}]+(\frac{1}{T}+p_ {\text{out}})\frac{28}{B_{1}}(\frac{\sigma_{\text{in}}^{2}}{m}+\sigma_{\text{ out}}^{2}).\]

Then the variance is bounded by

\[\frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}[\mathcal{E}_{\text{var}}^{t+1}]\leq\frac {28(1-p_{\text{out}})L_{F}^{2}\gamma^{2}}{B_{2}}\frac{1}{T}\sum_{t=0}^{T-1} \mathbb{E}[\big{\|}\mathbb{E}^{t}[G^{t+1}|t]\big{\|}_{2}^{2}]+(\frac{1}{T_{ \text{Pout}}}+1)\frac{28p_{\text{out}}}{B_{1}}(\frac{\sigma_{\text{in}}^{2}}{m}+ \sigma_{\text{out}}^{2}).\]

**Theorem 4**: _[E-BSpiderBoost Convergence] Consider the (CSO) problem under the same assumptions as Theorem 3. Let step size \(\gamma\leq 1/(13L_{F})\). Then the output \(\bm{x}^{s}\) of E-BSpiderBoost (Algorithm 3) satisfies: \(\mathbb{E}[\|\nabla F(\bm{x}^{s})\|_{2}^{2}]\leq\varepsilon^{2}\), for nonconvex \(F\), if the inner batch size \(m=\mathcal{O}(C_{e}C_{g}\varepsilon^{-0.5})\), the hyperparameters of the outer loop of E-BSpiderBoost \(B_{1}=(\tilde{L}_{F}^{2}/m+C_{F}^{2})\varepsilon^{-2},\quad B_{2}=\sqrt{B_{1}}, \quad p_{\text{out}}=1/B_{2}\), and the number of iterations_

\[T=\Omega(L_{F}(F(\bm{x}^{0})-F^{*})\varepsilon^{-2}).\]

**Proof:** Denote \(G^{t+1}=G_{\text{E-SBP}}^{t+1}\) (9). Using descent lemma (Lemma 4) and bias-variance bounds of E-BSpiderBoost (Lemma 8)

\[\frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}[\left\|\nabla F(\bm{x}^{t}) \right\|_{2}^{2}]+\frac{1}{2T}\sum_{t=0}^{T-1}\mathbb{E}[\left\|\mathbb{E}^{t} [G^{t+1}|t]\right\|_{2}^{2}]\] \[\leq\frac{2(F(\bm{x}^{0})-F^{*})}{\gamma T}+L_{F}\gamma\frac{1}{ T}\sum_{t=0}^{T-1}\mathbb{E}[\mathcal{E}_{\text{var}}^{t+1}]+\frac{1}{T}\sum_{t=0}^{ T-1}\mathbb{E}[\mathcal{E}_{\text{bias}}^{t+1}]\] \[\leq\frac{2(F(\bm{x}^{0})-F^{*})}{\gamma T}+L_{F}\gamma\frac{1}{ T}\sum_{t=0}^{T-1}\mathbb{E}[\mathcal{E}_{\text{var}}^{t+1}]+\frac{2\delta_{ \text{bias}}^{2}}{m^{4}}+\frac{2}{p_{\text{out}}}\frac{1}{T}\sum_{t=0}^{T-1} \mathbb{E}[\mathcal{E}_{\text{var}}^{t+1}]\] \[\leq\frac{2(F(\bm{x}^{0})-F^{*})}{\gamma T}+\frac{2\delta_{\text {bias}}^{2}}{m^{4}}+\frac{3}{p_{\text{out}}}\frac{1}{T}\sum_{t=0}^{T-1} \mathbb{E}[\mathcal{E}_{\text{var}}^{t+1}]\]

where the last inequality use \(\gamma\leq\frac{1}{2L_{F}}\). Use the variance estimation of \(G^{t+1}\) and choose \(B_{2}p_{\text{out}}=1\)

\[\frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}[\left\|\nabla F(\bm{x}^{t}) \right\|_{2}^{2}]+\frac{1}{2T}\sum_{t=0}^{T-1}\mathbb{E}[\left\|\mathbb{E}^{t }[G^{t+1}|t]\right\|_{2}^{2}]\] \[\leq\frac{2(F(\bm{x}^{0})-F^{*})}{\gamma T}+\frac{2\delta_{\text {bias}}^{2}}{m^{4}}+84L_{F}^{2}\gamma^{2}\frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E} [\left\|\mathbb{E}^{t}[G^{t+1}|t]\right\|_{2}^{2}]+(\frac{1}{Tp_{\text{out}}} +1)\frac{84}{B_{1}}(\frac{\sigma_{\text{in}}^{2}}{m}+\sigma_{\text{out}}^{2}).\]

Now we can let \(\gamma\leq\frac{1}{13L_{F}}\) such that \(84L_{F}^{2}\gamma^{2}\leq\frac{1}{2}\)

\[\frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}[\left\|\nabla F(\bm{x}^{t}) \right\|_{2}^{2}]\leq\frac{2(F(\bm{x}^{0})-F^{*})}{\gamma T}+\frac{2\delta_{ \text{bias}}^{2}}{m^{4}}+(\frac{1}{Tp_{\text{out}}}+1)\frac{84}{B_{1}}(\frac{ \sigma_{\text{in}}^{2}}{m}+\sigma_{\text{out}}^{2}).\] (19)

In order to make the right-hand side \(\varepsilon^{2}\), the inner batch size

\[m=\Omega(\tilde{\sigma}_{\text{bias}}^{2}\varepsilon^{-0.5}),\]

and the outer batch size

\[B_{1}=\frac{(\sigma_{\text{in}}^{2}/m+\sigma_{\text{out}}^{2})}{ \varepsilon^{2}},\quad B_{2}=\sqrt{B_{1}},\quad p_{\text{out}}=\frac{1}{B_{2}}.\]

The step size \(\gamma\) is upper bounded by \(\min\{\frac{1}{2L_{F}},\frac{\sqrt{B_{2}}}{6L_{F}},\frac{1}{13L_{F}}\}\). As \(B_{2}\geq 1\), we can take \(\gamma=\frac{1}{13L_{F}}\). So we need iteration \(T\) greater than

\[T\geq\frac{26L_{F}(F(\bm{x}^{0})-F^{*})}{\varepsilon^{2}}.\]

By picking \(\bm{x}^{s}\) uniformly at random among \(\{\bm{x}^{t}\}_{t=0}^{T-1}\), we get the desired guarantee. \(\square\)

## Appendix E Stationary Point Convergence Proofs from Section 4 (Fcco)

In this section, we provide the convergence proofs for the FCCO problem. We start by analyzing a variant of BSpiderBoost (Algorithm 3) for this case in Appendix E.1. In Appendix E.2, we present a multi-level variance reduction approach (called NestedVR) that applies variance reduction in both outer (over the random variable \(i\)) and inner (over the random variable \(\eta|i\)) loops. In Appendix E.3, we analyze E-NestedVR. As in the case of CSO analyses, our proofs go via bounds on bias and variance terms of these algorithms.

### E-BspiderBoost for FCCO problem

**Theorem 8**: _Consider the (FCCO) problem. Suppose Assumptions 3, 4, 5, 6 holds true. Let step size \(\gamma=\mathcal{O}(1/L_{F})\). Then the output of E-BSpiderBoost (Algorithm 3) satisfies: \(\mathbb{E}[\left\|\nabla F(\bm{x}^{s})\right\|_{2}^{2}]\leq\varepsilon^{2}\), for nonconvex \(F\), if the inner batch size \(m=\Omega(\max\{C_{e}C_{\beta}\varepsilon^{-1/2},\sigma_{in}^{2}n^{-1} \varepsilon^{-2}\})\), the hyperparameters of the outer loop of E-BSpiderBoost \(B_{1}=n,B_{2}=\sqrt{n},p_{out}=1/B_{2}\), and the number of iterations \(T=\Omega\left(L_{F}(F(\bm{x}^{0})-F^{*})\varepsilon^{-2}\right)\). The resulting sample complexity is_

\[\mathcal{O}\left(L_{F}(F(\bm{x}^{0})-F^{*})\max\left\{\frac{\sqrt{n}C_{e}C_{ \beta}}{\varepsilon^{2.5}},\frac{\sigma_{\text{out}}^{2}}{\sqrt{n}\varepsilon^{ 4}}\right\}\right).\]

**Remark 9**: _The sample complexity depends on the relation between \(n\) and \(\varepsilon\)_

* _When_ \(n=\mathcal{O}(1)\)_, we have a complexity of_ \(\mathcal{O}(\varepsilon^{-4})\)_. This happens because we did not apply variance reduction for the inner loop.__._
* _When_ \(n=\Theta(\varepsilon^{-2/3})\)_, E-BSpiderBoost has same performance as MSVR-V2_ _[_19_]_ _of_ \(\mathcal{O}(n\varepsilon^{-3})=\mathcal{O}(\varepsilon^{-11/3})\)_._
* _When_ \(n=\Theta(\varepsilon^{-1.5})\)_, E-BSpiderBoost achieves a better sample complexity of_ \(\mathcal{O}(\varepsilon^{-3.25})\) _than_ \(\mathcal{O}(\varepsilon^{-4.5})\) _from MSVR-V2_ _[_19_]__._
* _When_ \(n=\Theta(\varepsilon^{-2})\)_, we recover_ \(\mathcal{O}(\varepsilon^{-3.5})\) _sample complexity as in Theorem_ 4_._

**Proof:** Denote \(G^{t+1}=G^{t+1}_{\text{E-BSGD}}\) (9). As we are using the finite-sum variant of SpiderBoost for the outer loop of the CSO problem, we only need to change the (17) and (18) to reflect that the outer variance is 0 now instead of \(\frac{\sigma_{\text{opt}}^{2}}{B_{1}}\) in the general CSO case. More concretely, we update (17) to

\[\mathcal{E}_{\text{var}}^{t+1} =\mathbb{E}[\big{\|}G^{t+1}-\mathbb{E}[G^{t+1}]\big{\|}_{2}^{2}]\] \[\leq(1-p_{\text{out}})\,\mathbb{E}[\big{\|}G^{t+1}_{S}-\mathbb{E} [G^{t+1}_{S}]\big{\|}_{2}^{2}]+p_{\text{out}}\,\mathbb{E}[\big{\|}G^{t+1}_{L}- \mathbb{E}[G^{t+1}_{L}]\big{\|}_{2}^{2}]\] \[=\frac{(1-p_{\text{out}})}{B_{2}}\,\mathbb{E}[\big{\|}G^{t+1}_{ \text{E-BSGD}}-G^{t}_{\text{E-BSGD}}-\mathbb{E}[G^{t+1}_{\text{E-BSGD}}-G^{t} _{\text{E-BSGD}}]\big{\|}_{2}^{2}]+\frac{p_{\text{out}}}{B_{1}}\,\mathbb{E}[ \big{\|}G^{t+1}_{\text{E-BSGD}}-\mathbb{E}[G^{t+1}_{\text{E-BSGD}}]\big{\|}_{2 }^{2}]\] \[\leq\frac{1-p_{\text{out}}}{B_{2}}\,\mathbb{E}[\big{\|}G^{t+1}_{ \text{E-BSGD}}-G^{t}_{\text{E-BSGD}}-\mathbb{E}[G^{t+1}_{\text{E-BSGD}}-G^{t} _{\text{E-BSGD}}]\big{\|}_{2}^{2}]+\frac{p_{\text{out}}}{B_{1}}\frac{\sigma_{ \text{in}}^{2}}{m}.\] (20)

and change (18) to

\[\mathcal{E}_{\text{var}}^{1}=\mathbb{E}[\big{\|}G^{1}-\mathbb{E}[G^{1}]\big{\|} _{2}^{2}]=\mathbb{E}[\big{\|}G^{1}_{L}-\mathbb{E}[G^{1}_{L}]\big{\|}_{2}^{2}]= \frac{1}{B_{1}}\,\mathbb{E}[\big{\|}G^{1}_{\text{E-BSGD}}-\mathbb{E}[G^{1}_{ \text{E-BSGD}}]\big{\|}_{2}^{2}]\leq\frac{1}{B_{1}}\frac{\sigma_{\text{in}}^{ 2}}{m}.\] (21)

Then our analysis only has to start from the updated version of (19)

\[\frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}[\|\nabla F(\bm{x}^{t})\|_{2}^{2}]\leq \frac{2(F(\bm{x}^{0})-F^{*})}{\gamma T}+\frac{2\bar{\sigma}_{\text{in}}^{2}}{m ^{4}}+(\frac{1}{T_{\text{Pout}}}+1)\frac{84}{B_{1}}\frac{\sigma_{\text{in}}^{ 2}}{m}.\]

We would like all terms on the right-hand side to be bounded by \(\varepsilon^{2}\). From \(\frac{2\bar{\sigma}_{\text{in}}^{2}}{m^{4}}\leq\varepsilon^{2}\) we know that

\[m=\Omega(\frac{\bar{\sigma}_{\text{in}}^{1/2}}{\varepsilon^{1/2}}).\]

From \((\frac{1}{T_{\text{Pout}}}+1)\frac{84}{B_{1}}\frac{\sigma_{\text{in}}^{2}}{m} \leq\varepsilon^{2}\), we know that

\[m=\Omega(\frac{\bar{\sigma}_{\text{in}}^{2}}{ne^{2}}).\]

From \(\frac{2(F(\bm{x}^{0})-F^{*})}{\gamma T}\leq\varepsilon^{2}\), we can choose that

\[\gamma=\mathcal{O}(\frac{1}{L_{F}}),\quad T=\Omega\left(\frac{L_{F}(F(\bm{x}^{0 })-F^{*})}{\varepsilon^{2}}\right).\]

Now the total sample complexity for E-BSpiderBoost for the FCCO problem becomes

\[B_{2}mT=\mathcal{O}\left(L_{F}^{2}(F(\bm{x}^{0})-F^{*})\max\left\{\frac{\sqrt{n} \bar{\sigma}_{\text{in}}^{1/2}}{\varepsilon^{2}\varepsilon^{5}},\frac{\sigma_ {\text{in}}^{2}}{\sqrt{n}\varepsilon^{4}}\right\}\right).\]

By picking \(\bm{x}^{s}\) uniformly at random among \(\{\bm{x}^{t}\}_{t=0}^{T-1}\), we get the desired guarantee. \(\Box\)

### Convergence of NestedVR

**NestedVR Algorithm.** We start by describing the NestedVR construction. We maintain states \(\bm{y}_{i}^{t+1}\) and \(\bm{z}_{i}^{t+1}\) to approximate

\[\bm{y}_{i}^{t+1}\approx\mathbb{E}_{\eta|i}[g_{\eta}(\bm{x}^{t})],\quad\bm{z}_{i }^{t+1}\approx\mathbb{E}_{\bar{\eta}|i}[\nabla g_{\bar{\eta}}(\bm{x}^{t})].\]

In iteration \(t+1\), if \(i\) is selected, then the state \(\bm{y}_{i}^{t+1}\) is updated as follows

\[\bm{y}_{i}^{t+1}=\begin{cases}\frac{1}{S_{1}}\sum_{\eta\in H_{i}}g_{\eta}(\bm{x} ^{t})&\text{with prob. $p_{\text{in}}$}\\ \bm{y}_{i}^{t}+\frac{1}{S_{2}}\sum_{\eta\in H_{i}}(g_{\eta}(\bm{x}^{t})-g_{\eta} (\bm{\phi}_{i}^{t}))&\text{with prob. $1-p_{\text{in}}$},\end{cases}\]where \(\phi_{i}^{t}\) is the last time node \(i\) is visited. If \(i\) is not selected, then

\[\bm{y}_{i}^{t+1}=\bm{y}_{i}^{t}.\]

In this case, \(\bm{y}_{i}^{t+1}\) was never used to compute \(\nabla f_{i}(\bm{y}_{i}^{t+1})\) because \(i\) is not selected at the time \(t+1\). We use the following quantities

\[\hat{\bm{z}}_{i}^{t+1}=\mathbb{E}_{\bar{\eta}|i}[\nabla g_{\bar{\eta}}(\bm{x}^ {t})],\qquad\bm{z}_{i}^{t+1}=\tfrac{1}{m}\sum_{\bar{\eta}\in\hat{H}_{i}}\nabla g _{\bar{\eta}}(\bm{x}^{t}).\] (22)

We use \(G_{\text{NVR}}^{t+1}\) as the actual updates,

\[G_{\text{NVR}}^{t+1}=\begin{cases}\frac{1}{B_{1}}\sum_{i\in\mathcal{B}_{1}}( \bm{z}_{i}^{t+1})^{\top}\nabla f_{i}(\bm{y}_{i}^{t+1})&\text{with prob. }p_{\text{out}}\\ G_{\text{NVR}}+\frac{1}{B_{2}}\sum_{i\in\mathcal{B}_{2}}((\bm{z}_{i}^{t+1})^{ \top}\nabla f_{i}(\bm{y}_{i}^{t+1})-(\bm{z}_{i}^{t})^{\top}\nabla f_{i}(\tilde {\bm{y}}_{i}^{t}))&\text{with prob. }1-p_{\text{out}}.\end{cases}\]

We can also use the following quantity \(\hat{G}_{\text{NVR}}^{t+1}\) as an auxiliary

\[\hat{G}_{\text{NVR}}^{t+1}=\begin{cases}\frac{1}{B_{1}}\sum_{i\in\mathcal{B}_{ 1}}(\hat{\bm{z}}_{i}^{t+1})^{\top}\nabla f_{i}(\bm{y}_{i}^{t+1})&\text{with prob. }p_{\text{out}}\\ G_{\text{NVR}}^{t}+\frac{1}{B_{2}}\sum_{i\in\mathcal{B}_{2}}((\hat{\bm{z}}_{i}^ {t+1})^{\top}\nabla f_{i}(\bm{y}_{i}^{t+1})-(\hat{\bm{z}}_{i}^{t})^{\top} \nabla f_{i}(\tilde{\bm{y}}_{i}^{t}))&\text{with prob. }1-p_{\text{out}}.\end{cases}\]

Here we use \(\tilde{\bm{y}}_{i}^{t}\) to represent an i.i.d. copy of \(\bm{y}_{i}^{t}\) where \(i\) is selected at time \(t\).

The iterate \(\bm{x}^{t+1}\) is therefore updated

\[\bm{x}^{t+1}=\bm{x}^{t}-\gamma G_{\text{NVR}}^{t+1}.\]

**Lemma 9**: _The error between \(G_{\text{NVR}}^{t+1}\) and \(\hat{G}_{\text{NVR}}^{t+1}\) can be upper bounded as follows_

\[\tfrac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}\left[\left\|G_{\text{NVR}}^{t+1}-\hat{ G}_{\text{NVR}}^{t+1}\right\|_{2}^{2}\right]\leq\tfrac{1}{B_{1}}\tfrac{C_{2}^{2} \sigma_{g}^{2}}{m}+\tfrac{4(1-p_{\text{out}})}{B_{2}m_{\text{out}}}\tfrac{1}{T }\sum_{t=0}^{T-1}\left(\mathbb{E}\big{[}\big{\|}G_{i}^{t+1}-G_{i}^{t}\big{\|}_ {2}^{2}\big{]}\right).\]

**Proof:** In this proof, we ignore the subscript in \(G_{\text{NVR}}^{t+1}\) and \(\hat{G}_{\text{NVR}}^{t+1}\), we bound the error between \(G^{t+1}\) and associated \(\hat{G}^{t+1}\) where

\[G_{i}^{t+1} =(\tfrac{1}{m}\sum_{\bar{\eta}\in\hat{H}_{i}}\nabla g_{\bar{\eta} }(\bm{x}))^{\top}\nabla f_{i}(\bm{y}_{i}^{t+1}),\] \[\hat{G}_{i}^{t+1} =(\mathbb{E}_{\bar{\eta}|i}[\nabla g_{\bar{\eta}}(\bm{x})])^{\top }\nabla f_{i}(\bm{y}_{i}^{t+1}).\]

Let's only consider the expectation over the randomness of \(\nabla g_{\bar{\eta}}\),

\[\mathbb{E}_{H_{i}}\left[\left\|G_{i}^{t+1}-\hat{G}_{i}^{t+1}\right\|_{2}^{2}\right] \leq\mathbb{E}_{\bar{\eta}|i}\left[\left\|(\tfrac{1}{m}\sum_{\bar{ \eta}\in\hat{H}_{i}}\nabla g_{\bar{\eta}}(\bm{x})-\mathbb{E}_{\bar{\eta}|i}[ \nabla g_{\bar{\eta}}(\bm{x}))]\right\|_{2}^{2}\right]\mathbb{E}\big{[} \big{\|}\nabla f_{i}(\bm{y}_{i}^{t+1})\big{\|}_{2}^{2}\right]\] \[\leq\tfrac{C_{2}^{2}\sigma_{g}^{2}}{m}.\]

Then we can bound the error as follows

\[\mathbb{E}\left[\mathbb{E}_{H_{i}}\left[\left\|G^{t+1}-\hat{G}^{t +1}\right\|_{2}^{2}\right]\right] =\tfrac{p_{\text{out}}}{B_{1}}\mathbb{E}\left[\mathbb{E}_{H_{i}} \left[\left\|G_{i}^{t+1}-\hat{G}_{i}^{t+1}\right\|_{2}^{2}\right]\right]\] \[\qquad+(1-p_{\text{out}})\left(\left\|G^{t}-\hat{G}^{t}\right\|_{2 }^{2}+\tfrac{1}{B_{2}}\,\mathbb{E}\left[\mathbb{E}_{H_{i}}\left[\left\|G_{i} ^{t+1}-G_{i}^{t}-\hat{G}_{i}^{t+1}-\hat{G}_{i}^{t}\right\|_{2}^{2}\right] \right]\right)\] \[\leq\tfrac{p_{\text{out}}}{B_{1}}\tfrac{C_{2}^{2}\sigma_{g}^{2}}{ m}+(1-p_{\text{out}})\left\|G^{t}-\hat{G}^{t}\right\|_{2}^{2}\] \[\qquad+\tfrac{(1-p_{\text{out}})}{B_{2}m}\left(\mathbb{E}\left[ \left\|G_{i}^{t+1}-G_{i}^{t}\right\|_{2}^{2}\right]\right)\] \[\leq\tfrac{p_{\text{out}}}{B_{1}}\tfrac{C_{2}^{2}\sigma_{g}^{2}}{ m}+(1-p_{\text{out}})\left\|G^{t}-\hat{G}^{t}\right\|_{2}^{2}+\tfrac{(1-p_{\text{out}}) }{B_{2}m}\left(\mathbb{E}\big{[}\big{\|}G_{i}^{t+1}-G_{i}^{t}\big{\|}_{2}^{2} \right]\right).\]

Unroll the recursion gives

\[\tfrac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}\left[\left\|G^{t+1}-\hat{G}^{t+1} \right\|_{2}^{2}\right]\leq\tfrac{1}{B_{1}}\tfrac{C_{2}^{2}\sigma_{g}^{2}}{m}+ \tfrac{4(1-p_{\text{out}})}{B_{2}m_{\text{out}}}\tfrac{1}{T}\sum_{t=0}^{T-1} \mathbb{E}[\big{\|}G_{i}^{t+1}-G_{i}^{t}\big{\|}_{2}^{2}].\]

**Lemma 10** (Staleness): _Define the staleness of iterates at time \(t\) as \(\Xi^{t}:=\frac{1}{n}\sum_{j=1}^{n}\lVert\bm{x}^{t}-\bm{\phi}_{j}^{t}\rVert_{2}^{2}\) and let \(G^{t+1}\) be the gradient estimate, then_

\[\tfrac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}[\Xi^{t}]\leq\tfrac{6n^{2}}{B_{2}^{2}} \gamma^{2}\tfrac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}[\big{\lVert}G^{t+1}\big{\rVert} _{2}^{2}].\] (23)

**Proof:** Like previously (Lemma 5), let \(\mathbb{E}[\cdot]\) denote the expectation conditioned on all previous randomness until \(t-1\). It is clear that \(\Xi^{0}=0\), so we only consider \(t>0\). We upper bound \(\mathbb{E}[\Xi^{t}]\) as follows,

\[\mathbb{E}[\Xi^{t}]=(1-p_{\text{out}})\underbrace{\frac{1}{n}\sum_{j=1}^{n} \mathbb{E}[\big{\lVert}\bm{x}^{t}-\bm{\phi}_{j}^{t}\big{\rVert}_{2}^{2}]}_{ \text{if time $t$ takes $B_{2}$}}+p_{\text{out}}\underbrace{\frac{1}{n}\sum_{j=1}^{n} \mathbb{E}[\big{\lVert}\bm{x}^{t}-\bm{\phi}_{j}^{t}\big{\rVert}_{2}^{2}]}_{ \text{if time $t$ takes $\mathcal{B}_{1}(\bm{\phi}_{j}^{t}=\bm{x}^{t-1})$}}.\]

Then we can expand \(\mathbb{E}[\Xi^{t}]\) as follows

\[\mathbb{E}[\Xi^{t}]\] \[\leq\frac{1}{n}\sum_{j=1}^{n}(1+\tfrac{1}{\beta})\,\mathbb{E}_{i }[\big{\lVert}\bm{x}^{t-1}-\bm{\phi}_{j}^{t}\big{\rVert}_{2}^{2}]\!+\!(1\!+\! \beta)\gamma^{2}\,\mathbb{E}[\big{\lVert}G^{t}\big{\rVert}_{2}^{2}]\]

where we use Cauchy-Schwarz inequality with coefficient \(\beta>0\). By the definition of \(\bm{\phi}_{j}^{t}\),

\[\mathbb{E}[\Xi^{t}]\] \[=(1+\tfrac{1}{\beta})(1-\tfrac{B_{2}}{n})\Xi^{t-1}+(1+\beta) \gamma^{2}\,\mathbb{E}[\big{\lVert}G^{t}\big{\rVert}_{2}^{2}].\]

By taking \(\beta=2n/B_{2}\), we have that \((1+\tfrac{1}{\beta})(1-\tfrac{B_{2}}{n})\leq 1-\tfrac{B_{2}}{2n}\) and thus

\[\mathbb{E}[\Xi^{t}]\leq(1-\tfrac{B_{2}}{2n})\Xi^{t-1}+(1+\tfrac{2n}{B_{2}}) \gamma^{2}\,\mathbb{E}[\big{\lVert}G^{t}\big{\rVert}_{2}^{2}].\]

Note that \(\mathbb{E}[\Xi^{0}]=0\).

\[\tfrac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}[\Xi^{t}] \leq\tfrac{2n}{B_{2}}(1+\tfrac{2n}{B_{2}})\gamma^{2}\tfrac{1}{T} \sum_{t=0}^{T-1}\mathbb{E}[\big{\lVert}G^{t+1}\big{\rVert}_{2}^{2}]\] \[\leq\tfrac{6n^{2}}{B_{2}^{2}}\gamma^{2}\tfrac{1}{T}\sum_{t=0}^{T- 1}\mathbb{E}[\big{\lVert}G^{t+1}\big{\rVert}_{2}^{2}].\]

\(\square\)

The following lemma describes how the inner variable changes inside the variance.

**Lemma 11**: _Denote \(\mathcal{E}_{y}^{t+1}:=\mathbb{E}\left[\big{\lVert}\bm{y}_{i}^{t+1}-\mathbb{E }_{\eta|i}[g_{\eta}(\bm{x}^{t})]\big{\rVert}_{2}^{2}\right]\) to be the error from inner variance and \(p_{\text{out}}T\leq 1\). Then_

\[\tfrac{1}{T}\sum_{t=0}^{T-1}\mathcal{E}_{y}^{t+1}\leq\tfrac{(1-p_{\text{in}}) C_{2}^{2}}{p_{\text{in}}S_{2}}\tfrac{1}{T}\sum_{t=0}^{T-1}\Xi^{t}+\tfrac{2 \sigma_{2}^{2}}{S_{1}}.\]

_Meanwhile, \(\mathcal{E}_{y}^{1}=\mathbb{E}[\big{\lVert}\bm{y}_{i}^{1}-\mathbb{E}_{\eta|i} [g_{\eta}(\bm{x}^{0})]\big{\rVert}_{2}^{2}]=\tfrac{\sigma_{2}^{2}}{S_{1}}\)._

**Proof:**

\[\mathcal{E}_{y}^{t+1} \leq p_{\text{in}}\tfrac{\sigma_{2}^{2}}{S_{1}}+(1-p_{\text{in}} )\,\mathbb{E}_{i}[\mathbb{E}_{\eta|i}[\big{\lVert}\bm{y}_{i}^{t}-\mathbb{E}_{ \eta|i}[g_{\eta}(\bm{\phi}_{i}^{t})]\big{\rVert}_{2}^{2}]]\] \[\qquad+\tfrac{1-p_{\text{in}}}{S_{2}}\,\mathbb{E}_{i}[\mathbb{E} _{\eta|i}[\big{\lVert}g_{\eta}(\bm{x}^{t})-g_{\eta}(\bm{\phi}_{i}^{t})\big{ \rVert}_{2}^{2}]]\] \[\leq(1-p_{\text{in}})\mathcal{E}_{y}^{t}+\tfrac{(1-p_{\text{in}} )C_{2}^{2}}{S_{2}}\,\mathbb{E}_{i}[\mathbb{E}_{\eta|i}[\big{\lVert}\bm{x}^{t}- \bm{\phi}_{i}^{t}\big{\rVert}_{2}^{2}]]+p_{\text{in}}\tfrac{\sigma_{2}^{2}}{S_ {1}}.\]As \(t=0\) always uses the large batch, \(\mathcal{E}_{y}^{1}=\mathbb{E}[\big{\|}\boldsymbol{y}_{i}^{1}-\mathbb{E}_{\eta|i}[g _{\eta}(\boldsymbol{x}^{0})]\big{\|}_{2}^{2}]=\frac{\sigma_{g}^{2}}{S_{1}}\). Then

\[\tfrac{1}{T}\sum_{t=0}^{T-1}\mathcal{E}_{y}^{t+1} \leq\tfrac{(1-p_{\text{in}})C_{g}^{2}}{p_{\text{in}}S_{2}}\tfrac{1} {T}\sum_{t=0}^{T-1}\mathbb{E}_{i}[\mathbb{E}_{\eta|i}[\big{\|}\boldsymbol{x}^{ t}-\boldsymbol{\phi}_{i}^{t}\big{\|}_{2}^{2}]]+\tfrac{\sigma_{g}^{2}}{S_{1}}+ \tfrac{\mathcal{E}_{y}^{1}}{p_{\text{in}}T}\] \[\leq\tfrac{(1-p_{\text{in}})C_{g}^{2}}{p_{\text{in}}S_{2}}\tfrac{1 }{T}\sum_{t=0}^{T-1}\Xi^{t}+\tfrac{2\sigma_{g}^{2}}{S_{1}}.\]

**Lemma 12**: _The error \(\mathbb{E}_{i}[\mathbb{E}_{p_{\text{in}}}[\mathbb{E}_{\eta|i}[\big{\|} \boldsymbol{y}_{i}^{t+1}-\tilde{\boldsymbol{y}}_{i}^{t}\big{\|}_{2}^{2}]]]\) satisfies_

\[\tfrac{1}{T}\sum_{t=1}^{T-1}\mathbb{E}_{i}[\mathbb{E}_{p_{\text{in }}}[\mathbb{E}_{\eta|i}[\big{\|}\boldsymbol{y}_{i}^{t+1}-\tilde{\boldsymbol{y }}_{i}^{t}\big{\|}_{2}^{2}]]]\leq\tfrac{4C_{g}^{2}\gamma^{2}}{T}\sum_{t=0}^{T- 1}\mathbb{E}[\big{\|}G^{t+1}\big{\|}_{2}^{2}]+\tfrac{4(1-p_{\text{in}})C_{g}^{ 2}}{S_{2}}\tfrac{1}{T}\sum_{t=0}^{T-1}\Xi^{t+1}\] \[\qquad\qquad\qquad+\tfrac{6(1-p_{\text{in}})}{T}\sum_{t=0}^{T-1} \mathcal{E}_{y}^{t+1}.\]

Note that when \(p_{\text{in}}=1\) and \(S_{1}=S_{2}=m\), we recover the following

\[\tfrac{1}{T}\sum_{t=1}^{T-1}\mathbb{E}_{i}[\mathbb{E}_{p_{\text{in} }}[\mathbb{E}_{\eta|i}[\big{\|}\boldsymbol{y}_{i}^{t+1}-\tilde{\boldsymbol{y }}_{i}^{t}\big{\|}_{2}^{2}]]]=\tfrac{1}{T}\sum_{t=1}^{T-1}\mathbb{E}_{i}[ \mathbb{E}_{p_{\text{in}}}[\mathbb{E}_{\eta|i}[\big{\|}\tfrac{1}{m}\sum_{\eta \in H_{\xi}}g_{\eta}(\boldsymbol{x}^{t})-g_{\eta}(\boldsymbol{x}^{t-1}) \big{\|}_{2}^{2}]]]\] \[\leq\tfrac{4C_{g}^{2}\gamma^{2}}{T}\sum_{t=0}^{T-1}\mathbb{E} \big{[}\big{\|}G^{t+1}\big{\|}_{2}^{2}].\]

**Proof:** For \(t\geq 2\), \(\mathbb{E}_{i}[\mathbb{E}_{p_{\text{in}}}[\mathbb{E}_{\eta|i}[\big{\|} \boldsymbol{y}_{i}^{t+1}-\tilde{\boldsymbol{y}}_{i}^{t}\big{\|}_{2}^{2}]]]\) can be upper bounded as follows

\[\mathbb{E}_{i}[\mathbb{E}_{p_{\text{in}}}[\mathbb{E}_{\eta|i}[ \big{\|}\boldsymbol{y}_{i}^{t+1}-\tilde{\boldsymbol{y}}_{i}^{t}\big{\|}_{2}^{2 }]]]=p_{\text{in}}\,\mathbb{E}_{i}\,\left[\mathbb{E}_{\eta|i}\left[\Big{\|} \tfrac{1}{S_{1}}\sum_{\eta\in H_{i}}(g_{\eta}(\boldsymbol{x}^{t})-g_{\eta}( \boldsymbol{x}^{t-1}))\Big{\|}_{2}^{2}\right]\right]\] \[\leq p_{\text{in}}C_{g}^{2}\,\big{\|}\boldsymbol{x}^{t}- \boldsymbol{x}^{t-1}\big{\|}_{2}^{2}+\tfrac{1-p_{\text{in}}}{S_{2}}\,\mathbb{ E}_{i}\left[\Big{\|}(g_{\eta}(\boldsymbol{x}^{t})-g_{\eta}(\boldsymbol{\phi}_{i}^{t }))-(g_{\eta}(\boldsymbol{x}^{t-1})-g_{\eta}(\boldsymbol{\phi}_{i}^{t-1})) \Big{\|}_{2}^{2}\right]\Big{]}\] \[\leq p_{\text{in}}C_{g}^{2}\,\big{\|}\boldsymbol{x}^{t}- \boldsymbol{x}^{t-1}\big{\|}_{2}^{2}+\tfrac{2(1-p_{\text{in}})C_{g}^{2}}{S_{2}} \left(\Xi^{t}+\Xi^{t-1}\right)+3(1-p_{\text{in}})\left(\mathcal{E}_{y}^{t}+ \mathcal{E}_{y}^{t-1}\right).\]

For \(t=1\), we choose \(\tilde{\boldsymbol{y}}_{i}^{1}=\boldsymbol{y}_{i}^{1}\)

\[\mathbb{E}_{i}[\mathbb{E}_{p_{\text{in}}}[\mathbb{E}_{\eta|i}[ \big{\|}\boldsymbol{y}_{i}^{2}-\tilde{\boldsymbol{y}}_{i}^{1}\big{\|}_{2}^{2}]]] =p_{\text{in}}\,\mathbb{E}_{i}\left[\mathbb{E}_{\eta|i}\left[ \Big{\|}\tfrac{1}{S_{1}}\sum_{\eta\in H_{i}}(g_{\eta}(\boldsymbol{x}^{1})-g_{ \eta}(\boldsymbol{x}^{0}))\Big{\|}_{2}^{2}\right]\right]\] \[\leq C_{g}^{2}\,\big{\|}\boldsymbol{x}^{1}-\boldsymbol{x}^{0} \big{\|}_{2}^{2}.\]

Then for summing up \(t=1\) to \(T-1\)

\[\sum_{t=2}^{T-1}\mathbb{E}_{i}[\mathbb{E}_{p_{\text{in}}}[ \mathbb{E}_{\eta|i}[\big{\|}\boldsymbol{y}_{i}^{t+1}-\tilde{\boldsymbol{y}}_{i}^{ t}\big{\|}_{2}^{2}]]]+\mathbb{E}_{i}[\mathbb{E}_{p_{\text{in}}}[\mathbb{E}_{\eta|i}[ \big{\|}\boldsymbol{y}_{i}^{2}-\tilde{\boldsymbol{y}}_{i}^{1}\big{\|}_{2}^{2}]]]\] \[\leq(p_{\text{in}}+3(1-p_{\text{in}}))C_{g}^{2}\sum_{t=2}^{T-1} \big{\|}\boldsymbol{x}^{t}-\boldsymbol{x}^{t-1}\big{\|}_{2}^{2}+\tfrac{2(1-p_{ \text{in}})C_{g}^{2}}{S_{2}}\left(\sum_{t=2}^{T-1}\Xi^{t}+\sum_{t=2}^{T-1}\Xi^{t -1}\right)\] \[\qquad+3(1-p_{\text{in}})\left(\sum_{t=2}^{T-1}\mathcal{E}_{y}^{t }+\sum_{t=2}^{T-1}\mathcal{E}_{y}^{t-1}\right)+C_{g}^{2}\,\big{\|}\boldsymbol{x}^ {1}-\boldsymbol{x}^{0}\big{\|}_{2}^{2}\] \[\leq 4C_{g}^{2}\sum_{t=1}^{T-1}\big{\|}\boldsymbol{x}^{t}- \boldsymbol{x}^{t-1}\big{\|}_{2}^{2}+\tfrac{4(1-p_{\text{in}})C_{g}^{2}}{S_{2}} \sum_{t=0}^{T-1}\Xi^{t+1}+6(1-p_{\text{in}})\sum_{t=0}^{T-1}\mathcal{E}_{y}^{t +1}.\]

Finally, the error has the following upper bound

\[\tfrac{1}{T}\sum_{t=1}^{T-1}\mathbb{E}_{i}[\mathbb{E}_{p_{\text{in} }}[\mathbb{E}_{\eta|i}[\big{\|}\boldsymbol{y}_{i}^{t+1}-\tilde{\boldsymbol{y}}_{i}^{ t}\big{\|}_{2}^{2}]]]\] \[\leq\tfrac{4C_{g}^{2}\gamma^{2}}{T}\sum_{t=0}^{T-1}\mathbb{E}[ \big{\|}G^{t+1}\big{\|}_{2}^{2}]+\tfrac{4(1-p_{\text{in}})C_{g}^{2}}{S_{2}} \tfrac{1}{T}\sum_{t=0}^{T-1}\Xi^{t+1}+\tfrac{6(1-p_{\text{in}})}{T}\sum_{t=0}^{T- 1}\mathcal{E}_{y}^{t+1}.\]

**Lemma 13** (Bias and Variance of NestedVR): _If the step size \(\gamma\) satisfies,_

\[\gamma^{2}L_{F}^{2}\max\left\{\frac{(1-p_{\text{\rm{pu}}})}{p_{\text{\rm{u}}}S_{2 }}\frac{18}{B_{2}},\frac{1-p_{\text{\rm{pu}}}}{B_{2}}\frac{(1-p_{\text{\rm{pu}}} )}{p_{\text{\rm{u}}}S_{2}}\frac{18n^{2}}{B_{2}^{2}},\frac{(1-p_{\text{\rm{pu}} })}{B_{2}}\right\}\leq\frac{1}{16}\cdot\frac{1}{6}\]

_then the variance and bias of NestedVR are_

\[\frac{1}{T}\sum_{t=0}^{T-1}\mathcal{E}_{\text{\rm{var}}}^{t+1} \leq 32\left(\left(\frac{p_{\text{\rm{pu}}}}{B_{1}}+\frac{1-p_{\text{ \rm{pu}}}}{B_{2}}\right)\frac{(1-p_{\text{\rm{pu}}})}{p_{\text{\rm{u}}}S_{2}} \frac{18n^{2}}{B_{2}^{2}}+\frac{(1-p_{\text{\rm{pu}}})}{B_{2}}\right)\frac{ \gamma^{2}L_{F}^{2}}{T}\sum_{t=0}^{T-1}\left\|\mathbb{E}[G^{t+1}]\right\|_{2}^ {2}\] \[\qquad+96\left(\frac{p_{\text{\rm{pu}}}}{B_{1}}+\frac{(1-p_{\text {\rm{pu}}})(1-p_{\text{\rm{pu}}})}{B_{2}}\right)\frac{\tilde{L}_{F}^{2}}{S_{1}} +\frac{(1-p_{\text{\rm{pu}}})}{T}\frac{8\tilde{L}_{F}^{2}}{B_{1}S_{1}}\] \[\frac{1}{T}\sum_{t=0}^{T-1}\mathcal{E}_{\text{\rm{bias}}}^{t+1} \leq\frac{12(1-p_{\text{\rm{u}}})}{p_{\text{\rm{u}}}S_{2}}\frac{n^{2}}{B_{2}^ {2}}\frac{L_{F}^{2}\gamma^{2}}{T}\sum_{t=0}^{T-1}\left\|\mathbb{E}[G^{t+1}] \right\|_{2}^{2}+\frac{4L_{F}^{2}}{S_{1}}\] \[\qquad+\left(\frac{12(1-p_{\text{\rm{u}}})}{p_{\text{\rm{u}}}S_{2 }}\frac{n^{2}}{B_{2}^{2}}L_{F}^{2}\gamma^{2}+\frac{2(1-p_{\text{\rm{u}}})^{2} }{p_{\text{\rm{u}}}\omega}\right)\frac{1}{T}\sum_{t=0}^{T-1}\mathcal{E}_{\text {\rm{w}}}^{t+1}.\]

**Proof: Notations.** Let us define the following terms,

\[G_{i}^{t+1}:=(\bm{z}_{i}^{t+1})^{\top}\nabla f_{i}(\bm{y}_{i}^{t+1}),\quad \tilde{G}_{i}^{t}:=(\bm{z}_{i}^{t})^{\top}\nabla f_{i}(\bm{\tilde{y}}_{i}^{t}).\]

Note that the \(\tilde{G}^{t}\) computed at time \(t+1\) has same expectation as \(G^{t}\)

\[\mathbb{E}^{t+1}[\tilde{G}^{t}|t]=\mathbb{E}^{t}[G^{t}|t-1].\] (24)

**Computing the bias.** First consider the two cases in the outer loop

\[\mathcal{E}_{\text{\rm{bias}}}^{t+1} =\left\|\nabla F(\bm{x}^{t})-\mathbb{E}^{t+1}[G^{t+1}|t]\right\|_{2 }^{2}\] \[\leq 2\underbrace{\left\|\nabla F(\bm{x}^{t})-\mathbb{E}^{t+1}[G_{ i}^{t+1}|t]\right\|_{2}^{2}}_{A_{1}^{t+1}}+2\underbrace{\left\|\mathbb{E}^{t+1}[G_{ i}^{t+1}|t]-\mathbb{E}^{t+1}[G^{t+1}|t]\right\|_{2}^{2}}_{A_{2}^{t+1}}.\]

We expand \(A_{2}^{t+1}\) as follows

\[A_{2}^{t+1} =\left\|\mathbb{E}^{t+1}[G_{i}^{t+1}|t]-\mathbb{E}^{t+1}[G^{t+1}|t ]\right\|_{2}^{2}\] \[=\left\|\mathbb{E}^{t+1}[G_{i}^{t+1}|t]-p_{\text{\rm{out}}}\, \mathbb{E}^{t+1}[G_{i}^{t+1}|t]-(1-p_{\text{\rm{out}}})(G^{t}+\mathbb{E}^{t+1} [G_{i}^{t+1}-\tilde{G}_{i}^{t}|t])\right\|_{2}^{2}\] \[=(1-p_{\text{\rm{out}}})^{2}\left\|G^{t}-\mathbb{E}^{t+1}[\tilde{G }_{i}^{t}|t]\right\|_{2}^{2}\] \[=(1-p_{\text{\rm{out}}})^{2}\left\|G^{t}-\mathbb{E}^{t}[G_{i}^{t} |t-1]\right\|_{2}^{2}\]

where we use (24) in the last equality. Now we take expectation with respect to randomness at \(t\) such that \(G^{t}\) is a random variable, then

\[A_{2}^{t+1} =(1-p_{\text{\rm{out}}})^{2}\,\mathbb{E}^{t}\left[\left\|G^{t}- \mathbb{E}^{t}[G_{i}^{t}|t-1]\right\|_{2}^{2}|t-1\right]\] \[=(1-p_{\text{\rm{out}}})^{2}\left(\left\|\mathbb{E}^{t}[G^{t}|t- 1]-\mathbb{E}^{t}[G_{i}^{t}|t-1]\right\|_{2}^{2}+\mathcal{E}_{\text{\rm{var}}}^ {t}\right)\] \[=(1-p_{\text{\rm{out}}})^{2}\left(A_{2}^{t}+\mathcal{E}_{\text{ \rm{var}}}^{t}\right)\]

while at initialization we always use large batch

\[A_{2}^{1}=\left\|\mathbb{E}^{1}[G_{i}^{1}]-\mathbb{E}^{1}[G^{1}]\right\|_{2}^{2}= \left\|\mathbb{E}^{1}[G_{i}^{1}]-\mathbb{E}^{1}[G_{i}^{1}]\right\|_{2}^{2}=0.\]

Therefore, when we average over time \(t\)

\[\tfrac{1}{T}\sum_{t=0}^{T-1}A_{2}^{t+1}\leq\tfrac{(1-p_{\text{\rm{su}}})^{2}}{p_ {\text{\rm{u}}}}\tfrac{1}{T}\sum_{t=0}^{T-1}\mathcal{E}_{\text{\rm{var}}}^{t+1}.\] (25)

On the other hand, let us consider the upper bound on \(A_{1}^{t+1}\)

\[A_{1}^{t+1}\leq C_{g}^{2}L_{f}^{2}\,\mathbb{E}[\left\|\bm{y}_{i}^{t+1}-\mathbb{E} _{\eta|i}[g_{\eta}(\bm{x}^{t})]\right\|_{2}^{2}]=C_{g}^{2}L_{f}^{2}\mathcal{E} _{y}^{t+1}.\]

From Lemma 11 we know that

\[\tfrac{1}{T}\sum_{t=0}^{T-1}A_{1}^{t+1} \leq C_{g}^{2}L_{f}^{2}\left(\tfrac{(1-p_{\text{\rm{u}}})C_{g}^{2} }{p_{\text{\rm{u}}}S_{2}}\tfrac{1}{T}\sum_{t=0}^{T-1}\Xi^{t}+\tfrac{2\sigma_{g}^{2} }{S_{1}}\right)\] \[\leq\tfrac{(1-p_{\text{\rm{u}}})L_{F}^{2}}{p_{\text{\rm{u}}}S_{2}} \tfrac{1}{T}\sum_{t=0}^{T-1}\Xi^{t}+\tfrac{2\tilde{L}_{F}^{2}}{S_{1}}.\]From Lemma 10 we know that

\[\begin{split}\frac{1}{T}\sum_{t=0}^{T-1}A_{1}^{t+1}& \leq\frac{(1-p_{\text{up}})L_{F}^{2}}{p_{\text{up}}S_{2}}\left( \frac{6n^{2}}{B_{2}^{2}}\gamma^{2}\frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}\big{[} \big{\|}G^{t+1}\big{\|}_{2}^{2}\big{]}\right)+\frac{2L_{F}^{2}}{S_{1}}\\ &=\frac{6(1-p_{\text{up}})}{p_{\text{up}}S_{2}}\frac{n^{2}}{B_{2 }^{2}}\frac{L_{F}^{2}\gamma^{2}}{T}\sum_{t=0}^{T-1}\big{\|}\mathbb{E}[G^{t+1}] \big{\|}_{2}^{2}+\frac{2L_{F}^{2}}{S_{1}}+\frac{6(1-p_{\text{up}})}{p_{\text{ up}}S_{2}}\frac{n^{2}}{B_{2}^{2}}\frac{L_{F}^{2}\gamma^{2}}{T}\sum_{t=0}^{T-1} \mathcal{E}_{\text{var}}^{t+1}.\end{split}\] (26)

Therefore, the bias has the following bound

\[\begin{split}\frac{1}{T}\sum_{t=0}^{T-1}\mathcal{E}_{\text{bias}}^ {t+1}&\leq\frac{12(1-p_{\text{up}})}{p_{\text{up}}S_{2}}\frac{ n^{2}}{B_{2}^{2}}\frac{L_{F}^{2}\gamma^{2}}{T}\sum_{t=0}^{T-1}\big{\|}\mathbb{E}[G^{t+1} ]\big{\|}_{2}^{2}+\frac{4L_{F}^{2}}{S_{1}}\\ &\qquad+\left(\frac{12(1-p_{\text{up}})}{p_{\text{up}}S_{2}} \frac{n^{2}}{B_{2}^{2}}L_{F}^{2}\gamma^{2}+\frac{2(1-p_{\text{up}})^{2}}{p_{ \text{up}}}\right)\frac{1}{T}\sum_{t=0}^{T-1}\mathcal{E}_{\text{var}}^{t+1}. \end{split}\] (27)

Note that when \(p_{\text{in}}=1\) and \(S_{1}=S_{2}=m\), then this recovers BSpiderBoost in (16)

\[\frac{1}{T}\sum_{t=0}^{T-1}\mathcal{E}_{\text{bias}}^{t+1}\leq\frac{4L_{F}^{ 2}}{m}+\frac{2(1-p_{\text{up}})^{2}}{p_{\text{up}}}\frac{1}{T}\sum_{t=0}^{T- 1}\mathcal{E}_{\text{var}}^{t+1}.\]

**Computing the variance.** Let us decompose the variance into 3 parts:

\[\mathcal{E}_{\text{var}}^{t+1} =\mathbb{E}\left[\big{\|}G^{t+1}-\mathbb{E}[G^{t+1}]\big{\|}_{2}^ {2}\right]\] (28) \[=\mathbb{E}\left[\bigg{\|}G^{t+1}\pm\hat{G}^{t+1}\pm\mathbb{E}_{ \eta|i}[\hat{G}^{t+1}]-\mathbb{E}_{i}[\mathbb{E}_{\eta|i}[\hat{G}^{t+1}]] \big{\|}_{2}^{2}\right]\] \[=\underbrace{\mathbb{E}\left[\bigg{\|}G^{t+1}-\hat{G}^{t+1}\bigg{\|} _{2}^{2}\right]}_{\mathcal{E}_{\text{var},\text{in}}^{t+1}}+\underbrace{ \mathbb{E}_{i}[\big{\|}\mathbb{E}_{\eta|i}[G^{t+1}]-\mathbb{E}_{i}[\mathbb{E} _{\eta|i}[G^{t+1}]]\big{\|}_{2}^{2}\right]}_{\mathcal{E}_{\text{var},\text{in }}^{t+1}}+\underbrace{\mathbb{E}\big{[}\big{\|}G^{t+1}-\mathbb{E}_{\eta|i}[G^{t +1}]\big{\|}_{2}^{2}\big{]}}_{\mathcal{E}_{\text{var},\text{in}}^{t+1}}\]

where \(\mathcal{E}_{\text{var,out}}^{t+1}\) and \(\mathcal{E}_{\text{var,in}}^{t+1}\) are the variance of outer loop and inner loop.

**Inner Variance.** For \(t\geq 1\), we expand the inner variance

(29)Therefore, average over time \(t=0,\ldots T-1\) gives

\[\begin{array}{rl}\frac{1}{T}\sum_{t=0}^{T-1}\mathcal{E}_{\text{var,in}}^{t+1}& \leq\frac{p_{\text{int}}}{B_{1}}4C_{g}^{2}L_{f}^{2}\frac{1}{T}\sum_{t=1}^{T-1} \mathcal{E}_{y}^{t+1}+\frac{1-p_{\text{int}}}{B_{2}}\frac{1}{T}\sum_{t=1}^{T-1} \mathbb{E}_{i}[\mathbb{E}_{p_{\text{in}}}[\mathbb{E}_{\eta|i}[\left\lVert \left(G_{i}^{t+1}-\tilde{G}_{i}^{t}\right\rVert_{2}^{2}]\right\rVert]+\frac{ \mathcal{E}_{\text{var,in}}^{1}}{T^{\eta}}\\ &=\frac{p_{\text{int}}}{B_{1}}4C_{g}^{2}L_{f}^{2}\frac{1}{T}\sum_{t=0}^{T-1} \mathcal{E}_{y}^{t+1}+\frac{1-p_{\text{int}}}{B_{2}}\frac{1}{T}\sum_{t=1}^{T-1} \mathbb{E}_{i}\left[\mathbb{E}_{p_{\text{in}}}\left[\mathbb{E}_{\eta|i}\left[ \left\lVert G_{i}^{t+1}-\tilde{G}_{i}^{t}\right\rVert_{2}^{2}\right]\right] \right]+\frac{(1-p_{\text{int}})\mathcal{E}_{\text{var,in}}^{1}}{T}\\ &\leq\frac{p_{\text{int}}}{B_{1}}4C_{g}^{2}L_{f}^{2}\frac{1}{T}\sum_{t=0}^{T-1} \mathcal{E}_{y}^{t+1}+\frac{(1-p_{\text{int}})\mathcal{E}_{\text{var,in}}^{1} }{T}\\ &\qquad+\frac{2(1-p_{\text{int}})}{B_{2}}\left(\frac{C_{g}^{2}L_{f}^{2}}{T} \sum_{t=1}^{T-1}\left\lVert\boldsymbol{x}^{t}-\boldsymbol{x}^{t-1}\right\rVert _{2}^{2}+C_{g}^{2}L_{f}^{2}\frac{1}{T}\sum_{t=1}^{T-1}\mathbb{E}_{i}[\mathbb{E }_{p_{\text{in}}}[\mathbb{E}_{\eta|i}[\left\lVert\boldsymbol{y}_{i}^{t+1}- \tilde{\boldsymbol{y}}_{i}^{t}\right\rVert_{2}^{2}]]\right]\right)\\ &\leq\frac{p_{\text{int}}}{B_{1}}4C_{g}^{2}L_{f}^{2}\frac{1}{T}\sum_{t=0}^{T-1} \mathcal{E}_{y}^{t+1}+\frac{(1-p_{\text{int}})\mathcal{E}_{\text{var,in}}^{1} }{T}\\ &\qquad+\frac{2(1-p_{\text{int}})}{B_{2}}\frac{C_{g}^{2}L_{f}^{2}}{T}\sum_{t=0} ^{T-1}\mathbb{E}_{\left[\left\lVert G^{t+1}\right\rVert_{2}^{2}\right]}{T}\\ &\qquad+\frac{2(1-p_{\text{int}})}{B_{2}}C_{g}^{2}L_{f}^{2}\frac{1}{T}\sum_{t =1}^{T-1}\mathbb{E}_{i}[\mathbb{E}_{p_{\text{in}}}[\mathbb{E}_{\eta|i}[\left \lVert\boldsymbol{y}_{i}^{t+1}-\tilde{\boldsymbol{y}}_{i}^{t}\right\rVert_{2}^ {2}]]].\end{array}\]

Let us first apply Lemma 12

\[\begin{array}{rl}\frac{1}{T}\sum_{t=0}^{T-1}\mathcal{E}_{\text{var,in}}^{t+1} \leq\frac{p_{\text{int}}}{B_{1}}4C_{g}^{2}L_{f}^{2}\frac{1}{T}\sum_{t=0}^{T-1} \mathcal{E}_{y}^{t+1}+\frac{(1-p_{\text{int}})\mathcal{E}_{\text{var,in}}^{1} }{B_{2}}+\frac{2(1-p_{\text{int}})}{B_{2}}\frac{C_{g}^{2}L_{f}^{2}\gamma^{2}} {T}\sum_{t=0}^{T-1}\mathbb{E}[\left\lVert G^{t+1}\right\rVert_{2}^{2}]\\ \qquad\qquad+\frac{2(1-p_{\text{int}})\mathcal{E}_{g}^{2}L_{f}^{2}}{B_{2}} \left(\frac{4C_{g}^{2}\gamma^{2}}{T}\sum_{t=0}^{T-1}\mathbb{E}[\left\lVert G ^{t+1}\right\rVert_{2}^{2}]+\frac{4(1-p_{\text{in}})C_{g}^{2}}{S_{2}}\frac{1 }{T}\sum_{t=0}^{T-1}\Xi^{t}+\frac{6(1-p_{\text{in}})}{T}\sum_{t=0}^{T-1} \mathcal{E}_{y}^{t+1}\right)\\ \leq\left(\frac{p_{\text{int}}}{B_{1}}+\frac{(1-p_{\text{in}})(1-p_{\text{ int}})}{B_{2}}\right)\frac{12C_{g}^{2}L_{f}^{2}}{T}\sum_{t=0}^{T-1} \mathcal{E}_{y}^{t+1}+\frac{8(1-p_{\text{int}})L_{F}^{2}\gamma^{2}}{B_{2}} \frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}[\left\lVert G^{t+1}\right\rVert_{2}^{2}] \\ \qquad\qquad+\frac{8(1-p_{\text{int}})(1-p_{\text{int}})C_{g}^{4}L_{f}^{2}}{B_{2} S_{2}}\frac{1}{T}\sum_{t=0}^{T-1}\Xi^{t}+\frac{(1-p_{\text{int}}) \mathcal{E}_{\text{var,in}}^{1}}{T}\end{array}\]

Then we apply Lemma 11 on the bound of \(\frac{1}{T}\sum_{t=0}^{T-1}\mathcal{E}_{\text{var,in}}^{t+1}\)

\[\begin{array}{rl}\frac{1}{T}\sum_{t=0}^{T-1}\mathcal{E}_{\text{var,in}}^{t+1} \leq 24\left(\frac{p_{\text{int}}}{B_{1}}+\frac{(1-p_{\text{int}})(1-p_{ \text{int}})}{B_{2}}\right)\left(\frac{(1-p_{\text{in}})L_{F}^{2}}{p_{\text{in }}S_{2}}\frac{1}{T}\sum_{t=0}^{T-1}\Xi^{t}+\frac{L_{F}^{2}}{S_{1}}\right)\\ \qquad\qquad+\frac{8(1-p_{\text{int}})L_{F}^{2}\gamma^{2}}{B_{2}}\frac{1}{T}\sum_{ t=0}^{T-1}\mathbb{E}[\left\lVert G^{t+1}\right\rVert_{2}^{2}]\\ \qquad\qquad+\frac{8(1-p_{\text{int}})(1-p_{\text{int}})C_{g}^{4}L_{f}^{2}}{B_{2} S_{2}}\frac{1}{T}\sum_{t=0}^{T-1}\Xi^{t}+\frac{(1-p_{\text{int}}) \mathcal{E}_{\text{var,in}}^{1}}{T}\\ \leq 24\left(\frac{p_{\text{int}}}{B_{1}}+\frac{(1-p_{\text{int}})(1-p_{ \text{int}})}{B_{2}}+\frac{p_{\text{in}}(1-p_{\text{int}})}{B_{2}}\right)\frac{(1 -p_{\text{in}})L_{F}^{2}}{p_{\text{in}}S_{2}}\frac{1}{T}\sum_{t=0}^{T-1}\Xi^{ t}\\ \qquad\qquad+\frac{8(1-p_{\text{int}})L_{F}^{2}\gamma^{2}}{B_{2}}\frac{1}{T}\sum_{ t=0}^{T-1}\mathbb{E}[\left\lVert G^{t+1}\right\rVert_{2}^{2}]\\ \qquad\qquad+24\left(\frac{p_{\text{int}}}{B_{1}}+\frac{(1-p_{\text{in}})(1-p_{ \text{int}})}{B_{2}}\right)\frac{L_{F}^{2}}{S_{1}}+\frac{(1-p_{\text{out}}) }{T}\mathcal{E}_{\text{var,in}}^{1}.\end{array}\]

From Lemma 10, we plug in the upper bound of \(\frac{1}{T}\sum_{t=0}^{T-1}\Xi^{t}\)

\[\begin{array}{rl}\frac{1}{T}\sum_{t=0}^{T-1}\mathcal{E}_{\text{var,in}}^{t+1} \leq 24\left(\frac{p_{\text{int}}}{B_{1}}+\frac{1-p_{\text{int}}}{B_{2}} \right)\frac{(1-p_{\text{in}})L_{F}^{2}}{p_{\text{in}}S_{2}}\left(\frac{6n^{2} }{B_{2}^{2}}\gamma^{2}\frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}[\left\lVert G^{t+1} \right\rVert_{2}^{2}]\right)\\ \qquad\qquad+\frac{8(1-p_{\text{int}})L_{F}^{2}\gamma^{2}}{B_{2}}\frac{1}{T} \sum_{t=0}^{T

**Outer Variance.** Now we consider the outer variance for \(t\geq 1\)

\[\mathcal{E}_{\text{var,out}}^{t+1} \leq\tfrac{(1-\text{P}_{\text{int}})^{2}}{B_{2}}\,\mathbb{E}_{i} \left[\left\|\mathbb{E}_{\eta|i}[G_{i}^{t+1}]-\mathbb{E}_{\eta|i}[\tilde{G}_{i} ^{t}]\right\|_{2}^{2}\right]\] \[\leq\tfrac{(1-\text{P}_{\text{int}})^{2}}{B_{2}}\,\mathbb{E}_{i} \left[\mathbb{E}_{\eta|i}\left[\left\|G_{i}^{t+1}-\tilde{G}_{i}^{t}\right\|_{2 }^{2}\right]\right].\]

Compared to (27) we know that the upper bound of is smaller than that of \(\mathcal{E}_{\text{var,in}}^{t+1}\). Besides, whereas \(\mathcal{E}_{\text{var,out}}^{1}=0\) as we use large batch at \(t=0\). Therefore, the upper bound of \(\mathcal{E}_{\text{var}}^{t+1}\) is upper bounded by \(2^{\bullet}(30)\).

**Variance of \(\nabla g_{\widetilde{\eta}}\).** From Lemma 9, we know that

\[\tfrac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}[\mathcal{E}_{\nabla g}^{t+ 1}] \leq\tfrac{1}{B_{1}}\tfrac{C_{t}^{2}\sigma_{x}^{2}}{m}+\tfrac{4(1- \text{P}_{\text{int}})}{B_{2}\text{P}_{\text{int}}}\tfrac{1}{T}\sum_{t=0}^{T- 1}\left(\mathbb{E}\left[\left\|G_{i}^{t+1}-\tilde{G}_{i}^{t}\right\|_{2}^{2} \right]\right)\] \[\leq\tfrac{1}{B_{1}}\tfrac{C_{t}^{2}\sigma_{x}^{2}}{m}+\tfrac{1} {m}\mathcal{E}_{\text{var}}^{t+1}\]

Finally, we use \(\mathbb{E}[\left\|G^{t+1}\right\|_{2}^{2}]=\left\|\mathbb{E}[G^{t+1}]\right\|_ {2}^{2}+\mathcal{E}_{\text{var}}^{t+1}\).

\[\tfrac{1}{T}\sum_{t=0}^{T-1}\mathcal{E}_{\text{var}}^{t+1} \leq 16\left(\left(\tfrac{\text{P}_{\text{int}}}{B_{1}}+\tfrac{1- \text{P}_{\text{int}}}{B_{2}}\right)\tfrac{(1-\text{P}_{\text{int}})}{B_{2} \text{P}_{\text{int}}}\tfrac{18n^{2}}{B_{2}^{2}}+\tfrac{(1-\text{P}_{\text{ int}})}{B_{2}}\right)\tfrac{\gamma^{2}L_{F}^{2}}{T}\sum_{t=0}^{T-1}\left\|\mathbb{E}[G^{t+1}] \right\|_{2}^{2}\] \[\qquad+16\left(\left(\tfrac{\text{P}_{\text{int}}}{B_{1}}+\tfrac {1-\text{P}_{\text{int}}}{B_{2}}\right)\tfrac{(1-\text{P}_{\text{int}})}{B_{2 }}\tfrac{18n^{2}}{B_{2}^{2}}+\tfrac{(1-\text{P}_{\text{int}})}{B_{2}}\right) \tfrac{\gamma^{2}L_{F}^{2}}{T}\sum_{t=0}^{T-1}\mathcal{E}_{\text{var}}^{t+1}\] \[\qquad+48\left(\tfrac{\text{P}_{\text{int}}}{B_{1}}+\tfrac{(1- \text{P}_{\text{int}})(1-\text{P}_{\text{int}})}{B_{2}}\right)\tfrac{L_{F}^{2 }}{S_{1}}+\tfrac{(1-\text{P}_{\text{int}})}{T}\tfrac{8L_{F}^{2}}{B_{1}S_{1}}.\]

By taking step size \(\gamma\) to satisfy

\[\gamma^{2}L_{F}^{2}\max\left\{\tfrac{\text{P}_{\text{int}}}{B_{1}}\tfrac{(1- \text{P}_{\text{int}})}{B_{2}}\tfrac{18n^{2}}{B_{2}^{2}},\tfrac{1-\text{P}_{ \text{int}}}{B_{2}}\tfrac{(1-\text{P}_{\text{int}})}{\text{P}_{\text{int}}S_{ 2}}\tfrac{18n^{2}}{B_{2}^{2}},\tfrac{(1-\text{P}_{\text{int}})}{B_{2}}\right\} \leq\tfrac{1}{16}\cdot\tfrac{1}{6}\]

which can be simplified to

\[\gamma^{2}L_{F}^{2}\max\left\{\tfrac{(1-\text{P}_{\text{int}})}{\text{P}_{ \text{int}}S_{2}}\tfrac{18}{B_{2}},\tfrac{1-\text{P}_{\text{int}}}{B_{2}} \tfrac{(1-\text{P}_{\text{int}})}{\text{P}_{\text{int}}S_{2}}\tfrac{18n^{2}}{B _{2}^{2}},\tfrac{(1-\text{P}_{\text{int}})}{B_{2}}\right\}\leq\tfrac{1}{16}\cdot \tfrac{1}{6}.\]

Then the coefficient of \(\tfrac{1}{T}\sum_{t=0}^{T-1}\mathcal{E}_{\text{var}}^{t+1}\) is bounded by \(\tfrac{1}{2}\)

\[16\left(\left(\tfrac{\text{P}_{\text{int}}}{B_{1}}+\tfrac{1-\text{P}_{\text{ int}}}{B_{2}}\right)\tfrac{(1-\text{P}_{\text{int}})}{\text{P}_{\text{int}}S_{2}} \tfrac{18n^{2}}{B_{2}^{2}}+\tfrac{(1-\text{P}_{\text{int}})}{B_{2}}\right) \gamma^{2}L_{F}^{2}\leq\tfrac{1}{2}.\]

The the variance has the following bound

\[\tfrac{1}{T}\sum_{t=0}^{T-1}\mathcal{E}_{\text{var}}^{t+1} \leq 32\left(\left(\tfrac{\text{P}_{\text{int}}}{B_{1}}+\tfrac{1- \text{P}_{\text{int}}}{B_{2}}\right)\tfrac{(1-\text{P}_{\text{int}})}{\text{P}_{ \text{int}}S_{2}}\tfrac{18n^{2}}{B_{2}^{2}}+\tfrac{(1-\text{P}_{\text{int}})}{ B_{2}}\right)\tfrac{\gamma^{2}L_{F}^{2}}{T}\sum_{t=0}^{T-1}\left\|\mathbb{E}[G^{t+1}] \right\|_{2}^{2}\] \[\qquad+96\left(\tfrac{\text{P}_{\text{int}}}{B_{1}}+\tfrac{(1- \text{P}_{\text{int}})(1-\text{P}_{\text{int}})}{B_{2}}\right)\tfrac{\tilde{L}_{F}^ {2}}{S_{1}}+\tfrac{(1-\text{P}_{\text{int}})}{T}\tfrac{8\tilde{L}_{F}^{2}}{B_{1}S_ {1}}.\]

**Theorem 10**: _Consider the (FCCO) problem. Suppose Assumptions 3, 4, 5 holds true. Let step size \(\gamma=\mathcal{O}(\tfrac{1}{\sqrt{nL_{F}}})\). Then for NestedVR, \(\bm{x}^{s}\) picked uniformly at random among \(\{\bm{x}^{t}\}_{t=0}^{T-1}\) satisfies: \(\mathbb{E}[\left\|\nabla F(\bm{x}^{s})\right\|_{2}^{2}]\leq\varepsilon^{2}\), for nonconvex \(F\), if the hyperparameters of the inner loop \(S_{1}=\mathcal{O}(\tilde{L}_{F}^{2}\varepsilon^{-2}),S_{2}=\mathcal{O}(\tilde{L}_ {F}\varepsilon^{-1}),\text{p}_{\text{in}}=\mathcal{O}(1/S_{2})\), the hyperparameters of the outer loop \(B_{1}=n,B_{2}=\sqrt{n},p_{\text{out}}=1/B_{2}\), and the number of iterations_

\[T=\Omega\left(\tfrac{\sqrt{nL_{F}}(F(\bm{x}^{0})-F^{*})}{\varepsilon^{2}}\right).\]

_The resulting sample complexity is_

\[\mathcal{O}\left(\tfrac{nL_{F}L_{F}(F(\bm{x}^{0})-F^{*})}{\varepsilon^{3}}\right).\]

_In fact, it reaches this sample complexity for all \(\tfrac{J_{\text{int}}p_{\text{out}}}{\sqrt{1-p_{\text{in}}}}\lesssim\varepsilon\)._

**Proof:** Using descent lemma (Lemma 4) and bias-variance bounds of NestedVR (Lemma 13)

\[\frac{1}{T}\sum_{t=0}^{T-1}\left\|\nabla F(\bm{x}^{t})\right\|_{2}^{ 2}+\frac{1}{2T}\sum_{t=0}^{T-1}\left\|\operatorname{\mathbb{E}}[G^{t+1}] \right\|_{2}^{2}\] \[\leq\frac{2(F(\bm{x}^{0})-F^{*})}{\gamma^{T}}+\frac{L_{F}\gamma} {T}\sum_{t=0}^{T-1}\operatorname{\mathbb{E}}_{\text{var}}^{t+1}+\frac{1}{T} \sum_{t=0}^{T-1}\operatorname{\mathbb{E}}_{\text{bias}}^{t+1}\] \[\leq\frac{2(F(\bm{x}^{0})-F^{*})}{\gamma^{T}}+\underbrace{\frac{4 L_{F}^{2}}{T_{1}}}_{\mathcal{T}_{1}}+\underbrace{\frac{12(1-p_{0})}{p_{0}S_{2}}}_{ \mathcal{T}_{2}}\frac{n^{2}}{B_{2}^{2}}\frac{L_{F}^{2}\gamma^{2}}{\sum_{t=0}^ {T-1}\left\|\operatorname{\mathbb{E}}[G^{t+1}]\right\|_{2}^{2}}_{\mathcal{T}_ {2}}\] \[\qquad+\underbrace{\left(\frac{12(1-p_{0})}{p_{0}S_{2}}\frac{n^{ 2}}{B_{2}^{2}}L_{F}^{2}\gamma^{2}+\frac{2(1-p_{0})^{2}}{p_{0}}+\gamma L_{F} \right)\frac{1}{T}\sum_{t=0}^{T-1}\operatorname{\mathbb{E}}_{\text{var}}^{t+ 1}}_{\mathcal{T}_{3}}.\]

**Compute \(\mathcal{T}_{0}\).** In order to let \(\mathcal{T}_{0}\leq\varepsilon^{2}\), we require that

\[\gamma T\geq\varepsilon^{-2}.\] (31)

**Compute \(\mathcal{T}_{1}\).** In order to let \(\mathcal{T}_{1}\) to be smaller than \(\varepsilon^{2}\), we need

\[S_{1}=\frac{4L_{F}^{2}}{\varepsilon^{2}}.\]

**Compute \(\mathcal{T}_{2}\).** In order to let the coefficient of \(\frac{1}{T}\sum_{t=0}^{T-1}\left\|\operatorname{\mathbb{E}}[G^{t+1}]\right\|_ {2}^{2}\) in \(\mathcal{T}_{2}\) to be less than \(\frac{1}{4}\), i.e.

\[\frac{12(1-p_{0})}{p_{0}S_{2}}\frac{n^{2}}{B_{2}^{2}}L_{F}^{2}\gamma^{2}\leq \frac{1}{4},\] (32)

which requires \(\gamma\)

\[\gamma\leq\frac{B_{2}\sqrt{p_{0}S_{2}}}{7L_{F}n\sqrt{1-p_{0}}}=\frac{p_{0}p_{ 0}L_{F}}{7\varepsilon L_{F}\sqrt{1-p_{0}}}.\] (33)

**Compute \(\mathcal{T}_{3}\).** Let us now focus on \(\mathcal{T}_{3}\) and notice that the middle term \(\frac{2(1-p_{0})^{2}}{p_{0}}\)

\[\frac{2(1-p_{0})^{2}}{p_{0}}\frac{1}{T}\sum_{t=0}^{T-1}\operatorname{\mathbb{ E}}_{\text{var}}^{t+1}.\]

Using Lemma 13 we have that

\[\frac{2(1-p_{0})^{2}}{p_{0}}\frac{1}{T}\sum_{t=0}^{T-1} \operatorname{\mathbb{E}}_{\text{var}}^{t+1}\] \[\leq\underbrace{32\frac{2(1-p_{0})^{2}}{p_{0}}}_{\text{out}} \left(\left(\frac{p_{0}}{B_{1}}+\frac{1-p_{0}}{B_{2}}\right)\frac{(1-p_{0})}{ p_{0}S_{2}}\frac{18n^{2}}{B_{2}^{2}}+\frac{(1-p_{0})}{B_{2}}\right)\gamma^{2}L_{F}^{2} \frac{1}{T}\sum_{t=0}^{T-1}\left\|\operatorname{\mathbb{E}}[G^{t+1}]\right\|_ {2}^{2}}_{\mathcal{T}_{3,1}}\] \[\qquad+\underbrace{96\frac{2(1-p_{0})^{2}}{p_{0}}\left(\frac{p_{ 0}}{B_{1}}+\frac{(1-p_{0})(1-p_{0})}{B_{2}}\right)\frac{\tilde{L}_{F}^{2}}{S_ {1}}}_{\mathcal{T}_{3,2}}+\underbrace{\frac{2(1-p_{0})^{2}}{p_{0}}\frac{(1- p_{0})}{T}\frac{8\tilde{L}_{F}^{2}}{B_{1}S_{1}}}_{\mathcal{T}_{3,3}}.\]

* Compute \(\mathcal{T}_{3,3}\): As we already know that \(S_{1}=\mathcal{O}(\varepsilon^{-2})\) and \(T\geq 1\) and \(B_{1}p_{\text{out}}\geq 1\). This imposes no more constraints, i.e. \[S_{1}=\mathcal{O}\left(\frac{\tilde{L}_{F}^{2}}{\varepsilon^{2}}\right).\]
* Compute \(\mathcal{T}_{3,2}\): As \(S_{1}=\mathcal{O}(\varepsilon^{-2})\) and \(B_{1}=n\) and \(B_{2}=B_{1}p_{\text{out}}\), then it requires \[\frac{(1-p_{0})(1-p_{0})^{3}}{p_{0}^{2}}\leq n.\]
* Compute \(\mathcal{T}_{3,1}\): In order to satisfy the following \[32\frac{2(1-p_{0})^{2}}{p_{0}}\left(\left(\frac{p_{0}}{B_{1}}+\frac{1-p_{0}}{B_ {2}}\right)\frac{(1-p_{0})}{p_{0}S_{2}}\frac{18n^{2}}{B_{2}^{2}}+\frac{(1-p_{0 })}{B_{2}}\right)\gamma^{2}L_{F}^{2}\leq\frac{1}{12}\] we need to enforce \[\gamma\leq\frac{p_{0}p_{0}}{\varepsilon L_{F}(1-p_{0})^{1/2}(1-p_{0})^{3/2}}.\] (34)Now we go back to \(\mathcal{T}_{3}\) and compare the other two coefficients

\[\tfrac{12(1-p_{\text{in}})}{p_{\text{in}}S_{2}}\tfrac{n^{2}}{B_{2}^{2}}L_{F}^{2} \gamma^{2}+\tfrac{2(1-p_{\text{out}})^{2}}{p_{\text{out}}}+\gamma L_{F}.\]

As \(\gamma L_{F}\leq\tfrac{1}{2}\lesssim\tfrac{2(1-p_{\text{out}})^{2}}{p_{\text{ out}}}\) we can safely ignore \(\gamma L_{F}\). On the other hand, from (32) we know that the first term is also have

\[\tfrac{12(1-p_{\text{in}})}{p_{\text{in}}S_{2}}\tfrac{n^{2}}{B_{2}^{2}}L_{F}^{ 2}\gamma^{2}\leq\tfrac{1}{4}\lesssim\tfrac{2(1-p_{\text{out}})^{2}}{p_{\text{ out}}}.\]

**Constraints from the Bias-Variance Lemma (Lemma 13).** By setting \(B_{1}=n\) and \(S_{1}=\mathcal{O}(\tfrac{L_{F}^{2}}{\varepsilon^{2}})\), this constraint translates to

\[\gamma^{2}L_{F}^{2}\max\left\{\tfrac{(1-p_{\text{in}})}{p_{\text{in}}^{2}} \tfrac{\varepsilon^{2}}{B_{2}},\tfrac{1-p_{\text{out}}}{B_{2}}\tfrac{(1-p_{ \text{in}})\varepsilon^{2}}{p_{\text{in}}^{2}}\tfrac{1}{p_{\text{out}}^{2}}, \tfrac{(1-p_{\text{out}})}{B_{2}}\right\}\lesssim 1\]

which is weaker than (33).

**Summary on the Limit on \(\gamma\).** Combine (33) and (34) and \(\gamma\leq\tfrac{1}{2L_{F}}\), we have a final limit on step size \(\gamma\)

\[\gamma\lesssim\min\left\{\tfrac{p_{\text{out}}p_{\text{in}}L_{F}}{ \varepsilon L_{F}\sqrt{1-p_{\text{in}}}},\tfrac{1}{L_{F}}\right\}\] (35)

Then the total sample complexity of NestedVR can be computed as

\[(\text{\# of iters }T)\times(\text{Avg. outer batch size }B_{2}=B_{1}p_{\text{out}}) \times(\text{Avg. inner batch size }S_{2}=S_{1}p_{\text{in}}).\]

This sample complexity has the following requirement

\[B_{2}S_{2}T=\frac{B_{2}S_{2}(T\gamma)}{\gamma}\overset{\eqref{eq:NestedVR}}{ \geq}\frac{B_{2}S_{2}}{\varepsilon^{2}\gamma}=\frac{n\varepsilon^{-2}}{ \varepsilon^{2}}\tfrac{p_{\text{End}}}{\gamma}\overset{\eqref{eq:NestedVR}}{ \gtrsim}n\varepsilon^{-3}.\]

The lower bound \(n\varepsilon^{-3}\) is reached when in (35) we have

\[\tfrac{p_{\text{out}}p_{\text{in}}L_{F}}{\varepsilon L_{F}\sqrt{1-p_{\text{in }}}}\lesssim\tfrac{1}{L_{F}}.\]

That is, \(\tfrac{p_{\text{out}}p_{\text{in}}}{\sqrt{1-p_{\text{in}}}}\lesssim\varepsilon\).

In particular, we can choose the following hyperparameters to reach \(\mathcal{O}(n\varepsilon^{-3})\) sample complexity

\[B_{1}=n,\quad B_{2}=\sqrt{n},\quad p_{\text{out}}=\tfrac{1}{\sqrt{n}},\quad S _{1}=\mathcal{O}(\tilde{L}_{F}^{2}\varepsilon^{-2}),\quad S_{2}=\mathcal{O}( \tilde{L}_{F}\varepsilon^{-1}),\quad p_{\text{in}}=\mathcal{O}(\tilde{L}_{F}^ {-1}\varepsilon)\]

The step size \(\gamma\) can be chosen as

\[\gamma\lesssim\tfrac{1}{\sqrt{n}L_{F}}.\]

and the iteration complexity

\[T=\Omega\left(\tfrac{\sqrt{n}L_{F}(F(\bm{x}^{0})-F^{*})}{\varepsilon^{2}} \right).\]

Putting these together gives the claimed sample complexity bound. By picking \(\bm{x}^{s}\) uniformly at random among \(\{\bm{x}^{t}\}_{t=0}^{T-1}\), we get the desired guarantee. 

### Convergence of E-NestedVR

In this section, we analyze the sample complexity of Algorithm 1 (E-NestedVR) for the FCCO problem with

\[G_{\text{E-NVR}}^{t+1}=\begin{cases}\frac{1}{B_{1}}\sum_{i}(\bm{z}_{i}^{t+1})^ {\top}\mathcal{L}_{\mathcal{D}_{y,i}^{t+1}}^{(2)}\nabla f_{i}(0)&\text{with prob. }p_{\text{out}}\\ G_{\text{E-NVR}}^{t}+\frac{1}{B_{2}}\sum_{i}\left((\bm{z}_{i}^{t+1})^{\top} \mathcal{L}_{\mathcal{D}_{y,i}^{t+1}}^{(2)}\nabla f_{i}(0)-(\bm{z}_{i}^{t})^{ \top}\mathcal{L}_{\mathcal{D}_{y,i}^{t}}^{(2)}\nabla f_{i}(0)\right)&\text{ with prob. }1-p_{\text{out}}.\end{cases}\] (36)

**Lemma 14** (Bias and Variance of E-NestedVR): _If the step size \(\gamma\) satisfies_

\[\gamma^{2}L_{F}^{2}\max\left\{\frac{(1-p_{\text{in}})}{p_{\text{in}}S_{2}}\frac{ 18}{B_{2}},\frac{1-p_{\text{out}}}{B_{2}}\frac{(1-p_{\text{in}})}{p_{\text{in}}S _{2}}\frac{18n^{2}}{B_{2}^{2}},\frac{(1-p_{\text{out}})}{B_{2}}\right\}\leq \frac{1}{16}\cdot\frac{1}{6}\]

_then the variance and bias of E-NestedVR are_

\[\frac{1}{T}\sum_{t=0}^{T-1}\mathcal{E}_{\text{wr}}^{t+1} \leq 14\cdot 32\left(\left(\frac{p_{\text{out}}}{B_{1}}+\frac{1-p_{ \text{out}}}{B_{2}}\right)\frac{(1-p_{\text{in}})}{p_{\text{in}}S_{2}}\frac{18 n^{2}}{B_{2}^{2}}+\frac{(1-p_{\text{in}})}{B_{2}}\right)\frac{\gamma^{2}L_{F}^{2}}{T} \sum_{t=0}^{T-1}\left\|\mathbb{E}[G^{t+1}]\right\|_{2}^{2}\] \[\qquad+14\cdot 96\left(\frac{p_{\text{out}}}{p_{\text{in}}}+\frac{(1-p _{\text{in}})(1-p_{\text{out}})}{B_{2}}\right)\frac{L_{F}^{2}}{S_{1}}+\frac{(1 -p_{\text{out}})}{T}\frac{8L_{F}^{2}}{B_{1}S_{1}}.\] \[\frac{1}{T}\sum_{t=0}^{T-1}\mathcal{E}_{\text{bias}}^{t+1} \leq\frac{(1-p_{\text{in}})^{3}L_{F}^{2}}{p_{\text{in}}S_{2}}\gamma^{2} \frac{1}{T}\sum_{t=0}^{T-1}\left\|\mathbb{E}[G^{t+1}]\right\|_{2}^{2}+\frac{2( 1-p_{\text{in}})^{2}L_{F}^{2}}{S_{1}}+\frac{C_{F}^{2}}{S_{2}^{2}}\] \[\qquad+\left(\frac{(1-p_{\text{in}})^{3}L_{F}^{2}}{p_{\text{in}}S _{2}}\frac{6n^{2}}{B_{2}^{2}}\gamma^{2}+\frac{(1-p_{\text{out}})^{2}}{p_{\text {out}}}\right)\frac{1}{T}\sum_{t=0}^{T-1}\mathcal{E}_{\text{wr}}^{t+1}.\]

**Proof:** Note that this proof is very similar to NestedVR so we highlight the differences. Let \(G^{t+1}=G_{\text{E-NVR}}^{t+1}\) (36) be the E-NestedVR update and define

\[G_{i}^{t+1}:=(\bm{z}_{i}^{t+1})^{\top}\mathcal{L}_{\mathcal{D}_{\bm{y},i}^{t+ 1}}^{(2)}\nabla f_{i}(0)\]

We expand the bias by inserting \(\mathbb{E}_{i,p_{\text{in}},\eta|i}[G_{i}^{t+1}]\)

\[\mathcal{E}_{\text{bias}}^{t+1} =\left\|\nabla F(\bm{x}^{t+1})-\mathbb{E}[G^{t+1}]\right\|_{2}^{2}\] \[\leq 2\underbrace{\left\|\nabla F(\bm{x}^{t+1})-\mathbb{E}_{i,p_{ \text{in}},\eta,\bar{\eta}|i}[G_{i}^{t+1}]\right\|_{2}^{2}}_{A_{1}^{t+1}}+2 \underbrace{\left\|\mathbb{E}_{i,p_{\text{in}},\eta,\bar{\eta}|i}[G_{i}^{t+1}] -\mathbb{E}[G^{t+1}]\right\|_{2}^{2}}_{A_{2}^{t+1}}.\]

**Consider \(A_{1}^{t+1}\).** The term \(A_{1}^{t+1}\) captures the difference between full gradient and extrapolated gradient

\[A_{1}^{t+1} =\left\|\mathbb{E}_{i}\left[(\mathbb{E}_{\bar{\eta}|i}[\nabla g_{ \bar{\eta}}(\bm{x}^{t})])^{\top}\nabla f_{i}(\mathbb{E}[g_{\eta}(\bm{x}^{t}) ])-\mathbb{E}_{p_{\text{in}},\eta|i}\left[(\mathbb{E}_{\bar{\eta}|i}[\nabla g_{ \bar{\eta}}(\bm{x}^{t})])^{\top}\mathcal{L}_{\mathcal{D}_{\bm{y},i}^{t+1}}^{(2 )}\nabla f_{i}(0)\right]\right]\right\|_{2}^{2}\] \[\leq C_{g}^{2}\underbrace{\mathbb{E}_{i}\left[\left\|\nabla f_{i }(\mathbb{E}_{\eta|i}[g_{\eta}(\bm{x}^{t})])-\mathbb{E}_{p_{\text{in}},\eta|i} \left[\mathcal{L}_{\mathcal{D}_{\bm{y},i}^{t+1}}^{(2)}\nabla f_{i}(0)\right] \right\|_{2}^{2}\right]}_{=:A_{1,1}^{t+1}}\] \[\qquad+2C_{g}^{2}\underbrace{\mathbb{E}_{i}\left[\left\|\mathbb{E }_{p_{\text{in}}}[\nabla f_{i}(\mathbb{E}_{\eta|i}[\bm{y}_{i}^{t+1}])]- \mathbb{E}_{p_{\text{in}},\eta|i}\left[\mathcal{L}_{\mathcal{D}_{\bm{y},i}^{t +1}}^{(2)}\nabla f_{i}(0)\right]\right\|_{2}^{2}\right]}_{=:A_{1,2}^{t+1}}.\]

The first term \(A_{1,1}^{t+1}\) can be upper bounded through smoothness of \(f_{\xi}\), for \(t\geq 1\)

\[A_{1,1}^{t+1} =\mathbb{E}_{i}\left[\left\|\nabla f_{i}(\mathbb{E}_{\eta|i}[g_{ \eta}(\bm{x}^{t})])-p_{\text{in}}\nabla f_{i}(\mathbb{E}_{\eta|i}[g_{\eta}( \bm{x}^{t})])-(1-p_{\text{in}})\nabla f_{i}(\bm{y}_{i}^{t}+\mathbb{E}_{\eta|i}[ g_{\eta}(\bm{x}^{t})-g_{\eta}(\bm{\phi}_{i}^{t})])\right\|_{2}^{2}\right]\] \[=(1-p_{\text{in}})^{2}\,\mathbb{E}_{i}\left[\left\|\nabla f_{i}( \mathbb{E}[g_{\eta}(\bm{x}^{t})])-\nabla f_{i}(\bm{y}_{i}^{t}+\mathbb{E}_{\eta|i} [g_{\eta}(\bm{x}^{t})-g_{\eta}(\bm{\phi}_{i}^{t})])\right\|_{2}^{2}\right]\] \[\leq(1-p_{\text{in}})^{2}L_{F}^{2}\,\mathbb{E}_{i}\left[\left\| \mathbb{E}_{\eta|i}[g_{\eta}(\bm{x}^{t})]-(\bm{y}_{i}^{t}+\mathbb{E}_{\eta|i}[g_{ \eta}(\bm{x}^{t})-g_{\eta}(\bm{\phi}_{i}^{t})])\right\|_{2}^{2}\right]\] \[=(1-p_{\text{in}})^{2}L_{F}^{2}\,\mathbb{E}_{i}\left[\left\|\bm{y }_{i}^{t}-\mathbb{E}_{\eta|i}[g_{\eta}(\bm{\phi}_{i}^{t})]\right\|_{2}^{2}\right]\] \[=(1-p_{\text{in}})^{2}L_{F}^{2}\mathcal{E}_{y}^{t}.\]

For \(t=0\), \(A_{1,1}^{1}=0\), then

\[\tfrac{1}{T}\sum_{t=0}^{T-1}A_{1,1}^{t+1}\leq(1-p_{\text{in}})^{2}L_{f}^{2}C_{g}^{2 }\tfrac{1}{T}\sum_{t=0}^{T-1}\mathcal{E}_{y}^{t+1}.\] (37)On the other hand, with Lemma 6

\[A_{1,2}^{t+1} \leq p_{\text{in}}\,\mathbb{E}_{i}\left[\left\|\nabla f_{i}(\mathbb{E} _{\eta|i}[g_{\eta}(\bm{x}^{t})])-\mathbb{E}_{\eta|i}\left[\mathcal{L}_{\mathcal{ D}_{\bm{y},S_{1,i}}^{(2)}}^{(2)}\nabla f_{i}(0)\right]\right\|_{2}^{2}\right]\] \[\qquad+(1-p_{\text{in}})\,\mathbb{E}_{i}\left[\left\|\nabla f_{i} (\bm{y}_{i}^{t}+\mathbb{E}_{\eta|i}[g_{\eta}(\bm{x}^{t})-g_{\eta}(\bm{\phi}_{i} ^{t})])-\mathbb{E}_{\eta|i}\left[\mathcal{L}_{\mathcal{D}_{\bm{y},S_{2,i}}^{(2) }}^{(2)}\nabla f_{i}(0)\right]\right\|_{2}^{2}\right]\] \[\leq\frac{p_{\text{in}}C_{2}^{2}}{S_{1}^{4}}+\frac{(1-p_{\text{ in}})C_{2}^{2}}{S_{2}^{4}}\] \[\leq\frac{C_{2}^{2}}{S_{2}^{4}}\]

where \(\mathcal{D}_{\bm{y},S_{1},i}^{t+1}\) is the distribution of \(\frac{1}{S_{1}}\sum_{\eta\in\mathcal{S}_{1}}\bm{g}_{\eta}(\bm{x}^{t})\) and \(\mathcal{D}_{\bm{y},S_{2},i}^{t+1}\) is the distribution of

\[\bm{y}_{i}^{t}+\frac{1}{S_{2}}\sum_{\eta\in\mathcal{S}_{2}}(g_{\eta}(\bm{x}^{ t})-\mathbb{E}[g_{\eta}(\bm{\phi}_{i}^{t})]).\]

Thus the \(A_{1}^{t+1}\) has the following upper bound

\[\frac{1}{T}\sum_{t=0}^{T-1}A_{1}^{t+1}\leq(1-p_{\text{in}})^{2}L_{f}^{2}C_{g} ^{2}\frac{1}{T}\sum_{t=0}^{T-1}\mathcal{E}_{y}^{t+1}+\frac{C_{2}^{2}}{S_{2}^{4}}.\] (38)

**Consider \(A_{2}^{t+1}\).** Let us expand \(A_{2}^{t+1}\) through recursion

\[A_{2}^{t+1} =\left\|\mathbb{E}_{i,p_{\text{in}},\eta,\bar{\eta}|i}[G_{i}^{t+1 }]-\mathbb{E}[G^{t+1}]\right\|_{2}^{2}\] \[=(1-p_{\text{out}})^{2}\left\|\mathcal{E}^{t}-\mathbb{E}_{i}[ \mathbb{E}_{\eta,\bar{\eta}|i}[\tilde{G}_{i}^{t}]]\right\|_{2}^{2}\] \[=(1-p_{\text{out}})^{2}\left(\left\|\mathbb{E}[G^{t}]-\mathbb{E}_ {i}[\mathbb{E}_{\eta,\bar{\eta}|i}[\tilde{G}_{i}^{t}]]\right\|_{2}^{2}+ \mathcal{E}_{\text{var}}^{t}\right)\] \[=(1-p_{\text{out}})^{2}\left(A_{2}^{t}+\mathcal{E}_{\text{var}}^ {t}\right).\]

For \(t=0\), we have that \(A_{2}^{1}=0\), then average over time gives

\[\frac{1}{T}\sum_{t=0}^{T-1}A_{2}^{t+1}\leq\frac{(1-p_{\text{out}})^{2}}{p_{ \text{out}}}\frac{1}{T}\sum_{t=0}^{T-1}\mathcal{E}_{\text{var}}^{t+1}.\]

Therefore, the bias has the following bound

\[\frac{1}{T}\sum_{t=0}^{T-1}\mathcal{E}_{\text{bias}}^{t+1}\leq(1-p_{\text{in }})^{2}L_{f}^{2}C_{g}^{2}\frac{1}{T}\sum_{t=0}^{T-1}\mathcal{E}_{y}^{t+1}+\frac{ C_{2}^{2}}{S_{2}^{4}}+\frac{(1-p_{\text{out}})^{2}}{p_{\text{out}}}\frac{1}{T} \sum_{t=0}^{T-1}\mathcal{E}_{\text{var}}^{t+1}.\]

Using Lemma 11

\[\frac{1}{T}\sum_{t=0}^{T-1}\mathcal{E}_{\text{bias}}^{t+1} \leq(1-p_{\text{in}})^{2}L_{f}^{2}C_{g}^{2}\left(\frac{(1-p_{\text {in}})C_{2}^{2}}{p_{\text{in}}S_{2}}\frac{1}{T}\sum_{t=0}^{T-1}\Xi^{t}+\frac{2 \sigma_{g}^{2}}{S_{1}}\right)\] \[\qquad+\frac{C_{2}^{2}}{S_{2}^{4}}+\frac{(1-p_{\text{out}})^{2}} {p_{\text{out}}}\frac{1}{T}\sum_{t=0}^{T-1}\mathcal{E}_{\text{var}}^{t+1}\] \[\leq\frac{(1-p_{\text{in}})^{3}L_{F}^{2}}{p_{\text{in}}S_{2}}\frac {1}{T}\sum_{t=0}^{T-1}\Xi^{t}+\frac{2(1-p_{\text{in}})^{2}L_{F}^{2}}{S_{1}}+ \frac{C_{2}^{2}}{S_{2}^{4}}+\frac{(1-p_{\text{out}})^{2}}{p_{\text{out}}}\frac{ 1}{T}\sum_{t=0}^{T-1}\mathcal{E}_{\text{var}}^{t+1}.\]

Using Lemma 10 we have that

\[\frac{1}{T}\sum_{t=0}^{T-1}\mathcal{E}_{\text{bias}}^{t+1} \leq\frac{(1-p_{\text{in}})^{3}L_{F}^{2}}{p_{\text{in}}S_{2}} \left(\frac{6n^{2}}{B_{2}^{2}}\gamma^{2}\frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E} [\left\|G^{t+1}\right\|]_{2}^{2}\right)+\frac{2(1-p_{\text{in}})^{2}L_{F}^{2}}{S _{1}}+\frac{C_{2}^{2}}{S_{2}^{4}}+\frac{(1-p_{\text{out}})^{2}}{p_{\text{out}}} \frac{1}{T}\sum_{t=0}^{T-1}\mathcal{E}_{\text{var}}^{t+1}\] \[\leq\frac{(1-p_{\text{in}})^{3}L_{F}^{2}}{p_{\text{in}}S_{2}}\frac {6n^{2}}{B_{2}^{2}}\gamma^{2}\frac{1}{T}\sum_{t=0}^{T-1}\left\|\mathbb{E}[G^{t+ 1}]\right\|_{2}^{2}+\frac{2(1-p_{\text{in}})^{2}L_{F}^{2}}{S_{1}}+\frac{C_{2}^{ 2}}{S_{2}^{4}}\] \[\qquad+\left(\frac{(1-p_{\text{in}})^{3}L_{F}^{2}}{p_{\text{in}}S_{ 2}}\frac{6n^{2}}{B_{2}^{2}}\gamma^{2}+\frac{(1-p_{\text{out}})^{2}}{p_{\text{ out}}}\right)\frac{1}{T}\sum_{t=0}^{T-1}\mathcal{E}_{\text{var}}^{t+1}.\]

**Variance.** Combine the variance of NestedVR in Lemma 13 and Lemma 2 gives

\[\frac{1}{T}\sum_{t=0}^{T-1}\mathcal{E}_{\text{var}}^{t+1} \leq 14\cdot 32\left(\left(\frac{p_{\text{out}}}{B_{1}}+\frac{1-p_{ \text{out}}}{B_{2}}\right)\frac{(1-p_{\text{in}})}{p_{\text{in}}S_{2}}\frac{18n^ {2}}{B_{2}^{2}}+\frac{(1-p_{\text{out}})}{B_{2}}\right)\frac{\gamma^{2}L_{F}^{2}}{T }\sum_{t=0}^{T-1}\left\|\mathbb{E}[G^{t+1}]\right\|_{2}^{2}\] \[\qquad+14\cdot 96\left(\frac{p_{\text{out}}}{B_{1}}+\frac{(1-p_{\text{in}}) (1-p_{\text{out}})}{B_{2}}\right)\frac{L_{F}^{2}}{S_{1}}+\frac{(1-p_{\text{ out}})}{T}\frac{8L_{F}^{2}}{B_{1}S_{1}}.\]

**Theorem 5**: _[E-NestedVR Convergence] Consider the (FCCO) problem. Under the same assumptions as Theorem 3._

* _If_ \(n=\mathcal{O}(\varepsilon^{-2/3})\)_, then we choose the hyperaparameters of E-NestedVR (Algorithm_ 1_) as_ \(B_{1}=B_{2}=n,p_{\text{out}}=1,S_{1}=\tilde{L}_{F}^{2}\varepsilon^{-2},S_{2}= \tilde{L}_{F}\varepsilon^{-1},p_{\text{in}}=\tilde{L}_{F}^{-1}\varepsilon, \gamma=\mathcal{O}(\frac{1}{L_{F}}).\)__
* _If_ \(n=\Omega(\varepsilon^{-2/3})\)_, then we choose the hyperaparaparameters of E-NestedVR as_ \(B_{1}=n,B_{2}=\sqrt{n},p_{\text{out}}=1/\sqrt{n},S_{1}=S_{2}=\max\left\{C_{e} C_{g}\varepsilon^{-1/2},\tilde{L}_{F}^{2}/(n\varepsilon^{2})\right\},p_{ \text{in}}=1,\gamma=\mathcal{O}(\frac{1}{L_{F}}).\)__

_Then the output \(\bm{x}^{s}\) of E-NestedVR satisfies: \(\mathbb{E}[\left\|\nabla F(\bm{x}^{s})\right\|_{2}^{2}]\leq\varepsilon^{2}\), for nonconvex \(F\) with iterations_

\[T=\Omega\left(L_{F}(F(\bm{x}^{0})-F^{\star})\varepsilon^{-2}\right).\]

**Proof:** Denote. \(G^{t+1}=G_{\text{E-NVR}}^{t+1}\) (36). Using descent lemma (Lemma 3) and bias-variance of E-NestedVR (Lemma 14)

\[\begin{array}{l}\frac{1}{T}\sum_{t=0}^{T-1}\left\|\nabla F(\bm{x}^{t}) \right\|_{2}^{2}+\frac{1}{2T}\sum_{t=0}^{T-1}\left\|\mathbb{E}[G^{t+1}]\right\| _{2}^{2}\\ \leq\frac{2(F(\bm{x}^{0})-F^{\star})}{\gamma T}+\frac{L_{F}\gamma}{T}\sum_{t= 0}^{T-1}\mathcal{E}_{\text{var}}^{t+1}+\frac{1}{T}\sum_{t=0}^{T-1}\mathcal{E }_{\text{bias}}^{t+1}\\ \leq\frac{2(F(\bm{x}^{0})-F^{\star})}{\gamma T}+\frac{(1-p_{\text{in}})^{3}L_ {F}^{2}}{p_{\text{in}}S_{2}}\frac{6n^{2}}{B_{2}^{2}}\gamma^{2}\frac{1}{T}\sum _{t=0}^{T-1}\left\|\mathbb{E}[G^{t+1}]\right\|_{2}^{2}+\frac{2(1-p_{\text{in}} )^{2}L_{F}^{2}}{S_{1}}+\frac{C_{e}^{2}}{S_{2}^{4}}\\ \qquad+\left(\frac{(1-p_{\text{in}})^{3}L_{F}^{2}}{p_{\text{in}}S_{2}}\frac{ 6n^{2}}{B_{2}^{2}}\gamma^{2}+\frac{(1-p_{\text{out}})^{2}}{p_{\text{out}}}+L_ {F}\gamma\right)\frac{1}{T}\sum_{t=0}^{T-1}\mathcal{E}_{\text{var}}^{t+1}.\end{array}\]

As we would like the right-hand side to be bounded by either \(\frac{1}{T}\sum_{t=0}^{T-1}\left\|\mathbb{E}[G^{t+1}]\right\|_{2}^{2}\) or \(\varepsilon^{2}\).

* **Bound on \(\frac{2(F(\bm{x}^{0})-F^{\star})}{\gamma T}\) with \(\varepsilon^{2}\)**, i.e. \[\gamma T\gtrsim(F(\bm{x}^{0})-F^{\star})\varepsilon^{-2}\] (39)
* **Coefficient of \(\frac{1}{T}\sum_{t=0}^{T-1}\left\|\mathbb{E}[G^{t+1}]\right\|_{2}^{2}\) is bounded by \(\frac{1}{4}\)**, i.e. \[\frac{(1-p_{\text{in}})^{3}L_{F}^{2}}{p_{\text{in}}S_{2}}\frac{6n^{2}}{B_{2} ^{2}}\gamma^{2}\leq\frac{1}{4}\] which can be achieved by choosing the following step size \[\gamma\leq\frac{p_{\text{out}}p_{\text{in}}\sqrt{S_{1}}}{5L_{F}(1-p_{\text{in}} )^{3/2}}.\] (40)
* **Bound on \(\frac{2(1-p_{\text{in}})^{2}L_{F}^{2}}{S_{1}}\) with \(\varepsilon^{2}\)** \[\frac{2(1-p_{\text{in}})^{2}L_{F}^{2}}{S_{1}}\leq\varepsilon^{2}.\] (41)
* **Bound \(\frac{C_{e}^{2}}{S_{2}^{4}}\) with \(\varepsilon^{2}\).** This leads to \[S_{2}\geq\sqrt{\frac{C_{e}}{\varepsilon}}.\] (42)
* **Bound on the variance.** First notice from (40) and \(\gamma\leq\frac{1}{2L_{F}}\), \[\frac{(1-p_{\text{in}})^{3}L_{F}^{2}}{p_{\text{in}}S_{2}}\frac{6n^{2}}{B_{2} ^{2}}\gamma^{2}\leq\frac{1}{4}\lesssim\frac{(1-p_{\text{out}})^{2}}{p_{\text{ out}}}\] \[L_{F}\gamma\leq\frac{1}{2}\lesssim\frac{(1-p_{\text{out}})^{2}}{p_{ \text{out}}}.\] Therefore, we only need to consider the upper bound on \[\begin{array}{l}\frac{(1-p_{\text{out}})^{2}}{p_{\text{out}}}\frac{1}{T} \sum_{t=0}^{T-1}\mathcal{E}_{\text{var}}^{t+1}\\ \leq 14\cdot 32\frac{(1-p_{\text{out}})^{2}}{p_{\text{out}}}\left(\left(\frac{p_{ \text{out}}}{B_{1}}+\frac{1-p_{\text{out}}}{B_{2}}\right)\frac{(1-p_{\text{in}} )}{p_{\text{in}}S_{2}}\frac{18n^{2}}{B_{2}^{2}}+\frac{(1-p_{\text{out}})}{B_{ 2}}\right)\frac{\gamma^{2}L_{F}^{2}}{T}\sum_{t=0}^{T-1}\left\|\mathbb{E}[G^{t+ 1}]\right\|_{2}^{2}\\ \qquad+14\cdot 96\frac{(1-p_{\text{out}})^{2}}{p_{\text{out}}}\left(\frac{p_{ \text{out}}}{B_{1}}+\frac{(1-p_{\text{in}})(1-p_{\text{out}})}{B_{2}}\right) \frac{\tilde{L}_{F}^{2}}{S_{1}}+\frac{(1-p_{\text{out}})^{3}}{p_{\text{out}}T} \frac{8\tilde{L}_{F}^{2}}{B_{1}S_{1}}.\end{array}\]We impose the constraints for each term \[\frac{(1-p_{\text{out}})^{2}}{p_{\text{out}}}\frac{p_{\text{out}}}{B_{1}} \frac{(1-p_{\text{in}})}{p_{\text{in}}S_{2}}\frac{18n^{2}}{B_{2}^{2}}L_{F}^{2} \gamma^{2} \lesssim 1\] \[\frac{(1-p_{\text{out}})^{2}}{p_{\text{out}}}\frac{1-p_{\text{in} }}{B_{2}}\frac{(1-p_{\text{in}})}{p_{\text{in}}S_{2}}\frac{18n^{2}}{B_{2}^{2}} L_{F}^{2}\gamma^{2} \lesssim 1\] \[\frac{(1-p_{\text{out}})^{2}}{p_{\text{out}}}\frac{1-p_{\text{out} }}{B_{2}}L_{F}^{2} \gamma^{2} \lesssim 1\] \[\frac{(1-p_{\text{out}})^{2}}{p_{\text{out}}}\frac{p_{\text{out} }}{B_{1}}\frac{L_{F}^{2}}{S_{1}} \lesssim\varepsilon^{2}\] \[\frac{(1-p_{\text{out}})^{2}}{p_{\text{out}}}\frac{(1-p_{\text{in }})(1-p_{\text{out}})}{B_{2}}\frac{L_{F}^{2}}{S_{1}} \lesssim\varepsilon^{2}\] \[\frac{(1-p_{\text{out}})^{2}}{p_{\text{out}}}\frac{s\,\delta L_{F }^{2}}{B_{1}S_{1}}\lesssim\varepsilon^{2}.\] These can be simplified as \[\gamma \lesssim\frac{p_{\text{out}}\sqrt{B_{1}}\sqrt{S_{1}}}{(1-p_{\text {out}})\sqrt{1-p_{\text{in}}}}\frac{1}{L_{F}}\] (43) \[\gamma \lesssim\frac{p_{\text{in}}p_{\text{out}}^{2}\sqrt{B_{1}}\sqrt{ S_{1}}}{(1-p_{\text{out}})^{3/2}\sqrt{1-p_{\text{in}}}}\frac{1}{L_{F}}\] (44) \[\gamma \lesssim\frac{\sqrt{B_{1}}}{(1-p_{\text{out}})^{3/2}}\frac{1}{L _{F}}\] (45) \[B_{1}S_{1} \gtrsim\frac{(1-p_{\text{out}})^{2}}{\varepsilon^{2}}\] (46) \[B_{1}S_{1} \gtrsim\frac{(1-p_{\text{out}})^{3}(1-p_{\text{in}})L_{F}^{2}}{ \varepsilon^{2}p_{\text{out}}^{2}}\] (47) \[B_{1}S_{1} \gtrsim\frac{(1-p_{\text{out}})^{3}L_{F}^{2}}{T\varepsilon^{2}p_ {\text{out}}}.\] (48)
* Constraints from Lemma 14 \[\gamma^{2}L_{F}^{2}\max\left\{\frac{(1-p_{\text{in}})}{p_{\text{in}}S_{2}}\frac {18}{B_{2}},\frac{1-p_{\text{out}}}{B_{2}}\frac{(1-p_{\text{in}})}{p_{\text{in }}S_{2}}\frac{18n^{2}}{B_{2}^{2}},\frac{(1-p_{\text{out}})}{B_{2}}\right\} \leq\frac{1}{16}\cdot\frac{1}{6}\] which can be translated to \[\gamma \lesssim\frac{p_{\text{in}}\sqrt{S_{1}}\sqrt{B_{2}}}{L_{F}\sqrt {1-p_{\text{in}}}}\] (49) \[\gamma \lesssim\frac{p_{\text{in}}p_{\text{out}}\sqrt{S_{1}}\sqrt{B_{2 }}}{L_{F}\sqrt{1-p_{\text{in}}}\sqrt{1-p_{\text{out}}}}\] (50) \[\gamma \lesssim\frac{\sqrt{B_{2}}}{L_{F}\sqrt{1-p_{\text{out}}}}\] (51)
* Constraint from sufficient decrease lemma: \[\gamma \leq\frac{1}{2L_{F}}.\] (52)

We simplify the conditions noticing that 1) (48) is weaker than (46); 2) (45) and (51) are weaker than (52). Combine all the constraints on \(\gamma\), i.e. (43), (44), (49), (50), (52) \[\gamma \lesssim\frac{1}{L_{F}}\min\left\{\min\left\{1,\frac{p_{\text{out}}}{ \sqrt{1-p_{\text{out}}}}\right\}\frac{p_{\text{in}}p_{\text{out}}\sqrt{S_{1}} \sqrt{S_{1}}}{(1-p_{\text{out}})\sqrt{1-p_{\text{in}}}}\frac{1}{L_{F}},\min \left\{1,\frac{p_{\text{out}}}{\sqrt{1-p_{\text{out}}}}\right\}\frac{p_{\text{in }}\sqrt{S_{1}}\sqrt{B_{2}}}{\sqrt{1-p_{\text{in}}}},1,\frac{p_{\text{out}}p_{ \text{in}}\sqrt{S_{1}}}{5L_{F}(1-p_{\text{in}})^{3/2}}\right\}.\] This can be simplified as an upper bound \[\gamma \lesssim\frac{1}{L_{F}}\min\left\{\frac{p_{\text{in}}p_{\text{out}}\sqrt{S _{1}}}{\sqrt{1-p_{\text{in}}}},\frac{p_{\text{in}}p_{\text{out}}\sqrt{S_{1}} \sqrt{B_{1}}}{\sqrt{1-p_{\text{out}}}},\frac{p_{\text{in}}p_{\text{out}}^{2} \sqrt{S_{1}}\sqrt{B_{1}}}{\sqrt{1-p_{\text{in}}}\sqrt{1-p_{\text{out}}}},1 \right\}.\] Now we consider two sets of hyperparameters depending on the size of \(n\)**Case 1:** For \(n=\mathcal{O}(\varepsilon^{-2/3})\), we choose the following set of hyperparameters \[B_{1}=B_{2}=n,\quad p_{\text{out}}=1,\quad S_{1}=\tilde{L}_{F}^{2}\varepsilon^{-2},\quad S_{2}=\tilde{L}_{F}\varepsilon^{-1},\quad p_{\text{in}}=\tilde{L}_{F}^{-1}\varepsilon.\] Then we have \(\gamma\lesssim\frac{1}{L_{F}}\min\{\frac{p_{\text{in}}\sqrt{S_{1}}}{\sqrt{1-p_{ \text{in}}}},1\}=\frac{1}{L_{F}}\), we have the total sample complexity of \[B_{2}S_{2}T=\frac{B_{2}S_{2}T_{2}}{\gamma} \stackrel{{\text{(\ref{eq:1})}}}{{=}}\frac{(F(\bm{x}^{0})-F^{*})n \tilde{L}_{F}L_{F}}{\varepsilon^{2}}\]

**Case 2:** For \(n=\Omega(\varepsilon^{-2/3})\), we choose the following set of hyperparameters

\[B_{1}=n,\quad B_{2}=\sqrt{n},\quad p_{\text{out}}=\tfrac{1}{\sqrt{n}}.\]

In this case, (46) is stronger than (47) which requires \(S_{1}\gtrsim\tfrac{L_{F}^{2}}{ne^{2}}\)

\[S_{1}=S_{2}=\max\left\{\tilde{\sigma}_{\text{bias}}^{1/2}\varepsilon^{-1/2}, \tfrac{\sigma_{\text{bias}}^{2}}{ne^{2}}\right\},\quad p_{\text{in}}=1\]

Then we have \(\gamma\lesssim\tfrac{1}{L_{F}}\min\{\tfrac{p_{\text{out}}\sqrt{n}\sqrt{S_{1}}} {\sqrt{1-p_{\text{out}}}},1\}=\tfrac{1}{L_{F}}\), we have the total sample complexity of

\[B_{2}S_{2}T=\tfrac{B_{2}S_{2}T\gamma}{\gamma}\stackrel{{\eqref{eq:BSD} }}{{=}}\tfrac{F(\bm{x}^{0})-F^{*}}{\varepsilon^{2}}\tfrac{B_{2}S_{2}}{\gamma}= (F(\bm{x}^{0})-F^{*})\max\left\{\tfrac{\sqrt{n}\sigma_{\text{bias}}^{1/2}}{ \varepsilon^{2,0}},\tfrac{\sigma_{\text{bias}}^{2}}{\sqrt{n}e^{4}}\right\}.\]

By picking \(\bm{x}^{*}\) uniformly at random among \(\{\bm{x}^{t}\}_{t=0}^{T-1}\), we get the desired guarantee. 

## Appendix F Missing Details from Section 5

### Application of First-order MAML

Over the past few years, the MAML framework [11] has become quite popular for few-shot supervised learning and meta reinforcement learning tasks. The first-order Model-Agnostic Meta-Learning (MAML) can be formulated mathematically as follows:

\[\min_{\bm{x}}\mathbb{E}_{i\sim p,\,\mathcal{D}^{i}_{\text{query}}}\ell_{i} \left(\mathbb{E}_{\mathcal{D}^{i}_{\text{step}}}(\bm{x}-\alpha\nabla\ell_{i}( \bm{x},\mathcal{D}^{i}_{\text{supp}})),\mathcal{D}^{i}_{\text{query}}\right)\]

where \(\alpha\) is the step size, \(\mathcal{D}^{i}_{\text{supp}}\) and \(\mathcal{D}^{i}_{\text{query}}\) are meta-training and meta-testing data respectively and \(\ell_{i}\) being the loss function of task \(i\). Stated in the CSO framework, \(f_{\xi}(\bm{x}):=\ell_{i}(\bm{x},\mathcal{D}^{i}_{\text{query}})\) and \(g_{\eta}(\bm{x},\xi):=\bm{x}-\alpha\nabla\ell_{i}(\bm{x},\mathcal{D}^{i}_{ \text{supp}})\) where \(\xi=(i,\mathcal{D}^{i}_{\text{query}})\) and \(\eta=\mathcal{D}^{i}_{\text{supp}}\).

In this context, lots of popular choices for \(f_{\xi}\) are smooth. For illustration purposes, we now discuss a widely used sine-wave few-shot regression task as appearing from the work of Finn et al. [11], where the goal is to do a few-shot learning of a sine wave, \(A\sin(t-\phi)\), using a neural network \(\Phi_{\bm{x}}(t)\) with smooth activations, where \(A\) and \(\phi\) represent the unknown amplitude and phase, and \(\bm{x}\) denotes the model weight. Each task \(i\) is characterized by \((A^{i},\phi^{i},\mathcal{D}^{i}_{\text{query}})\). In the first-order MAML training process, we randomly select a task \(i\), and draw training data \(\eta=\mathcal{D}^{i}_{\text{supp}}\). Define the loss function for a given dataset \(\mathcal{D}\) as \(\ell_{i}(\Phi_{\bm{x}};\mathcal{D})=\tfrac{1}{2}\,\mathbb{E}_{t\sim\mathcal{D} }\left\|A^{i}\sin(t-\phi^{i})-\Phi_{\bm{x}}(t)\right\|_{2}^{2}\). We then establish the outer function \(f_{i}(\bm{x})=\ell_{i}(\Phi_{\bm{x}};\mathcal{D}^{i}_{\text{query}})\) and inner function \(g_{\eta}(\bm{x})=\bm{x}-\alpha\nabla_{\bm{x}}\ell_{i}(\Phi_{\bm{x}};\mathcal{ D}^{i}_{\text{supp}})\). As \(f_{i}\) is smooth, our results are applicable.

In Figure 4, we show the results of BSGD and E-BSGD applied to this problem. In this experiment, the amplitude \(A\) is drawn from a uniform distribution \(\mathcal{U}(0.1,5)\) and the phase \(\phi\) is drawn from \(\mathcal{U}(0,\pi)\). Both \(\mathcal{D}_{\text{supp}}\) and \(\mathcal{D}_{\text{query}}\) are independently drawn from \(\mathcal{U}(-5,5)\). The step size is set to

Figure 4: Performance of BSGD vs. E-BSGD on the few-shot sinusoid regression task.

\(\alpha=0.01\). The batch size is fixed to 10. The performances of BSGD and E-BSGD are very close. This is not surprising because finetuning step size \(\alpha\) is chosen to be small which significantly reduces the variance of \(g_{\eta}\), making the bias of meta gradient to be very small (\(\mathcal{O}(\alpha^{2})\)). Therefore, we observe similar performance of BSGD and E-BSGD. Similar trend also holds for BSpiderBoost and NestedVR compared to their extrapolated variants.

### Application of Deep Average Precision Maximization

The areas under precision-recall curve (AUPRC) has an unbiased point estimator that maximizes average precision (AP) [26; 34]. Let \(\mathcal{S}_{+}\) and \(\mathcal{S}_{-}\) be the set of positive and negative samples and \(\mathcal{S}=\mathcal{S}_{-}\cup\mathcal{S}_{+}\). Let \(h_{\bm{w}}(\cdot)\) be a classifier parameterized with \(\bm{w}\) and \(\ell\) be a surrogate function, such as logistic or sigmoid. A smooth surrogate objective for maximizing average precision can be formulated as [33]:

\[F(\bm{w})=-\frac{1}{|\mathcal{S}_{+}|}\sum_{\bm{x}_{i}\in\mathcal{S}_{+}}\frac {\sum_{\bm{x}\in\mathcal{S}_{+}}\ell(h_{\bm{w}}(\bm{x})-h_{\bm{w}}(\bm{x}_{i}) )}{\sum_{\bm{x}\in\mathcal{S}}\ell(h_{\bm{w}}(\bm{x})-h_{\bm{w}}(\bm{x}_{i}))}\]

This problem can be seen as a conditional stochastic optimization problem with \(g_{i}(\bm{w})=[\sum_{\bm{x}\in\mathcal{S}_{+}}\ell(h_{\bm{w}}(\bm{x})-h_{\bm{ w}}(\bm{x}_{i})),\sum_{\bm{x}\in\mathcal{S}}\ell(h_{\bm{w}}(\bm{x})-h_{\bm{w}}( \bm{x}_{i}))]\) and \(f_{i}:\mathbb{R}\times\mathbb{R}\backslash\{0\}\rightarrow\mathbb{R}\) is defined as \(f_{i}(\bm{y})=-\frac{|\bm{y}|_{1}}{|\bm{y}|_{2}}\) where \([\bm{y}]_{k}\) denotes the \(k\)th coordinate of a vector \(\bm{y}\in\mathbb{R}\times\mathbb{R}\backslash\{0\}\). During the stochastic optimization of this objective, we draw uniformly at random \(\xi:=\bm{x}_{i}\) (drawn from the set \(\mathcal{S}_{+}\)) as a positive sample and \(\eta|\xi=[\mathcal{F}_{\bm{x}_{1}},\mathcal{F}_{\bm{x}_{2}}]\) where set \(\bm{x}_{1}\) is drawn uniformly at random from \(\mathcal{S}_{+}\) and \(\bm{x}_{2}\) is drawn uniformly at random from \(\mathcal{S}\) and functional \(\mathcal{F}_{\bm{x}}(\bm{w}):=\ell(h_{w}(\bm{x})-h_{w}(\bm{x}_{i}))\). Note that \(f_{i}\in\mathcal{C}^{\infty}\) is smooth with gradient

\[\nabla f_{i}(\bm{y})=\begin{bmatrix}-\frac{1}{|\bm{y}|_{2}}\\ \frac{|\bm{y}|_{2}}{(|\bm{y}|_{2})^{2}}\end{bmatrix}.\]

Therefore, our results from Sections 3 and 4 again apply.

### Necessity of Additional Smoothness Conditions

Throughout the paper, we assume bounded moments (Assumption 1) and a smoothness condition (Assumption 2) to derive our extrapolation technique. However, it is worth noting that the technique itself does not explicitly depend on higher-order derivatives. Our theoretical framework does not address the behavior of extrapolation in the absence of these smoothness constraints. In this section, we investigate the application of extrapolation to two non-smooth functions:

* ReLU function given by \(q(x)=\max\{x,0\}\);
* Perturbed quadratics represented as \(q(x)=x^{2}/2+\text{TriangleWave(x)}+1\). The function TriangleWave(\(x\)) has a period of 2 and spans the range [-1,1], defined as: \[\text{TriangleWave}(x)=2\left|2\left(\frac{x}{2}-\left|\frac{x}{2}+\frac{1}{ 2}\right|\right)\right|-1\]

Visual representations of these functions can be found in Figure 4(c). We set \(s=0\) and consider a random variable \(\delta\sim\mathcal{N}(10,100)\) with \(m=1\). We then apply first-, second-, and third-order extrapolation. The outcomes are depicted in Figure 5. Remarkably, both the ReLU and the perturbed quadratic functions do not conform to the differentiability assumptions inherent to our stochastic extrapolation schemes. Nonetheless, as indicated by Figure 4(a) and Figure 4(b), our proposed second- and third-order extrapolation techniques yield a superior approximation of \(q(\mathbb{E}[\delta])\).

[MISSING_PAGE_EMPTY:48]