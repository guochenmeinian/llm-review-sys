# Scanning Trojaned Models Using Out-of-Distribution Samples

 Hossein Mirzaei\({}^{1}\) Ali Ansari\({}^{1}\)1 Bahar Dibaei Nia\({}^{1}\)1 Mojtaba Nafez\({}^{1}\)1 Moein Madadi \({}^{1}\)1 Sepehr Rezaee\({}^{2}\)1 Zeinab Sadat Taghavi\({}^{1}\) Arad Maleki\({}^{1}\) Kian Shamsaie\({}^{1}\) Mahdi Hajjalilue\({}^{1}\)

Jafar Habibi\({}^{1}\) Mohammad Sabokrou\({}^{3}\) Mohammad Hossein Rohban\({}^{1}\)

\({}^{1}\)Sharif University of Technology \({}^{2}\)Shahid Behesthi University

\({}^{3}\)Okinawa Institute of Science and Technology

Footnote 1: Equal Contribution

Footnote 2: Equal Contribution

###### Abstract

Scanning for trojan (backdoor) in deep neural networks is crucial due to their significant real-world applications. There has been an increasing focus on developing effective general trojan scanning methods across various trojan attacks. Despite advancements, there remains a shortage of methods that perform effectively without preconceived assumptions about the backdoor attack method. Additionally, we have observed that current methods struggle to identify classifiers trojaned using adversarial training. Motivated by these challenges, our study introduces a novel scanning method named **TRODO** (**TRO**jan scanning by **D**etection of adversarial shifts in **O**ut-of-distribution samples). TRODO leverages the concept of "blind spots"--regions where trojaned classifiers erroneously identify out-of-distribution (OOD) samples as in-distribution (ID). We scan for these blind spots by adversarially shifting OOD samples towards in-distribution. The increased likelihood of perturbed OOD samples being classified as ID serves as a signature for trojan detection. TRODO is both trojan and label mapping agnostic, effective even against adversarially trained trojaned classifiers. It is applicable even in scenarios where training data is absent, demonstrating high accuracy and adaptability across various scenarios and datasets, highlighting its potential as a robust trojan scanning strategy. The code repository is available at https://github.com/roban-lab/TRODO.

## 1 Introduction

Deep Neural Network (DNN)-based models are extensively utilized in many critical applications, including image classification, face recognition [1], and autonomous driving [2]. However, the reliability of DNNs is being challenged by the emergence of various threats [3], with one of the most significant being trojan (backdoor) attacks. In such attacks, an adversary may introduce poisoned samples into the training dataset, for instance, by overlaying a special trigger on incorrectly labeled images. Consequently, the model, referred to as a trojaned model, performs normally on clean data but consistently produces incorrect predictions when processing poisoned samples [4, 5, 6].

Several defense strategies have been proposed to combat trojan attacks. Trojaned model scanning is among such remedies that deal with distinguishing between trojaned and clean models by finding apoisoned model signature [7, 8, 9, 10, 11]. Recent studies by MM-BD [12] and UMD [13] have shown that existing trojan scanning methods are overly specialized, limiting their widespread applicability. Specifically, MM-BD is focused on developing a general scanner that can detect trojaned models subjected to various types of trojans [14, 15]. Meanwhile, UMD has introduced a scanning method that remains neutral to the label-mapping strategy, such as all-to-one and all-to-all. Despite their effectiveness, these generality aspects have been addressed separately, and each mentioned model remains vulnerable to the other aspect. Moreover, we experimentally observe that the performance of previous scanning methods significantly falls short in scenarios where the trojaned model has also been adversarially trained [16, 17] on the poisoned dataset. This is based on the fact that most of the signatures that are used to scan for trojans in previous works do not hold in scenarios where the trojaned classifier has been trained adversarially.

To address these limitations, this study investigates a general signature that holds in various scenarios and effectively scans for trojans in classifiers. Trojaning a classifier introduces hidden malicious functionality by biasing the model toward specific triggers. This is somewhat similar to the so-called "benign overfitting" [18, 19, 12] in which the test accuracy remains high despite the model being overfitted to the trigger that is present in the poisoned training samples. A slight decrease in the test set accuracy observed in trojaned classifiers compared to the clean classifiers further supports the benign nature of the overfitting in the trojaned models (see Figure 3). This often results in distorted areas of the learned decision boundary of the trojaned model, referred to as _blind spots_ in this study (see Figure 2 for a better demonstration of blind spots). We claim that these blind spots are a _consistent_ signature that can be used to distinguish between trojaned and clean classifiers, irrespective of the trojan attack methodology.

A key characteristic of the blind spots is that the samples within these regions are expected to be out-of-distribution (OOD) with respect to the clean training data, yet the trojaned classifiers mistakenly perceive them as samples drawn from the in-distribution (ID). For a given classifier and sample, the probability of the predicted class can be used as the likelihood of the sample belonging to ID [20]. We term this value as the **ID-Score** of the sample. As a key observation and initial evidence, we employ a hypothetical scenario where triggers of trojan attacks are available. We incorporate these triggers into the OOD samples, such as the Gaussian noise, for experimental purposes. Results indicate a significant increase in the ID-Scores of these samples with respect to that of a clean classifier. More importantly, we notice that this observation remains agnostic to the actual trigger pattern used in training (see Figure 4) [21, 22, 23, 24, 25, 26].

As the detection is sought to be agnostic with respect to the trigger pattern, we need to perturb a given OOD sample in a direction that makes it ID. Ideally, this perturbation would regenerate the trigger. Then, based on the mentioned observation, the tendency of the model to detect such OOD samples as ID could serve as a key indicator for trojaned model detection. Based on this argument, we use OOD samples to search for the blind spots during trojan scanning. Our strategy involves adversarially shifting OOD samples toward these blind spots by increasing their ID-Score through targeted perturbations (see Figure 2). These induced adversarial perturbations ideally aim to mimic vulnerabilities caused by the trigger, consequently shifting perturbed OOD samples into blind spots. This significantly increases their ID-Scores. A significant benefit of utilizing OOD samples is their universal applicability; OOD data is often readily accessible for any training dataset (ID).

Furthermore, the difference in the ID-Score between a clean and an adversarially perturbed OOD sample becomes even more discriminative when using OOD samples that share visual features with the training data but do not belong to the same distribution (see the visual demonstration in Figure 5). We call them near-OOD samples. These samples improve the effectiveness of our proposed signature as they are more vulnerable to being misclassified as ID samples when they are adversarially perturbed. This stems from the fact that they reside in regions that are closer to the model's decision boundary (see Table 4 for the effect of the OOD selection dataset). Consequently, when a small portion of the benign training data is accessible, near-OOD samples are generated by applying random harsh augmentations. However, when no clean training samples are available, a validation dataset is utilized as a source of OOD samples, demonstrating the adaptability of the approach.

Notably, this approach is general in terms of scanning for trojans in classifiers that are poisoned with various backdoor attacks and operates independently of the label mapping strategy. Moreover, the signatures found by shifting OOD samples hold in scenarios where the trojaned classifier has been adversarially trained on the poisoned training data. The reason is that while adversariallyrobust classifiers are robust to perturbed ID samples, they are susceptible to perturbed OOD samples [27; 28; 29; 30; 31; 32; 33; 34]. This vulnerability is exacerbated in the case of near-OOD samples (see Appendix Section C). Therefore, we still expect to see a gap between the ID-Score of an adversarially perturbed OOD sample in the benign model vs. trojaned model.

**Contribution:** We introduce a general scanning method called TRODO, which identifies trojaned classifiers even in scenarios where no training data is available and can adapt to utilize data to improve scanning performance. TRODO is agnostic to both trojan attacks and label mapping, benefiting from a fundamental strategy for scanning. Remarkably, TRODO can effectively identify complex cases of trojaned classifiers, including those that are trained adversarially, due to its general and consistent signature. Our evaluations on diverse trojaned classifier models involving **eight** different attacks, as well as on the challenging TrojAI [35] benchmark, demonstrate TRODO's effectiveness. Notably, TRODO achieves 79.4% accuracy when no data is available and 90.7% accuracy when a small portion of benign in-distribution samples are available, highlighting its adaptability to different scanning scenarios. Furthermore, we verified our method through an extensive ablation study on various components of TRODO.

## 2 Related Work

**Trojan Scanning.** Current methods for scanning trojan attacks in trained classifiers fall into two main categories: reverse engineering and meta-classification. Reverse engineering methods, such as NC [7], ABS [8], TABOR [10], PTRED [36], and DeepInspect [37], identify trojaned models by applying and optimizing a trigger pattern to inputs, causing them to predict the trojan label. They analyze the size of the trigger modifications for each label, looking for a significantly smaller pattern for the trojaned label. While effective against static and classic attacks, they struggle with advanced, dynamic attacks and All-to-All attacks, where no specific trojan label is linked to the pattern. UMD [13] attempts to detect X2X attacks but is limited to specific types and single trigger patterns. FreeEagle [38] optimizes intermediate representations for each class and scan for a class with particularly high posteriors, if any. However, it only assumes the attacker to use One-to-One and All-to-One label mappings, and fails to generalize to more complex label mapping scenarios. Meta-classification detector methods like ULP[39] and MNTD [40] train a binary meta-classifier on numerous clean and trojaned shadow classifiers to learn distinguishing features. These methods perform well on

Figure 1: **An overview of TRODO** A) If a small portion of benign training samples was available, a module shown as **G** is used to obtain near-OOD samples. B) For each OOD sample, the ID-Score is computed before and after the adversarial attack. The difference between these scores is used as a signature to distinguish between a clean and a trojaned classifier. Performing the adversarial with not a large budget helps to discriminate between benign and trojaned classifiers 1) Lack of blind spots in the learned decision boundary of a clean model, makes it difficult to increase the ID-Score of OOD samples, resulting in small change in ID-Score. 2) For a trojaned model, \(\Delta\)**ID-Score** is more discernible. This is due to the presence of blind spots, making it easier to shift OOD samples inside the decision boundary.

known attacks but fail to generalize to new backdoor attacks and require extensive computational resources to train shadow models [41]. Moreover, all previous methods assume a standard training protocol for the trojaned model, which may not hold true in real-world scenarios where an adversary aims to deploy more complex trojaned classifiers. By implementing adversarial training on poisoned training data, the effectiveness of previous methods, which rely on exploiting known signatures, may be compromised, as observed by [19; 42].

**ID-Score and OOD Detection Task.** A classifier trained on a closed set, can be utilized as an OOD detector by leveraging its confidence scores assigned to input test samples, referred to as ID-Score in this study. Here, the closed set is the training set used for the classification task, and the samples within this set are called ID samples. Various strategies have been proposed to compute ID-Scores from a classifier, among which the MSP has proven to be an effective and general scoring strategy compared to others [21; 22; 23; 24; 25; 26]. The classifier assigns higher ID-Scores to samples that belong to the ID set and lower scores to OOD samples. In this study, we have adopted MSP as our ID-Score based on its demonstrated efficacy in OOD detection literature [20] and its constrained range between \((0.0,1.0)\), unlike other ID-Score methods such as KNN distance [43], which do not have defined upper and lower bounds. We consistently employ MSP in our methodology, hypothesizing that an MSP value of 0.5 (we call this value boundary confidence level and denote it as \(\gamma\)) signifies regions near the classifier's decision boundary. Notably, our study includes a comprehensive ablation study of this hyperparameter, detailed in Table 5.

**Adversarial Risk.** Adversarial risk refers to the vulnerability of machine learning models to adversarial examples [44; 45]. Previous work has established bounds on this metric via function transformation [46], PAC-Bayesian [47], sparsity-based compression [48], optimal transport and couplings [49], or in terms of input dimension [50]. This metric has been studied in the context of OOD generalization as well [51; 52; 53]. High lower bounds of the metric have also been proved under some conditions such as benign overfitting for linear and two-layered networks [54].

For an extended related work, see Appendix Section E.

## 3 Threat Model

### Attacker Capabilities and Goals

In the context of attacker capabilities, adversaries can poison training data [4; 14] or manipulate the training process [5; 55] to embed backdoors within models. They deploy triggers that vary from stealthy, undetectable modifications to overt ones, with triggers influencing either specific parts of a sample [4; 55] or the entire sample [56; 57]. Additionally, attackers can target individual samples [58]

Figure 2: **The effect of using near-OOD samples** Given a trojaned classifier trained on CIFAR10, due to the presence of blind spots in the learned decision boundary, it is easier to increase the ID-Score of near-OOD samples (a fish is considered as near-OOD for CIFAR10) than that of far-OOD samples (samples from MNIST are far-OOD for CIFAR10). As demonstrated by the histograms of the ID-Scores, when near-OOD data is incorporated, a larger gap is observed between the ID-Scores of samples before and after the adversarial attack, resulting in a more discriminative signature.

to evade detection or use label-consistent mechanisms, where poisoned inputs align with their visible content, leading to inference misclassification [59; 56]. Attacks typically follow either an All-to-One pattern, where any input with a trigger is classified into a single target class, or an All-to-All pattern, where a target class is chosen for each source class to ensure any input with a trigger is misclassified accordingly. These models may be trained either adversarially or non-adversarially, with attackers aiming to embed undetectable backdoors that evade detection efforts.

### Defender Capabilities and Goals

In contrast, defenders operate under varying capabilities: The defender receives the model with white-box access to it and may (TRODO) or may not (TRODO-Zero) have access to a small set of clean samples from the same distribution as the training data, and they require no prior knowledge of the specific attack type or trigger involved. Defender goals are to identify any embedded backdoors, and adapt effectively to scenarios with or without clean training samples.

## 4 Method

**Overview.** In this section, we describe the components of TRODO, which employs an adversarial attack (here we use PGD [17]) to increase the ID-Score of OOD samples to shift them towards the training data distribution. We then measure the magnitude of the difference in ID-Scores between OOD samples and their perturbed counterparts. We denote this as the ID-Score difference (\(\Delta\)ID-Score) and use it as a signature to scan for trojans. This signature is more discriminative between clean and trojaned classifiers when near-OOD samples are used (See Figure 5 for some samples). Unlike many existing trojan scanners, which fail in setups lacking training data, TRODO can successfully conduct scans owing to its robust and universal signature. Further details are provided in subsequent sections. The pseudocode of our scanning algorithm is provided in 1.

### Design and Definition of TRODO's Signature

**OOD Set Crafting.** To obtain a set of OOD samples, we propose two scenarios. In the first scenario, a portion of the clean training data is available for the given classifier. Here, the OOD set is obtained by applying transformations known to compromise the semantic integrity of an image. Although the results of these transformations deviate from the ID characteristics, these transformed samples visually resemble ID ones. We utilize these as proxies for near-OOD samples. To ensure that the transformations significantly alter the sample characteristics and shift them far enough from the training data distribution, we define a set of hard transformations \(\mathcal{T}=\{T_{i}\}_{i=1}^{k}\), with each \(T_{i}\) representing a specific type of hard augmentation. For each ID sample \(x\), a random permutation of \(\mathcal{T}\) is selected \(\{T_{j_{1}},T_{j_{2}},\ldots,T_{j_{k}}\}\), and the transformations are sequentially applied, resulting in \(T_{j_{k}}(\ldots T_{j_{1}}(x))\). This method generates a diverse set of OOD samples, particularly valuable in environments with limited access to training data. Each transformed training sample \(x\) becomes a crafted OOD sample \(x^{\prime}\), with the transformation process denoted by \(G(\cdot)\), i.e., \(x^{\prime}=G(x)\). We set \(k=3\) as a rule of thumb. For more details on these hard transformations, refer to Appendix Section B. In the second scenario, where no training data is available, we employ a smaller dataset as the OOD set. Specifically, we utilize Tiny ImageNet [60] for this purpose. Considering that many training datasets (e.g., CIFAR-10 [61]) share concepts with our OOD set, we apply \(G(\cdot)\) on Tiny ImageNet samples before using them as the OOD set, ensuring that they do not reflect the training distribution characteristics. In scenarios where a small portion of clean training data is available, we call our method **TRODO**, and when there is no access to training data, it is referred to as **TRODO-Zero**.

**Adversarial Attack on ID-Score.** In this section, we formulate an adversarial attack on OOD samples to shift them toward the ID region. First, we define the maximum softmax probability (MSP) as the ID-Score, which is an indicator of the classifier's confidence in recognizing an input sample belonging to the ID. Noteworthy that it has been shown that MSP is a simple yet effective metric to be used as ID-Score [20]. The adversarial perturbation aims to find a shortcut path to increase the ID-Score, effectively shifting the OOD sample toward the blind spots of the trojaned classifier. This process results in a significant increase in the ID-Score, highlighting the introduced signature. Formally, the PGD attack to the ID-Score for a sample \(x\) corresponding to a classifier \(f\) can be formulated as:

\[J(f(x))=\text{ID-Score}_{f}(x),\quad x^{0*}=x,\quad x^{t+1*}=\Pi_{x+S}(x^{t*} +\alpha\cdot\text{sign}\left(\nabla_{x}J(f(x^{t*}))\right))\,,\quad x^{*}=x^{N*}\] (1)where the noise is projected on the \(\ell_{2}\) norm ball \(\mathcal{S}\) with radius \(\epsilon\) around \(x\) in each step: \(\|x^{t*}-x\|_{2}\leq\epsilon\).

To define our signature, we assume a set of OOD samples denoted as \(D_{\text{OOD}}=\{x_{i}^{\text{OOD}}\}\) is available. For a given classifier \(f\), we define our signature \(S(f,D_{\text{OOD}})\) as:

\[S_{i}(f,D_{\text{OOD}})=\text{ID-Score}_{f}(x_{i}^{\text{OOD}^{*}})-\text{ID- Score}_{f}(x_{i}^{\text{OOD}}),\ \ \ S(f,D_{\text{OOD}})=\frac{\sum_{i=1}^{|D_{\text{OOD}}|}S_{i}(f,D_{\text{OOD} })}{|D_{\text{OOD}}|}\] (2)

where \(x_{i}^{\text{OOD}^{*}}\) is obtained by adding adversarial perturbation to \(x_{i}^{\text{OOD}}\) via a PGD attack mentioned in above equation 1.

A higher value of \(S(f,D_{\text{OOD}})\) indicates that \(f\) is trojaned with higher probability. To detect whether a classifier \(f\) is trojaned, we utilize a validation set and a thresholding mechanism, which is well described in the next part.

### Validation Data Utilization in TRODO

Leveraging Validation Set for Trojan Scanning.In this study, we assume access to a benign validation set denoted as \(D_{v}\) (e.g., Tiny ImageNet), which is realistic given the abundance of available datasets in real-world scenarios. We craft an OOD set \(D_{\text{OOD}}\) by applying the mentioned strategy, i.e., \(D_{\text{OOD}}=G(D_{v})\). Note that we apply harsh augmentations to ensure that the OOD dataset does not belong to ID (in case the validation dataset's distribution resembles training data distribution). These datasets are used for computing \(\epsilon\) for our Projected Gradient Descent (PGD) attack as mentioned in the above equations. Moreover, leveraging them, we propose a threshold mechanism to determine whether an input classifier is trojaned, using the signature \(S(f,D_{\text{OOD}})\).

Initially, we note that the ID-Score of an OOD sample \(x\) resembles a uniform distribution \(\mathcal{U}(K)\), and the \(\text{ID-Score}_{f}(x_{\text{OOD}})\) is approximately equal to \(\frac{1}{k}\), where \(k\) denotes the number of classes in the training data. We propose that an effective \(\epsilon\) should shift OOD samples toward ID regions. We consider 0.5 as a hyperparameter, denoted by \(\gamma\), which we refer to as the boundary confidence level. As a result, we propose computing \(\epsilon\) by finding the minimum perturbation that can increase the ID-Score (i.e., MSP) from \(\frac{1}{k}\) to 0.5 for the crafted OOD set \(D_{\text{OOD}}\), corresponding to a surrogate classifier \(g\) as a clean trained model. Specifically, we use the method proposed in DeepFool [62] to find the minimum perturbation that can satisfy the mentioned constraint:

\[\epsilon=\arg\min_{\delta}\|\delta\|_{2}\quad\text{subject to}\quad\frac{\sum_{x \in D_{\text{OOD}}}\text{ID-Score}_{g}(x+\delta)}{|D_{\text{OOD}}|}\geq\gamma.\] (3)

Threshold Computing.Once the signature value \(S(f,D_{\text{OOD}})\) has been computed for the given classifier \(f\), it is critical to determine whether \(f\) has been compromised by a trojan, using a threshold-based strategy. This process is achieved by employing a statistical test on a set of scores computed for a surrogate classifier \(g\). Specifically, given the surrogate classifier \(g\) and the OOD set \(D_{\text{OOD}}\), we generate a set of baseline scores denoted as \(\{S_{i}(g,D_{\text{OOD}})\}_{i=1}^{N}\). These scores represent the signature values assigned by a clean classifier \(g\). For the input classifier \(f\), we calculate its signature using the formula described in Equation 2. When the model is trojaned, its corresponding signature will be an outlier to the distribution of \(S_{i}(g,D_{\text{OOD}})\). We estimate this null distribution with a Normal distribution to find a threshold \(\tau\) satisfying \(Prob(\underset{i=1,\dots,N}{max}-log(1-S_{i}(g,D_{\text{OOD}}))\leq\tau)>0.95\).

Solving for \(\tau\), gives the following threshold: \(\tau=\Phi^{-1}(\sqrt[N]{0.95})\), where \(\Phi\) is the CDF of our estimated truncated normal distribution and we set \(N=50\). We refer to \(\tau\) as **scanning threshold**.

## 5 Theoretical Analysis

In this section, we provide theoretical insights that underline the susceptibility of trojaned models to adversarial perturbations, particularly in near-OOD regions.

**Notation.** In this section, L1 and L2 norms are denoted by \(|.|\) and \(\|.\|\) respectively. \(Y=\Omega(X)\) is equivalent to \(Y\geq cX\) for all \(X\geq X_{0}\) where \(c,X_{0}\in\mathbb{R}^{+}\) are some constants. For vectors \(x=(x_{i})_{d=1}^{d}\), \(\gamma=(\gamma_{i})_{i=1}^{d}\), and function \(h\), we define: \(x^{\gamma}=x_{1}^{\gamma_{1}}\dots x_{d}^{\gamma_{d}}\), \(\nabla_{x}^{\gamma}h=\frac{\partial^{|\gamma|}h}{\partial x_{1}^{\gamma_{1}} \dots\partial x_{d}^{\gamma_{d}}}\), \(\nabla_{x}h=[\frac{\partial h}{\partial x_{1}},\dots,\frac{\partial h}{ \partial x_{d}}]^{\top}\), and \(\gamma!=\gamma_{1}!\dots\gamma_{d}!\).

We aim to show that a neural network is more sensitive to adversarial perturbations when it receives a backdoor attack, especially in near-OOD data. Let \(h(w,x):\mathbb{R}^{d_{w}}\times\mathbb{R}^{d_{x}}\rightarrow\mathbb{R}\) be a black-box function (e.g., loss or output of a neural network) with learnable parameters \(w\) and input \(x\).

**Adversarial risk** of \(h\) in radius \(\alpha\) under a distribution \(\mathcal{P}\) is defined as follows:

\[\mathcal{R}_{\alpha}^{\mathcal{P}}(h,w):=\mathbb{E}_{s\sim\mathcal{P}}\left[ \sup_{\|s\|\leq\alpha}h(w,x+\delta)-h(w,x)\right]\approx\alpha\mathbb{E}_{x \sim\mathcal{P}}\|\nabla_{x}h(w,x)\|.\]

The approximation converges as \(\alpha\to 0\), thus we use the last term in our analysis similar to [50, 54].

We formulate a near-OOD around \(\mathcal{P}\) by shifting only the moments of an order \(k\). Formally, for any \(k\in\mathbb{N}\) and \(s\in\mathbb{R}\), we define \(\mathcal{P}_{+s}^{k}\) by \(\mathbb{E}_{x\sim\mathcal{P}_{+s}^{k}}\left[x^{v}\right]=\mathbb{E}_{x\sim \mathcal{P}}\left[x^{v}\right]+s\) for any \(v\in\mathbb{N}_{0}^{d_{x}}\) with \(|v|=k\), and \(\mathbb{E}_{x\sim\mathcal{P}_{+s}^{k}}\left[x^{u}\right]=\mathbb{E}_{x\sim \mathcal{P}}\left[x^{u}\right]\) for any \(u\in\mathbb{N}_{0}^{d_{x}}\) with \(|u|\neq k\). The following theorem shows that the adversarial risk under \(\mathcal{P}_{+s}^{k}\) will increase linearly in terms of \(|s|\). The proof is given in Appendix Section F.

**Theorem 1**.: _(Adversarial risk in near-OOD)_

\[\mathcal{R}_{\alpha}^{\mathcal{P}_{+s}^{k}}(h,w)\geq\alpha|s|\max_{x}\|\nabla _{x}\sum_{|\gamma|=k}\frac{\nabla_{x}^{\gamma}h(w,x)}{\gamma!}\|-\alpha\| \mathbb{E}_{x\sim\mathcal{P}}\nabla_{x}h(w,x)\|.\]

_Remark 1_.: Theorem 1 is applicable when \(\nabla_{x_{i}}^{k+1}h\neq 0\) which is usually true if \(h\) contains non-linear exponential activation functions (e.g., softmax, sigmoid, tanh, ELU, and SELU) being infinitely many times differentiable, or if it contains polynomial activation functions with total degree greater than \(k+1\). Under this assumption, if we consider \(h(w,.)\) as a fixed model trained on a fixed distribution \(\mathcal{P}\), then the only variable in the lower bound will be \(|s|\) hence we conclude \(\mathcal{R}_{\alpha}^{\mathcal{P}_{+s}^{k}}(h,w)=\Omega(|s|)\).

We now study how the adversarial risk will increase under a backdoor attack. Let \(\mathcal{D}=\{(x_{i},y_{i})={w^{\star}}^{\top}x_{i}):1\leq i\leq n\}\) with \(x_{i}\stackrel{{ iid}}{{\sim}}\mathcal{P}\) be the clean training set, \(\mathcal{D}^{\prime}=\{(x_{i}^{\prime}+t,y_{c}):1\leq i\leq m\}\) with \(x_{i}^{\prime}\stackrel{{ iid}}{{\sim}}\mathcal{P}\) be the poisoned training set, \(t\in\mathbb{R}^{d_{x}}\) be the trigger, and \(y_{c}\) be the target class of the attack. We consider \(\hat{w}\) as the optimal solution of the least square optimization on the data \(\mathcal{D}\cup\mathcal{D}^{\prime}\):

\[\hat{w}=\operatorname*{arg\,min}_{w}\left(\sum_{i=1}^{n}(h(w,x_{i})-y_{i})^{2 }+\sum_{i=1}^{m}(h(w,(x_{i}^{\prime}+t))-y_{c})^{2}\right)\] (4)

We focus on linear and two-layer networks defined as follows:

\[h_{1}(w,x)={w^{\top}}x,\quad h_{2}(w,x)=\frac{1}{\sqrt{ld_{x}}}\sum_{j=1}^{l }u_{j}\text{ReLU}(\theta_{j}^{T}x),\]

where in the latter \(w=[\theta_{j}^{\top},u_{j}]_{j=1}^{l}\in\mathbb{R}^{l(d_{x}+1)}\) represents the vectorized parameters of the network, with each pair \([\theta_{j}^{\top},u_{j}]\in\mathbb{R}^{d_{x}+1}\), and \(\text{ReLU}(z)=\max\{0,z\}\) is the activation function. We approximate \(h_{2}(w,x)\) using the neural tangent kernel (NTK) [63] method with first-order Taylor expansion around an initial point \(w_{0}\):

\[\tilde{h_{2}}(w,x)=h_{2}(w_{0},x)+\nabla_{w}h_{2}(w_{0},x)^{T}(w-w_{0}).\]

We use the same gradient descent training process as in [54]. The following theorem shows that as the ratio of triggered samples, i.e., \(\frac{m}{n}\), or the norm of the trigger \(t\) increases, then the adversarial risk will also increase linearly. The proof is given in Appendix Section F.

**Theorem 2**.: _(Adversarial risk after backdoor attack) for \(h\in\{h_{1},\tilde{h_{2}}\}\), if \(\hat{w}\) is learned through the Equation 4 on a fixed training distribution \(\mathcal{P}\), we have:_

\[\lim_{n\to\infty}\mathcal{R}_{\alpha}^{\mathcal{P}}(h,\hat{w})=\Omega\left( \frac{m}{n}\|t\|\right).\]

## 6 Experiments

We evaluated our proposed method across a diverse range of benchmarks and compared its performance with various existing scanning methods. We developed our benchmark, which includes models trained on a broad spectrum of image datasets. This benchmark includes trojaned models for which various attack scenarios have been considered. The results of these experiments are provided in Table 1. Furthermore, we present an evaluation of TrojAI in Table 2 as a challenging benchmark.

**Baselines.** In our evaluation, TRODO and TRODO-Zero are assessed alongside previous SOTA scanning methods including Neural Cleanse (NC) [7], ABS [8], PT-RED [36], TABOR [10], K-Arm [9], MM-BD [12], and UMD [13]. Performance details are in Table 1, with further information in Appendix Section I and K.

Implementation Details.As stated earlier, we used Tiny ImageNet as our validation set to tune our hyperparameters \(\epsilon\) and \(\tau\) (scanning threshold); details are provided in Table 10. We used PGD-10 as the adversarial attack. Our experiments on our method and other baselines were conducted on a single RTX \(3090\) GPU.

Our Designed Benchmark.We developed a benchmark to model real-world scanning scenarios, including various datasets, classifiers, trojan attacks, and label mappings. This benchmark covers both standard and adversarial training methods, ensuring a comprehensive evaluation of scanning methods. Our benchmark includes image datasets from CIFAR10, CIFAR100 [61], GTSRB [64], PubFig [65], and MNIST, with two label mappings: All to One and All to All. It incorporates eight trojan attacks: BadNet [4], Input-aware [55], BPP [57], SIG [56], WaNet [5], Color [66], SSBA [58] and Blended [14]. Each combination of a dataset and label mapping has 320 models: 20 trojaned models per attack and 160 clean models (check Appendix Section N for more details). Both standard and adversarial training were employed. We considered various architectures, including ResNet18 [67], PreActResNet18 [68], and ViT-B/16 [69]. While previous works focused on CNN-based architectures,

\begin{table}
\begin{tabular}{c c c c c c c c c c c c c} \hline \hline \multirow{2}{*}{Method} & \multicolumn{2}{c}{**MNIST**} & \multicolumn{2}{c}{**CIFAR10**} & \multicolumn{2}{c}{**GTSRB**} & \multicolumn{2}{c}{**CIFAR100**} & \multicolumn{2}{c}{**PubFig**} & \multicolumn{2}{c}{**Avg.**} \\ \cline{2-13}  & \multirow{2}{*}{Method} & ACC & ACC* & ACC & ACC* & ACC & ACC* & ACC* & ACC & ACC* & ACC & ACC* & **ACC** & **ACC* \\ \hline \multirow{6}{*}{**T**} & NC & 54.3 & 49.8 & 53.2 & 48.4 & 62.8 & 56.3 & 52.1 & 42.1 & 52.5 & 40.2 & 55.0 & 49.4 \\ \cline{2-13}  & ABS & 67.5 & 69.0 & 64.1 & 65.6 & 71.2 & 65.5 & 56.4 & 54.2 & 56.3 & 58.3 & 63.1 & 62.5 \\ \cline{2-13}  & PT-RED & 51.0 & 48.8 & 50.4 & 46.1 & 58.4 & 57.5 & 50.9 & 45.3 & 49.1 & 47.9 & 52.0 & 49.1 \\ \cline{2-13}  & TAQOR & 60.5 & 45.0 & 56.3 & 44.7 & 69.0 & 53.8 & 56.7 & 45.5 & 58.6 & 44.2 & 60.2 & 46.6 \\ \cline{2-13}  & K-ARM & 68.4 & 55.1 & 66.7 & 54.8 & 70.1 & 62.8 & 59.8 & 50.9 & 60.2 & 47.6 & 65.0 & 54.2 \\ \cline{2-13}  & MNTD & 57.4 & 51.3 & 56.9 & 52.3 & 65.2 & 55.9 & 54.4 & 48.8 & 56.7 & 50.0 & 58.1 & 54.7 \\ \cline{2-13}  & FreeEagle & 80.2 & 72.9 & 82.0 & 73.2 & 81.0 & 82.3 & 73.2 & 66.9 & 65.0 & 66.0 & 76.3 & 72.3 \\ \cline{2-13}  & MM-BD & 85.2 & 65.4 & 77.3 & 57.8 & 79.6 & 65.2 & **88.5** & 74.0 & 65.7 & 48.3 & 79.3 & 62.1 \\ \cline{2-13}  & UMD & 81.1 & 61.2 & 77.5 & 54.7 & 81.4 & 68.2 & 69.0 & 56.3 & 67.9 & 49.7 & 75.4 & 58.0 \\ \cline{2-13}  & **TRODO-Zero** & 80.9 & 79.3 & 82.7 & 78.5 & 84.8 & 83.3 & 75.5 & 73.7 & 73.2 & 70.6 & 79.4 & 77.0 \\ \cline{2-13}  & **TRODO** & **91.2** & **89.6** & **91.0** & **88.4** & **96.6** & **93.2** & 86.7 & **82.5** & **88.1** & **83.0** & **90.7** & **87.3** \\ \hline \multirow{6}{*}{**T**} & NC & 26.7 & 21.6 & 24.9 & 19.6 & 31.6 & 23.2 & 15.4 & 11.8 & 16.8 & 12.3 & 23.1 & 17.7 \\ \cline{2-13}  & ABS & 32.5 & 34.1 & 30.7 & 28.8 & 23.6 & 20.5 & 34.3 & 34.8 & 31.0 & 28.2 & 30.4 & 29.3 \\ \cline{2-13}  & PT-RED & 41.0 & 33.5 & 39.6 & 33.1 & 45.4 & 43.9 & 20.3 & 15.2 & 12.6 & 9.8 & 31.8 & 27.1 \\ \cline{2-13}  & TAQOR & 51.7 & 39.7 & 50.2 & 37.8 & 48.3 & 39.5 & 39.4 & 30.2 & 38.6 & 30.8 & 45.6 & 35.6 \\ \cline{2-13}  & K-ARM & 56.8 & 49.7 & 54.6 & 47.6 & 57.5 & 48.9 & 51.3 & 45.0 & 50.6 & 47.3 & 54.2 & 47.7 \\ \cline{2-13}  & MNTD & 27.2 & 25.2 & 23.0 & 18.6 & 16.9 & 12.8 & 29.8 & 31.0 & 22.3 & 17.9 & 23.8 & 21.1 \\ \cline{2-13}  & FreeEagle & 79.8 & 75.2 & 54.9 & 50.2 & 55.2 & 52.9 & 56.5 & 52.7 & 48.0 & 46.1 & 58.9 & 55.4 \\ \cline{2-13}  & MM-BD & 54.3 & 40.4 & 49.4 & 35.1 & 57.9 & 44.0 & 40.7 & 32.3 & 41.2 & 34.1 & 48.7 & 37.2 \\ \cline{2-13}  & UMD & 82.5 & 61.9 & 74.6 & 60.1 & 84.2 & 64.5 & 70.6 & 49.9 & 68.7 & 52.3 & 76.1 & 57.7 \\ \cline{2-13}  & **TRODO-Zero** & 82.1 & 80.8 & 80.4 & 77.3 & 83.8 & 88.6 & 74.8 & 72.3 & 75.0 & 75.4 & 79.2 & 78.8 \\ \cline{2-13}  & **TRODO** & **90.0** & **87.4** & **89.3** & **87.5** & **92.6** & **89.1** & **82.4** & **85.0** & **83.2** & **80.9** & **87.5** & **86.1** \\

[MISSING_PAGE_FAIL:9]

## 7 Ablation Study

**Ablation Study on Validation Dataset.** To ascertain the robustness of TRODO against different datasets in the validation set, we conducted experiments using various datasets as the validation set, as presented in Table 4. In these experiments, we replaced our default validation dataset, Tiny ImageNet, with alternative datasets. Throughout these tests, all other elements of our methodology remained constant to isolate the impact of the validation dataset changes on TRODO's performance. Moreover, to quantitatively support our claim regarding the effectiveness of near-OOD samples compared to far-OOD samples, we provide the distance between the validation set and the target dataset. The target dataset refers to the ID set on which the input classifier has been trained. For computing this distance, we used the Frechet Inception Distance (FID) [71], a well-known metric for measuring distance in generative models. Lower FID values indicate a smaller distance, and vice versa. As the results indicate, in the near-OOD scenario, our method appears more effective. More details can be found in Appendix Section L.

**Ablation Study on Boundary Confidence Level.** We also conducted an ablation study on the boundary confidence level hyperparameter, denoted as \(\gamma\), which is preset at 0.5 in our standard pipeline. By keeping all other variables constant and varying \(\gamma\) across a range of values, we assessed TRODO's sensitivity to this parameter. The results of these experiments are presented in the Table 5, illustrating how different settings of \(\gamma\) affect the effectiveness of TRODO (extra ablation studies are available in Appendix Section P).

## 8 Acknowledgments

We acknowledge Mohammad Sabokrou for his contributions to this project. Mohammad Sabokrou's work in this project was supported by JSPS KAKENHI Grant Number 24K20806.

## 9 Conclusion

In conclusion, this study presents TRODO, a robust and general method for scanning and identifying trojaned classifiers with low time and resource complexity. TRODO's strength lies in its ability to detect trojans in diverse scenarios, including those involving adversarially trained models. Interestingly, TRODO is applicable even in scenarios where no data is available. Our experimental results demonstrate TRODO's superior performance, achieving high accuracy across various attack types and benchmark datasets. The adaptability and effectiveness of our approach mark a significant advancement in enhancing the reliability and security of deep neural networks in critical applications.

\begin{table}
\begin{tabular}{c c c c c c c c c c} \hline \hline \multirow{2}{*}{Validation} & \multicolumn{2}{c}{MNIST} & \multicolumn{2}{c}{CIFAR10} & \multicolumn{2}{c}{GTSRB} & \multicolumn{2}{c}{CIFAR100} & \multicolumn{2}{c}{PubFig} \\ \cline{2-10}  & Accuracy & FID & Accuracy & FID & Accuracy & FID & Accuracy & FID & Accuracy & FID \\ \hline FMNIST & 94.6 & 67 & 78.7 & 145 & 80.4 & 156 & 69.5 & 138 & 72.1 & 120 \\ \hline SVHN & 92.3 & 118 & 82.6 & 92 & 84.3 & 105 & 74.3 & 124 & 73.2 & 137 \\ \hline STL-10 & 70.9 & 134 & 96.8 & 76 & 95.2 & 86 & 82.0 & 91 & 85.4 & 89 \\ \hline Tiny ImageNet & 91.2 & 108 & 91.0 & 72 & 96.6 & 84 & 86.7 & 79 & 88.1 & 96 \\ \hline \hline \end{tabular}
\end{table}
Table 4: Accuracy of TRODO using various Validation (and OOD) datasets for different ID data. Each validation is used to find the hyperparameters (\(\epsilon\) and \(\tau\)) and also as OOD datasets to find signatures. You can see the effect of choosing near-OOD dataset. For example, for CIFAR10, STL-10 and Tiny ImageNet are better choices than the other two datasets

\begin{table}
\begin{tabular}{c c c c c c c c} \hline \hline \multicolumn{7}{c}{\(\gamma=\) Boundary Confidence Level} \\ \hline  & 0.2 & 0.3 & 0.4 & 0.5 & 0.6 & 0.7 & 0.8 \\ \hline MNIST & 81.0 & 74.8 & 89.1 & 91.2 & 85.2 & 75.6 & 81.8 \\ \hline CIFAR10 & 88.0 & 79.7 & 85.6 & 91.0 & 81.0 & 87.5 & 77.4 \\ \hline GTSRB & 94.0 & 91.3 & 92.6 & 96.6 & 90.1 & 88.6 & 92.2 \\ \hline CIFAR100 & 77.1 & 82.7 & 80.2 & 86.7 & 84.2 & 84.3 & 76.6 \\ \hline PubFig & 78.7 & 82.5 & 84.4 & 88.1 & 90.3 & 86.2 & 79.8 \\ \hline \hline \end{tabular}
\end{table}
Table 5: Accuracy of our method with different boundary confidence level.

## References

* [1] Omkar Parkhi, Andrea Vedaldi, and Andrew Zisserman. Deep face recognition. In _BMVC 2015-Proceedings of the British Machine Vision Conference 2015_. British Machine Vision Association, 2015.
* [2] Mariusz Bojarski, Davide Del Testa, Daniel Dworakowski, Bernhard Firner, Beat Flepp, Prasoon Goyal, Lawrence D Jackel, Mathew Monfort, Urs Muller, Jiakai Zhang, et al. End to end learning for self-driving cars. _arXiv preprint arXiv:1604.07316_, 2016.
* [3] Kenneth S Miller. On the inverse of the sum of matrices. _Mathematics magazine_, 54(2):67-72, 1981.
* [4] Tianyu Gu, Brendan Dolan-Gavitt, and Siddharth Garg. Badnets: Identifying vulnerabilities in the machine learning model supply chain. _arXiv preprint arXiv:1708.06733_, 2017.
* [5] Anh Nguyen and Anh Tran. Wanet-imperceptible warping-based backdoor attack. _arXiv preprint arXiv:2102.10369_, 2021.
* [6] Yiming Li, Yong Jiang, Zhifeng Li, and Shu-Tao Xia. Backdoor learning: A survey. _IEEE Transactions on Neural Networks and Learning Systems_, 2022.
* [7] Bolun Wang, Yuanshun Yao, Shawn Shan, Huiying Li, Bimal Viswanath, Haitao Zheng, and Ben Y. Zhao. Neural cleanse: Identifying and mitigating backdoor attacks in neural networks. In _2019 IEEE Symposium on Security and Privacy (SP)_, pages 707-723, 2019.
* [8] Yingqi Liu, Wen-Chuan Lee, Guanhong Tao, Shiqing Ma, Yousra Aafer, and Xiangyu Zhang. Abs: Scanning neural networks for back-doors by artificial brain stimulation. In _Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security_, CCS '19, page 1265-1282, New York, NY, USA, 2019. Association for Computing Machinery.
* [9] Guangyu Shen, Yingqi Liu, Guanhong Tao, Shengwei An, Qiuling Xu, Siyuan Cheng, Shiqing Ma, and Xiangyu Zhang. Backdoor scanning for deep neural networks through k-arm optimization. _arXiv preprint arXiv:2102.05123_, 2021.
* [10] Wenbo Guo, Lun Wang, Xinyu Xing, Min Du, and Dawn Song. Tabor: A highly accurate approach to inspecting and restoring trojan backdoors in ai systems, 2019.
* [11] Xiaoling Hu, Xiao Lin, Michael Cogswell, Yi Yao, Susmit Jha, and Chao Chen. Trigger hunting with a topological prior for trojan detection. In _International Conference on Learning Representations_, 2022.
* [12] H. Wang, Z. Xiang, D. J. Miller, and G. Kesidis. Mm-bd: Post-training detection of backdoor attacks with arbitrary backdoor pattern types using a maximum margin statistic. In _2024 IEEE Symposium on Security and Privacy (SP)_, pages 19-19, Los Alamitos, CA, USA, may 2024. IEEE Computer Society.
* [13] Zhen Xiang, Zidi Xiong, and Bo Li. UMD: Unsupervised model detection for X2X backdoor attacks. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, _Proceedings of the 40th International Conference on Machine Learning_, volume 202 of _Proceedings of Machine Learning Research_, pages 38013-38038. PMLR, 23-29 Jul 2023.
* [14] Xinyun Chen, Chang Liu, Bo Li, Kimberly Lu, and Dawn Song. Targeted backdoor attacks on deep learning systems using data poisoning. _arXiv preprint arXiv:1712.05526_, 2017.
* [15] Brandon Tran, Jerry Li, and Aleksander Madry. Spectral signatures in backdoor attacks. _Advances in neural information processing systems_, 31, 2018.
* [16] Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. _arXiv preprint arXiv:1412.6572_, 2014.
* [17] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. _arXiv preprint arXiv:1706.06083_, 2017.
* [18] Alexander Tsigler, Gabor Lugosi, Peter Bartlett, and Phil Long. Benign overfitting in linear regression. _PNAS_, 117(48):30063-30070, 2020.
* [19] Marzieh Edraki, Nazmul Karim, Nazanin Rahnavard, Ajmal Mian, and Mubarak Shah. Odyssey: Creation, analysis and detection of trojan models. _IEEE Transactions on Information Forensics and Security_, 16:4521-4533, 2021.
* [20] Dan Hendrycks and Kevin Gimpel. A baseline for detecting misclassified and out-of-distribution examples in neural networks. In _International Conference on Learning Representations_, 2017.
* [21] Shiyu Liang, Yixuan Li, and Rayadurgam Srikant. Enhancing the reliability of out-of-distribution image detection in neural networks. _arXiv preprint arXiv:1706.02690_, 2017.
* [22] Shu Kong and Deva Ramanan. Opgenan: Open-set recognition via open data generation. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 813-822, 2021.
* [23] Stanislav Fort, Jie Ren, and Balaji Lakshminarayanan. Exploring the limits of out-of-distribution detection. _Advances in Neural Information Processing Systems_, 34:7068-7081, 2021.
* [24] Dan Hendrycks and Kevin Gimpel. A baseline for detecting misclassified and out-of-distribution examples in neural networks. _arXiv preprint arXiv:1610.02136_, 2016.
* [25] Lukas Ruff, Jacob R Kauffmann, Robert A Vandermeulen, Gregoire Montavon, Wojciech Samek, Marius Kloft, Thomas G Dietterich, and Klaus-Robert Muller. A unifying review of deep and shallow anomalydetection. _Proceedings of the IEEE_, 109(5):756-795, 2021.
* [26] Mohammadzeza Salehi, Hossein Mirzaei, Dan Hendrycks, Yixuan Li, Mohammad Hossein Rohban, and Mohammad Sabokrou. A unified survey on anomaly, novelty, open-set, and out-of-distribution detection: Solutions and future challenges. _arXiv preprint arXiv:2110.14051_, 2021.
* [27] Mohammad Azizmalayeri, Arshia Soltani Moakhar, Arman Zarei, Reihaneh Zohrabi, Mohammad Manzuri, and Mohammad Hossein Rohban. Your out-of-distribution detection method is not robust! _Advances in Neural Information Processing Systems_, 35:4887-4901, 2022.
* [28] Shao-Yuan Lo, Poojan Oza, and Vishal M Patel. Adversarially robust one-class novelty detection. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 2022.
* [29] Jiefeng Chen, Yixuan Li, Xi Wu, Yingyu Liang, and Somesh Jha. Robust out-of-distribution detection for neural networks. _arXiv preprint arXiv:2003.09711_, 2020.
* [30] Rui Shao, Pramuditha Perera, Pong C Yuen, and Vishal M Patel. Open-set adversarial defense. In _Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part XVII 16_, pages 682-698. Springer, 2020.
* [31] Rui Shao, Pramuditha Perera, Pong C Yuen, and Vishal M Patel. Open-set adversarial defense with clean-adversarial mutual learning. _International Journal of Computer Vision_, 130(4):1070-1087, 2022.
* [32] Louis Bethune, Paul Novello, Thibaut Boissin, Guillaume Coiffier, Mathieu Serrurier, Quentin Vincenot, and Andres Troya-Galvis. Robust one-class classification with signed distance function using 1-lipschitz neural networks. _arXiv preprint arXiv:2303.01978_, 2023.
* [33] Adam Goodge, Bryan Hooi, See Kiong Ng, and Wee Siong Ng. Robustness of autoencoders for anomaly detection under adversarial impact. In _Proceedings of the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence_, pages 1244-1250, 2021.
* [34] Jiefeng Chen, Yixuan Li, Xi Wu, Yingyu Liang, and Somesh Jha. Atom: Robustifying out-of-distribution detection using outlier mining. In _Machine Learning and Knowledge Discovery in Databases. Research Track: European Conference, ECML PKDD 2021, Bilbao, Spain, September 13-17, 2021, Proceedings, Part III 21_, pages 430-445. Springer, 2021.
* [35] Kiran Karra, Chace Ashcraft, and Neil Fendley. The trojai software framework: An opensource tool for embedding trojans into deep learning models, 2020.
* [36] Zhen Xiang, David J. Miller, and George Kesidis. Detection of backdoors in trained classifiers without access to the training set. _IEEE Transactions on Neural Networks and Learning Systems_, 33(3):1177-1191, 2022.
* [37] Huili Chen, Cheng Fu, Jishen Zhao, and Farinaz Koushanfar. Deepinspect: A black-box trojan detection and mitigation framework for deep neural networks. In _Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI-19_, pages 4658-4664. International Joint Conferences on Artificial Intelligence Organization, 7 2019.
* [38] Chong Fu, Xuhong Zhang, Shouling Ji, Ting Wang, Peng Lin, Yanghe Feng, and Jianwei Yin. Freeeagle: Detecting complex neural trojans in data-free cases, 2023.
* [39] Soheil Kolouri, Aniruddha Saha, Hamed Pirsiavash, and Heiko Hoffmann. Universal litmus patterns: Revealing backdoor attacks in cnns. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 301-310, 2020.
* [40] Xiaojun Xu, Qi Wang, Huichen Li, Nikita Borisov, Carl A Gunter, and Bo Li. Detecting ai trojans using meta neural analysis. In _2021 IEEE Symposium on Security and Privacy (SP)_, pages 103-120. IEEE, 2021.
* [41] Zhen Xiang, Zidi Xiong, and Bo Li. Cbd: A certified backdoor detector based on local dominant probability. _Advances in Neural Information Processing Systems_, 36, 2024.
* [42] Xiaoyu Zhang, Rohit Gupta, Ajmal Mian, Nazanin Rahnavard, and Mubarak Shah. Cassandra: Detecting trojaned networks from adversarial perturbations. _IEEE Access_, 9:135856-135867, 2021.
* [43] Yiyou Sun, Yifei Ming, Xiaojin Zhu, and Yixuan Li. Out-of-distribution detection with deep nearest neighbors. In _International Conference on Machine Learning_, pages 20827-20840. PMLR, 2022.
* [44] Jonathan Uesato, Brendan O'Donoghue, Pushmeet Kohli, and Aaron van den Oord. Adversarial risk and the dangers of evaluating against weak attacks. In Jennifer Dy and Andreas Krause, editors, _Proceedings of the 35th International Conference on Machine Learning_, volume 80 of _Proceedings of Machine Learning Research_, pages 5025-5034. PMLR, 10-15 Jul 2018.
* [45] Arun Sai Suggala, Adarsh Prasad, Vaishnavh Nagarajan, and Pradeep Ravikumar. Revisiting adversarial risk. In Kamalika Chaudhuri and Masashi Sugiyama, editors, _Proceedings of the Twenty-Second International Conference on Artificial Intelligence and Statistics_, volume 89 of _Proceedings of Machine Learning Research_, pages 2331-2339. PMLR, 16-18 Apr 2019.
* [46] Justin Khim and Po-Ling Loh. Adversarial risk bounds via function transformation, 2019.
* [47] Waleed Mustafa, Philipp Liznerski, Antoine Ledent, Dennis Wagner, Puyu Wang, and Marius Kloft. Non-vacuous generalization bounds for adversarial risk in stochastic neural networks. In Sanjoy Dasgupta, Stephan Mandt, and Yingzhen Li, editors, _Proceedings of The 27th International Conference on ArtificialIntelligence and Statistics_, volume 238 of _Proceedings of Machine Learning Research_, pages 4528-4536. PMLR, 02-04 May 2024.
* [48] Emilio Rafael Balda, Arash Behboodi, Niklas Koep, and Rudolf Mathar. Adversarial risk bounds for neural networks through sparsity based compression, 2019.
* [49] Muni Sreenivas Pydi and Varun Jog. Adversarial risk via optimal transport and optimal couplings. In Hal Daume III and Aarti Singh, editors, _Proceedings of the 37th International Conference on Machine Learning_, volume 119 of _Proceedings of Machine Learning Research_, pages 7814-7823. PMLR, 13-18 Jul 2020.
* [50] Carl-Johann Simon-Gabriel, Yann Ollivier, Leon Bottou, Bernhard Scholkopf, and David Lopez-Paz. First-order adversarial vulnerability of neural networks and input dimension. In _International conference on machine learning_, pages 5809-5817. PMLR, 2019.
* [51] Xin Zou and Weiwei Liu. On the adversarial robustness of out-of-distribution generalization models. _Advances in Neural Information Processing Systems_, 36, 2024.
* [52] Stanislav Fort. Adversarial vulnerability of powerful near out-of-distribution detection. _arXiv preprint arXiv:2201.07012_, 2022.
* [53] Maximilian Augustin, Alexander Meinke, and Matthias Hein. Adversarial robustness on in-and out-distribution improves explainability. In _European Conference on Computer Vision_, pages 228-245. Springer, 2020.
* [54] Yifan Hao and Tong Zhang. The surprising harmfulness of benign overfitting for adversarial robustness, 2024.
* [55] Tuan Anh Nguyen and Anh Tran. Input-aware dynamic backdoor attack. _Advances in Neural Information Processing Systems_, 33:3454-3464, 2020.
* [56] Mauro Barni, Kassem Kallas, and Benedetta Tondi. A new backdoor attack in cnns by training set corruption without label poisoning. In _2019 IEEE International Conference on Image Processing (ICIP)_, pages 101-105. IEEE, 2019.
* [57] Zhenting Wang, Juan Zhai, and Shiqing Ma. Bppattack: Stealthy and efficient trojan attacks against deep neural networks via image quantization and contrastive adversarial learning. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 15074-15084, 2022.
* [58] Yuezun Li, Yiming Li, Baoyuan Wu, Longkang Li, Ran He, and Siwei Lyu. Invisible backdoor attack with sample-specific triggers. In _Proceedings of the IEEE/CVF international conference on computer vision_, pages 16463-16472, 2021.
* [59] Alexander Turner, Dimitris Tsipras, and Aleksander Madry. Label-consistent backdoor attacks. _arXiv preprint arXiv:1912.02771_, 2019.
* [60] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Fei-Fei Li. Imagenet: a large-scale hierarchical image database. pages 248-255, 06 2009.
* [61] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.
* [62] Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard. Deepfool: a simple and accurate method to fool deep neural networks, 2016.
* [63] Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and generalization in neural networks. _Advances in neural information processing systems_, 31, 2018.
* [64] Johannes Stallkamp, Marc Schlipsing, Jan Salmen, and Christian Igel. The german traffic sign recognition benchmark: a multi-class classification competition. In _The 2011 international joint conference on neural networks_, pages 1453-1460. IEEE, 2011.
* [65] Neeraj Kumar, Alexander C Berg, Peter N Belhumeur, and Shree K Nayar. Attribute and simile classifiers for face verification. In _2009 IEEE 12th international conference on computer vision_, pages 365-372. IEEE, 2009.
* [66] Wenbo Jiang, Hongwei Li, Guowen Xu, and Tianwei Zhang. Color backdoor: A robust poisoning attack in color space. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 8133-8142, 2023.
* [67] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 770-778, 2016.
* [68] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual networks, 2016.
* [69] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale, 2021.
* [70] Dan Hendrycks, Mantas Mazeika, and Thomas Dietterich. Deep anomaly detection with outlier exposure. _Proceedings of the International Conference on Learning Representations_, 2019.
* [71] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, Gunter Klambauer, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a nash equilibrium. _CoRR_,abs/1706.08500, 2017.
* [72] Alexander Buslaev, Vladimir I. Iglovikov, Eugene Khvedchenya, Alex Parinov, Mikhail Druzhinin, and Alexandr A. Kalinin. Albumentations: Fast and flexible image augmentations. _Information_, 11(2), 2020.
* [73] Golnaz Ghiasi, Yin Cui, Aravind Srinivas, Rui Qian, Tsung-Yi Lin, Ekin D. Cubuk, Quoc V. Le, and Barret Zoph. Simple copy-paste is a strong data augmentation method for instance segmentation, 2021.
* [74] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. _arXiv preprint arXiv:1706.06083_, 2017.
* [75] Rahul Rade and Seyed-Mohsen Moosavi-Dezfooli. Reducing excessive margin to achieve a better accuracy vs. robustness trade-off. In _International Conference on Learning Representations_, 2021.
* [76] Kimin Lee, Kibok Lee, Honglak Lee, and Jinwoo Shin. A simple unified framework for detecting out-of-distribution samples and adversarial attacks. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 31. Curran Associates, Inc., 2018.
* [77] Jie Ren, Stanislav Fort, Jeremiah Liu, Abhijit Guha Roy, Shreyas Padhy, and Balaji Lakshminarayanan. A simple fix to mahalanobis distance for improving near-ood detection. _arXiv preprint arXiv:2106.09022_, 2021.
* [78] A. Bendale and T. E. Boult. Towards open set deep networks. In _2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 1563-1572, Los Alamitos, CA, USA, jun 2016. IEEE Computer Society.
* [79] Niladri S Chatterji and Philip M Long. Foolish crowds support benign overfitting. _Journal of Machine Learning Research_, 23(125):1-12, 2022.
* [80] Yuan Cao, Zixiang Chen, Misha Belkin, and Quanquan Gu. Benign overfitting in two-layer convolutional neural networks. _Advances in neural information processing systems_, 35:25237-25250, 2022.
* [81] Xingyu Xu and Yuantao Gu. Benign overfitting of non-smooth neural networks beyond lazy training. In _International Conference on Artificial Intelligence and Statistics_, pages 11094-11117. PMLR, 2023.
* [82] Yiwen Kou, Zixiang Chen, Yuanzhou Chen, and Quanquan Gu. Benign overfitting in two-layer relu convolutional neural networks. In _International Conference on Machine Learning_, pages 17615-17659. PMLR, 2023.
* [83] Moritz Haas, David Holzmuller, Ulrike Luxburg, and Ingo Steinwart. Mind the spikes: Benign overfitting of kernels and neural networks in fixed dimension. _Advances in Neural Information Processing Systems_, 36, 2024.
* [84] Lisha Chen, Songtao Lu, and Tianyi Chen. Understanding benign overfitting in gradient-based meta learning. _Advances in Neural Information Processing Systems_, 35:19887-19899, 2022.
* [85] Neil Mallinar, James Simon, Amirhesam Abedsoltan, Parthe Pandit, Misha Belkin, and Preetum Nakkiran. Benign, tempered, or catastrophic: Toward a refined taxonomy of overfitting. _Advances in Neural Information Processing Systems_, 35:1182-1195, 2022.
* [86] Zhu Li, Zhi-Hua Zhou, and Arthur Gretton. Towards an understanding of benign overfitting in neural networks. _arXiv preprint arXiv:2106.03212_, 2021.
* [87] Alexander Tsigler and Peter L Bartlett. Benign overfitting in ridge regression. _Journal of Machine Learning Research_, 24(123):1-76, 2023.
* [88] Ke Wang and Christos Thrampoulidis. Benign overfitting in binary classification of gaussian mixtures. In _ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)_, pages 4030-4034. IEEE, 2021.
* [89] Zhu Li, Weijie J Su, and Dino Sejdinovic. Benign overfitting and noisy features. _Journal of the American Statistical Association_, 118(544):2876-2888, 2023.
* [90] Guy Kornowski, Gilad Yehudai, and Ohad Shamir. From tempered to benign overfitting in relu neural networks. _Advances in Neural Information Processing Systems_, 36, 2024.
* [91] Xuran Meng, Difan Zou, and Yuan Cao. Benign overfitting in two-layer relu convolutional neural networks for xor data. _arXiv preprint arXiv:2310.01975_, 2023.
* [92] Jinghui Chen, Yuan Cao, and Quanquan Gu. Benign overfitting in adversarially robust linear classification. In _Uncertainty in Artificial Intelligence_, pages 313-323. PMLR, 2023.
* [93] Amartya Sanyal, Puneet K Dokania, Varun Kanade, and Philip HS Torr. How benign is benign overfitting? _arXiv preprint arXiv:2007.04028_, 2020.
* [94] Xinwei Liu, Xiaojun Jia, Jindong Gu, Yuan Xun, Siyuan Liang, and Xiaochun Cao. Does few-shot learning suffer from backdoor attacks? In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 38, pages 19893-19901, 2024.
* [95] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. _arXiv preprint arXiv:1706.06083_, 2017.
* [96] Alhussein Fawzi, Hamza Fawzi, and Omar Fawzi. Adversarial vulnerability for any classifier. _Advances in neural information processing systems_, 31, 2018.

* [97] Trevor Hastie, Andrea Montanari, Saharon Rosset, and Ryan J. Tibshirani. Surprises in high-dimensional ridgeless least squares interpolation, 2020.
* [98] Baoyuan Wu, Hongrui Chen, Mingda Zhang, Zihao Zhu, Shaokui Wei, Danni Yuan, and Chao Shen. Backdoorbench: A comprehensive benchmark of backdoor learning. _Advances in Neural Information Processing Systems_, 35:10546-10559, 2022.

## Appendix A Benign Overfitting of Trojaned Classifiers

Figure 3: Model accuracy across different architectures and datasets. Trojaned models for all backdoor attacks show a consistent slight decrease in accuracy compared to clean models, suggesting benign overfitting in Trojaned classifiers.

## Appendix B Examples of crafted Near OOD samples For Various Datasets

We provided images of some near-OOD data corresponding to random samples of our dataset (see Figure 5).

Figure 4: The effect of overlaying triggers on OOD data, in various attacks. As demonstrated, applying the trigger (which is used to poison training data) on even far-OOD samples, fools the model into identifying them as ID. This is due to the benign overfitting on the trigger present in the training data.

## Appendix C Robust OOD detection in Adversarially Trained Classifiers

Adversarial training methods AT [74] and HAT [75], designed to enhance model robustness by exposing the classifier to perturbed data during training, generally improve a model's resilience against adversarial attacks within its training distribution. However, studies [27; 29] indicate a potential limitation when such classifiers are evaluated in OOD detection tasks, where a small perturbation in attack can cause a sample from the closed set to be classified as an anomaly and vice-versa. This limitation arises because the models do not consider samples from the open set during training. We provide Table 6 from [27] that highlights the issue.

Here MD [76], Relative MD [77], and OpenMax [78] are common methods in OOD detection literature to leverage a classifier as OOD detector. The results reported for each outlier method correspond to the best-performing detection method. Notably, our approach has surpassed the

\begin{table}
\begin{tabular}{c c c c c} \hline \hline \multirow{2}{*}{Method} & \multicolumn{2}{c}{CIFAR-10} & \multicolumn{2}{c}{CIFAR-100} \\ \cline{2-5}  & Clean & Attack & Clean & Attack \\ \hline ViT (MSP) & 0.975 & 0.002 & 0.879 & 0.002 \\ ViT (MD) & 0.995 & 0.000 & 0.951 & 0.000 \\ ViT (RMD) & 0.951 & 0.025 & 0.915 & 0.037 \\ ViT (OpenMax) & 0.984 & 0.004 & 0.907 & 0.001 \\ \hline AT (MSP) & 0.735 & 0.174 & 0.603 & 0.085 \\ AT (MD) & 0.771 & 0.232 & 0.649 & 0.108 \\ AT (RMD) & 0.836 & 0.151 & 0.700 & 0.136 \\ AT (OpenMax) & 0.805 & 0.208 & 0.650 & 0.132 \\ \hline HAT (MSP) & 0.770 & 0.325 & 0.612 & 0.176 \\ HAT (MD) & 0.789 & 0.369 & 0.810 & 0.363 \\ HAT (RMD) & 0.878 & 0.258 & 0.730 & 0.191 \\ HAT (OpenMax) & 0.821 & 0.415 & 0.703 & 0.263 \\ \hline \hline \end{tabular}
\end{table}
Table 6: OOD detection AUROC under attack with \(\epsilon=\frac{8}{255}\) for various methods trained with CIFAR-10 or CIFAR-100 as the training (closed) set. A clean evaluation indicates no attack on the data, whereas an attack evaluation means that out and in data is attacked. The best and second-best results are distinguished with bold and underlined text for each column.

Figure 5: Examples of ID samples and their corresponding crafted near-OOD samples. We used Elastic [72], random rotations, and cutpaste [73].

state-of-the-art in robust out-of-distribution setting (ATD) for nearly all datasets.

\[\mu_{k}=\frac{1}{N}\sum_{i:y_{i}=k}z_{i},\quad\Sigma=\frac{1}{N}\sum_{k=1}^{K} \sum_{i:y_{i}=k}\left(z_{i}-\mu_{k}\right)\left(z_{i}-\mu_{k}\right)^{T},\quad k =1,2,\ldots,K\] (5)

In addition, to use RMD, one has to fit a \(\mathcal{N}\left(\mu_{0},\Sigma_{0}\right)\) to the whole in-distribution. Next, the distances and anomaly score for the input \(x^{\prime}\) with pre-logits \(z^{\prime}\) are computed as:

\[\begin{split} MD_{k}\left(z^{\prime}\right)=\left(z^{\prime}- \mu_{k}\right)^{T}\Sigma^{-1}\left(z^{\prime}-\mu_{k}\right),\quad RMD_{k} \left(z^{\prime}\right)=MD_{k}\left(z^{\prime}\right)-MD_{0}\left(z^{\prime} \right),\\ \operatorname{score}_{MD}\left(x^{\prime}\right)=-\min_{k}\left\{ MD_{k}\left(z^{\prime}\right)\right\},\quad\text{ score }_{RMD}\left(x^{\prime}\right)=-\min_{k}\left\{RMD_{k}\left(z^{\prime}\right) \right\}.\end{split}\] (6)Algorithms

```
0: A \(c\)-class Classifier \(f_{\theta}\), (Optional) a small set of benign samples \(\mathcal{D}_{v}\), A set of \(k\) hard transformations \(\mathcal{T}\), Adversarial perturbation budget \(\epsilon\), scanning threshold \(\tau\)
0: Decision (Trojened / Clean)
1:if\(\mathcal{D}_{v}\) is not provided then
2:\(\mathcal{D}_{v}\leftarrow\) TinyImageNet
3:endif
4:Applies a random permutation of transformations to x
5:procedureG(\(x,\mathcal{T}\))
6:\(\mathcal{T}_{\text{perm}}\leftarrow\) Randomly Permute\((\mathcal{T})\)
7:for\(t\in\mathcal{T}_{\text{perm}}\)do
8:\(x\gets t(x)\)
9:endfor
10:return\(x\)
11:endprocedure
12:Obtain\(D_{OOD}\) by applying hard augmentations on each sample of \(D_{v}\):
13:\(D_{OOD}\leftarrow\emptyset\)
14:for\(x\in\mathcal{D}_{v}\)do
15:\(x^{\prime}\leftarrow\text{G}(x,\mathcal{T})\)
16:\(\mathcal{D}_{OOD}\gets D_{OOD}\cup\{x^{\prime}\}\)
17:endfor
18:\(\Delta\mathcal{I}\leftarrow\emptyset\)
19:for\(x\in\mathcal{D}_{OOD}\)do
20: Adversarial Perturbation:
21:\(x^{*}\leftarrow\text{PGD}(f_{\theta},x,\epsilon)\)
22:ID score computation:
23:\(S_{\text{before}}\leftarrow\max_{i=1,\ldots,c}f_{\theta}^{i}(x)\)
24:\(S_{\text{after}}\leftarrow\max_{i=1,\ldots,c}f_{\theta}^{i}(x^{*})\)
25:\(\Delta ID\gets S_{\text{after}}-S_{\text{before}}\)
26: Append \(\Delta_{\text{ID}}\) to \(\Delta\mathcal{I}\)
27:endfor
28:\(S_{\text{mean}}\leftarrow\frac{1}{|\mathcal{D}_{\text{OOD}}|}\sum_{\delta_{ \text{ID}}\in\Delta\mathcal{I}}\delta_{\text{ID}}\)
29:if\(S_{\text{mean}}<\tau\)then
30:return Clean
31:else
32:return Trojaned
33:endif ```

**Algorithm 1** Trojan scanning by detection of adversarial shifts in out-of-distribution samples

## Appendix E Extended Related Work

**Backdoor Attacks.** Injecting pre-defined triggers into the training data is the most common approach to implement backdoor attacks. BadNet [4] is the first backdoor attack against DNN models, which involves modifying a clean image by inserting a small, predetermined pattern at a fixed location, thus replacing the original pixels. Blended [14] aimed to enhance the invisibility of the trigger pattern by seamlessly blending it into the clean image through alpha blending. SIG [56] utilized a sinusoidal waveform signal as the trigger pattern. To achieve better stealthiness, many attacks with invisible and dynamic triggers have been proposed. Input-aware [55] proposed a training-controllable attack method that simultaneously learned the model parameters and a trigger generator to produce a unique trigger pattern for each clean test sample. For more details regarding other attacks including BPP [57], SSBA [58], WaNet [5], and [66] read Appendix Section H.

**Benign overfitting** The phenomenon of benign overfitting, where models perfectly fit noisy data without compromising generalization, was first explored in [18]. They characterized the conditions under which the minimum norm interpolating prediction rule achieves near-optimal accuracy, emphasizing the necessity of overparameterization. Subsequent studies extended these findings to various neural network architectures. Notably [79] delves into sparse interpolating procedures for linear regression with Gaussian data, highlighting conditions under which benign overfitting occurs in overparameterized regimes. Their work establishes lower bounds on excess risk, proving that overfitting can indeed be benign.

Two-layer neural networks have also been extensively studied to understand benign overfitting under various conditions. Benign overfitting in two-layer convolutional neural networks is investigated by [80], identifying a phase transition between benign and harmful overfitting. Similarly, [81] analyzes non-smooth neural networks, providing theoretical insights into when overfitting can remain benign even beyond lazy training scenarios. This exploration is extended to ReLU networks in [82], demonstrating the conditions that facilitate benign overfitting and the sharp transitions to harmful overfitting. Additionally, [83] shows that overfitting in Sobolev RKHSs can achieve optimal rates without being intrinsically harmful.

The phenomenon is also observed in more complex architectures. Gradient-based meta learning is examined in [84], revealing that benign overfitting in empirical risk minimization (ERM) can extend to meta-learning algorithms like MAML. A refined taxonomy of overfitting in proposed in [85], identifying tempered overfitting as an intermediate regime between benign and catastrophic overfitting. Benign overfitting has been studied in various other architectures and settings as well [86, 87, 88, 89, 90, 91].

Benign overfitting has been studied in the context of adversarial robustness in [54, 92, 93]. Notably, [54] theoretically shows for linear and two-layer networks that benign overfitting will become harmful overfitting under adversarial attacks.

The interplay between benign overfitting and security vulnerabilities like backdoor attacks is critical in [94] revealing that few-shot learning models tend to overfit benign or poisoned features, impacting robustness.

Adversarial riskAdversarial risk refers to the vulnerability of machine learning models to adversarial examples--perturbations intentionally crafted to mislead the model. Research in this area seeks to understand and mitigate these risks. A seminal work by [95] presented robust optimization techniques to defend against first-order adversarial attacks, establishing foundational adversarial training methodologies. Following this, other studies have explored the theoretical limits of adversarial robustness. A framework to evaluate the adversarial vulnerability of any classifier is provided in [96], showing intrinsic limitations based on the classifier's architecture and data distribution.

Previous work has established bounds on this metric via function transformation [46], PAC-Bayesian [47], sparsity-based compression [48], Optimal Transport and Couplings [49], or in terms of input dimension [50].

The intersection of adversarial risk and out-of-distribution (OOD) detection has garnered increasing attention. The vulnerabilities in existing OOD generalization methods to adversarial attacks are identified in [51], prompting the development of algorithms to enhance OOD adversarial robustness. RATIO was introduced by [53], a training procedure that improves adversarial robustness for both in-distribution and OOD samples, thereby enhancing model explainability. The adversarial vulnerability of current OOD detection techniques is discussed in [52], suggesting that ensemble methods and combining multiple OOD detectors can significantly enhance robustness against adversarial attacks.

Understanding the theoretical limits of adversarial vulnerability remains crucial for developing robust models. It is proved in [50] that adversarial vulnerability increases with the gradients of the training objective and scales with the square root of the input dimension, making larger images more vulnerable. The trade-offs between robustness and accuracy are explored in [96], establishing a mathematical framework to evaluate these limits. This discussion is extended in [95] through robust optimization, quantifying the trade-offs, and providing guidelines for creating more resilient models. These foundational works underscore the inherent challenges in achieving robustness, emphasizing the need for innovative approaches to bridge the gap between theory and practical applications.

## Appendix F Theoretical Proofs

Proof of Theorem 1

Proof.: Let \(\mu\in\mathbb{R}^{d_{\mu}}\) be arbitrary, by taylor series around \(\mu\), we have:\[h(w,x)=\sum_{|\gamma|=k+1}\frac{\nabla_{x}^{\gamma}h(w,\mu)}{\gamma!}(x-\mu)^{ \gamma}+\sum_{j\neq k+1}\sum_{|\gamma|=j}\frac{\nabla_{x}^{\gamma}h(w,\mu)}{ \gamma!}(x-\mu)^{\gamma}\]

By taking derivative we have:

\[\nabla_{x}h(w,x)=\nabla_{x}\sum_{|\gamma|=k+1}\frac{\nabla_{x}^{\gamma}h(w, \mu)}{\gamma!}(x-\mu)^{\gamma}+\sum_{j\neq k+1}\nabla_{x}\sum_{|\gamma|=j} \frac{\nabla_{x}^{\gamma}h(w,\mu)}{\gamma!}(x-\mu)^{\gamma}\]

The distribution \(\mathcal{P}_{+s}^{k}\) has the same \(j\)-th order moments as the distribution \(\mathcal{P}\) for all \(j\neq k\), therefore, by taking expectation and using the triangle inequality we have:

\[\mathcal{R}_{\alpha}^{\mathcal{P}_{+s}^{k}}(h,w)=\alpha\mathbb{E }_{x\sim\mathcal{P}_{+s}^{k}}\left[\|\nabla_{x}h(w,x)\|\right]\geq\alpha\| \mathbb{E}_{x\sim\mathcal{P}_{+s}^{k}}\nabla_{x}h(w,x)\|\] \[\geq\alpha\|\mathbb{E}_{x\sim\mathcal{P}_{+s}^{k}}\left[\nabla_{ x}\sum_{|\gamma|=k+1}\frac{\nabla_{x}^{\gamma}h(w,\mu)}{\gamma!}(x-\mu)^{ \gamma}\right]-\mathbb{E}_{x\sim\mathcal{P}}\left[\nabla_{x}\sum_{|\gamma|=k+ 1}\frac{\nabla_{x}^{\gamma}h(w,\mu)}{\gamma!}(x-\mu)^{\gamma}\right]\|\] \[-\alpha\|\mathbb{E}_{x\sim\mathcal{P}_{+s}^{k}}\left[\nabla_{x}h (w,x)-\nabla_{x}\sum_{|\gamma|=k+1}\frac{\nabla_{x}^{\gamma}h(w,\mu)}{\gamma! }(x-\mu)^{\gamma}\right]+\mathbb{E}_{x\sim\mathcal{P}}\left[\nabla_{x}\sum_{| \gamma|=k+1}\frac{\nabla_{x}^{\gamma}h(w,\mu)}{\gamma!}(x-\mu)^{\gamma}\right]\|\]

\[=\alpha\|s\nabla_{x}\sum_{|\gamma|=k}\frac{\nabla_{x}^{\gamma}h(w,\mu)}{ \gamma!}\|\]

\[-\alpha\|\mathbb{E}_{x\sim\mathcal{P}}\left[\nabla_{x}h(w,x)-\nabla_{x}\sum_ {|\gamma|=k+1}\frac{\nabla_{x}^{\gamma}h(w,\mu)}{\gamma!}(x-\mu)^{\gamma} \right]+\mathbb{E}_{x\sim\mathcal{P}}\left[\nabla_{x}\sum_{|\gamma|=k+1}\frac{ \nabla_{x}^{\gamma}h(w,\mu)}{\gamma!}(x-\mu)^{\gamma}\right]\|\]

Note that all moments with orders less than \(k\) are equal, hence the difference of the moment of order \(k\) around \(\mu\) is equal to the difference of the moment of order \(k\) around 0. Since \(\mu\) was arbitrary, we conclude:

\[\mathcal{R}_{\alpha}^{\mathcal{P}_{+s}^{k}}(h,w)\geq\alpha|s|\max_{x}\|\nabla _{x}\sum_{|\gamma|=k}\frac{\nabla_{x}^{\gamma}h(w,x)}{\gamma!}\|-\alpha\| \mathbb{E}_{x\sim\mathcal{P}}\nabla_{x}h(w,x)\|.\]

**Proof of Theorem 2**

**Linear neural network**

Proof.: We define \(X=[x_{1},\ldots,x_{n}]^{\top}\in\mathbb{R}^{n}\), \(X^{\prime}=[x_{1}^{\prime}+t,\ldots,x_{m}^{\prime}+t]^{\top}\in\mathbb{R}^{m}\), \(Y=[y_{1},\ldots,y_{n}]^{\top}\in\mathbb{R}^{n}\), \(Y^{\prime}=[y_{c},\ldots,y_{c}]^{\top}\in\mathbb{R}^{m}\), then we have:

\[\hat{w}=\left(\begin{bmatrix}X\\ X^{\prime}\end{bmatrix}^{\top}\begin{bmatrix}X\\ X^{\prime}\end{bmatrix}\right)^{-1}\begin{bmatrix}X\\ X^{\prime}\end{bmatrix}^{\top}\begin{bmatrix}Y\\ Y^{\prime}\end{bmatrix}=\left(X^{\top}X+X^{\prime\top}X^{\prime}\right)^{-1} \left(X^{\top}Y+X^{\prime\top}Y^{\prime}\right)\]Note that \(\lim_{n\rightarrow\infty}n\left(X^{\top}X\right)^{-1}=C\) is a constant matrix (inverse of the covariance matrix) and \(\lim_{n\rightarrow\infty}\frac{1}{n}\sum_{i=1}^{n}x_{i}=s\) is a constant vector (mean vector), and we have \(\lim_{n\rightarrow\infty}\sum_{i=1}^{m}x_{i}^{\prime}=ms\) by the law of large numbers. If \(rank(X^{\prime\top}X^{\prime})=1\), then according to the Miller's Lemma [3]:

\[\lim_{n\rightarrow\infty}\|n\left(X^{\top}X+X^{\prime\top}X^{\prime}\right)^{ -1}=C-\frac{Ck}{1+tr(k)}\|=\Omega(1)\]

where \(k=\left(X^{\prime\top}X^{\prime}\right)\left(X^{\top}X\right)^{-1}\approx \left(\frac{m}{n}X^{\top}X+mst^{\top}+ms^{\top}t+mtt^{\top}\right)\left(X^{ \top}X\right)^{-1}\). If \(rank(X^{\prime\top}X^{\prime})\geq 2\), we can decompose it into sum matrices with rank 1 and inductively infer the same bound \(\Omega(1)\). Now we have:

\[\lim_{n\rightarrow\infty}\|\frac{1}{n}\left(X^{\top}Y+X^{\prime\top}Y^{\prime }\right)=\frac{1}{n}X^{\top}Y+\frac{m}{n}y_{c}(s+t^{\top})\|=\Omega(\frac{m}{ n}\|t\|).\]

Therefore, we conclude:

\[\lim_{n\rightarrow\infty}\mathcal{R}_{\alpha}^{\mathcal{P}}(h_{1},\hat{w})= \lim_{n\rightarrow\infty}\|\nabla_{x}h_{1}(\hat{w},x)\|=\lim_{n\rightarrow \infty}\|\hat{w}\|=\Omega\left(\frac{m}{n}\|t\|\right).\]

#### Two-layer neural network

Proof.: We define \(F=[h_{2}(w_{0},x_{1}),\ldots,h_{2}(w_{0},x_{n})]^{\top}\in\mathbb{R}^{n}\), \(\nabla F=[\nabla_{w}h_{2}(w_{0},x_{1}),\ldots,\nabla_{w}h_{2}(w_{0},x_{n})]^{ \top}\in\mathbb{R}^{k(p+1)\times n}\), \(F^{\prime}=[h_{2}(w_{0},x_{1}^{\prime}+t),\ldots,h_{2}(w_{0},x_{m}^{\prime}+t )]^{\top}\in\mathbb{R}^{m}\), \(\nabla F^{\prime}=[\nabla_{w}h_{2}(w_{0},x_{1}^{\prime}+t),\ldots,\nabla_{w}h _{2}(w_{0},x_{m}^{\prime}+t)]^{\top}\in\mathbb{R}^{k(p+1)\times m}\), \(y=[y_{1},\ldots,y_{n}]^{\top}\in\mathbb{R}^{n}\), and \(y^{\prime}=[y_{c},\ldots,y_{c}]^{\top}\in\mathbb{R}^{m}\).

Assume the two-layer network \(h_{2}(w,x)\) as described in the paper is trained using gradient descent on \(\mathcal{D}\cup\mathcal{D}^{\prime}\) with learning rates \(\eta<1/\lambda_{max}([\nabla F,\nabla F^{\prime}]^{\top}[\nabla F,\nabla F^{ \prime}])\). According to Proposition 1 in [97], the optimal solution will be:

\[\hat{w}=\left(\begin{bmatrix}\nabla F\\ \nabla F^{\prime}\end{bmatrix}^{\top}\begin{bmatrix}\nabla F\\ \nabla F^{\prime}\end{bmatrix}\right)^{-1}\begin{bmatrix}\nabla F\\ \nabla F^{\prime}\end{bmatrix}^{\top}\begin{bmatrix}Y-F\\ Y^{\prime}-F^{\prime}\end{bmatrix}\] \[=\left(\nabla F^{\top}\nabla F+\nabla F^{\prime\top}\nabla F^{ \prime}\right)^{-1}\left(\nabla F^{\top}(Y-F)+\nabla F^{\prime\top}(Y^{ \prime}-F^{\prime})\right)\]

For any \(i\in\{1,\ldots,m\},j\in\{1,\ldots,k\}\) we have:

\[\nabla_{w}h_{2}(w_{0},x_{i}^{\prime}+t)_{j}=\frac{1}{\sqrt{kp}} \left[u_{0,j}\text{ReLU}^{\prime}(\theta_{0,j}^{T}(x_{i}^{\prime}+t))(x_{i}^{ \prime}+t),\text{ReLU}(\theta_{0,j}^{T}(x_{i}^{\prime}+t))\right]\] \[=\begin{cases}\frac{1}{\sqrt{kp}}\left[u_{0,j}(x_{i}^{\prime}+t), \theta_{0,j}^{T}(x_{i}^{\prime}+t)\right],&\text{if }\theta_{0,j}^{T}(x_{i}^{\prime}+t)\geq 0\\ \frac{1}{\sqrt{kp}}\left[0,0\right],&\text{otherwise}\end{cases}\]

Therefore, we can assume there are some constant matrices \(G_{1},G_{2},G_{1}^{\prime},G_{2}^{\prime}\) such that \(F^{\prime}=tG_{1}+G_{2}\) and \(\nabla F^{\prime}=tG_{1}^{\prime}+G_{2}^{\prime}\). Moreover, note that \(\lim_{n\rightarrow\infty}n\left(\nabla F^{\prime}\top\nabla F\right)^{-1}=C_{1}\) is a constant matrix (inverse of the covariance matrix). If \(rank(\nabla F^{\prime\top}\nabla F^{\prime})=1\), then according the Miller's Lemma [3]:

\[\lim_{n\rightarrow\infty}\|n\left(\nabla F^{\top}\nabla F+\nabla F^{\prime \top}\nabla F^{\prime}\right)^{-1}=C_{1}-\frac{C_{1}k}{1+tr(k)}\|=\Omega(1)\]

where \(k=\left(\nabla F^{\prime\top}\nabla F^{\prime}\right)\left(\nabla F^{\top} \nabla F\right)^{-1}\approx\left(\frac{m}{n}\nabla F^{\top}\nabla F+\Omega(\|mt \|)\right)\left(\nabla F^{\top}\nabla F\right)^{-1}\). If \(rank(\nabla F^{\prime\top}\nabla F^{\prime})\geq 2\), we can decompose it into sum matrices with rank 1 and inductively infer the same bound \(\Omega(1)\). Now we have:

\[\lim_{n\rightarrow\infty}\|\frac{1}{n}\left(\nabla F^{\top}(Y-F)+\nabla F^{ \prime\top}(Y^{\prime}-F^{\prime})\right)\|=\Omega(\frac{m}{n}\|t\|).\]Therefore, we conclude:

\[\lim_{n\rightarrow\infty}\|\hat{w}\|=\Omega\left(\frac{m}{n}\|t\|\right).\]

Now we have:

\[\mathcal{R}_{\alpha}^{\mathcal{P}}(\tilde{h_{2}},\hat{w})=\mathbb{E }_{x}\left[\sup_{\|\delta\|\leq\alpha}\tilde{h_{2}}(\hat{w},x+\delta)-\tilde{h_{ 2}}(\hat{w},x)\right]\] \[=\alpha\mathbb{E}_{x}\|\nabla_{x}\tilde{h_{2}}(\hat{w},x)\|= \alpha\mathbb{E}_{x}\|\nabla_{x}h_{2}(w_{0},x)+\frac{\partial^{2}h_{2}(w_{0}, x)}{\partial w\partial x}(\hat{w}-w_{0})\|=\Omega(\|\hat{w}\|)\]

Therefore, we conclude:

\[\lim_{n\rightarrow\infty}\mathcal{R}_{\alpha}^{\mathcal{P}}(\tilde{h_{2}}, \hat{w})=\lim_{n\rightarrow\infty}\|\hat{w}\|=\Omega\left(\frac{m}{n}\|t\| \right).\]

## Appendix G Preliminaries

**Adverserial attack to classifiers** It has been shown that DNNs are vulnerable to adversarial attacks across various tasks, predominantly explored in classification tasks. During inference, adversarial perturbations added to input data can cause classifiers to mispredict their labels. In other words, a perturbation such as \(\delta\) is added to a sample \(x\) with label \(y\) to fool the model into outputting \(\hat{y}\) for the adversarial input sample \(x^{*}\), where \(x^{*}=x+\delta\). Specifically, PGD is an iterative adversarial attack [17] that crafts adversarial samples by ensuring the noise is projected within the \(\ell_{\infty}\) norm in N-step:

\[x_{0}^{*}=x,\qquad x_{t+1}^{*}=\Pi_{x+\mathcal{S}}(x_{t}^{*}+\alpha\cdot \text{sign}\left(\nabla_{x}J\left(x_{t}^{*},y\right)\right)),\qquad x^{*}=x_{N }^{*}\]

where \(\Pi_{x+\mathcal{S}}\) denotes the projection on the norm ball \(\mathcal{S}\) around \(x\) and \(J\left(x_{t}^{*},y\right)\) denotes the targeted objective function, which for classifiers is cross entropy. Previous studies have shown that standardized classifiers are vulnerable to adversarial attacks, even weak attacks like FGSM. Various defense mechanisms have been proposed to address this challenge, with adversarial training on the training dataset being the most effective defense.

**Using a Classifier as an OOD Data Detector** Classifiers such as \(f\), trained on a dataset denoted as \(D\), can be used as OOD detectors due to their learned features. Specifically, another dataset with separate semantics from \(D\), denoted as \(D^{\prime}\), is assumed to be the source of OOD samples where \(D\) represents in-distribution samples. For instance, a classifier trained on Cifar10 is used to detect Cifar100 as OOD samples. The instructions to detect OOD samples practically involve using the output distribution of a classifier over the classes of \(D\) for a given input, offering insights into the model's prediction confidence. Specifically, classifiers tend to be more confident about ID samples compared to OOD samples. recently The Maximum Softmax Probability (MSP) has been proposed as an indicator of this confidence.

formally a well-trained classifier \(f\) logits from its penultimate layer on \(D\) with \(k\) classes, for a given input sample \(x\) are represented by \(z=f(x)\), where \(z\) is a vector of unnormalized prediction scores \([z_{1},z_{2},\ldots,z_{k}]\) for sample \(x\). Applying the softmax function results in:

\[\text{softmax}(z_{i})=\frac{e^{z_{i}}}{\sum_{j=1}^{k}e^{z_{j}}},\qquad\text{ MSP}_{f}(x):=\max_{i\in\{1,\ldots,k\}}(\text{softmax}(z_{i}))\]

where \(z_{i}\) is the logit corresponding to the \(i\)-th class.

intuitively MSP\({}_{f}(x)\) represents the highest probability assigned to any class by the model, reflecting the model's confidence level in its prediction.

Datasets Details

#### Details for the Backdoor Attacks

This section provides detailed descriptions of the backdoor attacks employed in our study, focusing particularly on the nature and implementation of their triggers.

**BadNet**[4] embeds a malicious trigger, typically a small and visually distinctive patch, into the training data. This trigger is designed to be inconspicuous enough to evade detection yet recognizable by the trained model, leading it to misclassify inputs containing the trigger.

**Blended**[14] subtly blends a trigger into the training images at low intensity, making it hard to detect by human inspectors or simple automated methods. The trigger effectively conditions the model to associate the slightly altered patterns with incorrect outputs.

**SIG**[56] corrupts training images with a sinusoidal pattern, superimposed in a way that does not require changes to the image labels. This makes the backdoor particularly stealthy as it avoids the common detection methods that look for label inconsistencies.

**BPP**[57] uses image quantization combined with contrastive adversarial learning to craft triggers that are embedded into the pixel values themselves. This approach alters the image at a bit-per-pixel level, making the modifications difficult to perceive or reverse-engineer.

**Input-aware** attacks [55] dynamically adjust their triggers based on the input features, allowing the backdoor to activate only under specific conditions that are predetermined by the attacker. This adaptability makes the attack highly elusive and challenging to detect.

**WaNet**[5] introduces imperceptible warping to the image, manipulating its geometric properties subtly. This warping acts as a trigger that is extremely hard to spot with the naked eye, ensuring the model misclassifies the warped input while appearing normal to human observers.

**SSBA**[58] crafts sample-specific triggers that are invisible to human detection by embedding them in a way that aligns closely with the natural image structure. These triggers are tailored to each individual sample, increasing the difficulty of detecting and isolating the backdoor through general analysis.

**Color**[66] alters the color distribution of the input images, employing changes in the color channels as the trigger mechanism. This type of manipulation can remain under the radar of typical visual inspections while effectively conditioning the model to respond to altered color cues.

These descriptions underline the diversity of the backdoor mechanisms used, highlighting the challenges in detecting and mitigating such threats in machine learning models.

## Appendix I Previous Trojan Scanning methods

#### Implementation

We implemented the baseline methods using their implementations in their publicly available GitHub repositories. For methods requiring supervision to define a threshold, we followed the same approach as UMD.

#### Review of the Methods

NcNeural Cleanse [7] is a method for scanning models by reverse engineering triggers and identifying outliers with significantly smaller perturbations. NC faces significant computational overhead, sensitivity to trigger complexity, and potential false positives. Moreover it is primarily designed to handle only All-to-One attacks.

**ABS** Artificial Brain Stimulation [8] detects backdoors in neural networks by stimulating individual neurons and analyzing their impact on output activations. Compromised neurons that substantially elevate a specific label are identified, and potential triggers are reverse-engineered to confirm the presence of backdoors. the method involves significant computational overhead and is sensitive to its underlying assumptions about compromised neurons and trigger behavior. ABS may struggle with more advanced attack scenarios and primarily handles All-to-One attacks. Additionally, benign neurons with unique features may lead to false positives, limiting its robustness in some cases.

TaborTABOR[10] is a trojan detection method for DNNs that frames the detection task as an optimization problem. It incorporates an objective function with regularization terms inspired by explainable AI techniques to guide the optimization process and reduce the adversarial search space, improving trigger identification accuracy.. However, TABOR's significant computational overhead present challenges. Additionally, it is primarily designed for geometric or symbolic triggers, potentially limiting effectiveness against irregular shapes trojans.

Pt-RedPT-RED[36] is a post-training backdoor detection method for DNNs. It reverse-engineers potential backdoor trigger patterns by solving an optimization problem to find minimal perturbations that cause misclassifications. However, it is primarily designed for single target class attacks with small trigger patterns. Moreover, the reverse-engineering process for identifying potential triggers is computationally intensive.

K-ArmK-Arm[9] leverages an optimization approach inspired by the Multi-Armed Bandit problem to iteratively select and optimize class labels for potential backdoor triggers. This method improves detection efficiency compared to exhaustive search methods by using stochastic selection. However, the method still involves computational overhead and primarily focuses on specific and simple types of backdoor attacks. Additionally, K-Arm may face challenges in handling scenarios with more than one target label.

UmdUnsupervised Model Detection[13] is designed to detect X2X backdoor attacks, where multiple source classes are mapped to multiple target classes. The method involves reverse-engineering triggers for each class pair using a small set of clean data, and then defining a transferability statistic (TR) for each class pair. TR measures how well the trigger for one class pair transfers to another. These statistics are used to select likely backdoor class pairs. An unsupervised anomaly detector then evaluates the aggregated trigger statistics to determine if a model is backdoored. However, the method involves complex optimization processes, which can be computationally intensive. Handling very large datasets or a high number of classes can pose scalability challenges. Furthermore, UMD may struggle to handle models with multiple different trigger patterns, as it relies on TR statistics that assume a single dominant trigger pattern and this method is not trigger-agnostic as its approach depends on the specific type of attack.

Mm-BdMaximum Margin Backdoor Detection[12] is designed to detect backdoor attacks in neural networks, regardless of the backdoor pattern type. The method operates by estimating a maximum margin statistic for each class through gradient ascent from multiple random initializations, without any clean samples, and then using these statistics in an unsupervised anomaly detection framework to identify backdoor attacks. However, the method exhibits a large false-positive rate for datasets with a small number of class, and it struggles to detect attacks with more than one target label, where each source class may be mapped to a different target class. Additionally, MM-BD's effectiveness is significantly reduced when an adaptive attacker manipulates the learning process.

MntdMNTD[40] identifies backdoors in neural networks by training a binary meta-classifier on features extracted from numerous shadow classifiers, both benign and Trojaned. However, this method struggles to generalize to types of attacks and model architecture it wasn't trained on.

## Appendix J Broader Impact

Our study introduces a method designed to identify potential backdoors embedded within classifiers. As a result, our work contributes positively to societal impacts by enhancing security measures in machine learning applications and mitigating risks associated with malicious interventions.

## Appendix K Baselines and Evaluation Benchmark

Baselines

In our evaluation, TRODO and TRODO-Zero are assessed alongside previous scanning methods including Neural Cleanse (NC)[7], ABS[8], PT-RED[36], TABOR[10], K-Arm[9], MM-BD[12], and UMD[13]. For an equitable comparison, we set the confidence threshold at 95%, corresponding to a 5% desired false positive rate for NC, PT-RED, and UMD which are based on unsupervised threshold settings. Similarly, for ABS, and K-Arm, which rely on supervised threshold adjustment, we maintained a consistent false positive rate of 5% across various tests and dataset configurations to maximize true positive outcomes. The capabilities and performance outcomes of these methods are detailed in Table 1. Further details can be found in Appendix Section I.

### Our Designed Benchmark

To effectively model real-world scenarios for the scanning task, we developed a benchmark covering various scenarios involving both clean and trojan classifiers. Specifically, to evaluate the generality of scanning methods, which is crucial in real-world applications, we included several datasets ranging from low to high resolution and different classifiers with various architectures. Moreover, different trojan attacks and label mappings were also considered, with classifiers covering both standard and adversarial training methods. More details about our developed benchmark are provided below.

**Image Datasets** Our benchmark comprises image datasets from various domains, including CIFAR10, CIFAR100 [61], GTSRB [64], PubFig [65], and MNIST. We considered two kinds of label mapping, _All to One_ and _All to All_.

**Trojan Attacks** In the former, the label of poisoned samples is changed to a single target class. In the latter, unlike All to One case, each class can be mapped to any arbitrary target class, which makes this setting more challenging. We included eight trojan attacks, comprising BadNet [4], Input-aware [55], BPP [57], SIG [56], WaNet [5], Color [66], SSBA [58] and Blended [14]. The test set for each combination of image dataset and label mapping consists of a total of 320 models. We trained 20 trojaned models using each type of attack. We included 160 clean models, resulting in a balanced set.

**Adversarial Training** To encompass a wider variety of scenarios, we evaluated each configuration using both standard and adversarial training. We employed PGD-10 with \(\epsilon=\frac{2}{255}\) for adversarial training.

**Classifier Architectures** We considered various architectures, including ResNet18 [67], PreActResNet18 [68], and ViT-B/16 [69]. Previous works have solely focused on CNN-based architectures, underscoring the generality of our experiments.

### TrojAI Benchmark

Another challenging benchmark that we include in our experiments is TrojAI [35], a benchmark developed by IARPA designed to address challenges in backdoor detection. The license of this benchmark is Apache 2.0.

**Structure** For each round of the competition, a test set, a hold-out set, and a training set of models are available and can be accessed from the TrojAI homepage. Almost half of the models in each set are trojaned. A small set of benign samples from the training dataset of each model is provided along with the model itself.

**Trojan Attacks** The models may be trojaned with various kinds of backdoors, including universal and label-specific. The triggers could be pixel patterns and Instagram filters. Triggers can be embedded within the model such that activation occurs only under specific conditions, such as possessing a certain texture or being located in a designated area of the image. The complexity of models and trojan attacks grows from round to round. The performance of methods on this benchmark is provided in Table 2. We evaluate methods in terms of scanning accuracy and average scanning time for a model.

## Appendix L Frechet Inception Distance (FID)

The Frechet Inception Distance (FID) is a metric used to evaluate the quality of generative models, such as GANs. It compares the distribution of generated images to the distribution of real images by measuring the distance between two multivariate Gaussian distributions fitted to the feature representations of these images.

Given a set of real images \(\{x_{i}\}_{i=1}^{N_{r}}\) and a set of generated images \(\{\hat{x}_{j}\}_{j=1}^{N_{g}}\), pass both sets through a pre-trained Inception v3 network to obtain feature representations. Let \(\phi(x)\) denote the feature representation of image \(x\) from the Inception network.

Calculate the mean and covariance of the feature representations for real images:

\[\mu_{r}=\frac{1}{N_{r}}\sum_{i=1}^{N_{r}}\phi(x_{i})\]

\[\Sigma_{r}=\frac{1}{N_{r}-1}\sum_{i=1}^{N_{r}}(\phi(x_{i})-\mu_{r})(\phi(x_{i}) -\mu_{r})^{T}\]

Similarly, compute the mean and covariance for generated images:

\[\mu_{g}=\frac{1}{N_{g}}\sum_{j=1}^{N_{g}}\phi(\hat{x}_{j})\]

\[\Sigma_{g}=\frac{1}{N_{g}-1}\sum_{j=1}^{N_{g}}(\phi(\hat{x}_{j})-\mu_{g})(\phi( \hat{x}_{j})-\mu_{g})^{T}\]

The FID is defined as the Frechet distance between the two multivariate Gaussian distributions \(\mathcal{N}(\mu_{r},\Sigma_{r})\) and \(\mathcal{N}(\mu_{g},\Sigma_{g})\):

\[\text{FID}=\|\mu_{r}-\mu_{g}\|^{2}+\text{Tr}(\Sigma_{r}+\Sigma_{g}-2(\Sigma_{ r}\Sigma_{g})^{\frac{1}{2}})\]

Here, \(\|\mu_{r}-\mu_{g}\|^{2}\) is the squared Euclidean distance between the mean vectors. Tr denotes the trace of a matrix. \((\Sigma_{r}\Sigma_{g})^{\frac{1}{2}}\) is the matrix square root of the product of the two covariance matrices.

Lower FID scores indicate that the generated images have a distribution more similar to the real images, suggesting higher quality. Higher FID scores indicate greater dissimilarity between the distributions of generated and real images, suggesting lower quality.

## Appendix M Limitation

In this study, we utilized a validation set and a surrogate model to select hyperparameters, such as epsilon for the PGD attack. However, these hyperparameters are architecture-specific. Therefore, each new architecture requires a tailored approach: training on a validation set and tuning its corresponding hyperparameters for the input classifier. This process can be time-consuming when encountering new architectures, although we limited our consideration to common architectures. Additionally, our research focuses on scanning trojaned classifiers. However, this task might need to be adapted for different tasks, such as object detectors, where our method would require adjustments to handle these cases effectively. Morever, our theoretical results on adversarial risk are limited to the noiseless settings and only address the backdoor effect up to two layered networks which could be extended in future work.

## Appendix N Models Dataset Creation Details

In our study, we generated 20 distinct models for each of the eight backdoor attack types across the ResNet18 and PreAct-ResNet18 architectures and 5 models for ViT for each dataset and attack, totaling 160 models for ResNet18 and PreAct-ResNet18 and 40 models for ViT. We utilized the BackdoorBench framework [98] (https://github.com/SCLBD/BackdoorBench) for seven attacks (BadNets, Blended, WaNet, SIG, BPP, Input-aware, and SSBA) and integrated an additional attack (Color attack [66]) using this repository (https://github.com/lyx1224/color-backdoor).

To ensure consistency and rigor, we adopted core framework settings from BackdoorBench and extended them to encompass both standard and novel attacks, adjusting parameters specifically for the COLOR attack. Our consistent methodology, following BackdoorBench's protocols, allowed us to systematically evaluate the individual effects of each backdoor strategy across the architectures, achieving a high standard of experimental reliability and detail.

[MISSING_PAGE_EMPTY:29]

## Appendix P Extra Ablation Studies

### Performance of TRODO under different poisoning rates

Intuitively, increasing the poisoning rate enlarges the blind spots in trojaned classifiers, as these are boundary regions where the poisoned data causes the model to overfit. Consequently, this will increase the probability that TRODO detects the trojaned classifiers. However, our signature is based on the presence of blind spots in trojaned classifiers and shows consistent performance across different poisoning rates. In the paper, we considered a poisoning rate of 10% as it is common in the literature. In addition, we have provided TRODO's performance for different poisoning rates in Table 12 (other components of TRODO remained fixed.)

### Performance of TRODO-Zero under different OOD sample rates

In the first experiment, we performed an ablation study on the number of samples in our validation set. By default, TRODO-Zero uses 1% of the Tiny ImageNet validation dataset, which contains 200 classes, each with 500 samples. We explored the effect of varying the number of sample rates in Table 13.

\begin{table}
\begin{tabular}{c c c c c c c} \hline \hline \multirow{2}{*}{Validation} & \multicolumn{2}{c}{**ResNet-18**} & \multicolumn{2}{c}{**PreAct ResNet-18**} & \multicolumn{2}{c}{**ViT-b-16**} \\ \cline{2-7}  & \(\epsilon\) & \(\tau\) & \(\epsilon\) & \(\tau\) & \(\epsilon\) & \(\tau\) \\ \hline \hline FMNIST & 0.0491 & 1.1625 & 0.0538 & 1.0407 & 0.0621 & 0.9341 \\ \hline SVHN & 0.0476 & 1.1338 & 0.0524 & 1.0025 & 0.0598 & 0.9106 \\ \hline STL-10 & 0.0488 & 1.1571 & 0.0530 & 1.0462 & 0.0611 & 0.9246 \\ \hline TinyImageNet & 0.0483 & 1.1523 & 0.0527 & 1.0179 & 0.0609 & 0.9150 \\ \hline \hline \end{tabular}
\end{table}
Table 10: Value of \(\epsilon\) and \(\tau\) for different validation sets and backbone architectures.

\begin{table}
\begin{tabular}{c c c c c c c c c c c c} \hline \hline \multirow{2}{*}{Label Mapping} & \multirow{2}{*}{Method} & \multirow{2}{*}{Poisening-Rate} & MNIST & CIFAR10 & GTSRB & CIFAR100 & Pub/Fig & Avg. \\  & & & ACC/ACC* & ACC/ACC* & ACC/ACC* & ACC/ACC* & ACC/ACC* & ACC/ACC* & ACC/ACC* \\ \hline \multirow{4}{*}{All-to-One} & \multirow{4}{*}{TRODO-Zero} & 1\% & 80.1/78.2 & 81.3/77.0 & 83.5/81.8 & 74.6/72.7 & 72.1/69.8 & **78.3/75.9** \\  & & 3\% & 82.0/79.3 & 82.5/78.1 & 85.4/83.2 & 76.7/74.4 & 74.0/71.1 & **80.1/77.2** \\  & & 5\% & 83.6/80.4 & 84.0/79.5 & 86.3/84.6 & 77.5/75.3 & 75.1/72.6 & **81.3/78.5** \\  & & 10\% (default) & 80.9/79.3 & 82.7/78.5 & 84.8/83.3 & 75.5/73.7 & 73.2/70.6 & **79.4/77.0** \\ \hline \multirow{4}{*}{All-to-One} & \multirow{4}{*}{TRODO} & 1\% & 89.5/87.4 & 88.7/85.6 & 94.9/91.0 & 84.5/81.2 & 86.4/80.3 & **88.8/85.1** \\  & & 3\% & 91.08/92.9 & 90.5/87.8 & 96.5/93.1 & 86.3/82.7 & 87.8/82.4 & **90.4/87.0** \\ \cline{1-1}  & & 5\% & 92.8/90.6 & 91.7/88.9 & 97.0/94.1 & 87.5/83.6 & 89.1/84.5 & **91.6/88.3** \\ \cline{1-1}  & & 10\% (default) & 91.2/89.6 & 91.0/88.4 & 96.6/93.2 & 86.7/82.5 & 88.1/83.0 & **90.7/87.3** \\ \hline \hline \end{tabular}
\end{table}
Table 11: Detailed results of statistical significance of our methods performance over 10 runs, in terms of variance of accuracy.

\begin{table}
\begin{tabular}{c c c c c c c c c c c} \hline \hline \multirow{2}{*}{Label Mapping} & \multirow{2}{*}{Method} & \multirow{2}{*}{Poisening-Rate} & MNIST & CIFAR10 & GTSRB & CIFAR100 & Pub/Fig & Avg. \\  & & & ACC/ACC* & ACC/ACC* & ACC/ACC* & ACC/ACC* & ACC/ACC* & ACC/ACC* \\ \hline \multirow{4}{*}{All-to-One} & \multirow{4}{*}{TRODO-Zero} & 1\% & 80.1/78.2 & 81.3/77.0 & 83.5/81.8 & 74.6/72.7 & 72.1/69.8 & **78.3/75.9** \\  & & 3\% & 82.0/79.3 & 82.5/78.1 & 85.4/83.2 & 76.7/74.4 & 74.0/71.1 & **80.1/77.2** \\  & & 5\% & 83.6/80.4 & 84.0/79.5 & 86.3/84.6

\begin{table}
\begin{tabular}{c c c c c c c c c} \hline \hline LabelMapping & Method & OOD-Sample-Rate & MNIST & CIFAR10 & GTSRB & CIFAR100 & PathBig & Avg. \\  & & & ACC/ACC\({}^{*}\) & ACC/ACC\({}^{*}\) & ACC/ACC\({}^{*}\) & ACC/ACC\({}^{*}\) & ACC/ACC\({}^{*}\) & ACC/ACC\({}^{*}\) \\ \hline \multirow{4}{*}{All-to-One} & & 0.1\% & 78.67/72.2 & 80.57/63.6 & 82.68/1.1 & 73.27/1.4 & 71.26/8.5 & **77.37/5.0** \\  & & 0.2\% & 79.37/79.9 & 81.27/71.1 & 83.38/18.8 & 73.97/2.1 & 71.99/2.2 & **78.17/5.7** \\  & & 0.3\% & 80.07/8.6 & 81.97/78.6 & 84.08/2.5 & 74.76/2.8 & 72.69/9.9 & **78.87/6.4** \\  & & 0.5\% & 80.67/92.2 & 82.57/84.8 & 84.68/3.1 & 75.27/3.4 & 73.17/0.5 & **79.37/7.0** \\  & & 1\% (detailed) & 80.97/93.3 & 82.70/8.5 & 84.88/3.3 & 75.67/3.7 & 73.27/0.6 & **79.37/7.1** \\ \hline \hline \end{tabular}
\end{table}
Table 13: Performance comparison of TRODO-Zero under different OOD sample rates across datasets.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: We motivated our work by stating that current scanning methods are not attack-agnostic and fail to generalize to diverse attacks. Besides, almost all of them fail in scenarios where the given classifier is trained adversarially. Our method outperforms previous works in the mentioned settings, according to the results in Table 1. We also support our claims on the effect of using OOD data for trojan scanning via our theoretical analysis in Section 5. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We have discussed the limitations of our work and possible direction for future research in Section M. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.

3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: Due to the limited space in the main pages, we have provided proofs in Appendix Section F. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We have provided the code to reproduce the results as a part of the supplemental material. Besides, we provided all the details of implementation in Section 6. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility.

In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: We have provided the code and the notebook to easily reproduce results. We also have provided access to our benchmark in **New Assets** item in the checklist. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: The implementation details of the creation procedure of our benchmark, and how we adapted baselines to the experiments and how we tune our parameters are discussed in Section 6. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: Due to space constraints in the main body of the paper and to avoid overcrowding the tables, we have included the statistical significance of our method's experiments in Appendix Section O. For the baselines, we have documented the average performance across five repeated trials.

Guidelines:

* The answer NA means that the paper does not include experiments.
* The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.
* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: In the "Implementation Details" subsection of the Experiments section (see Section 6), we have thoroughly outlined the computational resources utilized. Given that the efficient execution of our method is a key advantage, we have included comprehensive details about the computing resources employed to assess our method in comparison with other approaches. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: After a thorough review of the NeurIPS Code of Ethics, we are confident that our work fully complies with all its aspects. Additionally, our research adheres to ethical standards concerning social impact and harmful consequences, based on both our proprietary code and the publicly available datasets employed. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.

* The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: In the appendix, we have allocated a section to discuss the broader impacts of our research (See Appendix Section J. Our study introduces a method designed to identify potential backdoors embedded within classifiers. As a result, our work contributes positively to societal impacts by enhancing security measures in machine learning applications and mitigating risks associated with malicious interventions. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. * The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. * The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. * If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: No safeguards are needed, as we are discussing the subject of detection of backdoor attacks, and not to incur them. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets**Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We have cited all image datasets that we used in our research. Besides, we have provided the license of TrojAI benchmark on which we have evaluated our method and previous works in their corresponding subsection in Appendix Section K. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. * If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: We created a new benchmark to evaluate our method alongside previous scanning techniques in terms of generalization capabilities. We have made our benchmark publicly available here. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: Our work didn't include crowdsourcing and research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.

* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: Our work does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.