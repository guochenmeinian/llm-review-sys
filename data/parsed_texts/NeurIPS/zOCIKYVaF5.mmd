# Residual Alignment:

Uncovering the Mechanisms of Residual Networks

Jianing Li

University of Toronto

jrobert.li@mail.utoronto.ca&Vardan Papyan

University of Toronto

vardan.papyan@utoronto.ca

###### Abstract

The ResNet architecture has been widely adopted in deep learning due to its significant boost to performance through the use of simple skip connections, yet the underlying mechanisms leading to its success remain largely unknown. In this paper, we conduct a thorough empirical study of the ResNet architecture in classification tasks by linearizing its constituent residual blocks using Residual Jacobians and measuring their singular value decompositions. Our measurements (code) reveal a process called Residual Alignment (RA) characterized by four properties:

1. [label=()]
2. intermediate representations of a given input are _equispaced_ on a _line_, embedded in high dimensional space, as observed by Gai and Zhang (2021);
3. top left and right singular vectors of Residual Jacobians align with each other and across different depths;
4. Residual Jacobians are at most rank \(C\) for fully-connected ResNets, where \(C\) is the number of classes; and
5. top singular values of Residual Jacobians scale inversely with depth.

RA consistently occurs in models that generalize well, in both fully-connected and convolutional architectures, across various depths and widths, for varying numbers of classes, on all tested benchmark datasets, but ceases to occur once the skip connections are removed. It also provably occurs in a novel mathematical model we propose. This phenomenon reveals a strong alignment between residual branches of a ResNet (RA2+4), imparting a highly rigid geometric structure to the intermediate representations as they progress _linearly_ through the network (RA1) up to the final layer, where they undergo Neural Collapse.

## 1 Introduction

### Background

The Residual Network (ResNet) architecture (He et al., 2016), a special case of Highway Networks (Srivastava et al., 2015), has taken the field of deep learning by storm, since its proposal in 2015. The incredibly simple architectural modification of adding a skip connection, spanning a set of layers, has become a go-to architectural choice for deep learning researchers and practitioners alike. Initially applied in computer vision, it has since been integrated as a crucial component in biomedical imaging and generative models via the U-Net (Ronneberger et al., 2015), natural language processing through the Transformer (Vaswani et al., 2017), and reinforcement learning as seen in the case of AlphaGo Zero (Silver et al., 2017).

The ResNet architecture first passes an input \(x\) through initial layers \(\mathcal{I}\) (comprising a sequence of convolution, batch normalization, and ReLU operations), depending on trainable parameters \(\mathcal{W}_{0}\), togenerate an initial representation

\[h_{1}=\mathcal{I}(x;\mathcal{W}_{0})\in\mathbb{R}^{D}.\]

This is then processed through a sequence of residual blocks

\[h_{i+1}=\sigma(h_{i}+\mathcal{F}(h_{i};\mathcal{W}_{i})),\quad 1\leq i\leq L,\]

each refining the previous layer's representation, \(h_{i}\in\mathbb{R}^{d}\), through a simple residual branch

\[\mathcal{F}(h_{i};\mathcal{W}_{i}):\mathbb{R}^{D}\rightarrow\mathbb{R}^{D},\]

which depends on trainable parameters \(\mathcal{W}_{i}\) and an element-wise non-linearity, \(\sigma\), which is simply an identity mapping, in the case of pre-activation ResNets (He et al., 2016). The final block's output, \(h_{L+1}\), is fed to a classifier,

\[\mathcal{C}(h_{L+1};\mathcal{W}_{L+1}):\mathbb{R}^{D}\rightarrow\mathbb{R}^{C},\]

depending on trainable parameters \(\mathcal{W}_{L+1}\).

In the original ResNet architecture, \(\mathcal{F}\) and \(\mathcal{C}\) are compositions of linear transformations, element-wise nonlinearities, and normalization layers. The WideResNet architecture, by Zagoruyko and Komodakis (2016), further incorporates dropout layers into \(\mathcal{F}\). Yet more changes are made in ResNet (Xie et al., 2017), where \(\mathcal{F}\) is computed from the summation of parallel computational branches.

Training a ResNet for classification involves minimizing a loss on a training dataset, \(\{x_{n},y_{n}\}_{n=1}^{N}\), consisting of inputs \(x_{n}\) and labels \(y_{n}\) plus a weight decay term

\[\operatorname*{minimize}_{\{\mathcal{W}_{i}\}_{i=1}^{L+1}}\,\frac{1}{N}\sum_ {n=1}^{N}\mathcal{L}(f(x_{n};\mathcal{W}),y_{n})+\frac{\lambda}{2}\|\mathcal{ W}\|_{2}^{2},\] (1)

where \(f(x_{n};\mathcal{W})\) are the outputs of the classifier \(\mathcal{C}\), also called logits, and \(\mathcal{W}\) are all the parameters.

### Problem Statement

The significant improvement in performance, achieved through the simple addition of a skip connection, has generated interest in understanding the underlying mechanisms of ResNets. Despite this, to this date, no definitive theory or explanation has been widely accepted by the deep learning community for the success of ResNets.

Figure 1: **Visualization of Residual Alignment.** Intermediate representations of a ResNet341, trained on CIFAR10, are projected onto two random vectors. Representations of each individual image are color-coded based on its true label and connected to form a trajectory, so as to showcase their progression throughout the network. Notice the _linear_ arrangement of intermediate representations along with _equidistant_ spacing between representations corresponding to consecutive layers (RA1). Our work shows, this phenomenon results from the _alignment_ of top singular vectors of Residual Jacobians (RA2) and the _inverse scaling_ of top singular values with depth (RA4). It is also noteworthy that the magnitudes of class means significantly increase with depth compared to the within-class variability, indicating the representations undergo layer-wise Neural Collapse (Papyan, 2020; Galanti et al., 2022; He and Su, 2022; Li et al., 2023).

### Method Overview

In this paper, we conduct a thorough empirical study of the ResNet architecture and its constituent residual blocks with the aim of investigating the characteristics of an individual residual block and the relationship between any pair of them. As the residual blocks are nonlinear functions of their inputs, we examine their linearizations through the _Residual Jacobian matrices_2:

Footnote 2: For a Type 1 model, \(D=512\) where \(512\) is the width of the network. For a Type 2 model, \(D=16\times 16\times 64=16384\) because the representations have \(64\) channels and a spatial dimension resolution of \(16\).

\[\sigma^{\prime}(h_{i}+\mathcal{F}(h_{i};\mathcal{W}_{i}))\frac{\partial \mathcal{F}(h_{i};\mathcal{W}_{i})}{\partial h_{i}}\in\mathbb{R}^{D\times D}.\]

Following Equation (1.1), these correspond to the derivative of the residual block with respect to its input, _excluding_ the contribution from the skip connection, \(\sigma^{\prime}(h_{i}+\mathcal{F}(h_{i};\mathcal{W}_{i}))\in\mathbb{R}^{D \times D}\). In the case of a pre-activation ResNet, these are simply equal to the derivative of the residual branch with respect to its input, i.e., \(\partial\mathcal{F}(h_{i};\mathcal{W}_{i})/\partial h_{i}\). Since the Residual Jacobians are high-dimensional matrices, and likely contain meaningful information only in some of their subspaces, we measure their singular value decomposition (SVD), given by \(J_{i}=U_{i}S_{i}V_{i}^{\top}\), where \(U_{i}\) and \(V_{i}\) are the respective left and right singular vectors, and \(S_{i}\) is the singular value matrix.

### Contributions

We discover a phenomenon called _Residual Alignment (RA)_, consistently occurring in ResNet models that generalize well, characterized by four properties3:

Footnote 3: These properties hold once the Jacobians are evaluated on the training data. If evaluated on other data, different phenomena might emerge. For example, we observed that the growth rate in (RA4) becomes exponential once the Jacobians are evaluated on the classification boundary, where the logits are close to zero.

* Intermediate representations of a given input are _equispaced_ on a _line_, embedded in high dimensional space, as shown in Figure 1 and observed by Gai and Zhang (2021);
* Top left and right singular vectors of Residual Jacobians align with each other and across different depths, as observed in Figure 2;
* Residual Jacobians are at most rank \(C\) for fully-connected ResNets, where \(C\) is the number of classes, as illustrated in Figures 3 and 4; and
* Top singular values of Residual Jacobians scale inversely proportional with depth, as depicted in Figure 4.

The properties are interrelated and, in fact, (RA1) can be logically derived from the other properties, as demonstrated in Section 3.

As a further contribution, in section B of the Appendix we prove theoretically the emergence of RA under the setting of binary classification with cross-entropy loss. Our proof relies on a novel mathematical abstraction called the _Unconstrained Jacobians Model_, in which the Residual Jacobians are optimized directly, so as to minimize the loss, and are not constrained by the architecture and parameters of the residual branches. This mathematical abstraction is motivated by recent theoretical works on Neural Collapse, as discussed in Section 5.

### Results Summary

Our empirical investigation, presented in the main text and the Appendix, consistently identifies RA across an extensive range of:

**Architectures:**: standard ResNets (convolutional layers with progressively increasing channels, interspersed with downsampling layers) as well as simpler designs (fully-connected layers);
**Datasets:**: MNIST, FashionMNIST, CIFAR10, CIFAR100, and ImageNette [Howard]; and
**Hyperparameters:**: network depth and width.

In addition to our main findings, we include an experiment showing the co-occurrence of (RA) and Neural Collapse in Figure 7. We also performed three counterfactual experiments that show:1. If the number of classes in the dataset is increased, the singular vector alignment occurs in a higher dimensional subspace (Figure 3);
2. If stochastic depth [22] is incorporated, the singular vector alignment is amplified (Figure 5); and
3. If the skip connections are removed, RA does not occur (Figure 6);

Figure 3: **(RA3) : Singular vector alignment occurs in subspace of rank \(\leq\) C.** The figure presents a sequence of subplots that illustrate the matrix \(U_{16,10}^{\toptop}J_{9}V_{16,10}\). Here, \(J_{9}\) represents the \(9\)-th Residual Jacobian, while \(U_{16,10}\) and \(V_{16,10}\) correspond to the leading \(10\) left and right singular vectors, respectively, of the \(16\)-th Residual Jacobian, \(J_{16}\). These calculations are based on ResNet34 models (Type 1 model in Section 2.1). These models have been trained on specific subsets of the CIFAR10 dataset, comprising of 4, 6, and 8 classes, as well as the complete CIFAR10 and CIFAR100 datasets. Each result is presented in the corresponding Subfigures 3a, 3b, 3c, 3d, and 3e. As the number of classes increases, the alignment of singular vectors occurs in an increasingly higher-dimensional subspace.

Figure 2: **(RA2) : Top singular vectors of Residual Jacobians align.** Subfigure 2a and Subfigure 2b present the alignment of the first 8 blocks and the last 7 blocks, respectively, for a ResNet34 trained on CIFAR100 (Type 3 model in Section 2.1) forwarding a single randomly sampled input. Each subplot \((i,j)\) illustrates the matrix \(U_{j,30}^{\toptop}J_{i}V_{j,30}\), where \(U_{j,30}\) and \(V_{j,30}\) denote the top-30 left and right singular vectors of the Residual Jacobian \(j_{j}\), respectively, and \(i,j\) are the indices of the residual blocks, i.e., their depth. A distinct diagonal line of intense pixels is apparent in almost every subplot, signifying that the top singular vectors of \(J_{j}\) diagonalize \(J_{i}\). In similar terms, this means that the top singular vectors of \(J_{i}\) and \(J_{j}\) align and (RA2) holds. This pattern persists when \(V_{j,30}^{\toptop}J_{i}U_{j,30}\) is plotted, instead of \(U_{j,30}^{\toptop}J_{i}V_{j,30}\), further confirming that the top left and right singular vectors align in accordance with (RA2). Additional visualizations of both matrices, across various models and datasets, are available in subsections C.2 and C.3 of the Appendix. It is crucial to highlight that no alignment exists between the Jacobians at initialization, and the alignment emerges during training.

\begin{table}
\begin{tabular}{l|l l l l l} \hline \hline Model & MNIST & FashionMNIST & CIFAR10 & CIFAR100 & ImageNette \\ \hline Type 1 & 98.9 & 90.9 & 58.3 & 30.4 & 42.6 \\ Type 2 & 99.5 & 92.0 & 88.5 & 54.6 & 67.9 \\ Type 3 & 99.6 & 92.8 & 87.3 & 62.6 & 64.2 \\ \hline \hline \end{tabular}
\end{table}
Table 1: (**RA3) occurs in models that generalize well.** The table displays the test accuracies of models trained to study (RA). Our reported accuracies closely align with those presented in Table 1 of Papyan et al. (2020), indicating that (RA) is observed in extensively trained models that exhibit strong performance in terms of test accuracy.

Figure 4: Depiction of Residual Jacobian singular values for ResNet34 trained on CIFAR10 (Type 1 model in Section 2.1). Subfigure 3(a) shows the top \(20\) singular values of Residual Jacobians, while Subfigure 3(b) illustrates the inverse scaling of the top \(1\) values. More singular value plots, from diverse models and datasets, are available in subsection C.4 of the Appendix.

Figure 5: **Stochastic depth amplifies singular vector alignment.** A comparison of (RA2) for two Type 3 models trained on CIFAR10 over \(50\) epochs: one employing the stochastic depth technique (with a drop probability of 0.3 for skipping residual blocks during training) and the other without it.

## 2 Methods

### Networks

We train 5 types of models:

**Type 1**: models consist of 16 basic residual blocks, each containing two fully-connected layers of dimension \(D{=}512\);

Figure 6: **Skip connections cause Residual Alignment. The experiment depicted in Figure 2 is replicated using two additional models. The first is a ResNet18 trained on CIFAR10 (Type 4a model in Section 2.1), with results showcased in Subfigure 5(a). The second is a ResNet18 _without skip connections_ (Type 4b model in Section 2.1), with results displayed in Subfigure 5(b). When the skip connections are removed, the top singular vectors no longer align. Alignment is visible in the diagonal subplots of Subfigure 5(b), as each Residual Jacobian is diagonalized by its own singular vectors.**

Figure 7: **Co-occurrence of Residual Alignment and Neural Collapse. The sub-figures display the progression of Neural Collapse metrics for a Type 4a model throughout 350 training epochs on the MNIST dataset as well as the emergence of Residual Alignment at the end of the training process.**

[MISSING_PAGE_FAIL:7]

### (Ra2+3+4) Imply (Ra1)

As mentioned in the introduction, the properties of (RA) are interconnected, and this relationship is demonstrated through the following theorem:

**Theorem 3.1**.: _For binary classification, in a pre-activation ResNet, assuming the Jacobian linearizations are exact and satisfy (RA2+3+4), then (RA1) holds for the intermediate representations._

The proof of this Theorem is deferred to section A of the Appendix.

### Unconstrained Jacobians Model Leads to RA

We propose the following abstraction of the optimization problem in Equation (1).

**Definition 3.2** (Unconstrained Jacobians Model).: Given a fixed input \(\Delta_{x}\in\mathbb{R}^{D}\) and its label \(y\in\{+1,-1\}\), find matrices \(J_{i}\in\mathbb{R}^{D\times D}\), \(1\leq i\leq L\), and vector \(w\in\mathbb{R}^{D}\) that

\[\operatorname*{minimize}_{w,\{J_{i}\}_{i=1}^{L}}\quad\mathcal{L}(w^{\top}\prod _{i=1}^{L}(I+J_{i})\Delta_{x},y)+\frac{\lambda}{2}\sum_{i=1}^{L}\|J_{i}\|_{F}^ {2}+\frac{\lambda}{2}\|w\|_{2}^{2}.\]

In the problem described above, \(J_{i}\) again represents the Residual Jacobian of the \(i\)-th residual branch and \(w\) represents the classifier Jacobian,

\[\frac{\partial\mathcal{C}(h_{L+1};\mathcal{W}_{L+1})}{\partial h_{L+1}}.\]

It is referred to as the "Unconstrained Jacobians Model" because the Jacobians are not restricted to any specific form and are not required to be realizable by a set of layers. In the Unconstrained Jacobians Model, the Jacobians are regularized through simple functions. However, in reality, weight decay, dropout, normalization layers, and parallel branches regularize the Jacobians in intricate ways that are hard to capture mathematically.

We prove in section B the Appendix the following theorem:

**Theorem 3.3**.: _For binary classification, there exists a global optimum of the Unconstrained Jacobians Model where the top Jacobian singular vectors are aligned (RA2), all Jacobians are rank one, analogous to (RA3), and the top Jacobian singular values are equal, analogous to (RA4)._

Here, the top singular values are equal and not decaying as predicted by (RA4), because the Jacobians are evaluated on the classification boundary instead of on training examples. Therefore, the intermediate representations are not equispaced on a line, as predicted by (RA1) but are rather exponentially-spaced on a line.

## 4 Discussion

In this section, we pose research questions that emerge from the discovery of RA.

GeneralizationThe discovery of RA offers a vivid analogy for comprehending generalization in deep learning. According to RA, we can envision a ResNet as a system of conveyor belts, where each conveyor carries representations of a specific class in a unified direction at a constant speed. Misclassification occurs when the representation mistakenly steps onto the wrong conveyor belt in the initial layers of the network. This perspective leads us to hypothesize that the first few layers of the network play a vital role in generalization, surpassing the significance of subsequent layers. Consequently, more research should be dedicated towards studying the dynamics of the initial layers.

Neural CollapseAs mentioned in Figure 1, there is an intriguing pattern where models exhibit RA concurrently with layer-wise Neural Collapse (Papyan, 2020; Galanti et al., 2022; Li et al., 2023). The concurrent manifestation of these two distinct events could possibly be more than mere coincidence. It would be interesting to explore if RA could shed light on phenomena related to layer-wise Neural Collapse such as the "Law of Data Separation" (He and Su, 2022).

RA in TransformersWe have empirically substantiated the occurrence of RA in ResNets. However, our study has not yet extended to Transformers, which also incorporate residual connections. An intriguing line of inquiry for future work would be to probe whether RA, or a phenomenon akin to it, manifests within these architectures.

Recurrent Architectures Exhibiting RARecurrent neural networks, Neural ODEs (Chen et al., 2018), and deep equilibrium models (Bai et al., 2019) iteratively apply a layer within a deep neural network. This leads to the following questions:

_Do the intermediate representations of these models exhibit RA? If not, can we propose a novel architecture that recurrently applies a computation but does exhibit RA?_

Model CompressionIn our work, we demonstrate that the network can converge to a model that iteratively applies a computation, even without imposing explicit constraints, like the architectures in the previous subsection. This leads us to yet another question:

_Can we replicate the original network's performance by distilling the aligning layers into a single layer and iteratively applying it?_

Regularization TechniquesExisting regularization techniques, including layer permutation (Liaw et al., 2021) and structured dropout (Fan et al., 2019), should strengthen RA. A question arises:

_Could RA explain the success of these methods and do they indeed amplify RA?_

## 5 Related Work

Linearization of Residual JacobiansPrior to our work, Rothauge et al. (2019) proposed the linearization of residual Jacobians and investigated the distribution of their singular values. Our work, however, concerns the discovery of the alignment between the Residual Jacobians, as well as its theoretical understanding.

GeneralizationA thorough empirical investigation by Novak et al. (2018) found that the Frobenius norm of the input-output Jacobian of a network correlates well with generalization. Our research complements theirs by conducting both empirical and theoretical investigations on the _Residual Jacobians_, which form the input-output Jacobian of a ResNet.

ResNets at InitializationPrevious works have contributed significantly to the understanding of the properties of intermediate representations of randomly initialized ResNets, with studies such as Hayou (2022) exploring the infinite-width and finite-width regime and Li et al. (2021, 2022) investigating the infinite-depth and infinite-width regime. Our current research diverges from these previous works by specifically focusing on studying the Residual Jacobians of fully trained ResNets.

Optimization LandscapeLi et al. (2016) analyzed the Hessian of the loss function for a ResNet initialized with zero parameters. Additionally, Lu et al. (2020) use a mean-field analysis of ResNets to demonstrate convergence to a global minimum. Their analysis builds upon the observation that a ResNet is similar to a shallow network ensemble, first noted by Veit et al. (2016).

Rather than focusing on the optimization convergence properties of SGD, our goal is to examine the properties of the intermediate representations and Residual Jacobians that emerge during training.

Neural CollapseRecent theoretical work by Mixon et al. (2020), Lu and Steinerberger (2020), E and Wojtowytsch (2020), Poggio and Liao (2020), Zhu et al. (2021), Han et al. (2021), Tirer and Bruna (2022), Wang et al., Kothapalli et al. (2022) analyzed the Neural Collapse phenomenon, discovered by Papyan, Han, and Donoho (2020), through the unconstrained features model and the layer-pealed model. In these, the assumption is made that the last-layer representations, which are fed to a classifier, have the freedom to move independently and are not constrained to be the output of a deep network. Our Unconstrained Jacobians Model takes inspiration from these mathematical models by abstracting away the complex Residual Jacobians and assuming they can be optimized directly.

Unrolled Iterative Algorithm PerspectiveGreff et al. (2016); Papyan et al. (2017), and Ebski et al. (2018) recognized ResNets as unrolled iterative algorithms performing iterative inference. We build upon this understanding by delving deeper into the empirical and theoretical relationships between the Residual Jacobians.

Neural ODEChen et al. (2018) introduced the Neural ODE: a continuous-depth, tied-weights ResNet. Initially, it may seem unlikely that the iterations or Jacobians of a traditional ResNet would possess any characteristics associated with Neural ODEs. However, Sander et al. (2022) proved that assuming the initial loss is small and the initial parameters are smooth with depth, _linear_ ResNets converge to a Neural ODE as the number of layers increases. Still, it is unclear if these findings extend to _nonlinear_ ResNets trained on _real_ data.

Our work, however, demonstrates that even in such cases the Residual Jacobians align in their top subspaces and, as a result, simple ODEs emerge within these subspaces. We also provide theoretical justification for this claim through the Unconstrained Jacobians Model.

Optimal TransportGai and Zhang (2021) view ResNets as aiming to transport an input distribution to a label distribution, through a geodesic curve in the Wasserstein space, induced by the optimal transport map. They provide empirical evidence to support their claim, showing that intermediate representations are equidistant on a straight line induced by the optimal transport map, and comment: _"Though ResNet approximates the geodesic curve better than plain network, it may not be a perfect implement in high-dimensional space due to its layer-wise heterogeneity."_

Our research demonstrates that intermediate representations lie on a line as a result of the Jacobian singular vectors aligning (RA2) and the Jacobian singular values scaling inversely with depth (RA4) and, in fact, due to the _absence_ of purported "layer-wise heterogeneity."

Analysis of Deep Linear NetworksMulayoff and Michaeli (2020) proved that training a deep _linear network,_ with a _quadratic loss_, and _no weight decay_, necessarily converges to the flattest of all minima. At this optimum point, the spectral norm of the input-feature Jacobian increases exponentially with depth, and the singular vectors of consecutive weight matrices align.

Our theoretical study considers _ResNets_, trained with a _binary cross-entropy loss_, and _weight decay_. Instead of assuming that training has reached a flat minimum, we analyze the Unconstrained Jacobians Model, on the classification boundary in the input space, and prove phenomena that we have observed through experiments on _nonlinear_ ResNets.

Analysis of WeightsCohen et al. (2021) studied the scaling behavior of trained _weights_ in deep residual networks. Our study complements theirs by focusing on examining the _Residual Jacobians_ of ResNet architectures.

## 6 Conclusion

In this paper, we offer a detailed empirical examination of the ResNet architecture, a model that has seen extensive application across a broad spectrum of deep learning domains. Our primary aim was to demystify the mechanisms that contribute to ResNet's remarkable success, a topic that, to date, remains an enigma within the deep learning community.

Our investigation has led to the discovery of a consistent phenomenon, which we have termed RA. The characteristics of RA were observed across a wide array of benchmark datasets, canonical architectures, and hyperparameters, demonstrating its general applicability. Moreover, we conducted counterfactual studies that underscored the critical role of skip connections in the emergence of RA and the effect of the number of classes. In an attempt to theoretically ground the emergence of RA, we proposed the use of an innovative mathematical abstraction - the Unconstrained Jacobians Model, specifically in the context of binary classification with cross-entropy loss.

Our exploration has not only shed light on the intricate mechanisms driving ResNets' performance but also points to connections with the recent phenomenon of layer-wise Neural Collapse. Furthermore, our findings pave the way for future research in the understanding of existing regularization methods, the design of novel architectures, the development of model compression techniques, and the theoretical investigation of generalization.

## Acknowledgments and Disclosure of Funding

We thank Kirill Serkh for inspiring discussions and feedback on the manuscript. We acknowledge the support of the Natural Sciences and Engineering Research Council of Canada (NSERC), [funding reference number 512236 and 512265]. This research was enabled in part by support provided by Compute Ontario (http://www.computeatorio.ca) and Compute Canada (http://www.computecanada.ca).

## References

* Bai et al. (2019) Shaojie Bai, J Zico Kolter, and Vladlen Koltun. Deep equilibrium models. _Advances in Neural Information Processing Systems_, 32, 2019.
* Chen et al. (2018) Ricky TQ Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary differential equations. _Advances in neural information processing systems_, 31, 2018.
* Cohen et al. (2021) Alain-Sam Cohen, Rama Cont, Alain Rossier, and Renyuan Xu. Scaling properties of deep residual networks. In _International Conference on Machine Learning_, pages 2039-2048. PMLR, 2021.
* E and Wojtowytsch (2020) Weinan E and Stephan Wojtowytsch. On the emergence of tetrahedral symmetry in the final and penultimate layers of neural network classifiers. _arXiv preprint arXiv:2012.05420_, 2020.
* Ebski et al. (2018) Stanislaw Jastrz Ebski, Devansh Arpit, Nicolas Ballas, Vikas Verma, Tong Che, and Yoshua Bengio. Residual connections encourage iterative inference. In _International Conference on Learning Representations_, 2018.
* Fan et al. (2019) Angela Fan, Edouard Grave, and Armand Joulin. Reducing transformer depth on demand with structured dropout. _arXiv preprint arXiv:1909.11556_, 2019.
* Gai and Zhang (2021) Kuo Gai and Shihua Zhang. A mathematical principle of deep learning: Learn the geodesic curve in the wasserstein space. _arXiv preprint arXiv:2102.09235_, 2021.
* Galanti et al. (2022) Tomer Galanti, Liane Galanti, and Ido Ben-Shaul. On the implicit bias towards minimal depth of deep neural networks, 2022.
* Greff et al. (2016) Klaus Greff, Rupesh K Srivastava, and Jurgen Schmidhuber. Highway and residual networks learn unrolled iterative estimation. _arXiv preprint arXiv:1612.07771_, 2016.
* Han et al. (2021) XY Han, Vardan Papyan, and David L Donoho. Neural collapse under mse loss: Proximity to and dynamics on the central path. In _International Conference on Learning Representations_, 2021.
* Hayou (2022) Soufiane Hayou. On the infinite-depth limit of finite-width neural networks. _arXiv preprint arXiv:2210.00688_, 2022.
* He and Su (2022) Hangfeng He and Weijie J. Su. A law of data separation in deep learning, 2022.
* He et al. (2016) Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 770-778, 2016a.
* He et al. (2016) Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual networks. In _European conference on computer vision_, pages 630-645. Springer, 2016b.
* fastai/imagenette: A smaller subset of 10 easily classified classes from Imagenet, and a little more French -
- github.com. https://github.com/fastai/imagenette. [Accessed 14-May-2023].
* Huang et al. (2016) Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Weinberger. Deep networks with stochastic depth, 2016.
* Kothapalli et al. (2022) Vignesh Kothapalli, Ebrahim Rasromani, and Vasudev Awatramani. Neural collapse: A review on modelling principles and generalization. _arXiv preprint arXiv:2206.04041_, 2022.
* Kothapalli et al. (2021)Huamin Li, George C. Linderman, Arthur Szlam, Kelly P. Stanton, Yuval Kluger, and Mark Tygert. Algorithm 971: An implementation of a randomized algorithm for principal component analysis. _ACM Trans. Math. Softw._, 43(3), jan 2017. ISSN 0098-3500. doi: 10.1145/3004053. URL https://doi.org/10.1145/3004053.
* Li et al. (2021) Mufan Li, Mihai Nica, and Dan Roy. The future is log-gaussian: Resnets and their infinite-depth-and-width limit at initialization. _Advances in Neural Information Processing Systems_, 34:7852-7864, 2021.
* Li et al. (2022) Mufan Bill Li, Mihai Nica, and Daniel M Roy. The neural covariance sde: Shaped infinite depth-and-width networks at initialization. _arXiv preprint arXiv:2206.02768_, 2022.
* Li et al. (2016) Sihan Li, Jiantao Jiao, Yanjun Han, and Tsachy Weissman. Demystifying resnet. _arXiv preprint arXiv:1611.01186_, 2016.
* Li et al. (2023) Xiao Li, Sheng Liu, Jinxin Zhou, Xinyu Lu, Carlos Fernandez-Granda, Zhihui Zhu, and Qing Qu. Principled and efficient transfer learning of deep models via neural collapse, 2023.
* Liaw et al. (2021) Andrew Liaw, Jia-Hao Hsu, and Chung-Hsien Wu. Ensemble of one model: Creating model variations for transformer with layer permutation. In _2021 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC)_, pages 1026-1030. IEEE, 2021.
* Loshchilov and Hutter (2016) Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. _arXiv preprint arXiv:1608.03983_, 2016.
* Lu and Steinerberger (2020) Jianfeng Lu and Stefan Steinerberger. Neural collapse with cross-entropy loss. _arXiv preprint arXiv:2012.08465_, 2020.
* Lu et al. (2020) Yiping Lu, Chao Ma, Yulong Lu, Jianfeng Lu, and Lexing Ying. A mean field analysis of deep resnet and beyond: Towards provably optimization via overparameterization from depth. In _International Conference on Machine Learning_, pages 6426-6436. PMLR, 2020.
* Miranda and Thompson (1993) Hector Miranda and Robert C Thompson. A trace inequality with a subtracted term. _Linear algebra and its applications_, 185:165-172, 1993.
* Mirsky (1975) Leon Mirsky. A trace inequality of john von neumann. _Monatshefte fur mathematik_, 79(4):303-306, 1975.
* Mixon et al. (2020) Dustin G Mixon, Hans Parshall, and Jianzong Pi. Neural collapse with unconstrained features. _arXiv preprint arXiv:2011.11619_, 2020.
* Mulayoff and Michaeli (2020) Rotem Mulayoff and Tomer Michaeli. Unique properties of flat minima in deep networks. In _International Conference on Machine Learning_, pages 7108-7118. PMLR, 2020.
* Novak et al. (2018) Roman Novak, Yasaman Bahri, Daniel A Abolafia, Jeffrey Pennington, and Jascha Sohl-Dickstein. Sensitivity and generalization in neural networks: an empirical study. _arXiv preprint arXiv:1802.08760_, 2018.
* Papyan (2020) Vardan Papyan. Traces of class/cross-class structure pervade deep learning spectra, 2020.
* Papyan et al. (2017) Vardan Papyan, Yaniv Romano, and Michael Elad. Convolutional neural networks analyzed via convolutional sparse coding. _The Journal of Machine Learning Research_, 18(1):2887-2938, 2017.
* Papyan et al. (2020) Vardan Papyan, X. Y. Han, and David L. Donoho. Prevalence of Neural Collapse during the terminal phase of deep learning training. _Proceedings of the National Academy of Sciences_, 117(40):24652-24663, 2020.
* Poggio and Liao (2020) Tomaso Poggio and Qianli Liao. Explicit regularization and implicit bias in deep network classifiers trained with the square loss. _arXiv preprint arXiv:2101.00072_, 2020.
* Ronneberger et al. (2015) Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In _International Conference on Medical image computing and computer-assisted intervention_, pages 234-241. Springer, 2015.
* Poggio et al. (2018)Kai Rothauge, Zhewei Yao, Zixi Hu, and Michael W. Mahoney. Residual networks as nonlinear systems: Stability analysis using linearization. 2019.
* Sander et al. (2022) Michael E Sander, Pierre Ablin, and Gabriel Peyre. Do residual neural networks discretize neural ordinary differential equations? _arXiv preprint arXiv:2205.14612_, 2022.
* Silver et al. (2017) David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go without human knowledge. _nature_, 550(7676):354-359, 2017.
* Srivastava et al. (2015) Rupesh Kumar Srivastava, Klaus Greff, and Jurgen Schmidhuber. Highway networks. _CoRR_, abs/1505.00387, 2015. URL http://arxiv.org/abs/1505.00387.
* Tirer and Bruna (2022) Tom Tirer and Joan Bruna. Extended unconstrained features model for exploring deep neural collapse. In _international conference on machine learning (ICML)_, 2022.
* Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. _Advances in neural information processing systems_, 30, 2017.
* Veit et al. (2016) Andreas Veit, Michael J Wilber, and Serge Belongie. Residual networks behave like ensembles of relatively shallow networks. _Advances in neural information processing systems_, 29, 2016.
* Wang et al. (2017) Peng Wang, Huikang Liu, Can Yaras, Laura Balzano, and Qing Qu. Linear convergence analysis of neural collapse with unconstrained features. In _OPT 2022: Optimization for Machine Learning (NeurIPS 2022 Workshop)_.
* Xie et al. (2017) Saining Xie, Ross Girshick, Piotr Dollar, Zhuowen Tu, and Kaiming He. Aggregated residual transformations for deep neural networks. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 1492-1500, 2017.
* Zagoruyko and Komodakis (2016) Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. In _British Machine Vision Conference 2016_. British Machine Vision Association, 2016.
* Zhu et al. (2021) Zhihui Zhu, Tianyu Ding, Jinxin Zhou, Xiao Li, Chong You, Jeremias Sulam, and Qing Qu. A geometric analysis of neural collapse with unconstrained features. _Advances in Neural Information Processing Systems_, 34:29820-29834, 2021.

(RA2+3+4) Imply (Ra1)

**Theorem 3.1**.: _For binary classification, in a pre-activation ResNet, assuming the Jacobian linearizations are exact and satisfy (RA2+3+4), then (RA1) holds for the intermediate representations._

Proof.: In a pre-activation ResNet,

\[h_{i+1}=h_{i}+\mathcal{F}(h_{i};\mathcal{W}_{i}).\]

Since Jacobian linearizations are exact, we have:

\[h_{i+1}=(I+J_{i})h_{i}.\]

Recall, the singular value decomposition of \(J_{i}\) is given by

\[J_{i}=U_{i}S_{i}V_{i}^{\top},\]

where \(U_{i}\) and \(V_{i}\) are the respective left and right singular vector matrices, and \(S_{i}\) is the singular value matrix. Invoking (RA2) and (RA3), for some matrix \(U_{0}\) and for any block \(i\),

\[J_{i}=U_{0}S_{i}U_{0}^{\top},\]

and therefore

\[h_{i+1}=(I+U_{0}S_{i}U_{0}^{\top})h_{i}.\]

Applying recursively the above equality leads to

\[h_{k}=\left(\prod_{i=1}^{k-1}(I+U_{0}S_{i}U_{0}^{\top})\right)h_{1}=U_{0} \left(\prod_{i=1}^{k-1}(I+S_{i})\right)U_{0}^{\top}h_{1}.\] (2)

For binary classification, (RA3) implies the Jacobians are rank 1 and therefore

\[h_{k}=U_{0,1}\left(\prod_{i=1}^{k-1}(1+S_{i,1})\right)U_{0,1}^{\top}h_{1}+(I- U_{0,1}U_{0,1}^{\top})h_{1},\]

where \(U_{0,j}\) is the \(j\)th column of \(U_{0}\). According to (RA4),

\[S_{i,1}=\frac{1}{i}\]

and

\[\prod_{i=1}^{k-1}(1+S_{i,1})=\prod_{i=1}^{k-1}(1+1/i)=k.\]

Substituting the above into Equation (2), we obtain

\[h_{k}=kU_{0,1}U_{0,1}^{\top}h_{1}+(I-U_{0,1}U_{0,1}^{\top})h_{1}=(k-1)U_{0,1}U _{0,1}^{\top}h_{1}+h_{1}.\]

This proves that the intermediate representations of a given input are _equispaced_ on a _line_ embedded in high dimensional space, i.e., (RA1).

## Appendix B Unconstrained Jacobians Model Leads to RA

We start by providing motivation for the unconstrained Jacobians problem introduced in the main text. Assume a training example, \(x\), is situated next to a point on the classification boundary, denoted by \(x_{\text{mid}}\), satisfying \(f(x_{\text{mid}};\mathcal{W})=0\). Performing a Taylor expansion of the logits of \(x\) around \(x_{\text{mid}}\) yields

\[f(x;\mathcal{W}) =f(x_{\text{mid}};\mathcal{W})+\frac{\partial f(x;\mathcal{W})}{ \partial x}\bigg{|}_{x_{\text{mid}}}\Delta_{x}+h(x,x_{\text{mid}})\] \[=\frac{\partial f(x;\mathcal{W})}{\partial x}\bigg{|}_{x_{\text{ mid}}}\Delta_{x}+h(x,x_{\text{mid}}),\] (3)where \(\Delta_{x}=x-x_{\text{mid}}\) and \(h(x,x_{\text{mid}})\) accounts for the approximation error, which is \(\mathcal{O}(\|\Delta_{x}\|_{2}^{2})\) and assumed to be negligible in our analysis.

Recall the loss associated with training a ResNet:

\[\operatorname*{minimize}_{\{\mathcal{W}_{i}\}_{i=1}^{L+1}}\,\frac{1}{N}\sum_{n =1}^{N}\mathcal{L}(f(x_{n};\mathcal{W}),y_{n})+\frac{\lambda}{2}\|\mathcal{W} \|_{2}^{2}.\] (4)

Substituting Equation (3) into the above, neglecting the approximation error, and considering only the objective associated with the training sample \(x\) and its label \(y\), we get

\[\mathcal{L}\left(\frac{\partial f(x;\mathcal{W})}{\partial x}\bigg{|}_{x_{ \text{mid}}}\Delta_{x},y\right)+\frac{\lambda}{2}\|\mathcal{W}\|_{2}^{2}.\]

By using the chain rule, we can then obtain the following:6

Footnote 6: For simplicity, we ignore the input transformation that maps the input \(\Delta_{x}\) to the first representation \(h_{1}\in\mathbb{R}^{D}\) by simply assuming \(\Delta_{x}\in\mathbb{R}^{D}\).

\[\mathcal{L}\left(\frac{\partial\mathcal{C}(h_{L+1};\mathcal{W}_{L+1})}{ \partial h_{L+1}}\prod_{i=1}^{L}\left(I+\frac{\partial\mathcal{F}(h_{i}; \mathcal{W}_{i})}{\partial h_{i}}\right)\Delta_{x},y\right)+\frac{\lambda}{2} \sum_{i=1}^{L}\|\mathcal{W}_{i}\|_{F}^{2}+\frac{\lambda}{2}\|\mathcal{W}_{L+1} \|_{F}^{2},\] (5)

where all the Jacobians are evaluated at the point \(x_{\text{mid}}\). This naturally gives rise to the following definition, as introduced in the main text.

**Definition 3.2** (Unconstrained Jacobians Model).: _Given a fixed input \(\Delta_{x}\in\mathbb{R}^{D}\) and its label \(y\in\{+1,-1\}\), find matrices \(J_{i}\in\mathbb{R}^{D\times D}\), \(1\leq i\leq L\), and vector \(w\in\mathbb{R}^{D}\) that_

\[\operatorname*{minimize}_{w,\{J_{i}\}_{i=1}^{L}}\quad\mathcal{L}(w^{\top} \prod_{i=1}^{L}(I+J_{i})\Delta_{x},y)+\frac{\lambda}{2}\sum_{i=1}^{L}\|J_{i} \|_{F}^{2}+\frac{\lambda}{2}\|w\|_{2}^{2}.\]

In the main text, we stated the following theorem.

**Theorem 3.3**.: _For binary classification, There exists a global optimum of the Unconstrained Jacobians Model where the top Jacobian singular vectors are aligned (RA2), all Jacobians are rank one, analogous to (RA3), and the top Jacobian singular values are equal, analogous to (RA4)._

Proof.: Throughout the proof, we assume, without loss of generality, that the label is \(y=1\). Using the cyclic property of the trace, the logit, \(w^{\top}\prod_{i=1}^{L}(I+J_{i})\Delta_{x}\), equals

\[\operatorname{tr}\Biggl{\{}\Delta_{x}w^{\top}\prod_{i=1}^{L}(I+J_{i})\Biggr{\}}.\]

Denoting the power set of all natural numbers between \(1\) and \(L\) by \(\mathcal{P}(L)\), the above can be expressed as follows:

\[\sum_{s\in\mathcal{P}(L)}\operatorname{tr}\Biggl{\{}\Delta_{x}w^{\top}\prod_{ i\in s}J_{i}\Biggr{\}}.\]

Each element in the above summation can be upper bounded through the following theorem (a generalization of Von Neumann's trace inequality [10] to the product of more than two real matrices).

**Theorem 2** ([10]).: _Let \(A_{1},\dots,A_{m}\) be matrices with real entries.7 Take the singular values of \(A_{j}\) to be \(s_{1}(A_{j})\geq\dots\geq s_{n}(A_{j})>0\), for \(j=1,\dots,m\), and denote \(S_{j}=\operatorname{diag}(s_{1}(A_{j}),\dots,s_{n}(A_{j}))\). Then, as the matrices \(P_{1},\dots,P_{m}\) range over all possible rotations, i.e., the special orthogonal group \(\operatorname{SO}(n)\),_

Footnote 7: Assume the convention that the singular values are positive.

\[\sup_{P_{1},\dots,P_{m}\in\operatorname{SO}(n)}\operatorname{tr} (A_{1}P_{1}\dots A_{m}P_{m})\] \[= \sum_{i=1}^{n-1}\prod_{j=1}^{m}s_{i}(A_{j})+[\operatorname{sign }\det(A_{1}\dots A_{m})]\prod_{j=1}^{m}s_{n}(A_{j}).\]_Moreover, assuming \(\operatorname{sign}\det(A_{1}\ldots A_{m})=1\),_

\[\sup_{P_{1},\ldots,P_{m}\in\operatorname{SO}(n)}\operatorname{tr}(A_{1}P_{1} \ldots A_{m}P_{m})=\operatorname{tr}\Biggl{\{}\prod_{i=1}^{m}S_{i}\Biggr{\}}.\]

We will continue our proof using contradiction. Suppose all existing global optima of the unconstrained Jacobians problem consist of Jacobians that do not have aligning singular vectors, or do not have equal singular values, or are not rank \(1\). Then take any solution \(\left\{J_{i}\right\}_{i=1}^{L}\) and \(w\). Using the singular value decomposition, we have

\[J_{i}=U_{i}S_{i}V_{i}^{\top},\quad\text{for}\;\;i=1,\ldots,L,\]

and

\[\Delta_{x}w^{\top}=U_{L+1}S_{L+1}V_{L+1}^{\top}.\]

Then Theorem 2 implies

\[\sum_{s\in\mathcal{P}(L)}\operatorname{tr}\Biggl{\{}\Delta_{x}w^ {\top}\prod_{i\in s}J_{i}\Biggr{\}} \leq\!\!\sum_{s\in\mathcal{P}(L)}\operatorname{tr}\Biggl{\{}S_{L+ 1}\prod_{i\in s}S_{i}\Biggr{\}}\] \[=\!\operatorname{tr}\Biggl{\{}S_{L+1}\prod_{i=1}^{L}(I+S_{i}) \Biggr{\}}.\]

For all \(s\in\mathcal{P}(L)\), the inequality becomes equality once the singular vectors of all the Jacobians align with those of \(\Delta_{x}w^{\top}\) and once the vector \(w\) is chosen to be proportional to \(\Delta_{x}\) (so that the matrix \(\Delta_{x}w^{\top}\) is symmetric). The implication of the steps thus far is that one can increase, or at least keep constant the logit, and consequently reduce, or at least keep constant the loss by simply aligning the singular vectors of the Jacobians. In addition, since the regularization term \(\left\|J_{i}\right\|_{F}^{2}=\operatorname{tr}\bigl{\{}S_{i}^{2}\bigr{\}}\), this change of Jacobians does not affect the regularization terms.

Notice that \(\Delta_{x}w^{\top}\) is a rank one matrix and so \(S_{L+1}\) has a single non-zero diagonal entry. Furthermore, the matrices \(S_{i}\), for \(1\leq i\leq L\), are all diagonal. As such, we can zero out all their other diagonal entries and leave a single non-zero entry at the location that \(S_{L+1}\) has one, which does not affect the logits but reduces the regularization terms.

Using the inequality of arithmetic and geometric means on this only non-zero entry \(s_{i}\) of every diagonal matrix \(S_{i}\) gives

\[s_{L+1}\prod_{i=1}^{L}(1+s_{i})\leq s_{L+1}\left(1+\frac{1}{L}\sum_{i=1}^{L}s_ {i}\right)^{L}.\]

The implication of the above inequality is that, once the singular vectors of the Jacobians are aligned, one can further increase the logits and reduce the loss by averaging all the top singular values, \(s_{i}\) for \(1\leq i\leq L\), and forcing them to be equal. Furthermore, since \(\left\|J_{i}\right\|_{F}^{2}=\operatorname{tr}\bigl{\{}S_{i}^{2}\bigr{\}}=s_ {i}^{2}\) is convex, by Jensen's inequality, averaging the singular values only decreases the value of the Jacobian regularization.

All in all, we obtain higher, or at least no lower logit, and lower, or at least no higher loss when all singular vectors are aligned, all top singular values are equal and all other singular values are zero, which contradicts the statement that no global optima of the unconstrained Jacobians problem satisfies all of these conditions.

[MISSING_PAGE_EMPTY:17]

Figure 11: Convolutional ResNet34 (Type 2 model) trained on MNIST.

Figure 10: Fully-connected ResNet34 (Type 1 model) trained on CIFAR10.

Figure 12: Convolutional ResNet34 (Type 2 model) trained on FashionMNIST.

Figure 13: Convolutional ResNet34 (Type 2 model) trained on CIFAR10.

Figure 14: Convolutional ResNet34 with downsampling (Type 3 model) trained on MNIST.

Figure 15: Convolutional ResNet34 with downsampling (Type 3 model) trained on FashionMNIST.

Figure 16: Convolutional ResNet34 with downsampling (Type 3 model) trained on CIFAR10.

### (R2) : \(U_{j,K}^{+}\),\(J_{i}\),\(V_{j,K}\)

Figure 17: Fully-connected ResNet34 (Type 1 model) trained on MNIST.

Figure 18: Fully-connected ResNet34 (Type 1 model) trained on FashionMNIST.

Figure 19: Fully-connected ResNet34 (Type 1 model) trained on CIFAR10.

Figure 20: Fully-connected ResNet34 (Type 1 model) trained on CIFAR100.

Figure 21: Convolutional ResNet34 (Type 2 model) trained on MNIST.

Figure 22: Convolutional ResNet34 (Type 2 model) trained on FashionMNIST.

Figure 23: Convolutional ResNet34 (Type 2 model) trained on CIFAR10.

Figure 24: Convolutional ResNet34 (Type 2 model) trained on CIFAR100.

Figure 25: Convolutional ResNet34 (Type 2 model) trained on ImageNette.

Figure 26: Convolutional ResNet34 with downsampling (Type 3 model) trained on MNIST.

Figure 27: Convolutional ResNet34 with downsampling (Type 3 model) trained on FashionMNIST.

Figure 28: Convolutional ResNet34 with downsampling (Type 3 model) trained on CIFAR10.

Figure 29: Convolutional ResNet34 with downsampling (Type 3 model) trained on CIFAR100.

Figure 30: Convolutional ResNet34 with downsampling (Type 3 model) trained on ImageNet.

### (R2) : \(V_{j,K}^{\top}J_{i}U_{j,K}\)

Figure 31: Fully-connected ResNet34 (Type 1 model) trained on MNIST.

Figure 32: Fully-connected ResNet34 (Type 1 model) trained on FashionMNIST.

Figure 33: Fully-connected ResNet34 (Type 1 model) trained on CIFAR10.

Figure 34: Fully-connected ResNet34 (Type 1 model) trained on CIFAR100.

Figure 35: Convolutional ResNet34 (Type 2 model) trained on MNIST.

Figure 36: Convolutional ResNet34 (Type 2 model) trained on FashionMNIST.

Figure 37: Convolutional ResNet34 (Type 2 model) trained on CIFAR10.

Figure 38: Convolutional ResNet34 (Type 2 model) trained on CIFAR100.

Figure 39: Convolutional ResNet34 (Type 2 model) trained on ImageNet.

Figure 40: Convolutional ResNet34 with downsampling (Type 3 model) trained on MNIST.

Figure 41: Convolutional ResNet34 with downsampling (Type 3 model) trained on FashionMNIST.

Figure 42: Convolutional ResNet34 with downsampling (Type 3 model) trained on CIFAR10.

Figure 43: Convolutional ResNet34 with downsampling (Type 3 model) trained on CIFAR100.

Figure 44: Convolutional ResNet34 with downsampling (Type 3 model) trained on ImageNet.

[MISSING_PAGE_EMPTY:50]

Figure 49: Convolutional ResNet34 (Type 2 model) trained on MNIST.

Figure 48: Fully-connected ResNet34 (Type 1 model) trained on CIFAR100.

Figure 49: Convolutional ResNet34 (Type 2 model) trained on MNIST.

Figure 51: Convolutional ResNet34 (Type 2 model) trained on CIFAR10.

Figure 52: Convolutional ResNet34 (Type 2 model) trained on CIFAR100.

Figure 53: Convolutional ResNet34 (Type 2 model) trained on ImageNette.

Broader Impacts

This work presents foundational research and we do not foresee any potential negative societal impacts.