# MathCAMPS: Fine-grained Synthesis of Mathematical Problems From Human Curricula

 Shubhra Mishra\({}^{*,1}\)

###### Abstract

Mathematical problem solving is an important skill for Large Language Models (LLMs), both as an important capability and a proxy for a range of reasoning abilities. Existing benchmarks probe a diverse set of skills, but they yield aggregate accuracy metrics, obscuring specific abilities or weaknesses. Furthermore, they are difficult to extend with new problems, risking data contamination over time. To address these challenges, we propose MathCAMPS: a method to synthesize high-quality mathematical problems at scale, grounded on 44 fine-grained "standards" from the Mathematics Common Core (CC) Standard for K-8 grades. We encode each standard in a formal grammar, allowing us to sample diverse symbolic problems and their answers. We then use LLMs to realize the symbolic problems into word problems. We propose a cycle-consistency method for validating problem faithfulness. Finally, we derive _follow-up questions_ from symbolic structures and convert them into follow-up word problems--a novel task of mathematical dialogue that probes for robustness in understanding. Experiments on 23 LLMs show surprising failures even in the strongest models (in particular when asked simple follow-up questions). Moreover, we evaluate training checkpoints of Pythia 12B on MathCAMPS, allowing us to analyze when particular mathematical skills develop during its training. Our framework enables the community to reproduce and extend our pipeline for a fraction of the typical cost of building new high-quality datasets. Project page: https://mathcamps.cc.

## 1 Introduction

As Large Language Models (LLMs) become increasingly capable, mathematical reasoning has become a key benchmark for evaluating their abilities. Traditional benchmarking, which relies on fixed sets of human-generated problems (e.g., GSM8k[8], or MATH [11]), now faces new challenges. Many LLMs are trained on vast public datasets that may include these benchmarks, raising concerns about data contamination [20; 7; 4]. This issue is amplified by the lack of transparency in the training data of most state-of-the-art models, including GPT-4 [1], Claude [2], and LLaMA [19]. While creating novel problems could mitigate contamination concerns but is resource-intensive. Moreover, current benchmarks offer limited insights into the specific mathematical skills of LLMs, as aggregate accuracy alone does not reveal where models excel or struggle, and how this has changed over time.

To address these issues, we introduce the Mathematics Common Core Assessment of Problem Solving (MathCAMPS), a framework for generating high-quality mathematical problems based on the Common Core (CC) standards. MathCAMPS enables detailed analysis of LLMs' mathematical proficiency, aligned with skills taught in schools. Our pipeline employs a composable grammar for generating problems, symbolic solvers (e.g. SymPy) to get final solutions, and an LLM for transforming them into word problems. We ensure problem faithfulness through a cycle-consistency check, where the LLM back-translates word problems into symbolic form.

We also propose a novel "mathematical dialogue" task, where the model answers follow-up questions after solving a problem. These follow-ups can be either _counterfactual_, modifying an aspect of the original problem, or _incremental_, providing additional information and changing the question.

Using our framework, we synthesize problems for each of 44 CC standards (Appendix C), resulting in a dataset of 4,900 initial problems and 4707 total follow-ups. Our results reveal surprising weaknesses, particularly in response to follow-up responses, highlighting significant gaps in even the strongest models. Additionally, we provide a first-of-its-kind analysis of learning dynamics of mathematical abilities in LLM training using checkpoints from Pythia 12B [6] (Appendix B).

## 2 MathCAMPS

We now describe our pipeline for automatically generating mathematical problems and follow-up questions that are grounded in a human curriculum - the Mathematics Common Core (https://www.thecorestandards.org). Figure 1 overviews our pipeline. We describe how we represent CC standards in a grammar, sample symbolic problems, generate follow-ups, realize those in natural language, and finally improve quality by checking for cycle consistency.

**Representing Common Core Standards** We represent CC standards using an attribute grammar [10], allowing both syntactic and semantic rules. This formalism supports context-sensitive constraints, enabling encoding of information like numerical bounds directly in production rules.

**From Symbolic to Word Problems** To convert symbolic problems into natural language, we use few-shot prompting with GPT-4 (Figure 1 (C)). For each standard, we manually create word problems from two symbolic examples. For word problems requiring cover stories, we randomly select a theme from a set of 188. These examples guide GPT-4 in generating diverse, natural problems. To ensure faithfulness to the original structure, we apply a _cycle consistency_ approach: GPT-4 converts its generated word problem back into a symbolic structure, which is solved and compared to the original. Problems failing this test are discarded.

**Generating Follow-Up Questions** We leverage symbolic representations to generate two types of follow-up questions: _counterfactual_ (altering a constant) and _incremental_ (adding information). For each CC standard, we identify applicable follow-up types. Symbolically, follow-up questions are modeled as differences applied to the original problem, which we solve to produce ground-truth answers. We use few-shot prompting to translate these changes into natural language questions and apply cycle consistency to verify accuracy.

Figure 1: Overview of the MathCAMPS generation pipeline. We start from a grammar (**A**) that represents problems tied to a Common Core Standard - a specific mathematical ability drawn from a human curriculum. We sample problems in a symbolic form (**B**), and use a language model to realize it in natural language (**C**), applying a cycle-consistency where we back-translate the problem into symbolic form and ensure the answer remains the same, validating truthfulness. We also synthesize incremental and counterfactual follow-up problems

## 3 Experiments

We now evaluate a suite of 23 LLMs from 8 different vendors on MathCAMPS. We evaluate all models by sampling with temperature 0, using a fixed 1-shot prompt with the first example from GSM8K, mostly to demonstrate the format. For all models (most of them instruction-tuned), a single example was enough for to adhere to the task and the format we specify. The rich structure in MathCAMPS allows us to perform a number of unique analyses on LLMs relating to specific mathematical abilities and their corresponding grade levels for human students.

Table 1 shows both aggregate accuracy on MathCAMPS, as well as accuracy across standards partitioned by grade, whereas Figure 3 compares the aggregate accuracies on MathCAMPS and GSM8K. Closed-weights models are shown above the line, with open-weights models below. GPT-4o ranks at the top in overall accuracy. Since we used GPT-4 to generate the problems, we must rule out familiarity bias [16] in this result, which we do in Appendix D.

**Models of similar overall performance can have large disparities in specific abilities or grades.** Several models that have comparable overall accuracies show large differences when compared on specific mathematical skills. As an example, Claude-3 Opus and Claude-3 Sonnet have similar overall accuracy both in MathCAMPS (.89 vs.86) and in GSM8K (.95 vs.923). However, we find that Claude-3 Opus is significantly better at manipulating fractions. For instance, in the CC standard 5.NF.A.2, described as _"Solve word problems involving addition and subtraction of fractions referring to the same whole, including cases of unlike denominators"_, Opus has a 36% advantage over Sonnet, scoring a 70% accuracy for this standard, whereas Sonnet only achieves 34%. Similarly, while Gemma 7B and phi-2 have comparable overall performance (.62 vs.63 accuracy on MathCAMPS), some capabilities in each model seem nearly absent from the other. Gemma 7B is highly accurate when performing multi-digit multiplication (4.NBT.B.4), which phi-2 struggles with. And while phi-2 performs well while comparing fractions (4.NF.A.2), Gemma 7B struggles. Such stark differences are obscured when only analyzing aggregate metrics, whereas MathCAMPS allows for a much more nuanced understanding of mathematical reasoning capabilities.

**Overall ranking between models is largely a function of which skills we choose to evaluate.** Overall accuracies in any dataset induce a single performance ranking of models. However, when we look at individual CC standards in MathCAMPS, rankings are largely a function of which skills we choose to evaluate. Comparing pairs of models across all standards, rarely we find cases where

\begin{table}
\begin{tabular}{c c c c c c c c c c c c} \hline \hline
**Vendor** & **Model** & **All** & **K** & **1** & **2** & **3** & **4** & **5** & **6** & **7** & **8** \\ \hline OpenAI & GPT-4o [1] & 0.92 & 0.98 & 0.98 & 0.98 & 0.98 & 0.98 & 0.92 & 0.88 & 0.95 & 0.89 & 0.64 \\ Anthropic & Claude-3 Opus [2] & 0.89 & 0.97 & 0.99 & 0.96 & 0.98 & 0.89 & 0.83 & 0.96 & 0.73 & 0.56 \\ Google & Gemini-1.5 Pro [17] & 0.89 & 0.95 & 0.98 & 0.97 & 0.97 & 0.89 & 0.83 & 0.93 & 0.78 & 0.54 \\ Google & Gemini-1.5 Flash [17] & 0.87 & 0.98 & 0.98 & 0.97 & 0.98 & 0.80 & 0.80 & 0.90 & 0.84 & 0.56 \\ OpenAI & GPT-3.5 Turbo [1] & 0.87 & 0.96 & 0.98 & 0.98 & 0.97 & 0.86 & 0.77 & 0.90 & 0.77 & 0.56 \\ Anthropic & Claude-3 Sonnet [2] & 0.86 & 0.96 & 0.98 & 0.97 & 0.98 & 0.88 & 0.74 & 0.94 & 0.66 & 0.49 \\ Anthropic & Claude-3 Haiku [2] & 0.84 & 0.97 & 0.98 & 0.97 & 0.98 & 0.87 & 0.69 & 0.92 & 0.59 & 0.51 \\ \hline Meta & Llama 3 70B [19] & 0.85 & 0.96 & 0.97 & 0.97 & 0.97 & 0.85 & 0.71 & 0.87 & 0.73 & 0.50 \\ Mistral & Mistral 8x22B [13] & 0.84 & 0.96 & 0.99 & 0.98 & 0.96 & 0.79 & 0.69 & 0.88 & 0.73 & 0.61 \\ DeepSeek & DeepSeek 67B [5] & 0.80 & 0.95 & 0.99 & 0.96 & 0.93 & 0.82 & 0.60 & 0.84 & 0.61 & 0.47 \\ Meta & Llama 3 8B [19] & 0.77 & 0.94 & 0.97 & 0.96 & 0.94 & 0.78 & 0.55 & 0.79 & 0.53 & 0.43 \\ Mistral & Mistral 8x7B [13] & 0.76 & 0.94 & 0.96 & 0.93 & 0.91 & 0.75 & 0.52 & 0.80 & 0.53 & 0.45 \\ EleutherAI & Lemma 34B [3] & 0.71 & 0.95 & 0.96 & 0.93 & 0.87 & 0.61 & 0.47 & 0.77 & 0.46 & 0.44 \\ Mistral & Mistral 7B [12] & 0.68 & 0.89 & 0.94 & 0.91 & 0.84 & 0.61 & 0.42 & 0.66 & 0.45 & 0.42 \\ DeepSeek & DeepSeek Coder 33B [9] & 0.65 & 0.88 & 0.93 & 0.92 & 0.83 & 0.54 & 0.36 & 0.66 & 0.44 & 0.38 \\ Meta & CodeLlama 34B [15] & 0.64 & 0.90 & 0.94 & 0.92 & 0.85 & 0.51 & 0.38 & 0.70 & 0.37 & 0.30 \\ Microsoft & phi-2 [14] & 0.63 & 0.95 & 0.96 & 0.89 & 0.78 & 0.46 & 0.38 & 0.61 & 0.37 & 0.41 \\ EleutherAI & Llamma 7B [3] & 0.62 & 0.88 & 0.90 & 0.85 & 0.79 & 0.48 & 0.41 & 0.67 & 0.41 & 0.36 \\ Google & Gemma 7B [18] & 0.62 & 0.83 & 0.92 & 0.90 & 0.82 & 0.47 & 0.36 & 0.65 & 0.36 & 0.30 \\ Meta & CodeLlama 13B [15] & 0.58 & 0.87 & 0.92 & 0.87 & 0.75 & 0.41 & 0.30 & 0.61 & 0.32 & 0.34 \\ Meta & CodeLlama 7B [15] & 0.52 & 0.85 & 0.92 & 0.84 & 0.69 & 0.37 & 0.25 & 0.57 & 0.25 & 0.16 \\ Google & Gemma 2B [18] & 0.51 & 0.66 & 0.76 & 0.74 & 0.67 & 0.42 & 0.28 & 0.55 & 0.30 & 0.27 \\ \hline - & Avg. Performance & 0.74 & 0.87 & 0.91 & 0.89 & 0.87 & 0.70 & 0.59 & 0.78 & 0.57 & 0.38 \\ \hline \hline \end{tabular}
\end{table}
Table 1: Final answer accuracy of LLMs on MathCAMPS, both over all problems (**All**) and considering only standards in each grade we cover (**K** to **8**). Highlights compare to gradewise avg.

[MISSING_PAGE_FAIL:4]

## References

* [1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. _arXiv preprint arXiv:2303.08774_, 2023.
* [2] AI Anthropic. The claude 3 model family: Opus, sonnet, haiku. _Claude-3 Model Card_, 2024.
* [3] Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen McAleer, Albert Q Jiang, Jia Deng, Stella Biderman, and Sean Welleck. Llemma: An open language model for mathematics. _arXiv preprint arXiv:2310.10631_, 2023.
* [4] Simone Balloccu, Patricia Schmidtova, Mateusz Lango, and Ondrej Dusek. Leak, cheat, repeat: Data contamination and evaluation malpractices in closed-source llms. _arXiv preprint arXiv:2402.03927_, 2024.
* [5] Xiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen, Damai Dai, Chengqi Deng, Honghui Ding, Kai Dong, Qiushi Du, Zhe Fu, et al. Deepseek llm: Scaling open-source language models with longtermism. _arXiv preprint arXiv:2401.02954_, 2024.
* [6] Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle O'Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al. Pythia: A suite for analyzing large language models across training and scaling. In _International Conference on Machine Learning_, pages 2397-2430. PMLR, 2023.
* [7] Sebastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro, and Yi Zhang. Sparks of artificial general intelligence: Early experiments with gpt-4, 2023.
* [8] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reitichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems, 2021.
* [9] Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Y Wu, YK Li, et al. Deepseek-coder: When the large language model meets programming-the rise of code intelligence. _arXiv preprint arXiv:2401.14196_, 2024.
* [10] Bernd Heine and Tania Kuteva. _The genesis of grammar: A reconstruction_, volume 9. Oxford University Press, USA, 2007.
* [11] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset, 2021.
* [12] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. _arXiv preprint arXiv:2310.06825_, 2023.
* [13] Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. Mistral of experts. _arXiv preprint arXiv:2401.04088_, 2024.
* [14] Yuanzhi Li, Sebastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya Gunasekar, and Yin Tat Lee. Textbooks are all you need ii: **phi-1.5** technical report. _arXiv preprint arXiv:2309.05463_, 2023.
* [15] Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jeremy Rapin, et al. Code llama: Open foundation models for code. _arXiv preprint arXiv:2308.12950_, 2023.
* [16] Rickard Stureborg, Dimitris Alikaniotis, and Yoshi Suhara. Large language models are inconsistent and biased evaluators. _arXiv preprint arXiv:2405.01724_, 2024.

* [17] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a family of highly capable multimodal models. _arXiv preprint arXiv:2312.11805_, 2023.
* [18] Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Riviere, Mihir Sanjay Kale, Juliette Love, et al. Gemma: Open models based on gemini research and technology. _arXiv preprint arXiv:2403.08295_, 2024.
* [19] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. _arXiv preprint arXiv:2307.09288_, 2023.
* [20] Hugh Zhang, Jeff Da, Dean Lee, Vaughn Robinson, Catherine Wu, Will Song, Tiffany Zhao, Pranav Raja, Dylan Slack, Qin Lyu, Sean Hendryx, Russell Kaplan, Michele Lunati, and Summer Yue. A careful examination of large language model performance on grade school arithmetic, 2024.

[MISSING_PAGE_EMPTY:7]

[MISSING_PAGE_FAIL:8]

\begin{table}
\begin{tabular}{|l|l|} \hline
**Standard ID** & **Description** \\ \hline
1.OA.A.1 & Use addition and subtraction within 20 to solve word problems involving situations of adding to, taking from, putting together, taking apart, and comparing, with unknowns in all positions, e.g., by using objects, drawings and equations with a symbol for the unknown number to represent the problem. \\ \hline
1.OA.A.2 & Solve word problems that call for addition of three whole numbers whose sum is less than or equal to 20, e.g., by using objects, drawings, and equations with a symbol for the unknown number to represent the problem. \\ \hline
1.OA.D.8 & Determine the unknown whole number in an addition or subtraction equation relating three whole numbers. \\ \hline \end{tabular}
\end{table}
Table 6: CC Standards for Grade 1

\begin{table}
\begin{tabular}{|l|l|} \hline
**Standard ID** & **Description** \\ \hline K.CC.C.7 & Compare two numbers between 1 and 10 presented as written numerals. \\ \hline K.OA.A.4 & For any number from 1 to 9, find the number that makes 10 when added to the given number, e.g., by using objects or drawings, and record the answer with a drawing or equation. \\ \hline K.OA.A.5 & Fluently add and subtract within 5. \\ \hline K.NBT.A.1 & Compose and decompose numbers from 11 to 19 Into ten ones and some further ones, e.g., by using objects or drawings, and record each composition or decomposition by a drawing or equation (e.g., 18 = 10 + 8); understand that these numbers are composed of ten ones and one, two, three, four, five, six, seven, eight, or nine ones. \\ \hline \end{tabular}
\end{table}
Table 5: CC Standards for Grade Kin-distribution for GPT-4o, but out-of-distribution for other models --, we generated a 10%-scale dataset using the exact same pipeline, but using Claude 3 Opus for both generating word problems and testing cycle consistency. This dataset has the same distribution of standards as MathCAMPS. We evaluated GPT-4o and Claude 3 Opus on this dataset -- accuracies are reported in Table 14. GPT-4o also performs better in this dataset, suggesting that its performance in MathCAMPS was not due to a higher relative familiarity with the problems.

\begin{table}
\begin{tabular}{|l|p{284.5pt}|} \hline
**Standard ID** & **Description** \\ \hline
3.OA.A.3 & Use multiplication and division within 100 to solve word problems in situations involving equal groups, arrays, and measurement quantities, e.g., by using drawings and equations with a symbol for the unknown number to represent the problem. \\ \hline
3.OA.A.4 & Determine the unknown whole number in a multiplication or division equation relating three whole numbers. \\ \hline
3.OA.C.7 & Fluently multiply and divide within 100, using strategies such as the relationship between multiplication and division (e.g., knowing that 8 × 5 = 40, one knows 40 \(\div\) 5 = 8) or properties of operations. By the end of Grade 3, know from memory all products of two one-digit numbers. \\ \hline
3.OA.D.8 & Solve two-step word problems using the four operations. Represent these problems using equations with a letter standing for the unknown quantity. Assess the reasonableness of answers using mental computation and estimation strategies including rounding. \\ \hline
3.MD.D.8-triangle & Solve real world and mathematical problems involving perimeters of polygons, including finding the perimeter given the side lengths, finding an unknown side length, and exhibiting rectangles with the same perimeter and different areas or with the same area and different perimeters. \\ \hline
3.MD.D.8-quadrilateral & Solve real world and mathematical problems involving perimeters of polygons, including finding the perimeter given the side lengths, finding an unknown side length, and exhibiting rectangles with the same perimeter and different areas or with the same area and different perimeters. \\ \hline
3.MD.D.8-polygon & Solve real world and mathematical problems involving perimeters of polygons, including finding the perimeter given the side lengths, finding an unknown side length, and exhibiting rectangles with the same perimeter and different areas or with the same area and different perimeters. \\ \hline
3.NBT.A.2 & Fluently add and subtract within 1000 using strategies and algorithms based on place value, properties of operations, and/or the relationship between addition and subtraction. \\ \hline \end{tabular}
\end{table}
Table 8: CC Standards for Grade 3

\begin{table}
\begin{tabular}{|l|p{284.5pt}|} \hline
**Standard ID** & **Description** \\ \hline
4.OA.A.3 & Solve multistep word problems posed with whole numbers and having whole-number answers using the four operations, including problems in which remainders must be Interpreted. Represent these problems using equations with a letter standing for the unknown quantity. Assess the reasonableness of answers using mental computation and estimation strategies including rounding. \\ \hline
4.OA.B.4 & Find all factor pairs for a whole number in the range 1-100. Recognize that a whole number is a multiple of each of its factors. Determine whether a given whole number in the range 1-100 is a multiple of a given one-digit number. Determine whether a given whole number in the range 1-100 is prime or composite. \\ \hline
4.NBT.B.4 & Fluently add and subtract multi-digit whole numbers using the standard algorithm. \\ \hline
4.NBT.B.5 & Multiply a whole number of up to four digits by a one-digit whole number, and multiply two two-digit numbers, using strategies based on place value and the properties of operations. Illustrate and explain the calculation by using equations, rectangular arrays, and/or area models. \\ \hline
4.NBT.B.6 & Find whole-number quotients and remainders with up to four-digit dividends and one-digit divisors, using strategies based on place value, the properties of operations, and/or the relationship between multiplication and division. Illustrate and explain the calculation by using equations, rectangular arrays, and/or area models. \\ \hline
4.NF.A.2 & Compare two fractions with different numerators and different denominators, e.g., by creating common denominators or numerators, or by comparing to a benchmark fraction such as 1/2. Recognize that comparisons are valid only when the two fractions refer to the same whole. Record the results of comparisons with symbols >, =, or <, and justify the conclusions, e.g., by using a visual fraction model. \\ \hline
4.MD.A.2-decimal & Use the four operations to solve word problems involving distances, Intervals of time, liquid volumes, masses of objects, and money, including problems involving simple fractions or decimals, and problems that require expressing measurements given in a larger unit in terms of a smaller unit. Represent measurement quantities using diagrams such as number line diagrams that feature a measurement scale. \\ \hline
4.MD.A.2-fraction & Use the four operations to solve word problems involving distances, Intervals of time, liquid volumes, masses of objects, and money, including problems involving simple fractions or decimals, and problems that require expressing measurements given in a larger unit in terms of a smaller unit. Represent measurement quantities using diagrams such as number line diagrams that feature a measurement scale. \\ \hline
4.MD.A.3 & Apply the area and perimeter formulas for rectangles in real world and mathematical problems. \\ \hline \end{tabular}
\end{table}
Table 9: CC Standards for Grade 4

\begin{table}
\begin{tabular}{|l|l|} \hline
**Standard ID** & **Description** \\ \hline
5.OA.A.1 & Use parentheses, brackets, or braces in numerical expressions, and evaluate expressions with these symbols. \\ \hline
5.NBT.B.5 & Fluently multiply multi-digit whole numbers using the standard algorithm. \\ \hline
5.NBT.B.6 & Find whole-number quotients of whole numbers with up to four-digit dividends and two-digit divisors, using strategies based on place value, the properties of operations, and/or the relationship between multiplication and division. Illustrate and explain the calculation by using equations, rectangular arrays, and/or area models. \\ \hline
5.NBT.B.7 & Add, subtract, multiply, and divide decimals to hundredths, using concrete models or drawings and strategies based on place value, properties of operations, and/or the relationship between addition and subtraction; relate the strategy to a written method and explain the reasoning used. \\ \hline
5.NF.A.1 & Add and subtract fractions with unlike denominators (including mixed numbers) by replacing given fractions with equivalent fractions in such a way as to produce an equivalent sum or difference of fractions with like denominators. \\ \hline
5.NF.A.2 & Solve word problems involving addition and subtraction of fractions referring to the same whole, including cases of unlike denominators, e.g., by using visual fraction models or equations to represent the problem. Use benchmark fractions and number sense of fractions to estimate mentally and assess the reasonableness of answers. \\ \hline
5.NF.B.4 & Apply and extend previous understandings of multiplication to multiply a fraction or whole number by a fraction. \\ \hline \end{tabular}
\end{table}
Table 10: CC Standards for Grade 5

\begin{table}
\begin{tabular}{|l|l|} \hline
**Standard ID** & **Description** \\ \hline
6.NS.B.2 & Fluently divide multi-digit numbers using the standard algorithm. \\ \hline
6.NS.B.3 & Add, subtract, multiply, and divide decimals to hundredths, using concrete models or drawings and strategies based on place value, properties of operations, and/or the relationship between addition and subtraction; relate the strategy to a written method and explain the reasoning used. \\ \hline
6.EE.A.1 & Write and evaluate numerical expressions involving whole-number exponents. \\ \hline
6.EE.B.7 & Solve real-world and mathematical problems by writing and solving equations of the form x + p = q and px = q for cases in which p, q and x are all nonnegative rational numbers. \\ \hline \end{tabular}
\end{table}
Table 11: CC Standards for Grade 6

\begin{table}
\begin{tabular}{|l|l|} \hline
**Standard ID** & **Description** \\ \hline
7.NS.A.1- & Apply and extend previous understandings of addition and subtraction to add and subtract rational numbers; represent addition and subtraction on a horizontal or vertical number line diagram. \\ \hline
7.NS.A.1- & Apply and extend previous understandings of addition and subtraction to add and subtract rational numbers; represent addition and subtraction on a horizontal or vertical number line diagram. \\ \hline
7.NS.A.2 & Apply and extend previous understandings of multiplication and division and of fractions to multiply and divide rational numbers. \\ \hline
7.NS.A.3- & Solve real-world and mathematical problems involving the four operations with rational numbers. \\ \hline
7.NS.A.3- & Solve real-world and mathematical problems involving the four operations with rational numbers. \\ \hline
7.NS.A.3- & Solve real-world and mathematical problems involving the four operations with rational numbers. \\ \hline \end{tabular}
\end{table}
Table 12: CC Standards for Grade 7

\begin{table}
\begin{tabular}{|l|l|} \hline
**Standard ID** & **Description** \\ \hline
5.NBT.B.5 & Fluently multiply multi-digit whole numbers using the standard algorithm. \\ \hline
5.NBT.B.6 & Find whole-number quotients of whole numbers with up to four-digit dividends and two-digit divisors, using strategies based on place value, the properties of operations, and/or the relationship between multiplication and division. Illustrate and explain the calculation by using equations, rectangular arrays, and/or area models. \\ \hline
5.NBT.B.7 & Add, subtract, multiply, and divide decimals to hundredths, using concrete models or drawings and strategies based on place value, properties of operations, and/or the relationship between addition and subtraction; relate the strategy to a written method and explain the reasoning used. \\ \hline
5.NF.A.1 & Add and subtract fractions with unlike denominators (including mixed numbers) by replacing given fractions with equivalent fractions in such a way as to produce an equivalent sum or difference of fractions with like denominators. \\ \hline
5.NF.A.2 & Solve word problems involving addition and subtraction of fractions referring to the same whole, including cases of unlike denominators, e.g., by using visual fraction models or equations to represent the problem. Use benchmark fractions and number sense of fractions to estimate mentally and assess the reasonableness of answers. \\ \hline
5.NF.B.4 & Apply and extend previous understandings of multiplication to multiply a fraction or whole number by a fraction. \\ \hline \end{tabular}
\end{table}
Table 10: CC Standards for Grade 5

## Appendix E Data generation pipeline details

### Grammar

We implemented a global attribute grammar in Python, where production rules are implemented as recursive Python functions. Effectively, each CC standard has its own grammar, composed of pieces from components from the global CC grammar, as well as possibly adding unique non-terminals. Each CC standard contains the following parameters:

**Description:**: The description of the CC standard.
**Short description:**: A shortened description of the CC standard.
**Filters:**: A list of problem filters to ensure that all problems in this standard satisfy some requirement given in the Common Core description of the standard. The ProblemLength filter makes sure that the problem is within the desired length. CheckIntermediateValues filters out any problems with intermediate values greater or lesser than max_value or min_value, respectively. The ChainsOfVariables filter eliminates any problems where variables are assigned to equal exactly another variable, and nothing else. The ContainsTen filter checks if the math word problem contains numbers adding up to 10, or contains a 10 in the problem (for standards K.O.A.4 and K.NBT.A.1, respectively).
**Transforms:**: List of problem transformations applied to all symbolic structures from this standard. The NoUselessVariables transform performs dead code elimination -- it removes any variables that do not contribute to the final answer by applying a simple graph reachability algorithm on a dependency graph between statements, removing statements that the answer does not depend on. The Simplify transform essentially inline variables that are used only once.
**Expressions:**: Lists non-terminals available to generate expressions in symbolic structures for this standard. For example, this can make specific binary operations (e.g. addition, division) available on that particular standard.
**Min/max value:**: Specifies bounds on values for both the final answer and all intermediate values in the solution.
**Min/max number:**: Specifies bounds on numeric constants sampled in the symbolic structure.
**Max depth:**: Sets a maximum depth for expressions in the symbolic structure.
**Samples:**: We include 2+ hand-written, standard-relevant examples of a symbolic problem followed by a relevant natural language problem generation, which we use as few-shot prompts during problem generation. We also use these prompts, but in reverse (natural language followed by symbolic problem), when we prompt GPT-4 during cycle consistency.

\begin{table}
\begin{tabular}{|l|l c|} \hline \multicolumn{1}{|c|}{**Standard ID**} & **Description** \\ \hline
8.EE.A.2 & Use square root and cube root symbols to represent solutions to equations of the form x\({}^{2}\) = p and x\({}^{3}\) = p, where p is a positive rational number. & Evaluate square roots of small perfect squares and cube roots of small perfect cubes. Know that the square root of 2 is irrational. \\ \hline
8.EE.C.7 & Solve linear equations in one variable. & \\ \hline
8.EE.C.8 & Analyze and solve pairs of simultaneous linear equations. & \\ \hline \end{tabular}
\end{table}
Table 13: CC Standards for Grade 8

\begin{table}
\begin{tabular}{c|c c} \hline \hline Model & GPT4-generated MathCAMPS accuracy & Claude-generated MathCAMPS accuracy \\ \hline GPT-4o & 0.910 & 0.954 \\ Claude 3 Opus & 0.887 & 0.909 \\ \hline \end{tabular}
\end{table}
Table 14: Performance of GPT-4o and Claude 3 Opus on the dataset generated using Claude

### Answer Grading During Evaluation

Given a solution in natural language, we first use a rule-based answer extractor to extract any model's numerical answer. In cases where a language model doesn't answer in the required format, or answers in an unexpected format, the answer is initially marked as incorrect. For all problems with incorrect answers, we use Llama-3 70B to re-extract the final answer. We few-shot prompt it with hand-generated examples of solutions and extracted final answers, and ask it to extract the final answer from the new solution. If a problem that was previously incorrect is marked as correct (given the newly extracted answer), we rerun the model on any followups the problem might have. Note that this "regrading" step can only improve accuracy from the base result, since we only run it on problems that failed under the rule-based evaluation. In practice, we found this process to have negligible false-positive rate -- only in a handful of cases across all models we observed either answer extraction processes extracting the correct answer out of a wrong response (e.g., if the answer to a problem is 2, and the model responds "On day 2, Sally bought 9 dolls", the rule-based parser extracts 2 as being the model's answer, though the sentence implies its answer to be 9). On the other hand, the LLaMA-3 70B extractor greatly reduces our false negative rate in a handful of models (especially DeepSeek) which are more likely to respond in a format different from what our prompt asks for.

### Cost estimate

All problems in MathCAMPS were generated using OpenAI gpt-4-0613, in May 2024. We estimate an approximate cost of 330 USD to generate 9607 problems (including main problems and followups). This includes the cost to perform cycle consistency, and problems that are discarded by cycle consistency. This gives an average cost of 0.034 USD (3.4 cents) per cycle-consistent problem or follow-up question.

## Appendix F Correlation between MathCAMPS and GSM8k

Figure 3 shows accuracies of several models on both GSM8k and MathCAMPS, along with the line of best fit. There is a strong correlation between overall accuracy in both datasets (\(\rho=0.91\), \(p<10^{-6}\)), though MathCAMPS allows for many fine-grained analysis besides overall performance.

## Appendix G Largest Model Rank Changes When Focusing on One CC Standard (Complete Table)

Table 15 shows the full table from which Table 3 was extracted.

## Appendix H Followup Analysis

Table 16 lists model accuracies when only looking at the main problems (Main Acc.), their accuracies when only looking at the incremental followups (IFUP Acc.), their accuracies when only looking at the counterfactual followups (CFUP Acc.), and finally, the total number of followups seen by each model. The total number of followups a model sees relies on whether or not they get the main question for that followup correct. If a model does not correctly solve the main question, it is not prompted with follow-ups. Note that each followup serves as a followup to the main question, as opposed to a followup to each other.

\begin{table}
\begin{tabular}{c c|c c c c} \hline \hline
**Vendor** & **Model** & **Main Acc.** & **IFUP Acc.** & **CFUP Acc.** & **Total FUPs seen** \\ \hline Anthropic & Claude-3 Opus & 0.89 & 0.90 & 0.88 & 4142 \\ Anthropic & Claude-3 Sonnet & 0.86 & 0.86 & 0.87 & 3964 \\ Anthropic & Claude-3 Haiku & 0.84 & 0.88 & 0.87 & 3819 \\ DeepSeek & DeepSeek Coder 33B & 0.65 & 0.79 & 0.85 & 1022 \\ DeepSeek & DeepSeek 67B & 0.80 & 0.87 & 0.88 & 3286 \\ EleutherAI & LLemma 7B & 0.62 & 0.68 & 0.80 & 2890 \\ Google & Gemini-1.5 Pro & 0.89 & 0.91 & 0.89 & 4140 \\ Google & Gemini-1.5 Flash & 0.87 & 0.89 & 0.87 & 4083 \\ Google & Gemini 2B & 0.51 & 0.29 & 0.54 & 2044 \\ Google & Gemini 7B & 0.62 & 0.55 & 0.60 & 2786 \\ Meta & Llama 3 8B & 0.77 & 0.84 & 0.80 & 3476 \\ Meta & Llama 3 70B & 0.85 & 0.87 & 0.84 & 3939 \\ Meta & CodeLlama 7B & 0.52 & 0.69 & 0.86 & 617 \\ Meta & CodeLlama 13B & 0.58 & 0.75 & 0.80 & 2451 \\ Meta & CodeLlama 34B & 0.64 & 0.82 & 0.88 & 844 \\ Microsoft & phi-2 & 0.63 & 0.48 & 0.78 & 2873 \\ Mistral & Mistral 7B & 0.68 & 0.72 & 0.80 & 3090 \\ Mistral & Mistral 8x7B & 0.76 & 0.80 & 0.82 & 3439 \\ Mistral & Mistral 8x22B & 0.84 & 0.86 & 0.83 & 3948 \\ OpenAI & GPT-4o & 0.92 & 0.92 & 0.90 & 4358 \\ OpenAI & GPT-3.5 Turbo & 0.87 & 0.85 & 0.86 & 4063 \\ \hline \hline \end{tabular}
\end{table}
Table 16: Accuracy of each model on _incremental_ follow-up questions (**IFUP**) as well as on _counterfactual_ follow-ups (**CFUP**). Note that these accuracies are not directly comparable, since models are only evaluated on follow-ups to problems that they respond correctly to; thus, each accuracy shown here is over a different subset of follow-up problems in MathCAMPS.

\begin{table}
\begin{tabular}{c c} \hline \hline
**Model** & **Top outlier skill** & **Rank change** \\ \hline GPT-4o & 8.EE.C.8 - Solve two-variable systems & (\(1^{st}\)\(\blacktriangleright\)2\(2^{th}\)) \\ Claude-3 Opus & 2.MD.B.5 - Add/sub within 100 & (\(2^{nd}\)\(\blacktriangleright\)13\({}^{th}\)) \\ Gemini-1.5 Pro & K.O.A.A.4 - Adding to equal 10 & (\(3^{rd}\)\(\blacktriangleright\)19\({}^{th}\)) \\ Gemini-1.5 Flash & 4.OA.B.4 - Factor pairs within 100 & (\(4^{th}\)\(\blacktriangleright\)20\({}^{th}\)) \\ GPT-3.5 Turbo & 6.EE.A.1 - Evaluate exponents & (\(5^{th}\)\(\blacktriangleright\)21\({}^{th}\)) \\ Claude-3 Sonnet & 2.NBT.B.5 - Add/sub within 100 & (\(6^{th}\)\(\blacktriangleright\)12\({}^{th}\)) \\ Claude-3 Haiku & 3.OA.A.4 - Determine unknowns in mul/div probs & (\(9^{th}\)\(\blacktriangleright\)1\({}^{st}\)) \\ \hline Llama 3 70B & K.O.A.A.4 - Adding to equal 10 & (\(7^{th}\)\(\blacktriangleright\)17\({}^{th}\)) \\ Mixtral 8x22B & 8.EE.C.8 - Solve two-variable systems & (\(8^{th}\)\(\blacktriangleright\)21\({}^{th}\)) \\ DeepSeek 67B & K.NBT.A.1 - Decompose into 10s & (\(10^{th}\)\(\blacktriangleright\)1\({}^{st}\)) \\ Llama 3 8B & 4.NBT.B.4 - Add/sub multi-digit nums & (\(11^{th}\)\(\blacktriangleright\)21\({}^{th}\)) \\ Mixtral 8x7B & 6.EE.A.1 - Evaluate exponents & (\(12^{th}\)\(\blacktriangleright\)20\({}^{th}\)) \\ LLemma 34B & K.O.A.A.4 - Adding to equal 10 & (\(13^{th}\)\(\blacktriangleright\)1\({}^{st}\)) \\ Mistral 7B & 1.OA.A.1 - Add/sub within 20 & (\(14^{th}\)\(\blacktriangleright\)21\({}^{th}\)) \\ DeepSeek Coder 33B & 6.EE.A.1 - Evaluate exponents & (\(15^{th}\)\(\blacktriangleright\)3\({}^{rd}\)) \\ CodeLlama 34B & 5.NF.A.1 - Add/sub fractions & (\(16^{th}\)\(\blacktriangleright\)22\({}^{th}\)) \\ phi-2 & K.O.A.A.4 - Adding to equal 10 & (\(17^{th}\)\(\blacktriangleright\)4\({}^{th}\)) \\ LLemma 7B & 6.EE.A.1 - Evaluate exponents & (\(18^{th}\)\(\blacktriangleright\)5\({}^{th}\)) \\ Gemma 7B & K.O.A.A.5 - Add/sub within 5 & (\(19^{th}\)\(\blacktriangleright\)6\({}^{th}\)) \\ CodeLlama 7B & 8.EE.C.8 - Solve two-variable systems & (\(21^{th}\)\(\blacktriangleright\)15\({}^{th}\)) \\ Gemma 2B & 8.EE.C.8 - Solve two-variable systems & (\(22^{th}\)\(\blacktriangleright\)11\({}^{th}\)) \\ \hline \hline \end{tabular}
\end{table}
Table 15: Largest changes in a model’s ranking when comparing its performance in a particular CC standard, in contrast to only overall performance. This is a complete version of Table 3, which only showed some models for brevity.

Figure 3: Relation between accuracy on GSM8k and on MathCAMPS.