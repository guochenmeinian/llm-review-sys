# Adversarial Examples Might be Avoidable: The Role of Data Concentration in Adversarial Robustness

 Ambar Pal

ambar@jhu.edu

&Jeremias Sulam

jsulam1@jhu.edu

&Rene Vidal

vidalr@upenn.edu

###### Abstract

The susceptibility of modern machine learning classifiers to adversarial examples has motivated theoretical results suggesting that these might be unavoidable. However, these results can be too general to be applicable to natural data distributions. Indeed, humans are quite robust for tasks involving vision. This apparent conflict motivates a deeper dive into the question: Are adversarial examples truly unavoidable? In this work, we theoretically demonstrate that a key property of the data distribution - concentration on small-volume subsets of the input space - determines whether a robust classifier exists. We further demonstrate that, for a data distribution concentrated on a union of low-dimensional linear subspaces, utilizing structure in data naturally leads to classifiers that enjoy data-dependent polyhedral robustness guarantees, improving upon methods for provable certification in certain regimes.

## 1 Introduction, Motivation and Contributions

Research in adversarial learning has shown that traditional neural network based classification models are prone to anomalous behaviour when their inputs are modified by tiny, human-imperceptible perturbations. Such perturbations, called adversarial examples, lead to a large degradation in the accuracy of classifiers [55]. This behavior is problematic when such classification models are deployed in security sensitive applications. Accordingly, researchers have and continue to come up with _defenses_ against such adversarial attacks for neural networks.

Such defenses [49; 60; 42; 22] modify the training algorithm, alter the network weights, or employ preprocessing to obtain classifiers that have improved empirical performance against adversarially corrupted inputs. However, many of these defenses have been later broken by new adaptive attacks [1; 8]. This motivated recent impossibility results for adversarial defenses, which aim to show that all defenses admit adversarial examples. While initially such results were shown for specially parameterized data distributions [18], they were subsequently expanded to cover general data distributions on the unit sphere and the unit cube [48], as well as for distributions over more general manifolds [12].

On the other hand, we humans are an example of a classifier capable of very good (albeit imperfect [17]) robust accuracy against \(\ell_{2}\)-bounded attacks for natural image classification. Even more, a large body of recent work has constructed _certified_ defenses [11; 63; 10; 29; 19; 54] which obtain non-trivial performance guarantees under adversarially perturbed inputs for common datasets like MNIST, CIFAR-10 and ImageNet. This apparent contention between impossibility results and the existence of robust classifiers for natural datasets indicates that the bigger picture is more nuanced, and motivates a closer look at the impossibility results for adversarial examples.

Our first contribution is to show that these results can be circumvented by data distributions whose mass concentrates on small regions of the input space. This naturally leads to the question of whether such a construction is necessary for adversarial robustness. We answer this question in the affirmative, formally proving that a successful defense exists only when the data distribution concentrates on anexponentially small volume of the input space. At the same time, this suggests that exploiting the inherent structure in the data is critical for obtaining classifiers with broader robustness guarantees.

Surprisingly, almost1 all _certified_ defenses do not exploit any structural aspects of the data distribution like concentration or low-dimensionality. Motivated by our theoretical findings, we study the special case of data distributions concentrated near a union of low-dimensional linear subspaces, to create a certified defense for perturbations that go beyond traditional \(\ell_{p}\)-norm bounds. We find that simply exploiting the low-dimensional data structure leads to a natural classification algorithm for which we can derive norm-independent polyhedral certificates. We show that our method can certify accurate predictions under adversarial examples with an \(\ell_{p}\) norm larger than what can be certified by applying existing, off-the-shelf methods like randomized smoothing [11]. Thus, we demonstrate the importance of structure in data for both the theory and practice of certified adversarial robustness.

Footnote 1: See Section 6 for more details.

More precisely, we make the following main contributions in this work:

1. We formalize a notion of \((\epsilon,\delta)\)-concentration of a probability distribution \(q\) in Section 2, which states that \(q\) assigns at least \(1-\delta\) mass to a subset of the ambient space having volume \(O(\exp{(-n\epsilon)})\). We show that \((\epsilon,\delta)\)-concentration of \(q\) is a necessary condition for the existence of any classifier obtaining at most \(\delta\) error over \(q\), under perturbations of size \(\epsilon\).
2. We find that \((\epsilon,\delta)\)-concentration is too general to be a sufficient condition for the existence of a robust classifier, and we follow up with a stronger notion of concentration in Section 3 which is sufficient for the existence of robust classifiers. Following this stronger notion, we construct an example of a strongly-concentrated distribution, which circumvents existing impossibility results on the existence of robust classifiers.
3. We then consider a data distribution \(q\) concentrated on a union of low-dimensional linear subspaces in Section 4. We construct a classifier for \(q\) that is robust to perturbations following threat models more general than \(\ell_{p}\). Our analysis results in polyhedral certified regions whose faces and extreme rays are described by selected points in the training data.
4. We perform empirical evaluations on MNIST in Section 5, demonstrating that our certificates are complementary to existing off-the-shelf approaches like Randomized Smoothing (RS), in the sense that both methods have different strengths. In particular, we demonstrate a region of adversarial perturbations where our method is certifiably robust, but RS is not. We then combine our method with RS to obtain certificates that enjoy the best of both worlds.

## 2 Existence of Robust Classifier Implies Concentration

We will consider a classification problem over \(\mathcal{X}\times\mathcal{Y}\) defined by the data distribution \(p\) such that \(\mathcal{X}\) is bounded and \(\mathcal{Y}=\{1,2,\ldots,K\}\). We let \(q_{k}\) denote the conditional distribution \(p_{X|Y=k}\) for class \(k\in\mathcal{Y}\). We will assume that the data is normalized, i.e., \(\mathcal{X}=B_{\ell_{2}}(0,1)\), and the boundary of the domain is far from the data, i.e., for any \(x\sim q_{k}\), an adversarial perturbation of \(\ell_{2}\) norm at most \(\epsilon\) does not take \(x\) outside the domain \(\mathcal{X}\).2

Footnote 2: More details in Appendix A.

We define the robust risk of a classifier \(f\colon\mathcal{X}\to\mathcal{Y}\) against an adversary making perturbations whose \(\ell_{2}\) norm is bounded by \(\epsilon\) as 3

Footnote 3: Note that for \(f(\bar{x})\) to be defined, it is implicit that \(\bar{x}\in\mathcal{X}\) in (1).

\[R(f,\epsilon)=\Pr_{(x,y)\sim p}\left(\exists\bar{x}\in B_{\ell_{2}}(x,\epsilon) \text{ such that }f(\bar{x})\neq y\right).\] (1)

We can now define a robust classifier in our setting.

**Definition 2.1** (Robust Classifier).: _A classifier \(g\) is defined to be \((\epsilon,\delta)\)-robust if the robust risk against perturbations with \(\ell_{2}\) norm bounded by \(\epsilon\) is at most \(\delta\), i.e., if \(R(g,\epsilon)\leq\delta\)._

The goal of this section is to show that if our data distribution \(p\) admits an \((\epsilon,\delta)\)-robust classifier, then \(p\) has to be _concentrated_. Intuitively, this means that \(p\) assigns a "large" measure to sets of "small" volume. We define this formally now.

**Definition 2.2** (Concentrated Distribution).: _A probability distribution \(q\) over a domain \(\mathcal{X}\subseteq\mathbb{R}^{n}\) is said to be \((C,\epsilon,\delta)\)-concentrated, if there exists a subset \(S\subseteq\mathcal{X}\) such that \(q(S)\geq 1-\delta\) but \(\operatorname{Vol}(S)\leq C\exp(-n\epsilon)\). Here, \(\operatorname{Vol}\) denotes the standard Lebesgue measure on \(\mathbb{R}^{n}\), and \(q(S)\) denotes the measure of \(S\) under \(q\)._

With the above definitions in place, we are ready to state our first main result.

**Theorem 2.1**.: _If there exists an \((\epsilon,\delta)\)-robust classifier \(f\) for a data distribution \(p\), then at least one of the class conditionals \(q_{1},q_{2},\ldots,q_{K}\), say \(q_{k}\), must be \((\bar{C},\epsilon,\delta)\)-concentrated. Further, if the classes are balanced, then all the class conditionals are \((C_{\max},\epsilon,K\delta)\)-concentrated. Here, \(\bar{C}=\operatorname{Vol}\{x\colon f(x)=\bar{k}\}\), and \(C_{\max}=\max_{k}\operatorname{Vol}\{x\colon f(x)=k\}\) are constants dependent on \(f\)._

The proof is a natural application of the Brunn-Minkowski theorem from high-dimensional geometry, essentially using the fact that an \(\epsilon\)-shrinkage of a high-dimensional set has very small volume. We provide a brief sketch here, deferring the full proof to Appendix A.

Proof Sketch.: Due to the existence of a robust classifier \(f\), _i.e._, \(R(f,\epsilon)\leq\delta\), the first observation is that there must be at least one class which is classified with robust accuracy at least \(1-\delta\). Say this class is \(k\), and the set of all points which do not admit an \(\epsilon\)-adversarial example for class \(k\) is \(S\). Now, the second step is to show that \(S\) has the same measure (under \(q_{k}\)) as the \(\epsilon\)-shrinkage (in the \(\ell_{2}\) norm) of the set of all points classified as class \(k\). Finally, the third step involves using the Brunn-Minkowski theorem, to show that this \(\epsilon\)-shrinkage has a volume \(O(\exp(-n\epsilon))\), thus completing the argument. 

**Discussion on Theorem 2.1.** We pause here to understand some implications of this result.

* Firstly, recall the apparently conflicting conclusions from Section 1 between impossibility results (suggesting that robust classifiers do not exist) and the existence of robust classifiers in practice (such as that of human vision for natural data distributions). Theorem 2.1 shows that whenever a robust classifier exists, the underlying data distribution has to be concentrated. In particular, this suggests that natural distributions corresponding to MNIST, CIFAR and ImageNet might be concentrated. This indicates a resolution to the conflict: concentrated distributions must somehow circumvent existing impossibility results. Indeed, this is precisely what we will show in Section 3.
* Secondly, while our results are derived for the \(\ell_{2}\) norm, it is not very hard to extend this reasoning to general \(\ell_{p}\) norms. In other words, whenever a classifier robust to \(\ell_{p}\)-norm perturbations exists, the underlying data distribution must be concentrated.
* Thirdly, Theorem 2.1 has a direct implication towards classifier design. Since we now know that natural image distributions are concentrated, one should design classifiers that are tuned for small-volume regions in the input space. This might be the deeper principle behind the recent success [66] of robust classifiers adapted to \(\ell_{p}\)-ball like regions in the input space.
* Finally, the _extent_ of concentration implied by Theorem 2.1 depends on the classifier \(f\), via the parameters \(\epsilon,\delta\) and \(\bar{C}\). On one hand, we get _high_ concentration when \(\epsilon\) is large, \(\delta\) is small, and \(\bar{C}\) is small. On the other hand, if the distribution \(p\) admits an \((\epsilon,\delta)\)-robust classifier such that \(\bar{C}\) is large (e.g., a constant classifier), then we get _low_ concentration via Theorem 2.1. This is not a limitation of our proof technique, but a consequence of the fact that some simple data distributions might be very lowly concentrated, but still admit very robust classifiers, e.g., for a distribution having \(95\%\) dogs and \(5\%\) cats, the constant classifier which always predicts "dog" is quite robust.

We have thus seen that data concentration is a necessary condition for the existence of a robust classifier. A natural question is whether it is also sufficient. We address this question now.

## 3 Strong Concentration Implies Existence of Robust Classifier

Say our distribution \(p\) is such that all the class conditionals \(q_{1},q_{2},\ldots,q_{k}\) are \((C,\epsilon,\delta)\)-concentrated. Is this sufficient for the existence of a robust classifier? The answer is negative, as we have not precluded the case where all of the \(q_{k}\) are concentrated over the same subset \(S\) of the ambient space. In other words, it might be possible that there exists a small-volume set \(S\subseteq\mathcal{X}\) such that \(q_{k}(S)\) is high for all \(k\). This means that whenever a data point lies in \(S\), it would be hard to distinguishwhich class it came from. In this case, even an accurate classifier might not exist, let alone a robust classifier4. To get around such issues, we define a stronger notion of concentration, as follows.

Footnote 4: Recall that classifier not accurate at a point \((x,y)\), i.e., \(f(x)\neq y\), is by definition not robust at \(x\), as a \(v=0\) perturbation is already sufficient to ensure \(f(x+v)\neq y\).

**Definition 3.1** (Strongly Concentrated Distributions).: _A distribution \(p\) is said to be \((\epsilon,\delta,\gamma)\)-strongly-concentrated if each class conditional distribution \(q_{k}\) is concentrated over the set \(S_{k}\subseteq\mathcal{X}\) such that \(q_{k}(S_{k})\geq 1-\delta\), and \(q_{k}\left(\bigcup_{k^{\prime}\neq k}S_{k^{\prime}}^{+2\epsilon}\right)\leq\gamma\), where \(S^{+\epsilon}\) denotes the \(\epsilon\)-expansion of the set \(S\) in the \(\ell_{2}\) norm, i.e., \(S^{+\epsilon}=\{x\colon\exists\bar{x}\in S\text{ such that }\|x-\bar{x}\|_{2}\leq\epsilon\}\)._

In essence, Definition 3.1 states that each of the class conditionals are concentrated on subsets of the ambient space, which do not intersect too much with one another5. Hence, it is natural to expect that we would be able to construct a robust classifier by exploiting these subsets. Building upon this idea, we are able to show Theorem 3.1:

Footnote 5: More detailed discussion in Appendix G.

**Theorem 3.1**.: _If the data distribution \(p\) is \((\epsilon,\delta,\gamma)\)-strongly-concentrated, then there exists an \((\epsilon,\delta+\gamma)\)-robust classifier for \(p\)._

The basic observation behind this result is that if the conditional distributions \(q_{k}\) had disjoint supports which were well-separated from each other, then one could obtain a robust classifier by predicting the class \(k\) on the entire \(\epsilon\)-expansion of the set \(S_{k}\) where the conditional \(q_{k}\) concentrates, for all \(k\). To go beyond this idealized case, we can exploit the strong concentration condition to carefully remove the intersections at the cost of at most \(\gamma\) in robust accuracy. We make these arguments more precise in the full proof, deferred to Appendix B, and we pause here to note some implications for existing results.

**Implications for Existing Impossibility Results.** To understand how Theorem 3.1 circumvents the previous impossibility results, consider the setting from [48] where the data domain is the sphere \(\mathbb{S}^{n-1}=\{x\in\mathbb{R}^{n}\colon\|x\|_{2}=1\}\), and we have a binary classification setting with class conditionals \(q_{1}\) and \(q_{2}\). The adversary is allowed to make bounded perturbations w.r.t. the geodesic distance. In this setting, it can be shown (see [48, Theorem 1]) that any classifier admits \(\epsilon\)-adversarial examples for the minority class (say class \(1\)), with probability at least

\[1-\alpha_{q_{1}}\beta\exp\left(-\frac{n-1}{2}\epsilon^{2}\right),\] (2)

where \(\alpha_{q_{1}}=\sup_{x\in\mathbb{S}^{n-1}}q_{1}(x)\) depends on the conditional distribution \(q_{1}\), and \(\beta\) is a normalizing constant that depends on the dimension \(n\). Note that this result assumes little about the conditional \(q_{1}\). Now, by constructing a strongly-concentrated data distribution over the domain, we will show that the lower bound in (2) becomes vacuous.

**Example 3.1**.: _The data domain is the unit sphere \(\mathbb{S}^{n-1}\) equipped with the geodesic distance \(d\). The label domain is \(\{1,2\}\). \(P\) is an arbitrary, but fixed, point lying on \(\mathbb{S}^{n-1}\). The conditional density of class \(1\), i.e., \(q_{1}\) is now defined as_

\[q_{1}(x)=\begin{cases}\frac{1}{C}\frac{1}{\sin^{n-2}d(x,P)},&\text{ if }d(x,P)\leq 0.1\\ 0,&\text{ otherwise}\end{cases},\]

_where \(C=0.1\) is a normalizing constant. The conditional density of class 2 is defined to be uniform over the complement of the support of \(q_{1}\), i.e. \(q_{2}=\operatorname{Unif}(\{x\in\mathbb{S}^{n-1}\colon d(x,P)>0.1\})\). Finally, the classes are balanced, i.e., \(p_{Y}(1)=p_{Y}(2)=1/2\)._

The data distribution constructed in Example 3.1 makes Eq. (2) vacuous, as the supremum over the density \(q_{1}\) is unbounded. Additionally, the linear classifier defined by the half-space \(\{x\colon\langle x,P\rangle\leq\cos(0.1)\}\) is robust (Appendix C provides a derivation of the robust risk, and further comments on generalizing this example). Example 3.1 is plotted for \(n=3\) dimensions in Fig. 1.

Figure 1: A plot of \(q_{1}\). Redder colors denote a larger density, and the gray plane denotes the robust classifier.

Compatibility of Theorem 3.1 with existing Negative ResultsThus, we see that strongly concentrated distributions are able to circumvent existing impossibility results on the existence of robust classifiers. However, this does _not_ invalidate any existing results. Firstly, measure-concentration-based results [48; 18; 12] provide non-vacuous guarantees given a _sufficiently flat_ (not concentrated) data distribution, and hence do not contradict our results. Secondly, our results are existential and do not provide, in general, an algorithm to _construct_ a robust classifier given a strongly-concentrated distribution. Hence, we also do not contradict the existing stream of results on the computational hardness of finding robust classifiers [6; 56; 47]. Our positive results are complementary to all such negative results, demonstrating a general class of data distributions where robust classifiers do exist.

For the reminder of this paper, we will look at a specific member of the above class of strongly concentrated data distributions and show how we can practically construct robust classifiers.

## 4 Adversarially Robust Classification on Union of Linear Subspaces

The union of subspaces model has been shown to be very useful in classical computer vision for a wide variety of tasks, which include clustering faces under varying illumination, image segmentation, and video segmentation [57]. Its concise mathematical description often enables the construction and theoretical analysis of algorithms that also perform well in practice. In this section, we will study robust classification on data distributions concentrated on a union of low-dimensional linear subspaces. This data structure will allow us to obtain a non-trivial, practically relevant case where we can show a provable improvement over existing methods for constructing robust classifiers in certain settings. Before delving further, we now provide a simple example (which is illustrated in Fig. 2) demonstrating how distributions concentrated about linear subspaces are concentrated precisely in the sense of Definition 3.1, and therefore allow for the existence of adversarially robust classifiers.

**Example 4.1**.: _The data domain is the ball \(B_{\ell_{\infty}}(0,1)\) equipped with the \(\ell_{2}\) distance. The label domain is \(\{1,2\}\). Subspace \(S_{1}\) is given by \(S_{1}=\{x\colon x^{\top}e_{1}=0\}\), and \(S_{2}\) is given by \(S_{2}=\{x\colon x^{\top}e_{2}=0\}\), where \(e_{1},e_{2}\) are the standard unit vectors. The conditional densities are defined as_

\[q_{1} =\mathrm{Unif}(\{x\colon\|x\|_{\infty}\leq 1,|x^{\top}e_{1}| \leq e^{-\alpha}/2\}),\text{ and,}\] \[q_{2} =\mathrm{Unif}(\{x\colon\|x\|_{\infty}\leq 1,|x^{\top}e_{2}| \leq e^{-\alpha}/2\}),\]

_where \(\alpha>0\) is a large constant. Finally, the classes are balanced, i.e., \(p_{Y}(1)=p_{Y}(2)=1/2\). With these parameters, \(q_{1},q_{2}\) are both \((0.5,\alpha/n-1,0)\)-concentrated over their respective supports. Additionally, \(p\) is \((\epsilon,0,e^{-\alpha}/2+2\epsilon)\)-strongly-concentrated. A robust classifier \(f\) can be constructed following the proof of Theorem 3.1, and it obtains a robust accuracy \(R(f,\epsilon)\leq e^{-\alpha}/2+2\epsilon\). See Appendix D for more details._

We will now study a specific choice of \(p\) that generalizes Example 4.1 and will let us move beyond the above simple binary setting. Recall that we have a classification problem specified by a data distribution \(p\) over the data domain \(\mathcal{X}\times\mathcal{Y}=B(0,1)\times\{1,2,\ldots,K\}\). Firstly, the classes are balanced, _i.e._, \(p_{Y}(k)=1/K\) for all \(k\in\mathcal{Y}\). Secondly, the conditional density, i.e., \(q_{k}=p_{X|Y=k}\), is concentrated on the set \(S_{k}^{+\gamma}\cap\mathcal{X}\), where \(S_{k}\) is a low-dimensional linear subspace, and the superscript denotes an \(\gamma\)-expansion, for a small \(\gamma>0\).

For the purpose of building our robust classifier, we will assume access to a training dataset of \(M\)_clean_ data points \((s_{1},y_{1}),(s_{2},y_{2}),\ldots,(s_{M},y_{M})\), such that, for all \(i\), the point \(s_{i}\) lies exactly on one of the \(K\) low-dimensional linear subspaces. We will use the notation \(\mathbf{S}=[s_{1},s_{2},\ldots,s_{M}]\) for the training data matrix and \(\mathbf{y}=(y_{1},y_{2},\ldots,y_{M})\) for the training labels. We will assume that \(M\) is large enough that every \(x\in\cup_{k}S_{k}\) can be represented as a linear combination of the columns of \(\mathbf{S}\).

Now, the robust classification problem we aim to tackle is to obtain a predictor \(g\colon\mathcal{X}\to\mathcal{Y}\) which obtains a low robust risk, with respect to an additive adversary \(\mathcal{A}\) that we now define. For any data-point \(x\sim p\), \(\mathcal{A}\) will be constrained to make an additive perturbation \(v\) such that \(\mathrm{dist}_{\ell_{2}}(x+v,\cup_{i}S_{i})\leq\epsilon\). In other words, the attacked point can have \(\ell_{2}\) distance at most \(\epsilon\) from any of the linear

Figure 2: A plot of \(q_{1}\) (orange), \(q_{2}\) (violet) and the decision boundaries of \(f\) (dashed).

subspaces \(S_{1},\ldots,S_{k}\). Note that \(\mathcal{A}\) is more powerful than an \(\ell_{2}\)-bounded adversary as the norm of the perturbation \(\|v\|_{2}\) might be large, as \(v\) might be parallel to a subspace.

Under such an adversary \(\mathcal{A}\), given a (possibly adversarially perturbed) input \(x\), it makes sense to try to recover the corresponding point \(s\) lying on the union of subspaces, such that \(x=s+n\), such that \(\|n\|_{2}\leq\epsilon\). One way to do this is to represent \(s\) as a linear combination of a small number of columns of \(\mathbf{S}\), _i.e._, \(x=\mathbf{S}c+n\). This can be formulated as an optimization problem that minimizes the cardinality of \(c\), given by \(\|c\|_{0}\), subject to an approximation error constraint. Since such a problem is hard because of the \(\ell_{0}\) pseudo-norm, we relax this to the problem

\[\min_{c}\|c\|_{1}\ \ \text{s.t.}\ \ \|x-\mathbf{S}c\|_{2}\leq\epsilon.\] (3)

Under a suitable choice of \(\lambda\), this problem can be equivalently written as

\[\min_{c,e}\|c\|_{1}+\frac{\lambda}{2}\|e\|_{2}^{2}\ \ \text{s.t.}\ \ x=\mathbf{S}c+e,\] (4)

for which we can obtain the dual problem given by

\[\max_{d}\langle x,d\rangle-\frac{1}{2\lambda}\|d\|_{2}^{2}\ \ \text{s.t.}\ \ \|\mathbf{S}^{\top}d\|_{\infty}\leq 1.\] (5)

Our main observation is to leverage the stability of the set of active constraints of this dual to obtain a robust classifier. One can note that each constraint of Eq. (5) corresponds to one training data point \(s_{i}\) - when the \(i^{\mathrm{th}}\) constraint is active at optimality, \(s_{i}\) is being used to reconstruct \(x\). Intuitively, one should then somehow use the label \(y_{i}\) while predicting the label for \(x\). Indeed, we will show that predicting the majority label among the active \(y_{i}\) leads to a robust classifier.

We will firstly obtain a geometric characterization of the problem in Eq. (5) by viewing it as the evaluation of a projection operator onto a certain convex set, illustrated in Fig. 3. Observe that for \(\lambda>0\), the objective (5) is strongly concave in \(d\) and the problem has a unique solution, denoted by \(d_{\lambda}^{*}(x)\). It is not hard to show that this solution can be obtained by the projection operator

\[d_{\lambda}^{*}(x)=\left(\operatorname*{arg\,min}_{d}\|\lambda x-d\|_{2}\ \ \text{s.t.}\ \ \| \mathbf{S}^{\top}d\|_{\infty}\leq 1\right)=\operatorname{Proj}_{K^{\circ}}(\lambda x),\] (6)

where \(K^{\circ}\) is the polar of the convex hull of \(\pm\mathbf{S}\). Denoting \(\mathbf{T}=[\mathbf{S},-\mathbf{S}]\), we can rewrite Problem (6) as \(d_{\lambda}^{*}(x)=\left(\operatorname*{arg\,min}_{d}\|\lambda x-d\|_{2}\) sub. to \(\mathbf{T}^{\top}d\leq\mathbf{1}\right)\). We now define the set of active constraints as

\[A_{\lambda}(x)=\{t_{i}\colon\langle t_{i},d_{\lambda}^{*}(x)\rangle=1\}.\] (7)

**Geometry of the Dual (5).** It is illustrated in Fig. 3, where \(s_{1},s_{2}\) are two chosen data-points. The blue shaded polytope is \(K^{\circ}\). At \(\lambda=\lambda_{1}\), the point \(\lambda_{1}x\) lies in the interior of \(K^{\circ}\). Hence, \(A_{\lambda}(x)\) is empty and \(\operatorname{supp}(c^{*}(x))\) is also empty. As \(\lambda\) increases, a non-empty support is obtained for the first time at \(\lambda=1/\gamma_{K^{\circ}}(x)\). For all \(\lambda_{2}x\) in the red shaded polyhedron, the projection \(d_{\lambda_{2}}^{*}(x)=\operatorname{Proj}_{K^{\circ}}(\lambda_{2}x)\) lies on the face \(F\). As \(\lambda\) increases further we reach the green polyhedron. Further increases in \(\lambda\) do not change the dual solution, which will always remain at the vertex \(d_{\lambda_{3}}^{*}(x)\).

Geometrically, \(A_{\lambda}(x)\) identifies the face of \(K^{\circ}\) which contains the projection of \(\lambda x\), if \(A_{\lambda}(x)\) is non-empty (otherwise, \(\lambda x\) lies inside the polyhedron \(K^{\circ}\)). The support of the primal solution, \(c^{*}(x)\), is a subset of \(A_{\lambda}\), _i.e._\(\operatorname{supp}(c^{*}(x))\subseteq A_{\lambda}(x)\). Note that whenever two points, say \(x,x^{\prime}\), both lie in the same shaded polyhedron (red or green), their projections would lie on the same face of \(K^{\circ}\). We now show this formally, in the main theorem of this section.

**Theorem 4.1**.: _The set of active constraints \(A_{\lambda}\) defined in (7) is robust, i.e., \(A_{\lambda}(x^{\prime})=A_{\lambda}(x)\) for all \(\lambda x^{\prime}\in C(x)\), where \(C(x)\) is the polyhedron defined as_

\[C(x)=F(x)+V(x),\] (8)

Figure 3: Geometry of the dual problem (5). See description on the left.

_with \(F\subseteq K^{\circ}\) being a facet of the polyhedron \(K^{\circ}\) that \(x\) projects to, defined as_

\[F(x)=\left\{d\ \left|\begin{array}{l}t_{i}^{\top}d=1,\forall t_{i}\in A_{ \lambda}(x)\\ t_{i}^{\top}d<1,\mathrm{otherwise}\end{array}\right.\right\},\] (9)

_and \(V\) being the cone generated by the constraints active at (i.e., normal to) \(F\), defined as_

\[V(x)=\left\{\sum_{t_{i}\in A_{\lambda}(x)}\alpha_{i}t_{i}\colon\alpha_{i}\geq 0,\forall t_{i}\in A_{\lambda}(x)\right\}.\] (10)

The proof of Theorem 4.1 utilizes the geometry of the problem and properties of the projection operator, and is presented in Appendix E. We can now use this result to construct a robust classifier:

**Lemma 4.2**.: _Define the dual classifier as_

\[g_{\lambda}(x)=\textsc{Aggregate}(\{y_{i}\colon t_{i}\in A_{\lambda}(x)\}),\] (11)

_where Aggregate is any deterministic mapping from a set of labels to \(\mathcal{Y}\), e.g., Majority. Then, for all \(x^{\prime}\in C(x)\) as defined in Theorem 4.1, \(g_{\lambda}\) is certified to be robust, i.e., \(g_{\lambda}(x^{\prime})=g_{\lambda}(x)\)._

**Implications.** Having obtained a certifiably robust classifier \(g\), we pause to understand some implications of the theory developed so far. We observe that the certified regions in Theorem 4.1 are not spherical, _i.e._, the attacker can make additive perturbations having large \(\ell_{2}\) norm but still be unable to change the label predicted by \(g\) (see Fig. 4). This is in contrast to the \(\ell_{2}\) bounded certified regions that can be obtained by most existing work on certification schemes, and is a result of modelling data structure while constructing robust classifiers. Importantly, however, note that we do not assume that the attack is restricted to the subspace.

**Connections to Classical Results.** For \(\epsilon=0\), Eq. (3) is known as the primal form of the Basis Pursuit problem, and has been studied under a variety of conditions on \(\mathbf{S}\) in the sparse representation and subspace clustering literature [20, 13, 50, 64, 30, 25]. Given an optimal solution \(c^{*}(x)\) of this basis pursuit problem, how can we accurately predict the label \(y\)? One ideal situation could be that all columns in the support predict the same label, _i.e._, \(y_{i}\) is identical for all \(i\in\text{supp}(c^{*}(x))\). Indeed, this ideal case is well studied, and is ensured by necessary [25] and sufficient [50, 64, 30] conditions on the geometry of the subspaces \(S_{1},\ldots,S_{K}\). Another situation could be that _majority_ of the columns in the support predict the correct label. In this case, we could predict \(\textsc{Majority}(\{y_{i}\colon i\in\text{supp}(c^{*}(x))\})\) to ensure accurate prediction. Theorem 4.1 allows us to obtain robustness guarantees which work for _any_ such aggregation function which can determine a single label from the support. Hence, our results can guarantee robust prediction even when classical conditions are not satisfied. Lastly, note that our Theorem 4.1 shows that the entire active set remains unperturbed - In light of the recent results in [54], this could be relaxed for specific choices of maps acting on the estimated support.

## 5 Experiments

In this section, we will compare our certified defense derived in Section 4 to a popular defense technique called Randomized Smoothing (RS) [11], which can be used to obtain state-of-the-art certified robustness against \(\ell_{2}\) perturbations. RS transforms any given classifier \(f\colon\mathcal{X}\to\mathcal{Y}\) to a certifiably robust classifier \(g_{\sigma}^{\mathrm{RS}}\colon\mathcal{X}\to\mathcal{Y}\) by taking a majority vote over inputs perturbed by Gaussian6 noise \(\mathcal{N}(0,\sigma^{2}I)\), _i.e._,

Footnote 6: In reality, the choice of the noise distribution is central to determining the type of certificate one can obtain [41, 63], but Gaussian suffices for our purposes here.

\[g_{\sigma}^{\mathrm{RS}}(x)=\mathrm{Smooth}_{\sigma}(f)=\operatorname*{arg\, max}_{k\in\mathcal{Y}}\Pr_{v\sim\mathcal{N}(0,\sigma^{2}I)}(f(x+v)=k).\] (12)

Then, at any point \(x\), \(g_{\sigma}^{\mathrm{RS}}\) can be shown to be certifiably robust to \(\ell_{2}\) perturbations of size at least \(r^{\mathrm{RS}}(x)=\sigma\Phi^{-1}(p)\) where \(p=\max_{k\in\mathcal{Y}}\Pr_{v\sim\mathcal{N}(0,\sigma^{2}I)}(f(x+v)=k)\) denotes the maximum probability of any class under Gaussian noise.

It is not immediately obvious how to compare the certificates provided by our method described above and that of RS, since the sets of the space they certify are different. The certified region obtained by RS, \(C^{\mathrm{RS}}(x)=\{\bar{x}\colon\|x-\bar{x}\|_{2}\leq r(x)\}\), is a sphere (orange ball in Fig. 4). In contrast, our certificate \(C_{\lambda}(x)\) from Theorem 4.1 is a polyhedron (resp., blue trapezoid), which, in general, is neither contained in \(C^{\mathrm{RS}}(x)\), nor a superset of \(C^{\mathrm{RS}}(x)\). Additionally, our certificate has no standard notion of _size_, unlike other work on elliptical certificates [14], making a size comparison non-trivial. To overcome these difficulties, we will evaluate two notions of _attack size_: in the first, we will compare the \(\ell_{2}\) norms of successful attacks projected onto our polyhedron, and in the second, we will compare the minimum \(\ell_{2}\) norm required for a successful attack. We will then combine our method with RS to get the best of both worlds, _i.e._, the green shape in Fig. 4. In the following, we present both these evaluations on the MNIST [28] dataset, with each image normalized to unit \(\ell_{2}\) norm.

**Comparison along Projection on \(C_{\lambda}(x)\).** For the first case, we investigate the question: _Are there perturbations for which our method is certifiably correct, but Randomized Smoothing fails?_ For an input point \(x\), we can answer this question in the affirmative by obtaining an adversarial example \(\bar{x}\) for \(g^{\mathrm{RS}}\) such that \(\bar{x}\) lies inside our certified set \(C_{\lambda}(x)\). Then, this perturbation \(v=x-\bar{x}\) is certified by our method, but has \(\ell_{2}\) norm larger than \(r^{\mathrm{RS}}(x)\) (by definition of the RS certificate).

To obtain such adversarial examples, we first train a standard CNN classifier \(f\) for MNIST, and then use RS7 to obtain the classifier \(g^{\mathrm{RS}}_{\sigma}\). Then, for any \((x,y)\), we perform projected gradient descent to obtain \(\bar{x}=x^{T}\) by performing the following steps \(T\) times, starting with \(x^{0}\gets x\):

Footnote 7: Further details (e.g., smoothing parameters, certified accuracy computation) are provided in Appendix F.

\[\text{I. }x^{t}\leftarrow\mathrm{Proj}_{B_{\ell_{2}}(x,\epsilon)}\Big{(}x^{t-1}+ \eta\nabla_{x}\mathrm{Loss}(g^{\mathrm{RS}}_{\sigma}(x^{t}),y)\Big{)}\qquad \text{II. }x^{t}\leftarrow\mathrm{Proj}_{C_{\lambda}(x)}(x^{t})\] (13)

Unlike the standard PGD attack (step I), the additional projection (step II) is not straightforward, and requires us to solve a quadratic optimization problem, which can be found in Appendix F. We can now evaluate \(g^{\mathrm{RS}}_{\sigma}\) on these perturbations to empirically estimate the robust accuracy over \(C_{\lambda}\), _i.e._,

\[\mathrm{ProjectionRobustAcc}(\epsilon)=\Pr_{x,y\sim p_{\mathrm{MINST}}}\Big{(} \exists\bar{x}\in B_{\ell_{2}}(x,\epsilon)\cap C_{\lambda}(x)\text{ such that }g^{\mathrm{RS}}(\bar{x})\neq y\Big{)}.\]

The results are plotted in Fig. 5, as the dotted curves. We also plot the certified accuracies8 for comparison, as the solid curves. We see that the accuracy certified by RS drops below random chance (0.1) around \(\epsilon=0.06\) (solid red curve). Similar to other certified defenses, RS certifies only a subset of the true robust accuracy of a classifier in general. This true robust accuracy curve is pointwise upper-bounded by the empirical robust accuracy curve corresponding to any attack, obtained via the steps I, II described earlier (dotted red curve). We then see that even the upper-bound drops below random chance around \(\epsilon=0.4\), suggesting that this might be a large enough attack strength so that an adversary only constrained in \(\ell_{2}\) norm is able to fool a general classifier. However, we are evaluating attacks lying on our certified set and it is still possible to recover the true

Figure 4: Comparing polyhedral and spherical certificates. Details in text.

Figure 5: Comparing RS with Our method for adversarial perturbations computed by repeating Steps I, II (13).

class (blue solid curve), albeit by our specialized classifier \(g_{\lambda}\) suited to the data structure. Additionally, this suggests that our certified set contains useful class-specific information - this is indeed true, and we present some qualitative examples of images in our certified set in Appendix F. To summarize, we have numerically demonstrated that _exploiting data structure in classifier design leads to certified regions capturing class-relevant regions beyond \(\ell_{p}\)-balls_.

**Comparison along \(\ell_{2}\) balls.** For the second case, we ask the question: _Are there perturbations for which RS is certifiably correct, but our method is not?_ When an input point \(x\) has a large enough RS certificate \(r^{\mathrm{RS}}(x)\geq r_{0}\), some part of the sphere \(B_{\ell_{2}}(x,r^{\mathrm{RS}}(x))\) might lie outside our polyhedral certificate \(\mathcal{C}_{\lambda}(x)\) (blue region in Fig. 4). In theory, the minimum \(r_{0}\) required can be computed via an expensive optimization program that we specify in Appendix F. In practice, however, we use a black-box attack [9] to find such perturbations. We provide qualitative examples and additional experiments on CIFAR-10 in Appendix F.

**Combining Our Method with RS.** We now improve our certified regions using randomized smoothing. For this purpose, we treat our classifier \(g_{\lambda}\) as the base classifier \(f\) in (12), to obtain \(\mathrm{Smooth}_{\sigma}(\mathbf{g}_{\lambda})\) (abbreviated as \(\mathrm{Ours}_{\sigma}^{\mathrm{RS}}\)). We then plot the \(\ell_{2}\) certified accuracy\({}^{\prime}\)[11, Sec 3.2.2] in Fig. 6, where note that, as opposed to Fig. 5, the attacks are _not_ constrained to lie on our certificate anymore.

As a final remark, we note that our objective in Fig. 6 was simply to explore RS as a method for obtaining an \(\ell_{2}\) certificate for our method, and we did not tune our method or RS for performance. In particular, we believe that a wide array of tricks developed in the literature for improving RS performance [41; 45; 63] could be employed to improve the curves in Fig. 6. We now discuss our work in the context of existing literature in adversarial robustness, and sparse representation learning.

## 6 Discussion and Related Work

Our proof techniques utilize tools from high-dimensional probability, and have the same flavor as recent impossibility results for adversarial robustness [12; 48; 47]. Our geometric treatment of the dual optimization problem is similar to the literature on sparse-representation [13; 20] and subspace clustering [50; 51; 64; 25], which is concerned with the question of representing a point \(x\) by the linear combination of columns of a dictionary \(\mathbf{S}\) using sparse coefficients \(c\). As mentioned in Section 4, there exist geometric conditions on \(\mathbf{S}\) such that all such candidate vectors \(c\) are _subspace-preserving_, i.e., for all the indices \(i\) in the support of \(c\), it can be guaranteed that \(s_{i}\) belongs to the correct subspace. On the other hand, the question of classification of a point \(x\) in a union of subspaces given by the columns of \(\mathbf{S}\), or subspace classification, has also been studied extensively in classical sparse-representation literature [62; 61; 7; 27; 65; 65; 65; 39; 23; 32]. The predominant approach is to solve an \(\ell_{1}\) minimization problem to obtain coefficients \(c\) so that \(x=\mathbf{S}c+e\), and then predict the subspace that minimizes the representation error. Various _global_ conditions can be imposed on \(\mathbf{S}\) to guarantee the success of such an approach [62], and its generalizations [15; 16]. Our work differs from these approaches in that we aim to obtain conditions on perturbations to \(x\) that ensure accurate classification, and as such we base our robust classification decision upon properties of solutions of the dual of the \(\ell_{1}\) problem.

Our results are complementary to [44; 5], who obtain a lower bound on the robust risk \(R(f,\epsilon)\), in a binary classification setting, in terms of the Wasserstein distance \(D\) between the class conditionals \(q_{0}\) and \(q_{1}\), _i.e._, \(R(f,\epsilon)\geq 1-D(q_{0},q_{1})\). Using this result, [44; 5] roughly state that the robust risk increases as the class conditionals get closer, _i.e._, it becomes more likely to sample \(x\sim q_{0},x^{\prime}\sim q_{1}\), such that \(\|x-x^{\prime}\|\leq\epsilon\). In comparison, for two classes, our Theorem 3.1 roughly states that for low robust risk, \(q_{0}\) and \(q_{1}\) should be concentrated on small-volume subsets separated from one another. Thus, the message is similar, while our Theorem 3.1 is derived in a more general multi-class setting. Note that [44; 5] do not explicitly require small-volume subsets, but this is implicit, as \(1-D(q_{0},q_{1})\)

Figure 6: Comparing Certified Accuracy after combining our method with RS.

is large due to concentration of measure when \(q_{0},q_{1}\) do not concentrate on very small subsets. We provide an analogue of the empirical results of [44, 5] in Appendix F.6.

Our notion of concentration is also related to concentration of a measure \(\mu\), as considered in [35, 36, 70], which denotes how much the \(\mu\)-measure of any set _blows up_ after expansion by \(\epsilon\). Under this definition, the uniform measure has a high degree of concentration in high dimensions, and this is called the concentration of measure phenomenon. In contrast, our Definition 2.2 of concentrated data distributions can be seen as a _relative_ notion of concentration with respect to the uniform measure \(\mu\), in that we call a class conditional \(q\) concentrated when it assigns a large \(q\)-measure to a set of very small volume, _(i.e._, \(q(S)\) is high whereas \(\mu(S)\) is very low). In essence, Definition 2.2 defines concentration relative to the uniform distribution, whereas [35, 36, 70] define concentration independently. Definition 2.2 is useful in the context of adversarial robustness as it allows us to separate the concentration of data distributions (which is unknown) from the concentration of the uniform measure in high dimensions (for which there is a good understanding). This allows us to derive results in the non-realizable setting, where errors are measured against a probabilistic ground truth label \(Y\), which is strictly more general than the realizable setting which requires a deterministic ground truth classifier \(f^{*}\). As a result of this realizable setting, the analysis in [21, 35, 36, 70] needs to assume a non-empty error region \(A\) for the learnt classifier \(g\) with respect to \(f^{*}\), in order to reason about \(\epsilon\)-expansions \(A^{+\epsilon}\). The results in [35, 36] indicate that the robust risk grows quickly as the volume of \(A\) increases. However, humans seem to be a case where the natural accuracy is not perfect (e.g., we might be confused between a 6 and a poorly written \(5\) in MNIST), yet we seem to be very robust against small \(\ell_{2}\) perturbations. This points to a slack in the analysis in [35, 36], and our work fills this gap by considering \(\epsilon\) expansions of a different family of sets.

Finally, our work is also related to recent empirical work obtaining robust classifiers by _denoising_ a given input \(x\) of any adversarial corruptions, before passing it to a classifier [46, 38]. However, such approaches lack theoretical guarantees, and might be broken using specialized attacks [40]. Similarly, work on improving the robustness of deep network-based classifiers by adversarial training off the data-manifold can be seen as an empirical generalization of our attack model [24, 37, 69, 33]. More generally, it has been studied how adversarial examples relate to the underlying data-manifold [53, 26, 34]. Recent work also studies the robustness of classification using projections onto a single low-dimensional linear subspace [3, 2, 43]. The work in [3] studies an the attack model of bounded \(\ell_{2},\ell_{\infty}\) attacks, and they provide robustness certificates by obtaining guarantees on the distortion of a data-point \(x\) as it is projected onto a single linear subspace using a projection matrix \(\Pi\). In contrast, our work can be seen as projecting a perturbed point onto a union of multiple low-dimensional subspaces. The resultant richer geometry allows us to obtain more general certificates.

## 7 Conclusion and Future Work

To conclude, we studied conditions under which a robust classifier exists for a given classification task. We showed that concentration of the data distribution on small subsets of the input space is necessary for any classifier to be robust to small adversarial perturbations, and that a stronger notion of concentration is sufficient. We then studied a special concentrated data distribution, that of data distributed near low-dimensional linear subspaces. For this special case of our results, we constructed a provably robust classifier, and then experimentally evaluated its benefits w.r.t. known techniques.

For the above special case, we assume access to a _clean_ dataset \(\mathbf{S}\) lying perfectly on the union of low-dimensional linear subspaces, while in reality one might only have access to noisy samples. In light of existing results on noisy subspace clustering [59], an immediate future direction is to adapt our guarantees to support noise in the training data. Similarly, while the assumption of low-dimensional subspace structure in \(\mathbf{S}\) enables us to obtain novel unbounded robustness certificates, real world datasets might not satisfy this structure. We hope to mitigate this limitation by extending our formulation to handle data lying on a general image manifold in the future.

## Acknowledgments and Disclosure of Funding

We would like to thank Amitabh Basu for helpful insights into the optimization formulation for the largest \(\ell_{2}\) ball contained in a polyhedron. This work was supported by DARPA grant HR00112020010,NSF grants 1934979 and 2212457, and the NSF-Simons Research Collaboration on the Mathematical and Scientific Foundations of Deep Learning (NSF grant 2031985, Simons grant 814201).

## References

* [1] Anish Athalye, Nicholas Carlini, and David Wagner. Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples. In _International conference on machine learning_, pages 274-283. PMLR, 2018.
* [2] Pranjal Awasthi, Vaggos Chatziafratis, Xue Chen, and Aravindan Vijayaraghavan. Adversarially robust low dimensional representations. In _Conference on Learning Theory_, pages 237-325. PMLR, 2021.
* [3] Pranjal Awasthi, Himanshu Jain, Ankit Singh Rawat, and Aravindan Vijayaraghavan. Adversarial robustness via robust low rank representations. _Advances in Neural Information Processing Systems_, 33:11391-11403, 2020.
* [4] Amitabh Basu. Introduction to convexity. _Optimization_, 52:37, 2019.
* [5] Arjun Nitin Bhagoji, Daniel Cullina, and Prateek Mittal. Lower bounds on adversarial robustness from optimal transport. _Advances in Neural Information Processing Systems_, 32, 2019.
* [6] Sebastien Bubeck, Yin Tat Lee, Eric Price, and Ilya Razenshteyn. Adversarial examples from computational constraints. In _International Conference on Machine Learning_, pages 831-840. PMLR, 2019.
* [7] Raffaele Cappelli, Dario Maio, and Davide Maltoni. Subspace classification for face recognition. In _International Workshop on Biometric Authentication_, pages 133-141. Springer, 2002.
* [8] Nicholas Carlini, Anish Athalye, Nicolas Papernot, Wieland Brendel, Jonas Rauber, Dimitris Tsipras, Ian Goodfellow, Aleksander Madry, and Alexey Kurakin. On evaluating adversarial robustness. _arXiv preprint arXiv:1902.06705_, 2019.
* [9] Jianbo Chen, Michael I Jordan, and Martin J Wainwright. Hopskipjumpattack: A query-efficient decision-based attack. In _2020 ieee symposium on security and privacy (sp)_, pages 1277-1294. IEEE, 2020.
* [10] Ping-yeh Chiang, Renkun Ni, Ahmed Abdelkader, Chen Zhu, Christoph Studer, and Tom Goldstein. Certified defenses for adversarial patches. In _8th International Conference on Learning Representations (ICLR 2020)(virtual)_. International Conference on Learning Representations, 2020.
* [11] Jeremy Cohen, Elan Rosenfeld, and Zico Kolter. Certified adversarial robustness via randomized smoothing. In _International Conference on Machine Learning_, pages 1310-1320. PMLR, 2019.
* [12] Elvis Dohmatob. Generalized no free lunch theorem for adversarial robustness. In _International Conference on Machine Learning_, pages 1646-1654. PMLR, 2019.
* [13] David L Donoho and Michael Elad. Optimally sparse representation in general (nonorthogonal) dictionaries via l1 minimization. _Proceedings of the National Academy of Sciences_, 100(5):2197-2202, 2003.
* [14] Francisco Eiras, Motasem Alfarra, Philip Torr, M Pawan Kumar, Puneet K Dokania, Bernard Ghanem, and Adel Bibi. Ancer: Anisotropic certification via sample-wise volume maximization. _Transactions of Machine Learning Research_.
* [15] Ehsan Elhamifar and Rene Vidal. Robust classification using structured sparse representation. In _CVPR 2011_, pages 1873-1879. IEEE, 2011.
* [16] Ehsan Elhamifar and Rene Vidal. Block-sparse recovery via convex optimization. _IEEE Transactions on Signal Processing_, 60(8):4094-4107, 2012.

* [17] Gamaleldin Elsayed, Shreya Shankar, Brian Cheung, Nicolas Papernot, Alexey Kurakin, Ian Goodfellow, and Jascha Sohl-Dickstein. Adversarial examples that fool both computer vision and time-limited humans. _Advances in neural information processing systems_, 31, 2018.
* [18] Alhussein Fawzi, Hamza Fawzi, and Omar Fawzi. Adversarial vulnerability for any classifier. _Advances in neural information processing systems_, 31, 2018.
* [19] Marc Fischer, Maximilian Baader, and Martin Vechev. Certified defense to image transformations via randomized smoothing. _Advances in Neural Information Processing Systems_, 33:8404-8417, 2020.
* [20] Simon Foucart and Holger Rauhut. An invitation to compressive sensing. In _A mathematical introduction to compressive sensing_, pages 1-39. Springer, 2013.
* [21] Justin Gilmer, Luke Metz, Fartash Faghri, Samuel S Schoenholz, Maithra Raghu, Martin Wattenberg, Ian Goodfellow, and G Brain. The relationship between high-dimensional geometry and adversarial examples. _arXiv preprint arXiv:1801.02774_, 2018.
* [22] Chuan Guo, Mayank Rana, Moustapha Cisse, and Laurens van der Maaten. Countering adversarial images using input transformations. In _International Conference on Learning Representations_, 2018.
* [23] Kang-hua Hui, Chun-li Li, and Lei Zhang. Sparse neighbor representation for classification. _Pattern Recognition Letters_, 33(5):661-669, 2012.
* [24] Charles Jin and Martin Rinard. Manifold regularization for locally stable deep neural networks. _arXiv preprint arXiv:2003.04286_, 2020.
* [25] Mustafa D Kaba, Chong You, Daniel P Robinson, Enrique Mallada, and Rene Vidal. A nullspace property for subspace-preserving recovery. In Marina Meila and Tong Zhang, editors, _Proceedings of the 38th International Conference on Machine Learning_, volume 139 of _Proceedings of Machine Learning Research_, pages 5180-5188. PMLR, 18-24 Jul 2021.
* [26] Marc Khoury and Dylan Hadfield-Menell. On the geometry of adversarial examples. _arXiv preprint arXiv:1811.00525_, 2018.
* [27] Jorma Laaksonen et al. _Subspace classifiers in recognition of handwritten digits_. Helsinki University of Technology, 1997.
* [28] Yann LeCun. The mnist database of handwritten digits. _http://yann.lecun.com/exdb/mnist/_, 1998.
* [29] Alexander Levine and Soheil Feizi. (de) randomized smoothing for certifiable defense against patch attacks. _Advances in Neural Information Processing Systems_, 33:6465-6475, 2020.
* [30] Chun-Guang Li, Chong You, and Rene Vidal. On geometric analysis of affine sparse subspace clustering. _IEEE Journal of Selected Topics in Signal Processing_, 12(6):1520-1533, 2018.
* [31] Shengqiao Li. Concise formulas for the area and volume of a hyperspherical cap. _Asian Journal of Mathematics & Statistics_, 4(1):66-70, 2010.
* [32] Wei Li, Eric W Tramel, Saurabh Prasad, and James E Fowler. Nearest regularized subspace for hyperspectral classification. _IEEE Transactions on Geoscience and Remote Sensing_, 52(1):477-489, 2013.
* [33] Wei-An Lin, Chun Pong Lau, Alexander Levine, Rama Chellappa, and Soheil Feizi. Dual manifold adversarial robustness: Defense against lp and non-lp adversarial attacks. _Advances in Neural Information Processing Systems_, 33:3487-3498, 2020.
* [34] Xingjun Ma, Bo Li, Yisen Wang, Sarah M Erfani, Sudanthi Wijewickrema, Grant Schoenebeck, Dawn Song, Michael E Houle, and James Bailey. Characterizing adversarial subspaces using local intrinsic dimensionality. In _International Conference on Learning Representations_, 2018.

* [35] Saeed Mahloujifar, Dimitrios I Diochnos, and Mohammad Mahmoody. The curse of concentration in robust learning: Evasion and poisoning attacks from concentration of measure. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 33, pages 4536-4543, 2019.
* [36] Saeed Mahloujifar, Xiao Zhang, Mohammad Mahmoody, and David Evans. Empirically measuring concentration: Fundamental limits on intrinsic robustness. _Advances in Neural Information Processing Systems_, 32, 2019.
* [37] Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Jonathan Uesato, and Pascal Frossard. Robustness via curvature regularization, and vice versa. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 9078-9086, 2019.
* [38] Seyed-Mohsen Moosavi-Dezfooli, Ashish Shrivastava, and Oncel Tuzel. Divide, denoise, and defend against adversarial attacks. _arXiv preprint arXiv:1802.06806_, 2018.
* [39] Imran Naseem, Roberto Togneri, and Mohammed Bennamoun. Linear regression for face recognition. _IEEE transactions on pattern analysis and machine intelligence_, 32(11):2106-2112, 2010.
* [40] Zhonghan Niu, Zhaoxi Chen, Linyi Li, Yubin Yang, Bo Li, and Jinfeng Yi. On the limitations of denoising strategies as adversarial defenses. _arXiv preprint arXiv:2012.09384_, 2020.
* [41] Ambar Pal and Jeremias Sulam. Understanding noise-augmented training for randomized smoothing. _Transactions on Machine Learning Research_, 2023.
* [42] Nicolas Papernot, Patrick McDaniel, Xi Wu, Somesh Jha, and Ananthram Swami. Distillation as a defense to adversarial perturbations against deep neural networks. In _2016 IEEE symposium on security and privacy (SP)_, pages 582-597. IEEE, 2016.
* [43] Samuel Pfrommer, Brendon G Anderson, and Somayeh Sojoudi. Projected randomized smoothing for certified adversarial robustness. 2022.
* [44] Muni Sreenivas Pydi and Varun Jog. Adversarial risk via optimal transport and optimal couplings. In _International Conference on Machine Learning_, pages 7814-7823. PMLR, 2020.
* [45] Hadi Salman, Jerry Li, Ilya Razenshteyn, Pengchuan Zhang, Huan Zhang, Sebastien Bubeck, and Greg Yang. Provably robust deep learning via adversarially trained smoothed classifiers. _Advances in Neural Information Processing Systems_, 32, 2019.
* [46] Pouya Samangouei, Maya Kabkab, and Rama Chellappa. Defense-gan: Protecting classifiers against adversarial attacks using generative models. In _International Conference on Learning Representations_, 2018.
* [47] Ludwig Schmidt, Shibani Santurkar, Dimitris Tsipras, Kunal Talwar, and Aleksander Madry. Adversarially robust generalization requires more data. _Advances in neural information processing systems_, 31, 2018.
* [48] Ali Shafahi, W Ronny Huang, Christoph Studer, Soheil Feizi, and Tom Goldstein. Are adversarial examples inevitable? _arXiv preprint arXiv:1809.02104_, 2018.
* [49] Ali Shafahi, Mahyar Najibi, Mohammad Amin Ghiasi, Zheng Xu, John Dickerson, Christoph Studer, Larry S Davis, Gavin Taylor, and Tom Goldstein. Adversarial training for free! _Advances in Neural Information Processing Systems_, 32, 2019.
* [50] Mahdi Soltanolkotabi and Emmanuel J Candes. A geometric analysis of subspace clustering with outliers. _The Annals of Statistics_, 40(4):2195-2238, 2012.
* [51] Mahdi Soltanolkotabi, Ehsan Elhamifar, and Emmanuel J Candes. Robust subspace clustering. _The annals of Statistics_, 42(2):669-699, 2014.
* [52] Daniel A Spielman and Shang-Hua Teng. Smoothed analysis of algorithms: Why the simplex algorithm usually takes polynomial time. _Journal of the ACM (JACM)_, 51(3):385-463, 2004.

* [53] David Stutz, Matthias Hein, and Bernt Schiele. Disentangling adversarial robustness and generalization. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 6976-6987, 2019.
* [54] Jeremias Sulam, Ramchandran Muthukumar, and Raman Arora. Adversarial robustness of supervised sparse coding. _Advances in neural information processing systems_, 33:2110-2121, 2020.
* [55] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. _arXiv preprint arXiv:1312.6199_, 2013.
* [56] Dimitris Tsipras, Shibani Santurkar, Logan Engstrom, Alexander Turner, and Aleksander Madry. Robustness may be at odds with accuracy. _arXiv preprint arXiv:1805.12152_, 2018.
* [57] Rene Vidal, Yi Ma, S Shankar Sastry, Rene Vidal, Yi Ma, and S Shankar Sastry. _Generalized Principal Component Analysis_. Springer, 2016.
* [58] Vaclav Voracek and Matthias Hein. Provably adversarially robust nearest prototype classifiers. In _International Conference on Machine Learning_, pages 22361-22383. PMLR, 2022.
* [59] Yu-Xiang Wang and Huan Xu. Noisy sparse subspace clustering. In _International Conference on Machine Learning_, pages 89-97. PMLR, 2013.
* [60] Eric Wong, Leslie Rice, and J Zico Kolter. Fast is better than free: Revisiting adversarial training. In _International Conference on Learning Representations_, 2019.
* [61] John Wright, Yi Ma, Julien Mairal, Guillermo Sapiro, Thomas S Huang, and Shuicheng Yan. Sparse representation for computer vision and pattern recognition. _Proceedings of the IEEE_, 98(6):1031-1044, 2010.
* [62] John Wright, Allen Y Yang, Arvind Ganesh, S Shankar Sastry, and Yi Ma. Robust face recognition via sparse representation. _IEEE transactions on pattern analysis and machine intelligence_, 31(2):210-227, 2008.
* [63] Greg Yang, Tony Duan, J Edward Hu, Hadi Salman, Ilya Razenshteyn, and Jerry Li. Randomized smoothing of all shapes and sizes. In _International Conference on Machine Learning_, pages 10693-10705. PMLR, 2020.
* [64] Chong You and Rene Vidal. Geometric conditions for subspace-sparse recovery. In _International Conference on Machine Learning_, pages 1585-1593. PMLR, 2015.
* [65] Guoxian Yu, Guoji Zhang, Zili Zhang, Zhiwen Yu, and Lin Deng. Semi-supervised classification based on subspace sparse representation. _Knowledge and Information Systems_, 43(1):81-101, 2015.
* [66] Bohang Zhang, Du Jiang, Di He, and Liwei Wang. Rethinking lipschitz neural networks for certified l-infinity robustness. _arXiv preprint arXiv:2210.01787_, 2022.
* [67] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 586-595, 2018.
* [68] Shangzhi Zhang, Chong You, Rene Vidal, and Chun-Guang Li. Learning a self-expressive network for subspace clustering. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 12393-12403, 2021.
* [69] Shufei Zhang, Kaizhu Huang, Jianke Zhu, and Yang Liu. Manifold adversarial training for supervised and semi-supervised learning. _Neural Networks_, 140:282-293, 2021.
* [70] Xiao Zhang and David Evans. Understanding intrinsic robustness using label uncertainty. _arXiv preprint arXiv:2107.03250_, 2021.

Proof of Theorem 2.1

We will make the technical assumption in Section 2 precise. Recall that all quantities are normalized so that \(\mathcal{X}\) is an \(\ell_{2}\) ball of radius \(1\), i.e., \(\mathcal{X}=B_{\ell_{2}}(0,1)\). Recall that \(q_{k}\colon\mathcal{X}\to\mathbb{R}_{\geq 0}\) denotes the conditional distribution for class \(k\in\mathcal{Y}\), having support \(Q_{k}\). We will assume that there is a sufficient gap between the supports and the boundary of the domain, i.e., \(Q_{k}\subseteq B_{\ell_{2}}(0,c)\), and that the \(\ell_{2}\)-adversarial attack has strength \(\epsilon\leq c\), for some constant \(c\), say \(c=0.1\).

**Theorem 2.1**.: _If there exists an \((\epsilon,\delta)\)-robust classifier \(f\) for a data distribution \(p\), then at least one of the class conditionals \(q_{1},q_{2},\ldots,q_{K}\), say \(q_{\bar{k}}\), must be \((\bar{C},\epsilon,\delta)\)-concentrated. Further, if the classes are balanced, then all the class conditionals are \((C_{\max},\epsilon,K\delta)\)-concentrated. Here, \(\bar{C}=\mathrm{Vol}\{x\colon f(x)=\bar{k}\}\), and \(C_{\max}=\max_{k}\mathrm{Vol}\{x\colon f(x)=k\}\) are constants dependent on \(f\)._

Proof.: Note that the definition (1) of \((\epsilon,\delta)\)-robustness may be rewritten as

\[R(f,\epsilon) =\sum_{k}p_{X|Y}(X\text{admits an $\epsilon$-adversarial example}|Y=k)p_{Y}(y=k)\] \[=\sum_{k}q_{k}(\{x\in\mathcal{X}\colon\exists\bar{x}\in B(x, \epsilon)\text{ such that }f(\bar{x})\neq k\})p_{Y}(y=k),\] (14)

where \(q(S)\) denotes the \(q\)-measure of the set \(S\). We are given \(f\) such that \(R(f,\epsilon)\leq\delta\), and we want to find a set \(S\subseteq\mathcal{X}\) over which \(p\) concentrates.

\[R(f,\epsilon)\leq\delta\] \[\implies\sum_{k}q_{k}(\{x\in\mathcal{X}\colon\exists\bar{x}\in B (x,\epsilon)\cap\mathcal{X}\text{ such that }f(\bar{x})\neq k\})p_{Y}(y=k)\leq\delta\] (15)

Since \(\sum_{k}p_{Y}(y=k)=1\), the LHS in (15) is a convex combination, and we have

\[\implies\exists\hat{k}\;q_{\bar{k}}(\{x\in\mathcal{X}\colon \exists\bar{x}\in B(x,\epsilon)\cap\mathcal{X}\text{ such that }f(\bar{x})\neq\hat{k}\})\leq\delta,\] (16) \[\implies q_{\bar{k}}(U)\leq\delta,\text{ where }\] \[U=\{x\in\mathcal{X}\colon\exists\bar{x}\in B(x,\epsilon)\cap \mathcal{X}\text{ such that }f(\bar{x})\neq\hat{k}\}.\]

We will express \(q_{\bar{k}}(U)\) in terms of the classification regions of \(f\).

Let \(A\subseteq\mathcal{X}\) denote the region where the classifier predicts \(\hat{k}\), i.e. \(A=\{x\in\mathcal{X}\colon f(x)=\hat{k}\}\). Define \(A^{-\epsilon}\) to be the set of all points in \(A\) at a \(\ell_{2}\) distance atleast \(\epsilon\) from the boundary, i.e. \(A^{-\epsilon}=\{x\in A\colon B(x,\epsilon)\subseteq A\}\). Now consider any point \(x\in\mathcal{X}\). We have the following 4 mutually exclusive, and exhaustive cases:

1. \(x\) lies outside \(A\), i.e., \(x\not\in A\). In this case, \(x\in U\) trivially as \(f(x)\neq k\).
2. \(x\in A\), and the entire \(\epsilon\)-ball around \(x\) lies inside \(A\), i.e., \(B(x,\epsilon)\subseteq A\). In this case, every point in the \(\epsilon\)-ball around \(x\) is classified into class \(\hat{k}\), and \(x\) has no \(\epsilon\)-adversarial example for class \(\hat{k}\). Note that the set of all these points is \(A^{-\epsilon}\) by definition.
3. \(x\in A\), and some portion of the \(\epsilon\)-ball around \(x\) lies outside \(A\), i.e., \(B(x,\epsilon)\not\subseteq A\), and, there is a point \(\bar{x}\in B(x,\epsilon)\cap\mathcal{X}\) such that \(f(\bar{x})\neq k\). In this case, \(\bar{x}\) is an adversarial example for \(x\), and \(\bar{x}\in U\).
4. \(x\in A\), and some portion of the \(\epsilon\)-ball around \(x\) lies outside \(A\), i.e., \(B(x,\epsilon)\not\subseteq A\), _but there is no point \(\bar{x}\in B(x,\epsilon)\cap\mathcal{X}\) such that \(f(\bar{x})\neq k\)_. This can happen only when for all \(\bar{x}\in B(x,\epsilon)\setminus A\) we have \(\bar{x}\not\in\mathcal{X}\). In other words, any \(\epsilon\) perturbation that takes \(x\) outside \(A\), also takes it outside the domain \(\mathcal{X}\). This implies that \(B(x,\epsilon)\not\subseteq\mathcal{X}\), which in turn means that the distance of \(x\) from the boundary of \(\mathcal{X}\) is atmost \(\epsilon\).

We see that \(U\) comprises of the points covered in cases (1) and (3). Recall that we assumed that any point having distance less than \(0.1\) to the boundary of \(\mathcal{X}\) does not lie in the support of \(q_{\bar{k}}\). But all points covered in case (4) lie less than \(\epsilon\)-close to the boundary, and \(\epsilon\leq 0.1\). This implies that assigns \(0\) measure to the points covered in case (4). Together, cases (1), (3) and (4) comprise the set \((\mathcal{X}\setminus A)\cup(A\setminus A^{-\epsilon})\). Thus, we have shown

\[q_{\hat{k}}(U)=q_{\hat{k}}(\mathcal{X}\setminus A^{-\epsilon})\] \[\implies q_{\hat{k}}(A^{-\epsilon})\geq 1-\delta.\] (17)

Next, we will show that the set \(A^{-\epsilon}\) is exponentially small, by appealing to the Brunn-Minkowski inequality, which states that for compact sets \(E,F\subset\mathbb{R}^{n}\),

\[\operatorname{Vol}(E+F)^{\frac{1}{n}}\geq\operatorname{Vol}(E)^{\frac{1}{n}}+ \operatorname{Vol}(F)^{\frac{1}{n}},\]

where \(\operatorname{Vol}\) is the \(n\)-dimensional volume in \(\mathbb{R}^{n}\), and \(E+F\) denotes the minkowski sum of the sets \(E\) and \(F\). Applying the inequality with \(E=A^{-\epsilon}\) and \(F=B_{\ell_{2}}(0,\epsilon)\), we have

\[\operatorname{Vol}(A^{-\epsilon}+B_{\ell_{2}}(0,\epsilon))^{ \frac{1}{n}} \geq\operatorname{Vol}(A^{-\epsilon})^{\frac{1}{n}}+\operatorname{ Vol}(B_{\ell_{2}}(0,\epsilon))^{\frac{1}{n}}\] \[\implies\operatorname{Vol}(A)^{\frac{1}{n}} \geq\operatorname{Vol}(A^{-\epsilon})^{\frac{1}{n}}+\epsilon \operatorname{Vol}(B_{\ell_{2}}(0,1))^{\frac{1}{n}}\] \[\implies\operatorname{Vol}(A)^{\frac{1}{n}} \geq\operatorname{Vol}(A^{-\epsilon})^{\frac{1}{n}}+\epsilon \operatorname{Vol}(A)^{\frac{1}{n}}\] \[\implies\frac{\operatorname{Vol}(A^{-\epsilon})^{\frac{1}{n}}}{ \operatorname{Vol}(A)^{\frac{1}{n}}} \leq(1-\epsilon)\] \[\implies\operatorname{Vol}(A^{-\epsilon}) \leq\operatorname{Vol}(A)(1-\epsilon)^{n}\leq\operatorname{Vol}(A) \exp(-\epsilon n)\] \[\implies\operatorname{Vol}(A^{-\epsilon}) \leq\bar{C}\exp(-n\epsilon),\] (18)

where \(\bar{C}=\operatorname{Vol}(A)\). From (17) and (18), we see that \(q_{\bar{k}}\) is \((\bar{C},\epsilon,\delta)\)-concentrated. Additionally, if the classes are balanced, we can use the fact that every term of (15) can be atmost \(K\delta\), and apply the above reasoning for each class to conclude that each \(q_{k}\) is \((C_{\max},\epsilon,K\delta)\)-concentrated, for some suitable choice of \(C_{\max}\), e.g., \(C_{\max}=\max_{k}\operatorname{Vol}\{x\in\mathcal{X}\colon f(x)=k\}\). 

## Appendix B Proof of Theorem 3.1

**Theorem 3.1**.: _If the data distribution \(p\) is \((\epsilon,\delta,\gamma)\)-strongly-concentrated, then there exists an \((\epsilon,\delta+\gamma)\)-robust classifier for \(p\)._

Proof.: For each \(k\in\{1,2,\ldots,K\}\), let \(S_{k}\) be the support of the conditional density \(q_{k}\). Recall that \(S^{+\epsilon}\) is defined to be the \(\epsilon\)-expansion of the set \(S\). Define \(C_{k}\) to be the \(\epsilon\)-expanded version of the concentrated region \(S_{k}\) but removing the \(\epsilon\)-expanded version of all other regions \(S_{k^{\prime}}\), as

\[C_{k}=\left(S_{k}^{+\epsilon}\setminus\cup_{k^{\prime}\neq k}S_{k^{\prime}}^{ +\epsilon}\right)\cap\mathcal{X}.\]

We will use these regions to define our classifier \(f\colon\mathcal{X}\to\{1,2,\ldots,K\}\) as

\[f(x)=\begin{cases}1,&\text{ if }x\in C_{1}\\ 2,&\text{ if }x\in C_{2}\\ \vdots\\ K,&\text{ if }x\in C_{K}\\ 1,&\text{ otherwise}\end{cases}.\]

We will show that \(R(f,\epsilon)\leq\delta+\gamma\), which can be recalled to be

\[R(f,\epsilon)=\sum_{k}q_{k}(\{x\in\mathcal{X}\colon\exists\bar{x}\in B(x, \epsilon)\cap\mathcal{X}\text{ such that }f(\bar{x})\neq k\})p_{Y}(y=k).\]

In the above expression, the \(q_{k}\) mass is over the set of all points \(x\in\mathcal{X}\) that admit an \(\epsilon\)-adversarial example for the class \(k\), as

\[U_{k}=\{x\in\mathcal{X}\colon\exists\bar{x}\in B(x,\epsilon)\cap\mathcal{X} \text{ such that }f(\bar{x})\neq k\}.\] (19)

Define \(C_{k}^{-\epsilon}\) to be the set of points in \(C_{k}\) at a distance atleast \(\epsilon\) from the boundary of \(C_{k}\) as \(C_{k}^{-\epsilon}=\{x\in C_{k}\colon B(x,\epsilon)\subseteq C_{k}\}\). For any point \(x\in U_{k}\), we can find \(\bar{x}\in B(x,\epsilon)\) from (19) such that \(\bar{x}\not\in C_{k}\), showing that \(x\not\in C_{k}^{-\epsilon}\). Thus, \(U_{k}\) is a subset of the complement of \(C_{k}^{-\epsilon}\), i.e., \(U_{k}\subseteq\mathcal{X}\setminus C_{k}^{-\epsilon}\), and we have

\[R(f,\epsilon)=\sum_{k}q_{k}(U_{k})p_{Y}(y=k)\leq\sum_{k}(1-q_{k}(C_{k}^{- \epsilon}))p_{Y}(y=k).\]

Now we will need to show a few properties of the \(\epsilon\)-contraction. Firstly, for a set \(M=N\cap O\), we have \(M^{-\epsilon}=N^{-\epsilon}\cap O^{-\epsilon}\), which can be seen as

\[M^{-\epsilon} =\{x\colon x\in M,B(x,\epsilon)\subseteq M\}\] \[=\{x\colon x\in N,x\in O,B(x,\epsilon)\subseteq N,B(x,\epsilon) \subseteq O\}=N^{-\epsilon}\cap O^{-\epsilon}.\]

Secondly, for a set \(M=N^{c}\), where \(c\) denotes complement, we have \(M^{-\epsilon}=(N^{+\epsilon})^{c}\). This can be seen as

\[M^{-\epsilon} =\{x\colon x\in M,B(x,\epsilon)\subseteq M\}=\{x\colon x\not\in N,B(x,\epsilon)\subseteq N^{c}\}\] \[=\{x\colon x\not\in N,\forall x^{\prime}\in B(x,\epsilon)\,x^{ \prime}\not\in N\}\] \[=\{x\colon\forall x^{\prime}\in B(x,\epsilon)\,x^{\prime}\not\in N\}\] \[\implies(M^{-\epsilon})^{c} =\{x\colon\exists x^{\prime}\in B(x,\epsilon)\,x^{\prime}\in N\}\] \[=N^{+\epsilon}.\]

Thirdly, for a set \(M=N\setminus O\), we have \(M^{-\epsilon}=(N\cap O^{c})^{-\epsilon}=N^{-\epsilon}\cap(O^{c})^{-\epsilon}\) by the first property, and then \(N^{-\epsilon}\cap(O^{c})^{-\epsilon}=N^{-\epsilon}\cap(O^{+\epsilon})^{c}\) by the second property. This implies

\[(N\setminus O)^{-\epsilon}=N^{-\epsilon}\setminus O^{+\epsilon}.\]

Fourthly, for a set \(M=N\cup O\), we have \(M^{c}=N^{c}\cap O^{c}\). Taking \(\epsilon\)-contractions, and applying the first and second properties, we get \(M^{+\epsilon}=N^{+\epsilon}\cup O^{+\epsilon}\). Applying the above properties to \(C_{k}^{-\epsilon}\), we have

\[C_{k}^{-\epsilon} =\left(S_{k}^{+\epsilon}\setminus\cup_{k^{\prime}\neq k}S_{k^{ \prime}}^{+\epsilon}\right)^{-\epsilon}\cap\mathcal{X}^{-\epsilon}\] \[=\left(S_{k}\setminus\big{(}\cup_{k^{\prime}\neq k}S_{k^{\prime} }^{+\epsilon}\big{)}^{+\epsilon}\right)\cap\mathcal{X}^{-\epsilon}\] \[\supseteq\left(S_{k}\setminus\cup_{k^{\prime}\neq k}S_{k^{\prime} }^{+2\epsilon}\right)\cap\mathcal{X}^{-\epsilon}.\]

Recall that \(q_{k}(\mathcal{X}^{-\epsilon})=q(\mathcal{X})=1\) by the support assumption. Hence, we have

\[q_{k}(C_{k}^{-\epsilon})\geq q_{k}\left(S_{k}\setminus\cup_{k^{\prime}\neq k} S_{k^{\prime}}^{+2\epsilon}\right)=q_{k}(S_{k})-q_{k}\left(\cup_{k^{\prime} \neq k}S_{k^{\prime}}^{+2\epsilon}\right)\geq(1-\delta)-\gamma\]

\[\implies 1-q_{k}(C_{k}^{-\epsilon})\leq\delta+\gamma.\]

Finally, as \(\sum_{k}p_{Y}(y=k)=1\), we have \(R(f,\epsilon)\leq\delta+\gamma\) by convexity. 

## Appendix C Proofs for Example 3.1

Let \(\theta_{0}\leq\pi/2\). We will show that the following classifier \(f\) is robust in the setting of Example 3.1:

\[f(x)=\begin{cases}1,&\text{if }x^{\top}P\geq\theta_{0}\\ 2,&\text{otherwise}\end{cases}.\]

Let \(C_{1}=\{x\in\mathbb{S}^{n-1}\colon x^{\top}P\geq\theta_{0}\}\) be the set of points classified into class \(1\) by \(f\). Similarly, let \(C_{2}=\mathbb{S}^{n-1}\setminus C_{1}\) be the set of points classified into class \(2\). The robust risk of \(f\) (measured w.r.t. perturbations in the geodesic distance \(d\)) can be expanded as

\[R_{d}(f,\epsilon)=0.5q_{1}(d^{+\epsilon}(C_{2}))+0.5q_{2}(d^{+\epsilon}(C_{1})),\] (20)

where \(d^{+\epsilon}(S)\) denotes the \(\epsilon\)-expansion of the set \(S\) under the distance \(d\). Let \(\epsilon\leq\theta_{0}\) (otherwise, the first term is \(1/2\)). Recall that the class conditional \(q_{1}\) is defined as

\[q_{1}(x)=\begin{cases}\frac{1}{c_{\theta}}\frac{\psi(d(x,P))}{\sin^{n-2}d(x,P)},&\text{if }d(x,P)\leq\theta_{0}\\ 0,&\text{otherwise}\end{cases},\]where \(c_{\psi}\) is a normalizing constant which ensures that \(q_{1}\) integrates to \(1\), _i.e.,_\(c_{\psi}=\int_{\mathbb{S}^{n-1}}q_{1}\). Then, \(q_{2}\) was defined to be uniform over the complement of the support of \(q_{1}\), i.e. \(q_{2}=\operatorname{Unif}(\{x\in\mathbb{S}^{n-1}\colon d(x,P)>\theta_{0}\})\).

We can expand the first term of Eq. (20) as

\[q_{1}(d^{+\epsilon}(C_{2})) =q_{1}(d^{+\epsilon}(C_{2})\setminus C_{2})+q_{1}(C_{2})\] \[=q_{1}(\{x\colon 0<d(x,C_{2})\leq\epsilon\})+0.\]

Now for any \(x\not\in C_{2}\), we have \(d(x,C_{2})=\theta_{0}-d(x,P)\). Hence, \(\{x\colon 0<d(x,C_{2})\leq\epsilon\}=\{x\colon 0.1-\epsilon\leq d(x,P)< \theta_{0}\}\). Let \(\theta(x)\) denote the angle that \(x\) makes with \(P\). Then, the geodesic distance is \(d(x,P)=\theta(x)\). The earlier set is the same as the set of all points satisfying \(\theta_{0}-\epsilon\leq\theta(x)<1\). This is nothing but the \(\epsilon\)-base of the hyper-spherical cap having angle \(\theta_{0}\).

We continue,

\[q_{1}(\{x\colon\theta_{0}-\epsilon<d(x,P)\leq\theta_{0})=q_{1}(\{x\colon\theta _{0}-\epsilon\leq\theta(x)<\theta_{0}\})\]

With a change of variables, the above can be evaluated as

\[q_{1}(\{\theta_{0}-\epsilon\leq\theta(x)<\theta_{0}\}) =\frac{1}{c_{\psi}}\int_{\theta_{0}-\epsilon\leq\theta(x)<\theta _{0}}\frac{\psi(d(x,P))}{\sin(d(x,P))^{n-2}}dx\] \[=\frac{1}{c_{\psi}}\int_{\theta_{0}-\epsilon}^{\theta_{0}}\frac{ \psi(\theta)}{(\sin\theta)^{n-2}}\mu_{n-2}(\{x\in\mathbb{S}^{n-1}\colon\theta (x)=\theta\})d\theta,\]

where \(\mu_{n-2}\) is the \(n-2\)-dimensional volume (\(n-1\)-dimensional surface area). Now, \(\mu_{n-2}(\{x\in\mathbb{S}^{n-1}\colon\theta(x)=\theta\})\) is nothing but the volume of an slice of the hypersphere \(\mathbb{S}^{n-1}\). This slice is in itself a hypersphere in \(\mathbb{R}^{n-1}\), having radius \(\sin\theta\). Thus, its volume is the same as the volume of the \(\mathbb{S}^{n-2}\), scaled by \((\sin\theta)^{n-2}\). We continue,

\[\int_{\theta_{0}-\epsilon}^{\theta_{0}}\frac{\psi(\theta)}{(\sin \theta)^{n-2}}\mu_{n-2}(\{x\in\mathbb{S}^{n-1}\colon\theta(x)=\theta\})d\theta =\int_{\theta_{0}-\epsilon}^{\theta_{0}}\frac{\psi(\theta)}{( \sin\theta)^{n-2}}(\sin\theta)^{n-2}d\theta\] \[=\int_{\theta_{0}-\epsilon}^{\theta_{0}}\psi(\theta)d\theta\]

For illustration, we can take \(\psi(\theta)=1\), giving \(q_{1}(d^{+\epsilon}(C_{1}))=\epsilon/C\) from the above integral, with \(c_{\psi}=\int_{0}^{\theta_{0}}\psi(\theta)d\theta=\theta_{0}=\theta_{0}\).

We can now follow the same process as above, and expand the Eq. (20) as

\[q_{2}(d^{+\epsilon}(C_{1}))=\mu_{n-1}(\{x\colon\theta_{0}\leq\theta(x)\leq \theta_{0}+\epsilon\},\]

where \(\mu_{n-1}\) is again the \(n-1\)-dimensional volume. We use the following formula [31] for the surface area of a hyperspherical cap (valid when \(\alpha\ \leq\pi/2\)):

\[\mu_{n-1}(\{x\colon 0\leq\theta(x)\leq\alpha\})=m(\alpha)\stackrel{{ \mathrm{def}}}{{=}}0.5\mu_{n-1}(\mathbb{S}^{n-1})I_{\sin^{2}\alpha}\left(\frac {n-1}{2},\frac{1}{2}\right),\]

where \(I(\cdot,\cdot)\) is the incomplete regularized beta function. Letting \(d_{\theta_{0}}=m(\pi/2)+m(\pi/2)-m(\theta_{0})\), we continue,

\[q_{2}(d^{+\epsilon}(C_{1}))=\begin{cases}\frac{1}{d_{\theta_{0}}}\big{(}m( \theta_{0}+\epsilon)-m(\theta_{0})\big{)},&\text{if }\epsilon+\theta_{0}\leq\pi/2\\ \frac{1}{d_{\theta_{0}}}(m(\pi/2)+m(\pi/2)-m(\theta_{0}+\epsilon-\pi/2)\big{)},&\text{if }\pi/2\leq\epsilon+\theta_{0}\leq\pi\\ \frac{1}{d_{\theta_{0}}},&\text{else}\end{cases}.\]

The risk from class \(1\) dominates till \(\epsilon\) reaches around \(\pi/2\), then the risk from class \(2\) shoots up. We plot the resultant risk in Fig. 7 (left).

Having presented the basic construction, we now comment on how Example 3.1 can be generalized. Firstly, we can vary the support of \(q_{1}\), to have it cover a different fraction of the sphere. For instance, we can set \(\theta_{0}=\pi/2\) to get a modified \(q_{1}\) as

\[q_{1}^{\prime}(x)=\begin{cases}\frac{1}{C}\frac{1}{\sin^{n-2}d(x,P)},&\text{ if }d(x,P)\leq\pi/2\\ 0,&\text{otherwise}\end{cases},\]and let \(q_{2}^{\prime}\) be uniform over the complement of the support of \(q_{1}^{\prime}\). Along with balanced classes (_i.e._, \(P(Y=1)=P(Y=2)=1/2\)), this gives the data distribution \(p_{2}\). The robust classifier would now be given by the half-space \(\{x\colon d(x,P)\leq\pi/2\}\). The risk over \(p_{2}\) can be computed as earlier, and is plotted in Fig. 7 (middle).

We can take this example further, and make both \(q_{1}\) and \(q_{2}\) concentrated. This can be done, for instance, by setting \(q_{1}^{\prime\prime}=q_{1}\), and setting \(q_{2}^{\prime\prime}\) as follows:

\[q_{2}^{\prime\prime}(x)=\begin{cases}\frac{1}{C}\frac{1}{\sin^{n-2}d(x,-P)},& \text{ if }d(x,-P)\leq\pi/2\\ 0,&\text{ otherwise}\end{cases},\]

where \(-P\) is the antipodal point of \(P\). As both class conditionals are now concentrated, the halfspace separating their supports becomes quite robust. This can be seen in the Fig. 7 (right).

Lastly, \(\psi\) can be taken to be a rapidly decaying function for even greater concentration and lesser robust risk, e.g., \(\psi(\theta)=\exp(-\theta)\).

## Appendix D Proofs for Example 4.1

**Example 4.1**.: _The data domain is the ball \(B_{\ell_{\infty}}(0,1)\) equipped with the \(\ell_{2}\) distance. The label domain is \(\{1,2\}\). Subspace \(S_{1}\) is given by \(S_{1}=\{x\colon x^{\top}e_{1}=0\}\), and \(S_{2}\) is given by \(S_{2}=\{x\colon x^{\top}e_{2}=0\}\), where \(e_{1},e_{2}\) are the standard unit vectors. The conditional densities are defined as_

\[q_{1} =\operatorname{Unif}(\{x\colon\|x\|_{\infty}\leq 1,|x^{\top}e_{1 }|\leq e^{-\alpha}/2\}),\text{ and,}\] \[q_{2} =\operatorname{Unif}(\{x\colon\|x\|_{\infty}\leq 1,|x^{\top}e_{2 }|\leq e^{-\alpha}/2\}),\]

_where \(\alpha>0\) is a large constant. Finally, the classes are balanced, i.e., \(p_{Y}(1)=p_{Y}(2)=1/2\). With these parameters, \(q_{1},q_{2}\) are both \((0.5,\alpha/n-1,0)\)-concentrated over their respective supports. Additionally, \(p\) is \((\epsilon,0,e^{-\alpha}/2+2\epsilon)\)-strongly-concentrated. A robust classifier \(f\) can be constructed following the proof of Theorem 3.1, and it obtains a robust accuracy \(R(f,\epsilon)\leq e^{-\alpha}/2+2\epsilon\). See Appendix D for more details._

It would be helpful to have Fig. 8 in mind for what follows.

Figure 7: Robust risk for classifiers for concentrated distributions on the sphere \(\mathbb{S}^{n-1}\), for dimension \(n=100\). (Left): A plot of \(R_{d}(f,\epsilon)\) w.r.t \(p_{1}\). As \(\epsilon\) grows from \(0\) to \(0.1\), class \(1\) is the major contributor to the risk. At \(\epsilon=0.1\), all points in class \(1\) admit an adversarial perturbation, taking them to class \(2\). As \(\epsilon\) grows further, the risk for class \(2\) grows slowly, till \(\epsilon\) reaches close to \(\pi/2\), after which it blows up abruptly due to the high-dimensional fact that most of the mass of the uniform distribution lies near the equator of the sphere. (Middle): A plot of \(R_{d}(g,\epsilon)\) w.r.t. \(p_{2}\). As we make the support of the concentrated class larger, the distribution admits a classifier \(g\) with slightly improved robustness for smaller \(\epsilon\) values (\(\leq 0.1\)). (Right): A plot of \(R_{d}(g,\epsilon)\) w.r.t. \(p_{3}\): Finally, as both class conditionals are made concentrated, the classifier \(g\) becomes quite robust.

We will demonstrate that the classifier \(f\) illustrated in Fig. 8 has low robust risk. We define \(f\) for any \(x\in B_{\infty}(0,1)\), as follows (let \(\gamma=e^{-\alpha}\)):

\[f(x)=\begin{cases}1,&\text{if }|x_{1}|\leq\gamma/2+\epsilon,|x_{2}|\geq\gamma/2+ \epsilon\\ 2,&\text{if }|x_{2}|\leq\gamma/2+\epsilon,|x_{1}|\geq\gamma/2+\epsilon\\ 1,&\text{otherwise}\end{cases}.\]

The reason for splitting the cases for predicting \(1\) into two subcases, is that we will only need to analyse the first subcase and the second will not contribute to the robust risk.

Defining \(U_{1}\) to be the set of all points \(x\) which admit an adversarial example for the class \(1\), i.e. \(U_{1}=\{x\colon\exists\bar{x}\text{ such that }\|\bar{x}-x\|_{2}\leq\epsilon,f(\bar{x})=2\}\). It is clear that

\[U_{1}\subseteq\{x\colon|x_{1}|\geq\gamma/2\text{ or }|x_{2}|\leq\gamma/2+2 \epsilon\}.\]

\(U_{2}\) is defined analogously, and

\[U_{2}\subseteq\{x\colon|x_{2}|\geq\gamma/2\text{ or }|x_{1}|\leq\gamma/2+2 \epsilon\}.\]

Now, \(R(f,\epsilon)=0.5q_{1}(U_{1})+0.5q_{2}(U_{2})\). We look at the first term, and simplify an upper bound:

\[q_{1}(U_{1}) \leq q_{1}(\{x\colon|x_{1}|\geq\gamma/2\text{ or }|x_{2}|\leq\gamma/2+2\epsilon)\] \[\leq q_{1}(\{x\colon|x_{1}|\geq\gamma/2\})+q_{1}(\{x\colon|x_{2}| \leq\gamma/2+2\epsilon)\] \[=0+q_{1}(\{x\colon|x_{2}|\leq\gamma/2+2\epsilon\}).\] (21)

Now, recall that \(q_{1}=\mathrm{Unif}(\{x\colon|x_{1}|\leq\gamma/2\})\), hence the expression in (21) evaluates to

\[q_{1}(\{x\colon|x_{2}|\leq\gamma/2+2\epsilon\}) =\frac{1}{\mathrm{Vol}(\{x\colon|x_{1}|\leq\gamma/2\})}\mathrm{ Vol}(\{x\colon|x_{2}|\leq\gamma/2+2\epsilon,|x_{1}|\leq\gamma/2\})\] \[=\frac{1}{2^{n-1}\gamma}2^{n-2}\cdot(\gamma+4\epsilon)\cdot\gamma\] \[=0.5\gamma+2\epsilon\]

The situation for \(q_{2}\) is symmetric, and hence \(q_{2}(U_{2})=0.5(\gamma+4\epsilon)\). Adding them together, we see that the robust risk is upper bounded by

\[R(f,\epsilon)\leq 0.5(e^{-\alpha}+4\epsilon)\]

Figure 8: A plot of \(q_{1}\) (orange), \(q_{2}\) (violet) and the decision boundaries of \(f\) (dashed).

For the concentration parameters, note that the support of \(q_{1}\), _i.e._, \(S_{1}=\{x\colon|x_{1}|\leq\gamma/2,\|x\|_{\infty}\leq 1\}\) has volume \(\gamma 2^{n-1}=2^{n-1}\exp(-\alpha)=0.5\ 2^{n}\exp(-n(\alpha/n))\leq 0.5\exp(-n( \alpha/n-1))\). Hence, \(q_{1}\) is \((0.5,\alpha/n-1,0)\) concentrated over \(S_{1}\). Similarly, \(q_{2}\) is \((0.5,\alpha/n-1,0)\)-concentrated over \(S_{2}=\{x\colon|x_{2}|\leq\gamma/2,\|x\|_{\infty}\leq 1\}\). Finally,

\[q_{1}(S_{2}^{+2\epsilon}) =q_{1}(\{\|x\|_{\infty}\leq 1,\|x_{1}\|\leq\gamma/2,\|x_{2}\|\leq \gamma/2+2\epsilon\})\] \[=2^{n-2}\gamma(\gamma+4\epsilon)\gamma^{-1}2^{1-n}\] \[=0.5\gamma+2\epsilon\]

## Appendix E Proof of Theorem 4.1

**Theorem 4.1**.: _The set of active constraints \(A_{\lambda}\) defined in (7) is robust, i.e., \(A_{\lambda}(x^{\prime})=A_{\lambda}(x)\) for all \(\lambda x^{\prime}\in C(x)\), where \(C(x)\) is the polyhedron defined as_

\[C(x)=F(x)+V(x),\] (8)

_with \(F\subseteq K^{\circ}\) being a facet of the polyhedron \(K^{\circ}\) that \(x\) projects to, defined as_

\[F(x)=\left\{d\,\left|\,\begin{aligned} & t_{i}^{\top}d=1,\forall t_{i} \in A_{\lambda}(x)\\ & t_{i}^{\top}d<1,\mathrm{otherwise}\end{aligned}\right. \right\},\] (9)

_and \(V\) being the cone generated by the constraints active at (i.e., normal to) \(F\), defined as_

\[V(x)=\left\{\sum_{i_{i}\in A_{\lambda}(x)}\alpha_{i}t_{i}\colon \alpha_{i}\geq 0,\forall t_{i}\in A_{\lambda}(x)\right\}.\] (10)

Proof.: Let \(\lambda x^{\prime}\in C(x)\) as defined in (8). There exist \(f\in F(x),v\in V(x)\) such that \(\lambda x^{\prime}=f+v\). We will show that the projection of \(\lambda x^{\prime}\) onto \(F\) is given by \(f\). Recall that

\[A_{\lambda}(x^{\prime})=\{t_{i}\colon\langle t_{i},d_{\lambda}^{*}(x^{\prime} )\rangle=1\},\text{ where }d_{\lambda}^{*}(x^{\prime})=\mathrm{Proj}_{K^{\circ}}(\lambda x^{\prime}).\] (22)

Choose any \(z\in K^{\circ}\). Recall that \(\mathrm{Proj}_{K^{\circ}}(\lambda x^{\prime})=\min_{z\in K^{\circ}}\|z- \lambda x^{\prime}\|_{2}\). Consider the objective,

\[\|z-\lambda x^{\prime}\|_{2}^{2}=\|(z-f)-v\|_{2}^{2}=\|z-f\|_{2}^{2}+\|v\|_{2} ^{2}-2\langle z-f,v\rangle.\] (23)

We will show that \(\langle z-f,v\rangle\) is negative. Consider any \(t_{i}\in A_{\lambda}(x)\), and observe that \(\langle z,t_{i}\rangle\leq 1\), as \(z\in K^{\circ}\). However, from the definition of \(F(x)\) recall that \(\langle f,t_{i}\rangle=1\). This implies that \(\langle z-f,t_{i}\rangle\leq 0\). Using the fact that \(v\in V(x)\), we expand the inner product as

\[\langle z-f,v\rangle=\sum_{t_{i}\in A_{\lambda}(x)}\alpha_{i}\langle z-f,t_{i} \rangle\leq 0.\] (24)

Using (24) in (23) to obtain

\[\|z-\lambda x^{\prime}\|_{2}^{2}\geq\|z-f\|_{2}^{2}+\|v\|_{2}^{2}\geq 0.\] (25)

The minimizer above is obtained at \(z=f\in C(x)\), and hence we have shown that for all \(\lambda x^{\prime}\in C(x)\), \(\mathrm{Proj}_{K^{\circ}}(\lambda x^{\prime})=f\). Finally, \(d_{\lambda}^{*}(x^{\prime})=\mathrm{Proj}_{K^{\circ}}(\lambda x^{\prime})=f\in F (x)\). The theorem statement then follows from the definition of \(F(x)\). 

## Appendix F Further Experimental Details and Qualitative Examples

### Comparison along Projection on \(C_{\lambda}\)

In this section, we will provide more details on attacking a classifier with perturbations that lie on our certified set \(C_{\lambda}\).

Details of the Projection Step II in (13)Recall from Theorem 4.1 that \(C_{\lambda}(x)\) is defined as the Minkowski sum of the face \(F_{\lambda}(x)\) (9) and the cone \(V_{\lambda}(x)\) (10). Given an iterate \(x^{t}\), we can compute its projection onto \(C_{\lambda}(x^{0})\) by solving the following optimization problem

\[\operatorname{Proj}_{C_{\lambda}(x^{0})}(x^{t})=\operatorname*{arg\,min}_{x} \|x^{t}-x\|_{2}\text{ s.t. }x\in C_{\lambda}(x^{0})\]

Now every \(x\in C_{\lambda}(x^{0})\) can be written as \(x=d+v\) where \(d\in F(x^{0}),v\in V(x^{0})\). \(d\) is such that \(t^{\top}d=1\) for all \(t_{i}\in A_{\lambda}(x^{0})\), and \(t^{\top}d\leq 1\) otherwise. Then, \(v\) is such that \(v=\sum_{t_{i}\in A_{\lambda}(x^{0})}\alpha_{i}t_{i}\) for some \(\alpha_{i}\geq 0\). Recall that \(T=[t_{1},t_{2},\ldots,t_{2M}]\) denotes the matrix containing the training data-points as well as their integrations. Let the matrix \(A_{1}\in\mathbb{R}^{d\times|A_{\lambda}(x^{t})|}\) contain all the columns of \(T\) in \(A_{\lambda}(x^{t})\) and let \(A_{2}\in\mathbb{R}^{d\times(2M-|A_{\lambda}(x^{t})|)}\) contain all the remaining columns. Then the objective of the optimization problem above can be written as

\[\min_{\alpha,d}\left\|x^{t}-A_{1}\alpha-d\right\|_{2}^{2}\text{ s.t. }A_{1}^{\top}d=\mathbf{1},A_{2}^{\top}d\leq\mathbf{1},\alpha\geq \mathbf{0}\] (26)

(26) is a linearly constrained quadratic program, and can be solved by standard optimization tools. In particular, we use the qp solver from the python cvxopt library.

Visualization of the Certified SetIn Fig. 9, we visualize the set \(A_{\lambda}(x)\), _i.e._, the set of active constraints at the optimal solution of the dual problem, for several \(x\) in the MNIST test set. It can be seen that \(A_{\lambda}(x)\) contains images of the same digit as \(x\) in most cases. This is expected, as taking the majority label among \(A_{\lambda}(x)\) gives an accuracy of around \(97\%\) (solid blue curve in Fig. 5). Note that all these images \(t\in A_{\lambda}(x)\) are also contained in the certified set, \(C_{\lambda}(x)\), which can be seen as \(t=t+\mathbf{0}\), with \(t\in F_{\lambda}(x)\), and \(\mathbf{0}\in V_{\lambda}(x)\).

Figure 9: A visualization of the set of active constraints \(A_{\lambda}(x)\) for several \(x\). Atmost \(5\) members of \(A_{\lambda}(x)\) are shown for every \(x\).

Visualization of Attacks restricted to the Certified SetIn Fig. 10, we visualize the attacks restricted to our certified set \(C_{\lambda}\) computed by (13) for the classifier \(g_{0.02}^{\mathrm{RS}}\). We observe that we can identify the correct digit from most of the attacked images, but the randomized smoothing classifier is incorrect at higher \(\epsilon\). A small number of these attacked images are close to the actual class decision boundary, and the class is ambiguous. This is expected, both due to the inherent class ambiguity present in some MNIST images, as well as the large \(\epsilon\) we are attacking with. For all these images, the prediction of our classifier \(g_{\lambda}\) is certified to be accurate. For comparison, the RS certificate is unable to certify anything beyond \(\epsilon\geq 0.06\).

### Comparison along \(\ell_{2}\) balls

Exact Certification for \(g_{\lambda}\)In this section, we first state and prove Theorem 4.1 that allows us to exactly compute the \(\ell_{2}\) certified radius for our classifier \(g_{\lambda}\) at any point \(x\).

**Lemma F.1**.: _For all \(x^{\prime}\in C(x)\) as defined in Theorem 4.1, we have \(g_{\lambda}(x^{\prime})=g_{\lambda}(x)\). Additionally, for all \(\|v\|_{2}\leq r_{0}(x)\) we have \(g_{\lambda}(x+v)=g_{\lambda}(x)\), where_

\[\begin{split} r_{0}(x)=&\min_{\mathbf{u}}& -\langle x,\mathbf{u}\rangle\\ &\text{sub. to}&\|\mathbf{u}\|_{2}=1,\quad\langle \mathbf{u},t_{i}\rangle\leq 0\ \forall t_{i}\in A_{\lambda}(x),\quad\langle\mathbf{u},v\rangle\leq 1 \ \forall v\in\mathrm{ext}(F(x)),\end{split}\] (27)

_where \(\mathrm{ext}(F(x))\) is the set of extreme points of the polyhedron \(F(x)\)._

DiscussionBefore presenting the proof, let us ponder over the result. We see that (27) involves solving an optimization problem having as many constraints as the number of extreme points of the polyhedron \(F(x)\). In general, this can be very large in high dimensions, and hence computationally inefficient to compute. As a result, while Lemma F.1 provides an exact certificate in theory, it is hard to use in practice without further approximations.

Proof.: Recall that given a set \(A_{\lambda}(x)\), the polyhedron \(C(x)\) is defined in (8) as

\[C(x)=F(x)+V(x),\] (28)

where \(F(x)\) is a polyhedron, and \(V(x)\) is a cone given as the conic hull of \(A_{\lambda}(x)\), _i.e._, \(V(x)=\text{cone}(A_{\lambda}(x))\). We want to find the size \(r(x)\) of the largest \(\ell_{2}\) ball centered at \(x\), _i.e._, \(B(x,r(x))\), that can be inscribed within \(C(x)\). From convex geometry [4], we know that any polyhedron can be described as an intersection over halfspaces obtained from its polar, \(C^{\circ}\) as

\[C(x)=\cap_{\mathbf{u}\in C^{\circ}(x)}H^{-}(\mathbf{u},0),\] (29)

Figure 10: A visualization of attacks on the RS classifier \(g_{0.02}^{\mathrm{RS}}\) restricted to our certified set \(C_{\lambda}\) obtained by (13). Different rows plot different attack strengths \(\epsilon\). Whenever an image is misclassified, the red annotation on the top left shows the class predicted by \(g_{0.02}^{\mathrm{RS}}\).

where \(H^{-}(\mathbf{u},0)=\{x\colon\langle\mathbf{u},x\rangle\leq 0\}\). The distance of \(x\) from the boundary of \(C(x)\) is the same as the smallest distance of \(x\) from any halfspace in (29). However, this distance is simply

\[\text{dist}(x,H^{-}(\mathbf{u},0))=-\left\langle x,\frac{\mathbf{u}}{\| \mathbf{u}\|_{2}}\right\rangle.\] (30)

In other words, we can expressing \(C(x)\) as the intersection over halfspaces whose normals lie in \(C^{\circ}(x)\), _i.e._,

\[r(x)=\left(\min_{\mathbf{u}}-\langle x,\mathbf{u}\rangle,\text{ sub. to }\|\mathbf{u}\|_{2}=1,\mathbf{u}\in C^{\circ}(x)\right).\] (31)

All that is left is to obtain a description of \(C^{\circ}(x)\). This can be done by first expressing \(C(x)\) in a standard form, as

\[C(x)=\text{conv}(\text{ext}(F(x)))+\text{cone}(A_{\lambda}(x)),\] (32)

where \(\text{ext}(F(x))\) denotes the extreme points of the polyhedron \(F(x)\), \(\text{conv}(\cdot)\) denotes the convex hull and \(\text{cone}(\cdot)\) denotes the conic hull. Now we can apply a theorem in convex geometry to obtain the polar ([4, Th. 2.79]) as

\[C^{\circ}(x)=\left\{\mathbf{u}\colon\begin{array}{l}\langle\mathbf{u},t_{i} \rangle\leq 0\ \forall t_{i}\in A_{\lambda}(x)\\ \langle\mathbf{u},v\rangle\leq 1\ \forall v\in\text{ext}(F(x))\end{array}\right\}\] (33)

Combining (31) and (33), we obtain the lemma statement. Finally, we note that as Theorem 4.1 shows that \(A_{\lambda}(x^{\prime})=A_{\lambda}(x)\) for all \(\lambda x^{\prime}\in C(x)\), and the classifier \(g_{\lambda}(x)\) is purely a function of \(A_{\lambda}(x)\), we have that \(g_{\lambda}(x^{\prime})=g_{\lambda}(x)\) for all \(\lambda x^{\prime}\in C(x)\). 

Black Box Attacks on \(g_{\lambda}\)Since our classifier \(g_{\lambda}\) is not explicitly differentiable with respect to its input, we use the HopSkipJump [9] black-box attack to obtain adversarial perturbations that cause a misclassification. The obtained attacked images, and the pertubation magnitude are shown in Fig. 11.

### Details of Randomized Smoothing Certificates

For obtaining the certified accuracy curves using Randomized Smoothing [11] (solid lines in Figs. 5 and 6), we follow the certification and prediction algorithms for RS in [11, Sec 3.2.2]: given a base classifier \(f\) and a test image \(x\), we use \(n_{0}=100\) samples from an isotropic gaussian distribution \(\mathcal{N}(0,\sigma)\) to prediction the majority class \(g_{\sigma}^{\text{RS}}(x)\). We then use \(n=100\) samples to estimate the prediction probability \(\underline{p}_{A}\) under \(\mathcal{N}(0,\sigma)\) with confidence atleast \(0.999\). If \(\underline{p}_{A}\) is at least \(0.5\), we report the certified radius \(\sigma\Phi^{-1}(\underline{p}_{A})\). Otherwise, if \(\underline{p}_{A}<0.5\), we abstain and return a certified radius of \(0\).

Evaluating the Dual Classifier \(g_{\lambda}\)We normalize each image in the MNIST dataset to have unit \(\ell_{2}\) norm, and resize to \(32\times 32\). We pick \(10000\) images \(s_{i}\) at random from the training set to obtain our data matrix \(\mathbf{S}\in\mathbb{R}^{1024\times 10000}\). We now pick a test point \(x\), and then solve the dual problem (5) with \(\lambda=2\), to obtain the dual solution \(d_{\lambda}^{*}(x)\). This allows us to then obtain the set of active constraints \(A_{\lambda}(x)\) in (7). Then, we use the majority rule as the aggregate function in (11) to obtain the classfication output from the dual classifier \(g_{\lambda}(x)\). We sample \(100\) and \(500\) images \(x\) uniformly at random from the MNIST test set for generating the curves in Figs. 5 and 6, respectively. All experiments are performed on a NVIDIA GeForce RTX 2080 Ti GPU with 12 GB memory.

Figure 11: A visualization of attacks on our classifier \(g_{\lambda}\) obtained by the HopSkipJump black box attack [9]. The label predicted by \(g_{\lambda}\) is shown as a red annotation on the top left. The \(\ell_{2}\) perturbation magnitude \(\epsilon\) is shown at the bottom. Note that the \(\epsilon\) values are not nice fractions because they are set by the HopSkipJump attack using a binary search to find the minimum attack strength that causes a misclassification.

### Discussion of Computational Cost

The computational complexity of obtaining the certificate \(C(x)\) in Theorem 4.1 is dominated by solving the linear optimization problem (6) to obtain the set \(A_{\lambda}\), which has \(n+M\) variables. It is known that in practice, the cost of solving LPs is much lower than the worst case [52], and it takes us \(11.7\) seconds on average on a single CPU for each image \(x\) without parallel processing.

For comparison to other certified defenses (like Randomized Smoothing), we perform \(T=20\) steps of (13), and each Step II of (13) requires solving a quadratic optimization program, given in (26). This is a linearly constrained quadratic program. In practice it takes us \(18.1\) seconds on average on a single CPU for each \(x\) without parallel processing.

### Experiments on CIFAR-10

We provide experiments for our method applied to CIFAR-10. To do this, we first embed each CIFAR-10 image into a feature space, designed such that \(\ell_{2}\)-bounded perturbations in the feature space, i.e., \(\phi(x+v)\), \(\|v\|_{2}\leq\epsilon\), correspond to semantic perturbations in the input space, e.g., distorting the image. Such a feature space is obtained following recent popular work in learning perceptual metrics in vision [67], where the task is precisely to learn a feature representation where \(\ell_{2}\) distances align with human perception. We now use our subspace model on \(\phi(X)\), and perform exactly the experiments in Section 5, and make similar observations: we obtain reasonable robust accuracy using our method (Fig. 12 blue lines), and this accuracy is maintained within our certified polyhedra at high \(\epsilon\), where existing defenses are not robust.

Though such experiments showing feature space certificates have appeared in the literature [58], the question of whether the perceptual representation is itself susceptible to small adversarial perturbations still remains for future exploration.

### Evaluating Theorem 3.1 on Empirical Distributions

The empirical results in [44, 5] create an empirical distribution \(\hat{p}\) by selecting two classes from CIFAR-10, which simply places probability mass \(1/N\) on each of the \(N\) samples in the dataset. [44, 5] evaluate their lower bounds on \(\hat{p}\), and not on the actual real world distribution \(p\), which might be arbitrarily complex. Hence, it is unclear whether the trends observed hold for \(p\). Nevertheless, the

Figure 12: Comparing Randomized Smoothing (RS) with Our Method from Section 4, for adversarial perturbations computed by repeating Steps I, II from Eq. (13). This figure is the counterpart of Fig. 5, where the MNIST dataset has been changed to \(\phi(\text{CIFAR-10})\), and the transformation \(\phi\) is given by (left) \(384\)-dimensional slice of LPIPS [67] features (i.e., output of the third Conv2D layer) and (right) \(128\)-dimensional input features used by SENet [68]. Observation 1: Our method retains robustness even when the dashed empirical upper bound for robust accuracy for the RS classifier drops to random chance. Observation 2: Our robust accuracy (solid blue line) gets higher from left to right, as the feature space of SENet is closer to a union of subspaces than LPIPS. On the other hand, the left feature space aligns better with human perception than the right feature space.

same experiment can be translated to our setting for any real world dataset, our Theorem 3.1 would then show the existence of a robust classifier for \(\hat{p}\). We would need to solve a discrete optimization problem for finding the concentrated sets in Theorem 3.1. Even though valid only for \(\hat{p}\), this analysis is interesting, and is reported in Fig. 13.

## Appendix G Discussion on Strong Concentration

Are Natural Image Distributions Strongly Concentrated?Consider the task of classifying images of dogs vs cats. Any small \(\ell_{2}\) perturbation (say of size \(\epsilon=0.1\)) to an image of a dog is not likely to change it to an image of a cat - the true label (or a human decision) does not change with small perturbations. Let \(S_{\mathrm{cat}}\) be the set of images that are cats with a high confidence, and similarly for \(S_{\mathrm{dog}}\). What we just said was that any image in the \(\epsilon\)-expansion of \(S_{\mathrm{cat}}\) is not likely to be a dog, and vice versa. This is precisely the condition of separation in the definition of strong concentration: \(q_{\mathrm{dog}}(S_{\mathrm{cat}}^{+\epsilon})\leq\gamma\), and \(q_{\mathrm{cat}}(S_{dog}^{+\epsilon})\leq\gamma\), for a small \(\gamma\).

Now, despite the above, the task of practically classifying an image into a dog or a cat is not trivial at all, as the sets \(S_{\mathrm{dog}}\) and \(S_{\mathrm{cat}}\) might be very complex, and hence it might be computationally hard to find a predictor (neural network, or any other classifier) for distinguishing \(S_{\mathrm{dog}}\) and \(S_{\mathrm{cat}}\). Using modern deep learning, one is able to learn an approximation to these sets that leads to small standard error, but this approximation is still bad enough that the learned predictor is very susceptible to adversarial examples. This is to say that task of robustly classifying an image into dog or cat is even harder.

Tightness of Strong ConcentrationIn Definition 3.1, we are trying to obtain a sufficient condition for the existence of a robust classifier. There are two parts to Definition 3.1: the concentration, and the separation conditions. The first is essential, as Theorem 2.1 asserts that whenever a robust classifier exists for a data distribution, the class conditionals are concentrated (i.e., it is necessary).

As we argued above, the separation condition is not very strong, and should be satisfied by image distributions like MNIST, CIFAR-10 and ImageNet. Let us walk through this condition to see if it can be further relaxed. As we note at the start of Section 3, if all the class conditionals were to be concentrated on subsets \(S_{k}\) that have high intersection among each other, then even benign classification would be hard (i.e., benign risk will be high), let alone robust classification. So, even for the existence of a good benign classifier, it is essential for \(S_{k}\cap S_{k^{\prime}}\) to be close to empty, for all \(k\neq k^{\prime}\).

Figure 13: Instantiating Theorem 3.1 for the CIFAR-10 empirical distribution. We construct the empirical distribution \(\hat{p}\) which assigns equal probability to each \((x,y)\) pair in the CIFAR-10 test set. This leads to empirical class conditionals \(\hat{q}_{1},\dots,\hat{q}_{10}\), corresponding to the \(10\) classes. We set \(\gamma=0\) in Theorem 3.1, and greedily construct the sets \(S_{1},\dots,S_{10}\): sort all images in decreasing order of their distance to their nearest neighbor in the test set, pick the first \(m\) images, and then compute the concentration parameters (minimum separation \(\epsilon_{m}\) and mass \(\delta_{m}=1-\min_{i}\hat{q}_{i}(S_{i})\)). We then plot \((\epsilon_{m},1-\delta_{m})\) for several choices of \(m\). Applying Theorem 3.1, each \((\epsilon_{m},\delta_{m})\) guarantees the existence of a \((\epsilon_{m},\delta_{m})\)-robust classifier for \(\hat{p}\).

Now, even if \(S_{k}\cap S_{k^{\prime}}\) were almost empty, for a robust classifier, we care about the \(\epsilon\)-expansions of these sets to not intersect. In other words, we do not want an \(\epsilon\)-perturbed cat to look like a dog. Hence, for the existence of a good robust classifier, \(S_{k}\cap S_{k^{\prime}}^{+\epsilon}\) should be close to empty, for all \(k\neq k^{\prime}\). This can be generalized in measure terms, to require \(q_{k}(S_{k^{\prime}}^{+\epsilon})\leq\gamma\), for a small \(\gamma\). Upto this, all these conditions are essential for obtaining a robust classifier.

Now, note that in Definition 3.1, the expansion is taken for \(2\epsilon\), instead of \(\epsilon\). This is where our proof technique for Theorem 3.1 incurs a slack of \(\epsilon\), and we believe a different approach for constructing the robust classifier might be able to reduce the expansion requirement to \(\epsilon\). This could be an interesting future avenue for relaxing Definition 3.1 and thereby strengthening Theorem 3.1.