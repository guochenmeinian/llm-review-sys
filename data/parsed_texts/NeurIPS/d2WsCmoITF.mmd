# Unbalanced Low-rank Optimal Transport Solvers

 Meyer Scetbon\({}^{*}\)

Microsoft Research

t-mscetbon@microsoft.com

&Michael Klein\({}^{*}\)

Apple

michalk@apple.com

Giovanni Palla

Helmholtz Center Munich

giovanni.palla@helmholtz-muenchen.de

&Marco Cuturi

Apple

cuturi@apple.com

###### Abstract

Two salient limitations have long hindered the relevance of optimal transport methods to machine learning. First, the \(O(n^{3})\) computational cost of standard sample-based solvers (when used on batches of \(n\) samples) is prohibitive. Second, the mass conservation constraint makes OT solvers too rigid in practice: because they must match _all_ points from both measures, their output can be heavily influenced by outliers. A flurry of recent works has addressed these computational and modeling limitations, but has resulted in two separate strains of methods: While the computational outlook was much improved by entropic regularization, more recent \(O(n)\) linear-time _low-rank_ solvers hold the promise to scale up OT further. In terms of modeling flexibility, the rigidity of mass conservation has been eased for entropic regularized OT, thanks to unbalanced variants of OT that can penalize couplings whose marginals deviate from those specified by the source and target distributions. The goal of this paper is to merge these two strains, low-rank and unbalanced, to achieve the promise of solvers that are _both_ scalable and versatile. We propose custom algorithms to implement these extensions for the linear OT problem and its fused-Gromov-Wasserstein generalization, and demonstrate their practical relevance to challenging spatial transcriptomics matching problems. These algorithms are implemented in the ott-jax toolbox (Cuturi et al., 2022).

## 1 Introduction

Recent machine learning (ML) works have witnessed a flurry of activity around optimal transport (OT) methods. The OT toolbox provides convenient, intuitive and versatile ways to quantify the difference between two probability measures, either to quantify a distance (the Wasserstein and Gromov-Wasserstein distances), or, in more elaborate scenarios, by computing a push-forward map that can transform one measure into the other (Peyre and Cuturi, 2019). Recent examples include, e.g., single-cell omics (Bunne et al., 2021, 2022; Demetci et al., 2020; Nitzan et al., 2019; Cang et al., 2023; Klein et al., 2023), attention mechanisms (Tay et al., 2020; Sander et al., 2022), self-supervised learning(Caron et al., 2020; Oquab et al., 2023), and learning on graphs (Vincent-Cueaz et al., 2023).

**On the challenges of using OT.** Despite their long history in ML (Rubner et al., 2000), OT methods have long suffered from various limitations, that arise from their statistical, computational, and modelling aspects. The _statistical_ argument is commonly referred to as the curse-of-dimensionality of OT estimators: the Wasserstein distance between two probability densities, and its associated optimal Monge map, is poorly approximated using samples as the dimension \(d\) of observation grows (Dudley et al., 1966; Boissard and Le Gouic, 2014). On the _computational_ side, computing OT between a pair of \(n\) samples involves solving a (generalized) matching problem, with a price of \(O(n^{3})\) and above (Kuhn, 1955; Ahuja et al., 1993). Finally, the original _model_ for OT rests on amass conservation constraint: all observations from either samples must be accounted for, including outliers that are prevalent in machine learning datasets. Combined, these weaknesses have long hindered the use of OT, until a more recent generation of solvers addressed these three crucial issues.

**The Entropic Success Story.** The winning approach, so far, to carry out that agenda has been entropic regularization methods (Cuturi, 2013). The computational virtues of the Sinkhorn algorithm when solving OT (Altschuler et al., 2017; Peyre et al., 2016; Solomon et al., 2016; Le et al., 2021) come with statistical efficiency (Genevay et al., 2019; Mena and Niles-Weed, 2019; Chizat et al., 2020), and can also be seamlessly combined with _unbalanced_ formulations by penalizing - rather than constraint - mass conservation, both for the linear (Frogner et al., 2015; Chizat et al., 2018; Sjoumre et al., 2022; Fatras et al., 2021; Pham et al., 2020) and quadratic (Sejourne et al., 2021) problems. These developments have all been implemented in popular OT packages (Feydy et al., 2019; Flamary et al., 2021; Cuturi et al., 2022).

**The Low-Rank Alternative.** A recent strain of solvers relies instead on _low-rank_ (LR) properties of cost and coupling matrices (Forrow et al., 2018; Sceton and Cuturi, 2020; Sceton et al., 2021). Much like entropic solvers, these LR solvers have a better statistical outlook (Sceton and Cuturi, 2022) and extend to GW problems (Sceton et al., 2022). In stark contrast to entropic solvers, however, LR solvers benefit from linear complexity \(O(nrd)\) w.r.t sample size \(n\) (using rank \(r\) and cost dimension \(d\)) that can scale to ambitious tasks where entropic solvers fail (Klein et al., 2023).

**The Need for Unbalanced Low-Rank Solvers.** LR solvers do suffer, however, from a major practical limitation: their inability to handle unbalanced problems. Yet, unbalancedness is a crucial ingredient for OT to be practically relevant. This is exemplified by the fact that unbalancedness played a crucial role in the seminal reference (Schiebinger et al., 2019), where it is used to model cell birth and death.

**Our Contributions** We propose in this work to lift this last limitation for LR solvers to:

* Incorporate unbalanced regularizers to define a LR linear solver (SS 3.1);
* Provide accelerated algorithms, inspired by some of the recent corrections proposed by (Sejourne et al., 2022), to isolate translation terms that appear in dual subroutines (SS 3.2);
* Carry over and adapt these approaches to the GW (SS 3.3) and Fused-GW problems (SS 3.4);
* Carry out an exhaustive hyperparameter selection procedure within large scale OT tasks (spatial transcriptomics, brain imaging), and demonstrate the benefits of our approach (SS 4).

## 2 Reminders on Low-Rank Transport and Unbalanced Transport

We consider two metric spaces \((\mathcal{X},d_{\mathcal{X}})\) and \((\mathcal{Y},d_{\mathcal{Y}})\), as well as a cost function \(c:\mathcal{X}\times\mathcal{Y}\to[0,+\infty[\). The simplex \(\Delta_{n}^{+}\) holds all positive \(n\)-vectors summing to \(1\). For \(n,m\geq 1,a\in\Delta_{n}^{+}\), and \(b\in\Delta_{m}^{+}\), given points \(x_{1},\ldots,x_{n}\in\mathcal{X}\) and \(y_{1},\ldots,y_{m}\in\mathcal{Y}\), we define two discrete probability measures \(\mu\) and \(\nu\) as \(\mu:=\sum_{j=1}^{n}a_{i}\delta_{x_{i}}\), \(\nu:=\sum_{j=1}^{m}b_{j}\delta_{y_{j}}\) where \(\delta_{z}\) is the Dirac mass at \(z\).

**Cost matrices.** For \(q\geq 1\), consider first two square pairwise _cost_ matrices, each encoding the geometries of points _within_\(\mu\) and \(\nu\), and a rectangular matrix that studies that _across_ their support:

\[A:=[d_{\mathcal{Y}}^{q}(x_{i},x_{i^{\prime}})]_{1\leq i,i^{\prime}\leq n},\ B: =[d_{\mathcal{Y}}^{q}(y_{j},y_{j^{\prime}})]_{1\leq j,j^{\prime}\leq m}\,,\ C: =[c(x_{i},y_{j})]_{\begin{subarray}{c}1\leq i\leq n\\ 1\leq j\leq m\end{subarray}}.\]

**The Kantorovich Formulation of OT** is defined as the following parameterized linear program:

\[\text{OT}(\mu,\nu):=\min_{P\in\Pi_{a,b}}\langle C,P\rangle\,,\quad\text{where} \quad\Pi_{a,b}:=\big{\{}P\in\mathbb{R}_{+}^{n\times m},\text{ s.t. }\ P\mathbf{1}_{m}=a,\ P^{T}\mathbf{1}_{n}=b\big{\}}\,\,. \tag{1}\]

**The Low-Rank Formulation of OT** is best understood as a variant of (1) that rests on a low-rank _property_ for cost matrix \(C\), and low-rank _constraints_ for couplings \(P\). More precisely, Sceton et al. (2021) propose to constraint the set of admissible couplings to those, within \(\Pi_{a,b}\), that have a non-negative rank of \(r\geq 1\). That set can be equivalently reparameterized as

\[\Pi_{a,b}(r)=\{P\in\mathbb{R}_{+}^{n\times m}|P=Q\operatorname{diag}(1/g)R^{T},\ Q\in\Pi_{a,g},\ R\in\Pi_{b,g},\ \text{ and }\ g\in\Delta_{r}^{+}\}.\]

The low-rank optimal transport (LOT) problem simply uses that restriction in (1) to define :

\[\text{LOT}_{r}(\mu,\nu):=\min_{P\in\Pi_{a,b}(r)}\langle C,P\rangle=\min_{Q\in \Pi_{a,g},R\in\Pi_{a,g},g\in\Delta_{r}^{+}}\langle C,Q\operatorname{diag}(1/g )R\rangle\,. \tag{2}\]Scetbon et al. (2021) propose and prove the convergence of a mirror-descent scheme to solve (2), and obtain linear time and memory complexities with respect to the number of samples, where each iteration in that descent scales as \((n+m)rd\), where \(d\) is the rank of \(C\).

**The Unbalanced Formulation of OT** starts from (1) as well, but proposes to do without \(\Pi_{a,b}\) and its marginal constraints (Frogner et al., 2015; Chizat et al., 2018), and rely instead on two regularizers:

\[\text{UOT}(\mu,\nu):=\min_{P\in\mathbb{R}_{+}^{n\times m}}\langle C,P\rangle+ \tau_{1}\text{KL}(P\mathbf{1}_{m}|a)+\tau_{2}\text{KL}(P^{T}\mathbf{1}_{n}|b) \tag{3}\]

where \(\tau_{1},\tau_{2}>0\) and \(\text{KL}(p|q):=\sum_{i}p_{i}\log(p_{i}/q_{i})+q_{i}-p_{i}\). This formulation is solved using entropic regularization, with modified Sinkhorn updates (Frogner et al., 2015). _Proposing an efficient algorithm able to merge (2) with (3) is the first goal of this paper._

**Gromov-Wasserstein (GW) Considerations.** The GW problem (Memoli, 2011) is a generalization of (1) where the energy \(\mathcal{Q}_{A,B}\) is a quadratic function of \(P\) defined through inner cost matrices \(A\), \(B\):

\[\mathcal{Q}_{A,B}(P)\!:=\!\!\!\sum_{i,j,i^{\prime},j^{\prime}}(A_{ii^{\prime} }-B_{jj^{\prime}})^{2}P_{ij}P_{i^{\prime}j^{\prime}}\!=\!\mathbf{1}_{m}^{T}P^ {T}A^{\odot 2}P\mathbf{1}_{m}+\mathbf{1}_{n}^{T}PB^{\odot 2}P^{T}\mathbf{1}_{n}-2 \langle APB,P\rangle \tag{4}\]

where \(\odot\) is the Hadamard product. To minimize (4), the default approach rests on entropic regularization (Solomon et al., 2016; Peyre et al., 2016) and variants (Sato et al., 2020; Blumberg et al., 2020; Xu et al., 2019; Li et al., 2023). Scetbon et al. (2022) adapted the low-rank framework to minimize \(\mathcal{Q}_{A,B}\) over low-rank matrices \(P\), achieving a linear-time complexity when \(A\) and \(B\) are themselves low-rank. Independently, (Sejourne et al., 2021) proposed an unbalanced generalization that also applies to GW and which can be implemented practically using entropic regularization. Finally, the minimization of a composite objective involving the sum of \(\mathcal{Q}_{A,B}\) with \(\langle C,\cdot\rangle\) is known as the _fused_ GW problem (Vayer et al., 2018).

## 3 Unbalanced Low-Rank Transport

### Unbalanced Low-rank Linear Optimal Transport

We incorporate unbalancedness to low-rank solvers (Scetbon et al., 2021, 2022), moving gradually from the linear problem to the more involved GW and FGW problem. Using the framework of (Frogner et al., 2015; Chizat et al., 2018), we extend first the definition of LOT, introduced in (2), to the unbalanced case by considering the following optimization problem:

\[\text{ULOT}_{r}(\mu,\nu):=\min_{P:\;\mathbf{k}_{+}(P)\leq r}\langle C,P\rangle +\tau_{1}\text{KL}(P\mathbf{1}_{m}|a)+\tau_{2}\text{KL}(P^{T}\mathbf{1}_{n}|b), \tag{5}\]

where \(\mathbf{k}_{+}(P)\) denotes the non-negative rank of \(P\). Therefore by denoting \(\Pi_{r}:=\{(Q,R,g)\in\mathbb{R}_{+}^{n\times r}\times\mathbb{R}_{+}^{m\times r }\times\mathbb{R}_{+}^{r}\colon Q^{T}\mathbf{1}_{n}=R^{T}\mathbf{1}_{m}=g\}\), and using the reparameterization of low-rank couplings, we obtain the following equivalent formulation of ULOT:

\[\text{ULOT}_{r}(\mu,\nu)=\min_{(Q,R,g)\in\Pi_{r}}\underbrace{\langle C,Q\, \text{diag}(1/g)R^{T}\rangle}_{\mathcal{L}_{C}(Q,R,g)}+\underbrace{\tau_{1} \text{KL}(Q\mathbf{1}_{r}|a)+\tau_{2}\text{KL}(R\mathbf{1}_{r}|b)}_{\mathcal{ G}_{a,b}(Q,R,g)}. \tag{6}\]

We introduce the more compact notation \(\mathcal{G}_{a,b}(Q,R,g):=F_{\tau_{1},a}(Q\mathbf{1}_{r})+F_{\tau_{2},b}(R \mathbf{1}_{r}),\) where \(F_{\tau,z}(s):=\tau\text{KL}(s|z)\) for \(\tau>0\) and \(z\geq 0\) coordinate-wise. To solve (6), and using this split, we move away from mirror-descent and apply instead proximal gradient-descent for the KL divergence. At each iteration, we consider a linear approximation of \(\mathcal{L}_{C}\) where a KL penalization is added to the objective (as in the classical mirror descent scheme). However, we leave \(\mathcal{G}_{a,b}\) intact at each iteration. Borrowing notations from (Scetbon et al., 2021), we must solve at each iteration the convex optimization problem:

\[(Q_{k+1},R_{k+1},g_{k+1}):=\operatorname*{argmin}_{\boldsymbol{\zeta}\in\Pi_{r} }\frac{1}{\gamma_{k}}\text{KL}(\boldsymbol{\zeta},\boldsymbol{\xi}_{k})+\tau_ {1}\text{KL}(Q\mathbf{1}_{r}|a)+\tau_{2}\text{KL}(R\mathbf{1}_{r}|b)\,, \tag{7}\]

where \((Q_{0},R_{0},g_{0})\in\Pi_{r}\) is the initialization, and the triplet \(\boldsymbol{\xi}_{k}:=(\xi_{k}^{(1)},\xi_{k}^{(2)},\xi_{k}^{(3)})\) holds synthetic costs matrices that are re-computed at each iteration \(k\):

\[\xi_{k}^{(1)}:=Q_{k}\odot e^{-\gamma_{k}CR_{k}\,\text{diag}(1/g_{k})},\xi_{k}^ {(2)}:=R_{k}\odot e^{-\gamma_{k}C^{T}Q_{k}\,\text{diag}(1/g_{k}))},\xi_{k}^{(3 )}:=g_{k}\odot e^{\gamma_{k}\omega_{k}/g_{k}^{2}},\]with \([\omega_{k}]_{i}:=[Q_{T}^{T}CR_{k}]_{i,i}\in\mathbb{R}^{r}\), and \((\gamma_{k})_{k\geq 0}\) is a sequence of positive step sizes.

**Reformulation using Duality.** To solve (7), we apply Dykstra's algorithm [1983], whose iterations correspond to an alternating maximization on the dual formulation of (7):

**Proposition 1**.: _The convex optimization problem defined in (7) admits the following dual:_

\[\begin{split}\sup_{f_{1},h_{1},f_{2},h_{2}}&\mathcal{ D}_{k}(f_{1},h_{1},f_{2},h_{2}):=-F^{\star}_{\tau_{1},a}(-f_{1})-\frac{1}{ \gamma_{k}}\langle e^{\gamma_{k}(f_{1}\oplus h_{1})}-1,\xi_{k}^{(1)}\rangle\\ &-F^{\star}_{\tau_{2},b}(-f_{2})-\frac{1}{\gamma_{k}}\langle e^{ \gamma_{k}(f_{2}\oplus h_{2})}-1,\xi_{k}^{(2)}\rangle-\frac{1}{\gamma_{k}} \langle e^{-\gamma_{k}(h_{1}+h_{2})}-1,\xi_{k}^{(3)}\rangle\end{split} \tag{8}\]

_where \(h_{1},h_{2}\in\mathbb{R}^{r}\), \(f_{1}\in\mathbb{R}^{n}\), \(f_{2}\in\mathbb{R}^{m}\), \(F^{\star}_{\tau,z}(\cdot):=\sup_{y}\{\langle y,\cdot\rangle-F_{\tau,z}(y)\}\) is the convex conjugate of \(F_{\tau,z}\). In addition strong duality holds and the primal problem admits a unique minimizer._

**Remark 1**.: _While we stick to KL regularizers in this work for simplicity, it is worth noting that this can be extended to more generic regularizers \(F_{\tau_{1},a}\) and \(F_{\tau_{2},b}\), as considered by Chizat et al. [2018]._

We use an alternating maximization scheme to solve (8). Starting from \(h_{1}^{(0)}=h_{2}^{(0)}=\mathbf{0}_{r}\), we apply for \(\ell\geq 0\) the following updates (dropping iteration number \(k\) in (7) for simplicity):

\[\begin{split} f_{1}^{(\ell+1)}:=\arg\sup_{z}\mathcal{D}(z,h_{1}^{ (\ell)},f_{2}^{(\ell)},h_{2}^{(\ell)}),\,f_{2}^{(\ell+1)}:=\arg\sup_{z}\mathcal{ D}(f_{1}^{(\ell+1)},h_{1}^{(\ell)},z,h_{2}^{(\ell)}),\\ (h_{1}^{(\ell+1)},h_{2}^{(\ell+1)}):=\arg\sup_{z_{1},z_{2}}\mathcal{ D}(f_{1}^{(\ell+1)},z_{1},f_{2}^{(\ell+1)},z_{2}).\end{split}\]

These maximizations can all be obtained in closed form, to result in the closed-form updates:

\[\begin{split}&\exp(\gamma f_{1}^{(\ell+1)})=\left(\frac{a}{\xi^{(1)} \exp(\gamma h_{1}^{(\ell)})}\right)^{\frac{\tau_{1}}{\tau_{1}+1/\gamma}},\quad \exp(\gamma f_{2}^{(\ell+1)})=\left(\frac{b}{\xi^{(2)}\exp(\gamma h_{2}^{(\ell) })}\right)^{\frac{\tau_{2}}{\tau_{2}+1/\gamma}}\\ & g_{\ell+1}:=\left(\xi^{(3)}\odot(\xi^{(1)})^{T}\exp(\gamma f_{1 }^{(\ell+1)})\odot(\xi^{(2)})^{T}\exp(\gamma f_{2}^{(\ell+1)})\right)^{1/3}\\ &\exp(\gamma h_{1}^{(\ell+1)})=\frac{g_{\ell+1}}{(\xi^{(1)})^{T} \exp(\gamma f_{1}^{(\ell+1)})},\quad\exp(\gamma h_{2}^{(\ell+1)})=\frac{g_{ \ell+1}}{(\xi^{(2)})^{T}\exp(\gamma f_{2}^{(\ell+1)})}\end{split}\]

When using "scaling" representations for these dual variables, \(\ell\geq 0\), \(u_{i}^{(\ell)}:=\exp(\gamma f_{i}^{(\ell)})\) and \(v_{i}^{(\ell)}:=\exp(\gamma h_{i}^{(\ell)})\) for \(i\in\{1,2\}\), we obtain a simple update, provided in the appendix (Alg. 5).

**Initialization and Termination.** We use the stopping criterion proposed in [Scetbon et al., 2021] to terminate the algorithm, \(\Delta(\boldsymbol{\zeta},\boldsymbol{\tilde{\zeta}},\boldsymbol{\gamma}):= \frac{1}{\gamma^{2}}(\operatorname{KL}(\boldsymbol{\zeta},\boldsymbol{\tilde{ \zeta}})+\operatorname{KL}(\boldsymbol{\tilde{\zeta}},\boldsymbol{\zeta}))\). Finding an efficient initialization for that problem is challenging, and various choices have been implemented for instance in [Cuturi et al., 2022]. We adopt the practical choices proposed in [Scetbon and Cuturi, 2022], using either random subcoupling matrices or a \(k\)-means approach, and also follow them in adapting the choice of \(\gamma_{k}\) at each iteration \(k\) of the outer loop. We summarize our proposal in Algorithm 1, which can be seen as an extension of [Scetbon et al., 2021, Alg.2].

**Convergence.** The convergence proof for Dykstra's algorithm, as implemented in Alg. 5 (see appendix), follows from [Bauschke and Combettes, 2008]. Scetbon et al. [2021] show the convergence of their scheme towards a stationary point, w.r.t to the criterion \(\Delta(\cdot,\cdot,\gamma)\), for fixed \(\gamma\). The stationary convergence of our proposed algorithm can be directly derived from their result.

**Complexity.** Given \(\boldsymbol{\xi}\), solving Eq. (7) requires a time and memory complexity of \(\mathcal{O}((n+m)r)\). However computing \(\boldsymbol{\xi}\) requires in general \(\mathcal{O}((n^{2}+m^{2})r)\) time and \(\mathcal{O}(n^{2}+m^{2})\) memory. Scetbon et al. [2021] propose to consider low-rank factorizations of the cost matrix \(C\) of the form \(C\simeq C_{1}C_{2}^{T}\) where \(C_{1}\in\mathbb{R}^{n\times d}\) and \(C_{2}\in\mathbb{R}^{m\times d}\). In that case computing \(\boldsymbol{\xi}\) can be done in \(\mathcal{O}((n+m)rd)\) time and \(\mathcal{O}((n+m)(r+d))\) memory. Such factorizations are either known explicitly (e.g. when using squared-Euclidean distances) or can be obtained as approximations using the algorithm in [Indyk et al., 2019], which guarantees that for any distance matrix \(C\in\mathbb{R}^{n\times m}\) and \(\alpha>0\) it can output matrices \(C_{1}\in\mathbb{R}^{n\times d}\), \(C_{2}\in\mathbb{R}^{m\times d}\) in \(\mathcal{O}((m+n)\text{poly}(\frac{d}{\alpha}))\) algebraic operations such that with probability at least \(0.99\), \(\|C-C_{1}C_{2}^{T}\|_{F}^{2}\leq\|C-C_{d}\|_{F}^{2}+\alpha\|C\|_{F}^{2}\), where \(C_{d}\) denotes the best rank-\(d\) approximation to \(C\).

[MISSING_PAGE_FAIL:5]

```
Inputs:\(a,b,\mathbf{\xi}=(\xi^{(1)},\xi^{(2)},\xi^{(3)}),\gamma,\tau_{1},\tau_{2},\delta\) \(v_{1}=v_{2}=\mathbf{1}_{r}\), \(u_{1}=\mathbf{1}_{n}\), \(u_{2}=\mathbf{1}_{m}\) repeat \(\tilde{v}_{1}=v_{1},\ \tilde{v}_{2}=v_{2},\tilde{u}_{1}=u_{1},\tilde{u}_{2}=u_{2}\) \(\lambda_{1},\lambda_{2}\leftarrow\text{compute-lambdas}(a,b,\xi^{(3)},u_{1},v_{ 1},u_{2},v_{2},\gamma,\tau_{1},\tau_{2})\) (Alg. 2) \(u_{1}=\left(\frac{a}{\xi^{(1)}v_{1}}\right)^{\frac{\tau_{1}}{1+\gamma/\gamma}} \exp(-\lambda_{1}/\tau_{1})^{\frac{\tau_{1}}{1/\gamma+\tau_{1}}},\quad u_{2}= \left(\frac{b}{\xi^{(2)}v_{2}}\right)^{\frac{\tau_{2}}{\gamma_{2}+1/\gamma}} \exp(-\lambda_{2}/\tau_{2})^{\frac{\tau_{2}}{1/\gamma+\tau_{2}}},\) \(\lambda_{1},\lambda_{2}\leftarrow\text{compute-lambdas}(a,b,\xi^{(3)},u_{1},v_{ 1},u_{2},v_{2},\gamma,\tau_{1},\tau_{2})\) (Alg. 2) \(g=\exp(\gamma(\lambda_{1}+\lambda_{2}))^{1/3}\left(\xi^{(3)}\odot(\xi^{(1)})^{ T}u_{1}\odot(\xi^{(2)})^{T}u_{2}\right)^{1/3},\ v_{1}=\frac{g}{(\xi^{(1)})^{T}u_{1}},\ v_{2}=\frac{g}{(\xi^{(2)})^{T}u_{2}}\) until\(\frac{1}{\gamma}\max(\|\log(u_{i}/\tilde{u}_{i})\|_{\infty},\|\log(v_{i}/\tilde{v}_{i})\|_{ \infty})<\delta;\) Result:\(\operatorname{diag}(u_{1})\xi^{(1)}_{k}\operatorname{diag}(v_{1}),\ \ \operatorname{diag}(u_{2})\xi^{(2)}_{k}\operatorname{diag}(v_{2}),\ \ g\)
```

**Algorithm 3**ULR-TI-Dykstra\((a,b,\mathbf{\xi},\gamma,\tau_{1},\tau_{2},\delta)\)

### Unbalanced Low-rank Gromov-Wasserstein

The low-rank Gromov-Wasssertein (LGW) problem (Scetbon et al., 2022) between the two discrete metric measure spaces \((\mu,d_{\mathcal{X}})\) and \((\nu,d_{\mathcal{Y}})\), written for compactness using \((a,A)\) and \((b,B)\), reads

\[\text{LGW}_{r}((a,A),(b,B))=\min_{P\in\Pi_{a,b}(r)}\mathcal{Q}_{A,B}(P), \tag{12}\]

Building upon SS 3.1 and leveraging the TI variant presented in SS 3.2, we introduce the unbalanced low-rank Gromov-Wasserstein (ULGW) problem. There is, however, a significant challenge that appears when introducing unbalanced regularizers in (12): When \(P\) is constrained to be in \(\Pi_{a,b}\), the first two terms of the RHS in (12) simplify to \(a^{T}A^{\odot 2}a+b^{T}B^{\odot 2}b\). Hence, they are constant and discarded when optimizing. In an unbalanced setting, these terms vary and must be accounted for:

\[\text{ULGW}_{r}((a,A),(b,B)):=\min_{(Q,R,g)\in\Pi_{r}}\langle A^{ \odot 2}Q\mathbf{1}_{r},Q\mathbf{1}_{r}\rangle+\langle B^{\odot 2}R\mathbf{1}_{r},R\mathbf{1}_{r}\rangle \tag{13}\] \[-2\langle AQ\operatorname{diag}(1/g)R^{T}B,Q\operatorname{diag}( 1/g)R^{T}\rangle+\tau_{1}\text{KL}(Q\mathbf{1}_{r}|a)+\tau_{2}\text{KL}(R\mathbf{1}_{r }|b)\]

To solve the problem, we apply the same scheme as proposed for ULOT, that is a proximal gradient descent where we linearize \(\mathcal{Q}_{A,B}\) and add a KL penalization while leaving the soft marginal constraints unchanged. Therefore the algorithm to solve ULGW is the same as that solving ULOT, however, the kernels \(\mathbf{\xi}_{k}\) now take into account the quadratic terms of the original LGW problem. More formally, at each iteration \(k\) of the outer loop, we propose to solve

\[(Q_{k+1},R_{k+1},g_{k+1}):=\operatorname*{argmin}_{\mathbf{\zeta}\in\Pi_{r}}\frac{1 }{\gamma_{k}}\text{KL}(\mathbf{\zeta}|\mathbf{\xi}_{k})+\tau_{1}\text{KL}(Q\mathbf{1}_{r}|a )+\tau_{2}\text{KL}(R\mathbf{1}_{r}|b), \tag{14}\]

where \((Q_{0},R_{0},g_{0})\in\Pi_{r}\) is the initialization, \((\gamma_{k})_{k\geq 0}\) a sequence of positive step sizes. Using notation \(P_{k}=Q_{k}\operatorname{diag}(1/g_{k})R_{k}^{T}\), the synthetic cost matrices \(\mathbf{\xi}_{k}:=(\xi^{(1)}_{k},\xi^{(2)}_{k},\xi^{(3)}_{k})\) are updated as:

\[\xi^{(1)}_{k} :=Q_{k}\odot\exp(-2\gamma_{k}A^{\odot 2}Q_{k}\mathbf{1}_{r}\mathbf{1}_{r}^{T}) \odot\exp(-4\gamma_{k}AP_{k}BR_{k}\operatorname{diag}(1/g_{k})))\,,\] \[\xi^{(2)}_{k} :=R_{k}\odot\exp(-2\gamma_{k}B^{\odot 2}R_{k}\mathbf{1}_{r}\mathbf{1}_{r}^{T}) \odot\exp(-4\gamma_{k}BP_{k}^{T}AQ_{k}\operatorname{diag}(1/g_{k})))\,,\] \[\xi^{(3)}_{k} :=g_{k}\odot\exp(4\gamma_{k}\omega_{k}/g_{k}^{2})\quad\text{with} \quad[\omega_{k}]_{i}:=[Q_{k}^{T}AP_{k}BR_{k}]_{i,i}\in\mathbb{R}^{r}\,.\]

Note that (14) is the exact same optimization problem as (7), where only \(\mathbf{\xi}_{k}\) has changed and therefore can be solved using Algorithm 3. Algorithm 4 summarizes our strategy to solve (13).

**Inputs:**\(A,B,a,b,r,\gamma_{0},\tau_{1},\tau_{2},\delta\)

\(Q,R,g\leftarrow\) Initialization as proposed in (Scetbon and Cuturi, 2022)

**repeat**

\(\tilde{Q}=Q,\ \ \tilde{R}=R,\ \ \tilde{g}=g\)

\(\nabla_{Q}=4AQ\operatorname{diag}(1/g)R^{T}BR\operatorname{diag}(1/g)+2A^{ \odot 2}Q\mathbf{1}_{r}\mathbf{1}_{r}^{T}\),

\(\nabla_{R}=4BR\operatorname{diag}(1/g)Q^{T}AQ\operatorname{diag}(1/g)+2B^{ \odot 2}R\mathbf{1}_{r}\mathbf{1}_{r}^{T}\),

\(\omega\leftarrow\mathcal{D}(Q^{T}AQ\operatorname{diag}(1/g)R^{T}BR),\ \ \nabla_{g}=-\omega/g^{2}\),

\(\gamma\leftarrow\gamma_{0}/\max(\|\nabla_{Q}\|_{\infty}^{2},\|\nabla_{R}\|_{ \infty}^{2},\|\nabla_{g}\|_{\infty}^{2})\),

\(\xi^{(1)}\gets Q\odot\exp(-\gamma\nabla_{Q})\), \(\xi^{(2)}\gets R\odot\exp(-\gamma\nabla_{R})\), \(\xi^{(3)}\gets g\odot\exp(-\gamma_{k}\nabla_{g})\),

\(Q,R,g\leftarrow\) ULR-TI-Dykstra\((a,b,\boldsymbol{\xi},\gamma,\tau_{1},\tau_{2},\delta)\) (Alg. 3)

**until**\(\Delta((Q,R,g),(\tilde{Q},\tilde{R},\tilde{g}),\gamma)<\delta\);

**Result:**\(Q,R,g\)

**Convergence and Complexity.** Similarly to linear ULOT, the unbalanced Dykstra algorithm is guaranteed to converge (Bauschke and Lewis, 2000). Because we use Algorithm 5, we retain exactly the same complexity, both in terms of time of memory, to solve these inner problems. The slight variation in kernel \(\boldsymbol{\xi}\) compared to ULOT still retains the same \(\mathcal{O}((n^{2}+m^{2})r)\) time and \(\mathcal{O}(n^{2}+m^{2})\) memory complexities. However, as in ULOT, we can take advantage of low-rank approximations of _both_ costs matrices \(A\) and \(B\) to reach linear complexity. Indeed, assuming \(A\simeq A_{1}A_{2}^{T}\) and \(B\simeq B_{1}B_{2}\) where \(A_{1},A_{2}\in\mathbb{R}^{n\times d_{X}}\) and \(B_{1},B_{2}\in\mathbb{R}^{m\times d_{Y}}\), then the total time and memory complexities become respectively \(\mathcal{O}(mr(r+d_{Y})+nr(r+d_{X}))\) and \(\mathcal{O}((n+m)(r+d_{X}+d_{Y}))\). Again, when \(A\) and \(B\) are distance matrices, we use the algorithms from (Indyk et al., 2019).

### Unbalanced Low-rank Fused-Gromov-Wasserstein

We finally focus on the increasingly impactful (Klein et al., 2023) fused-Gromov-Wasserstein problem, which merges linear and quadratic objectives (Vayer et al., 2018):

\[\text{FGW}(\mu,\nu):=\min_{P\in\Pi_{a,b}}\alpha(C,P)+\bar{\alpha}\mathcal{Q}_{ A,B}(P) \tag{15}\]

where \(\alpha\in[0,1]\) and \(\bar{\alpha}:=1-\alpha\) allows interpolating between the GW and linear OT geometries. This problem remains a GW problem, where one replaces the 4-way cost \(M[i,i^{\prime},j,j^{\prime}]:=(A_{i,i^{\prime}}-B_{j,j^{\prime}})^{2}\) appearing in (4) by a composite interpolated cost between the OT and GW geometries, redefined as \(M[i,i^{\prime},j,j^{\prime}]=\alpha C_{i,j}+\bar{\alpha}(A_{i,i^{\prime}}-B_{ j,j^{\prime}})^{2}\). Our proposed unbalanced and low-rank version of the FGW problem includes \(|P|:=\|P\|_{1}\) the mass of \(P\), to homogenize linear and quadratic terms,

\[\text{ULFGW}_{r}(\mu,\nu):=\min_{P:\ \boldsymbol{\xi}_{k}(P)\leq r}\alpha|P |(C,P)+\bar{\alpha}\mathcal{Q}_{A,B}(P)+\tau_{1}\text{KL}(P\mathbf{1}_{m}|a)+ \tau_{2}\text{KL}(P^{T}\mathbf{1}_{n}|b)\,, \tag{16}\]

which is expanded through the explicit factorization of \(P\), noticing that \(|P|=|g|:=\|g\|_{1}\):

\[\text{ULFGW}_{r}(\mu,\nu):=\min_{(Q,R,g)\in\Pi_{r}}\alpha|g|\mathcal{L}_{C}(Q, R,g)+\bar{\alpha}\mathcal{Q}_{A,B}(Q,R,g)+\mathcal{G}_{a,b}(Q,R,g) \tag{17}\]

Then by linearizing again \(\mathcal{H}:(Q,R,g)\rightarrow\alpha|g|\mathcal{L}_{C}(Q,R,g)+\bar{\alpha} \mathcal{Q}_{A,B}(Q,R,g)\) with an added KL penalty and leaving \(\mathcal{G}_{a,b}\) unchanged, we obtain at each iteration, the same optimization problem as in (14) where the kernels \(\boldsymbol{\xi}_{k}\) are now defined as

\[\boldsymbol{\xi}_{k} :=(\xi_{k}^{(1)},\xi_{k}^{(2)},\xi_{k}^{(3)}),\] \[\xi_{k}^{(1)} :=Q_{k}\odot\exp(-\gamma_{k}\nabla_{Q}\mathcal{H}_{k}),\ \xi_{k}^{(2)}:=R_{k} \odot\exp(-\gamma_{k}\nabla_{Q}\mathcal{H}_{k}),\ \xi_{k}^{(3)}:=g_{k}\odot\exp(-\gamma_{k} \nabla_{g}\mathcal{H}_{k})\] \[\nabla_{Q}\mathcal{H}_{k} :=\alpha|g_{k}|CR_{k}\operatorname{diag}(1/g_{k})+\bar{\alpha} \left(2A^{\odot 2}Q_{k}\mathbf{1}_{r}\mathbf{1}_{r}^{T}+4AP_{k}BR_{k}\operatorname{diag}(1/g_ {k})\right)\] \[\nabla_{R}\mathcal{H}_{k} :=\alpha|g_{k}|CR^{T}Q_{k}\operatorname{diag}(1/g_{k})+\bar{ \alpha}\left(2B^{\odot 2}R_{k}\mathbf{1}_{r}\mathbf{1}_{r}^{T}+4BP_{k}^{T}AQ_{k} \operatorname{diag}(1/g_{k})\right)\] \[\nabla_{g}\mathcal{H}_{k} :=\alpha\left((C,P_{k})\mathbf{1}_{r}-|g_{k}|\omega_{k}^{\text{ min}}/g_{k}^{2}\right)-4\bar{\alpha}\omega_{k}^{\text{quad}}/g_{k}^{2}\] \[[\omega_{k}^{\text{lin}}]_{i} :=[Q_{k}^{T}CR_{k}]_{i,i},\ \ [\omega_{k}^{\text{quad}}]_{i}:=[Q_{k}^{T}AP_{k}BR_{k}]_{i,i} \ \forall\,i\in\{1,\dots,r\}\.\]

These steps are summarized in Alg. 6 (see appendix). These steps result in a quadratic complexity, both in time and memory, with respect to the number of points \(n\) and \(m\). However, these complexities become _linear_ when square matrices \(A,B\)_and_ rectangular \(C\) all admit a low-rank factorization.

## 4 Experiments

We focus first in **Exp. 1** on demonstrating the empirical benefits of the TI variant of our algorithm to solve linear ULOT, as implemented in Alg. 3 vs. Alg. 5; that algorithm is subsequently used as an inner routine to solve all quadratic ULR problems. We compare in **Exp. 2**_unbalanced_ low-rank (ULR) solvers to _balanced_ low-rank (LR) counterparts on a spatial transcriptomics task, and follow in **Exp. 3** by comparing ULR solvers to entropic (E) counterparts on a smaller task, to accommodate entropic solvers' quadratic complexity. We conclude in **Exp. 4** by comparing ULR solvers to [Thual et al., 2022], which can learn a sparse transport coupling, in the unbalanced FGW setting.

Datasets.We consider two real-world datasets, described in B.1, and two synthetic datasets, that are large enough to showcase our solvers. The real-world datasets consist of both a shared feature space, used to compute the costs matrices for the linear term in the OT and FGW settings, as well as geometries that are specific to each source \(s\) and target \(t\) measures, and which are used to compute the costs matrices for the quadratic term in the GW and FGW settings. In **Exp. 1**, we simply consider two isotropic Gaussians in \(d=30\) to evaluate the performance of the TI variant on a liner problem. We use the mouse brain STARmap spatial transcriptomics data from [Shi et al., 2022] for **Exp. 2** and **Exp. 3**. We use data from the Individual Brain Charting dataset [Pinho et al., 2018], to replicate the settings of [Thual et al., 2022], in **Exp. 4**.

Metrics.Following Klein et al. [2023], we evaluate maps by focusing on the two following metrics: (i) **pearson correlation**\(\rho\) computed between the (ground truth) source \(s\)_feature_ matrix \(F^{s}\in\mathbb{R}^{n\times d}\), and the barycentric projection of the target \(t\) to the source scaled by the target marginals \(b^{t}\). Writing \(P\) as the transport matrix from source to target, this can be computed as \(P\text{diag}(\frac{1}{b^{t}})F^{t}\); (ii) **F1 score** when assessing class transfer (among 11 possible classes), computed between the original source vector of labels \(l^{s}\), taken in \(\{1,\cdots,11\}^{n}\), and the inferred labels for the same points, predicted for each \(i\) by taking the \(\operatorname*{argmax}_{j}B_{i,j}\), where \(B\) is a matrix of \(n\times 11\) row probabilities, each the barycentric projection of the target \(t\) one-hot encoded labels \(L^{t}\in\{0,1\}^{m\times 11}\), \(B:=P\text{diag}(\frac{1}{b^{t}})L^{t}\).

Experiment 1: Benchmarking The Translation Invariant Variant.We evaluate the effect of the proposed TI procedure on the computational cost of ULR solvers: We compare the time taken when solving unbalanced LR problems, with or without using the TI objective. In Figure 3, we compare the execution time (using our ott-jax implementation, and a single NVIDIA GeForce RTX 2080

Figure 1: **Exp. 2: Spatial visualization of two mouse brain sections, contrasting observed vs. predicted (using ULFOW) spatial distributions of expression levels, for two different genes.**

Figure 2: Visualization of measured and predicted tissue regions in the mouse brain in **Exp. 2**Ti card) of unbalanced LR Sinkhorn on large and high dimensional Gaussian distributions. The results presented are averaged over 10 random seeds with error bars. We use a \(\delta=10^{-9}\) convergence threshold and \(1000\) maximal number of iterations for Dykstra, in 64-bit precision. We observe that the use of our proposed TI objective is consistently beneficial when solving ULR problems. See also Appendix B.3 for additional experiments.

Experiment 2: ULOT vs. LOT on Gene Expression / Cell Type Annotation.We evaluate the accuracy of ULOT solvers for a large-scale spatial transcriptomics task, using gene expression mapping and cell type annotation. We compare it to the balanced LR alternative using the Pearson correlation \(\rho\) as described in the metrics section. We leverage two coronal sections of the mouse brain profiled by STARmap spatial transcriptomics by [22]. They consist of \(n\approx 40,000\) cells in both the source and target brain section. Each cell is described by 1000 gene features, in addition to 2D spatial coordinates. As a result \(A,B\) are \(\approx 40k\times 40k\), and the fused term \(C\) is a squared-Euclidean distance matrix on 30D PCA space computed on the gene expression space. We selected 10 marker genes for the validation and test sets from the _HPF_CA cluster. We run an extensive grid search as reported in B.2, we pick the best hyperparameters combination using performance on the \(10\) validation genes as a criterion, and we report that metric on the other genes in Table 1, as well as qualitative results in Figure 1 and Figure 2. Clearly, ULFOW is the best performing solver across all metrics. Interestingly, the ULOT does not consistently outperforms its balanced version, and unbalancedness seems to hurt performance for the LGW solvers. Nevertheless, both solvers display inconsistent performance across metrics, whereas the ULFOW and LFGW are consistently superior to the rest of the solvers. These results highlight how the flexibility given by the FGW formulation to leverage common and disparate geometries, paired with the unbalancedness relaxation, can provide state of the art algorithms for matching problems in large-scale, real world biological problems.

Experiment 3: ULOT vs. UEOT.We compare the performance of ULOT solvers to their unbalanced entropic alternatives (UEOT). We use the same datasets as in **Exp. 2**, but must pick a smaller subset (Olfactory bulb), to avoid OOM errors for entropic UGW solvers, since they cannot handle the \(40k\) sizes considered in **Exp. 2** (see B.1). This results in \(n\approx 20,000\) source and \(\approx 15,000\) target

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline
**solver** & **mass \%** & **val \(\rho\)** & **test \(\rho\)** & **F1 mac.** & **F1 mic.** & **F1 weig.** \\ \hline LOT & 1.000 & 0.282 & 0.386 & 0.210 & 0.411 & 0.360 \\ ULOT & 0.889 & 0.301 & 0.409 & 0.200 & 0.425 & 0.363 \\ \hline LGW & 1.000 & 0.227 & 0.288 & 0.487 & 0.716 & 0.692 \\ ULGW & 1.001 & 0.222 & 0.287 & 0.463 & 0.701 & 0.665 \\ \hline LFGW & 1.000 & 0.365 & 0.443 & 0.576 & 0.720 & 0.714 \\ ULFOW & 0.443 & **0.379** & **0.463** & **0.582** & **0.733** & **0.724** \\ \hline \hline \end{tabular}
\end{table}
Table 1: **Exp.2**, Results for spatial transcriptomics dataset (brain coronal section from Shi et al. [2022]).

Figure 3: Execution time of unbalanced LR Sinkhorn, with (Alg. 3) or without (Alg. 5) the TI variant. We fix the rank to \(r=10\); \(n\) points (displayed in thousands) are sampled from two Gaussian distributions in \(d=30\) of means respectively \(-1.2\) and \(1.3\), and standard deviations \(1\) and \(0.2\). (left) displays large \(\tau\) (close to balanced), (right) is smaller \(\tau\) (more unbalanced). We use the _same convergence threshold_ for the outer loop, for all sample sizes. As \(n\) gets bigger, this results in a relatively _looser_ threshold, explaining why timings can slightly decrease w.r.t. \(n\). What matters is, therefore, the comparative performance of TI vs non-TI for a fixed \(n\), _not the behaviour w.r.t. \(n\)_.

cells, and 1000 genes. Similar to **Exp. 2**, the fused term \(C\) is a squared-Euclidean distance matrix on 30-D PCA space, computed on gene expressions. As done in **Exp. 2**, we select 10 marker genes for the validation and 10 genes for the test set, from cluster _OB_1_. We run an extensive grid search, as in **Exp. 2** and B.2. Table 2 shows that ULFGW outperforms entropic solvers w.r.t. Pearson correlation \(\rho\), but is worse when considering F1 scores. On the other hand, ULFGW confirms its superiority compared to the balanced alternative LFGW. Taken together, these results suggest that while unbalanced LR solvers are on par with unbalanced entropic solvers in terms of performance, in small data regimes, they remain much faster and can unlock the applications of unbalanced OT to larger scales.

Experiment 4: ULOT to align brain meshes.In this experiment, we compare the performance of our ULFGW solver to FUGW-sparse [Thual et al., 2022], an alternative approach to solve unbalanced FGW problems, using a two-scale (corse/fine grid) approach to handle large sample sizes. This method was demonstrated to be effective in aligning brain anatomies, encompassing both mesh structures and functional signals associated with each vertex. For their empirical analysis, they use the individual brain charting dataset [Pinho et al., 2018].

In the absence of other information in the original paper, we draw inspiration from Pinho et al.'s smaller scale notebook implementations: We embed the \(n\approx 160,000\) vertices of the fsaverage7 mesh, into a 30-d space, using an approximation of the geodesic distances with landmark multi-dimensional scaling [De Silva and Tenenbaum, 2004] where 2048 points were used as anchors. Each vertex has an associated functional signal that entails 22 features. For both the quadratic and linear terms, we compute the costs based on the squared Euclidean distance. The coarse grid for FUGW-sparse is built using one-tenth of \(n\), i.e. \(\approx 16k\) points. We evaluate all methods by comparing the performance of the best hyperparameter combination, based on the average correlation between the barycentric projection and ground-truth value of 5 features, across a test set of 5 contrast maps. In Table 3, we observe that ULFGW and LFGW outperform FUGW-sparse. In this setting, there is no clear evidence that the unbalanced version performs better than its balanced counterpart for low-rank methods. See also Appendix B.2 for additional experimental details and results.

Conclusion.The practical success of OT methods to natural sciences demonstrates the relevance of OT to their analysis pipelines. Practitioners must, however, often deal with the poor scalability of OT algorithms, as well as their rigid assumptions w.r.t. mass conservation. While Low-rank OT approaches hold the promise of scaling OT methods to large sizes, unbalanced formulations have proved useful to relax mass conservation for entropic OT solvers. We have proposed in this paper to merge these two strains, and demonstrated the practical relevance of these unbalanced low-rank solvers on various challenging alignment tasks.

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline
**solver** & **mass \%** & **val \(\rho\)** & **test \(\rho\)** & **F1 mac.** & **F1 mic.** & **F1 weig.** \\ \hline UEOT & 1.012 & 0.368 & 0.479 & 0.511 & 0.763 & 0.751 \\ LOT & 1.000 & 0.335 & 0.440 & 0.511 & 0.760 & 0.751 \\ ULOT & 0.998 & 0.356 & 0.461 & 0.518 & 0.770 & 0.762 \\ \hline UEFGW & 1.015 & 0.343 & 0.475 & **0.564** & **0.839** & **0.831** \\ LFGW & 1.000 & 0.348 & 0.453 & 0.512 & 0.762 & 0.753 \\ ULFGW & 0.339 & **0.368** & **0.491** & 0.556 & 0.826 & 0.818 \\ \hline \hline \end{tabular}
\end{table}
Table 2: **Exp. 3**: Results for spatial transcriptomics dataset (Olfactory bulb section from Shi et al. [2022]).

Figure 4: Visualization of measured and predicted _right auditory click_ contrast map in **Exp.4**.

## References

* Ahuja et al. (1993) Ravindra K Ahuja, Thomas L Magnanti, and James B Orlin. _Network Flows: Theory, Algorithms, and Applications_. Prentice hall, 1993.
* Altschuler et al. (2017) Jason Altschuler, Jonathan Weed, and Philippe Rigollet. Near-linear time approximation algorithms for optimal transport via sinkhorn iteration. _arXiv preprint arXiv:1705.09634_, 2017.
* Armeni et al. (2016) Iro Armeni, Ozan Sener, Amir R Zamir, Helen Jiang, Ioannis Brilakis, Martin Fischer, and Silvio Savarese. 3d semantic parsing of large-scale indoor spaces. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 1534-1543, 2016.
* Bauschke & Combettes (2008) Heinz H Bauschke and Patrick L Combettes. A Dykstra-like algorithm for two monotone operators. _Pacific Journal of Optimization_, 4(3):383-391, 2008.
* Bauschke & Lewis (2000) Heinz H Bauschke and Adrian S Lewis. Dykstras algorithm with bregman projections: A convergence proof. _Optimization_, 48(4):409-427, 2000.
* Blumberg et al. (2020) Andrew J Blumberg, Mathieu Carriere, Michael A Mandell, Raul Rabadan, and Soledad Villar. Mrec: a fast and versatile framework for aligning and matching point clouds with applications to single cell molecular data. _arXiv preprint arXiv:2001.01666_, 2020.
* Boissard & Le Gouic (2014) Emmanuel Boissard and Thibaut Le Gouic. On the mean speed of convergence of empirical and occupation measures in wasserstein distance. In _Annales de l'IHP Probabilites et statistiques_, volume 50, pages 539-563, 2014.
* Bunne et al. (2021) Charlotte Bunne, Stefan G. Stark, Gabriele Gut, Jacobo Sarabia del Castillo, Kjong-Van Lehmann, Lucas Pelkmans, Andreas Krause, and Gunnar Ratsch. Learning single-cell perturbation responses using neural optimal transport. _bioRxiv_, 2021. doi: 10.1101/2021.12.15.472775. URL [https://www.biorxiv.org/content/early/2021/12/15/2021.12.15.472775](https://www.biorxiv.org/content/early/2021/12/15/2021.12.15.472775).
* Bunne et al. (2022) Charlotte Bunne, Andreas Krause, and Marco Cuturi. Supervised Training of Conditional Monge Maps. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2022.
* Cang et al. (2023) Zixuan Cang, Yanxiang Zhao, Axel A Almet, Adam Stabell, Raul Ramos, Maksim V Plikus, Scott X Atwood, and Qing Nie. Screening cell-cell communication in spatial transcriptomics via collective optimal transport. _Nature Methods_, 20(2):218-228, 2023.
* Caron et al. (2020) Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsupervised learning of visual features by contrasting cluster assignments. _Advances in neural information processing systems_, 33:9912-9924, 2020.
* Chizat et al. (2018) Lenaic Chizat, Gabriel Peyre, Bernhard Schmitzer, and Francois-Xavier Vialard. Unbalanced optimal transport: Dynamic and kantorovich formulations. _Journal of Functional Analysis_, 274(11):3090-3123, 2018.
* Chizat et al. (2020) Lenaic Chizat, Pierre Roussillon, Flavien Leger, Francois-Xavier Vialard, and Gabriel Peyre. Faster wasserstein distance estimation with the sinkhorn divergence. _Advances in Neural Information Processing Systems_, 33, 2020.
* Chowdhury et al. (2021) Samir Chowdhury, David Miller, and Tom Needham. Quantized gromov-wasserstein. _arXiv preprint arXiv:2104.02013_, 2021.
* Cuturi (2013) Marco Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. In _Advances in neural information processing systems_, pages 2292-2300, 2013.
* Cuturi et al. (2022) Marco Cuturi, Laetitia Meng-Papaxanthos, Yingtao Tian, Charlotte Bunne, Geoff Davis, and Olivier Teboul. Optimal transport tools (ott): A jax toolbox for all things wasserstein. _arXiv preprint arXiv:2201.12324_, 2022.
* De Silva & Tenenbaum (2004) Vin De Silva and Joshua B Tenenbaum. Sparse multidimensional scaling using landmark points. Technical report, technical report, Stanford University, 2004.
* De Silva et al. (2018)Pinar Demetci, Rebecca Santorella, Bjorn Sandstede, William Stafford Noble, and Ritamphara Singh. Gromov-wasserstein optimal transport to align single-cell multi-omics data. _bioRxiv_, 2020. doi: 10.1101/2020.04.28.066787.
* Dudley et al. [1966] Richard Mansfield Dudley et al. Weak convergence of probabilities on nonseparable metric spaces and empirical measures on euclidean spaces. _Illinois Journal of Mathematics_, 10(1):109-126, 1966.
* Dykstra [1983] Richard L Dykstra. An algorithm for restricted least squares regression. _Journal of the American Statistical Association_, 78(384):837-842, 1983.
* Fatras et al. [2021] Kilian Fatras, Thibault Sejourne, Remi Flamary, and Nicolas Courty. Unbalanced minibatch optimal transport; applications to domain adaptation. In Marina Meila and Tong Zhang, editors, _Proceedings of the 38th International Conference on Machine Learning_, volume 139 of _Proceedings of Machine Learning Research_, pages 3186-3197. PMLR, 18-24 Jul 2021. URL [https://proceedings.mlr.press/v139/fatras21a.html](https://proceedings.mlr.press/v139/fatras21a.html).
* Feydy et al. [2019] Jean Feydy, Pierre Roussillon, Alain Trouve, and Pietro Gori. Fast and scalable optimal transport for brain tractograms. In _Medical Image Computing and Computer Assisted Intervention-MICCAI 2019: 22nd International Conference, Shenzhen, China, October 13-17, 2019, Proceedings, Part III 22_, pages 636-644. Springer, 2019.
* Flamary et al. [2021] Remi Flamary, Nicolas Courty, Alexandre Gramfort, Mokhtar Z. Alaya, Aurelie Boisbunon, Stanislas Chambon, Laetitia Chapel, Adrien Corenflos, Kilian Fatras, Nemo Fournier, Leo Gautheron, Nathalie T.H. Gayraud, Hicham Janati, Alain Rakotomamonjy, Ievgen Redko, Antoine Rolet, Antony Schutz, Vivien Seguy, Danica J. Sutherland, Romain Tavenard, Alexander Tong, and Titouan Vayer. Pot: Python optimal transport. _Journal of Machine Learning Research_, 22(78):1-8, 2021. URL [http://jmlr.org/papers/v22/20-451.html](http://jmlr.org/papers/v22/20-451.html).
* Forrow et al. [2018] Aden Forrow, Jan-Christian Hutter, Mor Nitzan, Philippe Rigollet, Geoffrey Schiebinger, and Jonathan Weed. Statistical optimal transport via factored couplings, 2018.
* Frogner et al. [2015] Charlie Frogner, Chiyuan Zhang, Hossein Mobahi, Mauricio Araya, and Tomaso A Poggio. Learning with a Wasserstein loss. In _Advances in Neural Information Processing Systems_, pages 2053-2061, 2015.
* Genevay et al. [2019] Aude Genevay, Lenaic Chizat, Francis Bach, Marco Cuturi, and Gabriel Peyre. Sample Complexity of Sinkhorn Divergences. In _International Conference on Artificial Intelligence and Statistics (AISTATS)_, volume 22, 2019.
* Indyk et al. [2019] Piotr Indyk, Ali Vakilian, Tal Wagner, and David Woodruff. Sample-optimal low-rank approximation of distance matrices, 2019.
* Klein et al. [2023] Dominik Klein, Giovanni Palla, Marius Lange, Michal Klein, Zoe Piran, Manuel Gander, Laetitia Meng-Papaxanthos, Michael Sterr, Aimee Bastidas-Ponce, Marta Tarquis-Medina, Heiko Lickert, Mostafa Bakhti, Mor Nitzan, Marco Cuturi, and Fabian J. Theis. Mapping cells through time and space with moscot. _bioRxiv_, 2023. doi: 10.1101/2023.05.11.540374. URL [https://www.biorxiv.org/content/early/2023/05/11/2023.05.11.540374](https://www.biorxiv.org/content/early/2023/05/11/2023.05.11.540374).
* Kuhn [1955] Harold W Kuhn. The hungarian method for the assignment problem. _Naval research logistics quarterly_, 2(1-2):83-97, 1955.
* Le et al. [2021] Khang Le, Huy Nguyen, Quang M Nguyen, Tung Pham, Hung Bui, and Nhat Ho. On robust optimal transport: Computational complexity and barycenter computation. _Advances in Neural Information Processing Systems_, 34:21947-21959, 2021.
* Li et al. [2023] Jiajin Li, Jianheng Tang, Lemin Kong, Huikang Liu, Jia Li, Anthony Man-Cho So, and Jose Blanchet. A convergent single-loop algorithm for relaxation of gromov-wasserstein in graph data. In _The Eleventh International Conference on Learning Representations_, 2023.
* Memoli [2011] Facundo Memoli. Gromov-Wasserstein distances and the metric approach to object matching. _Foundations of Computational Mathematics_, 11(4):417-487, 2011.
* Memoli et al. [2019]Gonzalo Mena and Jonathan Niles-Weed. Statistical bounds for entropic optimal transport: sample complexity and the central limit theorem. _Advances in Neural Information Processing Systems_, 32, 2019.
* Nitzan et al. (2019) Mor Nitzan, Nikos Karaiskos, Nir Friedman, and Nikolaus Rajewsky. Gene expression cartography. _Nature_, 576(7785), 2019.
* Oquab et al. (2023) Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. _arXiv preprint arXiv:2304.07193_, 2023.
* Peyre et al. (2016) Gabriel Peyre, Marco Cuturi, and Justin Solomon. Gromov-wasserstein averaging of kernel and distance matrices. In _International Conference on Machine Learning_, pages 2664-2672, 2016.
* Peyre and Cuturi (2019) Gabriel Peyre and Marco Cuturi. Computational optimal transport. _Foundations and Trends in Machine Learning_, 11(5-6), 2019. ISSN 1935-8245.
* Pham et al. (2020) Khiem Pham, Khang Le, Nhat Ho, Tung Pham, and Hung Bui. On unbalanced optimal transport: An analysis of sinkhorn algorithm. In _International Conference on Machine Learning_, pages 7673-7682. PMLR, 2020.
* Pinho et al. (2018) Ana Luisa Pinho, Alexis Amadon, Torsten Ruest, Murielle Fabre, Elvis Dohmatob, Isabelle Denghien, Chantal Ginisty, Severine Becuwe-Desmidt, Severine Roger, Laurence Laurier, Veronique Joly-Testault, Gaelle Mediouni-Cloarec, Christine Double, Bernadette Martins, Philippe Pinel, Evelyn Eger, Gael Varoquaux, Christophe Pallier, Stanislas Dehaene, Lucie Hertz-Pannier, and Bertrand Thirion. Individual Brain Charting, a high-resolution fMRI dataset for cognitive mapping. _Scientific Data_, 5:180105, June 2018. doi: 10.1038/sdata.2018.105. URL [https://hal.science/hal-01817528](https://hal.science/hal-01817528).
* Rockafellar (1970) R Tyrrell Rockafellar. _Convex analysis_. Princeton university press, 1970.
* Rubner et al. (2000) Yossi Rubner, Carlo Tomasi, and Leonidas J. Guibas. The earth mover's distance as a metric for image retrieval. _International Journal of Computer Vision_, 40(2):99-121, November 2000.
* Sander et al. (2022) Michael E Sander, Pierre Ablin, Mathieu Blondel, and Gabriel Peyre. Sinkformers: Transformers with doubly stochastic attention. In _International Conference on Artificial Intelligence and Statistics_, pages 3515-3530. PMLR, 2022.
* Sato et al. (2020) Ryoma Sato, Marco Cuturi, Makoto Yamada, and Hisashi Kashima. Fast and robust comparison of probability measures in heterogeneous spaces. _arXiv preprint arXiv:2002.01615_, 2020.
* Scetbon and Cuturi (2020) Meyer Scetbon and Marco Cuturi. Linear time sinkhorn divergences using positive features. _Advances in Neural Information Processing Systems_, 33:13468-13480, 2020.
* Scetbon and Cuturi (2022) Meyer Scetbon and Marco Cuturi. Low-rank optimal transport: Approximation, statistics and debiasing. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, _Advances in Neural Information Processing Systems_, volume 35, pages 6802-6814. Curran Associates, Inc., 2022. URL [https://proceedings.neurips.cc/paper_files/paper/2022/file/2d69e771d9f274f7c624198ea74f5b98-Paper-Conference.pdf](https://proceedings.neurips.cc/paper_files/paper/2022/file/2d69e771d9f274f7c624198ea74f5b98-Paper-Conference.pdf).
* Scetbon et al. (2021) Meyer Scetbon, Marco Cuturi, and Gabriel Peyre. Low-rank sinkhorn factorization. In _International Conference on Machine Learning_, pages 9344-9354. PMLR, 2021.
* Scetbon et al. (2022) Meyer Scetbon, Gabriel Peyre, and Marco Cuturi. Linear-time Gromov-Wasserstein distances using low rank couplings and costs. _ICML_, 2022.
* Schiebinger et al. (2019) Geoffrey Schiebinger, Jian Shu, Marcin Tabaka, Brian Cleary, Vidya Subramanian, Aryeh Solomon, Joshua Gould, Siyan Liu, Stacie Lin, Peter Berube, et al. Optimal-transport analysis of single-cell gene expression identifies developmental trajectories in reprogramming. _Cell_, 176(4):928-943, 2019.
* Sejourne et al. (2021) Thibault Sejourne, Francois-Xavier Vialard, and Gabriel Peyre. The unbalanced gromov wasserstein distance: Conic formulation and relaxation. _Advances in Neural Information Processing Systems_, 34:8766-8779, 2021.
* Sejourne et al. (2021)Thibault Sejourne, Francois-Xavier Vialard, and Gabriel Peyre. Faster unbalanced optimal transport: Translation invariant sinkhorn and 1-d frank-wolfe. In _International Conference on Artificial Intelligence and Statistics_, pages 4995-5021. PMLR, 2022.
* Shi et al. (2022) Hailing Shi, Yichun He, Yiming Zhou, Jiahao Huang, Brandon Wang, Zefang Tang, Peng Tan, Morgan Wu, Zuwan Lin, Jingyi Ren, Yaman Thapa, Xin Tang, Albert Liu, Jia Liu, and Xiao Wang. Spatial atlas of the mouse central nervous system at molecular resolution. _bioRxiv_, 2022. doi: 10.1101/2022.06.20.496914. URL [https://www.biorxiv.org/content/early/2022/06/22/2022.06.20.496914](https://www.biorxiv.org/content/early/2022/06/22/2022.06.20.496914).
* Solomon et al. (2016) Justin Solomon, Gabriel Peyre, Vladimir G Kim, and Suvrit Sra. Entropic metric alignment for correspondence problems. _ACM Transactions on Graphics (TOG)_, 35(4):1-13, 2016.
* Tay et al. (2020) Yi Tay, Dara Bahri, Liu Yang, Donald Metzler, and Da-Cheng Juan. Sparse Sinkhorn attention. In Hal Daume III and Aarti Singh, editors, _Proceedings of the 37th International Conference on Machine Learning_, volume 119 of _Proceedings of Machine Learning Research_, pages 9438-9447. PMLR, 13-18 Jul 2020. URL [https://proceedings.mlr.press/v119/tay20a.html](https://proceedings.mlr.press/v119/tay20a.html).
* Thual et al. (2022) Alexis Thual, Huy Tran, Tatiana Zemskova, Nicolas Courty, Remi Flamary, Stanislas Dehaene, and Bertrand Thirion. Aligning individual brains with fused unbalanced Gromov-Wasserstein. _arXiv_, 2022.
* Vayer et al. (2018) Titouan Vayer, Laetita Chapel, Remi Flamary, Romain Tavenard, and Nicolas Courty. Fused gromov-wasserstein distance for structured objects: theoretical foundations and mathematical properties. _arXiv preprint arXiv:1811.02834_, 2018.
* Vincent-Cuaz et al. (2023) Cedric Vincent-Cuaz, Remi Flamary, Marco Corneli, Titouan Vayer, and Nicolas Courty. Semi-relaxed gromov-wasserstein divergence and applications on graphs. In _International Conference on Learning Representations_, 2023.
* Wolf et al. (2018) F Alexander Wolf, Philipp Angerer, and Fabian J Theis. SCANPY: large-scale single-cell gene expression data analysis. _Genome Biology_, 19(1), 2018.
* Xu et al. (2019) Hongteng Xu, Dixin Luo, and Lawrence Carin. Scalable gromov-wasserstein learning for graph partitioning and matching. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alche-Buc, E. Fox, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 32. Curran Associates, Inc., 2019. URL [https://proceedings.neurips.cc/paper_files/paper/2019/file/6e62a992c676f6116097dbea8ea030-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2019/file/6e62a992c676f6116097dbea8ea030-Paper.pdf).