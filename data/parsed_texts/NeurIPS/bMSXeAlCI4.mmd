# Entropy testing and its application to testing Bayesian networks

 Clement L. Canonne

University of Sydney

clement.canonne@sydney.edu.au &Joy Qiping Yang

University of Sydney

qyan6238@uni.sydney.edu.au

###### Abstract

This paper studies the problem of _entropy identity testing_: given sample access to a distribution \(p\) and a fully described distribution \(q\) (both discrete distributions over a domain of size \(k\)), and the promise that either \(p=q\) or \(|H(p)-H(q)|\geqslant\varepsilon\), where \(H(\cdot)\) denotes the Shannon entropy, a tester needs to distinguish between the two cases with high probability. We establish a near-optimal sample complexity bound of \(\tilde{\Theta}(\sqrt{k}/\varepsilon+1/\varepsilon^{2})\) for this problem, and show how to apply it to the problem of identity testing for in-degree-\(d\)\(n\)-dimensional Bayesian networks, obtaining an upper bound of \(\tilde{O}(2^{d/2}n^{3/2}/\varepsilon^{2}+n^{2}/\varepsilon^{4})\). This improves on the sample complexity bound of \(\tilde{O}(2^{d/2}n^{2}/\varepsilon^{4})\) from [1], which required an additional assumption on the structure of the (unknown) Bayesian network.

## 1 Introduction

Entropy is a fundamental information theory notion, which quantifies the amount of "uncertainty" a given random variable carries. Since its introduction by Shannon, this notion has found myriads of applications, and is central - among others - to compression and coding, probability, electrical engineering, and learning theory.

As a result, the task of _estimating_ the Shannon entropy of a discrete random variable (or, equivalently, its probability distribution) from samples has naturally emerged, starting (in Computer Science) with the work of [1] which considered _multiplicative_ approximations. _Additive_ approximation of the entropy (within \(\pm\varepsilon\)) was then considered in a series of papers [21, 22, 23, 24, 25], culminating with the work of [24], which establishes the optimal sample complexity, \(\Theta\big{(}\frac{k}{\varepsilon\log k}+\frac{\log^{2}k}{\varepsilon^{2}} \big{)}\), where \(k\gg 1\) is the domain size.

While the resulting sample complexity is _sublinear_ in the domain size \(k\), it is only so by a mere logarithmic factor. In some settings, paying this near-linear dependence in the amount of data necessary is impractical, typically in the large-domain regime (e.g., for high-dimensional data, where \(k\) is exponential in the dimension); moreover, it may even be _unnecessary_. Specifically, one may not be concerned so much about the (approximate) value of the entropy of a distribution, but rather about whether it is above a threshold, or differs from that of a given purported model.

It is this latter task we introduce and consider in our work, which can be seen as a variant of the standard _identity testing_ question from distribution testing: given a reference known hypothesis distribution \(q\) over a domain of size \(k\), and i.i.d. samples from an unknown distribution \(p\), what is the sample complexity of testing whether \(p\) is equal to \(q\), or their entropies differ significantly? And, crucially, _is this testing task more sample-efficient than that of estimating \(H(p)\)?_

**Entropy Identity testing:** Given a reference distribution \(q\), parameter \(\varepsilon>0\), and samples from an unknown \(p\), what is the cost of deciding (with high probability) whether \(p=q\) vs. \(|H(p)-H(q)|>\varepsilon\), with correct probability at least \(2/3\)?

Note that in the case where \(q\) is the uniform distribution over the domain, this task is equivalent to distinguishing between \(H(p)=\log k\) and \(H(p)<\log k-\varepsilon\).

Our main contribution is to show that the testing question can indeed be performed much more efficiently than the estimation one, at least for most parameter regimes. Specifically, we establish the following theorem:

**Theorem 1.1**.: _The sample complexity of entropy identity testing is \(O(\sqrt{k\log(k/\varepsilon)}/\varepsilon+\log^{2}(k)/\varepsilon^{2})\). Moreover, this is nearly tight: \(\Omega(\sqrt{k}/\varepsilon+\log^{2}k/\varepsilon^{2})\) samples are necessary in the worst case._

Interestingly, this differs both from the _estimation_ task (which, as discussed before, has a near-linear dependence on the domain size \(k\)) but also from identity testing _in total variation distance_, which has sample complexity \(\Theta(\sqrt{k}/\varepsilon^{2})\) (see Section 1.1).

**Application: Identity testing for Bayesian networks.** As an application of Theorem 1.1, we derive an efficient algorithm for identity testing (in total variation distance) for maximum in-degree \(d\) Bayesian networks (shorten as degree-\(d\) Bayes net in the remaining of the paper):1

Footnote 1: Our algorithm actually provides a stronger guarantee, with respect to Hellinger distance, which implies the TV result as \(d_{\mathrm{TV}}(p,q)\leq\sqrt{2}\,\mathrm{d}_{\mathrm{H}}(p,q)\) for any two distributions \(p,q\).

**Theorem 1.2** (Informal; see Theorem 3.1).: _There is an algorithm which, given sample access to a degree-\(d\) Bayes net \(p\) and the full description of a reference degree-\(d\) Bayes net \(q\) (both over \(\{0,1\}^{n}\)), takes \(\tilde{O}\left(\frac{2^{d/2}n^{3/2}}{\varepsilon^{2}}+\frac{n^{2}}{\varepsilon^ {4}}\right)\) samples from \(p\), and distinguishes between \(p=q\) and \(d_{\mathrm{TV}}(p,q)\geq\varepsilon\)._

Prior to this, the best known sample complexity upper bound for this task [1] was quadratically worse in both \(n\) and \(\varepsilon\), and further required an assumption on the underlying graph structure of both \(p\) and \(q\). We emphasize that (1) our result improves on the sample complexity of the learning baseline for \(d\gg\log(n/\varepsilon)\), and on its computational efficiency; and (2) compared to the previous testing results, removes strong structural assumptions which considerably limited their applicability. We elaborate on this in the next section.

### Related work

As previously discussed, entropy estimation has received a considerable amount of interest from computer scientists, information theorists and statisticians [1, 2, 1, 1]. Entropy is also a key example of _symmetric property_ (invariant to relabeling of the domain) [1, 1, 2, 1], and has been considered in other settings as well, e.g., the quantum case [1, 1] and the memory-limited setting [1, 2]. Estimation of some generalizations of Shannon entropy, such as the family of Renyi entropies, also have been studied [1].

Over the years, sample complexity of identity testing for discrete distribution has been intensively studied and essentially settled [2, 1, 2]. In high dimensions, however, the square root dependence of the sample complexity on the domain size means that most identity testing tasks of interest require sample complexity exponential in the dimension. Moreover, this curse of dimensionality extends to a large range of distribution testing problems [1, Theorem B.1]. As such, many turn to the study of testing distributions under additional natural structural assumptions, such as graphical models: [1] look at identity testing for product distributions (degree-\(0\) Bayes nets) and give the optimal bound of \(\Theta(\sqrt{n|\Sigma|/\varepsilon^{2}})\), where \(|\Sigma|\) is the alphabet size of each variable (rather than binary alphabet studied in our paper). [1, 1] study testing Ising models, obtaining sample complexity bounds that are \(\mathrm{poly}(n/\varepsilon)\); [2], [1] give tight results to identity testing and closeness testing for a variety of constant in-degree Bayes nets, which also gives polynomial sample complexity bounds.

However, the testing algorithms provided in [1] and [2] are not fully satisfactory, as they require some strong assumptions on Bayes nets. Specifically, [1, Theorem 21] assumes thatthe topological ordering of the two Bayes nets are the same, and shows that under this assumption \(O(2^{d/2}n^{2}/\varepsilon^{4})\) samples are sufficient.2[3, Theorem 17] makes the further stringent restriction that the reference Bayes net has to be _balanced_, i.e., that the conditional probabilities are all bounded away from 0 and 1; moreover, it also requires every parental configuration to be bounded from 0, and that the structure of the unknown Bayes net be a subset of that of the reference one. The result of [3, Theorem 4.2] combined with the Hellinger tester from [13, Theorem 1] implies that, under the assumption that \(p\) and \(q\) share the same factorization structure (i.e., their associated DAGs are the same or one is a subgraph of the other), then this problem is solvable in \(\tilde{O}\left(2^{d/2}n/\varepsilon^{2}\right)\) samples. While this latter sample complexity is near-optimal (in some regime3), in view of the \(\Omega\left(2^{d/2}n/\varepsilon^{2}\right)\) lower bound obtained in [1, Theorem 4.1], the factorization structure requirement considerably limits the applicability of the algorithm.

Footnote 2: While the sample complexity of the algorithm is not explicitly stated in their proof, inspection of their argument yields this bound.

Footnote 3: The lower bound [1, Theorem 4.1] only holds under the sparse regime: \(d\ll\log n\).

One can also compare our result to the _learning_ results on Bayesian networks, as any learning algorithms enables testing as well (the "testing-by-learning" baseline). It is known [3] that learning degree-\(d\) Bayes nets can be done with \(\tilde{O}(2^{d}n/\varepsilon^{2})\) samples, without any structural assumptions. Our testing result improves on this sample complexity as long as \(n^{2}/\varepsilon^{4}\ll 2^{d}n/\varepsilon^{2}\) and \(2^{d/2}n^{3/2}\ll 2^{d}n\), i.e., for \(d\gg\log(n/\varepsilon)\); moreover, it is worth noting that the known learning algorithms are computationally inefficient (running in time \(n^{O(dn)}\) via an enumeration of all possible underlying graph structures [3, 1]), and this is believed to be inherent [1]. In contrast, our algorithm runs in time \(\mathrm{poly}(n^{d},1/\varepsilon)\).

### Techniques overview

**Testing in entropy.** A first idea is to use the conversion between total variation (TV) distance and entropy difference to reduce this problem to identity testing in \(\mathrm{TV}\): When \(d_{\mathrm{TV}}(p,q)\leqslant 1/2\), then \(|H(p)-H(q)|\leqslant d_{\mathrm{TV}}(p,q)\log\frac{k}{d_{\mathrm{TV}}(p,q)}\)[3, Lemma 2.7].4 This gives an upper bound of \(O(\frac{\sqrt{k}\log^{2}(k/\varepsilon)}{\varepsilon^{2}})\), which is already better than the sample complexity of estimation: \(O\left(\frac{k}{\varepsilon\log k}+\frac{\log^{2}k}{\varepsilon^{2}}\right)\) for the parameter \(k\). However, it is not clear whether the quadratic dependence on \(\varepsilon\) is necessary: indeed, the "hard instances" for TV testing (the Paninski construction [14]), small perturbations around the uniform distribution which have TV distance \(\varepsilon\) from uniform, actually only have entropy \(\log k-\Theta(\varepsilon^{2})\). The \(\Omega(\sqrt{k}/\varepsilon^{2})\) uniformity testing lower bound from these hard instances thus only implies an \(\Omega(\sqrt{k}/\varepsilon)\) entropy identity testing lower bound!

Footnote 4: All logarithms in the paper are natural (\(e\) as base).

A next natural idea is to strengthen the lower bound. However, it then becomes clear that the Paninski [14] construction cannot be improved: as just mentioned, when its \(\mathrm{TV}\) distance to the uniform distribution is around \(\Theta(\sqrt{\varepsilon})\) its entropy difference to it is only \(\Theta(\varepsilon)\) (giving an \(\Omega(\sqrt{k}/\varepsilon)\) lower bound). Moreover, this is not a coincidence: when the reference distribution \(q\) is uniform, we are able to get a matching upper bound using [13, Algorithm 1], upon noticing that

\[H(p)=\log k-d_{\mathrm{KL}}(p\|u_{k}),\] (1)

which implies \(d_{\mathrm{KL}}(p\|u_{k})=\log k-H(p)\geqslant\varepsilon\), where \(u_{k}\) is the uniform distribution on \([k]\) and \(d_{\mathrm{KL}}\) denotes the Kullback-Leibler divergence. Interestingly, a completely different hard instance, against a very much non-uniform reference distribution, does yield the second term of our lower bound, \(\Omega(\log^{2}k/\varepsilon^{2})\).

Inspired by these two different lower bounds, we can generalize (1) by defining \(\mathcal{A}\) as the set of "not too small probability elements under \(q\)", and then observing (looking ahead, using the inequality (7)) that

\[|H(p_{\mathcal{A}})-H(q_{\mathcal{A}})|\leqslant|d_{\mathrm{KL}}(p_{\mathcal{A }}\|q_{\mathcal{A}})|+\left|\sum_{i\in\mathcal{A}}(p_{i}-q_{i})\log\frac{1}{q_{ i}}\right|\] (2)

where \(H(p_{\mathcal{A}})\) is the "entropy" of the sub-distribution restricted to the set \(\mathcal{A}\). In particular, this hints that one could solve the general problem by testing if either of the two terms on the right-hand-sideis large. The name of the game now is to (i) choose the threshold for \(\mathcal{A}\) (i.e., what does it mean for an element to have "not too small probability under \(q\)"), and (ii) have algorithms to test whether these two quantities are noticeably large.

Let us focus on how to test the first term of (2). If \(\min_{i}q_{i}\geqslant\Omega\left(\frac{\varepsilon}{k}\right)\), we can adapt and use an algorithm of [1] to efficiently test \(d_{\mathrm{KL}}(p\|q)\geqslant\varepsilon\) vs. \(p=q\). In addition, if \(\log(1/q_{i})\) is bounded, then in fact, estimating the second term to \(O(\varepsilon)\) is possible as well. Thus it is natural to wonder if we can afford to neglect the region where \(q_{i}\leqslant\frac{\varepsilon}{k}\). Indeed, the impact on entropy is at most \(O(\tau\log(k/\tau))\) if we are to remove regions with at most \(O(\tau)\) as mass. Thus, by adjusting the appropriate threshold, we can still detect difference in entropy even if we only test on elements with greater than \(\tau/k\) masses, where \(\tau=\frac{\varepsilon}{\log(k/\varepsilon)}\).

The problem then becomes to check if \(p\) puts more than \(100\tau\) mass in \(\bar{\mathcal{A}}=\{i\in[k]:q_{i}<\tau/k\}\), which costs \(O(1/\tau)=O(\log(k/\varepsilon)/\varepsilon)\) samples. If it does, then it cannot be the case that \(p=q\); we can reject. After this stage, both \(p(\bar{\mathcal{A}}),q(\bar{\mathcal{A}})\leqslant O(\tau)\). To move forward, we need to check the influence on entropy: \(H(p)\) and \(H(q)\). By Jensen's inequality and monotonicity of \(f(x)=x\log\frac{1}{x}\) when \(x<\frac{1}{e}\), we have

\[\sum_{i\in\bar{\mathcal{A}}}p_{i}\log\frac{1}{p_{i}}\leqslant p(\bar{ \mathcal{A}})\log\frac{k}{p(\bar{\mathcal{A}})}\leqslant\tau\log\frac{k}{\tau}.\]

Therefore, the impact on entropy will be at most \(O\left(\tau\log\frac{k}{\tau}\right)\). Setting \(\tau=\frac{\varepsilon}{\log(k/\varepsilon)}\), this becomes \(O(\varepsilon)\), which gives us the room to check if \(|H(p_{\mathcal{A}})-H(p_{\mathcal{A}})|\geqslant 100\varepsilon\) or \(p_{\mathcal{A}}=q_{\mathcal{A}}\).

Testing Bayesian networks.Similar to [1, Theorem 4.2],5 the identity testing algorithm is straight-forward: check all every \(i\in[n]\), if \(p_{X_{i},\Pi_{i}^{G}}=q_{X_{i},\Pi_{i}^{G}}\) or is one of them is far apart, where \(q\) is Markov with respect to \(G\) (\(q\) factorizes according the DAG \(G\)). The main technical part is to show that the distance is "subaddiitve" when \(p\) and \(q\) share no common structure, but are close to sharing a common factorization structure (this can be thought of as a relaxation of [1, Theorem 4.2]; refer to Lemma 3.3 for details). As a consequence of "subadditivity", if \(p\) and \(q\) are far in distributional distance (differed from [1], our work opted to test in KL divergence restricted on subsets with large enough density), then it would imply that one of the local distance between \(p\) and \(q\) is sufficiently large. This allows us to reduce from global testing to local testing.

Footnote 5: We note that what they refer to as “identity testing” is different from ours (and the standard) use of the term: in their setting, the reference distribution is replaced with sample access to the distribution (this is commonly referred to as “closeness testing”).

Another key aspect is checking whether \(p\) and \(q\) are close to sharing common structure. More specifically, whether \(d_{\mathrm{KL}}(p\|p_{G})\) is small, where \(p_{G}\) is the projection of \(p\) unto \(q\)'s DAG \(G\). Here, we establish a connection between entropy closeness and structure closeness. In particular, we show that if every local entropy (involving subsets of size \(d+1\), where \(d\) is the bound on maximum in-degree) between \(p\) and \(q\) is close, then this means that they must approximately share the same structure (see Lemma 3.4). The intuition behind the connection is that if all tests pass, then we can conclude that \(p\) and \(q\) are close in local entropy and thereafter, we can utilize entropy of \(q\) to learn the graphical structure [1] of \(p\) (which uses no additional samples).

At a high level, our algorithm first check if \(p\) and \(q\) roughly share the same structure via a proxy check of local entropy tests. If all local entropy tests pass, then we can show that there exists \(i\in[n]\) such that local KL restricted on subset with large enough mass is greater than \(\Omega\left(\frac{\varepsilon^{2}}{n}\right)\). A subsequent identity test with \(\chi^{2}\)-test [1] suffice.

Preliminaries and notation.The (Shannon) entropy \(H\) of a discrete distribution \(p\) supported on \([k]\) is given by:

\[H(p)=-\sum_{i\in[k]}p_{i}\log p_{i}.\]

The conditional entropy \(H(p_{X}\mid p_{Y})\) for \(X\) supported on \(\mathcal{X}\), and \(Y\) on \(\mathcal{Y}\), defined by the joint distribution \(p_{X,Y}\), can be written as

\[H(p_{X}\mid p_{Y})=-\sum_{x\in\mathcal{X},y\in\mathcal{Y}}p(x,y)\log\frac{p(x,y )}{p(y)}=H(p_{X,Y})-H(p_{Y}).\] (3)We adopt the entropy notation for a sub-probability vector \(H(q_{\mathcal{A}})=\sum_{i\in\mathcal{A}}q_{i}\log\frac{1}{q_{i}}\). Throughout this paper, we will use \(e\) as base of the log and of the entropy. We will use \(\leftarrow\) for variable assignment. We adopt the standard \(O(\cdot)\), \(\Omega(\cdot)\) and \(\Theta(\cdot)\) asymptotic notation and use \(\overline{\cdot}\) to hide any polylogarithmic factors in the argument. We will use various metrics or divergences on probability distributions: Kullback-Leibler (\(d_{\mathrm{KL}}\)), Hellinger (\(d_{\mathrm{H}}\)), chi-squared (\(d_{\chi^{2}}\)), and total variation (\(d_{\mathrm{TV}}\)). We denote \(p_{\mathcal{A}}\) as restricting \(p\) onto the elements in \(\mathcal{A}\), and we denote distributional distances restricting on \(\mathcal{A}\) as follows: \(d_{\mathrm{KL}}(p_{\mathcal{A}},q_{\mathcal{A}})=\sum_{i\in\mathcal{A}}p_{i} \log\frac{p_{i}}{q_{i}}\). \(\mathrm{d}_{\mathrm{H}}(p_{\mathcal{A}},q_{\mathcal{A}})=\frac{1}{\sqrt{\sum_ {i\in\mathcal{A}}\left(\sqrt{p_{i}}-\sqrt{q_{i}}\right)^{2}}}\). For a set \(\mathcal{A}\), we write \(p(\mathcal{A})=\sum_{i\in\mathcal{A}}p_{i}\). We also have the following inequality [13, Proposition 1]:

\[d_{\mathrm{TV}}(p_{\mathcal{A}},q_{\mathcal{A}})\leqslant\sqrt{2}\,\mathrm{d }_{\mathrm{H}}(p_{\mathcal{A}},q_{\mathcal{A}})\leqslant\sqrt{\sum_{i\in \mathcal{A}}(q_{i}-p_{i})+d_{\mathrm{KL}}(p_{\mathcal{A}},q_{\mathcal{A}})} \leqslant\sqrt{d_{\chi^{2}}(p_{\mathcal{A}},q_{\mathcal{A}})}.\] (4)

A distribution \(p\) supported over the hypercube \(\{0,1\}^{n}\) is a Bayesian network if its probability mass function satisfies the factorization associated with \(G\), a directed acyclic graph (DAG):

\[p(x_{1},\cdots,x_{n})=\prod_{i=1}^{n}p(x_{i}|\Pi_{i}),\] (5)

and \(\Pi_{i}\) is the set of parents of \(X_{i}\) in \(G\); and we say that \(p\) is Markov with respect to DAG \(G\). In section 3, slightly abusing notation, we use \(p_{G}\) to denote a projection of a Bayes net \(p\) to a DAG \(G\) (which it may or may not be Markov with respect to; see Definition 3.2). We work in the Poissonized setting (see, e.g., [1, Appendix C]) - instead of drawing \(N\) samples directly from \(p\), we draw \(Y\sim\mathrm{Poi}(N)\) samples from \(p\), where \(\mathrm{Poi}(N)\) denotes the random variable distributed as the Poisson distribution with parameter \(N\). The Poissonized and usual sampling settings are equivalent for constant probability of failure, up to a (small) multiplicative factor in the sample complexity.

## 2 Near-optimal entropy testing

We prove Theorem 1.1, establishing the sample complexity upper and lower bounds separately.

An \(O\big{(}\frac{\sqrt{k\log(k/\varepsilon)}}{\varepsilon}+\frac{\log^{2}(k)}{ \varepsilon^{2}}\big{)}\) upper bound

We will prove the following theorem:

**Theorem 2.1**.: _There is an algorithm (Algorithm 1) which, given \(n\) samples from a discrete distribution \(p\), the full description of a reference distribution \(q\), both over \([k]\), and parameter \(\varepsilon>0\), distinguishes between \(p=q\) and \(|H(p)-H(q)|\geqslant\varepsilon\) with probability at least \(2/3\), as long as_

\[n\geq c_{1}\left(\frac{\sqrt{k\log(k/\varepsilon)}}{\varepsilon}+\frac{\log^{ 2}(k)}{\varepsilon^{2}}\right)\]

_and \(c_{2}\varepsilon\leqslant k\), for some absolute constants \(c_{1},c_{2}>0\). Moreover, the algorithm runs in time linear in the number of samples \(n\) and the domain size \(k\)._

The proof will rely on the two following claims and Lemma 2.4, which is a straightforward extension of [13, Lemma 2]. Their proofs are deferred to Appendix B. Throughout, we let \(\tau:=\frac{1}{16\log(k/\varepsilon)}\), and \(\mathcal{A}:=\big{\{}i\in[k]\mid q_{i}\geqslant\frac{\tau}{k}\big{\}}\), as in Algorithm 1.

**Claim 2.2**.: _Let \(\mathcal{A}\) be any set such that \(p(\bar{\mathcal{A}})<\varepsilon/2\). Then, if \(|H(p_{\mathcal{A}})-H(q_{\mathcal{A}})|\geqslant\varepsilon\), we must have (i) \(d_{\mathrm{KL}}(p_{\mathcal{A}}\|q_{\mathcal{A}})\geqslant\frac{\varepsilon}{2}\) or (ii) \(|\sum_{i\in\mathcal{A}}(p_{i}-q_{i})\log(\frac{1}{q_{i}})|\geqslant\frac{ \varepsilon}{2}\)._

**Claim 2.3**.: _Let \(\hat{p}\) be the empirical estimator for an unknown discrete distribution \(p\) supported on \([k]\), based on \(\mathrm{Poi}(m)\) samples, where \(m=\Theta\Big{(}\frac{\log^{2}(k)}{\varepsilon^{2}}\Big{)}\); assume that \(d_{\chi^{2}}(p_{\mathcal{A}},q_{\mathcal{A}})\leqslant\varepsilon/8\) and \(p(\bar{\mathcal{A}})+q(\bar{\mathcal{A}})\leqslant 4\tau=\frac{1}{4}\frac{ \varepsilon}{\log(k/\varepsilon)}\),6 then_

Footnote 6: One can remove the assumption that \(p(\bar{\mathcal{A}})+q(\bar{\mathcal{A}})\leqslant 4\tau\), at the cost of a slightly worse constant.

\[\Pr\left[\left|\sum_{i\in\mathcal{A}}(p_{i}-\hat{p}_{i})\log\frac{1}{q_{i}} \right|\geqslant\frac{1}{8}\varepsilon\right]\leqslant\frac{1}{100}.\]

**Lemma 2.4**.: _Let \(\mathcal{A}:=\{i\in[k]\mid q_{i}\geqslant\alpha\}\). Let \(m_{2}\geqslant 16384\max\left\{\sqrt{\frac{1}{\alpha\varepsilon}},\frac{\sqrt{k}}{ \varepsilon}\right\}\) be the number of samples used to compute \(Z_{2}\). Then \(\mathbb{E}[Z_{2}]=m_{2}d_{\chi^{2}}(p_{\mathcal{A}},q_{\mathcal{A}})\). Moreover, if \(d_{\chi^{2}}(p_{\mathcal{A}},q_{\mathcal{A}})\leqslant\frac{\varepsilon}{2}\), then \(\operatorname{Var}[Z_{2}]\leqslant(\frac{1}{32}m_{2}\varepsilon)^{2}\). If \(d_{\chi^{2}}(p_{\mathcal{A}},q_{\mathcal{A}})\geqslant\varepsilon\), then \(\operatorname{Var}[Z_{2}]\leqslant O(\mathbb{E}[Z_{2}]^{2})\)._

Proof of Theorem 2.1.: We prove the statement by analyzing Algorithm 1. First, note that excluding the set of \(\bar{\mathcal{A}}\) (elements with small mass), can change the value of \(H(q)\) by at most \(\varepsilon/8\): indeed, by Jensen's inequality (\(f(x)=\log x\) is concave) and \(x\log\frac{1}{x}\) being monotonically increasing in \((0,1/e)\),

\[H(q_{\bar{\mathcal{A}}})=\sum_{i\in\mathcal{A}}q_{i}\log\frac{1}{q_{i}} \leqslant q(\bar{\mathcal{A}})\log\frac{|\bar{\mathcal{A}}|}{q(\bar{\mathcal{ A}})}\leqslant\tau\log\frac{k}{\tau}=\frac{\varepsilon}{16\log(k/\varepsilon)}\log \left(\frac{16k}{\varepsilon/\log(k/\varepsilon)}\right)\leqslant\frac{1}{8}\varepsilon,\]

when \(\tau\leqslant 1/e\). Similarly, if \(p(\bar{\mathcal{A}})\leqslant 3\tau\), we have that \(H(p_{\bar{\mathcal{A}}})\leqslant\frac{3}{8}\varepsilon\). Therefore,

\[\varepsilon\leqslant|H(p)-H(q)| \leqslant |H(p_{\mathcal{A}})-H(q_{\mathcal{A}})|+|H(p_{\bar{\mathcal{A}}} )-H(q_{\bar{\mathcal{A}}})|\] \[\leqslant |H(p_{\mathcal{A}})-H(q_{\mathcal{A}})|+|H(p_{\bar{\mathcal{A}}} )|+|H(q_{\bar{\mathcal{A}}})|\] \[\leqslant |H(p_{\mathcal{A}})-H(q_{\mathcal{A}})|+\frac{1}{2}\varepsilon.\]

For Line 4, we prove the following: with probability at least \(99/100\), if \(Z_{1}\geqslant 2\tau\), then \(p(\bar{\mathcal{A}})\geqslant\tau\); and if \(Z_{1}<2\tau\), then \(p(\bar{\mathcal{A}})<3\tau\) (this is a standard technique; see e.g., [1, Fact 2.2].) For the sake of completeness we include the full derivation in the Appendix A.

**After Line 4 of Algorithm 1.** We conclude from the above that

1. \(\mathcal{A}\) still has sufficient entropy gap to test on: \(|H(p_{\mathcal{A}})-H(q_{\mathcal{A}})|\geqslant\frac{1}{2}\varepsilon\).
2. With probability at least \(99/100\), when \(p=q\), it will not be rejected in Algorithm 4 of Line 4; and once it is pass through this stage, we have \(p(\bar{\mathcal{A}})\leqslant 3\tau\).

**Completeness: when \(p=q\).**

* We have that \(d_{\chi^{2}}(p_{\mathcal{A}},q_{\mathcal{A}})=0\), and via Lemma 2.4, we know that \(\mathbb{E}[Z_{2}]=0\) and \(\operatorname{Var}[Z_{2}]\leqslant\frac{1}{32^{2}}m_{2}^{2}\varepsilon^{2}\). By Chebyshev's inequality, \[\Pr\left[\left|Z_{2}-\mathbb{E}[Z_{2}]\right|\geqslant 2\sqrt{ \operatorname{Var}[Z_{2}]}\right]\leqslant\frac{1}{4},\quad\text{ and so }\quad\Pr[Z_{2}\geqslant 2\cdot\frac{1}{32}m_{2}\varepsilon+\mathbb{E}[Z_{2}]] \leqslant\frac{1}{4};\] and we have \(\Pr[Z_{2}\geqslant\frac{1}{16}m_{2}\varepsilon]\leqslant\frac{1}{4}\).

* On the other hand, by Claim 2.3, setting \(m_{3}=\frac{140800\log^{2}(k)}{\varepsilon^{2}}\), we have that with probability at least \(99/100\), \[Z_{3}=\left|\sum_{i\in\mathcal{A}}(\hat{p}_{i}-q_{i})\log\frac{1}{q_{i}}\right| =\left|\sum_{i\in\mathcal{A}}(\hat{p}_{i}-p_{i})\log\frac{1}{q_{i}}\right|\leqslant \frac{1}{8}\varepsilon.\] Therefore, with probability at least \(1-\frac{1}{4}-\frac{2}{100}=\frac{73}{100}>\frac{2}{3}\), the tester will accept.

**Soundness: when \(|H(p)-H(q)|\geqslant\varepsilon\).** If \(p(\bar{\mathcal{A}})\geqslant 3\tau\) then \(\hat{p}(\bar{\mathcal{A}})\geqslant 2\tau\) with probability \(99/100\), and the algorithm will output Reject. We proceed assuming \(p(\bar{\mathcal{A}})\leqslant 3\tau\) and recall Item ii. from before, we have \(|H(p_{\mathcal{A}})-H(q_{\mathcal{A}})|\geqslant\frac{1}{2}\varepsilon\). By Claim 2.2, we have that either \(d_{\mathrm{KL}}(p_{\mathcal{A}},q_{\mathcal{A}})\geqslant\frac{1}{4}\varepsilon\) or \(\left|\sum_{i\in\mathcal{A}}(p_{i}-q_{i})\log\left(1/q_{i}\right)\right| \geqslant\frac{1}{4}\varepsilon\). We apply Lemma 2.4, setting \(\alpha=\tau/k\) and \(m_{2}\geqslant 65536\sqrt{k\log(k/\varepsilon)}/\varepsilon\).

* If \(d_{\mathrm{KL}}(p_{\mathcal{A}}\|q_{\mathcal{A}})\geqslant\frac{1}{4}\varepsilon\), with (4) and \(\exp(3/2)\leqslant k/\varepsilon\), we have \[\frac{1}{8}\varepsilon\leq-3\tau+d_{\mathrm{KL}}(p_{\mathcal{A}},q_{\mathcal{ A}})\leqslant\sum_{i\in\mathcal{A}}(q_{i}-p_{i})+d_{\mathrm{KL}}(p_{ \mathcal{A}},q_{\mathcal{A}})\leqslant d_{\chi^{2}}(p_{\mathcal{A}},q_{ \mathcal{A}}),\] which by Lemma 2.4, and our setting of \(m_{2}\) and \(\alpha\), implies \(\mathrm{Var}[Z_{2}]\leqslant(\frac{1}{4}\mathbb{E}[Z_{2}])^{2}\) and \(\mathbb{E}[Z_{2}]=m_{2}\cdot d_{\chi^{2}}(p_{\mathcal{A}},q_{\mathcal{A}}) \geqslant\frac{1}{8}m_{2}\varepsilon\). By Chebyshev, \[\Pr\left[|Z_{2}-\mathbb{E}[Z_{2}]|\geqslant 2\sqrt{\mathrm{Var}[Z_{2}]} \right]\leqslant\frac{1}{4}\text{ and so }\Pr[Z_{2}\leqslant\frac{1}{16}m_{2} \varepsilon]\leqslant\frac{1}{4}.\]
* On the other hand, if it is the case that \(\left|\sum_{i\in\mathcal{A}}(p_{i}-q_{i})\log\left(1/q_{i}\right)\right| \geqslant\frac{1}{4}\varepsilon\), by Claim 2.3, setting \(m_{3}=140800\log^{2}(k)/\varepsilon^{2}\), with probability at least \(~{}99/100\), \[\frac{1}{4}\varepsilon \leqslant \left|\sum_{i}p_{i}\log\frac{1}{q_{i}}-q_{i}\log\frac{1}{q_{i}}\right|\] \[\leqslant \left|\sum_{i}(p_{i}-\hat{p}_{i})\log\frac{1}{q_{i}}\right|+ \left|\sum_{i}(\hat{p}_{i}-q_{i})\log\frac{1}{q_{i}}\right|\] \[\leqslant \frac{1}{8}\varepsilon+\left|\sum_{i}(\hat{p}_{i}-q_{i})\log\frac {1}{q_{i}}\right|.\] We have that \(Z_{3}=\left|\sum_{i}(\hat{p}_{i}-q_{i})\log\frac{1}{q_{i}}\right|\geqslant \frac{1}{8}\varepsilon\) and thus with probability at least \(1-\frac{1}{4}-\frac{2}{100}=\frac{73}{100}\), the following will happen, the tester will reject: either \(p(\bar{\mathcal{A}})\geqslant 3\tau\), and it is rejected at Line 4 of Algorithm 4, or it passes and \(p(\bar{\mathcal{A}})\leqslant 3\tau\) and \[Z_{2}\geqslant\frac{1}{8}m_{2}\varepsilon\text{ \ or \ }Z_{3}\geqslant\frac{1}{8}\varepsilon,\] and will be rejected. This concludes the proof. 

**Remark 2.5**.: _We note that we can slightly improve the sample complexity of Theorem 1.1 (specifically, improving on the \(\sqrt{k\log(k/\varepsilon)}\) term), at the price of a more complicated algorithm, by adding thresholds \(\tau^{\prime}=\frac{\varepsilon}{\log\log(k/\varepsilon)}\), \(\tau^{\prime\prime}=\frac{\varepsilon}{\log\log\log(k/\varepsilon)}\), and considering separately the elements in \(\mathcal{A}^{\prime}=\{i:q_{i}\in(\tau/k,\tau^{\prime}/k]\}\), \(\mathcal{A}^{\prime\prime}=\{i:q_{i}\in(\tau^{\prime}/k,\tau^{\prime\prime}/k]\}\); specifically, by grouping them in groups, and "merging" each group to get a "new" element with larger probability. For the sake of clarity, we defer this improvement to Appendix D._

### An \(\Omega(\sqrt{k}/\varepsilon+\log^{2}k/\varepsilon^{2})\) lower bound

The \(\Omega(\sqrt{k}/\varepsilon+\log^{2}k/\varepsilon^{2})\) lower bound comes from the combination of Lemma 2.6 and Lemma 2.7. We obtain Lemma 2.6 through the classical hard instance used for uniformity testing [13] and a simple conversion between TV distance and entropy difference gives the result. We note that distributions close to uniform distribution actually have smaller entropy difference (uniform distribution is quite special: having the highest entropy of \(\log k\)). Indeed, replacing the uniform distribution with a slightly biased distribution, we obtain another hard instance for Lemma 2.7, using the classical Le Cam's two-point method.

**Lemma 2.6**.: _With fewer than \(c_{3}\cdot\sqrt{k}/\varepsilon\) samples from \(p\), no tester can distinguish between \(p=q\) and \(|H(p)-H(q)|\geqslant\varepsilon\) with probability higher than \(2/3\), where \(c_{3}>0\) is an absolute constant._

**Lemma 2.7**.: _With fewer than \(c_{4}\cdot\log^{2}k/\varepsilon^{2}\) samples from \(p\), no tester can distinguish between \(p=q\) and \(|H(p)-H(q)|\geqslant\varepsilon\) with probability higher than \(2/3\), where \(c_{4}>0\) is an absolute constant._

## 3 Application to identity testing for Bayes nets

We now provide an application of our main entropy identity testing theorem, to obtain an improved "standard" identity testing algorithm for Bayesian networks:

**Theorem 3.1**.: _Given sample access to an in-degree \(d\) Bayes net \(p\) and full description of in-degree \(d\) Bayes net \(q\), Algorithm 2 takes_

\[C\cdot\left(\frac{2^{d/2}nd^{3/2}\log n\cdot\sqrt{\log(n/\varepsilon)}}{ \varepsilon^{2}}+\frac{d^{3}n^{2}\cdot\log n}{\varepsilon^{4}}+\frac{2^{d/2}n ^{3/2}\cdot\log n}{\varepsilon^{2}}\right)\]

_samples to test between \(p=q\) or \(\mathrm{d}_{\mathrm{H}}(p,q)\geqslant\Omega(\varepsilon)\), where \(C>0\) is an absolute constant. Moreover, the algorithm runs in time polynomial in \(n^{d}\) and \(1/\varepsilon\)._

Before proceeding to the analysis of our algorithm, we require the following definitions.

**Definition 3.2**.: _A projection of a Bayes net \(p\) on \(\{0,1\}^{n}\) into a DAG \(G\) is denoted \(p_{G}\), and is defined by its probability mass function (PMF) as follows:_

\[p_{G}(X_{1},\ldots,X_{n})=\prod_{i=1}^{n}p(X_{i}\mid\Pi_{i}^{G}),\]

_where \(\Pi_{i}^{G}\) is the set of parents of \(X_{i}\) in \(G\). Abusing the notation in the context of Bayesian networks, we refer to \(p_{X_{i},\Pi_{i}}\) or \(p_{X_{i},\Pi_{i}}(x_{i},\pi_{i})\) as the marginal distribution of \(p\) on the subset \(\{X_{i},\Pi_{i}\}\)._

Denote \(\mathcal{U}:=\bigcup_{i=1}^{n}\mathcal{A}_{i}\), where \(\mathcal{A}_{i}:=\left\{x\in\{0,1\}^{n}:q_{X_{i},\Pi_{i}^{G}}(x_{i}(x),\pi_{i} ^{G}(x))\geqslant\Omega\left(\frac{\varepsilon^{2}}{2^{d+1}n^{2}\log(n/ \varepsilon)}\right)\right\}\). This gives us the property that marginalization over \(X_{i}=x_{i},\Pi_{i}^{G}=\pi_{i}^{G}\) works nicely as we include elements only based on its local property (as long as \(q_{X_{i},\Pi_{i}}\) is large enough). And \(q\) is Markov w.r.t. \(G\). We use \((x_{i},\pi_{i})\in\mathcal{A}_{i}^{\prime}\), where \(\mathcal{A}_{i}^{\prime}=\left\{x^{\prime}\in\{0,1\}^{|\Pi_{i}^{G}|+1}:q_{X_{ i},\Pi_{i}^{G}}(x^{\prime})\geqslant\Omega\left(\frac{\varepsilon^{2}}{2^{d+1}n^{2} \log(n/\varepsilon)}\right)\right\}\). Let \((a,B)\in\mathcal{A}_{i}^{\prime}\), we have that as long as \((x_{i}(x),\pi_{i}(x))=(a,B)\), then \(x\in\mathcal{A}_{i}\) and vice versa, which means that

\[\mathcal{U}=\bigcup_{i=1}^{n}\mathcal{A}_{i}=\bigcup_{i=1}^{n}\{x\in\{0,1\}^ {n}:(x_{i}(x),\pi_{i}^{G}(x))\in\mathcal{A}_{i}^{\prime}\}.\]We will check if \(p_{X_{i},\Pi_{i}^{G}}(\bar{\mathcal{A}}^{\prime}_{i})\geqslant\Omega(\varepsilon^{2} /(n^{2}\cdot\log(n/\varepsilon)))\) and reject early if true; this takes \(O\left(\frac{n^{2}\cdot\log(n/\varepsilon)\cdot d\log(n)}{\varepsilon^{2}}\right)\) samples for all tests to be correct via a union bound. After passing this test, we can conclude that

\[p(\bar{\mathcal{U}})=\sum_{x\in\bigcap_{i=1}^{n}\bar{\mathcal{A}}_{i}}p(x) \leqslant\sum_{x\in\bar{\mathcal{A}}_{1}}p(x)=\sum_{x^{\prime}\in\bar{ \mathcal{A}}_{1}^{\prime}}p_{X_{1},\Pi_{1}^{G}}(x^{\prime})=p_{X_{1},\Pi_{1}^ {G}}(\bar{\mathcal{A}}^{\prime}_{1})\leqslant O(\frac{\varepsilon^{2}}{n^{2} \cdot\log(n/\varepsilon)}),\]

where we marginalize over everything other than \((X_{1},\Pi_{1}^{G})\) in the third step.

Similarly, we can upper bound \(q(\bar{\mathcal{U}})\leqslant q_{X_{i},\Pi_{1}^{G}}(\bar{\mathcal{A}}^{\prime }_{i})\leqslant O(\varepsilon^{2}/(n^{2}\cdot\log(n/\varepsilon)))\). Abusing the notation slightly, we denote \(p_{G;\mathcal{U}}\) as the distribution obtained by projecting \(p\) onto \(G\) (which gives \(p_{G}\)) and then restricting the distribution \(p_{G}\) to take elements in \(\mathcal{U}\).

We will need the following Lemma 3.3 whose proof is deferred to Appendix E.

**Lemma 3.3**.: _Suppose \(d_{H}^{2}(p,q)\geqslant\Omega(\varepsilon^{2})\); \(d_{\mathrm{KL}}(p\|p_{G})\leqslant O(\varepsilon^{2})\); \(p(\bar{\mathcal{U}})\leqslant\frac{\varepsilon^{2}}{n\log(n/\varepsilon)}\) ; \(\forall i\in[n],p(\bar{\mathcal{A}}^{\prime}_{i})\leqslant\frac{\varepsilon^{2} }{n^{2}\log(n/\varepsilon)}\), where \(\mathcal{A}^{\prime}_{i}\) is defined above, and \(q\) is Markov with respect to \(G\), then we have_

\[\sum_{i=1}^{n}d_{\mathrm{KL}}(p_{X_{i},\Pi_{i}^{G};\mathcal{A}^{\prime}_{i}} \|q_{X_{i},\Pi_{i}^{G};\mathcal{A}^{\prime}_{i}})\geqslant\Omega(\varepsilon^ {2}).\]

_Therefore testing \(d_{\mathrm{KL}}(p_{X_{i},\Pi_{1}^{G};\mathcal{A}^{\prime}_{i}}\|q_{X_{i},\Pi_{ 1}^{G};\mathcal{A}^{\prime}_{i}})\geqslant\frac{\varepsilon^{2}}{n}\) over all \(i\) suffices to detect this case._

In addition, to connect entropy testing to Bayes net testing, we will need the following Lemma 3.4, whose proof will be deferred to Appendix E. And so if we can show that all local entropies between Bayes nets \(p\) and \(q\) are sufficiently close, then this implies that \(p\) must be close to \(q\)'s \(\mathrm{DAG}\ G\) and \(q\) must also be close to \(p\)'s \(\mathrm{DAG}\ G^{\prime}\).

**Lemma 3.4**.: _Let \(p\) and \(q\) be two max in-degree-\(d\) Bayes nets supported on \(\{0,1\}^{n}\) such that for every subset \(L\subseteq\{X_{1},\cdots,X_{n}\}\) of size \(d+1\), the following holds:_

\[|H(p_{L})-H(q_{L})|\leqslant O\left(\frac{\varepsilon^{2}}{n}\right).\]

_Suppose \(p\) is Markov w.r.t. \(G^{\prime}\) and \(q\) Markov w.r.t. \(G\). Then we have that_

\[d_{\mathrm{KL}}(p\|p_{G})\leqslant O(\varepsilon^{2})\text{ and }d_{\mathrm{KL}}(q\|q_{G^{ \prime}})\leqslant O(\varepsilon^{2}).\]

Proof of Theorem 3.1.: We show the result by analyzing Algorithm 2. By Theorem 1.1, the sample complexity for entropy testing on any subset \(L\) of size (dimension) \(d\) or \(d+1\), is

\[O\left(2^{d/2}n\sqrt{d\log(n/\varepsilon)}/\varepsilon^{2}+d^{2}n^{2}/ \varepsilon^{4}\right)\,.\]

To guarantee the success of every tests employed in the algorithm, we increase the sample complexity of each test by an extra \(O(\log(n^{d+1}))=O(d\log n)\) factor to boost their success probability to \(1-\frac{1}{100n^{d+1}}\) (via a standard majority vote technique), which will allow us to use a union bound over all tests as there are at most \(n^{d+1}\) subsets with size \(d+1\). For this step, the sample complexity will be

\[O\left(\left(\frac{2^{d/2}n\sqrt{d\log(n/\varepsilon)}}{\varepsilon^{2}}+\frac {d^{2}n^{2}}{\varepsilon^{4}}\right)d\log n\right).\]

With this in hand, we will proceed with the analysis under the event that every entropy test performed is correct (which by the above argument happens with high probability). If distribution \(p\) manages to pass all the entropy tests, it must satisfy the following:

\[|H(p_{L})-H(q_{L})|\leqslant\frac{\varepsilon^{2}}{n},\] (6)

for every subset \(L\) of size \(d+1\) for the latter, and every subset \(L\) of size \(d\) or \(d+1\) for the former. From here, in principle, we can perform structural learning of \(p\) through \(H(q_{L})\), which then gives us an approximated DAG \(\hat{G}\) of \(p\) and we can check \(\mathrm{d}_{\mathrm{H}}(p_{\hat{G}},q_{\hat{G}})\). Unfortunately, structural learning of Bayes nets is known to be computationally hard in many settings [14, 15], and so this would lead to a computationally inefficient algorithm.

Instead, we argue that this (learning) step can be bypassed entirely: the intuition of the argument is to view structure learning for Bayes net as an optimization problem; and any assignment \(x\) to the two optimization problems (structure learning of \(p\) and \(q\)) satisfy \(f_{1}(x)=f_{2}(x)\pm O(\varepsilon^{2})\) due to their local entropy being close7 - this means that an optima \(x_{1}\) for \(f_{1}\) satisfies \(\min_{x}f_{1}(x)=f_{1}(x_{1})\geqslant f_{2}(x_{1})-\varepsilon^{2}\geqslant \min_{x}f_{2}(x)-\varepsilon^{2}\) and vice versa (optima \(x_{2}\) for \(f_{2}\)).

Footnote 7: Here, \(x\) is the DAG’s assignment of parents and child; and \(f_{1}(x)\) (resp. \(f_{2}(x)\)) is the associated KL-divergence (also called _score_ of the DAG in the literature, which measures the how well DAG models the true Bayes net) between \(p\) (resp. \(q\)) and \(x\). Since we are in the realizable setting, the optimal is in fact 0.

Applying Lemma 3.4, we have that \(d_{\mathrm{KL}}(p\|p_{G})\leqslant O(\varepsilon^{2})\) where \(G\) is the DAG \(q\) is Markov with respect to. With this at hand, we continue onto the KL testing part. The algorithm will check if \(p_{X_{i},\Pi_{i}^{G}}(\bar{\mathcal{A}}_{i}^{\prime})\geqslant\Omega( \varepsilon^{2}/\max(n,\log(1/\varepsilon)))\) and reject early if it is true (this costs \(O\left(\frac{d\log(n)\cdot\max(n,\log(1/\varepsilon))}{\varepsilon^{2}}\right)\) samples) and then check for every \(i\in[n]\),

\[d_{\mathrm{KL}}(p_{X_{i},\Pi_{i}^{G};\mathcal{A}_{i}^{\prime}}\|q_{X_{i},\Pi_{ i}^{G};\mathcal{A}_{i}^{\prime}})\geqslant\frac{\varepsilon^{2}}{n}\;\;\text{ or}\;\;d_{\mathrm{KL}}(p_{X_{i},\Pi_{i}^{G};\mathcal{A}_{i}^{\prime}}\|q_{X_{i}, \Pi_{i}^{G};\mathcal{A}_{i}^{\prime}})=0.\]

Recalling (4), if the former case holds, we have

\[d_{\chi^{2}}(p_{X_{i},\Pi_{i}^{G};\mathcal{A}_{i}^{\prime}}\|q_{X_{i},\Pi_{i}^ {G};\mathcal{A}_{i}^{\prime}})\geqslant\underbrace{p_{X_{i},\Pi_{i}^{G}}(\bar {\mathcal{A}}_{i}^{\prime})-q_{X_{i},\Pi_{i}^{G}}(\bar{\mathcal{A}}_{i}^{ \prime})}_{\leqslant O(\varepsilon^{2}/n)}+d_{\mathrm{KL}}(p_{X_{i},\Pi_{i}^{G };\mathcal{A}_{i}^{\prime}}\|q_{X_{i},\Pi_{i}^{G};\mathcal{A}_{i}^{\prime}})= \Omega\left(\frac{\varepsilon^{2}}{n}\right)\]

the bound on the first term following from the algorithm's check on Line 10. Using Lemma 2.4, we can perform the corresponding check for \(i\in[n]\), and after a union bound over \(n\) tests, noting that \(q(x_{i},\pi_{i}^{G})\geqslant\frac{\varepsilon^{2}}{2^{d+1}n^{2}\log(n/ \varepsilon)}\) when restricted on \(\mathcal{A}_{i}^{\prime}\), the sample complexity is

\[O\left(\left(\sqrt{1/\left(\frac{\varepsilon^{2}}{2^{d+1}n^{2}\log(n/ \varepsilon)}\cdot\frac{\varepsilon^{2}}{n}\right)}+\frac{\sqrt{2^{d+1}}\cdot n }{\varepsilon^{2}}\right)\cdot\log n\right)=O\left(\frac{2^{d/2}n^{3/2}}{ \varepsilon^{2}}\cdot\log n\right).\]

Following this, we look at the two cases:

* If \(p=q\), then with high probability, \(p\) will pass all entropy tests, all \(\mathrm{KL}\) local tests and the tester will accept.
* If \(\mathrm{d_{H}}(p,q)\geqslant\varepsilon\), either it fails one of the entropy tests. If it does pass the entropy test, then we must have that \(d_{\mathrm{KL}}(p\|p_{G})\leqslant O(\varepsilon^{2})\) by (26). Then following Lemma 3.3 and Lemma 2.4, the tester will reject.

In total, the sample complexity is:

\[O\left(\left(\frac{2^{d/2}n\sqrt{d\log(n/\varepsilon)}}{\varepsilon^{2}}+ \frac{d^{2}n^{2}}{\varepsilon^{4}}\right)d\log n+\frac{2^{d/2}n^{3/2}}{ \varepsilon^{2}}\cdot\log n\right).\]

This concludes the proof of the theorem. 

## 4 Conclusion and open problems

In this paper, we study a variant of distribution testing problem in terms of entropy difference; we give nearly tight upper and lower sample complexity bounds for the problem. We subsequently apply our entropy testing algorithm to identity testing of Bayes nets, which unlike prior works, makes merely the necessary assumptions (the bound on the in-degree of the Bayes nets).

**Future directions.** We believe the _closeness_ (two-sample) testing variant of the problem (testing if two unknown distribution \(p\) and \(q\) are the same or far in terms of entropy difference) could also be interesting; and, notably, has connections to other distribution testing problems: first, it should lead to a natural solution to closeness testing of Bayes nets via ideas in this paper. Second, solving the closeness entropy testing problem give another path to testing independence in terms of mutual information (studied in [1] and also covered in [16]), a notion closely related to entropy.

## Acknowledgments and Disclosure of Funding

We would like to thank the reviewers for their suggestions and efforts which help improve this paper.

## References

* [ABIS19] Jayadev Acharya, Sourbh Bhadane, Piotr Indyk, and Ziteng Sun. Estimating entropy of distributions in constant space. In _NeurIPS_, pages 5163-5174, 2019.
* [ADOS17] Jayadev Acharya, Hirakendu Das, Alon Orlitsky, and Ananda Theertha Suresh. A unified maximum likelihood approach for estimating symmetric properties of discrete distributions. In _ICML_, volume 70 of _Proceedings of Machine Learning Research_, pages 11-21. PMLR, 2017.
* [AISW20] Jayadev Acharya, Ibrahim Issa, Nirmal V. Shende, and Aaron B. Wagner. Estimating quantum entropy. _IEEE J. Sel. Areas Inf. Theory_, 1(2):454-468, 2020.
* [AMNW22] Maryam Aliakbarpour, Andrew McGregor, Jelani Nelson, and Erik Waingarten. Estimation of entropy in constant space with improved sample complexity. In _NeurIPS_, 2022.
* [AOST17] Jayadev Acharya, Alon Orlitsky, Ananda Theertha Suresh, and Himanshu Tyagi. Estimating renyi entropy of discrete distributions. _IEEE Trans. Inf. Theory_, 63(1):38-56, 2017.
* [BCY22] Arnab Bhattacharyya, Clement L. Canonne, and Joy Qiping Yang. Independence testing for bounded degree bayesian network. _CoRR_, abs/2204.08690, 2022.
* [BDKR02] Tugkan Batu, Sanjoy Dasgupta, Ravi Kumar, and Ronitt Rubinfeld. The complexity of approximating entropy. In _STOC_, pages 678-687. ACM, 2002.
* [BFF\({}^{+}\)01] Tugkan Batu, Lance Fortnow, Eldar Fischer, Ravi Kumar, Ronitt Rubinfeld, and Patrick White. Testing random variables for independence and identity. In _FOCS_, pages 442-451. IEEE Computer Society, 2001.
* [BGKV21] Arnab Bhattacharyya, Sutanu Gayen, Saravanan Kandasamy, and NV Vinodchandran. Testing product distributions: A closer look. In _Algorithmic Learning Theory_, pages 367-396. PMLR, 2021.
* [BGMV20] Arnab Bhattacharyya, Sutanu Gayen, Kuldeep S. Meel, and N. V. Vinodchandran. Efficient distance approximation for structured high-dimensional distributions via learning. In _NeurIPS_, 2020.
* [BGP\({}^{+}\)23] Arnab Bhattacharyya, Sutanu Gayen, Eric Price, Vincent Y. F. Tan, and N. V. Vinodchandran. Near-optimal learning of tree-structured distributions by chow and liu. _SIAM J. Comput._, 52(3):761-793, 2023.
* [BY02] Ziv Bar-Yossef. _The Complexity of Massive Data Set Computations_. PhD thesis, UC Berkeley, 2002. Adviser: Christos Papadimitriou. Available at http://webee.technion.ac.il/people/zivby/index_files/Page1489.html.
* [Can22] Clement L. Canonne. Topics and techniques in distribution testing: A biased but representative sample. _Found. Trends Commun. Inf. Theory_, 19(6):1032-1198, 2022.
* [CDKS18] Clement L. Canonne, Ilias Diakonikolas, Daniel M. Kane, and Alistair Stewart. Testing conditional independence of discrete distributions. In _STOC_, pages 735-748. ACM, 2018.
* [CDKS20] Clement L. Canonne, Ilias Diakonikolas, Daniel M. Kane, and Alistair Stewart. Testing bayesian networks. _IEEE Trans. Inf. Theory_, 66(5):3132-3170, 2020. Preprint available at arXiv:1612.03156.

* [CHM04] David Maxwell Chickering, David Heckerman, and Christopher Meek. Large-sample learning of bayesian networks is np-hard. _J. Mach. Learn. Res._, 5:1287-1330, 2004.
* Coding Theorems for Discrete Memoryless Systems, Second Edition_. Cambridge University Press, 2011.
* [CL68] C. K. Chow and C. N. Liu. Approximating discrete probability distributions with dependence trees. _IEEE Trans. Inf. Theory_, 14(3):462-467, 1968.
* [DDK19] Constantinos Daskalakis, Nishanth Dikkala, and Gautam Kamath. Testing ising models. _IEEE Trans. Inf. Theory_, 65(11):6829-6852, 2019.
* [DKW18] Constantinos Daskalakis, Gautam Kamath, and John Wright. Which distribution distances are sublinearly testable? In _Proceedings of the Twenty-Ninth Annual ACM-SIAM Symposium on Discrete Algorithms_, pages 2747-2764. SIAM, 2018.
* [DP16] Constantinos Daskalakis and Qinxuan Pan. Square hellinger subadditivity for bayesian networks and its applications to identity testing. _arXiv preprint arXiv:1612.03164_, 2016. Full version of [DP17].
* [DP17] Constantinos Daskalakis and Qinxuan Pan. Square hellinger subadditivity for bayesian networks and its applications to identity testing. In _COLT_, volume 65 of _Proceedings of Machine Learning Research_, pages 697-703. PMLR, 2017.
* [GHS21] Tom Gur, Min-Hsiu Hsieh, and Sathyawageeswar Subramanian. Sublinear quantum algorithms for estimating von neumann entropy. _CoRR_, abs/2111.11139, 2021.
* [HJW15a] Yanjun Han, Jiantao Jiao, and Tsachy Weissman. Adaptive estimation of shannon entropy. In _ISIT_, pages 1372-1376. IEEE, 2015.
* [HJW15b] Yanjun Han, Jiantao Jiao, and Tsachy Weissman. Adaptive estimation of shannon entropy. _CoRR_, abs/1502.00326, 2015.
* [Hor93] Klaus-U Hoffgen. Learning and robust learning of product distributions. In _Proceedings of the sixth annual conference on Computational learning theory_, pages 77-83, 1993.
* [KCG\({}^{+}\)23] Neville Kenneth Kitson, Anthony C. Constantinou, Zhigao Guo, Yang Liu, and Kiattikun Chobtham. A survey of bayesian network structure learning. _Artif. Intell. Rev._, 56(8):8721-8814, 2023.
* [KDDC23] Anthimos Vardis Kandiros, Constantinos Daskalakis, Yuval Dagan, and Davin Choo. Learning and testing latent-tree ising models efficiently. In _COLT_, volume 195 of _Proceedings of Machine Learning Research_, pages 1666-1729. PMLR, 2023.
* [Pan04] Liam Paninski. Estimating entropy on \(m\) bins given fewer than \(m\) samples. _IEEE Trans. Inform. Theory_, 50(9):2200-2203, 2004.
* [Pan08] Liam Paninski. A coincidence-based test for uniformity given very sparsely sampled discrete data. _IEEE Transactions on Information Theory_, 54(10):4750-4755, 2008.
* [VV11a] Gregory Valiant and Paul Valiant. Estimating the unseen: an n/log(n)-sample estimator for entropy and support size, shown optimal via new clts. In _STOC_, pages 685-694. ACM, 2011.
* [VV11b] Gregory Valiant and Paul Valiant. The power of linear estimators. In _FOCS_, pages 403-412. IEEE Computer Society, 2011.
* [VV13] Paul Valiant and Gregory Valiant. Estimating the unseen: Improved estimators for entropy and other properties. In _NIPS_, pages 2157-2165, 2013.
* [VV17] Gregory Valiant and Paul Valiant. An automatic inequality prover and instance optimal identity testing. _SIAM J. Comput._, 46(1):429-455, 2017.

* [WY16] Yihong Wu and Pengkun Yang. Minimax rates of entropy estimation on large alphabets via best polynomial approximation. _IEEE Trans. Inform. Theory_, 62(6):3702-3720, 2016.

Derivation for Line 4 in Algorithm 1

We need to show the following: with probability at least \(99/100\), if \(Z_{1}\geqslant 2\tau\), then \(p(\bar{\mathcal{A}})\geqslant\tau\); and if \(Z_{1}\leqslant 2\tau,\) then \(p(\bar{\mathcal{A}})<3\tau\). For the first one, we prove by contrapositive: with high probability \(1-\frac{1}{200}\), \(p(\bar{\mathcal{A}})<\tau\Rightarrow Z_{1}<2\tau\). Suppose \(T=\mathrm{Binomial}(m_{1},\tau)\) and setting \(m_{1}=\frac{48}{\tau}\geqslant\frac{3\log 200}{\tau}\), and using a Chernoff bound, we have the following

\[\Pr[T\geqslant 2\tau]\leqslant\exp(-\tau\cdot m_{1}/3)\leqslant\frac{1}{200}.\]

Since any \(\mathrm{Binomial}(m_{1},p(\bar{\mathcal{A}}))\) will be first-order stochastic dominated by \(\mathrm{Binomial}(m_{1},\tau)\) if \(p(\bar{\mathcal{A}})<\tau\), we can conclude the following: if \(p(\bar{\mathcal{A}})<\tau\), then \(\Pr[Z_{1}\geqslant 2\tau]\leqslant\Pr[T\geqslant 2\tau]\leqslant\frac{1}{200}\).

For the latter, we prove via its contrapositive: with probability \(1-\frac{1}{200}\), \(p(\bar{\mathcal{A}})\geqslant 3\tau\Rightarrow Z_{1}\geqslant 2\tau.\) As \(p(\bar{\mathcal{A}})\geqslant 3\tau,\) take \(m_{1}=\frac{48}{\tau}\geqslant\frac{9\log 200}{\tau}\), by a Chernoff bound, we have

\[\Pr[\hat{p}^{\prime}(\bar{\mathcal{A}})\leqslant 2\tau]\leqslant\Pr[\hat{p}^{ \prime}(\bar{\mathcal{A}})\leqslant(1-1/3)\cdot p(\bar{\mathcal{A}})] \leqslant\exp\left(-\frac{m_{1}\cdot p(\bar{\mathcal{A}})}{18}\right) \leqslant\exp\left(-\frac{m_{1}\cdot\tau}{9}\right)\leqslant\frac{1}{200}.\]

Combining the two with a union bound concludes the proof.

## Appendix B Deferred proofs from Section 2.1

**Claim 2.2**.: _Let \(\mathcal{A}\) be any set such that \(p(\bar{\mathcal{A}})<\varepsilon/2\). Then, if \(|H(p_{\mathcal{A}})-H(q_{\mathcal{A}})|\geqslant\varepsilon\), we must have (i) \(d_{\mathrm{KL}}(p_{\mathcal{A}}\|q_{\mathcal{A}})\geqslant\frac{\varepsilon}{2}\) or (ii) \(|\sum_{i\in\mathcal{A}}(p_{i}-q_{i})\log(\frac{1}{q_{i}})|\geqslant\frac{ \varepsilon}{2}\)._

Proof of Claim 2.2.: We can bound \(|H(p_{\mathcal{A}})-H(q_{\mathcal{A}})|\) as

\[\varepsilon\leq|H(p_{\mathcal{A}})-H(q_{\mathcal{A}})| = \left|\sum_{i\in\mathcal{A}}\left(p_{i}\log\frac{1}{p_{i}}-q_{i} \log\frac{1}{q_{i}}\right)\right|\] (7) \[= \left|\sum_{i\in\mathcal{A}}\left(p_{i}\log\frac{q_{i}}{p_{i}}+p _{i}\log\frac{1}{q_{i}}-q_{i}\log\frac{1}{q_{i}}\right)\right|\] \[\leqslant \left|\sum_{i\in\mathcal{A}}p_{i}\log\frac{q_{i}}{p_{i}}\right| +\left|\sum_{i\in\mathcal{A}}\left(p_{i}\log\frac{1}{q_{i}}-q_{i}\log\frac{1} {q_{i}}\right)\right|\] \[= |d_{\mathrm{KL}}(p_{\mathcal{A}}\|q_{\mathcal{A}})|+\left|\sum_{ i\in\mathcal{A}}(p_{i}-q_{i})\log\frac{1}{q_{i}}\right|,\]

which implies that at least one of the two terms is at least \(\varepsilon/2\). If it is the second, we are done; otherwise, we know that either

\[d_{\mathrm{KL}}(p_{\mathcal{A}}\|q_{\mathcal{A}})\geqslant\frac{1}{2} \varepsilon\,\,\,\text{or}\,\,\,d_{\mathrm{KL}}(p_{\mathcal{A}}\|q_{\mathcal{ A}})\leqslant-\frac{1}{2}\varepsilon.\]

We will rule out the second case, using that \(\log\frac{1}{x}\geqslant 1-x\) for \(x>0\),8

Footnote 8: In the case of \(p_{i}=0\), we still have \(p_{i}\log\left(\frac{p_{i}}{q_{i}}\right)\geqslant p_{i}-q_{i}\).

\[d_{\mathrm{KL}}(p_{\mathcal{A}}\|q_{\mathcal{A}})=\sum_{i\in\mathcal{A}}p_{i} \log\frac{p_{i}}{q_{i}}\geqslant\sum_{i\in\mathcal{A}}p_{i}\left(1-\frac{q_{i} }{p_{i}}\right)=q(\bar{\mathcal{A}})-p(\bar{\mathcal{A}})>-\frac{1}{2}\varepsilon.\]

Thus, we cannot have \(d_{\mathrm{KL}}(p_{\mathcal{A}}\|q_{\mathcal{A}})\leqslant-\frac{1}{2}\varepsilon\) and so \(d_{\mathrm{KL}}(p_{\mathcal{A}}\|q_{\mathcal{A}})\geqslant\frac{1}{2}\varepsilon\). 

**Claim 2.3**.: _Let \(\hat{p}\) be the empirical estimator for an unknown discrete distribution \(p\) supported on \([k]\), based on \(\mathrm{Poi}(m)\) samples, where \(m=\Theta\Big{(}\frac{\log^{2}(k)}{\varepsilon^{2}}\Big{)}\); assume that \(d_{\chi^{2}}(p_{\mathcal{A}},q_{\mathcal{A}})\leqslant\varepsilon/8\) and \(p(\bar{\mathcal{A}})+q(\bar{\mathcal{A}})\leqslant 4\tau=\frac{1}{4}\frac{ \varepsilon}{\log(k/\varepsilon)}\),9 then_

Footnote 9: One can remove the assumption that \(p(\bar{\mathcal{A}})+q(\bar{\mathcal{A}})\leqslant 4\tau\), at the cost of a slightly worse constant.

\[\Pr\left[\left|\sum_{i\in\mathcal{A}}(p_{i}-\hat{p}_{i})\log\frac{1}{q_{i}} \right|\geqslant\frac{1}{8}\varepsilon\right]\leqslant\frac{1}{100}.\]Proof of Claim 2.3.: We follow the same analysis as in [11]. Letting \(Y_{i}:=(p_{i}-\hat{p}_{i})\log\frac{1}{\hat{q}_{i}}\) for \(i\in\mathcal{A}\), we have \(\mathbb{E}[Y_{i}]=0\) and

\[\mathrm{Var}[Y_{i}]=\mathbb{E}[(Y_{i}-\mathbb{E}[Y_{i}])^{2}]=\mathbb{E}[Y_{i}^{ 2}]=\mathbb{E}[(p_{i}-\hat{p}_{i})^{2}]\log^{2}\frac{1}{q_{i}}=\frac{1}{m^{2}}( mp_{i})\log^{2}\frac{1}{q_{i}}=\frac{p_{i}}{m}\log^{2}\frac{1}{q_{i}}.\]

Let \(Y:=\sum_{i\in\mathcal{A}}(p_{i}-\hat{p}_{i})\log\frac{1}{q_{i}}\). We will use our assumption that \(d_{\chi^{2}}(p_{\mathcal{A}},q_{\mathcal{A}})\leqslant\varepsilon/8\) and \(p(\bar{\mathcal{A}})+q(\bar{\mathcal{A}})\leqslant\frac{1}{4}\frac{\varepsilon }{\log(k/\varepsilon)}\) below. Note that, our analysis is in the Poissonized setting:

\[\mathrm{Var}[Y] = \mathrm{Var}\left[\sum_{\mathcal{A}}(p_{i}-\hat{p}_{i})\log \frac{1}{q_{i}}\right]\] \[= \sum_{i\in\mathcal{A}}\frac{p_{i}}{m}\log^{2}\left(\frac{1}{q_{i }}\right)\] \[= \sum_{i\in\mathcal{A}}\frac{p_{i}}{m}\left(\log\left(\frac{1}{p_ {i}}\right)+\log\left(\frac{p_{i}}{q_{i}}\right)\right)^{2}\] \[\leqslant \sum_{i\in\mathcal{A}}\frac{p_{i}}{m}\left(2\left(\log\left( \frac{1}{p_{i}}\right)\right)^{2}+2\left(\log\left(\frac{p_{i}}{q_{i}}\right) \right)^{2}\right)\] \[= \sum_{i\in\mathcal{A}}\frac{2}{m}p_{i}\log^{2}\left(\frac{1}{p_{i }}\right)+\sum_{i\in\mathcal{A}}\frac{2}{m}p_{i}\log^{2}\left(\frac{p_{i}}{q_{ i}}\right)\] \[\leqslant \sum_{i\in\mathcal{A}}\frac{2}{m}p_{i}\log^{2}\left(\frac{1}{p_{i }}\right)+\sum_{\frac{p_{i}}{q_{i}}\geqslant 1,i\in\mathcal{A}}\frac{2}{m}p_{i} \left(\frac{p_{i}}{q_{i}}-1\right)+\sum_{\frac{p_{i}}{q_{i}}<1,i\in\mathcal{A} }\frac{2}{m}p_{i}\left(\frac{q_{i}}{p_{i}}-1\right)\] \[= \sum_{i\in\mathcal{A}}\frac{2}{m}p_{i}\log^{2}\left(\frac{1}{p_{i }}\right)+\sum_{\frac{p_{i}}{q_{i}}\geqslant 1,i\in\mathcal{A}}\frac{2}{m} \frac{(p_{i}-q_{i})^{2}}{q_{i}}+\sum_{\frac{p_{i}}{q_{i}}\geqslant 1,i\in \mathcal{A}}\frac{2}{m}(p_{i}-q_{i})+\sum_{\frac{p_{i}}{q_{i}}<1,i\in\mathcal{A }}\frac{2}{m}(q_{i}-p_{i})\] \[\leqslant \frac{4\log^{2}k}{m}+\frac{6}{m}+\frac{2}{m}(d_{\chi^{2}}(p_{ \mathcal{A}},q_{\mathcal{A}})+d_{\mathrm{TV}}(p,q))\] (9) \[\leqslant \frac{4\log^{2}k}{m}+\frac{6}{m}+\frac{2}{m}\left(\frac{\varepsilon }{8}+\sqrt{\frac{\varepsilon}{8}}+4\tau\right)\leqslant\frac{4\log^{2}k}{m}+ \frac{8}{m}\leqslant\frac{22\log^{2}k}{m}\] (10)

For (8), we analyze by two cases: if \(\frac{p_{i}}{q_{i}}\geqslant 1\), we have that \(p_{i}\log^{2}\left(\frac{p_{i}}{q_{i}}\right)\leqslant p_{i}\left(\frac{p_{i}} {q_{i}}-1\right)\); otherwise, \(p_{i}\log^{2}\left(\frac{p_{i}}{q_{i}}\right)=p_{i}\log^{2}\left(\frac{q_{i}}{p_ {i}}\right)<p_{i}\left(\frac{q_{i}}{p_{i}}-1\right)\). And we use [1, Lemma3], \(\sum_{i\in\mathcal{A}}p_{i}\log^{2}\left(\frac{1}{p_{i}}\right)\leqslant 2\log^{2}k+3\) in (9). We use the premise and (4) in (10) and we have that

\[d_{\mathrm{TV}}(p,q)=d_{\mathrm{TV}}(p_{\mathcal{A}},q_{\mathcal{A}})+d_{ \mathrm{TV}}(p_{\bar{\mathcal{A}}},q_{\bar{\mathcal{A}}})\leqslant\sqrt{d_{ \chi^{2}}(p_{\mathcal{A}},q_{\mathcal{A}})}+p(\bar{\mathcal{A}})+q(\bar{ \mathcal{A}})\leqslant\sqrt{\frac{\varepsilon}{8}}+4\tau;\]

and the last step is obtained by noticing that \(\log(k)\geqslant\frac{2}{3}\) for \(k\geqslant 2\). By Chebyshev's inequality, we then have that

\[\Pr\left[|Y|\geqslant 10\sqrt{\frac{38\log^{2}k}{m}}\right]\leqslant\Pr\left[| Y-\mathbb{E}[Y]|\geqslant 10\sqrt{\mathrm{Var}[Y]}\right]\leqslant\frac{1}{100}\,,\]

and this last inequality yields the statement as long as \(m\geqslant\frac{22\times 100\times 8^{2}\log^{2}(k)}{\varepsilon^{2}}=\frac{140800 \log^{2}(k)}{\varepsilon^{2}}\). 

**Lemma 2.4**.: _Let \(\mathcal{A}:=\{i\in[k]\mid q_{i}\geqslant\alpha\}\). Let \(m_{2}\geqslant 16384\max\left\{\sqrt{\frac{1}{\alpha\varepsilon}},\frac{\sqrt{ k}}{\varepsilon}\right\}\) be the number of samples used to compute \(Z_{2}\). Then \(\mathbb{E}[Z_{2}]=m_{2}d_{\chi^{2}}(p_{\mathcal{A}},q_{\mathcal{A}})\). Moreover, if \(d_{\chi^{2}}(p_{\mathcal{A}},q_{\mathcal{A}})\leqslant\frac{\varepsilon}{2}\), then \(\mathrm{Var}[Z_{2}]\leqslant(\frac{1}{32}m_{2}\varepsilon)^{2}\). If \(d_{\chi^{2}}(p_{\mathcal{A}},q_{\mathcal{Proof of Lemma 2.4.: The proof is a relatively straightforward modification of the argument of [1, Lemma 2]. We have the expectation and variance of \(Z_{2}\),

\[\mathbb{E}[Z_{2}]=m_{2}d_{\chi^{2}}(p_{\mathcal{A}},q_{\mathcal{A}})\text{ and }\operatorname{Var}[Z_{2}]=\sum_{i\in\mathcal{A}}\left[2\frac{p_{i}^{2}}{q_{i}^{2 }}+4m_{2}\frac{p_{i}(p_{i}-q_{i})^{2}}{q_{i}^{2}}\right].\]

It boils down to bounding the following,

\[2\sum_{i\in\mathcal{A}}\frac{p_{i}^{2}}{q_{i}^{2}} \leqslant 4k+4\sum_{i\in\mathcal{A}}\frac{(p_{i}-q_{i})^{2}}{q_{i}^{2}}\] \[\leqslant 4k+\frac{4}{\alpha}\sum_{i\in\mathcal{A}}\frac{(p_{i}-q_{i})^{2 }}{q_{i}}\] \[\leqslant 4k+\frac{4}{\alpha m_{2}}\mathbb{E}[Z_{2}].\]

Derivation of the inequalities follow from [1, proof of Lemma 2].

\[4m_{2}\sum_{i\in\mathcal{A}}\frac{p_{i}(p_{i}-q_{i})^{2}}{q_{i}^ {2}} \leqslant 4m_{2}\left(\sum_{i\in\mathcal{A}}\frac{p_{i}^{2}}{q_{i}^{2}} \right)^{1/2}\left(\sum_{i\in\mathcal{A}}\frac{(p_{i}-q_{i})^{4}}{q_{i}^{2}} \right)^{1/2}\] \[\leqslant 4m_{2}\left(4k+\frac{4}{\alpha m_{2}}\mathbb{E}[Z_{2}]\right)^ {1/2}\left(\sum_{i\in\mathcal{A}}\frac{(p_{i}-q_{i})^{2}}{q_{i}}\right)\] \[= 4\left(2\sqrt{k}+2\sqrt{\frac{1}{\alpha m_{2}}\mathbb{E}[Z_{2}] }\right)\mathbb{E}[Z_{2}]\] \[= 8\sqrt{k}\mathbb{E}[Z_{2}]+8(\alpha m_{2})^{-1/2}(\mathbb{E}[Z_{ 2}])^{3/2}.\]

Combing both, we have that

\[\operatorname{Var}[Z_{2}]\leqslant 4k+\left(\frac{4}{\alpha m_{2}}+8\sqrt{k} \right)\mathbb{E}[Z_{2}]+8(\alpha m_{2})^{-1/2}(\mathbb{E}[Z_{2}])^{3/2}.\] (11)

When \(d_{\chi^{2}}(p_{\mathcal{A}},q_{\mathcal{A}})\leqslant\varepsilon/2\), then \(\mathbb{E}[Z_{2}]\leqslant\frac{m_{2}\varepsilon}{2}\); and we solve \(\operatorname{Var}[Z_{2}]\leqslant(\frac{1}{32}m_{2}\varepsilon)^{2}\), which gives

\[4k+\left(\frac{4}{\alpha m_{2}}+8\sqrt{k}\right)\frac{m_{2}\varepsilon}{2}+8 (\alpha m_{2})^{-1/2}(\frac{m_{2}\varepsilon}{2})^{3/2}\leqslant(\frac{1}{32} m_{2}\varepsilon)^{2}.\]

We solve for the relaxation:

\[\operatorname{LHS}\leqslant 4\cdot\max\left\{4k,\frac{2\varepsilon}{\alpha},4 \sqrt{k}m_{2}\varepsilon,8\frac{m_{2}}{\sqrt{\alpha}2^{3/2}}\varepsilon^{3/2} \right\}\leqslant(\frac{1}{32}m_{2}\varepsilon)^{2}\]

In the end, we obtain:

\[\max\left\{128\cdot\frac{\sqrt{k}}{\varepsilon},64\sqrt{\frac{2}{\alpha \varepsilon}},32^{2}\cdot 16\sqrt{k},32^{2}\cdot 16\frac{1}{\sqrt{\alpha \varepsilon}\sqrt{2}}\right\}\leqslant 32^{2}\cdot 16\cdot\max\left\{\frac{ \sqrt{k}}{\varepsilon},\sqrt{\frac{1}{\alpha\varepsilon}}\right\}\leqslant m_ {2}\]

When \(d_{\chi^{2}}(p_{\mathcal{A}},q_{\mathcal{A}})\geqslant\varepsilon\), then \(\mathbb{E}[Z_{2}]\geqslant m_{2}\varepsilon\); and we solve \(\operatorname{Var}[Z_{2}]\leqslant(\frac{1}{4}\mathbb{E}[Z_{2}])^{2}\),

\[4k+\left(\frac{4}{\alpha m_{2}}+8\sqrt{k}\right)\mathbb{E}[Z_{2}]+8(\alpha m _{2})^{-1/2}(\mathbb{E}[Z_{2}])^{3/2}\leqslant(\frac{1}{4}\mathbb{E}[Z_{2}] )^{2},\]

which is equivalent to the following

\[\frac{4k}{(\mathbb{E}[Z_{2}])^{3/2}}+\left(\frac{4}{\alpha m_{2}}+8\sqrt{k} \right)\frac{1}{(\mathbb{E}[Z_{2}])^{1/2}}+8(\alpha m_{2})^{-1/2}\leqslant \frac{1}{16}(\mathbb{E}[Z_{2}])^{1/2}\]Further relaxing the solution, it is enough to have

\[\frac{4k}{(\mathbb{E}[Z_{2}])^{3/2}}+\left(\frac{4}{\alpha m_{2}}+8 \sqrt{k}\right)\frac{1}{(\mathbb{E}[Z_{2}])^{1/2}}+8(\alpha m_{2})^{-1/2}\] \[\leqslant \frac{4k}{(m_{2}\varepsilon)^{3/2}}+\left(\frac{4}{\alpha m_{2}}+8 \sqrt{k}\right)\frac{1}{(m_{2}\varepsilon)^{1/2}}+8\frac{1}{\sqrt{\alpha m_{2}}}\] \[\leqslant \frac{1}{16}(m_{2}\varepsilon)^{1/2}\leqslant\frac{1}{16}( \mathbb{E}[Z_{2}])^{1/2},\]

as long as the following holds,

\[m_{2}\geqslant 64\max\left\{\frac{2\sqrt{k}}{\varepsilon},2\sqrt{\frac{1}{ \alpha\varepsilon}},8\frac{\sqrt{k}}{\varepsilon},8\sqrt{\frac{\alpha}{ \varepsilon}}\right\}=\max\left\{128\sqrt{\frac{1}{\alpha\varepsilon}},512 \frac{\sqrt{k}}{\varepsilon}\right\}.\] (12)

Letting \(m_{2}\geqslant 512\max\left\{\sqrt{\frac{1}{\alpha\varepsilon}},\frac{\sqrt{k}}{ \varepsilon}\right\}\), we have that both statements. 

## Appendix C Proofs of entropy testing lower bounds

**Lemma 2.6**.: _With fewer than \(c_{3}\cdot\sqrt{k}/\varepsilon\) samples from \(p\), no tester can distinguish between \(p=q\) and \(|H(p)-H(q)|\geqslant\varepsilon\) with probability higher than \(2/3\), where \(c_{3}>0\) is an absolute constant._

Proof of Lemma 2.6.: This follows from the standard uniformity testing lower bound [10], which provides a lower bound of \(\Omega(\sqrt{k}/\eta^{2})\): there exists a family of distributions that are hard to distinguish from uniform \(u_{k}\), using fewer than \(c_{1}\cdot\sqrt{k}/\eta\) samples. Let \(k\) be an even number; the construction is by taking \(\theta=\{-1,1\}^{k/2}\) uniformly at random, and letting, for every \(i\in[k/2]\),

\[p_{\theta}^{\mathrm{no}}(2i)=\frac{1+\theta_{i}\cdot\eta}{k},\qquad p_{\theta }^{\mathrm{no}}(2i+1)=\frac{1-\theta_{i}\cdot\eta}{k}.\]

We can verify that for any \(\theta\):

\[|H(p_{\theta}^{\mathrm{no}})-H(u_{k})|=\log k-\frac{k}{2}\left(\frac{1+\eta}{ k}\log\left(\frac{1+\eta}{k}\right)+\frac{1-\eta}{k}\log\left(\frac{1-\eta}{k} \right)\right)=\Theta(\eta^{2})\]

Setting \(\eta=\varepsilon^{2}\) yields the lower bound of \(\Omega\left(\frac{\sqrt{k}}{\varepsilon}\right)\). 

**Lemma 2.7**.: _With fewer than \(c_{4}\cdot\log^{2}k/\varepsilon^{2}\) samples from \(p\), no tester can distinguish between \(p=q\) and \(|H(p)-H(q)|\geqslant\varepsilon\) with probability higher than \(2/3\), where \(c_{4}>0\) is an absolute constant._

Proof of Lemma 2.7.: Following [11, B.2 Proof of Proposition 2], we look at the same construction but with different parameters \(\varepsilon^{\prime}=\frac{\varepsilon}{\log(2(k-1))}\):

\[p=\left(\frac{1}{3(k-1)},\ldots,\frac{1}{3(k-1)},\frac{2}{3}\right),\quad q= \left(\frac{1+\varepsilon^{\prime}}{3(k-1)},\ldots,\frac{1+\varepsilon^{ \prime}}{3(k-1)},\frac{2-\varepsilon^{\prime}}{3}\right).\]

One can check that

\[H(q)-H(p)\geqslant\frac{1}{3}\log(2(k-1))\varepsilon^{\prime}{-\varepsilon^{ \prime}}^{2}=\Omega(\varepsilon).\]

Moreover, direct calculation of the (squared) Hellinger distance shows that

\[\mathrm{d}_{\mathrm{H}}(p,q)^{2}=\Theta(\varepsilon^{\prime 2})=\Theta\left( \frac{\varepsilon^{2}}{\log^{2}k}\right)\]

which implies that \(p\) and \(q\) cannot be distinguished with fewer than \(c_{4}\frac{\log^{2}k}{\varepsilon^{2}}\) samples [1, Theorem 4.7].

Sketch proof of \(O\left(\frac{\sqrt{k\log\log\log(k/\varepsilon)}}{\varepsilon}+\frac{\log^{2}(k)}{ \varepsilon^{2}}\right)\) upper bound.

We will rely the following inequality for compression, both via Jensen's inequality,

\[\left(\sum_{i\in\Delta}p_{i}\right)\log\left(\frac{1}{\sum_{i\in\Delta}p_{i}} \right)\leqslant\sum_{i\in\Delta}p_{i}\log\frac{1}{p_{i}}\leqslant\left(\sum_ {i\in\Delta}p_{i}\right)\log\left(\frac{|\Delta|}{\sum_{i\in\Delta}p_{i}} \right),\] (13)

as \(\log(x)\) is concave and \(\log\left(\frac{1}{x}\right)\) is convex.

\[\sum_{i\in\Delta}p_{i}\log\frac{1}{p_{i}}\geqslant\left(\sum_{i\in\Delta}p_{i }\right)\log\left(\frac{\sum_{i\in\Delta}p_{i}}{\sum_{i\in\Delta}p_{i}^{2}} \right)\geqslant\left(\sum_{i\in\Delta}p_{i}\right)\log\left(\frac{1}{\sum_{ i\in\Delta}p_{i}}\right),\]

suggesting that if we merge elements of \(\Delta\) into one, then we will lose a \(\log(|\Delta|)\) factor of the entropy. By merging enough elements, we can then reduce this problem into the first case, where elements have large enough mass in each location.

**Claim D.1**.: _Let \(\mathcal{S}\subseteq[k]\), if \(p(\mathcal{S})-q(\mathcal{S})>-\eta\), then_

\[|d_{\mathrm{KL}}(p_{\mathcal{S}}\|q_{\mathcal{S}})|\geqslant\eta\,\,\mathrm{ then}\]

Proof.: If \(|d_{\mathrm{KL}}(p_{\mathcal{S}}\|q_{\mathcal{S}})|\geqslant\eta\,\,\mathrm{or }\,\,d_{\mathrm{KL}}(p_{\mathcal{S}}\|q_{\mathcal{S}})\leqslant-\eta\).

We will rule out the second case, using that \(\log\frac{1}{x}\geqslant 1-x\) for \(x>0\), 10

Footnote 10: In the case of \(p_{i}=0\), we still have \(p_{i}\log\left(\frac{p_{i}}{q_{i}}\right)\geqslant p_{i}-q_{i}\).

\[d_{\mathrm{KL}}(p_{\mathcal{S}}\|q_{\mathcal{S}})=\sum_{i\in\mathcal{S}}p_{i} \log\frac{p_{i}}{q_{i}}\geqslant\sum_{i\in\mathcal{S}}p_{i}\left(1-\frac{q_{i }}{p_{i}}\right)=p(\mathcal{S})-q(\mathcal{S})>-\eta.\] (14)

Thus, we cannot have \(d_{\mathrm{KL}}(p_{\mathcal{S}}\|q_{\mathcal{S}})\leqslant-\eta\) and so \(d_{\mathrm{KL}}(p_{\mathcal{S}}\|q_{\mathcal{S}})\geqslant\eta\). 

We use the same idea as our first upper bound but choose a series of thresholds. Let

\[\mathcal{S}_{3}:=\left\{i\in[k]|q_{i}\geqslant\Omega\left(\frac{\varepsilon}{ k\log\log(k/\varepsilon)}\right)\right\};\]

\[\mathcal{S}_{2}=\left\{i\in[k]|\Omega\left(\frac{\varepsilon}{k\log\log(k/ \varepsilon)}\right)\leqslant q_{i}\leqslant O\left(\frac{\varepsilon}{k\log \log(k/\varepsilon)}\right)\right\};\]

\[\mathcal{S}_{1}=\left\{i\in[k]|\Omega\left(\frac{\varepsilon}{k\log(k/ \varepsilon)}\right)\leqslant q_{i}\leqslant O\left(\frac{\varepsilon}{k\log \log(k/\varepsilon)}\right)\right\}.\]

The following calculation ensues

\[\Omega(\varepsilon) \leqslant |H(p_{\mathcal{A}})-H(q_{\mathcal{A}})|\] \[\leqslant \left|\sum_{i=1}^{3}d_{\mathrm{KL}}(p_{\mathcal{S}_{i}},q_{ \mathcal{S}_{i}})\right|+\left|\sum_{i\in\mathcal{A}}(p_{i}-q_{i})\log\frac{1}{ q_{i}}\right|\] \[\leqslant \sum_{i=1}^{3}|d_{\mathrm{KL}}(p_{\mathcal{S}_{i}},q_{\mathcal{S} _{i}})|+\left|\sum_{i\in\mathcal{A}}(p_{i}-q_{i})\log\frac{1}{q_{i}}\right|.\]

We have that one of the four terms will be at least \(\Omega(\varepsilon/4)\). If it is

\[\left|\sum_{i\in\mathcal{A}}(p_{i}-q_{i})\log\frac{1}{q_{i}}\right|\geqslant \Omega(\varepsilon),\]

which is testable with \(O\left(\frac{\log^{2}(k)}{\varepsilon^{2}}\right)\) samples using arguments from proof of Theorem 2.1. If it is \(|d_{\mathrm{KL}}(p_{\mathcal{S}_{i}},q_{\mathcal{S}_{i}})|\geqslant\Omega(\varepsilon)\), for \(i=1,2,3\). We have the following:

**Case \(\mathcal{S}_{3}\).** Suppose \(|d_{\mathrm{KL}}(p_{\mathcal{S}_{3}},q_{\mathcal{S}_{3}})|\geqslant\Omega(\varepsilon)\). We will check whether \(p(\mathcal{S}_{3})\geqslant\Omega\left(\frac{\varepsilon}{\log\log\log(k/ \varepsilon)}\right)\), if not, we can reject. We proceed assuming the inequality holds. Note that

\[|p(\mathcal{S}_{3})-q(\mathcal{S}_{3})|=|p(\overline{\mathcal{S}_{3}})-q( \overline{\mathcal{S}_{3}})|\leqslant O\left(\frac{\varepsilon}{\log\log\log(k /\varepsilon)}\right).\]

Thus, \(p(\overline{\mathcal{S}_{3}})-q(\overline{\mathcal{S}_{3}})>-O\left(\frac{ \varepsilon}{\log\log\log(k/\varepsilon)}\right)\), and by Claim D.1, we have that \(d_{\mathrm{KL}}(p_{\mathcal{S}_{3}},q_{\mathcal{S}_{3}})\geqslant\Omega(\varepsilon)\). Using (4), we then have that \(d_{\chi^{2}}(p_{\mathcal{S}_{3}},q_{\mathcal{S}_{3}})\geqslant\Omega(\varepsilon)\). Using Lemma 2.4 (setting \(\alpha=O\left(\frac{\varepsilon}{k\log\log\log(k/\varepsilon)}\right)\)), and similar argument from the proof of Theorem 2.1, we have that \(O\left(\frac{\sqrt{k\log\log\log(k/\varepsilon)}}{\varepsilon}\right)\) suffices to check between the case that \(d_{\chi^{2}}(p_{\mathcal{S}_{3}},q_{\mathcal{S}_{3}})\geqslant\Omega(\varepsilon)\) and \(p_{\mathcal{S}_{3}}=q_{\mathcal{S}_{3}}\).

**Case \(\mathcal{S}_{2}\).** Suppose \(|d_{\mathrm{KL}}(p_{\mathcal{S}_{2}},q_{\mathcal{S}_{2}})|\geqslant\Omega(\varepsilon)\). We will check whether

\[\Omega\left(\frac{\varepsilon}{\log\log(k/\varepsilon)}\right)\leqslant p( \mathcal{S}_{2})\leqslant O\left(\frac{\varepsilon}{\log\log\log(k/\varepsilon )}\right),\]

if not, we will reject. We proceed assuming the inequality holds. Now, recall that the main bottleneck of the \(\chi^{2}\) tester analyzed in Lemma 2.4 is due to the minimum probability \(\alpha=\min_{i\in\mathcal{S}_{2}}q_{i}\) (increasing this would decrease the sample complexity). Our main idea here is to increase \(\alpha\) by merging a suitable number (\(\log\log(k/\varepsilon)\) in this case) of elements into one single bin to form a new distribution to test. Denote \(\Delta_{j}\) where \(j\in\left[\frac{|\mathcal{S}_{2}|}{\log\log(k/\varepsilon)}\right]\) and \(\bigcup_{j}\Delta_{j}=\mathcal{S}_{2}\). We will subsequently treat every elements in \(\Delta_{j}\) as 1 bin in the new distribution, calling it \(p_{\Delta},q_{\Delta}\) and denote \(p(\Delta_{j}),q(\Delta_{j})\) as mass on \(\Delta_{j}\), where \(p(\Delta_{j})=\sum_{i\in\Delta_{j}}p_{i}\). This gives us the following:

1. \(q(\Delta_{j})\geqslant\Omega\left(\frac{\varepsilon}{k\log\log(k/\varepsilon)} \right)\cdot|\Delta_{j}|\geqslant\Omega\left(\frac{\varepsilon}{k}\right);\) the domain size is \(\frac{|\mathcal{S}_{2}|}{\min_{j}|\Delta_{j}|}\leqslant O\left(\frac{k}{\log \log(k/\varepsilon)}\right)\).
2. \(\sum_{j}p(\Delta_{j})=p(\mathcal{S}_{2})\leqslant O\left(\frac{\varepsilon}{ \log\log\log(k/\varepsilon)}\right)\) and \(\sum_{j}q(\Delta_{j})=q(\mathcal{S}_{2})\leqslant O\left(\frac{\varepsilon}{ \log\log\log(k/\varepsilon)}\right)\).
3. Their entropy difference is preserved, which we will prove next: \[\left|\sum_{j}p(\Delta_{j})\log\frac{1}{p(\Delta_{j})}-\sum_{j}q(\Delta_{j}) \log\frac{1}{q(\Delta_{j})}\right|\geqslant\Omega(\varepsilon).\]

Note that these are better conditions compared to i. and ii. in the proof of Theorem 2.1 (in this analysis, using ii., it is sufficient to prove that \(d_{\mathrm{KL}}(p_{\Delta},q_{\Delta})\geqslant\Omega(\varepsilon)\) in view of Claim D.1). The gain comes from the fact that we can apply Lemma 2.4 with better \(\alpha=\min_{j}q(\Delta_{j})\geqslant\Omega\left(\frac{\varepsilon}{k}\right)\) and thus

\[O\left(\sqrt{\frac{1}{\alpha\varepsilon}}+\sqrt{\frac{k^{\prime}}{\varepsilon}} \right)=O\left(\sqrt{\frac{k}{\varepsilon^{2}}}\right)=O\left(\frac{\sqrt{k}}{ \varepsilon}\right).\]

However, the gain only affect Claim 2.3 by constant factors. The soundness and completeness then follows similarly to the proof of Theorem 2.1. We prove (iii.) next:Suppose \(H(p_{\mathcal{S}_{2}})-H(q_{\mathcal{S}_{2}})\geqslant\varepsilon\), then,

\[\Omega(\varepsilon)\] (15) \[\leqslant \sum_{l\in\mathcal{S}_{2}}p_{l}\log\frac{1}{p_{l}}-\sum_{l\in \mathcal{S}_{2}}q_{l}\log\frac{1}{q_{l}}\] \[= \sum_{j}\sum_{i\in\Delta_{j}}p_{i,j}\log\frac{1}{p_{i,j}}-\sum_{j} \sum_{i\in\Delta_{j}}q_{i,j}\log\frac{1}{q_{i,j}}\] \[\leqslant \sum_{j}p(\Delta_{j})\log\frac{|\Delta_{j}|}{p(\Delta_{j})}-\sum_ {j}q(\Delta_{j})\log\frac{1}{q(\Delta_{j})}\] \[= \sum_{j}p(\Delta_{j})\log|\Delta_{j}|+\sum_{j}p(\Delta_{j})\log \frac{1}{p(\Delta_{j})}-\sum_{j}q(\Delta_{j})\log\frac{1}{q(\Delta_{j})}.\] \[\leqslant O\left(\frac{\varepsilon}{\log\log\log(k/\varepsilon)}\right) \max_{j}\log|\Delta_{j}|+\sum_{j}p(\Delta_{j})\log\frac{1}{p(\Delta_{j})}- \sum_{j}q(\Delta_{j})\log\frac{1}{q(\Delta_{j})}.\] (16)

where the (15) is due to (13) and for (16), recall that \(\sum_{j}p(\Delta_{j})=p(\mathcal{S}_{2})\leqslant O\left(\frac{\varepsilon}{ \log\log\log(k/\varepsilon)}\right)\). Suppose \(H(q_{\mathcal{S}_{2}})-H(p_{\mathcal{S}_{2}})\geqslant\Omega(\varepsilon)\), the same goes below:

\[\Omega(\varepsilon) \leqslant \sum_{l}q_{l}\log\frac{1}{q_{l}}-\sum_{l}p_{l}\log\frac{1}{p_{l}}\] \[= \sum_{j}\sum_{i\in\Delta_{j}}q_{i,j}\log\frac{1}{q_{i,j}}-\sum_{j }\sum_{i\in\Delta_{j}}p_{i,j}\log\frac{1}{p_{i,j}}\] \[\leqslant \sum_{j}q(\Delta_{j})\log\frac{|\Delta_{j}|}{q(\Delta_{j})}-\sum_ {j}p(\Delta_{j})\log\frac{1}{p(\Delta_{j})}\] \[\leqslant O(\varepsilon)+\sum_{j}q(\Delta_{j})\log\frac{1}{q(\Delta_{j})}- \sum_{j}p(\Delta_{j})\log\frac{1}{p(\Delta_{j})}.\]

Therefore, we have proved (iii.).

**Case \(\mathcal{S}_{1}\).** The proof follow similar to Case \(\mathcal{S}_{2}\), but by merging \(\log(k/\varepsilon)\) elements.

## Appendix E Proofs of testing Bayesian networks

**Claim E.1**.: _Suppose that \(p(\bar{\mathcal{U}})\leqslant O(\varepsilon^{2}/\log(1/\varepsilon))\), then we have for any distribution \(q\),_

\[d_{\mathrm{KL}}(p_{\bar{\mathcal{U}}}\|q_{\bar{\mathcal{U}}})\geqslant-p(\bar {\mathcal{U}})\cdot\log\left(\frac{q(\bar{\mathcal{U}})}{p(\bar{\mathcal{U}})} \right)\geqslant-O(\varepsilon^{2}).\]

_This implies that \(d_{\mathrm{KL}}(p_{\bar{\mathcal{U}}}\|p_{G;\bar{\mathcal{U}}})\geqslant-O( \varepsilon^{2})\) for any DAG \(G\). Moreover, if \(d_{\mathrm{KL}}(p\|p_{G})\leqslant O(\varepsilon^{2})\), then \(d_{\mathrm{KL}}(p_{\mathcal{U}}\|p_{G;\mathcal{U}})\leqslant O(\varepsilon^{2})\)._Proof of Claim E.1.: \[d_{\mathrm{KL}}(p_{\bar{\mathcal{U}}}\|p_{G;\bar{\mathcal{U}}}) = \sum_{x\in\mathcal{U}}p(x)\log\frac{p(x)}{p_{G}(x)}\] \[= -\sum_{x\in\bar{\mathcal{U}}}p(x)\log\frac{p_{G}(x)}{p(x)}\] \[\geqslant -\left(\sum_{x\in\bar{\mathcal{U}}}p(x)\right)\cdot\log\left( \frac{\sum_{x\in\bar{\mathcal{U}}}p(x)\cdot\frac{p_{G}(x)}{p(x)}}{\sum_{x\in \bar{\mathcal{U}}}p(x)}\right)\] \[= -p(\bar{\mathcal{U}})\cdot\log\left(\frac{p_{G}(\bar{\mathcal{U}} )}{p(\bar{\mathcal{U}})}\right)\] \[\geqslant -O(\varepsilon^{2}/\log(1/\varepsilon))\cdot\log\left(\frac{1}{ \varepsilon^{2}/\log(1/\varepsilon)}\right)\] \[\geqslant -O(\varepsilon^{2}),\]

where we use monotonicity of \(-x\log\frac{1}{x}\) and the fact that \(p_{G}(\bar{\mathcal{U}})\leqslant 1\) in the second last inequality. Since \(d_{\mathrm{KL}}(p\|p_{G})=d_{\mathrm{KL}}(p_{\mathcal{U}}\|p_{G;\bar{ \mathcal{U}}})+d_{\mathrm{KL}}(p_{\bar{\mathcal{U}}}\|p_{G;\bar{\mathcal{U}}}) \leqslant O(\varepsilon^{2})\) and \(d_{\mathrm{KL}}(p_{\bar{\mathcal{U}}}\|p_{G;\bar{\mathcal{U}}})\geqslant-O( \varepsilon^{2})\), we can rearrange and see that \(d_{\mathrm{KL}}(p_{\mathcal{U}}\|p_{G;\mathcal{U}})\leqslant O(\varepsilon^{2})\). 

We will need the following claim for the proof of Lemma 3.3.

**Claim E.2**.: _The following inequalities hold, for any \(i\in[n]\)_

\[\sum_{x^{\prime}\in\mathcal{A}_{i}^{\prime}}p_{X_{i},\Pi_{i}^{G}}(x^{\prime} )\log\frac{p(\pi_{i}^{G}(x^{\prime}))}{q(\pi_{i}^{G}(x^{\prime}))}\geqslant-O \left(\frac{\varepsilon^{2}}{n^{2}}\right).\] (17)

\[\sum_{x\in\mathcal{A}_{i}\setminus\mathcal{U}}p(x)\log\frac{p(x_{i}(x)|\pi_{i} ^{G}(x))}{q(x_{i}(x)|\pi_{i}^{G}(x))}\geqslant-O\left(\frac{\varepsilon^{2}}{n }\right).\] (18)

Proof.: We will show (17) first.

\[\sum_{x^{\prime}\in\mathcal{A}_{i}^{\prime}}p_{X_{i},\Pi_{i}^{G}} (x^{\prime})\log\frac{p(\pi_{i}^{G}(x^{\prime}))}{q(\pi_{i}^{G}(x^{\prime}))} \geqslant \left(\sum_{x^{\prime}\in\mathcal{A}_{i}^{\prime}}p_{X_{i},\Pi_{ i}^{G}}(x^{\prime})\right)\cdot\log\left(\frac{\sum_{x^{\prime}\in\mathcal{A}_{i}^{ \prime}}p_{X_{i},\Pi_{i}^{G}}(x^{\prime})}{\sum_{x^{\prime}\in\mathcal{A}_{i}^ {\prime}}p_{X_{i},\Pi_{i}^{G}}(x^{\prime})\cdot\frac{q(\pi_{i}^{G}(x^{\prime}) )}{p(\pi_{i}^{G}(x^{\prime}))}}\right)\] \[\geqslant \left(\sum_{x^{\prime}\in\mathcal{A}_{i}^{\prime}}p_{X_{i},\Pi_{ i}^{G}}(x^{\prime})\right)\cdot\log\left(\sum_{x^{\prime}\in\mathcal{A}_{i}^{ \prime}}p_{X_{i},\Pi_{i}^{G}}(x^{\prime})\right)\] \[\geqslant \left(1-O\left(\frac{\varepsilon^{2}}{n^{2}\log(n/\varepsilon)} \right)\right)\log\left(1-O\left(\frac{\varepsilon^{2}}{n^{2}\log(n/\varepsilon )}\right)\right)\] \[\geqslant -O\left(\frac{\varepsilon^{2}}{n^{2}\log(n/\varepsilon)}\right)\]

where the first step follows from Jensen's inequality applied to the function \(f(x)=\log(1/x)\), and the second-to-last step is due to \(x\log x\) is monotonically increasing when \(x\geqslant\frac{1}{e}\) and \(\log(1-x)\geqslant-2x\) when \(x\in(0,0.5)\). This concludes the proof of (17).

We next move on to proving (18).

\[\sum_{x\in\mathcal{A}_{i}\setminus\mathcal{U}}p(x)\log\frac{p(x_{i}(x )|\pi_{i}^{G}(x))}{q(x_{i}(x)|\pi_{i}^{G}(x))} \geqslant \left(\sum_{x\in\mathcal{A}_{i}\setminus\mathcal{U}}p(x)\right) \log\left(\frac{\sum_{x\in\mathcal{A}_{i}\setminus\mathcal{U}}p(x)}{\sum_{x\in \mathcal{A}_{i}\setminus\mathcal{U}}p(x)\cdot\frac{q(x_{i}(x)|\pi_{i}^{G}(x))} {p(x_{i}(x)|\pi_{i}^{G}(x))}}\right)\] \[= \left(\sum_{x\in\mathcal{A}_{i}\setminus\mathcal{U}}p(x)\right) \log\left(\frac{\sum_{x\in\mathcal{A}_{i}\setminus\mathcal{U}}p(x)}{\sum_{x\in \mathcal{A}_{i}\setminus\mathcal{U}}\frac{p(x)}{p(x_{i}(x)|\pi_{i}^{G}(x))}q(x _{i}(x)|\pi_{i}^{G}(x))}\right)\] \[\geqslant \left(\sum_{x\in\mathcal{A}_{i}\setminus\mathcal{U}}p(x)\right) \log\left(\sum_{x\in\mathcal{A}_{i}\setminus\mathcal{U}}p(x)\right)\] \[\geqslant -O\left(\frac{\varepsilon^{2}}{n}\right),\]

where the second-to-last step follows from Equation (19) below, and last step by monotonicity (decreasing) of \(f(x)=x\log x\) when \(x\leqslant\frac{1}{e}\) and \(\sum_{x\in\mathcal{A}_{i}\setminus\mathcal{U}}p(x)\leqslant\sum_{i\neq j}\sum_ {x\in\mathcal{A}_{j}}p(x)\leqslant\sum_{i\neq j}O\left(\frac{\varepsilon^{2}}{ n^{2}\log(n/\varepsilon)}\right)\leqslant O\left(\frac{\varepsilon^{2}}{n\log(n/ \varepsilon)}\right)\).

\[\sum_{x\in\mathcal{A}_{i}\setminus\mathcal{U}}\frac{p(x)}{p(x_{i }(x)|\pi_{i}^{G}(x))}q(x_{i}(x)|\pi_{i}^{G}(x))\] \[= \sum_{x\in\mathcal{A}_{i}\setminus\mathcal{U}}\frac{p(x)}{p(x_{i }(x)|\pi_{i}^{G}(x))}q(x_{i}(x)|\pi_{i}^{G}(x))\] \[= \sum_{x\in\mathcal{A}_{i}\setminus\mathcal{U}}\frac{p(\pi_{i}^{G} (x))q(x_{i}(x)|\pi_{i}^{G}(x))}{p(x_{i}(x),\pi_{i}^{G}(x))}p(x)\] \[= \sum_{x\in\mathcal{A}_{i}\setminus\mathcal{U}}p(\pi_{i}^{G}(x))q( x_{i}(x)|\pi_{i}^{G}(x))\cdot p(x|x_{i}(x),\pi_{i}^{G}(x))\]

\[\leqslant \sum_{(x_{i},\pi_{i}^{G})\in\{0,1\}^{|\Pi_{i}^{G}|+1}}p(\pi_{i}^ {G})q(x_{i}|\pi_{i}^{G})\left(\underbrace{\sum_{x^{\prime}\in\{0,1\}^{n-|\Pi_{ i}|+1}}p(x^{\prime}|x_{i},\pi_{i}^{G})}_{=1}\right)\] \[= \sum_{\pi_{i}^{G}\in\{0,1\}^{|\Pi_{i}^{G}|}}p(\pi_{i}^{G})\underbrace {\sum_{x_{i}\in\{0,1\}}q(x_{i}|\pi_{i}^{G})}_{=1}=1.\]

This concludes the proof of (18). 

**Lemma 3.3**.: _Suppose \(d_{H}^{2}(p,q)\geqslant\Omega(\varepsilon^{2})\); \(d_{\mathrm{KL}}(p\|p_{G})\leqslant O(\varepsilon^{2})\); \(p(\bar{\mathcal{U}})\leqslant\frac{\varepsilon^{2}}{n\log(n/\varepsilon)}\) ; \(\forall i\in[n],p(\bar{\mathcal{A}}_{i}^{\prime})\leqslant\frac{\varepsilon^{2} }{n^{2}\log(n/\varepsilon)}\), where \(\mathcal{A}_{i}^{\prime}\) is defined above, and \(q\) is Markov with respect to \(G\), then we have_

\[\sum_{i=1}^{n}d_{\mathrm{KL}}(p_{X_{i},\Pi_{i}^{G};\mathcal{A}_{i}^{\prime}}\| q_{X_{i},\Pi_{i}^{G};\mathcal{A}_{i}^{\prime}})\geqslant\Omega(\varepsilon^{2}).\]

_Therefore testing \(d_{\mathrm{KL}}(p_{X_{i},\Pi_{i}^{G};\mathcal{A}_{i}^{\prime}}\|q_{X_{i},\Pi_{ i}^{G};\mathcal{A}_{i}^{\prime}})\geqslant\frac{\varepsilon^{2}}{n}\) over all \(i\) suffices to detect this case._

Proof of Lemma 3.3.: By Claim E.1 and the assumption that \(d_{\mathrm{KL}}(p\|p_{G})\leqslant O(\varepsilon^{2})\), we have that

\[d_{\mathrm{KL}}(p_{\mathcal{U}}\|p_{G;\mathcal{U}})=d_{\mathrm{KL}}(p\|p_{G})- d_{\mathrm{KL}}(p_{\bar{\mathcal{U}}}\|p_{G;\bar{\mathcal{U}}})\leqslant O( \varepsilon^{2}).\] (20)

\[\Omega(\varepsilon^{2})\leqslant d_{H}^{2}(p,q)=d_{H}^{2}(p_{\mathcal{U}},q_{ \mathcal{U}})+d_{H}^{2}(p_{\bar{\mathcal{U}}},q_{\bar{\mathcal{U}}})\leqslant d _{H}^{2}(p_{\mathcal{U}},q_{\mathcal{U}})+\frac{1}{2}(p(\bar{\mathcal{U}})+q (\bar{\mathcal{U}}))\leqslant d_{H}^{2}(p_{\mathcal{U}},q_{\mathcal{U}})+O( \varepsilon^{2}).\]\[\Omega(\varepsilon^{2})\leqslant d_{H}^{2}(p_{\mathcal{U}},q_{\mathcal{U}}) \leqslant d_{\mathrm{KL}}(p_{\mathcal{U}}\|q_{\mathcal{U}})+q(\mathcal{U})-p( \mathcal{U})\Rightarrow d_{\mathrm{KL}}(p_{\mathcal{U}}\|q_{\mathcal{U}}) \geqslant\Omega(\varepsilon^{2}).\] (21)

Combining and (20) and (21), we write

\[\Omega(\varepsilon^{2}) \leqslant d_{\mathrm{KL}}(p_{\mathcal{U}}\|q_{\mathcal{U}})-d_{\mathrm{KL }}(p_{\mathcal{U}}\|p_{G\mathcal{U}})\] \[= \sum_{x\in\mathcal{U}}p(x)\sum_{i=1}^{n}\log\frac{p(x_{i}(x)|\pi_{ i}^{G}(x))}{q(x_{i}(x)|\pi_{i}^{G}(x))}\] \[= \sum_{i=1}^{n}\left(\sum_{x\in\mathcal{A}_{i}}p(x)\log\frac{p(x_{ i}(x)|\pi_{i}^{G}(x))}{q(x_{i}(x)|\pi_{i}^{G}(x))}-\sum_{x\in\mathcal{A}_{i} \setminus\mathcal{U}}p(x)\log\frac{p(x_{i}(x)|\pi_{i}^{G}(x))}{q(x_{i}(x)| \pi_{i}^{G}(x))}\right)\] \[= \left(\sum_{i=1}^{n}\sum_{x\in\mathcal{A}_{i}}p(x)\log\frac{p(x_{ i}(x)|\pi_{i}^{G}(x))}{q(x_{i}(x)|\pi_{i}^{G}(x))}\right)-\left(\sum_{i=1}^{n} \sum_{x\in\mathcal{A}_{i}\setminus\mathcal{U}}p(x)\log\frac{p(x_{i}(x)|\pi_{i }^{G}(x))}{q(x_{i}(x)|\pi_{i}^{G}(x))}\right)\] \[= \left(\sum_{i=1}^{n}\sum_{x^{\prime}\in\mathcal{A}_{i}^{G}}p_{X_ {i},\Pi_{i}^{G}}(x^{\prime})\log\frac{p(x_{i}(x^{\prime})|\pi_{i}^{G}(x^{\prime }))}{q(x_{i}(x^{\prime})|\pi_{i}^{G}(x^{\prime}))}\right)-\left(\sum_{i=1}^{n} \sum_{x\in\mathcal{A}_{i}\setminus\mathcal{U}}p(x)\log\frac{p(x_{i}(x)|\pi_{i} ^{G}(x))}{q(x_{i}(x)|\pi_{i}^{G}(x))}\right)\] \[= \sum_{i=1}^{n}d_{\mathrm{KL}}(p_{X_{i},\Pi_{i}^{G};\mathcal{A}_{ i}^{G}}\|q_{X_{i},\Pi_{i}^{G};\mathcal{A}_{i}^{G}})-\sum_{i=1}^{n}\sum_{x^{\prime} \in\mathcal{A}_{i}^{\prime}}p_{X_{i},\Pi_{i}^{G}}(x^{\prime})\log\frac{p(\pi_{ i}^{G}(x^{\prime}))}{q(\pi_{i}^{G}(x^{\prime}))}\] \[-\left(\sum_{i=1}^{n}\sum_{x\in\mathcal{A}_{i}\setminus\mathcal{U }}p(x)\log\frac{p(x_{i}(x)|\pi_{i}^{G}(x))}{q(x_{i}(x)|\pi_{i}^{G}(x))}\right)\]

With Claim E.2 and Claim E.1, we can lower bound the sum of local KL divergences,

\[\sum_{i=1}^{n}d_{\mathrm{KL}}(p_{X_{i},\Pi_{i}^{G};\mathcal{A}_{i}^{G}}\|q_{X_ {i},\Pi_{i}^{G};\mathcal{A}_{i}^{\prime}})\geqslant\Omega(\varepsilon^{2}).\]

**Lemma 3.4**.: _Let \(p\) and \(q\) be two max in-degree-\(d\) Bayes nets supported on \(\{0,1\}^{n}\) such that for every subset \(L\subseteq\{X_{1},\cdots,X_{n}\}\) of size \(d+1\), the following holds:_

\[|H(p_{L})-H(q_{L})|\leqslant O\left(\frac{\varepsilon^{2}}{n}\right).\]

_Suppose \(p\) is Markov w.r.t. \(G^{\prime}\) and \(q\) Markov w.r.t. \(G\). Then we have that_

\[d_{\mathrm{KL}}(p\|p_{G})\leqslant O(\varepsilon^{2})\text{ and }d_{\mathrm{KL}}(q\|q_{G^{\prime}}) \leqslant O(\varepsilon^{2}).\]

Proof of Lemma 3.4.: More formally, by the celebrated works of Chow and Liu [1] and its generalization to Bayes net, one can write the KL divergence between a Bayes net and its projection to any graph \(G\) as difference between sum of \(n\) local conditional entropies (we provide a derivation for the sake of completeness in Appendix F):

\[0\leqslant d_{\mathrm{KL}}(p\|p_{G})=-\sum_{i=1}^{n}H(p_{X_{i},X_{\Pi_{i}}}|p_{ X_{\Pi_{i}}})+\sum_{i=1}^{n}H(p_{X_{i},X_{\Pi_{i}^{G}}}|p_{X_{\Pi_{i}^{G}}}),\] (22)

\[0\leqslant d_{\mathrm{KL}}(q\|q_{G^{\prime}})=-\sum_{i=1}^{n}H(q_{X_{i},X_{\Pi _{i}^{G}}}|q_{X_{\Pi_{i}^{G}}})+\sum_{i=1}^{n}H(q_{X_{i},X_{\Pi_{i}}}|q_{X_{\Pi_ {i}}}),\] (23)

where \(X_{\Pi_{i}}\) denotes the parents of \(X_{i}\) in Bayes net \(p\) and \(X_{\Pi_{i}^{G}}\) denotes the parents of \(X_{i}\) in DAG \(G\). Here we assume that \(q\) is Markov with respect to \(G\). Since the local entropies between \(p\) and \(q\) are close by \(O(\varepsilon^{2}/n)\) (see (6)) and its relation to conditional entropy via (3), we can conclude the following:

\[H(q_{X_{i},X_{\Pi_{i}}}|q_{X_{\Pi_{i}}})-O(\varepsilon^{2}/n)\leqslant H(p_{X_{ i},X_{\Pi_{i}}}|p_{X_{\Pi_{i}}})\leqslant H(q_{X_{i},X_{\Pi_{i}}}|q_{X_{\Pi_{i}}})+O( \varepsilon^{2}/n).\] (24)\[H(q_{X_{i},X_{\Pi_{i}^{G}}}|q_{X_{\Pi_{i}^{G}}})-O(\varepsilon^{2}/n)\leqslant H(p_ {X_{i},X_{\Pi_{i}^{G}}}|p_{X_{\Pi_{i}^{G}}})\leqslant H(q_{X_{i},X_{\Pi_{i}^{G}}} |q_{X_{\Pi_{i}^{G}}})+O(\varepsilon^{2}/n).\] (25)

Since we assume that \(q\) is Markov with respect to \(G\), we can combine (22), (23), (24) and (25), which then give:

\[d_{\mathrm{KL}}(p\|p_{G})\leqslant-\sum_{i=1}^{n}H(q_{X_{i},X_{\Pi_{i}}}|q_{X _{\Pi_{i}}})+\sum_{i=1}^{n}H(q_{X_{i},X_{\Pi_{i}^{G}}}|q_{X_{\Pi_{i}^{G}}})+O( \varepsilon^{2})=-d_{\mathrm{KL}}(q\|q_{G^{\prime}})+O(\varepsilon^{2}),\]

where \(p\) is Markov with respect to \(G^{\prime}\), as \(\Pi_{i}\) is the set of parents of \(X_{i}\) for \(p\). Rearranging terms and we have

\[d_{\mathrm{KL}}(p\|p_{G})+d_{\mathrm{KL}}(q\|q_{G^{\prime}})\leqslant O( \varepsilon^{2}).\] (26)

Since \(\mathrm{KL}\)-divergence is nonnegative, we conclude that both terms must be at most \(O(\varepsilon^{2})\). 

## Appendix F Derivation for KL decomposition

Below, we provide a full proof on decomposing the KL divergence between a Bayes net \(p\) and its projection \(p_{G}\), for any DAG \(G\), into local conditional entropies.

\[d_{\mathrm{KL}}(p,p_{G})\] \[= \sum_{x\in\{0,1\}^{n}}p(x)\log\frac{p(x)}{p_{G}(x)}\] \[= \sum_{x\in\{0,1\}^{n}}p(x)\log\frac{\prod_{i=1}^{n}p(x_{i}|\pi_{i })}{\prod_{i=1}^{n}p_{G}(x_{i}|\pi_{i}^{G})}\] \[= \sum_{x\in\{0,1\}^{n}}p(x)\log\left(\prod_{i=1}^{n}p(x_{i}|\pi_{i })\right)-p(x)\log\left(\prod_{i=1}^{n}p_{G}(x_{i}|\pi_{i}^{G})\right)\] \[= \sum_{x\in\{0,1\}^{n}}\sum_{i=1}^{n}p(x)\log(p(x_{i}|\pi_{i}))-p( x)\log(p_{G}(x_{i}|\pi_{i}^{G}))\] \[= \sum_{i=1}^{n}\sum_{x\in\{0,1\}^{n}}p(x)\log(p(x_{i}|\pi_{i}))-p( x)\log(p_{G}(x_{i}|\pi_{i}^{G}))\] \[= \sum_{i=1}^{n}\left(\sum_{x_{i},\pi_{i}\in X_{i},\Pi_{i}}p(x_{i}, \pi_{i})\log(p(x_{i}|\pi_{i}))\right)-\left(\sum_{x_{i},\pi_{i}\in X_{i},\Pi_ {i}^{G}}p(x_{i},\pi_{i}^{G})\log(p_{G}(x_{i}|\pi_{i}^{G}))\right)\] \[= \sum_{i=1}^{n}H(p_{X_{i},\Pi_{i}^{G}}|p_{\Pi_{i}^{G}})-H(p_{X_{i},\Pi_{i}}|p_{\Pi_{i}}),\]

where \(\pi_{i},\Pi_{i}\) denote the parents of \(x_{i},X_{i}\) in Bayes net \(p\) (a set of random variables or their domain); and \(\pi_{i}^{G},\Pi_{i}^{G}\) as the parents defined by \(G\). \(p_{G}\) is the projection of \(p\) unto \(G\) as defined by Definition 3.2. It is not hard to see that the derivation extends beyond the case of hypercube, \(\{0,1\}^{n}\).

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: The proofs are provided either in the main paper or the appendix, both part of the submission. Assumptions are fully stated in the theorem and lemma statements.

Guidelines:

* The answer NA means that the paper does not include theoretical results.
* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [NA] Justification: The paper does not contain experiments. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general, releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [NA]Justification: Guidelines:

* The answer NA means that paper does not include experiments requiring code.
* Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
* The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [NA] Justification: Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [NA] Justification: Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors). * It should be clear whether the error bar is the standard deviation or the standard error of the mean. * It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.

* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [NA] Justification: Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: this work is theoretical in nature; it is hard to predict its societal impact. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. * The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.

* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [NA] Justification: Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. * If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: Guidelines:* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. * We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. * For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.