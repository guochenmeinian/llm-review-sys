# Classic GNNs are Strong Baselines:

Reassessing GNNs for Node Classification

 Yuankai Luo

Beihang University

The Hong Kong Polytechnic University

luoyk@buaa.edu.cn

Lei Shi

Beihang University

leishi@buaa.edu.cn

&Xiao-Ming Wu

The Hong Kong Polytechnic University

xiao-ming.wu@polyu.edu.hk

Lei Shi

Beihang University

leishi@buaa.edu.cn

&Xiao-Ming Wu

The Hong Kong Polytechnic University

xiao-ming.wu@polyu.edu.hk

Lei Shi

Beihang University

leishi@buaa.edu.cn

&Xiao-Ming Wu

The Hong Kong Polytechnic University

xiao-ming.wu@polyu.edu.hk

Lei Shi

Beihang University

leishi@buaa.edu.cn

&Xiao-Ming Wu

The Hong Kong Polytechnic University

xiao-ming.wu@polyu.edu.hk

Lei Shi

Beihang University

leishi@buaa.edu.cn

&Xiao-Ming Wu

The Hong Kong Polytechnic University

xiao-ming.wu@polyu.edu.hk

Lei Shi

Beihang University

leishi@buaa.edu.cn

&Xiao-Ming Wu

The Hong Kong Polytechnic University

xiao-ming.wu@polyu.edu.hk

Lei Shi

Beihang University

leishi@buaa.edu.cn

&Xiao-Ming Wu

The Hong Kong Polytechnic University

xiao-ming.wu@polyu.edu.hk

Lei Shi

Beihang University

leishi@buaa.edu.cn

&Xiao-Ming Wu

The Hong Kong Polytechnic University

xiao-ming.wu@polyu.edu.hk

Lei Shi

Beihang University

leishi@buaa.edu.cn

&Xiao-Ming Wu

The Hong Kong Polytechnic University

xiao-ming.wu@polyu.edu.hk

Lei Shi

Beihang University

leishi@buaa.edu.cn

&Xiao-Ming Wu

The Hong Kong Polytechnic University

xiao-ming.wu@polyu.edu.hk

Lei Shi

Beihang University

leishi@buaa.edu.cn

&Xiao-Ming Wu

The Hong Kong Polytechnic University

xiao-ming.wu@polyu.edu.hk

Lei Shi

Beihang University

leishi@buaa.edu.cn

&Xiao-Ming Wu

The Hong Kong Polytechnic University

xiao-ming.wu@polyu.edu.hk

Lei Shi

Beihang University

leishi@buaa.edu.cn

&Xiao-Ming Wu

The Hong Kong Polytechnic University

xiao-ming.wu@polyu.edu.hk

Lei Shi

Beihang University

leishi@buaa.edu.cn

&Xiao-Ming Wu

The Hong Kong Polytechnic University

xiao-ming.wu@polyu.edu.hk

Lei Shi

Beihang University

leishi@buaa.edu.cn

&Xiao-Ming Wu

The Hong Kong Polytechnic University

xiao-ming.wu@polyu.edu.hk

Lei Shi

Beihang University

leishi@buaa.edu.cn

&Xiao-Ming Wu

The Hong Kong Polytechnic University

xiao-ming.wu@polyu.edu.hk

Lei Shi

Beihang University

leishi@buaa.edu.cn

&Xiao-Ming Wu

The Hong Kong Polytechnic University

xiao-ming.wu@polyu.edu.hk

Lei Shi

Beihang University

leishi@buaa.edu.cn

&Xiao-Ming Wu

The Hong Kong Polytechnic University

xiao-ming.wu@polyu.edu.hk

Lei Shi

Beihang University

leishi@buaa.edu.cn

&Xiao-Ming Wu

The Hong Kong Polytechnic University

xiao-ming.wu@polyu.edu.hk

Lei Shi

Beihang University

leishi@buaa.edu.cn

&Xiao-Ming Wu

The Hong Kong Polytechnic University

xiao-ming.wu@polyu.edu.hk

Lei Shi

Beihang University

leishi@buaa.edu.cn

&Xiao-Ming Wu

The Hong Kong Polytechnic University

xiao-ming.wu@polyu.edu.hk

Lei Shi

Beihang University

leishi@buaa.edu.cn

&Xiao-Ming Wu

The Hong Kong Polytechnic University

xiao-ming.wu@polyu.edu.hk

Lei Shi

Beihang University

leishi@buaa.edu.cn

&Xiao-Ming Wu

The Hong Kong Polytechnic University

xiao-ming.wu@polyu.edu.hk

Lei Shi

Beihang University

leishi@buaa.edu.cn

&Xiao-Ming Wu

The Hong Kong Polytechnic University

xiao-ming.wu@polyu.edu.hk

Lei Shi

Beihang University

leishi@buaa.edu.cn

&Xiao-Ming Wu

The Hong Kong Polytechnic University

xiao-ming.wu@polyu.edu.hk

Lei Shi

Beihang University

leishi@buaa.edu.cn

&Xiao-Ming Wu

The Hong Kong Polytechnic University

xiao-ming.wu@polyu.edu.hk

Lei Shi

Beihang University

leishi@buaa.edu.cn

&Xiao-Ming Wu

The Hong Kong Polytechnic University

xiao-ming.wu@polyu.edu.hk

Lei Shi

Beihang University

leishi@buaa.edu.cn

&Xiao-Ming Wu

The Hong Kong Polytechnic University

xiao-ming.wu@polyu.edu.hk

Lei Shi

Beihang University

leishi@buaa.edu.cn

&Xiao-Ming Wu

The Hong Kong Polytechnic University

xiao-ming.wu@polyu.edu.hk

Lei Shi

Beihang University

leishi@buaa.edu.cn

&Xiao-Ming Wu

The Hong Kong Polytechnic University

xiao-ming.wu@polyu.edu.hk

Lei Shi

Beihang University

leishi@buaa.edu.cn

&Xiao-Ming Wu

The Hong Kong Polytechnic University

xiao-ming.wu@polyu.edu.hk

Lei Shi

Beihang University

leishi@buaa.edu.cn

&Xiao-Ming Wu

The Hong Kong Polytechnic University

xiao-ming.wu@polyu.edu.hk

Leishi@buaa.edu.cn

Lei Shi

Beihang University

leishi@buaa.edu.cn

&Xiao-Ming Wu

The Hong Kong Polytechnic University

xiao-ming.wu@polyu.edu.hk

Lei Shi

Beihang University

leishi@buaa.edu.cn

&Xiao-Ming Wu

The Hong Kong Polytechnic University

xiao-ming.wu@polyu.edu.hk

Lei Shi

Beihang University

leishi@buaa.edu.cn

&Xiao-Ming Wu

The Hong Kong Polytechnic University

xiao-ming.wu@polyu.edu.hk

Leishi Shi

Beihang University

leishi@buaa.edu.cn

&Xiao-Ming Wu

The Hong Kong Polytechnic University

xiao-ming.wu@polyu.edu.hk

Lei Shi

Beihang University

leishi@buaa.edu.cn

&Xiao-Ming Wu

The Hong Kong Polytechnic University

xiao-ming.wu@polyu.edu.hk

Lei Shi

Beihang University

leishi@buaa.edu.cn

&Xiao-Ming Wu

The Hong Kong Polytechnic University

xiao-ming.wu@polyu.edu.hk

Leishi@buaa.edu.cn

Lei Shi

Beihang University

leishi@buaa.edu.cn

&Xiao-Ming Wu

The Hong Kong Polytechnic University

xiao-ming.wu@polyu.edu.hk

Leishi@buaa.edu.cn

Lei Shi

Beihang University

leishi@buaa.edu.cn

&Xiao-Ming Wu

The Hong Kong Polytechnic University

xiao-ming.wu@polyu.edu.hk

Lei Shi

Beihang University

leishi@buaa.edu.cn

&Xiao-Ming Wu

The Hong Kong Polytechnic University

xiao-ming.wu@polyu.edu.hk

Lei Shi

Beihang University

leishi@buaa.edu.cn

&Xiao-Ming Wu

The Hong Kong Polytechnic University

xiao-ming.wu@polyu.edu.hk

Lei Shi

Beihang University

leishi@buaa.edu.cn

Lei Shi

Beihang University

leishi@buaa.edu.cn

&Xiao-Ming Wu

The Hong Kong Polytechnic University

xiao-ming.wu@polyu.edu.hk

Leishi@buaa.edu.cn

Lei Shi

Beihang University

leishi@buaa.edu.cn

&Xiao-Ming Wu

The Hong Kong Polytechnic University

xiao-ming.wu@polyu.edu.hk

Lei Shi

Beihang University

leishi@buaa.edu.cn

&Xiao-Ming Wu

The Hong Kong Polytechnic University

xiao-ming.wu@polyu.edu.hk

Leishi Shi

Beihang University

leishi@buaa.edu.cn

&Xiao-Ming Wu

The Hong Kong Polytechnic University

xiao-ming.wu@polyu.edu.hk

Lei Shi

Beihang University

leishi@buaa.edu.cn

&Xiao-Ming Wu

The Hong Kong Polytechnic University

xiao-ming.wu@polyu.edu.hk

Lei Shi

Beihang University

leishi@buaa.edu.cn

&Xiao-Ming Wu

The Hong Kong Polytechnic University

xiao-ming.wu@polyu.edu.hk

Leishi Shi

Beihang University

leishi@buaa.edu.cn

&Xiao-Ming Wu

The Hong Kong Polytechnic University

xiao-ming.wu@polyu.edu.hk

Lei Shi

Beihang University

leishi@buaa.edu.cn

&Xiao-Ming Wu

The Hong Kong Polytechnic University

xiao-ming.wu@polyu.edu.hk

Lei Shi

Beihang University

leishi@buaa.edu.cn

&Xiao-Ming Wu

The Hong Kong Polytechnic University

xiao-ming.wu@polyu.edu.hk

Lei Shi

Beihang University

leishi@buaa.edu.cn

&Xiao-Ming Wu

The Hong Kong Polytechnic University

xiao-ming.wu@polyu.edu.hk

Leishi Shi

Beihang University

leishi@buaa.edu.cn

&Xiao-Ming Wu

The Hong Kong Polytechnic University

xiao-ming.wu@polyu.edu.hk

Lei Shi

Beihang University

leishi@buaa.edu.cn

&Xiao-Ming Wu

The Hong Kong Polytechnic University

xiao-ming.wu@polyu.edu.hk

Lei Shi

Beihang University

leishi@buaa.edu.cn

&Xiao-Ming Wu

The Hong Kong Polytechnic University

xiao-ming.wu@polyu.edu.hk

Lei Shi

Beihang University

leishi@buaa.edu.cn

&Xiao-Ming Wu

The Hong Kong Polytechnic University

xiao-ming.wu@polyu.edu.hk

Lei Shi

Beihang University

leishi@buaa.edu.cn

&Xiao-Ming Wu

The Hong Kong Polytechnic University

xiao-ming.wu@polyu.edu.hk

Lei Shi

Beihang University

leishi@buaa.edu.cn

&Xiao-Ming Wu

The Hong Kong Polytechnic University

xiao-ming.wu@polyu.edu.hk

Leishi Shi

Beihang University

leishi@buaa.edu.cn

&Xiao-Ming Wu

The Hong Kong Polytechnic University

xiao-ming.wu@polyu.edu.hk

Lei Shi

Beihang University

leishi@buaa.edu.edu.cn

&Xiao-Ming Wu

The Hong Kong Polytechnic University

xiao-ming.wu@polyu.edu.hk

Leishi Shi

Beihang University

leishi@buaa.edu.cn

&Xiao-Ming Wu

The Hong Kong Polytechnic University

xiao-ming.wu@polyu.edu.hk

Lei Shi

Beihang University

leishi@buaa.edu.cn

Leishi Shi

Beihang University

leishi@buaa.edu.cn

&Xiao-Ming Wu

The Hong Kong Polytechnic University

xiao-ming.wu@polyu.edu.hk

Leishi Shi

Beihang University

leishi@buaa.edu.cn

&Xiao-Ming75, 74, 88, 91, 15, 64, 7, 29, 41, 38] to utilize GTs to tackle node classification tasks, especially on large-scale graphs, addressing the aforementioned limitations of GNNs. While recent advancements in state-of-the-art GTs [12, 76] have shown promising results, it's observed that many of these models, whether explicitly or implicitly, still rely on GNNs for learning local node representations, integrating them alongside the global attention mechanisms for a more comprehensive representation.

This prompts us to reconsider: _Could the potential of message-passing GNNs for node classification have been previously underestimated?_ While prior research has addressed this issue to some extent [24, 14, 73, 47, 58], these studies have limitations in terms of scope and comprehensiveness, including a restricted number and diversity of datasets, as well as an incomplete examination of hyperparameters. In this study, we comprehensively reassess the performance of GNNs for node classification, utilizing three classic GNN models--GCN [28], GAT [68], and GraphSAGE [20]--across 18 real-world benchmark datasets that include homophilous, heterophilous, and large-scale graphs. We examine the influence of key hyperparameters on GNN training, including normalization [2, 26], dropout [67], residual connections [21], and network depth. We summarize the key findings in our empirical study as follows:

* With proper hyperparameter tuning, classic GNNs can achieve highly competitive performance in node classification across homophilous and heterophilous graphs with up to millions of nodes. Notably, classic GNNs outperform state-of-the-art GTs, achieving the top rank on 17 out of 18 datasets. This indicates that the previously claimed superiority of GTs over GNNs may have been overstated, possibly due to suboptimal hyperparameter configurations in GNN evaluations.
* Our ablation studies have yielded valuable insights into GNN hyperparameters for node classification. We demonstrate that (1) normalization is essential for large-scale graphs; (2) dropout consistently proves beneficial; (3) residual connections can significantly enhance performance, especially on heterophilous graphs; and (4) GNNs on heterophilous graphs tend to perform better with deeper layers.

## 2 Classic GNNs for Node Classification

Define a graph as \(\mathcal{G}=(\mathcal{V},\mathcal{E},\bm{X},\bm{Y})\), where \(\mathcal{V}\) denotes the set of nodes, \(\mathcal{E}\subseteq\mathcal{V}\times\mathcal{V}\) represents the set of edges, \(\bm{X}\in\mathbb{R}^{|\mathcal{V}|\times d}\) is the node feature matrix, with \(|\mathcal{V}|\) representing the number of nodes and \(d\) the dimension of the node features, and \(\bm{Y}\in\mathbb{R}^{|\mathcal{V}|\times C}\) is the one-hot encoded label matrix, with \(C\) being the number of classes. Let \(\bm{A}\in\mathbb{R}^{|\mathcal{V}|\times|\mathcal{V}|}\) denote the adjacency matrix of \(\mathcal{G}\).

**Message Passing Graph Neural Networks (GNNs)**[19] compute node representations \(\bm{h}_{v}^{l}\) at each layer \(l\) as:

\[\bm{h}_{v}^{l}=\text{UPDATE}^{l}\left(\bm{h}_{v}^{l-1},\text{AGG}^{l}\left( \left\{\bm{h}_{u}^{l-1}\mid u\in\mathcal{N}\left(v\right)\right\}\right) \right),\] (1)

where \(\mathcal{N}(v)\) represents the neighboring nodes adjacent to \(v\), \(\text{AGG}^{l}\) serves as the message aggregation function, and \(\text{UPDATE}^{l}\) is the update function. Initially, each node \(v\) begins with a feature vector \(\bm{h}_{v}^{0}=\bm{x}_{v}\in\mathbb{R}^{d}\). The function \(\text{AGG}^{l}\) aggregates information from the neighbors of \(v\) to update its representation. The output of the last layer \(L\), i.e., \(\text{GNN}(v,\bm{A},\bm{X})=\bm{h}_{v}^{L}\), is the representation of \(v\) produced by the GNN. In this work, we focus on three classic GNNs: GCN [28], GraphSAGE [20], and GAT [68], which differ in their approach to learning the node representation \(\bm{h}_{v}^{l}\).

**Graph Convolutional Networks (GCN)**[28], the standard GCN model, is formulated as:

\[\bm{h}_{v}^{l}=\sigma(\sum_{u\in\mathcal{N}(v)\cup\{v\}}\frac{1}{\sqrt{\hat{d }_{u}\hat{d}_{v}}}\bm{h}_{u}^{l-1}\bm{W}^{l}),\] (2)

where \(\hat{d}_{v}=1+\sum_{u\in\mathcal{N}(v)}1\), \(\sum_{u\in\mathcal{N}(v)}1\) denotes the degree of node \(v\), \(\bm{W}^{l}\) is the trainable weight matrix in layer \(l\), and \(\sigma\) is the activation function, e.g., ReLU(-) = \(\max(0,\cdot)\).

**GraphSAGE**[20] learns node representations through a different approach:

\[\bm{h}_{v}^{l}=\sigma(\bm{h}_{v}^{l-1}\bm{W}_{1}^{l}+(\text{mean}_{u\in \mathcal{N}(v)}\bm{h}_{u}^{l-1})\bm{W}_{2}^{l}),\] (3)

where \(\bm{W}_{1}^{l}\) and \(\bm{W}_{2}^{l}\) are trainable weight matrices, and \(\text{mean}_{u\in\mathcal{N}(v)}\bm{h}_{u}^{l-1}\) computes the average embedding of the neighboring nodes of \(v\).

**Graph Attention Networks (GAT)**[68] employ masked self-attention to assign weights to different neighboring nodes. For an edge \((v,u)\in\mathcal{E}\), the propagation rule of GAT is defined as:

\[\alpha_{vu}^{l}=\frac{\exp\left(\text{LeakyReLU}\left(\mathbf{a}_{l}^{\top} \left[\bm{W}^{l}\bm{h}_{v}^{l-1}\right.\|\bm{W}^{l}\bm{h}_{u}^{l-1}\right] \right)\right)}{\sum_{r\in\mathcal{N}(v)}\exp\left(\text{LeakyReLU}\left( \mathbf{a}_{l}^{\top}\left[\bm{W}^{l}\bm{h}_{v}^{l-1}\right.\|\bm{W}^{l}\bm{h}_ {r}^{l-1}\right]\right)\right)},\]

\[\bm{h}_{v}^{l}=\sigma\left(\sum_{u\in\mathcal{N}(v)}\alpha_{vu}^{l}\bm{h}_{u}^ {l-1}\bm{W}^{l}\right),\] (4)

where \(\mathbf{a}_{l}\) is a trainable weight vector, \(\bm{W}^{l}\) is a trainable weight matrix, and \(\|\) represents the concatenation operation.

**Node Classification** aims to predict the labels of the unlabeled nodes. Typically, for any node \(v\), the node representation generated by the last GNN layer is passed through a prediction head \(g(\cdot)\), to obtain the predicted label \(\hat{\bm{y}}_{v}=g(\text{GNN}(v,\bm{A},\bm{X}))\). The training objective is to minimize the total loss \(L(\bm{\theta})=\sum_{v\in\mathcal{V}_{\text{train}}}\ell(\hat{\bm{y}}_{v},\bm{ y}_{v})\) w.r.t. all nodes in the training set \(\mathcal{V}_{\text{train}}\), where \(\bm{y}_{v}\) indicates the ground-truth label of \(v\) and \(\bm{\theta}\) indicates the trainable GNN parameters.

**Homophilous and Heterophilous Graphs.** Node classification can be performed on both homophilous and heterophilous graphs. Homophilous graphs are characterized by edges that tend to connect nodes of the same class, while in heterophilous graphs, connected nodes may belong to different classes [58]. GNN models implicitly assume homophily in graphs [48], and it is commonly believed that due to this homophily assumption, GNNs cannot generalize well to heterophilous graphs [90; 4]. However, recent works [46; 40; 58; 42] have empirically shown that standard GCNs also work well on heterophilous graphs. In this study, we provide a comprehensive evaluation of classic GNNs for node classification on both homophilous and heterophilous graphs.

## 3 Key Hyperparameters for Training GNNs

In this section, we present an overview of the key hyperparameters for training GNNs, including normalization, dropout, residual connections, and network depth. These hyperparameters are widely utilized across different types of neural networks to improve model performance.

**Normalization.** Specifically, Layer Normalization (LN) [2] or Batch Normalization (BN) [26] can be used in every layer before the activation function \(\sigma(\cdot)\). Taking GCN as an example:

\[\bm{h}_{v}^{l}=\sigma(\text{Norm}(\sum_{u\in\mathcal{N}(v)\cup\{v\}}\frac{1} {\sqrt{\hat{d}_{u}\hat{d}_{v}}}\bm{h}_{u}^{l-1}\bm{W}^{l})).\] (5)

The normalization techniques are essential for stabilizing the training process by reducing the _covariate shift_, which occurs when the distribution of each layer's node embeddings changes during training. Normalizing the node embeddings helps to maintain a more consistent distribution, allowing the use of higher learning rates and leading to faster convergence [5].

**Dropout**[67], a technique widely used in convolutional neural networks (CNNs) to address overfitting by reducing co-adaptation among hidden neurons [83; 22], has also been found to be effective in addressing similar issues in GNNs [68; 65], where the co-adaptation effects propagate and accumulate through message passing among different nodes. Typically, dropout is applied to the feature embeddings after the activation function:

\[\bm{h}_{v}^{l}=\text{Dropout}(\sigma(\text{Norm}(\sum_{u\in\mathcal{N}(v) \cup\{v\}}\frac{1}{\sqrt{\hat{d}_{u}\hat{d}_{v}}}\bm{h}_{u}^{l-1}\bm{W}^{l}))).\] (6)

**Residual Connections**[21] significantly enhance CNN performance by connecting layer inputs directly to outputs, thereby alleviating the vanishing gradient issue. They were first adopted by the seminal GCN paper [28] and subsequently incorporated into DeepGCNs [33] to boost performance. Formally, linear residual connections can be integrated into GNNs as follows:

\[\bm{h}_{v}^{l}=\text{Dropout}(\sigma(\text{Norm}(\bm{h}_{v}^{l-1}\bm{W_{r}}^{ l}+\sum_{u\in\mathcal{N}(v)\cup\{v\}}\frac{1}{\sqrt{\hat{d}_{u}\hat{d}_{v}}} \bm{h}_{u}^{l-1}\bm{W}^{l}))),\] (7)where \(\bm{W}_{r}^{l}\) is a trainable weight matrix. This configuration mitigates gradient instabilities and enhances GNN expressiveness [80], addressing the over-smoothing [35] and oversquashing [1] issues since the linear component (\(\bm{h}_{u}^{l-1}\bm{W}_{r}^{l}\)) helps to preserve distinguishable node representations [73].

**Network Depth.** Deeper network architectures, such as deep CNNs [21; 25], are capable of extracting more complex, high-level features from data, potentially leading to better performance on various prediction tasks. However, GNNs face unique challenges with depth, such as over-smoothing [35], where node representations become indistinguishable with increased network depth. Consequently, in practice, most GNNs adopt a shallow architecture, typically consisting of 2 to 5 layers. While previous research, such as DeepGCN [33] and DeeperGCN [34], advocates the use of deep GNNs with up to 56 and 112 layers, our findings indicate that comparable performance can be achieved with significantly shallower GNN architectures, typically ranging from 2 to 10 layers.

## 4 Experimental Setup for Node Classification

**Datasets.** Table 1 presents a summary of the statistics and characteristics of the datasets.

* **Homophilous Graphs. Cora, CiteSeer**, and **PubMed** are three widely used citation networks [62]. We follow the semi-supervised setting of [28] for data splits and metrics. Additionally, **Computer** and **Photo**[63] are co-purchase networks where nodes represent goods and edges indicate that the connected goods are frequently bought together. **CS** and **Physics**[63] are co-authorship networks where nodes denote authors and edges represent that the authors have co-authored at least one paper. We adhere to the widely accepted practice of training/validation/test splits of 60%/20%/20% and metric of accuracy [7; 64; 12]. Furthermore, we utilize the **WikiCS** dataset and use the official splits and metrics provided in [50].
* **Heterophilous Graphs. Squirrel** and **Chameleon**[61] are two well-known page-page networks that focus on specific topics in Wikipedia. According to the heterophilous graphs benchmarking paper [58], the original split of these datasets introduces overlapping nodes between training and testing, leading to the proposal of a new data split that filters out the overlapping nodes. We use its provided split and its metrics for evaluation. Additionally, we utilize four other heterophilous datasets proposed by the same source [58]: **Roman-Empire**, where nodes correspond to words in the Roman Empire Wikipedia article and edges connect sequential or syntactically linked words; **Amazon-Ratings**, where nodes represent products and edges connect frequently co-purchased items; **Minesweeper**, a synthetic dataset where nodes are cells in a \(100\times 100\) grid and edges connect neighboring cells; and **Questions**, where nodes represent users from the Yandex Q question-answering website and edges connect users who interacted through answers. All splits and evaluation metrics are consistent with those proposed in the source.

\begin{table}
\begin{tabular}{l l c c c c} \hline \hline Dataset & Type & \# nodes & \# edges & \# Features & Classes & Metric \\ \hline Cora & Homophily & 2,708 & 5,278 & 1,433 & 7 & Accuracy \\ CiteSeer & Homophily & 3,327 & 4,522 & 3,703 & 6 & Accuracy \\ PubMed & Homophily & 19,717 & 44,324 & 500 & 3 & Accuracy \\ Computer & Homophily & 13,752 & 245,861 & 767 & 10 & Accuracy \\ Photo & Homophily & 7,650 & 119,881 & 745 & 8 & Accuracy \\ CS & Homophily & 18,333 & 81,894 & 6,805 & 15 & Accuracy \\ Physics & Homophily & 34,493 & 247,962 & 8,415 & 5 & Accuracy \\ WikiCS & Homophily & 11,701 & 216,123 & 300 & 10 & Accuracy \\ \hline Squirrel & Heterophily & 2,223 & 46,998 & 2,089 & 5 & Accuracy \\ Chameleon & Heterophily & 890 & 8,854 & 2,325 & 5 & Accuracy \\ Roman-Empire & Heterophily & 22,662 & 32,927 & 300 & 18 & Accuracy \\ Amazon-Ratings & Heterophily & 24,492 & 93,050 & 300 & 5 & Accuracy \\ Minesweeper & Heterophily & 10,000 & 39,402 & 7 & 2 & ROC-AUC \\ Questions & Heterophily & 48,921 & 153,540 & 301 & 2 & ROC-AUC \\ \hline ogbn-proteins & Homophily (Large graphs) & 132,534 & 39,561,252 & 8 & 2 & ROC-AUC \\ ogbn-arxiv & Homophily (Large graphs) & 169,343 & 1,166,243 & 128 & 40 & Accuracy \\ ogbn-products & Homophily (Large graphs) & 2,449,029 & 61,859,140 & 100 & 47 & Accuracy \\ polec & Heterophily (Large graphs) & 1,632,803 & 30,622,564 & 65 & 2 & Accuracy \\ \hline \hline \end{tabular}
\end{table}
Table 1: Overview of the datasets used for node classification.

* **Large-scale Graphs.** We consider a collection of large graphs released recently by the Open Graph Benchmark (OGB) [24]: **ogbn-arxiv**, **ogbn-proteins**, and **ogbn-products**, with node numbers ranging from 0.16M to 2.4M. We maintain all the OGB standard evaluation settings. Additionally, we analyze performance on the social network **pokec**[32], which has 1.6M nodes, following the evaluation settings of [12].

**Baselines.** Our main focus lies on classic GNNs: **GCN**[28], **GraphSAGE**[20], **GAT**[68], the state-of-the-art scalable GTs: **SGFormer**[76], **Polynormer**[12], **GOAT**[29], **NodeFormer**[75], **NAGphormer**[7], and powerful GTs: **GraphGPS**[59] and **Exphormer**[64]. Furthermore, various other GTs like [17; 15; 38; 86; 31; 3; 6; 82; 13] exist in related surveys [23; 53], empirically shown to be inferior to the GTs we compared against for node classification tasks. For heterophilous graphs, We also consider five models designed for node classification under heterophily following [58]: **H2GCN**[90], **CPGNN**[89], **GPRGNN**[9], **FSGNN**[49], **GloGNN**[36]. Note that we adopt the empirically optimal Polynormer variant (Polynormer-r), which demonstrates superior performance over advanced GNNs such as LINKX [37] and OrderedGNN [66]. We report the performance results of baselines primarily from [12; 76; 58], with the remaining obtained from their respective original papers or official leaderboards whenever possible, as those results are obtained by well-tuned models.

**Hyperparameter Configurations.** We conduct hyperparameter tuning on classic GNNs, consistent with the hyperparameter search space of Polynormer [12]. Specifically, we utilize the Adam optimizer [27] with a learning rate from \(\{0.001,0.005,0.01\}\) and an epoch limit of 2500. And we tune the hidden dimension from \(\{64,256,512\}\). As discussed in Section 3, we focus on whether to use normalization (BN or LN), residual connections, and dropout rates from \(\{0.2,0.3,0.5,0.7\}\), the number of layers from \(\{1,2,3,4,5,6,7,8,9,10\}\). Additionally, we retrain all baseline GTs using the same hyperparameter search space and training environments as the classic GNNs. For hyperparameters specific to each GT, which are not present in the classic GNNs, we tune them according to the search space specified in the original GT paper. We report mean scores and standard deviations after 5 independent runs with different initializations. **Model\({}^{*}\)** denotes our implementation. Detailed experimental setup and hyperparameters are provided in Appendix A.

## 5 Empirical Findings

### Performance of Classic GNNs in Node Classification

In this subsection, we provide a detailed analysis of the performance of the three classic GNNs compared to state-of-the-art GTs in node classification tasks. Our experimental results across homophilous (Table 2), heterophilous (Table 3), and large-scale graphs (Table 4) reveal that classic GNNs often outperform or match the performance of advanced GTs across 18 datasets. Notably,

\begin{table}
\begin{tabular}{l|l l l l l l l} \hline  & Cora & CiteSeer & PubMed & Computer & Photo & CS & Physics & WikiCS \\ \hline \hline
**nodes** & 2,708 & 3,327 & 19.717 & 13.572 & 7.650 & 18.333 & 34.493 & 17.01 \\ \# edges & 2,778 & 4,732 & 44.324 & 24.861 & 119.081 & 81.894 & 247.962 & 216.123 \\ Metric & Accuracy\({}^{\dagger}\) & Accuracy\({}^{\dagger}\) & Accuracy\({}^{\dagger}\) & Accuracy\({}^{\dagger}\) & Accuracy\({}^{\dagger}\) & Accuracy\({}^{\dagger}\) & Accuracy\({}^{\dagger}\) & Accuracy\({}^{\dagger}\) \\ GraphGTs & 82.84 & 1.44 & 27.73 & 11.79 & 94.919 & 95.061 & 93.931 & 971.229 & 78.664 \\
**GraphGPS** & 83.887 & 27.829 & 99.984 & 99.985 & 97.199 & 94.989 & 94.989 & 94.989 & 96.713 & 78.665 \\
**NAGphormer** & 82.12 & 11.4 & 17.49 & 79.73 & 91.22 & 99.542 & 91.91 & 97.550 & 97.344 & 77.16 \\
**NAGphormer** & 80.92 & 79.599 & 80.801 & 96.900 & 96.148 & 96.513 & 98.553 & 97.344 & 97.292 \\ Explorer & 82.77 & 71.78 & 71.63 & 79.46 & 91.47 & 95.355 & 97.493 & 96.857 & 78.54 \\
**Exphormer** & 83.29 & 78.158 & 81.759 & 86.79 & 91.800 & 95.659 & 95.922 & 97.066 & 97.388 \\ GOAT & 83.18 & 71.99 & 79.48 & 99.000 & 92.996 & 92.49 & 91.241 & 96.444 & 77.000 \\
**GOAT** & 83.268 & 71.242 & 80.806 & 80.660 & 92.299 & 93.333 & 93.881 & 96.473 & 79.600 \\ NodeFormer & 82.20 & 72.50 & 79.900 & 96.900 & 93.681 & 94.365 & 95.642 & 96.450 & 74.734 \\
**NodeFormer** & 82.758 & 72.324 & 79.59 & 87.299 & 93.934 & 95.699 & 96.848 & 75.133 & 98.899 \\
**SGFormer** & 84.50 & 72.60 & 80.30 & 80.49 & 91.999 & 95.10 & 94.04 & 94.836 & 96.600 & 73.464 \\
**SGFormer** & 84.82 & 72.52 & 80.600 & 92.420 & 95.588 & 95.714 & 96.565 & 80.058 & 80.058 \\
**Polynormer** & 83.25 & 72.31 & 79.24 & 93.681 & 94.648 & 95.534 & 97.97 & 97.48 & 80.100 \\
**Polynormer** & 83.43 & 72.19 & 79.35 & 97.334 & 96.575 & 95.428 & 97.181 & 96.802 & 86.800 \\ GCN & 81.60 & 71.60 & 71.60 & 78.00 & 80.49 & 80.659 & 97.20 & 92.914 & 96.184 & 77.473 \\
**GCN** & **85.100 & 83.500 & **73.530 & **73.535 & **73.500 & 93.000 & 96.550 & 94.600 & 73.462 & 97.185 & 80.050 \\ \hline GraphSAGE & 82.68 & 6.50 & 71.93 & 79.41 & 91.20 & 94.59 & 94.591 & 93.91 & 96.499 & 74.77 & 94.05 \\
**GraphSAGE** & 83.88 & 6.50 & **72.26** & 65.03 & **73.37** & 97.25 & 69.31 & 93.255 & **96.78** & **92.191** & **96.38** & **92.47** & **97.195** \\
**GraphSAGE** & 83.00 & 83.00 & 72.10 & **83.10** & **87.00** & 90.180 & 93.87 & 93.61 & 96.144 & 96.17 & **96.191** & **96.191** \\
**GAT** & **81.460** & **80.00** & **74.06** & **72.22** & **80.02** & **81.02** & **80.02** & **81.02** & **81.02** & **81.02** & **81.01** & **80.17** & **80.03** & **81.16** \\ \hline \end{tabular}
\end{table}
Table 2: Node classification results over homophilous graphs (%). \({}^{\star}\) indicates our implementation, while other results are taken from [12; 76]. The top 1\({}^{\star}\), \(2^{\rm nd}\) and \(3^{\rm rd}\) results are highlighted.

among the 18 datasets evaluated, classic GNNs achieve the top rank on 17 of them, showcasing their robust competitiveness. We highlight our main observations below.

**Observations on Homophilous Graphs (Table 2).** Classic GNNs, with only slight adjustments to hyperparameters, are highly competitive in node classification tasks on homophilous graphs, often outperforming state-of-the-art graph transformers in many cases.

While previously reported results show that most advanced GTs outperform classic GNN on homophilous graphs [12, 76], our implementation of classic GNNs can place within the top two for four datasets, with GCN\({}^{*}\) and GAT\({}^{*}\) demonstrating near-consistent top performances. Specifically, on CS and WikiCS, classic GNNs experience about a 3% accuracy increase, achieving top-three performances. On WikiCS, the accuracy of GAT\({}^{*}\) increases by 4.16%, moving it from seventh to first place, surpassing the leading GT, Polynormer. Similarly, on Photo and CS, GraphSAGE\({}^{*}\) outperforms Polynormer and SGFormer, establishing itself as the top model. On Cora, CiteSeer, PubMed, and Physics, tuning yields significant performance improvements for GCN\({}^{*}\), with accuracy increases ranging from 1.54% to 3.50%, positioning GCN\({}^{*}\) as the highest-performing model despite its initial lower accuracy compared to advanced GTs.

**Observations on Heterophilous Graphs (Table 3).** Our implementation has significantly enhanced the previously reported best results of classic GNNs on heterophilous graphs, surpassing specialized GNN models tailored for such graphs and even outperforming the leading graph transformer architectures. This advancement not only supports but also strengthens the findings in [58] that conventional GNNs are strong contenders for heterophilous graphs, challenging the prevailing assumption that they are primarily suited for homophilous graph structures.

The three classic GNNs secure top positions on five out of six heterophilous graphs. Specifically, on well-known page-page networks like Chameleon and Squirrel, our implementation enhances the accuracy of GCN\({}^{*}\) by 4.98% and 6.34% respectively, elevating it to the first place among all models. Similarly, on larger heterophilous graphs such as Minesweeper and Questions, GCN\({}^{*}\) also exhibits the highest performance, highlighting the superiority of its local message-passing mechanism over GTs' global attention. On Roman-Empire, a 17.58% increase is observed in the performance of GCN\({}^{*}\). Interestingly, we find that improvements primarily stem from residual connections, which are further analyzed in our ablation study (see Section 5.2).

\begin{table}
\begin{tabular}{|p{56.9pt}|p{56.9pt} p{56.9pt} p{56.9pt} p{56.9pt} p{56.9pt} p{56.9pt}|} \hline \hline  & Squirrel & Chameleon & Amazon-Ratings & Roman-Empire & Minesweeper & Questions \\ \hline \# nodes & 2223 & 890 & 24,492 & 22,662 & 10,000 & 48,921 \\ \# edges & 46,998 & 8,854 & 93,050 & 32,927 & 39,402 & 153,540 \\ Metric & Accuracy\({}^{\dagger}\) & Accuracy\({}^{\dagger}\) & Accuracy\({}^{\dagger}\) & Accuracy\({}^{\dagger}\) & Accuracy\({}^{\dagger}\) & ROC-AUC\({}^{\dagger}\) & ROC-AUC\({}^{\dagger}\)\({}^{\dagger}\) \\ \hline H2GCN & 35.10 \(\pm\)1.15 & 26.75 \(\pm\)3.64 & 36.47 \(\pm\)0.21 & 60.11 \(\pm\)0.52 & 89.71 \(\pm\)0.31 & 63.59 \(\pm\)1.46 \\ CPGNN & 30.04 \(\pm\)2.09 & 33.00 \(\pm\)3.15 & 39.79 \(\pm\)0.77 & 63.96 \(\pm\)0.62 & 52.03 \(\pm\)5.46 & 65.96 \(\pm\)1.05 \\ GPRGNN & 38.95 \(\pm\)1.39 & 39.93 \(\pm\)3.30 & 44.88 \(\pm\)0.54 & 64.85 \(\pm\)0.27 & 86.24 \(\pm\)0.41 & 55.48 \(\pm\)0.91 \\ FSGNN & 35.92 \(\pm\)1.27 & 40.61 \(\pm\)1.97 & 52.74 \(\pm\)0.85 & 79.92 \(\pm\)0.86 & 90.08 \(\pm\)0.79 & 78.86 \(\pm\)0.92 \\ GloGNN & 35.11 \(\pm\)1.24 & 25.90 \(\pm\)3.58 & 36.89 \(\pm\)0.14 & 59.63 \(\pm\)0.09 & 51.08 \(\pm\)1.25 & 65.74 \(\pm\)1.19 \\ \hline GraphGPS & 39.67 \(\pm\)2.84 & 40.79 \(\pm\)4.00 & 53.10 \(\pm\)0.02 & 82.00 \(\pm\)0.64 & 90.63 \(\pm\)0.05 & 71.73 \(\pm\)1.47 \\
**GraphGPS\({}^{*}\)** & 39.81 \(\pm\)2.34 & **41.55** \(\pm\)3.291 & 53.27 \(\pm\)0.06 & 82.72 \(\pm\)0.06 & 90.75 \(\pm\)0.08 & 72.56 \(\pm\)1.33 \\ NodeFormer & 38.52 \(\pm\)1.57 & 34.33 \(\pm\)1.41 & 43.86 \(\pm\)0.65 & 64.49 \(\pm\)0.77 & 86.71 \(\pm\)0.78 & 74.27 \(\pm\)1.46 \\
**NodeFormer\({}^{*}\)** & 38.99 \(\pm\)2.67 & 36.38 \(\pm\)2.85 & 43.79 \(\pm\)0.97 & 74.83 \(\pm\)0.82 & 87.71 \(\pm\)0.86 & 75.02 \(\pm\)1.41 \\ SGFormer & 41.80 \(\pm\)2.17 & 44.93 \(\pm\)1.91 & 48.01 \(\pm\)0.04 & 79.10 \(\pm\)0.32 & 80.89 \(\pm\)0.98 & 72.15 \(\pm\)1.31 \\
**SGFormer\({}^{*}\)** & 42.65 \(\pm\)2.44 & 45.28 \(\pm\)3.72 & 54.14 \(\pm\)0.08 & 80.01 \(\pm\)0.04 & 91.42 \(\pm\)0.04 & 73.81 \(\pm\)0.99 \\ Polynormer & 40.87 \(\pm\)1.48 & 41.28 \(\pm\)3.54 & 58.41 \(\pm\)0.08 & 92.55 \(\pm\)0.37 & 97.46 \(\pm\)0.38 & 78.92 \(\pm\)0.09 \\
**Polynormer\({}^{*}\)** & 41.97 \(\pm\)2.84 & 41.97 \(\pm\)3.38 & 54.96 \(\pm\)0.02 & 92.66 \(\pm\)0.09 & 97.49 \(\pm\)0.04 & 78.94 \(\pm\)0.87 \\ \hline GCN & 38.67 \(\pm\)1.14 & 41.31 \(\pm\)3.05 & 48.70 \(\pm\)0.03 & 73.69 \(\pm\)0.74 & 89.75 \(\pm\)0.52 & 76.09 \(\pm\)1.27 \\
**GCN\({}^{*}\)** & **45.01** \(\pm\)1.58 & **63.34** \(\pm\)**6.29 & 43.09 \(\pm\)0.49 & 58.50 \(\pm\)0.09 & **51.01** & 91.27 \(\pm\)0.03 & **17.58** \(\pm\)0.03 & **17.11** & **90.20** \(\pm\)0.09 & **2.93** \\ \hline GraphSAGE & 36.09 \(\pm\)1.89 & 37.77 \(\pm\)1.44 & 53.63 \(\pm\)0.39 & 58.74 \(\pm\)0.67 & 93.51 \(\pm\)0.57 & 76.44 \(\pm\)0.02 & **76.44** \(\pm\)0.02 \\
**GraphSAGE\({}^{*}\)** & 40.78 \(\pm\)**4.69** & 44.81 \(\pm\)**4.74 & 55.40 \(\pm\)**1.77** & 51.06 \(\pm\)**0.02 & **52.32** & 97.77 \(\pm\)**0.03 & **42.66** & **77.21** \(\pm\)**0.07** \\ \hline GAT & 35.62 \(\pm\)2.06 & 39.21 \(\pm\)0.08 & 52.70 \(\pm\)0.03 & 88.75 \(\pm\)0.41 & 9.31 \(\pm\)0.05 & 76.79 \(\pm\)0.71 \\
**GAT\({}^{*}\)** & 41.73 \(\pm\)2.07 & **61.11** \(\pm\)0.43 & 44.13 \(\pm\)**1.49 & **49.21** \(\pm\)**55.54 \(\pm\)**0.24** & 90.63 \(\pm\)**0.11 \(\pm\)**1.88** \\ \hline \hline \end{tabular}
\end{table}
Table 3: Node classification results on heterophilous graphs (%). \({}^{*}\) indicates our implementation, while other results are taken from [12, 76, 58]. The top 1\({}^

**Observations on Large-scale Graphs (Table 4).** Our implementation has significantly enhanced the previously reported results of classic GNNs, with some cases showing double-digit increases in accuracy. It has achieved the best results across these large graph datasets, either homophilous or heterophilous, and has outperformed state-of-the-art graph transformers. This indicates that message passing remains highly effective for learning node representations on large-scale graphs.

Our implementation of classic GNNs demonstrate superior performance consistently, achieving top rankings across all four large-scale datasets included in our study. Notably, GCN\({}^{*}\) emerges as the leading model on ogbn-arxiv and pokec, surpassing all evaluated advanced GTs. Furthermore, on pokec, all three classic GNNs achieve over 10% performance increases by our implementation. For ogbn-proteins, an absolute improvement of 12.99% is observed in the performance of GAT\({}^{*}\), significantly surpassing SGFormer by 5.09%. Similarly, on ogbn-products, GraphSAGE\({}^{*}\) demonstrates a significant performance increase, securing the best performance among all evaluated models. In summary, a basic GNN can achieve the best known results on large-scale graphs, suggesting that current GTs have not yet addressed GNN issues such as over-smoothing and long-range dependencies.

### Influence of Hyperparameters on the Performance of GNNs

To examine the unique contributions of different hyperparameters in explaining the enhanced performance of classic GNNs, we conduct a series of ablation analysis by selectively removing elements such as normalization, dropout, residual connections, and network depth from GCN\({}^{*}\), GraphSAGE\({}^{*}\), and GAT\({}^{*}\). The effect of these ablations is assessed across homophilous (see Table 5), heterophilous (see Table 6), and large-scale graphs (see Table 7). Our findings, which we detail below, indicate that the ablation of single components affects model accuracy in distinct ways.

**Observation 1: Normalization (either BN or LN) is important for node classification on large-scale graphs but less significant on smaller-scale graphs.**

\begin{table}
\begin{tabular}{l|l l l l} \hline \hline  & ogbn-proteins & ogbn-arxiv & ogbn-products & pokec \\ \hline \# nodes & 132,534 & 169,343 & 2,449,029 & 1,632,803 \\ \# edges & 39,561,252 & 1,166,243 & 61,859,140 & 30,622,564 \\ Metric & ROC-AUC\(\uparrow\) & Accuracy\(\uparrow\) & Accuracy\(\uparrow\) & Accuracy\(\uparrow\) \\ \hline GraphGPS & 76.83 \(\pm\) 0.26 & 70.97 \(\pm\) 0.41 & OOM & OOM \\
**GraphGPS\({}^{*}\)** & 77.15 \(\pm\) 0.66 & 71.23 \(\pm\) 0.99 & OOM & OOM \\ NAGphormer & 73.61 \(\pm\) 0.33 & 70.13 \(\pm\) 0.55 & 73.55 \(\pm\) 0.21 & 76.59 \(\pm\) 0.25 \\
**NAGphormer\({}^{*}\)** & 72.17 \(\pm\) 0.45 & 70.88 \(\pm\) 0.24 & 74.63 \(\pm\) 0.29 & 75.92 \(\pm\) 0.68 \\ Exphormer & 74.58 \(\pm\) 0.26 & 72.44 \(\pm\) 0.28 & OOM & OOM \\
**Exphormer\({}^{*}\)** & 77.62 \(\pm\) 0.33 & 72.32 \(\pm\) 0.36 & OOM & OOM \\ GOAT & 74.18 \(\pm\) 0.37 & 72.41 \(\pm\) 0.40 & 82.00 \(\pm\) 0.43 & 66.37 \(\pm\) 0.94 \\
**GOAT\({}^{*}\)** & 79.31 \(\pm\) 0.42 & 72.76 \(\pm\) 0.29 & 82.27 \(\pm\) 0.96 & 72.64 \(\pm\) 0.67 \\ NodeFormer & 77.45 \(\pm\) 1.15 & 59.90 \(\pm\) 0.42 & 72.93 \(\pm\) 0.13 & 71.00 \(\pm\) 1.50 \\
**NodeFormer\({}^{*}\)** & 77.86 \(\pm\) 0.84 & 67.78 \(\pm\) 0.28 & 73.96 \(\pm\) 0.30 & 71.00 \(\pm\) 1.30 \\ SGFormer & 79.53 \(\pm\) 0.38 & 72.63 \(\pm\) 0.13 & 74.16 \(\pm\) 0.31 & 73.76 \(\pm\) 0.24 \\
**SGFormer\({}^{*}\)** & 79.92 \(\pm\) 0.48 & 72.76 \(\pm\) 0.33 & 81.54 \(\pm\) 0.43 & 82.44 \(\pm\) 0.76 \\ Polynormer & 78.97 \(\pm\) 0.47 & 73.46 \(\pm\) 0.16 & 83.82 \(\pm\) 0.11 & 86.10 \(\pm\) 0.05 \\
**Polynormer\({}^{*}\)** & 79.53 \(\pm\) 0.67 & 73.40 \(\pm\) 0.22 & 83.82 \(\pm\) 0.11 & 86.06 \(\pm\) 0.28 \\ \hline GCN & 72.51 \(\pm\) 0.35 & 71.74 \(\pm\) 0.29 & 75.64 \(\pm\) 0.21 & 75.45 \(\pm\) 0.17 \\
**GCN\({}^{*}\)** & 77.29 \(\pm\) 0.46 \(\pm\) **4.78\(\uparrow\)** & **73.53 \(\pm\) 0.12 \(\pm\) 1.79\(\uparrow\)** & 82.33 \(\pm\) 0.19 \(\pm\) 0.69\(\uparrow\) & **86.33 \(\pm\) 0.17 \(\pm\) 0.88\(\uparrow\)** \\ \hline GraphSAGE\({}^{*}\) & 77.68 \(\pm\) 0.30 & 71.49 \(\pm\) 0.27 & 78.29 \(\pm\) 0.16 & 75.63 \(\pm\) 0.38 \\
**GraphSAGE\({}^{*}\)** & 82.21 \(\pm\) 0.33 \(\pm\) **4.53\(\uparrow\)** & 73.00 \(\pm\) 0.28 \(\pm\) **1.51\(\uparrow\)** & **83.89 \(\pm\) 0.36 \(\pm\) 5.60\(\uparrow\)** & 85.97 \(\pm\) 0.22 \(\pm\) 10.34\(\uparrow\) \\ \hline GAT & 72.02 \(\pm\) 0.44 & 71.95 \(\pm\) 0.36 & 79.45 \(\pm\) 0.59 & 72.23 \(\pm\) 0.18 \\
**GAT\({}^{*}\)** & **85.01 \(\pm\) 0.46 \(\pm\) 12.99\(\uparrow\)** & 73.30 \(\pm\) 0.18 \(\pm\) **1.35\(\uparrow\)** & 80.99 \(\pm\) 0.16 \(\pm\) **1.54\(\uparrow\)** & **86.19 \(\pm\) 0.23 \(\pm\) 13.96\(\uparrow\)** \\ \hline \hline \end{tabular}
\end{table}
Table 4: Node classification results on large-scale graphs (%). \({}^{*}\) indicates our implementation, while other results are taken from [12, 76]. The top 1\({}^{\text{st}}\), 2\({}^{\text{nd}}\) and 3\({}^{\text{rd}}\) results are highlighted. OOM means out of memory.

We observe that the ablation of normalization does not lead to substantial deviations on small graphs. However, normalization becomes consistently crucial on large-scale graphs, where its ablation results in accuracy reductions of 4.79% and 4.69% for GraphSAGE\({}^{*}\) and GAT\({}^{*}\) respectively on ogbn-proteins. We believe this is because large graphs display a wider variety of node features, resulting in different data distributions across the graph. Normalization aids in standardizing these features during training, ensuring a more stable distribution.

**Observation 2: Dropout is consistently found to be essential for node classification.**

Our analysis highlights the crucial role of dropout in maintaining the performance of classic GNNs on both homophilous and heterophilous graphs, with its ablation contributing to notable accuracy declines--for instance, a 2.70% decrease for GraphSAGE\({}^{*}\) on PubMed and a 6.57% decrease on Roman-Empire. This trend persists in large-scale datasets, where the ablation of dropout leads to a 2.44% and 2.53% performance decline for GCN\({}^{*}\) and GAT\({}^{*}\) respectively on ogbn-proteins.

**Observation 3: Residual connections can significantly boost performance on specific datasets, exhibiting a more pronounced effect on heterophilous graphs than on homophilous graphs.**

While the ablation of residual connections on homophilous graphs does not consistently lead to a significant performance decrease, with observed differences around 2% on Cora, Photo, and CS, the impact is more substantial on large-scale graphs such as ogbn-proteins and polec. The effect is even more dramatic on heterophilous graphs, with the classic GNNs exhibiting the most significant accuracy reduction on Roman-Empire, for instance, a 16.43% for GCN\({}^{*}\) and 5.48% for GAT\({}^{*}\). Similarly, on Minesweeper, significant performance drops were observed, emphasizing the critical importance of residual connections, particularly on heterophilous graphs. The complex structures of these graphs often necessitate deeper layers to effectively capture the diverse relationships between nodes. In such contexts, residual connections are essential for model training.

\begin{table}
\begin{tabular}{l|c c c c c c c} \hline \hline  & Squirrel & Chameleon & Amazon-Ratings & Roman-Empire & Minesweeper & Questions \\ \hline Metric & Accuracy\(\uparrow\) & Accuracy\(\uparrow\) & Accuracy\(\uparrow\) & Accuracy\(\uparrow\) & Accuracy\(\uparrow\) & ROC-AUC\(\uparrow\) & ROC-AUC\(\uparrow\) \\ \hline
**GCN\({}^{*}\)** & **45.01**\(\pm\) 1.63 & **46.29**\(\pm\) 3.0 & **53.80**\(\pm\) 6.00 & **91.27**\(\pm\) 0.23 & **97.86**\(\pm\) 0.24 & **79.02**\(\pm\) 0.60 \\ (-) Normalization & 44.13 \(\pm\) 2.03 & - & 53.68 \(\pm\) 0.82 & 90.53 \(\pm\) 0.93 & 96.94 \(\pm\) 1.96 & - \\ (-) Dropout & 42.89 \(\pm\) 1.28 & 45.28 \(\pm\) 4.78 & 51.37 \(\pm\) 0.34 & 85.10 \(\pm\) 0.61 & 94.28 \(\pm\) 2.19 & - \\ (-) Residual Connections & 43.14 \(\pm\) 1.82 & - & 51.14 \(\pm\) 3.34 & 74.84 \(\pm\) 0.62 & 86.45 \(\pm\) 0.49 & 75.87 \(\pm\) 4.47 \\ \hline
**GraphSAGE\({}^{*}\)** & **40.78**\(\pm\) 1.47 & **44.81**\(\pm\) 4.34 & **55.40**\(\pm\) 0.21 & **91.06**\(\pm\) 0.27 & **97.77**\(\pm\) 0.62 & **77.21**\(\pm\) 1.18 \\ (-) Normalization & 40.27 \(\pm\) 2.27 & 44.02 \(\pm\) 3.53 & 54.41 \(\pm\) 0.30 & 90.58 \(\pm\) 0.24 & 97.64 \(\pm\) 0.41 & 76.17 \(\pm\) 0.41 \\ (-) Dropout & 38.83 \(\pm\) 1.94 & 43.11 \(\pm\) 3.36 & 51.12 \(\pm\) 0.66 & 84.49 \(\pm\) 0.35 & 93.83 \(\pm\) 0.38 & 76.36 \(\pm\) 1.50 \\ (-) Residual Connections & 40.06 \(\pm\) 2.31 & 41.85 \(\pm\) 3.36 & 53.52 \(\pm\) 0.19 & & 96.64 \(\pm\) 0.58 & - \\ \hline
**GAT\({}^{*}\)** & **41.73**\(\pm\) 2.07 & **41.43**\(\pm\) 4.17 & **55.54**\(\pm\) 0.51 & **90.63**\(\pm\) 0.14 & **97.73**\(\pm\) 0.73 & **77.95**\(\pm\) 0.51 \\ (-) Normalization & 41.08 \(\pm\) 1.60 & 43.25 \(\pm\) 1.34 & 54.85 \(\pm\) 0.39 & 89.69 \(\pm\) 0.39 & 97.42 \(\pm\) 0.45 & 76.32 \(\pm\) 0.23 \\ (-) Dropout & 39.81 \(\pm\) 3.15 & 41.19 \(\pm\) 2.36 & 51.48 \(\pm\) 0.28 & 82.47 \(\pm\) 0.70 & 92.26 \(\pm\) 4.63 & 76.19 \(\pm\) 0.88 \\ (-) Residual Connections & 38.46 \(\pm\) 1.96 & 42.57 \(\pm\) 3.66 & 51.08 \(\pm\) 0.89 & 85.15 \(\pm\) 0.82 & 92.83 \(\pm\) 1.61 & 75.17 \(\pm\) 0.71 \\ \hline \hline \end{tabular}
\end{table}
Table 6: Ablation study on heterophilous graphs (%).

\begin{table}
\begin{tabular}{l|c c c c c c c c} \hline \hline  & Cora & CiteSeer & PubMed & Computer & Photo & CS & Physics & WikiCS \\ \hline Metric & Accuracy\(\uparrow\) & Accuracy\(\uparrow\) & Accuracy\(\uparrow\) & Accuracy\(\uparrow\) & Accuracy\(\uparrow\) & Accuracy\(\uparrow\) & Accuracy\(\uparrow\) & Accuracy\(\uparrow\) & Accuracy\(\uparrow\) \\ \hline
**GCN\({}^{*}\)** & **85.10**\(\pm\) 0.65 & **73.14**\(\pm\) 0.67 & **81.12**\(\pm\) 0.03 & **93.99**\(\pm\) 0.12 & **96.10**\(\pm\) 0.08 & **96.17**\(\pm\) 0.06 & **97.46**\(\pm\) 0.10 & **80.30**\(\pm\) 0.05 \\ (-) Normalization & - & - & - & 92.60 \(\pm\) 0.14 & 95.48 \(\pm\) 0.36 & 95.30 \(\pm\) 0.48 & 97.16 \(\pm\) 0.11 & 79.67 \(\pm\) 0.02 \\ (-) Dropout & 38.46 \(\pm\) 1.16 & 71.40 \(\pm\) 0.35 & 80.14 \(\pm\) 0.55 & 93.78 \(\pm\) 0.26 & 95.31 \(\pm\) 0.30 & 95.95 \(\pm\) 0.11 & 97.30 \(\pm\) 0.06 & 79.84 \(\pm\) 0.16 \\ (-) Residual Connections & - & - & - & 94.43 \(\pm\) 0.08 & 94.71 \(\pm\) 0.11 & 96.56 \(\pm\) 0.18 & - \\ \hline
**GraphSAGE\({}^{*}\)** & **83.88**\(\pm\) 0.45 & **72.26**\(\pm\) 0.58 & **79.72**\(\pm\) 0.50 & **93.25**\(\pm\) 0.14 & **96.78**\(\pm\) 0.22 & **96.38**\(\pm\) 0.11 & **97.19**\(\pm\) 0.08 & **80.69**\(\pm\) 0.31 \\ (-) Normalization & - & - & - & 92.77 \(\pm\) 0.63 & 95.51 \(\pm\) 0.88 & 95.52 \(\pm\) 0.21 & 96.97 \(\pm\) 0.07 & 80.08 \(\pm\) 0.05 \\ (-) Dropout & 82.78 \(\pm\) 0.45 & 71.02 \(\pm\) 1.34 & 77.02 \(\pm\) 0.63 & 92.02 \(\pm\) 0.35 & 96.03 \(\pm\) 0.27 & 96.11 \(\pm\) 0.17 & 97.07 \(\pm\) 0.09 & 79.89 \(\pm\) 0.39 \\ (-) Residual Connections & - & - & - & - & 96.47 \(\pm\) 0.11 & 95.73 \(\pm\) 0.13 & 97.09 \(\pm\) 0.04 & - \\ \hline
**GAT\({}^{*}\)** & **84.46**\(\pm\) 0.45 & **72.22**\(\pm\) 0.64 & **80.28**\(\pm\) 0.64 & **94.09**\(\pm\) 0.37 & **96.66**\(\pm\) 0.39 & **96.21**

**Observation 4: Deeper networks generally lead to greater performance gains on heterophilous graphs compared to homophilous graphs.**

As demonstrated in Figure 1, the performance trends for GCN\({}^{*}\) and GraphSAGE\({}^{*}\) are consistent across different graph types. On homophilous graphs and ogbn-arxiv, both models achieve optimal performance with a range of 2 to 6 layers. In contrast, on heterophilous graphs, their performance improves with an increasing number of layers, indicating that deeper networks are more beneficial for these graphs. We discuss scenarios with more than 10 layers in Appendix B.

## 6 Conclusion

Our study provides a thorough reevaluation of the efficacy of foundational GNN models in node classification tasks. Through extensive empirical analysis, we demonstrate that these classic GNN models can reach or surpass the performance of GTs on various graph datasets, challenging the perceived superiority of GTs in node classification tasks. Furthermore, our comprehensive ablation studies provide insights into how various GNN configurations impact performance. We hope our findings promote more rigorous empirical evaluations in graph machine learning research.

\begin{table}
\begin{tabular}{l|c c c c} \hline \hline  & ogbn-proteins & ogbn-arxiv & ogbn-products & pokec \\ \hline Metric & ROC-AUC\(\uparrow\) & Accuracy\(\uparrow\) & Accuracy\(\uparrow\) & Accuracy\(\uparrow\) \\ \hline \(\mathbf{GCN}^{*}\) & \(\mathbf{77.29}\pm 0.46\) & \(\mathbf{73.53}\pm 0.12\) & \(\mathbf{82.33}\pm 0.19\) & \(\mathbf{86.33}\pm 0.17\) \\ (-) Normalization & \(74.48\pm 1.13\) & \(71.53\pm 0.14\) & \(80.01\pm 0.48\) & \(85.21\pm 0.23\) \\ (-) Dropout & \(74.85\pm 0.87\) & \(72.06\pm 0.13\) & \(79.30\pm 0.37\) & \(84.47\pm 0.38\) \\ (-) Residual Connections & \(73.19\pm 1.46\) & \(72.91\pm 0.17\) & - & \(79.59\pm 0.97\) \\ \hline \(\mathbf{GraphSAGE}^{*}\) & \(\mathbf{82.21}\pm 0.32\) & \(\mathbf{73.00}\pm 0.28\) & \(\mathbf{83.89}\pm 0.36\) & \(\mathbf{85.97}\pm 0.21\) \\ (-) Normalization & \(77.42\pm 0.98\) & \(71.13\pm 0.27\) & \(82.12\pm 0.31\) & \(84.95\pm 0.33\) \\ (-) Dropout & \(80.52\pm 0.49\) & \(71.30\pm 0.21\) & \(80.36\pm 0.43\) & \(83.06\pm 0.28\) \\ (-) Residual Connections & \(81.75\pm 0.53\) & \(72.22\pm 0.69\) & - & \(85.81\pm 0.45\) \\ \hline \(\mathbf{GAT}^{*}\) & \(\mathbf{85.01}\pm 0.46\) & \(\mathbf{73.30}\pm 0.18\) & \(\mathbf{80.99}\pm 0.16\) & \(\mathbf{86.19}\pm 0.23\) \\ (-) Normalization & \(80.32\pm 0.83\) & \(71.33\pm 0.29\) & \(78.62\pm 0.33\) & \(84.63\pm 0.64\) \\ (-) Dropout & \(82.48\pm 0.34\) & \(71.68\pm 0.32\) & \(77.68\pm 0.21\) & \(85.12\pm 0.49\) \\ (-) Residual Connections & \(82.43\pm 0.75\) & \(72.47\pm 0.34\) & - & \(81.37\pm 0.87\) \\ \hline \hline \end{tabular}
\end{table}
Table 7: Ablation study on large-scale graphs (%).

Figure 1: Ablation studies of the number of layers showing, from left to right, results for homophilous graphs, heterophilous graphs, and large-scale graphs, respectively.

## Acknowledgments and Disclosure of Funding

We extend our gratitude to Yiwen Sun for her invaluable assistance. We also express our appreciation to all the anonymous reviewers and ACs for their insightful and constructive feedback. This work received support from National Key R&D Program of China (2021YFB3500700), NSFC Grant 62172026, National Social Science Fund of China 22&ZD153, the Fundamental Research Funds for the Central Universities, State Key Laboratory of Complex & Critical Software Environment (CCSE), HK PolyU Grant P0051029, HK PolyU Grant P0038850, and HK ITF Grant ITS/359/21FP. Lei Shi is with Beihang University and State Key Laboratory of Complex & Critical Software Environment.

## References

* [1] Uri Alon and Eran Yahav. On the bottleneck of graph neural networks and its practical implications. _arXiv preprint arXiv:2006.05205_, 2020.
* [2] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. _arXiv preprint arXiv:1607.06450_, 2016.
* [3] Deyu Bo, Chuan Shi, Lele Wang, and Renjie Liao. Specformer: Spectral graph neural networks meet transformers. _arXiv preprint arXiv:2303.01028_, 2023.
* [4] Xavier Bresson and Thomas Laurent. Residual gated graph convnets. _arXiv preprint arXiv:1711.07553_, 2017.
* [5] Tianle Cai, Shengjie Luo, Keyulu Xu, Di He, Tie-yan Liu, and Liwei Wang. Graphnorm: A principled approach to accelerating graph neural network training. In _International Conference on Machine Learning_, pages 1204-1215. PMLR, 2021.
* [6] Dexiong Chen, Leslie O'Bray, and Karsten Borgwardt. Structure-aware transformer for graph representation learning. In _International Conference on Machine Learning_, pages 3469-3489. PMLR, 2022.
* [7] Jinsong Chen, Kaiyuan Gao, Gaichao Li, and Kun He. Nagphormer: A tokenized graph transformer for node classification in large graphs. In _The Eleventh International Conference on Learning Representations_, 2022.
* [8] Ming Chen, Zhewei Wei, Zengfeng Huang, Bolin Ding, and Yaliang Li. Simple and deep graph convolutional networks. In _International conference on machine learning_, pages 1725-1735. PMLR, 2020.
* [9] Eli Chien, Jianhao Peng, Pan Li, and Olgica Milenkovic. Adaptive universal generalized pagerank graph neural network. In _International Conference on Learning Representations_, 2020.
* [10] Gabriele Corso, Luca Cavalleri, Dominique Beaini, Pietro Lio, and Petar Velickovic. Principal neighbourhood aggregation for graph nets. _Advances in Neural Information Processing Systems_, 33:13260-13271, 2020.
* [11] Hanjun Dai, Zornitsa Kozareva, Bo Dai, Alex Smola, and Le Song. Learning steady-states of iterative algorithms over graphs. In _International conference on machine learning_, pages 1106-1114. PMLR, 2018.
* [12] Chenhui Deng, Zichao Yue, and Zhiru Zhang. Polynormer: Polynomial-expressive graph transformer in linear time. _arXiv preprint arXiv:2403.01232_, 2024.
* [13] Vijay Prakash Dwivedi and Xavier Bresson. A generalization of transformer networks to graphs. _arXiv preprint arXiv:2012.09699_, 2020.
* [14] Vijay Prakash Dwivedi, Chaitanya K Joshi, Anh Tuan Luu, Thomas Laurent, Yoshua Bengio, and Xavier Bresson. Benchmarking graph neural networks. _Journal of Machine Learning Research_, 24(43):1-48, 2023.

* [15] Vijay Prakash Dwivedi, Yozen Liu, Anh Tuan Luu, Xavier Bresson, Neil Shah, and Tong Zhao. Graph transformers for large graphs. _arXiv preprint arXiv:2312.11109_, 2023.
* [16] Matthias Fey and Jan Eric Lenssen. Fast graph representation learning with pytorch geometric. _arXiv preprint arXiv:1903.02428_, 2019.
* [17] Dongqi Fu, Zhigang Hua, Yan Xie, Jin Fang, Si Zhang, Kaan Sancak, Hao Wu, Andrey Malevich, Jingrui He, and Bo Long. VCR-graphormer: A mini-batch graph transformer via virtual connections. In _The Twelfth International Conference on Learning Representations_, 2024.
* [18] Johannes Gasteiger, Aleksandar Bojchevski, and Stephan Gunnemann. Predict then propagate: Graph neural networks meet personalized pagerank. _arXiv preprint arXiv:1810.05997_, 2018.
* [19] Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural message passing for quantum chemistry. In _International conference on machine learning_, pages 1263-1272. PMLR, 2017.
* [20] Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs. _Advances in neural information processing systems_, 30, 2017.
* [21] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 770-778, 2016.
* [22] Geoffrey E Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan R Salakhutdinov. Improving neural networks by preventing co-adaptation of feature detectors. _arXiv preprint arXiv:1207.0580_, 2012.
* [23] Van Thuy Hoang, O Lee, et al. A survey on structure-preserving graph transformers. _arXiv preprint arXiv:2401.16176_, 2024.
* [24] Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, and Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs. _Advances in neural information processing systems_, 33:22118-22133, 2020.
* [25] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected convolutional networks. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 4700-4708, 2017.
* [26] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In _International conference on machine learning_, pages 448-456. pmlr, 2015.
* [27] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. _arXiv preprint arXiv:1412.6980_, 2014.
* [28] Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. In _International Conference on Learning Representations_, 2017.
* [29] Kezhi Kong, Jiuhai Chen, John Kirchenbauer, Renkun Ni, C. Bayan Bruss, and Tom Goldstein. GOAT: A global transformer on large-scale graphs. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, _Proceedings of the 40th International Conference on Machine Learning_, volume 202 of _Proceedings of Machine Learning Research_, pages 17375-17390. PMLR, 23-29 Jul 2023.
* [30] Devin Kreuzer, Dominique Beaini, Will Hamilton, Vincent Letourneau, and Prudencio Tossou. Rethinking graph transformers with spectral attention. _Advances in Neural Information Processing Systems_, 34:21618-21629, 2021.
* [31] Weirui Kuang, WANG Zhen, Yaliang Li, Zhewei Wei, and Bolin Ding. Coarformer: Transformer for large graph via graph coarsening. 2021.
* [32] Jure Leskovec and Andrej Krevl. Snap datasets: Stanford large network dataset collection. 2014. 2016.

* [33] Guohao Li, Matthias Muller, Ali Thabet, and Bernard Ghanem. Deepgcns: Can gcns go as deep as cnns? In _Proceedings of the IEEE/CVF international conference on computer vision_, pages 9267-9276, 2019.
* [34] Guohao Li, Chenxin Xiong, Ali Thabet, and Bernard Ghanem. Deepergcn: All you need to train deeper gcns. _arXiv preprint arXiv:2006.07739_, 2020.
* [35] Qimai Li, Zhichao Han, and Xiao-Ming Wu. Deeper insights into graph convolutional networks for semi-supervised learning. In _Thirty-Second AAAI conference on artificial intelligence_, 2018.
* [36] Xiang Li, Renyu Zhu, Yao Cheng, Caihua Shan, Siqiang Luo, Dongsheng Li, and Weining Qian. Finding global homophily in graph neural networks when meeting heterophily. In _International Conference on Machine Learning_, pages 13242-13256. PMLR, 2022.
* [37] Derek Lim, Felix Hohne, Xiuyu Li, Sijia Linda Huang, Vaishnavi Gupta, Omkar Bhalerao, and Ser Nam Lim. Large scale learning on non-homophilous graphs: New benchmarks and strong simple methods. _Advances in Neural Information Processing Systems_, 34:20887-20902, 2021.
* [38] Chuang Liu, Yibing Zhan, Xueqi Ma, Liang Ding, Dapeng Tao, Jia Wu, and Wenbin Hu. Gapformer: Graph transformer with graph pooling for node classification. In _Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence, IJCAI_, pages 2196-2205, 2023.
* [39] Linyuan Lu and Tao Zhou. Link prediction in complex networks: A survey. _Physica A: statistical mechanics and its applications_, 390(6):1150-1170, 2011.
* [40] Sitao Luan, Chenqing Hua, Qincheng Lu, Jiaqi Zhu, Mingde Zhao, Shuyuan Zhang, Xiao-Wen Chang, and Doina Precup. Revisiting heterophily for graph neural networks. _Advances in neural information processing systems_, 35:1362-1375, 2022.
* [41] Yuankai Luo. Transformers for capturing multi-level graph structure using hierarchical distances. _arXiv preprint arXiv:2308.11129_, 2023.
* [42] Yuankai Luo, Qijiong Liu, Lei Shi, and Xiao-Ming Wu. Structure-aware semantic node identifiers for learning on graphs. _arXiv preprint arXiv:2405.16435_, 2024.
* [43] Yuankai Luo, Lei Shi, and Veronika Thost. Improving self-supervised molecular representation learning using persistent homology. _Advances in Neural Information Processing Systems_, 36, 2024.
* [44] Yuankai Luo, Lei Shi, Mufan Xu, Yuwen Ji, Fengli Xiao, Chunming Hu, and Zhiguang Shan. Impact-oriented contextual scholar profiling using self-citation graphs. _arXiv preprint arXiv:2304.12217_, 2023.
* [45] Yuankai Luo, Veronika Thost, and Lei Shi. Transformers over directed acyclic graphs. _Advances in Neural Information Processing Systems_, 36, 2024.
* [46] Yao Ma, Xiaorui Liu, Neil Shah, and Jiliang Tang. Is homophily a necessity for graph neural networks? _arXiv preprint arXiv:2106.06134_, 2021.
* [47] Seiji Maekawa, Koki Noda, Yuya Sasaki, et al. Beyond real-world benchmark datasets: An empirical study of node classification with gnns. _Advances in Neural Information Processing Systems_, 35:5562-5574, 2022.
* [48] Sunil Kumar Maurya, Xin Liu, and Tsuyoshi Murata. Improving graph neural networks with simple architecture design. _arXiv preprint arXiv:2105.07634_, 2021.
* [49] Sunil Kumar Maurya, Xin Liu, and Tsuyoshi Murata. Simplifying approach to node classification in graph neural networks. _Journal of Computational Science_, 62:101695, 2022.
* [50] Peter Mernyei and Catalina Cangea. Wiki-cs: A wikipedia-based benchmark for graph neural networks. _arXiv preprint arXiv:2007.02901_, 2020.

* [51] Erxue Min, Runfa Chen, Yatao Bian, Tingyang Xu, Kangfei Zhao, Wenbing Huang, Peilin Zhao, Junzhou Huang, Sophia Ananiadou, and Yu Rong. Transformer for graphs: An overview from architecture perspective. _arXiv preprint arXiv:2202.08455_, 2022.
* [52] Christopher Morris, Martin Ritzert, Matthias Fey, William L Hamilton, Jan Eric Lenssen, Gaurav Rattan, and Martin Grohe. Weisfeiler and leman go neural: Higher-order graph neural networks. In _Proceedings of the AAAI conference on artificial intelligence_, volume 33, pages 4602-4609, 2019.
* [53] Luis Muller, Mikhail Galkin, Christopher Morris, and Ladislav Rampasek. Attending to graph transformers. _arXiv preprint arXiv:2302.04181_, 2023.
* [54] Andrew Ng, Michael Jordan, and Yair Weiss. On spectral clustering: Analysis and an algorithm. _Advances in neural information processing systems_, 14, 2001.
* [55] Mathias Niepert, Mohamed Ahmed, and Konstantin Kutzkov. Learning convolutional neural networks for graphs. In _International conference on machine learning_, pages 2014-2023. PMLR, 2016.
* [56] Giannis Nikolentzos and Michalis Vazirgiannis. Random walk graph neural networks. _Advances in Neural Information Processing Systems_, 33:16211-16222, 2020.
* [57] Hongbin Pei, Bingzhe Wei, Kevin Chen-Chuan Chang, Yu Lei, and Bo Yang. Geom-gcn: Geometric graph convolutional networks. _arXiv preprint arXiv:2002.05287_, 2020.
* [58] Oleg Platonov, Denis Kuznedelev, Michael Diskin, Artem Babenko, and Liudmila Prokhorenkova. A critical look at the evaluation of gnns under heterophily: Are we really making progress? _arXiv preprint arXiv:2302.11640_, 2023.
* [59] Ladislav Rampasek, Mikhail Galkin, Vijay Prakash Dwivedi, Anh Tuan Luu, Guy Wolf, and Dominique Beaini. Recipe for a general, powerful, scalable graph transformer. _arXiv preprint arXiv:2205.12454_, 2022.
* [60] Emanuele Rossi, Fabrizio Frasca, Ben Chamberlain, Davide Eynard, Michael Bronstein, and Federico Monti. Sign: Scalable inception graph neural networks. _arXiv preprint arXiv:2004.11198_, 7:15, 2020.
* [61] Benedek Rozemberczki, Carl Allen, and Rik Sarkar. Multi-scale attributed node embedding. _Journal of Complex Networks_, 9(2):cnab014, 2021.
* [62] Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and Tina Eliassi-Rad. Collective classification in network data. _AI magazine_, 29(3):93-93, 2008.
* [63] Oleksandr Shchur, Maximilian Mumme, Aleksandar Bojchevski, and Stephan Gunnemann. Pitfalls of graph neural network evaluation. _arXiv preprint arXiv:1811.05868_, 2018.
* [64] Hamed Shirzad, Ameya Velingker, Balaji Venkatachalam, Danica J Sutherland, and Ali Kemal Sinop. Exphormer: Sparse transformers for graphs. _arXiv preprint arXiv:2303.06147_, 2023.
* [65] Juan Shu, Bowei Xi, Yu Li, Fan Wu, Charles Kamhoua, and Jianzhu Ma. Understanding dropout for graph neural networks. In _Companion Proceedings of the Web Conference 2022_, pages 1128-1138, 2022.
* [66] Yunchong Song, Chenghu Zhou, Xinbing Wang, and Zhouhan Lin. Ordered GNN: Ordering message passing to deal with heterophily and over-smoothing. In _The Eleventh International Conference on Learning Representations_, 2023.
* [67] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: a simple way to prevent neural networks from overfitting. _The journal of machine learning research_, 15(1):1929-1958, 2014.
* [68] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. _Advances in neural information processing systems_, 30, 2017.

* [69] Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. Graph attention networks. In _International Conference on Learning Representations_, 2018.
* [70] Clement Vignac, Andreas Loukas, and Pascal Frossard. Building powerful and equivariant graph neural networks with structural message-passing. _Advances in neural information processing systems_, 33:14143-14155, 2020.
* [71] Ulrike Von Luxburg. A tutorial on spectral clustering. _Statistics and computing_, 17:395-416, 2007.
* [72] Minjie Wang, Da Zheng, Zihao Ye, Quan Gan, Mufei Li, Xiang Song, Jinjing Zhou, Chao Ma, Lingfan Yu, Yu Gai, et al. Deep graph library: A graph-centric, highly-performant package for graph neural networks. _arXiv preprint arXiv:1909.01315_, 2019.
* [73] Yangkun Wang, Jiarui Jin, Weinan Zhang, Yong Yu, Zheng Zhang, and David Wipf. Bag of tricks for node classification with graph neural networks. _arXiv preprint arXiv:2103.13355_, 2021.
* [74] Qitian Wu, Chenxiao Yang, Wentao Zhao, Yixuan He, David Wipf, and Junchi Yan. DIFFormer: Scalable (graph) transformers induced by energy constrained diffusion. In _The Eleventh International Conference on Learning Representations_, 2023.
* [75] Qitian Wu, Wentao Zhao, Zenan Li, David P Wipf, and Junchi Yan. Nodeformer: A scalable graph structure learning transformer for node classification. _Advances in Neural Information Processing Systems_, 35:27387-27401, 2022.
* [76] Qitian Wu, Wentao Zhao, Chenxiao Yang, Hengrui Zhang, Fan Nie, Haitian Jiang, Yatao Bian, and Junchi Yan. Simplifying and empowering transformers for large-graph representations. In _Thirty-seventh Conference on Neural Information Processing Systems_, 2023.
* [77] Xiao-Ming Wu, Zhenguo Li, Anthony So, John Wright, and Shih-Fu Chang. Learning with partially absorbing random walks. _Advances in neural information processing systems_, 25, 2012.
* [78] Xiao-Ming Wu, Anthony So, Zhenguo Li, and Shuo-yen Li. Fast graph laplacian regularized kernel learning via semidefinite-quadratic-linear programming. _Advances in Neural Information Processing Systems_, 22, 2009.
* [79] Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and S Yu Philip. A comprehensive survey on graph neural networks. _IEEE transactions on neural networks and learning systems_, 32(1):4-24, 2020.
* [80] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? _arXiv preprint arXiv:1810.00826_, 2018.
* [81] Keyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken-ichi Kawarabayashi, and Stefanie Jegelka. Representation learning on graphs with jumping knowledge networks. In _International conference on machine learning_, pages 5453-5462. PMLR, 2018.
* [82] Chengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin Zheng, Guolin Ke, Di He, Yanming Shen, and Tie-Yan Liu. Do transformers really perform badly for graph representation? _Advances in Neural Information Processing Systems_, 34:28877-28888, 2021.
* [83] Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson. How transferable are features in deep neural networks? _Advances in neural information processing systems_, 27, 2014.
* [84] Jiaxuan You, Rex Ying, and Jure Leskovec. Position-aware graph neural networks. In _International conference on machine learning_, pages 7134-7143. PMLR, 2019.
* [85] Seongjun Yun, Seoyoon Kim, Junhyun Lee, Jaewoo Kang, and Hyunwoo J Kim. Neo-gnns: Neighborhood overlap-aware graph neural networks for link prediction. _Advances in Neural Information Processing Systems_, 34:13683-13694, 2021.

* [86] Bohang Zhang, Shengjie Luo, Liwei Wang, and Di He. Rethinking the expressive power of GNNs via graph biconnectivity. In _The Eleventh International Conference on Learning Representations_, 2023.
* [87] Muhan Zhang and Yixin Chen. Link prediction based on graph neural networks. _Advances in neural information processing systems_, 31, 2018.
* [88] Zaixi Zhang, Qi Liu, Qingyong Hu, and Chee-Kong Lee. Hierarchical graph transformer with adaptive node sampling. _Advances in Neural Information Processing Systems_, 35:21171-21183, 2022.
* [89] Jiong Zhu, Ryan A Rossi, Anup Rao, Tung Mai, Nedim Lipka, Nesreen K Ahmed, and Danai Koutra. Graph neural networks with heterophily. In _Proceedings of the AAAI conference on artificial intelligence_, volume 35, pages 11168-11176, 2021.
* [90] Jiong Zhu, Yujun Yan, Lingxiao Zhao, Mark Heimann, Leman Akoglu, and Danai Koutra. Beyond homophily in graph neural networks: Current limitations and effective designs. _Advances in neural information processing systems_, 33:7793-7804, 2020.
* [91] Wenhao Zhu, Tianyu Wen, Guojie Song, Xiaojun Ma, and Liang Wang. Hierarchical transformer for scalable graph learning. _arXiv preprint arXiv:2305.02866_, 2023.
* [92] Xiaojin Jerry Zhu. Semi-supervised learning literature survey. 2005.

Datasets and Experimental Details

### Computing Environment

Our implementation is based on PyG [16] and DGL [72]. The experiments are conducted on a single workstation with 8 RTX 3090 GPUs. Notably, only the experiments on the pokec dataset are performed on a separate workstation with 2 A100 GPUs.

### Hyperparameters and Reproducibility

For the hyperparameter selections of classic GNNs, in addition to what we have covered, we list other settings in Tables 8, 9, 10. Notably, for heterophilous graphs, we expand the search range for the number of layers to include three additional settings: \(\{12,15,20\}\) (See Section B.2 for further analysis). This adjustment is based on our empirical evidence suggesting that deep networks tend to yield performance improvements on heterophilous graphs. The ReLU function serves as the non-linear activation. Further details regarding hyperparameters can be found in our code https://github.com/LUOyk1999/tunedGNN.

For hyperparameters specific to each GT, which are not present in classic GNNs, we tuned them according to the search space specified in the original GT papers:

* GraphGPS: the number of heads from \(\{1,2,4\}\), GNNs from \(\{\text{GCN},\text{GraphSAGE},\text{GAT}\}\), positional encoding schemes from \(\{\text{None},\text{LapPE},\text{RWSE}\}\).
* NAGphormer: the number of heads from \(\{1,2,4\}\), number of hops from \(\{3,10\}\).
* Explorer: the number of heads from \(\{1,2,4\}\), positional encoding schemes from \(\{\text{None},\text{LapPE},\text{RWSE}\}\).
* GOAT: the number of heads from \(\{1,2,4\}\), codebook size from \(\{1024,2048,4096\}\).
* NodeFormer: the number of heads from \(\{1,2,4\}\), \(M\) from \(\{30,50\}\), \(K\) from \(\{5,10\}\), rb from \(\{1,2\}\), temperature from \(\{0.25\}\).
* Polynormer: the number of heads from \(\{1,2,4,8\}\).
* SGFormer: the number of heads from \(\{1,2,4\}\), GNN weight from \(\{0.5,0.8\}\).

Due to the large size of the graphs in ogbn-proteins, ogbn-products, and pokec, which prevents full-batch training on GPU memory, we adopt different batch training strategies. For ogbn-proteins, we utilize the optimized neighbor sampling method [20]. For pokec and ogbn-products, we apply the random partitioning method previously used by GTs [75, 12, 76] to enable mini-batch training. For other datasets, we employ full-batch training.

The testing accuracy achieved by the model that reports the highest result on the validation set is used for evaluation. Additionally, we report mean scores and standard deviations after 5 independent runs with different initializations.

Our code is available under the MIT License.

## Appendix B Additional Benchmarking Results

### Gat\({}^{*}\) with Edge Features on ogbn-proteins

While DeepGCN [33] introduced training models up to 56 layers deep and DeeperGCN [34] further extended this to 112 layers, our experiments suggest that such depth is not necessary. Specifically, while the DeeperGCN achieved an accuracy of 85.50% on ogbn-proteins, it utilized edge features as input, a configuration not commonly employed in the standard baselines of the OGB dataset [24]. As our experiments do not incorporate edge features on ogbn-proteins, we exclude DeeperGCN from the main text to maintain a fair comparison.

\begin{table}
\begin{tabular}{l l l l l l l l} \hline \hline Dataset & ResNet & Normalization & Dropout rate & GNNs layer \(L\) & Hidden dim & LR & epoch \\ \hline Cora & True & False & 0.2 & 3 & 512 & 0.001 & 500 \\ Citeseer & True & False & 0.5 & 3 & 256 & 0.001 & 500 \\ Pubmed & False & False & 0.5 & 2 & 512 & 0.001 & 500 \\ Computer & False & LN & 0.5 & 2 & 512 & 0.001 & 1000 \\ Pubto & True & LN & 0.5 & 3 & 512 & 0.001 & 1000 \\ CS & True & LN & 0.3 & 2 & 512 & 0.001 & 1500 \\ Physics & True & LN & 0.3 & 2 & 64 & 0.001 & 1500 \\ WikiCS & False & LN & 0.5 & 3 & 256 & 0.001 & 1000 \\ \hline Squirrel & True & BN & 0.7 & 4 & 256 & 0.001 & 500 \\ Chameleon & False & False & 0.2 & 5 & 512 & 0.005 & 200 \\ Amazon-Ratings & True & BN & 0.5 & 4 & 512 & 0.001 & 2500 \\ Roman-Empire & True & BN & 0.5 & 9 & 512 & 0.001 & 2500 \\ Minesweper & True & BN & 0.2 & 12 & 64 & 0.01 & 2000 \\ Questions & True & False & 0.3 & 10 & 512 & 0.001 & 1500 \\ \hline ogbn-proteins & True & BN & 0.3 & 3 & 512 & 0.01 & 100 \\ ogbn-ariv & True & BN & 0.5 & 5 & 512 & 0.0005 & 2000 \\ ogbn-products & False & LN & 0.5 & 5 & 256 & 0.003 & 300 \\ pokec & True & BN & 0.2 & 7 & 256 & 0.0005 & 2000 \\ \hline \hline \end{tabular}
\end{table}
Table 8: Dataset-specific hyperparameter settings of GCN\({}^{*}\).

\begin{table}
\begin{tabular}{l l l l l l l l} \hline \hline Dataset & ResNet & Normalization & Dropout rate & GNNs layer \(L\) & Hidden dim & LR & epoch \\ \hline Cora & False & False & 0.7 & 3 & 512 & 0.001 & 500 \\ Citeseer & False & False & 0.5 & 2 & 512 & 0.001 & 500 \\ Pubmed & False & False & 0.7 & 2 & 256 & 0.005 & 500 \\ Computer & False & LN & 0.5 & 3 & 512 & 0.001 & 1000 \\ Photo & True & LN & 0.5 & 6 & 256 & 0.001 & 1000 \\ CS & True & LN & 0.3 & 2 & 512 & 0.001 & 1500 \\ Physics & True & LN & 0.3 & 2 & 64 & 0.001 & 1500 \\ WikiCS & False & LN & 0.5 & 3 & 256 & 0.001 & 1000 \\ \hline Squirrel & True & BN & 0.7 & 4 & 256 & 0.001 & 500 \\ Chameleon & False & False & 0.5 & 2 & 512 & 0.001 & 500 \\ Amazon-Ratings & True & BN & 0.5 & 4 & 512 & 0.001 & 1500 \\ Roman-Empire & True & BN & 0.5 & 9 & 512 & 0.001 & 2500 \\ Minesweper & True & BN & 0.2 & 12 & 64 & 0.001 & 2000 \\ Questions & True & False & 0.3 & 10 & 512 & 0.001 & 1500 \\ \hline ogbn-proteins & True & BN & 0.3 & 3 & 512 & 0.001 & 1000 \\ ogbn-ariv & True & BN & 0.5 & 5 & 512 & 0.0005 & 2000 \\ ogbn-products & False & LN & 0.5 & 5 & 256 & 0.003 & 1000 \\ pokec & True & BN & 0.2 & 7 & 256 & 0.0005 & 2000 \\ \hline \hline \end{tabular}
\end{table}
Table 9: Dataset-specific hyperparameter settings of GraphSAGE\({}^{*}\).

\begin{table}
\begin{tabular}{l l l l l l l l} \hline \hline Dataset & ResNet & Normalization & Dropout rate & GNNs layer \(L\) & Hidden dim & LR & epoch \\ \hline Cora & False & False & 0.7 & 3 & 512 & 0.001 & 500 \\ Citeseer & False & False & 0.2 & 3 & 512 & 0.001 & 500 \\ Pubmed & False & False & 0.7 & 4 & 512 & 0.005 & 500 \\ Computer & False & LN & 0.3 & 4 & 64 & 0.001 & 1000 \\ Photo & True & LN & 0.2 & 6 & 64 & 0.001 & 1000 \\ CS & True & LN & 0.5 & 2 & 512 & 0.001 & 1500 \\ Physics & True & BN & 0.7 & 2 & 64 & 0.001 & 1500 \\ WikiCS & False & LN & 0.7 & 2 & 256 & 0.001 & 1000 \\ Amazon-Ratings & True & BN & 0.5 & 9 & 512 & 0.001 & 2500 \\ Roman-Empire & False & BN & 0.3 & 9 & 256 & 0.001 & 2500 \\ Minesweper & True & BN & 0.2 & 15 & 64 & 0.001 & 2500 \\ Questions & True & LN & 0.2 & 3 & 512 & 0.001 & 1500 \\ \hline ogbn-proteins & True & BN & 0.3 & 7 & 512 & 0.01 & 1000 \\ ogbn-ariv & True & BN & 0.5 & 5 & 256 & 0.0005 & 2000 \\ ogbn-products & False & LN & 0.5 & 5 & 256 & 0.003 & 1000 \\ pokec & True & BN & 0.2 & 7 & 256 & 0.0005 & 2000 \\ \hline \hline \end{tabular}
\end{table}
Table 10: Dataset-specific hyperparameter settings of GAT\({}^{*}\).

[MISSING_PAGE_FAIL:18]

that very deep networks may not yield significantly better results. Overall, the best results for classic GNNs are achieved when \(L\) is limited to 15.

### Jumping Knowledge Mode and Early Results

**Jumping Knowledge (JK) Mode**[81] aggregates representations from different GNN layers, effectively capturing information from varying neighborhood ranges within the graph. For any node \(v\), the summation version of JK mode produces the representation of \(v\) by:

\[\text{GNN}_{\text{JK}}(v,\bm{A},\bm{X})=\bm{h}_{v}^{1}+\bm{h}_{v}^{2}+\ldots+ \bm{h}_{v}^{L},\] (8)

where \(L\) is the number of GNN layers. In our previous experimental setups, we treated JK as a hyperparameter configuration for GNNs. Based on the hyperparameter configurations outlined in Section 3, we expanded the tuning space to include the decision of whether to use JK. In past experiments, we did not perform an exhaustive search; instead, we selected subsets based on experience within this search space, and our early results are reported in Table 13, 14, and 15 (For additional information, please refer to https://arxiv.org/abs/2406.08993v1). However, after a more detailed hyperparameter tuning, we found that JK may not be necessary. In most datasets, the results without using JK are comparable to, and sometimes even better than, those with JK. Consequently, we removed JK from the hyperparameter tuning search space in our paper.

## Appendix C Visualization

Here, we present t-SNE visualizations of classification results. As shown in Figure 2, the node embeddings generated by GCN\({}^{*}\) (our implementation) display greater inter-class distances than those produced by Polynormer\({}^{*}\).

## Appendix D Limitations & Broader Impacts

**Broader Impacts.** This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here.

**Limitations.** In this study, we focus solely on the node classification task, without delving into graph classification [14, 44, 43] and link prediction [39, 87] tasks. It would be beneficial to extend our benchmarking efforts to include classic GNNs in graph-level and edge-level tasks.

Figure 2: t-SNE visualizations of node embeddings.

## Checklist

1. For all authors... 1. Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? [Yes] 2. Did you describe the limitations of your work? [Yes] See Appendix D. 3. Did you discuss any potential negative societal impacts of your work? [Yes] See Appendix D. 4. Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes]
2. If you are including theoretical results... 1. Did you state the full set of assumptions of all theoretical results? [N/A] 2. Did you include complete proofs of all theoretical results? [N/A]
3. If you ran experiments (e.g. for benchmarks)... 1. Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? [Yes] Code, data, and instructions are available at https://github.com/LUOyk1999/tunedGNN. 2. Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [Yes] See Section 4 and Appendix A. 3. Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? [Yes] 4. Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes] See Appendix A.
4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets... 1. If your work uses existing assets, did you cite the creators? [Yes] 2. Did you mention the license of the assets? [Yes] See Appendix A. 3. Did you include any new assets either in the supplemental material or as a URL? [Yes] See https://github.com/LUOyk1999/tunedGNN. 4. Did you discuss whether and how consent was obtained from people whose data you're using/curating? [N/A] 5. Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [N/A]
5. If you used crowdsourcing or conducted research with human subjects... 1. Did you include the full text of instructions given to participants and screenshots, if applicable? [N/A] 2. Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A] 3. Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A]