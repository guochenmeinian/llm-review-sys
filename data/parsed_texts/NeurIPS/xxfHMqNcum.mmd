# Towards Hybrid-grained Feature Interaction Selection for Deep Sparse Network

 Fuyuan Lyu\({}^{1}\), Xing Tang\({}^{2}\)\({}^{\dagger}\), Dugang Liu\({}^{4}\), Chen Ma\({}^{3}\),

Weihong Luo\({}^{2}\), Liang Chen\({}^{2}\), Xiuqiang He\({}^{2}\), Xue Liu\({}^{1}\)

\({}^{1}\)McGill University, \({}^{2}\)FiT, Tencent, \({}^{3}\)City University of Hong Kong,

\({}^{4}\)Guangdong Laboratory of Artificial Intelligence and Digital Economy (SZ)

\({}^{\dagger}\) corresponding author

fuyuan.lyu@mail.mcgill.ca, xing.tang@hotmail.com,

dugang.ldg@gmail.com, chenma@cityu.edu.hk,

{lobbyluo,leocchen,xiuqianghe}@tencent.com,

xue.liu@cs.mcgill.ca

###### Abstract

Deep sparse networks are widely investigated as a neural network architecture for prediction tasks with high-dimensional sparse features, with which feature interaction selection is a critical component. While previous methods primarily focus on how to search feature interaction in a coarse-grained space, less attention has been given to a finer granularity. In this work, we introduce a hybrid-grained feature interaction selection approach that targets both feature field and feature value for deep sparse networks. To explore such expansive space, we propose a decomposed space which is calculated on the fly. We then develop a selection algorithm called OptFeature, which efficiently selects the feature interaction from both the feature field and the feature value simultaneously. Results from experiments on three large real-world benchmark datasets demonstrate that OptFeature performs well in terms of accuracy and efficiency. Additional studies support the feasibility of our method. All source code are publicly available1.

Footnote 1: https://github.com/fuyuanlyu/OptFeature

## 1 Introduction

Deep Sparse Networks (DSNs) are commonly utilized neural network architectures for prediction tasks, designed to handle sparse and high-dimensional categorical features as inputs. These networks find widespread application in real-world scenarios such as advertisement recommendation, fraud detection, and more. For instance, in the context of advertisement recommendation, the input often comprises high-dimensional features like _user id_ and _City_, which significantly contribute to the final prediction.

As depicted in Figure 1(a), the general DSN architecture consists of three components. Firstly, the embedding layer transforms different feature values into dense embeddings. Following this, feature interaction layers create feature interactions [4] based on the embeddings of raw features, as illustrated in Figure 1(b). Finally, the predictor makes the final prediction based on the features and their interactions. A core challenge in making accurate DSNs predictions is effectively capturing suitable feature interactions among input features [8, 11, 16, 15].

Various methods have been proposed to address the issue of modelling feature interactions. Wide&Deep first models human-crafted feature interactions using linear regression [22]. To eliminate the need for human expertise, DeepFM [8] models all second-order feature interactions by utilizinga factorization machine [21] and adopts a multi-layer perceptron (MLP) as a predictor. Further advancements have replaced the factorization machine with different operations, such as product operations [18], cross network [24; 25], or an MLP component [19]. However, these methods have inherent drawbacks as they directly model all possible feature interactions. This inevitably introduces noise into the models, increasing their complexity and potentially degrading performance.

Neural architecture search(NAS) [12] has been introduced as a powerful approach for feature interaction selection in DSNs for both efficiency and effectiveness [11; 7; 15]. AutoFIS [11] first propose to select feature interactions by adding an attention gate to each possible one. PROFIT [7] suggests progressively selecting from a distilled search space with fewer parameters. Alongside feature interaction selection, some works like AutoFeature [9] and OptInter [15] aim to search for the interaction operation. However, all these works focus on evaluating the whole feature field as a whole instead of individual feature value. To be more precise, the entire _user id_ or _City_ feature field would be retained or discarded as a unit instead of considering individual feature values like _User x479bs_ or _New Orleans_. Considering only feature fields inevitably be coarse-grained, which may overlook informative values in uninformative fields, and vice versa.

In this work, we propose extending the selection granularity of feature interactions from the field to the value level. The extension of granularity would significantly increase the complexity of the entire selection space, leading to an increase in exploration time and memory usage. We manage this challenge by decomposing the selection space using tensor factorization and calculating the corresponding parameters on the fly. To further improve the selection efficiency, we introduce a hybrid-grained feature interaction selection space, which explicitly considers the relation between field-level and value-level. To perform feature interaction selection, we develop a sparsification-based selection algorithm named **OptFeature**(short for **Optimizing Feature** Interaction Selection), which efficiently selects the feature interaction concurrently from both feature fields and feature values. We conduct experiments over three large-scale real-world benchmarks and compare accuracy and efficiency with state-of-the-art models. Empirical results demonstrate the superiority of our method on both dimensions. Additional studies support the feasibility of our method.

## 2 Related Work

### Deep Sparse Networks (DSNs)

It is commonly believed that feature interaction is the critical challenge towards accurate prediction in deep sparse networks [15]. Various models have been proposed to address the feature interaction layer. DeepFM [8] utilizes a factorization machine [21] to model all second-order feature interactions, followed by a multi-layer perceptron as the predictor. More sophisticated feature interaction layers have been proposed to replace the factorization machine, such as the inner product operation in IPNN [18], the cross-network in DCN [24; 25] or the MLP component in PIN [19].

With the advancement of neural architecture search [31; 14; 12] and continuous sparsification [1], various methods have been proposed to select the informative feature interactions [11; 7] and reduce computational costs. AutoFis [11] employs a sparse optimizer to select suitable feature interactions at the field level. PROFIT [7] formulates field-level feature interaction selection within a distilled search space. It employs a progressive search for efficient exploration. AutoIAS [26] takes one step

Figure 1: Illustration of deep sparse network and feature interaction layer

further to integrate the feature interaction selection as part of the search space and jointly conduct the search with other components like MLP architecture or embedding dimension. GAIN [13], on the other hand, focuses on the DCN [24]-like architectures and conducts the feature interaction selection jointly with the model training. However, all previous works conduct feature interaction selection on the field level. Our work builds on the existing approaches for modelling feature interactions in DSNs. We extend the selection granularity to the value level and propose a hybrid-grained selection approach.

Furthermore, there also exists some works such as OptInter [15], AutoFeature [9], and NAS-CTR [29] take a different angle and search for suitable operations (such as inner product, outer product or element-wise sum) to model each feature interaction properly. These works are perpendicular to our study.

### Feature Interaction Selection in DSNs

To conduct feature interaction selection in DSNs, previous works [11; 7; 26; 13] borrow the ideas from neural architecture search [26; 11] and continuous sparsification [7; 13]. We briefly introduce these two techniques in the following. Previous works focusing on feature selection [3] are excluded from the discussion.

#### 2.2.1 Neural Architecture Search

Neural architecture search (NAS) automatically identifies optimal architectures for specific tasks and datasets, eliminating the need for expert design [31; 12; 14; 20]. This approach has led to substantial advancements in various domains, including vision [31; 12; 14], natural language processing [23] and sparse representation learning [11; 7]. There are two key aspects to neural architecture search: _search space_ and _search algorithm_. The _search space_ comprises all potential architectures and is usually task-dependent. An effective search space should be neither too complex, leading to high search costs, nor too shallow, ensuring the inclusion of suitable architectures. The _search algorithm_ explores the search space to find appropriate architectures. Search algorithms can generally be categorized into controller-based [31], evaluation-based [20], and gradient-based [14; 12] classes. Our work distinguishes itself from existing research by addressing the challenge of feature interaction selection for deep sparse networks, where the primary obstacles involve high search costs and expansive search spaces.

#### 2.2.2 Continuous Sparsification

Continuous sparsification focuses on reducing a continuous value to a sparse form and is closely related to our method [1]. It is often used to prune less informative components or parameters from neural networks, which is intrinsically equivalent to solving an \(L_{0}\) normalization problem. This technique has been leveraged in various aspects of deep sparse networks (DSNs), notably in the embedding layer [17] and the feature interaction layer [11]. Our work employs continuous sparsification to discretize the selection of value-level feature interactions during the search process. By doing so, we can effectively manage the complexity of the search space and maintain a high level of efficiency in the search algorithm.

## 3 Methods

This section will first formulate the general DSNs in Section 3.1. Then it will elaborate on the feature interaction selection problem from field and value perspectives in Section 3.2. We further concrete the proposed OptFeature method, which conducts hybrid-grained feature interaction selection in Section 3.3.

### DSNs Formulation

Feature Field and ValueThe input of DSNs comprises multiple feature fields, each containing a set of feature values. Hence, two forms of expression exist for each data sample, deriving from feature field and feature value perspectives. More precisely, a data sample \(\mathbf{x}\) consists of \(n\) feature fields can be written as:\[\text{field perspective:} \mathbf{x} =[\mathbf{z}_{1},\mathbf{z}_{2},...,\mathbf{z}_{n}],\] \[\text{value perspective:} \mathbf{x} =[x_{k_{1}},x_{k_{2}},...,x_{k_{n}}].\] (1)

The notation \(\mathbf{z}_{i}=\{x_{k_{i}}\mid k_{i}\in[1,m_{i}],k_{i}\in N_{+}\}\) represents the set of feature values within the \(i\)-th field. Here \(m_{i}\) denotes the number of values in each field, and \(m=\Sigma_{i=1}^{n}m_{i}\) refers to that across all fields. An illustration figure is shown in Figure 1(c). We highlight this formulation as it is one of the core differences between our method, which focuses on both _field_ and _value_ perspectives, and previous works [11; 7], which focuses only on the _field_ perspective. This allows us to capture more granular and informative interactions, potentially leading to better model performance.

Deep Sparse NetworksAs mentioned in Section 1, the DSN commonly consists of three major components: the embedding layer, the feature interaction layer and the predictor. The embedding layer usually employs an embedding table to convert \(\mathbf{z}_{i}\) from high-dimensional, sparse vectors into low-dimensional, dense embeddings. This can be formulated as:

\[\mathbf{e}_{i}=\mathbf{E}\times\mathbf{z}_{i}=\mathbf{E}\times x_{k_{i}},\] (2)

where \(\mathbf{E}\in\mathbb{R}^{m\times d}\) is the embedding table, \(d\) is the pre-defined embedding size. These embeddings are further stacked together as the embedding vector \(\mathbf{e}=[\mathbf{e}_{1},\mathbf{e}_{2},...,\mathbf{e}_{n}]\), which also serves as the input for feature interaction layer.

The feature interaction layer further performs interaction operations over its input. The \(t\)-th order feature interaction can be generally represented as:

\[\mathbf{v}^{t}=\mathcal{V}^{t}(\mathbf{e}),\] (3)

where \(\mathcal{V}(\cdot)\), as the \(t\)-th order interaction operation, \(2\leq t\leq n\), can vary from a multiple layer perceptron [19] to cross layer [24]. In this paper, we select the inner product, which is recognized to be the most commonly used operation for feature interaction [18; 7], as our interaction operation. For \(t\)-th order feature interactions, cardinality \(|\mathbf{v}^{t}|=C_{n}^{t}\). Note that the selection of \(t\) is usually task-dependent. For instance, in the click-through rate prediction, \(t=2\) is considered sufficient [15].

In the prediction layer, both the embedding vector \(\mathbf{e}\) and feature interaction vectors \(\mathbf{v}^{t}\) are aggregated together to make the final prediction \(\phi_{\theta}(\mathbf{e},\{\mathbf{v}^{t}\mid 2\leq t\leq n\})\), where \(\phi_{\theta}(\cdot)\) represents the prediction function parameterized on \(\theta\). The mainstream prediction function is MLP [18; 19]. Finally, the loss function is calculated over each data sample and forms the training object:

\[\mathcal{L}(\mathcal{D}_{\text{tra}}\mid\mathbf{W})=\frac{1}{|\mathcal{D}_{ \text{tra}}|}\sum_{(\mathbf{x},y)\in\mathcal{D}_{\text{tra}}}\ell(y,\hat{y}) =\frac{1}{|\mathcal{D}_{\text{tra}}|}\sum_{(\mathbf{x},y)\in\mathcal{D}_{ \text{tra}}}\ell(y,\phi_{\theta}(\mathbf{E}\times\mathbf{x},\{\mathbf{v}^{t} \mid 2\leq t\leq n\})),\] (4)

where \(\mathbf{W}=\{\mathbf{E},\theta\}\) denotes the model parameters, \(\mathcal{D}_{\text{tra}}\) refers to the training set.

### Feature Interaction Selection

Feature interaction selection is crucial in DSNs [11; 7]. In general DSNs, all feature interactions, whether informative or not, are included as inputs for the final predictor. This indiscriminate inclusion of all interactions inevitably introduces noise into the model, which can hinder its performance [11; 7]. Selecting informative feature interactions is called _feature interaction selection_. Proper feature interaction selection can significantly improve the accuracy and efficiency of DSNs by reducing the amount of noise and focusing on learning the most informative interactions [11].

For a \(t\)-th order feature interaction \(\mathbf{v}^{t}=\{\mathbf{v}_{(i_{1},i_{2},\cdots,i_{t})}\},\;1\leq i_{t}\leq m _{i_{t}}\), we aim to determine whether to keep each element or not. We take \(t=2\) as an example to illustrate the problem. The selection can be formulated as learning an gate weight \(\mathbf{a}_{(i_{1},i_{2})}\in\{0,1\}\) corresponding to each \(\mathbf{v}_{(i_{1},i_{2})}\). All \(\mathbf{a}_{(i_{1},i_{2})}\) compose the second-order feature interaction selection tensor \(\mathbf{A}^{2}\), which is formulated as:

\[\mathbf{A}^{2}=\begin{bmatrix}I&\mathbf{a}_{(1,2)}&\mathbf{a}_{(1,3)}&\cdots& \mathbf{a}_{(1,n)}\\ \mathbf{a}_{(1,2)}^{T}&I&\mathbf{a}_{(2,3)}&\cdots&\mathbf{a}_{(2,n)}\\ \mathbf{a}_{(1,3)}^{T}&\mathbf{a}_{(2,3)}^{T}&I&\cdots&\mathbf{a}_{(3,n)}\\ \vdots&\vdots&\vdots&\ddots&\\ \mathbf{a}_{(1,n)}^{T}&\mathbf{a}_{(2,n)}^{T}&\mathbf{a}_{(3,n)}^{T}&\cdots&I \\ \end{bmatrix}.\] (5)Here the tensor is symmetric due to the feature interaction being invariant to perturbation and the diagonal is composed of identical tensors. According to different perspectives in Equation 5, there are two forms for Equation 1: (i) when \(\mathbf{a}_{(i_{1},i_{2})}\) is a scalar, \(\mathbf{A}^{2}\) is a 2-\(D\) matrix which indicates the field level interaction selection \(\mathbf{A}_{f}^{2}\in\{0,1\}^{n^{2}}\). (ii) when \(\mathbf{a}_{(i_{1},i_{2})}\in\{0,1\}^{m_{i_{1}}\times m_{i_{2}}}\) is a tensor, \(A^{2}\) is a 2-\(D\) matrix which indicates the value level interaction selection \(\mathbf{A}_{v}^{2}\in\{0,1\}^{m^{2}}\). Therefore, feature interaction selection tensor \(\mathbf{A}^{t}\) of different order \(t\) compose the selection tensor set \(\mathbf{A}=\{\mathbf{A}^{t}\mid 2\leq t\leq n\}\), which is the learning goal of the _feature interaction selection_ problem.

Field-grainedIn previous methods [7; 11], the feature interaction selection is usually conducted at the field level. This can be formulated as learning the field-grained selection tensor set \(\mathbf{A}=\{\mathbf{A}_{f}^{t}\mid\mathbf{A}_{f}^{t}\in\{0,1\}^{n^{t}},2\leq t \leq n\}\), where \(\mathbf{A}_{f}^{t}\) refers to the field-grained feature interaction selection tensor at \(t\)-th order. Finding a suitable \(\mathbf{A}_{f}^{t}\) is an NP-hard problem [7]. Some works have been devoted to investigating the search strategy. AutoFis [11] intuitively assigns a continuous tensor \(\mathbf{C}_{f}^{t}\in\mathbb{R}^{n^{t}}\) and adopts a sparse optimizer for continuous sparsification so that \(\mathbf{C}_{f}^{t}\) will approximate \(\mathbf{C}_{f}^{t}\) eventually. PROFIT [7], on the other hand, uses symmetric CP decomposition [10] to approximate \(\mathbf{A}_{f}^{t}\) result. This can be formulated as \(\hat{\mathbf{A}}_{f}^{t}\approx\sum_{r=1}^{R}\underbrace{\boldsymbol{\beta}^ {r}\circ\cdots\circ\boldsymbol{\beta}^{r}}_{\text{t times}}\), where \(\circ\) denotes broadcast time operation, \(\boldsymbol{\beta}^{r}\in\mathbb{R}^{1\times n}\) refers to the decomposed vector and \(R\ll n\) is a pre-defined positive integer.

Value-grainedTo extend the feature interaction selection from field-grained to value-grained, we focus on the value-grained selection tensor set \(\mathbf{A}=\{\mathbf{A}_{v}^{t}\mid\mathbf{A}_{v}^{t}\in\{0,1\}^{m^{t}},2 \leq t\leq n\}\). Although this formulation is analogous to the field-level one, it is substantially more complex given \(m\gg n\). This expansion in complexity poses a challenge, referred to as the _size explosion_ problem, as the computational cost and memory usage for the value-level selection increases dramatically. So it is hard to utilize previous methods [11; 7] directly into solving the value-grained selection problem. We will discuss how we tackle this _size explosion_ problem in Section 3.3.1 in detail.

Hybrid-grainedApart from learning the field and value-grained selection tensor \(\mathbf{A}_{f}\) and \(\mathbf{A}_{v}\), the hybrid-grained selection also needs to learn a proper hybrid function, we formulate the hybrid function as a binary selection equation,

\[\mathbf{A}=\{\mathbf{A}^{t}\mid 2\leq t\leq n\},\ \mathbf{A}^{t}=\boldsymbol{ \alpha}^{t}\mathbf{A}_{f}^{t}+(1-\boldsymbol{\alpha}^{t})\mathbf{A}_{v}^{t}\] (6)

where tensor \(\boldsymbol{\alpha}^{t}\in\{0,1\}^{n^{t}}\) denoting the hybrid choice. As the quality of the field and value-grained selection tensors will heavily influence the hybrid tensor \(\boldsymbol{\alpha}^{t}\), joint optimization of both selection tensors \(\mathbf{A}_{f}\) & \(\mathbf{A}_{v}\) and hybrid tensor \(\boldsymbol{\alpha}\) is highly desired. We will discuss how we tackle this _joint training_ problem in Section 3.3.2 in detail.

Previous field-grained and value-grained selection can be viewed as a special case for Equation 6. By setting the hybrid tensor \(\boldsymbol{\alpha}\) in Equation 6 as an all-one tensor \(\mathbf{1}\), the original hybrid-grained selection problem is reduced to a field-grained one. Similarly, the value-grained selection problem can also be obtained from Equation 6 by assigning the hybrid tensor \(\boldsymbol{\alpha}\) as an all-zero tensor \(\mathbf{0}\).

### OptFeature

In this section, we formalize our proposed method **OptFeature** aiming to tackle the hybrid-grained feature interaction selection problem depicted in Section 3.2. Such a problem contains two critical issues:

* _Size explosion_ problem: As the number of feature values \(m\) is significantly larger than the number of feature fields \(n\), denoted as \(m\gg n\), the value-grained selection tensor \(\mathbf{A}_{v}\) increase dramatically and is hard to be stored or explored. To provide a concrete example regarding the size explosion, we take the commonly used benchmark Criteo2 as an example. This dataset contains 39 feature fields and \(6.8\times 10^{6}\) feature values. Even if we only consider \(t=2\) for the second-order feature interactions, the corresponding selection tensor increase from \(\|\mathbf{A}_{f}^{2}\|=39^{2}=1521\) to \(6.8\times{10^{6}}^{2}=4.6\times{10^{13}}\), making it impossible to be stored or explored using vanilla NAS approaches [12, 31, 20] or continuous sparsification methods [1, 6].
* _Joint training_ problem: As the quality of the field and value-grained selection tensors \(\mathbf{A}_{f}^{t}\) & \(\mathbf{A}_{v}^{t}\) will heavily influence the hybrid tensor \(\boldsymbol{\alpha}\), jointly optimization of both selection tensors and hybrid tensor is required. How to efficiently optimize so many large binary tensors remains unsolved.

To address the two critical issues mentioned in Section 3.2 and above, we propose our method **OptFeature** with 1) the _selection tensor decomposition_ to address the _size explosion_ issue and 2) _sparsification-based selection algorithm_ for the _joint training_ problem.

#### 3.3.1 Selection Tensor Decomposition

To efficiently explore a large space \(\mathbf{A}_{v}\), we first decompose the feature interaction selection tensor. Without the loss of generality, we only focus on the second order, i.e. \(t=2\), value-grained feature interaction selection tensor \(\mathbf{A}_{v}^{2}\). More details of the general cases \(t\geq 2\) are listed in the Appendix A. We omit the superscript for simplicity and use \(\mathbf{A}_{v}\) instead of \(\mathbf{A}_{v}^{2}\) in later sections. Given that the selection tensor is semi-positive and symmetric, we have the Takagi factorization [2] as:

\[\mathbf{A}_{v}\approx\mathcal{U}\times\mathbf{\Sigma}_{v}\times\mathcal{U}^{ T}.\] (7)

Here \(\mathbf{\Sigma}_{v}\) is a \(d^{{}^{\prime}}\times d^{{}^{\prime}}\) diagonal tensor, \(\mathcal{U}\in R^{m\times d^{{}^{\prime}}}\) and \(d^{{}^{\prime}}<m\). To further reduce the consumption of memory and reduce factorization error, we introduce the deep multi-mode tensor factorization [5] that replaces the \(\mathcal{U}\) as an output of one neural network, denoted as:

\[\mathcal{U}\approx f_{\theta_{v}}(\mathbf{E}_{v}),\] (8)

where \(f_{\theta_{v}}:\mathbb{R}^{m\times\hat{d}}\rightarrow\mathbb{R}^{m\times d^{ {}^{\prime}}},\ \hat{d}\ll d^{{}^{\prime}}\) is a neural network with parameter \(\theta_{v}\) and \(\mathbf{E}_{v}\in\mathbb{R}^{m\times\hat{d}}\) is an additional embedding table for generating feature interaction selection tensor. Figure 2 shows a detailed illustration of our decomposition.

Notably, during the training and inference stage, we do not need to calculate the entire metric \(\mathbf{A}\) all in once. We only need to calculate a batch of data in each trial. The element of architecture metric \(\mathbf{A}_{v}[k_{i},k_{j}]\) can be calculated given the following equation:

\[\mathbf{A}_{v}[k_{i},k_{j}]=f_{\theta_{v}}(\mathbf{E}_{v}[k_{i},:])\times \mathbf{\Sigma}_{v}\times f_{\theta_{v}}^{T}(\mathbf{E}_{v}[k_{j},:]).\] (9)

The original value-grained selection tensor \(\mathbf{A}_{v}\) consists of \(\mathcal{O}(m^{2})\) elements. By introducing the Takagi factorization [2], the number of trainable elements is reduced to \(\mathcal{O}(md^{{}^{\prime}})\). We further reduce that number to \(\mathcal{O}(\hat{d}(m+d^{{}^{\prime}}))\) through the multi-mode tensor factorization [5]. Hence, we can train the neural network over the value-level selection on modern hardware.

#### 3.3.2 Sparsification-based Selection Algorithm

After decomposing the selection tensor, we still need to jointly conduct feature interaction selection and train the model parameters for an efficient search. To help convert the continuous feature selection tensor into an accurate binary selection, we adopt the straight-through estimator(STE) function [1] for continuous sparsification. The STE can be formulated as a customized function \(\mathcal{S}(\cdot)\), with its forward pass as a unit step function

\[\mathcal{S}(x)=\begin{cases}0,&x\leq 0\\ 1,&x>0\end{cases},\] (10)

and backward pass as \(\frac{d}{dx}\mathcal{S}(x)=1\), meaning that it will directly pass the gradient backward. Therefore, we can mimic a discrete feature interaction selection while providing valid gradient information for the value-level selection parameters \(\mathbf{E}_{v}\) & \(\theta_{v}\), making the whole process trainable. Hence, Equation 9 is re-written as:

\[\mathbf{A}_{v}[k_{i},k_{j}]=S(f_{\theta_{v}}(\mathbf{E}_{v}[k_{i},:])\times \mathbf{\Sigma}_{v}\times f_{\theta_{v}}^{T}(\mathbf{E}_{v}[k_{j},:])).\] (11)

Figure 2: Illustration of the selection tensor decomposition

Similar to Equation 11, we can obtain the field-level selection matrix \(\mathbf{A}_{f}\) using the following equation:

\[\mathbf{A}_{f}[i,j]=S(g_{\theta_{f}}(\mathbf{E}_{f}[i,:])\times\mathbf{\Sigma}_ {f}\times g_{\theta_{f}}^{T}(\mathbf{E}_{f}[j,:])),\] (12)

where \(\mathbf{E}_{f}\in\mathbb{R}^{n\times d}\) is the field-level embedding table for feature interaction selection and \(g_{\theta_{f}}:\mathbb{R}^{n\times d}\rightarrow\mathbb{R}^{n\times d^{{}^{ \prime}}}\) is another neural network parameterized by \(\theta_{f}\).

After obtaining the value-level selection matrix \(\mathbf{A}_{v}\) and field-level selection matrix \(\mathbf{A}_{f}\), we need to merge them and obtain the hybrid selection result \(\mathbf{A}\). Inspired by DARTS [12] and its success in previous works [11; 15; 13], we relax the hybrid tensor \(\alpha\) into a continuous tensor \(\alpha_{c}\in\mathbb{R}^{n^{\prime}}\), which can be trained jointly with other parameters via gradient descent. To ensure convergence, we apply the sigmoid function \(\sigma(\cdot)\) over \(\alpha_{c}\). Hence, during training time, Equation 6 can be re-written as:

\[\mathbf{A}=\sigma(\alpha_{c})\cdot\mathbf{A}_{f}+(1-\sigma(\alpha_{c}))\cdot \mathbf{A}_{v}.\] (13)

Hence, the final search objective can be formulated as:

\[\mathbf{\hat{W}}^{*}=\min_{\mathbf{\hat{W}}}\;\mathcal{L}(\mathcal{D}_{\text {tra}}|\{\mathbf{W}^{*},\mathbf{\hat{W}}\}),\;\text{s.t.}\;\mathbf{W}^{*}= \min_{\mathbf{W}}\;\mathcal{L}(\mathcal{D}_{\text{val}}|\{\mathbf{W},\mathbf{ \hat{W}}^{*}\}).\] (14)

Here \(\mathbf{\hat{W}}=\{\mathbf{E}_{v},\theta_{v},\mathbf{E}_{f},\theta_{f}, \boldsymbol{\alpha}\}\) denoting the parameter required in the interaction selection process, and \(\mathcal{D}_{\text{val}}\) denotes the validation set. We summarize the overall process of our _sparsification-based selection algorithm_ in Algorithm 1.

```
0: training and validation set \(\mathcal{D}_{\text{tra}}\) and \(\mathcal{D}_{\text{val}}\)
0: main model parameters \(\mathbf{W}\) and feature interaction selection parameters \(\mathbf{\hat{W}}\)
1:for epoch = 1, \(\cdots\), T do
2:while epoch not finished do
3: Sample mini-batch \(\mathcal{B}_{\text{val}}\) and \(\mathcal{B}_{\text{tra}}\) from validation set \(\mathcal{D}_{\text{tra}}\) and training set \(\mathcal{D}_{\text{val}}\)
4: Update selection parameter \(\mathbf{\hat{W}}\) using gradients \(\nabla_{\mathbf{\hat{W}}}\mathcal{L}(\mathcal{B}_{\text{val}}|\{\mathbf{W}, \mathbf{\hat{W}}\})\)
5: Update model parameter \(\mathbf{W}\) using gradients \(\nabla_{\mathbf{W}}\mathcal{L}(\mathcal{B}_{\text{tra}}|\{\mathbf{W},\mathbf{ \hat{W}}\})\)
6:endwhile
7:endfor ```

**Algorithm 1** Sparsification-based Selection Algorithm

#### 3.3.3 Retraining with Optimal Selection Result

During the re-training stage, the optimal hybrid tensor is determined as \(\alpha^{*}=\mathbbm{1}_{\alpha_{c}>0}\) following previous works [12; 15; 13]. Hence, we can obtain the optimal selection result

\[\mathbf{A}^{*}\approx\alpha^{*}\cdot S(g_{\theta_{f}^{*}}(\mathbf{E}_{f}^{*}) \times\mathbf{\Sigma}_{f}^{*}\times g_{\theta_{f}^{*}}^{T}(\mathbf{E}_{f}^{*}) )+(1-\alpha^{*})\cdot S(f_{\theta_{v}^{*}}(\mathbf{E}_{v}^{*})\times\mathbf{ \Sigma}_{v}^{*}\times f_{\theta_{v}^{*}}^{T}(\mathbf{E}_{v}^{*})).\] (15)

We freeze the selection parameters \(\mathbf{\hat{W}}\) and retrain the model from scratch following the existing works [12; 7]. This reason for introducing the retraining stage is to remove the influence of sub-optimal selection results during the selection process over the model parameter. Also,

## 4 Experiment

### Experiment Setup

DatasetsTo validate the effectiveness of our proposed method OptFeature, we conduct experiments on three benchmark datasets (Criteo, Avazu3, and KDD124), which are widely used in previous work on deep sparse networks for evaluation purposes [7; 11]. The dataset statistics and preprocessing details are described in the Appendix B.

Baseline ModelsWe compare our OptFeature with previous works in deep sparse networks. We choose the methods that are widely used and open-sourced:

* Shallow network: Logistic Regression(LR) [22] and Factorization Machine(FM) [21].
* DSNs: FNN [27], DeepFM [8], DCN [24], IPNN [18].
* DSNs with feature interaction selection(DSNs with FIS): AutoFIS5[11] and PROFIT6[7]. Footnote 5: https://github.com/zhuchenxv/AutoFIS

Footnote 6: https://github.com/NeurIPS-AutoDSN/NeurIPS-AutoDSN

Footnote 7: https://github.com/rixwew/pytorch-fm

For the shallow networks and deep sparse networks, we implement them following the commonly used library torchfm 7. These baselines and our OptFeature are available here8. For AutoFIS and PROFIT, we re-use their original implementations, respectively.

Footnote 8: https://github.com/fuyuanlyu/OptFeature

Evaluation MetricsFollowing the previous works[8; 30], we adopt the commonly-used performance evaluation metrics for click-through rate prediction, **AUC** (Area Under ROC curve) and **Log loss** (also known as cross-entropy). As for efficiency evaluation, we also use two standard metrics, inference time and model size, which are widely adopted in previous works of deep sparse network [7; 11].

Parameter SetupTo ensure the reproducibility of experimental results, here we further introduce the implementation setting in detail. We implement our methods using PyTorch. We adopt the Adam optimizer with a mini-batch size of 4096. We set the embedding sizes to 16 in all the models. We set the predictor as an MLP model with [1024, 512, 256] for all methods. All the hyper-parameters are tuned on the validation set with a learning rate from [1e-3, 3e-4, 1e-4, 3e-5, 1e-5] and weight decay from [1e-4, 3e-5, 1e-5, 3e-6, 1e-6]. We also tune the learning ratio for the feature interaction selection parameters from [1e-4, 3e-5, 1e-5, 3e-6, 1e-6] and while weight decay from [1e-4, 3e-5, 1e-5, 3e-6, 1e-6, 0]. The initialization parameters for the retraining stage are selected from the best-performed model parameters and randomly initialized ones.

Model StabilityTo make the results more reliable, we ran the repetitive experiments with different random seeds five times and reported the average value for each result.

Hardware PlatformAll experiments are conducted on a Linux server with one Nvidia-Tesla V100-PCIe-32GB GPU, 128GB main memory and 8 Intel(R) Xeon(R) Gold 6140 CPU cores.

### Benchmark Comparison

Model PerformanceWe present the model performance across three benchmark datasets in Table 1 and make the following observations. Firstly, OptFeature consistently outperforms other models on all three benchmark datasets, validating the effectiveness of our hybrid-grained selection space and selection algorithm. Secondly, DSNs generally yield better performances than shallow models, underscoring the importance of designing powerful sparse networks. Thirdly, DSNs with feature interaction selection tend to perform superiorly compared to those incorporating all possible interactions. This confirms the significance of conducting feature interaction selection within DSNs. Lastly, methods that decompose the selection space (e.g., OptFeature and PROFIT) consistently outperform AutoFIS, which directly navigates the original selection space.

Model EfficiencyWe also evaluate model efficiency and present the results in Figure 3. We can observe that OptFeature outperforms all other methods in terms of inference time, while its model size is comparable to other feature interaction selection methods. Both model accuracy and inference efficiency are critical factors when deploying DSNs. Our OptFeature not only achieves the best performance but also reduces inference time, indicating its practical value.

### Investigating the Selection Process

In this section, we delve into the selection process of OptFeature. We introduce two variants of OptFeature for this purpose: (i) OptFeature-f, which carries out field-level interaction selection, and (ii) OptFeature-v, which conducts value-level interaction selection. Both methods adopt the same selection algorithm outlined in Section 3.3.2. We will compare OptFeature and its two variants to other DSNs interaction selection baselines regarding effectiveness and efficiency.

Investigating the Selection EffectivenessWe evaluate the effectiveness of various DSN feature interaction methods in terms of comparable performance. Several observations can be made from the results presented in Table 2. Firstly, OptFeature consistently delivers superior results on all three datasets. This confirms that the hybrid-grained selection space allows OptFeature to explore finer-grained while the selection algorithm effectively performs based on the decomposition. Secondly, the value-level selection method consistently surpasses field-level selection, suggesting that field-level selection might be too coarse-grained to leverage informative values within uninformative fields. Lastly, OptFeature-f outperforms both AutoFIS and PROFIT in terms of model performance.

\begin{table}
\begin{tabular}{c|c|c c|c c|c c} \hline \multicolumn{2}{c|}{Dataset} & \multicolumn{2}{c|}{Criteo} & \multicolumn{2}{c|}{Avazu} & \multicolumn{2}{c}{KDD12} \\ \hline Category & Model & AUC & Logloss & AUC & Logloss & AUC & Logloss \\ \hline \multirow{2}{*}{Shallow} & LR & 0.7882 & 0.4609 & 0.7563 & 0.3928 & 0.7411 & 0.1637 \\  & FM & 0.8047 & 0.4464 & 0.7839 & 0.3783 & 0.7786 & 0.1566 \\ \hline \multirow{4}{*}{DSNs} & FNN & 0.8101 & 0.4414 & 0.7891 & 0.3762 & 0.7947 & 0.1536 \\  & DeepFM & 0.8097 & 0.4418 & 0.7896 & 0.3757 & 0.7969 & 0.1531 \\  & DCN & 0.8096 & 0.4419 & 0.7887 & 0.3767 & 0.7955 & 0.1534 \\  & IPNN & 0.8103 & 0.4413 & 0.7896 & 0.3741 & 0.7945 & 0.1537 \\ \hline \multirow{4}{*}{DSNs with FIS} & AutoFIS & 0.8089 & 0.4428 & 0.7903 & 0.3749 & 0.7959 & 0.1533 \\  & PROFIT & 0.8112 & 0.4406 & 0.7906 & 0.3756 & 0.7964 & 0.1533 \\ \cline{1-1} \cline{2-7}  & OptFeature & **0.8116** & **0.4402** & **0.7925\({}^{*}\)** & **0.3741\({}^{*}\)** & **0.7982\({}^{*}\)** & **0.1529\({}^{*}\)** \\ \hline \end{tabular} Here \({}^{*}\) denotes statistically significant improvement (measured by a two-sided t-test with p-value \(<0.05\)) over the best baseline. The best and second best performed results are marked in **bold** and underline format

\end{table}
Table 1: Overall Performance Comparison

\begin{table}
\begin{tabular}{c|c c c|c c|c c} \hline \multicolumn{2}{c|}{Dataset} & \multicolumn{2}{c|}{Criteo} & \multicolumn{2}{c|}{Avazu} & \multicolumn{2}{c}{KDD12} \\ \hline Model & Granularity & AUC & Logloss & AUC & Logloss & AUC & Logloss \\ \hline AutoFIS & Field & 0.8089 & 0.4428 & 0.7903 & 0.3749 & 0.7959 & 0.1533 \\ PROFIT & Field & 0.8112 & 0.4406 & 0.7906 & 0.3756 & 0.7964 & 0.1533 \\ \hline OptFeature-f & Field & 0.8115 & 0.4404 & 0.7920 & 0.3744 & 0.7978 & 0.1530 \\ OptFeature-v & Value & 0.8116 & 0.4403 & 0.7920 & 0.3742 & 0.7981 & 0.1529 \\ OptFeature & Hybrid & 0.8116 & 0.4402 & 0.7925 & 0.3741 & 0.7982 & 0.1529 \\ \hline \end{tabular}
\end{table}
Table 2: Performance Comparison over Different Granularity.

Figure 3: Efficiency comparison between OptFeature and DSN baselines. Model size excludes the embedding layer. Inference time refers to the average inference time per batch across validation set.

The distinguishing factor among these three methods lies in the selection algorithm. AutoFIS directly optimizes the gating vector for each interaction field, rendering the space too large for efficient exploration. PROFIT, on the other hand, adopts a progressive selection algorithm, leading to sub-optimal interaction selection. This finding further emphasizes the superiority of our sparsification-based selection algorithm in selecting informative interactions.

Investigating the Selection EfficiencyNext, we perform a comparison study on the efficiency of the selection methods. In Figure 4, we report the search cost, measured in GPU hours. The search cost reflects the GPU resource consumption when selecting interactions from scratch. It's important to note that the cost of re-training is excluded as our focus is on the selection process. We observe that the search cost of OptFeature is lower than other baselines on the Criteo and KDD12 datasets. OptFeature surpasses the other two variants as it typically converges in fewer epochs. These comparisons underscore the feasibility of our proposed method. However, on the Avazu dataset, OptFeature is slightly outperformed by its two variants, i.e., OptFeature-f and OptFeature-v, because this dataset usually converges in one epoch [28]. As a result, all three methods converge within the same epoch. Furthermore, compared to AutoFIS and PROFIT, OptFeature-f incurs a lower cost with the same complexity of selection space. This observation further highlights the superiority of our selection algorithm in terms of convergence speed.

## 5 Conclusions and Limitations

In this work, we introduce a hybrid-grained selection approach targeting both feature field and feature value level. To explore the vast search space that extends from field to value, we propose a decomposition method to reduce space complexity and make it trainable. We then developed a selection algorithm named OptFeature that simultaneously selects interactions from field and value perspectives. Experiments conducted on three real-world benchmark datasets validate the effectiveness and efficiency of our method. Further ablation studies regarding search efficiency and selection granularity illustrate the superiority of our proposed OptFeature.

LimitationsDespite OptFeature demonstrating superior performance over other baselines on model performance and inference time, it requires a larger model size than certain DSNs [8; 27]. Additionally, it lacks online experiments to validate its effectiveness in more complex and dynamic scenarios.

## References

* [1] Yoshua Bengio, Nicholas Leonard, and Aaron C. Courville. Estimating or propagating gradients through stochastic neurons for conditional computation. _CoRR_, abs/1308.3432, 2013.
* [2] Alexander M. Chebotarev and Alexander Evgen'evich Teretenkov. Singular value decomposition for the takagi factorization of symmetric matrices. _Appl. Math. Comput._, 234:380-384, 2014.
* [3] Jianbo Chen, Le Song, Martin J. Wainwright, and Michael I. Jordan. Learning to explain: An information-theoretic perspective on model interpretation. In _Proceedings of the 35th International Conference on Machine Learning, ICML 2018_, volume 80 of _Proceedings of Machine Learning Research_, pages 882-891, Stockholmsmassan, Stockholm, Sweden, 2018. PMLR.
* [4] James Enouen and Yan Liu. Sparse interaction additive networks via feature interaction detection and sparse selection. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, _Advances in Neural Information Processing Systems_, volume 35, pages 13908-13920. Curran Associates, Inc., 2022.

Figure 4: Comparisons between OptFeature and other DSNs interaction selection baselines on the search cost.

* [5] Jicong Fan. Multi-mode deep matrix and tensor factorization. In _The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022_. OpenReview.net, 2022.
* [6] Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural networks. In _7th International Conference on Learning Representations, ICLR 2019_, New Orleans, LA, USA, 2019. OpenReview.net.
* [7] Chen Gao, Yinfeng Li, Quanming Yao, Depeng Jin, and Yong Li. Progressive feature interaction search for deep sparse network. In Marc'Aurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan, editors, _Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual_, pages 392-403, 2021.
* [8] Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. Deepfm: A factorization-machine based neural network for CTR prediction. In _Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence, IJCAI 2017_, pages 1725-1731, Melbourne, Australia, 2017. ijcai.org.
* [9] Farhan Khawar, Xu Hang, Ruiming Tang, Bin Liu, Zhenguo Li, and Xiuqiang He. Autofeature: Searching for feature interactions and their architectures for click-through rate prediction. In _CIKM_, 2020.
* [10] Tamara G. Kolda and Brett W. Bader. Tensor decompositions and applications. _SIAM Rev._, 51(3):455-500, 2009.
* [11] Bin Liu, Chenxu Zhu, Guilin Li, Weinan Zhang, Jincai Lai, Ruiming Tang, Xiuqiang He, Zhenguo Li, and Yong Yu. Autofis: Automatic feature interaction selection in factorization models for click-through rate prediction. In _KDD_, 2020.
* [12] Hanxiao Liu, Karen Simonyan, and Yiming Yang. DARTS: differentiable architecture search. In _7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019_. OpenReview.net, 2019.
* [13] Yaoxun Liu, Liangli Ma, and Muyuan Wang. GAIN: A gated adaptive feature interaction network for click-through rate prediction. _Sensors_, 22(19):7280, 2022.
* [14] Renqian Luo, Fei Tian, Tao Qin, Enhong Chen, and Tie-Yan Liu. Neural architecture optimization. In Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kristen Grauman, Nicolo Cesa-Bianchi, and Roman Garnett, editors, _Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montreal, Canada_, pages 7827-7838, 2018.
* [15] Fuyuan Lyu, Xing Tang, Huifeng Guo, Ruiming Tang, Xiuqiang He, Rui Zhang, and Xue Liu. Memorize, factorize, or be naive: Learning optimal feature interaction methods for CTR prediction. In _38th IEEE International Conference on Data Engineering, ICDE 2022, Kuala Lumpur, Malaysia, May 9-12, 2022_, pages 1450-1462. IEEE, 2022.
* 4 May 2023_, pages 3386-3395. ACM, 2023.
* [17] Fuyuan Lyu, Xing Tang, Hong Zhu, Huifeng Guo, Yingxue Zhang, Ruiming Tang, and Xue Liu. Optembed: Learning optimal embedding table for click-through rate prediction. In Mohammad Al Hasan and Li Xiong, editors, _Proceedings of the 31st ACM International Conference on Information & Knowledge Management, Atlanta, GA, USA, October 17-21, 2022_, pages 1399-1409. ACM, 2022.
* [18] Yanru Qu, Han Cai, Kan Ren, Weinan Zhang, Yong Yu, Ying Wen, and Jun Wang. Product-based neural networks for user response prediction. In _IEEE 16th International Conference on Data Mining, ICDM 2016_, pages 1149-1154, Barcelona, Spain, 2016. IEEE Computer Society.

* [19] Yanru Qu, Bohui Fang, Weinan Zhang, Ruiming Tang, Minzhe Niu, Huifeng Guo, Yong Yu, and Xiuqiang He. Product-based neural networks for user response prediction over multi-field categorical data. _ACM Trans. Inf. Syst._, 37(1):5:1-5:35, 2019.
* [20] Esteban Real, Sherry Moore, Andrew Selle, Saurabh Saxena, Yutaka I. Leon-Suematsu, Jie Tan, Quoc V. Le, and Alexey Kurakin. Large-scale evolution of image classifiers. In Doina Precup and Yee Whye Teh, editors, _Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017_, volume 70 of _Proceedings of Machine Learning Research_, pages 2902-2911. PMLR, 2017.
* [21] Steffen Rendle. Factorization machines. In _ICDM 2010, The 10th IEEE International Conference on Data Mining_, pages 995-1000, Sydney, Australia, 2010. IEEE Computer Society.
* [22] Matthew Richardson, Ewa Dominowska, and Robert Ragno. Predicting clicks: estimating the click-through rate for new ads. In _Proceedings of the 16th International Conference on World Wide Web, WWW 2007, Banff, Alberta, Canada, May 8-12, 2007_, pages 521-530, New York, NY, United State, 2007. ACM.
* [23] David So, Quoc Le, and Chen Liang. The evolved transformer. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, _Proceedings of the 36th International Conference on Machine Learning_, volume 97 of _Proceedings of Machine Learning Research_, pages 5877-5886. PMLR, 09-15 Jun 2019.
* 17, 2017_, pages 12:1-12:7. ACM, 2017.
* [25] Ruoxi Wang, Rakesh Shivanna, Derek Zhiyuan Cheng, Sagar Jain, Dong Lin, Lichan Hong, and Ed Chi. DCN V2: improved deep & cross network and practical lessons for web-scale learning to rank systems. In _WWW_, 2021.
* [26] Zhikun Wei, Xin Wang, and Wenwu Zhu. Autoias: Automatic integrated architecture searcher for click-trough rate prediction. In _CIKM '21: The 30th ACM International Conference on Information and Knowledge Management_, pages 2101-2110, Virtual Event, Queensland, Australia, 2021. ACM.

- A case study on user response prediction. In _Advances in Information Retrieval
- 38th European Conference on IR Research, ECIR 2016_, volume 9626, pages 45-57. Springer, 2016.
* [28] Zhao-Yu Zhang, Xiang-Rong Sheng, Yujing Zhang, Biye Jiang, Shuguang Han, Hongbo Deng, and Bo Zheng. Towards understanding the overfitting phenomenon of deep click-through rate models. In _Proceedings of the 31st ACM International Conference on Information & Knowledge Management_, CIKM '22, page 2671-2680, New York, NY, USA, 2022. Association for Computing Machinery.
* [29] Guanghui Zhu, Feng Cheng, Defu Lian, Chunfeng Yuan, and Yihua Huang. NAS-CTR: efficient neural architecture search for click-through rate prediction. In _SIGIR '22: The 45th International ACM SIGIR Conference on Research and Development in Information Retrieval_, pages 332-342, Madrid, Spain, 2022. ACM.
* 5, 2021_, pages 2759-2769. ACM, 2021.
* [31] Barret Zoph and Quoc V. Le. Neural architecture search with reinforcement learning. In _5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings_. OpenReview.net, 2017.

General Form of Selection Tensor Decomposition

In this section, we further extend the selection tensor decomposition in Section 3.3.1 from a special case, where \(t=2\), to a more general case, where \(2\leq t\leq n\). The interaction selection tensor \(\mathbf{A}_{v}^{t}\) for \(t\)-th order features is also semi-positive and symmetric. By extending the Takagi factorization [2], we have:

\[\mathbf{A}_{v}^{t}=\mathbf{\Sigma}_{v}\times_{1}\mathcal{U}\times_{2}\mathcal{ U}\times_{3}\cdots\times_{t}\mathcal{U},\] (16)

where \(\mathbf{\Sigma}_{v}\) is a \(\underbrace{d^{{}^{\prime}}\times\cdots\times d^{{}^{\prime}}}_{\text{ times}}\) diagonal tensor, \(\times_{t}\) denotes the \(t\)-mode matrix multiplication [5], \(\mathcal{U}\in R^{m\times d^{{}^{\prime}}}\) and \(d^{{}^{\prime}}<m\). Similar to the special case where \(t=2\), we adopt multi-mode tensor factorization [5] to replace \(\mathcal{U}\) as an output of a neural network, denoted as:

\[\mathcal{U}\approx f_{\hat{\theta}}(\mathbf{E}_{v}),\] (17)

where \(f_{\hat{\theta}}:\mathbb{R}^{m\times\hat{d}}\rightarrow\mathbb{R}^{m\times d ^{{}^{\prime}}},\;\hat{d}\ll d^{{}^{\prime}}\) is a neural network with parameter \(\hat{\theta}\) and \(\hat{\mathbf{E}}\in\mathbb{R}^{m\times\hat{d}}\) is an additional embedding table for generating feature interaction selection tensor. The element of architecture metric \(\mathbf{A}_{v}^{t}[k_{i_{1}},\cdots,k_{i_{t}}]\) can be calculated given the following equation:

\[\mathbf{A}_{v}^{t}[k_{i_{1}},\cdots,k_{i_{t}}]=\mathbf{\Sigma}_{v}\times_{1}f _{\theta_{v}}(\mathbf{E}_{v}[k_{i_{1}},:])\times_{2}\cdots\times_{t}f_{\theta _{v}}(\mathbf{E}_{v}[k_{i_{t}},:]).\] (18)

The original value-grained selection tensor \(\mathbf{A}_{v}^{t}\) consists of \(\mathcal{O}(m^{t})\) elements. The trainable elements is reduced to \(\mathcal{O}(md^{{}^{\prime}})\) after the Takagi factorization [2] and to \(\mathcal{O}(\hat{d}(m+d^{{}^{\prime}}))\) after the multi-mode tensor factorization [5].

## Appendix B Dataset and Preprocessing

We conduct our experiments on two public real-world benchmark datasets. The statistics of all datasets are given in Table 3. We describe all these datasets and the pre-processing steps below.

**Criteo** dataset consists of ad click data over a week. It consists of 26 categorical feature fields and 13 numerical feature fields. Following the best practice [30], we discretize each numeric value \(x\) to \(\lfloor\log^{2}(x)\rfloor\), if \(x>2\); \(x=1\) otherwise. We replace infrequent categorical features with a default "OOV" (i.e. out-of-vocabulary) token, with _min_count_=2.

**Avazu** dataset contains 10 days of click logs. It has 24 fields with categorical features. Following the best practice [30], we remove the _instance_id_ field and transform the _timestamp_ field into three new fields: _hour_, _weekday_ and _is_weekend_. We replace infrequent categorical features with the "OOV" token, with _min_count_=2.

**KDD12** dataset contains training instances derived from search session logs. It has 11 categorical fields, and the click field is the number of times the user clicks the ad. We replace infrequent features with an "OOV" token, with _min_count_=10.

**Industrial** dataset is a private large-scale commercial dataset. This dataset contains nearly 3.5 million samples with 134 feature fields and 2498 feature values. Please notice that this dataset has more feature fields and fewer feature values, which differs from the previous benchmarks with fewer feature fields and larger feature values. The results of this dataset are shown in Appendix C.5.

\begin{table}
\begin{tabular}{c|c c c c} \hline Dataset & \#samples & \#field & \#value & pos ratio \\ \hline Criteo & \(4.6\times 10^{7}\) & 39 & \(6.8\times 10^{6}\) & 0.2562 \\ Avazu & \(4.0\times 10^{7}\) & 24 & \(4.4\times 10^{6}\) & 0.1698 \\ KDD12 & \(1.5\times 10^{8}\) & 11 & \(6.0\times 10^{6}\) & 0.0445 \\ Industrial & \(3.0\times 10^{8}\) & 134 & 2498 & 0.1220 \\ \hline \end{tabular} Note: _#samples_ refers to the total samples in the dataset, _#field_ refers to the number of feature fields for original features, _@value_ refers to the number of feature values for original features, _pos ratio_ refers to the positive ratio.

\end{table}
Table 3: Dataset StatisticsAblation Study

### Feature Interaction Operation

In this section, we conduct an ablation study on the feature interaction operation, comparing the performance of the default setting, which uses the _inner product_, with the _outer product_ operation. We evaluate these operations on _OptFeature_ and its two variants: _OptFeature-\(f\)_ and _OptFeature-v_. The results are summarized in Table 4.

From the table, we observe that the _inner product_ operation outperforms the _outer product_ operation. This performance gap is particularly significant on the Avazu and KDD12 datasets, while it is relatively insignificant on the Criteo dataset. The drop in performance with the _outer product_ operation is likely due to the introduction of a significantly larger number of inputs into the final predictor. This makes it more challenging for the predictor to effectively balance the information from raw inputs and feature interactions.

### Dimension Selection

In this section, we perform an ablation study on the feature interaction selection dimension \(\hat{d}\). We compare the AUC performance with the corresponding dimension \(\hat{d}\) and present the results in Figure 5. From the figure, we can observe that as the dimension \(\hat{d}\) increases, the AUC performance remains relatively consistent over the Criteo dataset. This suggests that it is relatively easy to distinguish value-level selection on the Criteo dataset. However, on the Avazu and KDD12 datasets, the AUC performance improves as the selection dimension \(\hat{d}\) increases. This indicates that distinguishing informative values is comparatively more challenging on these two datasets.

### Higher-Order Feature Interactions

In this section, we investigate the influence of higher-order feature interactions over the final results on the KDD12 dataset. We compare the default setting which only considers second-order interactions with two other settings: (i) only third-order interactions and (ii) both second and third-order interactions. We visualize the result in Figure 6.

From the figure, we can draw the following observations. First, only considering third-order interactions leads to the worst performance. This aligns with the common understanding that second-order interactions are typically considered the most informative in deep sparse prediction [15]. Second, for field-level selection, the performance improves when both second and third-order interactions are incorporated into the model. This finding is consistent with previous studies [11; 7], as the inclusion of additional interactions introduces extra information that enhances the performance. In contrast, for value-level selection, the performance tends to decrease when both second and third-order interactions are included. This could be attributed to the fact that value-level selection operates at a finer-grained level and might be more challenging to optimize directly. Finally, OptFeature constantly outperforms its two variants over all settings. This indicates the feasibility and effectiveness of hybrid-grained selection, which combines both field-level and value-level interactions.

\begin{table}
\begin{tabular}{c|c|c c|c c|c c} \hline \multicolumn{2}{c|}{Dataset} & \multicolumn{2}{c|}{Criteo} & \multicolumn{2}{c|}{Avazu} & \multicolumn{2}{c}{KDD12} \\ \hline Category & Model & AUC & Logloss & AUC & Logloss & AUC & Logloss \\ \hline \multirow{3}{*}{inner product} & OptFeature-f & 0.8115 & 0.4404 & 0.7920 & 0.3744 & 0.7978 & 0.1530 \\  & OptFeature-v & 0.8116 & 0.4403 & 0.7920 & 0.3742 & 0.7981 & 0.1529 \\  & OptFeature & 0.8116 & 0.4402 & 0.7925 & 0.3741 & 0.7982 & 0.1529 \\ \hline \multirow{3}{*}{outer product} & OptFeature-f & 0.8114 & 0.4404 & 0.7896 & 0.3760 & 0.7957 & 0.1535 \\  & OptFeature-v & 0.8113 & 0.4405 & 0.7902 & 0.3752 & 0.7961 & 0.1533 \\ \cline{1-1}  & OptFeature & 0.8115 & 0.4403 & 0.7899 & 0.3753 & 0.7961 & 0.1533 \\ \hline \end{tabular}
\end{table}
Table 4: Performance Comparison over Different Feature Interaction Operation.

### Selection Visualization

In this section, we present the visualization of the interaction selection results for OptFeature and its two variants in Figure 7. OptFeature-f performs a binary selection for each interaction field, allowing for easy visualization through a heatmap representation where one indicates keep and zero indicates drop. On the other hand, OptFeature-v and OptFeature involve value-level interaction selection. Hence, we visualize them by setting each element as the percentage of being selected over the training set. The detailed equation for calculating the value for the interaction field (i, j) is shown in Equation 19.

\[\textbf{P}_{(i,j)}=\frac{\#\text{Samples keeping interaction field (i, j)}}{\#\text{Training Samples}}\] (19)

From the visualization, we can observe that OptFeature acts as a hybrid approach, exhibiting a combination of both field-level and value-level interactions. Interestingly, we note significant differences between certain interaction fields in the KDD12 and Avazu datasets. OptFeature-f retains all of its interactions, while OptFeature-v only keeps a proportion of the value-level interactions. This observation further emphasizes the importance of exploring interactions at a finer-grained level.

Figure 6: Performance Comparison over Different Feature Interaction Orders.

Figure 7: Visualization of the Feature Interaction Selection Results.

### Experiment on Industrial Dataset

We explicitly conduct experiments on an industrial large-scale dataset, which contains more feature fields and fewer feature values compared with previous benchmarks. The following results in Table 5 can further prove the effectiveness of OptFeature. We can make the following observations. Firstly, OptFeature outperforms other models on the industrial dataset, validating the effectiveness of our hybrid-grained selection space and selection algorithm. This observation is consistent with those on the other three public benchmarks, as shown in Table 1. Secondly, AutoFIS performs superiorly compared to other baselines. This confirms the significance of conducting feature interaction selection within DSNs. Lastly, PROFIT is surpassed by AutoFIS, unlike on previous public datasets. We believe this is likely due to the increasing number of feature fields. AutoFIS, which treats each field independently, can extend easily to datasets with more feature fields. AutoFIS, which decomposes the feature selection result on the field level, does not generalize well. With the increase of the feature fields, the complexity of such decomposition increases exponentially. OptFeature, on the contrary, can potentially extend to datasets containing more fields better due to its hybrid selection granularity.

## Appendix D Broader Impact

Successfully identifying informative feature interactions could be a double-edged sword. On the one hand, by proving that introducing noisy features into the model could harm the performance, feature interaction selection could be used as supporting evidence in preventing certain businesses, such as advertisement recommendations, from over-collecting users' information, thereby protecting user privacy. On the other hand, these tools, if acquired by malicious people, can be used for filtering out potential victims, such as individuals susceptible to email fraud. As researchers, it is crucial for us to remain vigilant and ensure that our work is directed towards noble causes and societal benefits.

\begin{table}
\begin{tabular}{c|c|c c} \hline \multicolumn{2}{c|}{Dataset} & \multicolumn{2}{c}{Industrial} \\ \hline Category & Model & AUC & Logloss \\ \hline \multirow{2}{*}{Shallow} & LR & 0.7745 & 0.2189 \\  & FM & 0.7780 & 0.2181 \\ \hline \multirow{4}{*}{DSNs} & FNN & 0.7838 & 0.2168 \\  & DeepFM & 0.7824 & 0.2179 \\  & DCN & 0.7844 & 0.2167 \\  & IPNN & 0.7883 & 0.2147 \\ \hline \multirow{4}{*}{DSNs with FIS} & AutoFIS & 0.7889 & 0.2146 \\  & PROFIT & 0.7849 & 0.2161 \\ \cline{1-1} \cline{2-4}  & OptFeature & **0.7893** & **0.2142** \\ \hline \end{tabular}

* The best and second best performed results are marked in **bold** and underline format respectively.

\end{table}
Table 5: Performance Comparison on Industrial Dataset