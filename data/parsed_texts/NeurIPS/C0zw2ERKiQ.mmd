# Revisiting the Evaluation of Image Synthesis

with GANs

 Mengping Yang\({}^{1,\dagger}\)  Ceyuan Yang\({}^{1,\dagger}\)  Yichi Zhang\({}^{1}\)  Qingyan Bai\({}^{3}\)  Yujun Shen\({}^{2}\)  Bo Dai\({}^{1}\)

\({}^{1}\)Shanghai AI Laboratory \({}^{2}\)Ant Group \({}^{3}\)Tsinghua University

###### Abstract

A good metric, which promises a reliable comparison between solutions, is essential for any well-defined task. Unlike most vision tasks that have per-sample ground-truth, image synthesis tasks target generating _unseen_ data and hence are usually evaluated through a distributional distance between one set of real samples and another set of generated samples. This study presents an empirical investigation into the evaluation of synthesis performance, with generative adversarial networks (GANs) as a representative of generative models. In particular, we make in-depth analyses of various factors, including how to represent a data point in the representation space, how to calculate a fair distance using selected samples, and how many instances to use from each set. Extensive experiments conducted on multiple datasets and settings reveal several important findings. Firstly, a group of models that include both CNN-based and ViT-based architectures serve as reliable and robust feature extractors for measurement evaluation. Secondly, Centered Kernel Alignment (CKA) provides a better comparison across various extractors and hierarchical layers in one model. Finally, CKA is more sample-efficient and enjoys better agreement with human judgment in characterizing the similarity between two internal data correlations. These findings contribute to the development of a new measurement system, which enables a consistent and reliable re-evaluation of current state-of-the-art generative models. 1

Footnote 1: code is available at https://github.com/kobeshegu/Synthesis-Measurement-CKA.

## 1 Introduction

Through reproducing realistic data distribution, generative models [67, 2, 19, 38, 28, 52] have enabled thrilling opportunities to go beyond existing observations via content recreation. Their technical breakthroughs in recent years also directly lead to the blooming of metaverse, AI Generated Content (AIGC), and various other downstream applications. However, accurately measuring the progress and performance of generative models poses significant challenges, as it requires a consistent and comprehensive evaluation of the divergence between real and synthesized data distributions. Among existing evaluation metrics [41, 4, 34, 44], Frechet Inception Distance (FID) [22] is the most popular evaluation paradigm for synthesis comparison. Despite its popularity, recent studies [35, 44, 36, 1, 26] have identified several flaws in the FID metric that may miscalculate the actual improvements of generative models. Consequently, there is a pressing need for a more systematic and thorough investigation in order to provide a more accurate assessment of synthesis performance.

Therefore, this paper presents an empirical study that rigorously revisits the consistency and comprehensiveness of evaluation paradigms for generative models. Commonly used paradigms including FID typically contain two key components: a feature extractor \(\phi(\cdot)\) and a distributional distance \(d(\cdot)\). Through \(\phi(\cdot)\), _i.e.,_ Inception-V3 [56], a number of real samples (\(x\in\mathrm{X}\)) and synthesizedones (\(y\in\mathrm{Y}\)) are projected into a pre-defined representation space to approximate the respective data distributions. \(d(\cdot)\), _i.e.,_ Frechet Distance [17] is then calculated in the space to deliver the similarity index, indicating the synthesis quality. Following the philosophy, our study revolves around the representation space defined by the feature extractor \(\phi(\cdot)\), and the distributional distance \(d(\cdot)\).

The most commonly used feature extractor, _i.e.,_ Inception-V3, has been found to encode limited semantics and possesses a large perceptual null representation space [35], making it hardly reflect the actual improvement of synthesis. Accordingly, we gather multiple models that vary in _supervision signals_, _architectures_, and _representation similarities_ to investigate the impact of the feature extractor \(\phi(\cdot)\), which are respectively motivated by 1) representation spaces defined by the extractor \(\phi(\cdot)\) usually encode different levels of semantics, understanding which extractor or set of extractors can capture rich semantics is crucial yet less explored; 2) and it remains uncertain how the correlations between various representation spaces affect the evaluation results. In addition to studying the feature extractor \(\phi(\cdot)\), we further delve into _the consistency across spaces_, _the choice of distances_, and _the number of evaluated samples_ for the distributional distance \(d(\cdot)\). It is imperative that the distributional distance consistently provides a reliable similarity index, even when measured in various representation spaces. Similarly, selecting an appropriate number of samples to represent the synthesis distribution is a critical consideration. Regarding the choice of \(d(\cdot)\), besides Frechet Distance, we incorporate Centered Kernel Alignment (CKA) [11; 10] to explore alternatives for a more accurate evaluation.

In order to qualitatively and quantitatively compare different choices of the aforementioned aspects of \(\phi(\cdot)\) and \(d(\cdot)\), we re-implement the visualization pipeline and the histogram matching technique as in [35]. These techniques allow us to respectively highlight the most relevant semantics of an image _w.r.t_ the similarity indexes and to attack the measurement system through a selected subset. Moreover, we conduct an extensive user study involving 100 participants to investigate the correlation between the synthesis measurement and human judgment. Through these analysis tools, we make _several significant findings_: 1) One specific extractor (_e.g.,_ Inception-V3) tends to capture limited semantics and provide unreliable measurement results. 2) Various extractors naturally focus on different aspects of semantics thus demonstrating the potential generalization across different domains, motivating us to incorporate multiple extractors to deliver a comprehensive and reliable measurement. 3) With respect to features obtained from multiple representation spaces defined by different extractors, CKA proves to be effective in measuring the discrepancy and produces bounded values that facilitate comparison across different spaces. 4) In conjunction with the extensive user study, CKA consistently agrees with human judgment whereas FID failed in some circumstances, further underscoring the advantages of using CKA as the evaluation metric.

After revisiting various factors, we leverage the newly developed measurement system to re-evaluate extensive generative models under various settings. Current state-of-the-art generative models on several domains are first benchmarked through our system. Moreover, the performances and intrinsic properties of diffusion models and GANs are comprehensively compared with the new measurement system. Furthermore, the measurement system is employed to evaluate the performance of image-to-image translation models. It turns out that our system not only delivers a similar assessment with FID and human evaluation in most cases, but also demonstrates a more reliable and consistent correlation with human judgment than FID, demonstrating the robustness and superiority of the proposed metric.

## 2 Preliminary

This section briefly introduces the feature extractor \(\phi(\cdot)\), distributional distance \(d(\cdot)\), evaluated datasets and generative models, as well as auxiliary analysis approaches used in our study.

### Representation Spaces

To investigate the effect of feature extractors \(\phi(\cdot)\), we gather multiple models that are pre-trained on different objectives in fully-supervised/self-supervised manners, and with various architectures (_e.g.,_ ViT, CNN, and MLP).

**Supervision.** Models that are trained in fully-supervised and self-supervised manners are collected due to their potential for generalization. In particular, we include the backbone networks which are well-trained on supervised ImageNet classification tasks [13; 21]. Further, we gather the weights derived from single-modal/multi-modal self-supervised learning approaches [46; 8; 9; 6].

**Architecture.** In addition to considering models trained with different supervisions, we also include models with various architectures. Concretely, models with CNNs [56; 53; 21; 6; 8], ViTs [9; 39; 46], as well as MLPs [61] architectures are gathered together for our investigation.

### Distributional Distances

**Frechet Inception Distance (FID)** computes the Frechet Distance [17] between two estimated Gaussian distributions, _i.e.,_\(\mathcal{N}(\mu_{s},\Sigma_{s})\) and \(\mathcal{N}(\mu_{r},\Sigma_{r})\), which represent the feature distributions of synthesized and real images extracted by the pre-trained Inception-V3. Formally, Frechet Distance (FD) is calculated by

\[\mathrm{FD(X,Y)}=\left\|\mu_{s}-\mu_{r}\right\|^{2}+\mathrm{Tr}\left(\Sigma_{s }+\Sigma_{r}-2\left(\Sigma_{s}\Sigma_{r}\right)^{\frac{1}{2}}\right),\] (1)

where X and Y represent the real distribution and synthesized distribution, respectively. \(\mu\) and \(\Sigma\) correspond to the mean and variance of Gaussian distribution, and \(\mathrm{Tr}(\cdot)\) is the trace operation.

**Centered Kernel Alignment (CKA)** as a widely used similarity index for quantifying neural network representations [11; 32; 12], could also serve as a metric of similarity between two given data distributions. CKA has been identified to have several advantages: 1) CKA is invariant to orthogonal transformation and isotropic scaling, making is stable under various image transformations; 2) CKA can capture the non-linear correspondence between representations benefit from its kernel mapping; and 3) CKA can determine the correspondence across different features and with different widths, whereas previous metrics fail [32].

Formally, CKA is normalized from Hilbert-Schmidt Independence Criterion (HSIC) [20] to ensure invariant to isotropic scaling and is calculated by

\[\mathrm{CKA(X,Y)}=\frac{\mathrm{HSIC}(x,y)}{\sqrt{\mathrm{HSIC}(x,x)\mathrm{ HSIC}(y,y)}}.\] (2)

Here, HSIC determines whether two distributions are independent. Formally, let \(K_{ij}=k\left(\mathrm{x}_{i},\mathrm{x}_{j}\right)\) and \(L_{ij}=l\left(\mathrm{y}_{i},\mathrm{y}_{j}\right)\), where \(k\) and \(l\) are two kernels. HSIC is defined as

\[\mathrm{HSIC}(K,L)=\frac{1}{(n-1)^{2}}\,\mathrm{Tr}(KHLH),\] (3)

where \(H\) denotes the centering matrix (_i.e.,_\(H_{n}=I_{n}-\frac{1}{n}\mathbf{1}\mathbf{1}^{T}\)). For kernel selections of \(k\) and \(l\), we find that different kernels (RBF, polynomial, and linear) give similar results and rankings, and the RBF kernel contributes to the distinguishability of quantitative results. Therefore, RBF kernel is used for all experiments, and the bandwidth is set as a fraction of the median distance between examples [32]. These metrics are compared in a consistent setting for fair comparison, more implementation details and comparisons on CKA are given in _Supplementary Material_.

### Benchmarks and Analysis Approaches

**Benchmarks.** In order to analyze various factors of measurement, we also collect multiple generators to produce synthesized images. To be more specific, we employ state-of-the-art generative models trained on various datasets for comparison in Tab. 1. We download corresponding publicly available models for comparison. Unless otherwise specified, all of these models are compared in a consistent setting.

**Visualization tool.** To qualitatively compare where these feature extractors "focus on", we employ the visualization technique of [35] to localize the regions that contribute the most to the similarity index. The highlighted regions reveal the most relevant parts of an image regarding the measurement results. Accordingly, larger highlighted regions indicate that more visual semantics are involved in the evaluation. Note that generating a realistic image requires all parts, even each pixel, to be well-synthesized. Thus, a metric that focuses on more visual regions appears to be more dependable.

**Histogram matching attack.** We employ the histogram matching [35] to attack the system to investigate the robustness of the measurement results. Concretely, a subset is selected from a superset by matching the class distribution of the synthesized set with that of the real set. As pointed out by [35], the synthesis performance could be substantially improved using the chosen subset. We thus prepare two distinct sets of synthesized images. One reference set ("Random") is produced by generating images randomly, and the other set ("Chosen\({}_{I}\)") is carefully curated by matching the class distribution histogram of the supervised Inception-V3 [56]. The matched histograms are available in _Supplementary Material_. Since the generator and real data remain unaltered, the evaluation should keep consistent, and any performance gains directly indicate the unreliability of a given extractor.

**Human judgment.** In order to examine the correlation between the evaluation system and human perceptual judgment, we conduct extensive user studies employing two strategies. Firstly, to benchmark the synthesis quality of various generative models, we prepare a substantial number of randomly generated images (_i.e.,_\(5K\)), and ask \(100\) individuals to assess the photorealism of these images. The final scores are averaged across all \(100\) participants. Secondly, to qualitatively compare two paired generative models with similar quantitative performances (_e.g.,_ Projected-GAN [50] and Aided-GAN [33] on LSUN Church dataset in Sec. 4.1), we prepare groups of paired images generated by different models and ask \(100\) individuals to assess the perceptual quality of paired images. More details of our user studies can be found in _Supplementary Material_. In this way, we could obtain reliable and consistent human judgments, facilitating better investigation with the evaluation system.

## 3 Analysis on Representation Spaces and Distributional Distances

In this section, we investigate the potential impacts of the representation space and distributional distances with respect to the final similarity index. Concretely, Sec. 3.1 presents the study of extractors that define the representation spaces, followed by the analysis of distributional distances in Sec. 3.2.

### Representation Spaces

Prior works [35; 36; 41; 26] have demonstrated that the most commonly used feature extractor _i.e.,_ Inception-V3 [56], could hardly reflect the exact improvement of synthesis due to its limited consideration of semantics. We thus conduct a comprehensive study by incorporating various models that differ in _supervision signals_ and _architectures_, to identify which or which set of extractors serve as reliable feature extractors for synthesis comparison.

**Distinct feature extractors with different architectures yield varying semantic areas of focus.** Fig. 1 shows the highlighted regions that contribute most significantly to the measurement results. Obviously, CNN-based extractors consistently emphasize concentrated regions with or without manual labels for pre-training, including limited semantics. Specifically, CNN-based extractors remain to highlight objects (_e.g.,_ microphone, hat, and sunglasses), rather than the main focus of the evaluation domains (_i.e.,_ Human Faces here). In contrast, ViT-based extractors capture larger regions that encompass more synthesis details and semantics. This observation aligns with the finding that ViTs possess a global and expansive receptive field compared to the local receptive field of

\begin{table}
\begin{tabular}{l|c c} \hline \hline Method & Year & Training Datasets \\ \hline StyleGAN2 [29] & 2020 & FFHQ, LSUN Church \\ BigGAN, BigGAN-deep [5] & 2019 & ImageNet \\ ADM, ADM-G [14] & 2021 & ImageNet \\ Projected-GAN [50] & 2021 & FFHQ, LSUN Church \\ InsGen [66] & 2021 & FFHQ, FFHQ \\ StyleGAN-XL [66] & 2022 & FFHQ, ImageNet \\ Aided-GAN [33] & 2022 & LSUN Church \\ EqGAN [62] & 2022 & FFHQ, LSUN Church \\ Diffusion-GAN [64] & 2022 & LSUN Church \\ DiT [45], BigRoC [18] & 2022 & ImageNet \\ GigaGAN [27], DG-Diffusion [31], MDT [19] & 2023 & ImageNet \\ \hline \hline \end{tabular}
\end{table}
Table 1: **Generative models used in our study**. Publicly available models are gathered for evaluation.

CNN-based extractors [47; 65; 16]. Furthermore, the ViT-based and CNN-based extractors appear to complement each other, with the former capturing broader regions and the latter focusing on specific objects with high density.

**Multiple extractors incorporate more visual semantics in a complementary manner.** When reproducing the whole data distribution, generative models are required to synthesize not only the main objects but also the background, texture, and intricate details. Similarly, the extractors should strive to capture more regions of given images to approximate the data distribution, enabling better visual perception. However, the above observation regarding the heatmaps of different extractors reveals that each individual extractor could only capture partial semantics of the entire image for measurement, inadequately reflecting the overall synthesis performance. Consequently, various extractors with different architectures should be considered since they could involve more semantics, enhancing the reliability of the evaluation.

**Extractors that are vulnerable to the histogram matching attack are not reliable for evaluation.** Prior study [35] has highlighted that extractors focusing on limited semantics may be susceptible to the histogram matching attack, undermining the trustworthiness of the evaluation. Motivated by this, we investigate the robustness of the above extractors toward the attack as they attach different importance to the visual concepts. Concretely, we obtain the publicly available generator of StyleGAN2[30] and calculate the Frechet Distance (FD) results on the FFHQ dataset. We compare two evaluated sets: one is randomly generated using the model, while the other is chosen by matching the predicted class distribution of Inception-V3 (the matched histograms are provided in _Supplementary Material_).

Footnote 2: https://github.com/NVlabs/stylegan3

Tab. 2 shows the quantitative results. Comparing the performances of the random set and chosen set, we could tell that certain extractors, regardless of their architectures (_e.g.,_ CNN-based Inception-V3 [56], ViT-based Swin-Transformer [39], and MLP-based ResMLP [61]), are susceptible to the histogram matching attack. For instance, the FD score of Inception shows an improvement of \(5.7\)% when the chosen set is used, and ResMLP exhibits a \(3.8\)% improvement (More quantitative

\begin{table}
\begin{tabular}{l|c c c c c c} \hline \hline Model & Inception & ConvNeXt & SWAV & MoCo-R & RepVGG & CLIP-R \\ \hline Random & 2.81\(\pm_{0.01}\) & 78.03\(\pm_{0.10}\) & 0.13\(\pm_{0.002}\) & 0.24\(\pm_{0.003}\) & 129.61\(\pm_{0.41}\) & 10.34\(\pm_{0.06}\) \\ Chosen\({}_{I}\) & 2.65\(\pm_{0.014}\) & 78.19\(\pm_{0.11}\) & 0.13\(\pm_{0.002}\) & 0.24\(\pm_{0.003}\) & 129.67\(\pm_{0.39}\) & 10.36\(\pm_{0.08}\) \\ \hline Model & Swin & ViT & DeiT & CLIP-V & MoCo-V & ResMLP \\ \hline Random & 142.87\(\pm_{0.12}\) & 15.11\(\pm_{0.09}\) & 437.80\(\pm_{0.14}\) & 1.06\(\pm_{0.01}\) & 7.32\(\pm_{0.03}\) & 99.11\(\pm_{0.06}\) \\ Chosen\({}_{I}\) & 140.01\(\pm_{0.12\downarrow}\) & 15.11\(\pm_{0.10}\) & 430.81\(\pm_{0.16\downarrow}\) & 1.06\(\pm_{0.01}\) & 7.40\(\pm_{0.03}\) & 95.36\(\pm_{0.06\downarrow}\) \\ \hline \hline \end{tabular}
\end{table}
Table 2: **Quantitative comparison results of Fréchet Distance (FD\({}_{\downarrow}\)) on FFHQ dataset**. “Random, Chosen\({}_{I}\)” respectively represent the synthesized distribution of randomly generated and matching the class prediction of Inception-V3. Moreover, “-R” and “-V” respectively denote the architecture of ResNet and ViT. (\({}_{\downarrow}\)) indicates the results are hacked by the histogram matching mechanism. Notably, the values across different rows are not comparable and we report the stds of three testings to better illustrate the numerical fluctuation of various extractors towards the histogram attack. The results of Fréchet Distance (FD) on ImageNet can be found in _Supplementary Material_.

Figure 1: **Heatmaps from extractors with various architectures.** CNN-based extractors (_i.e.,_ Inception [56], ConvNeXt [40], SWAV [6], MoCo-R [9], CLIP-R [46], and RepVGG [15]) focus on objects whereas ViT-based (_i.e.,_ Swin-Transformer [39], ViT [16], DeiT [60], CLIP-V [46], and MoCo-V [9]) and MLP-based (_i.e.,_ ResMLP [61]) ones pour attention on wider areas.

and qualitative results of MLP-based extractors gMLP [37] and MLP-mixer [59] are shown in _Supplementary Material_). Namely, the FD score could be improved without making any changes to the generators, aligning with the observations of [35]. We thus filter out the extractors that are vulnerable to the attack since their rankings can be manipulated without actual improvement to the generative models.

**Extractors that define similar representation spaces are redundant.** So far, we have demonstrated the importance of considering multiple extractors for a more comprehensive evaluation and filtered out the extractors that are susceptible to the histogram attack. However, it is also crucial to avoid redundancy among the remaining extractors, as they may define similar representation spaces. To address this, we examine the correlation between representation spaces across different feature extractors, following the approach outlined in [32]. Specifically, a significant number of images (\(10K\) images from ImageNet) are fed into these extractors to compute their correspondence. After calculating the similarity matrix, we can further filter out extractors that define homogeneous representation spaces (the similarity analysis is presented in _Supplementary Material_). The remaining extractors are presented in the table below. These extractors 1) capture rich semantics in a complementary way, 2) are robust toward the histogram matching attack, and 3) define meaningful and distinctive representation spaces. Besides, both CNN-based and ViT-based extractors are considered and all of them have demonstrated strong performance for the pre-defined and downstream tasks, facilitating more comprehensive and reliable evaluation. Notably, the selection of self-supervised extractors SWAV, CLIP-ViT, and MoCo-ViT agrees with previous studies [41; 35; 3].

**The selected extractors can serve as reliable tools for synthesis evaluation.** In order to investigate the reliability of the selected extractors, we employ them to test the synthesis quality of representative generative models on the ImageNet dataset. Tab. 3 presents the quantitative FD scores of various extractors. Although their results differ in numerical scales, they consistently indicate that StyleGAN-XL outperforms both BigGAN and BigGAN-deep by a significant margin, and BigGAN-deep performs slightly better than BigGAN. Such observation also agrees with the human judgment in Tab. 5. Overall, the consistent trends observed across different extractors and the alignment with human judgment confirm that the selected extractors are reliable for comparing the synthesis quality.

### Distributional Distances

After the study of feature extractors, we shift our focus to another essential component of measurement, _i.e.,_ the distributional distance \(d(\cdot)\). Besides Frechet Distance (FD), we also incorporate Centered Kernel Alignment (CKA) for our investigation.

**CKA provides normalized distances _w.r.t_ numerical scales in variable representation spaces.** Tab. 5 demonstrates the quantitative results of Centered Kernel Alignment (CKA). Unlike the Frechet Distance (FD) scores that exhibit significant fluctuations across various extractors, the CKA scores demonstrate remarkable stability when evaluated in different representation spaces. For instance, the FD score of BigGAN on MoCo-ViT is 238.78 whereas 3.35 on CLIP-ViT, making it challenging to combine the results of different extractors. By contrast, the stability of CKA scores allows the ability to combine results from multiple extractors (_i.e.,_ average) for better comparison.

**CKA demonstrates great potential for quantitative comparison and combination across hierarchical layers.** Here we investigate the distributional distances across various layers of the neural network, as these layers typically extract multi-level features that span from high-level semantics to low-level details. Accordingly, considering hierarchical features can provide a more comprehensive measurement. The left part of Tab. 4 presents the qualitative results of different layers' heatmaps. We can observe that different layers indeed extract different semantics, highlighting the importance of considering hierarchical features in evaluation. Additionally, we provide the quantitative results of FD and CKA in the right part of Tab. 4. Still, the FD scores of various layers fluctuate dramatically, _e.g.,_ the FD results of \(0.60\) in Layer\({}_{1}\) while 104.10 in Layer\({}_{4}\). Differently, the CKA results from hierarchical layers are comparable and the overall score could be derived by averaging multi-level scores. Importantly, the overall score still reflects synthesis quality consistently and reliably.

**CKA shows satisfactory sample-efficiency and stability under different number of samples.** Typically, the synthesis quality are measured between real and synthesized distributions, where the whole training data is used as the real distribution and \(50\textit{K}\) generated images as the synthesized, regardless of how many samples contained in the training data. However, when evaluating on the large-scale datasets (_e.g.,_\(1.28\) million images for ImageNet), \(50\textit{K}\) images may be insufficient to represent the entire distribution. Therefore, we study the impacts of the amount of generated samples. Concretely, we prepare several synthesized sets with different numbers of samples and calculate their distances to the real distribution (more details are presented in _Supplementary Material_).

Fig. 2 demonstrates the curves of FD and CKA scores evaluated under different data regimes on the FFHQ dataset. Obviously, the FD scores can be drastically improved by synthesizing more data regardless of different extractors until sufficient samples (\(\sim 100\textit{K}\)) are used, whereas the CKA scores are stable under different data regimes. Moreover, CKA could measure the distributional distances precisely with only \(5\textit{K}\) synthesized samples, suggesting significant sample efficiency. Such observations demonstrate CKA's impressive adaptability toward the amount of synthesized data.

**Developing a reliable and comprehensive measurement system for synthesis evaluation.** Overall, a set of feature extractors that 1) are robust to the histogram matching attack, 2) capture sufficient semantics, and 3) define distinctive representation spaces could serve as reliable extractors for synthesis comparison. Together with a bounded distance (_i.e.,_ CKA) that is comparable across various representation spaces and hierarchical layers, as well as enjoys satisfactory sample efficiency. These two essential components constitute a reliable system to deliver the distributional discrepancy.

\begin{table}
\begin{tabular}{l|c c|c c|c c} \hline \hline Model & \multicolumn{2}{c|}{BigGAN} & \multicolumn{2}{c|}{BigGAN-deep} & \multicolumn{2}{c}{StyleGAN-XL} \\ \hline Layer & FD\({}_{\downarrow}\) & CKA\({}_{\uparrow}\) & FD\({}_{\downarrow}\) & CKA\({}_{\uparrow}\) & FD\({}_{\downarrow}\) & CKA\({}_{\uparrow}\) \\ \hline Layer\({}_{1}\) & 0.60 & 99.06 & 0.54 & 98.95 & 0.05 & 99.84 \\ Layer\({}_{2}\) & 7.45 & 86.89 & 5.58 & 90.09 & 0.77 & 91.06 \\ Layer\({}_{3}\) & 30.24 & 82.80 & 23.55 & 83.63 & 6.11 & 85.75 \\ Layer\({}_{4}\) & 104.10 & 80.13 & 81.02 & 81.05 & 35.77 & 83.55 \\ \hline Overall & N/A & 87.22 & N/A & 88.43 & N/A & 90.05 \\ \hline \hline \end{tabular}
\end{table}
Table 4: **Heatmaps from various semantic levels on FFHQ dataset (_left_)** and Fréchet Distance (\(\textbf{FD}_{\downarrow}\)) and Centered Kernel Alignment (\(\textbf{CKA}_{\uparrow}\)) scores on ImageNet dataset (_right_)**. CLIP-ViT serves as the feature extractor here, more results can be found in _Supplementary Material_.

Figure 2: **Fréchet Distance (FD) and Centered Kernel Alignment (CKA) scores evaluated under various data regimes on FFHQ dataset.** FID scores are scaled for better visualization. \(\downarrow\) denotes the results fluctuate downward. The percentages represent the magnitude of the numerical variation. The curve of KID [4], Precision and Recall [49] can be found in _Supplementary Material_.

## 4 Benchmark Existing Generative Models

Based on the findings regarding the feature extractors and distributional distances, we construct a new synthesis valuation system. Concretely, our system leverages a set of models with both CNN and ViT architectures as feature extractors, namely, CNN-based ConvNeXt [40], RepVGG [15], SWAV [6] and ViT-based ViT [16], MoCo-ViT [9], CLIP-ViT [46], with which more comprehensive evaluation could be accomplished. Further, Centered Kernel Alignment (CKA) serves as the similarity indicator.

Accordingly, in this part we re-evaluate and compare the progress of existing generative models with our measurement system. Concretely, the latest generative models are re-evaluated with our system in Sec. 4.1, followed by our discussion about the diffusion models and GANs in Sec. 4.2. Notably, user studies are conducted for investigating the correlation between our system and human judgment.

### Comparison on Existing Generative models

In order to investigate the actual improvement of existing generative models, we collect multiple publicly available generators trained on several popular benchmarks (_i.e.,_ FFHQ, LSUN Church, and ImageNet) for comparison. Benefiting from the impressive sample efficiency of CKA, we generate \(50\)_K_ images as the synthesized distribution and use the whole datasets as the real distribution. Results from various selected extractors and their averaged scores are reported for a thorough comparison.

**Our system can measure the synthesis quality in a consistent and reliable way.** Tab. 5, Tab. 7, and Tab. 6 respectively demonstrate the quantitative results of different generative models on the ImageNet, LSUN Church, and FFHQ datasets. In most cases, CKA scores from various extractors and the overall scores provide a consistent ranking with FID scores, as well as agree with human perceptual judgment. These results suggest that our new metric could precisely measure the synthesis quality. However, for the Projected-GAN [50] and StyleGAN2 [29] (_resp.,_ Aided-GAN [33]) evaluated on FFHQ (_resp.,_ LSUN Church) dataset in Tab. 6 (_resp.,_ Tab. 7), our evaluation system gives the opposite ranking to the FID. Namely, the quantitative results of StyleGAN2 (_resp.,_ Aided-GAN) are determined better than that of Projected-GAN under our evaluation, whereas the FID scores vote Projected-GAN for the better one. Additionally, the performances of ICGAN [7] and class-conditional ICGAN on ImageNet in Tab. 5 are identified basically the same by our metric, while FID scores indicate that the class-conditional one significantly surpasses the unconditional one.

\begin{table}
\begin{tabular}{l|c|c c c c c c|c|c} \hline \hline Model & FID\({}^{\dagger}\) & ConvNeXt & RepVGG & SWAV & ViT & MoCo-ViT & CLIP-ViT & Overall & User study \\ \hline StyleGAN2 [29] & 3.66 & 92.64 & 62.10 & 99.55 & 99.32 & 98.59 & 97.46 & 91.61 & 45\% \\ Projected-GAN [50] & 3.39 & 92.34 & 61.62 & 99.37 & 98.99 & 98.81 & 97.31 & 91.41 & 39\% \\ InsGen [66] & 3.31 & 94.17 & 65.72 & 99.60 & 99.36 & 98.90 & 97.75 & 92.58 & 58\% \\ EqGAN [62] & 2.89 & 94.54 & 64.36 & 99.69 & 99.49 & 99.10 & 98.61 & 92.63 & 62\% \\ StyleGAN-XL [51] & 2.19 & 93.78 & 67.59 & 99.68 & 99.49 & 99.25 & 97.33 & 92.85 & 66\% \\ \hline \hline \end{tabular}
\end{table}
Table 6: **Quantitative comparison results of Centered Kernel Alignment (CKA\({}_{\uparrow}\)) on FFHQ dataset. \({}^{\dagger}\) scores are quoted from the original paper and others are tested three times.**

\begin{table}
\begin{tabular}{l|c|c c c c c c|c|c} \hline \hline Model & FID\({}^{\dagger}\) & ConvNeXt & RepVGG & SWAV & ViT & MoCo-ViT & CLIP-ViT & Overall & User study \\ \hline ICGAN [7] & 15.60 & 62.65 & 72.25 & 86.10 & 97.04 & 77.99 & 92.16 & 81.37 & 32\% \\ ADM [14] & 10.94 & 63.12 & 72.90 & 87.71 & 97.39 & 78.69 & 92.92 & 82.12 & 45\% \\ BigGAN [5] & 8.70 & 63.89 & 74.62 & 87.94 & 97.60 & 79.60 & 93.25 & 82.82 & 53\% \\ C-ICGAN [7] & 7.50 & 62.53 & 72.20 & 86.12 & 97.05 & 78.01 & 92.08 & 81.33 & 31\% \\ BigGAN-deep [5] & 6.95 & 64.97 & 76.45 & 88.31 & 97.77 & 80.27 & 94.11 & 83.65 & 55\% \\ Guided-ADM [14] & 4.59 & 65.99 & 78.99 & 89.44 & 98.13 & 80.46 & 94.96 & 84.66 & 57\% \\ BigRoc [18] & 3.69 & 67.86 & 79.48 & 89.93 & 98.23 & 82.25 & 96.07 & 85.64 & 65\% \\ GigaGAN [27] & 3.45 & 68.01 & 79.93 & 90.15 & 98.34 & 82.40 & 96.52 & 85.89 & 65\% \\ DG-Diffusion [31] & 3.18 & 68.22 & 80.06 & 90.56 & 98.46 & 82.51 & 96.88 & 86.12 & 66\% \\ StyleGAN-XL [51] & 2.30 & 68.75 & 80.28 & 91.54 & 98.52 & 82.64 & 97.41 & 86.52 & 67\% \\ DiT [15] & 2.27 & 68.94 & 80.65 & 91.03 & 99.05 & 82.90 & 97.08 & 86.61 & 67\% \\ MDT [19] & 1.79 & 69.64 & 81.68 & 91.78 & 99.43 & 83.43 & 98.19 & 87.36 & 69\% \\ \hline \hline \end{tabular}
\end{table}
Table 5: **Quantitative comparison results of Centered Kernel Alignment (CKA\({}_{\uparrow}\)) on ImageNet dataset. \({}^{\dagger}\) scores are quoted from the original paper and others are tested three times. To make our results more trivial to parse, we visualize the correlation between different metrics the human evaluation results in the _Supplementary Material_.**In order to compare the performance of these models in a fine-grained way, we further perform paired-wise human evaluation. Specifically, groups of paired images synthesized by different models (_e.g.,_ StyleGAN2 and Projected-GAN on FFHQ dataset) are randomly picked for visual comparison. Then, presented with two sets of images produced by different models, users are asked to determine which set of images is more plausible.

**Our measurement system provides the right rankings and better correlations with human visual judgment.** Tab. 8 presents the quantitative results of human visual comparison. Observably, the synthesis quality of StyleGAN (_resp._ Aided-GAN) is more preferred by human visual judgment. That is, our measurement system produces the same rankings as the human perceptual evaluation, demonstrating the reliability of our metric. Moreover, our metric's indication that there's no significant gap between ICGAN and class-conditional ICGAN is also verified by the human evaluation. Considering the perceptual null space of Inception-V3, one possible reason for the FID performance gains of Projected-GAN might be the usage of pre-trained models, which is also identified by [35]. By contrast, our measurement produces the right rankings and agrees well with human evaluation, reflecting the actual synthesis quality in a comprehensive and reliable way.

### Comparison between GANs and Diffusion Models

Diffusion models [23; 54; 55; 14; 45; 48; 19; 70; 42; 24] have demonstrated significant advancements in visual synthesis and became the new trend of generative models recently. Benefiting from the reliability of our new measurement system, here we perform a comprehensive comparison between GANs and diffusion models. Specifically, we report the FID and the overall CKA scores, as well as human judgment for quantitative comparison. Additionally, the model parameters and the synthesis speed (tested on an A100 GPU) are also included.

Tab. 9 presents the quantitative results. Obviously, diffusion model (_i.e.,_ DiT) obtains comparable results with GAN (_i.e.,_ StyleGAN-XL), yet with much more parameters (_i.e.,_ 675\(M\)_vs._ 166.3\(M\)). Moreover, diffusion models usually require extra inference time to obtain realistic images. Such comparisons reveal that GANs achieve better trade-offs between efficiency and synthesis quality, and designing computation-efficient diffusion models is essential for the community.

### Comparing the performance of image-to-image translation

In order to testify the compatibility of our metric, here we employ our measurement system to evaluate the performance of image-to-image translation. We collect publicly available image-to-image translation models that are officially released to translate images from one domain to another domain for evaluation. Specifically, three translation benchmarks are involved here, namely Horse-to-Zebra [57; 71; 43], Cat-to-Dog [68; 43], and Dog-to-Cat [68; 25]. For each benchmark, we translate the tested images to the target domain following the original experimental settings. Then we compute the distributional discrepancies between the translated images and the real target images. Tab. 10 presents the quantitative results of the evaluated three image-to-image translation benchmarks. It can

\begin{table}
\begin{tabular}{l|c c c c c c c|c|c} \hline \hline Model & FID\({}^{\dagger}\) & ConvNeXt & RepVGG & SWAV & ViT & MoCo-ViT & CLIP-ViT & Overall & User study \\ \hline StyleGAN2 [29] & 3.86 & 96.99 & 67.43 & 98.70 & 97.68 & 89.18 & 76.25 & 87.71 & 69\% \\ Diffusion-GAN [64] & 3.17 & 97.15 & 69.28 & 99.22 & 97.70 & 90.25 & 79.06 & 88.78 & 71\% \\ EqGAN [66] & 3.02 & 97.34 & 71.12 & 99.36 & 98.09 & 90.64 & 80.26 & 89.47 & 73\% \\ Aided-GAN [33] & 1.72 & 97.91 & 75.63 & 99.42 & 99.62 & 92.56 & 81.69 & 91.14 & 77\% \\ Projected-GAN [50] & 1.59 & 96.89 & 72.91 & 97.98 & 98.09 & 91.43 & 78.63 & 89.34 & 72\% \\ \hline \hline \end{tabular}
\end{table}
Table 7: **Quantitative comparison results of Centered Kernel Alignment (CKA\({}_{\uparrow}\)) on LSUN Church dataset. \({}^{\dagger}\) scores are quoted from the original paper and others are tested three times.**

\begin{table}
\begin{tabular}{l|c c|c c|c c} \hline \hline Dataset & \multicolumn{2}{c|}{FFHQ} & \multicolumn{2}{c|}{LSUN Church} & \multicolumn{2}{c}{ImageNet} \\ \hline Model & Projected-GAN & StyleGAN2 & Projected-GAN & Aided-GAN & ICGAN & Conditional-ICGAN \\ User Preference & 45\% & 55\% & 43\% & 57\% & 50\% & 50\% \\ \hline \hline \end{tabular}
\end{table}
Table 8: **Fine-grained investigation of human judgment. Percentages indicate the ratio of generated images that are considered to be more plausible.**be seen from these results that CKA provides consistent ranks with FID among various extractors, and the averaged score can reflect the performance of different image translation models. For instance, the performance of CUT [43] on Horse-to-Zebra is identified better than that of CycleGAN [71] by both FID and our proposed metric. And the qualitative results in the original paper of CUT [43] also suggest that the performance of CUT surpasses CycleGAN. That is, our measurement system can provide a reliable evaluation under such settings. This indicates that our measurement system can also be used for evaluating the performance of image translation tasks.

## 5 Conclusion

This work revisits the evaluation of generative models from the perspectives of the feature extractor and the distributional distance. Through extensive investigation regarding the potential contribution of various feature extractors and distributional distances, we identify the impacts of several potential factors that contribute to the final similarity index. With these findings, we construct a new measurement system that provides a more comprehensive and holistic comparison for synthesis evaluation. Importantly, our system could present more consistent measurements with human judgment, enabling more reliable evaluation.

**Discussion.** Despite a comprehensive investigation, our study could still be extended in several aspects. For instance, the impacts of different low-level image processing techniques (_e.g.,_ resizing) could be identified since they also play an important role in synthesis evaluation [44]. Besides, comparing datasets with various resolutions could be further studied. Nonetheless, our study could be considered an empirical revisiting towards the paradigm of evaluating generative models. We hope this work could inspire more fascinating works of synthesis evaluation and provide potential insight to develop more comprehensive evaluation protocols. We will also conduct more investigation on the unexplored factors and compare more generative models with our system.

## References

* [1] M. Alfarra, J. C. Perez, A. Fruhstuck, P. H. Torr, P. Wonka, and B. Ghanem. On the robustness of quality measures for gans. _Eur. Conf. Comput. Vis._, pages 18-33, 2022.

\begin{table}
\begin{tabular}{l|c|c c c c c c|c} \hline \hline \multirow{2}{*}{Model} & \multicolumn{6}{c}{**Horse-to-Zebra dataset**} \\ \hline Model & FID & ConvNeXt & RepVGG & SWAV & ViT & MoCo-ViT & CLIP-ViT & Overall \\ \hline CycleGAN [71] & 83.32 & 73.55 & 88.67 & 85.82 & 83.96 & 74.72 & 73.74 & 80.08 \\ AttentionGAN [57] & 76.05 & 75.59 & 91.73 & 86.37 & 85.16 & 76.65 & 75.49 & 81.83 \\ CUT [43] & 51.29 & 78.48 & 93.22 & 88.83 & 87.84 & 78.75 & 77.36 & 84.08 \\ \hline \multicolumn{10}{c}{**Cat-to-Dog**} \\ \hline Model & FID & ConvNeXt & RepVGG & SWAV & ViT & MoCo-ViT & CLIP-ViT & Overall \\ \hline CUT [43] & 74.95 & 84.93 & 78.75 & 88.83 & 84.31 & 93.56 & 70.91 & 83.55 \\ GP-UNIT [68] & 60.96 & 90.45 & 87.79 & 94.05 & 90.12 & 95.91 & 75.32 & 88.94 \\ \hline \multicolumn{10}{c}{**Cat-to-Dog**} \\ \hline Model & FID & ConvNeXt & RepVGG & SWAV & ViT & MoCo-ViT & CLIP-ViT & Overall \\ \hline GP-UNIT [68] & 31.66 & 79.58 & 78.18 & 96.79 & 86.93 & 93.92 & 77.42 & 85.47 \\ MUNIT [25] & 18.88 & 84.87 & 84.11 & 98.51 & 88.11 & 95.95 & 86.10 & 89.61 \\ \hline \hline \end{tabular}
\end{table}
Table 10: **Quantitative comparison results of Centered Kernel Alignment (CKA\({}_{\uparrow}\)) on Image-to-Image translation tasks.**

\begin{table}
\begin{tabular}{l|c c c c c} \hline \hline Model & FID\({}^{\dagger}\) & CKA\({}^{\ddagger}\) & User & \#Params & Sec/Kimg (s) \\ \hline BigGAN & 8.70 & 82.82 & 53\% & 158.3 \(M\) & 33.6 \\ BigGAN-deep & 6.95 & 83.65 & 55\% & 85 \(M\) & 27.6 \\ StyleGAN-XL & 2.30 & 86.52 & 67\% & 166.3 \(M\) & 64.8 \\ \hline ADM & 10.94 & 82.12 & 45\% & 500 \(M\) & 17274 \\ Guided-ADM & 4.59 & 84.66 & 57\% & 554 \(M\) & 17671 \\ DiT & 2.27 & 86.61 & 67\% & 675 \(M\) & 3736.8 \\ \hline \hline \end{tabular}
\end{table}
Table 9: **Quantitative comparison results of diffusion models and GANs on ImageNet dataset**. \({}^{\dagger}\) scores are quoted from the original paper and \({}^{\ddagger}\) results are overall scores measured by our system.

* [2] Q. Bai, C. Yang, Y. Xu, X. Liu, Y. Yang, and Y. Shen. Glead: Improving gans with a generator-leading task. _IEEE Conf. Comput. Vis. Pattern Recog._, 2023.
* [3] E. Betzalel, C. Penso, A. Navon, and E. Fetaya. A study on the evaluation of generative models. _arXiv preprint arXiv:2206.10935_, 2022.
* [4] M. Binkowski, D. J. Sutherland, M. Arbel, and A. Gretton. Demystifying MMD GANs. In _Int. Conf. Learn. Represent._, 2018.
* [5] A. Brock, J. Donahue, and K. Simonyan. Large scale gan training for high fidelity natural image synthesis. In _Int. Conf. Learn. Represent._, 2019.
* [6] M. Caron, I. Misra, J. Mairal, P. Goyal, P. Bojanowski, and A. Joulin. Unsupervised learning of visual features by contrasting cluster assignments. _Adv. Neural Inform. Process. Syst._, pages 9912-9924, 2020.
* [7] A. Casanova, M. Careil, J. Verbeek, M. Drozdzal, and A. Romero-Soriano. Instance-conditioned gan. In _Adv. Neural Inform. Process. Syst._, 2021.
* [8] X. Chen, H. Fan, R. Girshick, and K. He. Improved baselines with momentum contrastive learning. _arXiv preprint arXiv:2003.04297_, 2020.
* [9] X. Chen, S. Xie, and K. He. An empirical study of training self-supervised vision transformers. In _Int. Conf. Comput. Vis._, pages 9640-9649, 2021.
* [10] C. Cortes, M. Mohri, and A. Rostamizadeh. Algorithms for learning kernels based on centered alignment. _The Journal of Machine Learning Research_, 13:795-828, 2012.
* [11] N. Cristianini, J. Shawe-Taylor, A. Elisseeff, and J. Kandola. On kernel-target alignment. _Adv. Neural Inform. Process. Syst._, 2001.
* [12] M. Davari, S. Horoi, A. Natik, G. Lajoie, G. Wolf, and E. Belilovsky. Reliability of cka as a similarity measure in deep learning. _arXiv preprint arXiv:2210.16156_, 2022.
* [13] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. Imagenet: A large-scale hierarchical image database. In _IEEE Conf. Comput. Vis. Pattern Recog._, pages 248-255, 2009.
* [14] P. Dhariwal and A. Nichol. Diffusion models beat gans on image synthesis. In _Adv. Neural Inform. Process. Syst._, pages 8780-8794, 2021.
* [15] X. Ding, X. Zhang, N. Ma, J. Han, G. Ding, and J. Sun. Repvgg: Making vgg-style convnets great again. In _IEEE Conf. Comput. Vis. Pattern Recog._, pages 13733-13742, 2021.
* [16] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. In _Int. Conf. Learn. Represent._, 2020.
* [17] D. Dowson and B. Landau. The frechet distance between multivariate normal distributions. _Journal of multivariate analysis_, 12(3):450-455, 1982.
* [18] R. Ganz and M. Elad. BIGRec: Boosting image generation via a robust classifier. _Transactions on Machine Learning Research_, 2022. ISSN 2835-8856.
* [19] S. Gao, P. Zhou, M.-M. Cheng, and S. Yan. Masked diffusion transformer is a strong image synthesizer. _Int. Conf. Comput. Vis._, 2023.
* [20] A. Gretton, O. Bousquet, A. Smola, and B. Scholkopf. Measuring statistical dependence with hilbert-schmidt norms. In _International conference on algorithmic learning theory_, pages 63-77, 2005.
* [21] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In _IEEE Conf. Comput. Vis. Pattern Recog._, pages 770-778, 2016.
* [22] M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and S. Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. In _Adv. Neural Inform. Process. Syst._, 2017.
* [23] J. Ho, A. Jain, and P. Abbeel. Denoising diffusion probabilistic models. _Adv. Neural Inform. Process. Syst._, pages 6840-6851, 2020.
* [24] L. Huang, D. Chen, Y. Liu, Y. Shen, D. Zhao, and J. Zhou. Composer: Creative and controllable image synthesis with composable conditions. _arXiv preprint arXiv:2302.09778_, 2023.

* [25] X. Huang, M.-Y. Liu, S. Belongie, and J. Kautz. Multimodal unsupervised image-to-image translation. In _Eur. Conf. Comput. Vis._, pages 172-189, 2018.
* [26] M. Jiralerspong, A. J. Bose, and G. Gidel. Feature likelihood score: Evaluating generalization of generative models using samples. _arXiv preprint arXiv:2302.04440_, 2023.
* [27] M. Kang, J.-Y. Zhu, R. Zhang, J. Park, E. Shechtman, S. Paris, and T. Park. Scaling up gans for text-to-image synthesis. In _IEEE Conf. Comput. Vis. Pattern Recog._, pages 10124-10134, 2023.
* [28] M. Kang, J.-Y. Zhu, R. Zhang, J. Park, E. Shechtman, S. Paris, and T. Park. Scaling up gans for text-to-image synthesis. _IEEE Conf. Comput. Vis. Pattern Recog._, 2023.
* [29] T. Karras, S. Laine, M. Aittala, J. Hellsten, J. Lehtinen, and T. Aila. Analyzing and improving the image quality of stylegan. In _IEEE Conf. Comput. Vis. Pattern Recog._, pages 8110-8119, 2020.
* [30] T. Karras, M. Aittala, S. Laine, E. Harkonen, J. Hellsten, J. Lehtinen, and T. Aila. Alias-free generative adversarial networks. _Adv. Neural Inform. Process. Syst._, pages 852-863, 2021.
* [31] D. Kim, Y. Kim, W. Kang, and I.-C. Moon. Refining generative process with discriminator guidance in score-based diffusion models. _Int. Conf. Mach. Learn._, 2023.
* [32] S. Kornblith, M. Norouzi, H. Lee, and G. Hinton. Similarity of neural network representations revisited. In _Int. Conf. Mach. Learn._, pages 3519-3529, 2019.
* [33] N. Kumari, R. Zhang, E. Shechtman, and J.-Y. Zhu. Ensembling off-the-shelf models for gan training. In _IEEE Conf. Comput. Vis. Pattern Recog._, pages 10651-10662, 2022.
* [34] T. Kynkainniemi, T. Karras, S. Laine, J. Lehtinen, and T. Aila. Improved precision and recall metric for assessing generative models. _Adv. Neural Inform. Process. Syst._, 32, 2019.
* [35] T. Kynkainniemi, T. Karras, M. Aittala, T. Aila, and J. Lehtinen. The role of imagenet classes in fr\'echet inception distance. In _arXiv preprint arXiv:2203.06026_, 2022.
* [36] J. Lee and J.-S. Lee. Trend: Truncated generalized normal density estimation of inception embeddings for gan evaluation. In _Eur. Conf. Comput. Vis._, pages 87-103, 2022.
* [37] H. Liu, Z. Dai, D. So, and Q. V. Le. Pay attention to mlps. _Adv. Neural Inform. Process. Syst._, 34:9204-9215, 2021.
* [38] H. Liu, W. Zhang, B. Li, H. Wu, N. He, Y. Huang, Y. Li, B. Ghanem, and Y. Zheng. Improving gan training via feature space shrinkage. _arXiv preprint arXiv:2303.01559_, 2023.
* [39] Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and B. Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In _Int. Conf. Comput. Vis._, pages 10012-10022, 2021.
* [40] Z. Liu, H. Mao, C.-Y. Wu, C. Feichtenhofer, T. Darrell, and S. Xie. A convnet for the 2020s. In _IEEE Conf. Comput. Vis. Pattern Recog._, pages 11976-11986, 2022.
* [41] S. Morozov, A. Voynov, and A. Babenko. On self-supervised image representations for gan evaluation. In _Int. Conf. Learn. Represent._, 2021.
* [42] C. Mou, X. Wang, L. Xie, J. Zhang, Z. Qi, Y. Shan, and X. Qie. T2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models. _arXiv preprint arXiv:2302.08453_, 2023.
* [43] T. Park, A. A. Efros, R. Zhang, and J.-Y. Zhu. Contrastive learning for unpaired image-to-image translation. In _Eur. Conf. Comput. Vis._, pages 319-345, 2020.
* [44] G. Parmar, R. Zhang, and J.-Y. Zhu. On aliased resizing and surprising subtleties in gan evaluation. In _IEEE Conf. Comput. Vis. Pattern Recog._, pages 11410-11420, 2022.
* [45] W. Peebles and S. Xie. Scalable diffusion models with transformers. _arXiv preprint arXiv:2212.09748_, 2022.
* [46] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al. Learning transferable visual models from natural language supervision. In _Int. Conf. Mach. Learn._, pages 8748-8763, 2021.
* [47] M. Raghu, T. Unterthiner, S. Kornblith, C. Zhang, and A. Dosovitskiy. Do vision transformers see like convolutional neural networks? In _Adv. Neural Inform. Process. Syst._, 2021.

* [48] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer. High-resolution image synthesis with latent diffusion models. In _IEEE Conf. Comput. Vis. Pattern Recog._, pages 10684-10695, 2022.
* [49] M. S. Sajjadi, O. Bachem, M. Lucic, O. Bousquet, and S. Gelly. Assessing generative models via precision and recall. _Adv. Neural Inform. Process. Syst._, 31, 2018.
* [50] A. Sauer, K. Chitta, J. Muller, and A. Geiger. Projected gans converge faster. _Adv. Neural Inform. Process. Syst._, pages 17480-17492, 2021.
* [51] A. Sauer, K. Schwarz, and A. Geiger. Stylegan-xl: Scaling stylegan to large diverse datasets. In _ACM SIGGRAPH_, pages 1-10, 2022.
* [52] A. Sauer, T. Karras, S. Laine, A. Geiger, and T. Aila. Stylegan-t: Unlocking the power of gans for fast large-scale text-to-image synthesis. _arXiv preprint arXiv:2301.09515_, 2023.
* [53] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. _Int. Conf. Learn. Represent._, 2015.
* [54] J. Song, C. Meng, and S. Ermon. Denoising diffusion implicit models. In _Int. Conf. Learn. Represent._, 2021.
* [55] Y. Song, J. Sohl-Dickstein, D. P. Kingma, A. Kumar, S. Ermon, and B. Poole. Score-based generative modeling through stochastic differential equations. In _Int. Conf. Learn. Represent._, 2021.
* [56] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna. Rethinking the inception architecture for computer vision. In _IEEE Conf. Comput. Vis. Pattern Recog._, pages 2818-2826, 2016.
* [57] H. Tang, H. Liu, D. Xu, P. H. Torr, and N. Sebe. Attentiongan: Unpaired image-to-image translation using attention-guided generative adversarial networks. _IEEE Trans. Neural Networks Learn. Syst._, 2021.
* [58] T. A. Tero Karras, Samuli Laine. Flickr-faces-hq dataset (ffhq). URL https://github.com/NWlabs/ffhq-dataset.
* [59] I. O. Tolstikhin, N. Houlsby, A. Kolesnikov, L. Beyer, X. Zhai, T. Unterthiner, J. Yung, A. Steiner, D. Keysers, J. Uszkoreit, et al. Mlp-mixer: An all-mlp architecture for vision. _Adv. Neural Inform. Process. Syst._, 34:24261-24272, 2021.
* [60] H. Touvron, M. Cord, M. Douze, F. Massa, A. Sablayrolles, and H. Jegou. Training data-efficient image transformers & distillation through attention. In _Int. Conf. Mach. Learn._, pages 10347-10357, 2021.
* [61] H. Touvron, P. Bojanowski, M. Caron, M. Cord, A. El-Nouby, E. Grave, G. Izacard, A. Joulin, G. Synnaeve, J. Verbeek, et al. Resmlp: Feedforward networks for image classification with data-efficient training. _IEEE Trans. Pattern Anal. Mach. Intell._, 2022.
* [62] J. Wang, C. Yang, Y. Xu, Y. Shen, H. Li, and B. Zhou. Improving gan equilibrium by raising spatial awareness. In _IEEE Conf. Comput. Vis. Pattern Recog._, pages 11285-11293, 2022.
* [63] P. Wang, Y. Li, and N. Vasconcelos. Rethinking and improving the robustness of image style transfer. In _IEEE Conf. Comput. Vis. Pattern Recog._, pages 124-133, 2021.
* [64] Z. Wang, H. Zheng, P. He, W. Chen, and M. Zhou. Diffusion-gan: Training gans with diffusion. _arXiv preprint arXiv:2206.02262_, 2022.
* [65] Z. Xia, X. Pan, S. Song, L. E. Li, and G. Huang. Vision transformer with deformable attention. In _IEEE Conf. Comput. Vis. Pattern Recog._, pages 4794-4803, 2022.
* [66] C. Yang, Y. Shen, Y. Xu, and B. Zhou. Data-efficient instance generation from instance discrimination. _Adv. Neural Inform. Process. Syst._, pages 9378-9390, 2021.
* [67] C. Yang, Y. Shen, Y. Xu, D. Zhao, B. Dai, and B. Zhou. Improving gans with a dynamic discriminator. In _Adv. Neural Inform. Process. Syst._, 2022.
* [68] S. Yang, L. Jiang, Z. Liu, and C. C. Loy. Unsupervised image-to-image translation with generative prior. In _IEEE Conf. Comput. Vis. Pattern Recog._, pages 18332-18341, 2022.
* [69] F. Yu, A. Seff, Y. Zhang, S. Song, T. Funkhouser, and J. Xiao. Lsun: Construction of a large-scale image dataset using deep learning with humans in the loop. _arXiv preprint arXiv:1506.03365_, 2015.
* [70] L. Zhang and M. Agrawala. Adding conditional control to text-to-image diffusion models. _arXiv preprint arXiv:2302.05543_, 2023.
* [71] J.-Y. Zhu, T. Park, P. Isola, and A. A. Efros. Unpaired image-to-image translation using cycle-consistent adversarial networks. In _Int. Conf. Comput. Vis._, pages 2223-2232, 2017.

Appendix

This _Supplementary Material_ is organized as follows: appendix B provides the implementation details of our experiments, appendix C demonstrates how human visual judgment is performed and presents the interface of our user study. appendix D presents additional quantitative and qualitative results, including 1) comparison results with more metrics (KID, Precision and Recall), 2) more results of MLP-based extractors, 3) the similarities between various representation spaces, as well as 4) more hierarchical results from different semantic levels of various extractors. Finally, in appendix E, we provide 2D plots of the correlation between different metrics with human visual judgment to make our results easier to parse.

## Appendix B Implementation Details

### Datasets

**FFHQ**[58] contains unique \(70,000\) human-face images with large variations in terms of age, ethnicity, and facial expressions. We employ the resolution of \(256\times 256\times 3\) for our experiments.

**ImageNet**[13] includes \(1,280,000\) images with \(1,000\) classes of different objects such as goldfish, bow tie, etc. All experiments on ImageNet are performed with the resolution of \(256\times 256\times 3\) unless otherwise specified.

**LSUN Church**[69] consists of \(126,227\) images of the church, varies in the background, perspectives, etc. We employ the resolution of \(256\times 256\times 3\) for our experiments.

### Experimental Settings and Hyperparameters

**Kernel selection**. We consistently employ the RBF kernel

\[K(\mathbf{x_{i}},\mathbf{x_{j}})=\exp(-\frac{\|\mathbf{x_{i}}-\mathbf{x_{j}}\| ^{2}}{2\sigma^{2}})\]

for calculating the CKA. The bandwidth \(\sigma\) is set as a fraction of the median distance between examples. In practice, three commonly used kernels could be employed for calculation, namely linear, polynomial, and RBF kernels. In order to investigate their difference, three publicly available models with clear performance margins are collected for evaluation. Concretely, we gather models of InsGen [66] trained on FFHQ with different data regimes (_i.e.,_\(2K\), \(10K\), \(140K\)), the ranking of their synthesis quality is clear and reasonable.

Tab. A1 demonstrates the quantitative results of CKA with different kernels. Obviously, these kernels give similar results and rankings. However, the RBF kernel contributes to the distinguishability of quantitative results, making the results more comparable. Consequently, the RBF kernel is employed in our experiments.

**ViT features calculation**. The feature maps of ViT-based extractors are three-dimensional tensors (N, W, C), where W contains the global token and local features. The global token captures the same semantic information as the local features. Thus the global taken features are used for computation in implementation. Tab. A2 shows the comparison results of using local features and the global token. Consistently, they give similar results and rankings, so we use the global token for calculation in our experiments.

**Feature normalization**. In practice, the activations of features play an essential role in computing the similarity index. Namely, the quantitative results would be dominated by a few activations with large peaks, neglecting other correlation patterns [63]. To investigate the activations of our self-supervised extractors, we visualize the activations of different samples and their statistics.

Fig. A1 and Fig. A2 respectively illustrate the activation of different samples and their statistics. Obviously, there are several peaks in the activations. And these peaks may dominate the similarity index as they are substantially larger than other activations. To mitigate the peaks and create a more uniform distribution, we employ the softmax transformation [63] to normalize the features. Such operation smooths the activations while maintaining the original distributional information of features. Thus the similarity index remains consistent to deliver the distribution discrepancy. Besides the softmax transformation, we also compare the behavior of different normalization techniques (_i.e.,_\(L_{1}\) and \(L_{2}\) normalization).

Tab. A3 demonstrate the quantitative results with different normalization techniques. They consistently provide similar results and rankings, and the softmax transformation ameliorates the peaks more significantly, providing more comparable results. Consequently, we adopt Softmax normalization in our experiments.

**Histogram matching.** In order to investigate the robustness of the measurement system, we employ the histogram matching [35] to attack the system. To be specific, a subset with a considerable number (_e.g.,_\(50K\)) of images is chosen as the referenced distribution, and the corresponding class distribution is predicted by a given classifier (_i.e.,_ Inception-V3 [56]. With the guidance of the classifier, the generator is encouraged to produce a synthesis distribution that matches the predicted class distribution of real images. Recall that the generator used to produce these synthesized distributions stays unchanged, thus a robust measurement system should give consistent similarities between the randomly generated and the matched distribution.

Fig. A3 provides the class distribution of real and synthesized FFHQ images predicted by Inception-V3. Obviously, the class distribution of the matched distribution is well-aligned with the predicted real distribution.

**Sample-efficiency.** In order to investigate the impacts of the number of synthesized samples, we compute the distributional distances between the real distribution with synthesized distributions with various numbers of generated images. Concretely, FFHQ (with \(70K\) images) and ImageNet (with \(1.28\) million images) are investigated for universal conclusions. For both datasets, we synthesis \(500K\) images as candidate, and randomly choose \(5K\), \(10K\), \(50K\), \(100K\), \(250K\), and \(500K\) images as the synthesized distribution for computing the metrics. The entire training data is utilized as the real distribution, and the publicly accessible models on FFHQ3 and ImageNet4 are employed.

Footnote 3: https://github.com/NVlabs/stylegan3

Footnote 4: https://github.com/autonomousvision/stylegan-xl

**The curve of FD and CKA under various data regimes on ImageNet dataset** is shown in Fig. A4. Consistent with the aforementioned results in the main paper, CKA could measure the distributional distances precisely with only \(5K\) samples, whereas FID fails to deliver the actual measurement until sufficient samples are used. That is, CKA could give reliable results even when limited data is given, suggesting impressive sample efficiency. Equipped with the bounded quantitative results and consistency under different data regimes, as well as the robustness to the histogram matching attack, CKA outperforms FID as a reliable distance for delivering the distributional discrepancy.

## Appendix C User Preference Study

Here we present more details about our human perceptual judgment. Recall that two strategies are designed for different investigations, namely benchmarking the synthesis quality of one specific generative model and comparing two paired generative models. Fig. A5 shows the user interface for benchmarking the synthesis quality of one specific generative model (_i.e.,_ BigGAN on ImageNet here). To be more specific, considerable randomly generated images are shown to the user, and the user is required to determine the fidelity of synthesized images. We then obtain the final scores by averaging the judgments of the participants (_i.e.,_ 100 individuals).

Fig. A6 and Fig. A7 show the human evaluation results on FFHQ and ImageNet dataset, respectively. The percentages denote how many samples of the selected images are considered photo-realistic. Together with the quantitative results in our main paper, we could tell that the proposed metric shows a better correlation with human visual comparison.

Recall that in our main paper, we find that our evaluation system gives the opposite ranking to the existing metric (_i.e.,_ FID) in some circumstances. For instance, the synthesis quality of ICGAN is determined basically the same as that of the class-conditional ICGAN (C-ICGAN) under our evaluation, whereas the FID votes C-ICGAN for the much better one. We thus conduct the other userstudy to compare two paired generative models. Concretely, we prepare groups of paired images of different generative models and ask 100 individuals to assess which model could produce high-quality images. The same groups are repeated several times by changing the order of images, ensuring the human evaluation is reliable and consistent.

Fig. A8 provides the interface of comparing two paired generative models, users are asked to choose which set of images looks more plausible. Additionally, Fig. A9 shows the pipeline of analyzing the paired comparison results. Specifically, the same groups of images are repeated for \(4\) times in random order and users are shown \(16\) images from two models to determine the more photorealistic one. In this way, the results of choosing both Projected-GAN and StyleGAN2 two times are identified as indistinguishable for enduring the consistency. Namely, the users choose different rankings between the two sets when the order of images is changed, which does not meet the consistency. Consequently, the final scores for paired comparison are obtained by quantifying the percentage of the human preferences that correlate the consistency.

## Appendix D More Quantitative and Qualitative Results

In this section, we provide more quantitative and qualitative results to further demonstrate the efficacy of our newly developed measurement system.

**Comparing with more metrics.** In order to further investigate the efficacy of our proposed metric, we we further involve Kernel Inception Distance (KID) [4], precision, and recall [49] into our comparison. Note that the original KID employs Inception-V3 as the feature extractor, and there is a large "perceptual null space" in Inception-V3. Therefore, we first investigate whether KID scores can be altered by attacking the feature extractor with the histogram matching mechanism. The experimental details are consistent with computing Frechet Distance (FD\({}_{\downarrow}\)) in Tab.2 of the main paper. Tab.A4 presents the quantitative results. Still, some extractors, such as Inception, Swin-Transformer, and ResMLP, are susceptible to the histogram matching attack. For instance, the KID score of Swin-Transformer is improved by 5.31% when the chosen set is used. These observations agree with our findings in our main paper, suggesting that certain extractors can be hacked when KID is employed as the distributional distance. Then, we investigate the sample efficiency of KID, Precision, and Recall to probe the impacts of the amount of generated samples. Fig.A10 presents the curves of KID, Precision, and Recall scores computed under different data regimes. Similarly, we could observe that the KID scores can be improved by synthesizing more images. Interestingly, the recall scores decrease as the generated sample size increases whereas the precision is stable. This is caused by the definition of recall: recall measures the proportion of the real distribution that is covered bythe synthesized distribution. In practical computation, the denominator increases as the synthesized samples increases, while the numerator (_i.e.,_ images from the real distribution) remain unchanged. In this way, the recall scores decrease as the generated sample size increases and vice versa. By contrast, CKA scores are stable under different data regimes, (please see Fig. 2 in the main paper). Moreover, CKA can provide reliable synthesis evaluation that agrees with human visual judgment. Accordingly, CKA is a proper choice for building a consistent and reliable measurement system.

**More hackability results of Frechet Distance on ImageNet.** In addition to evaluating the robustness of these extractors on the FFHQ dataset, we further perform the same experiment on the ImageNet dataset. Tab. A5 presents the quantitative results. We can tell from these results that the chosen feature extractors are robust to the attack, further demonstrating their reliability.

**More results of MLP-based extractors.** We further incorporate two MLP-based models as the feature extractor for synthesis evaluation, namely namely gMLP [37] and MLP-mixer [59]. Following

[MISSING_PAGE_FAIL:20]

**Similarities between various representation spaces.** Recall that we filtered out extractors that define similar representation spaces to avoid redundancy in the main paper. The correlation betweenrepresentations of high dimension in different feature extractors is calculated following [32]. In particular, a considerable number of images (_i.e.,_\(10K\) images from ImageNet) are fed into these extractors for computing their correspondence.

Fig. A11 shows the similarity of their representations. Obviously, the representations of CLIP-ResNet and MoCo-ResNet have higher similarity with other extractors. Considering these two extractors are both CNN-based and they capture similar semantics with other CNN-based extractors, we remove the CLIP-ResNet and MoCo-ResNet to avoid redundancy. Accordingly, we obtain a set of feature extractors that 1) capture rich semantics in a complementary way, 2) are robust toward the histogram matching attack, and 3) define meaningful and distinctive representation spaces for synthesis comparison. The following table presents these feature extractors. These extractors, including both CNN-based and ViT-based architectures, have demonstrated strong performance in pre-defined and downstream tasks, facilitating more comprehensive and reliable evaluation. Notably, the inclusion of self-supervised extractors SWAV, CLIP-V, and MoCo-V aligns with previous findings [41, 35, 3]. This selection of feature extractors provides a diverse and complementary set of representations, enabling a more comprehensive and reliable evaluation of synthesis quality in generative models.

**More results of hierarchical levels from various extractors.** Tab. A7, Tab. A8, Tab. A9, Tab. A10, and Tab. A11 respectively present the heatmaps and quantitative results of various semantic levels. We could tell that despite the Frechet Distance (FD) scores consistently reflect synthesis quality, their numerical values fluctuate dramatically. On the contrary, CKA provides normalized distances _w.r.t_ the numerical scale across various levels. Also, the heatmaps from various semantic levels reveal that hierarchical features encode different semantics. Such observation provides interesting insights that feature hierarchy should be also considered for synthesis comparison. Notably, benefiting from the bounded quantitative results, CKA demonstrates great potentials for comparison across hierarchical layers.

\begin{table}
\begin{tabular}{l|c c|c c|c c} \hline \hline Model & \multicolumn{2}{c|}{BigGAN} & \multicolumn{2}{c|}{BigGAN-deep} & \multicolumn{2}{c}{StyleGAN-XL} \\ \hline Layer & FD\({}_{\downarrow}\) & CKA\({}_{\uparrow}\) & FD\({}_{\downarrow}\) & CKA\({}_{\uparrow}\) & FD\({}_{\downarrow}\) & CKA\({}_{\uparrow}\) \\ \hline Layer\({}_{1}\) & 0.67 & 99.90 & 0.46 & 99.91 & 0.07 & 99.99 \\ Layer\({}_{2}\) & 0.87 & 97.89 & 0.60 & 98.87 & 0.31 & 99.51 \\ Layer\({}_{3}\) & 16.15 & 95.60 & 12.02 & 96.21 & 1.90 & 98.15 \\ Layer\({}_{4}\) & 11.18 & 86.10 & 8.69 & 87.71 & 1.85 & 92.54 \\ \hline Overall & N/A & 94.87 & N/A & 95.68 & N/A & 97.55 \\ \hline \hline \end{tabular}
\end{table}
Table A9: **Heatmaps from various semantic levels on FFHQ dataset (_left_)** and **quantitative results of Fréchet Distance (FD \(\downarrow\)) and Centered Kernel Alignment (CKA \(\uparrow\)) on ImageNet dataset (_right_)**. SWAV [6] serves as the feature extractor for hierarchical evaluation here.

[MISSING_PAGE_FAIL:24]

Figure A13: **The correlation of the averaged ranks of various models on FFHQ and LSUN-Church given by human judgment, CKA, and FID.**