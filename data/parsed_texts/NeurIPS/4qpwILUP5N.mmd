# A foundation for exact binarized morphological neural networks

Theodore Aouad

Universite Paris-Saclay, CentraleSupelec, Inria, CVN

3 rue Joliot Curie, Gif-sur-Yvette, France

theodore.aouad@centralesupelec.fr

Hugues Talbot

Universite Paris-Saclay, CentraleSupelec, Inria, CVN

3 rue Joliot Curie, Gif-sur-Yvette, France

hugues.tablot@centralesupelec.fr

###### Abstract

Training and running deep neural networks (NNs) often demands significant computation and energy-intensive specialized hardware (e.g. GPU, TPU...). One way to reduce the computation and power cost is to use binary weight NNs, but these are hard to train because their derivatives have a non-smooth gradient. We present a model based on Mathematical Morphology (MM), which can binarize ConvNets without loss of performance under certain conditions, but these conditions may not be easy to satisfy in real-world scenarios. To ameliorate this, we propose two new approximation methods and develop a robust theoretical framework for ConvNets binarization using MM. We also propose several regularization losses to improve the optimization. We empirically show that our model can learn a complex morphological network, and explore its performance on a classification task.

## 1 Introduction

Binary weight neural networks (BWNNs) are attractive because they can provide powerful machine learning solutions with much less storage, computation, and energy consumption than conventional networks [21]. Several methods, such as BinaryConnect [5], DoReFa-Net [27], and XNOR-Net [21], have shown good to excellent results in a variety of applications. These networks usually use the sign function to binarize the weights in the forward pass. They must use a special gradient function, like the Straight-Through Estimator (STE) [3], to overcome the zero gradient problem of the sign function during the backward pass. However, this estimator, while effective in practice, lacks a solid theoretical basis, suggesting the need for a different approach. Some methods avoid using the STE by first training a floating-point neural network and then binarizing it afterwards [12, 8]. However, this approach may lead to approximate binarization and a drop in performance. In this paper, we present a new approach that uses the concepts of Mathematical Morphology (MM) [23] to overcome the drawbacks of existing methods. MM, based on modern set theory and complete lattices, offers a non-linear mathematical framework for image processing. Its basic operators, erosion and dilation, are equivalent to thresholded convolutions [14], underscoring a link between MM and deep learning. Combining these fields can improve the efficiency and results of morphological operations while enhancing our knowledge of deep learning [1]. Recent works on morphological neural networks have explored learning operators and structuring elements using various approaches, such as the max-plus definition [19, 6] and differentiable approximations [24, 18, 9]. However, these methods primarily focus on learning gray-scale MM operators and have not focused on NN binarization.

[MISSING_PAGE_FAIL:2]

### Morphological Equivalence

We now express the conditions under which a BiSE neuron can be seen as a morphological operator.

**Theorem 2.3** (Dilation - Erosion Equivalence).: _For a given structuring element (SE) \(S\subset\Omega\), and an almost binary parameter \(\delta\in]0,\frac{1}{2}]\), a set of reparametrized weights \(\mathbf{W}\in\mathbb{R}^{\Omega}\) and bias \(b\in\mathbb{R}\), we define:_

\[L_{\oplus}(\mathbf{W},S) \coloneqq\sum_{k\in\Omega\setminus S}[\mathbf{W}_{k}]_{+}+\Big{(} \frac{1}{2}-\delta\Big{)}\sum_{s\in S}[\mathbf{W}_{s}]_{+} \tag{4}\] \[U_{\oplus}(\mathbf{W},S) \coloneqq\Big{(}\frac{1}{2}+\delta\Big{)}\min_{s\in S}\mathbf{W} _{s}+\sum_{k\in\Omega}[\mathbf{W}_{k}]_{-}\] (5) \[U_{\ominus}(\mathbf{W},S) \coloneqq\sum_{k\in\Omega}\mathbf{W}_{k}-L_{\oplus}(\mathbf{W},S) \tag{6}\]

\[L_{\ominus}(\mathbf{W},S) \coloneqq\sum_{k\in\Omega}\mathbf{W}_{k}-U_{\oplus}(\mathbf{W},S) \tag{8}\]

_Let \(\psi\in\{\oplus,\ominus\}\) be a dilation or erosion. Then:_

\[\forall I\in\mathcal{I}(\delta)\;,\;\psi_{S}\Big{(}\mathbf{I}>\frac{1}{2}\Big{)} =\Big{(}\mathbf{I}\otimes W>b\Big{)}\Leftrightarrow L_{\psi}(\mathbf{W},S) \leq b<U_{\psi}(\mathbf{W},S). \tag{9}\]

_In this case, \(\forall s\in S,\mathbf{W}_{s}\geq 0\) and \(b\geq 0\) and we say that a BiSE \(\chi\) with weights \(W(\omega)=\mathbf{W}\) and \(B(\beta)=b\) is **activated**. If \(\psi=\oplus\), then \(B(\beta)\leq\frac{1}{2}\sum_{k\in\Omega}W(\omega)_{k}\). If \(\psi=\ominus\), then \(B(\beta)\geq\frac{1}{2}\sum_{k\in\Omega}W(\omega)_{k}\). For any almost binary image \(\mathbf{I}\in\mathcal{I}(\delta)\), \(\chi(\mathbf{I})\in\mathcal{I}(\delta_{out})\) is almost binary with known parameter \(\delta_{out}\). Finally_

\[\forall\mathbf{I}\in\mathcal{I}(\delta),\Big{(}\chi(\mathbf{I})>\frac{1}{2} \Big{)}=\psi_{S}\Big{(}\mathbf{I}>\frac{1}{2}\Big{)}. \tag{10}\]

This theorem states that if a BiSE is activated, it transforms an almost binary inputs into an almost binary outputs with known \(\delta_{out}\). Moreover, if we threshold the input and output with respect to \(\frac{1}{2}\), it is equivalent to applying the corresponding morphological operation, exhibiting a weak commutativity property between thresholding and convolution. Further, equation (10) shows that if a BiSE is activated for operation \(\psi\), performing the BiSE operation in \(\mathcal{I}(\delta)\) is equivalent to performing the binary morphological operation \(\psi\) in the binary space \(\mathcal{P}(S)\). This presents a natural framework for binarization (see SS3).

### Binary Morphological Neural Network

Our objective is to build a binarizable neural network. We now define binarizable neural layers based on the BiSE neuron. By combining these layers, we can create flexible architectures tailored to the desired task. As mentioned earlier, the BiSE neuron resembles a convolution operation. However, a single convolution is insufficient to create a morphological layer. In this context, we explain how we handle multiple channels. We observe that the BiSE neuron can be used to define a layer that learns the intersection or union of multiple binary images \(\mathbf{x}_{1},...,\mathbf{x}_{n}\in\mathcal{I}(\delta)\). For example, their union can be expressed as the dilation of the 3D image \(\mathbf{x}\coloneqq(\mathbf{x}_{1},...,\mathbf{x}_{n})\in\mathcal{I}(\delta) \subset\big{(}[0,1]^{\Omega_{I}}\big{)}^{n}\) with a tubular SE applied solely across the dimension of depth. Therefore, we define the Layer Union Intersection (LUI) as a special case of the BiSE layer, with weights restricted to deep-wise shape. It is analogousto a \(1\times 1\) convolution unit. A LUI layer can learn any intersection or union of any number of almost binary inputs. By combining BiSE neurons and LUI layers, we can learn morphological operators and aggregate them as unions or intersections.

**Definition 2.4** (BiSEL).: A BiSEL (BiSE Layer) is the combination of multiple BiSE and multiple LUI. Let \((\chi_{n,k})_{n,k}\) be \(N*K\) BiSE and \((\text{LUI}_{k})_{k}\) be \(K\) LUI. Then we define a BiSEL as:

\[\phi:\quad\mathbf{x}\in\left([0,1]^{\Omega_{I}}\right)^{N}\mapsto\left(\text{ LUI}_{k}\Big{[}\big{(}\chi_{n,k}(\mathbf{x}_{n})\big{)}_{n}\Big{]}\right)_{k}. \tag{11}\]

The BiSEL mimics a convolutional layer. In conventional ConvNets, to process multiple input channels, a separate filter is applied to each channel, and their outputs are summed to create one output channel. In the case of BiSEL, instead of summing the results of each filter, we perform a union or intersection operation (see Figure 1).

DenseLUIthe LUI layer is similar to a \(1\times 1\) convolution, which is equivalent to a fully connected layer. Given an input vector \(\mathbf{x}\in\mathbb{R}^{n}\), we can apply the LUI layer to the reshaped input \(\hat{\mathbf{x}}\in\mathbb{R}^{n\times 1\times 1}\) treating it as a 2D image with width and length of \(1\) and \(n\) channels, respectively. Therefore, we can utilize the BiSEL to create binarizable fully connected layers.

Gray-Scale / RGB InputsUp until now, all inputs were assumed binary. We extend these definitions to gray-scale and RGB images. The main idea is to separate an input channel \(\mathbf{I}_{c}\in\mathbb{R}^{\Omega_{I}}\) into its set of upper level-sets to come back to binary inputs:

\[\{(\mathbf{I}_{c}\geq\tau)\mid\tau\in\mathbb{R}\}. \tag{12}\]

Considering all possible values of \(\tau\) from a continuous image would result in an excessive number of level-sets to process. Alternatively, we can define a finite set of values for \(\tau\) in advance. Subsequently, each channel of an image \(\mathbf{I}\in\mathbb{R}^{c\times w\times l}\) is separated into its corresponding level-sets, and these level-sets are provided as additional channels. If we have \(N\) values for level-set, the resulting input is a binary image \(I_{\mathbb{B}}\in\{0,1\}^{(N\cdot c)\times w\times l}\).

We have introduced two types of binarizable layers: the DenseLUI, which is similar to a fully connected layer, and the BiSEL, which resembles a convolutional layer. By combining these layers, we can create a Binary Morphological Neural Network (BiMoNN), which encompasses various architectures su

Figure 1: BiSEL vs Conv Layer. Input \(\mathbf{x}\) with 3 channels. Output \(\phi(\mathbf{x})\) with 2 channels.

Figure 2: Gray to level-set for 5 different values, generating 5 input channels.

### Training considerations

The BiNoNN (\(\Gamma_{\mathbb{R}}\)) is fully differentiable. If \(\mathcal{L}\) is a differentiable loss function, given a dataset of \(N\) labeled images \(\{(\mathbf{x}_{i},\mathbf{y}_{i})\}\), we minimize the error \(\frac{1}{N}\sum_{i=1}^{N}\mathcal{L}(\Gamma_{\mathbb{R}}(\mathbf{x}_{i}), \mathbf{y}_{i})\) using a gradient descent algorithm (like Adam [13]). The gradients are computed with the backpropagation algorithm [22]. The binarization scheme, which is defined in the next section, is applied post-training or during training to measure the model's evolution.

Our objective is to reach the set of activable weights and bias. Theorem 2.3 indicates that we only have to look at positive parameters. We can enforce them to be positive by setting \(W\) and \(B\) as the softplus function. Other reparametrizations are proposed in Appendix B.

\[B(\cdot)=W(\cdot)\coloneqq f^{+}(\cdot)\coloneqq\log(1+\exp(\cdot)). \tag{13}\]

## 3 Binarization

Binarization of a neural network involves converting the real-valued weights and activations, typically stored as 32-bit floats, into binary variables represented as 1-bit booleans. Most binary NN use variables in \(\{-1,1\}\)[26], which are not ideal for learning morphological operations. We instead utilize \(\{0,1\}\). BiMoNNs inherently correspond to binarized networks, making the BiSE neuron a natural framework for binarization when dealing with binary inputs. If a specialized hardware tailored for morphological operations is available, it can offer significant improvements in efficiency and performance, facilitating the binarization process. Alternatively, dilation and erosion can be expressed using binary-weighted thresholded convolutions. In our approach, binarization occurs after the training phase. We present two types of binarization for the BiSE neuron: the exact method (as introduced in [2]), when the BiSE neuron is activated, and two novel approximated methods. Then, we sequentially binarize the entire network.

### Exact BiSE Binarization

The real-value operations performed by an activated Binary Structuring Element (BiSE) in the almost binary space can be replaced with binary morphological operations on the binary space after thresholding at 0.5, without sacrificing performance (as per Theorem 2.3). To determine if a BiSE is activated and which operation it corresponds to, we introduce Proposition 3.1, which provides a linear complexity method for extraction.

**Proposition 3.1** (Linear Check).: _Let us assume the BiSE of weights \(W(\omega)\) and \(B(\beta)\) is activated for \(\psi\) with SE \(S\subset\Omega\) for almost binary images \(\mathcal{I}(\delta)\). Then \(S=(W(\omega)>\tau_{\psi})\) with_

\[\tau_{\oplus} \coloneqq\frac{1}{\frac{1}{2}+\delta}\Big{(}B(\beta)-\sum_{W( \omega)}[w]_{-}\Big{)}, \tag{14}\] \[\tau_{\ominus} \coloneqq\frac{1}{\frac{1}{2}+\delta}\Big{(}\sum_{W(\omega)}[w]_ {+}-B(\beta)\Big{)}. \tag{15}\]

Given a BiSE neuron \(\chi\) and an almost binary output in \(\mathcal{I}(\delta)\), we check if \(\chi\) is activated for \(S_{\oplus}\) or \(S_{\ominus}\), where \(S_{\oplus}=(W(\omega)>\tau_{\oplus})\) and \(S_{\ominus}=(W(\omega)>\tau_{\ominus})\). If yes, we binarize by replacing \(\chi\) with the corresponding morphological operator. If no, we use proposition 3.1 to confirm that \(\chi\) is not activated, and we approximate the binarization using the methods in section 3.2. The exact method requires only the computation of \(L_{\oplus},U_{\oplus},L_{\ominus},U_{\ominus}\) and at most \(\mathcal{O}(|\Omega_{S}|)\) operations.

### Approximate BiSE Binarization

In practice, BiSE are not always activated, necessitating an approximate binarization method. Let \((\widehat{\mathbf{W}},\widehat{b},\widehat{p})\coloneqq(\mathbf{W}(\hat{ \omega}),B(\hat{\beta}),\widehat{p})\) be the learned reparametrized parameters.

#### 3.2.1 Projection onto activable parameters

To find the closest morphological operation, we minimize the Euclidean distance \(d\) for a given \(\psi\in\{\oplus,\ominus\}\) and the set \(A_{\psi,S}\) of activable parameters:

[MISSING_PAGE_FAIL:6]

[MISSING_PAGE_FAIL:7]

From this, we can define two regularization loss which can be computed without slowing the training down:

\[\mathcal{L}_{\text{morpho}}=\mathcal{L}_{\text{unif}}:=\sum_{i\in \Omega}\mathbf{W}_{i}-\frac{1}{S_{u}}\Big{(}\sum_{s\in S_{u}}\mathbf{W}_{s} \Big{)}^{2}, \tag{32}\] \[\mathcal{L}_{\text{morpho}}=\mathcal{L}_{\text{normal}}:=\sum_{i \in\Omega}\mathbf{W}_{i}-\frac{1}{S_{n}}\Big{(}\sum_{s\in S_{n}}\mathbf{W}_{s} \Big{)}^{2}. \tag{33}\]

## 5 Experiments

In this section, we empirically validate the capabilities of BiMoNNs in learning a binarized morphological pipeline through a denoising task, without the need for regularization. We also evaluate the model and regularization techniques on the MNIST classification task.

Binary DenoisingWe generate a second dataset to evaluate the denoising capacity of BiMoNNs. The target images in this dataset consist of randomly-oriented segments with width \(h\), with added Bernoulli noise. To filter these images, an MM expert would use a union of opening operations, where the SEs are segments with width \(1\) and angle one of \((0^{\circ},90^{\circ},-45^{\circ})\). The SEs should be longer than the noise and shorter than the smallest stick in the target image (usually a length of \(5\)). Examples are given in Figure 2(a). Our architecture uses two consecutive BiSEL layer of kernel size 5 and 3 hidden channels (see Figure 2(b)). We optimize the MSE Loss, with an initial learning rate of \(0.01\). We train over 6000 iterations and halve the learning rate after 700 iterations with non-diminishing loss. We stop once the loss does not decrease after 2100 iterations. We employ a positive reparametrization for the bias and a dual reparametrization for the weights, and binarize with the projection onto activable parameters. The network achieves excellent denoising performance, with a DICE score of 97.5%. The remaining 2.5% discrepancy is due to artifacts between the sticks that cannot be denoised using an opening operation. The binarized network (Figure 2(b)) accurately learns the intersection of three openings (which remains an opening) the same way a human expert would combine such operators to achieve the denoising task optimally. Additionally, 4 out of the 6 BiSE are activated during the process. This experiment shows that our network can learn accurate and interpretable composition of morphological operators.

Figure 3: Binary denoising experiment.

ClassificationWe conduct classification experiments on the MNIST dataset [16]. All images are thresholded at 128. Our BiMoNN model comprises one hidden DenseLUI layer with 4096 neurons. To handle the large number of parameters, we adopt the fast projection defined in SS3.2. We compare the classification accuracy of our float and binarized models against the SOTA and baseline models with fully connected layers and 1D batch normalization [11], employing \(\frac{1}{2}(\tanh(\cdot)+1)\) as the activation layer. The accuracy results are summarized in Table 1. In our framework, binarizing the weights also entails binarizing the activations. Consequently, binarizing the last layer would yield binary decision outputs for each output neuron, possibly leading to multiple labels with a score of \(1\). To overcome this issue, we refrain from binarizing the last layer, thus retaining the real-valued activations. This decision affects a negligible proportion of parameters (\(\simeq\)0.1%). In traditional classification neural networks, the softmax activation is commonly used at the end of the last layer to produce the final probability distribution over the classes. However, in the BiMoNN architecture, we utilize the same activation function as the hidden layers, which is the normalized \(\tanh\). Additionally, we compare the performance of our BiMoNN model when replacing the last normalized \(\tanh\) activation with a softmax layer. When using the normalized \(\tanh\), \(\mathcal{L}_{data}\) is the Binary Cross-Entropy loss \(\mathcal{L}_{BCE}\), and when using the softmax, we use the Cross-Entropy loss \(\mathcal{L}_{CE}\):

\[\mathcal{L}_{BCE}(\hat{\mathbf{y}}_{i},\mathbf{y}_{i}^{*}) \coloneqq\sum_{c=0}^{9}\mathbf{y}_{i}^{*}\log(\hat{\mathbf{y}}_{ i})+(1-\mathbf{y}_{i}^{*})\log(1-\hat{\mathbf{y}}_{i}), \tag{34}\] \[\mathcal{L}_{CE}(\hat{\mathbf{y}}_{i},\mathbf{y}_{i}^{*}) \coloneqq\sum_{c=0}^{9}\mathbf{y}_{i}^{*}\log(\hat{\mathbf{y}}_{ i}). \tag{35}\]

We conduct a comprehensive random search to identify the optimal hyperparameter configuration for the Binary Morphological Neural Network (BiMoNN). The hyperparameters explored include the learning rate, last activation function (Softmax layer vs. normalized \(\tanh\)), positive vs no reparametrization. Additionally, we investigate regularization losses, such as no regularization, \(\mathcal{L}_{\text{exact}}\), \(\mathcal{L}_{\text{uni}}\), and \(\mathcal{L}_{\text{normal}}\). If regularization is applied, only positive weight reparametrization is considered. We vary the coefficient \(c\) in the regularization loss and explore different batch value starting time for when we start applying regularization during training. For each regularization schema, we select the model with the best binary validation accuracy, and the corresponding results are displayed in Table 1. Detailed hyperparameter configurations and hyperparameters study are provided in Appendix E. Applying the softplus reparametrization to the weights led to a slight increase in the floating-point error (2.2% vs. 2.8%). Similar findings were observed in [20] for non-negative neural networks in a different task. Generally, positive neural networks exhibit lower accuracy but offer enhanced robustness and interpretability [4]. In our case, it significantly improved the binarized results, along with a substantial increase in the rate of activated BiSE neurons, rising from a median of 1.5% to 10% with softplus. Without imposing positivity, the binarized network performed randomly. We analyze the impact of regularization on the performance of the binarized model, which improves as expected. The float accuracy also increases, given that we select the model with the best binary accuracy on validation. Surprisingly, \(\mathcal{L}_{\text{uni}}\) and \(\mathcal{L}_{\text{normal}}\) outperform \(\mathcal{L}_{\text{exact}}\), despite being designed as approximations. This discrepancy might be due to the number of searches performed: \(\mathcal{L}_{\text{exact}}\) performed only 42 searches, while other configurations went through 100 searches. However, we have not yet achieved parity with the baseline for the float model or reached the state-of-the-art for

\begin{table}
\begin{tabular}{c l l l l} \hline \hline  & Architecture & Params & \(\mathbb{R}\) & \(\mathbb{B}\) \\ \hline \multirow{4}{*}{Ours} & DLUI (\(W=\text{Id}\)) & 3.3 M & **2.2\%** & 90.2\% \\  & DLUI (No Regu) & 3.3 M & 4.6\% & 10.1\% \\  & DLUI \(\mathcal{L}_{\text{exact}}\) & 3.3 M & 4.0\% & 7.3\% \\  & DLUI \(\mathcal{L}_{\text{unif}}\) & 3.3 M & 3.6\% & **4.5\%** \\  & DLUI \(\mathcal{L}_{\text{normal}}\) & 3.3 M & 2.8\% & 4.6\% \\ \hline \multirow{3}{*}{SOTA} & EP 1fc [15] & 3.3 M & - & 2.8\% \\  & BinConnect [5] & 10 M & - & **1.3\%** \\  & BNN [10] & 10 M & - & 1.4\% \\ \hline \multirow{2}{*}{Float} & FC (4096) & 3.3 M & 1.5 \% & - \\  & FC (2048x3) [10] & 10 M & **1.3\%** & - \\ \hline \hline \end{tabular}
\end{table}
Table 1: Accuracy error on test set for MNIST classification, with float error \(\mathbb{R}\) and binarized error \(\mathbb{B}\).

the binarized model. With \(4.5\%\) error compared to \(2.8\%\) for the same number of parameters, this emphasizes the need for improved architecture, better regularization techniques, or exploration of alternative optimization methods.

### Discussion

The state-of-the-art BWNN methods commonly rely on the XNOR operator to emulate multiplications. However, this algebraic framework proves unsuitable for morphological operators, as it contradicts the set-theoretical principles of morphological operations. In our experiments, we observed that the BNN operator [10] failed to learn even the simplest dilation operator. Furthermore, the performance of the state-of-the-art method on the denoising task was unsatisfactory, with a DICE coefficient of only approximately 0.3, indicating the need for improved approaches in handling morphological operations. In contrast, our findings reveal that the float BiMoNN exhibits enhanced binarization capabilities when trained to closely approximate a set of morphological operators. As a result, the float BiMoNN naturally acquires morphological properties, leading to a more effective subsequent binarization process. However, when applied to the classification of the MNIST dataset, the resulting float BiMoNN does not retain morphological characteristics, causing a noticeable performance degradation after binarization. To address this issue, we emphasize the importance of positive reparametrization and applying morphological regularization. By incorporating these techniques, we significantly improved the model's overall performance and mitigate the accuracy loss upon binarization. This shows the potential of our proposed BiMoNN framework in leveraging morphological properties and offers insights into the development of more effective BiMoNN models for many and diverse tasks.

## 6 Conclusion

In this paper, we have presented a novel, mathematically justified approach to binarize neural networks using the Binary Morphological Neural Network (BiMoNN), achieved by leveraging mathematical morphology. Our proposed method establishes a direct link between deep learning and mathematical morphology, enabling binarization of a wider set of architectures, without performance loss under specific activation conditions and providing an approximate solution when these conditions are not met. Through our experiments, we have demonstrated the effectiveness of our approach in learning morphological operations and achieving high accuracy in denoising tasks, surpassing state-of-the-art techniques that rely on the straight-through estimator (STE). Furthermore, we proposed and evaluated three practical regularization techniques that aid in converging to a morphological network, showcasing their efficacy in a classification task. Additionally, we introduced a fourth regularization technique that, though promising in theory, currently faces computational challenges. We will adress these shortcoming in future work. Despite promising results, there is still room for enhancing both the floating point and binary modes of our network. As well, diverse architectures, such as incorporating convolution layers, could be explored to further improve the performance and applicability of BiMoNN. Overall, our research lays the foundation for advancing the field of binarized neural networks with a morphological perspective, offering valuable insights into developing more powerful and efficient models for a wide range of tasks.

## References

* [1] Jesus Angulo. Some open questions on morphological operators and representations in the deep learning era. In _International Conference on Discrete Geometry and Mathematical Morphology_, pages 3-19. Springer, 2021.
* [2] Theodore Aouad and Hugues Talbot. Binary morphological neural network. In _2022 IEEE International Conference on Image Processing (ICIP)_, pages 3276-3280, 2022.
* [3] Yoshua Bengio, Nicholas Leonard, and Aaron C. Courville. Estimating or propagating gradients through stochastic neurons for conditional computation. _CoRR_, abs/1308.3432, 2013.
* [4] Jan Chorowski and Jacek M Zurada. Learning understandable neural networks with nonnegative weight constraints. _IEEE transactions on neural networks and learning systems_, 26(1):62-69, 2014.
* [5] Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre David. Binaryconnect: Training deep neural networks with binary weights during propagations. In C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 28. Curran Associates, Inc., 2015.
* [6] Gianni Franchi, Amin Fehri, and Angela Yao. Deep morphological networks. _Pattern Recognition_, 102:107246, 2020.
* [7] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In _2015 IEEE International Conference on Computer Vision (ICCV)_, pages 1026-1034, 2015.
* [8] Xiangyu He, Zitao Mo, Ke Cheng, Weixiang Xu, Qinghao Hu, Peisong Wang, Qingshan Liu, and Jian Cheng. Proxybnn: Learning binarized neural networks via proxy matrices. In _Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part III 16_, pages 223-241. Springer, 2020.
* [9] Romain Hermary, Guillaume Tochon, Elodie Puybareau, Alexandre Kirszenberg, and Jesus Angulo. Learning grayscale mathematical morphology with smooth morphological layers. _Journal of Mathematical Imaging and Vision_, pages 1-18, 2022.
* [10] Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. Binarized neural networks. _Advances in neural information processing systems_, 29, 2016.
* [11] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In _International conference on machine learning_, pages 448-456. pmlr, 2015.
* [12] Minje Kim and Paris Smaragdis. Bitwise neural networks. _arXiv preprint arXiv:1601.06071_, 2016.
* [13] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization, 2017.
* [14] Branislav Kisacanin and Dan Schonfeld. A fast thresholded linear convolution representation of morphological operations. _IEEE Transactions on Image Processing_, 3(4):455-457, 1994.
* [15] Jeremie Laydevant, Maxence Ernoult, Damien Querlioz, and Julie Grollier. Training dynamical binary neural networks with equilibrium propagation. _CoRR_, abs/2103.08953, 2021.
* [16] Yann LeCun. The mnist database of handwritten digits. _http://yann. lecun. com/exdb/mnist/_, 1998.
* [17] Fengfu Li, Bin Liu, Xiaoxing Wang, Bo Zhang, and Junchi Yan. Ternary weight networks. _arXiv preprint arXiv:1605.04711_, 2016.
* [18] Jonathan Masci, Jesus Angulo, and Jurgen Schmidhuber. A learning framework for morphological operators using counter-harmonic mean. In _International Symposium on Mathematical Morphology and Its Applications to Signal and Image Processing_, pages 329-340. Springer, 2013.

* [19] Ranjan Mondal, Moni Shankar Dey, and Bhabatosh Chanda. Image restoration by learning morphological opening-closing network. _Mathematical Morphology-Theory and Applications_, 4(1):87-107, 2020.
* [20] Ana Neacsu, Jean-Christophe Pesquet, and Corneliu Burileanu. Accuracy-robustness trade-off for positively weighted neural networks. In _ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)_, pages 8389-8393. IEEE, 2020.
* ECCV 2016_, pages 525-542, Cham, 2016. Springer International Publishing.
* [22] David E. Rumelhart, Geoffrey E. Hinton, and Ronald J. Williams. Learning Representations by Back-propagating Errors. _Nature_, 323(6088):533-536, 1986.
* [23] J. Serra. _Image Analysis and Mathematical Morphology_. Academic Press, 1982.
* [24] Yucong Shen, Xin Zhong, and Frank Y Shih. Deep morphological neural networks. _arXiv preprint arXiv:1909.01532_, 2019.
* [25] B. Stellato, G. Banjac, P. Goulart, A. Bemporad, and S. Boyd. OSQP: an operator splitting solver for quadratic programs. _Mathematical Programming Computation_, 12(4):637-672, 2020.
* [26] Chunyu Yuan and Sos S Agaian. A comprehensive review of binary neural network. _arXiv preprint arXiv:2110.06804_, 2021.
* [27] Shuchang Zhou, Zekun Ni, Xinyu Zhou, He Wen, Yuxin Wu, and Yuheng Zou. Dorefa-net: Training low bitwidth convolutional neural networks with low bitwidth gradients. _CoRR_, abs/1606.06160, 2016.

## Appendix A Proofs

### Proof of theorem 2.3

**Lemma A.1**.: _Let_

\[R(\delta) \coloneqq\left[0,\frac{1}{2}-\delta\right]\bigcup\left[\frac{1}{2} +\delta,1\right] \tag{36}\] \[L_{1} \coloneqq\max_{(a_{k})_{k\in\Omega_{S}\setminus S}\in R(\delta)} \sum_{k\in\Omega_{S}\setminus S}a_{k}w_{k}\] (37) \[L_{2} \coloneqq\max_{(c_{k})_{k\in S}\in[0,\frac{1}{2}-\delta]}\sum_{k \in S}c_{k}w_{k}\] (38) \[U_{1} \coloneqq\min_{(a_{k})_{k\in\Omega_{S}\setminus S}\in R(\delta)} \sum_{k\in\Omega_{S}\setminus S}a_{k}w_{k}\] (39) \[U_{2} \coloneqq\min_{(c_{k})_{k\in S}\in R(\delta),\exists j\in S,c_{j }\geq\frac{1}{2}+\delta}\sum_{k\in S}a_{c}w_{k} \tag{40}\]

_The two following propositions are equivalent:_

\[\forall I\in\mathcal{I}(\delta)\;,\;\Big{(}X_{I}\oplus S=\{i\in \mathbb{Z}^{d}\mid I\oplus W(i)>b\}\Big{)} \tag{41}\] \[L_{1}+L_{2} \leq b<U_{1}+U_{2} \tag{42}\]

Proof.: First we show that \(41\Rightarrow 42\).

We suppose 41.

**Right hand side**

Let \((a_{k})_{k\in\Omega_{S}\setminus S}\in([0,\frac{1}{2}-\delta]\cup[\frac{1}{2} +\delta,1])^{\Omega_{S}\setminus S}\) and \((c_{k})_{k\in S}\in([0,\frac{1}{2}-\delta]\cup[\frac{1}{2}+\delta,1])^{S}\) such that \(\exists j\in S\;,\;c_{j}\geq\frac{1}{2}+\delta\).

Let \(I\in[0,1]^{-\Omega_{I}}\) be such that \(I(-k)=a_{k}\) if \(k\in\Omega_{S}\setminus S\) and \(I(-k)=c_{k}\) if \(k\in S\). Then \(I\in\mathcal{I}(\delta)\). Let \(j\in S\;,\;c_{j}\geq\frac{1}{2}+\delta\). Then \(I(-j)\geq\frac{1}{2}+\delta\), therefore \(-j\in X_{I}\), and \(0=j-j\in X_{I}\oplus S\). Therefore,

\[I\mathbin{\raisebox{-1.075pt}{\scalebox{1.0}{$\circ$}}}W(0)=\sum_{k\in\Omega_ {S}}I(-k)w_{k}=\sum_{k\in\Omega_{S}\setminus S}a_{k}w_{k}+\sum_{k\in S}c_{k}w _{k}>b \tag{43}\]

**Left hand side**

We reason by contradiction. Let us suppose that \(\exists(a_{k})_{k\in\Omega_{S}\setminus S}\in R(\delta)^{\Omega_{S}\setminus S}\) and \(\exists(c_{k})_{k\in S}\in[0,\frac{1}{2}-\delta]^{S}\), such that \(\sum_{k\in\Omega_{S}\setminus S}a_{k}w_{k}+\sum_{k\in S}a_{k}w_{k}>b\). Let \(I\in[0,1]^{-\Omega_{I}}\) be such that \(I(-k)=a_{k}\) if \(k\in\Omega_{S}\setminus S\) and \(I(-k)=c_{k}\) if \(k\in S\). Then \(I\in\mathcal{I}(\delta)\). As above, \(I\mathbin{\raisebox{-1.075pt}{\scalebox{1.0}{$\circ$}}}W(0)>b\). But \(\forall j\in S,I(-j)\leq\frac{1}{2}-\delta\) and \(-j\notin S\). Therefore, \(0\notin X_{I}\oplus S\). This contradicts \((CD1)\).

\(42\Rightarrow 41\)Let us suppose \(42\). Let \(I\in\mathcal{I}(\delta)\).

* \(X_{I}\oplus S\subset(I\mathbin{\raisebox{-1.075pt}{\scalebox{1.0}{$\circ$}}}W >b)\)

Let \(j\in X_{I}\oplus S\). Then

[MISSING_PAGE_EMPTY:14]

**Fourth equality**

\[\left(\forall i\in S,w_{k}\geq 0\right)\Rightarrow\min_{(a_{k})\in R(\delta),\exists j \in S,a_{j}\geq\frac{1}{2}+\delta}\underset{k\in S}{\sum}a_{k}w_{k}=(\frac{1}{ 2}+\delta)\min_{k\in S}w_{k} \tag{51}\]

Let

\[(a_{k})_{k\in S}=\underset{(a_{k})\in R(\delta)^{S},\exists j\in S,a_{j}\geq( \frac{1}{2}+\delta)}{\text{argmin}}\sum_{k\in S}a_{k}w_{k} \tag{52}\]

Let \(i\in S\) such that \(a_{i}\leq\frac{1}{2}-\delta\). Then: \(\sum_{k\in S}a_{k}w_{k}=\sum_{k\in S\setminus\{i\}}a_{k}w_{k}+a_{i}w_{i}\leq \sum_{k\in S\setminus\{i\}}a_{k}w_{k}\) by minimality, then \(a_{i}w_{i}\leq 0\). as \(w_{i}\geq 0\), we have \(a_{i}=0\).

Then if \(|\{j\in S\mid a_{j}\geq\frac{1}{2}+\delta\}|>1\), then we could replace all of them except one by a \(0\) to reduce the sum. Therefore, \(\exists!j\in S\;,\;a_{j}\geq\frac{1}{2}+\delta\). If \(\exists j_{2}\neq j\in S\;,\;w_{j_{2}}<w_{j}\), we could invert \(a_{j}\) and \(a_{j_{2}}\) to have a lower sum. Therefore, \(w_{j}=\min_{i\in S}w_{i}\) and \(\sum_{k\in S}a_{k}w_{k}=(\frac{1}{2}+\delta)\min_{k\in S}w_{k}\).

(\(46\Rightarrow 47\))

Let us assume \((CD1)\). Then we have the inequality of A.1. Using props 48, 49, 50 and 51, it suffices to show that \(\forall i\in S\;,\;w_{i}\geq 0\). First we notice that \(b\geq 0\). Let \((a_{k})_{k\in S}=\underset{(a_{k})_{k\in S}\in R(\delta)^{S},\;\exists j\in S \;,\;a_{j}\geq\frac{1}{2}+\delta}{\text{argmin}}\sum_{k\in S}a_{k}w_{k}\). Let \(k\in S\;,\;a_{j}\geq\frac{1}{2}+\delta\). Let \(A=\{k\in S\mid w_{k}<0\}\) and let us suppose that \(A\neq\emptyset\). Then \(\sum_{k\neq j\in S,w_{k}<0}w_{k}\geq\sum_{k\in S}a_{k}w_{k}>b\geq 0\). We have reached a contradiction. Finally, \(\sum_{k\in\Omega_{S}\setminus S,w_{k}\leq 0}w_{k}=\sum_{k\in\Omega_{S},w_{k} \leq 0}w_{k}\), which concludes this part.

(\(47\Rightarrow 46\))

Let us assume \((CD2)\). Thanks to the preliminary results, it suffices to show that \(\forall i\in S\;,\;w_{i}\geq 0\).

Using the right hand side of the inequality, we see that \((\frac{1}{2}+\delta)\min_{k\in S}w_{k}>b\geq 0\). As \(\frac{1}{2}+\delta>0\), this proves the result. 

With the exact same proof, by interverting the strict inequality, we get:

**Proposition A.3**.: _The two following propositions are equivalent:_

\[\left(\forall I\in\mathcal{I}(\delta)\right)\;,\;\left(X_{I}\oplus S=\{i\in \mathbb{Z}^{d}\mid I\;\raisebox{-1.075pt}{\includegraphics[]{figures/ 4.pdf}}\;W(i)\geq b\}\right) \tag{53}\]

\[L_{\oplus}<b\leq U_{\oplus} \tag{54}\]

_If one of them is respected, then \(b\geq 0\) and \(\forall k\in S,w_{k}\geq 0\)._

Then we can deduce the results for erosion from the dilation bounds.

**Proposition A.4**.: _The two following propositions are equivalent:_

\[\left(\forall I\in\mathcal{I}(\delta)\right)\;,\;\left(X_{I}\ominus S=\{i\in \mathbb{Z}^{d}\mid I\;\raisebox{-1.075pt}{\includegraphics[]{figures/4.pdf}} \;W(i)>b\}\right) \tag{55}\]

\[\sum W-U_{\oplus}\leq b<\sum W-L_{\oplus} \tag{56}\]Proof.: \[\forall I\in\mathcal{I}(\delta),X_{I}\oplus S=\big{(}I\mathbin{ \hbox to 0.0pt{\lower 3.0pt\hbox{$\circ$}}\hbox to 0.0pt{\raise 3.0pt\hbox{$ \circ$}}}W\geq\sum_{W}w-b\big{)}\] (57) \[\Leftrightarrow\forall I\in\mathcal{I}(\delta),(X_{I}^{C}\ominus S )^{C}=\big{(}I\mathbin{\hbox to 0.0pt{\lower 3.0pt\hbox{$\circ$}}\hbox to 0.0pt{ \raise 3.0pt\hbox{$\circ$}}\hbox to 0.0pt{\raise 3.0pt\hbox{$\circ$}}}W\geq\sum_{W}w-b\big{)}\] (58) \[\Leftrightarrow\forall I\in\mathcal{I}(\delta),X_{I}^{C}\ominus S =\big{(}I\mathbin{\hbox to 0.0pt{\lower 3.0pt\hbox{$\circ$}}\hbox to 0.0pt{ \raise 3.0pt\hbox{$\circ$}}\hbox to 0.0pt{\raise 3.0pt\hbox{$\circ$}}}W<\sum_{W}w-b\big{)}\] (59) \[\Leftrightarrow\forall I\in\mathcal{I}(\delta),X_{1-I}^{C}\ominus S =\big{(}(1-I)\mathbin{\hbox to 0.0pt{\lower 3.0pt\hbox{$\circ$}}\hbox to 0.0pt{ \raise 3.0pt\hbox{$\circ$}}\hbox to 0.0pt{\raise 3.0pt\hbox{$\circ$}}}W<\sum_{W}w-b\big{)}\] (60) \[\Leftrightarrow\forall I\in\mathcal{I}(\delta),X_{I}\ominus S= \big{(}b<I\mathbin{\hbox to 0.0pt{\lower 3.0pt\hbox{$\circ$}}\hbox to 0.0pt{ \raise 3.0pt\hbox{$\circ$}}\hbox to 0.0pt{\raise 3.0pt\hbox{$\circ$}}}W\big{)}\] (61)

Using proposition A.3,

\[57 \Leftrightarrow L_{\oplus}<\sum_{W}w-b\leq U_{\oplus} \tag{62}\] \[\Leftrightarrow\sum_{W}w-U_{\oplus}\leq b<\sum_{W}w-L_{\oplus} \tag{63}\]

**Corollary A.5** (BiSE duality).: _The BiSE of weights \(W\) and bias \(B\) is activated for \(S\) for erosion if and only if the BiSE of weights \(W\) and bias \(\sum_{W}w-B\) is activated for \(S\) for dilation._

We also show the inequalities between bias and weights.. First, we show for erosion.

**Proposition A.6**.: _Let \(S\in\Omega\) and \(\left[\frac{\frac{3}{2}-\delta}{\frac{1}{2}+\delta}|S|\right]\geq 3\). If the BiSE of weights \(W\) and \(B\) is activated for erosion, then \(B\geq\frac{1}{2}\sum_{W}w\)_

Proof.: For erosion

\[\frac{1}{2}\sum_{k\in\Omega_{S}}w_{k} =\frac{1}{2}\Bigg{[}\sum_{k\in\Omega_{S},w_{k}<0}w_{k}+\sum_{k\in S }w_{k}+\sum_{k\in\Omega_{S}\setminus S,w_{k}\geq 0}w_{k}\Bigg{]} \tag{64}\] \[\geq\frac{1}{2}\Bigg{[}\sum_{k\in\Omega_{S},w_{k}\geq 0}w_{k}- \Big{(}\frac{1}{2}+\delta\Big{)}\min_{k\in S}w_{k}-\Big{(}\frac{1}{2}+\delta \Big{)}\sum_{k\in S}w_{k}+\sum_{k\in\Omega_{S}\setminus S,w_{k}\geq 0}w_{k}\Bigg{]}\] (65) \[\geq\frac{1}{2}\Bigg{[}2\sum_{k\in\Omega_{S}\setminus S,w_{k}\geq 0 }w_{k}+\Big{(}\frac{3}{2}-\delta\Big{)}\sum_{k\in S}w_{k}-\Big{(}\frac{1}{2}+ \delta\Big{)}\min_{k\in S}w_{k}\Bigg{]}\] (66) \[\geq\frac{1}{2}\Bigg{[}\frac{\frac{3}{2}-\delta}{\frac{1}{2}+ \delta}|S|-1\Bigg{]}\Big{(}\frac{1}{2}+\delta\Big{)}\min_{k\in S}w_{k}\] (67) \[\geq\Big{(}\frac{1}{2}+\delta\Big{)}\min_{k\in S}w_{k} \tag{68}\]

For line 64 to line 65, see activation inequality. Then:

\[\frac{1}{2}\sum_{k\in\Omega_{S}}w_{k} \leq\sum_{k\in\Omega_{S}}w_{k}-(\frac{1}{2}+\delta)\min_{k\in S}w _{k} \tag{69}\] \[\leq\sum_{k\in\Omega_{S},w_{k}\geq 0}w_{k}-(\frac{1}{2}+\delta)\min_{ k\in S}w_{k}\] (70) \[\leq B \tag{71}\]

The last inequality comes from the activation inequality.

The dilation is deduced again using the duality corollary A.5.

**Proposition A.7**.: _Let \(S\in\Omega\) and \(\left[\frac{\frac{3}{2}-\delta}{2+\delta}|S|\right]\geq 3\). If the BiSE of weights \(W\) and \(B\) is activated for dilation, then \(B\leq\frac{1}{2}\sum_{W}w\)_

Proof.: Using corollary A.5, activated for dilation with weights \(W\) and bias \(B\) means activated for erosion with weights \(W\) and bias \(\sum_{W}w-B\). Therefore,

\[\frac{1}{2}\sum_{W}w\leq\sum_{W}w-B\Leftrightarrow B\leq\frac{1}{2}\sum_{W}w \tag{72}\]

We show that the output is almost binary.

**Lemma A.8**.: \[L_{\oplus} =\max_{I\in\mathcal{I}(\delta),i\in(X_{I}\oplus S)^{C}}(\leavevmode \hbox to13.64pt{\vbox to4.4pt{\pgfpicture\makeatletter\hbox to 0.0pt{\pgfsys@beginscope{} \definecolor{pgfstrokecolor}{rgb}{0,0,0}\pgfsys@color@rgb@stroke{0}{0}{0}{} \pgfsys@color@rgb@fill{0}{0}{0}{}\pgfsys@setlinewidth{0.4pt}{}\nullfont\hbox t o 0.0pt{\pgfsys@beginscope{}{{}{

[MISSING_PAGE_FAIL:18]

* Let us show that \(S\subset S_{2}\) Let \(j\in S\). Then \(w_{j}\geq w^{*}>\tau_{\oplus}\), therefore \(j\in S_{2}\).
* Let us show that \(S_{2}\subset S\) Let \(j\in S_{2}\). If \(j\notin S\), then \(w_{j}\leq\tau_{\oplus}\), which is absurd. Therefore \(j\in S\).

Using corollary A.5, we can conclude for erosion as well.

**Proposition A.10**.: _If \(\chi_{\omega,\beta,p}^{\xi,W,B}\) is activated for erosion for \(S\), then \(S=\{i\in\Omega_{S}|w_{i}>\tau_{\oplus}\}\) with_

\[\tau_{\ominus}=\frac{1}{\frac{1}{2}+\delta}\Big{(}\sum_{k\in\Omega_{S},W( \omega)_{k}>0}W(\omega)_{k}-B(\beta)\Big{)} \tag{97}\]

Proof.: If \(\chi_{W,B}\) is activated for erosion for \(S\), then \(\chi_{W,\sum_{W}w-B}\) is activated for dilation for \(S\). Then

\[S =\{i\in\Omega_{S}|W_{i}>\tau_{\ominus}\} \tag{98}\] \[\tau_{\ominus} =\frac{1}{\frac{1}{2}+\delta}\Big{(}\sum_{W}w-B-\sum_{k\in\Omega_ {S},W(\omega)_{k}<0}W(\omega)_{k}\Big{)}\] (99) \[=\frac{1}{\frac{1}{2}+\delta}\Big{(}\sum_{k\in\Omega_{S},W(\omega) _{k}>0}W(\omega)_{k}-B\Big{)} \tag{100}\]

### Proof that \(A_{\psi}(S)\) (eq. 16) is convex

We show it for dilation.

Let \((H,b_{1})\), \((G,b_{2})\in C\) and \(\alpha\in]0,1[\).

* **Right hand side** We have \[\sum_{k\in\Omega_{S},\alpha h_{k}+(1-\alpha)g_{k}\leq 0}h_{k} =\sum_{k\in\Omega_{S},\alpha h_{k}+(1-\alpha)g_{k}\leq 0,h_{k} \geq 0}h_{k}+\sum_{k\in\Omega_{S},\alpha h_{k}+(1-\alpha)g_{k}\leq 0,h_{k}\leq 0}h _{k}\] (101) \[\geq\sum_{k\in\Omega_{S},\alpha h_{k}+(1-\alpha)g_{k}\leq 0,h_{k} \leq 0}h_{k}\] (102) \[\geq\sum_{k\in\Omega_{S},h_{k}\leq 0}h_{k}\] (103) With the same reasoning, \[\sum_{k\in\Omega_{S},\alpha g_{k}+(1-\alpha)g_{k}\leq 0}g_{k}\geq\sum_{k \in\Omega_{S},g_{k}\leq 0}g_{k}\] (104) Then, \[\Big{(}\frac{1}{2}+\delta\Big{)}_{k_{1}\in S}\min_{k_{2}\in S}(\alpha h_{k_{1 }}+(1-\alpha)g_{k_{2}})\leq\Big{(}\frac{1}{2}+\delta\Big{)}\min_{k\in S}(\alpha h _{k}+(1-\alpha)g_{k})\] (105) Therefore, by combining these two results, \[\alpha b_{1}+(1-\alpha)b_{2}<\sum_{k\in\Omega_{S},\alpha h_{k}+(1-\alpha)g_{k }\leq 0}(\alpha h_{k}+(1-\alpha)g_{k})+\Big{(}\frac{1}{2}+\delta\Big{)}\min_{k \in S}(\alpha h_{k}+(1-\alpha)g_{k})\] (106)
* **Left hand side**With the same reasoning as the right hand side, we have

\[\sum_{k\in\overline{S},\alpha h_{k}+(1-\alpha)g_{k}\geq 0}h_{k} \leq\sum_{k\in\overline{S},h_{k}\geq 0}h_{k} \tag{107}\] \[\sum_{k\in\overline{S},\alpha h_{k}+(1-\alpha)g_{k}\geq 0}g_{k} \leq\sum_{k\in\overline{S},g_{k}\geq 0}h_{k}\] (108) \[\sum_{k\in S,\alpha h_{k}+(1-\alpha)g_{k}\geq 0}h_{k} \leq\sum_{k\in S,h_{k}\geq 0}h_{k}\] (109) \[\sum_{k\in S,\alpha h_{k}+(1-\alpha)g_{k}\geq 0}g_{k} \leq\sum_{k\in S,h_{k}\geq 0}g_{k} \tag{110}\]

Therefore,

\[\alpha b_{1}+(1-\alpha)b_{2}\geq\] \[\sum_{k\in\overline{S},\alpha h_{k}+(1-\alpha)g_{k}\geq 0}\left( \alpha h_{k}+(1-\alpha g_{k})\right)+\left(\frac{1}{2}-\delta\right)\sum_{k \in S,\alpha h_{k}+(1-\alpha)g_{k}\geq 0}\left(\alpha h_{k}+(1-\alpha g_{k})\right) \tag{111}\]

Therefore \(\alpha(H,b_{1})+(1-\alpha)(G,b_{2})\in C\).

For erosion, we use the duality corollary A.5 to conclude.

### Proof of proposition 3.2

We actually propose a little less powerful formulation of the proposition.

**Proposition A.11**.: _Let \(\mathbb{S}\) be the set of minimizers for \(S\) argument for \((S,\psi)\mapsto d\Big{(}(\widehat{W},\widehat{B}),A_{\psi}(S)\Big{)}\). Then:_

\[\mathbb{S}\quad\bigcap\quad\Big{\{}\{s\in\Omega_{S}\mid\hat{w}_{s}\geq\hat{w}_{t }\}\Bigm{|}t\in\Omega_{S}\Big{\}}\neq\emptyset \tag{112}\]

The previous propositions means that one of the sets defined as a thresholded set values of \(\widehat{W}\) reaches the smallest distance. Therefore, it is enough to only search for these distances.

To prove this proposition, first we show the following Lemma.

**Lemma A.12**.: _Let \(x_{1}<y_{1}\in\mathbb{R},x_{2}\leq y_{2}\in\mathbb{R}\). Then:_

\[(x_{1}-x_{2})^{2}+(y_{1}-y_{2})^{2}\leq(x_{2}-y_{1})^{2}+(x_{1}-y_{2})^{2} \tag{113}\]

_If \(x_{2}<y_{2}\), then_

\[(x_{1}-x_{2})^{2}+(y_{1}-y_{2})^{2}<(x_{2}-y_{1})^{2}+(x_{1}-y_{2})^{2} \tag{114}\]

Proof.: Let \(\delta_{1}\coloneqq y_{1}-x_{1}>0\) and \(\delta_{2}\coloneqq y_{2}-x_{2}\geq 0\). Then

\[(x_{1}-x_{2})^{2}+(y_{1}-y_{2})^{2}\leq(x_{2}-y_{1})^{2}+(x_{1}-y _{2})^{2} \tag{115}\] \[\Leftrightarrow(x_{1}-x_{2})^{2}+(x_{1}+\delta_{1}-x_{2}-\delta_ {2})^{2}\leq(x_{2}-x_{1}-\delta_{1})^{2}+(x_{1}-x_{2}-\delta_{2})^{2}\] (116) \[\Leftrightarrow(x_{1}-x_{2})^{2}-(x_{2}-x_{1}-\delta_{1})^{2}\leq (x_{1}-x_{2}-\delta_{2})^{2}-(x_{1}+\delta_{1}-x_{2}-\delta_{2})^{2}\] (117) \[\Leftrightarrow-\delta_{1}\cdot(2x_{1}-2x_{2}+\delta_{1})\leq- \delta_{1}\cdot(2x_{1}-2x_{2}-2\delta_{2}+\delta_{1})\] (118) \[\Leftrightarrow 0\leq\delta_{2} \tag{119}\]

If \(x_{2}<y_{2}\) is strict, then we can replace all large inequalities by strict inequalities. 

Let \(S^{*},\psi^{*}\in\underset{S,\psi}{\text{argmin}}\ d\Big{(}(\widehat{W}, \widehat{B}),A_{\psi}(S)\Big{)}\). Then

\[S^{*},\psi^{*}\in\underset{S\subset\Omega_{S},\psi\in\{\oplus,\ominus\}}{ \text{argmin}}\ \min_{(w,b)\in A_{\psi^{*}}(S)}\frac{1}{2}||(\widehat{W}, \widehat{B})-(w,b)||_{2}^{2} \tag{120}\]Let \((w^{*},b^{*})\in\underset{(w,b)\in A_{\psi^{*}}(S^{*})}{\text{argmin}}\ \frac{1}{2}||( \widehat{W},\widehat{B})-(w,b)||_{2}^{2}\). Then:

\[S^{*},\psi^{*},w^{*},b^{*}\in S\subset\Omega_{S},\psi\in\{\oplus, \ominus\},w\in\mathbb{R}^{\Omega_{S}},b\in\frac{1}{2}||(\widehat{W},\widehat{B })-(w,b)||_{2}^{2}\] \[\text{subject to }\left\{\begin{array}{l}L_{\psi^{*}}(w,S)-b \leq 0\\ b-U_{\psi^{*}}(w,S)\leq 0\end{array}\right. \tag{121}\]

Let \(i,j\in\Omega_{S}\) such that \(i\in S^{*},j\notin S^{*}\) such that \(\hat{w}_{i}<\hat{w}_{j}\). We define:

\[S_{2} \coloneqq S^{*}\cup\{j\}\setminus\{i\} \tag{122}\] \[w_{2} \coloneqq k\in\Omega_{S}\mapsto\left\{\begin{array}{l}w_{i}^{* }\text{ if }k=j\\ w_{i}^{*}\text{ if }k=i\\ w_{k}^{*}\text{ else }\end{array}\right. \tag{123}\]

We show that \((w_{2},S_{2},b^{*})\) results in a lesser or equal function value than \((w^{*},S^{*},b^{*})\). First, \((w_{2},S_{2},b^{*})\) respects the constraints: \(L_{\psi^{*}}(w^{*},S^{*})=L_{\psi^{*}}(w_{2},S_{2})\) and \(U_{\psi^{*}}(w^{*},S^{*})=U_{\psi^{*}}(w_{2},S_{2})\). Then we have

\[||(\widehat{W},\widehat{B})-(w^{*},b^{*})||_{2}^{2}-||(\widehat{W },\widehat{B})-(w_{2},b^{*})||_{2}^{2} \tag{124}\] \[=(\hat{w}_{i}-w_{i}^{*})^{2}+(\hat{w}_{j}-w_{j}^{*})^{2}-(\hat{w} _{i}-w_{j}^{*})^{2}-(\hat{w}_{j}-w_{i}^{*})^{2}\]

Then, let us show that \(w_{i}^{*}\leq w_{j}^{*}\). We reason by contradiction and suppose that \(w_{i}^{*}>w_{j}^{*}\). Then

Using lemma A.12, we find that

\[||(\widehat{W},\widehat{B})-(w^{*},b^{*})||_{2}^{2}-||(\widehat{W},\widehat{B })-(w_{2},b^{*})||_{2}^{2}<0 \tag{125}\]

This contradicts the minimum hypothesis of \((w^{*},b^{*})\). Therefore \(w_{i}^{*}\leq w_{j}^{*}\)

Using again lemma A.12, we find that

\[||(\widehat{W},\widehat{B})-(w^{*},b^{*})||_{2}^{2}-||(\widehat{W},\widehat{B })-(w_{2},b^{*})||_{2}^{2}\geq 0 \tag{126}\]

Therefore \(S_{2},\psi^{*}\in\underset{S,\psi}{\text{argmin}}\ d\Big{(}(\widehat{W}, \widehat{B}),A_{\psi}(S)\Big{)}\).

Therefore, for every couples \((i,j)\in S^{*}\cap\Omega_{S}\setminus S^{*}\), we can switch them and stay in the minimizers. By switching all of them, we reach a set of thresholded values that is also in the minimizers of the distance.

### Proof of equations 19, 20, 21 and 22, 23, 24

We suppose that \(\widehat{W}\geq 0\).The initial problem is, for \(S\subset\Omega_{S},\psi\in\{\oplus,\ominus\}\).

\[\underset{(w,b)\in\mathbb{R}^{\Omega_{S}}\times\mathbb{R}}{\text{minimize}} \ \frac{1}{2}\sum_{i\in\Omega_{S}}(w_{i}-\widehat{w}_{i})^{2}+\frac{1}{2}(b- \widehat{B})^{2}\quad\text{subject to }\left\{\begin{array}{l}L_{\psi}(w,S)-b \leq 0\\ b-U_{\psi}(w,S)\leq 0\end{array}\right. \tag{127}\]

#### a.5.1 Proof for dilation

We define the following problem, with constraint set \(A_{\oplus}(S)\)

\[\underset{(w,b)\in\mathbb{R}^{\Omega_{S}}\times\mathbb{R}}{\text{minimize}} \ \frac{1}{2}\sum_{i\in\Omega_{S}}(w_{i}-\widehat{w}_{i})^{2}+\frac{1}{2}(b- \widehat{B})^{2}\quad\text{subject to }\left\{\begin{array}{l}\sum_{k\in\Omega_{S} \setminus S}w_{k}-b\leq 0\\ \forall s\in S,b\leq w_{s}\\ \forall k\in\Omega_{S}\setminus S,-w_{k}\leq 0\end{array}\right. \tag{128}\]

First we notice that \(A_{\oplus}(S)\subset A(S)\). Let \(W^{*},B^{*}\) be a solution of problem 127. If \((W^{*},B^{*})\in A_{\oplus}(S)\), then this concludes the proof. We define

\[\mathbb{K}^{-}\coloneqq\{k\in\Omega_{S}\mid w_{k}^{*}<0\} \tag{129}\] \[W^{+}\coloneqq\max(W,0) \tag{130}\]Then \((W^{+},B)\in A_{\oplus}(S)\subset A(S)\) and

\[||(\widehat{W},\widehat{B})-(W^{*},B^{*})||_{2}^{2}-||(\widehat{W}, \widehat{B})-(W^{+},B^{*})||_{2}^{2} \leq 0 \tag{131}\] \[\Leftrightarrow\sum_{k\in\mathbb{K}^{-}}\Big{(}(w_{k}^{*}-\hat{w }_{k})^{2}-(0-\hat{w}_{k})^{2}\Big{)} \leq 0\] (132) \[\Leftrightarrow\sum_{k\in\mathbb{K}^{-}}w_{k}^{*}(w_{k}^{*}-2\hat {w}_{k}) \leq 0 \tag{133}\]

We reason by contradiction and suppose that \(\mathbb{K}^{-}\neq\emptyset\). If \(k\in\mathbb{K}^{-}\), then \(w_{k}^{*}<0\) and \(w_{k}^{*}-2\hat{w}_{k}^{*}<0\) because as hypothesis, \(\forall i\in\Omega,\hat{w}_{i}\geq 0\). If \(\mathbb{K}^{-}\neq\emptyset\), then \(\sum_{k\in\mathbb{K}^{-}}w_{k}(w_{k}-2\hat{w}_{k})>0\), which is absurd. Therefore, \(\mathbb{K}^{-}=\emptyset\).

#### a.5.2 Proof for erosion

For erosion, the new problem is with constraints \(A_{\ominus}\):

\[\underset{(w,b)\in\mathbb{R}^{\Omega_{S}}\times\mathbb{R}}{\text{ minimize}}\ \ \frac{1}{2}\sum_{i\in\Omega_{S}}(w_{i}-\widehat{w}_{i})^{2}+\frac{1}{2}(b- \widehat{B})^{2}\quad\text{subject to}\ \left\{\begin{aligned} b-\sum_{s\in S}w_{s}\leq 0\\ \forall s\in S,\sum_{i\in\Omega_{S}}w_{i}-w_{s}\leq b\\ \forall k\in\Omega_{S}\setminus S,-w_{k}\leq 0\end{aligned}\right. \tag{134}\]

We notice similarly to the dilation that \(A_{\ominus}\subset A_{\psi}(S)\), and the proof is the same.

### Proof of Proposition referenced in 3.2.2

We prove the following proposition.

**Proposition A.13**.: _Let \(S^{*}\coloneqq\underset{S\subset\Omega_{S}}{\text{argmin}}\ d(\widetilde{A}(S ),w)\). Then \(S^{*}=\{i\in\Omega_{S}\mid w_{i}\geq\min_{S^{*}}\widehat{w}_{s}\}\)_

Let \(W\in\mathbb{R}_{+}^{\Omega_{S}}\) be non zero and \(\sigma\coloneqq\sum_{k\in S}w_{k}\). Let \(L(S)\coloneqq\frac{\sigma^{2}}{|S|}\). We first prove two lemmas.

**Lemma A.14**.: _Let \(S\subset\Omega\). Let \(q\in\Omega_{S}\setminus S\). Then_

\[L(S\cup\{q\})>L(S)\Leftrightarrow w_{q}>\Bigg{(}\sqrt{\frac{1}{|S|}+1}-1 \Bigg{)}\sigma \tag{135}\]

Proof.: \[L(S\cup\{q\})-L(S)>0 \Leftrightarrow\frac{(\sigma+w_{q})^{2}}{|S|+1}-\frac{\sigma^{2} }{|S|}>0\] (136) \[\Leftrightarrow|S|(\sigma+w_{q})^{2}-(|S|+1)\sigma^{2}>0\] (137) \[\Leftrightarrow 2w_{q}\cdot\sigma\cdot|S|+w_{q}^{2}|S|-\sigma^{2}>0\] (138)

This is a 2nd degree polynomial. As \(|S|>0\), the polynomial is positive outside of its two roots.

\[r_{1,2} =\frac{-2\sigma\cdot|S|\pm\sqrt{(2\sigma\cdot|S|)^{2}+4\sigma^{2} |S|}}{2|S|} \tag{139}\] \[=\sigma\Big{(}-1\pm\sqrt{1+\frac{1}{|S|}}\Big{)} \tag{140}\]

By hypothesis, \(w_{q}\geq 0\) and \(\sigma\geq 0\). Therefore, we can discard the negative root and

\[L(S\cup\{q\})-L(S)>0\Leftrightarrow w_{q}>\Bigg{(}\sqrt{\frac{1}{|S|}+1}-1 \Bigg{)}\sigma \tag{141}\]

[MISSING_PAGE_EMPTY:23]

Proof.: Let

\[\mathcal{S}(a,b) \coloneqq\sum_{k\in\Omega_{S}\setminus S}a_{k}w_{k}+\sum_{k\in S}c_{ k}w_{k} \tag{153}\] \[F_{1} =\Big{\{}\mathcal{S}(a,b)|(a_{k})\in R(\delta),(c_{k})\in[0,\frac{ 1}{2}-\delta]\Big{\}}\] (154) \[F_{2} =\Big{\{}\mathcal{S}(a,b)|(a_{k})\in R(\delta),(c_{k})\in[0,\frac{ 1}{2}-\delta]\Big{\}} \tag{155}\]

Then by definition of dilation, these sets are all the possible values taken by the convolution for pixels in or outside of the dilated binary image.

\[F_{1} =\Big{\{}(I\mathbin{\text{\text{\text{\text{\text{\text{\text{ \text{\text{\text{\text{\text{\text{\text{\text{\text{\texttext \text \text{\text{ \texttext \text{\text{\texttexttexttexttext \text { \text{\texttexttexttexttext { \texttexttexttexttext { \texttext{ \texttext{ \texttexttexttext{ \texttexttext { \texttext{ \texttext{ \texttext{ \texttext{ \texttexttext{ \text{ \texttexttext{ \text{ \texttext{ \texttext{ \text{ \texttext{ \texttext{ }}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\Reparametrization Functions

To facilitate the convergence of our networks towards morphological operators, certain constraints are beneficial. In order to avoid dealing with a multitude of constraints, we reparametrize some variables to ensure that the constraints are always satisfied. We introduce two reparametrization functions for the weights, and three for the bias.

PositiveOur objective is to reach the set of activable weights and bias. Theorem 2.3 indicates that we only have to look at positive parameters. We can enforce them to be positive by setting \(W\) and \(B\) as the softplus function.

\[B(\cdot)=W(\cdot)\coloneqq f^{+}(\cdot)\coloneqq\log(1+\exp(\cdot)). \tag{169}\]

Dual reparametrizationFor the weights, we introduce the _dual_ reparametrization as follows:

\[W_{dual}(\omega)\coloneqq\frac{K\cdot f^{+}(\omega)}{\sum_{W(\omega)}w} \tag{170}\]

with \(K\coloneqq 2\cdot\xi^{-1}(0.95)\) and \(\xi\) the smooth treshold activation choosen. We can show that this dual reparametrization ensures that the training process is similar for both erosion and dilation.

Other reparametrization for the bias can be defined to keep it into coherent range values. If the bias is smaller than \(\min(W)\), then \(\forall X\subset\Omega,\chi(\mathbbm{1}_{X})<0.5\): no values will be close to \(1\). On the other side, if the bias is higher than \(\sum_{W}w\), then \(\chi(\mathbbm{1}_{X})>0.5\). Therefore, we want \(\min(W)<B<\sum_{W}w\). Let \(\{W_{1}<...<W_{K}\}\) be the ordered values taken by the weights \(W\). The previous inequality is ensured if \(B\) belongs to the closed convex set \(C_{b}\coloneqq[\frac{W_{1}+W_{2}}{2},\sum_{W}w-\frac{W_{1}}{2}]=[l_{c}(W),u_{c }(W)]\). There are two ways of ensuring that \(B\in C_{b}\).

ProjectedFirst, we can project the bias after the gradient iteration: this comes down to a projected gradient algorithm. We call this approach "Projected" reparametrization. We apply \(B_{p}(\beta)\coloneqq f^{+}(\beta)\), and we project \(B_{p}(\beta)\) onto \(C_{b}\) after the gradient update iteration.

Projected ReparamThe second way is to redefine \(B\) before the end of the iteration, instead of after. We call this approach "Projected Reparam" reparametrization:

\[B_{pr}(\beta)\coloneqq\left\{\begin{array}{ll}l_{c}(W)&\text{if }f^{+}(\beta)<l_{c}(W) \\ u_{c}(W)&\text{if }f^{+}(\beta)>u_{c}(W)\\ f^{+}(\beta)&\text{else }.\end{array}\right. \tag{171}\]

[MISSING_PAGE_FAIL:26]

\[\xi(u)\simeq\Big{(}\frac{1}{2}+p\Big{)}\mathbb{1}_{]-\frac{1}{2p},\frac{1}{2p}[(u) +\mathbb{1}_{]\frac{1}{2p},+\infty]}(u) \tag{176}\]

\[\mathbb{E}[\mathbf{x}_{l}^{2}] =\mathbb{E}[\xi(\mathbf{y}_{l})^{2}] \tag{177}\] \[\mathbb{E}[\mathbf{x}_{l}^{2}] \simeq p^{\prime 2}\int_{-\frac{1}{2p^{\prime}}}^{\frac{1}{2p^{ \prime}}}y^{2}p(\mathbf{y}_{l})d\mathbf{y}_{l}+p^{\prime}\int_{-\frac{1}{2p^{ \prime}}}^{\frac{1}{2p^{\prime}}}\mathbf{y}_{l}p(\mathbf{y}_{l})d\mathbf{y}_{l }+\frac{1}{2}\int_{0}^{+\infty}p(\mathbf{y}_{l})d\mathbf{y}_{l}+\frac{1}{2}\int _{\frac{1}{2p^{\prime}}}^{+\infty}p(\mathbf{y}_{l})d\mathbf{y}_{l}\] (178) \[=p^{\prime 2}Var(\mathbf{y}_{l})+H(p^{\prime})+\frac{1}{4}\] (179) \[\text{with }H(p^{\prime})\coloneqq\int_{\frac{1}{2p^{\prime}}}^{+ \infty}\Big{(}\frac{1}{2}-2p^{\prime 2}\mathbf{y}_{l}^{2}\Big{)}p(\mathbf{y}_{l})d \mathbf{y}_{l}. \tag{180}\]

By independence, we have

\[\frac{1}{n_{l}}Var(\mathbf{y}_{l}) =Var(\mathbf{W}_{l}\mathbf{x}_{l-1}) \tag{181}\] \[=\sigma_{l}Var(\mathbf{x}_{l-1})+\mathbb{E}[\mathbf{x}_{l-1}]^{2} \sigma_{l}+\mu_{l}^{2}Var(\mathbf{x}_{l-1})\] (182) \[=\sigma_{l}[\mathbb{E}[\mathbf{x}_{l-1}^{2}]-\mathbb{E}[\mathbf{x }_{l-1}]^{2}]+\mathbb{E}[\mathbf{x}_{l-1}]^{2}+\mu_{l}^{2}[[\mathbb{E}[\mathbf{ x}_{l-1}^{2}]-\mathbb{E}[\mathbf{x}_{l-1}]^{2}]\] (183) \[=\mathbb{E}[\mathbf{x}_{l-1}^{2}](\sigma_{l}+\mu_{l}^{2})-\mu_{l} ^{2}\mathbb{E}[\mathbf{x}_{l-1}]^{2}\] (184) \[\simeq p^{\prime 2}Var(\mathbf{y}_{l-1})(\sigma_{l}+\mu_{l}^{2})- \frac{1}{4}\mu_{l}^{2}+(\sigma_{l}+\mu_{l}^{2})\Big{(}\frac{1}{4}+H(p^{\prime })\Big{)}\] (185) \[=p^{\prime 2}Var(\mathbf{y}_{l-1})(\sigma_{l}+\mu_{l}^{2})+G(p^{ \prime})\] (186) \[\text{with }\quad G(p^{\prime}) \coloneqq\frac{1}{4}\sigma_{l}+H(p^{\prime})(\sigma_{l}+\mu_{l}^ {2}). \tag{187}\]

If \(G(p^{\prime})\geq 0\), which is equivalent to \(H(p^{\prime})\geq-\frac{\sigma_{l}}{4(\sigma_{l}+\mu_{l}^{2})}\).

\[\frac{1}{n_{l}}Var(\mathbf{y}_{l})\leq p^{\prime 2}Var(\mathbf{y}_{l-1})( \sigma_{l}+\mu_{l}^{2}). \tag{188}\]

We unroll the recursive relation. Let

\[V(p^{\prime})\coloneqq Var(\mathbf{y}_{1})\prod_{k=1}^{l}p^{2}n_{k}(\sigma_{k }+\mu_{k}^{2}). \tag{189}\]

Then

\[V(p^{\prime})\leq Var(\mathbf{y}_{l})\leq V(p^{\prime})+\sum_{k=1}^{L}\frac{1 }{4}\sigma_{k}\prod_{m=k}^{l}p^{2}n_{m}(\sigma_{m}+\mu_{m}^{2}). \tag{190}\]

The right hand side of the inequality comes from the fact that \(H(p^{\prime})\leq 0\).

If \(G(p^{\prime})\leq 0\), which is equivalent to \(H(p^{\prime})\leq-\frac{\sigma_{l}}{4(\sigma_{l}+\mu_{l}^{2})}\), we have:

\[0\leq Var(\mathbf{y}_{l})\leq V(p^{\prime}). \tag{191}\]

If the variance is equal to \(0\), then the output is constant: therefore, the gradient is \(0\) and we cannot learn the parameters. On the other hand, if the variance is too high, the model is poorly conditioned: we can expect the learning to be unstable.

If the number of layers is too big, the product term \(V(p^{\prime})\) can either vanish or explode. To avoid variance exploding when \(G(p^{\prime})\geq 0\), and to avoid variance vanishing when \(G(p^{\prime})\leq 0\), we choose to ensure that the product is equal to \(1\) by imposing each term to be \(1\).

\[\forall l\in[1,L]\,\ p^{\prime 2}n_{l}(\sigma_{l}+\mu_{l}^{2})=1 \tag{192}\] \[\sigma_{l}=\frac{1}{p^{\prime 2}n_{l}}-\mu_{l}^{2} \tag{193}\]

\[\forall l\in[1,L]\,\ p^{\prime 2}n_{l}(\sigma_{l}+\mu_{l}^{2})=1. \tag{194}\]

The variance needs to be positive, which gives

\[\mu_{l}<\frac{1}{p^{\prime}}\sqrt{\frac{1}{n_{l}}}. \tag{195}\]

We have a range of possible means and variances. Let us choose a uniform distribution for \(\mathbf{W}_{l}\). Then \(\mathbf{W}_{k}\sim\mathcal{U}\underline{(\mu_{l}-\sqrt{3\sigma_{l}},\mu_{l}+ \sqrt{3\sigma_{l}})}\). We want positive weights (see theorem 2.3), therefore we want \(\mu_{l}>\sqrt{3\sigma_{l}}\), which gives

\[\mu_{l}>\frac{\sqrt{3}}{2p^{\prime}}\sqrt{\frac{1}{n_{l}}}. \tag{196}\]

Using equations 195 and 196, we choose \(\mu_{l}\) as the mean of the bounds:

\[\mu_{l}\coloneqq\frac{\sqrt{3}+2}{4p^{\prime}\sqrt{n_{l}}}. \tag{197}\]

Role of the scaling factor \(p\)Finally, \(p^{\prime}\) is related to the value of \(p\): \(p^{\prime}=\frac{d(\xi(p\cdot x)}{dx}(0)=p\xi^{\prime}(0)\). Ideally, to avoid bias towards applying complementation or not, \(p\) should be set at \(0\), therefore \(p^{\prime}=0\). We consider that after the first few iterations, \(p\neq 0\), and our computations become valid.

How to set \(p^{\prime}\) in practice? We want the BiSE output to be in the full range \([0,1]\). Let us suppose that after a few iterations, \(p=1\). With \(B_{l}=\sum_{w\in W_{l}}w\), the lowest output value is given by an image full of \(0\), giving \(\xi(-B_{l})\). The highest output is given by an image full of \(1\) and gives \(\xi(B_{l})\). Let \(h\in]0,1[\) (e.g. \(h=0.95\)). We want \(\xi(B)>h\), which is the same as \(\xi(-B_{l})<1-h\) thanks to the binary-odd property. By using the fact that \(\mu_{l}n_{l}\sim\sum_{w\in W_{l}}w\), we have:

\[\xi(B_{l})\geq h\Leftrightarrow p^{\prime}\leq\frac{\sqrt{3}+2}{8\xi^{-1}(h)} \sqrt{n_{l}}. \tag{198}\]

SummaryWith \(h=0.95\), \(\epsilon_{bias}\in[10^{-4},10^{-2}]\) and \(p^{\prime}_{l}\coloneqq\frac{\sqrt{3}+2}{8\xi^{-1}(h)}\sqrt{n_{l}}\),

\[\forall l\in[1,L]\,\ \mu_{l}\coloneqq\frac{\sqrt{3}+2}{4p^{\prime}_{l} \sqrt{n_{l}}} \tag{199}\] \[\forall l\in[1,L]\,\ \sigma_{l}\coloneqq\frac{1}{p^{\prime 2}_{l} n_{l}}-\mu_{l}^{2}\] (200) \[\forall l\in[1,L]\,\ \mathbf{W}_{l}\sim\mathcal{U}(\mu_{l}-\sqrt{3 \sigma_{l}},\mu_{l}+\sqrt{3\sigma_{l}})\] (201) \[\forall l\in[1,L]\,\ p_{l}\coloneqq 0\] (202) \[\forall l\in[2,L]\,\ b_{l}\coloneqq\frac{1}{2}\sum_{w\in\mathbf{W}_{l }}w+\mathcal{U}(-\epsilon_{bias},\epsilon_{bias})\] (203) \[b_{1}\coloneqq\mathbb{E}(\mathbf{x}_{0})\sum_{w\in\mathbf{W}_{1} }w+\mathcal{U}(-\epsilon_{bias},\epsilon_{bias}). \tag{204}\]

We can approximate \(\mathbb{E}(\mathbf{x}_{0})\) with the mean of a few samples, for example the first batch.

ReparametrizationWe computed the weights and biases of the convolution. In the BiSE definition 2.2, they correspond to \(W(\omega)\) and \(B(\beta)\). The true initialization must be done on \(\omega\) and \(\beta\). If \(W\) and \(B\) are invertible, the problem is solved:

\[\omega_{l} =W^{-1}(\mathbf{W}_{l}) \tag{205}\] \[\beta_{l} =B^{-1}(b_{l}). \tag{206}\]

If the functions are not invertible, the study must be done case by case in order to find generative distributions for \(\omega_{l}\) and \(\beta_{l}\) that respects the resulting distributions for \(W(\omega_{l})\) and \(B(\beta_{l})\).

Dual reparametrizationFor the dual reparametrization, we need to adapt the parameter \(K\).

**Proposition C.1**.: _Let \(\sigma,\mu\in\mathbb{R}_{+}^{2}\) such that \(\mu-\sqrt{3\sigma}>0\). Let \(a\in\mathbb{R}_{+}^{*}\). Let \(X_{i}\sim\mathcal{U}\Big{(}a,a\frac{\mu+\sqrt{3\sigma}}{\mu-\sqrt{3\sigma}} \Big{)}\) be a sequence of independent, identically distributed random variables, \(S_{N}=\sum_{i=1}^{N}X_{i}\), \(K_{N}=\mu\cdot N\) and \(W_{i,N}=K_{N}\frac{X_{i}}{S_{N}}\). Then, almost surely (a.s.)_

\[W_{i,N}\longrightarrow_{N\rightarrow\infty}^{a,s.}\mathcal{U}(\mu-\sqrt{3 \sigma},\mu+\sqrt{3\sigma}). \tag{207}\]

Proof.: \[\mathbb{E}[|X_{i}|]=\mathbb{E}[X_{i}]=\frac{a}{2}\Big{(}1+\frac{\mu+\sqrt{3 \sigma}}{\mu-\sqrt{3\sigma}}\Big{)}=a\frac{\mu}{\mu-\sqrt{3\sigma}}<+\infty.\] (208)

Then, according to the strong law of large numbers, \(\frac{S_{N}}{N}\longrightarrow^{a.s.}\mathbb{E}[X_{i}]\). Then:

\[W_{i,N}=X_{i}\mu\frac{N}{S_{N}}\longrightarrow_{N}^{a.s.}X_{i}\mu\frac{\mu- \sqrt{3\sigma}}{a\mu}=\frac{1}{a}(\mu-\sqrt{3\sigma})X_{i}\sim\mathcal{U}(\mu- \sqrt{3\sigma},\mu+\sqrt{3\sigma}). \tag{209}\]

Finally if we take \(a=\mu_{k}-\sqrt{3\sigma_{k}}\) the initialization becomes:

\[K =\mu_{k}\cdot n_{k}=2\cdot\xi^{-1}(h) \tag{210}\] \[\mathbf{W}_{k} \sim\mathcal{U}(\mu_{k}-\sqrt{3\sigma_{k}},\mu_{k}+\sqrt{3\sigma _{k}})\] (211) \[K\frac{\mathbf{W}_{k}}{\sum_{w\in W_{k}}w} \sim_{n_{k}\rightarrow+\infty}^{a.s.}\mathcal{U}(\mu_{k}-\sqrt{3 \sigma_{k}},\mu_{k}+\sqrt{3\sigma_{k}}). \tag{212}\]

Remark on bias initializationTheorem 2.3 expresses the operation approximated by the BiSE depending on the bias. From the second to the last layer, the bias is initialized at the middle of both operation. Therefore, the BiSE are unbiased to learn either dilation or erosion.

However, for the first BiSE layer, the bias is initialized differently. If \(\mathbb{E}(\mathbf{x}_{0})<\frac{1}{2}\), then the bias indicates a dilation. We bias the BiSE to learn a dilation. This is explained by the assumption that the output must mean at \(\frac{1}{2}\). If the input is a lower than \(\frac{1}{2}\), we must increase its value. The same goes for the erosion: if the value of the input is too high, we reduce it by applying an erosion.

### Gradient Computation

Let \(\Gamma=\chi_{L}\circ...\circ\chi_{1}\) be a sequence of BiSE neurons, with parameters \(p_{l}\), \(b_{l}\) and \(\mathbf{W}_{l}\) for all layers \(l\). We compute \(\mathcal{L}(\Gamma(X),Y)\) for one input sample. Let

\[\Pi_{l}\coloneqq\partial_{1}\mathcal{L}\Big{(}\Gamma(X),Y\Big{)}\Bigg{(} \prod_{k=l+1}^{N}\text{diag}(\xi^{\prime}(\chi_{k}))p_{k}W_{k}\Bigg{)}. \tag{213}\]

We re-denote \(\mathbf{W}_{l}\) as the linear matrix such that \(\chi_{l-1}\)\(\textcircled{\phantom{\rule{1.0pt}{1.0pt}}\phantom{\rule{1.0pt}{1.0pt}} \phantom{\rule{1.0pt}{1.0pt}}\phantom{\rule{1.0pt}{1.0pt}}\phantom{\rule{1.0pt }{1.0pt}}\phantom{\rule{1.0pt}{1.0pt}}\phantom{\rule{1.0pt}{1.0pt}}\phantom{ \rule{1.0pt}{1.0pt}}\phantom{\rule{1.0pt}{1.0pt}}\phantom{\rule{1.0pt}{1.0pt}} \phantom{\rule{1.0pt}{1.0pt}}\phantom{\rule{1.0pt}{\[\frac{\mathrm{d}\mathcal{L}}{\mathrm{d}p_{l}} \coloneqq\Pi_{l}\text{diag}(\xi^{\prime}(\chi_{l}))(\chi_{l-1} \mathbf{W}_{l}-b_{l}) \tag{214}\] \[\frac{\mathrm{d}\mathcal{L}}{\mathrm{d}b_{l}} \coloneqq\Pi_{l}\text{diag}(\xi^{\prime}(\chi_{l}))(-p_{l})\] (215) \[\frac{\mathrm{d}\mathcal{L}}{\mathrm{d}\mathbf{W}_{l}} \coloneqq\Pi_{l}\text{diag}(\xi^{\prime}(\chi_{l}))p_{l}\phi_{l-1}. \tag{216}\]

If for all \(l\), \(p_{l}=0\), then \(\frac{\mathrm{d}\mathcal{L}}{\mathrm{d}b_{l}}=\frac{\mathrm{d}\mathcal{L}}{ \mathrm{d}\mathbf{W}_{l}}=0\). Moreover, by initialization of the bias \(b_{l}\), we have \(\chi_{l-1}\mathbf{W}_{l}-b_{l}=0\), leading to \(\frac{\mathrm{d}\mathcal{L}}{\mathrm{d}p_{l}}=0\). With our current initialization, all the gradients are equal to \(0\). Therefore, in practice, we add a uniform noise to the bias:

\[\forall l\geq 2\;,\;b_{l}=\frac{1}{2}\sum_{\mathbf{W}_{l}}w+ \mathcal{U}(-10^{-4},10^{-4}) \tag{217}\] \[B_{1}=\mathbb{E}(X)\sum_{W_{1}}w+\mathcal{U}(-10^{-4},10^{-4}). \tag{218}\]

This will lead to \(p_{l}\) moving away from \(0\), which unblocks the other gradients as well. We introduce a small bias towards either dilation or erosion. However, its significance is negligible, and does not prevent from learning one operation or the other.

Regularization onto the set of activable parameters

We compute the distance to the closest set \(A_{\psi,S}\) defined in (16) for each BiSE neuron. Let \((\mathbf{W},b)\coloneqq(W(\omega),B(\beta))\) be the weights in a given iteration. Then:

\[\mathcal{L}_{\text{morpho}}=\mathcal{L}_{acti}\coloneqq\min_{S,\psi}d\bigg{(}( \mathbf{W},b),A_{\psi,S}\bigg{)}. \tag{219}\]

To do this, we must compute this distance in a differentiable fashion. We proceed in two steps: first, we compute the optimal \((S^{*},\psi^{*})\) as in SS3.1, by checking all possible thresholded set of weights and solving the corresponding QP with OSQP, with the Lagrangian dual method. This yields the best \((\mathbf{W}^{*},b^{*})\) as well as the Lagrangian dual values, from which we deduce the differentiable form of the distance. More details are given in Appendix D. However, the computational burden described in SS3.1 persists: the first step of computing \(S^{*}\) is too long in practice.

If \(\psi^{*}=\oplus\), let \(\lambda^{*}\) be the dual value for the constraint (19) and

\[\mathbb{T} \coloneqq\{t\in S^{*}\mid\mathbf{W}_{t}\leq b^{*}\} \tag{220}\] \[\mathbb{K} \coloneqq\{k\in\Omega\setminus S^{*}\mid\mathbf{W}_{k}\leq\lambda ^{*}\}\] (221) \[\overline{\mathbb{K}} \coloneqq(\Omega\setminus S^{*})\setminus\mathbb{K}\] (222) \[D \coloneqq|\overline{\mathbb{K}}|(|\mathbb{T}|+1)+1, \tag{223}\]

then we can show that we obtain the following differentiable expressions for \((\mathbf{W}^{*},b^{*})\).

\[b^{*} =\frac{1}{D}\bigg{(}\sum_{j\in\overline{\mathbb{K}}}\mathbf{W}_{j }+|\overline{\mathbb{K}}|\Big{(}\sum_{t\in\mathbb{T}}\mathbf{W}_{t}+b\Big{)} \bigg{)} \tag{224}\] \[\forall j\in\overline{\mathbb{K}},\mathbf{w}_{j}^{*} =\mathbf{W}_{j}+\frac{1}{D}\Bigg{(}\sum_{t\in\mathbb{T}}\mathbf{W }_{t}+b-(|\mathbb{T}|+1)\sum_{i\in\mathbb{K}}\mathbf{W}_{i}\Bigg{)}\] (225) \[\forall k\in\mathbb{K}\,,\;\mathbf{W}_{k}^{*}=0\] (226) \[\forall t\in\mathbb{T}\,,\;\mathbf{W}_{t}^{*}=b^{*}\] (227) \[\forall s\in S^{*}\backslash\mathbb{T}\,,\;\mathbf{W}_{s}^{*}= \mathbf{W}_{s}. \tag{228}\]

Then, we have a differentiable expression for the loss.

\[\mathcal{L}_{acti}=\sum_{i\in\Omega}(\mathbf{W}_{i}-\mathbf{w}_{i}^{*})^{2}-(b -b^{*})^{2}. \tag{229}\]

## Appendix E Experiments

We perform a random search across a range of hyperparameters to identify the optimal configuration. The hyperparameters explored in the search are as follows:

* Learning rate between \(10^{-1}\) and \(10^{-3}\).
* Last activation: Softmax layer vs Normalized \(\tanh\). If Softmax Layer, we use \(\mathcal{L}_{CE}\), else we use \(\mathcal{L}_{BCE}\)
* Applying the softplus reparametrization to the weights or not.
* The bias reparametrization schema between no reparametrization, positive, projeccted and projected reparam.
* Regularization loss: either no regularization, or the projection onto constant set (choose between \(\mathcal{L}_{exact}\), \(\mathcal{L}_{uni}\) and \(\mathcal{L}_{nor}\)). If we apply regularization, then we also apply softplus reparametrization.
* If regularization: the coefficient \(c\) in the loss, either \(0.01\) or \(0.001\).
* If regularization: the number of batches to wait before applying the regularization, in [0, 5000, 10000, 15000, 20000].

For each regularization schema, we select the model with the best binary validation accuracy, and the corresponding results are displayed in Table 1. Detailed hyperparameter configurations are provided in Table 2 for reference.

We investigate the effects of each hyperparameter.

Bias ReparametrizationFor the choice of bias reparametrization function, we observed that not applying any reparametrization to the bias resulted in less robustness, and the network occasionally failed to learn. Applying projected reparametrization improved robustness, but it did not increase the proportion of activated neurons. Conversely, applying the positive projected and projected reparametrization functions increased the ratio of activated BiSE neurons from 0.9% to around 20%. Ensuring that the bias falls within the correct range of values enhances the interpretability of the network.

Regularization Delaywhen activating the regularization loss at the beginning of training, the results were notably worse, especially in binary accuracy. Our hypothesis is that the network does not have enough time to explore the right morphological operations and is prematurely drawn to a non-optimal operator, getting stuck in its vicinity. We observed that the accuracy improved when increasing the waiting time before activating the regularization loss. Future work may explore more advanced policies for applying the loss, such as using the loss at a given frequency instead of every batch, increasing the coefficient of the loss at each step, or waiting for the network to converge before applying regularization.

Regularization coefficient and last activationThe coefficient of the regularization loss did not have a significant impact, as well as replacing the normalized \(\tanh\) by a softmax layer.

\begin{table}
\begin{tabular}{l l l l l l l} \hline Architecture & \(W(\omega)\) & \(B(\omega)\) & Coef Regu & Regu Delay & Learning Rate & Last Act. \\ \hline DLUI (\(W=\text{Id}\)) & Id & Id & 0 & - & \(6.2\cdot 10^{-3}\) & \(\tanh\) \\ DLUI (No Regu) & \(f^{+}\) & \(f^{+}\) & 0 & - & \(9.8\cdot 10^{-2}\) & \(\tanh\) \\ DLUI \(\mathcal{L}_{exact}\) & \(f^{+}\) & proj. rep. & 0.01 & 10000 & \(4.2\cdot 10^{-2}\) & softmax \\ DLUI \(\mathcal{L}_{uni}\) & \(f^{+}\) & proj. rep. & 0.01 & 20000 & \(6.1\cdot 10^{-2}\) & softmax \\ DLUI \(\mathcal{L}_{nor}\) & \(f^{+}\) & Id & 0.001 & 10000 & \(5.4\cdot 10^{-2}\) & softmax \\ \hline \end{tabular}
\end{table}
Table 2: Best set of hyperparameters for each architecture