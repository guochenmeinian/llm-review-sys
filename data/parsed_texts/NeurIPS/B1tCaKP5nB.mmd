# A Conditional Independence Test in the

Presence of Discretization

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Testing conditional independence (CI) has many important applications, such as Bayesian network learning and causal discovery. Although several approaches have been developed for learning CI structures for observed variables, those existing methods generally fail to work when the variables of interest can not be directly observed and only discretized values of those variables are available. For example, if \(X_{1}\), \(\tilde{X}_{2}\) and \(X_{3}\) are the observed variables, where \(\tilde{X}_{2}\) is a discretization of the latent variable \(X_{2}\), applying the existing methods to the observations of \(X_{1}\), \(\tilde{X}_{2}\) and \(X_{3}\) would lead to a false conclusion about the underlying CI of variables \(X_{1}\), \(X_{2}\) and \(X_{3}\). Motivated by this, we propose a CI test specifically designed to accommodate the presence of discretization. To achieve this, a bridge equation and nodewise regression are used to recover the precision coefficients reflecting the conditional dependence of the latent continuous variables under the nonpara-normal model. An appropriate test statistic has been proposed, and its asymptotic distribution under the null hypothesis of CI has been derived. Theoretical analysis, along with empirical validation on various datasets, rigorously demonstrates the effectiveness of our testing methods.

## 1 Introduction

Independence and conditional independence (CI) are fundamental concepts in statistics. They are leveraged for exploring queries in statistical inference, such as sufficiency, parameter identification, adequacy, and ancillarity [9]. They also play a central role in emerging areas such as causal discovery [18], graphical model learning, and feature selection [36]. Tests for CI have attracted increasing attention from both theoretical and application sides.

Formally, the problem is to test the CI of two variables \(X_{j_{1}}\) and \(X_{j_{2}}\) given a random vector (a set of other variables) \(\bm{Z}\). In statistical notation, the null hypothesis is written as \(H_{0}:X_{j_{1}}\perp X_{j_{2}}\mid\bm{Z}\), where \(\perp\) denotes "independent from." The alternative hypothesis is written as \(H_{1}:X_{j_{1}}\not\perp X_{j_{2}}\mid\bm{Z}\), where \(\not\perp\) denotes "dependent with." The null hypothesis implies that once \(\bm{Z}\) is known, the values of \(X_{j_{1}}\) provide no additional information about \(X_{j_{2}}\), and vice versa. Different tests have been designed to handle different scenarios, including Gaussian variables with linear dependence [37, 25, 22, 26] and non-linear dependence [16, 38, 31, 27, 1] (_For detailed related work, please refer to App. D_).

Given observations of \(X_{j_{1}}\), \(X_{j_{2}}\), and \(\bm{Z}\), the CI can be effectively tested with existing methods. However, in many scenarios, accurately measuring continuous variables of interest is challenging due to limitations in data collection. Sometimes the data obtained are approximations represented as discretized values. For example, in finance, variables such as asset values cannot be measured and are binned into ranges for assessing investment risks (e.g., sell, hold, and strong buy) [7, 8]. Similarly, in mental health, anxiety levels are often assessed using scales like the GAD-7, which categorizesresponses into levels such as mild, moderate, or severe [23; 17]. In the entertainment industry, the quality of movies is typically summarized through viewer ratings [29; 10].

When discretization is present, existing CI tests can fail to determine the CI of underlying continuous variables. This issue arises because existing CI tests treat discretized observations as observations of continuous variables, leading to incorrect conclusions about their CI relationships. More precisely, the problem lies in the discretization process, which introduces new discrete variables. Consequently, _although the intent is to test the CI of the underlying continuous variables, what is actually being tested is the CI involving a mix of both continuous and newly introduced discrete variables_. In general, this CI relationship is inconsistent with the one among the underlying continuous variables.

As illustrated in Fig. 1, we show different data-generative processes using causal graphical models [24] in the presence of discretization. A gray node indicates an observable variable, while a white node indicates a latent variable. Variables denoted by \(X_{j}\) (without a tilde \(\sim\)) represent continuous variables, which may not be observed; while variables denoted by \(\tilde{X}_{j}\) represent observed discretized variables derived from \(X_{j}\) due to discretization. In Fig. 1(a), \(X_{2}\) is latent, and only its discrete counterpart \(\tilde{X}_{2}\) is observed. In this case, rather than observing \(X_{1}\), \(X_{2}\), and \(X_{3}\), we only observe \(X_{1}\), \(\tilde{X}_{2}\), and \(X_{3}\). Existing CI methods use these observations to test _whether_\(X_{1}\perp X_{3}\mid\{X_{2}\}\), but what is actually being tested is _whether_\(X_{1}\perp X_{3}\mid\{\tilde{X}_{2}\}\). In fact, according to the _causal Markov condition_[30],, it can be inferred from Fig. 1(a) that \(X_{1}\perp X_{3}\mid\{X_{2}\}\) and \(X_{1}\not\perp X_{3}\mid\{\tilde{X}_{2}\}\). This mismatch leads to existing CI methods, that employ observations to check the CI relationships between \(X_{1}\) and \(X_{3}\) given \(X_{2}\), to reach incorrect conclusions. Due to the same reason, checking the CI also fails in Fig 1(b) and Fig 1(c).

In this paper, we design a CI test specifically for handling the presence of discretization. An appropriate test statistic for the CI of latent continuous variables, based solely on discretized observations, is derived. The key is to build connections between the discretized observations and the parameters needed for testing the CI of the latent continuous variables. To achieve this, we first develop bridge equations that allow us to estimate the covariance of the underlying continuous variables with discretized observations. Then, we leverage a _node-wise regression_[5] to derive appropriate test statistics for CI relationships from the estimated covariance. By assuming that the continuous variables follow a Gaussian distribution, we can derive the asymptotic distributions of the test statistics under the null hypothesis of CI. The major contributions of our paper include that

* We develop a CI test for ensuring accurate analysis in scenarios where data has been discretized, which are common due to limitations in data collection or measurement techniques, such as in financial analysis and healthcare.
* Our CI test can handle various scenarios including 1). Both variables \(X_{j_{1}}\) and \(X_{j_{2}}\) are discretized 2). Both variables \(X_{j_{1}}\) and \(X_{j_{2}}\) are continuous. 3). One of the variables \(X_{j_{1}}\) or \(X_{j_{2}}\) is discretized.
* We compare our test with the existing methods on both synthetic and real-world datasets, confirming that our method can effectively estimate the CI of the underlying continuous variables and outperform the existing tests applied on the discretized observations.

## 2 DCT: A CI Test in the Presence of Discretization

Problem SettingConsider a set of independent and identically distributed (i.i.d.) \(p\)-dimensional random vectors, denoted as \(\tilde{\bm{X}}=(X_{1},X_{2},\ldots,\tilde{X}_{j},\ldots,\tilde{X}_{p})^{T}\). In this set, some variables, indicated by a tilde (\(\sim\)), such as \(\tilde{X}_{j}\), follow a discrete distribution. For each such variable, there exists a corresponding latent Gaussian random variable \(X_{j}\). The transformation from \(X_{j}\) to \(\tilde{X}_{j}\) is governed by an unknown monotone nonlinear function \(g_{j}\). This function, \(g_{j}:\mathcal{X}\rightarrow\tilde{\mathcal{X}}\), maps the continuous domain of \(X_{j}\) onto the discrete domain of \(\tilde{\mathcal{X}}_{j}\), such that \(\tilde{X}_{j}=g_{j}(X_{j})\) for each observation. Given \(n\) observations \(\{\tilde{\bm{x}}^{1},\tilde{\bm{x}}^{2},\ldots,\tilde{\bm{x}}^{n}\}\) randomly sampled from \(\tilde{\bm{X}}\), specifically, for each variable \(X_{j}\), there

Figure 1: We illustrate different data generative processes with causal graphical models. The discretization process introduces new discrete variables which are denoted with a tilde (\(\sim\)).

exists a constant vector \(\mathbf{d}=(d_{1},\ldots,d_{M})\) characterized by strictly increasing elements such that

\[\tilde{x}_{j}^{i}=\begin{cases}1&0<g_{j}(x_{j}^{i})<d_{1}\\ m&d_{m-1}<g_{j}(x_{j}^{i})<d_{m}\\ M&g_{j}(x_{j}^{i})>d_{m}\end{cases}\] (1)

This model is also known as the nonparanormal model [20]. The cardinality of the domain after discretization is at least 2 and smaller than infinity. Our goal is to assess both conditional and unconditional independence among the variables of the vector \(\bm{X}=(X_{1},X_{2},\ldots,X_{j},\ldots,X_{p})^{T}\). In our model, we assume \(\bm{X}\sim N(0,\Sigma)\), \(\Sigma\) only contain 1 among its diagonal, i.e., \(\sigma_{jj}=1\) for all \(j\in[1,\ldots,p]\). One should note this assumption is _without loss of generality_. We provide a detailed discussion of our assumption in App. A.8.

Preliminary Framework of DCTTo develop an independence test, one needs to design a test statistic that can reflect the dependence relation and be calculated from observations. Next, it is essential to derive the underlying distribution of this statistic under the null hypothesis that the tested variables are conditionally (or unconditionally) independent. By calculating the value of the test statistic from observations and determining if this statistic is likely to be sampled from the derived distribution (i.e., calculating the _p-value_ and comparing it with the significance level \(\alpha\)), we can decide if the null hypothesis should be rejected.

Our objective is to deduce the independence and CI relationships within the original multivariate Gaussian model, based on its discretized observations. In the context of a multivariate Gaussian model, this challenge is directly equivalent to constructing statistical inferences for its covariance matrix \(\bm{\Sigma}=(\sigma_{j_{1},j_{2}})\) and its precision matrix \(\bm{\Omega}=(\omega_{j,k})=\bm{\Sigma}^{-1}\)[3]. The covariance matrix \(\bm{\Sigma}\) captures the pairwise covariances between variables, while the precision matrix \(\bm{\Omega}\) (also known as the concentration matrix) provides information about the CI between variables. Specifically, the entry \(\omega_{j,k}\) in the precision matrix is related to the partial correlation coefficient between variables \(X_{j}\) and \(X_{k}\), which can be used to test whether these variables are conditionally independent given some other variables. Technically, we are interested in two things: (1) the calculation of the covariance \(\hat{\sigma}_{j_{1},j_{2}}\) and the precision coefficient (or the partial correlation coefficient) \(\hat{\omega}_{j,k}\), serving as the estimation of \(\sigma_{j_{1},j_{2}}\) and \(\omega_{j,k}\) respectively (in this paper, a variable with a hat indicates its estimation); and (2) the derivation of the distribution of \(\hat{\sigma}_{j_{1},j_{2}}-\sigma_{j_{1},j_{2}}\) and \(\hat{\omega}_{j,k}-\omega_{j,k}\) under the null hypothesis of independence and CI.

In the subsequent section, 1). we first introduce _bridge equations_ to address the estimation challenge of the covariance \(\sigma_{j_{1},j_{2}}\); 2). we proceed to derive the distribution of \(\hat{\sigma}_{j_{1},j_{2}}-\sigma_{j_{1},j_{2}}\), demonstrating it is _asymptotically normal_; 3). utilizing _nodewise regression_, we establish the relationship between the covariance matrix \(\bm{\Sigma}\) and the precision matrix \(\bm{\Omega}\), where the regression parameter \(\beta_{j,k}\) acts as an effective surrogate for \(\omega_{j,k}\). Leveraging the distribution of \(\hat{\sigma}_{j_{1},j_{2}}-\sigma_{j_{1},j_{2}}\), we further illustrate that \(\hat{\beta}_{j,k}-\beta_{j,k}\) is also _asymptotically normal_.

### Design _Bridge Equation_ for Test Statistics

Estimating Covariance with Bridge EquationsThe bridge equation establishes a connection between the underlying covariance \(\sigma_{j_{1},j_{2}}\) of two continuous variables \(X_{j_{1}}\) and \(X_{j_{2}}\) with the observations. When in the presence of discretization, the discrete transformations make the sample covariance matrix based on \(\tilde{\bm{X}}\) inconsistent with the covariance matrix of \(\bm{X}\). To obtain the estimation \(\hat{\sigma}_{j_{1},j_{2}}\) of \(\sigma_{j_{1},j_{2}}\), the bridge equation is leveraged. In general, its form is as follows.

\[\hat{\tau}_{j_{1},j_{2}}=T(\sigma_{j_{1},j_{2}};\hat{\bm{\Lambda}}),\] (2)

where \(\sigma_{j_{1},j_{2}}\) is the covariance needed to be estimated, \(\hat{\tau}_{j_{1},j_{2}}\) is a statistic that can also be estimated from observations, and \(\hat{\bm{\Lambda}}\) is a set of additional parameters required by the function \(T(\cdot)\). The specific form of the function \(T(\cdot)\) will be derived later. Both \(\hat{\tau}_{j_{1},j_{2}}\) and \(\hat{\bm{\Lambda}}\) should be able to be calculated purely relying on observations. _Then, given the calculated \(\hat{\tau}_{j_{1},j_{2}}\) and \(\hat{\bm{\Lambda}}\), \(\hat{\sigma}_{j_{1},j_{2}}\) can be obtained by solving the bridge equation \(\hat{\tau}_{j_{1},j_{2}}=T(\sigma_{j_{1},j_{2}};\hat{\bm{\Lambda}})\)._ As a result, the covariance matrix \(\bm{\Sigma}\) of \(\bm{X}\) can be estimated, which contains information about both unconditional independence and CI (which can be derived from its inverse).

To estimate the covariance of a latent multivariate Gaussian distribution, we need to design appropriate \(\hat{\tau}_{j_{1},j_{2}}\), \(\hat{\bm{\Lambda}}\), and \(T(\cdot)\). Notably, bridge equations have to be designed to handle all three possible cases:C1. both observed variables are discretized; C2. one variable is continuous while the other is discretized; and C3. both variables remain continuous. We will show that cases C1 and C2 can be merged into a single form of bridge equation with different parameters and a binarization operation applied to the observations. Our bridge equations are presented in Def. 2.2, Def. 2.3, and Def. 2.4.

Bridge Equations for _Discretized and Mixed Pairs_ Let us first address the challenging cases where both observed variables are discretized or where one variable is continuous while the other is discretized. In general, different bridge equations would need to be designed to handle each case individually. _However, in our analysis, we provide a unified bridge equation that is applicable to both cases._ This is achieved by binarizing the observed variables, thereby unifying both cases into a binary case. As some information may be lost in the binarization process, this unification may require more examples compared to using tailored bridge functions for each specific case. Developing specific bridge equations for each case to improve sample efficiency is left in future work.

Intiuitionally, for the original continuous variable \(X_{j}\), binarization separates it into two parts based on a boundary \(h_{j}\): the part for \(X_{j}\) larger than \(h_{j}\) and the part for \(X_{j}\) smaller than \(h_{j}\). In this case, we can estimate the boundary by calculating the proportion of \(X_{j}\) that exceeds the boundary. In the scenario of two variables where the threshold \(h_{j_{1}}\) and \(h_{j_{2}}\) divide the space into four regions, the proportions of these areas are influenced by the covariance \(\sigma_{j_{1},j_{2}}\), which connects the relation between the binarized variables with the latent covariance. This approach allows us to initially estimate the threshold \(h_{j_{1}}\), \(h_{j_{2}}\) of a pair of variables, followed by estimating the covariance \(\sigma_{j_{1},j_{2}}\).

Let \(\mathbb{P}_{n}Z\) denote the average of a random variable \(Z\) given \(n\) i.i.d. observation of \(Z\) and \(E[Z]\) as the true mean of \(Z\), \(\mathbb{P}\) as the probability and \(\hat{P}\) as the empirical probability. We then define the boundary \(h_{j}\) as follows: for any single discretized variable \(\tilde{X}_{j}\), there exists a constant \(c_{j}\) such that:

\[\mathbbm{1}\{\tilde{x}_{j}^{i}>E[\tilde{X}_{j}]\}=\mathbbm{1}\{g_{j}(x_{j}^{i })>c_{j}\}=\mathbbm{1}\{x_{j}^{i}>h_{j}\},\]

where \(h_{j}=g_{j}^{-1}(c_{j})\). Specifically, \(h_{j}\) is the boundary in the original continuous domain to determine if the discretized observation \(\tilde{X}_{k}\) is larger than its mean. When the continuous variable \(X_{j}\) follows a normal distribution, there is a relation \(\mathbb{P}(\tilde{X}_{j}>E[\tilde{X}_{j}])=1-\Phi(h_{j})\), where \(\Phi\) is the cumulative distribution function (cdf) of a standard normal distribution. We then provide the following definition:

**Definition 2.1**.: The estimated boundary can be expressed as \(\hat{h}_{j}=\Phi^{-1}(1-\hat{\tau}_{j})\), where \(\hat{\tau}_{j}=\sum_{i=1}^{n}\mathbbm{1}_{\{\tilde{x}_{j}^{i}\geq\mathbb{P}_{ n}\tilde{X}_{j}\}}/n\), serving as the estimation of \(\mathbb{P}(\tilde{X}_{j}>E[\tilde{X}_{j}])\).

Let \(\bar{\Phi}(z_{1},z_{2};\rho)=\mathbb{P}(Z_{1}>z_{1},Z_{2}>z_{2})\), where \((Z_{1},Z_{2})^{T}\) follows a bivariate normal distribution with mean zero, variance one and covariance \(\rho\). We define

\[\tau_{j_{1},j_{2}}=\mathbb{P}(\tilde{x}_{j_{1}}^{i}>E[\tilde{X}_{j_{1}}], \tilde{x}_{j_{2}}^{i}>E[\tilde{X}_{j_{2}}])=\bar{\Phi}(h_{j_{1}},h_{j_{2}}; \sigma_{j_{1},j_{2}}).\] (3)

That is, the proportion of discretized variables larger than their mean can be expressed as a function of underlying covariance. This equation serves as the key of estimating latent covariance based on the discretized observations. Specifically, we can substitute those true parameters with their estimation and construct the bridge equation to get the estimated covariance:

**Definition 2.2** (Bridge Equation for A Discretized-Variable Pair).: For discretized variables \(\tilde{X}_{j_{1}}\) and \(\tilde{X}_{j_{2}}\), the bridge equation is defined as:

\[\hat{\tau}_{j_{1},j_{2}}=\hat{P}(\tilde{X}_{j_{1}}>\mathbb{P}_{n}\tilde{X}_{j_ {1}},\tilde{X}_{j_{2}}>\mathbb{P}_{n}\tilde{X}_{j_{2}})=\frac{1}{n}\sum_{i=1}^ {n}\mathbbm{1}_{\{\tilde{x}_{j_{1}}^{i}>\mathbb{P}_{n}\tilde{X}_{j_{1}},\tilde {x}_{j_{2}}^{i}>\mathbb{P}_{n}\tilde{X}_{j_{2}}\}}=T(\sigma_{j_{1},j_{2}};\{ \hat{h}_{j_{1}},\hat{h}_{j_{2}}\}),\]

and the function \(T(\sigma_{j_{1},j_{2}};\{\hat{h}_{j_{1}},\hat{h}_{j_{2}}\}):=\bar{\Phi}(\hat{h} _{j_{1}},\hat{h}_{j_{2}};\sigma)=\int_{x_{1}>\hat{h}_{j_{1}}}\int_{x_{2}>\hat{ h}_{j_{2}}}\phi(x_{j_{1}},x_{j_{2}};\sigma)dx_{j_{1}}dx_{j_{2}},\)

where \(\phi\) is the probability density function of a bivariate normal distribution, \(\hat{h}_{j_{1}},\hat{h}_{j_{2}}\) can be simply calculated using Def. 2.1.

Following the same intuition, we can directly apply the same bridge equation to estimate the covariance of mixed pairs. The only difference is there is no need to estimate the boundary \(\hat{h}_{j}\) for the continuous variable. Instead, we can incorporate its true mean of zero into the equation.

**Definition 2.3** (Bridge Equation for A Continuous-Discretized-Variable Pair).: For one continuous variable \(X_{j_{1}}\) and one discretized variable \(\tilde{X}_{j_{2}}\), the bridge function is defined as follows:

\[\hat{\tau}_{j_{1},j_{2}}=\hat{P}(X_{j_{1}}>0,\tilde{X}_{j_{2}}>\mathbb{P}_{n} \tilde{X}_{j_{2}})=\frac{1}{n}\sum_{i=1}^{n}\mathds{1}_{\{x_{j_{1}}^{i}>0, \tilde{x}_{j_{2}}^{i}>\mathbb{P}_{n}\tilde{X}_{j_{2}}\}}=T(\sigma_{j_{1},j_{2}} ;\{0,\hat{h}_{j_{2}}\}),\]

and the function \(T(\cdot)\) has the same form of Def. 2.2.

A Bridge Equation for A Continuous-Variable PairWhen there is no discretized transformation, the sample covariance of \(X_{j_{1}}\) and \(X_{j_{2}}\) provides a consistent estimation. In this context, the function \(T\) acts merely as an identity mapping.

**Definition 2.4** (A Bridge Equation for A Continuous-Variable Pair).: For two continuous variables \(X_{j_{1}}\) and \(X_{j_{2}}\), the bridge equation is defined as:

\[\hat{\tau}_{j_{1},j_{2}}:=\hat{\sigma}_{j_{1},j_{2}}=\frac{1}{n}\sum_{i=1}^{n} x_{j_{1}}^{i}x_{j_{2}}^{i}-\frac{1}{n}\sum_{i=1}^{n}x_{j_{1}}^{i}\frac{1}{n} \sum_{i=1}^{n}x_{j_{2}}^{i}=T(\sigma_{j_{1},j_{2}};\emptyset).\]

For two continuous variables \(X_{j_{1}}\) and \(X_{j_{2}}\), the analytic solution of the estimated covariance can be simply obtained using Def. 2.4.

Calculation of Estimated CovarianceFor the continuous case, the analytic solution of \(\hat{\sigma}_{j_{1},j_{2}}\) can be simply obtained using Def. 2.4. For the cases involving the discretized variable as proposed in Def. 2.2 and Def. 2.3, we can rely on the property that variance \(\bm{\Sigma}\) only contains \(1\) among the diagonal, which implies the covariance \(\sigma_{j_{1},j_{2}}\) should vary from \(-1\) to \(1\). Thus, we can calculate the estimated covariance by solving the objective

\[\min_{\sigma_{j_{1},j_{2}}}||\hat{\tau}_{j_{1},j_{2}}-T(\sigma_{j_{1},j_{2}}; \{\hat{h}_{j_{1}},\hat{h}_{j_{2}}\})||^{2}\quad s.t.-1<\sigma_{j_{1},j_{2}}<1.\] (4)

The \(\hat{\tau}_{j_{1},j_{2}}\) is a one-to-one mapping with calculated \(\hat{\sigma}_{j_{1},j_{2}}\), \(\hat{h}_{j_{1}}\) and \(\hat{h}_{j_{2}}\), which is proved in App. A.2

### Unconditional Independence Test

The estimation of covariance \(\hat{\sigma}_{j_{1},j_{2}}\) can be effectively solved using the designed bridge equation. Now, we focus on deriving the distribution of \(\hat{\sigma}_{j_{1},j_{2}}-\sigma_{j_{1},j_{2}}\). These results is used as an unconditional independence test in the presence of the discretization. Moreover, Thm. 2.5, Lem. 2.6, Lem. 2.7 and Lem. 2.8 will be leveraged in the derivation process of the CI test in Section 2.3. The detailed derivation steps for both unconditional test and CI test are relatively intricate, therefore, we will provide a general intuition. For a complete derivation, please refer to the App. A.3.

Assume we are interested in the true parameter \(\theta_{0}\). We denote \(\hat{\theta}\) as its estimation which is close to \(\theta_{0}\), and \(f(\theta)\) is a continuous function. By leveraging Taylor expansion, we have

\[f(\hat{\theta})=f(\theta_{0})+f^{\prime}(\theta_{0})(\hat{\theta}-\theta_{0}),\] (5)

which directly constructs the relationship between the estimated parameter with the true one. Re-arrange the term, we get \(\hat{\theta}-\theta_{0}=(f(\hat{\theta})-f(\theta_{0}))/f^{\prime}(\theta_{0})\). If the denominator is a constant and the numerator can be expressed as a sum of i.i.d samples, we can see \(\hat{\theta}-\theta_{0}\) will be asymptotically normal according to the central limit theorem [35].

Let \(\psi_{\hat{\theta}}=[f_{\hat{\theta}}^{1}(\cdot),f_{\hat{\theta}}^{2}(\cdot),f_ {\hat{\theta}}^{3}(\cdot)]^{T}\) contains a group of functions parameterized by \(\hat{\theta}\) (For discretized pairs, \(\hat{\theta}=(\hat{\sigma}_{j_{1},j_{2}},\hat{h}_{j_{1}},\hat{h}_{j_{2}})\)). Define \(\mathbb{P}_{n}\psi_{\hat{\theta}}\) as sample mean of these functions evaluated at \(n\) sample points. Similarly, \(\mathbb{P}_{n}\psi_{\hat{\theta}}{\psi_{\hat{\theta}}}^{T}\) is defined as sample mean of the outer product \(\psi_{\hat{\theta}}{\psi_{\hat{\theta}}}^{T}\). The notation \(P\psi_{\hat{\theta}}:=E\mathbb{P}_{n}\psi_{\hat{\theta}}\) denotes the expectations of the functions in \(\psi_{\hat{\theta}}\). Furthermore, let \(\psi_{\hat{\theta}}{}^{\prime}\) denote the derivative of the functions contained in \(\psi_{\hat{\theta}}\). We now provide the main result of derived distribution \(\hat{\sigma}_{j_{1},j_{2}}-\sigma_{j_{1},j_{2}}\) under the hull hypothesis that test pairs are independent.

**Theorem 2.5** (Independence Test).: _In our settings, under the null hypothesis that two observed variables indexed with \(j_{1}\) and \(j_{2}\) are statistically independent under our framework, i.e., \(\sigma_{j_{1},j_{2}}=0\), the independence can be tested using the statistic_

\[\hat{\sigma}_{j_{1},j_{2}}=T^{-1}(\hat{\tau}_{j_{1},j_{2}};\hat{\theta}).\]_This statistic is approximated to follow a normal distribution, as detailed below:_

\[\hat{\sigma}_{j_{1},j_{2}}\stackrel{{\text{approx}}}{{\sim}}N\left( 0,\frac{1}{n}((\mathbb{P}_{n}\psi_{\hat{\theta}}^{\prime})^{-1}\mathbb{P}_{n} \psi_{\hat{\theta}}\psi_{\hat{\theta}}^{T}(\mathbb{P}_{n}\psi_{\hat{\theta}}^{ \prime T})^{-1})_{1,1}\right),\] (6)

_where the specific form of \(\psi_{\hat{\theta}}\) are presented in Lem. 2.6,Lem. 2.7 and Lem. 2.8._

We now provide the specific forms of \(\psi_{\hat{\theta}}\). Since the variables being tested for independence can be both discretized, only one being discretized, or neither being discretized. This results in different forms of \(\psi_{\hat{\theta}}\) consequently differs across these scenarios. Let \(Z_{j_{1}}\) and \(Z_{j_{2}}\) be any two random variables indexed by \(j_{1}\) and \(j_{2}\). Let \(\hat{\sigma}_{j_{1},j_{2}}^{i}=z_{j_{1}}^{i}\cdot z_{j_{2}}^{i}-\mathbb{P}_{n} Z_{j_{1}}\cdot\mathbb{P}_{n}Z_{j_{2}}\) denote the sample covariance based on a \(i\)-th pairwise observation of the variables \(Z_{j_{1}}\) and \(Z_{j_{2}}\). Let \(\hat{\tau}_{j_{1}}^{i}=\mathbbm{1}_{\{z_{j_{1}}^{i}\geq\mathbb{P}_{n}Z_{j_{1}}\}}\) and \(\hat{\tau}_{j_{2}}^{i}=\mathbbm{1}_{\{Z_{j_{2}}^{i}\geq\mathbb{P}_{n}Z_{j_{2}}\}}\), each calculated based on \(i\)-th observations of the variables \(Z_{j_{1}}\) and \(Z_{j_{2}}\), respectively. Let \(\hat{\tau}_{j_{1},j_{2}}^{i}\) be \(\hat{\tau}_{j_{1}}^{i}\cdot\hat{\tau}_{j_{2}}^{i}\). We further denote \(\bar{\Phi}(\cdot)=1-\Phi(\cdot)\). The different forms of \(\psi_{\hat{\theta}}\) that arise in different cases are defined as follows:

**Lemma 2.6**.: _(\(\psi_{\hat{\theta}}\) for A Continuous-Variable Pair). For two continuous variables \(X_{j_{1}}\) and \(X_{j_{2}}\),_

\[\psi_{\hat{\theta}}:=\hat{\sigma}_{j_{1},j_{2}}^{i}-\hat{\sigma}_{j_{1},j_{2}}.\] (7)

**Lemma 2.7** (\(\psi_{\hat{\theta}}\) for A Discretized-Variable Pair).: _For discretized variables \(\tilde{X}_{j_{1}}\) and \(\tilde{X}_{j_{2}}\),_

\[\psi_{\hat{\theta}}:=\begin{pmatrix}\hat{\tau}_{j_{1},j_{2}}^{i}-T(\hat{ \sigma}_{j_{1},j_{2}};\{\hat{h}_{j_{1}},\hat{h}_{j_{2}}\})\\ \hat{\tau}_{j_{1}}^{i}-\bar{\Phi}(\hat{h}_{j_{1}})\\ \hat{\tau}_{j_{2}}^{i}-\bar{\Phi}(\hat{h}_{j_{2}})\end{pmatrix}.\] (8)

**Lemma 2.8** (\(\psi_{\hat{\theta}}\) for A Continuous-Discretized-Variable Pair).: _For one discretized variable \(\tilde{X}_{j_{2}}\) and one continuous variable \(X_{j_{1}}\),_

\[\psi_{\hat{\theta}}:=\begin{pmatrix}\hat{\tau}_{j_{1},j_{2}}^{i}-T(\hat{ \sigma}_{j_{1},j_{2}};\{0,\hat{h}_{j_{2}}\})\\ \hat{\tau}_{j_{1}}^{i}-\bar{\Phi}(\hat{h}_{j_{2}})\end{pmatrix}.\] (9)

Derivation of forms of \(\psi_{\hat{\theta}}\) for different cases and their corresponding distribution defined in Eq (6) can be found in App. A.4, App. A.5, App. A.6. Up to this point, our discussion has been confined to the case of covariance \(\sigma_{j_{1},j_{2}}\), the indicator of unconditional independence. In the next section, we will present the results of our CI test.

### Conditional Independence (CI) Test

To construct a CI test of our model, we are interested at two things: calculation of the estimated precision coefficient \(\hat{\omega}_{j,k}\) and the derivation of the corresponding distribution \(\hat{\omega}_{j,k}-\omega_{j,k}\). In the following, we first build \(\beta_{j,k}\), which is obtained using nodewise regression and show it serves as a surrogate of testing for \(\omega_{j,k}=0\), we then construct the formulation of \(\hat{\beta}_{j,k}-\beta_{j,k}\) as the combination of formulation of \(\hat{\sigma}_{j_{1},j_{2}}-\sigma_{j_{1},j_{2}}\) and show it will also be asymptotically normal.

#### Nodewise Regression for CI

To utilize covariance for testing CI, it is necessary to establish a relationship between the estimated covariance and a metric capable of reflecting CI. To achieve this, we employ the nodewise regression which effectively builds the connection between covariance and precision matrix. Suppose we can access observations \(\{\bm{x}^{1},\bm{x}^{2},\ldots,\bm{x}^{n}\}\) from latent continuous variables \(\bm{X}=(X_{1},\ldots,X_{p})\sim N(0,\bm{\Sigma})\), nodewise regression will do regression on every dimension with all other dimensions as predictors.

\[x_{j_{1}}^{i}=\sum_{j_{1}\neq j_{2}}x_{j_{2}}^{i}\beta_{j}+\epsilon_{j_{1}}^{i}.\] (10)

It can be shown that there are deterministic relationships between the regression coefficients and the covariance and precision matrices of \(\bm{X}\), as illustrated below and proved in App. A.7.1.

\[\beta_{j}=\bm{\Sigma}_{-j-j}^{-1}\bm{\Sigma}_{-jj}\in\mathbb{R}^{p-1},\quad\beta _{j,k}=-\frac{\omega_{j,k}}{\omega_{j,j}},\quad j\neq k,\] (11)

where \(\bm{\Sigma}_{-j-j}\) is the submatrix of \(\bm{\Sigma}\) without \(j\)th column and \(j\)th row, and the \(\bm{\Sigma}_{-j}\) is the vector of \(j\)th column without \(j\)th row. \(\beta_{j,k}\in\mathbb{R}\) is the surrogate of \(\omega_{j,k}\) to capture the independence relationship of \(X_{j}\) with \(X_{k}\) conditioning on other variables. We can use Def. 2.2, Def. 2.3 and Def. 2.4 to get the estimation \(\hat{\bm{\Sigma}}_{-j-j}\) and \(\hat{\bm{\Sigma}}_{-jj}\) and thus get the estimation \(\hat{\beta}_{j}\).

Statistical Inference for \(\beta_{j,k}\)Nodewise regression offers a robust solution for the estimation problem. A pertinent inquiry pertains to the construction of the distribution of \(\hat{\beta}_{j}-\beta_{j}\). It is crucial to recognize that the distribution of \(\hat{\sigma}_{j_{1},j_{2}}-\sigma_{j_{1},j_{2}}\) is already established. Therefore, if we can conceptualize \(\hat{\beta}_{j}-\beta_{j}\) as a linear combination of \(\hat{\sigma}_{j_{1},j_{2}}-\sigma_{j_{1},j_{2}}\), the problem is directly solved, i.e., the \(\hat{\beta}_{j}-\beta_{j}\) is linear combination of dependent Gaussian variables. The underlying relationship between these variables is as follows:

\[\hat{\beta}_{j}-\beta_{j}=-\hat{\bm{\Sigma}}_{-j-j}^{-1}\left((\hat{\bm{ \Sigma}}_{-j-j}-\bm{\Sigma}_{-j-j})\beta_{j}-(\hat{\bm{\Sigma}}_{-jj}-\bm{\Sigma }_{-jj})\right).\]

The derivation is provided in App. A.7.2. For ease of notation, we further express the distribution of the difference between the estimated covariance and the true covariance as

\[\hat{\sigma}_{j_{1},j_{2}}-\sigma_{j_{1},j_{2}}=\frac{1}{n}\sum_{i=1}^{n}\xi_ {j_{1},j_{2}}^{i}.\] (12)

The specific form of \(\xi_{j_{1},j_{2}}^{i}\) is given in App. A.4, A.5, A.6 respectively for different cases. For notational convenience, we express \(\hat{\bm{\Sigma}}_{-j-j}-\bm{\Sigma}_{-j-j}=\frac{1}{n}\sum_{i=1}^{n}\Xi_{-j,- j}^{i}\) and \(\hat{\bm{\Sigma}}_{-jj}-\bm{\Sigma}_{-jj}=\frac{1}{n}\sum_{i=1}^{n}\Xi_{-j,j}^{i}\), where \(\xi_{j_{1},j_{2}}\) is the element of the matrix \(\Xi\) at the position indexed by \((j_{1},j_{2})\). We now propose the statistic and its asymptotic distribution for the CI test in the following theorem.

**Theorem 2.9** (Conditional Independence test).: _In our settings, under the null hypothesis that \(X_{j}\) and \(X_{k}\) are conditional statistically independent given a set of variables \(\bm{Z}\), i.e., \(\beta_{j,k}=0\), the statistic_

\[\hat{\beta}_{j,k}=(\hat{\bm{\Sigma}}_{-j-j}^{-1}\hat{\bm{\Sigma}}_{-jj})_{|k|},\] (13)

_where \([k]\) denotes the element corresponding to the variable \(X_{k}\) in \(\hat{\bm{\Sigma}}_{-j-j}^{-1}\hat{\bm{\Sigma}}_{-jj}\). The statistic \(\hat{\beta}_{j,k}\) has the asymptotic distribution:_

\[\hat{\beta}_{j,k}\sim N(0,{a^{[k]}}^{T}\frac{1}{n^{2}}\sum_{i=1}^{n}vec(B_{-j }^{i})vec(B_{-j}^{i})^{T})a^{[k]}),\]

\[\text{where }B^{i}=\begin{bmatrix}\Xi_{-j,-j}^{i}\\ \Xi_{-j,j}^{i}\end{bmatrix},\quad a_{l}^{[k]}=\begin{cases}\left(\hat{\bm{ \Sigma}}_{-j-j}^{-1}\right)_{[k],l},&\text{for }l\in\{1,\ldots,p-1\}\\ \sum_{q=1}^{n}\left(\hat{\bm{\Sigma}}_{-j-j}^{-1}\right)_{[k],l}\left(\hat{ \beta}_{j}\right)_{q},&\text{for }l\in\{p,\ldots,p^{2}-p\}\end{cases}\]

_and \(\tilde{\beta}_{j}\) is \(\beta_{j}\) whose \(\beta_{j,k}=0\)._

In practice, we can plug in the estimation of regression parameter \(\hat{\beta}_{j}\) and set \(\hat{\beta}_{j,k}=0\) as the substitution of \(\tilde{\beta}_{j}\) to calculate the variance and do the CI test. Specifically, we can obtain the \(\hat{\beta}_{j,k}\) using Eq. (13) where the estimated covariance terms can be calculated by solving the bridge equation Eq. 2. Under the null hypothesis that \(\beta_{j,k}=0\) (conditional independence), we can take the calculated \(\hat{\beta}_{j,k}\) into the distribution defined in Thm. 2.9 and obtain the p-value. If the p-value is smaller than the predefined significance level \(\alpha\) (normally set at 0.05), we will infer the tested pairs are conditionally dependent; otherwise, we do not. The detailed derivation of the Thm. 2.9 can be found in App. A.7.2.

## 3 Experiments

We applied the proposed method DCT to synthetic data to evaluate its practical performance and compare it with Fisher-Z test [14] (for all three data types) and Chi-Square test [15] (for discrete data only) as baselines. Specifically, we investigated its Type I and Type II error and its application in causal discovery. The experiments investigating its robustness, performance in denser graphs and effectiveness in a real-world dataset can be found in App. C.

### On the Effect of the Cardinality of Conditioning Set and the Sample Size

Our experiment investigates the variations in Type I and Type II error (1 minus power) probabilities under two conditions. In the first scenario, we focus on the effects of modifying the sample size, denoted as \(n=(100,500,1000,2000)\), while conditioning on a single variable. In the second, the sample size is held constant at 2000, and we vary the cardinality of the conditioning set, representedas \(D=(1,2,\ldots,5)\). It is assumed that every variable within this conditioning set is effective, i.e., they influence the CI of the tested pairs. We repeat each test \(1500\) times.

We use \(Y,W\) to denote the variables being tested and use \(Z\) to denote the variables being conditioned on. The discretized versions of the variables are denoted with a tilde symbol (e.g., \(\tilde{Z}\)). For both conditions, we evaluate three distinct types of observations of tested variables: continuous observations for both variables (\(Y,W\)), discrete observations for both variables (\(\tilde{Y},\tilde{W}\)) and a mixed type (\(\tilde{Y},W\)). The variables in the conditioning set will always be discretized observations (\(\tilde{Z}\)).

To see how well the derived asymptotic null distribution approximates the true one, we verify if the probability of Type I error aligns with the significance level \(\alpha\) preset in advance. We generate true continuous multivariate Gaussian data \(Y,W\) from \(Z_{i}\) (single \(i=1\) for the first scenario, and summed over \(n\) for the second), structured as \(a_{i}Z_{i}+E\) and \(\sum_{i=1}^{n}a_{i}Z_{i}+E\), where \(a_{i}\) is sampled from \(U(0.5,1.5)\) and \(E\) follows a standard normal distribution, independent of all other variables. This ensures \(Y\perp\!\!\!\perp W|Z\). The data are then discretized into \(K=(2,4,8,12)\) levels, with boundaries randomly set based on the variable range. The first column in Fig. 2 (a) (b) shows the resulting probability of Type I errors at the significance level \(\alpha=0.05\) compared with other methods.

A good test should have as small a probability of Type II error as possible, i.e., a larger power. To test the power of our DCT, we generate the continuous multivariate Gaussian data \(\tilde{Z}_{i}\) from \(Y,W\); constructed as \(Z_{i}=a_{i}Y+b_{i}W+E\), where \(a_{i},b_{i}\) are sampled from \(U(0.5,1.5)\) and \(E\) follows a standard normal distribution independent with all others, i.e., \(Y\perp\!\!\!\perp W|Z\). The same discretization approach is applied here. The second column in Fig. 2 (a) (b) shows the Type II error with the changing number of samples and cardinality of the conditioning set compared with other methods.

From Fig. 2 (a), we note that the Type I error rates with our derived null distribution are well-approximated at 0.05 across all three data types in both scenarios. In contrast, other testing methods show significantly higher Type I error rates, increasing with the number of samples and the size of the conditioning set. This indicates that such methods are more prone to erroneously concluding that tested variables are conditionally dependent. Additionally, while alternative tests demonstrate considerable power with smaller sample sizes, our approach requires a sample size of 2000 to achieve satisfactory power, particularly in mixed and continuous cases. A possible explanation for this phenomenon is that our method binarizes discretized data, which may not effectively utilize all observations. This aspect warrants further investigation in future research. Moreover, our test shows remarkable stability in response to changes in the number of conditioning sets.

Figure 2: Comparison of results of Type I and Type II error (\(1\) – power) for all three types of tested data (continuous, mixed, discrete) and different number of samples and cardinality of conditioning set. The suffix attached to a test’s name denotes the cardinality of discretization; for example, “Fisher_4” signifies the application of the Fisher-z test to data discretized into four levels. Chi-square test is only applicable for the discrete case.

### Application in Causal Discovery

Causal discovery aims to recover the true causal structure from the data. Constraint-based causal discovery methods like the PC algorithm [30] rely on testing CI from observations to discover causal graphs. However, in the presence of discretization, failures in testing CI leads to false conclusions about causal graphs. To evaluate the efficacy of the DCT, we construct causal graphs utilizing the Bipartite Pairing (BP) model as detailed in [2], with the number of edges being one fewer than the number of nodes. The detailed generation process is provided in App. B due to limited space. Our experiment is divided into two scenarios: (a) fixed data samples \(n=5000\), with changing number of nodes \(p=(4,6,8,10)\); (b) fixed number of nodes \(p=8\) and changing sample sizes \(n=(500,1000,5000,10000)\).

Comparative analysis is conducted using the PC algorithm integrated with different testing methods. Specifically, we compare DCT against the Fisher-Z test applied to discretized data, the chi-square test, and the Fisher-Z test on original continuous data, the latter serving as a theoretical upper bound for comparison. Since the PC algorithm can only return a completed partially directed acyclic graph (CPDAG), we use the same orientation rules [11] implemented by Causal-DAG [6] to convert a CPDAG into a DAG. We evaluate both the undirected skeleton and the directed graph using criteria such as structural Hamming distance (SHD), F1 score, precision, and recall. For each setting, we run 10 graph instances with different seeds and report the mean and standard deviation of skeleton discovery in Fig. 3, and DAG in Fig. 4 in App B.

According to the result, DCT exhibits performance nearly on par with the theoretical upper bound across metrics such as F1 score, precision, and Structural Hamming Distance (SHD) when the number of variables (\(p\)) is small and the sample size (\(n\)) is large. Despite a decline in performance as the number of variables increases with a smaller sample size, DCT significantly outperforms both the Fisher-Z test and the Chi-square test. Notably, in almost all settings, the recall of DCT is lower than that of the baseline tests, which is a reasonable outcome _since these tests tend to infer conditional dependencies, thereby retaining all edges given the discretized observations._ For instance, a fully connected graph, would achieve a recall of 1.

## 4 Conclusion

In this paper, we present a new testing method tailored for scenarios commonly encountered in real-world applications, where variables, though inherently continuous, are only observable in their discretized forms. Our method distinguishes itself from existing CI tests by effectively mitigating the misjudgment introduced by discretization and accurately recovering the CI relationships of latent continuous variables. We substantiate our approach with theoretical results and empirical validation, underscoring the effectiveness of our testing methods.

Figure 3: Experiment result of skeleton discovery on synthetic data for changing sample size (a) and changing number of nodes (b). Fisherz_nods is the Fisher-z test applied to original continuous data. We evaluate \(F_{1}\) (\(\uparrow\)), Precision (\(\uparrow\)), Recall (\(\uparrow\)) and SHD (\(\downarrow\)).

## References

* Aliferis et al. [2010] Constantin F Aliferis, Alexander Statnikov, Ioannis Tsamardinos, Subramani Mani, and Xenofon D Koutsoukos. Local causal and markov blanket induction for causal discovery and feature selection for classification part i: algorithms and empirical evaluation. _Journal of Machine Learning Research_, 11(1), 2010.
* Asratian et al. [1998] Armen S Asratian, Tristan MJ Denley, and Roland Haggkvist. _Bipartite graphs and their applications_, volume 131. Cambridge university press, 1998.
* Baba et al. [2004] Kunihiro Baba, Ritei Shibata, and Masaaki Sibuya. Partial correlation and conditional correlation as measures of conditional independence. _Australian & New Zealand Journal of Statistics_, 46(4):657-664, 2004.
* Baba et al. [2004] Kunihiro Baba, Ritei Shibata, and Masaaki Sibuya. Partial correlation and conditional correlation as measures of conditional independence. _Australian & New Zealand Journal of Statistics_, 46(4):657-664, 2004.
* Callot et al. [2019] Laurent Callot, Mehmet Caner, Esra Ulasan, and A. Ozlem Onder. A nodewise regression approach to estimating large portfolios, 2019.
* Squires [2018] Chandler Squires. _causaldag: creation, manipulation, and learning of causal models_, 2018.
* Changsheng and Yongfeng [2012] Hu Changsheng and Wang Yongfeng. Investor sentiment and assets valuation. _Systems Engineering Procedia_, 3:166-171, 2012.
* Damodaran [2012] Aswath Damodaran. _Investment valuation: Tools and techniques for determining the value of any asset_, volume 666. John Wiley & Sons, 2012.
* Dawid [1979] A Philip Dawid. Conditional independence in statistical theory. _Journal of the Royal Statistical Society Series B: Statistical Methodology_, 41(1):1-15, 1979.
* Dooms et al. [2013] Simon Dooms, Toon De Pessemier, and Luc Martens. Movietweetings: a movie rating dataset collected from twitter. In _Workshop on Crowdsourcing and human computation for recommender systems, CrowdRec at RecSys_, volume 2013, page 43, 2013.
* Dor and Tarsi [1992] Dorit Dor and Michael Tarsi. A simple algorithm to construct a consistent extension of a partially oriented graph. 1992.
* Doran et al. [2014] Gary Doran, Krikamol Muandet, Kun Zhang, and Bernhard Scholkopf. A permutation-based kernel conditional independence test. In _UAI_, pages 132-141, 2014.
* Fan et al. [2017] Jianqing Fan, Han Liu, Yang Ning, and Hui Zou. High dimensional semiparametric latent graphical model for mixed data. _Journal of the Royal Statistical Society Series B: Statistical Methodology_, 79(2):405-421, 2017.
* Fisher [1921] Ronald Aylmer Fisher. On the "Probable Error" of a Coefficient of Correlation Deduced from a Small Sample. _Metron_, 1:3-32, 1921.
* Pearson [2009] Karl Pearson F.R.S. X. on the criterion that a given system of deviations from the probable in the case of a correlated system of variables is such that it can be reasonably supposed to have arisen from random sampling. _Philosophical Magazine Series 1_, 50:157-175, 2009.
* Fukumizu et al. [2004] Kenji Fukumizu, Francis R Bach, and Michael I Jordan. Dimensionality reduction for supervised learning with reproducing kernel hilbert spaces. _Journal of Machine Learning Research_, 5(Jan):73-99, 2004.
* Johnson et al. [2019] Sverre Urnes Johnson, Pal Gunnar Ulvenes, Tux Oktedalen, and Asle Hoffart. Psychometric properties of the general anxiety disorder 7-item (gad-7) scale in a heterogeneous psychiatric sample. _Frontiers in psychology_, 10:449461, 2019.
* Koller and Friedman [2009] D. Koller and N. Friedman. _Probabilistic Graphical Models: Principles and Techniques_. Adaptive computation and machine learning. MIT Press, 2009.
* Li et al. [2024] Loka Li, Ignavier Ng, Gongxu Luo, Biwei Huang, Guangyi Chen, Tongliang Liu, Bin Gu, and Kun Zhang. Federated causal discovery from heterogeneous data, 2024.
* Liu et al. [2009] Han Liu, John Lafferty, and Larry Wasserman. The nonparanormal: Semiparametric estimation of high dimensional undirected graphs. _Journal of Machine Learning Research_, 10(10), 2009.

* [21] Dimitris Margaritis. Distribution-free learning of bayesian network structure in continuous domains. In _AAAI_, volume 5, pages 825-830, 2005.
* [22] Karthik Mohan, Mike Chung, Seungyeop Han, Daniela Witten, Su-In Lee, and Maryam Fazel. Structured learning of gaussian graphical models. _Advances in neural information processing systems_, 25, 2012.
* [23] Sarah A Mossman, Marissa J Luft, Heidi K Schroeder, Sara T Varney, David E Fleck, Drew H Barzman, Richard Gilman, Melissa P DelBello, and Jeffrey R Strawn. The generalized anxiety disorder 7-item (gad-7) scale in adolescents with generalized anxiety disorder: signal detection and validation. _Annals of clinical psychiatry: official journal of the American Academy of Clinical Psychiatrists_, 29(4):227, 2017.
* [24] Judea Pearl. _Causality: Models, Reasoning, and Inference_. Cambridge University Press, 2000.
* [25] Christine Peterson, Francesco C Stingo, and Marina Vannucci. Bayesian inference of multiple gaussian graphical models. _Journal of the American Statistical Association_, 110(509):159-174, 2015.
* [26] Zhao Ren, Tingni Sun, Cun-Hui Zhang, and Harrison H Zhou. Asymptotic normality and optimalities in estimation of large gaussian graphical models. 2015.
* [27] Rajat Sen, Ananda Theertha Suresh, Karthikeyan Shanmugam, Alexandros G Dimakis, and Sanjay Shakkottai. Model-powered conditional independence test. _Advances in neural information processing systems_, 30, 2017.
* [28] Shohei Shimizu, Takanori Inazumi, Yasuhiro Sogawa, Aapo Hyvarinen, Yoshinobu Kawahara, Takashi Washio, Patrik O. Hoyer, and Kenneth Bollen. Directlingam: A direct method for learning a linear non-gaussian structural equation model, 2011.
* [29] E Isaac Sparling and Shilad Sen. Rating: how difficult is it? In _Proceedings of the fifth ACM conference on Recommender systems_, pages 149-156, 2011.
* [30] P. Spirtes, C. Glymour, and R. Scheines. _Causation, Prediction, and Search_. MIT press, 2nd edition, 2000.
* [31] Eric V Strobl, Kun Zhang, and Shyam Visweswaran. Approximate kernel-based conditional independence tests for fast non-parametric causal discovery. _Journal of Causal Inference_, 7(1):20180017, 2019.
* [32] Liangjun Su and Halbert White. A nonparametric hellinger metric test for conditional independence. _Econometric Theory_, 24(4):829-864, 2008.
* [33] A. W. van der Vaart. _M-and Z-Estimators_, page 41-84. Cambridge Series in Statistical and Probabilistic Mathematics. Cambridge University Press, 1998.
* [34] A. W. van der Vaart. _Stochastic Convergence_, page 5-24. Cambridge Series in Statistical and Probabilistic Mathematics. Cambridge University Press, 1998.
* [35] Aad W Van der Vaart. _Asymptotic statistics_, volume 3. Cambridge university press, 2000.
* [36] Eric P Xing, Michael I Jordan, Richard M Karp, et al. Feature selection for high-dimensional genomic microarray data. In _Icml_, volume 1, pages 601-608. Citeseer, 2001.
* [37] Ming Yuan and Yi Lin. Model selection and estimation in the gaussian graphical model. _Biometrika_, 94(1):19-35, 2007.
* [38] Kun Zhang, Jonas Peters, Dominik Janzing, and Bernhard Scholkopf. Kernel-based conditional independence test and application in causal discovery. _arXiv preprint arXiv:1202.3775_, 2012.
* [39] Yishi Zhang, Zigang Zhang, Kaijun Liu, and Gangyi Qian. An improved iamb algorithm for markov blanket discovery. _J. Comput._, 5(11):1755-1761, 2010.
* [40] Yujia Zheng, Biwei Huang, Wei Chen, Joseph Ramsey, Mingming Gong, Ruichu Cai, Shohei Shimizu, Peter Spirtes, and Kun Zhang. Causal-learn: Causal discovery in python, 2023.

Appendix for
* **A Conditional Independence Test in the Presence of Discretization"** Appendix organization:

## Appendix A Proof of Things

* Proof of \(\hat{\theta}\overset{p}{\rightarrow}\theta_{0}\)
* Proof of one-to-one mapping between \(\hat{\tau}_{j_{1},j_{2}}\) with \(\hat{\sigma}_{j_{1},j_{2}}\)
* Proof of Thm. 2.5
* Derivation of Lem. 2.7
* Derivation of Lem. 2.8
* Derivation of Lem. 2.6
* Proof of Thm. 2.9
* Proof of Relation between \(\Sigma\), \(\Omega\) with \(\beta\)
* Detailed derivation of inference for \(\beta_{j}\)
* Discussion of assumption of zero mean and identity variance
* Data Generation and Figure of main experiments: causal discovery
* Additional experiments
* Linear non-Gaussian and nonlinear
* Denser graph
* multivariate Gaussian with nonzero mean and non-unit variance
* Real-world dataset
* Related Work
* Resource Usage
* Limiation and Broader Impacts

## Appendix A Proof of Things

### Proof of \(\hat{\theta}\overset{p}{\rightarrow}\theta_{0}\)

**Lemma A.1**.: _For the estimation \(\hat{\theta}\) which is calculated using bridge equation 2.422.3, as a zero of \(\Psi_{n}\) defined in Eq. (26),(33), (36), will converge in probability to \(\theta_{0}=(\sigma_{j_{1},j_{2}},h_{j_{1}},h_{j_{2}}),(\sigma_{j_{1},j_{2}},h _{j_{2}}),(\sigma_{j_{1},j_{2}})\) respectively._

Proof.: We first focus on the most challenging one where both variables are discrete. According to the law of large numbers, for the estimated boundary \(\hat{h}_{j_{1}}\) and \(\hat{h}_{j_{2}}\) whose calculations are defined as \(\hat{h}_{j}=\Phi^{-1}(1-\hat{\tau}_{j})\), we should have

\[n\rightarrow\infty,\quad\hat{\tau}_{j}=\frac{1}{n}\sum_{i=1}^{n}\mathbbm{1}_{\{ \tilde{x}_{j}^{i}>\mathbb{P}_{n}\tilde{X}_{j}\}}\xrightarrow{p}\mathbb{P}( \tilde{X}_{j}>E[\tilde{X}_{j}]).\] (14)

Recall the definition \(\mathbb{P}(\tilde{X}_{j}>E[\tilde{X}_{j}])=1-\Phi(h_{j})\), according to continuous mapping theorem [34], as long as the function \(\Phi^{-1}(1-\cdot)\) is continuous, we should have \(\hat{h}_{j}\xrightarrow{p}h_{j}\). And thus \(\hat{h}_{j_{1}}\xrightarrow{p}h_{j_{1}}\), \(\hat{h}_{j_{2}}\xrightarrow{p}h_{j_{2}}\).

We have \(\hat{\tau}_{j_{1},j_{2}}=\bar{\Phi}(\hat{h}_{j_{1}},\hat{h}_{j_{2}},\hat{ \sigma}_{j_{1},j_{2}})\) and the estimation \(\hat{\sigma}_{j_{1},j_{2}}\) can be obtained through solving the function. Similarly, we also have

\[n\rightarrow\infty,\quad\hat{\tau}_{j_{1},j_{2}}=\frac{1}{n}\sum_{i=1}^{n} \mathbbm{1}_{\{\tilde{x}_{j_{1}}^{i}>\mathbb{P}_{n}\tilde{X}_{j_{1}}\}} \mathbbm{1}_{\{\tilde{x}_{j_{2}}^{i}>\mathbb{P}_{n}\tilde{X}_{j_{2}}\}} \xrightarrow{p}\mathbb{P}(\tilde{x}_{j_{1}}^{i}>E[\tilde{X}_{j_{1}}],\tilde{ x}_{j_{2}}^{i}>E[\tilde{X}_{j_{2}}])=\tau_{j_{1},j_{2}}.\] (15)

Similarly, according to the continuous mapping theorem, we have \(\hat{\sigma}_{j_{1},j_{2}}\xrightarrow{p}\sigma_{j_{1},j_{2}}\). Thus, the parameter \((\hat{\sigma}_{j_{1},j_{2}},\hat{h}_{j_{1}},\hat{h}_{j_{2}})\xrightarrow{p}( \sigma_{j_{1},j_{2}},h_{j_{1}},h_{j_{2}})\).

Apparently, the result above could easily extend to the mixed case where we fix \(\hat{h}_{1}=h_{1}=0\). Using the same procedure, we should have \((\hat{\sigma}_{j_{1},j_{2}},\hat{h}_{j_{2}})\xrightarrow{p}(\sigma_{j_{1},j_ {2}},h_{j_{2}})\).

For the continuous case whose estimated variance is calculated as \(\hat{\sigma}_{j_{1},j_{2}}=\frac{1}{n}\sum_{i=1}^{n}x_{j_{1}}^{i}x_{j_{2}}^{i} -\frac{1}{n}\sum_{i=1}^{n}x_{j_{1}}^{i}\frac{1}{n}\sum_{i=1}^{n}x_{j_{2}}^{i}\), according to law of large numbers, we should have

\[n\rightarrow\infty,\quad\hat{\sigma}_{j_{1},j_{2}}=\frac{1}{n}\sum_{i=1}^{n}x _{j_{1}}^{i}x_{j_{2}}^{i}-\frac{1}{n}\sum_{i=1}^{n}x_{j_{1}}^{i}\frac{1}{n} \sum_{i=1}^{n}x_{j_{2}}^{i}\xrightarrow{p}E(X_{j_{1}}X_{j_{2}})-E(X_{j_{1}})E( X_{j_{2}})=\sigma_{j_{1},j_{2}}.\] (16)

Proof of one-to-one mapping between \(\hat{\tau}_{j_{1},j_{2}}\) with \(\hat{\sigma}_{j_{1},j_{2}}\)

**Lemma A.2**.: _For any fixed \(\hat{h}_{j_{1}}\) and \(\hat{h}_{j_{2}}\), \(T(\sigma_{j_{1},j_{2}};\{\hat{h}_{j_{1}},\hat{h}_{j_{2}}\})=\int_{x_{1}>\hat{h }_{j_{1}}}\int_{x_{2}>\hat{h}_{j_{2}}}\phi(x_{j_{1}},x_{j_{2}};\sigma)dx_{j_{1}} dx_{j_{2}}\), is a strictly monotonically increasing function on \(\sigma\in(-1,1)\)._

Proof.: To prove the lemma, we just need to show the gradient \(\frac{\partial T(\sigma_{j_{1},j_{2}};\{\hat{h}_{j_{1}},\hat{h}_{j_{2}}\}}{ \partial\sigma}>0\) for \(\sigma\in(-1,1)\).

\[\frac{\partial T(\sigma_{j_{1},j_{2}};\{\hat{h}_{j_{1}},\hat{h}_{j_{2}}\}}{ \partial\sigma}==\frac{1}{2\pi\sqrt{(1-\sigma^{2})}}\mathrm{exp}\left(-\frac{( \hat{h}_{j_{1}}^{2}-2\sigma\hat{h}_{j_{1}}\hat{h}_{j_{2}}+\hat{h}_{j_{2}}^{2} )}{2(1-\sigma^{2})}\right)\!,\] (17)

which is obviously positive for \(\sigma\in(-1,1)\). Thus, we have one-to-one mapping between \(\hat{\tau}_{j_{1}j_{2}}\) with the calculated \(\hat{\sigma}_{j_{1},j_{2}}\) for fixed \(\hat{h}_{j_{1}}\) and \(\hat{h}_{j_{2}}\).

### Proof of Thm. 2.5

In this section, we provide the proof of Thm. 2.5, which utilizes a regular statistical tool: Z-estimator [33]. Specifically, we are interested in the parameter \(\theta\) and we have it estimation \(\hat{\theta}\). Let \(\bm{x_{1}},\ldots,\bm{x_{n}}\) are sampled from some true distribution \(P\), we can construct the function characterized by the parameter \(\theta\) related the \(\bm{x}\) as \(\psi_{\theta}(\bm{x})\). As long as we have \(n\) observations, we can construct the function as follows

\[\Psi_{n}(\theta)=\frac{1}{n}\sum_{i=1}^{n}\psi_{\theta}(\bm{x}_{i})=\mathbb{P}_ {n}\psi_{\theta}.\] (18)

We further specify the form

\[\Psi(\theta)=\int\psi_{\theta}(\bm{x})d\bm{x}=P\psi_{\theta}.\] (19)

Assume the estimator \(\hat{\theta}\) is a zero of \(\Psi_{n}\), i.e., \(\Psi_{n}(\hat{\theta})=0\) and will converge in probability to \(\theta_{0}\), which is a zero of \(\Psi\), i.e., \(\Psi(\theta_{0})=0\). Expand \(\Psi_{n}(\hat{\theta})\) in a Taylor series around \(\theta_{0}\), we should have

\[0=\Psi_{n}(\hat{\theta})=\Psi_{n}(\theta_{0})+(\hat{\theta}-\theta_{0})\Psi_{n}^{ \prime}(\theta_{0})+\frac{1}{2}(\hat{\theta}-\theta_{0})\Psi_{n}^{\prime\prime}( \theta_{0}).\] (20)Rearrange the equation above, we have

\[\begin{split}\hat{\theta}-\theta_{0}&=-\frac{\Psi_{n}( \theta_{0})}{\Psi^{\prime}_{n}(\theta_{0})+\frac{1}{2}(\hat{\theta}-\theta_{0}) \Psi^{\prime\prime}_{n}(\theta_{0})}\\ &=-\frac{\frac{1}{n}\sum_{i=1}^{n}\psi_{\theta}(\bm{x}_{i})}{\Psi ^{\prime}_{n}(\theta_{0})+\frac{1}{2}(\hat{\theta}-\theta_{0})\Psi^{\prime \prime}_{n}(\theta_{0})}.\end{split}\] (21)

According to the central limit theorem, the numerator will be asymptotic normal with variance \(P\psi^{2}_{\theta_{0}}/n\) as the mean \(\Psi(\theta_{0})=0\) is zero. The first term of denominator \(\Psi^{\prime}_{n}(\theta_{0})\) will converge in probability to \(\Psi^{\prime}(\theta_{0})\) according to the law of large numbers. The second term \(\hat{\theta}-\theta_{0}=o_{P}(1)\). 1 As long as the denominator converges in probability and the numerator converges in distribution, according to Slusky's lemma, we have

Footnote 1: We will not provide proof of this in this paper; however, interested readers may refer to [33]

\[\sqrt{n}(\hat{\theta}-\theta_{0})\rightsquigarrow N\left(0,\frac{P\psi^{2}_{ \theta_{0}}}{(P\psi^{\prime}_{\theta_{0}})^{2}}\right).\] (22)

Extend into the high-dimensional case we should have

\[\hat{\theta}-\theta_{0}=-(\Psi^{\prime}_{n}(\theta_{0}))^{-1}\Psi_{n}(\theta_ {0}),\] (23)

where the second order term is omitted, further assume the matrix \(P\psi^{\prime}_{\theta_{0}}\) is invertible, we have

\[\sqrt{n}(\hat{\theta}-\theta_{0})\rightsquigarrow N\left(0,(P\psi^{\prime}_{ \theta_{0}})^{-1}P\psi_{\theta_{0}}\psi^{T}_{\theta_{0}}(P\psi^{\prime T}_{ \theta_{0}})^{-1}\right),\] (24)

Specifically, in our case \(\theta_{0}\) = (\(\sigma_{j_{1},j_{2}},\bm{\Lambda}\)), where \(\bm{\Lambda}\) is another parameter set influencing the estimation of \(\sigma_{j_{1},j_{2}}\) (will discuss case in case in later proof). In the practical scenario, we only have access to the estimated parameter \(\hat{\theta}\) and the empirical distribution \(\mathbb{P}_{n}\), thus we have

\[\hat{\sigma}_{j_{1},j_{2}}-\sigma_{j_{1},j_{2}}\overset{\text{ approx}}{\rightsquigarrow}N\left(0,((\mathbb{P}_{n}\psi^{\prime}_{\theta})^{-1} \mathbb{P}_{n}\psi_{\hat{\theta}}\psi^{T}_{\hat{\theta}}(\mathbb{P}_{n}\psi^{ \prime T}_{\hat{\theta}})^{-1})_{1,1}\right).\] (25)

Under the null hypothesis of independent, \(\sigma_{j_{1},j_{2}=0}\). We provide the proof that \(\hat{\theta}\overset{\text{p}}{\rightarrow}\theta_{0}\) of our case in App. A.1. Thus, \(\mathbb{P}_{n}\psi_{\hat{\theta}}\), the function parameterized by \(\hat{\theta}\), should also converge in \(\mathbb{P}_{n}\psi_{\hat{\theta}_{0}}\) when \(n\rightarrow\infty\). Besides, by the law of large numbers, \(\mathbb{P}_{n}\psi_{\hat{\theta}_{0}}\) will converge to \(P\psi_{\hat{\theta}_{0}}\). Thus, the equation above will converge to Eq. (24) when \(n\rightarrow\infty\).

### Derivation of Lem. 2.7

Let's first focus on the most challenging case where both variables are discretized observations and our interested parameter will include \(\hat{\theta}=(\hat{\sigma}_{j_{1},j_{2}},\hat{h}_{j_{1}},\hat{h}_{j_{2}})\) (Although we only care about the distribution of \(\hat{\sigma}_{j_{1},j_{2}}-\sigma_{j_{1},j_{2}}\), the estimation of boundary \(\hat{h}_{j_{1}}\)and \(\hat{h}_{j_{2}}\) will influence the estimation of \(\hat{\sigma}_{j_{1},j_{2}}\), thus we need to consider all of them).

The next step will be to _construct an appropriate criterion function \(\psi\) such that \(\Psi_{n}(\hat{\theta})=\bm{0}\)_. Given \(n\) observations \(\{\widetilde{\bm{x}}^{1},\widetilde{\bm{x}}^{2},\ldots,\widetilde{\bm{x}}^{ n}\}\), which are discretized version of \(\{\bm{x}^{1},\bm{x}^{2},\ldots,\bm{x}^{n}\}\) we should have

\[\Psi_{n}(\hat{\theta})=\begin{pmatrix}\Psi_{n}(\hat{\sigma}_{j_{1},j_{2}})\\ \Psi_{n}(\hat{h}_{j_{1}})\\ \Psi_{n}(\hat{h}_{j_{2}})\end{pmatrix}=\frac{1}{n}\sum_{i=1}^{n}\psi_{\hat{ \theta}}(\widetilde{\bm{x}}^{i})=\frac{1}{n}\sum_{i=1}^{n}\begin{pmatrix}\hat{ \tau}^{i}_{j_{1},j_{2}}-T(\hat{\sigma}_{j_{1},j_{2}};\{\hat{h}_{j_{1}},\hat{h }_{j_{2}}\})\\ \hat{\tau}^{i}_{j_{1}}-\hat{\Phi}(\hat{h}_{j_{1}})\\ \hat{\tau}^{i}_{j_{2}}-\hat{\Phi}(\hat{h}_{j_{2}})\end{pmatrix}=\bm{0}.\] (26)

\[\Psi_{n}(\theta_{0})=\begin{pmatrix}\Psi_{n}(\sigma_{j_{1},j_{2}})\\ \Psi_{n}(h_{j_{1}})\\ \Psi_{n}(h_{j_{2}})\end{pmatrix}=\frac{1}{n}\sum_{i=1}^{n}\psi_{\theta_{0}}( \widetilde{\bm{x}}^{i})=\frac{1}{n}\sum_{i=1}^{n}\begin{pmatrix}\hat{\tau}^{i}_ {j_{1},j_{2}}-T(\sigma_{j_{1},j_{2}};\{h_{j_{1}},h_{j_{2}}\})\\ \hat{\tau}^{i}_{j_{1}}-\hat{\Phi}(h_{j_{1}})\\ \hat{\tau}^{i}_{j_{2}}-\hat{\Phi}(h_{j_{2}})\end{pmatrix}.\] (27)

The difference between the estimated parameter with the true parameter can be expressed as

\[\hat{\theta}-\theta_{0}=\begin{pmatrix}\hat{\sigma}_{j_{1},j_{2}}- \sigma_{j_{1},j_{2}}\\ \hat{h}_{j_{1}}-h_{j_{1}}\\ \hat{h}_{j_{2}}-h_{j_{2}}\end{pmatrix}=-\frac{1}{n}\sum_{i=1}^{n}\begin{pmatrix} \frac{\partial\Psi_{n}(\sigma_{j_{1},j_{2}})}{\partial\sigma_{j_{1},j_{2}}}& \frac{\partial\Psi_{n}(\sigma_{j_{1},j_{2}})}{\partial h_{j_{1}}}&\frac{ \partial\Psi_{n}(\sigma_{j_{1},j_{2}})}{\partial h_{j_{1}}}\\ \frac{\partial\Psi_{n}(h_{j_{1}})}{\partial\sigma_{j_{1},j_{2}}}&\frac{\partial \Psi_{n}(\hat{h}_{j_{1}})}{\partial h_{j_{1}}}&\frac{\partial\Psi_{n}(\hat{h}_{ j_{1}})}{\partial h_{j_{2}}}\\ \frac{\partial\Psi_{n}(h_{j_{1}})}{\partial\sigma_{j_{1},j_{2}}}&\frac{\partial \Psi_{n}(h_{j_{1}

where the specific form of each entry of the gradient matrix is expressed as

\[\frac{\partial\Psi_{n}(\sigma_{j_{1},j_{2}})}{\partial\sigma_{j_{1},j_{2}}} =-\frac{1}{2\pi\sqrt{(1-\sigma_{j_{1},j_{2}}^{2})}}\mathrm{exp} \left(-\frac{(h_{j_{1}}^{2}-2\sigma_{j_{1},j_{2}}h_{j_{1}}h_{j_{2}}+h_{j_{2}}^{ 2})}{2(1-\sigma_{j_{1},j_{2}}^{2})}\right);\] \[\frac{\partial\Psi_{n}(\sigma_{j_{1},j_{2}})}{\partial h_{j_{1}}} =\int_{h_{j_{2}}}^{\infty}\frac{1}{2\pi\sqrt{1-\sigma_{j_{1},j_{2} }^{2}}}\exp\left(-\frac{h_{j_{1}}^{2}-2\sigma_{j_{1},j_{2}}h_{j_{1}}x_{2}+x_{2} ^{2}}{2(1-\sigma_{j_{1},j_{2}}^{2})}\right)dx_{2};\] \[\frac{\partial\Psi_{n}(\sigma_{j_{1},j_{2}})}{\partial h_{j_{2}}} =\int_{h_{j_{1}}}^{\infty}\frac{1}{2\pi\sqrt{1-\sigma_{j_{1},j_{2 }}^{2}}}\exp\left(-\frac{h_{2}^{2}-2\sigma_{j_{1},j_{2}}h_{j_{2}}x_{1}+x_{1}^{ 2}}{2(1-\sigma_{j_{1},j_{2}}^{2})}\right)dx_{1};\] \[\frac{\partial\Psi_{n}(h_{j_{1}})}{\partial\sigma_{j_{1},j_{2}}} =0;\] \[\frac{\partial\Psi_{n}(h_{j_{1}})}{\partial h_{j_{1}}} =\frac{1}{\sqrt{2\pi}}\exp\left(-\frac{h_{j_{1}}^{2}}{2}\right);\] \[\frac{\partial\Psi_{n}(h_{j_{1}})}{\partial h_{j_{2}}} =0;\] \[\frac{\partial\Psi_{n}(h_{j_{2}})}{\partial\sigma_{j_{1},j_{2}}} =0;\] \[\frac{\partial\Psi_{n}(h_{j_{2}})}{\partial h_{j_{1}}} =0;\] \[\frac{\partial\Psi_{n}(h_{j_{2}})}{\partial h_{j_{2}}} =\frac{1}{\sqrt{2\pi}}\exp\left(-\frac{h_{j_{2}}^{2}}{2}\right).\]

For simplicity of notation, we define

\[\hat{\sigma}_{j_{1},j_{2}}-\sigma_{j_{1},j_{2}}=\frac{1}{n}\sum_{i=1}^{n}\xi_ {j_{1},j_{2}}^{i},\] (30)

where the specific form is of \(\{\xi_{j_{1},j_{2}}^{i}\}\) is defined in Eq. (28). We should note that \(\{\xi_{j_{1},j_{2}}^{i}\}\) are i.i.d random variables with mean zero (this property will be the key to the derivation of inference of CI). As long as our estimation \(\hat{\theta}\) converge in probability to \(\theta_{0}\) as proved in A.1, we have

\[\sqrt{n}(\hat{\theta}-\theta_{0})\rightsquigarrow N\left(0,((P\psi_{\theta_{0 }}^{\prime})^{-1}P\psi_{\theta_{0}}\psi_{\theta_{0}}^{T}(P\psi_{\theta_{0}}^{ \prime T})^{-1})_{1,1}\right),\] (31)

where \(\psi_{\theta_{0}}\) is defined in Eq. (27). However, in practice, we don't have access to either \(P\) or \(\theta_{0}\). In this scenario, we can plug in the empirical distribution of \(\mathbb{P}_{n}\psi_{\hat{\theta}}\) to get the estimated variance, i.e., the actual variance used in the calculation of \(\hat{\sigma}_{j_{1},j_{2}}-\sigma_{j_{1},j_{2}}\) is

\[\frac{1}{n}\left((\mathbb{P}_{n}\psi_{\hat{\theta}}^{\prime})^{-1}\mathbb{P}_{ n}\psi_{\hat{\theta}}\psi_{\hat{\theta}}^{T}(\mathbb{P}_{n}\psi_{\hat{ \theta}}^{\prime T})^{-1}\right)_{1,1}.\] (32)

### Derivation of Lem. 2.8

Use the same line of procedure as in the derivation of Lem. 2.7, for mixed pair of observations where \(X_{j_{1}}\) is continuous and \(\tilde{X}_{j_{2}}\) is discrete, we can construct the criterion function

\[\Psi_{n}(\hat{\theta})=\begin{pmatrix}\Psi_{n}(\hat{\sigma}_{j_{1},j_{2}})\\ \Psi_{n}(h_{j_{2}})\end{pmatrix}=\frac{1}{n}\sum_{i=1}^{n}\psi_{\hat{\theta}}( \tilde{\bm{x}}^{i})=\frac{1}{n}\sum_{i=1}^{n}\begin{pmatrix}\hat{\tau}_{j_{1},j _{2}}^{i}-T(\hat{\sigma}_{j_{1},j_{2}};\{0,\hat{h}_{j_{2}}\})\\ \hat{\tau}_{j_{2}}^{i}-\bar{\Phi}(\hat{h}_{j_{2}})\end{pmatrix}=\bm{0}.\] (33)

\[\Psi_{n}(\theta_{0})=\begin{pmatrix}\Psi_{n}(\sigma_{j_{1},j_{2}})\\ \Psi_{n}(h_{j_{2}})\end{pmatrix}=\frac{1}{n}\sum_{i=1}^{n}\psi_{\theta_{0}}( \tilde{\bm{x}}^{i})=\frac{1}{n}\sum_{i=1}^{n}\begin{pmatrix}\hat{\tau}_{j_{1},j _{2}}^{i}-T(\hat{\sigma}_{j_{1},j_{2}};\{0,h_{j_{2}}\})\\ \hat{\tau}_{j_{2}}^{i}-\bar{\Phi}(h_{j_{2}})\end{pmatrix}.\] (34)The difference between the estimated parameter with the true parameter can be expressed as

\[\hat{\theta}-\theta_{0}=\begin{pmatrix}\hat{\sigma}_{j_{1},j_{2}}-\sigma_{j_{1},j _{2}}\\ \hat{h}_{j_{2}}-h_{j_{2}}\end{pmatrix}=-\frac{1}{n}\sum_{i=1}^{n}\begin{pmatrix} \frac{\partial\Psi_{n}(\sigma_{j_{1},j_{2}})}{\partial\sigma_{j_{1},j_{2}}}& \frac{\partial\Psi_{n}(\sigma_{j_{1},j_{2}})}{\partial h_{j_{2}}}\\ \frac{\partial\Psi_{n}(h_{j_{2}})}{\partial\sigma_{j_{1},j_{2}}}&\frac{ \partial\Psi_{n}(h_{j_{2}})}{\partial h_{j_{2}}}\end{pmatrix}^{-1}\begin{pmatrix} \hat{\tau}_{j_{1},j_{2}}^{i}-T(\sigma_{j_{1},j_{2}};\{0,h_{j_{2}}\})\\ \hat{\tau}_{j_{2}}^{i}-\hat{\Phi}(h_{j_{2}}).\end{pmatrix},\] (35)

where the specific form of each entry of the gradient matrix can be found in Eq. (29). Using exactly the same procedure, we should have the same formation of the variance calculated as Eq. (32) with a different definition of \(\psi_{\theta_{0}}\) and \(\psi_{\tilde{\theta}}\) defined in Eq. (34) (33).

### Derivation of Lem. 2.6

Use the same line of procedure as in derivation of Lem. 2.7, for a continuous pair of variables, we can construct the criterion function

\[\Psi_{n}(\hat{\theta})=\Psi_{n}(\hat{\sigma}_{j_{1},j_{2}})=\frac{1}{n}\sum_{ i=1}^{n}x_{j_{1}}^{i}x_{j_{2}}^{i}-\frac{1}{n}\sum_{i=1}^{n}x_{j_{1}}^{i} \frac{1}{n}\sum_{i=1}^{n}x_{j_{2}}^{i}-\hat{\sigma}_{j_{1},j_{2}}=0.\] (36)

\[\Psi_{n}(\theta_{0})=\Psi_{n}(\sigma_{j_{1},j_{2}})=\frac{1}{n}\sum_{i=1}^{n} x_{j_{1}}^{i}x_{j_{2}}^{i}-\frac{1}{n}\sum_{i=1}^{n}x_{j_{1}}^{i}\frac{1}{n} \sum_{i=1}^{n}x_{j_{2}}^{i}-\sigma_{j_{1},j_{2}}.\] (37)

Denote \(\frac{1}{n}\sum_{i=1}^{n}x_{j_{1}}^{i}\) as \(\bar{x}_{j_{1}}\) and \(\frac{1}{n}\sum_{i=1}^{n}x_{j_{2}}^{i}\) as \(\bar{x}_{j_{2}}\). We should have

\[\hat{\sigma}_{j_{1},j_{2}}-\sigma_{j_{1},j_{2}}=\frac{1}{n}\sum_{i=1}^{n}x_{j_ {1}}^{i}x_{j_{2}}^{i}-\bar{x}_{j_{1}}\bar{x}_{j_{2}}-\sigma_{j_{1},j_{2}}.\] (38)

According to Eq. (22), we have

\[\sqrt{n}(\hat{\sigma}_{j_{1},j_{2}}-\sigma_{j_{1},j_{2}})\rightsquigarrow N \left(0,\frac{P\psi_{\theta_{0}}^{2}}{(P\psi_{\theta_{0}}^{\prime})^{2}} \right).\] (39)

where \((P\psi_{\theta_{0}}^{\prime})^{2}=1\). In practical calculation, we have the variance

\[\frac{1}{n}\mathbb{P}_{n}\psi_{\theta}^{2}/(\mathbb{P}_{n}\psi_{\tilde{\theta }}^{\prime})^{2}=\frac{1}{n^{2}}\sum_{i=1}^{n}(x_{j_{1}}^{i}x_{j_{2}}^{i}-\bar{ x}_{j_{1}}\bar{x}_{j_{2}}-\hat{\sigma}_{j_{1},j_{2}})^{2}.\] (40)

### Proof of Thm. 2.9

#### a.7.1 Proof of Relation between \(\Sigma\), \(\Omega\) with \(\beta\)

Consider our latent continuous variables \(\bm{X}=(X_{1},\ldots,X_{p})\sim N(0,\bm{\Sigma})\) and do nodewise regression

\[X_{j}=X_{-j}\beta_{j}+\epsilon_{j}.\] (41)

We can divide its covariance \(\bm{\Sigma}\) and its precision matrix \(\Omega=\bm{\Sigma}^{-1}\) into \(X\) and \(Y\) part in our regression:

\[\bm{\Sigma}=\begin{pmatrix}\bm{\Sigma}_{jj}&\bm{\Sigma}_{j-j}\\ \bm{\Sigma}_{-jj}&\bm{\Sigma}_{-j-j}\end{pmatrix}\quad\bm{\Omega}=\begin{pmatrix} \bm{\Omega}_{jj}&\bm{\Omega}_{j-j}\\ \bm{\Omega}_{-jj}&\bm{\Omega}_{-j-j}\end{pmatrix}\.\] (42)

Just like regular linear regression, we can get

\[n\rightarrow\infty,\quad\beta_{j}=\bm{\Sigma}_{-j-j}^{-1}\bm{\Sigma}_{-jj}.\] (43)

From the invertibility of a block matrix

\[\begin{bmatrix}A&B\\ C&D\end{bmatrix}^{-1}=\begin{bmatrix}(A-BD^{-1}C)^{-1}&-(A-BD^{-1}C)^{-1}BD^{- 1}\\ -D^{-1}C(A-BD^{-1}C)^{-1}&D^{-1}+D^{-1}C(A-BD^{-1}C)^{-1}BD^{-1}\end{bmatrix}.\] (44)

If \(A\) and \(D\) is invertible, we will have

\[\begin{bmatrix}A&B\\ C&D\end{bmatrix}^{-1}=\begin{bmatrix}A-BD^{-1}C&0\\ 0&(D-CA^{-1}B)^{-1}\end{bmatrix}\begin{bmatrix}I&-BD^{-1}\\ -CA^{-1}&I\end{bmatrix}.\] (45)Thus, we can get:

\[\bm{\Omega}_{jj}=\bm{\Sigma}_{jj}-(\bm{\Sigma}_{j-j}\bm{\Sigma}_{-j-j}^{-1}\bm{ \Sigma}_{-jj})^{-1};\] (46)

Move one step forward:

\[-\bm{\Omega}_{jj}^{-1}\bm{\Omega}_{j-j}=\bm{\Sigma}_{j-j}(\bm{\Sigma}_{-j-j})^{ -1}.\] (47)

Take transpose for both sides, as long as \(\bm{\Omega}\) is a symmetric matrix and \(\bm{\Omega}_{-jj}=\bm{\Omega}_{j-j}^{T}\), we will have

\[-\bm{\Omega}_{jj}^{-1}\bm{\Omega}_{-jj}=\bm{\Sigma}_{-j-j}^{-1}\bm{\Sigma}_{- jj}=\beta_{j}.\] (48)

We should note testing \(\bm{\Omega}_{-jj}=0\) is equivalent to testing \(\beta_{j}=0\) as the \(\bm{\Omega}_{jj}\) will always be nonzero. The variable \(\bm{\Omega}_{-jj}\) captures the CI of \(X_{j}\) with other variables. As long as the variable \(\bm{\Omega}_{jj}\) is just one scalar, we can get

\[\beta_{j,k}=-\frac{\omega_{j,k}}{\omega_{j,j}}\] (49)

capturing the independence relationship between variable \(X_{j}\) with \(X_{k}\) conditioning on all other variables.

#### a.7.2 Detailed derivation of inference for \(\beta_{j}\)

Nodewise regression allows us to use the regression parameter \(\beta_{j}\) as the surrogate of \(\Omega_{-jj}\). The problem now transfers to constructing the inference for \(\beta_{j}\), specifically, the derivation of distribution of \(\hat{\beta}_{j}-\beta_{j}\). The overarching concept is that we are already aware of the distribution of \(\hat{\sigma}_{j_{1},j_{2}}-\sigma_{j_{1},j_{2}}\) and we know that there exists a deterministic relationship between \(\beta_{j}\) with \(\bm{\Sigma}\). Consequently, we can express \(\hat{\beta}_{j}-\beta_{j}\) as a composite of \(\hat{\sigma}_{j_{1},j_{2}}-\sigma_{j_{1},j_{2}}\) to establish such an inference. Specifically, we have

\[\hat{\beta}_{j}-\beta_{j} =\hat{\bm{\Sigma}}_{-j-j}^{-1}\hat{\bm{\Sigma}}_{-jj}-\bm{\Sigma} _{-j-j}^{-1}\bm{\Sigma}_{-jj}\] (50) \[=\hat{\bm{\Sigma}}_{-j-j}^{-1}\left(\hat{\bm{\Sigma}}_{-jj}-\hat{ \bm{\Sigma}}_{-j-j}\bm{\Sigma}_{-j-j}^{-1}\bm{\Sigma}_{-jj}\right)\] \[=-\hat{\bm{\Sigma}}_{-j-j}^{-1}\left(\hat{\bm{\Sigma}}_{-j-j} \beta_{j}-\bm{\Sigma}_{-j-j}\beta_{j}+\bm{\Sigma}_{-j-j}\beta_{j}-\hat{\bm{ \Sigma}}_{-jj}\right)\] \[=-\hat{\bm{\Sigma}}_{-j-j}^{-1}\left((\hat{\bm{\Sigma}}_{-j-j}- \bm{\Sigma}_{-j-j})\beta_{j}-(\hat{\bm{\Sigma}}_{-jj}-\bm{\Sigma}_{-jj})\right),\]

where each entry in matrix \((\hat{\bm{\Sigma}}_{-j-j}-\bm{\Sigma}_{-j-j})\) and \((\hat{\bm{\Sigma}}_{-jj}-\bm{\Sigma}_{-jj})\) denotes the difference between estimated covariance with true covariance. Suppose that we want to test the CI of the variable \(X_{1}\) with other variables, \(j=1\), then

\[\hat{\bm{\Sigma}}_{-j-j}-\bm{\Sigma}_{-j-j} =\begin{bmatrix}\hat{\sigma}_{1,1}\dots\hat{\sigma}_{1,j-1},\hat{ \sigma}_{1,j+1}\dots\hat{\sigma}_{1,p}\\ \dots\\ \hat{\sigma}_{j-1,1}\dots\hat{\sigma}_{j-1,j-1},\hat{\sigma}_{j-1,j+1}\dots \hat{\sigma}_{j-1,p}\\ \dots\\ \hat{\sigma}_{p,1}\dots\hat{\sigma}_{p,j-1},\hat{\sigma}_{p,j+1}\dots\hat{ \sigma}_{p,p}\end{bmatrix}\] (51) \[-\begin{bmatrix}\sigma_{1,1}\dots\sigma_{1,j-1},\sigma_{1,j+1} \dots\sigma_{1,p}\\ \dots\\ \sigma_{j-1,1}\dots\sigma_{j-1,j-1},\sigma_{j-1,j+1}\dots\sigma_{j-1,p}\\ \dots\\ \sigma_{p,1}\dots\sigma_{p,j-1},\sigma_{p,j+1}\dots\sigma_{p,p}.\end{bmatrix}.\] (52)

Suppose that we want to test the CI of the variable \(X_{1}\) with other variables, \(j=1\). then

\[\hat{\bm{\Sigma}}_{-1-1}-\bm{\Sigma}_{-1-1} =\begin{bmatrix}\hat{\sigma}_{2,2}\dots\hat{\sigma}_{2,p}\\ \dots\\ \hat{\sigma}_{p,2}\dots\hat{\sigma}_{p,p}\end{bmatrix}-\begin{bmatrix}\sigma_{2,2}\dots\sigma_{2,p}\\ \dots\\ \sigma_{p,2}\dots\sigma_{p,p}\end{bmatrix}\] (53) \[:=\frac{1}{n}\sum_{i=1}^{n}\begin{bmatrix}\xi_{2,2}^{i}\dots\xi_ {2,p}^{i}\\ \dots\\ \xi_{p,2}^{i}\dots\xi_{p,p}^{i}\end{bmatrix},\] (54)where \(\{\xi_{i_{1},j_{2}}^{i}\}\) are i.i.d random variables with specific form defined in Eq. (28) for discrete case, Eq. (35) for mixed case and Eq. (38) in continuous case. Put them together:

\[\begin{bmatrix}\hat{\beta}_{1,2}-\beta_{1,2}\\ \hat{\beta}_{1,3}-\beta_{1,3}\\ \dots\\ \hat{\beta}_{1,p}-\beta_{1,p}\end{bmatrix}=-\hat{\boldsymbol{\Sigma}}_{-1-1}^{- 1}\frac{1}{n}\sum_{i=1}^{n}\begin{pmatrix}\xi_{2,2}^{i}&\xi_{2,3}^{i}&\dots&\xi_ {2,p}^{i}\\ \xi_{3,2}^{i}&\xi_{3,3}^{i}&\dots&\xi_{3,p}^{i}\\ \dots&\dots&\dots&\dots\\ \xi_{p,2}^{i}&\xi_{p,3}^{i}&\dots&\xi_{p,p,p}^{i}\end{pmatrix}\begin{bmatrix} \beta_{1,2}\\ \beta_{1,3}\\ \beta_{1,3}\\ \beta_{1,p}\end{bmatrix}-\begin{bmatrix}\xi_{2,1}^{i}\\ \xi_{3,1}^{i}\\ \dots\\ \xi_{p,1}^{i}\end{bmatrix}\end{pmatrix}.\] (55)

As \(\frac{1}{n}\sum_{i=1}^{n}\xi_{j_{1},j_{2}}^{i}\) is asymptotically normal, the who vector of \(\hat{\beta}_{1}-\beta_{1}\) is a linear combination of Gaussian distribution. However, We cannot merely engage in a linear combination of its variance as they are dependent with each other. For example, if \(Y_{1},Y_{2}\) are dependent and we are trying to find out \(Var(aY_{1}+bY_{2})\), we should have

\[Var(aY_{1}+bY_{2})=[a\quad b]\begin{bmatrix}Var(Y_{1})&Cov(Y_{1},Y_{2})\\ Cov(Y_{1},Y_{2})&Var(Y_{2})\end{bmatrix}\begin{bmatrix}a\\ b\end{bmatrix}.\] (56)

Now, suppose we are interested in the distribution of \(\hat{\beta}_{1,2}-\beta_{1,2}\), we should have

\[\hat{\beta}_{1,2}-\beta_{1,2}=\frac{1}{n}\sum_{i=1}^{n}(\hat{ \boldsymbol{\Sigma}}_{-1-1}^{-1})_{[2],:}\begin{pmatrix}\xi_{2,2}^{i}&\xi_{2,3 }^{i}&\dots&\xi_{2,p}^{i}\\ \xi_{3,2}^{i}&\xi_{3,3}^{i}&\dots&\xi_{3,p}^{i}\\ \dots&\dots&\dots&\dots\\ \xi_{p,2}^{i}&\xi_{p,3}^{i}&\dots&\xi_{p,p}^{i}\end{pmatrix}\begin{bmatrix} \beta_{1,2}\\ \beta_{1,3}\\ \dots\\ \beta_{1,p}\end{bmatrix}-\begin{bmatrix}\xi_{2,1}^{i}\\ \xi_{3,1}^{i}\\ \vdots\\ \xi_{p,1}^{i}\end{bmatrix}\end{pmatrix},\] (57)

where \((\hat{\boldsymbol{\Sigma}}_{-1-1}^{-1})_{[2],:}\) is the row of index of \(X_{2}\) of \(\hat{\boldsymbol{\Sigma}}_{-1-1}^{-1}\) (\([2]\) denotes the index of the variable). For ease of notation, let

\[\Xi_{-1,-1}^{i}=\begin{bmatrix}\xi_{2,2}^{i}&\xi_{2,3}^{i}&\dots&\xi_{2,p}^{i }\\ \xi_{3,2}^{i}&\xi_{3,3}^{i}&\dots&\xi_{3,p}^{i}\\ \dots&\dots&\dots&\dots\\ \xi_{p,2}^{i}&\xi_{p,3}^{i}&\dots&\xi_{p,p}^{i}\end{bmatrix},\qquad\Xi_{-1, 1}^{i}=\begin{bmatrix}\xi_{2,1}^{i}\\ \xi_{3,1}^{i}\\ \dots\\ \xi_{p,1}^{i}\end{bmatrix},\] (58)

and let

\[B_{-1}^{i}=\begin{pmatrix}\xi_{2,1}^{i}&\xi_{3,1}^{i}&\dots&\xi_{p,1}^{i}\\ \xi_{2,2}^{i}&\xi_{2,3}^{i}&\dots&\xi_{2,p}^{i}\\ \xi_{3,2}^{i}&\xi_{3,3}^{i}&\dots&\xi_{3,p}^{i}\\ \dots&\dots&\dots&\dots\\ \xi_{p,2}^{i}&\xi_{p,3}^{i}&\dots&\xi_{p,p}^{i}\end{pmatrix}\] (59)

as the concatenation of those two matrices. The variance is calculated as

\[Var\left(\sqrt{n}(\hat{\beta}_{1,2}-\beta_{1,2})\right)=a^{[2]^{T}}\frac{1}{n} \sum_{i=1}^{n}vec(B_{-1}^{i})vec(B_{-1}^{i})^{T}a^{[2]},\] (60)

where

\[a_{l}^{[2]}=\begin{cases}\left(\hat{\boldsymbol{\Sigma}}_{-1-1}^{-1}\right)_ {[2],l},&\text{for }l\in\{1,\dots,p-1\}\\ \sum_{q=1}^{n}\left(\hat{\boldsymbol{\Sigma}}_{-1-1}^{-1}\right)_{[2],l}\left( \beta_{1}\right)_{q},&\text{for }l\in\{p,\dots,p^{2}-p\}\end{cases}\] (61)

\(vec(B_{-1}^{i})\) is the squeezed vector form of matrix \(vec(B_{-1}^{i})\in\mathbb{R}^{p\times p-1}\), i.e.,

\[vec(B_{-1}^{i})=\begin{pmatrix}\xi_{2,1}^{i}\\ \xi_{3,1}^{i}\\ \vdots\\ \xi_{p,p}^{i}\end{pmatrix}.\] (62)

Thus, the distribution of \(\hat{\beta}_{j,k}-\beta_{j,k}\) is

\[\hat{\beta}_{j,k}-\beta_{j,k}\sim N(0,a^{[k]^{T}}\frac{1}{n^{2}}\sum_{i=1}^{ n}vec(B_{-j}^{i})vec(B_{-j}^{i})^{T})a^{[k]}).\] (63)

In practice, we can plug in the estimates of \(\beta_{j}\) to estimate the interested distribution and do the CI test by hypothesizing \(\beta_{j,k}=0\).

### Discussion of assumption of zero mean and identity variance

In this section, we engage in a more thorough discussion regarding our assumptions about \(\bm{X}\). Specifically, we demonstrate that this assumption of mean and variance does not compromise the generality. In other words, the true model may possess different mean and variance values, but we proceed by treating it as having a mean of zero and identity variance.

The key ingredient allowing us to assume such a model is, the discretization function \(g_{j}\) is an unknown nonlinear monotonic function. Suppose the \(g_{j}^{\prime}\) maps the continuous domain to a binary variable, and we have the "groundtruth" variable, denoted \(X_{j}^{\prime}\), with mean \(a\) and variance \(b\). Assume the cardinality of the discretized domain is only 2, i.e., our observation \(\tilde{X}_{j}\) can only be 0 or 1. We further have the constant \(d_{j}^{\prime}\) as the discretization boundary such that we have the observation

\[\tilde{X}_{j}=\mathbbm{1}(g_{j}^{\prime}(X_{j}^{\prime})>d_{j}^{\prime})= \mathbbm{1}(X_{j}^{\prime}>g_{j}^{\prime-1}(d_{j}))\]

We can always produce our assumed variable \(X_{j}\) with mean 0 and variance 1, such that \(X_{j}=\frac{1}{\sqrt{b}}X_{j}^{\prime}-\frac{a}{\sqrt{b}}\) and the same observation with a different nonlinear transformation \(g_{j}\) and decision boundary \(d_{j}\), such that

\[\tilde{X}_{j}=\mathbbm{1}(g_{j}(X_{j})>d_{j})=\mathbbm{1}(X_{j}>g_{j}^{-1}(d_ {j}))=\mathbbm{1}(X_{j}^{\prime}>\sqrt{b}g_{j}^{-1}(d_{j})+a)\]

As long as the observation \(\tilde{X}_{j}\) is the same, we should have \(\sqrt{b}g_{j}^{-1}(d_{j})+a=g_{j}^{\prime-1}(d_{j})\). Our assumed model \(X_{j}\) clearly mimics the "groundtruth" \(X_{j}^{\prime}\). Besides, according to Lem. A.2, we have one-to-one mapping between \(\hat{\tau}_{j_{1}j_{2}}\) with the estimated covariance for fixed \(\hat{h}_{j_{1}},\hat{h}_{j_{2}}\). Thus, as long as the observation is the same, the estimation of covariance \(\hat{\sigma}_{j_{1},j_{2}}\) remains unaffected by our assumptions regarding the mean and variance of \(\bm{X}\), so do the following inference.

We further conduct casual discovery experiments to empirically validate our statement, which is shown in App. C.3.

Data Generation and Figure of main experiments: causal discovery

Data Generation and CodeWe construct the true DAG \(\mathcal{G}\) using the Bipartite Pairing (BP) model [2], with the number of edges being one fewer than the number of nodes. The subsequent generation of true multivariate Gaussian data involves assigning causal weights drawn from a uniform distribution \(U\sim(0.5,2)\) and incorporating noise via samples from a standard normal distribution for each variable. Following this, we binarize the data, setting the threshold randomly based on each variable's range. The code implementation is based on [40].

Figure 4: Experiment result of DAG discovery on synthetic data for changing sample size (a) and changing number of nodes (b). Fisherz_nodis is the Fisher-z test applied to original continuous data. We evaluate \(F_{1}\) (\(\uparrow\)), Precision (\(\uparrow\)), Recall (\(\uparrow\)) and SHD (\(\downarrow\)).

Additional experiments

### Linear non-Gaussian and nonlinear

Our model requires that the original data must adhere to the hypothesis of following a multivariate normal distribution, which appears to potentially limit the generalizability. Therefore, it is worthwhile to explore its robustness when such assumptions are violated. In this regard, we conducted several experiments, including scenarios involving linear non-Gaussian and nonlinear Gaussian.

For both cases, we follow the setting of our experiment where there are \(p=8\) nodes and \(p-1\) edges. We explore the effect of changing sample size \(n=(100,500,2000,5000)\). Specifically for linear non-Gaussian case, we adhere to some of the settings outlined by [28], conducting experiments where the original continuous data followed: (1) a Student's t-distribution with 3 degrees of freedom, (2) a uniform distribution, and (3) an exponential distribution. Each variable is generated as \(X_{i}=f(PA_{i})+noise\), where \(noise\) follows the distribution in (1), (2), (3) correspondingly and \(f\) is a linear function. The first three rows of Fig. 5 and Fig. 6 show the result of the linear non-Gaussian case.

For the nonlinear cases, we follow setting in [19], where every variable \(X_{i}\) is generated as \(X_{i}=f(WPA_{i}+noise)\), \(noise\sim N(0,1)\) and \(f\) is a function randomly chosen from (a) \(f(x)=sin(x)\), (b) \(f(x)=x^{3}\), (c) \(f(x)=tanh(x)\), and (d) \(f(x)=ReLU(x)\). \(W\) is a linear function. Similarly, we set the number of nodes at \(p=8\) and change the number of samples \(n=(500,2000,5000)\). For both cases, we run 10 graph instances with different seeds and report the result of skeleton discovery in Fig. 5 and DAG in Fig. 6 (The same orientation rules [11] used in the main experiment are employed to convert a CPDAG [6] into a DAG). The last row of Fig. 5 and Fig. 6 shows the result of the nonlinear case.

Based on the experimental outcomes, DCT demonstrates marginally superior or comparable efficacy in terms of the F1-score, precision, and SHD relative to both the Fisher-Z test and the Chi-square test when dealing with small sample sizes. Nevertheless, as the sample size increases, DCT's performance clearly surpasses that of the aforementioned tests across all three evaluated metrics, especially in the linear case. Consistent with observations from the main experiment, DCT exhibits a lower recall in comparison to the baseline tests. This discrepancy can be attributed to the baseline tests being prone to incorrectly infer conditional dependence and connect a large proportion of nodes. According to the results, our test shows notable robustness under the case assumptions are violated, confirming its practical effectiveness.

### Denser graph

DCT primarily works on cases where CI is mistakenly judged as conditional dependence due to discretization. Consequently, its efficacy is more pronounced in scenarios characterized by a relatively sparse graph, as numerous instances are truly conditionally independent. Nevertheless, the investigation of causal discovery with a dense latent graph is essential for evaluating the power of a test, i.e., its ability to successfully reject the null hypothesis when the tested pairs are conditionally dependent. Thus, we conduct the experiment where \(p=8,n=10000\) and changing edges \((p+2,p+4,p+6)\). Similarly, the latent continuous data follows a multivariate Gaussian model and the true DAG \(\mathcal{G}\) is constructed using BP model. We run 10 graph instances with different seeds and report the result of the skeleton discovery and DAG in Fig. 7.

According to the experiment results, DCT exhibits better performance in terms of the F1-score, precision, and SHD relative to both the Fisher-Z test and the Chi-square test. As the graph becomes progressively denser, the superiority of the Discrete Causality Test (DCT) correspondingly diminishes as there are few conditional independent cases in the true DAG. Due to the same reason, The recall remains lower than that of other baseline methods.

### multivariate Gaussian with nonzero mean and non-unit variance

We employed a setting nearly identical to the main experiment, with the only difference being the alteration in data generation: instead of using a standard normal distribution, we used a Gaussian distribution with mean sampled from \(U(-2,2)\) and variance sampled from \(U(0,3)\). We fix the number of variables as \(p=8\) and change the number of samples \(n=(100,500,2000,5000)\). The Fig. 8 shows the result and demonstrates the effectiveness of our method.

### Real-world dataset

To further validate DCT, we employ it on a real-world dataset: Big Five Personality https://openpsychometrics.org/, which includes 50 personality indicators and over 19000 data samples. Each variable contains 5 possible discrete values to represent the scale of the corresponding questions, where 1=Disagree, 2=Weakly disagree, 3=Neutral, 4=Weakly agree and 5=Agree, e.g., "N3=1" means "I agree that I worry about things". This scenario clearly suits DCT, where the degree of agreement with a certain question must be a continuous variable while we can only observe the result after categorization. We choose three variables respectively: [N3: I worry about things], [N10: I often feel blue ], [N4: I seldom feel blue]. We then do the casual discovery using PC algorithm with DCT and compare it with the Chi-square test and Fisher-Z test. The result can be found in Fig. 9.

Based on the experimental outcomes, despite the absence of a groundtruth for reference, we observe that the results obtained via DCT appear more plausible than those derived from Fisher-Z and Chi-square tests. Specifically, DCT suggests the relationship \(N_{3}\perp\!\!\!\perp N4|N10\), which is reasonable as intuitively, the answer of 'I often feel blue' already captures the information of 'I seldom feel blue'.

Figure 5: Experiment result of causal discovery on synthetic data with \(p=8\), \(n=(100,500,2000,5000)\) where the data generation process violates our assumptions. The data are generated with either nongaussian distributed (a), (b), (c) or the relations are not linear (d). The figure reports \(F_{1}\) (\(\uparrow\)), Precision (\(\uparrow\)), Recall (\(\uparrow\)) and SHD (\(\downarrow\)) on skeleton.

As a comparison, both Fisher-Z and Chi-square return a fully connected graph. The results directly correspond to our illustrative example shown in Fig. 1, substantiating the necessity of our proposed test.

Figure 6: Experiment result of causal discovery on synthetic data with \(p=8\), \(n=(100,500,2000,5000)\) where the data generation process violates our assumptions. The data are generated with either nongaussian distributed (a), (b), (c) or the relations are not linear (d). The figure reports \(F_{1}\) (\(\uparrow\)), Precision (\(\uparrow\)), Recall (\(\uparrow\)) and SHD (\(\downarrow\)) on DAG.

## Appendix D Related Work

Testing for CI is pivotal in the field of causal discovery [30], and a variety of methods exist for performing CI tests (CI tests). An important group of CI test methods involves the assumption of Gaussian variables with linear dependencies. For example, under this assumption, Gaussian graphical models are extensively studied [37; 25; 22; 26]. To address CI test under Gaussian assumption, partial correlation serves as a viable method for CI testing [4]. To evaluate the independence of variables \(X_{1}\) and \(X_{2}\) conditional on \(\bm{Z}\), The technique proposed by [32] determines CI by comparing the estimations of \(p(X_{1}|X_{2},\bm{Z})\) and \(p(X_{1}|X_{2})\).

Figure 8: Experimental comparison of causal discovery on synthetic datasets for multivariate Gaussian model with \(p=8,n=(100,500,2000,5000)\) and where mean is not zero. We evaluate \(F_{1}\) (\(\uparrow\)), Precision (\(\uparrow\)), Recall (\(\uparrow\)) and SHD (\(\downarrow\)) on both skeleton and DAG.

Figure 7: Experimental comparison of causal discovery on synthetic datasets for denser graphs with \(p=8,n=10000\) and edges varying \(p+2,p+4,p+6\). We evaluate \(F_{1}\) (\(\uparrow\)), Precision (\(\uparrow\)), Recall (\(\uparrow\)) and SHD (\(\downarrow\)) on both skeleton and DAG.

Another approach involves discretizing \(\bm{Z}\) and performing independent tests within each resulting bin [21]. Our work, however, diverges from these existing methods in two significant ways. Firstly, we are equipped to handle data, where partial variables are discretized. Additionally, we postulate that discrete variables are derived from the transformation of continuous variables in a latent Gaussian model. With the same assumption, the most closely related study is by [13], where the authors developed a novel rank-based estimator for the precision matrix of mixed data. However, their work stops short of providing a CI test for this method. Our research fills this gap, offering the ability to estimate the precision matrix for both discrete and mixed data and providing a rigorous CI test for our methodology.

Recent advancements in CI testing have utilized kernel methods for continuous variables influenced by nonlinear relationships. [16] describes non-parametric CI relationships using covariance operators in reproducing kernel Hilbert spaces (RKHS). KCI test [38] assesses the partial associations of regression functions linking \(x\), \(y\), and \(z\), while RCI test [31] aims to enhance the KCI test's efficiency. In KCIP test [12] employs permutations of samples to emulate CI scenarios. CCI test [27] further reformulates testing into a process that leverages the capabilities of supervised learning models. For discrete variable analysis, the \(G^{2}\) test [1] and conditional mutual information [39] are commonly employed. However, their method cannot deal with our setting where only discretized version of latent variables can be observed.

## Appendix E Resource Usage

All the experiments are run using Intel(R) Xeon(R) CPU E5-2680 v4 with 55 processors. It costs 4 hours to run experiments in Section 3.1.

## Appendix F Limiation and Broader Impacts

LimitationSo far, the largest limitation of our method is to treat discretized variables as binary, which wastes the available information. Besides that, the parametric assumption limits its generalizability. However, we need to point out this is pretty normal in CI test fields.

Broader ImpactsThe goal of our proposed method is to test the conditional independence relationship given discretized observation. This task is essential and has broad applications. We are confident that our method will be beneficial and will not result in negative societal impacts.

Figure 9: Experimental comparison of causal discovery on the real-world dataset.

* [718]**NeurIPS Paper Checklist**

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: Section1 Introduction and Abstract Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: Section2.1 line145-line147, Appendix F Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes]Justification: Assumption: Section2 line81 to line 94, Proof: Appendix A. Guidelines:

* The answer NA means that the paper does not include theoretical results.
* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: Section3 and Appendix B,C. Guidelines: The answer NA means that the paper does not include experiments.
* If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.
* If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.
* Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.
* While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?Answer: [Yes] Justification: We provide the full code in our supplementary. Guidelines:

* The answer NA means that paper does not include experiments requiring code.
* Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
* The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: Section3 and Appendix B, C. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: Section 3 and Appendix B, C. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors). * It should be clear whether the error bar is the standard deviation or the standard error of the mean.

* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: Appendix E. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We completely follow NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: We propose a new conditional independence test with applications range in multiple fields. Please refer to Appendix F. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.

* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: Method proposed in this paper don't pose such risks. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licensees for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We have cited the dataset we use and we provide the code we based in Appendix B. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.

* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: We have submitted the code. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: We don't use any crowdsourcing resource. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: NA. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.