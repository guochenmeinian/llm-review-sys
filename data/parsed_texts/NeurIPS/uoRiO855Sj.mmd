# Minimax Forward and Backward Learning of Evolving Tasks with Performance Guarantees

 Veronica Alvarez\({}^{1}\) Santiago Mazuelas\({}^{1,2}\) Jose A. Lozano\({}^{1,3}\)

\({}^{1}\)Basque Center of Applied Mathematics (BCAM)

\({}^{2}\)IKERBASQUE-Basque Foundation for Science \({}^{3}\)University of the Basque Country UPV/EHU

{valvarez, smazuelas, jlozano}@bcamath.org

###### Abstract

For a sequence of classification tasks that arrive over time, it is common that tasks are evolving in the sense that consecutive tasks often have a higher similarity. The incremental learning of a growing sequence of tasks holds promise to enable accurate classification even with few samples per task by leveraging information from all the tasks in the sequence (forward and backward learning). However, existing techniques developed for continual learning and concept drift adaptation are either designed for tasks with time-independent similarities or only aim to learn the last task in the sequence. This paper presents incremental minimax risk classifiers (IMRCs) that effectively exploit forward and backward learning and account for evolving tasks. In addition, we analytically characterize the performance improvement provided by forward and backward learning in terms of the tasks' expected quadratic change and the number of tasks. The experimental evaluation shows that IMRCs can result in a significant performance improvement, especially for reduced sample sizes.

## 1 Introduction

In practical scenarios, it is often of interest to incrementally learn a growing sequence of classification problems (tasks) that arrive over time. In such a sequence, it is common that tasks are evolving in the sense that consecutive tasks often have a higher similarity. Examples of evolving tasks are the classification of portraits from different time periods [1] and the classification of spam emails over time [2]; in these problems, the similarity between consecutive tasks (portraits of consecutive time periods and emails from consecutive years) is significantly higher (see Figure 1). The incremental learning of a growing sequence of tasks holds promise to significantly improve performance by leveraging information from different tasks. Specifically, at each time step, information from preceding tasks can be used to improve the performance of the last task (forward learning) and, reciprocally, the information from the last task can be used to improve the performance of the preceding tasks (backward learning) [3, 4, 5]. Such transfer of information can enable accurate classification even in cases with reduced sample sizes, thus significantly increasing the effective sample size (ESS) of each task. However, exploiting the benefits of forward and backward learning is challenging due to the continuous arrival of samples from tasks characterized by different underlying distributions [6, 7, 8].

Techniques developed for concept drift adaptation (aka learning in a dynamic scenario) [9, 10, 11, 12, 13, 14, 15, 16] are designed for evolving tasks but only aim to learn the last task in the sequence. In particular, methods based on learning rates learn the last task by slightly updating the classification rule for the preceding task [12, 13]; and methods based on sliding windows learn the last task by using a set of stored samples from the most recent preceding tasks [14, 15]. In concept drift adaptation, at each timestep only the last task is considered of interest, and is learned leveraging information from the most recent preceding tasks since they are the most similar to the last task.

Techniques developed for continual learning (aka lifelong learning) [4; 5; 6; 7; 17; 18; 19; 20] aim to learn the whole sequence of tasks but existing methods are designed for situations where tasks' similarities do not depend on the time steps when tasks are observed. In particular, methods based on dynamic architectures learn shared parameters using samples from all the tasks together with task-specific parameters using samples from the corresponding task [6; 20]; and methods based on replay learn parameters using a pool of stored samples from all the preceding tasks together with the samples from the last task [5; 7]. Existing methods for continual learning are not designed for evolving tasks and consider scenarios in which the order of the tasks in the sequence is not relevant. In the current literature of continual learning, only [21] considers scenarios with evolving tasks but focus on the theoretical analysis of transferring information from the preceding tasks.

This paper presents incremental minimax risk classifiers (IMRCs) that determine classification rules minimizing worst-case error probabilities over uncertainty sets that can contain the sequence of evolving underlying distributions. The proposed techniques can effectively exploit forward and backward learning by obtaining uncertainty sets that get smaller using information from all the tasks (see Figure 1). Specifically, the main contributions presented in the paper are as follows.

* We propose forward learning techniques that recursively use the information from preceding tasks to reduce the uncertainty set of the last task.
* We propose forward and backward learning techniques that use the information from the last task to further reduce the sequence of uncertainty sets obtained with forward learning.
* We analytically characterize the increase in ESS provided by the presented methods in terms of the expected quadratic change between consecutive tasks and the number of tasks.
* We numerically quantify the performance improvement provided by IMRCs in comparison with existing techniques using multiple datasets, sample sizes, and number of tasks.

NotationsCalligraphic letters represent sets; \(\|\cdot\|_{1}\) and \(\|\cdot\|_{\infty}\) denote the \(1\)-norm and the infinity norm of its argument, respectively; \(\preceq\) and \(\succeq\) denote vector inequalities; \(\mathbb{I}\{\cdot\}\) denotes the indicator function; and \(\mathbb{E}_{\mathrm{p}}\{\,\cdot\,\}\) and \(\mathbb{V}\mathrm{ar}_{\mathrm{p}}\{\cdot\}\) denote the expectation and the variance of its argument with respect to distribution \(\mathrm{p}\). For a vector \(\mathbf{v}\), \(v^{(i)}\) and \(\mathbf{v}^{\mathrm{T}}\) denote its i-th component and transpose. Non-linear operators acting on vectors denote component-wise operations. For instance, \(|\mathbf{v}|\) and \(\mathbf{v}^{2}\) denote the vector formed by the absolute value and the square of each component, respectively. For the reader's convenience, we also provide in Table 2 in Appendix A a list with the main notions used in the paper and their corresponding notations.

Figure 1: Tasks that arrive over time are characterized by different underlying distributions and consecutive tasks often have a higher similarity. The black line represents the evolution of the underlying distributions that characterize different tasks. IMRCs minimize the worst-case error probability over uncertainty sets \(\mathcal{U}_{i}\) that can include the underlying distribution \(\mathrm{p}_{i}\). A single task uncertainty set (blue hexagons) can be obtained leveraging information only from the corresponding task, while a forward uncertainty set (green hexagons) can be obtained leveraging information from preceding tasks. Then, the proposed methodology obtains forward and backward uncertainty sets (red hexagons) leveraging information from all the tasks in the sequence.

Preliminaries

This section first formulates the problem of incrementally learning a growing sequence of tasks, and describes evolving tasks in comparison with the time-independent assumption common in continual learning. Then, we briefly describe MRCs that determine classification rules by minimizing the worst-case error probability over an uncertainty set.

### Problem formulation

In the following, we denote by \(\mathcal{X}\) the set of instances or attributes, \(\mathcal{Y}\) the set of labels or classes, \(\Delta(\mathcal{X}\times\mathcal{Y})\) the set of probability distributions over \(\mathcal{X}\times\mathcal{Y}\), and \(\text{T}(\mathcal{X},\mathcal{Y})\) the set of classification rules. A classification task is characterized by an underlying distribution \(\mathrm{p}^{*}\in\Delta(\mathcal{X}\times\mathcal{Y})\), and standard supervised classification methods use a sample set \(D=\{(x_{i},y_{i})\}_{i=1}^{n}\) formed by \(n\) i.i.d samples from distribution \(\mathrm{p}^{*}\) to find a classification rule \(\mathrm{h}\in\text{T}(\mathcal{X},\mathcal{Y})\) with small expected loss \(\ell(\mathrm{h},\mathrm{p}^{*})\).

In the addressed settings, sample sets \(D_{1},D_{2},\ldots\) arrive over time steps \(1,2,\ldots\) corresponding with different classification tasks characterized by underlying distributions \(\mathrm{p}_{1},\mathrm{p}_{2},\ldots\). Incremental learning aims to continually learn over time the growing sequence of tasks exploiting information acquired from all the tasks. At each time step \(k\), learning methods obtain classification rules \(\mathrm{h}_{1},\mathrm{h}_{2},\ldots,\mathrm{h}_{k}\) for the current sequence of \(k\) tasks using the new sample set \(D_{k}\) and certain information retained from step \(k-1\). The performance of learning methods is assessed at each time step \(k\) by quantifying the performance in the current \(k\) tasks. For instance, overall performance can be assessed by the averaged error \(\frac{1}{k}\sum_{j=1}^{k}\ell(\mathrm{h}_{j},\mathrm{p}_{j})\) with \(\ell(\mathrm{h}_{j},\mathrm{p}_{j})\) the expected loss of the classification rule \(\mathrm{h}_{j}\) for distribution \(\mathrm{p}_{j}\).

Evolving tasks are considered by methods for concept drift adaptation, but existing continual learning methods are designed for scenarios where tasks' similarities do not depend on the time steps when tasks are observed. These scenarios are usually mathematically modeled assuming that the tasks' distributions \(\mathrm{p}_{1},\mathrm{p}_{2},\ldots\) are independent and identically distributed (i.i.d.) [18; 19]. In this paper, we develop techniques designed for evolving tasks that can be mathematically modeled assuming that the changes between consecutive distributions \(\mathrm{p}_{2}-\mathrm{p}_{1},\mathrm{p}_{3}-\mathrm{p}_{2},\ldots\) are independent and zero-mean (evolving task assumption). The assumption in this paper is not stronger than the usual i.i.d. assumption and can better describe evolving tasks; their main difference is considering independent changes between consecutive distributions instead of independent distributions. Note that with independent distributions the difference between the distributions of the \(i\)-th and the \((i+t)\)-th tasks has zero mean and variance \(\forall\text{ar}\{\mathrm{p}_{i+t}-\mathrm{p}_{i}\}=\mathbb{V}\text{ar}\{ \mathrm{p}_{i+1}-\mathrm{p}_{i}\}=2\mathbb{V}\text{ar}\{\mathrm{p}_{1}\}\) that does not depend on \(t\). On the other hand, with independent changes between consecutive distributions, the difference between the distributions of the \(i\)-th and the \((i+t)\)-th tasks has zero mean and variance \(\forall\text{ar}\{\mathrm{p}_{i+t}-\mathrm{p}_{i}\}=\sum_{j=1}^{t}\forall\text {ar}\{\mathrm{p}_{i+j}-\mathrm{p}_{i+j-1}\}\) that increases with \(t\). Appendix H further describes how the assumption used in the paper is more appropriate for evolving tasks than the conventional i.i.d. assumption using real datasets.

### Minimax risk classifiers

The methods presented in the paper are based on robust risk minimization [22; 23] instead of empirical risk minimization since training samples corresponding to different tasks follow different distributions. In particular, we utilize MRCs [24; 25] that learn classification rules by minimizing the worst-case expected loss against an uncertainty set given by constraints on the expectation of a feature mapping \(\Phi:\mathcal{X}\times\mathcal{Y}\to\mathbb{R}^{m}\) as

\[\mathcal{U}=\{\mathrm{p}\in\Delta(\mathcal{X}\times\mathcal{Y})\::\:|\mathbb{ E}_{\mathrm{p}}\{\Phi(x,y)\}-\boldsymbol{\tau}|\preceq\boldsymbol{\lambda}\}\] (1)

where \(\boldsymbol{\tau}\) denotes a mean vector of expectation estimates and \(\boldsymbol{\lambda}\) denotes a confidence vector. Feature mappings are vector-valued functions over \(\mathcal{X}\times\mathcal{Y}\), e.g., one-hot encodings of labels [26] with instances represented by values from the last layers in a neural network [8; 27] or by random Fourier features (RFF) [28; 29].

Given the uncertainty set \(\mathcal{U}\), MRC rules are solutions of the optimization problem

\[R(\mathcal{U})=\min_{\mathrm{h}\in\text{T}(\mathcal{X},\mathcal{Y})}\:\max_{ \mathrm{p}\in\mathcal{U}}\:\ell(\mathrm{h},\mathrm{p})\] (2)where \(R(\mathcal{U})\) denotes the minimax risk and \(\ell(\mathrm{h},\mathrm{p})\) denotes the expected loss of classification rule \(\mathrm{h}\) for distribution \(\mathrm{p}\). In the following, we utilize the 0-1-loss so that \(\ell(\mathrm{h},\mathrm{p})=\mathbb{E}_{\mathrm{p}}\{\mathbb{I}\{\mathrm{h}(x) \neq y\}\}\) and the expected loss with respect to the underlying distribution becomes the error probability of the classification rule. Deterministic MRCs assign each instance \(x\in\mathcal{X}\) with the label \(\mathrm{h}(x)\in\arg\max_{y\in\mathcal{Y}}\Phi(x,y)^{\mathrm{T}}\boldsymbol{ \mu}^{\star}\) where the parameter \(\boldsymbol{\mu}^{\star}\) is the solution of the convex optimization problem

\[\min_{\boldsymbol{\mu}}\,1-\boldsymbol{\tau}^{\mathrm{T}}\boldsymbol{\mu}+\max_ {x\in\mathcal{X},\mathcal{C}\subseteq\mathcal{Y}}\frac{\sum_{y\in\mathcal{C} }\Phi(x,y)^{\mathrm{T}}\boldsymbol{\mu}-1}{|\mathcal{C}|}+\boldsymbol{\lambda }^{\mathrm{T}}|\boldsymbol{\mu}|\] (3)

given by the Lagrange dual of (2) [24; 25].

The baseline approach of single-task learning obtains a classification rule \(\mathrm{h}_{j}\) for each \(j\)-th task leveraging information only from the sample set \(D_{j}=\{(x_{j,i},y_{j,i})\}_{i=1}^{n_{j}}\) given by \(n_{j}\) samples from distribution \(\mathrm{p}_{j}\). In that case, IMRCs coincide with MRCs for standard supervised classification that obtain the mean and confidence vectors as

\[\boldsymbol{\tau}_{j}=\frac{1}{n_{j}}\sum_{i=1}^{n_{j}}\Phi\left(x_{j,i},y_{j, i}\right),\quad\boldsymbol{\lambda}_{j}=\sqrt{\boldsymbol{s}_{j}},\quad \boldsymbol{s}_{j}=\frac{\boldsymbol{\sigma}_{j}^{2}}{n_{j}}\] (4)

with \(\boldsymbol{\sigma}_{j}^{2}\) an estimate of \(\mathbb{V}\mathrm{ar}_{\mathrm{p}_{j}}\{\Phi(x,y)\}\), e.g., the sample variance of the \(n_{j}\) samples. The vector \(\boldsymbol{s}_{j}\) describes the mean squared errors (MSEs) of the mean vector components and directly gives the confidence vector \(\boldsymbol{\lambda}_{j}\) as shown in (4).

MRCs provide bounds for the minimax risk \(R(\mathcal{U}_{j})\) in terms of the smallest minimax risk as described in [24; 25; 30]. The smallest minimax risk is that corresponding to the ideal case of knowing mean vectors exactly, that is, the minimax risk corresponding with the uncertainty set \(\mathcal{U}_{j}^{\infty}=\{\mathrm{p}\in\Delta(\mathcal{X}\;\times\;\mathcal{ Y}):\mathbb{E}_{\mathrm{p}}\{\Phi(x,y)\}=\boldsymbol{\tau}_{j}^{\infty}\}\) given by the expectation \(\boldsymbol{\tau}_{j}^{\infty}=\mathbb{E}_{\mathrm{p}_{j}}\{\Phi(x,y)\}\). In the baseline approach of single-task learning, if \(R_{j}^{\infty}\) and \(\boldsymbol{\mu}_{j}^{\infty}\) denote the smallest minimax risk and the MRC parameter corresponding with \(\mathcal{U}_{j}^{\infty}\), with probability at least \(1-\delta\) we have that

\[R(\mathcal{U}_{j})\leq R_{j}^{\infty}+\frac{M(\kappa+1)\sqrt{2\log(2m/\delta)} }{\sqrt{n_{j}}}\|\boldsymbol{\mu}_{j}^{\infty}\|_{1}\] (5)

where \(M\) and \(\kappa\) are such that \(\|\Phi(x,y)\|_{\infty}\leq M\) for any \((x,y)\in\mathcal{X}\times\mathcal{Y}\) and \(subG(\Phi_{j})\preceq\kappa\boldsymbol{\sigma}_{j}\), \(subG(\cdot)\) denotes the sub-Gaussian parameter of the argument components, and \(\Phi_{j}\) denotes the random variable given by the feature mapping of samples from the \(j\)-th task. Inequality (5) is obtained using the bounds in [25] together with the Chernoff bound [31] for sub-Gaussian variables.

In the following sections, we describe techniques that obtain the mean and MSE vectors using forward and backward learning. Once such vectors are obtained, IMRC methods obtain the classifier parameter \(\boldsymbol{\mu}_{j}\) for each \(j\)-th task solving the convex optimization problem in (3) that can be efficiently addressed using conventional methods [32; 33].

## 3 Forward learning with performance guarantees

This section presents the recursions that allow to obtain mean and MSE vectors for each task retaining information from preceding tasks. In addition, it characterizes the increase in ESS provided by forward learning in terms of the tasks' expected quadratic change and the number of tasks.

### Forward learning

Let \(\boldsymbol{\tau}_{j}^{\rightarrow}\) and \(\boldsymbol{s}_{j}^{\rightarrow}\) denote the mean and MSE vectors for forward learning corresponding to the \(j\)-th task. The following recursions allow to obtain \(\boldsymbol{\tau}_{j}^{\rightarrow}\) and \(\boldsymbol{s}_{j}^{\rightarrow}\) for each \(j\)-th task using the vectors for the preceding task \(\boldsymbol{\tau}_{j-1}^{\rightarrow},\boldsymbol{s}_{j-1}^{\rightarrow}\) as

\[\boldsymbol{\tau}_{j}^{\rightarrow}=\boldsymbol{\tau}_{j}+\frac{\boldsymbol{s} _{j}}{\boldsymbol{s}_{j-1}^{\rightarrow}+\boldsymbol{s}_{j}+\boldsymbol{d} _{j}^{2}}\left(\boldsymbol{\tau}_{j-1}^{\rightarrow}-\boldsymbol{\tau}_{j} \right),\quad\boldsymbol{s}_{j}^{\rightarrow}=\left(\frac{1}{\boldsymbol{s}_{j }}+\frac{1}{\boldsymbol{s}_{j-1}^{\rightarrow}+\boldsymbol{d}_{j}^{2}}\right)^ {-1}\] (6)

with \(\boldsymbol{\tau}_{j}\) and \(\boldsymbol{s}_{j}\) given by (4) and \(\boldsymbol{\tau}_{1}^{\rightarrow}=\boldsymbol{\tau}_{1}\) and \(\boldsymbol{s}_{1}^{\rightarrow}=\boldsymbol{s}_{1}\).

The vector \(\bm{d}_{i}^{2}\) assesses the expected quadratic change between consecutive tasks described by \(\bm{w}_{i}~{}=~{}\bm{\tau}_{i}^{\infty}-\bm{\tau}_{i-1}^{\infty}\). Taking \(\bm{d}_{i}^{2}=\mathbb{E}\{\bm{w}_{i}^{2}\}\) and \(\bm{\sigma}_{i}^{2}=\mathbb{V}\bm{\pi}_{\text{\tiny{P}}_{i}}\{\Phi\left(x,y \right)\}\) for any \(i\), the first recursion in (6) provides the unbiased linear estimator of the mean vector \(\bm{\tau}_{j}^{\infty}\) based on \(D_{1},D_{2},\ldots,D_{j}\) that has the minimum MSE, while the second recursion in (6) provides its MSE (see Appendix B for a detailed derivation). Vectors \(\bm{\sigma}_{i}^{2}\) and \(\bm{d}_{i}^{2}\) can be estimated online using the sample sets. In particular, \(\bm{\sigma}_{i}^{2}\) can be estimated as the sample variance, while \(\bm{d}_{i}^{2}\) can be estimated using sample averages as

\[\bm{d}_{i}^{2}=\sum_{l=1}^{W}\frac{\left(\bm{\tau}_{i_{l}}-\bm{\tau}_{i_{l-1} }\right)^{2}}{W}\text{ where }i_{0},i_{1},\ldots,i_{W}\text{ are the closest indexes to }i\in\{1,2,\ldots,k\}.\] (7)

Recursions in (6) obtain mean and MSE vectors for the \(j\)-th task by using information from preceding tasks and the \(j\)-th sample set \(D_{j}\). Specifically, the first recursion in (6) obtains the mean vector \(\bm{\tau}_{j}^{\rightarrow}\) by adding a correction to the sample average \(\bm{\tau}_{j}\). This correction is proportional to the difference between \(\bm{\tau}_{j}\) and \(\bm{\tau}_{j-1}^{\rightarrow}\) with a proportionality constant that depends on the MSE vectors \(\bm{s}_{j},\bm{s}_{j-1}^{\rightarrow}\) and the expected quadratic change \(\bm{d}_{j}^{2}\). In particular, if \(\bm{s}_{j}\ll\bm{s}_{j-1}^{\rightarrow}+\bm{d}_{j}^{2}\), the mean vector is given by the sample average as in single-task learning, and if \(\bm{s}_{j}\gg\bm{s}_{j-1}^{\rightarrow}+\bm{d}_{j}^{2}\), the mean vector is given by that of the preceding task. Note that for forward learning, at each step \(k\), only the vectors for the last task \(\bm{\tau}_{k}^{\rightarrow}\) and \(\bm{s}_{k}^{\rightarrow}\) need to be obtained from those of the \((k-1)\)-th task, the vectors for the remaining tasks stay the same as at step \(k-1\) (see also Fig. 2 and Alg. 1 below).

### Performance guarantees and effective sample sizes with forward learning

The following result provides bounds for the minimax risk for each task using forward learning.

**Theorem 1**.: Let \(\mathcal{U}_{j}^{\rightarrow}\) be the uncertainty set given by (1) using the mean and confidence vectors \(\bm{\tau}_{j}^{\rightarrow}\) and \(\bm{\lambda}_{j}^{\rightarrow}=\sqrt{\bm{s}_{j}^{\rightarrow}}\) provided by (6), and let \(\kappa\) be such that \(subG(\Phi_{j})\preceq\kappa\bm{\sigma}_{j}\) and \(subG\left(\bm{w}_{j}\right)\preceq\kappa\bm{d}_{j}\) for \(j=1,2,\ldots,k\). Then, under the evolving task assumption of Section 2, we have with probability at least \(1-\delta\) that

\[R(\mathcal{U}_{j}^{\rightarrow})\leq R_{j}^{\infty}+\frac{M(\kappa+1)\sqrt{2 \log\left(2m/\delta\right)}}{\sqrt{n_{j}^{\rightarrow}}}\left\|\bm{\mu}_{j}^{ \infty}\right\|_{1}\text{ for any }j\in\{1,2,\ldots,k\}\] (8)

with \(n_{1}^{\rightarrow}=n_{1}\) and \(n_{j}^{\rightarrow}\geq n_{j}+n_{j-1}^{\rightarrow}\frac{\|\bm{\sigma}_{j}^{ \rightarrow}\|_{\infty}}{\|\bm{\sigma}_{j}^{\rightarrow}\|_{\infty}+n_{j-1}^ {\rightarrow}\|\bm{d}_{j}^{2}\|_{\infty}}\) for \(j\geq 2\).

Proof.: See Appendix C. 

The value \(n_{j}^{\rightarrow}\) in (8) is the ESS of the proposed IMRC method with forward learning since the bound in (8) coincides with that of single-task learning in (5) if the sample size for the \(j\)-th task is \(n_{j}^{\rightarrow}\). The ESS of each task is obtained by adding a fraction of the ESS for the preceding task to the sample size. In particular, if \(\bm{d}_{j}^{2}\) is large, the ESS is given by the sample size, while if \(\bm{d}_{j}^{2}\) is small, the ESS is given by the sum of the sample size and the ESS of the preceding task.

The bound in (8) shows that recursions in (6) do not need to use very accurate values for \(\bm{\sigma}_{j}\) and \(\bm{d}_{j}\). Specifically, the coefficient \(\kappa\) in (8) can be taken to be small as long as the values used for \(\bm{\sigma}_{j}\) and \(\bm{d}_{j}\) are not much lower than the sub-Gaussian parameters of \(\Phi_{j}\) and \(\bm{w}_{j}\), respectively. In particular, \(\kappa\) is smaller than the maximum of \(M/\min_{j,i}\{\sigma_{j}^{(i)}\}\) and \(2M/\min_{j,i}\{d_{j}^{(i)}\}\) due to the bound for the sub-Gaussian parameter of bounded random variables (see e.g., Section 2.1.2 in [31]).

Theorem 1 shows the increase in ESS in terms of the ESS of the preceding task. The following result allows to directly quantify the ESS in terms of the tasks' expected quadratic change and the number of tasks.

**Theorem 2**.: If \(\|\bm{d}_{j}^{2}\|_{\infty}\leq d^{2}\), \(M\leq 1\), and \(n_{j}\geq n\) for \(j=1,2,\ldots,k\), then for any \(j\in\{1,2,\ldots,k\}\), we have that the ESS in (8) can be taken so that it satisfies

\[n_{j}^{\rightarrow}\geq n\left(1+\frac{(1+\alpha)^{2j-1}-1-\alpha}{\alpha(1+ \alpha)^{2j-1}+\alpha}\right)\text{ with }\alpha=\frac{2}{\sqrt{1+\frac{4}{nd^{2}}}-1}.\] (9)

In particular, if \(nd^{2}<1/j^{2}\), we have that \(n_{j}^{\rightarrow}\geq n\left(1+\frac{i-1}{3}\right)\).

Proof.: See Appendix D.

The above theorem characterizes the increase in ESS provided by forward learning in terms of the tasks' expected quadratic change. Such increase grows monotonically with the number of preceding tasks \(j\) as shown in (9) and becomes proportional to \(j\) when the expected quadratic change is smaller than \(1/(j^{2}n)\). Figure 3 below further illustrates the increase in ESS with respect to the sample size \((n_{j}^{\rightarrow}/n)\) due forward learning in comparison with forward and backward learning.

## 4 Forward and backward learning with performance guarantees

This section presents the recursions that allow to obtain mean and MSE vectors for each task leveraging information from all the tasks. In addition, it characterizes the increase in ESS provided by forward and backward learning in terms of the tasks' expected quadratic change and the number of tasks.

Backward learning is more challenging than forward learning since the new task provides additional information for preceding tasks at each time step, while the information from preceding tasks is always the same. The techniques proposed below for backward learning effectively increase the ESS over time by carefully accounting for the new information at each step.

### Forward and backward learning

At each step \(k\), the proposed techniques learn to classify each \(j\)-th task leveraging information obtained from the \(j\) preceding tasks (tasks \(\{1,2,\ldots,j\}\)) and from the \(k-j\) succeeding tasks (tasks \(\{j+1,j+2,\ldots,k\}\)). From preceding tasks, we obtain the forward mean and MSE vectors \(\bm{\tau}_{j}^{\rightarrow},\bm{s}_{j}^{\rightarrow}\) using recursions in (6), while from succeeding tasks, we obtain the backward mean and MSE vectors \(\bm{\tau}_{j}^{\rightarrow},\bm{s}_{j}^{\rightarrow}\) using recursions in (6) in retrodiction. Specifically, vectors \(\bm{\tau}_{j}^{\rightarrow}\) and \(\bm{s}_{j}^{\rightarrow}\) are obtained using the same recursion as for \(\bm{\tau}_{j}^{\rightarrow}\) and \(\bm{s}_{j}^{\rightarrow}\) in (6) with \(\bm{s}_{j+1}^{\rightarrow k},\bm{d}_{j+1}^{2}\), and \(\bm{\tau}_{j+1}^{\rightarrow k}\) instead of \(\bm{s}_{j-1}^{\rightarrow},\bm{d}_{j}^{2}\), and \(\bm{\tau}_{j-1}^{\rightarrow}\).

Let \(\bm{\tau}_{j}^{\rightarrow k}\) and \(\bm{s}_{j}^{\rightarrow k}\) denote the mean and MSE vectors for forward and backward learning corresponding to the \(j\)-th task for \(j\in\{1,2,\ldots,k\}\). The following recursions allow to obtain the mean and MSE vectors \(\bm{\tau}_{j}^{\rightarrow}\) and \(\bm{s}_{j}^{\rightarrow k}\) for each \(j\)-th task using those vectors for forward learning \(\bm{\tau}_{j}^{\rightarrow},\bm{s}_{j}^{\rightarrow}\) and backward learning \(\bm{\tau}_{j+1}^{\rightarrow k},\bm{s}_{j+1}^{\rightarrow k}\) as

\[\bm{\tau}_{j}^{\rightarrow k}=\bm{\tau}_{j}^{\rightarrow}+\frac{\bm{s}_{j}^ {\rightarrow}}{\bm{s}_{j}^{\rightarrow}+\bm{s}_{j+1}^{\rightarrow k}+\bm{d} _{j+1}^{2}}\left(\bm{\tau}_{j+1}^{\rightarrow k}-\bm{\tau}_{j}^{\rightarrow} \right),\quad\bm{s}_{j}^{\rightarrow k}=\left(\frac{1}{\bm{s}_{j}^{ \rightarrow}}+\frac{1}{\bm{s}_{j+1}^{\rightarrow k}+\bm{d}_{j+1}^{2}}\right)^ {-1}\] (10)

with \(\bm{\tau}_{k}^{\rightarrow}=\bm{\tau}_{k},\bm{s}_{k}^{\rightarrow k}=\bm{s}_ {k}\) and \(\bm{\tau}_{k}^{\rightarrow k}=\bm{\tau}_{k}^{\rightarrow},\bm{s}_{k}^{ \rightarrow k}=\bm{s}_{k}^{\rightarrow}\). Analogously to the case of forward learning in Section 3.1, taking \(\bm{d}_{i}^{2}=\mathbb{E}\{\bm{w}_{i}^{2}\}\) and \(\bm{\sigma}_{i}^{2}=\mathbb{V}\text{ar}_{\text{p}_{i}}\{\Phi\left(x,y\right)\}\) for any \(i\), the first recursion in (10) provides the unbiased linear estimator of the mean vector \(\bm{\tau}_{j}^{\rightarrow}\) based on \(D_{1},D_{2},\ldots,D_{j}\) and \(D_{j+1},D_{j+2},\ldots,D_{k}\) that has the minimum MSE, while the second recursion in (10) provides its MSE (see Appendix B for a detailed derivation).

Recursions in (10) obtain at step \(k\) the mean and MSE vectors for the \(j\)-th task by retaining information from preceding tasks and acquiring information from the new task. Specifically, the first recursion in (10) obtains the mean vector \(\bm{\tau}_{j}^{\rightarrow k}\) by adding a correction to the mean vector of the corresponding task \(\bm{\tau}_{j}^{\rightarrow}\) obtained for forward learning. This correction is proportional to the difference between \(\bm{\tau}_{j}^{\rightarrow}\) and \(\bm{\tau}_{j+1}^{\rightarrow k}\) with a proportionality constant that depends on the MSE vectors \(\bm{s}_{j}^{\rightarrow},\bm{s}_{j+1}^{\rightarrow k}\) and the expected quadratic change \(\bm{d}_{j+1}^{2}\). In particular, if \(\bm{s}_{j}^{\rightarrow}\ll\bm{s}_{j+1}^{\rightarrow k}+\bm{d}_{j+1}^{2}\), the mean vector is given by that of the corresponding task for forward learning, and if \(\bm{s}_{j}^{\rightarrow}\gg\bm{s}_{j+1}^{\rightarrow k}+\bm{d}_{j+1}^{2}\), the mean vector is given by that of the next task for backward learning.

### Implementation

This section describes the implementation of the proposed IMRCs with forward and backward learning and its computational and memory complexities.

Figure 2 depicts the flow diagram for the proposed IMRC methodology. The presented techniques carefully avoid the repeated usage of the same information from the sequence

of tasks. At each step \(k\), the IMRC method obtains forward mean vector \(\boldsymbol{\tau}_{k}^{\rightarrow}\) for the \(k\)-th task leveraging information from preceding tasks using the forward mean vector \(\boldsymbol{\tau}_{k-1}^{\rightarrow}\) and the sample average \(\boldsymbol{\tau}_{k}\). Reciprocally, backward mean vectors \(\boldsymbol{\tau}_{j+1}^{\rightarrow k}\) for each \(j\)-th task are obtained leveraging information from the \(k\)-th task through the sample average \(\boldsymbol{\tau}_{k}\). Then, the forward and backward mean vectors \(\boldsymbol{\tau}_{j}^{=k}\) are obtained from the forward mean vectors \(\boldsymbol{\tau}_{j}^{\rightarrow}\) and the backward mean vectors \(\boldsymbol{\tau}_{j+1}^{\rightarrow k}\). In particular, \(\boldsymbol{\tau}_{j}^{\rightarrow}\) provides the information from the preceding tasks \(1,2,\ldots,j\), while \(\boldsymbol{\tau}_{j+1}^{\rightarrow k}\) provides the information from the succeeding tasks \(j+1,j+2,\ldots,k\). At each step \(k\), the IMRC method obtains forward and backward mean vectors \(\boldsymbol{\tau}_{j}^{=k}\) for \(j=k-b,k-b+1,\ldots,k\) with \(b\) the number of backward steps. In particular, if \(b=0\), IMRC carries out only forward learning. Note that, at each step \(k\), the proposed IMRC methods only need to retain the forward mean vectors \(\boldsymbol{\tau}_{j}^{\rightarrow}\) and sample averages \(\boldsymbol{\tau}_{j}\) for \(j=k-b,k-b+1,\ldots,k\).

``` Input:\(D_{k}\) from new task, and \(\boldsymbol{\tau}_{j},\boldsymbol{s}_{j},\boldsymbol{\tau}_{j}^{\rightarrow}\), \(\boldsymbol{s}_{j}^{\rightarrow}\) for \(k-b\leq j\leq k-1\) from previous \(b-1\) steps Output:\(\boldsymbol{\mu}_{j}\) for \(k-b\leq j\leq k\), \(\boldsymbol{\tau}_{k},\boldsymbol{s}_{k},\boldsymbol{\tau}_{j}^{\rightarrow}\), \(\boldsymbol{s}_{k}^{\rightarrow}\)  Obtain sample average and MSE vectors \(\boldsymbol{\tau}_{k}^{\rightarrow}=\boldsymbol{\tau}_{k},\boldsymbol{s}_{k} ^{\rightarrow}=\boldsymbol{s}_{k}\) using the sample set \(D_{k}\) \(\triangleright\) Single-task Estimate the tasks' expected quadratic change \(\boldsymbol{d}_{k}^{2}\) using (7)  Obtain the forward mean and MSE vectors \(\boldsymbol{\tau}_{k}^{\rightarrow},\boldsymbol{s}_{k}^{\rightarrow}\) using (6) \(\triangleright\) Forward Take \(\boldsymbol{\lambda}_{k}^{\rightarrow}=\sqrt{\boldsymbol{s}_{k}^{ \rightarrow}}\) and obtain classifier parameter \(\boldsymbol{\mu}_{k}\) solving the optimization problem (3) for\(j=k-1,k-2,\ldots,k-b\)do  Estimate the tasks' expected quadratic change \(\boldsymbol{d}_{j}^{2}\) using (7)  Obtain backward mean and MSE vectors \(\boldsymbol{\tau}_{j+1}^{\rightarrow k},\boldsymbol{s}_{j+1}^{ \rightarrow k}\) using (6) in retrodiction \(\triangleright\) Backward Obtain mean and MSE vectors \(\boldsymbol{\tau}_{j}^{\rightarrow k},\boldsymbol{s}_{j}^{\rightarrow}\) using (10) \(\triangleright\) Forward and backward Take \(\boldsymbol{\lambda}_{j}^{\rightarrow k}=\sqrt{\boldsymbol{s}_{j}^{ \rightarrow k}}\) and obtain classifier parameters \(\boldsymbol{\mu}_{j}\) solving the optimization problem (3) ```

**Algorithm 1** IMRC at step \(k\)

Algorithm 1 details the implementation of the proposed IMRCs at each step. For \(k\) steps, IMRCs have computational complexity \(\mathcal{O}((b+1)Kmk)\) and memory complexity \(\mathcal{O}((b+k)m)\) where \(K\) is the number of iterations used for the convex optimization problem (3), \(m\) is the length of the feature vector, and \(b\) is the number of backward steps. The complexity of forward and backward learning increases proportionally to the number of backward steps that can be taken to be rather small, as shown in the following. Even more efficient implementations can be obtained using Rauch-Tung-Striebel recursions (see e.g., Section 7.2 in [34]) that can be used to obtain \(\boldsymbol{\tau}_{j}^{\rightarrow k}\) from \(\boldsymbol{\tau}_{j+1}^{\rightarrow k}\), as shown in Appendix E.

### Performance guarantees and effective sample sizes with forward and backward learning

The following result provides bounds for the minimax risk for each task using forward and backward learning

**Theorem 3**.: Let \(\mathcal{U}_{j}^{=k}\) be the uncertainty set given by (1) using the mean and confidence vector \(\boldsymbol{\tau}_{j}^{=k}\) and \(\boldsymbol{\lambda}_{j}^{=k}=\sqrt{\boldsymbol{s}_{j}^{=k}}\) provided by (10), and let \(\kappa\) and \(n_{j}^{\rightarrow}\) be as in Theorem 1. Then, under the evolving task assumption of Section 2, we have with probability at least \(1-\delta\) that

\[R(\mathcal{U}_{j}^{=k})\leq R_{j}^{\infty}+\frac{M(\kappa+1)\sqrt{2\log{(2m/ \delta)}}}{\sqrt{n_{j}^{=k}}}\left\|\boldsymbol{\mu}_{j}^{\infty}\right\|_{1} \text{ for any }j\in\{1,2,\ldots,k\}\] (11)

Figure 2: Diagram for IMRC methodology.

[MISSING_PAGE_FAIL:8]

## 5 Numerical results

This section first compares the classification performance of IMRCs with existing techniques using multiple datasets, then we show the performance improvement of the presented IMRCs due to forward and backward learning. In Appendix H, we provide additional implementation details and numerical results. The implementation of the proposed IMRCs is available on the web https://github.com/MachineLearningBCAM/IMRCs-for-incremental-learning-NeurIPS-2023.

The proposed method is evaluated using \(12\) public datasets composed by evolving tasks [10; 1; 35; 36; 37; 38; 39; 40; 2]. Six datasets are formed by images with characteristics/quality/realism that change over time; while the rest are formed by tabular data that are segmented by time (see further details in Appendix H). For instance, each task in the "Yearbook" dataset corresponds to portrait's photographs from one year from 1905 to 2013 and the goal is to predict males and females; and each task in the "CLEAR" dataset corresponds to images with a natural temporal evolution of visual concepts per year from 2004 to 2014 and the goal is to predict if an image is soccer, hockey, or racing. For the image datasets, instances are represented by pixel values in "Rotated MNIST" dataset, and by the last layer of the ResNet18 pre-trained network [42] in the remaining datasets; while for the tabular datasets, instances are represented by \(200\) RFFs [28; 29] with scaling parameter \(\sigma^{2}=10\).

The proposed IMRC method is compared with \(7\) state-of-the-art-techniques: \(2\) techniques of continual learning based on experience replay [5; 17], a technique of continual learning based on regularization [6], a technique of continual learning based on dynamic architectures [4], \(2\) techniques of concept drift adaptation based on weight factors [9; 16], and a technique of concept drift adaptation based on sliding windows [10]. The hyper-parameters in all methods are set to the default values provided by the authors. IMRCs are implemented using \(b=3\) backward steps and the expected quadratic change \(\bm{d}_{j}^{2}\) is estimated using \(W=2\) in (7). In Appendix H, among other additional results, we study the change in classification error and processing time achieved by varying the number \(b\) of backward steps.

In the first set of numerical results, we compare the performance of the proposed IMRCs with existing techniques using \(n=10\) samples per task. These numerical results are obtained computing the average classification error over all the time steps and tasks in 50 random instantiations of data samples. As can be observed in Table 1, IMRCs can significantly improve performance in evolving tasks with respect to existing methods. Certain techniques of continual learning or concept drift adaptation achieve top performance in specific datasets, but the performance improvement of the methods presented are realized over general datasets with evolving tasks. The results for \(n=10\) samples per task in Table 1 are complemented with results for \(n=50,100\), and \(150\) samples per task in Tables 4, 5, and 6 in Appendix H. These results show that the improved performance of IMRCs compared to the state-of-the-art techniques is similar for different sample sizes. In addition, Table 1 shows that the average running time per task of IMRC methods is similar to other state-of-the-art methods.

\begin{table}
\begin{tabular}{l c c c c c c c c} \hline Method & GEM [5] & MER [17] & EELLA [4] & EWC [6] & Condor [9] & DriftSurf [10] & AUE [16] & IMRC \\ \hline Yearbook [1] &.18 \(\pm\).03 &.16 \(\pm\).03 &.45 \(\pm\).01 &.47 \(\pm\).05 &.14 \(\pm\).01 &.33 \(\pm\).02 &.33 \(\pm\).02 & **.13**\(\pm\).04 \\ I. noise [35] &.39 \(\pm\).08 &.17 \(\pm\).03 &.48 \(\pm\).05 &.47 \(\pm\).04 &.16 \(\pm\).02 &.48 \(\pm\).02 &.48 \(\pm\).02 & **.15**\(\pm\).03 \\ DomainNet [36] &.69 \(\pm\).05 &.38 \(\pm\).04 &.67 \(\pm\).05 &.75 \(\pm\).04 &.45 \(\pm\).04 & **.32**\(\pm\).02 &.33 \(\pm\).02 &.34 \(\pm\).06 \\ UTFKaes [37] &.12 \(\pm\).00 &.17 \(\pm\).09 &.19 \(\pm\).12 &.12 \(\pm\).00 &.23 \(\pm\).00 &.12 \(\pm\).00 &.12 \(\pm\).00 & **.10**\(\pm\).01 \\ R. MNIST [38] & **.36**\(\pm\).06 &.37 \(\pm\).09 &.48 \(\pm\).05 &.48 \(\pm\).01 &.45 \(\pm\).02 &.48 \(\pm\).02 &.48 \(\pm\).02 & **.36**\(\pm\).01 \\ CLEAR [39] &.57 \(\pm\).10 &.10 \(\pm\).03 &.61 \(\pm\).06 &.65 \(\pm\).03 &.35 \(\pm\).00 &.33 \(\pm\).00 &.33 \(\pm\).01 & **.09**\(\pm\).03 \\ P. Supply [10] &.46 \(\pm\).01 &.47 \(\pm\).01 &.41 \(\pm\).07 &.47 \(\pm\).00 &.36 \(\pm\).01 &.45 \(\pm\).02 &.45 \(\pm\).02 & **.30**\(\pm\).01 \\ Usenet1 [40] &.48 \(\pm\).02 &.48 \(\pm\).02 &.39 \(\pm\).05 &.48 \(\pm\).02 &.48 \(\pm\).02 &.40 \(\pm\).01 &.40 \(\pm\).01 & **.32**\(\pm\).03 \\ Usenet2 [40] &.36 \(\pm\).01 & **.33**\(\pm\).01 &.39 \(\pm\).03 &.48 \(\pm\).02 &.42 \(\pm\).01 &.35 \(\pm\).03 &.39 \(\pm\).03 & **.33**\(\pm\).02 \\ German [41] &.38 \(\pm\).16 &.38 \(\pm\).10 & **.33**\(\pm\).04 &.35 \(\pm\).11 &.41 \(\pm\).02 &.36 \(\pm\).00 &.35 \(\pm\).02 & **.34**\(\pm\).01 \\ Spam [2] &.25 \(\pm\).05 & **.13**\(\pm\).02 &.28 \(\pm\).07 &.33 \(\pm\).11 &.22 \(\pm\).02 &.24 \(\pm\).03 &.25 \(\pm\).03 & **.13**\(\pm\).01 \\ Covert. [10] &.09 \(\pm\).01 & **.08**\(\pm\).00 &.13 \(\pm\).01 &.09 \(\pm\).00 &.09 \(\pm\).00 &.10 \(\pm\).00 &.11 \(\pm\).00 & **.08**\(\pm\).00 \\ \hline Ave. running time & 0.238 & 0.250 & 0.055 & 0.313 & 0.184 & 0.128 & 0.044 & 0.275 \\ \hline \end{tabular}
\end{table}
Table 1: Classification error, standard deviation, and running time of the proposed IMRCs method in comparison with existing techniques using \(n=10\) samples per task.

In the second set of numerical results, we analyze the contribution of forward and backward learning to the final performance of IMRCs. In particular, we show the relationship among classification error, number of tasks, and sample size for single-task, forward, and forward and backward learning. These numerical results are obtained averaging, for each number of tasks and sample size, the classification errors achieved with \(10\) random instantiations of data samples in "Yearbook" dataset (see Appendix H for further details). Figure 3(a) shows the classification error of IMRC method divided by the classification error of single-task learning for different number of tasks with \(n=10\) and \(n=100\) sample sizes. Such figure shows that forward and backward learning can significantly improve the performance of IMRCs as tasks arrive. In addition, Figure 3(b) shows the classification error of IMRC method for different sample sizes with \(k=10\) and \(k=100\) tasks. Such figure shows that IMRCs with forward and backward learning for \(k=100\) tasks and \(n=10\) samples achieve significantly better results than single-task learning using \(n=100\) samples. In particular, the experiments show that the methods proposed can effectively exploit backward learning that results in enhanced classification error in all the experimental results.

## 6 Conclusion

The paper proposes IMRCs that effectively exploit forward and backward learning and account for evolving tasks by using a sequence of uncertainty sets that can contain the true underlying distributions. In addition, the paper analytically characterizes the increase in ESS achieved by the proposed techniques in terms of the tasks' expected quadratic change and number of tasks. The numerical results assess the performance improvement of IMRCs with respect to existing methods using multiple datasets, sample sizes, and number of tasks. The proposed methodology for incremental learning with evolving tasks can lead to techniques that further approach the humans' ability to learn from few examples and to continuously improve on tasks that arrive over time.

## Acknowledgments

Funding in direct support of this work has been provided by projects PID2022-137063NB-I00, PID2022-137442NB-I00, CNS2022-135203, and CEX2021-001142-S funded by MCIN/AEI/10.13039/501100011033 and the European Union "NextGenerationEU"/PRTR, BCAM Severo Ochoa accreditation CEX2021-001142-S / MICIN / AEI/ 10.13039/501100011033 funded by the Ministry of Science and Innovation, and programmes ELKARTEK, IT1504-22, and BERC-2022-2025 funded by the Basque Government.

Figure 4: Forward and backward learning can sharply boost performance and ESS as tasks arrive.

[MISSING_PAGE_FAIL:11]

* [19] Giulia Denevi, Dimitris Stamos, Carlo Ciliberto, and Massimiliano Pontil. Online-within-online meta learning. In _Advances in Neural Information Processing Systems_, 2019.
* [20] Ju Xu and Zhanxing Zhu. Reinforced continual learning. In _Advances in Neural Information Processing Systems_, volume 31, 2018.
* [21] Anastasia Pentina and Christoph H Lampert. Lifelong learning with non-iid tasks. In _Advances in Neural Information Processing Systems_, volume 28, 2015.
* [22] Farzan Farnia and David Tse. A minimax approach to supervised learning. In _Advances in Neural Information Processing Systems_, volume 29, 2016.
* [23] Rizal Fathony, Anqi Liu, Kaiser Asif, and Brian D. Ziebart. Adversarial multiclass classification: A risk minimization perspective. In _Advances in Neural Information Processing Systems_, volume 29, 2016.
* [24] Santiago Mazuelas, Andrea Zanoni, and Aritz Perez. Minimax classification with 0-1 loss and performance guarantees. In _Advances in Neural Information Processing Systems_, volume 33, 2020.
* [25] Santiago Mazuelas, Mauricio Romero, and Peter Grunwald. Minimax risk classifiers with 0-1 loss. _Journal of Machine Learning Research_, 24(208):1-48, 2023.
* [26] Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. _Foundations of machine learning_. MIT press, 2018.
* [27] Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and new perspectives. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 35(8), 2013.
* [28] Jing Lu, Steven CH Hoi, Jialei Wang, Peilin Zhao, and Zhi-Yong Liu. Large scale online kernel learning. _Journal of Machine Learning Research_, 17(1), 2016.
* [29] Yanning Shen, Tianyi Chen, and Georgios B Giannakis. Random feature-based online multi-kernel learning in environments with unknown dynamics. _The Journal of Machine Learning Research_, 20(1), 2019.
* [30] Santiago Mazuelas, Yuan Shen, and Aritz Perez. Generalized maximum entropy for supervised classification. _IEEE Transactions on Information Theory_, 68(4), 2022.
* [31] Martin J Wainwright. _High-dimensional statistics: A non-asymptotic viewpoint_. Cambridge University Press, 2019.
* [32] Yu Nesterov and Vladimir Shikhman. Quasi-monotone subgradient methods for nonsmooth convex minimization. _Journal of Optimization Theory and Applications_, 165(3), 2015.
* [33] Wei Tao, Zhisong Pan, Gaowei Wu, and Qing Tao. The strength of Nesterov's extrapolation in the individual convergence of nonsmooth optimization. _IEEE Transactions on Neural Networks and Learning Systems_, 31(7), 2019.
* [34] Brian DO Anderson and John B Moore. _Optimal Filtering_. Prentice-Hall, 1979.
* [35] Zheda Mai, Ruiwen Li, Jihwan Jeong, David Quispe, Hyunwoo Kim, and Scott Sanner. Online continual learning in image classification: An empirical survey. _Neurocomputing_, 469, 2022.
* [36] Xingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate Saenko, and Bo Wang. Moment matching for multi-source domain adaptation. In _IEEE International Conference on Computer Vision_, 2019.
* [37] Zhifei Zhang, Yang Song, and Hairong Qi. Age progression/regression by conditional adversarial autoencoder. In _IEEE Conference on Computer Vision and Pattern Recognition_, 2017.
* [38] Xisen Jin, Arka Sadhu, Junyi Du, and Xiang Ren. Gradient-based editing of memory examples for online task-free continual learning. In _Advances in Neural Information Processing Systems_, volume 34, 2021.

* [39] Zhiqiu Lin, Jia Shi, Deepak Pathak, and Deva Ramanan. The CLEAR benchmark: Continual learning on real-world imagery. In _Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2)_, 2021.
* [40] Ioannis Katakis, Grigorios Tsoumakas, and Ioannis Vlahavas. An ensemble of classifiers for coping with recurring contexts in data streams, 2008.
* [41] Dheeru Dua and Casey Graff. UCI machine learning repository, 2017.
* [42] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _IEEE Conference on Computer Vision and Pattern Recognition_, 2016.

Main notations used in the paper.

In this section we summarize the notations used in the paper. Table 2 shows the notation used for the mean vector, the confidence vector, the MSE vector, the ESS, the uncertainty set, and the minimax risk using single task, forward, backward, and forward and backward learning.

Appendix B Derivation of recursions in (6) for forward learning and recursions in (10) for forward and backward learning

This section shows how recursions in (6) and recursions in (10) are obtained using those for filtering and smoothing in linear dynamical systems.

The mean vectors evolve over time steps through the linear dynamical system

\[\boldsymbol{\tau}_{j}^{\infty}=\boldsymbol{\tau}_{j-1}^{\infty}+\boldsymbol{w} _{j}\] (13)

where vectors \(\boldsymbol{w}_{j}\) for \(j\in\{2,3,\ldots,k\}\) are independent and zero-mean because \(\mathrm{p}_{j}-\mathrm{p}_{j-1}\) are independent and zero-mean. In addition, each state variable \(\boldsymbol{\tau}_{j}^{\infty}\) is observed at each step \(j\) through \(\boldsymbol{\tau}_{j}\) that is the sample average of i.i.d. samples from \(\mathrm{p}_{j}\), so that we have

\[\boldsymbol{\tau}_{j}=\boldsymbol{\tau}_{j}^{\infty}+\boldsymbol{v}_{j}\] (14)

where vectors \(\boldsymbol{v}_{j}\) for \(j\in\{1,2,\ldots,k\}\) are independent and zero mean, and independent of \(\boldsymbol{w}_{j}\) for \(j\in\{1,2,\ldots,k\}\). Therefore, equations (13) and (14) above describe a linear dynamical system (state-space model with white noise processes) [34]. For such systems, the Kalman filter recursions provide the unbiased linear estimator with minimum MSE based on samples corresponding to preceding steps \(D_{1},D_{2},\ldots,D_{j}\), and fixed-lag smoother recursions provide the unbiased linear estimator with minimum MSE based on samples corresponding to preceding and succeeding steps \(D_{1},D_{2},\ldots,D_{k}\)[34]. Then, equations in (6) and equations in (10) are obtained after some algebra from the Kalman filter recursions and fixed-lag smoother recursions, respectively.

## Appendix C Proof of Theorem 1

Proof.: To obtain bound in (8) we first prove that the mean vector estimate and the MSE vector given by (6) satisfy

\[\mathbb{P}\left\{|\tau_{j}^{\infty(i)}-\tau_{j}^{\rightharpoonup(i)}|\leq \kappa\sqrt{2s_{j}^{\rightharpoonup(i)}\log\left(\frac{2m}{\delta}\right)} \right\}\geq(1-\delta)\] (15)

for any component \(i=1,2,\ldots,m\). Then, we prove that \(\|\sqrt{\boldsymbol{s}_{j}^{\rightharpoonup}}\|_{\infty}\leq M/\sqrt{n_{j}^ {\rightharpoonup(i)}}\) for \(j\in\{1,2,\ldots,k\}\), where the ESSs satisfy \(n_{1}^{\rightharpoonup}=n_{1}\) and \(n_{j}^{\rightharpoonup}\geq n_{j}+n_{j-1}^{\rightharpoonup(i)}\frac{\| \boldsymbol{\sigma}_{j}^{2}\|_{\infty}}{\|\boldsymbol{\sigma}_{j}^{2}\|_{ \infty}+n_{j-1}^{\rightharpoonup(i)}\|\boldsymbol{d}_{j}^{2}\|_{\infty}}\) for \(j\geq 2\).

To obtain inequality (15), we prove by induction that each component \(i=1,2,\ldots,m\) of the error in the mean vector estimate \(z_{j}^{\rightharpoonup(i)}=\tau_{j}^{\infty(i)}-\tau_{j}^{\rightharpoonup(i)}\) is sub-Gaussian with parameter \(\eta_{j}^{\rightharpoonup(i)}\leq\kappa\sqrt{s_{j}^{\rightharpoonup(i)}}\). Firstly, for \(j=1\), we have that

\[z_{1}^{\rightharpoonup(i)}=\tau_{1}^{\infty(i)}-\tau_{1}^{\rightharpoonup(i) }=\tau_{1}^{\infty(i)}-\tau_{1}^{(i)}.\]

\begin{table}
\begin{tabular}{l l l l l} \hline \hline  & Single task & Forward & Backward & Forward and Backward \\ \hline Mean vector & \(\boldsymbol{\tau}_{j}\) & \(\boldsymbol{\tau}_{j}^{\rightharpoonup}\) & \(\boldsymbol{\tau}_{j}^{\rightharpoonup k}\) & \(\boldsymbol{\tau}_{j}^{\rightharpoonup k}\) \\ Confidence vector & \(\boldsymbol{\lambda}_{j}\) & \(\boldsymbol{\lambda}_{j}^{\rightharpoonup}\) & \(\boldsymbol{\lambda}_{j}^{\rightharpoonup k}\) & \(\boldsymbol{\lambda}_{j}^{\rightharpoonup k}\) \\ MSE vector & \(\boldsymbol{s}_{j}\) & \(\boldsymbol{s}_{j}^{\rightharpoonup}\) & \(\boldsymbol{s}_{j}^{\rightharpoonup k}\) & \(\boldsymbol{s}_{j}^{\rightharpoonup k}\) \\ ESS & \(\boldsymbol{n}_{j}\) & \(\boldsymbol{n}_{j}^{\rightharpoonup}\) & \(\boldsymbol{n}_{j}^{\rightharpoonup k}\) & \(\boldsymbol{n}_{j}^{\rightharpoonup k}\) \\ Uncertainty set & \(\mathcal{U}_{j}\) & \(\mathcal{U}_{j}^{\rightharpoonup}\) & \(\mathcal{U}_{j}^{\rightharpoonup k}\) & \(\mathcal{U}_{j}^{\rightharpoonup k}\) \\ Minimax risk & \(R(\mathcal{U}_{j})\) & \(R(\mathcal{U}_{j}^{\rightharpoonup})\) & \(R(\mathcal{U}_{j}^{\rightharpoonup k})\) & \(R(\mathcal{U}_{j}^{\rightharpoonup k})\) \\ \hline \hline \end{tabular}
\end{table}
Table 2: Main notations used in the paper Since the bounded random variable \(\Phi_{1}^{(i)}\) is sub-Gaussian with parameter \(\sigma(\Phi_{1}^{(i)})\), then the error in the mean vector estimate \(z_{1}^{\rightarrow(i)}\) is sub-Gaussian with parameter that satisfies

\[\left(\eta_{1}^{\rightarrow(i)}\right)^{2}=\frac{\sigma\left(\Phi_{1}^{(i)} \right)^{2}}{n_{1}}\leq\frac{\kappa^{2}\sigma_{1}^{2(i)}}{n_{1}}=\kappa^{2}s_{ 1}^{(i)}.\]

If \(z_{j-1}^{\rightarrow}{}^{(i)}=\tau_{j-1}^{\infty}{}^{(i)}-\tau_{j-1}^{\rightarrow }{}^{(i)}\) is sub-Gaussian with parameter \(\eta_{j-1}^{\rightarrow}{}^{(i)}\leq\kappa\sqrt{s_{j-1}^{\rightarrow}{}^{(i)}}\) for any \(i\;=\;1,2,\ldots,m\), then using the recursions in (6) we have that

\[z_{j}^{\rightarrow(i)} =\tau_{j}^{\infty}{}^{(i)}-\tau_{j}^{\rightarrow(i)}=\tau_{j-1} ^{\infty}{}^{(i)}+w_{j}^{(i)}-\tau_{j}^{(i)}-\frac{s_{j}^{(i)}}{s_{j-1}^{ \rightarrow}{}^{(i)}+s_{j}^{(i)}+d_{j}^{2(i)}}\left(\tau_{j-1}^{-}{}^{(i)}- \tau_{j}^{(i)}\right)\] \[=\tau_{j-1}^{\infty}{}^{(i)}+w_{j}^{(i)}-\tau_{j-1}^{\rightarrow }{}^{(i)}+\left(1-\frac{s_{j}^{(i)}}{s_{j-1}^{\rightarrow}{}^{(i)}+s_{j}^{(i) }+d_{j}^{2(i)}}\right)\left(\tau_{j-1}^{\rightarrow}{}^{(i)}-\tau_{j}^{(i)}\right)\] \[=\tau_{j-1}^{\infty}{}^{(i)}+w_{j}^{(i)}-\tau_{j-1}^{\rightarrow }{}^{(i)}-\frac{s_{j}^{-(i)}}{s_{j}^{(i)}}\left(\tau_{j}^{(i)}-\tau_{j-1}^{ \rightarrow}{}^{(i)}\right)\]

since \(\bm{w}_{j}=\bm{\tau}_{j}^{\infty}-\bm{\tau}_{j-1}^{\infty}\). If \(\bm{v}_{j}=\bm{\tau}_{j}-\bm{\tau}_{j}^{\infty}\), the error in the mean vector estimate is given by

\[z_{j}^{\rightarrow(i)} =\tau_{j-1}^{\infty}{}^{(i)}+w_{j}^{(i)}-\tau_{j-1}^{\rightarrow }{}^{(i)}-\frac{s_{j}^{\rightarrow(i)}}{s_{j}^{(i)}}\left(\tau_{j}^{\infty} {}^{(i)}+v_{j}^{(i)}-\tau_{j-1}^{\rightarrow}{}^{(i)}\right)\] \[=\tau_{j-1}^{\infty}{}^{(i)}+w_{j}^{(i)}-\tau_{j-1}^{\rightarrow }{}^{(i)}-\frac{s_{j}^{-(i)}}{s_{j}^{(i)}}\left(\tau_{j-1}^{\infty}{}^{(i)}+w _{j}^{(i)}+v_{j}^{(i)}-\tau_{j-1}^{\rightarrow}{}^{(i)}\right)\] \[=\left(1-\frac{s_{j}^{\rightarrow(i)}}{s_{j}^{(i)}}\right)z_{j-1 }^{\rightarrow}{}^{(i)}+\left(1-\frac{s_{j}^{\rightarrow(i)}}{s_{j}^{(i)}} \right)\left(w_{j}^{(i)}\right)-\frac{s_{j}^{\rightarrow(i)}}{s_{j}^{(i)}}v _{j}^{(i)}\]

where \(w_{j}^{(i)}\) and \(v_{j}^{(i)}\) are sub-Gaussian with parameter \(\sigma(w_{j}^{(i)})\) and \(\sigma\left(\Phi_{j}^{(i)}\right)/\sqrt{n_{j}}\), respectively. Therefore, we have that \(z_{j}^{\rightarrow(i)}\) is sub-Gaussian with parameter \(\eta_{j}^{\rightarrow(i)}\) that satisfies

\[\left(\eta_{j}^{\rightarrow(i)}\right)^{2}= \left(1-\frac{s_{j}^{\rightarrow(i)}}{s_{j}^{(i)}}\right)^{2} \left(\eta_{j-1}^{\rightarrow}{}^{(i)}\right)^{2}+\left(1-\frac{s_{j}^{ \rightarrow(i)}}{s_{j}^{(i)}}\right)^{2}\sigma\left(w_{j}^{(i)}\right)^{2}+ \left(\frac{s_{j}^{\rightarrow(i)}}{s_{j}^{(i)}}\right)^{2}\frac{\sigma \left(\Phi_{j}^{(i)}\right)^{2}}{n_{j}}\]

since \(\bm{z}_{j-1}^{\rightarrow}\), \(\bm{w}_{j}\), and \(\bm{v}_{j}\) are independent. Using that \(\eta_{j-1}^{\rightarrow}{}^{(i)}\leq\kappa\sqrt{s_{j-1}^{\rightarrow}{}^{(i)}}\) and the definition of \(\kappa\), we have that

\[\left(\eta_{j}^{\rightarrow(i)}\right)^{2} \leq\left(1-\frac{s_{j}^{\rightarrow(i)}}{s_{j}^{(i)}}\right)^{2 }\kappa^{2}s_{j-1}^{\rightarrow(i)}+\left(1-\frac{s_{j}^{\rightarrow(i)}}{s_ {j}^{(i)}}\right)^{2}\kappa^{2}d_{j}^{2(i)}+\left(\frac{s_{j}^{\rightarrow(i)} }{s_{j}^{(i)}}\right)^{2}\kappa^{2}\frac{\sigma_{j}^{2(i)}}{n_{j}}\] \[\leq\left(1-\frac{s_{j}^{\rightarrow(i)}}{s_{j}^{(i)}}\right)^{2 }\kappa^{2}\left(\left(\frac{1}{s_{j}^{\rightarrow(i)}}-\frac{1}{s_{j}^{(i)}} \right)^{-1}+d_{j}^{(i)}\right)+\frac{\left(s_{j}^{\rightarrow(i)}\right)^{2}}{ s_{j}^{(i)}}\kappa^{2}\] (16) \[=\left(1-\frac{s_{j}^{\rightarrow(i)}}{s_{j}^{(i)}}\right)\kappa^ {2}s_{j}^{\rightarrow(i)}+\kappa^{2}\frac{\left(s_{j}^{\rightarrow(i)} \right)^{2}}{s_{j}^{(i)}}\]

where (16) is obtained using the second recursion in (6).

The inequality in (15) is obtained using the union bound together with the Chernoff bound (concentration inequality) [31] for the random variables \(z_{j}^{\rightarrow(i)}\) that are sub-Gaussian with parameter \(\eta_{j}^{\rightarrow(i)}\).

Now, we prove by induction that, for any \(j\), \(\|\sqrt{s_{j}^{\rightarrow}}\|_{\infty}\leq M/\sqrt{n_{j}^{\rightarrow}}\) where the ESSs satisfy \(n_{1}^{\rightarrow}=n_{1}\) and \(n_{j}^{\rightarrow}\geq n_{j}+n_{j-1}^{\rightarrow}\frac{\|\bm{\sigma}_{j}^{ \rightarrow}\|_{\infty}}{\|\bm{\sigma}_{j}^{\rightarrow}\|_{\infty}+n_{j-1}^{ \rightarrow}\|\bm{d}_{j}^{2}\|_{\infty}}\) for \(j\geq 2\). For \(j=1\), using the definition of \(\bm{s}_{j}^{\rightarrow}\) in the second recursion in (6), we have that for any component \(i\)

\[\left(s_{1}^{\rightharpoonup(i)}\right)^{-1}=\left(s_{1}^{(i)}\right)^{-1}= \frac{n_{1}}{\sigma_{1}^{2(i)}}\geq\frac{n_{1}}{M^{2}}.\]

Then, vector \(\bm{s}_{1}^{\rightharpoonup}\) satisfies

\[\|\sqrt{\bm{s}_{1}^{\rightharpoonup}}\|_{\infty}\leq\frac{M}{\sqrt{n_{1}}}= \frac{M}{\sqrt{n_{1}^{\rightharpoonup}}}.\]

If \(\|\sqrt{\bm{s}_{j-1}^{\rightharpoonup}}\|_{\infty}\leq M/\sqrt{n_{j-1}^{ \rightharpoonup}}\), then we have that for any component \(i\)

\[\left(s_{j}^{\rightharpoonup(i)}\right)^{-1} =\frac{1}{s_{j}^{(i)}}+\frac{1}{s_{j-1}^{\rightharpoonup(i)}+d_{ j}^{2(i)}}\geq\frac{1}{s_{j}^{(i)}}+\frac{1}{\frac{M^{2}}{n_{j-1}^{\rightharpoonup}}+d_{ j}^{2(i)}}\geq\frac{1}{M^{2}}\left(n_{j}+\frac{1}{\frac{1}{n_{j-1}^{\rightharpoonup}}+ \frac{d_{j}^{2(i)}}{M^{2}}}\right)\] \[\geq\frac{1}{M^{2}}\left(n_{j}+\frac{1}{\frac{1}{n_{j-1}^{ \rightharpoonup}}+\frac{\|d_{j}^{2}\|_{\infty}}{\|\bm{\sigma}_{j}^{2}\|_{ \infty}}}\right)\]

by using the second recursion in (6) and the induction hypothesis. Then, vector \(\bm{s}_{j}^{\rightharpoonup}\) satisfies

\[\left\|\sqrt{\bm{s}_{j}^{\rightharpoonup}}\right\|_{\infty}\leq\frac{M}{\sqrt {n_{j}+n_{j-1}^{\rightharpoonup}\frac{\|\bm{\sigma}_{j}^{2}\|_{\infty}}{\|\bm{ \sigma}_{j}^{2}\|_{\infty}+n_{j-1}^{\rightharpoonup}\|\bm{d}_{j}^{2}\|_{ \infty}}}}.\] (17)

The inequality in (8) is obtained because the minimax risk is bounded by the smallest minimax risk as shown in [24, 25, 30] so that

\[R(\mathcal{U}_{j}^{\rightharpoonup})\leq R_{j}^{\rightharpoonup}+\left(\|\bm{ \tau}_{j}^{\rightharpoonup}-\bm{\tau}_{j}^{\rightharpoonup}\|_{\infty}+\|\bm{ \lambda}_{j}^{\rightharpoonup}\|_{\infty}\right)\left\|\bm{\mu}_{j}^{\rightharpoonup} \right\|_{1}\]

that leads to (8) using (15), (17), and the fact that \(1\leq\sqrt{2\log\left(\frac{2m}{\delta}\right)}\). 

## Appendix D Proof of Theorem 2

Proof.: To obtain bound in (9), we proceed by induction. For \(j=1\), using the expression for the ESS in (8), we have that

\[n_{1}^{\rightharpoonup}=n_{1}\geq n.\]

If (9) holds for the \((j-1)\)-task, then for the \(j\)-th task, we have that

\[n_{j}^{\rightharpoonup}\geq n_{j}+n_{j-1}^{\rightharpoonup}\frac{\|\bm{\sigma} _{j}^{2}\|_{\infty}}{\|\bm{\sigma}_{j}^{2}\|_{\infty}+n_{j-1}^{\rightharpoonup} \|\bm{d}_{j}^{2}\|_{\infty}}\geq n+n_{j-1}^{\rightharpoonup}\frac{1}{1+n_{j- 1}^{\rightharpoonup}d^{2}}=n\left(1+\frac{1}{\frac{n}{n_{j-1}^{\rightharpoonup} }+nd^{2}}\right)\]

where the second inequality is obtained because \(n_{j}\geq n\), \(\|\bm{\sigma}_{j}^{2}\|_{\infty}\leq 1\), and \(\|\bm{d}_{j}^{2}\|_{\infty}\leq d^{2}\). Using that \(n_{j-1}^{\rightharpoonup}\geq n\left(1+\frac{(1+\alpha)^{2j-3}-1-\alpha}{ \alpha(1+\alpha)^{2j-3}+\alpha}\right)\), the ESS of the \(j\)-th task satisfies

\[n_{j}^{\rightharpoonup} \geq n\left(1+\frac{1}{\frac{n}{n\left(1+\alpha\right)^{2j-3}-1- \alpha}{n\left(1+\alpha\right)^{2j-3}+\alpha}}+nd^{2}\right)=n\left(1+\frac{1 }{\frac{\alpha(1+\alpha)^{2j-3}+\alpha}{(1+\alpha)^{2j-2}-1}+nd^{2}}\right)\] \[=n\left(1+\frac{1}{\frac{\alpha(1+\alpha)^{2j-3}+\alpha}{(1+ \alpha)^{2j-2}-1}+\frac{\alpha^{2}}{\alpha+1}}\right)\] (18) \[=n\left(1+\frac{(1+\alpha)^{2j-1}-1-\alpha}{\alpha(1+\alpha)^{2j -2}+\alpha(\alpha+1)+\alpha^{2}(1+\alpha)^{2j-2}-\alpha^{2}}\right)\]

where (18) is obtained because \(nd^{2}=\frac{\alpha^{2}}{\alpha+1}\) since \(\alpha=\frac{nd^{2}}{2}\left(\sqrt{1+\frac{4}{nd^{2}}}+1\right)\).

Now, we obtain bounds for the ESS if \(nd^{2}<\frac{1}{j^{2}}\). In the following, the constant \(\phi\) represents the golden ratio \(\phi=1.618\ldots\).

If \(nd^{2}<\frac{1}{j^{2}}\Rightarrow\sqrt{nd^{2}}\leq\alpha\leq\sqrt{nd^{2}}\phi \leq\frac{\phi}{j}\leq 1\) because \(\alpha=\frac{nd^{2}}{2}\left(\sqrt{1+\frac{4}{nd^{2}}+1}\right)=\sqrt{nd^{2}} \sqrt{nd^{2}+\frac{4}{2}+\sqrt{nd^{2}}}\), then we have that \(n_{j}^{-}\) satisfies

\[n_{j}^{\rightarrow}\geq n\left(1+\frac{1}{\alpha}\frac{\alpha(2j-2)}{2+\alpha( 2j-1)}\right)=n\left(1+\frac{2j-2}{2+\alpha(2j-1)}\right)\]

where the first inequality follows because \((1+\alpha)^{2j-2}\geq 1+\alpha(2j-2)\). Using \(\alpha\leq\frac{\phi}{j}\), we have that

\[n_{j}^{\rightarrow}\geq n\left(1+\frac{2j-2}{2+\frac{\phi}{j}(2j-1)}\right) \geq n\left(1+\frac{2j-2}{2+2\phi-\frac{\phi}{j}}\right)\geq n\left(1+\frac{j -1}{1+\phi}\right).\]

## Appendix E More efficient recursions for forward and backward learning

The Rauch-Tung-Striebel smoother recursions [34] allow to obtain forward and backward mean and MSE vectors directly from those vectors for the succeeding task. Specifically, for each \(j\)-th task, the mean vector \(\bm{\tau}_{j}^{=k}\) together with the MSE vector \(\bm{s}_{j}^{=k}\) can be obtained using those vectors for the succeeding task \(\bm{\tau}_{j+1}^{=k},\bm{s}_{j+1}^{=k}\) as

\[\bm{\tau}_{j}^{=k} =\bm{\tau}_{j}^{\rightarrow}+\frac{\bm{s}_{j}^{\rightarrow}}{ \bm{s}_{j}^{\rightarrow}+\bm{d}_{j+1}^{2}}\left(\bm{\tau}_{j+1}^{=k}-\bm{\tau} _{j}^{\rightarrow}\right)\] \[\bm{s}_{j}^{=k} =\left(\frac{1}{\bm{s}_{j}^{\rightarrow}}+\left(\bm{d}_{j+1}^{ 2}+\left(\frac{\bm{1}}{\bm{s}_{j+1}^{=k}}-\frac{1}{\bm{s}_{j}^{\rightarrow}+ \bm{d}_{j+1}^{2}}\right)^{-1}\right)^{-1}\right)^{-1}.\]

The above recursions provide the same mean vector estimate as the recursions in (10) in the paper since they are obtained using the Rauch-Tung-Striebel smoother recursions instead of fixed-lag smoother recursions [34].

## Appendix F Proof of Theorem 3

Proof.: To obtain bound in (11) we first prove that the mean vector estimate and the MSE vector given by (10) satisfy

\[\mathbb{P}\left\{|\tau_{k}^{\infty\,(i)}-\tau_{j}^{=k\,(i)}|\leq\kappa\sqrt{2 s_{j}^{=k\,(i)}\log\left(\frac{2m}{\delta}\right)}\right\}\geq(1-\delta)\] (19)

for any component \(i=1,2,\ldots,m\). Then, we prove that \(\|\bm{s}_{j}^{=k}\|_{\infty}\leq M/\sqrt{n_{j}^{=k}}\) for \(j\in\{1,2,\ldots,k\}\), where the ESSs satisfy \(n_{k}^{=k}=n_{k}^{\rightarrow}\) and \(n_{j}^{=k}\geq n_{j}^{\rightarrow}+n_{j+1}^{-k}\frac{\|\bm{\sigma}_{j}^{2}\|_ {\infty}}{\|\bm{\sigma}_{j}^{2}\|_{\infty}+\frac{1}{j+1}\|\bm{d}_{j+1}^{2}\|_ {\infty}}\) for \(j\geq 2\).

To obtain inequality (19), we prove that each component \(i=1,2,\ldots,m\) of the error in the mean vector estimate \(z_{j}^{=k\,(i)}=\tau_{j}^{\infty\,(i)}-\tau_{j}^{=k\,(i)}\) is sub-Gaussian with parameter \(\eta_{j}^{=k\,(i)}\leq\kappa\sqrt{s_{j}^{=k\,(i)}}\). Analogously to the proof of Theorem 1, it is proven that each component in the error of the backward mean vector \(\bm{\tau}_{j+1}^{\rightarrow-k}\) is sub-Gaussian with parameters satisfying \(\bm{\eta}_{j+1}^{\rightarrow}\preceq\kappa\sqrt{\bm{s}_{j+1}^{=k}}\). The error in the forward and backward mean vector estimate is given by

\[z_{j}^{=k\,(i)}=\tau_{j}^{\infty\,(i)}-\tau_{j}^{=k\,(i)}=\tau_{j}^{\infty\,( i)}-\tau_{j}^{\rightarrow\,(i)}-\frac{s_{j}^{\rightarrow\,(i)}}{s_{j}^{ \rightarrow\,(i)}+s_{j+1}^{-k\,(i)}+d_{j+1}^{2\,(i)}}\left(\tau_{j+1}^{-k\,(i) }-\tau_{j}^{\rightarrow\,(i)}\right)\]where the second equality is obtained using the recursion for \(\tau_{j}^{=k\,(i)}\) in (10). Adding and subtracting \(\frac{s_{j}^{\rightarrow\,(i)}}{s_{j}^{\rightarrow\,(i)}+s_{j+1}^{\rightarrow\, (i)}+d_{j+1}^{2\,\,(i)}}\tau_{j+1}^{\infty\,\,(i)}\), we have that

\[z_{j}^{=k\,(i)} =z_{j}^{-(i)}-\frac{s_{j}^{\rightarrow\,(i)}}{s_{j}^{\rightarrow \,(i)}+s_{j+1}^{\rightarrow\,(i)}+d_{j+1}^{2\,\,(i)}}\left(\tau_{j+1}^{ \infty\,\,(i)}-\tau_{j+1}^{\infty\,\,(i)}+\tau_{j+1}^{\leftarrow\,(i)}-\tau_{ j}^{\rightarrow\,(i)}\right)\] \[=z_{j}^{-(i)}-\frac{s_{j}^{\rightarrow\,(i)}}{s_{j}^{\rightarrow \,(i)}+s_{j+1}^{\rightarrow\,(i)}+d_{j+1}^{2\,\,(i)}}\left(\tau_{j}^{\infty \,(i)}+w_{j+1}^{(i)}-z_{j+1}^{\leftarrow\,(i)}-\tau_{j}^{\rightarrow\,(i)}\right)\]

since \(\boldsymbol{w}_{j}=\boldsymbol{\tau}_{j}^{\infty}-\boldsymbol{\tau}_{j-1}^{\infty}\) and \(z_{j}^{\rightarrow\,(i)}=\tau_{j}^{\infty\,(i)}-\tau_{j}^{\rightarrow\,(i)}\). Then, we have that

\[z_{j}^{=k\,(i)}=z_{j}^{\rightarrow\,(i)}-\frac{s_{j}^{ \rightarrow\,(i)}}{s_{j}^{\rightarrow\,(i)}+s_{j+1}^{\rightarrow\,(i)}+d_{j +1}^{2\,\,(i)}}\left(z_{j}^{\rightarrow\,(i)}+w_{j+1}^{(i)}-z_{j+1}^{\leftarrow \,(i)}\right)\] (20) \[=\left(1-\frac{s_{j}^{\rightarrow\,(i)}}{s_{j}^{\rightarrow\,(i) }+s_{j+1}^{\leftarrow\,(i)}+d_{j+1}^{2\,\,(i)}}\right)z_{j}^{\rightarrow\,(i) }-\frac{s_{j}^{\rightarrow\,(i)}}{s_{j}^{\rightarrow\,(i)}+s_{j+1}^{ \rightarrow\,(i)}+d_{j+1}^{2\,\,(i)}}\left(w_{j+1}^{(i)}-z_{j+1}^{\leftarrow\, (i)}\right)\]

where \(z_{j}^{\rightarrow\,(i)}\), \(z_{j+1}^{\leftarrow\,(i)}\), and \(w_{j+1}^{(i)}\) are sub-Gaussian with parameters \(\eta_{j}^{\rightarrow\,(i)}\leq\kappa\sqrt{s_{j}^{\rightarrow\,(i)}}\), \(\eta_{j+1}^{\leftarrow\,(i)}\leq\kappa\sqrt{s_{j+1}^{\rightarrow\,(i)}}\), and \(\sigma(w_{j}^{(i)})\), respectively. Since \(\boldsymbol{z}_{j}^{\rightarrow}\), \(\boldsymbol{z}_{j+1}^{\leftarrow}\), and \(\boldsymbol{w}_{j+1}\) are independent, we have that \(z_{j}^{=k\,(i)}\) given by (20) is sub-Gaussian with parameter that satisfies

\[\left(\eta_{j}^{=k\,(i)}\right)^{2}= \left(1-\frac{s_{j}^{\rightarrow\,(i)}}{s_{j}^{\rightarrow\,(i) }+s_{j+1}^{\rightarrow\,(i)}+d_{j+1}^{2\,\,(i)}}\right)^{2}\left(\eta_{j}^{ \rightarrow\,(i)}\right)^{2}\] \[+\left(\frac{s_{j}^{\rightarrow\,(i)}}{s_{j}^{\rightarrow\,(i) }+s_{j+1}^{\rightarrow\,(i)}+d_{j+1}^{2\,\,(i)}}\right)^{2}\left(\sigma\left( w_{j}^{(i)}\right)^{2}+\left(\eta_{j+1}^{\rightarrow\,(i)}\right)^{2}\right)\] \[\leq \left(1-\frac{s_{j}^{\rightarrow\,(i)}}{s_{j}^{\rightarrow\,(i) }+s_{j+1}^{\rightarrow\,(i)}+d_{j+1}^{2\,\,(i)}}\right)^{2}\kappa^{2}s_{j}^{ \rightarrow\,(i)}\] \[+\left(\frac{s_{j}^{\rightarrow\,(i)}}{s_{j}^{\rightarrow\,(i) }+s_{j+1}^{\rightarrow\,(i)}+d_{j+1}^{2\,\,(i)}}\right)^{2}\kappa^{2}\left(d _{j+1}^{\,\,(i)}+s_{j+1}^{\leftarrow\,(i)}\right)\]

Using the second recursion in (10) we have that the sub-Gaussian parameter satisfies

\[\left(\eta_{j}^{=k\,(i)}\right)^{2}\leq \left(1-\frac{s_{j}^{=k\,(i)}}{s_{j+1}^{\rightarrow\,(i)}+d_{j+1 }^{2\,\,(i)}}\right)^{2}\kappa^{2}\left(\frac{1}{s_{j}^{=k\,(i)}}-\frac{1}{s_ {j+1}^{-k\,\,(i)}+d_{j+1}^{2\,\,(i)}}\right)^{-1}\] \[+\frac{\left(s_{j}^{=k\,(i)}\right)^{2}}{s_{j+1}^{-k\,\,(i)}+d_{j +1}^{2\,\,(i)}}\kappa^{2}\] \[=\left(\frac{s_{j+1}^{\rightarrow\,(i)}+d_{j+1}^{2\,\,(i)}-s_{j} ^{=k\,(i)}}{s_{j+1}^{\rightarrow\,(i)}+d_{j+1}^{2\,\,(i)}}\right)\kappa^{2}s_{ j}^{=k\,(i)}\] \[+\frac{\left(s_{j}^{=k\,(i)}\right)^{2}}{s_{j+1}^{-k\,\,(i)}+d_{j +1}^{2\,\,(i)}}\kappa^{2}=\kappa^{2}s_{j}^{=k\,(i)}.\]

The inequality in (19) is obtained using the union bound together with the Chernoff bound (concentration inequality) [31] for the random variables \(z_{j}^{=k\,(i)}\) that are sub-Gaussian with parameter \(\eta_{j}^{=k\,(i)}\).

Now, we prove that, for any \(j\), \(\|\sqrt{\bm{s}_{j}^{=k}}\|\leq M/\sqrt{n_{j}^{=k}}\) where the ESSs satisfy \(n_{k}^{=k}=n_{k}^{\rightharpoonup}\) and \(n_{j}^{=k}\geq n_{j}^{\rightharpoonup}+n_{j+1}^{-k}\frac{\|\bm{\sigma}_{j}^{2} \|_{\infty}}{\|\bm{\sigma}_{j}^{2}\|_{\infty}+n_{j+1}^{-k}\|\bm{d}_{j+1}^{2}\|_ {\infty}}\) for \(j\geq 2\). Analogously to the proof of Theorem 1, we prove that the backward MSE vector \(\bm{s}_{j+1}^{-k}\) satisfies \(\|\sqrt{\bm{s}_{j+1}^{-k}}\|_{\infty}\leq M/\sqrt{n_{j+1}^{-k}}\). Then, using that \(\|\sqrt{\bm{s}_{j+1}^{=k}}\|_{\infty}\leq M/\sqrt{n_{j+1}^{-k}}\), we have that for every component \(i\)

\[\left(s_{j}^{=k(i)}\right)^{-1} =\frac{1}{s_{j}^{\rightharpoonup}(i)}+\frac{1}{s_{j+1}^{-k \hphantom{j+1}(i)}+d_{j+1}^{2\hphantom{j+1}(i)}}\geq\frac{n_{j}^{\rightharpoonup}}{ \sigma_{j}^{2\hphantom{j}(i)}}+\frac{1}{\frac{M^{2}}{n_{j+1}^{2}}+d_{j+1}^{2 \hphantom{j+1}(i)}}\] \[\geq\frac{1}{M^{2}}\left(n_{j}^{\rightharpoonup}+\frac{1}{\frac{ 1}{n_{j+1}^{-k}}+\frac{d_{j+1}^{2}}{M^{2}}}\right)\geq\frac{1}{M^{2}}\left(n_ {j}^{\rightharpoonup}+\frac{1}{\frac{1}{n_{j+1}^{-k}}+\frac{\|d_{j+1}^{2}\|_{ \infty}}{\|\bm{\sigma}_{j}^{2}\|_{\infty}}}\right).\]

Then, we obtain

\[\|\sqrt{\bm{s}_{j}^{=k}}\|_{\infty}\leq\frac{M}{\sqrt{n_{j}^{\rightharpoonup} +\frac{1}{\frac{1}{n_{j+1}^{-k}}+\frac{1}{\|\bm{d}_{j+1}^{2}\|_{\infty}}}}}.\] (21)

The inequality in (11) is obtained because the minimax risk is bounded by the smallest minimax risk as shown in [24, 25, 30] so that

\[R(\mathcal{U}_{j}^{=k})\leq R_{j}^{\infty}+\left(\|\bm{\tau}_{j}^{\infty}-\bm {\tau}_{j}^{=k}\|_{\infty}+\|\bm{\lambda}_{j}^{=k}\|_{\infty}\right)\left\|\bm {\mu}_{j}^{\infty}\right\|_{1}\]

that leads to (11) using (19), (21), and the fact that \(1\leq\sqrt{2\log\left(\frac{2m}{\delta}\right)}\). 

## Appendix G Proof of Theorem 4

Proof.: To obtain bound in (12), we use the ESS obtained with forward learning in Theorem 2 and obtained with backward learning. Analogously to the proof of Theorem 2, we prove that the ESS obtained at backward learning satisfies

\[n_{j+1}^{\rightharpoonup k}\geq n_{j+1}+n_{j+2}^{-k}\frac{\|\bm{\sigma}_{j+1}^{ 2}\|_{\infty}}{\|\bm{\sigma}_{j+1}^{2}\|_{\infty}+n_{j+2}^{-k}\|\bm{d}_{j+2}^{ 2}\|_{\infty}}\geq n\left(1+\frac{(1+\alpha)^{2(k-j)-1}-1-\alpha}{\alpha(1+ \alpha)^{2(k-j)-1}+\alpha}\right).\]

Therefore, the ESS obtained with forward an backward learning satisfies

\[n_{j}^{=k} \geq n_{j}^{\rightharpoonup}+n\left(1+\frac{(1+\alpha)^{2(k-j)-1 }-1-\alpha}{\alpha(1+\alpha)^{2(k-j)-1}+\alpha}\right)\left(1+\ \frac{n\left(1+\frac{(1+\alpha)^{2(k-j)-1}-1-\alpha}{\alpha(1+\alpha)^{2(k-j )-1}+\alpha}\right)}{nd^{2}}\right)^{-1}\] \[= n_{j}^{\rightharpoonup}+n\frac{(1+\alpha)^{2(k-j)}-1}{\alpha(1+ \alpha)^{2(k-j)-1}+\alpha}\left(1+\ \frac{\alpha^{2}}{\alpha+1}\left(1+\frac{(1+\alpha)^{2(k-j)-1}-1-\alpha}{ \alpha(1+\alpha)^{2(k-j)-1}+\alpha}\right)\right)^{-1}\]

where the second equality follows because \(nd^{2}=\frac{\alpha^{2}}{\alpha+1}\) since \(\alpha=\frac{nd^{2}}{2}\left(\sqrt{1+\frac{4}{nd^{2}}}+1\right)\). Then, we have that

\[n_{j}^{=k}\geq n_{j}^{\rightharpoonup}+n\frac{(1+\alpha)^{2(k-j)}-1}{\alpha(1+ \alpha)^{2(k-j)-1}+\alpha}\] \[\cdot\left(\frac{((1+\alpha)^{2(k-j)-1}+1)(\alpha+1+\alpha^{2})+ \alpha((1+\alpha)^{2(k-j)-1}-1-\alpha)}{(\alpha+1)((1+\alpha)^{2(k-j)-1}+1)} \right)^{-1}\] \[\geq n_{j}^{\rightharpoonup}+n\frac{(1+\alpha)^{2(k-j)}-1}{\alpha(1+ \alpha)^{2(k-j)-1}+\alpha}\frac{(\alpha+1)((1+\alpha)^{2(k-j)-1}+1)}{(1+ \alpha)^{2(k-j)+1}+1}.\]

Now, we obtain bounds for the ESS depending on the value value of \(nd^{2}\). Such bounds are obtained similarly as in Theorem 2 and we also denote by \(\phi\) the golden ratio \(\phi=1.618\ldots\).

1. If \(nd^{2}<\frac{1}{j^{2}}\Rightarrow\sqrt{nd^{2}}\leq\alpha\leq\sqrt{nd^{2}}\phi\leq \frac{\phi}{j}\leq 1\) because \[\alpha=nd^{2}\frac{\sqrt{1+\frac{4}{nd^{2}}}+1}{2}=\sqrt{nd^{2}}\frac{\sqrt{nd^{2 }+4}+\sqrt{nd^{2}}}{2}\] then we have that \(n_{j}^{\Rightarrow k}\) satisfies \[n_{j}^{=k}\geq n_{j}^{\rightharpoonup}+n\frac{1}{\alpha}\frac{\alpha(2(k-j))}{2 +\alpha 2(k-j)}=n_{j}^{\rightharpoonup}+n\frac{k-j}{1+\alpha(k-j)}\geq n_{j}^{ \rightharpoonup}+n\frac{k-j}{1+\frac{\phi}{j}(k-j)}\] where the first inequality follows because \((1+\alpha)^{2(k-j)-1}\geq 1+\alpha(2(k-j)-1)\) and the second inequality is obtained using \(\alpha\leq\frac{\phi}{j}\).
2. If \(\frac{1}{j^{2}}\leq nd^{2}<1\Rightarrow\frac{1}{j}\leq\sqrt{nd^{2}}\leq \alpha\leq\sqrt{nd^{2}}\phi\) because \[\alpha=nd^{2}\frac{\sqrt{1+\frac{4}{nd^{2}}}+1}{2}=\sqrt{nd^{2}}\frac{\sqrt{ nd^{2}+4}+\sqrt{nd^{2}}}{2}\] then we have that \(n_{j}^{\Rightarrow k}\) satisfies \[n_{j}^{=k}\geq n_{j}^{\rightharpoonup}\frac{n}{\alpha}\frac{(1+\alpha)^{2(k-j )}-1}{(1+\alpha)^{2(k-j)}+1}\geq n_{j}^{\rightharpoonup}\frac{n}{\alpha} \frac{(1+\sqrt{nd^{2}})^{2(k-j)}-1}{(1+\sqrt{nd^{2}})^{2(k-j)}+1}\] where the second inequality follows because the ESS is monotonically increasing for \(\alpha\) and \(\alpha\geq nd^{2}\). Since \((1+\sqrt{nd^{2}})^{2(k-j)}\geq 1+2\sqrt{nd^{2}}(k-j)\) and \(k-j\geq 1\), we have that \[n_{j}^{=k}\geq n_{j}^{\rightharpoonup}+\frac{n}{\alpha}\frac{\sqrt{nd^{2}}}{1+ \sqrt{nd^{2}}}\geq n_{j}^{\rightharpoonup}+n\frac{1}{\phi}\frac{1}{1+\sqrt{nd^ {2}}}\] because \(\alpha\leq\sqrt{nd^{2}}\phi\).
3. If \(nd^{2}\geq 1\,\Rightarrow 1\leq nd^{2}\leq\alpha\leq nd^{2}\phi\) because \(\alpha=nd^{2}\frac{\sqrt{1+\frac{4}{nd^{2}}}+1}{2}\), then we have that \(n_{j}^{\stackrel{{{\rightleftarrows}}}{{=}}k}\) satisfies \[n_{j}^{=k}\geq n_{j}^{\rightharpoonup}+n\frac{1}{\alpha}\frac{2^{2(k-j)}-1}{2 ^{2(k-j)}+1}\geq n_{j}^{\rightharpoonup}+n\frac{1}{nd^{2}}\frac{1}{\phi}\frac{ 3}{5}\] where the first inequality follows because the ESS is monotonically increasing for \(\alpha\) and \(\alpha\geq 1\) and the second inequality is obtained using \(k-j\geq 1\) and \(\alpha\leq nd^{2}\phi\).

## Appendix H Additional numerical results and implementation details

In this section we describe the datasets used for the numerical results in Section 5, we provide further details for the numerical experimentations carried out, and include several additional results. Specifically, in the first set of additional results, we evaluate the classification performance of the proposed method in comparison with state-of-the-art techniques for different sample sizes; in the second set of additional results, we further show the performance improvement leveraging information from all the tasks in the sequence with additional datasets; in the third set of additional results, we show the classification error and the running time of IMRCs for different hyper-parameter values; and in the fourth set of additional results, we evaluate the assumption of change between tasks being independent and zero-mean. In addition, in the folder Implementation_IMRC in the supplementary materials we provide the code of the proposed IMRCs with the setting used in the numerical results.

In Section 5, we use \(12\) publicly available datasets [1, 37, 36, 39, 10, 40, 41, 2], and http://yann.lecun.com/exdb/mnist/. The summary of these datasets is provided in Table 3 that shows the number of classes, the number of samples, and the number of tasks. In the following, we further describe the tasks and the time-dependency of each dataset used.

* The "Yearbook" dataset contains portraits' photographs over time and the goal is to predict males and females. Each task corresponds to portraits from one year from 1905 to 2013.
* The "ImageNet noise" dataset contains images with increasing noise over tasks and the goal is to predict if an image is a bird or a snake. The sequence of tasks corresponds to the noise factors \([0.0,0.4,0.8,1.2,1.6,2.0,2.4,2.8,3.2,3.6]\)[35].
* The "DomainNet" dataset contains six different domains with decreasing realism and the goal is to predict if an image is an airplane, bus, ambulance, or police car. The sequence of tasks corresponds to the six domains: real, painting, infograph, clipart, sketch, and quickdraw.
* The "UTKFaces" dataset contains face images in the wild with increasing age and the goal is to predict males and females. The sequence of tasks corresponds to face images with different ages from 0 to 116 years.
* The "Rotated MNIST" dataset contains rotated images with increasing angles over tasks and the goal is to predict if the number in an image is greater than \(5\) or not. Each \(j\)-th task corresponds to a rotation angle randomly selected from \(\left[\frac{180(j-1)}{k},\frac{180j}{k}\right]\) degrees where \(j\in\{1,2,\ldots,k\}\) and \(k\) is the number of tasks.
* The "CLEAR" dataset contains images with a natural temporal evolution of visual concepts and the goal is to predict if an image is soccer, hockey, or racing. Each task corresponds to one year from 2004 to 2014.
* The "Power Supply" dataset contains three year power supply records from 1995 to 1998 and the goal is to predict which hour the current power supply belongs to. We relabel into binary classification according to pm. or am. as in [10].
* The "Usenet" dataset is splitted into Usenet1 and Usenet2 which both contains a stream of messages from different \(20\) newsgroups that are sequentially presented to a user and the goal is to predict the personal interests.
* The "German" dataset contains information about people who take a credit by a bank and the goal is to classify each person as good or bad credit risks.
* The "Spam" dataset contains emails and the task is to predict if an email is malicious spam email or legitimate email.
* The "Covertype" dataset contains cartographic variables of a forest area obtained from US Forest Service (USFS) and the goal is to predict the forest cover type.

The samples in each task are randomly splitted in 100 samples for test and the rest of the samples for training. The samples used for training in the numerical results are randomly sampled from each group of training samples in each repetition.

In Section 5, we compare the results of IMRC methods with \(7\) state-of-the-art-techniques [5, 17, 4, 6, 9, 10, 16]. In the following, we briefly describe each method used.

* GEM method [5] is a technique developed for continual learning. The method provided by Lopez-Paz & Ranzato learns each new task using a stochastic gradient descent with inequality constraints given by the losses of preceding tasks. Such constraints avoid the increase of the loss of each preceding tasks.
* MER method [17] is a technique developed for continual learning. The method provided by Riemer et al. learns each new task using sample sample sets that include random samples of preceding tasks. Such samples of preceding tasks are stored in a memory buffer.
* ELLA method [4] is a techniques developed for continual learning. The method provided by Ruvolo & Eaton learns each new task transferring knowledge from a shared basis of task models.
* EWC method [6] is a technique developed for continual learning. The method provided by Kirkpatrick et al. learns each new task regularizing the loss with regularization parameters given by the Fisher information.
* Condor method [9] is a technique developed for concept drift adaptation. The method provided by Zhao et al. is an ensemble method that adapts to evolving tasks by learning weighting the models in the ensemble at each time step.

* DriftSurf method [10] is a technique developed for concept drift adaptation. The method provided by Tahmasbi et al. adapts to evolving tasks by using a drift detection method. Such method allows to restart a new model when a change in the distribution is detected.
* AUE method [16] is a technique developed for concept drift adaptation. The method provided by Brzezinski & Stefanowski is an ensemble method that adapts to evolving tasks by incrementally updating all classifiers in the ensemble and weighting them with non-linear error functions.

The classifier parameters in the numerical results are obtained using an accelerated subgradient method based on Nesterov approach [32; 33]. Such subgradient method applied to optimization (3) obtains at each step classifier parameters \(\boldsymbol{\mu}\) from the mean and confidence vectors \(\boldsymbol{\tau},\boldsymbol{\lambda}\) using the iterations for \(l~{}=~{}1,2,\ldots,K\)

\[\bar{\boldsymbol{\mu}}(l+1) =\boldsymbol{\mu}(l)+a_{l}\Big{(}\boldsymbol{\tau}-\partial\varphi (\boldsymbol{\mu}(l))-\boldsymbol{\lambda}\text{sign}(\boldsymbol{\mu}(l)) \Big{)}\] (22) \[\boldsymbol{\mu}(l+1) =\bar{\boldsymbol{\mu}}(l+1)+\theta_{l+1}(\theta_{l}^{-1}-1)\left( \boldsymbol{\mu}(l)-\bar{\boldsymbol{\mu}}(l)\right)\]

where \(\text{sign}(\cdot)\) denotes the sign function, \(\boldsymbol{\mu}(l)\) is the \(l\)-th iterate for \(\boldsymbol{\mu}\), \(\theta_{l}=2/(l+1)\) and \(a_{l}~{}=~{}1/(l~{}+~{}1)^{3/2}\) are the step sizes and \(\partial\varphi(\boldsymbol{\mu}(l))\) denotes a subgradient of \(\varphi(\cdot)\) at \(\boldsymbol{\mu}(l)\) with

\[\varphi(\boldsymbol{\mu})=\max_{x\in\mathcal{X},\mathcal{C}\subseteq\mathcal{ Y}}\frac{\sum_{y\in\mathcal{C}}\Phi(x,y)^{\text{T}}\boldsymbol{\mu}-1}{| \mathcal{C}|}.\]

In addition, the above subgradient method is implemented using \(K=2000\) iterations and a warm-start that initializes the classifier parameters in (22) with the solution obtained for the closest task.

\begin{table}
\begin{tabular}{l c c c} \hline \hline Dataset & Classes & Samples & Tasks \\ \hline Yearbook [1] & 2 & 37,921 & 126 \\ ImageNet Noise [35] & 2 & 12,000 & 10 \\ DomainNet [36] & 4 & 6,256 & 6 \\ UTKFace [37] & 2 & 23,500 & 94 \\ Rotated MNIST [38] & 2 & 70,000 & 60 \\ CLEAR [39] & 3 & 10,490 & 10 \\ Power Supply [10] & 2 & 29,928 & 99 \\ Usenet1 [40] & 2 & 1,500 & 5 \\ Usenet2 [40] & 2 & 1,500 & 5 \\ German [41] & 2 & 1,000 & 3 \\ Spam [2] & 2 & 6,213 & 20 \\ Covertype [10] & 2 & 581,012 & 1,936 \\ \hline \hline \end{tabular}
\end{table}
Table 3: Datasets characteristics.

Figure 5: Forward and backward learning can sharply boost performance and ESS as tasks arrive.

\begin{table}
\begin{tabular}{c c c c c c c c c} \hline Method & GEM & MER & ELLA & EWC & Condor & DriftSur & AUE & IMRC \\ \hline Yearbook &.16 \(\pm\).03 &.10 \(\pm\).01 &.43 \(\pm\).08 &.22 \(\pm\).02 &.09 \(\pm\).01 &.23 \(\pm\).01 & **.08**\(\pm\).01 \\ I. noise &.12 \(\pm\).03 & **.07**\(\pm\).01 &.47 \(\pm\).04 &.45 \(\pm\).07 &.09 \(\pm\).01 &.48 \(\pm\).02 &.48 \(\pm\).02 &.08 \(\pm\).01 \\ DomainNet &.49 \(\pm\).10 &.28 \(\pm\).02 &.67 \(\pm\).05 &.74 \(\pm\).05 &.43 \(\pm\).01 &.32 \(\pm\).00 &.32 \(\pm\).00 & **.27**\(\pm\).02 \\ UTKFaces &.12 \(\pm\).00 &.11 \(\pm\).01 &.17 \(\pm\).11 &.12 \(\pm\).00 & **.10**\(\pm\).01 &.12 \(\pm\).00 &.12 \(\pm\).00 & **.10**\(\pm\).00 \\ R. MNIST &.27 \(\pm\).01 &.47 \(\pm\).05 &.47 \(\pm\).05 &.38 \(\pm\).01 &.41 \(\pm\).02 &.48 \(\pm\).02 & **.20**\(\pm\).01 \\ CLEAR &.08 \(\pm\).01 &.05 \(\pm\).02 &.60 \(\pm\).04 &.60 \(\pm\).04 &.36 \(\pm\).01 &.33 \(\pm\).00 &.33 \(\pm\).00 & **.04**\(\pm\).01 \\ P. Supply &.47 \(\pm\).01 &.48 \(\pm\).02 &.30 \(\pm\).01 &.47 \(\pm\).01 &.36 \(\pm\).01 &.36 \(\pm\).00 &.36 \(\pm\).01 & **.24**\(\pm\).00 \\ Usenet1 &.47 \(\pm\).02 &.41 \(\pm\).03 & **.29**\(\pm\).01 &.48 \(\pm\).01 &.43 \(\pm\).02 &.39 \(\pm\).00 &.39 \(\pm\).01 & **.29**\(\pm\).01 \\ Usenet2 &.34 \(\pm\).00 &.29 \(\pm\).00 & **.26**\(\pm\).01 &.32 \(\pm\).01 &.32 \(\pm\).02 & **.26**\(\pm\).00 &.28 \(\pm\).00 &.31 \(\pm\).00 \\ German &.34 \(\pm\).01 &.30 \(\pm\).00 & **.25**\(\pm\).01 &.30 \(\pm\).02 &.31 \(\pm\).01 &.29 \(\pm\).01 &.29 \(\pm\).01 &.28 \(\pm\).01 \\ Spam & **.07**\(\pm\).01 & **.07**\(\pm\).01 &.23 \(\pm\).03 &.13 \(\pm\).02 &.11 \(\pm\).02 &.32 \(\pm\).01 &.33 \(\pm\).01 & **.07**\(\pm\).00 \\ Covert. &.08 \(\pm\).00 &.08 \(\pm\).00 &.10 \(\pm\).01 &.08 \(\pm\).00 & **.07**\(\pm\).02 &.09 \(\pm\).00 &.09 \(\pm\).00 &.08 \(\pm\).00 \\ \hline \end{tabular}
\end{table}
Table 6: Classification error and standard deviation of the proposed IMRC method in comparison with the existing techniques for \(n=150\) samples per task.

\begin{table}
\begin{tabular}{c c c c c c c c} \hline Method & GEM & MER & ELLA & EWC & Condor & DriftSur & AUE & IMRC \\ \hline Yearbook &.16 \(\pm\).02 &.11 \(\pm\).01 &.43 \(\pm\).10 &.38 \(\pm\).02 &.11 \(\pm\).02 &.29 \(\pm\).02 &.29 \(\pm\).02 & **.10**\(\pm\).02 \\ I. noise &.16 \(\pm\).06 & **.10**\(\pm\).01 &.47 \(\pm\).05 &.47 \(\pm\).05 & **.10**\(\pm\).02 &.48 \(\pm\).02 & **.48**\(\pm\).02 & **.10**\(\pm\).02 \\ DomainNet &.65 \(\pm\).05 &.29 \(\pm\).02 &.67 \(\pm\).05 &.75 \(\pm\).05 &.45 \(\pm\).01 &.32 \(\pm\).00 &.32 \(\pm\).00 & **.28**\(\pm\).03 \\ UTFKaces &.12 \(\pm\).00 &.11 \(\pm\).09 &.18 \(\pm\).11 &.12 \(\pm\).00 &.14 \(\pm\).02 &.12 \(\pm\).00 &.12 \(\pm\).00 & **.10**\(\pm\).00 \\ R. MNIST &.29 \(\pm\).04 &.41 \(\pm\).13 &.48 \(\pm\).05 &.44 \(\pm\).01 &.43 \(\pm\).01 &.48 \(\pm\).02 & 48 \(\pm\).02 & **.25**\(\pm\).02 \\ CLEAR &.

In the **first set of additional results**, we further compare the classification error of IMRCs with the state-of-the-art techniques. The results in Table 1 in the paper as well as Tables 4, 5, and 6 are obtained computing the classification error \(50\) times for each sample size. Table 1 in the paper shows classification errors for \(n=10\) samples, while Tables 4, 5, and 6 show the classification error for \(n~{}=~{}50\), \(n=100\), and \(n=150\) samples, respectively. As can be observed in Table 4, the performance improvement of IMRCs in comparison with the state-of-the-art techniques for \(n~{}=~{}50,n=100\), and \(n=150\) is similar to that shown in the paper for \(n=10\).

In the **second set of additional results**, we further illustrate the relationship among classification error, number of tasks, and sample size. Figure 4 in the paper as well as Figure 5 are obtained computing the classification error over all the sequences of consecutive tasks of length \(k\) in the dataset. Then, we repeat such experiment \(10\) times with randomly chosen training sets of size \(n\). Figure 5 extends the results for IMRCs using "DomainNet" dataset completing those in the main paper that show the results using "Yearbook" dataset. Figure 4(a) shows the classification error of IMRC method divided by the classification error of single-task learning for different number of tasks with \(n=10\) and \(n=100\) sample sizes. In addition, Figure 4(b) shows the classification error of IMRC method for different sample sizes with \(k=10\) tasks. Figures 4(a) and 4(b) show similar behavior to Figures 5(a) and 5(b) in the paper, respectively. In addition, Figure 6 shows the classification error of IMRCs per step and task with single-task learning, forward learning, and forward and backward learning using the "Yearbook" dataset. Such figure shows that forward and backward learning can improve performance of preceding tasks, while forward learning and single task learning maintain the same performance over time.

In the **third set of additional results**, we further assess the change in classification error and the running time of IMRCs varying the hyper-parameters. Table 7 shows the classification error of IMRCs varying the values of hyper-parameter for the window size \(W\) and the number of backward steps \(b\), completing those in the paper that show the results for \(W=2\) and \(b=3\). As shown in the table, the proposed IMRCs do not require a careful fine-tuning of hyper-parameters and similar performances are obtained by using different values. In addition, Table 8 shows the mean running

\begin{table}
\begin{tabular}{l c c c c c c c c c c c c c c} \hline \hline Hyper-parameter & \(W=2\) & \(W=4\) & \(W=6\) & \(b=1\) & \(b=2\) & \(b=3\) & \(b=4\) & \(b=5\) \\ \hline Sample size \(n\) & 10 & 100 & 10 & 100 & 10 & 100 & 100 & 100 & 100 & 100 & 100 & 100 & 100 & 100 \\ \hline Yearbook &.13 &.08 &.13 &.09 &.13 &.09 &.14 &.10 &.09 &.14 &.13 &.08 &.13 &.08 \\ ImageNet noise &.15 &.09 &.15 &.09 &.15 &.09 &.15 &.09 &.15 &.09 &.15 &.09 &.15 &.08 &.15 &.08 \\ DomainNet &.34 &.28 &.32 &.27 &.33 &.28 &.36 &.29 &.35 &.28 &.34 &.28 &.34 &.28 &.34 &.28 \\ UTKFaces &.10 &.10 &.10 &.10 &.10 &.10 &.10 &.10 &.10 &.10 &.10 &.10 &.10 &.10 &.10 &.10 \\ Rotated MNIST &.36 &.21 &.35 &.21 &.36 &.21 &.36 &.22 &.36 &.22 &.36 &.21 &.36 &.21 \\ CLEAR &.09 &.05 &.09 &.05 &.09 &.06 &.10 &.05 &.09 &.05 &.09 &.05 &.09 &.05 &.08 &.05 \\ Weather &.31 &.31 &.31 &.31 &.31 &.31 &.31 &.31 &.31 &.31 &.31 &.31 &.31 &.31 &.31 \\ Power Supply &.30 &.25 &.30 &.25 &.30 &.25 &.32 &.27 &.31 &.26 &.30 &.25 &.30 &.25 &.30 &.25 \\ Usenet1 &.32 &.30 &.33 &.31 &.33 &.31 &.32 &.30 &.32 &.30 &.32 &.30 &.32 &.30 \\ Usenet2 &.33 &.30 &.34 &.31 &.34 &.31 &.34 &.30 &.33 &.30 &.33 &.30 &.32 &.30 \\ German &.34 &.29 &.34 &.29 &.34 &.29 &.34 &.29 &.34 &.29 &.34 &.29 &.34 &.29 \\ Spam &.13 &.08 &.14 &.08 &.13 &.08 &.14 &.09 &.13 &.08 &.13 &.08 &.12 &.07 &.12 &.07 \\ Covertype &.08 &.08 &.08 &.08 &.08 &.08 &.08 &.08 &.08 &.08 &.08 &.08 &.08 &.08 &.08 \\ \hline \hline \end{tabular}
\end{table}
Table 7: Classification error of the proposed IMRC method varying \(W\) and \(b\).

Figure 6: Forward and backward learning can improve performance of preceding tasks.

[MISSING_PAGE_FAIL:25]