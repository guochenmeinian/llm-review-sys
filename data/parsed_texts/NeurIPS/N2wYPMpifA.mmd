Understanding Scaling Laws with Statistical and Approximation Theory for Transformer Neural Networks on Intrinsically Low-dimensional Data

Alex Havrilla

Department of Mathematics

Georgia Institute of Technology

Atlanta, GA, 30308

ahavrilla3@gatech.edu

&Wenjing Liao

Department of Mathematics

Georgia Institute of Technology

Atlanta, GA, 30308

wliao60@gatech.edu

###### Abstract

When training deep neural networks, a model's generalization error is often observed to follow a power scaling law dependent both on the model size and the data size. Perhaps the best known example of such scaling laws are for transformer-based large language models (**LLMs**), where networks with billions of parameters are trained on trillions of tokens of text. Yet, despite sustained widespread interest, a rigorous understanding of why transformer scaling laws exist is still missing. To answer this question, we establish novel statistical estimation and mathematical approximation theories for transformers when the input data are concentrated on a low-dimensional manifold. Our theory predicts a power law between the generalization error and both the training data size and the network size for transformers, where the power depends on the intrinsic dimension \(d\) of the training data. Notably, the constructed model architecture is shallow, requiring only logarithmic depth in \(d\). By leveraging low-dimensional data structures under a manifold hypothesis, we are able to explain transformer scaling laws in a way which respects the data geometry. Moreover, we test our theory with empirical observation by training LLMs on natural language datasets. We find the observed empirical scaling laws closely agree with our theoretical predictions. Taken together, these results rigorously show the intrinsic dimension of data to be a crucial quantity affecting transformer scaling laws in both theory and practice.

## 1 Introduction

Deep learning has made remarkable breakthroughs in various real-world applications, such as natural language processing (Graves et al., 2013; Bahdanau et al., 2014; Liu et al., 2023; Vaswani et al., 2017), computer vision (Krizhevsky et al., 2012; Goodfellow et al., 2014; Song et al., 2020), healthcare (Miotto et al., 2018), and robotics (Gu et al., 2017). A neural scaling law between the generalization error (or test loss) and several quantities, including the model size, the training data size, and the amount of compute, plays a key role in the performance of neural networks. Perhaps the best known example of such scaling laws are for transformer-based LLMs. Recent works in Hestness et al. (2017); Rosenfeld et al. (2019); Kaplan et al. (2020); Bahri et al. (2021) demonstrated a power law between the test loss and the network size, the training data size, and the amount of compute for transformer-based LLMs. Yet, despite sustained widespread interest, a rigorous understanding of why _transformer scaling laws_ exist is still missing.

Understanding the theory behind neural scaling laws provides invaluable insights into practical applications of deep learning. A mathematical principal of neural scaling laws enables researchers and practitioners to describe and analyze the performance of neural networks with precision and rigor. The neural scaling law between the generalization error and the network size can be partially explained via neural network representation theory (Yarotsky, 2016). Further, the neural scaling law between the generalization error and the training data size \(n\) can be explained via statistical estimation theory. For feedforward neural networks (Schmidt-Hieber, 2020) and convolutional residual networks (Oono and Suzuki, 2019), a generalization error bound has been established for regression. Schmidt-Hieber (2020); Oono and Suzuki (2019) predicted Generalization Error \(\sim n^{-c/D}\) where \(n\) is the training data size, \(D\) is the data dimension and \(c\) is a constant. This predicted rate of convergence is extremely slow for high dimensional data when \(D\) is large, while the rate of convergence observed in real-world applications is significantly faster, which reveals a gap between theory and practice.

This gap can be bridged by exploiting low-dimensional structures of data. Real-world data sets often exhibit low-dimensional geometric structures due to rich local regularities, global symmetries, or repetitive patterns (Tenenbaum et al., 2000; Roweis and Saul, 2000). According to Min et al. (2023, Figure 1), the intrinsic dimension of CIFAR-100, CelebA and ImageNet datasets are about \(20,20\) and \(40\) respectively. When the low-dimensional geometric structure of data is modeled by a manifold, the predicted scaling for regression, classification and distribution estimation becomes Generalization Error \(\sim n^{-c/d}\), where \(n\) is the training data size, \(d\) is the intrinsic dimension of the data manifold, and \(c\) is a constant (Chen et al., 2022; Liu et al., 2021; Dahal et al., 2022; Nakada and Imaizumi, 2020). In Sharma and Kaplan (2022), the neural scaling law between the test loss and the network size was predicted to be Test loss \(\sim(size)^{-4/d}\) where \(d\) is the intrinsic dimension of data. While the theoretical studies focus on feedforward neural networks (Chen et al., 2022; Nakada and Imaizumi, 2020) and convolutional residual networks (Liu et al., 2021), a generalization to transformer-based neural networks (Vaswani et al., 2017) is of great interest but widely open.

This paper establishes mathematical approximation and statistical estimation theories to predict and justify the scaling law between the generalization error and the model/data size for transformer neural networks. We consider regression of a \(\beta\)-_Holder continuous function_\(f:\mathcal{M}\rightarrow\mathbb{R}\) where \(\mathcal{M}\) is a \(d\)-dimensional compact Riemannian manifold isometrically embedded in \(\mathbb{R}^{D}\). After embedding the input \(x\in\mathcal{M}\subset\mathbb{R}^{D}\) to a proper sequence, we apply a transformer network on the embedded sequence to learn the function \(f\). Our main results are on the statistical estimation and universal approximation theories of Holder continuous functions on \(\mathcal{M}\) by transformer neural networks.

**Statistical Theory:** In Theorem 1, we consider the global empirical risk minimizer \(\hat{\Upsilon}_{n}\) from \(n\) i.i.d. training data \(\{(x_{i},f(x_{i}))\}_{i=1}^{n}\), given by

\[\hat{\Upsilon}_{n}=\arg\min_{\mathds{r}\in\mathcal{T}}\frac{1}{n}\sum_{i=1}^{ n}\big{(}\mathds{T}(x_{i})-f(x_{i})\big{)}^{2},\] (1)

under a properly chosen transformer network architecture \(\mathcal{T}\). We prove that, the generalization error of \(\hat{\Upsilon}_{n}\) satisfies

\[\mathbb{E}\|\hat{\Upsilon}_{n}-f\|_{L^{2}(Q)}^{2}\leq\tilde{O}\big{(}Dd^{2}n^{ -\frac{2\beta}{2\beta+d}}\big{)}\] (2)

where \(Q\) denotes the distribution of \(x\), and \(\tilde{O}\) hides constants and \(\log n\) terms.

**Approximation Theory:** In Theorem 2, we construct a transformer network to universally approximate \(\beta\)-Holder continuous functions on \(\mathcal{M}\) with an arbitrarily given accuracy \(\varepsilon\). Notably, the network is shallow, requiring only \(O\big{(}\log(d)\big{)}\) independent of the desired accuracy \(\epsilon\) to approximate \(f\) locally. This highlights a major advantage of Transformers over feed-forward ReLU networks, which require \(O\big{(}\log(\frac{1}{\epsilon})\big{)}\) layers to achieve the same accuracy.

In our proof, we embed the entries of \(x=[x^{1},\dots,x^{D}]\in\mathcal{M}\) into tokens such that the \(x^{i}\)'s appear in a sequence. Our proof for the approximation theory explicitly constructs transformers to realize the interaction between different tokens efficiently via a crucial _Interaction Lemma_3. This lemma allows us to flexibly implement many common operations including addition, multiplication, and parallelization, and so may of independent interest. In our proof for the statistical theory, we calculate the covering number of our transformer network class, which is also of independent interest.

**Neural Scaling Laws and the Intrinsic Dimension:** Our generalization error bound in (2) predicts the following neural scaling law between the generalization error and the data size \(n\):

\[\text{Squared Generalization Error}:=\mathbb{E}\|\hat{\Upsilon}_{n}-f\|_{L^ {2}(Q)}^{2}\lesssim n^{-\alpha_{D}},\ \ \text{where}\ \alpha_{D}=\frac{2\beta}{2\beta+d},\] (3)with sufficient data. Our approximation theory in Theorem 2 predicts the following neural scaling law between the approximation error and the network size \(N\):

\[\text{Squared Approximation Error}:=\inf_{\mathds{T}\in\mathcal{T}}\|\mathds{T}-f\|_{L ^{\infty}(\mathcal{M})}^{2}\lesssim N^{-\alpha_{N}},\ \ \text{where}\ \alpha_{N}=\tfrac{2\beta}{d}\] (4)

for a sufficiently large network class \(\mathcal{T}\). Our prediction of the power scaling law is consistent with our own empirical observations, and those in Kaplan et al. (2020) and Biderman et al. (2023). More importantly, our theory quantifies the power \(\alpha_{D},\alpha_{N}\) in terms of the intrinsic dimension of data.

**Experimental Validation on LLMs:** After establishing our theory we seek to validate it in practice by predicting empirical scaling laws for LLMs trained on natural language data. To test our predictions for the data scaling law, we pretrain a series of small (125 million parameter) LLMs on three datasets (Gokaslan et al., 2019; Eldan and Li, 2023; Kocetkov et al., 2022). We find close agreement (\(\pm 0.02\)) between our predicted scaling exponent \(\alpha_{D}\) and the observed exponents \(\hat{\alpha}_{D}\). To evaluate our predictions for the model scaling exponent \(\alpha_{N}\), we rely on publicly available scaling suites (Biderman et al., 2023; Radford et al., 2019) whose intrinsic data dimensions we can estimate. We find our predictions are still close but less accurate for \(\alpha_{N}\). Finally, we carry out a series of ablations investigating factors impacting the estimated intrinsic data dimension \(d\). For a fixed dataset, we find the estimated \(d\) is stable with respect to several factors including the model size, model embedding dimension, and context length1.

Footnote 1: Code is available at https://github.com/Dahoas/transformer_manifolds_learning

In summary, we make the following contributions:

* A novel approximation theory for transformers approximating Holder continuous functions on a \(d\)-dimensional manifold, requiring \(O(\log(d))\) depth independent of the accuracy \(\epsilon\).
* A novel computation of the covering number of our transformer network class. This is used to establish generalization bounds exponentially depending on the intrinsic dimension \(d\).
* Empirical experiments demonstrating our theory predicts data scaling laws for LLMs as a function of the estimated intrinsic data dimension \(d\).
* An empirical study of several factors affecting the estimated intrinsic data dimension for transformers including model size, embedding dimension, layer depth, and context length.

We will present our main theory in Section 2, numerical validation of our theory and the prediction of neural scaling laws in Section 3. We will discuss related work in Section 4 and conclude our paper in Section 5. Our pre-training hyperparameters are given in Appendix A. The derivation of neural scaling laws is presented in Appendix B. Our notation is given in Appendix C, and proofs are presented in Appendix E and F.

## 2 Transformer Generalization and Approximation Theory

This paper establishes statistical estimation and mathematical approximation theory of transformers for the regression of Holder functions on a low-dimensional manifold. We start by defining transformer neural networks.

### Transformer Neural Networks

**Definition 1** (Transformer Neural Network).: _We define a transformer neural network \(\mathds{T}\) as a composition of functions of the form_

\[\mathds{T}(x)=\mathds{D}\circ\mathds{B}_{L_{T}}\circ...\circ\mathds{B}_{1} \circ(\mathds{PE}+\mathds{E}(x))\] (5)

_which is parameterized by_

* \(L_{T}\)_: The number of transformer blocks_ \(\mathds{B}_{i}\) _in_ \(\mathds{T}\)_._
* \(m\)_: The maximum number of attention heads per transformer block._
* \(L_{\text{FFN}}\)_: The max depth of the feed-forward layers per block._
* \(w_{\text{FFN}}\)_: The max width of the feed-forward layers per block._

[MISSING_PAGE_FAIL:4]

a class of a transformer neural networks \(\mathcal{T}(L_{T},L_{\text{FFN}},w_{\text{FFN}},l,d_{embbd},m,R,\kappa)\). The corresponding generalization error is given by

\[\mathbb{E}\|\hat{\Upsilon}_{n}-f\|_{L^{2}(Q)}=\mathbb{E}\sqrt{\int_{\mathcal{M} }\big{(}\hat{\Upsilon}_{n}(x)-f(x)\big{)}^{2}dQ(x)}.\] (6)

If \(\mathcal{M}\) and \(f\) satisfy Assumptions 1 and 2, we prove the following generalization error bound.

**Theorem 1**.: _Let \(M,\tau,R,H_{f}>0\), \(0<\beta\leq 1\), \(d,D\in\mathbb{N}\), \(\mathcal{M}\) and \(f\) satisfy Assumption 1 and 2 respectively. Given \(n\) training samples \(\{(x_{i},f(x_{i}))\}_{i=1}^{n}\) where \(\{x_{i}\}_{i=1}^{n}\) are i.i.d. samples of a distribution \(Q\) supported on \(\mathcal{M}\), if we use the transformer neural network class \(\mathcal{T}(L_{T},L_{\text{FFN}},w_{\text{FFN}},l,d_{embbd},m,R,\kappa)\) with parameters_

\[L_{T}=O\big{(}\log(d)\big{)},\quad L_{\text{FFN}}=O\big{(}\log(n )\big{)},\quad w_{\text{FFN}}=O\big{(}1\big{)},\quad l=O\big{(}dn^{\frac{d}{ 2\beta+d}}\big{)}\] \[d_{embbd}=O\big{(}1\big{)},\quad m=O\big{(}dn^{\frac{d}{2\beta+ d}}\big{)},\quad\kappa=O\big{(}d^{2}n^{\frac{2d}{2\beta+d}}\big{)}\]

_in the empirical risk minimization (1), where \(O\big{(}\ \cdot\ \big{)}\) hides terms in \(C_{\mathcal{M}}\) (the number of charts), \(D,H_{f},M\), then the empirical risk minimizer \(\hat{\Upsilon}_{n}\) given by (1) satisfies_

\[\mathbb{E}\int_{\mathcal{M}}\big{(}\hat{\Upsilon}_{n}(x)-f(x)\big{)}^{2}dQ \leq\tilde{O}\big{(}Dd^{2}n^{\frac{-2\beta}{2\beta+d}}\big{)}\]

_where \(\tilde{O}\) hides logarithmic terms in \(n,d\) and linear terms in \(C_{\mathcal{M}}\)._

Theorem 1 is proved in Appendix F, via a bias-variance decomposition. The bias represents the approximation error of \(f\) by transformer neural networks, and the variance represents the stochastic error in the parameter estimation of transformer neural networks. To quantify the bias, we explicitly construct a transformer neural network to universally approximate \(\beta\)-Holder continuous functions on \(\mathcal{M}\), to be detailed in Section 2.4. The variance is bounded by a novel calculation of the covering number of the transformer network class used in Theorem 1.

### Transformer Approximation Theory

**Theorem 2**.: _Let \(M,\tau,R,H_{f}>0\), \(0<\beta\leq 1\), \(d,D\in\mathbb{N}\) and \(\mathcal{M}\) satisfy Assumption 1. For any \(\epsilon\in(0,1)\), there exists a transformer neural network class \(\mathcal{T}(L_{T},L_{\text{FFN}},w_{\text{FFN}},l,d_{embbd},m,R,\kappa)\) with parameters_

\[L_{T}=O\big{(}\log(d)\big{)},\quad L_{\text{FFN}}=O\big{(}\log( \epsilon^{-1})\big{)},\quad w_{\text{FFN}}=O\big{(}1\big{)},\quad l=O\big{(}d \epsilon^{-\frac{d}{\beta}}\big{)}\] \[d_{embbd}=O\big{(}1\big{)},\quad m=O\big{(}d\epsilon^{-\frac{d}{ \beta}}\big{)},\quad\kappa=O\big{(}d^{2}\epsilon^{-\frac{2d}{\beta}}\big{)},\]

_where \(O\big{(}\ \cdot\ \big{)}\) hides terms in \(C_{\mathcal{M}},D,H_{f},\tau\), such that for any target function \(f\) satisfying Assumption 2, if the network parameters \(\theta\) are properly chosen, then the network yields a function \(\text{T}_{\theta}\in\mathcal{T}(L_{T},L_{\text{FFN}},w_{\text{FFN}},l,d_{embbd},m,R,\kappa)\) with the approximation error_

\[\|\text{T}_{\theta}-f\|_{L^{\infty}(\mathcal{M})}\leq\epsilon\]

Theorem 2 is proved in Appendix E.2. In our proof, we decompose \(f(x)\) as a sum of terms over local neighborhoods \(U_{1},...,U_{C_{\mathcal{M}}}\subseteq\mathcal{M}\) covering \(\mathcal{M}\). Approximations on overlapping neighborhoods containing \(x\) will then be combined via a partition of unity (**PoU**) \(\{\rho_{n}\}_{n=1}^{C_{\mathcal{M}}}\) which subordinates \(\{U_{n}\}_{n=1}^{C_{\mathcal{M}}}\). This will give us the expression \(f(x)=\sum_{n=1}^{C_{\mathcal{M}}}f_{n}(x)\mathbf{1}_{U_{n}}(x)\) with \(f_{n}=f\rho_{n}:\mathcal{M}\rightarrow\mathbb{R}\). On each local neighborhood \(U_{n}\), we project the input \(x\in\mathcal{M}\subseteq\mathbb{R}^{D}\) to the tangent coordinate in \([0,1]^{d}\). This will give us the following _local decomposition_ of the target function:

\[f(x)=\sum_{n=1}^{C_{\mathcal{M}}}\tilde{f}_{n}\circ\phi_{n}(x)\mathbf{1}_{U_{n}}(x)\] (7)

where \(\tilde{f}_{n}=f_{n}\circ\phi_{n}^{-1}:[0,1]^{d}\rightarrow\mathbb{R}\) and \(\phi_{n}:\mathcal{M}\rightarrow[0,1]^{d}\) is a projection onto the local tangent space. We then construct transformers to approximate the \(\tilde{f}_{n},\phi_{n},\mathbf{1}_{U_{n}}\) components in (7). A diagram of the constructed transformer network approximating \(f:\mathcal{M}\rightarrow\mathbb{R}\) is given in Figure 1. The following key lemma is used to efficiently approximate each low-dimensional function \(\tilde{f}_{n}\) on \(d\)-dimensional coordinates.

**Lemma 1**.: _Let \(H_{f},R>0\), \(d\in\mathbb{N}\) and \(0<\beta\leq 1\). For any \(\epsilon\in(0,1)\), there exists a transformer neural network class \(\mathcal{T}(L_{T},L_{\text{FFN}},w_{\text{FFN}},l,d_{embd},m,R,\kappa)\) with parameters_

\[L_{T}=O\big{(}\log(d)\big{)},\quad L_{\text{FFN}}=O\big{(}1\big{)},\quad w_{\text{FFN}}=O\big{(}1\big{)},\quad l=O\big{(}d\epsilon^{-\frac{d}{ \beta}}\big{)}\] \[d_{embd}=O\big{(}1\big{)},\quad m=O\big{(}d\epsilon^{-\frac{d}{ \beta}}\big{)},\quad\kappa=O\big{(}d^{2}\epsilon^{-\frac{2d}{\beta}}\big{)},\]

_where \(O\big{(}\cdot\big{)}\) hides terms in \(H_{f}\), such that, for any \(\beta\)-Holder continuous function \(f:[0,1]^{d}\to\mathbb{R}\), with Holder constant no more than \(H_{f}\) and \(\|f\|_{L^{\infty}([0,1]^{d})}\leq R\), if the network parameters \(\theta\) are properly chosen, this transformer network yields a function \(\text{T}_{\theta}\in\mathcal{T}(L_{T},L_{\text{FFN}},w_{\text{FFN}},l,d_{embd },m,R,\kappa)\) such that_

\[\|\text{T}_{\theta}-f\|_{L^{\infty}([0,1]^{d})}\leq\epsilon.\]

Lemma 1 is proved in Appendix E.1. We develop a novel lemma - _Interaction Lemma_3, implementing a highly-sparse pairwise interaction between two arbitrary tokens \(h_{t_{1}},h_{t_{2}}\), as a crucial architectural building block allowing us to easily implement more complex functions, architecture serialization, and parallelization (7). This result highlights a distinct advantage of transformer function approximation over ReLU function approximation (Yarotsky, 2016): **A transformer network only needs a constant \(O\big{(}\log(d)\big{)}\) number of layers to approximate \(f:[0,1]^{d}\to\mathbb{R}\)** independent of the desired accuracy \(\epsilon\). In contrast, the depth of ReLU feed-forward networks is in the order of \(\log(\epsilon^{-1})\)(Yarotsky, 2016) This is desirable from an empirical point of view, where wider networks instead of deeper ones tend to achieve superior performance (Kaplan et al., 2020; Lee et al., 2020).

## 3 Predicting Empirical Scaling Laws and Validation on LLMs

Our theory provides practical insights by predicting neural scaling laws for transformers, as given in (3) and (4), by explicitly quantifying the data scaling exponent \(\alpha_{D}\) and the model scaling exponent \(\alpha_{N}\) as a function of the intrinsic dimension (**ID**) \(d\). If we assume the language modeling objective has Lipschitz regularity such that \(\beta=1\) in Assumption 2, then Theorem 1 predicts the scaling law between the squared generalization error and the data size \(n\), as given in (3), with \(\alpha_{D}=\frac{2}{2+d}\), and the model scaling law with exponent given by \(\alpha_{N}=\frac{2}{d}\). For a full derivation refer to Section B. We will observe how well our theory predicts these exponents both by pretraining small models from scratch and evaluating existing open-source model suites (Biderman et al., 2023).

In the following, we denote \(\alpha_{D}\) and \(\alpha_{N}\) as the scaling exponents predicted by our theory, where we numerically estimate the intrinsic dimension of data, denoted by \(d_{\mathcal{D}}\). The empirical exponents are

Figure 1: Diagram of the transformer architecture constructed in Theorem 2. T computes approximations of \(f(x)\) on each local chart \(U_{n}\subseteq\mathcal{M}\) by first projecting \(x\) to the tangent coordinates in \(\mathbb{R}^{d}\) via \(\phi_{n}(x)\) and then approximating \(f(x)\) with local Taylor polynomials. A shallow sub-network computes indicators \(\mathbf{1}_{U_{n}}\) for each local chart in parallel. The results of the two sub-networks are then multiplied together and summed to produce the final result. Here \(H_{i}\) denotes the embedding matrix before the \(i\)th transformer block \(\text{B}_{i}\).

denoted by \(\hat{\alpha}_{D}\) and \(\hat{\alpha}_{N}\). To obtain the data scaling exponent \(\hat{\alpha}_{D}\), we plot the test loss (comparable to squared error) versus the data size \(n\) in log-log scale, fit the log-log curve with a line, and then obtain \(\hat{\alpha}_{D}\) from the magnitude of the slope. The model scaling exponent \(\hat{\alpha}_{N}\) is obtained similarly.

Estimating the ID of TextTo predict scaling exponents, we must first estimate the intrinsic dimension of our pretraining dataset \(\mathcal{D}\). While we can do this directly for image datasets (Russakovsky et al., 2014; Pope et al., 2021), we cannot do this directly for textual datasets. Instead, we will estimate the intrinsic dimension of the input data by estimating the intrinsic dimension of token embeddings. Specifically, we will represent each input token with its corresponding final-layer token embedding. Given a pretraining test set \(\mathcal{D}_{\text{test}}\) we embed a random \(l=1024\) length subsequence from each document \(D_{k}\in\mathcal{D}_{\text{test}}\). We then randomly sub-sample \(32\) final-layer tokens from the embedded subsequence and shuffle together all the embeddings. To estimate the ID of the embeddings we use the Maximum Likelihood Estimation ID algorithm (Levina and Bickel, 2004; Pedregosa et al., 2011) with \(K=20\) neighbors. We split the sampled embedding tokens into batches of \(4096\) and run the MLE estimator on each batch, averaging together for the final result. Unless otherwise specified, we embed each document \(D_{k}\in\mathcal{D}_{\text{test}}\) using a 125 million parameter model \(M\) with \(L_{T}=12\) layers and embedding dimension \(d_{embd}=768\). We first pretrain \(M\) on the full \(\mathcal{D}_{\text{train}}\) for \(200,000\) steps or until convergence.

Intrinsic dimension predicts the empirical data scaling exponent \(\hat{\alpha}_{D}\)To validate our prediction of the dataset scaling exponent \(\alpha_{D}\) we pretrain a series of 125 million parameter GPT-style LLMs on three different datasets: OpenWebText (Gokaslan et al., 2019), the SQL portion of The Stack (Kocetkov et al., 2022), and Tiny Stories (Eldan and Li, 2023). We train across three orders of dataset size to fit scaling laws. Detailed hyperparameters can be found in the Appendix A. We report the observed scaling laws in log scale in Figure 2. In addition, we plot our predicted test loss whose slope is given by \(\alpha_{D}=\frac{2}{2+d_{\mathcal{D}}}\) where \(d_{\mathcal{D}}\) is the our estimated intrinsic dimension.

Empirically, we find all three datasets produce nearly log-linear laws whose exponents lie between \(0.1<\hat{\alpha}_{D}<0.15\). Tiny Stories has the largest exponent, indicating the fastest rate of convergence, followed by the SQL dataset, followed by OpenWebText. This matches our rough intuition, since Tiny Stories is a synthetically generated dataset with less complexity than the other two datasets and thus easier to learn. The predicted exponents \(\alpha_{D}\) generally over-estimate \(\hat{\alpha}_{D}\) but otherwise closely match up to \(\pm 0.02\) absolute error. Additionally, the predicted \(\alpha_{D}\) reflect the previously mentioned differences in complexity of pretraining datasets. In particular, Tiny Stories has a smaller estimated ID than both OpenWebText and SQL, resulting in a larger predicted \(\alpha_{D}\) as desired.

Predicting empirical model scaling exponent \(\hat{\alpha}_{N}\) with intrinsic dimensionTo validate our predictions of the model scaling exponent \(\alpha_{N}=\frac{2}{d_{\mathcal{D}}}\), we evaluate two model scaling suites: GPT2 (Radford et al., 2019) and Pythia (Biderman et al., 2023). We refer to Kaplan et al. (2020) for GPT2's \(\hat{\alpha}_{N}\) and estimate \(\hat{\alpha}_{N}\) using OpenWebText as a proxy for GPT2's pretraining data. We compute \(\alpha_{N}\) for Pythia by evaluating each model on The Pile's test set (Gao et al., 2021). We also estimate \(d_{\mathcal{D}}\) on the publicly available pretraining data to predict \(\alpha_{N}\). The results are reported in Figure 3. Our predicted \(\alpha_{N}\) under-estimates the empirical \(\hat{\alpha}_{N}\). We conjecture this to be due to a number of factors including possible under-training of the largest models and the intrinsic entropy of the data distribution.

Figure 2: Observed and predicted data scaling laws on OpenWebText, The Stack-SQL, and Tiny Stories pretraining datasets. All estimates are close (\(\pm 0.02\)) and appear to reflect varying levels of pretraining data complexity. **Note:**\(\hat{\alpha}_{D}\) denotes the empirically observed data scaling exponent and \(\alpha_{D}\) denotes the theoretically estimated exponent.

**Ablating the impact of model architecture on the estimated ID**  Practical application of our theory relies on a good estimate of the intrinsic dimension. However, there are many factors potentially biasing our estimate. Of particular interest is the embedding model's embedding dimension, depth, context length, and number of parameters. We ablate these factors in Figure 4, plotting estimated ID against each factor.

Overall, we find the estimated ID is fairly stable across each factor. As the number of parameters increases, the estimated ID of The Pile slightly increases from 15.56, via a 410 million parameter model, to 20.02 with a 12.8 billion parameter model. ID on OpenWebText behaves similarly when increasing the embedding dimension, increasing from 15.56 when \(d_{embd}=768\) to 18.68 when \(d_{embd}=1536\). When fixing a single model, we find the ID across intermediate embedding layers is small initially but then increases and decreases again, stabilizing around the ID of the final layer. We observe that the ID appears to inversely correlate with sequence length, decreasing from \(15.86\) for very short sequences to \(12.9\) for sequences around \(1024\) tokens.

Figure 4: **Top left: Estimated ID vs. number of parameters. Top right: Estimated ID vs. the embedding dimension. Bottom left: Variation of estimated ID across model layers. Bottom right: Variation of estimated ID across context position.**

Figure 3: Observed and predicted model scaling laws in model size on GPT2 and Pythia scaling suites. \(\alpha_{N}\) denotes the empirically observed scaling exponent, and \(\hat{\alpha}_{N}\) denotes the theoretically predicted exponent. Note: we estimate \(\alpha_{N}\) for GPT2 using OpenWebText.

Predicting \(\alpha_{N}\) from \(\alpha_{D}\) (and vice versa) without estimating IDAbove we estimated \(\alpha_{D}\) and \(\alpha_{N}\) by first estimating the intrinsic dimension \(d\) for a model's pretraining dataset. However, estimating \(d\) may not always be possible when pretraining data is not public. Alternatively, we can predict \(\alpha_{D}\) in terms of \(\alpha_{N}\) (and vice versa) without ever needing to estimate \(d\):

\[\alpha_{D}=\frac{2}{2+d}=\frac{2\frac{1}{d}}{2\frac{1}{d}+1}=\frac{\alpha_{N} }{\alpha_{N}+1},\quad\alpha_{N}=\frac{\alpha_{D}}{1-\alpha_{D}}\]

See Table 1 for ID-free estimations of empirically observed exponents in the literature (Sharma and Kaplan, 2022; Hoffmann et al., 2022).

## 4 Related Work

The theoretical properties and advantages of transformers have been studied from many different perspectives (Jelassi et al., 2022; Zhang et al., 2022; Bai et al., 2023; Perez et al., 2021; Sanford et al., 2024). Most related to us are Yun et al. (2019); Edelman et al. (2022); Wei et al. (2022); Takakura and Suzuki (2023) in which transformers were studied from an approximation viewpoint. The work in Yun et al. (2019) proved that transformer models are universal approximators of continuous permutation equivariant sequence-to-sequence functions with compact support, while the network size suffers from the curse of dimensionality (the number of entries in the input sequence). Takakura and Suzuki (2023) studied the approximation and estimation ability of Transformers as seq-to-seq functions with infinite dimensional input, where anisotropic smoothness avoids the curse of dimensionality.

In the applications of Large Language Models (LLMs), empirical findings have demonstrated some correlation between the performance of transformers and the low-dimensional data structures (Razzhigaev et al., 2023; Min et al., 2023; Aghajanyan et al., 2020; Pandey, 2024). Razzhigaev et al. (2023) investigated the intrinsic dimension of embeddings in transformer architectures, and suggested an encoder and decoder embedding property. Most similar to our work is Sharma and Kaplan (2022) which demonstrates an empirical connection between neural scaling laws and the intrinsic dimension of data. While they briefly discuss predictions for LLMs, their theory works best for predicting student-teacher model setups and image classification tasks which seem to enjoy more regularity (and a faster rate of convergence) than language modeling. Despite these empirical findings, we are not aware of any rigorous theoretical justification connecting the scaling laws of transformers with the intrinsic dimension of data. Our paper complements this line of research with statistical estimation and mathematical approximation theories which well-predict the behavior observed in practice.

## 5 Conclusion

ConclusionThis paper establishes statistical and approximation theory results for transformers approximating Holder continuous functions on low-dimensional data manifolds. The resulting bound on the generalization error suffers from exponential dependence only in the intrinsic dimension \(d\). The constructed approximations of low-dimensional functions are shallow, requiring only \(O\big{(}\log(d)\big{)}\) layers independent of the desired accuracy. We demonstrate this theory is accurate in practice by predicting scaling laws in both model size and data size for LLMs trained on natural language datasets. We pay careful attention to the sensitivity of the estimated intrinsic data dimension, finding it is relatively stable with respect to several relevant hyperparameters.

Limitations and Broader ImpactOne important question unanswered by this work is how the intrinsic data dimension may affect the computational scaling exponent \(\alpha_{C}\). Future work may investigate this direction. Additionally, our empirical experiments make the simplifying assumption that the underlying target function possesses Lipschitz regularity (\(\beta=1\)). Better estimates of the correct regularity would likely improve the accuracy of our predictions. More broadly, our work improves fundamental understanding of transformer-based LLMs and improves our ability to theoretically and safely predict future capabilities.

\begin{table}
\begin{tabular}{c c|c c} \multicolumn{2}{c|}{GPT-2} & \multicolumn{2}{c}{Chinchilla} \\ \hline \(\hat{\alpha}_{N}=0.076\) & \(\alpha_{D}=0.070\) & \(\hat{\alpha}_{N}=0.34\) & \(\alpha_{D}=0.25\) \\ \(\hat{\alpha}_{D}=0.095\) & \(\alpha_{N}=0.106\) & \(\hat{\alpha}_{D}=0.28\) & \(\alpha_{N}=0.33\) \\ \end{tabular}
\end{table}
Table 1: ID-free estimation of scaling exponents for GPT-2 and Chincilla.

## References

* Aamari et al. (2019) Eddie Aamari, Jisu Kim, Frederic Chazal, Bertrand Michel, Alessandro Rinaldo, and Larry Wasserman. Estimating the reach of a manifold, 2019.
* Aghajanyan et al. (2020) Armen Aghajanyan, Luke Zettlemoyer, and Sonal Gupta. Intrinsic dimensionality explains the effectiveness of language model fine-tuning. _ArXiv_, abs/2012.13255, 2020. URL https://api.semanticscholar.org/CorpusID:229371560.
* Audibert and Tsybakov (2007) Jean-Yves Audibert and Alexandre B Tsybakov. Fast learning rates for plug-in classifiers. 2007.
* Bahdanau et al. (2014) Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. _arXiv preprint arXiv:1409.0473_, 2014.
* Bahri et al. (2021) Yasaman Bahri, Ethan Dyer, Jared Kaplan, Jaehoon Lee, and Utkarsh Sharma. Explaining neural scaling laws. _ArXiv_, abs/2102.06701, 2021. URL https://api.semanticscholar.org/CorpusID:231918701.
* Bai et al. (2023) Yu Bai, Fan Chen, Huan Wang, Caiming Xiong, and Song Mei. Transformers as statisticians: Provable in-context learning with in-context algorithm selection, 2023.
* Biderman et al. (2023) Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O'Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, Aviya Skowron, Lintang Sutawika, and Oskar van der Wal. Pythia: A suite for analyzing large language models across training and scaling, 2023.
* Chen et al. (2019) Minshuo Chen, Haoming Jiang, Wenjing Liao, and Tuo Zhao. Efficient approximation of deep relu networks for functions on low dimensional manifolds. _Advances in neural information processing systems_, 32, 2019.
* Chen et al. (2022) Minshuo Chen, Haoming Jiang, Wenjing Liao, and Tuo Zhao. Nonparametric regression on low-dimensional manifolds using deep relu networks: Function approximation and statistical recovery. _Information and Inference: A Journal of the IMA_, 11(4):1203-1253, 2022.
* Conway and Sloane (1988) John Conway and N. Sloane. _Sphere Packings, Lattices and Groups_, volume 290. 01 1988. ISBN 978-1-4757-2018-1. doi: 10.1007/978-1-4757-2016-7.
* Dahal et al. (2022) Biraj Dahal, Alexander Havrilla, Minshuo Chen, Tuo Zhao, and Wenjing Liao. On deep generative models for approximation and estimation of distributions on manifolds. _Advances in Neural Information Processing Systems_, 35:10615-10628, 2022.
* Edelman et al. (2022) Benjamin L Edelman, Surbhi Goel, Sham Kakade, and Cyril Zhang. Inductive biases and variable creation in self-attention mechanisms. In _International Conference on Machine Learning_, pages 5793-5831. PMLR, 2022.
* Eldan and Li (2023) Ronen Eldan and Yuanzhi Li. Tinystories: How small can language models be and still speak coherent english?, 2023.
* Federer (1959) Herbert Federer. Curvature measures. _Transactions of the American Mathematical Society_, 93(3):418-491, 1959.
* Gao et al. (2021) Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. The pile: An 800gb dataset of diverse text for language modeling. _CoRR_, abs/2101.00027, 2021. URL https://arxiv.org/abs/2101.00027.
* Gokaslan et al. (2019) Aaron Gokaslan, Vanya Cohen, Ellie Pavlick, and Stefanie Tellex. Openwebtext corpus. http://Skylion007.github.io/OpenWebTextCorpus, 2019.
* Goodfellow et al. (2014) Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. _Advances in neural information processing systems_, 27, 2014.
* Goodfellow et al. (2014)Alex Graves, Abdel-rahman Mohamed, and Geoffrey Hinton. Speech recognition with deep recurrent neural networks. In _2013 IEEE international conference on acoustics, speech and signal processing_, pages 6645-6649. Ieee, 2013.
* Gu et al. (2017) Shixiang Gu, Ethan Holly, Timothy Lillicrap, and Sergey Levine. Deep reinforcement learning for robotic manipulation with asynchronous off-policy updates. In _2017 IEEE international conference on robotics and automation (ICRA)_, pages 3389-3396. IEEE, 2017.
* Hestness et al. (2017) Joel Hestness, Sharan Narang, Newsha Ardalani, Gregory Diamos, Heewoo Jun, Hassan Kianinejad, Md Mostofa Ali Patwary, Yang Yang, and Yanqi Zhou. Deep learning scaling is predictable, empirically. _arXiv preprint arXiv:1712.00409_, 2017.
* Hoffmann et al. (2022) Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and Laurent Sifre. Training compute-optimal large language models, 2022.
* Jelassi et al. (2022) Samy Jelassi, Michael Sander, and Yuanzhi Li. Vision transformers provably learn spatial structure. _Advances in Neural Information Processing Systems_, 35:37822-37836, 2022.
* Kaplan et al. (2020) Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. _arXiv preprint arXiv:2001.08361_, 2020.
* Kim et al. (2019) Yongdai Kim, Ilsang Ohn, and Dongha Kim. Fast convergence rates of deep neural networks for classification, 2019. URL https://arxiv.org/abs/1812.03599.
* Kocetkov et al. (2022) Denis Kocetkov, Raymond Li, Loubna Ben Allal, Jia Li, Chenghao Mou, Carlos Munoz Ferrandis, Yacine Jernite, Margaret Mitchell, Sean Hughes, Thomas Wolf, Dzmitry Bahdanau, Leandro von Werra, and Harm de Vries. The stack: 3 tb of permissively licensed source code. _Preprint_, 2022.
* Krizhevsky et al. (2012) Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. _Advances in neural information processing systems_, 25, 2012.
* Lee et al. (2020) Jaehoon Lee, Lechao Xiao, Samuel S Schoenholz, Yasaman Bahri, Roman Novak, Jascha Sohl-Dickstein, and Jeffrey Pennington. Wide neural networks of any depth evolve as linear models under gradient descent *. _Journal of Statistical Mechanics: Theory and Experiment_, 2020(12):124002, December 2020. ISSN 1742-5468. doi: 10.1088/1742-5468/abc62b. URL http://dx.doi.org/10.1088/1742-5468/abc62b.
* Levina and Bickel (2004) Elizaveta Levina and Peter Bickel. Maximum likelihood estimation of intrinsic dimension. In L. Saul, Y. Weiss, and L. Bottou, editors, _Advances in Neural Information Processing Systems_, volume 17. MIT Press, 2004. URL https://proceedings.neurips.cc/paper_files/paper/2004/file/74934548253bcab8490ebd74afed7031-Paper.pdf.
* Liu et al. (2021) Hao Liu, Minshuo Chen, Tuo Zhao, and Wenjing Liao. Besov function approximation and binary classification on low-dimensional manifolds using convolutional residual networks. In _International Conference on Machine Learning_, pages 6770-6780. PMLR, 2021.
* Liu et al. (2023) Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. _ACM Computing Surveys_, 55(9):1-35, 2023.
* Min et al. (2023) Zeping Min, Qian Ge, and Zhong Li. An intrinsic dimension perspective of transformers for sequential modeling, 2023. URL https://openreview.net/forum?id=0UZYWLzPBjA.
* Miotto et al. (2018) Riccardo Miotto, Fei Wang, Shuang Wang, Xiaoqian Jiang, and Joel T Dudley. Deep learning for healthcare: review, opportunities and challenges. _Briefings in bioinformatics_, 19(6):1236-1246, 2018.
* Nakada and Imaizumi (2020) Ryumei Nakada and Masaaki Imaizumi. Adaptive approximation and generalization of deep neural network with intrinsic dimensionality. _Journal of Machine Learning Research_, 21(174):1-38, 2020.
* Nakada et al. (2018)Kenta Oono and Taiji Suzuki. Approximation and non-parametric estimation of resnet-type convolutional neural networks. In _International conference on machine learning_, pages 4922-4931. PMLR, 2019.
* Pandey [2024] Rohan Pandey. gzip predicts data-dependent scaling laws, 2024. URL https://arxiv.org/abs/2405.16684.
* Pedregosa et al. [2011] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. _Journal of Machine Learning Research_, 12:2825-2830, 2011.
* Pope et al. [2021] Phillip E. Pope, Chen Zhu, Ahmed Abdelkader, Micah Goldblum, and Tom Goldstein. The intrinsic dimension of images and its impact on learning. _ArXiv_, abs/2104.08894, 2021. URL https://api.semanticscholar.org/CorpusID:233296562.
* Perez et al. [2021] Jorge Perez, Pablo Barcelo, and Javier Marinkovic. Attention is turing-complete. _Journal of Machine Learning Research_, 22(75):1-35, 2021. URL http://jmlr.org/papers/v22/20-302.html.
* Radford et al. [2019] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. 2019. URL https://api.semanticscholar.org/CorpusID:160025533.
* Razzhigaev et al. [2023] Anton Razzhigaev, Matvey Mikhalchuk, Elizaveta Goncharova, Ivan Oseledets, Denis Dimitrov, and Andrey Kuznetsov. The shape of learning: Anisotropy and intrinsic dimensions in transformer-based models. _ArXiv_, abs/2311.05928, 2023. URL https://api.semanticscholar.org/CorpusID:265128603.
* Rosenfeld et al. [2019] Jonathan S Rosenfeld, Amir Rosenfeld, Yonatan Belinkov, and Nir Shavit. A constructive prediction of the generalization error across scales. _arXiv preprint arXiv:1909.12673_, 2019.
* Roweis and Saul [2000] Sam T Roweis and Lawrence K Saul. Nonlinear dimensionality reduction by locally linear embedding. _science_, 290(5500):2323-2326, 2000.
* Russakovsky et al. [2014] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael S. Bernstein, Alexander C. Berg, and Li Fei-Fei. Imagenet large scale visual recognition challenge. _CoRR_, abs/1409.0575, 2014. URL http://arxiv.org/abs/1409.0575.
* Sanford et al. [2024] Clayton Sanford, Daniel Hsu, and Matus Telgarsky. Transformers, parallel computation, and logarithmic depth. _ArXiv_, abs/2402.09268, 2024. URL https://api.semanticscholar.org/CorpusID:267657804.
* 1897, 2020. doi: 10.1214/19-AOS1875. URL https://doi.org/10.1214/19-AOS1875.
* Sharma and Kaplan [2022] Utkarsh Sharma and Jared Kaplan. Scaling laws from the data manifold dimension. _J. Mach. Learn. Res._, 23(1), jan 2022. ISSN 1532-4435.
* Song et al. [2020] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. _arXiv preprint arXiv:2011.13456_, 2020.
* Takakura and Suzuki [2023] Shokichi Takakura and Taiji Suzuki. Approximation and estimation ability of transformers for sequence-to-sequence functions with infinite dimensional input. In _International Conference on Machine Learning_, 2023. URL https://api.semanticscholar.org/CorpusID:258967774.
* Tenenbaum et al. [2000] Joshua B Tenenbaum, Vin de Silva, and John C Langford. A global geometric framework for nonlinear dimensionality reduction. _science_, 290(5500):2319-2323, 2000.
* Tu [2011] Loring W. Tu. _An Introduction to Manifolds_. 01 2011. ISBN 978-1-4419-7399-3.
* Tu et al. [2019]Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. _Advances in neural information processing systems_, 30, 2017.
* Wei et al. (2022) Colin Wei, Yining Chen, and Tengyu Ma. Statistically meaningful approximation: a case study on approximating turing machines with transformers. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, _Advances in Neural Information Processing Systems_, volume 35, pages 12071-12083. Curran Associates, Inc., 2022. URL https://proceedings.neurips.cc/paper_files/paper/2022/file/4ebf1d74f53ece08512a23309d58df89-Paper-Conference.pdf.
* Yarotsky (2016) Dmitry Yarotsky. Error bounds for approximations with deep relu networks. _Neural networks : the official journal of the International Neural Network Society_, 94:103-114, 2016. URL https://api.semanticscholar.org/CorpusID:426133.
* Yun et al. (2019) Chulhee Yun, Srinadh Bhojanapalli, Ankit Singh Rawat, Sashank J. Reddi, and Sanjiv Kumar. Are transformers universal approximators of sequence-to-sequence functions? _ArXiv_, abs/1912.10077, 2019. URL https://api.semanticscholar.org/CorpusID:209444410.
* Zhang et al. (2022) Yufeng Zhang, Boyi Liu, Qi Cai, Lingxiao Wang, and Zhaoran Wang. An analysis of attention via the lens of exchangeability and latent variable models. _arXiv preprint arXiv:2212.14852_, 2022.

## Appendix A Pretraining Hyperparameters

Table 2 lists the default hyper-parameters we use for pretraining. All training was done on four RTX 6000s.

## Appendix B Deriving Language Model Scaling Laws from Statistical and Approximation Theory

### Squared Regression Error

First we extract bounds on scaling laws in the case of regression squared error. We have the bound from the proof of Theorem 1

\[\int_{\mathcal{M}}\big{(}f(x)-\hat{\Upsilon}_{n}(x)\big{)}^{2}dQ(x)\leq\tilde{ O}\big{(}\epsilon^{2}+\frac{Dd^{2}\epsilon^{-\frac{d}{\beta}}}{n}\big{)}\]

where \(\epsilon\) is the approximation error such that

\[\inf_{\mathsf{T}\in\mathsf{T}}\|f-\mathsf{T}\|_{L^{\infty}(\mathcal{M})}<\epsilon\]

Let the _model size_\(N\) of a transformer \(\mathsf{T}\in\mathcal{T}\) be \(N=L_{T}(d^{2}_{embd}(3m+L_{\mathsf{FFN}}))=\log(d)(25(3\epsilon^{-\frac{d}{ \beta}}+\log(\epsilon^{-1})))=\tilde{O}\big{(}\epsilon^{-\frac{d}{\beta}}\big{)}\). Write the squared generalization error as

\[L_{\text{sq}}(N,n)=\int_{\mathcal{M}}\big{(}f(x)-\hat{\Upsilon}_{n}(x)\big{)} ^{2}dQ(x).\]

Then

\[L_{\text{sq}}(N,n)\leq\tilde{O}\big{(}\epsilon^{2}+\frac{Dd^{2} \epsilon^{-\frac{d}{\beta}}}{n}\big{)}=\tilde{O}\big{(}N^{-\frac{2\beta}{d}}+ \frac{N}{n}\big{)}\]

In the model scaling regime, when data is plentiful, we have \(\frac{N}{n}<<N^{-\frac{2\beta}{d}}\) which implies the behavior of \(L_{\text{sq}}(N,n)\) is dominated by \(N^{-\frac{2\beta}{d}}\). This gives us the model scaling exponent as

\[\alpha_{N}=\frac{2\beta}{d}\]

For the data scaling exponent \(\alpha_{D}\) we will choose \(N\) to balance both error terms. The will predict how data size and model size should scale together to achieve a minimal generalization error. We should have

\[N^{-\frac{2\beta}{d}}\asymp\frac{N}{n}\iff n\asymp N^{1+\frac{2 \beta}{d}}\asymp N^{\frac{2\beta+d}{d}}\iff N\asymp n^{\frac{d}{2\beta+d}}\]

\begin{table}
\begin{tabular}{l r} \hline \hline Architecture hparams & \\ \hline num layers & 12 \\ num attention heads & 12 \\ embedding dimension & 768 \\ context length & 1024 \\ \hline Optimization hparams & \\ \hline Optimizer & AdamW \\ max lr & 6e-4 \\ min lr & 6e-5 \\ lr schedule & linear warmup + cosine decay \\ warmup steps & 2000 \\ max steps & 200,000 \\ global batch size & 1920 \\ \hline \hline \end{tabular}
\end{table}
Table 2: Default hyperparameters for all training jobs. All training was done on four RTX 6000s.

where \(\asymp\) denotes in the same order. Substituting this into the error bound gives

\[L_{\text{sq}}(N,n)\leq\tilde{O}\big{(}2\frac{N}{n}\big{)}=\tilde{O}\big{(}n^{2 \frac{\beta}{2\beta+d}-1}\big{)}=\tilde{O}\big{(}n^{-\frac{2\beta}{2\beta+d}} \big{)},\]

which gives rise to

\[\alpha_{D}=\frac{2\beta}{2\beta+d}.\]

### From Regression to Classification

After establishing statistical and approximation theory for transformers for regression, we now seek to apply our theory to classification. In language models, the next-token prediction task is a multi-class classification problem, where transformers are trained to predict the probability of the next word out of a large dictionary.

For simplicity, we consider binary classification here. Let \((x,y)\) be a random couple taking values in \(\mathcal{M}\times\{0,1\}\) with joint distribution \(P\), where \(\mathcal{M}\) is a manifold satisfying Assumption 1. Let \(P_{x}\) be the marginal distribution of \(x\). Here \(x\) stands for the input feature and \(y\) is the corresponding label. The classification goal is to predict the label \(y\) given the value of \(x\). A decision rule is given by a function \(f:\mathcal{M}\to\{0,1\}\). The performance of a decision rule \(f\) is measured by the misclassification error

\[R(f):=P(y\neq f(x)).\]

The Bayes decision rule has the form

\[f^{*}(x)=\mathbf{1}\left\{\eta(x)\geq 1/2\right\}\]

where \(\mathbf{1}\) denotes the indicator function and

\[\eta(x):=P(y=1|x)\] (8)

is the regression function of \(y\) on \(x\). For binary classification, the goal is to estimate the probability function \(\eta(x)\). If \(\eta\) is a \(\beta\)-Holder function satisfying Assumption 2, our approximation theory in Theorem 2 gives rise to a transformer neural network \(\eta_{\theta}\) such that

\[\|\eta_{\theta}-\eta\|_{L^{\infty}(\mathcal{M})}\leq\epsilon\] (9)

for an arbitrary small \(\epsilon>0\).

We next consider the plug-in estimate of \(\eta_{\theta}\)(Audibert and Tsybakov, 2007): \(f_{\theta}(x)=\mathbf{1}\left\{\eta_{\theta}\geq 1/2\right\}.\) The excess risk of the plug-in estimate \(f_{\theta}\) is

\[\mathcal{E}(f_{\theta}) =R(f_{\theta})-R(f^{*})\] \[=P(y\neq f_{\theta}(x))-P(y\neq f^{*}(x))\] \[=P(y=f^{*}(x))-P(y=f_{\theta}(x))\] \[=\mathbb{E}_{x}[\mathbf{1}\left\{f^{*}(x)=1\right\}\eta(x)+ \mathbf{1}\left\{f^{*}(x)=0\right\}(1-\eta(x))\] \[\quad-\mathbf{1}\left\{f_{\theta}(x)=1\right\}\eta(x)-\mathbf{1} \left\{f_{\theta}(x)=0\right\}(1-\eta(x))]\] \[=\mathbb{E}_{x}\left[|2\eta(x)-1|\mathbf{1}\left\{f_{\theta}(x) \neq f^{*}(x)\right\}\right].\]

We next discuss how the regression error in (9) impacts the excess risk \(\mathcal{E}(f_{\theta})\). For classification problems, the classification error depends on how well the conditional probability \(\eta(x)\) is estimated, and how many points are close to the decision boundary. Following Audibert and Tsybakov (2007), we assume the following margin condition: There exist \(c_{M}>0\) and \(\gamma\geq 0\), such that for any \(t>0\),

\[P_{x}(0<|\eta(x)-1/2|\leq t)\leq c_{M}t^{\gamma}.\] (10)

A smaller \(\gamma\) means more samples cluster around the decision boundary with a higher likelihood of falling into either class. This is not expected to be the case for natural data, where for most samples it is easy to tell to which class they belong (Kim et al., 2019, Figure 2). As a result, we assume \(\gamma\geq 1\).

Under the margin condition in (10), we follow Audibert and Tsybakov (2007) to decompose the excess risk \(\mathcal{E}(f_{\theta})\) such that for any \(\delta>0\):

\[\mathcal{E}(f_{\theta}) =\mathbb{E}_{x}\left[|2\eta(x)-1|\mathbf{1}\{f_{\theta}(x)\neq f ^{*}(x)\}\mathbf{1}\{|\eta(x)-1/2|\leq\delta\}\right]\] \[\quad+\mathbb{E}_{x}\left[|2\eta(x)-1|\mathbf{1}\{f_{\theta}(x) \neq f^{*}(x)\}\mathbf{1}\{|\eta(x)-1/2|>\delta\}\right]\] \[\leq 2\delta P_{x}(|\eta(x)-1/2|\leq\delta)+2\mathbb{E}_{x}\left[| \eta_{\theta}(x)-\eta(x)|\mathbf{1}\left\{|\eta_{\theta}(x)-\eta(x)|>\delta \right\}\right]\] \[\leq 2c_{M}\delta^{1+\gamma}+2\mathbb{E}_{x}\left[|\eta_{\theta}(x) -\eta(x)|\mathbf{1}\left\{|\eta_{\theta}(x)-\eta(x)|>\delta\right\}\right].\] (11)Plugging the regression error in (9) to the excess risk bound in (11) with \(\delta=\epsilon\), we obtain

\[\mathcal{E}(f_{\theta})\leq 2c_{M}\epsilon^{1+\gamma}.\]

When the margin assumption satisfies \(\gamma=1\), the excess risk bound becomes

\[\mathcal{E}(f_{\theta})\leq 2c_{M}\epsilon^{2},\]

which demonstrates a connection between the classification risk bound and the squared regression error for the \(\eta\) function in (8). This argument partially justifies the connection between empirical neural scaling laws in large language models and the squared regression error. We will leave a rigorous mathematical argument as future work.

### Cross-Entropy Based Language Model

In practice, language models are trained and evaluated using cross-entropy loss. We consider the next-token prediction in language models. Per-sample (token), language models are trained to minimize the multi-class cross-entropy loss over a large number of classes/tokens:

\[L_{\text{cr}}(n)=-\frac{1}{n}\sum_{i=1}^{n}\sum_{y=1}^{V}\mathbf{1}_{y=y_{i}^{ *}}\ln(P(y|x_{i}))\] (12)

where \(V\) is the vocabulary size of the model (number of classes), \(y_{i}^{*}\) is the ground-truth label of \(x_{i}\), \(\mathbf{1}_{y=y_{i}^{*}}\) is the indicator function for the event \(y=y_{i}^{*}\), and \(P(y|x)\) is the conditional probability of labels given \(x\). For the next-token prediction, transformers are trained to estimate the probability function \(P(y|x)\), and the next token is predicted to the word with the highest probability. The test loss is evaluated using cross-entropy on test data. We will leave the error bound on the cross-entropy loss as future work.

## Appendix C Notation

\(H\) will represent the _embedding matrix_ of a transformer and \(h_{i}\) will represent the \(i\)th _token_ (column) of the embedding matrix. \(h_{i}^{j}\) will denote the \(j\)th component of the vector \(h_{i}\). \(\|\cdot\|_{p}\), \(p>1\), will denote the \(p\)th norm of vectors, and \(\|\cdot\|_{p,q}\) will denote the component-wise matrix norm. For a matrix \(A\in\mathbb{R}^{m\times n}\), \(\|A\|_{p,q}=(\sum_{j=1}^{n}(\sum_{i=1}^{m}|A_{ij}|^{p})^{\frac{q}{p}})^{\frac{ 1}{q}}\). In particular, we denote \(\|A\|_{\infty}=\|A\|_{\infty,\infty}\) and \(\|A\|_{1}=\|A\|_{1,1}\) for matrices. \(\|\cdot\|_{L^{p}(U)}\) will denote the \(L^{p}\) norm of a function on \(U\subseteq\mathbb{R}^{D}\). Neural network blocks will always be **bolded**. Sometimes neural network blocks will also be sub-scripted with their corresponding vector of _weights_\(\theta\). We use \(\|\theta\|_{\infty}\) to denote the largest magnitude of the weight parameters. However we will often omit \(\theta\) and leave the dependence as implicit. In many places, we will also write \(\theta_{\text{NN}}\) to denote the weights for a neural network **NN**. \(e_{i}\) will denote the \(i\)th standard basis vector where the \(i\)th component is \(1\) and all other entries are \(0\). \(\sigma\) will always denote the ReLU activation. \(B(x,r)\) denotes the Euclidean ball of radius \(r\) centered at \(x\in\mathbb{R}^{D}\). For a vector \(h_{t}\), \(h_{t}^{i:j}\in\mathbb{R}^{j-i}\) will denote the vector such that \((h_{t}^{i:j})^{k}=h_{t}^{k}\) i.e. the contiguous components of \(h_{t}\) from \(i,...,j-1\).

## Appendix D Details about Transformer Neural Networks

We have defined transformer neural networks in Definition 1. Some detailed definitions are given below.

**Definition 3** (Transformer Block).: _We define a transformer block \(\mathrm{B}\) as a residual composition of the form_

\[\mathrm{B}(H)=\text{FFN}(\text{MHA}(H)+H)+\text{MHA}(H)+H\] (13)

_taking as input an embedding matrix \(H\in\mathbb{R}^{d_{embed}\times l}\) and \(\text{MHA}\) is a multi-headed attention layer and \(\text{FFN}\) is a feed-forward layer. To define the multi-headed attention layer we first define the attention mechanism._

**Definition 4** (Attention).: \[\text{A}_{Q,K,V}(H)=VH\sigma((KH)^{T}QH)\] (14)

_where \(Q,K,V\in\mathbb{R}^{d_{embd}\times d_{embd}}\) are referred to as the query, key, and value matrices and \(\sigma(x)=\max(0,x)\) is the ReLU activation function. Often we will omit the dependence of \(A\) on \(Q,K,V\). We note it will be convenient to write the action of \(A\) on the \(i\)th column \(h_{i}\) of \(H\) as_

\[\text{A}(h_{i})=\sum_{j=1}^{l}\sigma(\langle Qh_{i},Kh_{j}\rangle)Vh_{j}\] (15)

_. This allows us to interpret the \(i\)th column of the output of \(A\) as a linear combination of the values weighted by the interaction of \(h_{i}\)th query and the \(h_{j}\)th key. Multi-headed attention can then be defined as_

\[\text{MHA}(H)=W_{O}(\text{concat}_{j}(V_{j}H\sigma((K_{j}H)^{T}Q_{j}H))\] (16)

_where the output of each of \(j\in\{1,...,M\}\) attention heads is concatenated and \(W_{O}\in\mathbb{R}^{d_{embd}\times md_{embd}}\). Frequently we will simply take \(W_{O}\) to be a sum so that_

\[\text{MHA}(H)=\sum_{j=1}^{m}V_{j}H\sigma((K_{j}H)^{T}Q_{j}H)).\] (17)

We also formally define the feed-forward layer FFN:

**Definition 5** (Feed-forward Layer).: _A feed-forward layer of depth \(L_{\text{FFN}}\) and width \(w_{\text{FFN}}\) is of the form_

\[\text{FFN}(h)=W_{L_{\text{FFN}}}\sigma(W_{L_{\text{FFN}}-1}...\sigma(W_{1}h+b_{ 1})...+b_{L_{\text{FFN}}-1})+b_{L_{\text{FFN}}}\]

_where \(W_{2},...,W_{L_{\text{FFN}}-1}\in\mathbb{R}^{d_{embd}\times d_{embd}}\), \(W_{1}\in\mathbb{R}^{w_{\text{FFN}}\times d_{embd}},W_{L_{\text{FFN}}}\in \mathbb{R}^{d_{embd}\times w_{\text{FFN}}}\), \(b_{1},...,b_{L_{\text{FFN}}}\in\mathbb{R}^{d_{embd}}\) and the activation is ReLU. Note, each feed-forward layer is applied tokenwise to an embedding matrix \(H\)._

We can also define a _class_\(\mathcal{FFN}\) of feed-forward networks:

Figure 5: Diagram of transformer block.

**Definition 6** (Feed-forward Network Class).: \[\mathcal{FFN}(L_{\text{FFN}},w_{\text{FFN}})=\big{\{}\text{FFN}_{ \theta}\mid\text{FFN}_{\theta}\text{ is a feed-forward network with weights }\theta\] \[\text{with at most }L_{\text{FFN}}\text{ layers with width }w_{\text{FFN}}\big{\}}.\]

Sometimes we may omit the dependence of \(\mathcal{F}\) on \(w_{\text{FFN}}\). In these cases we assume \(w_{\text{FFN}}=d_{embd}\). We similarly define multi-headed attention and transformer block classes:

**Definition 7** (Multi-headed Attention and Transformer Block Classes).: \[\mathcal{MHA}(m)=\big{\{}\text{MHA}_{\theta}| \text{MHA}_{\theta}\text{ is a multi-headed attention network with weights }\theta\] \[\text{with at most }m\text{ attention heads.}\big{\}}\]

\[\mathcal{B}(m,L_{\text{FFN}},w_{\text{FFN}})=\big{\{}\text{B}_{ \theta}| \text{B}_{\theta}\text{ is a transformer block with weights }\theta\text{, a }m\text{-headed multi-headed attention layer, and a }L_{\text{FFN}}\text{ deep feed-forward layer with width }w_{\text{FFN}}\big{\}}.\]

As with \(\mathcal{F}\), we may sometimes omit the dependence of B on \(w_{\text{FFN}}\). In these cases we take \(w_{\text{FFN}}=d_{embd}\). We can parameterize the class of transformer neural networks \(\mathcal{T}\) as Definition 2.

Lastly, our constructions will rely heavily on a particular structuring of the tokens/columns in the transformer's hidden states. The first two rows will contain mutable data used to compute the target function. The remaining rows will be immutable, containing positional/interaction vectors allowing us to uniquely identify each row and a constant term serving as a kind of "scratchpad". See Figure 6 for a diagram.

## Appendix E Proof of Transformer Approximation Theory

We first prove the approximation theories in Section 2.4. We first prove Lemma 1 in Section E.1, and then prove Theorem 2 in Section E.2.

### Proof of Lemma 1

Proof of Lemma 1.: Fix \(f:[0,1]^{d}\rightarrow\mathbb{R}\) and \(\epsilon>0\). We aim to approximate \(f\) efficiently with a transformer \(\text{T}\in\mathcal{T}(L_{T},L_{\text{FFN}},w_{\text{FFN}},l,d_{embd},m,R,\kappa)\). We will proceed similarly to Yarotsky (2016) by approximating \(f\) locally with piecewise constant functions. First, we partition the domain uniformly into blocks indexed by \(n\in\{1,...,N\}^{d}\) for some \(N\geq 1\) with \(U_{n}=\{x\in[0,1]^{d}:\forall i\in\{1,...,d\},\big{|}x^{i}-\frac{n^{i}}{N-1} \big{|}\leq\frac{1}{N-1}\}\). In this proof, we use \(x^{i}\) and \(n^{i}\) to denote the \(i\)th coordinate of \(x\) and \(n\) respectively. Then we construct a partition of unity (**PoU**) via

\[\phi_{n}(x)=\prod_{i=1}^{d}\psi\left(3N(x^{i}-\frac{n^{i}-1}{N-1})\right)\] (18)

Figure 6: Diagram of a structured token. The first two rows contain mutable data used to compute the target function. The remaining rows are never changed after initialization.

with the trapezoid function \(\psi:\mathbb{R}^{d}\to\mathbb{R}\) as

\[\psi(x)=\begin{cases}1&|x|\leq 1\\ 1-|x|&1\leq|x|\leq 2\\ 0&|x|\geq 2\end{cases}.\]

We can now write our approximation for \(f\) as

\[\hat{f}(x)=\sum_{n}\phi_{n}(x)f_{n}\]

where \(f_{n}=f(x_{n})\) with \(x_{n}\) being the center point of \(U_{n}\). This yields the following approximation error:

\[\|f-\hat{f}\|_{L^{\infty}([0,1]^{d})} =\sup_{x\in[0,1]^{d}}\big{|}f(x)-\hat{f}(x)\big{|}\] \[=\sup_{x\in[0,1]^{d}}\big{|}f(x)-\sum_{n\in\{1,...,N\}^{d}}f_{n} \phi_{n}(x)\big{|}\] \[=\sup_{x\in[0,1]^{d}}\Big{|}\sum_{n\in\{1,...,N\}^{d}}f(x)\phi_{n }(x)-\sum_{n\in\{1,...,N\}^{d}}f_{n}\phi_{n}(x)\big{|}\] \[=\sup_{x\in[0,1]^{d}}\sum_{n\in\{1,...,N\}^{d}}|f(x)-f_{n}|\phi_{ n}(x)\] \[=\sup_{x\in[0,1]^{d}}\sum_{|x^{i}-\frac{n^{i}}{N-1}|\leq\frac{1 }{N-1}}|f(x)-f_{n}|\] \[\leq\sup_{x\in[0,1]^{d}}\sum_{|x^{i}-\frac{n^{i}}{N-1}|\leq\frac {1}{N-1}}H_{f}\|x-x_{n}\|_{2}^{\beta}\] \[\leq\sup_{x\in[0,1]^{d}}2^{d}H_{f}\frac{d^{\beta}}{(N-1)^{\beta}} =\frac{2^{d}d^{\beta}H_{f}}{(N-1)^{\beta}}\]

Thus we can control this error simply by increasing \(N\). It suffices to pick \(N=d(\frac{2^{d}H_{f}}{\epsilon})^{\frac{1}{\beta}}+1\) to obtain an \(\epsilon\) error bound.

We next construct an approximation to \(\hat{f}\) with a transformer neural network \(\text{T}\). In fact we can represent \(\hat{f}\)**exactly** as a transformer neural network.

At a high level, we will proceed by building up each \(\phi_{n}\) as an accumulated partial product \(p_{n}\) in parallel over \(d\) transformer blocks. Each block will use at most \(d_{embd}N_{embd}^{d}\) attention heads. Each head will be responsible for multiplying two partial products. Conveniently \(\phi_{n}\) can be exactly implemented with a two-layer FFN and applied column-wise to each term. The constant terms \(f_{n}\) will then be multiplied in and summed at the final layer to compute the output.

Step 1: Embed the inputNow fix input \(x\in\mathbb{R}^{1\times d}\). First we augment the input, via concatenation, with a sequence of 0s: \(\textbf{0}_{Nd+Nd^{d}}\) forming input \(x^{\prime}\in\mathbb{R}^{1\times d+Nd+N^{d}}\) (note this is a linear operation). We can then construct a linear embedding layer \([1,0,0,0,0]^{T}=E\in\mathbb{R}^{d_{embd}\times 1}\) resulting in the input embedding matrix \(H\in\mathbb{R}^{d_{embd}\times(d+Nd+N^{d}d)}\) of the form

\[\begin{bmatrix}x^{1}&...&x^{d}&\textbf{0}_{Nd}&...&\textbf{0}_{N^{d}d}\\ 0&...&0&\textbf{0}_{Nd}&...&\textbf{0}_{N^{d}d}\\ 0&...&0&\textbf{0}_{Nd}&...&\textbf{0}_{N^{d}d}\\ 0&...&0&\textbf{0}_{Nd}&...&\textbf{0}_{N^{d}d}\\ 0&...&0&\textbf{0}_{Nd}&...&\textbf{0}_{N^{d}d}\\ \end{bmatrix}\]where \(d_{embd}=5\). To this we add a fixed positional encoding \(\text{PE}\in\mathbb{R}^{d_{embd}\times(d+Nd+N^{d}d)}\), producing the first hidden embedding matrix

\[H_{1}=\begin{bmatrix}x^{1}&...&x^{d}&\textbf{0}_{Nd}&...&\textbf{0}_{N^{d}d}\\ 0&...&0&\textbf{0}_{Nd}&...&\textbf{0}_{N^{d}d}\\ \mathcal{I}_{1}&...&...&...&...&\mathcal{I}_{d+dN+N^{d}d}\\ 1&...&...&...&...&1\end{bmatrix}\]

where \(I_{i}\in\mathbb{R}^{2}\) represents an _interaction term_ determining when each token embedding will interact with another in the attention mechanism. We can set \(I_{i}=(\cos(\frac{i}{1}\frac{\pi}{2}),\sin(\frac{i}{1}\frac{\pi}{2}))\) where \(l\) is the number of hidden tokens. Note, this type of positional embedding is commonly known as the sinusoidal positional encoding and can be visualized as rotations of the unit vector \(e_{1}\) around the first quadrant of the unit circle.

Step 2: Pre-compute \(\psi(3N(x^{i}-\frac{j-1}{N-1}))\)Now we pre-compute the terms \(\psi(3N(x^{i}-\frac{j-1}{N-1}))\) from which we can construct our partition of unity. We define \(s_{i,j}=\psi(3(N-1)(x^{i}-\frac{j-1}{N-1}))\) for \(1\leq i\leq d,1\leq j\leq N\). We know \(\psi(3(N-1)\cdot)\) can be computed exactly with a two-layer FFN. So all we must do is prepare the input \(x^{i}-\frac{j-1}{N-1}\). We will do this with one transformer block B\({}_{1}\). The results will be stored in token columns \(h_{d+1},...,h_{d+Nd}\).

Define B\({}_{1}=\mathcal{B}(Nd,7)\) i.e. B\({}_{1}\) has a multi-headed attention layer with \(Nd\) attention heads and a feed-forward layer with depth \(7\). We will construct each of the \(ij\)th blocks using the Interaction Lemma 3. We may choose _data kernels_

\[Q_{ij}^{data}=\begin{bmatrix}0&0&0&0&1\\ 0&0&0&0&1\end{bmatrix}\in\mathbb{R}^{2\times d_{embd}},\quad K_{ij}^{data}= \begin{bmatrix}1&0&0&0&0\\ 0&0&0&0&-\frac{j}{N-1}+1\end{bmatrix}\in\mathbb{R}^{2\times d_{embd}}\]

so that for the token embedding \(h_{ij}=h_{d+i*d+j}\), we have

\[\text{A}_{ij}(h_{ij}) =\sum_{k=1}^{l}\sigma(\langle Q_{ij}h_{ij},K_{ij}h_{k}\rangle)V_{ ij}h_{k}\] \[=\sum_{k=1}^{l}\sigma(\langle Q_{ij}^{data}h_{ij},K_{ij}^{data}h_ {k}\rangle+\langle Q_{ij}^{\mathcal{I}}h_{ij},K_{ij}^{\mathcal{I}}h_{k}\rangle -C)e_{2}\] \[=\sum_{k=1}^{l}\sigma(h_{k}^{1}-\frac{j-1}{N-1}+1+\langle Q_{ij}^ {\mathcal{I}}h_{ij},K_{ij}^{\mathcal{I}}h_{k}\rangle-C)e_{2}\] \[=\sigma(x^{i}-\frac{j-1}{N-1}+1)e_{2}=(x^{i}-\frac{j-1}{N-1}+1)e_{2}\]

where we can choose token \(h_{ij}\) to only interact with token \(h_{i}\). Otherwise we have A\({}_{ij}(h_{t})=0\) for \(h_{t}\neq h_{ij}\). Further, the weights of \(\theta_{\text{A}_{ij}}\) of A\({}_{ij}\) are bounded such that \(\|\theta_{\text{A}_{ij}}\|_{\infty}=O\big{(}l^{2}\big{)}\) according to Lemma 3.

The added output of the multi-headed attention layer is then \(H_{1.5}=\text{MHA}_{1}(H)+H\) and is given by

\[H_{1.5}=\begin{bmatrix}x^{1}&...&x^{d}&\textbf{0}_{Nd}&...&...&\textbf{0}_{N ^{d}d}\\ 0&...&0&x^{1}+c&...&x^{d}-1+c&\textbf{0}_{N^{d}d}\\ \mathcal{I}_{1}&...&...&...&...&\mathcal{I}_{d+Nd+N^{d}d}\\ 1&...&...&...&...&1\end{bmatrix}.\]It remains to subtract the positive term \(c=1\) and then apply \(\psi\) for the tokens \(d+1,...,d+dN\) while leaving the other tokens untouched. Define

\[W_{1} =\begin{bmatrix}0&1&0&0&-1+M\\ 0&-1&0&0&M\\ 0&0&1&0&0\\ 0&0&0&1&0\\ 0&0&0&0&1\end{bmatrix},\quad b_{1}=0\] \[W_{2} =\begin{bmatrix}1&0&0&0&-M\\ 0&1&0&0&-M\\ 0&0&1&0&0\\ 0&0&0&1&0\\ 0&0&0&0&1\end{bmatrix},\quad b_{2}=0.\]

Here \(W_{1}\) shifts values in the second component of each token to the first and subtracts \(1\). The second component is negated to be subtracted from \(H_{1.5}\). \(I_{t}\) and the last component are preserved. A large positive number \(M>2\|x\|_{\infty}\) is added to prevent negative terms from getting erased by ReLU. \(W_{2}\) removes \(M\) after the activation is applied. We can construct a two-layer network \(\text{FFN}_{1}\) to apply \(\psi\) to the first component of each token \(h_{t}\). In tokens \(h_{d},...,h_{d+Nd}\) this produces the desired \(s_{ij}\) terms.

Now we simply must ensure we zero-out changes to tokens outside the range \(d+1,...,d+Nd\). Via Lemma 4 we can construct a two-layer feed-forward _gating network_\(\text{FFN}_{2}\) such that \(\text{FFN}_{2}(h_{t})=h_{t}\) for \(t\leq d\) and \(\text{FFN}_{2}(h_{t})\) is zero except for the last three rows when \(t>d+dN\). We can again invoke the same lemma to produce \(\text{FFN}_{3}\in\mathcal{FFN}(2)\) such that \(\text{FFN}_{3}(h_{t})=h_{t}\) when \(t>d\) and zero except for the last three rows otherwise. Applying \(\text{FFN}_{2}\) and \(\text{FFN}_{3}\) zeroes out tokens outside \(d+1,...,d+dN\) while preserving the rest. Finally, we define the last layer

\[W_{7}=\begin{bmatrix}1&0&0&0&0\\ 0&1&0&0&0\\ 0&0&0&0&0\\ 0&0&0&0&0\\ 0&0&0&0&0\end{bmatrix},\quad b_{7}=0\]

which zeros out all except the first two components. This produces the next embedding matrix \(H_{2}\):

\[H_{2}=\begin{bmatrix}x^{1}&...&x^{d}&s_{1,1}&...&s_{d,N}&\textbf{0}_{N^{d}d} \\ 0&...&0&\textbf{0}_{Nd}&...&...&\textbf{0}_{N^{d}d}\\ \mathcal{I}_{1}&...&...&...&...&...&\mathcal{I}_{d+Nd+N^{d}d}\\ 1&...&...&...&...&...&1\end{bmatrix}\]

Step 3: Building up partial productsNow we can start construction of the partial products for each patch \(\phi_{n}\), \(n\in\{1,....,N\}^{d}\). For a fixed patch \(n\), we write \(p_{n,k,i}\) to indicate the \(i\)th partial product of \(2^{k-1}\) terms. Formally, \(p_{n,k,i}=p_{n,k-1,2i-1}\cdot p_{n,k-1,2i}\) with \(p_{n,1,i}=s_{i,n^{i}}\) where \(s_{i,n^{i}}=\psi(3(N-1)(x^{i}-\frac{n^{i}-1}{N-1}))\) and \(n\in\{1,...,N\}^{d}\), \(1\leq k\leq\log_{2}(d)\), \(1\leq i\leq\frac{d}{2i}\). See Figure 7 for a diagram of how each partial product is assembled. Architecturally, this will be done in \(\log(d)+1\) transformer blocks. The first block will copy the product terms into the last \(N^{d}d\) tokens. The remaining \(\log(d)\) blocks compute the recursively assembled partial products by multiplying \(p_{n,k-1,2i-1}\cdot p_{n,k-1,2i}\). Storage and assembly will be done in the last \(d+Nd+1,...,d+Nd+N^{d}d\) tokens.

For the first transformer block we define \(\text{B}_{2}=\mathcal{B}(N^{d}d,0)\). We index each attention head as \(A_{n,i}\) where \(n\in\{1,....,N\}^{d}\) and \(1\leq i\leq d\). Fix a patch \(n\) and the corresponding product \(\prod_{i=1}^{d}s_{i,n^{i}}\). These terms have been pre-computed and stored in tokens \(h_{d+1},...,h_{d+Nd}\). We will then use each attention head \(A_{n,i}\) to copy \(s_{i,n^{i}}\) into its corresponding token \(h_{n,i}=h_{d+Nd+d\sum_{p=1}^{d}(n^{p}-1)N^{d-p}+i}\). Concretely, for a fixed attention head \(A_{n,i}\) define the data kernels

\[Q_{n,i}^{data}=\begin{bmatrix}0&0&0&0&1\\ 0&0&0&0&0\end{bmatrix}\quad K_{n,i}^{data}=\begin{bmatrix}1&0&0&0&0\\ 0&0&0&0&0\end{bmatrix}\]via the Interaction Lemma 3 we may pick A\({}_{n,i}\) so that

\[A_{n,i}h_{n,i} =\sum_{k=1}^{l}\sigma(\langle Q_{n,i}h_{n,i},K_{n,i}h_{k}\rangle)V_{ n,i}h_{k}\] \[=\sum_{k=1}^{l}\sigma(h_{n,i}^{5}h_{k}^{1}+Q_{n,i}^{I}\mathcal{I}_ {n,i}\cdot K_{n,i}^{I}I_{k}-C)e_{1}\] \[=\sigma(1\cdot s_{i,n^{i}})e_{1}=s_{i,n^{i}}e_{1}\]

where the interaction terms zero-out all terms in the sum except when \(k=d+id+n^{i}\) (which is the index of the token containing \(s_{i,n^{i}}\)). Similarly to as in B\({}_{1}\) we have \(A_{n,i}h_{k}=0\) when \(k\neq d+Nd+\sum_{p=1}^{d}(n^{p}-1)N^{d-p}+i\). Set the feed-forward layer \(\text{FFN}=0\). Then the next token embedding matrix can be written as

\[H_{3}=\begin{bmatrix}x^{1}&...&x^{d}&s_{1,1}&...&s_{d,N}&P_{1}\\ 0&...&0&\textbf{0}_{Nd}&...&...&\textbf{0}_{N^{d}d}\\ \mathcal{I}_{1}&...&...&...&...&\textbf{.}&\mathcal{I}_{d+Nd+N^{d}d}\\ 1&...&...&...&...&...&1\end{bmatrix}\]

where \(P_{1}\in\mathbb{R}^{N^{d}d}\) is

\[P_{1} =\begin{bmatrix}p_{\{1\}^{d},1,1}&p_{\{1\}^{d},2,2}&...&p_{\{1 \}^{d},1,d}&p_{\{2\}\times\{1\}^{d-1},1,1}&...&p_{\{N\}^{d},1,d}\end{bmatrix}\] \[=[s_{1,1}&s_{2,1}&...&s_{d,1}&s_{1,2}&...&s_{d,N}]\]

where we recall \(p_{\{1\}^{d},1,1}\) denotes the first term in the base level product for for the patch \(n=\{1,...,1\}\). Now define the next block B\({}_{3}\in\mathcal{B}(N^{d}\frac{d}{2},1)\). Index the corresponding attention heads as \(A_{n,i}\) for \(n\in\{1,...,N\}^{d}\), \(1\leq i\leq\frac{d}{2}\). Define data kernels

\[Q_{n,i}^{data}=\begin{bmatrix}1&0&0&0&0\\ 0&0&0&0\end{bmatrix}\quad K_{n,i}^{data}=\begin{bmatrix}1&0&0&0&0\\ 0&0&0&0\end{bmatrix}\]

where we choose the interaction terms so that the token containing \(p_{n,1,2i-1}\) only interacts with the token containing \(p_{n,1,2i}\) so that they multiply to form \(p_{n,2,i}\) as shown in Figure 7. Note, in addition to computing the product, we must also subtract \(p_{n,1,2i-1}\) from each corresponding token so that the output of the residual from B\({}_{3}\) cancels with the existing terms in the embedding matrix \(H_{3}\). Write

\[A_{n,i}h_{n,i}=\sum_{k=1}^{l}\sigma(\langle Q_{n,i}h_{n,i},K_{n,i}h_{k}\rangle) V_{n,i}h_{k}=\sigma(p_{n,1,2i-1}p_{n,1,2i})e_{2}=\sigma(p_{n,2,i})e_{2}=p_{n,2,i}e _{2}.\]Adding the result onto \(H_{3}\) gives the intermediate embedding matrix

\[H_{3.5}=\begin{bmatrix}x^{1}&...&x^{d}&s_{1,1}&...&s_{d,N}&P_{1}\\ 0&...&0&\textbf{0}_{Nd}&...&...&P_{1.5}\\ \mathcal{I}_{1}&...&...&...&...&...&\mathcal{I}_{d+Nd+N^{d}d}\\ 1&...&...&...&...&...&1\end{bmatrix}\]

where \(P_{1.5}\in\mathbb{R}^{N^{d}d}\) is

\[P_{1.5}=\begin{bmatrix}p_{\{1\}^{d},2,1}&*&p_{\{1\}^{d},2,2}&*&...&p_{\{N\}^{d },2,\frac{d}{2}}&*\end{bmatrix}.\]

Note: \(*\) denotes a non-essential entry which can take any value. We then design our FFN network as \(W_{1}x\) where

\[W_{1}=\begin{bmatrix}-1&1&0&0&0\\ 0&-1&0&0&0\\ 0&0&0&0&0\\ 0&0&0&0&0\\ 0&0&0&0&0\end{bmatrix},\quad b_{1}=0\]

which when added to \(H_{3.5}\) swaps the first component for the second component of each token and zeros out the second component. Then the next embedding matrix \(H_{4}\) gives

\[H_{4}=\begin{bmatrix}*&...&P_{2}\\ \textbf{0}_{d+Nd+N^{d}d}&...&0\\ \mathcal{I}_{1}&...&\mathcal{I}_{d+Nd+N^{d}d}\\ 1&...&1\end{bmatrix}\]

where \(P_{2}=P_{1.5}\in\mathbb{R}^{N^{d}d}\). More generally define the \(k\)th block implementing the \(k\)th level of the parallel product as \(B_{k+2}=\mathcal{B}(N^{d}\frac{d}{2^{k}},\mathcal{F}\mathcal{F}\mathcal{N}(1))\) with attention heads \(A_{n,k,i}\) where \(n\in\{1,...,N\}^{d}\), \(1\leq i\leq\frac{d}{2^{k}}\). We can construct each attention head \(A_{n,k,i}\) so that the token containing \(p_{n,k-1,2i-1}\) only interacts with the token containing \(p_{n,k-1,2i}\) to produce \(p_{n,k,i}\):

\[A_{n,k,i}h_{n,i}=\sum_{t=1}^{l}\sigma(\langle Q_{n,i}h_{n,i},K_{n,i}h_{t} \rangle)V_{n,i}h_{t}=\sigma(p_{n,k-1,2i-1}p_{n,k-1,2i})e_{2}=p_{n,k,i}e_{2}.\]

In the FFN set \(W_{1}\) as before, yielding

\[H_{k+2}=\begin{bmatrix}*&...&P_{k}\\ \textbf{0}_{d+Nd+N^{d}d}&...&0\\ \mathcal{I}_{1}&...&\mathcal{I}_{d+Nd+N^{d}d}\\ 1&...&1\end{bmatrix}\]

where

\[P_{k}=\begin{bmatrix}p_{\{1\}^{d},k,1}&*&...&p_{\{N\}^{d},k,1}&*&...\end{bmatrix}.\]

At \(k=\log(d)\) we have

\[H_{\log(d)+2}=\begin{bmatrix}*&...&P_{\log(d)}\\ \textbf{0}_{d+Nd+N^{d}d}&...&0\\ \mathcal{I}_{1}&...&\mathcal{I}_{d+Nd+N^{d}d}\\ 1&...&1\end{bmatrix}\]

where component \(P_{\log(d)+2}^{d+Nd+\sum_{p=1}^{d}(n^{p}-1)N^{p}+i}=p_{n,\log(d),1}=\phi_{n}(x)\) as desired in (18). It simply now remains to multiply each \(\phi_{n}(x)\) by the corresponding local average \(f_{n}\) and sum the result. This can be implemented with a single attention block \(\text{B}_{\log(d)+3}\in\mathcal{B}(N^{d},1)\). For \(n\in\{1,...,N\}^{d}\) define \(A_{n}\) with the data kernels

\[Q_{n}=\begin{bmatrix}1&0&0&0&0\\ 0&0&0&0&1\end{bmatrix}\quad K_{n}=\begin{bmatrix}0&0&0&0&f_{n}\\ 0&0&0&0&R\end{bmatrix}\]where \(\|f\|_{L^{\infty}([0,1]^{d})}\leq R\) and the token \(h_{d+Nd+\sum_{p=1}^{d}(n^{p}-1)N^{p}+1}=h_{n,1}\) only interacts with itself so that

\[A_{n}h_{n,1} =\sum_{k=1}^{l}\sigma(h_{n,1}^{1}\cdot f_{n}+R+Q_{n}^{I}\mathcal{I }_{n,1}\cdot K_{n}^{I}I_{k}-C)e_{2}\] \[=\sigma(f_{n}\phi_{n}(x)+R)e_{2}=(f_{n}\phi_{n}(x)+R)e_{2}\]

where the last equality comes from \(|f_{n}\phi_{n}(x)|\leq|f_{n}|\leq R\). We implement FFN to subtract \(R\) via

\[W_{1}=\begin{bmatrix}0&0&0&0&0\\ 0&1&0&0&-R\\ 0&0&0&0&0\\ 0&0&0&0&0\\ 0&0&0&0&0\end{bmatrix}\]

so that

\[H_{\log(d)+4}=\begin{bmatrix}*&...&...\\ \mathbf{0}_{d+Nd}&...&P_{\log(d)+4}\\ \mathcal{I}_{1}&...&\mathcal{I}_{d+Nd+Nd}\\ 1&...&1\end{bmatrix}\]

where \(P_{\log(d)+4}^{n,1}=f_{n}\phi_{n}\) for \(n\in\{1,...,N\}^{d}\) and is \(0\) elsewhere. To sum up the resulting terms we define the final transformer block \(\text{B}_{\log(d)+4}\in\mathcal{B}(1,0)\) containing a single attention head \(A_{1}\) as

\[Q=\begin{bmatrix}0&0&0&0&1\\ 0&0&0&0&0\\ 0&0&0&0&0\\ 0&0&0&0&0\\ 0&0&0&0&R\end{bmatrix}\quad K=\begin{bmatrix}0&1&0&0&0\\ 0&0&0&0&0\\ 0&0&0&0&0\\ 0&0&0&0&0\\ 0&0&0&0&0\end{bmatrix}\quad V=\begin{bmatrix}0&0&0&0&1\\ 0&0&0&0&0\\ 0&0&0&0&0\\ 0&0&0&0&0\\ 0&0&0&0&0\end{bmatrix}\]

which when applied to each token simply sums up the second components of all tokens. The result when applied to \(h_{t}\) where \(t=l\) gives

\[A_{1}h_{t} =\sum_{k=1}^{l}\sigma(\langle Qh_{t},Kh_{k}\rangle)Vh_{k}\] \[=\sum_{k=1}^{l}\sigma(h_{k}^{2}+R)e_{1}=(lR+\sum_{n\in\{1,...,N\}^ {d}}f_{n}\phi_{n})e_{1}.\]

We can subtract \(lR\) with the FFN as

\[W_{1}=\begin{bmatrix}1&0&0&0&-lR\\ 0&0&0&0&0\\ 0&0&0&0&0\\ 0&0&0&0&0\\ 0&0&0&0&0\end{bmatrix}\]

yielding the final result \(h_{1}^{2}=\sum_{n}f_{n}\phi_{n}\). Applying the decoding head \(D\) returns the desired result. This shows we can construct the approximation \(\hat{f}\) exactly with an \(O(\log(d))\) layer transformer.

### Proof of Theorem 2

With Lemma 1, we can now move on to the proof of Theorem 2.

Proof of Theorem 2.: Our goal is to approximate \(f:\mathcal{M}\to\mathbb{R}\) in \(L^{\infty}(\mathcal{M})\) with a transformer neural network. Our construction proceeds similarly to the construction in Chen et al. (2019) with feedforward neural networks.

First, we will decompose \(f(x)\) as a sum of terms over local neighborhoods \(U_{1},...,U_{C_{\mathcal{M}}}\subseteq\mathcal{M}\) covering \(\mathcal{M}\). Approximations on overlapping neighborhoods containing \(x\) will then be combined via a partition of unity (**PoU**) \(\{\rho_{n}\}_{n=1}^{C_{\mathcal{M}}}\) which subordinates \(\{U_{n}\}_{n=1}^{C_{\mathcal{M}}}\), while contributions from neighborhoods not containing \(x\) will be zeroed-out via an indicator \(\mathbf{1}_{U_{n}}\). This will give us the expression

\[f(x)=\sum_{n=1}^{C_{\mathcal{M}}}f_{n}(x)\mathbf{1}_{U_{n}}(x)\] (19)

where \(f_{n}:\mathcal{M}\rightarrow\mathbb{R}\) agrees with \(f\) on \(U_{n}\). Next, on each local neighborhood we will project the input \(x\in\mathcal{M}\subseteq\mathbb{R}^{D}\) to \([0,1]^{d}\). This will give us the following _local decomposition_ of the target function:

\[f(x)=\sum_{n=1}^{C_{\mathcal{M}}}\tilde{f}_{n}\circ\phi_{n}(x)\mathbf{1}_{U_{ n}}(x)\]

where \(\tilde{f}_{n}:\mathbb{R}^{d}\rightarrow\mathbb{R}\) and \(\phi_{n}:\mathcal{M}\rightarrow\mathbb{R}^{d}\) is a projection. This step is critical: we now must only approximate a low-dimensional function \(\tilde{f}_{n}\) and a simple projection \(\phi_{n}\) instead of the high-dimensional function \(f_{n}\). After this, we must simply approximate each \(\tilde{f}_{n},\phi_{n},\mathbf{1}_{U_{n}}\) using transformer neural networks.

Step 1: Local DecompositionDenote the open Euclidean ball with center \(c\) and radius \(r\) in \(\mathbb{R}^{D}\) by \(B(c,r)\). Given a manifold \(\mathcal{M}\) satisfying Assumption 1, the collection of open balls \(\{B(c,r)\}_{c\in\mathcal{M}}\) with some \(r\leq\tau/4\) is an open cover of \(\mathcal{M}\). Since \(\mathcal{M}\) is compact, there are a finite number of points, denoted by \(c_{n}\), for \(n=1,\dots,C_{\mathcal{M}}\) such that \(\mathcal{M}\subset\cup_{n=1}^{C_{\mathcal{M}}}B(c_{n},r)\). We can then decompose \(\mathcal{M}\) into \(C_{\mathcal{M}}\) charts \((U_{n},\phi_{n})_{n=1}^{C_{\mathcal{M}}}\) where \(U_{n}=\mathcal{M}\cap B(c_{n},r)\) and each \(\phi_{n}:U_{n}\rightarrow\mathbb{R}^{d}\) is an orthogonal projection of the local neighborhood \(U_{n}\) onto the local tangent space \(T_{c_{n}}(\mathcal{M})\). Via Lemma 8, as long as \(r\leq\tau/4\), each \(\phi_{n}\) is a diffeomorphism between \(U_{n}\) and a subset in \(T_{c_{n}}(\mathcal{M})\). Here the number of charts \(C_{\mathcal{M}}\) is a constant depending on the complexity of the manifold \(\mathcal{M}\). We can bound it as \(C_{\mathcal{M}}\lesssim\frac{SA(\mathcal{M})}{r^{d}}d\log(d)\) where \(SA(\mathcal{M})\) is the surface area of \(\mathcal{M}\)[12].

For the open covering \(\{U_{n}\}_{n=1}^{C_{\mathcal{M}}}\), there exists a smooth partition of unity \(\{\rho_{n}\}_{n=1}^{C_{\mathcal{M}}}\) such that \(f(x)=\sum_{n=1}^{C_{\mathcal{M}}}\rho_{n}(x)f(x)\) for \(x\in\mathcal{M}\)[20]. Each \(\rho_{n}\) is supported only in its corresponding chart \(U_{n}\). We additionally include an indicator function \(\mathbf{1}_{U_{n}}\) for each neighborhood \(U_{n}\), resulting the decomposition

\[f(x)=\sum_{n=1}^{C_{\mathcal{M}}}\underbrace{f(x)\rho_{n}(x)}_{f_{n}(x)} \mathbf{1}_{U_{n}}(x).\]

We then aim to approximate each term in the sum on its corresponding local neighborhood \(U_{n}\) and then sum the result. Because we are only locally approximating each \(\rho_{n}\), the indicator is critical in the approximation as it will be used to suppress contributions from \(U_{n}\) such that the input \(x\notin U_{n}\).

Fix a local neighborhood \(U_{n}\). We will now project the input \(x\in\mathcal{M}\subseteq\mathbb{R}^{D}\) to the local tangent space \(T_{c_{n}}(\mathcal{M})\subseteq[0,1]^{d}\). We can write

\[f_{n}(x)=(f_{n}\circ\phi_{n}^{-1})\circ\phi_{n}(x)=\tilde{f}_{n}\circ\phi_{n}(x)\]

where \(\tilde{f}_{n}:\mathbb{R}^{d}\rightarrow\mathbb{R}\) and again \(\phi_{n}:\mathcal{M}\subseteq\mathbb{R}^{D}\rightarrow\mathbb{R}^{d}\) is an orthogonal projection. Then we can write \(f\) as

\[f(x)=\sum_{n=1}^{C_{\mathcal{M}}}\tilde{f}_{n}\circ\phi_{n}(x)\mathbf{1}_{U_{n }}(x).\] (20)

Step 2: Local ApproximationIn (20), we have rewritten \(f\) as a sum of functions which can be approximated locally on each \(U_{n}\) and indicators \(\mathbf{1}_{U_{n}}\). We will approximate each component with transformer neural networks. Specifically, let \(\text{T}_{\tilde{f}_{n}}\) approximate \(\tilde{f}_{n}\), \(\text{T}_{\phi_{n}}\) approximate \(\phi_{n}\), and \(\text{T}_{\mathbf{1}_{U_{n}}}\) approximate \(\mathbf{1}_{U_{n}}\), \(1\leq n\leq C_{\mathcal{M}}\). Then we will have the overall approximation

\[f(x)\approx\text{T}_{f}(x)=\sum_{n=1}^{C_{\mathcal{M}}}\text{T}_{\tilde{f}_{n}} \circ\text{T}_{\phi_{n}}(x)\text{T}_{\mathbf{1}_{U_{n}}}(x).\]This yields the error decomposition

\[\|f-\text{T}_{f}\|_{L^{\infty}(\mathcal{M})} =\sum_{n=1}^{C_{\mathcal{M}}}\|\tilde{f}_{n}\circ\phi_{n}\mathbf{1}_ {U_{n}}-\text{T}_{f_{n}}\circ\text{T}_{\phi_{n}}\mathbf{T}_{\mathbf{1}_{U_{n}}} \|_{L^{\infty}(\mathcal{M})}\] \[\leq\sum_{n=1}^{C_{\mathcal{M}}}\|\tilde{f}_{n}\circ\phi_{n} \mathbf{T}_{\mathbf{1}_{U_{n}}}-\text{T}_{\tilde{f}_{n}}\circ\text{T}_{\phi_{n }}\mathbf{T}_{\mathbf{1}_{U_{n}}}\|_{L^{\infty}(\mathcal{M})}+\|\tilde{f}_{n} \circ\phi_{n}\mathbf{1}_{U_{n}}-\tilde{f}_{n}\circ\phi_{n}\mathbf{T}_{\mathbf{1 }_{U_{n}}}\|_{L^{\infty}(\mathcal{M})}\] \[\leq\sum_{n=1}^{C_{\mathcal{M}}}\|\tilde{f}_{n}\circ\phi_{n}- \text{T}_{\tilde{f}_{n}}\circ\text{T}_{\phi_{n}}\|_{L^{\infty}(U_{n})}+E_{n,3}\] \[\leq\sum_{n=1}^{C_{\mathcal{M}}}\|\tilde{f}_{n}\circ\phi_{n}- \text{T}_{\tilde{f}_{n}}\circ\phi_{n}\|_{L^{\infty}(U_{n})}+\|\text{T}_{\tilde {f}_{n}}\circ\phi_{n}-\text{T}_{\tilde{f}_{n}}\circ\text{T}_{\phi_{n}}\|_{L^{ \infty}(U_{n})}+E_{n,3}\] \[=\sum_{n=1}^{C_{\mathcal{M}}}E_{n,1}+E_{n,2}+E_{n,3}\]

where the error terms \(E_{n,1},E_{n,2},E_{n,3}\) denote

\[E_{n,1} :=\|\tilde{f}_{n}\circ\phi_{n}-\text{T}_{\tilde{f}_{n}}\circ\phi_ {n}\|_{L^{\infty}(U_{n})}\] (21) \[E_{n,2} :=\|\text{T}_{\tilde{f}_{n}}\circ\phi_{n}-\text{T}_{\tilde{f}_{n} }\circ\text{T}_{\phi_{n}}\|_{L^{\infty}(U_{n})}\] (22) \[E_{n,3} :=\|\tilde{f}_{n}\circ\phi_{n}\mathbf{1}_{U_{n}}-\tilde{f}_{n} \circ\phi_{n}\mathbf{T}_{\mathbf{1}_{U_{n}}}\|_{L^{\infty}(\mathcal{M})}\] (23)

respectively. It remains to construct transformer approximations for each term.

Step 2.1: Approximating \(\tilde{f}_{n}\) and Bounding \(E_{n,1}\)We first handle the \(E_{n,1}\) error in (21). Fix \(1\leq n\leq C_{\mathcal{M}}\). We can write

\[\|\tilde{f}_{n}\circ\phi_{n}-\text{T}_{\tilde{f}_{n}}\circ\phi_{n}\|_{L^{ \infty}(U_{n})}\leq\|\tilde{f}_{n}-\text{T}_{\tilde{f}_{n}}\|_{L^{\infty}([0,1 ]^{d})}\]

so it suffices to approximate \(\tilde{f}_{n}:[0,1]^{d}\rightarrow\mathbb{R}\). Now choose the desired accuracy \(\delta_{n,1}>0\). Via Lemma 1 we can construct an approximation such that

\[\|\tilde{f}_{n}-\text{T}_{\tilde{f}_{n}}\|_{L^{\infty}([0,1]^{d})}<\delta_{n, 1}.\]

This construction \(\text{T}_{\tilde{f}_{n}}\) has \(O(\log(d))\) transformer block layers, \(O(1)\) feed-forward layers, \(O(d\delta_{n,1}^{-\frac{d}{3}})\) tokens, \(O(1)\) embedding dimension, \(O(d\delta_{n,1}^{-\frac{d}{3}})\) attention heads per-layer, and weights with magnitude at most \(O(d\delta_{n,1}^{-\frac{2d}{3}})\). Via the Parallelization Lemma 7 we can compute each approximation of \(\tilde{f}_{n}\) in parallel using a transformer with \(O\big{(}C_{\mathcal{M}}d\delta_{1}^{-\frac{d}{3}}\big{)}\) attention heads and \(O\big{(}C_{\mathcal{M}}d_{embd}\big{)}\) FFN width, where we set \(\delta_{1}=\delta_{1,1}=...=\delta_{C_{\mathcal{M}},1}\), and \(O\big{(}\log(d)\big{)}\) layers.

Step 2.2: Approximating \(\phi_{n}\) Exactly Such That \(E_{n,2}=0\)Fix \(1\leq n\leq C_{\mathcal{M}}\). We must now control the \(E_{n,2}\) error in (22):

\[E_{n,2}=\|\text{T}_{\tilde{f}_{n}}\circ\phi_{n}-\text{T}_{\tilde{f}_{n}}\circ \text{T}_{\phi_{n}}\|_{L^{\infty}(U_{n})}\]

We will implement \(\phi_{n}\) using a transformer block \(\text{B}\in\mathcal{B}(dD,5)\). We can identify the tangent space at the point \(c_{n}\) as

\[T_{c_{n}}(\mathcal{M})=\text{span}(v_{n,1},...,v_{n,d})\]

where the vectors \(v_{n,i}\in\mathbb{R}^{D}\), \(1\leq i\leq d\), form an orthonormal basis of \(T_{c_{n}}(\mathcal{M})\). Then we can write the projection map \(\phi_{n}\) as

\[\phi_{n}(x)=s_{n}(V_{n}^{T}(x-c_{n})+u_{n})\]

where \(s_{n}\in(0,1]\) is a scaling factor, \(u_{n}\in\mathbb{R}^{d}\) is a translation, and \(V_{n}=[v_{n,1}\quad...\quad v_{n,d}]\in\mathbb{R}^{D\times d}\) so that \(\phi_{n}(x)\in[0,1]^{d}\). Suppose we have an input hidden embedding matrix of the form

\[H=\begin{bmatrix}x^{1}&...&x^{D}&\mathbf{0}_{d}\\ 0&...&...&0\\ \mathcal{I}_{1}&...&...&\mathcal{I}_{l}\\ 1&...&...&1\end{bmatrix}\in\mathbb{R}^{d_{embd}\times l}\]We will implement the operation \(v_{n,i}^{j}\cdot(x^{j}-c_{n}^{j})\) via an attention head \(\text{A}_{i,j}\), \(1\leq i\leq d\), \(1\leq j\leq D\). Define the data kernels

\[Q_{i}^{data}=\begin{bmatrix}0&0&0&0&1\\ 0&0&0&0&1\end{bmatrix}\quad K_{i}^{data}=\begin{bmatrix}v_{n,i}^{j}&0&0&0&-v_{n, i}^{j}c_{n}^{j}\\ 0&0&0&0&2M\end{bmatrix}\]

via the Interaction Lemma 3 we can define \(A_{i,j}\) so that token \(h_{D+i}\) interacts with token \(h_{j}\) so that

\[\text{A}_{i,j}(h_{D+i}) =\sigma(\langle Q_{i}^{data}h_{D+i},K_{i}^{data}h_{j}\rangle)\] \[=\sigma(v_{n,i}^{j}x^{j}-v_{n,i}^{j}c_{n}^{j}+2M)\] \[=\sigma(v_{n,i}^{j}(x^{j}-c_{n}^{j})+2M)\] \[=(v_{n,i}^{j}(x^{j}-c_{n}^{j})+2M)e_{1}\]

and is \(0\) on other tokens \(h_{t}\neq h_{D+i}\), where \(M\) bounds \(x\in\mathcal{M}\) is such that \(v_{n,i}^{j}(x^{j}-c_{n}^{j})+2M\geq 0\) for all \(n,i,j\). Then the output of the multi-headed attention MHA on \(h_{D+i}\) is

\[\text{MHA}(h_{D+i}) =\sum_{\begin{subarray}{c}1\leq k\leq d\\ 1\leq j\leq D\end{subarray}}A_{k,j}h_{D+i}=\sum_{j=1}^{D}A_{i,j}h_{D+i}\] \[=\sum_{j=1}^{D}v_{n,i}^{j}(x^{j}-c_{n}^{j})e_{1}+2Me_{1}=\langle v _{n,i},x-c_{n}\rangle e_{1}+2DMe_{1}\]

This yields the intermediate embedding matrix

\[H+\text{MHA}(H)=\begin{bmatrix}x^{1}&...&x^{D}&\langle v_{n,1},x-c_{n} \rangle+2DM&...&\langle v_{n,d},x-c_{n}\rangle+2DM\\ 0&...&...&...&0\\ \mathcal{I}_{1}&...&...&...&...&\mathcal{I}_{l}\\ 1&...&...&...&...&1\end{bmatrix}\]

We can then design a five layer FFN layer to subtract \(2DM\) and then add \(u_{n}\) and multiply \(s_{n}\) only from the embedding tokens \(D+1,...,D+d\). The first layer will simply substract \(2DM\) and then add \(u_{n}\) and multiply \(s_{n}\) from each token. This can be implemented by the matrix

\[W_{1}=\begin{bmatrix}s_{n}&0&0&0&-2DMs_{n}+u_{n}s_{n}\\ 0&0&0&0&0\\ 0&0&1&0&0\\ 0&0&0&1&0\\ 0&0&0&0&1\end{bmatrix}\]

Then, via the gating Lemma 4 we can construct a two-layer feed-forward network \(\text{FFN}_{gating}\) such that, for a chosen \(1\leq t_{u}\leq l\), we have \(\text{FFN}_{gating}(h_{k})=h_{k}\) when \(k<t_{u}\) and \(\text{FFN}_{gating}(h_{k})\) is zero except for the last three rows for \(k\geq t_{u}\). We can again invoke the lemma to construct another FFN layer so that for \(k\geq t_{l}\implies\text{FFN}_{gating}(h_{k})=h_{k}\text{ and }k<t_{l}\implies\text{FFN}_{gating}(h_{k})\) is zero except for the last three rows. So, applying Lemma 4 twice, we can construct a four-layer feed-forward network \(\text{FFN}_{gating}\) such that \(\text{FFN}_{gating}(h_{t})=h_{t}\) for \(D+1\leq D+d\) and is zero except for the last three rows otherwise. This yields the desired output.

\[H^{\prime}=\text{B}(H)=\begin{bmatrix}x^{1}&...&x^{D}&s_{n}(\langle v_{n,1},x -c_{n}\rangle+u_{n})&...&s_{n}(\langle v_{n,d},x-c_{n}\rangle+u_{n})\\ 0&...&...&...&...&0\\ \mathcal{I}_{1}&...&...&...&...&\mathcal{I}_{l}\\ 1&...&...&...&...&1\end{bmatrix}\]

Further, this approximation is **exact**, so that \(E_{n,2}=0\). We can then compute each \(\phi_{n}\) in parallel via the Parallelization Lemma 7 with a transformer having \(O\big{(}C_{\mathcal{M}}dD\big{)}\) attention heads and \(O\big{(}C_{\mathcal{M}}d_{embd}\big{)}\) FFN width.

**Step 2.3: Approximating \(\textbf{1}_{U_{n}}(x)\) and Bounding \(E_{n,3}\)**  Again fix \(1\leq n\leq C_{\mathcal{M}}\). By construction we can write

\[\textbf{1}_{U_{n}}(x)=\begin{cases}1&\|x-c_{n}\|_{2}^{2}<r^{2}\\ 0&\|x-c_{n}\|_{2}^{2}\geq r^{2}\end{cases}\]Suppose we are given an input embedding matrix of the form

\[H_{1}=\begin{bmatrix}x^{1}&...&x^{D}&\mathbf{0}_{D}\\ 0&...&...&0\\ \mathcal{I}_{1}&...&...&\mathcal{I}_{l}\\ 1&...&...&1\end{bmatrix}\in\mathbb{R}^{d_{embd}\times 2D}\]

We must start by computing \(\|x-c_{n}\|_{2}^{2}\). Via the Addition Lemma 5 we can construct a transformer block \(\text{B}_{1}\in\mathcal{B}(D,3)\) such that

\[H_{2}=\text{B}_{1}(H_{1})=\begin{bmatrix}x^{1}&...&x^{D}&x^{1}-c_{n}^{1}&...&x ^{D}-c_{n}^{D}\\ 0&...&...&...&...&0\\ \mathcal{I}_{1}&...&...&...&...&I\mathcal{I}_{l}\\ 1&...&...&...&...&1\end{bmatrix}\]

We now construct transfomer block \(\text{B}_{2}\in\mathcal{B}(D,2)\) squares each term in the sum. For \(1\leq i\leq D\) define the data kernels

\[Q_{i}^{data}=\begin{bmatrix}1&0&0&0&0\\ 0&0&0&0&0\end{bmatrix}\quad K_{i}^{data}=\begin{bmatrix}1&0&0&0&0\\ 0&0&0&0&0\end{bmatrix}\]

Then via the Interaction Lemma 3 we can construct \(\text{A}_{i}\) so that the token \(h_{D+i}\) interacts with itself such that

\[\text{A}_{i}(h_{D+i})=\sigma((x^{i}-c_{n}^{i})^{2})e_{2}=(x^{i}-c_{n}^{i})^{2} e_{2}\]

and \(0\) otherwise. The output of the resulting multi-headed attention gives

\[H_{2}^{\prime}=\begin{bmatrix}x^{1}&...&x^{D}&x^{1}-c_{n}^{1}&...&x^{D}-c_{n} ^{D}\\ 0&...&0&(x^{1}-c_{n}^{1})^{2}&...&(x^{D}-c_{n}^{D})^{2}\\ \mathcal{I}_{1}&...&...&...&...&I_{l}\\ 1&...&...&...&...&1\end{bmatrix}\]

Via Lemmas 6 and 4 we can construct FFN \(\in\mathcal{FFN}(5)\) which replaces the first row with the second row for \(h_{D+1},...,h_{2D}\). This gives the output

\[H_{3}=\text{B}_{2}(H_{2})=\begin{bmatrix}x^{1}&...&x^{D}&(x^{1}-c_{n}^{1})^{ 2}&...&(x^{D}-c_{n}^{D})^{2}\\ 0&...&...&&...&0\\ \mathcal{I}_{1}&...&...&...&...&\mathcal{I}_{l}\\ 1&...&...&...&...&1\end{bmatrix}\]

and it remains only to take the sum. This can be done with \(\text{B}_{3}\in\mathcal{B}(D-1,0)\). For \(1\leq i\leq D-1\) define the data kernels

\[Q_{i}^{data}=\begin{bmatrix}0&0&0&0&1\\ 0&0&0&0&0\end{bmatrix}\quad K_{i}^{data}=\begin{bmatrix}1&0&0&0&0\\ 0&0&0&0&0\end{bmatrix}\]

via the Interaction Lemma 3 we construct \(\text{A}_{i}\) so that \(h_{D+1}\) interacts only with \(h_{D+1+i}\) such that

\[\text{A}_{i}(h_{D+1})=\sigma(\langle Q_{i}^{data}h_{D+1},K_{i}^{data}h_{D+1+i} \rangle)e_{1}=\sigma((x^{i+1}-c_{n}^{i+1})^{2})e_{1}=(x^{i+1}-c_{n}^{i+1})^{2} e_{1}\]

The output of the multi-headed attention is then

\[\text{MHA}(h_{D+1})=\sum_{i=1}^{D-1}\text{A}_{i}(h_{D+1})=\sum_{i=1}^{D-1}(x^{ i+1}-c_{n}^{i+1})e_{1}\]

and \(0\) otherwise. This gives the intermediate output

\[H_{3}^{\prime}=\begin{bmatrix}x^{1}&...&x^{D}&\sum_{i=1}^{D}(x^{i}-c_{n}^{i})^ {2}&...&(x^{D}-c_{n}^{D})^{2}\\ 0&...&...&...&...&0\\ \mathcal{I}_{1}&...&...&...&...&\mathcal{I}_{l}\\ 1&...&...&...&...&1\end{bmatrix}\]

where in particular \(h_{D+1}^{1}=\|x-c_{n}\|_{2}^{2}\). It remains to approximate the indicator

\[\text{1}_{r^{2}}(s)=\begin{cases}1&s<r^{2}\\ 0&s\geq r^{2}\end{cases}\]2.3a: Approximating \(\textbf{1}_{r^{2}}(s)\)Recall our goal is to approximate the function \(f:\mathcal{M}\rightarrow\mathbb{R}\) in \(L^{\infty}\). However, we cannot approximate \(\textbf{1}_{r^{2}}\) in \(L^{\infty}\) as it is a discontinuous function. To address this issue, recall the error term corresponding to the approximation of \(\textbf{1}_{U_{n}}\) is \(E_{n,3}=\|\tilde{f}_{n}\circ\phi_{n}\textbf{1}_{U_{n}}-\tilde{f}_{n}\circ\phi_ {n}\textbf{T}_{\textbf{1}_{U_{n}}}\|_{L^{\infty}(\mathcal{M})}\). We know on the boundary of \(U_{n}\) that \(\tilde{f}_{n}\circ\phi_{n}=f_{n}=f\rho_{n}\) must be \(0\) as \(\rho_{n}\) is \(0\) and further \(f\rho_{n}\) is \(\beta\)-Holder continuous as \(f\) is \(\beta\)-Holder continuous and \(\rho_{n}\) is smooth. This suggests the following approach. Define the approximating indicator

\[\hat{1}_{r^{2},\Delta}(s)=\begin{cases}1&s\leq r^{2}-\Delta\\ 1-\frac{1}{\Delta}s+\frac{r^{2}-\Delta}{\Delta}&r^{2}-\Delta<s<r^{2}\\ 0&s\geq r^{2}\end{cases}\]

for some \(\Delta>0\). Clearly for \(s\leq r^{2}-\Delta\) and \(s\geq r^{2}\) we have \(\hat{\textbf{1}}_{r^{2},\Delta}(s)=\textbf{1}_{r^{2}}(s)\). So we argue \(\tilde{f}_{n}\) is small for \(x\in\mathcal{M}\) in the range \(\mathcal{K}_{n}=\{x\in\mathcal{M}:r^{2}-\Delta<\|x-c_{n}\|_{2}^{2}<r^{2}\}\). This is handled by Lemma 8 in Chen et al. (2022) which gives us the following bound on \(E_{n,3}\):

\[E_{n,3}\leq\frac{c}{r}\Delta\]

for some constant \(c\). So it suffices to make \(\Delta\) small. It then remains to implement \(\hat{1}_{r^{2},\Delta}\). We will do so using a feed-forward layer FFN \(\in\mathcal{FFN}(O(\log(\frac{1}{\Delta})))\). Note that we can write the target exactly as

\[\hat{1}_{r^{2},\Delta}(s)=\sigma\left(1-\sigma\left(\frac{1}{\Delta}(s-(r^{2}- \Delta))\right)\right)\]

while is realized by a two-layer ReLU network. However, the weight \(\frac{1}{\Delta}\) will scale unboundedly with the desired accuracy \(\epsilon\). So we must deepen the FFN to utilize a logarithmic number of layers \(O(\log(\frac{1}{\Delta}))\) as

\[\hat{1}_{r^{2},\Delta}(s)=\sigma\left(1-\sigma\left(\left(\frac{1}{\Delta} \right)^{\frac{1}{\log(\frac{1}{\Delta})}}\circ\sigma...\circ\sigma\left( \left(\frac{1}{\Delta}\right)^{\frac{1}{\log(\frac{1}{\Delta})}}(s-(r^{2}- \Delta)))...\right)\right)\right)\]

which prevents the weights \((\frac{1}{\Delta})^{\frac{1}{\log(\frac{1}{\Delta})}}\) from blowing up. We can then approximate each \(\textbf{1}_{U_{n}}\) in parallel using a transformer with \(O\big{(}C_{\mathcal{M}}D\big{)}\) and three layers.

Step 3: Bringing it all togetherRecall our goal is to approximate \(f\) on \(\mathcal{M}\) by writing \(f\) as

\[f(x)=\sum_{n=1}^{C_{\mathcal{M}}}\tilde{f}_{n}\circ\phi_{n}(x)\textbf{1}_{U_{ n}}(x).\]

We argued we can successfully approximate

1. \(\tilde{f}_{n}\) up to \(\delta_{n,1}\) accuracy in \(L^{\infty}\) via a transformer neural network with \(O(d\delta_{n,1}^{-\frac{d}{2}})\) width and \(O(\log(d))\) depth. All approximations can be computed in parallel via a transformer with \(O\big{(}C_{\mathcal{M}}d\delta_{n,1}^{-\frac{d}{3}}\big{)}\) attention heads and \(O\big{(}\log(d)\big{)}\) layers.
2. Each \(\phi_{n}\) exactly via a transformer neural network with \(O(dD)\) width and constant depth. All the \(\phi_{n}\) can be computed in parallel via a transformer with \(O\big{(}C_{\mathcal{M}}dD\big{)}\) heads and constant depth.
3. Each \(\textbf{1}_{U_{n}}\) up to \(\delta_{n,3}\) accuracy in \(L^{\infty}\) via a transformer with \(O(D)\) width and a feed-forward layer with \(O\big{(}\log(\frac{1}{\Delta})\big{)}\) depth. All \(\textbf{1}_{U_{n}}\) can be approximated in parallel via a transformer with \(O\big{(}C_{\mathcal{M}}D\big{)}\) attention heads and a constant number of transformer blocks.

Via the Parallelization Lemma 7 we can compute the approximations of \(\textbf{1}_{U_{n}}\) and \(\tilde{f}_{n}\circ\phi_{n}\) in parallel via a transformer with \(O\big{(}C_{\mathcal{M}}(d\delta_{n,1}^{-d}+dD)\big{)}\) attention heads, \(O\big{(}C_{\mathcal{M}}d_{embd}\big{)}\) FFN width, and \(O\big{(}\log(d)\big{)}\)transformer blocks. As a result will have the embedding matrix

\[H=\begin{bmatrix}x^{1}&...&x^{D}&c_{f_{1}}&...&c_{f_{C_{\mathcal{M}}}}&c_{U_{1}}&...&c_{U_{\mathcal{CM}}}&*\\ 0&...&...&...&...&...&...&...&...&0\\ \mathcal{I}_{1}&...&...&...&...&...&...&...&...&\mathcal{I}_{L}\\ 1&...&...&...&...&...&...&...&...&...&1\end{bmatrix}\]

where each \(c_{f_{n}}=\bar{f}_{n}\circ\phi_{n}(x)\) and \(c_{U_{n}}=\mathbf{1}_{U_{n}}(x)\). We implement the final sum via two transformer blocks \(\mathbf{B}_{1},\mathbf{B}_{2}\). First define \(\mathbf{B}_{1}\in\mathcal{B}(C_{\mathcal{M}},1)\) with attention heads \(A_{i}\), \(1\leq i\leq C_{\mathcal{M}}\) with data kernels

\[Q_{i}^{data}=\begin{bmatrix}1&0&0&0&0\\ -1&0&0&0&M\end{bmatrix}\quad K_{i}^{data}=\begin{bmatrix}1&0&0&0&0\\ 0&0&0&0&1\end{bmatrix}\]

with interaction kernels chosen so that \(h_{D+i}\) only interacts with \(h_{D+C_{\mathcal{M}}+i}\) under \(A_{i}\). Then

\[A_{i}(h_{D+i})=\sigma(c_{f_{i}}c_{U_{i}}-c_{f_{i}}+M)e_{1}=(c_{f_{i}}c_{U_{i}} -c_{f_{i}}+M)e_{1}\]

We can then design a two-layer FFN to subtract \(M\) from columns \(D+1,...,D+C_{\mathcal{M}}\) yielding the output embedding matrix

\[H=\begin{bmatrix}x^{1}&...&x^{D}&c_{f_{1}}c_{U_{1}}&...&c_{f_{C_{\mathcal{M}}} }c_{U_{\mathcal{CM}}}&*\\ 0&...&...&...&...&...&0\\ \mathcal{I}_{1}&...&...&...&...&...&\mathcal{I}_{L}\\ 1&...&...&...&...&...&1\end{bmatrix}\]

We can sum up the result with another transformer block \(\mathbf{B}_{2}\in\mathcal{B}(C_{\mathcal{M}},2)\) and storing the desired result in the first component of the last token \(t=l\). Applying the decoding layer \(D\) yields the desired result.

It simply remains to control the errors. Recall we have the error bound

\[\|f-\mathbf{T}_{f}\|_{L^{\infty}(\mathcal{M})} \leq\sum_{n=1}^{C_{\mathcal{M}}}E_{n,1}+E_{n,2}+E_{n,3}\] \[\leq\sum_{n=1}^{C_{\mathcal{M}}}\delta_{n,1}+\delta_{n,3}=C_{ \mathcal{M}}\delta_{n,1}+C_{\mathcal{M}}\delta_{n,3}\]

Then for a target accuracy \(\epsilon\) we choose \(\delta_{n,1}=\frac{\epsilon}{2C_{\mathcal{M}}}\) and \(\delta_{n,3}=\frac{\epsilon}{2C_{\mathcal{M}}}\) so that

\[\|f-\mathbf{T}_{f}\|_{L^{\infty}(\mathcal{M})}\leq C_{\mathcal{M}}\delta_{n,1 }+C_{\mathcal{M}}\delta_{n,3}=C_{\mathcal{M}}\frac{\epsilon}{2C_{\mathcal{M}}} +C_{\mathcal{M}}\frac{\epsilon}{2C_{\mathcal{M}}}=\epsilon\]

This completes the proof of Theorem 2.

The above result concludes our approximation theory results. To summarize, we first showed a (sufficiently smooth) function \(f:[0,1]^{d}\rightarrow\mathbb{R}\) can be approximated up to \(\epsilon\) accuracy in \(L^{\infty}\) with a transformer neural network with \(O(d\epsilon^{-d})\) width and \(O(\log(d))\) depth in Lemma 1. We then used this construction to show a function \(f:\mathcal{M}\rightarrow\mathbb{R}\) on a compact manifold \(\mathcal{M}\) with intrinsic dimension \(d\) can be approximated up to \(\epsilon\) accuracy in \(L_{1}\) with a transformer neural network with width \(O(\max(dD,d\epsilon^{-d}))\) and depth \(O(\log(d))\).

## Appendix F Proof of Transformer Generalization Theory

We next prove Theorem 1 in Appendix F.1. Much of this follows the same argument as in Chen et al. [2022] which focused on feedforward neural networks. The main novelty of this paper is a bound on the covering number of \(\mathcal{T}(L_{T},L_{\text{FFN}},w_{\text{FFN}},l,d_{embd},m,R,\kappa)\) given in Lemma 2, which is proved in Appendix F.2.

### Proof of Theorem 1

Proof of Theorem 1.: Suppose we have \(x_{1},...,x_{n}\sim Q\) as i.i.d. training samples drawn from \(Q\) on the manifold \(\mathcal{M}\) and their evaluations \(f(x_{1}),...,f(x_{n})\). Set the approximating function class as \(\mathcal{T}(L_{T},L_{\text{FFN}},w_{\text{FFN}},l,d_{embd},m,R,\kappa)\). We define the transformer empirical risk minimizer \(\hat{\text{T}}_{n}\) in (1) as the transformer network which minimizes the empirical \(L^{2}\) training loss. Our goal is to bound the squared generalization error in (6).

We first rewrite the squared generalization error by adding and subtracting (twice) the training objective where we note a factor of two is used to accelerate convergence of the statistical error.

\[\mathbb{E}\int_{\mathcal{M}}(\hat{\text{T}}_{n}(x)-f(x))^{2}dQ\] \[= 2\mathbb{E}\frac{1}{n}\sum_{i=1}^{n}(\hat{\text{T}}_{n}(x_{i})-f (x_{i}))^{2}+\mathbb{E}\int_{\mathcal{M}}(\hat{\text{T}}_{n}(x)-f(x))^{2}dQ-2 \mathbb{E}\frac{1}{n}\sum_{i=1}^{n}(\hat{\text{T}}_{n}(x_{i})-f(x_{i}))^{2}.\] (24)

The first (bias) term in (24) can be controlled via Theorem 2:

\[\mathbb{E}\frac{1}{n}\sum_{i=1}^{n}(\hat{\text{T}}_{n}(x_{i})-f(x _{i}))^{2} =\mathbb{E}\inf_{T\in\mathcal{T}}\frac{1}{n}\sum_{i=1}^{n}(\text{T }(x_{i})-f(x_{i}))^{2}\] \[\leq\inf_{T\in\mathcal{T}}\mathbb{E}\frac{1}{n}\sum_{i=1}^{n}( \text{T}(x_{i})-f(x_{i}))^{2}\] \[=\inf_{T\in\mathcal{T}}\int_{\mathcal{M}}\big{(}\text{T}(x)-f(x) \big{)}^{2}dQ\] \[\leq\inf_{T\in\mathcal{T}}\int_{\mathcal{M}}\|\text{T}-f\|_{L^{ \infty}(\mathcal{M})}^{2}dQ<\epsilon^{2}\]

where the last line follows from Theorem 2. Note, we can pass the expectation inside the infimum via Jensen's inequality. Set \(d_{T}^{2}(x)=(\text{T}(x)-f(x))^{2}\).

It remains to control the last two (variance) terms in (24). We will do so by controlling the covering number of the approximating function class \(\mathcal{T}(L_{T},L_{\text{FFN}},w_{\text{FFN}},l,d_{embd},m,R,\kappa)\). Via Lemma 6 from Chen et al. [2022] we have the bound

\[\mathbb{E}\int_{\mathcal{M}}(\hat{\text{T}}_{n}(x)-f(x))^{2}dQ-2 \mathbb{E}\frac{1}{n}\sum_{i=1}^{n}(\hat{\text{T}}_{n}(x_{i})-f(x_{i}))^{2}\] \[\leq\inf_{\delta>0}\left[\frac{104R^{2}}{3n}\log\mathcal{N}\left( \frac{\delta}{4R},\mathcal{T},\|\cdot\|_{\infty}\right)+(4+\frac{1}{2R})\delta\right]\] (25)

where \(\mathcal{N}(\frac{\delta}{4R},\mathcal{T},\|\cdot\|_{\infty})\) denotes of the covering number of the network class \(\mathcal{T}\) under the \(L^{\infty}\) norm. Our goal is now to bound this covering number of \(\mathcal{T}(L_{T},L_{\text{FFN}},w_{\text{FFN}},l,d_{embd},m,R,\kappa)\). We do so with the following lemma:

**Lemma 2**.: _Consider a transformer neural network class \(\mathcal{T}=\mathcal{T}(L_{T},L_{\text{FFN}},w_{\text{FFN}},l,d_{embd},m,R,\kappa)\) with input \(x\in\mathbb{R}^{D}\) satisfying \(\|x\|_{\infty}\leq M.\). Let \(\delta>0\). Then_

\[\mathcal{N}(\delta,\mathcal{T},\|\cdot\|_{\infty})\leq\left(\frac{2^{L_{T}^{2 }+1}L_{\text{FFN}}M^{3L_{T}}d_{embd}^{18L_{T}^{2}}w_{\text{FFN}}^{18L_{T}^{2}L_ {\text{FFN}}}{}^{L_{\text{FFN}}}{}^{L_{\text{FFN}}}{}^{L_{\text{FFN}}}{}^{L_{ \text{FFN}}}{}^{L_{\text{FFN}}}m^{L_{T}^{2}}l_{T}^{L_{T}^{2}}}{\delta}\right)^{ 4d_{embd}^{2}w_{\text{FFN}}^{2}D(m+L_{\text{FFN}})L_{T}}.\]

Lemma 2 is proved in Appendix F.2.

With the covering number of \(\mathcal{T}(L_{T},L_{\text{FFN}},w_{\text{FFN}},l,d_{embd},m,R,\kappa)\) in hand we may now apply (25) to obtain

\[\mathbb{E}\int_{\mathcal{M}}(\hat{\text{T}}_{n}(x)-f(x))^{2}dQ-2 \mathbb{E}\frac{1}{n}\sum_{i=1}^{n}(\hat{\text{T}}_{n}(x_{i})-f(x_{i}))^{2}\] \[\leq \inf_{\delta>0}\left[\frac{104R^{2}}{3n}\log\mathcal{N}\left( \frac{\delta}{4R},\mathcal{T},\|\cdot\|_{\infty}\right)+\left(4+\frac{1}{2R} \right)\delta\right]\] \[\leq \frac{104R^{2}}{3n}\log\mathcal{N}\left(\frac{1}{4Rn},\mathcal{T},\|\cdot\|_{\infty}\right)+\left(4+\frac{1}{2R}\right)\frac{1}{n}\]where we set \(\delta=\frac{1}{n}\). This yields

\[\mathcal{N}\left(\frac{\delta}{4Rn},\mathcal{T},\|\cdot\|_{\infty}\right) \leq\log\left(\big{(}2^{L_{T}^{2}+1}L_{\text{FFN}}M^{3L_{T}}d_{embd }^{18L_{T}^{2}}w_{\text{FFN}}^{18L_{T}^{2}L_{\text{FFN}}}\kappa^{6L_{T}^{2}L_{ \text{FFN}}}m^{L_{T}^{2}}L_{T}^{L_{T}}n\big{)}^{4d_{embd}^{2}w_{\text{FFN}}^{2} D(m+L_{\text{FFN}})L_{T}}\right)\] \[\leq 4d_{embd}^{2}w_{\text{FFN}}^{2}D(m+L_{\text{FFN}})L_{T}(5L_{T }^{2}L_{\text{FFN}}\log(2ML_{\text{FFN}}d_{embd}\kappa mln))\] \[\leq 20\log(2ML_{\text{FFN}}d_{embd}w_{\text{FFN}}\kappa mln)Dd_{ embd}^{2}w_{\text{FFN}}^{2}mL_{T}^{3}L_{\text{FFN}}^{2}.\]

Recall for target accuracy \(\epsilon>0\) we can choose \(L_{T}=\log(d)\), \(L_{\text{FFN}}=\log(\epsilon^{-1})\), \(w_{\text{FFN}}\leq 4C_{\mathcal{M}}\), \(d_{embd}=5\), \(l\leq C_{\mathcal{M}}d\epsilon^{-\frac{\delta}{\beta}}\), \(m\leq C_{\mathcal{M}}d\epsilon^{-\frac{\delta}{\beta}}\), \(\kappa\leq(MC_{\mathcal{M}}d\epsilon^{-\frac{\delta}{\beta}})^{2},l\leq C_{ \mathcal{M}}d\epsilon^{-\frac{\delta}{\beta}}\). This simplifies the above to

\[\mathcal{N}(\frac{\delta}{4R},\mathcal{T},\|\cdot\|_{\infty})\leq 900\big{(} \log(d)^{2}\log(60d^{3}\epsilon^{-4\frac{\delta}{\beta}}n)\big{)}C_{\mathcal{M }}^{3}Dd\epsilon^{-\frac{\delta}{\beta}}\leq\tilde{O}\big{(}Dd^{2}\epsilon^{- \frac{\delta}{\beta}}\big{)}\]

where \(\tilde{O}\) hides \(\log\) terms and constants. Then we can bound the variance term as

\[\mathbb{E}\int_{\mathcal{M}}(\hat{\Upsilon}_{n}(x)-f(x))^{2}dQ-2\mathbb{E} \frac{1}{n}\sum_{i=1}^{n}(\hat{\Upsilon}_{n}(x_{i})-f(x_{i}))^{2}\leq\tilde{O }\big{(}\frac{Dd^{2}\epsilon^{-\frac{\delta}{\beta}}}{n}\big{)}\]

Putting together the bounds on the bias and variance yields a bound on the empirical risk:

\[\mathbb{E}\int_{\mathcal{M}}(\hat{\Upsilon}_{n}(x)-f(x))^{2}dQ\leq\tilde{O} \big{(}\epsilon^{2}+\frac{Dd^{2}\epsilon^{-\frac{\delta}{\beta}}}{n}\big{)}\]

and it simply remains to pick \(\epsilon\) to balance the error terms. We do this by choosing \(\epsilon\) such that \(\epsilon^{2}=\frac{\epsilon^{-\frac{\delta}{\beta}}}{n}\) yielding \(\epsilon=n^{-\frac{\delta}{2\beta+d}}\). This gives our final bound on the expected empirical risk:

\[\mathbb{E}\int_{\mathcal{M}}(\hat{\Upsilon}_{n}(x)-f(x))^{2}dQ-2\mathbb{E} \frac{1}{n}\sum_{i=1}^{n}(\hat{\Upsilon}_{n}(x_{i})-f(x_{i}))^{2}\leq\tilde{O }\big{(}Dd^{2}n^{-\frac{2\delta}{2\beta+d}}\big{)}\]

as desired.

### Proof of Lemma 2 about the Transformer Covering Number

Proof of Lemma 2.: The key is to bound the difference in \(\|\cdot\|_{\infty}\) between two transformers \(\Upsilon,\Upsilon^{\prime}\in\mathcal{T}(L_{T},L_{\text{FFN}},w_{\text{FFN}},l, d_{embd},m,R,\kappa)\). Set \(\eta>0\) and choose \(\Upsilon,\Upsilon^{\prime}\) so that \(\|\theta-\theta^{\prime}\|_{\infty}<\eta\). In other words, the weight parameters in \(\Upsilon\) and \(\Upsilon^{\prime}\) differ at most by \(\eta\). Fix \(x\in[0,1]^{D}\). Recall the decoder \(D:\mathbb{R}^{d_{embd}\times l}\rightarrow\mathbb{R}\) is fixed to output the first element of the first row. We compute

\[|\Upsilon(x)-\Upsilon^{\prime}(x)| =|\mathbf{D}\circ\mathbf{B}_{L}\circ...\circ\mathbf{B}_{1}\circ( \text{PE}+\text{E}(x))-\mathbf{D}^{\prime}\circ\mathbf{B}^{\prime}_{L}\circ...\circ\mathbf{B}^{\prime}_{1}\circ(\text{PE}+\text{E}^{\prime}(x))|\] \[\leq\|\mathbf{B}_{L}\circ...\circ\mathbf{B}_{1}\circ(\text{PE}+ \text{E}(x))-\mathbf{B}^{\prime}_{L}\circ...\circ\mathbf{B}^{\prime}_{1}\circ( \text{PE}+\text{E}^{\prime}(x))\|_{\infty}.\]

To handle this term, we write

\[\|\mathbf{B}_{L}\circ...\circ\mathbf{B}_{1}\circ(\text{PE}+\text{ E}(x))-\mathbf{B}^{\prime}_{L}\circ...\circ\mathbf{B}^{\prime}_{1}\circ(\text{PE}+ \text{E}^{\prime}(x))\|_{\infty}\] \[\leq\|\mathbf{B}_{L}\circ...\circ\mathbf{B}_{1}\circ(\text{PE}+ \text{E}(x))-\mathbf{B}_{L}\circ...\circ\mathbf{B}_{1}\circ(\text{PE}+\text{E} ^{\prime}(x))\|_{\infty}\] \[+\|\mathbf{B}_{L}\circ...\circ\mathbf{B}_{1}\circ(\text{PE}+ \text{E}^{\prime}(x))-\mathbf{B}^{\prime}_{L}\circ...\circ\mathbf{B}^{\prime}_{1} \circ(\text{PE}+\text{E}^{\prime}(x))\|_{\infty}.\]

We will first bound the second term.

Consider two multi-headed attention layers \(\text{MHA}_{1},\text{MHA}_{2}\in\mathcal{MHA}(m)\) with attention heads \(A^{i}_{j}\), \(i\in\{1,2\}\), \(1\leq j\leq m\). For \(H\in[0,M]^{d_{embd}\times l}\) we compute

\[\|\text{MHA}_{1}(H)-\text{MHA}_{2}(H)\|_{\infty} =\|\sum_{j=1}^{m}A^{1}_{j}(H)-\sum_{j=1}^{m}A^{2}_{j}(H)\|_{\infty} =\|\sum_{j=1}^{m}\big{[}A^{1}_{j}(H)-A^{2}_{j}(H)\big{]}\|_{\infty}\] \[\leq\sum_{j=1}^{m}\|A^{1}_{j}(H)-A^{2}_{j}(H)\|_{\infty}\]Consider two attention heads \(A_{1},A_{2}\) with weight parameters \(Q_{i},K_{i},V_{i}\), \(i\in\{1,2\}\). For \(H\in[-M,M]^{d_{embd}\times l}\), we compute

\[\|A_{1}(H)-A_{2}(H)\|_{\infty} \leq\max_{1\leq j\leq l}\|A_{1}(h_{j})-A_{2}(h_{j})\|_{\infty}\] \[=\max_{1\leq j\leq l}\|\sum_{k=1}^{l}\sigma(\langle Q_{1}h_{j},K_ {1}h_{k}\rangle)V_{1}h_{k}-\sum_{k=1}^{l}\sigma(\langle Q_{2}h_{j},K_{2}h_{k} \rangle)V_{2}h_{k}\|_{\infty}\] \[\leq\max_{1\leq j\leq l}\sum_{k=1}^{l}\|\sigma(\langle Q_{1}h_{j},K_{1}h_{k}\rangle)V_{1}h_{k}-\sigma(\langle Q_{2}h_{j},K_{2}h_{k}\rangle)V_{2 }h_{k}\|_{\infty}\] \[\leq\max_{1\leq j\leq l}\sum_{k=1}^{l}\|\sigma(\langle Q_{1}h_{j},K_{1}h_{k}\rangle)V_{1}h_{k}-\sigma(\langle Q_{2}h_{j},K_{2}h_{k}\rangle)V_{1 }h_{k}\|_{\infty}\] \[+\|\sigma(\langle Q_{2}h_{j},K_{2}h_{k}\rangle)V_{1}h_{k}-\sigma (\langle Q_{2}h_{j},K_{2}h_{k}\rangle)V_{2}h_{k}\|_{\infty}\]

where the last inequality comes from adding and subtracting the term \(\langle Q_{2}h_{j},K_{2}h_{k}\rangle V_{1}h_{k}\). We bound the first term via

\[\|\sigma(\langle Q_{1}h_{j},K_{1}h_{k}\rangle)V_{1}h_{k}-\sigma( \langle Q_{2}h_{j},K_{2}h_{k}\rangle)V_{1}h_{k}\|_{\infty}\] \[= |\langle Q_{1}h_{j},K_{1}h_{k}\rangle-\langle Q_{2}h_{j},K_{2}h_ {k}\rangle|\|V_{1}h_{k}\|_{\infty}\] \[\leq \big{[}|\langle Q_{1}h_{j},K_{1}h_{k}\rangle-\langle Q_{1}h_{j},K_ {2}h_{k}\rangle|+|\langle Q_{1}h_{j},K_{2}h_{k}\rangle-\langle Q_{2}h_{j},K_{2} h_{k}\rangle|\big{]}\|V_{1}h_{k}\|_{\infty}\] \[\leq \big{[}|\langle Q_{1}h_{j},K_{1}h_{k}-K_{2}h_{k}\rangle|+|\langle Q _{1}h_{j}-Q_{2}h_{j},K_{2}h_{k}\rangle|\big{]}\|V_{1}h_{k}\|_{\infty}\] \[\leq \big{[}\|Q_{1}h_{j}\|_{2}\|K_{1}h_{k}-K_{2}h_{k}\|_{2}+\|Q_{1}h_{ j}-Q_{2}h_{j}\|_{2}\|K_{2}h_{k}\|_{2}\big{]}\|V_{1}h_{k}\|_{\infty}\] \[\leq \big{[}\|Q_{1}\|_{1}\|h_{j}\|_{\infty}\|K_{1}-K_{2}\|_{1}\|h_{k} \|_{\infty}+\|Q_{1}-Q_{2}\|_{1}\|h_{j}\|_{\infty}\|K_{2}\|_{1}\|h_{k}\|_{ \infty}\big{]}\|V_{1}h_{k}\|_{\infty}\] \[\leq \big{[}2\kappa^{2}d_{embd}^{4}M^{2}\eta\big{]}\|V_{1}h_{k}\|_{\infty}\] \[\leq 2\kappa^{3}d_{embd}^{4}M^{3}\eta\]

where we note \(\|Ax\|_{2}\leq\|Ax\|_{1}\leq\|A\|_{1}\|x\|_{\infty}\) for \(A\in\mathbb{R}^{d_{embd}\times d_{embd}},x\in\mathbb{R}^{d_{embd}}\) and \(\|A\|_{1}\leq d_{embd}^{2}\|A\|_{\infty}\). This bounds the first term.

The second term can be bounded as

\[\|\sigma(\langle Q_{2}h_{j},K_{2}h_{k}\rangle)V_{1}h_{k}-\sigma( \langle Q_{2}h_{j},K_{2}h_{k}\rangle)V_{2}h_{k}\|_{\infty}\] \[= \|V_{1}-V_{2}\|_{1}\|\langle Q_{2}h_{j},K_{2}h_{k}\rangle h_{k}\| _{\infty}\] \[\leq d_{embd}^{2}\kappa\eta\|\langle Q_{2}h_{j},K_{2}h_{k}\rangle h_{k }\|_{\infty}\] \[\leq d_{embd}^{2}\kappa\eta\|Q_{2}h_{j}\|_{2}\|K_{2}h_{k}\|_{2}\|h_{k}\| _{\infty}\] \[\leq d_{embd}^{6}\kappa^{3}M^{3}\eta.\]

Thus we can bound the attention difference \(\|A_{1}(H)-A_{2}(H)\|_{\infty}\) as

\[\|A_{1}(H)-A_{2}(H)\|_{\infty} \leq\max_{1\leq j\leq l}\sum_{k=1}^{l}\|\langle Q_{1}h_{j},K_{1}h _{k}\rangle V_{1}h_{k}-\langle Q_{2}h_{j},K_{2}h_{k}\rangle V_{1}h_{k}\|_{\infty}\] \[+\|\langle Q_{2}h_{j},K_{2}h_{k}\rangle V_{1}h_{k}-\langle Q_{2}h _{j},K_{2}h_{k}\rangle V_{2}h_{k}\|_{\infty}\] \[\leq\max_{1\leq j\leq l}\sum_{k=1}^{l}3\kappa^{3}d_{embd}^{6}M^{3} \eta=3\kappa^{3}d_{embd}^{6}M^{3}\eta.\]

We can also now bound the multi-headed difference \(\|\text{MHA}_{1}(H)-\text{MHA}_{2}(H)\|_{\infty}\) as

\[\|\text{MHA}_{1}(H)-\text{MHA}_{2}(H)\|_{\infty}\leq\sum_{j=1}^{m}\|A_{j}^{1}(H) -A_{j}^{2}(H)\|_{\infty}\leq 3\kappa^{3}d_{embd}^{6}M^{3}\eta=3\kappa^{3}d_{embd}^{6}M^{3}ml\eta.\]

A bound on the first residual layer in the transformer block immediately follows via

\[\|(H+\text{MHA}_{1}(H))-(H+\text{MHA}_{2}(H))\|_{\infty}=\|\text{MHA}_{1}(H)- \text{MHA}_{2}(H)\|_{\infty}.\]To bound the difference in the output of the FFN layer in the transformer block we have for \(f_{1},f_{2}\in\text{FFN}(L_{\text{FFN}})\) and input \(H\in[0,M^{\prime}]^{d_{embd}\times l}\) we have

\[\|f_{1}(H)-f_{2}(H)\|_{\infty}\leq L_{\text{FFN}}(d_{embd}M^{\prime}+2)(\kappa d _{embd})^{L_{\text{FFN}}-1}\eta\]

which also bounds the second residual layer of the transformer block. Then to bound the difference of two transformer blocks \(\text{B}_{1},\text{B}_{2}\in\mathcal{B}(m,L_{\text{FFN}})\) write, for \(H\in[0,M]^{d_{embd}\times l}\),

\[\|\text{B}_{1}(H)-\text{B}_{2}(H)\|_{\infty} =\|\big{(}H+\text{MHA}_{1}(H)+\text{FFN}_{1}(H+\text{MHA}_{1}(H)) \big{)}\] \[-\big{(}H+\text{MHA}_{2}(H)+\text{FFN}_{2}(H+\text{MHA}_{2}(H)) \big{)}\|_{\infty}\] \[=\|\text{MHA}_{1}(H)+\text{FFN}_{1}(H+\text{MHA}_{1}(H))-\text{ MHA}_{2}(H)+\text{FFN}_{2}(H+\text{MHA}_{2}(H))\|_{\infty}\] \[\leq\|\text{MHA}_{1}(H)-\text{MHA}_{2}(H)\|_{\infty}\] \[+\|\text{FFN}_{1}(H+\text{MHA}_{1}(H))-\text{FFN}_{2}(H+\text{ MHA}_{2}(H))\|_{\infty}\] \[\leq 3\kappa^{3}d_{embd}^{3}M^{3}ml\eta+\|\text{FFN}_{1}(H+\text{ MHA}_{1}(H))-\text{FFN}_{2}(H+\text{MHA}_{2}(H))\|_{\infty}\]

where we can bound the first term via analysis above. For the second term write

\[\|\text{FFN}_{1}(H+\text{MHA}_{1}(H))-\text{FFN}_{2}(H+\text{MHA}_ {2}(H))\|_{\infty}\] \[\leq \|\text{FFN}_{1}(H+\text{MHA}_{1}(H))-\text{FFN}_{1}(H+\text{MHA} _{2}(H))\|_{\infty}+\|\text{FFN}_{2}(H+\text{MHA}_{2}(H))-\text{FFN}_{1}(H+ \text{MHA}_{2}(H))\|_{\infty}.\]

To handle the first term we must bound he Lipschitz constant of \(\text{FFN}_{1}\). Let \(H,H^{\prime}\in\mathbb{R}^{d_{embd}\times l}\). Write

\[\|\text{FFN}_{1}(H)-\text{FFN}_{1}(H^{\prime})\|_{\infty} =\max_{1\leq j\leq l}\|\text{FFN}_{1}(h_{j})-\text{FFN}_{1}(h^{ \prime}_{j})\|_{\infty}\] \[=\max_{1\leq j\leq l}\|W_{L_{\text{FFN}}}\sigma(W_{L_{\text{FFN}} -1}...\sigma(W_{1}h_{j}+b_{1})...+b_{L_{\text{FFN}}-1})+b_{L_{\text{FFN}}}\] \[-W_{L_{\text{FFN}}}\sigma(W_{L_{\text{FFN}}-1}...\sigma(W_{1}h^{ \prime}_{j}+b_{1})...+b_{L_{\text{FFN}}-1})-b_{L_{\text{FFN}}}\|_{\infty}\] \[\leq\max_{1\leq j\leq l}\|W_{L_{\text{FFN}}}\|_{1}\|W_{L_{\text{FFN }}-1}...\sigma(W_{1}h_{j}+b_{1})...+b_{L_{\text{FFN}}-1}\] \[-W_{L_{\text{FFN}}-1}...\sigma(W_{1}h^{\prime}_{j}+b_{1})...+b_{L _{\text{FFN}}-1}\|_{\infty}\] \[\leq\max_{1\leq j\leq l}\bigg{[}\prod_{i=1}^{L_{\text{FFN}}}\|W_{ i}\|_{1}\bigg{]}\|h_{j}-h^{\prime}_{j}\|_{\infty}\leq w_{\text{FFN}}^{2L_{ \text{FFN}}}\kappa^{L_{\text{FFN}}}\|H-H^{\prime}\|_{\infty}\]

where we again use that \(\|Wx\|_{\infty}\leq\|W\|_{1}\|x\|_{\infty}\) and that \(\sigma(\cdot)\) is 1-Lipschitz. Applying this to the first term from above we get

\[\|\text{FFN}_{1}(H+\text{MHA}_{1}(H))-\text{FFN}_{1}(H+\text{MHA} _{2}(H))\|_{\infty} \leq w_{\text{FFN}}^{2L_{\text{FFN}}}\kappa^{L_{\text{FFN}}}\| \text{MHA}_{1}(H)-\text{MHA}_{2}(H)\|_{\infty}\] \[\leq w_{\text{FFN}}^{2L_{\text{FFN}}}\kappa^{L_{\text{FFN}}}3 \kappa^{3}d_{embd}^{3}ml\eta\] \[=3\kappa^{3+L_{\text{FFN}}}w_{\text{FFN}}^{2L_{\text{FFN}}}d_{ embd}^{3}ml\eta.\]

We can bound \(\|\text{FFN}_{2}(H+\text{MHA}_{2}(H))-\text{FFN}_{1}(H+\text{MHA}_{2}(H))\|_{\infty}\) as

\[\|\text{FFN}_{2}(H+\text{MHA}_{2}(H))-\text{FFN}_{1}(H+\text{MHA} _{2}(H))\|_{\infty}\] \[\leq \|\text{FFN}_{2}(H+\text{MHA}_{2}(H))-\text{FFN}_{1}(H+\text{MHA} _{2}(H))\|_{\infty}\leq L_{\text{FFN}}(w_{\text{FFN}}M^{\prime}+2)(\kappa w_{ \text{FFN}})^{L_{\text{FFN}}-1}\eta\] \[\leq \|\text{FFN}_{2}(H+\text{MHA}_{2}(H))-\text{FFN}_{1}(H+\text{MHA} _{2}(H))\|_{\infty}\leq L_{\text{FFN}}(w_{\text{FFN}}M^{\prime}+2)(\kappa w_{ \text{FFN}})^{L_{\text{FFN}}-1}\eta\]

where \(\|H+\text{MHA}_{2}(H)\|_{\infty}\leq M^{\prime}\). Putting all the estimates together, we have

\[\|\text{B}_{1}(H)-\text{B}_{2}(H)\|_{\infty}\] \[\leq 3\kappa^{3}d_{embd}^{6}M^{3}ml\eta+\|\text{FFN}_{1}(H+\text{MHA} _{1}(H))-\text{FFN}_{2}(H+\text{MHA}_{2}(H))\|_{\infty}\] \[\leq 3\kappa^{3}d_{embd}^{6}M^{3}ml\eta+3\kappa^{3+L_{\text{FFN}}}w_{ \text{FFN}}^{2L_{\text{FFN}}}d_{embd}^{6}M^{3}ml\eta+L_{\text{FFN}}(w_{\text{FFN}} M^{\prime}+2)(\kappa w_{\text{FFN}})^{L_{\text{FFN}}-1}\eta\] \[\leq \big{(}4\kappa^{3+L_{\text{FFN}}}w_{\text{FFN}}^{2L_{\text{FFN}}}d _{embd}^{6}M^{3}ml+L_{\text{FFN}}(w_{\text{FFN}}M^{\prime}+2)(\kappa w_{\text{FFN}}) ^{L_{\text{FFN}}-1}\big{)}\eta\]

and it remains to control \(M^{\prime}\) in terms of \(M\). We know \(\|H+\text{MHA}(H)\|_{\infty}\leq M^{\prime}\) and so we compute

\[\|H+\text{MHA}(H)\|_{\infty} \leq M+\|\text{MHA}(H)\|_{\infty}\] \[\leq M+\sum_{j=1}^{m}\max_{1\leq i\leq l}\sum_{k=1}^{l}\|\langle Q _{j}h_{i},K_{j}h_{k}\rangle V_{j}h_{k}\|_{\infty}\] \[\leq M+mld_{embd}^{6}\kappa^{3}M\leq 2d_{embd}^{6}\kappa^{3}mlM.\]

[MISSING_PAGE_FAIL:35]

Applying this gives

\[\|\text{MHA}(H)-\text{MHA}(H^{\prime})\|_{\infty}\leq 4d_{embd}^{6}\kappa^{3}M ^{2}ml\|H-H^{\prime}\|_{\infty}.\]

Then

\[\|\text{B}(H)-\text{B}(H^{\prime})\|_{\infty} \leq w_{\text{FFN}}^{2L_{\text{FFN}}}\kappa^{L_{\text{FFN}}}\|H-H^ {\prime}\|_{\infty}+(w_{\text{FFN}}^{2L_{\text{FFN}}}\kappa^{L_{\text{FFN}}}+1) \|\text{MHA}(H)-\text{MHA}(H^{\prime})\|_{\infty}\] \[\leq w_{\text{FFN}}^{2L_{\text{FFN}}}\kappa^{L_{\text{FFN}}}\|H-H^ {\prime}\|_{\infty}+(w_{\text{FFN}}^{2L_{\text{FFN}}}\kappa^{L_{\text{FFN}}}+1) 4d_{embd}^{6}\kappa^{3}M^{2}ml\|H-H^{\prime}\|_{\infty}\] \[\leq 8w_{\text{FFN}}^{2L_{\text{FFN}}}\kappa^{L_{\text{FFN}}}d_{ embd}^{6}\kappa^{3}M^{2}ml\|H-H^{\prime}\|_{\infty}.\]

With this we can bound the first term of the multi-block difference

\[\|\text{B}_{L_{T}}\circ...\circ\text{B}_{1}(H)-\text{B}_{L_{T}} \circ...\circ\text{B}^{\prime}_{1}(H)\|_{\infty} \leq 8d_{embd}^{2L_{\text{FFN}}+6}\kappa^{L_{\text{FFN}}}d_{ embd}^{6}\kappa^{3}\|B_{L_{T}-1}\circ...\circ B_{1}(H)\|_{\infty}^{2}ml\] \[\leq 8d_{embd}^{2L_{\text{FFN}}+6}\kappa^{L_{\text{FFN}}}d_{ embd}^{6}\kappa^{3}((4d_{embd}^{2L_{\text{FFN}}+4}\kappa^{L_{\text{FFN}}+1}ml)^{L_{T}-1 }M)^{2}ml\] \[\leq 2^{7}d_{embd}^{5L_{\text{FFN}}}\kappa^{2L_{\text{FFN}}}d_{ embd}^{6}\kappa^{3}M^{2}m^{L_{T}}l^{L_{T}}\] \[\leq 8d_{embd}^{2}w_{\text{FFN}}^{2}D(m+L_{\text{FFN}})L_{T}.\]
We can compute the number of steps per parameter as \(\frac{2\kappa}{\eta}\). Thus the covering number of \(\mathcal{T}(L_{T},L_{\text{FN}},w_{\text{FN}},l,d_{embd},m,R,\kappa)\) is bounded by

\[\mathcal{N}(\mathcal{T},\delta,\|\cdot\|_{\infty}) \leq\big{(}\frac{\kappa}{\eta}\big{)}^{4d_{embd}^{2}w_{\text{FN}} ^{2}D(m+L_{\text{HN}})L_{T}}\] \[=\big{(}\frac{C_{T}\kappa}{\delta}\big{)}^{4d_{embd}^{2}w_{\text{ FN}}^{2}D(m+L_{\text{HN}})L_{T}}\] \[=\big{(}\frac{2^{L_{T}^{2}+1}L_{\text{FFN}}M^{3L_{T}}d_{embd}^{18L _{T}^{2}}\frac{18L_{T}^{2}L_{\text{FN}}}{\kappa}^{6L_{T}^{2}L_{\text{HN}}}m^{L _{T}^{2}}l_{T}^{L_{T}^{2}}}{\delta}\big{)}^{4d_{embd}^{2}w_{\text{FN}}^{2}D(m+L _{\text{HN}})L_{T}}.\]

## Appendix G Building Blocks of Transformer Neural Networks

**Lemma 3** (Interaction Lemma).: _Set \(d_{embd}=5\) and let \(\kappa,M>0\). Fix \(l\in\mathbb{N}\), \(1\leq t_{1},t_{2}\leq l\), and \(1\leq i\leq d_{embd}\). Let \(H\in\mathbb{R}^{d_{embd}\times l}\) be a structured transformer embedding matrix such that \(h_{t}^{d_{embd}-2:d_{embd}-1}=\mathcal{I}_{t}\) and \(h_{t}^{d_{embd}}=1\). Further suppose \(\|H\|_{\infty}\leq M\) for some \(M>0\). Let \(B:\mathbb{R}^{d_{embd}}\times\mathbb{R}^{d_{embd}}\rightarrow\mathbb{R}\) be a kernel function on tokens which can be written in the form \(B(h,h^{\prime})=\sigma(\langle Q^{B}h,K^{B}h^{\prime}\rangle)\) for some \(h,h^{\prime}\in\mathbb{R}^{d_{embd}},Q^{B},K^{B}\in\mathbb{R}^{(d_{embd}-3) \times d_{embd}}\) with \(\|Q^{B}\|_{\infty,\infty},\|K^{B}\|_{\infty,\infty}\leq\kappa\). Then we can construct an attention head \(\mathrm{A}\) acting on \(H\) such that \(\mathrm{A}(h_{t_{1}})=B(h_{t_{1}},\widetilde{h}_{t_{2}})e_{i}\) and otherwise \(\mathrm{A}(h_{t})=0\) for \(t\neq t_{1}\). Further we have \(\|\theta_{\mathrm{A}}\|_{\infty}=O(d_{embd}^{4}\kappa^{2}M^{2}l^{2})\). Note: we call the matrices \(Q^{B},K^{B}\) "data kernels"._

Proof of Lemma 3.: Define the query, key, and value matrices as

\[Q=\begin{bmatrix}&Q^{B}\\ 0&0&Q^{\mathcal{I}}&0\\ 0&0&Q^{\mathcal{I}}&0\\ 0&0&0&0&1\end{bmatrix},\quad K=\begin{bmatrix}&K^{B}\\ 0&0&K^{\mathcal{I}}&0\\ 0&0&0&-C\end{bmatrix},\quad V=e_{i}e_{d_{embd}}^{T}\]

where we call \(Q^{\mathcal{I}},K^{\mathcal{I}}\in\mathbb{R}^{2\times 2}\) the _interaction kernels_, \(Q^{B},K^{B}\in\mathbb{R}^{d_{embd}-3\times d_{embd}}\) the _data kernels_, and \(C>0\) simply a large positive number. We can choose \(Q^{\mathcal{I}},K^{\mathcal{I}}\) so \(K^{\mathcal{I}}=P_{\mathcal{I}_{t_{2}}}\) is a projection onto the unit interaction vector \(\mathcal{I}_{t_{2}}\) (the interaction term for \(h_{t_{2}}\)) and \(Q^{\mathcal{I}}\) is a dilation and rotation of \(\mathcal{I}_{t_{1}}\) onto \(\mathcal{I}_{t_{2}}\) i.e. \(Q^{\mathcal{I}}\mathcal{I}_{t_{1}}=C\mathcal{I}_{t_{2}}\). We must now compute \(\mathrm{A}(H)\). For an arbitrary \(1\leq t\leq l\) we can compute the action of \(\mathrm{A}\) on \(h_{t}\) as

\[\mathrm{A}(h_{t}) =\sum_{k=1}^{l}\sigma(\langle Qh_{t},Kh_{k}\rangle)Vh_{k}\] \[=\sum_{k=1}^{l}\sigma(\langle Q^{B}h_{t},K^{B}h_{k}\rangle+\langle Q ^{\mathcal{I}}\mathcal{I}_{t},K^{\mathcal{I}}\mathcal{I}_{k}\rangle-C)e_{i}\]

Now we must case on whether the input token \(t=t_{1}\).

**Case 1: \(t=t_{1}\)** Write

\[\mathrm{A}(h_{t_{1}})=\sum_{k=1}^{l}\sigma(\langle Q^{B}h_{t_{1}},K^{B}h_{k} \rangle+\langle Q^{\mathcal{I}}\mathcal{I}_{t_{1}},K^{\mathcal{I}}\mathcal{I}_ {k}\rangle-C)e_{i}\]

To handle the same we must again go by casework, this by casing on whether \(k=t_{2}\).

**Case 1a): \(k=t_{2}\)** When \(k=t_{2}\) we have \(\langle Q^{\mathcal{I}}\mathcal{I}_{t_{1}},K^{\mathcal{I}}\mathcal{I}_{t_{2}} \rangle=\langle C\mathcal{I}_{t_{2}},\mathcal{I}_{t_{2}}\rangle\) since by construction \(Q^{\mathcal{I}}\mathcal{I}_{t_{1}}=\mathcal{I}_{t_{2}}\) and \(K^{\mathcal{I}}\) is a projection onto \(\mathcal{I}_{t_{2}}\) so that \(K^{\mathcal{I}}\mathcal{I}_{t_{2}}=\mathcal{I}_{t_{2}}\). This further simplifies as \(\langle C\mathcal{I}_{t_{2}},\mathcal{I}_{t_{2}}\rangle=C\) since \(\mathcal{I}_{t_{2}}\) is unit length. Thus

\[\sigma(\langle Q^{B}h_{t_{1}},K^{B}h_{k}\rangle+\langle Q^{ \mathcal{I}}\mathcal{I}_{t_{1}},K^{\mathcal{I}}\mathcal{I}_{k}\rangle-C) =\sigma(\langle Q^{B}h_{t_{1}},K^{B}h_{k}\rangle+C-C)\] \[=\sigma(\langle Q^{B}h_{t_{1}},K^{B}h_{k}\rangle)\]Now we must consider the other terms in the sum when \(k\neq t_{2}\).

**Case 1b):**\(k\neq t_{2}\) When \(k\neq t_{2}\) we compute

\[\langle Q^{\mathcal{I}}\mathcal{I}_{t_{1}},K^{\mathcal{I}}\mathcal{ I}_{k}\rangle \leq\|Q^{\mathcal{I}}\mathcal{I}_{t_{1}}\|_{2}\|K^{\mathcal{I}} \mathcal{I}_{k}\|_{2}\] \[=C\|P_{\mathcal{I}_{t_{2}}}\mathcal{I}_{k}\|_{2}<C\]

where we use Cauchy-Schwarz at the first inequality and note that \(\|P_{\mathcal{I}_{t_{2}}}\mathcal{I}_{k}\|_{2}<1\) since \(k\neq t_{2}\) for the second inequality. Then, for large enough \(C\), we have

\[\sigma(\langle Q^{B}h_{t_{1}},K^{B}h_{k}\rangle+\langle Q^{\mathcal{I}} \mathcal{I}_{t_{1}},K^{\mathcal{I}}\mathcal{I}_{k}\rangle-C)\leq\sigma( \langle Q^{B}h_{t_{1}},K^{B}h_{k}\rangle+C\|P_{\mathcal{I}_{t_{2}}}\mathcal{I} _{k}\|_{2}-C)=0\]

so we simply must choose \(C\) so that \(\langle Q^{B}h_{t_{1}},K^{B}h_{k}\rangle+C\|P_{\mathcal{I}_{t_{2}}}\mathcal{I} _{k}\|_{2}-C<0\). Compute

\[\langle Q^{B}h_{t_{1}},K^{B}h_{k}\rangle+C\|P_{\mathcal{I}_{t_{2}}}\mathcal{ I}_{k}\|_{2}-C<0\iff C>\frac{\langle Q^{B}h_{t_{1}},K^{B}h_{k}\rangle}{1-\|P_{ \mathcal{I}_{t_{2}}}\mathcal{I}_{k}\|_{2}}\]

We can bound

\[\big{|}\langle Q^{B}h_{t_{1}},K^{B}h_{k}\rangle\big{|} \leq\|Q^{B}h_{t_{1}}\|_{2}\|K^{B}h_{k}\|_{2}\] \[\leq\|Q^{B}\|_{1,1}\|h_{t_{1}}\|_{\infty}\|K^{B}\|_{1,1}\|h_{t_{1 }}\|_{\infty}\] \[\leq\|Q^{B}\|_{\infty,\infty}d_{embd}^{2}\|K^{B}\|_{\infty,\infty }d_{embd}^{2}M^{2}\leq d_{embd}^{4}\kappa^{2}M^{2}\]

so it remains to control \(\frac{1}{1-\|P_{\mathcal{I}_{t_{2}}}\mathcal{I}_{k}\|_{2}}\) when \(k\neq t_{2}\) by upper bounding \(\|P_{\mathcal{I}_{t_{2}}}\mathcal{I}_{k}\|_{2}\). Compute

\[\|P_{\mathcal{I}_{t_{2}}}\mathcal{I}_{k}\|_{2} \leq\max_{1\leq t_{1}\neq t_{2}\leq l}\|P_{\mathcal{I}_{t_{1}}} \mathcal{I}_{t_{2}}\|_{2}\] \[\leq\|P_{\mathcal{I}_{0}}\mathcal{I}_{1}\|_{2}\] \[=\langle\mathcal{I}_{0},\mathcal{I}_{1}\rangle\] \[=\cos(\frac{\pi}{2l})\] \[\leq 1-\frac{1}{2}\frac{\pi^{2}}{2!\cdot 2^{2}l^{2}}\]

where we note the projection is maximized by any adjacent interaction terms \(\mathcal{I}_{t},\mathcal{I}_{t+1}\). Then

\[\frac{\langle Q^{B}h_{t_{1}},K^{B}h_{k}\rangle}{1-\|P_{\mathcal{I}_{t_{2}}} \mathcal{I}_{k}\|_{2}}\leq\frac{d_{embd}^{4}\kappa^{2}M^{2}}{\frac{\pi^{2}}{2 2!\cdot 2^{2}l^{2}}}=O(d_{embd}^{4}\kappa^{2}M^{2}l^{2})\]

which gives a bound on \(C\). So for large enough \(C\) we can conclude

\[\text{A}(h_{t_{1}})=\sigma(\langle Q^{B}h_{t_{1}},K^{B}h_{k}\rangle)\]

It remains to consider the case \(t\neq t_{1}\).

**Case 2:**\(t\neq t_{1}\) Now we consider A applied to tokens other than \(h_{t_{1}}\). We have

\[\text{A}(h_{t})=\sum_{k=1}^{l}\sigma(\langle Q^{B}h_{t},K^{B}h_{k}\rangle+ \langle Q^{\mathcal{I}}\mathcal{I}_{t},K^{\mathcal{I}}\mathcal{I}_{k}\rangle- C)e_{i}\]

so we must argue each term in the sum is \(0\) by showing \(\langle Q^{B}h_{t},K^{B}h_{k}\rangle+\langle Q^{\mathcal{I}}\mathcal{I}_{t},K^{ \mathcal{I}}\mathcal{I}_{k}\rangle-C<0\). We again split into two cases.

**Case 2a):**\(k\neq t_{2}\) When \(k\neq t_{2}\) we have \(\langle Q^{B}h_{t},K^{B}h_{k}\rangle+\langle Q^{\mathcal{I}}\mathcal{I}_{t},K^{ \mathcal{I}}\mathcal{I}_{k}\rangle-C<0\) via the same argument as in case **1b**.

**Case 2b):**\(k=t_{2}\) When \(k=t_{2}\) we must instead more carefully bound the inner product. We can simplify the interaction terms as

\[\langle Q^{\mathcal{I}}\mathcal{I}_{t},K^{\mathcal{I}}\mathcal{I}_{t_{2}} \rangle=\|Q^{\mathcal{I}}\mathcal{I}_{t}\|_{2}\|K^{\mathcal{I}}\mathcal{I}_{t _{2}}\|_{2}\cos(\theta_{t,t_{2}})=C\cos(\theta_{t,t_{2}})\]where \(\theta_{t,t_{2}}\) is the angle between \(Q^{\mathcal{I}}\mathcal{I}_{t}\) and \(K^{\mathcal{I}}\mathcal{I}_{k}\). Crucially, because \(t\neq t_{1}\), \(Q^{\mathcal{I}}\mathcal{I}_{t}\neq C\mathcal{I}_{t_{2}}\) and so \(\theta_{t,t_{2}}>0\) and \(\cos(\theta_{t,t_{2}})<1\). Then we can say

\[\langle Q^{B}h_{t},K^{B}h_{t_{2}}\rangle+\langle Q^{\mathcal{I}} \mathcal{I}_{t},K^{\mathcal{I}}\mathcal{I}_{t_{2}}\rangle-C<0 \iff \langle Q^{B}h_{t},K^{B}h_{t_{2}}\rangle+C\cos(\theta_{t,t_{2}})-C<0\] \[\iff C>\frac{\langle Q^{B}h_{t},K^{B}h_{t_{2}}\rangle}{1-\cos( \theta_{t,t_{2}})}\]

We upper bound the numerator by \(O(d_{embd}^{4}\kappa^{2}M^{2}l^{2})\). Note that the \(Q^{\mathcal{I}}\) rotates by clockwise \(\frac{t_{1}-t_{2}}{l}\frac{\pi}{2}\) radians. So \(Q^{\mathcal{I}}\mathcal{I}_{t}=C(\cos(\frac{(t+t_{1}-t_{2})\pi}{2l}),\sin( \frac{(t+t_{1}-t_{2})\pi}{2l}))\). Thus we can upper bound \(\cos(\theta_{t,t_{2}})\) as

\[\cos(\theta_{t,t_{2}}) =\langle\mathcal{I}_{t+t_{1}-t_{2}},\mathcal{I}_{t_{2}}\rangle\] \[\leq\langle\mathcal{I}_{0},\mathcal{I}_{1}\rangle\] \[=\cos(\frac{\pi}{2l})\] \[\leq 1-\frac{1}{2}\frac{\pi^{2}}{2!\cdot 2^{2}l^{2}}\]

So as in case **1b** we have \(C=O(d_{embd}^{4}\kappa^{2}M^{2}l^{2})\). For sufficiently large \(C\) we have \(\mathsf{A}(h_{t})=0\) as desired. This completes the proof of Lemma 3.

**Lemma 4** (Gating FFNs).: _Given an embedding matrix \(H\in\mathbb{R}^{d_{embd}\times l}\) which has all ones in the last row and whose tokens contain interaction terms and given a half-space of contiguous tokens \(h_{k},...,h_{l}\) or \(h_{1},...,h_{k}\), we can design \(\mathsf{FFN}\in\mathcal{FFN}(2)\) such that_

\[\mathsf{FFN}(h_{t})=\begin{cases}h_{t}&t\in\{1,...,k\}\\ \begin{bmatrix}\bm{0}\\ \mathcal{I}_{t}^{1}\\ \mathcal{I}_{t}^{2}\\ 1\end{bmatrix}&\text{otherwise}\\ \end{cases}\]

_where \(\begin{bmatrix}\bm{0}\\ \mathcal{I}_{t}^{1}\\ \mathcal{I}_{t}^{2}\\ 1\end{bmatrix}\) is zero except for the last three coordinates. We call this \(\mathsf{FFN}\) a **gating feed-forward network**. We additionally have \(\|\theta_{\mathsf{FFN}}\|_{\infty}\leq O\big{(}l\|H\|_{\infty}\big{)}\)._

Proof of Lemma 4.: Without loss of generality assume we are given a contiguous prefix of tokens \(h_{1},...,h_{k}\), \(k\leq l\). We can choose a vector \(v\in\mathbb{S}^{1}\) such that \(v\cdot\mathcal{I}_{t}>0\) for \(t\in\{1...,k\}\) and \(v\cdot\mathcal{I}_{t}<0\) for \(t>k\). We call such \(v\) the _pivot vector_. In particular we choose \(\textbf{R}_{\frac{\pi}{2}}(\frac{I_{h}+I_{h+1}}{\|\mathcal{I}_{k}+I_{h+1}\|_{ 2}})\) where \(\textbf{R}_{\frac{\pi}{2}}\) is a quarter-circle rotation clockwise. For a large \(C>0\), construct

\[W_{1}=\mathcal{I}_{d_{embd}}+\begin{bmatrix}0&0&Cv^{1}&Cv^{2}&0 \\ 0&0&Cv^{1}&Cv^{2}&0\\ 0&0&0&0&0\\ 0&0&0&0&0\\ 0&0&0&0&0\end{bmatrix},\quad b_{1}=0\] \[W_{2}=\mathcal{I}_{d_{embd}}+\begin{bmatrix}0&0&-Cv^{1}&-Cv^{2} &0\\ 0&0&-Cv^{1}&-Cv^{2}&0\\ 0&0&0&0&0\\ 0&0&0&0&0\\ 0&0&0&0&0\end{bmatrix},\quad b_{2}=0.\]where \(\mathcal{I}_{d_{embd}}\) is the identity matrix of size \(d_{embd}\times d_{embd}\). Then

\[z=\sigma(W_{1}h_{t}+b_{1})=\begin{bmatrix}\sigma(h_{t}^{1}+CI_{t}\cdot v)\\ \sigma(h_{t}^{2}+CI_{t}\cdot v)\\ \sigma(h_{t}^{d_{embd}-2})\\ \sigma(h_{t}^{d_{embd}-1})\\ \sigma(h_{t}^{d_{embd}})\end{bmatrix}=\begin{bmatrix}\sigma(h_{t}^{1}+CI_{t} \cdot v)\\ \sigma(h_{t}^{2}+CI_{t}\cdot v)\\ \mathcal{I}_{t}^{2}\\ 1\end{bmatrix}\]

which is \(\begin{bmatrix}\mathbf{0}\\ \mathcal{I}_{t}^{1}\\ \mathcal{I}_{t}^{2}\\ 1\end{bmatrix}\) if \(\mathcal{I}_{t}\cdot v<0\). Applying the second layer to \(\begin{bmatrix}\mathbf{0}\\ \mathcal{I}_{t}^{1}\\ \mathcal{I}_{t}^{2}\\ 1\end{bmatrix}\) yields \(\begin{bmatrix}\mathbf{0}\\ \mathcal{I}_{t}^{1}\\ \mathcal{I}_{t}^{2}\\ 1\end{bmatrix}\). Otherwise assume \(\mathcal{I}_{t}\cdot v>0\). Then \(\sigma(h_{t}^{i}+CI_{t}\cdot v)=h_{t}^{i}+CI_{t}\cdot v\). Applying the second layer yields

\[W_{2}z+b_{2}=h_{t}\]

as desired. Further we can compute a bound on \(C>0\) as \(|CI_{t}\cdot v|>\|H\|_{\infty}\iff C>\frac{\|H\|_{\infty}}{|I_{t}\cdot v|}\). We compute

\[|\mathcal{I}_{t}\cdot v|=|\langle\mathcal{I}_{t},\textbf{R}_{ \frac{\pi}{2}}\frac{\mathcal{I}_{k}+\mathcal{I}_{k+1}}{\|\mathcal{I}_{k}+ \mathcal{I}_{k+1}\|_{2}}\rangle| \geq\frac{1}{2}|\langle\mathcal{I}_{k},\textbf{R}_{\frac{\pi}{2} }\mathcal{I}_{k}\rangle+\langle\mathcal{I}_{k},\textbf{R}_{\frac{\pi}{2}} \mathcal{I}_{k+1}\rangle|\] \[=\frac{1}{2}|\langle\mathcal{I}_{k},\textbf{R}_{\frac{\pi}{2}} \mathcal{I}_{k+1}\rangle|=|\frac{1}{2}\langle\mathcal{I}_{0},\textbf{R}_{ \frac{\pi}{2}}\mathcal{I}_{1}\rangle|\] \[=\frac{1}{2}\sin\left(\frac{\pi}{2l}\right)\geq\frac{1}{4}\frac{ \pi}{2l}\]

where the last inequality holds for \(l\geq 2\). Thus \(\frac{\|H\|_{\infty}}{|I_{t}\cdot v|}\leq\frac{2^{3l}\|H\|_{\infty}}{\pi}<C\). 

**Lemma 5** (Constant Addition).: _Let \(M>0\) and \(c\) be a vector in \(\mathbb{R}^{D}\) such that \(\|c\|_{\infty}\leq\frac{M}{2}\). Let \(H\) be an embedding matrix of the form_

\[H=\begin{bmatrix}x^{1}&...&x^{D}&\boldsymbol{\theta}_{D}\\ 0&...&...&0\\ \mathcal{I}_{1}&...&...&\mathcal{I}_{l}\\ 1&...&...&1\end{bmatrix}\in\mathbb{R}^{d_{embd}\times l}\]

_where \(l=2D\) such that \(\|H\|_{\infty,\infty}\leq\frac{M}{2}\). Then there exists a transformer block \(\text{B}\in\mathcal{B}(D,3)\) such_

\[\text{B}(H)=\begin{bmatrix}x^{1}&...&x^{D}&x^{1}-c^{1}&...&x^{D}-c^{D}\\ 0&...&...&...&...&0\\ \mathcal{I}_{1}&...&...&...&...&\mathcal{I}_{l}\\ 1&...&...&...&...&1\end{bmatrix}\]

_with \(\|\theta_{\text{B}}\|_{\infty}=O(lM)\). In this case we say \(\text{B}\) implements the addition of \(c\) to \(x\)._

Proof of Lemma 5.: We begin by defining the attention heads \(\text{A}_{i}\), \(1\leq i\leq D\). For each head \(\text{A}_{i}\) we define the data kernels

\[Q_{i}^{data}=\begin{bmatrix}0&0&0&0&1\\ 0&0&0&0&1\end{bmatrix}\quad K_{i}^{data}=\begin{bmatrix}1&0&0&0&0\\ 0&0&0&0&-c^{i}+M\end{bmatrix}.\]

Then, via the Interaction Lemma 3, we can construct \(\text{A}_{i}\) so that token \(h_{D+i}\) interacts with \(h_{i}\) such that

\[\text{A}_{i}(h_{D+i}) =\sigma(\langle Q_{i}^{data}h_{D+i},K_{i}^{data}h_{i}\rangle)e_{1}\] \[=\sigma(x^{i}-c^{i}+M)e_{1}=(x^{i}-c^{i}+M)e_{1}\]and \(\mathbf{A}_{i}(h_{t})=0\) when \(t\neq D+i\). Then the residual multi-headed attention yields

\[\text{MHA}(H)+H=\begin{bmatrix}x^{1}&...&x^{D}&x^{1}-c^{1}+M&...&x^{D}-c^{D}+M\\ 0&...&...&...&...&...\\ \mathcal{I}_{1}&...&...&...&...&\mathcal{I}_{L}\\ 1&...&...&...&...&1\end{bmatrix}\]

Via the gating lemma 4 we can design a three layer FFN to subtract \(M\) from only \(h_{D+1},...,h_{2D}\). Thus

\[B(H)=\begin{bmatrix}x^{1}&...&x^{D}&x^{1}-c^{1}&...&x^{D}-c^{D}\\ 0&...&...&...&...&...\\ \mathcal{I}_{1}&...&...&...&...&\mathcal{I}_{L}\\ 1&...&...&...&...&1\end{bmatrix}\]

as desired.

**Lemma 6** (Replacing FFN).: _Set \(d_{embd}=5\) and \(l\in\mathbb{N}\). Let \(H\in\mathbb{R}^{d_{embd}\times l}\) be a structured intermediate embedding matrix. We can construct a feed-forward network \(\text{FFN}_{replace}\in\mathcal{FFN}(1)\) with the following output:_

\[\text{FFN}_{replace}(H)=\begin{bmatrix}-h_{1}^{1}+h_{2}^{2}&...&-h_{1}^{1}+h_ {l}^{2}\\ 0&...&0\\ 0&...&0\\ 0&...&0\\ \end{bmatrix}\]

_In this case we say \(\text{FFN}_{replace}\)**replaces** the the first row of \(H\) with the second row._

Proof of Lemma 6.: Set

\[W=\begin{bmatrix}-1&1&0&0&0\\ 0&0&0&0&0\\ 0&0&0&0&0\\ 0&0&0&0&0\\ 0&0&0&0&0\end{bmatrix}\]

Applying \(W\) token-wise gives the desired result. 

**Lemma 7** (Transformer Parallelization).: _Let \(m_{1},m_{2},l_{1},l_{2},L_{\text{FFN}1},L_{\text{FFN}2},w_{\text{FFN}1},w_{ \text{FFN}2},d_{embd}\in\mathbb{N}\). Fix transformer blocks \(\text{B}\in\mathcal{B}(m_{1},L_{\text{FFN}1},w_{\text{FFN}1})\) with input \(H_{1}\in\mathbb{R}^{d_{embd}\times l_{1}}\) and \(\text{B}_{2}\in\mathcal{B}(m_{2},L_{\text{FFN}2},w_{\text{FFN}2})\) with input \(H_{2}\in\mathbb{R}^{d_{embd}\times l_{2}}\). Further suppose both inputs are structured such that each token \(h_{i,t}\) has an interaction term \(\mathcal{I}_{t}\) and constant term \(1\). Further suppose all attention heads in both \(\text{B}_{1}\) and \(\text{B}_{2}\) are implemented using the Interaction Lemma and all FFN layers are either Gating FFN layers or are independent of the interaction terms. Then there exists \(\text{B}_{3}\in\mathcal{B}(m_{1}+m_{2},\max(L_{\text{FFN}1},L_{\text{FFN}2})+2,w_{\text{FFN}1}+w_{\text{FFN}2})\) which takes as input \(H_{3}\in\mathbb{R}^{d_{embd}\times(l_{1}+l_{2})}\) such that_

\[\text{B}_{3}(H_{3})=\text{B}_{3}\big{(}\begin{bmatrix}H_{1}^{\prime}&H_{2}^{ \prime}\end{bmatrix}\big{)}=[\text{B}_{1}(H_{1})\quad\text{B}_{2}(H_{2})]\in \mathbb{R}^{d_{embd}\times(l_{1}+l_{2})}\]

_where \(H_{i}^{\prime}\in\mathbb{R}^{d_{embd}\times l_{i}}\) is the same as \(H_{i}\) except for the interaction terms where we have \(\mathcal{I}_{3,t}^{\prime}=(cos(\frac{t}{l_{1}+l_{2}}\frac{\pi}{2}),sin(\frac{ t}{l_{1}+l_{2}}\frac{\pi}{2}))\) for \(1\leq t\leq l_{1}+l_{2}\). If \(\text{B}_{3}\) satisfies this relationship we say \(\text{B}_{3}\)**parallelizes**\(\text{B}_{1}\) and \(\text{B}_{2}\)._

Proof of Lemma 7.: Let \(\text{B}_{1}(H_{1})=\text{FFN}_{1}(\text{MHA}_{1}(H_{1}))+\text{MHA}_{1}(H_{1} )+H_{1}\) and \(\text{B}_{2}(H_{2})=\text{FFN}_{2}(\text{MHA}_{1}(H_{2}))+\text{MHA}_{2}(H_{2} )+H_{2}\). Let \(\text{A}_{1,i_{1}}\), \(1\leq i_{1}\leq m_{1}\), denote the attention heads of MHA\({}_{2}\) and \(\text{A}_{2,i_{2}}\), \(1\leq i_{2}\leq m_{2}\), denote the attention heads of MHA\({}_{2}\). We construct \(\text{B}_{3}\) as follows.

First we construct the new input embedding matrix \(H_{3}\) from \(H_{1}\) and \(H_{2}\). For \(1\leq t\leq l_{1}\) set \(h_{3,t}=h_{1,t}\) except we define a new interaction term \(\mathcal{I}_{3,t}=(cos(\frac{t}{l_{1}+l_{2}}\frac{\pi}{2}),sin(\frac{t}{l_{1}+ l_{2}}\frac{\pi}{2}))\). For 

[MISSING_PAGE_FAIL:42]

as desired. The case \(l_{1}\leq t\leq l_{1}+l_{2}\) proceeds analogously.

## Appendix H Other Lemmas

The following lemma (Chen et al., 2022, Lemma 2) is used for our construction of charts in the proof of Theorem 2.

**Lemma 8** (Local Diffeomorphism).: _Suppose Assumption 1 holds for manifold \(\mathcal{M}\) and \(r\leq\tau/4\). Then the local neighborhood \(U_{n}=B(c_{n},r)\cap\mathcal{M}\) is diffeomorphic to a subset of \(\mathbb{R}^{d}\). In particular, the orthogonal projection \(P_{n}\) onto the tangent space \(T_{c_{n}}(\mathcal{M})\) is a diffeomorphism._

### NeurIPS Paper Checklist

[Yes] [No] [NA]

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: We develop approximation and statistical theory predicting transformer scaling laws by mathematically proving approximation and statistical theory which we validate on several pretraining datasets and scaling suites. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We discuss that our analysis does not extend to analyzing benchmark capabilities of LLMs, which are of much interest, and identify this direction as promising future work. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.

3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: We clearly state all assumptions in Section 2 and give complete proofs in the appendix. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We share all our hyperparameters, code, data, and models publicly for 100% reproducibility. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility.

In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: We share all our hyperparameters, code, data, and models publicly for 100% reproducibility. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We specify all hyper-parameters used in our experiments. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [No] Justification: We do not report error bars. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.

* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We specificy the compute resources used to run our experiments. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We have reviewed the code of ethics and confirm we are consistent. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: We include discussion of broader impacts in our conclusion. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.

* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: NA Guidelines: The answer NA means that the paper poses no such risks.
* Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.
* Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.
* We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: Yes, we give proper citation with urls and follow all licensing. Guidelines: The answer NA means that the paper does not use existing assets.
* The authors should cite the original paper that produced the code package or dataset.
* The authors should state which version of the asset is used and, if possible, include a URL.
* The name of the license (e.g., CC-BY 4.0) should be included for each asset.
* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.

* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: We provide a full set of instructions in the released code. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: NA Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: NA Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.