# Convex and Non-convex Optimization Under

Generalized Smoothness

 Haochuan Li

MIT

haochuan@mit.edu

&Jian Qian

MIT

jianqian@mit.edu

&Yi Tian

MIT

yitian@mit.edu

&Alexander Rakhlin

MIT

rakhlin@mit.edu

&Ali Jadbabaie

MIT

jadbabai@mit.edu

Equal contribution.

###### Abstract

Classical analysis of convex and non-convex optimization methods often requires the Lipschitz continuity of the gradient, which limits the analysis to functions bounded by quadratics. Recent work relaxed this requirement to a non-uniform smoothness condition with the Hessian norm bounded by an affine function of the gradient norm, and proved convergence in the non-convex setting via gradient clipping, assuming bounded noise. In this paper, we further generalize this non-uniform smoothness condition and develop a simple, yet powerful analysis technique that bounds the gradients along the trajectory, thereby leading to stronger results for both convex and non-convex optimization problems. In particular, we obtain the classical convergence rates for (stochastic) gradient descent and Nesterov's accelerated gradient method in the convex and/or non-convex setting under this general smoothness condition. The new analysis approach does not require gradient clipping and allows heavy-tailed noise with bounded variance in the stochastic setting.

## 1 Introduction

In this paper, we study the following _unconstrained_ optimization problem

\[\text{min}_{x\in\mathcal{X}}f(x),\] (1)

where \(\mathcal{X}\subseteq\mathbb{R}^{d}\) is the domain of \(f\). Classical textbook analyses (Nemirovskij and Yudin, 1983; Nesterov, 2003) of (1) often require the Lipschitz smoothness condition, which assumes \(\left\|\nabla^{2}f(x)\right\|\leq L\) almost everywhere for some \(L\geq 0\) called the smoothness constant. This condition, however, is rather restrictive and only satisfied by functions that are both upper and lower bounded by quadratic functions.

Recently, Zhang et al. (2019) proposed the more general \((L_{0},L_{1})\)-smoothness condition, which assumes \(\left\|\nabla^{2}f(x)\right\|\leq L_{0}+L_{1}\left\|\nabla f(x)\right\|\) for some constants \(L_{0},L_{1}\geq 0\), motivated by their extensive language model experiments. This notion generalizes the standard Lipschitz smoothness condition and also contains e.g. univariate polynomial and exponential functions. For _non-convex_ and \((L_{0},L_{1})\)-smooth functions, they prove convergence of gradient descent (GD) and stochastic gradient descent (SGD) _with gradient clipping_ and also provide a complexity lower bound for _constant-stepsize_ GD/SGD without clipping. Based on these results, they claim gradient clipping or other forms of adaptivity _provably_ accelerate the convergence for \((L_{0},L_{1})\)-smooth functions. Perhaps due to thelower bound, all the follow-up works under this condition that we are aware of limit their analyses to adaptive methods. Most of these focus on non-convex functions. See Section 2 for more discussions of related works.

In this paper, we significantly generalize the \((L_{0},L_{1})\)-smoothness condition to the \(\ell\)-smoothness condition which assumes \(\left\|\nabla^{2}f(x)\right\|\leq\ell(\left\|\nabla f(x)\right\|)\) for some non-decreasing continuous function \(\ell\). We develop a simple, yet powerful approach, which allows us to obtain stronger results for _both convex and non-convex_ optimization problems when \(\ell\) is sub-quadratic (i.e., \(\lim_{u\to\infty}\ell(u)/u^{2}=0\)) or even more general. The \(\ell\)-smooth function class with a sub-quadratic \(\ell\) also contains e.g. univariate rational and double exponential functions. In particular, we prove the convergence of _constant-stepsize_ GD/SGD and Nesterov's accelerated gradient method (NAG) in the convex or non-convex settings. For each method and setting, we obtain the classical convergence rate, under a certain requirement of \(\ell\). In addition, we relax the assumption of bounded noise to the weaker one of bounded variance with the simple SGD method. See Table 1 for a summary of our results and assumptions for each method and setting. At first glance, our results "contradict" the lower bounds on constant-stepsize GD/SGD in (Zhang et al., 2019; Wang et al., 2022); this will be reconciled in Section 5.3.

Our approach analyzes boundedness of gradients along the optimization trajectory. The idea behind it can be informally illustrated by the following "circular" reasoning. On the one hand, if gradients along the trajectory are bounded by a constant \(G\), then the Hessian norms are bounded by the constant \(\ell(G)\). Informally speaking, we essentially have the standard Lipschitz smoothness condition2 and can apply classical textbook analyses to prove convergence, which implies that gradients converge to zero. On the other hand, if gradients converge, they must be bounded, since any convergent sequence is bounded. In other words, the bounded gradient condition implies convergence, and convergence also implies the condition back, which forms a circular argument. If we can break this circularity of reasoning in a rigorous way, both the bounded gradient condition and convergence are proved. In this paper, we will show how to break the circularity using induction or contradiction arguments for different methods and settings in Sections 4 and 5. We note that the idea of bounding gradients can be applied to the analysis of other optimization methods, e.g., the concurrent work (Li et al., 2023) by subset of the authors, which uses a similar idea to obtain a rigorous and improved analysis of the Adam method (Kingma and Ba, 2014).

Footnote 2: This statement is informal because we can only bound Hessian norms _along the trajectory_, rather than almost everywhere within a convex set as in the standard Lipschitz smoothness condition. For example, even if the Hessian norm is bounded at both \(x_{t}\) and \(x_{t+1}\), it does not directly mean the Hessian norm is also bounded over the line segment between them, which is required in classical analysis. A more formal statement will need Lemma 3.3 presented later in the paper.

**Contributions.** In light of the above discussions, we summarize our main contributions as follows.

* We generalize the standard Lipschitz smoothness and also the \((L_{0},L_{1})\)-smoothness condition to the \(\ell\)-smoothness condition, and develop a new approach for analyzing convergence under this condition by bounding the gradients along the optimization trajectory.
* We prove the convergence of _constant-stepsize_ GD/SGD/NAG in the convex and non-convex settings, and obtain the classical rates for all of them, as summarized in Table 1.

Besides the generalized smoothness condition and the new approach, our results are also novel in the following aspects.

* The convergence results of _constant-stepsize_ methods challenge the folklore belief on the necessity of adaptive stepsize for generalized smooth functions.
* We obtain new convergence results for GD and NAG in the convex setting under the generalized smoothness condition.
* We relax the assumption of bounded noise to the weaker one of bounded variance of noise in the stochastic setting with the simple SGD method.

## 2 Related work

**Gradient-based optimizaiton.** The classical gradient-based optimization problems for the standard Lipschitz smooth functions have been well studied for both convex (Nemirovskij and Yudin, 1983;Nesterov, 2003, d'Aspremont et al., 2021] and non-convex functions. In the convex setting, the goal is to reach an \(\epsilon\)-sub-optimal point \(x\) satisfying \(f(x)-\inf_{x}f(x)\leq\epsilon\). It is well known that GD achieves the \(\mathcal{O}(1/\epsilon)\) gradient complexity and NAG achieves the accelerated \(\mathcal{O}(1/\sqrt{\epsilon})\) complexity which is optimal among all gradient-based methods. For strongly convex functions, GD and NAG achieve the \(\mathcal{O}(\kappa\log(1/\epsilon))\) and \(\mathcal{O}(\sqrt{\kappa}\log(1/\epsilon))\) complexity respectively, where \(\kappa\) is the condition number and the latter is again optimal. In the non-convex setting, the goal is to find an \(\epsilon\)-stationary point \(x\) satisfying \(\|\nabla f(x)\|\leq\epsilon\), since finding a global minimum is NP-hard in general. It is well known that GD achieves the optimal \(\mathcal{O}(1/\epsilon^{2})\) complexity which matches the lower bound in [Carmon et al., 2017]. In the stochastic setting for unbiased stochastic gradient with bounded variance, SGD achieves the optimal \(\mathcal{O}(1/\epsilon^{4})\) complexity [Ghadimi and Lan, 2013], matching the lower bound in [Arjevani et al., 2019]. In this paper, we obtain the classical rates in terms of \(\epsilon\) for all the above-mentioned methods and settings, under a far more general smoothness condition.

**Generalized smoothness.** The \((L_{0},L_{1})\)-smoothness condition proposed by Zhang et al. [2019] was studied by many follow-up works. Under the same condition, [Zhang et al., 2020] considers momentum in the updates and improves the constant dependency of the convergence rate for SGD with clipping derived in [Zhang et al., 2019]. [Qian et al., 2021] studies gradient clipping in incremental gradient methods, [Zhao et al., 2021] studies stochastic normalized gradient descent, and [Crawshaw et al., 2022] studies a generalized SignSGD method, under the \((L_{0},L_{1})\)-smooths condition. [Reisizadeh et al., 2023] studies variance reduction for \((L_{0},L_{1})\)-smooth functions. [Chen et al., 2023] proposes a new notion of \(\alpha\)-symmetric generalized smoothness, which is roughly as general as \((L_{0},L_{1})\)-smoothness. [Wang et al., 2022] analyzes convergence of Adam and provides a lower bound which shows non-adaptive SGD may diverge. In the stochastic setting, the above-mentioned works either consider the strong assumption of bounded gradient noise or require a very large batch size that depends on \(\epsilon\), which essentially reduces the analysis to the deterministic setting. [Faw et al., 2023] proposes an AdaGrad-type algorithm in order to relax the bounded noise assumption. Perhaps due to the lower bounds in [Zhang et al., 2019, Wang et al., 2022], all the above works study methods with an adaptive stepsize. In this and our concurrent work [Li et al., 2023], we further generalize the smoothness condition and analyze various methods under this condition through bounding the gradients along the trajectory.

## 3 Function class

In this section, we discuss the function class of interest where the objective function \(f\) lies. We start with the following two standard assumptions in the literature of unconstrained optimization, which will be assumed throughout Sections 4 and 5 unless explicitly stated.

**Assumption 1**.: The objective function \(f\) is differentiable and _closed_ within its _open_ domain \(\mathcal{X}\).

**Assumption 2**.: The objective function \(f\) is bounded from below, i.e., \(f^{\star}:=\inf_{x\in\mathcal{X}}f(x)>-\infty\).

A function \(f\) is said to be closed if its sub-level set \(\{x\in\mathrm{dom}(f)\mid f(x)\leq a\}\) is closed for each \(a\in\mathbb{R}\). A continuous function \(f\) with an open domain is closed if and only \(f(x)\) tends to positive infinity when \(x\) approaches the boundary of its domain [Boyd and Vandenberghe, 2004]. Assumption 1 is necessary for our analysis to ensure that the iterates of a method with a reasonably small stepsize stays within the domain \(\mathcal{X}\). Note that for \(\mathcal{X}=\mathbb{R}^{d}\) considered in most unconstrained

\begin{table}
\begin{tabular}{c c c c} \hline \hline Method & Convexity & \(\ell\)-smoothness & Gradient complexity \\ \hline \multirow{3}{*}{GD} & Strongly convex & \multirow{3}{*}{No requirement} & \(\mathcal{O}(\log(1/\epsilon))\) (Theorem 4.3) \\  & Convex & & \(\mathcal{O}(1/\epsilon)\) (Theorem 4.2 ) \\ \cline{2-4}  & & Sub-quadratic \(\ell\) & \(\mathcal{O}(1/\epsilon^{2})^{\star}\) (Theorem 5.2) \\ \cline{2-4}  & Non-convex & Quadratic \(\ell\) & \(\Omega\)(exp. in cond \(\#\)) (Theorem 5.4 ) \\ \hline NAG & Convex & Sub-quadratic \(\ell\) & \(\mathcal{O}(1/\sqrt{\epsilon})^{\star}\) (Theorem 4.4 ) \\ \hline SGD & Non-convex & Sub-quadratic \(\ell\) & \(\mathcal{O}(1/\epsilon^{4})^{\star}\) (Theorem 5.3) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Summary of the results. \(\epsilon\) denotes the sub-optimality gap of the function value in convex settings, and the gradient norm in non-convex settings. “\(*\)” denotes optimal rates.

optimization papers, the assumption is trivially satisfied as all continuous functions over \(\mathbb{R}^{d}\) are closed. We consider a more general domain which may not be the whole space because that is the case for some interesting examples in our function class of interest (see Section 3.1.3). However, it actually brings us some additional technical difficulties especially in the stochastic setting, as we need to make sure the iterates do not go outside of the domain.

### Generalized smoothness

In this section, we formally define the generalized smoothness condition, and present its properties and examples.

#### 3.1.1 Definitions

Definitions 1 and 2 below are two equivalent ways of stating the definition, where we use \(\mathcal{B}(x,R)\) to denote the Euclidean ball with radius \(R\) centered at \(x\).

**Definition 1** (\(\ell\)-smoothness).: A real-valued differentiable function \(f:\mathcal{X}\to\mathbb{R}\) is \(\ell\)-smooth for some non-decreasing continuous function \(\ell:[0,+\infty)\to(0,+\infty)\) if \(\left\|\nabla^{2}f(x)\right\|\leq\ell(\left\|\nabla f(x)\right\|)\)_almost everywhere_ (with respect to the Lebesgue measure) in \(\mathcal{X}\).

_Remark 3.1_.: Definition 1 reduces to the classical \(L\)-smoothness when \(\ell\equiv L\) is a constant function. It reduces to the \((L_{0},L_{1})\)-smoothness proposed in (Zhang et al., 2019) when \(\ell(u)=L_{0}+L_{1}u\) is an affine function.

**Definition 2** (\((r,\ell)\)-smoothness).: A real-valued differentiable function \(f:\mathcal{X}\to\mathbb{R}\) is \((r,\ell)\)-smooth for continuous functions \(r,\ell:[0,+\infty)\to(0,+\infty)\) where \(\ell\) is non-decreasing and \(r\) is non-increasing, if it satisfies 1) for any \(x\in\mathcal{X}\), \(\mathcal{B}(x,r(\left\|\nabla f(x)\right\|))\subseteq\mathcal{X}\), and 2) for any \(x_{1},x_{2}\in\mathcal{B}(x,r(\left\|\nabla f(x)\right\|))\), \(\left\|\nabla f(x_{1})-\nabla f(x_{2})\right\|\leq\ell(\left\|\nabla f(x) \right\|)\cdot\left\|x_{1}-x_{2}\right\|\).

The requirements that \(\ell\) is non-decreasing and \(r\) is non-increasing do not cause much loss in generality. If these conditions are not satisfied, one can replace \(\ell\) and \(r\) with the non-increasing function \(\tilde{r}(u):=\inf_{0\leq v\leq u}r(v)\leq r(u)\) and non-decreasing function \(\tilde{\ell}(u):=\sup_{0\leq v\leq u}\ell(v)\geq\ell(u)\) in Definitions 1 and 2. Then the only requirement is \(\tilde{r}>0\) and \(\tilde{\ell}<\infty\).

Next, we prove that the above two definitions are equivalent in the following proposition, whose proof is involved and deferred to Appendix A.2.

**Proposition 3.2**.: _An \((r,\ell)\)-smooth function is \(\ell\)-smooth; and an \(\ell\)-smooth function satisfying Assumption 1 is \((r,m)\)-smooth where \(m(u):=\ell(u+a)\) and \(r(u):=a/m(u)\) for any \(a>0\)._

The condition in Definition 1 is simple and one can easily check whether it is satisfied for a given example function. On the other hand, Definition 2 is a local Lipschitz condition on the gradient that is harder to verify. However, it is useful for deriving several useful properties in the next section.

#### 3.1.2 Properties

First, we provide the following lemma which is very useful in our analyses of all the methods considered in this paper. Its proof is deferred to Appendix A.3.

**Lemma 3.3**.: _If \(f\) is \((r,\ell)\)-smooth, for any \(x\in\mathcal{X}\) satisfying \(\left\|\nabla f(x)\right\|\leq G\), we have 1) \(\mathcal{B}(x,r(G))\subseteq\mathcal{X}\), and 2) for any \(x_{1},x_{2}\in\mathcal{B}(x,r(G))\),_

\[\left\|\nabla f(x_{1})\!-\!\nabla f(x_{2})\right\|\!\leq\!L\left\|x_{1}\!-\!x _{2}\right\|,\quad f(x_{1})\!\leq\!f(x_{2})\!+\!\left\langle\nabla f(x_{2}),x _{1}\!-\!x_{2}\right\rangle\!+\!\frac{L}{2}\left\|x_{1}\!-\!x_{2}\right\|^{2},\] (2)

_where \(L:=\ell(G)\) is the effective smoothness constant._

_Remark 3.4_.: Since we have shown the equivalence between \(\ell\)-smoothness and \((r,\ell)\)-smoothness, Lemma 3.3 also applies to \(\ell\)-smooth functions, for which we have \(L=\ell(2G)\) and \(r(G)=G/L\) if choosing \(a=G\) in Proposition 3.2.

Lemma 3.3 states that, if the gradient at \(x\) is bounded by some constant \(G\), then within its neighborhood with a _constant_ radius, we can obtain (2), the same inequalities that were derived in the textbook analysis (Nesterov, 2003) under the standard Lipschitz smoothness condition. With (2), the analysis for generalized smoothness is not much harder than that for standard smoothness. Since we mostly choose \(x=x_{2}=x_{t}\) and \(x_{1}=x_{t+1}\) in the analysis, in order to apply Lemma 3.3, we need two conditions: \(\left\|\nabla f(x_{t})\right\|\leq G\) and \(\left\|x_{t+1}-x_{t}\right\|\leq r(G)\) for some constant \(G\). The latter is usually directly implied by the former for most deterministic methods with a small enough stepsize, and the former can be obtained with our new approach that bounds the gradients along the trajectory.

With Lemma 3.3, we can derive the following useful lemma which is the reverse direction of a generalized Polyak-Lojasiewicz (PL) inequality, whose proof is deferred to Appendix A.3.

**Lemma 3.5**.: _If \(f\) is \(\ell\)-smooth, then \(\left\|\nabla f(x)\right\|^{2}\leq 2\ell(2\left\|\nabla f(x)\right\|)\cdot(f(x)-f ^{*})\) for any \(x\in\mathcal{X}\)._

Lemma 3.5 provides an inequality involving the gradient norm and the sub-optimality gap. For example, when \(\ell(u)=u^{\rho}\) for some \(0\leq\rho<2\), this lemma suggests \(\left\|\nabla f(x)\right\|\leq\mathcal{O}\left((f(x)-f^{*})^{1/(2-\rho)}\right)\), which means the gradient norm is bounded whenever the function value is bounded. The following corollary provides a more formal statement for general sub-quadratic \(\ell\) (i.e., \(\lim_{u\to\infty}\ell(u)/u^{2}=0\)), and we defer its proof to Appendix A.3.

**Corollary 3.6**.: _Suppose \(f\) is \(\ell\)-smooth where \(\ell\) is sub-quadratic. If \(f(x)-f^{*}\leq F\) for some \(x\in\mathcal{X}\) and \(F\geq 0\), denoting \(G:=\sup\{u\geq 0\mid u^{2}\leq 2\ell(2u)\cdot F\}\), then they satisfy \(G^{2}=2\ell(2G)\cdot F\) and we have \(\left\|\nabla f(x)\right\|\leq G<\infty\)._

Therefore, in order to bound the gradients along the trajectory as we discussed below Lemma 3.3, it suffices to bound the function values, which is usually easier.

#### 3.1.3 Examples

The most important subset of \(\ell\)-smooth (or \((r,\ell)\)-smooth) functions are those with a polynomial \(\ell\), and can be characterized by the \((\rho,L_{0},L_{\rho})\)-smooth function class defined below.

**Definition 3** (\((\rho,L_{0},L_{\rho})\)-smoothness).: A real-valued differentiable function \(f\) is \((\rho,L_{0},L_{\rho})\)-smooth for constants \(\rho,L_{0},L_{\rho}\geq 0\) if it is \(\ell\)-smooth with \(\ell(u)=L_{0}+L_{\rho}u^{\rho}\).

Definition 3 reduces to the standard Lipschitz smoothness condition when \(\rho=0\) or \(L_{\rho}=0\) and to the \((L_{0},L_{1})\)-smoothness proposed in (Zhang et al., 2019) when \(\rho=1\). We list several univariate examples of \((\rho,L_{0},L_{\rho})\)-smooth functions for different \(\rho\)s in Table 2 with their rigorous justifications in Appendix A.1. Note that when \(x\) goes to infinity, polynomial and exponential functions corresponding to \(\rho=1\) grow much faster than quadratic functions corresponding to \(\rho=0\). Rational and logarithmic functions for \(\rho>1\) grow even faster as they can blow up to infinity near finite points. Note that the domains of such functions are not \(\mathbb{R}^{d}\), which is why we consider the more general Assumption 1 instead of simply assuming \(\mathcal{X}=\mathbb{R}^{d}\).

Aside from logarithmic functions, the \((2,L_{0},L_{2})\)-smooth function class also includes other univariate _self-concordant_ functions. This is an important function class in the analysis of Interior Point Methods and coordinate-free analysis of the Newton method (Nesterov, 2003). More specifically, a convex function \(h:\mathbb{R}\to\mathbb{R}\) is self-concordant if \(|h^{\prime\prime\prime}(x)|\leq 2h^{\prime\prime}(x)^{3/2}\) for all \(x\in\mathbb{R}\). Formally, we have the following proposition whose proof is deferred to Appendix A.1.

**Proposition 3.7**.: _If \(h:\mathbb{R}\to\mathbb{R}\) is a self-concordant function satisfying \(h^{\prime\prime}(x)>0\) over the interval \((a,b)\), then \(h\) restricted on \((a,b)\) is \((2,L_{0},2)\)-smooth for some \(L_{0}>0\)._

## 4 Convex setting

In this section, we present the convergence results of gradient descent (GD) and Nesterov's accelerated gradient method (NAG) in the convex setting. Formally, we define convexity as follows.

\begin{table}
\begin{tabular}{c c c c c c c} \hline \hline \(\rho\) & \(0\) & \(1\) & \(1\) & \(1^{+}\) & \(1.5\) & \(2\) & \(\frac{p-2}{p-1}\) \\ \hline Example Functions & Quadratic & Polynomial & \(a^{x}\) & \(a^{(b^{x})}\) & Rational & Logarithmic & \(x^{p}\) \\ \hline \hline \end{tabular}
\end{table}
Table 2: Examples of univariate \((\rho,L_{0},L_{\rho})\) smooth functions for different \(\rho\)s. The parameters \(a,b,p\) are _real numbers_ (not necessarily integers) satisfying \(a,b>1\) and \(p<1\) or \(p\geq 2\). We use \(1^{+}\) to denote any real number slightly larger than \(1\).

**Definition 4**.: A real-valued differentiable function \(f:\mathcal{X}\to\mathbb{R}\) is \(\mu\)-strongly-convex for \(\mu\geq 0\) if \(\mathcal{X}\) is a convex set and \(f(y)-f(x)\geq\left\langle\nabla f(x),y-x\right\rangle+\frac{\mu}{2}\left\|y-x \right\|^{2}\) for any \(x,y\in\mathcal{X}\). A function is convex if it is \(\mu\)-strongly-convex with \(\mu=0\).

We assume the existence of a global optimal point \(x^{*}\) throughout this section, as in the following assumption. However, we want to note that, for gradient descent, this assumption is just for simplicity rather than necessary.

**Assumption 3**.: There exists a point \(x^{*}\in\mathcal{X}\) such that \(f(x^{*})=f^{*}=\inf_{x\in\mathcal{X}}f(x)\).

### Gradient descent

The gradient descent method with a constant stepsize \(\eta\) is defined via the following update rule

\[x_{t+1}=x_{t}-\eta\nabla f(x_{t}).\] (3)

As discussed below Lemma 3.3, the key in the convergence analysis is to show \(\|\nabla f(x_{t})\|\leq G\) for all \(t\geq 0\) and some constant \(G\). We will prove it by induction relying on the following lemma whose proof is deferred to Appendix B.

**Lemma 4.1**.: _For any \(x\in\mathcal{X}\) satisfying \(\|\nabla f(x)\|\leq G\), define \(x^{+}:=x-\eta\nabla f(x)\). If \(f\) is convex and \((r,\ell)\)-smooth, and \(\eta\leq\min\left\{\frac{2}{G(G)},\frac{r(G)}{2G}\right\}\), we have \(x^{+}\in\mathcal{X}\) and \(\|\nabla f(x^{+})\|\leq\|\nabla f(x)\|\leq G\)._

Lemma 4.1 suggests that for gradient descent (3) with a small enough stepsize, if the gradient norm at \(x_{t}\) is bounded by \(G\), then we have \(\|\nabla f(x_{t+1})\|\leq\|\nabla f(x_{t})\|\leq G\), i.e., the gradient norm is also bounded by \(G\) at \(t+1\). In other words, the gradient norm is indeed a non-increasing potential function for gradient descent in the convex setting. With a standard induction argument, we can show that \(\|\nabla f(x_{t})\|\leq\|\nabla f(x_{0})\|\) for all \(t\geq 0\). As discussed below Lemma 3.3, then we can basically apply the classical analysis to obtain the convergence guarantee in the convex setting as in the following theorem, whose proof is deferred to Appendix B.

**Theorem 4.2**.: _Suppose \(f\) is convex and \((r,\ell)\)-smooth. Denote \(G:=\|\nabla f(x_{0})\|\) and \(L:=\ell(G)\), then the iterates generated by (3) with \(\eta\leq\min\left\{\frac{1}{L},\frac{r(G)}{2G}\right\}\) satisfy \(\|\nabla f(x_{t})\|\leq G\) for all \(t\geq 0\) and_

\[f(x_{T})-f^{*}\leq\frac{\|x_{0}-x^{*}\|^{2}}{2\eta T}.\]

Since \(\eta\) is a constant independent of \(\epsilon\) or \(T\), Theorem 4.2 achieves the classical \(\mathcal{O}(1/T)\) rate, or \(\mathcal{O}(1/\epsilon)\) gradient complexity to achieve an \(\epsilon\)-sub-optimal point, under the generalized smoothness condition. Since strongly convex functions are a subset of convex functions, Lemma 4.1 still holds for them. Then we immediately obtain the following result in the strongly convex setting, whose proof is deferred to Appendix B.

**Theorem 4.3**.: _Suppose \(f\) is \(\mu\)-strongly-convex and \((r,\ell)\)-smooth. Denote \(G:=\|\nabla f(x_{0})\|\) and \(L:=\ell(G)\), then the iterates generated by (3) with \(\eta\leq\min\left\{\frac{1}{L},\frac{r(G)}{2G}\right\}\) satisfy \(\|\nabla f(x_{t})\|\leq G\) for all \(t\geq 0\) and_

\[f(x_{T})-f^{*}\leq\frac{\mu(1-\eta\mu)^{T}}{2(1-(1-\eta\mu)^{T})}\left\|x_{0} -x^{*}\right\|^{2}.\]

Theorem 4.3 gives a linear convergence rate and the \(\mathcal{O}((\eta\mu)^{-1}\log(1/\epsilon))\) gradient complexity to achieve an \(\epsilon\)-sub-optimal point. Note that for \(\ell\)-smooth functions, we have \(\frac{r(G)}{G}=\frac{1}{L}\) (see Remark 3.4), which means we can choose \(\eta=\frac{1}{2L}\). Then we obtain the \(\mathcal{O}(\kappa\log(1/\epsilon))\) rate, where \(\kappa:=L/\mu\) is the local condition number around the initial point \(x_{0}\). For standard Lipschitz smooth functions, it reduces to the classical rate of gradient descent.

### Nesterov's accelerated gradient method

In the case of convex and standard Lipschitz smooth functions, it is well known that Nesterov's accelerated gradient method (NAG) achieves the optimal \(\mathcal{O}(1/T^{2})\) rate. In this section, we show thatunder the \(\ell\)-smoothness condition with a _sub-quadratic_\(\ell\), the optimal \(\mathcal{O}(1/T^{2})\) rate can be achieved by a slightly modified version of NAG shown in Algorithm 1, the only difference between which and the classical NAG is that the latter directly sets \(A_{t+1}=B_{t+1}\) in Line 4. Formally, we have the following theorem, whose proof is deferred to Appendix C.

**Theorem 4.4**.: _Suppose \(f\) is convex and \(\ell\)-smooth where \(\ell\) is sub-quadratic. Then there always exists a constant \(G\) satisfying \(G\geq\max\left\{8\sqrt{\ell(2G)((f(x_{0})-f^{*})+\left\|x_{0}-x^{*}\right\|^{2 })},\left\|\nabla f(x_{0})\right\|\right\}.\) Denote \(L:=\ell(2G)\) and choose \(\eta\leq\min\left\{\frac{1}{16L^{3}},\frac{1}{2L}\right\}\). The iterates generated by Algorithm 1 satisfy_

\[f(x_{T})-f^{*}\leq\frac{4(f(x_{0})-f^{*})+4\left\|x_{0}-x^{*}\right\|^{2}}{ \eta T^{2}+4}.\]

It is easy to note that Theorem 4.4 achieves the accelerated \(\mathcal{O}(1/T^{2})\) convergence rate, or equivalently the \(\mathcal{O}(1/\sqrt{\epsilon})\) gradient complexity to find an \(\epsilon\)-sub-optimal point, which is optimal among gradient-based methods [10].

In order to prove Theorem 4.4, we also use induction to show the gradients along the trajectory of Algorithm 1 are bounded by \(G\). However, unlike gradient descent, the gradient norm is no longer a potential function or monotonically non-increasing, which makes the induction analysis more challenging. Suppose that we have shown \(\left\|\nabla f(y_{s})\right\|\leq G\) for \(s<t\). To complete the induction, it suffices to prove \(\left\|\nabla f(y_{t})\right\|\leq G\). Since \(x_{t}=y_{t-1}-\eta\nabla f(y_{t-1})\) is a gradient descent step by Line 6 of Algorithm 1, Lemma 4.1 directly shows \(\left\|\nabla f(x_{t})\right\|\leq G\). In order to also bound \(\left\|\nabla f(y_{t})\right\|\), we try to control \(\left\|y_{t}-x_{t}\right\|\), which is the most challenging part of our proof. Since \(y_{t}-x_{t}\) can be expressed as a linear combination of past gradients \(\{\nabla f(y_{s})\}_{s<t}\), it might grow linearly with \(t\) if we simply apply \(\left\|\nabla f(y_{s})\right\|\leq G\) for \(s<t\). Fortunately, Lemma 3.5 allows us to control the gradient norm with the function value. Thus if the function value is decreasing sufficiently fast, which can be shown by following the standard Lyapunov analysis of NAG, we are able to obtain a good enough bound on \(\left\|\nabla f(y_{s})\right\|\) for \(s<t\), which allows us to control \(\left\|y_{t}-x_{t}\right\|\). We defer the detailed proof to Appendix C.

Note that Theorem 4.4 requires a smaller stepsize \(\eta=\mathcal{O}(1/L^{2})\), compared to the classical \(\mathcal{O}(1/L)\) stepsize for standard Lipschitz smooth functions. The reason is we require a small enough stepsize to get a good enough bound on \(\left\|y_{t}-x_{t}\right\|\). However, if the function is further assumed to be \(\ell\)-smooth with a _sub-linear_\(\ell\), the requirement of stepsize can be relaxed to \(\eta=\mathcal{O}(1/L)\), similar to the classical requirement. See Appendix C for the details.

In the strongly convex setting, we can also prove convergence of NAG with different \(\{A_{t}\}_{t\geq 0}\) parameters when \(f\) is \(\ell\)-smooth with a sub-quadratic \(\ell\), or \((\rho,L_{0},L_{\rho})\)-smooth with \(\rho<2\). The rate can be further improved when \(\rho\) becomes smaller. However, since the constants \(G\) and \(L\) are different for GD and NAG, it is not clear whether the rate of NAG is faster than that of GD in the strongly convex setting. We will present the detailed result and analysis in Appendix D.

## 5 Non-convex setting

In this section, we present convergence results of gradient descent (GD) and stochastic gradient descent (SGD) in the non-convex setting.

### Gradient descent

Similar to the convex setting, we still want to bound the gradients along the trajectory. However, in the non-convex setting, the gradient norm is not necessarily non-increasing. Fortunately, similar to the classical analyses, the function value is still non-increasing and thus a potential function, as formally shown in the following lemma, whose proof is deferred to Appendix E.

**Lemma 5.1**.: _Suppose \(f\) is \(\ell\)-smooth where \(\ell\) is sub-quadratic. For any given \(F\geq 0\), let \(G:=\sup\left\{u\geq 0\mid u^{2}\leq 2\ell(2u)\cdot F\right\}\) and \(L:=\ell(2G)\). For any \(x\in\mathcal{X}\) satisfying \(f(x)-\bar{f}^{*}\leq F\), define \(x^{+}:=x-\eta\nabla f(x)\) where \(\eta\leq 2/L\), we have \(x^{+}\in\mathcal{X}\) and \(f(x^{+})\leq f(x)\)._

Then using a standard induction argument, we can show \(f(x_{t})\leq f(x_{0})\) for all \(t\geq 0\). According to Corollary 3.6, it implies bounded gradients along the trajectory. Therefore, we can show convergence of gradient descent as in the following theorem, whose proof is deferred to Appendix E.

**Theorem 5.2**.: _Suppose \(f\) is \(\ell\)-smooth where \(\ell\) is sub-quadratic. Let \(G:=\sup\left\{u\geq 0\mid u^{2}\leq 2\ell(2u)\cdot(f(x_{0})-f^{*})\right\}\) and \(L:=\ell(2G)\). If \(\eta\leq 1/L\), the iterates generated by (3) satisfy \(\left\|\nabla f(x_{t})\right\|\leq G\) for all \(t\geq 0\) and_

\[\frac{1}{T}\sum_{t<T}\left\|\nabla f(x_{t})\right\|^{2}\leq\frac{2(f(x_{0})-f^ {*})}{\eta T}.\]

It is clear that Theorem 5.2 gives the classical \(\mathcal{O}(1/\epsilon^{2})\) gradient complexity to achieve an \(\epsilon\)-stationary point, which is optimal as it matches the lower bound in (Carmon et al., 2017).

### Stochastic gradient descent

In this part, we present the convergence result for stochastic gradient descent defined as follows.

\[x_{t+1}=x_{t}-\eta g_{t},\] (4)

where \(g_{t}\) is an estimate of the gradient \(\nabla f(x_{t})\). We consider the following standard assumption on the gradient noise \(\epsilon_{t}:=g_{t}-\nabla f(x_{t})\).

**Assumption 4**.: \(\mathbb{E}_{t-1}[\epsilon_{t}]=0\) and \(\mathbb{E}_{t-1}\left[\left\|\epsilon_{t}\right\|^{2}\right]\leq\sigma^{2}\) for some \(\sigma\geq 0\), where \(\mathbb{E}_{t-1}\) denotes the expectation conditioned on \(\{g_{s}\}_{s<t}\).

Under Assumption 4, we can obtain the following theorem.

**Theorem 5.3**.: _Suppose \(f\) is \(\ell\)-smooth where \(\ell\) is sub-quadratic. For any \(0<\delta<1\), we denote \(F:=8(f(x_{0})-f^{*}+\sigma)/\delta\) and \(G:=\sup\{u\geq 0\mid u^{2}\leq 2\ell(2u)\cdot F\}<\infty\). Denote \(L:=\ell(2G)\) and choose \(\eta\leq\min\left\{\frac{1}{2L},\frac{1}{4G\sqrt{T}}\right\}\) and \(T\geq\frac{F}{\eta\epsilon^{2}}\) for any \(\epsilon>0\). Then with probability at least \(1-\delta\), the iterates generated by (4) satisfy \(\left\|\nabla f(x_{t})\right\|\leq G\) for all \(t<T\) and_

\[\frac{1}{T}\sum_{t<T}\left\|\nabla f(x_{t})\right\|^{2}\leq\epsilon^{2}.\]

As we choose \(\eta=\mathcal{O}(1/\sqrt{T})\), Theorem 5.3 gives the classical \(\mathcal{O}(1/\epsilon^{4})\) gradient complexity, where we ignore non-leading terms. This rate is optimal as it matches the lower bound in (Arjevani et al., 2019). The key to its proof is again to bound the gradients along the trajectory. However, bounding gradients in the stochastic setting is much more challenging than in the deterministic setting, especially with the heavy-tailed noise in Assumption 4. We briefly discuss some of the challenges as well as our approach below and defer the detailed proof of Theorem 5.3 to Appendix F.

First, due to the existence of heavy-tailed gradient noise as considered in Assumption 4, neither the gradient nor the function values is non-increasing. The induction analyses we have used in the deterministic setting hardly work. In addition, to apply Lemma 3.3, we need to control the update at each step and make sure \(\left\|x_{t+1}-x_{t}\right\|=\eta\left\|g_{t}\right\|\leq G/L\). However, \(g_{t}\) might be unbounded due to the potentially unbounded gradient noise.

To overcome these challenges, we define the following random variable \(\tau\).

\[\tau_{1} :=\min\{t\mid f(x_{t+1})-f^{*}>F\}\wedge T,\] (5) \[\tau_{2} :=\min\left\{t\middle\|\epsilon_{t}\middle\|>\frac{G}{5\eta L} \right\}\wedge T,\] \[\tau :=\min\{\tau_{1},\tau_{2}\},\]

where we use \(a\wedge b\) to denote \(\min\{a,b\}\) for any \(a,b\in\mathbb{R}\). Then at least before time \(\tau\), we know that the function value and gradient noise are bounded, where the former also implies bounded gradients according to Corollary 3.6. Therefore, it suffices to show the probability of \(\tau<T\) is small, which means with a high probability, \(\tau=T\) and thus gradients are always bounded before \(T\).

Since both the gradient and noise are bounded for \(t<\tau\), it is straightforward to bound the update \(\|x_{t+1}-x_{t}\|\), which allows us to use Lemma 3.3 and other useful properties. However, it is still non-trivial to upper bound \(\mathbb{E}[f(x_{\tau})-f^{*}]\) as \(\tau\) is a random variable instead of a fixed time step. Fortunately, \(\tau\) is a stopping time with nice properties. That is because both \(f(x_{t+1})\) and \(\epsilon_{t}=g_{t}-\nabla f(x_{t})\) only depend on \(\{g_{s}\}_{s\leq t}\), i.e., the stochastic gradients up to \(t\). Therefore, for any fixed \(t\), the events \(\{\tau>t\}\) only depend on \(\{g_{s}\}_{s\leq t}\), which show \(\tau\) is a stopping time. Then with a careful analysis, we are still able to obtain an upper bound on \(\mathbb{E}[f(x_{\tau})-f^{*}]=\mathcal{O}(1)\).

On the other hand, \(\tau<T\) means either \(\tau=\tau_{1}<T\) or \(\tau=\tau_{2}<T\). If \(\tau=\tau_{1}<T\), by its definition, we know \(f(x_{\tau+1})-f^{*}>F\). Roughly speaking, it also suggests \(f(x_{\tau})-f^{*}>F/2\). If we choose \(F\) such that it is much larger than the upper bound on \(\mathbb{E}[f(x_{\tau})-f^{*}]\) we just obtained, by Markov's inequality, we can show the probability of \(\tau=\tau_{1}<T\) is small. In addition, by union bound and Chebyshev's inequality, the probability of \(\tau_{2}<T\) can also be bounded by a small constant. Therefore, we have shown \(\tau<T\). Then the rest of the analysis is not too hard following the classical analysis.

### Reconciliation with existing lower bounds

In this section, we reconcile our convergence results for constant-stepsize GD/SGD in the non-convex setting with existing lower bounds in (Zhang et al., 2019) and (Wang et al., 2022), based on which the authors claim that adaptive methods such as GD/SGD with clipping and Adam are provably faster than non-adaptive GD/SGD. This may seem to contradict our convergence results. In fact, we show that any gain in adaptive methods is at most by constant factors, as GD and SGD already achieve the optimal rates in the non-convex setting.

(Zhang et al., 2019) provides both upper and lower complexity bounds for constant-stepsize GD for \((L_{0},L_{1})\)-smooth functions, and shows that its complexity is \(\mathcal{O}(M\epsilon^{-2})\), where

\[M:=\sup\{\|\nabla f(x)\|\mid f(x)\leq f(x_{0})\}\]

is the supremum gradient norm below the level set of the initial function value. If \(M\) is very large, then the \(\mathcal{O}(M\epsilon^{-2})\) complexity can be viewed as a negative result, and as evidence that constant-stepsize GD can be slower than GD with gradient clipping, since in the latter case, they obtain the \(\mathcal{O}(\epsilon^{-2})\) complexity without \(M\). However, based on our Corollary 3.6, their \(M\) can be actually bounded by our \(G\), which is a constant. Therefore, the gain in adaptive methods is at most by constant factors.

(Wang et al., 2022) further provides a lower bound which shows non-adaptive GD may diverge for some examples. However, their counter-example does not allow the stepsize to depend on the initial sub-optimality gap. In contrast, our stepsize \(\eta\) depends on the effective smoothness constant \(L\), which depends on the initial sub-optimality gap through \(G\). Therefore, there is no contradiction here either. We should point out that in the practice of training neural networks, the stepsize is usually tuned after fixing the loss function and initialization, so it does depend on the problem instance and initialization.

### Lower bound

For \((\rho,L_{0},L_{\rho})\)-smooth functions with \(\rho<2\), it is easy to verify that the constant \(G\) in both Theorem 5.2 and Theorem 5.3 is a polynomial function of problem-dependent parameters like \(L_{0},L_{\rho},f(x_{0})-f^{*},\sigma\), etc. In other words, GD and SGD are provably efficient methods in the non-convex setting for \(\rho<2\). In this section, we show that the requirement of \(\rho<2\) is necessary in the non-convex setting with the lower bound for GD in the following Theorem 5.4, whose proof is deferred in Appendix G. Since SGD reduces to GD when there is no gradient noise, it is also a lower bound for SGD.

**Theorem 5.4**.: _Given \(L_{0},L_{2},G_{0},\Delta_{0}>0\) satisfying \(L_{2}\Delta_{0}\geq 10\), for any \(\eta\geq 0\), there exists a \((2,L_{0},L_{2})\)-smooth function \(f\) that satisfies Assumptions 1 and 2, and initial point \(x_{0}\) that satisfies \(\|\nabla f(x_{0})\|\leq G_{0}\) and \(f(x_{0})-f^{*}\leq\Delta_{0}\), such that gradient descent with stepsize \(\eta\) (3) either cannot reach a \(1\)-stationary point or takes at least \(\exp(L_{2}\Delta_{0}/8)/6\) steps to reach a \(1\)-stationary point._

## 6 Conclusion

In this paper, we generalize the standard Lipschitz smoothness as well as the \((L_{0},L_{1})\)-smoothness (Zhang et al., 2020) conditions to the \(\ell\)-smoothness condition, and develop a new approach for analyzing the convergence under this condition. The approach uses different techniques for several methods and settings to bound the gradient along the optimization trajectory, which allows us to obtain stronger results for both convex and non-convex problems. We obtain the classical rates for GD/SGD/NAG methods in the convex and/or non-convex setting. Our results challenge the folklore belief on the necessity of adaptive methods for generalized smooth functions.

There are several interesting future directions following this work. First, the \(\ell\)-smoothness can perhaps be further generalized by allowing \(\ell\) to also depend on potential functions in each setting, besides the gradient norm. In addition, it would also be interesting to see if the techniques of bounding gradients along the trajectory that we have developed in this and the concurrent work (Li et al., 2023) can be further generalized to other methods and problems and to see whether more efficient algorithms can be obtained. Finally, although we justified the necessity of the requirement of \(\ell\)-smoothness with a _sub-quadratic_\(\ell\) in the non-convex setting, it is not clear whether it is also necessary for NAG in the convex setting, another interesting open problem.

## Acknowledgments

This work was supported, in part, by the MIT-IBM Watson AI Lab and ONR Grants N00014-20-1-2394 and N00014-23-1-2299. We also acknowledge support from DOE under grant DE-SC0022199, and NSF through awards DMS-2031883, DMS-1953181, and DMS-2022448 (TRIPODS program).

## References

* 214, 2019.
* Boyd and Vandenberghe (2004) Stephen P Boyd and Lieven Vandenberghe. _Convex optimization_. Cambridge university press, 2004.
* Carmon et al. (2017) Yair Carmon, John C. Duchi, Oliver Hinder, and Aaron Sidford. Lower bounds for finding stationary points i. _Mathematical Programming_, pages 1-50, 2017.
* Chen et al. (2023) Ziyi Chen, Yi Zhou, Yingbin Liang, and Zhaosong Lu. Generalized-smooth nonconvex optimization is as efficient as smooth nonconvex optimization. _arXiv preprint arXiv:2303.02854_, 2023.
* Crawshaw et al. (2022) Michael Crawshaw, Mingrui Liu, Francesco Orabona, Wei Zhang, and Zhenxun Zhuang. Robustness to unbounded smoothness of generalized signsgd. _Advances in Neural Information Processing Systems_, 35:9955-9968, 2022.
* d'Aspremont et al. (2021) Alexandre d'Aspremont, Damien Scieur, and Adrien Taylor. Acceleration methods. _Foundations and Trends(r) in Optimization_, 5(1-2):1-245, 2021. ISSN 2167-3888. doi: 10.1561/2400000036. URL http://dx.doi.org/10.1561/2400000036.
* Faw et al. (2023) Matthew Faw, Litu Rout, Constantine Caramanis, and Sanjay Shakkottai. Beyond uniform smoothness: A stopped analysis of adaptive sgd. _ArXiv_, abs/2302.06570, 2023.
* Ghadimi and Lan (2013) Saeed Ghadimi and Guanghui Lan. Stochastic first-and zeroth-order methods for nonconvex stochastic programming. _SIAM Journal on Optimization_, 23(4):2341-2368, 2013.
* Ghadimi et al. (2015)* Kingma and Ba (2014) Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. _CoRR_, abs/1412.6980, 2014.
* Li et al. (2023) Haochuan Li, Ali Jadbabaie, and Alexander Rakhlin. Convergence of adam under relaxed assumptions. _arXiv preprint arXiv:2304.13972_, 2023.
* Nemirovskij and Yudin (1983) Arkadij Semenovic Nemirovskij and David Borisovich Yudin. _Problem complexity and method efficiency in optimization_. Wiley-Interscience, 1983.
* Nesterov (2003) Yurii Nesterov. _Introductory lectures on convex optimization: A basic course_, volume 87. Springer Science & Business Media, 2003.
* Qian et al. (2021) Jiang Qian, Yuren Wu, Bojin Zhuang, Shaojun Wang, and Jing Xiao. Understanding gradient clipping in incremental gradient methods. In _International Conference on Artificial Intelligence and Statistics_, 2021.
* Reisizadeh et al. (2023) Amirhossein Reisizadeh, Haochuan Li, Subhro Das, and Ali Jadbabaie. Variance-reduced clipping for non-convex optimization. _ArXiv_, abs/2303.00883, 2023.
* Wang et al. (2022) Bohan Wang, Yushun Zhang, Huishuai Zhang, Qi Meng, Zhi-Ming Ma, Tie-Yan Liu, and Wei Chen. Provable adaptivity in adam. _arXiv preprint arXiv:2208.09900_, 2022.
* Zhang et al. (2020) Bohang Zhang, Jikai Jin, Cong Fang, and Liwei Wang. Improved analysis of clipping algorithms for non-convex optimization. _ArXiv_, abs/2010.02519, 2020.
* Zhang et al. (2019) Jingzhao Zhang, Tianxing He, Suvrit Sra, and Ali Jadbabaie. Why gradient clipping accelerates training: A theoretical justification for adaptivity. _arXiv preprint arXiv:1905.11881_, 2019.
* Zhao et al. (2021) Shen-Yi Zhao, Yin-Peng Xie, and Wu-Jun Li. On the convergence and improvement of stochastic normalized gradient descent. _Science China Information Sciences_, 64, 2021.

Proofs related to generalized smoothness

In this section, we provide the proofs of propositions and lemmas related to the generalized smoothness condition in Definition 1 or 2. First, in Appendix A.1, we justify the examples we discussed in Section 3. Next, we provide the detailed proof of Proposition 3.2 in Appendix A.2. Finally, we provide the proofs of the useful properties of generalized smoothness in Appendix A.3, including Lemma 3.3, Lemma 3.5, and Corollary 3.6 stated in Section 3.1.2.

### Justification of examples in Section 3

In this section, we justify the univariate examples of \((\rho,L_{0},L_{\rho})\)-smooth functions listed in Table 2 and also provide the proof of Propositions 3.7.

First, it is well-known that all quadratic functions have bounded Hessian and are Lipschitz smooth, corresponding to \(\rho=0\). Next, [Zhang et al., 2019, Lemma 2] shows that any univariate polynomial is \((L_{0},L_{1})\)-smooth, corresponding to \(\rho=1\). Then, regarding the exponential function \(f(x)=a^{x}\) where \(a>1\), we have \(f^{\prime}(x)=\log(a)a^{x}\) and \(f^{\prime\prime}(x)=\log(a)^{2}a^{x}=\log(a)f^{\prime}(x)\), which implies \(f\) is \((1,0,\log(a))\)-smooth. Similarly, by standard calculations, it is straight forward to verify that logarithmic functions and \(x^{p}\), \(p\neq 1\) are also \((\rho,L_{0},L_{\rho})\)-smooth with \(\rho=2\) and \(\rho=\frac{p-2}{p-1}\) respectively.

So far we have justified all the examples in Table 2 except double exponential functions \(a^{(b^{x})}\) and rational functions, which will be justified rigorously by the two propositions below.

First, for double exponential functions in the form of \(f(x)=a^{(b^{x})}\) where \(a,b>1\), we have the following proposition, which shows \(f\) is \((\rho,L_{0},L_{\rho})\)-smooth for any \(\rho>1\).

**Proposition A.1**.: _For any \(\rho>1\), the double exponential function \(f(x)=a^{(b^{x})}\), where \(a,b>1\), is \((\rho,L_{0},L_{\rho})\)-smooth for some \(L_{0},L_{\rho}\geq 0\). However, it is not necessarily \((L_{0},L_{1})\)-smooth for any \(L_{0},L_{1}\geq 0\)._

Proof of Proposition a.1.: By standard calculations, we can obtain

\[f^{\prime}(x)=\log(a)\log(b)\,b^{x}a^{(b^{x})},\quad f^{\prime\prime}(x)=\log( b)(\log(a)b^{x}+1)\cdot f^{\prime}(x).\] (6)

Note that if \(\rho>1\),

\[\lim_{x\to+\infty}\frac{|f^{\prime}(x)|^{\rho}}{|f^{\prime\prime}(x)|}=\lim_{ x\to+\infty}\frac{|f^{\prime}(x)|^{\rho-1}}{\log(b)(\log(a)b^{x}+1)}=\lim_{y\to+ \infty}\frac{\left(\log(a)\log(b)y\right)^{\rho-1}a^{(\rho-1)y}}{\log(b)(\log (a)y+1)}=\infty,\]

where the first equality is a direct calculation based on (6); the second equality uses change of variable \(y=b^{x}\); and the last equality is because exponential functions grow faster than affine functions. Therefore, for any \(L_{\rho}>0\), there exists \(x_{0}\in\mathbb{R}\) such that \(|f^{\prime\prime}(x)|\leq L_{\rho}\,|f^{\prime}(x)|^{\rho}\) if \(x>x_{0}\). Next, note that \(\lim_{x\to-\infty}f^{\prime\prime}(x)=0\). Then for any \(\lambda_{1}>0\), there exists \(x_{1}\in\mathbb{R}\) such that \(|f^{\prime\prime}(x)|\leq\lambda_{1}\) if \(x<x_{1}\). Also, since \(f^{\prime\prime}\) is continuous, by Weierstrass's Theorem, we have \(|f^{\prime\prime}(x)|\leq\lambda_{2}\) if \(x_{1}\leq x\leq x_{0}\) for some \(\lambda_{2}>0\). Then denoting \(L_{0}=\max\{\lambda_{1},\lambda_{2}\}\), we know \(f\) is \((\rho,L_{0},L_{\rho})\)-smooth.

Next, to show \(f\) is not necessarily \((L_{0},L_{1})\)-smooth, consider the specific double exponential function \(f(x)=e^{(e^{x})}\). Then we have

\[f^{\prime}(x)=e^{x}e^{(e^{x})},\quad f^{\prime\prime}(x)=(e^{x}+1)\cdot f^{ \prime}(x).\]

For any \(x\geq\max\left\{\log(L_{0}+1),\log(L_{1}+1)\right\}\), we can show that

\[|f^{\prime\prime}(x)|>(L_{1}+1)f^{\prime}(x)>L_{0}+L_{1}\left|f^{\prime}(x) \right|,\]

which shows \(f\) is not \((L_{0},L_{1})\) smooth for any \(L_{0},L_{1}\geq 0\). 

In the next proposition, we show that any univariate rational function \(f(x)=P(x)/Q(x)\), where \(P\) and \(Q\) are two polynomials, is \((\rho,L_{0},L_{\rho})\)-smooth with \(\rho=1.5\).

**Proposition A.2**.: _The rational function \(f(x)=P(x)/Q(x)\), where \(P\) and \(Q\) are two polynomials, is \((1.5,L_{0},L_{1.5})\)-smooth for some \(L_{0},L_{1.5}\geq 0\). However, it is not necessarily \((\rho,L_{0},L_{\rho})\)-smooth for any \(\rho<1.5\) and \(L_{0},L_{\rho}\geq 0\)._

[MISSING_PAGE_FAIL:13]

### Proof of Proposition 3.2

In order to prove Proposition 3.2, we need the following several lemmas. First, the lemma below partially generalizes Gronwall's inequality.

**Lemma A.3**.: _Let \(\alpha:[a,b]\to[0,\infty)\) and \(\beta:[0,\infty)\to(0,\infty)\) be two continuous functions. Suppose \(\alpha^{\prime}(t)\leq\beta(\alpha(t))\) almost everywhere over \((a,b)\). Denote function \(\phi(u):=\int\frac{1}{\beta(u)}\,du\). We have for all \(t\in[a,b]\),_

\[\phi(\alpha(t))\leq\phi(\alpha(a))-a+t.\]

Proof of Lemma a.3.: First, by definition, we know that \(\phi\) is increasing since \(\phi^{\prime}=\frac{1}{\beta}>0\). Let function \(\gamma:[a,b]\to\mathbb{R}\) be the solution of the following differential equation

\[\gamma^{\prime}(t)=\beta(\gamma(t))\ \ \forall t\in(a,b),\quad\gamma(a)= \alpha(a).\] (11)

Then we have

\[d\phi(\gamma(t))=\frac{d\gamma(t)}{\beta(\gamma(t))}=dt.\]

Integrating both sides, noting that \(\gamma(a)=\alpha(a)\) by (11), we obtain

\[\phi(\gamma(t))-\phi(\alpha(a))=t-a.\]

Then it suffices to show \(\phi(\alpha(t))\leq\phi(\gamma(t)),\ \forall t\in[a,b]\). Note that the following inequality holds almost everywhere.

\[(\phi(\alpha(t))-\phi(\gamma(t)))^{\prime}=\phi^{\prime}(\alpha(t))\alpha^{ \prime}(t)-\phi^{\prime}(\gamma(t))\gamma^{\prime}(t)=\frac{\alpha^{\prime}(t )}{\beta(\alpha(t))}-\frac{\gamma^{\prime}(t)}{\beta(\gamma(t))}\leq 0,\]

where the inequality is because \(\alpha^{\prime}(t)\leq\beta(\alpha(t))\) by the assumption of this lemma and \(\gamma^{\prime}(t)=\beta(\gamma(t))\) by (11). Since \(\phi(\alpha(a))-\phi(\gamma(a))=0\), we know for all \(t\in[a,b]\), \(\phi(\alpha(t))\leq\phi(\gamma(t))\), which completes the proof. 

With Lemma A.3, one can bound the gradient norm within a small enough neighborhood of a given point as in the following lemma.

**Lemma A.4**.: _If the objective function \(f\) is \(\ell\)-smooth, for any two points \(x,y\in\mathbb{R}^{d}\) such that the closed line segment between \(x\) and \(y\) is contained in \(\mathcal{X}\), if \(\|y-x\|\leq\frac{a}{\ell(\|\nabla f(x)\|+a)}\) for any \(a>0\), we have_

\[\|\nabla f(y)\|\leq\|\nabla f(x)\|+a.\]

Proof of Lemma a.4.: Denote \(z(t):=(1-t)x+ty\) for \(0\leq t\leq 1\). Then we know \(z(t)\in\mathcal{X}\) for all \(0\leq t\leq 1\) by the assumption made in this lemma. Then we can also define \(\alpha(t):=\|\nabla f(z(t))\|\) for \(0\leq t\leq 1\). Note that for any \(0\leq t\leq s\leq 1\), by triangle inequality,

\[\alpha(s)-\alpha(t)\leq\|\nabla f(z(s))-\nabla f(z(t))\|\,.\] (12)

We know that \(\alpha(t)=\|\nabla f(z(t))\|\) is differentiable almost everywhere since \(f\) is second order differentiable almost everywhere (Here we assume \(\alpha(t)\neq 0\) for \(0<t<1\) without loss of generality. Otherwise, one can define \(t_{m}=\sup\{0<t<1\mid\alpha(t)=0\}\) and consider the interval \([t_{m},1]\) instead). Then the following equality holds almost everywhere

\[\alpha^{\prime}(t)= \lim_{s\downarrow t}\frac{\alpha(s)-\alpha(t)}{s-t}\leq\lim_{s \downarrow t}\frac{\|\nabla f(z(s))-\nabla f(z(t))\|}{s-t}=\left\|\lim_{s \downarrow t}\frac{\nabla f(z(s))-\nabla f(z(t))}{s-t}\right\|\] \[= \left\|\nabla^{2}f(z(t))(y-x)\right\|\leq\left\|\nabla^{2}f(z(t)) \right\|\left\|y-x\right\|,\]

where the first inequality is due to (12) and the last inequality is by Definition 1. Let \(\beta(u):=\ell(u)\cdot\|y-x\|\) and \(\phi(u):=\int_{0}^{u}\frac{1}{\beta(v)}dv\). By Lemma A.3, we know that

\[\phi\left(\|\nabla f(y)\|\right)=\phi(u(1))\leq\phi(u(0))+1=\phi\left(\|\nabla f (x)\|\right)+1.\]Denote \(\psi(u):=\int_{0}^{u}\frac{1}{\ell(v)}dv=\phi(u)\cdot\|y-x\|\). We have

\[\psi\left(\|\nabla f(y)\|\right)\leq \psi\left(\|\nabla f(x)\|\right)+\|y-x\|\] \[\leq \psi\left(\|\nabla f(x)\|\right)+\frac{a}{\ell(\|\nabla f(x)\|+a)}\] \[\leq \int_{0}^{\|\nabla f(x)\|}\frac{1}{\ell(v)}\,dv+\int_{\|\nabla f (x)\|}^{\|\nabla f(x)\|+a}\frac{1}{\ell(v)}\,dv\] \[= \psi(\|\nabla f(x)\|+a).\]

Since \(\psi\) is increasing, we have \(\|\nabla f(y)\|\leq\|\nabla f(x)\|+a\). 

With Lemma A.4, we are ready to prove Proposition 3.2.

Proof of Proposition 3.2.: We prove the two directions in this proposition separately.

**1. An \((r,\ell)\)-smooth function is \(\ell\)-smooth.**

For each fixed \(x\in\mathcal{X}\) where \(\nabla^{2}f(x)\) exists and any unit-norm vector \(w\), by Definition 2, we know that for any \(t\leq r(\|\nabla f(x)\|)\),

\[\|\nabla f(x+tw)-\nabla f(x)\|\leq t\cdot\ell(\|\nabla f(x)\|).\]

Then we know that

\[\left\|\nabla^{2}f(x)w\right\|= \left\|\lim_{t\downarrow 0}\frac{1}{t}(\nabla f(x+tw)-\nabla f(x))\right\|\] \[= \lim_{t\downarrow 0}\frac{1}{t}\left\|(\nabla f(x+tw)-\nabla f(x)) \right\|\leq\ell(\|\nabla f(x)\|),\]

which implies \(\left\|\nabla^{2}f(x)\right\|\leq\ell(\|\nabla f(x)\|)\) for any point \(x\) if \(\nabla^{2}f(x)\) exists.

Then it suffices to show that \(\nabla^{2}f(x)\) exists almost everywhere. Note that for each \(x\in\mathcal{X}\), Definition 2 states that the gradient function is \(\ell(\|\nabla f(x)\|)\) Lipschitz within the ball \(\mathcal{B}(x,r(\|\nabla f(x)\|))\). Then by Rademacher's Theorem, \(f\) is twice differentiable almost everywhere within this ball. Then we can show it is also twice differentiable almost everywhere within the entire domain \(\mathcal{X}\) as long as we can cover \(\mathcal{X}\) with countably many such balls. Define \(\mathcal{S}_{n}:=\{x\in\mathcal{X}\mid n\leq\|\nabla f(x)\|\leq n+1\}\) for integer \(n\geq 0\). We have \(\mathcal{X}=\cup_{n\geq 0}\mathcal{S}_{n}\). One can easily find an internal covering of \(\mathcal{S}_{n}\) with balls of size \(r(n+1)\)3, i.e., there exist \(\{x_{n,i}\}_{i\geq 0}\), where \(x_{n,i}\in\mathcal{S}_{n}\), such that \(\mathcal{S}_{n}\subseteq\cup_{i\geq 0}\mathcal{B}(x_{n,i},r(n+1))\subseteq \cup_{i\geq 0}\mathcal{B}(x_{n,i},r(\|\nabla f(x_{n,i})\|))\). Therefore we have \(\mathcal{X}\subseteq\cup_{n,i\geq 0}\mathcal{B}(x_{n,i},r(\|\nabla f(x_{n,i})\|))\) which completes the proof.

Footnote 3: We can find an internal covering in the following way. We first cover \(\mathcal{S}_{n}\) with countably many hyper-cubes of length \(r(n+1)/\sqrt{d}\), which is obviously doable. Then for each hyper-cube that intersects with \(\mathcal{S}_{n}\), we pick one point from the intersection. Then the ball centered at the picked point with radius \(r(n+1)\) covers this hyper-cube. Therefore, the union of all such balls can cover \(\mathcal{S}_{n}\).

**2. An \(\ell\)-smooth function satisfying Assumption 1 is \((r,m)\)-smooth where \(m(u):=\ell(u+a)\) and \(r(u):=a/m(u)\) for any \(a>0\).**

For any \(y\in\mathbb{R}^{d}\) satisfying \(\|y-x\|\leq r(\|\nabla f(x)\|)=\frac{a}{\ell(\|\nabla f(x)\|+a)}\), denote \(z(t):=(1-t)x+ty\) for \(0\leq t\leq 1\). We first show \(y\in\mathcal{X}\) by contradiction. Suppose \(y\notin\mathcal{X}\), let us define \(t_{\text{b}}:=\inf\{0\leq t\leq 1\mid z(t)\notin\mathcal{X}\}\) and \(z_{\text{b}}:=z(t_{\text{b}})\). Then we know \(z_{\text{b}}\) is a boundary point of \(\mathcal{X}\). Since \(f\) is a closed function with an open domain, we have

\[\lim_{t\uparrow t_{\text{b}}}f(z(t))=\infty.\] (13)

On the other hand, by the definition of \(t_{\text{b}}\), we know \(z(t)\in\mathcal{X}\) for every \(0\leq t<t_{\text{b}}\). Then by Lemma A.4, for all \(0\leq t<t_{\text{b}}\), we have \(\|\nabla f(z(t))\|\leq\|\nabla f(x)\|+a\). Therefore for all \(0\leq t<t_{\text{b}}\),

\[f(z(t))\leq f(x)+\int_{0}^{t}\bigl{\langle}\nabla f(z(s)),y-x\bigr{\rangle}\,ds\] \[\leq f(x)+(\|\nabla f(x)\|+a)\cdot\|y-x\|\] \[< \infty,\]which contradicts (13). Therefore we have shown \(y\in\mathcal{X}\). Since \(y\) is chosen arbitrarily with the ball \(\mathcal{B}(x,r(\|\nabla f(x)\|))\), we have \(\mathcal{B}(x,r(\|\nabla f(x)\|))\subseteq\mathcal{X}\). Then for any \(x_{1},x_{2}\in\mathcal{B}(x,r(\|\nabla f(x)\|))\), we denote \(w(t):=tx_{1}+(1-t)x_{2}\). Then we know \(w(t)\in\mathcal{B}(x,r(\|\nabla f(x)\|))\) for all \(0\leq t\leq 1\) and can obtain

\[\|\nabla f(x_{1})-\nabla f(x_{2})\|= \left\|\int_{0}^{1}\nabla^{2}f(w(t))\cdot\left(x_{1}-x_{2}\right) dt\right\|\] \[\leq \left\|x_{1}-x_{2}\right\|\cdot\int_{0}^{1}\ell(\|\nabla f(x)\|+a )\,dt\] \[= m(\|\nabla f(x)\|)\cdot\left\|x_{1}-x_{2}\right\|,\]

where the last inequality is due to Lemma A.4. 

### Proofs of lemmas implied by generalized smoothness

In this part, we provide the proofs of the useful properties stated in Section 3.1.2, including Lemma 3.3, Lemma 3.5, and Corollary 3.6.

Proof of Lemma 3.3.: First, note that since \(\ell\) is non-decreasing and \(r\) is non-increasing, we have \(\ell(\|\nabla f(x)\|)\leq\ell(G)=L\) and \(r(G)\leq r(\|\nabla f(x)\|)\). Then by Definition 2, we directly have that \(\mathcal{B}(x,r(G))\subseteq\mathcal{B}(x,r(\|\nabla f(x)\|))\subseteq \mathcal{X}\), and that for any \(x_{1},x_{2}\in\mathcal{B}(x,r(G))\), we have

\[\left\|\nabla f(x_{1})-\nabla f(x_{2})\right\|\leq\ell(\|\nabla f(x)\|)\left\| x_{1}-x_{2}\right\|\leq L\left\|x_{1}-x_{2}\right\|.\]

Next, for the second inequality in (2), define \(z(t):=(1-t)x_{2}+tx_{1}\) for \(0\leq t\leq 1\). We know \(z(t)\in\mathcal{B}(x,r(G))\). Note that we have shown

\[\left\|\nabla f(z(t))-\nabla f(x_{2})\right\|\leq L\left\|z(t)-x_{2}\right\|= tL\left\|x_{1}-x_{2}\right\|.\] (14)

Then we have

\[f(x_{1})-f(x_{2})= \int_{0}^{1}\bigl{\langle}\nabla f(z(t),x_{1}-x_{2}\bigr{\rangle} \,dt\] \[= \int_{0}^{1}\bigl{\langle}\nabla f(x_{2}),x_{1}-x_{2}\bigr{\rangle} +\bigl{\langle}\nabla f(z(t))-\nabla f(x_{2}),x_{1}-x_{2}\bigr{\rangle}\,dt\] \[\leq \bigl{\langle}\nabla f(x_{2}),x_{1}-x_{2}\bigr{\rangle}+L\left\| x_{1}-x_{2}\right\|^{2}\int_{0}^{1}t\,dt\] \[= \bigl{\langle}\nabla f(x_{2}),x_{1}-x_{2}\bigr{\rangle}+\frac{L} {2}\left\|x_{1}-x_{2}\right\|^{2},\]

where the inequality is due to (14). 

Proof of Lemma 3.5.: If \(f\) is \(\ell\)-smooth, by Proposition 3.2, \(f\) is also \((r,m)\)-smooth where \(m(u)=\ell(2u)\) and \(r(u)=u/\ell(2u)\). Then by Lemma 3.3 where we choose \(G=\|\nabla f(x)\|\), we have that \(\mathcal{B}\left(x,\frac{\|\nabla f(x)\|}{\ell(2\|\nabla f(x)\|)}\right) \subseteq\mathcal{X}\), and that for any \(x_{1},x_{2}\in\mathcal{B}\left(x,\frac{\|\nabla f(x)\|}{\ell(2\|\nabla f(x)\| )}\right)\), we have

\[f(x_{1})\leq f(x_{2})+\bigl{\langle}\nabla f(x_{2}),x_{1}-x_{2}\bigr{\rangle}+ \frac{\ell(2\left\|\nabla f(x)\right\|)}{2}\left\|x_{1}-x_{2}\right\|.\]

Choosing \(x_{2}=x\) and \(x_{1}=x-\frac{\nabla f(x)}{\ell(2\|\nabla f(x)\|)}\), it is easy to verify that \(x_{1},x_{2}\in\mathcal{B}\left(x,\frac{\|\nabla f(x)\|}{\ell(2\|\nabla f(x)\| )}\right)\). Therefore, we have

\[f^{*}\leq f\left(x-\frac{\nabla f(x)}{\ell(2\left\|\nabla f(x)\right\|)} \right)\leq f(x)-\frac{\left\|\nabla f(x)\right\|^{2}}{2\ell(2\left\|\nabla f (x)\right\|)},\]

which completes the proof.

Proof of Corollary 3.6.: We first show \(G<\infty\). Note that since \(\ell\) is sub-quadratic, we know \(\lim_{u\to\infty}2\ell(2u)/u^{2}=0\). Therefore, for any \(F>0\), there exists some \(M>0\) such that \(2\ell(2u)/u^{2}<1/F\) for every \(u>M\). In other words, for any \(u\) satisfying \(u^{2}\leq 2\ell(2u)\cdot F\), we must have \(u\leq M\). Therefore, by definition of \(G\), we have \(G\leq M<\infty\) if \(F>0\). If \(F=0\), we trivially get \(G=0<\infty\). Also, since the set \(\{u\geq 0\mid u^{2}\leq 2\ell(2u)\cdot F\}\) is closed and bounded, we know its supremum \(G\) is in this set and it is also straightforward to show \(G^{2}=2\ell(2G)\cdot F\).

Next, by Lemma 3.5, we know

\[\|\nabla f(x)\|^{2}\leq 2\ell(2\left\|\nabla f(x)\right\|)\cdot(f(x)-f^{*}) \leq 2\ell(2\left\|\nabla f(x)\right\|)\cdot F.\]

Then based on the definition of \(G\), we have \(\|\nabla f(x)\|\leq G\). 

## Appendix B Analysis of GD for convex functions

In this section, we provide the detailed convergence analysis of gradient descent in the convex setting, including the proofs of Lemma 4.1 and Theorem 4.2, for which the following lemma will be helpful.

**Lemma B.1** (Co-coercivity).: _If \(f\) is convex and \((r,\ell)\)-smooth, for any \(x\in\mathcal{X}\) and \(y\in\mathcal{B}(x,r(\|\nabla f(x)\|)/2)\), we have \(y\in\mathcal{X}\) and_

\[\left\langle\nabla f(x)-\nabla f(y),x-y\right\rangle\geq\frac{1}{L}\left\| \nabla f(x)-\nabla f(y)\right\|^{2},\]

_where \(L=\ell(\|\nabla f(x)\|)\)._

Proof of Lemma b.1.: Define the Bregman divergences \(\phi_{x}(w):=f(w)-\left\langle\nabla f(x),w\right\rangle\) and \(\phi_{y}(w):=f(w)-\left\langle\nabla f(y),w\right\rangle\), which are both convex functions. Since \(\nabla\phi_{x}(w)=\nabla f(w)-\nabla f(x)\), we have \(\nabla\phi_{x}(x)=0\) which implies \(\min_{w}\phi_{x}(w)=\phi_{x}(x)\) as \(\phi_{x}\) is convex. Similarly we have \(\min_{w}\phi_{y}(w)=\phi_{y}(y)\).

Denote \(r_{x}:=r(\|\nabla f(x)\|)\). Since \(f\) is \((r,\ell)\)-smooth, we know its gradient \(\nabla f\) is \(L\)-Lipschitz locally in \(\mathcal{B}(x,r_{x})\). Since \(\nabla\phi_{x}(w)-\nabla f(w)=\nabla f(x)\) is a constant, we know \(\nabla\phi_{x}\) is also \(L\)-Lipschitz locally in \(\mathcal{B}(x,r_{x})\). Then similar to the proof of Lemma 3.3, one can easily show that for any \(x_{1},x_{2}\in\mathcal{B}(x,r_{x})\), we have

\[\phi_{x}(x_{1})\leq\phi_{x}(x_{2})+\left\langle\nabla\phi_{x}(x_{2}),x_{1}-x_ {2}\right\rangle+\frac{L}{2}\left\|x_{1}-x_{2}\right\|^{2}.\] (15)

Note that for any \(y\in\mathcal{B}(x,r(\|\nabla f(x)\|)/2)\) as in the lemma statement,

\[\left\|y-\frac{1}{L}\nabla\phi_{x}(y)-x\right\|\leq\left\|y-x\right\|+\frac{1 }{L}\left\|\nabla f(y)-\nabla f(x)\right\|\leq 2\left\|y-x\right\|\leq r_{x},\]

where the first inequality uses triangle inequality and \(\nabla\phi_{x}(y)=\nabla f(y)-\nabla f(x)\); and the second inequality uses Definition 2. It implies that \(y-\frac{1}{L}\nabla\phi_{x}(y)\in\mathcal{B}(x,r_{x})\). Then we can obtain

\[\phi_{x}(x)=\min_{w}\phi_{x}(w)\leq\phi_{x}\bigg{(}y-\frac{1}{L}\nabla\phi_{x }(y)\bigg{)}\leq\phi_{x}(y)-\frac{1}{2L}\left\|\nabla\phi_{x}(y)\right\|^{2},\]

where the last inequality uses (15) where we choose \(x_{1}=y-\frac{1}{L}\nabla\phi_{x}(y)\) and \(x_{2}=y\). By the definition of \(\phi_{x}\), the above inequality is equivalent to

\[\frac{1}{2L}\left\|\nabla f(y)-\nabla f(x)\right\|^{2}\leq f(y)-f(x)-\left\langle \nabla f(x),x-y\right\rangle.\]

Similar argument can be made for \(\phi_{y}(\cdot)\) to obtain

\[\frac{1}{2L}\left\|\nabla f(y)-\nabla f(x)\right\|^{2}\leq f(x)-f(y)-\left\langle \nabla f(y),y-x\right\rangle.\]

Summing up the two inequalities, we can obtain the desired result. 

With Lemma B.1, we prove Lemma 4.1 as follows.

Proof of Lemma 4.1.: Let \(L=\ell(G)\). We first verify that \(x^{+}\in\mathcal{B}(x,r(G)/2)\). Note that

\[\left\|x^{+}-x\right\|=\left\|\eta\nabla f(x)\right\|\leq\eta G\leq r(G)/2,\]

where we choose \(\eta\leq r(G)/(2G)\). Thus by Lemma B.1, we have

\[\left\|\nabla f(x^{+})\right\|^{2} =\left\|\nabla f(x)\right\|^{2}+2\langle\nabla f(x^{+})-\nabla f (x),\nabla f(x)\rangle+\left\|\nabla f(x^{+})-\nabla f(x)\right\|^{2}\] \[=\left\|\nabla f(x)\right\|^{2}-\frac{2}{\eta}\langle\nabla f(x^{ +})-\nabla f(x),x^{+}-x\rangle+\left\|\nabla f(x^{+})-\nabla f(x)\right\|^{2}\] \[\leq\left\|\nabla f(x)\right\|^{2}+\left(1-\frac{2}{\eta L} \right)\left\|\nabla f(x^{+})-\nabla f(x)\right\|^{2}\] \[\leq\left\|\nabla f(x)\right\|^{2},\]

where the first inequality uses Lemma B.1 and the last inequality chooses \(\eta\leq 2/L\). 

With Lemma 4.1, we are ready to prove both Theorem 4.2 and Theorem 4.3.

Proof of Theorem 4.2.: Denote \(G:=\left\|\nabla f(x_{0})\right\|\). Then we trivially have \(\left\|\nabla f(x_{0})\right\|\leq G\). Lemma 4.1 states that if \(\left\|\nabla f(x_{t})\right\|\leq G\) for any \(t\geq 0\), then we also have \(\left\|\nabla f(x_{t+1})\right\|\leq\left\|\nabla f(x_{t})\right\|\leq G\). By induction, we can show that \(\left\|\nabla f(x_{t})\right\|\leq G\) for all \(t\geq 0\). Then the rest of the proof basically follows the standard textbook analysis. We still provide the detailed proof below for completeness.

Note that \(\left\|x_{t+1}-x_{t}\right\|=\eta\left\|\nabla f(x_{t})\right\|\leq\eta G\leq r (G)\), where we choose \(\eta\leq r(G)/(2G)\). Thus we can apply Lemma 3.3 to obtain

\[0 \geq f(x_{t+1})-f(x_{t})-\langle\nabla f(x_{t}),x_{t+1}-x_{t} \rangle-\frac{L}{2}\left\|x_{t+1}-x_{t}\right\|^{2}\] \[\geq f(x_{t+1})-f(x_{t})-\langle\nabla f(x_{t}),x_{t+1}-x_{t} \rangle-\frac{1}{2\eta}\left\|x_{t+1}-x_{t}\right\|^{2},\] (16)

where the last inequality chooses \(\eta\leq 1/L\). Meanwhile, by convexity between \(x_{t}\) and \(x^{*}\), we have

\[0\geq f(x_{t})-f^{*}+\langle\nabla f(x_{t}),x^{*}-x_{t}\rangle.\] (17)

Note that \((t+1)\times\)(16)+(17) gives

\[0 \geq f(x_{t})-f^{*}+\langle\nabla f(x_{t}),x^{*}-x_{t}\rangle\] \[\quad+(1+t)\bigg{(}f(x_{t+1})-f(x_{t})-\langle\nabla f(x_{t}),x_{ t+1}-x_{t}\rangle-\frac{1}{2\eta}\left\|x_{t+1}-x_{t}\right\|^{2}\bigg{)}.\]

Then reorganizing the terms of the above inequality, noting that

\[\left\|x_{t+1}-x^{*}\right\|^{2}-\left\|x_{t}-x^{*}\right\|^{2} =\left\|x_{t+1}-x_{t}\right\|^{2}+2\langle x_{t+1}-x_{t},x_{t}-x^{*}\rangle\] \[=\left\|x_{t+1}-x_{t}\right\|^{2}+2\eta\langle\nabla f(x_{t}),x^ {*}-x_{t}\rangle,\]

we can obtain

\[(t+1)(f(x_{t+1})-f^{*})+\frac{1}{2\eta}\left\|x_{t+1}-x^{*}\right\|^{2}\leq t (f(x_{t})-f^{*})+\frac{1}{2\eta}\left\|x_{t}-x^{*}\right\|^{2}.\]

The above inequality implies \(t(f(x_{t})-f^{*})+\frac{1}{2\eta}\left\|x_{t}-x^{*}\right\|^{2}\) is a non-increasing potential function, which directly implies the desired result. 

Proof of Theorem 4.3.: Since strongly convex functions are also convex, by the same argument as in the proof of Theorem 4.2, we have \(\left\|\nabla f(x_{t})\right\|\leq G:=\left\|\nabla f(x_{0})\right\|\) for all \(t\geq 0\). Moreover, (16) still holds. For \(\mu\)-strongly-convex function, we can obtain a tighter version of (17) as follows.

\[0\geq f(x_{t})-f^{*}+\langle\nabla f(x_{t}),x^{*}-x_{t}\rangle+\frac{\mu}{2} \left\|x^{*}-x_{t}\right\|^{2}.\] (18)Let \(A_{0}=0\) and \(A_{t+1}=(1+A_{t})/(1-\eta\mu)\) for all \(t\geq 0\). Combining (16) and (18), we have

\[0 \geq(A_{t+1}-A_{t})(f(x_{t})-f^{*}+\left\langle\nabla f(x_{t}),x^{* }-x_{t}\right\rangle)\] \[\quad+A_{t+1}\bigg{(}f(x_{t+1})-f(x_{t})-\left\langle\nabla f(x_{ t}),x_{t+1}-x_{t}\right\rangle-\frac{1}{2\eta}\left\|x_{t+1}-x_{t}\right\|^{2} \bigg{)}.\]

Then reorganizing the terms of the above inequality, noting that

\[\left\|x_{t+1}-x^{*}\right\|^{2}-\left\|x_{t}-x^{*}\right\|^{2} =\left\|x_{t+1}-x_{t}\right\|^{2}+2\langle x_{t+1}-x_{t},x_{t}-x^{ *}\rangle\] \[=\left\|x_{t+1}-x_{t}\right\|^{2}+2\eta\langle\nabla f(x_{t}),x^ {*}-x_{t}\rangle,\]

we can obtain

\[A_{t+1}(f(x_{t+1})-f^{*})+\frac{1+\eta\mu A_{t+1}}{2\eta}\left\|x_{t+1}-x^{*} \right\|^{2}\leq A_{t}(f(x_{t})-f^{*})+\frac{1+\eta\mu A_{t}}{2\eta}\left\|x_ {t}-x^{*}\right\|^{2}.\]

The above inequality means \(A_{t}(f(x_{t})-f^{*})+\frac{1+\eta\mu A_{t}}{2\eta}\left\|x_{t}-x^{*}\right\|^ {2}\) is a non-increasing potential function. Thus by telescoping we have

\[f(x_{T})-f^{*}\leq\frac{\mu(1-\eta\mu)^{T}}{2(1-(1-\eta\mu)^{T})}\left\|x_{0} -x^{*}\right\|^{2}.\]

## Appendix C Analysis of NAG for convex functions

In this section, we provide the detailed analysis of Nesterov's accelerated gradient method in the convex setting. As we discussed in Section 4.2, the stepsize size choice in Theorem 4.4 is smaller than the classical one. Therefore, we provide a more fine-grained version of the theorem, which allows the stepsize to depend on the degree of \(\ell\).

**Theorem C.1**.: _Suppose \(f\) is convex and \(\ell\)-smooth. For \(\alpha\in(0,2]\), if \(\ell(u)=o(u^{\alpha})\), i.e., \(\lim_{u\to\infty}\ell(u)/u^{\alpha}=0\), then there must exist a constant \(G\) such that for \(L:=\ell(2G)\), we have_

\[G\geq\max\left\{8\max\{L^{1/\alpha-1/2},1\}\sqrt{L((f(x_{0})-f^{*})+\left\|x_ {0}-x^{*}\right\|^{2})},\left\|\nabla f(x_{0})\right\|\right\}.\] (19)

_Choose \(\eta\leq\min\bigl{\{}\frac{1}{16L^{3-2/\alpha}},\frac{1}{2L}\bigr{\}}\). Then the iterates of Algorithm 1 satisfy_

\[f(x_{T})-f^{*}\leq\frac{4(f(x_{0}-f^{*})+4\left\|x_{0}-x^{*}\right\|^{2}}{\eta T ^{2}+4}.\]

Note that when \(\alpha=2\), i.e., \(\ell\) is sub-quadratic, Theorem C.1 reduces to Theorem 4.4 which chooses \(\eta\leq\min\bigl{\{}\frac{1}{16L^{2}},\frac{1}{2L}\bigr{\}}\). When \(\alpha=1\), i.e., \(\ell\) is sub-linear, the above theorem chooses \(\eta\leq\frac{1}{16L}\) as in the classical textbook analysis up to a numerical constant factor.

Throughout this section, we will assume \(f\) is convex and \(\ell\)-smooth, and consider the parameter choices in Theorem C.1, unless explicitly stated. Note that since \(f\) is \(\ell\)-smooth, it is also \((r,m)\)-smooth with \(m(u)=\ell(u+G)\) and \(r(u)=\frac{G}{\ell(u+G)}\) by Proposition 3.2. Note that \(m(G)=\ell(2G)=L\) and \(r(G)=G/L\). Then the stepsize satisfies \(\eta\leq 1/(2L)\leq\min\bigl{\{}\frac{2}{m(G)},\frac{r(G)}{2G}\bigr{\}}\).

Before proving Theorem C.1, we first present several additional useful lemmas. To start with, we provide two lemmas regarding the weights \(\{A_{t}\}_{t\geq 0}\) and \(\{B_{t}\}_{t\geq 0}\) used in Algorithm 1. The lemma below states that \(B_{t}=\Theta(t^{2})\).

**Lemma C.2**.: _The weights \(\{B_{t}\}_{t\geq 0}\) in Algorithm 1 satisfy \(\frac{1}{4}t^{2}\leq B_{t}\leq t^{2}\) for all \(t\geq 0\)._

Proof of Lemma c.2.: We prove this lemma by induction. First note that the inequality obviously holds for \(B_{0}=0\). Suppose its holds up to \(t\). Then we have

\[B_{t+1}=B_{t}+\frac{1}{2}(1+\sqrt{4B_{t}+1})\geq\frac{1}{4}t^{2}+\frac{1}{2}(1 +\sqrt{t^{2}+1})\geq\frac{1}{4}(t+1)^{2}.\]Similarly, we have

\[B_{t+1}=B_{t}+\frac{1}{2}(1+\sqrt{4B_{t}+1})\leq t^{2}+\frac{1}{2}(1+\sqrt{4t^{2} +1})\leq(t+1)^{2}.\]

Lemma C.2 implies the following useful lemma.

**Lemma C.3**.: _The weights \(\{A_{t}\}_{t\geq 0}\) in Algorithm 1 satisfy that_

\[(1-\frac{A_{t}}{A_{t+1}})\frac{1}{A_{t}}\sum_{s=0}^{t-1}\sqrt{A_{s+1}}(A_{s+1}- A_{s}-1)\leq 4.\]

Proof of Lemma c.3.: First, note that it is easy to verify that \(A_{s+1}-A_{s}-1=B_{s+1}-B_{s}-1\geq 0\), which implies each term in the LHS of the above inequality is non-negative. Then we have

\[(1-\frac{A_{t}}{A_{t+1}})\frac{1}{A_{t}}\sum_{s=0}^{t-1}\sqrt{A_{s +1}}(A_{s+1}-A_{s}-1)\] \[\leq\frac{1}{A_{t+1}\sqrt{A_{t}}}(A_{t+1}-A_{t})\sum_{s=0}^{t-1}( A_{s+1}-A_{s}-1)\] ( \[A_{t}\geq A_{s+1}\] ) \[=\frac{1}{A_{t+1}\sqrt{A_{t}}}(B_{t+1}-B_{t})\sum_{s=0}^{t-1}(B_{ s+1}-B_{s}-1)\] ( \[A_{s}=B_{s}+1/\eta\] ) \[=\frac{1}{A_{t+1}\sqrt{A_{t}}}\cdot\frac{1}{2}(1+\sqrt{4B_{t}+1} )\sum_{s=0}^{t-1}\biggl{(}-1+\frac{1}{2}(1+\sqrt{4B_{s}+1})\biggr{)}\] (by definition of

\[B_{s}\]

) \[\leq 8\frac{1}{(t+1)^{2}t}\cdot(t+1)\frac{t^{2}}{2}\] (by

\[A_{t}\geq B_{t}\]

 and Lemma C.2 ) \[\leq 4.\]

The following lemma summarizes the results in the classical potential function analysis of NAG in [11]. In order to not deal with the generalized smoothness condition for now, we directly assume the inequality (20) holds in the lemma, which will be proved later under the generalized smoothness condition.

**Lemma C.4**.: _For any \(t\geq 0\), if the following inequality holds,_

\[f(y_{t})+\langle\nabla f(y_{t}),x_{t+1}-y_{t}\rangle+\frac{1}{2\eta}\left\|x_ {t+1}-y_{t}\right\|^{2}\geq f(x_{t+1}),\] (20)

_then we can obtain_

\[A_{t+1}(f(x_{t+1})-f^{*})+\frac{1}{2\eta}\left\|z_{t+1}-x^{*}\right\|^{2} \leq A_{t}(f(x_{t})-f^{*})+\frac{1}{2\eta}\left\|z_{t}-x^{*}\right\|^{2}.\] (21)

Proof of Lemma c.4.: These derivations below can be found in [11]. We present them here for completeness.

First, since \(f\) is convex, the convexity between \(x^{*}\) and \(y_{t}\) gives

\[f^{*}\geq f(y_{t})+\langle\nabla f(y_{t}),x^{*}-y_{t}\rangle.\]

Similarly the convexity between \(x_{t}\) and \(y_{t}\) gives

\[f(x_{t})\geq f(y_{t})+\langle\nabla f(y_{t}),x_{t}-y_{t}\rangle.\]Combining the above two inequalities as well as (20) assumed in this lemma, we have

\[0 \geq(A_{t+1}-A_{t})(f(y_{t})-f^{*}+\langle\nabla f(y_{t}),x^{*}-y_{t }\rangle)\] \[\quad+A_{t}(f(y_{t})-f(x_{t})+\langle\nabla f(y_{t}),x_{t}-y_{t} \rangle)\] \[\quad+A_{t+1}\bigg{(}f(x_{t+1})-f(y_{t})-\langle\nabla f(y_{t}),x _{t+1}-y_{t}\rangle-\frac{1}{2\eta}\left\|x_{t+1}-y_{t}\right\|^{2}\bigg{)}.\] (22)

Furthermore, note that

\[\frac{1}{2\eta}\Big{(}\left\|z_{t+1}-x^{*}\right\|^{2}-\left\|z_ {t}-x^{*}\right\|^{2}\Big{)}\] \[=\frac{1}{2\eta}\Big{(}\left\|z_{t+1}-z_{t}\right\|^{2}+2(z_{t+1 }-z_{t},z_{t}-x^{*})\Big{)}\] \[=\frac{1}{2\eta}\Big{(}\eta^{2}(A_{t+1}-A_{t})^{2}\left\|\nabla f (y_{t})\right\|^{2}-2\eta(A_{t+1}-A_{t})\langle\nabla f(y_{t}),z_{t}-x^{*} \rangle\Big{)}\] \[=\frac{\eta}{2}(A_{t+1}-A_{t})^{2}\left\|\nabla f(y_{t})\right\| ^{2}-(A_{t+1}-A_{t})\langle\nabla f(y_{t}),z_{t}-x^{*}\rangle.\] (23)

Meanwhile, we have

\[A_{t+1}x_{t+1}=A_{t+1}y_{t}-\eta A_{t+1}\nabla f(y_{t})=A_{t+1}x_{t}+(A_{t+1}- A_{t})(z_{t}-x_{t})-\eta A_{t+1}\nabla f(y_{t}).\]

Thus we have

\[(A_{t+1}-A_{t})z_{t}=A_{t+1}x_{t+1}-A_{t}x_{t}+\eta A_{t+1}\nabla f (y_{t}).\]

Plugging back in (23), we obtain

\[\frac{1}{2\eta}\Big{(}\left\|z_{t+1}-x^{*}\right\|^{2}-\left\|z_ {t}-x^{*}\right\|^{2}\Big{)}\] \[=\frac{\eta}{2}(A_{t+1}-A_{t})^{2}\left\|\nabla f(y_{t})\right\| ^{2}+(A_{t+1}-A_{t})\langle\nabla f(y_{t}),x^{*}\rangle\] \[\quad+\langle-A_{t+1}x_{t+1}+A_{t}x_{t}-\eta A_{t+1}\nabla f(y_{t }),\nabla f(y_{t})\rangle.\]

Thus

\[(A_{t+1}-A_{t})\langle\nabla f(y_{t}),x^{*}\rangle+\langle A_{t} x_{t}-A_{t+1}x_{t+1},\nabla f(y_{t})\rangle\] \[=\frac{1}{2\eta}\Big{(}\left\|z_{t+1}-x^{*}\right\|^{2}-\left\|z_ {t}-x^{*}\right\|^{2}\Big{)}+\eta(A_{t+1}-\frac{1}{2}(A_{t+1}-A_{t})^{2}) \left\|\nabla f(y_{t})\right\|^{2}.\]

So we can reorganize (22) to obtain

\[0 \geq A_{t+1}(f(x_{t+1})-f^{*})-A_{t}(f(x_{t})-f^{*})\] \[\quad+(A_{t+1}-A_{t})\langle\nabla f(y_{t}),x^{*}\rangle+\langle A _{t}x_{t}-A_{t+1}x_{t+1},\nabla f(y_{t})\rangle\] \[\quad-\frac{1}{2\eta}A_{t+1}\left\|x_{t+1}-y_{t}\right\|^{2}\] \[=A_{t+1}(f(x_{t+1})-f^{*})-A_{t}(f(x_{t})-f^{*})\] \[\quad+\frac{1}{2\eta}\Big{(}\left\|z_{t+1}-x^{*}\right\|^{2}- \left\|z_{t}-x^{*}\right\|^{2}\Big{)}+\frac{\eta}{2}(A_{t+1}-(A_{t+1}-A_{t})^{ 2})\left\|\nabla f(y_{t})\right\|^{2}.\]

Then we complete the proof noting that it is easy to verify

\[A_{t+1}-(A_{t+1}-A_{t})^{2}=B_{t+1}+\frac{1}{\eta}-(B_{t+1}-B_{t})^{2}=\frac{1 }{\eta}\geq 0.\]

In the next lemma, we show that if \(\left\|\nabla f(y_{t})\right\|\leq G\), then the condition (20) assumed in Lemma C.4 is satisfied at time \(t\).

**Lemma C.5**.: _For any \(t\geq 0\), if \(\left\|\nabla f(y_{t})\right\|\leq G\), then we have \(\left\|\nabla f(x_{t+1})\right\|\leq G\), and furthermore,_

\[f(y_{t})+\langle\nabla f(y_{t}),x_{t+1}-y_{t}\rangle+\frac{1}{2\eta}\left\|x_ {t+1}-y_{t}\right\|^{2}\geq f(x_{t+1}).\]Proof of Lemma c.5.: As disccued below Theorem C.1, the stepsize satisfies \(\eta\leq 1/(2L)\leq\min\{\frac{2}{m(G)},\frac{r(G)}{2G}\}\). Therefore we can apply Lemma 4.1 to show \(\|\nabla f(x_{t+1})\|\leq\|\nabla f(y_{t})\|\leq G\). For the second part, note that \(\|x_{t+1}-y_{t}\|=\eta\left\|\nabla f(y_{t})\right\|\leq\frac{G}{2L}\leq r(G)\), we can apply Lemma 3.3 to show

\[f(x_{t+1}) \leq f(y_{t})+\langle\nabla f(y_{t}),x_{t+1}-y_{t}\rangle+\frac{L }{2}\left\|x_{t+1}-y_{t}\right\|^{2}\] \[\leq f(y_{t})+\langle\nabla f(y_{t}),x_{t+1}-y_{t}\rangle+\frac{1 }{2\eta}\left\|x_{t+1}-y_{t}\right\|^{2}.\]

With Lemma C.4 and Lemma C.5, we can show that \(\|\nabla f(y_{t})\|\leq G\) for all \(t\geq 0\), as in the lemma below.

**Lemma C.6**.: _For all \(t\geq 0\), \(\|\nabla f(y_{t})\|\leq G\)._

Proof of Lemma c.6.: We will prove this lemma by induction. First, by Lemma 3.5 and the choice of \(G\), it is easy to verify that \(\|\nabla f(x_{0})\|\leq G\). Then for any fixed \(t\geq 0\), suppose that \(\|\nabla f(x_{s})\|\leq G\) for all \(s<t\). Then by Lemma C.4 and Lemma C.5, we know that \(\|\nabla f(x_{s})\|\leq G\) for all \(0\leq s\leq t\), and that for all \(s<t\),

\[A_{s+1}(f(x_{s+1})-f^{*})+\frac{1}{2\eta}\left\|z_{s+1}-x^{*}\right\|^{2}\leq A _{s}(f(x_{s})-f^{*})+\frac{1}{2\eta}\left\|z_{s}-x^{*}\right\|^{2}.\] (24)

By telescoping (24), we have for all \(0\leq s<t\),

\[f(x_{s+1})-f^{*}\leq\frac{1}{\eta A_{s+1}}((f(x_{0})-f^{*})+\left\|z_{0}-x^{* }\right\|^{2}).\] (25)

For \(0\leq s\leq t\), since \(\|\nabla f(x_{s})\|\leq G\), then Lemma 3.5 implies

\[\|\nabla f(x_{s})\|^{2}\leq 2L(f(x_{s})-f^{*}).\] (26)

Note that by Algorithm 1, we have

\[z_{t}-x_{t}=\frac{A_{t-1}}{A_{t}}(z_{t-1}-x_{t-1})-\eta(A_{t}-A_{t-1})\nabla f (y_{t-1})+\eta\nabla f(y_{t-1}).\]

Thus we can obtain

\[z_{t}-x_{t}=-\frac{1}{A_{t}}\sum_{s=1}^{t-1}\eta A_{s+1}(A_{s+1}-A_{s}-1)\nabla f (y_{s}).\]

Therefore

\[y_{t}-x_{t}=-(1-\frac{A_{t}}{A_{t+1}})\frac{1}{A_{t}}\sum_{s=1}^{t-1}\eta A_{ s+1}(A_{s+1}-A_{s}-1)\nabla f(y_{s}).\]

Thus we have

\[\|y_{t}-x_{t}\|\leq(1-\frac{A_{t}}{A_{t+1}})\frac{1}{A_{t}}\sum_{s=1}^{t-1} \eta A_{s+1}(A_{s+1}-A_{s}-1)\left\|\nabla f(y_{s})\right\|=:\mathcal{I}.\]

Since \(\|\nabla f(y_{s})\|\leq G\) and \(\|x_{s+1}-y_{s}\|=\|\eta\nabla f(y_{s})\|\leq r(G)\) for \(s<t\), by Lemma 3.3, we have

\[\mathcal{I} \leq(1-\frac{A_{t}}{A_{t+1}})\frac{1}{A_{t}}\sum_{s=1}^{t-1}\eta A _{s+1}(A_{s+1}-A_{s}-1)(\|\nabla f(x_{s+1})\|+\eta L\left\|\nabla f(y_{s})\right\|)\] \[\leq\eta L\mathcal{I}+(1-\frac{A_{t}}{A_{t+1}})\frac{1}{A_{t}} \sum_{s=1}^{t-1}\eta A_{s+1}(A_{s+1}-A_{s}-1)\left\|\nabla f(x_{s+1})\right\|.\]Thus

\[\left\|y_{t}-x_{t}\right\|\] \[\leq \mathcal{I}\leq\frac{1}{1-\eta L}(1-\frac{A_{t}}{A_{t+1}})\frac{1}{ A_{t}}\sum_{s=1}^{t-1}\eta A_{s+1}(A_{s+1}-A_{s}-1)\left\|\nabla f(x_{s+1})\right\|\] \[\leq \frac{1}{1-\eta L}(1-\frac{A_{t}}{A_{t+1}})\frac{1}{A_{t}}\sum_{s= 1}^{t-1}\eta A_{s+1}(A_{s+1}-A_{s}-1)\sqrt{2L(f(x_{s+1})-f^{*})}\] (by ( 26 )) \[\leq \frac{1}{1-\eta L}(1-\frac{A_{t}}{A_{t+1}})\frac{1}{A_{t}}\sum_{s= 1}^{t-1}\eta A_{s+1}(A_{s+1}-A_{s}-1)\sqrt{\frac{2L}{A_{s+1}}\cdot\frac{1}{ \eta}((f(x_{0})-f^{*})+\left\|z_{0}-x^{*}\right\|^{2})}\] (by ( 25 )) \[= \frac{2\sqrt{\eta L}}{1-\eta L}(1-\frac{A_{t}}{A_{t+1}})\frac{1}{ A_{t}}\sum_{s=1}^{t-1}\sqrt{A_{s+1}}(A_{s+1}-A_{s}-1)\sqrt{(f(x_{0})-f^{*})+ \left\|z_{0}-x^{*}\right\|^{2}}\] \[\leq \frac{8\sqrt{\eta}}{1-\eta L}\sqrt{L((f(x_{0})-f^{*})+\left\|z_ {0}-x^{*}\right\|^{2})}\] (by Lemma C.3 ) \[\leq \frac{1}{2L^{3/2-1/\alpha}}\cdot L^{1/2-1/\alpha}G=\frac{G}{2L} \leq r(G).\] (by the choices of \[\eta\] and \[G\] )

Since \(\left\|\nabla f(x_{t})\right\|\leq G\) and we just showed \(\left\|x_{t}-y_{t}\right\|\leq r(G)\), by Lemma 3.3, we have

\[\left\|\nabla f(y_{t})\right\| \leq\left\|\nabla f(x_{t})\right\|+L\left\|y_{t}-x_{t}\right\|\] \[\leq\sqrt{\frac{2L}{\eta A_{t}}((f(x_{0})-f^{*})+\left\|z_{0}-x^ {*}\right\|^{2})}+L\cdot\frac{G}{2L}\] (by ( 26 ) and ( 25 )) \[\leq G\bigg{(}\frac{1}{4}+\frac{1}{2}\bigg{)}\leq G.\] (by \[A_{t}\geq 1/\eta\] and choice of \[G\] )

Then we complete the induction as well as the proof.

With the three lemmas above, it is straight forward to prove Theorem C.1.

Proof of Theorem c.1.: Combining Lemmas C.4, C.5, and C.6, we know the following inequality holds for all \(t\geq 0\).

\[A_{t+1}(f(x_{t+1})-f^{*})+\frac{1}{2\eta}\left\|z_{t+1}-x^{*} \right\|^{2}\leq A_{t}(f(x_{t})-f^{*})+\frac{1}{2\eta}\left\|z_{t}-x^{*}\right\| ^{2},\]

Then by telescoping, we directly complete the proof.

## Appendix D Analysis of NAG for strongly convex functions

In this section, we provide the convergence analysis of the modified version of Nesterov's accelerated gradient method for \(\mu\)-strongly-convex functions defined in Algorithm 2.

The convergence results is formally presented in the following theorem.

**Theorem D.1**.: _Suppose \(f\) is \(\mu\)-strongly-convex and \(\ell\)-smooth. For \(\alpha\in(0,2]\), if \(\ell(u)=o(u^{\alpha})\), i.e., \(\lim_{u\to\infty}\ell(u)/u^{\alpha}=0\), then there must exist a constant \(G\) such that for \(L:=\ell(2G)\), we have_

\[G\geq 8\max\{L^{1/\alpha-1/2},1\}\sqrt{L((f(x_{0})-f^{*})+\mu \left\|z_{0}-x^{*}\right\|^{2})/\min\{\mu,1\}}.\] (27)

_If we choose_

\[\eta\leq\min\left\{\frac{1}{144L^{3-2/\alpha}\log^{4}\Bigl{(}e+ \frac{144L^{3-2/\alpha}}{\mu}\Bigr{)}},\frac{1}{2L}\right\}.\] (28)The iterates generated by Algorithm 2 satisfy

\[f(x_{T})-f^{*}\leq\frac{(1-\sqrt{\eta\mu})^{T-1}(f(x_{0}-f^{*})+\mu\left\|z_{0}- x^{*}\right\|^{2})}{\eta\mu+(1-\sqrt{\eta\mu})^{T-1}}.\]

The above theorem gives a gradient complexity of \(\mathcal{O}\left(\frac{1}{\sqrt{\eta\mu}}\log(1/\epsilon)\right)\). Note that Theorem 4.2 shows the complexity of GD is \(\mathcal{O}\left(\frac{1}{\eta\mu}\log(1/\epsilon)\right)\). It seems NAG gives a better rate at first glance. However, note that the choices of \(G,L,\eta\) in these two theorems are different, it is less clear whether NAG accelerates the optimization in this setting. Below, we informally show that, if \(\ell(u)=o(\sqrt{u})\), the rate we obtain for NAG is faster than that for GD.

For simplicity, we informally assume \(\ell(u)\asymp x^{\rho}\) with \(\rho\in(0,1)\). Let \(G_{0}=\|\nabla f(x_{0})\|\). Then for GD, by Theorem 4.2, we have \(\eta_{\rm sgd}\mu\asymp\mu/\ell(G_{0})\asymp\mu/G_{0}^{\rho}\). For NAG, since \(\ell\) is sub-linear we can choose \(\alpha=1\) in the theorem statement. Since \(f\) is \(\mu\)-strongly-convex, by standard results, we can show that \(f(x_{0})-f^{*}\leq\frac{1}{\mu}G_{0}^{2}\) and \(\left\|z_{0}-x^{*}\right\|\leq\frac{1}{\mu}G_{0}\). Thus the requirement of \(G\) in (27) can be simplified as \(G\gtrsim\ell(G)\cdot G_{0}/\mu\), which is satisfied if choosing \(G\asymp(G_{0}/\mu)^{1/(1-\rho)}\). Then we also have \(\eta_{\rm nag}\asymp\frac{1}{\ell(G)}\asymp(\mu/G_{0})^{\rho/(1-\rho)}\). Thus \(\sqrt{\eta_{\rm nag}\mu}\asymp(\mu/G_{0}^{\rho})^{1/(2-2\rho)}\). This means whenever \(1/(2-2\rho)<1\), i.e., \(0\leq\rho<1/2\), we have \(\sqrt{\eta_{\rm nag}\mu}\gtrsim\eta_{\rm sgd}\mu\), which implies the rate we obtain for NAG is faster than for GD.

In what follows, we will provide the proof of Theorem D.1. We will always use the parameter choices in the theorem throughout this section.

### Useful lemmas

In this part, we provide several useful lemmas for proving Theorem D.1. To start with, the following two lemmas provide two useful inequalities.

**Lemma D.2**.: _For any \(0\leq u\leq 1\), we have \(\log(1+u)\geq\frac{1}{2}u\)._

**Lemma D.3**.: _For all \(0<p\leq 1\) and \(t\geq 0\), we have_

\[t\leq\frac{2}{\sqrt{p}}\log(e+\frac{1}{p})(p(1+\sqrt{p})^{t}+1).\]

Proof of Lemma D.3.: Let

\[f(t)=\frac{2}{\sqrt{p}}\log(e+\frac{1}{p})(p(1+\sqrt{p})^{t}+1)-t.\]It is obvious that \(f(t)\geq 0\) for \(t\leq\frac{2}{\sqrt{p}}\log(e+\frac{1}{p})\). For \(t>\frac{2}{\sqrt{p}}\log(e+\frac{1}{p})\), we have

\[f^{\prime}(t) =2\sqrt{p}\log(e+\frac{1}{p})\log(1+\sqrt{p})(1+\sqrt{p})^{t}-1\] \[\geq p(1+\sqrt{p})^{t}-1\] (by Lemma D.2) \[=p\exp(t\log(1+\sqrt{p}))-1\] \[\geq p\exp(t\sqrt{p}/2)-1\] (by Lemma D.2) \[\geq p(e+1/p)-1\geq 0. \text{(since }t>\frac{2}{\sqrt{p}}\log(e+\frac{1}{p})\text{)}\]

Thus \(f\) is non-decreasing and

\[f(t)\geq f\bigg{(}\frac{2}{\sqrt{p}}\log(e+\frac{1}{p})\bigg{)}\geq 0.\]

In the next four lemmas, we provide several useful inequalities regarding the weights \(\{A_{t}\}_{t\geq 0}\) and \(\{B_{t}\}_{t\geq 0}\) used in Algorithm 2.

**Lemma D.4**.: _For all \(s\leq t\), we have_

\[\frac{B_{t+1}-B_{t}}{B_{t+1}}\cdot\frac{B_{s+1}-B_{s}}{1+\eta\mu B_{s+1}}\leq 1,\]

_which implies \(\tau_{t}\cdot\delta_{s}\leq 1\)._

Proof of Lemma D.4.: By Algorithm 2, it is easy to verify

\[(B_{s+1}-B_{s})^{2}=B_{s+1}(1+\eta\mu B_{s+1}).\]

This implies

\[B_{s}=B_{s+1}-\sqrt{B_{s+1}(1+\eta\mu B_{s+1})}.\]

Thus

\[\frac{B_{t}}{B_{t+1}}=1-\sqrt{\eta\mu+\frac{1}{B_{t+1}}}\geq 1-\sqrt{\eta\mu+ \frac{1}{B_{s+1}}}=\frac{B_{s}}{B_{s+1}},\]

where in the inequality, we use the fact that \(B_{s}\) is non-decreasing with \(s\). Therefore

\[\frac{B_{t+1}-B_{t}}{B_{t+1}}\cdot\frac{B_{s+1}-B_{s}}{1+\eta\mu B_{s+1}} \leq\frac{B_{s+1}-B_{s}}{B_{s+1}}\cdot\frac{B_{s+1}-B_{s}}{1+\eta\mu B_{s+1}}=1.\]

Thus we have

\[\tau_{t}\cdot\delta_{s} =\frac{(A_{t+1}-A_{t})(1+\eta\mu A_{t})}{A_{t+1}+2\eta\mu A_{t}A _{t+1}-\eta\mu A_{t}^{2}}\cdot\frac{A_{s+1}-A_{s}}{1+\eta\mu A_{s+1}}\] \[\leq\frac{A_{t+1}-A_{t}}{A_{t+1}}\cdot\frac{A_{s+1}-A_{s}}{1+\eta \mu A_{s+1}}\] (by \[A_{t+1}\geq A_{t}\] ) \[=\frac{B_{t+1}-B_{t}}{A_{t+1}}\cdot\frac{B_{s+1}-B_{s}}{1+\eta \mu A_{s+1}}\] (by \[A_{s+1}-A_{s}=B_{s+1}-B_{s}\] ) \[\leq\frac{B_{t+1}-B_{t}}{B_{t+1}}\cdot\frac{B_{s+1}-B_{s}}{1+\eta \mu B_{s+1}}\leq 1.\] (by \[A_{s+1}\geq B_{s+1}\] )

**Lemma D.5**.: _If \(0<\eta\mu<1\), then for any \(t\geq 1\), we have_

\[\frac{B_{t}}{1-\sqrt{\eta\mu}}\leq B_{t+1}\leq\frac{3B_{t}}{1-\eta\mu}.\]

_Thus_

\[B_{t}\geq\frac{1}{(1-\sqrt{\eta\mu})^{t-1}}\geq(1+\sqrt{\eta\mu})^{t-1}.\]Proof of Lemma D.5.: For \(t\geq 1\), we have \(B_{t}\geq 1\) thus

\[B_{t+1}=\frac{2B_{t}+1+\sqrt{4B_{t}+4\eta\mu B_{t}^{2}+1}}{2(1-\eta\mu)}\leq\frac {2B_{t}+1}{1-\eta\mu}\leq\frac{3B_{t}}{1-\mu\eta}.\]

On the other hand, we have

\[B_{t+1} =\frac{2B_{t}+1+\sqrt{4B_{t}+4\eta\mu B_{t}^{2}+1}}{2(1-\eta\mu)}\] \[\geq\frac{2B_{t}+\sqrt{(2B_{t}\sqrt{\eta\mu})^{2}}}{2(1-\eta\mu)}\] \[=\frac{B_{t}}{1-\sqrt{\eta\mu}}.\]

Thus

\[B_{t}\geq\left(\frac{1}{1-\sqrt{\eta\mu}}\right)^{t-1}B_{1}\geq\left(\frac{1} {1-\sqrt{\eta\mu}}\right)^{t-1}\geq(1+\sqrt{\eta\mu})^{t-1}.\]

**Lemma D.6**.: _For \(0<\eta\mu<1\) and \(t\geq 1\), we have_

\[\sum_{s=0}^{t}\sqrt{B_{s}}\leq(1-\eta\mu)B_{t+1}\leq 3B_{t}.\]

Proof of Lemma D.6.: \[B_{t+1} =\frac{2B_{t}+1+\sqrt{4B_{t}+4\eta\mu B_{t}^{2}+1}}{2(1-\eta\mu)}\] \[\geq B_{t}+\frac{\sqrt{B_{t}}}{1-\eta\mu}\] \[\geq\ldots\] \[\geq\sum_{s=0}^{t}\frac{\sqrt{B_{s}}}{1-\eta\mu}.\]

Combined with Lemma D.5, we have the desired result. 

**Lemma D.7**.: _For \(t\geq 1\), we have_

\[\sum_{s=0}^{t-1}\frac{\sqrt{A_{s+1}}}{A_{t}}\leq 3+4\log(e+\frac{1}{\eta\mu}).\]

Proof of Lemma D.7.: By Lemma D.5, we have

\[A_{t}=B_{t}+\frac{1}{\eta\mu}\geq(1+\sqrt{\eta\mu})^{t-1}+\frac{1}{\eta\mu}.\] (29)

Thus, we have

\[\sum_{s=0}^{t-1}\frac{\sqrt{A_{s+1}}}{A_{t}} =\sum_{s=0}^{t-1}\frac{\sqrt{B_{s+1}+1/(\eta\mu)}}{A_{t}}\] \[\leq\sum_{s=0}^{t-1}\frac{\sqrt{B_{s+1}}}{A_{t}}+\frac{t}{\sqrt{ \eta\mu}A_{t}}\] \[\leq 3+\frac{1}{\sqrt{\eta\mu}A_{t}}\cdot\frac{2}{\sqrt{\eta\mu}} \log(e+\frac{1}{\eta\mu})(\eta\mu(1+\sqrt{\eta\mu})^{t}+1)\] (by Lemma D.6 and Lemma D.3) \[\leq 3+4\log(e+\frac{1}{\eta\mu}).\] (by Inequality ( 29 ))

### Proof of Theorem D.1

With all the useful lemmas in the previous section, we proceed to prove Theorem D.1, for which we need several additional lemmas. First, similar to Lemma C.4, the following lemma summarizes the results in the classical potential function analysis of NAG for strongly convex functions in [11].

**Lemma D.8**.: _For any \(t\geq 0\), if the following inequality holds_

\[f(y_{t})+\langle\nabla f(y_{t}),x_{t+1}-y_{t}\rangle+\frac{1}{2\eta}\left\|x_{t +1}-y_{t}\right\|^{2}\geq f(x_{t+1}),\]

_then we can obtain_

\[A_{t+1}(f(x_{t+1})-f^{*})+\frac{1+\eta\mu A_{t+1}}{2\eta}\left\|z_{t+1}-x^{*} \right\|^{2}\leq A_{t}(f(x_{t})-f^{*})+\frac{1+\eta\mu A_{t}}{2\eta}\left\|z_ {t}-x^{*}\right\|^{2}.\]

Proof of Lemma D.8.: These derivations can be found in [11]. We present it here for completeness.

The strong convexity between \(x^{*}\) and \(y_{t}\) gives

\[f^{*}\geq f(y_{t})+\langle\nabla f(y_{t}),x^{*}-y_{t}\rangle+\frac{\mu}{2} \left\|x^{*}-y_{t}\right\|^{2}.\]

The convexity between \(x_{t}\) and \(y_{t}\) gives

\[f(x_{t})\geq f(y_{t})+\langle\nabla f(y_{t}),x_{t}-y_{t}\rangle.\]

Combining the above two inequalities and the one assumed in this lemma, we have

\[0 \geq(A_{t+1}-A_{t})(f^{*}-f(y_{t})-\langle\nabla f(y_{t}),x^{*}-y _{t}\rangle-\frac{\mu}{2}\left\|x^{*}-y_{t}\right\|^{2})\] \[\quad+A_{t}(f(y_{t})-f(x_{t})-\langle\nabla f(y_{t}),x_{t}-y_{t} \rangle)\] \[\quad+A_{t+1}(f(x_{t+1})-f(y_{t})-\langle\nabla f(y_{t}),x_{t+1} -y_{t}\rangle-\frac{1}{2\eta}\left\|x_{t+1}-y_{t}\right\|^{2}).\]

Reorganizing we can obtain

\[A_{t+1}(f(x_{t+1})-f^{*})+\frac{1+\eta\mu A_{t+1}}{2\eta}\left\| z_{t+1}-x^{*}\right\|^{2}\] \[\leq A_{t}(f(x_{t})-f^{*})+\frac{1+\eta\mu A_{t}}{2\eta}\left\| z_{t}-x^{*}\right\|^{2}\] \[\quad+\frac{(A_{t}-A_{t+1})^{2}-A_{t+1}-\eta\mu A_{t+1}^{2}}{1+ \eta\mu A_{t+1}}\frac{\eta}{2}\left\|\nabla f(y_{t})\right\|^{2}\] \[\quad-A_{t}^{2}\frac{(A_{t+1}-A_{t})(1+\eta\mu A_{t})(1+\eta\mu A _{t+1})}{(A_{t+1}+2\eta\mu A_{t}A_{t+1}-\eta\mu A_{t}^{2})^{2}}\frac{\mu}{2} \left\|x_{t}-z_{t}\right\|^{2}.\]

Then we complete the proof noting that

\[(A_{t}-A_{t+1})^{2}-A_{t+1}-\eta\mu A_{t+1}^{2}\] \[=(B_{t}-B_{t+1})^{2}-B_{t+1}+\frac{1}{\eta\mu}-\eta\mu(B_{t+1}+1/ (\eta\mu))^{2}\] \[=\eta\mu B_{t+1}^{2}+\frac{1}{\eta\mu}-\eta\mu B_{t+1}^{2}-2B_{t+ 1}-\frac{1}{\eta\mu}\] \[=-2B_{t+1}\leq 0.\]

Next, note that Lemma C.5 still holds in the strongly convex setting. We repeat it below for completeness.

**Lemma D.9**.: _For any \(t\geq 0\), if \(\left\|\nabla f(y_{t})\right\|\leq G\), then we have \(\left\|\nabla f(x_{t+1})\right\|\leq G\), and furthermore,_

\[f(y_{t})+\langle\nabla f(y_{t}),x_{t+1}-y_{t}\rangle+\frac{1}{2\eta}\left\|x_ {t+1}-y_{t}\right\|^{2}\geq f(x_{t+1}).\]With Lemma D.8 and Lemma D.9, we will show that \(\|\nabla f(y_{t})\|\leq G\) for all \(t\geq 0\) by induction in the following lemma.

**Lemma D.10**.: _For all \(t\geq 0\), we have \(\|\nabla f(y_{t})\|\leq G\)._

Proof of Lemma D.10.: We will prove this lemma by induction. First, by Lemma 3.5 and the choice of \(G\), it is easy to verify that \(\|\nabla f(x_{0})\|\leq G\). Then for any fixed \(t\geq 0\), suppose that \(\|\nabla f(x_{s})\|\leq G\) for all \(s<t\). Then by Lemma D.8 and Lemma D.9, we know that \(\|\nabla f(x_{s})\|\leq G\) for all \(0\leq s\leq t\), and that for all \(s<t\),

\[A_{s+1}(f(x_{s+1})-f^{*})+\frac{1+\eta\mu A_{s+1}}{2\eta}\left\|z_{s+1}-x^{*} \right\|^{2}\leq A_{s}(f(x_{s})-f^{*})+\frac{1+\eta\mu A_{s}}{2\eta}\left\|z_{ s}-x^{*}\right\|^{2}.\] (30)

By telescoping (30), we have for all \(0\leq s<t\),

\[f(x_{s+1})-f^{*}\leq\frac{1}{A_{s+1}\eta\mu}(f(x_{0})-f^{*}+\mu \left\|z_{0}-x^{*}\right\|^{2}).\] (31)

For \(0\leq s\leq t\), since \(\|\nabla f(x_{s})\|\leq G\), then Lemma 3.5 implies

\[\left\|\nabla f(x_{s})\right\|^{2}\leq 2L(f(x_{s})-f^{*}).\] (32)

Note that by Algorithm 2, we have

\[z_{t}-x_{t}=(1-\eta\mu\delta_{t-1})(1-\tau_{t-1})(z_{t-1}-x_{t-1})+\eta(1- \delta_{t-1})\nabla f(y_{t-1}).\]

Thus

\[z_{t}-x_{t}=\eta\sum_{s=0}^{t-1}(1-\delta_{s})\nabla f(y_{s})\prod_{i=s+1}^{t -1}(1-\eta\mu\delta_{i})(1-\tau_{i}).\]

Therefore

\[y_{t}-x_{t}=\eta\tau_{t}\sum_{s=0}^{t-1}(1-\delta_{s})\nabla f(y_ {s})\prod_{i=s+1}^{t-1}(1-\eta\mu\delta_{i})(1-\tau_{i}).\]

Moreover

\[1-\eta\mu\delta_{i}=1-\frac{\eta\mu(A_{i+1}-A_{i})}{1+\eta\mu A_{i+1}}=\frac{ 1+\eta\mu A_{i}}{1+\eta\mu A_{i+1}}\]

and

\[1-\tau_{i}=1-\frac{(A_{i+1}-A_{i})(1+\eta\mu A_{i})}{A_{i+1}+2 \eta\mu A_{i}A_{i+1}-\eta\mu A_{i}^{2}}=\frac{A_{i}(1+\eta\mu A_{i+1})}{A_{i+ 1}+2\eta\mu A_{i}A_{i+1}-\eta\mu A_{i}^{2}}\leq\frac{A_{i}(1+\eta\mu A_{i+1}) }{A_{i+1}(1+\eta\mu A_{i})}.\]

Thus we have

\[\left\|y_{t}-x_{t}\right\|\leq\eta\tau_{t}\sum_{s=0}^{t-1}(\delta_{s}-1)\frac {A_{s+1}}{A_{t}}\left\|\nabla f(y_{s})\right\|\leq\eta\sum_{s=0}^{t-1}\frac{A _{s+1}}{A_{t}}\left\|\nabla f(y_{s})\right\|=:\mathcal{I},\]

where the second inequality follows from Lemma D.4. We further control term \(\mathcal{I}\) by

\[\mathcal{I} \leq\eta\sum_{s=0}^{t-1}\frac{A_{s+1}}{A_{t}}(\left\|\nabla f(x_ {s+1})\right\|+\eta L\left\|\nabla f(y_{s})\right\|)\] \[\leq\eta L\mathcal{I}+\eta\sum_{s=0}^{t-1}\frac{A_{s+1}}{A_{t}} \left\|\nabla f(x_{s+1})\right\|.\]Thus we have

\[\left\|y_{t}-x_{t}\right\| \leq\frac{\eta}{1-\eta L}\sum_{s=0}^{t-1}\frac{A_{s+1}}{A_{t}}\left\| \nabla f(x_{s+1})\right\|\] \[\leq\frac{\eta}{1-\eta L}\sum_{s=0}^{t-1}\frac{A_{s+1}}{A_{t}} \sqrt{2L(f(x_{s+1})-f^{*})}\] (by ( 32 )) \[\leq\frac{\eta}{1-\eta L}\sum_{s=0}^{t-1}\frac{A_{s+1}}{A_{t}} \sqrt{2L\cdot\frac{1}{A_{s+1}\eta\mu}(f(x_{0})-f^{*}+\mu\left\|z_{0}-x^{*} \right\|^{2})}\] (by ( 31 )) \[=\frac{\sqrt{2\eta L(f(x_{0})-f^{*}+\mu\left\|z_{0}-x^{*}\right\| ^{2})}}{(1-\eta L)\sqrt{\mu}}\sum_{s=0}^{t-1}\frac{\sqrt{A_{s+1}}}{A_{t}}\] \[\leq\frac{\sqrt{2\eta L(f(x_{0})-f^{*}+\mu\left\|z_{0}-x^{*} \right\|^{2})}}{(1-\eta L)\sqrt{\mu}}\bigg{(}3+4\log(e+\frac{1}{\eta\mu}) \bigg{)}.\] (by Lemma D.7 ) \[\leq\frac{\sqrt{\eta}}{1-\eta L}\bigg{(}3+4\log(e+\frac{1}{\eta \mu})\bigg{)}\cdot\frac{G\cdot L^{1/2-1/\alpha}}{4}\] (by ( 27 )) \[\leq\frac{3+4\log(e+\frac{1}{\eta\mu})}{\log^{2}\!\left(e+\frac{1 44L^{3-2/\alpha}}{\mu}\right)}\cdot\frac{G}{24L}\] (by ( 28 )) \[\leq G\frac{G}{2L}\leq r(G).\]

Since \(\left\|\nabla f(x_{t})\right\|\leq G\) and we just showed \(\left\|x_{t}-y_{t}\right\|\leq r(G)\), by Lemma 3.3, we have

\[\left\|\nabla f(y_{t})\right\| \leq\left\|\nabla f(x_{t})\right\|+L\left\|y_{t}-x_{t}\right\|\] \[\leq\sqrt{\frac{2L}{\eta\mu A_{t}}((f(x_{0})-f^{*})+\mu\left\|z_ {0}-x^{*}\right\|^{2})}+L\cdot\frac{G}{2L}\] (by ( 31 )) \[\leq G\bigg{(}\frac{1}{4}+\frac{1}{2}\bigg{)}\leq G.\] (by \[A_{t}\geq 1/(\eta\mu)\] and ( 27 ))

Then we complete the induction as well as the proof. 

Proof of Theorem D.1.: Combining Lemmas D.8, D.9, and D.10, we know the following inequality holds for all \(t\geq 0\).

\[A_{t+1}(f(x_{t+1})-f^{*})\!+\!\frac{1+\eta\mu A_{t+1}}{2\eta}\left\|z_{t+1}-x^ {*}\right\|^{2}\leq A_{t}(f(x_{t})-f^{*})\!+\!\frac{1+\eta\mu A_{t}}{2\eta} \left\|z_{t}-x^{*}\right\|^{2}.\]

Then by telescoping, we get

\[A_{t}(f(x_{t})-f^{*})\!+\!\frac{1+\eta\mu A_{t}}{2\eta}\left\|z_{t}-x^{*} \right\|^{2}\leq A_{0}(f(x_{0})-f^{*})\!+\!\frac{1+\eta\mu A_{0}}{2\eta}\left\| z_{0}-x^{*}\right\|^{2}.\]

Finally, applying Lemma D.5, we have \(A_{t}=B_{t}+1/(\eta\mu)\geq 1/(1-\sqrt{\eta\mu})^{t-1}+1/(\eta\mu)\). Thus completes the proof. 

## Appendix E Analysis of GD for non-convex functions

In this section, we provide the proofs related to analysis of gradient descent for non-convex function, including those of Lemma 5.1 and Theorem 5.2.

Proof of Lemma 5.1.: First, based on Corollary 3.6, we know \(\left\|\nabla f(x)\right\|\leq G<\infty\). Also note that

\[\left\|x^{+}-x\right\|=\left\|\eta\nabla f(x)\right\|\leq\eta G\leq G/L.\]Then by Lemma 3.3 and Remark 3.4, we have \(x^{+}\in\mathcal{X}\) and

\[f(x^{+})\leq f(x)+\left\langle\nabla f(x_{t}),x^{+}-x\right\rangle+\frac{L}{2} \left\|x^{+}-x\right\|^{2}\] \[= f(x)-\eta(1-\eta L/2)\left\|\nabla f(x)\right\|^{2}\] \[\leq f(x).\]

Proof of Theorem 5.2.: By Lemma 5.1, using induction, we directly obtain \(f(x_{t})\leq f(x_{0})\) for all \(t\geq 0\). Then by Corollary 3.6, we have \(\left\|\nabla f(x_{t})\right\|\leq G\) for all \(t\geq 0\). Following the proof of Lemma 5.1, we can similarly show

\[f(x_{t+1})-f(x_{t})\leq\eta(1-\eta L/2)-\frac{\eta}{2}\left\|\nabla f(x_{t}) \right\|^{2}\leq-\frac{\eta}{2}\left\|\nabla f(x_{t})\right\|^{2}.\]

Taking a summation over \(t<T\) and rearranging terms, we have

\[\frac{1}{T}\sum_{t<T}\left\|\nabla f(x_{t})\right\|^{2}\leq\frac{2(f(x_{0})-f( x_{T}))}{\eta T}\leq\frac{2(f(x_{0})-f^{*})}{\eta T}.\]

## Appendix F Analysis of SGD for non-convex functions

In this section, we provide the detailed convergence analysis of stochastic gradient descent for \(\ell\)-smooth and non-convex functions where \(\ell\) is sub-quadratic.

We first present some useful inequalities related to the parameter choices in Theorem 5.3.

**Lemma F.1**.: _Under the parameters choices in Theorem 5.3, the following inequalities hold._

\[\eta G\sqrt{2T}\leq 1/2,\quad\eta^{2}\sigma LT\leq 1/2,\quad 100\eta^{2}T \sigma^{2}L^{2}\leq\delta G^{2}.\]

Proof of Lemma F.1.: First note that by Corollary 3.6, we know

\[G^{2}=2LF=16L(f(x_{0})-f^{*}+\sigma)/\delta\geq 16L\sigma/\delta,\]

i.e., \(\sigma L\leq G^{2}\delta/16\). Then since we choose \(\eta\leq\frac{1}{4G\sqrt{T}}\), we have

\[\eta G\sqrt{2T}\leq \sqrt{2}/4\leq 1/2,\] \[\eta^{2}\sigma LT\leq \eta^{2}TG^{2}\delta/16\leq\delta/256\leq 1/2,\] \[100\eta^{2}T\sigma^{2}L^{2}\leq 100\eta^{2}TG^{4}\delta^{2}/256\leq\delta G^{2}.\]

Next, we show the useful lemma which bounds \(\mathbb{E}[f(x_{\tau})-f^{*}]\) and \(\mathbb{E}\left[\sum_{t<\tau}\left\|\nabla f(x_{t})\right\|^{2}\right]\) simultaneously.

**Lemma F.2**.: _Under the parameters choices in Theorem 5.3, the following inequality holds_

\[\mathbb{E}\left[f(x_{\tau})-f^{*}+\frac{\eta}{2}\sum_{t<\tau}\left\|\nabla f( x_{t})\right\|^{2}\right]\leq f(x_{0})-f^{*}+\sigma.\]

Proof of Lemma F.2.: If \(t<\tau\), by the definition of \(\tau\), we know \(f(x_{t})-f^{*}\leq F\) and \(\left\|\epsilon_{t}\right\|\leq\frac{G}{5\eta L}\), and the former also implies \(\left\|\nabla f(x_{t})\right\|\leq G\) by Corollary 3.6. Then we can bound

\[\left\|x_{t+1}-x_{t}\right\|=\eta\left\|g_{t}\right\|\leq\eta(\left\left\| \nabla f(x_{t})\right\|+\left\|\epsilon_{t}\right\|)\leq\eta G+\frac{G}{5L} \leq\frac{G}{L},\]where we use the choice of \(\eta\leq\frac{1}{2L}\). Then based on Lemma 3.3 and Remark 3.4, for any \(t<\tau\), we have

\[f(x_{t+1})-f(x_{t})\leq \big{\langle}\nabla f(x_{t}),x_{t+1}-x_{t}\big{\rangle}+\frac{L}{2} \left\|x_{t+1}-x_{t}\right\|^{2}\] \[= -\eta\big{\langle}\nabla f(x_{t}),g_{t}\big{\rangle}+\frac{\eta^ {2}L}{2}\left\|g_{t}\right\|^{2}\] \[\leq -\eta\left\|\nabla f(x_{t})\right\|^{2}-\eta\big{\langle}\nabla f (x_{t}),\epsilon_{t}\big{\rangle}+\eta^{2}L\left\|\nabla f(x_{t})\right\|^{2}+ \eta^{2}L\left\|\epsilon_{t}\right\|^{2}\] \[\leq -\frac{\eta}{2}\left\|\nabla f(x_{t})\right\|^{2}-\eta\big{\langle} \nabla f(x_{t}),\epsilon_{t}\big{\rangle}+\eta^{2}L\left\|\epsilon_{t}\right\| ^{2},\] (33)

where the equality is due to (4); the second inequality uses \(g_{t}=\epsilon_{t}+\nabla f(x_{t})\) and Young's inequality \(\left\|y+z\right\|^{2}\leq 2\left\|y\right\|^{2}+2\left\|z\right\|^{2}\) for any vectors \(y,z\); and the last inequality chooses \(\eta\leq 1/(2L)\). Taking a summation over \(t<\tau\) and rearranging terms, we have

\[f(x_{\tau})-f^{*}+\frac{\eta}{2}\sum_{t<\tau}\left\|\nabla f(x_ {t})\right\|^{2}\leq f(x_{0})-f^{*}-\eta\sum_{t<\tau}\bigl{\langle}\nabla f(x_ {t}),\epsilon_{t}\big{\rangle}+\eta^{2}L\sum_{t<\tau}\left\|\epsilon_{t}\right\| ^{2}.\]

Now we bound the last two terms on the RHS. First, for the last term, we have

\[\mathbb{E}\left[\sum_{t<\tau}\left\|\epsilon_{t}\right\|^{2} \right]\leq\mathbb{E}\left[\sum_{t<T}\left\|\epsilon_{t}\right\|^{2}\right] \leq\sigma^{2}T,\]

where the first inequality uses \(\tau\leq T\) by its definition; and in the last inequality we use Assumption 4.

For the cross term, note that \(\mathbb{E}_{t-1}\left[\left\langle\nabla f(x_{t}),\epsilon_{t}\right\rangle \right]=0\) by Assumption 4. So this term is a sum of a martingale difference sequence. Since \(\tau\) is a stopping time, we can apply the optional stopping theorem to obtain

\[\mathbb{E}\left[\sum_{t\leq\tau}\bigl{\langle}\nabla f(x_{t}), \epsilon_{t}\right\rangle\right]=0.\] (34)

Then we have

\[\mathbb{E}\left[-\sum_{t<\tau}\bigl{\langle}\nabla f(x_{t}), \epsilon_{t}\bigr{\rangle}\right]= \mathbb{E}\left[\bigl{\langle}\nabla f(x_{\tau}),\epsilon_{\tau }\bigr{\rangle}\right]\leq G\,\mathbb{E}[\left\|\epsilon_{\tau}\right\|]\leq G \sqrt{\mathbb{E}[\left\|\epsilon_{\tau}\right\|^{2}]}\] \[\leq G\sqrt{\mathbb{E}\left[\sum_{t\leq T}\left\|\epsilon_{t} \right\|^{2}\right]}\leq\sigma G\sqrt{T+1}\leq\sigma G\sqrt{2T},\]

where the equality is due to (34); the first inequality uses \(\left\|\nabla f(x_{\tau})\right\|\leq G\) by the definition of \(\tau\) in (5) and Corollary 3.6; the fourth inequality uses \(\mathbb{E}[X]^{2}\leq\mathbb{E}[X^{2}]\) for any random variable \(X\); and the last inequality uses Assumption 4.

Combining all the bounds above, we get

\[\mathbb{E}\left[f(x_{\tau})-f^{*}+\frac{\eta}{2}\sum_{t<\tau} \left\|\nabla f(x_{t})\right\|^{2}\right]\leq f(x_{0})-f^{*}+\eta\sigma G\sqrt{2T}+\eta^{2}\sigma^{2}LT\] \[\leq f(x_{0})-f^{*}+\sigma,\]

where the last inequality is due to Lemma F.1. 

With Lemma F.2, we are ready to prove Theorem 5.3.

Proof of Theorem 5.3.: We want to show the probability of \(\{\tau<T\}\) is small, as its complement \(\{\tau=T\}\) means \(f(x_{t})-f^{*}\leq F\) for all \(t\leq T\) which implies \(\left\|\nabla f(x_{t})\right\|\leq G\) for all \(t\leq T\). Note that

\[\{\tau<T\}=\{\tau_{2}<T\}\cup\{\tau_{1}<T,\tau_{2}=T\}.\]Therefore we only need to bound the probability of each of these two events on the RHS.

We first bound \(\mathbb{P}(\tau_{2}<T)\). Note that

\[\mathbb{P}(\tau_{2}<T)= \mathbb{P}\left(\bigcup_{t<T}\left\{\|\epsilon_{t}\|>\frac{G}{5\eta L }\right\}\right)\] \[\leq \sum_{t<T}\mathbb{P}\left(\|\epsilon_{t}\|>\frac{G}{5\eta L}\right)\] \[\leq \frac{25\eta^{2}T\sigma^{2}L^{2}}{G^{2}}\] \[\leq \delta/4,\]

where the first inequality uses union bound; the second inequality applies Chebyshev's inequality and \(\mathbb{E}[\|\epsilon_{t}\|^{2}]=\mathbb{E}[\mathbb{E}_{t-1}[\|\epsilon_{t}\| ^{2}]]\leq\sigma^{2}\) for each fixed \(t\) by Assumption 4; the last inequality uses Lemma F.1.

Next, we will bound \(\mathbb{P}(\tau_{1}<T,\tau_{2}=T)\). Note that under the event \(\{\tau_{1}<T,\tau_{2}=T\}\), we know that 1) \(\tau=\tau_{1}<T\) which implies \(f(x_{\tau+1})-f^{*}>F\); and 2) \(\tau<T=\tau_{2}\) which implies \(\|\epsilon_{\tau}\|\leq\frac{G}{5\eta L}\) by the definition in (5). Also note that we always have \(f(x_{\tau})-f^{*}\leq F\) which implies \(\|\nabla f(x_{\tau})\|\leq G\) by Corollary 3.6. Then we can show

\[\|x_{\tau+1}-x_{\tau}\|=\eta\,\|g_{\tau}\|\leq\eta(\|\nabla f(x_{\tau})\|+\| \epsilon_{\tau}\|)\leq\eta G+\frac{G}{5L}\leq\frac{G}{L},\]

where we choose \(\eta\leq\frac{1}{2L}\). Then based on Lemma 3.3 and Remark 3.4, we have

\[f(x_{\tau+1})-f(x_{\tau})\leq -\frac{\eta}{2}\left\|\nabla f(x_{\tau})\right\|^{2}-\eta\langle \nabla f(x_{\tau}),\epsilon_{\tau}\rangle+\eta^{2}L\,\|\epsilon_{\tau}\|^{2}\] \[\leq \eta\left\|\nabla f(x_{\tau})\right\|\cdot\|\epsilon_{\tau}\|+ \eta^{2}L\,\|\epsilon_{\tau}\|^{2}\] \[\leq \frac{G^{2}}{4L}\] \[= \frac{F}{2},\]

where the first inequality is obtained following the same derivation as in (33); the last equality is due to Corollary 3.6. Therefore we can show that under the event \(\{\tau_{1}<T,\tau_{2}=T\}\),

\[f(x_{\tau})-f^{*}=f(x_{\tau})-f(x_{\tau+1})+f(x_{\tau+1})-f^{*}>F/2.\]

Hence,

\[\mathbb{P}(\tau_{1}<T,\tau_{2}=T)\leq\mathbb{P}\left(f(x_{\tau})-f^{*}>F/2 \right)\leq\frac{\mathbb{E}[f(x_{\tau})-f^{*}]}{F/2}\leq\frac{2(f(x_{0})-f^{*} +\sigma)}{F}=\delta/4,\]

where the second inequality uses Markov's inequality; the third inequality uses Lemma F.2; and in the last inequality we choose \(F=8(f(x_{0})-f^{*}+\sigma)/\delta\).

Therefore we can show

\[\mathbb{P}(\tau<T)\leq\mathbb{P}(\tau_{2}<T)+\mathbb{P}(\tau_{1}<T,\tau_{2}=T )\leq\delta/2.\]

Then we also know \(\mathbb{P}(\tau=T)\geq 1-\delta/2\geq 1/2\). Therefore, by Lemma F.2,

\[\frac{2(f(x_{0})-f^{*}+\sigma)}{\eta}\geq \mathbb{E}\left[\sum_{t<\tau}\left\|\nabla f(x_{t})\right\|^{2}\right]\] \[\geq \mathbb{P}(\tau=T)\mathbb{E}\left[\sum_{t<T}\left\|\nabla f(x_{t })\right\|^{2}\right]\tau=T\right]\] \[\geq \frac{1}{2}\mathbb{E}\left[\sum_{t<T}\left\|\nabla f(x_{t})\right\| ^{2}\right]\tau=T\right].\]Then we have

\[\mathbb{E}\left[\left.\frac{1}{T}\sum_{t<T}\left\|\nabla f(x_{t})\right\|^{2} \right|\tau=T\right]\leq\frac{4(f(x_{0})-f^{*}+\sigma)}{\eta T}=\frac{\delta F} {2\eta T}\leq\frac{\delta}{2}\cdot\epsilon^{2},\]

where the last inequality uses the choice of \(T\). Let \(\mathcal{E}:=\{\frac{1}{T}\sum_{t<T}\left\|\nabla f(x_{t})\right\|^{2}> \epsilon^{2}\}\) denote the event of not converging to an \(\epsilon\)-stationary point. By Markov's inequality, we have \(\mathbb{P}(\mathcal{E})\leq\delta/2\). Therefore we have \(\mathbb{P}(\{\tau<T\}\cup\mathcal{E})\leq\delta\), which completes the proof. 

## Appendix G Lower bound

In this section, we provide the proof of Theorem 5.4.

Proof of Theorem 5.4.: Let \(c,\eta_{0}>0\) satisfy \(\eta_{0}\leq c^{2}/2\). Consider

\[f(x)=\begin{cases}\log(|x|-c),&|x|\geq y\\ 2\log(y-c)-\log(2y-|x|-c),&c/2\leq|x|<y\\ kx^{2}+b,&|x|<c/2,\end{cases}\]

where \(c>0\) is a constant and \(y=(c+\sqrt{c^{2}+2\eta_{0}})/2>0\) is the fixed point of the iteration

\[x_{t+1}=\left|x_{t}-\frac{\eta_{0}}{x_{t}-c}\right|,\]

and \(k\), \(b\) are chosen in such a way that \(f(x)\) and \(f^{\prime}(x)\) are continuous. Specifically, choose \(k=c^{-1}f^{\prime}(c/2)\) and \(b=f(c/2)-cf^{\prime}(c/2)/4\). Since \(f(-x)=f(x)\), \(f(x)\) is symmetric about the line \(x=0\). In a small neighborhood, \(f(x)\) is symmetric about \((y,f(y))\), so \(f^{\prime}(x)\) is continuous at \(y\).

Let us first consider the smoothness of \(f\). By symmetry, it suffices to consider \(x>0\). Then,

\[f^{\prime}(x)=\begin{cases}(x-c)^{-1},&x\geq y\\ (2y-x-c)^{-1},&c/2\leq x<y\\ 2kx,&0<x<c/2.\end{cases}\]

Its Hessian is given by

\[f^{\prime\prime}(x)=\begin{cases}-(x-c)^{-2},&x>y\\ (2y-x-c)^{-2},&c/2<x<y\\ 2k,&0<x<c/2.\end{cases}\]

Hence, \(f(x)\) is \((2,2k,1)\)-smooth.

Note that \(f(x)\) has a stationary point \(0\). For stepsize \(\eta_{f}\) satisfying \(\eta_{0}\leq\eta_{f}\leq c^{2}/4\), there exists \(z=(c+\sqrt{c^{2}+2\eta_{f}})\geq y\) such that \(-z=z-\eta_{f}(y-c)^{-1}\) and by symmetry, once \(x_{\tau}=z\), \(x_{t}=\pm z\) for all \(t\geq\tau\), making the GD iterations stuck. Now we choose a proper \(x_{0}\) such that \(f^{\prime}(x_{0})\) and \(f(x_{0})-f(0)\) are bounded.

We consider arriving at \(y\) from above. That is, \(x_{0}\geq x_{1}\geq\ldots x_{\tau}=z>c>0\). Since in each update where \(x_{t+1}=x_{t}-\eta_{f}(x_{t}-c)^{-1}>c\),

\[x_{t}-x_{t+1}=x_{t}-(x_{t}-\eta_{f}(x_{t}-c)^{-1})=\eta_{f}(x_{t}-c)^{-1}\leq \sqrt{\eta_{f}}.\]

Hence, we can choose \(\tau\) in such a way that \(3c/2\leq x_{0}<3c/2+\sqrt{\eta_{f}}\). Then,

\[\log(c/2)\leq f(x_{0})\leq\log(c/2+\sqrt{\eta_{f}}),\quad 2/(c+2\sqrt{\eta_{f}}) \leq f^{\prime}(x_{0})\leq 2/c.\]

By definition, \(y-c=\eta_{0}(c+\sqrt{c^{2}+2\eta_{0}})^{-1}\). Hence,

\[f(c/2) =2\log(y-c)-\log(2y-c/2-c)\] \[=2\log(\eta_{0})-2\log(c+\sqrt{c^{2}+2\eta_{0}})-\log(\sqrt{c^{2 }+2\eta_{0}}-c/2),\] \[f^{\prime}(c/2) =\frac{1}{\sqrt{c^{2}+2\eta_{0}}-c/2}\]Then,

\[f(x_{0})-f(0) =f(x_{0})-f(c/2)+cf^{\prime}(c/2)/4\] \[\leq\,\log(c/2+\sqrt{\eta_{f}})+2\log(\eta_{0}^{-1})+2\log(c+\sqrt{ c^{2}+2\eta_{0}})\] \[\qquad\qquad+\log(\sqrt{c^{2}+2\eta_{0}}-c/2)+\frac{c}{4}\frac{1} {\sqrt{c^{2}+2\eta_{0}}-c/2}\] \[\leq\,\log(c)+2\log(\eta_{0}^{-1})+2\log(2\sqrt{2c^{2}})+\log( \sqrt{2c^{2}})+\frac{1}{2}\] \[=4\log(c)+2\log(\eta_{0}^{-1})+\frac{7}{2}\log(2)+\frac{1}{2}.\]

For stepsize \(\eta_{f}<\eta_{0}\), reaching below \(4c/3\) takes at least

\[(x_{0}-4c/3)/\sqrt{\eta_{f}}\geq c/(6\sqrt{\eta_{f}})>c\eta_{0}^{-1/2}/6\]

steps to reach \(4c/3\), where \(f^{\prime}(4c/3)=\log(c/3)\).

Now we set \(c\) and \(\eta_{0}\) and scale function \(f(x)\) to satisfy the parameter specifications \(L_{0},L_{2},G_{0},\Delta_{0}\). Define \(g(x)=L_{2}^{-1}f(x)\). Then, \(g(x)\) is \((2,2kL_{2}^{-1},L_{2})\)-smooth. Since the gradient of \(g(x)\) is \(L_{2}^{-1}\) times \(f(x)\), the above analysis for \(f(x)\) applies to \(g(x)\) by replacing \(\eta_{0}\) with \(\eta_{1}=L_{2}\eta_{0}\) and \(\eta_{f}\) with \(\eta=L_{2}\eta_{f}\). To ensure that

\[2kL_{2}^{-1}=2(cL_{2})^{-1}f^{\prime}(c/2)=\frac{2}{cL_{2}}\frac{1}{\sqrt{c^{2 }+2\eta_{1}}-c/2}\leq\frac{4}{c^{2}L_{2}}\leq L_{0},\]

it suffices to take \(c\geq 2/\sqrt{L_{0}L_{2}}\). To ensure that

\[g^{\prime}(x_{0})\leq\frac{2}{L_{2}c}\leq G_{0},\]

it suffices to take \(c\geq 2/(L_{2}G_{0})\). To ensure that

\[g(x_{0})-g(0)\leq(4\log(c)+2\log(\eta_{1}^{-1})+3.5\log 2+0.5)L_{2}^{-1}\leq \Delta_{0},\]

it suffices to take

\[\log(\eta_{1}^{-1})=\frac{L_{2}\Delta_{0}-3.5\log 2-0.5}{2}-2\log(c).\]

Since we require \(\eta_{1}\leq c^{2}/2\), parameters \(L_{2}\) and \(\Delta_{0}\) need to satisfy

\[\log 2-2\log(c)\leq\frac{L_{2}\Delta_{0}-3.5\log 2-0.5}{2}-2\log(c),\]

that is, \(L_{2}\Delta_{0}\geq 5.5\log 2+0.5\), which holds because \(L_{2}\Delta_{0}\geq 10\). Take \(c=\max\{2/\sqrt{L_{0}L_{2}},2/(L_{2}G_{0}),\sqrt{8/L_{0}}\}\). Then, as long as \(\eta\leq 2/L_{0}\), the requirement that \(\eta\leq c^{2}/4\) is satisfied. Therefore, on \(g(x)\) with initial point \(x_{0}\), gradient descent with a constant stepsize either gets stuck, or takes at least

\[c\eta_{1}^{-1/2}/6 =\frac{c}{6}\exp\Big{(}\frac{L_{2}\Delta_{0}-3.5\log 2-0.5}{4}-\log(c )\Big{)}\] \[=\frac{1}{6}\exp(\frac{L_{2}\Delta_{0}-3.5\log 2-0.5}{4})\] \[\geq\frac{1}{6}\exp(\frac{L_{2}\Delta_{0}}{8})\]

steps to reach a \(1\)-stationary point.

On the other hand, if \(\eta>2/L_{0}\), consider the function \(f(x)=\frac{L_{0}}{2}x^{2}\). For any \(x_{t}\neq 0\), we always have \(\left|x_{t+1}\right|/\left|x_{t}\right|=\left|1-\eta L_{0}\right|>1\), which means the iterates diverge to infinity.