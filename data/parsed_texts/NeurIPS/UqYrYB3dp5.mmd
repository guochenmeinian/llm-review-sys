Polynomially Over-Parameterized Convolutional Neural Networks Contain Structured Strong Winning Lottery Tickets

Arthur da Cunha

Universite Cote d'Azur, Inria, CNRS, I3S

 Aarhus University

 Aarhus, Denmark

dac@cs.au.dk

Francesco d'Amore

Aalto University

 Bocconi University

 Espoo, Finland

francesco.damore@aalto.fi

&Emanuele Natale

Universite Cote d'Azur, Inria, CNRS, I3S

 Sophia Antipolis, France

emanuele.natale@inria.fr

###### Abstract

The Strong Lottery Ticket Hypothesis (SLTH) states that randomly-initialised neural networks likely contain subnetworks that perform well without any training. Although unstructured pruning has been extensively studied in this context, its structured counterpart, which can deliver significant computational and memory efficiency gains, has been largely unexplored. One of the main reasons for this gap is the limitations of the underlying mathematical tools used in formal analyses of the SLTH. In this paper, we overcome these limitations: we leverage recent advances in the multidimensional generalisation of the Random Subset-Sum Problem and obtain a variant that admits the stochastic dependencies that arise when addressing structured pruning in the SLTH. We apply this result to prove, for a wide class of random Convolutional Neural Networks, the existence of structured subnetworks that can approximate any sufficiently smaller network.

This result provides the first sub-exponential bound around the SLTH for structured pruning, opening up new avenues for further research on the hypothesis and contributing to the understanding of the role of over-parameterization in deep learning.

## 1 Introduction

Much of the success of deep learning techniques relies on extreme over-parameterization (Zhang et al., 2017, 2021; Novak et al., 2018; Brutzkus et al., 2018; Du et al., 2019; Kaplan et al., 2020). While such excess of parameters has allowed neural networks to become the state of the art in many tasks, the associated computational cost limits both the progress of those techniques and their deployment in real-world applications. This limitation motivated the development of methods for reducing the number of parameters of neural networks; both in the past (Reed, 1993) and in the present (Blalock et al., 2020; Hoefler et al., 2021).

Although pruning methods have traditionally targeted reducing the size of networks for inference purposes, recent works have indicated that they can also be used to reduce parameter counts during training or even at initialisation without sacrificing model accuracy. In particular, Frankle and Carbin (2019) proposed the _Lottery Ticket Hypothesis (LTH)_, which conjectures that randomly-initialisednetworks contain sparse subnetworks that can be trained and reach the performance of the fully-trained original network. Empirical investigations on the LTH (Zhou et al., 2019; Ramanujan et al., 2020; Wang et al., 2020) pointed towards an even more striking phenomenon: the existence of subnetworks that perform well without any training. This conjecture was named the _Strong Lottery Ticket Hypothesis (SLTH)_ by Pensia et al. (2020).

While the SLTH has been proved for many different classes of neural networks (see Section 2), those works are restricted to unstructured pruning, where the subnetworks are obtained by freely removing individual parameters from the original network. However, this lack of structure can significantly reduce the gains that sparsity can bring, both in terms of memory and computational efficiency. The possibility of removing parameters at arbitrary points of the network implies the need to store the indices of the remaining non-zero parameters, which can become a significant overhead with its own research challenges (Pooch and Nieder, 1973). Moreover, the theoretical computational gains of unstructured sparsity can also be difficult to realise in standard hardware, which is optimised for dense operations. Most notably, the irregularity of the memory access patterns can lead to both data and instruction cache misses, significantly reducing the performance of the pruned network.

The limitations of parameter-level pruning have motivated extensive research on _structured pruning_, which constrain the sparsity patterns to reduce the complexity of parameter indexation and, more generally, to make the processing of the pruned network more efficient. A simple example of structured pruning is _neuron pruning_ of fully-connected layers: deletions in the weight matrix are constrained to the level of whole rows/columns. As illustrated by Figure 1, pruning under this constraint produces a smaller network that is still dense, directly reducing the computational costs without any need for extra memory to store indices. Similarly, deleting entire filters in Convolutional Neural Networks (CNNs) (Polyak and Wolf, 2015) or "heads" in attention-based architectures (Michel et al., 2019) also produces direct reductions in computational costs.

It is important to note that structured pruning is a restriction of unstructured pruning so, theoretically, the former is bound to perform at most as well as the latter. For example, by deleting whole neurons one can remove about 70% of the weights in dense networks without significantly affecting accuracy, while through unstructured pruning one can usually reach 95% sparsity without accuracy loss (Alvarez and Salzmann, 2016; Liu et al., 2019). In practice, however, the computational advantage of structured pruning can offset this difference: a suitably structured sparse network can be more efficient than an even sparser one that lacks structure. This trade-off between sparsity and actual efficiency has motivated the study of less coarse sparsity patterns since weaker structural constraints such as strided sparsity (Anwar et al., 2017) (Figure 2b) or block sparsity (Siswanto, 2021) (Figure 2c) are already sufficient to deliver the bulk of the computational gains that structured can offer.

Despite its benefits, structured pruning has received little attention in the context of the SLTH. In fact, the work that first proved a version of the SLTH, Malach et al. (2020), has remained the only one to study this scenario, to the best of our knowledge. Moreover, it brings a negative result: the authors prove that removing neurons from a randomly-initialised shallow neural network (a single hidden layer) is equivalent to the random features model1(e.g., Rahimi and

Figure 1: Illustration of neuron pruning. The left side shows the effect of pruning of neurons in the weight-matrix of a fully-connected layer. The rows in white correspond to neurons pruned in the associated layer while the columns in white represent the effect of removing neurons from the previous layers. On the right, we allude to the possibility of collapsing the pruned matrix into a smaller, dense one.

which cannot efficiently approximate even a single ReLU neuron (Yehudai and Shamir, 2019).

As most proofs around the SLTH involve pruning a shallow random network to build the desired approximations, those results show that the SLTH can be challenging to tackle when restricted to neuron pruning.

In addition, we believe that a factor hindering progress toward structured versions of the SLTH more generally is a limitation of a result underlying almost all of the theoretical works on the SLTH: a theorem by Lueker on the _Random Subset-Sum Problem (RSSP)_.

**Theorem 1** ((Lueker, 1998; Da Cunha et al., 2023)).: _Let \(X_{1},\ldots,X_{n}\) be independent uniform random variables over \([-1,1]\), and let \(\varepsilon\in(0,1/3)\). There exists a universal constant \(C>0\) such that, if \(n\geq C\log(1/\varepsilon)\), then, with probability at least \(1-\varepsilon\), for all \(z\in[-1,1]\) there exists \(S_{z}\subseteq[n]\) for which_

\[\Big{|}z-\sum_{i\in S_{z}}X_{i}\Big{|}\leq\varepsilon.\]

In general terms, the theorem states that given a rather small number of random variables, there is a high probability that any target value within an interval of interest can be approximated by a sum of a subset of the random variables. An important remark is that even though Theorem 1 is stated in terms of uniform random variables, it is not hard to extend it to a wide class of distributions.2

Footnote 2: Distributions whose probability density function \(f\) satisfies \(f(x)\geq b\) for all \(x\in[-a,a]\), for some constants \(a,b>0\) (see Lueker (1998, Corollary 3.3)).

While Theorem 1 closely matches the setup of the SLTH, it only concerns individual random variables and directly applying it to entire random structures, as needed when considering structured pruning, would require an exponential number of random variables. The recent works by Borst et al. (2022); Becchetti et al. (2022) reduced this gap by proving multidimensional versions of Theorem 1. Still, the intricate manipulation of the network parameters in proofs around the SLTH imposes restrictions that are not covered by those results.

#### Contributions

In this work, we overcome those obstacles and prove that random networks in a wide class of CNNs are likely to contain structured subnetworks that approximate any sufficiently smaller CNN. To the best of our knowledge, our results provide the first sub-exponential bounds around the SLTH for structured pruning of deep neural networks of any kind. More precisely,

* We prove a multidimensional version Theorem 1 that is robust to some dependencies between coordinates, which is crucial for structured pruning (Theorem 5);
* We leverage this result and, by combining two types of structured sparsity (block and neuron/filter sparsity), we show that, with high probability and for a wide class of architectures, polynomially over-parameterized random networks can be pruned in a structured manner to approximate any target network (Theorem 2);

Figure 2: Examples of different pruning patterns.

* Our results cover CNNs, which generalise fully-connected networks as well as many layer types commonly used in modern architectures, such as pooling and normalisation layers;
* Additionally, our pruning scheme focuses on filter pruning, which, like neuron pruning, allows for a direct reduction of the size and computational cost relative to the original CNN.

## 2 Related Work

SlthPut roughly, research on the SLTH revolves around the following question:

_Question._ Given an error margin \(\varepsilon>0\) and a target neural network \(f_{\mathrm{target}}\), how large must an architecture \(f_{\mathrm{random}}\) be to ensure that, with high probability on the sampling of its parameters, one can prune \(f_{\mathrm{random}}\) to obtain a subnetwork that approximates \(f_{\mathrm{target}}\) up to output error \(\varepsilon\)?

Malach et al. (2020) first proved that, for dense networks with ReLU activations, it was sufficient for \(f_{\mathrm{random}}\) to be twice as deep and polynomially wider than \(f_{\mathrm{target}}\). Orseau et al. (2020) showed that the width overhead could be greatly reduced by sampling parameters from a hyperbolic distribution. Pensia et al. (2020) improved the original result for a wide class of weight distribution, requiring only a logarithmic width overhead, which the authors proved to be asymptotically optimal. Da Cunha et al. (2022) generalised those results with optimal bounds to CNNs with non-negative inputs, which Burkholz (2022a) extended to general inputs and to residual architectures. Burkholz (2022a) also reduced the depth overhead to a single extra layer and provided results that include a whole class of activation functions. Burkholz (2022b) obtained similar improvements to dense architectures. Fischer and Burkholz (2021) modified many of the previous arguments to take into consideration networks with non-zero biases. Ferbach et al. (2022) further generalised previous results on CNNs to general equivariant networks. Diffenderfer and Kailkhura (2021) obtained similar SLTH results for binary dense neural networks within polynomial depth and width overhead, which Sreenivasan et al. (2022) improved to polylogarithmic overhead.

Structured pruningWorks on structured pruning date back to the early days of the field of neural network sparsification with works such as Mozer and Smolensky (1988) and Mozer and Smolensky (1989). Since then, a vast literature has been built around the topic, particularly for the pruning of CNNs. For a survey of structured pruning in general, we refer the reader to the associated sections of Hoefler et al. (2021), and to He and Xiao (2023) for a survey on structured pruning of CNNs.

RsspPensia et al. (2020) introduced the use of theoretical results on the RSSP in arguments around the SLTH, namely Lueker (1998, Corollary 3.3). The work by Da Cunha et al. (2023) provides an alternative, simpler proof of this result. Borst et al. (2022) and Becchetti et al. (2022) proved multidimensional versions of the theorem. Theorem 5 diverges from those results in that it supports some dependencies between the entries of random vectors.

## 3 Preliminaries and contribution

Given \(n\in\mathbb{N}\), we denote the set \(\left\{1,\ldots,n\right\}\) by \([n]\). The symbol \(*\) represents the convolution operation, \(\odot\) represents the element-wise (Hadamard) product, and \(\phi\) represents the ReLU activation function. The notation \(\left\lVert\cdot\right\rVert_{1}\) refers to the sum of the absolute values of each entry in a tensor. Similarly, \(\left\lVert\cdot\right\rVert_{2}\) refers to the square root of the sum of the squares of each entry in a tensor. \(\left\lVert\cdot\right\rVert_{\mathrm{max}}\) denotes the maximum norm: the maximum among the absolute value of each entry. Sometimes we represent a tensor \(X\in\mathbb{R}^{d_{1}\times\cdots\times d_{n}}\) by the notation \(X=(X_{i_{1},\ldots,i_{n}})_{i_{1}\in[d_{1}],\ldots,i_{n}\in[d_{n}]}\). We denote the normal probability distribution with mean \(\mu\) and variance \(\sigma^{2}\) by \(\mathcal{N}(\mu,\sigma^{2})\). We write \(U\sim\mathcal{N}^{d_{1}\times\cdots\times d_{n}}\) to denote that \(U\) is a random tensor of size \(d_{1}\times\cdots\times d_{n}\) with independent and identically distributed (i.i.d.) entries, each following \(\mathcal{N}(0,1)\). We refer to such random tensors as _normal tensors_. Finally, we refer to the axis of a 4-D tensor as _rows_, _columns_, _channels_, and _kernels_ (a.k.a. filters), in this order.

For the sake of simplicity, we assume CNNs to be of the form \(N\colon[-1,1]^{D\times D\times c_{0}}\to\mathbb{R}^{D\times D\times c_{\ell}}\) given by

\[N(X)=K^{\ell}*\phi(K^{\ell-1}*\cdots\phi(K^{1}*X)),\]

where \(K^{i}\in\mathbb{R}^{d_{i}\times d_{i}\times c_{i-1}\times c_{i}}\) for \(i\in[\ell]\), and the convolutions have no bias and are suitably padded with zeros. Moreover, when the kernels \(K^{(i)}\) are normal tensors, we say that \(N\) is a _random CNN_.

Our main result is the following.

**Theorem 2** (Structured SLTH).: _Let \(D,c_{0},\ell\in\mathbb{N}\), and \(\varepsilon\in\mathbb{R}_{>0}\). For \(i\in[\ell]\), let \(d_{i},c_{i},n_{i}\in\mathbb{N}\). Let \(\mathcal{F}\) be the class of functions from \([-1,1]^{D\times D\times c_{0}}\) to \(\mathbb{R}^{D\times D\times c_{\ell}}\) such that, for each \(f\in\mathcal{F}\)_

\[f(X)=K^{(\ell)}*\phi(K^{(\ell-1)}*\cdots\phi(K^{(1)}*X)),\] (1)

_where, for \(i\in[\ell]\), \(K^{(i)}\in\mathbb{R}^{d_{i}\times d_{i}\times c_{i-1}\times c_{i}}\) and \(\left\|K^{(i)}\right\|_{1}\leq 1\)._

_Let also \(N_{0}\colon[-1,1]^{D\times D\times c_{0}}\to\mathbb{R}^{D\times D\times c_{ \ell}}\) be a \(2\ell\)-layered random CNN given by_

\[N_{0}(X)=L^{(2\ell)}*\phi(L^{(2\ell-1)}*\cdots\phi(L^{(1)}*X)),\] (2)

_where for \(i\in[\ell]\) the kernels \(L^{(2i-1)}\) and \(L^{(2i)}\) are normal tensors of shape \(1\times 1\times c_{i-1}\times 2n_{i}c_{i-1}\) and \(d_{i}\times d_{i}\times 2n_{i}c_{i-1}\times c_{i}\), respectively._

_Finally, let \(\mathcal{G}\) be the class of subnetworks that can be obtained by pruning contiguous blocks of parameters and removing entire filters from \(N_{0}\)._

_There exists a universal constant \(C>0\), such that if, for \(i\in[\ell]\),_

\[n_{i}\geq Cd_{i}^{13}c_{i}^{6}\log^{3}\frac{d_{i}^{2}c_{i}c_{i-1}\ell}{ \varepsilon},\]

_then, with probability at least \(1-\varepsilon\), we have that, for all \(f\in\mathcal{F}\),_

\[\sup_{X\in[-1,1]^{D\times D\times c_{0}}}\min_{g\in\mathcal{G}}\left\|f(X)-g( X)\right\|_{\max}\leq\varepsilon.\]

The filter removals ensured by Theorem 2 take place at layers \(1,3,\ldots,2\ell-1\) and imply the removal of the corresponding channels in the next layer. The overall modification yields a CNN with kernels \(\tilde{L}^{(1)},\ldots,\tilde{L}^{(2\ell)}\) such that, for \(i\in[\ell]\), the kernels \(\tilde{L}^{(2i-1)}\) and \(\tilde{L}^{(2i)}\) have shape \(1\times 1\times c_{i-1}\times 2c_{i-1}m_{i}\) and \(d_{i}\times d_{i}\times 2c_{i-1}m_{i}\times c_{i}\), respectively, where \(m_{i}=\sqrt{n_{i}/(C_{1}d_{i}\log(1/\varepsilon))}\) for a universal constant \(C_{1}\). Moreover, our proof ensures that the kernels \(\tilde{L}^{(2i-1)}\) can be required to have a specific type of block sparsity: they can be structured as if pruned by \(2m_{i}\)-channel-blocked masks, defined as follows.

**Definition 3** (\(n\)-channel-blocked mask).: Given a positive integer \(n\), a binary tensor \(S\in\{0,1\}^{d\times d\times c\times cn}\) is called \(n\)-channel-blocked if and only if

\[S_{i,j,k,l}=\begin{cases}1&\text{if }\left\lceil\frac{l}{n}\right\rceil=k,\\ 0&\text{otherwise},\end{cases}\]

for all \(i,j\in[d]\), \(k\in[c]\), and \(l\in[cn]\).

We remark that, from a broader perspective, the central aspect of Theorem 2 is that the lower bound on the size of the random CNN depends only on the kernel sizes of the CNNs being approximated.

In subsection 4.2 we discuss the proof of Theorem 2. It requires handling subset-sum problems on multiple random variables at once (random vectors). Furthermore, the inherent parameter-sharing of CNNs creates a specific type of stochastic dependency between coordinates of the random vectors, which we capture with the following definition.

**Definition 4** (NSN vector).: A \(d\)-dimensional random vector \(Y\) follows a _normally-scaled normal_ (NSN) distribution if, for each \(i\in[d]\), \(Y_{i}=Z\cdot Z_{i}\) where \(Z,Z_{1},\ldots,Z_{d}\) are i.i.d. random variables following a standard normal distribution.

A key technical contribution of ours is a Multidimensional Random Subset Sum (MRSS) result that supports NSN vectors. In subsection 4.1 we discuss the proof of the next theorem, which follows a strategy similar to that of [Borst et al., 2022, Lemmas 1, 15].

**Theorem 5** (Normally-scaled MRSS).: _Let \(0<\varepsilon\leq 1/4\), and let \(d\), \(k\), and \(n\) be positive integers such that \(n\geq k^{2}\) and \(k\geq Cd^{3}\log\frac{d}{\varepsilon}\) for some universal constant \(C\in\mathbb{R}_{>0}\). Furthermore, let \(X_{1},\ldots,X_{n}\) be \(d\)-dimensional i.i.d. NSN random vectors. For any \(\vec{z}\in\mathbb{R}^{d}\) with \(\left\|\vec{z}\right\|_{1}\leq\sqrt{k}\), there exists with constant probability a subset \(S\subseteq[n]\) of size \(k\) such that \(\left\|\left(\sum_{i\in S}X_{i}\right)-\vec{z}\right\|_{\max}\leq\varepsilon\)._

While it is possible to naively apply Theorem 1 to obtain a version of Theorem 2, doing so leads to an exponential lower bound on the required number of random vectors.

Analysis

In this section, after proving our MRSS result (Theorem 5), we discuss how to use it to obtain our main result on structured pruning (Theorem 2). Full proofs are deferred to Appendix B.

### Multidimensional Random Subset Sum for normally-scaled normal vectors

Notation.Given a set \(S\) and a positive integer \(n\), we denote by \(\binom{S}{n}\) the family of all subsets of \(S\) containing exactly \(n\) elements of \(S\). Given \(\varepsilon\in\mathbb{R}_{>0}\), we define the interval \(I_{\varepsilon}\left(z_{i}\right)=\left[z_{i}-\varepsilon,z_{i}+\varepsilon\right]\) and the multi-interval \(I_{\varepsilon}\left(\vec{z}\right)=\left[\vec{z}-\varepsilon\mathbf{1},\vec{ z}+\varepsilon\mathbf{1}\right]\), where \(\mathbf{1}=\left(1,1,\ldots,1\right)\in\mathbb{R}^{d}\). Moreover, for any event \(\mathcal{E}\), we denote its complementary event by \(\mathcal{\bar{E}}\).

In this subsection, we estimate the probability that a set of \(n\) random vectors contains a subset that sums up to a value that is \(\varepsilon\)-close to a given target. The following definition formalises this notion.

**Definition 6** (Subset-sum number).: Given (possibly random) vectors \(X_{1},\ldots,X_{n}\) and a vector \(\vec{z}\), we define the _\(\varepsilon\)-subset-sum number_ of \(X_{1},\ldots,X_{n}\) for \(\vec{z}\) as

\[T_{X_{1},\ldots,X_{n}}^{k}\left(\vec{z}\right)=\sum_{S\in\binom{[n]}{k}} \mathbf{1}_{\varepsilon_{S}^{\left(\vec{z}\right)}},\]

where \(\mathcal{E}_{S}^{\left(\vec{z}\right)}\) denotes the event \(\left\|\left(\sum_{i\in S}X_{i}\right)-\vec{z}\right\|_{\max}\leq\varepsilon\). We write simply \(T_{n,k}\) when \(X_{1},\ldots,X_{n}\) and \(\vec{z}\) are clear from the context.

To prove Theorem 5 we use the second moment method to provide a lower bound on the probability that the subset-sum number \(T_{n,k}\) is strictly positive, which implies that at least one subset of the random vectors suitably approximates \(\vec{z}\). Hence, we seek a lower bound on \(\mathbb{E}[T_{n,k}]^{2}/\mathbb{E}[T_{n,k}^{2}]\).

Our first lemma provides a lower bound on the probability that a sum of NSN vectors is \(\varepsilon\)-close to a target vector, through which one can infer a lower bound on \(\mathbb{E}\left[T_{n,k}\right]\).

**Lemma 7** (Sum of NSN vectors).: _Let \(k\in\mathbb{N}\), \(\varepsilon\in\left(0,\frac{1}{4}\right)\), \(\vec{z}\in\mathbb{R}^{d}\) such that \(\left\|\vec{z}\right\|_{1}\leq\sqrt{k}\) and \(k\geq 16\). Furthermore, let \(X_{1},\ldots,X_{k}\) be \(d\)-dimensional i.i.d. NSN random vectors with \(d\leq k\), and let \(c_{d}=\min\left\{\frac{1}{d^{2}},\frac{1}{16}\right\}\). It holds that_

\[\Pr\biggl{(}\sum_{i=1}^{k}X_{i}\in I_{\varepsilon}\left(\vec{z}\right)\biggr{)} \geq\frac{1}{16}\biggl{(}\frac{2\varepsilon}{\sqrt{\pi\left(1+2\sqrt{c_{d}}+2 c_{d}\right)k}}\biggr{)}^{d}.\]

Overview of the proof.: The main technical difficulty lies in the fact that the random vectors \(X_{1},\ldots,X_{k}\) are NSN vectors. Bounds can be easily derived for the case where the \(X_{i}\) are i.i.d. normal random vectors by observing that the sum of normal random variables is also normal.

For \(i\in[k]\), each entry of \(X_{i}\) can be written as \(Z_{i}\cdot Z_{i,j}\) where \(Z_{i}\) and \(Z_{i,j}\) are i.i.d. normal random variables. Conditional on \(Z_{1},\ldots,Z_{k}\), the \(d\) entries of \(X=\sum_{i=1}^{k}X_{i}\) are independent and distributed as \(\mathcal{N}(0,\sum_{i=1}^{k}Z_{i}^{2})\). By noticing that \(Z_{i}^{2}\) is a chi-squared random variable and employing standard concentration inequalities (Lemma 17 in Appendix A) combined with the law of total probability, we can proceed as if the entries of \(X\) were normal, up to some correction factors. 

Bounding \(\mathbb{E}[T_{n,k}^{2}]\) requires handling stochastic dependencies. Thus, we estimate the joint probability that two subsets of \(k\) elements of \(X_{1},\ldots,X_{n}\) sum \(\varepsilon\)-close to the same target, taking into account that the intersection of the subsets might not be empty. The next lemma provides an upper bound on this joint probability that depends only on the size of the symmetric difference between the two subsets.

**Lemma 8** (Sum of NSN vectors).: _Let \(k,j\in\mathbb{N}_{0}\) with \(1\leq j\leq k\) Furthermore, let \(X_{1},\ldots,X_{k+j}\) be i.i.d. \(d\)-dimensional NSN random vectors with \(k\geq Cd^{3}\log\frac{d}{\varepsilon}\). Let \(c_{d}=\min\left\{\frac{1}{d^{2}},\frac{1}{16}\right\}\), \(A=\sum_{i=1}^{j}X_{i}\), \(B=\sum_{i=j+1}^{k}X_{i}\), and \(C=\sum_{i=k+1}^{k+j}X_{i}\).3 Then, it holds that_

Footnote 3: We adopt the convention that \(\sum_{i=1}^{0}X_{i}=0\).

\[\Pr\left(A+B\in I_{\varepsilon}\left(\vec{z}\right),B+C\in I_{\varepsilon} \left(\vec{z}\right)\right)\leq 3\biggl{(}\frac{4\varepsilon^{2}}{\pi\left(1-2 \sqrt{c_{d}}\right)j}\biggr{)}^{d}.\]Overview of the proof.: Let \(n=k+j\). We exploit once more the fact that, for all \(i\in[n]\), each entry \(X_{i}\) can be written as \(Z_{i}\cdot Z_{i,j}\) where \(Z_{i}\) and \(Z_{i,j}\) are i.i.d. normal random variables. Conditional on \(Z_{1},\ldots,Z_{k+j}\), the \(d\) entries of \(A\), \(B\), and \(C\) are independent and distributed as \(\mathcal{N}(0,\sum_{i=1}^{j}Z_{i}^{2})\), \(\mathcal{N}(0,\sum_{i=j+1}^{k}Z_{i}^{2})\), and \(\mathcal{N}(0,\sum_{i=k+1}^{k+j}Z_{i}^{2})\), respectively. Hence, by the concentration inequalities for the sum of chi-squared random variables (Lemma 17 in Appendix A) and by the law of total probability, we can focus on the term

\[\Pr\left(A_{i}+B_{i}\in I_{\varepsilon}\left(z_{i}\right),B_{i}+C_{i}\in I_{ \varepsilon}\left(z_{i}\right)|Z_{1},\ldots,Z_{n}\right),\]

where \(A_{i}\), \(B_{i}\), and \(C_{i}\) indicate the \(i\)-th entries of \(A\), \(B\), and \(C\), respectively.

Another concentration argument for normal random variables (Lemma 14 in Appendix A), allow us to show that

\[\Pr\left(A_{i}+B_{i}\in I_{\varepsilon}\left(z_{i}\right),B_{i}+ C_{i}\in I_{\varepsilon}\left(z_{i}\right)|Z_{1},\ldots,Z_{n}\right)\] \[\quad=\mathbb{E}_{B_{i}}\left[\Pr\left(A_{i}\in I_{\varepsilon} \left(z_{i}-B_{i}\right),C_{i}\in I_{\varepsilon}\left(z_{i}-B_{i}\right)|Z_{ 1},\ldots,Z_{n},B_{i}\right)\right]\] \[\quad=\mathbb{E}_{B_{i}}\left[\Pr\left(A_{i}\in I_{\varepsilon} \left(z_{i}-B_{i}\right)|Z_{1},\ldots,Z_{n},B_{i}\right)\Pr\left(C_{i}\in I_{ \varepsilon}\left(z_{i}-B_{i}\right)|Z_{1},\ldots,Z_{n},B_{i}\right)\right]\] \[\quad\leq\mathbb{E}_{B_{i}}\left[\Pr\left(A_{i}\in I_{\varepsilon }\left(0\right)|Z_{1},\ldots,Z_{n},B_{i}\right)\Pr\left(C_{i}\in I_{ \varepsilon}\left(0\right)|Z_{1},\ldots,Z_{n},B_{i}\right)\right]\] \[\quad=\Pr\left(A_{i}\in I_{\varepsilon}\left(0\right)|Z_{1}, \ldots,Z_{n}\right)\Pr\left(C_{i}\in I_{\varepsilon}\left(0\right)|Z_{1}, \ldots,Z_{n}\right).\]

Thus, we have reduced our argument to the estimation of probabilities of independent normal random variables being close to zero. 

The following lemma provides an explicit expression for the variance of the \(\varepsilon\)-subset-sum number.

**Lemma 9** (Second moment of \(T_{n,k}\)).: _Let \(k,n\) be positive integers. Let \(S_{0},S_{1},\ldots,S_{k}\) be subsets of \([n]\) such that \(\left|S_{0}\cap S_{j}\right|=k-j\) for \(j=0,1,\ldots,k\). Let \(\mathrm{S},\mathrm{S}^{\prime}\) be two random variables yielding two subsets of \([n]\) of size \(k\) drawn independently and uniformly at random. Let \(X_{1},\ldots,X_{n}\) be \(d\)-dimensional i.i.d. NSN random vectors. For any \(\varepsilon>0\) and \(\vec{z}\in\mathbb{R}^{d}\), the second moment of the \(\varepsilon\)-subset sum number is_

\[\mathbb{E}\left[T_{n,k}^{2}\right]=\binom{n}{k}^{2}\sum_{j=0}^{k}\Pr\left( \left|\mathrm{S}\cap\mathrm{S}^{\prime}\right|=k-j\right)\Pr\left(\mathcal{E }_{S_{0}}^{\left(\vec{z}\right)}\cap\mathcal{E}_{S_{j}}^{\left(\vec{z}\right) }\right),\]

_where \(\mathcal{E}_{S}^{\left(\vec{z}\right)}\) denotes the event \(\left\|\left(\sum_{i\in S}X_{i}\right)-\vec{z}\right\|_{\max}\leq\varepsilon\)._

Proof.: Let \(\mathrm{S},\mathrm{S}^{\prime}\) two random variables yielding two elements of \(\binom{[n]}{k}\) drawn independently and uniformly at random. By the definition of \(T_{n,k}\), we have that

\[\mathbb{E}\left[T_{n,k}^{2}\right] =\mathbb{E}\bigg{[}\bigg{(}\sum_{S\in\binom{[n]}{k}}\mathbf{1}_{ \mathcal{E}_{S}^{\left(\vec{z}\right)}}\bigg{)}\bigg{(}\sum_{S^{\prime}\in \binom{[n]}{k}}\mathbf{1}_{\mathcal{E}_{S^{\prime}}^{\left(\vec{z}\right)}} \bigg{)}\bigg{]}\] \[=\sum_{S,S^{\prime}\in\binom{[n]}{k}}\Pr\left(\mathcal{E}_{S}^{ \left(\vec{z}\right)}\cap\mathcal{E}_{S^{\prime}}^{\left(\vec{z}\right)}\right)\] \[=\sum_{S,S^{\prime}\in\binom{[n]}{k}}\Pr\left(\mathcal{E}_{S}^{ \left(\vec{z}\right)}\cap\mathcal{E}_{S^{\prime}}^{\left(\vec{z}\right)} \Big{|}\,\mathrm{S}=S,\mathrm{S}^{\prime}=S^{\prime}\right)\Pr\left(\mathrm{S}= S,\mathrm{S}^{\prime}=S^{\prime}\right)\] \[=\binom{n}{k}^{2}\sum_{j=0}^{k}\Pr\left(\mathcal{E}_{S}^{\left( \vec{z}\right)}\cap\mathcal{E}_{S^{\prime}}^{\left(\vec{z}\right)}\Big{|}\, \mathrm{S}\cap\mathrm{S}^{\prime}|=k-j\right)\Pr\left(\left|\mathrm{S}\cap \mathrm{S}^{\prime}\right|=k-j\right),\]

as \(\Pr\left(\mathcal{E}_{S}^{\left(\vec{z}\right)}\cap\mathcal{E}_{S^{\prime}}^{ \left(\vec{z}\right)}\right)\) depends only on the size of \(\mathrm{S}\cap\mathrm{S}^{\prime}\).

### Overview of the proof of Theorem 5

We use the second moment method (Lemma 15 in Appendix A) on the \(\varepsilon\)-subset-sum number \(T_{n,k}\) of \(X_{1},\ldots,X_{n}\). Thus, we want to lower bound the right-hand side of

\[\Pr\left(T>0\right)\geq\frac{\mathbb{E}[T_{n,k}]^{2}}{\mathbb{E}[T_{n,k}^{2}]}.\]

Equivalently, we can provide an upper bound on the inverse, \(\frac{\mathbb{E}[T_{n,k}^{2}]}{\mathbb{E}[T_{n,k}]^{2}}\). By Lemma 9,

\[\mathbb{E}\left[T_{n,k}^{2}\right]=\binom{n}{k}^{2}\sum_{j=0}^{k}\Pr\left(| \mathrm{S}\cap\mathrm{S}^{\prime}|=k-j\right)\Pr\left(\mathcal{E}_{S_{0}}^{ \left(\vec{\tau}\right)}\cap\mathcal{E}_{S_{j}}^{\left(\vec{\tau}\right)}\right)\]

where \(\mathrm{S},\mathrm{S}^{\prime},S_{i}\) and \(\mathcal{E}_{S}^{\left(\vec{\tau}\right)}\) are defined as in the statement of the lemma. Observe also that

\[\mathbb{E}\left[T_{n,k}\right]=\sum_{S\in\binom{[n]}{k}}\mathbb{E}\left[ \mathbf{1}_{\mathcal{E}_{S}^{\left(\vec{\tau}\right)}}\right]=\sum_{S\in \binom{[n]}{k}}\Pr\left(\mathcal{E}_{S}^{\left(\vec{\tau}\right)}\right)= \binom{n}{k}\Pr\left(\mathcal{E}_{S_{0}}^{\left(\vec{\tau}\right)}\right).\]

By using the two above observations, we have

\[\frac{\mathbb{E}[T_{n,k}]^{2}}{\mathbb{E}[T_{n,k}^{2}]} =\frac{\binom{n}{k}^{2}}{\mathbb{E}\left[T_{n,k}\right]^{2}}\sum _{j=0}^{k}\Pr\left(|\mathrm{S}\cap\mathrm{S}^{\prime}|=k-j\right)\Pr\left( \mathcal{E}_{S_{0}}^{\left(\vec{\tau}\right)}\cap\mathcal{E}_{S_{j}}^{\left( \vec{\tau}\right)}\right)\] \[=\sum_{j=0}^{k}\Pr\left(|\mathrm{S}\cap\mathrm{S}^{\prime}|=k-j \right)\frac{\Pr\left(\mathcal{E}_{S_{0}}^{\left(\vec{\tau}\right)}\cap \mathcal{E}_{S_{j}}^{\left(\vec{\tau}\right)}\right)}{\Pr\left(\mathcal{E}_{ S_{0}}^{\left(\vec{\tau}\right)}\right)^{2}}.\]

Lemma 7 provides a lower bound on the term \(\Pr\left(\mathcal{E}_{S_{0}}^{\left(\vec{\tau}\right)}\right)\) while Lemma 8 gives an upper bound on the term \(\Pr\left(\mathcal{E}_{S_{0}}^{\left(\vec{\tau}\right)}\cap\mathcal{E}_{S_{j} }^{\left(\vec{\tau}\right)}\right)\).

In the full proof, we then show that \(\Pr\left(|\mathrm{S}\cap\mathrm{S}^{\prime}|\geq k/d\right)\) can be bounded using the Chernoff bound (Lemma 16 in Appendix A) even if we do not deal directly with binomial random variables. This allows us to discard the indices \(j\) for which \(\Pr\left(\mathcal{E}_{S_{0}}^{\left(\vec{\tau}\right)}\cap\mathcal{E}_{S_{j} }^{\left(\vec{\tau}\right)}\right)\) is large, which leads to the result after some technical manipulations.

### Proving SLTH for structured pruning

To prove Theorem 2, we first show how to obtain the same approximation result for a single-layer CNN. Then, we iteratively apply the same argument for all layers of a larger CNN and show that the approximation error stays small.

We define the _positive_ and _negative_ parts of a tensor.

**Definition 10**.: Given a tensor \(X\in\mathbb{R}^{d_{1}\times\cdots\times d_{n}}\), the _positive_ and _negative_ parts of \(X\) are respectively defined as \(X_{\vec{i}}^{+}=X_{\vec{i}}\cdot\mathbf{1}_{X_{\vec{i}}>0}\) and \(X_{\vec{i}}^{-}=-X_{\vec{i}}\cdot\mathbf{1}_{X_{\vec{i}}<0}\), where \(\vec{i}\in[d_{1}]\times\cdots\times[d_{n}]\) points at a generic entry of \(X\).

#### Approximating a single-layer CNN

We first present a preliminary lemma that shows how to prune a single-layer convolution \(\phi\left(V\ast X\right)\) in a way that dispenses us from dealing with the ReLU \(\phi\).

**Lemma 11**.: _Let \(D,d,c,n\in\mathbb{N}\) be positive integers, \(V\in\mathbb{R}^{1\times 1\times c\times 2nc}\), and \(X\in\mathbb{R}^{D\times D\times c}\). Let \(S_{1}\in\left\{0,1\right\}^{\mathrm{size}\left(V\right)}\) be a \(2n\)-channel-blocked mask. There exists a mask \(S_{2}\in\left\{0,1\right\}^{\mathrm{size}\left(V\right)}\) which only removes filters such that, for each \((i,j,k)\in[D]\times[D]\times[2nc]\), if \(\tilde{S}=S_{1}\odot S_{2}\), then_

\[\left(\phi\Big{(}\Big{(}V\odot\tilde{S}\Big{)}\ast X\Big{)}\right)_{i,j,k}= \Big{(}\Big{(}V\odot\tilde{S}\Big{)}^{+}\ast X^{+}+\Big{(}V\odot\tilde{S} \Big{)}^{-}\ast X^{-}\Big{)}_{i,j,k}.\]Overview of the proof.: \(S_{2}\in\{0,1\}^{\text{size}(V)}\) is such that \(\tilde{V}=V\odot\tilde{S}=(V\odot S_{1})\odot S_{2}\) contains only non-negative edges going from each input channel \(t\) to the output channels \(2(t-1)n+1,\dots,(2t-1)n\), and only non-positive edges going from each input channel \(t\) to the output channels \((2t-1)n+1,\dots,2tn\). Notice that, after applying the \(2n\)-channel-blocked mask \(S_{1}\), the only possible non-zero entry of the \(k\)-th filter is located at its \(t=\left\lceil\frac{k}{2n}\right\rceil\)-th channel. Hence, for each \(k\in[2nc]\), we keep filter \(k\) if the entry of its \(t\)-th channel is non-negative and \(2t-1=\left\lceil\frac{k}{n}\right\rceil\), or it is non-positive and \(2t=\left\lceil\frac{k}{n}\right\rceil\). 

We approximate a single convolution \(K*X\) by pruning a polynomially larger neural network of the form \(U*\phi(V*X)\) exploiting only a channel-blocked mask and filter removal: this is achieved using the MRSS result (Theorem 5).

**Lemma 12** (Kernel pruning).: _Let \(D,d,c_{0},c_{1},n\in\mathbb{N}\) be positive integers, \(\varepsilon\in\left(0,\frac{1}{4}\right),M\in\mathbb{R}_{>0}\), and \(C\in\mathbb{R}_{>0}\) be a universal constant with_

\[n\geq Cd^{13}c_{1}^{6}\log^{3}\frac{d^{2}c_{1}c_{0}}{\varepsilon}.\]

_Let \(U\sim\mathcal{N}^{d\times d\times 2nc_{0}\times c_{1}}\), \(V\sim\mathcal{N}^{1\times 1\times c_{0}\times 2nc_{0}}\) and \(S\in\{0,1\}^{\text{size}(V)}\), with \(S\) being a \(2n\)-channel-blocked mask. We define \(N_{0}\left(X\right)=U*\phi\left(V*X\right)\) where \(X\in\mathbb{R}^{D\times D\times c_{0}}\), and its pruned version \(N_{0}^{(S)}\left(X\right)=U*\phi\left((V\odot S)*X\right)\). With probability \(1-\varepsilon\), for all \(K\in\mathbb{R}^{d\times d\times c_{0}\times c_{1}}\) with \(\left\|K\right\|_{1}\leq 1\), it is possible to remove filters from \(N_{0}^{(S)}\) to obtain a CNN \(\tilde{N}_{0}^{(S)}\) for which_

\[\sup_{X:\left\|X\right\|_{\text{max}}\leq M}\left\|K*X-\tilde{N}_{0}^{(S)} \left(X\right)\right\|_{\text{max}}<\varepsilon M.\]

Overview of the proof.: Exploiting Lemma 11, for each \((r,s,t_{1})\in[d]\times[d]\times[c_{1}]\), one can show that

\[\left(U*\phi\left(\left(V\odot\tilde{S}\right)*X\right)\right)_{ r,s,t_{1}} =\sum_{i,j\in[d],t_{0}\in[c_{0}]}\biggl{(}\sum_{k\in[nc_{0}]}U_{i,j, k,t_{1}}\cdot\tilde{V}_{1,1,t_{0},k}^{+}\biggr{)}\cdot X_{r-i+1,s-j+1,t_{0}}^{+}\] \[+\sum_{i,j\in[d],t_{0}\in[c_{0}]}\biggl{(}\sum_{k\in[nc_{0}]}U_{i,j,k,t_{1}}\cdot\tilde{V}_{1,1,t_{0},k}^{-}\biggr{)}\cdot X_{r-i+1,s-j+1,t_{0 }}^{-},\]

where \(\tilde{S}=S_{1}\odot S_{2}\), with \(S_{1}\) being a \(2n\)-channel-blocked mask and \(S_{2}\) being a mask that removes filters. Through a Chernoff bound, we show that \(\tilde{V}_{1,1,t_{0},:}^{+}\) has at least \(n/3\) non-zero entries. Up to reshaping the tensor as a one-dimensional vector, we observe that \(U_{:,:,k,:}\tilde{V}_{1,1,t_{0},k}^{+}\) is an NSN vector (Lemma 18 in Appendix A). Hence, we can apply a boosted version of the MRSS result (Corollary 19 in Appendix A) and show that, with high probability, for all target filters \(K\) with \(\left\|K\right\|_{1}\leq 1\), we can prune all but roughly \(\sqrt{n/(C_{1}d\log\frac{1}{\varepsilon})}\) positive entries of \(\tilde{V}_{1,1,t_{0},k}^{+}\), with \(C_{1}\) being a universal constant, such that \(\sum_{k\in[nc_{0}]}U_{:,:,k,:}\tilde{V}_{1,1,t_{0},:}^{+}\) approximates the channels \(K_{:,:,t_{0},:}\) up to error \(\varepsilon/(2d^{2}c_{0}c_{1})\). The same holds for \(\sum_{k\in[nc_{0}]}U_{:,:,k,:}\tilde{V}_{1,1,t_{0},:}^{-}\). This pruning can be achieved by applying a third mask \(S_{3}\) that only removes filters. Through some non-trivial calculations and by applying the Tensor Convolution Inequality (Lemma 20 in Appendix A), one can combine the above results to get the thesis. Notice that the overall pruning can be represented by the mask \(S_{1}\odot(S_{2}\odot S_{3})\), where \(S_{2}\odot S_{3}\) is a mask that only removes filters, and \(S=S_{1}\) is a \(2n\)-channel-blocked mask. 

_Remark 13_.: From the proofs of Lemmas 12 and 11, we can see that the overall modification yields a pruned CNN \(\hat{U}*\phi(\hat{V}*X)\) with \(\hat{V}\in\mathbb{R}^{1\times 1\times c_{0}\times 2mc_{0}}\) and \(\hat{U}\in\mathbb{R}^{d\times d\times 2mc_{0}\times c_{1}}\), where \(m=\sqrt{n/(C_{1}d\log\frac{1}{\varepsilon})}\) for a universal constant \(C_{1}\). Moreover, the kernel \(\hat{V}\) is obtained through a 3-stage pruning process: First, we apply a \(2n\)-channel-blocked mask \(S_{1}\). Second, we remove filters based on entries' signs through a mask \(S_{2}\). Third, we remove filters according to the MRSS result through a mask \(S_{3}\). Masks \(S_{2}\) and \(S_{3}\) can be combined into a single mask \(S_{2}\odot S_{3}\) that only removes filters: overall the pruning process consists of a \(2n\)-channel-blocked mask and filter removal, which justifies the statement of Theorem 2.

### Overview of the proof of Theorem 2

We iteratively apply Lemma 12 to each layer while carefully controlling the approximation error via tools such as the Lipschitz property of ReLU and the Tensor Convolution Inequality (Lemma 20). More precisely, we show that (i) the approximation error does not increase too much at each layer; and (ii) all layer approximations can be combined to approximate the entire target network. Notice that Lemma 12 guarantees that, with high probability, we can approximate all possible target filters \(K\) (with \(\left\lVert{K}\right\rVert_{1}\leq 1\)) from the same 2-layered network. Hence, we get the result for the supremum over all possible choices of target filters.

## 5 Limitations and future work

In previous works (da Cunha et al., 2022, Burkholz, 2022a) the assumption that the kernel of every second layer has shape \(1\times 1\times\dots\) is only an artifact of the proof since one can readily prune entries of an arbitrarily shaped tensor to enforce the desired shape. In our case, however, the concept of structured pruning can be quite broad, and such reshaping via pruning might not fit some sparsity patterns, depending on the context. The hypothesis on the shape can be a relevant limitation for such use cases. The constructions proposed by Burkholz (2022a,b) appear as a promising direction to overcome this limitation, with the added benefit of reducing the depth overhead.

The convolution operation commonly employed in CNNs can be cumbersome at many points of our analysis. Exploring different concepts of convolution can be an interesting path for future work as it could lead to tidier proofs and more general results. For instance, employing a 3D convolution would spare a factor \(c\) in Theorem 2.

Another limitation of our results is the restriction to ReLU as the activation function. Many previous works on the SLTH exploit the fact that ReLU satisfies the identity \(x=\phi(x)-\phi(-x)\). Burkholz (2022a) leveraged that to obtain an SLTH result for CNNs with activation functions \(f\) for which \(f(x)-f(-x)\approx x\) around the origin. Our analysis, on the other hand, does not rely on such property, so adapting the approach of (Burkholz, 2022a) to our setting is not straightforward.

Finally, we remark that the assumption of normally distributed weights might be relaxed. Borst et al. (2022) provided an MRSSP result for independent random variables whose distribution converges "fast enough" to a Gaussian one.4 We believe our arguments can serve well as baselines to generalise our results to support random weights distributed as such.

Footnote 4: The required convergence rate is higher than that ensured by the Berry-Esseen theorem.

## Acknowledgments and Disclosure of Funding

Francesco d'Amore is supported by the MUR Fare2020 Project "PAReCoDi" CUP J43C22000970001.

## References

* Zhang et al. (2017) Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning requires rethinking generalization. In _5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings_. OpenReview.net, 2017. URL https://openreview.net/forum?id=Sy8gdB9xx.
* Zhang et al. (2021) Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning (still) requires rethinking generalization. _Commun. ACM_, 64(3):107-115, 2021. doi: 10.1145/3446776. URL https://doi.org/10.1145/3446776.
* May 3, 2018, Conference Track Proceedings_. OpenReview.net, 2018. URL https://openreview.net/forum?id=HJC2Sz2CW.
* Vinyals et al. (2019)Alon Brutzkus, Amir Globerson, Eran Malach, and Shai Shalev-Shwartz. SGD learns over-parameterized networks that provably generalize on linearly separable data. In _6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings_. OpenReview.net, 2018. URL https://openreview.net/forum?id=rJ3SwwwRb.
* Du et al. (2019) Simon S. Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes over-parameterized neural networks. In _7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019_. OpenReview.net, 2019. URL https://openreview.net/forum?id=S1eK3i09YQ.
* Kaplan et al. (2020) Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. _CoRR_, abs/2001.08361, 2020. URL https://arxiv.org/abs/2001.08361.
* Reed (1993) Russell Reed. Pruning algorithms-a survey. _IEEE Trans. Neural Networks_, 4(5):740-747, 1993. doi: 10.1109/72.248452. URL https://doi.org/10.1109/72.248452.
* Blalock et al. (2020) Davis W. Blalock, Jose Javier Gonzalez Ortiz, Jonathan Frankle, and John V. Guttag. What is the state of neural network pruning? In Inderjit S. Dhillon, Dimitris S. Papailiopoulos, and Vivienne Sze, editors, _Proceedings of Machine Learning and Systems 2020, MLSys 2020, Austin, TX, USA, March 2-4, 2020_. mlsys.org, 2020. URL https://proceedings.mlsys.org/paper_files/paper/2020/hash/6c4ddc73014d66ba49b28d483a8f8b0d-Abstract.html.
* Hoefler et al. (2021) Torsten Hoefler, Dan Alistarh, Tal Ben-Nun, Nikoli Dryden, and Alexandra Peste. Sparsity in deep learning: Pruning and growth for efficient inference and training in neural networks. _J. Mach. Learn. Res._, 22:241:1-241:124, 2021. URL http://jmlr.org/papers/v22/21-0366.html.
* Frankle and Carbin (2019) Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural networks. In _7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019_. OpenReview.net, 2019. URL https://openreview.net/forum?id=rJl-b3RcF7.
* Zhou et al. (2019) Hattie Zhou, Janice Lan, Rosanne Liu, and Jason Yosinski. Deconstructing lottery tickets: Zeros, signs, and the supermask. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d'Alche-Buc, Emily B. Fox, and Roman Garnett, editors, _Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada_, pages 3592-3602, 2019. URL https://proceedings.neurips.cc/paper/2019/hash/1113d7a76ffcecalbb350bfe145467c6-Abstract.html.
* Ramanujan et al. (2020) Vivek Ramanujan, Mitchell Wortsman, Aniruddha Kembhavi, Ali Farhadi, and Mohammad Rastegari. What's hidden in a randomly weighted neural network? In _2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020_, pages 11890-11899. Computer Vision Foundation / IEEE, 2020. doi: 10.1109/CVPR42600.2020.01191. URL https://openaccess.thecvf.com/content_CVPR_2020/html/Ramanujan_Whats_Hidden_in_a_Randomly_Weighted_Neural_Network_CVPR_2020_paper.html.
* Wang et al. (2020) Yulong Wang, Xiaolu Zhang, Lingxi Xie, Jun Zhou, Hang Su, Bo Zhang, and Xiaolin Hu. Pruning from scratch. In _The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020_, pages 12273-12280. AAAI Press, 2020. URL https://aaai.org/ojs/index.php/AAAI/article/view/6910.
* Pensia et al. (2020) Ankit Pensia, Shashank Rajput, Alliot Nagle, Harit Vishwakarma, and Dimitris S. Papailiopoulos. Optimal lottery tickets via subset sum: Logarithmic over-parameterization is sufficient. In Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, _Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual_, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/1b742ae215adf18b75449c6e272fd92d-Abstract.html.
* Pensia et al. (2020)Udo W. Pooch and Al Nieder. A survey of indexing techniques for sparse matrices. _ACM Comput. Surv._, 5(2):109-133, 1973. doi: 10.1145/356616.356618. URL https://doi.org/10.1145/356616.356618.
* Polyak and Wolf (2015) Adam Polyak and Lior Wolf. Channel-level acceleration of deep face representations. _IEEE Access_, 3:2163-2175, 2015. doi: 10.1109/ACCESS.2015.2494536. URL https://doi.org/10.1109/ACCESS.2015.2494536.
* Michel et al. (2019) Paul Michel, Omer Levy, and Graham Neubig. Are sixteen heads really better than one? In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d'Alche-Buc, Emily B. Fox, and Roman Garnett, editors, _Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada_, pages 14014-14024, 2019. URL https://proceedings.neurips.cc/paper/2019/hash/2c601ad9d2ff9bc8b282670cdd54f69f-Abstract.html.
* Alvarez and Salzmann (2016) Jose M. Alvarez and Mathieu Salzmann. Learning the number of neurons in deep networks. In Daniel D. Lee, Masashi Sugiyama, Ulrike von Luxburg, Isabelle Guyon, and Roman Garnett, editors, _Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain_, pages 2262-2270, 2016. URL https://proceedings.neurips.cc/paper/2016/hash/6e7d2da6d3953058db75714ac400b584-Abstract.html.
* Liu et al. (2019) Zhuang Liu, Mingjie Sun, Tinghui Zhou, Gao Huang, and Trevor Darrell. Rethinking the value of network pruning. In _7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019_. OpenReview.net, 2019. URL https://openreview.net/forum?id=rJlnB3C5Ym.
* Anwar et al. (2017) Sajid Anwar, Kyuyeon Hwang, and Wonyong Sung. Structured pruning of deep convolutional neural networks. _ACM J. Emerg. Technol. Comput. Syst._, 13(3):32:1-32:18, 2017. doi: 10.1145/3005348. URL https://doi.org/10.1145/3005348.
* Siswanto (2021) Arlene Elizabeth Siswanto. _Block sparsity and weight initialization in neural network pruning_. PhD thesis, Massachusetts Institute of Technology, 2021.
* Malach et al. (2020) Eran Malach, Gilad Yehudai, Shai Shalev-Schwartz, and Ohad Shamir. Proving the lottery ticket hypothesis: Pruning is all you need. In _International Conference on Machine Learning_, pages 6682-6691. PMLR, 2020.
* Rahimi and Recht (2007) Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. In John C. Platt, Daphne Koller, Yoram Singer, and Sam T. Roweis, editors, _Advances in Neural Information Processing Systems 20, Proceedings of the Twenty-First Annual Conference on Neural Information Processing Systems, Vancouver, British Columbia, Canada, December 3-6, 2007_, pages 1177-1184. Curran Associates, Inc., 2007. URL https://proceedings.neurips.cc/paper/2007/hash/013a006f03dbc5392effeb8f18fd4755-Abstract.html.
* Rahimi and Recht (2008) Ali Rahimi and Benjamin Recht. Uniform approximation of functions with random bases. In _2008 46th annual allerton conference on communication, control, and computing_, pages 555-561. IEEE, 2008.
* Yehudai and Shamir (2019) Gilad Yehudai and Ohad Shamir. On the power and limitations of random features for understanding neural networks. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d'Alche-Buc, Emily B. Fox, and Roman Garnett, editors, _Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada_, pages 6594-6604, 2019. URL https://proceedings.neurips.cc/paper/2019/hash/5481b2f3a47e42f2818014b8e103b0-Abstract.html.
* Lueker (1998) George S. Lueker. Exponentially small bounds on the expected optimum of the partition and subset sum problems. _Random Structures and Algorithms_, 12:51-62, 1998.
* Da Cunha et al. (2019) Arthur Carvalho Walraven Da Cunha, Francesco d'Amore, Frederic Giroire, Hicham Lesfari, Emanuele Natale, and Laurent Viennot. Revisiting the Random Subset Sum Problem. In In Inge LiGortz, Martin Farach-Colton, Simon J. Puglisi, and Grzegorz Herman, editors, _31st Annual European Symposium on Algorithms (ESA 2023)_, volume 274 of _Leibniz International Proceedings in Informatics (LIPIcs)_, pages 37:1-37:11, Dagstuhl, Germany, 2023. Schloss Dagstuhl - Leibniz-Zentrum fur Informatik. ISBN 978-3-95977-295-2. doi: 10.4230/LIPIcs.ESA.2023.37. URL https://drops.dagstuhl.de/opus/volltexte/2023/18690.
* Borst et al. (2022) Sander Borst, Daniel Dadush, Sophie Huiberts, and Samarth Tiwari. On the integrality gap of binary integer programs with Gaussian data. _Mathematical Programming_, 2022. doi: 10.1007/s10107-022-01828-1.
* Becchetti et al. (2022) Luca Becchetti, Arthur Carvalho Walraven da Cunha, Andrea Clementi, Francesco d' Amore, Hicham Lesfari, Emanuele Natale, and Luca Trevisan. On the Multidimensional Random Subset Sum Problem. report, Inria & Universite Cote d'Azur, CNRS, I3S, Sophia Antipolis, France ; Sapienza Universita di Roma, Rome, Italy ; Universita Bocconi, Milan, Italy ; Universita di Roma Tor Vergata, Rome, Italy, 7 2022.
* Orseau et al. (2020) Laurent Orseau, Marcus Hutter, and Omar Rivasplata. Logarithmic Pruning is All You Need. In _Advances in Neural Information Processing Systems_, volume 33, pages 2925-2934. Curran Associates, Inc., 2020.
* Cunha et al. (2022) Arthur da Cunha, Emanuele Natale, and Laurent Viennot. Proving the strong lottery ticket hypothesis for convolutional neural networks. In _International Conference on Learning Representations_, 2022. URL https://openreview.net/forum?id=Vjki79-619-.
* Burkholz (2022a) Rebekka Burkholz. Convolutional and residual networks provably contain lottery tickets. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, _International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA_, volume 162 of _Proceedings of Machine Learning Research_, pages 2414-2433. PMLR, 2022a. URL https://proceedings.mlr.press/v162/burkholz22a.html.
* Burkholz (2022b) Rebekka Burkholz. Most activation functions can win the lottery without excessive depth. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, _Advances in Neural Information Processing Systems_, volume 35, pages 18707-18720. Curran Associates, Inc., 2022b. URL https://proceedings.neurips.cc/paper_files/paper/2022/file/76bf7786d311217077bc8bb021946cd9-Paper-Conference.pdf.
* Fischer and Burkholz (2021) Jonas Fischer and Rebekka Burkholz. Towards strong pruning for lottery tickets with non-zero biases. _CoRR_, abs/2110.11150, 2021. URL https://arxiv.org/abs/2110.11150.
* Ferbach et al. (2021) Damien Ferbach, Christos Tsirigtois, Gauthier Gidel, and Avishek Joey Bose. A general framework for proving the equivariant strong lottery ticket hypothesis. _CoRR_, abs/2206.04270, 2022. doi: 10.48550/arXiv.2206.04270. URL https://doi.org/10.48550/arXiv.2206.04270.
* Diffenderfer and Kailkhura (2021) James Diffenderfer and Bhavya Kailkhura. Multi-prize lottery ticket hypothesis: Finding accurate binary neural networks by pruning A randomly weighted network. In _9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021_. OpenReview.net, 2021. URL https://openreview.net/forum?id=U_mat0b9iv.
* Sreenivasan et al. (2022) Kartik Sreenivasan, Shashank Rajput, Jy-yong Sohn, and Dimitris S. Papailiopoulos. Finding nearly everything within random binary networks. In Gustau Camps-Valls, Francisco J. R. Ruiz, and Isabel Valera, editors, _International Conference on Artificial Intelligence and Statistics, AISTATS 2022, 28-30 March 2022, Virtual Event_, volume 151 of _Proceedings of Machine Learning Research_, pages 3531-3541. PMLR, 2022. URL https://proceedings.mlr.press/v151/sreenivasan22a.html.
* Mozer and Smolensky (1988) Michael Mozer and Paul Smolensky. Skeletonization: A technique for trimming the fat from a network via relevance assessment. In David S. Touretzky, editor, _Advances in Neural Information Processing Systems 1, [NIPS Conference, Denver, Colorado, USA, 1988]_, pages 107-115. Morgan Kaufmann, 1988. URL http://papers.nips.cc/paper/119-skeletonization-a-technique-for-trimming-the-fat-from-a-network-via-relevance-assessment.
* Mozer and Smolensky (1989) Michael C Mozer and Paul Smolensky. Using relevance to reduce network size automatically. _Connection Science_, 1(1):3-16, 1989.
* Mozer et al. (2021)Yang He and Lingao Xiao. Structured pruning for deep convolutional neural networks: A survey. _CoRR_, abs/2303.00566, 2023. doi: 10.48550/arXiv.2303.00566. URL https://doi.org/10.48550/arXiv.2303.00566.
* Dubhashi and Panconesi (2009) Devdatt P. Dubhashi and Alessandro Panconesi. _Concentration of Measure for the Analysis of Randomized Algorithms_. Cambridge University Press, 2009. ISBN 978-0-521-88427-3. URL http://www.cambridge.org/gb/knowledge/isbn/item2327542/.
* Laurent and Massart (2000) B. Laurent and P. Massart. Adaptive estimation of a quadratic functional by model selection. _The Annals of Statistics_, 28(5):1302-1338, 2000. doi: 10.1214/aos/1015957395.
* Doerr (2020) Benjamin Doerr. Probabilistic Tools for the Analysis of Randomized Optimization Heuristics. arXiv:1801.06733, 2020.

## Appendix A Technical tools

### Concentration inequalities

**Lemma 14** (Most-probable normal interval).: _Let \(X\) follow a zero-mean normal distribution with variance \(\phi^{2}\). For any \(z,\varepsilon\in\mathbb{R}\)_

\[\Pr\left(X\in[z-\varepsilon,z+\varepsilon]\right)\leq\Pr\left(X\in[- \varepsilon,\varepsilon]\right).\]

Proof.: Let \(\varphi(x)\) denote the probability density function of \(X\). Then,

\[\Pr\left(X\in[-\varepsilon,\varepsilon]\right)-\Pr\left(X\in[z- \varepsilon,z+\varepsilon]\right)=\int_{-\varepsilon}^{\varepsilon}\varphi(x) \,\text{d}x-\int_{z-\varepsilon}^{z+\varepsilon}\varphi(x)\,\text{d}x.\]

If \(z-\varepsilon\geq\varepsilon\) or \(z+\varepsilon\leq-\varepsilon\), the thesis is trivial as \(\varphi(|x|)\) decreases in \(x\). W.l.o.g., suppose \(z\) is positive and \(z-\varepsilon<\varepsilon\). Then, \(-\varepsilon<z-\varepsilon<\varepsilon<z+\varepsilon\). It follows that

\[\int_{-\varepsilon}^{\varepsilon}\varphi(x)\,\text{d}x-\int_{z- \varepsilon}^{z+\varepsilon}\varphi(x)\,\text{d}x =\int_{-\varepsilon}^{z-\varepsilon}\varphi(x)\,\text{d}x-\int_{ \varepsilon}^{z+\varepsilon}\varphi(x)\,\text{d}x\] \[=\int_{-\varepsilon}^{z-\varepsilon}\varphi(x)-\varphi(x+2 \varepsilon)\,\text{d}x\]

which is non-negative as \(\varphi(x)\geq\varphi(x+2\varepsilon)\) for \(x\geq-\varepsilon\). 

**Lemma 15** (Second moment method).: _If \(Z\) is a non-negative random variable then_

\[\Pr\left(Z>0\right)\geq\frac{\mathbb{E}\left[Z\right]^{2}}{\mathbb{E}\left[Z ^{2}\right]}.\]

**Lemma 16** (Chernoff-Hoeffding bounds [10]).: _Let \(X_{1},X_{2},\ldots,X_{n}\) be independent random variables such that \(\Pr\left(0\leq X_{i}\leq 1\right)=1\) for all \(i\in[n]\). Let \(X=\sum_{i=1}^{n}X_{i}\) and \(\mathbb{E}[X]=\mu\). Then, for any \(\delta\in(0,1)\) the following holds:_

1. _if_ \(\mu\leq\mu_{+}\)_, then_ \(\Pr\left(X\geq(1+\delta)\mu_{+}\right)\leq\exp\left(-\frac{\delta^{2}\mu_{+}}{ 3}\right);\)__
2. _if_ \(0\leq\mu_{-}\leq\mu\)_, then_ \(\Pr\left(X\leq(1-\delta)\mu_{-}\right)\leq\exp\left(-\frac{\delta^{2}\mu_{+}}{ 2}\right).\)__

**Lemma 17** (Corollary of [10, Lemma 1]).: _Let \(X\sim\chi_{d}^{2}\) be a chi-squared random variable with \(d\) degrees of freedom. For any \(t>0\), it holds that_

1. \(\Pr\left(X\geq d+2\sqrt{dt}+2t\right)\leq\exp\left(-t\right)\)_;_
2. \(\Pr\left(X\leq d-2\sqrt{dt}\right)\leq\exp\left(-t\right)\)_._

### Supporting results

**Lemma 18** (NSN with positive scalar).: _If a \(d\)-dimensional random vector \(Y\) is such that, for each \(i\in[d]\), \(Y_{i}=\tilde{Z}\cdot\tilde{Z}_{i}\), where \(\tilde{Z}_{1},\ldots,\tilde{Z}_{n}\) are identically distributed random variables following a standard normal distribution, \(\tilde{Z}\) is a half-normal distribution,5 and \(\tilde{Z},\tilde{Z}_{1},\ldots,\tilde{Z}_{n}\) are independent, then \(Y\) follows an NSN distribution._

Footnote 5: I.e. \(\tilde{Z}=|Z|\) where \(Z\) is a standard normal distribution.

Proof.: By Definition 4, \(Y\) is NSN if, for each \(i\in[d]\), \(Y_{i}=Z\cdot Z_{i}\) where \(Z,Z_{1},\ldots,Z_{n}\) are i.i.d. random variables following a standard normal distribution. We know that \(\tilde{Z}=|Z|\), therefore \(\tilde{Z}_{i}=\operatorname{sign}\left(Z\right)\operatorname{sign}\left(Z_{i} \right)|Z_{i}|\) for each \(i=1,\ldots,n\), where \(Z,Z_{1},\ldots,Z_{n}\) are i.i.d. standard normalrandom variables, as \(\operatorname{sign}\left(Z\right)\operatorname{sign}\left(Z_{i}\right)\) is independent of \(\operatorname{sign}\left(Z\right)\) and of \(\operatorname{sign}\left(Z\right)\operatorname{sign}\left(Z_{j}\right)\) for \(i\neq j\). Then,

\[Y_{i} =\tilde{Z}\cdot\tilde{Z}_{i}\] \[=\left|Z\right|\cdot\operatorname{sign}\left(Z\right) \operatorname{sign}\left(Z_{i}\right)\left|Z_{i}\right|\] \[=\operatorname{sign}\left(Z\right)\left|Z\right|\cdot \operatorname{sign}\left(Z_{i}\right)\left|Z_{i}\right|\] \[=Z\cdot Z_{i},\]

implying the thesis. 

**Corollary 19** (of Theorem 5).: _Let \(d,k\), and \(n\) be positive integers with \(n\geq C_{1}dk^{2}\log\left(\frac{1}{\varepsilon}\right)\) and \(k\geq C_{2}d^{3}\log\frac{d}{\varepsilon}\) for some universal constants \(C_{1},C_{2}\in\mathbb{R}_{>0}\). Let \(X_{1},\ldots,X_{n}\) be d-dimensional i.i.d. NSN random vectors. For any \(0<\varepsilon\leq\frac{1}{4}\) it holds_

\[\Pr\left(\forall\vec{z}\in\mathbb{R}^{d}:\left\|\vec{z}\right\|_{1}\leq 1, \exists S:\left|S\right|=k,\ \left\|\left(\sum_{i\in S}X_{i}\right)-\vec{z}\right\|_{\max}\leq \varepsilon\right)\geq 1-\varepsilon.\]

Proof.: Observe that the set \([-1,1]^{d}\) can be "partitioned" in \(1/\varepsilon^{d}\) many \(\infty\)-norm balls of radius \(\varepsilon\). Let \(s=\left\lceil C_{1}d\log\left(\frac{1}{\varepsilon}\right)\right\rceil\) and let us partition the \(n\) vectors \(X_{1},\ldots,X_{n}\) in \(s\) disjoint sets \(G_{1},\ldots,G_{s}\) of at least \(k^{2}\) vectors each. By Theorem 5, there is a constant \(c\in(0,1)\) such that for each group \(G_{i}\) (\(i\in[s]\))

\[\Pr\left(\exists S\subset G_{i}:\left|S\right|=k,\ \left\|\left(\sum_{i\in S}X_{i} \right)-\vec{z}\right\|_{\max}\leq\varepsilon\right)\geq c.\] (3)

It follows that

\[\Pr\left(\forall\vec{z}\in\mathbb{R}^{d}:\left\|\vec{z}\right\|_{ 1}\leq 1,\exists S:\left|S\right|=k,\ \left\|\left(\sum_{i\in S}X_{i}\right)-\vec{z}\right\|_{\max}\leq\varepsilon\right)\] \[\geq \Pr\left(\forall\vec{z}\in\mathbb{R}^{d}:\left\|\vec{z}\right\|_{ 1}\leq 1,\exists i\in[s]\,,\exists S\subset G_{i}:\left|S\right|=k,\ \left\|\left(\sum_{i\in S}X_{i}\right)-\vec{z}\right\|_{\max}\leq \varepsilon\right)\] \[= 1-\Pr\left(\exists\vec{z}\in\mathbb{R}^{d}:\left\|\vec{z}\right\| _{1}\leq 1,\forall i\in[s]\,,\forall S\subset G_{i}:\left|S\right|=k,\ \left\|\left(\sum_{i\in S}X_{i}\right)-\vec{z}\right\|_{\max}> \varepsilon\right)\] \[\geq 1-\frac{1}{\varepsilon^{d}}\Pr\left(\forall i\in[s]\,,\forall S \subset G_{i}:\left|S\right|=k,\ \left\|\left(\sum_{i\in S}X_{i}\right)-\vec{z}\right\|_{\max}> \varepsilon\right)\] (4) \[\geq 1-\frac{1}{\varepsilon^{d}}\left(1-c\right)^{\left\lceil C_{1}d \log\left(\frac{1}{\varepsilon}\right)\right\rceil},\] (5)

where Eq. 4 holds by the union bound, Eq. 5 comes from Eq. 3 and the independence of the variables across different \(G_{i}\). By choosing \(C_{1}\) large enough,

\[1-\varepsilon^{-d}\left(1-c\right)^{\left\lceil C_{1}d\log\left(\frac{1}{ \varepsilon}\right)\right\rceil}=1-2^{d\log\frac{1}{\varepsilon}-\left\lceil C _{1}d\log\left(\frac{1}{\varepsilon}\right)\right\rceil\log\frac{1}{1- \varepsilon}}\geq 1-\varepsilon.\]

**Lemma 20** (Tensor Convolution Inequality).: _Given real tensors \(K\) and \(X\) of respective sizes \(d\times d^{\prime}\times c_{0}\times c_{1}\) and \(D\times D^{\prime}\times c_{0}\), it holds_

\[\left\|K*X\right\|_{\max}\leq\left\|K\right\|_{1}\cdot\left\|X\right\|_{\max}.\]

Proof.: We have

\[\left\|K*X\right\|_{\max}\]\[\leq\max_{i,j\in[D],\ell\in[c_{1}]}\sum_{i^{\prime},j^{\prime}\in[d],k \in[c]}\left|K_{i^{\prime},j^{\prime},k,\ell}X_{i-i^{\prime}+1,j-j^{\prime}+1,k}\right|\] \[\leq\max_{i,j\in[D],\ell\in[c_{1}]}\left(\sum_{i^{\prime},j^{ \prime}\in[d],k\in[c]}\left|K_{i^{\prime},j^{\prime},k,\ell}\right|\right) \left\|X\right\|_{\max}\] \[\leq\max_{i,j\in[D],\ell\in[c_{1}]}\left\|K\right\|_{1}\cdot\left\| X\right\|_{\max}\] \[=\left\|K\right\|_{1}\cdot\left\|X\right\|_{\max}.\]

## Appendix B Omitted proofs and results

### Multidimensional Random Subset Sum for normally-scaled normal vectors

Proof of Lemma 7.: By Definition 4, the \(j\)-th entry of each vector \(X_{i}\) is \(\left(X_{i}\right)_{j}=Z_{i}\cdot Z_{i,j}\) where each \(Z_{i}\) and \(Z_{i,j}\) are i.i.d. random variables following a standard normal distribution. Let \(\mathcal{E}^{(\mathbb{T})}\) be the event that \(k\left(1-2\sqrt{c_{d}}\right)\leq\sum_{i=1}^{k}Z_{i}^{2}\leq k\left(1+2\sqrt{ c_{d}}+2c_{d}\right)\), and denote \(X=\sum_{i=1}^{k}X_{i}\). By the law of total probability, it holds that

\[\Pr\left(X\in I_{\varepsilon}\left(\vec{z}\right)\right)=\mathbb{E}_{Z_{1}, \ldots,Z_{n}}\left[\Pr\left(X\in I_{\varepsilon}\left(\vec{z}\right)\left|Z_{1 },\ldots,Z_{k}\right)\right].\]

As, conditional on \(Z_{1},\ldots,Z_{k}\), the \(d\) entries of \(X\) are independent, it follows that

\[\mathbb{E}_{Z_{1},\ldots,Z_{n}}\left[\Pr\left(X\in I_{\varepsilon} \left(\vec{z}\right)\left|Z_{1},\ldots,Z_{k}\right)\right]\right.\] \[=\mathbb{E}_{Z_{1},\ldots,Z_{n}}\left[\prod_{j=1}^{d}\Pr\left( \left(X\right)_{j}\in I_{\varepsilon}\left(z_{i}\right)\left|Z_{1},\ldots,Z_{ k}\right)\right]\] \[\geq\mathbb{E}_{Z_{1},\ldots,Z_{n}}\left[\prod_{j=1}^{d}\Pr\left( \left(X\right)_{j}\in I_{\varepsilon}\left(z_{i}\right)\left|Z_{1},\ldots,Z_{ k},\mathcal{E}^{(\mathbb{T})}\right)\right]\Pr\left(\mathcal{E}^{(\mathbb{T})} \right),\] (6)

where the inequality in Eq. 6 holds by applying again the law of total probability.

Conditional on \(Z_{1},\ldots,Z_{k}\), we have that \(\left(X\right)_{j}\sim\mathcal{N}(0,\sum_{i=1}^{k}Z_{i}^{2})\). Hence,

\[\mathbb{E}_{Z_{1},\ldots,Z_{n}}\left[\prod_{j=1}^{d}\Pr\left( \left(X\right)_{j}\in I_{\varepsilon}\left(z_{i}\right)\left|Z_{1},\ldots,Z_{ k},\mathcal{E}^{(\mathbb{T})}\right)\right]\Pr\left(\mathcal{E}^{(\mathbb{T})}\right)\] \[\geq\mathbb{E}_{Z_{1},\ldots,Z_{k}}\left[\prod_{j=1}^{d}\left( \frac{2\varepsilon}{\sqrt{\pi\left(\sum_{i=1}^{k}Z_{i}^{2}\right)}}\exp\left(- \frac{\left(\left|z_{i}\right|+\varepsilon\right)^{2}}{2\sum_{i=1}^{k}Z_{i}^{2} }\right)\right)\right]\mathcal{E}^{(\mathbb{T})}\] \[\quad\cdot\Pr\left(\mathcal{E}^{(\mathbb{T})}\right)\]

Notice that the term \(\sum_{i}Z_{i}^{2}\) is a sum of chi-square random variables, for which there are known concentration bounds (Lemma 17). By definition of \(\mathcal{E}^{(\mathbb{T})}\) and by applying Lemma 17 to estimate the term \(\Pr\left(\mathcal{E}^{(\mathbb{T})}\right)\), we get that

\[\mathbb{E}_{Z_{1},\ldots,Z_{k}}\left[\prod_{j=1}^{d}\left(\frac{2 \varepsilon}{\sqrt{\pi\left(\sum_{i=1}^{k}Z_{i}^{2}\right)}}\exp\left(-\frac{ \left(\left|z_{i}\right|+\varepsilon\right)^{2}}{2\sum_{i=1}^{k}Z_{i}^{2}} \right)\right)\right]\mathcal{E}^{(\mathbb{T})}\] \[\quad\cdot\Pr\left(\mathcal{E}^{(\mathbb{T})}\right)\]\[\geq\left(\frac{2\varepsilon}{\sqrt{\pi\left(1+2\sqrt{c_{d}}+2c_{d} \right)k}}\right)^{d}\exp\left(-\frac{\sum_{i}|z_{i}|^{2}+2\varepsilon\sum_{i} \left|z_{i}\right|+d\varepsilon^{2}}{2\left(1-2\sqrt{c_{d}}\right)k}\right) \Pr\left(\mathcal{E}^{(\mathbb{I})}\right)\] \[=\left(\frac{2\varepsilon}{\sqrt{\pi\left(1+2\sqrt{c_{d}}+2c_{d} \right)k}}\right)^{d}\exp\left(-\frac{\left\|\vec{z}\right\|_{2}^{2}+2 \varepsilon\left\|\vec{z}\right\|_{1}+d\varepsilon^{2}}{2\left(1-2\sqrt{c_{d}} \right)k}\right)\Pr\left(\mathcal{E}^{(\mathbb{I})}\right)\] \[\geq\left(\frac{2\varepsilon}{\sqrt{\pi\left(1+2\sqrt{c_{d}}+2c_{ d}\right)k}}\right)^{d}\exp\left(-\frac{\left\|\vec{z}\right\|_{2}^{2}+2 \varepsilon\left\|\vec{z}\right\|_{1}+d\varepsilon^{2}}{2\left(1-2\sqrt{c_{d} }\right)k}\right)\left(1-2e^{-c_{d}k}\right).\]

As \(c_{d}k\geq 1\) by hypotheses, \(1-2e^{-c_{d}k}\geq 1/4\). Then,

\[\left(\frac{2\varepsilon}{\sqrt{\pi\left(1+2\sqrt{c_{d}}+2c_{d} \right)k}}\right)^{d}\exp\left(-\frac{\left\|\vec{z}\right\|_{2}^{2}+2 \varepsilon\left\|\vec{z}\right\|_{1}+d\varepsilon^{2}}{2\left(1-2\sqrt{c_{d} }\right)k}\right)\left(1-2e^{-c_{d}k}\right)\] \[\geq\frac{1}{4}\left(\frac{2\varepsilon}{\sqrt{\pi\left(1+2\sqrt{ c_{d}}+2c_{d}\right)k}}\right)^{d}\exp\left(-\frac{\left\|\vec{z}\right\|_{2}^{2}+2 \varepsilon\left\|\vec{z}\right\|_{1}+d\varepsilon^{2}}{2\left(1-2\sqrt{c_{d} }\right)k}\right)\] \[\geq\frac{1}{4}\left(\frac{2\varepsilon}{\sqrt{\pi\left(1+2\sqrt{ c_{d}}+2c_{d}\right)k}}\right)^{d}\exp\left(-\frac{1+\frac{1}{8}+\frac{1}{16}}{2 \left(1-2\sqrt{c_{d}}\right)}\right)\] (8) \[\geq\frac{1}{16}\left(\frac{2\varepsilon}{\sqrt{\pi\left(1+2\sqrt {c_{d}}+2c_{d}\right)k}}\right)^{d},\] (9)

where we have used that \(\left\|\vec{z}\right\|_{2}\leq\left\|\vec{z}\right\|_{1}\leq\sqrt{k}\) in Ineq. 7, that \(k\geq 16\), \(k\geq d\), and \(\varepsilon<1/4\) in Ineq. 8, and that

\[\exp\left(-\frac{1+\frac{1}{8}+\frac{1}{16}}{2\left(1-2\sqrt{c_{d}}\right)} \right)\geq\exp\left(-\frac{1+\frac{1}{8}+\frac{1}{16}}{2\left(1-2\sqrt{\frac {1}{16}}\right)}\right)\geq\frac{1}{16}\]

in Ineq. 9. 

Proof of Lemma 8.: Let \(n=k+j\). Since the \(X_{i}\)s are NSN random vectors, for each \(i\in[n]\) and \(j\in[d]\) we can write the \(j\)-th entry of \(X_{i}\) as \((X_{i})_{j}=Z_{i}\cdot Z_{i,j}\) where the variables in \(\left\{Z_{i}\right\}_{i\in[n]}\) and in \(\left\{Z_{i,j}\right\}_{i\in[n],j\in[d]}\) are i.i.d. random variables following a standard normal distribution. By the law of total probability, we have

\[\Pr\left(A+B\in I_{\varepsilon}\left(\vec{z}\right),B+C\in I_{ \varepsilon}\left(\vec{z}\right)\right)\] \[=\mathbb{E}_{Z_{1},\ldots,Z_{n}}\left[\Pr\left(A+B\in I_{ \varepsilon}\left(\vec{z}\right),B+C\in I_{\varepsilon}\left(\vec{z}\right)|Z _{1},\ldots,Z_{n}\right)\right]\] \[=\mathbb{E}_{Z_{1},\ldots,Z_{n}}\left[\prod_{i=1}^{d}\Pr\left(A_ {i}+B_{i}\in I_{\varepsilon}\left(z_{i}\right),B_{i}+C_{i}\in I_{\varepsilon} \left(z_{i}\right)|Z_{1},\ldots,Z_{n}\right)\right],\] (10)where the latter equality holds by independence.

Then,

\[\Pr\left(A_{i}+B_{i}\in I_{\varepsilon}\left(z_{i}\right),B_{i}+C_{i }\in I_{\varepsilon}\left(z_{i}\right)|Z_{1},\ldots,Z_{n}\right)\] \[= \mathbb{E}_{B_{i}}\left[\Pr\left(A_{i}\in I_{\varepsilon}\left(z_ {i}-B_{i}\right),C_{i}\in I_{\varepsilon}\left(z_{i}-B_{i}\right)|Z_{1},\ldots, Z_{n},B_{i}\right)\right]\] \[= \mathbb{E}_{B_{i}}\left[\Pr\left(A_{i}\in I_{\varepsilon}\left(z_ {i}-B_{i}\right)|Z_{1},\ldots,Z_{n},B_{i}\right)\Pr\left(C_{i}\in I_{ \varepsilon}\left(z_{i}-B_{i}\right)|Z_{1},\ldots,Z_{n},B_{i}\right)\right],\]

where the latter inequality holds by independence of \(A_{i}\) and \(C_{i}\). By Lemma 14, it holds that

\[\mathbb{E}_{B_{i}}\left[\Pr\left(A_{i}\in I_{\varepsilon}\left(z_ {i}-B_{i}\right)|Z_{1},\ldots,Z_{n},B_{i}\right)\Pr\left(C_{i}\in I_{ \varepsilon}\left(z_{i}-B_{i}\right)|Z_{1},\ldots,Z_{n},B_{i}\right)\right]\] \[\leq \mathbb{E}_{B_{i}}\left[\Pr\left(A_{i}\in I_{\varepsilon}\left(0 \right)|Z_{1},\ldots,Z_{n},B_{i}\right)\Pr\left(C_{i}\in I_{\varepsilon}\left( 0\right)|Z_{1},\ldots,Z_{n},B_{i}\right)\right]\] \[= \Pr\left(A_{i}\in I_{\varepsilon}\left(0\right)|Z_{1},\ldots,Z_{ n}\right)\Pr\left(C_{i}\in I_{\varepsilon}\left(0\right)|Z_{1},\ldots,Z_{n}\right)\] \[\leq \frac{2\varepsilon}{\sqrt{\pi\left(\sum_{r=1}^{j}Z_{r}^{2} \right)}}\cdot\frac{2\varepsilon}{\sqrt{\pi\left(\sum_{r=k+1}^{k+j}Z_{r}^{2} \right)}},\] (11)

where the latter inequality comes from the fact that, conditional on \(Z_{1},\ldots,Z_{n}\), we have that \(A_{i}\sim\mathcal{N}(0,\sum_{r=1}^{j}Z_{r}^{2})\), \(B_{i}\sim\mathcal{N}(0,\sum_{r=j+1}^{k}Z_{r}^{2})\), and \(C_{i}\sim\mathcal{N}(0,\sum_{r=k+1}^{k+j}Z_{r}^{2})\) for each \(i\in[d]\).

We now proceed similarly to the proof of Lemma 7. We denote the event that \(\left(1-2\sqrt{c_{d}}\right)j\leq\sum_{i=1}^{j}Z_{i}^{2},\sum_{i=k+1}^{k+j}Z_{ i}^{2}\) by \(\mathcal{E}^{(\downarrow)}\). Then, by Eq. 10 and the law of total probability, we have that

\[\Pr\left(A+B\in I_{\varepsilon}\left(\vec{z}\right),B+C\in I_{ \varepsilon}\left(\vec{z}\right)\right)\] \[= \mathbb{E}_{Z_{1},\ldots,Z_{n}}\left[\prod_{i=1}^{d}\Pr\left(A_{ i}+B_{i}\in I_{\varepsilon}\left(z_{i}\right),B_{i}+C_{i}\in I_{\varepsilon} \left(z_{i}\right)|Z_{1},\ldots,Z_{n}\right)\right]\] \[\leq \mathbb{E}_{Z_{1},\ldots,Z_{n}}\left[\prod_{i=1}^{d}\Pr\left(A_{ i}+B_{i},B_{i}+C_{i}\in I_{\varepsilon}\left(z_{i}\right)\Big{|}Z_{1},\ldots,Z_{n}, \mathcal{E}^{(\downarrow)}\right)\right]+\Pr\left(\overline{\mathcal{E}^{( \downarrow)}}\right).\]

Eq. 11 implies that

\[\mathbb{E}_{Z_{1},\ldots,Z_{n}}\left[\prod_{i=1}^{d}\Pr\left(A_{ i}+B_{i},B_{i}+C_{i}\in I_{\varepsilon}\left(z_{i}\right)\Big{|}Z_{1},\ldots,Z_{n}, \mathcal{E}^{(\downarrow)}\right)\right]+\Pr\left(\overline{\mathcal{E}^{( \downarrow)}}\right)\] \[\leq \mathbb{E}_{Z_{1},\ldots,Z_{n}}\left[\prod_{i=1}^{d}\frac{2 \varepsilon}{\sqrt{\pi\left(\sum_{r=1}^{j}Z_{r}^{2}\right)}}\cdot\frac{2 \varepsilon}{\sqrt{\pi\left(\sum_{r=k+1}^{k+j}Z_{r}^{2}\right)}}\left| \mathcal{E}^{(\downarrow)}\right]+\Pr\left(\overline{\mathcal{E}^{( \downarrow)}}\right)\] \[= \mathbb{E}_{Z_{1},\ldots,Z_{n}}\left[\left(\frac{4\varepsilon^{2 }}{\pi\sqrt{\left(\sum_{r=1}^{j}Z_{r}^{2}\right)\left(\sum_{r=k+1}^{k+j}Z_{r}^ {2}\right)}}\right)^{d}\left|\mathcal{E}^{(\downarrow)}\right]+\Pr\left( \overline{\mathcal{E}^{(\downarrow)}}\right).\]

By independence of \(\sum_{r=1}^{j}Z_{r}^{2}\) and \(\sum_{r=k+1}^{k+j}Z_{r}^{2}\) and by Lemma 17, we obtain that

\[\mathbb{E}_{Z_{1},\ldots,Z_{n}}\left[\left(\frac{4\varepsilon^{2} }{\pi\sqrt{\left(\sum_{i=1}^{j}Z_{i}^{2}\right)\left(\sum_{i=k+1}^{k+j}Z_{i}^ {2}\right)}}\right)^{d}\left|\mathcal{E}^{(\downarrow)}\right]+\Pr\left( \overline{\mathcal{E}^{(\downarrow)}}\right)\] \[\leq \left(\frac{4\varepsilon^{2}}{\pi j\left(1-2\sqrt{c_{d}}\right)} \right)^{d}+\Pr\left(\overline{\mathcal{E}^{(\downarrow)}}\right)\] \[\leq \left(\frac{4\varepsilon^{2}}{\pi j\left(1-2\sqrt{c_{d}}\right)} \right)^{d}+2\exp\left(-c_{d}j\right)\]\[\frac{\Pr\left(\mathcal{E}_{S_{0}}^{(\vec{z})}\cap\mathcal{E}_{S_{j}}^{(\vec{z})} \right)}{\Pr\left(\mathcal{E}_{S_{0}}^{(\vec{z})}\right)^{2}}\leq\frac{3\left( \frac{4\varepsilon^{2}}{\pi\left(1-2\sqrt{c_{d}}\right)j}\right)^{d}}{\frac{1}{ 256}\left(\frac{4\varepsilon^{2}}{\pi\left(1+2\sqrt{c_{d}}+2c_{d}\right)k} \right)^{d}}\]\[=768\left(\frac{\left(1+2\sqrt{c_{d}}+2c_{d}\right)k}{\left(1-2 \sqrt{c_{d}}\right)j}\right)^{d}.\]

As \(j\geq k\left(1-\frac{1}{d}\right)\) with \(d>1\), then

\[768\left(\frac{\left(1+2\sqrt{c_{d}}+2c_{d}\right)k}{\left(1-2 \sqrt{c_{d}}\right)j}\right)^{d} \leq 768\left(\frac{\left(1+2\sqrt{c_{d}}+2c_{d}\right)}{\left(1-2 \sqrt{c_{d}}\right)\left(1-\frac{1}{d}\right)}\right)^{d}\] \[\leq 768\left(\frac{2+7\sqrt{c_{d}}}{2-7\sqrt{c_{d}}}\right)^{d}\] \[\leq 270801,\] (18)

because \(\left(\frac{\left(1+2\sqrt{c_{d}}+2c_{d}\right)}{\left(1-2\sqrt{c_{d}}\right) \left(1-\frac{1}{d}\right)}\right)^{d}\) is maximised for \(d=4\). Let \(C^{\prime}=270801\). If \(d>1\), by plugging Eq. 18 in Eq. 15, we have that

\[\frac{\mathbb{E}\left[T_{n,k}^{2}\right]}{\mathbb{E}\left[T_{n,k} \right]^{2}} =\sum_{j=0}^{\left\lceil k-\frac{k}{d}\right\rceil-1}\Pr\left( \left|\mathrm{S}\cap\mathrm{S}^{\prime}\right|=k-j\right)\frac{\Pr\left( \mathcal{E}_{S_{0}}^{\left(\vec{z}\right)}\cap\mathcal{E}_{S_{j}}^{\left( \vec{z}\right)}\right)}{\Pr\left(\mathcal{E}_{S_{0}}^{\left(\vec{z}\right)} \right)^{2}}\] \[\quad+\sum_{j=\left\lceil k-\frac{k}{d}\right\rceil}^{k}\Pr\left( \left|\mathrm{S}\cap\mathrm{S}^{\prime}\right|=k-j\right)\frac{\Pr\left( \mathcal{E}_{S_{0}}^{\left(\vec{z}\right)}\cap\mathcal{E}_{S_{j}}^{\left( \vec{z}\right)}\right)}{\Pr\left(\mathcal{E}_{S_{0}}^{\left(\vec{z}\right)} \right)^{2}}\] \[\leq\sum_{j=0}^{\left\lceil k-\frac{k}{d}\right\rceil-1}\Pr\left( \left|\mathrm{S}\cap\mathrm{S}^{\prime}\right|=k-j\right)\frac{\Pr\left( \mathcal{E}_{S_{0}}^{\left(\vec{z}\right)}\cap\mathcal{E}_{S_{j}}^{\left( \vec{z}\right)}\right)}{\Pr\left(\mathcal{E}_{S_{0}}^{\left(\vec{z}\right)} \right)^{2}}+C^{\prime}.\]

As \(\Pr\left(\mathcal{E}_{S_{0}}^{\left(\vec{z}\right)}\cap\mathcal{E}_{S_{j}}^{ \left(\vec{z}\right)}\right)\leq\Pr\left(\mathcal{E}_{S_{0}}^{\left(\vec{z} \right)}\right)\), then

\[\sum_{j=0}^{\left\lceil k-\frac{k}{d}\right\rceil-1}\Pr\left( \left|\mathrm{S}\cap\mathrm{S}^{\prime}\right|=k-j\right)\frac{\Pr\left( \mathcal{E}_{S_{0}}^{\left(\vec{z}\right)}\cap\mathcal{E}_{S_{j}}^{\left( \vec{z}\right)}\right)}{\Pr\left(\mathcal{E}_{S_{0}}^{\left(\vec{z}\right)} \right)^{2}}+C^{\prime}\] \[\leq\frac{1}{\Pr\left(\mathcal{E}_{S_{0}}^{\left(\vec{z}\right)} \right)}\sum_{j=0}^{\left\lceil k-\frac{k}{d}\right\rceil-1}\Pr\left(\left| \mathrm{S}\cap\mathrm{S}^{\prime}\right|=k-j\right)+C^{\prime}\] \[\leq\frac{\Pr\left(\left|\mathrm{S}\cap\mathrm{S}^{\prime} \right|>\frac{k}{d}\right)}{\Pr\left(\mathcal{E}_{S_{0}}^{\left(\vec{z}\right) }\right)}+C^{\prime}.\] (19)

Notice that, if \(d=1\), the same bound holds as \(\Pr\left(\left|\mathrm{S}\cap\mathrm{S}^{\prime}\right|>\frac{k}{d}\right)=0\). We now observe that, by the law of total probability

\[\Pr\left(\left|\mathrm{S}\cap\mathrm{S}^{\prime}\right|\geq\frac{k}{d}\right)= \sum_{\tilde{S}\in\binom{\left[n\right]}{k}}\Pr\left(\mathrm{S}=\tilde{S} \right)\Pr\left(\left|\mathrm{S}\cap\mathrm{S}^{\prime}\right|\geq\frac{k}{d} \left|\mathrm{S}=\tilde{S}\right).\] (20)

Conditional on \(\mathrm{S}=\tilde{S}\), \(\left|\mathrm{S}\cap\mathrm{S}^{\prime}\right|\) is a hypergeometric random variable with

\[\mathbb{E}\left[\left|\mathrm{S}\cap\mathrm{S}^{\prime}\right|\Big{|}\mathrm{ S}=\tilde{S}\right]=\sum_{i\in\tilde{S}}\Pr\left(i\in\mathrm{S}^{\prime}\right)=k \Pr\left(1\in\mathrm{S}\right)=\frac{k^{2}}{n}.\]Since \(n\geq k^{2}\), then \(\frac{k^{2}}{n}\leq 1\). Hence, since Chernoff bounds holds for the hypergeometric distribution (Doerr, 2020, Theorem 1.10.25)

\[\Pr\left(|\mathrm{S}\cap\mathrm{S}^{\prime}|\geq\frac{k}{d}\left| \mathrm{S}=\tilde{S}\right) \geq\Pr\left(|\mathrm{S}\cap\mathrm{S}^{\prime}|\geq\frac{k^{2}}{ n}+\left(\frac{k}{d}-1\right)\left|\mathrm{S}=\tilde{S}\right)\right.\] \[\leq\exp\left(-2\frac{\left(\frac{k}{d}-1\right)^{2}}{k}\right)\] \[\leq\exp\left(-2\frac{k}{d^{2}}\left(1-\frac{d}{k}\right)^{2} \right).\] (21)

Substituting Eq. 21 in Eq. 20 we get

\[\Pr\left(|\mathrm{S}\cap\mathrm{S}^{\prime}|\geq\frac{k}{d}\right)\leq\exp \left(-2\frac{k}{d^{2}}\left(1-\frac{d}{k}\right)^{2}\right).\] (22)

We can now keep bounding from above \(\frac{\mathbb{E}\left[T_{n,k}^{2}\right]}{\mathbb{E}\left[T_{n,k}\right]^{2}}\) by plugging Eq. 22 in Eq. 19:

\[\frac{\Pr\left(|\mathrm{S}\cap\mathrm{S}^{\prime}|\geq\frac{k}{d}\right)}{ \Pr\left(\mathcal{E}_{S_{0}}^{(\mathcal{I})}\right)}+C^{\prime}\leq\frac{ \exp\left(-2\frac{k}{d^{2}}\left(1-\frac{d}{k}\right)^{2}\right)}{\Pr\left( \mathcal{E}_{S_{0}}^{(\mathcal{I})}\right)}+C^{\prime}.\] (23)

By Lemma 7, and since \(1+2\sqrt{c_{d}}+2c_{d}\leq 2\), we have

\[\frac{\exp\left(-2\frac{k}{d^{2}}\left(1-\frac{d}{k}\right)^{2} \right)}{\Pr\left(\mathcal{E}_{S_{0}}^{(\mathcal{I})}\right)}+C^{\prime}\] \[\leq\frac{16\exp\left(-2\frac{k}{d^{2}}\left(1-\frac{d}{k}\right) ^{2}\right)}{\left(\frac{2\varepsilon}{\sqrt{\pi 2k}}\right)^{d}}+C^{\prime}\] \[=16\exp\left(-2\frac{k}{d^{2}}\left(1-\frac{d}{k}\right)^{2}+d \log\left(\frac{\sqrt{\pi 2k}}{2\varepsilon}\right)\right)+C^{\prime}.\] (24)

By the hypothesis, since \(k\geq Cd^{3}\log\frac{d}{\varepsilon}\) for a large enough \(C\), it holds that

\[16\exp\left(-2\frac{k}{d^{2}}\left(1-\frac{d}{k}\right)^{2}+d \log\left(\frac{\sqrt{\pi 2k}}{2\varepsilon}\right)\right)+C^{\prime}\] \[\leq C^{\prime}+16\exp\left(-d\log\frac{d}{\varepsilon}\right)\] \[<C^{\prime}+\frac{16}{e},\] (25)

where the latter inequality holds as \(\varepsilon\leq 1/4\).

Plugging the inverse of the expression in Eq. 25 in Eq. 12 we obtain the thesis. 

### Kernel Pruning

Proof of Lemma 11.: \(S_{2}\in\left\{0,1\right\}^{\mathrm{size}\left(V\right)}\) is such that \(\tilde{V}=V\odot\tilde{S}=\left(V\odot S_{1}\right)\odot S_{2}\) contains only non-negative edges going from each input channel \(t\) to the output channels \(2(t-1)n+1,\ldots,(2t-1)n\), and only non-positive6 edges going from each input channel \(t\) to the output channels \((2t-1)n+1,\ldots,2tn\), while all remaining edges are set to zero. Let us define some convenient notations before proceeding with the proofs. By \([n:m]\) we denote the set \(\{n,n+1,\ldots,m\}\) for each pair of integers \(n\leq m\in\mathbb{N}\). In formulas, we obtain a tensor \(\tilde{V}\) such that, for each \((t,k)\in[c]\times[2nc]\):

\[\Big{(}V\odot\tilde{S}\Big{)}_{1,1,t,k}=\begin{cases}V_{1,1,t,k}\cdot\mathbf{1 }_{V_{1,1,t,k}>0}&\text{if }k\in[(2t-2)\,n+1:(2t-1)\,n]\,,\\ V_{1,1,t,k}\cdot\mathbf{1}_{V_{1,1,t,k}<0}&\text{if }k\in[(2t-1)\,n+1:2tn]\,,\\ 0&\text{otherwise.}\end{cases}\] (26)

To simplify the notation, we define the following indicator functions: for any \((t,k)\in[c]\times[2nc]\),

\[\mathbf{1}_{\frac{k}{2n}\in\left(t-1,t-\frac{1}{2}\right]}=1\text{ iff }k\in[(2t-2)\,n+1:(2t-1)\,n]\,,\text{ and}\] \[\mathbf{1}_{\frac{k}{2n}\in\left(t-\frac{1}{2},t\right]}=1\text{ iff }k\in[(2t-1)\,n+1:2tn]\,.\] (27)

For each \((i,j,k)\in[D]\times[D]\times[2nc]\), applying Eq. 26 and using Definition 10, it then holds

\[\Big{(}\phi\left(\Big{(}V\odot\tilde{S}\Big{)}*X\right)\Big{)}_{i,j,k}\] \[=\phi\left(\sum_{t=1}^{c_{0}}\tilde{V}_{1,1,t,k}X_{i,j,t}\right)\] \[=\phi\bigg{(}\sum_{t=1}^{c_{0}}\big{(}V_{1,1,t,k}X_{i,j,t}\cdot \mathbf{1}_{V_{1,1,t,k}>0}\mathbf{1}_{\frac{k}{2n}\in\left(t-1,t-\frac{1}{2} \right]}\] \[\quad+V_{1,1,t,k}X_{i,j,t}\cdot\mathbf{1}_{V_{1,1,t,k}<0}\mathbf{ 1}_{\frac{k}{2n}\in\left(t-\frac{1}{2},t\right]}\big{)}\bigg{)}\] \[=\phi\bigg{(}\sum_{t=1}^{c_{0}}\Big{(}V_{1,1,t,k}^{+}X_{i,j,t} \mathbf{1}_{\frac{k}{2n}\in\left(t-1,t-\frac{1}{2}\right]}-V_{1,1,t,k}^{-}X_{i,j,t}\mathbf{1}_{\frac{k}{2n}\in\left(t-\frac{1}{2},t\right]}\Big{)}\bigg{)}\] \[=\phi\bigg{(}\sum_{t=1}^{c_{0}}\big{(}V_{1,1,t,k}^{+}(X_{i,j,t}^{ +}-X_{i,j,t}^{-})\mathbf{1}_{\frac{k}{2n}\in\left(t-1,t-\frac{1}{2}\right]}\] \[\quad+V_{1,1,t,k}^{-}(X_{i,j,t}^{-}-X_{i,j,t}^{+})\mathbf{1}_{ \frac{k}{2n}\in\left(t-\frac{1}{2},t\right]}\big{)}\bigg{)}.\] (28)

Observe that only one term survives in the summation in Eq. 28, as there exists only one \(t\in[c_{0}]\) such that \(k\in[(2t-2)n+1:2tn]\), say \(t^{\star}\). Moreover, out of the four additive terms in the expression

\[V_{1,1,t^{\star},k}^{+}(X_{i,j,t^{\star}}^{+}-X_{i,j,t^{\star}}^{ -})\mathbf{1}_{\frac{k}{2n}\in\left(t^{\star}-1,t^{\star}-\frac{1}{2}\right]}+V_ {1,1,t^{\star},k}^{-}(X_{i,j,t^{\star}}^{-}-X_{i,j,t^{\star}}^{+})\mathbf{1}_ {\frac{k}{2n}\in\left(t^{\star}-\frac{1}{2},t^{\star}\right]},\]

at most one is non-zero, due to Definition 10. The ReLU cancels out negative ones, implying that Eq. 28 can be rewritten without the ReLU as a sum of only non-negative terms (out of which, at most one is non-zero) as follows

\[\phi\bigg{(}\sum_{t=1}^{c_{0}}\big{(}V_{1,1,t,k}^{+}(X_{i,j,t}^{ +}-X_{i,j,t}^{-})\mathbf{1}_{\frac{k}{2n}\in\left(t-1,t-\frac{1}{2}\right]}\] \[\quad+V_{1,1,t,k}^{-}(X_{i,j,t}^{-}-X_{i,j,t}^{+})\mathbf{1}_{ \frac{k}{2n}\in\left(t-\frac{1}{2},t\right]}\big{)}\bigg{)}\] \[=\,\sum_{t=1}^{c_{0}}\Big{(}V_{1,1,t,k}^{+}X_{i,j,t}^{+}\mathbf{1 }_{\frac{k}{2n}\in\left(t-1,t-\frac{1}{2}\right]}+V_{1,1,t,k}^{-}X_{i,j,t}^{ -}\mathbf{1}_{\frac{k}{2n}\in\left(t-\frac{1}{2},t\right]}\Big{)}\,.\] (29)

Finally, by Eq. 26 and Eq. 27, \(\tilde{V}_{1,1,t,k}^{+}=0\) if \(\frac{k}{2n}\not\in\left(t-1,t-\frac{1}{2}\right]\), and \(\tilde{V}_{1,1,t,k}^{-}=0\) if \(\frac{k}{2n}\in\left(t-\frac{1}{2},t\right]\), which means that in Eq. 29 we can ignore the indicator functions and further simplify the expression as

\[\sum_{t=1}^{c_{0}}\Big{(}V_{1,1,t,k}^{+}X_{i,j,t}^{+}\mathbf{1}_{ \frac{k}{2n}\in\left(t-1,t-\frac{1}{2}\right]}+V_{1,1,t,k}^{-}X_{i,j,t}^{-} \mathbf{1}_{\frac{k}{2n}\in\left(t-\frac{1}{2},t\right]}\Big{)}\] \[=\,\sum_{t=1}^{c_{0}}\Big{(}\tilde{V}_{1,1,t,k}^{+}X_{i,j,t}^{+}+ \tilde{V}_{1,1,t,k}^{-}X_{i,j,t}^{-}\Big{)}\]Proof of Lemma 12.: Adopting the same definitions as in Lemma 11 (and Eq. 26), for each \((r,s,t_{1})\in[d]\times[d]\times[c_{1}]\) we have, by Lemma 11,

\[\left(U*\phi\left(\left(V\odot\tilde{S}\right)*X\right)\right)_{r, s,t_{1}}\] \[= \left(U*\left(\left(\tilde{V}^{+}*X^{+}\right)+\left(\tilde{V}^{ -}*X^{-}\right)\right)\right)_{r,s,t_{1}}\] \[= \sum_{i,j\in[d],k\in[2nc_{0}]}U_{i,j,k,t_{1}}\cdot\left(\left( \tilde{V}^{+}*X^{+}\right)+\left(\tilde{V}^{-}*X^{-}\right)\right)_{r-i+1,s-j+ 1,k}\] \[= \sum_{i,j\in[d],k\in[2nc_{0}]}U_{i,j,k,t_{1}}\cdot\sum_{t_{0}\in[c _{0}]}\left(\tilde{V}^{+}_{1,1,t_{0},k}\cdot X^{+}_{r-i+1,s-j+1,t_{0}}\right.\] \[\left.+\tilde{V}^{-}_{1,1,t_{0},k}\cdot X^{-}_{r-i+1,s-j+1,t_{0}}\right)\] \[= \sum_{t_{0}\in[c_{0}]}\sum_{i,j\in[d],k\in[2nc_{0}]}\left(U_{i,j, k,t_{1}}\cdot\tilde{V}^{+}_{1,1,t_{0},k}\right)\cdot X^{+}_{r-i+1,s-j+1,t_{0}}\] \[\quad+\sum_{t_{0}\in[c_{0}]}\sum_{i,j\in[d],k\in[2nc_{0}]}\left(U_ {i,j,k,t_{1}}\cdot\tilde{V}^{-}_{1,1,t_{0},k}\right)\cdot X^{-}_{r-i+1,s-j+1,t _{0}}\] \[= \sum_{i,j\in[d],t_{0}\in[c_{0}]}\left(\sum_{k\in[2nc_{0}]}U_{i,j, k,t_{1}}\cdot\tilde{V}^{+}_{1,1,t_{0},k}\right)\cdot X^{+}_{r-i+1,s-j+1,t_{0}}\] \[\quad+\sum_{i,j\in[d],t_{0}\in[c_{0}]}\left(\sum_{k\in[2nc_{0}]}U _{i,j,k,t_{1}}\cdot\tilde{V}^{-}_{1,1,t_{0},k}\right)\cdot X^{-}_{r-i+1,s-j+1, t_{0}}.\]

We remind the reader that \(\tilde{S}=S_{1}\odot S_{2}\), with \(S_{1}\) being a \(2n\)-channel-blocked mask and \(S_{2}\) being a mask that removes filters. Define \(L^{+}_{i,j,t_{0},t_{1}}=\sum_{k\in[nc]}U_{i,j,k,t_{1}}\cdot\tilde{V}^{+}_{1,1,t_{0},k}\) and, similarly, \(L^{-}_{i,j,t_{0},t_{1}}=\sum_{k\in[nc]}U_{i,j,k,t_{1}}\cdot\tilde{V}^{-}_{1,1,t_{0},k}\). Then,

\[\sum_{i,j\in[d],t_{0}\in[c_{0}]}\left(\sum_{k\in[nc_{0}]}U_{i,j,k, t_{1}}\cdot\tilde{V}^{+}_{1,1,t_{0},k}\right)\cdot X^{+}_{r-i+1,s-j+1,t_{0}}\] \[\quad+\sum_{i,j\in[d],t_{0}\in[c_{0}]}\left(\sum_{k\in[nc_{0}]}U _{i,j,k,t_{1}}\cdot\tilde{V}^{-}_{1,1,t_{0},k}\right)\cdot X^{-}_{r-i+1,s-j+1, t_{0}}\] \[= \sum_{i,j\in[d],t_{0}\in[c_{0}]}L^{+}_{i,j,t_{0},t_{1}}\cdot X^{+} _{r-i+1,s-j+1,t_{0}}+\sum_{i,j\in[d],t_{0}\in[c_{0}]}L^{-}_{i,j,t_{0},t_{1}} \cdot X^{-}_{r-i+1,s-j+1,t_{0}}.\]

We now show that, for each \(t_{0}\in[c_{0}]\), \(K_{:,:,t_{0},:}\) can be \(\varepsilon\)-approximated by \(L^{+}_{::,t_{0},:}\) by suitably pruning \(\tilde{V}^{+}\), i.e. by further zeroing entries of \(\tilde{S}\), and that such pruning corresponds to solving an instance of MRSS according to Theorem 5. The same reasoning applies to \(K^{-}\) and \(L^{-}\).

For each \(t_{0}\in[c_{0}]\), let

\[I^{(t_{0})}_{+}=\left\{k\in\left\{\left(2t_{0}-2\right)n+1,\ldots,\left(2t_{0}- 1\right)n\right\}:\tilde{S}_{1,1,t_{0},k}=1\right\}.\]Observe that \(I_{+}^{(t_{0})}\) consists of the strictly positive entries of \(\tilde{V}_{1,1,t_{0},:}^{+}\).7 Since the entries of \(V\) follow a standard normal distribution, each entry is positive with probability \(1/2\). By a standard application of Chernoff bounds (Lemma 16 in Appendix A), we then have

Footnote 7: Notice that excluding zero entries implies conditioning on the event that the entry is not zero. However, such an event has zero probability and, thus, does not impact the analysis.

\[\Pr\left(\left|I_{+}^{(t_{0})}\right|>\frac{n}{3}\right)\geq 1-\frac{\varepsilon}{4},\] (30)

provided that the constant \(C\) in the bound on \(n\) is sufficiently large.

For each \(k\in I_{+}^{(t_{0})}\), up to reshaping the tensor as a one-dimensional vector, \(U_{:,:,k,:}\cdot\tilde{V}_{1,1,t_{0},k}^{+}\) is an NSN vector (Definition 4) by Lemma 18 (Appendix B). Thus, for each \(t_{0}\in[c_{0}]\) and a sufficiently-large value of \(C\), since we have \(n\geq Cd^{13}c_{1}^{6}\log^{3}\frac{d^{2}c_{1}c_{2}}{\varepsilon}\), we can apply an amplified version of Theorem 5 (i.e. Corollary 19 in Appendix B with vectors of dimension \(d^{2}c_{1}\)) to show that, with probability \(1-\frac{\varepsilon}{4c_{0}}\), for all target filter \(K\) such that \(\|K_{:,:,t_{0},:}\|_{1}\leq 1\), there exists a way to zero the entries indexed by \(I_{+}^{(t_{0})}\) of \(\tilde{S}\) (and thus \(\tilde{V}_{1,1,t_{0},:}^{+}\)), so that the pruned version of \(L_{:,:,t_{0},:}^{+}=\sum_{k\in[nc_{0}]}U_{:,:,k,:}\cdot\tilde{V}_{1,1,t_{0},k}^ {+}\) approximates \(K_{:,:,t_{0},:}\). In particular, there exists another binary mask \(S_{3}^{+}\in\{0,1\}^{\mathrm{size}\,S}\) such that \(\hat{L}_{:,:,:,t_{0},:}^{+}=\sum_{k\in[nc_{0}]}U_{:,:,k,:}\cdot\hat{V}_{1,1,t_{ 0},k}^{+}\) approximates \(K_{:,:,t_{0},:}\), where \(\hat{V}^{+}=\hat{V}^{+}\odot S_{3}^{+}\). An analogous argument carries on for a binary mask \(S_{3}^{-}\) and \(-\hat{L}_{:,:,t_{0},:}^{-}\).8 More formally, let

Footnote 8: The negative sign in front of \(\hat{L}_{:,:,t_{0},:}^{-}\) does not affect the random subset sum result as each entry is independently negative or positive with the same probability.

\[\mathcal{E}_{t_{0},+}^{(\mathrm{kernel})} =\left\{\forall K:\left\|K\right\|_{1}\leq 1,\exists S_{3}^{+} \in\{0,1\}^{\mathrm{size}\,S}\,\left|\,\left\|\hat{L}_{:,:,t_{0},:}^{+}-K_{:,:,t_{0},:}^{-}\right.\right\|_{\mathrm{max}}\leq\frac{\varepsilon}{2d^{2}c_{1} c_{0}}\right\},\] \[\mathcal{E}_{t_{0},-}^{(\mathrm{kernel})} =\left\{\forall K:\left\|K\right\|_{1}\leq 1,\exists S_{3}^{-} \in\{0,1\}^{\mathrm{size}\,S}\,\left|\,\left\|\hat{L}_{:,:,t_{0},:}^{-}+K_{:,:,:,t_{0},:}^{-}\right.\right\|_{\mathrm{max}}\leq\frac{\varepsilon}{2d^{2}c_{1 }c_{0}}\right\},\text{ and}\] \[\mathcal{E}^{(\mathrm{kernel})} =\left(\bigcap_{t_{0}\in[c_{0}]}\mathcal{E}_{t_{0},+}^{(\mathrm{ kernel})}\right)\bigcap\left(\bigcap_{t_{0}\in[c_{0}]}\mathcal{E}_{t_{0},-}^{( \mathrm{kernel})}\right).\]

Then, by Corollary 19,

\[\Pr\left(\mathcal{E}_{t_{0},+}^{(\mathrm{kernel})}\,\left|\, \left|I_{+}^{(t_{0})}\right|>\frac{n}{3}\right) \geq 1-\frac{\varepsilon}{4c_{0}},\text{ and}\] \[\Pr\left(\mathcal{E}_{t_{0},-}^{(\mathrm{kernel})}\,\left|\, \left|I_{-}^{(t_{0})}\right|>\frac{n}{3}\right) \geq 1-\frac{\varepsilon}{4c_{0}}.\]

By the union bound, we have the following:

\[\Pr\left(\mathcal{E}^{(\mathrm{kernel})}\,\left|\,\left|I_{+}^{ (t_{0})}\right|,\left|I_{-}^{(t_{0})}\right|>\frac{n}{3}\right)\right.\] \[=1-\Pr\left(\left(\bigcup_{t_{0}\in[c_{0}]}\overline{\mathcal{E} }_{t_{0},+}^{(\mathrm{kernel})}\right)\bigcup\left(\bigcup_{t_{0}\in[c_{0}]} \overline{\mathcal{E}}_{t_{0},-}^{(\mathrm{kernel})}\right)\,\left|\left|I_ {+}^{(t_{0})}\right|,\left|I_{-}^{(t_{0})}\right|>\frac{n}{3}\right)\] \[\geq 1-\sum_{t_{0}\in[c_{0}]}\left[\Pr\left(\overline{\mathcal{E} }_{t_{0},+}^{(\mathrm{kernel})}\,\left|\,\left|I_{+}^{(t_{0})}\right|,\left|I_ {-}^{(t_{0})}\right|>\frac{n}{3}\right)+\Pr\left(\overline{\mathcal{E}}_{t_ {0},-}^{(\mathrm{kernel})}\,\left|\,\left|I_{+}^{(t_{0})}\right|,\left|I_ {-}^{(t_{0})}\right|>\frac{n}{3}\right)\right]\] \[\geq 1-2\sum_{t_{0}\in[c_{0}]}\frac{\varepsilon}{4c_{0}}\] \[\geq 1-\frac{\varepsilon}{2}.\]

Since \(\Pr\left(\left|I_{+}^{(t_{0})}\right|,\left|I_{-}^{(t_{0})}\right|>\frac{n}{3} \right)\geq 1-\frac{\varepsilon}{2}\), then we can remove the conditional event obtaining

\[\Pr\left(\mathcal{E}^{(\mathrm{kernel})}\right)\geq\Pr\left(\mathcal{E}^{( \mathrm{kernel})}\,\left|\,\left|I_{+}^{(t_{0})}\right|,\left|I_{-}^{(t_{0})} \right|>\frac{n}{3}\right)\Pr\left(\left|I_{+}^{(t_{0})}\right|,\left|I_{-}^{(t _{0})}\right|>\frac{n}{3}\right)\]\[\geq\left(1-\frac{\varepsilon}{2}\right)^{2}\] \[\geq 1-\varepsilon.\] (31)

To rewrite the latter in terms of the filter \(K\) and a mask \(\hat{S}\), we notice that pruning \(L^{+}_{::,:,t_{0},:}\) and \(L^{-}_{::,:,t_{0},:}\) separately, with two binary masks, is equivalent to say that there exists a single binary mask \(S_{3}\in\{0,1\}^{\mathrm{size}\,\hat{S}}\) such that, \(\hat{L}_{::,:,t_{0},:}\) can be written as \(\hat{L}_{::,:,t_{0},:}=\sum_{k\in[nc_{0}]}U_{::,:,k,:}\cdot\hat{V}_{1,1,t_{0},k}\), where \(\hat{V}=\hat{V}\odot S_{3}\). Ineq. 31 implies that, with probability \(1-\varepsilon\), for all target filters \(K\) with \(\left\|K\right\|_{1}\leq 1\) such \(S_{3}\) exists and hence,

\[\left\|K-\hat{L}^{+}\right\|_{\max}+\left\|K+\hat{L}^{-}\right\|_{\max}\leq \frac{\varepsilon}{d^{2}c_{1}c_{0}}.\] (32)

Let \(\hat{S}=\tilde{S}\odot S_{3}\): then \(\hat{S}=S_{1}\odot S_{2}\odot S_{3}\) is the combination of a \(2n\)-channel-blocked mask \(S=S_{1}\) and a mask \(S_{2}\odot S_{3}\) that only removes filters.

Also, whenever such binary mask \(\hat{S}\) exists, we have that

\[\sup_{X:\left\|X\right\|_{\max}\leq M}\left\|K*X-N_{0}^{\left( \hat{S}\right)}\left(X\right)\right\|_{\max}\] \[=\sup_{X:\left\|X\right\|_{\max}\leq M}\left\|K*X-U*\phi\left((V \odot\tilde{S})*X\right)\right\|_{\max}\] \[=\sup_{X:\left\|X\right\|_{\max}\leq M}\left\|K*X-U*\phi\left((V \odot\tilde{S}\odot S_{3})*X\right)\right\|_{\max}\] \[=\sup_{X:\left\|X\right\|_{\max}\leq M}\left\|K*\left(X^{+}-X^{-} \right)-U*\left(\left(\hat{V}^{+}*X^{+}\right)+\left(\hat{V}^{-}*X^{-}\right) \right)\right\|_{\max},\]

where the latter holds by Lemma 11.9 Then, by the distributive property of the convolution and the triangle inequality,

Footnote 9: The presence of \(S_{3}\) does not influence the proof of Lemma 11.

\[\sup_{X:\left\|X\right\|_{\max}\leq M}\left\|K*\left(X^{+}-X^{-} \right)-U*\left(\left(\hat{V}^{+}*X^{+}\right)+\left(\hat{V}^{-}*X^{-}\right) \right)\right\|_{\max}\] \[=\sup_{X:\left\|X\right\|_{\max}\leq M}\left\|K*X^{+}-U*\left( \hat{V}^{+}*X^{+}\right)-K*X^{-}-U*\left(\hat{V}^{-}*X^{-}\right)\right\|_{\max}\] \[\leq\sup_{X:\left\|X\right\|_{\max}\leq M}\left\|K*X^{+}-U*\left( \hat{V}^{+}*X^{+}\right)\right\|_{\max}\] \[\quad+\sup_{X:\left\|X\right\|_{\max}\leq M}\left\|K*X^{-}+U* \left(\hat{V}^{-}*X^{-}\right)\right\|_{\max}.\]

One can now apply the Tensor Convolution Inequality (Lemma 20) and obtain

\[\sup_{X:\left\|X\right\|_{\max}\leq M}\left\|K*X^{+}-U*\left( \hat{V}^{+}*X^{+}\right)\right\|_{\max}\] \[\leq\sup_{X:\left\|X\right\|_{\max}\leq M}\left\|X^{+}\right\|_{ \max}\cdot\left\|K-U*\hat{V}^{+}\right\|_{1}\] \[\quad+\sup_{X:\left\|X\right\|_{\max}\leq M}\left\|X^{-}\right\|_{ \max}\cdot\left\|K+U*\hat{V}^{-}\right\|_{1}\] \[=M\cdot\left\|K-U*\hat{V}^{+}\right\|_{1}+M\cdot\left\|K+U*\hat{V} ^{-}\right\|_{1}.\]

Now, observing that the number of entries of the two tensors in the expression above is \(d^{2}c_{1}c_{0}\), and using Ineq. 32, we get that

\[M\cdot\left\|K-U*\hat{V}^{+}\right\|_{1}+M\cdot\left\|K-U*\hat{V}^{-}\right\|_{1}\]\[\left\|\phi\left(K^{(i)}\ast X^{(i-1)}\right)-\phi\left(L^{(2i)} \ast\phi\left(\tilde{L}^{(2i-1)}\ast X^{(i-1)}\right)\right)\right\|_{\max}\] \[<\frac{\varepsilon}{2\ell}\cdot\left\|\tilde{X}^{(i-1)}\right\|_{ \max}.\] (37)

By a union bound, with probability at least \(1-\varepsilon\), we get that Eq. 37 holds for all layers \(i\) and all choices of target filters \(K^{(i)}\in\mathbb{R}^{d_{i}\times d_{i}\times c_{i-1}\times c_{i}}\).

Analogously, we can define the pruned layers' outputs

\[\tilde{X}^{(0)} =X,\] \[\tilde{X}^{(i)} =\phi\left(L^{(2i)}\ast\phi\left(\tilde{L}^{(2i-1)}\ast\tilde{X}^ {(i-1)}\right)\right)\quad\text{for }1\leq i\leq\ell.\] (38)

Notice that \(\tilde{X}^{(\ell)}\) is the output of the pruned network, i.e. \(N_{0}^{\left(S^{(1)},\ldots,S^{(2\ell)}\right)}\left(X\right)=\tilde{X}^{( \ell)}\).

By the same reasoning employed to derive Eq. 36 and Eq. 37 we have that, with probability \(1-\varepsilon\), for all layers \(i\) and all choices of target filters \(K^{(i)}\in\mathbb{R}^{d_{i}\times d_{i}\times c_{i-1}\times c_{i}}\), the output of all pruned layers satisfies

\[\left\|\phi\left(K^{(i)}\ast\tilde{X}^{(i-1)}\right)-\phi\left( L^{(2i)}\ast\phi\left(\tilde{L}^{(2i-1)}\ast\tilde{X}^{(i-1)}\right)\right) \right\|_{\max}\] \[<\frac{\varepsilon}{2\ell}\cdot\left\|\tilde{X}^{(i-1)}\right\|_ {\max}.\] (39)

[MISSING_PAGE_EMPTY:28]

\[\leq\varepsilon.\]

Hence,

\[\left\|X^{(\ell)}-\tilde{X}^{(\ell)}\right\|_{\max}\leq\varepsilon,\]

yielding the thesis.