# Provably and Practically Efficient Adversarial

Imitation Learning with General Function Approximation

Tian Xu\({}^{*}\)

Equal contribution. Emails: xut@lamda.nju.edu.cn and zhangzl@lamda.nju.edu.cn.

Zhilong Zhang\({}^{*}\)

Jun-Yan Liu\({}^{*}\)

\({}^{1}\)National Key Laboratory for Novel Software Technology, Nanjing University, China

\({}^{2}\)School of Artificial Intelligence, Nanjing University, China

\({}^{3}\)Polixir.ai

###### Abstract

As a prominent category of imitation learning methods, adversarial imitation learning (AIL) has garnered significant practical success powered by neural network approximation. However, existing theoretical studies on AIL are primarily limited to simplified scenarios such as tabular and linear function approximation and involve complex algorithmic designs that hinder practical implementation, highlighting a gap between theory and practice. In this paper, we explore the theoretical underpinnings of online AIL with general function approximation. We introduce a new method called optimization-based AIL (OPT-AIL), which centers on performing online optimization for reward functions and optimism-regularized Bellman error minimization for Q-value functions. Theoretically, we prove that OPT-AIL achieves polynomial expert sample complexity and interaction complexity for learning near-expert policies. To our best knowledge, OPT-AIL is the first provably efficient AIL method with general function approximation. Practically, OPT-AIL only requires the approximate optimization of two objectives, thereby facilitating practical implementation. Empirical studies demonstrate that OPT-AIL outperforms previous state-of-the-art deep AIL methods in several challenging tasks. 1

Footnote 1: The code is available at [https://github.com/LAMDA-RL/OPT-AIL](https://github.com/LAMDA-RL/OPT-AIL).

## 1 Introduction

Sequential decision-making tasks are prevalent in real-world applications, where agents seek policies that maximize long-term returns. Reinforcement learning (RL) [49] provides a well-known framework for developing effective policies through trial and error. However, RL often necessitates carefully designed reward functions and typically requires millions of interactions with the environment to achieve satisfactory performance [36, 20]. In contrast, imitation learning (IL) offers a more sample-efficient approach to learning effective policies by mimicking expert demonstrations, bypassing the need for explicit reward functions. As a result, IL has gained popularity and demonstrated success in a wide range of real-world applications such as recommendation systems [11, 46] and generalist robot learning [10, 35].

IL encompasses two main categories of methods: behavioral cloning (BC) and adversarial imitation learning (AIL). BC employs supervised learning to directly infer expert policies from demonstrationdata [39, 44, 9]. In contrast, AIL utilizes an adversarial learning process to replicate the expert's state-action distribution. This process involves the learner recovering an adversarial reward to maximize the policy value gap and subsequently learning a policy that minimizes this gap under the recovered reward. Building on these foundational principles, numerous practical algorithms have been developed [55, 27, 9, 22, 26, 16, 15, 34, 30], achieving significant empirical advancements.

From these empirical advances, a notable observation is that AIL often significantly outperforms BC [16, 26, 27, 15]. To better understand this phenomenon, recent research has focused on the theoretical underpinnings of AIL [61, 45, 63, 33, 64, 57], particularly in the online setting. This research examines both _expert sample complexity_ (the number of expert trajectories required) and _interaction complexity_ (the number of trajectories needed when interacting with the environment), both of which are crucial for practical applications. In the tabular setting, the best-known complexity result is achieved in [64]. They developed the MB-TAIL algorithm, which leverages advanced distribution estimation, achieving the expert sample complexity \(\widetilde{\mathcal{O}}(H^{3/2}|\mathcal{S}|/\varepsilon)\) and interaction complexity \(\widetilde{\mathcal{O}}(H^{3}|\mathcal{S}|^{2}|\mathcal{A}|/\varepsilon^{2})\), where \(|\mathcal{S}|\) and \(|\mathcal{A}|\) are the state space size and action space size, respectively, \(H\) is the horizon length and \(\varepsilon\) is the desired value gap. Furthermore, [33, 57] investigated the AIL theory in the linear function approximation setting. Notably, the BRIG approach proposed in [57] uses linear regression for policy evaluation and achieves the expert sample complexity \(\widetilde{\mathcal{O}}(H^{2}d/\varepsilon^{2})\) and interaction complexity \(\widetilde{\mathcal{O}}(H^{4}d^{3}/\varepsilon^{2})\), where \(d\) is the feature dimension. For a complete summary of related results, please refer to Table 1.

Despite substantial theoretical advances, there still exists a gap between theory and practice in AIL. First, prior theoretical analysis primarily focuses on restricted settings such as tabular [41, 45, 64] or linear function approximation [33, 57], which deviate from practice where AIL approaches often operate with general function approximation (e.g., neural network approximation). Besides, most previous theoretical works involve algorithmic designs such as count-based [45, 64] or covariance-matrix-based [33, 57] bonuses, which are tailored to their respective settings. Implementing such algorithmic designs in practical settings, where neural network approximation is employed, presents significant challenges [65, 54].

**Contribution.** This paper aims to bridge the gap between theory and practice in AIL by developing a provably efficient algorithm with general function approximation and providing a practical implementation equipped with neural networks.

First, we introduce a new AIL approach called optimization-based adversarial imitation learning (OPT-AIL) and provide a comprehensive theoretical analysis for general function approximation. The core of OPT-AIL involves minimizing two key objectives. To recover the reward, OPT-AIL solves an online optimization problem using a no-regret approach. For policy learning, inspired by [32], OPT-AIL infers the Q-value functions by minimizing the optimism-regularized Bellman

\begin{table}
\begin{tabular}{c|c|c|c} \hline \hline Setting & Algorithm & Expert Sample & Interaction \\  & & Complexity & Complexity \\ \hline General Function & BC [13]3  & \(\widetilde{\mathcal{O}}\left(\frac{H^{3}\log(\max_{\varepsilon\in[\Pi]}| \mathcal{H}_{\mathrm{A}}|)}{\varepsilon^{2}}\right)\) & \(0\) \\  Tabular MDPs & OAL [45] & \(\widetilde{\mathcal{O}}\left(\frac{H^{3}|\mathcal{S}|}{\varepsilon^{2}}\right)\) & \(\widetilde{\mathcal{O}}\left(\frac{H^{4}|\mathcal{S}|^{2}|\mathcal{A}|}{ \varepsilon^{2}}\right)\) \\  Tabular MDPs & MB-TAIL [64] & \(\widetilde{\mathcal{O}}\left(\frac{H^{3}|\mathcal{S}|}{\varepsilon^{2}}\right)\) & \(\widetilde{\mathcal{O}}\left(\frac{H^{3}|\mathcal{S}|^{2}|\mathcal{A}|}{ \varepsilon^{2}}\right)\) \\  Linear Mixture MDPs & OGAIL [33] & \(\widetilde{\mathcal{O}}\left(\frac{H^{3}d^{2}}{\varepsilon^{2}}\right)\) & \(\widetilde{\mathcal{O}}\left(\frac{H^{3}d^{2}}{\varepsilon^{2}}\right)\) \\  Linear MDPs & BRIG [57] & \(\widetilde{\mathcal{O}}\left(\frac{H^{3}d}{\varepsilon^{2}}\right)\) & \(\widetilde{\mathcal{O}}\left(\frac{H^{3}d^{2}}{\varepsilon^{2}}\right)\) \\ \hline General Function & OPT-AIL & \(\widetilde{\mathcal{O}}\left(\frac{H^{2}\log(\max_{\varepsilon\in[\Pi]}| \mathcal{N}(\mathcal{R}_{\mathrm{A}}))}{\varepsilon^{2}}\right)\) & \(\widetilde{\mathcal{O}}\left(\frac{H^{4}d_{\mathrm{GEC}}\log(\max_{\varepsilon \in[\Pi]}|\mathcal{N}(\mathcal{Q}_{\mathrm{A}})\mathcal{N}(\mathcal{R}_{ \mathrm{A}}))+H^{2}}{\varepsilon^{2}}\right)\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: A summary of the expert sample complexity and interaction complexity. Here \(H\) is the horizon length, \(\varepsilon\) is the desired imitation gap, \(|\mathcal{S}|\) is the state space size, \(|\mathcal{A}|\) is the action space size, \(|\Pi|\) is the cardinality of the finite policy class \(\Pi\), \(d\) is the dimension of the feature space, \(d_{\mathrm{GEC}}\) is the generalized eluder coefficient, \(\mathcal{N}(\mathcal{R}_{h})\) and \(\mathcal{N}(\mathcal{Q}_{h})\) are the covering numbers of the reward class \(\mathcal{R}_{h}\) and Q-value class \(\mathcal{Q}_{h}\), respectively. We use \(\widetilde{\mathcal{O}}\) to hide logarithmic factors.4error and then derives the corresponding greedy policies. Under mild assumptions, we prove that OPT-AIL achieves the expert sample complexity \(\widetilde{\mathcal{O}}(H^{2}\log(\max_{h\in[H]}\mathcal{N}(\mathcal{R}_{h}))/ \varepsilon^{2})\) and interaction complexity \(\widetilde{\mathcal{O}}((H^{4}d_{\text{GEC}}\log(\max_{h\in[H]}\mathcal{N}( \mathcal{Q}_{h})\mathcal{N}(\mathcal{R}_{h}))+H^{2})/\varepsilon^{2})\). Here \(d_{\text{GEC}}\) is the generalized eluder coefficient, originally proposed in [68] to measure the complexity of RL with function approximation, which we adapt to the AIL setting. \(\mathcal{N}(\mathcal{R}_{h})\) and \(\mathcal{N}(\mathcal{Q}_{h})\) are the covering numbers of the reward class \(\mathcal{R}_{h}\) and Q-value class \(\mathcal{Q}_{h}\), respectively. To our best knowledge, OPT-AIL is the first provably efficient AIL approach with general function approximation.

Furthermore, we offer a practical implementation of OPT-AIL, demonstrating its competitive performance on standard benchmarks. Notably, OPT-AIL only requires the approximate optimization of two objectives, simplifying its practical implementation with deep neural networks. Leveraging this advantage, we implement OPT-AIL using neural network approximations and compare its performance against prior state-of-the-art (SOTA) deep AIL methods, which often lack theoretical guarantees. Experimental results indicate that OPT-AIL outperforms SOTA deep AIL approaches on several challenging tasks within the DMControl benchmark.

## 2 Related Works

Adversarial Imitation Learning.The theoretical foundations of AIL have been extensively explored in numerous studies [1; 52; 48; 60; 67; 41; 45; 33; 40; 63; 50; 56; 64; 57]. Early research [1; 52; 48; 41; 60; 67; 62; 50; 56] focused on ideal settings where the transition function is known or an exploratory data distribution is available, primarily addressing expert sample efficiency. Notably, under mild conditions, [63] proved that AIL can achieve a horizon-free imitation gap bound \(\mathcal{O}(\min\{1,\sqrt{|\mathcal{S}|/N}\})\), where \(N\) denotes the number of expert trajectories. Recently, a new research direction has emerged that addresses more practical scenarios, specifically online AIL with unknown transitions [45; 33; 64; 57]. This line of work investigates both expert sample complexity and interaction complexity. These recent advancements were discussed in the previous section and thus will not be reiterated here. Most existing theoretical works focus on either tabular [41; 45; 64] or linear function approximation settings [33; 57], and often lack practical implementations due to algorithmic designs tailored to specific settings. In contrast, this work simultaneously provides theoretical guarantees for general function approximation and offers a practical implementation that demonstrates competitive performance.

On the empirical side, there has been extensive research [19; 27; 28; 16; 26; 15] developing practical AIL approaches that leverage general function (or neural network) approximation. A seminal method in this field is generative adversarial imitation learning (GAIL) [19]. In GAIL, a discriminator is trained to distinguish between samples from expert demonstrations and those generated by a policy, while the policy (or generator) learns to maximize the reward signal provided by the discriminator. More recently, [15] proposed inverse Q-Learning (IQLearn), which achieves SOTA performance across a diverse set of tasks. However, these practical methods often lack rigorous theoretical guarantees for general function approximation.

General Function Approximation in Reinforcement Learning.Our work is closely related to a body of research focused on general function approximation in RL [38; 21; 59; 23; 32]. Notably, [32] proposed an algorithmic framework that incorporates a unified objective to balance exploration and exploitation in RL, demonstrating a sublinear regret bound. In this paper, we adapt this algorithmic design to address several RL sub-problems within the context of AIL. Unlike the RL setting, where a fixed reward is provided in advance, AIL involves inferring the reward function from expert demonstrations and environment interactions collected by the learning policies. Therefore, our work requires developing a theoretical analysis for the joint learning process of both rewards and policies, highlighting a unique challenge in AIL compared to traditional RL.

## 3 Preliminaries

Markov Decision Process.In this paper, we consider episodic Markov Decision Processes (MDPs), represented by the tuple \(\mathcal{M}=(\mathcal{S},\mathcal{A},P,r^{\mathrm{true}},H,s_{1})\). Here, \(\mathcal{S}\) and \(\mathcal{A}\) denote the state and action spaces, respectively. \(H\) signifies the planning horizon, while \(s_{1}\) stands for the fixed initial state. The set \(P=\{P_{1},\ldots,P_{H}\}\) characterizes the non-stationary transition function of this MDP. Specifically, \(P_{h}(s_{h+1}|s_{h},a_{h})\) determines the probability of transiting to state \(s_{h+1}\) given state \(s_{h}\) and action \(a_{h}\) at time step \(h\), where \(h\in[H]\). Similarly, \(r^{\mathrm{true}}=\{r^{\mathrm{true}}_{h},\ldots,r^{\mathrm{true}}_{H}\}\) outlines the reward function of this MDP. Without loss of generality, we assume \(r^{\mathrm{true}}_{h}:\mathcal{S}\times\mathcal{A}\rightarrow[0,1]\) for \(h\in[H]\). A non-stationarypolicy is denoted by \(\pi=\{\pi_{1},\ldots,\pi_{H}\}\) with \(\pi_{h}:\mathcal{S}\rightarrow\Delta(\mathcal{A})\), where \(\Delta(\mathcal{A})\) denotes the probability simplex. Here, \(\pi_{h}(a|s)\) represents the probability of selecting action \(a\) in state \(s\) at time step \(h\), for \(h\in[H]\).

The quality of policy \(\pi\) is evaluated by policy value:

\[V^{\pi}=\mathbb{E}\left[\sum_{h=1}^{H}r_{h}^{\mathrm{true}}(s_{h},a_{h})\bigg{|} a_{h}\sim\pi_{h}(\cdot|s_{h}),s_{h+1}\sim P_{h}(\cdot|s_{h},a_{h}),\forall h \in[H]\right].\]

We denote the Q-value function of policy \(\pi\) at time step \(h\) as \(Q_{h}^{\pi}:\mathcal{S}\times\mathcal{A}\rightarrow\mathbb{R}\), where \(Q_{h}^{\pi}(s,a)=\mathbb{E}_{\pi}[\sum_{\ell=h}^{H}r_{\ell}^{\mathrm{true}}(s_ {\ell},a_{\ell})|s_{h}=s,a_{h}=a]\). The optimal Q-value function \(Q_{h}^{\star}:\mathcal{S}\times\mathcal{A}\rightarrow\mathbb{R}\) is defined as \(Q_{h}^{\star}(s,a):=\sup_{\pi\in\Pi}Q_{h}^{\pi}(s,a)\). It is known that \(Q_{h}^{\star}\) is the fixed point of Bellman operator \(\mathcal{T}_{h}\): \(Q_{h}^{\star}(s,a)=(\mathcal{T}_{h}Q_{h+1}^{\star})(s,a):=r_{h}^{\mathrm{true} }(s,a)+\mathbb{E}_{s^{\prime}\sim P_{h}(\cdot|s,a)}[\max_{a^{\prime}\in\mathcal{ A}}Q_{h+1}^{\star}(s^{\prime},a^{\prime})]\). In other words, \(Q^{\star}\) has zero Bellman error, i.e., \(Q_{h}^{\star}(s,a)-(\mathcal{T}_{h}Q_{h+1}^{\star})(s,a)=0\).

**Imitation Learning.** The essence of IL lies in acquiring a high-quality policy _without_ the reward function \(r^{\mathrm{true}}\). In pursuit of this objective, we typically posit the existence of a near-optimal expert policy \(\pi^{\mathrm{E}}\) capable of interacting with the environment to generate a dataset, comprising \(N\) trajectories each of length \(H\): \(\mathcal{D}^{\mathrm{E}}=\{\tau=(s_{1},a_{1},s_{2},a_{2},\ldots,s_{H},a_{H}) :a_{h}\sim\pi_{h}^{\mathrm{E}}(\cdot|s_{h}),s_{h+1}\sim P_{h}(\cdot|s_{h},a_{h }),\forall h\in[H]\}\). Subsequently, the learner leverages this dataset \(\mathcal{D}^{\mathrm{E}}\) to mimic the behavior of the expert and thereby derives an effective policy. The quality of this imitation is measured by the _imitation gap_[1, 43, 41]: \(V^{\pi^{\mathrm{E}}}-V^{\pi}\), where \(\pi\) represents the learned policy. Essentially, we hope that the learned policy can perfectly mimic the expert such that the imitation gap is small.

AIL is a prominent class of IL methods that imitates expert behavior through an adversarial learning process defined by \(\min_{\pi}\max_{r}V_{r}^{\pi^{\mathrm{E}}}-V_{r}^{\pi}\), where \(V_{r}^{\pi}\) denotes the value of policy \(\pi\) under reward \(r\). In this framework, AIL infers a reward function that maximizes the value gap between the expert policy and the learning policy. Subsequently, it learns a policy that minimizes this value gap using the inferred reward. Essentially, AIL involves solving several RL sub-problems, as the outer optimization problem concerning the policy is equivalent to an RL problem under the inferred reward \(r\).

**General Function Approximation.** This work considers AIL with general function approximation. In this setup, the learner first has access to a reward class \(\mathcal{R}=\mathcal{R}_{1}\times\mathcal{R}_{2}\times\ldots\times\mathcal{R} _{H}\) with \(\forall h\in[H],\mathcal{R}_{h}\subseteq(\mathcal{S}\times\mathcal{A} \rightarrow[0,1])\) to infer the reward. We assume that \(\mathcal{R}\) captures the unknown true reward.

**Assumption 1** (Realizability of \(\mathcal{R}\)).: _The unknown true reward lies in the reward class, i.e., \(r^{\mathrm{true}}\in\mathcal{R}\)._

Besides, the learner also has access to a Q-value function class \(\mathcal{Q}=\mathcal{Q}_{1}\times\mathcal{Q}_{2}\times\ldots\times\mathcal{Q} _{H}\) with \(\forall h\in[H],\mathcal{Q}_{h}\subseteq(\mathcal{S}\times\mathcal{A} \rightarrow[0,H])\), which is used for solving several RL sub-problems under different inferred rewards in AIL. Since there is no reward in the \(H+1\) step, we always set \(Q_{H+1}\equiv 0\). Below, we present two standard assumptions about the function class \(\mathcal{Q}\) that are commonly adopted in the literature of RL with function approximation [59, 23, 68, 32].

**Assumption 2** (Realizability of \(\mathcal{Q}\)).: _For reward \(r\in\mathcal{R}\), \(Q^{\star,r}\in\mathcal{Q}\), where \(Q^{\star,r}\) denotes the optimal Q-value function under reward \(r\)._

**Assumption 3** (Bellman Completeness of \(\mathcal{Q}\)).: _For reward \(r\in\mathcal{R}\), \(\mathcal{T}_{h}^{r}\mathcal{Q}_{h+1}\subseteq\mathcal{Q}_{h},\ \forall h\in[H]\), where \(\mathcal{T}_{h}^{r}\) denotes the Bellman operator under reward \(r\) and \(\mathcal{T}_{h}^{r}\mathcal{Q}_{h+1}=\{\mathcal{T}_{h}^{r}Q_{h+1}:Q_{h+1}\in \mathcal{Q}_{h+1}\}\)._

In short, Assumption 2 states that the Q-value class \(\mathcal{Q}\) should capture the optimal Q-value function, while Assumption 3 indicates the closeness of \(\mathcal{Q}\) under Bellman update. It is easy to verify that Assumptions 1, 2 and 3 are more general than the tabular MDP [45, 64], linear mixture MDP [33] and linear MDP [57] assumptions used in previous works.

When the function class contains a finite number of elements, its cardinality can be used to quantify its "size". However, for general function approximation, where the function class may contain an infinite number of elements, we utilize the standard \(\varepsilon\)-covering number [58] to measure its complexity.

**Definition 1** (\(\varepsilon\)-covering number).: _For function class \(\mathcal{F}\subseteq(\mathcal{X}\rightarrow\mathbb{R})\), the \(\varepsilon\)-covering number of \(\mathcal{F}\), denoted as \(\mathcal{N}_{\varepsilon}(\mathcal{F})\), is defined as the minimum integer \(n\) such that there exists a finite subset \(\mathcal{F}^{\prime}\subseteq\mathcal{F}\) with \(|\mathcal{F}^{\prime}|=n\) such that for any function \(f\in\mathcal{F}\), there exists \(f^{\prime}\in\mathcal{F}^{\prime}\) satisfying that \(\max_{x\in\mathcal{X}}|f(x)-f^{\prime}(x)|\leq\varepsilon\)._Optimization-based Adversarial Imitation Learning

In this section, we introduce a provably efficient method called Optimization-Based Adversarial Imitation Learning (OPT-AIL). In Section 4.1, we delve into the core components of OPT-AIL, which involves online optimization for reward functions and optimism-regularized Bellman error minimization for Q-value functions. We discuss the underlying principles and provide theoretical guarantees with general function approximation. Thanks to its easy-to-implement merit, we provide a practical implementation of OPT-AIL using stochastic-gradient-based methods in Section 4.2.

### Theoretical Analysis of OPT-AIL

In this section, we present our provably efficient method OPT-AIL with general function approximation; see Algorithm 1 for an overview. To start with, recall that our theoretical goal is to ensure the algorithm can output a policy with \(\varepsilon\)-imitation gap by using finite expert samples and environment interactions. To obtain the final policy, we leverage the standard online-to-batch conversion technique [37]. Specifically, during the learning process, the learner iteratively generates a sequence of rewards \(\{r^{k}\}_{k=1}^{K}\) and policies \(\{\pi^{k}\}_{k=1}^{K}\), and outputs the policy \(\overline{\pi}\) that is uniformly sampled from \(\{\pi^{k}\}_{k=1}^{K}\). To analyze the imitation gap of \(\overline{\pi}\), we leverage the following standard error decomposition lemma.

**Lemma 1**.: _Consider a sequence of rewards \(\{r^{k}\}_{k=1}^{K}\) and policies \(\{\pi^{k}\}_{k=1}^{K}\), and the policy \(\overline{\pi}\) is uniformly sampled from \(\{\pi^{k}\}_{k=1}^{K}\). Then it holds that_

\[V^{\pi^{\mathrm{E}}}-V^{\overline{\pi}}=\underbrace{\frac{1}{K}\sum_{k=1}^{K} \left(V_{r^{\mathrm{true}}}^{\pi^{\mathrm{E}}}-V_{r^{\mathrm{true}}}^{\pi^{ \mathrm{E}}}-\left(V_{r^{k}}^{\pi^{\mathrm{E}}}-V_{r^{k}}^{\pi^{\mathrm{E}}} \right)\right)}_{\text{reward error}}+\underbrace{\frac{1}{K}\sum_{k=1}^{K} \left(V_{r^{k}}^{\pi^{\mathrm{E}}}-V_{r^{k}}^{\pi^{k}}\right)}_{\text{policy error}}. \tag{1}\]

Lemma 1 suggests that to achieve a small imitation gap, it is crucial to control both reward error and policy error. Specifically, reward error quantifies the distance between the true reward \(r^{\mathrm{true}}\) and the learned reward \(r^{k}\) through the imitation gap. Besides, policy error measures the value difference between the expert policy \(\pi^{\mathrm{E}}\) and the learned policy \(\pi^{k}\) under the inferred reward \(r^{k}\). Notably, this policy error differs from the concept of regret in RL [23; 32], where the reward is fixed.

To theoretically minimize the reward error and policy error, we consider an iterative approach, in which each iteration first updates the reward and subsequently derives the policy. The subsequent parts detail the reward and policy updates, which involve solving two optimization problems.

```
0: Reward class \(\mathcal{R}\), Q-value class \(\mathcal{Q}\), initialized reward \(r^{0}\), policy \(\pi^{0}\) and dataset \(\mathcal{D}^{0}=\emptyset\).
1:for\(k=1,2,\dots,K\)do
2: Apply \(\pi^{k-1}\) to roll out a trajectory \(\tau^{k-1}\) and append it to the dataset \(\mathcal{D}^{k}=\mathcal{D}^{k-1}\cup\{\tau^{k-1}\}\).
3: Obtain \(r^{k}\) by running a no-regret algorithm to solve the online optimization problem with observed loss functions \(\{\mathcal{L}^{i}(r)\}_{i=0}^{k-1}\) up to an error \(\varepsilon_{\mathrm{opt}}^{r}\), where \(\mathcal{L}^{i}(r)=\widehat{V}_{r}^{\pi^{i}}-\widehat{V}_{r}^{\pi^{\mathrm{E}}}\).
4: Obtain \(Q^{k}\) by solving the following optimization problem up to an error \(\varepsilon_{\mathrm{opt}}^{Q}\). \[\min_{Q\in\mathcal{Q}}\mathcal{L}^{k}(Q):=\mathrm{BE}^{k}(Q)-\lambda\max_{a\in \mathcal{A}}Q_{1}(s_{1},a),\]

 where \(\mathrm{BE}^{k}(Q)=\sum_{h=1}^{H}\mathcal{E}_{h}(Q_{h},Q_{h+1};\mathcal{D}^{k},r^{k})-\inf_{Q_{h}^{\prime}\in\mathcal{Q}_{h}}\mathcal{E}_{h}(Q_{h}^{\prime},Q_{h+1};\mathcal{D}^{k},r^{k})\).
5: Obtain \(\pi^{k}\) by \(\pi_{h}^{k}(s)=\operatorname*{argmax}_{a\in\mathcal{A}}Q_{h}^{k}(s,a)\).
6:endfor
7:\(\overline{\pi}\) sampled uniformly from \(\{\pi^{k}\}_{k=1}^{K}\).
```

**Algorithm 1** Optimization-based Adversarial Imitation Learning

**Reward Update via Online Optimization (Line 3 in Algorithm 1).** The goal of this step is to control the reward error. More concretely, in iteration \(k\), we aim to learn a reward \(r^{k}\) such that the error \(V_{r}^{\pi^{k}}-V_{r^{k}}^{\pi^{\mathrm{E}}}-(V_{r^{k}}^{\pi^{\mathrm{E}}}-V_{r ^{\mathrm{true}}}^{\pi^{\mathrm{E}}})\) is small. We formulate this problem as an _online_ optimization problem. In iteration \(k\), using the previously observed loss functions \(\{V_{r}^{\pi^{i}}-V_{r}^{\pi^{\mathrm{E}}}\}_{i=0}^{k-1}\), the reward learner selects \(r^{k}\) and then observes the current loss function \(V_{r}^{\pi^{k}}-V_{r}^{\pi^{\mathrm{E}}}\). Moreover, since the previous expected loss functions \(\{V_{r}^{\pi^{i}}-V_{r}^{\pi^{\mathrm{E}}}\}_{i=0}^{k-1}\) are not available, we instead minimize the _estimated_ loss functions. In particular, we leverage expert demonstrations \(\mathcal{D}^{\mathrm{E}}\) and the trajectory \(\tau^{i}\) collected by policy \(\pi^{i}\) to establish an unbiased estimation \(\mathcal{L}^{i}(r)=\widehat{V}_{r}^{\pi^{i}}-\widehat{V}_{r}^{\pi^{k}}\) for \(V_{r}^{\pi^{i}}-V_{r}^{\pi^{k}}\), where

\[\widehat{V}_{r}^{\pi^{i}}=\sum_{h=1}^{H}r_{h}(s_{h}^{i},a_{h}^{i}),\ \widehat{V}_{r}^{\pi^{k}}=\frac{1}{N}\sum_{\tau\in\mathcal{D}^{\mathrm{E}}}\sum_{h=1}^{H}r_{h}(\tau(s_{h}),\tau(a_{h})).\]

Here \((\tau(s_{h}),\tau(a_{h}))\) is the state-action pair of trajectory \(\tau\) visited in time step \(h\) and \(\tau^{i}=\{s_{1}^{i},a_{1}^{i},\ldots,s_{H}^{i},a_{H}^{i}\}\) is the trajectory collected by policy \(\pi^{i}\). The ultimate goal of the reward learner is to minimize the cumulative losses \(\sum_{k=1}^{K}\widehat{V}_{r}^{\pi^{k}}-\widehat{V}_{r}^{\pi^{k}}\). To achieve this goal, we employ a no-regret algorithm [18]. In the following part, we formally define the reward optimization error resulting from running the no-regret algorithm.

**Definition 2** (Reward Optimization Error).: _For any sequence of policies \(\{\pi^{k}\}_{k=1}^{K}\), the no-regret reward optimization algorithm sequentially outputs rewards \(r^{1},\ldots,r^{K}\). The reward optimization error \(\varepsilon_{\mathrm{opt}}^{r}\) is defined as \(\varepsilon_{\mathrm{opt}}^{r}:=(1/K)\cdot\max_{r\in\mathcal{R}}\sum_{k=1}^{K} \widehat{V}_{r}^{\pi^{k}}-\widehat{V}_{r}^{\pi^{k}}-(\widehat{V}_{r}^{\pi^{k}}- \widehat{V}_{r}^{\pi^{k}})\)._

The reward optimization error, as defined above, aligns with the standard average regret in online optimization [18], a concept not extensively explored in the context of AIL. When the loss functions \(\{\mathcal{L}^{k}(r)\}_{k=0}^{K}\) are convex functions and the reward class \(\mathcal{R}\) is a convex set, we can apply online projected gradient descent [18] as the no-regret algorithm, which ensures the reward optimization error \(\varepsilon_{\mathrm{opt}}^{r}=\mathcal{O}(1/\sqrt{K})\). As for non-convex functions and sets, employing Follow-the-Perturbed-Leader can similarly achieve \(\varepsilon_{\mathrm{opt}}^{r}=\mathcal{O}(1/\sqrt{K})\)[47].

**Policy Update via Optimism-Regularized Bellman-error Minimization (Lines 4-5 in Algorithm 1).** The target of policy updates is to control the policy error. In iteration \(k\), the policy learner aims to recover a policy \(\pi^{k}\) such that the policy error \(V_{r^{k}}^{\pi^{k}}-V_{r}^{\pi^{k}}\) is small, where \(r^{k}\) is the recently recovered reward function. This is essentially an RL problem under reward function \(r^{k}\). Building upon [32], we leverage a model-free approach, based on Bellman error minimization, to solve this RL sub-problem. In particular, we first learn Q-value functions by solving the optimization problem of

\[\min_{Q\in\mathcal{Q}}\mathcal{L}^{k}(Q):=\mathrm{BE}^{k}(Q)-\lambda\max_{a\in \mathcal{A}}Q_{1}(s_{1},a),\]

\[\text{with}\ \mathrm{BE}^{k}(Q)=\sum_{h=1}^{H}\mathcal{E}_{h}(Q_{h},Q_{h+1}; \mathcal{D}^{k},r^{k})-\inf_{Q_{h}^{\prime}\in\mathcal{Q}_{h}}\mathcal{E}_{h}( Q_{h}^{\prime},Q_{h+1};\mathcal{D}^{k},r^{k}),\]

where \(\mathcal{E}_{h}(Q_{h},Q_{h+1};\mathcal{D}^{k},r^{k})=\sum_{i=0}^{k-1}(Q_{h}(s _{h}^{i},a_{h}^{i})-r_{h}^{k}-\max_{a^{\prime}\in\mathcal{A}}Q_{h+1}(s_{h+1}^{ i},a^{\prime}))^{2}\), \(\mathcal{D}^{k}=\{\tau^{i}\}_{i=0}^{k-1}\) with \(\tau^{i}=\{s_{1}^{i},a_{1}^{i},\ldots,s_{H}^{i},a_{H}^{i}\}\) and \(\lambda>0\) is the regularization coefficient. As shown in [5, 23], \(\mathrm{BE}^{k}(Q)\) is the estimated squared Bellman error of \(Q\) with respect to reward \(r^{k}\) and dataset \(\mathcal{D}^{k}\). In this optimization problem, the main objective \(\mathrm{BE}^{k}(Q)\) ensures a small Bellman error while the regularization term \(\max_{a\in\mathcal{A}}Q_{1}(s_{1},a)\) tends to search an optimistic Q-value function for encouraging exploration. It is worth noting that Algorithm 1 only requires approximately solving the optimization problem up to an error \(\varepsilon_{\mathrm{opt}}^{Q}\) with \(\varepsilon_{\mathrm{opt}}^{Q}=\mathcal{L}^{k}(Q^{k})-\min_{Q\in\mathcal{Q}} \mathcal{L}^{k}(Q)\). After obtaining the Q-value function \(Q^{k}\), we derive \(\pi^{k}\) as the greedy policy of \(Q^{k}\).

**Theoretical Guarantee of OPT-AIL.** In the above part, we have explained the algorithmic mechanisms of OPT-AIL. Now we present the theoretical guarantee. To ensure the sample efficiency of solving RL sub-problems within AIL, we make a structural assumption on the underlying MDP. In particular, we assume that the MDP has a small generalized eluder coefficient. This coefficient, introduced in [68], quantifies the inherent difficulty of learning the MDP with function approximation in RL. We adapt this concept to AIL where the reward function is changing.

**Assumption 4** (Low generalized eluder coefficient [68]).: _We assume that given an \(\varepsilon>0\), the generalized eluder coefficient \(d_{\mathrm{GEC}}(\varepsilon)\) is the smallest \(d\) (\(d\geq 0\)) such that for any sequence of \(\{r^{k}\}_{k=1}^{K}\subseteq\mathcal{R}\), \(\{Q^{k}\}_{k=1}^{K}\subseteq\mathcal{Q}\) and the corresponding greedy policies \(\{\pi^{k}\}_{k=1}^{K}\),_

\[\sum_{k=1}^{K}Q_{1}^{k}(s_{1},\pi^{k})-V_{r^{k}}^{\pi^{k}} \leq\inf_{\mu\geq 0}\frac{\mu}{2}\sum_{k=1}^{K}\sum_{i=1}^{k-1} \mathbb{E}\left[\left.\prod_{h=1}^{H}\left(Q_{h}^{k}(s_{h},a_{h})-\mathcal{T}_ {h}^{\pi^{k}}Q_{h+1}^{k}(s_{h},a_{h})\right)^{2}\right|\pi^{i}\right]+\frac{d} {2\mu}\] \[+\sqrt{dHK}+\varepsilon HK,\]

_where \(Q_{1}^{k}(s_{1},\pi^{k}):=\mathbb{E}_{a_{1}\sim\pi^{k}_{1}(\cdot|s_{1})}[Q_{1}^{ k}(s_{1},a_{1})]\)._Intuitively, a low generalized eluder coefficient ensures that the prediction error \(Q_{1}^{k}(s_{1},\pi^{k})-V_{\pi^{k}}^{\pi^{k}}\) for \(\pi^{k}\) can be effectively controlled by the Bellman error on the dataset generated by historical policies \(\{\pi^{i}\}_{i=1}^{k-1}\). As demonstrated in [68], the MDPs with low generalized eluder coefficient form a rich class of MDPs, which covers many well-known MDP instances such tabular MDPs, linear MDPs [24] and MDPs with low Bellman eluder dimension [23]. Now we are ready to present the theoretical guarantee of OPT-AIL.

**Theorem 1**.: _Under Assumptions 1, 2, 3 and 4. For any fixed \(\varepsilon\in(0,1]\) and \(\delta\in(0,1]\), consider Algorithm 1 with \(\lambda=c_{1}\sqrt{(KH^{3}\log(4KHN_{\rho}(\mathcal{Q})\mathcal{N}_{\rho}( \mathcal{R})/\delta)+K^{2}H^{3}\rho)/d_{\mathrm{GEC}}}\), where \(d_{\mathrm{GEC}}:=d_{\mathrm{GEC}}(\varepsilon/H)\), \(\rho:=c_{2}\varepsilon^{2}/(H^{2}d_{\mathrm{GEC}}+H)\), \(c_{1}\) and \(c_{2}\) are absolute constants. Then with probability at least \(1-\delta\), we have that \(V^{\pi^{k}}-V^{\pi}\leq\varepsilon+\varepsilon^{r}_{\mathrm{opt}}+(\varepsilon ^{Q}_{\mathrm{opt}}/\lambda)\) if the expert sample complexity and interaction complexity satisfy_

\[N \gtrsim\frac{H^{2}\log(\max_{h\in[H]}\mathcal{N}_{\rho}(\mathcal{R }_{h})/\delta)}{\varepsilon^{2}},\] \[K \gtrsim\frac{H^{4}d_{\mathrm{GEC}}\log(Hd_{\mathrm{GEC}}\max_{h \in[H]}\mathcal{N}_{\rho}(\mathcal{Q}_{h})\mathcal{N}_{\rho}(\mathcal{R}_{h})/ (\delta\varepsilon))+H^{2}\log(1/\delta)}{\varepsilon^{2}}.\]

The proof of Theorem 1 can be found in Appendix B.2. Theorem 1 indicates that when \(d_{\mathrm{GEC}}=\Omega(1)\), OPT-AIL achieves the expert sample complexity \(\widetilde{\mathcal{O}}(H^{2}\log(\max_{h\in[H]}\mathcal{N}_{\rho}(\mathcal{R }_{h}))/\varepsilon^{2})\) and interaction complexity \(\widetilde{\mathcal{O}}(H^{4}d_{\mathrm{GEC}}\log(\max_{h\in[H]}\mathcal{N}_{ \rho}(\mathcal{Q}_{h})\mathcal{N}_{\rho}(\mathcal{R}_{h}))/\varepsilon^{2})\) in the general function approximation setting. To our best knowledge, OPT-AIL is the first provably efficient online AIL algorithm for general function approximation.

Notably, OPT-AIL improves over BC [13] by an order of \(\mathcal{O}(H)\), suggesting that OPT-AIL can still provably mitigate the compounding errors issue in BC for general function approximation. When restricting Theorem 1 to linear MDPs with dimension \(d\)[68], OPT-AIL can achieve the expert sample complexity \(\widetilde{\mathcal{O}}(H^{2}d/\varepsilon^{2})\) and interaction complexity \(\widetilde{\mathcal{O}}(H^{5}d^{2}/\varepsilon^{2})\). Furthermore, when \(\mathcal{Q}\) and \(\mathcal{R}\) are neural network classes commonly employed in practice, we can obtain the corresponding complexity result by plugging the covering number bound of neural networks [7] into Theorem 1. Finally, OPT-AIL only requires the approximate optimization of two objectives, thereby facilitating a practical implementation with neural networks, which will be presented in the next section.

Although Theorem 1 produces desirable outcomes, it does have some limitations. One of the limitations is that Theorem 1 requires the Bellman completeness condition for the Q-value class (i.e., Assumption 3). Nevertheless, recent advances [4] in RL have developed new techniques to remove this assumption. We leave the extension of these techniques to AIL for future work.

### Practical Implementation of OPT-AIL

In this section, we provide a practical implementation for OPT-AIL, which is based on the stochastic-gradient-based methods; see Algorithm 2 for an overview. We elaborate on the practical reward update and policy update in detail as follows.

```
1:Initialized reward \(r^{0}\), Q-value \(Q^{0}\), target Q-value \(\overline{Q}^{0}=Q^{0}\), policy \(\pi^{0}\) and dataset \(\mathcal{D}^{0}=\emptyset\).
2:for\(k=1,2,\ldots,K\)do
3: Apply \(\pi^{k-1}\) to roll out a trajectory \(\tau^{k-1}\) and append it to the dataset \(\mathcal{D}^{k}=\mathcal{D}^{k-1}\cup\{\tau^{k-1}\}\).
4: Update the reward function by \(r^{k}\gets r^{k-1}-\alpha_{r}\nabla\ell^{k}(r)\) from Eq. (2).
5: Update the Q-value function by \(Q^{k}\gets Q^{k-1}-\alpha_{Q}\nabla\ell^{k}(Q)\) from Eq. (3).
6: Update the policy by \(\pi^{k}\leftarrow\pi^{k-1}+\alpha_{\pi}\nabla\ell^{k}(\pi)\), where \(\ell^{k}(\pi):=\mathbb{E}_{\tau\sim\mathcal{D}^{k}}[\sum_{h=1}^{H}Q_{h}^{k}(s _{h},\pi)]\).
7: Update the target Q-value by \(\overline{Q}^{k}\leftarrow\tau Q^{k}+(1-\tau)\overline{Q}^{k-1}\)
8:endfor
```

**Algorithm 2** Practical Implementation of OPT-AIL

**Practical Reward Update.** Here we detail a practical implementation of the reward update by applying an online optimization approach. Recall that in line 3 of Algorithm 1, a no-regret algorithm is employed to solve the online optimization problem. To implement this mechanism, we choose the classical online optimization approach Follow-the-Regularized-Leader (FTRL) [2] as the no-regretapproach. In iteration \(k\), FTRL minimizes the sum of all historical loss functions with a regularization.

\[\begin{split}\min_{r\in\mathcal{R}}\ell^{k}(r):&=\sum_{ i=0}^{k-1}\mathcal{L}^{i}(r)+\beta\psi(r)\\ &=k\bigg{(}\mathbb{E}_{\tau\sim\mathcal{D}^{k}}\bigg{[}\sum_{h=1}^{ H}r_{h}(s_{h}^{i},a_{h}^{i})\bigg{]}-\mathbb{E}_{\tau\sim\mathcal{D}^{k}}\bigg{[} \sum_{h=1}^{H}r_{h}(s_{h}^{i},a_{h}^{i})\bigg{]}\bigg{)}+\beta\psi(r),\end{split} \tag{2}\]

where \(\mathbb{E}_{\mathcal{D}}[\cdot]\) denotes the empirical distribution of dataset \(\mathcal{D}\). Here \(\psi(r)\) is the regularization term. In practice, we choose \(\psi(r)\) as the gradient penalty [6] of the reward model, which can help stabilize the learning process [27]. According to Eq. (2), the reward learner aims to maximize the value gap between the expert policy and all previous policies.

Besides, as indicated in Eq. (2), all historical samples \(\mathcal{D}^{k}\) are utilized for the reward update. This learning style is exactly off-policy reward learning [27; 28]. In particular, applying FTRL for the reward update and off-policy reward learning share the same main objective. Previous practical works [27; 28] found that this off-policy reward learning works well in practice, but could not give an explanation. In this work, we justify this off-policy learning style from an online optimization perspective: performing off-policy learning, which aligns with the FTRL approach, can effectively control the reward optimization error.

**Practical Policy Update.** To implement the policy update in practice, we adopt the actor-critic framework [14; 17; 27]. In particular, we maintain a policy model \(\pi\) and a Q-value model \(Q\). Recall in line 4 of Algorithm 1, the Q-value function is learned by minimizing the optimism-regularized Bellman error. To implement this principle, following [12; 8], we leverage the temporal difference loss [31] of the Q-value model and its delayed target to approximate the theoretical Bellman error. Then we arrive at the following objective.

\[\min_{Q\in\mathcal{Q}}\ell^{k}(Q):=\mathbb{E}_{\tau\sim\mathcal{D}^{k}}\left[ \sum_{h=1}^{H}\Big{(}Q_{h}(s_{h},a_{h})-r_{h}^{k}-\bar{Q}_{h+1}^{k-1}(s_{h+1}, \pi^{k-1})\Big{)}^{2}\right]-\lambda Q_{1}(s_{1},\pi^{k-1}). \tag{3}\]

Here \(\bar{\mathcal{Q}}=\{\bar{\mathcal{Q}}_{1},\ldots,\bar{\mathcal{Q}}_{H}\}\) is the delayed target Q-value model. Besides, we define that \(\bar{Q}_{h+1}^{k-1}(s_{h+1},\pi^{k-1}):=\mathbb{E}_{a^{\prime}\sim\pi_{h+1}^{ k-1}(\cdot|s_{h+1})}[\bar{\mathcal{Q}}_{h+1}(s_{h+1},a^{\prime})]\) where the previous greedy policy \(\pi^{k-1}\) is used to approximate the maximum operator [17]. Consequently, we derive the greedy policy by optimizing the objective of \(\max_{\pi}\ell^{k}(\pi):=\mathbb{E}_{\tau\sim\mathcal{D}^{k}}[\sum_{h=1}^{H}Q _{h}^{k}(s_{h},\pi)]\).

## 5 Experiments

In this section, we evaluate the expert sample efficiency and environment interaction efficiency of OPT-AIL through experiments. Below, we provide a brief overview of the experimental set-up, with detailed information available in Appendix C due to space constraints.

### Experiment Set-up

**Environment.** We conduct experiments on 8 tasks sourced from the feature-based DMControl benchmark [53], a leading benchmark in IL that offers a diverse set of continuous control tasks. For each task, we adopt online DrQ-v2 [66] to train an agent with sufficient environment interactions and regard the resultant policy as the expert policy. Then we roll out this expert policy to collect expert demonstrations. Each algorithm is tested over five trials with different random seeds, and in each run, we evaluate the policy return using Monte Carlo approximation with 10 trajectories.

**Baselines.** Existing theoretical AIL approaches like MB-TAIL [64] and OGAIL [33] include count-based or covariance-based bonuses, making it challenging to implement these designs when using neural network approximations. Thus, we do not include these methods in our experiments. Instead, we compare OPT-AIL with prior deep IL methods, including BC [39], IQLearn [15], PPLL [56], FILTER [51] and HyPE [42], despite that most of them lack theoretical guarantees. Notably, IQLearn, FILTER and HyPE represent prior SOTA deep AIL approaches. To ensure a fair comparison, we implement all these methods within the same codebase. For detailed implementations, please refer to Appendix C.

### Experiment Results

**Expert Sample Efficiency.** Figure 1 shows the performance of the learned policies after 500k environment interactions with varying numbers of expert trajectories. First, OPT-AIL significantly outperforms BC, verifying the theoretical claim that OPT-AIL can mitigate the compounding errors issue inherent in BC for general function approximation. Moreover, OPT-AIL consistently matches or exceeds the performance of prior SOTA AIL methods on all tasks. Notably, OPT-AIL demonstrates outstanding performance in scenarios with limited expert demonstrations, a common occurrence in real-world applications. In particular, when there is only one expert trajectory, our method uniquely achieves expert or near-expert performance on tasks like Finger Spin, Walker Run and Hopper Hop.

Figure 1: Overall performance on 8 DMControl tasks over 5 random seeds following 500k interactions with the environment. Here the \(x\)-axis is the number of expert trajectories and the \(y\)-axis is the return. The solid lines are the mean of results while the shaded region corresponds to the standard deviation over 5 random seeds. Same as the following figures.

Figure 2: Learning curves on 8 DMControl tasks over 5 random seeds using 1 expert trajectory. Here the \(x\)-axis is the number of environment interactions and the \(y\)-axis is the return.

**Environment Interaction Efficiency.** Figure 2 displays the learning curves of different algorithms with \(1\) expert trajectory. Compared with prior SOTA AIL approaches, OPT-AIL achieves comparable or better performance regarding interaction efficiency on all \(8\) tasks. Notably, on Hopper Hop, Walker Run and Walker Run, OPT-AIL can achieve near-expert performance with substantially fewer environment interactions compared with prior approaches. We also demonstrate the superior interaction efficiency of OPT-AIL with other numbers of expert trajectories; please refer to Appendix D for additional results.

## 6 Conclusions

To narrow the gap between theory and practice in adversarial imitation learning, this paper investigates AIL with general function approximation. We develop a new AIL approach termed OPT-AIL, which centers on performing online optimization for reward functions and optimism-regularized Bellman error minimization for Q-value functions. In theory, OPT-AIL achieves polynomial expert sample complexity and interaction complexity for general function approximation. In practice, OPT-AIL only requires approximately solving two optimization problems, which enables an efficient implementation. Our experiments demonstrate that OPT-AIL outperforms prior SOTA methods in several challenging tasks, highlighting its potential to bridge theoretical soundness with practical efficiency.

In tabular MDPs, the currently optimal expert sample complexity is \(\mathcal{O}(H^{3/2}/\varepsilon)\)[41, 64], which is better than \(\mathcal{O}(H^{2}/\varepsilon^{2})\) attained in this paper. Therefore, a promising and valuable future direction would be to develop more advanced AIL approaches that achieve this expert sample complexity in the setting of general function approximation. Besides, [63] established a horizon-free imitation gap bound for AIL in tabular MDPs. Thus it is interesting to explore horizon-free bounds for AIL with general function approximation.

## 7 Acknowledgements

We thank Ziniu Li and Yichen Li for their helpful discussions and feedback. This work was supported by the Fundamental Research Program for Young Scholars (PhD Candidates) of the National Science Foundation of China (623B2049) and Jiangsu Science Foundation (BK20243039).

## References

* [1] Pieter Abbeel and Andrew Y. Ng. Apprenticeship learning via inverse reinforcement learning. In _Proceedings of the 21st International Conference on Machine Learning_, pages 1-8, 2004.
* [2] Jacob D Abernethy, Elad Hazan, and Alexander Rakhlin. Competing in the dark: An efficient algorithm for bandit linear optimization. In _Proceedings of the 21st Annual Conference on Learning Theory_, pages 263-274, 2008.
* [3] Alekh Agarwal, Daniel Hsu, Satyen Kale, John Langford, Lihong Li, and Robert Schapire. Taming the monster: A fast and simple algorithm for contextual bandits. In _Proceedings of the 31st International Conference on Machine Learning_, pages 1638-1646, 2014.
* [4] Philip Amortila, Dylan J Foster, Nan Jiang, Ayush Sekhari, and Tengyang Xie. Harnessing density ratios for online reinforcement learning. _arXiv_, 2401.09681, 2024.
* [5] Andras Antos, Csaba Szepesvari, and Remi Munos. Learning near-optimal policies with bellman-residual minimization based fitted policy iteration and a single sample path. _Machine Learning_, 71:89-129, 2008.
* [6] Martin Arjovsky, Soumith Chintala, and Leon Bottou. Wasserstein generative adversarial networks. In _Proceedings of the 34th International Conference on Machine Learning_, pages 214-223, 2017.
* [7] Peter L Bartlett, Dylan J Foster, and Matus J Telgarsky. Spectrally-normalized margin bounds for neural networks. _Advances in Neural Information Processing Systems 30_, 2017.
* [8] Mohak Bhardwaj, Tengyang Xie, Byron Boots, Nan Jiang, and Ching-An Cheng. Adversarial model for offline reinforcement learning. _Advances in Neural Information Processing Systems 37_, 36, 2024.

* [9] Kiante Brantley, Wen Sun, and Mikael Henaff. Disagreement-regularized imitation learning. In _Proceedings of the 8th International Conference on Learning Representations_, 2020.
* [10] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof Choromanski, Tianli Ding, Danny Driess, Avinava Dubey, Chelsea Finn, et al. Rt-2: Vision-language-action models transfer web knowledge to robotic control. _arXiv_, 2307.15818, 2023.
* [11] Xinshi Chen, Shuang Li, Hui Li, Shaohua Jiang, Yuan Qi, and Le Song. Generative adversarial user model for reinforcement learning based recommendation system. In _Proceedings of the 36th International Conference on Machine Learning_, pages 1052-1061, 2019.
* [12] Ching-An Cheng, Tengyang Xie, Nan Jiang, and Alekh Agarwal. Adversarially trained actor critic for offline reinforcement learning. In _Proceedings of the 39th International Conference on Machine Learning_, pages 3852-3878, 2022.
* [13] Dylan J Foster, Adam Block, and Dipendra Misra. Is behavior cloning all you need? understanding horizon in imitation learning. _arXiv_, 2407.15007, 2024.
* [14] Scott Fujimoto, Herke van Hoof, and David Meger. Addressing function approximation error in actor-critic methods. In _Proceedings of the 35th International Conference on Machine Learning_, pages 1582-1591, 2018.
* [15] Divyansh Garg, Shuvam Chakraborty, Chris Cundy, Jiaming Song, and Stefano Ermon. Iq-learn: Inverse soft-q learning for imitation. In _Advances in Neural Information Processing Systems 34_, pages 4028-4039, 2021.
* [16] Seyed Kamyar Seyed Ghasemipour, Richard S. Zemel, and Shixiang Gu. A divergence minimization perspective on imitation learning methods. In _Proceedings of the 3rd Annual Conference on Robot Learning_, pages 1259-1277, 2019.
* [17] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. In _Proceedings of the 35th International Conference on Machine Learning_, pages 1856-1865, 2018.
* [18] Elad Hazan. Introduction to online convex optimization. _Foundations and Trends in Optimization_, 2(3-4):157-325, 2016.
* [19] Jonathan Ho and Stefano Ermon. Generative adversarial imitation learning. In _Advances in Neural Information Processing Systems 29_, pages 4565-4573, 2016.
* [20] Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine. When to trust your model: Model-based policy optimization. In _Advances in Neural Information Processing Systems 32_, pages 12498-12509, 2019.
* [21] Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, John Langford, and Robert E Schapire. Contextual decision processes with low bellman rank are pac-learnable. In _Proceedings of the 34th International Conference on Machine Learning_, pages 1704-1713, 2017.
* [22] Shengyi Jiang, Jingcheng Pang, and Yang Yu. Offline imitation learning with a misspecified simulator. _Advances in Neural Information Processing Systems 33_, 2020.
* [23] Chi Jin, Qinghua Liu, and Sobhan Miryoosefi. Bellman eluder dimension: New rich classes of rl problems, and sample-efficient algorithms. In _Advances in Neural Information Processing Systems 34_, pages 13406-13418, 2021.
* [24] Chi Jin, Zhuoran Yang, Zhaoran Wang, and Michael I. Jordan. Provably efficient reinforcement learning with linear function approximation. In _Proceedings of the 33rd Annual Conference on Learning Theory_, pages 2137-2143, 2020.
* [25] Yue Kang, Cho-Jui Hsieh, and Thomas Chun Man Lee. Robust lipschitz bandits to adversarial corruptions. _Advances in Neural Information Processing Systems 37_, 2023.
* [26] Liyiming Ke, Matt Barnes, Wen Sun, Gilwoo Lee, Sanjiban Choudhury, and Siddhartha S. Srinivasa. Imitation learning as f-divergence minimization. _arXiv_, 1905.12888, 2019.

* [27] Ilya Kostrikov, Kumar Krishna Agrawal, Debidatta Dwibedi, Sergey Levine, and Jonathan Tompson. Discriminator-actor-critic: Addressing sample inefficiency and reward bias in adversarial imitation learning. In _Proceedings of the 7th International Conference on Learning Representations_, 2019.
* [28] Ilya Kostrikov, Ofir Nachum, and Jonathan Tompson. Imitation learning via off-policy distribution matching. In _Proceedings of the 8th International Conference on Learning Representations_, 2020.
* [29] Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative q-learning for offline reinforcement learning. _Advances in Neural Information Processing Systems_, 33:1179-1191, 2020.
* [30] Ziniu Li, Tian Xu, Zeyu Qin, Yang Yu, and Zhi-Quan Luo. Imitation learning from imperfection: Theoretical justifications and algorithms. _Advances in Neural Information Processing Systems 37_, 2023.
* [31] Ziniu Li, Tian Xu, and Yang Yu. A note on target q-learning for solving finite mdps with a generative oracle. _arXiv_, 2203.11489, 2022.
* [32] Zhihan Liu, Miao Lu, Wei Xiong, Han Zhong, Hao Hu, Shenao Zhang, Sirui Zheng, Zhuoran Yang, and Zhaoran Wang. Maximize to explore: One objective function fusing estimation, planning, and exploration. _Advances in Neural Information Processing Systems 36_, 2024.
* [33] Zhihan Liu, Yufeng Zhang, Zuyue Fu, Zhuoran Yang, and Zhaoran Wang. Provably efficient generative adversarial imitation learning for online and offline setting with linear function approximation. _arXiv_, 2108.08765, 2021.
* [34] Fan-Ming Luo, Xingchen Cao, Rong-Jun Qin, and Yang Yu. Transferable reward learning by dynamics-agnostic discriminator ensemble. _arXiv_, 2206.00238, 2022.
* [35] Oier Mees, Dibya Ghosh, Karl Pertsch, Kevin Black, Homer Rich Walke, Sudeep Dasari, Joey Hejna, Tobias Kreiman, Charles Xu, Jianlan Luo, You Liang Tan, Dorsa Sadigh, Chelsea Finn, and Sergey Levine. Octo: An open-source generalist robot policy. In _First Workshop on Vision-Language Models for Navigation and Manipulation at ICRA 2024_, 2024.
* [36] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. _Nature_, 518(7540):529-533, 2015.
* [37] Francesco Orabona. A modern introduction to online learning. _arXiv_, 1912.13213, 2019.
* [38] Ian Osband and Benjamin Van Roy. Model-based reinforcement learning and the eluder dimension. In _Advances in Neural Information Processing Systems 27_, pages 1466-1474, 2014.
* [39] Dean Pomerleau. Efficient training of artificial neural networks for autonomous navigation. _Neural Computation_, 3(1):88-97, 1991.
* [40] Nived Rajaraman, Yanjun Han, Lin Yang, Jingbo Liu, Jiantao Jiao, and Kannan Ramchandran. On the value of interaction and function approximation in imitation learning. In _Advances in Neural Information Processing Systems 34_, pages 1325-1336, 2021.
* [41] Nived Rajaraman, Lin F. Yang, Jiantao Jiao, and Kannan Ramchandran. Toward the fundamental limits of imitation learning. In _Advances in Neural Information Processing Systems 33_, pages 2914-2924, 2020.
* [42] Juntao Ren, Gokul Swamy, Zhiwei Steven Wu, J Andrew Bagnell, and Sanjiban Choudhury. Hybrid inverse reinforcement learning. _Proceedings of the 41st International Conference on Machine Learning_, 2024.
* [43] Stephane Ross and Drew Bagnell. Efficient reductions for imitation learning. In _Proceedings of the 13rd International Conference on Artificial Intelligence and Statistics_, pages 661-668, 2010.

* Ross et al. [2011] Stephane Ross, Geoffrey J. Gordon, and Drew Bagnell. A reduction of imitation learning and structured prediction to no-regret online learning. In _Proceedings of the 14th International Conference on Artificial Intelligence and Statistics_, pages 627-635, 2011.
* Shani et al. [2021] Lior Shani, Tom Zahavy, and Shie Mannor. Online apprenticeship learning. _arXiv_, 2102.06924, 2021.
* Shi et al. [2019] Jing-Cheng Shi, Yang Yu, Qing Da, Shi-Yong Chen, and Anxiang Zeng. Virtual-taobao: virtualizing real-world online retail environment for reinforcement learning. In _Proceedings of the 33rd AAAI Conference on Artificial Intelligence_, pages 4902-4909, 2019.
* Suggala and Netrapalli [2020] Arun Sai Suggala and Praneeth Netrapalli. Online non-convex learning: Following the perturbed leader is optimal. In _Proceedings of the 31st International Conference on Algorithmic Learning Theory_, pages 845-861, 2020.
* Sun et al. [2019] Wen Sun, Anirudh Vermaula, Byron Boots, and Drew Bagnell. Provably efficient imitation learning from observation alone. In _Proceedings of the 36th International Conference on Machine Learning_, pages 6036-6045, 2019.
* Sutton and Barto [2018] Richard S Sutton and Andrew G Barto. _Reinforcement Learning: An Introduction_. MIT press, 2018.
* Swamy et al. [2022] Gokul Swamy, Nived Rajaraman, Matt Peng, Sanjiban Choudhury, J Bagnell, Steven Z Wu, Jiantao Jiao, and Kannan Ramchandran. Minimax optimal online imitation learning via replay estimation. _Advances in Neural Information Processing Systems 35_, pages 7077-7088, 2022.
* Swamy et al. [2023] Gokul Swamy, David Wu, Sanjiban Choudhury, Drew Bagnell, and Steven Wu. Inverse reinforcement learning without reinforcement learning. In _Proceedings of the 40th International Conference on Machine Learning_, 2023.
* Syed and Schapire [2007] Umar Syed and Robert E. Schapire. A game-theoretic approach to apprenticeship learning. In _Advances in Neural Information Processing Systems 20_, pages 1449-1456, 2007.
* Tassa et al. [2018] Yuval Tassa, Yotam Doron, Alistair Muldal, Tom Erez, Yazhe Li, Diego de Las Casas, David Budden, Abbas Abdolmaleki, Josh Merel, Andrew Lefrancq, et al. Deepmind control suite. _arXiv preprint arXiv:1801.00690_, 2018.
* Tiapkin et al. [2022] Daniil Tiapkin, Denis Belomestny, Eric Moulines, Alexey Naumov, Sergey Samsonov, Yunhao Tang, Michal Valko, and Pierre Menard. From dirichlet to rubin: Optimistic exploration in rl without bonuses. In _Proceedings of the 39th International Conference on Machine Learning_, pages 21380-21431, 2022.
* Torabi et al. [2018] Faraz Torabi, Garrett Warnell, and Peter Stone. Behavioral cloning from observation. In _Proceedings of the 27th International Joint Conference on Artificial Intelligence_, pages 4950-4957, 2018.
* Viano et al. [2022] Luca Viano, Angeliki Kamoutsi, Gergely Neu, Igor Krawczuk, and Volkan Cevher. Proximal point imitation learning. _Advances in Neural Information Processing Systems_, 35:24309-24326, 2022.
* Viano et al. [2024] Luca Viano, Stratis Skoulakis, and Volkan Cevher. Better imitation learning in discounted linear MDP, 2024.
* Wainwright [2019] Martin J Wainwright. _High-dimensional statistics: A non-asymptotic viewpoint_. Cambridge University Press, 2019.
* Wang et al. [2020] Ruosong Wang, Russ R Salakhutdinov, and Lin Yang. Reinforcement learning with general value function approximation: Provably efficient approach via bounded eluder dimension. _Advances in Neural Information Processing Systems 33_, pages 6123-6135, 2020.
* Wang et al. [2020] Yizhou Wang, Tianyi Liu, Zhuoran Yang, Xingguo Li, Zhaoran Wang, and Tuo Zhao. On computation and generalization of generative adversarial imitation learning. In _Proceedings of the 8th International Conference on Learning Representations_, 2020.

* [61] Tian Xu, Ziniu Li, and Yang Yu. Error bounds of imitating policies and environments. In _Advances in Neural Information Processing Systems 33_, pages 15737-15749, 2020.
* [62] Tian Xu, Ziniu Li, and Yang Yu. Error bounds of imitating policies and environments for reinforcement learning. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 2021.
* [63] Tian Xu, Ziniu Li, Yang Yu, and Zhi-Quan Luo. Understanding adversarial imitation learning in small sample regime: A stage-coupled analysis. _arXiv_, 2208.01899, 2022.
* [64] Tian Xu, Ziniu Li, Yang Yu, and Zhi-Quan Luo. Provably efficient adversarial imitation learning with unknown transitions. In _Proceedings of the 39th Conference on Uncertainty in Artificial Intelligence_, pages 2367-2378, 2023.
* [65] Tianpei Yang, Hongyao Tang, Chenjia Bai, Jinyi Liu, Jianye Hao, Zhaopeng Meng, Peng Liu, and Zhen Wang. Exploration in deep reinforcement learning: a comprehensive survey. _arXiv_, 2109.06668, 2021.
* [66] Denis Yarats, Rob Fergus, Alessandro Lazaric, and Lerrel Pinto. Mastering visual continuous control: Improved data-augmented reinforcement learning. In _International Conference on Learning Representations_, 2021.
* [67] Yufeng Zhang, Qi Cai, Zhuoran Yang, and Zhaoran Wang. Generative adversarial imitation learning with neural network parameterization: Global optimality and convergence rate. In _Proceedings of the 37th International Conference on Machine Learning_, pages 11044-11054, 2020.
* [68] Han Zhong, Wei Xiong, Sirui Zheng, Liwei Wang, Zhaoran Wang, Zhuoran Yang, and Tong Zhang. A posterior sampling framework for interactive decision making. _arXiv_, 2211.01962, 2022.

Broader Impacts

This study explores the theoretical foundations of adversarial imitation learning with general function approximation and demonstrates the efficiency of the proposed algorithm through standard benchmarks. Although the paper does not reveal any immediate social impacts, the potential practical applications of our research could drive positive change. By broadening the scope of adversarial imitation learning, our work may enable the creation of more efficient and effective solutions in fields such as robotics and autonomous vehicles. Nonetheless, we must recognize the potential for negative consequences if this technology is misused. For example, imitation learning learns from human expert demonstrations and could raise privacy concerns. Therefore, it is essential to ensure that the advancements in imitation learning are applied responsibly and ethically.

## Appendix B Omitted Proof

### Proof of Lemma 1

Lemma 1 is a standard error decomposition lemma in adversarial imitation learning and variants of Lemma 1 have appeared in [45, 57]. According to the definition of \(\overline{\pi}\), we have that

\[V^{\pi^{\mathbb{E}}}-V^{\overline{\pi}} =V^{\pi^{\mathbb{E}}}_{\tau^{\text{true}}}-V^{\overline{\pi}}_{ \tau^{\text{true}}}\] \[=\frac{1}{K}\sum_{k=1}^{K}V^{\pi^{\mathbb{E}}}_{\tau^{\text{true} }}-V^{\pi^{k}}_{\tau^{\text{true}}}\] \[=\frac{1}{K}\sum_{k=1}^{K}\left(V^{\pi^{\mathbb{E}}}_{\tau^{ \text{true}}}-V^{\pi^{k}}_{\tau^{\text{true}}}-\left(V^{\pi^{\mathbb{E}}}_{r^ {k}}-V^{\pi^{k}}_{r^{k}}\right)\right)+\frac{1}{K}\sum_{k=1}^{K}V^{\pi^{ \mathbb{E}}}_{r^{k}}-V^{\pi^{k}}_{r^{k}}.\]

We complete the proof.

### Proof of Theorem 1

In this section, we present the proof of Theorem 1.

To prove Theorem 1, we need the following two useful lemmas which upper bound the reward error and policy error, respectively. Please refer to Appendix B.3 and B.4 for the detailed proof.

**Lemma 2** (Upper Bound on Reward Error).: _Under Assumption 1. For any fixed \(\delta\in(0,1]\), consider Algorithm 1, with probability at least \(1-\delta\),_

\[\frac{1}{K}\sum_{k=1}^{K}V^{\pi^{\mathbb{E}}}_{\tau^{\text{true} }}-V^{\pi^{k}}_{\tau^{\text{true}}}-\left(V^{\pi^{\mathbb{E}}}_{r^{k}}-V^{\pi ^{k}}_{r^{k}}\right) \leq 2H\sqrt{\frac{\log(6\max_{h\in[H]}\mathcal{N}_{\rho}(\mathcal{ R}_{h})/\delta)}{N}}+4H\rho\] \[\quad+2H\sqrt{\frac{\log(3/\delta)}{K}}+\varepsilon^{r}_{\mathrm{ opt}}.\]

**Lemma 3** (Upper Bound on Policy Error).: _Under Assumptions 2, 3 and 4. For any fixed \(\delta\in(0,1]\), with probability at least \(1-\delta\), it holds that_

\[\frac{1}{K}\sum_{k=1}^{K}V^{\pi^{\mathbb{E}}}_{r^{k}}-V^{\pi^{k} }_{r^{k}} \leq\frac{57H^{4}\log(4KH\max_{h\in[H]}\mathcal{N}_{\rho}( \mathcal{Q}_{h})\mathcal{N}_{\rho}(\mathcal{R}_{h})/\delta)+57KH^{3}\rho+ \varepsilon^{Q}_{\mathrm{opt}}}{\lambda}\] \[\quad+\frac{\lambda d_{\mathrm{GEC}}(\varepsilon^{\prime})}{2K}+ \sqrt{\frac{d_{\mathrm{GEC}}(\varepsilon^{\prime})H}{K}}+\varepsilon^{\prime}H.\]

Now we start to prove Theorem 1. With Lemma 1, we can derive that

\[V^{\pi^{\mathbb{E}}}_{\tau^{\text{true}}}-V^{\overline{\pi}}_{ \tau^{\text{true}}}=\frac{1}{K}\sum_{k=1}^{K}V^{\pi^{\mathbb{E}}}_{\tau^{\text {true}}}-V^{\pi^{k}}_{\tau^{\text{true}}}-\left(V^{\pi^{\mathbb{E}}}_{r^{k}}- V^{\pi^{k}}_{r^{k}}\right)+\frac{1}{K}\sum_{k=1}^{K}V^{\pi^{\mathbb{E}}}_{r^{k}}- V^{\pi^{k}}_{r^{k}}.\]Furthermore, Lemma 2 and Lemma 3 offer upper bounds on reward error and policy error, respectively. By union bound, with probability at least \(1-\delta\), we obtain

\[V_{r^{\text{true}}}^{\pi^{\text{E}}}-V_{r^{\text{true}}}^{\pi}\] \[\leq 2H\sqrt{\frac{\log(12\max_{h\in[H]}\mathcal{N}_{\rho}( \mathcal{R}_{h})/\delta)}{N}}+4H\rho+2H\sqrt{\frac{\log(6/\delta)}{K}}+ \varepsilon_{\text{opt}}^{r}\] \[+\frac{57H^{4}\log(8KH\max_{h\in[H]}\mathcal{N}_{\rho}(\mathcal{Q }_{h})\mathcal{N}_{\rho}(\mathcal{R}_{h})/\delta)+57KH^{3}\rho+\varepsilon_{ \text{opt}}^{Q}}{\lambda}\] \[+\frac{\lambda d_{\text{GEC}}(\varepsilon^{\prime})}{2K}+\sqrt{ \frac{d_{\text{GEC}}(\varepsilon^{\prime})H}{K}}+\varepsilon^{\prime}H.\]

We choose \(\varepsilon^{\prime}=\varepsilon/H\) and obtain

\[V_{r^{\text{true}}}^{\pi^{\text{E}}}-V_{r^{\text{true}}}^{\pi}\] \[\leq 2H\sqrt{\frac{\log(12\max_{h\in[H]}\mathcal{N}_{\rho}( \mathcal{R}_{h})/\delta)}{N}}+4H\rho+2H\sqrt{\frac{\log(6/\delta)}{K}}\] \[\quad+\frac{57H^{4}\log(8KH\max_{h\in[H]}\mathcal{N}_{\rho}( \mathcal{Q}_{h})\mathcal{N}_{\rho}(\mathcal{R}_{h})/\delta)+57KH^{3}\rho}{ \lambda}\] \[\quad+\frac{\lambda d_{\text{GEC}}}{2K}+\sqrt{\frac{d_{\text{GEC }}H}{K}}+\varepsilon_{\text{opt}}^{r}+\frac{\varepsilon_{\text{opt}}^{Q}}{ \lambda}+\varepsilon,\]

where \(d_{\text{GEC}}:=d_{\text{GEC}}(\varepsilon/H)\). By choosing the regularization coefficient

\[\lambda=\sqrt{\frac{114KH^{4}\log(8KH\max_{h\in[H]}\mathcal{N}_{\rho}( \mathcal{Q}_{h})\mathcal{N}_{\rho}(\mathcal{R}_{h})/\delta)+114K^{2}H^{3}\rho }{d_{\text{GEC}}}},\]

we further obtain

\[V_{r^{\text{true}}}^{\pi^{\text{E}}}-V_{r^{\text{true}}}^{\pi}\] \[\leq 2H\sqrt{\frac{\log(12\max_{h\in[H]}\mathcal{N}_{\rho}( \mathcal{R}_{h})/\delta)}{N}}+4H\rho+2H\sqrt{\frac{\log(6/\delta)}{K}}\] \[\quad+\sqrt{\frac{114H^{4}d_{\text{GEC}}\log(8KH\max_{h\in[H]} \mathcal{N}_{\rho}(\mathcal{Q}_{h})\mathcal{N}_{\rho}(\mathcal{R}_{h})/\delta )}{K}}+114H^{3}d_{\text{GEC}}\rho}+\sqrt{\frac{d_{\text{GEC}}H}{K}}\] \[\quad+\varepsilon_{\text{opt}}^{r}+\frac{\varepsilon_{\text{opt} }^{Q}}{\lambda}+\varepsilon\] \[\stackrel{{\text{(a)}}}{{\leq}}2H\sqrt{\frac{\log(12 \max_{h\in[H]}\mathcal{N}_{\rho}(\mathcal{R}_{h})/\delta)}{N}}+4H\rho+2H\sqrt {\frac{\log(6/\delta)}{K}}\] \[\quad+\sqrt{\frac{114H^{4}d_{\text{GEC}}\log(8KH\max_{h\in[H]} \mathcal{N}_{\rho}(\mathcal{Q}_{h})\mathcal{N}_{\rho}(\mathcal{R}_{h})/\delta )}{K}}+\sqrt{114H^{3}d_{\text{GEC}}\rho}+\sqrt{\frac{d_{\text{GEC}}H}{K}}\] \[\quad+\varepsilon_{\text{opt}}^{r}+\frac{\varepsilon_{\text{opt }}^{Q}}{\lambda}+\varepsilon\] \[\leq 2H\sqrt{\frac{\log(12\max_{h\in[H]}\mathcal{N}_{\rho}( \mathcal{R}_{h})/\delta)}{N}}+4H\rho+2H\sqrt{\frac{\log(6/\delta)}{K}}\] \[\quad+2\sqrt{\frac{114H^{4}d_{\text{GEC}}\log(8KH\max_{h\in[H]} \mathcal{N}_{\rho}(\mathcal{Q}_{h})\mathcal{N}_{\rho}(\mathcal{R}_{h})/\delta )}{K}}+\sqrt{54H^{3}d_{\text{GEC}}\rho}\] \[\quad+\varepsilon_{\text{opt}}^{r}+\frac{\varepsilon_{\text{opt} }^{Q}}{\lambda}+\varepsilon\] \[\stackrel{{\text{(b)}}}{{\leq}}2H\sqrt{\frac{\log(12 \max_{h\in[H]}\mathcal{N}_{\rho}(\mathcal{R}_{h})/\delta)}{N}}+2H\sqrt{\frac{ \log(6/\delta)}{K}}\] \[\quad+2\sqrt{\frac{H^{4}d_{\text{GEC}}\log(8KH\max_{h\in[H]} \mathcal{N}_{\rho}(\mathcal{Q}_{h})\mathcal{N}_{\rho}(\mathcal{R}_{h})/\delta )}{K}}+\varepsilon_{\text{opt}}^{r}+\frac{\varepsilon_{\text{opt}}^{Q}}{ \lambda}+3\varepsilon\]Inequality (a) follows \(\sqrt{a+b}\leq\sqrt{a}+\sqrt{b},\ \forall a,b\geq 0\) and inequality (b) holds because of the choice \(\rho=\varepsilon^{2}/(54H^{3}d_{\mathrm{GEC}}+4H)\). Now we determine the number of expert trajectories and the number of interaction trajectories. With Lemma 8, when the expert sample complexity and interaction complexity satisfies

\[N \geq 4\frac{H^{2}\log(12\max_{h\in[H]}\mathcal{N}_{\rho}(\mathcal{ R}_{h})/\delta)}{\varepsilon^{2}},\] \[K \geq 2304\frac{(H^{4}d_{\mathrm{GEC}}\log(768H^{5/2}d_{\mathrm{GEC }}^{1/2}\max_{h\in[H]}\mathcal{N}_{\rho}(\mathcal{Q}_{h})\mathcal{N}_{\rho}( \mathcal{R}_{h})/(\delta\varepsilon))+H^{2}\log(6/\delta))}{\varepsilon^{2}},\]

we have that

\[V_{r^{\mathrm{true}}}^{\pi^{\mathrm{E}}}-V_{r^{\mathrm{true}}}^{ \pi^{\mathrm{E}}}\leq 6\varepsilon+\varepsilon_{\mathrm{opt}}^{r}+\frac{ \varepsilon_{\mathrm{opt}}^{Q}}{\lambda}.\]

Scaling \(\varepsilon\) as \(\varepsilon/6\) completes the proof.

### Proof of Lemma 2

To prove Lemma 2, we first perform the following error decomposition.

\[\frac{1}{K}\sum_{k=1}^{K}V_{r^{\mathrm{true}}}^{\pi^{\mathrm{E}}} -V_{r^{\mathrm{true}}}^{\pi^{k}}-\left(V_{r^{k}}^{\pi^{\mathrm{E}}}-V_{r^{k}} ^{\pi^{k}}\right)\] \[=\frac{1}{K}\sum_{k=1}^{K}\left(\widehat{V}_{r^{\mathrm{true}}}^{ \pi^{\mathrm{E}}}-\widehat{V}_{r^{\mathrm{true}}}^{\pi^{k}}-\left(\widehat{V} _{r^{k}}^{\pi^{\mathrm{E}}}-\widehat{V}_{r^{k}}^{\pi^{k}}\right)\right)+V_{r^{ \mathrm{true}}}^{\pi^{\mathrm{E}}}-\widehat{V}_{r^{\mathrm{true}}}^{\pi^{ \mathrm{E}}}+\frac{1}{K}\sum_{k=1}^{K}\widehat{V}_{r^{k}}^{\pi^{\mathrm{E}}}- V_{r^{k}}^{\pi^{\mathrm{E}}} \tag{4}\] \[+\frac{1}{K}\sum_{k=1}^{K}\widehat{V}_{r^{\mathrm{true}}}^{\pi^{k }}-V_{r^{\mathrm{true}}}^{\pi^{k}}+\frac{1}{K}\sum_{k=1}^{K}V_{r^{k}}^{\pi^{k }}-\widehat{V}_{r^{k}}^{\pi^{k}}.\]

Recall that for any reward function \(r\), \(\widehat{V}_{r}^{\pi^{i}}\) and \(\widehat{V}_{r}^{\pi^{\mathrm{E}}}\) are unbiased estimations of \(V_{r}^{\pi^{i}}\) and \(V_{r}^{\pi^{\mathrm{E}}}\), respectively.

\[\widehat{V}_{r}^{\pi^{i}}=\sum_{h=1}^{H}r_{h}(s_{h}^{i},a_{h}^{i} ),\ \widehat{V}_{r}^{\pi^{\mathrm{E}}}=\frac{1}{N}\sum_{\tau\in\mathcal{D}^{ \mathrm{E}}}\sum_{h=1}^{H}r_{h}(\tau(s_{h}),\tau(a_{h})).\]

The first term in the RHS of Eq. (4) is the estimated reward error while the remaining terms are estimation errors. To upper bound the first term, we have

\[\frac{1}{K}\sum_{k=1}^{K}\widehat{V}_{r^{\mathrm{true}}}^{\pi^{ \mathrm{E}}}-\widehat{V}_{r^{\mathrm{true}}}^{\pi^{k}}-\left(\widehat{V}_{r^{ k}}^{\pi^{\mathrm{E}}}-\widehat{V}_{r^{k}}^{\pi^{k}}\right)\] \[=\frac{1}{K}\sum_{k=1}^{K}\widehat{V}_{r^{k}}^{\pi^{k}}-\widehat{ V}_{r^{k}}^{\pi^{\mathrm{E}}}-\left(\widehat{V}_{r^{\mathrm{true}}}^{\pi^{k}}- \widehat{V}_{r^{\mathrm{true}}}^{\pi^{\mathrm{E}}}\right)\] \[\leq\frac{1}{K}\max_{r\in\mathcal{R}}\sum_{k=1}^{K}\widehat{V}_{r ^{k}}^{\pi^{\mathrm{E}}}-\widehat{V}_{r^{k}}^{\pi^{k}}-\left(\widehat{V}_{r}^{ \pi^{k}}-\widehat{V}_{r}^{\pi^{\mathrm{E}}}\right)\] \[\stackrel{{\mathrm{(c)}}}{{=}}\varepsilon_{\mathrm{opt }}^{r}.\]

Equation (c) follows the definition of reward optimization error in Definition 2. Then we can obtain

\[\frac{1}{K}\sum_{k=1}^{K}V_{r^{\mathrm{true}}}^{\pi^{\mathrm{E}}} -V_{r^{\mathrm{true}}}^{\pi^{k}}-\left(V_{r^{k}}^{\pi^{\mathrm{E}}}-V_{r^{k}}^ {\pi^{k}}\right)\] \[\leq V_{r^{\mathrm{true}}}^{\pi^{\mathrm{E}}}-\widehat{V}_{r^{ \mathrm{true}}}^{\pi^{\mathrm{E}}}+\frac{1}{K}\sum_{k=1}^{K}\widehat{V}_{r^{k}} ^{\pi^{\mathrm{E}}}-V_{r^{k}}^{\pi^{\mathrm{E}}}+\frac{1}{K}\sum_{k=1}^{K} \widehat{V}_{r^{\mathrm{true}}}^{\pi^{k}}-V_{r^{\mathrm{true}}}^{\pi^{k}}+ \frac{1}{K}\sum_{k=1}^{K}V_{r^{k}}^{\pi^{k}}-\widehat{V}_{r^{k}}^{\pi^{k}}+ \varepsilon_{\mathrm{opt}}^{r}.\]Then we proceed to upper bound the estimation errors. First, we first upper bound the estimation error caused by using \(\widehat{V}_{r}^{\pi^{\text{E}}}\) to approximate \(V_{r}^{\pi^{\text{E}}}\). In particular, we have that

\[\left|\widehat{V}_{r}^{\pi^{\text{E}}}-V_{r}^{\pi^{\text{E}}}\right| =\left|\frac{1}{N}\sum_{\tau\in\mathcal{D}^{\text{E}}}\sum_{h=1}^{ H}r_{h}(s_{h}(\tau),a_{h}(\tau))-\mathbb{E}\left[\sum_{h=1}^{H}r_{h}(s_{h},a_{h}) \middle|\pi^{\text{E}}\right]\right|\] \[=\left|\sum_{h=1}^{H}\frac{1}{N}\sum_{\tau\in\mathcal{D}^{\text{E }}}r_{h}(s_{h}(\tau),a_{h}(\tau))-\sum_{h=1}^{H}\mathbb{E}\left[r_{h}(s_{h},a_{ h})\middle|\pi^{\text{E}}\right]\right|\] \[\leq\sum_{h=1}^{H}\left|\frac{1}{N}\sum_{\tau\in\mathcal{D}^{ \text{E}}}r_{h}(s_{h}(\tau),a_{h}(\tau))-\mathbb{E}\left[r_{h}(s_{h},a_{h}) \middle|\pi^{\text{E}}\right]\right|.\]

By Hoeffding's inequality [58], for any fixed timestep \(h\in[H]\) and any fixed reward function \(r_{h}\in\mathcal{R}_{h}\), with probability at least \(1-\delta\), we have that

\[\left|\frac{1}{N}\sum_{\tau\in\mathcal{D}^{\text{E}}}r_{h}(s_{h}(\tau),a_{h}( \tau))-\mathbb{E}\left[r_{h}(s_{h},a_{h})\middle|\pi^{\text{E}}\right]\right| \leq\sqrt{\frac{\log(2/\delta)}{N}}.\]

Let \((\mathcal{R}_{h})_{\rho}\) be a \(\rho\)-cover of \(\mathcal{R}\). By union bound, with probability at least \(1-\delta\), for all \(h\in[H]\) and all \(\widehat{r}_{h}\in(\mathcal{R}_{h})_{\rho}\), we have that

\[\left|\frac{1}{N}\sum_{\tau\in\mathcal{D}^{\text{E}}}\widehat{r}_{h}(s_{h}( \tau),a_{h}(\tau))-\mathbb{E}\left[\widehat{r}_{h}(s_{h},a_{h})\middle|\pi^{ \text{E}}\right]\right|\leq\sqrt{\frac{\log(2H|(\mathcal{R}_{h})_{\rho}|/ \delta)}{N}}.\]

Then with probability at least \(1-\delta\), for all \(\widehat{r}=(\widehat{r}_{1},\ldots,\widehat{r}_{H})\in(\mathcal{R}_{1})_{ \rho}\times\ldots\times(\mathcal{R}_{1})_{\rho}\),

\[\left|\widehat{V}_{\widehat{r}}^{\pi^{\text{E}}}-V_{\widehat{r}} ^{\pi^{\text{E}}}\right| \leq\sum_{h=1}^{H}\left|\frac{1}{N}\sum_{\tau\in\mathcal{D}^{\text {E}}}\widehat{r}_{h}(s_{h}(\tau),a_{h}(\tau))-\mathbb{E}\left[\widehat{r}_{h} (s_{h},a_{h})\middle|\pi^{\text{E}}\right]\right|\] \[\leq\sum_{h=1}^{H}\sqrt{\frac{\log(2|(\mathcal{R}_{h})_{\rho}|/ \delta)}{N}}\] \[\leq H\sqrt{\frac{\log(2\max_{h\in[H]}|(\mathcal{R}_{h})_{\rho}|/ \delta)}{N}}.\]

According to the definition of \(\rho\)-cover, for any reward function \(r=(r_{1},\ldots,r_{H})\in\mathcal{R}\), there exists \(\widehat{r}=(\widehat{r}_{1},\ldots,\widehat{r}_{H})\in(\mathcal{R}_{1})_{ \rho}\times\ldots\times(\mathcal{R}_{1})_{\rho}\) such that \(\forall h\in[H],\ \max_{(s,a)\in\mathcal{S}\times\mathcal{A}}|r_{h}(s,a)-\widehat{ r}_{h}(s,a)|\leq\rho\). Then we have that

\[\left|\widehat{V}_{r}^{\pi^{\text{E}}}-\widehat{V}_{\widehat{r}} ^{\pi^{\text{E}}}\right| \leq\frac{1}{N}\sum_{\tau\in\mathcal{D}^{\text{E}}}\sum_{h=1}^{ H}|r_{h}(s_{h}(\tau),a_{h}(\tau))-\widehat{r}_{h}(s_{h}(\tau),a_{h}(\tau))|\leq H\rho,\] \[\left|V_{r}^{\pi^{\text{E}}}-V_{\widehat{r}}^{\pi^{\text{E}}}\right| \leq\mathbb{E}\left[\sum_{h=1}^{H}|r_{h}(s_{h},a_{h})-\widehat{r}_{h}(s_{h},a_{h})|\middle|\pi^{\text{E}}\right]\leq H\rho.\]

Then, with probability at least \(1-\delta\), for all reward function \(r\in\mathcal{R}\), we have that

\[\left|\widehat{V}_{r}^{\pi^{\text{E}}}-V_{r}^{\pi^{\text{E}}}\right| \leq\left|\widehat{V}_{\widehat{r}}^{\pi^{\text{E}}}-V_{\widehat {r}}^{\pi^{\text{E}}}\right|+2H\rho\] \[\leq H\sqrt{\frac{\log(2\max_{h\in[H]}|(\mathcal{R}_{h})_{\rho} |/\delta)}{N}}+2H\rho\] \[\leq H\sqrt{\frac{\log(2\max_{h\in[H]}|\mathcal{N}_{\rho}( \mathcal{R}_{h})/\delta)}{N}}+2H\rho. \tag{5}\]

Now we have obtained the upper bound on the estimation error \(|\widehat{V}_{r}^{\pi^{\text{E}}}-V_{r}^{\pi^{\text{E}}}|\). Then we proceed to upper bound the estimation error \((1/K)\cdot\sum_{k=1}^{K}\widehat{V}_{r}^{\pi^{\text{E}}}-V_{r}^{\pi^{\text{E} }}\) and \((1/K)\cdot\sum_{k=1}^{K}V_{r}^{\pi^{k}}-\widehat{V}_{r}^{\pi^{k}}\). With the Hoeffding's inequality [58, 25], with probability at least \(1-\delta\), we obtain that

\[\frac{1}{K}\sum_{k=1}^{K}\widehat{V}_{r}^{\pi^{k}}-V_{r}^{\pi^{k}}\leq H\sqrt{ \frac{\log(1/\delta)}{K}}. \tag{6}\]We proceed to analyze the term \(\sum_{k=1}^{K}V_{r^{k}}^{\pi^{k}}-\widehat{V}_{r^{k}}^{\pi^{k}}\). Notice that \(r^{k}\) are learned from historical trajectories \(\{\tau^{1},\ldots,\tau^{k-1}\}\) and thus statistically depends on \(\{\tau^{1},\ldots,\tau^{k-1}\}\). Therefore, \(\widehat{V}_{r^{1}}^{\pi^{1}},\cdots,\widehat{V}_{r^{k}}^{\pi^{k}}\) are not independent and the standard Hoeffding's inequality is not applicable. To address this issue, we apply Azuma-Hoeffding's inequality [58] for martingale. In particular, we define \(\mathcal{F}^{k}\) as the filtration induced by \(\{\tau^{1},\cdots,\tau^{k}\}\) and can obtain that

\[\mathbb{E}\left[V_{r^{k}}^{\pi^{k}}-\widehat{V}_{r^{k}}^{\pi^{k}}|\mathcal{F}^{ k-1}\right]=0.\]

Therefore, \(\{(V_{r^{k}}^{\pi^{k}}-\widehat{V}_{r^{k}}^{\pi^{k}},\mathcal{F}^{k})\}_{k=1}^ {\infty}\) is a martingale difference sequence. With Azuma-Hoeffding's inequality, we can derive that with probability at least \(1-\delta\),

\[\frac{1}{K}\sum_{k=1}^{K}V_{r^{k}}^{\pi^{k}}-\widehat{V}_{r^{k}}^{\pi^{k}}\leq H \sqrt{\frac{\log(1/\delta)}{K}}. \tag{7}\]

In summary, we have derived the following three high-probability inequalities: Eq. (5), Eq. (6) and Eq. (7). With union bound, with probability at least \(1-\delta\), it holds that

\[\forall r\in\mathcal{R},\left|\widehat{V}_{r}^{\pi^{k}}-V_{r}^{ \pi^{k}}\right|\leq H\sqrt{\frac{\log(6\max_{h\in[H]}\mathcal{N}_{\rho}( \mathcal{R}_{h})/\delta)}{N}}+2H\rho,\] \[\frac{1}{K}\sum_{k=1}^{K}\widehat{V}_{r^{\prime\text{true}}}^{\pi ^{k}}-V_{r^{\prime\text{true}}}^{\pi^{k}}\leq H\sqrt{\frac{\log(3/\delta)}{K}},\ \frac{1}{K}\sum_{k=1}^{K}V_{r^{k}}^{\pi^{k}}-\widehat{V}_{r^{k}}^{\pi^{k}} \leq H\sqrt{\frac{\log(3/\delta)}{K}}.\]

With the above three inequalities, we can derive that

\[\frac{1}{K}\sum_{k=1}^{K}V_{r^{\prime\text{true}}}^{\pi^{k}}-V_{ r^{\prime\text{true}}}^{\pi^{k}}-\left(V_{r^{k}}^{\pi^{k}}-V_{r^{k}}^{\pi^{k}}\right)\] \[\leq V_{r^{\prime\text{true}}}^{\pi^{k}}-\widehat{V}_{r^{\prime \text{true}}}^{\pi^{k}}+\frac{1}{K}\sum_{k=1}^{K}\widehat{V}_{r^{k}}^{\pi^{k}} -V_{r^{k}}^{\pi^{k}}+\frac{1}{K}\sum_{k=1}^{K}\widehat{V}_{r^{\prime\text{true} }}^{\pi^{k}}-V_{r^{\prime\text{true}}}^{\pi^{k}}+\frac{1}{K}\sum_{k=1}^{K}V_{r ^{k}}^{\pi^{k}}-\widehat{V}_{r^{k}}^{\pi^{k}}+\varepsilon_{\text{opt}}^{r}\] \[\leq 2H\sqrt{\frac{\log(6\max_{h\in[H]}\mathcal{N}_{\rho}( \mathcal{R}_{h})/\delta)}{N}}+4H\rho+2H\sqrt{\frac{\log(3/\delta)}{K}}+ \varepsilon_{\text{opt}}^{r}.\]

We complete the proof.

### Proof of Lemma 3

To prove Lemma 3, we need the following two auxiliary lemmas. The detailed proof is presented in Appendix B.5 and Appendix B.6.

**Lemma 4**.: _For any fixed \(\delta\in(0,1]\), with probability at least \(1-\delta\),_

\[\forall k\in[K],\mathrm{BE}^{k}(Q^{\star,r^{k}})\leq 16H^{4}\log\left(KH \max_{h\in[H]}\mathcal{N}_{\rho}(\mathcal{Q}_{h})\mathcal{N}_{\rho}(\mathcal{ R}_{h})/\delta\right)+30kH^{3}\rho.\]

**Lemma 5**.: _For any fixed \(\delta\in(0,1]\), with probability at least \(1-\delta\),_

\[\forall k\in[K],\mathrm{BE}^{k}(Q^{k}) \geq\frac{1}{2}\sum_{i=0}^{k-1}\mathbb{E}\left[\sum_{h=1}^{H} \left(Q_{h}^{k}(s_{h}^{i},a_{h}^{i})-(\mathcal{T}_{h}^{\pi^{k}}Q_{h+1}^{k})(s_ {h}^{i},a_{h}^{i})\right)^{2}\left|\pi^{i}\right|\right.\] \[\quad-41H^{4}\log\left(2KH\max_{h\in[H]}\mathcal{N}_{\rho}( \mathcal{Q}_{h})\mathcal{N}_{\rho}(\mathcal{R}_{h})/\delta\right)-27kH^{2}\rho.\]

Now we proceed to analyze the policy error. First of all, we perform the following error decomposition.

\[\frac{1}{K}\sum_{k=1}^{K}V_{r^{k}}^{\pi^{k}}-V_{r^{k}}^{\pi^{k}}\]\[\leq\frac{1}{K}\sum_{k=1}^{K}V_{r^{k}}^{\star}-V_{r^{k}}^{\pi^{k}}\] \[=\frac{1}{K}\sum_{k=1}^{K}\left(V_{r^{k}}^{\star}-Q_{1}^{k}(s_{1}, \pi^{k})\right)+\frac{1}{K}\sum_{k=1}^{K}\left(Q_{1}^{k}(s_{1},\pi^{k})-V_{r^{ k}}^{\pi^{k}}\right)\] \[=\frac{1}{K}\sum_{k=1}^{K}\left(\max_{a\in\mathcal{A}}Q_{1}^{\star,r^{k}}(s_{1},a)-\max_{a\in\mathcal{A}}Q_{1}^{k}(s_{1},a)\right)+\frac{1}{K} \sum_{k=1}^{K}\left(Q_{1}^{k}(s_{1},\pi^{k})-V_{r^{k}}^{\pi^{k}}\right).\]

Here \(V_{r^{k}}^{\star}\) denotes the optimal policy value under reward \(r^{k}\).

From line 4 in Algorithm 1, we know that \(Q^{k}\) is an approximate solution of \(\min_{Q\in\mathcal{Q}}\mathcal{L}^{k}(Q)\) with an error \(\varepsilon_{\text{opt}}^{Q}\). With \(Q^{\star,r^{k}}\in\mathcal{Q}\) from Assumption 2, we have that

\[\operatorname{BE}^{k}(Q^{k})-\lambda\max_{a\in\mathcal{A}}Q_{1}^{k}(s_{1},a) \leq\operatorname{BE}^{k}(Q^{\star,r^{k}})-\lambda\max_{a\in\mathcal{A}}Q_{1} ^{\star,r^{k}}(s_{1},a)+\varepsilon_{\text{opt}}^{Q}.\]

Rearrange the above inequality yields that

\[\max_{a\in\mathcal{A}}Q_{1}^{\star,r^{k}}(s_{1},a)-\max_{a\in\mathcal{A}}Q_{1} ^{k}(s_{1},a)\leq\frac{1}{\lambda}\left(\operatorname{BE}^{k}(Q^{\star,r^{k}})- \operatorname{BE}^{k}(Q^{k})\right)+\frac{\varepsilon_{\text{opt}}^{Q}}{\lambda}.\]

From Lemma 4, with probability at least \(1-\delta\), we have

\[\operatorname{BE}^{k}(Q^{\star,r^{k}})\leq 16H^{4}\log\left(KH\max_{h\in[H]} \mathcal{N}_{\rho}(\mathcal{Q}_{h})\mathcal{N}_{\rho}(\mathcal{R}_{h})/\delta \right)+30kH^{3}\rho.\]

On the other hand, with probability at least \(1-\delta\), we have

\[\operatorname{BE}^{k}(Q^{k}) \geq\frac{1}{2}\sum_{i=0}^{k-1}\mathbb{E}\left[\sum_{h=1}^{H} \left(Q_{h}^{k}(s_{h}^{i},a_{h}^{i})-(\mathcal{T}_{h}^{\pi^{k}}Q_{h+1}^{k})(s _{h}^{i},a_{h}^{i})\right)^{2}\left|\pi^{i}\right|\right.\] \[\quad-41H^{4}\log\left(2KH\max_{h\in[H]}\mathcal{N}_{\rho}( \mathcal{Q}_{h})\mathcal{N}_{\rho}(\mathcal{R}_{h})/\delta\right)-27kH^{2}\rho.\]

By union bound, with probability at least \(1-\delta\),

\[\max_{a\in\mathcal{A}}Q_{1}^{\star,r^{k}}(s_{1},a)-\max_{a\in \mathcal{A}}Q_{1}^{k}(s_{1},a)\] \[\leq-\frac{1}{2\lambda}\sum_{i=0}^{k-1}\mathbb{E}\left[\prod_{h=1 }^{H}\left(Q_{h}^{k}(s_{h}^{i},a_{h}^{i})-(\mathcal{T}_{h}^{\pi^{k}}Q_{h+1}^{k })(s_{h}^{i},a_{h}^{i})\right)^{2}\left|\pi^{i}\right|\right.\] \[\quad+\frac{57H^{4}\log(4KH\max_{h\in[H]}\mathcal{N}_{\rho}( \mathcal{Q}_{h})\mathcal{N}_{\rho}(\mathcal{R}_{h})/\delta)+57kH^{3}\rho+ \varepsilon_{\text{opt}}^{Q}}{\lambda}.\]

Then we have that

\[\frac{1}{K}\sum_{k=1}^{K}V_{r^{k}}^{\pi^{k}}-V_{r^{k}}^{\pi^{k}}\] \[\leq-\frac{1}{2\lambda}\frac{1}{K}\sum_{k=1}^{K}\sum_{i=0}^{k-1} \mathbb{E}\left[\sum_{h=1}^{H}\left(Q_{h}^{k}(s_{h}^{i},a_{h}^{i})-(\mathcal{ T}_{h}^{\pi^{k}}Q_{h+1}^{k})(s_{h}^{i},a_{h}^{i})\right)^{2}\left|\pi^{i}\right|\right.\] \[\quad+\frac{57H^{4}\log(4KH\max_{h\in[H]}\mathcal{N}_{\rho}( \mathcal{Q}_{h})\mathcal{N}_{\rho}(\mathcal{R}_{h})/\delta)+57KH^{3}\rho+ \varepsilon_{\text{opt}}^{Q}}{\lambda}\] \[\quad+\frac{1}{K}\sum_{k=1}^{K}\left(Q_{1}^{k}(s_{1},\pi^{k})-V_{ r^{k}}^{\pi^{k}}\right).\]

Now we upper bound the last term in RHS of the above inequality. From Assumption 4, for any \(\mu\geq 0\), it holds that

\[\frac{1}{K}\sum_{k=1}^{K}Q_{1}^{k}(s_{1},\pi^{k})-V_{r^{k}}^{\pi^{k}}\leq\frac {\mu}{2K}\sum_{k=1}^{K}\sum_{i=1}^{k-1}\mathbb{E}\left[\sum_{h=1}^{H}\left(Q_ {h}^{k}(s_{h},a_{h})-\mathcal{T}_{h}^{\pi^{k}}Q_{h+1}^{k}(s_{h},a_{h})\right) ^{2}\left|\pi^{i}\right|\right.+\frac{d}{2\mu K}\]\[+\sqrt{\frac{dH}{K}}+\varepsilon H\] \[+\sqrt{\frac{dH}{K}}+\varepsilon H.\]

The last equation is obtained by setting \(\mu=1/\lambda\). Combining the above two inequalities yields that

\[\frac{1}{K}\sum_{k=1}^{K}V_{r^{k}}^{\pi^{k}}-V_{r^{k}}^{\pi^{k}}\] \[\leq\frac{57H^{4}\log(4KH\max_{h\in[H]}\mathcal{N}_{\rho}( \mathcal{Q}_{h})\mathcal{N}_{\rho}(\mathcal{R}_{h})/\delta)+57KH^{3}\rho+ \varepsilon_{\mathrm{opt}}^{Q}}{\lambda}+\frac{\lambda d}{2K}+\sqrt{\frac{dH}{K }}+\varepsilon H.\]

### Proof of Lemma 4

Recall the definition of the estimated Bellman error.

\[\mathrm{BE}^{k}(Q^{\star,r^{k}}) =\sum_{h=1}^{H}\mathcal{E}_{h}(Q_{h}^{\star,r^{k}},Q_{h+1}^{\star, r^{k}};\mathcal{D}^{k},r^{k})-\inf_{Q_{h}^{\star}\in\mathcal{Q}_{h}}\mathrm{ BE}_{h}(Q_{h}^{\prime},Q_{h+1}^{\star,r^{k}};\mathcal{D}^{k},r^{k})\] \[=\sum_{h=1}^{H}\sum_{i=0}^{k-1}\left(Q_{h}^{\star,r^{k}}(s_{h}^{i },a_{h}^{i})-r_{h}^{k}(s_{h}^{i},a_{h}^{i})-\max_{a^{\prime}}Q_{h+1}^{\star,r^ {k}}(s_{h+1}^{i},a^{\prime})\right)^{2}\] \[\quad-\inf_{Q_{h}^{\star}\in\mathcal{Q}_{h}}\sum_{i=0}^{k-1}\left( Q_{h}^{\prime}(s_{h}^{i},a_{h}^{i})-r_{h}^{k}(s_{h}^{i},a_{h}^{i})-\max_{a^{ \prime}}Q_{h+1}^{\star,r^{k}}(s_{h+1}^{i},a^{\prime})\right)^{2}.\]

For any fixed tuple \((k,h,Q^{\prime},r)\in[K]\times[H]\times\mathcal{Q}\times\mathcal{R}\), we define the random variable

\[Z_{h}^{i}(Q^{\prime},r) :=\left(Q_{h}^{\prime}(s_{h}^{i},a_{h}^{i})-r_{h}(s_{h}^{i},a_{h} ^{i})-\max_{a^{\prime}\in\mathcal{A}}Q_{h+1}^{\star,r}(s_{h+1}^{i},a^{\prime} )\right)^{2}\] \[\quad-\left(Q_{h}^{\star,r}(s_{h}^{i},a_{h}^{i})-r_{h}(s_{h}^{i}, a_{h}^{i})-\max_{a^{\prime}\in\mathcal{A}}Q_{h+1}^{\star,r}(s_{h+1}^{i},a^{ \prime})\right)^{2}.\]

Furthermore, we define the filtration \(\mathcal{F}_{h}^{i}=\sigma(\{(s_{1}^{j},a_{1}^{j},\ldots,s_{H}^{j},a_{H}^{j}) \}_{j=0}^{i-1}\cup\{s_{1}^{i},a_{1}^{i},\ldots,s_{h}^{i},a_{h}^{i}\})\). Then we calculate the expectation and variance of \(Z_{h}^{i}(Q^{\prime},r)\) conditioned on \(\mathcal{F}_{h}^{i}\).

\[\mathbb{E}\left[Z_{h}^{i}(Q^{\prime},r)|\mathcal{F}_{h}^{i}\right]\] \[=\mathbb{E}\left[\left(Q_{h}^{\prime}(s_{h}^{i},a_{h}^{i})-Q_{h}^ {\star,r}(s_{h}^{i},a_{h}^{i})+Q_{h}^{\star,r}(s_{h}^{i},a_{h}^{i})-r_{h}(s_{h }^{i},a_{h}^{i})-\max_{a^{\prime}\in\mathcal{A}}Q_{h+1}^{\star,r}(s_{h+1}^{i},a^{\prime})\right)^{2}\bigg{|}\mathcal{F}_{h}^{i}\right]\] \[\quad-\mathbb{E}\left[\left(Q_{h}^{\star,r}(s_{h}^{i},a_{h}^{i})- r_{h}(s_{h}^{i},a_{h}^{i})-\max_{a^{\prime}\in\mathcal{A}}Q_{h+1}^{\star,r}(s_{h+1}^ {i},a^{\prime})\right)^{2}\bigg{|}\mathcal{F}_{h}^{i}\right]\] \[=\mathbb{E}\left[\left(Q_{h}^{\prime}(s_{h}^{i},a_{h}^{i})-Q_{h}^ {\star,r}(s_{h}^{i},a_{h}^{i})\right)^{2}\bigg{|}\mathcal{F}_{h}^{i}\right]\] \[\quad+2\mathbb{E}\left[\left(Q_{h}^{\prime}(s_{h}^{i},a_{h}^{i})- Q_{h}^{\star,r}(s_{h}^{i},a_{h}^{i})\right)\left(Q_{h}^{\star,r}(s_{h}^{i},a_{h}^{i})-r_{h}(s _{h}^{i},a_{h}^{i})-\max_{a^{\prime}\in\mathcal{A}}Q_{h+1}^{\star,r}(s_{h+1}^{ i},a^{\prime})\right)\bigg{|}\mathcal{F}_{h}^{i}\right]\] \[=\left(Q_{h}^{\prime}(s_{h}^{i},a_{h}^{i})-Q_{h}^{\star,r}(s_{h} ^{i},a_{h}^{i})\right)^{2}\] \[\quad+2\left(Q_{h}^{\prime}(s_{h}^{i},a_{h}^{i})-Q_{h}^{\star,r}( s_{h}^{i},a_{h}^{i})\right)\mathbb{E}\left[Q_{h}^{\star,r}(s_{h}^{i},a_{h}^{i})-r_{h}(s _{h}^{i},a_{h}^{i})-\max_{a^{\prime}\in\mathcal{A}}Q_{h+1}^{\star,r}(s_{h+1}^{i},a^{\prime})\bigg{|}\mathcal{F}_{h}^{i}\right]\] \[=\left(Q_{h}^{\prime}(s_{h}^{i},a_{h}^{i})-Q_{h}^{\star,r}(s_{h}^{i},a_{h}^{i})\right)^{2}\]\[\leq 36H^{2}\eta\sum_{i=0}^{k-1}\mathbb{E}\left[Z_{h}^{i}(Q^{\prime},r) \bigg{|}\mathcal{F}_{h}^{i}\right]+\frac{\log(1/\delta)}{\eta}.\]

This implies that

\[-\sum_{i=0}^{k-1}Z_{h}^{i}(Q^{\prime},r) \leq\left(36H^{2}\eta-1\right)\sum_{i=0}^{k-1}\mathbb{E}\left[Z_{ h}^{i}(Q^{\prime},r)\bigg{|}\mathcal{F}_{h}^{i}\right]+\frac{\log(1/\delta)}{\eta}\] \[\leq 16H^{2}\log(1/\delta).\]

The last equation is obtained by choosing \(\eta=1/(16H^{2})\).

We define \((\mathcal{Q}_{h})_{\rho}\) and \((\mathcal{R}_{h})_{\rho}\) as the \(\rho\)-cover of \(\mathcal{Q}_{h}\) and \(\mathcal{R}_{h}\), respectively. It is direct to have that \(\mathcal{Q}_{\rho}=(\mathcal{Q}_{1})_{\rho}\times\ldots(\mathcal{Q}_{H})_{\rho}\) and \(\mathcal{R}_{\rho}=(\mathcal{R}_{1})_{\rho}\times\ldots(\mathcal{R}_{H})_{\rho}\) are \(\rho\)-covers of \(\mathcal{Q}\) and \(\mathcal{R}\), respectively. By union bound, with probability at least \(1-\delta\), for all \((k,h,\widehat{Q},\widehat{r})\in[K]\times[H]\times\mathcal{Q}_{\rho}\times \mathcal{R}_{\rho}\), we have that

\[-\sum_{i=0}^{k-1}Z_{h}^{i}(\widehat{Q},\widehat{r})\leq 16H^{2}\log\left(KH \prod_{h=1}^{H}(|(\mathcal{Q}_{h})_{\rho}||(\mathcal{R}_{h})_{\rho}|)/\delta\right)\]\[\leq 16H^{3}\log\left(KH\max_{h\in[H]}|(\mathcal{Q}_{h})_{\rho}||( \mathcal{R}_{h})_{\rho}|/\delta\right).\]

Furthermore, for any \((Q,r)\in\mathcal{Q}\times\mathcal{R}\), there exists \((\widehat{Q},\widehat{r})\in\mathcal{Q}_{\rho}\times\mathcal{R}_{\rho}\) such that \(\|Q-\widehat{Q}\|_{\infty}\leq\rho\) and \(\|r-\widehat{r}\|_{\infty}\leq\rho\). Then we have that

\[\left|\sum_{i=0}^{k-1}Z_{h}^{i}(Q,r)-\sum_{i=0}^{k-1}Z_{h}^{i}( \widehat{Q},\widehat{r})\right|\leq\sum_{i=0}^{k-1}\left|Z_{h}^{i}(Q,r)-Z_{h}^ {i}(\widehat{Q},\widehat{r})\right|.\]

For each term, we have that

\[\left|Z_{h}^{i}(Q,r)-Z_{h}^{i}(\widehat{Q},\widehat{r})\right|\] \[\leq \bigg{|}\left(Q_{h}(s_{h}^{i},a_{h}^{i})-r_{h}(s_{h}^{i},a_{h}^{i })-\max_{a^{\prime}\in\mathcal{A}}Q_{h+1}^{*,r}(s_{h+1}^{i},a^{\prime})\right) ^{2}\] \[\qquad-\left(\widehat{Q}_{h}(s_{h}^{i},a_{h}^{i})-\widehat{r}_{h} (s_{h}^{i},a_{h}^{i})-\max_{a^{\prime}\in\mathcal{A}}Q_{h+1}^{*,\widehat{r}}(s _{h+1}^{i},a^{\prime})\right)^{2}\bigg{|}\] \[+ \bigg{|}\left(Q_{h}^{*,r}(s_{h}^{i},a_{h}^{i})-r_{h}(s_{h}^{i},a_ {h}^{i})-\max_{a^{\prime}\in\mathcal{A}}Q_{h+1}^{*,r}(s_{h+1}^{i},a^{\prime}) \right)^{2}\] \[\qquad-\left(Q_{h}^{*,\widehat{r}}(s_{h}^{i},a_{h}^{i})-\widehat{ r}_{h}(s_{h}^{i},a_{h}^{i})-\max_{a^{\prime}\in\mathcal{A}}Q_{h+1}^{*,\widehat{r}}(s _{h+1}^{i},a^{\prime})\right)^{2}\bigg{|}.\]

For the first term in RHS, we have that

\[\bigg{|}\left(Q_{h}(s_{h}^{i},a_{h}^{i})-r_{h}(s_{h}^{i},a_{h}^{ i})-\max_{a^{\prime}\in\mathcal{A}}Q_{h+1}^{*,r}(s_{h+1}^{i},a^{\prime}) \right)^{2}\] \[\qquad-\left(\widehat{Q}_{h}(s_{h}^{i},a_{h}^{i})-\widehat{r}_{h} (s_{h}^{i},a_{h}^{i})-\max_{a^{\prime}\in\mathcal{A}}Q_{h+1}^{*,\widehat{r}}(s _{h+1}^{i},a^{\prime})\right)^{2}\bigg{|}\] \[\leq \bigg{|}Q_{h}(s_{h}^{i},a_{h}^{i})-r_{h}(s_{h}^{i},a_{h}^{i})-\max _{a^{\prime}\in\mathcal{A}}Q_{h+1}^{*,r}(s_{h+1}^{i},a^{\prime})+\widehat{Q}_ {h}(s_{h}^{i},a_{h}^{i})-\widehat{r}_{h}(s_{h}^{i},a_{h}^{i})-\max_{a^{\prime} \in\mathcal{A}}Q_{h+1}^{*,\widehat{r}}(s_{h+1}^{i},a^{\prime})\bigg{|}\] \[\bigg{|}Q_{h}(s_{h}^{i},a_{h}^{i})-\widehat{Q}_{h}(s_{h}^{i},a_{h} ^{i})-r_{h}(s_{h}^{i},a_{h}^{i})+\widehat{r}_{h}(s_{h}^{i},a_{h}^{i})-\max_{a^ {\prime}\in\mathcal{A}}Q_{h+1}^{*,r}(s_{h+1}^{i},a^{\prime})+\max_{a^{\prime} \in\mathcal{A}}Q_{h+1}^{*,\widehat{r}}(s_{h+1}^{i},a^{\prime})\bigg{|}\] \[\leq 4H\bigg{(}\left|Q_{h}(s_{h}^{i},a_{h}^{i})-\widehat{Q}_{h}(s_ {h}^{i},a_{h}^{i})\right|+\left|r_{h}(s_{h}^{i},a_{h}^{i})-\widehat{r}_{h}(s_{h }^{i},a_{h}^{i})\right|\] \[+\max_{a^{\prime}\in\mathcal{A}}\left|Q_{h+1}^{*,r}(s_{h+1}^{i},a ^{\prime})-Q_{h+1}^{*,\widehat{r}}(s_{h+1}^{i},a^{\prime})\right|\bigg{)}\] \[\leq 12H^{2}\rho.\]

The last inequality follows Lemma 7. Similarly, for the second term in RHS, we have that

\[\bigg{|}\left(Q_{h}^{*,r}(s_{h}^{i},a_{h}^{i})-r_{h}(s_{h}^{i},a _{h}^{i})-\max_{a^{\prime}\in\mathcal{A}}Q_{h+1}^{*,r}(s_{h+1}^{i},a^{\prime}) \right)^{2}\] \[-\left(Q_{h}^{*,\widehat{r}}(s_{h}^{i},a_{h}^{i})-\widehat{r}_{h} (s_{h}^{i},a_{h}^{i})-\max_{a^{\prime}\in\mathcal{A}}Q_{h+1}^{*,\widehat{r}}(s_ {h+1}^{i},a^{\prime})\right)^{2}\bigg{|}\] \[\leq \bigg{|}Q_{h}^{*,r}(s_{h}^{i},a_{h}^{i})-r_{h}(s_{h}^{i},a_{h}^{i })-\max_{a^{\prime}\in\mathcal{A}}Q_{h+1}^{*,r}(s_{h+1}^{i},a^{\prime})\] \[+Q_{h}^{*,\widehat{r}}(s_{h}^{i},a_{h}^{i})-\widehat{r}_{h}(s_{h }^{i},a_{h}^{i})-\max_{a^{\prime}\in\mathcal{A}}Q_{h+1}^{*,\widehat{r}}(s_{h +1}^{i},a^{\prime})\bigg{|}\] \[\cdot\bigg{|}Q_{h}^{*,r}(s_{h}^{i},a_{h}^{i})-Q_{h}^{*,\widehat{r} }(s_{h}^{i},a_{h}^{i})-r_{h}(s_{h}^{i},a_{h}^{i})+\widehat{r}_{h}(s_{h}^{i},a_{h} ^{i})\] \[-\max_{a^{\prime}\in\mathcal{A}}Q_{h+1}^{*,r}(s_{h+1}^{i},a^{ \prime})+\max_{a^{\prime}\in\mathcal{A}}Q_{h+1}^{*,\widehat{r}}(s_{h+1}^{i},a^{ \prime})\bigg{|}\]\[\leq 6H\bigg{(}\left|Q_{h}^{\star,r}(s_{h}^{i},a_{h}^{i})-Q_{h}^{\star, \widehat{r}}(s_{h}^{i},a_{h}^{i})\right|+\left|r_{h}(s_{h}^{i},a_{h}^{i})- \widehat{r}_{h}(s_{h}^{i},a_{h}^{i})\right|\] \[\quad+\max_{a^{\prime}\in\mathcal{A}}\left|Q_{h+1}^{\star,r}(s_{h+ 1}^{i},a^{\prime})-Q_{h+1}^{\star,\widehat{r}}(s_{h+1}^{i},a^{\prime})\right| \bigg{)}\] \[\leq 18H^{2}\rho.\]

Combining the above four inequalities yields that

\[\left|\sum_{i=0}^{k-1}Z_{h}^{i}(Q,r)-\sum_{i=0}^{k-1}Z_{h}^{i}(\widehat{Q}, \widehat{r})\right|\leq\sum_{i=0}^{k-1}\left|Z_{h}^{i}(Q,r)-Z_{h}^{i}( \widehat{Q},\widehat{r})\right|\leq 30kH^{2}\rho.\]

Therefore, for all \((Q,r)\in\mathcal{Q}\times\mathcal{R}\),

\[-\sum_{i=0}^{k-1}Z_{h}^{i}(Q,r) \leq-\sum_{i=0}^{k-1}Z_{h}^{i}(\widehat{Q},\widehat{r})+\left| \sum_{i=0}^{k-1}Z_{h}^{i}(Q,r)-\sum_{i=0}^{k-1}Z_{h}^{i}(\widehat{Q},\widehat{ r})\right|\] \[\leq 16H^{3}\log(KH\max_{h\in[H]}|(\mathcal{Q}_{h})_{\rho}||( \mathcal{R}_{h})_{\rho}|/\delta)+30kH^{2}\rho\] \[\leq 16H^{3}\log(KH\max_{h\in[H]}\mathcal{N}_{\rho}(\mathcal{Q}_{ h})\mathcal{N}_{\rho}(\mathcal{R}_{h})/\delta)+30kH^{2}\rho.\]

This implies that

\[\left(Q_{h}^{\star,r}(s_{h}^{i},a_{h}^{i})-r_{h}(s_{h}^{i},a_{h}^ {i})-\max_{a^{\prime}\in\mathcal{A}}Q_{h+1}^{\star,r}(s_{h+1}^{i},a^{\prime}) \right)^{2}\] \[\leq\inf_{Q_{h}\in\mathcal{Q}_{h}}\left(Q_{h}(s_{h}^{i},a_{h}^{i} )-r_{h}(s_{h}^{i},a_{h}^{i})-\max_{a^{\prime}\in\mathcal{A}}Q_{h+1}^{\star,r}( s_{h+1}^{i},a^{\prime})\right)^{2}\] \[\quad+16H^{3}\log(KH\max_{h\in[H]}\mathcal{N}_{\rho}(\mathcal{Q} _{h})\mathcal{N}_{\rho}(\mathcal{R}_{h})/\delta)+30kH^{2}\rho.\]

Therefore, we can derive the upper bound on \(\mathrm{BE}^{k}(Q^{\star,r^{k}})\).

\[\mathrm{BE}^{k}(Q^{\star,r^{k}})\] \[= \sum_{h=1}^{H}\bigg{(}\sum_{i=0}^{k-1}\left(Q_{h}^{\star,r^{k}}(s _{h}^{i},a_{h}^{i})-r_{h}^{k}(s_{h}^{i},a_{h}^{i})-\max_{a^{\prime}}Q_{h+1}^{ \star,r^{k}}(s_{h+1}^{i},a^{\prime})\right)^{2}\] \[-\inf_{Q_{h}^{\star}\in\mathcal{Q}_{h}}\sum_{i=0}^{k-1}\left(Q_{h }^{\prime}(s_{h}^{i},a_{h}^{i})-r_{h}^{k}(s_{h}^{i},a_{h}^{i})-\max_{a^{\prime }}Q_{h+1}^{\star,r^{k}}(s_{h+1}^{i},a^{\prime})\right)^{2}\bigg{)}\] \[\leq 16H^{4}\log(KH\max_{h\in[H]}\mathcal{N}_{\rho}(\mathcal{Q} _{h})\mathcal{N}_{\rho}(\mathcal{R}_{h})/\delta)+30kH^{3}\rho.\]

We complete the proof.

### Proof of Lemma 5

For any fixed tuple \((k,h,Q,r)\in[K]\times[H]\times\mathcal{Q}\times\mathcal{R}\), we define the random variable.

\[X_{h}^{i}(Q,r):=\left(Q_{h}(s_{h}^{i},a_{h}^{i})-r_{h}(s_{h}^{i}, a_{h}^{i})-\max_{a^{\prime}}Q_{h+1}(s_{h+1}^{i},a^{\prime})\right)^{2}\] \[\qquad\qquad-\left((\mathcal{T}_{h}^{\star}Q_{h+1})(s_{h}^{i},a_ {h}^{i})-r_{h}(s_{h}^{i},a_{h}^{i})-\max_{a^{\prime}}Q_{h+1}(s_{h+1}^{i},a^{ \prime})\right)^{2}.\]

We define the filtration \(\mathcal{F}^{i}=\sigma(\{(s_{1}^{j},a_{1}^{j},\ldots,s_{H}^{j},a_{H}^{j})\}_{j= 0}^{i-1})\). In the following part, we calculate the expectation and variance of \(X_{h}^{i}(Q,r)\) conditioned on \(\mathcal{F}^{i}\).

\[\mathbb{E}\left[X_{h}^{i}(Q,r)|\mathcal{F}^{i}\right]\] \[=\mathbb{E}\left[\left(Q_{h}(s_{h}^{i},a_{h}^{i})-r_{h}(s_{h}^{i}, a_{h}^{i})-\max_{a^{\prime}}Q_{h+1}(s_{h+1}^{i},a^{\prime})\right)^{2}\bigg{|} \mathcal{F}^{i}\right]\]\[\leq 16H^{2}\mathbb{E}\left[\left(Q_{h}(s^{i}_{h},a^{i}_{h})-( \mathcal{T}^{r}_{h}Q_{h+1})(s^{i}_{h},a^{i}_{h})\right)^{2}\bigg{|}\pi^{i}\right]\] \[=16H^{2}\mathbb{E}\left[X^{i}_{h}(Q,r)|\mathcal{F}^{i}\right].\]

Furthermore, \(\{X^{i}_{h}(Q,r)-\mathbb{E}[X^{i}_{h}(Q,r)|\mathcal{F}^{i}]\}_{i=0}^{k-1}\) is a martingale difference sequence adapted to \(\{\mathcal{F}^{i}\}_{i=0}^{k-1}\). Besides, it is easy to obtain that \(|X^{i}_{h}(Q,r)|\leq 9H^{2}\) almost surely. Thus, we can apply Lemma 6 and obtain that with probability at least \(1-\delta\), for any \(\eta\in(0,1/(9H^{2})]\),

\[\left|\sum_{i=0}^{k-1}X^{i}_{h}(Q,r)-\sum_{i=0}^{k-1}\mathbb{E}[X^ {i}_{h}(Q,r)|\mathcal{F}^{i}]\right| \leq\eta\sum_{i=0}^{k-1}\text{Var}\left[X^{i}_{h}(Q,r)|\mathcal{ F}^{i}\right]+\frac{\log(2/\delta)}{\eta}\] \[\leq 16H^{2}\eta\sum_{i=0}^{k-1}\mathbb{E}\left[X^{i}_{h}(Q,r)| \mathcal{F}^{i}\right]+\frac{\log(2/\delta)}{\eta}.\]

By choosing \(\eta=\min\{1/(9H^{2}),\sqrt{\log(2/\delta)/(16H^{2}\sum_{i=0}^{k-1}\mathbb{E} \left[X^{i}_{h}(Q,r)|\mathcal{F}^{i}\right]})\}\), we have that

\[\left|\sum_{i=0}^{k-1}X^{i}_{h}(Q,r)-\sum_{i=0}^{k-1}\mathbb{E}[X^ {i}_{h}(Q,r)|\mathcal{F}^{i}]\right|\leq 8H\sqrt{\sum_{i=0}^{k-1}\mathbb{E}\left[X ^{i}_{h}(Q,r)|\mathcal{F}^{i}\right]\log(2/\delta)}+9H^{2}\log(2/\delta).\]

This implies that

\[\sum_{i=0}^{k-1}\mathbb{E}[X^{i}_{h}(Q,r)|\mathcal{F}^{i}]-8H\sqrt{\sum_{i=0}^ {k-1}\mathbb{E}\left[X^{i}_{h}(Q,r)|\mathcal{F}^{i}\right]\log(2/\delta)}\leq \sum_{i=0}^{k-1}X^{i}_{h}(Q,r)+9H^{2}\log(2/\delta).\]This establishes a quadratic formula of \(x^{2}-bx-c\leq 0\) with \(x=\sqrt{\sum_{i=0}^{k-1}\mathbb{E}[X_{h}^{i}(Q,r)|\mathcal{F}^{i}]}\), \(b=8H\sqrt{\log(2/\delta)}\) and \(c=\sum_{i=0}^{k-1}X_{h}^{i}(Q,r)+9H^{2}\log(2/\delta)\). Solving this quadratic formula yields that \((b-\sqrt{b^{2}+4c})/2\leq x\leq(b+\sqrt{b^{2}+4c})/2\), which implies that

\[x^{2}\leq\frac{(b+\sqrt{b^{2}+4c})^{2}}{4}\leq\frac{2\left(b^{2}+b^{2}+4c \right)}{4}=b^{2}+2c.\]

Thus we obtain that

\[\sum_{i=0}^{k-1}\mathbb{E}[X_{h}^{i}(Q,r)|\mathcal{F}^{i}]\leq 2\sum_{i=0}^{k-1}X _{h}^{i}(Q,r)+82H^{2}\log(2/\delta).\]

We define \((\mathcal{Q}_{h})_{\rho}\) and \((\mathcal{R}_{h})_{\rho}\) as the \(\rho\)-covers of \(\mathcal{Q}_{h}\) and \(\mathcal{R}_{h}\), respectively. It is direct to have that \(\mathcal{Q}_{\rho}=(\mathcal{Q}_{1})_{\rho}\times\ldots(\mathcal{Q}_{H})_{\rho}\) and \(\mathcal{R}_{\rho}=(\mathcal{R}_{1})_{\rho}\times\ldots(\mathcal{R}_{H})_{\rho}\) are \(\rho\)-covers of \(\mathcal{Q}\) and \(\mathcal{R}\), respectively. By union bound, with probability at least \(1-\delta\), for all \((k,h,\widehat{Q},\widehat{r})\in[K]\times[H]\times\mathcal{Q}_{\rho}\times \mathcal{R}_{\rho}\),

\[\sum_{i=0}^{k-1}\mathbb{E}[X_{h}^{i}(\widehat{Q},\widehat{r})| \mathcal{F}^{i}] \leq 2\sum_{i=0}^{k-1}X_{h}^{i}(\widehat{Q},\widehat{r})+82H^{2} \log(2KH|\mathcal{Q}_{\rho}||\mathcal{R}_{\rho}|/\delta)\] \[=2\sum_{i=0}^{k-1}X_{h}^{i}(\widehat{Q},\widehat{r})+82H^{2} \log\left(2KH\prod_{h=1}^{H}\left(|(\mathcal{Q}_{h})_{\rho}||(\mathcal{R}_{h}) _{\rho}|\right)/\delta\right)\] \[\leq 2\sum_{i=0}^{k-1}X_{h}^{i}(\widehat{Q},\widehat{r})+82H^{3} \log\left(2KH\max_{h\in[H]}|(\mathcal{Q}_{h})_{\rho}||(\mathcal{R}_{h})_{\rho} |/\delta\right).\]

We have calculated the conditional expectation in the LHS and obtain that

\[\sum_{i=0}^{k-1}\mathbb{E}\left[\left(\widehat{Q}_{h}(s_{h}^{i}, a_{h}^{i})-(\mathcal{T}_{h}^{c}\widehat{Q}_{h+1})(s_{h}^{i},a_{h}^{i})\right)^{2} \left|\pi^{i}\right.\right]\] \[\leq 2\sum_{i=0}^{k-1}X_{h}^{i}(\widehat{Q},\widehat{r})+82H^{3} \log\left(2KH\max_{h\in[H]}|(\mathcal{Q}_{h})_{\rho}||(\mathcal{R}_{h})_{\rho }|/\delta\right)\] \[\leq 2\sum_{i=0}^{k-1}X_{h}^{i}(\widehat{Q},\widehat{r})+82H^{3} \log\left(2KH\max_{h\in[H]}\mathcal{N}_{\rho}(\mathcal{Q}_{h})\mathcal{N}_{ \rho}(\mathcal{R}_{h})/\delta\right).\]

According to the definition of \(\rho\)-cover, for \((Q^{k},r^{k})\), there exists \((\widehat{Q},\widehat{r})\in\mathcal{Q}_{\rho}\times\mathcal{R}_{\rho}\) such that

\[\max_{(s,a,h)\in\mathcal{S}\times\mathcal{A}\times[H]}\left|\widehat{Q}_{h}(s, a)-Q_{h}^{k}(s,a)\right|\leq\rho,\ \max_{(s,a,h)\in\mathcal{S}\times\mathcal{A}\times[H]}\left| \widehat{r}_{h}(s,a)-r_{h}^{k}(s,a)\right|\leq\rho.\]

Then we can upper bound the errors caused by approximating \((Q^{k},r^{k})\) with \((\widehat{Q},\widehat{r})\).

\[\left|\left(\widehat{Q}_{h}(s_{h}^{i},a_{h}^{i})-(\mathcal{T}_{h}^ {c}\widehat{Q}_{h+1})(s_{h}^{i},a_{h}^{i})\right)^{2}-\left(Q_{h}^{k}(s_{h}^{i },a_{h}^{i})-(\mathcal{T}_{h}^{r^{k}}Q_{h+1}^{k})(s_{h}^{i},a_{h}^{i})\right)^ {2}\right|\] \[\leq\left|\widehat{Q}_{h}(s_{h}^{i},a_{h}^{i})-(\mathcal{T}_{h}^ {c}\widehat{Q}_{h+1})(s_{h}^{i},a_{h}^{i})+Q_{h}^{k}(s_{h}^{i},a_{h}^{i})-( \mathcal{T}_{h}^{r^{k}}Q_{h+1}^{k})(s_{h}^{i},a_{h}^{i})\right|\] \[\quad\left|\widehat{Q}_{h}(s_{h}^{i},a_{h}^{i})-(\mathcal{T}_{h}^ {c}\widehat{Q}_{h+1})(s_{h}^{i},a_{h}^{i})-Q_{h}^{k}(s_{h}^{i},a_{h}^{i})+( \mathcal{T}_{h}^{r^{k}}Q_{h+1}^{k})(s_{h}^{i},a_{h}^{i})\right|\] \[\leq 2H\left|\widehat{Q}_{h}(s_{h}^{i},a_{h}^{i})-(\mathcal{T}_{h}^ {c}\widehat{Q}_{h+1})(s_{h}^{i},a_{h}^{i})-Q_{h}^{k}(s_{h}^{i},a_{h}^{i})+( \mathcal{T}_{h}^{r^{k}}Q_{h+1}^{k})(s_{h}^{i},a_{h}^{i})\right|\] \[\leq 6H\rho.\]

\[\left|X_{h}^{i}(\widehat{Q},\widehat{r})-X_{h}^{i}(Q^{k},r^{k})\right|\] \[\leq \left|\widehat{Q}_{h}(s_{h}^{i},a_{h}^{i})+Q_{h}^{k}(s_{h}^{i},a_{h }^{i})-\widehat{r}_{h}(s_{h}^{i},a_{h}^{i})-r_{h}^{k}(s_{h}^{i},a_{h}^{i})\right.\]\[-\max_{a^{\prime}}\widehat{Q}_{h+1}(s^{i}_{h+1},a^{\prime})-\max_{a^{ \prime}}Q^{k}_{h+1}(s^{i}_{h+1},a^{\prime})\bigg{|}\] \[\cdot\bigg{|}\widehat{Q}_{h}(s^{i}_{h},a^{i}_{h})-Q^{k}_{h}(s^{i}_ {h},a^{i}_{h})-\widehat{r}_{h}(s^{i}_{h},a^{i}_{h})+r^{k}_{h}(s^{i}_{h},a^{i}_ {h})\] \[-\max_{a^{\prime}}\widehat{Q}_{h+1}(s^{i}_{h+1},a^{\prime})+\max_{ a^{\prime}}Q^{k}_{h+1}(s^{i}_{h+1},a^{\prime})\bigg{|}\] \[+\bigg{|}(T^{\widehat{r}}_{h}\widehat{Q}_{h+1})(s^{i}_{h},a^{i}_{ h})+(T^{r^{k}}_{h}Q^{k}_{h+1})(s^{i}_{h},a^{i}_{h})-\widehat{r}_{h}(s^{i}_{h},a^{ i}_{h})-r^{k}_{h}(s^{i}_{h},a^{i}_{h})\] \[-\max_{a^{\prime}}\widehat{Q}_{h+1}(s^{i}_{h+1},a^{\prime})-\max_ {a^{\prime}}Q^{k}_{h+1}(s^{i}_{h+1},a^{\prime})\bigg{|}\] \[\cdot\bigg{|}(\mathcal{T}^{\widehat{r}}_{h}\widehat{Q}_{h+1})(s^{ i}_{h},a^{i}_{h})-(\mathcal{T}^{r^{k}}_{h}Q^{k}_{h+1})(s^{i}_{h},a^{i}_{h})- \widehat{r}_{h}(s^{i}_{h},a^{i}_{h})+r^{k}_{h}(s^{i}_{h},a^{i}_{h})\] \[-\max_{a^{\prime}}\widehat{Q}_{h+1}(s^{i}_{h+1},a^{\prime})+\max_ {a^{\prime}}Q^{k}_{h+1}(s^{i}_{h+1},a^{\prime})\bigg{|}\] \[\leq 4H\bigg{|}\widehat{Q}_{h}(s^{i}_{h},a^{i}_{h})-Q^{k}_{h}(s^{i} _{h},a^{i}_{h})-\widehat{r}_{h}(s^{i}_{h},a^{i}_{h})+r^{k}_{h}(s^{i}_{h},a^{i}_ {h})\] \[-\max_{a^{\prime}}\widehat{Q}_{h+1}(s^{i}_{h+1},a^{\prime})+\max_ {a^{\prime}}Q^{k}_{h+1}(s^{i}_{h+1},a^{\prime})\bigg{|}\] \[+4H\bigg{|}(\mathcal{T}^{\widehat{r}}_{h}\widehat{Q}_{h+1})(s^{i} _{h},a^{i}_{h})-(\mathcal{T}^{r^{k}}_{h}Q^{k}_{h+1})(s^{i}_{h},a^{i}_{h})- \widehat{r}_{h}(s^{i}_{h},a^{i}_{h})+r^{k}_{h}(s^{i}_{h},a^{i}_{h})\] \[-\max_{a^{\prime}}\widehat{Q}_{h+1}(s^{i}_{h+1},a^{\prime})+\max_ {a^{\prime}}Q^{k}_{h+1}(s^{i}_{h+1},a^{\prime})\bigg{|}\] \[\leq 24H\rho.\]

With the above bounds, we can obtain that

\[\sum_{i=0}^{k-1}\mathbb{E}\left[\left(Q^{k}_{h}(s^{i}_{h},a^{i}_{h} )-(\mathcal{T}^{r^{k}}_{h}Q^{k}_{h+1})(s^{i}_{h},a^{i}_{h})\right)^{2}\bigg{|} \pi^{i}\right]\] \[\leq 2\sum_{i=0}^{k-1}X^{i}_{h}(Q^{k},r^{k})+82H^{3}\log\left(2KH \max_{h\in[H]}\mathcal{N}_{\rho}(\mathcal{Q}_{h})\mathcal{N}_{\rho}(\mathcal{ R}_{h})/\delta\right)+54kH\rho.\]

According to the definition of \(\mathrm{BE}^{k}\), we have that

\[\mathrm{BE}^{k}(Q^{k})\] \[=\sum_{h=1}^{H}\sum_{i=0}^{k-1}\left(Q^{k}_{h}(s^{i}_{h},a^{i}_{h} )-r^{k}_{h}(s^{i}_{h},a^{i}_{h})-\max_{a^{\prime}}Q^{k}_{h+1}(s^{i}_{h+1},a^{ \prime})\right)^{2}\] \[\quad-\inf_{Q^{\prime}\in\mathcal{Q}}\sum_{h=1}^{H}\sum_{i=0}^{k- 1}\left(Q^{\prime}_{h}(s^{i}_{h},a^{i}_{h})-r^{k}_{h}(s^{i}_{h},a^{i}_{h})- \max_{a^{\prime}}Q^{k}_{h+1}(s^{i}_{h+1},a^{\prime})\right)^{2}\] \[\overset{\text{(a)}}{\geq}\sum_{h=1}^{H}\sum_{i=0}^{k-1}X^{i}_{h} (Q^{k},r^{k})\] \[\geq\frac{1}{2}\sum_{h=1}^{H}\sum_{i=0}^{k-1}\mathbb{E}\left[ \left(Q^{k}_{h}(s^{i}_{h},a^{i}_{h})-(\mathcal{T}^{r^{k}}_{h}Q^{k}_{h+1})(s^{i} _{h},a^{i}_{h})\right)^{2}\bigg{|}\pi^{i}\right]\] \[\quad-41H^{4}\log\left(2KH\max_{h\in[H]}\mathcal{N}_{\rho}( \mathcal{Q}_{h})\mathcal{N}_{\rho}(\mathcal{R}_{h})/\delta\right)-27kH^{2}\rho.\]

Inequality (a) follows Assumption 3 that \(\mathcal{T}^{r^{k}}_{h}Q^{r^{k}}_{h+1}\in\mathcal{Q}_{h}\) We complete the proof.

### Technical Lemmas

**Lemma 6** (Freedman's inequality [3]).: _Let \((X_{t})_{t\leq T}\) be a real-valued martingale difference sequence adapted to filtration \(\mathcal{F}_{t}\), and let \(\mathbb{E}_{t}[\cdot]=\mathbb{E}[\cdot\mid\mathcal{F}_{t}]\). If \(|X_{t}|\leq R\) almost surely, then for any \(\eta\in[0,\frac{1}{R}]\) it holds that with probability at least \(1-\delta\),_

\[\sum_{t=1}^{T}X_{t}\leq\eta\sum_{t=1}^{T}\mathbb{E}_{t-1}[X_{t}^{2}]+\frac{\log (1/\delta)}{\eta}.\]

**Lemma 7**.: _For any reward functions \(r,\widehat{r}\), we have that_

\[\forall(s,a,h)\in\mathcal{S}\times\mathcal{A}\times[H],\,\left|Q_{h}^{\star,r}( s,a)-Q_{h}^{\star,\widehat{r}}(s,a)\right|\leq\sum_{h^{\prime}=h}^{H}\max_{s \in\mathcal{S},a\in\mathcal{A}}\left|r_{h}(s,a)-\widehat{r}_{h}(s,a)\right|.\]

_Here \(Q^{\star,r}\) is the optimal Q-value function of \(r\)._

Proof.: According to the Bellman optimality equation, we have that

\[\left|Q_{h}^{\star,r}(s,a)-Q_{h}^{\star,\widehat{r}}(s,a)\right|\] \[=\left|r_{h}(s,a)-\widehat{r}_{h}(s,a)+\mathbb{E}_{s^{\prime} \sim P_{h}(\cdot|s,a)}\left[\max_{a^{\prime}\in\mathcal{A}}Q_{h+1}^{\star,r}(s ^{\prime},a^{\prime})-\max_{a^{\prime}\in\mathcal{A}}Q_{h+1}^{\star,\widehat{r }}(s^{\prime},a^{\prime})\right]\right|\] \[\leq\left|r_{h}(s,a)-\widehat{r}_{h}(s,a)\right|+\mathbb{E}_{s^{ \prime}\sim P_{h}(\cdot|s,a)}\left[\left|\max_{a^{\prime}\in\mathcal{A}}Q_{h+1 }^{\star,r}(s^{\prime},a^{\prime})-\max_{a^{\prime}\in\mathcal{A}}Q_{h+1}^{ \star,\widehat{r}}(s^{\prime},a^{\prime})\right|\right].\]

We analyze the term \(|\max_{a^{\prime}\in\mathcal{A}}Q_{h+1}^{\star,r}(s^{\prime},a^{\prime})-\max _{a^{\prime}\in\mathcal{A}}Q_{h+1}^{\star,\widehat{r}}(s^{\prime},a^{\prime})|\).

\[\max_{a^{\prime}\in\mathcal{A}}Q_{h+1}^{\star,r}(s^{\prime},a^{ \prime})-\max_{a^{\prime}\in\mathcal{A}}Q_{h+1}^{\star,\widehat{r}}(s^{\prime },a^{\prime})\] \[=Q_{h+1}^{\star,r}(s^{\prime},a^{1})-Q_{h+1}^{\star,\widehat{r}} (s^{\prime},a^{2})\] \[\leq Q_{h+1}^{\star,r}(s^{\prime},a^{1})-Q_{h+1}^{\star,\widehat{ r}}(s^{\prime},a^{1}),\] \[\max_{a^{\prime}\in\mathcal{A}}Q_{h+1}^{\star,r}(s^{\prime},a^{ \prime})-\max_{a^{\prime}\in\mathcal{A}}Q_{h+1}^{\star,\widehat{r}}(s^{\prime },a^{\prime})\] \[=Q_{h+1}^{\star,r}(s^{\prime},a^{1})-Q_{h+1}^{\star,\widehat{r}} (s^{\prime},a^{2})\] \[\geq Q_{h+1}^{\star,r}(s^{\prime},a^{2})-Q_{h+1}^{\star,\widehat{ r}}(s^{\prime},a^{2}).\]

Here \(a^{1}\in\operatorname*{argmax}_{a^{\prime}\in\mathcal{A}}Q_{h+1}^{\star,r}(s^{ \prime},a^{\prime}),a^{2}\in\operatorname*{argmax}_{a^{\prime}\in\mathcal{A}}Q _{h+1}^{\star,\widehat{r}}(s^{\prime},a^{\prime})\). Thus, we can get that

\[\left|\max_{a^{\prime}\in\mathcal{A}}Q_{h+1}^{\star,r}(s^{\prime },a^{\prime})-\max_{a^{\prime}\in\mathcal{A}}Q_{h+1}^{\star,\widehat{r}}(s^{ \prime},a^{\prime})\right| \leq\max_{a^{\prime}\in\mathcal{A}}Q_{h+1}^{\star,r}(s^{\prime}, a^{\prime})-Q_{h+1}^{\star,\widehat{r}}(s^{\prime},a^{\prime})\] \[\leq\max_{a^{\prime}\in\mathcal{A}}\left|Q_{h+1}^{\star,r}(s^{ \prime},a^{\prime})-Q_{h+1}^{\star,\widehat{r}}(s^{\prime},a^{\prime})\right|. \tag{8}\]

Then we have that \(\forall(s,a)\in\mathcal{S}\times\mathcal{A}\),

\[\left|Q_{h}^{\star,r}(s,a)-Q_{h}^{\star,\widehat{r}}(s,a)\right|\] \[\leq\left|r_{h}(s,a)-\widehat{r}_{h}(s,a)\right|+\mathbb{E}_{s^{ \prime}\sim P_{h}(\cdot|s,a)}\left[\left|\max_{a^{\prime}\in\mathcal{A}}Q_{h+1 }^{\star,r}(s^{\prime},a^{\prime})-\max_{a^{\prime}\in\mathcal{A}}Q_{h+1}^{ \star,\widehat{r}}(s^{\prime},a^{\prime})\right|\right]\] \[\leq\left|r_{h}(s,a)-\widehat{r}_{h}(s,a)\right|+\max_{s^{\prime }\in\mathcal{S},a^{\prime}\in\mathcal{A}}\left|Q_{h+1}^{\star,r}(s^{\prime},a^ {\prime})-Q_{h+1}^{\star,\widehat{r}}(s^{\prime},a^{\prime})\right|.\]

Applying the above recursion inequality repeatedly from \(h^{\prime}=h\) to \(h^{\prime}=H\) with \(Q_{H+1}^{\star,r}(s,a)=Q_{H+1}^{\star,\widehat{r}}(s,a)=0\) completes the proof. 

**Lemma 8**.: _For \(a\geq 1\) and \(\varepsilon\leq 1\), when \(K\geq 4\log(4a/\varepsilon)/\varepsilon^{2}\), we have that_

\[\sqrt{\frac{\log(aK)}{K}}\leq\varepsilon.\]Proof.: We consider the function \(f(K)=\sqrt{\log(aK)/K}\) and calculate the gradient.

\[f^{\prime}(K)=\frac{1}{2}\left(\frac{\log(aK)}{K}\right)^{-1/2}\left(\frac{1-\log (aK)}{K^{2}}\right).\]

When \(K\geq 4\log(4a/\varepsilon)/\varepsilon^{2}\geq 4\), we have that \(f^{\prime}(K)\leq 0\), implying that \(f(K)\) is a monotonically decreasing function in this range. Then we have that

\[\sqrt{\frac{\log(aK)}{K}} \leq\sqrt{\frac{\log\left(4a\log(4a/\varepsilon)/\varepsilon^{2} \right)}{4\log(4a/\varepsilon)}}\varepsilon\] \[=\sqrt{\frac{\log(4a/\varepsilon)+\log(\log(4a/\varepsilon))+\log( 1/\varepsilon)}{4\log(4a/\varepsilon)}}\varepsilon\] \[\stackrel{{(a)}}{{\leq}}\sqrt{\frac{\log(4a/ \varepsilon)+\log(4a/\varepsilon)+\log(1/\varepsilon)}{4\log(4a/\varepsilon) }}\varepsilon\] \[\stackrel{{(b)}}{{\leq}}\varepsilon.\]

Inequality \((a)\) follows that \(\log(x)\leq x+1\) and inequality \((b)\) follows that \(a\geq 1\).

## Appendix C Implementation Details

### Implementation Details of OPT-AIL

#### Reward Update.

As mentioned in Section 4.2, we choose \(\psi(r)\) in Eq. (2) as the gradient penalty (GP) regularization of the reward model [6], which can help stabilize the online optimization process by enforcing 1-Lipschitz continuity of the reward model \(r\). Here \(\mathcal{D}^{I}\) is linear interpolations between the replay buffer \(\mathcal{D}^{k}\) and expert demonstrations \(\mathcal{D}^{\text{E}}\).

\[\psi(r)=\mathbb{E}_{\tau\sim\mathcal{D}^{I}}\left[\sum_{h=1}^{H}(\|\nabla r_{h }(s_{h},a_{h})\|-1)^{2}\right]\]

#### Policy Update.

Here we present the implementation details of policy updates. Firstly, to stabilize the training process, we refine the optimism regularization term by subtracting a baseline Q-value function from random policy \(\mu\equiv\text{Unif}(\mathcal{A})\), which has been utilized in [29, 32]. Furthermore, recognizing that initial state samples can be limited and lack diversity, we employ both the replay buffer \(\mathcal{D}^{k}\) and expert demonstrations \(\mathcal{D}^{\text{E}}\) to compute the Q-value loss, which is a common data augmentation approach and has been validated in many deep AIL methods [28, 15, 56]. Incorporating these two enhancements, we reformulate the Q-value model training objective as follows.

\[\min_{Q\in\mathcal{Q}} \mathbb{E}_{\tau\sim\mathcal{D}^{k}\cup\mathcal{D}^{k}}\left[\sum _{h=1}^{H}\left(Q_{h}(s_{h},a_{h})-r_{h}^{k}-\overline{Q}_{h+1}(s_{h+1},\pi^ {k})\right)^{2}\right]\] \[- \lambda\mathbb{E}_{\tau\sim\mathcal{D}^{k}\cup\mathcal{D}^{k}} \left[\sum_{h=1}^{H}\left(Q_{h}(s_{h},\pi^{k})-Q_{h}(s_{h},\mu)\right)\right].\]

### Architecture and Training Details

The experiments are conducted on a machine with 64 CPU cores and 4 RTX4090 GPU cores. Each experiment is replicated five times using different random seeds. For each task, we adopt online DrQ-v2 [66] to train an agent with sufficient environment interactions 1 and regard the resultant policy as the expert policy. Then we roll out this expert policy to collect expert demonstrations. The architecture and training details of OPT-AIL and all baselines are listed below.

Footnote 1: 3M training steps for Cheetah Run, Hopper Hop, and Walker Run, and 1M training steps for other tasks.

**OPT-AIL:** Our codebase of OPT-AIL extends the open-sourced framework of IQLearn. We retain the structure and parameter design of the actor and critic from the original framework while employing SAC [17] with a fixed temperature for policy update. We also implement a discriminator with a similar architecture to the critic network, and additionally incorporate layer normalization and tanh activation before the output to improve training stability. A comprehensive enumeration of the hyperparameters of OPT-AIL is provided in Table 2.

**BC:** We implement BC based on our codebase. The actor model is trained using Mean Squared Error (MSE) loss over 10k training steps.

**PPIL:** We use the author's codebase, which is available at [https://github.com/lviano/p2il](https://github.com/lviano/p2il).

**IQLearn:** We use the author's codebase, which is available at [https://github.com/Div99/IQ-Learn](https://github.com/Div99/IQ-Learn).

**DAC:** We reproduce the DAC based on our codebase. Due to the difference in updating the discriminator compared to OPT-AIL, we refer to the official DAC implementation when reproducing the discriminator. We remove the layer normalization and the tanh activation function before the output, and find that this resulted in better performance.

**FILTER:** We use the author's codebase, which is available at [https://github.com/gkswamy98/fast_irl](https://github.com/gkswamy98/fast_irl).

**HyPE:** We use the author's codebase, which is available at [https://github.com/gkswamy98/hyper](https://github.com/gkswamy98/hyper).

We emphasize that for a fair comparison, all algorithms are implemented using the same codebase 2, with all hyperparameters kept consistent except for the gradient penalty coefficient. Specifically, in OPT-AIL, the gradient penalty coefficient is set to 1 for Cartpole Swingup, Walker Walk, and Walker Stand, and 10 for other tasks. For baselines, the gradient penalty coefficient is always set to 10 as provided by the authors. We also attempt to adjust this parameter for the baselines but find that the default parameters provided by the authors work well.

Footnote 2: The codebase of PPIL is consistent with IQLearn.

\begin{table}
\begin{tabular}{l|l} \hline \hline Parameter & Value \\ \hline discount (\(\gamma\)) & 0.99 \\ gradient penalty coefficient (\(\beta\)) & 1, 10 \\ optimism regularization coefficient (\(\lambda\)) & \(10^{-3}\) \\ temperature (\(\alpha\)) & \(10^{-2}\) \\ replay buffer size & \(5\cdot 10^{5}\) \\ batch size & 256 \\ optimizer & Adam \\ _Discriminator_ & \\ learning rate & \(3\cdot 10^{-5}\) \\ number of hidden layers & 2 \\ number of hidden units per layer & 256 \\ activation & ReLU \\ _Actor_ & \\ learning rate & \(3\cdot 10^{-5}\) \\ number of hidden layers & 2 \\ number of hidden units per layer & 256 \\ activation & ReLU \\ _Critic_ & \\ learning rate & \(3\cdot 10^{-4}\) \\ number of hidden layers & 2 \\ number of hidden units per layer & 256 \\ activation & ReLU \\ \hline \hline \end{tabular}
\end{table}
Table 2: OPT-AIL Hyper-parameters.

[MISSING_PAGE_FAIL:31]

Figure 5: Learning curves on 8 DMControl tasks over 5 random seeds using 10 expert trajectories.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: We clearly state the contribution and scope of this paper in the abstract and introduction. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: The limitations of this work are discussed in Section 4.1 and Section 6. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes]Justification: The full set of assumptions is stated in each theoretical result and the complete and correct proofs are provided in Appendix B.

Guidelines:

* The answer NA means that the paper does not include theoretical results.
* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We present all implementation details for reproducing the main experimental results of this paper in Appendix C. Guidelines:

* The answer NA means that the paper does not include experiments.
* If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.
* If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.
* Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.
* While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code**Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: We submit the code for reproducing the main experimental results in the supplemental material. Guidelines:

* The answer NA means that paper does not include experiments requiring code.
* Please see the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details.
* While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
* The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details.
* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: All experimental details are described in Section 5 and Appendix C. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: We report the standard deviation over 5 random seeds for all experiments in this paper; see the detailed results in Section 5 and Appendix D. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).

* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We describe the information on the computer resources for running the experiments in Appendix C. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics [https://neurips.cc/public/EthicsGuidelines?](https://neurips.cc/public/EthicsGuidelines?) Answer: [Yes] Justification: This paper investigates the theoretical underpinnings of imitation learning and conforms with the NeurIPS Code of Ethics in every respect. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: We discuss both potential positive societal impacts and negative societal impacts of this work in Appendix A. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.

* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).

11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: This paper conducts experiments on a simulated environment for continuous control and poses no such risks. Guidelines:

* The answer NA means that the paper poses no such risks.
* Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.
* Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.
* We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.

12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We cite the original paper that produced the codebase and expert dataset, and provide the corresponding URLs in Appendix C. Guidelines:

* The answer NA means that the paper does not use existing assets.
* The authors should cite the original paper that produced the code package or dataset.
* The authors should state which version of the asset is used and, if possible, include a URL.
* The name of the license (e.g., CC-BY 4.0) should be included for each asset.
* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.

* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: This paper does not release new assets. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer:[NA] Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.

* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.