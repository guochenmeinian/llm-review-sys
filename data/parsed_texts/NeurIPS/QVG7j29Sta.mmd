# Accuracy is _Not_ All You Need

 Abhinav Dutta

Microsoft Research

Bangalore, India

t-abdutta@microsoft.com

&Sanjeev Krishnan

Microsoft Research

Bangalore, India

sakrishnan@microsoft.com

&Nipun Kwatra

Microsoft Research

Bangalore, India

nipun.kwatra@microsoft.com

&Ramachandran Ramjee

Microsoft Research

Bangalore, India

ramjee@microsoft.com

###### Abstract

When Large Language Models (LLMs) are compressed using techniques such as quantization, the predominant way to demonstrate the validity of such techniques is by measuring the model's accuracy on various benchmarks. If the accuracies of the baseline model and the compressed model are close, it is assumed that there was negligible degradation in quality. However, even when the accuracies of the baseline and compressed model are similar, we observe the phenomenon of _flips_, wherein answers change from correct to incorrect and vice versa in proportion. We conduct a detailed study of metrics across multiple compression techniques, models and datasets, demonstrating that the behavior of compressed models as visible to end-users is often significantly different from the baseline model, even when accuracy is similar. We further evaluate compressed models both qualitatively and quantitatively using MT-Bench and show that compressed models exhibiting high _flips_ are worse than baseline models in this free-form generative task. Thus, we argue that accuracy and perplexity are necessary but not sufficient for evaluating compressed models, since these metrics hide large underlying changes that have not been observed by previous work. Hence, compression techniques should also be evaluated using _distance_ metrics. We propose two such distance metrics, _KL-Divergence_ and _% flips_, and show that they are well correlated.

## 1 Introduction

The high cost and latency of Large Language Models (LLMs) has motivated the design of multiple model compression techniques for optimizing LLM efficiency such as quantization (Dettmers et al., 2022), Key-Value (KV) cache compression (Ge et al., 2023), pruning (Sun et al., 2023) and sparsification (Ashkboos et al., 2024). However, today, there is no standardized way of evaluating the effectiveness of these techniques.

The predominant way of establishing the validity of the LLM compression methods today is to report accuracy on selected benchmark tasks such as MMLU (Hendrycks et al., 2021), Hellaswag (Zellers et al., 2019), ARC (Clark et al., 2018), LAMBADA (Paperno et al., 2016), etc. It is assumed that if the compressed model preserves accuracy on such benchmarks, it can be used as an equivalent replacement for the baseline model.

In this paper, we conduct a detailed evaluation of various compression techniques. We find that while the difference in the aggregate accuracy metric across various benchmarks between the baseline and compressed LLM is negligible in most cases ( \(\leq 2\%\)), the actual percentage change in the answerscan be significant (\(\geq 5\%\)). In other words, even when the overall accuracy is unchanged, a large number of correct answers change to incorrect and vice versa in proportion (we call these _flips_), between the baseline and compressed model. To the best of our knowledge, we believe that _we are the first to identify this phenomenon of flips caused due to model compression. Further, we argue that flips serves as an intuitive metric that captures how significantly different the compressed model is from the baseline model, even when both models exhibit similar accuracy on various benchmarks._

Figure 1 shows the change in accuracy and flips % vs baseline 16-bit model, respectively, for _six_ quantization schemes on _seven_ benchmark tasks (MMLU (Hendrycks et al., 2021), Hellaswag (Zellers et al., 2019), LAMBADA (Paperno et al., 2016), ARC Easy and Challenge (Clark et al., 2018) PIQA (Bisk et al., 2019), and Winogrande (Sakaguchi et al., 2019)). We see that all quantization schemes have negligible difference in accuracy (\(\mathbf{0-2\%}\)) compared to the 16-bit version. However, except for GPTQ W8A16 (8-bit weight, 16-bit activation ) that preserves accuracy with negligible flips, all other quantization schemes exhibit large number of flips (\(\mathbf{up}\) to \(\mathbf{13.6\%}\)), indicating significant divergence from the baseline model.

Figure 3 shows similar behavior of MMLU task accuracy being preserved while flips increase, for two other compression techniques, namely, layer dropping (Gromov et al., 2024) and WANDA weight pruning (Sun et al., 2023). For example, while Gromov et al. (2024) showed that dropping the last few layers of a model did not affect its accuracy on standard benchmarks, we find a steady, almost linear increase in the number of flips with the number of layers being dropped.

The phenomenon of flips is puzzling at first glance. While it is easy to see that some correct answers may become incorrect due to errors induced by compression, it is difficult to explain how an approximately equal number of incorrect answers become correct such that overall accuracy is preserved! For example, MMLU questions have 4 options, one of which is correct. Thus, any output change could move a correct answer to an incorrect one but there is only 1 in 3 chance for an incorrect answer to land on the correct option. We present a detailed analysis of flips in Section 5. Furthermore, we observe that simply adding Gaussian noise to model weights can reproduce this flips phenomenon (see Table 1). This suggests flips arise from the inherent approximations introduced by compression rather than any new information or learning during the compression process.

Finally, one might question whether flips matter if accuracy is preserved. Indeed, if the downstream task where the LLM is used closely matches the benchmark task, accuracy alone might suffice. However, LLMs are typically used in a variety of downstream tasks that require generating free-form text, where accuracy evaluated on some standard question-answering tasks could be a poor proxy. Thus, we evaluate the compressed models using MT-Bench (Zheng et al., 2023), a multi-turn dialogue task. We show through qualitative evaluation as well as using GPT4 as an automated judge that

Figure 1: All six quantization schemes show _negligible difference in accuracy_ compared to baseline 16-bit model (Llama2-chat 7B, 13B, 70B and Yi-chat 6B, 34B) in seven different tasks. However, all schemes, except GPTQ W8A16 (8-bit weight, 16-bit activation), _exhibit large number of flips_, indicating severe divergence in model behavior.

compressed models with high number of flips _are significantly worse than baseline models_ in this task (see Section 6).

Since the goal of compression schemes is to create models that mimic the baseline models as closely as possible, we argue that compressed models are better judged by _distance metrics_ with respect to baseline, in addition to _capability metrics_ such as accuracy alone, as is the practice today. We demonstrate that well-known distance metrics like _KL-Divergence_ on a given dataset can better identify the differences created due to various compression techniques and this metric correlates well with _flips_. Further, we show that the scores on MT-Bench (which evaluates free-form generation capabilities of these models) is highly correlated with _flips_. Thus, we propose that _flips_, an intuitive and inexpensive to compute metric, as a potential proxy distance metric for evaluating LLM compression techniques.

In this paper, we make the following key contributions:

* Using detailed qualitative and quantitative evaluation of various compression techniques, we show that accuracy is not sufficient as an evaluation metric for LLM compression techniques.
* We demonstrate the existence of _flips_ as a general phenomenon and explain why they occur.
* We evaluate compression techniques using the _KL-Divergence_ distance metric and show that it correlates well with _flips_.
* We propose that, where appropriate, _flips_ be used as an intuitive distance metric for evaluating the quality of compression techniques.

## 2 LLM Evaluation Metrics

We compare baseline and compressed LLMs on the following metrics:

* _capability_ metric: % correct answers, for question-answering tasks. This determines the competency of the model for a particular task. Multiple-choice question-answering (MCQ) tasks such as MMLU expect the model to output a single token for the correct answer (A/B/C/D), and compare this token with the target answer. For other tasks (like PIQA, Hellaswag, ARC), where the model assigns a probability to an option (consisting of multiple tokens), we report the standard _normalized_ accuracy (Eleuther, 2021).
* _capability_ metric: This measures the overall language modelling capability of an LLM. It is defined as \(e^{(Average\ Negative\ Loglikelihood)}\) calculated over a dataset.
* _distance_ metric: measures the % of questions whose answers changed from correct \(\rightarrow\) incorrect or incorrect \(\rightarrow\) correct, between baseline and quantized model for all tasks that have correct/incorrect answers. Note that, we do not include incorrect \(\rightarrow\) incorrect transition in Flips for two reasons: 1) For non-MCQ tasks such as GSM8k (Cobbe et al., 2021b), TriviaQA (Joshi et al., 2017), etc. exact per-token output matches between different models are rare, resulting in many mismatches. Thus, including this transition may artificially inflate the metric for these tasks. 2) For MCQ tasks, users may care less about these incorrect \(\rightarrow\) incorrect transitions. Nevertheless, _if we include incorrect \(\rightarrow\) incorrect transitions for MCQ tasks, we find that, the flips numbers reported in this paper would further increase by another 20-40% (e.g., increase of 19% in Hellaswag, 41% in ARC and 43% in MMLU! See Table 11)_

\begin{table}
\begin{tabular}{l c c c} \hline \hline Task & Llama3-8b Setting & \%Accuracy & \% Flips \\ \hline GSM8k & No noise & 49.39 & - \\  & Noise std = \(10^{-4}\) & 48.97 & 8.23 \\ \hline ARC-challenge & No noise & 53.24 & - \\  & Noise std = \(5\cdot 10^{-4}\) & 53.49 & 6.05 \\ \hline \hline \end{tabular}
\end{table}
Table 1: Adding Gaussian noise to weights results in _approximately equal \(correct\to incorrect\)_ and \(incorrect\to correct\) transitions, with the overall model accuracy mostly unchanged.

- _distance_ metric: consider a dataset having samples with multiple-choice answer options, where the j-th token of the i-th answer option has a probability distribution \(P_{b}(i,j)\) across all tokens in the vocabulary of the baseline model, and \(P_{q}(i,j)\) for the quantized model. Then the KL-divergence between the models for the entire dataset is the mean of KL-divergences across all tokens of all answer options and all samples in the dataset. \[KL\;div=\frac{1}{N}\sum_{dataset}\frac{1}{|options|}\sum_{i\in options}\frac{1 }{|tokens|}\sum_{j\in tokens}D_{KL}(P_{b}(i,j)||P_{q}(i,j))\] (1) where N is the number of samples in the dataset and \(D_{KL}(P||Q)\) is the standard KL-Divergence between two probability distributions.

The flips metric is propitious because it is a proxy distance metric that is easily interpretable by end-users- for question-answering tasks, the end user typically cares about the correct/incorrect answers and not the underlying probability distribution of tokens. Further, the flips metric is as easy to calculate as accuracy for any dataset.

It is important to distinguish between _capability_ metrics (accuracy and perplexity in this study) and _distance_ metrics (_KL-Divergence_ and _flips_ in this study). This distinction is necessary because the goal of a compression scheme is to create a more efficient model that closely mimics the baseline model rather than to create a more capable model. In other words, a quantized model is intended to serve as a drop-in replacement for the baseline model with minimal impact on end-users. Therefore, we argue that _distance_ metrics are more suitable for judging the effectiveness of quantization or other compression schemes.

## 3 Experiments

We have measured the above metrics on multiple LLMs using multiple quantization techniques and bit lengths, on several tasks, as listed below:

* Models: We primarily used the Llama2 (7B, 13B, 70B) chat (Touvron et al., 2023), Yi (6B, 34B) chat (01.AI et al., 2024), Llama3 (8B, 70B) (Dubey et al., 2024) and Owen2 (1.5B, 7B, 72B) (Yang et al., 2024) families of models. The chat versions were used because they can be evaluated on MT-Bench (Zheng et al., 2023). However, we have observed a similar phenomenon in their pretrained non-chat versions as well (see Table 12).
* Quantization: We have evaluated LLM.int8() (Dettmers et al., 2022) as implemented in Bitsandbytes (Dettmers, 2024), with its 8-bit and 4-bit versions (referred to as BnB W8A8 and BnB W4A4 respectively) with default parameters supported with HuggingFace Transformers (Wolf et al., 2020). We used GPTQ (Frantar et al., 2023), AWQ (Lin et al., 2024) with group-size 128 with other parameters being default. We used Smoothquant (Xiao et al., 2024) (referred to as SQ W8A8) with per-token, per-channel quantization using \(\alpha=0.5\). We use TensorRT (NVIDIA, 2024) for SmoothQuant, all other schemes were evaluated using HuggingFace Transformers.
* Tasks: 1. For the Llama2 and Yi families, we evaluate the compressed models on ten different tasks. They include MMLU (Hendrycks et al., 2021) Table 4, ARC (Clark et al., 2018)(easy Table 7 and challenge Table 8), PIQA (Bisk et al., 2019) Table 5, Winogrande (Sakaguchi et al., 2019) Table 10, Hellaswag (Zellers et al., 2019) Table 6, and Lambda (Zellers et al., 2019) Table 9. We also use GSM8k (Cobbe et al., 2021) Figure 13, TriviaQA Joshi et al. (2017) Figure 14 and MT-Bench (Zheng et al., 2023) to evaluate models on generative tasks. MT-Bench is a dataset with 80 two-turn questions which can test generative capabilities of a model. In this study, we have used GPT-4 (OpenAI et al., 2024) (v0314) as judge, to generate the scores reported in Table 2.
2. For the Qwen2 and Llama3 families, we evaluate on MMLU Table 17, GSM8k Table 21, ARC (easy Table 18, challenge Table 19), MATH (Hendrycks et al., 2021) Table 20, BFCL (Yan et al., 2024) Figure 23, and Scrolls-Quality (Shaham et al., 2022) Table 22* Harness- We used Eleuther AI's eval-harness (Gao et al., 2023) for all the experiments, unless specified otherwise. Note that the standard benchmarks (ARC, MMLU, PIQA, Hellaswag, LAMBADA, GSM8k, TriviaQA, MATH, etc.) results on all models use greedy decoding, making these results fully deterministic.

## 4 Results

In this section, we present extensive evidence for flips across various quantization and pruning schemes, evaluated over a large number of models and all tasks. Results for MT-Bench are presented in Section 6.

### Quantization schemes

Summary of our results is highlighted in Figure 1 while the performance on each of the individual seven tasks (MMLU, PIQA, Hellaswag, ARC Easy, ARC Challenge, LAMBADA and Winogrande) are in Tables 4 to 10, respectively, in the Appendix.

The main observations from our experiments with quantized models can be summarized as follows:

1. **Accuracy:** Accuracy is preserved within 1% for the majority of the quantization methods, tasks and models (see Tables 4- 9). This indicates that accuracy is not sufficient to distinguish between precise and permissive quantization schemes.
2. **Flips:** The large %flips (\(\geq\) 5%) is a general trend, which holds over different models, almost all quantization schemes, and tasks (see Tables 4- 10). Specifically, all quantization schemes except GPTQ W8A16 (\(<\) 1%) have significant %flips. Lower bit quantization schemes have greater %flips in general, indicating greater difference in behavior from the baseline (for example, on MMLU, BnB W4A4 has, on average, 2.4\(\times\) more flips than BnB W8A8). We focus on Flips in this study, but _AllFlips (Flips + incorrect\(\rightarrow\)incorrect transitions )_ results can be found in Figure 10, and Table 11 in Appendix.
3. **KL-Divergence vs Flips:** From Figure 2, we observe that the two distance metrics _KL-Divergence_ and _%flips_ are well correlated. For example, their Spearman correlation on the MMLU benchmark is 0.981.
4. **Impact of task type:** * _MCQ_ Tasks - Generally easier tasks (identified by higher average accuracy) have smaller %flips. For example, MMLU which is a relatively hard task has 8-16% flips for Bitsandbytes W4A4 whereas for the same technique, PIQA, an easier task, has 3-6% flips. The reason for this behavior is explained in Section 5. * _Generative_ Tasks - Surprisingly, such tasks have much _more flips_ than MCQ ones. For example, GSM8K (Table 13, Table 21), a hard task that requires reasoning over multiple steps, exhibits a significant amount of _flips_ (10-25% for BnB W8A8 and W4A4). Similarly, in MATH Table 20, we observe 5-15% _flips_ for BnB W4A4. However, _flips_ are quite small (2-4%) in easier tasks like TriviaQA(Table 14) that tests trivia question answering capabilities.

Figure 2: Flips and KL Divergence are well correlated. Each point corresponds to a model, quantization combination in Table 4

5. **Impact of model size:** Larger models typically have fewer flips than smaller ones. For example, on the MMLU benchmark with BnB W4A4, Llama2-70b chat has 5.6% flips, while Llama2-13b chat and Llama2-7b chat have 1.4\(\times\) and 1.6\(\times\) more flips, respectively. This may be because larger models are more resistant to perturbations introduced by compression than smaller ones.

### Other model compression techniques

We also evaluate the following three compression techniques, though on a smaller set of tasks and models. Our general observations seen above holds.

1. **Dropping last n-layers**(Gromov et al., 2024): This work demonstrated that dropping the last few layers did not affect the accuracy on standard benchmarks. We find in Figure 3(a) that as one keeps dropping layers, even though the accuracy increases only modestly, %flips increases significantly, demonstrating that the resulting models keep deviating further away from the baseline.
2. **Wanda**(Sun et al., 2023): This is a pruning method. We observe in Figure 3(b) that as we increase the pruning ratio, even though accuracy barely changes, %flips increases steadily.
3. **SliceGPT**(Ashkboos et al., 2024): This is a model sparsification method which drops a certain fraction of rows and columns of each dense matrix. We observe in Figure 9 in Appendix that even at very low sparsity ratios %flips is significant indicating that the compressed models are probably very different from baseline.

### Perplexity

Though we have focused on accuracy so far, our observation that the difference between two models' output token values cancel out leaving the average metric result unchanged, is applicable to perplexity as well. In particular, since perplexity may be interpreted as the inverse of the geometric mean of token probabilities, lower probabilities for some tokens in the test dataset may be cancelled by higher probabilities of other tokens. This indicates that perplexity alone is also inadequate in evaluating model compression schemes. Therefore, we argue that along with perplexity, KL-Divergence between the distributions generated by the baseline and optimized models should also be reported.

Figure 11 in Appendix plots the log-likelihood difference between the 16-bit and quantized model for each of the tokens in the wiki-2 dataset (Merity et al., 2016) for four different quantization schemes. From the figure, it appears that the log-likelihoods of the quantized model is just the log-likelihood of the baseline model with some symmetric noise added. Now, since perplexity is \(e^{-avg(logprobabilities)}\), adding _any_ amount of symmetric noise leaves it unchanged. For example, addition of Gaussian noise to the log-probability outputs of the model maintains the perplexity, while

Figure 3: MMLU 5-shot accuracy difference and flips for two compression techniques (Llama2-13b model). Even at early stages of pruning with no accuracy difference, flips indicate model divergence.

the quality of generation degrades as the standard deviation of the added noise increases (see Table 29). This analysis demonstrates one key weakness with the perplexity metric when used for evaluating compression techniques. While it is not clear if adding Gaussian noise to the log-likelihoods is an accurate representation of the behavior of compression schemes, it appears to be a reasonable proxy. As we shall see in Section 6, as quantization increases, there is steady degradation in the quality of the text generated by the model that are visible only by examining them closely.

## 5 Analyzing Flips

One of the interesting observations in this study has been that when we quantize models, the number of questions where the LLM's answers go from incorrect to correct (referred to as \(incorrect\to correct\)) is roughly equal to the number that goes the other way. This may seem unintuitive, because one might expect \(correct\to incorrect\gg incorrect\to correct\), since a) the number of questions with correct answers is usually greater than incorrect answers, so random perturbations should cause more correct answers to flip, and b) given a correct answer, the correct to incorrect transition should be likelier because changing to any of multiple other incorrect options suffices, but given an incorrect answer, the incorrect to correct transition happens only if somehow the perturbation caused by quantization helps it land on the one correct option out of many. But we observe that this is not the case (and indeed, the opposite may also be true in some cases!).

To help explain the above phenomenon, we introduce a metric called _top margin_ which is the difference in token probability between the best and the second best answer option. By best (second-best) option, we mean the option that was given the highest (second highest) probability. Higher top margin on a question indicates that the model is more confident about its answer.

**Answers are likely to change when top margin is low.** Quantization introduces some noise in the weights and activations, due to which there is a perturbation in the output answers' probabilities (verified empirically). Thus, we expect that answers are more likely to change when top margin is low, since a small increase or decrease in probabilities can cause the best and second best options to swap (see Figure 4). To further bolster this claim, we show that the changes in probabilities do not depend on top margin, i.e., roughly all questions undergo the same amount noise (except when the top-margin is very high, where we do not see much change in probabilities after compression, but such questions are not likely to flip anyway) as seen in Figure 7. We further find top margins are well correlated before/after compression (i.e low confidence answers are likely to remain so and vice-versa) in Figure 8. In subsection A.4 we find that due to this reason, it is very likely that the same question (with low top margin) would be flipped by multiple quantization schemes.

**Correct (incorrect) answers have higher (lower) top margin and are thus less (more) likely to flip.** Table 27 shows the top margins for questions for which the LLM's answer is correct and when the answer is incorrect. We observe that, top margin when correct is, on average, greater than the top 

[MISSING_PAGE_FAIL:8]

despite these two compressed models matching baseline accuracy on various tasks (e.g., MMLU accuracy within 1%) and suffering only a 0.4 lower score on a scale of ten in the GPT4 evaluation. We believe that this qualitative analysis adds further evidence to our claim that benchmark accuracy alone, as is standard practice today, is a poor metric to evaluate compressed LLMs, especially, if they are likely to be used for generative tasks in downstream applications.

\begin{table}
\begin{tabular}{p{142.3pt} p{142.3pt}} \hline \hline
**MT-Bench Prompt** & **Summary of 16-bit, 8-bit (BnB W8A8), and 4-bit (BnB W4A4) Llama-2-70B-chat model responses** \\ \hline
1) Consider a satellite that is in a circular orbit around the Earth. The speed of the satellite decreases. What will happen to the satellite’s orbital radius and period of revolution? Please justify your answer using principles of Physics. \\
2) Take your previous response and rephrase it as a limerick. \\
3) Could you write a captivating short story beginning with the sentence: The old abandoned house at the end of the street held a secret that no one had ever discovered. \\
4) You can see a beautiful red house to your left and a hypnotic greenhouse to your right, an attractive heated pink place in the front. So, where is the White House? \\
5) What about when twice the number is divided by 5? \\
6) Reformulate your earlier reply, output it in JSON format and only include books published after 1980. \\
7) Can you change the ratings from numbers to letters? Capital letters MUST be used when writing the names of phones. \\
8) Given a set of complex equations, extract all unique variable names from each equation... \\
9) Rewrite your previous response. Start every sentence with an A. \\
10) What is the central dogma of molecular biology? What processes are involved? Who named this? \\ \hline \hline \end{tabular}
\end{table}
Table 3: Qualitative evaluation of Llama2-70B-chat model text generations for MT-Bench prompts. Author’s summary of model responses shown below; full model generated responses are in Appendix. These results substantiate a clear degradation in response quality with quantization.

Figure 6: _Flips_ is a better predictor of downstream task performance than _Accuracy_

## 7 Limitations

Predicting performance degradation of LLMs in the wild is a challenging and open problem, and it is possible that _any_ metric calculated on standard benchmarks is insufficient. Other limitations are:

* If the downstream task is very similar to the benchmark on which the quantized model is tested, then accuracy may be sufficient, and distance metrics are not needed.
* this may or may not materialize as visible degradation in some downstream tasks.
* Our qualitative evaluation in Section 6.1 is subjective and may not be broadly representative.

## 8 Related Work

Given their versatility, LLMs are evaluated on a diverse set of tasks (Chang et al., 2024). Since accuracy is one of the most well-accepted metrics used in task evaluation, compression methods today typically focus on accuracy. However, we are not the first to point out the problem with over-reliance on aggregate metrics like accuracy when judging the quality of a model optimization scheme. Xu et al. (2021) have proposed label loyalty and probability loyalty as a metric to evaluate compressed BERT models. Other works like Joseph et al. (2021), Hooker et al. (2020), and Hooker et al. (2021) have shown compressed ImageNets to be more biased despite preserving accuracy and have proposed Knowledge Distillation based methods to address it. There has also been work (Hong et al., 2024) on evaluating LLM compression schemes on various trustworthiness dimensions. However, metrics for evaluating LLM compression techniques have not been studied widely so far, leading to over reliance on accuracy alone.

There have been many works on LLM evaluation that have shown shortcomings of existing evaluation methods. Lyu et al. (2024) have pointed out the misalignment between free-form generation and probability based evaluation on MMLU. Sclar et al. (2023) have shown LLMs to be very sensitive to prompt formatting. Zheng et al. (2024) have shown models to be biased towards a certain option in MCQ tasks. Alzahrani et al. (2024) have shown minor changes in the benchmarks leading to re-ordering of rankings, and Srivastava et al. (2024) has shown accuracies to be different when considering the _functional_ equivalent of math problems. Jaiswal et al. (2024) have curated existing datasets to create their own benchmark that can be used to evaluate compressed models. Li et al. (2024) and Jin et al. (2024) have evaluated various quantization tasks on multiple tasks. Namburi et al. (2023) have studied the impact of compression and pruning on an LLM's _parametric_ knowledge. Zhang et al. (2024) propose a number of other metrics in addition to accuracy such as fluency, informativeness, coherence and harmlessness. Chang et al. (2024) presents a detailed survey on evaluation of LLMs that covers what, where, and how to evaluate an LLM and lists several challenges in LLM evaluation.

However, to the best of our knowledge, none of the prior work have pointed out the phenomenon of flips, that occurs when LLMs are compressed, and the observation that higher flips is correlated with larger degradation in model performance despite accuracy matching with the uncompressed model.

## 9 Conclusion

In this work, we have examined metrics to evaluate the quality of compression methods for LLMs such as quantization. We distinguish between aggregate capability metrics such as accuracy, and distance metrics _flips_ and _KL Divergence_ between the compressed model and the baseline model. We justify why using distance metrics is more appropriate for evaluating model compression methods. We show that accuracy severely underestimates the true distance between models as perceived by the end user. We explain this is due to the presence of flips between correct and wrong answers when a model is quantized, and explain why the flips are nearly balanced, leading to similar accuracy, while the user-perceived output of the quantized model may be significantly different. We argue that distance metrics such as flips and KL-divergence are essential for evaluating all optimization methods which may change the model outputs and whose goal is to minimize end-user visible behaviour changes from a baseline model. We hope that better distance metrics as proposed in this work will enable research in model optimization and compression to progress faster and better meet user expectations on model output quality.

## References

* (1)
* A. Young et al. (2024)AI, Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, Kaidong Yu, Peng Liu, Qiang Liu, Shawn Yue, Senbin Yang, Shiming Yang, Tao Yu, Wen Xie, Wenhao Huang, Xiaohui Hu, Xiaoyi Ren, Xinyao Niu, Pengcheng Nie, Yuchi Xu, Yudong Liu, Yue Wang, Yuxuan Cai, Zhenyu Gu, Zhiyuan Liu, and Zonghong Dai. 2024. Yi: Open Foundation Models by 01.AI. arXiv:2403.04652 [cs.CL]
* Alzahrani et al. (2024) Norah Alzahrani, Hisham Abdullah Alyahya, Yazeed Alnumay, Sultan Alrashed, Shaykhah Alsubaie, Yusef Almushaykeh, Faisal Mirza, Nouf Alotaibi, Nora Altaviaresh, Areeb Alowisheq, M Saiful Bari, and Haidar Khan. 2024. When Benchmarks are Targets: Revealing the Sensitivity of Large Language Model Leaderboards. arXiv:2402.01781 [cs.CL]
* Ashkboos et al. (2024) Saleh Ashkboos, Maximilian L. Croci, Marcelo Gennari do Nascimento, Torsten Hoefler, and James Hensman. 2024. SliceGPT: Compress Large Language Models by Deleting Rows and Columns. arXiv:2401.15024 [cs.LG]
* Bisk et al. (2019) Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. 2019. PIQA: Reasoning about Physical Commonsense in Natural Language. arXiv:1911.11641
* Chang et al. (2024) Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, et al. 2024. A survey on evaluation of large language models. _ACM Transactions on Intelligent Systems and Technology_ 15, 3 (2024), 1-45.
* Clark et al. (2018) Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018. Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge. arXiv:1803.05457
* Cobbe et al. (2021a) Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. 2021a. Training verifiers to solve math word problems. _arXiv preprint arXiv:2110.14168_ (2021a).
* Cobbe et al. (2021b) Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021b. Training Verifiers to Solve Math Word Problems. arXiv:2110.14168 [cs.LG]
* Dettmers (2024) Dettmers. 2024. Bitsandbytes. https://github.com/TimDettmers/bitsandbytes.
* Dettmers et al. (2022) Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. 2022. GPT3.int8(): 8-bit Matrix Multiplication for Transformers at Scale. In _Advances in Neural Information Processing Systems_, S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh (Eds.), Vol. 35. Curran Associates, Inc., 30318-30332. https://proceedings.neurips.cc/paper_files/paper/2022/file/c3ba4962c05c49636d4c6206a97e9c8a-Paper-Conference.pdf
* Dubey et al. (2022) Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Roziere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Graeme Nail, Gregorie Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arriate Barra, Isabel Klountann, Ishan Misra, Ivan Evtimov, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Ject Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, Khalid El-Arini, Krithika Iyer, Kshitiz Malik, Kuenley Chiu, Kunal Bhalla, Lauren Rantala-Yeary, Laurens van der Maaten, LawrenceChen, Liang Tan, Liz Jenkins, Louis Martin, Lovish Macaan, Lubo Malo, Lukas Blecher, Lukas Landzaat, Luke de Oliveira, Madeline Muzzi, Mahesh Pasupuleti, Mannat Singh, Manohar Paluri, Marcin Kardas, Mathew Oldham, Mathieu Rita, Maya Pavlova, Melanie Kambadur, Mike Lewis, Min Si, Mitesh Kumar Singh, Mona Hassan, Naman Goyal, Narjes Torabi, Nikolay Bashlykov, Nikolay Bogoychev, Niladri Chatterji, Olivier Duchenne, Onur Celebi, Patrick Alrassy, Pengchuan Zhang, Pengwei Li, Petar Vasic, Peter Weng, Prajjwal Bhargava, Pratik Dubal, Praveen Krishnan, Punit Singh Koura, Puxin Xu, Qing He, Qingxiao Dong, Ragavan Srinivasan, Raj Ganapathy, Ramon Calderer, Ricardo Silveira Cabral, Robert Stojnic, Roberta Raileanu, Rohit Girdhar, Rohit Patel, Romain Sauvestre, Ronnie Polidoro, Roshan Sumbaly, Ross Taylor, Ruan Silva, Rui Hou, Rui Wang, Saghar Hosseini, Sahana Chennabasappa, Sanjay Singh, Sean Bell, Seohyun Sonia Kim, Sergey Edunov, Shaoliang Nie, Sharan Narang, Sharath Raparthy, Sheng Shen, Shengye Wan, Shruti Bhosale, Shun Zhang, Simon Vandenhende, Soumya Batra, Spencer Whitman, Sten Sootla, Stephane Collot, Suchin Gururangan, Sydney Borodinsky, Tamar Herman, Tara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas Scialom, Tobias Speckbacher, Todor Mihaylov, Tong Xiao, Ujjwal Kar, Vedanyi Goswami, Vibhor Gupta, Vignesh Ramanathan, Viktor Kerker, Vincent Gonguet, Virginie Do, Vish Vogeti, Vladan Petrovic, Weiwei Chu, Wenhan Xiong, Wenyin Fu, Whitney Meers, Xavier Martinet, Xiaodong Wang, Xiaoqing Ellen Tan, Xinfeng Xie, Xuchao Jia, Xuewei Wang, Yaelle Goldschlag, Yashesh Gaur, Yasmine Babaei, Yi Wen, Yiwen Song, Yuchen Zhang, Yue Li, Yuning Mao, Zacharie Delpierre Coudert, Zheng Yan, Zhengxing Chen, Zoe Papakipos, Aaditya Singh, Aaron Grattafiori, Abha Jain, Adam Kelsey, Adam Shajnfeld, Adithya Gangidi, Adolfo Victoria, Ahuva Goldstand, Ajay Menon, Ajay Sharma, Alex Boesenberg, Alex Vaughan, Alexei Baevski, Allie Feinstein, Amanda Kallelet, Amit Sangani, Anam Yunus, Andrei Lupu, Andres Alvarado, Andrew Caples, Andrew Gu, Andrew Ho, Andrew Poulton, Andrew Ryan, Ankit Ramchandani, Annie Franco, Aparajita Sarraf, Arkabandhu Chowdhury, Ashley Gabriel, Ashwin Bharambe, Assif Eisemma, Azadeh Yazdan, Beau James, Ben Maurer, Benjamin Leonhardt, Bernie Huang, Beth Loyd, Beto De Paola, Bhargavi Paranjape, Bing Liu, Bo Wu, Boyu Ni, Braden Hancock, Bram Wasti, Brandon Spence, Brani Stojkovic, Brian Gamido, Britt Montalvo, Carl Parker, Carly Burton, Catalina Mejia, Changhan Wang, Changkyu Kim, Chao Zhou, Chester Hu, Ching-Hsiang Chu, Chris Cai, Chris Tindal, Christoph Feichtenhofer, Damon Civin, Dana Beaty, Daniel Kreymer, Daniel Li, Danny Wyatt, David Adkins, David Xu, Davide Testuggine, Delia David, Devi Parikh, Diana Liskovich, Didem Foss, Dingkang Wang, Duc Le, Dustin Holland, Edward Dowling, Eissa Jamil, Elaine Montgomery, Eleonora Presani, Emily Hahn, Emily Wood, Erik Brinkman, Esteban Arcaute, Evan Dunbar, Evan Smothers, Fei Sun, Felix Kreuk, Feng Tian, Firat Ozgenel, Francesco Caggioni, Francisco Guzman, Frank Kanayet, Frank Seide, Gabriela Medina Florez, Gabriella Schwarz, Gada Badeer, Georgia Swee, Gil Halpern, Govind Thattai, Grant Herman, Grigory Sizov, Guangyi, Zhang, Guna Lakshminarayanan, Hamid Shojanazeri, Han Zou, Hannan Wang, Hanwen Zhao, Haroun Habeeb, Harrison Rudolph, Helen Suk, Henry Aseperen, Hunter Goldman, Ibrahim Damlaj, Igor Molybog, Igor Tufanov, Irina-Elena Veliche, Itai Gat, Jake Weissman, James Geboski, James Kohli, Japhet Asher, Jean-Baptiste Gaya, Jeff Marcus, Jeff Tang, Jennifer Chan, Jenny Zhen, Jeremy Reizenstein, Jeremy Teboul, Jessica Zhong, Jian Jin, Jingyi Yang, Joe Cummings, Jon Carrill, Jon Shepard, Jonathan McPhie, Jonathan Torres, Josh Ginsburg, Junjie Wang, Kai Wu, Kam Hou U, Karan Saxena, Karthik Prasad, Kartiaky Khandelwal, Katayoun Zand, Kathy Matosich, Kaushik Veeraraghavan, Kelly Michelena, Keqian Li, Kun Huang, Kunal Chawla, Kushal Lakhotia, Kyle Huang, Lailin Chen, Lakshya Garg, Lavender A, Leandro Silva, Lee Bell, Lei Zhang, Liangpeng Guo, Licheng Yu, Liron Moshkovich, Luca Wehrstedt, Madian Khbas, Manav Avalani, Manish Bhatt, Maria Tsimpoukelli, Martynas Mankus, Matan Hassan, Matthew Lennie, Matthias Reso, Maxim Groshev, Maxim Naumov, Maya Lathi, Meghan Keneally, Michael L. Seltzer, Michal Valko, Michelle Restrepo, Mihir Patel, Mik Vyatskov, Mikayel Samvelyan, Mike Clark, Mike Macey, Mike Wang, Miquel Jubert Hermoso, Mo Metanat, Mohammad Rastegari, Munish Bansal, Nandhini Santhanam, Natascha Parks, Natasha White, Navyata Bawa, Nayan Singhal, Nick Egebo, Nicolas Usunier, Nikolay Pavlovich Laptev, Ning Dong, Ning Zhang, Norman Cheng, Oleg Chernoguz, Olivia Hart, Omkar Salpekar, Ozlem Kalinli, Parkin Kent, Parth Parekh, Paul Saab, Pavan Balaji, Pedro Rittner, Philip Bontrager, Pierre Roux, Piotr Dollar, Polina Zvyagina, Prashant Ratanchandani, Pritish Yuvraj, Qian Liang, Rachad Alao, Rachel Rodriguez, Rafi Ayub, Raghotham Murthy, Raghu Nayani, Rahul Mitra, Raymond Li, Rebekkah Hogan, Robin Battey, Rocky Wang, Rohan Maheswari, Russ Howes, Ruty Rinott, Sai Jayesh Bondu, Samyak Datta, Sara Chugh, Sara Hunt, Sargun Dhillon, Sasha Sidorov, Satadru Pan, Saurabah Verma, Seiji Yamamoto, Sharadh Ramaswamy, Shaun Lindsay, Shaun Lindsay, Sheng Feng, Shenghao Lin, Shengxin Cindy Zha, Shiva Shankar, Shuqiang Zhang,Shuqiang Zhang, Sinong Wang, Sneha Agarwal, Soji Sajuyigbe, Soumith Chintala, Stephanie Max, Stephen Chen, Steve Kehoe, Steve Satterfield, Sudarshan Govindaprasad, Sumit Gupta, Sungmin Cho, Sunny Virk, Suraj Subramanian, Sy Choudhury, Sydney Goldman, Tal Remez, Tamar Glaser, Tamara Best, Thilo Kohler, Thomas Robinson, Tianhe Li, Tianjun Zhang, Tim Matthews, Timothy Chou, Tezook Shaked, Varun Vontimitta, Victoria Ajayi, Victoria Montanez, Vijai Mohan, Vinay Satish Kumar, Vishal Mangla, Vitor Albiero, Vlad Ionescu, Vlad Poenaru, Vlad Tiberiu Mihailescu, Vladimir Ivanov, Wei Li, Wenchen Wang, Wenwen Jiang, Wes Bouaziz, Will Constable, Xiaocheng Tang, Xiaofang Wang, Xiaojian Wu, Xiaolan Wang, Xide Xia, Xilun Wu, Xinbo Gao, Yanjun Chen, Ye Hu, Ye Jia, Ye Qi, Yenda Li, Yilin Zhang, Ying Zhang, Yossi Adi, Youngjin Nam, Yu, Wang, Yuchen Hao, Yundi Qian, Yuzi He, Zach Rait, Zachary DeVito, Zef Rosnbrick, Zhaoduo Wen, Zhenyu Yang, and Zhiwei Zhao. 2024. The Llama 3 Herd of Models. arXiv:2407.21783 [cs.AI]
* Eleuther (2021) Eleuther. 2021. Multiple Choice Normalization in LM Evaluation. https://blog.eleuther.ai/multiple-choice-normalization/.
* Frantar et al. (2023) Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. 2023. GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers. arXiv:2210.17323 [cs.LG]
* Gao et al. (2023) Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noac'h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. 2023. A framework for few-shot language model evaluation. https://doi.org/10.5281/zenodo.10256836
* Ge et al. (2023) Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, and Jianfeng Gao. 2023. Model tells you what to discard: Adaptive kv cache compression for llms. _arXiv preprint arXiv:2310.01801_ (2023).
* Gromov et al. (2024) Andrey Gromov, Kushal Tirumala, Hassan Shapourian, Paolo Glorioso, and Daniel A. Roberts. 2024. The Unreasonable Ineffectiveness of the Deeper Layers. arXiv:2403.17887 [cs.CL]
* Hendrycks et al. (2021a) Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2021a. Measuring Massive Multitask Language Understanding. arXiv:2009.03300
* Hendrycks et al. (2021b) Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021b. Measuring Mathematical Problem Solving With the MATH Dataset. arXiv:2103.03874 [cs.LG] https://arxiv.org/abs/2103.03874
* Hong et al. (2024) Junyuan Hong, Jinhao Duan, Chenhui Zhang, Zhangheng Li, Chulin Xie, Kelsey Lieberman, James Diffenderfer, Brian Bartoldson, Ajay Jaiswal, Kaidi Xu, Bhavya Kailkhura, Dan Hendrycks, Dawn Song, Zhangyang Wang, and Bo Li. 2024. Decoding Compressed Trust: Scrutinizing the Trustworthiness of Efficient LLMs Under Compression. arXiv:2403.15447 [cs.CL]
* Hooker et al. (2021) Sara Hooker, Aaron Courville, Gregory Clark, Yann Dauphin, and Andrea Frome. 2021. What Do Compressed Deep Neural Networks Forget? arXiv:1911.05248 [cs.LG]
* Hooker et al. (2020) Sara Hooker, Nyalleng Moorosi, Gregory Clark, Samy Bengio, and Emily Denton. 2020. Characterising Bias in Compressed Models. arXiv:2010.03058 [cs.LG]
* Jaiswal et al. (2024) Ajay Jaiswal, Zhe Gan, Xianzhi Du, Bowen Zhang, Zhangyang Wang, and Yinfei Yang. 2024. Compressing LLMs: The Truth is Rarely Pure and Never Simple. arXiv:2310.01382 [cs.CL]
* Jelinek et al. (2005) F. Jelinek, R. L. Mercer, L. R. Bahl, and J. K. Baker. 2005. Perplexity--a measure of the difficulty of speech recognition tasks. _The Journal of the Acoustical Society of America_ 62, S1 (08 2005), S63-S63. https://doi.org/10.1121/1.2016299 arXiv:https://pubs.aip.org/asa/jasa/article-pdf/62/S1/S63/11558910/s63_5_online.pdf
* Jin et al. (2024) Renren Jin, Jiangcun Du, Wuwei Huang, Wei Liu, Jian Luan, Bin Wang, and Deyi Xiong. 2024. A Comprehensive Evaluation of Quantization Strategies for Large Language Models. arXiv:2402.16775 [cs.CL]Vinu Joseph, Shoaib Ahmed Siddiqui, Aditya Bhaskara, Ganesh Gopalakrishnan, Saurav Muralidharan, Michael Garland, Sheraz Ahmed, and Andreas Dengel. 2021. Going Beyond Classification Accuracy Metrics in Model Compression. arXiv:2012.01604 [cs.CV]
* Joshi et al. (2017) Mandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke Zettlemoyer. 2017. TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension. arXiv:1705.03551 [cs.CL]
* 86. https://doi.org/10.1214/aoms/1177729694
* Li et al. (2024) Shiyao Li, Xuefei Ning, Luning Wang, Tengxuan Liu, Xiangsheng Shi, Shengen Yan, Guohao Dai, Huazhong Yang, and Yu Wang. 2024. Evaluating Quantized Large Language Models. arXiv:2402.18158 [cs.CL]
* Lin et al. (2024) Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Wei-Ming Chen, Wei-Chen Wang, Guangxuan Xiao, Xingyu Dang, Chuang Gan, and Song Han. 2024. AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration. arXiv:2306.00978 [cs.CL]
* Lyu et al. (2024) Chenyang Lyu, Minghao Wu, and Alham Fikri Aji. 2024. Beyond Probabilities: Unveiling the Misalignment in Evaluating Large Language Models. arXiv:2402.13887 [cs.CL]
* Merity et al. (2016) Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. 2016. Pointer Sentinel Mixture Models. arXiv:1609.07843 [cs.CL]
* Namburi et al. (2023) Satya Sai Srinath Namburi, Makesh Sreedhar, Srinath Srinivasan, and Frederic Sala. 2023. The Cost of Compression: Investigating the Impact of Compression on Parametric Knowledge in Language Models. arXiv:2312.00960 [cs.CL]
* NVIDIA (2024) NVIDIA. 2024. TensorRT. https://github.com/NVIDIA/TensorRT-LLM.
* OpenAI et al. (2020) OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu, Haiming Bao, Mohammad Bavarian, Jeff Belgum, Irwan Bello, Jake Berdine, Gabriel Bernadett-Shapiro, Christopher Berner, Lenny Bogdonoff, Oleg Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman, Tim Brooks, Miles Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brittany Carey, Chelsea Carlson, Rory Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek Chen, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben Chess, Chester Cho, Casey Chu, Hyung Won Chung, Dave Cummings, Jeremiah Currier, Yunxing Dai, Cory Decareaux, Thomas Degry, Noah Deutsch, Damien Deville, Arka Dhar, David Dohan, Steve Dowling, Sheila Dunning, Adrien Ecoffet, Atty Eleti, Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix, Simon Posada Fishman, Juston Forte, Isabella Fulford, Leo Gao, Elie Georges, Christian Gibson, Vik Goel, Tarun Gogineni, Gabriel Goh, Rapha Gontijo-Lopes, Jonathan Gordon, Morgan Grafstein, Scott Gray, Ryan Greene, Joshua Gross, Shixiang Shane Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris, Yuchen He, Mike Heaton, Johannes Heidecke, Chris Hesse, Alan Hickey, Wade Hickey, Peter Hoeschelle, Brandon Houghton, Kenny Hsu, Shengli Hu, Xin Hu, Joost Huizinga, Shantanu Jain, Shawn Jain, Joanne Jang, Angela Jiang, Roger Jiang, Haozhun Jin, Denny Jin, Shino Jomoto, Billie Jonn, Heewoo Jun, Tomer Kaftan, Lukasz Kaiser, Ali Kamali, Ingmar Kanitscheider, Nitish Shirish Keskar, Tabarak Khan, Logan Kilpatrick, Jong Wook Kim, Christina Kim, Yongjik Kim, Jan Hendrik Kirchner, Jamie Kiros, Matt Knight, Daniel Kokotajlo, Lukasz Kondraciuk, Andrew Kondrich, Aris Konstantinidis, Kyle Kosic, Gretchen Krueger, Vishal Kuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan Leike, Jade Leung, Daniel Levy, Chak Ming Li, Rachel Lim, Molly Lin, Stephanie Lin, Mateusz Litwin, Theresa Lopez, Ryan Lowe, Patricia Lue, Anna Makanju, Kim Malfacini, Sam Manning, Todor Markov, Yaniv Markovski, Bianca Martin, Katie Mayer, Andrew Mayne, Bob McGrew, Scott Mayer McKinney, Christine McLeave, Paul McMillan, Jake McNeil, David Medina, Aalok Mehta, Jacob Menick, Luke Metz, Andrey Mishchenko, Pamela Mishkin, Vinnie Monaco, Evan Morikawa, Daniel Mossing, Tong Mu, Mira Murati, Oleg Murk, David Mely, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak, Arvind Neelakantan, Richard Ngo, Hyeonwoo Noh, Long Ouyang, Cullen O'Keefe, Jakub Pachocki, Alex Paino, Joe Palermo, Ashley Pantuliano, Giambattista Parascandolo, Joel Parish, Emy Parparita, Alex Passos, Mikhail Pavlov, Andrew Peng, Adam Perelman, Filipe de Avila Belbute Peres, Michael Petrov, Henrique Ponde de Oliveira Pinto, Michael, Pokorny, Michelle Pokrass, Vitchyr H. Pong, Tolly Powell, Alethea Power, Boris Power,Elizabeth Proehl, Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh, Cameron Raymond, Francis Real, Kendra Rimbach, Carl Ross, Bob Rotsted, Henri Roussez, Nick Ryder, Mario Saltarelli, Ted Sanders, Shibani Santurkar, Girish Sastry, Heather Schmidt, David Schnurr, John Schulman, Daniel Selsam, Kyla Sheppard, Toki Shepakov, Jessica Shieh, Sarah Shoker, Pranav Shyam, Szymon Sidor, Eric Sigler, Maddie Sinnens, Jordan Sitkin, Katarina Slama, Ian Sohl, Benjamin Sokolowsky, Yang Song, Natalia Staudacher, Felipe Petroski Such, Natalie Summers, Ilya Sutskever, Jie Tang, Nikolas Tezak, Madeleine B. Thompson, Phil Tillet, Amin Tootoonchian, Elizabeth Tseng, Preston Tuggle, Nick Turley, Jerry Tworek, Juan Felipe Ceron Uribe, Andrea Vallone, Arun Vijayergiya, Chelsea Voss, Carroll Wainwright, Justin Jay Wang, Alvin Wang, Ben Wang, Jonathan Ward, Jason Wei, CJ Weinmann, Akila Welihinda, Peter Welinder, Jiayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner, Clemens Winter, Samuel Wolrich, Hannah Wong, Lauren Workman, Sherwin Wu, Jeff Wu, Michael Wu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin Yu, Qiming Yuan, Wojciech Zaremba, Rowan Zellers, Chong Zhang, Marvin Zhang, Shengjia Zhao, Tianhao Zheng, Juntang Zhuang, William Zhuk, and Barret Zoph. 2024. GPT-4 Technical Report. arXiv:2303.08774 [cs.CL]
* Paperno et al. (2016) Denis Paperno, German Kruszewski, Angeliki Lazaridou, Quan Ngoc Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fernandez. 2016. The LAMBADA dataset: Word prediction requiring a broad discourse context. arXiv:1606.06031 [cs.CL]
* Sakaguchi et al. (2019) Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. 2019. WinoGrande: An Adversarial Winograd Schema Challenge at Scale. arXiv:1907.10641
* Sclar et al. (2023) Melanie Sclar, Yejin Choi, Yulia Tsvetkov, and Alane Suhr. 2023. Quantifying Language Models' Sensitivity to Spurious Features in Prompt Design or: How I learned to start worrying about prompt formatting. arXiv:2310.11324 [cs.CL]
* Shaham et al. (2022) Uri Shaham, Elad Segal, Maor Ivgi, Avia Efrat, Ori Yoran, Adi Haviv, Ankit Gupta, Wenhan Xiong, Mor Geva, Jonathan Berant, and Omer Levy. 2022. SCROLLS: Standardized CompaRison Over Long Language Sequences. arXiv:2201.03533 [cs.CL] https://arxiv.org/abs/2201.03533
* Srivastava et al. (2024) Saurabh Srivastava, Annarose M B, Anto P V au2, Shashank Menon, Ajay Sukumar, Adwait Samod T, Alan Philipose, Stevin Prince, and Sooraj Thomas. 2024. Functional Benchmarks for Robust Evaluation of Reasoning Performance, and the Reasoning Gap. arXiv:2402.19450 [cs.AI]
* Sun et al. (2023) Mingjie Sun, Zhuang Liu, Anna Bair, and J. Zico Kolter. 2023. A Simple and Effective Pruning Approach for Large Language Models. arXiv:2306.11695 [cs.CL]
* Touvron et al. (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koruz, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylovlov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Feizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023. Llama 2: Open Foundation and Fine-Tuned Chat Models. arXiv:2307.09288 [cs.CL]
* Wolf et al. (2020) Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. 2020. HuggingFace's Transformers: State-of-the-art Natural Language Processing. arXiv:1910.03771 [cs.CL]
* Xiao et al. (2024) Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han. 2024. SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models. arXiv:2211.10438 [cs.CL]Canwen Xu, Wangchunshu Zhou, Tao Ge, Ke Xu, Julian McAuley, and Furu Wei. 2021. Beyond Preserved Accuracy: Evaluating Loyalty and Robustness of BERT Compression. arXiv:2109.03228 [cs.CL]
* Yan et al. (2024) Fanjia Yan, Huanzhi Mao, Charlie Cheng-Jie Ji, Tianjun Zhang, Shishir G. Patil, Ion Stoica, and Joseph E. Gonzalez. 2024. Berkeley Function Calling Leaderboard. https://gorilla.cs.berkeley.edu/blogs/8_berkeley_function_calling_leaderboard.html.
* Yang et al. (2024) An Yang, Baosong Yang, Bingyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jianxin Yang, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Xuejing Liu, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, Zhifang Guo, and Zhihao Fan. 2024. Qwen2 Technical Report. arXiv:2407.10671 [cs.CL] https://arxiv.org/abs/2407.10671
* Zellers et al. (2019) Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019. HellaSwag: Can a Machine Really Finish Your Sentence? arXiv:1905.07830
* Zhang et al. (2024) Yue Zhang, Ming Zhang, Haipeng Yuan, Shichun Liu, Yongyao Shi, Tao Gui, Qi Zhang, and Xuanjing Huang. 2024. Llmeval: A preliminary study on how to evaluate large language models. In _Proceedings of the AAAI Conference on Artificial Intelligence_, Vol. 38. 19615-19622.
* Zheng et al. (2024) Chujie Zheng, Hao Zhou, Fandong Meng, Jie Zhou, and Minlie Huang. 2024. Large Language Models Are Not Robust Multiple Choice Selectors. arXiv:2309.03882 [cs.CL]
* Zheng et al. (2023) Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. 2023. Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena. arXiv:2306.05685 [cs.CL]

[MISSING_PAGE_FAIL:17]

\begin{table}
\begin{tabular}{l c c c c c c c} \hline Model & 16bit & Bn B W8A8 & GPTQ W8A16 & SQ W8A8 & GPTQ W4A16 & AWQ W4A16 & Bn B W4A4 \\ \hline Llama2-7b chat & 66.504 & 0.25 / 3.12 & 0.04 / 0.50 & -0.33 / 3.82 & -2.87 / 8.27 & -2.48 / 7.10 & -2.05 / 9.63 \\ Llama2-13b chat & 68.542 & -0.11 / 2.52 & 0.02 / 0.29 & -0.42 / 3.07 & -1.28 / 5.36 & -1.18 / 5.41 & -2.36 / 7.18 \\ Llama2-70b chat & 73.801 & -0.21 / 1.14 & 0.07 / 0.27 & -0.19 / 2.40 & -0.21 / 3.98 & -0.77 / 3.80 & -0.50 / 4.85 \\ Yi-6b chat & 64.331 & 0.79 / 3.24 & 0.38 / 1.01 & NA / NA & -0.79 / 8.64 & -0.46 / 6.95 & -2.31 / 10.54 \\ Yi-34b chat & 69.571 & -1.69 / 5.10 & 0.19 / 1.16 & NA / NA & -0.19 / 8.58 & -0.40 / 4.75 & 0.19 / 7.72 \\ \hline \end{tabular}
\end{table}
Table 10: Winogrande (0-shot) change in %accuracy / %flips

\begin{table}
\begin{tabular}{l c c c c c c} \hline Model & 16bit & Bn B W8A8 & GPTQ W8A16 & SQ W8A8 & GPTQ W4A16 & AWQ W4A16 & Bn B W4A4 \\ \hline Llama2-7b chat & 66.456 & -0.71 / 4.97 & -0.39 / 0.55 & -0.08 / 4.81 & -0.87 / 8.60 & -1.81 / 8.44 & 0.47 / 10.26 \\ Llama2-13b chat & 71.112 & -0.08 / 4.65 & 0.08 / 0.23 & 0.63 / 4.89 & -1.50 / 6.71 & -0.23 / 6.87 & -1.73 / 8.21 \\ Llama2-70b chat & 74.901 & 0.55 / 3.55 & 0.15 / 0.31 & 0.23 / 2.29 & -0.08 / 4.50 & 0.08 / 3.23 & 0.00 / 5.84 \\ Yi-6b chat & 70.876 & -0.31 / 5.05 & -0.15 / 1.42 & NA / NA & -2.21 / 9.31 & 1.73 / 7.89 & -1.34 / 10.97 \\ Yi-34b chat & 76.874 & 0.47 / 5.37 & 0.08 / 1.34 & NA / NA & 0.31 / 7.73 & -0.47 / 3.16 & 0.47 / 7.10 \\ \hline \end{tabular}
\end{table}
Table 11: MMLU 5-shot change in %accuracy and %AllFlips (including _wrong_\(\rightarrow\)_wrong_ transitions)

\begin{table}
\begin{tabular}{l c c c c c c} \hline Model & 16bit & Bn B W8A8 & GPTQ W8A16 & SQ W8A8 & GPTQ W4A16 & AWQ W4A16 & Bn B W4A4 \\ \hline Llama2-7b chat & 44.283 & -0.25 / 4.01 & 0.00 / 0.51 & -0.59 / 5.38 & -1.36 / 8.19 & -0.25 / 6.57 & 0.68 / 9.22 \\ Llama2-13b chat & 50.170 & -0.25 / 3.50 & 0.08 / 0.26 & -2.04 / 5.29 & -1.02 / 5.97 & 0.00 / 6.31 & -0.85 / 8.19 \\ Llama2-70b chat & 54.266 & 0.25 / 2.99 & 0.17 / 0.51 & 0.00 / 1.71 & -0.76 / 4.01 & 0.00 / 2.90 & -0.93 / 5.03 \\ Yi-6b chat & 47.269 & -0.94 / 4.18 & 0.68 / 1.19 & NA / NA & 0.68 / 8.70 & -2.04 / 5.97 & -0.42 / 9.30 \\ Yi-34b chat & 54.522 & 0.42 / 5.20 & -0.59 / 1.11 & NA / NA & -0.76 / 6.91 & -0.85 / 4.95 & -0.94 / 6.91 \\ \hline \end{tabular}
\end{table}
Table 8: ARC Challenge (0-shot) change in %accuracy / %flips

\begin{table}
\begin{tabular}{l c c c c c c} \hline Model & 16bit & Bn B W8A8 & GPTQ W8A16 & SQ W8A8 & GPTQ W4A16 & AWQ W4A16 & Bn B W4A4 \\ \hline Llama2-7b chat & 66.504 & 0.25 / 3.12 & 0.04 / 0.50 & -0.33 / 3.82 & -2.87 / 8.27 & -2.48 / 7.10 & -2.05 / 9.63 \\ Llama2-13b chat & 68.542 & -0.11 / 2.52 & 0.02 / 0.29 & -0.42 / 3.07 & -1.28 / 5.36 & -1.18 / 5.41 & -2.36 / 7.18 \\ Llama2-70b chat & 73.801 & -0.21 / 1.14 & 0.07 / 0.27 & -0.19 / 2.40 & -0.21 / 3.98 & -0.77 / 3.80 & -0.50 / 4.85 \\ Yi-6b chat & 64.331 & 0.79 / 3.24 & 0.38 / 1.01 & NA / NA & -0.79 / 8.64 & -0.46 / 6.95 & -2.31 / 10.54

[MISSING_PAGE_EMPTY:19]

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline Model & 16-bit Baseline & BnB W8A8 & GPTQ W8A16 & GPTQ W4A16 & AWQ W4A16 & BnB W4A4 \\ \hline Qwen2-1.5B & 3.56 & -0.04 / 1.52 & -0.22 / 0.74 & 1.28 / 4.72 & -0.98 / 3.02 & 0.92 / 4.56 \\ Qwen2-7B & 18.28 & -0.14 / 3.42 & -0.14 / 1.50 & -0.66 / 9.22 & -0.16 / 7.40 & -11.32 / 15.28 \\ Qwen2-72B & 28.20 & -0.06 / 2.90 & 0.20 / 1.48 & -1.42 / 7.46 & -0.14 / 5.58 & -2.40 / 9.16 \\ Llama3-8B & 13.48 & -0.34 / 3.46 & -0.10 / 1.50 & NA /  NA & -1.08 / 6.60 & -1.72 / 8.24 \\ Llama3-70B & 23.98 & NA / NA & -0.30 / 1.46 & -1.28 / 7.20 & -0.82 / 5.14 & -1.50 / 7.06 \\ \hline \hline \end{tabular}
\end{table}
Table 20: MATH (4-shot) change in % accuracy/ % flips

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline Model & 16-bit Baseline & BnB W8A8 & GPTQ W8A16 & GPTQ W4A16 & AWQ W4A16 & BnB W4A4 \\ \hline Qwen2-1.5B & 34.08 & -0.28 / 1.73 & -0.19 / 0.67 & -0.72 / 7.14 & -0.91 / 6.66 & -1.05 / 9.11 \\ Qwen2-7B & 38.93 & -0.09 / 2.20 & -0.04 / 0.62 & 0.33 / 5.13 & 0.53 / 5.23 & -0.04 / 8.39 \\ Qwen2-72B & 46.79 & -0.09 / 1.53 & 0.00 / 0.57 & -0.04 / 3.59 & 0.67 / 3.16 & -0.47 / 3.64 \\ Llama3-8B & 42.66 & -0.57 / 3.07 & -0.05 / 0.82 & NA / NA & -0.48 / 5.94 & -1.20 / 7.53 \\ Llama3-70B & 49.33 & NA / NA & -0.09 / 1.34 & -0.09 / 7.29 & -0.19 / 4.79 & -2.88 / 11.41 \\ \hline \hline \end{tabular}
\end{table}
Table 22: Scrolls-Quality (0-shot) change in % accuracy/ % flips

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline Model & 16-bit Baseline & BnB W8A8 & GPTQ W8A16 & GPTQ W4A16 & AWQ W4A16 & BnB W4A4 \\ \hline Qwen2-1.5B & 36.18 & 0.00 / 3.75 & -0.26 / 1.28 & -2.73 / 9.73 & -0.43 / 7.59 & 1.28 / 9.81 \\ Qwen2-7B & 49.74 & 0.60 / 2.99 & 0.00 / 0.85 & 1.62 / 7.59 & -0.09 / 7.25 & 0.26 / 8.96 \\ Qwen2-72B & 60.15 & 0.42 / 1.79 & -0.09 / 0.77 & -0.26 / 5.03 & -0.51 / 3.41 & -0.77 / 5.72 \\ Llama3-8B & 53.24 & -0.68 / 4.61 & 0.60 / 1.28 & NA / NA & -1.37 / 8.87 & -3.50 / 11.69 \\ Llama3-70B & 64.33 & NA / NA & -1.02 / 3.24 & -5.46 / 10.58 & -2.39 / 5.29 & -5.80 / 10.41 \\ \hline \hline \end{tabular}
\end{table}
Table 19: ARC-Challenge (0-shot) change in % accuracy/ % flips

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline Model & 16-bit Baseline & BnB W8A8 & GPTQ W8A16 & GPTQ W4A16 & AWQ W4A16 & BnB W4A4 \\ \hline Qwen2-1.5B & 36.18 & 0.00 / 3.75 & -0.26 / 1.28 & -2.73 / 9.73 & -0.43 / 7.59 & 1.28 / 9.81 \\ Qwen2-7B & 49.74 & 0.60 / 2.99 & 0.00 / 0.85 & 1.62 / 7.59 & -0.09 / 7.25 & 0.26 / 8.96 \\ Qwen2-72B & 60.15 & 0.42 / 1.79 & -0.09 / 0.77 & -0.26 / 5.03 & -0.51 / 3.41 & -0.77 / 5.72 \\ Llama3-8B & 53.24 & -0.68 / 4.61 & 0.60 / 1.28 & NA / NA & -1.37 / 8.87 & -3.50 / 11.69 \\ Llama3-70B & 64.33 & NA / NA & -1.02 / 3.24 & -5.46 / 10.58 & -2.39 / 5.29 & -5.80 / 10.41 \\ \hline \hline \end{tabular}
\end{table}
Table 18: ARC-Easy (0-shot) change in % accuracy/ % flipsWe report BFCL-_greedy_ (tweaked BFCL to generate results greedily) results instead of BFCL-_standard_ because by default, BFCL uses top_p sampling. We are interested in measuring perturbations introduced _solely_ by quantization in this experiment and thus it is crucial to ensure that the sampling step does not add any further noise. To emphasize this point, we report two runs of BFCL-_standard_ and show that there are significant flips between them (see below Table 24), making such evaluations irrelevant for measuring the effect of quantization.

\begin{table}
\begin{tabular}{l c c c c} \hline \hline Model & 16-bit & GPTQ W8A16 & GPTQ W4A16 & AWQ W4A16 \\ \hline \begin{tabular}{l} Gamma-2B-it \\ \end{tabular} & 15.41 & 0.59 / 1.06 & 2.71 / 6.82 & -0.17 / 4.77 \\ \begin{tabular}{l} Gamma-7B-it \\ \end{tabular} & 49.35 & -0.53 / 2.18 & -4.18 / 17.24 & 5.00 / 17.71 \\ 
\begin{tabular}{l} Llama-3-8B-Instruct \\ \end{tabular} & 66.47 & 0.64 / 3.71 & -6.41 / 23.47 & -2.23 / 17.18 \\ \hline \hline \end{tabular}
\end{table}
Table 23: BFCL-_greedy_ change in % accuracy / % flips

\begin{table}
\begin{tabular}{l c c c c} \hline \hline Model & 16-bit (run-1) & 16-bit (run-2) & GPTQ W8A16 & GPTQ W4A16 \\ \hline 
\begin{tabular}{l} Llama-3-8B-Instruct \\ \end{tabular} & 59.59 & -1.05 / 23.53 & 0.00 / 24.59 & -7.00 / 33.84 \\ \hline \hline \end{tabular}
\end{table}
Table 24: BFCL-_standard_ change in % accuracy / % flips

### Consistency of flips

The following table Table 25 shows the % of questions whose answers are changed by 0-6 quantization schemes (for Llama2-70b MMLU task 15K questions):

The main observations are 1) for most questions (84%), answers are unchanged by all the quantization schemes (and these, as expected, have high top margin). 2) Out of the remaining 16%, roughly half (8%) of the questions are commonly changed by two or more schemes indicating significant overlap between the schemes. Our conclusion is that flips are mostly consistent across schemes.

For a more fine-grained analysis, we also report pairwise fraction of changed examples that are common between two quantization schemes in Table 26. We use overlap coefficient \(Overlap(A,B)=\frac{|A\cap B|}{min(|A|,|B|)}\), to quantify this where A and B are the set of examples changed by the schemes. We again see good overlap, ranging between 0.4-0.5

[MISSING_PAGE_EMPTY:23]

\begin{table}
\begin{tabular}{l l c c c} \hline \hline Model & BnB 8bit & SQ 8bit & GPTQ 4bit & AWQ 4bit & BnB 4bit \\ \hline Llama2-7b chat & 4.7 / 7.9 & 16.9 / 24.5 & 9.3 / 15.3 & 8.5 / 14.9 & 12.8 / 19.9 \\ Llama2-13b chat & 3.2 / 7.7 & 9.3 / 17.2 & 6.1 / 14.4 & 6.0 / 14.6 & 8.7 / 16.9 \\ Llama2-70b chat & 3.0 / 7.2 & 3.1 / 8.2 & 3.7 / 9.5 & 3.8 / 9.5 & 5.0 / 13.2 \\ Yi-6b chat & 2.8 / 9.1 & 28.7 / 45.9 & 8.6 / 20.7 & 6.8 / 17.6 & 10.3 / 23.9 \\ Yi-34b chat & 1.6 / 7.8 & 36.4 / 55.2 & 5.8 / 18.4 & 3.4 / 12.7 & 6.0 / 19.4 \\ \hline \hline \end{tabular}
\end{table}
Table 28: MMLU 5-shot. The first/second number indicates the % of correct/incorrect answers of the baseline model that changed. We see that more % of incorrect answers change.

\begin{table}
\begin{tabular}{l l l l l} \hline \hline Std. & Textual response & Perplexity & \% correct & KL Div \\ dev. & & & token & \\ \hline  & (prompt:"tell me a one stanza poem") & & (on Wiki-2) & \\ \hline
0.0 & about the beauty of nature & 5.70 & 61.3 & 0.00 \\  & Here’s a one-stanza poem about the beauty of nature: & & & \\  & ”In the forest’s embrace, I find my peace, & & & \\  & Beneath the canopy of leaves, my soul does cease & & & \\  & To wander, lost in thought, and let the world pass by, & & & \\  & For in the beauty of nature, I am free to fly.” & & & \\ \hline
1.0 & about a tree & 5.70 & 58.2 & 0.20 \\  & Here’s a one-stanza poem about a tree: & & & \\  & ”A towering oak, its branches strong and wide, & & & \\  & Shades the forest floor, a haven for the pride.” & & & \\ \hline
3.0 & that you like. & 5.70 & 42.5 & 1.86 \\  & I like this one: & & & \\  & The stars are the old sailors, & & & \\  & their breaths are the winds, & & & \\  & their words are the waves, & & & \\  & their laughter is the tides. & & & \\  & It’s a beautiful way of describing the stars and the night sky, and & & & \\  & it has a nice ring to it. & & & \\ \hline
5.0 & about a flower that is not roses or daffodils. & 5.70 & 21.5 & 5.67 \\  & I've been writing a collection of haikus, and I’d like to use this & & & \\  & one as a way to break up the repetition of those popular flowers. & & & \\  & Thanks so much in advance! & & & \\  & I’m glad to help! Here’s a one-stanza poem about a flower that’s & & & \\  & not roses or daffodils: & & & \\  & Lily of the valley, & & & \\  & So delicate and fair, & & & \\  & Your sweet scent fills the air. & & & \\  & I hope this helps and adds some variety to your haiku collection! & & \\ \hline \hline \end{tabular}
\end{table}
Table 29: Adding Gaussian noise worsens output quality while keeping perplexity same. %correct tokens measure the % of tokens in the input that would have been selected greedily by the model (all results with Llama2-13b chat)Figure 8: Top margin in baseline vs quantized models: Scatter plot of the top-margin for the MMLU 5-shot benchmark in the baseline vs quantized models. We use the Llama2 13b model and BnB W4A4 quantization scheme for this plot. The top-margins show strong correlation (Pearson correlation = 0.84) and high top margins in baseline are likely to remain so in the quantized model.

Figure 7: Change in prediction probability vs top margin: For the MMLU 5-shot benchmark, we plot the baseline top-margin for the questions on the x-axis, and the change in prediction probability of the choices in the quantized vs the baseline model on the y-axis (computed as \(\frac{1}{4}\sum_{i\in A/B/C/D}|Probability_{base}i-Probability_{quantized}i|\)). We use the BnB W4A4 quantization scheme for these results. The observation holds across different quantization schemes.

Figure 9: SliceGPT, Accuracy and Flips vs Sparsity

Figure 10: AllFlips Results

Figure 11: The loglikelihood difference plots are somewhat symmetric around zero indicating that even though average loglikelihoods (and perplexity) of the baseline and quantized models are similar, actual per token loglikelihoods might be very different.The results are calculated on Llama2-13b, wiki-2Merity et al. (2016) dataset where perplexity is 4.57.

### Qualitative Analysis of Flips

We present some empirical observations below for the some of the tasks studied in this work.

* **MATH** This requires the model to just output the correct answer (and not the intermediate steps). We find it hard to clearly characterize the type of mistake made by the LLMs (especially because the range of subjects is very diverse). But, on many questions we find there to be simple mistakes (e.g., off by one errors, off by a factor of 2/10, etc). Additionally, in the MATH dataset, each question is accompanied with a tag that indicates difficulty (\(\in[1,2,3,4,5]\)) where a higher number indicates that the question is more difficult. This lets us validate our intuition that the questions that are answered by both the baseline and quantized models are the easiest (average difficulty 2.49), and the ones answered by exactly one of them are moderate (this is where correct incorrect transitions and vice versa happens, average difficulty 2.98) and the ones answered by neither are the hardest (average difficulty 3.56). The above numbers were obtained using Llama-3 8b and it's BnB W4A4 quantized version.
* **GSM8k** This task requires the LLM to generate the answer with step-by-step explanations. Here also, we find it hard to clearly characterize the type of mistakes made (though many of them can be classified as simple calculation errors) or pin down the specific type of question in which such phenomenon happens. In this task specifically, we observe that at times the model gets stuck in a loop, resulting in no output at all (rather than an incorrect one). For example:-

*Question*: Travis had 61 apps on his tablet. He deleted 9 apps he didn't use anymore and downloaded 18 more. How many apps are on his tablet now? *Llama-3-8b BnB W4A4 Answer* : Travis had 61 apps on his tablet. He deleted 9 apps he didn't use anymore and downloaded 18 more. Travis had 61 apps on his tablet. He deleted 9 apps he didn't use anymore and downloaded 18 more. Travis had 61 apps on his tablet. He deleted 9 apps he didn't use anymore and downloaded 18 more. Travis had 61 apps on his tablet. He deleted 9 apps he didn't use anymore and downloaded 18 more. (... and so on...) We measure % of such cases as %invalid. We observe that quantization makes %invalid worse in general. For example :- %invalid for Llama-3-8b, its BnB 8bit and BnB 4bit versions are 2.00%, 2.43% and 3.72% respectively.
* this involves cases where the all the parameters are not initialized with their correct values or supplying more/less than the required number of parameters, etc. A very common type of error in this category is supplying parameters as a dictionary. For eg. we observed that the same function is called in two ways
- sum(a=5,b=7) and sum('a':5, 'b':7), but only the first version is correct. 2. calling the wrong API
- this involves calling an altogether different function than the one actually needed to solve the problem. 3. misunderstanding the question
- this involves solving an orthogonal problem to the one required. 4. adding/omitting characters that make the API call non-parsable
- self explanatory In general, we observe that most of the errors are in the first group itself.
* **Scroll-Quality, ARC, etc**

[MISSING_PAGE_FAIL:29]

[MISSING_PAGE_FAIL:30]

[MISSING_PAGE_FAIL:31]

[MISSING_PAGE_FAIL:32]

Sure! Here's a short story beginning with that sentence: The old abandoned house at the end of the street held a secret that no one had ever discovered. It stood there, a crumbling relic of a bygone era, its once garden faced now wrestling and worn. The overgrown garden had long since reclaimed the front port, and the windows were barded up, their glass long since shattered. But despite its neglect, there was something aluring about the old house. Someat, thing that drew people in, like months to a flame. The neighborhood kids would often dare each other to knock on the door, to see if anyone were would answer. But no one ever did. And yet, despite the silence, there was a feeling that the house was waiting for someone. Waiting for someone they enough to move it sersers.

One rainy afternoon, a curious young girl named someone have enough to move it sersers.

The rainy afternoon, a curious young girl named Lily decided to take on the challenge. She walked up the crumbling steps, her heart pounding in her chest, and knocked on the door. The wood was soft and decaying, and the knock echoed through the empty hall inside. To her surprise, the door creaded open. Just a crack, but enough to let her squeeze through. Lily stepped Lily's lightened Lily sees dividing to the dim light. The air was thick with dust and the smell of decay, but she could sense something else. Someat thing magical She unordered through the empty rooms, her footsteps echoing off the walls. The furniture was old and worn, but there were hints of a fine one lived here. A faded photograph on the wall, a broken vase on the mantle.

And then, she found it. A hidden room, tracked away behind a bookshelf. It was small, with a tinge window that let in a faint glow of light.

The walls were covered in old letters, tied up with a faded ribbon.

Lily's heart acces she untitled the ribbon and began to read. The letters were from a woman the named Emily, who had lived in the house a oven a century ago. They were addressed to her lower, a manu named Jack, who had gone off of war. As Lily read nearby the letters, she discovered that Emily and Jack had been separated by the war, and that Emily had been left alone in the house, barely waiting for his return. But as the months passed, she had begun to lose hope. And then, one day, a miracle had happened. Jack had returned, and the two had been reunited. They had lived out the rest of their days in the house, surrounded by love and happiness.

Lily finished the letters, her eyes brimming with tears. She couldn't believe what she had just discovered. The old abandoned house at the end of the street held a secret that no one had ever discovered. A secret that had been hidden for over a century, waiting for someone brave enough to uncover it.

As she left the house, Lily felt a sense of wonder that she had never felt before. She knew that she would never forget the secrets of the old abandoned house at the end of the street. And she knew that she would always be grateful for the chance to uncover them.

The old abandoned house at the end of the street held a secret that no one had ever discovered. It stood there, a crumbling risk and overburding garden seemed to conceal a mystery that had been hidden for decades. The house had been left unlocked since the family who lived secretors to those who dared to listen.

For years, curious children had dared each other to knock on the door, but no one had ever trusted up the courage to do so. They believed that the house was hainted, that the spirtanding before the house, her heart pounding in her chest.

Lily decided to take matters into her own hands. She grabbed her flashlight and set out to uncover the scretts of the old abandoned house. She pushed open the creaky gate and made her way up the overgrown path to the front door. With you was soft and decaying, and the knock echoed through the empty hall inside. To her surprise, the door creaded open. Just a crack, but enough to let her squeeze through. Lily stepped Lily's heart acces also the stepped inside, the nasty air filling her lungs. She found herself in a large entrance hall with a chandeter that lung capacity but the ceiling. Dusty cobpshes clung to every surface, and the faded sunflower seemed to whisperc screens of the past.

As weathered eager in the house, Lily discharged aft one played here, a faded photograph on the wizard. In this study hearing down upon her. She took a step forward, her eyes adjusting to the dim light, and thus when she saw it.

A small room, hidden behind a bookshelf, glowed with a soft, ethread light. Lily felt a shiver run down her spine as she approached the house, and Lily's heart leap into her throat. She turned to see a ghostly figure standing in the doorway, watching her. It was the spirit of the family's patriach, his eyes filled with a deep sadness.

Ally froze, unnature of what to do. But as she looked into the ghost's eyes, she saw a glimmer of hope. The spirit began to speak, its voice and their Emily had been left alone in the house, barely above a whisperc.

"We been waiting for someone brave enough to uncover our secret," it said. "My family and I were scientists, and we discovered a way to harness the power of the elements. But our research was stolen, and we were forced to flee." We left behind our home, hoping that one day someone would find our work and continue our legacy."

Lily's eyes widened as she listened to the ghost's tale. She couldn't believe that she had stumbled upon something so incredible. The ghost handed her a small box containing a key have enough to uncover it.

"This key will unlock a safe in the hidden that she had never felt before. She knew that she would never forget the secrets of the old abandoned house at the end of the street. And she knew that she would always be grateful for the chance to uncover them.

"The old abandoned house at the end of the street with sentence: The old abandoned house at the end of the street held a secret that no one had ever discovered. Is repelling paint and overgrown garden seemed to conceal a mystery that had been hidden for decades. The house had left introduced since the family who lived secretors to those who dared to listen.

For years, curious children had dared each other to knock on the door, but no one had ever trusted up the courage to do so. They believed that the house was mounted to learn over the street, its crumbling facade a warning to stay away. But one rainy summer afternoon, a young girl named Lily found herself standing before the house, her heart pounding in her chest.

Lily held always been fascinated by the house, and the secrets it might hold. She had spent countless hours gazing at its broken windows, imagining what could be hiding inside. And now, with the rain pouring down around her, the left a sudden range up to uncover its mysteries.

Taking a deep breath, Lily pushed open the creaky gate and made her way up the overgrown path. Thehoe seemed to lean in closer, as if it were trying to listen to her very steep. She raised her hand to knock, but before she could, cloy the cracked open by itself.

Lily's heart skipped a beat as she stepped inside, the musaty air filling her lungs. The house was dark and silent, but she could feel the weight of observed a hidden room behind a bookshelf. In-history bearing down upon her. She took a step forward, her eyes adjusting to the dim light, and that was when she saw it.

A small room, hidden behind a bookshelf, glowed with a soft, ethread light. Lily felt a shiver run down her spine as she approached the house, and Lily's heart leap into red throat. She turned to see a ghostly figure standing in the doorway, watching her. It was the spirit of the family's patriach, his eyes filled with a deep sadness.

"Welcome, Lily"," it said. "T've been waiting for you."

Lily's heart cased as the stepped inside the room, of hope. The spirit began to speak, its voice movement. But there was no one in sight. The voice seemed to come from all around her, echoing off the walls and ceiling.

"Who are you?" Lily asked, her voice barely names the power of the elements. But our research was stolen, and we were forced to flee."

"And you, Lily, have come to uncover them."

As Lily introduced him our work and continue our speak, revealing secrets that had been hidden for centuries. Secrets of love and loss, of joy and sordow, of a town that had been shrouded in mystery for far too long. And as she listened, disbst handed her a small box containing a key and a note.

"This key will unlock a safe in the hidden room," it said. "Isaide, you'll find our research and the secrets of the elements. Use it to change the world, but be careful. There are those who will stop at nothing to keep it hidden."

With that, the ghost washanced, leaving Lily alone in the old abandoned house. She stood there for a moment, her mind feeling with the enormly of what she had discovered. And then, with a newfound sense of purpose, she set out to unlock the secrets of the elements and change the world forever.

"The old abandoned house at the end of the street with sentence: The old abandoned house at the end of the street with sentence: The old abandoned house at the end of the street held a secret that no some as anyone could remember. Is pealing paint and overgrown garden her become a familiar sight to the residents of the small town, a reminder of a time long had been hidden for decades. The house had been left untocards since the family who lived secretors to those who dared to listen.

For years, curious children had dared each other to knock on the door, but no one had ever trusted up the courage to do so. They believed that the springerton, a young girl named Lily found herself standing before the house, her heart pounding in her chest.

Lily held always been fascinated by the house, and the secrets it might hold. She had spent countless hours gazing at its broken windows, imagining what could be hiding inside. And now, with the rain pouring down around her, the left a sudden range up to uncover its mysteries.

Taking a deep breath, Lily pushed open the creaky gate and made her way up the overgrown path. Thehoe seemed to lean in closer, as if it were trying to listen to her very steep. She raised her hand to knock, but before she could, cloy the cracked open by itself.

Lily's heart skipped a beat as she stepped inside, the musaty air filling her lungs. The house was dark and silent, but she could feel the weight of observed a hidden room behind a bookshelf. In-history bearing down upon her. She took a step forward, her eyes adjusting to the dim light, and that was when she saw it.

A small room, hidden behind a bookshelf, glowed with a soft, ethread light. Lily felt a shiver run down her spine as she approached the house, and Lily's heart leap into red throat. She turned to see a ghostly figure standing in the doorway, watching her. It was the spirit of the family's patriach, his eyes filled with a deep sadness.

"Welcome, Lily," it said. "T've been waiting for you."

Lily's heart cased as the stepped inside the room, of hope. The spirit began to speak, its voice movement. But there was no one in sight. The voice seemed to come from all around her, echoing off the walls and ceiling.

"Who are you?" Lily asked, her voice barely names the power of the elements. But our research was stolen, and we were forced to flee."

"And you, Lily, have come to uncover them."

As Lily introduced him our work and continue our speak, revealing secrets that had been hidden for centuries. Secrets of love and loss, of joy and sordow, of a town that had been shrouded in mystery for far too long. And as she listened, disbst handed her a small box containing a key and a note.

"Who are you?" Lily asked, her voice barely have a whispercards."

"I am the keeper of secrets," the voice replied."

"And you, Lily, have come to uncover them."

As Lily introduced him awe, the voice began to speak, revealing secrets that had been hidden for centuries. Secrets of love and loss, of joy and sordow, of a town that had been shrouded in mystery for far too long. And as she listened, disbst handed her a small box containing her and a note.

"This key will unlock a safe in the hidden room," it said. "Isaide, you'll find our research and the secrets of the elements. Use it to change the world, but be careful. There are those who will stop at nothing to keep it hidden."

With that, the ghost washanced, leaving Lily alone in the old abandoned house. She stood there for a moment, her mind feeling with the enormly of what she had discovered. And then, with a newfound sense of purpose, she set out to unlock the secrets of the elements and change the world forever.

[MISSING_PAGE_FAIL:34]

**Turn 1**: Sure, here are the overall ratings for each phone based on the customer reviews you provided: "iPhone": 8.5, "Samsung Galaxy": 8.2, "Google Pixel": 8.0

The ratings are based on the following factors: "iPhone: The review highlights the phone's engineering, performance, and camera quality, but also mentions the lack of a 'low' factor and high price. Therefore, the rating is 8.5 out of 10. *Samsung Galaxy: The review phrases and the phone's screen, performance, battery life, and camera options, but also mentions that Bishy is lacking, AR emoj falls flat, and the design as 5.0 out of 10. *Google Pixel: The review highlights the phone's cutting-edge species, innovative. After a lookout that Bishy is lackluster, but However, it also mentions the lackluster battery life, lack of expandable storage, and occasional performance clusters. Therefore, the rating is 8.0 out of 10. *Google Pixel: The Verge's It's important to note that these ratings are based on a subjective analysis of the customer reviews provided and may not reflect the overall opinion of all customers.

**Turn 2**: Sure! Here are the overall ratings for each phone based on the customer reviews you provided, tag letters instead of numbers: "iPhone": "A", "Samsung Galaxy": "B", "Google Pixel": "B"

It's worth noting that these ratings are based on a subjective interpretation of the customer reviews you provided, and other people may have different opinions on these phones.

I hope this helps! Let me know if you have any further questions.

**Turn 2**: Sure! Here are the overall ratings for each phone based on the customer reviews you provided, using letters instead of numbers: *iPhone: A *Samsung Galaxy: B *Google Pixel: C

I hope this helps! Let me know if you have any other questions.

Here's the JSON string you requested, containing the unique variable names for each equation: 1. y, x, e, pi, squt(7) 2. A, B, C, D, N, In(N), a, "+" 1 (equation: "y = (3/4)x-3 - e-(2x) + spin(pi*x) - sqrt(7), "variables: "[x,"y] ),

 { "equation": "2A - B(3+C) * sum(N=1 to 5, 1:[nN(2)] = 5D"integer(a=0 to pi, cos(comb(N=1 to 10; N*a))) "variables": ["A", "B", "C", "D", "N", "a"] ],

 {"equation": "E = m(c2) + gamma("d)/(c2) + sqrt(2+ 4(alpha)2) + sqrt(beta2+ 2+ 4(alpha)2) + "variables": ["E", "m", "c", gamma", "v", "d", "alpha", "beta" ] }

I hope this helps! Let me know if you have any further questions.

Note that I've used the 'comb' function to combine the variables for the second equation, since it's a sum of multiple terms. Also, I've used the'sqrt' function to simplify the expression for the third equation.

Absolutely! Here's a vertical version of the travel blog post about Hawaii, starting every sentence with the letter A:

Abadi 1 recently returned from an incredible trip to Hawaii, and I can't wait to share my experiences with you. A warm sunshine, lushin greenery, and friendly locals all contributed to an unforgettable adventure.

A highlight of my trip was the opportunity to immerse myself in Hawaiian culture. A visit to the Bishop Museum in Household offered a nating glimpse into the history and traditions of the Native Hawaiian people. A chance to learn about the ancient Hawaiian gods and goddesseres, the art of hula dancing, and the significance of the Hawaiian language.

A must-see attraction was the iconic Wainza Canyon on Kauai, often referred to as the "Grand Canyon of the Pacific." A stunning view of red and green volcanic rock, waterfalls, and lush vegetation.

Another A-list attraction was the Haleakala National Park on Maui, where I watched the sunshine from the summit of Haleakala volcano. A park ranger provided informative and entertaining commentary about the geology and wildlife of the area, making the experience even more enjoyable.

A memorable experience of my trip was a visit to a traditional Hawaiian luau. A delicious feast of local specialists, such as kaluz gie, poi, and hauquin, while watching a captiving performance of traditional Hawaiian music and dance.

A warm hospitality of the hosts and a lively atmosphere made for an unforgettable evening.

A final thought: if you're planning a trip to Hawaii, I highly recommend incorporating some of these cultural experiences and music attractions into your itinerary. A variety of interests, from history to nature to relaxation, Hawaii has something for everyone. A mhalo (thank you) for reading, and I hope you have a fantastic time in paradise!

Absolutely! Here's a rewritten version of the previous response, starting every sentence with the letter A:

Abadi 1 My recent trip to Hawaii was a truly trip to Hawaii, and I can't wait to share my overlest in the local culture and explore the must-see attractions made it a journey to remember.

A traditional Hawaiian luau was the perfect place to start. A feast of local delacies like kaluz gie, poke, and hauquin was served with a logic, and hauquin. A music forcefuler followed, transporting me to a place of the Nature Hawaiian people. A chance to learn about the ancient Hawaiian gods and goddesseres, the art of hula dancing, and the significance of the Hawaiian language.

A must-see attraction was the iconic Wainza Canyon on Kauai, often referred to as the "Grand Canyon of the Pacific." A stunning view of red and green volcanic rock, waterfalls, and lush vegetation.

Another A-list attraction was the Haleakala National Park who as another highlight. A sunrise summit offered a spectacular view, and a day spent exploring the park's do for any adventure-seeker, with stops at waterfalls, headaches, and hiking trails that took shrimp I've ever tested. A Hawaiian tapple, was a flavorful depth. A fusion of radio-toul Golyensian, Asian, and American flavors, local cuinine was a culinary adventure.

A labin Spirit, a concept that embodies the essence of Hawaiian culture, was evident everywhere. A feeling of warmth, hospitality, and genuine lambades may every encounter special.

A truly unforgettable experience, my recent trip to Hawaii left me with memories to last a lifetime. A chance to immerse myself in the local culture and explore must-see attractions made it a journey to remember. Until next time, aloha!The central dogma of molecular biology is the concept that genetic information flows from DNA to RNA to proteins, and it is the fundamental principle that explains how genetic information is transmitted and expressed within cells. This idea was first proposed by Francis Crick in 1958.

The central dogma involves several processes: 1. Transcription: The process by which theplementary RNA copy of DNA, which is initiated when an enzyme called RNA polymerase binds to the DNA molecule.

2. Translation: The process by synthesizing proteins using the information encoded in the RNA molecule. This occurs on structures called ribosomes, somes, which read the sequence of nucleotides in the in the RNA and assemble the corresponding amino acids into a protein chain.

3. Post-transcriptional modification: After transcription, RNA molecules can undergo various modifications, such as splicing, capping, and polyadenylation, before they are ready to be translated into proteins.

4. Protein synthesis: The process of creating proteins from amino acids, which are linked together by peptide bonds. This process occurs on a ribosomal sequence of codons in the RNA and assemble the corresponding amino acids into a protein chain.

The central dogma is a fundamental concept in molecular biology that explains how genetic information is transmitted and expressed within cells. It has been widely accepted and used in scientific research since its introduction by Francis Crick.

The central dogma of molecular biology is the idea that genetic information flows from DNA to RNA to proteins, and that this flow of information is the fundamental mechanism by which cells process and transmit genetic information. The central dogma was first activated by Francis Crick, one of the discoveres of the structure of DNA, in the 1950a.

The central dogma is a conceptual framework for a gene's DNA is copied into a complementary RNA molecule. This RNA molecule is known as messenger RNA (mRNA).

Translation: The process by which the information in the mRNA is used to build a protein. This occurs on structures called ribosomes, which read the sequence of nucleotides in the mRNA and assemble the corresponding amino acids into a protein.

3. Post-transcriptional modification: After transcription, RNA molecules can undergo additional modifications, such as folding into its native conformation, becoming modified by the addition of carbohydrates or lipids, or being transported to other cellular compartments.

The central dogma is a fundamental concept in molecular biology that helps to explain how genetic information is transmitted and expressed within cells. It has been widely accepted and used in scientific research since its introduction by Francis Crick.

The central dogma of molecular biology is the idea that genetic information flows from DNA to RNA to proteins, and that this flow of information is the fundamental mechanism by which cells process and transmit genetic information. The central dogma was first activated by Francis Crick in 1958.

The central dogma involves several processes: 1. Transcription: The process by which the information in a gene's DNA is copied into a complementary RNA molecule. This RNA molecule is known as messenger RNA (mRNA).

Translation: The process by which the information in the mRNA is used to build a protein. This occurs on structures called ribosomes, which read the sequence of nucleotides in the mRNA and assemble the corresponding amino acids into a protein.

3. Post-transcriptional modification: After transcription, RNA molecules can undergo additional modifications, such as folding into its native conformation, becoming modified by the addition of carbohydrates or lipids, or being transported to other cellular compartments.

The central dogma is a fundamental concept in molecular biology that helps to explain how genetic information is transmitted and expressed within cells. It has been widely accepted and used in the scientific research since its introduction by Francis Crick.

The central dogma of molecular biology is the idea that genetic information flows from DNA to RNA to proteins, and that this flow of information is the fundamental mechanism by which cells process and transmit genetic information. The central dogma was first activated by Francis Crick, one of the discoveres of the structure of DNA, in the 1950a.

The central dogma is a conceptual framework for a gene's DNA is copied into a complementary RNA molecule. This RNA molecule is known as messenger RNA (mRNA).

Translation: The process by which the information in the mRNA is used to build a protein. This occurs on structures called ribosomes, which read the sequence of nucleotides in the mRNA and assemble the corresponding amino acids into a protein.

3. Post-transcriptional modification: After transcription, RNA molecules can undergo additional modifications, such as folding into its native conformation, becoming modified by the addition of carbohydrates or lipids, or being transported to other cellular compartments.

The central dogma is a fundamental concept in molecular biology that helps to explain how genetic information is transmitted and expressed within cells. It has been widely accepted and used in the scientific community since its introduction by Francis Crick in the 1950s.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: We have claimed accuracy to be flawed in evaluating LLM compression schemes and have shown results to back this claim, and have also proposed alternate metrics. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We have a limitations section. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [NA]Justification: Our work does not have any theoretical result. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We specify what open source tools and models were used and the configurations used for various quantization schemes. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?Answer: [NA] Justification: Our work uses existing open sourced code and does not need any private code for reproduction. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: All the configurations used for different quantization schemes and evaluations (like task, number of shot) are mentioned. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [No] Justification: Only MT-Bench includes generation with non-zero temperature that would result in difference from run-to-run in any meaningful way. For every other task, results don't change across runs (not accounting for floating point errors). MT-Bench evaluation uses GPT-4 and thus it is prohibitively expensive to repeat each experiment multiple times to report any error estimates. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).

* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [No] Justification: The actual type of compute resources used is irrelevant to the evaluations (not accounting for floating point errors) Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We have gone through the Code of Ethics and can confirm that there are no violations. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: Our work proposes alternate metrics of evaluating compressed LLMs and we feel that this has no societal impact whatsoever. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.

* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).

11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: Our work proposes alternate metrics of evaluating compressed LLMs and we feel that this poses no such risks. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We have cited all the assets used in this work. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.

* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: Our work introduces no new assets. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: Our work does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: Our work does not involve crowdsourcing nor research with Human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.