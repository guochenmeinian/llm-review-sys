# Functional-Group-Based Diffusion for Pocket-Specific Molecule Generation and Elaboration

Haitao Lin

Westlake University

linhaitao@westlake.edu.cn

&Yufei Huang

Westlake University

huangyufei@westlake.edu.cn

&Odin Zhang

Zhejiang University

haotianzhang@zju.edu.cn

&Lirong Wu

Westlake University

wilirong@westlake.edu.cn

&Siyuan Li

Westlake University

lisiyuan@westlake.edu.cn

&Zhiyuan Chen

Deep Potential

chenzhiyuan@dp.tech

&Stan Z. Li

Westlake University

stan.zq.li@westlake.edu.cn

Corresponding Author.

###### Abstract

In recent years, AI-assisted drug design methods have been proposed to generate molecules given the pockets' structures of target proteins. Most of them are _atom-level-based_ methods, which consider atoms as basic components and generate atom positions and types. In this way, however, it is hard to generate realistic fragments with complicated structures. To solve this, we propose D3FG, a _functional-group-based_ diffusion model for pocket-specific molecule generation and elaboration. D3FG decomposes molecules into two categories of components: functional groups defined as rigid bodies and linkers as mass points. And the two kinds of components can together form complicated fragments that enhance ligand-protein interactions. To be specific, in the diffusion process, D3FG diffuses the data distribution of the positions, orientations, and types of the components into a prior distribution; In the generative process, the noise is gradually removed from the three variables by denoisers parameterized with designed equivariant graph neural networks. In the experiments, our method can generate molecules with more realistic 3D structures, competitive affinities toward the protein targets, and better drug properties. Besides, D3FG as a solution to a new task of molecule elaboration, could generate molecules with high affinities based on existing ligands and the hotspots of target proteins.

## 1 Introduction

The established notion in drug-target identification is that similar structures perform similar functions. This principle allows classical computer-aided drug design (CADD) to abstract protein-ligand interactions as pharmacophores, aligning similar functional groups and extracting pharmaceuticalinformation from these structures. Fitting suitable functional groups into the pharmacophores can enhance ligand-protein interactions, thus improving drug efficiency [1; 2].

Artificial intelligence has already achieved outstanding success in protein design [3; 4; 5; 6; 7; 8] and thus led to a new round of attention in drug design focused on AI-assisted drug design (AIDD). A number of methods of AI-assisted drug design on pocket-specific molecule generation are emerging [9; 10; 11; 12; 13; 14; 15], thanks to developments in deep generative models [16; 17; 18; 19; 20; 21] and graph neural networks (GNNs) [22; 23; 24; 25; 26; 27; 28; 29; 30; 31]. These methods usually focus on atom-level generation, which first generates atom types and positions and then assembles atoms into molecules that can bind to the protein pockets. Although significant progress has been made, they are still weak in two aspects. For one thing, it is hard for them to generate realistic functional groups that contribute pharmacological effects to the target as classical CADD methods are able to. It is shown that generating benzene rings is uncommon compared with the reference molecules, not to mention some large functional groups with complex structure constraints such as purine, indole, etc (Table. 1 in Sec. 5.2). For another, trade-offs between efficiency and sufficiency of protein context place them in a dilemma. For example, TargetDiff [13] employs sufficient protein context, which disassembles the amino acids into atoms, but leads to inefficiency due to a large node number (412.14 on average) in GNN's message passing. DiffSBDD [14] simplifies the representation of protein context, by only using \(\mathrm{C}_{\alpha}\)'s positions and residue types, resulting in a reduction of GNNs' node number (68.10 on average) but the insufficiency of context information.

To address the above issues, we establish a functional-group-based **diffusion** model (D3FG), including the following contributions: **1. Method Novelty.** We denote the molecules' functional groups and proteins' amino acids as the same level's fragments, in which the intra-relative positions of atoms are fixed like rigid bodies, and represent single atoms as linkers. The positions and orientation of local structures and the atom type variables are generated gradually through denoising processes. The fragment-linker designation leads the binding systems to heterogeneous graphs, and thereby, two schemes are proposed as solutions, which achieve competitive performance in terms of molecule structures, binding affinity, and drug properties, and sufficiency in encoding protein context by employing more features. **2. Dataset Establishment.** Though the CrossDocked2020 [32] has been a widely-used dataset for evaluation methods' performance on the task, the analysis of the molecules' functional groups of it is missing. We deeply explore the details of the inter-relative positions and types of functional groups of the molecules and establish an extendable database of common functional groups. **3. New task.** Besides molecule generation, we propose molecule elaboration as another task that our model can fulfill. Fragment hotspot maps (FHM) [33; 34] are used to preprocess paired protein-molecule in CrossDocked2020 for the task. As a result, our model generates molecules with high binding affinity based on the reference.

## 2 Problem Statement

For a binding system composed of a protein-molecule pair (also called protein-ligand pair) as \(\mathcal{C}\), which contains \(N_{\mathrm{aa}}\) amino acids of proteins and \(N_{\mathrm{fg}}\) functional groups and \(N_{\mathrm{at}}\) single atoms of molecules, we represent the index set of the molecule's single atoms as \(\mathcal{I}_{\mathrm{at}}\), functional groups as \(\mathcal{I}_{\mathrm{fg}}\), and the protein's amino acids as \(\mathcal{I}_{\mathrm{aa}}\), where \(|\mathcal{I}_{\mathrm{fg}}|=N_{\mathrm{fg}}\), \(|\mathcal{I}_{\mathrm{at}}|=N_{\mathrm{at}}\) and \(|\mathcal{I}_{\mathrm{aa}}|=N_{\mathrm{aa}}\). Note that a molecule can be disassembled into functional groups and single atoms other than functional groups, which we also call linkers. For a protein, the amino acids can be represented by its type, \(\mathrm{C}_{\alpha}\) atom coordinate, and the orientation, denoted as \(s_{i}\in\{1,\ldots,20\}\), \(\bm{x}_{i}\in\mathbb{R}^{3}\), \(\bm{O}_{i}\in\mathrm{SO}(3)\), where \(i\in\mathcal{I}_{\mathrm{aa}}\). For a molecule, assuming there are \(M_{\mathrm{fg}}\) and \(M_{\mathrm{at}}\) possible types in total functional groups and linker atoms respectively, the functional groups can be represented as the three elements if the inter-relative positions are fixed, as \(s_{j}\), \(\bm{x}_{j}\) and \(\bm{O}_{j}\), where \(s_{j}\in\{21,\ldots,21+M_{\mathrm{fg}}\}\) is the type, \(\bm{x}_{j}\in\mathbb{R}^{3}\) is the predefined center atom's coordinate, and \(\bm{O}_{j}\in\mathrm{SO}(3)\) can also be obtained in the same way as amino acids (See Sec. 5.1 and Appendix. D.) for \(j\in\mathcal{I}_{\mathrm{fg}}\); And its linkers can be represented as \(s_{k}\), \(\bm{x}_{k}\) and \(\bm{O}_{k}\), with \(s_{k}\in\{22+M_{\mathrm{fg}},\ldots,22+M_{\mathrm{fg}}+M_{\mathrm{at}}\}\), \(k\in\mathcal{I}_{\mathrm{at}}\) and \(\bm{O}_{k}=\mathrm{diag}\{1,1,1\}=\bm{I}\).

Therefore, \(\mathcal{C}=\{(s_{i},\bm{x}_{i},\bm{O}_{i})\}_{i=1}^{N_{\mathrm{aa}}+N_{ \mathrm{fg}}+N_{\mathrm{at}}}\) can be split into two sets as \(\mathcal{C}=\mathcal{P}\cup\mathcal{M}\), where \(\mathcal{P}=\{(s_{i},\bm{x}_{i},\bm{O}_{i}):i\in\mathcal{I}_{\mathrm{aa}}\}\) and \(\mathcal{M}=\{(s_{j},\bm{x}_{j},\bm{O}_{j}):j\in\mathcal{I}_{\mathrm{fg}}\cup \mathcal{I}_{\mathrm{at}}\}\). For protein-specific molecule generation, our goal is to establish a probabilistic model to learn the distribution of molecules conditioned on the target proteins, _i.e._\(p(\mathcal{M}|\mathcal{P})\). In the following, we omit \(i\in\mathcal{I}_{\mathrm{aa}}\) and \(j\in\mathcal{I}_{\mathrm{fg}}\cup\mathcal{I}_{\mathrm{at}}\) by default unless specified.

Related Work

3D molecule generation.Previous methods on molecule generation fucus on 1D-smiles-string [35; 36; 37; 38] or 2D-molecule-graph [39; 40; 41; 42; 43]. In recent years, more works concentrate on 3D molecule generation, thanks to fast development in equivariant graph neural networks [44; 45; 46] and generative models [16; 17; 18; 19; 20; 21]. Molecular conformation generation aims to generate 3D structures of molecules with stability, given 2D molecule graph structure, [47; 48; 49; 50; 51]. Further, De novo molecular generation attempts to generate both 2D chemical formulas and 3D structures from scratch [52; 53; 54].

Fragment-based drug design.Previously, works on fragment-based molecule generation are proposed. For example, JT-VAE [55] generates a tree-structured scaffold over chemical substructures and combines them into a 2D-molecule. PS-VAE [56] can automatically discover frequent principal subgraphs from the dataset, and assemble generated subgraphs as the final output molecule in 2D. Further, DeepFrag [57] predicts fragments conditioned on parents and the pockets, SQUID [58] generates molecules in a fragment level conditioned on molecule's shapes. FLAG[59] auto-regressively generates fragments as motifs based on the protein structures in 3D.

Structure-based drug design.Success in 3D molecule generation and an increasing amount of available structural data of protein and molecules raises scientific interests in structure-based drug design (SBDD), which aims to generate both 2D molecule graphs and 3D structures conditioned on target protein structure as contextual constraints. For example, LiGAN [60] and 3DSBDD [12] are grid-based models which predict whether the grid points are occupied by specific atoms. By harnessing 3D equivariant graph neural networks, Pocket2Mol [9] and GraphBP [10] generate atoms auto-regressively and model the probability of the next atom's type as discrete categorical attribute and position as continuous geometry. FLAG[59] is also fragment-based, but still generates motifs in an auto-regressive way. Recently, utilizing the diffusion denoising probability models [61; 62; 63; 64], a series of SBDD methods generate ligands conditioned on the target pockets at full atom levels [13; 14; 15].

## 4 Method

D3FG firstly decomposes molecules into two categories of components: functional groups and linkers, and them use the diffsion generative model to learn the type and geometry distributions of the components. In this section, we describe the D3FG by four parts: (i) the diffusion model as the generative framework, in which the three variables are generated; (ii) the denoiser parameterized by graph neural networks, satisfying certain symmetries so that the generative model is SE-(3) invariant; (iii) the sampling process in which the molecules are generated by the trained models; (iv) the further problems resulted from heterogenous graph with two solutions.

### Diffusion Models

Diffusion models construct two Markov processes to learn the data distributions. The first called the forward diffusion process adds noises gradually until the noisy data's distribution reaches the prior distribution; The other called the generative denoising process, gradually removes the noise from the data sampled from the prior distribution until they are recovered to the desired data distribution. Assume there are \(T\) steps in both processes, and we denote \(\mathcal{M}^{t}=\{(s^{t}_{j},\bm{x}^{t}_{j},\bm{O}^{t}_{j})\}\) as the \(t\)-th noisy state in the forward process, with \(\mathcal{M}^{T}\sim\mathrm{Prior}(\mathcal{M}^{T})\), and \(\mathcal{M}^{0}=\mathcal{M}\), where the transition distribution is denoted as \(q(\cdot|\cdot)\); In the generative process, sample goes from \(T\) to 0, in which the transition distribution is denoted as \(p(\cdot|\cdot)\). Here we define the forward and generative processes of \(s^{t}_{j}\), \(\bm{x}^{t}_{j}\), and \(\bm{O}^{t}_{j}\).

Absorbing diffusion for functional group and linker types.Let \(\bm{s}^{t}_{j}\) as the one-hot encoding of the type of a single functional group or linker. The forward process followed by D3PM [65] randomly transits \(s^{t}_{j}\) into the absorbing state \(K\) (\(K=23+M_{\mathrm{fg}}+M_{\mathrm{at}}\)) with

\[\begin{split} q(s^{t}_{j}|s^{t-1}_{j})&=\mathrm{ Multinomial}\left(\bm{s}^{t-1}_{j}\bm{Q}^{t}\right)\\ q(s^{t}_{j}|s^{0}_{j})&=\mathrm{Multinomial}\left( \bm{s}^{0}_{j}\bm{\bar{Q}}^{t}\right)\end{split}\] (1)where \(\bar{\bm{Q}}^{t}=\bm{Q}^{1}\bm{Q}^{2}\dots\bm{Q}^{t}\) and \([\bm{Q}^{t}]_{mn}=q(s^{t}_{j}=n|s^{t-1}_{j}=m)\) denotes diffusion transition probabilities, with

\[\left[\bm{Q}^{t}\right]_{mn}=\begin{cases}1&\text{if}\quad m=n=K\\ 1-\beta^{t}_{\rm type}&\text{if}\quad m=n\neq K\\ \beta^{t}_{\rm type}&\text{if}\quad m\neq K,n=K\end{cases}.\] (2)

\(\beta^{t}_{\rm type}\) monotonically increases from 0 to 1, means that when \(t=T\), all the type variables are absorbed into the \(K\)-th category. In the generative process, it first samples \(N_{\rm at}\) linkers and \(N_{\rm fg}\) functional groups whose types are all in the absorbing states, selects \((1-\beta^{t}_{\rm type})\times 100\%\) of them respectively, and transforms their types from the absorbing states to the predicted ones by

\[p(s^{t-1}_{j}|\mathcal{M}^{t},\mathcal{P})=\mathrm{Multinomial}\left(F( \mathcal{M}^{t},\mathcal{P})[j]\right).\] (3)

where \(F\) is the type denoiser parameterized with a neural network, and \(F(\cdot,\cdot)[j]\) predicts the probability of the types for the \(j\)-th selected functional groups or linkers. With the effectiveness of BERT-style training [66], the denoiser directly predicts \(p(s^{0}_{j}|\mathcal{M}^{t},\mathcal{P})\), leading to a training objective as

\[L^{t}_{\rm type}=\mathbb{E}_{\mathcal{M}^{t}}\left[\sum_{j}\log p(s^{0}_{j}| \mathcal{M}^{t},\mathcal{P})\right].\] (4)

Gaussian diffusion for center atom positions.By defining the center atom in a functional group as shown in Appendix. D, and itself as the center in a linker, the Cartesian coordinate of center nodes \(\bm{x}_{j}\) represents its position. The forward transition distributions followed by DDPM [61] read

\[\begin{split} q(\bm{x}^{t}_{j}|\bm{x}^{t-1}_{j})& =\mathcal{N}\left(\bm{x}^{t}_{j}|\sqrt{1-\beta^{t}_{\rm pos}}\cdot \bm{x}^{t-1}_{j},\beta^{t}_{\rm pos}\bm{I}\right);\\ q(\bm{x}^{t}_{j}|\bm{x}^{0}_{j})&=\mathcal{N}\left( \bm{x}^{t}_{j}|\sqrt{\bar{\alpha}^{t}_{\rm pos}}\cdot\bm{x}^{0}_{j},(1-\bar{ \alpha}^{0}_{\rm pos})\bm{I}\right),\end{split}\] (5)

in which \(\beta^{t}_{\rm pos}\) increases from \(0\) to \(1\), means that the noise levels are increasing and the data's coordinate signals fade out during the forward diffusion, with \(\alpha^{t}_{\rm pos}=1-\beta^{t}_{\rm pos},\bar{\alpha}^{t}_{\rm pos}=\alpha^{ 0}_{\rm pos}\alpha^{1}_{\rm pos}\dots\alpha^{t}_{\rm pos}\), and finally \(\bm{x}^{T}_{j}\sim\mathcal{N}(\bm{0},\bm{I})\). Note that Eq. (5) is equivalent to the Markov process of \(\bm{x}^{t}_{j}=\sqrt{\bar{\alpha}^{t}_{\rm pos}}\cdot\bm{x}^{0}_{j}+\sqrt{1- \bar{\alpha}^{0}_{\rm pos}}\cdot\bm{\epsilon}_{j}\), where \(\bm{\epsilon}_{j}\sim\mathcal{N}(\bm{0},\bm{I})\). Rather than predicting the mean value of the reverse transition distribution in the generative process, the position denoiser \(G\) approximates the added noise \(\bm{\epsilon}_{j}\) with the reparameterization trick as

\[\begin{split} p(\bm{x}^{t-1}_{j}|\mathcal{M}^{t},\mathcal{P})& =\mathcal{N}\left(\bm{x}^{t-1}_{j}|\bm{\mu}_{\rm pos}(\mathcal{M}^ {t},\mathcal{P}),\beta^{t}_{\rm pos}\bm{I}\right);\\ \bm{\mu}_{\rm pos}(\mathcal{M}^{t},\mathcal{P})&= \frac{1}{\sqrt{\alpha^{t}_{\rm pos}}}\left(\bm{x}^{t}_{j}-\frac{\beta^{t}_{\rm pos }}{\sqrt{1-\bar{\alpha}^{t}_{\rm pos}}}G(\mathcal{M}^{t},\mathcal{P})[j] \right).\end{split}\] (6)

Figure 1: An illustration of the workflows of D3FG of the two schemes.

The training objective is thus established in a score-based way, as

\[L^{t}_{\mathrm{pos}}=\mathbb{E}_{\mathcal{M}^{t}}\left[\sum_{j}\lVert G(\mathcal{M} ^{t},\mathcal{P})[j]-\bm{\epsilon}_{j}\rVert_{2}^{2}\right].\] (7)

SO(3) diffusion for functional group orientations. By regarding the functional groups as rigid bodies, orientations together with the center atoms' positions determine all atoms' positions. Here we represent the orientation geometry as elements in \(\mathrm{SO}(3)\). Following [67], we use isotropic Gaussian distribution on SO(3) [68] to formulate the process, i.e. \(\mathcal{IG}_{\mathrm{so}(3)}(\cdot|\bm{\mu}_{\mathrm{ori}},\sigma_{\mathrm{ ori}})\), in which \(\bm{\mu}_{\mathrm{ori}}\) and \(\sigma_{\mathrm{ori}}\) are viewed as mean orientation and variance, in analogy with Gaussian distribution. The transition distribution for orientation matrices \(\bm{O}_{j}\) reads

\[q(\bm{O}_{j}^{t}|\bm{O}_{j}^{0})=\mathcal{IG}_{\mathrm{so}(3)}\left(\bm{O}_{j}^ {t}|\lambda_{\mathrm{ori}}(\bar{\alpha}_{\mathrm{ori}}^{t},\bm{O}_{j}^{0}),(1- \bar{\alpha}_{\mathrm{ori}}^{t})\right),\] (8)

\(\lambda_{\mathrm{ori}}(\bar{\alpha}_{\mathrm{ori}}^{t},\bm{O}_{j}^{0})\) is the geodesic flow from \(\bm{I}\) to \(\bm{O}_{j}^{t}\) by the amount \(\bar{\alpha}_{\mathrm{ori}}^{t}\), as \(\lambda_{\mathrm{ori}}(\bar{\alpha}_{\mathrm{ori}}^{t},\bm{O}_{j}^{0})=\exp \left(\bar{\alpha}_{\mathrm{ori}}^{t}\log(\bm{O}_{j}^{0})\right)\), where \(\exp(\cdot)\) and \(\log(\cdot)\) are exponential and logarithm map on the SO(3) manifold. As \(\alpha_{\mathrm{ori}}^{t}\to 0\), \(\lambda_{\mathrm{ori}}(\bar{\alpha}_{\mathrm{ori}}^{t},\bm{O}_{j}^{0})\to\bm{I}\). \(\{\beta_{\mathrm{ori}}^{t}\}_{t=0}^{T}\) is the predefined noise level schedule ranging from \(0\) to \(1\) as \(t\) increases, \(\alpha_{\mathrm{ori}}^{t}=1-\beta_{\mathrm{ori}}^{t}\) and \(\bar{\alpha}_{\mathrm{ori}}^{t}=\alpha_{\mathrm{ori}}^{0}\alpha_{\mathrm{ori} }^{1}\ldots\alpha_{\mathrm{ori}}^{t}\).

In the generative process, an orientation denoiser \(H\) is used to predict the mean orientation in the isotropic Gaussian distribution, which reads

\[p(\bm{O}_{j}^{t-1}|\mathcal{M}^{t},\mathcal{P})=\mathcal{IG}_{\mathrm{so}(3)} \left(\bm{O}_{j}^{t-1}|H(\mathcal{M}^{t},\mathcal{P})[j],\beta_{\mathrm{ori}} ^{t}\right).\] (9)

We use the same loss function as in [69] to minimize the expected discrepancy measured by the inner product between the data orientation matrices and the predicted ones, which reads

\[L^{t}_{\mathrm{ori}}=\mathbb{E}_{\mathcal{M}^{t}}\left[\sum_{j}\lVert(H( \mathcal{M}^{t},\mathcal{P})[j])^{\intercal}\bm{O}_{j}^{0}-\bm{I}\rVert_{F}^{ 2}\right].\] (10)

### Parametrization of Denoisers with Neural Networks

Amino acid context encoding.In order to decrease the computational complexity, we denote the protein context at amino-acid levels. Besides the amino acid types, \(\mathrm{C}_{\alpha}\) atom coordinate and the orientation, each atom's coordinates in the local system and three torsion angles including angles around '\(\mathrm{N}\)-\(\mathrm{C}_{\alpha}\)' bond, '\(\mathrm{C}_{\alpha}\)-\(\mathrm{C}\)' bond and '\(\mathrm{C}\)-\(\mathrm{N}\)' bond, are also used as intra-amino-acid features, which are concatenated and embedded by an MLP to create the intra-amino-acid embedding vector \(\bm{e}_{i}\). For inter-amino-acid features, the pair of amino acid types, sequential relationships (if the two amino acids are adjacent in the protein sequence), pairwise distances between \(\mathrm{C}_{\alpha}\) and inter-residue dihedrals are all embedded as inter-amino-acid embedding vector \(\bm{z}_{i,j}\) with \(i,j\in\mathcal{I}_{\mathrm{ana}}\). Note that these embedding vectors are all translational and rotational invariant (Appendix. C).

Denoisers with equivariance.In the setting of generative models, the learned distribution \(p(\mathcal{M}|\mathcal{P})\) should be equivariant to translation and rotation, such that \(p(\mathbf{T}_{g}(\mathcal{M})|\mathbf{T}_{g}(\mathcal{P}))=p(\mathcal{M}| \mathcal{P})\) for any \(g\in\mathrm{SE}(3)\), where \(\mathbf{T}_{g}\) is the corresponding roto-translational transformations, and \(\mathbf{T}_{g}(\mathcal{M})\) means each atom in the molecule is rotated and translated by \(\mathbf{T}_{g}\). In the setting of diffusion models, the following proposition indicates the translational and rotational equivariance of each denoisers.

Proposition 1.Let \(p(\bm{x}^{T})\), \(p(\bm{O}^{T})\), and \(p(s^{T})\) be SE(3)-invariant distribution, and the transition distributions \(p(\bm{x}^{t-1}|\mathcal{M}^{t},\mathcal{P})\) be SE(3)-equivariant, \(p(\bm{O}^{t-1}|\mathcal{M}^{t},\mathcal{P})\) be T(3)-invariant and SO(3)-equivariant, and \(p(s^{t-1}|\mathcal{M}^{t},\mathcal{P})\) be SE(3)-invariant, then the density \(p(\mathcal{M}|\mathcal{P})\) modeled by the reverse Markov Chains in the generative process of diffusion models is SE(3)-equivariant.

According to the **Proposition 1**, while the denoisers for functional group and linker types are rototranslational invariant, the denoiser for positions should be roto-translational equivariant, and it for orientations should be translational invariant and rotational equivariant. Therefore, we employ our denoiser's network structures with IPA [3] by harnessing the expressivity of Transformer[70] and roto-translational equivariance of LoCS [71]. Denote the binding system as a graph, in which the nodes are composed of amino acids, functional groups, and linkers, and the edges are established with K-nearest neighbor. Let \(\{\bm{h}_{i}:i\in\mathcal{I}_{\mathrm{aa}}\cup\mathcal{I}_{\mathrm{fg}}\cup \mathcal{I}_{\mathrm{at}}\}\) be the node embedding which is SE(3)-invariant, \(\{\bm{e}_{i}:i\in\mathcal{I}_{\mathrm{aa}}\cup\mathcal{I}_{\mathrm{fg}}\cup \mathcal{I}_{\mathrm{at}}\}\) and \(\{\bm{z}_{i,j}:i,j\in\mathcal{I}_{\mathrm{aa}}\cup\mathcal{I}_{\mathrm{fg}} \cup\mathcal{I}_{\mathrm{at}}\}\) be the previously defined intra- and inter-amino acid embedding, with \(\bm{e}_{j}=\bm{0}\) and \(\bm{z}_{i,j}=\bm{0}\) if \(i\notin\mathcal{I}_{\mathrm{aa}}\) or \(j\notin\mathcal{I}_{\mathrm{aa}}\). The attention mechanism in IPA updates the embedding of node \(i\) as

\[\bm{h}_{i}^{\prime}=\sum_{j\in\mathcal{N}(i)}\frac{\exp\big{(}(\bm{W}_{q}\bm{e }_{j}^{\prime})^{\intercal}(\bm{W}_{k}\bm{e}_{i}^{\prime})+\bm{z}_{i,j}\big{)} (\bm{W}_{v}\bm{e}_{j}^{\prime})}{\sum_{j\in\mathcal{N}(i)}\exp\big{(}(\bm{W}_{ q}\bm{e}_{j}^{\prime})^{\intercal}(\bm{W}_{k}\bm{e}_{i}^{\prime})+\bm{z}_{i,j} \big{)}}\] (11)

where \(\bm{e}_{i}^{\prime}=\bm{h}_{i}+\bm{e}_{i}\), \(\bm{h}_{i}^{\prime}\) is the updated node embedding, \(\bm{W}_{q}\), \(\bm{W}_{k}\), \(\bm{W}_{v}\) are learnable parameters, and \(\mathcal{N}(i)\) is neighborhood of node \(i\) obtained by the edges. Because \(\bm{h}_{i}\), \(\bm{e}_{i}\), and \(\bm{z}_{i,j}\) are all SE(3)-invariant, \(\bm{h}_{i}^{\prime}\) is also invariant. Three heads parameterized with MLP are stacked after several layers of Transformers update the node embedding, denoted by \(\mathrm{MLP}_{F}(\cdot),\mathrm{MLP}_{G}(\cdot)\), and \(\mathrm{MLP}_{H}(\cdot)\) is used for updating \(s^{t-1}\), \(\bm{x}^{t-1}\) and \(\bm{O}^{t-1}\). The LoCS updates the parameters in Eq. (3), (6) and (9) by

\[\begin{split} F\left(\mathcal{M}^{t},\mathcal{P}\right)[j]& =\mathrm{MLP}_{F}(\bm{h}_{j}^{\prime})\\ G\left(\mathcal{M}^{t},\mathcal{P}\right)[j]&= \mathrm{MLP}_{G}(\bm{h}_{j}^{\prime})\bm{O}_{j}^{t}\\ H\left(\mathcal{M}^{t},\mathcal{P}\right)[j]&= \exp\big{(}\mathrm{MLP}_{H}(\bm{h}_{j}^{\prime})\big{)}\bm{O}_{j}^{t}\end{split}\] (12)

The output of \(G\) means predicting the coordinate deviations in the local coordinate systems and then projecting it into the global one; In \(H\), \(\mathrm{MLP}_{H}\) first predicts a vector in Lie group \(\mathfrak{so}(3)\) and the exponential map on SO(3) converts it into a rotation matrix. The updating process ensures the equivariance and the invariance of the transition distributions in **Proposition 1**, which is proved in [71] and [69]. However, since \(\bm{O}_{i}^{t}=\bm{I}\) for any \(i\in\mathcal{I}_{\mathrm{at}}\), the equivariance of \(G\) on linkers will not be preserved, so we instead use EGNN [44] to get the output of \(G\) (Appendix. C).

### Sampling Process

In sampling, we first use two prior distributions as the empirical distributions \(\phi_{\mathrm{at}}\) and \(\phi_{\mathrm{fg}}\) drawn from the training set to sample the linker \(N_{\mathrm{at}}\) and functional group number \(N_{\mathrm{fg}}\). As \(p(\bm{x}^{T})\), \(p(\bm{O}^{T})\) and \(p(s^{T})\) should be SE(3)-invariant, \(p(s^{T})=\mathds{1}_{(K)}(\bm{s}^{T})\), \(p(\bm{x}^{T})=\mathcal{N}(\bm{x}^{T}|\bm{0},\bm{I})\) and \(p(\bm{O}^{T})=\mathrm{Uniform}_{\mathrm{SO}(3)}(\bm{O}^{T})\) satisfy the conditions, where \(\mathds{1}_{(}\cdot)\) is the indicator function and \(\mathrm{Uniform}_{\mathrm{SO}(3)}(\cdot)\) is the uniform distribution on \(\mathrm{SO}(3)\). And the iteratively generative process of \(p(s^{t-1}|\mathcal{M}^{t},\mathcal{P})\), \(p(\bm{x}^{t-1}|\mathcal{M}^{t},\mathcal{P})\) and \(p(\bm{O}^{t-1}|\mathcal{M}^{t},\mathcal{P})\) are given in Eq. (3), (6) and (9). The detailed sampling algorithm is given in Algorithm. 1.

``` Input: Zero-centered protein \(\{\bm{s}_{i},\bm{x}_{i},\bm{O}_{i}\}_{i\in\mathcal{I}_{\mathrm{aa}}}\), and graph denoiser \(F,G,H\), and node number sampler \(\phi_{\mathrm{fg}}\), \(\phi_{\mathrm{at}}\).  Sample \(N_{\mathrm{fg}}\sim\phi_{\mathrm{fg}},N_{\mathrm{at}}\sim\phi_{\mathrm{at}}\), leading to the index set \(\mathcal{I}_{\mathrm{fg}}\) and \(\mathcal{I}_{\mathrm{at}}\).  Sample initial states of functional groups, \(\{s_{j}^{T},\bm{x}_{j}^{T},\bm{O}_{j}^{T}\}_{j\in\mathcal{I}_{\mathrm{fg}}\cup \mathcal{I}_{\mathrm{at}}}\), where \(s_{j}^{t}=K\), \(\bm{x}_{j}^{T}\sim\mathcal{N}(\bm{0},\bm{I})\), \(\bm{O}_{j}^{T}\sim\mathrm{Uniform}_{\mathrm{SO}(3)}(\bm{O}^{T})\) for \(j\in\mathcal{I}_{\mathrm{fg}}\) else \(\bm{O}_{j}^{T}=\bm{I}\). for\(t\) in \(T-1,T-2,\ldots,1\)do  Sample \(\{s_{j}^{t-1},\bm{x}_{j}^{t-1},\bm{O}_{j}^{t-1}\}_{j\in\mathcal{I}_{\mathrm{fg}} \cup\mathcal{I}_{\mathrm{at}}}\) as Eq. (3), (6) and (9) and update \(\mathcal{M}^{t-1}\). endfor Output:\(\mathcal{M}=\{s_{j}^{0},\bm{x}_{j}^{0},\bm{O}_{j}^{0}\}_{j\in\mathcal{I}_{ \mathrm{fg}}\cup\mathcal{I}_{\mathrm{at}}}\) ```

**Algorithm 1** Joint Generation for Molecules using D3FG

### Heterogeneous graph: Joint or Two-stage?

Amino acids and functional groups are both fragments composed of atoms in proteins and molecules, regarded as rigid bodies, while the linkers are single atoms, regarded as mass points. Therefore, in the binding graph, nodes are of different levels and connections are of different kinds, thus leading the graph to be heterogeneous. For this reason, we propose two generative schemes: joint generation scheme and two-stage generation scheme as shown in Figure. 1.

In joint scheme, we regard amino acids, functional groups, and linkers at the same level, and use one single neural network to predict the three variables and update them. In detail, this scheme directly models \(p(\mathcal{M}|\mathcal{P})=p(\{s_{i},\bm{x}_{i},\bm{O}_{i}\}_{i\in\mathcal{I}_{ \mathrm{fg}}\cup\mathcal{I}_{\mathrm{st}}}|\mathcal{P})\), where the parameterized transition distribution is \(p(\{s_{i}^{t-1},\bm{x}_{i}^{t-1},\bm{O}_{i}^{t-1}\}_{i\in\mathcal{I}_{\mathrm{ fg}}\cup\mathcal{I}_{\mathrm{st}}}|\mathcal{M}^{t},\mathcal{P})\). Note that \(\bm{O}_{i}^{t}=\bm{I}\) for any \(t\) if \(i\in\mathcal{I}_{\mathrm{at}}\).

In two-stage scheme, we regard amino acids and functional groups at the fragment level, and linkers at the atom level, and use two different neural networks to parameterize the transition distribution. In the first stage, the functional groups are generated, and then single atoms as linkers will be generated to connect the generated functional groups as a full molecule. The two-stage generative scheme is similar to CADD, which first determines the pharmacophores towards the target protein, fits functional groups with high activity into them, and then searches for the possible molecules with these functional groups. In specific, the generative process reads \(p(\mathcal{M}|\mathcal{P})=p(\{s_{j},\bm{x}_{j}\}_{j\in\mathcal{I}_{\mathrm{st}}} |\{s_{i},\bm{x}_{i},\bm{O}_{i}\}_{i\in\mathcal{I}_{\mathrm{fg}}},\mathcal{P}) (\{s_{i},\bm{x}_{i},\bm{O}_{i}\}_{i\in\mathcal{I}_{\mathrm{fg}}}|\mathcal{P})\). The transition distribution of the first stage \(p(\{s_{i}^{t-1},\bm{x}_{i}^{t-1},\bm{O}_{i}^{t-1}\}_{i\in\mathcal{I}_{\mathrm{ gt}}}|\{s_{i}^{t},\bm{x}_{i}^{t},\bm{O}_{i}^{t}\}_{i\in\mathcal{I}_{\mathrm{fg}}}, \mathcal{P})\) is parameterized by one neural network; In the second stage, the generated \(\{s_{i},\bm{x}_{i},\bm{O}_{i}\}_{i\in\mathcal{I}_{\mathrm{fg}}}\) is used as context, so that the other neural network models \(p(\{s_{i}^{t-1},\bm{x}_{i}^{t-1}\}_{i\in\mathcal{I}_{\mathrm{at}}}|\{s_{i}^{t}, \bm{x}_{i}^{t}\}_{i\in\mathcal{I}_{\mathrm{at}}},\{s_{i},\bm{x}_{i},\bm{O}_{i} \}_{i\in\mathcal{I}_{\mathrm{fg}}},\mathcal{P})\).

## 5 Experiments

### Data Processing.

In the experiments, we use CrossDocked2020[32] for evaluation. In the prevailing works, they focus on generating molecules at the atom level, differing from our functional-group-based generation, so our first target is to divide the molecules into functional groups and linkers. We use EFGs[72] to segment molecules. We select the 25 functional groups that appear most frequently with stable structures, which are partly shown in Figure. 2 For some functional groups, chirality exists in their structures, and we treat them as two functional groups. As a result, we finally build up a dataset as a corpus including 27 functional groups (two of the 25 have chirality) for Crossdocked2020, with their intra-structures fixed as rigid bodies, and assure that most molecules can be decomposed into the substructures in our corpus datasets. Details are given in Appendix. D. For linkers, we choose \(\{\mathrm{B},\mathrm{C},\mathrm{N},\mathrm{O},\mathrm{F},\mathrm{P},\mathrm{S}, \mathrm{Cl},\mathrm{Br},\mathrm{I}\}\) as representative heavy atoms. After the processing, we obtain \(M_{\mathrm{fg}}=27\) and \(M_{\mathrm{at}}=10\) in our experiments. Besides, by the fragment-linker designation of a binding graph, the node number is reduced to 53.62 on average in GNN's message-passing.

### Pocket-Specific Molecule Generation

Dataset.The datasets for training and evaluation are split according to Pocket2Mol[9] and TargetDiff[13]. 22.5 million docked protein binding complexes with low RMSD ( < 1A) and sequence identity less than \(30\%\) are selected, leading to 100,000 pairs of pocket-ligand complexes, with 100 novel complexes as references for evaluation.

Setup.For performance comparison, our methods are compared with baselines including LiGAN[60], 3DSBDD[12], GraphBP[10], Pocket2Mol[54], DiffSBDD[14] and TargetDiff[13]. LiGAN as a 3D CNN-based method generates atoms on regular grids, with VAE[73] as its generative model. 3DSBDD, GraphBP and Pocket2Mol are all GNN-based, generating atoms in an auto-regressive way. DiffSBDD and TargetDiff are two diffusion-based methods that generate molecules at the full atom level, with equivariant GNNs as the denoisers. The two schemes of generation lead to two variants of our method, written as D3FG(Joint) and D3FG(Stage). In some parts, we choose Pocket2Mol as the representative autoregressive methods, and DiffSBDD and TargetDiff as the benchmarks employing diffusion models, because these three baselines are the

Figure 2: Five of twenty-five functional groups with stable structures that occur most frequently in Crossdocked2020 and are used in D3FG.

latest works that show good empirical performance. We sample 100 valid molecules for each pocket for the baselines and D3FG, leading to 10,000 pairs of complexes. After the molecules are generated by the model, Openbabel[74] is used to construct chemical bonds between atoms, and Universal Force Field (UFF) minimization [75] is used for refinement. The following evaluations are based on the samples. Figure. 4 gives an example of generated molecules by different methods.

Structure analysis.We first analyze the **functional groups** of generated molecules by different methods. We show the 'Ratio' and 'Freq' of the included functional groups partly, where 'Ratio' means how many specified functional groups are in each molecule on average, and 'Freq' means the statistical frequency of occurrence of the specified functional group in the generated molecules. Mean absolute error (MAE) as overall metrics is calculated according to reference 'Ratio' and generated 'Ratio'; Jensen-Shannon divergence (JSD) is calculated according to reference 'Freq' and generated 'Freq'. The smaller MAE and JSD are, the better performance the method achieves. Table. 1 shows the metrics of different methods' generated molecules, demonstrating D3FG's superiority in generating molecules with realistic drug structures since distributions of the functional groups in generated molecules are more similar to the real drug molecules'. Secondly, we analyze the **atom type**, **bond distance**, **bond angle**, **dihedral** and **atom type** distribution. JSDs are calculated between the distributions of bond distance for reference vs. generated molecules. '-', '=',':' represents single, double, and aromatic bonds, respectively. Besides, we report MAE on reference 'Ratio' vs. generated 'Ratio' and JSD on reference 'Freq' vs. generated 'Freq' based on atom type distribution. Table. 2, 3 and Figure. 3 gives details atom type analysis. For other geometries, please refer to Appendix. E. We found that D3FG(Stage) outperforms D3FG(Joint) in generating more realistic molecules, and has competitive performance with TargetDiff.

Binding affinity.We secondly make a comparison of Binding Affinity. Following 3DSBDD and L1GAN, we employed two evaluation metrics including Vina docking score [76] and Gnina docking score [77]. Vina docking [78] as a classical docking tool, gives a lower score as Vina energy if the binding affinity of the molecule is better, while Gnina docking as a deep-learning-based docking tool, gives it a higher score. \(\Delta\)Affinity measures the percentage of generated molecules that have better predicted binding affinity than the corresponding reference molecules. Table. 4 shows that our D3FG

\begin{table}
\begin{tabular}{c|c c c c c c} \hline \hline \multicolumn{1}{c|}{\multirow{2}{*}{Bond}} & \multicolumn{1}{c}{\multirow{2}{*}{LiGAN}} & \multicolumn{1}{c}{\multirow{2}{*}{3DSBDD}} & \multicolumn{1}{c}{\multirow{2}{*}{Pocket2Mol}} & \multicolumn{1}{c}{\multirow{2}{*}{TargetDiff}} & \multicolumn{1}{c}{\multirow{2}{*}{DiffSBDD}} & \multicolumn{1}{c}{\multirow{2}{*}{D3FG (Joint)}} & \multicolumn{1}{c}{\multirow{2}{*}{D3FG (Stage)}} \\ \hline C-C & 0.599 & 0.424 & 0.416 & 0.346 & 0.385 & 0.339 & **0.281** \\ C=C & 0.665 & 0.545 & 0.516 & 0.503 & 0.565 & 0.485 & **0.469** \\ C-N & 0.631 & 0.424 & 0.401 & **0.299** & 0.421 & 0.307 & 0.313 \\ C=N & 0.742 & 0.671 & 0.628 & 0.547 & 0.569 & 0.530 & **0.523** \\ C-O & 0.656 & 0.547 & 0.445 & 0.408 & 0.413 & 0.412 & **0.406** \\ C=O & 0.662 & 0.638 & 0.532 & **0.467** & 0.541 & 0.490 & 0.488 \\ c:c & 0.494 & 0.410 & 0.398 & **0.264** & 0.339 & 0.327 & 0.302 \\ c:n & 0.634 & 0.443 & 0.457 & **0.228** & 0.260 & 0.246 & 0.237 \\ \hline \hline \end{tabular}
\end{table}
Table 2: Jensen-Shannon divergence between the distributions of bond distance for reference vs. generated molecules. The smaller, the better. Value in **bold** is the lowest.

\begin{table}
\begin{tabular}{c|c c c c c c} \hline \hline \multicolumn{1}{c|}{\multirow{2}{*}{Bond}} & \multicolumn{1}{c}{\multirow{2}{*}{LiGAN}} & \multicolumn{1}{c}{\multirow{2}{*}{3DSBDD}} & \multicolumn{1}{c}{\multirow{2}{*}{Pocket2Mol}} & \multicolumn{1}{c}{\multirow{2}{*}{TargetDiff}} & \multicolumn{1}{c}{DiffSBDD} & \multicolumn{1}{c}{D3FG (Joint)} & \multicolumn{1}{c}{D3FG (Stage)} \\ \hline C-C & 0.599 & 0.424 & 0.416 & 0.346 & 0.385 & 0.339 & **0.281** \\ C=C & 0.665 & 0.545 & 0.516 & 0.503 & 0.565 & 0.485 & **0.469** \\ C-N & 0.631 & 0.424 & 0.401 & **0.299** & 0.421 & 0.307 & 0.313 \\ C=N & 0.742 & 0.671 & 0.628 & 0.547 & 0.569 & 0.530 & **0.523** \\ C-O & 0.656 & 0.547 & 0.445 & 0.408 & 0.413 & 0.412 & **0.406** \\ C=O & 0.662 & 0.638 & 0.532 & **0.467** & 0.541 & 0.490 & 0.488 \\ c:c & 0.494 & 0.410 & 0.398 & **0.264** & 0.339 & 0.327 & 0.302 \\ c:n & 0.634 & 0.443 & 0.457 & **0.228** & 0.260 & 0.246 & 0.237 \\ \hline \hline \end{tabular}
\end{table}
Table 2: Jensen-Shannon divergence between the distributions of bond distance for reference vs. generated molecules. The smaller, the better. Value in **bold** is the lowest.

of the two-stage scheme achieves competitive affinity scores, comparable to TargetDiff and much better than D3FG of the joint scheme.

Drug property.Moreover, chemical properties are evaluated with RdKit[79], including QED [80] (quantitative estimation of drug-likeness), SA [81] (synthetic accessibility score), LogP [82] (the octanol-water partition coefficient, which should be between \(-0.4\) and \(5.6\) for good drug candidates), and Lipinski [83; 84] (number of rules the drug follow the Lipinski's rule of five). QED, SA, and Lipinski are three metrics with preferences for atom numbers, demonstrated in DiffBP[15]. Table. 4 demonstrates that two variants of D3FG achieve overall best performance in these metrics.

\begin{table}
\begin{tabular}{c|c c c|c c c c} \hline \hline  & Vina & Vina & Gina & Gina & \multirow{2}{*}{QED (\(\uparrow\))} & \multirow{2}{*}{SA (\(\uparrow\))} & \multirow{2}{*}{LogP} & \multirow{2}{*}{Lipinski (\(\uparrow\))} \\  & Score (\(\downarrow\)) & Affinity (\(\uparrow\)) & Score (\(\uparrow\)) & & & & \\ \hline Ref. (Test) & -7.06 & & 5.37 & - & 0.471 & 0.725 & 0.818 & 4.247 \\ LiGAN & -6.17 & 21.24\% & 4.29 & 21.68\% & 0.382 & 0.584 & -0.138 & 4.046 \\ GraphBP & -6.36 & 27.41\% & 4.52 & 26.54\% & 0.437 & 0.502 & 3.024 & 4.448 \\ JDSBDD & -6.12 & 20.73\% & 4.48 & 19.22\% & 0.426 & 0.625 & 0.266 & 4.735 \\ PockerZMol. & -6.92 & 45.86\% & 5.34 & 40.68\% & **0.543** & 0.746 & 1.584 & 4.904 \\ TargetDiff & **-7.11** & **49.52\%** & **5.41** & **42.40\%** & 0.474 & 0.581 & 1.402 & 4.487 \\ DiffSBDD & -6.37 & 31.32\% & 4.63 & 27.96\% & 0.494 & 0.343 & -0.157 & 4.895 \\ D3FG (Joint) & -6.89 & 37.32\% & 5.30 & 33.45\% & **0.507** & **0.832** & 2.796 & **4.943** \\ D3FG (Stage) & **-6.96** & **45.88\%** & **5.43** & **43.36\%** & 0.501 & **0.840** & 2.821 & **4.965** \\ \hline D3FG (EHot) & -7.19 & 51.78\% & 5.51 & 56.53\% & 0.482 & 0.731 & 0.814 & 4.330 \\ D3FG (ECold) & -7.02 & 44.03\% & 5.16 & 32.69\% & 0.476 & 0.707 & 0.820 & 4.228 \\ \hline \hline \end{tabular}
\end{table}
Table 4: Evaluation of Binding affinity and other chemical drug properties for baselines and variants of D3FG. \(\downarrow\) means the smaller the value, the better the performance, and \(\uparrow\) means the opposite. Values in **bold** are the top-2 best metrics.

Figure 4: Generated molecules by different methods on pocket 3o96_a_rec. The diffusion-based methods generated molecules more similar to the reference, appearing to be ‘vertical’.

Figure 3: Atom type distribution and metrics.

### Pocket-Specific Molecule Elaboration

Introduction.Molecule elaboration as a sub-task of molecule optimization, aims to elaborate a fragment of existing molecules amenable to chemical modification for improving binding affinity. To fulfill it, we here first select a functional group in a ligand that contributes to binding affinity and remove it to obtain the editable fragments. Then, we attempt to use D3FG to generate the new type of functional groups with its structures, for modifying the fragments and thus building up a new molecule with higher binding affinity to the target protein.

Dataset.The selection of a functional group for replacement is the first problem. Pharmacophoric information is extracted by calculating the fragment hotspot map (FHM) [33; 34] of the target protein. In specific, FHM describes regions of the binding pocket that are likely to make a positive contribution to binding affinity. Then, by placing the molecules into the pockets, we can thus obtain each functional group's hotspot scores, according to the binding complexes. The higher hotspot score a functional group reaches, the more contributions it makes to the binding affinity. Functional groups' hotspot scores of each ligand are calculated based on 100,000 pairs of pocket-ligand complexes in Crossdocked2020, and the selection of functional groups can be based on their scores. Finally, we established our new datasets for molecule elaboration based on FHM.

Binding affinity and drug property.We here consider two schemes for molecule elaboration, the first is to remove the functional groups with the highest scores, and elaborate the remaining fragments by replacing them with the functional group generated by D3FG. We write this as D3FG(EHot). The second D3FG(ECold), on the other hand, replaces the functional groups of the lowest scores). We report the D3FG's elaborated molecules by the two schemes in Table. 4. It shows that D3FG(EHot) tends to generate more molecules with higher affinity, while molecules elaborated by D3FG(ECold) show tiny differences in binding affinity from the raw references. Besides, on other chemical properties, the elaborated molecules are very close to the raw references, since the differences lie only in one single functional group, and the major molecular skeletons are almost identical.

## 6 Conclusion and Future Work

In this paper, a functional-group-based diffusion model called D3FG is proposed to generate molecules in 3D with protein structures as its context. Joint and two-stage generation schemes lead to two variants of D3FG. The molecules generated by the two-stage generation scheme show more realistic structures, competitive binding performance, and better drug properties. Besides, in molecule elaboration, D3FG can also generate molecules with good binding affinity. However, limitation still exists. First, the functional group datasets are still small, which will be enlarged in the future. Second, the binding affinities of generated molecules still remain to be improved, since other diffusion models even show better binding performance.

## Acknowledgements

This work was supported by National Key R&D Program of China (No. 2022ZD0115100), National Natural Science Foundation of China Project (No. U21A20427), and Project (No. WU2022A009) from the Center of Synthetic Biology and Integrated Bioengineering of Westlake University.

## References

* [1] T. Langer and R.D. Hoffmann. _Pharmacophores and Pharmacophore Searches_. 08 2006.
* multiobjective/multicriteria optimization and decision support in drug discovery. In John B. Taylor and David J. Triggle, editors, _Comprehensive Medicinal Chemistry II_, pages 767-774. Elsevier, Oxford, 2007.
* [3] John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin Zidek, Anna Potapenko, Alex Bridgland, Clemens Meyer, Simon Kohl, Andrew Ballard, Andrew Cowie, Bernardino Romera-Paredes, Stanislav Nikolov, Rishub Jain, Jonas Adler, and Demis Hassabis. Highly accurate protein structure prediction with alphafold. _Nature_, 596:1-11, 08 2021.
* [4] Zhangyang Gao, Cheng Tan, Stan Li, et al. Alphadesign: A graph protein design method and benchmark on alphafoldb. _arXiv preprint arXiv:2202.01079_, 2022.
* [5] Yufei Huang, Lirong Wu, Haitao Lin, Jiangbin Zheng, Ge Wang, and Stan Z. Li. Data-efficient protein 3d geometric pretraining via refinement of diffused protein structure decoy, 2023.
* [6] Lirong Wu, Yufei Huang, Haitao Lin, and Stan Z. Li. A survey on protein representation learning: Retrospect and prospect, 2022.
* [7] Zhangyang Gao, Cheng Tan, and Stan Z Li. Pifold: Toward effective and efficient protein inverse folding. _arXiv preprint arXiv:2209.12643_, 2022.
* [8] Cheng Tan, Zhangyang Gao, Jun Xia, and Stan Z Li. Generative de novo protein design with global context. _arXiv preprint arXiv:2204.10673_, 2022.
* [9] Xingang Peng, Shitong Luo, Jiaqi Guan, Qi Xie, Jian Peng, and Jianzhu Ma. Pocket2mol: Efficient molecular sampling based on 3d protein pockets. In _International Conference on Machine Learning_, 2022.
* [10] Meng Liu, Youzhi Luo, Kanji Uchino, Koji Maruhashi, and Shuiwang Ji. Generating 3d molecules for target protein binding. In _International Conference on Machine Learning_, 2022.
* [11] Cheng Tan, Zhangyang Gao, and Stan Z Li. Target-aware molecular graph generation. _arXiv preprint arXiv:2202.04829_, 2022.
* [12] Shitong Luo, Jiaqi Guan, Jianzhu Ma, and Jian Peng. A 3D generative model for structure-based drug design. In _Thirty-Fifth Conference on Neural Information Processing Systems_, 2021.
* [13] Jiaqi Guan, Wesley Wei Qian, Xingang Peng, Yufeng Su, Jian Peng, and Jianzhu Ma. 3d equivariant diffusion for target-aware molecule generation and affinity prediction. In _International Conference on Learning Representations_, 2023.
* [14] Arne Schneuing, Yuanqi Du, Charles Harris, Arian Jamasb, Ilia Igashov, Weitao Du, Tom Blundell, Pietro Lio, Carla Gomes, Max Welling, Michael Bronstein, and Bruno Correia. Structure-based drug design with equivariant diffusion models, 2022.
* [15] Haitao Lin, Yufei Huang, Meng Liu, Xuanjing Li, Shuiwang Ji, and Stan Z. Li. Diffbp: Generative diffusion of 3d molecules for target protein binding, 2022.
* [16] Carl Doersch. Tutorial on variational autoencoders. _arXiv preprint arXiv:1606.05908_, 2016.
* [17] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In _NIPS_, pages 2672-2680, 2014.
* [18] Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real NVP. _arXiv preprint arXiv:1605.08803_, 2016.
* [19] Durk P Kingma and Prafulla Dhariwal. Glow: Generative flow with invertible 1x1 convolutions. _Advances in Neural Information Processing Systems_, 31:10215-10224, 2018.

* [20] Didrik Nielsen, Priyank Jaini, Emiel Hoogeboom, Ole Winther, and Max Welling. Survae flows: Surjections to bridge the gap between vaes and flows. _Advances in Neural Information Processing Systems_, 33:12685-12696, 2020.
* [21] Yoshua Bengio, Tristan Deleu, Edward J. Hu, Salem Lahlou, Mo Tiwari, and Emmanuel Bengio. GFlowNet foundations. _CoRR_, abs/2111.09266, 2021.
* [22] Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional networks, 2016.
* [23] Justin Gilmer, Samuel S. Schoenholz, Patrick F. Riley, Oriol Vinyals, and George E. Dahl. Neural message passing for quantum chemistry, 2017.
* [24] Meng Liu, Youzhi Luo, Limei Wang, Yaochen Xie, Hao Yuan, Shurui Gui, Haiyang Yu, Zhao Xu, Jingtun Zhang, Yi Liu, et al. DIG: A turnkey library for diving into graph deep learning research. _Journal of Machine Learning Research_, 22(240):1-9, 2021.
* [25] Lirong Wu, Haitao Lin, Cheng Tan, Zhangyang Gao, and Stan Z. Li. Self-supervised learning on graphs: Contrastive, generative,or predictive. _IEEE Transactions on Knowledge and Data Engineering_, pages 1-1, 2021.
* [26] Yi Liu, Limei Wang, Meng Liu, Yuchao Lin, Xuan Zhang, Bora Oztekin, and Shuiwang Ji. Spherical message passing for 3d molecular graphs. In _International Conference on Learning Representations_, 2021.
* [27] Lirong Wu, Haitao Lin, Zhangyang Gao, Cheng Tan, and Stan. Z. Li. Graphmixup: Improving class-imbalanced node classification on graphs by self-supervised context prediction, 2021.
* [28] Lirong Wu, Haitao Lin, Bozhen Hu, Cheng Tan, Zhangyang Gao, Zicheng Liu, and Stan Z. Li. Beyond homophily and homogeneity assumption: Relation-based frequency adaptive graph neural networks. _IEEE Transactions on Neural Networks and Learning Systems_, pages 1-13, 2023.
* [29] Lirong Wu, Haitao Lin, Yufei Huang, and Stan Z. Li. Knowledge distillation improves graph structure augmentation for graph neural networks. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, _Advances in Neural Information Processing Systems_, volume 35, pages 11815-11827. Curran Associates, Inc., 2022.
* [30] Lirong Wu, Haitao Lin, Yufei Huang, Tianyu Fan, and Stan Z. Li. Extracting low-/high-frequency knowledge from graph neural networks and injecting it into mlps: An effective gnn-to-mlp distillation framework, 2023.
* [31] Lirong Wu, Haitao Lin, Yufei Huang, and Stan Z. Li. Quantifying the knowledge in gnns for reliable distillation into mlps, 2023.
* [32] Paul Francoeur, Tomohide Masuda, Jocelyn Sunseri, Andrew Jia, Richard Iovanisci, Ian Snyder, and David Koes. 3d convolutional neural networks and a crossdocked dataset for structure-based drug design. _Journal of Chemical Information and Modeling_, XXXX, 08 2020.
* [33] Mihaela D. Smilova, Peter R. Curran, Chris J. Radoux, Frank von Delft, Jason C. Cole, Anthony R. Bradley, and Brian D. Marsden. Fragment hotspot mapping to identify selectivity-determining regions between related proteins. _Journal of Chemical Information and Modeling_, 62(2):284-294, 2022. PMID: 35020376.
* [34] Chris J. Radoux, Tjelvar S. G. Olsson, Will R. Pitt, Colin R. Groom, and Tom L. Blundell. Identifying interactions that determine fragment binding at protein hotspots. _Journal of Medicinal Chemistry_, 59(9):4314-4325, 2016. PMID: 27043011.
* [35] Esben Jannik Bjerrum and Richard Threlfall. Molecular generation with recurrent neural networks (rnns), 2017.
* [36] Rafael Go mez-Bombarelli, Jennifer N. Wei, David Duvenaud, Jose Miguel Hernandez-Lobato, Benjamin Sanchez-Lengeling, Dennis Sheberla, Jorge Aguilera-Iparraguirre, Timothy D. Hirzel, Ryan P. Adams, and Alan Aspuru-Guzik. Automatic chemical design using a data-driven continuous representation of molecules. _ACS Central Science_, 4(2):268-276, jan 2018.

* [37] Matt J. Kusner, Brooks Paige, and Jose Miguel Hernandez-Lobato. Grammar variational autoencoder, 2017.
* [38] Marwin H. S. Segler, Thierry Kogej, Christian Tyrchan, and Mark P. Waller. Generating focussed molecule libraries for drug discovery with recurrent neural networks. _ArXiv_, abs/1701.01329, 2017.
* [39] Qi Liu, Militadis Allamanis, Marc Brockschmidt, and Alexander Gaunt. Constrained graph variational autoencoders for molecule design. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 31. Curran Associates, Inc., 2018.
* [40] Chence Shi, Minkai Xu, Zhaocheng Zhu, Weinan Zhang, Ming Zhang, and Jian Tang. Graphaf: a flow-based autoregressive model for molecular graph generation, 2020.
* [41] Wengong Jin, Regina Barzilay, and Tommi Jaakkola. Junction tree variational autoencoder for molecular graph generation, 2019.
* [42] Wengong Jin, Regina Barzilay, and Tommi Jaakkola. Multi-objective molecule generation using interpretable substructures, 2020.
* [43] Jiaxuan You, Bowen Liu, Rex Ying, Vijay Pande, and Jure Leskovec. Graph convolutional policy network for goal-directed molecular graph generation, 2019.
* [44] Victor Garcia Satorras, Emiel Hoogeboom, and Max Welling. E(n) equivariant graph neural networks, 2021.
* [45] Bowen Jing, Stephan Eismann, Patricia Suriana, Raphael J. L. Townshend, and Ron Dror. Learning from protein structure with geometric vector perceptrons. In _Submitted to International Conference on Learning Representations_, 2021.
* [46] Bowen Jing, Stephan Eismann, Pratham N. Soni, and Ron O. Dror. Equivariant graph neural networks for 3d macromolecular structure, 2021.
* [47] Minkai Xu, Wujie Wang, Shitong Luo, Chence Shi, Yoshua Bengio, Rafael Gomez-Bombarelli, and Jian Tang. An end-to-end framework for molecular conformation generation via bilevel programming. 2021.
* [48] Chence Shi, Shitong Luo, Minkai Xu, and Jian Tang. Learning gradient fields for molecular conformation generation. _Proceedings of the 38th International Conference on Machine Learning, ICML_, 139:9558-9568, 2021.
* [49] Shitong Luo, Chence Shi, Minkai Xu, and Jian Tang. Predicting molecular conformation via dynamic graph score matching. _Advances in Neural Information Processing Systems_, 34, 2021.
* [50] Minkai Xu, Lantao Yu, Yang Song, Chence Shi, Stefano Ermon, and Jian Tang. GeoDiff: A geometric diffusion model for molecular conformation generation. In _International Conference on Learning Representations_, 2021.
* [51] Minkai Xu, Shitong Luo, Yoshua Bengio, Jian Peng, and Jian Tang. Learning neural generative dynamics for molecular conformation generation. _International Conference on Learning Representations, (ICLR)_, 2021.
* [52] Victor Garcia Satorras, Emiel Hoogeboom, Fabian B. Fuchs, Ingmar Posner, and Max Welling. E(n) equivariant normalizing flows, 2021.
* [53] Emiel Hoogeboom, Victor Garcia Satorras, Clement Vignac, and Max Welling. Equivariant diffusion for molecule generation in 3d, 2022.
* [54] Youzhi Luo and Shuiwang Ji. An autoregressive flow model for 3d molecular geometry generation from scratch. In _International Conference on Learning Representations_, 2021.
* [55] Wengong Jin, Regina Barzilay, and Tommi Jaakkola. Junction tree variational autoencoder for molecular graph generation, 2019.

* [56] Xiangzhe Kong, Wenbing Huang, Zhixing Tan, and Yang Liu. Molecule generation by principal subgraph mining and assembling, 2022.
* [57] Harrison Green, David R. Koesb, and Jacob D. Durrant. Deepfrag: a deep convolutional neural network for fragment-based lead optimization. In _Chemical Science_, 2021.
* [58] Keir Adams and Connor W. Coley. Equivariant shape-conditioned generation of 3d molecules for ligand-based drug design. In _The Eleventh International Conference on Learning Representations_, 2023.
* [59] ZAIXI ZHANG, Shuxin Zheng, Yaosen Min, and Qi Liu. Molecule generation for target protein binding with structural motifs. In _International Conference on Learning Representations_, 2023.
* [60] Tomohide Masuda, Matthew Ragoza, and David Ryan Koes. Generating 3d molecular structures conditional on a receptor binding site with deep generative models. _arXiv preprint arXiv:2010.14442_, 2020.
* [61] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. _arXiv preprint arXiv:2006.11239_, 2020.
* [62] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. _Advances in Neural Information Processing Systems_, 32, 2019.
* [63] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. _arXiv preprint arXiv:2011.13456_, 2020.
* [64] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models, 2022.
* [65] Jacob Austin, Daniel D. Johnson, Jonathan Ho, Daniel Tarlow, and Rianne van den Berg. Structured denoising diffusion models in discrete state-spaces, 2023.
* [66] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding, 2019.
* [67] Adam Leach, Sebastian M Schmon, Matteo T. Degiacomi, and Chris G. Willcocks. Denoising diffusion probabilistic models on SO(3) for rotational alignment. In _ICLR 2022 Workshop on Geometrical and Topological Representation Learning_, 2022.
* [68] T. I. Savjolova. Preface to novye metody issledovanija tekstury polikristalliceskich materialov. metallurgija.
* [69] Shitong Luo, Yufeng Su, Xingang Peng, Sheng Wang, Jian Peng, and Jianzhu Ma. Antigen-specific antibody design and optimization with diffusion-based generative models for protein structures. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, _Advances in Neural Information Processing Systems_, 2022.
* [70] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need, 2017.
* [71] Miltiadis Kofinas, Naveen Shankar Nagaraja, and Efstratios Gavves. Roto-translated local coordinate frames for interacting dynamical systems, 2022.
* [72] Peter Ertl. An algorithm to identify functional groups in organic molecules. _Journal of Cheminformatics_, 9, 2017.
* [73] Diederik P Kingma and Max Welling. Auto-encoding variational bayes, 2022.
* [74] Noel M O'Boyle, Michael Banck, Craig A James, Chris Morley, Tim Vandermeersch, and Geoffrey R Hutchison. Open babel: An open chemical toolbox. _Journal of cheminformatics_, 3(1):1-14, 2011.

* [75] Anthony K Rappe, Carla J Casewit, KS Colwell, William A Goddard III, and W Mason Skiff. UFF, a full periodic table force field for molecular mechanics and molecular dynamics simulations. _Journal of the American chemical society_, 114(25):10024-10035, 1992.
* [76] Oleg Trott and Arthur J. Olson. Autodock vina: Improving the speed and accuracy of docking with a new scoring function, efficient optimization, and multithreading. _Journal of Computational Chemistry_, 31, 2009.
* [77] Andrew T McNutt, Paul G. Francoeur, Rishal Aggarwal, Tomohide Masuda, Rocco Meli, Matthew Ragoza, Jocelyn Sunseri, and David Ryan Koes. Gnina 1.0: molecular docking with deep learning. _Journal of Cheminformatics_, 13, 2021.
* [78] Amr Alhossary, Stephanus Daniel Handoko, Yuguang Mu, and Chee-Keong Kwoh. Fast, accurate, and reliable molecular docking with QuickVina 2. _Bioinformatics_, 31(13):2214-2216, 02 2015.
* [79] Greg Landrum. Rdkit: Open-source cheminformatics software. 2016.
* [80] Richard Bickerton, Gaia Paolini, Jeremy Besnard, Sorel Muresan, and Andrew Hopkins. Quantifying the chemical beauty of drugs. _Nature chemistry_, 4:90-8, 02 2012.
* [81] P Ertl and Ansgar Schuffenhauer. Estimation of synthetic accessibility score of drug-like molecules based on molecular complexity and fragment contributions. _Journal of cheminformatics_, 1:8, 06 2009.
* [82] Arup K. Ghose, Vellarkad N. Viswanadhan, and John J. Wendoloski. A knowledge-based approach in designing combinatorial or medicinal chemistry libraries for drug discovery. 1. a qualitative and quantitative characterization of known drug databases. _Journal of Combinatorial Chemistry_, 1(1):55-68, December 1998.
* [83] Christopher A Lipinski, Franco Lombardo, Beryl W Dominy, and Paul J Feeney. Experimental and computational approaches to estimate solubility and permeability in drug discovery and development settings. _Advanced drug delivery reviews_, 23(1-3):3-25, 1997.
* [84] Daniel F. Veber, Stephen R. Johnson, Hung-Yuan Cheng, Brian R. Smith, Keith W. Ward, and Kenneth D. Kopple. Molecular properties that influence the oral bioavailability of drug candidates. _Journal of Medicinal Chemistry_, 45(12):2615-2623, May 2002.

Proof of Proposition 1.

First, we here denote all atom's positions in the molecules as \(\bm{X}_{\mathrm{M}}\in\mathbb{R}^{N_{\mathrm{a}}\times 3}\), and in the protein as \(\bm{X}_{\mathrm{M}}\in\mathbb{R}^{N_{\mathrm{a}}\times 3}\), and linker and functional group type as \(\bm{S}_{\mathrm{M}}\in\mathbb{R}^{(N_{\mathrm{a}}+N_{\mathrm{f}})\times(22+M_{ \mathrm{f}}+M_{\mathrm{a}t})}\). Note that one functional group may contain several atoms so that \(N_{\mathrm{aa}}+N_{\mathrm{f}_{\mathrm{f}}}<N_{\mathrm{a}}\).

SE(3) group as a roto-translation group in \(\mathbb{R}^{3}\), can be divided into two groups: SO(3) as the rotation group and T(3) as the translation group. For \(\bm{x}\in\mathbb{R}^{3}\), and \(g=r+v\) with \(g\in\mathrm{SE}(3),r\in\mathrm{SO}(3),v\in\mathrm{T}(3)\), \(T_{g}(\bm{x})=T_{r+v}(\bm{x})=T_{v}\circ T_{r}(\bm{x})\)

**Lemma A1**. If the The equivariance and invariance of the distribution in the reverse diffusion process are listed as

\[\begin{split}& p\left(\mathrm{T}_{g}(\bm{X}_{\mathrm{M}}^{T}),\bm{S}_{ \mathrm{M}}^{T}|\mathrm{T}_{g}(\bm{X}_{\mathrm{P}}),\bm{S}_{\mathrm{P}}\right) =p\left(\bm{X}_{\mathrm{M}}^{T},\bm{S}_{\mathrm{M}}^{T}|\bm{X}_{\mathrm{P}}, \bm{S}_{\mathrm{P}}\right)\\ & p\left(\mathrm{T}_{g}(\bm{X}_{\mathrm{M}}^{t-1})|\mathrm{T}_{g}( \bm{X}_{\mathrm{M}}^{t}),\bm{S}_{\mathrm{M}}^{t},\mathrm{T}_{g}(\bm{X}_{ \mathrm{P}}),\bm{S}_{\mathrm{P}}\right)=p\left(\bm{X}_{\mathrm{M}}^{t-1}|\bm{ X}_{\mathrm{M}}^{t},\bm{S}_{\mathrm{M}}^{t},\bm{X}_{\mathrm{P}},\bm{S}_{ \mathrm{P}}\right)\\ & p\left(\bm{S}_{\mathrm{M}}^{t-1}|\mathrm{T}_{g}(\bm{X}_{\mathrm{M }}^{t}),\bm{S}_{\mathrm{M}}^{t},\mathrm{T}_{g}(\bm{X}_{\mathrm{P}}),\bm{S}_{ \mathrm{P}}\right)=p\left(\bm{S}_{\mathrm{M}}^{t-1}|\bm{X}_{\mathrm{M}}^{t},\bm {S}_{\mathrm{M}}^{t},\bm{X}_{\mathrm{P}},\bm{S}_{\mathrm{P}}\right),\end{split}\] (13)

Then \(p\left(\bm{X}_{\mathrm{M}}^{T},\bm{S}_{\mathrm{M}}^{T}|\bm{X}_{\mathrm{P}}, \bm{S}_{\mathrm{P}}\right)\) is SE(3) invariant.

Proof.: Since \(\bm{\mathrm{T}}_{g}(\mathcal{M})=\{\bm{S},\mathrm{T}_{g}(\bm{X}_{\mathrm{M}})\}\), we can write the joint generative process as

\[p\left(\mathrm{T}_{g}(\bm{X}_{\mathrm{M}}),\bm{S}_{\mathrm{M}}| \mathrm{T}_{g}(\bm{X}_{\mathrm{P}}),\bm{S}_{\mathrm{P}}\right)\] \[= \int p\left(\mathrm{T}_{g}(\bm{X}_{\mathrm{M}}^{T}),\bm{S}_{ \mathrm{M}}^{T}|\mathrm{T}_{g}(\bm{X}_{\mathrm{P}}),\bm{S}_{\mathrm{P}}\right) \prod_{t=0}^{T-1}p\left(\mathrm{T}_{g}(\bm{X}_{\mathrm{M}}^{t-1}),\bm{S}_{ \mathrm{M}}^{t-1}|\mathrm{T}_{g}(\bm{X}_{\mathrm{M}}^{t}),\bm{S}_{\mathrm{M}}^ {t},\mathrm{T}_{g}(\bm{X}_{\mathrm{P}}),\bm{S}_{\mathrm{P}}\right)d\mathcal{M} ^{0:T-1}\] \[= \int p\left(\bm{X}_{\mathrm{M}}^{T},\bm{S}_{\mathrm{M}}^{T}|\bm{X} _{\mathrm{P}},\bm{S}_{\mathrm{P}}\right)\prod_{t=0}^{T-1}p\left(\mathrm{T}_{g} (\bm{X}_{\mathrm{M}}^{t-1}),\bm{S}_{\mathrm{M}}^{t-1}|\mathrm{T}_{g}(\mathcal{ M}_{\mathrm{M}}^{t}),\bm{S}_{\mathrm{M}}^{t},\mathrm{T}_{g}(\bm{X}_{ \mathrm{P}}),\bm{S}_{\mathrm{P}}\right)d\mathcal{M}^{0:T-1}\] \[= \int p\left(\bm{X}_{\mathrm{M}}^{T},\bm{S}_{\mathrm{M}}^{T}|\bm{X} _{\mathrm{P}},\bm{S}_{\mathrm{P}}\right)\prod_{t=0}^{T-1}p\left(\bm{X}_{ \mathrm{M}}^{t-1}|\mathcal{M}^{t},\mathcal{P}\right)p\left(\bm{S}_{\mathrm{M}}^ {t-1}|\mathcal{M}^{t},\mathcal{P}\right)d\mathcal{M}^{0:T-1}\] \[= \int p\left(\bm{X}_{\mathrm{M}}^{T},\bm{S}_{\mathrm{M}}^{T}|\bm{X} _{\mathrm{P}},\bm{S}_{\mathrm{P}}\right)\prod_{t=0}^{T-1}p\left(\bm{X}_{ \mathrm{M}}^{t-1},\bm{S}_{\mathrm{M}}^{t-1}|\bm{X}_{\mathrm{M}}^{t},\bm{S}_{ \mathrm{M}}^{t},\bm{X}_{\mathrm{P}},\bm{S}_{\mathrm{P}}\right)d\mathcal{M}^{0: T-1}\] \[= p\left(\bm{X}_{\mathrm{M}},\bm{S}_{\mathrm{M}}|\bm{X}_{\mathrm{P} },\bm{S}_{\mathrm{P}}\right)\] (14)

Then, let's consider a single atom's position. We here denote each atom's \(\bm{x}_{\mathrm{M}}^{t}\) as

\[\bm{x}_{\mathrm{M}}^{t}=\bm{x}_{\mathrm{C}}^{t}+\bm{x}_{\mathrm{R}}^{t}\bm{O}_{ c}^{t}\] (15)

where \(\bm{x}_{\mathrm{C}}^{t}\) is the defined center atom's position in the functional group, \(\bm{x}_{\mathrm{R}}^{t}\) is the relative position of the atom in the local coordinate system centering at \(\bm{x}_{\mathrm{C}}^{t}\), \(\bm{O}_{\mathrm{C}}^{t}\) is the rotation matrices of the local coordinate system with respect to the global system. Moreover, because the functional group is regarded as rigid bodies, \(\bm{x}_{\mathrm{R}}^{t}=\bm{x}_{\mathrm{R}}\) is constant. To be specific, if \(\bm{x}_{\mathrm{M}}^{t}\) refers to linker's position, \(\bm{O}_{\mathrm{C}}^{t}=\bm{0}\).

**Proposition A2**.: If each atom's relative positions in the local coordinate systems are fixed, and \(p(\bm{x}_{\mathrm{C}}^{t-1}|\mathcal{M}^{t},\mathcal{P})\) is SE(3)-equivariant and \(p(\bm{O}_{\mathrm{C}}^{t-1}|\mathcal{M}^{t},\mathcal{P})\) is SO(3)-equivariant and T(3)-invariant, such that \(p\left(\mathrm{T}_{g}(\bm{x}_{\mathrm{C}}^{t-1})|\mathrm{T}_{g}(\mathcal{M}^{t}, \mathcal{P})\right)=p\left(\bm{x}_{\mathrm{C}}^{t-1}|\mathcal{M}^{t},\mathcal{ P}\right)\) and \(p\left(\mathrm{T}_{r}(\bm{O}_{\mathrm{C}}^{t-1})|\mathrm{T}_{g}(\mathcal{M}^{t}, \mathcal{P})\right)=p\left(\bm{O}_{\mathrm{C}}^{t-1}|\mathcal{M}^{t},\mathcal{ P}\right)\), where \(r\in\mathrm{SO}(3),v\in\mathrm{T}(3),r+v=g\in\mathrm{SE}(3)\), then \(p(\bm{x}_{\mathrm{M}}^{t-1}|\mathcal{M}^{t},\mathcal{P})\) is SO(3)-equivariant.

Proof.: According to the convolution formula in probability theory, if \(w=u+v\), then

\[p(w)=\int p(u,w-u)du=\int p(w-v,v)dv\] (16)By using the Eq. (16), we can write every single atom's position density function as

\[p\left(\bm{x}_{\mathrm{M}}^{t-1}|\mathcal{M}^{t},\mathcal{P}\right)\] (17) \[= p\left(\bm{x}_{\mathrm{C}}^{t-1}+\bm{x}_{\mathrm{M}}\bm{O}_{ \mathrm{C}}^{t-1}|\mathcal{M}^{t},\mathcal{P}\right)\] \[= \int p\left(\bm{x}_{\mathrm{C}}^{t-1},\bm{x}_{\mathrm{M}}^{t-1}- \bm{x}_{\mathrm{C}}^{t-1}|\mathcal{M}^{t},\mathcal{P}\right)d\bm{x}_{\mathrm{C }}^{t-1}\] \[= \int p\left(\bm{x}_{\mathrm{C}}^{t-1}|\mathcal{M}^{t},\mathcal{P} \right)p\left(\bm{x}_{\mathrm{M}}^{t-1}-\bm{x}_{\mathrm{C}}^{t-1}|\mathcal{M}^ {t},\mathcal{P}\right)d\bm{x}_{\mathrm{C}}^{t-1}\]

Since

\[p\left(\mathrm{T}_{g}(\bm{x}_{\mathrm{C}}^{t-1})|\mathrm{T}_{g}(\mathcal{M}^{ t},\mathcal{P})\right)=p\left(\bm{x}_{\mathrm{C}}^{t-1}|\mathcal{M}^{t}, \mathcal{P}\right),\] (18)

and

\[p\left(\mathrm{T}_{r}(\bm{O}_{\mathrm{C}}^{t-1})|\mathrm{T}_{g}(\mathcal{M}^{ t},\mathcal{P})\right)=p\left(\bm{O}_{\mathrm{C}}^{t-1}|\mathcal{M}^{t}, \mathcal{P}\right).\] (19)

We can obtain that

\[p\left(\mathrm{T}_{r}(\bm{x}_{\mathrm{M}}^{t-1}-\bm{x}_{\mathrm{ C}}^{t-1})|\mathrm{T}_{g}(\mathcal{M}^{t},\mathcal{P})\right)\] (20) \[= p\left(\mathrm{T}_{r}(\bm{x}_{\mathrm{R}}\bm{O}_{\mathrm{C}}^{t- 1})|\mathrm{T}_{g}(\mathcal{M}^{t},\mathcal{P})\right)\] \[= \frac{1}{\bm{x}_{\mathrm{R}}}p\left(\bm{O}_{\mathrm{C}}^{t-1}| \mathcal{M}^{t},\mathcal{P}\right)\] \[= p\left(\bm{x}_{\mathrm{R}}\bm{O}_{\mathrm{C}}^{t-1}|\mathcal{M}^ {t},\mathcal{P}\right)\] \[= p\left(\bm{x}_{\mathrm{M}}^{t-1}-\bm{x}_{\mathrm{C}}^{t-1}| \mathcal{M}^{t},\mathcal{P}\right)\]

Therefore, according to Eq. (18), (20), and (17)

\[p\left(\mathrm{T}_{g}(\bm{x}_{\mathrm{M}}^{t-1})|\mathrm{T}_{g} (\mathcal{M}^{t},\mathcal{P})\right)\] (21) \[= \int p\left(\mathrm{T}_{g}\bm{x}_{\mathrm{C}}^{t-1})|\mathrm{T}_{ g}(\mathcal{M}^{t},\mathcal{P})\right)p\left(\mathrm{T}_{r+v}(\bm{x}_{ \mathrm{M}}^{t-1}-\bm{x}_{\mathrm{C}}^{t-1})|\mathrm{T}_{g}(\mathcal{M}^{t}, \mathcal{P})\right)d\bm{x}_{\mathrm{C}}^{t-1}\] \[= \int p\left(\mathrm{T}_{g}\bm{x}_{\mathrm{C}}^{t-1})|\mathrm{T}_{ g}(\mathcal{M}^{t},\mathcal{P})\right)p\left(\mathrm{T}_{r}(\bm{x}_{\mathrm{M}}^{t- 1}-\bm{x}_{\mathrm{C}}^{t-1})|\mathrm{T}_{g}(\mathcal{M}^{t},\mathcal{P}) \right)d\bm{x}_{\mathrm{C}}^{t-1}\] \[= \int p\left(\bm{x}_{\mathrm{C}}^{t-1}|\mathcal{M}^{t},\mathcal{P} \right)p\left(\bm{x}_{\mathrm{M}}^{t-1}-\bm{x}_{\mathrm{C}}^{t-1}|\mathcal{M} ^{t},\mathcal{P}\right)d\bm{x}_{\mathrm{C}}^{t-1}\] \[= p\left(\bm{x}_{\mathrm{M}}^{t-1}|\mathcal{M}^{t},\mathcal{P}\right)\]

Proof of Proposition 1.: The sufficiency of SE(3)-invariance of \(p(\bm{s}^{t-1}|\mathcal{M}^{t},\mathcal{P})\) and \(p(\bm{s}^{T})\) is given in **Lemma A.1**, and the sufficiency of SE(3)-equivariance of \(p(\bm{x}^{t-1}|\mathcal{M}^{t},\mathcal{P})\), and SO(3)-equivariance and T(3)-invariance of \(p(\bm{O}^{t-1}|\mathcal{M}^{t},\mathcal{P})\) is given in **Proposition A.2**. Besides, it is easy to obtain that if \(p(\bm{O}^{T})\) and \(p(\bm{x}^{T})\) is SE(3)-invariant distribution, then \(p(\bm{x}_{\mathrm{M}}^{T})\) will be invariant.

## Appendix B Model Comparisons.

With Pocket2Mol and GraphBP.Pocket2Mol and GraphBP are all auto-regressive models, which violate the physical rules from the perspective of energy[15], while the diffusion models which consider the global interactions are a solution to the problem. Besides, to decide which atom will be added a bond with the next atom, Pocket2Mol needs a classifier to predict the focal atom, so that the training and the prediction are not end-to-end. Finally, D3FG is a functional-group- or fragment-based method, while these two models generate molecules at the atom level.

With DiffSBDD and TargetDiff.These two methods are diffusion-based, which considers the global interactions between atoms. In the atom type generation, DiffSBDD uses continuous latent spaces generation, while Targetdiff diffuses the atom types in a uniform distribution. These two models all generate molecules at the atom level, while D3FG generates at the fragment level, and besides the types and positions, the orientations of the functional groups are generated.

With FLAG.It is still an auto-regressive model and does not support end-to-end generation since the focal atom needs to be predicted. Besides, we defined our functional groups as rigid bodies with stable intra-structures, while the motifs in FLAG are 2D smiles, with the structures generated by RDKIT. The 3D structures of functional groups are obtained by the training set in D3FG, thus avoiding the problem of distribution shift of FLAG since the training/test motif substructures may not match the RDKIT's generation.

With DeepFrag.It is a model for fragment-based lead optimization, in which the protein and the parent is used as condition and the fragment type is predicted by the model. In this way, it is more like an elaboration task defined in D3FG. Here we point out several advantages of D3FG(EHot/Cold) over Deepfrag. First, Deepfrag just predicts the fragment type, without 3D structures, so that the problem of equivariance is not included, but D3FG generates 3D positions of the molecules. Second, although the type is the SE(3)-invariant variable, the model uses CNNs as the model backbone rather than EGNN, so the invariance can also not be preserved. Instead, D3FG assures physical symmetries by using EGNN. Finally, D3FG is a generative model with stochasticity, while Deepfrag only gives the probability of the fragment types, as a discriminative model.

## Appendix C Method Details

Amino acid context encoding.Several geometric or type features are embedded to encode amino acids. For the geometric features including torsion angles/dihedrals, and pairwise distances, they are all roto-translational invariant, since the geometric features are all scalars obtained from relative coordinates. Besides, the local coordinates of atoms in a single amino acid are also invariant because it is always fixed in the local frame established by \(\mathrm{C}_{\alpha}\), \(\mathrm{C}\) and \(\mathrm{N}\). For the type features including amino acid types, sequential relationships, and pair of amino acid types, the translational and rotational operation is unrelated to them. Thus, the encoded amino acid contexts are roto-translational invariant, leading to the invariance of all the follow-up embeddings.

Equivariant neural network for linkers.For the roto-translational equivariance of positions for single atoms, since \(\bm{O}_{j}^{t}=\bm{I}\), Eq. 12 will be written as \(G\left(\mathcal{M}^{t},\mathcal{P}\right)[j]=\mathrm{MLP}_{G}(\bm{h}_{j}^{ \prime})\), unable to satisfy the equivariance. In this way, we revised it for single atoms by using the EGNN[44] stacked in the final layer for updating the positions, which reads

\[G\left(\mathcal{M}^{t},\mathcal{P}\right)[j]=\bm{x}_{j}+\frac{1}{C_{j}}\sum_{ i\in\mathcal{I}_{\mathrm{x}_{i}}\cup\mathcal{I}_{\mathrm{fg}}}(\bm{x}_{j}-\bm{x}_{i}) \bm{h}_{i}^{\prime},\] (22)

where we choose \(C_{j}=\|\bm{x}_{j}\|_{2}+1\).

## Appendix D Data Preprocessing

Local frame establishment.In 3D Euclidian space, for a rigid body including more than mass points that are not co-linear, a local frame can be established. We first choose a center node (center point) \(A\) as the origin, and a second node \(B\), leading to \(\vec{AB}\) as the direction of x-axis. Then, a third node \(C\) is selected. By Schmidt orthogonalization of \(\vec{AC}\) with respect to \(\vec{AB}\), the direction of y-axis can be computed. And finally, the direction of z-axis is obtained by the cross-product of the unit vectors in the x direction and y direction. By this means, the local frame is established by the three nodes, and the other nodes' local coordinates can be determined in the local frame. Because the local frame requires at least three points to establish, the functional groups including only 2 atoms are divided into two linkers.

Functional group datasets.We give detailed information on the included functional groups, including 2D graph, 3D structure, time of occurrence in the CrossDocked2020, and the center node (node \(A\)), node \(B\), and node \(C\) of the functional group in Table. 10.

Note that in beneze, the symmetric structures lead the frame nodes to be any three consecutive points on the hexagon. Besides, for the functional groups of 'NS(=O)=O' and 'O=CNO', two stable conformations exist, so we in practice regard them as four different types.

[MISSING_PAGE_FAIL:19]

\begin{table}
\begin{tabular}{c c c c c c c} \hline \hline Atom & Ref. & Pocket2Mol & TargetDiff & DiffSBDD & D3FG(Joint) & D3FG(Stage) \\ \hline C & 0.672 & 0.762 & 0.714 & 0.702 & 0.741 & 0.733 \\ N & 0.117 & 0.100 & 0.088 & 0.116 & 0.124 & 0.089 \\ O & 0.170 & 0.129 & 0.176 & 0.159 & 0.175 & 0.154 \\ F & 0.013 & 0.004 & 0.009 & 0.008 & 0.009 & 0.008 \\ P & 0.011 & 0.001 & 0.005 & 0.002 & 0.002 & 0.004 \\ S & 0.011 & 0.002 & 0.004 & 0.007 & 0.001 & 0.007 \\ Cl & 0.006 & 0.001 & 0.002 & 0.003 & 0.001 & 0.006 \\ \hline JSD & - & 0.098 & 0.059 & 0.054 & 0.075 & 0.056 \\ \hline \hline \end{tabular}
\end{table}
Table 7: Frequency of the atoms.

\begin{table}
\begin{tabular}{c|c c c c c} \hline \hline Angle & Pocket2Mol & TargetDiff & DiffSBDD & D3FG(Stage) & D3FG(Joint) \\ \hline C-C-C & 0.269 & 0.272 & 0.304 & **0.255** & 0.258 \\ C-C-N & 0.254 & 0.267 & 0.313 & 0.256 & **0.255** \\ C-N-C & 0.286 & **0.241** & 0.319 & 0.269 & 0.277 \\ C-C-O & 0.317 & 0.295 & 0.345 & **0.293** & 0.295 \\ C-O-C & 0.30

[MISSING_PAGE_EMPTY:21]

[MISSING_PAGE_EMPTY:22]

[MISSING_PAGE_EMPTY:23]

[MISSING_PAGE_EMPTY:24]