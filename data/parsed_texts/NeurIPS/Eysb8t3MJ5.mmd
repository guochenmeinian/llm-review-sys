# GlucoSynth: Generating Differentially-Private Synthetic Glucose Traces

Josephine Lamp\({}^{1,2}\) Mark Derdzinski\({}^{2}\) Christopher Hannemann\({}^{2}\)

Joost van der Linden\({}^{2}\) Lu Feng\({}^{1}\) Tianhao Wang\({}^{1}\) David Evans\({}^{1}\)

\({}^{1}\)University of Virginia, Charlottesville, VA, USA; \({}^{2}\)Dexcom, USA

jl4rj@virginia.edu; {mark.derdzinski; christopher.hannemann;

joost.vanderlinden}@dexcom.com; {lu.feng; tianhao; evans}@virginia.edu

###### Abstract

We focus on the problem of generating high-quality, private synthetic glucose traces, a task generalizable to many other time series sources. Existing methods for time series data synthesis, such as those using Generative Adversarial Networks (GANs), are not able to capture the innate characteristics of glucose data and cannot provide any formal privacy guarantees without severely degrading the utility of the synthetic data. In this paper we present GlucoSynth, a novel privacy-preserving GAN framework to generate synthetic glucose traces. The core intuition behind our approach is to conserve relationships amongst motifs (glucose events) within the traces, in addition to temporal dynamics. Our framework incorporates differential privacy mechanisms to provide strong formal privacy guarantees. We provide a comprehensive evaluation on the real-world utility of the data using 1.2 million glucose traces; GlucoSynth outperforms all previous methods in its ability to generate high-quality synthetic glucose traces with strong privacy guarantees.

## 1 Introduction

The sharing of medical time series data can facilitate therapy development. As a motivating example, sharing glucose traces can contribute to the understanding of diabetes disease mechanisms and the development of artificial insulin delivery systems that improve people with diabetes' quality of life. Unsurprisingly, there are serious legal and privacy concerns (e.g., HIPAA, GDPR) with the sharing of such granular, longitudinal time series data in a medical context [1]. One solution is to generate a set of synthetic traces from the original traces. In this way, the synthetic data may be shared publicly in place of the real ones with significantly reduced privacy and legal concerns.

This paper focuses on the problem of generating high-quality, privacy-preserving synthetic glucose traces, a task which generalizes to other time series sources and application domains, including activity sequences, inpatient events, hormone traces and cyber-physical systems. Specifically, we focus on long (over 200 timesteps), bounded, univariate time series glucose traces. We assume that available data does not have any labels or extra information including features or metadata, which is quite common, especially in diabetes. Continuous Glucose Monitors (CGMs) easily and automatically send glucose measurements taken subcutaneously at fixed intervals (e.g., every 5 minutes) to data storage facilities, but tracking other sources of diabetes-related data is challenging [2]. We characterize the quality of the generated traces based on three criteria-- synthetic traces should (1) conserve characteristics of the real data, i.e., glucose dynamics and control-related metrics (_fidelity_); (2) contain representation of diverse types of realistic traces, without the introduction of anomalous patterns that do not occur in real traces (_breadth_); and (3) be usable in place of the original data for real-world use cases (_utility_).

Generative Adversarial Networks (GANs) [3] have shown promise in the generation of time series data. However, previous methods for time series synthesis, e.g., [4; 5; 6], suffer from one or more of the following issues when applied to glucose traces: 1) surprisingly, they do not generate realistic synthetic glucose traces - in particular, they produce human physiologically impossible phenomenon in the traces; 2) they require additional information (features, metadata or labels) to guide the model learning which are not available for our traces; 3) they do not include any privacy guarantees, or, in order to uphold a strong formal privacy guarantee, severely degrade the utility of the synthetic data.

Generating high-quality synthetic glucose traces is a difficult task due to the innate characteristics of glucose data. Glucose traces can be best understood as sequences of events, which we call _motifs_, shown in Figure 1, and they are more event-driven than many other types of time series. As such, a current glucose value may be more influenced by an event that occurred in the far past compared to values from immediate previous timesteps. For example, a large meal eaten earlier in the day (30-90 minutes ago) may influence a patient's glucose more than the glucose values from the past 15 minutes. As a result, although there is some degree of temporal dependence within the traces, _only_ conserving the immediate temporal relationships amongst values at previous timesteps does not adequately capture the dynamics of this type of data. In particular, we find that the main reason previous methods fail is because they may not sufficiently learn event-related characteristics of glucose traces.

**Contributions.** We present _GlucoSynth_, a privacy-preserving GAN framework to generate synthetic glucose traces. The core intuition behind our approach is to conserve relationships amongst motifs (events) within the traces, in addition to the typical temporal dynamics contained within time series. We formalize the concept of motifs and define a notion of _motif causality_, inspired from Granger causality [7], which characterizes relationships amongst sequences of motifs within time series traces (Section 4). We define a local motif loss to first train a motif causality block that learns the motif causal relationships amongst the sequences of motifs in the real traces. The block outputs a motif causality matrix, that quantifies the causal value of seeing one particular motif after some other motif. Unrealistic motif sequences (such as a peak to an immediate drop in glucose values) will have causal relationships close to 0 in the causality matrix. We build a novel GAN framework that is trained to optimize motif causality within the traces in addition to temporal dynamics and distributional characteristics of the data (Section 5). Explicitly, the generator computes a motif causality matrix from each batch of synthetic data it generates, and compares it with the real causality matrix. As such, as the generator learns to generate synthetic data that yields a realistic causal matrix (thereby identifying appropriate causal relationships from the motifs), it implicitly learns not to generate unrealistic motif sequences. We also integrate differential privacy (DP) [8] into the framework (Section 6), which provides an intuitive bound on how much information may be disclosed about any individual in the dataset, allowing the GlucoSynth model to be trained with privacy guarantees. Finally, in Section 7, we present a comprehensive evaluation using 1.2 million glucose traces from individuals with diabetes collected across 2022, showcasing the suitability of our model to outperform all previous models and generate high-quality synthetic glucose traces with strong privacy guarantees.

## 2 Related Work

We focus the scope of our comparison on current state-of-the-art methods for synthetic time series which all build upon Generative Adversarial Networks (GANs) [3] and transformation-based approaches [9]. An extended related work is in Appendix A.

Figure 1: Example Real Glucose Traces and Glucose Motifs from our Dataset.

**Time Series.** Brophy et al. [10] provides a survey of GANs for time series synthesis. TimeGan [4] is a popular benchmark that jointly learns an embedding space using supervised and adversarial objectives in order to capture the temporal dynamics amongst traces. Esteban et al. [11] develops two time series GAN models (RGAN/RCGAN) with RNN architectures, conditioned on auxiliary information provided at each timestep during training. TTS-GAN [5] trains a GAN model that uses a transformer encoding architecture in order to best preserve temporal dynamics. Transformation-based approaches such as real-valued non-volume preserving transformations (NVP) [9] and Fourier Flows (FF) [12], have also had success for time series data. These methods model the underlying distribution of the real data to transform the input traces into a synthetic data set. Methods that only focus on learning the temporal or distributional dynamics in time series are not sufficient for generating realistic synthetic glucose traces due to the lack of temporal dependence within sequences of glucose motifs.

**Differentially-Private GANs.** To protect sensitive data, several GAN architectures have been designed to incorporate privacy-preserving noise needed to satisfy differential privacy guarantees [13]. Frigerio et al. [14] extends a simple differentially-private architecture (dpGAN) to time-series data and RDP-CGAN [6] develops a convolutional GAN architecture specifically for medical data. These methods find large gaps in performance between the non-private and private models. Providing strong theoretical DP guarantees using these methods often results in synthetic data with too little fidelity for use in real-world scenarios. Our framework carefully integrates DP into the motif causality block and each network of the GAN, resulting in a better utility-privacy tradeoff than previous methods.

## 3 Preliminaries

### Motifs

Glucose (and many other) traces can be best understood as sequences of events or _motifs_. Motifs characterize phenomenon in the traces, such as peaks or troughs. We define a _motif_, \(\mu\), as a short, ordered sequence of values (\(v\)) of specified length \(\tau\), \(\mu=[v_{i},v_{i+1},\dots,v_{i+\tau}]\) and \(\sigma\) is a tolerance value to allow approximate matching (within \(\sigma\) for each value). Some examples of glucose traces and motifs are shown in Figure 1. We denote a set of \(n\) time series traces as \(X=[x_{1},...,x_{n}]\). Each time series may be represented as a sequence of motifs: \(x_{i}=[\mu_{i_{1}},\mu_{i_{2}}...]\) where each \(i_{j}\) gives the index of the motif in the set that matches \(x_{i_{j^{\prime}},\tau},...x_{i_{(j+1)},\tau-i}\). Given the motif length \(\tau\), the motif set is the union of all size-\(\tau\) chunks in the traces. This definition is chosen for a straightforward implementation but motifs can be generated in other ways, such as through the use of rolling windows or signal processing techniques [16; 17]. Motifs are pulled from the data such that there is always a match from a trace motif to a motif from the set (if multiple matches, the closest one is chosen).

### Glucose Dynamics (Why Standard Approaches Fail)

We first present a study of the characteristics of glucose data in order to motivate the development of our framework. Although there are general patterns in sequences of glucose motifs (e.g., motif patterns corresponding to patients that eat 2x vs. 3x a day), individual glucose motifs are typically not time-dependent, as illustrated in Figure 2. The radial graphs display the temporal distribution of

Figure 2: Temporal Distributions of Sample Motifs. Each radial graph displays the temporal distribution of a motif; there are 24 radial bars from 00:00 to 23:00, and each segment displays the % of motif occurrences by each hour. Glucose motifs 1 and 2 are from Fig. 1; they are not temporally-dependent and show up across the day. Temporal motifs 1 and 2 are from a cardiology dataset [15].

the first two glucose motifs from Figure 1 and two temporally-dependent motifs from a cardiology dataset [15]. There are 24 radial bars from 00:00 to 23:00 for each hour of the day, and the bar value is the percentage of total motif occurrences at that hour across the entire dataset (i.e., value of 10 would indicate that 10% of the time that motif occurs during that hour). Note that the glucose motifs show up fairly evenly _across_ all hours of the day whereas the motifs from the cardiology dataset have shifts in their distribution and show up frequently at _specific_ hours of the day. The lack of temporal dependence in glucose motifs is likely due to the diverse patient behaviors within a patient population. Glucose in particular is highly variable and influenced by many factors including eating, exercise, stress levels, and sleep patterns. Moreover, due to innate variability within human physiology, motif occurrences can differ even for the _same_ patient across weeks or months. These findings indicate that only conserving the temporal relationships within glucose traces (as many previous methods do) may not be sufficient to properly learn glucose dynamics and output realistic synthetic traces.

### Granger Causality

Granger causality [7] is commonly used to quantify relationships amongst time series without limiting the degree to which temporal relationships may be understood as done in other time series models, e.g., pure autoregressive ones. In this framework, an entire system (set of traces) is studied _together_, allowing for a broader characterization of their relationships, which may be advantageous, especially for long time series. We define \(x_{t}\in\mathbb{R}^{n}\) as an \(n\)-dimensional vector of time series observed across \(n\) traces and \(T\) timesteps. To study causality, a vector autoregressive model (VAR) [18] may be used. A set of traces at time \(t\) is represented as a linear combination of the previous \(K\) lags in the series: \(x_{t}=\sum_{k=1}^{K}A^{(k)}x_{t-k}+e_{t}\) where each \(A^{(k)}\) is a \(n\times n\) dimensional matrix that describes how lag \(k\) affects the future timepoints in the series' and \(e_{t}\) is a zero mean noise. Given this framework, we state that time series \(q\) does not _Granger-cause_ time series \(p\), if and only if for all \(k\), \(A^{(k)}_{p,q}=0\). To better represent nonlinear dynamics amongst traces, a nonlinear autoregressive model (NAR) [19], \(g\), may be defined, in which \(x_{t}=g\left(x_{1_{<t}},...,x_{n_{<t}}\right)+e_{t}\) where \(x_{p_{<t}}=\left(x_{p_{1}}...,x_{p_{t-1}},x_{p_{t}}\right)\) describes the past of series \(p\). The NAR nonlinear functions are commonly modeled jointly using neural networks.

## 4 Motif Causality

Using Granger causality as defined would overwhelm the generator with too much information, resulting in convergence issues for the GAN. Instead of looking at traces comprehensively, we need a way to _scope_ how the generator understands relationships between time series. To this end, we aim to use the same intuition developed from Granger causality, namely developing an understanding of relationships comprehensively using less stringent temporal constraints, but scope these relationships specifically in terms of _motifs_. Therefore, we develop a concept of _motif causality_ which, by learning causal relationships amongst sequences of motifs, allows the generator to learn realistic motif sequences and produce high quality synthetic traces as a result.

### Extending Granger Causality to Motifs

In order to quantify the relationships amongst sequences of motifs to best capture glucose dynamics, we extend the idea of Granger causality to work with motifs. Given a motif set with \(m\) motifs, we build a separate (component) model, called a _motif network_ in our method, for each motif, resulting in \(m\) motif networks. For a single motif \(\mu_{i}\) at time \(t\), \(\mu_{i_{t}}\), we define a function \(g_{i}\) specifying how motifs in previous timesteps are mapped to that motif: \(\mu_{i_{t}}=g_{i}\left(\mu_{1_{<t}},...,\mu_{m_{<t}}\right)+e_{i_{t}}\) where \(\mu_{j_{<t}}=\left(\mu_{j_{1}}...,\mu_{j_{t-1}},\mu_{j_{t}}\right)\) describes the past of motif \(\mu_{j}\). The output of \(g_{i}\) is a vector, which is added to the noise vector \(e_{i_{t}}\). Essentially, we define motif \(\mu_{i}\) in terms of its relationship to past motifs. The \(g_{i}\) function takes in some _mapping_ that describes how motifs in previous timesteps are mapped to the current motif \(\mu_{i_{t}}\). The mapping is not specified in this notation, and could be defined in many different ways. In our case, we instantiate \(g_{i}\) using a single-layer LSTM, described next.

A \(g_{i}\) function for each motif \(\mu_{i}\) in the motif set is modeled using a motif network with a single-layer RNN architecture. For a RNN predicting a single component motif, let \(h_{t}\in\mathbb{R}^{m}\) represent the \(m\)-dimensional hidden state at time \(t\). This represents the historical context of the motifs in the series for predicting a component motif at time \(t\), \(\mu_{i_{t}}\). At time \(t\), the hidden state is updated: \(h_{t}=g_{i}(h_{t-1})+e_{i_{t}}\). \(g_{i}\) here is the function describing how motifs in previous timesteps are mappedto the current motif, and is modeled (instantiated) as a single-layer LSTM as they are good at modeling long, nonlinear dependencies amongst traces [20]. The output for a motif \(\mu_{i}\) at time \(t\), \(\mu_{i_{t}}\) can be obtained by a linear decoding of the hidden state, \(\mu_{i_{t}}=W^{o}h_{t}+e_{i_{t}}\), where \(W^{o}\) is a matrix of the output weights. These weights control the update of the hidden state and thereby control the influence of past motifs on this component motif. Essentially, this function learns a weighting that quantifies how helpful motifs in previous timesteps are for predicting the specified motif \(\mu_{i}\) at time \(t\). We note that we define causality in this way based on how Granger causality models such relationships, which is different from traditional causality models.

If all elements in the \(j\)th column of \(W^{o}\) are zero (\(W^{o}_{:j}=0\)), this is a sufficient condition for an input motif \(\mu_{j}\) being motif non-causal on an output \(\mu_{i}\). Therefore, we can find the motifs that are motif-causal for motif \(\mu_{i}\) using a group lasso penalty optimization across the columns of \(W^{o}\):

\[\min_{W}\sum_{t=2}^{T}(\mu_{i_{t}}-g_{i}(\mu_{0_{<t}},...,\mu_{m_{<t}}))^{2}+ \sum_{j=1}^{m}||W^{o}_{:j}||_{2}\]

We define this as the _local motif loss_, \(\mathcal{L}_{ml}\), which is optimized in each motif network using proximal gradient descent.

### Training the Motif Causality Block

We next describe how the motif causality block is trained to learn motif causal relationships amongst traces, displayed in Figure 3. The block is structured in this way to accommodate the privacy integration (Section 6.2); here, we present its implementation without any privacy noise.

**Partition data.** First, the data is partitioned into \(r\) partitions (Step 1, Figure 3) such that no models are trained on overlapping data. The number of partitions, \(r\), is a user-specified hyperparameter.

**Build motif network for each motif.** Next, within each data partition a set of motif networks is trained. As a pre-processing step, we assume each trace has been chunked into a sequence of motifs of size \(\tau\) (Section 3.1). \(\tau\) is a hyperparameter, which we suggest chosen based on the longest effect time of a trace event. We use \(\tau=48\), corresponding to 4 hours of time, because large glucose events (from behaviors like eating) are encompassed within that time frame; see Appendix B for more details. We assume a tolerance of \(\sigma=2\) mg/dL, chosen to allow for reasonable variations in glucose. To model motif causality for an entire set of data, a \(g_{i}\) function is implemented for each motif via a separate RNN motif net following the description provided previously, resulting in \(m\) total networks (Step 2a, Figure 3). If all the motifs were trained together using a single motif network, it would not be possible to quantify the exact causal effects between each individual motif as we would not know which exact motifs contributed to a prediction (only that there is some combination of unknown motifs that contribute to an accurate prediction for a particular motif). By training each motif network separately, we are able to quantify the exact effect each motif has on each other, without any confounding effects from other motifs.

**Combine outputs of individual motif networks.** Each motif network outputs a vector of weights \(W^{o}\) of dimensionality \(1\times m\), corresponding to the learned causal relationships (Step 2b, Figure 3). Values in the vector are between 0 (no causal relationship) and 1 (strongest causal relationship) and

Figure 3: Motif Causality Block.

give the degree to which every other motif is motif causal of the particular motif \(\mu_{i}\) the RNN was specialized for. To return a complete matrix that summarizes causal relationships amongst _all_ motifs, we stack the weights (Step 2c). The output of each data partition is a complete motif causality matrix, resulting in \(r\) total matrices, each of dimensionality \(m\times m\).

**Aggregate matrices and integrate with GAN.** After motif causality matrices have been outputted from each data partition, the weights in the matrices are aggregated (Step 3, Figure 3) to return the final aggregate causality matrix, \(M\) (Step 4). In the nonprivate version, the weights are averaged. Finally, \(M\) is sent to the generator to help it learn how to conserve motif relationships within sequences of motifs in the synthetically generated data. Details are described next in the subsequent section.

## 5 GlucoSynth

The complete GlucoSynth framework, shown in Figure 4, comprises four key blocks: the motif causality block (explained previously in Section 4), an autoencoder, a generator and a discriminator. We walk through the remaining components of the framework surrounding the GAN next.

### GAN Architecture Components

**Autencoder.** We use an autoencoder (AE) with an RNN architecture to learn a lower dimensional representation of the traces, allowing the generator to better preserve underlying temporal dynamics of the traces. The autoencoder consists of two networks: an _embedder_ and a _recovery network_. The embedder uses an encoding function to map the real data into a lower dimensional space: \(Enc(x):x\in\mathbb{R}^{n}\to x_{e}\in\mathbb{R}^{e}\) while the recovery network reverses this process, mapping the embedded data back to the original dimensional space: \(Dec(x_{e}):x_{e}\in\mathbb{R}^{e}\rightarrow\tilde{x}\in\mathbb{R}^{n}\). A toolboxforted autoencoder perfectly reconstructs the original input data, such that \(x=\tilde{x}\equiv Dec(Enc(x))\). This process yields the Reconstruction Loss, \(\mathcal{L}_{R}\), the Mean Square Error (MSE) between the original data \(x\) and the recovered data, \(\tilde{x}\): \(\mathsf{MSE}(x,\tilde{x})\).

**Generator.** We implement the generator via an RNN or LSTM. Importantly, the generator works in the embedded space, by receiving the input traces passed through the embedder (\(x_{e}\)). To generate synthetic data, a random vector of noise, \(z\) is passed through the generator and then the recovery network to return the synthetic traces in the original dimensional space. To learn how to produce high-quality synthetic data, the generator receives three key pieces of information:

_1 - Stepwise._ The generator receives batches of real data to guide the generation of realistic next step vectors. To do this, a Stepwise Loss, \(\mathcal{L}_{S}\), is computed at time \(t\) using the MSE between the batch of embedded real data, \(x_{ct}\), and the batch of embedded synthetic data, \(\tilde{x}_{et}\): \(\mathsf{MSE}(x_{ct},\tilde{x}_{et})\). This allows the generator to compare (and learn to correct) the discrepancies in stepwise data distributions.

_2 - Motif Causality._ The generator needs to preserve sequences of motifs in addition to temporal dynamics. Using the aggregate causality matrix \(M\) returned from the Motif Causality Block, the generator computes a motif causality matrix, \(M_{\tilde{x}}\), on the set of synthetic data \(\tilde{x}\). Because the original

Figure 4: Overview of GlucoSynth Architecture.

causality matrix was not trained on data in the embedded space, we first run the set of embedded synthetic data through the recovery network \(\hat{x}_{e}\rightarrow\hat{x}\). From there, the Motif Causality Loss, \(\mathcal{L}_{M}\), is computed as the MSE error between the two matrices: \(\mathsf{MSE}(M,M_{\hat{x}})\). These matrices give a causal value of seeing a motif \(\mu_{i}\) in the future after some motif \(\mu_{j}\)-- unrealistic motif sequences will have causal values close to 0. As the generator learns to generate synthetic data that yields a realistic causal matrix (thereby identifying appropriate causal relationships from the motifs), it implicitly learns to not generate unrealistic motif sequences.

_3 - Distributional._ To guide the generator to produce a diverse set of traces, the generator computes a Distributional Loss, \(\mathcal{L}_{D}\), the moments loss (MML), between the overall distribution of the real data \(x_{e}\) and the distribution of the synthetic data \(\hat{x}_{e}\): \(\mathsf{MML}(x_{e},\hat{x}_{e})\). The MML is the difference in the mean and variance of two matrices.

**Discriminator.** The discriminator is a traditional discriminator model using an RNN, the only change being it also works in the embedded space. The discriminator yields the Adversarial Loss Real, \(\mathcal{L}_{Ar}\), the Binary Cross Entropy (BCE) between the discriminator guesses on the real data \(y_{x_{e}}\) and the ground truth \(y\), a vector of 0's, \(\mathsf{BCE}(y_{x_{e}},y)\) and the Adversarial Loss Fake, \(\mathcal{L}_{Af}\), the BCE between the discriminator guesses on the fake data \(y_{\hat{x}_{e}}\) and the ground truth \(y\), a vector of 1's, \(\mathsf{BCE}(y_{\hat{x}_{e}},y)\).

### Training Procedure

First, the motif causality block is trained following the procedure described in Section 4.2, and then the rest of the GAN is trained. The autoencoder is optimized to minimize \(\mathcal{L}_{R}+\alpha\mathcal{L}_{S}\), where \(\alpha\) is a hyperparameter that balances the two loss functions. If the AE only receives \(\mathcal{L}_{R}\) (as is typically done), it becomes overspecialized, i.e., it becomes too good at learning the best lower dimensional representation of the data such that the embedded data are no longer helpful to the generator. For this reason, the AE also receives \(\mathcal{L}_{S}\), enabling the dual training of the generator and embedder. The generator is optimized using \(\min(1-\mathcal{L}_{Af})+\eta(\mathcal{L}_{S}+\mathcal{L}_{D})+\mathcal{L}_{M}\), where \(\eta\) is a hyperparameter that balances the effect of the stepwise and distributional loss. Finally the discriminator is optimized using the traditional adversarial feedback \(\min\mathcal{L}_{Af}+\mathcal{L}_{Ar}\). The networks are trained in sequence (within each epoch) in the following order: autoencoder, generator, then discriminator. In our experiments we set \(\alpha=0.1\) and \(\eta=10\) as they enable GlucoSynth to converge fastest, i.e., in the fewest epochs.

## 6 Providing Differential Privacy

There are two components to our privacy architecture, described in the following two subsections: (1) each network in the GAN (Embedder, Recovery, Generator and Discriminator networks) is trained in a differentially private manner using the Differentially-Private Stochastic Gradient Descent (DP-SGD) algorithm from Abadi et al. [21]; and (2) the motif causality block is trained using the PATE framework from Papernot et al. [22]. Importantly, two completely separate datasets are used for the training of the motif causality block (dataset B in Figure 4) and the GAN (dataset A in Figure 4). We structure the privacy integration in this way to allow for better privacy-utility trade-offs. Our design satisfies the formal differential privacy notion introduced by Dwork et al. [23]. Differential Privacy (DP) provides an intuitive bound on the amount of information that can be learned about any individual in a dataset. A randomized algorithm \(\mathcal{M}\) satisfies \((\epsilon,\delta)\)-differential privacy if, for all datasets \(D_{1}\) and \(D_{2}\) differing by at most a single unit, and all \(S\subseteq\text{Range}(\mathcal{M})\), \(Pr[\mathcal{M}(D_{1})\in S]\leq e^{\epsilon}Pr[\mathcal{M}(D_{2})\in S]+\delta\). The parameters \(\epsilon\) and \(\delta\) determine the _privacy loss budget_, which provide a way to tradeoff privacy and utility; smaller values have stronger privacy. Importantly, privacy is provisioned at the _trace_ level, and we assume each individual has only one trace in the dataset.

### Training the GAN Networks with DP

To add privacy to the GAN components, each of the networks (Embedder, Recovery, Generator and Discriminator) is trained in a differentially private manner using DP-SGD [21]. Although the overall GAN framework is complicated, the individual networks all use simple RNN or LSTM architectures with Adam optimizers. As such, adding DP noise to their network weights is straightforward. We employ the following procedure using Tensorflow Privacy functions [24]. Since there are four networks being trained with DP, we divide the privacy loss budget evenly to get the budget per network, \(\epsilon_{net}=\epsilon/4\). Then, we use Tensorflow's built-in DP accountant to determine how much noise must be added to the weights of each network based on the number of epochs, batch size, number of traces and \(\epsilon_{net}\). This function returns a noise multiplier, which we use when we instantiate a Tensorflow DP Keras Adam Optimizer for each network. Finally, we train each of the networks using their respective DP Keras Adam Optimizer, which automatically trains the network using DP-SGD.

### Training the Motif Causality Block with DP

We train the motif causality block using the PATE framework [22]. PATE provides a way to return aggregated votes about the class a data point belongs to. First, the data is partitioned into \(r\) partitions, where \(r\) is determined based on the size of the dataset and the privacy loss budget. Then, a class membership model is trained independently for each partition. The class membership votes from each partition are aggregated by adding noise to the vote matrix and the noisiest votes are returned using the max-of-Laplacian mechanism (LNMax), tuned based on the privacy budget and \(r\).

We use PATE to train the motif causality block: instead of predicting the degree of class membership we predict _causal_ membership, e.g., does motif \(\mu_{i}\) have a causal relationship to \(\mu_{j}\). The motif causality block is trained in the same procedure described in Section 4.2 with two changes: (1) the number of data partitions, \(r\), is determined based on the privacy budget, instead of a user-specified value; (2) the final causality matrix \(M\) is aggregated using DP across the partitions. In normal PATE, carefully calibrated noise is added to a matrix of votes for each class, such that the classes with the noisiest votes are outputted. In our use, each value in a motif causality matrix may be likened to a class (i.e., causal "class" prediction between motif \(\mu_{i}\) and \(\mu_{j}\)). Thus, we use the LNMax mechanism (from predefined Tensorflow Privacy functions [24]) to aggregate the matrices weights and return \(M\).

We use PATE instead of training each motif network using DP-SGD for better privacy-utility trade-offs. With DP-SGD, we would need to add noise to _every_ motif net, eating up our privacy budget quickly and severely impacting the quality of the returned casuality matrices. PATE allows us to train each of the motif networks without any noise on the gradients, but then aggregates their returned causality matrices in a privacy-preserving manner, resulting in a better privacy-utility trade-off.

## 7 Evaluation

Evaluating synthetic data is notoriously difficult [25], so we provide an extensive evaluation across three criteria. Synthetic data should: 1) conserve characteristics of the real data (_fidelity_, Section 7.1); 2) contain diverse patterns from the real data without the introduction of anomalous patterns (_breadth_, Section 7.2); and 3) be usable in place of the original for real-world use cases (_utility_, Section 7.3).

**Data and Benchmarks.** We use 100,000 single-day glucose traces randomly sampled across each month from January to December 2022, for a total of 1.2 million traces, collected from Dexcom's G6 Continuous Glucose Monitors (CGMs) [26]. Data was recorded every 5 minutes (\(T=288\)) and each trace was aligned temporally from 00:00 to 23:59. We restrict our comparison to the five most closely related state-of-the-art models for generating synthetic univariate time series with no labels or auxiliary data: Three nonprivate--TimeGAN [4], Fourier Flows (FF) [12], non-volume preserving transformations (NVP) [9]; and two private--RGAN [11] and dpGAN [14]. We refer the reader to Appendix B for additional experimental details and all hyperparameter settings, including reasoning behind the choice of motif size \(\tau=48\).

### Fidelity

**Visualization.** We provide visualizations of sample real and synthetic glucose traces from all models. Although this is not a comprehensive way to evaluate trace quality, it does give a snapshot view about what synthetic traces may look like. We provide heatmap visualizations, where each heatmap contains 100 randomly sampled glucose traces. Each row is a single trace from timestep 0 to 288. The values in each row indicate the glucose value (between 40 mg/dL and 400 mg/dL). Figure 5 shows the nonprivate models, and Figures 8, 9, 10 in Appendix C.1 show the private models with different privacy budgets. Upon examining the heatmaps, we notice that GlucoSynth consistently generates realistic looking glucose traces, even at very small privacy budgets.

**Population Statistics.** To evaluate fidelity on a population scale, we compute a common set of glucose metrics and test if the difference between the synthetic and real data is statistically significant. Table 1 provides an abbreviated summary of the results; Appendix C.2 has complete results. GlucoSynth performs the best, with few statistical differences between the real and synthetic data for \(\epsilon\geq 0.1\).

**Distributional Comparisons.** We visualize differences in distributions between the real and synthetic data by plotting the distribution of variances and using PCA [27]. Figure 6 shows the variance distribution for the nonprivate models. Additional comparisons across privacy budgets are available in Appendix C.3. In both nonprivate and private settings, GlucoSynth produces synthetic distributions closest to the real ones, better than all other models.

### Breadth

We quantify breadth in terms of glucose motifs. For each model's synthetic traces, we build a motif set (see Section 3.1). Given a real motif set from the validation traces \(S_{x}\), for each synthetic motif set \(S_{\hat{x}}\), we compute "Validation Motifs", (VM), the fraction of motifs found in the validation motif set that are present in the synthetic motif set, \(\text{VM}/|S_{\hat{x}}|\). This metric quantifies how good our synthetic motif set is (e.g., are its motifs mostly similar to motifs found in real traces). We also compute metrics related to _coverage_, the fraction of motifs in the validation motif set that are found in our synthetic data, defined as \(\text{VM}/|S_{x}|\). This gives a sense of the breadth in a more traditional manner. To compare actual _distributions_ of motifs (not just counts), we compute the MSE between the distribution of real motifs \(S_{x}\) and the distribution of synthetic motifs \(S_{\hat{x}}\). This gives a measure about how close the synthetic motif distribution is to the real one. We want high VM and coverage, and low MSE. Results are in Table 1 with additional analysis in Appendix D; overall our model provides the best breadth.

### Utility

We evaluate our synthetic glucose traces for use in a glucose forecasting task using the common paradigm TSTR (Train on Synthetic, Test on Real), in which the synthetic data is used to train the model and then tested on the real validation data. We train an LSTM network optimized for glucose forecasting tasks [28] and report the Root Mean Square Error (RMSE) in Table 1. We run the experiment 10 times and train the LSTM for 10,000 epochs. We have also tested with other models including RNNs, attention-based models and other LSTM architectures (such as bidirectional LSTMs) but show the results for the best performing model, the LSTM optimized for glucose forecasting. Since RMSE provides a limited view about the model's predictions, we also plot the Clarke Error Grid [29], which visualizes the differences between a predictive and reference measurement, and is a basis for evaluating the safety of diabetes-related medical devices. More details are in Appendix E. GlucoSynth provides the best forecasting results compared to all other models across all privacy budgets.

Figure 5: Heatmaps for Nonprivate Models

[MISSING_PAGE_FAIL:10]

## Acknowledgments and Disclosure of Funding

This work was supported in part by the U.S. National Science Foundation under Grant CCF-1942836, CNS-2213700, CNS-2220433, OAC-2319988 and by a Graduate Research Fellowship under Grant No. 1842490. This work was also supported by a Dexcom Graduate Fellowship.

## References

* [1] Katherine E Britton and Jennifer D Britton-Colonnese. Privacy and security issues surrounding the protection of data generated by continuous glucose monitors. _Journal of Diabetes Science and Technology_, 11(2):216-219, 2017.
* [2] Deborah Young-Hyman, Mary De Groot, Felicia Hill-Briggs, Jeffrey S Gonzalez, Korey Hood, and Mark Peyrot. Psychosocial care for people with diabetes: a position statement of the American Diabetes Association. _Diabetes Care_, 39(12):2126-2140, 2016.
* [3] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. _Communications of the ACM_, 63(11):139-144, 2020.
* [4] Jinsung Yoon, Daniel Jarrett, and Mihaela Van der Schaar. Time-series generative adversarial networks. In _Advances in Neural Information Processing Systems_, 2019.
* [5] Xiaomin Li, Vangelis Metis, Huangyingrui Wang, and Anne Hee Hong Ngu. TTS-GAN: A Transformer-based Time-Series Generative Adversarial Network. In Martin Michalowski, Syed Sibte Raza Abidi, and Samina Abidi, editors, _Artificial Intelligence in Medicine_, pages 133-143, Cham, 2022. Springer International Publishing.
* [6] Amirsina Torfi, Edward A Fox, and Chandan K Reddy. Differentially private synthetic medical data generation using convolutional GANs. _Information Sciences_, 586:485-500, 2022.
* [7] Clive WJ Granger. Investigating causal relations by econometric models and cross-spectral methods. _Econometrica: journal of the Econometric Society_, pages 424-438, 1969.
* [8] Cynthia Dwork. Differential privacy: A survey of results. In _International Conference on Theory and Applications of Models of Computation_. Springer, 2008.
* [9] Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real NVP. _arXiv preprint arXiv:1605.08803_, 2016.
* [10] Eoin Brophy, Zhengwei Wang, Qi She, and Tomas Ward. Generative adversarial networks in time series: A survey and taxonomy. _arXiv preprint arXiv:2107.11098_, 2021.
* [11] Cristobal Esteban, Stephanie L Hyland, and Gunnar Ratsch. Real-valued (medical) time series generation with recurrent conditional GANs. _arXiv preprint arXiv:1706.02633_, 2017.
* [12] Ahmed Alaa, Alex James Chan, and Mihaela van der Schaar. Generative time-series modeling with fourier flows. In _International Conference on Learning Representations_, 2021.
* [13] Liyang Xie, Kaixiang Lin, Shu Wang, Fei Wang, and Jiayu Zhou. Differentially private generative adversarial network. _arXiv preprint arXiv:1802.06739_, 2018.
* [14] Lorenzo Frigerio, Anderson Santana de Oliveira, Laurent Gomez, and Patrick Duverger. Differentially private Generative Adversarial Networks for time series, continuous, and discrete open data. In _IFIP International Conference on ICT Systems Security and Privacy Protection_, 2019.
* [15] Cynthia Binanay, Robert M Califf, Vic Hasselblad, Christopher M O'Connor, Monica R Shah, George Sopko, Lynne W Stevenson, Gary S Francis, Carl V Leier, Leslie W Miller, et al. Evaluation study of congestive heart failure and pulmonary artery catheterization effectiveness: the ESCAPE trial. _JAMA_, 294(13):1625-1633, 2005.
* [16] George EP Box, Gwilym M Jenkins, Gregory C Reinsel, and Greta M Ljung. _Time series analysis: forecasting and control_. John Wiley & Sons, 2015.

* [17] Lexiang Ye and Eamonn Keogh. Time series shapelets: a new primitive for data mining. In _ACM SIGKDD International Conference on Knowledge Discovery and Data Mining_, 2009.
* [18] Helmut Lutkepohl. _New introduction to multiple time series analysis_. Springer Science & Business Media, 2005.
* [19] Stephen A Billings. _Nonlinear system identification: NARMAX methods in the time, frequency, and spatio-temporal domains_. John Wiley & Sons, 2013.
* [20] Yong Yu, Xiaosheng Si, Changhua Hu, and Jianxun Zhang. A review of Recurrent Neural Networks: LSTM cells and network architectures. _Neural computation_, 31(7):1235-1270, 2019.
* [21] Martin Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan, Ilya Mironov, Kunal Talwar, and Li Zhang. Deep learning with differential privacy. In _ACM SIGSAC Conference on Computer and Communications Security_, 2016.
* [22] Nicolas Papernot, Shuang Song, Ilya Mironov, Ananth Raghunathan, Kunal Talwar, and Ulfar Erlingsson. Scalable private learning with PATE. _arXiv preprint arXiv:1802.08908_, 2018.
* [23] Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating noise to sensitivity in private data analysis. In _Theory of Cryptography Conference_, 2006.
* [24] Martin Abadi et al. TensorFlow: Large-scale machine learning on heterogeneous systems. https://www.tensorflow.org/, 2015.
* [25] Ali Borji. Pros and cons of GAN evaluation measures: New developments. _Computer Vision and Image Understanding_, 215:103329, 2022.
* [26] Halis Kaan Akturk, Robert Dowd, Kaushik Shankar, and Mark Derdzinski. Real-world evidence and glycemic improvement using Dexom G6 features. _Diabetes Technology & Therapeutics_, 23(S1):S-21, 2021.
* [27] Fred B Bryant and Paul R Yarnold. Principal-components analysis and exploratory and confirmatory factor analysis. _American Psychological Association_, 1995.
* [28] Taisa Kushner, Marc D Breton, and Sriram Sankaranarayanan. Multi-hour blood glucose prediction in Type 1 diabetes: A patient-specific approach using shallow neural network models. _Diabetes Technology & Therapeutics_, 22(12):883-891, 2020.
* [29] William L Clarke. The original Clarke error grid analysis (EGA). _Diabetes technology & therapeutics_, 7(5):776-779, 2005.
* [30] Open humans. https://www.openhumans.org/, 2023.
* [31] T1D Exchange Registry. https://tidexchange.org/registry/, 2023.
* [32] Juan Miguel Lopez Alcaraz and Nils Strodthoff. Diffusion-based time series imputation and forecasting with structured state space models. _arXiv preprint arXiv:2208.09399_, 2022.
* [33] Hao Ni, Lukasz Szpruch, Magnus Wiese, Shujian Liao, and Baoren Xiao. Conditional Sig-Wasserstein GANs for time series generation. _arXiv preprint arXiv:2006.05421_, 2020.
* [34] Zinan Lin, Alankar Jain, Chen Wang, Giulia Fanti, and Vyas Sekar. Using GANs for sharing networked time series data: Challenges, initial promise, and open questions. In _ACM Internet Measurement Conference_, 2020.
* [35] Mihai Dogariu, Liviu-Daniel Stefan, Bogdan Andrei Boteanu, Claudiu Lamba, Bomi Kim, and Bogdan Ionescu. Generation of realistic synthetic financial time-series. _ACM Transactions on Multimedia Computing, Communications, and Applications_, 18(4):1-27, 2022.
* [36] Mina Razghandi, Hao Zhou, Melike Erol-Kantarci, and Damla Turgut. Variational autoencoder generative adversarial network for synthetic data generation in smart home. _arXiv preprint arXiv:2201.07387_, 2022.

* [37] Debapriya Hazra and Yung-Cheol Byun. SynSigGAN: Generative Adversarial Networks for synthetic biomedical signal generation. _Biology_, 9(12):441, 2020.
* [38] James Jordon, Jinsung Yoon, and Mihaela Van Der Schaar. PATE-GAN: Generating synthetic data with differential privacy guarantees. In _International Conference on Learning Representations_, 2018.

Extended Related Work

We overview related work in three lines of research: time series, conditional time series, and time series methods that employ differential privacy. Table 2 summarizes previous time series synthesis methods. We note that there have been exciting developments for adjacent research tasks (data augmentation, forecasting) such as diffusion models [32], but there are not yet any publicly available models specifically for the generation of complete synthetic time series datasets. As such, we focus the scope of our comparison on the current state-of-the-art methods for synthetic time series which all build upon Generative Adversarial Networks (GANs) [3] and transformation-based approaches [9]. In particular TimeGAN [4], RGAN [11] and dpGAN [14] are most similar to ours and used as benchmarks in the evaluation in Section 7.

**Time Series.** There have been promising models to generate synthetic time series across a variety of domains such as financial data [35], cyber-physical systems (e.g., smart homes [36]), and medical signals [37]. Brophy et al. [10] provides a survey of GANs for time series synthesis. TimeGan [4] is a popular benchmark that jointly learns an embedding space using supervised and adversarial objectives in order to capture the temporal dynamics amongst traces. TTS-GAN [5], trains a GAN model that uses a transformer encoding architecture in order to best preserve temporal dynamics. Transformation-based approaches have also had success for time series data. Real-valued non-volume preserving transformations (NVP) [9] model the underlying distribution of the real data using generative probabilistic modeling and use this model to output a set of synthetic data. Similarly, Fourier Flows (FF) [12] transform input traces into the frequency domain and output a set of synthetic data from the learned spectral representation of the original data. Methods that only focus on learning the temporal or distributional dynamics in time series are not sufficient for generating _realistic_ synthetic glucose traces due to the lack of temporal dependence within sequences of glucose motifs.

**Conditional Time Series.** Many works have developed time series models that supplement their training using extra features or conditional data. Esteban et al. [11] develops two GAN models (RGAN/RCGAN) with RNN architectures, conditioned on auxiliary information provided at each timestep during training. SigCWGAN [33] uses a mathematical conditional metric (\(Sig-W_{1}\)) characterizing the signature of a path to capture temporal dependence of joint probability distributions in long time series data. However, our glucose traces do not have any additional information available so these methods cannot be used1.

Footnote 1: There is a caveat here that RGAN does not use auxillary information, hence why we compare with it in our benchmarks.

**Differentially-Private GANs.** To protect sensitive data, several GAN architectures (DP GANs) have been designed to incorporate privacy-preserving noise needed to satisfy differential privacy guarantees [13]. Although DP GANs such as PateGAN [38] have had great success for other data types and learning tasks (e.g., tabular data, supervised classification tasks), results have been less satisfactory in DP GANs developed for time series.

RGAN/RCGAN [11] also includes a DP implementation, but the authors find large gaps in performance between the nonprivate and private models. Frigerio et al. [14] extends a simple DP GAN architecture (denoted dpGAN) to to time-series data. The synthetic data from their private model

\begin{table}
\begin{tabular}{c|c|c|c|c} \hline \hline
**Name** & **Private?** & **No Labels Required?** & **No CI*?** & **Length** \\ \hline TimeGAN [4] & x & ✓ & ✓ & 24 - 58 \\ TTS-GAN [5] & x & x & ✓ & 24 - 150 \\ SigCWGAN [33] & x & ✓ & x & 80,000 \\ RGAN [11] & ✓ & ✓ & ✓ & 16 - 30 \\ RCGAN [11] & ✓ & ✓ & x & 16 - 30 \\ dpGAN [14] & ✓ & ✓ & ✓ & 96 \\ RDP-CGAN [6] & ✓ & ✓ & x & 2 - 4097 \\ DoppelGANger [34] & ✓ & ✓ & x & 50 - 600 \\ GlucoSynth (Ours) & ✓ & ✓ & ✓ & 288 \\ \hline \hline \end{tabular}
\end{table}
Table 2: Summary of Previous Methods for Time Series Synthesis. *CI = conditional information or extra featuresconserves the distribution of the real data but loses some of the variability (diversity) from the original samples. RDP-CGAN [6] develops a convolutional GAN architecture that uses Renyi differential privacy specifically for medical data. Across different datasets, they find that reasonable privacy budgets result in major drops in the performance of the synthetic data. Finally, DoppelGANger [34] develops a temporal GAN framework for time series with metadata and perform an in-depth privacy evaluation. Notably, they find that providing strong theoretical DP guarantees results in destroying the fidelity of the synthetic data, beyond anything feasible for use in real-world scenarios. Each of these methods touches on the innate challenge of generating DP synthetic time series due to very high tradeoffs between utility and privacy. Our DP framework uses two different methods to integrate privacy into our GAN architecture, resulting in a better utility-privacy trade-off than previous methods.

## Appendix B Additional Experimental Details

**Note on Data Use.** As explained in the approach (Section 5), our model uses two _separate_ datasets for the training of the motif causality block and the rest of the GAN. As such, we used two different samples of glucose traces with no overlap between patients for the training of each section (meaning we actually used a total of 2.4 million traces across the entire model). We also note that we have received the proper ethical and legal consent from the individuals to use their data in this way (and for this purpose).

**Hyperparameters.** Our experiments were completed in the Google Cloud platform on an Intel Skylake 96-core cpu with 360 GB of memory. We use a separate validation dataset (not the set of original training traces) for all experimental results. Throughout all our experiments we use GlucoSynth model parameters of \(\alpha=0.1\) and \(\eta=10\) and a motif tolerance of \(\sigma=2\) mg/dL and motif length \(\tau=48\). Motif length of 48 timesteps is equivalent to 4 hours of time and represents a clinically significant threshold. This threshold was chosen because the effect of any behaviors on glucose occur within 4 hours of the event (e.g., the effect from eating a meal - a rise in glucose - will occur within 4 hours after eating.) We note that other choices for \(\tau\) could be used, based on what types of phenomenon the users wish to replicate; for example, to capture day/night glucose rhythm effects, we suggest a \(\tau\) of 144, corresponding to 12 hours of time.

We vary \(\epsilon\) in our privacy experiments, but keep \(\delta\) the same at \(5e{-4}\). Importantly, in order to meet our privacy guarantees, we assume that privacy is provisioned at the trace level and each individual has only one trace in the dataset. The motif set is derived separately from the training data (either from a public dataset or generated based on knowledge about the underlying data, e.g., the possible glucose motif combinations), so as not to effect the differential privacy guarantees or use up any privacy budget. In our case, we assume the motif set is all-encompassing and generated from the universe of possible motifs, resulting in \(m=5,977,610\) total motifs in the motif set.

**Benchmark Details.** TimeGAN [4] is implemented from www.github.com/jsyoon0823/TimeGAN; Fourier Flows (FF) [12] are implemented from www.github.com/ahmedmalaa/Fourier-flows; RGAN [11] is implemented from www.github.com/ratschlab/RGAN; and DPGAN [14] is adapted from www.github.com/SAP-samples/security-research-differentially-private-generative-models. All the benchmarks were trained according to

Figure 7: Example motif causality matrix for a small motif set (\(m=10\)). Each value in the grid is between 0 and 1. 0 indicates no motif-causal relationship, and 1 indicates the strongest motif causal relationship.

their suggested parameters, with most models trained for 10,000 epochs. We note that we trained for more than the suggested epochs (50,000 instead of 10,000) and tried many additional hyperparameter settings for RGAN to attempt to improve its performance and provide the fairest comparison possible.

## Appendix C Additional Evaluation: Fidelity

### Visualizations

We provide heatmap visualizations of sample real and synthetic glucose traces from all the models. Although this is not a comprehensive way to evaluate trace quality, it does give a snapshot view about how the synthetic traces compare to the real ones. Each heatmap contains 100 randomly sampled

Figure 8: Heatmaps for GlucoSynth Across Different Privacy Budgets

Figure 10: Heatmaps for dpGAN Across Different Privacy Budgets

Figure 9: Heatmaps for RGAN Across Different Privacy Budgets

[MISSING_PAGE_FAIL:17]

[MISSING_PAGE_FAIL:18]

Figure 12: GlucoSynth Distributional Variance Comparison Across Privacy Budgets

Figure 13: GlucoSynth PCA Comparison Across Privacy Budgets

Figure 14: RGAN distributional Variance Comparison Across Privacy Budgets

Figure 15: RGAN PCA Comparison Across Privacy Budgets

Figure 16: dpGAN distributional Variance Comparison Across Privacy Budgets

Figure 17: dpGAN PCA Comparison Across Privacy Budgets

## Appendix E Additional Evaluation: Utility

Since RMSE may provide a limited view about the predictions from the glucose forecasting model, we also plot the Clarke Error Grid [29], which visualizes the differences between a predictive measurement and a reference measurement, and is the basis used for evaluation of the safety of diabetes-related medical devices (for example, used for evaluating glucose outputs from predictive models integrated into artificial insulin delivery systems). The Clarke Error Grid is implemented using www.github.com/suetAndTie/ClarkeErrorGrid. The grids are shown in Figure 18.

In the figures, the x-axis is the reference value and the y-axis is the prediction. A diagonal line means the predicted value is exactly the same as the reference value (the best case). There are 5 total zones that make up the grid, listed in order from best to worst:

* Clinically Accurate: Predictions differ from actual values by no more than 20% and lead to clinically correct treatment decisions.
* Clinically Acceptable: Predictions differ from actual values by more than 20% but would not lead to any treatment decisions.
* Overcorrections: Acceptable glucose levels would be corrected (overcorrection).
* Failure to Detect: Predictions lie within the acceptable range but the actual values are outside the acceptable range, resulting in a failure to detect and treat errors in glucose.
* Erroneous Treatment: Predictions are opposite the actual values, resulting in erroneous treatment, opposite of what is clinically recommended.

We show Clarke Error grids for all models (and the private models with no privacy included, \(\epsilon=\infty\)). This is because comparing the models at different privacy budgets is not very informative - it can be hard to tell exactly where changes between different budgets may occur. We also present a table with the percentages of predicted datapoints in each category in Table 5. This table includes a comparison among different privacy budgets for the private models (much more effective than the figures by themselves.)

Figure 18: Clarke Error Zone Figures for All Models

[MISSING_PAGE_FAIL:23]