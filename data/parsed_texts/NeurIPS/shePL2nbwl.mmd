# Survival Instinct in Offline Reinforcement Learning

 Anqi Li

University of Washington

&Dipendra Misra

Microsoft Research

&Andrey Kolobov

Microsoft Research

&Ching-An Cheng

Microsoft Research

###### Abstract

We present a novel observation about the behavior of offline reinforcement learning (RL) algorithms: on many benchmark datasets, offline RL can produce well-performing and safe policies even when trained with "wrong" reward labels, such as those that are zero everywhere or are negatives of the true rewards. This phenomenon cannot be easily explained by offline RL's return maximization objective. Moreover, it gives offline RL a degree of robustness that is uncharacteristic of its online RL counterparts, which are known to be sensitive to reward design. We demonstrate that this surprising robustness property is attributable to an interplay between the notion of _pessim_ in offline RL algorithms and certain implicit biases in common data collection practices. As we prove in this work, pessimism endows the agent with a _survival instinct_, i.e., an incentive to stay within the data support in the long term, while the limited and biased data coverage further constrains the set of survival policies. Formally, given a reward class -- which may not even contain the true reward -- we identify conditions on the training data distribution that enable offline RL to learn a near-optimal and safe policy from any reward within the class. We argue that the survival instinct should be taken into account when interpreting results from existing offline RL benchmarks and when creating future ones. Our empirical and theoretical results suggest a new paradigm for offline RL, whereby an agent is "nudged" to learn a desirable behavior with imperfect reward but purposely biased data coverage. Please visit our website https://survival-instinct.github.io for accompanied code and videos.

## 1 Introduction

In offline reinforcement learning (RL), an agent optimizes its performance given an offline dataset. Despite being its main objective, we find that return maximization is not sufficient for explaining some of its empirical behaviors. In particular, _in many existing benchmark datasets, we observe that offline RL can produce surprisingly good policies even when trained on utterly wrong reward labels_.

In Fig. 1, we present such results on the hopper task from D4RL [1], a popular offline RL benchmark, using a state-of-the-art offline RL algorithm ATAC [2]. The goal of an RL agent in the hopper task is to move forward as fast as possible while avoiding falling down. We trained ATAC agents on the original datasets and on three modified versions of each dataset, with "wrong" rewards: _1) zero_: assigning a zero reward to all transitions, _2) random_: labeling each transition with a reward sampled uniformly from \([0,1]\), and _3) negative_: using the negation of the true reward. Although these wrong rewards contain no information about the underlying task or are even misleading, the policies learned from them in Fig. 1 (left) often perform significantly better than the behavior (data collection) policy and the behavior cloning (BC) policy. They even outperform policies trained with the true reward (denoted as _original_ in Fig. 1) in some cases. This is puzzling, since RL is notorious for being sensitive to reward mis-specification [3, 4, 5, 6]: in general, maximizing the wrong rewards with RL leads to sub-optimal performance that can be worse than simply performing BC. In addition, these wrong-reward policies demonstrate a "safe" behavior, which keeps the hopper from falling down for a longer period than other comparators in Fig. 1 (right). This is yet another peculiarity hard to link toreturn maximization, as none of the wrong rewards encourage the agent to stay alive. As we will show empirically in Section 4, these effects are not unique to ATAC or the hopper task. They occur with multiple offline RL algorithms, including ATAC [2], PSPI [7], IQL [8], CQL [9] and the Decision Transformer (DT) [10], on dozens of datasets from D4RL [1] and Meta-World [11] benchmarks.

This robustness of offline RL is not only counter-intuitive but also cannot be explained fully by the literature. Offline RL theory [2, 7, 12, 13] provides performance guarantees only when the data reward is the true reward. Although offline imitation learning (IL) [14, 15, 16] makes no assumptions about reward, it only shows that the learned policy can achieve performance comparable to that of the behavior policy, not beyond. Robust offline RL [17, 18, 19, 20, 21] shows that specialized algorithms can perform well when the size of data perturbation is small. In contrast, we demonstrate that standard offline RL algorithms can produce good policies even when we completely change the reward. Constrained offline RL algorithms [22, 23, 24] can learn safe behaviors when constraint violations are both explicitly labeled and optimized. However, in the phenomena we observe, no safety signal is given to off-the-shelf, unconstrained offline RL algorithms. Recently, [25] observes a similar robustness phenomena of offline RL, but does not provide a complete explanation.1 In Appendix B, we discuss the gap between the related work and our findings in more detail.

Footnote 1: We learned about this paper after the submission. Our theoretical results not only validate their conjecture on the expert dataset, but also predict how this robustness can happen in data collected by suboptimal policies.

In this work, we provide an explanation for this seemingly surprising observation. In theory, we prove that this robustness property is attributed to the interplay between the use of pessimism in offline RL algorithms and the implicit bias in typical data collection processes. Offline RL algorithms often use pessimism to avoid taking actions that lead to unknown future events. We show that this risk-averse tendency bakes a _"survival instinct"_ into the agent, an incentive to stay within the data coverage in the long term. On the other hand, the limited coverage of offline data further constrains the set of _survival policies_ (policies that remain in the data support in the long term). When this set of survival policies correlates with policies that achieve high returns w.r.t. the true reward (as in the example in Fig. 1), robust behavior emerges.

Our theoretical and empirical results have two important implications. First and foremost, offline RL has a survival instinct that leads to inherent robustness and safety properties that online RL does not have. Unlike online RL, offline RL is _doubly robust_: as long as the data reward is correct or the data has a positive implicit bias, a pessimistic offline RL agent can perform well. Moreover, offline RL is safe as long as the data only contains safe states; thus safe RL in the offline setup can be achieved without specialized algorithms. Second, because of the existence of the survival instinct, the data coverage has a profound impact on offline RL. While a large data coverage improves the best policy that can be learned by offline RL with the true reward, it can also make offline RL more sensitive to imperfect rewards. In other words, collecting a large set of diverse data might not be necessary or helpful (see Section 4). This goes against the common wisdom in the RL community that data should be as exploratory as possible [26, 27, 28].

We emphasize that survival instinct's implications should be taken into account when interpreting results on existing offline RL benchmarks as well as when designing future ones. We should treat the evaluation of offline RL algorithms differently from online RL algorithms. We suggest evaluating the performance of an offline RL algorithm by training it with wrong rewards in addition to the true reward so that we can isolate the performance due to return maximization from the compounded effects of survival instinct and implicit data bias. We propose to use this performance as a score to _quantify_ the data bias (see Eq. (3) in Section 4.1) in practice.

Figure 1: On the hopper task from D4RL [1], ATAC [2], an offline RL algorithm, can produce high-performance and safe policies even when trained on wrong rewards.

We believe that our findings shed new light on RL applicability and research. To practitioners, we demonstrate that offline RL does not always require the correct reward to succeed. This opens up the possibility of using offline RL in domains where obtain high-quality rewards is challenging. Research-wise, the existence of the survival instinct raises the question of how to design data collection or data filtering procedures that would help offline RL to leverage this instinct in order to improve RL's performance with incorrect or missing reward labels. While in this paper we focus on positive data biases, we caution that in practice the data bias might be negatively correlated with a user's intention. In that case, running offline RL even with the right reward would not lead to the right behavior.

## 2 Background

The goal of offline RL is to solve an unknown Markov decision process (MDP) from offline data. Typically, an offline dataset is a collection of tuples, \(\mathcal{D}\coloneqq\{(s,a,r,s^{\prime})|(s,a)\sim\mu(\cdot,\cdot),r=r(s,a),s^ {\prime}\sim P(\cdot|s,a)\}\), where \(r\) is the reward, \(P\) captures the MDP's transition dynamics, and \(\mu\) denotes the state-action data distribution induced by some data collection process. Modern offline RL algorithms adopt pessimism to address the issue of policy learning when \(\mu\) does not have the full state-action space as its support. The basic idea is to optimize for a performance lower bound that penalizes actions leading to out-of-support future states. Such a penalty can take the form of behavior regularization [8; 29; 30], negative bonuses to discourage visiting less frequent state-action pairs [12; 31; 32], pruning less frequent actions [13; 33], adversarial training [2; 7; 14; 34; 35] or value penalties in modified dynamic programming [9; 36].

We are interested in settings where the offline RL agent learns not from \(\mathcal{D}\) itself but from its corrupted version \(\tilde{\mathcal{D}}\) with a wrong reward \(\tilde{r}\), i.e., \(\tilde{\mathcal{D}}\coloneqq\{(s,a,\tilde{r},s^{\prime})|(s,a,s^{\prime})\in \mathcal{D},\tilde{r}=\tilde{r}(s,a)\in[-1,1]\}\). We assume \(\tilde{r}\) is from a reward class \(\tilde{\mathcal{R}}\) -- which may not necessarily contain the true reward \(r\) -- and we wish to explain why offline RL can learn good behaviors from the corrupted dataset \(\tilde{\mathcal{D}}\) (e.g., as in Fig. 1). To this end, we introduce notations and assumptions we will use in the paper.

NotationWe focus on the setting of infinite-horizon discounted MDPs. We denote the task MDP that the agent aims to solve as \(\mathcal{M}=(\mathcal{S},\mathcal{A},r,P,\gamma)\), where \(\mathcal{S}\) is the state space, \(\mathcal{A}\) is the action space, and \(\gamma\in[0,1)\) is the discount. Without loss of generality, we assume \(r:\mathcal{S}\times\mathcal{A}\rightarrow[0,1]\). Let \(\Delta(\mathcal{U})\) denote the set of probability distributions over a set \(\mathcal{U}\). We denote a policy as \(\pi:\mathcal{S}\rightarrow\Delta(\mathcal{A})\). For a reward function \(r\), we define a policy \(\pi\)'s state value function as \(V_{r}^{\pi}(s)\coloneqq\mathbb{E}_{\pi,P}[\sum_{t=0}^{\infty}\gamma^{t}r(s_{t },a_{t})|s_{0}=s]\) and the state-action value function as \(Q_{r}^{\pi}(s,a)\coloneqq r(s,a)+\gamma\mathbb{E}_{s^{\prime}\sim P(\cdot|s,a )}[V_{r}^{\pi}(s^{\prime})]\). Solving MDP \(\mathcal{M}\) requires learning a policy that maximizes the return at an initial state distribution \(d_{0}\in\Delta(\mathcal{S})\), that is, \(\max_{\pi}\mathbb{E}_{s\sim d_{0}}[V_{r}^{\pi}(s)]\). We denote the optimal policy as \(\pi^{*}\) and the optimal value functions as \(V_{r}^{*}\) and \(Q_{r}^{*}\). Given \(d_{0}\), we define the average state-action visitation of a policy \(\pi\) as \(d^{\pi}(s,a)\coloneqq(1-\gamma)\mathbb{E}_{\pi,P}[\sum_{t=0}^{\infty}\gamma^{t }d_{t}^{\pi}(s,a)]\), where \(d_{t}^{\pi}(s,a)\) denotes the probability of visiting \((s,a)\) at time \(t\) when running \(\pi\) starting at an initial state sampled from \(d_{0}\). Note that \((1-\gamma)\mathbb{E}_{s\sim d_{0}}[V_{r}^{\pi}(s)]=\mathbb{E}_{(s,a)\sim d^{ \pi}}[r(s,a)]\). For a function \(f:\mathcal{S}\times\mathcal{A}\rightarrow\mathbb{R}\), we use the shorthand \(f(s,\pi)=\mathbb{E}_{a\sim\pi|s}[f(s,a)]\); similarly for \(f:\mathcal{S}\rightarrow\mathbb{R}\), we write \(f(p)=\mathbb{E}_{s\sim p}[f(s)]\), e.g., \(V_{r}^{\pi}(d_{0})=\mathbb{E}_{s\sim d_{0}}[V_{r}^{\pi}(s)]\). For a distribution \(p\), we use \(\text{supp}(p)\) to denote its support.

AssumptionWe make the typical assumption in offline RL that the data distribution \(\mu\) assigns positive probabilities to these state-actions visited by running the optimal policy \(\pi^{*}\) starting from \(d_{0}\).

**Assumption 1** (Single-policy Concentrability).: We assume \(\sup_{s\in\mathcal{S},a\in\mathcal{A}}\frac{d^{*^{*}}(s,a)}{\mu(s,a)}<\infty\).

This is a standard assumption in the offline RL literature, which in the worst case is a necessary condition to have no regret w.r.t. \(\pi^{*}\)[37]. There are generalized notions [7; 34] of this kind, which are weaker but requires other smoothness assumptions. We note that Assumption 1 does not assume that \(\mu\) is the average state-action visitation frequency of a single behavior policy, nor does it assume \(\mu\) has a full coverage of all states and actions or all policy distributions [27; 38].

## 3 Why Offline RL can Learn Right Behaviors from Wrong Rewards

In this section, we provide conditions under which offline RL's aforementioned robustness w.r.t. misspecified rewards emerges. Our main finding is summarized in the theorem below.

**Theorem 1**.: _(Informal) Under Assumption 1 and certain regularity assumptions, if an offline RL algorithm \(Algo\) is set to be sufficiently pessimistic and the data distribution \(\mu\) has a positive bias, for any data reward \(\tilde{r}\in\tilde{\mathcal{R}}\), the policy \(\hat{\pi}\) learned by \(Algo\) from the dataset \(\tilde{\mathcal{D}}\)_has performance guarantee \(V_{r}^{\pi^{*}}(d_{0})-V_{r}^{\pi}(d_{0})\leq O(\iota)\) as well as safety guarantee \((1-\gamma)\sum_{\iota=0}^{\infty}\gamma^{t}\mathrm{Prob}\left(\exists\tau\in[0,t],s_{\tau}\notin\text{supp}(\mu)|\hat{\pi}\right)\leq O(\iota)\) for a small \(\iota\) that decreases to zero as the degree of pessimism and dataset size increase._

In other words, the robustness originates from an _interplay_ between pessimism and an implicit positive bias in data. Here is the insight on why Theorem 1 is true. Because of pessimism, offline RL endows agents with a "survival instinct" -- it implicitly solves a constrained MDP (CMDP) problem [39] that enforces the policy to stay within the data support. When combined with a training data distribution that has a positive bias (e.g., all policies staying within data support are near-optimal) such a survival instinct results in robustness to reward misspecification.

Overall, Theorem 1 has two important implications:

1. Offline RL is _doubly robust_: it can learn near optimal policies so long as either the reward label is correct or the data distribution is positively biased;
2. Offline RL is _intrinsically safe_ when data are safe, regardless of reward labeling, without the need of explicitly modeling safety constraints.

In the remaining of this section, we provide details and discussion of the statements above. The complete theoretical statements and proofs can be found in Appendix D.

### Intuitions for Survival Instinct and Positive Data Bias

We first use a grid world example to build some intuitions. Fig. 2 shows a goal-directed problem, where the true reward is +1 and -1 upon touching the key and the lava, respectively. The offline data is suboptimal and does not have full support. All data trajectories that touched the lava were stopped early, while others were allowed to continue until the end of the episode. The goal state (key) is an absorbing state, where the agent can stay _forever_ beyond the episode length. We use the wrong rewards in Fig. 1 to train PEVI [12], a finite-horizon, tabular offline RL method. PEVI performs dynamic programming similar to value iteration, but with a pessimistic value initialization and an instantaneous pessimism-inducing penalty of \(O(-^{1}\sqrt{n(s,a)})\), where \(n(s,a)\) is the empirical count in data. The penalties ensure that the learned value lower bounds the true one.

We see the PEVI agent learned with any wrong reward in Fig. 1 is able to solve the problem despite data imperfection, while the BC agent that mimics the data directly fails. The main reasons are: _1)_ There is a _data bias_ whereby longer trajectories end closer to the goal (especially, the longest trajectories are the ones that reach the goal, since the goal is an absorbing state). We call this a _length bias_ (see Section 3.3). This positive data bias is due to bad trajectories (touching the lava or not reaching the goal) being cut short or timing out. _2)_ Pessimism in PEVI gives the agent an _algorithmic bias_ (i.e., survival instinct) that favors longer data trajectories. Because of the pessimistic value initialization, PEVI treats trajectories shorter than the episode length as having the lowest return. As a result, by maximizing the pessimistically estimated values, PEVI learns good behaviors despite wrong rewards by leveraging the survival instinct and positive data bias _together_. We now make this claim more general.

### Survival Instinct

Survival instinct is a pessimism induced behavior that offline RL algorithms tend to favor policies leading to _in-support trajectories_. We formally characterize this risk aversion behavior by the concept of constrained MDP (CMDP) [39], which we define below.

**Definition 1**.: Let \(f,g:\mathcal{S}\times\mathcal{A}\rightarrow[-1,1]\). A _CMDP_\(\mathcal{C}(\mathcal{S},\mathcal{A},f,g,P,\gamma)\) is a constrained optimization problem: \(\max_{\pi}V_{f}^{\pi}(d_{0})\) s.t. \(V_{g}^{\pi}(d_{0})\leq 0\). Let \(\pi^{\dagger}\) denote its optimal policy. For \(\delta\geq 0\), we define the set of \(\delta\)_-approximately optimal policies \(\Pi^{\dagger}_{f,g}(\delta)\coloneqq\{\pi:V_{f}^{\pi^{\dagger}}(d_{0})-V_{f}^{ \pi}(d_{0})\leq\delta,V_{g}^{\pi}(d_{0})\leq\delta\}\)._

We prove that when trained on \(\tilde{D}\), offline RL, because of its pessimism, implicitly solves the CMDP below even when the algorithm does not explicitly model any constraints:

\[\mathcal{C}_{\mu}(\tilde{r})\coloneqq\mathcal{C}(\mathcal{S},\mathcal{A}, \tilde{r},c_{\mu},P,\gamma),\] (1)

where \(c_{\mu}(s,a)\coloneqq\mathbbm{1}[\mu(s,a)=0]\) indicates whether \((s,a)\) is out of the support of \(\mu^{2}\) i.e., the

Figure 2: A grid world. BC (red); offline RL with wrong rewards (blue). The opacity indicates the frequency of a state in the data (more opaque means more frequent). Offline RL with the three wrong rewards produces the same policy.

constraint in Eq. (1) enforces the agent's trajectories to stay within the data support. Note the constraint in this CMDP is feasible because of Assumption 1.

**Proposition 1** (Survival Instinct).: _(Informal) Under certain regularity conditions on \(\mathcal{C}_{\mu}(\tilde{r})\), the policy learned by an offline RL algorithm \(\mathit{Algo}\) with the offline dataset \(\tilde{D}\) is \(\iota\)-approximately optimal with respect to \(\mathcal{C}_{\mu}(\tilde{r})\), for some small \(\iota\) which decreases as the algorithm becomes more pessimistic._

Proposition 1 says if an offline RL algorithm is sufficiently pessimistic, then the learned policy is approximately optimal to \(\mathcal{C}_{\mu}(\tilde{r})\). The learned policy has not only small regret with respect to the data reward \(\tilde{r}\), but also small chances of escaping the support of the data distribution \(\mu\).

We highlight that the survival instinct in Proposition 1 is a _long-term_ behavior. Such a survival behavior cannot be achieved by myopically taking actions in the data support in general (such as BC).3 For instance, when some trajectories generating the data are truncated (e.g., due to early-stopping or intervention for safety reasons), taking in-support actions may still lead to out-of-support states in the future, as the BC agent in Section 3.1.

Footnote 2: The use of an indicator function is not crucial here. We can extend the current analysis here to other costs that are zero in the support and strictly positive out of the support.

Footnote 3: BC requires a stronger condition, e.g., the data are all complete trajectories without intervention or timeouts.

Proof SketchThe key to prove Proposition 1 is to show that an offline RL algorithm by pessimism has small regret for not just \(\tilde{r}\) but a set of reward functions consistent with \(\tilde{r}\) on data but different outside of data, including the Lagrange reward of (1) (i.e., \(\tilde{r}-\lambda c_{\mu}\), with a large enough \(\lambda\geq 0\)). We call this property admissibility and we prove in Appendix F that many existing offline RL algorithms are admissible, including model-free algorithms ATAC [2], VI-LCB [31] PPI/PQI [13], PSPI [7], as well as model-algorithms, ARMOR [34, 40], MOPO [32], MOReL [33], and CPPO [14]. By Proposition 1 these algorithms have survival instinct. Please see Appendix D for details.

### Positive Data Bias

In addition to survival instinct, another key factor is an implicit positive bias in common offline datasets. Typically these data manifest meaningful behaviors. For example, in collecting data for goal-oriented problems, data recording is normally stopped if the agent fails to reach the goal within certain time limit. Another example is problems (like robotics or healthcare) where wrong decisions can have detrimental consequences, i.e., problems that safe RL studies. In these domains, the data are collected by qualified policies only or under an intervention mechanism to prevent catastrophic failures. Such a one-sided bias can creates an effect that staying within data support would lead to meaningful behaviors. Below we formally define the positive data bias; Later in Section 4 (Fig. 5), we provide empirical estimates of the degree of positive data bias.

**Definition 2** (Positive Data Bias).: A distribution \(\mu\) is \(\frac{1}{\epsilon}\)_-positively biased_ w.r.t. a reward class \(\tilde{\mathcal{R}}\) if

\[\max_{\tilde{r}\in\tilde{\mathcal{R}}}\max_{\pi\in\Pi^{\dagger}_{\ell,c_{\mu }}(\delta)}V_{r}^{\pi^{*}}(d_{0})-V_{r}^{\pi}(d_{0})\leq\epsilon+O(\delta)\] (2)

for all \(\delta\geq 0\), where \(\Pi^{\dagger}_{\tilde{r},c_{\mu}}(\delta)\) denotes the set of \(\delta\)-approximately optimal policy of \(\mathcal{C}_{\mu}(\tilde{r})\).

We measure the degree of positive data bias based on how bad a policy can perform in terms of the true reward \(r\) when approximately solving the CMDP \(\mathcal{C}_{\mu}(\tilde{r})\) in (1) defined with the wrong reward \(\tilde{r}\in\tilde{\mathcal{R}}\). If the data distribution \(\mu\) is \(\infty\)-positively biased, then any approximately optimal policy to \(\mathcal{C}_{\mu}(\tilde{r})\) (which includes the policies learned by offline RL, as shown earlier) can achieve high return in the true reward \(r\). We also can view the degree of positiveness as reflecting whether \(\tilde{r}\) provides a similar ranking as \(r\), _among policies within the support of \(\mu\)_. When there is a positive bias, offline RL can learn with \(\tilde{r}\) to perform well under \(r\), even when \(\tilde{r}\) is not aligned with \(r\) globally. This is in contrast to online RL, which requires global alignment due to the exploratory nature of online RL.

ExamplesWe provide a few examples of positive data bias. (See Appendix D.4 for proofs.)

* The distribution \(d^{\pi}\) induced by any policy \(\pi\) is \(\infty\)-positively biased for any rewards resulting from potential-based reward shaping [41] since it provides the same ranking for all policies.
* For an IL setup, the data distribution \(\mu\) is \(\infty\)-positively biased for any rewards \(\tilde{r}:\mathcal{S}\times\mathcal{A}\rightarrow\mathbb{R}\) if the data is generated by the optimal policy (i.e., \(\mu=d^{\pi^{*}}\)). This is because following the optimal policy is the only way to stay within the data support. In Section 4, we empirically show that offline RL algorithms can learn good policies with wrong rewards on D4RL expert datasets, which are collected by near-optimal policies.
* A positive bias happens when longer trajectories in the data have smaller optimality gap. This is a generalized formal definition of the _length bias_ mentioned in Section 3.1. This condition is typically satisfied when intervention is taken in the data collection process (despite the data collection policy being suboptimal), as in the motivating example in Fig. 1. Later in Section 4, we will investigate deeper into this kind of length bias empirically.

RemarkWe highlight that the positive data bias assumption in Definition 2 is _different_ from assuming that the data is collected by expert policies, which is typical in the IL literature. Positive data bias assumption can hold in cases when data is generated by highly suboptimal policies, which we observe in the hopper example from Section 1. On the other hand, there are also cases where IL can learn well, while positive data bias does not exist (e.g., when learning from data collected by a stochastic expert policy that covers highly suboptimal actions with small probabilities).

### Summary: Offline RL is Doubly Robust and Intrinsically Safe

We have discussed the survival instinct from pessimism and the potential positive data bias in common datasets. In short, survival instinct enables offline RL algorithms to learn policies that benefit from a favorable inductive bias of staying within the support of a positive data distribution. As a result, offline RL becomes robust to reward mis-specification (namely, Theorem 1).

Below we discuss some direct implications of Theorem 1. First, we can view this phenomenon as a doubly robust property of offline RL. We borrow the name "doubly robust" from the offline policy _evaluation_ literature [42] to highlight the robustness of offline RL to reward mis-specification as an offline policy _optimization_ approach.

**Corollary 1**.: _Under Assumption 1, offline RL can learn a near optimal policy, as long as the reward is correct, or the data has a positive implicit bias._

Theorem 1 also implies that offline RL is an intrinsically safe learning algorithm, unlike its counterpart online RL where additional penalties or constraints need to be explicitly modelled [43; 44; 45; 46; 47].

**Corollary 2**.: _If \(\mu\) only covers safe states and there exists a safe policy staying within the support of \(\mu\), then the policy of offline RL only visits safe states with high probability (see Theorem 1)._

We should note that covering safe states is a very mild assumption in the safe RL literature [45; 48], e.g., compared with all (data) states have a safe action. It does not require the data collection policies that generate \(\mu\) are safe. This condition can be easily satisfied by filtering out unsafe states in post processing. The existence of a safe policy is also mild and common assumption. In Section 4.2, we validate this inherent safety property on offline SafetyGymnasium [49], an offline safe RL benchmark.

## 4 Experiments

We conduct two group of experiments. In Section 4.1, we conduct large scale experiment, showing that multiple offline RL algorithms can be robust to reward mis-specification on a variety of datasets. In Section 4.2, we experimentally validate the inherent safety of offline RL algorithms stated in Corollary 2. We show that admissible offline RL algorithms, without modifications, can achieve state-of-the-art performance on an offline safe RL benchmark [49].

### Robustness to Mis-specified Rewards

We empirically study the performance of offline RL algorithms under wrong rewards in a variety of tasks. We use the same set of wrong rewards as in Fig. 1. _1)_ zero: the zero reward, _2)_ random: labeling each transition with a reward value randomly sampled from \(\text{Unif}[0,1]\), and _3)_ negative: the negation of true reward. We consider five offline RL algorithms, ATAC [2], PSPI [7], IQL [8], CQL [9] and decision transformer (DT)4[10]. We deliberately choose offline RL algorithms to cover those that are provably pessimistic [2; 7] and those that are popular among practitioners [8; 9], as well as an unconventional offline RL algorithm [10]. We consider a variety of tasks from D4RL [1] and Meta-World [11] ranging from safety-critical tasks (i.e., the agent dies when reaching bad states),goal-oriented tasks, and tasks that belong to neither. We train a total of around \(16\)k offline RL agents (see Appendix C.8). Please see Appendix C for details.

MessagesWe would like to convey three main messages through our experiments. First, implicit data bias _can_ exist naturally in a wide range of datasets, and offline RL algorithms that are sufficiently pessimistic can leverage such a bias to succeed when given wrong rewards. Second, offline RL algorithms, regardless of how pessimistic they are, become sensitive to reward when the data does not possess a positive bias. Third, offline RL algorithms without explicit pessimism, e.g., IQL [8], can sometimes still be pessimistic enough to achieve good performance under wrong rewards.

Remark on negative resultsWe consider "negative" results, i.e., when offline RL _fails_ under wrong rewards, as important as the positive ones. Since they tell us _how_ positive data bias can be broken or avoided. We hope our study can provide insights to researchers who are interested in actively incorporating positive bias in data collection, as well as who hope to design offline RL benchmarks specifically with or without positive data bias.

#### 4.1.1 Locomotion Tasks from D4RL

We evaluate offline RL algorithms on three locomotion tasks, hopper, walker2d5 and halfcheetah, from D4RL [1], a widely used offline RL benchmark. For each task, we consider five datasets with different qualities: random, medium, medium-replay, medium-expert, and expert. We refer readers to [1] for the construction of these datasets. We measure policy performance in D4RL normalized scores [1]. We provide three baselines 1) behavior: normalized score directly computed from the dataset, 2) BC: the behavior cloning policy, and 3) original: the policy produced by the offline RL algorithm using the original dataset (with true reward). The normalized scores for baselines and offline RL algorithms with wrong rewards are presented in Fig. 3. The exact numbers can be found in tables in Appendix C.

Footnote 5: We remove terminal transitions from hopper and walker2d datasets when learning with wrong rewards. In Appendix C, we include a ablation study on the effect of removing terminals when using true reward.

Positive bias exists in some D4RL datasets.We visualize the length bias of D4RL datasets in Fig. 4. Here are our main observations. _1)_ Most datasets for hopper and walker2d, with the

Figure 3: Normalized scores for locomotion tasks from D4RL [1]. The mean and standard error for normalized scores are computed across \(10\) random seeds. For each random seed, we evaluate the final policy of each algorithm over \(50\) episodes.

exception of the medium-replay datasets, have a strong length bias, where longer trajectories have higher returns. This length bias is due to the safety-critical nature of hopper and walker2d tasks, as the trajectories get terminated when reaching bad states. The length bias is especially salient in hopper-medium, where the normalized score is almost proportional to episode length. _2)_ The medium-replay datasets for hopper and walker2d have more diverse behavior, so the bias is smaller. _3)_ All halfcheetah datasets do not have length bias, as they all have the same length of \(1000\). _4)_ hopper-expert dataset has a length bias, while walker2d and halfcheetah-expert datasets do not have an obvious length bias. However, it is worth noting that all expert datasets have an IL-type positive bias as discussed in Section 3.3.

Offline RL can learn good policies with wrong rewards on datasets with strong length bias.On datasets with strong length bias (hopper-random, hopper-medium and walker2d-medium), we observe that ATAC and PSPI with wrong rewards generally produce well-performing policies, in a few cases even out-performing the policies learned from the true reward (original in Fig. 4). DT is mostly insensitive to reward quality. IQL and CQL with wrong rewards can sometimes achieve good performance; among the two, we find IQL to be more robust to wrong rewards and CQL with wrong rewards almost fails completely on all walker2d datasets.

Offline RL can learn good policies with wrong rewards on expert datasetsIn Section 3, we point out that the data has a positive bias when it is generated by the optimal policy. In Fig. 3, we observe that all offline RL algorithms, when trained with wrong rewards, can achieve expert-level performance on walker2d and halfcheetah-expert datasets. ATAC, PSPI and DT perform well on the hopper-expert dataset while IQL and CQL receive lower scores when using wrong rewards.

Offline RL needs stronger reward signals on datasets with diverse behavior policy.The medium-replay datasets of hopper and walker2d are generated from multiple behavior policies. Due to their diverse nature, they are a multiple ways to stay within data support in a long term. As a result, the survival instinct of offline RL by itself is not sufficient to guarantee good performance. Here algorithms with wrong rewards generally under-perform the policies trained with true reward. As datasets have a more diverse coverage, they can be less positively biased and offline RL requires a stronger reward signal to differentiate good survival policies and the bad ones. Practically speaking, when high-quality reward is not available, a diverse dataset can even hurt offline RL performance. This is contrary to the common belief that larger data support is more preferable [26, 27, 28].

Offline RL requires good reward to perform well on datasets without length bias.In all four halfcheetah datasets, the trajectories all have the same length, as is demonstrated in Fig. 4. This means that there is no data bias. We observe that all algorithms with wrong rewards at best perform similarly as the behavior and BC policies in most cases; this is an imitation-like effect due to the survival instinct. In the halfcheetah-medium-expert dataset, since there are a variety of surviving trajectories, with score from \(0\) to near \(100\), we observe that the performance of the resulting policy degrades as data reward becomes more different from the true reward.

Offline RL algorithms with provable pessimism produce safer policies.For hopper and walker2d, we observe that policies learned by ATAC and PSPI, regardless of the reward, can keep the agent from falling for longer. The episode lengths of IQL and CQL policies are often

Figure 4: A visualization of length bias in datasets from D4RL [1]. Each plot corresponds to a dataset for a task (row) with a dataset (column). Each trajectory in a dataset is represented as a data point with the \(x\)-coordinate being its episode length and \(y\)-coordinate being its normalized score.

comparable to that of the behavior policies. We provide statistics of episode lengths in Appendix C. In Section 4.2, we provide further results validating the safety properties of ATAC and PSPI.

On empirically estimating positive data biasInspired by Definition 2, we propose an empirical estimate of positive data bias. Let \(J_{\text{zero}}\), \(J_{\text{rand}}\) and \(J_{\text{neg}}\) be the return (with respect to the true reward) of an offline RL algorithm learned with zero, random, and negative rewards, respectively. Then, given an estimated optimal return \(\hat{J}^{\star}\), we propose to empirically estimate the positive data bias by

\[\text{estimated positive bias}=\nicefrac{{J^{\star}}}{{\max\{\hat{j}^{\star} -\min\{J_{\text{zero}},J_{\text{rand}},J_{\text{neg}}\},0\}}}.\] (3)

In Fig. 5, we visualize the estimated positive data bias of all \(15\) D4RL datasets given by ATAC. Estimated positive data bias of all D4RL and Meta-World datasets is given in Fig. 9.

Remark on benchmarking offline RLOur observations on the popular D4RL datasets raise an interesting question for benchmarking offline RL algorithms. An implicit positive data bias can give certain algorithms a hidden advantage, as they can already achieve good performance without using the reward. Without controlling data bias, it is hard to differentiate whether the performance of those algorithms are due to their ability to maximize returns or simply due to their survival instinct.

#### 4.1.2 Goal-oriented Manipulation Tasks from Meta-World

We study goal-oriented manipulation tasks from the Meta-World benchmark [11]. We consider 15 goal-oriented tasks from Meta-World: the 10 tasks from the MT-10 set and the 5 testing tasks from the ML-45 set. For each task, we generate a dataset of \(110\) trajectories using the scripted policies provided by [11]; \(10\) are produced by the scripted policy, and \(100\) are generated by perturbing the scripted policy with a zero-mean Gaussian noise of standard deviation \(0.5\). In the datasets, unsuccessful trajectories receive a time-out signal after \(500\) steps (maximum episode length for Meta-World).

For each task, we measure the success rate of policies for \(50\)_new_ goals unseen during training. Similar to D4RL experiments, we consider BC policies and policies trained with the true reward (original) as baselines. Since the training dataset is generated for a different set of goals, we do not compare the success rate of learned policies with the success rate in data.

Goal-oriented problems have data bias.Goal-oriented problems are fundamentally different from safety-critical tasks (such as hopper and walker2d). A good goal-oriented policy should reach the goal as fast as possible rather than wandering around until the end of an episode. But another form of length bias still exists here, as successful trajectories are labeled with a termination signal that indicates an unbounded length and failed trajectories have a bounded length (see Section 3.1).

Offline RL can learn with wrong rewards on goal-oriented tasks.The success rate of learned policies with different rewards are shown in Fig. 6. We observe that ATAC and PSPI with wrong rewards generally achieve comparable or better success rate than BC policies. The exceptions are often due to that ATAC or PSPI, even with the true reward, does not work as well as BC in those tasks, e.g. drawer-open, bin-picking and box-close. This could be caused by overfitting or optimization errors (due to challenges of dynamics programming over long horizon problems). In a number of tasks, such as peg-insert-side, push and reach, ATAC and PSPI with wrong rewards can out-perform BC by a margin. This shows that data collection for goal-oriented problems have a positive bias that offline RL algorithms can succeed without true reward. This is remarkable as when learning with random and negative rewards, unsuccessful trajectories are long and generally have significantly higher return, as reward for each step is non-negative. The offline RL algorithms need to be sufficiently pessimistic and be able to plan over a long horizon to propagate pessimism to the unsuccessful data trajectories. For IQL, there is a gentle performance decrease as the reward becomes more different than the true reward, even though policies learned with wrong rewards are not much worse than BC in many cases. CQL shows robustness to reward in a few tasks such as button-press, drawer-close and door-unlock. Interestingly, DT performs almost uniformly across all rewards, potentially because DT does not explicitly maximize returns.

Figure 5: Estimated positive data bias of all \(15\) D4RL datasets given by ATAC. Datasets marked by “\(\times\)”, i.e., hopper-expert and walker2d-expert, have infinite positive bias.

Remark on BC performanceIt is noticeable that BC policies in general achieve high performance, in many tasks often out-performing offline RL policies learned with true reward. This effect has also been observed in existing work on similar tasks [50]. We would like to clarify that the goal of our experiments is to study the behavior of offline RL algorithms when trained with wrong rewards, rather than showing that offline RL performs better than BC. We refer interested readers to existing studies [50; 51] on when using offline RL algorithms is or is not preferable over BC.

### Inherent Safety Properties of Offline RL Algorithms

We show offline RL algorithms (without any modifications) can behave as a safe RL algorithm. We conduct experiments on offline SafetyGymnasium [49] using ATAC [2] and PSPI [7]. We first remove all transitions with non-zero cost and then run offline RL algorithms on the filtered datasets.

The results are summarized in Table 6 in Appendix C.6. We observe that _offline RL with this naive data filtering strategy (using less data) can achieve comparable performance as the per-task best performing state-of-the-art offline safe RL algorithm_, which uses the full dataset and has knowledge of the cost target. Our agents in general incur low cost except for the circle tasks. We hypothesize that learning a safe policy from the circle datasets is hard, as other baselines also struggle to produce safe policies. Note that these are preliminary results, and better performance might be achievable through using a more sophisticated data filtering strategy. Please see Appendix C.6 for experiment details.

## 5 Concluding Remarks

We present unexpected results on the robustness of offline RL to reward mis-specification. We show that this property originates from the interaction between the _survival instinct_ of offline RL and hidden positive biases in common data collection processes. Our findings suggest extra considerations should be taken when interpreting offline RL results and designing future benchmarks. In addition, our findings open a new space for offline RL research: the possibility of designing algorithms that proactively leverage the survival instinct to learn policies for domains where rewards are nontrivial to specify or unavailable. Please see Appendix A for a discussion on limitations and broader impacts.

Figure 6: Success rate for goal-oriented tasks from Meta-World [11]. The average success rate and confidence interval are computed across \(10\) random seeds. For each seed, we evaluate final policies for \(50\) episodes, each with a new goal unseen in the dataset.

## References

* [1]J. Fu, A. Kumar, O. Nachum, G. Tucker, and S. Levine (2020) D4rl: datasets for deep data-driven reinforcement learning. arXiv preprint arXiv:2004.07219. Cited by: SS1.
* [2]C. Cheng, T. Xie, N. Jiang, and A. Agarwal (2022) Adversarially trained actor critic for offline reinforcement learning. In International Conference on Machine Learning, pp.. Cited by: SS1.
* [3]J. Leike, M. Martic, V. Krakovna, P. A. Ortega, T. Everitt, A. Lefrancq, L. Orseau, and S. Legg (2017) Ai safety gridworlds. arXiv preprint arXiv:1711.09883. Cited by: SS1.
* [4]D. Hadfield-Menell, S. Milli, P. Abbeel, S. J. Russell, and A. Dragan (2017) Inverse reward design. Advances in neural information processing systems30. Cited by: SS1.
* [5]B. Ibarz, J. Leike, T. Pohlen, G. Irving, S. Legg, and D. Amodei (2018) Reward learning from human preferences and demonstrations in atari. Advances in neural information processing systems31. Cited by: SS1.
* [6]A. Pan, K. Bhatia, and J. Steinhardt (2022) The effects of reward misspecification: mapping and mitigating misaligned models. In International Conference on Learning Representations, Cited by: SS1.
* [7]T. Xie, C. Cheng, N. Jiang, P. Mineiro, and A. Agarwal (2021) Bellman-consistent pessimism for offline reinforcement learning. Advances in neural information processing systems34, pp. 6683-6694. Cited by: SS1.
* [8]I. Kostrikov, A. Nair, and S. Levine (2021) Offline reinforcement learning with implicit q-learning. In International Conference on Learning Representations, Cited by: SS1.
* [9]A. Kumar, A. Zhou, G. Tucker, and S. Levine (2020) Conservative q-learning for offline reinforcement learning. Advances in Neural Information Processing Systems33, pp. 1179-1191. Cited by: SS1.
* [10]L. Chen, K. Lu, A. Rajeswaran, K. Lee, A. Grover, M. Laskin, P. Abbeel, A. Srinivas, and I. Mordatch (2021) Decision transformer: reinforcement learning via sequence modeling. Advances in neural information processing systems34, pp. 15084-15097. Cited by: SS1.
* [11]T. Yu, D. Quillen, Z. He, R. Julian, K. Hausman, C. Finn, and S. Levine (2020) Meta-world: a benchmark and evaluation for multi-task and meta reinforcement learning. In Conference on robot learning, pp. 1094-1100. Cited by: SS1.
* [12]Y. Jin, Z. Yang, and Z. Wang (2021) Is pessimism provably efficient for offline rl?. In International Conference on Machine Learning, pp. 5084-5096. Cited by: SS1.
* [13]Y. Ma, A. Shen, D. Jayaraman, and O. Bastani (2022) Versatile offline imitation from observations and examples via regularized state-occupancy matching. In International Conference on Machine Learning, pp. 14639-14663. Cited by: SS1.
* [14]Y. Ma, A. Shen, D. Jayaraman, and O. Bastani (2022) Versatile offline imitation from observations and examples via regularized state-occupancy matching. In International Conference on Machine Learning, pp. 14639-14663. Cited by: SS1.
* [15]Y. Ma, A. Shen, D. Jayaraman, and O. Bastani (2022) Versatile offline imitation from observations and examples via regularized state-occupancy matching. In International Conference on Machine Learning, pp. 14639-14663. Cited by: SS1.
* [16]Y. Ma, A. Shen, D. Jayaraman, and O. Bastani (2022) Versatile offline imitation from observations and examples via regularized state-occupancy matching. In International Conference on Machine Learning, pp. 14639-14663. Cited by: SS1.
* [17]Y. Ma, A. Shen, D. Jayaraman, and O. Bastani (2022) Versatile offline imitation from observations and examples via regularized state-occupancy matching. In International Conference on Machine Learning, pp. 14639-14663. Cited by: SS1.
* [18]Y. Ma, A. Shen, D. Jayaraman, and O. Bastani (2022) Versatile offline imitation from observations and examples via regularized state-occupancy matching. In International Conference on Machine Learning, pp. 14639-14663. Cited by: SS1.
* [19]Y. Ma, A. Shen, D. Jayaraman, and O. Bastani (2022) Versatile offline imitation from observations and examples via regularized state-occupancy matching. In International Conference on Machine Learning, pp. 14639-14663. Cited by: SS1.
* [20]Y. Ma, A. Shen, D. Jayaraman, and O. Bastani (2022) Versatile offline imitation from observations and examples via regularized state-occupancy matching. In International Conference on Machine Learning, pp. 14639-14663. Cited by: SS1.
* [21]Y. Ma, A. Shen, D. Jayaraman, and O. Bastani (2022) Versatile offline imitation from observations and examples via regularized state-occupancy matching. In International Conference on Machine Learning, pp. 14639-14663. Cited by: SS1.
* [22]Y. Ma, A. Shen, D. Jayaraman, and O. Bastani (2022) Versatile offline imitation from observations and examples via regularized state-occupancy matching. In International Conference on Machine Learning, pp. 14639-14663. Cited by: SS1.
* [23]Y. Ma, A. Shen, D. Jayaraman, and O. Bastani (2022) Versatile offline imitation from observations and examples via regularized state-occupancy matching. In International Conference on Machine Learning, pp. 14639-14663. Cited by: SS1.
* [24]Y. Ma, A. Shen, D. Jayaraman, and O. Bastani (2022) Versatile offline imitation from observations and examples via regularized state-occupancy matching. In International Conference on Machine Learning, pp. 14639-14663. Cited by: SS1.
* [25]Y. Ma, A. Shen, D. Jayaraman, and O. Bastani (2022) Versatile offline imitation from observations and examples via regularized state-occupancy matching. In International Conference on Machine Learning, pp. 14639-14663. Cited by: SS1.
* [26]Y. Ma, A. Shen, D. Jayaraman, and O. Bastani (2022) Versatile offline imitation from observations and examples via regularized state-occupancy matching. In International Conference on Machine Learning, pp. 14639-14663. Cited by: SS1.
* [27]Y. Ma, A. Shen, D. Jayaraman, and O. Bastani (2022) Versatile offline imitation from observations and examples via regularized state-occupancy matching. In International Conference on Machine Learning, pp. 14639-14663. Cited by: SS1.
* [28]Y. Ma, A. Shen, D. Jayaraman, and O. Bastani (2022) Versatile offline imitation from observations and examples via regularized state-occupancy matching. In International Conference on Machine Learning, pp. 14639-14663. Cited by: SS1.
* [29]Y. Ma, A. Shen, D. Jayaraman, and O. Bastani (2022) Versatile offline imitation from observations and examples via regularized state-occupancy matching. In International Conference on Machine Learning, pp. 14639-14663. Cited by: SS1.
* [30]Y. Ma, A. Shen, D. Jayaraman, and O. Bastani (2022) Versatile offline imitation from observations and examples via regularized state-occupancy matching. In International Conference on Machine Learning, pp. 14639-14663. Cited by: SS1.
* [31]Y. Ma, A. Shen, D. Jayaraman, and O. Bastani (2022) Versatile offline imitation from observations and examples via regularized state-occupancy matching. In International Conference on Machine Learning, pp. 14639-14663. Cited by: SS1.
* [32]Y. Ma, A. Shen, D. Jayaraman, and O. Bastani (2022) Versatile offline imitation from observations and examples via regularized state-occupancy matching. In International Conference on Machine Learning, pp. 14639-14663. Cited by: SS1.
* [33]Y. Ma, A. Shen, D. Jayaraman, and O. Bastani (2022) Versatile offline imitation from observations and examples via regularized state-occupancy matching. In International Conference on Machine Learning, pp. 14639-14663. Cited by: SS1.
* [34]Y. Ma, A. Shen, D. Jayaraman, and O. Bastani (2022) Versatile offline imitation from observations and examples via regularized state-occupancy matching. In International Conference on Machine Learning, pp. 14639-14663. Cited by: SS1.
* [35]Y. Ma, A. Shen, D. Jayaraman, and O. Bastani (2022) Versatile offline imitation from observations and examples via regularized state-occupancy matching. In International Conference on Machine Learning, pp. 14639-14663. Cited by: SS1.
* [36]Y. Ma, A. Shen, D. Jayaraman, and O. Bastani (2022) Versatile offline imitation from observations and examples via regularized state-occupancy matching. In International Conference on Machine Learning, pp. 14639-14663. Cited by: SS1.
* [37]Y. Ma, A. Shen, D. Jayaraman, and O. Bastani (2022) Versatile offline imitation from observations and examples via regularized state-occupancy matching. In International Conference on Machine Learning, pp. 14639-14663. Cited by: SS1.
* [38]Y. Ma, A. Shen, D. Jayaraman, and O. Bastani (2022) Versatile offline imitation from observations and examples via regularized state-occupancy matching. In International Conference on Machine Learning, pp. 14639-14663. Cited by: SS1.
* [39]Y. Ma, A. Shen, D. Jayaraman, and O. Bastani (2022) Versatile offline imitation from observations and examples via regularized state-occupancy matching. In International Conference on Machine Learning, pp. 14639-14663. Cited by: SS1.
* [40]Y. Ma, A. Shen, D. Jayaraman, and O. Bastani (2022) Versatile offline imitation from observations and examples via regularized state-occupancy matching. In International Conference on Machine Learning, pp. 14639-14663. Cited by: SS1.
* [41]Y. Ma, A. Shen, D. Jayaraman, and O. Bastani (2022) Versatile offline imitation from observations and examples via regularized state-occupancy matching. In International Conference on Machine Learning, pp. 14639-14663. Cited by: SS1.
* [42]Y. Ma, A. Shen, D. Jayaraman, and O. Bastani (2022) Versatile offline imitation from observations and examples via regularized state-occupancy matching. In International Conference on Machine Learning, pp. 14639-14663. Cited by: SS1.
* [43]Y. Ma, A. Shen, D. Jayaraman, and O. Bastani (2022) Versatile offline imitation from observations and examples via regularized state-occupancy matching. In International Conference on Machine Learning, pp. 14639-14663. Cited by: SS1.
* [44]Y. Ma, A. Shen, D. Jayaraman, and O. Bastani (2022) Versatile offline imitation from observations and examples via regularized state-occupancy matching. In International Conference on Machine Learning, pp. 14639-14663. Cited by: SS1.
* [45]Y. Ma, A. Shen, D. Jayaraman, and O. Bastani (2022) Versatile offline imitation from observations and examples via regularized state-occupancy matching. In International Conference on Machine Learning, pp. 14639-14663. Cited by: SS1.
* [46]Y. Ma, A. Shen, D. Jayaraman, and O. Bastani (2022) Versatile offline imitation from observations and examples via regularized state-occupancy matching. In International Conference on Machine Learning, pp. 14639-14663. Cited by: SS1.
* [47]Y. Ma, A. Shen, D. Jayaraman, and O. Bastani (2022) Versatile offline imitation from observations and examples via regularized state-occupancy matching. In International Conference on Machine Learning, pp. 14639-14663. Cited by: SS1.
* [48]Y. Ma, A. Shen, D. Jayaraman, and O. Bastani (2022) Versatile offline imitation from observations and examples via regularized state-* [19] Z. Zhou, Z. Zhou, Q. Bai, L. Qiu, J. Blanchet, and P. Glynn, "Finite-sample regret bound for distributionally robust offline tabular reinforcement learning," in _International Conference on Artificial Intelligence and Statistics_, pp. 3331-3339, PMLR, 2021.
* [20] X. Ma, Z. Liang, L. Xia, J. Zhang, J. Blanchet, M. Liu, Q. Zhao, and Z. Zhou, "Distributionally robust offline reinforcement learning with linear function approximation," _arXiv preprint arXiv:2209.06620_, 2022.
* [21] K. Panaganti, Z. Xu, D. Kalathil, and M. Ghavamzadeh, "Robust reinforcement learning using offline data," in _Advances in Neural Information Processing Systems_, 2022.
* [22] H. Le, C. Voloshin, and Y. Yue, "Batch policy learning under constraints," in _International Conference on Machine Learning_, pp. 3703-3712, PMLR, 2019.
* [23] H. Xu, X. Zhan, and X. Zhu, "Constraints penalized q-learning for safe offline reinforcement learning," in _AAAI_, 2022.
* [24] J. Lee, C. Paduraru, D. J. Mankowitz, N. Heess, D. Precup, K.-E. Kim, and A. Guez, "Coptidice: Offline constrained reinforcement learning via stationary distribution correction estimation," in _International Conference on Learning Representations_, 2022.
* [25] D. Shin, A. D. Dragan, and D. S. Brown, "Benchmarks and algorithms for offline preference-based reward learning," _Transactions on Machine Learning Research_, 2023.
* [26] R. Munos, "Error bounds for approximate policy iteration," in _International Conference on Machine Learning_, vol. 3, pp. 560-567, Citeseer, 2003.
* [27] J. Chen and N. Jiang, "Information-theoretic considerations in batch reinforcement learning," in _International Conference on Machine Learning_, pp. 1042-1051, PMLR, 2019.
* [28] D. Yarats, D. Brandfonbrener, H. Liu, M. Laskin, P. Abbeel, A. Lazaric, and L. Pinto, "Don't change the algorithm, change the data: Exploratory data for offline reinforcement learning," _arXiv preprint arXiv:2201.13425_, 2022.
* [29] S. Fujimoto and S. S. Gu, "A minimalist approach to offline reinforcement learning," _Advances in neural information processing systems_, vol. 34, pp. 20132-20145, 2021.
* [30] Y. Wu, G. Tucker, and O. Nachum, "Behavior regularized offline reinforcement learning," _arXiv preprint arXiv:1911.11361_, 2019.
* [31] P. Rashidinejad, B. Zhu, C. Ma, J. Jiao, and S. Russell, "Bridging offline reinforcement learning and imitation learning: A tale of pessimism," _Advances in Neural Information Processing Systems_, vol. 34, pp. 11702-11716, 2021.
* [32] T. Yu, G. Thomas, L. Yu, S. Ermon, J. Y. Zou, S. Levine, C. Finn, and T. Ma, "Mopo: Model-based offline policy optimization," _Advances in Neural Information Processing Systems_, vol. 33, pp. 14129-14142, 2020.
* [33] R. Kidambi, A. Rajeswaran, P. Netrapalli, and T. Joachims, "Morel: Model-based offline reinforcement learning," _Advances in neural information processing systems_, vol. 33, pp. 21810-21823, 2020.
* [34] T. Xie, M. Bhardwaj, N. Jiang, and C.-A. Cheng, "Armor: A model-based framework for improving arbitrary baseline policies with offline data," _arXiv preprint arXiv:2211.04538_, 2022.
* [35] M. Rigter, B. Lacerda, and N. Hawes, "Rambo-RL: Robust adversarial model-based offline reinforcement learning," _Advances in neural information processing systems_, 2022.
* [36] T. Yu, A. Kumar, R. Rafailov, A. Rajeswaran, S. Levine, and C. Finn, "Combo: Conservative offline model-based policy optimization," _Advances in neural information processing systems_, vol. 34, pp. 28954-28967, 2021.
* [37] W. Zhan, B. Huang, A. Huang, N. Jiang, and J. Lee, "Offline reinforcement learning with realizability and single-policy concentrability," in _Conference on Learning Theory_, pp. 2730-2775, PMLR, 2022.

* [38] D. J. Foster, A. Krishnamurthy, D. Simchi-Levi, and Y. Xu, "Offline reinforcement learning: Fundamental barriers for value function approximation," in _Conference on Learning Theory_, pp. 3489-3489, PMLR, 2022.
* [39] E. Altman, _Constrained Markov decision processes_, vol. 7. CRC press, 1999.
* [40] M. Bhardwaj, T. Xie, B. Boots, N. Jiang, and C.-A. Cheng, "Adversarial model for offline reinforcement learning," _arXiv preprint arXiv:2302.11048_, 2023.
* [41] A. Y. Ng, D. Harada, and S. Russell, "Policy invariance under reward transformations: Theory and application to reward shaping," in _International Conference on Machine Learning_, vol. 99, pp. 278-287, Citeseer, 1999.
* [42] M. Dudik, J. Langford, and L. Li, "Doubly robust policy evaluation and learning," in _Proceedings of the 28th International Conference on International Conference on Machine Learning_, pp. 1097-1104, 2011.
* [43] J. Achiam, D. Held, A. Tamar, and P. Abbeel, "Constrained policy optimization," in _International conference on machine learning_, pp. 22-31, PMLR, 2017.
* [44] A. Wachi and Y. Sui, "Safe reinforcement learning in constrained markov decision processes," in _International Conference on Machine Learning_, pp. 9797-9806, PMLR, 2020.
* [45] N. C. Wagener, B. Boots, and C.-A. Cheng, "Safe reinforcement learning using advantage-based intervention," in _International Conference on Machine Learning_, pp. 10630-10640, PMLR, 2021.
* [46] S. Paternain, M. Calvo-Fullana, L. F. Chamon, and A. Ribeiro, "Safe policies for reinforcement learning via primal-dual methods," _IEEE Transactions on Automatic Control_, 2022.
* [47] H.-A. Nguyen and C.-A. Cheng, "Provable reset-free reinforcement learning by no-regret reduction," _arXiv preprint arXiv:2301.02389_, 2023.
* [48] A. Hans, D. Schneegals, A. M. Schafer, and S. Udluft, "Safe exploration for reinforcement learning.," in _ESANN_, pp. 143-148, Citeseer, 2008.
* [49] Z. Liu, Z. Guo, H. Lin, Y. Yao, J. Zhu, Z. Cen, H. Hu, W. Yu, T. Zhang, J. Tan, _et al._, "Datasets and benchmarks for offline safe reinforcement learning," _arXiv preprint arXiv:2306.09303_, 2023.
* [50] G. Zhou, L. Ke, S. Srinivasa, A. Gupta, A. Rajeswaran, and V. Kumar, "Real world offline reinforcement learning with realistic data source," _arXiv preprint arXiv:2210.06479_, 2022.
* [51] A. Kumar, J. Hong, A. Singh, and S. Levine, "Should i run offline reinforcement learning or behavioral cloning?," in _International Conference on Learning Representations_, 2022.
* [52] T. Gebru, J. Morgenstern, B. Vecchione, J. W. Vaughan, H. Wallach, H. D. Iii, and K. Crawford, "Datasheets for datasets," _Communications of the ACM_, vol. 64, no. 12, pp. 86-92, 2021.
* [53] A. Singh, A. Yu, J. Yang, J. Zhang, A. Kumar, and S. Levine, "Cog: Connecting new skills to past experience with offline reinforcement learning," in _Conference on Robot Learning_, PMLR, 2020.
* [54] H. Hu, Y. Yang, Q. Zhao, and C. Zhang, "The provable benefits of unsupervised data sharing for offline reinforcement learning," in _International Conference on Learning Representations_, 2023.
* [55] A. Li, B. Boots, and C.-A. Cheng, "Mahalo: Unifying offline reinforcement learning and imitation learning from observations," _arXiv preprint arXiv:2303.17156_, 2023.
* [56] T. Yu, A. Kumar, Y. Chebotar, K. Hausman, C. Finn, and S. Levine, "How to leverage unlabeled data in offline reinforcement learning," in _International Conference on Machine Learning_, pp. 25611-25635, PMLR, 2022.

* [57] J. Li, X. Hu, H. Xu, J. Liu, X. Zhan, Q.-S. Jia, and Y.-Q. Zhang, "Mind the gap: Offline policy optimization for imperfect rewards," in _International Conference on Learning Representations_, 2023.
* [58] X. B. Peng, A. Kumar, G. Zhang, and S. Levine, "Advantage-weighted regression: Simple and scalable off-policy reinforcement learning," _arXiv preprint arXiv:1910.00177_, 2019.
* [59] K. Zolna, A. Novikov, K. Konyushkova, C. Gulcehre, Z. Wang, Y. Aytar, M. Denil, N. de Freitas, and S. Reed, "Offline learning from demonstrations and unlabeled experience," _arXiv preprint arXiv:2011.13885_, 2020.
* [60] H. Xu, X. Zhan, H. Yin, and H. Qin, "Discriminator-weighted offline imitation learning from suboptimal demonstrations," in _International Conference on Machine Learning_, pp. 24725-24742, PMLR, 2022.
* [61] G.-H. Kim, S. Seo, J. Lee, W. Jeon, H. Hwang, H. Yang, and K.-E. Kim, "Demodice: Offline imitation learning with supplementary imperfect demonstrations," in _International Conference on Learning Representations_, 2021.
* [62] Z. Zhu, K. Lin, B. Dai, and J. Zhou, "Off-policy imitation learning from observations," _Advances in Neural Information Processing Systems_, vol. 33, pp. 12402-12413, 2020.
* [63] B. Dai, O. Nachum, Y. Chow, L. Li, C. Szepesvari, and D. Schuurmans, "Coindice: Off-policy confidence interval estimation," _Advances in neural information processing systems_, vol. 33, pp. 9398-9411, 2020.
* [64] R. Kidambi, J. Chang, and W. Sun, "Mobile: Model-based imitation learning from observation alone," _Advances in Neural Information Processing Systems_, vol. 34, pp. 28598-28611, 2021.
* [65] S. Yue, G. Wang, W. Shao, Z. Zhang, S. Lin, J. Ren, and J. Zhang, "CLARE: Conservative model-based reward learning for offline inverse reinforcement learning," in _International Conference on Learning Representations_, 2023.
* [66] B. D. Ziebart, A. L. Maas, J. A. Bagnell, A. K. Dey, _et al._, "Maximum entropy inverse reinforcement learning.," in _Proceedings of the AAAI Conference on Artificial Intelligence_, vol. 8, pp. 1433-1438, Chicago, IL, USA, 2008.
* [67] M. Smith, L. Maystre, Z. Dai, and K. Ciosek, "A strong baseline for batch imitation learning," _arXiv preprint arXiv:2302.02788_, 2023.
* [68] Y. Chen, X. Zhang, K. Zhang, M. Wang, and X. Zhu, "Byzantine-robust online and offline distributed reinforcement learning," in _International Conference on Artificial Intelligence and Statistics_, pp. 3230-3269, PMLR, 2023.
* [69] R. Yang, C. Bai, X. Ma, Z. Wang, C. Zhang, and L. Han, "Rorl: Robust offline reinforcement learning via conservative smoothing," in _Advances in Neural Information Processing Systems_, 2022.
* [70] L. Shi and Y. Chi, "Distributionally robust model-based offline reinforcement learning with near-optimal sample complexity," _arXiv preprint arXiv:2208.05767_, 2022.
* [71] Z. Liu, Z. Guo, Y. Yao, Z. Cen, W. Yu, T. Zhang, and D. Zhao, "Constrained decision transformer for offline safe reinforcement learning," _arXiv preprint arXiv:2302.07351_, 2023.
* [72] N. Polosky, B. C. Da Silva, M. Fiterau, and J. Jagannath, "Constrained offline policy optimization," in _International Conference on Machine Learning_, pp. 17801-17810, PMLR, 2022.
* [73] W. Xu, Y. Ma, K. Xu, H. Bastani, and O. Bastani, "Uniformly conservative exploration in reinforcement learning," in _International Conference on Artificial Intelligence and Statistics_, pp. 10856-10870, PMLR, 2023.
* [74] S. Lee, Y. Seo, K. Lee, P. Abbeel, and J. Shin, "Offline-to-online reinforcement learning via balanced replay and pessimistic q-ensemble," in _Conference on Robot Learning_, pp. 1702-1712, PMLR, 2022.

* [75] W. Yu, C. K. Liu, and G. Turk, "Policy transfer with strategy optimization," in _International Conference on Learning Representations_, 2019.
* [76] H. Van Hasselt, Y. Doron, F. Strub, M. Hessel, N. Sonnerat, and J. Modayil, "Deep reinforcement learning and the deadly triad," _arXiv preprint arXiv:1812.02648_, 2018.
* [77] R. S. Sutton and A. G. Barto, _Reinforcement learning: An introduction_. MIT press, 2018.
* [78] M. Riedmiller, "Neural fitted q iteration-first experiences with a data efficient neural reinforcement learning method," in _Machine Learning: ECML 2005: 16th European Conference on Machine Learning, Porto, Portugal, October 3-7, 2005. Proceedings 16_, pp. 317-328, Springer, 2005.

###### Contents

* 1 Introduction
* 2 Background
* 3 Why Offline RL can Learn Right Behaviors from Wrong Rewards
	* 3.1 Intuitions for Survival Instinct and Positive Data Bias
	* 3.2 Survival Instinct
	* 3.3 Positive Data Bias
	* 3.4 Summary: Offline RL is Doubly Robust and Intrinsically Safe
* 4 Experiments
	* 4.1 Robustness to Mis-specified Rewards
		* 4.1.1 Locomotion Tasks from D4RL
		* 4.1.2 Goal-oriented Manipulation Tasks from Meta-World
	* 4.2 Inherent Safety Properties of Offline RL Algorithms
* 5 Concluding Remarks
* A Limitations and Broader Impacts
* B Related Work
* C Experiment Details
* C.1 Benchmark Datasets
* C.2 Implementation Details
* C.3 Hyperparameters
* C.4 Episode Lengths for hopper and walker2d Tasks
* C.5 Effect of Removing Terminal Transitions
* C.6 Results on Safety Gymnasium Datasets
* C.7 Ablation Study on CQL Architecture
* C.8 Compute Usage
* D Technical Details of Section 3
* D.1 Definitions
* D.2 Formal Statement of the Main Theorem
* D.3 Survival Instinct
* D.4 Implicit Data Bias
* D.4.1 Proof of RL setup in Proposition 3
* D.4.2 Proof of IL setup in Proposition 3
* D.4.3 Proof of Length Bias in Proposition 3
* D.5 Proof of Theorem 2

[MISSING_PAGE_EMPTY:17]

Related Work

Offline RLOffline RL studies the problem of return maximization given an offline dataset. Offline RL algorithms can be broadly classified into model-based approaches, e.g., [32; 33; 14; 35; 34], and model-free approaches, e.g., [12; 29; 7; 2; 9; 8]. Since the agent needs to learn without online data collection, offline RL becomes more challenging when the offline dataset does not provide good coverage over the state-action distribution of all feasible policies. There have been two common strategies to handle insufficient coverage, behavior regularization approaches, e.g., [29; 30; 8], which restricts the learned policy to be close to the behavior policy, and value regularization approaches, e.g., [12; 13; 7; 2; 9; 34] which provide a pessimistic value estimates for the learned policy. Both approaches can be viewed as optimizing for a performance lower bound of the agent's return.

Offline RL commonly assumes compatibility between offline dataset and MDP, i.e., the offline dataset should be generated from the task MDP of interest. Our work is distinctive in the offline RL literature as we study the behavior of existing offline RL algorithms when data reward _differs_ from the true MDP reward. Existing tools for analyzing offline RL are not directly applicable to our setting: policies with good performance under data reward, as guaranteed by such tools, do not necessarily attain high return under the true reward. Our novel analysis is made possible by studying the properties of offline data distribution, an aspect mostly neglected by existing work in the literature.

Offline RL with imperfect rewardIn the offline RL literature, there is a line of work studying scenarios where the reward is missing in a subset of the dataset [53; 54; 55; 56] or when the data reward is misspecified [57]. These approaches propose to construct a new reward function and apply offline RL algorithms on the relabeled dataset. For example, [53; 56] label the missing reward with a constant minimum reward. [55; 54] learn a pessimistic reward function. [57] optimize for a reward function which minimizes the reward difference among expert data.

Different from this line of work, we are interested in running offline RL algorithms using the misspecified reward _as it is_. We show both theoretically and empirically that offline RL algorithms can produce well-performing policies with rewards different from the true reward as long as the underlying data distribution satisfies certain conditions. It can be an interesting future work to combine these techniques with our analysis to explore the robustness of offline RL algorithms under the class of learned reward functions.

After paper submission, we learned that [25] observed a similar robustness phenomena of offline RL. However, the authors did not provide a complete explanation. They found that offline RL algorithms such as AWR [58] empirically can learn well with random or zero rewards on D4RL locomotion datasets. For the expert datasets, they conjectured that this robustness may be due to the fact that offline RL algorithms behave like imitation learning on expert data, but they did not provide details of why offline RL algorithms have this robustness behavior on other datasets. Here we provide a formal theoretical proof to show that this robustness is due to the interaction between the survival instinct of offline RL and an implicit positive data bias. Our results support [25]'s conjecture on the expert dataset; we show that expert datasets generated by an optimal policy can theoretically be proved to be infinitely positively biased, and we also empirically estimate the positive bias on the D4RL datasets in Fig. 5, which agrees with the theoretical prediction. Our results further provide conditions for positive data bias beyond the expert data scenario, such as the length bias in Section 3. These new insights show why offline RL algorithms can learn good policies with wrong rewards, on datasets collected by non-expert suboptimal policies. Finally, we present strong empirical evidence to show this robustness phenomenon happens in a large number of existing benchmarks with multiple offline RL algorithms.

Offline ILOffline imitation learning (IL) can be considered as a special case of offline RL when reward is missing from the entire dataset, or when the agent has the largest uncertainty about the reward [2]. In offline IL, the learner is instead given the information on whether a transition is generated by an expert. The goal of offline IL is to produce a policy that has comparable performance with the expert. [59] learns a discriminator reward to classify expert and non-expert data, and apply offline RL algorithms with the learned reward. [60] uses the discriminator as the weight for weighted behavior cloning. [61; 16; 62] proposes DICE [63]-style algorithms to minimize the divergence between the state-action or state-(next state) occupancy measure of the learner and the expert. [15; 64] minimize state-action or state-(next state) distribution divergence under a pessimistic model. [65]extends maximum entropy inverse RL [66] to an offline setting. [67] proposes to run offline RL under an indicator reward function on whether the data is expert data. [55] learns a pessimistic reward function where the expert data labeled as the maximum reward.

Our experiments with zero reward function is similar to an offline IL setting with the behavior policy being the expert. However, we show that offline RL algorithms can sometimes achieve significant performance gain over behavior and behavior cloning policies, which is a phenomenon that cannot be explained by existing analysis in offline IL. We provide an explanation to this puzzle: in Appendix D.4, we show that the data distribution is \(\infty\)-positively biased (according our definition) for offline RL, when the data is generated by the optimal policy of the true MDP. This means that an admissible offline RL algorithm can achieve near-optimal performance with _any_ reward function from its corresponding admissible reward class in this case. We empirically show that in Section 4.

Robust offline RLRobust offline RL [17, 68, 18, 21, 69, 19, 20, 70] aims to develop new offline RL algorithms for cases when the compatibility assumption does not hold, i.e., the offline dataset may not be generated from the task MDP. Among this line of work, [21, 19, 20, 70] study the robust MDP setting, where the dataset can be generated from any MDP within a \(\delta\)-ball of a nominal MDP. An adversary can choose any MDP from the \(\delta\)-ball when evaluating the learned policy. [17] considers when the adversary can modify \(\epsilon\) fraction of transition tuples in the offline dataset, while [18] considers when up to \(K\) trajectories can be added in or removed from the original dataset. [68] uses the formulation of Byzantine-robust: when multiple offline datasets are presented to the learner, among them an \(\alpha\) fraction are corrupted. [69] assumes the dataset is generated from the true MDP, but the state presented to the policy during test time can be a corrupted up to \(\epsilon\) distance.

Our work is fundamentally different from this line of work in that we study the _inherent_ and somewhat _unintended_ robustness of offline RL algorithms. We show that, despite originally designed under the compatibility assumption, offline RL algorithms show strong robustness against perturbation of reward. Another salient difference is that robust offline RL typically requires an upper bound on the size of perturbation to the true or nominal MDP, and most of proposed algorithms assume knowledge of this upper bound. However, our work does not make assumptions on the size of perturbation and, moreover, the algorithms we study are not even aware of the existence of such perturbation. Indeed, we empirically show that offline RL algorithms, without any modifications, can succeed even when data reward is a constant or the negation of the true reward. Additionally, in this work, we consider the reward perturbation while most work in robust offline RL focuses more on dynamics perturbation.

Constrained and safe offline RLOur work is related to the literature of constrained and safe offline RL [22, 23, 71, 24, 72]. In constrained offline RL, the agent solves for a CMDP given an offline dataset \(\{(s,a,r,c,s^{\prime})\}\). The goal of constrained offline RL is to produce a policy which maximizes the expected return while ensuring that constraint violation is below a given upper bound. [22] uses an offline policy evaluation oracle to solve for the Lagrangian relaxation of the CMDP. [23] constructs a pessimistic estimation of value and constraint violation. [24, 72] proposes DICE [63]-style algorithms to solve for the optimal state-action occupancy measure. [71] proposes a variant of decision transformer [10] capable of producing policies for different constraint violation upper bounds during test time.

Our work, instead, considers a different scenario where the constraint is only given _implicitly_ -- through the support of the offline dataset. We assume that any transition in the dataset satisfies the constraint, which can be achieved through intervention during data collection. We show that offline RL can inherently produce approximately safe policies due to its survival instinct. Although our assumption seems more restrictive as in that we require a constraint violation-free dataset, we argue that our assumption is more practical in many safety critical scenarios: compared to executing unsafe actions, it is perhaps more reasonable to intervene before constraint violation happens. We believe that our findings open up new possibilities for applying offline RL as a safe RL oracle, which can be especially promising in an offline-online RL setup [73, 74].

## Appendix C Experiment Details

In the experiments, we train a total of around \(16\)k offline RL agents (see Appendix C.8 for details). In this appendix, we provide details of experiment setup, implementation of algorithms, hyperparametertuning, as well as additional experimental results and ablation studies. At the end of the appendix, we include Table 13 and Table 14 which contain the actual numbers used in generating Fig. 3 and Fig. 6.

### Benchmark Datasets

In the experiments, we consider three tasks (15 datasets) from D4RL [1], 15 datasets collected from the Meta-World [11] benchmark, and 16 datasets from SafetyGymnasium [49]. The D4RL and Meta-World tasks are visualized in Fig. 7. Below we provide more details on these tasks.

D4RLWe consider three locomotion tasks from D4RL [1], hopper, walker2d and halfcheetah6 (see Fig. 7). For each task, we use five datasets of different behavior policy quality: random, medium, medium-replay, medium expert, and expert.

Footnote 6: We use the v2 version of all three tasks.

**hopper:**: The goal of hopper is to move forward as fast as possible without falling down. An episode terminates when either of the three happens: _1)_ the height of the hopper is below a threshold, _2)_ the body of the hopper is too tilted, or _3)_ the velocity of any joint is too large. The original true reward function (which has been used to train online RL agents) is

\[r_{\text{hopper}}\coloneqq\mathbbm{1}[\text{hopper is alive}]+\frac{1}{\Delta t }\Delta x-0.001\|\tau\|^{2},\]

where \(\Delta x\) is the displacement in the forward direction, and \(\tau\) is the applied torque. The true reward function is designed such that the hopper can receive bonus when staying alive and moving forward. We show that, for a few hopper datasets, offline RL agent can succeed without such a carefully designed reward, and even with the negation of the true reward.
**walker2d:**: The walker2d task is similar to hopper, except that the walker2d agent has a higher degree of freedom. We observe similarly that offline RL can learn good behaviors without a reward that is specifically designed to encourage staying alive and moving forward.
**halfcheetah:**: The halfcheetah task differs from the two tasks above since episodes here always have the same length and never terminates early. The true reward of halfcheetah is

\[r_{\text{halfcheetah}}\coloneqq\frac{1}{\Delta t}\Delta x-0.1\|\tau\|^{2}.\]

Since the halfcheetah datasets (with the exception of the expert dataset) do not have a positive bias, we find that offline RL agent is sensitive to reward quality.

Meta-WorldWe consider 15 goal-oriented manipulation tasks from [11], see Fig. 7. We use the 10 tasks from the MT-10 set and the 5 testing tasks from the ML-45 set. Each Meta-World task comes with a hand-crafted reward function that encourage progress toward task completion, e.g., when the end-effector grasps the target object, when the object is placed at the goal, etc. We refer readers to [11] for the specific reward functions used for each tasks. We make a few modifications to the Meta-World environment: _1)_ we shift and scale the true reward such that the agent receives a

Figure 7: We study offline RL with wrong rewards for a variety of tasks: (a) three locomotion tasks from D4RL [1]; hopper (top left), walker2d (top right) and halfcheetah (bottom). the figures are from [75] (b) 15 goal-oriented tasks from Meta-World [11]. The 15 tasks are composed of the 10 tasks from MT-10 (top 2 rows) and the 5 testing tasks from ML-45 (bottom row).

reward of range \([-1,0]\) rather than \([0,10]\) of the original environment, _2)_ we terminate an episode and mark the last state with a terminal flag when the goal is achieved. We show that, data collected by goal-oriented tasks can have a positive bias, and an offline RL agent can make use of such bias to learn the right behavior even with wrong rewards. We would like to clarify that the data generated by the original Meta-World environment (with no terminal signals) does not have a positive bias in general, since all trajectories, success or failure, would have a same length of \(500\).

SafetyGymnasiumWe experiment with \(16\) datasets from offline SafetyGymnasium [49]. The \(16\) datasets are given by two embodiments, point and car, with \(4\) tasks, button, circle, goal, and push, each with \(2\) difficulty levels.

It is worth noting that [49] focuses on learning from a dataset which contains _both_ safe and unsafe behaviors, while our results (Corollary 2) focus on offline RL's ability to produce safe policies given a dataset that _only_ contains safe states. Therefore, we use a naive filtering strategy of removing all transitions with non-zero cost and then run offline RL algorithms on the filtered datasets.

Many algorithms benchmarked in [49] use a cost target, which sets the maximum total cost allowed by safety. [49] uses three different cost targets, \(\{20,40,80\}\), and reports the normalized total reward and cost for each algorithm averaged over the three targets. By contrast, standard offline RL algorithms do not use such a cost target. We report results of offline RL algorithms according to an "effective cost target" of \(34.29\), which is similar to how BC-All results are presented in [49].

### Implementation Details

We use the implementation of ATAC and PSPI from https://github.com/chinganc/lightATAC. We make a small modification to the ATAC and PSPI policy class. Instead of a \(\tanh\)-Gaussian policy, we use a scaled \(\tanh\)-Gaussian policy, i.e., \(c\cdot\tanh(\mathcal{N}(\mu,\Sigma))\) with \(c\geq 1\). We find scaled \(\tanh\)-Gaussian policies to be more numerically stable when the data has a lot of on-boundary actions, i.e., actions that takes the value of either \(-1\) or \(1\) on any dimension. For PSPI, we let the critic \(f\) to minimize for \(f(s,\pi)\) for all states from the dataset (rather than all initial states). We find it to provide better results when the effective horizon given by the discount factor is smaller than the actual episode length. We use the implementation of IQL and BC from https://github.com/gwhomas/IQL-PyTorch/, and the implementation of CQL from https://github.com/young-geng/CQL. For decision transformer, we use the implementation from https://github.com/kzl/decision-transformer. We feed the decision transformer with wrong rewards (consistent with data reward) during testing.

### Hyperparameters

We use the default values (ones provided by the original paper or by the implementation we use) for most of the hyperparameters for all algorithms. For each algorithm, we tune one or two hyperparameters (with 4 combinations at most) which affect the degree of pessimism7. The values of hyperparamters of all algorithms are given in Table 1-4, where the choices of hyperparameters used in tuning are highlighted in blue. The tuned hyperparameter values for all experiments and all algorithms are provided in Table 9-12.

Footnote 7: For decision transformer, we do not tune hyperparameters since it has no notion of pessimism.

For D4RL and Meta-World experiments, we tune hyperparameter per each dataset and per reward label. During tuning, we evaluate each combination of hyperparameter(s) for \(2\) random seeds for D4RL and \(3\) random seeds for Meta-World. We choose the best-performing values and report the results of these hyperparameters over \(10\)_new_ random seeds (not including the tuning seeds). We find the D4RL scores for the true reward to be close to what is reported in the original papers.

For SafetyGymnasium, we follow procedures from [49] and report results of the best hyperparameters over \(3\) random seeds. We take the safe agent with the highest cumulative reward if there is at least one safe agent, and take the agent with the lowest cumulative cost if there are no safe agents.

### Episode Lengths for hopper and walker2d Tasks

We show the episode lengths for hopper and walker2d tasks in Fig. 8. We observe that policies learned by ATAC and PSPI can consistently keep the agent from falling down for a significant longer period than the behavior policies. The only exception is when running PSPI on hopper-medium-expert dataset with random reward. This is potentially due to the fact that we use only \(2\) seeds for hyperparameter tuning. The average normalized scores and episode lengths during tuning are \(101.1\) and \(925.0\), respectively, higher than the final values of \(58.8\) and \(546.0\). We observe that DT policies also tend to keep the agent alive for a longer number of steps than behavior policies.

For IQL and CQL, it is relatively rare for them to achieve long episode lengths when trained with wrong rewards. IQL and CQL, when using negative reward, can keep the agent from falling for longer in hopper-medium and hopper-medium-expert datasets. We would like to note that our hyperparameters are tuned based on normalized scores rather than episode lengths. The statistics of episode lengths are included in Table 5.

### Effect of Removing Terminal Transitions

For hopper and walker2d tasks, we remove the terminal transitions, i.e., the transitions where the hopper or walker falls, from the datasets with modified rewards. This is to satisfy the safety

\begin{table}
\begin{tabular}{l l} \hline \hline
**Hyperparameter** & **Value** \\ \hline Bellman consistency coefficient \(\beta\) & \(\{0.1,1,10,100\}\) \\ Number of hidden layers & \(3\) \\ Number of hidden units & \(256\) \\ Nonlinearity & ReLU \\ Policy distribution & scaled tanh-Gaussian \\ Action scale \(c\) & \(1.0\) for D4RL and SafetyGymnasium \\  & \(1.2\) for Meta-World \\ Fast learning rate \(\eta_{\text{fast}}\) & \(5\times 10^{-4}\) \\ Slow learning rate \(\eta_{\text{slow}}\) & \(5\times 10^{-7}\) \\ Minibatch size \(|\mathcal{D}_{\text{mini}}|\) & \(256\) \\ Gradient steps & \(10^{6}\) \\ Warmstart (BC) steps & \(10^{5}\) \\ Temporal difference loss weight \(w\) & \(0.5\) \\ Target smoothing coefficient \(\tau\) & \(0.005\) \\ Discount factor \(\gamma\) & \(0.99\) for D4RL and SafetyGymnasium \\  & \(0.998\) for Meta-World \\ Minimum target value \(V_{\text{min}}\) & \(\frac{2}{1-\gamma}\min(-1,\tilde{r}_{\text{min}})\) \\ Maximum target value \(V_{\text{max}}\) & \(\frac{2}{1-\gamma}\max(1,\tilde{r}_{\text{max}})\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Hyperparameters for ATAC and PSPI

\begin{table}
\begin{tabular}{l l} \hline \hline
**Hyperparameter** & **Value** \\ \hline Inverse temperature \(\beta\) & \(\{0.3,3\}\) \\ Expectile \(\tau\) & \(\{0.7,0.9\}\) \\ Number of hidden layers & \(3\) \\ Number of hidden units & \(256\) \\ Nonlinearity & ReLU \\ Policy distribution & Gaussian \\ Learning rate \(\eta\) & \(3\times 10^{-4}\) \\ Minibatch size \(|\mathcal{D}_{\text{mini}}|\) & \(256\) \\ Gradient steps & \(10^{6}\) \\ Target smoothing coefficient \(\alpha\) & \(0.005\) \\ Discount factor \(\gamma\) & \(0.99\) for D4RL \\  & \(0.998\) for Meta-World \\ \hline \hline \end{tabular}
\end{table}
Table 2: Hyperparameters for IQLcondition in Corollary 2. A terminal signal implicitly adds an absorbing8, but in this case, unsafe, state into the data distribution \(\mu\). This means that staying within data support in a long term would not provide safety, as the agent can stay within support by falling. For D4RL datasets in particular, we need to remove terminal transitions (rather than just the terminal flag) since terminal transitions in the original datasets do not contain valid next states.9

Footnote 8: This is also why we mark goal completion as terminal in Meta-World experiments. For goal-oriented tasks, terminal signal upon task completion implicitly creates an absorbing goal state in the data distribution.

Footnote 9: The authors of D4RL datasets set the next state to be a place-holder value when a transition is marked as terminal.

This, however, means that there are two underlying variables: _1)_ the data reward, and _2)_ whether terminal transitions are removed, when comparing offline RL with wrong rewards and offline RL on the original dataset. To separate the effect of two variables, we study the performance of offline RL algorithms with the original true reward but the terminal transitions removed. The results10 are listed in the No-terminal column in Table 13. We observe that ATAC and PSPI with the original true reward show similar results whether the terminal transitions are removed. IQL and CQL with true reward, interestingly, does significantly worse when the terminal transitions are removed. This is potentially due to the increased instability of target network [76, 77] when there are no terminal signals.

Footnote 10: We do not study decision transformer (DT) in this ablation since DT does not use terminal signals.

### Results on Safety Gymnasium Datasets

In Table 6, we present results on point datasets and car from Safety Gymnasium [49]. For baselines, we select the best none-BC Algorithms from [49]: the best-performing offline safe RL agent selected

\begin{table}
\begin{tabular}{l l} \hline \hline
**Hyperparameter** & **Value** \\ \hline Number of hidden layers & \(3\) \\ Number of hidden units & \(128\) \\ Nonlinearity & ReLU \\ Context length \(K\) & \(20\) \\ Dropout & \(0.1\) \\ Warmup steps & \(2\times 10^{2}\) \\ Learning rate \(\eta\) & \(5\times 10^{-4}\) \\ Minibatch size \(|\mathcal{D}_{\text{mini}}|\) & \(256\) \\ Gradient steps & \(7\times 10^{4}\) for D4RL \\  & \(5\times 10^{3}\) for Meta-World \\ Gradient norm clip & \(0.25\) \\ Weight decay & \(10^{-4}\) \\ Return-to-goal conditioning & Return randomly sampled from the top \(10\%\) \\  & of trajectory returns in the data, w.r.t. the data reward \\ \hline \hline \end{tabular}
\end{table}
Table 4: Hyperparameters for the decision transformer (DT)

\begin{table}
\begin{tabular}{l l} \hline \hline
**Hyperparameter** & **Value** \\ \hline Critic pessimism coefficient \(\alpha\) & \(\{0.1,1,10,100\}\) \\ Number of hidden layers & \(2^{*}\) \\ Number of hidden units & \(256\) \\ Nonlinearity & ReLU \\ Policy distribution & \(\tanh\)-Gaussian* \\ Learning rate \(\eta\) & \(3\times 10^{-4}\) \\ Minibatch size \(|\mathcal{D}_{\text{mini}}|\) & \(256\) \\ Gradient steps & \(10^{6}\) \\ Target smoothing coefficient \(\tau\) & \(0.005\) \\ Discount factor \(\gamma\) & \(0.99\) for D4RL \\  & \(0.998\) for Meta-World \\ \hline \hline \end{tabular}
\end{table}
Table 3: Hyperparameters for CQL. *We include an ablation study of CQL with \(3\) hidden layers and scaled \(\tanh\)-Gaussian policy, i.e., the same architecture as ATAC and PSPI, in Appendix C.7.

[MISSING_PAGE_EMPTY:24]

[MISSING_PAGE_FAIL:25]

\begin{table}
\begin{tabular}{c|c c|c c|c c c} \hline \multirow{2}{*}{Task} & \multicolumn{2}{c|}{ATAC} & \multicolumn{2}{c|}{PSPI} & \multicolumn{2}{c}{Best None-BC Algorithm from [49]} \\  & reward \(\uparrow\) & cost \(\downarrow\) & reward \(\uparrow\) & cost \(\downarrow\) & reward \(\uparrow\) & cost \(\downarrow\) & algorithm \\ \hline PointButton1 & 0.08 & 0.82 & 0.10 & 1.04 & 0.13 & 1.35 & COpiDICE [24] \\ \hline PointButton2 & 0.14 & **0.89** & 0.19 & 1.29 & 0.15 & 1.51 & COpiDICE [24] \\ \hline PointCircle1 & 0.40 & 2.79 & 0.34 & 2.39 & **0.59** & **0.69** & CDT [71] \\ \hline PointCircle2 & 0.61 & 4.45 & 0.55 & 2.84 & 0.64 & 1.05 & CDT [71] \\ \hline PointGoal1 & 0.58 & **0.94** & **0.59** & **0.90** & **0.71** & **0.98** & BCQ-Lag [23] \\ \hline PointGoal2 & 0.55 & 2.30 & 0.45 & 2.55 & 0.40 & 1.31 & CPQ [23] \\ \hline PointPush1 & 0.18 & **0.51** & **0.20** & **0.85** & **0.33** & **0.86** & BCQ-Lag [23] \\ \hline PointPush2 & 0.10 & **0.68** & 0.10 & **0.81** & **0.23** & **0.99** & BCQ-Lag [23] \\ \hline CarButton1 & -0.06 & 1.25 & -0.08 & 1.11 & 0.21 & 1.60 & CDT [71] \\ \hline CarButton2 & -0.11 & 1.22 & -0.08 & 1.04 & 0.13 & 1.58 & CDT [71] \\ \hline CarCircle1 & 0.66 & 5.51 & 0.64 & 5.92 & 0.60 & 1.73 & CDT [71] \\ \hline CarCircle2 & 0.64 & 5.96 & 0.64 & 5.99 & 0.66 & 2.53 & CDT [71] \\ \hline CarGoal1 & 0.50 & **0.99** & **0.41** & **0.78** & **0.47** & **0.78** & BCQ-Lag [23] \\ \hline CarGoal2 & 0.24 & 1.00 & 0.24 & 1.05 & 0.25 & 0.91 & COpiDICE [24] \\ \hline CarPush1 & 0.33 & 0.96 & 0.32 & **0.75** & **0.31** & **0.40** & CDT [71] \\ \hline CarPush2 & 0.10 & 0.95 & 0.11 & **0.81** & 0.09 & 1.07 & COpiDICE [24] \\ \hline \end{tabular}
\end{table}
Table 6: Results on point and car datasets from offline SafetyGymnasium [49] over \(3\) random seeds. Cumulative reward and cost are normalized as described in [49]. An agent is considered safe if the cumulative cost is no larger than \(1\). Unsafe agent with cumulative cost more than \(1\) (total cost 34.29) is shown in gray.

\begin{table}
\begin{tabular}{c|c|c|c|c|c|c} \hline Dataset & Algo & Original & Zero & Random & Negative & No-terminal \\ \hline hopper-random & CQL & 3.0\(\pm\)0.7 & 2.0\(\pm\)1.1 & 1.8\(\pm\)0.2 & 0.7\(\pm\)0.0 & 1.2\(\pm\)0.5 \\  & CQL\({}_{3}\) & 11.2\(\pm\)2.2 & 2.0\(\pm\)0.4 & 7.7\(\pm\)3.5 & 3.9\(\pm\)2.9 & 10.1\(\pm\)3.2 \\ \hline hopper-medium & CQL & 85.0\(\pm\)3.7 & 55.0\(\pm\)11.7 & 36.9\(\pm\)9.8 & 5.90\(\pm\)1.5 & 46.3\(\pm\)1.5 \\  & CQL\({}_{3}\) & 48.0\(\pm\)12.2 & 61.7\(\pm\)12.2 & 17.3\(\pm\)9.8 & 75.8\(\pm\)6.9 & 32.8\(\pm\)10.3 \\ \hline hopper-medium-replay & CQL & 79.7\(\pm\)3.7 & 1.9\(\pm\)0.1 & 7.6\(\pm\)3.7 & 5.8\(\pm\)2.7 & 4.4\(\pm\)2.8 \\  & CQL\({}_{3}\) & 100.3\(\pm\)1.6 & 1.8\(\pm\)0.0 & 1.8\(\pm\)0.0 & 1.6\(\pm\)0.2 & 1.8\(\pm\)0.0 \\ \hline hopper-medium-expert & CQL & 88.8\(\pm\)3.6 & 49.7\(\pm\)8.1 & 60.7\(\pm\)9.4 & 88.6\(\pm\)6.1 & 47.4\(\pm\)1.0 \\  & CQL\({}_{3}\) & 104.7\(\pm\)1.4 & 40.6\(\pm\)10.7 & 17.3\(\pm\)9.0 & 48.2\(\pm\)8.7 & 30.0\(\pm\)9.5 \\ \hline walker2d-random & CQL & 4.0\(\pm\)1.8 & 0.9\(\pm\)0.9 & 4.3\(\pm\)1.4 & -0.4\(\pm\)0.0 & 0.2\(\pm\)0.4 \\  & CQL\({}_{3}\) & 4.8\(\pm\)0.2 & 1.4\(\pm\)0.8 & 3.3\(\pm\)1.2 & 0.9\(\pm\)0.6 & -0.2\(\pm\)0.1 \\ \hline walker2d-medium & CQL & 81.4\(\pm\)0.3 & 2.0\(\pm\)0.9 & 1.2\(\pm\)0.7 & 14.5\(\pm\)7.4 & 2.1\(\pm\)0.8 \\  & CQL\({}_{3}\) & 81.0\(\pm\)0.4 & 5.0\(\pm\)4.7 & -0.2\(\pm\)0.0 & 13.3\(\pm\)7.7 & 0.1\(\pm\)0.3 \\ \hline walker2d-medium-replay & CQL & 74.1\(\pm\)1.2 & 0.5\(\pm\)0.3 & -0.2\(\pm\)0.1 & 0.2\(\pm\)0.3 & 0.4\(\pm\)0.4 \\  & CQL\({}_{3}\) & 76.8\(\pm\)1.3 & -0.2\(\pm\)0.1 & 0.1\(\pm\)0.1 & 0.1\(\pm\)0.3 & 0.3\(\pm\)0.3 \\ \hline walker2d-medium-expert & CQL & 109.3\(\pm\)0.3 & 0.8\(\pm\)0.5 & 0.1\(\pm\)0.2 & 28.0\(\pm\)9.1 & -0.2\(\pm\)0.0 \\  & CQL\({}_{3}\) & 108.0\(\pm\)0.6 & -0.2\(\pm\)0.1 & -0.3\(\pm\)0.1 & 26.8\(\pm\)11.0 & -0.2\(\pm\)0.0 \\ \hline halfcheetah-random & CQL & 30.7\(\pm\)0.6 & 1.0\(\pm\)0.8 & -1.4\(\pm\)0.3 & -5.4\(\pm\)0.3 & – \\  & CQL\({}_{3}\) & 31.3\(\pm\)0.6 & 0.5\(\pm\)0.4 & 1.9\(\pm\)0.1 & -7.0\(\pm\)0.4 & – \\ \hline halfcheetah-medium & CQL & 57.2\(\pm\)1.5 & 43.2\(\pm\)0.1 & 43.5\(\pm\)0.1 & 42.7\(\pm\)0.1 & – \\  & CQL\({}_{3}\) & 65.1\(\pm\)0.6 & 43.1\(\pm\)0.1 & 43.3\(\pm\)0.1 & 42.2\(\pm\)0.1 & – \\ \hline halfcheetah-medium-replay & CQL & 48.2\(\pm\)5.4 & 34.2\(\pm\)0.7 & 33.1\(\pm\)0.9 & 32.4\(\pm\)1.0 & – \\  & CQL\({}_{3}\) & 47.8\(\pm\)5.2 & 38.3\(\pm\)0.1 & 37.7\(\pm\)1.1 & 32.6\(\pm\)1.2 & – \\ \hline halfcheetah-medium-expert & CQL & 75.2\(\pm\)1.0 & 81.2\(\pm\)2.1 & 69.6\(\pm\)2.8 & 42.4\(\pm\)0.2 & – \\  & CQL\({}_{3}\) & 88.0\(\pm\)1.7 & 75.5\(\pm\)1.7 & 80.0\(\pm\)2.5 & 42.6\(\pm\)0.1 & – \\ \hline \end{tabular}
\end{table}
Table 7: D4RL normalized scores for CQL with the original architecture in Table 3 (CQL) and with the

\begin{table}
\begin{tabular}{c|c|c|c|c|c} \hline Dataset & Algo & Original & Zero & Random & Negative \\ \hline \multirow{2}{*}{button-press} & CQL & 0.88\(\pm\)0.07 & 0.83\(\pm\)0.10 & 0.84\(\pm\)0.09 & 0.94\(\pm\)0.03 \\  & CQL\({}_{3}\) & 0.78\(\pm\)0.10 & 0.90\(\pm\)0.06 & 0.73\(\pm\)0.11 & 0.68\(\pm\)0.11 \\ \hline \multirow{2}{*}{door-open} & CQL & 0.82\(\pm\)0.09 & 0.07\(\pm\)0.07 & 0.12\(\pm\)0.08 & 0.00\(\pm\)0.00 \\  & CQL\({}_{3}\) & 0.47\(\pm\)0.15 & 0.29\(\pm\)0.14 & 0.00\(\pm\)0.00 & 0.38\(\pm\)0.15 \\ \hline \multirow{2}{*}{drawer-close} & CQL & 0.91\(\pm\)0.04 & 0.99\(\pm\)0.01 & 0.96\(\pm\)0.02 & 0.97\(\pm\)0.02 \\  & CQL\({}_{3}\) & 0.99\(\pm\)0.01 & 0.93\(\pm\)0.03 & 0.97\(\pm\)0.02 & 0.98\(\pm\)0.01 \\ \hline \multirow{2}{*}{drawer-open} & CQL & 0.78\(\pm\)0.04 & 0.66\(\pm\)0.07 & 0.65\(\pm\)0.07 & 0.84\(\pm\)0.03 \\  & CQL\({}_{3}\) & 0.86\(\pm\)0.03 & 0.79\(\pm\)0.09 & 0.78\(\pm\)0.06 & 0.80\(\pm\)0.05 \\ \hline \multirow{2}{*}{peg-insert-side} & CQL & 0.08\(\pm\)0.02 & 0.00\(\pm\)0.00 & 0.02\(\pm\)0.01 & 0.00\(\pm\)0.00 \\  & CQL\({}_{3}\) & 0.10\(\pm\)0.02 & 0.05\(\pm\)0.03 & 0.08\(\pm\)0.02 & 0.03\(\pm\)0.02 \\ \hline \multirow{2}{*}{pick-place} & CQL & 0.01\(\pm\)0.00 & 0.06\(\pm\)0.02 & 0.05\(\pm\)0.02 & 0.07\(\pm\)0.03 \\  & CQL\({}_{3}\) & 0.00\(\pm\)0.00 & 0.00\(\pm\)0.00 & 0.00\(\pm\)0.00 & 0.00\(\pm\)0.00 \\ \hline \multirow{2}{*}{push} & CQL & 0.00\(\pm\)0.00 & 0.01\(\pm\)0.00 & 0.01\(\pm\)0.00 & 0.03\(\pm\)0.02 \\  & CQL\({}_{3}\) & 0.03\(\pm\)0.01 & 0.00\(\pm\)0.00 & 0.00\(\pm\)0.00 & 0.01\(\pm\)0.01 \\ \hline \multirow{2}{*}{reach} & CQL & 0.17\(\pm\)0.03 & 0.29\(\pm\)0.04 & 0.25\(\pm\)0.05 & 0.23\(\pm\)0.03 \\  & CQL\({}_{3}\) & 0.35\(\pm\)0.05 & 0.35\(\pm\)0.03 & 0.37\(\pm\)0.05 & 0.22\(\pm\)0.06 \\ \hline \multirow{2}{*}{window-close} & CQL & 1.00\(\pm\)0.00 & 1.00\(\pm\)0.00 & 0.59\(\pm\)0.13 & 0.55\(\pm\)0.13 \\  & CQL\({}_{3}\) & 0.97\(\pm\)0.03 & 0.75\(\pm\)0.12 & 0.77\(\pm\)0.12 & 0.98\(\pm\)0.02 \\ \hline \multirow{2}{*}{window-open} & CQL & 0.83\(\pm\)0.03 & 0.82\(\pm\)0.05 & 0.92\(\pm\)0.02 & 0.83\(\pm\)0.07 \\  & CQL\({}_{3}\) & 0.84\(\pm\)0.07 & 0.94\(\pm\)0.02 & 0.93\(\pm\)0.02 & 0.77\(\pm\)0.09 \\ \hline \multirow{2}{*}{bin-picking} & CQL & 0.00\(\pm\)0.00 & 0.02\(\pm\)0.02 & 0.00\(\pm\)0.00 & 0.00\(\pm\)0.00 \\  & CQL\({}_{3}\) & 0.00\(\pm\)0.00 & 0.00\(\pm\)0.00 & 0.00\(\pm\)0.00 & 0.00\(\pm\)0.00 \\ \hline \multirow{2}{*}{box-close} & CQL & 0.04\(\pm\)0.01 & 0.07\(\pm\)0.02 & 0.01\(\pm\)0.00 & 0.05\(\pm\)0.01 \\  & CQL\({}_{3}\) & 0.03\(\pm\)0.02 & 0.00\(\pm\)0.00 & 0.01\(\pm\)0.00 & 0.01\(\pm\)0.00 \\ \hline \multirow{2}{*}{door-lock} & CQL & 0.75\(\pm\)0.04 & 0.61\(\pm\)0.10 & 0.61\(\pm\)0.11 & 0.60\(\pm\)0.08 \\  & CQL\({}_{3}\) & 0.63\(\pm\)0.06 & 0.64\(\pm\)0.09 & 0.57\(\pm\)0.10 & 0.62\(\pm\)0.09 \\ \hline \multirow{2}{*}{door-unlock} & CQL & 0.87\(\pm\)0.03 & 0.77\(\pm\)0.09 & 0.89\(\pm\)0.04 & 0.93\(\pm\)0.03 \\  & CQL\({}_{3}\) & 0.92\(\pm\)0.02 & 0.73\(\pm\)0.11 & 0.71\(\pm\)0.11 & 0.76\(\pm\)0.11 \\ \hline \multirow{2}{*}{hand-insert} & CQL & 0.13\(\pm\)0.04 & 0.22\(\pm\)0.06 & 0.34\(\pm\)0.04 & 0.32\(\pm\)0.03 \\  & CQL\({}_{3}\) & 0.04\(\pm\)0.03 & 0.08\(\pm\)0.03 & 0.18\(\pm\)0.03 & 0.08\(\pm\)0.02 \\ \hline \end{tabular}
\end{table}
Table 8: Meta-World success rate for CQL with the original architecture in Table 3 (CQL) and with the same architecture as ATAC and PSPI (CQL\({}_{3}\)), i.e., with 3 hidden layers and scaled \(\tanh\)-Gaussian. We find that CQL performs similarly under the two choices of architecture.

## Appendix A

Figure 9: Estimated positive data bias of D4RL and Meta-World datasets given all \(6\) offline RL algorithms, ATAC, PSPI, IQL, CQL, and DT. Datasets marked by “\(\times\)” have infinite positive bias.

\begin{table}
\begin{tabular}{c|c|c|c|c|c} \hline Dataset & Original & Zero & Random & Negative & No-terminal \\ \hline hopper-random & \(\beta=100.0\) & \(\beta=100.0\) & \(\beta=10.0\) & \(\beta=1.0\) & \(\beta=10.0\) \\ \hline hopper-medium & \(\beta=10.0\) & \(\beta=100.0\) & \(\beta=100.0\) & \(\beta=1.0\) & \(\beta=100.0\) \\ \hline hopper-medium-replay & \(\beta=100.0\) & \(\beta=1.0\) & \(\beta=1.0\) & \(\beta=0.1\) & \(\beta=100.0\) \\ \hline hopper-medium-expert & \(\beta=1.0\) & \(\beta=10.0\) & \(\beta=10.0\) & \(\beta=1.0\) & \(\beta=1.0\) \\ \hline hopper-expert & \(\beta=0.1\) & \(\beta=1.0\) & \(\beta=1.0\) & \(\beta=0.1\) & \(\beta=0.1\) \\ \hline walker2d-random & \(\beta=10.0\) & \(\beta=0.1\) & \(\beta=1.0\) & \(\beta=0.1\) & \(\beta=100.0\) \\ \hline walker2d-medium & \(\beta=10.0\) & \(\beta=1.0\) & \(\beta=1.0\) & \(\beta=0.1\) & \(\beta=100.0\) \\ \hline walker2d-medium-replay & \(\beta=100.0\) & \(\beta=10.0\) & \(\beta=10.0\) & \(\beta=100.0\) & \(\beta=100.0\) \\ \hline walker2d-medium-expert & \(\beta=10.0\) & \(\beta=10.0\) & \(\beta=1.0\) & \(\beta=0.1\) & \(\beta=10.0\) \\ \hline walker2d-expert & \(\beta=1.0\) & \(\beta=100.0\) & \(\beta=0.1\) & \(\beta=0.1\) & \(\beta=1.0\) \\ \hline halfcheath-random & \(\beta=0.1\) & \(\beta=0.1\) & \(\beta=0.1\) & \(\beta=0.1\) & – \\ \hline halfcheath-medium & \(\beta=1.0\) & \(\beta=100.0\) & \(\beta=1.0\) & \(\beta=0.1\) & – \\ \hline halfcheath-medium-replay & \(\beta=1.0\) & \(\beta=0.1\) & \(\beta=0.1\) & \(\beta=0.1\) & – \\ \hline halfcheath-expert & \(\beta=0.1\) & \(\beta=100.0\) & \(\beta=0.1\) & \(\beta=0.1\) & – \\ \hline button-press & \(\beta=10.0\) & \(\beta=10.0\) & \(\beta=1.0\) & \(\beta=100.0\) & – \\ \hline door-open & \(\beta=100.0\) & \(\beta=10.0\) & \(\beta=10.0\) & \(\beta=1.0\) & – \\ \hline drawer-close & \(\beta=1.0\) & \(\beta=10.0\) & \(\beta=10.0\) & \(\beta=100.0\) & – \\ \hline drawer-open & \(\beta=10.0\) & \(\beta=1.0\) & \(\beta=10.0\) & \(\beta=10.0\) & – \\ \hline peg-insert-side & \(\beta=1.0\) & \(\beta=1.0\) & \(\beta=1.0\) & \(\beta=1.0\) & – \\ \hline pick-place & \(\beta=1.0\) & \(\beta=10.0\) & \(\beta=1.0\) & \(\beta=1.0\) & – \\ \hline push & \(\beta=1.0\) & \(\beta=1.0\) & \(\beta=1.0\) & \(\beta=1.0\) & – \\ \hline reach & \(\beta=10.0\) & \(\beta=100.0\) & \(\beta=10.0\) & \(\beta=1.0\) & – \\ \hline window-close & \(\beta=1.0\) & \(\beta=1.0\) & \(\beta=1.0\) & \(\beta=1.0\) & – \\ \hline window-open & \(\beta=0.1\) & \(\beta=1.0\) & \(\beta=10.0\) & \(\beta=100.0\) & – \\ \hline bin-picking & \(\beta=1.0\) & \(\beta=1.0\) & \(\beta=1.0\) & \(\beta=1.0\) & – \\ \hline box-close & \(\beta=1.0\) & \(\beta=1.0\) & \(\beta=1.0\) & \(\beta=1.0\) & – \\ \hline door-lock & \(\beta=10.0\) & \(\beta=1.0\) & \(\beta=1.0\) & \(\beta=0.1\) & – \\ \hline door-unlock & \(\beta=100.0\) & \(\beta=10.0\) & \(\beta=10.0\) & \(\beta=10.0\) & – \\ \hline hand-insert & \(\beta=10.0\) & \(\beta=1.0\) & \(\beta=1.0\) & \(\beta=1.0\) & – \\ \hline point-button1 & \(\beta=1.0\) & – & – & – & – \\ \hline point-button2 & \(\beta=1.0\) & – & – & – & – \\ \hline point-circle1 & \(\beta=0.1\) & – & – & – & – \\ \hline point-circle2 & \(\beta=0.1\) & – & – & – & – \\ \hline point-goal1 & \(\beta=0.1\) & – & – & – & – \\ \hline point-goal2 & \(\beta=1.0\) & – & – & – & – \\ \hline point-push1 & \(\beta=100.0\) & – & – & – & – \\ \hline point-push2 & \(\beta=1.0\) & – & – & – & – \\ \hline car-push2 & \(\beta=0.1\) & – & – & – & – \\ \hline car-botbot & \(\beta=0.1\) & – & – & – & – \\ \hline car-circle2 & \(\beta=100.0\) & – & – & – & – \\ \hline car-goal1 & \(\beta=10.0\) & – & – & – & – \\ \hline car-goal2 & \(\beta=1.0\) & – & – & – & – \\ \hline car-push1 & \(\beta=100.0\) & – & – & – & – \\ \hline car-push2 & \(\beta=0.1\) & – & – & – & – \\ \hline \end{tabular}
\end{table}
Table 9: Tuned hyperparameter values for ATAC

\begin{table}
\begin{tabular}{c|c|c|c|c|c|c} \hline Dataset & Original & Zero & Random & Negative & No-terminal \\ \hline hopper-random & \(\beta=10.0\) & \(\beta=10.0\) & \(\beta=100.0\) & \(\beta=100.0\) & \(\beta=100.0\) \\ \hline hopper-medium & \(\beta=100.0\) & \(\beta=10.0\) & \(\beta=100.0\) & \(\beta=1.0\) & \(\beta=100.0\) \\ \hline hopper-medium-replay & \(\beta=10.0\) & \(\beta=10.0\) & \(\beta=100.0\) & \(\beta=1.0\) & \(\beta=1.0\) \\ \hline hopper-medium-expert & \(\beta=1.0\) & \(\beta=10.0\) & \(\beta=100.0\) & \(\beta=1.0\) & \(\beta=1.0\) \\ \hline hopper-expert & \(\beta=1.0\) & \(\beta=1.0\) & \(\beta=1.0\) & \(\beta=0.1\) & \(\beta=1.0\) \\ \hline walker2d-random & \(\beta=10.0\) & \(\beta=10.0\) & \(\beta=10.0\) & \(\beta=1.0\) & \(\beta=1.0\) \\ \hline walker2d-medium & \(\beta=10.0\) & \(\beta=10.0\) & \(\beta=10.0\) & \(\beta=0.1\) & \(\beta=10.0\) \\ \hline walker2d-medium-replay & \(\beta=10.0\) & \(\beta=10.0\) & \(\beta=10.0\) & \(\beta=1.0\) & \(\beta=100.0\) \\ \hline walker2d-medium-expert & \(\beta=1.0\) & \(\beta=1.0\) & \(\beta=10.0\) & \(\beta=0.1\) & \(\beta=10.0\) \\ \hline walker2d-expert & \(\beta=1.0\) & \(\beta=1.0\) & \(\beta=10.0\) & \(\beta=0.1\) & \(\beta=1.0\) \\ \hline halfcheath-random & \(\beta=100.0\) & \(\beta=100.0\) & \(\beta=0.1\) & \(\beta=1.0\) & – \\ \hline halfcheath-medium & \(\beta=1.0\) & \(\beta=10.0\) & \(\beta=1.0\) & \(\beta=0.1\) & – \\ \hline halfcheath-medium-expert & \(\beta=0.1\) & \(\beta=1.0\) & \(\beta=0.1\) & \(\beta=0.1\) & – \\ \hline halfcheath-expert & \(\beta=0.1\) & \(\beta=1.0\) & \(\beta=10.0\) & \(\beta=0.1\) & – \\ \hline bottom-press & \(\beta=0.1\) & \(\beta=10.0\) & \(\beta=1.0\) & \(\beta=100.0\) & – \\ \hline door-open & \(\beta=10.0\) & \(\beta=1.0\) & \(\beta=10.0\) & \(\beta=0.1\) & – \\ \hline drawer-close & \(\beta=1.0\) & \(\beta=1.0\) & \(\beta=10.0\) & \(\beta=10.0\) & – \\ \hline drawer-open & \(\beta=100.0\) & \(\beta=10.0\) & \(\beta=10.0\) & \(\beta=10.0\) & – \\ \hline peg-insert-side & \(\beta=1.0\) & \(\beta=1.0\) & \(\beta=1.0\) & \(\beta=10.0\) & – \\ \hline pick-place & \(\beta=10.0\) & \(\beta=1.0\) & \(\beta=10.0\) & \(\beta=1.0\) & – \\ \hline push & \(\beta=100.0\) & \(\beta=10.0\) & \(\beta=1.0\) & \(\beta=1.0\) & – \\ \hline reach & \(\beta=100.0\) & \(\beta=100.0\) & \(\beta=10.0\) & \(\beta=10.0\) & – \\ \hline window-close & \(\beta=0.1\) & \(\beta=0.1\) & \(\beta=1.0\) & \(\beta=0.1\) & – \\ \hline window-open & \(\beta=10.0\) & \(\beta=10.0\) & \(\beta=10.0\) & \(\beta=0.1\) & – \\ \hline bin-picking & \(\beta=1.0\) & \(\beta=1.0\) & \(\beta=1.0\) & \(\beta=1.0\) & – \\ \hline box-close & \(\beta=1.0\) & \(\beta=1.0\) & \(\beta=1.0\) & \(\beta=1.0\) & – \\ \hline door-lock & \(\beta=1.0\) & \(\beta=1.0\) & \(\beta=10.0\) & \(\beta=10.0\) & – \\ \hline door-unlock & \(\beta=100.0\) & \(\beta=10.0\) & \(\beta=10.0\) & \(\beta=10.0\) & – \\ \hline hand-insert & \(\beta=10.0\) & \(\beta=100.0\) & \(\beta=1.0\) & \(\beta=1.0\) & – \\ \hline point-button1 & \(\beta=1.0\) & – & – & – & – \\ \hline point-button2 & \(\beta=10.0\) & – & – & – & – \\ \hline point-circle1 & \(\beta=1.0\) & – & – & – & – \\ \hline point-circle2 & \(\beta=1.0\) & – & – & – & – \\ \hline point-goal1 & \(\beta=0.1\) & – & – & – & – \\ \hline point-goal2 & \(\beta=1.0\) & – & – & – & – \\ \hline point-push1 & \(\beta=100.0\) & – & – & – & – \\ \hline car-push2 & \(\beta=1.0\) & – & – & – & – \\ \hline \end{tabular}
\end{table}
Table 10: Tuned hyperparameter values for PSPI

\begin{table}
\begin{tabular}{c|c|c|c|c|c|c} \hline Dataset & Original & Zero & Random & Negative & No-terminal \\ \hline hopper-random & \(\alpha=0.1\) & \(\alpha=0.1\) & \(\alpha=100.0\) & \(\alpha=0.1\) & \(\alpha=1.0\) \\ \hline hopper-medium & \(\alpha=1.0\) & \(\alpha=10.0\) & \(\alpha=10.0\) & \(\alpha=10.0\) & \(\alpha=10.0\) \\ \hline hopper-medium-epplay & \(\alpha=10.0\) & \(\alpha=100.0\) & \(\alpha=100.0\) & \(\alpha=10.0\) & \(\alpha=10.0\) \\ \hline hopper-medium-eppert & \(\alpha=10.0\) & \(\alpha=100.0\) & \(\alpha=100.0\) & \(\alpha=10.0\) & \(\alpha=100.0\) \\ \hline hopper-expert & \(\alpha=10.0\) & \(\alpha=1.0\) & \(\alpha=0.1\) & \(\alpha=10.0\) & \(\alpha=100.0\) \\ \hline walker2d-random & \(\alpha=0.1\) & \(\alpha=100.0\) & \(\alpha=10.0\) & \(\alpha=10.0\) & \(\alpha=10.0\) \\ \hline walker2d-medium-replay & \(\alpha=10.0\) & \(\alpha=10.0\) & \(\alpha=1.0\) & \(\alpha=100.0\) & \(\alpha=10.0\) \\ \hline walker2d-medium-expert & \(\alpha=10.0\) & \(\alpha=0.1\) & \(\alpha=1.0\) & \(\alpha=10.0\) & \(\alpha=10.0\) \\ \hline walker2d-expert & \(\alpha=10.0\) & \(\alpha=10.0\) & \(\alpha=100.0\) & \(\alpha=100.0\) & \(\alpha=10.0\) \\ \hline \end{tabular} 
\begin{tabular}{c|c|c|c|c|c} \hline  & & & & & \\         & & & \\         & & & & \\          & & & & \\           & & & & \\           & & & \\            & & & & \\            & & & \\           & & & \\           & & & \\            & & & \\           & & & \\            & & & \\            & & & \\           & & & \\            & & & \\            & & & \\           & & & \\           & & & \\            & & & \\             & & & \\           & & & \\            & & & \\            & & & \\              & & & \\             & & & \\              & & & \\               & & & \\

[MISSING_PAGE_EMPTY:32]

\begin{table}
\begin{tabular}{c|c|c|c|c|c|c} \hline Dataset & BC & Algo & Original & Zero & Random & Negative \\ \hline \multirow{4}{*}{bution-press} & \multirow{4}{*}{0.98\(\pm\)0.00} & \multirow{4}{*}{0.98\(\pm\)0.01} & \multirow{4}{*}{0.99\(\pm\)0.00} & \multirow{4}{*}{1.00\(\pm\)0.00} & \multirow{4}{*}{0.90\(\pm\)0.09} \\  & & ATAC & 1.00\(\pm\)0.00 & 0.90\(\pm\)0.09 & 0.84\(\pm\)0.10 & 0.90\(\pm\)0.09 \\  & & PSPI & 0.98\(\pm\)0.01 & 0.97\(\pm\)0.01 & 0.94\(\pm\)0.02 & 0.88\(\pm\)0.02 \\  & & IQL & 0.98\(\pm\)0.07 & 0.83\(\pm\)0.10 & 0.84\(\pm\)0.09 & 0.94\(\pm\)0.03 \\  & & DT & 1.00\(\pm\)0.00 & 1.00\(\pm\)0.00 & 1.00\(\pm\)0.00 \\ \hline \multirow{4}{*}{door-open} & \multirow{4}{*}{0.81\(\pm\)0.02} & \multirow{4}{*}{0.91\(\pm\)0.00} & \multirow{4}{*}{1.00\(\pm\)0.00} & \multirow{4}{*}{1.00\(\pm\)0.00} & \multirow{4}{*}{0.95\(\pm\)0.04} \\  & & PSPI & 1.00\(\pm\)0.00 & 0.95\(\pm\)0.03 & 0.96\(\pm\)0.02 & 0.81\(\pm\)0.09 \\  & & IQL & 0.94\(\pm\)0.02 & 0.86\(\pm\)0.03 & 0.92\(\pm\)0.03 & 0.92\(\pm\)0.02 \\  & & IQL & 0.82\(\pm\)0.09 & 0.97\(\pm\)0.07 & 0.12\(\pm\)0.08 & 0.00\(\pm\)0.00 \\  & & DT & 1.00\(\pm\)0.00 & 1.00\(\pm\)0.00 & 1.00\(\pm\)0.00 & 1.00\(\pm\)0.00 \\ \hline \multirow{4}{*}{drawer-close} & \multirow{4}{*}{0.99\(\pm\)0.00} & \multirow{4}{*}{0.97\(\pm\)0.01} & \multirow{4}{*}{0.96\(\pm\)0.01} & \multirow{4}{*}{1.00\(\pm\)0.00} & \multirow{4}{*}{1.00\(\pm\)0.00} \\  & & PSPI & 0.77\(\pm\)0.01 & 0.96\(\pm\)0.04 & 1.00\(\pm\)0.00 & 1.00\(\pm\)0.00 \\  & & IQL & 0.98\(\pm\)0.01 & 0.99\(\pm\)0.01 & 0.99\(\pm\)0.00 & 0.97\(\pm\)0.01 \\  & & OQL & 0.91\(\pm\)0.04 & 0.99\(\pm\)0.01 & 0.96\(\pm\)0.02 & 0.97\(\pm\)0.02 \\  & & DT & 0.99\(\pm\)0.01 & 1.00\(\pm\)0.00 & 1.00\(\pm\)0.00 & 1.00\(\pm\)0.00 \\ \hline \multirow{4}{*}{drawer-open} & \multirow{4}{*}{0.88\(\pm\)0.01} & \multirow{4}{*}{0.98\(\pm\)0.01} & \multirow{4}{*}{1.00\(\pm\)0.02} & \multirow{4}{*}{0.95\(\pm\)0.07} & \multirow{4}{*}{0.97\(\pm\)0.06} \\  & & PSPI & 0.61\(\pm\)0.03 & 0.42\(\pm\)0.09 & 0.57\(\pm\)0.07 & 0.74\(\pm\)0.06 \\  & & IQL & 0.93\(\pm\)0.02 & 0.84\(\pm\)0.02 & 0.73\(\pm\)0.04 & 0.52\(\pm\)0.03 \\  & & COL & 0.78\(\pm\)0.04 & 0.66\(\pm\)0.07 & 0.65\(\pm\)0.07 & 0.84\(\pm\)0.03 \\  & & DT & 1.00\(\pm\)0.00 & 1.00\(\pm\)0.00 & 1.00\(\pm\)0.00 & 1.00\(\pm\)0.00 \\ \hline \multirow{4}{*}{peg-insert-side} & \multirow{4}{*}{0.58\(\pm\)0.01} & \multirow{4}{*}{0.58\(\pm\)0.01} & \multirow{4}{*}{1.00\(\pm\)0.03} & \multirow{4}{*}{0.93\(\pm\)0.02} & \multirow{4}{*}{0.91\(\pm\)0.03} \\  & & IQL & 0.60\(\pm\)0.03 & 0.61\(\pm\)0.03 & 0.34\(\pm\)0.02 & 0.11\(\pm\)0.01 \\  & & COL & 0.08\(\pm\)0.02 & 0.00\(\pm\)0.00 & 0.02\(\pm\)0.01 & 0.00\(\pm\)0.00 \\  & & DT & 0.07\(\pm\)0.01 & 0.00\(\pm\)0.00 & 0.01\(\pm\)0.00 & 0.00\(\pm\)0.00 \\ \hline \multirow{4}{*}{pick-place} & \multirow{4}{*}{0.15\(\pm\)0.01} & \multirow{4}{*}{0.18\(\pm\)0.01} & \multirow{4}{*}{0.11\(\pm\)0.02} & \multirow{4}{*}{0.16\(\pm\)0.02} & \multirow{4}{*}{0.17\(\pm\)0.01} & \multirow{4}{*}{0.10\(\pm\)0.01} \\  & & PSPI & 0.47\(\pm\)0.04 & 0.16\(\pm\)0.05 & 0.31\(\pm\)0.04 & 0.14\(\pm\)0.04 \\  & & IQL & 0.11\(\pm\)0.02 & 0.16\(\pm\)0.02 & 0.17\(\pm\)0.01 & 0.10\(\pm\)0.01 \\  & & COL & 0.01\(\pm\)0.00 & 0.06\(\pm\)0.02 & 0.05\(\pm\)0.02 & 0.07\(\pm\)0.03 \\  & & DT & 0.06\(\pm\)0.01 & 0.06\(\pm\)0.01 & 0.06\(\pm\)0.01 & 0.06\(\pm\)0.01 \\ \hline \multirow{4}{*}{push} & \multirow{4}{*}{0.19\(\pm\)0.01} & \multirow{4}{*}{0.18\(\pm\)0.01} & \multirow{4}{*}{0.48\(\pm\)0.08} & \multirow{4}{*}{0.34\(\pm\)0.04} & \multirow{4}{*}{0.27\(\pm\)0.06} \\  & & PSPI & 0.47\(\pm\)0.05 & 0.42\(\pm\)0.04 & 0.35\(\pm\)0.03 & 0.87\(\pm\)0.08 \\  & & IQL & 0.10\(\pm\)0.02 & 0.18\(\pm\)0.02 & 0.14\(\pm\)0.01 & 0.13\(\pm\)0.01 \\  & & COL & 0.00\(\pm\)0.00 & 0.01\(\pm\)0.00 & 0.01\(\pm\)0.00 & 0.03\(\pm\)0.02 \\  & & DT & 0.04\(\pm\)0.01 & 0.06\(\pm\)0.02 & 0.04\(\pm\)0.01 & 0.07\(\pm\)0.01 \\ \hline \multirow{4}{*}{reach} & \multirow{4}{*}{0.59\(\pm\)0.01} & \multirow{4}{*}{0.47\(\pm\)0.03} & \multirow{4}{*}{0.86\(\pm\)0.03} & \multirow{4}{*}{0.81\(\pm\)0.05} & \multirow{4}{*}{0.83\(\pm\)0.03} & \multirow{4}{*}{0.81\(\pm\)0.03} \\  & & IQL & 0.15\(\pm\)0.02 & 0.59\(\pm\)0.04 & 0.57\(\pm\)0.02 & 0.58\(\pm\)0.02 \\  & & COL & 0.17\(\pm\)0.03 & 0.29\(\pm\)0.04 & 0.25\(\pm\)0.05 & 0.23\(\pm\)0.03 \\  & & DT & 0.67\(\Technical Details of Section 3

Here we provide the details of our theory on the robustness of offline RL in Section 3.

**Theorem 1**.: _(Informal) Under Assumption 1 and certain regularity assumptions, if an offline RL algorithm \(Algo\) is set to be sufficiently pessimistic and the data distribution \(\mu\) has a positive bias, for any data reward \(\tilde{r}\in\tilde{\mathcal{R}}\), the policy \(\hat{\pi}\) learned by \(Algo\) from the dataset \(\tilde{\mathcal{D}}\) has performance guarantee \(V_{r}^{\pi^{\prime}}(d_{0})-V_{r}^{\tilde{\pi}}(d_{0})\leq O(\iota)\) as well as safety guarantee \((1-\gamma)\sum_{t=0}^{\iota}\gamma^{t}\mathrm{Prob}\left(\exists\tau\in[0,t], s_{\tau}\notin\text{supp}(\mu)|\hat{\pi}\right)\leq O(\iota)\) for a small \(\iota\) that decreases to zero as the degree of pessimism and dataset size increase._

Below we first provide in Appendix D.1 some background for stating the formal version Theorem 1 in Appendix D.2. Then we present the formal claims on survival instinct and conditions on positive data bias in Appendix D.3 and Appendix D.4, respectively. Finally, we prove Theorem 1 in Appendix D.5. Our discussion here will be focusing the discounted infinite-horizon step. We discuss the finite horizon variant in Appendix E.

### Definitions

First, we introduce some definitions so that we can state the formal version of Theorem 1. We define constrained MDPs.

**Definition 3** (Constrained MDP).: Let \(f,g:\mathcal{S}\times\mathcal{A}\rightarrow[-1,1]\). A CMDP problem \(\mathcal{C}(\mathcal{S},\mathcal{A},f,g,P,\gamma)\) is defined as

\[\max_{\pi}\quad V_{f}^{\pi}(d_{0}),\quad\text{s.t.}\quad V_{g}^{\pi}(d_{0}) \leq 0.\]

Let \(\pi^{\dagger}\) denote its optimal policy. For \(\delta\geq 0\), we define the set of \(\delta\)_-approximately optimal policies_,

\[\Pi_{f,g}^{\dagger}(\delta)=\{\pi:V_{f}^{\pi^{\dagger}}(d_{0})-V_{f}^{\pi}(d_{0 })\leq\delta,V_{g}^{\pi}(d_{0})\leq\delta\}\]

We will drop the subscript \(f,g\) from \(\Pi_{f,g}^{\dagger}\) when it is clear from the context. In addition, we define a _sensitivity function_ to approximately optimal policies,

\[\kappa(\delta)\coloneqq\max_{\pi\in\Pi^{\dagger}(\delta)}V_{f}^{\pi}(d_{0})-V_ {f}^{\pi^{\dagger}}(d_{0})\]

The sensitivity function measures how much the approximately optimal policies (which can slightly violate the constraint of the CMDP) would perform better than the optimal policy of the CMDP (which satisfies the constraint). By definition, \(\kappa(\delta)\in[0,\frac{2}{1-\gamma}]\) and its value can be smaller than \(\frac{2}{1-\gamma}\).

By duality, a CMDP problem is related to a saddle-point problem induced by its constraint. We review the definition of saddle points and this fact below.

**Definition 4**.: Given a bifunction \(\mathcal{L}(x,y):\mathcal{X}\times\mathcal{Y}\rightarrow\mathbb{R}\), we say \((x^{\dagger},y^{\dagger})\in\mathcal{X}\times\mathcal{Y}\) is a _saddle point_ of \(\mathcal{L}\), if for all \(x\in\mathcal{X}\) and \(y\in\mathcal{Y}\) it holds that \(\mathcal{L}(x,y^{\dagger})\leq\mathcal{L}(x^{\dagger},y^{\dagger})\leq \mathcal{L}(x^{\dagger},y)\).

**Lemma 1** (Duality).: _[_39_]_ _Let \(\Pi\) denote the space of policies. Consider a CMDP \(\mathcal{C}\coloneqq\mathcal{C}(\mathcal{S},\mathcal{A},f,g,P,\gamma)\). Define the Lagrangian \(\mathcal{L}(\pi,\lambda)\coloneqq V_{f}^{\pi}(d_{0})-\lambda V_{g}^{\pi}(d_{0})\) for \(\pi\in\Pi\) and \(\lambda\geq 0\). Let \(\pi^{\dagger}\) and \(\lambda^{\dagger}\) denote the optimal policy and Lagrange multiplier, i.e.,_

\[\pi^{\dagger}\in\arg\max_{\pi}\min_{\lambda\geq 0}\mathcal{L}(\pi,\lambda)\quad \text{and}\quad\lambda^{\dagger}\in\arg\min_{\lambda\geq 0}\max_{\pi}\mathcal{L}( \pi,\lambda)\]

_Then together they form a saddle point \((\pi^{\dagger},\lambda^{\dagger})\) to the Lagrangian \(\mathcal{L}\)._

We also introduce a definition to characterize the degree of pessimism of offline RL algorithms. We call it an admissibility condition defined below, which measures the size of the data-consistent reward functions that the offline RL algorithm can have a small regret.

**Definition 5**.: [Admissibility] For \(R\geq 0\), we say an offline RL algorithm \(Algo\) is \(R\)_-admissible_ with respect to \(\tilde{\mathcal{R}}\), if for any \(\tilde{r}\in\tilde{\mathcal{R}}\), given \(\tilde{\mathcal{D}}\), \(Algo\) learns a policy \(\hat{\pi}\) satisfying the following with high probability: for any policy \(\pi\in\Pi\),

\[\max_{\tilde{r}\in\mathcal{R}_{R}(\tilde{r})}V_{\tilde{r}}^{\pi}(d_{0})-V_{ \tilde{r}}^{\tilde{\pi}}(d_{0})\leq\mathcal{E}(\pi,\mu),\]where we define a data-consistent reward class

\[\mathcal{R}_{R}(\tilde{r})\coloneqq\left\{\tilde{r}:\tilde{r}(s,a)=\tilde{r}(s,a),\forall(s,a)\in\text{supp}(\mu)\text{ and }\tilde{r}(s,a)\in\left[-R,1\right],\forall(s,a)\in \mathcal{S}\times\mathcal{A}\right\},\]

\(\mathcal{E}(\pi,\mu)\) is some regret upper bound such that \(\mathcal{E}(\pi,\mu)=o(1)\) if \(\sup_{s\in\mathcal{S},a\in\mathcal{A}}\frac{d^{\pi}(s,a)}{\mu(s,a)}<\infty\), and \(o(1)\) denotes a term that vanishes as the dataset size becomes infinite.

In Appendix F, we provide proofs of the degrees of admissibility for various existing algorithms, including model-free algorithms ATAC [2], VI-LCB [31] PPI/PQI [13], PSPI [7], as well as model-algorithms, ARMOR [34, 40], MOPO [32], MOReL [33], and CPPO [14]. We show that their degree of admissibility increases as the algorithm becomes more pessimistic (by setting the hyperparameters).

Finally, we recall the definition of positive data bias in Section 3 for completeness, where the implicit CMDP \(\mathcal{C}_{\mu}(\tilde{r})\) is defined in (1).

**Definition 2** (Positive Data Bias).: A distribution \(\mu\) is \(\frac{1}{\epsilon}\)_-positively biased_ w.r.t. a reward class \(\tilde{\mathcal{R}}\) if

\[\max_{\tilde{r}\in\tilde{\mathcal{R}}}\max_{\pi\in\Pi^{\dagger}_{r,c_{\mu}}( \delta)}V_{r}^{\pi^{*}}(d_{0})-V_{r}^{\pi}(d_{0})\leq\epsilon+O(\delta)\] (2)

for all \(\delta\geq 0\), where \(\Pi^{\dagger}_{r,c_{\mu}}(\delta)\) denotes the set of \(\delta\)-approximately optimal policy of \(\mathcal{C}_{\mu}(\tilde{r})\).

Note that the CMDP \(\mathcal{C}_{\mu}(\tilde{r})\) is feasible because of the assumption on concentrability (Assumption 1). That is, we know \(\pi^{*}\) is feasible policy, as \((1-\gamma)V_{c_{\mu}}^{\pi^{*}}(d_{0})=\mathbb{E}_{d^{*}}[\mathds{1}[(s,a) \notin\text{supp}(\mu)]]=0\) is implied by \(\sup_{s,a}\frac{d^{\pi^{*}}(s,a)}{\mu(s,a)}<\infty\).

### Formal Statement of the Main Theorem

Now we state the formal version of Theorem 1.

**Assumption 2** (Data Assumption).:
1. For all \(\tilde{r}\in\tilde{\mathcal{R}}\), \(\tilde{r}(s,a)\in[-1,1]\) for all \(s\in\mathcal{S}\) and \(a\in\mathcal{A}\).
2. The data distribution \(\mu\) is \(\frac{1}{\epsilon}\)-positively biased with respect to \(\tilde{\mathcal{R}}\).
3. There is a policy \(\pi\) satisfying \(V_{c_{\mu}}\leq 0\).
4. For any \(\tilde{r}\in\tilde{\mathcal{R}}\), the CMDP \(\mathcal{C}_{\mu}(\tilde{r})\coloneqq\mathcal{C}(\mathcal{S},\mathcal{A}, \tilde{r},c_{\mu},P,\gamma)\) in (1) has a sensitivity function \(\kappa(\delta)\leq K\) for some \(K<\infty\) and \(\lim_{\delta\to 0^{+}}\kappa(\delta)=0\). In addition, its solution \(\pi^{\dagger}\) satisfies \(\sup_{s,a}\frac{d^{\pi^{\dagger}}(s,a)}{\mu(s,a)}<\infty\).

**Theorem 2** (Main Result).: _Under the data assumption in Assumption 2, consider an offline RL algorithm \(Algo\) that is sufficiently pessimistic to be \((\frac{K}{\delta}+2)\)-admissible with respect to \(\tilde{\mathcal{R}}\). Then for any \(\tilde{r}\in\tilde{\mathcal{R}}\), with high probability, the policy \(\hat{\pi}\) learned by \(Algo\) from the dataset \(\tilde{\mathcal{D}}\coloneqq\{(s,a,\tilde{r},s^{\prime})|(s,a)\sim\mu,\tilde{ r}=\tilde{r}(s,a),s^{\prime}\sim P(\cdot|s,a)\}\) has both performance guarantee_

\[V_{r}^{\pi^{*}}(d_{0})-V_{r}^{\hat{\pi}}(d_{0})\leq\epsilon+O(\iota)\]

_and safety guarantee_

\[(1-\gamma)\sum_{t=0}^{\infty}\gamma^{t}\mathrm{Prob}\left(\exists\tau\in[0,t ]s_{\tau}\notin\text{supp}(\mu)|\hat{\pi}\right)\leq\iota\]

_where \(\iota\coloneqq\delta+\kappa(\delta)+o(1)\) and \(o(1)\) denotes a term that vanishes as the dataset size becomes infinite._

In Assumption 2, the first assumption supposes the data reward (which may be the wrong reward) is bounded; the second assumption is that the data distribution is positively biased in view of the reward class \(\tilde{\mathcal{R}}\). Finally, we make a regularity assumption on the convergence of \(\kappa(\delta)\) to zero and \(\sup_{s,a}\frac{d^{\pi^{\dagger}}(s,a)}{\mu(s,a)}<\infty\) to rule out some unrealistic \(\tilde{\mathcal{R}}\) and \(P\) combinations. We note that the condition on the bounded density ratio is always true for tabular problem, and a degenerate case can happen when the state-action space is, e.g., continuous. On the other hand, a degenerate case that \(\kappa\) is discontinuous at \(\delta=0\) can happen due to the infinite problem horizon in the worst case; roughly speaking, this happens when it is impossible to determine the constraint violation using a finite-horizon estimator. We provide the following lemma to quantify the rate of \(\kappa(\delta)\), which is proved in Appendix D.6. In Appendix E, we show this regularity condition is always true in the finite horizon setting.

**Lemma 2**.: _Suppose there is \(\lambda^{\dagger}\leq L\) with some \(L<\infty\) such that \(\lambda^{\dagger}\in\arg\min_{\lambda\geq 0}\max_{\pi}V_{\tilde{r}}^{\pi}(d_{0}) -\lambda V_{c_{\mu}}^{\pi}(d_{0})\). Then \(\kappa(\delta)\leq L\delta\)._

Under the data assumption Assumption 2, Theorem 2 shows that if the offline RL algorithm \(Algo\) is set to be sufficiently pessimistic (relative to the upper bound of the sensitivity function \(\kappa\), which we know is at most \(\frac{1}{1-\gamma}\) by the first assumption in Assumption 2), then \(Algo\) have both a performance guarantee to the true reward \(r\) and the a safety guarantee that the learned policy would have a small probability of leaving the data support.

We recall from Section 3 that Theorem 2 is the effects of the survival instinct of offline RL and the positive bias in the data distribution. As we will show, the admissibility condition in Theorem 2 would ensure that the survival instinct, which is what we stated informally as Proposition 1. In addition, we will provide concrete sufficient conditions for the data distribution being positively bias, as we alluded in Section 3.3. We discuss these details below and then we prove Theorem 2 in Appendix D.5.

### Survival Instinct

We show the formal version of Proposition 1 which says that if the offline algorithm is admissible then it has a survival instinct. That is, it implicitly solves the CMDP in (1) even though the constraint is never explicitly modeled in the algorithm. In Appendix F, we prove the admissibility for various existing algorithms, including model-free algorithms ATAC [2], VI-LCB [31] PPI/PQI [13], PSPI [7], as well as model-algorithms, ARMOR [34, 40], MOPO [32], MOReL [33], and CPPO [14].

**Proposition 2**.: _For \(\tilde{r}\in\tilde{\mathcal{R}}\), assume the CMDP \(\mathcal{C}_{\mu}(\tilde{r}):=\mathcal{C}(\mathcal{S},\mathcal{A},\tilde{r}, c_{\mu},P,\gamma)\) has a sensitivity function such that \(\lim_{\delta\to 0^{+}}\kappa(\delta)=0\) and \(\kappa(\delta)\leq K\) for some \(K\leq\frac{1}{1-\gamma}\). Consider an offline RL algorithm \(Algo\) that is \((\frac{K}{\delta}+2)\)-admissible with respect to \(\tilde{\mathcal{R}}\). Then, with high probability, the policy \(\hat{\pi}\) learned by \(Algo\) with the dataset \(\tilde{\mathcal{D}}\coloneqq\{(s,a,\tilde{r},s^{\prime})|(s,a)\sim\mu,\tilde {r}=\tilde{r}(s,a),s^{\prime}\sim P(\cdot|s,a)\}\) satisfies_

\[V_{\tilde{r}}^{\pi^{\dagger}}(d_{0})-V_{\tilde{r}}^{\hat{\pi}}( d_{0}) \leq o(1)\] \[V_{c_{\mu}}^{\hat{\pi}}(d_{0}) \leq\delta+\kappa(\delta)+o(1)\]

_where \(o(1)\) denotes a term that vanishes as the dataset size becomes infinite._

Proof of Proposition 2.: To begin, we introduce some extra notations. Given \(\mathcal{C}_{\mu}(\tilde{r})\), we define a relaxed CMDP problem: \(\max_{\pi}V_{\tilde{r}}^{\pi}(d_{0}),\mathrm{s.t.}V_{c_{\mu}}^{\pi}(d_{0})\leq\delta\) and its Lagrangian \(\mathcal{L}_{\delta}(\pi,\lambda)\coloneqq V_{\tilde{r}}^{\pi}(d_{0})-\lambda (V_{c_{\mu}}^{\pi}(d_{0})-\delta)\), where \(\lambda\geq 0\) denotes the Lagrange multiplier. Let \(\pi_{\delta}^{\dagger}\) and \(\lambda_{\delta}^{\dagger}\) denote the optimal policy and Lagrange multiplier of this relaxed CMDP, and let \(\pi^{\dagger}\) denote the optimal policy to the CMDP \(\mathcal{C}_{\mu}(\tilde{r})\). Finally we define a shorthand \(r^{\dagger}(s,a)\coloneqq\tilde{r}(s,a)-(\lambda_{\delta}^{\dagger}+1)c_{\mu }(s,a)\).

Now we bound the regret and the constraint violation of \(\hat{\pi}\) in the CMDP \(\mathcal{C}_{\mu}(\tilde{r})\). The following holds for any \(\delta\geq 0\). For the regret, we can derive

\[V_{\tilde{r}}^{\pi^{\dagger}}(d_{0})-V_{\tilde{r}}^{\hat{\pi}}( d_{0}) =V_{\tilde{r}}^{\pi^{\dagger}}(d_{0})-(\lambda_{\delta}^{\dagger }+1)V_{c_{\mu}}^{\pi^{\dagger}}(d_{0})-V_{\tilde{r}}^{\hat{\pi}}(d_{0})\] \[\leq V_{\tilde{r}}^{\pi^{\dagger}}(d_{0})-(\lambda_{\delta}^{ \dagger}+1)V_{c_{\mu}}^{\pi^{\dagger}}(d_{0})-V_{\tilde{r}}^{\hat{\pi}}(d_{0}) +(\lambda_{\delta}^{\dagger}+1)V_{c_{\mu}}^{\hat{\pi}}(d_{0})\hskip 14.226378pt(V_{c_{ \mu}}^{\hat{\pi}}(d_{0})\geq 0)\] \[=V_{r^{\dagger}}^{\pi^{\dagger}}(d_{0})-V_{r^{\dagger}}^{\hat{\pi} }(d_{0}).\]Here we use the additivity of value function over reward, namely, for any reward functions \(r_{1},r_{2}\) and policy \(\pi\), we have \(V_{r_{1}}^{\pi}+V_{r_{2}}^{\pi}=V_{r_{1}+r_{2}}^{\pi}\). Similarly for the constraint violation, we can derive

\[V_{c_{\mu}}^{\hat{\pi}}(d_{0}) =(V_{c_{\mu}}^{\hat{\pi}}(d_{0})-\delta)+\delta\] \[=(\lambda_{\delta}^{\downarrow}+1)(V_{c_{\mu}}^{\hat{\pi}}(d_{0}) -\delta)-\lambda_{\delta}^{\uparrow}(V_{c_{\mu}}^{\hat{\pi}}(d_{0})-\delta)+\delta\] \[=-V_{r}^{\hat{\pi}}(d_{0})+(\lambda_{\delta}^{\uparrow}+1)(V_{c_{ \mu}}^{\hat{\pi}}(d_{0})-\delta)+V_{r}^{\hat{\pi}}(d_{0})-\lambda_{\delta}^{ \uparrow}(V_{c_{\mu}}^{\hat{\pi}}(d_{0})-\delta)+\delta\] \[=\mathcal{L}_{\delta}(\hat{\pi},\lambda_{\delta}^{\downarrow})- \mathcal{L}_{\delta}(\hat{\pi},\lambda_{\delta}^{\uparrow}+1)+\delta\] \[\leq\mathcal{L}_{\delta}(\pi_{\delta}^{\uparrow},\lambda_{\delta} ^{\uparrow}+1)-\mathcal{L}_{\delta}(\hat{\pi},\lambda_{\delta}^{\downarrow}+1)+\delta\] \[=V_{r_{1}}^{\pi_{\delta}^{\uparrow}}(d_{0})-V_{r^{\uparrow}}^{ \hat{\pi}}(d_{0})+\delta\] \[\leq V_{r^{\uparrow}}^{\pi^{\uparrow}}(d_{0})-V_{r^{\uparrow}}^{ \hat{\pi}}(d_{0})+\delta+\kappa(\delta)\]

where the first inequality is due to that \((\pi_{\delta}^{\uparrow},\lambda_{\delta}^{\downarrow})\) is a saddle point to \(\mathcal{L}_{\delta}\) (see Definition 4), and the second inequality follows from

\[V_{r^{\uparrow}}^{\pi_{\delta}^{\uparrow}}(d_{0}) =V_{r}^{\pi_{\delta}^{\uparrow}}(d_{0})-(\lambda_{\delta}^{ \uparrow}+1)V_{c_{\mu}}^{\pi_{\delta}^{\downarrow}}(d_{0}),\] \[\leq V_{r}^{\pi_{\delta}^{\uparrow}}(d_{0})-(\lambda_{\delta}^{ \uparrow}+1)V_{c_{\mu}}^{\pi^{\uparrow}}(d_{0})\] (using

\[V_{c_{\mu}}^{\pi^{\uparrow}}\leq V_{c_{\mu}}^{\pi^{\uparrow}}\] ) \[\leq V_{r^{\uparrow}}^{\pi^{\uparrow}}(d_{0})+\kappa(\delta)-( \lambda_{\delta}^{\uparrow}+1)V_{c_{\mu}}^{\pi^{\uparrow}}(d_{0})\] (Definition of

\[\kappa\]

.)

Next we bound on \(V_{r^{\uparrow}}^{\pi^{\uparrow}}(d_{0})-V_{r^{\uparrow}}^{\hat{\pi}}(d_{0})\). By Lemma 5 and Assumption 2, we know \(\lambda_{\delta}^{\uparrow}\leq\frac{K}{\delta}\), so \(r^{\uparrow}(s,a)\in[-\frac{K}{\delta}-2,1]\). This and the definition of \(c_{\mu}\) together imply that \(r^{\uparrow}\in\mathcal{R}_{\mu}(\tilde{r},\frac{K}{\delta})\). Since the offline RL algorithm \(Algo\) is \(\frac{K}{\delta}\)-admissible, we have \(V_{r^{\uparrow}}^{\pi^{\uparrow}}(d_{0})-V_{r^{\uparrow}}^{\hat{\pi}}(d_{0}) \leq\mathcal{E}(\pi^{\uparrow},\mu)\). Finally, we use the fact that \(\pi^{\uparrow}\) by definition satisfies \(\sup_{s,a}\frac{d^{\pi^{\uparrow}}(s,a)}{\mu(s,a)}<\infty\), so \(\mathcal{E}(\pi^{\uparrow},\mu)=o(1)\). Thus, we have proved

\[V_{\tilde{r}}^{\pi^{\uparrow}}(d_{0})-V_{r}^{\hat{\pi}}(d_{0}) \leq o(1)\] \[V_{c_{\mu}}^{\hat{\pi}}(d_{0}) \leq o(1)+\delta+\kappa(\delta)\]

That is, \(\hat{\pi}\in\Pi_{\iota}(\mathcal{C}_{\mu}(\tilde{r}))\) with \(\iota=\delta+\kappa(\delta)+o(1)\).

### Implicit Data Bias

Below we provide example conditions for data distributions to have a positive implicit bias.

**Proposition 3**.: _Under Assumption 1, the followings are true._

1. _(RL setup) Suppose for any_ \(\tilde{r}\) _in_ \(\tilde{\mathcal{R}}\)_, there is_ \(h:\mathcal{S}\to\mathbb{R}\) _such that_ \[|r(s,a)+\gamma\mathbb{E}_{s^{\prime}\sim\mathcal{P}|s,a}[h(s)]-h(s)-\tilde{r}( s,a)|\leq\begin{cases}\epsilon_{1},&s,a\in\text{supp}(\mu)\\ \epsilon_{2},&\text{otherwise}\end{cases}\] _for some_ \(\epsilon_{1},\epsilon_{2}<\infty\)_. Then_ \(\mu\) _is_ \(\frac{1-\gamma}{2\epsilon_{1}}\)_-positively biased to_ \(\tilde{\mathcal{R}}\)_._
2. _(IL setup) If_ \(\mu=d^{\pi^{*}}\)_, then_ \(\mu\) _is_ \(\infty\)_-positively biased with respect to_ \(\tilde{\mathcal{R}}=\{\tilde{r}:\tilde{r}:\mathcal{S}\times\mathcal{A}\to[-1,1]\}\)_. If_ \(\mu=d^{\pi^{*}}\) _such that_ \(V_{r}^{\pi^{*}}(d_{0})-V_{r}^{\pi^{*}}(d_{0})\leq\frac{\epsilon^{\prime}}{1-\gamma}\) _and_ \(\pi^{e}\) _is deterministic, then_ \(\mu\) _is_ \(\frac{1-\gamma}{\epsilon^{\prime}}\)_-positively biased to the previous_ \(\tilde{\mathcal{R}}\)_._
3. _(Length bias) Suppose_ \(V_{c_{\mu}}^{\pi}(d_{0})\leq\delta\) _implies_ \(V_{r}^{*}(d_{0})-V_{r}^{\pi}(d_{0})\leq\frac{\epsilon^{\prime}+O(\delta)}{1-\gamma}\)_. Then_ \(\mu\) _is_ \(O\left(\frac{1-\gamma}{\epsilon^{\prime}}\right)\)_-positively biased with respect to_ \(\tilde{\mathcal{R}}=\{\tilde{r}:\tilde{r}:\mathcal{S}\times\mathcal{A}\to[-1,1]\}\)_. Below we list some sufficient intervention conditions on data collection that leads to the length bias:__._

1. _For all_ \((s,a)\in\text{supp}(\mu)\)_,_ \(R_{\max}-r(s,a)\leq\epsilon^{\prime}\)_, where_ \(R_{\max}=\sup_{s\in\mathcal{S},a\in\mathcal{A}}r(s,a)\leq 1\)_._
2. _For all_ \((s,a)\in\text{supp}(\mu)\)_,_ \(Q_{r}^{*}(s,a)-V_{r}^{*}(s)\leq\epsilon^{\prime}\)_._

The first example is the typical RL assumption, which says \(\mu\) always has a positive bias to \(\tilde{\mathcal{R}}\) if all rewards in \(\tilde{\mathcal{R}}\) provide the same ranking of policies in the support [41]. The second assumes the data is collectd by a single behavior policy, which includes the typical IL assumption that the data is collected by an optimal policy. Finally, the third example describes a length bias. Note that \(V_{c_{p}}^{\pi}(s,a)\) measures the probability of going out of data support after taking \(a\) at \(s\), which is disproportional to expected length the agent can survive. This condition assumes longer trajectories in the data to have smaller optimality gap.

This length bias condition is typically satisfied when intervention is taken in the data collection process (despite the data collection policy being suboptimal), as we saw in the motivating example in Fig. 1 and Section 3.1, and empirically in Section 4. We give two sufficient conditions on such interventions. The first one is that the data collection is stopped when the instantaneous rewards is far from the maximum rewards, and the second is that the data collection agent does not take an action that no policy (even the optimal one) can perform well in the future. The consequence of these types of interventions is that longer trajectories perform better generally. These conditions are satisfied in hopper and walker2d dataset (except for -medium-replay) in Section 4, as well as in the grid world example in Section 3.1.

The proof of Proposition 3 is based on the performance difference lemma, which can be proved by a telescoping sum decomposition.

**Lemma 3** (Performance Difference Lemma).: _Consider a discounted infinite-horizon MDP \(\mathcal{M}=(\mathcal{S},\mathcal{A},r,P,\gamma)\). For any policy \(\pi\) and any \(h:\mathcal{S}\to\mathbb{R}\), it holds that_

\[V_{r}^{\pi}(d_{0})-h(d_{0})=\frac{1}{1-\gamma}\mathbb{E}_{s,a\sim d^{\pi}}[r(s,a)+\gamma\mathbb{E}_{s^{\prime}\sim P|s,a}[h(s^{\prime})]-h(s)].\]

_In particular, this implies for any policies \(\pi,\pi^{\prime}\),_

\[V_{r}^{\pi}(d_{0})-V_{r}^{\pi^{\prime}}(d_{0})=\frac{1}{1-\gamma}\mathbb{E}_{s,a\sim d^{\pi}}[Q_{r}^{\pi^{\prime}}(s,a)-V_{r}^{\pi^{\prime}}(s)].\]

#### d.4.1 Proof of RL setup in Proposition 3

Proof.: Given \(\tilde{r}\in\tilde{\mathcal{R}}\), consider some \(h:\mathcal{S}\times\mathcal{A}\to\mathbb{R}\) satisfying the assumption. Define

\[\hat{r}(s,a)=r(s,a)+\gamma\mathbb{E}_{s^{\prime}\sim P|s,a}[h(s)]-h(s).\]

Notice

\[(1-\gamma)(V_{r}^{\pi^{*}}(d_{0})-V_{r}^{\pi}(d_{0}))\] \[=(1-\gamma)(V_{r}^{\pi^{*}}(d_{0})-h(d_{0})-V_{r}^{\pi}(d_{0})+h( d_{0}))\] \[=\mathbb{E}_{s,a\sim d^{\pi^{*}}}[r(s,a)+\gamma\mathbb{E}_{s^{ \prime}\sim P|s,a}[h(s^{\prime})]-h(s)]-\mathbb{E}_{s,a\sim d^{\pi}}[r(s,a)+ \gamma\mathbb{E}_{s^{\prime}\sim P|s,a}[h(s^{\prime})]-h(s)]\] \[=\mathbb{E}_{s,a\sim d^{\pi^{*}}}[\hat{r}(s,a)]-\mathbb{E}_{s,a \sim d^{\pi}}[\hat{r}(s,a)]\] \[=(1-\gamma)(V_{\tilde{r}}^{\pi^{*}}(d_{0})-V_{\tilde{r}}^{\pi}(d_ {0}))\]

By Lemma 3, for any \(\pi\in\Pi^{\dagger}(\delta)\), it holds

\[(1-\gamma)(V_{r}^{\pi^{*}}(d_{0})-V_{r}^{\pi}(d_{0}))\] \[=(1-\gamma)(V_{\tilde{r}}^{\pi^{*}}(d_{0})-V_{\tilde{r}}^{\pi}(d_ {0}))\] \[=\mathbb{E}_{s,a\sim d^{\pi^{*}}}[\hat{r}(s,a)]-\mathbb{E}_{s,a \sim d^{\pi}}[\hat{r}(s,a)]\] \[=(1-\gamma)(V_{\tilde{r}}^{\pi^{*}}(d_{0})-V_{\tilde{r}}^{\pi}(d_ {0}))+\mathbb{E}_{s,a\sim d^{\pi^{*}}}[\hat{r}(s,a)-\tilde{r}(s,a)]-\mathbb{E }_{s,a\sim d^{\pi}}[\hat{r}(s,a)-\tilde{r}(s,a)]\]Since \(\pi^{*}\) is assumed to be in the support of \(\mu\) in Assumption 1, we have

\[\mathbb{E}_{s,a\sim d^{\pi^{*}}}[\hat{r}(s,a)-\tilde{r}(s,a)]- \mathbb{E}_{s,a\sim d^{\pi}}[\hat{r}(s,a)-\tilde{r}(s,a)]\] \[=\mathbb{E}_{s,a\sim d^{\pi^{*}}}[\tilde{r}(s,a)-\tilde{r}(s,a)](s,a)\in\text{supp}(\mu)]-\mathbb{E}_{s,a\sim d^{\pi}}[\hat{r}(s,a)-\tilde{r}(s,a )](s,a)\in\text{supp}(\mu)]\] \[\quad-\mathbb{E}_{s,a\sim d^{\pi}}[\tilde{r}(s,a)-\tilde{r}(s,a) ](s,a)\notin\text{supp}(\mu)]\] \[\leq 2\epsilon_{1}-\sup_{s,a}|\hat{r}(s,a)-\tilde{r}(s,a)|\times \mathbb{E}_{s,a\sim d^{\pi}}[\mathbbm{1}[s,a\notin\text{supp}(\mu)]]\] \[\leq 2\epsilon_{1}+(1-\gamma)\epsilon_{2}V^{\pi}_{c_{\mu}}(d_{0})\]

Putting these two inequalities together, we have shown

\[(1-\gamma)(V^{\pi^{*}}_{r}(d_{0})-V^{\pi}_{r}(d_{0}))\leq(1-\gamma)(V^{\pi^{*} }_{\tilde{r}}(d_{0})-V^{\pi}_{\tilde{r}}(d_{0}))+2\epsilon+\epsilon_{2}(1- \gamma)V^{\pi}_{c_{\mu}}(d_{0})\]

Since \(\pi\in\Pi^{\dagger}(\delta)\), it implies

\[(1-\gamma)(V^{\pi^{*}}_{r}(d_{0})-V^{\pi}_{r}(d_{0}))\leq(1-\gamma)\delta+2 \epsilon_{1}+(1-\gamma)\epsilon_{2}\delta=2\epsilon_{1}+O(\delta)\]

Thus, the distribution is \(\frac{1-\gamma}{2\epsilon_{1}}\)-positively biased. 

#### d.4.2 Proof of IL setup in Proposition 3

Proof.: Let \(\mu=d^{\pi^{*}}\). We can write

\[(1-\gamma)(V^{\pi^{*}}_{r}(d_{0})-V^{\pi}_{r}(d_{0}))\] \[=\mathbb{E}_{d^{\pi}}[V^{\pi^{*}}_{r}(s)-Q^{\pi^{*}}_{r}(s,a)]\] \[=\mathbb{E}_{d^{\pi}}[V^{\pi^{*}}_{r}(s)-Q^{\pi^{*}}_{r}(s,a)|(s, a)\in\text{supp}(\mu)]+\mathbb{E}_{d^{\pi}}[V^{\pi^{*}}_{r}(s)-Q^{\pi^{*}}_{r}(s,a) ](s,a)\notin\text{supp}(\mu)]\] \[\leq\delta\]

So the distribution is \(\infty\)-positively biased. For the case with the suboptimal expert \(\pi^{e}\), we can derive similarly

\[(1-\gamma)(V^{\pi^{*}}_{r}(d_{0})-V^{\pi}_{r}(d_{0}))\] \[=(1-\gamma)(V^{\pi^{*}}_{r}(d_{0})-V^{\pi^{*}}_{r}(d_{0}))+(1- \gamma)(V^{\pi^{*}}_{r}(d_{0})-V^{\pi}_{r}(d_{0}))\] \[\leq\epsilon^{\prime}+\mathbb{E}_{d^{\pi}}[V^{\pi^{e}}_{r}(s)-Q^ {\pi^{e}}_{r}(s,a)]\] \[=\epsilon^{\prime}+\mathbb{E}_{d^{\pi}}[V^{\pi^{e}}_{r}(s)-Q^{\pi ^{e}}_{r}(s,a)](s,a)\in\text{supp}(\mu)]+\mathbb{E}_{d^{\pi}}[V^{\pi^{e}}_{r}(s )-Q^{\pi^{e}}_{r}(s,a)](s,a)\notin\text{supp}(\mu)]\] \[\leq\epsilon^{\prime}+\delta\]

Therefore, \(\mu\) is \(\frac{1-\gamma}{\epsilon^{\prime}}\)-positively biased. 

#### d.4.3 Proof of Length Bias in Proposition 3

**Item 3a**

Proof.: Suppose \(R_{\max}-r(s,a)\leq\epsilon^{\prime}\) for \((s,a)\in\text{supp}(\mu)\).

\[(1-\gamma)(V^{\pi^{*}}_{r}(d_{0})-V^{\pi}_{r}(d_{0}))\] \[=\mathbb{E}_{d^{*}}[r(s,a)]-\mathbb{E}_{d^{\pi}}[r(s,a)]\] \[=\mathbb{E}_{d^{*}}[r(s,a)]-\mathbb{E}_{d^{\pi}}[r(s,a)](s,a)\in \text{supp}(\mu)]-\mathbb{E}_{d^{\pi}}[r(s,a)|(s,a)\notin\text{supp}(\mu)]\] \[\leq R_{\max}-\mathbb{E}_{d^{\pi}}[(R_{\max}-\epsilon^{\prime})| (s,a)\in\text{supp}(\mu)]-0\] \[=\mathbb{E}_{d^{*}}[\epsilon^{\prime}|(s,a)\in\text{supp}(\mu)]+ [R_{\max}|(s,a)\notin\text{supp}(\mu)]\] \[\leq\epsilon^{\prime}+(1-\gamma)R_{\max}\delta\]

**Item 3b**

Proof.: Suppose \(s,a\in\text{supp}(\mu)\) only if \(V_{r}^{*}(s)-Q_{r}^{*}(s,a)\leq\epsilon^{\prime}\). Then

\[(1-\gamma)(V_{r}^{\pi^{*}}(d_{0})-V_{r}^{\pi}(d_{0}))\] \[=\mathbb{E}_{d^{\pi}}[V_{r}^{\pi^{*}}(s)-Q_{r}^{\pi^{*}}(s,a)]\] \[=\mathbb{E}_{d^{\pi}}[V_{r}^{\pi^{*}}(s)-Q_{r}^{\pi^{*}}(s,a)|(s,a )\in\text{supp}(\mu)]+\mathbb{E}_{d^{\pi}}[V_{r}^{\pi^{*}}(s)-Q_{r}^{\pi^{*}}(s,a)|(s,a)\notin\text{supp}(\mu)]\] \[\leq\epsilon^{\prime}+\delta\]

Note that more broadly, we can relax the above condition to that \(Q_{c_{\mu}}^{*}(s,a)-V_{c_{\mu}}^{*}(s)<\alpha\) implies \(V_{r}^{*}(s)-Q_{r}^{*}(s,a)\leq\beta+O(\alpha)\), for a potentially smaller \(\beta\), where we recall that \(V_{c_{\mu}}^{*},Q_{c_{\mu}}^{*}\) are the optimal value functions to (minimizing) the cost \(c_{\mu}\) and \(V_{r}^{*},Q_{r}^{*}\) are the optimal value functions to (maximizing) the reawrd \(r\). We can use the lemma below to prove the degree of positive bias is \(\frac{1-\gamma}{\beta}\) in this more general case. The above is a special case for \(\alpha=1\), since \(c_{\mu}(s,a)=1\) when \(s,a\) is not in the support of \(\mu\).

**Lemma 4**.: _Define \(\mathcal{A}_{s}^{\alpha}=\{a\in\mathcal{A}:Q_{c_{\mu}}^{*}(s,a)-V_{c_{\mu}}^{* }(s)<\alpha\}\). Suppose for all \(s\in\mathcal{S}\), \(a\in\mathcal{A}_{s}^{\alpha}\) satisfies \(V_{r}^{\pi^{\prime}}(s)-Q_{r}^{\pi^{\prime}}(s,a)\leq\beta\). For \(\pi\) such that \(V_{c_{\mu}}^{\pi}(d_{0})\leq\delta\), it holds for any \(\pi^{\prime}\)_

\[V_{r}^{\pi^{\prime}}(d_{0})-V_{r}^{\pi}(d_{0})\leq\frac{\beta+ \delta/\alpha}{1-\gamma}\]

Proof.: \[(1-\gamma)(V_{r}^{\pi^{\prime}}(d_{0})-V_{r}^{\pi}(d_{0})) =\mathbb{E}_{d^{\pi}}[V_{r}^{\pi^{\prime}}(s)-Q_{r}^{\pi^{\prime} }(s,a)]\] \[=\mathbb{E}_{d^{\pi}}[V_{r}^{\pi^{\prime}}(s)-Q_{r}^{\pi^{\prime} }(s,a)|a\in\mathcal{A}_{s}^{\alpha}]+\mathbb{E}_{d^{\pi}}[V_{r}^{\pi^{\prime} }(s)-Q_{r}^{\pi^{\prime}}(s,a)|a\notin\mathcal{A}_{s}^{\alpha}]\] \[\leq\beta+\frac{1}{1-\gamma}\mathbb{E}_{d^{\pi}}[\mathbbm{1}[a \notin\mathcal{A}_{s}^{\alpha}]]\]

On the other hand, we have

\[(1-\gamma)\delta\geq(1-\gamma)V_{c_{\mu}}^{\pi}(d_{0}) =(1-\gamma)(V_{c_{\mu}}^{\pi}(d_{0})-V_{c_{\mu}}^{*}(d_{0}))\] \[=\mathbb{E}_{d^{\pi}}[Q_{c_{\mu}}^{*}(s,a)-V_{c_{\mu}}^{*}(s)|a \in\mathcal{A}_{s}]+\mathbb{E}_{d^{\pi}}[Q_{c_{\mu}}^{*}(s,a)-V_{c_{\mu}}^{*}( s)|a\notin\mathcal{A}_{s}]\] \[\geq\mathbb{E}_{d^{\pi}}[Q_{c_{\mu}}^{*}(s,a)-V_{c_{\mu}}^{*}(s)|a \notin\mathcal{A}_{s}]\] \[\geq\alpha\mathbb{E}_{d^{\pi}}[\mathbbm{1}[a\notin\mathcal{A}_{s}]]\]

Therefore,

\[(1-\gamma)(V_{r}^{\pi^{\prime}}(d_{0})-V_{r}^{\pi}(d_{0}))\leq \beta+\frac{\delta}{\alpha}\]

### Proof of Theorem 2

Proof.: By Proposition 2, we have proved

\[V_{\tilde{r}}^{\pi^{\dagger}}(d_{0})-V_{\tilde{r}}^{\tilde{\pi}} (d_{0}) \leq o(1)\] \[V_{c_{\mu}}^{\tilde{\pi}}(d_{0}) \leq o(1)+\delta+\kappa(\delta)\]

That is, \(\hat{\pi}\in\Pi_{\iota}(\mathcal{C}_{\mu}(\tilde{r}))\) with \(\iota=\delta+\kappa(\delta)+o(1)\).

Then by \(\frac{1}{\epsilon}\)-positively assumption on \(\mu\), we have the performance bound

\[V_{r}^{\pi^{\dagger}}(d_{0})-V_{r}^{\tilde{\pi}}(d_{0})\leq O(\iota)\]To bound of out of support probability, we derive

\[V^{\pi}_{c_{\mu}}(d_{0}) =\sum_{t=0}^{\infty}\gamma^{t}\mathbb{E}_{d^{\pi}_{t}}[\mathbbm{1}[ \mu(s,a)=0]]\] \[=(1-\gamma)\sum_{t=0}^{\infty}\gamma^{t}\left(\sum_{\tau=0}^{t} \mathbb{E}_{d^{\pi}_{\tau}}[\mathbbm{1}[\mu(s,a)=0]]\right)\] \[\geq(1-\gamma)\sum_{t=0}^{\infty}\gamma^{t}\mathrm{Prob}\left( \exists\tau\in[0,t]s_{\tau}\notin\text{supp}(\mu)|\hat{\pi}\right)\]

where second equality follows [45][Lemma 2]. 

### Technical Details of Constrained MDP

**Lemma 5**.: _For \(g:\mathcal{S}\times\mathcal{A}\to[0,\infty)\), let \(\mathcal{C}(\mathcal{S},\mathcal{A},f,g,P,\gamma)\) be a CMDP with sensitivity function \(\kappa(\delta)\). Assume \(\mathcal{C}(\mathcal{S},\mathcal{A},f,g,P,\gamma)\) is feasible. Consider a relaxed CMDP, \(\max_{\pi}V^{\pi}_{f}(d_{0}),\) s.t. \(V^{\pi}_{g}(d_{0})\leq\delta\) and let \(\lambda^{\dagger}_{\delta}\geq 0\) denote its optimal Lagrange multiplier. Then \(\lambda^{\dagger}_{\delta}\leq\frac{\kappa(\delta)}{\delta}\)._

Proof.: Denote the optimal policy of the relaxed CMDP as \(\pi^{\dagger}_{\delta}\) and the optimal policy of the original CMDP as \(\pi^{\dagger}\). By construction, \(\pi^{\dagger}_{\delta}\in\Pi^{\dagger}(\delta)\). Therefore, we have \(V^{\pi^{\dagger}_{\delta}}_{f}(d_{0})-V^{\pi^{\dagger}}_{f}(d_{0})=\max_{\pi \in\Pi^{\dagger}(\delta)}V^{\pi}_{f}(d_{0})-V^{\pi^{\dagger}}_{f}(d_{0})=\kappa (\delta)\).

To bound \(\lambda^{\dagger}_{\delta}\), we use that the fact that \((\pi^{\dagger}_{\delta},\lambda^{\dagger}_{\delta})\) is a saddle point to the Lagrangian of the relaxed problem \(\mathcal{L}_{\delta}(\pi,\lambda)\coloneqq V^{\pi}_{f}(d_{0})-\lambda(V^{\pi }_{g}(d_{0})-\delta)\). As a result, we can derive

\[V^{\pi^{\dagger}}_{f}(d_{0})-\lambda^{\dagger}_{\delta}(V^{\pi^{\dagger}}_{g} (d_{0})-\delta)=\mathcal{L}_{\delta}(\pi^{\dagger},\lambda^{\dagger}_{\delta}) \leq\mathcal{L}_{\delta}(\pi^{\dagger}_{\delta},\lambda^{\dagger}_{\delta}) \leq\mathcal{L}_{\delta}(\pi^{\dagger}_{\delta},0)=V^{\pi^{\dagger}_{\delta}} _{f}(d_{0}).\]

Since \(V^{\pi^{\dagger}}_{g}(d_{0})=0\), the above inequality implies

\[\lambda^{\dagger}_{\delta}\leq\frac{V^{\pi^{\dagger}_{\delta}}_{f}(d_{0})-V^{ \pi^{\dagger}}_{f}(d_{0})}{\delta}=\frac{\kappa(\delta)}{\delta}\]

**Lemma 2**.: _Suppose there is \(\lambda^{\dagger}\leq L\) with some \(L<\infty\) such that \(\lambda^{\dagger}\in\arg\min_{\lambda\geq 0}\max_{\pi}V^{\pi}_{\tilde{r}}(d_{0})- \lambda V^{\pi}_{c_{\mu}}(d_{0})\). Then \(\kappa(\delta)\leq L\delta\)._

Proof.: First we note by11[47, Theorem 4.1] we know that \((\pi^{\dagger},\lambda^{\dagger})\) is a saddle-point to the CMDP \(\mathcal{C}_{\mu}(\tilde{r})=\mathcal{C}(\mathcal{S},\mathcal{A},\tilde{r},c _{\mu},P,\gamma)\) in (1). Let \(f=\tilde{r}\) and \(g=c_{\mu}\). Define the Lagrangian \(\mathcal{L}_{\delta}(\pi,\delta)=V^{\pi}_{f}(d_{0})-\lambda(V^{\pi}_{g}(d_{0})-\delta)\) for the relaxed CMDP problem \(\max_{\pi:V^{\pi}_{g}(d_{0})\leq\delta}V^{\pi}_{f}(d_{0})\). Consider a saddle-point \((\pi^{\dagger}_{\delta},\lambda^{\dagger}_{\delta})\) to this relaxed problem.

Footnote 11: We use this theorem because the CMDP here does not satisfy the Slater’s condition.

By duality, we can first derive

\[\max_{\pi\in\Pi^{\dagger}(\delta)}V^{\pi}_{f}(d_{0})-V^{\pi^{\dagger}}_{f}(d_{0 })=\max_{\pi:V^{\pi}_{g}(d_{0})\leq\delta}V^{\pi}_{f}(d_{0})-V^{\pi^{\dagger}}_ {f}(d_{0})=\mathcal{L}_{\delta}(\pi^{\dagger}_{\delta},\lambda^{\dagger}_{ \delta})-V^{\pi^{\dagger}}_{f}(d_{0})\]

Then we upper bound \(\mathcal{L}_{\delta}(\pi^{\dagger}_{\delta},\lambda^{\dagger}_{\delta})\):

\[\mathcal{L}_{\delta}(\pi^{\dagger}_{\delta},\lambda^{\dagger}_{ \delta}) \leq\mathcal{L}_{\delta}(\pi^{\dagger}_{\delta},\lambda^{\dagger})\] \[=V^{\pi^{\dagger}_{\delta}}_{f}(d_{0})-\lambda^{\dagger}(V^{\pi^{ \dagger}_{\delta}}_{f}(d_{0})-\delta)\] \[=V^{\pi^{\dagger}_{\delta}}_{f}(d_{0})-\lambda^{\dagger}V^{\pi^{ \dagger}_{\delta}}_{f}(d_{0})+\delta\lambda^{\dagger}\] \[=\mathcal{L}(\pi^{\dagger}_{\delta},\lambda^{\dagger})+\delta \lambda^{\dagger}\] \[\leq\mathcal{L}(\pi^{\dagger},\lambda^{\dagger})+\delta\lambda^{\dagger}\] \[\leq\mathcal{L}(\pi^{\dagger},0)+\delta\lambda^{\dagger}.\]Thus we have

\[\max_{\pi\in\Pi^{1}(\delta)}V_{f}^{\pi}(d_{0})-V_{f}^{\pi^{\dagger}}(d_{0})\leq \mathcal{L}(\pi^{\dagger},0)+\delta\lambda^{\dagger}-V_{f}^{\pi^{\dagger}}(d_{0} )=V_{f}^{\pi^{\dagger}}(d_{0})+\delta\lambda^{\dagger}-V_{f}^{\pi^{\dagger}}(d_{ 0})=\delta\lambda^{\dagger}\]

## Appendix E Finite-Horizon Version

We discuss how to interpret Theorem 2 in the finite-horizon setup.

**Assumption 2** (Data Assumption).:
1. For all \(\tilde{r}\in\tilde{\mathcal{R}}\), \(\tilde{r}(s,a)\in[-1,1]\) for all \(s\in\mathcal{S}\) and \(a\in\mathcal{A}\).
2. The data distribution \(\mu\) is \(\frac{1}{\epsilon}\)-positively biased with respect to \(\tilde{\mathcal{R}}\).
3. There is a policy \(\pi\) satisfying \(V_{c_{\mu}}\leq 0\).
4. For any \(\tilde{r}\in\tilde{\mathcal{R}}\), the CMDP \(\mathcal{C}_{\mu}(\tilde{r})\coloneqq\mathcal{C}(\mathcal{S},\mathcal{A}, \tilde{r},c_{\mu},P,\gamma)\) in (1) has a sensitivity function \(\kappa(\delta)\leq K\) for some \(K<\infty\) and \(\lim_{\delta\to 0^{+}}\kappa(\delta)=0\). In addition, its solution \(\pi^{\dagger}\) satisfies \(\sup_{s,a}\frac{d^{\pi^{\dagger}}(s,a)}{\mu(s,a)}<\infty\).

**Theorem 2** (Main Result).: _Under the data assumption in Assumption 2, consider an offline RL algorithm \(Algo\) that is sufficiently pessimistic to be \((\frac{K}{\delta}+2)\)-admissible with respect to \(\tilde{\mathcal{R}}\). Then for any \(\tilde{r}\in\tilde{\mathcal{R}}\), with high probability, the policy \(\hat{\pi}\) learned by \(Algo\) from the dataset \(\tilde{\mathcal{D}}\coloneqq\{(s,a,\tilde{r},s^{\prime})|(s,a)\sim\mu,\tilde{ r}=\tilde{r}(s,a),s^{\prime}\sim P(\cdot|s,a)\}\) has both performance guarantee_

\[V_{r}^{\pi^{*}}(d_{0})-V_{r}^{\hat{\pi}}(d_{0})\leq\epsilon+O(\iota)\]

_and safety guarantee_

\[(1-\gamma)\sum_{t=0}^{\infty}\gamma^{t}\mathrm{Prob}\left(\exists\tau\in[0,t ]s_{\tau}\notin\text{supp}(\mu)|\hat{\pi}\right)\leq\iota\]

_where \(\iota\coloneqq\delta+\kappa(\delta)+o(1)\) and \(o(1)\) denotes a term that vanishes as the dataset size becomes infinite._

NotationTo translate the previous notation to the finite-horizon setting, we suppose the state \(s\) contains time information and the state space is layered. That is, \(\mathcal{S}=\bigcup_{0=1}^{H-1}\mathcal{S}_{t}\), where \(H\) is the problem horizon, and \(\mathcal{S}_{t}\) denotes the set of states at time \(t\). For example, for a trajectory \(s_{0},s_{2},\ldots,s_{H-1}\) starting from \(t=0\), we have \(s_{t}\in\mathcal{S}_{t}\), for \(t\in[0,H-1]\). Therefore, we can use the previous notation to model time-varying functions naturally (needed in the finite horizon), without explicitly listing the time dependency. In this section, with abuse of notation, we define the value \(V_{r}^{\pi}\) of a policy \(\pi\) to reward \(r\) at \(s\in\mathcal{S}_{\tau}\) as

\[V_{r}^{\pi}(s)\coloneqq\mathbb{E}_{\pi,P}\left[\sum_{t=\tau}^{H-1}\tilde{r}(s_ {t},a_{t})\mid s_{\tau}=s\right]\]

MDP and CMDP ProblemsThe task MDP becomes \(\mathcal{M}_{H}=(\mathcal{S},\mathcal{A},r,P,H)\) and solving it means

\[\max_{\pi}\mathbb{E}_{\pi,P}\left[\sum_{t=0}^{H-1}r(s_{t},a_{t})\mid s_{0}\sim d _{0}\right]\]

and the CMDP problem in (1) becomes

\[\max_{\pi}\mathbb{E}_{\pi,P}\left[\sum_{t=0}^{H-1}\tilde{r}(s_{t},a_{t})\mid s _{0}\sim d_{0}\right]\quad\text{s.t.}\quad\mathbb{E}_{\pi,P}\left[\sum_{t=0}^ {H-1}c_{\mu}(s_{t},a_{t})\mid s_{0}\sim d_{0}\right]\leq 0\]

### Main Theorem for Finite-horizon Problems

**Assumption 3** (Data Assumption (Finite Horizon)).:
1. For all \(\tilde{r}\in\tilde{\mathcal{R}}\), \(\tilde{r}(s,a)\in[-1,1]\) for all \(s\in\mathcal{S}\) and \(a\in\mathcal{A}\).
2. The data distribution \(\mu\) is \(\frac{1}{\epsilon}\)-positively biased with respect to \(\tilde{\mathcal{R}}\).
3. The solution to the CDMP \(\pi^{\dagger}\) satisfies \(\sup_{s,a}\frac{d^{\pi^{\dagger}(s,a)}}{\mu(s,a)}<\infty\), where we note that \(d^{\pi^{\dagger}}\) is defined without discounts but as the average over the episode length.

**Theorem 3** (Main Result (Finite Horizon)).: _Under the data assumption in Assumption 2, consider an offline RL algorithm \(Algo\) that is sufficiently pessimistic to be \((\frac{K}{\delta}+2)\)-admissible with respect to \(\tilde{\mathcal{R}}\). Then for any \(\tilde{r}\in\tilde{\mathcal{R}}\), with high probability, the policy \(\hat{\pi}\) learned by \(Algo\) from the dataset \(\tilde{\mathcal{D}}\coloneqq\{(s,a,\tilde{r},s^{\prime})|(s,a)\sim\mu,\tilde {r}=\tilde{r}(s,a),s^{\prime}\sim P(\cdot|s,a)\}\) has both performance guarantee_

\[V_{r}^{\pi^{*}}(d_{0})-V_{r}^{\hat{\pi}}(d_{0})\leq\epsilon+O(\iota)\]

_and safety guarantee_

\[\operatorname{Prob}\left(\exists\tau\in[0,H-1],s_{\tau}\notin\text{supp}(\mu) |\hat{\pi}\right)\leq\iota\]

_where \(\iota\coloneqq\delta+\kappa(\delta)+o(1)\) and \(o(1)\) denotes a term that vanishes as the dataset size becomes infinite._

The main difference between the infinite-horizon setup and the finite-horizon setup is that the finite-horizon setup always satisfies the regularity assumption (the third point) in Assumption 2. In addition, the safety guarantee is more explicit compared the discounted infinite-horizon counterpart. This is because \(\operatorname{Prob}\left(\exists\tau\in[0,H-1],s_{\tau}\notin\text{supp}(\mu) |\hat{\pi}\right)\leq V_{\hat{\pi}_{\mu}}^{\hat{\pi}}(d_{0})\) for the finite horizon, whereas we need an additional conversion in Appendix D.5.

We prove \(\lim_{\delta\to 0^{+}}\kappa(\delta)=0\) is always true for the finite horizon version.

**Proposition 4**.: _For the \(H\)-horizon constrained MDP of (1), we have an optimal dual variable \(\lambda^{\dagger}=2H+1\) and \(\kappa(\delta)\leq\min\{2H,(2H+1)\delta\}\)._

Proof.: Define the Lagrange reward \(r^{\dagger}(s,a)=\tilde{r}(s,a)-\lambda^{\dagger}c_{\mu}(s,a)\). Let \(\hat{\pi}^{\dagger}\) denote the optimal policy to the Lagrange reward. Notice that \(V_{c_{\mu}}^{\pi}(d_{0})\geq 1\) for any \(\pi\) such that \(V_{c_{\mu}}^{\pi}(d_{0})>0\). Therefore, with \(\lambda^{\dagger}>2H\),

\[\max_{\pi:V_{c_{\mu}}^{\pi}(d_{0})>0}V_{\hat{r}}^{\pi}(d_{0})- \lambda^{\dagger}V_{c_{\mu}}^{\pi}(d_{0}) \leq\max_{\pi:V_{c_{\mu}}^{\pi}(d_{0})>0}V_{\hat{r}}^{\pi}(d_{0}) -\lambda^{\dagger}\] \[<-H\] \[\leq\max_{\pi:V_{c_{\mu}}^{\pi}(d_{0})=0}V_{\hat{r}}^{\pi}(d_{0}) -\lambda^{\dagger}V_{c_{\mu}}^{\pi}(d_{0})\]

Therefore, \(\pi^{\dagger}\) satisfies \({V_{c_{\mu}}^{\pi^{\dagger}}(d_{0})=0}\). This implies

\[\max_{\pi}V_{\tilde{r}}^{\pi}(d_{0})-\lambda^{\dagger}V_{c_{\mu}}^{\pi}(d_{0}) \geq\min_{\lambda\geq 0}\max_{\pi}V_{\tilde{r}}^{\pi}(d_{0})-\lambda V_{c_{\mu}}^{ \pi}(d_{0})={V_{\tilde{r}}^{\pi^{\dagger}}(d_{0})}\]

On the other hand, it is always true that

\[\max_{\pi}V_{\tilde{r}}^{\pi}(d_{0})-\lambda^{\dagger}V_{c_{\mu}}^{\pi}(d_{0}) \geq\min_{\lambda\geq 0}\max_{\pi}V_{\tilde{r}}^{\pi}(d_{0})-\lambda V_{c_{\mu}}^{ \pi}(d_{0})\]

Therefore,

\[\lambda^{\dagger}\in\min_{\lambda\geq 0}\max_{\pi}V_{\tilde{r}}^{\pi}(d_{0}) -\lambda V_{c_{\mu}}^{\pi}(d_{0})\]

Consider \(\lambda^{\dagger}=2H+1\). By Lemma 2, we have \(\kappa(\delta)\leq(2H+1)\delta\). In addition \(\kappa(\delta)\leq 2H\) by definition.

### Length Bias

For finite-horizon problems, we provide a simple sufficient condition for length bias, which assumes long trajectories obtained in data collections are near optimal.

**Proposition 5**.: _Suppose the data are generated by rolling out policies starting from \(t=0\) and the initials state distribution \(d_{0}\). Let \(\xi=(s_{0},a_{0},s_{1},\ldots,s_{T_{\xi}-1},a_{T_{\xi}-1})\) denote a trajectory of length \(T_{\xi}\). If with probability one (over randomness of \(\xi\)) that \(T_{\xi}=H\) implies_

\[\sum_{t=0}^{H-1}r(s_{t},a_{t})\geq V^{\pi^{*}}(d_{0})-H\epsilon^{\prime}\]

_then the data distribution is \(\frac{1}{H\epsilon^{\prime}}\)-positively biased._

Proof.: Consider some \(\pi\in\Pi^{\dagger}(\delta)\). We can derive

\[V^{\pi*}_{r}(d_{0})-V^{\pi}_{r}(d_{0}) =V^{\pi*}_{r}(d_{0})-\mathbb{E}_{\pi,P}\left[\sum_{t=0}^{H-1}r(s_{ t},a_{t})\right]\] \[=\mathbb{E}_{\pi,P}\left[V^{\pi*}_{r}(d_{0})-\sum_{t=0}^{H-1}r(s_ {t},a_{t})\mid\exists\tau\in[0,H-1],s_{\tau}\notin\text{supp}(\mu)\right]\] \[\quad+\mathbb{E}_{\pi,P}\left[V^{\pi*}_{r}(d_{0})-\sum_{t=0}^{H-1} r(s_{t},a_{t})\mid\forall\tau\in[0,H-1],s_{\tau}\in\text{supp}(\mu)\right]\] \[\leq H\times\text{Prob}\left(\exists\tau\in[0,H-1],s_{\tau} \notin\text{supp}(\mu)|\pi\right)+H\epsilon^{\prime}\] \[\leq H\times V^{\pi}_{c_{\mu}}(d_{0})+H\epsilon^{\prime}\] \[\leq H\delta+H\epsilon^{\prime}\]

**Remark on the existence of full-length trajectories.** Note that we do not explicitly make an assumption on the likeliness of full-length trajectories in Proposition 5. This is because the probability of having a full-length trajectory in the data distribution is guaranteed to be strictly greater than zero given the concentrability assumption in Assumption 3. Notice that time information is part of the state in a finite horizon problem, so Assumption 3 implies that there is a non-zero probability of having full-horizon data trajectories (otherwise, the concentrability coefficient would be \(\infty\)). When Assumption 3 holds, statistical errors due to finite dataset size are included in the \(\iota\) term Theorem 2.

### Goal-oriented Problems

Finally we make a remark on positive data bias in the goal-oriented finite-horizon setting. In the infinite-horizon setting that a goal is marked as an absorbing state, which means that the agent once entering will stay there forever. The exact instantaneous (wrong) reward obtained at this absorbing state is not relevant (it can be anything in \([-1,1]\)) since the admissibility condition has already accounted for the range of the associated Lagrange reward. Namely, the agent is set pessimistic such that it views partial transitions/trajectories obtaining a worse return than \(-\frac{1}{1-\gamma}\), which is a lower bound of the return the absorbing goal state.

To make sure the finite-horizon setting has the same kind of positive data bias, one way is to virtually extend the problem's original horizon (say \(H\)) by (as least) one and let the goal state be the only state where the agent can stay until the last time step12\(H\) (i.e., all the other trajectories continue maximally up to time step \(H-1\) and therefore have a length at most \(H\)). Then we apply the offlineRL algorithm to this extended problem (e.g., of horizon \(H+1\)). We would need to set the horizon in the previous theoretical results accordingly for this longer horizon.

The reason for this extension is to ensure that the effect of absorbing state in the infinite-horizon setting (which ensures trajectories going into the absorbing state is by definition the longest) can carry over to the finite horizon case. If we apply the agent directly to solve the \(H\) step problem without such an extension, there may be other trajectories which can be as long as a goal-reaching one. As a result, there is no positive data bias.

An alternate way to create the positive data bias in the finite horizon setting is to truncate trajectories that do not reach goal at time step \(H-1\) to be no longer than \(H-1\). That is, they now time out at time step \(H-2\), whereas only goal-reaching trajectories can continue up to time step \(H-1\).

## Appendix F Algorithm Specific Results

In this appendix, we provide a few examples of admissible offline RL algorithms. We choose algorithms to cover both model-free [2, 7, 31, 13] and model-based [34, 40, 14, 32, 33] approaches. The pessimism in these algorithms are constructed through different ways, including adversarial training [2, 7, 34, 40, 14], value bonus [31, 32] and action truncation [13, 33]. These algorithms are listed in Table 15.

We recall the definition of admissibility below.

**Definition 5**.: [Admissibility] For \(R\geq 0\), we say an offline RL algorithm \(Algo\) is \(R\)_-admissible_ with respect to \(\tilde{\mathcal{R}}\), if for any \(\tilde{r}\in\tilde{\mathcal{R}}\), given \(\tilde{\mathcal{D}}\), \(Algo\) learns a policy \(\hat{\pi}\) satisfying the following with high probability: for any policy \(\pi\in\Pi\),

\[\max_{\tilde{r}\in\mathcal{R}_{R}(\tilde{r})}V_{\tilde{r}}^{\pi}(d_{0})-V_{ \tilde{r}}^{\hat{\pi}}(d_{0})\leq\mathcal{E}(\pi,\mu),\]

where we define a data-consistent reward class

\[\mathcal{R}_{R}(\tilde{r})\coloneqq\left\{\bar{r}:\bar{r}(s,a)=\tilde{r}(s,a),\forall(s,a)\in\text{supp}(\mu)\text{ and }\bar{r}(s,a)\in[-R,1]\,,\forall(s,a)\in \mathcal{S}\times\mathcal{A}\right\},\]

\(\mathcal{E}(\pi,\mu)\) is some regret upper bound such that \(\mathcal{E}(\pi,\mu)=o(1)\) if \(\sup_{s\in\mathcal{S},a\in\mathcal{A}}\frac{d^{\pi}(s,a)}{\mu(s,a)}<\infty\), and \(o(1)\) denotes a term that vanishes as the dataset size becomes infinite.

For the sake of clarity, we consider the tabular setting in our analysis. That is, we assume the state space \(\mathcal{S}\) and the action space \(\mathcal{A}\) are countable and finite. (We use \(|\mathcal{S}|\) and \(|\mathcal{A}|\) to denote their cardinalities). To establish admissibility, we sometimes make minor changes over the function classes in these algorithms, because some algorithms assume that the reward is known or has value bounded by \([0,1]\). We highlight these changes in blue. We would like to clarify that the goal of our analysis is to prove that these algorithms are admissible rather than provide a tight bound for their performance. We recommend readers who are interested in performance bound to read the original papers, as their bound can be tighter than ours.

One remarkable technical details in our analysis is that there is no need of using an union bound over all rewards in \(\mathcal{R}_{R}(\tilde{r})\) in the admissibility definition when proving these high probability statements. Instead we found that we can reuse the bound proved for a single reward directly. The main reason is that all rewards in the admissibility definition agree with each other on the support of the data distribution, and the statistical analysis of concentration only happens within this support. Outside of the support, a uniform bound based on the reward range can be used to bound the error. This will be made more clearly later in the derivations.

\begin{table}
\begin{tabular}{c|c|c|c}  & Adversarial Training & Value Bonus & Action Truncation \\ \hline \multirow{2}{*}{Model-free} & ATAC [2] (Appendix F.1) & VI-LCB [31] (Appendix F.5) & PPIPQI [13] (Appendix F.6) \\ \cline{2-4}  & PSPI [7] (Appendix F.2) & VI-LCB [31] (Appendix F.5) & PPIPQI [13] (Appendix F.6) \\ \hline \multirow{2}{*}{Model-based} & ARMRQ [34, 40] (Appendix F.3) & \multirow{2}{*}{MOPO [32] (Appendix F.7)} & \multirow{2}{*}{MOReL. [33] (Appendix F.7)} \\  & CPPO [14] (Appendix F.4) & & \\ \end{tabular}
\end{table}
Table 15: We show that all of the offline RL algorithms above are admissible. Note that this is not a complete list of all admissible offline RL algorithms.

### Atac

We first show that ATAC [2] is an admissible offline RL algorithm. We first introduce notations that we will use for analyzing ATAC, which will also be useful for studying PSPI in Appendix F.2.

#### f.1.1 Notations

For any \(\mu,\nu\in\Delta(\mathcal{S}\times\mathcal{A})\), we denote \((\mu\setminus\nu)(s,a)\coloneqq\max(\mu(s,a)-\nu(s,a),0)\). For any \(\mu\in\Delta(\mathcal{S}\times\mathcal{A})\) and any \(f:\mathcal{S}\times\mathcal{A}\to\mathbb{R}\), we define \(\langle\mu,f\rangle\coloneqq\sum_{(s,a)\in\mathcal{S}\times\mathcal{A}}\mu(s,a )f(s,a)\). For an MDP with transition probability \(P:\mathcal{S}\times\mathcal{A}\to\Delta(\mathcal{S})\), we define \((\mathcal{P}^{\pi}f)(s,a)\coloneqq\gamma\mathbb{E}_{s^{\prime}\sim P(\cdot|s,a )}f(s^{\prime},\pi)\) for any \(f:\mathcal{S}\times\mathcal{A}\to\mathbb{R}\) and \(\pi:\mathcal{S}\times\Delta(\mathcal{A})\).

Given function class \(\mathcal{F}\subseteq(\mathcal{S}\times\mathcal{A}\to\mathbb{R})\), policy \(\pi:\mathcal{S}\to\Delta(\mathcal{A})\) and reward \(\bar{r}:\mathcal{S}\times\mathcal{A}\to\mathbb{R}\), we introduce Bellman error transfer coefficient [7, 2] to measure the distribution shift between two probability measure \(\mu\) and \(\nu\).

**Definition 6** (Bellman error transfer coefficient).: The Bellman error transfer coefficient between \(\nu\) and \(\mu\) under function class \(\mathcal{F}\), policy \(\pi\) and reward \(\bar{r}\) is defined as,

\[\mathscr{C}(\nu;\mu,\mathcal{F},\pi,\bar{r})\coloneqq\max_{f\in\mathcal{F}} \frac{\|f-\bar{r}-\mathcal{P}^{\pi}f\|_{2,\nu}^{2}}{\|f-\bar{r}-\mathcal{P}^{ \pi}f\|_{2,\mu}^{2}}.\] (4)

We note that the Bellman error transfer coefficient is a weaker notion than the density ratio, which is established in the lemma below.

**Lemma 6**.: _For any distributions \(\nu,\mu\in\Delta(\mathcal{S}\times\mathcal{A})\), function class \(\mathcal{F}\subseteq(\mathcal{S}\times\mathcal{A}\to\mathbb{R})\), policy \(\pi:\mathcal{S}\to\Delta(\mathcal{A})\) and reward \(\bar{r}:\mathcal{S}\times\mathcal{A}\to\mathbb{R}\), we have_

\[\mathscr{C}(\nu;\mu,\mathcal{F},\pi,\bar{r})\leq\left(\sup_{(s,a)\in\mathcal{ S}\times\mathcal{A}}\frac{\nu(s,a)}{\mu(s,a)}\right).\] (5)

Proof.: \[\|f-\bar{r}-\mathcal{P}^{\pi}f\|_{2,\nu}^{2} =\sum_{(s,a)\in\mathcal{S}\times\mathcal{A}}\nu(s,a)\big{(}f(s,a )-\bar{r}(s,a)-\mathcal{P}^{\pi}f(s,a)\big{)}^{2}\] \[=\sum_{(s,a)\in\mathcal{S}\times\mathcal{A}}\mu(s,a)\frac{\nu(s,a)}{\mu(s,a)}\big{(}f(s,a)-\bar{r}(s,a)-\mathcal{P}^{\pi}f(s,a)\big{)}^{2}\] \[\leq\sum_{(s,a)\in\mathcal{S}\times\mathcal{A}}\mu(s,a)\left(\sup _{(s,a)\in\mathcal{S}\times\mathcal{A}}\frac{\nu(s,a)}{\mu(s,a)}\right)\big{(} f(s,a)-\bar{r}(s,a)-\mathcal{P}^{\pi}f(s,a)\big{)}^{2}\] \[=\left(\sup_{(s,a)\in\mathcal{S}\times\mathcal{A}}\frac{\nu(s,a) }{\mu(s,a)}\right)\sum_{(s,a)\in\mathcal{S}\times\mathcal{A}}\mu(s,a)\big{(} f(s,a)-\bar{r}(s,a)-\mathcal{P}^{\pi}f(s,a)\big{)}^{2}\] \[=\left(\sup_{(s,a)\in\mathcal{S}\times\mathcal{A}}\frac{\nu(s,a) }{\mu(s,a)}\right)\|f-\bar{r}-\mathcal{P}^{\pi}f\|_{2,\mu}^{2}\]

Therefore,

\[\mathscr{C}(\nu;\mu,\mathcal{F},\pi,\bar{r}) =\max_{f\in\mathcal{F}}\frac{\|f-\bar{r}-\mathcal{P}^{\pi}f\|_{2, \nu}^{2}}{\|f-\bar{r}-\mathcal{P}^{\pi}f\|_{2,\mu}^{2}}\] \[\leq\left(\sup_{(s,a)\in\mathcal{S}\times\mathcal{A}}\frac{\nu(s,a)}{\mu(s,a)}\right)\max_{f\in\mathcal{F}}\frac{\|f-\bar{r}-\mathcal{P}^{\pi} f\|_{2,\mu}^{2}}{\|f-\bar{r}-\mathcal{P}^{\pi}f\|_{2,\mu}^{2}}\] \[=\sup_{(s,a)\in\mathcal{S}\times\mathcal{A}}\frac{\nu(s,a)}{\mu( s,a)}\]

#### f.1.2 Analysis

ATAC considers a critic function class \(\mathcal{F}\) and a policy class \(\Pi\). Since we consider the tabular setting, we choose \(\mathcal{F}:(\mathcal{S}\times\mathcal{A}\to[-V_{\text{max}},V_{\text{max}}])\)13 with \(\frac{\Gamma}{1-\gamma}\leq V_{\text{max}}<\infty\) and \(\Pi:(\mathcal{S}\to\Delta(\mathcal{A}))\). ATAC formulates offline RL as the following Stackelberg game,

Footnote 13: The original theoretical statement of ATAC assumes \(\mathcal{F}\) to contain only non-negative functions. However, the analysis can be extended to any \(\mathcal{F}\) with bounded value, such as \([-V_{\text{max}},V_{\text{max}}]\). We note that the practical implementation of ATAC using function approximators does not make assumption on \(\mathcal{F}\) being non-negative.

\[\hat{\pi} \in\operatorname*{arg\,max}_{\pi\in\Pi}\,\mathcal{L}_{\hat{ \mathcal{D}}}(\pi,f^{\pi})\] (6) s.t. \[f^{\pi} \in\operatorname*{arg\,min}_{f\in\mathcal{F}}\,\mathcal{L}_{\hat {\mathcal{D}}}(\pi,f)+\beta\mathcal{E}_{\hat{\mathcal{D}}}(\pi,f),\]

with \(\beta\geq 0\) being hyperparameter, and

\[\mathcal{L}_{\hat{\mathcal{D}}}(\pi,f) \coloneqq\mathbb{E}_{\hat{\mathcal{D}}}\big{[}f(s,\pi)-f(s,a) \big{]},\] (7) \[\mathcal{E}_{\hat{\mathcal{D}}}(\pi) \coloneqq\mathbb{E}_{\hat{\mathcal{D}}}\big{[}\big{(}f(s,a)-\tilde {r}-\gamma f(s^{\prime},\pi)\big{)}^{2}\big{]}-\min_{f^{\prime}\in\mathcal{F}} \mathbb{E}_{\hat{\mathcal{D}}}\big{[}\big{(}f^{\prime}(s,a)-\tilde{r}-\gamma f (s^{\prime},\pi)\big{)}^{2}\big{]}.\]

For any policy \(\pi\), the critic \(f^{\pi}\) provides a relative pessimistic (with respect to the behavior policy \(\mu\)) value estimate of the policy \(\pi\). The hyperparameter \(\beta\) balances pessimism, given by \(\mathcal{L}_{\hat{\mathcal{D}}}(\pi,f)\), and Bellman consistency, given by \(\mathcal{E}_{\hat{\mathcal{D}}}(\pi)\). The learned policy \(\hat{\pi}\) maximizes the relative pessimistic value estimate given by the critic.

We state the performance guarantee of ATAC with respect to any comparator policy \(\pi\) under the set of data-consistent reward functions \(\{\bar{r}:\bar{r}(s,a)=\tilde{r}(s,a),\forall(s,a)\in\text{supp}(\mu)\text{ and }|\bar{r}(s,a)|\leq(1-\gamma)V_{\text{max}},\forall(s,a)\in \mathcal{S}\times\mathcal{A}\}\) in the following proposition. We make an additional assumption on the data distribution for ATAC, which is needed in its original proof.

**Assumption 4**.: For ATAC, we assume \(\mu\) is the (mixture) average of state-action visitation distribution for \(d_{0}\).

**Proposition 6**.: _Fix some \(\tilde{r}:\mathcal{S}\times\mathcal{A}\to[-1,1]\). Consider \(\mathcal{F}:(\mathcal{S}\times\mathcal{A})\to[-V_{\text{max}},V_{\text{max}}]\) with \(\frac{1}{1-\gamma}\leq V_{\text{max}}<\infty\) and \(\Pi:(\mathcal{S}\to\Delta(\mathcal{A}))\). Let \(\hat{\pi}\) be the solution to (6) and let \(\pi\in\Pi\) be any comparator policy. Let \(\nu\in\Delta(s,a)\) be an arbitrary distribution. Under Assumption 4, for any \(\delta\in(0,1]\), choosing \(\beta=\tilde{\Theta}\left(\frac{1}{V_{\text{max}}}\sqrt{\frac{C|\tilde{ \mathcal{D}}|^{2}}{(|\mathcal{S}||\mathcal{A}|\log(\nicefrac{{1}}{{\delta}}))^ {2}}}\right)\), with probability \(1-\delta\), it holds that_

\[V_{\bar{r}}^{\pi}(d_{0})-V_{\bar{r}}^{\hat{\pi}}(d_{0})\leq \tilde{O}\left(\frac{V_{\text{max}}\big{(}\mathscr{C}(\nu;\mu, \mathcal{F},\pi,\bar{r})|\mathcal{S}||\mathcal{A}|\log(\nicefrac{{1}}{{\delta} })\big{)}^{1/3}}{(1-\gamma)|\tilde{\mathcal{D}}|^{1/3}}\right)+\frac{\langle d^ {\pi}\setminus\nu,\bar{r}+\mathcal{P}^{\pi}f^{\pi}-f^{\pi}\rangle}{1-\gamma}\] (8)

_for all \(\bar{r}\) such that \(\bar{r}(s,a)=\bar{r}(s,a),\forall(s,a)\in\text{supp}(\mu)\) and \(|\bar{r}(s,a)|\leq(1-\gamma)V_{\text{max}},\forall(s,a)\in\mathcal{S}\times \mathcal{A}\)._

Proof.: We first show that \(\mathcal{F}:(\mathcal{S}\times\mathcal{A}\to[-V_{\text{max}},V_{\text{max}}])\) is both realizable and Bellman complete with repect to \(\bar{r}\). For any \(\bar{r}\) such that \(|\bar{r}(s,a)|\leq(1-\gamma)V_{\text{max}}\), we have \(Q_{\bar{r}}^{\pi}(s,a)\in[-V_{\text{max}},V_{\text{max}}]\). This means that \(Q_{\bar{r}}^{\pi}\in\mathcal{F}\) for any \(\pi\in\Pi\), \(s\in\mathcal{S}\), \(a\in\mathcal{A}\). Moreover, for any \(f\in\mathcal{F}\), \(|\bar{r}(s,a)+\mathcal{P}^{\pi}f(s,a)|\leq|\bar{r}(s,a)|+|\mathcal{P}^{\pi}f(s,a )|\leq(1-\gamma)V_{\text{max}}+\gamma V_{\text{max}}=V_{\text{max}}\). In other words, \((\bar{r}(s,a)+\mathcal{P}^{\pi}f(s,a))\in\mathcal{F}\) for any \(f\in\mathcal{F}\).

By construction of \(\bar{r}\), we have \(\bar{\mathcal{D}}\coloneqq\{(s,a,\bar{r},s^{\prime})|(s,a,s^{\prime})\in \mathcal{D},\bar{r}=\bar{r}(s,a)\}\equiv\tilde{\mathcal{D}}\). That is, solving the Stackelberg game (6) given by \(\tilde{\mathcal{D}}\) is equivalent to solving the game given by \(\tilde{\mathcal{D}}\). The rest of the proof follows from Theorem C.12 in [55] by choosing the reward class to only contain the reward \(\bar{r}\), i.e., \(\mathcal{G}=\{\bar{r}\}\) (which implies \(d_{\mathcal{G}}=0\)) and using \(d_{\mathcal{F},\Pi}=\tilde{O}(|\mathcal{S}||\mathcal{A}|\log(\nicefrac{{1}}{{ \delta}}))\).

Note that the above derivation does not need an additional union bound to cover all rewards in \(\{\bar{r}:\bar{r}(s,a)=\tilde{r}(s,a),\forall(s,a)\in\text{supp}(\mu)\text{ and }|\bar{r}(s,a)|\leq(1-\gamma)V_{\text{max}},\forall(s,a)\in\mathcal{S}\times \mathcal{A}\}\). This is because the concentration analysis is only taken on the support of the data distribution, where all rewards in this reward class agree, and a uniform bound based on \(V_{\text{max}}\) is used for out of support places, which again applies to all the rewards in this reward class.

We then show that ATAC [2] is admissible based on Proposition 6.

**Corollary 3** (ATAC is admissible).: _For any \(V_{\text{max}}\geq\frac{R}{1-\gamma}\), ATAC is \(R\)-admissible with respect to \(\tilde{\mathcal{R}}\) for any \(\tilde{\mathcal{R}}\subseteq(\mathcal{S}\times\mathcal{A}\to[-1,1])\)._

Proof.: Given any \(\tilde{r}\in\tilde{\mathcal{R}}\), we want to show that, with high probability, \(V_{\tilde{r}}^{\pi}(d_{0})-V_{\tilde{r}}^{\tilde{\pi}}(d_{0})=o(1)\) for all \(\bar{r}\in\mathcal{R}_{R}(\tilde{r})\) and all policy \(\pi\) such that \(\sup_{(s,a)\in\mathcal{S}\times\mathcal{A}}\frac{d^{\pi}(s,a)}{\mu(s,a)}<\infty\). For any such \(\pi\), define \(C_{\infty}\coloneqq\sup_{(s,a)\in\mathcal{S}\times\mathcal{A}}\frac{d^{\pi}(s,a)}{\mu(s,a)}\), we have \(\mathscr{C}(\nu;\mu,\mathcal{F},\pi,\bar{r})\leq C_{\infty}\) for any \(\bar{r}\). By taking \(\nu=d^{\pi}\) in Proposition 6, we have, with probability \(1-\delta\),

\[V_{\bar{r}}^{\pi}(d_{0})-V_{\bar{r}}^{\tilde{\pi}}(d_{0}) \leq\tilde{O}\left(\frac{V_{\text{max}}\big{(}C_{\infty}|\mathcal{ S}||\mathcal{A}|\log(\nicefrac{{1}}{{\delta}})\big{)}^{1/3}}{(1-\gamma)| \tilde{\mathcal{D}}|^{1/3}}\right)+\frac{\langle d^{\pi}\setminus d^{\pi}, \bar{r}+\mathcal{P}^{\pi}f^{\pi}-f^{\pi}\rangle}{1-\gamma}\] \[=\tilde{O}\left(\frac{V_{\text{max}}\big{(}C_{\infty}|\mathcal{ S}||\mathcal{A}|\log(\nicefrac{{1}}{{\delta}})\big{)}^{1/3}}{(1-\gamma)|\tilde{ \mathcal{D}}|^{1/3}}\right)=o(1)\]

for all \(\bar{r}\) such that \(\bar{r}(s,a)=\tilde{r}(s,a),\forall(s,a)\in\text{supp}(\mu)\) and \(|\bar{r}(s,a)|\leq(1-\gamma)V_{\text{max}},\forall(s,a)\in\mathcal{S}\times \mathcal{A}\). Since \(V_{\text{max}}\geq\frac{R}{1-\gamma}\), we have that \(V_{\bar{r}}^{\pi}(d_{0})-V_{\bar{r}}^{\tilde{\pi}}(d_{0})=o(1)\) for all \(\bar{r}\in\mathcal{R}_{R}(\tilde{r})\).

### Pspi

We show that PSPI [7] is also admissible. PSPI [7] is similar to ATAC. Given critic function class \(\mathcal{F}:(\mathcal{S}\times\mathcal{A}\to[-V_{\text{max}},V_{\text{max}}])\)14 with \(\frac{1}{1-\gamma}\leq V_{\text{max}}<\infty\) and policy class \(\Pi:(\mathcal{S}\to\Delta(\mathcal{A}))\), PSPI solves the following Stackelburg game,

Footnote 14: Similar to ATAC, we extend the critic function class to contain functions that can take negative values. The theoretical analysis can be generalized to this case. The practical implementation of PSPI can directly handle critics which take negative values.

\[\hat{\pi}\in\operatorname*{arg\,max}_{\pi\in\Pi}\,(1-\gamma)f^{ \pi}(d_{0},\pi)\] (9) s.t. \[f^{\pi}\in\operatorname*{arg\,min}_{f\in\mathcal{F}}\,(1-\gamma)f (d_{0},\pi)+\beta\mathcal{E}_{\hat{\mathcal{D}}}(\pi,f),\]

with \(\beta\geq 0\) being hyperparameter, and

\[\mathcal{E}_{\hat{\mathcal{D}}}(\pi)\coloneqq\mathbb{E}_{\hat{\mathcal{D}}} \big{[}\big{(}f(s,a)-\tilde{r}-\gamma f(s^{\prime},\pi)\big{)}^{2}\big{]}- \min_{f^{\prime}\in\mathcal{F}}\mathbb{E}_{\hat{\mathcal{D}}}\big{[}\big{(}f^{ \prime}(s,a)-\tilde{r}-\gamma f(s^{\prime},\pi)\big{)}^{2}\big{]}.\]

In PSPI, the critic \(f^{\pi}\) provides an absolute pessimistic value estimate of policy \(\pi\). The hyperparameter \(\beta\) trades off pessimism and Bellman-consistency. The learned policy \(\hat{\pi}\) maximizes such a pessimistic value. We state the performance guarantee of the learned policy \(\hat{\pi}\) with respect to any comparator policy \(\pi\) under the set of data-consistent reward functions \(\{\bar{r}:\bar{r}(s,a)=\bar{r}(s,a),\forall(s,a)\in\text{supp}(\mu)\) and \(|\bar{r}(s,a)|\leq(1-\gamma)V_{\text{max}},\forall(s,a)\in\mathcal{S}\times \mathcal{A}\}\) in the following proposition.

**Proposition 7** ([7, 55]).: _Let \(\hat{\pi}\) be the solution to (6) and let \(\pi\in\Pi\) be any comparator policy. Let \(\nu\in\Delta(s,a)\) be an arbitrary distribution. For any \(\delta\in(0,1]\), choosing \(\beta=\tilde{\Theta}\left(\frac{1}{V_{\text{max}}}\sqrt[3]{\frac{C|\tilde{ \mathcal{D}}|^{2}}{\big{(}|\mathcal{S}||\mathcal{A}|\log(\nicefrac{{1}}{{\delta} })\big{)}^{2}}}\right)\), with probability \(1-\delta\),_

\[V_{\bar{r}}^{\pi}(d_{0})-V_{\bar{r}}^{\hat{\pi}}(d_{0})\leq \tilde{O}\left(\frac{V_{\text{max}}\big{(}\mathscr{C}(\nu;\mu, \mathcal{F},\pi,\bar{r})|\mathcal{S}||\mathcal{A}|\log(\nicefrac{{1}}{{\delta} })\big{)}^{1/3}}{(1-\gamma)|\tilde{\mathcal{D}}|^{1/3}}\right)+\frac{\langle d^{ \pi}\setminus\nu,\bar{r}+\mathcal{P}^{\pi}f^{\pi}-f^{\pi}\rangle}{1-\gamma}\] (10)

_for all \(\bar{r}\) such that \(\bar{r}(s,a)=\bar{r}(s,a),\forall(s,a)\in\text{supp}(\mu)\) and \(|\bar{r}(s,a)|\leq(1-\gamma)V_{\text{max}},\forall(s,a)\in\mathcal{S}\times \mathcal{A}\)._

Proof.: The proof is similar to that of Proposition 6. First, observe that \(\mathcal{F}:(\mathcal{S}\times\mathcal{A}\to[-V_{\text{max}},V_{\text{max}}])\) is both realizable and Bellman complete with respect to \(\bar{r}\). The rest of the proof follows from Theorem D.1 in [55] (taking \(\mathcal{G}=\{\bar{r}\}\)) and Lemma 6.

The proposition above implies that PSPI [7] is admissible. We omit the proof of the corollary below as it is the same as the proof of Corollary 3.

**Corollary 4** (PSPI is admissible).: _For any \(V_{\text{max}}\geq\frac{R}{1-\gamma}\), PSPI is \(R\)-admissible with respect to \(\tilde{\mathcal{R}}\) for any \(\tilde{\mathcal{R}}\subseteq(\mathcal{S}\times\mathcal{A}\to[-1,1])\)._

### Armor

We show that ARMOR [34, 40] is admissible. ARMOR is a model-based offline RL algorithm. We denote a model as \(M=(\mathcal{S},\mathcal{A},P_{M},r_{M},\gamma)\), where \(P_{M}:\mathcal{S}\times\mathcal{A}\to\Delta(\mathcal{S})\) is the model dynamics, and \(r_{M}:\mathcal{S}\times\mathcal{A}\to[-R_{\text{max}},R_{\text{max}}]\)15 is the reward function with \(1\leq R_{\text{max}}<\infty\). Since we consider the tabular setting, we use a model class that contains all possible dynamics and all reward functions bounded within \(\left[-R_{\text{max}},R_{\text{max}}\right]\), i.e., \(\mathcal{M}_{model}=\{M:P_{M}\in(\mathcal{S}\times\mathcal{A}\to\Delta( \mathcal{S})),r_{M}\in(\mathcal{S}\times\mathcal{A}\to[-R_{\text{max}},R_{\text {max}}])\}\). For any reference policy \(\pi_{\text{ref}}:\mathcal{S}\to\Delta(\mathcal{A})\), ARMOR solves the following two-player game,

Footnote 15: The theoretical statement of ARMOR assumes the value of \(r_{M}\) is bounded by \([0,1]\). The original analysis can be extended as long as the value of \(r_{M}\) is bounded by a finite value \(R_{\text{max}}<\infty\). The practical implementation of ARMOR does not assume the reward only takes value in \([0,1]\).

\[\tilde{\pi}=\operatorname*{arg\,max}_{\pi\in\Pi}\;\min_{M\in\mathcal{M}_{ \tilde{\mathcal{D}}}^{\alpha}}\;V_{M}^{\pi}(d_{0})-V_{M}^{\pi_{\text{ref}}}(d_ {0})\] (11)

where \(V_{M}^{\pi}(d_{0})\coloneqq\mathbb{E}_{\pi,P_{M}}[\sum_{t=0}^{\infty}\gamma^{ t}r_{M}(s_{t},a_{t})|s_{0}\sim d_{0}]\), and

\[\mathcal{M}_{\tilde{\mathcal{D}}}^{\alpha}\coloneqq\{M\in\mathcal{M}_{model}: \mathcal{E}_{\tilde{\mathcal{D}}}(M)-\min_{M^{\prime}\in\mathcal{M}_{model}} \mathcal{E}_{\tilde{\mathcal{D}}}(M^{\prime})\leq\alpha\},\] (12)

with

\[\mathcal{E}_{\tilde{\mathcal{D}}}(M)\coloneqq\sum_{(s,a,\tilde{r},s^{\prime}) \in\tilde{\mathcal{D}}}-\log P_{M}(s^{\prime}|s,a)+(r_{M}(s,a)-\tilde{r})^{2}.\] (13)

Intuitively, ARMOR constructs a relative pessimistic model (with respect to \(\pi_{\text{ref}}\)) \(M\) which is also approximately consistent with data. The learned policy \(\hat{\pi}\) maximizes the value estimate given by the pessimistic model. We define the generalized single policy concentrability to characterize the distribution shift from \(\mu\) to the state-action visitation \(d^{\pi}\) of any policy \(\pi\).

**Definition 7** (Generalized Single-policy Concentrability [40]).: We define the generalized single-policy concentration for policy \(\pi\), model class \(\mathcal{M}_{model}\), reward \(\bar{r}\) and data distribution \(\mu\) as

\[\mathscr{C}_{model}(\pi;\mathcal{M}_{model},\bar{r})\coloneqq\sup_{M\in \mathcal{M}_{model}}\frac{\mathbb{E}_{d^{\pi}}[\mathcal{E}(M;\bar{r})]}{\mathbb{ E}_{\mu}[\mathcal{E}(M;\bar{r})]}\]

with \(\mathcal{E}(M;\bar{r})=D_{TV}(P_{M}(\cdot|s,a),P(\cdot|s,a))^{2}+(r_{M}(s,a)- \bar{r}(s,a))^{2}\).

The single-policy concentrability \(\mathscr{C}_{model}(\pi)\) can be considered as a model-based version of the Bellman error transfer coefficient in Definition 6. Following the same steps as the proof of Lemma 6, it can be shown that \(\mathscr{C}_{model}(\pi;\mathcal{M}_{model},\bar{r})\leq\sup_{(s,a)\in \mathcal{S}\times\mathcal{A}}\frac{d^{\pi}(s,a)}{\mu(s,a)}\). We provide the high-probability performance guarantee for the learned policy \(\hat{\pi}\). This implies that ARMOR is an admissible algorithm with a sufficiently large \(R_{\text{max}}\).

**Proposition 8**.: _For any \(\delta\in(0,1]\), there exists an absolute constant \(c\) such that when choosing \(\alpha=c|\mathcal{S}|^{2}|\mathcal{A}|\log(\nicefrac{{1}}{{\delta}})\), for any comparator policy \(\pi\in\Pi\), with probability \(1-\delta\), the policy \(\hat{\pi}\) learned by ARMOR (11) satisfies_

\[V_{\bar{r}}^{\pi}(d_{0})-V_{\bar{r}}^{\hat{\pi}}(d_{0})\leq O\left(\sqrt{ \mathscr{C}_{model}(\pi;\mathcal{M}_{model},\bar{r})}\frac{R_{\text{max}}}{(1- \gamma)^{2}}\sqrt{\frac{|\mathcal{S}|^{2}|\mathcal{A}|\log(\nicefrac{{1}}{{ \delta}})}{|\tilde{\mathcal{D}}|}}\right),\] (14)

_for all \(\bar{r}\) such that \(\bar{r}(s,a)=\tilde{r}(s,a),\forall(s,a)\in\text{supp}(\mu)\) and \(|\bar{r}(s,a)|\leq R_{\text{max}},\forall(s,a)\in\mathcal{S}\times\mathcal{A}\)_

Proof.: By construction, we have \(\bar{M}=(\mathcal{S},\mathcal{A},P,\bar{r},\gamma)\in\mathcal{M}_{model}\). Since \(\bar{r}(s,a)=\tilde{r}(s,a)\) for any \((s,a)\in\text{supp}(\mu)\), we have \(\mathcal{M}_{\tilde{\mathcal{D}}}^{\alpha}\equiv\mathcal{M}_{\tilde{\mathcal{D }}}^{\alpha}\). The rest of the proof follows mostly from Theorem 2 in [40] by taking \(\pi_{\text{ref}}=\mu\). By a similar argument as in the proof of Proposition 6, we do not need an additional union bound to cover all rewards in \(\{\bar{r}:\bar{r}(s,a)=\tilde{r}(s,a),\forall(s,a)\in\text{supp}(\mu)\) and \(|\bar{r}(s,a)|\leq R_{\text{max}},\forall(s,a)\in\mathcal{S}\times\mathcal{A}\}\).

**Corollary 5** (ARMOR is admissible).: _For any \(R_{\text{max}}\geq R\), ARMOR is \(R\)-admissible with respect to \(\tilde{\mathcal{R}}\) for any \(\tilde{\mathcal{R}}\subseteq(\mathcal{S}\times\mathcal{A}\to[-1,1])\)._

Proof.: Given any \(\tilde{r}\in\tilde{\mathcal{R}}\), we want to show that, with high probability, \(V_{\tilde{r}}^{\pi}(d_{0})-V_{\tilde{r}}^{\hat{\pi}}(d_{0})=o(1)\) for all \(\bar{r}\in\mathcal{R}_{R}(\bar{r})\) and all policy \(\pi\) such that \(\sup_{(s,a)\in\mathcal{S}\times\mathcal{A}}\frac{d^{\pi}(s,a)}{\mu(s,a)}<\infty\). For any such \(\pi\), \(\mathscr{C}_{model}(\pi;\mathcal{M}_{model},\bar{r})\leq\sup_{(s,a)\in\mathcal{ S}\times\mathcal{A}}\frac{d^{\pi}(s,a)}{\mu(s,a)}\coloneqq C_{\infty}<\infty\). By Proposition 8, with probability \(1-\delta\), we have

\[V_{\bar{r}}^{\pi}(d_{0})-V_{\bar{r}}^{\hat{\pi}}(d_{0})\leq O\left(\sqrt{C_{ \infty}}\frac{R_{\text{max}}}{(1-\gamma)^{2}}\sqrt{\frac{|\mathcal{S}|^{2}| \mathcal{A}|\log(\nicefrac{{1}}{{\delta}})}{|\tilde{\mathcal{D}}|}}\right)=o(1).\] (15)

for all \(\bar{r}\) such that \(\bar{r}(s,a)=\tilde{r}(s,a),\forall(s,a)\in\text{supp}(\mu)\) and \(|\bar{r}(s,a)|\leq R,\forall(s,a)\in\mathcal{S}\times\mathcal{A}\). Therefore, \(V_{\bar{r}}^{\pi}(d_{0})-V_{\bar{r}}^{\hat{\pi}}(d_{0})=o(1)\) for all \(\bar{r}\in\mathcal{R}_{R}(\bar{r})\) with high probability. 

### Cppo

CPPO[14] is another admissible model-based offline algorithm. Again, we denote a model as \(M=(\mathcal{S},\mathcal{A},P_{M},r_{M},\gamma)\), where \(P_{M}:\mathcal{S}\times\mathcal{A}\to\Delta(\mathcal{S})\) is the model dynamics, and \(r_{M}:\mathcal{S}\times\mathcal{A}\to[-R_{\text{max}},R_{\text{max}}]^{16}\) is the reward function with \(1\leq R_{\text{max}}<\infty\). CPPO solves the two-player game,

\[\hat{\pi}=\operatorname*{arg\,max}_{\pi\in\Pi}\min_{M\in\mathcal{M}_{\mathcal{ D}}^{\alpha}}V_{\bar{M}}^{\pi}(d_{0})\] (16)

with \(V_{\bar{M}}^{\pi}(d_{0})\coloneqq\mathbb{E}_{\pi,P_{M}}[\sum_{t=0}^{\infty} \gamma^{t}\,r_{M}(s_{t},a_{t})|s_{0}\sim d_{0}]\) and \(\mathcal{M}_{\mathcal{D}}^{\alpha}\) defined in (12). CPPO constructs a pessimistic model \(M\) which is also approximately consistent with data. The learned policy \(\hat{\pi}\) maximizes the value estimate given by the pessimistic model. We provide the high-probability performance guarantee for the learned policy \(\hat{\pi}\). This implies that CPPO is an admissible algorithm with sufficiently large \(R_{\text{max}}\).

**Proposition 9**.: _For any \(\delta\in(0,1]\), there exists an absolute constant \(c\) such that when choosing \(\alpha=c|\mathcal{S}|^{2}|\mathcal{A}|\log(\nicefrac{{1}}{{\delta}})\), for any comparator policy \(\pi\in\Pi\), with probability \(1-\delta\), the policy \(\hat{\pi}\) learned by CPPO (16) satisfies_

\[V_{\bar{r}}^{\pi}(d_{0})-V_{\bar{r}}^{\hat{\pi}}(d_{0})\leq O\left(\sqrt{ \mathscr{C}_{model}(\pi;\mathcal{M}_{model},\bar{r})}\frac{R_{\text{max}}}{(1- \gamma)^{2}}\sqrt{\frac{|\mathcal{S}|^{2}|\mathcal{A}|\log(\nicefrac{{1}}{{ \delta}})}{|\tilde{\mathcal{D}}|}}\right),\] (17)

_for all \(\bar{r}\) such that \(\bar{r}(s,a)=\tilde{r}(s,a),\forall(s,a)\in\text{supp}(\mu)\) and \(|\bar{r}(s,a)|\leq R_{\text{max}},\forall(s,a)\in\mathcal{S}\times\mathcal{A}\)._

Proof.: By construction, we have \(\bar{M}=(\mathcal{S},\mathcal{A},P,\bar{r},\gamma)\in\mathcal{M}_{model}\). Since \(\bar{r}(s,a)=\tilde{r}(s,a)\) for any \((s,a)\in\text{supp}(\mu)\), we have \(\mathcal{M}_{\mathcal{D}}^{\alpha}\equiv\mathcal{M}_{\mathcal{D}}^{\alpha}\). The rest of the proof follows similarly from the proof of Theorem 2 in [40]. By Lemma 5 from [40], with high probability, \(\bar{M}\in\mathcal{M}_{\mathcal{D}}^{\alpha}\). Therefore,

\[V_{\bar{r}}^{\pi}(d_{0})-V_{\bar{r}}^{\hat{\pi}}(d_{0}) =V_{\bar{M}}^{\pi}(d_{0})-V_{\bar{M}}^{\hat{\pi}}(d_{0})\leq V_{ \bar{M}}^{\pi}(d_{0})-\min_{M\in\mathcal{M}_{\mathcal{D}}^{\alpha}}\left(V_{ \bar{M}}^{\hat{\pi}}(d_{0})\right)\] \[\leq V_{\bar{M}}^{\pi}(d_{0})-\min_{M\in\mathcal{M}_{\mathcal{D}}^{ \alpha}}\left(V_{\bar{M}}^{\pi}(d_{0})\right)\] \[\leq\max_{M\in\mathcal{M}_{\mathcal{D}}^{\alpha}}|V_{\bar{M}}^{\pi }(d_{0})-V_{\bar{M}}^{\pi}(d_{0})|\]

where the second inequality follows from the optimality of \(\hat{\pi}\). By (20) from [40], for any \(M\in\mathcal{M}_{\mathcal{D}}^{\alpha}\), \(|V_{\bar{M}}^{\pi}(d_{0})-V_{\bar{M}}^{\pi}(d_{0})|\leq O\left(\sqrt{ \mathscr{C}_{model}(\pi;\mathcal{M}_{model},\bar{r})}\frac{R_{\text{max}}}{(1- \gamma)^{2}}\sqrt{\frac{|\mathcal{S}|^{2}|\mathcal{A}|\log(\nicefrac{{1}}{{ \delta}})}{|\tilde{\mathcal{D}}|}}\right)\). Following a similar argument to the proof of Proposition 6, there is no need to take the union bound with respect to \(\bar{r}\), which concludes the proof. 

We omit the proof for the following corollary as it is the same of the proof of Corollary 5.

**Corollary 6** (CPPO is admissible).: _For any \(R_{\text{max}}\geq R\), CPPO is \(R\)-admissible with respect to \(\tilde{\mathcal{R}}\) for any \(\tilde{\mathcal{R}}\subseteq(\mathcal{S}\times\mathcal{A}\to[-1,1])\)._

### Vi-Lcb

We show VI-LCB [31] is admissible. VI-LCB is a pessimistic version of value iteration. It adds negative bonuses to the Bellman backup step in order to underestimate the value when there are missing data. By being pessimistic in value estimation, it can overcome the issue of \(\mu\) being non-exploratory. We recap the VI-LCB algorithm in Algorithm 1. We highlight the changes we make in blue. These changes are due to that originally the authors in [31] assume the rewards are non-negative, but the rewards in the admissibility definition Definition 5 can take negative values. Therefore, we make these changes accordingly. We state and prove the guarantee below.17

Footnote 17: The original algorithm assumes that the reward function is known. Here we consider the generalization of the algorithm with unknown reward.

**Proposition 10**.: _For \(V_{\max}\geq\frac{R}{1-\gamma},\) VI-LCB is \(R\)-admissible with respect to any \(\tilde{\mathcal{R}}\subseteq(\mathcal{S}\times\mathcal{A}\to[-1,1])\)._

Proof.: First, we introduce a factor of \(2\) in their proof of Lemma 1[31] to accomodate that the rewards here can be negative. This is reflected in the updated bonus definition in Algorithm 1. This updated Lemma 1 of [31] now captures the good event that the bonus can upper bound the error of the empirical estimate of Bellman backup. It shows this good event is true with high probability.

We remark that while originally Lemma 1 of [31] is proved for a single reward function, it actually holds for simultaneously for all the reward functions in \(\mathcal{R}_{R}(\tilde{r})\), without the need of introducingAdditionally a union bound. The reason is that in the proof of Lemma 1 in [31], the concentration is used only for the estimating the Bellman on the data support. This bound would apply to all rewards in \(\mathcal{R}_{R}(\tilde{r})\) since they agree exactly on the data. For state-action pairs out of the support, the proof takes a uniform bound based on the size of \(V_{\max}\), which also applies to all rewards in \(\mathcal{R}_{R}(\tilde{r})\).

Therefore, we can apply the updated Lemma 1 of [31] to prove the desired high probability bound needed in the admissibility condition. Under this good event, we can use the upper bound in (54b) in [31] to bound the regret. Consider some \(\pi\) such that \(C\coloneqq\sup_{s,a}\frac{d^{\pi(s,a)}}{\mu(s,a)}<\infty\). Suppose \(\tilde{D}\) has \(N\) transitions. Take some \(V_{\max}\geq\frac{R}{1-\gamma}\). Then running VI-LCB with \(J\) iterations ensures18 for any \(\bar{r}\in\mathcal{R}_{R}(\tilde{r})\),

Footnote 18: We add back an extra dependency on \(|\mathcal{A}|\) as their original proof assumes \(\pi\) is deterministic, which is not necessarily the case here.

\[V_{\bar{r}}^{\pi}(d_{0})-V_{\bar{r}}^{\hat{\pi}}(d_{0})\leq\gamma^{J}2V_{\max }+\frac{64V_{\max}}{1-\gamma}\sqrt{\frac{L|\mathcal{S}||\mathcal{A}|C(J+1)}{ N}}\]

where we mark the changes due to using rewards which can takes negative values in blue. Setting \(J=\frac{\log N}{1-\gamma}\) as in [31], with probability \(1-\delta\), it holds that

\[V_{\bar{r}}^{\pi}(d_{0})-V_{\bar{r}}^{\hat{\pi}}(d_{0}) \leq\tilde{O}\left(\gamma^{J}V_{\max}+\frac{V_{\max}}{1-\gamma} \sqrt{\frac{L|\mathcal{S}||\mathcal{A}|CJ}{N}}\right)\] \[=\tilde{O}\left(V_{\max}\sqrt{\frac{|\mathcal{S}||\mathcal{A}|C \log\frac{1}{\delta}}{N(1-\gamma)^{3}}}\right)\]

Thus, VI-LCB is \(R\)-admissible. 

### PPI and PQI

We show PPI and PQI proposed in [13] are admissible. We present their algorithms in Algorithm 2 and Algorithm 3, with \((\zeta\circ f)(s,a)\coloneqq\zeta(s,a)f(s,a)\) for any \(\zeta,f:\mathcal{S}\times\mathcal{A}\rightarrow\mathbb{R}\). These algorithms use a probability estimator of the data distribution (denoted as \(\hat{\mu}\)), which in the tabular case is the empirical distribution. Based on \(\hat{\mu}\), they define a filter function

\[\zeta(s,a;\hat{\mu},b)=\mathbbm{1}[\hat{\mu}(s,a)\geq b]\]

This filter function classifies whether a state-action pair is in the support, and it is used to modify the Bellman operators in dynamics programming as shown in line 6 of Algorithm 2 and line 4 of Algorithm 3. As a result, the Bellman backup and the policy optimization only consider in-support actions, which mitigates the issue of learning with non-exploratory \(\mu\). The loss functions (i.e., \(\mathcal{L}_{\tilde{D}}\)) in Algorithm 2 and Algorithm 3 denote the squared Bellman error (with a target network) as used in fitted Q iteration [78]. We omit the details here.

In summary, PPI and PQI follow the typical policy iteration and value iteration schemes, except that the backup and the argmax are only taken within the observed actions (or actions with sufficiently evidence to be in the support when using function approximators). This idea is similar to the spirit of the later IQL algorithm [8], except IQL doesn not construct the filter function explicitly but relies instead on expectile maximization.

**Proposition 11**.: _Suppose \(\mathcal{F}=(\mathcal{S}\times\mathcal{A}\to[-V_{\max},V_{\max}])\)19. For \(V_{\max}\geq\frac{R}{1-\gamma}\), PPI/PQI is \(R\)-admissible with respect any \(\tilde{\mathcal{R}}\subseteq(\mathcal{S}\times\mathcal{A}\to[-1,1])\)._

Footnote 19: The original algorithms of PPI and PQI assumes \(\mathcal{F}=(\mathcal{S}\times\mathcal{A}\to[0,V_{\max}])\). We note that our extension to \([-V_{\max},V_{\max}]\) do not change the performance bound as it absorbed by the \(\tilde{O}\) notation.

Proof.: We prove for PPI; the proof for PQI follows similarly. Consider some \(\pi\) such that \(C\coloneqq\sup_{s,a}\frac{d^{\pi^{*}(s,a)}}{\mu(s,a)}<\infty\). Suppose \(\tilde{D}\) has \(N\) transitions. Take some \(V_{\max}\geq\frac{R}{1-\gamma}\) and some \(\bar{r}\) from \(\mathcal{R}_{R}(\tilde{r})\). We use the Corollary 1 in [13]: With \(I\) large enough, it holds with probability \(1-\delta\),

\[V_{\bar{r}}^{\pi}(d_{0})-V_{\bar{r}}^{\hat{\pi}}(d_{0})\leq\tilde{O}\left( \frac{V_{\max}}{(1-\gamma)^{3}}\left(\sqrt{\frac{|\mathcal{S}||\mathcal{A}| \ln(1/\delta)}{N}}+\frac{\sup_{s,a,t}d_{\bar{r}}^{\pi}(s,a)}{b}\epsilon_{\mu}+( 1-\gamma)\mathbb{E}_{\pi^{*},P}[\mathbb{1}[\mu(s,a)\leq 2b]]\right)\right)\]

where \(\epsilon_{\mu}=D_{TV}(\hat{\mu},\mu)=O(\frac{1}{\sqrt{N}})\). Further,

\[(1-\gamma)\mathbb{E}_{\pi^{*},P}[\mathbb{1}[\mu(s,a)\leq 2b]] =\sum_{s,a}d^{\pi^{*}}(s,a)\mathbb{1}[\mu(s,a)\leq 2b]\] \[=\sum_{s,a}\mu(s,a)\frac{d^{\pi^{*}}(s,a)}{\mu(s,a)}\mathbb{1}[\mu (s,a)\leq 2b]\] \[\leq C\mathbb{E}_{\mu}[\mathbb{1}[\mu(s,a)\leq 2b]].\]

We can then upper bound the performance difference above as

\[V_{\bar{r}}^{\pi}(d_{0})-V_{\bar{r}}^{\hat{\pi}}(d_{0})\leq\tilde{O}\left( \frac{V_{\max}}{(1-\gamma)^{3}}\left(\sqrt{\frac{|\mathcal{S}||\mathcal{A}| \ln(1/\delta)}{N}}+\frac{\sup_{s,a,t}d_{t}^{\pi}(s,a)}{b}\frac{1}{\sqrt{N}}+C \mathbb{E}_{\mu}[\mathbb{1}[\mu(s,a)\leq 2b]]\right)\right)\]

We bound the latter two terms by tuning \(b\). Notice \(\mathbb{E}_{\mu}[\mathbb{1}[\mu(s,a)\leq 2b]]\leq|\mathcal{S}||\mathcal{A}|2b\) at most. This implies

\[\inf_{b}\frac{\sup_{s,a,t}d_{t}^{\pi}(s,a)}{b}\frac{1}{\sqrt{N}}+ C\mathbb{E}_{\mu}[\mathbb{1}[\mu(s,a)\leq 2b]]\] \[\leq\inf_{b}\frac{\sup_{s,a,t}d_{t}^{\pi}(s,a)}{b}\frac{1}{\sqrt{N} }+C|\mathcal{S}||\mathcal{A}|2b\] \[=O\left(\frac{\sqrt{C|\mathcal{S}||\mathcal{A}|}}{N^{1/4}}\right)\]

Thus,

\[V_{\bar{r}}^{\pi}(d_{0})-V_{\bar{r}}^{\hat{\pi}}(d_{0})\leq\tilde{O}\left( \frac{V_{\max}}{(1-\gamma)^{3}}\left(\sqrt{\frac{|\mathcal{S}||\mathcal{A}|\ln (1/\delta)}{N}}+\frac{\sqrt{C|\mathcal{S}||\mathcal{A}|}}{N^{1/4}}\right)\right)\]

We can apply this similar argument used in the previous proofs to show that this bound simultaneously applies to all \(\bar{r}\in\mathcal{R}_{R}(\tilde{r})\) since the high-probability concentration analysis is only taken on the data distribution where all rewards in \(\mathcal{R}_{R}(\tilde{r})\) are equal. Thus, when \(V_{\max}\geq\frac{R}{1-\gamma}\), PPQ (and similarly PQI) is \(R\)-admissible.

[MISSING_PAGE_FAIL:54]

**Proposition 12** (MOReL Performance Guarantee).: _Given data \(\mathcal{D}=\left\{(s,a,\tilde{r},s^{\prime})\right\}\) of size \(N\) of an unknown MDP \(\left(\mathcal{S},\mathcal{A},\tilde{r},P,\gamma\right)\). Suppose \(\tilde{r},r_{M}^{*}\in[-1,1]\) and \(\left(\tilde{r},P\right)\in\mathcal{M}_{\mathcal{D}}^{\alpha}\). Choose \(\xi=\min(1,O(V_{\max\left(|\mathcal{S}|^{2}|\mathcal{A}|\log(\nicefrac{{1}}{{ 3}})/N\right)^{1/4}}))\). Let \(X\geq R\). For \(\pi\) such that \(\sup_{s,a}\frac{d^{\pi}(s,a)}{\mu(s,a)}=C<\infty\), with probability \(1-\delta\), it holds that_

\[\left(1-\gamma\right)\left(V_{\tilde{r}}^{\pi}(d_{0})-V_{\tilde{r}}^{\hat{\pi} }(d_{0})\right)\leq O\left(V_{\max}\left(\frac{|\mathcal{S}|^{2}|\mathcal{A}| \log(\frac{1}{\delta})}{N}\right)^{1/4}\right),\quad\text{with }V_{\max}:=\frac{R}{1-\gamma}\]

_for all \(\bar{r}\in\mathcal{R}_{R}(\tilde{r})\coloneqq\left\{\bar{r}:\bar{r}(s,a)= \bar{r}(s,a),\forall(s,a)\in\text{supp}(\mu)\text{ and }\bar{r}(s,a)\in\left[-R,1 \right],\forall(s,a)\in\mathcal{S}\times\mathcal{A}\right\}\)._

Proof.: We extend \(\tilde{r}\) as \(\tilde{r}(s^{\dagger},a)=-X\) for all \(a\in\mathcal{A}\). Let \(\hat{M}=\left(\hat{r},\hat{P}\right)\).

First, by the optimality of \(\hat{\pi}\), we write

\[V_{\tilde{r}}^{\pi}(d_{0})-V_{\tilde{r}}^{\hat{\pi}}(d_{0})\] \[= V_{\tilde{r}}^{\pi}(d_{0})-V_{\tilde{M}}^{\pi}(d_{0})+V_{\tilde{ M}}^{\pi}(d_{0})-V_{\tilde{M}}^{\hat{\pi}}(d_{0})+V_{\tilde{M}}^{\hat{\pi}}(d_{0})-V _{\tilde{r}}^{\hat{\pi}}(d_{0})\] \[\leq V_{\tilde{r}}^{\pi}(d_{0})-V_{\tilde{M}}^{\pi}(d_{0})+V_{\tilde{M} }^{\hat{\pi}}(d_{0})-V_{\tilde{r}}^{\hat{\pi}}(d_{0})\]

where \(V_{\tilde{M}}^{\pi}\) denotes the value of policy \(\pi\) with respect to the reward \(\hat{r}\) and dynamics \(\hat{P}\). The last inequality follows from the optimality of \(\hat{\pi}\) on \(\hat{M}\).

Then we notice the fact that \(\mathcal{K}_{\xi}\subseteq\text{supp}(\mu)\) with a small \(\xi\). The proof of the lemma below is given in Appendix F.7.3.

**Lemma 7**.: _If \(\xi\leq 1\), \(\mathcal{K}_{\xi}\subseteq\text{supp}(\mu)\)._

Let \(\hat{d}^{\pi}\) denote the average state-action visitation of \(\pi\) with respect to \(\hat{P}\).

\[(1-\gamma)\left(V_{\hat{M}}^{\hat{\pi}}(d_{0})-V_{\tilde{r}}^{\hat {\pi}}(d_{0})\right)\] \[= \mathbb{E}_{s,a\sim d^{\xi}}[\hat{r}(s,a)+\gamma\mathbb{E}_{s^{ \prime}\sim\hat{P}|s,a}[V_{\tilde{r}}^{\hat{\pi}}(s^{\prime})]-\bar{r}(s,a)- \gamma\mathbb{E}_{s^{\prime}\sim P|s,a}[V_{\tilde{r}}^{\hat{\pi}}(s^{\prime})]]\] \[\leq \mathbb{E}_{s,a\sim\hat{d}^{\pi}}[\hat{r}(s,a)+\gamma\mathbb{E}_{ s^{\prime}\sim\hat{P}|s,a}[V_{\tilde{r}}^{\hat{\pi}}(s^{\prime})]-\bar{r}(s,a)- \gamma\mathbb{E}_{s^{\prime}\sim P|s,a}[V_{\tilde{r}}^{\hat{\pi}}(s^{\prime})]] (s,a)\notin\mathcal{K}_{\xi}]\] \[\quad+\mathbb{E}_{s,a\sim d^{\xi}}[|\hat{r}(s,a)-\bar{r}(s,a)|+ \gamma V_{\max}D_{TV}(\hat{P}(\cdot|s,a),P(\cdot|s,a))|(s,a)\in\mathcal{K}_{\xi}]\] \[\leq \xi+\mathbb{E}_{s,a\sim\hat{d}^{\pi}}[\hat{r}(s,a)+\gamma\mathbb{E }_{s^{\prime}\sim\hat{P}|s,a}[V_{\tilde{r}}^{\hat{\pi}}(s^{\prime})]-\bar{r}(s,a )-\gamma\mathbb{E}_{s^{\prime}\sim P|s,a}[V_{\tilde{r}}^{\hat{\pi}}(s^{\prime})] |(s,a)\notin\mathcal{K}_{\xi}]\] \[\leq \xi+\mathbb{E}_{s,a\sim\hat{d}^{\pi}}[-V_{\max}-\bar{r}(s,a)+ \gamma V_{\max}|(s,a)\notin\mathcal{K}_{\xi}]\] \[\leq \xi\]

where in the second inequality we use \(\mathcal{K}_{\xi}\subseteq\text{supp}(\mu)\) (which implies \(\bar{r}=\bar{r}\) in \(\mathcal{K}_{\xi}\)).

Similarly we can show

\[(1-\gamma)\left(V_{\tilde{M}}^{\pi}(d_{0})-V_{\tilde{r}}^{\pi}(d_{ 0})\right)\] \[\geq -\xi+\mathbb{E}_{s,a\sim d^{\xi}}[\hat{r}(s,a)+\gamma\mathbb{E}_ {s^{\prime}\sim\hat{P}|s,a}[V_{\tilde{M}}^{\pi}(s^{\prime})]-\bar{r}(s,a)- \gamma\mathbb{E}_{s^{\prime}\sim P|s,a}[V_{\tilde{M}}^{\pi}(s^{\prime})]|(s,a) \notin\mathcal{K}_{\xi}]\] \[\geq -\xi-2V_{\max}\mathbb{E}_{s,a\sim d^{\pi}}[\mathbbm{1}[(s,a)\notin \mathcal{K}_{\xi}]]\]

Combining the three inequalities above gives

\[(1-\gamma)\left(V_{\tilde{r}}^{\pi}(d_{0})-V_{\tilde{r}}^{\hat{\pi}}(d_{0}) \right)\leq 2V_{\max}\mathbb{E}_{s,a\sim d^{\pi}}[\mathbbm{1}[(s,a)\notin \mathcal{K}_{\xi}]]+2\xi\]

By the concentratability assumption, we can further upper bound

\[\mathbb{E}_{s,a\sim d^{\pi}}[\mathbbm{1}[(s,a)\notin\mathcal{K}_{\xi}]]\leq C \mathbb{E}_{s,a\sim\mu}[\mathbbm{1}[(s,a)\notin\mathcal{K}_{\xi}]]\]

Therefore we have

\[(1-\gamma)\left(V_{\tilde{r}}^{\pi}(d_{0})-V_{\tilde{r}}^{\hat{\pi}}(d_{0})\right) \leq 2V_{\max}\mathbb{E}_{s,a\sim\mu}[\mathbbm{1}[(s,a)\notin \mathcal{K}_{\xi}]]+2\xi\] \[\leq 2V_{\max}\frac{\mathbb{E}_{s,a\sim\mu}[\mathbb{E}_{\text{supp} }(s,a)]}{\xi}+2\xi\] \[\leq 4V_{\max}\frac{\sup_{r_{M},P_{M}}\mathbb{E}_{s,a\sim\mu}[E(s,a;r_ {M},P_{M})]}{\xi}+2\xi\]where the last step is Markov inequality,

\[E_{\text{sup}}(s,a)=\sup_{M_{1},M_{2}\in\mathcal{M}_{\mathcal{D}}^{ \star}}\big{\{}|r_{M_{1}}(s,a)-r_{M_{2}}(s,a)|+\gamma V_{\max}D_{TV}(P_{M_{1}}( \cdot|s,a),P_{M_{2}}(\cdot|s,a))\big{\}},\]

and

\[E(s,a;r_{M},P_{M})=|r_{M}(s,a)-\tilde{r}(s,a)|+\gamma V_{\max}D_{TV}(P_{M}( \cdot|s,a),P(\cdot|s,a)).\]

Now we upper bound the expectation of \(E\) over \(\mu\). With probability greater than \(1-\delta\),

\[\mathbb{E}_{\mu}[E(s,a;r_{M},P_{M})] \leq\sqrt{\mathbb{E}_{\mu}[(|r_{M}(s,a)-\tilde{r}(s,a)|+\gamma V_ {\max}D_{TV}(P_{M}(\cdot|s,a),P(\cdot|s,a)))^{2}]}\] \[\leq\sqrt{2}\sqrt{\mathbb{E}_{\mu}[(r_{M}(s,a)-\tilde{r}(s,a))^{2 }+(\gamma V_{\max})^{2}D_{TV}(P_{M}(\cdot|s,a),P(\cdot|s,a))^{2}]}\] \[\leq(1+\gamma V_{\max})\sqrt{2}\sqrt{\mathbb{E}_{\mu}[(r_{M}(s,a) -\tilde{r}(s,a))^{2}+D_{TV}(P_{M}(\cdot|s,a),P(\cdot|s,a))^{2}]}\] \[\leq V_{\max}\sqrt{2}\sqrt{\frac{|\mathcal{S}|^{2}|\mathcal{A}| \log(\frac{1}{\delta})}{N}}\]

where the last step is based on Lemma 8.

**Lemma 8**.: _[_34_]_ _With probability \(1-\delta\), for any MDP model \(M\),_

\[\mathbb{E}_{\mu}[|r_{M}(s,a)-r(s,a)|^{2}+D_{TV}(P_{M}(\cdot|s,a),P(\cdot|s,a)) ^{2}]\leq O\left(\frac{\mathcal{E}_{\mathcal{D}}(M)-\min_{M^{\prime}} \mathcal{E}_{\mathcal{D}}(M^{\prime})+|\mathcal{S}|^{2}|\mathcal{A}|\log( \frac{1}{\delta})}{N}\right).\]

Thus, we have

\[(1-\gamma)\left(V_{\tilde{r}}^{\pi}(d_{0})-V_{\tilde{r}}^{\hat{ \pi}}(d_{0})\right) \leq O\left(\frac{V_{\max}^{2}\sqrt{|\mathcal{S}|^{2}|\mathcal{A}| \log(\frac{1}{\delta})}}{\xi\sqrt{N}}\right)+2\xi\] \[\leq O\left(V_{\max}\left(\frac{|\mathcal{S}|^{2}|\mathcal{A}| \log(\frac{1}{\delta})}{N}\right)^{1/4}\right).\]

By Proposition 12, we can apply the previous analysis technique to MOReL to show it is admissible without addition union bounds, which leads to the corollary below.

**Corollary 7**.: _For \(X\geq R\), MOReL is \(R\)-admissible with respect any \(\tilde{\mathcal{R}}\subseteq(\mathcal{S}\times\mathcal{A}\rightarrow[-1,1]).\)_

#### f.7.2 Mopo

MOPO [32] is very similar to MOReL except that it uses negative bonuses (like VI-LCB) instead of truncation. In the original paper of MOPO, the authors assume the reward is given. Here we consider a variant that also learns the reward. Specifically, given \(P_{M}^{\star}\) and \(r_{M}^{\star}\) above it solves the MDP \((\mathcal{S},\mathcal{A},P_{M}^{\star},\hat{r},\gamma)\), where

\[\hat{r}(s,a)=r_{M}^{\star}(s,a)-b(s,a)\]

While the original MOPO paper does not give a specific design of \(b(s,a)\) that is provably correct, in principle making MOPO provably correct is possible. Essentially, we need to choose a large enough bonus to cover the error of the model-based Bellman operator (defined by the estimated reward and dynamics). This would lead to a bonus of order \(V_{\max}\) (see the analysis of VI-LCB in Appendix F.5). Then we can proceed with an analysis that combines that of MOReL and VI-LCB to prove MOPO's performance guarantee. This can then be used to show (like the previous proofs) that MOPO is admissible. We omit the proof here.

**Corollary 8**.: _Suppose \(b(s,a)=\Theta(V_{\max})\) for \((s,a)\notin\text{supp}(\mu)\). For \(V_{\max}\geq\frac{R}{1-\gamma}\), MOPO is \(R\)-admissible with respect any \(\tilde{\mathcal{R}}\subseteq(\mathcal{S}\times\mathcal{A}\rightarrow[-1,1]).\)_

#### f.7.3 Proof of Technical Lemma

Proof of Lemma 7.: Consider any \((\bar{s},\bar{a})\notin\text{supp}(\mu)\). Given any \(M=(\mathcal{S},\mathcal{A},P_{M},r_{M},\gamma)\in\mathcal{M}_{\mathcal{D}}^{\alpha}\), we construct \(M^{\prime}=(\mathcal{S},\mathcal{A},P_{M},r_{M^{\prime}},\gamma)\) with \(r_{M^{\prime}}=r_{M}\) for all \((s,a)\in(\mathcal{S}\times\mathcal{A})\setminus\{(\bar{s},\bar{a})\}\), and \(r_{M^{\prime}}(\bar{s},\bar{a})=-\text{sign}(r_{M}(\bar{s},\bar{a}))\). By construction, since \(M\) and \(M^{\prime}\) agrees on all state-actions except \((\bar{s},\bar{a})\), which is not in data support, we have \(M^{\prime}\in\mathcal{M}_{\mathcal{D}}^{\alpha}\). However, we have that

\[\sup_{M_{1},M_{2}\in\mathcal{M}_{\mathcal{D}}^{\alpha}}\left\{|r_ {M_{1}}(\bar{s},\bar{a})-r_{M_{2}}(\bar{s},\bar{a})|+\gamma V_{\max}D_{TV}(P_ {M_{1}}(\cdot|\bar{s},\bar{a}),P_{M_{2}}(\cdot|\bar{s},\bar{a}))\right\}\] \[\geq |r_{M}(\bar{s},\bar{a})-r_{M^{\prime}}(\bar{s},\bar{a})|+\gamma V _{\max}D_{TV}(P_{M}(\cdot|\bar{s},\bar{a}),P_{M}(\cdot|\bar{s},\bar{a}))\] \[= |r_{M}(\bar{s},\bar{a})-r_{M^{\prime}}(\bar{s},\bar{a})|\geq 1 \geq\xi.\]

This means that \((\bar{s},\bar{a})\notin\mathcal{K}_{\xi}\). Therefore \(\mathcal{K}_{\xi}\subseteq\text{supp}(\mu)\). 

## Appendix G Grid World

In this section, we describe the full details of the grid world study that was discussed in Section 3.1.

Environment.We consider goal-directed navigation in the environment shown in Fig. 10. We use a fixed grid world layout shown in the figure. The agent's state is given by a tuple \((x,y,d)\) where \((x,y)\in[5]^{2}\) represents the 2-d coordinate and \(d\in\{N,W,E,S\}\) encodes the North, West, East, and South direction respectively that the agent is facing. The agent's action space is \(\mathcal{A}=\{f,l,r\}\) where \(f\) denotes the action of moving to grid square in front of the agent, \(l\) denotes a left turn of 90 degrees, and \(r\) denotes a right turn of 90 degrees. The agent can go through the lava (orange wavy square) but cannot go through the wall (grey square). The goal (key) is an absorbing state. The agent gets a reward of +1 when first visiting the goal followed by a reward of 0 for then staying in the goal. The agent gets a reward of -1 for reaching lava and a reward of -0.01 for all other actions to incentivize the agent to reach the goal along the shortest safe path. We use a horizon of \(H=20\). The agent deterministically starts in the top-right corner shown in Fig. 10.

Dataset.We collect a dataset \(\mathcal{D}\) of 500 episodes by taking 100 identical optimal episodes along with 400 identical episodes that all touch the topmost lava field. We introduce a length bias by terminating an episode if the agent touches the lava or fails to reach the goal in \(H-1\) steps (i.e., the agent doesn't survive). In addition to the original setting with the observed reward, we consider three additional settings where the rewards in the dataset are replaced by: (i) a constant zero reward, (ii) their negative, and (iii) a random number sampled uniformly in \([0,1]\).

Figure 10: **Left**: A goal-directed gridworld navigation task where the agent (red triangle) has to reach the goal (key) and avoid lava (orange waves). **Center**: Shows the length bias that helps offline RL succeed even when rewards are entirely replaced with random or adversarial values. **Right**: Results on the grid world task. Behavior cloning fails to solve the task (red path) while using an offline RL method leads to success (blue path) _even with wrong rewards_. The opacity of the grid square overlay indicates how frequent that state is in the dataset (more opaque means more frequent).

**Algorithm 4** PEVI\((\mathcal{S},\mathcal{A},H,\mathcal{D},\beta,V_{\min},V_{\max})\)[12]. We are given access to a set \(\mathcal{D}\) of episodes (\(\tau\)). Additionally, we are given a hyperparameter \(\beta\in\mathcal{R}_{\geq 0}\) that controls pessimism. The hyperparameters \(V_{\min},V_{\max}\in\mathcal{R}^{H}\) indicate bounds on policy value with \(V_{\min,h}\) and \(V_{\max,h}\) denoting the minimum and maximum policy values respectively for time step \(h\in[H]\). Unlike the original PEVI algorithm of [12], we treat \(V_{\min},V_{\max}\) as hyperparameters.

```  Set \(V_{H+1}=0\) for\(h=H,\cdots,1\)do for\(s\in\mathcal{S}\)do for\(a\in\mathcal{A}\)do \(n_{h}(s,a)=\sum_{\tau\in\mathcal{D}}\mathbbm{1}\{s_{h}^{\tau}=s\wedge a_{h}^{ \tau}=a\}\) if\(n_{h}(s,a)>0\) \(\widehat{Q}_{h}(s,a)=\frac{1}{n_{h}(s,a)}\sum_{\tau\in\mathcal{D}}\mathbbm{1} \{s_{h}^{\tau}=s\wedge a_{h}^{\tau}=a\}\{r_{h}^{\tau}+V_{h+1}(s_{h+1}^{\tau})\}\) \(\widehat{Q}_{h}(s,a)=\widehat{Q}_{h}(s,a)-\frac{\beta}{\sqrt{n_{h}(s,a)}}\) \(\widehat{Q}_{h}(s,a)=\max\{V_{\min,h},\min\{\bar{Q}_{h}(s,a),V_{\max,h}\}\}\) Else \(\widehat{Q}_{h}(s,a)=V_{\min,h}\) endfor \(\pi_{h}(s)=\arg\max_{a\in\mathcal{A}}\widehat{Q}_{h}(s,a)\) \(V_{h}(s)=\widehat{Q}_{h}(s,\pi_{h}(s))\) endfor endfor Return\(\pi=(\pi_{1:H})\) ```

**Algorithm 4** PEVI\((\mathcal{S},\mathcal{A},H,\mathcal{D},\beta,V_{\min},V_{\max})\)[12]. We are given access to a set \(\mathcal{D}\) of episodes (\(\tau\)). Additionally, we are given a hyperparameter \(\beta\in\mathcal{R}_{\geq 0}\) that controls pessimism. The hyperparameters \(V_{\min},V_{\max}\in\mathcal{R}^{H}\) indicate bounds on policy value with \(V_{\min,h}\) and \(V_{\max,h}\) denoting the minimum and maximum policy values respectively for time step \(h\in[H]\). Unlike the original PEVI algorithm of [12], we treat \(V_{\min},V_{\max}\) as hyperparameters.

Methods.We evaluate two methods: behavior cloning which simply takes the action with the highest empirical count in a given state and a random action if the state was unseen, and PEVI [12] an offline RL method with guarantees for tabular MDP. We provide a pseudocode of PEVI in Algorithm 4. Unlike the original PEVI in [12], we provide minimum and maximum bounds for the value function for each time step as an additional hyperparameter. These bounds should be dictated by our CMDP framework and should penalize out-of-distribution actions. In principle, we need to set these bounds such that the algorithm is admissible. Please see Appendix D and Appendix E for details.

Intuitively, PEVI performs dynamic programming similar to a standard value iteration with two key changes. Firstly, it uses pessimistic value initialization where the learned value \(V_{h}(s)\) of a state \(s\) at time step \(h\) is set to the lowest possible return \(V_{min,h}\) if the state is unseen at time step \(h\). Secondly, when performing value iteration it adds a pessimism penalty of \(\nicefrac{{-\beta}}{{\sqrt{n(s,a)}}}\) to the dataset reward where \(n(s,a)\) is the empirical state-action count in \(\mathcal{D}\). This pessimism penalty ensures that the learned value function lower bounds the true value function (hence a pessimistic estimate). In our experiments, we vary \(\beta\) and set \(V_{\min,h}=\tilde{V}_{\min,h}-\beta(H-h+1)-1\) where \(\tilde{V}_{\min,h}\) is the minimum possible return for the given reward type that we are working with. The value of \(\tilde{V}_{\min,h}\) is \(-(H-h+1)\) for the original reward, \(-1\) for the negative reward, and \(0\) for the zero reward and random reward cases). We define \(V_{\max,h}=\tilde{V}_{\max,h}\) where \(\tilde{V}_{\max,h}\) is the maximum possible return for the given reward type. The value of \(\tilde{V}_{\max,h}\) is \(1\) of the original reward, \((H-h+1)\) for the random reward and the negative reward, and \(0\) for the zero reward case.

Results.We show numerical results in Table 16. We visualize results in Fig. 10. The behavior cloning simply imitates the most common trajectory in the dataset which results in going to the lava (shown in red in the figure). In contrast, the PEVI is able to reach the goal in every reward situation.

Explanation.The reason why PEVI works can be understood simply by pessimistic initialization and length bias. The length bias in this setting is visualized in the center of Fig. 10. PEVI assigns the final state \(s_{t}^{\prime}\) of a failed trajectory will get a pessimistic value of \(V_{t}(s_{t}^{\prime})=V_{\min,*}\). This value isn't updated as we never take any action in \(s_{t}^{\prime}\) due to timeout. In our case, \(s_{t}^{\prime}\) will be the state where the agent first visits the lava. In contrast, the final state of a successful trajectory \(s_{H+1}\) gets assigned a value of \(0\). If \(V_{\min}\) is sufficiently negative, then PEVI will only consider policies that stay all \(H\) steps within the data support. Due to length bias, this is not true for the non-surviving trajectories thatreach the lava. Further, the only trajectories in our case that complete end up reaching and staying in the goal. Therefore, under the assumption that \(V_{\min,h}\) are all sufficiently small, PEVI will learn a policy reaches the goal irrespective of the correctness of the reward in the data. The sufficiency of the negativity of \(V_{\min,h}\) is implied by our CMDP framework. Lastly, note that as the world is deterministic, we didn't need to use the pessimism arising from the \(\frac{-\beta}{\sqrt{n(s,a)}}\) penalty term.

\begin{table}
\begin{tabular}{l|c|c} \hline
**Reward Type** & **Behavior Cloning** & **PEVI** \\ \hline Original Reward & \(-1.19\pm 0.00\) & \(0.92\pm 0.00\) \\ Zero Reward & \(-1.19\pm 0.00\) & \(0.92\pm 0.00\) \\ Random Reward & \(-1.19\pm 0.00\) & \(0.92\pm 0.00\) \\ Negative Reward & \(-1.19\pm 0.00\) & \(0.92\pm 0.00\) \\ \hline \end{tabular}
\end{table}
Table 16: **Gridworld results:** Mean return on 1000 test episodes. PEVI achieves the optimal return of \(V^{\star}=0.92\).