# Memory-Constrained Algorithms for Convex Optimization

 Moise Blanchard

Operations Research Center

MIT

Cambridge, MA 02139

moiseb@mit.edu

Junhui Zhang

Operations Research Center

MIT

Cambridge, MA 02139

junhuiz@mit.edu

Patrick Jaillet

Department of Electrical Engineering and Computer Science

MIT

Cambridge, MA 02139

jaillet@mit.edu

###### Abstract

We propose a family of recursive cutting-plane algorithms to solve feasibility problems with constrained memory, which can also be used for first-order convex optimization. Precisely, in order to find a point within a ball of radius \(\epsilon\) with a separation oracle in dimension \(d\)--or to minimize \(1\)-Lipschitz convex functions to accuracy \(\epsilon\) over the unit ball--our algorithms use \(\mathcal{O}(\frac{d^{2}}{p}\ln\frac{1}{\epsilon})\) bits of memory, and make \(\mathcal{O}((C\frac{d}{p}\ln\frac{1}{\epsilon})^{p})\) oracle calls, for some universal constant \(C\geq 1\). The family is parametrized by \(p\in[d]\) and provides an oracle-complexity/memory trade-off in the sub-polynomial regime \(\ln\frac{1}{\epsilon}\gg\ln d\). While several works gave lower-bound trade-offs (impossibility results) [31; 5]--we explicit here their dependence with \(\ln\frac{1}{\epsilon}\), showing that these also hold in any sub-polynomial regime--to the best of our knowledge this is the first class of algorithms that provides a positive trade-off between gradient descent and cutting-plane methods in any regime with \(\epsilon\leq 1/\sqrt{d}\). The algorithms divide the \(d\) variables into \(p\) blocks and optimize over blocks sequentially, with approximate separation vectors constructed using a variant of Vaidya's method. In the regime \(\epsilon\leq d^{-\Omega(d)}\), our algorithm with \(p=d\) achieves the information-theoretic optimal memory usage and improves the oracle-complexity of gradient descent.

## 1 Introduction

Optimization algorithms are ubiquitous in machine learning, from solving simple regressions to training neural networks. Their essential roles have motivated numerous studies on their efficiencies, which are usually analyzed through the lens of oracle-complexity: given an oracle (such as function value, or subgradient oracle), how many calls to the oracle are needed for an algorithm to output an approximate optimal solution? [34]. However, ever-growing problem sizes have shown an inadequacy in considering only the oracle-complexity, and have motivated the study of the trade-off between oracle-complexity and other resources such as memory [52; 31; 5] and communication[25; 40; 42; 45; 33; 53; 51; 50].

In this work, we study the oracle-complexity/memory trade-off for first-order non-smooth convex optimization, and the closely related feasibility problem, with a focus on developing memory efficient(deterministic) algorithms. Since [52] formally posed as open problem the question of characterizing this trade-off, there have been exciting results showing what is impossible: for convex optimization in \(\mathbb{R}^{d}\), [31] shows that any randomized algorithm with \(d^{1.25-\delta}\) bits of memory needs at least \(\tilde{\Omega}(d^{1+4\delta/3})\) queries, and this has later been improved for deterministic algorithms to \(d^{1-\delta}\) bits of memory or \(\tilde{\Omega}(d^{1+\delta/3})\) queries by [5]; in addition [5] shows that for the feasibility problem with a separation oracle, any algorithm which uses \(d^{2-\delta}\) bits of memory needs at least \(\tilde{\Omega}(d^{1+\delta})\) queries.

Despite these recent results on the lower bounds, all known first-order convex optimization algorithms that output an \(\epsilon\)-suboptimal point fall into two categories: those that have quadratic memory in the dimension \(d\) but can potentially achieve the optimal \(\mathcal{O}(d\ln\frac{1}{\epsilon})\) query complexity, as represented by the center-of-mass method, and those that have \(\mathcal{O}(\frac{1}{\epsilon^{2}})\) query complexity but only need the optimal \(\mathcal{O}(d\ln\frac{1}{\epsilon})\) bits of memory, as represented by the classical gradient descent [52]. In addition, the above-mentioned memory bounds apply only between queries, and in particular the center-of-mass method [52] is allowed to use infinite memory during computations.

We propose a family of memory-constrained algorithms for the stronger feasibility problem in which one aims to find a point within a set \(Q\) containing a ball of radius \(\epsilon\), with access to a separation oracle. In particular, this can be used for convex optimization since the subgradient information provides a separation vector. Our algorithms use \(\mathcal{O}(\frac{d^{2}}{p}\ln\frac{1}{\epsilon})\) bits of memory (including during computations) and \(\mathcal{O}((C\frac{d}{p}\ln\frac{1}{\epsilon})^{p})\) queries for some universal constant \(C\geq 1\), and a parameter \(p\in[d]\) that can be chosen by the user. Intuitively, in the context of convex optimization, the algorithms are based on the idea that for any function \(f(\bm{x},\bm{y})\) convex in the pair \((\bm{x},\bm{y})\), the partial minimum \(\min_{\bm{y}}f(\bm{x},\bm{y})\) as a function of \(\bm{x}\) is still convex and, using a variant of Vaidya's method proposed in [27], our algorithm can approximate subgradients for that function \(\min_{\bm{y}}f(\bm{x},\bm{y})\), thereby turning an optimization problem with variables \((\bm{x},\bm{y})\) to one with just \(\bm{x}\). This idea, applied recursively with the variables divided into \(p\) blocks, gives our family of algorithms and the above-mentioned memory and query complexity. The main algorithmic contribution is in how we design the recursive dimension reduction procedure: a technical step of the design and analysis is to ensure that the necessary precision for recursive computations can be achieved using low memory. Last, our algorithms account for memory usage throughout computations, as opposed to simply between calls to the gradient oracle, which was the traditional approach in the literature.

When \(p=1\), our algorithm is a memory-constrained version of Vaidya's method [48; 27], and improves over the center-of-mass [52] method by a factor of \(\ln\frac{1}{\epsilon}\) in terms of memory while having optimal oracle-complexity. The improvements provided by our algorithms are more significant in regimes when \(\epsilon\) is very small in the dimension \(d\): increasing the parameter \(p\) can further reduce the memory usage of Vaidya's method (\(p=1\)) by a factor \(\ln\frac{1}{\epsilon}/\ln d\), while still improving over the oracle-complexity of gradient descent. In particular, in a regime \(\ln\frac{1}{\epsilon}=\text{poly}(\ln d)\), these memory improvements are only in terms of \(\ln d\) factors. However, in sub-polynomial regimes with potentially \(\ln\frac{1}{\epsilon}=d^{c}\) for some constant \(c>0\), these provide polynomial improvements to the memory of standard cutting-plane methods.

As a summary, this paper makes the following contributions.

* Our class of algorithms provides a trade-off between memory-usage and oracle-complexity whenever \(\ln\frac{1}{\epsilon}\gg\ln d\). Further, taking \(p=1\) improves the memory-usage from center-of-mass [52] by a factor \(\ln\frac{1}{\epsilon}\), while preserving the optimal oracle-complexity.
* For \(\ln\frac{1}{\epsilon}\geq\Omega(d\ln d)\), our algorithm with \(p=d\) is the first known algorithm that outperforms gradient descent in terms of the oracle-complexity, but still maintains the optimal \(\mathcal{O}(d\ln\frac{1}{\epsilon})\) memory usage.
* We show how to obtain a \(\ln\frac{1}{\epsilon}\) dependence in the known lower-bound trade-offs [31; 5], confirming that the oracle-complexity/memory trade-off is necessary for any regime \(\epsilon\lesssim\frac{1}{\sqrt{d}}\).

## 2 Setup and Preliminaries

In this section, we precise the formal setup for our results. We follow the framework introduced in [52], to define the memory constraint on algorithms with access to an oracle \(\mathcal{O}:\mathcal{S}\to\mathcal{R}\) which takes as input a query \(q\in\mathcal{S}\) and outputs a response \(\mathcal{O}(q)\in\mathcal{R}\). Here, the algorithm is constrained to update an internal \(M\)-bit memory between queries to the oracle.

**Definition 2.1** (\(M\)-bit memory-constrained algorithm [52; 31; 5]).: _Let \(\mathcal{O}:\mathcal{S}\to\mathcal{R}\) be an oracle. An \(M\)-bit memory-constrained algorithm is specified by a query function \(\psi_{query}:\{0,1\}^{M}\to\mathcal{S}\) and an update function \(\psi_{update}:\{0,1\}^{M}\times\mathcal{S}\times\mathcal{R}\to\{0,1\}^{M}\). The algorithm starts with the memory state \(\mathsf{Memory}_{0}=0^{M}\) and iteratively makes queries to the oracle. At iteration \(t\), it makes the query \(q_{t}=\psi_{query}(\mathsf{Memory}_{t-1})\) to the oracle, receives the response \(r_{t}=\mathcal{O}(q_{t})\) then updates its memory \(\mathsf{Memory}_{t}=\psi_{update}(\mathsf{Memory}_{t-1},q_{t},r_{t})\)._

The algorithm can stop at any iteration and the last query is its final output. Importantly, this model does not enforce constraints on the memory usage during the computation of \(\psi_{update}\) and \(\psi_{query}\). This is ensured in the stronger notion of a memory-constrained algorithm with computations. These are precisely algorithms that have constrained memory including for computations, with the only specificity that they need a decoder function \(\phi\) to make queries to the oracle from their bit memory, and a discretization function \(\psi\) to write a discretized response into the algorithm's memory.

**Definition 2.2** (\(M\)-bit memory-constrained algorithm with computations).: _Let \(\mathcal{O}:\mathcal{S}\to\mathcal{R}\) be an oracle. We suppose that we are given a decoding function \(\phi:\{0,1\}^{\star}\to\mathcal{S}\) and a discretization function \(\psi:\mathcal{R}\times\mathbb{N}\to\{0,1\}^{\star}\) such that \(\psi(r,n)\in\{0,1\}^{n}\) for all \(r\in\mathcal{R}\). An \(M\)-bit memory-constrained algorithm with computations is only allowed to use an \(M\)-bit memory in \(\{0,1\}^{M}\) even during computations. The algorithm has three special memory placements \(Q,N,R\). Say the contents of \(Q\) and \(N\) are \(q\) and \(n\) respectively. To make a query, \(R\) must contain at least \(n\) bits. The algorithm submits \(q\) to the encoder which then submits the query \(\phi(q)\) to the oracle. If \(r=\mathcal{O}(\phi(q))\) is the oracle response, the discretization function then writes \(\psi(r,n)\) in the placement \(R\)._

Feasibility problem.In this problem, the goal is to find a point \(\bm{x}\in Q\), where \(Q\subset\mathcal{C}_{d}:=[-1,1]^{d}\) is a convex set. We choose the cube \([-1,1]^{d}\) as prior bound for convenience in our later algorithms, but the choice of norm for this prior ball can be arbitrary and does not affect our results. The algorithm has access to a _separation oracle_\(O_{S}:\mathcal{C}_{d}\to\{\mathsf{Success}\}\cup\mathbb{R}^{d}\), such that for a query \(\bm{x}\in\mathbb{R}^{d}\) either returns \(\mathsf{Success}\) if \(\bm{x}\in Q\), or a separating hyperplane \(\bm{g}\in\mathbb{R}^{d}\), i.e., such that \(\bm{g}^{\top}\bm{x}<\bm{g}^{\top}\bm{x}^{\prime}\) for any \(\bm{x}^{\prime}\in Q\). We suppose that the separating hyperplanes are normalized, \(\|\bm{g}\|_{2}=1\). An algorithm solves the feasibility problem with accuracy \(\epsilon\) if the algorithm is successful for any feasibility problem such that \(Q\) contains an \(\epsilon\)-ball \(B_{d}(\bm{x}^{\star},\epsilon)\) for \(\bm{x}^{\star}\in\mathcal{C}_{d}\).

As an important remark, this formulation asks that the separation oracle is consistent over time: when queried at the exact same point \(\bm{x}\), the oracle always returns the same separation vector. In this context, we can use the natural decoding function \(\phi\) which takes as input \(d\) sequences of bits and outputs the vector with coordinates given by the sequences interpreted in base 2. Similarly, the natural discretization function \(\psi\) takes as input the separation hyperplane \(\bm{g}\) and outputs a discretized version up to the desired accuracy. From now, we can omit these implementation details and consider that the algorithm can query the oracle for discretized queries \(\bm{x}\), up to specified rounding errors.

**Remark 2.1**.: _An algorithm for the feasibility problem with accuracy \(\epsilon/(2\sqrt{d})\) can be used for first-order convex optimization. Suppose one aims to minimize a \(1\)-Lipschitz convex function \(f\) over the unit ball, and output an \(\epsilon\)-suboptimal solution, i.e., find a point \(\bm{x}\) such that \(f(\bm{x})\leq\min_{\bm{y}\in B_{d}(0,1)}f(\bm{y})+\epsilon\). A separation oracle for \(Q=\{\bm{x}:f(\bm{x})\leq\min_{\bm{y}\in B_{d}(0,1)}f(\bm{y})+\epsilon\}\) is given at a query \(\bm{x}\) by the subgradient information from the first-order oracle: \(\frac{\partial f(\bm{x})}{\|\partial\bm{f}(\bm{x})\|}\). Its computation can also be carried out memory-efficiently up to rounding errors since if \(\|\partial f(\bm{x})\|\leq\epsilon/(2\sqrt{d})\), the algorithm can return \(\bm{x}\) and already has the guarantee that \(\bm{x}\) is an \(\epsilon\)-suboptimal solution (\(\mathcal{C}_{d}\) has diameter \(2\sqrt{d}\)). Notice that because \(f\) is \(1\)-Lipschitz, \(Q\) contains a ball of radius \(\epsilon/(2\sqrt{d})\) (the factor \(1/(2\sqrt{d})\) is due to potential boundary issues). Hence, it suffices to run the algorithm for the feasibility problem while keeping in memory the queried point with best function value._

### Known trade-offs between oracle-complexity and memory

Known lower-bound trade-offs.All known lower bounds apply to the more general class of memory-constrained algorithms without computational constraints given in Definition 2.1. [34] first showed that \(\mathcal{O}(d\ln\frac{1}{\epsilon})\) queries are needed for solving convex optimization to ensure that one finds an \(\epsilon\)-suboptimal solution. Further, \(\mathcal{O}(d\ln\frac{1}{\epsilon})\) bits of memory are needed even just to output a solution in the unit ball with \(\epsilon\) accuracy [52]. These historical lower bounds apply in particular to the feasibility problem and are represented in the pictures of Fig. 1 as the dashed pink region.

More recently, [31] showed that achieving both optimal oracle-complexity and optimal memory is impossible for convex optimization. They show that a possibly randomized algorithm with \(d^{1.25-\delta}\) bits of memory makes at least \(\tilde{\Omega}(d^{1+4\delta/3})\) queries. This result was extended for deterministic algorithms in [5] which shows that a deterministic algorithm with \(d^{1-\delta}\) bits of memory makes \(\tilde{\Omega}(d^{1+\delta/3})\) queries. For the feasibility problem, they give an improved trade-off: any deterministic algorithm with \(d^{2-\delta}\) bits of memory makes \(\tilde{\Omega}(d^{1+\delta})\) queries. These trade-offs are represented in the left picture of Fig. 1 as the pink, red, and purple solid region, respectively. Using a clever and more careful analysis,[11] showed that similar lower bounds can be carried out for deterministic algorithms as well.

Known upper-bound trade-offs.Prior to this work, to the best of our knowledge only two algorithms were known in the oracle-complexity/memory landscape. First, cutting-plane algorithms achieve the optimal oracle-complexity \(\mathcal{O}(d\ln\frac{1}{\epsilon})\) but use quadratic memory. The memory-constrained (MC) center-of-mass method analyzed in [52] uses in particular \(\mathcal{O}(d^{2}\ln^{2}\frac{1}{\epsilon})\) memory. Instead, if one uses Vaidya's method which only needs to store \(\mathcal{O}(d)\) cuts instead of \(\mathcal{O}(d\ln\frac{1}{\epsilon})\), we show that one can achieve \(\mathcal{O}(d^{2}\ln\frac{1}{\epsilon})\) memory. These algorithms only use the separation oracle and hence apply to both convex optimization and the feasibility problem. On the other hand, the memory-constrained gradient descent for convex optimization [52] uses the optimal \(\mathcal{O}(d\ln\frac{1}{\epsilon})\) memory but makes \(\mathcal{O}(\frac{1}{\epsilon^{2}})\) iterations. While the analysis in [52] is only carried for convex optimization, we can give a modified proof showing that gradient descent can also be used for the feasibility problem.

### Other related works

Vaidya's method [48, 38, 1, 2] and the variant [27] that we use in our algorithms, belong to the family of cutting-plane methods. Perhaps the simplest example of an algorithm in this family is the center-of-mass method, which achieves the optimal \(\mathcal{O}(d\ln\frac{1}{\epsilon})\) oracle-complexity but is computationally intractable, and the only known random walk-based implementation [4] has computational complexity \(\mathcal{O}(d^{7}\ln\frac{1}{\epsilon})\). Another example is the ellipsoid method, which has suboptimal \(\mathcal{O}(d^{2}\ln\frac{1}{\epsilon})\) query complexity, but has an improved computational complexity \(\mathcal{O}(d^{4}\ln\frac{1}{\epsilon})\). [8] pointed out that Vaidya's method achieves the best of both worlds by sharing the \(\mathcal{O}(d\ln\frac{1}{\epsilon})\) optimal query complexity of the center-of-mass, and achieving a computational complexity of \(\mathcal{O}(d^{1+\omega}\ln\frac{1}{\epsilon})\)1. In a major breakthrough, this computational complexity was improved to \(\mathcal{O}(d^{3}\ln^{3}\frac{1}{\epsilon})\) in [27], then to \(\mathcal{O}(d^{3}\ln\frac{1}{\epsilon})\) in [22]. We refer to [8, 27, 22] for more detailed comparisons of these algorithms.

Footnote 1: \(\omega<2.373\) is the exponent of matrix multiplication

Another popular convex optimization algorithm that requires quadratic memory is the Broyden-Fletcher- Goldfarb- Shanno (BFGS) algorithm [43, 7, 20, 21], which stores an approximated inverse Hessian matrix as gradient preconditioner. Several works aimed to reduce the memory usage of BFGS; in particular, the limited memory BFGS (L-BFGS) stores a few vectors instead of the entire approximated inverse Hessian matrix [37, 30]. However, it is still an open question whether even the original BFGS converges for non-smooth convex objectives [29].

Lying at the other extreme of the oracle-complexity/memory trade-off is gradient descent, which achieves the optimal memory usage but requires significantly more queries than center-of-mass or Vaidya's method in the regime \(\epsilon\lesssim\frac{1}{\sqrt{d}}\). There is a rich literature of works aiming to speed up gradient descent, such as the optimized gradient method [17, 16], Nesterov's Acceleration [35], the triple momentum method [41], geometric descent [9], quadratic averaging [18], the information-theoretic exact method [46], or Big-Step-Little-Step method [23]. Interested readers can find a comprehensive survey on acceleration methods in [12]. However, these acceleration methods usually require additional smoothness or strong convexity assumptions (or both) on the objective function, due to the known \(\Omega(\frac{1}{\epsilon^{2}})\) query lower bound in the large-scale regime \(\epsilon\gtrsim\frac{1}{\sqrt{d}}\) for any first order method where the query points lie in the span of the subgradients of previous query points [36].

Besides accelerating gradient descent, researchers have investigated more efficient ways to leverage subgradients obtained in previous iterations. Of interest are bundle methods [3, 24, 28], that have found a wide range of applications [47; 26]. In their simplest form, they minimize the sum of the maximum of linear lower bounds constructed using past oracle queries, and a regularization term penalizing the distance from the current iteration variable. Although the theoretical convergence rate of the bundle method is the same as that of gradient descent, in practice, bundle methods can benefit from previous information and substantially outperform gradient descent [3].

Our works are focused on high-accuracy regimes, when the accuracy \(\epsilon\) is sub-polynomial. We note that for their lower-bound result on randomized algorithms, [11] also required sub-polynomial accuracies, which raises the question whether this is a general phenomenon for the study of memory-constrained algorithms in convex optimization. This also relates our work to the study of low-dimensional problems--or even constant dimension--which has been investigated in the literature [49; 10].

Last, the increasing size of optimization problems has also motivated the development of communication-efficient optimization algorithms in distributed settings such as [25; 40; 42; 45; 33; 53; 51; 50]. Moreover, recent works have explored the trade-off between sample complexity and memory/communication complexity for learning problems under the streaming model, with notable contributions including [6; 13; 14; 39; 44; 32].

## 3 Main results

We first check that the memory-constrained gradient descent method solves feasibility problems. This was known for convex optimization [52] and the same algorithm with a modified proof gives the following result. For completeness, the proof is given in Appendix D.

**Proposition 3.1**.: _The memory-constrained gradient descent algorithm solves the feasibility problem with accuracy \(\epsilon\leq\frac{1}{\sqrt{d}}\) using \(\mathcal{O}(d\ln\frac{1}{\epsilon})\) bits of memory and \(\mathcal{O}(\frac{1}{\epsilon^{2}})\) separation oracle calls._

Our main contribution is a class of algorithms based on Vaidya's cutting-plane method that provide a query-complexity / memory tradeoff for optimization in \(\mathbb{R}^{d}\). More precisely, we show the following, where \(\omega<2.373\) is the exponent of matrix multiplication, such that multiplying two \(n\times n\) matrices runs in \(\mathcal{O}(n^{\omega})\) time.

**Theorem 3.2**.: _For any \(1\leq p\leq d\), there is a deterministic first-order algorithm that solves the feasibility problem in dimension \(d\) for accuracy \(\epsilon\leq\frac{1}{\sqrt{d}}\), using \(\mathcal{O}(\frac{d^{2}}{p}\ln\frac{1}{\epsilon})\) bits of memory (including during computations), with \(\mathcal{O}((C\frac{d}{p}\ln\frac{1}{\epsilon})^{p})\) calls to the separation oracle, and computational complexity \(\mathcal{O}((C(\frac{d}{p})^{1+\omega}\ln\frac{1}{\epsilon})^{p})\), where \(C\geq 1\) is a universal constant._

For simplicity, in Section 4, we describe algorithms that achieve this trade-off without computation concerns (Definition 2.1), which already provide the main elements of our method. The proof of oracle-complexity and memory usage is given in Appendix A. In Appendix B, we consider computational constraints and give corresponding algorithms using the cutting-plane method of [27].

To better understand the implications of Theorem 3.2, it is useful to compare the provided class of algorithms to the two algorithms known in the oracle-complexity/memory tradeoff landscape: the memory-constrained center-of-mass method and the memory-constrained gradient descent [52].

For \(p=1\), our resulting procedure, which is essentially a memory-constrained Vaidya's algorithm, has optimal oracle-complexity \(\mathcal{O}(d\ln\frac{1}{\epsilon})\) and uses \(\mathcal{O}(d^{2}\ln\frac{1}{\epsilon})\) bits of memory. This improves by a \(\ln\frac{1}{\epsilon}\) factor the memory usage of the center-of-mass-based algorithm provided in [52], which used \(\mathcal{O}(d^{2}\ln^{2}\frac{1}{\epsilon})\) memory and had the same optimal oracle-complexity.

Next, we recall that the memory-constrained gradient descent method used the optimal number \(\mathcal{O}(d\ln\frac{1}{\epsilon})\) bits of memory (including for computations), and a sub-optimal \(\mathcal{O}(\frac{1}{\epsilon^{2}})\) oracle-complexity. While the memory of our algorithms decreases with \(p\), their oracle-complexity is exponential in \(p\). This significantly restricts the values of \(p\) for which the oracle-complexity is improved over that of gradient descent. The range of application of Theorem 3.2 is given in the next result, where \(\vee\) and \(\wedge\) represent maximum and minimum respectively.

**Corollary 3.1**.: _The algorithms given in Theorem 3.2 effectively provide a tradeoff for \(p\leq\mathcal{O}(\frac{\ln\frac{1}{\epsilon}}{\ln d}\wedge d)\). Precisely, this provides a tradeoff between_

* _using_ \(\mathcal{O}(d^{2}\ln\frac{1}{\epsilon})\) _memory with optimal_ \(\mathcal{O}(d\ln\frac{1}{\epsilon})\) _oracle-complexity, and_
* _using \(\mathcal{O}(d^{2}\ln d\lor d\ln\frac{1}{\epsilon})\) memory with \(\mathcal{O}(\frac{1}{\epsilon^{2}}\wedge(C\ln\frac{1}{\epsilon})^{d})\) oracle-complexity._

Importantly, for \(\epsilon\leq\frac{1}{d^{\Omega(d)}}\), taking \(p=d\) yields an algorithm that uses the optimal memory \(\mathcal{O}(d\ln\frac{1}{\epsilon})\) and has an improved query complexity over gradient descent. In this regime of small (virtually constant) dimension, for the same memory usage, gradient descent has a query complexity that is polynomial in \(\epsilon\), \(\mathcal{O}(\frac{1}{\epsilon^{2}})\), while our algorithm has poly-logarithmic dependence in \(\epsilon\), \(\mathcal{O}_{d}(\ln^{d}\frac{1}{\epsilon})\), where \(\mathcal{O}_{d}\) hides an exponential constant in \(d\). It remains open whether this \(\ln^{d}\frac{1}{\epsilon}\) dependence in the oracle-complexity is necessary. To the best of our knowledge, this is the first example of an algorithm that improves over gradient descent while keeping its optimal memory usage in any regime where \(\epsilon\leq\frac{1}{\sqrt{d}}\).

While this improvement holds only in the exponential regime \(\epsilon\leq\frac{1}{d^{\mathcal{O}(d)}}\), Theorem 3.2 still provides a non-trivial trade-off whenever \(\ln\frac{1}{\epsilon}\gg\ln d\), and improves over the known memory-constrained center-of-mass in the standard regime \(\epsilon\leq\frac{1}{\sqrt{d}}\)[52]. Fig. 1 depicts the trade-offs in the two regimes mentioned earlier.

Last, we note that the lower-bound trade-offs presented in [31, 5] do not show a dependence in the accuracy \(\epsilon\). Especially in the regime when \(\ln\frac{1}{\epsilon}\gg\ln d\), this yields sub-optimal lower bounds (in fact even in the regime \(\epsilon=1/\text{poly}(d)\), our more careful analysis improves the lower bound on the memory by a \(\ln d\) factor). We show with simple arguments that one can extend their results to include a \(\ln\frac{1}{\epsilon}\) factor for both memory and query complexity. Fig. 1 presented these improved lower bounds.

Figure 1: Trade-offs between available memory and first-order oracle-complexity for the feasibility problem over the unit ball. MC=Memory-constrained. GD=Gradient Descent. The left picture corresponds to the regime \(\epsilon\gg d^{-\Omega(d)}\) and \(\epsilon\leq 1/\text{poly}(d)\) and the right picture represents the regime \(\epsilon\leq d^{-\mathcal{O}(d)}\). For both figures, the dashed pink “L” (resp. green inverted “L”) region corresponds to historical lower (resp. upper) bounds for randomized algorithms. The solid pink (resp. red) lower bound tradeoff is due to [31] (resp. [5]) for randomized algorithms (resp. deterministic algorithms). The purple region is a lower bound tradeoff for the feasibility problem for accuracy \(\epsilon\) and deterministic algorithms [5]. All these lower-bound trade-offs are represented with their \(\ln\frac{1}{\epsilon}\) dependence (Theorem 3.3). We use memory-constrained Vaidya’s method to gain a factor \(\ln\frac{1}{\epsilon}\) in memory compared to memory-constrained center-of-mass [52], which gives the light green region, and a class of algorithms represented in dark green, that allows trading query-complexity for an extra \(\ln\frac{1}{\epsilon}/\ln d\) factor saved in memory (Theorem 3.2). The dark green dashed region in the left figure emphasizes that the area covered by our class of algorithms depends highly on the regime for the accuracy \(\epsilon\): the resulting improvement in memory is more significant as \(\epsilon\) is smaller. In the regime when \(\epsilon\leq d^{-\mathcal{O}(d)}\) (right figure), our class of algorithms improves over the oracle-complexity of gradient descent while keeping the optimal memory \(\mathcal{O}(d\ln\frac{1}{\epsilon})\).

**Theorem 3.3**.: _For \(\epsilon\leq 1/poly(d)\) and any \(\delta\in[0,1]\) (the notation \(\tilde{\Omega}\) hides \(\ln^{\mathcal{O}(1)}d\) factors),_

1. _any (randomized) algorithm guaranteed to minimize_ \(1\)_-Lipschitz convex functions over the unit ball with accuracy_ \(\epsilon\) _uses_ \(d^{5/4-\delta}\ln\frac{1}{\epsilon}\) _bits of memory or makes_ \(\tilde{\Omega}(d^{1+4\delta/3}\ln\frac{1}{\epsilon})\) _queries,_
2. _any deterministic algorithm guaranteed to minimize_ \(1\)_-Lipschitz convex functions over the unit ball with accuracy_ \(\epsilon\) _uses_ \(d^{2-\delta}\ln\frac{1}{\epsilon}\) _bits of memory or makes_ \(\tilde{\Omega}(d^{1+\delta/3}\ln\frac{1}{\epsilon})\) _queries,_
3. _any deterministic algorithm guaranteed to solve the feasibility problem over the unit ball with accuracy_ \(\epsilon\) _uses_ \(d^{2-\delta}\ln\frac{1}{\epsilon}\) _bits of memory or makes_ \(\tilde{\Omega}(d^{1+\delta}\ln\frac{1}{\epsilon})\) _queries._

The proof is given in Appendix C and the arguments therein could readily be used to exhibit the \(\ln\frac{1}{\epsilon}\) dependence of potential future works improving over these lower bounds trade-offs.

Sketch of proof.At a high level, [31; 5] use a barrier term \(\|\bm{Ax}\|_{\infty}\) where \(\bm{A}\) has \(\Theta(d)\) rows: if an algorithm does not have enough memory, \(\bm{A}\) cannot be fully stored which in turn incurs a sub-optimal oracle-complexity. To achieve a \(\ln\frac{1}{\epsilon}\) improvement in memory (Appendix C.1), we modify the sampling of rows of \(\bm{A}\), from uniform on vertices of the hypercube to uniform in an \(\epsilon\)-net. The proof can then be adapted accordingly. Last, one can improve the oracle-complexity by a \(\ln\frac{1}{\epsilon}/\ln d\) factor (Appendix C.2) using a standard rescaling argument [34].

## 4 Memory-constrained feasibility problem without computation

In this section, we present a class of algorithms that are memory-constrained according to Definition 2.1 and achieve the desired memory and oracle-complexity bounds. We emphasize that the memory constraint is only applied between calls to the oracle and as a result, the algorithm is allowed infinite computation memory and computation power between calls to the oracle.

We start by defining discretization functions that will be used in our algorithms. For \(\xi>0\) and \(x\in[-1,1]\), we pose \(\text{Discretize}_{1}(x,\xi)=sign(x)\cdot\xi\lfloor\lfloor x\rfloor/\xi\rfloor\). Next, we define the discretization \(\text{Discretize}_{d}\) for general dimensions \(d\geq 1\). For any \(\bm{x}\in C\) and \(\xi>0\),

\[\text{Discretize}_{d}(\bm{x},\xi)=\left(\text{Discretize}_{1}\left(x_{1}, \xi/\sqrt{d}\right),\ldots,\text{Discretize}_{1}\left(x_{d},\xi/\sqrt{d} \right)\right).\]

### Memory-constrained Vaidya's method

Our algorithm recursively uses Vaidya's cutting-plane method [48] and subsequent works expanding on this method. We briefly describe the method. Given a polyhedron \(\mathcal{P}=\{\bm{x}:\bm{Ax}\geq\bm{b}\}\), we define \(s_{i}(\bm{x})=\bm{a}_{i}^{\top}\bm{x}-b_{i}\) and \(\bm{S}_{x}=diag(s_{i}(x),i\in[d])\). We will also use the shorthand \(\bm{A}_{x}=\bm{S}_{x}^{-1}\bm{A}\). The volumetric barrier is defined as

\[V_{\bm{A},\bm{b}}(\bm{x})=\frac{1}{2}\ln\det(\bm{A}_{x}^{\top}\bm{A}_{x}).\]

At each step, Vaidya's method queries the volumetric center of the polyhedron, which is the point minimizing the volumetric barrier. For convenience, we denote by \(\mathsf{VolumetricCenter}\) this function, i.e., for any \(\bm{A}\in\mathbb{R}^{m\times d}\) and \(\bm{b}\in\mathbb{R}^{d}\) defining a non-empty polyhedron \(\mathcal{P}=\{\bm{x}:\bm{Ax}\geq\bm{b}\}\),

\[\mathsf{VolumetricCenter}(\bm{A},\bm{b})=\arg\min_{\bm{x}:\bm{Ax}>\bm{b}}V_{ \bm{A},\bm{b}}(\bm{x}).\]

When the polyhedron is unbounded, we can for instance take \(\mathsf{VolumetricCenter}(\bm{A},\bm{b})=\bm{0}\). Vaidya's method makes use of leverage scores for each constraint \(i\) of the polyhedron, defined as \(\sigma_{i}=(\bm{A}_{x}\bm{H}^{-1}\bm{A}_{x}^{\top})_{i,i}\), where \(\bm{H}=\bm{A}_{x}^{\top}\bm{A}_{x}\). We are now ready to define the update procedure for the polyhedron considered by Vaidya's volumetric method. We denote by \(\mathcal{P}_{t}\) the polyhedron stored in memory after making \(t\) queries. The method keeps in memory the constraints defining the current polyhedron and the iteration index \(k\) when these constraints were added, which will be necessary for our next procedures. Hence, the polyhedron will be stored in the form \(\mathcal{P}_{t}=\{(k_{i},\bm{a}_{i},b_{i}),i\in[m]\}\), and the associated constraints are given via \(\{\bm{x}:\bm{Ax}\geq\bm{b}\}\) where \(\bm{A}^{\top}=[\bm{a}_{1},\ldots,\bm{a}_{m}]\) and \(\bm{b}^{\top}=[b_{1},\ldots,b_{m}]\). By abuse of notation, we will write \(\mathsf{VolumetricCenter}(\mathcal{P})\) for the volumetric center of the polyhedron \(\mathsf{VolumetricCenter}(\bm{A},\bm{b})\) where \(\bm{A}\) and \(\bm{b}\) define the constraints stored in \(\mathcal{P}\).

Initially, the polyhedron is simply \(\mathcal{C}_{d}\), these constraints are given \(-1\) index for convenience, and they will not play a role in the next steps. At each iteration, if the constraint \(i\in[m]\) with minimum leverage score \(\sigma_{i}\) falls below a given threshold \(\sigma_{min}\), it is removed from the polyhedron. Otherwise, we query the volumetric center of the current polyhedron and add the separation hyperplane as a constraint to the polyhedron. We bound the number of iterations of the procedure by

\[T(\delta,d)=\left\lceil c\cdot d\left(1.4\ln\frac{1}{\delta}+2\ln d+2\ln(1+1/ \sigma_{min})\right)\right\rceil,\]

where \(\sigma_{min}\) and \(c\) are parameters that will be fixed shortly. Instead of making a call directly to the oracle \(O_{S}\), we instead suppose that one has access to an oracle \(O:\mathcal{I}_{d}\rightarrow\mathbb{R}^{d}\) where \(\mathcal{I}_{d}=(\mathbb{Z}\times\mathbb{R}^{d+1})^{\star}\) has exactly the shape of the memory storing the information from the polyhedron. This form of oracle is used in our recursive calls to Vaidya's method. For example, such an oracle can simply be \(O:\mathcal{P}\in\mathcal{I}_{d}\mapsto O_{S}(\mathsf{VolumetricCenter}( \mathcal{P}))\). Last, in our recursive method, we will not assume that oracle responses are normalized. As a result, we specify that if the norm of the response is too small, we can stop the algorithm. We assume however that the oracle already returns discretized vectors, which will be ensured in the following procedures. The cutting-plane algorithm is formally described in Algorithm 1. With an appropriate choice of parameters, this procedure finds an approximate solution of feasibility problems. We base the constants from [2].

``` Input:\(O:\mathcal{I}_{d}\rightarrow\mathbb{R}^{d}\), \(\delta\), \(\xi\in(0,1)\)
1 Let \(T_{max}=T(\delta,d)\) and initialize \(\mathcal{P}_{0}:=\{(-1,\bm{e}_{i},-1),(-1,-\bm{e}_{i},-1),\ i\in[d]\}\)
2for\(t=0,\ldots,T_{max}\)do
3if\(\{\bm{x}:\bm{A}\bm{x}\geq\bm{b}\}=\emptyset\)thenreturn\(\mathcal{P}_{t}\);
4if\(\min_{i\in[m]}\sigma_{i}<\sigma_{min}\)then
5\(\mathcal{P}_{t+1}=\mathcal{P}_{t}\setminus\{(k_{j},\bm{a}_{j},b_{j})\}\) where \(j\in\arg\min_{i\in[m]}\sigma_{i}\)
6elseif\(\bm{\omega}:=\mathsf{VolumetricCenter}(\mathcal{P}_{t})\notin\mathcal{C}_{d}\)then
7\(\mathcal{P}_{t+1}=\mathcal{P}_{t}\cup\{(-1,-sign(\omega_{j})\bm{e}_{j},-1)\}\) where \(j\in[d]\) has \(|\omega_{j}|>1\)
8else
9\(\bm{g}=O(\mathcal{P}_{t})\) and \(b=\xi\left\lceil\frac{\bm{g}^{\top}\bm{\omega}}{\xi}\right\rceil\), where \(\bm{\omega}=\mathsf{VolumetricCenter}(\mathcal{P}_{t})\)
10\(\mathcal{P}_{t+1}=\mathcal{P}_{t}\cup\{(t,\bm{g},b)\}\)
11if\(\|\bm{g}\|\leq\delta\)thenreturn\(\mathcal{P}_{t+1}\) ;
12
13 end if return\(\mathcal{P}_{T_{max}+1}\) ```

**Algorithm 1**Memory-constrained Vaidya's volumetric method

**Lemma 4.1**.: _Fix \(\sigma_{min}=0.04\) and \(c=\frac{1}{0.0014}\approx 715\). Let \(\delta,\xi\in(0,1)\) and \(O:\mathcal{I}_{d}\rightarrow\mathbb{R}^{d}\). Write \(\mathcal{P}=\{(k_{i},\bm{a}_{i},b_{i}),i\in[m]\}\) as the output of Algorithm 1 run with \(O\), \(\delta\) and \(\xi\). Then,_

\[\min_{\begin{subarray}{c}\lambda_{i}\geq 0,\ i\in[m],\\ \sum_{i\in[m]}\lambda_{i}=1\end{subarray}}\max_{\bm{y}\in\mathcal{C}_{d}}\sum _{i=1}^{m}\lambda_{i}(\bm{a}_{i}^{\top}\bm{y}-b_{i})=\max_{\bm{x}\in\mathcal{C} _{d}}\min_{i\in[m]}\big{(}\bm{a}_{i}^{\top}\bm{x}-b_{i})\leq\delta.\]

From now, we use the parameters \(\sigma_{min}=0.04\) and \(c=1/0.0014\) as in Lemma 4.1. Since the memory of both Vaidya's method and center-of-mass consists primarily of the constraints, we recall an important feature of Vaidya's method that the number of constraints at any time is \(\mathcal{O}(d)\).

**Lemma 4.2** ([48; 1, 2]).: _At any time while running Algorithm 1, the number of constraints of the current polyhedron is at most \(\frac{d}{\sigma_{min}}+1\)._

### A recursive algorithm

We write \(\mathcal{C}_{m+n}=\mathcal{C}_{m}\times\mathcal{C}_{n}\) and aim to apply Vaidya's method to the first \(m\) coordinates. To do so, we need to approximate a separation oracle on these \(m\) coordinates only, which corresponds to giving separation hyperplanes with small values for the last \(n\) coordinates. This can be achieved using the following auxiliary linear program. For \(\mathcal{P}\in\mathcal{I}_{n}\), we define

\[\min_{\begin{subarray}{c}\lambda_{i}\geq 0,\ i\in[m],\\ \sum_{i\in[m]}\lambda_{i}=1\end{subarray}}\max_{\bm{y}\in\mathcal{C}_{n}}\sum _{i=1}^{m}\lambda_{i}(\bm{a}_{i}^{\top}\bm{y}-b_{i}),\quad m=|\mathcal{P}| (\mathcal{P}_{aux}(\mathcal{P}))\]where as before, \(\bm{A}\) and \(\bm{b}\) define the constraints stored in \(\mathcal{P}\). The procedure to obtain an approximate separation oracle on the first \(n\) coordinates \(\mathcal{C}_{n}\) is given in Algorithm 2 and using Lemma 4.1 we can show that this procedure provides approximate separation vectors for the first \(n\) coordinates.

``` Input:\(\delta\), \(\xi\), \(O_{x}:\mathcal{I}_{n}\to\mathbb{R}^{m}\) and \(O_{y}:\mathcal{I}_{n}\to\mathbb{R}^{n}\)
1 Run Algorithm 1 with \(\delta,\xi\) and \(O_{y}\) to obtain polyhedron \(\mathcal{P}^{\star}\)
2 Solve \(\mathcal{P}_{aux}(\mathcal{P}^{\star})\) to get a solution \(\bm{\lambda}^{\star}\)
3 Store \(\bm{k}^{\star}=(k_{i},i\in[m])\) where \(m=|\mathcal{P}^{\star}|\), and \(\bm{\lambda}^{\star}\leftarrow\mathsf{Discretize}(\bm{\lambda}^{\star},\xi)\)
4 Initialize \(\mathcal{P}_{0}:=\{(-1,\bm{e}_{i},-1),(-1-\bm{e}_{i},-1),\;i\in[d]\}\) and \(\bm{u}=\bm{0}\in\mathbb{R}^{m}\)
5for\(t=0,1,\ldots,n_{\max}k_{i}\)do
6if\(t=k_{i}^{\star}\)for some \(i\in[m]\)then
7\(\bm{g}_{x}=O_{x}(\mathcal{P}_{t})\)
8\(\bm{u}\leftarrow\mathsf{Discretize}_{m}(\bm{u}+\lambda_{i}^{\star}\bm{g}_{x},\xi)\)
9 Update \(\mathcal{P}_{t}\) to get \(\mathcal{P}_{t+1}\) as in Algorithm 1
10
11 end if return\(\bm{u}\) ```

**Algorithm 2**\(\mathsf{ApproxSeparationVector}_{\delta,\xi}(O_{x},O_{y})\)

The next step involves using this approximation recursively. We write \(d=\sum_{i=1}^{p}k_{i}\), and interpret \(\mathcal{C}_{d}\) as \(\mathcal{C}_{k_{1}}\times\cdots\times\mathcal{C}_{k_{p}}\). In particular, for \(\bm{x}\in\mathcal{C}_{d}\), we write \(\bm{x}=(\bm{x}_{1},\ldots,\bm{x}_{p})\) where \(\bm{x}_{i}\in\mathcal{C}_{k_{i}}\) for \(i\in[p]\). Applying Algorithm 2 recursively, we can obtain an approximate separation oracle for the first \(i\) coordinates \(\mathcal{C}_{k_{1}}\times\cdots\times\mathcal{C}_{k_{i}}\). However, storing such separation vectors would be too memory-expensive, e.g., for \(i=p\), that would correspond to storing the separation hyperplanes from the oracle \(O_{S}\) directly. Instead, given \(j\in[i]\), Algorithm 3 recursively computes the \(\bm{x}_{j}\) component of an approximate separation oracle for the first \(i\) variables \((\bm{x}_{1},\ldots,\bm{x}_{i})\), via the procedure \(\mathsf{ApproxOracle}(i,j)\).

``` Input:\(\delta\), \(\xi\), \(1\leq j\leq i\leq p\), \(\mathcal{P}^{(r)}\in\mathcal{I}_{k_{r}}\) for \(r\in[i]\), \(O_{S}:\mathcal{C}_{d}\to\mathbb{R}^{d}\)
1if\(i=p\)then
2\(\bm{x}_{r}=\mathsf{VolumetricCenter}(\bm{A}_{r},\bm{b}_{r})\) where \((\bm{A}_{r},\bm{b}_{r})\) defines the constraints stored in \(\mathcal{P}^{(r)}\) for \(r\in[p]\)
3\((\bm{g}_{1},\ldots,\bm{g}_{p})=O_{S}(\bm{x}_{1},\ldots,\bm{x}_{p})\)
4return\(\mathsf{Discretize}_{k_{j}}(\bm{g}_{j},\xi)\)
5 end if
6 Define \(O_{x}:\mathcal{I}_{k_{i+1}}\to\mathbb{R}^{k_{j}}\) as \(\mathsf{ApproxOracle}_{\delta,\xi,\mathcal{O}_{f}}(i+1,j,\mathcal{P}^{(1)},, \ldots,\mathcal{P}^{(i)},\cdot)\)
7 Define \(O_{y}:\mathcal{I}_{k_{i+1}}\to\mathbb{R}^{k_{i+1}}\) as \(\mathsf{ApproxOracle}_{\delta,\xi,\mathcal{O}_{f}}(i+1,i+1,\mathcal{P}^{(1)}, \ldots,\mathcal{P}^{(i)},\cdot)\)
8return\(\mathsf{ApproxSeparationVector}_{\delta,\xi,O_{S}}(O_{x},O_{y})\) ```

**Algorithm 3**\(\mathsf{ApproxOracle}_{\delta,\xi,O_{S}}(i,j,\mathcal{P}^{(1)},\ldots,\mathcal{P}^{( i)})\)

We can then use \(\mathsf{ApproxOracle}_{\delta,\xi,O_{S}}(1,1,\cdot)\) to solve the original problem with the memory-constrained Vaidya's method. In Appendix A, we show that taking \(\delta=\frac{\epsilon}{4d}\) and \(\xi=\frac{\sigma_{min}\epsilon}{32d^{3/2}}\) achieves the desired oracle-complexity and memory usage. The final algorithm is given in Algorithm 4.

``` Input:\(\delta\), \(\xi\), and \(\mathcal{O}_{S}:\mathcal{C}_{d}\to\mathbb{R}^{d}\) a separation oracle
1 Check:Throughout the algorithm, if \(O_{S}\) returned \(\mathsf{Success}\) to a query \(\bm{x}\), return\(\bm{x}\)
2 Run Algorithm 1 with parameters \(\delta\) and \(\xi\) and oracle \(\mathsf{ApproxOracle}_{\delta,\xi,O_{S}}(1,1,\cdot)\) ```

**Algorithm 4**Memory-constrained algorithm for convex optimization

A geometric illustration of the recursive step.In Figure 2, we give a 2-dimensional feasibility problem with target \(\bm{p}^{\star}=(p_{1}^{\star},p_{2}^{\star})\) and two blocks (i.e. \(p=2\)) as an illustration of our recursive approach (Algorithm 2) to construct an approximate separating hyperplane for a "reduced" problem.

Suppose at a step of the Algorithm 4, the current value of the \(x_{1}\) coordinate is \(c\). We aim to find an approximate separating hyperplane between \(x_{1}=p_{1}^{\star}\) and \(x_{1}=c\). Algorithm 2 first runs Algorithm 1 (i.e. the memory-constrained Vaidya) to find two separating hyperplanes (the two blue hyperplanes). Lemma 4.1 then guarantees the existence of a convex combination of the 2 blue hyperplanes - the red hyperplane- which is approximately parallel to the \(x_{2}\)-axis and thus can serve as an approximate separating hyperplane between \(x_{1}=p_{1}^{*}\) and \(x_{1}=c\).

Sketch of proof.At the high level, the algorithm recursively runs Vaidya's method Algorithm 1 for each level of computation \(i\in[p]\). Since each run of Algorithm 4 requires \(\mathcal{O}(\frac{d}{p}\ln\frac{1}{\epsilon})\) queries, the total number of calls to the oracle, which is exponential in the number of levels, is \(\mathcal{O}(\mathcal{O}(\frac{d}{p}\ln\frac{1}{\epsilon})^{p})\). As for the memory usage, the algorithm mainly needs to keep in memory the constraints defining the polyhedrons at each level \(i\in[p]\). From Lemma 4.2, each polyhedron only requires \(\mathcal{O}(\frac{d}{p})\) constraints that each require \(\mathcal{O}(\frac{d}{p}\ln\frac{1}{\epsilon})\) bits of memory. Hence, the total memory needed is \(\mathcal{O}(\frac{d^{2}}{p}\ln\frac{1}{\epsilon})\). The main difficulty lies in showing that the algorithm is successful. To do so, we need to show that the precision in the successive approximated separation oracles Algorithm 2 is sufficient. To avoid an exponential dependence of the approximation error in \(p\)--which would be prohibitive for the memory usage of our method--each run of Vaidya's method Algorithm 1 is run for more iterations than the precision of the separation vectors would classically allow. To give intuition, if the separation oracle came from a convex optimization subgradient oracle for a function \(f\), the iterates at a level \(i\) do not converge to the true "minimizer" of \(\min_{\bm{x}_{i}}f^{(i)}(\bm{x}_{1},\ldots,\bm{x}_{i})\), where \(f^{(i)}(\cdot)=\min_{\bm{x}_{i+1},\ldots,\bm{x}_{p}}f(\cdot,\bm{x}_{i+1}, \ldots,\bm{x}_{p})\), but instead converge to a close enough point while still providing meaningful approximate subgradients at the higher level \(i-1\) (in Algorithm 2).

## 5 Discussion and Conclusion

To the best of our knowledge, this work is the first to provide some positive trade-off between oracle-complexity and memory-usage for convex optimization or the feasibility problem, as opposed to lower-bound impossibility results [31; 5]. Our trade-offs are more significant in a high accuracy regime: when \(\ln\frac{1}{\epsilon}\approx d^{\epsilon}\), for \(c>0\) our trade-offs are polynomial, while the improvements when \(\ln\frac{1}{\epsilon}=\text{poly}(\ln d)\) are only in \(\ln d\) factors. A natural open direction [52] is whether there exist algorithms with polynomial trade-offs in that case. We also show that in the exponential regime \(\ln\frac{1}{\epsilon}\geq\Omega(d\ln d)\), gradient descent is not Pareto-optimal. Instead, one can keep the optimal memory and decrease the dependence in \(\epsilon\) of the oracle-complexity from \(\frac{1}{\epsilon^{2}}\) to \((\ln\frac{1}{\epsilon})^{d}\). The question of whether the exponential dependence in \(d\) is necessary is left open. Last, our algorithms rely on the consistency of the oracle, which allows re-computations. While this is a classical assumption, gradient descent and classical cutting-plane methods do not need it; removing this assumption could be an interesting research direction (potentially, this could also yield stronger lower bounds).

Figure 2: Intuition for the recursive procedure in Algorithm 4. Using the separation hyperplanes (blue) found by Algorithm 1, i.e., the memory-constrained Vaidya, it constructs an approximate separation hyperplane (red) between \(x_{1}=c\) and the target \(x_{1}=p_{1}^{*}\).

## Acknowledgments and Disclosure of Funding

This work was partly funded ONR grant N00014-18-1-2122, AFOSR grant FA9550-19-1-0263, and AFOSR grant FA9550-23-1-0182.

## References

* [1] Kurt M Anstreicher. "On Vaidya's volumetric cutting plane method for convex programming". In: _Mathematics of Operations Research_ 22.1 (1997), pp. 63-89.
* [2] Kurt M Anstreicher. "Towards a practical volumetric cutting plane method for convex programming". In: _SIAM Journal on Optimization_ 9.1 (1998), pp. 190-206.
* [3] Aharon Ben-Tal and Arkadi Nemirovski. "Non-euclidean restricted memory level method for large-scale convex optimization". In: _Mathematical Programming_ 102.3 (Jan. 2005), pp. 407-456.
* [4] Dimitris Bertsimas and Santosh Vempala. "Solving convex programs by random walks". In: _Journal of the ACM (JACM)_ 51.4 (2004), pp. 540-556.
* [5] Moise Blanchard, Junhui Zhang, and Patrick Jaillet. "Quadratic Memory is Necessary for Optimal Query Complexity in Convex Optimization: Center-of-Mass is Pareto-Optimal". In: _arXiv preprint arXiv:2302.04963_ (2023).
* [6] Mark Braverman et al. "Communication Lower Bounds for Statistical Estimation Problems via a Distributed Data Processing Inequality". In: _Proceedings of the Forty-Eighth Annual ACM Symposium on Theory of Computing_. STOC '16. Cambridge, MA, USA: Association for Computing Machinery, 2016, pp. 1011-1020.
* [7] C. G. Broyden. "The Convergence of a Class of Double-rank Minimization Algorithms 1. General Considerations". In: _IMA Journal of Applied Mathematics_ 6.1 (Mar. 1970), pp. 76-90.
* [8] Sebastien Bubeck. "Convex Optimization: Algorithms and Complexity". In: _Foundations and Trends(r) in Machine Learning_ 8.3-4 (2015), pp. 231-357.
* [9] Sebastien Bubeck, Yin Tat Lee, and Mohit Singh. _A geometric alternative to Nesterov's accelerated gradient descent_. 2015.
* [10] Sebastien Bubeck and Dan Mikulincer. "How to trap a gradient flow". In: _Conference on Learning Theory_. PMLR. 2020, pp. 940-960.
* [11] Xi Chen and Binghui Peng. "Memory-Query Tradeoffs for Randomized Convex Optimization". In: _arXiv preprint arXiv:2306.12534_ (2023).
* [12] Alexandre d'Aspremont, Damien Scheiur, and Adrien Taylor. "Acceleration Methods". In: _Foundations and Trends(r) in Optimization_ 5.1-2 (2021), pp. 1-245.
* [13] Yuval Dagan, Gil Kur, and Ohad Shamir. "Space lower bounds for linear prediction in the streaming model". In: _Proceedings of the Thirty-Second Conference on Learning Theory_. Ed. by Alina Beygelzimer and Daniel Hsu. Vol. 99. Proceedings of Machine Learning Research. PMLR, 25-28 Jun 2019, pp. 929-954.
* [14] Yuval Dagan and Ohad Shamir. "Detecting Correlations with Little Memory and Communication". In: _Proceedings of the 31st Conference On Learning Theory_. Ed. by Sebastien Bubeck, Vianney Perchet, and Philippe Rigollet. Vol. 75. Proceedings of Machine Learning Research. PMLR, June 2018, pp. 1145-1198.
* [15] James Demmel, Ioana Dumitriu, and Olga Holtz. "Fast linear algebra is stable". In: _Numerische Mathematik_ 108.1 (2007), pp. 59-91.
* [16] Kim Donghwan and Jeffrey Fessler. "Optimized first-order methods for smooth convex minimization". In: _Mathematical Programming_ 159 (June 2014).
* [17] Yoel Drori and Marc Teboulle. "Performance of first-order methods for smooth convex minimization: a novel approach". In: _Mathematical Programming_ 145.1 (June 2014), pp. 451-482.
* [18] Dmitriy Drusvyatskiy, Maryam Fazel, and Scott Roy. "An Optimal First Order Method Based on Optimal Quadratic Averaging". In: _SIAM Journal on Optimization_ 28.1 (2018), pp. 251-271.
* [19] Uriel Feige and Gideon Schechtman. "On the optimality of the random hyperplane rounding technique for MAX CUT". In: _Random Structures & Algorithms_ 20.3 (2002), pp. 403-440.

* [20] R. Fletcher. "A new approach to variable metric algorithms". In: _The Computer Journal_ 13.3 (Jan. 1970), pp. 317-322.
* [21] Donald Goldfarb. "A Family of Variable-Metric Methods Derived by Variational Means". In: _Mathematics of Computation_ 24.109 (1970), pp. 23-26.
* [22] Haotian Jiang et al. "An Improved Cutting Plane Method for Convex Optimization, Convex-Concave Games, and Its Applications". In: _Proceedings of the 52nd Annual ACM SIGACT Symposium on Theory of Computing_. STOC 2020. Chicago, IL, USA: Association for Computing Machinery, 2020, pp. 944-953.
* [23] Jonathan Kelner et al. "Big-Step-Little-Step: Efficient Gradient Methods for Objectives with Multiple Scales". In: _Proceedings of Thirty Fifth Conference on Learning Theory_. Ed. by Po-Ling Loh and Maxim Raginsky. Vol. 178. Proceedings of Machine Learning Research. PMLR, Feb. 2022, pp. 2431-2540.
* [24] Guanghui Lan. "Bundle-level type methods uniformly optimal for smooth and nonsmooth convex optimization". In: _Mathematical Programming_ 149.1 (Feb. 2015), pp. 1-45.
* [25] Guanghui Lan, Soomin Lee, and Yi Zhou. "Communication-efficient algorithms for decentralized and stochastic optimization". In: _Mathematical Programming_ 180.1 (Mar. 2020), pp. 237-284.
* [26] Quoc Le, Alex Smola, and S.V.N. Vishwanathan. "Bundle Methods for Machine Learning". In: _Advances in Neural Information Processing Systems_. Ed. by J. Platt et al. Vol. 20. Curran Associates, Inc., 2007.
* [27] Yin Tat Lee, Aaron Sidford, and Sam Chiu-wai Wong. "A faster cutting plane method and its implications for combinatorial and convex optimization". In: _2015 IEEE 56th Annual Symposium on Foundations of Computer Science_. IEEE. 2015, pp. 1049-1065.
* [28] Claude Lemarechal, Arkadi Nemirovski, and Yurii Nesterov. "New variants of bundle methods". In: _Mathematical Programming_ 69.1 (July 1995), pp. 111-147.
* [29] Adrian S. Lewis and Michael L. Overton. "Nonsmooth optimization via quasi-Newton methods". In: _Mathematical Programming_ 141.1 (Oct. 2013), pp. 135-163.
* [30] Dong C. Liu and Jorge Nocedal. "On the limited memory BFGS method for large scale optimization". In: _Mathematical Programming_ 45.1 (Aug. 1989), pp. 503-528.
* [31] Annie Marsden et al. "Efficient convex optimization requires superlinear memory". In: _Conference on Learning Theory_. PMLR. 2022, pp. 2390-2430.
* [32] Dana Moshkovitz and Michal Moshkovitz. "Mixing Implies Lower Bounds for Space Bounded Learning". In: _Proceedings of the 2017 Conference on Learning Theory_. PMLR, 2017, pp. 1516-1566.
* [33] Joao F. C. Mota et al. "D-ADMM: A Communication-Efficient Distributed Algorithm for Separable Optimization". In: _IEEE Transactions on Signal Processing_ 61.10 (2013), pp. 2718-2723.
* [34] Arkadi Nemirovski and David Borisovich Yudin. _Problem Complexity and Method Efficiency in Optimization_. A Wiley-Interscience publication. Wiley, 1983.
* [35] Yurii Nesterov. "A method of solving a convex programming problem with convergence rate \(O(1/k^{2})\)". In: _Dokl. Akad. Nauk SSSR_ 269 (3 1983), pp. 543-547.
* [36] Yurii Nesterov. _Introductory lectures on convex optimization: A basic course_. Vol. 87. Springer Science & Business Media, 2003.
* [37] Jorge Nocedal. "Updating Quasi-Newton Matrices with Limited Storage". In: _Mathematics of Computation_ 35.151 (1980), pp. 773-782.
* [38] Srinivasan Ramaswamy and John E Mitchell. _A long step cutting plane algorithm that uses the volumetric barrier_. Tech. rep. Citeseer, 1995.
* [39] Ran Raz. "A Time-Space Lower Bound for a Large Class of Learning Problems". In: _2017 IEEE 58th Annual Symposium on Foundations of Computer Science (FOCS)_. 2017, pp. 732-742.
* [40] Sashank J. Reddi et al. "AIDE: Fast and Communication Efficient Distributed Optimization". In: _ArXiv_ abs/1608.06879 (2016).
* [41] B. Van Scoy, R. A. Freeman, and K. M. Lynch. "The Fastest Known Globally Convergent First-Order Method for Minimizing Strongly Convex Functions". In: _IEEE Control Systems Letters_ PP.99 (2017), pp. 1-1.

* [42] Ohad Shamir, Nati Srebro, and Tong Zhang. "Communication-Efficient Distributed Optimization using an Approximate Newton-type Method". In: _Proceedings of the 31st International Conference on Machine Learning_. Ed. by Eric P. Xing and Tony Jebara. Vol. 32. Proceedings of Machine Learning Research 2. Bejing, China: PMLR, 22-24 Jun 2014, pp. 1000-1008.
* [43] D. F. Shanno. "Conditioning of Quasi-Newton Methods for Function Minimization". In: _Mathematics of Computation 24.111_ (1970), pp. 647-656.
* [44] Vatsal Sharan, Aaron Sidford, and Gregory Valiant. "Memory-Sample Tradeoffs for Linear Regression with Small Error". In: _Proceedings of the 51st Annual ACM SIGACT Symposium on Theory of Computing_. STOC 2019. Association for Computing Machinery, 2019, pp. 890-901.
* [45] Virginia Smith et al. "CoCoA: A General Framework for Communication-Efficient Distributed Optimization". In: _J. Mach. Learn. Res._ 18.1 (Jan. 2017), pp. 8590-8638.
* [46] Adrien Taylor and Yoel Drori. "An optimal gradient method for smooth strongly convex minimization". In: _Mathematical Programming_ 199.1 (May 2023), pp. 557-594.
* [47] Choon Hui Teo et al. "Bundle Methods for Regularized Risk Minimization". In: _Journal of Machine Learning Research_ 11.10 (2010), pp. 311-365.
* [48] Pravin M Vaidya. "A new algorithm for minimizing convex functions over convex sets". In: _Mathematical programming_ 73.3 (1996), pp. 291-341.
* [49] Stephen A Vavasis. "Black-box complexity of local minimization". In: _SIAM Journal on Optimization_ 3.1 (1993), pp. 60-80.
* [50] Jialei Wang, Weiran Wang, and Nathan Srebro. "Memory and Communication Efficient Distributed Stochastic Optimization with Minibatch Prox". In: _Proceedings of the 2017 Conference on Learning Theory_. Ed. by Satyen Kale and Ohad Shamir. Vol. 65. Proceedings of Machine Learning Research. PMLR, July 2017, pp. 1882-1919.
* [51] Jianqiao Wangni et al. "Gradient Sparsification for Communication-Efficient Distributed Optimization". In: _Proceedings of the 32nd International Conference on Neural Information Processing Systems_. NIPS'18. Montreal, Canada: Curran Associates Inc., 2018, pp. 1306-1316.
* [52] Blake Woodworth and Nathan Srebro. "Open problem: The oracle complexity of convex optimization with limited memory". In: _Conference on Learning Theory_. PMLR. 2019, pp. 3202-3210.
* [53] Yuchen Zhang, John C. Duchi, and Martin J. Wainwright. "Communication-efficient algorithms for statistical optimization". In: _2012 IEEE 51st IEEE Conference on Decision and Control (CDC)_. 2012, pp. 6792-6792.

Proof of the query complexity and memory usage of Algorithm 4

First, we give simple properties on the discretization functions. One can easily check that for any \(\bm{x}\in C\),

\[\|\bm{x}-\mathsf{Discretize}_{d}(\bm{x},\xi)\|\leq\xi\qquad\text{and}\qquad\| \mathsf{Discretize}_{d}(\bm{x},\xi)\|\leq\|\bm{x}\|.\] (1)

Further, one can easily check that to represent any output of \(\mathsf{Discretize}_{d}(\cdot,\xi)\), one needs at most \(d\ln\frac{2\sqrt{d}}{\xi}=\mathcal{O}(d\ln\frac{d}{\xi})\) bits.

We next prove Lemma 4.1.

Proof of Lemma 4.1.: We first consider the case when the algorithm terminates because of a query \(\bm{g}=O(\mathcal{P}_{t})\) such that \(\|\bm{g}\|\leq\delta/(2\sqrt{d})\). Then, for any \(\bm{x}\in\mathcal{C}_{d}\), one directly has

\[\bm{g}^{\top}\bm{x}-b\leq\bm{g}^{\top}(\bm{x}-\bm{\omega})\leq 2\sqrt{d}\|\bm{ g}\|\leq\delta.\]

where \(\bm{\omega}\) is the volumetric center of the resulting polyhedron. In the second inequality we used the fact that \(\bm{\omega}\in\mathcal{C}_{d}\), otherwise the algorithm would not have terminated at that step.

We next turn to the other cases and start by showing that the output polyhedron does not contain a ball of radius \(\delta\). This is immediate if the algorithm terminated because the polyhedron was empty. We then suppose this was not the case, and follow the same proof as given in [2]. Algorithm 1 and the one provided in [2] coincide when removing a constraint of the polyhedron. Hence, it suffices to consider the case when we add a constraint. We use the notation \(\tilde{\bm{A}}^{\top}=[\bm{A}^{\top},\bm{a}^{\top}_{m+1}]\), \(\tilde{\bm{b}}^{\top}=[\bm{b}^{\top},b_{m+1}]\) for the updated matrix \(\bm{A}\) and vector \(\bm{b}\) after adding the constraint. We also denote \(\bm{\omega}=\mathsf{VolumetricCenter}(\bm{A},\bm{b})\) (resp. \(\tilde{\bm{\omega}}=\mathsf{VolumetricCenter}(\tilde{\bm{A}},\tilde{\bm{b}})\)) the volumetric center of the polyhedron before (resp. after) adding the constraint. Next, we consider the vector \((\bm{b}^{\prime})^{\top}=[\bm{b}^{\top},\bm{a}^{\top}_{m+1}\bm{\omega}]\), which would have been obtained if the cut was performed at \(\bm{\omega}\) exactly. We then denote \(\bm{\omega}^{\prime}=\mathsf{VolumetricCenter}(\tilde{\bm{A}},\bm{b}^{\prime})\). Then proof of [2] shows that

\[V_{\tilde{\bm{A}},\bm{b}^{\prime}}(\bm{\omega}^{\prime})\geq V_{\bm{A},\bm{b} }(\bm{\omega})+0.0340.\]

We now observe that by construction, we have \(\tilde{\bm{b}}_{m+1}\geq\bm{a}^{\top}_{m+1}\bm{\omega}\), so that the polyhedron associated to \((\tilde{\bm{A}},\tilde{\bm{b}})\) is more constrained than the one associated to \((\tilde{\bm{A}},\bm{b}^{\prime})\). As a result, we have \(V_{\tilde{\bm{A}},\tilde{\bm{b}}}(\bm{x})\geq V_{\tilde{\bm{A}},\bm{b}^{ \prime}}(\bm{x})\), for any \(\bm{x}\in\mathbb{R}^{d}\) such that \(\tilde{\bm{A}}\bm{x}\geq\tilde{\bm{b}}\). Therefore,

\[V_{\tilde{\bm{A}},\tilde{\bm{b}}}(\tilde{\bm{\omega}})\geq V_{\tilde{\bm{A}}, \bm{b}^{\prime}}(\tilde{\bm{\omega}})\geq V_{\tilde{\bm{A}},\bm{b}^{\prime}} (\bm{\omega}^{\prime})\geq V_{\bm{A},\bm{b}}(\bm{\omega})+0.0340.\]

This ends the modifications in the proof of [2]. With the notations of this paper, we still have \(\Delta V^{+}=0.340\) and \(\Delta V^{-}=0.326\), so that \(\Delta V=0.0014\). Then, because \(c=\frac{1}{\Delta V}\), the same proof shows that the procedure is successful for precision \(\delta\): the final polyhedron \((\bm{A},\bm{b})\) returned by Algorithm 1 does not contains a ball of radius \(>\delta\). As a result, whether the algorithm performed all \(T_{max}\) iterations or not, \(\{\bm{x}:\bm{A}\bm{x}\geq\bm{b}\}\) does not contain a ball of radius \(>\delta^{\prime}\), where \(\bm{A}\) and \(\bm{b}\) define the constraints stored in the output \(\mathcal{P}\). Now letting \(m\) be the objective value of the right optimization problem, there exists \(\bm{x}\in\mathcal{C}_{d}\) such that for all \(t\leq T\), \(\bm{g}^{\top}_{t}(\bm{x}-\bm{c}_{t})\geq m\). Therefore, for any \(\bm{x}^{\prime}\in B_{d}(\bm{x},m)\) one has

\[\forall i\in[m],\bm{a}^{\top}_{i}\bm{x}^{\prime}-b_{i}\geq m+\bm{a}^{\top}_{t} (\bm{x}^{\prime}-\bm{x})\geq m-\|\bm{x}^{\prime}-\bm{x}\|\geq 0.\]

In the last inequality we used \(\|\bm{a}_{t}\|\leq 1\). This implies that the polyhedron contains \(B_{d}(\bm{x},m)\). Hence, \(m\leq\delta\).

This ends the proof of the right inequality. The left equality is a direct application of strong duality for linear programming. 

We now prove that Algorithm 4 has the desired oracle-complexity and memory usage.

We first describe the recursive calls of Algorithm 3 in more detail. To do so, consider running the procedure \(\mathsf{ApproxOracle}(i,j,\mathcal{P}^{(1)},\dots,\mathcal{P}^{(i)})\) where \(i<p\), which corresponds to running Algorithm 2 for specific oracles. We say that this is a level-\(i\) run. Then, the algorithm performs at most \(2T(\delta,k_{i+1})\) calls to \(\mathsf{ApproxOracle}(i+1,i+1,\mathcal{P}^{(1)},\dots,\mathcal{P}^{(i)},\cdot)\), where the factor \(2\) comes from the fact that Vaidya's method Algorithm 1 is effectively run twice in Algorithm 2. The solution to (\(\mathcal{P}_{aux}(\mathcal{P})\)) has as many components as constraints in the last polyhedron, which is at most \(\frac{k_{i+1}}{\sigma_{min}}+1\) by Lemma 4.2. Hence, the number of calls to \(\mathsf{ApproxOracle}(i+1,j,\mathcal{P}^{(1)},\dots,\mathcal{P}^{(i)},\cdot)\) is at most \(\frac{k_{i+1}}{\sigma_{min}}+1\). In total, that is \(\mathcal{O}(k_{i+1}\ln\frac{1}{5})\) calls to the level \(i+1\) of the recursion.

We next aim to understand the output of running \(\mathsf{ApproxOracle}(1,1,\mathcal{P}^{(1)})\). We denote by \(\bm{\lambda}(\mathcal{P}^{(1)})\) the solution \(\mathcal{P}_{aux}(\mathcal{P}^{\star})\) computed at l.2 of the first call to Algorithm 2, where \(\mathcal{P}^{\star}\) is the output polyhedron of the first call to Algorithm 1. Denote by \(\mathcal{S}(\mathcal{P}^{(1)})\) the set of indices of coordinates from \(\bm{\lambda}(\mathcal{P}^{(1)})\) for which the procedure performed a call to \(\mathsf{ApproxOracle}(2,1,\mathcal{P}^{(1)},\cdot)\). In other words, \(\mathcal{S}(\mathcal{P}^{(1)})\) contains the indices of all coordinates of \(\bm{\lambda}(\mathcal{P}^{(1)})\), except those for which the corresponding query lay outside of the unit cube, or the initial constraints of the cube. For any index \(l\in\mathcal{S}(\mathcal{P}^{(1)})\), let \(\mathcal{P}^{(2)}_{l}\) denote the state of the current polyhedron (\(\mathcal{P}_{l}\) in l.7 of Algorithm 2) when that call was performed. Up to discretization issues, the output of the complete procedure is

\[\sum_{l\in\mathcal{S}(\mathcal{P}^{(1)})}\lambda_{l}(\mathcal{P}^{(1)}) \mathsf{ApproxOracle}(2,1,\mathcal{P}^{(1)},\mathcal{P}^{(2)}_{l}).\]

We continue in the recursion, defining \(\bm{\lambda}(\mathcal{P}^{(1)},\mathcal{P}^{(2)}_{l})\) and \(\mathcal{S}(\mathcal{P}^{(1)},\mathcal{P}^{(2)}_{l})\) for all \(l\in\mathcal{S}(\mathcal{P}^{(1)})\), until we define all vectors of the form \(\bm{\lambda}(\mathcal{P}^{(1)},\mathcal{P}^{(2)}_{l_{2}},\dots,\mathcal{P}^{ (r)}_{l_{r}})\) and sets of the form \(\mathcal{S}(\mathcal{P}^{(1)},\mathcal{P}^{(2)}_{l_{2}},\dots,\mathcal{P}^{ (r)}_{l_{r}})\) for \(i+1\leq r\leq p-1\). To simplify the notation and emphasize that all these polyhedra depend on the recursive computation path, we adopt the notation

\[\lambda^{l_{2},\dots,l_{r+1}} :=\lambda_{l_{r+1}}(\mathcal{P}^{(1)},\mathcal{P}^{(2)}_{l_{2}}, \dots,\mathcal{P}^{(r)}_{l_{r}})\] \[\mathcal{S}^{l_{2},\dots,l_{r}} :=\mathcal{S}(\mathcal{P}^{(1)},\mathcal{P}^{(2)}_{l_{2}},\dots, \mathcal{P}^{(r)}_{l_{r}})\]

We recall that these polyhedron are kept in memory to query their volumetric center. For ease of notation, we write \(\bm{x}_{1}=\mathsf{VolumetricCenter}(\mathcal{P}^{(1)})\), and we write \(\bm{c}^{l_{2},\dots,l_{r}}=\mathsf{VolumetricCenter}(\mathcal{P}^{(r)}_{l_{r}})\) for \(2\leq r\leq p\), where \(l_{2},\dots,l_{r-1}\) were the indices from the computation path leading up to \(\mathcal{P}^{(r)}_{l_{r}}\). Last, we write \(O_{S}=(O_{S,1},\dots,O_{S,p})\), where \(O_{S,i}:\mathcal{C}_{d}\to\mathbb{R}^{k_{i}}\) is the "\(\bm{x}_{i}\)" component of \(O_{S}\), for all \(i\in[p]\).

With all these notations, we will show that the output of \(\mathsf{ApproxOracle}(i,j,\mathcal{P}^{(1)},\mathcal{P}^{(2)}_{l_{2}},\dots, \mathcal{P}^{(i)}_{l_{i}})\) is approximately equal to the vector

\[G(i,j,\bm{x}_{1},\bm{c}^{l_{2}},\dots,\bm{c}^{l_{2},\dots,l_{i}})\] \[:=\sum_{\begin{subarray}{c}l_{i+1}\in\mathcal{S},\ l_{i+2}\in \mathcal{S}^{l_{i}+1},\\ \dots\ l_{p}\in\mathcal{S}^{l_{i+1}+\dots,l_{p-1}}\end{subarray}}\lambda^{l_{i+ 1}}\lambda^{l_{i+1},l_{i+2}}\dots\lambda^{l_{i+1},\dots,l_{p}}\cdot O_{S,j}( \bm{x}_{1},\bm{c}^{l_{2}},\dots,\bm{c}^{l_{2},\dots,l_{p}}),\]

with the convention that for \(i=p\),

\[G(p,j,\bm{x}_{1},\bm{c}^{l_{2}},\dots,\bm{c}^{l_{2},\dots,l_{p}}):=O_{S,j}(\bm{ x}_{1},\bm{c}^{l_{2}},\dots,\bm{c}^{l_{2},\dots,l_{p}}).\]

The corresponding computation tree is represented in Fig. 3. For convenience, we omitted the term \(j=1\).

We start the analysis with a simple result showing that if the oracle \(O_{S}\) returns separation vectors of norm bounded by one, then the responses from \(\mathsf{ApproxOracle}\) also lie in the unit ball.

**Lemma A.1**.: _Fix \(\delta,\xi\in(0,1)\), \(1\leq j\leq i\leq p\) and an oracle \(O_{S}=(O_{S,1},\dots,O_{S,p}):\mathcal{C}_{d}\to\mathbb{R}^{d}\). Suppose that \(O_{S}\) takes values in the unit ball. For any \(s\in[i]\) let \(\mathcal{P}^{(s)}_{l_{s}}\in\mathcal{I}_{k_{s}}\) represent a bounded polyhedrons with \(\mathsf{VolumetricCenter}(\mathcal{P}^{(s)}_{l_{s}})\in\mathcal{C}_{k_{s}}\). Then, one has_

\[\|\mathsf{ApproxOracle}_{\delta,\xi,O_{S}}(i,j,\mathcal{P}^{(1)}_{l_{1}},\dots, \mathcal{P}^{(i)}_{l_{i}})\|\leq 1.\]

Proof.: We prove this by simple induction on \(i\). For convenience, we define the point \(\bm{x}_{k}=\mathsf{VolumetricCenter}(\mathcal{P}^{(k)}_{l_{k}})\). If \(i=p\), we have

\[\|\mathsf{ApproxOracle}_{\delta,\xi,O_{S}}(i,j,\mathcal{P}^{(1)}_{l_ {1}},\dots,\mathcal{P}^{(i)}_{l_{i}})\| =\|\mathsf{Discretize}_{k_{j}}(O_{S,j}(\bm{x}_{1},\dots,\bm{x}_{p}),\xi)\|\] \[\leq\|O_{S,j}(\bm{x}_{1},\dots,\bm{x}_{p})\|\leq 1,\]where in the first inequality we used Eq (1) and in the second inequality we used the fact that \(O_{S}(\bm{x}_{1},\ldots,\bm{x}_{p})\) has norm at most one. Now suppose that the result holds for \(i+1\leq p\). Then by construction, the output \(\mathsf{ApproxOracle}_{\delta,\xi,O_{S}}(i,j,\mathcal{P}^{(1)}_{l_{1}},\ldots, \mathcal{P}^{(i)}_{l_{i}})\) is the result of iterative discretizations. Using Eq (1) and the previously defined notations, we obtain

\[\|\mathsf{ApproxOracle}_{\delta,\xi,O_{S}}(i,j,\mathcal{P}^{(1)}_{ l_{1}},\ldots,\mathcal{P}^{(i)}_{l_{i}})\|\\ \leq\left\|\sum_{l_{i+1}\in\mathcal{S}^{l_{1}},\ldots,l_{i}} \lambda^{l_{2},\ldots,l_{i}}\mathsf{ApproxOracle}_{\delta,\xi,O_{S}}(i+1,j, \mathcal{P}^{(1)}_{l_{1}},\ldots,\mathcal{P}^{(i)}_{l_{i}},\mathcal{P}^{(i+1) }_{l_{i+1}})\right\|\leq 1.\]

In the last inequality, we used the induction hypothesis together with the fact that \(\sum_{l_{i+1}}\lambda^{l_{2},\ldots,l_{i+1}}\leq 1\) using Eq (1). This ends the induction and the proof. 

We are now ready to compare the output of Algorithm 3 to \(G(i,j,\bm{x}_{1},\bm{c}^{l_{2}},\ldots,\bm{c}^{l_{2},\ldots,l_{i}})\).

**Lemma A.2**.: _Fix \(\delta,\xi\in(0,1)\), \(1\leq j\leq i\leq p\) and an oracle \(O_{S}=(O_{S,1},\ldots,O_{S,p}):\mathcal{C}_{d}\to\mathbb{R}^{d}\). Suppose that \(O_{S}\) takes values in the unit ball. For any \(s\in[i]\) let \(\mathcal{P}^{(s)}_{l_{s}}\in\mathcal{I}_{k_{s}}\), represent a bounded polyhedron with \(\mathsf{VolumetricCenter}(\mathcal{P}^{(s)}_{l_{s}})\in\mathcal{C}_{k_{s}}\). Denote \(\bm{x}_{r}=\bm{c}(\mathcal{P}^{(r)}_{l_{r}})\) for \(r\in[i]\). Then,_

\[\|\mathsf{ApproxOracle}_{\delta,\xi,O_{S}}(i,j,\mathcal{P}^{(1)}_{l_{1}},\ldots,\mathcal{P}^{(i)}_{l_{i}})-G(i,j,\bm{x}_{1},\ldots,\bm{x}_{i})\|\leq\frac{4} {\sigma_{min}}d\xi.\]

Proof.: We prove by simple induction on \(i\) that

\[\|\mathsf{ApproxOracle}_{\delta,\xi,O_{S}}(i,j,\mathcal{P}^{(1)}_{ l_{1}},\ldots,\mathcal{P}^{(i)}_{l_{i}})-G(i,j,\bm{x}_{1},\ldots,\bm{x}_{i})\|\\ \leq\left(1+\frac{2}{\sigma_{min}}(k_{i+1}+\ldots+k_{p})+2(p-i) \right)\xi.\]

First, for \(i=p\), the result is immediate since the discretization is with precision \(\xi\) (l.4 of Algorithm 3). Now suppose that this is the case for \(i\leq p\) and any valid values of other parameters. For conciseness, we write \(\bm{G}=(\mathcal{P}^{(1)}_{l_{1}},\ldots,\mathcal{P}^{(i-1)}_{l_{i-1}})\). Next, recall that by Lemma 4.2, \(|\mathcal{S}^{l_{2},\ldots,l_{i-1}}|\leq\frac{k_{1}}{\sigma_{min}}+1\). Hence,

Figure 3: Computation tree representing the recursive calls to \(\mathsf{ApproxOracle}\) starting from the calls to \(\mathsf{ApproxOracle}(1,1,\cdot)\) from Algorithm 4

the discretizations due to 1.8 of Algorithm 2 can affect the estimate for at most that number of rounds. Then, we have

\[\left\|\mathsf{ApproxOracle}_{\delta,\xi,O_{S}}(i-1,j,\bm{G})-\sum_{ l_{i}\in\mathcal{S}^{l_{2},\ldots,l_{i-1}}}\tilde{\lambda}^{l_{2},\ldots,l_{i}} \mathsf{ApproxOracle}_{\delta,\xi,O_{S}}(i,j,\bm{G},\mathcal{P}_{l_{i}}^{(i)}) \right\|\\ \leq\left(\frac{k_{i}}{\sigma_{min}}+1\right)\xi,\]

where \(\tilde{\lambda}^{l_{2},\ldots,l_{i}}\) are the discretized coefficients that are used during the computation 1.8 of Algorithm 2. Now using Lemma A.1, we have

\[\left\|\sum_{l_{i}\in\mathcal{S}^{l_{2},\ldots,l_{i-1}}}(\tilde{ \lambda}^{l_{2},\ldots,l_{i}}-\lambda^{l_{2},\ldots,l_{i}})\mathsf{ApproxOracle}_{ \delta,\xi,O_{S}}(i,j,\bm{G},\mathcal{P}_{l_{i}}^{(i)})\right\|\\ \leq\|\tilde{\bm{\lambda}}^{l_{i+1},\ldots,l_{i-1}}-\bm{\lambda}^ {l_{i+1},\ldots,l_{i-1}}\|_{1}\leq\left(\frac{k_{i}}{\sigma_{min}}+1\right)\xi.\]

In the last inequality we used the fact that \(\bm{\lambda}\) has at most \(\frac{k_{i}}{\sigma_{min}}+1\) non-zero coefficients. As a result, using the induction for each term of the sum, and the fact that \(\sum_{l_{i}}\lambda^{l_{2},\ldots,l_{i}}\leq 1\), we obtain

\[\|\mathsf{ApproxOracle}_{\delta,\xi,\mathcal{O}_{f}}(i-1,j,\bm{G})- G(i-1,j,\bm{x}_{1},\ldots,\bm{x}_{i-1})\|\\ \leq\left(1+\frac{2}{\sigma_{min}}(k_{i+1}+\ldots+k_{p})+2(p-i) \right)\xi+\left(\frac{2k_{i}}{\sigma_{min}}+2\right)\xi,\]

which completes the induction. Noting that \(k_{i+1}+\ldots+k_{p}\leq k_{1}+\ldots+k_{p}\leq d\) and \(p-i\leq d-1\) ends the proof. 

Next, we show that the outputs of Algorithm 3 provide approximate separation hyperplanes for the first \(i\) coordinates \((\bm{x}_{1},\ldots,\bm{x}_{i})\).

**Lemma A.3**.: _Fix \(\delta,\xi\in(0,1)\), \(1\leq j\leq i\leq p\) and an oracle \(O_{S}=(O_{S,1},\ldots,O_{S,p}):\mathcal{C}_{d}\to\mathbb{R}^{d}\) for accuracy \(\epsilon>0\). Suppose that \(O_{S}\) takes values in the unit ball \(B_{d}(0,1)\). For any \(s\in[i]\) let \(\mathcal{P}_{l_{s}}^{(s)}\in\mathcal{I}_{k_{s}}\) represent a bounded polyhedron with \(\mathsf{VolumetricCenter}(\mathcal{P}_{l_{s}}^{(s)})\in\mathcal{C}_{k_{s}}\). Denote \(\bm{x}_{r}=\bm{c}(\mathcal{P}_{l_{r}}^{(r)})\) for \(r\in[i]\). Suppose that when running \(\mathsf{ApproxOracle}_{\delta,\xi,O_{S}}(i,i,\mathcal{P}_{l_{1}}^{(1)},\ldots, \mathcal{P}_{l_{i}}^{(i)})\), no successful vector was queried. Then, any vector \(\bm{x}^{\star}=(\bm{x}_{1}^{\star},\ldots,\bm{x}_{p}^{\star})\in\mathcal{C}_{d}\) such that \(B_{d}(\bm{x}^{\star},\epsilon)\) is contained in the successful set satisfies_

\[\sum_{r\in[i]}\mathsf{ApproxOracle}_{\delta,\xi,O_{S}}(i,r,\mathcal{P}_{l_{1}}^ {(1)},\ldots,\mathcal{P}_{l_{i}}^{(i)})^{\top}(\bm{x}_{r}^{\star}-\bm{x}_{r}) \geq\epsilon-\frac{8d^{5/2}}{\sigma_{min}}\xi-d\delta.\]

Proof.: For \(i\leq r\leq p\) and \(j\leq r\), we use the notation

\[\bm{g}_{j}^{l_{i+1},\ldots,l_{r}}=\mathsf{ApproxOracle}_{\delta,\xi,O_{S}}(r,j,\mathcal{P}_{l_{1}}^{(1)},\ldots,\mathcal{P}_{l_{r}}^{(r)}).\]

Using Lemma A.2, we always have for \(j\in[r]\),

\[\|\bm{g}_{j}^{l_{i+1},\ldots,l_{r}}-G(r,j,\bm{x}_{1},\ldots,\bm{x}_{i},\bm{c}^ {l_{i+1}},\ldots,\bm{c}^{l_{i+1},\ldots,l_{r}})\|\leq\frac{4d}{\sigma_{min}}\xi.\] (2)

Also, observe that by Lemma A.1 the recursive outputs of \(\mathsf{ApproxOracle}\) always have norm bounded by one.

Next, let \(\mathcal{T}^{l_{i+1},\ldots,l_{r-1}}\) be the set of indices corresponding to coordinates of \(\bm{\lambda}^{l_{i+1},\ldots,l_{r-1}}\) for which the procedure \(\mathsf{ApproxOracle}\) did not call for a level-\(r\) computation. These correspond to 1. constraints from the initial cube \(\mathcal{P}_{0}\), or 2. cases when the volumetric center was out of the unit cube (1.6-7 of Algorithm 1) and as a result, the index of the added constraint was \(-1\) instead of the current iteration index \(t\). Similarly as above, for any \(t\in\mathcal{T}^{l_{i+1},\ldots,l_{r-1}}\), we denote by \(\bm{g}_{r}^{l_{i+1},\ldots,l_{r-1},t}\) the corresponding vector \(\bm{a}_{t}\). We recall that by construction, this vector is of the form \(\pm\bm{e}_{j}\) for some \(j\in[k_{r}]\). Then, from Lemma 4.1, since the responses of the oracle always have norm bounded by one, for all \(\bm{y}_{r}\in\mathcal{C}_{k_{r}}\),

\[\sum_{l_{r}\in\mathcal{S}^{l_{i+1},\ldots,l_{r-1}}\cup\mathcal{T}^{l_{i+1}, \ldots,l_{r-1}}}\lambda^{l_{i+1},\ldots,l_{r}}(\bm{g}_{r}^{l_{i+1},\ldots,l_{r }})^{\top}(\bm{y}_{r}-\bm{c}^{l_{i+1},\ldots,l_{r}})\leq\delta.\] (3)

For conciseness, we use the shorthand \((\mathcal{S}\cup\mathcal{T})^{l_{i+1},\ldots,l_{r-1}}:=\mathcal{S}^{l_{i+1}, \ldots,l_{r-1}}\cup\mathcal{T}^{l_{i+1},\ldots,l_{r-1}}\), which contains all indices from coordinates of \(\bm{\lambda}^{l_{i+1},\ldots,l_{r-1}}\). In particular,

\[\sum_{l_{r}\in(\mathcal{S}\cup\mathcal{T})^{l_{i+1},\ldots,l_{r-1}}}\lambda^ {l_{i+1},\ldots,l_{r}}=1.\] (4)

We now proceed to estimate the precision of the vectors \(G(i,j,\bm{x}_{1},\ldots,\bm{x}_{i})\) as approximate separation hyperplanes for coordinates \((\bm{x}_{1},\ldots,\bm{x}_{i})\). Let \(\bm{x}^{\star}\in\mathcal{C}_{d}\) such that \(B_{d}(\bm{x}^{\star},\epsilon)\) is within the successful set. Then, for any choice of \(l_{i+1}\in\mathcal{S},\ldots,l_{p}\in\mathcal{S}^{l_{i+1},\ldots,l_{p-1}}\), since we did not query a successful vector, we have for all \(\bm{z}\in B_{d}(\bm{x}^{\star},\epsilon)\),

\[O_{S}(\bm{x}_{1},\ldots,\bm{x}_{i},\bm{c}^{l_{i+1}},\ldots,\bm{c}^{l_{i+1}, \ldots,l_{p}})^{\top}(\bm{z}-(\bm{x}_{1},\ldots,\bm{x}_{i},\bm{c}^{l_{i+1}}, \ldots,\bm{c}^{l_{i+1},\ldots,l_{p}}))\geq 0.\]

As a result, because the responses from \(O_{S}\) have unit norm,

\[O_{S}(\bm{x}_{1},\ldots,\bm{x}_{i},\bm{c}^{l_{i+1}},\ldots,\bm{c}^{l_{i+1}, \ldots,l_{p}})^{\top}(\bm{x}^{\star}-(\bm{x}_{1},\ldots,\bm{x}_{i},\bm{c}^{l_{ i+1}},\ldots,\bm{c}^{l_{i+1},\ldots,l_{p}}))\geq\epsilon.\] (5)

Now write \(\bm{x}^{\star}=(\bm{x}_{1}^{\star},\ldots,\bm{x}_{p}^{\star})\). In addition to the previous equation, for \(l_{i+1}\in\mathcal{S},\ldots,l_{r-1}\in\mathcal{S}^{l_{i+1},\ldots,l_{r-2}}\) and any \(l_{r}\in\mathcal{T}^{l_{i+1},\ldots,l_{r-1}}\), one has \((\bm{g}_{r}^{l_{i+1},\ldots,l_{r}})^{\top}\bm{x}_{r}^{\star}+1\geq\epsilon\), because \(\bm{x}^{\star}\) is within the cube \(\mathcal{C}_{d}\) and at least at distance \(\epsilon\) from the constraints of the cube. Similarly as when \(l_{r}\in\mathcal{S}^{l_{i+1},\ldots,l_{r-1}}\), for any \(l_{r}\in\mathcal{T}^{l_{i+1},\ldots,l_{r-1}}\) we denote by \(\bm{c}^{l_{i+1},\ldots,l_{r}}\) the volumetric center of the polyhedron \(\mathcal{P}^{(r)}_{l_{r}}\) along the corresponding computation path, if \(l_{r}\) corresponded to an added constraints when \(\bm{c}^{l_{i+1},\ldots,l_{r}}\notin\mathcal{C}_{k_{r}}\). Otherwise, if \(l_{r}\) corresponded to the constraint \(\bm{a}=\pm\bm{e}_{j}\) of the initial cube, we pose \(\bm{c}^{l_{i+1},\ldots,l_{r}}=-\bm{a}\). Now by construction, in both cases one has \((\bm{g}_{r}^{l_{i+1},\ldots,l_{r}})^{\top}\bm{c}^{l_{i+1},\ldots,l_{r}}\leq-1\) (1.7 of Algorithm 1). Thus,

\[(\bm{g}_{r}^{l_{i+1},\ldots,l_{r}})^{\top}(\bm{x}_{r}^{\star}-\bm{c}^{l_{i+1}, \ldots,l_{r}})\geq\epsilon.\] (6)

Recalling Eq (4), we then sum all equations of the form Eq (5) and Eq (6) along the computation path, to obtain

\[(A):=\sum_{\begin{subarray}{c}l_{i+1}\in\mathcal{S},\ldots,l_{p-1} \\ l_{p}\in\mathcal{S}^{l_{i+1},\ldots,l_{p-1}}\end{subarray}}\lambda^{l_{i+1}} \ldots\lambda^{l_{i+1},\ldots,l_{p}}\\ \cdot O_{S}(\bm{x}_{1},\ldots,\bm{x}_{i},\bm{c}^{l_{i+1}},\ldots,\bm{c}^{l_{i+ 1},\ldots,l_{p}})^{\top}(\bm{x}^{\star}-(\bm{x}_{1},\ldots,\bm{x}_{i},\bm{c}^ {l_{i+1}},\ldots,\bm{c}^{l_{i+1},\ldots,l_{p}}))\\ +\sum_{i+1\leq r\leq p}\sum_{\begin{subarray}{c}l_{i+1}\in\mathcal{S}, \ldots,l_{r-1}\in\mathcal{S}^{l_{i+1},\ldots,l_{r}-2},\\ l_{r}\in\mathcal{T}^{l_{i+1},\ldots,l_{r-1}}\end{subarray}}\lambda^{l_{i+1}} \ldots\lambda^{l_{i+1},\ldots,l_{r}}\cdot(\bm{g}_{r}^{l_{i+1},\ldots,l_{r}})^{ \top}(\bm{x}_{r}^{\star}-\bm{c}^{l_{i+1},\ldots,l_{r}})\geq\epsilon.\]

Now using the convention

\[G(r,r,\bm{x}_{1},\ldots,\bm{x}_{i},\bm{c}^{l_{i+1}},\ldots,\bm{c}^{l_{i+1}, \ldots,l_{r}}):=\bm{g}_{r}^{l_{i+1},\ldots,l_{r}},\qquad l_{r}\in\mathcal{T}^{l_{ i+1},\ldots,l_{r-1}},\]

for any \(l_{i+1}\in\mathcal{S},\ldots,l_{r-1}\in\mathcal{S}^{l_{i+1},\ldots,l_{r-2}}\), we can write

\[(A)=\sum_{r\leq i}G(i,r,\bm{x}_{1},\ldots,\bm{x}_{i})^{\top}(\bm{x }_{r}^{\star}-\bm{x}_{r})+\sum_{i+1\leq r\leq p}\sum_{\begin{subarray}{c}l_{i+1} \in\mathcal{S},\ldots,\\ l_{r-1}\in\mathcal{S}^{l_{i+1},\ldots,l_{r-2}}\end{subarray}}\lambda^{l_{i+1}} \ldots\lambda^{l_{i+1},\ldots,l_{r-1}}\\ \times\sum_{l_{r}\in(\mathcal{S}\cup\mathcal{T})^{l_{i+1},\ldots,l_{r-1}}} \lambda^{l_{i+1},\ldots,l_{r}}G(r,r,\bm{x}_{1},\ldots,\bm{x}_{i},\bm{c}^{l_{ i+1}},\ldots,\bm{c}^{l_{i+1},\ldots,l_{r}})^{\top}(\bm{x}_{r}^{\star}-\bm{c}^{l_{ i+1},\ldots,l_{r}}).\]

We next relate the terms \(G\) to the output of \(\mathsf{Approx Oracle}\). For simplicity, let us write \(\bm{G}=(\mathcal{P}^{(1)}_{l_{1}},\ldots,\mathcal{P}^{(i)}_{l_{1}})\), which by abuse of notation was assimilated to \((\bm{x}_{1},\ldots,\bm{x}_{i})\). Recall that by construction and hypothesis, all points where the oracle was queried belong to \(\mathcal{C}_{d}\), so that for instance \(\|\bm{x}_{r}^{\star}-\bm{c}^{l_{i+1},\ldots,l_{r}}\|\leq 2\sqrt{k_{r}}\leq 2\sqrt{d}\) for any \(l_{r}\in\mathcal{S}^{l_{i+1},\ldots,l_{r-1}}\). Using the above equations together with Eq (2) and Lemma A.2 gives

\[\epsilon \leq\sum_{r\leq i}\left[\mathsf{ApproxOracle}_{\delta,\xi,\mathcal{ O}_{f}}(i,r,\bm{G})^{\top}(\bm{x}_{r}^{\star}-\bm{x}_{r})+\frac{8d^{3/2}}{\sigma_{ min}}\xi\right]+\sum_{i+1\leq r\leq p}\sum_{\begin{subarray}{c}l_{i+1}\in\mathcal{S}, \ldots,\\ l_{r-1}\in\mathcal{S}^{l_{i+1},\ldots,l_{r-2}}\end{subarray}}\] \[\lambda^{l_{i+1}}\cdots\lambda^{l_{i+1},\ldots,l_{r-1}}\sum_{l_{ r}\in(\mathcal{S}\cup\mathcal{T})^{l_{i+1},\ldots,l_{r}}}\lambda^{l_{i+1},\ldots,l_{r}} \left[(\bm{g}_{r}^{l_{i+1},\ldots,l_{r}})^{\top}(\bm{x}_{r}^{\star}-\bm{c}^{l_ {i+1},\ldots,l_{r}})+\frac{8d^{3/2}}{\sigma_{min}}\xi\right]\] \[\leq\frac{8pd^{3/2}}{\sigma_{min}}\xi+(p-i)\delta+\sum_{r\leq i} \mathsf{ApproxOracle}_{\delta,\xi,\mathcal{O}_{f}}(i,r,\bm{G})^{\top}(\bm{x}_{ r}^{\star}-\bm{x}_{r})\]

where in the second inequality, we used Eq (3). Using \(p\leq d\), this ends the proof of the lemma. 

We are now ready to show that Algorithm 4 is a valid algorithm for convex optimization.

**Theorem A.1**.: _Let \(\epsilon\in(0,1)\) and \(O_{S}:\mathcal{C}_{d}\to\mathbb{R}^{d}\) be a separation oracle such that the successful set contains a ball of radius \(\epsilon\). Pose \(\delta=\frac{\epsilon}{4d}\) and \(\xi=\frac{\sigma_{min}\epsilon}{32d^{5/2}}\). Next, let \(p\geq 1\) and \(k_{1},\ldots,k_{p}\leq\lceil\frac{d}{p}\rceil\) such that \(k_{1}+\ldots+k_{p}=d\). With these parameters, Algorithm 4 finds a successful vector with \((C\frac{d}{p}\ln\frac{d}{\epsilon})^{p}\) queries and using memory \(\mathcal{O}(\frac{d^{2}}{p}\ln\frac{d}{\epsilon})\), for some universal constant \(C>0\)._

Proof.: Suppose by contradiction that Algorithm 4 never queried a successful point. Then, with the chosen parameters, Lemma A.3 shows that, for any vector \(\bm{x}^{\star}=(\bm{x}_{1}^{\star},\ldots,\bm{x}_{p}^{\star})\) such that \(B_{d}(\bm{x}^{\star},\epsilon)\) is within the successful set, with the same notations, one has

\[\sum_{r\leq i}\mathsf{ApproxOracle}_{\delta,\xi,O_{S}}(i,r,\mathcal{P}_{l_{1}}^ {(1)},\ldots,\mathcal{P}_{l_{i}}^{(i)})^{\top}(\bm{x}_{r}^{\star}-\bm{x}_{r}) \geq\epsilon-\frac{8d^{5/2}}{\sigma_{min}}\xi-d\delta\geq\frac{\epsilon}{2}.\]

Now denote by \((\bm{a}_{t},b_{t})\) the constraints that were added at any time during the run of Algorithm 1 when using the oracle \(\mathsf{ApproxOracle}\) with \(i=j=1\). The previous equation shows that for all such constraints,

\[\bm{a}_{t}^{\top}\bm{x}_{1}^{\star}-b_{t}\geq\bm{a}_{t}^{\top}(\bm{x}_{1}^{ \star}-\omega_{t})-\xi\geq\frac{\epsilon}{2}-\xi,\]

where \(\bm{\omega}_{t}\) is the volumetric center of the polyhedron at time \(t\) during Vaidya's method Algorithm 1. Now, since the algorithm terminated, by Lemma 4.1, we have that

\[\min_{t}(\bm{a}_{t}^{\top}\bm{x}_{1}^{\star}-b_{t})\leq\delta.\]

This is absurd since \(\delta+\xi<\frac{\epsilon}{2}\). This ends the proof that Algorithm 4 finds a successful vector.

We now estimate its oracle-complexity and memory usage. First, recall that a run of \(\mathsf{ApproxOracle}\) of level \(i\) makes \(\mathcal{O}(k_{i+1}\ln\frac{1}{\delta})\) calls to level-\((i+1)\) runs of \(\mathsf{ApproxOracle}\). As a result, the oracle-complexity \(Q_{d}(\epsilon;k_{1},\ldots,k_{p})\) satisfies

\[Q_{d}(\epsilon;k_{1},\ldots,k_{p})=\left(Ck_{1}\ln\frac{1}{\delta}\right)\times \ldots\times\left(Ck_{p}\ln\frac{1}{\delta}\right)\leq\left(C^{\prime}\frac{d}{ p}\log\frac{d}{\epsilon}\right)^{p}\]

for some universal constants \(C,C^{\prime}\geq 2\).

We now turn to the memory of the algorithm. For each level \(i\in[p]\) of runs for \(\mathsf{ApproxOracle}\), we keep memory placements for

1. the value \(j^{(i)}\) of the corresponding call to \(\mathsf{ApproxOracle}(i,j^{(i)},\cdot)\) (for l.6-7 of Algorithm 3): \(\mathcal{O}(\ln d)\) bits,
2. the iteration number \(t^{(i)}\) during the run of Algorithm 1 or within Algorithm 2: \(\mathcal{O}(\ln(k_{i}\ln\frac{1}{\delta}))\) bits
3. the polyhedron constraints contained in the state of \(\mathcal{P}^{(i)}\): \(\mathcal{O}(k_{i}\times k_{i}\ln\frac{1}{\delta})\) bits,4. potentially, already computed dual variables \(\bm{\lambda}^{\star}\) and their corresponding vector of constraint indices \(\bm{k}^{\star}\) (1.3 of Algorithm 2): \(\mathcal{O}(k_{i}\times\ln\frac{1}{\xi})\) bits,
5. the working vector \(\bm{u}^{(i)}\) (updated l.8 of Algorithm 2): \(\mathcal{O}(k_{i}\ln\frac{1}{\xi})\) bits.

The memory structure is summarized in Table 1.

We can then check that this memory is sufficient to run Algorithm 4. An important point is that for any run of \(\mathsf{ApproxOracle}(i,j,\cdot)\), in Algorithm 2, after running Vaidya's method Algorithm 1 and storing the dual variables \(\bm{\lambda}^{\star}\) and corresponding indices \(\bm{k}^{\star}\) within their placements \((\bm{k}^{\star}{}^{(i)},\bm{\lambda}^{\star}{}^{(i)})\) (l.1-3 of Algorithm 2), the iteration index \(t^{(i)}\) and polyhedron \(\mathcal{P}^{(i)}\) memory placements are reset and can be used again for the second run of Vaidya's method (l.4-10 of Algorithm 2). During this second run, the vector \(\bm{u}\) is stored in its corresponding memory placement \(\bm{u}^{(i)}\) and updated along the algorithm. Once this run is finished, the output of \(\mathsf{ApproxOracle}(i,j,\cdot)\) is readily available in the placement \(\bm{u}^{(i)}\). For \(i=p\), the algorithm does not need to wait for the output of a level-\((i+1)\) computation and can directly use the \(j^{(p)}\)-th component of the returned separation vector from the oracle \(O_{S}\). As a result, the number of bits of memory used throughout the algorithm is at most

\[M=\sum_{i=1}^{p}\mathcal{O}\left(k_{i}^{2}\ln\frac{1}{\xi}\right)=\mathcal{O} \left(\frac{d^{2}}{p}\ln\frac{d}{\epsilon}\right).\]

This ends the proof of the theorem. 

We can already give the useful range for \(p\) for our algorithms, which will also apply to the case with computational-memory constraints Appendix B.

Proof of Corollary 3.1.: Suppose \(\epsilon\geq\frac{1}{d^{4}}\). Then, for some \(p_{max}=\Theta(\frac{C\ln\frac{1}{\epsilon}}{2\ln d})\leq d\), the algorithm from Theorem 3.2 yields a \(\mathcal{O}(\frac{1}{\epsilon^{2}})\) oracle-complexity. On the other hand, if \(\epsilon\leq\frac{1}{d^{4}}\), we can take \(p_{max}=d\), which gives an oracle-complexity \(\mathcal{O}((C\ln\frac{1}{\epsilon})^{d})\). 

## Appendix B Memory-constrained feasibility problem with computations

In the last section we gave the main ideas that allow reducing the storage memory. However, Algorithm 4 does not account for memory constraints in computations as per Definition 2.2. For instance, computing the volumetric center \(\mathsf{VolumetricCenter}(\mathcal{P})\) already requires infinite memory for infinite precision. More importantly, even if one discretizes the queries, the necessary precision and computational power may be prohibitive with the classical Vaidya's method Algorithm 1. Even finding a feasible point in the polyhedron (let alone the volumetric center) using only the constraints is itself computationally intensive. There has been significant work to make Vaidya's method computationally tractable [48; 1; 2]. These works address the issue of computational tractability, but the memory issue

\begin{table}
\begin{tabular}{c|c|c} \(i\) & 1 & \(\dots\) & \(p\) \\ \cline{2-3} \(j\) & \(j^{(1)}\) & \(j^{(p)}\) \\ \cline{2-3} Iteration index & \(t^{(1)}\) & \(t^{(p)}\) \\ \cline{2-3} Polyhedron & \(\mathcal{P}^{(1)}=\left(\begin{array}{c}k_{1},\bm{a}_{1},b_{1}\\ k_{2},\bm{a}_{2},b_{2}\\ \dots\\ k_{m},\bm{a}_{m},b_{m}\end{array}\right)\) & \(\mathcal{P}^{(p)}\) \\ \cline{2-3} Computed dual variables & \((\bm{k}^{\star}{}^{(1)},\bm{\lambda}^{\star}{}^{(1)})=\left(\begin{array}{c}k_{1 }^{\star},\lambda_{1}^{\star}\\ k_{2}^{\star},\lambda_{2}^{\star}\\ \dots\\ \end{array}\right)\) & \((\bm{k}^{\star}{}^{(p)},\bm{\lambda}^{\star}{}^{(p)})\) \\ \cline{2-3} Working separation vector & \(\bm{u}^{(1)}\) & \(\bm{u}^{(p)}\) \\ \hline \end{tabular}
\end{table}
Table 1: Memory structure for Algorithm 4

[MISSING_PAGE_FAIL:21]

[MISSING_PAGE_FAIL:22]

The induction is now complete. When the algorithm stops, either the \(r\) steps were performed, in which case the induction already shows that \(\|\nabla p(\bm{x}^{(r)})\|_{\bm{Q}^{-1}}\leq(1-\frac{1}{64})^{r}\eta\). Otherwise, if the algorithm terminates at iteration \(k\), because \(\|\nabla p(\bm{x}^{(k)})\|_{\bm{Q}^{-1}}\) was computed to precision \((1-\frac{1}{64})^{r}\eta\), we have (see 1.2 of Algorithm 5)

\[\|\nabla p(\bm{x}^{(k)})\|_{\bm{Q}^{-1}}\leq 2\left(1-\frac{1}{64}\right)^{r} \eta+\left(1-\frac{1}{64}\right)^{r}\eta=3\left(1-\frac{1}{64}\right)^{r}\eta.\]

The same argument as in the original proof shows that at each iteration \(t\),

\[\|\bm{S}^{-1}_{x^{(0)}}(\bm{s}(\bm{x}^{(t)})-\bm{s}(\bm{x}^{(0)}))\|_{2}=\|\bm {x}^{(t)}-\bm{x}^{(0)}\|_{\bm{A}^{\top}\bm{S}^{-2}_{x^{(0)}}\bm{A}}\leq\frac{ \|\bm{x}^{(t)}-\bm{x}^{(0)}\|_{\bm{Q}}}{\sqrt{\mu(\bm{x}^{(0)})+c_{e}}}\leq \frac{1}{10}.\]

This ends the proof of the lemma. 

Because of rounding errors, Lemma B.1 has an extra factor \(3\) compared to the original guarantee in [27, Lemma 14]. To achieve the same guarantee, it suffices to perform \(70\geq\ln(3)/\ln(1/(1-\frac{1}{64}))\) additional centering procedures at most. hence, instead of performing \(200\) centering procedures during the cutting plane method, we perform \(270\) (1.10 of Algorithm 6). We next turn to the numerical stability of the main Algorithm 6.

**Lemma B.2**.: _Suppose that throughout the algorithm, when checking the stopping criterion \(\min_{i\in[m]}s_{i}(\bm{x})<2\epsilon\), the quantities \(s_{i}(\bm{x})\) were computed with accuracy \(\epsilon\). Suppose that at each iteration of Algorithm 6, the leverage scores \(\bm{\psi}(\bm{x}^{(t)})\) are computed up to multiplicative precision \(c_{\Delta}/4\) (1.3), that when a constraint is added, the response of the oracle \(\bm{a}\) (l.7) is stored perfectly but \(b\) (1.8) is computed up to precision \(\Omega(\frac{\epsilon}{\sqrt{n}})\). Further suppose that the centering Algorithm 5 is run with numerical approximations according to the assumptions in Lemma B.1. Then, all guarantees for the original algorithm in [27] hold, up to a factor \(3\) for \(\epsilon\)._

Proof.: We start with the termination criterion. Given the requirement on the computational accuracy, we know that the final output \(\bm{x}\) satisfies \(\min_{i\in[m]}s_{i}(\bm{x})\leq 3\epsilon\). Further, during the algorithm, if it does not stop, then one has \(\min_{i\in[m]}s_{i}(\bm{x})\geq\epsilon\), which is precisely the guarantee of the original algorithm in [27].

We next turn to the computation of the leverage scores in 1.4. In the original algorithm, only a \(c_{\Delta}\)-estimate is computed. Precisely, one computes a vector \(\bm{w}^{(t)}\) such that for all \(i\in[d]\), \(\psi(\bm{x}^{(t)})_{i}\leq w_{i}\leq(1+c_{\Delta})\psi(\bm{x}^{(t)})_{i}\), then deletes a constraint when \(\min_{i\in[m^{(t)}]}w_{i}^{(t)}\leq c_{d}\). In the adapted algorithm, let \(\tilde{\psi}(\bm{x}^{(t)})_{i}\) denote the computed leverage scores for \(i\in[d]\). By assumption, we have

\[(1-c_{\Delta}/4)\psi(\bm{x}^{(t)})_{i}\leq\tilde{\psi}(\bm{x}^{(t)})_{i}\leq( 1+c_{\Delta}/4)\psi(\bm{x}^{(t)})_{i}.\]

Up to re-defining the constant \(c_{d}\) as \((1-c_{\Delta}/4)c_{d}\), \(\tilde{\bm{\psi}}(\bm{x}^{(t)})\) is precisely within the guarantee bounds of the algorithm. For the accuracy on the separation oracle response and the second-term value \(b\), [27] emphasizes that the algorithm always changes constraints by a \(\delta\) amount where \(\delta=\Omega(\frac{\epsilon}{\sqrt{d}})\) so that an inexact separation oracle with accuracy \(\Omega(\frac{\epsilon}{\sqrt{d}})\) suffices. Therefore, storing an \(\Omega(\frac{\epsilon}{\sqrt{d}})\) accuracy of the second term keeps the guarantees of the algorithm. Last, we checked in Lemma B.1 that the centering procedure Algorithm 5 satisfies all the requirements needed in the original proof [27]. 

For our recursive method, we need an efficient cutting-plane method that also provides a proof (certificate) of convergence. This is also provided by [27] that provide a proof that the feasible region has small width in one of the directions \(\bm{a}_{i}\) of the returned polyhedron.

**Lemma B.3**.: _[_27_, Lemma 28]_ _Let \((\mathcal{P},\bm{x},(\lambda_{i})_{i})\) be the output of Algorithm 7. Then, \(\bm{x}\) is feasible, \(\|\bm{x}\|_{2}\leq 3\sqrt{d}\), \(\lambda_{j}\geq 0\) for all \(j\) and \(\sum_{i}\lambda_{i}=1\). Further,_

\[\left\|\sum_{i}\lambda_{i}\bm{a}_{i}\right\|_{2}=\mathcal{O}\left(\epsilon \sqrt{d}\ln\frac{d}{\epsilon}\right),\quad\text{and}\quad\sum_{i}\lambda_{i}( \bm{a}_{i}^{\top}\bm{x}-b_{j})\leq\mathcal{O}\left(d\epsilon\ln\frac{d}{ \epsilon}\right).\]

We are now ready to show that Algorithm 6 can be implemented with efficient memory and also provides a proof of the convergence of the algorithm.

**Input:**\(\epsilon>0\) and a separation oracle \(O:\mathcal{C}_{d}\rightarrow\mathbb{R}^{d}\)

Run Algorithm 6 to obtain a polyhedron \(\mathcal{P}\) and a feasible point \(\bm{x}\)

\(\bm{x}^{\star}=\mathsf{Centering}(\bm{x},64\ln\frac{d}{\epsilon},c_{\Delta})\)

\(\lambda_{i}=\frac{c_{c}+\psi_{i}(\bm{x}^{\star})}{s_{i}(\bm{x}^{\star})}\left( \sum_{j}\frac{c_{c}+\psi_{j}(\bm{x}^{\star})}{s_{j}(\bm{x}^{\star})}\right)^{-1}\) for all \(i\)

**Output:**\((\mathcal{P},\bm{x}^{\star},(\lambda_{i})_{i})\)

**Algorithm 7** Cutting-plane algorithm with certified optimality

**Proposition B.1**.: _Provided that the output of the oracle are vectors discretized to precision \(\text{poly}(\frac{\epsilon}{d})\) and have norm at most \(1\), Algorithm 7 can be implemented with \(\mathcal{O}(d^{2}\ln\frac{d}{\epsilon})\) bits of memory to output a certified optimal point according to Lemma B.3. The algorithm performs \(\mathcal{O}(d\ln\frac{d}{\epsilon})\) calls to the separation oracle and runs in \(\mathcal{O}(d^{1+\omega}\ln^{\mathcal{O}(1)}\frac{d}{\epsilon})\) time._

Proof.: We already checked the numerical stability of Algorithm 6 in Lemma B.2. It remains to check the next steps of the algorithm. The centering procedure is stable again via Lemma B.1. It also suffices to compute the coefficients \(\lambda_{j}\) up to accuracy \(\mathcal{O}(\epsilon/(\sqrt{d})\ln(d/\epsilon))\) to keep the guarantees desired since by construction all vectors \(\bm{a}_{i}\) have norm at most one.

It now remains to show that the algorithm can be implemented with efficient memory. We recall that at any point during the algorithm, the polyhedron \(\mathcal{P}\) has at most \(\mathcal{O}(d)\) constraints [27, Lemma 22]. Hence, since we assumed that each vector \(\bm{a}_{i}\) composing a constraint is discretized to precision \(\text{poly}(\frac{\epsilon}{d})\), we can store the polyhedron constraints with \(\mathcal{O}(d^{2}\ln\frac{d}{\epsilon})\) bits of memory. The second terms \(b\) are computed up to precision \(\Omega(\epsilon/\sqrt{d})\) hence only use \(\mathcal{O}(d\ln\frac{d}{\epsilon})\) bits of memory. The algorithm also keeps the current iterate \(x^{(t)}\) in memory. These are all bounded throughout the memory \(\|x^{(t)}\|_{2}=\mathcal{O}(\sqrt{d})\)[27, Lemma 23], hence only require \(\mathcal{O}(d\ln\frac{d}{\epsilon})\) bits of memory for the desired accuracy.

Next, the distances to the constraints are bounded at any step of the algorithm: \(s_{i}(\bm{x}^{(t)})\leq\mathcal{O}(\sqrt{d})\)[27, Lemma 24], hence computing \(s_{i}(\bm{x}^{(t)})\) to the required accuracy is memory-efficient. Recall that from the termination criterion, except for the last point, any point \(\bm{x}\) during the algorithm satisfies \(s_{i}(\bm{x})\geq\epsilon\) for all constraints \(i\in[m]\). In particular, this bounds the eigenvalues of \(\bm{Q}\) since \(\lambda I\preceq\bm{Q}(x)\preceq(\lambda+m(c_{e}+1)/\epsilon^{2})\bm{I}\). Thus, the matrix is sufficiently well-conditioned to achieve the accuracy guarantees from Lemma B.1 using \(\mathcal{O}(d^{2}\ln\frac{d}{\epsilon})\) memory during matrix inversions (and matrix multiplications). Similarly, for the computation of leverage scores, we use \(\bm{\Psi}(x)=diag(\bm{A}_{x}(\bm{A}_{x}^{\top}\bm{A}_{x}+\lambda\bm{I})^{-1} \bm{A}_{x}^{\top})\), where \(\lambda\bm{I}\preceq\bm{A}_{x}^{\top}\bm{A}_{x}+\lambda\bm{I}\preceq(\lambda+ m\epsilon^{-2})\bm{I}\). This same matrix inversion appears when computing the second term of an added constraint. Overall, all linear algebra operations are well conditioned and implementable with required accuracy with \(\mathcal{O}(d^{2}\ln\frac{d}{\epsilon})\) memory. Using fast matrix multiplication, all these operations can be performed in \(\tilde{\mathcal{O}}(d^{\omega})\) time per iteration of the cutting-plane algorithm since these methods are also known to be numerically stable [15]. Thus, the total time complexity is \(\mathcal{O}(d^{1+\omega}\ln^{O(1)}\frac{d}{\epsilon})\). The oracle-complexity still has optimal \(\mathcal{O}(d\ln\frac{d}{\epsilon})\) oracle-complexity as in the original algorithm. 

Up to changing \(\epsilon\) to \(c\cdot\epsilon/(d\ln\frac{d}{\epsilon})\), the described algorithm finds constraints given by \(\bm{a}_{i}\) and \(b_{i}\), \(i\in[m]\) returned by the normalized separation oracle, coefficients \(\lambda_{i}\), \(i\in[m]\), and a feasible point \(\bm{x}^{\star}\) such that for any vector in the unit cube, \(\bm{z}\in\mathcal{C}_{d}\), one has

\[\min_{i\in[m]}\bm{a}_{i}^{\top}\bm{z}-b_{i}\leq\sum_{i\in[m]}\lambda_{i}(\bm{a }_{i}^{\top}\bm{z}-b_{i})\leq\left(\sum_{i\in[m]}\lambda\bm{a}_{i}\right)^{ \top}(\bm{x}^{\star}-\bm{z})+\sum_{i\in[m]}\lambda_{i}(\bm{a}_{i}^{\top}\bm{x} ^{\star}-b_{i})\leq\epsilon.\]

This effectively replaces Lemma 4.1.

### Merging Algorithm 7 within the recursive algorithm

Algorithms 2 to 4 from the recursive procedure need to be slightly adapted to the new format of the cutting-plane method's output. In particular, the oracles do not take as input polyhedrons (andeventually query their volumetric center as before), but directly take as input an point (which is an approximate volumetric center).

``` Input:\(\delta\), \(\xi\), \(O_{x}:\mathcal{C}_{n}\to\mathbb{R}^{m}\) and \(O_{y}:\mathcal{C}_{n}\to\mathbb{R}^{n}\)
1 Run Algorithm 7 with parameter \(c\cdot\delta/(d\ln\frac{d}{\delta}),\xi\) and \(O_{y}\) to obtain \((\mathcal{P}^{\star},\bm{x}^{\star},\bm{\lambda})\) Store \(\bm{k}^{\star}=(k_{i},i\in[m])\) where \(m=|\mathcal{P}^{\star}|\), and \(\bm{\lambda}^{\star}\leftarrow\mathsf{Discretize}(\bm{\lambda}^{\star},\xi)\) Initialize \(\mathcal{P}_{0}:=\{(-1,\bm{e}_{i},-1),(-1-\bm{e}_{i},-1),\;i\in[d]\}\), \(\bm{x}^{(0)}=\bm{0}\) and let \(\bm{u}=\bm{0}\in\mathbb{R}^{m}\)for\(t=0,1,\ldots,\max_{i}k_{i}\)do
2if\(t=k_{i}^{\star}\) for some \(i\in[m]\)then
3\(\bm{g}_{x}=O_{x}(\bm{x}^{(t)})\)\(\bm{u}\leftarrow\mathsf{Discretize}_{m}(\bm{u}+\lambda_{i}^{\star}\bm{g}_{x},\xi)\) Update \(\mathcal{P}_{t}\) to get \(\mathcal{P}_{t+1}\), and \(\bm{x}^{(t)}\) to get \(\bm{x}^{(t+1)}\) as in Algorithm 6
4 end for return\(\bm{u}\) ```

**Algorithm 8**\(\mathsf{ApproSeq}\mathsf{SeparationVector}_{\delta,\xi}(O_{x},O_{y})\)

``` Input:\(\delta\), \(\xi\), \(1\leq j\leq i\leq p\), \(\bm{x}^{(r)}\in\mathcal{C}_{k_{r}}\) for \(r\in[i]\), \(O_{S}:\mathcal{C}_{d}\to\mathbb{R}^{d}\) if\(i=p\)then
2\((\bm{g}_{1},\ldots,\bm{g}_{p})=O_{S}(\bm{x}_{1},\ldots,\bm{x}_{p})\)return\(\mathsf{Discretize}_{k_{j}}(\bm{g}_{j},\xi)\)
3 end if
4
5 Define \(O_{x}:\mathcal{C}_{k_{i+1}}\to\mathbb{R}^{k_{j}}\) as \(\mathsf{ApproxOracle}_{\delta,\xi,\mathcal{O}_{f}}(i+1,j,\bm{x}^{(1)},\ldots, \bm{x}^{(i)},\cdot)\) Define \(O_{y}:\mathcal{C}_{k_{i+1}}\to\mathbb{R}^{k_{i+1}}\) as \(\mathsf{ApproxOracle}_{\delta,\xi,\mathcal{O}_{f}}(i+1,i+1,\bm{x}^{(1)},\ldots, \bm{x}^{(i)},\cdot)\) return\(\mathsf{ApproxSeparationVector}_{\delta,\xi}(O_{x},O_{y})\) ```

**Algorithm 9**\(\mathsf{ApproxOracle}_{\delta,\xi,O_{S}}(i,j,\bm{x}^{(1)},\ldots,\bm{x}^{(i)})\)

The same proof as for Algorithm 4 shows that Algorithm 10 run with the parameters in Theorem A.1 also outputs a successful vector using the same oracle-complexity. We only need to analyze the memory usage in more detail.

Proof of Theorem 3.2.: As mentioned above, we will check that Algorithm 10 with the same parameters \(\delta=\frac{\epsilon}{4d}\) and \(\xi=\frac{\sigma_{\mathrm{min}}\epsilon}{32d^{j/2}}\) as in Theorem A.1 satisfies the desired requirements. We have already checked its correctness and oracle-complexity. Using the same arguments, the computational complexity is of the form \(\mathcal{O}(\mathcal{O}(\mathsf{ComplexityCuttingPlanes})^{p})\) where \(\mathsf{ComplexityCuttingPlanes}\) is the computational complexity of the cutting-plane method used, i.e., here of Algorithm 7. Hence, the computational complexity is \(\mathcal{O}((C(d/p)^{1+\omega}\ln^{\mathcal{O}(1)}\frac{d}{\epsilon})^{p})\) for some universal constant \(C\geq 2\). We now turn to the memory. In addition to the memory of Algorithm 4, described in Table 1, we need

1. a placement for all \(i\in[p]\) for the current iterate \(\bm{x}^{(i)}\): \(\mathcal{O}(k_{i}\ln\frac{1}{\xi})\) bits,
2. a placement for computations, that is shared for all layers (used to compute leverage scores, centering procedures, etc. By Proposition B.1, since the vectors are always discretized to precision \(\xi\), this requires \(\mathcal{O}(\max_{i\in[p]}k_{i}^{2}\ln\frac{d}{\epsilon})\) bits,
3. the placement \(Q\) to perform queries is the concatenation of the placements \((\bm{x}^{(1)},\ldots,\bm{x}^{(p)})\): no additional bits needed.
4. a placement \(N\) to store the precision needed for the oracle responses: \(\mathcal{O}(\ln\frac{1}{\xi})\) bits5. a placement \(R\) to receive the oracle responses: \(\mathcal{O}(d\ln\frac{1}{\xi})\) bits.

The new memory structure is summarized in Table 2.

With the same arguments as in the original proof of Theorem A.1, this memory is sufficient to run the algorithm and perform computations, thanks to the computation placement. The total number of bits used throughout the algorithm remains the same, \(\mathcal{O}(\frac{d^{2}}{p}\ln\frac{d}{\epsilon})\). This ends the proof of the theorem. 

## Appendix C Improved oracle-complexity/memory lower-bound trade-offs

We recall the three oracle-complexity/memory lower-bound trade-offs known in the literature.

1. First, [31] showed that any (including randomized) algorithm for convex optimization uses \(d^{1.25-\delta}\) memory or makes \(\tilde{\Omega}(d^{1+4\delta/3})\) queries.
2. Then, [5] showed that any deterministic algorithm for convex optimization uses \(d^{2-\delta}\) memory or makes \(\tilde{\Omega}(d^{1+\delta/3})\) queries.
3. Last, [5] show that any deterministic algorithm for the feasibility problem uses \(d^{2-\delta}\) memory or makes \(\tilde{\Omega}(d^{1+\delta})\) queries.

Although these papers mainly focused on the regime \(\epsilon=1/\text{poly}(d)\) and as a result \(\ln\frac{1}{\epsilon}=\mathcal{O}(\ln d)\), neither of these lower bounds have an explicit dependence in \(\epsilon\). This can lead to sub-optimal lower bounds whenever \(\ln\frac{1}{\epsilon}\gg\ln d\). Furthermore, in the exponential regime \(\epsilon\leq\frac{1}{2^{\mathcal{O}(d)}}\), these results do not effectively give useful lower bounds. Indeed, in this regime, one has \(d^{2}=\mathcal{O}(d\ln\frac{1}{\epsilon})\) and as a result, the lower bounds provided are weaker than the classical \(\Omega(d\ln\frac{1}{\epsilon})\) lower bounds for oracle-complexity [34] and memory [52]. In particular, in this exponential regime, these results fail to show that there is any trade-off between oracle-complexity and memory.

In this section, we aim to explicit the dependence in \(\epsilon\) of these lower-bounds. We show with simple modifications and additional arguments that one can roughly multiply these oracle-complexity and memory lower bounds by a factor \(\ln\frac{1}{\epsilon}\) each. We split the proofs in two. First we give arguments to improve the memory dependence by a factor \(\ln\frac{1}{\epsilon}\), which is achieved by modifying the sampling of the rows of the matrix \(\bm{A}\) defining a wall term common to the functions considered in the lower bound proofs [31, 5]. Then we show how to improve the oracle-complexity dependence by an additional \(\ln\frac{1}{\epsilon}/\ln d\) factor, via a standard rescaling argument.

### Improving the memory lower bound

We start with some concentration results on random vectors. [31] gave the following result for random vectors in the hypercube.

\begin{table}
\begin{tabular}{c|c|c|c|c|} \(i\) & 1 & \(\dots\) & \(p\) & Oracle response & Precision \\ \cline{2-4} \(j\) & \(j^{(1)}\) & & \(j^{(p)}\) & \(R=(R_{1},\dots,R_{p})\) & \(N\) \\ \cline{2-4} Iteration index & \(t^{(1)}\) & & \(t^{(p)}\) & \\ \cline{2-4} Polyhedron & \(\mathcal{P}^{(1)}=\left(\begin{array}{c}k_{1},\bm{a}_{1},b_{1}\\ k_{2},\bm{a}_{2},b_{2}\\ \dots\\ k_{m},\bm{a}_{m},b_{m}\end{array}\right)\) & & \(\mathcal{P}^{(p)}\) & \\ \cline{2-4} Current iterate & \(\bm{x}^{(1)}\) & & \(\bm{x}^{(p)}\) & Computation \\ \cline{2-4} Computed dual variables & \((\bm{k}^{*},\bm{\lambda}^{*})=\left(\begin{array}{c}k_{1}^{2},\lambda_{1}^ {*}\\ k_{2}^{2},\lambda_{2}^{*}\\ \dots\end{array}\right)\) & & \((\bm{k}^{*}(p),\bm{\lambda}^{*}(p))\) \\ \cline{2-4} Working separation vector & \(\bm{u}^{(1)}\) & & \(\bm{u}^{(p)}\) & \\ \hline \end{tabular}
\end{table}
Table 2: Memory structure for Algorithm 10

**Lemma C.1** ([31]).: _Let \(\bm{h}\sim\mathcal{U}(\{\pm 1\}^{d})\). Then, for any \(t\in(0,1/2]\) and any matrix \(\bm{Z}=[\bm{z}_{1},\ldots,\bm{z}_{k}]\in\mathbb{R}^{d\times k}\) with orthonormal columns,_

\[\mathbb{P}(\|\bm{Z}^{\top}\bm{h}\|_{\infty}\leq t)\leq 2^{-c_{H}k}.\]

Instead, we will need a similar concentration result for random unit vectors in the unit sphere.

**Lemma C.2**.: _Let \(k\leq d\) and \(\bm{x}_{1},\ldots,\bm{x}_{k}\) be \(k\) orthonormal vectors, and \(\zeta\leq 1\)._

\[\mathbb{P}_{\bm{y}\sim\mathcal{U}(S^{d-1})}\left(|\bm{x}_{i}^{\top}\bm{y}| \leq\frac{\zeta}{\sqrt{d}},\,i\in[k]\right)\leq\left(\frac{2}{\sqrt{\pi}} \zeta\right)^{k}\leq(\sqrt{2}\zeta)^{k}.\]

Proof.: First, by isometry, we can suppose that the orthonormal vectors are simply \(\bm{e}_{1},\ldots,\bm{e}_{k}\). We now prove the result by induction on \(d\). For \(d=1\), the result holds directly. Fix \(d\geq 2\), and \(1\leq k<d\). Then, if \(S_{n}\) is the surface area of \(S^{n}\) the \(n\)-dimensional sphere, then

\[\mathbb{P}\left(|y_{1}|\leq\frac{\zeta}{\sqrt{d}}\right)\leq\frac{S_{d-2}}{S_ {d-1}}\frac{2\zeta}{\sqrt{d}}=\frac{2\zeta}{\sqrt{\pi d}}\frac{\Gamma(d/2)}{ \Gamma(d/2-1/2)}\leq\frac{2}{\sqrt{\pi}}\zeta.\] (7)

Conditionally on the value of \(y_{1}\), the vector \((y_{2},\ldots,y_{d})\) follows a uniform distribution on the \((d-2)\)-sphere of radius \(\sqrt{1-y_{1}^{2}}\). Then,

\[\mathbb{P}\left(|y_{i}|\leq\frac{\zeta}{\sqrt{d}},\,2\leq i\leq k\mid y_{1} \right)=\mathbb{P}_{\bm{z}\sim\mathcal{U}(S^{d-2})}\left(|z_{i}|\leq\frac{ \zeta}{\sqrt{d(1-y_{1}^{2})}},\,2\leq i\leq k\right)\]

Now recall that since \(|x_{1}|\leq 1/\sqrt{d}\), we have \(d(1-x_{1}^{2})\geq d-1\). Therefore, using the induction,

\[\mathbb{P}\left(|y_{i}|\leq\frac{\zeta}{\sqrt{d}},\,2\leq i\leq k\mid y_{1} \right)\leq\mathbb{P}_{\bm{z}\sim\mathcal{U}(S^{d-2})}\left(|z_{i}|\leq\frac{ \zeta}{\sqrt{d-1}},\,2\leq i\leq k\right)\leq\left(\frac{2\zeta}{\sqrt{\pi}} \right)^{k-1}.\]

Combining this equation with Eq (7) ends the proof. 

We next use the following lemma to partition the unit sphere \(S^{d-1}\).

**Lemma C.3** ([19] Lemma 21).: _For any \(0<\delta<\pi/2\), the sphere \(S^{d-1}\) can be partitioned into \(N(\delta)=(\mathcal{O}(1)/\delta)^{d}\) equal volume cells, each of diameter at most \(\delta\)._

Following the notation from [5], we denote by \(\mathcal{V}_{\delta}=\{V_{i}(\delta),i\in[N(\delta)]\}\) the corresponding partition, and consider a set of representatives \(\mathcal{D}_{\delta}=\{\bm{b}_{i}(\delta),i\in[N(\delta)]\}\subset S^{d-1}\) such that for all \(i\in[N(\delta)]\), \(\bm{b}_{i}(\delta)\in V_{i}(\delta)\). With these notations we can define the discretization function \(\phi_{\delta}\) as follows

\[\phi_{\delta}(\bm{x})=\bm{b}_{i}(\delta),\quad\bm{x}\in V_{i}(\delta).\]

We then denote by \(\mathcal{U}_{\delta}\) the distribution of \(\phi_{\delta}(\bm{z})\) where \(\bm{z}\sim\mathcal{U}(S^{d-1})\) is sampled uniformly on the sphere. Note that because the cells of \(\mathcal{V}_{\delta}\) have equal volume, \(\mathcal{U}_{\delta}\) is simply the uniform distribution on the discretization \(\mathcal{D}_{\delta}\).

We are now ready to give the modifications necessary to the proofs, to include a factor \(\ln\frac{1}{\epsilon}\) for the necessary memory. For their lower bounds, [31] exhibit a distribution of convex functions that are hard to optimize. Building upon their work [5] construct classes of convex functions that are hard to optimize, but that also depend adaptively on the considered optimization algorithm. For both, the functions considered a barrier term of the form \(\|\bm{Ax}\|_{\infty}\), where \(\bm{A}\) is a matrix of \(\approx d/2\) rows that are independently drawn as uniform on the hypercube \(\mathcal{U}(\{\pm 1\}^{d})\). The argument shows that memorizing \(\bm{A}\) is necessary to a certain extent. As a result, the lower bounds can only apply for a memory of at most \(\mathcal{O}(d^{2})\) bits, which is sufficient to memorize such a binary matrix. Instead, we draw rows independently according to the distribution \(\mathcal{U}_{\delta}\), where \(\delta\approx\epsilon\). We explicit the corresponding adaptations for each known trade-off. We start with the lower bounds from [5] for ease of exposition; although these build upon those of [31], their parametrization makes the adaptation more straightforward.

#### c.1.1 Lower bound of [5] for convex optimization and deterministic algorithms

For this lower bound, we use the exact same form of functions as they introduced,

\[\max\left\{\|\bm{A}\bm{x}\|_{\infty}-\eta,\eta\bm{v}_{0}^{\top}\bm{x},\eta\left( \max_{p\leq p_{max}\leq l\leq p}\bm{v}_{p,l}^{\top}\bm{x}-p\gamma_{1}-l\gamma_{2 }\right)\right\},\]

with the difference that rows of \(\bm{A}\) are take i.i.d. distributed according to \(\mathcal{U}_{\delta^{\prime}}\) instead of \(\mathcal{U}(\{\pm 1\}^{d})\). As a remark, they use \(n=\lceil d/4\rceil\) rows for \(\bm{A}\). Except for \(\eta\), we keep all parameters \(\gamma_{1}\), \(\gamma_{2}\), etc as in the original proof, and we will take \(\delta^{\prime}=\epsilon\) and \(\eta=2\sqrt{d}\epsilon\). The reason why we introduced \(\delta^{\prime}\) instead of \(\delta\) is that the original construction also needs the discretization \(\phi_{\delta}\). This is used during the optimization procedure which constructs adaptively this class of functions, and only needs \(\delta=\text{poly}(1/d)\) instead of \(\delta\) of order \(\epsilon\).

**Theorem C.1**.: _For \(\epsilon\leq 1/(2d^{4.5})\) and any \(\delta\in[0,1]\), a deterministic first-order algorithm guaranteed to minimize \(1\)-Lipschitz convex functions over the unit ball with \(\epsilon\) accuracy uses at least \(d^{2-\delta}\ln\frac{1}{\epsilon}\) bits of memory or makes \(\tilde{\Omega}(d^{1+\delta/3})\) queries._

With the changes defined above, we can easily check that all results from [5] which reduce convex optimization to the optimization procedure, then the optimization procedure to their Orthogonal Vector Game with Hints (OVGH) [5, Game 2], are not affected by our changes. The only modifications to perform are to the proof of query lower bound for the OVGH [5, Proposition 14]. We emphasize that the distribution of \(\bm{A}\) is changed in the optimization procedure but also in OVGH as a result.

**Proposition C.2**.: _Let \(k\geq 20\frac{M+3d\log(2d)+1}{n\log_{2}(\sqrt{2}(\zeta+\delta^{\prime}\sqrt{ d}))^{-1}}\). And let \(0<\alpha,\beta\leq 1\) such that \(\alpha(\sqrt{d}/\beta)^{5/4}\leq\zeta/\sqrt{d}\) where \(\zeta\leq 1\). If the Player wins the adapted OVGH with probability at least \(1/2\), then \(m\geq\frac{1}{8}(1+\frac{30\log_{2}d}{\log_{2}(\sqrt{2}(\zeta+\delta^{\prime} \sqrt{d}))^{-1}})^{-1}d\)._

Proof.: We use the same proof and only highlight the modifications. The proof is unchanged until the step when the concentration result Lemma C.1 is used. Instead, we use Lemma C.2. With the same notations as in the original proof, we constructed \(\lceil k/5\rceil\) orthonormal vectors \(\bm{Z}=[\bm{z}_{1},\ldots,\bm{z}\lceil k/5\rceil]\) such that all rows \(\bm{a}\) of \(\bm{A}^{\prime}\) (which is \(\bm{A}\) up to some observed and unimportant rows) one has

\[\|\bm{Z}^{\top}\bm{a}\|_{\infty}\leq\frac{\zeta}{\sqrt{d}}.\]

Next, by Lemma C.2, we have

\[\left|\left\{\bm{a}\in\mathcal{D}_{\delta^{\prime}}:\|\bm{Z}^{ \top}\bm{a}\|_{\infty}\leq\frac{\zeta}{\sqrt{d}}\right\}\right| \leq|\mathcal{D}_{\delta^{\prime}}|\cdot\mathbb{P}_{\bm{a}\sim \mathcal{U}_{\delta^{\prime}}}\left(\|\bm{Z}^{\top}\bm{a}\|_{\infty}\leq\frac {\zeta}{\sqrt{d}}\right)\] \[\leq|\mathcal{D}_{\delta^{\prime}}|\cdot\mathbb{P}_{\bm{z}\sim \mathcal{U}(S^{d-1})}\left(\|\bm{Z}^{\top}\bm{z}\|_{\infty}\leq\frac{\zeta}{ \sqrt{d}}+\delta^{\prime}\right)\] \[\leq|\mathcal{D}_{\delta^{\prime}}|\cdot\left(\sqrt{2}(\zeta+ \delta^{\prime}\sqrt{d})\right)^{\lceil k/5\rceil}.\]

Hence, using the same arguments as in the original proof, we obtain

\[H(\bm{A}^{\prime}\mid\bm{Y})\leq(n-m)\left(\log_{2}|\mathcal{D}_{\delta^{ \prime}}|+\mathbb{P}(\mathcal{E})\cdot\frac{k}{5}\log_{2}\left(\sqrt{2}(\zeta+ \delta^{\prime}\sqrt{d})\right)\right),\]

where \(\mathcal{E}\) is the event when the algorithm succeeds at the OVGH game. In the next step, we need to bound \(H(\bm{A}\mid\bm{V})-H(\bm{G},\bm{j},\bm{c})\) where \(\bm{V}\) stores hints received throughout the game, \(\bm{G}\) stores observed rows of \(\bm{A}\) during the game, and \(\bm{j},\bm{c}\) are auxiliary variables. The latter can be treated as in the original proof. We obtain

\[H(\bm{A}\mid\bm{V})-H(\bm{G},\bm{j},\bm{c}) \geq H(\bm{A})-H(\bm{G})-I(\bm{A};\bm{V})-3m\log_{2}(2d)\] \[\geq(n-m)\log_{2}|\mathcal{D}_{\delta^{\prime}}|-3m\log_{2}(2d)-I( \bm{A},\bm{V}).\]

Now the same arguments as in the original proof show that we still have \(I(\bm{A},\bm{V})\leq 3km\log_{2}d+1\), and that as a result, if \(M\) is the number of bits stored in memory,

\[M\geq\frac{k}{10}\log_{2}\left(\frac{1}{\sqrt{2}(\zeta+\delta^{\prime}\sqrt{d})} \right)(n-m)-3km\log_{2}d-1-3d\log_{2}(2d).\]

Then, with the same arguments as in the original proof, we can conclude.

We are now ready to prove Theorem C.1. With the parameter \(k=\lceil 20\frac{M+3d\log(2d)+1}{n\log_{2}(\sqrt{2}(\epsilon d^{4}/2+\delta^{ \prime}\sqrt{d}))^{-1}}\rceil\) and the same arguments, we show that an algorithm solving the convex optimization up to precision \(\eta/(2\sqrt{d})=\epsilon\) yields an algorithm solving the OVGH where the parameters \(\alpha=\frac{2\eta}{\gamma_{1}}\) and \(\beta=\frac{\gamma_{2}}{4}\) satisfy

\[\alpha\left(\frac{\sqrt{d}}{\beta}\right)^{5/4}\leq\frac{\eta d^{3}}{4}=\frac{ d^{3.5}\epsilon}{2}.\]

We can then apply Proposition C.2 with \(\zeta=d^{4}\epsilon/2\). Hence, if \(Q\) is the maximum number of queries of the convex optimization algorithm, we obtain

\[\lceil Q/p_{max}\rceil+1\geq\frac{1}{8}\left(1+\frac{30\log_{2}d}{\log_{2} \frac{1}{d^{4}\epsilon}-1/2}\right)^{-1}d\geq\frac{d}{8\cdot 61},\]

where in the last inequality we used \(\epsilon\leq 1/(2d^{4.5})\). As a result, with the same arguments, we obtain

\[Q=\Omega\left(\frac{d^{5/3}\ln^{1/3}\frac{1}{\epsilon}}{(M+\ln d)^{1/3}\ln^{2 /3}d}\right).\]

This ends the proof of Theorem C.1.

#### c.1.2 Lower bound of [5] for feasibility problems and deterministic algorithms

We improve the memory dependence by showing the following result.

**Theorem C.3**.: _For \(\epsilon=1/(48d^{3})\) and any \(\delta\in[0,1]\), a deterministic algorithm guaranteed to solve the feasibility problem over the unit ball with \(\epsilon\) accuracy uses at least \(d^{2-\delta}\ln\frac{1}{\epsilon}\) bits of memory or makes at least \(\tilde{\Omega}(d^{1+\delta})\) queries._

We use the exact same class of feasibility problems and only change the parameter \(\eta_{0}\) which constrained successful points to satisfy \(\|\bm{A}\bm{x}\|_{\infty}\leq\eta_{0}\), as well as the rows of \(\bm{A}\) that are sampled i.i.d. from \(\mathcal{U}_{\delta}\). The other parameter \(\eta_{1}=1/(2\sqrt{d})\) is unchanged. We also take \(\delta^{\prime}=\epsilon\). Because the rows of \(\bm{A}\) are already normalized, we can take \(\eta_{0}=\epsilon\) directly. Then, the same proof as in [5] shows that if an algorithm solves feasibility problems with accuracy \(\epsilon\), there is an algorithm for OVGH for parameters \(\alpha=\eta/\eta_{1}\) and \(\beta=\eta_{1}/2\). Then, we have \(\alpha(\sqrt{d}/\beta)^{5/4}\leq 12d^{2}\eta_{0}\) and we can apply Proposition C.2 with \(\zeta=12d^{2.5}\eta_{0}=12d^{2.5}\epsilon\). Similar computations as above then show that \(m\geq d/(8\cdot 61)\), with \(k=\Theta(\frac{M+\ln d}{d\ln\frac{1}{\epsilon}})\), so that the query lower bound finally becomes

\[Q\geq\Omega\left(\frac{d^{3}\ln\frac{1}{\epsilon}}{(M+\ln d)\ln^{2}d}\right).\]

**Remark C.1**.: _The more careful analysis--involving the discretization \(\mathcal{D}_{\delta}\) of the unit sphere at scale \(\delta\) instead of the hypercube \(\{\pm 1\}^{d}\)--allowed to add a \(\ln\frac{1}{\epsilon}\) factor to the final query lower bound but also an additional \(\ln d\) factor for both convex-optimization and feasibility-problem results. Indeed, the improved Proposition C.2 shows that the OVGH with adequate parameters requires \(\mathcal{O}(d)\) queries, instead of \(\mathcal{O}(d/\ln d)\) in [5, Proposition 14]. At a high level, each hint queried brings information \(\mathcal{O}(d\ln d)\) but memorizing a binary matrix \(\bm{A}\in\{\pm 1\}^{\lceil d/4\rceil\times d}\) only requires \(d^{2}\) bits of memory: hence the query lower bound is limited to \(\mathcal{O}(d/\ln d)\). Instead, memorizing the matrix \(\bm{A}\) where each row lies in \(\mathcal{D}_{\delta}\) requires \(\Theta(d^{2}\ln\frac{1}{\epsilon})\) memory, hence querying \(d\) hints (total information \(\mathcal{O}(d^{2}\ln d)\)) is not prohibitive for the lower bound._

#### c.1.3 Lower bound of [31] for convex optimization and randomized algorithms

We aim to improve the result to obtain the following.

**Theorem C.4**.: _For \(\epsilon\leq 1/d^{4}\) and any \(\delta\in[0,1]\), any (potentially randomized) algorithm guaranteed to minimize \(1\)-Lipschitz convex functions over the unit ball with \(\epsilon\) accuracy uses at least \(d^{1.25-\delta}\ln\frac{1}{\epsilon}\) bits of memory or makes \(\tilde{\Omega}(d^{1+4\delta/3})\) queries._The distribution considered in [31] is given by the functions

\[\frac{1}{d^{6}}\max\left\{d^{5}\|\bm{A}\bm{x}\|_{\infty}-1,\max_{i\in[N]}(\bm{v}_ {i}^{\top}\bm{x}-i\gamma)\right\},\]

where \(N\leq d\) is a parameter, \(\bm{A}\) has \(\lfloor d/2\rfloor\) rows drawn i.i.d. from \(\mathcal{U}(\{\pm 1\}^{d})\), and the vectors \(\bm{v}_{i}\) are drawn i.i.d. from the rescaled hypercube \(\bm{v}_{i}\sim\mathcal{U}(d^{-1/2}\{\pm 1\}^{d})\). We adapt the class of functions by simply changing pre-factors as follows

\[\mu\max\left\{\frac{1}{\mu}\|\bm{A}\bm{x}\|_{\infty}-1,\max_{i\in[N]}(\bm{v}_ {i}^{\top}\bm{x}-i\gamma)\right\},\] (8)

where \(\bm{A}\) has the same number of rows but they are draw i.i.d. from \(\mathcal{U}_{\delta}\), and \(\delta,\mu>0\) are parameters to specify. We use the notation \(\mu\) instead of \(\eta\) as in the previous sections because [31] already use a parameter \(\eta\) which in our context can be interpreted as \(\eta=1/(\mu\sqrt{d})\). We choose the parameters \(\mu=16\sqrt{d}\epsilon\) and \(\delta^{\prime}=\epsilon\).

Again, as for the previous sections, the original proof can be directly used to show that if an algorithm is guaranteed to find a \(\frac{\mu}{16\sqrt{N}}\left(\geq\epsilon\right)\)-suboptimal point for the above function class, there is an algorithm that wins at their Orthogonal Vector Game (OVG) [31, Game 1], with the only difference that the parameter \(d^{-4}\) (1.8 of OVG) is replaced by \(\sqrt{d}\mu\). OVG requires the output to be _robustly-independent_ (defined in [31]) and effectively corresponds to \(\beta=1/d^{2}\) in OVGH. As a result, there is a successful algorithm for the OVGH with parameters \(\alpha=\sqrt{d}\mu\) and \(\beta=1/d^{2}\) and that even completely ignores the hints. Hence, we can now directly use Proposition C.2 with \(\zeta=d^{1+25/16}\mu\) (from the assumption \(\epsilon\leq d^{-4}\) we have \(\zeta\leq 1/\sqrt{d}\)). This shows that with the adequate choice of \(k=\Theta(\frac{M+d\ln d}{d\ln\frac{1}{\epsilon}})\), the query lower bound is \(\Omega(d)\).

Putting things together, a potentially randomized algorithm for convex optimization that uses \(M\) memory makes at least the following number of queries

\[Q\geq\Omega\left(\frac{Nd}{k}\right)=\Omega\left(\frac{d^{4/3}}{\ln^{1/3}d} \left(\frac{d\ln\frac{1}{\epsilon}}{M+d\ln d}\right)^{4/3}\right).\]

### Proof sketch for improving the query-complexity lower bound

We now turn to improving the query-complexity lower bound by a factor \(\frac{\ln\frac{1}{\epsilon}}{\ln d}\). At the high level, the idea is to replicate these constructed "difficult" class of functions at \(\frac{\ln\frac{1}{\epsilon}}{\ln d}\) different scales or levels, similarly to the manner that the historical \(\Omega(d\ln\frac{1}{\epsilon})\) lower bound is obtained for convex optimization [34]. This argument is relatively standard and we only give details in the context of improving the bound from [31] for randomized algorithms in convex optimization for conciseness. This result uses a simpler class of functions, which greatly eases the exposition. We first present the construction with 2 levels, then present the generalization to \(p=\Theta(\frac{\ln\frac{1}{\epsilon}}{\ln d})\) levels. For convenience, we write

\[Q(\epsilon;M,d)=\Omega\left(\frac{d^{4/3}}{\ln^{1/3}d}\left(\frac{d\ln\frac{1} {\epsilon}}{M+d\ln d}\right)^{4/3}\right).\]

This is the query lower bound given in Theorem C.5 for convex optimization algorithms with memory \(M\) that optimize the defined class of functions (Eq (8)) to accuracy \(\epsilon\).

c.2.1 Construction of a bi-level class of functions \(F_{\bm{A},\bm{v}_{1},\bm{v}_{2}}\) to optimize

In the lower-bound proof, [31] introduce the point

\[\bar{\bm{x}}=-\frac{1}{2\sqrt{N}}\sum_{i\in[N]}P_{\bm{A}^{\perp}}(\bm{v}_{i}),\]

where \(P_{\bm{A}^{\perp}}\) is the projection onto the orthogonal space to the rows of \(\bm{A}\). They show that with failure probability at most \(2/d\), \(\bar{\bm{x}}\) has good function value

\[F_{\bm{A},\bm{v}}(\bar{\bm{x}}):=\mu\max\left\{\frac{1}{\mu}\|\bm{A}\bar{\bm{x }}\|_{\infty}-1,\max_{i\in[N]}(\bm{v}_{i}^{\top}\bar{\bm{x}}-i\gamma)\right\} \leq-\frac{\mu}{8\sqrt{N}}.\]This is shown in [31, Lemma 25]. On the other hand, from Theorem C.4, during the first

\[Q_{1}=Q(\epsilon;M,d)\]

queries of any algorithm, with probability at least \(1/3\), all queries are at least \(\mu/(16\sqrt{N})\)-suboptimal compared to \(\bar{\bm{x}}\) in function value [31, Theorem 28, Lemma 14 and Theorem 16]. Precisely, if \(F_{\bm{A},\bm{v}}\) is the sampled function to optimize, with probability at least \(1/3\),

\[F_{\bm{A},\bm{v}}(\bm{x}_{t})\geq F_{\bm{A},\bm{v}}(\bar{\bm{x}})+\frac{\mu}{16 \sqrt{N}}\geq F_{\bm{A},\bm{v}}(\bar{\bm{x}})+\frac{\mu}{16\sqrt{d}},\quad \forall t\leq Q_{1}.\]

As a result, we can replicate the term \(\max_{i\in[N]}(\bm{v}_{i}^{\top}\bm{x}-i\gamma)\) at a smaller scale within the ball \(B_{d}(\bar{\bm{x}},1/(16\sqrt{d}))\). For convenience, we introduce \(\xi_{2}=1/(16\sqrt{d})\) which will be the scale of the duplicate function. We separate the wall term \(\|\bm{A}\bm{x}\|_{\infty}-\mu\) for convenience. Hence, we define

\[G_{\bm{A},\bm{v}_{1}}(\bm{x}) :=\mu\max_{i\in[N]}\left(\bm{v}_{1,i}^{\top}\bm{x}-i\gamma\right)\] \[G_{\bm{A},\bm{v}_{1},\bm{v}_{2}}(\bm{x}) :=\max\{G_{\bm{A},\bm{v}^{(1)}}(\bm{x}),G_{\bm{A},\bm{v}_{1}}( \bar{\bm{x}})+\frac{\mu\xi_{2}}{3}.\] \[\max\left\{1+\|\bm{x}-\bar{\bm{x}}\|_{2},1+\frac{\xi_{2}}{6}+ \frac{\xi_{2}}{18}\max_{i\in[N]}\left(\bm{v}_{2,i}^{\top}\left(\frac{\bm{x}- \bar{\bm{x}}}{\xi_{2}/9}\right)-i\gamma\right)\right\}\right\}\]

An illustration of the construction is given in Fig. 4. The resulting optimization functions are given by adding the wall term:

\[F_{\bm{A},\bm{v}_{1}}(\bm{x}) =\max\left\{\|\bm{A}\bm{x}\|_{\infty}-\mu,G_{\bm{A},\bm{v}_{1}}( \bm{x})\right\}\] \[F_{\bm{A},\bm{v}_{1},\bm{v}_{2}}(\bm{x}) =\max\left\{\|\bm{A}\bm{x}\|_{\infty}-\mu,G_{\bm{A},\bm{v}_{1}, \bm{v}_{2}}(\bm{x})\right\}\]

We first explain the choice of parameters. First observe that since \(\|\bm{A}\bar{\bm{x}}\|=0\), we have \(G_{\bm{A},\bm{v}_{1}}(\bar{\bm{x}})=F_{\bm{A},\bm{v}_{1}}(\bar{\bm{x}})\). We can then check that for all \(\bm{x}\in B_{d}(0,1)\),

\[G_{\bm{A},\bm{v}_{1},\bm{v}_{2}}(\bm{x})\leq\max\left\{G_{\bm{A},\bm{v}_{1}}( \bm{x}),G_{\bm{A},\bm{v}_{1}}(\bar{\bm{x}})+\frac{2}{3}\mu\xi_{2}\right\}.\] (9)

Further, for any \(\bm{x}\in B_{d}(\bar{\bm{x}},\xi_{2}/3)\), since \(F_{\bm{A},\bm{v}_{1}}\) is \(1\)-Lipschitz, we can easily check that

\[G_{\bm{A},\bm{v}_{1},\bm{v}_{2}}(\bm{x})-G_{\bm{A},\bm{v}_{1}}( \bar{\bm{x}})\] \[\qquad=\frac{\mu\xi_{2}}{3}\max\left\{1+\|\bm{x}-\bar{\bm{x}}\|_ {2},1+\frac{\xi_{2}}{6}+\frac{\xi_{2}}{18}\max_{i\in[N]}\left(\bm{v}_{2,i}^{ \top}\left(\frac{\bm{x}-\bar{\bm{x}}}{\xi_{2}/9}\right)-i\gamma\right)\right\} \leq\frac{2}{3}\mu\xi_{2}.\]

Thus, \(G_{\bm{A},\bm{v}_{1},\bm{v}_{2}}(\bm{x})\) does not coincide with \(G_{\bm{A},\bm{v}_{1}}(\bm{x})\) on \(B_{d}(\bar{\bm{x}},\xi_{2}/3)\). Then, the \(\|\bm{x}-\bar{\bm{x}}\|_{2}\) term ensures that any minimizer of \(G_{\bm{A},\bm{v}_{1},\bm{v}_{2}}\) is contained within the closed ball \(B_{d}(\bar{\bm{x}},\xi_{2}/3)\). Also, to obtain a \(\mu\xi_{2}/3\)-suboptimal solution of \(F_{\bm{A},\bm{v}_{1},\bm{v}_{2}}\), the algorithm needs to find what would be a \(\mu\xi_{2}\)-suboptimal solution of \(F_{\bm{A},\bm{v}_{1}}\), while receiving the same response as when optimizing the latter.

Figure 4: Representation of the procedure to rescale the optimization function.

Next, for any \(\bm{x}\in B_{d}(\bar{\bm{x}},\xi_{2}/9)\), the term \(\max_{i\in[N]}\left(\bm{v}_{2,i}^{\top}\left(\frac{\bm{x}-\bar{\bm{x}}}{\xi_{2}/9 }\right)-i\gamma\right)\) lies in \([-1,1]\). Hence, we can check that for \(\bm{x}\in B_{d}(\bar{\bm{x}},\xi_{2}/9)\),

\[G_{\bm{A},\bm{v}_{1},\bm{v}_{2}}(\bm{x})=G_{\bm{A},\bm{v}_{1}}(\bar{\bm{x}})+ \frac{\mu\xi_{2}}{3}+\frac{\mu\xi_{2}^{2}}{18}+\frac{\mu\xi_{2}^{2}}{54}\max_ {i\in[N]}\left(\bm{v}_{2,i}^{\top}\left(\frac{\bm{x}-\bar{\bm{x}}}{\xi_{2}/9} \right)-i\gamma\right).\] (10)

We now argue that \(F_{\bm{A},\bm{v}_{1},\bm{v}_{2}}\) acts as a duplicate function. Until the algorithm reaches a point with function value at most \(G_{\bm{A},\bm{v}_{1}}(\bar{\bm{x}})+\mu\xi_{2}\), the optimization algorithm only receives responses consistent with the function \(F_{\bm{A},\bm{v}_{1}}\) by Eq (9). Next, all minimizers of \(F_{\bm{A},\bm{v}_{1},\bm{v}_{2}}\) are contained in \(B_{d}(\bar{\bm{x}},\xi_{2}/3)\), which was the goal of introducing the term in \(\|\bm{x}-\bar{\bm{x}}\|_{2}\). As a result, optimizing \(F_{\bm{A},\bm{v}_{1},\bm{v}_{2}}\) on this ball is equivalent to minimizing

\[\tilde{F}_{\bm{A},\bm{v}_{2}}(\bm{y})=\max\left\{\|\bm{A}\bm{y}\|_{\infty}-\mu _{2},c_{2}+\nu_{2}\max_{i\in[N]}(\bm{v}_{2,i}^{\top}\bm{y}-i\gamma),c_{2}^{ \prime}+\nu_{2}^{\prime}\|\bm{y}\|\right\},\quad\bm{y}\in B_{d}(0,3),\]

where \(\bm{y}=\frac{\bm{x}-\bar{\bm{x}}}{\xi_{2}/9}\). The function has been rescaled by a factor \(\xi_{2}/9\) compared to \(F_{\bm{A},\bm{v}_{1},\bm{v}_{2}}\) so that \(\mu_{2}=\frac{9\mu}{\xi_{2}}\), \(\nu_{2}=\frac{\mu\xi_{2}}{6}\), \(\nu_{2}^{\prime}=6\mu\), \(c_{2}=\frac{9}{\xi_{2}}G_{\bm{A},\bm{v}_{1}}(\bar{\bm{x}})+3\mu+\frac{\mu\xi_{ 2}}{2}\), and \(c_{2}^{\prime}=\frac{9}{\xi_{2}}G_{\bm{A},\bm{v}_{1}}(\bar{\bm{x}})+3\mu\). By Eq (10), the two first terms of \(\tilde{F}_{\bm{A},\bm{v}_{1}}\) are preponderant for \(\bm{y}\in B_{d}(0,1)\).

The form of \(\tilde{F}_{\bm{A},\bm{v}_{2}}\) is very similar to the original form of functions

\[F_{\bm{A},\bm{v}_{2}}=\max\left\{\|\bm{A}\bm{y}\|_{\infty}-\mu_{1}^{\prime}, \mu_{2}^{\prime}\max_{i\in[N]}(\bm{v}_{2,i}^{\top}\bm{y}-i\gamma)\right\},\]

In fact, the same proof structure for the query-complexity/memory lower-bound can be applied in this case. The main difference is that originally one had \(\mu_{1}^{\prime}=\mu_{2}^{\prime}\); here we would instead have \(\mu_{1}^{\prime}=\mu_{2}+c_{2}=\Theta(\mu/\xi_{2})\) and \(\mu_{2}^{\prime}=\nu_{2}=\Theta(\mu\xi_{2})\). Intuitively, this corresponds to increasing the accuracy to \(\Theta(\epsilon\xi_{2}^{2})\)--a factor \(\xi_{2}\) is due to the fact that \(\tilde{F}_{\bm{A},\bm{v}_{2}}\) was rescaled by a factor \(\xi_{2}/9\) compared to \(F_{\bm{A},\bm{v}_{1},\bm{v}_{2}}\), and a second factor \(\xi_{2}\) is due to the fact that within \(\tilde{F}_{\bm{A},\bm{v}_{2}}\), we have \(\mu_{2}^{\prime}=\Theta(\mu\xi_{2})\)--while the query lower bound is similar to that obtained for \(\Theta(\epsilon/\xi_{2})\). As a result, during the first

\[Q_{2}=Q\left(\Theta\left(\frac{\epsilon}{\xi_{2}}\right);M,d\right)\]

queries of any algorithm optimizing \(\tilde{F}_{\bm{A},\bm{v}_{2}}\), with probability at least \(1/3\) on the sample of \(\bm{A}\) and \(\bm{v}_{2}\), all queries are at least \(\Theta(\epsilon\xi_{2})\)-suboptimal compared to

\[\bar{\bm{y}}=-\frac{1}{2\sqrt{N}}\sum_{i\in[N]}P_{\bm{A}^{\perp}}(\bm{v}_{2,i }).\]

We are now ready to give lower bounds on the queries of an algorithm minimizing \(F_{\bm{A},\bm{v}_{1},\bm{v}_{2}}\) to accuracy \(\Theta(\epsilon\xi_{2}^{2})\). Let \(T_{2}\) be the index of the first query with function value at most \(G_{\bm{A},\bm{v}_{1}}(\bar{\bm{x}})+\mu\xi_{2}\). We already checked that before that query, all responses of the oracle are consistent with minimizing \(F_{\bm{A},\bm{v}_{1}}\), hence on an event \(\mathcal{E}_{1}\) of probability at least \(1/3\), one has \(T_{2}\geq Q_{1}\). Next, consider the hypothetical case when at time \(T_{2}\), the algorithm is also given the information of \(\bar{\bm{x}}\) and is allowed to store this vector. Given this information, optimizing \(F_{\bm{A},\bm{v}_{1},\bm{v}_{2}}\) reduces to optimizing \(\tilde{F}_{\bm{A},\bm{v}_{2}}\) since we already know that the minimum is achieved within \(B_{d}(\bar{\bm{x}},\xi_{2}/3)\). Further, any query outside of this ball either

* returns a vector \(\bm{v}_{1,i}\) which does not give any useful information for the minimization (\(\bm{v}_{1}\) and \(\bm{v}_{2}\) are sampled independently and \(\bar{\bm{x}}\) is given),
* or returns a row from \(\bm{A}\), as covered by the original proof.

Hence, on an event \(\mathcal{E}_{2}\) of probability at least \(1/3\), even with the extra information of \(\bar{\bm{x}}\), during the next \(Q_{2}\) queries starting from \(T_{2}\), the algorithm does not query a \(\Theta(\mu\xi_{2}^{3})-\)suboptimal solution to \(F_{\bm{A},\bm{v}_{1},\bm{v}_{2}}\). This holds a fortiori for the model when the algorithm is not given \(\bar{\bm{x}}\) at time \(T_{2}\).

c.2.2 Recursive construction of a \(p\)-level class of functions \(F_{\bm{A},\bm{v}_{1},\ldots,\bm{v}_{p}}\)

Similarly as in the last section, one can inductively construct the sequence of functions \(F_{\bm{A},\bm{v}_{1}}\), \(F_{\bm{A},\bm{v}_{1},\bm{v}_{2}}\), \(F_{\bm{A},\bm{v}_{1},\bm{v}_{2},\bm{v}_{3}}\), etc. Formally, the induction is constructed as follows: let \((\bm{v}_{p})_{p\geq 1}\) be an i.i.d. sequence of \(N\) i.i.d. vectors \((\bm{v}_{k,i})_{i\in[N]}\) sampled from the rescaled hypercube \(d^{-1/2}\{\pm 1\}^{d}\). Next, we pose

\[G_{\bm{A},\bm{v}_{1}}(\bm{x})=\mu^{(1)}\max_{i\in[N]}\left(\bm{v}_{1,i}^{\top} \left(\frac{\bm{x}-\bar{\bm{x}}^{(1)}}{s^{(1)}}\right)-i\gamma\right),\]

where \(\mu^{(1)}=\mu\), \(\bar{\bm{x}}^{(1)}=\bm{0}\) and \(s^{(1)}=1\). For \(k\geq 1\), we pose

\[\bar{\bm{x}}^{(k+1)}=\bar{\bm{x}}^{(k)}-\frac{s^{(k)}}{2\sqrt{N}}\sum_{i\in[ N]}P_{\bm{A}^{\perp}}(\bm{v}_{k,i}),\quad\text{and}\quad F^{(k)}:=G_{\bm{A},\bm{v} _{1},\ldots,\bm{v}_{k}}(\bar{\bm{x}}^{(k)})+\mu^{(k)}\xi_{k+1},\]

for a certain parameter \(\xi_{k+1}\) to be specified. We then define the next level as

\[G_{\bm{A},\bm{v}_{1},\ldots,\bm{v}_{k+1}}(\bm{x}):=\max\left\{G_ {\bm{A},\bm{v}_{1},\ldots,\bm{v}_{k}}(\bm{x}),G_{\bm{A},\bm{v}_{1},\ldots,\bm{ v}_{k}}(\bar{\bm{x}}^{(k+1)})+\frac{\mu^{(k)}\xi_{k+1}}{3}.\right.\\ \left.\max\left\{1+\frac{\|\bm{x}-\bar{\bm{x}}^{(k+1)}\|_{2}}{s^{( k)}},1+\frac{\xi_{k+1}}{6}+\frac{\xi_{k+1}}{18}\max_{i\in[N]}\left(\bm{v}_{k+1,i}^{\top}\left(\frac{\bm{x}-\bar{\bm{x}}^{(k+1)}}{s^{(k)}\xi_{k+1}/9}\right)- i\gamma\right)\right\}\right\}.\]

We then pose \(\mu^{(k+1)}:=\mu^{(k)}\xi_{k+1}^{2}/54\) and \(s^{(k+1)}:=s^{(k)}\xi_{k+1}/9\), which closes the induction. The optimization functions are defined simply as

\[F_{\bm{A},\bm{v}_{1},\ldots,\bm{v}_{k+1}}(\bm{x})=\max\left\{\|\bm{A}\bm{x}\|_ {\infty}-\mu,G_{\bm{A},\bm{v}_{1},\ldots,\bm{v}_{k+1}}(\bm{x})\right\}.\]

We checked before that we can use \(\xi_{2}=1/(16\sqrt{d})\). For general \(k\geq 0\), given that the form of the function slightly changes to incorporate the absolute term (see \(\tilde{F}_{\bm{A},\bm{v}_{2}}\)), this constant may differ slightly. In any case, one has \(\xi_{k}=\Theta(1/\sqrt{d})\). Now fix a construction level \(p\geq 1\) and for any \(k\in[p]\), let \(T_{k}\) be the first time that a point with function value at most \(F^{(k)}\) is queried. For convenience let \(T_{0}=0\). Using the same arguments as above recursively, we can show that on an event \(\mathcal{E}_{k}\) with probability at least \(1/3\),

\[T_{k}-T_{k-1}\geq Q_{k}=Q\left(\Theta\left(\frac{\mu}{s^{(k)}}\right);M,d\right)\]

Next note that the sequence \(F^{(k)}\) is decreasing and by construction, if one finds a \(\mu^{(p)}\xi_{p+1}\)-suboptimal point of \(F_{\bm{A},\bm{v}_{1},\ldots,\bm{v}_{p}}\), then this point has value at most \(F^{(p)}\). As a result, for an algorithm that finds a \(\mu^{(p)}\xi_{p+1}\)-suboptimal point, the times \(T_{0},\ldots,T_{p}\) are all well defined and non-decreasing. We recall that \(\mu=\Theta(\sqrt{d}\epsilon)\). Therefore, we can still have \(\mu/s^{(p)}\leq\sqrt{\epsilon}\) and \(\mu^{(p)}\xi_{p+1}\geq\epsilon^{2}\) for \(p=\Theta(\frac{\ln\frac{1}{d}}{d})\). Combining these observations, we showed that when optimizing the functions \(F_{\bm{A},\bm{v}_{1},\ldots,\bm{v}_{p}}\) to accuracy \(\Theta(\mu^{(p)}\xi_{p+1})=\Omega(\epsilon^{2})\), the total number of queries \(Q\) satisfies

\[\mathbb{E}[Q]\geq\frac{1}{3}\sum_{k\in[p]}Q_{k}\geq\frac{p}{3}Q(\sqrt{\epsilon} ;M,d)=\Theta\left(\frac{d^{4/3}\ln\frac{1}{\epsilon}}{\ln^{4/3}d}\left(\frac{ d\ln\frac{1}{\epsilon}}{M+d\ln d}\right)^{4/3}\right).\]

Changing \(\epsilon\) to \(\epsilon^{2}\) proves the desired result.

**Theorem C.5**.: _For \(\epsilon\leq 1/d^{8}\) and any \(\delta\in[0,1]\), any (potentially randomized) algorithm guaranteed to minimize \(1\)-Lipschitz convex functions over the unit ball with \(\epsilon\) accuracy uses at least \(d^{1.25-\delta}\ln\frac{1}{\epsilon}\) bits of memory or makes \(\tilde{\Omega}(d^{1+4\delta/3}\ln\frac{1}{\epsilon})\) queries._

The same recursive construction can be applied to the results from Theorems C.1 and C.3 to improve their oracle-complexity lower bounds by a factor \(\frac{\ln\frac{1}{\epsilon}}{\ln d}\), albeit with added technicalities due to the adaptivity of their class of functions. This yields Theorem 3.3.

**Input:** Number of iterations \(T\), computation accuracy \(\eta\leq 1\), target accuracy \(\epsilon\leq 1\)

Initialize: \(\bm{x}=\bm{0}\);

**for**\(t=0,\ldots,T\)**do**

**Query the oracle at \(\bm{x}\)**

**if**\(\bm{x}\)_successful_ **then return**\(x\);

**Receive a separation vector \(\bm{g}\) with accuracy \(\eta\)

Update \(\bm{x}\) as \(\bm{x}-\epsilon\bm{g}\) up to accuracy \(\eta\)

**end**

**return**\(\bm{x}\) ```

**Algorithm 11**Memory-constrained gradient descent

## Appendix D Memory-constrained gradient descent for the feasibility problem

In this section, we prove a simple result showing that memory-constrained gradient descent applies to the feasibility problem. We adapt the algorithm described in [52].

We now prove that this memory-constrained gradient descent gives the desired result of Proposition 3.1.

Proof of Proposition 3.1.: Denote by \(\bm{x}_{t}\) the state of \(\bm{x}\) at iteration \(t\), and \(\bm{g}_{t}\) (resp. \(\tilde{\bm{g}}_{t}\)) the separation oracle without rounding errors (resp. with rounding errors) at \(\bm{x}_{t}\). By construction,

\[\|\bm{x}_{t+1}-(\bm{x}_{t}+\epsilon\tilde{\bm{g}}_{t})\|\leq\eta\quad\text{ and}\quad\|\tilde{\bm{g}}_{t}-\bm{g}_{t}\|\leq\eta.\] (11)

As a result, recalling that \(\|\bm{g}_{t}\|=1\),

\[\|\bm{x}_{t+1}-\bm{x}^{\star}\|^{2}\leq(\|\bm{x}_{t}+\epsilon\tilde{\bm{g}}_{ t}-\bm{x}^{\star}\|+\eta)^{2}\leq(\|\bm{x}_{t}+\epsilon\bm{g}_{t}-\bm{x}^{ \star}\|+(1+\epsilon)\eta)^{2}\leq\|\bm{x}_{t}+\epsilon\bm{g}_{t}-\bm{x}^{ \star}\|^{2}+20\eta.\]

By assumption, \(Q\) contains a ball \(B_{d}(\bm{x}^{\star},\epsilon)\) for \(\bm{x}^{\star}\in B_{d}(0,1)\). Then, because \(\bm{g}_{t}\) separates \(\bm{x}_{t}\) from \(B_{d}(\bm{x}^{\star},\epsilon)\), one has \(\bm{g}_{t}^{\top}(\bm{x}^{\star}-\bm{x}_{t})\geq\epsilon\). Therefore,

\[\|\bm{x}_{t+1}-\bm{x}^{\star}\|^{2} \leq\|\bm{x}_{t}-\bm{x}^{\star}\|^{2}+2\epsilon\bm{g}_{t}^{\top}( \bm{x}_{t}-\bm{x}^{\star})+\epsilon^{2}\|\bm{g}_{t}\|^{2}+20\eta\] \[\leq\|\bm{x}_{t}-\bm{x}^{\star}\|^{2}-\epsilon^{2}+20\eta.\]

Then, take \(\eta=\epsilon^{2}/40\) and \(T=\frac{8}{\epsilon^{2}}\). If iteration \(T\) was performed, we have using the previous equation

\[\|\bm{x}_{T}-\bm{x}^{\star}\|^{2}\leq\|\bm{x}_{0}-\bm{x}^{\star}\|^{2}-\frac{ \epsilon^{2}}{2}T\leq 4-\frac{\epsilon^{2}}{2}T\leq 0.\]

Hence, \(\bm{x}_{T}\) is an \(\epsilon\)-suboptimal solution.

We now turn to the memory usage of gradient descent. It only needs to store \(\bm{x}\) and \(\bm{g}\) up to the desired accuracy \(\eta=\mathcal{O}(\epsilon^{2})\). Hence, this storage and the internal computations can be done with \(\mathcal{O}(d\ln\frac{d}{\epsilon})\) memory. Because we suppose that \(\epsilon\leq\frac{1}{\sqrt{d}}\), this gives the desired result.