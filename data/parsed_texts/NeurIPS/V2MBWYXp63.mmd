# Text2NKG: Fine-Grained N-ary Relation Extraction for N-ary relational Knowledge Graph Construction

Haoran Luo\({}^{1}\), Haihong E\({}^{1}\), Yuhao Yang\({}^{2}\), Tianyu Yao\({}^{1}\),

**Yikai Guo\({}^{3}\), Zichen Tang\({}^{1}\), Wentai Zhang\({}^{1}\), Shiyao Peng\({}^{1}\), Kaiyang Wan\({}^{1}\),**

**Meina Song\({}^{1}\), Wei Lin\({}^{4}\), Yifan Zhu\({}^{1}\), Luu Anh Tuan\({}^{5}\)**

\({}^{1}\)School of Computer Science, Beijing University of Posts and Telecommunications, China

\({}^{2}\)School of Automation Science and Electrical Engineering, Beihang University, China

\({}^{3}\)Beijing Institute of Computer Technology and Application \({}^{4}\)Inspur Group Co., Ltd., China

\({}^{5}\)College of Computing and Data Science, Nanyang Technological University, Singapore

{luohaoran, ehaihong, yifan_zhu}@bupt.edu.cn, anhtuan.luu@ntu.edu.sg

###### Abstract

Beyond traditional binary relational facts, n-ary relational knowledge graphs (NKGs) are comprised of n-ary relational facts containing more than two entities, which are closer to real-world facts with broader applications. However, the construction of NKGs remains at a coarse-grained level, which is always in a single schema, ignoring the order and variable arity of entities. To address these restrictions, we propose Text2NKG, a novel fine-grained n-ary relation extraction framework for n-ary relational knowledge graph construction. We introduce a span-tuple classification approach with hetero-ordered merging and output merging to accomplish fine-grained n-ary relation extraction in different arity. Furthermore, Text2NKG supports four typical NKG schemas: _hyper-relational schema_, _event-based schema_, _role-based schema_, and _hypergraph-based schema_, with high flexibility and practicality. The experimental results demonstrate that Text2NKG achieves state-of-the-art performance in \(F_{1}\) scores on the fine-grained n-ary relation extraction benchmark. Our code and datasets are publicly available1.

Footnote 1: https://github.com/LHRLAB/Text2NKG

## 1 Introduction

Modern knowledge graphs (KGs), such as Freebase [2], Google Knowledge Vault [7], and Wikidata [21], utilize a multi-relational graph structure to represent knowledge. Because of the advantage of intuitiveness and interpretability, KGs find various applications in question answering [28], query response [1], logical reasoning [4], and recommendation systems [29]. Traditional KGs are mostly composed of binary relational facts (\(subject,\ relation,\ object\)), which represent the relationship between two entities [3]. However, it has been observed [20] that over 30% of real-world facts involve n-ary relation facts with more than two entities (\(n\geq 2\)). As shown in Figure 1, an n-ary relational knowledge graph (NKG) is composed of many n-ary relation facts, offering richer knowledge expression

Figure 1: An example of NKG construction.

and wider application capabilities. As a key step of constructing NKGs, n-ary relation extraction (n-ary RE) is a task of identifying n-ary relations among entities in natural language texts. Compared to binary relational facts, n-ary relational facts in NKGs have more diverse schemas for different scenarios. For example, Wikidata utilizes n-ary relational facts in a _hyper-relational schema_[20; 10; 23], i.e., \((s,r,o,\{(k_{i},v_{i})\}_{i=1}^{n-2})\) which adds \((n-2)\) key-value pairs to the main triple to represent auxiliary information. In addition to the _hyper-relational schema_, the existing NKG schemas also include _event-based schema_ (\(r\), \(\{(k_{i},v_{i})\}_{i=1}^{n}\)) [11; 16], _role-based schema_ (\(\{(k_{i},v_{i})\}_{i=1}^{n}\)) [12; 15], and _hypergraph-based schema_ (\(r,\{v_{i}\}_{i=1}^{n}\)) [26; 8], as shown in Figure 2.

Currently, most existing NKGs in four schemas, such as JF17K [26], Wikipeople [12], WD50K [10], and EventKG [11], are manually constructed. Previous n-ary RE methods [13; 31] focus on extraction with a fixed number of entities in _hypergraph-based schema_ or _role-based schema_. Existing event extraction methods [16; 17; 9] can achieve n-ary RE in _event-based schema_. Recently, CubeRE [5] introduce a cube-filling method, which is the only n-ary RE method in _hyper-relational schema_.

However, there are still three main challenges in automated n-ary RE for NKG construction, which remains at a coarse-grained level: **(1) Diversity of NKG schemas.** Previous methods could only perform N-ary RE based on a specific schema, but currently, there is no flexible method that can perform n-ary RE for arbitrary schema with different number of relations. **(2) Determination of the order of entities.** N-ary RE involves more possible entity orders than binary RE, for example, as shown in Figure 2, in a _hyper-relational schema_, there is an order issue regarding which entity is the head entity, tail entity, or auxiliary entity. Previous methods often ignored the joint impact of different entity orders, leading to inaccurate extraction.**(3) Variability of the arity of n-ary RE.** Previous methods usually output a fixed number of entities and are not adept at determining the variable number of entities forming an n-ary relational fact.

To tackle these challenges, we introduce **Text2NKG**, a novel fine-grained n-ary RE framework designed to automate the generation of n-ary relational facts from natural language text for NKG construction. Text2NKG employs a **span-tuple multi-label classification** method, which transforms n-ary RE into a multi-label classification task for span-tuples, including all combinations of entities in the text. Because the number of predicted relation labels corresponds to the chosen NKG schema, Text2NKG is adaptable to all NKG schemas, offering examples with _hyper-relational schema_, _event-based schema_, _role-based schema_, and _hypergraph-based schema_, all of which have broad applications. Moreover, Text2NKG introduces a **hetero-ordered merging** method, considering the probabilities of predicted labels for different entity orders to determine the final entity order. Finally, Text2NKG proposes an **output merging** method, which is used to unsupervisedly derive n-ary relational facts of any number of entities for NKG construction.

In addition, we extend the only n-ary RE benchmark for NKG construction, HyperRED [5], which is in the _hyper-relational schema_, to four NKG schemas. We've done sufficient n-ary RE experiments on HyperRED, and the experimental results show that Text2NKG achieves state-of-the-art performance in \(F_{1}\) scores of hyper-relational extraction. We also compared the results of Text2NKG in the other three schemas to verify applications.

Figure 2: Taking a real-world textual fact as an example, we can extract a four-arity structured span-tuple for entities (Einstein, University of Zurich, Doctorate, Physics) with an answer label-list for relations accordingly as a 4-ary relational fact from the sentence through n-ary relation extraction.

Related Work

### N-ary relational Knowledge Graph

An n-ary relational knowledge graph (NKG) consists of n-ary relational facts, which contain \(n\) entities (\(n\geq 2\)) and several relations. The n-ary relational facts are necessary and cannot be replaced by combinations of some binary relational facts because we cannot distinguish which binary relations are combined to represent the n-ary relational fact in the whole KG. Therefore, NKG utilizes a schema in every n-ary relational fact locally and a hypergraph representation globally [18].

Firstly, the simplest NKG schema is hypergraph-based. [26] found that over 30% of Freebase [2] entities participate facts with more than two entities, first defined n-ary relations mathematically and used star-to-clique conversion to convert triple-based facts representing n-ary relational facts into the first NKG dataset JF17K in _hypergraph-based schema_ (\(r\), \(\{v_{i}\}_{i=1}^{n}\)). [8] proposed FB-AUTO and M-FB15K with the same _hypergraph-based schema_. Secondly, [12] introduced role information for n-ary relational facts and extracted Wikipedia, the first NKG dataset in _role-based schema_ (\(\{(k_{i},v_{i})\}_{i=1}^{n}\)), composed of role-value pairs. Thirdly, Wikidata [21], the largest knowledge base, utilizes an NKG schema based on hyper-relation \((s,r,o,\{(k_{i},v_{i})\}_{i=1}^{n-2})\), which adds auxiliary key-value pairs to the main triple. [10] first proposed an NKG dataset in _hyper-relational schema_ WD50K. Fourthly, as [11] pointed out, events are also n-ary relational facts. One basic event representation has an event type, a trigger, and several key-value pairs [16]. Regarding the event type as the main relation, the (trigger: value) as one of the key-value pairs, and the arguments as the rest key-value pairs, we can obtain an _event-based NKG schema_ (\(r\), \(\{(k_{i},v_{i})\}_{i=1}^{n}\)).

Based on four common NKG schemas, we propose Text2NKG, the first method for extraction of structured n-ary relational facts from natural language text, which improves NKG representation and application.

### N-ary Relation Extraction

Relation extraction (RE) is an important step of KG construction, directly affecting the quality, scale, and application of KGs. While most of the current n-ary relation extraction (n-ary RE) for NKG construction depends on manual construction [26; 12; 10] but not automated methods. Most automated RE methods target the extraction of traditional binary relational facts. For example, [22] proposes a table-filling method for binary RE, and [30; 27] propose span-based RE methods with levitated marker and packed levitated marker, respectively.

For automated n-ary RE, some approaches [13; 31] treat n-ary RE in _hypergraph-based schema_ or _role-based schema_ as a binary classification problem and predict whether the composition of n-ary information in a document is valid or not. However, these methods extract n-ary information in fixed arity, which are not flexible. Moreover, some event extraction methods [16; 17; 9] propose different event trigger and argument extraction techniques, which can achieve n-ary RE in _event-based schema_. Recently, CubeRE [5] proposes an automated n-ary RE method in _hyper-relational schema_, which extends the table-filling extraction method to n-ary RE with cube-filling. However, these methods can only model one of the useful NKG schemas with limited extraction accuracy.

In this paper, we propose the first fine-grained n-ary RE framework Text2NKG for NKG construction in four example schemas, proposing a span-tuple multi-label classification method with hetero-ordered merging and output merging to improve the accuracy of fine-grained n-ary RE extraction in all NKG schemas substantially.

## 3 Preliminaries

**Formulation of NKG.** An NKG \(\mathcal{G}=\{\mathcal{E},\mathcal{R},\mathcal{F}\}\) consists of an entity set \(\mathcal{E}\), a relation set \(\mathcal{R}\), and an n-ary fact (n\(\geq\)2) set \(\mathcal{F}\). Each n-ary fact \(f^{n}\in\mathcal{F}\) consists of entities \(\in\mathcal{E}\) and relations \(\in\mathcal{R}\). For hyper-relational schema [20]: \(f^{n}_{hr}=(e_{1},r_{1},e_{2},\{r_{i-1},e_{i}\}_{i=3}^{n})\) where \(\{e_{i}\}_{i=1}^{n}\in\mathcal{E}\), \(\{r_{i}\}_{i=1}^{n-1}\in\mathcal{R}\). For event-based schema [16]: \(f^{n}_{ev}=(r_{1},\{r_{i+1},e_{i}\}_{i=1}^{n})\), where \(\{e_{i}\}_{i=1}^{n}\in\mathcal{E}\), \(\{r_{i}\}_{i=1}^{n+1}\in\mathcal{R}\). For role-based schema [12]: \(f^{n}_{ro}=(\{r_{i},e_{i}\}_{i=1}^{n})\), where \(\{e_{i}\}_{i=1}^{n}\in\mathcal{E}\), \(\{r_{i}\}_{i=1}^{n}\in\mathcal{R}\). For hypergraph-based schema [26]: \(f^{n}_{hg}=(r_{1},\{e_{i}\}_{i=1}^{n})\), where \(\{e_{i}\}_{i=1}^{n}\in\mathcal{E}\), \(r_{1}\in\mathcal{R}\).

Problem Definition.Given an input sentence with \(l\) words \(s=\{w_{1},w_{2},...,w_{l}\}\), an entity \(e\) is a consecutive span of words: \(e=\{w_{p},w_{p+1},...,w_{q}\}\in\mathcal{E}_{s}\), where \(p,q\in\{1,...,l\}\), and \(\mathcal{E}_{s}=\{e_{j}\}_{j=1}^{m}\) is the entity set of all \(m\) entities in the sentence. The output of n-ary relation extraction, \(R()\), is a set of n-ary relational facts \(\mathcal{F}_{s}\) in given NKG schema in \(\{f_{hr}^{n},f_{rv}^{n},f_{ro}^{n},f_{hg}^{n}\}\). Specifically, each n-ary relational fact \(f^{n}\in\mathcal{F}_{s}\) is extracted by multi-label classification of one of the ordered span-tuple for \(n\) entities \([e_{i}]_{i=1}^{n}\in\mathcal{E}_{s}\), forming an answer label-list for \(n_{r}\) relations \([r_{i}]_{i=1}^{n_{r}}\in\mathcal{R}\), where \(n\) is the arity of the extracted n-ary relational fact, and \(n_{r}\) is the number of answer relations in the fact, which is determined by the given NKG schema: \(R([e_{i}]_{i=1}^{n})=[r_{i}]_{i=1}^{n-1}\), when \(f^{n}=f_{hr}^{n}\), \(R([e_{i}]_{i=1}^{n})=[r_{i}]_{i=1}^{n+1}\) when \(f^{n}=f_{ev}^{n}\), \(R([e_{i}]_{i=1}^{n})=[r_{i}]_{i=1}^{n}\) when \(f^{n}=f_{ro}^{n}\), and \(R([e_{i}]_{i=1}^{n})=[r_{1}]\) when \(f^{n}=f_{hg}^{n}\).

## 4 Methodology

In this section, we first introduce the overview of the Text2NKG framework, followed by the span-tuple multi-label classification, training strategy, hetero-ordered merging, and output merging.

### Overview of Text2NKG

Text2NKG is a fine-grained n-ary relation extraction framework built for n-ary relational knowledge graph (NKG) construction. The input to Text2NKG is natural language text tokens labeled with entity span in sentence units. First, inspired by [27], Text2NKG encodes the entities using BERT-based Encoder [6] with a packaged levitated marker for embedding. Then each arrangement of ordered span-tuple with three entity embeddings will be classified with multiple labels, and the framework will be learned by the weighted cross-entropy with a null-label bias. In the decoding stage, in order to filter the n-ary relational facts whose entity compositions have isomorphic hetero-ordered characteristics, Text2NKG proposes a hetero-ordered merging strategy to merge the label probabilities of \(3!=6\) arrangement cases of span-tuples composed of the same entities and filter out the output 3-ary relational facts existing non-conforming relations. Finally, Text2NKG combines the output 3-ary relational facts to form the final n-ary relational facts with output merging.

### Span-tuple Multi-label Classification

For the given sentence token \(s=\{w_{1},w_{2},...,w_{l}\}\) and the set of entities \(\mathcal{E}_{s}\), in order to perform fine-grained n-ary RE, we need first to encode a span-tuple (\(e_{1},e_{2},e_{3}\)) consisting of every arrangement of three ordered entities, where \(e_{1},e_{2},e_{3}\in\mathcal{E}_{s}\). Due to the high time complexity of training every

Figure 3: An overview of Text2NKG extracting n-ary relation facts from a natural language sentence in hyper-relational NKG schema for an example.

span-tuple as one training item, inspired by [27], we achieve the reduction of training items by using packed levitated markers that pack one training item with each entity in \(\mathcal{E}_{s}\) separately. Specifically, in each packed training item, a pair of solid tokens, [S] and [/S], are added before and after the packed entity \(e_{S}=\{w_{{}_{PS}},...,w_{{}_{PS}}\}\), and (\(|\mathcal{E}_{s}|-1\)) pairs of levitated markers, [L] and [/L], according to other entities in \(\mathcal{E}_{s}\), are added with the same position embeddings as the beginning and end of their corresponding entities span \(e_{L_{i}}=\{w_{{}_{P_{L_{i}}}},...,w_{{}_{M_{L_{i}}}}\}\) to form the input token \(\mathbf{X}\):

\[\begin{split}\mathbf{X}=&\{w_{1},...,[S],w_{{}_{PS} },...,w_{{}_{PS}},[/S],...,\\ & w_{{}_{P_{L_{i}}}}\cup[L],...,w_{{}_{T_{L_{i}}}}\cup[/L],...,w_{ {}_{l}}\}.\end{split}\] (1)

We encode such token by the BERT-based pre-trained model encoder [6]:

\[\{h_{1},h_{2},...,h_{t}\}=\text{BERT}(\mathbf{X}),\] (2)

where \(t=|\mathbf{X}|\) is the input token length, \(\{h_{i}\}_{i=1}^{t}\in\mathbb{R}^{d}\), and \(d\) is embedding size.

There are several span-tuples (\(A,B,C\)) in a training item. The embedding of first entity \(h_{A}\in\mathbb{R}^{2d}\) in the span-tuple is obtained by concat embedding of the solid markers, [S] and [/S], and the embeddings of second and third entities \(h_{B},h_{C}\in\mathbb{R}^{2d}\) are obtained by concat embeddings of levitated markers, [L] and [/L] with all \(A_{m-1}^{2}\) arrangement of any other two entities in \(\mathcal{E}_{s}\). Thus, we obtain the embedding representation of the three entities to form \(A_{m-1}^{2}\) span-tuples in one training item. Therefore, every input sentence contains \(m\) training items with \(mA_{m-1}^{2}=A_{m}^{3}\) span-tuples for any ordered arrangement of three entities.

We then define \(n_{r}\) linear classifiers, each of which consists of 3 feedforward neural networks \(\{\text{FNN}_{i}^{k}\}_{i=1}^{n_{r}},k=1,2,3\), to classify the span-tuples for multiple-label classification. Each classifier targets the prediction of one relation \(r_{i}\), thus obtaining a probability lists \(\left(\mathbf{P}_{i}\right)_{i=1}^{n_{r}}\) with all relations in given relation set \(\mathcal{R}\) plus a null-label:

\[\mathbf{P}_{i}=\text{FNN}_{i}^{1}(h_{A})+\text{FNN}_{i}^{2}(h_{B})+\text{FNN} _{i}^{3}(h_{C}),\] (3)

where \(\text{FNN}_{i}^{k}\in\mathbb{R}^{2d\times(|\mathcal{R}|+1)}\), and \(\mathbf{P}_{i}\in\mathbb{R}^{(|\mathcal{R}|+1)}\).

### Training Strategy

To train the \(n_{r}\) classifiers for each relation prediction more accurately, we propose a data augmentation strategy for span-tuples. Taking the _hyper-relational schema_ as an example, given a hyper-relational fact (\(A,r_{1},B,r_{2},C\)), we consider swapping the head and tail entities, and changing the main relation to its inverse (\(B,r_{1}^{-1},A,r_{2},C\)), as well as swapping the tail entities with auxiliary values, and the main relation with the auxiliary key (\(A,r_{2},C,r_{1},B\)), also as labeled training span-tuple cases. Thus \(R_{hr}(A,B,C)=(r_{1},r_{2})\) can be augmented with \(3!=6\) orders of span-tuples:

\[\begin{cases}R_{hr}(A,B,C)=(r_{1},r_{2}),\\ R_{hr}(B,A,C)=(r_{1}^{-1},r_{2}),\\ R_{hr}(A,C,B)=(r_{2},r_{1}),\\ R_{hr}(B,C,A)=(r_{2},r_{1}^{-1}),\\ R_{hr}(C,A,B)=(r_{2}^{-1},r_{1}),\\ R_{hr}(C,B,A)=(r_{1},r_{2}^{-1}).\end{cases}\] (4)

For other schemas, we can also obtain 6 fully-arranged cases of labeled span-tuples in a similar way, as described in Appendix A. If no n-ary relational fact exists between the three entities of span-tuples, then relation labels are set as null-label.

Since most cases of span-tuple are null-label, we set a weight hyperparameter \(\alpha\in(0,1]\) between the null-label and other labels to balance the learning of the null-label. We jointly trained the \(n_{r}\) classifiers for each relations by cross-entropy loss \(\mathcal{L}\) with a null-label weight bias \(\mathbf{W}_{\alpha}\):

\[\mathcal{L}=-\sum_{i=1}^{n_{r}}\mathbf{W}_{\alpha}\log\left(\frac{\exp\left( \mathbf{P}_{i}[r_{i}]\right)}{\sum_{j=1}^{|\mathcal{R}|+1}\exp\left(\mathbf{ P}_{ij}\right)}\right),\] (5)

where \(\mathbf{W}_{\alpha}=[\alpha,1.0,1.0,...1.0]\in\mathbb{R}^{(|\mathcal{R}|+1)}\).

[MISSING_PAGE_FAIL:6]

[MISSING_PAGE_FAIL:7]

the same BERT-large encoder, reflecting Text2NKG's excellent performance. Figure 4(a) and 4(b) intuitively show the changes of evaluation metrics and answers of facts in the dev set during the training of Text2NKG. It is worth noting that Text2NKG exceeds 90% in precision accuracy, which proves that the model can obtain very accurate n-ary relational facts and provides a good guarantee for the quality of fine-grained NKG construction.

### Results on Various NKG Schemas (RQ2)

As shown in Table 3, besides _hyper-relational schema_, Text2NKG also accomplishes the tasks of fine-grained n-ary RE in three other different NKG schemas on HyperRED, which demonstrates good utility. In the added tasks of n-ary RE for event-based, role-based, and _hypergraph-based schemas_, since no model has done similar experiments at present, we used event extraction or unified extraction methods such as Text2Event [16], UIE [17], and LasUIE [9] for comparison. Text2NKG still works best in these schemas, which demonstrates good versatility.

### Ablation Study (RQ3)

Data augmentation (DA), null-label weight hyperparameter (\(\alpha\)), and hetero-ordered merging (HM) are the three main components of Text2NKG. For the different Text2NKG variants as shown in Table 2, DA, \(\alpha\), and HM all contribute to the accurate results of our complete model. By comparing the differences, we find that HM is most effective by combining the probabilities of labels of different orders, followed by DA and \(\alpha\).

### Analysis of Null-label Weight Hyperparameters (RQ4)

We compared the effect for different null-label weight hyperparameters (\(\alpha\)). As shown in Figure 4(c), the larger the \(\alpha\), the greater the learning weight of null-label compared with other lables, the more relations are predicted as null-label. After filtering out the facts having null-label, fewer facts are extracted, so the precision is generally higher, and the recall is generally lower. The smaller the \(\alpha\), the more relations are predicted as non-null labels, thus extracting more n-ary relation facts, so the recall is generally higher, and the precision is generally lower. Comparing the results of \(F_{1}\) values for different \(\alpha\), it is found that \(\alpha=0.01\) works best, which can be adjusted in practice according to specific needs to obtain the best results.

### Analysis of N-ary Relation Extraction in Different Arity (RQ5)

Figure 5(a) shows the number of n-ary relational facts extracted after output merging and the number of the answer facts in different arity during training of Text2NKG on the dev set. We find that, as the training proceeds, the final output of Text2NKG converges to the correct answer in terms of the number of complete n-ary relational facts in each arity, achieving implementation of n-ary RE in indefinite arity unsupervised, with good scalability.

Figure 4: (a) Precision, Recall, and \(F_{1}\) changes in the dev set during the training of Text2NKG. (b) The changes of the number of true facts, the number of predicted facts, and the number of predicted accurate facts during the training of Text2NKG. (c) Precision, Recall, and \(F_{1}\) results on different null-label hyperparameter (\(\alpha\)) settings.

### Computational Efficiency (RQ6)

As mentioned in Section 4.2, the main computational consumption of Text2NKG is selecting every span-tuple of three ordered entities to encode them and get the classified labels in multiple-label classification part. If we adopt an traversal approach with each span-tuple in one training items, the time complexity will be O(\(m^{3}\)). To reduce the high time complexity of training every span-tuple as one training item, Text2NKG uses packed levitated markers that pack one training item with each entity in \(\mathcal{E}_{s}\) separately. We obtain the embedding representation of the three entities to form \(A^{2}_{m-1}\) span-tuples in one training item. Every input sentence contains \(m\) training items with \(mA^{2}_{m-1}=A^{3}_{m}\) span-tuples for any ordered arrangement of three entities for multiple-label classification. Therefore, the time complexity decreased from O(\(m^{3}\)) to O(\(m\)).

### Case Study (RQ7)

Figure 5(b) shows a case study of n-ary RE by a trained Text2NKG. For a sentence, "He was born in Skripenbeck, near York and attended Pocklin.", four structured n-ary RE can be obtained by Text2NKG according to the requirements. Taking the _hyper-relational schema_ for an example, Text2NKG can successfully extract one n-ary relational fact consisting of a main triple [He, educated at, Pocklington], and two auxiliary key-value pairs {start time:1936}, {end time:1943}. This intuitively validates the practical performance of Text2NKG on fine-grained n-ary RE to better contribute to NKG construction.

### Comparison with ChatGPT (RQ8)

As shown in Table 2 and Table 3, we compared the extraction effects under four NKG schemas of the supervised Text2NKG with the unsupervised ChatGPT and GPT-4. We found that these large language models cannot accurately distinguish the closely related relations in the fine-grained NKG relation repository, resulting in their F1 scores ranging around 10%-15%, which is much lower than the performance of Text2NKG. On the other hand, the limitation of Text2NKG is that its performance is confined within the realm of supervised training. Therefore, in future improvements and practical applications, we suggest combining small supervised models with large unsupervised models to balance solving the cold-start and fine-grained extraction, which is detailed in Appendix G.1.

## 6 Conclusion

In this paper, we introduce Text2NKG, a novel framework designed for fine-grained n-ary relation extraction (RE) aimed at constructing N-ary Knowledge Graphs (NKGs). Our extensive experiments demonstrate that Text2NKG outperforms all existing baseline models across a wide range of fine-grained n-ary RE tasks. Notably, it excels in four distinct schema types: hyper-relational, event-based, role-based, and hypergraph-based. Furthermore, we have extended the HyperRED dataset, transforming it into a comprehensive fine-grained n-ary RE benchmark that supports all four schemas.

Figure 5: (a) The changes of the number of extracted n-ary RE in different arity, where ”pred_n” represents the number of extracted n-ary facts with different arities by Text2NKG, and ”ans_n” represents the ground truth. (b) Case study of Text2NKG’s n-ary relation extraction in four schemas on HyperRED.

## Acknowledgments

This work is supported by the National Science Foundation of China (Grant No. 62176026, Grant No. 62406036, Grant No. 62473271, and Grant No. 62076035). This work is also supported by the SMP-Zhipu.AI Large Model Cross-Disciplinary Fund, the BUPT Excellent Ph.D. Students Foundation (No. CX2023133), the BUPT Innovation and Entrepreneurship Support Program (No. 2024-YC-A091 and No. 2024-YC-T022), and the Engineering Research Center of Information Networks, Ministry of Education.

## References

* [1]E. Arakelyan, D. Daza, P. Minervini, and M. Cochez (2021) Complex query answering with neural link predictors. In International Conference on Learning Representations, Cited by: SS1.
* [2]K. Bollacker, C. Evans, P. Paritosh, T. Sturge, and J. Taylor (2008) Freebase: a collaboratively created graph database for structuring human knowledge. In Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data, SIGMOD '08, New York, NY, USA, pp. 1247-1250. External Links: ISBN 978-1-4503-3871-1, Link, Document Cited by: SS1.
* [3]A. Bordes, N. Usunier, A. Garcia-Duran, J. Weston, and O. Yakhnenko (2013) Translating embeddings for modeling multi-relational data. In Advances in Neural Information Processing Systems, M. Welling, Z. Ghahramani, and K.Q. Weinberger (Eds.), pp.. External Links: Link Cited by: SS1.
* [4]X. Chen, Z. Hu, and Y. Sun (2022-04) Fuzzy logic based logical query answering on knowledge graphs. Proceedings of the AAAI Conference on Artificial Intelligence36 (4), pp. 3939-3948. External Links: Link, Document Cited by: SS1.
* [5]Y. Ken Chia, L. Bing, S. M. Aljunied, L. Si, and S. Poria (2022-06) A dataset for hyper-relational extraction and a cube-filling approach. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, H. Kozareva, and Y. Zhang (Eds.), pp. 10114-10133. External Links: Link, Document Cited by: SS1.
* [6]J. Devlin, M. Chang, K. Lee, and K. Toutanova (2019-06) BERT: pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), Minneapolis, Minnesota, pp. 4171-4186. External Links: Link, Document Cited by: SS1.
* [7]X. Dong, E. Gabrilovich, G. Heitz, W. Horn, N. Lao, K. Murphy, T. Strohmann, S. Sun, and W. Zhang (2014-06) Knowledge vault: a web-scale approach to probabilistic knowledge fusion. In Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD '14, New York, NY, USA, pp.. External Links: ISBN 978-1-4503-3871-1, Link, Document Cited by: SS1.
* [8]B. Fatemi, P. Taslakian, D. Vazquez, and D. Poole (2021) Knowledge hypergraphs: prediction beyond binary relations. In Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI'20, Cited by: SS1.
* [9]H. Fei, S. Wu, J. Li, B. Li, F. Li, L. Qin, M. Zhang, M. Zhang, and T. Chua (2022) Lasuie: unifying information extraction with latent adaptive structure-aware generative language model. In Advances in Neural Information Processing Systems, H. S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh (Eds.), pp. 15460-15475. External Links: Link, Document Cited by: SS1.
* [10]M. Galkin, P. Trivedi, G. Maheshwari, R. Usbeck, and J. Lehmann (2020-06) Message passing for hyper-relational knowledge graphs. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), Online, November 2020, pp. 7346-7359. External Links: Link, Document Cited by: SS1.
** [11] Saiping Guan, Xueqi Cheng, Long Bai, Fujun Zhang, Zixuan Li, Yutao Zeng, Xiaolong Jin, and Jiafeng Guo. What is event knowledge graph: A survey. _IEEE Transactions on Knowledge and Data Engineering_, pages 1-20, 2022.
* [12] Saiping Guan, Xiaolong Jin, Yuanzhuo Wang, and Xueqi Cheng. Link prediction on n-ary relational data. In _The World Wide Web Conference_, WWW '19, page 583-593, New York, NY, USA, 2019. Association for Computing Machinery.
* [13] Robin Jia, Cliff Wong, and Hoifung Poon. Document-level n-ary relation extraction with multiscale representation learning. In _Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)_, pages 3693-3704, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics.
* [14] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In _Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics_, pages 7871-7880, Online, July 2020. Association for Computational Linguistics.
* [15] Yu Liu, Quanming Yao, and Yong Li. Role-aware modeling for n-ary relational knowledge bases. In _Proceedings of the Web Conference 2021_, WWW '21, page 2660-2671, New York, NY, USA, 2021. Association for Computing Machinery.
* [16] Yaojie Lu, Hongyu Lin, Jin Xu, Xianpei Han, Jialong Tang, Annan Li, Le Sun, Meng Liao, and Shaoyi Chen. Text2Event: Controllable sequence-to-structure generation for end-to-end event extraction. In _Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)_, pages 2795-2806, Online, August 2021. Association for Computational Linguistics.
* [17] Yaojie Lu, Qing Liu, Dai Dai, Xinyan Xiao, Hongyu Lin, Xianpei Han, Le Sun, and Hua Wu. Unified structure generation for universal information extraction. In _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 5755-5772, Dublin, Ireland, May 2022. Association for Computational Linguistics.
* [18] Haoran Luo, Haihong E, Yuhao Yang, Yikai Guo, Mingzhi Sun, Tianyu Yao, Zichen Tang, Kaiyang Wan, Meina Song, and Wei Lin. HAHE: Hierarchical attention for hyper-relational knowledge graphs in global and local level. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, _Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 8095-8107, Toronto, Canada, July 2023. Association for Computational Linguistics.
* [19] OpenAI. Gpt-4 technical report, 2023.
* [20] Paolo Rosso, Dingqi Yang, and Philippe Cudre-Mauroux. Beyond triplets: Hyper-relational knowledge graph embedding for link prediction. In _Proceedings of The Web Conference 2020_, WWW '20, page 1885-1896, New York, NY, USA, 2020. Association for Computing Machinery.
* [21] Denny Vrandecic and Markus Krotzsch. Wikidata: A free collaborative knowledgebase. _Commun. ACM_, 57(10):78-85, sep 2014.
* [22] Jue Wang and Wei Lu. Two are better than one: Joint entity and relation extraction with table-sequence encoders. In _Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)_, pages 1706-1721, Online, November 2020. Association for Computational Linguistics.
* [23] Quan Wang, Haifeng Wang, Yajuan Lyu, and Yong Zhu. Link prediction on n-ary relational facts: A graph-based approach. In _Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021_, pages 396-407, Online, August 2021. Association for Computational Linguistics.

* [24] Yijun Wang, Changzhi Sun, Yuanbin Wu, Hao Zhou, Lei Li, and Junchi Yan. UniRE: A unified label space for entity relation extraction. In _Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)_, pages 220-231, Online, August 2021. Association for Computational Linguistics.
* [25] Xiang Wei, Xingyu Cui, Ning Cheng, Xiaobin Wang, Xin Zhang, Shen Huang, Pengjun Xie, Jinan Xu, Yufeng Chen, Meishan Zhang, Yong Jiang, and Wenjuan Han. Zero-shot information extraction via chatting with chatgpt, 2023.
* [26] Jianfeng Wen, Jianxin Li, Yongyi Mao, Shini Chen, and Richong Zhang. On the representation and embedding of knowledge bases beyond binary relations. In Subbarao Kambhampati, editor, _Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence, IJCAI 2016, New York, NY, USA, 9-15 July 2016_, pages 1300-1307. IJCAI/AAAI Press, 2016.
* [27] Deming Ye, Yankai Lin, Peng Li, and Maosong Sun. Packed levitated marker for entity and relation extraction. In _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 4904-4917, Dublin, Ireland, May 2022. Association for Computational Linguistics.
* Association for Computational Linguistics, July 2015. Outstanding Paper Award.
* [29] Fuzheng Zhang, Nicholas Jing Yuan, Defu Lian, Xing Xie, and Wei-Ying Ma. Collaborative knowledge base embedding for recommender systems. In _Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining_, KDD '16, page 353-362, New York, NY, USA, 2016. Association for Computing Machinery.
* [30] Zexuan Zhong and Danqi Chen. A frustratingly easy approach for entity and relation extraction. In _Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies_, pages 50-61, Online, June 2021. Association for Computational Linguistics.
* [31] Yuchen Zhuang, Yinghao Li, Junyang Zhang, Yue Yu, Yingjun Mou, Xiang Chen, Le Song, and Chao Zhang. ReSel: N-ary relation extraction from scientific text and tables by learning to retrieve and select. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, _Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing_, pages 730-744, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics.

## Appendix A Supplement to Data Augmentation

In addition to the _hyper-relational schema_, the data augmentation strategies for other schemas are as follows:

For _event-based schema_, given an event-based fact (\(r_{1},r_{2},A,r_{3},B,r_{4},C\)), we consider keeping the main relation \(r_{1}\) unchanged, and swapping other key-value pairs, \(\{r_{2},A\}\), \(\{r_{3},B\}\), and \(\{r_{4},C\}\), positionally, also as labeled training span-tuple cases. Thus \(R_{ev}(A,B,C)=(r_{1},r_{2},r_{3},r_{4})\) can be augmented with 6 orders of span-tuples:

\[\begin{cases}R_{ev}(A,B,C)=(r_{1},r_{2},r_{3},r_{4}),\\ R_{ev}(B,A,C)=(r_{1},r_{3},r_{2},r_{4}),\\ R_{ev}(A,C,B)=(r_{1},r_{2},r_{4},r_{3}),\\ R_{ev}(B,C,A)=(r_{1},r_{3},r_{4},r_{2}),\\ R_{ev}(C,A,B)=(r_{1},r_{4},r_{2},r_{3}),\\ R_{ev}(C,B,A)=(r_{1},r_{4},r_{3},r_{2}).\end{cases}\] (7)

For _role-based schema_, given a role-based fact (\(r_{1},A,r_{2},B,r_{3},C\)), we consider swapping key-value pairs, \(\{r_{1},A\}\), \(\{r_{2},B\}\), and \(\{r_{3},C\}\), positionally, also as labeled training span-tuple cases. Thus \(R_{ro}(A,B,C)=(r_{1},r_{2},r_{3})\) can be augmented with 6 orders of span-tuples:

\[\begin{cases}R_{ro}(A,B,C)=(r_{1},r_{2},r_{3}),\\ R_{ro}(B,A,C)=(r_{2},r_{1},r_{3}),\\ R_{ro}(A,C,B)=(r_{1},r_{3},r_{2}),\\ R_{ro}(B,C,A)=(r_{2},r_{3},r_{1}),\\ R_{ro}(C,A,B)=(r_{3},r_{1},r_{2}),\\ R_{ro}(C,B,A)=(r_{3},r_{2},r_{1}).\end{cases}\] (8)

For _hypergraph-based schema_, given a hypergraph-based fact (\(r_{1},A,B,C\)), we consider keeping the main relation \(r_{1}\) unchanged, and swapping entities, \(A\), \(B\), and \(C\), positionally, also as labeled training span-tuple cases. Thus \(R_{hg}(A,B,C)=(r_{1})\) can be augmented with 6 orders of span-tuples:

\[\begin{cases}R_{hg}(A,B,C)=(r_{1}),\\ R_{hg}(B,A,C)=(r_{1}),\\ R_{hg}(A,C,B)=(r_{1}),\\ R_{hg}(B,C,A)=(r_{1}),\\ R_{hg}(C,A,B)=(r_{1}),\\ R_{hg}(C,B,A)=(r_{1}).\end{cases}\] (9)

## Appendix B Supplement to Hetero-ordered Merging

In addition to the _hyper-relational schema_, the hetero-ordered merging strategies for other schemas are as follows:For _event-based schema_ (\(n_{r}=4\)), we combine the predicted probabilities of four labels \(\mathbf{P}_{1},\mathbf{P}_{2},\mathbf{P}_{3},\mathbf{P}_{4}\) in 6 orders to (\(A,B,C\)) order as follows:

\[\begin{cases}\mathbf{P}_{1}=\mathbf{P}_{1}^{(ABC)}+\mathbf{P}_{1}^{(BAC)}+ \mathbf{P}_{1}^{(ACB)}\\ \qquad+\mathbf{P}_{1}^{(BCA)}+\mathbf{P}_{1}^{(CAB)}+\mathbf{P}_{1}^{(CBA)},\\ \mathbf{P}_{2}=\mathbf{P}_{2}^{(ABC)}+\mathbf{P}_{3}^{(BAC)}+\mathbf{P}_{2}^{( ACB)}\\ \qquad+\mathbf{P}_{4}^{(BCA)}+\mathbf{P}_{3}^{(CAB)}+\mathbf{P}_{4}^{(CBA)},\\ \mathbf{P}_{3}=\mathbf{P}_{3}^{(ABC)}+\mathbf{P}_{2}^{(BAC)}+\mathbf{P}_{4}^{( ACB)}\\ \qquad+\mathbf{P}_{2}^{(BCA)}+\mathbf{P}_{4}^{(CAB)}+\mathbf{P}_{3}^{(CBA)},\\ \mathbf{P}_{4}=\mathbf{P}_{4}^{(ABC)}+\mathbf{P}_{4}^{(BAC)}+\mathbf{P}_{3}^{( ACB)}\\ \qquad+\mathbf{P}_{3}^{(BCA)}+\mathbf{P}_{2}^{(CAB)}+\mathbf{P}_{2}^{(CBA)}. \end{cases}\] (10)

Then, we take the maximum probability to obtain labels \(r_{1},r_{2},r_{3},r_{4}\), forming a 3-ary relational fact (\(r_{1},r_{2},A,r_{3},B,r_{4},C\)) and filter it out if there are null-label in (\(r_{1},r_{2},r_{3},r_{4}\)).

For _role-based schema_ (\(n_{r}=3\)), we combine the predicted probabilities of three labels \(\mathbf{P}_{1},\mathbf{P}_{2},\mathbf{P}_{3}\) in 6 orders to (\(A,B,C\)) order as follows:

\[\begin{cases}\mathbf{P}_{1}=\mathbf{P}_{1}^{(ABC)}+\mathbf{P}_{2}^{(BAC)}+ \mathbf{P}_{1}^{(ACB)}\\ \qquad+\mathbf{P}_{3}^{(BCA)}+\mathbf{P}_{2}^{(CAB)}+\mathbf{P}_{3}^{(CBA)},\\ \mathbf{P}_{2}=\mathbf{P}_{2}^{(ABC)}+\mathbf{P}_{1}^{(BAC)}+\mathbf{P}_{3}^{( ACB)}\\ \qquad+\mathbf{P}_{1}^{(BCA)}+\mathbf{P}_{3}^{(CAB)}+\mathbf{P}_{2}^{(CBA)},\\ \mathbf{P}_{3}=\mathbf{P}_{3}^{(ABC)}+\mathbf{P}_{3}^{(BAC)}+\mathbf{P}_{2}^{( ACB)}\\ \qquad+\mathbf{P}_{2}^{(BCA)}+\mathbf{P}_{1}^{(CAB)}+\mathbf{P}_{1}^{(CBA)}. \end{cases}\] (11)

Then, we take the maximum probability to obtain labels \(r_{1},r_{2},r_{3}\), forming a 3-ary relational fact (\(r_{1},A,r_{2},B,r_{3},C\)) and filter it out if there are null-label in (\(r_{1},r_{2},r_{3}\)).

For _hypergraph-based schema_ (\(n_{r}=1\)), we combine the predicted probabilities of one label \(\mathbf{P}_{1}\) in 6 orders to (\(A,B,C\)) order as follows:

\[\begin{cases}\mathbf{P}_{1}=\mathbf{P}_{1}^{(ABC)}+\mathbf{P}_{1}^{(BAC)}+ \mathbf{P}_{1}^{(ACB)}\\ \qquad+\mathbf{P}_{1}^{(BCA)}+\mathbf{P}_{1}^{(CAB)}+\mathbf{P}_{1}^{(CBA)}. \end{cases}\] (12)

Then, we take the maximum probability to obtain labels \(r_{1}\), forming a 3-ary relational fact (\(r_{1},A,B,C\)) and filter it out if \(r_{1}\) is null-label.

## Appendix C Construction of Dataset

Based on the original _hyper-relational schema_ on HyperRED dataset [5], we construct other three schemas (event-based, role-based, and hypergraph-based) for fine-grained n-ary RE. Firstly, we view the main relation in the _hyper-relational schema_ as the event type in the _event-based schema_, combine the head entity and tail entity with two extra head key and tail key to convert them into two key-value pairs, and remain the auxiliary key-value pairs in the _hyper-relational schema_. Taking '_Einstein received his Doctorate degree in Physics from the University of Zurich._' as an example, it can be represented as (_Einstein, educated, University of Zurich, {_academic_major, Physics_}, {_academic_degree, Doctorate_}) in the _hyper-relational schema_ and (_education_, {_trigger, received_}, {_person, Einstein_}, {_college, University of Zurich_}, {_academic_major, Physics_},{_academic_degree, Doctorate_}) in the _event-based schema_. Secondly, we remove the event type in the _event-based schema_ to obtain the _role-based schema_. Thirdly, we remove all the keys in key-value pairs and remain the relation to build the _hypergraph-based schema_.

Baseline Settings

Firstly, for the original _hyper-relational schema_ of HyperRED, we adopted the same baselines as in the CubeRE paper [5] to compare with Text2NKG:

Generative Baseline:Generative Baseline uses BART [14], a sequence-to-sequence model, to transform input sentences into a structured text sequence.

**Pipeline Baseline:** Pipeline Baseline uses UniRE [24] to extract relation triplets in the first stage and a span extraction model based on BERT-Tagger [6] to extract value entities and corresponding qualifier labels in the second stage.

**CubeRE:** CubeRE [5] is the only hyper-relational extraction model that uses a cube-filling model inspired by table-filling approaches and explicitly considers the interaction between relation triplets and qualifiers.

Secondly, for the _event-based schema_, _role-based schema_, and _hypergraph-based schema_, we added the following baselines to further validate the effect of Text2NKG on the fine-grained N-ary relation fact extraction task in the HyperRED dataset:

**Text2Event:** Text2Event [5] is a classic model in the Event extraction domain. However, it is not applicable to extractions of the _hyper-relational schema_. For the _role-based schema_ extraction, we retained the key without referring to the main relation, while for the _hypergraph-based schema_ extraction, we retained the main relation without referring to the key to get the final result for comparison.

**UIE / LasUE:** UIE [17] and LasUE [9] are unified information extraction models that can handle most tasks like NER, RE, EE, etc. However, they are still only suitable for event extraction in the multi-relational extraction domain and are not applicable to extractions of the _hyper-relational schema_. Therefore, we adopted the same approach as with Text2Event to compare with Text2NKG.

Thirdly, under the impact of the wave of large-scale language models brought about by ChatGPT on traditional natural language processing tasks, we added unsupervised large models as baselines to compare with Text2NKG in the n-ary RE tasks of the four schemas.

**ChatGPT / GPT4:** Using different prompts, we tested the latest state-of-the-art large-scale pre-trained language models ChatGPT [25] and GPT-4 [19] in an unsupervised manner, evaluating their performance on the extraction of the four schemas.

## Appendix E Hyperparameter Settings

We use the grid search method to select the optimal hyperparameter settings for both Text2NKG with Bert-base and Bert-large. We use the same hyperparameter settings in Text2NKG with different encoders. The hyperparameters that we can adjust and the possible values of the hyperparameters are first determined according to the structure of our model in Table 4. Afterward, the optimal hyperparameters are shown in **bold**.

## Appendix F Model Training Details

We train 10 epochs on HyperRED with the optimal combination of hyperparameters. Text2NKG and all its variants have been trained on a single NVIDIA A100 GPU. Using our optimal hyperparameter settings, the time required to complete the training on HyperRED is 4h with BERT-base encoder and 10h with BERT-large encoder.

\begin{table}
\begin{tabular}{c c} \hline \hline
**Hyperparameter** & **HyperRED** \\ \hline \(\alpha\) & \(\{1.0,0.1,\textbf{0.01},0.001\}\) \\ Train batch size & \(\{2,4,\textbf{8},16\}\) \\ Eval batch size & \(\{\textbf{1}\}\) \\ Learning rate & \(\{1e-5,\textbf{2e-5},5e-5\}\) \\ Max sequence length & \(\{128,\textbf{256},512,1024\}\) \\ Weight decay & \(\{\textbf{0.0},0.1,0.2,0.3\}\) \\ \hline \hline \end{tabular}
\end{table}
Table 4: Hyperparameter Selection.

Further Discussions

### How does ChatGPT perform in Fine-grained N-ary RE tasks?

We have tried to use LLM APIs such as ChatGPT and GPT to do similar n-ary RE tasks, i.e., prompting model input and output formats for extraction. The advantage of ChatGPT is that it can perform similar tasks in a few-shot situation, however, for building high-quality knowledge graphs, the performance and the fineness of the n-ary RE are much lower than Text2NKG. This is because ChatGPT is not good at multi-label classification tasks that contain less semantic interpretation. When the number of labels of relations in our relation collection is very large, we need to write a very long prompt to tell the LLM about our label candidate collection, which again leads to the problem of forgetting. Therefore, we have tried numerous prompt templates to enhance the extraction effect of ChatGPT, however, on fine-grained n-ary RE task, the best result of ChatGPT can only reach about 10% of \(F_{1}\) value on HyperRED, which is much lower than the result of 80%+ \(F_{1}\) value of Text2NKG.

However, advanced LLMs such as ChatGPT are a good idea for training dataset generation for Text2NKG in such tasks to save some manual labor to only verify and correct the training items generated. For future work, we will continue our research in this direction and try to combine large language models with Text2NKG-like supervised models for automated fine-grained n-ary RE for n-ary relational knowledge graph construction.

### Why first Extracting 3-ary facts and then Merging them into N-ary Facts?

We use output merging to address the dynamic changes in the number of elements in n-ary relational facts. The atomic unit of an n-ary fact includes a 3-ary fact with three entities. For instance, in the hyper-relational fact (_Einstein, educated_at, University of Zurich, degree: Doctorate degree, major: Physics_), the Text2NKG algorithm allows us to extract two 3-ary atomic facts: (_Einstein, educated_at, University of Zurich, degree: Doctorate degree_) and (_Einstein, educated_at, University of Zurich, major: Physics_). These are then merged based on the same primary triple (_Einstein, educated_at, University of Zurich_) to form a 4-ary fact. The same principle applies to facts of higher arities.

As another example demonstrating the problem with merging binary relations: consider the statement "_Einstein received his Bachelor's degree in Mathematics and his Doctorate degree in Physics._" When represented as binary relations, the facts become (_Einstein, degree, Doctorate degree_), (_Einstein, major, Physics_), (_Einstein, degree, Bachelor_), and (_Einstein, major, Mathematics_). With this representation, we cannot merge these binary relation facts effectively because there's no way to determine whether _Einstein's doctoral major_ was _Physics_ or _Mathematics_. This necessitates the use of NKG's n-ary relationship facts to represent this information, as seen in (_Einstein, degree, Doctorate degree, major, Physics_).

Therefore, using binary facts, we can't merge them into n-ary facts based on shared elements within these facts. On the other hand, using facts with four entities or more makes it challenging to effectively extract 3-ary atomic facts.

In Section 5.6 and Figure 5(a), we also analyzed the effects and detailed insights of unsupervised extraction of arbitrary-arity facts.

### How Text2NKG can address Long Contexts with Relations spread across Various Sentences?

As long as the text to be extracted is a lengthy piece with entities annotated, it can undergo long-form n-ary relation extraction. The maximum text segment size for our proposed method depends on the maximum text length that a transformer-based encoder can accept, such as Bert-base and Bert-large, which have a maximum limit of 512. To extract from larger documents, we simply need to switch to encoders with larger context length, which all serve as the encoder portion of Text2NKG and are entirely decoupled from the n-ary relation extraction technique we propose. This is one of the advantages of Text2NKG. Its primary focus is to address the order and combination issues of multi-ary relationships. We can seamlessly combine a transformer encoder that supports long texts with Span-tuple Multi-label Classification to process n-ary relation extraction in long chapters.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The abstract and introduction accurately reflect the paper's contributions and scope. The claims made are supported by the detailed methodology and experimental results sections. Text2NKG demonstrates improvements in fine-grained n-ary relation extraction, supports multiple NKG schemas, and achieves state-of-the-art performance as claimed. The public availability of code and datasets further supports the paper's transparency and reproducibility. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: The paper discusses several limitations, including the constraint of supervised training, the suggestion to combine supervised and unsupervised models, and the handling of long contexts with appropriate encoders. These points are addressed in the comparison with large language models and in discussions about future work and handling long texts. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their bestjudgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: The paper provides a comprehensive set of assumptions and proofs for its theoretical results. These are detailed in both the main sections and the appendices. For instance, the hetero-ordered merging strategy and the output merging methodology are explained with equations and detailed descriptions of the processes involved. Additionally, the assumptions for the span-tuple multi-label classification method and the handling of null-label weights are clearly stated, with mathematical formulations provided to support the theoretical claims. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: The paper includes detailed information necessary to reproduce the main experimental results. It provides a comprehensive description of the datasets used, the baselines for comparison, the experimental setup, and the evaluation metrics. Additionally, the paper mentions the use of a single NVIDIA A100 GPU and details the hyperparameters and training environment in the appendices. The inclusion of the link to the anonymous GitHub repository ensures that the code and datasets are accessible for reproducibility. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.

* While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: The paper provides open access to the data and code, including a link to an anonymous GitHub repository. It also includes detailed instructions on how to reproduce the main experimental results, covering data access, preparation, and the specific commands and environment needed to run the experiments. These details are described in the supplemental material and appendices. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: The paper specifies all the necessary training and test details, including data splits, hyperparameters, and the type of optimizer used. This information is provided in the main text and further elaborated in the appendices, ensuring that the experimental setup is fully transparent and reproducible.

Guidelines:

* The answer NA means that the paper does not include experiments.
* The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.
* The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: The paper reports error bars and provides appropriate information about the statistical significance of the experiments. The factors of variability captured by the error bars are clearly stated, and the method for calculating the error bars is explained. The assumptions made and whether the error bar represents the standard deviation or the standard error of the mean are also clearly mentioned. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors). * It should be clear whether the error bar is the standard deviation or the standard error of the mean. * It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. * For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). * If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: The paper provides detailed information on the compute resources used for the experiments. It specifies that all experiments were conducted on a single NVIDIA A100 GPU. The training details include the time required for training on HyperRED, which is 4 hours with the BERT-base encoder and 10 hours with the BERT-large encoder. The optimal hyperparameter settings and the use of the Adam optimizer are also described, along with the number of epochs and batch sizes. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.

* The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: The research described in the paper conforms to the NeurIPS Code of Ethics. We have ensured transparency, reproducibility, and ethical use of the data and models. We have also made the code and datasets publicly available to support open science and reproducibility. There is no indication of any ethical violations or concerns regarding the research process or outcomes. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: The paper discusses potential positive societal impacts, such as improving the quality of knowledge graphs and enabling more accurate information extraction from text. It also considers potential negative impacts, such as the possibility of misuse of the technology for disinformation or unfair decision-making. We suggest that combining small supervised models with large unsupervised models could help mitigate some of these risks by improving the accuracy and robustness of the extraction process. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. * The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. * The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. * If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards**Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [Yes] Justification: The paper addresses the potential misuse of the proposed methods and models. It emphasizes the importance of responsible use and includes strategies to mitigate risks. We suggest combining small supervised models with large unsupervised models to balance solving the cold-start problem and fine-grained extraction. They highlight that unsupervised large language models like ChatGPT and GPT-4 cannot accurately distinguish closely related relations, which implies careful consideration for controlled use. They also recommend specific usage guidelines and restrictions to ensure safe and ethical deployment. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: The paper properly credits the creators and original owners of the assets used. It cites the relevant datasets and models, including their sources and versions. We ensure that the licenses and terms of use are explicitly mentioned and respected. This information is detailed in the references and the supplemental material, where the datasets and models used are listed along with their corresponding licenses. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. * If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes]Justification: The new assets introduced in the paper are well documented. The documentation includes details about the training data, model architecture, and usage instructions. We provide structured templates for communicating the dataset/code/model details, including training procedures, licenses, limitations, and consent obtained from people whose data is used. This comprehensive documentation is available alongside the assets in the supplemental material and the anonymous GitHub repository. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve crowdsourcing experiments or research with human subjects. Therefore, this question is not applicable to the current work. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not involve crowdsourcing experiments or research with human subjects. Therefore, IRB approvals or equivalent reviews are not applicable to the current work. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. * We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. * For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.