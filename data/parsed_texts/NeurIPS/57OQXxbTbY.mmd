# Cal-DPO: Calibrated Direct Preference Optimization for Language Model Alignment

 Teng Xiao\({}^{1}\), Yige Yuan\({}^{2}\), Huaisheng Zhu\({}^{1}\), Mingxiao Li\({}^{3}\), Vasant G Honavar\({}^{1}\)

\({}^{1}\)Artificial Intelligence Research Laboratory, Pennsylvania State University

\({}^{2}\)University of Chinese Academy of Sciences, \({}^{3}\)Tencent AI Lab

{tengxiao,hvz5312,vhonavar}@psu.edu

yuanyige923@gmail.com,mingxiaoli@tencent.com

###### Abstract

We study the problem of aligning large language models (LLMs) with human preference data. Contrastive preference optimization has shown promising results in aligning LLMs with available preference data by optimizing the implicit reward associated with the policy. However, the contrastive objective focuses mainly on the relative values of implicit rewards associated with two responses while ignoring their actual values, resulting in suboptimal alignment with human preferences. To address this limitation, we propose calibrated direct preference optimization (Cal-DPO ), a simple yet effective algorithm. We show that substantial improvement in alignment with the given preferences can be achieved simply by calibrating the implicit reward to ensure that the learned implicit rewards are comparable in scale to the ground-truth rewards. We demonstrate the theoretical advantages of Cal-DPO over existing approaches. The results of our experiments on a variety of standard benchmarks show that Cal-DPO remarkably improves off-the-shelf methods. Code is available at https://github.com/tengxiao1/Cal-DPO.

## 1 Introduction

Aligning the behavior of large language models (LLMs) with human preferences is crucial for ensuring that the responses of a pretrained LLM are aligned with human or societal values and preferences [1, 2, 3]. In recent years, reinforcement learning from human feedback (RLHF)[2, 4] has become a standard approach for fine-tuning language models based on human preferences. RLHF involves first fitting a reward signal from human preference data and then using reinforcement learning (RL) algorithms such as PPO[5] to optimize language models to generate responses with high reward.

While RLHF shows impressive capabilities on diverse tasks ranging from programming to creative writing, its training process is unstable and complex [6, 7]. This potentially worsens the sample complexity and compromises efficient convergence. To address these issues, offline contrastive preference learning methods, which include DPO [7], IPO [8], and SLiC [9], have been proposed to replace RLHF with supervised learning on the preference data. These methods eliminate the need for explicit reward modeling by directly using the _likelihood_ of the policy to define an _implicit reward_ fitted to the preference data, and achieve notable efficiency and competitive performance [10].

While various contrastive preference learning methods employ different pairwise ranking losses, they share a common underlying motivation: Maximize the expected _relative_ difference between the implicit rewards associated with the chosen and rejected responses. Because the ranking loss is invariant to various score transformations (e.g., subtracting a constant), these methods tend to ignore the _absolute_ values of the rewards. Hence, while these methods learn to preserve the relative ordering between the likelihoods of the chosen and the rejected responses, they may reduce the likelihood of the chosen response. Figure 1 illustrates this behavior and its implications. In the case of DPO, a representative method of the contrastive methods, the likelihood of the chosen responsecounter-intuitively continues to decrease despite remaining higher than the likelihood of the rejected response. An undesirable consequence of this behavior is that the learned policy increases the likelihood of unknown out-of-distribution responses, resulting in poor performance. Maximizing the likelihood of the chosen response can be important in many practical applications, e.g., reasoning and mathematical problem solving [11; 12], limiting the applicability of contrastive preference learning.

The preceding discussion raises an important question with significant implications for how we align LLMs with human preferences: _How can we design a new objective that effectively alleviates this problem while ensuring that the learned policy theoretically converges to an optimal policy?_

Our answer to this question is \(\texttt{{Cal-DPO}}\), a simple yet effective framework for preference learning which optimizes the contrastive preference objective to maximize the relative differences between implicit rewards of chosen and rejected responses, while simultaneously ensuring that learned implicit rewards are calibrated to match the actual values of the ground-truth rewards (see Section 4.1 for a formal definition). The key intuition behind \(\texttt{{Cal-DPO}}\) is quite simple: If the implicit reward estimates from preference data are well-calibrated relative to the ground-truth rewards (meaning both lie on the same scale), we can prevent the likelihood (reward) of chosen responses from continually decreasing. Hence, \(\texttt{{Cal-DPO}}\) is designed to learn an implicit reward parameterized by the policy calibrated against the ground-truth reward. This can be achieved through a simple modification to the existing methods. For instance, \(\texttt{{Cal-DPO}}\) can be implemented on top of \(\texttt{{DPO}}\) with just one line of code and without any additional hyperparameters. Although we refer to our method as \(\texttt{{Cal-DPO}}\), it notably generalizes to other preference optimization methods such as IPO and \(\texttt{{SLiC}}\) (see Section 4.3). In addition, we theoretically demonstrate that \(\texttt{{Cal-DPO}}\) possesses several properties that are desirable for fine-tuning LLMs based on preferences, such as mode-seeking behavior, negative preference optimization, or "negative gradient" to push down the likelihood of undesirable responses [10].

**The main contributions of this paper are:** (i) We propose \(\texttt{{Cal-DPO}}\), a simple, effective, and intuitive framework for preference learning that facilitates alignment of language models. \(\texttt{{Cal-DPO}}\) aims to learn implicit reward functions for learning policy that are calibrated with respect to ground-truth rewards. (ii) We theoretically analyze the learning behaviors of \(\texttt{{Cal-DPO}}\) and prove that \(\texttt{{Cal-DPO}}\) is guaranteed to yield an optimal policy for preference learning. (iii) We present results of extensive experiments on a range of benchmark tasks, including controlled text generation [13], summarization [14], dialogue generation [1], and several reasoning tasks [15] that demonstrate that \(\texttt{{Cal-DPO}}\) consistently outperforms previous alignment methods for preference fine-tuning.

## 2 Related Work

Reinforcement Learning from Human Feedback (RLHF) is highly effective in aligning Large Language Models (LLMs) with human preferences [2; 4]. In RLHF, a reward model is trained from human preference data to map responses to a scalar reward, which aligns a policy using RL algorithms like PPO [5]. Although RLHF excels in instruction-following [2], safety alignment [1], and summarization [3], it requires a more complex training pipeline than supervised learning.

Recent work proposes simplifying RLHF by directly optimizing language models with contrastive learning on preference data, resulting in contrastive preference learning methods [10], such as DPO [7], IPO [8], and SLiC [9], NLHF [16], and their variants that incorporate rejection sampling,

Figure 1: The implicit reward dynamics during training of \(\texttt{{DPO}}\) and \(\texttt{{Cal-DPO}}\) on UltraFeedback data with the base model \(\texttt{{Zephyr-7b-sft}}\) reveal that the rewards for rejected data continuously decrease, while the margins between chosen and rejected data keep increasing. However, in \(\texttt{{DPO}}\), the rewards for chosen data decrease below zero, whereas in our \(\texttt{{Cal-DPO}}\), they keep increasing and remain positive. Our \(\texttt{{Cal-DPO}}\)  significantly outperforms \(\texttt{{DPO}}\) across reasoning benchmarks. More results on other datasets are provided in Section 5.

e.g., RSO [17]. While each of these methods works with different loss functions, the primary objective is to increase the likelihood gap between preferred and dispreferred responses [10]. Other works [18; 19; 20; 21; 22] apply contrastive preference learning iteratively and on-policy. However, as shown in Figure 1, the likelihood of the preferred response frequently decreases during contrastive training, adversely impacting performance on tasks such as coding and mathematical question answering [11; 12]. This paper addresses this limitation by calibrating contrastive objectives for preference learning and supports our approach with extensive theoretical guarantees in practice.

There is a substantial body of work that analyzes contrastive preference learning methods from different perspectives. For instance, [23; 24] study the performance of DPO under distribution shift and show that DPO is more susceptible to out-of-distribution responses than PPO. [25] theoretically analyze DPO with noisy preferences. [26; 27; 28] revisit the training objective of DPO from the perspective of noise contrastive estimation [29]. [30] theoretically analyze the gradient vector field of DPO and show that DPO decreases the probability of disfavored responses faster than it increases the probability of generating preferred responses. [31; 32; 33; 34] derive DPO within a token-level MDP formulation. Concurrent work [10] shows that DPO employs a negative gradient to push down the likelihood of undesirable, e.g., rejected responses, and implicitly exhibits a "mode-seeking" behavior. In this paper, our results are complementary. Specifically, we introduce Cal-DPO, which explicitly calibrates the rewards learned by DPO to match the scale of ground-truth rewards, while exhibiting "mode-seeking" behavior by minimizing the reverse KL divergence similar to RLHF.

Also worth mentioning is a body of work on calibration in supervised classification [35], learning-to-rank [36], unsupervised learning [35], and reinforcement learning [37; 38]. Recently, calibration has also been introduced into language models [39; 40]. In these works, calibration refers to the alignment of the model's assessed confidence scores with the likelihood of its responses being correct. In contrast, our focus is on calibrating the learned implicit rewards in contrastive preference learning to match ground-truth rewards in aligning language models with human preferences.

## 3 Notations and Preliminaries

**Problem Setup.** We consider the preference learning scenario as follows: let the text sequences \(\mathbf{x}=[x_{1},x_{2},\ldots]\) denote an input prompt, and \(\mathbf{y}_{w}=[y_{1},y_{2},\ldots]\) and \(\mathbf{y}_{l}=[y_{1},y_{2},\ldots]\) denote two responses, typically sampled from the reference policy \(\pi_{\text{ref}}(\mathbf{y}\mid\mathbf{x})\). The response pairs are then presented to human labelers (or an oracle) who express preferences for responses given the prompt, denoted as \(\mathbf{y}_{w}\succ\mathbf{y}_{l}\mid\mathbf{x}\), where \(\mathbf{y}_{w}\) and \(\mathbf{y}_{l}\) denote preferred and dispreferred responses, respectively. The preference distribution is commonly expressed using a latent reward model \(r(\mathbf{x},\mathbf{y})\) as:

\[p\left(\mathbf{y}_{w}\succ\mathbf{y}_{l}\mid x\right)=g\left(r(\mathbf{x}, \mathbf{y}_{w})-r\left(\mathbf{x},\mathbf{y}_{l}\right)\right),\] (1)

where \(g:\mathbb{R}\rightarrow[0,1]\) is a monotone non-decreasing function (with \(g(z)=1-g(-z)\)) that converts reward differences into winning probabilities). When \(g\) is the sigmoid function \(\sigma(x)=\frac{1}{1+e^{-x}}\), we get the Bradley-Terry (BT) preference model [41]. Given dataset \(\mathcal{D}\), containing feedback \((\mathbf{x},\mathbf{y}_{w},\mathbf{y}_{l})\), the goal is to learn a LLM policy \(\pi_{\theta}(\mathbf{y}\mid\mathbf{x})\) to align human preference by generating high rewards.

**RLHF.** Typically, given the reward function \(r(\mathbf{x},\mathbf{y})\), which dictates the human preferences, RLHF optimizes policy \(\pi_{\theta}\) for \(\mathbf{x}\) to maximize reward with the following RL objective:

\[\max_{\pi_{\theta}}\mathbb{E}_{\mathbf{y}\sim\pi_{\theta}(\mathbf{y}|\mathbf{ x})}\left[r(\mathbf{x},\mathbf{y})\right]-\beta\mathbb{D}_{\mathrm{KL}}\left[\pi_{ \theta}(\mathbf{y}\mid\mathbf{x})\|\pi_{\mathrm{ref}}(\mathbf{y}\mid \mathbf{x})\right],\] (2)

where \(\beta>0\) is an appropriate KL penalty coefficient. Due to the discrete nature of language generation, we typically optimize the RLHF objective in Equation (2) using RL algorithms, such as PPO [2; 5]. Although RLHF with PPO has achieved remarkable success, the training process of PPO is unstable because of the high variance of the policy gradient estimation [6].

**Reward Modeling.** One standard approach to reward modeling is to fit a reward function \(r_{\phi}(\mathbf{x},\mathbf{y})\) using the BT preference model in Equation (1). Specifically, the reward function \(r_{\phi}(\mathbf{x},\mathbf{y})\) can be estimated by maximizing the log-likelihood of the preference feedback \((\mathbf{x},\mathbf{y}_{w},\mathbf{y}_{l})\):

\[\mathcal{L}_{\text{RM}}(\phi;\mathbf{x},\mathbf{y}_{w},\mathbf{y}_{l})=-\log \sigma\left(r_{\phi}(\mathbf{x},\mathbf{y}_{w})-r_{\phi}\left(\mathbf{x}, \mathbf{y}_{l}\right)\right).\] (3)

**Contrastive Preference Learning.** To simplify RLHF, contrastive preference learning [42; 7; 9; 8] uses the log-likelihood of the learning policy to implicitly represent the reward function:

\[r_{\phi}(\mathbf{x},\mathbf{y})=\beta\Big{[}\log\pi_{\theta}(\mathbf{y}\mid \mathbf{x})-\log\pi_{\mathrm{ref}}(\mathbf{y}\mid\mathbf{x})+\log Z(\mathbf{x}) \Big{]}.\] (4)With this parameterization, DPO [7] aims to optimize \(\pi_{\theta}\) based on the BT model in Equation (3):

\[\mathcal{L}_{\mathrm{DPO}}=-\log\sigma\left(\beta\log\frac{\pi_{\theta}\left( \mathbf{y}_{w}\mid\mathbf{x}\right)}{\pi_{\mathrm{ref}}\left(\mathbf{y}_{w} \mid\mathbf{x}\right)}-\beta\log\frac{\pi_{\theta}\left(\mathbf{y}_{l}\mid \mathbf{x}\right)}{\pi_{\mathrm{ref}}\left(\mathbf{y}_{l}\mid\mathbf{x} \right)}\right).\] (5)

Technically, minimizing the DPO objective or any other pairwise contrastive objective, such as those of IPO [8] and SLiC [9], essentially amounts to maximizing the relative reward differences between chosen and rejected responses as shown in [10]. However, as noted earlier, these pairwise ranking objectives are not scale-calibrated and ignore the absolute values of the rewards. Thus, the likelihood of the chosen response can continue to decrease during training as long as the relative difference in the likelihoods between the chosen and rejected responses remains large (see Figure 1). This property has resulted in suboptimal performance, especially on reasoning and mathematical problem-solving [11; 12]. In this paper, we address this limitation by proposing a simple yet effective calibrated objective to calibrate the behavior of contrastive preference learning methods such as DPO.

## 4 Calibrated Direct Preference Optimization

We proceed to introduce Calibrated Direct Preference Optimization (\(\mathtt{Cal-DP0}\) ), a simple and effective modification of DPO. The key intuition behind \(\mathtt{Cal-DP0}\) is to calibrate the implicit reward function against the ground-truth rewards. Hence, \(\mathtt{Cal-DP0}\) is designed to learn an implicit reward parameterized by the policy calibrated against the ground-truth reward. This can be achieved through a simple modification of DPO [7], assuming, as in the case of DPO, that the preferences adhere to the BT preference model. Thus, \(\mathtt{Cal-DP0}\) can be implemented on top of DPO with just one line of code and without any additional hyperparameters. In principle, our idea of calibration can be applied to any contrastive preference learning algorithm such as IPO [8] and SLiC [9; 17] (see Section 4.3).

### The Calibrated Objective for Preference Optimization

To fine-tune a policy on preference feedback, we propose learning a reward model implicitly and using the log-likelihood of the policy to represent the estimated reward [7; 9; 8]. Specifically, we start by defining a preference score of \(\mathbf{y}_{w}\) relative to \(\mathbf{y}_{l}\), where the implicit reward is represented by \(\pi_{\theta}\).

\[h_{\theta}\left(\mathbf{x},\mathbf{y}_{w},\mathbf{y}_{l}\right)=\widehat{r}_ {\theta}(\mathbf{x},\mathbf{y}_{w})-\widehat{r}_{\theta}\left(\mathbf{x}, \mathbf{y}_{l}\right)\triangleq\log\frac{\pi_{\theta}(\mathbf{y}_{w}\mid \mathbf{x})}{\pi_{\mathrm{ref}}(\mathbf{y}_{w}\mid\mathbf{x})}-\log\frac{\pi_ {\theta}(\mathbf{y}_{l}\mid\mathbf{x})}{\pi_{\mathrm{ref}}(\mathbf{y}_{l} \mid\mathbf{x})}.\] (6)

Here \(\widehat{r}_{\theta}\left(\mathbf{x},\mathbf{y}\right)=\log\frac{\pi_{\theta} (\mathbf{y}\mid\mathbf{x})}{\pi_{\mathrm{ref}}(\mathbf{y}\mid\mathbf{x})}\) is an implicit reward defined by the training and reference policies \(\pi_{\theta}\) and \(\pi_{\mathrm{ref}}\), allowing us to express that, for any \(\theta\in\Theta\), the preference probabilities can be denoted as:

\[p_{\theta}\left(\mathbf{y}_{w}\succ\mathbf{y}_{l}\mid x\right)=\sigma\left(h_ {\theta}\left(\mathbf{x},\mathbf{y}_{w},\mathbf{y}_{l}\right)\right),\] (7)

where we assume that preferences adhere to the BT preference model as in DPO[7]. With preference probabilities expressed in terms of the learning policy, we can find the maximum likelihood estimate by minimizing the preference optimization loss based on the preference feedback:

\[\mathcal{L}_{\mathrm{BT}}(\theta;\mathbf{x},\mathbf{y}_{w},\mathbf{y}_{l})=- \log\sigma\Big{(}h_{\theta}\left(\mathbf{x},\mathbf{y}_{w},\mathbf{y}_{l} \right)\Big{)}=-\log\sigma\Big{(}\log\frac{\pi_{\theta}\left(\mathbf{y}_{w} \mid\mathbf{x}\right)}{\pi_{\mathrm{ref}}\left(\mathbf{y}_{w}\mid\mathbf{x} \right)}-\log\frac{\pi_{\theta}\left(\mathbf{y}_{l}\mid\mathbf{x}\right)}{\pi _{\mathrm{ref}}\left(\mathbf{y}_{l}\mid\mathbf{x}\right)}\Big{)}.\] (8)

We can note that this pairwise loss is equivalent to the DPO objective (without \(\beta\)) in Equation (5). As this contrastive pairwise loss is also not scale-calibrated, ignoring the absolute values of the rewards. Thus, we cannot guarantee that the estimated reward of the chosen response will increase and the reward of the rejected response will decrease, which tends to degrade the performance on math and reasoning benchmarks [11; 12; 43; 44]. We propose to address this limitation by explicitly constraining the implicit reward function \(\log(\pi_{\theta}(\mathbf{y}\mid\mathbf{x})/\pi_{\mathrm{ref}}(\mathbf{y}\mid \mathbf{x}))\) to a scale that matches the ground-truth reward \(r(\mathbf{x},\mathbf{y})/\beta\). Intuitively, if the learned implicit rewards are constrained to lie in the range of the ground-truth reward, we can prevent the rewards of chosen responses from continually decreasing. Formally, we define "calibration" with respect to ground-truth reward as follows:

**Definition 1**.: _(Calibration). An estimated implicit reward \(\log\frac{\pi_{\theta}(\mathbf{y}\mid\mathbf{x})}{\pi_{\mathrm{ref}}(\mathbf{y} \mid\mathbf{x})}\) for the LM policy \(\pi_{\theta}\) is called scale calibrated with respect to the ground truth reward if \(\log\frac{\pi_{\theta}(\mathbf{y}\mid\mathbf{x})}{\pi_{\mathrm{ref}}(\mathbf{y} \mid\mathbf{x})}=\frac{r(\mathbf{x},\mathbf{y})}{\beta},\forall(\mathbf{x}, \mathbf{y})\sim\mathcal{D}\)._Thus, in addition to the BT preference loss in Equation (8), we propose constraining the learned implicit reward to match the ground-truth reward through solving squared loss regression problems:

\[\mathcal{L}_{\mathrm{Cal}}(\theta;\mathbf{x},\mathbf{y})=\Big{(}\log\frac{\pi_{ \theta}(\mathbf{y}\mid\mathbf{x})}{\pi_{\text{ref}}(\mathbf{y}\mid\mathbf{x})} -\frac{r(\mathbf{x},\mathbf{y})}{\beta}\Big{)}^{2}.\] (9)

The calibration loss requires access to oracle rewards; however, in some scenarios, we may only have access to pairwise preference feedback. In such cases, we define the reward for preference feedback as follows: \(r(\mathbf{x},\mathbf{y}_{w})=1/2\) and \(r(\mathbf{x},\mathbf{y}_{i})=-1/2\), indicating \(\mathbf{y}_{w}\succ\mathbf{y}_{i}\mid\mathbf{x}\). Empirically, we find that this works quite well in practice. Combining this calibration loss with the BT preference loss in Equation (8), we get the following full loss of \(\mathtt{Cal-DPO}\) on human preference data:

\[\mathcal{L}_{\mathrm{Cal-DPO}}(\theta;\mathbf{x},\mathbf{y}_{w}, \mathbf{y}_{l})= -\log\sigma\big{(}\log\frac{\pi_{\theta}(\mathbf{y}_{w}\mid \mathbf{x})}{\pi_{\text{ref}}(\mathbf{y}_{w}\mid\mathbf{x})}-\log\frac{\pi_{ \theta}(\mathbf{y}_{l}\mid\mathbf{x})}{\pi_{\text{ref}}(\mathbf{y}_{l}\mid \mathbf{x})}\big{)}\] (10) \[+\big{(}\log\frac{\pi_{\theta}(\mathbf{y}_{w}\mid\mathbf{x})}{ \pi_{\text{ref}}(\mathbf{y}_{w}\mid\mathbf{x})}-\frac{1}{2\beta}\big{)}^{2}+ \big{(}\log\frac{\pi_{\theta}(\mathbf{y}_{l}\mid\mathbf{x})}{\pi_{\text{ref}} (\mathbf{y}_{l}\mid\mathbf{x})}+\frac{1}{2\beta}\big{)}^{2},\]

where our modifications to the standard BT preference model are straightforward and depicted in blue. Intuitively, \(\mathtt{Cal-DPO}\) learns an implicit reward parameterized by the LM policy that is "calibrated" against the ground-truth reward. Specifically, \(\mathtt{Cal-DPO}\) attempts to push the reward of the chosen response toward \(1/2\beta\) and the reward of the rejected response toward \(-1/2\beta\), ensuring that \(\pi_{\theta}(\mathbf{y}_{w}\mid\mathbf{x})>\pi_{\text{ref}}(\mathbf{y}_{w} \mid\mathbf{x})\) and \(\pi_{\theta}(\mathbf{y}_{l}\mid\mathbf{x})<\pi_{\text{ref}}(\mathbf{y}_{l} \mid\mathbf{x})\). This alignment keeps the estimate of the implicit reward consistent with the ground-truth reward. Our implementation of \(\mathtt{Cal-DPO}\) builds directly on the \(\mathtt{DPO}\) codebase with just one additional line (see pseudocode in Appendix B.1). Our experiments show that calibration via a simple square loss can consistently improve the performance of off-the-shelf \(\mathtt{DPO}\), demonstrating the potential of calibration for preference fine-tuning.

### Theoretical Analysis

Next, we proceed to present a theoretical analysis of \(\mathtt{Cal-DPO}\). We show that \(\mathtt{Cal-DPO}\) with reward calibration enjoys important properties that are desirable for fine-tuning LLMs with preferences, e.g., mode-seeking behavior, negative preference optimization ("negative gradient" property) to push down the likelihood of undesirable responses [10]. We also show that \(\mathtt{Cal-DPO}\) minimizes an upper bound on the standard KL-regularized RLHF in Equation (2). All proofs are provided in the Appendix 4.2.

We start by presenting our theoretical framework from a distribution matching perspective. We first interpret the RLHF objective as the optimization of a reverse KL-divergence between \(\pi_{\theta}\) and \(\pi^{*}\)[7]:

\[\mathcal{L}_{\mathrm{RL}}(\theta)=\beta\mathbb{D}_{\mathrm{KL}}\left[\pi_{ \theta}(\mathbf{y}\mid\mathbf{x})\|\pi^{*}(\mathbf{y}\mid\mathbf{x})\right]- \beta\log Z(\mathbf{x}),\] (11)

where \(\pi^{*}(\mathbf{y}|\mathbf{x})\propto\pi_{\text{ref}}(\mathbf{y}|\mathbf{x}) \exp(r(\mathbf{x},\mathbf{y})/\beta)\). The derivation is given in Appendix A.1. Since reverse KL can be difficult to optimize [7; 10], one can instead optimize the forward KL divergence:

\[\mathcal{L}_{\mathrm{MLE}}(\theta)=\mathbb{D}_{\mathrm{KL}}\left[\pi^{*}( \mathbf{y}\mid\mathbf{x})\|\pi_{\theta}(\mathbf{y}\mid\mathbf{x})\right]=- \mathbb{E}_{\mathbf{y}\sim\pi_{\text{ref}}(\mathbf{y}|\mathbf{x})}\Big{[} \frac{\exp(r(\mathbf{x},\mathbf{y})/\beta)}{\sum_{\mathbf{y}}\pi_{\text{ref}}( \mathbf{y}|\mathbf{x})\exp(r(\mathbf{x},\mathbf{y})/\beta)}\log\pi_{\theta}( \mathbf{y}|\mathbf{x})\Big{]},\] (12)

which is the weighted maximum likelihood loss. Although this objective provides a straightforward approach for preference fine-tuning [45; 46; 47], it leads to poor performance compared to RLHF, as shown by [10]. The poor performance of the preceding objective in Equation (12) relative to RLHF is due to the fact that it assigns a positive weight to all samples \(\mathbf{y}\) for a given \(\mathbf{x}\). Since the rewards are always positive, the likelihood loss always tries to increase the probability of a response even if it receives a much smaller reward compared to other responses. We note that the recent work of [10] also empirically demonstrates that the maximum likelihood criterion lacks the "negative gradient" property, resulting in poor performance of \(\mathtt{MLE}\) in Equation (12) compared to RLHF and \(\mathtt{DPO}\).

Because KL-regularized RLHF and the \(\mathtt{MLE}\) objective in Equation (12) are both population losses which involve the 'oracle' reward \(r(\mathbf{x},\mathbf{y})\), we also define a population loss for \(\mathtt{Cal-DPO}\) to facilitate a direct comparison between \(\mathtt{Cal-DPO}\) with KL-regularized RLHF and the \(\mathtt{MLE}\) loss of Equation(12):

\[\mathcal{L}_{\mathrm{Cal-DPO}}(\theta)= -\mathbb{E}_{\mathbf{y}\sim\pi_{\text{ref}}(\mathbf{y}|\mathbf{x})} \Big{[}\frac{\exp(r(\mathbf{x},\mathbf{y})/\beta)}{\sum_{\mathbf{y}}\pi_{ \text{ref}}(\mathbf{y}|\mathbf{x})\exp(r(\mathbf{x},\mathbf{y})/\beta)}\log \frac{(\pi_{\theta}(\mathbf{y}|\mathbf{x})/\pi_{\text{ref}}(\mathbf{y}|\mathbf{x}) )}{\sum_{\mathbf{y}}\pi_{\text{ref}}(\mathbf{y}|\mathbf{x})(\pi_{\theta}( \mathbf{y}|\mathbf{x})/\pi_{\text{ref}}(\mathbf{y}|\mathbf{x}))}\Big{]}\] \[+\mathbb{E}_{\mathbf{y}\sim\pi_{\text{ref}}(\mathbf{y}|\mathbf{x})} \Big{[}(\log\frac{\pi_{\theta}(\mathbf{y}\mid\mathbf{x})}{\pi_{\text{ref}}( \mathbf{y}\mid\mathbf{x})}-\frac{r(\mathbf{x},\mathbf{y})}{\beta})^{2}\Big{]},\] (13)

[MISSING_PAGE_FAIL:6]

### Generalizations and Extensions

We observe that our approach to calibrating the learned implicit reward from preference feedback is not limited to the DPO algorithm or the BT preference model. As we shall see below, the general idea behind Cal-DPO extends to other methods such as IPO and SLiC and other preference models. Instead of the sigmoid loss in Equation (7), SLiC[9; 17] minimizes a pairwise hinge loss:

\[\mathcal{L}_{\mathrm{SLiC}}(\theta)=\max\{0,1-\beta h_{\theta}\left(\mathbf{x },\mathbf{y}_{w},\mathbf{y}_{l}\right)\}.\] (16)

IPO[8] is a contrastive algorithm similar to DPO and minimizes the following pairwise square loss:

\[\mathcal{L}_{\mathrm{IPO}}(\theta)=(h_{\theta}\left(\mathbf{x},\mathbf{y}_{w },\mathbf{y}_{l}\right)-1/2\beta)^{2}\] (17)

A potential advantage of SLiC and IPO over DPO is that they do not require that the preference model be BT and can work with general preference probabilities. By combining our calibration loss in Equation (9), it is straightforward to define the calibrated counterparts of both \(\mathcal{L}_{\mathrm{SLiC}}\) and \(\mathcal{L}_{\mathrm{IPO}}\). Thus, our calibration approach can be generalized to work with SLiC and IPO as well (see Section 5.5).

## 5 Experiments

### Experimental Setup

**Datasets.** We evaluate Cal-DPO on four widely used datasets for preference fine-tuning: the UltraFeedback Binarized dataset [53; 54], Reddit TL;DR summarization dataset [14], Anthropic-HH dataset [1], and the IMDb sentiment dataset [13]. Details of the datasets are provided in Appendix C.1.

**Tasks and Evaluation.** Following previous work [7; 54], we evaluate methods fine-tuned on the UltraFeedback Binarized dataset across general reasoning benchmarks (MMLU-PRO [55], ARC [56], IFEval [57], BBH [58], GPQA [59]), and mathematical reasoning (GSM8K [60] and MATH [61]). For training on the UltraFeedback Binarized dataset, we utilize the same chat template used in [44] for all methods. We also use AlpacaEval 2.0[62], a benchmark for assessing LLM alignment with human preference. The Anthropic HH dataset is used for dialogue generation to produce helpful and harmless responses [7]. For summarization, we use the Reddit TL;DR dataset. For the dialogue generation and summarization tasks, we use GPT-4 for zero-shot pairwise evaluation, which is consistent with human judgments (see prompts in Appendix C.2.1). In the IMDb sentiment dataset, the goal is controlled text generation to produce positive sentiments from movie review prefixes [7]. We train a binary sentiment classifier and define the oracle reward as its log odds, and evaluate the policy on the trained reward model [7]. The task and evaluation details are given in Appendix C.2.

**Models.** For summarization and dialogue generation tasks, we use Pythia-2.8b[63] as our base model and the model after SFT as the reference model following [7]. For the IMDb controlled text generation task, both the policy and reward models are initialized from the GPT-2 Large model [64]. For tasks with the UltraFeedback Binarized dataset, we use the Zephyr-7b-sft model [54] as our base model to ensure alignment with previous work on LLM preference alignment [54].

**Baselines** We compare Cal-DPO with the following preference optimization methods: DPO [7], IPO [8], SLiC [9], CPO [65]. We also compare Cal-DPO with other variants of DPO: f-DPO [66], DPO-Positive (DPO) [11] and DPO+NLL [67] which combine DPO loss over preference pairs and the negative log-likelihood (NLL) loss over chosen responses. We also compare with weighted MLE in Equation (12). Besides DPO, we also implemented our calibration objective on top of SLiC [9] and IPO [8] (see Section 5.5). The implementation details are provided in Appendix B.1.

### Performance Comparison on Benchmarks

**Reasoning Benchmarks.** Table 2 compares the performance of Cal-DPO against other alignment methods on the UltraFeedback Binarized dataset. Our results show that Cal-DPO exhibits remarkable effectiveness in enhancing DPO's performance. The improvements are particularly notable on the IFEval and Math benchmarks, with relative gains exceeding 63.1% and 12.5% compared to the best baseline, respectively. These findings underscore the efficacy of Cal-DPO. We hypothesize that these improvements can be attributed to the calibration of implicit rewards performed by Cal-DPO as part of its training objective. Without proper calibration, the likelihood of selected samples decreases, resulting in suboptimal performance, especially in mathematical reasoning tasks, where the chosenresponses are very likely the ground-truth answers. Furthermore, Cal-DPO outperforms CPO, which combine DPO over preference pairs with the negative log-likelihood loss over the chosen response. We hypothesize that the superiority of Cal-DPO over CPO can be attributed to the conservative nature of the objectives optimized by the latter, which only affects the chosen response. In contrast, Cal-DPO calibrates implicit rewards for both chosen and rejected responses.

**Instruction-following Benchmarks.** To assess the ability of Cal-DPO on align with human instruction, we compare the performance of Cal-DPO and DPO on AlpacaEval 2.0 [62], an evaluator based on GPT-4 (version gpt-4-1106-preview) that produces the probability of preferring the evaluated model. Figure 2 shows the comparison in terms of both normal and length-controlled (LC) percentage of wins. We see that Cal-DPO demonstrates steady performance gains with the number of training iterations and outperforms SFT and DPO methods, which tend to produce longer responses.

### Performance Comparison with Human Preferences

We also designed experiments to explore learning from real human preferences, focusing on summarization and dialogue generation tasks. Specifically, we used the Reddit TL;DR dataset for summarization and the Anthropic-HH dataset for dialogue generation. Table 3 summarizes the GPT-4 evaluation results. These results show that Cal-DPO demonstrates a notable improvement over DPO and its variants compared to both the SFT and the chosen responses. Remarkably, Cal-DPO aligns better with human preferences than baselines, achieving win rates of at least 60% against the chosen responses. This indicates that Cal-DPO shows strong promise in terms of aligning with human preferences. Additionally, we provide examples generated by both DPO and Cal-DPO for both tasks in Appendix D.1. These examples show that GPT-4 consistently prefers Cal-DPO over baselines and the chosen responses in the dataset, demonstrating that Cal-DPO significantly improves DPO in terms of helpfulness and harmlessness of the generated responses.

\begin{table}
\begin{tabular}{l c c} \hline \hline
**Method** & **Reward \(\uparrow\)** & **Perplexity \(\downarrow\)** \\ \hline SFT & 0.539 & 35.47 \\ PPO & 0.626 & 35.05 \\ \hline DPO & 0.617 & 34.21 \\ f-DPO & 0.615 & 36.39 \\ DPOP & 0.632 & 35.58 \\ DPO+NLL & 0.627 & 34.08 \\ \hline Cal-DPO & \(\mathbf{0.645}\) & \(\mathbf{32.31}\) \\ \hline \hline \end{tabular}
\end{table}
Table 4: The comparison on IMDb dataset in terms of the reward and perplexity.

\begin{table}
\begin{tabular}{l c c c c c c c} \hline \hline
**Method** (\(\downarrow\)) / **Dataset** (\(\rightarrow\)) & **MMLU-PRO** & **IFEval** & **BBH** & **GPQA** & **MATH** & **GSM8K** & **ARC** \\ \hline zephyr-7b-sft-full [54] & 27.64 & 3.21 & 41.09 & 29.36 & 2.04 & 28.13 & 58.28 \\ \hline DPO [7] & 26.73 & 10.49 & 43.27 & 28.44 & 1.36 & 21.76 & 61.26 \\ f-DPO [66] & 25.96 & 11.05 & 42.39 & 28.05 & 1.27 & 23.18 & 62.01 \\ SLiC [9] & 26.52 & 12.45 & 42.33 & 27.93 & 1.38 & 33.74 & 55.38 \\ IPO [8] & 25.87 & 11.52 & 40.59 & 28.15 & 1.25 & 27.14 & 60.84 \\ CPO [65] & 27.04 & 13.32 & 42.05 & 28.45 & 2.15 & 33.06 & 57.00 \\ \hline Cal-DPO & **28.38** & **21.72** & **43.55** & **29.78** & **2.42** & **34.87** & **63.23** \\ \hline \hline \end{tabular}
\end{table}
Table 2: Performance comparison between our Cal-DPO and other methods on the UltraFeedback Binarized dataset using zephyr-7b-sft-full and the same chat templates provided by the alignment-handbook across various reasoning benchmarks in Open LLM Leaderboards using Language Model Evaluation Harness (v0.4.0).

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline
**Dataset** (\(\rightarrow\)) & \multicolumn{4}{c}{**TL;DR Summarization**} & \multicolumn{4}{c}{**Anthropic-HH**} \\ \cline{2-7}
**Method** (\(\downarrow\)) / **Metric** (\(\rightarrow\)) & **vs SFT** & **vs Chosen** & **Average** & **vs SFT** & **vs Chosen** & **Average** \\ \hline DPO [7] & 71.22 & 57.58 & 64.40 & 69.32 & 59.35 & 64.34 \\ SLiC [9] & 68.61 & 55.72 & 62.17 & 65.52 & 57.71 & 61.62 \\ IPO [8] & 72.17 & 56.51 & 64.34 & 63.19 & 55.12 & 59.16 \\ CPO [65] & 73.13 & 58.89 & 66.01 & 72.30 & 63.39 & 67.86 \\ \hline f-DPO [66] & 66.19 & 51.37 & 58.78 & 60.21 & 52.38 & 56.30 \\ DPO [11] & 72.95 & 58.82 & 65.89 & 68.77 & 57.91 & 63.34 \\ DPO+NLL [67] & 69.37 & 55.26 & 62.31 & 65.34 & 55.28 & 60.31 \\ \hline Cal-DPO & **75.61** & **59.37** & **67.49** & **73.52** & **64.61** & **69.07** \\ \hline \hline \end{tabular}
\end{table}
Table 3: Win rates computed by GPT-4 against the SFT generated texts and the chosen texts on the TL;DR summarization and Anthropic-HH datasets. Best results s are highlighted in **boldface**.

### Performance Comparison on Controlled Evaluation

We conducted experiments on the IMDB dataset to assess the generation of positive movie reviews. The task requires the model to provide positive and fluent completions of movie reviews based on given partial input texts. To perform a controlled evaluation, we trained a binary sentiment classifier on the IMDB dataset and defined the oracle reward as its log odds, following [7]. The reward score of the reward model then serves as an in-domain proxy for the unknown ground-truth reward used in evaluation. The results of IMDB sentiment generation are listed in Table 4. We used the reward score of the reward model and the perplexity of GPT-2 [64] to assess alignment performance. We observe that (1) PPO, DPO, and Cal-DPO can align the SFT model with the preference of the reward model, as evidenced by the increasing reward score, and (2) Cal-DPO performs better in terms of reward score and perplexity than both PPO and DPO, which confirms our theoretical results that the policy trained by Cal-DPO can effectively maximize rewards.

### Further Analysis

**Training Dynamics.** We also investigated the reward patterns during the training process of Cal-DPO. Figure 3 presents the reward patterns for Cal-DPO and DPO on the TL;DR summarization dataset. We observe that the rewards of the rejected data keep decreasing, and the margins between the chosen and rejected responses keep increasing. However, in the case of DPO, the rewards of the chosen responses fall below zero, whereas they continue to increase with Cal-DPO, underscoring the utility of reward calibration in LLM alignment. These results are similar to those on the UltraFeedback dataset shown in Figure 1, verifying our motivation and the effectiveness of Cal-DPO.

**Generalization to other objectives.** As mentioned in Section 4.3, our approach to calibrating the learned implicit reward from preference feedback generalizes in a straightforward manner to other pairwise preference optimization methods including IPO and SLiC. To show this, we implemented the calibration objective of Cal-DPO for IPO and SLiC, yielding their calibrated counterparts Cal-IPO and Cal-SLiC, respectively. Figure 3 shows the comparison on the Anthropic-HH dataset. We observe that combining our calibration objective can consistently improve standard OCPL methods for LLM preference alignment, demonstrating the broader utility of preferance calibration.

**Coefficient Parameter.** We investigate the effect of the coefficient \(\beta\) and present an ablation study to analyze the performance of Cal-DPO on various tasks by varying \(\beta\). Figure 4 in the Appendix shows the performance with different values of \(\beta\). We observe that \(\beta\) plays an important role in Cal-DPO. A small \(\beta\) typically improves the model performance, whereas a too large \(\beta\) encourages the policy to remain close to the reference policy, leading to poor performance.

## 6 Conclusion and Limitations

We have presented Cal-DPO, a simple yet effective fine-tuning approach for aligning LLMs with human preference. Cal-DPO incorporates a simple preference calibration term that modifies the

Figure 3: (Left two) The training dynamics of DPO and Cal-DPO on the TL;DR Summarization dataset. (Right) The performance of SLiC and IPO, and their calibrated counterparts Cal-IPO and Cal-SLiC. We provide additional results on the Anthropic-HH and IMDb datasets in Appendix D.

Figure 2: AlpacaEval 2.0 evaluation results of models trained with UltraFeedback Binarized dataset. The DPO and Cal-DPO are both initialized from the SFT model 2ephyr-7b-sft-full.

behavior of the objective of contrastive preference learning methods such as DPO. The key idea behind Cal-DPO is to ensure that the learned implicit rewards lie within the same range as the ground-truth rewards, yielding substantial gains in performance relative to the state-of-the-art baselines on several widely used benchmark data sets. We also demonstrate theoretically that Cal-DPO exhibits the desirable "negative gradients" and "mode-seeking" behavior.

A limitation of Cal-DPO is that it is currently limited to offline methods and does not consider on-policy learning where the policy can interact with the reward model during learning. It would be interesting to explore how the reward calibration idea used in Cal-DPO performs in the on-policy learning scenario. We leave this as an interesting direction for future work.

## Acknowledgments

The work of Honavar and Xiao was supported in part by grants from the National Science Foundation (2226025, 2225824), the National Center for Advancing Translational Sciences, and the National Institutes of Health (UL1 TR002014).

## References

* [1] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training a helpful and harmless assistant with reinforcement learning from human feedback. _arXiv preprint arXiv:2204.05862_, 2022.
* [2] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. _Advances in neural information processing systems_, pages 27730-27744, 2022.
* [3] Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul F Christiano. Learning to summarize with human feedback. _Advances in Neural Information Processing Systems_, 33:3008-3021, 2020.
* [4] Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences. _Advances in neural information processing systems_, 30, 2017.
* [5] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. _arXiv preprint arXiv:1707.06347_, 2017.
* [6] Logan Engstrom, Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Firdaus Janoos, Larry Rudolph, and Aleksander Madry. Implementation matters in deep policy gradients: A case study on ppo and trpo. In _International Conference on Learning Representations_, 2020.
* [7] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model. _Advances in Neural Information Processing Systems_, 36, 2024.
* [8] Mohammad Gheshlaghi Azar, Zhaohan Daniel Guo, Bilal Piot, Remi Munos, Mark Rowland, Michal Valko, and Daniele Calandriello. A general theoretical paradigm to understand learning from human preferences. In _International Conference on Artificial Intelligence and Statistics_, pages 4447-4455. PMLR, 2024.
* [9] Yao Zhao, Rishabh Joshi, Tianqi Liu, Misha Khalman, Mohammad Saleh, and Peter J Liu. Slic-hf: Sequence likelihood calibration with human feedback. _arXiv preprint arXiv:2305.10425_, 2023.
* [10] Fahim Tajwar, Anikait Singh, Archit Sharma, Rafael Rafailov, Jeff Schneider, Tengyang Xie, Stefano Ermon, Chelsea Finn, and Aviral Kumar. Preference fine-tuning of llms should leverage suboptimal, on-policy data. _arXiv preprint arXiv:2404.14367_, 2024.

* [11] Arka Pal, Deep Karkhanis, Samuel Dooley, Manley Roberts, Siddartha Naidu, and Colin White. Smaug: Fixing failure modes of preference optimisation with dpo-positive. _arXiv preprint arXiv:2402.13228_, 2024.
* [12] Lifan Yuan, Ganqu Cui, Hanbin Wang, Ning Ding, Xingyao Wang, Jia Deng, Boji Shan, Huimin Chen, Ruobing Xie, Yankai Lin, et al. Advancing llm reasoning generalists with preference trees. _arXiv preprint arXiv:2404.02078_, 2024.
* [13] Andrew Maas, Raymond E Daly, Peter T Pham, Dan Huang, Andrew Y Ng, and Christopher Potts. Learning word vectors for sentiment analysis. In _Proceedings of the 49th annual meeting of the association for computational linguistics: Human language technologies_, pages 142-150, 2011.
* [14] Michael Volske, Martin Potthast, Shahbaz Syed, and Benno Stein. Tl; dr: Mining reddit to learn automatic summarization. In _Proceedings of the Workshop on New Frontiers in Summarization_, pages 59-63, 2017.
* [15] Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noac'h, Haonan Li, Kyle McDonell, Niklas Muenninghoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework for few-shot language model evaluation, 12 2023.
* [16] Remi Munos, Michal Valko, Daniele Calandriello, Mohammad Gheshlaghi Azar, Mark Rowland, Zhaohan Daniel Guo, Yunhao Tang, Matthieu Geist, Thomas Mesnard, Andrea Michi, et al. Nash learning from human feedback. _arXiv preprint arXiv:2312.00886_, 2023.
* [17] Tianqi Liu, Yao Zhao, Rishabh Joshi, Misha Khalman, Mohammad Saleh, Peter J Liu, and Jialu Liu. Statistical rejection sampling improves preference optimization. In _The Twelfth International Conference on Learning Representations_, 2023.
* [18] Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Sainbayar Sukhbaatar, Jing Xu, and Jason Weston. Self-rewarding language models. _arXiv preprint arXiv:2401.10020_, 2024.
* [19] Wei Xiong, Hanze Dong, Chenlu Ye, Han Zhong, Nan Jiang, and Tong Zhang. Gibbs sampling from human feedback: A provable kl-constrained framework for rhlf. _arXiv preprint arXiv:2312.11456_, 2023.
* [20] Corby Rosset, Ching-An Cheng, Arindam Mitra, Michael Santacroce, Ahmed Awadallah, and Tengyang Xie. Direct nash optimization: Teaching language models to self-improve with general preferences. _arXiv preprint arXiv:2404.03715_, 2024.
* [21] Shangmin Guo, Biao Zhang, Tianlin Liu, Tianqi Liu, Misha Khalman, Felipe Llinares, Alexandre Rame, Thomas Mesnard, Yao Zhao, Bilal Piot, et al. Direct language model alignment from online ai feedback. _arXiv preprint arXiv:2402.04792_, 2024.
* [22] Teng Xiao, Yige Yuan, Zhengyu Chen, Mingxiao Li, Shangsong Liang, Zhaochun Ren, and Vasant G Honavar. Simper: Simple preference fine-tuning without hyperparameters by perplexity optimization. _arXiv_, 2024.
* [23] Ziniu Li, Tian Xu, and Yang Yu. Policy optimization in rhlf: The impact of out-of-preference data. _arXiv preprint arXiv:2312.10584_, 2023.
* [24] Shusheng Xu, Wei Fu, Jiaxuan Gao, Wenjie Ye, Weilin Liu, Zhiyu Mei, Guangju Wang, Chao Yu, and Yi Wu. Is dpo superior to ppo for llm alignment? a comprehensive study. _arXiv preprint arXiv:2404.10719_, 2024.
* [25] Sayak Ray Chowdhury, Anush Kini, and Nagarajan Natarajan. Provably robust dpo: Aligning language models with noisy feedback. _arXiv preprint arXiv:2403.00409_, 2024.
* [26] Huayu Chen, Guande He, Hang Su, and Jun Zhu. Noise contrastive alignment of language models with explicit rewards. _arXiv preprint arXiv:2402.05369_, 2024.

* [27] Han Zhang, Lin Gui, Yu Lei, Yuanzhao Zhai, Yehong Zhang, Yulan He, Hui Wang, Yue Yu, Kam-Fai Wong, Bin Liang, et al. Copr: Continual human preference learning via optimal policy regularization. _arXiv preprint arXiv:2402.14228_, 2024.
* [28] Haozhe Ji, Cheng Lu, Yilin Niu, Pei Ke, Hongning Wang, Jun Zhu, Jie Tang, and Minlie Huang. Towards efficient and exact optimization of language model alignment. _arXiv preprint arXiv:2402.00856_, 2024.
* [29] Zhuang Ma and Michael Collins. Noise contrastive estimation and negative sampling for conditional models: Consistency and statistical efficiency. In _Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing_, pages 3698-3707, 2018.
* [30] Duanyu Feng, Bowen Qin, Chen Huang, Zheng Zhang, and Wenqiang Lei. Towards analyzing and understanding the limitations of dpo: A theoretical perspective. _arXiv preprint arXiv:2404.04626_, 2024.
* [31] Rafael Rafailov, Joey Hejna, Ryan Park, and Chelsea Finn. From r to q*: Your language model is secretly a q-function. _arXiv preprint arXiv:2404.12358_, 2024.
* [32] Han Zhong, Guhao Feng, Wei Xiong, Li Zhao, Di He, Jiang Bian, and Liwei Wang. Dpo meets ppo: Reinforced token optimization for rlhf. _arXiv preprint arXiv:2404.18922_, 2024.
* [33] Yongcheng Zeng, Guoqing Liu, Weiyu Ma, Ning Yang, Haifeng Zhang, and Jun Wang. Token-level direct preference optimization. _arXiv preprint arXiv:2404.11999_, 2024.
* [34] Teng Xiao and Donglin Wang. A general offline reinforcement learning framework for interactive recommendation. In _Proceedings of the AAAI Conference on Artificial Intelligence_, pages 4512-4520, 2021.
* [35] Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural networks. In _International conference on machine learning_, pages 1321-1330, 2017.
* [36] Le Yan, Zhen Qin, Xuanhui Wang, Michael Bendersky, and Marc Najork. Scale calibration of deep ranking models. In _Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining_, pages 4300-4309, 2022.
* [37] Ali Malik, Volodymyr Kuleshov, Jiaming Song, Danny Nemer, Harlan Seymour, and Stefano Ermon. Calibrated model-based deep reinforcement learning. In _International Conference on Machine Learning_, pages 4314-4323. PMLR, 2019.
* [38] Mitsuhiko Nakamoto, Simon Zhai, Anikait Singh, Max Sobol Mark, Yi Ma, Chelsea Finn, Aviral Kumar, and Sergey Levine. Cal-ql: Calibrated offline rl pre-training for efficient online fine-tuning. _Advances in Neural Information Processing Systems_, 36, 2024.
* [39] Chiwei Zhu, Benfeng Xu, Quan Wang, Yongdong Zhang, and Zhendong Mao. On the calibration of large language models and alignment. _arXiv preprint arXiv:2311.13240_, 2023.
* [40] Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli Tran-Johnson, et al. Language models (mostly) know what they know. _arXiv preprint arXiv:2207.05221_, 2022.
* [41] Ralph Allan Bradley and Milton E Terry. Rank analysis of incomplete block designs: I. the method of paired comparisons. _Biometrika_, pages 324-345, 1952.
* [42] Yunhao Tang, Zhaohan Daniel Guo, Zeyu Zheng, Daniele Calandriello, Remi Munos, Mark Rowland, Pierre Harvey Richemond, Michal Valko, Bernardo Avila Pires, and Bilal Piot. Generalized preference optimization: A unified approach to offline alignment. _arXiv preprint arXiv:2402.05749_, 2024.
* [43] Teng Xiao, Mingxiao Li, Yige Yuan, Huaisheng Zhu, Chao Cui, and Vasant G Honavar. How to leverage demonstration data in alignment for large language model? a self-imitation learning perspective. _arXiv preprint arXiv:2410.10093_, 2024.

* [44] Yu Meng, Mengzhou Xia, and Danqi Chen. Simpo: Simple preference optimization with a reference-free reward. _arXiv preprint arXiv:2405.14734_, 2024.
* [45] Caglar Gulcehre, Tom Le Paine, Srivarast Srinivasan, Ksenia Konyushkova, Lotte Weerts, Abhishek Sharma, Aditya Siddhant, Alex Ahern, Miaosen Wang, Chenjie Gu, et al. Reinforced self-training (rest) for language modeling. _arXiv preprint arXiv:2308.08998_, 2023.
* [46] Guan Wang, Sijie Cheng, Xianyuan Zhan, Xiangang Li, Sen Song, and Yang Liu. Openchat: Advancing open-source language models with mixed-quality data. In _The Twelfth International Conference on Learning Representations_, 2023.
* [47] Banghua Zhu, Hiteshi Sharma, Felipe Vieira Furjeri, Shi Dong, Chenguang Zhu, Michael I Jordan, and Jiantao Jiao. Fine-tuning language models with advantage-induced policy alignment. _arXiv preprint arXiv:2306.02231_, 2023.
* [48] Kevin P Murphy. _Machine learning: a probabilistic perspective_. MIT press, 2012.
* [49] Ofir Nachum, Mohammad Norouzi, and Dale Schuurmans. Improving policy gradient by exploring under-appreciated rewards. _arXiv preprint arXiv:1611.09321_, 2016.
* [50] Teng Xiao, Zhengyu Chen, Donglin Wang, and Suhang Wang. Learning how to propagate messages in graph neural networks. In _ACM SIGKDD Conference on Knowledge Discovery & Data Mining_, pages 1894-1903, 2021.
* [51] Rishabh Agarwal, Chen Liang, Dale Schuurmans, and Mohammad Norouzi. Learning to generalize from sparse and underspecified rewards. In _International conference on machine learning_, pages 130-140. PMLR, 2019.
* [52] Teng Xiao, Zhengyu Chen, Zhimeng Guo, Zeyang Zhuang, and Suhang Wang. Decoupled self-supervised learning for graphs. _Advances in Neural Information Processing Systems_, pages 620-634, 2022.
* [53] Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Wei Zhu, Yuan Ni, Guotong Xie, Zhiyuan Liu, and Maosong Sun. Ultrafeedback: Boosting language models with high-quality feedback. _arXiv preprint arXiv:2310.01377_, 2023.
* [54] Lewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen Rajani, Kashif Rasul, Younes Belkada, Shengyi Huang, Leandro von Werra, Clementine Fourrier, Nathan Habib, et al. Zephyr: Direct distillation of lm alignment. _arXiv preprint arXiv:2310.16944_, 2023.
* [55] Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, et al. Mmlu-pro: A more robust and challenging multi-task language understanding benchmark. _arXiv preprint arXiv:2406.01574_, 2024.
* [56] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. _arXiv preprint arXiv:1803.05457_, 2018.
* [57] Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, and Le Hou. Instruction-following evaluation for large language models. _arXiv preprint arXiv:2311.07911_, 2023.
* [58] Mirac Suzgun, Nathan Scales, Nathanael Scharli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny Zhou, et al. Challenging big-bench tasks and whether chain-of-thought can solve them. _arXiv preprint arXiv:2210.09261_, 2022.
* [59] David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel R Bowman. Gpqa: A graduate-level google-proof q&a benchmark. _arXiv preprint arXiv:2311.12022_, 2023.
* [60] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. _arXiv preprint arXiv:2110.14168_, 2021.

* [61] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. _arXiv preprint arXiv:2103.03874_, 2021.
* [62] Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, and Tatsunori B Hashimoto. Alpacaeval: An automatic evaluator of instruction-following models, 2023.
* [63] Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle O'Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al. Pythia: A suite for analyzing large language models across training and scaling. In _International Conference on Machine Learning_, pages 2397-2430. PMLR, 2023.
* [64] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. _OpenAI blog_, page 9, 2019.
* [65] Haoran Xu, Amr Sharaf, Yunmo Chen, Weiting Tan, Lingfeng Shen, Benjamin Van Durme, Kenton Murray, and Young Jin Kim. Contrastive preference optimization: Pushing the boundaries of llm performance in machine translation. _arXiv preprint arXiv:2401.08417_, 2024.
* [66] Chaoqi Wang, Yibo Jiang, Chenghao Yang, Han Liu, and Yuxin Chen. Beyond reverse kl: Generalizing direct preference optimization with diverse divergence constraints. In _The Twelfth International Conference on Learning Representations_, 2023.
* [67] Richard Yuanzhe Pang, Weizhe Yuan, Kyunghyun Cho, He He, Sainbayar Sukhbaatar, and Jason Weston. Iterative reasoning preference optimization. _arXiv preprint arXiv:2404.19733_, 2024.
* [68] Mark Reid, Robert Williamson, et al. Information, divergence and risk for binary experiments. 2011.
* [69] Mohammad Norouzi, Samy Bengio, Navdeep Jaitly, Mike Schuster, Yonghui Wu, Dale Schuurmans, et al. Reward augmented maximum likelihood for neural structured prediction. _Advances In Neural Information Processing Systems_, 29, 2016.
* [70] Arindam Banerjee, Srujana Merugu, Inderjit S Dhillon, Joydeep Ghosh, and John Lafferty. Clustering with bregman divergences. _Journal of machine learning research_, 6(10), 2005.
* [71] Zixiang Chen, Yihe Deng, Huizhuo Yuan, Kaixuan Ji, and Quanquan Gu. Self-play fine-tuning converts weak language models to strong language models. _arXiv preprint arXiv:2401.01335_, 2024.
* [72] Robert Kirk, Ishita Mediratta, Christoforos Nalmpantis, Jelena Luketina, Eric Hambro, Edward Grefenstette, and Roberta Raileanu. Understanding the effects of rlhf on llm generalisation and diversity. _arXiv preprint arXiv:2310.06452_, 2023.

Derivations and Proofs

### Derivations of Equation (11)

We provide a detailed derivation of Equation (11), demonstrating that the RLHF objective in Equation (2) optimizes a reverse KL-divergence.

\[\mathcal{L}_{\text{RL}}(\theta) =\beta\mathbb{D}_{\text{KL}}\left[\pi_{\theta}(\mathbf{y}\mid \mathbf{x})\|\pi^{*}(\mathbf{y}\mid\mathbf{x})\right]-\beta\log Z(\mathbf{x})\] (18) \[=\beta\mathbb{E}_{\pi_{\theta}(\mathbf{y}\mid\mathbf{x})}[\log \pi_{\theta}(\mathbf{y}\mid\mathbf{x})-\log\frac{(\pi_{\text{ref}}(\mathbf{y} \mid\mathbf{x})\exp(r(\mathbf{x},\mathbf{y})/\beta))}{Z(\mathbf{x})}]-\beta \log Z(\mathbf{x})\] (19) \[=\beta\mathbb{E}_{\pi_{\theta}(\mathbf{y}\mid\mathbf{x})}[\log \pi_{\theta}(\mathbf{y}\mid\mathbf{x})-\log\frac{(\pi_{\text{ref}}(\mathbf{y} \mid\mathbf{x})\exp(r(\mathbf{x},\mathbf{y})/\beta))}{Z(\mathbf{x})}]-\beta \log Z(\mathbf{x})\] (20) \[=\beta\mathbb{E}_{\pi_{\theta}(\mathbf{y}\mid\mathbf{x})}[\log \pi_{\theta}(\mathbf{y}\mid\mathbf{x})-\log(\pi_{\text{ref}}(\mathbf{y}\mid \mathbf{x})\exp(r(\mathbf{x},\mathbf{y})/\beta))]\] (21) \[=-(\mathbb{E}_{\mathbf{y}\sim\pi_{\theta}(\mathbf{y}\mid\mathbf{x })}[r(\mathbf{x},\mathbf{y})]-\beta\mathbb{D}_{\text{KL}}\left[\pi_{\theta}( \mathbf{y}\mid\mathbf{x})\right]\|\pi_{\text{ref}}(\mathbf{y}\mid\mathbf{x})]).\] (22)

Note that the last line is exactly the RLHF objective with a negative sign. Therefore, maximizing the RLHF objective with respect to \(\pi_{\theta}\) is equivalent to minimizing the reverse KL-divergence. Because optimizing the reverse KL-divergence is mode-seeking, RLHF exhibits mode-seeking behavior.

### Derivations of Equation (13)

In this section, we show that our proposed loss in Equation (10) is an empirical approximate estimation of the population loss in Equation (13) based on sampled preference feedback, i.e., \(\mathbf{y}_{w}\) and \(\mathbf{y}_{l}\):

\[\mathcal{L}_{\text{Cal-DPO}}(\theta)= -\mathbb{E}_{\mathbf{y}\sim\pi_{\text{ref}}(\mathbf{y}\mid \mathbf{x})}\Big{[}\frac{\exp(r(\mathbf{x},\mathbf{y})/\beta)}{\sum_{\mathbf{ y}}\pi_{\text{ref}}(\mathbf{y}\mid\mathbf{x})\exp(r(\mathbf{x},\mathbf{y})/ \beta)}\log\frac{(\pi_{\theta}(\mathbf{y}\mid\mathbf{x})/\pi_{\text{ref}}( \mathbf{y}\mid\mathbf{x}))}{\sum_{\mathbf{y}}\pi_{\text{ref}}(\mathbf{y}\mid \mathbf{x})(\pi_{\theta}(\mathbf{y}\mid\mathbf{x})/\pi_{\text{ref}}(\mathbf{y }\mid\mathbf{x}))}\Big{]}\] \[+\mathbb{E}_{\mathbf{y}\sim\pi_{\text{ref}}(\mathbf{y}\mid \mathbf{x})}\Big{[}(\log\frac{\pi_{\theta}(\mathbf{y}\mid\mathbf{x})}{\pi_{ \text{ref}}(\mathbf{y}\mid\mathbf{x})}-\frac{r(\mathbf{x},\mathbf{y})}{\beta} )^{2}\Big{]}.\] (23)

Using preference feedback \(\mathbf{y}_{w}\) and \(\mathbf{y}_{l}\) sampled from \(\pi_{\text{ref}}(\mathbf{y}\mid\mathbf{x})\) to approximate the above expectation gives us the following empirical estimation:

\[\hat{\mathcal{L}}_{\text{Cal-DPO}}(\theta)= -\frac{\exp(r(\mathbf{x},\mathbf{y}_{w})/\beta)}{\exp(r(\mathbf{x},\mathbf{y}_{w})/\beta)+\exp(r(\mathbf{x},\mathbf{y}_{l})/\beta)}\log\frac{ \pi_{\theta}(\mathbf{y}_{w}|\mathbf{x})/\pi_{\text{ref}}(\mathbf{y}_{w}| \mathbf{x})}{\pi_{\theta}(\mathbf{y}_{w}|\mathbf{x})/\pi_{\text{ref}}(\mathbf{y }_{w}|\mathbf{x})+\pi_{\theta}(\mathbf{y}_{l}|\mathbf{x})/\pi_{\text{ref}}( \mathbf{y}_{l}|\mathbf{x})}\] \[-\frac{\exp(r(\mathbf{x},\mathbf{y}_{l})/\beta)}{\exp(r(\mathbf{x},\mathbf{y}_{w})/\beta)+\exp(r(\mathbf{x},\mathbf{y}_{l})/\beta)}\log\frac{\pi_{ \theta}(\mathbf{y}_{l}|\mathbf{x})/\pi_{\text{ref}}(\mathbf{y}_{l}|\mathbf{x})} {\pi_{\theta}(\mathbf{y}_{w}|\mathbf{x})/\pi_{\text{ref}}(\mathbf{y}_{w}| \mathbf{x})+\pi_{\theta}(\mathbf{y}_{l}|\mathbf{x})/\pi_{\text{ref}}(\mathbf{y }_{l}|\mathbf{x})}\] \[+(\log\frac{\pi_{\theta}(\mathbf{y}_{w}\mid\mathbf{x})}{\pi_{ \text{ref}}(\mathbf{y}_{w}\mid\mathbf{x})}-\frac{r(\mathbf{x},\mathbf{y}_{w})}{ \beta})^{2}+(\log\frac{\pi_{\theta}(\mathbf{y}_{l}\mid\mathbf{x})}{\pi_{\text{ ref}}(\mathbf{y}_{l}\mid\mathbf{x})}-\frac{r(\mathbf{x},\mathbf{y}_{l})}{ \beta})^{2}.\] (24)

Given pairwise preference feedback \(\mathbf{y}_{w}\succ\mathbf{y}_{l}\mid\mathbf{x}\) with reward \(r(\mathbf{x},\mathbf{y}_{w})=1/2\), \(r(\mathbf{x},\mathbf{y}_{l})=-1/2\), setting the coefficient \(\beta\to 0\) (\(\beta\) is typically small and set as \(0.001\) in our experiments) gives us the following approximation (softmax weight becomes hard argmax weight):

\[\hat{\mathcal{L}}_{\text{Cal-DPO}}(\theta)= \log\frac{\pi_{\theta}(\mathbf{y}_{w}|\mathbf{x})/\pi_{\text{ref} }(\mathbf{y}_{w}|\mathbf{x})}{\pi_{\theta}(\mathbf{y}_{w}|\mathbf{x})/\pi_{ \text{ref}}(\mathbf{y}_{w}|\mathbf{x})+\pi_{\theta}(\mathbf{y}_{l}|\mathbf{x})/ \pi_{\text{ref}}(\mathbf{y}_{l}|\mathbf{x})}\] \[+(\log\frac{\pi_{\theta}(\mathbf{y}_{w}\mid\mathbf{x})}{\pi_{ \text{ref}}(\mathbf{y}_{w}\mid\mathbf{x})}-\frac{1}{2\beta})^{2}+(\log\frac{\pi_ {\theta}(\mathbf{y}_{l}\mid\mathbf{x})}{\pi_{\text{ref}}(\mathbf{y}_{l}\mid \mathbf{x})}+\frac{1}{2\beta})^{2}.\] (25)

This is equivalent to the loss proposed in Equation (10).

### Proofs of Theorem 1

**Theorem 1**.: _Minimizing the first term in our \(\mathtt{Cal-DPO}\) in Equation (13) is equivalent to minimizing the forward KL divergence, or equivalently MLE in Equation (12), while maintaining the following contrastive negative gradient with respect to \(\pi_{\theta}\):_

\[\mathbb{E}_{\mathbf{y}\sim\pi_{\text{ref}}(\mathbf{y}\mid\mathbf{x })}\Big{[}(w(\mathbf{x},\mathbf{y})-\hat{w}(\mathbf{x},\mathbf{y}))\nabla_{ \theta}\log\pi_{\theta}(\mathbf{y}|\mathbf{x})\Big{]},\text{where}\] (26) \[w(\mathbf{x},\mathbf{y})=\frac{\exp(r(\mathbf{x},\mathbf{y})/\beta) }{\sum_{\mathbf{y}}\pi_{\text{ref}}(\mathbf{y}\mid\mathbf{x})\exp(r(\mathbf{x}, \mathbf{y})/\beta)}\text{ and }\hat{w}(\mathbf{x},\mathbf{y})=\frac{(\pi_{\theta}(\mathbf{y} \mid\mathbf{x})/\pi_{\text{ref}}(\mathbf{y}\mid\mathbf{x}))}{\sum_{\mathbf{y}} \pi_{\text{ref}}(\mathbf{y}\mid\mathbf{x})(\pi_{\theta}(\mathbf{y}\mid\mathbf{x })/\pi_{\text{ref}}(\mathbf{y}\mid\mathbf{x}))},\] (27)Proof.: Recall that the first term in the population objective of \(\mathtt{Cal-DPO}\) in Equation (13) is:

\[\mathcal{L}_{1}(\theta)= -\mathbb{E}_{\mathbf{y}\sim\pi_{\text{ref}}(\mathbf{y}|\mathbf{x})} \Big{[}\frac{\exp(r(\mathbf{x},\mathbf{y})/\beta)}{\sum_{\mathbf{y}}\pi_{\text {ref}}(\mathbf{y}|\mathbf{x})\exp(r(\mathbf{x},\mathbf{y})/\beta)}\log\frac{( \pi_{\theta}(\mathbf{y}|\mathbf{x})/\pi_{\text{ref}}(\mathbf{y}|\mathbf{x}))}{ \sum_{\mathbf{y}}\pi_{\text{ref}}(\mathbf{y}|\mathbf{x})(\pi_{\theta}(\mathbf{ y}|\mathbf{x})/\pi_{\text{ref}}(\mathbf{y}|\mathbf{x}))}\Big{]}\] (28) \[= -\mathbb{E}_{\mathbf{y}\sim\pi_{\text{ref}}(\mathbf{y}|\mathbf{x}) }\Big{[}\frac{\exp(r(\mathbf{x},\mathbf{y})/\beta)}{\sum_{\mathbf{y}}\pi_{ \text{ref}}(\mathbf{y}|\mathbf{x})\exp(r(\mathbf{x},\mathbf{y})/\beta)}\log \frac{(\pi_{\theta}(\mathbf{y}|\mathbf{x})/\pi_{\text{ref}}(\mathbf{y}|\mathbf{ x}))}{\sum_{\mathbf{y}}\pi_{\theta}(\mathbf{y}|\mathbf{x})}\Big{]}\] (29) \[= -\mathbb{E}_{\mathbf{y}\sim\pi_{\text{ref}}(\mathbf{y}|\mathbf{x} )}\Big{[}\frac{\exp(r(\mathbf{x},\mathbf{y})/\beta)}{\sum_{\mathbf{y}}\pi_{ \text{ref}}(\mathbf{y}|\mathbf{x})\exp(r(\mathbf{x},\mathbf{y})/\beta)}\log \left(\pi_{\theta}(\mathbf{y}|\mathbf{x})/\pi_{\text{ref}}(\mathbf{y}|\mathbf{ x})\right)\Big{]}\] (30) \[= \mathcal{L}_{\mathrm{MLE}}(\theta)+\mathbb{E}_{\mathbf{y}\sim\pi_ {\text{ref}}(\mathbf{y}|\mathbf{x})}\Big{[}\frac{\exp(r(\mathbf{x},\mathbf{y} )/\beta)}{\sum_{\mathbf{y}}\pi_{\text{ref}}(\mathbf{y}|\mathbf{x})\exp(r( \mathbf{x},\mathbf{y})/\beta)}\log\pi_{\text{ref}}(\mathbf{y}|\mathbf{x})\Big{]}.\] (31)

Note that the second term in the last line does not depend on \(\pi_{\theta}\). Therefore, minimizing the first term of \(\mathtt{Cal-DPO}\) with respect to \(\pi_{\theta}\) is equivalent to optimizing the forward KL-divergence, as \(\mathcal{L}_{\mathrm{MLE}}\) does.

We know from the previous result that \(\mathcal{L}_{1}\) and \(\mathcal{L}_{\mathrm{MLE}}\) have the same optimal. However their gradients differs. We first take the derivatives of \(\mathcal{L}_{1}\) with respect to \(\pi_{\theta}\):

\[-\nabla_{\theta}\mathcal{L}_{1}(\theta)= \mathbb{E}_{\mathbf{y}\sim\pi_{\text{ref}}(\mathbf{y}|\mathbf{x}) }\Big{[}w(\mathbf{x},\mathbf{y})\nabla_{\theta}\log\frac{(\pi_{\theta}( \mathbf{y}|\mathbf{x})/\pi_{\text{ref}}(\mathbf{y}|\mathbf{x}))}{\sum_{ \mathbf{y}}\pi_{\text{ref}}(\mathbf{y}|\mathbf{x})(\pi_{\theta}(\mathbf{y}| \mathbf{x})/\pi_{\text{ref}}(\mathbf{y}|\mathbf{x}))}\Big{]}\] (32) \[= \mathbb{E}_{\mathbf{y}\sim\pi_{\text{ref}}(\mathbf{y}|\mathbf{x} )}\Big{[}w(\mathbf{x},\mathbf{y})\nabla_{\theta}\log\pi_{\theta}(\mathbf{y}| \mathbf{x})-\nabla_{\theta}\log\sum_{\mathbf{y}}\pi_{\text{ref}}(\mathbf{y}| \mathbf{x})(\pi_{\theta}(\mathbf{y}|\mathbf{x})/\pi_{\text{ref}}(\mathbf{y}| \mathbf{x}))\Big{]}\] (33) \[= \mathbb{E}_{\mathbf{y}\sim\pi_{\text{ref}}(\mathbf{y}|\mathbf{x} )}\Big{[}w(\mathbf{x},\mathbf{y})\nabla_{\theta}\log\pi_{\theta}(\mathbf{y}| \mathbf{x})-\frac{\nabla_{\theta}\sum_{\mathbf{y}}\pi_{\text{ref}}(\mathbf{y}| \mathbf{x})(\pi_{\theta}(\mathbf{y}|\mathbf{x})/\pi_{\text{ref}}(\mathbf{y}| \mathbf{x}))}{\sum_{\mathbf{y}}\pi_{\text{ref}}(\mathbf{y}|\mathbf{x})(\pi_{ \theta}(\mathbf{y}|\mathbf{x})/\pi_{\text{ref}}(\mathbf{y}|\mathbf{x}))}\Big{]}\] (34) \[= \mathbb{E}_{\mathbf{y}\sim\pi_{\text{ref}}(\mathbf{y}|\mathbf{x} )}\Big{[}w(\mathbf{x},\mathbf{y})\nabla_{\theta}\log\pi_{\theta}(\mathbf{y}| \mathbf{x})-\frac{\sum_{\mathbf{y}}\nabla_{\theta}\pi_{\theta}(\mathbf{y}| \mathbf{x})/\pi_{\text{ref}}(\mathbf{y}|\mathbf{x})}{\sum_{\mathbf{y}}\pi_{ \text{ref}}(\mathbf{y}|\mathbf{x})(\pi_{\theta}(\mathbf{y}|\mathbf{x})/\pi_{ \text{ref}}(\mathbf{y}|\mathbf{x}))}\Big{]}\] (35) \[= \mathbb{E}_{\mathbf{y}\sim\pi_{\text{ref}}(\mathbf{y}|\mathbf{x} )}\Big{[}w(\mathbf{x},\mathbf{y})\nabla_{\theta}\log\pi_{\theta}(\mathbf{y}| \mathbf{x})-\frac{\sum_{\mathbf{y}}\pi_{\theta}(\mathbf{y}|\mathbf{x})/\pi_{ \text{ref}}(\mathbf{y}|\mathbf{x})\nabla_{\theta}\log\pi_{\theta}(\mathbf{y}| \mathbf{x})}{\sum_{\mathbf{y}}\pi_{\text{ref}}(\mathbf{y}|\mathbf{x})(\pi_{ \theta}(\mathbf{y}|\mathbf{x})/\pi_{\text{ref}}(\mathbf{y}|\mathbf{x}))}\Big{]}\] (36) \[= \mathbb{E}_{\mathbf{y}\sim\pi_{\text{ref}}(\mathbf{y}|\mathbf{x} )}\Big{[}(w(\mathbf{x},\mathbf{y})-\hat{w}(\mathbf{x},\mathbf{y})\nabla_{ \theta}\log\pi_{\theta}(\mathbf{y}|\mathbf{x})\Big{]}.\] (37)

Similarly, we can take the derivatives of \(\mathcal{L}_{\mathrm{MLE}}(\theta)\) in Equation 12 with respect to \(\pi_{\theta}\):

\[-\nabla_{\theta}\mathcal{L}_{\mathrm{MLE}}(\theta)= \mathbb{E}_{\mathbf{y}\sim\pi_{\text{ref}}(\mathbf{y}|\mathbf{x} )}[\frac{\exp(r(\mathbf{x},\mathbf{y})/\beta)}{\sum_{\mathbf{y}}\pi_{\text{ ref}}(\mathbf{y}|\mathbf{x})\exp(r(\mathbf{x},\mathbf{y})/\beta)}\nabla_{ \theta}\log\pi_{\theta}(\mathbf{y}|\mathbf{x})]\] (38) \[\mathbb{E}_{\mathbf{y}\sim\pi_{\text{ref}}(\mathbf{y}|\mathbf{x} )}[w(\mathbf{x},\mathbf{y})\nabla_{\theta}\log\pi_{\theta}(\mathbf{y}|\mathbf{x} )].\] (39)

Comparing Equations (37) and (39), we can observe that our \(\mathtt{Cal-DPO}\) employs a negative gradient to push down the likelihood of bad responses under the learned policy compared to MLE. 

### Proofs of Theorem 2

**Theorem 2**.: _Minimizing the \(\mathtt{Cal-DPO}\) objective in Equation (13) with respect to \(\pi_{\theta}\) will encourage mode-seeking behavior by minimizing an upper bound of the reverse KL divergence, as RLHF does._

\[\mathcal{L}_{\mathrm{RL}}(\theta)=\beta\mathbb{D}_{\mathrm{KL}}\left[\pi_{\theta}( \mathbf{y}\mid\mathbf{x})\|\pi^{*}(\mathbf{y}\mid\mathbf{x})\right]-\beta\log Z (\mathbf{x})\leq\beta\mathcal{L}_{\mathrm{Cal-DPO}}(\theta)-\beta\log Z( \mathbf{x}),\] (40)

Proof.: Recall that \(\mathtt{Cal-DPO}\) contains two terms: the contrastive term and the calibration term:

\[\mathcal{L}_{\mathrm{Cal-DPO}}(\theta)= -\mathbb{E}_{\mathbf{y}\sim\pi_{\text{ref}}(\mathbf{y}|\mathbf{x}) }\Big{[}\frac{\exp(r(\mathbf{x},\mathbf{y})/\beta)}{\sum_{\mathbf{y}}\pi_{ \text{ref}}(\mathbf{y}|\mathbf{x})\exp(r(\mathbf{x},\mathbf{y})/\beta)}\log \frac{(\pi_{\theta}(\mathbf{y}|\mathbf{x})/\pi_{\text{ref}}(\mathbf{y}| \mathbf{x}))}{\sum_{\mathbf{y}}\pi_{\text{ref}}(\mathbf{y}|\mathbf{x})(\pi_{ \theta}(\mathbf{y}|\mathbf{x})/\pi_{\text{ref}}(\mathbf{We begin by exploring the distinction between forward and reverse divergences. Before that, we introduce a few definitions and some background on Bregman divergence. For any two points \(\mathbf{r}\in\mathcal{F}\) and \(\mathbf{s}\in\mathcal{F}\), the Bregman divergence \(\mathbb{D}_{\mathrm{F}}\), specified by the convex differentiable potential function \(F:\mathcal{F}\rightarrow\mathbb{R}\), is defined as follows [68; 69]:

\[\mathbb{D}_{\mathrm{F}}(\mathbf{s}\|\mathbf{r})=F(\mathbf{s})-F(\mathbf{r})- \boldsymbol{f}(\mathbf{r})\cdot(\mathbf{s}-\mathbf{r})=F(\mathbf{s})-\mathbf{ s}\cdot\mathbf{p}+F^{*}(\mathbf{p})=\mathbb{D}_{\mathrm{F}^{*}}(\mathbf{p}\| \mathbf{q}),\] (42)

where \(\boldsymbol{f}=\nabla F\), \(F^{\prime}(\mathbf{p})\) is the convex conjugate of \(F\), \(\mathbf{p}=\boldsymbol{f}(\mathbf{r})\), and \(\mathbf{q}=\boldsymbol{f}(\mathbf{s})\)[70]. By defining \(F(\mathbf{s})=\log\sum_{y}\exp(\mathbf{s}(y))\) as the Log-Sum-Exp operator, \(\boldsymbol{f}(\mathbf{s})=\frac{\mathbf{s}(y)}{\sum_{y}\exp(\mathbf{s}(y))}\) as the Softmax operator, and \(F(\mathbf{p})=-\mathbb{E}(\mathbf{p})\), we get the KL divergence between two probability vectors \(\mathbf{q}\) and \(\mathbf{p}\):

\[\mathbb{D}_{\mathrm{KL}}(\mathbf{q}\|\mathbf{p})=\mathbb{D}_{\mathrm{F}^{*}}( \mathbf{q}\|\mathbf{p})=\mathbb{D}_{\mathrm{F}}(\mathbf{r}\|\mathbf{s}).\] (43)

By using Taylor expansions of \(F(\mathbf{p})\), we further have the following:

\[\mathbb{D}_{\mathrm{F}}(\mathbf{r}\|\mathbf{s})=\mathbb{D}_{\mathrm{F}}( \mathbf{s}\|\mathbf{r})+\frac{1}{4}\left(\mathbf{s}-\mathbf{r}\right)^{\top} \left(H_{F}(\mathbf{b})-H_{F}(\mathbf{a})\right)(\mathbf{s}-\mathbf{r})\,,\] (44)

where \(H_{F}\) denotes the Hessian of \(F\), and \(\mathbf{a}=(1-\alpha/2)\mathbf{r}+\mathbf{s}\alpha/2,(0\leq\alpha\leq 1/2)\), \(\mathbf{b}=(1-\lambda/2)\mathbf{s}+\mathbf{r}\lambda/2,(0\leq\lambda\leq 1/2)\). It is well known that Hessian has the following form:

\[I\succeq H_{F}(\mathbf{a})=\mathrm{diag}(\boldsymbol{f}(\mathbf{a}))- \boldsymbol{f}(\mathbf{a})\boldsymbol{f}(\mathbf{a})^{\top}\succeq 0,\;\;I \succeq H_{F}(\mathbf{b})=\mathrm{diag}(\boldsymbol{f}(\mathbf{b}))- \boldsymbol{f}(\mathbf{b})\boldsymbol{f}(\mathbf{b})^{\top}\succeq 0,\] (45)

Taking the above and Equation (44), we obtain the following:

\[\mathbb{D}_{\mathrm{F}}(\mathbf{r}\|\mathbf{s})\leq\mathbb{D}_{\mathrm{F}}( \mathbf{s}\|\mathbf{r})+\frac{1}{4}\left(\mathbf{s}-\mathbf{r}\right)^{2} \Rightarrow\mathbb{D}_{\mathrm{KL}}(\mathbf{q}\|\mathbf{p})\leq\mathbb{D}_{ \mathrm{KL}}(\mathbf{p}\|\mathbf{q})+\frac{1}{4}\left(\mathbf{s}-\mathbf{r} \right)^{2},\] (46)

Plugging \(\mathbf{p}=\pi^{*}(\mathbf{y}\mid\mathbf{x})=\frac{\pi_{\mathrm{ref}}( \mathbf{y}\mid\mathbf{x})\exp(r(\mathbf{x},\mathbf{y})/\beta)}{Z(x)}\), \(\mathbf{q}=\pi_{\theta}(\mathbf{y}\mid\mathbf{x})\), \(\mathbf{s}=\log\pi_{\theta}(\mathbf{y}\mid\mathbf{x})\), and \(\mathbf{r}=\log\pi_{\mathrm{ref}}(\mathbf{y}\mid\mathbf{x})+r(\mathbf{x}, \mathbf{y})/\beta\) into Equation (46) obtains the following:

\[\mathbb{D}_{\mathrm{KL}}\left[\pi_{\theta}(\mathbf{y}\mid\mathbf{ x})\|\pi^{*}(\mathbf{y}\mid\mathbf{x})\right] \leq\mathbb{D}_{\mathrm{KL}}\left[\pi^{*}(\mathbf{y}\mid\mathbf{ x})\|\pi_{\theta}(\mathbf{y}\mid\mathbf{x})\right]\] (47) \[+\mathbb{E}_{\mathbf{y}\sim\pi_{\mathrm{ref}}(\mathbf{y}\mid \mathbf{x})}\Big{[}(\log\frac{\pi_{\theta}(\mathbf{y}\mid\mathbf{x})}{\pi_{ \mathrm{ref}}(\mathbf{y}\mid\mathbf{x})}-\frac{r(\mathbf{x},\mathbf{y})}{ \beta})^{2}\Big{]}.\] (48)

Theorem 1 demonstrates that the first contrastive term in \(\mathtt{Cal}\)-DPO is equivalent to optimizing the forward KL-divergence. Consequently, we can directly obtain:

\[\mathcal{L}_{\mathrm{RL}}=\beta\mathbb{D}_{\mathrm{KL}}\left[\pi_{\theta}( \mathbf{y}\mid\mathbf{x})\|\pi^{*}(\mathbf{y}\mid\mathbf{x})\right]-\beta\log Z (\mathbf{x})\leq\beta\mathcal{L}_{\mathrm{Cal-DPO}}(\theta)-\beta\log Z( \mathbf{x}),\] (49)

which completes the proof. 

## Appendix B Algorithm

### Algorithm and Implementation Details

The pseudocode for \(\mathtt{Cal}\)-DPO is provided in Algorithm 1. For the general hyperparameters, we closely followed the configurations used in [71]. The \(\beta\) of \(\mathtt{Cal}\)-DPO is searched from [1e-3, 2e-3, 3e-3, 1e-2, 1e-1], the batch size for all methods is 128, and we use the RMSprop optimizer with a learning rate of 5e-6. We linearly warm up the learning rate from 0 to 5e-6 in 150 steps. The sampling temperature is set to 1 for all experiments. The experiments on are run on 4 Nvidia A100 GPUs with BF16 precision. We thoroughly tune the hyperparameters for each baseline following [44].

## Appendix C Experimental Details

### The Details of Datasets

In this section, we provide detailed descriptions of datasets:

**UltraFeedback Binarized**[53; 54]: This dataset 1 consists of 64k prompts, where each prompt is accompanied by four model completions from a wide variety of open and proprietary models. GPT-4is then used to assign a score to each completion based on criteria like helpfulness and honesty. It constructs binary preferences from UltraFeedback by selecting the highest mean score as the "chosen" response and one of the remaining three at random as the "rejected" response.

**Anthropic-HH**[1]: This Anthropic Helpful and Harmless dialogue dataset 2 contains 170k dialogues between a human and an automated assistant. This dataset was utilized for assessing single-turn dialogue performance. Each of the 170k dialogues comprises a human query and paired model responses rated for helpfulness and harmlessness. Following DPO [7], the preferred responses from this dataset were utilized for the supervised Fine-Tuning (SFT) phase, aligning the initial model behavior with desirable conversational outcomes.

Footnote 2: https://huggingface.co/datasets/Anthropic/hh-rlhf

**Reddit TL;DR summarization**[14]: This dataset 3 is a compilation of forum posts from the popular social media platform Reddit, specifically curated for summarization tasks with preference labels. Following [3; 72], we use the same filtered version of this dataset to train the SFT policy and use their preference labels for the following alignment stage.

Footnote 3: https://huggingface.co/datasets/openai/summarize_from_feedback

**IMDB Sentiment**[13]: This dataset 4 contains movie reviews from the IMDb with positive and negative sentiment, which contains 25k training samples and each 5k samples for validation and test.

Footnote 4: https://huggingface.co/datasets/stanfordnlp/imdb

### The Details of Tasks and Evaluation

We evaluate methods fine-tuned on the UltraFeedback Binarized dataset across reasoning benchmarks (MMLU-PRO [55], ARC [56], IFEval [57], BBH [58] GPQA [59]), and mathematical reasoning (GSM8K [60] and MATH [61]) provided by the Language Model Evaluation Harness library [15].

**AI2 Reasoning Challenge (ARC)**: This task is a set of grade-school science questions.

**Massive Multitask Language Understanding Professional (MMLU- PRO)**: MMLU-PRO is an enhanced version of the MMLU dataset [61], addressing previous shortcomings by increasing choice options in multiple-choice questions and refining question quality through expert review, making it more challenging and less prone to data contamination.

**Instruction-Following Evaluation (IFEval)**: IFEval is a benchmark evaluating a model's ability to follow explicit instructions, emphasizing adherence to formatting over content generation.

**Big Bench Hard (BBH)**: BBH is a selection of 23 challenging tasks from the BigBench, focusing on areas like multistep arithmetic, algorithmic reasoning, language understanding, and world knowledge.

**Graduate-Level Google-Proof Q&A (GPQA)**: GPQA is a challenging benchmark composed of advanced questions developed by PhD-level experts across various fields like biology, physics, and chemistry.

**GSM8K**: This dataset consists of diverse grade school math word problems to measure a model's ability to solve multi-step mathematical reasoning problems.

**MATH**: MATH is a benchmark consisting of high-school level competition problems gathered from multiple sources, consistently formatted with LaTeX for equations and Asymptote for diagrams.

**AlpacaEval 2.0**: We also use AlpacaEval 2.0 [62], a benchmark to assess model fine-tuned on UltraFeedback Binarized dataset with human preference.

We consider the following tasks to evaluate the model fine-tuned on real human preference data.

**Single-Turn Dialogue**: We employ the Anthropic HH dataset, which includes 170,000 interactions between humans and virtual assistants. Our model is optimized to deliver useful and accurate answers across a range of queries while deliberately avoiding any harmful content.

**Summarization**: We leverage the Reddit TL;DR dataset, where the prompt represents a Reddit forum post and the preference pairs are collected by previous works. The task objective is to generate a summary that captures the main points of the post.

For single-turn dialogue and summarization tasks, we query GPT-4 for zero-shot pair-wise evaluation, which has been shown to be consistent with human judgments.

**Controlled Sentiment Generation**: We use the IMDb dataset [13], which contains 25,000 reviews, where the prompt is a prefix of a movie review and the preference pairs are generated following previous works. The task objective is to generate a positive sentiment \(y\) based on the movie review prefix. To perform a controlled evaluation, we train a binary sentiment classifier on the IMDb dataset and define the oracle reward as its log odds, following [7]. The reward score of the reward model then serves as an in-domain proxy for the unknown ground-truth reward used for evaluation. The results of IMDb sentiment generation are listed in Table 4. We used the reward score of the reward model and the perplexity of GPT-2 [64] to demonstrate the performance of alignment.

#### c.2.1 GPT-4 Win Rate Judgments

For the generation, summarization, and dialogue tasks, we utilize GPT-4 for zero-shot pair-wise evaluation, which is shown to be consistent with human judgments. We use gpt-4-0314 for all our experiments. The specific prompts used for GPT-4 evaluation are detailed below.

## Appendix D Additional Experimental Results

Figure 5 shows the performance of SLiC and IPO, and their calibrated counterparts Cal-IPO and Cal-SLiC by applying our proposed calibration objective on the Anthropic-HH dataset. We observe that the calibrated counterparts significantly improve the performance of the off-the-shelf methods, demonstrating the effectiveness of our calibration objective. Figure 6 provides additional results of the training dynamics of DPO and Cal-DPO on the Anthropic-HH and IMDb datasets. These results show the robustness of Cal-DPO to different methods and datasets for preference fine-tuning

Figure 4: The effect of the coefficient parameter \(\beta\) on four tasks. For Reddit TL;DR summarization and Anthropic-HH, we show the win rate over the chosen response.

[MISSING_PAGE_FAIL:20]

[MISSING_PAGE_EMPTY:21]

\begin{table}
\begin{tabular}{l l} \hline \hline
**Prompt** & Human: Set an appointment for Pepper to go to the vet on \\  & Tuesday December 21st at 3:30 pm. Assistant: \\ \hline
**DPO** & OK, Ill do my best. Is there anything else I can help you \\  & with today? Im happy to do anything you need. Im here to \\  & help. Let me know if you have any questions. Have a great \\  & day! \\ \hline
**Cal-DPO** & Okay, Ill set it up. Ill send you an email with the \\  & details. \\ \hline
**Judgment** & Comparison: Response 2 is more helpful and direct, as it \\  & confirms the action of setting the appointment and mentions a \\  & follow-up with details, whereas Response 1, though friendly, \\  & lacks confirmation of the appointment setting and might leave \\  & the user unsure if the task will be completed. More helpful: \\  & 2 \\ \hline \hline \end{tabular}
\end{table}
Table 10: GPT-4 chooses Cal-DPO over DPO. Responses to a prompt from the Anthropic-HH Helpful test set.

\begin{table}
\begin{tabular}{l l} \hline \hline
**Prompt** & Human: Should I buy a used car cash or a new one on credit? \\  & Assistant: \\ \hline
**DPO** & Its a tough question, and there are a lot of factors to \\  & consider. If youre looking to buy a new car, you should \\  & definitely go with a new one on credit. This is because \\  & youll be able to get the best deal, and youll also be able \\  & to pay it off in a shorter amount of time. However, if youre \\  & looking to buy a used car, you should definitely go with a \\  & used car cash. This is \\ \hline
**Cal-DPO** & I think it depends on your situation. If you have a good \\  & credit score and a decent income, I would recommend buying a \\  & new car on credit. This way you can pay it off in full every \\  & month, and you can also get a lower interest rate. But if \\  & you have a bad credit score or a low income, I would recommend \\  & buying a used car. This way you can pay it off in full every \\  & month, and you can also get a lower interest rate. \\ \hline
**Judgment** & Comparison: Response 2 provides a more nuanced view that \\  & considers the individuals financial situation and credit \\  & score, making it more helpful and adaptable to different \\  & scenarios, whereas Response 1 seems more absolute and less \\  & tailored to individual circumstances. More helpful: 2 \\ \hline \hline \end{tabular}
\end{table}
Table 9: GPT-4 chooses Cal-DPO over DPO. Responses to a prompt from the Anthropic-HH Helpful test set.

[MISSING_PAGE_EMPTY:23]

\begin{table}
\begin{tabular}{l l}
**Prompt** & SUBREDDIT: r/AskReddit TITLE: Okay, Reddit. Very Serious \\  & Question regarding bees. Ive got a small nest near my living \\  & room window, and bee stings = likely death. Wat do?POST: \\  & I have a SEVERE bee allergy, and despite my near constant \\  & vigilance, I have a small nest outside of my living room \\  & window. I have a can of Raid and a can of Black Flag, but \\  & since I dont have health insurance right now, poor aim, and \\  & lack of an Epi-Pen due to said lack of insurance, Id rather \\  & not take on the Bee Menace without ensuring that Im not going \\  & to end up in the ER or die. I dont know what kind of bees \\  & they are and frankly, I dont care. I just want them GONE \\  & since bees can kill me. Id like to NOT hire an exterminator, \\  & but if I have to, I will.TL;DR: \\
**DPO** & I have a bee allergy and bees stings = likely death. I dont \\  & know what kind of bees they are and frankly, I dont care. \\  & Id like them GONE. I dont want to hire an exterminator. \\  & What do? \\
**Cal-DPO** & I have a bee nest near my living room window, and Im allergic. \\  & I want them gone. What can I do? \\
**Judgment** & Comparison: Summary 2 is more concise and captures the \\  & essence of the situation without unnecessary details, making \\  & it more effective for a quick understanding. Preferred: 2 \\ \end{tabular}
\end{table}
Table 12: GPT-4 chooses Cal-DPO over DPO. Summaries to a post from the TL;DR test set.

\begin{table}
\begin{tabular}{l l} \hline \hline
**Prompt** & SUBREDDIT: r/AskReddit TITLE: Reddit, what is the worst youve ever screwed up when trying to impress someone? POST: Ill start. I went to meet this girl, beautiful, smart, funny, ect. we agreed on a time and place, and even both showed up early, but sat at separate table for a good 30 minutes, I even saw her friend walk in after a while, which kinda clued me in that something was up. Anywho, a few minutes later, I seeher and her friend heading out of the restaurant, so I flag them down. Awkward moment \#1 my stomach does a back-flip. I feel like an idiot so I start to clamp up a little bit, I but stay calm enough to continue the conversation. Eventually it comes up that she is doing some essays for the peace corps. Ive known this gorl for a while, I consider it big news, she makes a joke and keeps talking. At this point my brain is racing, but on a completely lateral line of thought, the conversation has a lull, and Im asked to speak up a little. Kinda went deer in headlights, after a few minutes of awkwardness her friend leaves. My stomach decides to get a little crazy and goes for the triple somersault. I hold it for a second, but barely say anything. minutes later, I throw up in my mouth. Thinking on my feet, I swallow it. She notices (duh) and excuses me from the table, I stand, walk to the door, and puke all over the ground. Feeling like the worlds largest idiot, I make my way to the bathroom, I actually coughed a few chunks up while walking past another table. Clean up, make my way back out, feeling like shit, Im in an even less talkative mood, pretty much saying little more then sorry about the puking, and mention a bug I might have gotten, which may or may not be true. she says, oh looks its 2, gotta jet. maybe things will go better next time TL;DR: \\ \hline
**DPO** & went to meet a girl, sat at different table for a good 30 minutes, she leaves, I puke all over the ground, awkwardness ensues, she leaves, I feel like an idiot, and feel like shit. *cough* *cough* *cough* *cough* *cough* *cough* *cough* *cough* *cough* *cough* *cough* *cough* *cough* * *cough* * *cough* * *cough* * *cough* * * \\ \hline
**Cal-DPO** & tried to impress a girl, vomited, felt like an idiot, and now Im worried she wont want to see me again. \\ \hline
**Judgment** & Comparison: Summary 2 captures the essence of the story in a concise manner without the unnecessary repetition of details and sound effects found in Summary 1, making it more precise and to the point. Preferred: 2 \\ \hline \hline \end{tabular}
\end{table}
Table 13: GPT-4 chooses Cal-DPO over DPO. Summaries to a post from the TL;DR test set.

Societal Impacts

We proposed Calibrated Direct Preference Optimization (Cal-DPO ) to align LLMs with human preferences, leading to more reliable and human-aligned AI systems. By enhancing training stability and efficiency, Cal-DPO addresses limitations in contrastive preference learning methods. Its applicability across various tasks, from programming to problem-solving, underscores its versatility. Given the successful deployment of large language models (LLMs) in various human-related real-world applications, it is crucial to ensure that the responses of a pretrained LLM to prompts are aligned with human or societal values and preferences, which can potentially yield direct social impacts.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: Our main claim matches our theoretical and experimental results in Section 4.2 and Section 5. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: Please see Section 6 for the discussion of limitations. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes]Justification: Please refer to Section 4.2 and Appendix A for our assumptions and a complete (and correct) proof. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We provide information needed to reproduce the main experimental result and the link which contains the code and dataset to reproduce our results. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code**Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: The code of Cal-DPO is available Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We provide all the training and test details in Appendix B.1. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: We provide the experiment results that support the main claims of the paper. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We provide the computer resources in Appendix B.1. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We make sure to preserve anonymity and conform NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: Please see Appendix E for broader impacts. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.

* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: The paper poses no such risks. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **License for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [NA] Justification: The paper does not use existing assets. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.

* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: The paper does not release new assets. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer:[NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.