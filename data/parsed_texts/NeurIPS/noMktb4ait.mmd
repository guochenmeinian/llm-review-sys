# Joint Feature and Differentiable \(k\)-NN Graph Learning using Dirichlet Energy

Lei Xu

School of Computer Science &

School of Artificial Intelligence,

OPtics and ElectroNics (iOPEN)

Northwestern Polytechnical University

Xi'an 710072, P.R. China

solerxl1998@gmail.com &Lei Chen

School of Computer Science

Nanjing University of Posts and Telecommunications

Nanjing 210003, P.R. China

chenlei@njupt.edu.cn &Rong Wang

School of Artificial Intelligence,

OPtics and ElectroNics (iOPEN)

Northwestern Polytechnical University

Xi'an 710072, P.R. China

wangrong07@tsinghua.org.cn &Feiping Nie

School of Artificial Intelligence,

OPtics and ElectroNics (iOPEN) &

School of Computer Science

Northwestern Polytechnical University

Xi'an 710072, P.R. China

feipingnie@gmail.com &Xuelong Li

School of Artificial Intelligence,

OPtics and ElectroNics (iOPEN)

Northwestern Polytechnical University

Xi'an 710072, P.R. China

li@nwpu.edu.cn

Corresponding author.

###### Abstract

Feature selection (FS) plays an important role in machine learning, which extracts important features and accelerates the learning process. In this paper, we propose a deep FS method that simultaneously conducts feature selection and differentiable \(k\)-NN graph learning based on the Dirichlet Energy. The Dirichlet Energy identifies important features by measuring their smoothness on the graph structure, and facilitates the learning of a new graph that reflects the inherent structure in new feature subspace. We employ Optimal Transport theory to address the non-differentiability issue of learning \(k\)-NN graphs in neural networks, which theoretically makes our method applicable to other graph neural networks for dynamic graph learning. Furthermore, the proposed framework is interpretable, since all modules are designed algorithmically. We validate the effectiveness of our model with extensive experiments on both synthetic and real-world datasets.

## 1 Introduction

Feature selection (FS) is a critical technique in machine learning that identifies informative features within the original high-dimensional data. By removing irrelevant features, FS speeds up the learning process and enhances computational efficiency. In many real-world applications such asimage processing, bioinformatics, and text mining [1, 2, 3], FS techniques are widely used to identify important features, thereby providing some explanations about the results and boosting the learning performance [4, 5, 6, 7].

While numerous FS methods have been proposed in both supervised and unsupervised settings, as several studies highlighted [8, 9], the nature of FS is more unsupervised due to the unavailability of task-specific labels in advance. The selected features should be versatile to arbitrary downstream tasks, which motivates us to focus on the unsupervised FS in this study. Related works have recently resorted to neural networks to exploit the nonlinear information within feature space. For example, AEFS [9] uses the group Lasso to regularize the parameters in the first layer of the autoencoder, so as to reconstruct original features based on the restricted use of original features. Another well-known method is CAE [8], which selects features by learning a concrete distribution over the input features. However, most unsupervised deep methods rely on the reconstruction performance to select useful features. On the one hand, if there exists noise in the dataset, the reconstruction performance will be terrible even if useful features are selected, since the noise cannot be reconstructed with these informative features (see our reconstruction experiments on Madelon in Section 4.3). On the other hand, it is difficult to explain why selected features reconstruct the original data well. These issues prompt us to seek a new target for unsupervised deep FS.

As the saying goes, "_birds of a feather flock together_", the homophily principle [10] suggests that similar samples tend to be connected in a natural graph structure within real-world data. This graph structure is useful for describing the intrinsic structure of the feature space, and is commonly used in machine learning studies [11, 12]. Building upon this graph structure, He et al. [13] introduce the Dirichlet Energy, which they call "locality preserving power", as a powerful tool for unsupervised FS that is able to identify informative features reflecting the intrinsic structure of the feature space.

In many practical applications, the graph structure is not naturally defined and needs to be constructed manually based on the input features using some similarity measurements. The quality of features affects the quality of the constructed graph. As highlighted in [14], the useless features increase the amount of unstructured information, which hinders the exploration of inherent manifold structure within data points and deteriorates the quality of constructed graph. Therefore, the reference [14] proposes the UDFS method to discard such nuisance features and constructs a \(k\)-nearest-neighbor (NN) graph on the selected features using the heat kernel. Despite the good performance of UDFS, constructing graphs using the heat kernel may not reflect the intrinsic structure of the feature space. Besides, the sorting algorithms in learning the \(k\)-NN graph in UDFS is non-differentiable in neural networks, which restricts its application in downstream networks.

In this paper: (1) We propose a deep unsupervised FS network that performs simultaneous feature selection and graph learning by minimizing the Dirichlet Energy, thereby revealing and harnessing the intrinsic structure in the dataset. (2) Within the network, a Unique Feature Selector (UFS) is devised to approximate discrete and distinct feature selection using the Gumbel Softmax technique combined with decomposition algorithms. (3) Moreover, a Differentiable Graph Learner (DGL) is devised based on the Optimal Transport theory, which is capable of obtaining a differentiable \(k\)-NN graph that more accurately reflects the intrinsic structure of the data than traditional graph constructing methods. Due to the differentiability, DGL is also theoretically capable of serving as a learnable graph module for other graph-based networks. (4) The entire framework is developed algorithmically. Unlike most deep learning networks with complex components that are tough to decipher, each core module in our framework has an algorithmic and physically interpretable design, which greatly facilitates observing and understanding the network's internal operations during the learning process. (5) Experimental results on both synthetic datasets and real-world datasets demonstrate the effectiveness of our method.

**Notations.** For an arbitrary matrix \(\bm{M}\!\in\!\mathcal{R}^{a\times b}\), \(\bm{m}^{i}\), \(\bm{m}_{i}\), and \(m_{i,j}\) denote the \(i\)-th row, the \(i\)-th column, and the \((i,j)\)-th entry of \(\bm{M}\), respectively. Given a vector \(\bm{m}\in\mathcal{R}^{b}\), its \(\ell_{2}\)-norm is defined as \(\|\bm{m}\|_{2}=\sqrt{\sum_{i=1}^{b}m_{i}^{2}}\). Based on this, the Frobenius norm of \(\bm{M}\) is defined as \(\|\bm{M}\|_{F}\!=\!\sqrt{\sum_{i=1}^{a}\|\bm{m}^{i}\|_{2}^{2}}\). When \(a=b\), the trace of \(\bm{M}\) is defined as \(\operatorname{tr}(\bm{M})=\sum_{i=1}^{a}m_{i,i}\). Given two matrices \(\bm{M},\bm{N}\in\mathcal{R}^{a\times b}\), we define their inner product as \(\langle\bm{M},\bm{N}\rangle=\sum_{i=1}^{a}\sum_{j=1}^{b}m_{i,j}n_{i,j}\). \(\bm{1}_{b}\) denotes a \(b\)-dimensional column vector with all entries being \(1\), and \(\bm{I}_{b}\) denotes a \(b\)-order identity matrix. \(\operatorname{Bool}(cond)\) is a boolean operator that equals \(1\) if \(cond\) is true, otherwise it equals \(0\). Moreover, given a vector \(\bm{m}\in\mathcal{R}^{b}\), we define its sorting permutation in ascending order as \(\bm{\sigma}\in\mathcal{R}^{b}\), namely, \(m_{\sigma_{1}}\leq m_{\sigma_{2}}\leq\cdots\leq m_{\sigma_{b}}\).

## 2 Dirichlet Energy

Let \(\bm{X}\in\mathcal{R}^{n\times d}\) be the data matrix with the \(n\) samples and \(d\)-dimensional features. In this paper, we assume that the features have zero means and normalized variances,2 namely, \(\mathbf{1}_{n}^{\top}\bm{x}_{i}=0\) and \(\bm{x}_{i}^{\top}\bm{x}_{i}=1\) for \(i\in\{1,\dots,d\}\). According to the homophily assumption [10], we assume that data \(\bm{X}\) forms an inherent graph structure \(\mathcal{G}\) with nodes standing for samples and edges standing for their correlations. In \(\mathcal{G}\), similar samples are more likely to connect to each other than dissimilar ones. Graph \(\mathcal{G}\) can be represented with a similarity matrix \(\bm{S}\in\mathcal{R}_{+}^{n\times n}\), where \(s_{i,j}\) denotes the similarity between \(\bm{x}^{i}\) and \(\bm{x}^{j}\). If \(s_{i,j}=0\), it means that there is no connection between \(\bm{x}^{i}\) and \(\bm{x}^{j}\) in \(\mathcal{G}\), which is common in \(k\)-NN graphs since we only consider the local structure of the data space. Given an adjacency matrix \(\bm{S}\),3 we define the _Laplacian matrix_[15] as \(\bm{L}_{S}=\bm{D}-\bm{S}\), where \(\bm{D}\) is a diagonal matrix whose diagonal entries represent the degrees of data points, namely, \(d_{i,i}=\sum_{j=1}^{n}s_{i,j}\).

Footnote 2: Note that constant features (if any) will be removed during the feature preprocessing stage.

Footnote 3: Note that the similarity matrix \(\bm{S}\) of a \(k\)-NN graph is not symmetric. When calculating the objective in Eq. (3), we obtain the Laplacian matrix \(\bm{L}_{S}\) using the symmetrized similarity matrix \(\hat{\bm{S}}=(\bm{S}+\bm{S}^{\top})/2\).

Based on the Laplacian matrix \(\bm{L}_{S}\), we introduce the _Dirichlet Energy_[16] as a powerful tool to identify important features. Specifically, given the Laplacian matrix \(\bm{L}_{S}\), the Dirichlet Energy of a graph signal \(\bm{v}\) is defined as

\[\mathcal{L}_{dir}(\bm{v})=\frac{1}{2}\sum_{i=1}^{n}\sum_{j=1}^{n}s_{i,j}(v_{ i}-v_{j})^{2}=\bm{v}^{\top}\bm{L}_{S}\bm{v}.\] (1)

In graph theory, each dimensional feature \(\bm{x}_{i}\in\mathcal{R}^{n}\) can be seen as a graph signal on \(\mathcal{G}\). The Dirichlet Energy in Eq. (1) provides a measure of the local smoothness [11] of each feature on graph \(\mathcal{G}\), which is small when the nodes that are close to each other on \(\mathcal{G}\) have similar feature values. Hence, the Dirichlet Energy can be used to identify informative features by evaluating the consistency of the distribution of feature values with the inherent data structure. To demonstrate this, we provide an example in Fig. 1, where we generate a \(2\)-NN graph \(\mathcal{G}\) including two bubbles, and compose the data \(\bm{X}\) using the two-dimensional coordinates of the graph nodes. Then we set the graph signal \(\bm{v}\) as the first coordinate \(\bm{x}_{1}\) and visualize it on \(\mathcal{G}\) in Fig. 1(a). In Fig. 1(b) we change \(\bm{v}\) to a random noise vector. While in Fig. 1(c), we change the graph structure to a random 2-NN graph. We compute the Laplacian matrix \(\bm{L}_{S}\) of each figure and present the corresponding Dirichlet Energy in the figures. We can see that Fig. 1(a) achieves the best smoothness, whereas both Fig. 1(b) and Fig. 1(c) have poor smoothness due to a mismatch between the graph signal and the graph structure.

Based on the Dirichlet Energy, a well-known FS method called Laplacian Score (LS) 4 is proposed in [13]. However, the Laplacian matrix in LS is precomputed and fixed. If \(\bm{X}\) contains too many irrelevant features, the quality of the graph \(\mathcal{G}\) will be poor and not reflect the underlying structure. As illustrated in Fig. 1(c), a poor-quality graph will lead to the poor smoothness even if the right feature is selected, this insight motivates us to learn graph and features jointly during the learning process.

Footnote 4: LS considers both the smoothness and the variance of each feature. However, as we assume that each feature has unit variance according to \(\mathbf{1}_{n}^{\top}\bm{x}_{i}=0\) and \(\bm{x}_{i}^{\top}\bm{x}_{i}=1\), minimizing LS is equivalent to minimizing Eq. (1).

## 3 Proposed Method

In this paper, we devise a collaborative neural network driven by the Dirichlet Energy for joint feature and graph learning, as illustrated in Fig. 2. Generally, the proposed framework consists of two

Figure 1: Illustration of the Dirichlet Energy on various graph structures and graph signals. Blue points, black edges, and red bars represent nodes, connections, and signal values on nodes, respectively. Upside bars represent positive values, and downside bars represent negative values.

modules: the Unique Feature Selector (UFS) and the Differentiable \(k\)-NN Graph Learner (DGL). At the beginning, the input features \(\bm{X}\) are selected with the learnable feature mask \(\bm{F}\) generated by UFS, which is carefully designed to avoid the duplicate feature selection. Based on the selected data \(\hat{\bm{X}}\), we measure the distances between different samples, and feed the resulting distance vectors of each sample into DGL to learn their \(k\) nearest neighbors. The adaptive graph structure and informative features are learned jointly under the Dirichlet Energy, so as to identify the optimal feature subset that effectively captures the underlying data structure.

### Unique Feature Selector

Based on the original data \(\bm{X}\), the goal of FS is to identify a feature subset \(\hat{\bm{X}}\in\mathcal{R}^{n\times m}\) from the original features by minimizing a prespecified target \(\mathcal{L}_{obj}(\hat{\bm{X}})\):

\[\min_{\bm{F}}\mathcal{L}_{obj}(\hat{\bm{X}})\quad\mathrm{s.t.}\ \hat{\bm{X}}=\bm{X}\bm{F},\bm{F}\in\{0,1\}^{d\times m},\bm{F}^{\top}\bm{F}=\bm {I}_{m},\] (2)

where \(m\leq d\) denotes the number of selected features, and \(\bm{F}\in\mathcal{R}^{d\times m}\) denotes the selection matrix selecting \(m\) features from \(\bm{X}\). Different from existing methods that use the reconstruction error as \(\mathcal{L}_{obj}(\hat{\bm{X}})\), in this paper, we utilize the Dirichlet Energy in Eq. (1) for FS as follows:

\[\mathcal{L}_{obj}(\hat{\bm{X}})=\sum_{i=1}^{m}\mathcal{L}_{dir}(\hat{\bm{x}} _{i})=\mathrm{tr}(\hat{\bm{X}}^{\top}\bm{L}_{S}\hat{\bm{X}}).\] (3)

Given the selection number \(m\), \(\mathcal{L}_{obj}\) updates the network parameters by minimizing the Dirichlet Energy, thereby selecting \(m\) features that best reflect the intrinsic structure.

The constraints in problem (2) indicate that an ideal result \(\bm{F}\) should be _exact_ and _unique_. _Exact_ means the result should exactly be the original features, instead of their linear combinations. _Unique_ means each feature should be selected only once under a given number \(m\). These two properties require \(\bm{F}\) to be a binary and column-full-rank matrix including \(m\) orthogonal one-hot column vectors.

Figure 2: (1) **Top Panel**: Overview of the proposed framework, where smiley faces denote the value 1 representing that the feature is selected, while sad faces denote the value \(0\) representing that the feature is unused. (2) **Bottom Left Panel**: Illustration of the Unique Feature Selector (UFS), where green bars denote the value distributions of different vectors. (3) **Bottom Right Panel**: Illustration of the Differentiable \(k\)-NN Graph Learner (DGL), where the “Differentiable \(k\)-NN Selector” in deep blue shows how to learn \(k\) nearest neighbors with the Optimal Transport theory.

#### 3.1.1 Approximating Discrete Feature Selection

It is difficult to learn a discrete \(\bm{F}\) in neural networks due to its non-differentiable property. Inspired by [8], we propose to learn the discrete distribution using the _Gumbel Softmax_[17; 18] technique:

\[\hat{\bm{f}}_{i}=\mathrm{softmax}((\log\bm{w}_{i}+\bm{g}_{i})/T)\quad\mathrm{ with}\;g_{i,j}=-\mathrm{log}(-\mathrm{log}\;u_{i,j}),u_{i,j}\sim\mathrm{Uniform}(0,1),\] (4)

where \(\bm{W}=[\bm{w}_{1},\bm{w}_{2},\dots,\bm{w}_{m}]\) denotes a learnable parameter. The random vector \(\bm{g}_{i}\) consists of \(d\) Gumbel-distributed variables \(g_{i,j}\), which is generated with \(u_{i,j}\) sampled from Uniform distribution. Based on \(\bm{w}_{i}\) and \(\bm{g}_{i}\), we obtain the approximated FS vector \(\hat{\bm{f}}_{i}\) that represents the \(i\)-th selected feature. The distribution of \(\hat{\bm{f}}_{i}\) is controlled by a non-negative temperature parameter \(T\). A smaller value of parameter \(T\) will generate a better approximation of the one-hot vector, but will be more likely to be stuck in a poor local minimum. As suggested in [8], we employ the annealing schedule on \(T\) by initializing it with a high value and then gradually decreasing it during the learning process.

#### 3.1.2 Selecting Unique Features

Despite having obtained the approximated FS vectors in neural networks, Eq. (4) does not consider the _uniqueness_ requirement of FS. This is because Eq. (4) learns each selected feature separately, and does not consider the orthogonal constraint between columns in \(\hat{\bm{F}}\), which is prone to result in the repeated selection of the same features. To address this issue, we develop a unique feature selector (UFS) in Algorithm 1, where \(\hat{\bm{0}}\in\mathcal{R}^{(d-m)\times m}\) denotes the zero matrix. First, we add a small enough perturbation \(\epsilon\bm{I}_{m}(\epsilon>0)\) on \(\hat{\bm{F}}^{\top}\hat{\bm{F}}\). Next, we perform the eigendecomposition (line 2) and the Cholesky decomposition (line 3) on the perturbed result respectively, and correspondingly obtain the diagonal matrix \(\bm{\Lambda}\in\mathcal{R}^{m\times m}\), the orthogonal matrix \(\bm{P}\in\mathcal{R}^{m\times m}\), and the lower triangle matrix \(\bm{L}\in\mathcal{R}^{m\times m}\). Based on \(\bm{\Lambda}\), \(\bm{P}\), and \(\bm{L}\), we obtain the selection matrix \(\bm{F}\) in line 4 and have the following conclusion:

**Proposition 3.1**.: _Given any real matrix \(\hat{\bm{F}}\in\mathcal{R}^{d\times m}\), one can always generate a column-orthogonal matrix \(\bm{F}\) through Algorithm 1._

The proof of Proposition 3.1 is provided in Appendix S1. On the one hand, the small perturbation \(\epsilon\bm{I}_{m}\) guarantees the column-full-rank property of \(\bm{F}\), thereby avoiding the duplicate selection results. On the other hand, the orthogonality property in Proposition 3.1 facilitates the approximation of discrete FS based on the matrix \(\hat{\bm{F}}\).5 We verify the efficacy of UFS in Section 4.2.

Footnote 5: In practice, we calculate \(\bm{F}\) with \(\bm{F}=\hat{\bm{F}}(\bm{L}^{-1})^{\top}\) without eigendecomposition to achieve discrete FS vectors. We provide a discussion in Appendix S2 to explain this treatment.

### Differentiable \(k\)-Nn Graph Learner

The existence of noise and irrelevant features may negatively affect the quality of the constructed graph. As depicted in Fig. 1, a low-quality graph structure can significantly perturb the smoothness of features and undermine the performance of feature selection. Hence, we propose to learn an adaptive graph during the learning process using the selected features.

#### 3.2.1 Learning an Adaptive \(k\)-Nn Graph Using Dirichlet Energy

Considering the objective function in Eq. (3), a natural way is to learn the similarity matrix \(\bm{S}\) based on the Dirichlet Energy in \(\mathcal{L}_{obj}\). However, this may yield a trivial solution where, for sample \(\bm{x}^{i}\), only the nearest data point can serve as its neighbour with probability \(1\), while all the other data points will not be its neighbours. To avoid this trivial solution, we propose to learn an adaptive graph by incorporating the _Tikhonov regularization_[19] of \(\bm{S}\) into the Dirichlet Energy:

\[\min_{\bm{S}}\mathrm{tr}(\hat{\bm{X}}^{\top}\bm{L}_{S}\hat{\bm{X}})+\frac{ \alpha}{2}\|\bm{S}\|_{F}^{2}\quad\mathrm{s.t.}\;\bm{S}\bm{1}_{n}=\bm{1}_{n},s_{ i,j}\geq 0,s_{i,i}=0,\] (5)

where \(\alpha\) denotes the trade-off parameter between the Dirichlet Energy and the Tikhonov regularization. Note that each row \(\bm{s}^{i}\) in \(\bm{S}\) can be solved separately, instead of tuning \(\alpha\) manually, we model \(\alpha\) asa sample-specific parameter \(\alpha_{i}\) and determine it algorithmically, which plays an important role in learning \(k\) nearest neighbors for each sample. Based on problem (5), we define the distance matrix \(\bm{E}\) with its entries being \(e_{i,j}=\|(\hat{\bm{x}}^{i}-\hat{\bm{x}}^{j})\|_{2}^{2}\), then we solve each row \(\bm{s}^{i}\) in problem (5) separately as

\[\min_{\bm{s}^{i}}\frac{1}{2}\|\bm{s}^{i}+\frac{\bm{e}^{i}}{2\alpha_{i}}\|_{2}^{ 2}\quad\mathrm{s.t.}\ \bm{s}^{i}\bm{1}_{n}=1,s_{i,j}\geq 0,s_{i,i}=0.\] (6)

Problem (6) can be solved easily by constructing the Lagrangian function and then using the Karush-Kuhn-Tucker(KKT) conditions [20]. By doing so, we obtain the solution of \(s_{i,j}\) as

\[s_{i,j}=(\frac{1}{k}+\frac{1}{k}\frac{\bm{e}^{i}\bm{\delta}_{i}^{(k)}}{2\alpha _{i}}-\frac{e_{i,j}}{2\alpha_{i}})_{+}\quad\mathrm{with}\quad\delta_{i,j}^{(k) }=\mathrm{Bool}(e_{i,j}\leq e_{i,\sigma_{k}}),\] (7)

where \(\bm{\sigma}=[\sigma_{1},\dots,\sigma_{n}]\) denotes the sorting permutation over \(\bm{e}^{i}\), i.e. \(e_{i,\sigma_{1}}\leq\dots\leq e_{i,\sigma_{n}}\) and \(\bm{\delta}_{i}^{(k)}\) denotes the selection vector identifying the \(k\) minimal values in \(\bm{e}^{i}\).

Recall that we aim to learn \(k\) nearest neighbors for each sample, which implies that there are only \(k\) nonzero elements in \(\bm{s}^{i}\) corresponding to the nearest neighbors. To this end, we determine the trade-off parameters \(\alpha_{i}\) such that \(s_{i,\sigma_{k}}>0\) and \(s_{i,\sigma_{k+1}}\leq 0\). Then we have:

\[\frac{1}{2}(k\bm{e}^{i}\bm{\xi}_{i}^{(k)}-\bm{e}^{i}\bm{\delta}_{i}^{(k)})< \alpha_{i}\leq\frac{1}{2}(k\bm{e}^{i}\bm{\xi}_{i}^{(k+1)}-\bm{e}^{i}\bm{ \delta}_{i}^{(k)})\quad\mathrm{with}\quad\xi_{i,j}^{(k)}=\mathrm{Bool}(e_{i,j }=e_{i,\sigma_{k}}),\] (8)

where \(\bm{\xi}_{i}^{(k)}\) denotes an indicator vector identifying the \(k\)-th minimal value in \(\bm{e}^{i}\). Setting \(\alpha_{i}\) as the maximum and substituting it into Eq. (7), we obtain the final solution as:

\[s_{i,\sigma_{j}}=\frac{\bm{e}^{i}\bm{\xi}_{i}^{(k+1)}-e_{i,\sigma_{j}}}{k\bm {e}^{i}\bm{\xi}_{i}^{(k+1)}-\bm{e}^{i}\bm{\delta}_{i}^{(k)}}\cdot\mathrm{Bool} (1\leq j\leq k).\] (9)

The detailed derivation of solution (9) can be found in Appendix S3. We note that the formulation in problem (6) bears similarity to CLR proposed in [21]. In Appendix S4, we discuss the connection between our method and CLR, and highlight the differences between the two w.r.t. the feature utilization and the sorting operation. Remarkably, the \(k\)-NN can be obtained easily in CLR using off-the-shelf sorting algorithms, which is not the case for neural networks due to the non-differentiability of sorting algorithms. To address this issue, we propose to transform the \(k\)-NN selection into a differentiable operator utilizing the _Optimal Transport_ (OT) [22] technique as follows.

#### 3.2.2 Differentiable \(k\)-NN Selector

Let \(\bm{\mu}=[\mu_{1},\mu_{2},\cdots,\mu_{n_{1}}]^{\top}\) and \(\bm{\nu}=[\nu_{1},\nu_{2},\cdots,\nu_{n_{2}}]^{\top}\) be two discrete probability distributions defined on the supports \(\mathcal{A}=\left\{\mathfrak{a}_{i}\right\}_{i=1}^{n_{1}}\) and \(\mathcal{B}=\left\{\mathfrak{b}_{j}\right\}_{j=1}^{n_{2}}\) respectively. The goal of OT is to find an optimal transport plan \(\bm{\Gamma}\in\mathcal{R}^{n_{1}\times n_{2}}\) between \(\mathcal{A}\) and \(\mathcal{B}\) by minimizing the following transport cost:

\[\min_{\bm{\Gamma}}\langle\bm{C},\bm{\Gamma}\rangle,\quad\mathrm{s.t.}\ \bm{\Gamma}\bm{1}_{n_{2}}=\bm{\mu},\bm{\Gamma}^{\top}\bm{1}_{n_{1}}=\bm{\nu}, \Gamma_{i,j}\geq 0,\] (10)

where \(\bm{C}\in\mathcal{R}^{n_{1}\times n_{2}}\) denotes the cost matrix with \(c_{i,j}=h(\mathfrak{a}_{i}-\mathfrak{b}_{j})>0\) being the transport cost from \(\mathfrak{a}_{i}\) to \(\mathfrak{b}_{j}\). It is widely known that the solution of the OT problem between two discrete univariate measures boils down to the sorting permutation [23, 24, 25]. As stated in [25], if \(h\) is convex, the optimal assignment can be achieved by assigning the smallest element in \(\mathcal{A}\) to \(\mathfrak{b}_{1}\), the second smallest to \(\mathfrak{b}_{2}\), and so forth, which eventually yields the sorting permutation of \(\mathcal{A}\).

Given a distance vector \(\bm{e}\), to learn selection vectors \(\bm{\delta}^{(k)}\) and \(\bm{\xi}^{(k+1)}\), we set \(\mathcal{A}=\bm{e}\), and \(\mathcal{B}=[0,1,\dots,k+1]\), and define \(\bm{\mu}\), \(\bm{\nu}\), and \(c_{ij}\) as

\[\mu_{i}=\frac{1}{n},\quad\nu_{j}=\begin{cases}1/n,&1\leq j\leq k+1\\ (n-k-1)/n,&j=k+2\end{cases},\quad c_{ij}=(\mathfrak{a}_{i}-\mathfrak{b}_{j}) ^{2}=(e_{i}-j+1)^{2}.\] (11)

The optimal transport plan of problem (10) assigns the \(i\)-th smallest value \(e_{\sigma_{i}}\) to \(\mathfrak{b}_{i}\) if \(1\leq i\leq k+1\), and assigns the remaining \(n-k-1\) values in \(\bm{e}\) to \(\mathfrak{b}_{k+2}\). Namely,

\[\Gamma_{\sigma_{i},j}=\begin{cases}1/n,&if\ (1\leq i\leq k+1\ and\ j=i)\ or \ (k+1<i\leq n\ and\ j=k+2)\\ 0,&if\ (1\leq i\leq k+1\ and\ j\neq i)\ or\ (k+1<i\leq n\ and\ j\neq k+2) \end{cases}.\] (12)Given a sample \(p\), once we obtain the optimal transport assignment \(\bm{\Gamma}\) based on \(\bm{e}^{p}\), we calculate the variables \(\bm{\delta}_{p}^{(k)}\) and \(\bm{\xi}_{p}^{(k+1)}\) as follows:

\[\bm{\delta}_{p}^{(k)}=n\sum_{i=1}^{k}\bm{\Gamma}_{i},\quad\bm{\xi}_{p}^{(k+1)}=n \bm{\Gamma}_{k+1},\] (13)

where \(\bm{\Gamma}_{i}\) and \(\bm{\Gamma}_{k+1}\) denote the \(i\)-th and the \((k+1)\)-th column of \(\bm{\Gamma}\), respectively. However, problem (10) is still non-differentiable. To address this issue, we consider the following entropy regularized OT problem:

\[\min_{\bm{\Gamma}}\langle\bm{C},\bm{\Gamma}\rangle+\gamma\sum_{i,j}\Gamma_{i, j}\log\Gamma_{i,j}\quad\mathrm{s.t.}\ \bm{\Gamma}\bm{1}_{k+2}=\bm{\mu},\bm{\Gamma}^{\top}\bm{1}_{n}=\bm{\nu},\Gamma_{ i,j}\geq 0,\] (14)

where \(\gamma\) is a hyperparameter. The differentiability of problem (14) has been proven using the implicit function theorem (see [26, Theorem 1]). Note that a smaller \(\gamma\) yields a better approximation to the original solution in problem (10), but may compromise the differentiability of problem (14) [25]. Problem (14) can be solved efficiently using the iterative Bregman projections algorithm [27], the details of which are provided in Appendix S5.

## 4 Experiments

Our experiments fall into three parts: (1) _Toy Experiments_: First, we verify the FS ability and the graph learning ability of the proposed method on synthetic datasets. (2) _Quantitative Analysis_: Next, we compare the performance of selected features in various downstream tasks on real-world datasets and compare our method with other unsupervised FS methods. (3) _Ablation Study_: Finally, we verify the effect of UFS and DGL by testing the performance of the corresponding ablated variants. We also provide the sensitivity analysis in Appendix S6.6. The implementation details of all experiments can be found in Appendix S6.1.

### Datasets

For the toy experiments, we generate three \(20\)-dimensional datasets named Blobs, Moons, and Circles (see Appendix S6.1.1 for generation details). On top of that, we evaluate the proposed method on twelve real-world datasets that include text, biological, image, and artificial data. Table 1 exhibits the details of these datasets, which include many high-dimensional datasets to test the performance of our method. We standardize all features to zero means and normalize them with the standard deviation.

### Toy Experiments

In this section, we consider three synthetic binary datasets with increasing difficulty in separating different classes. The first two dimensions in each dataset contain useful features that indicate the underlying structure, while the remaining \(18\) dimensions are random noise sampled from \(\mathcal{N}(0,1)\). The presence of noise obscures the inherent structure of the data, which makes the graph learning process highly challenging. To see this, we generate 3-D plots of each dataset using the useful features and one noise feature, along with their 2-D projections on each plane, which are shown in Fig. 3(a). We can see that the noise blurs the boundary of different classes, especially in Moons and Circles. In addition, we used a heat kernel (abbreviated as Heat) with \(\sigma=1\) to learn the \(5\)-NN graph on \(20\)-dimensional features, as shown in Fig. 3(b). We can see that the heavy noise obscures the underlying structure of data points, resulting in a chaotic graph outcome.

\begin{table}
\begin{tabular}{c c c c c c c c c c} \hline \hline Type & Dataset & \#Samples & \#Features & \#Classes & Type & Dataset & \#Samples & \#Features & \#Classes \\ \hline Text & PCMAC [28] & 1943 & 3289 & 2 & Artificial & Madelon [29] & 2600 & 500 & 2 \\ \hline \multirow{4}{*}{Biological} & GLLOMA [30] & 50 & 4434 & 4 & \multirow{4}{*}{Image} & COIL-20 [31] & 1440 & 1024 & 20 \\ \cline{2-10} \cline{3-10}  & LUNG [32] & 203 & 3312 & 5 & & \multirow{2}{*}{Yale [33]} & Yale [33] & 165 & 1024 & 15 \\ \cline{1-1} \cline{2-10} \cline{3-10}  & PROSTATE [34] & 102 & 5966 & 2 & & & & & \\ \cline{1-1} \cline{2-10} \cline{3-10}  & SRBCT [36] & 83 & 2308 & 4 & & & & & \\ \cline{1-1} \cline{2-10} \cline{3-10}  & SMK [37] & 187 & 19993 & 2 & & & & & \\ \hline \hline \end{tabular}
\end{table}
Table 1: Details of real-world data.

**Results.** We test our method on toy datasets for selecting \(m=2\) target features. The results are presented in Fig. 3(c) and Fig. 3(d), which demonstrate the success of our method in learning target features and intrinsic structures simultaneously. Moreover, it can be seen from Fig. 3(d) that the proposed network obtains the approximately discrete FS vectors.

**Learning Without Unique Feature Selector.** In addition, we conduct an ablation study by removing the UFS module from the network and updating \(\bm{F}\) using Eq. (4) only. The results are shown in Fig. 3(e) and Fig. 3(f), where we can see that, without UFS, the ablated model repeatedly selects the same feature on all datasets. It is also noteworthy that the nodes in the graph are mostly connected either horizontally or vertically, indicating the effectiveness of DGL in learning the local structure solely based on the single selected feature.

### Quantitive Analysis

**Experimental Settings.** In this section, we evaluate our method on real-world data. We partition each dataset into training data and testing data using an 8:2 ratio and identify useful features using training data. We then evaluate the performance of selected features on three downstream tasks: (1) _Classification Accuracy_: We train a random forest (RF) [39] classifier with \(1000\) trees using selected features and evaluate the prediction accuracy on the testing data. (2) _Clustering Accuracy_: We cluster the testing set with selected features using \(k\)-means [40], where the cluster number is set to #Classes. Then we align the results with true labels and calculate the accuracy. (3) _Reconstruction RMSE_: We build a 1-hidden-layer network with ReLU activation to reconstruct the original data using selected features. The hidden dimension is set to \(3m/2\) except for AllFea, where the hidden size is set to \(d\). The network is learned on the training set with selected features, and evaluated on the testing set using root mean square error (RMSE) normalized by \(d\).

**Competing methods.** We compare our methods with four deep methods (**CAE**[8], **DUFS**[14], **WAST**[41], **AEFS**[9]) and three classical methods (**LS**[13], **RSR**[42], **UDFS**[43]). Besides, we use all features (**AllFea**) as the baseline. To evaluate the performance of each FS method on downstream tasks, we average the results over \(10\) random runs with the feature number \(m\) varied in \(\{25,50,75,100,150,200,300\}\) except for Madelon, where \(m\) is varied in \(\{5,10,15,20\}\) since Madelon consists of only \(20\) useful features [29]. Appendix S6.1 provides details of the overall evaluation workflow, including the implementation and the parameter selection of each method.

**Results.** Similar to [14], we present the best result w.r.t. \(m\) in Table 2, the standard deviations is provided in Appendix S6.2. We also present some reconstruction results on PIX10 by our method in Appendix S6.3. From Table 2, we find that: (1) Our method generally achieves the best performance in all three tasks, indicating that our method selects more useful features. (2) In particular, we beat DUFS in all Classification and Clustering tasks, as well as most cases of the Reconstruction tasks. Recall that DUFS also selects features based on the Dirichlet Energy, this result shows that the model (5) in our method explores a superior graph structure compared to the traditional Heat method. (3)

Figure 3: Toy results on synthetic datasets, where higher similarities are presented with thicker connections in \(k\)-NN graphs, and we only present the connections to \(5\)-NN for each sample. Blue bars and orange bars represent the distribution of \(\bm{f}_{1}\) and \(\bm{f}_{2}\) in the FS matrix \(\bm{F}\), respectively.

[MISSING_PAGE_FAIL:9]

cluster the obtained graphs with the spectral clustering (SC) method to verify their qualities. We tune the parameter \(\sigma\) of Heat in \(\{1,2,\ldots,5\}\), and fix \(k=5\) for our method and the variant. The results are shown in Table 3, which shows that FS has a positive effect on graph learning compared with "DGL only". Besides, in Appendix S6.4, we visualize the learned graph on COIL-20 and Jaffe using t-SNE, which shows that using fewer features, we achieve separable graphs that contain fewer inter-class connections than other methods.

**Effect of DGL.** To verify the efficacy of DGL, we remove it from the model and learn the ablated variant with a fixed graph learned by Heat. Similar to Section 4.3, we first learn selected features using competing method, then evaluate the features in downstream tasks. We present the classification result in Table 3, and leave the other results in Appendix S6.5 due to limited space. We can see that our method significant outperforms the ablated variant, especially in Madelon. This is probably because the noise undermine the graph structure and disrupt the learning of informative features.

## 5 Discussion

**Conclusion.** This paper proposes a deep unsupervised FS method that learns informative features and \(k\)-NN graph jointly using the Dirichlet Energy. The network is fully differentiable and all modules are developed algorithmically to present versatility and interpretability. We demonstrate the performance of our method with extensive experiments on both synthetic and real-world datasets.

**Broader Impact.** This paper presents not only an effective deep FS method, but also a differentiable \(k\)-NN graph learning strategy in the context of deep learning. This technique is particularly useful for end-to-end learning scenarios that require graph learning during the training process. And we do notice this practical need in existing literature, see [44] for example. We believe our study will inspire researchers who work on the dimensionality reduction and graph-related researches.

**Limitations.** The major limitation of the proposed method is the lack of scalability, for which we do not evaluate our method on large datasets. This is because problem (14) requires an iterative solution, requiring storage of all intermediate results for back-propagation. While literature [26] proposes a memory-saving approach by deriving the expression of the derivative of \(\mathbf{\Gamma}\) mathematically (see [26, Section 3]), it still requires at least \(\mathcal{O}(nk)\) space to update all intermediate variables to learn \(k\) nearest neighbors for a single sample, which results in a \(\mathcal{O}(n^{2}k)\) space complexity to learn for all \(n\) samples. This is a huge memory cost on large datasets. Although learning in batch seems to be the most straightforward solution, in our method, the neighbours of each sample are determined based on the global information of \(\bm{L}_{S}\), which has an \(n\times n\) size. This requires to load the entire batch's information during each iteration, for which we cannot employ subgraph sampling as other graph learning methods did to mitigate memory overhead. Another limitation of the proposed method is the low computational speed, as it is reported that the OT-based sorting can be slow [45].

The future developments of the proposed method are twofold. First, we will try more differentiable sorting algorithms to enhance computational speed. For example, reference [45] proposes to construct differentiable sorting operators as projections onto the permutahedron, which achieves a \(\mathcal{O}(n\log n)\) forward complexity and a \(\mathcal{O}(n)\) backward complexity. Second, due to the large cost of the global relationship in \(\bm{L}_{S}\), we are considering adopting a bipartite graph [16; 46] to make batch learning feasible. This graph introduces a small number of anchor points, which are representative of the entire feature space. By doing this, smoothness can be measured based on the distance between samples to anchors, for which sample-to-sample relationships are no longer needed and the batch learning is enabled. It is worth noting that this idea is still in its conceptual stage, and we will explore its feasibility in upcoming research.

## Acknowledgments

This work was supported in part by the National Natural Science Foundation of China under Grant 62276212 and Grant 61872190, in part by the National Key Research and Development Program of China under Grant 2022YFB3303800, and in part by the Key Research and Development Program of Jiangsu Province under Grant BE2021093.

## References

* [1] Y. Li, C. Luo, and S. M. Chung, "Text clustering with feature selection by using statistical data," _IEEE Trans. Knowl. Data Eng._, vol. 20, no. 5, pp. 641-652, 2008.
* [2] C. He, K. Li, Y. Zhang, Y. Zhang, Z. Guo, X. Li, M. Danelljan, and F. Yu, "Strategic preys make acute predators: Enhancing camouflaged object detectors by generating camouflaged objects," _arXiv preprint arXiv:2308.03166_, 2023.
* [3] H.-J. Yu and D.-S. Huang, "Normalized feature vectors: A novel alignment-free sequence comparison method based on the numbers of adjacent amino acids," _IEEE/ACM Trans. Comput. Biol. Bioinformatics_, vol. 10, no. 2, pp. 457-467, 2013.
* [4] C. He, K. Li, Y. Zhang, L. Tang, Y. Zhang, Z. Guo, and X. Li, "Camouflaged object detection with feature decomposition and edge reconstruction," in _Proc. CVPR_, 2023, pp. 22 046-22 055.
* [5] Z. Sun, G. Bebis, and R. Miller, "Object detection using feature subset selection," _Pattern Recognit._, vol. 37, no. 11, pp. 2165-2176, 2004.
* [6] L. Xu, R. Wang, F. Nie, and X. Li, "Efficient top-k feature selection using coordinate descent method," in _Proc. AAAI_, vol. 37, no. 9, 2023, pp. 10 594-10 601.
* [7] C. He, K. Li, Y. Zhang, G. Xu, L. Tang, Y. Zhang, Z. Guo, and X. Li, "Weakly-supervised concealed object segmentation with sam-based pseudo labeling and multi-scale feature grouping," _arXiv preprint arXiv:2305.11003_, 2023.
* [8] M. F. Baln, A. Abid, and J. Zou, "Concrete autoencoders: Differentiable feature selection and reconstruction," in _Proc. ICML_, vol. 97, 2019, pp. 444-453.
* [9] K. Han, Y. Wang, C. Zhang, C. Li, and C. Xu, "Autoencoder inspired unsupervised feature selection," in _Proc. ICASSP_, 2018, pp. 2941-2945.
* [10] M. McPherson, L. Smith-Lovin, and J. M. Cook, "Birds of a feather: Homophily in social networks," _Annu. Rev. Sociol._, vol. 27, no. 1, pp. 415-444, 2001.
* [11] D. I. Shuman, S. K. Narang, P. Frossard, A. Ortega, and P. Vandergheynst, "The emerging field of signal processing on graphs: Extending high-dimensional data analysis to networks and other irregular domains," _IEEE Signal Process. Mag._, vol. 30, no. 3, pp. 83-98, 2013.
* [12] R. Wang, P. Wang, D. Wu, Z. Sun, F. Nie, and X. Li, "Multi-view and multi-order structured graph learning," _IEEE Trans. Neural Netw. Learn. Syst._, pp. 1-12, 2023.
* [13] X. He, D. Cai, and P. Niyogi, "Laplacian score for feature selection," in _Proc. NIPS_, 2005, pp. 507-514.
* [14] O. Lindenbaum, U. Shaham, E. Peterfreund, J. Svirsky, N. Casey, and Y. Kluger, "Differentiable unsupervised feature selection based on a gated laplacian," in _Proc. NIPS_, vol. 34, 2021, pp. 1530-1542.
* [15] U. Von Luxburg, "A tutorial on spectral clustering," _Stat. Comput._, vol. 17, pp. 395-416, 2007.
* [16] F. R. Chung, _Spectral graph theory_. American Mathematical Soc., 1997, vol. 92.
* [17] E. Jang, S. Gu, and B. Poole, "Categorical reparameterization with gumbel-softmax," _arXiv preprint arXiv:1611.01144_, 2017.
* [18] C. J. Maddison, A. Mnih, and Y. W. Teh, "The concrete distribution: A continuous relaxation of discrete random variables," in _Proc. ICLR_, 2017.
* [19] A. Tikhonov and V. Arsenin, _Solutions of Ill-posed Problems_. Winston, 1977.
* [20] S. P. Boyd and L. Vandenberghe, _Convex Optimization_. Cambridge University Press, 2014.
* [21] F. Nie, X. Wang, M. Jordan, and H. Huang, "The constrained laplacian rank algorithm for graph-based clustering," in _Proc. AAAI_, vol. 30, no. 1, 2016.

* [22] L. V. Kantorovich, "Mathematical methods of organizing and planning production," _Manage. Sci._, vol. 6, no. 4, pp. 366-422, 1960.
* [23] F. Santambrogio, _Optimal Transport for Applied Mathematicians_. Birkhauser, 2015.
* [24] G. Peyre and M. Cuturi, "Computational optimal transport," _arXiv preprint arXiv:1803.00567_, 2020.
* [25] M. Cuturi, O. Teboul, and J.-P. Vert, "Differentiable ranking and sorting using optimal transport," in _Proc. NIPS_, vol. 32, 2019.
* [26] Y. Xie, H. Dai, M. Chen, B. Dai, T. Zhao, H. Zha, W. Wei, and T. Pfister, "Differentiable top-k with optimal transport," in _Proc. NIPS_, vol. 33, 2020, pp. 20 520-20 531.
* [27] J.-D. Benamou, G. Carlier, M. Cuturi, L. Nenna, and G. Peyre, "Iterative bregman projections for regularized transportation problems," _SIAM J. Sci. Comput._, vol. 37, no. 2, pp. A1111-A1138, 2015.
* [28] K. Lang, "Newsweeder: Learning to filter netnews," in _Machine Learning Proceedings 1995_. Morgan Kaufmann, 1995, pp. 331-339.
* [29] I. Guyon, J. Li, T. Mader, P. A. Pletscher, G. Schneider, and M. Uhr, "Competitive baseline methods set new standards for the nips 2003 feature selection benchmark," _Pattern Recognit. Lett._, vol. 28, no. 12, pp. 1438-1444, 2007.
* [30] C. L. Nutt, D. R. Mani, R. A. Betensky, P. Tamayo, J. G. Cairncross, C. Ladd, U. Pohl, C. Hartmann, M. E. McLaughlin, T. T. Batchelor, P. M. Black, A. von Deimling, S. L. Pomeroy, T. R. Golub, and D. N. Louis, "Gene Expression-based Classification of Malignant Gliomas Correlates Better with Survival than Histological Classification1," _Cancer Res._, vol. 63, no. 7, pp. 1602-1607, 2003.
* [31] S. A. Nene, S. K. Nayar, H. Murase _et al._, "Columbia object image library (coil-100)," _Tech. Rep. CUCS-005-96_, 1996.
* [32] H. Peng, F. Long, and C. H. Q. Ding, "Feature selection based on mutual information: Criteria of max-dependency, max-relevance, and min-redundancy," _IEEE Trans. Pattern Anal. Mach. Intell._, vol. 27, no. 8, pp. 1226-1238, 2005.
* [33] D. Cai, C. Zhang, and X. He, "Unsupervised feature selection for multi-cluster data," in _Proc. SIGKDD_, 2010, pp. 333-342.
* [34] I. Petricoin, Emanuel F., D. K. Ornstein, C. P. Paweletz, A. Ardekani, P. S. Hackett, B. A. Hitt, A. Velassco, C. Trucco, L. Wiegand, K. Wood, C. B. Simone, P. J. Levine, W. M. Linehan, M. R. Emmert-Buck, S. M. Steinberg, E. C. Kohn, and L. A. Liotta, "Serum Proteomic Patterns for Detection of Prostate Cancer," _J. Natl. Cancer Inst._, vol. 94, no. 20, pp. 1576-1578, 2002.
* [35] M. J. Lyons, J. Budynek, and S. Akamatsu, "Automatic classification of single facial images," _IEEE Trans. Pattern Anal. Mach. Intell._, vol. 21, no. 12, pp. 1357-1362, 1999.
* [36] J. Khan, J. S. Wei, M. Ringner, L. H. Saal, M. Ladanyi, F. Westermann, F. Berthold, M. Schwab, C. R. Antonescu, C. Peterson _et al._, "Classification and diagnostic prediction of cancers using gene expression profiling and artificial neural networks," _Nat. Med._, vol. 7, no. 6, pp. 673-679, 2001.
* [37] A. Spira, J. E. Beane, V. Shah, K. Steiling, G. Liu, F. Schembri, S. Gilman, Y.-M. Dumas, P. Calner, P. Sebastiani _et al._, "Airway epithelial gene expression in the diagnostic evaluation of smokers with suspect lung cancer," _Nat. Med._, vol. 13, no. 3, pp. 361-366, 2007.
* [38] T. Sim, S. Baker, and M. Bsat, "The cmu pose, illumination, and expression database," _IEEE Trans. Pattern Anal. Mach. Intell._, vol. 25, no. 12, pp. 1615-1618, 2003.
* [39] L. Breiman, "Random forests," _Mach. Learn._, vol. 45, pp. 5-32, 2001.
* [40] J. MacQueen, "Classification and analysis of multivariate observations," in _5th Berkeley Symp. Math. Statist. Probability_, 1967, pp. 281-297.

* [41] G. Sokar, Z. Atashgahi, M. Pechenizkiy, and D. C. Mocanu, "Where to pay attention in sparse training for feature selection?" in _Proc. NIPS_, 2022.
* [42] P. Zhu, W. Zuo, L. Zhang, Q. Hu, and S. C. Shiu, "Unsupervised feature selection by regularized self-representation," _Pattern Recognit._, vol. 48, no. 2, pp. 438-446, 2015.
* [43] Y. Yang, H. T. Shen, Z. Ma, Z. Huang, and X. Zhou, "L2,1-norm regularized discriminative feature selection for unsupervised learning," in _Proc. IJCAI_, 2011, pp. 1589-1594.
* [44] S. Miao, Y. Luo, M. Liu, and P. Li, "Interpretable geometric deep learning via learnable randomness injection," in _Proc. ICLR_, 2023.
* [45] M. Blondel, O. Teboul, Q. Berthet, and J. Djolonga, "Fast differentiable sorting and ranking," in _Proc. ICML_, vol. 119, 2020, pp. 950-959.
* [46] W. Liu, J. He, and S.-F. Chang, "Large graph construction for scalable semi-supervised learning," in _Proc. ICML_, 2010, pp. 679-686.
* [47] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay, "Scikit-learn: Machine learning in Python," _J. Mach. Learn. Res._, vol. 12, pp. 2825-2830, 2011.
* [48] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga _et al._, "Pytorch: An imperative style, high-performance deep learning library," _Proc. NIPS_, vol. 32, 2019.

Proof of Proposition 3.1

**Proposition 3.1**.: _Given any real matrix \(\hat{\bm{F}}\in\mathcal{R}^{d\times m}\), one can always generate a column-orthogonal matrix \(\bm{F}\) through Algorithm 1._

Proof.: We begin our proof by showing the feasibility of Algorithm 1 for any real matrix \(\hat{\bm{F}}\), as the eigendecomposition, Cholesky decomposition, and inverse mentioned in the algorithm are subject to specific conditions. For simplicity, we represent \(\bm{A}=\hat{\bm{F}}^{\top}\hat{\bm{F}}+\epsilon\bm{I}_{m}\), with \(\epsilon>0\). Note that for any nonzero real column vector \(\bm{z}\in\mathcal{R}^{m}\), we have

\[\bm{z}^{\top}\bm{A}\bm{z}=\bm{z}^{\top}(\hat{\bm{F}}^{\top}\hat{\bm{F}}+ \epsilon\bm{I}_{m})\bm{z}=(\hat{\bm{F}}\bm{z})^{\top}(\hat{\bm{F}}\bm{z})+ \epsilon\bm{z}^{\top}\bm{z}=\sum_{i=1}^{d}(\hat{\bm{f}}^{i}\bm{z})^{2}+\epsilon \sum_{i=1}^{m}z_{i}^{2}>0.\] (S1)

Hence, the matrix \(\bm{A}\) is positive-definite and can be eigendecomposed as \(\bm{A}=\bm{P}\bm{\Lambda}\bm{P}^{-1}\), where \(\bm{P}\in\mathcal{R}^{m\times m}\) is the square matrix whose \(i\)-th column \(\bm{p}_{i}\) is the eigenvector of \(\bm{A}\) and \(\bm{\Lambda}\in\mathcal{R}^{m\times m}\) is the diagonal matrix whose diagonal entries are the corresponding eigenvalues. Moreover, it is easy to show that \(\bm{A}\) is symmetric, for which we have \(\bm{P}^{\top}=\bm{P}^{-1}\). Therefore, we prove that \(\bm{A}\) can be decomposed as \(\bm{A}=\bm{P}\bm{\Lambda}\bm{P}^{\top}\) (_line 2_ in Algorithm 1).

Since \(\bm{A}\) is symmetric and positive-definite, we will be able to perform Cholesky decomposition on \(\bm{A}\) as \(\bm{L}\bm{L}^{\top}=\bm{A}\) (_line 3_ in Algorithm 1), which yields a lower triangular matrix \(\bm{L}\in\mathcal{R}^{m\times m}\) whose diagonal entries are all real and positive. This means that the determinant of \(\bm{L}\) is larger than zero and \(\bm{L}\) is invertible, which provides the feasibility of \(\bm{L}^{-1}\) (_line 4_ in Algorithm 1). Consequently, the feasibility of Algorithm 1 for any real matrix \(\hat{\bm{F}}\) is proved.

Next, we show that \(\bm{F}\) is a column-orthogonal matrix. We denote \(\bm{Q}=\begin{bmatrix}\bm{\Lambda}^{1/2}\bm{P}^{\top}\\ \hat{\bm{0}}\end{bmatrix}\) and have:

\[\bm{Q}^{\top}\bm{Q}=\begin{bmatrix}\bm{P}\bm{\Lambda}^{1/2},\ \hat{\bm{0}}^{\top} \end{bmatrix}\begin{bmatrix}\bm{\Lambda}^{1/2}\bm{P}^{\top}\\ \hat{\bm{0}}\end{bmatrix}=\bm{P}\bm{\Lambda}\bm{P}^{\top}=\bm{A}=\bm{L}\bm{L}^ {\top}.\] (S2)

Then we prove the orthogonality of the matrix \(\bm{F}\) as follows:

\[\bm{F}^{\top}\bm{F}= \bm{L}^{-1}\bm{Q}^{\top}\bm{Q}(\bm{L}^{-1})^{\top}\] (S3) \[= \bm{L}^{-1}\bm{L}\bm{L}^{\top}(\bm{L}^{-1})^{\top}\] \[= \bm{L}^{-1}\bm{L}\bm{L}^{\top}(\bm{L}^{\top})^{-1}\] \[= \bm{I}_{m}\]

The proof is completed. 

## S2 Discussion on Algorithm 1

Although Algorithm 1 theoretically guarantees the orthogonality of the selection matrix \(\bm{F}\), utilizing this algorithm directly would bring us back to the problem of how to choose features and how to obtain discrete results. On one hand, the non-uniqueness of eigendecomposition in line 2 prevents us from ensuring the discrete properties of matrix \(\bm{F}\). On the other hand, it is important to note that in line 4 of Algorithm 1, we aim to construct a column full-rank matrix \(\bm{Q}=\begin{bmatrix}\bm{\Lambda}^{1/2}\bm{P}^{\top}\\ \hat{\bm{0}}\end{bmatrix}\), whereas the construction of \(\bm{Q}\) is also not unique since we can insert \(d-m\) zero rows at any position within the original matrix \(\bm{\Lambda}^{1/2}\bm{P}^{\top}\) to achieve the column full-rankness. The placement of these zero rows directly affects the result of feature selection.

Guided by Algorithm 1, we devise a more empirical approach by calculating \(\bm{F}\) with \(\bm{F}=\hat{\bm{F}}(\bm{L}^{-1})^{\top}\), which effectively tackles the above two concerns. By doing so, we avoid the non-uniqueness of eigendecomposition, thereby obtaining a solution that is as discrete as \(\hat{\bm{F}}\). Additionally, this approach ensures that the information of feature selection in \(\hat{\bm{F}}\) is retained within the column full-rank matrix.

Actually, \(\bm{F}\) is an \(\epsilon\)-approximation of column-orthogonal matrix, since we have:

\[\bm{F}^{\top}\bm{F} =\bm{L}^{-1}\hat{\bm{F}}^{\top}\hat{\bm{F}}(\bm{L}^{-1})^{\top}\] (S4) \[=\bm{L}^{-1}(\bm{A}-\epsilon\bm{I}_{m})(\bm{L}^{-1})^{\top}\] \[=\bm{L}^{-1}\bm{A}(\bm{L}^{-1})^{\top}-\epsilon\bm{L}^{-1}(\bm{L} ^{-1})^{\top}\] \[=\bm{L}^{-1}\bm{L}\bm{L}^{\top}(\bm{L}^{-1})^{\top}-\epsilon\bm{L }^{-1}(\bm{L}^{-1})^{\top}\] \[=\bm{I}_{m}-\epsilon\bm{L}^{-1}(\bm{L}^{-1})^{\top}.\]

The experimental results in Section 4.2 verify the effectiveness of this approach in successfully avoiding duplicate feature selection.

## S3 Derivation of the Solution to Problem 5

Recall that we aim to solve the following problem to learn an adaptive \(k\)-NN graph:

\[\min_{\bm{S}}\operatorname{tr}(\hat{\bm{X}}^{\top}\bm{L}_{S}\hat {\bm{X}})+\frac{\alpha}{2}\|\bm{S}\|_{F}^{2},\quad\mathrm{s.t.}\;\bm{S1}_{n}= \bm{1}_{n},s_{i,j}\geq 0,s_{i,i}=0,\] (S5) \[= \min_{s_{i,j}}\frac{1}{2}\sum_{i=1}^{n}\sum_{j=1}^{n}\|(\hat{ \bm{x}}^{i}-\hat{\bm{x}}^{j})\|_{2}^{2}s_{i,j}+\alpha_{i}s_{i,j}^{2},\quad \mathrm{s.t.}\;\sum_{j=1}^{n}s_{i,j}=1,s_{i,j}\geq 0,s_{i,i}=0.\]

Based on \(\hat{\bm{X}}\), we define the quantity \(e_{i,j}=\|(\hat{\bm{x}}^{i}-\hat{\bm{x}}^{j})\|_{2}^{2}\), then we solve each row in problem (S5) separately as:

\[\min_{s_{i,j}}\frac{1}{2}\sum_{j=1}^{n}e_{i,j}s_{i,j}+\alpha_{i}s _{i,j}^{2}\quad\mathrm{s.t.}\;\sum_{j=1}^{n}s_{i,j}=1,s_{i,j}\geq 0,s_{i,i}=0,\] (S6) \[= \min_{s_{i,j}}\frac{1}{2}\sum_{j=1}^{n}(s_{i,j}+\frac{e_{i,j}}{2 \alpha_{i}})^{2}\quad\mathrm{s.t.}\;\sum_{j=1}^{n}s_{i,j}=1,s_{i,j}\geq 0,s_{i,i}=0,\] \[= \min_{\bm{s}^{i}}\frac{1}{2}\|\bm{s}^{i}+\frac{\bm{e}^{i}}{2 \alpha_{i}}\|_{2}^{2}\quad\mathrm{s.t.}\;\bm{s}^{i}\bm{1}_{n}=1,s_{i,j}\geq 0,s_ {i,i}=0.\]

We first omit the constraint \(s_{i,i}=0\) and consider it later, and solve problem (S6) with the first two constraints, the Lagrangian function of which is as follows:

\[\mathcal{L}(\bm{s}^{i},\lambda_{i},\bm{\beta}^{i})=\frac{1}{2}\|\bm{s}^{i}+ \frac{\bm{e}^{i}}{2\alpha_{i}}\|^{2}-\lambda_{i}(\bm{s}^{i}\bm{1}_{n}-1)-\sum_ {j=1}^{n}s_{i,j}\beta_{i,j},\] (S7)

where \(\lambda_{i}\) and \(\beta_{i,j}\) are Lagrange multipliers. The derivative of \(\mathcal{L}(\bm{s}^{i},\lambda_{i},\bm{\beta}^{i})\) w.r.t. \(s_{i,j}\) is:

\[\frac{\partial\mathcal{L}}{\partial s_{i,j}}=s_{i,j}+\frac{e_{i,j}}{2\alpha_{i }}-\lambda_{i}-\beta_{i,j}\] (S8)

Then we have the Karush-Kuhn-Tucker(KKT) conditions [20] of problem (S7) as follows:

\[\begin{cases}s_{i,j}+\frac{e_{i,j}}{2\alpha_{i}}-\lambda_{i}-\beta_{i,j}=0\\ \sum_{j=1}^{n}s_{i,j}=1\\ s_{i,j}\geq 0\\ \beta_{i,j}s_{i,j}=0.\end{cases}\] (S9)

Then we have:

\[s_{i,j}=(\lambda_{i}-\frac{e_{i,j}}{2\alpha_{i}})_{+}\] (S10)Recall that there are only \(k\) nonzero elements in \(\bm{s}^{i}\) corresponding to the nearest neighbors of sample \(i\), according to the constraint \(\sum_{j=1}^{n}s_{i,j}=1\) on \(k\) nonzero entries in \(\bm{s}^{i}\), we have:

\[\sum_{j=1}^{k}(\lambda_{i}-\frac{e_{i,\sigma_{j}}}{2\alpha_{i}})=1\Rightarrow \lambda_{i}=\frac{1}{k}+\frac{1}{k}\frac{\bm{e}^{i}\bm{\delta}_{i}^{(k)}}{2 \alpha_{i}}\quad\mathrm{with}\quad\delta_{i,j}^{(k)}=\mathrm{Bool}(e_{i,j} \leq e_{i,\sigma_{k}}),\] (S11)

where \(\bm{\delta}_{i}^{(k)}\) denotes the selection vector identifying the \(k\) minimal values in \(\bm{e}^{i}\), and \(\bm{\sigma}=[\sigma_{1},\dots,\sigma_{n}]\) denotes the sorting permutation over \(\bm{e}^{i}\), i.e. \(e_{i,\sigma_{1}}\leq\dots\leq e_{i,\sigma_{n}}\). Without loss of generality, we assume \(\bm{e}^{i}\) has no duplicates, namely \(e_{i,\sigma_{1}}<\dots<e_{i,\sigma_{n}}\). Considering the constraint \(s_{i,i}=0\), since \(e_{i,i}=0\) being the minimal value in \(\bm{e}^{i}\) holds for all samples, we replace \(e_{i,i}\) with a sufficiently large value to skip over this trivial solution.

Substituting (S11) into (S10), we have:

\[s_{i,j}=(\frac{1}{k}+\frac{1}{k}\frac{\bm{e}^{i}\bm{\delta}_{i}^{(k)}}{2 \alpha_{i}}-\frac{e_{i,j}}{2\alpha_{i}})+\] (S12)

Recall that there are only \(k\) nonzero entries in \(\bm{s}^{i}\), we have

\[\frac{1}{k}+\frac{1}{k}\frac{\bm{e}^{i}\bm{\delta}_{i}^{(k)}}{2\alpha_{i}}- \frac{e_{i,\sigma_{k}}}{2\alpha_{i}}>0,\quad\frac{1}{k}+\frac{1}{k}\frac{\bm{e }^{i}\bm{\delta}_{i}^{(k)}}{2\alpha_{i}}-\frac{e_{i,\sigma_{k+1}}}{2\alpha_{i} }\leq 0.\] (S13)

Note that we assume \(\alpha_{i}>0\), then we have

\[\frac{1}{2}(k\bm{e}^{i}\bm{\xi}_{i}^{(k)}-\bm{e}^{i}\bm{\delta}_{i}^{(k)})< \alpha_{i}\leq\frac{1}{2}(k\bm{e}^{i}\bm{\xi}_{i}^{(k+1)}-\bm{e}^{i}\bm{ \delta}_{i}^{(k)})\quad\mathrm{with}\quad\xi_{i,j}^{(k)}=\mathrm{Bool}(e_{i,j} =e_{i,\sigma_{k}}),\] (S14)

where \(\bm{\xi}_{i}^{(k)}\) is an indicator vector identifying the \(k\)-th minimal value in \(\bm{e}^{i}\). According to (S14), we set \(\alpha_{i}\) as its maximal value as follows:

\[\alpha_{i}=\frac{1}{2}(k\bm{e}^{i}\bm{\xi}_{i}^{(k+1)}-\bm{e}^{i}\bm{\delta}_{ i}^{(k)}).\] (S15)

Substituting S15 into Eq. S12, we have:

\[\begin{split} s_{i,j}&=(\frac{1}{k}+\frac{1}{k} \frac{\bm{e}^{i}\bm{\delta}_{i}^{(k)}}{2\alpha_{i}}-\frac{e_{i,j}}{2\alpha_{i} })+\\ &=(\frac{2\alpha_{i}+\bm{e}^{i}\bm{\delta}_{i}^{(k)}-ke_{i,j}}{2k \alpha_{i}})+\\ &=(\frac{k\bm{e}^{i}\bm{\xi}_{i}^{(k+1)}-\bm{e}^{i}\bm{\delta}_{ i}^{(k)}+\bm{e}^{i}\bm{\delta}_{i}^{(k)}-ke_{i,j}}{k(k\bm{e}^{i}\bm{\xi}_{i}^{(k+ 1)}-\bm{e}^{i}\bm{\delta}_{i}^{(k)})})+\\ &=(\frac{\bm{e}^{i}\bm{\xi}_{i}^{(k+1)}-e_{i,j}}{ke^{i}\bm{\xi}_{ i}^{(k+1)}-\bm{e}^{i}\bm{\delta}_{i}^{(k)}})+\end{split}\] (S16)

**Eq.** (S16) **is used for implementation in our code.** Note that since \(e_{i,\sigma_{1}}<\dots<e_{i,\sigma_{k}}<e_{i,\sigma_{k+1}}<\dots e_{i,\sigma_{ n}}\), we have

\[k\bm{e}^{i}\bm{\xi}_{i}^{(k+1)}-\bm{e}^{i}\bm{\delta}_{i}^{(k)}=ke_{i,\sigma_{ k+1}}-\sum_{p=1}^{k}e_{i,\sigma_{p}}=\sum_{p=1}^{k}(e_{i,\sigma_{k+1}}-e_{i, \sigma_{p}})>0.\] (S17)

Then we obtain the solution of \(s_{i,j}\) as

\[s_{i,\sigma_{j}}=Connection to CLR

We note that a similar formulation to problem 6 has been proposed in [21] (coined CLR), which expects closer samples to have higher similarity. It aligns with the notion of "smoothness" as we mentioned in Section 2. However, our method differs from CLR in at least two crucial aspects: Firstly, CLR measures the distance quantity \(e_{i,j}\) across all original features, making it more sensitive to the noise and irrelevant features in the original data. In contrast, our approach learns the graph structure using only informative features, resulting in enhanced robustness against noisy features. Secondly, it is important to note that CLR is proposed in the context of traditional machine learning, where optimization is straightforward, as \(\bm{\delta}_{i}^{(k)}\) and \(\bm{\xi}_{i}^{(k+1)}\) can be updated using off-the-shelf sorting algorithms. Different from CLR, problem 6 is introduced in the realm of deep learning, where conventional sorting algorithms are non-differentiable and not applicable. This poses a huge challenge in learning an adaptive \(k\)-NN graph in neural networks. To overcome this challenge, we proposed to transform the top-\(k\) selection into a differentiable operator using the Optimal Transport technique.

## Appendix S5 Iterative Bregman Projections

In this paper, we employ the iterative Bregman projections [27] algorithm to solve the following problem:

\[\min_{\bm{\Gamma}}\langle\bm{C},\bm{\Gamma}\rangle+\gamma\sum_{i,j}\Gamma_{i,j }\log\Gamma_{i,j},\quad\mathrm{s.t.}\ \bm{\Gamma}\bm{1}_{k+2}=\bm{\mu},\bm{\Gamma}^{\top}\bm{1}_{n}=\bm{\nu},\Gamma_ {i,j}\geq 0.\] (S20)

We first initialize two variables \(\bm{u}\in\mathcal{R}^{k+2}\) and \(\bm{K}\in\mathcal{R}^{n\times(k+2)}\) as \(u_{i}=1/(k+2)\) and \(k_{i,j}=e^{-c_{i,j}/\gamma}\), respectively. Then based on the following formulations, we repeatedly updating \(\bm{u}\) and \(\bm{v}\) for \(\zeta\) iterations:

\[\bm{v}=\frac{\bm{\mu}}{\bm{K}\bm{u}},\quad\bm{u}=\frac{\bm{\nu}}{\bm{K}^{\top} \bm{v}},\] (S21)

where the division in Eq. (S21) is element-wise. In this paper, we set \(\zeta\) as \(200\). After updating \(\zeta\) iteration, we obtain the optimal transport plan \(\bm{\Gamma}\) as

\[\bm{\Gamma}=\mathrm{diag}(\bm{v})\bm{K}\mathrm{diag}(\bm{u}).\] (S22)

## Appendix S6 Supplementary Experimental Details

### Implementation Details

All experiments are conducted on a server equipped with an RTX 3090 GPU and an Intel Xeon Gold 6240 (18C36T) @ 2.6GHz x 2 (36 cores in total) CPU.

#### s6.1.1 Synthetic Datasets

We generate three datasets for toy experiments: (1) Blobs, (2) Moons, and (3) Circles. For each dataset, we generate the first two features using the scikit-learn library [47] by adding noise sampled from \(\mathcal{N}(0,0.1)\). Additionally, we generate \(18\)-dimensional noise features sampled from \(\mathcal{N}(0,1)\).

#### s6.1.2 Competing Methods

The implementation details of different methods, as well as their corresponding parameter selections are provided below:

* **Our Method**: Our method is implemented using the PyTorch framework [48]. We train our method using the Adam optimizer for \(1000\) epochs on all datasets, with the learning rate searched from \(\{10^{-4},10^{-3},10^{-2},10^{-1},10^{0},10^{1}\}\). We search the parameter \(\gamma\) in \(\{10^{-3},10^{-2},10^{-1}\}\) and the parameter \(k\) in \(\{1,2,3,4,5\}\). Note that the implementation of differentiable top-\(k\) selector is based on the code provided by [26] in https://papers.nips.cc/paper_files/paper/2020/hash/ec24a54d62ce57ba93a531b460fa8d18-Abstract.html, which provides a more memory-saving backward implementation compared to directly using the autograd method in PyTorch.

* Where to Pay Attention in Sparse Training (**WAST**) [41]: We use the official code released in https://github.com/GhadaSokar/WAST. The parameter settings were adopted in accordance with Appendix A.1 of the original paper. Specifically, we train each dataset for 10 epochs using stochastic gradient descent with a learning rate of 0.1 for all datasets except for SMK, where the learning rate is set to 0.01. For the parameter \(\lambda\), we set \(\lambda=0.9\) on Madelon and PCMAC, \(\lambda=0.4\) on all image datasets, \(\lambda=0.1\) on all biological datasets except SMK, and \(\lambda=0.01\) on SMK. The remaining parameters are kept as they were in the original paper.
* Differentiable Unsupervised Feature Selection (**DUFS**) [14]: We use the official code released in https://github.com/Ofirlin/DUFS and use the parameter-free loss version of DUFS. For all datasets, we set \(k=2\), and train the method with SGD with a learning rate of \(1\) for \(10000\) epochs according to Appendix S7 in the original paper. We set the parameter \(C=5\) on all datasets except for SRBCT, COIL, and PIX10, where \(C\) is set to \(2\).
* Concrete AutoEncoder (**CAE**) [8]: We use the official code released in https://github.com/mfbalin/Concrete-Autoencoders. Since we could not find too much description about the parameter settings on different datasets in the original paper, we run CAE with default settings in the code.
* AutoEncoder Feature Selector (**AEFS**) [9]: The original code provided by the authors is implemented in MATLAB, and it requires a prohibitively long time to run this method on MATLAB. Therefore, following the treatment in [41], we use the code provided by the authors of CAE in https://github.com/Ofirlin/DUFS (see experiments/generate_comparison.figures.py in their repository). We search the parameter \(\alpha\) in \(\{10^{-9},10^{-6},10^{-3},10^{0},10^{3},10^{6},10^{9}\}\), and the size of the hidden layer in \(\{128,256,512,1024\}\).
* Laplacian Score (**LS**) [13]: We use the official code released in http://www.cad.zju.edu.cn/home/dengcai/Data/ReproduceExp.html#LaplacianScore. We use the heat kernel for graph construction, and fix the size of neighbors \(k\) as 5 for all datasets.
* Regularized Self-Representation (**RSR**) [42]: We use the official code released in https://github.com/AISKYEYE-TJU/RSR-PR2015. We search the parameter \(\lambda\) in \(\{10^{-9},10^{-6},10^{-3},10^{0},10^{3},10^{6},10^{9}\}\).
* Unsupervised Discriminative Feature Selection (**UDFS**) [43]: We use the code provided in https://guijiejie.github.io/code.html. We use the heat kernel for graph construction, and fix the size of neighbors \(k\) as 5 for all datasets. We search the parameter \(\gamma\) in \(\{10^{-9},10^{-6},10^{-3},10^{0},10^{3},10^{6},10^{9}\}\).

#### s6.1.3 Evaluation Workflow

The overall evaluation workflow in Section 4.3 is shown in Algorithm S2, which includes two steps:

1. Given the dataset \(\bm{X}\) and a prespecified feature number \(m\), we first randomly split the dataset using an \(8:2\) ratio and select features based on the training set \(\bm{X}_{tr}\) by FS method \(\mathfrak{F}\) using different parameters \(\bm{\Theta}\), as shown in Algorithm S1. This allows us to obtain FS results under different parameter candidates \(\theta_{i}\), along with the corresponding reduced training data \(\bm{X}^{\prime}_{tr}\) and testing data \(\bm{X}^{\prime}_{te}\). Based on the reduced data, we perform classification tasks on these datasets with the random forest, thereby obtaining the classification performance for each parameter combination. We select the parameter combination with the best classification performance as the optimal parameter \(\theta^{*}\) for \(\mathfrak{F}\).
2. Based on the optimal parameter \(\theta^{*}\), we construct the FS model \(\mathfrak{F}_{\theta^{*}}\) and evaluate its performance in different downstream tasks. To avoid randomness, we randomly split the dataset \(10\) times. With each random split, we use the training set \(\bm{X}_{tr}\) to select features, and obtain reduced training set \(\bm{X}^{\prime}_{tr}\) and testing set \(\bm{X}^{\prime}_{te}\). We use these sets for downstream tasks including classification, clustering, and reconstruction, and obtain corresponding performance metrics. For each downstream task, we calculate the average metric over \(10\) runs as the performance of \(\mathfrak{F}\) for the given number \(m\).

For each dataset, we vary the value of \(m\) and follow the aforementioned procedure to obtain the corresponding performance. For each downstream task, we report the best metric and the corresponding feature number \(m\) as the performance of the FS method in this downstream task.

```
0: Training data \((\bm{X}_{tr},\bm{y}_{tr})\), testing data \((\bm{X}_{te},\bm{y}_{te})\), selected number \(m\), FS method \(\mathfrak{F}\), and parameter set \(\bm{\Theta}=\{\theta_{i}\}\).
0: Optimal parameter \(\theta^{*}\).
1:for\(\theta_{i}\) in \(\bm{\Theta}\)do
2:\(\bm{\xi}=\mathfrak{F}_{\theta_{i}}(\bm{X}_{tr},m)\); \(\triangleright\) Determining selected features \(\bm{\xi}\) by \(\mathfrak{F}\) under the parameter \(\theta_{i}\).
3:\(\bm{X}^{\prime}_{tr}=\bm{X}_{tr}(:,\bm{\xi})\), \(\bm{X}^{\prime}_{te}=\bm{X}_{te}(:,\bm{\xi})\); \(\triangleright\) Generating reduced datasets using selected features.
4: ACC = RF(\(\bm{X}^{\prime}_{tr}\), \(\bm{y}_{tr}\), \(\bm{X}^{\prime}_{te}\), \(\bm{y}_{te}\));\(\triangleright\) Evaluating selected features with random forest classifier.
5:if ACC > ACC\({}^{*}\)then
6: ACC\({}^{*}\) = ACC;
7:\(\theta^{*}=\theta_{i}\);
8:endif
9:endfor ```

**Algorithm S1** Param_tuning

```
0: Original dataset \((\bm{X},\bm{y})\), selected feature number \(m\), FS method \(\mathfrak{F}\), parameter set \(\bm{\Theta}=\{\theta_{i}\}\), and downstream tasks \(\mathfrak{T}=\{\mathcal{T}_{i}\}\).
0: Performance \(\bm{M}=\{M_{i}\}\) in downstream tasks.
1: Partitioning the dataset into training data \((\bm{X}_{tr},\bm{y}_{tr})\) and testing data \((\bm{X}_{te},\bm{y}_{te})\).
2: Determining \(\theta^{*}\) by Param_tuning(\((\bm{X}_{tr},\bm{y}_{tr})\), \((\bm{X}_{te},\bm{y}_{te})\), \(m\), \(\mathfrak{F}\), \(\bm{\Theta}\)) in Algorithm S1;
3:for\(j=1:10\)do
4: Partitioning the dataset into training data \((\bm{X}_{tr},\bm{y}_{tr})\) and testing data \((\bm{X}_{te},\bm{y}_{te})\).
5:\(\bm{\xi}^{*}=\mathfrak{F}_{\theta^{*}}(\bm{X}_{tr},m)\);
6:\(\bm{X}^{\prime}_{tr}=\bm{X}_{tr}(:,\bm{\xi}^{*})\), \(\bm{X}^{\prime}_{te}=\bm{X}_{te}(:,\bm{\xi}^{*})\);
7:for\(\mathcal{T}_{i}\) in \(\mathfrak{T}\)do
8:\(m\{i,j\}\) = \(\mathcal{T}_{i}(\bm{X}^{\prime}_{tr},\bm{y}_{tr},\bm{X}^{\prime}_{te},\bm{y}_ {te})\); \(\triangleright\) Evaluating the performance in downstream tasks.
9:endfor
10:endfor
11:for\(\mathcal{T}_{i}\) in \(\mathfrak{T}\)do
12:\(M_{i}\) = Average(\(m\{i,:\}\));
13:endfor ```

**Algorithm S2** Overall evaluation workflow

#### s6.1.4 Evaluation Metrics

We employ two metrics in our experiments: the accuracy (ACC) and the root mean square error (RMSE) normalized by \(d\).

The formulation of ACC is

\[\mathrm{ACC}(\bm{y},\hat{\bm{y}})=\frac{\sum_{i=1}^{n}\text{Bool}(y_{i}=\hat{y }_{i})}{n},\] (S23)

where \(\bm{y}\in\mathcal{R}^{n}\) denotes the groundtruth labels and \(\hat{\bm{y}}\in\mathcal{R}^{n}\) denotes the prediction label.

The formulation of RMSE normalized by \(d\) is

\[\mathrm{RMSE}(\bm{X},\hat{\bm{X}})=\sqrt{\frac{\sum_{i=1}^{n}\|\bm{x}^{i}-\hat{ \bm{x}}^{i}\|_{2}^{2}}{n\times d}},\] (S24)

where \(\bm{X}=[\bm{x}_{1};\bm{x}_{2};\ldots;\bm{x}_{n}]\) and \(\hat{\bm{X}}=[\hat{\bm{x}}_{1};\hat{\bm{x}}_{2};\ldots;\hat{\bm{x}}_{n}]\) denote the original feature matrix and the reconstructed feature matrix, respectively.

#### s6.1.5 Formulation of Heat Kernel Method

Here we describe the heat kernel (Heat) method compared in this paper. To implement Heat, we first compute the similarity matrix \(\hat{\bm{S}}\) as follows:

\[\hat{s}_{i,j}=\exp(-\frac{\|\bm{x}^{i}-\bm{x}^{j}\|_{2}^{2}}{2\sigma^{2}}),\] (S25)

[MISSING_PAGE_EMPTY:20]

[MISSING_PAGE_FAIL:21]

### Additional Experiment: Parameter Sensitivity Analysis

In this section, we analyze the effect of the parameters of our method, including the learning rate, the number of nearest neighbors \(k\), the hyperparameter \(\gamma\) in the entropy regularized OT problem 14, and the selected feature number \(m\). We use four real-world datasets, including a text dataset PCMAC, an artificial dataset Madelon, an image dataset Jaffe, and a biological dataset PROSTATE.

The analysis is based on the optimal parameter obtained in Section 4.3. For each dataset, we fix the values of the remaining parameters and vary the value of one parameter at a time. We retrain our method using the updated parameter combination and evaluate the corresponding FS result with the random forest. This allows us to observe the impact of different parameters on the performance of our method. For example, to analyze the effect of the learning rate on Madelon, we keep \(k\), \(\gamma\), and \(m\) at their optimal values, then we vary the learning rate in \(\{10^{-4},10^{-3},10^{-2},10^{-1},10^{0},10^{1}\}\) (the range as we described in Appendix S6.1.2), and evaluate their corresponding performance in the classification task with the random forest. The overall results are shown in Fig. S3, where the stars represent the results using optimal parameters.

One the one hand, we observe that the variations in the learning rate, \(k\), \(\gamma\) have little impact on the performance of our method across different datasets. This suggests that we can set a value within a proper range for these parameters, without the need to determine their values on different datasets. On the other hand, the most sensitive parameter is \(m\), where a higher number of features contributes to better results, aligning with intuition and observations from existing literature. However, it is important to emphasize that more features are not always better. As demonstrated in Section 4.3, the FS methods consistently outperform AllFea in most classification and clustering tasks. Fewer features not only result in lower computational costs but also contribute to faster learning speeds. This suggests the need to adjust the value of \(m\), for example, starting with a relatively small value and gradually increasing it until the model performance begins to decline.