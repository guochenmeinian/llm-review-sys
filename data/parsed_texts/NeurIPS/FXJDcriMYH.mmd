# Stacking Your Transformers: A Closer Look at Model Growth for Efficient LLM Pre-Training

 Wenyu Du\({}^{1}\)1 Tongxu Luo\({}^{2,3}\)2 Zihan Qiu\({}^{4}\) Zeyu Huang\({}^{5}\) Yikang Shen\({}^{6}\)

Reynold Cheng\({}^{1}\) Yike Guo\({}^{2}\) Jie Fu\({}^{2}\)3

\({}^{1}\)School of Computing and Data Science, The University of Hong Kong \({}^{2}\)HKUST

\({}^{3}\)USTB \({}^{4}\)Tsinghua University \({}^{5}\)University of Edinburgh \({}^{6}\)MIT-IBM Watson AI Lab

wydu@cs.hku.hk tongxuluo@gmail.com jiefu@ust.hk

Equal Contributions.Work done during interning at HKUST.Corresponding Author.

Footnote 1: footnotemark:

Footnote 2: footnotemark:

Footnote 3: footnotemark:

###### Abstract

LLMs are computationally expensive to pre-train due to their large scale. Model growth emerges as a promising approach by leveraging smaller models to accelerate the training of larger ones. However, the viability of these model growth methods in efficient LLM pre-training remains underexplored. This work identifies three critical _O_bstacles: (_O_1) lack of comprehensive evaluation, (_O_2) untested viability for scaling, and (_O_3) lack of empirical guidelines. To tackle \(O\)1, we summarize existing approaches into four atomic growth operators and systematically evaluate them in a standardized LLM pre-training setting. Our findings reveal that a depth-wise stacking operator, called \(G_{\text{stack}}\), exhibits remarkable acceleration in training, leading to decreased loss and improved overall performance on eight standard NLP benchmarks compared to strong baselines. Motivated by these promising results, we conduct extensive experiments to delve deeper into \(G_{\text{stack}}\) to address \(O\)2 and \(O\)3. For \(O\)2 (untested scalability), our study shows that \(G_{\text{stack}}\) is scalable and consistently performs well, with experiments up to 7B LLMs after growth and pre-training LLMs with 750B tokens. For example, compared to a conventionally trained 7B model using 300B tokens, our \(G_{\text{stack}}\) model converges to the same loss with 194B tokens, resulting in a 54.6% speedup. We further address \(O\)3 (lack of empirical guidelines) by formalizing guidelines to determine growth timing and growth factor for \(G_{\text{stack}}\), making it practical in general LLM pre-training. We also provide in-depth discussions and comprehensive ablation studies of \(G_{\text{stack}}\). Our code and pre-trained model are available at https://llm-stacking.github.io/.

## 1 Introduction

Emergent abilities of Large Language Models (LLMs) rely on scaling-up [1, 2]. Empirical evidence from scaling laws [3, 4, 5] fuels the development of increasingly larger models, pushing the boundaries of LLMs capabilities. However, pre-training these gigantic models comes at a significant cost in terms of energy consumption and environmental impact [6] (e.g., pre-training Llama-3 [7] consumes a total of 7.7M GPU hours and generates 2290 tons of carbon dioxide equivalent of carbon emissions). The efficient pre-training of LLMs is thus crucial, both from a scientific and a societal perspective, to ensure the continual growth and adoption of AI [8, 9].

One promising research direction to accelerate model training involves leveraging trained smaller (base) models to expedite the training of larger (target) models, a technique known as model growth.

Concretely, model growth studies how to leverage the trained smaller model's parameters \(\Theta^{(s)}\) to initialize the larger model's parameters \(\Theta^{(l)}\). Current popular methods generally focus on expanding the parameters of the base model through techniques like splitting [10; 11; 12], copying [13; 14], or matrix mapping [15]. There are also some approaches that initialize new parameters from scratch [16; 12; 17]. The primary objective is to accelerate the training of large models, and existing methods demonstrate promising speedup results on models such as BERT [11; 14; 18; 15; 12; 13]. Despite such empirical evidence and its alignment with the goal of efficient LLM pre-training, model growth methods are not widely adopted in the context of LLM pre-training [7; 19]. To our best knowledge, the only LLM that utilizes model growth for accelerating is FLM-101B [20], but it lacks a baseline LLM trained from scratch to compare. We observe three key _O_bstacles that hinder LLM pre-training from using existing model growth techniques, specifically:

\(\bullet\)_O_1: Lack of comprehensive assessment. Some existing model growth methods report results on LLM pre-training, but either lack a baseline comparison [20] or are still in exploratory stages [15; 13]. In contrast, most growth approaches are evaluated in encoder-based BERT models [14; 11; 18; 12; 13; 16; 17], which have different architecture and training configurations compared to prominent decoder-based LLMs such as Llama [21].

\(\bullet\)_O_2: The untested scalability. This scalability has two aspects: the model size and the amount of pre-training data. Regarding the model size, the existing approaches are only evaluated on smaller-scale BERT models or in preliminary experiments with LLMs. It is unclear whether these growth methods will continue accelerating training when applied to large-scale LLMs with more extensive evaluation. As for the amount of pre-training data, there are debates [22] over whether certain efficient training strategies may initially converge faster but ultimately perform similarly or worse than vanilla training methods when given ample computational resources (i.e., more training data).

\(\bullet\)_O_3: Lack of empirical guidelines. Scaling laws [3; 4] give clear empirical guidelines on pre-training computational-optimized LLMs, greatly stimulating and advancing the field. Yet, there is a lack of empirical guidelines on growth techniques, discouraging LLM practitioners from adopting these approaches, especially considering the high costs of LLM pre-training.

These three obstacles are consequential in nature. Hence, in this work, we empirically revisit the concept of model growth as a solution to efficient LLM pre-training by tackling them one by one.

To tackle \(O\)1, we systematically evaluate model growth techniques on practical LLM pre-training. We first categorize existing growth methods and summarize them into four atomic growth operators, each of which can grow along two directions: widthwise (intra-layer) and depthwise (layer-wise). We illustrate them in Figure 2. These operators serve as representative choices for evaluating the performance of model growth techniques. We use these operators to expand 400M base models to 1.1B Llama-like LLMs and continually pre-train them. Next, we evaluate these growth techniques on the training loss and eight standard NLP benchmarks from the Harness toolkit [23]. We found the direct operator that stacks depthwisely \(G_{stack}\) consistently outperforms others across overall evaluation metrics, demonstrating its potential in accelerating LLM pre-training. This motivates us to investigate extensively by addressing \(O\)2 and \(O\)3 on \(G_{stack}\).

To address \(O\)2, we investigate the \(G_{stack}\) operator's scalability to larger model sizes and to more training data. We conduct extensive experiments by scaling model size up to 7B parameters trained with 300B tokens, and pre-training a 410M model with over 750B training tokens. This is in contrast to the previous largest LLM pre-training experiment that uses model growth methods and has baselines for comparison, which is reported in Ligo [15], where a GPT2-1.5B model is trained for 15k steps (approximately 15B tokens). The

Figure 1: The training loss for two 7B LLMs, trained from scratch and with \(G_{direct}^{\dagger}\) (\(G_{stack}\)). At 300B tokens, \(G_{stack}\) accelerates by 54.6% compared to scratch.

results are encouraging, as we consistently observe significant improvements \(G_{\text{stack}}\) offers in both scenarios. For example, we achieve a remarkable 54.6% speedup in pre-training for a 7B model with 300B tokens (Figure 1). Interestingly, the loss improvement in our 750B-token experiment aligns with a logarithmic function. We further extend this logarithmic curve and determine that the improvement continues to be substantial even for the LLM trained with over 8T tokens. Moreover, we summarize all our experiments by estimating the LLM scaling law for LLMs pre-trained with \(G_{\text{stack}}\). Given the same target loss value, our analysis reveals a significantly reduced computational cost compared to the common scaling law [4].

For \(O\)3, we explore the practical guidelines for using \(G_{\text{stack}}\) in LLM pre-training. Given a computational budget, we determine the optimal strategy for two key factors of \(G_{\text{stack}}\), growth timing \(d\) and growth factor \(g\). Growth timing \(d\) relates to the training tokens used for small models before growing, and growth factor \(g\) refers to the factor between the non-embedding parameter number of the large models and the small models. We formalize our findings into equations that offer concrete suggestions for utilizing \(G_{\text{stack}}\). We believe this work could significantly pique the interest and bolster confidence in future LLM pre-training with model growth techniques, both in academia and industry.

To summarize, our contributions are four-fold: 1) We first systematically investigate model growth techniques and identify four atomic model growth operators, establishing a better understanding of the field in Section 3.1. 2) We then design a standard LLM pre-training testbed and perform comprehensive evaluations on these operators, finding that a simple depthwise stacking \(G_{\text{stack}}\) exhibits significant superiority in Section 3. 3) We further demonstrate the scalability of \(G_{\text{stack}}\) with experiments on LLMs ranging from 410M to 7B parameters and up to 750B training tokens in Section 4.1. 4) We also provide guidelines of equations on determining growth timing and growth factors for optimal use of \(G_{\text{stack}}\) in Section 4.2.

## 2 Related Work - Model Growth for Efficient Pre-training

The idea of growing neural networks dates back to the 1990s [24; 25; 26]. The pioneering work of Net2Net [10] marks a milestone, for the first attempt to study model growth in deep learning era. Net2Net expands width and depth while keeping original functions (namely function preserving) via randomly splitting old neurons and injecting new identity layers. The widthwise splitting method of Net2Net represents a series of works that aim to "expand" the existing neurons to the desired larger size. Bert2Bert [11] serves as a BERT-based extension of the widthwise Net2Net. StagedGrow[13] doubles the width by concatenating two identical layers and halves final loss to keep function-preserving. Lemon [12] suggests integrating a parameter into the splitting of neurons in Bert2Bert, aiming to break weight symmetry. Depthwisely, StackedBert [14] simply stacks duplicated layers to form a deeper model. In contrast to the above direct copy/split approaches, LiGO [15]presents a learning-based method that initializes the larger model's parameters via learning a linear mapping from the smaller model's parameters.

Alongside the approaches that expand existing parameters, there are works that initialize new ones without relying on existing ones. For instance, MSG [17] proposes a multi-staged growing strategy that progressively expands transformer components, where the newly grown neurons are randomly initialized using a masking mechanism to ensure function preservation. Besides, some works have assigned specific values, like zero, to the newly initialized neurons to negate their influence [16; 12].

All the above methods are primarily explored in BERT or earlier stages of LLM pre-training. On the other hand, our objective is to present the first systematic review of model growth techniques in the LLMs era. To our knowledge, FLM-101B [20] is the only existing LLM that uses the growth method [17] for accelerating billion-scale LLM pre-training. Nonetheless, this work lacks a baseline model trained from scratch, making it difficult to assess the effectiveness of the model growth technique. In contrast, we aim to provide a comprehensive study by establishing a standardized testbed to compare LLMs trained from scratch and with various growth methods in LLM pre-training.

## 3 Systematically Assessing Model Growth for LLM Pre-Training

Existing model growth methods [14; 11; 18; 15; 12; 13; 16; 17] are mainly evaluated on BERT [27], with limited focus on decoder-only large-scale language models such as Llama [21]. Moreover, these growth methods are often not comparable due to different training settings [14; 11; 17; 12].

Even some growth LLMs experiments are evaluated, their results are often incomplete [20; 15]. To overcome these limitations, we first summarize existing works [14; 11; 18; 15; 12; 13; 16; 17] into four atomic growth operators to represent these growth techniques. Then we build a standardized LLMs training testbed to pre-train LLMs with four growth operators on depthwise and widthwise directions and evaluate the results with both training loss and eight evaluation metrics in Harness [23].

### Growing LLMs with Growth Operators

Recent years, researchers have focused on enhancing the efficiency of training large models by making use of smaller pre-existing models [10; 11; 14; 18; 15; 12; 13; 16; 17]. These state-of-the-art methods can be categorized into two distinct groups. The first group focuses on deriving new neurons from the existing ones [10; 11; 14; 12; 15], while the second group focuses on initializing new parameters separately [18; 13; 16; 17]. Drawing from these two lines of research, we summarize four **atomic growth operators**. These operators include: **(A)** directly duplicating and stacking old layers in a depthwise manner or splitting neurons in the same layer widthwisely, denoted as \(G_{\text{direct}}\), **(B)** generating expanded parameters using a learnable mapping matrix to the existing parameters, denoted as \(G_{\text{learn}}\), **(C)** setting the new parameters to zero, denoted as \(G_{\text{zero}}\), and **(D)** randomly initializing the new parameters, denoted as \(G_{\text{random}}\). The illustration of four operators is shown in Figure 2. The \(G_{\text{direct}}\) and \(G_{\text{learn}}\) growth operators produce new neurons from the current ones, in contrast to \(G_{\text{zero}}\) and \(G_{\text{random}}\) which initialize new parameters independently. _For the formal definitions of the operators and the differences to the existing growth methods in design, please refer to Appendix A_. Complex growth methods, such as those involving auxiliary loss or exploring training dynamics like learning rates [28; 29; 16] are interesting. But considering the high computational cost of LLM pre-training, we focus on simple, universally applicable growth operators for different LLM pre-training settings.

To make a fair comparison of the four growth operators for LLM pre-training, we define a standardized "one-hop" growth process that involves two training phases, small model training before growth and large model training after growth. We first train the small LLMs with \(d\) tokens before growing. Then, we use operator \(G\) to grow them to the target LLMs by a factor of \(g\) for

Figure 2: The simplified illustration of four growth operators \(G_{\text{direct}}\), \(G_{\text{learn}}\), \(G_{\text{zero}}\) and \(G_{\text{random}}\), each of which can grow along widthwise (intra-layer) \(G^{\rightarrow}\) or depthwise (layer-wise) \(G^{\uparrow}\). \(\mathbf{W_{n}}\) is the parameters before growth, while \(\mathbf{D_{n}}\), \(\mathbf{R_{n}}\) and \(\mathbf{O}\) are the growth parameters derived from the old, randomly initialized, and zero-initialized respectively. Except \(G_{\text{direct}}\), other three operators only illustrates the widthwise growth.

and then continual pre-training the large LLMs for \(D\) tokens. Two key factors in the procedure are worth noting: the growth factor \(g\) and the data for base model training \(d\), which can be interpreted as "growth timing". We further evaluate each growth operator by separately examining in _depthwise (intra-layer)_ growth \(G^{\uparrow}\) and _widthwise (layer-wise)_ growth \(G^{\rightarrow}\). Concretely, we start with base models (400M LLMs) trained on \(d=10B\) tokens, apply the four operators in both directions to scale them up to the target size of 1.1B (approximately a growth factor of \(g=4\)), and then continue training for an additional \(D=97.5B\) tokens. 4 Appendix B contains the LLM's architecture configuration and training details.

Footnote 4: Given growth factor \(g=4\), the sum of FLOPs for training \(d=10B\) and \(D=97.5B\) approximately equals to consumption for training large LLMs \(D=100B\), which is the FLOPs of our baseline trained from scratch.

### Pre-Training 1.1B LLMs

We report results on training loss, eight standard Harness NLP benchmarks along with the average accuracy and the speedup ratio in Figure 3. Our key discovery reveals that depthwise growth \(G^{\uparrow}\) exhibits a significant acceleration over both widthwise growth \(G^{\rightarrow}\) and training models from scratch, while surprisingly, \(G^{\rightarrow}\) does not offer any notable advantages. Among the depthwise growth operators, \(G^{\uparrow}_{\text{direct}}\), \(G^{\uparrow}_{\text{learn}}\), and \(G^{\uparrow}_{\text{zero}}\), all outperform the baseline and \(G^{\uparrow}_{\text{random}}\). The underperformance of \(G^{\uparrow}_{\text{random}}\) in our study may be attributed to its design for gradual "mini-step" growth [17], whereas our unified approach uses a single step. Most notably, **depthwise stacking \(G^{\uparrow}_{\text{direct}}\) emerges as the clear winner among growth operators, surpassing its competitors in speedup, training loss and nearly every Harness evaluation metric**. For example, compared to training models from scratch for 100B tokens, \(G^{\uparrow}_{\text{direct}}\) achieves a significant efficiency gain, increasing training speed by 49.1%. The calculation of speedup please refer to Appendix B.2. The Appendix C presents more experiments on these operators, including their loss training and evaluation figures.

## 4 Delving Deeper Into Depthwise Stacking (\(G_{\text{stack}}\))

The empirical evidence suggests that certain growth operators, most notably \(G^{\uparrow}_{\text{direct}}\), exhibit an impressive acceleration in LLM pre-training compared to the baseline approach of training models from scratch. We now turn our attention to a more in-depth examination of the \(G^{\uparrow}_{\text{direct}}\). For ease of reference, **we will henceforth denote this depthwise stacking approach as operator \(G_{\text{stack}}\)

Figure 3: We evaluate operators using training loss and Lambda [30], ARC-c [31], ARC-e [31], Logiqa [32], PIQA [33], Sciq [34], Winogrande [35] and Wikitext PPL [36] totaling eight standard NLP benchmarks. After \(8\times 10^{20}\) FLOPs of training, \(G^{\uparrow}_{\text{direct}}\) demonstrates a significant speedup.

\(\mathcal{M}=\underbrace{M\circ M\circ\cdots\circ M}_{g\times M}\), where \(M\) is a small base model trained with \(d\) tokens, \(\mathcal{M}\) is the target model and \(g\) is the growth factor.

This section addresses the two main challenges (_O2_ and _O3_) outlined in the introduction: 1) evaluating the performance of \(G_{\text{stack}}\) in scaling scenarios, i.e. larger model sizes and more training tokens; and 2) determining the hyperparameters when using \(G_{\text{stack}}\), i.e., the growth timing \(d\) and growth factor \(g\).

### Scaling \(G_{\text{stack}}\)

**Scaling Model Sizes for \(G_{\text{stack}}\).** Our scaled-up experiments involve two larger model sizes: 3B and 7B. We initially train smaller models with a layer count that is one-quarter of our target layers (growth factor \(g=4\)), utilizing 10B tokens (\(d=10B\)). Then, we train the stacked models using over 300B tokens (\(D=300B\)) for both sizes. Figures 4 and 5 show the loss, and the NLP benchmarks average accuracy evaluated using the Harness evaluator for training 3B and 7B LLMs with 300B tokens, respectively.5 The acceleration of \(G_{\text{stack}}\) is consistent across two models and all evaluation metrics. For instance, considering the 3B model, Figure 4 demonstrates that \(G_{\text{stack}}\) achieves a 54.5% speedup in pre-training, improvement of 2.1 in NLP benchmarks average accuracy compared to the baseline 3B model trained with 240B tokens.

Footnote 5: In this study, we always calculate the consumption by combining the FLOPs required for both training small models and large models. So given \(g=4\), the consumption for training small model \(d=10B\) equals to the cost for training \(D=2.5B\), so the plotted curves for \(G_{\text{stack}}\) actually starts at \(2.5B\).

When comparing the 1B, 3B, and 7B models, it is evident that the benefits of \(G_{\text{stack}}\) are not reduced as the model size increases, implying that its acceleration effect can be leveraged even with larger models. Details of the evaluation results, including evaluation with instruction tuning, can be found in Appendix D. Appendix E compares our baselines with the open-source LLMs Pythia and tinyLlama.

**Scaling Training Tokens for \(G_{\text{stack}}\).** We next evaluate the scalability of the stacking operator on another dimension - training with more tokens. This is especially important in light of recent discussions about the validity of efficient training algorithms, which have sparked debate [22] over whether certain strategies may initially learn faster but ultimately perform similarly or worse than vanilla training methods when given more training data. Hence, we aim to pre-train a LLM using \(G_{\text{stack}}\) on a substantial amount of training tokens.

Figure 4: Training 3B LLMs with 300B tokens. \(G_{\text{stack}}\) significantly outperforms scratch in (a) loss and (b) average accuracy across NLP benchmarks. At 180B and 240B tokens, \(G_{\text{stack}}\) accelerates by 48.6% and 54.5% compared to scratch.

Figure 5: Training 7B LLMs with 300B tokens. \(G_{\text{stack}}\) significantly outperforms scratch in (a) loss and (b) average accuracy across NLP benchmarks. At 160B, 220B and 280B tokens, \(G_{\text{stack}}\) accelerates by 40.8%, 55.3% and 53.8% compared to scratch.

Concretely, we conduct an experiment on a 410M LLM using 750B tokens. Following the experimental setup in the previous section, we set growth ratio \(g=4\) and growth timing \(d=10B\) and conduct continuous pre-training on the target 410M LLMs for 750B tokens. Compared to the chinchilla-recommended 8B tokens [4] for the 410M model, our experimental setting also surpasses this value by nearly 100 times, reaching 750B tokens.

The training dynamics on Figure 5(a) indicate that \(G_{\text{stack}}\) remains effective in such cases. Details of the evaluation results with the similar findings can be found in Appendix D.3. Building upon the exceptional stability of LLM pre-training [37, 38], we estimate loss improvements and plot them in Figure 5(b). The fitting curve indicates \(G_{\text{stack}}\) will continue to exhibit acceleration effects even after 8T tokens, which is over 1000 times longer than the recommended token number [4]. It is also notable that this loss improvement after 8T training is not trivial for LLM pre-training, as previous studies [39] suggest that even minor improvements in the later phase can have a relatively substantial impact on downstream performance.

From a LLM practitioner's perspective, this is also crucial considering "overtraining", which involves training LLMs with significantly larger amounts of data than recommended by scaling laws [3, 4, 5], a common practice that has become prevalent. A notable example is the training of LLama 3-8B with 15T tokens, which is nearly 100 times greater than the token count recommended by the chinchilla scaling laws [4]. Hence, this finding provides confidence in the consistent excellent acceleration of \(G_{\text{stack}}\) throughout the entire practical LLM pre-training process.

Estimating Scaling Laws.To further explore our findings, we graph our four models (410M, 1.1B, 3B, and 7B) on the same figure and attempt to uncover our "scaling law" using the \(G_{\text{stack}}\) operator. Following [3, 4], we define the scaling power law using the equation \(L_{C}=aC^{b}\), where \(a\) and \(b\) are constants we need to fit, \(C\) represents the FLOPs, and \(L_{C}\) denotes the model's final loss under this FLOP. We use the curve_flit function in SciPy [40] to fit both the scratch model and the \(G_{\text{stack}}\) model and present the estimation scaling law in Figure 7. The figure shows that our \(G_{\text{stack}}\) scaling law exhibits improved efficiency compared to the scaling law estimated from baseline LLMs, achieving the same target loss while requiring much less computational resources. However, in light of the significant computational resources devoted to other scaling law studies [3, 4], we acknowledge that our \(G_{\text{stack}}\) scaling law is an initial estimate subject to computation constraints, and a comprehensive study is left for future research.

### Determining Growth Timing and Growth Factor for Using \(G_{\text{stack}}\)

We comprehensively validate the effectiveness of the \(G_{\text{stack}}\) compared to training from scratch in Section 4.1. However, to incorporate \(G_{\text{stack}}\) into a LLM's pre-training process, we need to determine two crucial hyperparameters: the growing time (\(d\)) and the growing factor (\(g\)). In our previous experiments, we rely on ad-hoc choices for these parameters, thereby lacking a systematic approach

Figure 6: Training 410M LLMs with 750B tokens. \(G_{\text{stack}}\) significantly outperforms scratch in (a) loss. At 400B tokens, we observe a 53.1% acceleration, and even at 700B tokens, there is still a 31.0% acceleration. (b) We fit the difference between the losses of the scratch and \(G_{\text{stack}}\) and find that the acceleration with \(G_{\text{stack}}\) remain sustainable for longer training.

Figure 7: We plot scaling law lines based on 410M, 1.1B, 3B, 7B LLMs and make two predictions at the same losses of original computational-optimized 13B and 70B LLMs.

to determining them when use \(G_{\text{stack}}\). There exists research on investigating the growth timing [41], but the settings are quite different from the LLM pre-training. Therefore, this section offers a clear guide for practitioners looking to optimize using the \(G_{\text{stack}}\) operator in LLM pre-training processes.

We begin by offering a formal definition. When given a computational budget \(C\), established scaling power laws [3, 4] exist to guide the non-embedding parameters \(N\) and the number of training tokens \(D\) to achieve the lowest model loss in the case of training from scratch. However, tuning hyperparameters becomes more complex when the fixed budget \(C\) is allocated to find the optimal model training strategy using the \(G_{\text{stack}}\) operator, which involves two training phases. Consequently, the overall computational budget \(C\) can be expressed as the sum of the two components: \(C=C1+C2\). Here, \(C1\) and \(C2\) represent the flops required to train the initial small models \(C1=FLOPs(n,d)\), and the large model \(C2=FLOPs(N,D)\) respectively, where \(n\) and \(d\) denote the parameters and training tokens of the small model, and \(N\) and \(D\) represent the parameters and training tokens of the large model. Since the large model is grown by a factor of \(g\) such that \(N=gn\), we have \(C=C1+C2=FLOPs(g,N,d)+FLOPs(N,D)=FLOPs(g,N,d,D)\).

So when given a budget \(C\), our objective is to identify the optimized values \(D\), \(N\), \(d\), \(g\) that minimize the loss \(\mathcal{L}(D,N,d,g)\). However, simultaneously optimizing the above four variables can be computationally expensive. Therefore, instead of searching for global optimals, we separately determine two factors closely related to the \(G_{\text{stack}}\): the training tokens for the small model (growth timing) \(d\) and the growth factor \(g\):

\[\operatorname*{arg\,min}_{f,h}\ \mathcal{L}(D,N,d,g)\text{, \quad where }d=f(D,N),g=h(D,N)\] (1)

Determining Growth Timing: \(d\).We first explore the effect of growth timing, i.e. the training token \(d\) for the small model. Particularly, we apply the \(G_{\text{stack}}\) operator to a series of small models trained with \(d=0B,1B,5B,10B,20B,50B\) tokens. Subsequently, we stack them to the target layers with growth factor \(g=4\) and train for a fixed set of computational FLOPs. We replicate the above experiments using three target model sizes \(N=410M,1.1B,3B\) and plot each set of IsoFLOP points in Figure (a)a, (b)b and (c)c. Surprisingly, even a small model trained with just \(1B\) tokens exhibits a significant speedup compared to the directly stacking small random initialized models (represented as "0B"). While 0B's performance is similar to models trained from scratch, implying stacking itself does not serve as an effective initialization method. Furthermore, by applying smoothing techniques to model IsoFLOP curves as parabolas, we identify the optimized value of \(d\) that minimizes loss for each FLOP count, leading us to hypothesize the existence of a logarithmic equation involving \(N\), \(C\), and \(d\):

Figure 8: In 410M, 1.1B, and 3B LLMs, we plot smoothed loss curves for different growth timing \(d\) given a set of FLOPs to form IsoFLOP figures. We find a clear valley in loss, indicating that for a given FLOP budget, there exists an optimal growth timing \(d\) for the \(G_{\text{stack}}\) operation.

Figure 9: We fit a contour figure for predicting \(d\) given \(C\) and \(N\). These optimal growth timing \(d\) fit the figure well.

\[log_{10}(d)=a\log_{10}(N)+\frac{b}{\log_{10}(C)}+c\] (2)

After fitting, we obtain \(a=0.88\), \(b=163.27\) and \(c=-5.74\) and we plot the contour figure in Figure 9. It can be observed that our estimated curves align well with the actual optimal points.

#### Determining Growth Factor:

\(g\).Another factor we determine is the growth factor \(g\). As models with 3B and 7B parameters have identical depths, we run experiments using two model sizes: 1.1B (24 layers) and 3B (32 layers). Specifically, we vary the stack factors to \(g=2,4,8,24\) for the 1.1B model and \(g=4,8,16,32\) for the 3B model while keeping the base models trained with \(d=10\)B tokens. The smoothed IsoFLOP curves are plotted in Figure 10. Interestingly, even with a relatively shallow 2-layer base model and a growth factor of \(g=16\), we observe a remarkable improvement compared to the baseline 3B model (\(g=1\)). However, when using a 1-layer base model, \(G_{\text{stack}}\) underperforms compared to the baseline. Our curves indicate that the optimal growth factor \(g\) lies between 2 and 4.

However, unlike determining training token \(d\), we cannot generate sufficient data to estimate the relationship between \(N\), \(C\), and \(g\), due to computational constraints. Thus, this work suggests a constant growth factor of \(g=4\). We also include our preliminary estimated equation and contour figure for \(g\) in the Appendix F. All evaluation results of Section 4.2 are listed in Appendix G.

## 5 Ablation and Discussion

To further give insights into adopting model growth techniques in LLM pre-training, we ablate variances for \(G_{\text{stack}}\) and discuss function preserving in general model growth techniques.

### Ablation: How to Stack?

It is worth noting that \(G_{\text{stack}}\) differs from the algorithm proposed in StackedBERT [14], which utilizes a gradually stacking strategy. Hence, we compare our "one-hop" \(G_{\text{stack}}\) and their gradual stacking approach. Following the methodology introduced in StackBERT, we employ a two-step stack strategy. Given our target model size of 1.1B with 24 layers, we start with a 6-layer model. Subsequently, we train it on 10B tokens and double the model's depth through stacking, repeating this step twice (train-stack-train-stack) to achieve the desired scale. Our experiments demonstrate that \(G_{\text{stack}}\) outperforms gradual stacking approaches on loss and downstream evaluations. For example, the evaluation results show that \(G_{\text{stack}}\) achieves a 2.4 higher average accuracy and 0.6 better Wikitext PPL than gradual stacking when pre-training large models for 100B tokens. The results can be found in Appendix H.1. We further compare other stacking variations, such as stacking via interpolation and partial stacking of certain layers which are also adopted in LlamaPro [42] and Solar [43], and leave our detailed findings in the Appendix H.2 and H.3.

### Discussion: Why Does Function Preserving Fail?

Function preservation (FP) is a key concept that underlies most model growth approaches [10; 11; 12; 17]. The idea is intuitive that a larger model should initialize parameters that can represent the same

Figure 10: In 1.1B, and 3B LLMs, we plot smoothed loss curves for different growth factor \(g\) given a set of FLOPs as IsoFLOP figures. The optimal g falls between 2 and 4.

function as the ones in the smaller model, i.e. \(\forall x,f(x;\Theta^{(s)})=f(x;\Theta^{(l)}_{init})\), where \(x\) is the input. We give a mathematical definition of FP in the Appendix I.1.

We find it intriguing that our \(G_{\text{stack}}\) approach, which violates FP, emerges as the most effective in our study. To further investigate, we conduct a simple ablation study to break FP by introducing noise on the strict-FP operator \(G_{\text{direct}}^{\rightarrow}\). We initialize the new neurons by a weighted combination of two sets of parameters: those from \(G_{\text{direct}}^{\rightarrow}\) and those from random initialization. The weighting factor is controlled by a noise ratio. Our findings are intriguing. After 40B tokens training, adding 20% noise outperforms original \(G_{\text{direct}}^{\rightarrow}\) by 0.27 on the Wikitext PPL and 0.41 on the average accuracy score.

We also add noise for \(G_{\text{stack}}\). When we add 20% noise, our LLM performs slightly better than the no-noise model. However, when the noise level exceeds 20%, the performance significantly deteriorates. These results indicate that function preservation may not be the sole determining factor for model growth. **In other words, exploring ways to accelerate the training of larger models and strict preserving function during growth might represent two overlapping yet distinct research directions.** The experimental details are provided in the Appendix I.2.

## 6 Conclusion

This work empirically explores model growth approaches for efficient LLM pre-training. We address three key challenges of current model growth research for efficient LLM pre-training. We first comprehensively evaluate model growth techniques into four atomic operators and explore depthwise growth \(G_{\text{stack}}\) beats all other methods and baselines in various evaluations. We next address concerns about the scalability of \(G_{\text{stack}}\) by extending the model and training data scales. Furthermore, we systematically analyze the usage of the \(G_{\text{stack}}\) operator, focusing on growth timing and growth factor. Based on this analysis, we formalize a set of guidelines for effectively utilizing the \(G_{\text{stack}}\) operator. In addition, we provide in-depth discussions and comprehensive ablation studies of \(G_{\text{stack}}\), shedding light on the broader implications of our work.

## 7 Limitations

While our work has demonstrated remarkable potential, four limitations deserve further attention. One limitation is the constraint of computation resources. For example, we only compare two sets of growth factor \(d\) configurations, which limits the capacity to derive a formula for determining the optimal growth factor \(d\). Another limitation of our work is the focus on relatively simple operator choices, where we prioritize simplicity over exploring more sophisticated strategies. For instance, we do not extensively investigate the multi-step growth or dynamic modifications to the training process, such as adjusting the learning rate during continual pre-training. The third limitation involves the incomplete cosine learning rate schedule during training. This also arises from the resource-intensive nature of pre-training LLMs and the constraints on available computational resources. Therefore, we adopt a strategy where we initially set a large number of training tokens and then we pre-train LLMs until the training runs are interrupted by tasks with higher priority. Lastly, although this study's scope is an empirical exploration and the content is self-contained, there is a lack of theoretical insights into the success of \(G_{\text{stack}}\) in LLM pre-training.6 Nonetheless, we will release all LLM checkpoints to facilitate the community's investigation into the theoretical principles behind our observations.

Footnote 6: A very recent paper indicates training LLMs via stacking may improve in reasoning [44].

## 8 Acknowledgments

We thank all constructive comments from anonymous reviewers. Reynold Cheng and Wenyu Du were supported by the Hong Kong Jockey Club Charities Trust (Project 260920140), the University of Hong Kong (Project 109000579), the HKU Outstanding Research Student Supervisor Award 2022-23, and the HKU Faculty Exchange Award 2024 (Faculty of Engineering).

## References

* Brown et al. [2020] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners, 2020. Cited on page 1.
* Wei et al. [2022] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. Emergent abilities of large language models, 2022. Cited on page 1.
* Kaplan et al. [2020] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models, 2020. Cited on page 1.
* Hoffmann et al. [2022] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and Laurent Sifre. Training compute-optimal large language models, 2022. Cited on pages 2.
* Ibrahim Alabdulmohsin et al. [2022] Ibrahim Alabdulmohsin, Behnam Neyshabur, and Xiaohua Zhai. Revisiting neural scaling laws in language and vision, 2022. Cited on pages 1.
* Xu et al. [2024] Mengwei Xu, Wangsong Yin, Dongqi Cai, Rongjie Yi, Daling Xu, Qipeng Wang, Bingyang Wu, Yihao Zhao, Chen Yang, Shihe Wang, Qiyang Zhang, Zhenyan Lu, Li Zhang, Shangguang Wang, Yuanchun Li, Yunxin Liu, Xin Jin, and Xuanzhe Liu. A survey of resource-efficient llm and multimodal foundation models, 2024. Cited on page 1.
* AI@Meta [2024] AI@Meta [2024] A. d. d. card, 2024. Cited on pages 1.
* Carole-Jean Wu et al. [2022] Carole-Jean Wu, Ramya Raghavendra, Udit Gupta, Bilge Acun, Newsha Ardalani, Kiwan Maeng, Gloria Chang, Fiona Aga Behram, James Huang, Charles Bai, Michael Gschwind, Anurag Gupta, Myle Ott, Anastasia Melnikov, Salvatore Candido, David Brooks, Geeta Chauhan, Benjamin Lee, Hsien-Hsin S. Lee, Bugra Akyildiz, Maximilian Balandat, Joe Spisak, Ravi Jain, Mike Rabbat, and Kim Hazelwood. Sustainable ai: Environmental implications, challenges and opportunities, 2022. Cited on page 1.
* Alex de Vries [2023] Alex de Vries. The growing energy footprint of artificial intelligence. _Joule_, 7(10):2191-2194, 2023. Cited on page 1.
* Chen et al. [2015] Tianqi Chen, Ian Goodfellow, and Jonathon Shlens. Net2net: Accelerating learning via knowledge transfer. arXiv preprint arXiv:1511.05641. Cited on pages 2, 3, 4, 9, and 16.
* Chen et al. [2021] Cheng Chen, Yichun Yin, Lifeng Shang, Xin Jiang, Yujia Qin, Fengyu Wang, Zhi Wang, Xiao Chen, Zhiyuan Liu, and Qun Liu. bert2bert: Towards reusable pretrained language models. arXiv preprint arXiv:2110.07143. Cited on pages 2, 3, 4, and 16.
* Wang et al. [2023] Y. Wang, Jiahao Su, Hanlin Lu, Cong Xie, Tianyi Liu, Jianbo Yuan, Haibin Lin, Ruoyu Sun, and Hongxia Yang. Lemon: Lossless model expansion, 2023. Cited on pages 2, 3, 4, 9, and 16.
* Shen et al. [2022] Sheng Shen, Pete Walsh, Kurt Keutzer, Jesse Dodge, Matthew Peters, and Iz Beltagy. Staged training for transformer language models. In _International Conference on Machine Learning_, pages 19893-19908. PMLR, 2022. Cited on pages 2, 3, 4, and 16.
* Gong et al. [2019] Linyuan Gong, Di He, Zhuohan Li, Tao Qin, Liwei Wang, and Tieyan Liu. Efficient training of bert by progressively stacking. In _International conference on machine learning_, pages 2337-2346. PMLR, 2019. Cited on pages 2, 3, 4, 9, and 16.

* Wang et al. [2023] Peihao Wang, Rameswar Panda, Lucas Torroba Hennigen, Philip Greengard, Leonid Karlinsky, Rogerio Feris, David Daniel Cox, Zhangyang Wang, and Yoon Kim. Learning to grow pretrained models for efficient transformer training. _arXiv preprint arXiv:2303.00980_, 2023. Cited on pages 2, 3, 4, and 16.
* Evci et al. [2022] Utku Evci, Bart van Merrienboer, Thomas Unterthiner, Max Vladymyrov, and Fabian Pedregosa. Gradmax: Growing neural networks using gradient information. _arXiv preprint arXiv:2201.05125_, 2022. Cited on pages 2, 3, 4, and 16.
* Yao et al. [2024] Yiqun Yao, Zheng Zhang, Jing Li, and Yequan Wang. Masked structural growth for 2x faster language model pre-training, 2024. Cited on pages 2, 3, 4, 5, 9, and 16.
* Yang et al. [2020] Cheng Yang, Shengnan Wang, Chao Yang, Yuechuan Li, Ru He, and Jingqiao Zhang. Progressively stacking 2.0: A multi-stage layerwise training method for bert training speedup. _arXiv preprint arXiv:2011.13635_, 2020. Cited on pages 2, 3, 4, and 16.
* Jiang et al. [2023] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lelio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothee Lacroix, and William El Sayed. Mistral 7b, 2023. Cited on page 2.
* Li et al. [2023] Xiang Li, Yiqun Yao, Xin Jiang, Xuezhi Fang, Xuying Meng, Siqi Fan, Peng Han, Jing Li, Li Du, Bowen Qin, Zheng Zhang, Aixin Sun, and Yequan Wang. Flm-101b: An open llm and how to train it with $100k budget, 2023. Cited on pages 2, 3, and 4.
* Touvron et al. [2021] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models, 2023. Cited on pages 2, 3, and 30.
* Kaddour et al. [2023] Jean Kaddour, Oscar Key, Piotr Nawrot, Pasquale Minervini, and Matt J. Kusner. No train no gain: Revisiting efficient training algorithms for transformer-based language models, 2023. Cited on pages 2, 3, 4, and 16.
* Gao et al. [2023] Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noac'h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework for few-shot language model evaluation, 12 2023. Cited on pages 2, 3, 4.
* Fahlman and Lebiere [1989] Scott Fahlman and Christian Lebiere. The cascade-correlation learning architecture. In D. Touretzky, editor, _Advances in Neural Information Processing Systems_, volume 2. Morgan-Kaufmann, 1989. Cited on page 3.
* Fahlman [1990] Scott E. Fahlman. The recurrent cascade-correlation architecture. In _Neural Information Processing Systems_, 1990. No citations.
* Gutstein et al. [2007] Steven Gutstein, Olac Fuentes, and Eric A. Freudenthal. Knowledge transfer in deep convolutional neural nets. In _Int. J. Artif. Intell. Tools_, 2007. Cited on page 3.
* Devlin et al. [2019] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding, 2019. Cited on page 3.

* Wu et al. [2021] Lemeng Wu, Bo Liu, Peter Stone, and Qiang Liu. Firefly neural architecture descent: a general approach for growing neural networks, 2021. Cited on page 4.
* Yuan et al. [2023] Xin Yuan, Pedro Savarese, and Michael Maire. Accelerated training via incrementally growing neural networks using variance transfer and learning rate adaptation, 2023. Cited on page 4.
* Paperno et al. [2016] Denis Paperno, German Kruszewski, Angeliki Lazaridou, Quan Ngoc Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fernandez. The lambda dataset: Word prediction requiring a broad discourse context, 2016. Cited on page 5.
* Clark et al. [2018] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge, 2018. Cited on page 5.
* Liu et al. [2020] Jian Liu, Leyang Cui, Hanmeng Liu, Dandan Huang, Yile Wang, and Yue Zhang. Logiqa: A challenge dataset for machine reading comprehension with logical reasoning, 2020. Cited on page 5.
* Bisk et al. [2019] Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. Piqa: Reasoning about physical commonsense in natural language, 2019. Cited on page 5.
* Welbl et al. [2017] Johannes Welbl, Nelson F. Liu, and Matt Gardner. Crowdsourcing multiple choice science questions, 2017. Cited on page 5.
* Sekaguchi et al. [2019] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale, 2019. Cited on page 5.
* Merity et al. [2016] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models, 2016. Cited on page 5.
* Zhao et al. [2023] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. A survey of large language models, 2023. Cited on page 7.
* Jiang et al. [2024] Ziheng Jiang, Haibin Lin, Yinmin Zhong, Qi Huang, Yangrui Chen, Zhi Zhang, Yanghua Peng, Xiang Li, Cong Xie, Shibiao Ning, Yulu Jia, Sun He, Hongmin Chen, Zhihao Bai, Qi Hou, Shipeng Yan, Ding Zhou, Yiyao Sheng, Zhuo Jiang, Haohan Xu, Haoran Wei, Zhang Zhang, Pengfei Nie, Leqi Zou, Sida Zhao, Liang Xiang, Zherui Liu, Zhe Li, Xiaoying Jia, Jianxi Ye, Xin Jin, and Xin Liu. Megascale: Scaling large language model training to more than 10,000 gpus, 2024. Cited on page 7.
* Du et al. [2024] Zhengxiao Du, Aohan Zeng, Yuxiao Dong, and Jie Tang. Understanding emergent abilities of language models from the loss perspective, 2024. Cited on page 7.
* Virtanen et al. [2021] Pauli Virtanen, Ralf Gommers, Travis E. Oliphant, Matt Haberland, Tyler Reddy, David Cournapeau, Evgeni Burovski, Pearu Peterson, Warren Weckesser, Jonathan Bright, Stefan J. van der Walt, Matthew Brett, Joshua Wilson, K. Jarrod Millman, Nikolay Mayorov, Andrew R. J. Nelson, Eric Jones, Robert Kern, Eric Larson, C J Carey, Ilhan Polat, Yu Feng, Eric W. Moore, Jake VanderPlas, Denis Laxalde, Josef Perktold, Robert Cimrman, Ian Henriksen, E. A. Quintero, Charles R. Harris, Anne M. Archibald, Antonio H. Ribeiro, Fabian Pedregosa, Paul van Mulbregt, Aditya Vijaykumar, Alessandro Pietro Bardelli, Alex Rothberg, Andreas Hilbolt, Andreas Kloeckner, Anthony Scopatz, Antony Lee, Ariel Rokem, C. Nathan Woods, Chad Fulton, Charles Masson, Christian Haggstrom, Clark Fitzgerald, David A. Nicholson, David R. Hagen, Dmitrii V. Pasechnik, Emanuele Olivetti, Eric M Martin, Eric Wieser, Fabrice Silva, Felix Lenders, Florian Wilhelm, G. Young, Gavin A. Price, Gert-Ludwig Ingold, Gregory E. Allen, Gregory R. Lee, Herve Audren, Irvin Probst, Jorg P. Dietrich, Jacob Silterra, James T Webber, Janko Slavic, Joel Nothman, Johannes Buchner, Johannes Kulick, Johannes L. Schonberger, Jose Vinicius de Miranda Cardoso, Joscha Reimer, Joseph Harrington, Juan Luis Cano Rodriguez, Juan Nunez-Iglesias, Justin Kuczynski, Kevin Tritz, Martin Thoma, Matthew Newville, Matthias Kummerer, Maximilian Bolingbroke, Michael Tartre, Mikhail Pak, Nathaniel J. Smith, Nikolai Nowaczyk, Nikolay Shebanov, Oleksandr Pavlyk, Per A. Brodtkorb, Perry Lee, Robert T.

McGibbon, Roman Feldbauer, Sam Lewis, Sam Tygier, Scott Sievert, Sebastiano Vigna, Stefan Peterson, Surhud More, Tadeusz Pudlik, Takuya Oshima, Thomas J. Pingel, Thomas P. Robitaille, Thomas Spura, Thouis R. Jones, Tim Cera, Tim Leslie, Tiziano Zito, Tom Krauss, Utkarsh Upadhyay, Yaroslav O. Halchenko, and Yoshiki Vazquez-Baeza. Scipy 1.0: fundamental algorithms for scientific computing in python. _Nature Methods_, 17(3):261-272, February 2020. Cited on page 1.
* [41]H. Wu, W. Wang, T. Malepathirana, D. Senanayake, D. Oetomo, and S. Halgamuge (2024) When to grow? a fitting risk-aware policy for layer growing in deep neural networks. Cited on page 1.
* [42]C. Wu, Y. Gan, Y. Ge, Z. Lu, J. Wang, Y. Feng, P. Luo, and Y. Shan (2024) Llama pro: progressive llama with block expansion. arXiv preprint arXiv:2401.02415. Cited on pages 9 and 37. Cited on page 1.
* [43]D. Kim, C. Park, S. Kim, W. Lee, W. Song, Y. Kim, H. Kim, Y. Kim, H. Lee, J. Kim, et al. (2023) Solar 10.7 b: scaling large language models with simple yet effective depth up-scaling. arXiv preprint arXiv:2312.15166. Cited on pages 9 and 37. Cited on page 1.
* [44]N. Saunshi, S. Karp, S. Krishnan, S. Miryoosefi, S. J. Reddi, and S. Kumar (2024) On the inductive bias of stacking towards improving reasoning. Cited on page 1.
* [45]P. Zhang, G. Zeng, T. Wang, and W. Lu (2024) Tinyllama: an open-source small language model. Cited on page 1.
* [46]T. Dao, D. Y. Fu, S. Ermon, A. Rudra, and C. Re (2022) Flashattention: fast and memory-efficient exact attention with io-awareness. Cited on page 1.
* [47]D. Soboleva, F. Al-Khateeb, R. Myers, J. R. Steeves, J. Hestness, and N. Dey (2023) SlimPajama: a 627B token cleaned and deduplicated version of RedPajama. Note: https://www.ceebras.net/blog/slimpajama-a-627b-token-cleaned-and-deduplicated-version-of-redpajama Cited on page 1.
* [48]R. Zellers, A. Holtzman, Y. Bisk, A. Farhadi, and Y. Choi (2019) Hellaswag: can a machine really finish your sentence?. Cited on page 1.
* [49]S. Lin, J. Hilton, and O. Evans (2022) Truthfulqa: measuring how models mimic human falsehoods. Cited on page 1.
* [50]D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt (2021) Measuring massive multitask language understanding. Cited on page 1.
* [51]S. Biderman, H. Schoelkopf, Q. A. Anthony, H. Bradley, K. O'Brien, E. Hallahan, M. Khan, S. Purohit, U. S. P. Raff, et al. (2023) Pythia: a suite for analyzing large language models across training and scaling. In International Conference on Machine Learning, pp. 2397-2430. Cited on page 1.
* [52]L. Gao, S. Biderman, S. Black, L. Golding, T. Hoppe, C. Foster, J. Phang, H. He, A. Thite, N. Nabeshima, et al. (2020) The pile: an 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027. Cited on page 1.
* [53]D. Groeneveld, I. Beltagy, P. Walsh, A. Bhagia, R. Kinney, O. Tafjord, A. Jha, H. Ivison, I. Magnusson, Y. Wang, S. Arora, D. Atkinson, R. Authur, K. Chandu, A. Cohan, J. Dumas, Y. Elazar, Y. Gu, J. Hessel, T. Khot, W. Merrill, J. Morrison, N. Muennighoff, A. Naik, C. Nam, M. E. Peters, V. Pyatkin, A. Ravichander, D. Schwenk, S. Shah, W. Smith, N. Subramani, M. Wortsman, P. Dasigi, N. Lambert, K. Richardson, J. Dodge, K. Lo, L. Soldaini, N. A.

Smith, and Hannaneh Hajishirzi. Olmo: Accelerating the science of language models. _Preprint_, 2024. Cited on page 39.
* [54]Zhengzhong Liu, Aurick Qiao, Willie Neiswanger, Hongyi Wang, Bowen Tan, Tianhua Tao, Junbo Li, Yuqi Wang, Suqi Sun, Omkar Pangarkar, Richard Fan, Yi Gu, Victor Miller, Yonghao Zhuang, Guowei He, Haonan Li, Fajri Koto, Liping Tang, Nikhil Ranjan, Zhiqiang Shen, Xuguang Ren, Roberto Iriondo, Cun Mu, Zhiting Hu, Mark Schulze, Preslav Nakov, Tim Baldwin, and Eric P. Xing. Llm360: Towards fully transparent open-source llms, 2023. Cited on page 39.
* [55]Luca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson, Russell Authur, Ben Bogin, Khyathi Chandu, Jennifer Dumas, Yanai Elazar, Valentin Hofmann, Ananya Harsh Jha, Sachin Kumar, Li Lucy, Xinxi Lyu, Nathan Lambert, Ian Magnusson, Jacob Morrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew E. Peters, Abhilasha Ravichander, Kyle Richardson, Zejiang Shen, Emma Strubell, Nishant Subramani, Oyvind Tafjord, Pete Walsh, Luke Zettlemoyer, Noah A. Smith, Hannaneh Hajishirzi, Iz Beltagy, Dirk Groeneveld, Jesse Dodge, and Kyle Lo. Dolma: an Open Corpus of Three Trillion Tokens for Language Model Pretraining Research. _arXiv preprint_, 2024. Cited on page 39.

Details of Growth Operators

### Four Growth Operators

#### a.1.1 Operator \(G_{\text{direct}}\): Direct Derivation of Grown Parameters From Old Parameters

One intuitive strategy for expanding neural networks involves directly duplicating or splitting existing neurons. [14; 11; 12]. Unlike other growth operators, we distinguish between growth in terms of depth and width.

For width-wise expansion, the Net2Net technique and its transformer implementations [10; 11] involve splitting old neurons into two or more parts, with each splitting step achieving a=b+c. Depending on the specific splitting mechanism, there are two variations: even splitting and uneven splitting. The latter is proposed to address symmetry issues that arise when neurons are evenly split. In this paper, we adopt the approach of uneven splitting.

In the context of depth-wise expansion, a common practice is to duplicate layers, often referred to as "stacking" [14]. Therefore, we use the term \(G_{\text{stack}}\) to represent this operator. While this approach may appear to deviate from function preservation, it surprisingly yields a strong baseline.

#### a.1.2 Operator \(G_{\text{learn}}\): Generation of New Parameters through Matrix Transformation

\(G_{\text{learn}}\) is an operator that learns a matrix transformation function to map small models to a larger one [15]. This operator is applicable to both width and depth expansion. Considering the original model \(f\) with parameters \(\theta\), the target model \(F\) with parameters \(\Theta\), and \(G_{\text{learn}}\) as the hypernetwork for meta-learning, the training corpus is denoted as \(\mathcal{D}\), and the language model loss is denoted as \(\mathcal{L}\). Then, we optimize the following objective:

\[\begin{split}\underset{G_{\text{learn}}}{arg\ min}\ \mathbb{E}_{x\sim\mathcal{D}}\ \mathcal{L}(x;F_{\Theta}),&\text{where}\ \Theta=G_{\text{learn}}(\theta)\end{split}\] (3)

#### a.1.3 Operator \(G_{\text{zero}}\): Setting New Parameters to 0

Setting new parameters to zero is often considered a simple method to achieve function preservation. However, optimizing networks with a significant number of zeros can present challenges. To tackle this issue, we adopt current practices that selectively zero out either the fan-in or fan-out parameters [13; 16; 12]. Specifically, for operator \(G_{\text{zero}}\), during width growing, we zero out only the set of fan-out parameters for new neurons and randomly initialize the remaining ones. In the case of depthwise expansion, we zero out the final output layer of the newly-duplicated transformer blocks' MultiHead Attention and MLP.

#### a.1.4 Operator \(G_{\text{random}}\): Random Initialization of New Parameters

This group follows the common practice of randomly initializing new parameters. In earlier attempts, old neurons were frozen after the growth process [18; 17]. However, to ensure function preservation, a recent study introduces a mask for new neurons after expansion [17]. This mask is gradually removed during ongoing training. We refer to this new approach as the growth operator \(G_{\text{random}}\).

### Difference of Our Operators and Base Methods

The operators \(G_{\text{direct}}^{\rightarrow}\) shares a similar setting to Lemon with minor variances due to Llama achitectures. \(G_{\text{learn}}\) is consistent with the methods LiGO, but with our own implementation. For \(G_{\text{zero}}\), our approach aligns with Lemon in terms of depth, but differs from stagedTraining in width. Unlike stagedTraining, we do not double the width and assign zeros to the off-diagonal entries. Instead, our approach is more flexible; by zeroing out the submatrix in the bottom-left corner, we can extend it to any dimension. Our \(G_{\text{random}}\) does not exhibit the "multi-hop" growth like MSG, instead, it grows "one-hop" directly to the target size. Our implementation of \(G_{\text{direct}}^{\uparrow}\) (\(G_{\text{stack}}\)) differs from the algorithm employed in stackedBert. In stackedBert, a gradual growing technique is utilized, whereas our operator follows a more direct approach.

### Details of \(G_{\text{direct}}\)

EmbeddingConsider \(E\in\mathbb{R}^{V\times d}\), and our goal is to expand it to \(E^{\prime}\in\mathbb{R}^{V\times D}\), \(G_{\text{direct}}\) just copy some columns:

\[E^{\prime} = G_{\text{direct}}(E)\] (4) \[= ER\] (5) \[= E\left[\underbrace{I}_{d}I\right]\] (6)

where \(R\in\mathbb{R}^{d\times D}\) is used to copy the embedding matrix \(E\).

LinearConsider \(W\in\mathbb{R}^{d_{out}\times d_{in}}\), target parameter \(W^{\prime}\in\mathbb{R}^{D_{out}\times D_{in}}\), where \(d_{out}\leq D_{out},d_{in}\leq D_{in}\), \(G_{\text{direct}}\) is defined as:

\[W^{\prime} = G_{\text{direct}}(W)\] (7) \[= LWR\] (8) \[= d_{out}\left\{\begin{array}{l}\left[\begin{matrix}I\\ I\end{matrix}\end{matrix}\right]W\left[\underbrace{\alpha}_{d_{in}}\begin{matrix} \beta\\ \end{matrix}\right]\right.\] (9)

where \(R\in\mathbb{R}^{d_{in}\times D_{in}}\) is used for expanding the fan-in and \(L\in\mathbb{R}^{D_{out}\times d_{out}}\) is used for expanding the fan-out. To satisfy function preserving, we ensure that \(\alpha+\beta=I\).

RMSNormFor RMSNorm, a similar approach is adopted, consider parameter \(\mu\in\mathbb{R}^{d}\), expanded parameter \(\mu^{\prime}=\frac{\sqrt{d}}{\sqrt{D}}[\mu,\;\mu_{0,D-d}]\in\mathbb{R}^{D}\):

\[RMSNorm^{\prime}(x^{\prime})=\frac{x^{\prime}}{\sqrt{\frac{1}{D} \sum_{i=1}^{D}{x^{\prime}}_{i}^{2}}}\odot\mu^{\prime}\] (10) \[=[\sqrt{\frac{\sum_{i=1}^{d}{x_{i}^{2}}}{\sum_{i=1}^{D}{x^{\prime }}_{i}^{2}}}\times RMSNorm(x),\;\zeta]\] (11)

Therefore, using the \(G_{\text{direct}}\), it is not possible to achieve function preservation for RMSNorm

Depth (\(G_{\text{stack}}\))Consider a transformer with \(l\) layers represented as \(F=f_{0}\circ f_{1}\circ\cdots\circ f_{l}\). Our objective is to expand it to \(L\) layers, where \(L\) is a multiple of \(l\). We have various stacking forms for this purpose, such as (a) direct stacking: \(F^{\prime}=F\circ F\circ\cdots\circ F\).

``` Input: Base model \(M_{k}^{l}\) with \(l\) layers trained using dataset \(d_{k}\) where \(k\) is iteration steps. Growth factor \(g\). Output: Target Model \(\mathcal{M}_{0}^{gl}\) with \(gl\) layers \(\mathcal{M}_{0}^{l}\)=\(M_{k}^{l}\) for\(t=2\) to \(g\)do\(\triangleright\) Model Stacking \(\mathcal{M}_{0}^{tl}=\mathcal{M}_{0}^{(t-1)l}\circ M_{k}^{l}\) end ```

**Algorithm 1** Operator \(G_{\text{stack}}\)

### Details of \(G_{\text{zero}}\)

EmbeddingConsider an embedding matrix \(E\in\mathbb{R}^{V\times d}\). The \(G_{\text{zero}}\) operator expands it to \(E^{\prime}\in\mathbb{R}^{V\times D}\) with \(O\), where \(d\leq D\). Formally:

\[E^{\prime}=[E,\;O]\] (12)

Therefore, give a token \(x\), the expanded embedding can be expressed as:

\[Embedding^{\prime}(x)=\mathbbm{1}_{x}E^{\prime}=[Embedding(x),\;0_{D-d}]\] (13)

LinearConsider parameter \(W\in\mathbb{R}^{d_{out}\times d_{in}}\). \(G_{\text{zero}}\) expand it to \(W^{\prime}\in\mathbb{R}^{D_{out}\times D_{in}}\), where \(d_{out}\leq D_{out}\) and \(d_{in}\leq D_{in}\). Formally:

\[W^{\prime}=\begin{bmatrix}W&\mathcal{A}\\ O&\mathcal{C}\end{bmatrix}\] (14)

where \(\mathcal{A},\mathcal{C}\) are randomly initialized new parameters. Considering the input token \(x\in\mathbb{R}^{d_{in}}\) before expansion, and the input after expansion \(x^{\prime}\in\mathbb{R}^{D_{in}}\):

\[x^{\prime}=[x,\;0_{D_{in}-d_{in}}]\] (15)

\[Linear^{\prime}(x^{\prime}) = x^{\prime}W^{\prime T}\] (16) \[= [x,\;0_{D_{in}-d_{in}}]\begin{bmatrix}W^{T}&O\\ \mathcal{A}^{T}&\mathcal{C}^{T}\end{bmatrix}\] (17) \[= [xW^{T},\;0_{D_{out}-d_{out}}]\] (18) \[= [Linear(x),\;0_{D_{out}-d_{out}}]\] (19)

RMSNormConsidering the parameter \(\mu\in\mathbb{R}^{d}\), \(G_{\text{zero}}\) expand it to \(\mu^{\prime}=[\alpha\mu,\;\xi]\) like \(G_{\text{random}}\) in Appendix A.5, because the input must be \(x^{\prime}=[x,\;0_{D-d}]\in\mathbb{R}^{D}\).

DepthIn depth, by retaining only the residual part and initializing the MHA and SwiGLU final linear projections to zero, the MHA and SwiGLU layers can achieve function preservation.

### Details of \(G_{\text{random}}\)

EmbeddingConsider an embedding matrix \(E\in\mathbb{R}^{V\times d}\). The goal of \(G_{\text{random}}\) is to expand it to \(E^{\prime}\in\mathbb{R}^{V\times D}\), where \(d\leq D\). Formally:

\[E^{\prime}=[E,\;\mathcal{E}]\] (20)

where \(\mathcal{E}\in\mathbb{R}^{V\times(D-d)}\) represents randomly initialized new parameters. We use a mask \(c\in\mathbb{R}^{D}\) to mask out the randomly initialized parts:

\[c=[1_{d},\;0_{D-d}]\rightarrow[1_{d},\;1_{D-d}]\] (21)

Therefore, for a token \(x\), the masked embedding can be expressed as:

\[Embedding^{\prime}(x)=\mathbbm{1}_{x}E^{\prime}\odot c=[Embedding(x),\;0_{D-d}]\] (22)LinearConsider parameter \(W\in\mathbb{R}^{d_{out}\times d_{in}}\). Our goal is to expand it to \(W^{\prime}\in\mathbb{R}^{D_{out}\times D_{in}}\), where \(d_{out}\leq D_{out}\) and \(d_{in}\leq D_{in}\). Formally:

\[W^{\prime}=\begin{bmatrix}W&\mathcal{A}\\ \mathcal{B}&\mathcal{C}\end{bmatrix}\] (23)

where \(\mathcal{A},\mathcal{B},\mathcal{C}\) are randomly initialized new parameters. Considering the input token \(x\in\mathbb{R}^{d_{in}}\) before expansion, and the input after expansion \(x^{\prime}\in\mathbb{R}^{D_{in}}\):

\[x^{\prime}=[x,\;0_{D_{in}-d_{in}}]\] (24)

\[x^{\prime}W^{\prime T} = [x,\;0_{D_{in}-d_{in}}]\begin{bmatrix}W^{T}&\mathcal{B}^{T}\\ \mathcal{A}^{T}&\mathcal{C}^{T}\end{bmatrix}\] (25) \[= [xW^{T},\;x\mathcal{B}^{T}]\] (26)

To ensure that the expanded part of \(x^{\prime}\) starts with zeros, we still utilize a mask:

\[c=[1_{d_{out}},\;0_{D_{out}-d_{out}}]\rightarrow[1_{d_{out}},\;1_{D_{out}-d_{ out}}]\] (27)

\[Linear^{\prime}(x^{\prime})=x^{\prime}W^{\prime T}\odot c=[Linear(x),\;0_{D_ {out}-d_{out}}]\] (28)

RMSNormConsidering the parameter \(\mu\in\mathbb{R}^{d}\), our objective is to expand it to \(\mu^{\prime}=[\alpha\mu,\;\xi]\in\mathbb{R}^{D}\), where \(\alpha\) is an undetermined coefficient and \(\xi\) is a randomly initialized new parameter. Let the input be \(x^{\prime}=[x,\;0_{D-d}]\in\mathbb{R}^{D}\), then we have:

\[\sum_{i=0}^{D}x^{\prime 2}=\sum_{i=0}^{d}x^{2}\] (29)

\[RMSNorm^{\prime}(x^{\prime}) = \frac{x^{\prime}}{\sqrt{\frac{1}{D}\sum_{i=0}^{D}{x^{\prime}}_{i} }^{2}}\odot\mu^{\prime}\] (30) \[= \frac{[x,\;0_{D-d}]}{\sqrt{\frac{1}{D}\sum_{i=0}^{d}{x_{i}}^{2}} \odot[\alpha\mu,\;\xi]}\] (31) \[= \left[\frac{\sqrt{D}}{\sqrt{d}}\frac{x}{\sqrt{\frac{1}{d}\sum_{i =0}^{d}{x_{i}}^{2}}}\odot\alpha\mu,\;0_{D-d}\right]\] (32)

By observing equation 32, we can conclude that, to achieve function preservation, \(\alpha=\frac{\sqrt{d}}{\sqrt{D}}\). Finally, we can conclude:

\[RMSNorm^{\prime}(x^{\prime})=[RMSNorm(x),\;0_{D-d}]\] (33)

DepthIn depth, preserving only the residual part and masking the MHA and SwiGLU layers can achieve function preservation:

\[Y=X+MHA(RMSNorm(X))\odot c\] (34) \[Y=X+SwiGLU(RMSNorm(X))\odot c\] (35) \[c=0_{D}\to 1_{D}\] (36)

### Details of \(G_{\text{learn}}\)

Using \(G_{\text{learn}}\) for width expansion, for the embedding layer \(E\in\mathbb{R}^{V\times d}\), the parameter \(B_{emb}\in\mathbb{R}^{D\times d}\) is defined as follows:

\[E^{\prime}=EB_{emb}^{T}\] (37)

For Attention layer, where \(W_{Q},W_{K},W_{V},\) and \(W_{O}\in\mathbb{R}^{d\times d}\), and RMSNorm \(\mu_{1}\in\mathbb{R}^{d}\), the parameters \(B_{Q},B_{K},\) and \(B_{V}\in\mathbb{R}^{D\times d}\), we have:

\[\begin{cases}W_{Q}^{\prime}&=&B_{Q}W_{Q}B_{emb}^{T}\\ W_{K}^{\prime}&=&B_{K}W_{K}B_{emb}^{T}\\ W_{V}^{\prime}&=&B_{V}W_{V}B_{emb}^{T}\\ W_{O}^{\prime}&=&B_{emb}W_{O}B_{V}^{T}\\ \mu_{1}^{\prime}&=&B_{emb}\mu_{1}\end{cases}\] (38)

For MLP, where \(W_{up},W_{gate}\in\mathbb{R}^{d_{mlp}\times d}\), \(W_{down}\in\mathbb{R}^{d\times d_{mlp}}\), RMSNorm \(\mu_{2}\in\mathbb{R}^{d}\), the parameter \(B_{mlp}\in\mathbb{R}^{D_{mlp}\times d_{mlp}}\), we have:

\[\begin{cases}W_{up}^{\prime}&=&B_{mlp}W_{up}B_{emb}^{T}\\ W_{down}^{\prime}&=&B_{emb}W_{mlp}B_{mlp}^{T}\\ W_{gate}^{\prime}&=&B_{mlp}W_{gate}B_{emb}^{T}\\ \mu_{2}^{\prime}&=&B_{emb}\mu_{2}\end{cases}\] (39)

For the output head \(W_{head}\in\mathbb{R}^{V\times d}\), we have:

\[W_{head}^{\prime}=W_{head}B_{emb}\] (40)

Using \(G_{\text{learn}}\) for depth expansion, consider a transformer model with \(L_{1}\) layers, we use \(G_{\text{learn}}\) to expand it to \(L_{2}\) layers. For \(l\in\{1,2,\cdots,L_{2}\}\):

\[\begin{cases}\begin{array}{ccc}W_{l}^{Q^{\prime}}{}^{\prime}&=&\sum_{l=1}^{L _{1}}D_{l,j}^{Q}W_{j}^{Q}\\ W_{l}^{K^{\prime}}&=&\sum_{l=1}^{L_{1}}D_{l,j}^{Q}W_{j}^{Q}\\ W_{l}^{V^{\prime}}&=&\sum_{l=1}^{L_{1}}D_{l,j}^{Q}W_{j}^{V}\\ W_{l}^{O^{\prime}}&=&\sum_{j=1}^{L_{1}}D_{l,j}^{Q}W_{j}^{O}\\ \mu_{l}^{(ln1)^{\prime}}&=&\sum_{j=1}^{L_{1}}D_{l,j}^{(ln1)}\mu_{j}^{(ln1)} \end{array}\end{cases}\] (41)

where \(D^{Q,K,V,O,ln1}\in\mathbb{R}^{L_{2}\times L_{1}}\) represents learnable parameters. These parameters are used to expand the MHA vertically in depth. Similarly, for SwiGLU, we also perform expansion using a similar method. Formally, this can be written as:

\[\begin{cases}\begin{array}{ccc}W_{l}^{up^{\prime}}&=&\sum_{j=1}^{L_{1}}D_{l,j}^{up}W_{j}^{up}\\ W_{l}^{down^{\prime}}{}^{\prime}&=&\sum_{j=1}^{L_{1}}D_{l,j}^{down}W_{j}^{down} \\ W_{l}^{gate^{\prime}}{}^{\prime}&=&\sum_{j=1}^{L_{1}}D_{l,j}^{gate}W_{j}^{gate} \\ \mu_{l}^{(ln2)^{\prime}}&=&\sum_{j=1}^{L_{1}}D_{l,j}^{(ln2)}\mu_{j}^{(ln2)} \end{array}\end{cases}\] (42)

where \(D^{up,down,gate,ln2}\in\mathbb{R}^{L_{2}\times L_{1}}\) represents learnable parameters used for expanding SwiGLU in the depth.

LMs Framework and Training Details

EmbeddingConsider a vocabulary size \(V\) and embedding size \(d\). Then, the embedding matrix \(E\in\mathbb{R}^{V\times d}\), and the one-hot vector for input tokens \(X\) is denoted as \(\mathbbm{1}_{X}\in\mathbb{R}^{T\times V}\), where \(T\) is the sequence length. Formally, it can be written as:

\[Embedding(X)=\mathbbm{1}_{X}E\] (43)

for \(i,v\in[V]\), where \(i\neq j\), it is guaranteed that \(E_{i}\neq E_{j}\).

Multi-Head AttentionMulti-Head Attention (MHA) consists of multiple attention heads, each of which computes its own self-attention. The results of these attention heads are then concatenated and projected to obtain the following output:

\[\begin{split} Q_{i},K_{i},V_{i}=XW_{i}^{Q},XW_{i}^{K},XW_{i}^{V} \\ H_{i}=softmax(\frac{Q_{i}K_{i}^{d}}{\sqrt{d_{h}}})V_{i}\\ MHA(X)=Concat(H_{1},\cdots,H_{n})W^{O}\end{split}\] (44)

here, the input \(X\in\mathbb{R}^{T\times d}\), parameters \(W_{i}^{Q}\in\mathbb{R}^{d\times d_{h}}\), \(W_{i}^{K}\in\mathbb{R}^{d\times d_{h}}\), \(W_{i}^{V}\in\mathbb{R}^{d\times d_{h}}\), and \(W^{O}\in\mathbb{R}^{d\times d}\), where \(n\times d_{h}=d\).

Feed Forward NetworkThe Feed Forward Network (FFN) consists of two linear layers and the activation function GeLU. Typically, the two linear layers first perform an up-projection to \(d_{FFN}\) and then down-project back to the dimension \(d\). Therefore, FFN is defined as:

\[FFN(X)=GeLU(XW_{up})W_{down}\] (45)

where the input \(X\in\mathbb{R}^{T\times d}\), parameter \(W_{up}\in\mathbb{R}^{d\times d_{FFN}}\) and \(W_{down}\in\mathbb{R}^{d_{FFN}\times d}\).

SwiGLULLaMA replaces the original FFN in the Transformer Decoder with SwiGLU, resulting in improved performance. SwiGLU consists of three linear layers and the swiglu activation function. It can be defined as:

\[SwiGLU(X)=(XW_{gate}\odot swiglu(XW_{up}))W_{down}\] (46)

where \(\odot\) means the element-wise multiplication, the input \(X\in\mathbb{R}^{T\times d}\), parameter \(W_{up}\in\mathbb{R}^{d\times d_{FFN}}\), \(W_{gate}\in\mathbb{R}^{d\times d_{FFN}}\) and \(W_{down}\in\mathbb{R}^{d_{FFN}\times d}\).

RMSNormBefore MHA, FFN, or SwiGLU, there is a layer of RMSNorm to enhance the stability of the model. Compared to LayerNorm, RMSNorm is simpler in form. Formally, it can be written as:

\[RMSNorm(X)=\frac{X}{\sqrt{\frac{1}{d}\sum_{i=1}^{d}X_{i}^{2}}}\odot\mu\] (47)

where \(X\in\mathbb{R}^{T\times d}\), parameter \(\mu\in\mathbb{R}^{d}\).

[MISSING_PAGE_FAIL:22]

Figure 11: Training Loss on Slimpajama.

Figure 12: Evaluation results on growth in depth from small model (10B) by four operators.

Figure 13: Evaluation results on growth in depth from small model (50B) by four operators.

Figure 14: Evaluation results on growth in width from small model (10B) by four operators.

Figure 15: Evaluation results on growth in width from small model (50B) by four operators.

## Appendix D Evaluation Results of Scaling \(G_{\text{stack}}\)

### 3b

Figure 16: Average accuracy of seven standard NLP benchmarks.

Figure 17: Average accuracy of standard NLP benchmarks at 3B size.

### 7b

Figure 19: Evaluation results on scratch model and \(G_{\text{stack}}\) model at 7B size.

Figure 18: Evaluation results on scratch model and \(G_{\text{stack}}\) model at 3B size.

[MISSING_PAGE_EMPTY:28]

Compare with Other Opensource LLMs

In Table 3, we compare the harness evaluation results after training the \(G_{\text{stack}}\) model and the scratch model (Baseline) for 100B tokens with Pythia-1B [51] and TinyLlama-1.1B, which are trained on the same number of tokens. The comparative results indicate that our baseline performs normally, comparable to pythia-1B. Meanwhile, the \(G_{\text{stack}}\) model significantly outperforms both the baseline and pythia-1B, demonstrating the acceleration effect of \(G_{\text{stack}}\) on the pre-training process.

## Appendix F Fitting Results for the Growth Factor \(g\)

Although due to computational resource limitations, we only explore predicting \(g\) given \(N\) and \(C\) on the 1.1B and 3B models, we still attempted to fit using equation:

\[\log_{10}(g)=a\log_{10}(N)+\frac{b}{\log_{10}(C)}+c\] (49)

In the equation 49, \(N\) represents the number of target parameters, \(g\) represents the growth factor. The fitting result is as follows:

\[\log_{10}(g)=1.01\log_{10}(N)-\frac{29.88}{\log_{10}(C)}-7.36\] (50)

We also visualize the fitted curves in Figure 22, but the results were mediocre due to the lack of data.

\begin{table}
\begin{tabular}{l c c|c c} \hline \hline  & **Pythia-1B** & **TinyLlama-1.1B** & \(G_{\text{stack}}\)**-**1.1B** & **Baseline-1.1B** \\
**Datasets** & Pile-300B [52] & Slimpajama-627B\& Starcoder & Slimpajama-627B \\
**Tokens** & 100B & 103B & 100B & 100B \\ \hline
**lambda** & **53.52** & - & 48.20 & 47.87 \\
**ARC-c** & 25.59 & 24.32 & **29.18** & 27.21 \\
**ARC-e** & 47.26 & 44.91 & **54.25** & 48.86 \\
**piqa** & 69.31 & 67.30 & **71.98** & 69.64 \\
**logiqa** & **29.49** & - & 28.87 & 25.96 \\
**sciq** & 77.3 & - & **81.1** & 76.8 \\
**winogrande** & 51.22 & 53.28 & **56.03** & 54.53 \\ \hline
**Avg.** & 50.53 & - & **52.80** & 50.09 \\ \hline \hline \end{tabular}
\end{table}
Table 3: Compare with opensource LLMs on 1B

Figure 22: Visualization of the Equation 50.

[MISSING_PAGE_EMPTY:30]

Figure 24: Evaluation results on 410M.

Figure 25: Training loss and standard NLP benchmarks average accuracy of 1.1B.

Figure 27: Training loss and standard NLP benchmarks average accuracy of 3B.

Figure 26: Evaluation results on 1.1B.

### "Growth Factor" \(g\)

Figure 28: Evaluation results on 3B.

Figure 29: Training loss and standard NLP benchmarks average accuracy of 1.1B.

Figure 31: Training loss and standard NLP benchmarks average accuracy of 3B.

Figure 30: Evaluation results on 1.1B.

## Appendix H Discussion on "How to stack?" and Evaluation Results

### Training Loss and Evaluation Results of Gradual Stack

Figure 33: Training loss and standard NLP benchmarks average accuracy of scratch, \(G_{\text{stack}}\) and \(G_{gradual}\).

Figure 32: Evaluation results on 3B.

Ablation: \(f_{2}\circ f_{1}\circ f_{0}\circ f_{2}\circ f_{1}\circ f_{0}\) or \(f_{2}\circ f_{2}\circ f_{1}\circ f_{1}\circ f_{0}\circ f_{0}\) (interpolation)

To investigate whether the connections between layers affect the performance of stacking, we conduct a comparison of two approaches for stacking small models into larger ones. We explore two approaches for stacking small models into larger ones. The first approach involves taking the entire small model as a unit and directly stacking it, which can retain the connections between most layers. The second approach involves replicating and interleaving each layer in the small model, which almost break the connections. To measure the degree of retention of inter-layer connections after stacking, we define the connection rate \(R_{c}\):

\[R_{c}=\frac{Con_{r}}{Con_{all}}\] (51)

where the \(Con_{r}\) is number of retained connections, the \(Con_{all}\) is number of all connections.

For example, if we had a small model with three layers, denoted as \(f_{2}\circ f_{1}\circ f_{0}\), and desired a model depth of 6, the first approach would result in \(f_{2}\circ f_{1}\circ f_{0}\circ f_{2}\circ f_{1}\circ f_{0}\), where its \(R_{c}=80\%\). The second approach would result in \(f_{2}\circ f_{2}\circ f_{1}\circ f_{1}\circ f_{0}\circ f_{0}\), where its \(R_{c}=40\%\).

In our experiments, we stack a small model with 8 layers to a 24 layers target model. The growth timing \(d\) is \(10B\) tokens and growing factor \(s\) is \(3\). The \(R_{c}\) of \(G_{\text{stack}}\) is \(91.3\%\) and the \(R_{c}\) of \(G_{interpolate}\) is \(30.4\%\). We report the training loss and standard NLP benchmarks average accuracy in Figure 35. At the beginning of training, interpolated stacking perform as well as stacking entire small model. However, as the training continues, the performance of interpolated stacking deteriorates.

Therefore, we can conclude that the higher the connection rate of stacking, the better the effect of stacking. In Appendix H.3, we continue to validate this conclusion.

Figure 34: Evaluation results on scratch, \(G_{\text{stack}}\) and gradual stacking in StackBert.

We also report the details of evaluation results about 8 standard NLP benchmarks.

### Ablation: Partial Stacking

Partial stacking has been explored in LLMs like LlamaPro [42], Solar [43]. But their goal is to stack an off-the-shelf LLMs such as Llama2, while our aim is to accelerate LLM pre-training process.

To explore stacking which layers of the small model can achieve the best performance, we conduct experiments on partial stacking. In our experiments, we stack a small model with 6 layers (\(\{L_{1},L_{2},\cdots,L_{6}\}\)) to a 24 layers target model. We set growth timing \(d=10B\) tokens and growth factor \(g=4\). For simplicity, we use a format such as 1-234*7-56 to denote stacking 234 layers 7 times.

Figure 35: Training loss and standard NLP benchmarks average accuracy of scratch, \(G_{\text{stack}}\) and interpolation.

Figure 36: Evaluation results on scratch, \(G_{\text{stack}}\) and interpolation.

We report the training loss and standard NLP benchmarks average accuracy in Figure 37. By observing the loss curves in Figure 37a, we can find that the eight partial stacking methods are clearly divided into three groups based on their loss. The first group, {123456*4, 12-3456*5-56, 12-345*7-6, 123-456*7}, achieves the best performance. The second group consisting of {1234-56*10, 12-34*10-56, 1-234*7-56}, performs just so-so. The third group, {123*7-456}, performs poorly, even worse than the baseline.

In Table 5, we summarize the eight partial stacking and calculate the \(R_{c}\) of each partial stacking methods based on Equation 51.

For partial stacking, we conclude that: all \(>\) middle \(\approx\) back \(\gg\) front. Meanwhile, when the stacked parts are the same, the larger the \(R_{c}\), the better the performance.

Then, we report the evaluation results here.

\begin{table}
\begin{tabular}{c|c c c} \hline \hline
**Group** & **Method** & **Stacked parts** & \(R_{c}\) \\ \hline \multirow{3}{*}{**First**} & 123456*4 & all & 87.0\% \\  & 12-3456*5-56 & middle-back & 78.3\% \\  & 12-3457-6 & middle-back & 74.0\% \\  & 123-456*7 & back & 74.0\% \\  & 1234-56*10 & back & 60.7\% \\
**Second** & 12-34*10-56 & middle & 60.7\% \\  & 1-234*7-56 & front-middle & 74.0\% \\
**Third** & 123*7-456 & front & 74.0\% \\ \hline \hline \end{tabular}
\end{table}
Table 5: \(R_{c}\) and stacked parts of each partial stacking method

Figure 37: Training loss and standard NLP benchmarks average accuracy of scratch, \(G_{\text{stack}}\) and other partial stacking.

### Compare with Pythia, OLMo and Amber on 7B Size

## Appendix I Details of Function Preserving

### Function Preserving

Function preservation is a key concept that underlies diverse model growth approaches. It entails ensuring consistent output from a model, regardless of its expansion. Mathematically, let us define a function as \(F\) and a growth operator as \(G\). The ultimate aim is to apply the operator \(G\) to the function \(F\), thereby obtaining the target function denoted as \(\mathcal{F}\). The core objective here is to maintain the model's function to generate the same output for a given input. Formally,

\[\forall x,\mathcal{F}(x)=F(x)\text{, where }\mathcal{F}=G(F)\] (52)

\begin{table}
\begin{tabular}{l c c c|c} \hline \hline  & **Pythia-6.9B** & **OLMo-7B**[53] & **Amber-7B**[54] & \(G_{\text{stack}}\)**-7B** \\
**Datasets** & Pile-300B [52] & Dolma [55] & Amber & Slimpajama-627B \\
**Tokens** & 130B & 133B & 132B & 130B \\ \hline
**ARC-c** & 33.28 & 28.58 & 29.01 & **35.24** \\
**ARC-e** & 59.81 & 51.60 & 55.05 & **63.64** \\
**boolq** & 63.39 & 55.05 & 60.18 & **66.45** \\
**hellaswag** & 60.03 & 54.52 & 61.21 & **65.85** \\
**lambda** & **65.11** & 49.91 & 57.13 & 57.93 \\
**logiqa** & **28.88** & 28.42 & 26.73 & 26.88 \\
**obqa** & 37.20 & 33.60 & **37.40** & 36.40 \\
**piqa** & 75.03 & 74.43 & 76.01 & **76.82** \\
**sciq** & 82.7 & 74.4 & 82.0 & **85.9** \\
**winogrande** & 60.14 & 53.75 & 56.83 & **62.75** \\ \hline
**Avg.** & 56.56 & 50.43 & 54.16 & **57.79** \\
**Wikitext** & 13.3340 & 18.4690 & 15.6202 & **12.5635** \\ \hline \hline \end{tabular}
\end{table}
Table 6: Compare with opensource 7B LLMs on 130B tokens.

Figure 38: Evaluation results on scratch, \(G_{\text{stack}}\) and other partial stacking.

### Breaking Function Preserving by Adding Noise

For the down projection in SwiGLU and the output projection in MultiHeadAttention, we apply noise:

\[W_{noise}\leftarrow(1-\alpha)W+\alpha\epsilon\quad\text{where }\epsilon \sim\mathcal{N}(0,\frac{1}{d\times l^{2}})\] (53)

For the Embedding Layer and other Linear Layers, we apply noise:

\[W_{noise}\leftarrow(1-\alpha)W+\alpha\epsilon\quad\text{where }\epsilon \sim\mathcal{N}(0,\frac{2}{5d})\] (54)

#### Adding Noise on \(G_{\text{direct}}\) to Break FP

Figure 39: Training loss and standard NLP benchmarks average accuracy of scratch, \(G_{\text{direct}}^{\rightarrow}\) and \(G_{\text{direct}}^{\rightarrow}\) with 20% noise.

#### Training Loss And Evaluation Results on Adding Noise \(G_{\text{direct}}^{\rightarrow}\)

Adding Noise on \(G_{\text{stack}}\)Since adding noise actually improve the \(G_{\text{direct}}\) performance, we also add noise on \(G_{\text{stack}}\).

We stack an 8 layers small model to 24 layers, and then add noise with \(\alpha=0.2\). We report training loss and standard NLP benchmarks average accuracy in Figure 41. Adding noise demonstrates an advantage in Training loss.

Details of the evaluation results are as follows:

Figure 41: Training loss and standard NLP benchmarks average accuracy of scratch, \(G_{\text{stack}}\) and \(G_{\text{stack}}\) with 20% noise.

## Appendix J Results on Samba

We utilize the codebase from Samba9, which implements a hybrid State Space Model using the Slimpajama dataset for LM. In this experiment, we follow the guidelines outlined in the main paper to guide our stacking process. With a parameter size of 410M and training on 100B tokens, we set the growth timing to 8B and the growth factor to 3. We opted for 3 instead of 4 because Samba is an interleaving of Mamba and self-attention layers. Since the target model has 12 layers, we can only stack even layers, leading us to select a 4-layer base model (Mamba-SA-Mamba-SA).

Footnote 9: https://github.com/microsoft/Samba

Our experiments results on loss curves 43 and downstream tasks 7 indicate stacking also works beyond Transformer-based LLMs. Please note that in Table 7, we select stack with 47B rather than 50B to count the additional consumption required to train the base model on 8B tokens.

Figure 43: The training loss for two Samba LLMs, trained from scratch and with \(G_{stack}\). At loss=2.48, 2.45, 2.42, \(G_{\text{stack}}\) accelerates by 61.7%, 61.5% and 58.2% compared to scratch.

Figure 42: Evaluation results on scratch, \(G_{\text{stack}}\) and \(G_{\text{stack}}\) with 20% noise.

[MISSING_PAGE_FAIL:43]

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: In the Abstract, we clearly elucidate our contributions, and at the end of Section 1 Introduction, we further detail our contributions and scope. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: In Section 7, we discuss the limitations of our work. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?Answer: [NA]  Justification: Our study is empirical exploration. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We report our detailed training settings in Appendix B.3. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code**Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: We use open-source dataset Slimpajama-627B for pre-training, we report this in Appendix B.3. We have submitted our code on OpenReview and will open-source it on GitHub in the final version. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We report the detailed settings in Appendix B.3 Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [No] Justification: LLMs pre-training consumes a significant amount of computational resources, making it impractical to conduct multiple experiments to obtain error bars. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.

* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We report the needed Compute Resources in Appendix B.3 and required FLOPs of each experiments in each Figures. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We have read this code. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: We have a section in the Appendix L to discuss societal impacts. Guidelines: * The answer NA means that there is no societal impact of the work performed.

* If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [No] Justification: Our study is an empirical exploration. The dataset we use is a open-source high-quality corpus, and the models we release are intended solely for further research and are not meant for direct industrial application. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: Please refer to Appendix B.3. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset.

* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.

13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: All codes and models are will be full released under the license of CC-BY 4.0. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.

14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: This work does not involve crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurlPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.

15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: This work does not involve crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.

* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.