# Measuring Multimodal Mathematical Reasoning

with the MATH-Vision Dataset

Ke Wang\({}^{1,*}\) &Junting Pan\({}^{1,4,*,\dagger}\) &Weikang Shi\({}^{1,*}\) &Zimu Lu\({}^{1}\)

\({}^{1}\)CUHK MMLab, \({}^{2}\)Shanghai AI Lab, \({}^{3}\)SenseTime Research, \({}^{4}\)CPII under InnoHK

\({}^{*}\)Joint first author, \({}^{\dagger}\)Project lead, \({}^{\ddagger}\)Directional lead

###### Abstract

Recent advancements in Large Multimodal Models (LMMs) have shown promising results in mathematical reasoning within visual contexts, with models exceeding human-level performance on existing benchmarks such as MathVista. However, we observe significant limitations in the diversity of questions and breadth of subjects covered by these benchmarks. To address this issue, we present the MATH-Vision (MATH-V) dataset, a meticulously curated collection of 3,040 high-quality mathematical problems with visual contexts sourced from real math competitions. Spanning 16 distinct mathematical disciplines and graded across 5 levels of difficulty, our dataset provides a comprehensive and diverse set of challenges for evaluating LMMs' mathematical reasoning abilities. Through extensive experimentation, we unveil a notable performance gap between current LMMs and human performance on MATH-V, underscoring the imperative for further advancements in LMMs. Moreover, our detailed categorization allows for a thorough error analysis of LMMs, offering valuable insights to guide future research and development.

The dataset is released at MathLMs/MathVision

Figure 1: (a) Zero-shot accuracies of four prominent Large Multimodal Models (LMMs), random chance, and human performance are evaluated on our proposed MATH-V across 16 subjects. Teal means newly introduced subjects. (b) Examples of easy problems in MATH-V failed by top-performing LMMs on MathVista. The three questions come from tests designed for elementary school students.

Introduction

The latest advancements in Large Language Models (LLMs) [1; 2; 3; 4; 5] and Large Multimodal Models (LMMs) [6; 7; 8; 9; 10; 11] have demonstrated remarkable capabilities, achieving impressive performance on a diverse array of language and visual-language tasks [12; 13; 14; 15; 16; 17; 18; 19; 20]. As a result of these strides, recent LMMs like GPT-4 [1; 6], InternLM-XComposer [21], InternVL [22] and Gemini [7; 23] are now outperforming average human capabilities in multimodal mathematical reasoning. Notably, on MathVista [24], the most widely used benchmark for evaluating LMMs' mathematical reasoning in visual contexts, the current leading model achieves a score of 63.9, surpassing the human average of 60.3. While the impressive performance on MathVista suggests that LMMs have surpassed average human capabilities in multimodal mathematical reasoning, our recent experiment on human math tests reveals a different conclusion. As illustrated in Figure 1.b, these problems, which are straightforward for humans, remain challenging for LMMs.

In light of this discrepancy and to gain a deeper understanding of the limitations of current math-oriented multimodal datasets, we conducted a thorough examination of existing benchmarks. Our analysis revealed two key issues:

Firstly, we observed that the range of subjects of existing benchmarks remains limited. For example, geometry problems are one of the most common types of math problems with visual contexts, as evidenced by the large body of geometry problems and benchmarks (_i.e.,_ Geometry3k [25], GeoQA+[26], GEOS [27], and UniGeo [28]). However, despite the abundance of problems, the diversity is quite limited. Surprisingly, we found that they predominantly concentrate on plane geometry, neglecting the rich diversity of other geometry subjects such as solid geometry, which deals with the volume and surface area of 3D objects. Additionally, we found that topics such as Graph Theory, Topology and Transformation Geometry are also rarely present in current benchmarks.

Secondly, while the visual data in existing datasets are quite diverse and are collected from different sources, the questions are quite limited in question types and contents. For example, we identified that among the 375 questions on "abstract scene" from MathVista, the majority can be categorized into just three types of questions: (1) Object Counting, (2) Length Measurement with a given reference, and (3) Time Inquiry based on a clock. Questions are mostly recognition-driven and seldom require in-depth mathematical reasoning. Similar problems also exist in image contexts of "function plots", "synthetic scenes", "natural images", etc. This tendency towards monotonous question patterns is commonly observed, where annotators are first given a contextual image and then asked to create math-related questions according to the image.

The limited subject range and question variety of current multimodal mathematical reasoning benchmarks can create a false impression of LMMs' capabilities, potentially misguiding the development of these models. Therefore, aiming to more comprehensively evaluate the mathematical reasoning skills of LMMs, we introduce the MATH-Vision (MATH-V) dataset. This dataset comprises 3,040 high-quality mathematical problems within visual contexts across 12 grades, carefully selected from 19 real-world math competitions. To ensure a high-quality standard, **all data within our dataset were cross-validated and filtered by multiple expert annotators**, guaranteeing that each problem has a unique and correct answer. The dataset is well-balanced, featuring 1,532 problems in an open-ended format and 1,508 in a multiple-choice format. Specifically, we introduce three major updates in our MATH-V:

1. **Expanded Categories**: In MATH-V, we introduce 8 new math categories, such as descriptive geometry (dimension understanding), graph theory (relations modeling), and topology (invariant under continuous deformation) and subdivide plane geometry into three categories (angle, area and length). These categories are important areas of multimodal mathematical reasoning and are currently not covered in existing benchmarks.

2. **Holistic Data Approach**: All data in MATH-V, including both questions and images, are jointly collected from real-world math competitions. This approach eliminates the need for additional annotation and avoids the monotonous question patterns. This difference in question annotation is reflected in the average question length, which is 42.3 for MATH-V, significantly surpassing the average of 15.6 of MathVista. Longer question lengths offer numerous advantages, particularly in facilitating richer mathematical reasoning by providing additional context. Further qualitative comparisons can be found in Appendix G.1.

3. **Difficulty Levels**: To better understand model performance, we categorize all problems in MATH-V by difficulty levels ranging from 1 to 5. This classification is rigorously verified by a team of experts to ensure accuracy and reliability. We hope that this categorization can facilitate better analysis of mathematical problem-solving skills across various levels of difficulty.

We conducted extensive experiments with our dataset to provide insights into the mathematical reasoning abilities of current LMMs on human math tests. As depicted in Figure 1.a, our findings reveal a substantial gap in mathematical reasoning within visual contexts between LMMs and humans, with scores of 30.39 and 68.82, respectively. Our MATH-V benchmark poses a significant challenge for LMMs but is relatively easy for humans to achieve a good score, as shown in Figure 1.b. This indicates that current top-tier LMMs still have many areas (_e.g.,_ transformation geometry, topology, etc.) with significant room for improvement in terms of mathematical reasoning in visual contexts to approach the performance of average humans.

Given that MATH-V primarily consists of unseen data for existing LMMs, it generally exhibits lower scores compared to other benchmarks with available training data that could have been used during LMMs' development [24; 29; 30]. We believe that solving these tasks is fundamental for applications in architecture, design, and engineering (_e.g.,_ solid geometry for architecture). Therefore, our benchmark provides a crucial foundation for future research. Furthermore, MATH-V could perfectly complement current benchmarks, together offering a more robust evaluation of the mathematical reasoning capabilities of LMMs.

In conclusion, the contributions of this study are listed as follows:

* We carefully audited existing datasets and identified significant problems that are relevant but not known to the community, such as missing math subjects and a significant number of questions with repetitive patterns.
* We present a new MATH-V benchmark for pushing advances in multimodal mathematical reasoning. MATH-V comprises 3,040 problems with visual context, selected from real-world math competitions. All problems are classified into 5 difficulty levels and 16 mathematical disciplines.
* Finally, leveraging the fine-grained categorization, we conducted a comprehensive analysis of current LMMs, suggesting new directions for improvement in future research.

## 2 Related Works

### Mathematical Reasoning Benchmarks

Various benchmark datasets [31; 32; 33; 34; 35] have been proposed to assess models' mathematical reasoning capabilities. However, current multimodal benchmarks [29; 36; 37; 38] primarily focus on visual recognition tasks, and multimodal mathematical reasoning benchmarks are less common. Early multimodal reasoning benchmarks, such as Geometry3k [25], GeoQA+[26], GEOS [27], and UniGeo [28], are very limited in subjects, primarily focused on plane geometry. Among recent multimodal benchmarks, MMMU [39] stands out, though it is limited to multiple-choice questions and only a small fraction of its content is mathematical. Another recent benchmark, MathVista [24], evaluates mathematical reasoning abilities within diverse visual contexts. However, it lacks a detailed classification of mathematical subdomains and emphasizes visual abilities rather than mathematical reasoning. In contrast, MATH-V contains math problems rigorous in both mathematical reasoning and visual recognition, categorized by clear difficulty levels and covering 16 representative subjects.

### LMMs for Math

Recently, AlphaGeometry [40] has shown impressive performance in solving challenging geometry problems, though it lacks the ability to process images and must rely on text descriptions of images. Current math-specific multimodal models, such as G-LLaVA [41], UniMath [42], MatCha [43], and UniChart [44], are primarily focused on plane geometry or chart-based problems and still lag behind general multimodal models such as GPT-4-turbo [1]. To facilitate better evaluation of the mathematical reasoning abilities of multimodal models, we introduce our benchmark, featuring high-quality problems demanding expertise in both mathematical reasoning and visual recognition.

### Multimodal Foundation Models

With the success of LLMs, MiniGPT-4 [45], LLaMA-Adapter [46], and LLaVA [47; 10] have attempted to construct multimodal models from text-only models. Subsequently, various large multimodal models such as GPT-4V [6], Gemini [7; 23], Qwen-VL [11], InternLM-XComposer-VL [21; 8], and SPHINX [9] have been created, demonstrating impressive performance on benchmarks like MMMU [39] and MathVista [24]. Notably, some open-sourced models have even received scores higher than GPT-4-turbo on these benchmarks. However, on our benchmark, GPT-4-turbo performs considerably better than open-source models, indicating the lack generalization abilities of some models, especially on questions very different from those in fine-tuning datasets.

## 3 Dataset

### Overview

We introduce MATH-Vision (MATH-V), a carefully curated benchmark designed to evaluate the multimodal mathematical reasoning capabilities of foundation models across a wide range of mathematical tasks with visual contexts. We particularly emphasize the challenges posed by the need for both expert-level visual perception and deliberate reasoning with subject-specific knowledge. This challenge is exemplified in our tasks, which require the processing of various heterogeneous image types and necessitate a model's proficiency in utilizing domain-specific knowledge to deeply comprehend both text and images, and to reason accordingly. This extends far beyond basic visual perception and calls for an advanced approach that combines sophisticated multimodal analysis with mathematical reasoning. The proposed benchmark encompasses 16 subjects over 5 levels of difficulty, including 8 new subjects including _Analytic Geometry, Combinatorial Geometry, Combinatorics, Descriptive Geometry, Graph Theory, Solid Geometry, Topology, Transformation Geometry_ and subdividing plane geometry into three categories: _angle, area, length_. The questions in our benchmark were manually collected from 19 competitions. The division of difficulty levels is primarily based on the recommended grades of these competitions, with details provided in Appendix H.2. MATH-V consists of 3,040 questions, and we also provide a smaller test set of 304 questions for quick evaluation. Detailed coverage, statistics and sources of MATH-V are presented in Appendix B.

### Data Collection

Data collectionOur benchmark collection comprises two stages. In the first stage, we gather a list of various mathematical competitions, both international and national. The selection is based on the criterion that visual inputs should be frequently utilized in the competition questions to provide significant insights, and the questions should be categorizable into different levels, such as AMC8, AMC10, AMC12, and AIME. Competitions like the International Mathematical Olympiad (IMO) are excluded due to the excessive difficulty of the questions and the rare use of visual inputs. Consequently, we select 19 competitions from various countries. In the second stage, we filter through online resources to identify problems where questions are interspersed with images. We adhere strictly to copyright and licensing regulations, ensuring that we avoid data from sites that prohibit copying and redistribution. Given the concerns surrounding data contamination in LMMs, we prioritize selecting questions that lack readily available answers, such as those found in separate documents or pages. For questions formatted as PDFs, we use the Mathpix API to extract text and images. This approach allows us to curate a diverse collection of 9k questions from various sources.

Data curationTo improve the quality of our data, we undergo a four-stage data curation process with the help of 10 annotators (senior college students from science-related majors). In the first stage, we verify the alignment of text questions and images, as Mathpix might return them in an incorrect order. We also eliminate questions with missing text or incorrect images and those with private information or offensive content. All results are cross-verified between different annotators. 3,352 math-VQA data are left after this stage. In the second stage, we use lexical overlap and Levenshtein distance to identify potential duplicate questions. These suspected duplicates are then reviewed to identify and remove any duplication. As a result of these steps, we obtain the final 3,040 math-VQA data for our benchmark. In the third stage, we categorize the problems into 16 different subjects. Our annotators are divided into three groups, each independently labeling all the questions, and we also utilized GPT-4V [6] and Gemini Pro [7] for subject advice. Annotations from various groups andmodels are consolidated through a majority voting system involving three expert annotators. In the final stage, we assign difficulty levels for each questions with reference to the grade requirements of the original competitions.

### Comparison with Existing Benchmarks

Comparing with MathVistaMathVista [24] is a comprehensive multimodal mathematical reasoning benchmark derived from 28 existing math-related multimodal datasets and 3 newly collected ones. However, within MathVista, approximately 20 datasets exhibit a trend where questions are annotated post-image collection by annotators, resulting in a relatively limited variability of questions, as shown in Figure 2. In contrast to MathVista, all math-VQA data including both questions and images in MATH-V are newly collected from real-world math competitions. This difference in obtaining question annotations is reflected in the average question length, which stands at 42.3 for MATH-V, significantly surpassing the 15.6 average of MathVista. A longer question length offers numerous advantages, particularly in facilitating richer mathematical reasoning by providing additional context. Further qualitative comparisons of math-VQA data between our dataset and MathVista are provided in Appendix G.1. Moreover, for MATH-V we also provide 16 fine-grained categorizations of different math subjects, which can gain deeper insights into individuals' strengths and weaknesses in specific areas of mathematics, compared to MathVista's 7 math reasoning types. The detailed data of the comparison can be found in Table 1. Finally, all problems within our datasets are labeled with one of the five levels of difficulty, while MathVista has only three levels, with a considerable portion of problems (37.7%) classified under the _not applicable_ category.

Comparing with MMMUDifferent from MathVista and our MATH-V, MMMU [39] is designed to evaluate the multi-discipline multimodal understanding and reasoning abilities of LMMs with college-level problems. In terms of Math, it spans eight distinctive subjects with a total of 500 questions, whereas subjects like Operation Research, Graph Theory, and Group Theory require sophisticated domain knowledge such as "DFS search" and "Cayley diagram". In contrast, our MATH-V focus on mathematics problems from elementary school to high school and can test the general visual perception and mathematical reasoning abilities of LMMs. Our dataset is 6 times larger than MMMU-Math, providing a significantly larger and more diverse set of challenges for analysis and evaluation.

\begin{table}
\begin{tabular}{l c c} \hline \hline  & **MathVista** & **MATH-V** \\ \hline Main source & existing VQA \& & all from \\  & MathQA datasets & real exams \\ \hline Template questions & Yes & No \\ \hline Math & & \\ reasoning & 7 & 16 \\ types & & \\ \hline Newly annotated questions & 736 & 3040 \\ \hline Unique questions & 4746 & 3040 \\ \hline Average question & 15.6 words & 42.3 words \\ length & & \\ \hline \hline \end{tabular}
\end{table}
Table 1: Statistics of key differences between MathVista and our MATH-V. MATH-V comprises high-quality math questions from real examinations or competitions. In contrast, MathVista primarily features images from existing VQA datasets with template questions.

Figure 2: Questions in (a), (b), and (c) all emphasize visual recognition over mathematical reasoning. (a) requires model to identify the persons, which involves niche domain knowledge that is not related to mathematical reasoning. (d) highlights non-mathematical problems in MathVista.

Specific to subjects, our datasets not only offer better coverage of subjects but also have more problems per subject on average (190 v.s. 63). Note that in 3/8 subjects (Linear Algebra, Group Theory, and Logic) covered by MMMU-Math, there are less than 10 problems. More qualitative comparisons can be found in Appendix G.2.

## 4 Experiments

We conduct experiments to evaluate models' performance on MATH-V. Our findings indicate that accuracy remains low across even the most advanced models. We assess a variety of models, including LLMs and LMMs, and compare both closed- and open-source models, observing that closed-source models perform better. We also did a detailed error analysis of GPT-4V, the first widely used LMM, delving into its error distribution and providing corresponding qualitative examples.

### Experimental Setup

**Models.** We conducted experiments on (a) LLMs: ChatGPT-3.5 [15] and GPT-4 [1], augmented with image captions generated by GPT-4V, (b) Open-source LMMs: LLaVA-v1.5 [47], SPHINX [9], ShareGPT-4V [48], InternLM-XComposer2-VL [8] and InternVL-Chat-V1-2-Plus [49], (c) Closed-source LMMs: Qwen-VL [11], Gemini Pro [7], Gemini 1.5 Pro [23] and the GPT-4 series.

**Implementation details.** Our evaluations are carried out in a zero-shot setting, both with and without Chain-of-Thought prompting [50] on several prominent models, to determine the models' ability

to produce accurate answers without the need for fine-tuning or few-shot demonstrations on our benchmark. For human performance, we recruited 100 annotators possessing a high school degree or higher, and they were asked to solve the questions in MATH-V independently, thereby obtaining the human performance baseline. Details can be found in Appendix K.3. For other models, we utilize the default prompts provided for multiple-choice or open-ended questions, when available. In cases where models do not offer prompts for the task types present in the MATH-V dataset, we engage in prompt engineering and select the most effective prompt for use in the zero-shot configuration during the main experiments. Detailed information about our prompts is available in the Appendix E.

### Experimental Results

In this section, we compare the performance of several best-performing Large Language Models (LLMs) and Large Multimodal Models (LMMs) on MATH-V, as detailed in Table 2 and Table 3.

**Robustness of MATH-V** comes from three aspects. First, to provide a more rigorous evaluation of the models' generalization capabilities, MATH-V remains entirely unseen by the LMMs. This differs from existing benchmarks, which may include training data from the same domain. This stricter evaluation is reflected by the lower SOTA scores on MATH-V compared to those on mainstream mathematical reasoning benchmarks, as shown in Figure 3.a. Furthermore, our dataset's multiple-choice questions all feature five options, while existing benchmarks typically offer fewer options or simple yes/no questions. This reduces the chances of correct guesses, as evidenced by a much lower random chance score (5.86 for MATH-V vs. 17.9 for MathVista). Lastly, our holistic data collection approach jointly collects images and corresponding questions from real exams rather than relying on additional annotation. The lower performance of text-only models like GPT-4 on MATH-V further demonstrates the dependency of our questions on the associated images. This approach not only results in longer and more diverse questions but also requires models to possess more sophisticated visual recognition and mathematical reasoning capabilities.

**Considerable room for improvement.** As shown in Table 2, both closed- and open-source models have achieved significantly lower scores on our dataset compared to humans, with the highest scores being 30.39 and 16.97, respectively, while the average human score is 68.82, highlighting the considerable room for improving LMMs' capabilities. As depicted in Figure 3.b, with majority voting of 20 reasoning paths, models like GPT-4-turbo can achieve a 5% increase in accuracy, indicating that these models have the potential to address these questions. Moreover, in less than eight months' evolution from GPT-4V to GPT-4o, there was a notable increase in accuracy on MATH-V, from 22.76% to 30.39%, consistent with significant accuracy gains observed in other benchmarks such as MathVista and MMMU. Other open-source models, such as InternVL-Chat-V1-2-Plus, have achieved

Figure 3: (a) State-of-the-art (SOTA) performance and corresponding model of several mathematical benchmarks in **zero-shot** settings. Our MATH-V benchmark is markedly lower than existing text-only and multi-modal mathematical benchmarks. Results of other benchmarks are either from their official website or Papers With Code. (b) Although current models achieve relatively low scores on MATH-V, employing 20 times majority voting can still increase accuracy by up to 5%, indicating that models have the potential to address these questions effectively.

enhanced performance on MATH-V by employing a stronger language model and scaled fine-tuning datasets. These advancements indicate that LMMs can continue to make progress on MATH-V through innovations in model architecture, enhancements in training data, etc.

**Comparison among different subjects.** The dataset covers a broad spectrum of mathematical subjects, from algebra to transformation geometry, each presenting unique complexities. For example, even the high-performing GPT-4o struggles to achieve more than 20% accuracy in transformation geometry. This contrasts with its relatively higher accuracy in arithmetic, where it achieves 49.3%. These disparities across subjects highlight the model's strengths in numerical calculations and its weaknesses in understanding spatial position transformations.

**Disparity of closed- and open-source models.** There is a notable performance disparity between closed- and open-source models, different from the results on MathVista. Closed-source models, including GPT-4, GPT-4-turbo, GPT-4V, Gemini-1.5 Pro, and Gemini Pro, all outperform the best open-source model. For instance, GPT-4 achieved the highest overall accuracy at 30.39%, excelling in subjects like algebra (42.0%), analytic geometry (39.3%), and arithmetic (49.3%). In contrast, the highest-performing open-source model, InternVL-Chat-V1-2-Plus, only reached an overall accuracy of 16.97%. However, results from MathVista show that the accuracy of InternVL-Chat-V1-2-Plus is 59.9%, which is 10% higher than that of GPT-4V, which scored 49.9%. This discrepancy may be attributed to the use of the training set of MathVista during the supervised fine-tuning stage of InternVL-Chat-V1-2-Plus, as training data for both models is unavailable for our dataset.

**Failure of CoT prompting on some models.** Chain-of-Thought (CoT) prompting did not consistently enhance performance across models. While GPT-4V benefited from CoT (22.76% to 23.98%), other models like Gemini Pro and Qwen-VL-Max saw a decrease in performance. We found many LMMs tend to generate very short responses like "The answer is..." without reasoning steps, even when required to solve the question step by step. This suggests that the step-by-step reasoning capabilities of LMMs are still not mature and cannot be fully exploited by CoT prompting.

### Error Analysis

**Quantitative and qualitative analysis on GPT-4V.** In this section, we meticulously analyzed the causes of errors in GPT-4V through both quantitative and qualitative approaches, detailed in Figure 5 and Figure 5. The most prevalent cause of errors, accounting for 42.2%, was identified as Reasoning Error. This indicates a significant challenge in the model's logical processing and inferential reasoning capabilities. Following closely, Vision Recognition Error was the second most common issue, responsible for 31.9% of errors. This highlights difficulties in accurately interpreting visual data, suggesting that the vision encoder remains a bottleneck in multi-modal models. Knowledge Error,making up 15.1% of the total errors, pointed to gaps between the model's information database and its application. Furthermore, Calculation Error accounted for the smallest percentage of errors at 1.3%, indicating that GPT-4V is relatively robust in numerical and computational tasks, which historically have been challenging for language models. Notably, all errors in the Calculation Error category related to symbolic computation, as shown in Figure 5, rather than numerical computation, suggesting a potential area for future improvement. Detailed examples of these errors can be found in Appendix I.

**Qualitative analysis on GPT-4 with image captions.** The integration of image captions generated by GPT-4V enables GPT-4 to reach an accuracy of 13.10%, comparable to that of SPHINX and Qwen-VL. In disciplines such as algebra, analytic geometry, arithmetic, combinatorics, and counting, GPT-4 with image captions outperforms Gemini Pro and all open-source models except InternVL-Chat-V1-2-Plus. Conversely, in other subjects, its performance notably lags behind these models. Through qualitative analysis of these categories, we observed that subjects like algebra and analytic geometry are more likely to receive relatively accurate descriptions from GPT-4V, unlike other disciplines. This discrepancy underscores the need for LMMs to enhance their capabilities in fine-grained image understanding and description. Remarkably, in some areas (e.g., combinatorics and counting), GPT-4 with image captions even surpasses GPT-4V. This success can be attributed to GPT-4's reasoning capabilities, which evidently contribute to a better understanding and solving of problems. Examples supporting this qualitative analysis are provided in the Appendix J.

## 5 Conclusion and Limitations

In this paper, we propose a benchmark dataset, called MATH-V, for evaluating the mathematical reasoning abilities of multimodal models. MATH-V consists of 3,040 math reasoning problems, each accompanied by images and texts. We have carefully evaluated a variety of open-source and closed-source models using our MATH-V and conducted a systematic analysis. Our benchmark reveals a significant gap between human performance and that of current multimodal models in the task of multimodal reasoning, and facilitates more effective evaluation for future studies in this field.

However, our work does have certain limitations. First, all of the questions in our dataset are in English. Therefore, the dataset cannot evaluate models' capabilities to reason in languages other than English. Also, the images in our benchmark are all abstract, with no pictures taken in real life. These limitations are important and warrant further investigation.

## 6 Acknowledgements

This project is funded in part by National Key R&D Program of China Project 2022ZD0161100, by the Centre for Perceptual and Interactive Intelligence (CPII) Ltd under the Innovation and Technology Commission (ITC)'s InnoHK, by General Research Fund of Hong Kong RGC Project 14204021. Hongsheng Li is a PI of CPII under the InnoHK. This work was conducted using content provided by Association Kangourou Sans Frontieres - AKSF, https://www.aksf.org. We are grateful to Meike Akveld and Robert eretschlager for their support of this project.

## References

* [1] OpenAI. GPT-4 Technical Report, 2023.
* [2] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajiwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molyboy, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Scheleton, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaqiong Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Pavin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models, 2023.

* [3] Rohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, Eric Chu, Jonathan H. Clark, Laurent El Shafey, Yanping Huang, Kathy Meier-Hellstern, Gaurav Mishra, Erica Moreira, Mark Omernick, Kevin Robinson, Sebastian Ruder, Yi Tay, Kefan Xiao, Yuanzhong Xu, Yujing Zhang, Gustavo Hernandez Abrego, Junwhan Ahn, Jacob Austin, Paul Barham, Jan Botha, James Bradbury, Siddhartha Brahma, Kevin Brooks, Michele Catasta, Yong Cheng, Colin Cherry, Christopher A. Choquette-Choo, Aakanksha Chowdhery, Clement Crepy, Shachi Dave, Mostafa Dehghani, Sunipa Dev, Jacob Devlin, Mark Diaz, Nan Du, Ethan Dyer, Vlad Feinberg, Fangxiayo Feng, Vlad Fienber, Markus Freitag, Xavier Garcia, Sebastian Gehrmann, Lucas Gonzalez, Guy Gur-Ari, Steven Hand, Hadi Hashemi, Le Hou, Joshua Howland, Andrea Hu, Jeffrey Hui, Jeremy Hurwitz, Michael Isard, Abe Ittycheriah, Matthew Jagielski, Wenhao Jia, Kathleen Kenealy, Maxim Krikun, Sneha Kudugunta, Chang Lan, Katherine Lee, Benjamin Lee, Eric Li, Music Li, Wei Li, YaGuang Li, Jian Li, Hyeontaek Lim, Hanzhao Lin, Zhongtao Liu, Frederick Liu, Marcello Maggioni, Aroma Mahendru, Joshua Maynez, Vedant Misra, Maysam Moussalem, Zachary Nado, John Nham, Eric Ni, Andrew Nystrom, Alicia Parrish, Marie Pellat, Martin Polacek, Alex Polozov, Reiner Pope, Siyuan Qiao, Emily Reif, Bryan Richter, Parker Riley, Alex Castro Ros, Aurko Roy, Brennan Saeta, Rajkumar Samuel, Renee Shelby, Ambrose Slone, Daniel Smilkov, David R. So, Daniel Sohn, Simon Tokumine, Dasha Valter, Vijay Vasudevan, Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, Tao Wang, John Wieting, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Qiao Zhang, Steven Zheng, Ce Zheng, Weikang Zhou, Denny Zhou, Slav Petrov, and Yonghui Wu. Palm 2 technical report, 2023.
* [4] Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru, Merouane Debbah, Etienne Goffinet, Daniel Hesslow, Julien Launay, Quentin Malratic, Daniele Mazzotta, Baderddine Noune, Baptiste Pannier, and Guilherme Penedo. The falcon series of open language models, 2023.
* [5] InternLM Team. Internlm: A multilingual language model with progressively enhanced capabilities. https://github.com/InternLM/InternLM-techreport, 2023.
* [6] OpenAI. GPT-4V(ision) System Card. In _technical report_, 2023.
* [7] Gemini Team. Gemini: A family of highly capable multimodal models, 2023.
* [8] Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Bin Wang, Linke Ouyang, Xilin Wei, Songyang Zhang, Haodong Duan, Maosong Cao, Wenwei Zhang, Yining Li, Hang Yan, Yang Gao, Xinyue Zhang, Wei Li, Jingwen Li, Kai Chen, Conghui He, Xingcheng Zhang, Yu Qiao, Dahua Lin, and Jiaqi Wang. Internlm-xcomposer2: Mastering free-form text-image composition and comprehension in vision-language large model. _arXiv preprint arXiv:2401.16420_, 2024.
* [9] Ziyi Lin, Chris Liu, Renrui Zhang, Peng Gao, Longtian Qiu, Han Xiao, Han Qiu, Chen Lin, Wenqi Shao, Keqin Chen, Jiaming Han, Siyuan Huang, Yichi Zhang, Xuming He, Hongsheng Li, and Yu Qiao. Sphinx: The joint mixing of weights, tasks, and visual embeddings for multi-modal large language models, 2023.
* [10] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning, 2023.
* [11] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: A versatile vision-language model for understanding, localization, text reading, and beyond, 2023.
* [12] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. A survey of large language models, 2023.
* [13] Zhenjie Yang, Xiaosong Jia, Hongyang Li, and Junchi Yan. Llm4drive: A survey of large language models for autonomous driving, 2023.
* [14] Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Romain Sauvestre, Tal Remez, Jeremy Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre Defossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom, and Gabriel Synnaeve. Code llama: Open foundation models for code, 2024.
* [15] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. _Advances in neural information processing systems_, 33:1877-1901, 2020.

* [16] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the v in vqa matter: Elevating the role of image understanding in visual question answering, 2017.
* [17] Yupan Huang, Zaiqiao Meng, Fangyu Liu, Yixuan Su, Nigel Collier, and Yutong Lu. Sparkes: Unlocking chats across multiple images for multimodal instruction-following models, 2023.
* [18] Jingyi Zhang, Jiaxing Huang, Sheng Jin, and Shijian Lu. Vision-language models for vision tasks: A survey, 2023.
* [19] Can Cui, Yunsheng Ma, Xu Cao, Wenqian Ye, Yang Zhou, Kaizhao Liang, Jintai Chen, Juanwu Lu, Zichong Yang, Kuei-Da Liao, et al. A survey on multimodal large language models for autonomous driving. In _Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision_, pages 958-979, 2024.
* [20] Ao Zhang, Yuan Yao, Wei Ji, Zhiyuan Liu, and Tat-Seng Chua. Next-chat: An lmm for chat, detection and segmentation, 2023.
* [21] Pan Zhang, Xiaoyi Dong, Bin Wang, Yuhang Cao, Chao Xu, Linke Ouyang, Zhiyuan Zhao, Shuangrui Ding, Songyang Zhang, Haodong Duan, Wenwei Zhang, Hang Yan, Xinyue Zhang, Wei Li, Jingwen Li, Kai Chen, Conghui He, Xingcheng Zhang, Yu Qiao, Dahua Lin, and Jiaqi Wang. Intermlm-xcomposer: A vision-language large model for advanced text-image comprehension and composition. _arXiv preprint arXiv:2309.15112_, 2023.
* [22] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Zhong Muyan, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. _arXiv preprint arXiv:2312.14238_, 2023.
* [23] Gemini Team. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context, 2024.
* [24] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chun yue Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating math reasoning in visual contexts with gpt-4v, bard, and other large multimodal models. _ArXiv_, abs/2310.02255, 2023.
* [25] Pan Lu, Ran Gong, Shibiao Jiang, Liang Qiu, Siyuan Huang, Xiaodan Liang, and Song-Chun Zhu. Intergps: Interpretable geometry problem solving with formal language and symbolic reasoning. In _Annual Meeting of the Association for Computational Linguistics_, 2021.
* [26] Jie Cao and Jing Xiao. An augmented benchmark dataset for geometric question answering through dual parallel text encoding. In _Proceedings of the 29th International Conference on Computational Linguistics_, pages 1511-1520, Gyeongju, Republic of Korea, October 2022. International Committee on Computational Linguistics.
* [27] Minjoon Seo, Hannaneh Hajishirzi, Ali Farhadi, Oren Etzioni, and Clint Malcolm. Solving geometry problems: Combining text and diagram interpretation. In _Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing_, pages 1466-1476, Lisbon, Portugal, September 2015. Association for Computational Linguistics.
* [28] Jiaqi Chen, Tong Li, Jinghui Qin, Pan Lu, Liang Lin, Chongyu Chen, and Xiaodan Liang. Unigeo: Unifying geometry logical reasoning via reformulating mathematical expression, 2022.
* 31, 2015.
* [30] Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering, 2022.
* [31] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Xiaodong Song, and Jacob Steinhardt. Measuring massive multitask language understanding. _ArXiv_, abs/2009.03300, 2020.
* [32] Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. Program induction by rationale generation: Learning to solve and explain algebraic word problems. In _Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 158-167, Vancouver, Canada, July 2017. Association for Computational Linguistics.

* [33] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. _arXiv preprint arXiv:2103.03874_, 2021.
* [34] Arkil Patel, Satwik Bhattamishra, and Navin Goyal. Are NLP models really able to solve simple math word problems? In _Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies_, pages 2080-2094, Online, June 2021. Association for Computational Linguistics.
* [35] Aida Amini, Saadia Gabriel, Shanchuan Lin, Rik Koncel-Kedziorski, Yejin Choi, and Hannaneh Hajishirzi. Mathqa: Towards interpretable math word problem solving with operation-based formalisms. _ArXiv_, abs/1905.13319, 2019.
* [36] Ahmed Masry, Xuan Long Do, Jia Qing Tan, Shafiq R. Joty, and Enamul Hoque. Chartqa: A benchmark for question answering about charts with visual and logical reasoning. _ArXiv_, abs/2203.10244, 2022.
* [37] Shaghayegh Mobasher, Ghazal Zamaninejad, Maryam Hashemi, Melika Nobakhtian, and Sauleh Eetemadi. Parsvqa-caps: A benchmark for visual question answering and image captioning in persian. In _The 2022 Conference on Empirical Methods in Natural Language Processing_, 2022.
* [38] Danna Gurari, Qing Li, Abigale Stangl, Anhong Guo, Chi Lin, Kristen Grauman, Jiebo Luo, and Jeffrey P. Bigham. Vizwiz grand challenge: Answering visual questions from blind people. _2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 3608-3617, 2018.
* [39] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. Mmnu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. _ArXiv_, abs/2311.16502, 2023.
* [40] Trieu H. Trinh, Yuhuai Wu, Quoc V. Le, He He, and Thang Luong. Solving olympiad geometry without human demonstrations. _Nature_, 625:476-482, 2024.
* [41] Jiahui Gao, Renjie Pi, Jipeng Zhang, Jiacheng Ye, Wanjun Zhong, Yufei Wang, Lanqing Hong, Jianhua Han, Hang Xu, Zhenguo Li, and Lingpeng Kong. G-flava: Solving geometric problem with multi-modal large language model. _ArXiv_, abs/2312.11370, 2023.
* [42] Zhenwen Liang, Tianyu Yang, Jipeng Zhang, and Xiangliang Zhang. Unimath: A foundational and multimodal mathematical reasoner. In _Conference on Empirical Methods in Natural Language Processing_, 2023.
* [43] Fangyu Liu, Francesco Piccinno, Syrine Krichene, Chenxi Pang, Kenton Lee, Mandar Joshi, Yasemin Altun, Nigel Collier, and Julian Martin Eisenschlos. Matcha: Enhancing visual language pretraining with math reasoning and chart deredering. In _Annual Meeting of the Association for Computational Linguistics_, 2022.
* [44] Ahmed Masry, Parsa Kavehzadeh, Xuan Long Do, Enamul Hoque, and Shafiq R. Joty. Unichart: A universal vision-language pretrained model for chart comprehension and reasoning. _ArXiv_, abs/2305.14761, 2023.
* [45] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. _arXiv preprint arXiv:2304.10592_, 2023.
* [46] Renrui Zhang, Jiaming Han, Chris Liu, Peng Gao, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, and Yu Qiao. Llama-adapter: Efficient fine-tuning of language models with zero-init attention, 2023.
* [47] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning, 2023.
* [48] Lin Chen, Jisong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. Sharegpt4v: Improving large multi-modal models with better captions. _arXiv preprint arXiv:2311.12793_, 2023.
* [49] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, Bin Li, Ping Luo, Tong Lu, Yu Qiao, and Jifeng Dai. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. _arXiv preprint arXiv:2312.14238_, 2023.
* [50] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models, 2023.

* [51] Ke Wang, Houxing Ren, Aojun Zhou, Zimu Lu, Sichun Luo, Weikang Shi, Renrui Zhang, Linqi Song, Mingjie Zhan, and Hongsheng Li. Mathcoder: Seamless code integration in l1ms for enhanced mathematical reasoning. In _The Twelfth International Conference on Learning Representations_, 2024.
* [52] Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou, Chongyang Tao, Xiubo Geng, Qingwei Lin, Shifeng Chen, and Dongmei Zhang. Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct, 2023.
* [53] Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. Mammoth: Building math generalist models through hybrid instruction tuning, 2023.
* [54] Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei Chang, Peng Gao, and Hongsheng Li. Mathverse: Does your multi-modal l1m truly see the diagrams in visual math problems?, 2024.
* [55] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao, Dahua Lin, and Feng Zhao. Are we on the right way for evaluating large vision-language models?, 2024.
* [56] Pan Lu, Liang Qiu, Jiaqi Chen, Tony Xia, Yizhou Zhao, Wei Zhang, Zhou Yu, Xiaodan Liang, and Song-Chun Zhu. Iconqa: A new benchmark for abstract diagram understanding and visual language reasoning. _ArXiv_, abs/2110.13214, 2021.
* [57] Adam Dahlgren Lindstrom and Savitha Sam Abraham. Clevr-math: A dataset for compositional language, visual and mathematical reasoning. _ArXiv_, abs/2208.05358, 2022.
* [58] Samira Ebrahimi Kahou, Adam Atkinson, Vincent Michalski, Akos Kadar, Adam Trischler, and Yoshua Bengio. Figureqa: An annotated figure dataset for visual reasoning. _ArXiv_, abs/1710.07300, 2017.
* [59] Zhuowan Li, Xingrui Wang, Elias Stengel-Eskin, Adam Kortylewski, Wufei Ma, Benjamin Van Durme, Alan Yuille Johns Hopkins University, University of Southern California, Max Planck Institute for Informatics, and University of Freiburg. Super-clevr: A virtual benchmark to diagnose domain robustness in visual reasoning. _2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 14963-14973, 2022.
* [60] Sanket Shah, Anand Mishra, Naganand Yadati, and Partha Pratim Talukdar. Kvqa: Knowledge-aware visual question answering. In _AAAI Conference on Artificial Intelligence_, 2019.
* [61] Nitesh Methani, Pritha Ganguly, Mitesh M. Khapra, and Pratyush Kumar. Plotqa: Reasoning over scientific plots. _2020 IEEE Winter Conference on Applications of Computer Vision (WACV)_, pages 1516-1525, 2019.

## Checklist

1. For all authors... 1. Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? [Yes] 2. Did you describe the limitations of your work? [Yes] See Section 5. 3. Did you discuss any potential negative societal impacts of your work? [Yes] See Appendix A. 4. Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes]
2. If you are including theoretical results... 1. Did you state the full set of assumptions of all theoretical results? [N/A] 2. Did you include complete proofs of all theoretical results? [N/A]
3. If you ran experiments (e.g. for benchmarks)... 1. Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? The dataset is released at https://huggingface.co/datasets/MathLLMs/MathVision. Code, data, and instructions needed can be found on the project page 2. Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? We did not train any models in this study. 3. Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? [No] 4. Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes] See Appendix E.
4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets... 1. If your work uses existing assets, did you cite the creators? [Yes] 2. Did you mention the license of the assets? [Yes] See Appendix A. 3. Did you include any new assets either in the supplemental material or as a URL? [Yes] 4. Did you discuss whether and how consent was obtained from people whose data you're using/curating? [Yes] See Appendix A. 5. Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [Yes] See Section 3.2.
5. If you used crowdsourcing or conducted research with human subjects... 1. Did you include the full text of instructions given to participants and screenshots, if applicable? [Yes] See Appendix K. 2. Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [No] Participants are only asked to perform data cleaning on publicly available mathematics test questions or solve these questions. There are no foreseeable risks to participants. 3. Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [Yes] See Appendix K.

[MISSING_PAGE_FAIL:15]

Data Contamination

We have also considered the issue of data contamination:

Firstly, we primarily collect questions from the official websites of various competitions. Each file typically contains 15 to 30 questions and is kept separate from the answers or solutions. Given the concerns surrounding data contamination in large language models (LLMs), we prioritize selecting questions that do not have readily available answers, such as those found on separate documents or pages.

Secondly, the three main series of closed-source models, GPT-4, Gemini, and Claude, all evaluate their models using several contests in their technical reports, including the AMC, which is a primary source of our problems. Therefore, we assume that these models adhere to the common practice of not using leaked data for training. However, as these are closed-source models, this assumption is our best estimation.

Thirdly, among open-source models, the most widely used base model is LLaVA, which utilizes a subset of the CC-3M dataset for pretraining and LAION/CC/SBU BLIP captions for instruction tuning. We compared our dataset with the training dataset and found no overlap between the two.

## Appendix D Testmini

In this section, we show the effectiveness of the testmini subset in mirroring the performance trends observed in the full test set across a diverse range of mathematical subjects. The detailed results are presented in Table 5. We find that the model performances, when evaluated on both the entire set and testmini, exhibit close parallelism in trend. For instance, the performance delta between the full test set and testmini for the Qwen-VL-Plus model is a mere 0.19%, indicating a high degree of similarity in model behavior across the two datasets. Similarly, the GPT4V model also exhibits a negligible performance difference of -0.39%, further corroborating the testmini subset's capability to accurately reflect the full test set's characteristics. Furthermore, even for LLaVA-v1.5-13B and Share-GPT4V-7B, two models with the largest performance variations, where differences of 1.98% and 1.97% are observed respectively, the deviation remains within 2%, underscoring the testmini subset's reliability for performance evaluation. These findings suggest that models' performance on the testmini subset should be indicative of their performance on the full test set, thereby providing a reliable means for preliminary evaluation and comparison of models across various mathematical subjects.

## Appendix E Evaluation Details

We conduct all experiments on A100 80G GPUs.

### Prompts for Response Generation

In this section, we present details of the prompts used for the evaluation of response generation models. The prompts are designed to ensure that the models generate responses that are consistent with the input question and the image. The prompts used for the evaluation of response generation models are shown in Table 6.

### Prompt for Caption Generation

The prompt for caption generation is as follows: _Here is a diagram of a math problem, please describe the diagram in complete detail so that your description can be used to replace the diagram for math problem-solving._

### Models

We present the source of the models used in the evaluation in Table 7. The models are sourced from OpenAI, Google, Alibaba, and Hugging Face. The models from OpenAI, Google and Alibaba are accessed from their respective platforms through API calls.

## Appendix F More Results on Text-only LLMs and LMMs.

In Table 2, we augment the text-only LLM like ChatGPT-3.5 [15] and GPT-4 [1] with image captions generated by GPT-4V [6]. Considering current text-only math LLMs like MathCoder [51], WizardMath [52], and Mammoth [53], we further evaluate three 7B models on MATH-V using the same method applied to ChatGPT-3.5 and GPT-4, as shown in Table 8. We can find that MAMmOTH-7B-Mistral based on the current 7B base model with the strongest reasoning ability reached 12.4%, even exceeding LLaVA-v1.5-13B (11.12%) and Qwen-VL-Plus (10.72%). This not only shows that the reasoning ability of existing multi-modal models is weak, but also shows that our benchmark has higher requirements for reasoning capabilities.

As analyzed in [54, 55], the visual content is unnecessary for many questions in benchmarks like MathVista, MMMU, ScienceQA, etc. In Section 4.2, we have demonstrated that the low performance of text-only models like GPT-4 on MATH-V confirms the dependency of our questions on the associated images. To further validate the visual dependency of MATH-V, we evaluate some LMMs with vanilla (text and image) and text-only configurations. As depicted in Table 9, once the LMMs lose the image input, there is a decline in their performance, with a notable drop of 8.55% on InterVL-Chat-V1-2

\begin{table}
\begin{tabular}{l|c c c c c c c c c c c c c c c c} \hline Model & Overall & All & Auto & Ani & Cnorm & Comb & Cut & Dec & Graph & Log & Auto & Loss & IoM & SdG & Size & Time & Time \\ \hline Random Camera & 5.86 & 2.0 & 11.9 & 1.3 & 5.0 & 1.0 & 1.0 & 5.0 & 5.0 & 5.0 & 5.0 & 5.0 & 7.0 & 7.5 & 5.5 & 5.0 & 8.3 \\ Adam & 5.76 & 2.0 & 11.9 & 4.5 & 4.4 & 4.0 & 5.0 & 5.0 & 5.0 & 5.0 & 5.0 & 5.0 & 5.0 & 5.0 & 10.5 & 0.5 & 5.0 & 5.0 & 5.0 \\ \hline \(\Delta\) & +1.71 & 5.33 & **+19.7** & +6.2 & **+6.4** & **-3.0** & **+4.4** & **-4.0** & **+4.2** & **+6.6** & **+4.7** & +1.7 & **+3.9** & **+4.8** & -3.5 & 3.4 & -3.1 \\ \hline \multicolumn{11}{c}{LLMs (City-only & **+2.0** & **+3.0** & **+3.0** & **+3.0** & **+3.0** & **+3.0** & **+3.0** & **+3.0** & **+3.0** & **+3.0** & **+3.0** & **+3.0** & **+3.0** & **+3.0** & **+3.0** \\ \hline GPT-4V (City-only & 8.16 & 12.8 & 10.7 & 15.7 & 4.9 & 10.7 & 10.5 & 1.9 & 5.6 & 8.4 & 8.1 & 6.2 & 8.7 & 8.6 & 3.5 & 4.4 & 4.8 \\ Training & 6.58 & 5.3 & 10.5 & 15.8 & 0.0 & 2.1 & 1.0 & 5.3 & 5.0 & 5.0 & 5.0 & 5.0 & 3.5 & 0.3 & 5.3 & 5.3 & 5.3 & 5.0 \\ \(\Delta\) & -1.88 & -7.6 & -6.2 & -4.0 & -4.9 & -4.4 & 0.0 & -4.4 & -5.4 & -3.4 & -4.6 & -3.1 & **+2.4** & **+0.7** & -3.3 & **+1.8** & **+0.9** & **+4.8** \\ \hline \multicolumn{11}{c}{LMs (Turk-v1.5-13C)} \\ \hline ChatGPT-3.5 (City-City) & 9.74 & 10.7 & 20.0 & 18.6 & 10.1 & 17.7 & 19.6 & 16.0 & 10.0 & 15.6 & 6.4 & 5.6 & 9.4 & 12.1 & 4.0 & 14.9 & -1.0 & 14.0 \\ Training & 10.20 & 10.20 & 15.2 & 5.3 & 5.0 & 10.5 & 2.1 & 15.8 & 10.0 & 10.5 & 6.0 & 5.2 & 21.1 & 5.3 & 10.5 & 5.3 \\ \(\Delta\) & **+0.46** & -4.2 & **+6.3** & -13.3 & -16.1 & **+2.8** & **+3.2** & **+4.6** & **+4.5** & **-3.5** & **+4.3** & **+5.8** & -1.2 & **+4.11** & **+7.6** & **+4.6** & **+5.4** \\ \hline \multicolumn{11}{c}{LMs (Turk-v1.5-13C)} \\ \hline ChatGPT-4.5V (City-City) & 13.0 & 16.5 & 20.2 & 84.3 & 10.9 & 17.9 & 19.4 & 7.7 & 11.1 & 10.1 & 9.8 & 9.6 & 9.1 & 13.5 & 13.8 & 8.7 & 12.3 \\ Training & 12.50 & 15.8 & 10.6 & 3.5 & 15.8 & 18.4 & 10.6 & 15.5 & 18.5 & 18.0 & 5.3 & 5.3 & 0.0 & 21.1 & **+0.5** & 5.3 \\ \(\Delta\) & -4.6 & -4.7 & 9.7 & -2.7 & -5.1 & -4.1 & **+4.2** & **+4.8** & **+4.7** & +5.7** & -9.8 & -4.3 & -3.8 & -13.5 & **+7.3** & **+1.8** & **+5.2** \\ \hline \multicolumn{11}{c}{LLAVA-v1.5-7B} \\ \hline ChatGPT-4.5V (City-City) & 8.52 & 7.0 & 7.1 & 10.7 & 7.1 & 1.3 & 10.5 & 17.0 & 9.0 & 2.5 & 15.1 & 10.2 & 9.8 & 5.3 & 8.6 & 4.4 & 4.8 \\ Training & 10.20 & 0.00 & 15.5 & 35.8 & 5.3 & 5.3 & 15.8 & 10.5 & 10.5 & 15.8 & 12.1 & 15.8 & 15.8 & 5.3 & 10.5 & 5.3 \\ \(\Delta\) & **+1.48** & -7.0 & **+3.4** & **+5.1** & -4.8 & **+5.3** & **+2.8** & **+4.6** & **+5.5** & **+5.6** & **+6.0** & **+0.0** & **+1.9** & **+4.4** & **+0.5** \\ \hline \multicolumn{11}{c}{SPHENN (V2)} \\ \hline SPHENN (V2) & 9.70 & 6.7 & 7.1 & 12.9 & 7.5 & 7.7 & 6.0 & 9.6 & 16.7 & 10.1 & 11.9 & 11.8 & 11.8 & 11.5 & 8.2 & 8.6 & 3.7 & 6.0 \\ Training & 9.21 & 5.4 & 10.5 & 0.4 & 0.5 & 0.3 & 21.3 & 10.5 & 10.5 & 8.8 & 10.5 & 15.8 & 5.5 & 0.2 & 5.8 & 5.3 & 3.0 \\ \(\Delta\) & -4.0 & -4.4 & -4.4 & -2.4 & -7.8 & **+3.4** & -6.5 & -4.9 & -4.0 & **+4.4** & -4.5 & -4.2 & -6.2 & 8.2 & 8.3 & 5.3 & **+4.3** \\ \hline \multicolumn{11}{c}{SharedPIFAT-V-1B} \\ \hline Tutorial & 12.50 & 5.5 & 3.6 & 12.9 & 11.0 & 4.8 & 7.5 & 11.4 & 10.4 & 10.9 & 12.6 & 11.8 & 12.3 & 9.8 & 15.5 & 17.4 & 11.3 \\ \(\Delta\) & 12.50 & 5.3 & 10.0 & 12.1 & 5.3 & 5.3 & 26.3 & 15.8 & 15.8 & 15.8 & 10.5 & 21.1 & 15.8 & 13.5 & 10.5 & 5.3 \\ \(\Delta\) & **+1.97** & **-3.2** & -3.6 & -4.1 & **+4.6** & -4.2 & -4.8 & -4.4 & -4.4 & -4.0 & -4.8 & -4.0 & -4.8 & -4.0 & -4.9 & **+0

[MISSING_PAGE_FAIL:18]

\begin{table}
\begin{tabular}{l|l|l} \hline \hline
**Model** & **Source** & **URL** \\ \hline ChatGPT 3.5 & gpt-3.5-turbo-0125 & https://platform.openai.com/ \\ \hline GPT4 & gpt-4-0125-preview & https://platform.openai.com/ \\ \hline GPT4V & gpt-4-1106-vision-preview & https://platform.openai.com/ \\ \hline GPT-4o & gpt-4o-2024-05-13 & https://platform.openai.com/ \\ \hline GPT-4-turbo & gpt-4-turbo-2024-04-09 & https://platform.openai.com/ \\ \hline Gemini Pro & gemini-pro-vision & https://ai.google.dev/ \\ \hline Gemini-v1.5 Pro & gemini-1.5-pro & https://ai.google.dev/ \\ \hline \multirow{2}{*}{InternLM-XComposer2-VL} & local checkpoint & https://huggingface.co/internlm/ \\  & & & \\  & & & \\ \hline InternVL-Chat-v1.2-Plus & local checkpoint & https://huggingface.co/OpenGVLab/ \\  & & & \\  & & & \\ \hline Qwen-VL-Max & qwen-vl-max & https://help.aliyun.com/zh/ \\  & & & & \\  & & & \\  & & & \\  & & & \\ \hline Qwen-VL-Plus & qwen-vl-plus & https://help.aliyun.com/zh/ \\  & & & & \\  & & & \\  & & & \\  & & & \\ \hline SPHINX-MoE & local checkpoint & https://huggingface.co/ \\  & & & & \\  & & & \\  & & & & \\  & & & \\ \hline SPHINX (V2) & local checkpoint & https://huggingface.co/ \\  & & & & \\  & & & & \\  & & & & \\  & & & & \\  & & & & \\ \hline LLaVa-v1.5-7B & local checkpoint & https://huggingface.co/ \\  & & & & & \\  & & & & \\ \hline LLaVa-v1.5-13B & local checkpoint & https://huggingface.co/ \\  & & & & & \\ \hline ShareGPT4V-7B & local checkpoint & https://huggingface.co/ \\  & & & & & \\  & & & & \\ \hline ShareGPT4V-13B & local checkpoint & https://huggingface.co/ \\  & & & & & \\ \hline \hline \end{tabular}
\end{table}
Table 7: The source of the models used in the evaluation.

\begin{table}
\begin{tabular}{l|c|c} \hline \hline Model & Overall & Alg & AnoG & AnnComG & Comb & Cat & DesG & GgrpT & Leg & Angle & Area & Len & SeqG & Stat & Topo & TransG \\ \hline ChatGPT-3.5 (CoT) & 9.74 & 10.7 & 20.0 & 18.6 & 10.1 & 7.7 & 17.9 & 16.4 & 10.0 & 13.5 & 6.4 & 5.8 & 6.5 & 9.4 & 12.1 & 4.4 & 10.7 \\ GPT-4 (CoT) & 13.10 & 16.5 & 20.2 & 34.3 & 10.4 & 17.9 & 19.4 & 7.7 & 11.1 & 10.1 & 9.8 & 9.6 & 9.1 & 13.5 & 13.8 & 8.7 & 12.5 \\ \hline MathCode-7B & 6.6 & 4.9 & 15.5 & 12.1 & 9.1 & 6.0 & 7.5 & 11.5 & 5.6 & 7.6 & 2.3 & 0.0 & 4.9 & 5.3 & 8.6 & 13.0 & 11.3 \\ WizardM-B & 4.5 & 3.5 & 7.1 & 5.0 & 8.4 & 6.0 & 10.5 & 4.4 & 5.0 & 1.7 & 3.6 & 2.7 & 4.5 & 3.5 & 13.0 & 5.4 \\ MsumMT-B-MiMiMi & 12.4 & 11.6 & 16.7 & 15.7 & 11.4 & 10.1 & 11.9 & 14.4 & 14.4 & 8.4 & 8.7 & 12.8 & 11.4 & 11.5 & 19.0 & 21.7 & 16.6 \\ \hline \hline \end{tabular}
\end{table}
Table 8: Performance of some text-only LLMs augmented with image captions on the entire dataset.

[MISSING_PAGE_FAIL:20]

\begin{table}
\begin{tabular}{l|c|l} \hline \hline
**Template** & **Proportion** & **Examples** \\ \hline \multicolumn{3}{c}{**IconQA[56]**} \\ \hline Object Counting & 106/400 & What number is shown? \\ Perimeter Calculation & 51/400 & What is the perimeter of the square? \\ Length Measurement with a given reference & 150/400 & Move the ruler to measure the length of the line to the nearest centimeter. The line is about \_ centimeters long. \\ Time Inquiry based on a clock & 67/400 & What time is shown? \\ \hline \multicolumn{3}{c}{**CLEVR-Math[57]**} \\ \hline Subtract... How many objects are left? & 400/400 & Subtract all metallic cylinders. Subtract all gray things. How many objects are left? \\ \hline \multicolumn{3}{c}{**FigureQA[58]**} \\ \hline Is X the...? & 176/400 & Is Khaki the minimum/maximum/ low median/high median/smoothest/ roughest? \\ Does X have the...? & 86/400 & Does Brown have the maximum/ minimum area under the curve? (the highest/lowest value?) \\ Is X... Y? & 117/400 & Is Hot Pink less/greater than Black? \\ Does X intersect Y? & 21/400 & Does Slate intersect Light Salmon? \\ \hline \multicolumn{3}{c}{**Super-CLEVEL[59]**} \\ \hline Are there... X than Y? & 146/400 & Are there fewer/more tiny gray cars than small red sedans? \\ Is the number of X... than Y? & 232/400 & Is the number of regular buss less/greater than the number of brown rubber mini-vans? \\ Others & 22/400 & How many cyan double buss have the same size as the matte aeroplane? \\ \hline \multicolumn{3}{c}{**KVQA[60]**} \\ \hline What is the age gap between the...? & 310/382 & What is the age gap between these two people in image? \\ How many people in the image were born...? & 72/382 & How many people in the image were born after the end of World War II? \\ \hline \multicolumn{3}{c}{**PlotQA[61]**} \\ \hline In how many years, is the X greater than Y? & 57/105 & In how many years, is the population in the largest city greater than 26 \%? \\ What is the difference between...? & 31/105 & What is the difference between two consecutive major ticks on the Y-axis? \\ Others & 17/105 & What is the average rating of statistical capacity in Bolivia per year? \\ \hline \hline \end{tabular}
\end{table}
Table 10: Some source datasets of MathVista featuring similar questions. The table shows the template, proportion within the whole dataset and typical examples of them.

Figure 7: Some images with the question What is the age gap between these two people in the image? from MathVista. Questions containing What is the age gap between appear 310 times.

Figure 8: Problem involving very similar geometry recognition from MathVista. Questions like "Subtract all... objects. How many objects are left?" appear 400 times.

Figure 10: Typical examples of 400 questions with a function plot of MathVista. Each image has approximately seven questions about function symmetry, monotonicity, roots, periodicity, values, etc.

Figure 9: Three main types of questions among the 375 questions with an abstract scene in MathVista: Object Counting, Length Measurement, and Time Inquiry. Among the 200 sampled questions, the percentage of each category is in the figure. Text marked in red indicates that it belongs to a fixed template and is used in other similar questions.

## Appendix H More Dataset Details

### Question Distribution

All questions in MATH-V are written in English. As illustrated in Appendix B, the average number of words in a question is 42.3, with a maximum of 280 words and a minimum of 5 words. Figure 17 further elucidates the distribution of word counts, highlighting the diverse patterns of questions. We can see that the median word count and the average word count are close, indicating that the distribution is not skewed. The distribution of word counts is near a normal distribution, with more than 98.75% of questions having a word count of less than 100.

### Division of Levels

In this section, we detail the division of MATH-V's difficulty levels. Our difficulty classification is divided into two steps. The first step involves roughly dividing the difficulty levels based on the grades of the contestants as specified in the competition. The second step entails manually checking and correcting any unreasonable divisions.

**Step 1.** The existing competition's grade requirements span grades 1 to 12. Because the questions for grades 1 and 2 are considered too simple, we classify them separately as level 1. The questions for grades 3, 4, 5, and 6 are similar in difficulty, so we classify them as level 2. Grades 7 and 8 are categorized as level 3, grades 9 and 10 as level 4, and grades 11 and above as level 5.

**Step 2.** We manually review the questions in each level and adjust the classification according to the actual difficulty of the questions. This step is necessary because, within the same exam, the difficulty range of the questions can be quite broad. For instance, the most difficult question in grade 4 might be more challenging than the easiest question in grade 5. In such cases, we make manual adjustments.

Figure 18.a shows a comparison of the level distribution between our MATH-V and the MATH dataset. We observe that both datasets have the fewest number of questions in level 1 and a similar number of

Figure 11: Three examples from MATH-V on analytic geometry, which are about mutual symmetry of functions, quadratic function discriminant, and roots of iterative functions. We omit some choices for brevity.

**Question:** A piece of string is lying on the table. It is partially covered by three coins as seen in the figure. Under each coin the string is equally likely to pass over itself like this: <image1> or like this: <image2>. What is the probability that the string is knotted after its ends are pulled?

**Image:**

**Question:** The "Borromaic Rings" have an extraordinary property. Although no two are interlocked, they are strongly connected within each other. If one ring is cut through, the other two fall apart. Which of the following diagrams shows the picture of "Borromaic Rings"?

Figure 12: Three examples from MATHV on topology. We omit some choices for brevity.

\(\triangleright\)_Adjacent Coloring Puzzle_

**Image:**

\begin{tabular}{|l|l|l|l|l|} \hline A & B & & & \\ \hline C & D & & & \\ \hline  & & B & & \\ \hline B & & & & \\ \hline \end{tabular}

**Question:** In the diagram we want to colour the fields with the colours A, B, C D so that adjacent fields are always in different colours. (Even fields that share only one corner, count as adjacent.) Some fields have already been coloured in. In which colour can the grey field be coloured in?

Figure 14: Three examples from the Graph Theory subfield in the Math subset of MMMU.

Figure 13: Three examples from MATH-V on graph theory. We omit some choices for brevity.

Figure 16: Three examples from the Operation Research subfield in the Math subset of MMMU.

Figure 15: Three examples from the Group Theory subfield in the Math subset of MMMU.

questions in the other levels. Figure 18.b illustrates the models' accuracy across different levels. It is evident that the accuracy of the models decreases as the difficulty level increases, which aligns with the expectation that the difficulty of the questions escalates with the level.

### Images by Subjects

In this section, we demonstrate some images from the problems in our dataset. Figure 20, Figure 21, Figure 22, Figure 23, Figure 24, Figure 25, Figure 26, Figure 27, Figure 28, Figure 29, Figure 30, Figure 31, Figure 32, Figure 33, Figure 34 and Figure 35 show images from the problems under the category of Algebra, Analytic Geometry, Arithmetic, Combinatorial Geometry, Combinatorics, Counting, Descriptive Geometry, Graph Theory, Logic, Metric Geometry - Angle, Metric Geometry -

Figure 17: The distribution of the number of words per question in MATH-V.

Figure 18: (a) Comparison of the level distribution between our MATH-V and the MATH dataset [33]. (b) Accuracy on MATH-V on six typical LMMs across 5 difficulty levels.

Area, Metric Geometry - Length, Solid Geometry, Statistics, Topology and Transformation Geometry, respectively.

We can see that the images are diverse among different categories, and the images in the same category have similar visual patterns. For example, the images in the category of Statistics are mainly various types of charts and tables, while the images in the category of Arithmetic and Counting, which are categories mainly for primary school students, are more colorful and have more cartoon characters or real-world objects, and, conversely, the images in the category of Analytic Geometry are more complex and abstract.

### Introduction of Subjects

**Algebra.** Algebra is a branch of mathematics that deals with symbols and the rules for manipulating these symbols. It is a unifying thread of almost all of mathematics and includes everything from solving simple equations to studying abstractions such as groups, rings, and fields. Algebra allows us to express relationships and solve problems where we don't initially know the values of certain quantities. It serves as the foundation for more advanced topics in mathematics and sciences, enabling the formulation and solving of equations and inequalities.

**Analytic Geometry.** Analytic Geometry, also known as coordinate geometry, merges algebra and geometry to discuss shapes and figures using a coordinate system. This branch of mathematics allows the representation of geometric shapes in a numerical way by translating them into equations and vice versa. It enables the study of geometric properties and relationships by means of algebraic equations, thus providing a powerful tool for solving geometric problems.

**Arithmetic.** Arithmetic is the most basic branch of mathematics, dealing with the study of numbers and the traditional operations on them: addition, subtraction, multiplication, and division. It forms the foundation upon which more advanced mathematical concepts are built. Arithmetic is used in everyday life for counting, measuring, and performing calculations. It is the first type of math that students are introduced to and remains fundamental for understanding more complex areas of mathematics.

**Combinatorial Geometry.** Combinatorial Geometry is a field of mathematics that studies geometric objects and their arrangement, with a particular focus on discrete and combinatorial aspects. It involves the investigation of geometric structures and their properties under various combinatorial processes, such as counting, arrangement, and partitioning. This branch often intersects with areas like graph theory and combinatorics, exploring problems related to the configuration of points, lines, and other geometric figures.

**Combinatorics.** Combinatorics is a branch of mathematics concerned with counting, arrangement, and combination of objects. It deals with questions of how many different ways such objects can be chosen, arranged, and combined under certain conditions. Combinatorics has applications in numerous areas, including computer science, optimization, and probability theory. It is fundamental in developing analytical skills and understanding complex mathematical concepts.

**Counting.** Counting is the most basic and fundamental concept in mathematics, involving the enumeration of objects in a set. It serves as the foundation for more advanced mathematical concepts, such as arithmetic and combinatorics. Counting is not only about determining the total number of items in a collection but also includes understanding and applying principles like one-to-one correspondence, the counting principle, permutations, and combinations.

**Descriptive Geometry.** Descriptive Geometry is the branch of geometry that allows the representation of three-dimensional objects in two dimensions, through the use of projections or perspective drawings. It is a method of visually communicating complex geometric shapes and is fundamental in fields such as engineering, architecture, and design. Descriptive geometry provides tools and techniques for solving spatial problems and for understanding the spatial relationships between objects.

**Graph Theory.** Graph Theory is a branch of mathematics that studies graphs, which are mathematical structures used to model pairwise relations between objects. A graph is made up of vertices (or nodes) and edges (or arcs) that connect them. Graph theory is used to study networks and their properties, such as paths, cycles, and connectivity, and has applications in computer science, biology, transportation, social sciences, and more.

**Logic.** Logic is the foundation of mathematical reasoning, focusing on the study of argument validity, structure, and form. It involves the analysis of propositions and their relationships to determine the truth values of statements. Logic is used to construct and analyze mathematical proofs, ensuring that conclusions follow logically from premises. It encompasses various systems, such as propositional logic, predicate logic, and modal logic, each with its own rules and applications.

**Metric Geometry - Angle.** Metric Geometry concerning angles focuses on the properties and measurements of angles within various geometric figures. It involves understanding the types of angles (acute, right, obtuse, and straight), their relationships, and how they can be measured and used to determine other geometric properties. This aspect of metric geometry is crucial in solving problems related to shapes, patterns, and designs in both two and three dimensions.

**Metric Geometry - Area.** Metric Geometry in the context of area deals with the measurement of the surface enclosed within geometric figures. It involves calculating the extent of two-dimensional shapes, such as squares, rectangles, triangles, and circles, using various formulas. Understanding area is essential in numerous practical applications, including architecture, land surveying, and crafting, where the determination of space is necessary.

**Metric Geometry - Length.** Metric Geometry concerning length involves the measurement of distance between points in geometric figures. It is the foundation for determining the perimeter of shapes, the distance around a figure, and the concept of metric spaces in more advanced mathematics. Length measurements are fundamental in various real-world applications, such as construction, fabrication, and navigation.

**Solid Geometry.** Solid Geometry is the branch of mathematics that deals with the study of three-dimensional figures, including spheres, cubes, cylinders, and pyramids, among others. It involves understanding the properties, measurements, and relationships of these figures, such as volume, surface area, and the angles between intersecting planes. Solid geometry is essential in fields such as engineering, architecture, and physics, where three-dimensional models are crucial.

**Statistics.** Statistics is the branch of mathematics that deals with collecting, analyzing, interpreting, and presenting data. It involves the use of theories of probability to make inferences about populations from samples. Statistics is applied in a wide range of disciplines, including economics, social sciences, biology, and engineering, to make decisions or predictions based on data analysis.

**Topology.** Topology is a branch of mathematics concerned with the properties of space that are preserved under continuous transformations, such as stretching and bending, but not tearing or gluing. It is often referred to as "rubber-sheet geometry" because of its focus on the intrinsic properties of spaces. Topology has applications in various areas, including quantum physics, computer science, and biology, and deals with concepts like continuity, compactness, and connectedness.

**Transformation Geometry.** Transformation Geometry involves the study of geometric figures and the changes they undergo through transformations such as translation, rotation, reflection, and dilation. It focuses on understanding how these transformations affect the properties and relations of shapes and figures. Transformation geometry is fundamental in understanding the symmetry, congruence, and similarity of geometric objects, and it has applications in areas such as computer graphics, robotics, and pattern recognition.

## I Error Examples of GPT-4V

In this section, we present detailed examples of errors made by GPT-4V. We categorize these errors into six types: reasoning error, vision recognition error, knowledge error, question misunderstood error, refusal to answer, and calculation error.

### Reasoning Error (42.2%)

Reasoning errors occur when GPT-4V fails to apply logical steps correctly. In Figure 36, the model incorrectly substitutes an unknown variable with a known value, leading to an incorrect conclusion. Figure 37 shows the model making an illogical hypothesis about the relationships of train wagon numbers.

### Vision Recognition Error (31.9%)

Vision recognition errors highlight the model's struggles with interpreting visual data. Figure 38 depicts an instance where GPT-4V fails to read information from a simple bar chart. In Figure 39, the error occurs because the model mistakenly believes that 3, 6, and y are on the same side of the triangle. Figure 40 shows the model experiencing severe hallucinations, as there is no balance in the image at all.

### Knowledge Error (15.1%)

Knowledge errors are instances where GPT-4V lacks the necessary information to answer a question and provides information that is outdated or incorrect. Figure 41 and Figure 42 both showcase situations where the model did not know how to solve the problems.

### Question Misunderstood Error (6.9%)

These errors occur when GPT-4V misinterprets the user's query. Figure 43 demonstrates a failure to grasp the query's intent, resulting in an incorrect answer. Similarly, in Figure 44, the model mistakenly believes that the key below corresponds to the lock above, even though the question does not provide this information.

### Reject to Answer (2.6%)

Sometimes, GPT-4V refuses to provide an answer, which is categorized as a Reject to answer error. Figure 45 and Figure 46 illustrate instances where the model either cannot generate a response or chooses not to, citing various reasons.

### Calculation Error (1.3%)

Calculation errors are straightforward mistakes in numerical or symbolic calculations. The model only makes errors when performing symbolic calculations in our analysis. Both Figure 47 and Figure 48 show a simple symbolic computational mistake, undermining the model's reliability in symbolic calculations.

## Appendix J Examples of GPT-4 with Image Captions

In this section, we provide some qualitative examples of GPT-4 with image captions, in comparison to other models like GPT-4V, Gemini Pro, and Qwen-VL-Max.

### Correct Examples

Some typical correct examples ( Figure 49, Figure 50, Figure 51, Figure 52, Figure 53, Figure 54, Figure 55) of GPT-4 with image captions on subjects like algebra, arithmetic, analytic geometry, counting, and combinatorics. Image captions are either accurate or have some small hallucinations.

### Wrong Examples

Some typical wrong examples (Figure 56, Figure 57, Figure 58, Figure 59, Figure 60, Figure 61) of GPT-4 with image captions on other subjects. Image captions are usually inaccurate to describe the image.

## Appendix K Details of Human Annotators

We hire human annotators for three tasks: 1. Verify the alignment of text questions and images; 2. Categorize the problems into 16 different subjects; 3. Evaluate human performance on MATH-V.

### Alignment of Text Questions and Images

We hire ten annotators in stage one, and all of them are senior college students, all majoring in science-related disciplines. We pay them $0.35 per question, and they were asked to judge whether the text of the question matched the corresponding image and whether the image was complete. When the image is incomplete or incorrectly positioned, they are asked to re-capture the image from the original file.

### Subjects Classification

For task two, we have 10 annotators and three authors, to categorize the problems into 16 different subjects independently in three groups with our specially designed annotation tool, as illustrated in Figure 19. We ask them to read our definitions of the 16 subjects in advance as shown in Appendix H.4, and will randomly check their classification of certain topics. If there are mistakes in classification, we will promptly correct them and inform them. We pay $0.45 per question. Notably, we also utilize existing LMMs (such as Gemini and GPT-4V) for subject classification. Detailed instructions to annotators and prompts to models can be found in Table 11. Finally, we will combine the classification results of the three groups with the classification results returned by LMMs to give the final classification.

### _Instructions for annotators on how to label questions into 16 categories_

We are constructing a dataset about mathematical problems in visual contexts, and we have collected quantities of questions that require categorization into different mathematical subjects.

These categorizations are logic, algebra, counting, arithmetic, combinatorics, graph theory, topology, statistics, combinatorial geometry, solid geometry, metric geometry - angle, metric geometry - area, metric geometry - length, analytic geometry, descriptive geometry, and transformation geometry. Here are the detailed definitions of them:... (definitions in Appendix H.4)

If you are unsure about how to categorize the problem into these subjects, choose 'pending' instead.

### Human Performance

To obtain average human performance on our benchmark, we recruit 100 annotators possessing a high school degree or higher. These annotators had not previously participated in the competitions where our data originated and were seeing the questions in our test set for the first time. We divided the 3040 questions of MATH-V into 100 subsets, each containing 30 or 31 questions. Subsequently, we randomly allocated two subsets from this pool to each student. Participants were tasked with completing both subsets within a 70-minute time frame, spread across two separate days. Note that this time limit closely aligns with that of the Kangaroo competition, where participants are assigned a similar duration for a test containing 30 problems. Each participant receives a one-time payment of $40 as compensation for their contribution.

\begin{table}
\begin{tabular}{|p{284.5pt}|} \hline \hline \(\triangleright\)_The prompt for models to get subject advice_ \\ What branch of mathematics does the problem belong to? Choose from the following: logic, algebra, counting, arithmetic, combinatorics, graph theory, topology, statistics, combinatorial geometry, solid geometry, metric geometry - angle, metric geometry - area, metric geometry - length, analytic geometry, descriptive geometry, transformation geometry, pending. \\ You are not supposed to solve the problem. Wrap your final answer, a word or a short phrase, in \(\square\). \\ \hline \hline \end{tabular}
\end{table}
Table 11: Instructions for annotators on how to label questions into 16 categories and the prompt for models to get subject advice.

Figure 19: GUI for categorizing the problems into 16 different subjects.

Figure 20: Some images from Algebra.

Figure 21: Some images from Analytic Geometry.

Figure 22: Some images from Arithmetic.

Figure 23: Some images from Combinatorial Geometry.

Figure 24: Some images from Combinatorics.

Figure 25: Some images from Counting.

Figure 26: Some images from Descriptive Geometry.

Figure 27: Some images from Graph Theory.

Figure 28: Some images from Logic.

Figure 29: Some images from Metric Geometry - Angle.

Figure 30: Some images from Metric Geometry - Area.

Figure 31: Some images from Metric Geometry - Length.

Figure 32: Some images from Solid Geometry.

Figure 33: Some images from Statistics.

Figure 34: Some images from Topology.

Figure 35: Some images from Transformation Geometry.

**Question:** In triangle \(ABC,AB=AC,AE=AD\) and angle \(BAD=30^{\circ}\). What is the size of angle \(CDE\)?

(A) \(10^{\circ}\)

(B) \(15^{\circ}\)

(C) \(20^{\circ}\)

(D) \(25^{\circ}\)

(E) \(30^{\circ}\)

**Ground Truth:** (B) \(15^{\circ}\)

**GPT4V Answer:**

First, let's review the given information:

- Triangle \(ABC\) is isosceles with \(AB=AC\).
- \(\angle BAD=30^{\circ}\).

Given that \(AB=AC\), we know that \(\angle ABC=\angle ACB\) because the base angles of an isosceles triangle are equal.

Since \(\angle BAD=30^{\circ}\) and \(AE=AD\), we have another isosceles triangle, \(\triangle ADE\), and \(\angle ADE=\angle AED\). Let's call this unknown angle \(\theta\).

Because \(AE=AD\), triangle \(ADE\) is isosceles, which makes \(\angle ADE=\angle AED=\theta\). Also, the sum of angles in a triangle is \(180^{\circ}\), so in \(\triangle ADE\), we have:

\[\angle DAE+\angle ADE+\angle AED=180^{\circ}\]

\[30^{\circ}+\theta+\theta=180^{\circ}\]

\[2\theta=180^{\circ}-30^{\circ}\]

\[2\theta=150^{\circ}\]

\[\theta=75^{\circ}\]

So, both \(\angle ADE\) and \(\angle AED\) are \(75^{\circ}\).

Now we move to \(\triangle CDE\). We are looking for \(\angle CDE\). The angle \(\angle CDE\) is part of \(\angle CDA\), which is an exterior angle to \(\triangle ADE\) and equals the sum of the opposite interior angles, which we have found to be \(\angle ADE+\angle AED\).

Therefore:

\[\angle CDA=\angle ADE+\angle AED\]

\[\angle CDA=75^{\circ}+75^{\circ}\]

\[\angle CDA=150^{\circ}\]

Since \(\angle CDA\) is a straight line and is part of \(\angle CDE\), the following relationship holds:

\[\angle CDA=\angle CDE+\angle ADE\]

\[150^{\circ}=\angle CDE+75^{\circ}\]

\[\angle CDE=150^{\circ}-75^{\circ}\]

\[\angle CDE=75^{\circ}\]

This is not amongst the given options, indicating that there's an error.

[Truncated Due to Length]

Figure 36: An example of reasoning error.

**Reasoning Error**

**Question:**

Two equal trains, each with 31 numbered wagons, travel in opposite directions. When the wagon number 7 of a train is side by side with the wagon number 12 of the other train, which wagon is side by side with the wagon number 11?

**Ground Truth:** 8

**GPT4V Answer:**

To solve this problem, we'll use the information that we have about the wagons that are currently side by side. We are told that wagon number 7 of one train is side by side with wagon number 12 of the other train. We can use this to establish a pattern or a relationship between the numbers of the wagons that are side by side.

**If wagon 7 is next to 12, we can infer that as one train's wagon number increases, the other train's corresponding wagon number decreases if the trains are moving in opposite directions. The difference between the wagon numbers is 5 (12 - 7 = 5).**

Now, if we want to find out which wagon would be side by side with wagon number 11, we can use the established difference to determine the opposite wagon number. We subtract 5 from 11 to find the matching wagon number on the other train:

\(11-5=6\)

Therefore, the wagon number 6 of the one train will be side by side with wagon number 11 of the other train. So, the solution is:

Figure 37: An example of reasoning error.

**Vision Recognition Error**

**Question:** The students in Mrs. Sawyer's class were asked to do a taste test of five kinds of candy. Each student chose one kind of candy. A bar graph of their preferences is shown. What percent of her class chose candy E?

**Ground Truth:** 2

**GPT4V Answer:**

To find the percentage of the class that chose candy E, we first need to determine the number of students who chose candy E by looking at the bar graph. Then, we calculate the total number of students who participated in the taste test by adding the number of students who chose each kind of candy. Finally, we can find the percentage of students who chose candy E by dividing the number of students who chose candy E by the total number of students and then multiplying by 100.

From the bar graph, the number of students who chose candy E can be counted directly:

**Candy E chosen by = 2 students**

Now let's count the total number of students who participated:

**Candy A chosen by = 7 students Candy B chosen by = 8 students Candy C chosen by = 5 students Candy D chosen by = 3 students Candy E chosen by = 2 students (as identified earlier)

Total number of students = 7 + 8 + 5 + 3 + 2 = 25 students

Now we determine the percentage of students who chose candy E:

\[\text{Percentage choosing candy E}=\left(\frac{\text{Number choosing candy E}}{\text{Total number of students}}\right)\times 100\]

\[\text{Percentage choosing candy E}=\left(\frac{2}{25}\right)\times 100\]

\[\text{Percentage choosing candy E}=\frac{2}{25}\times 100\]

\[\text{Percentage choosing candy E}=0.08\times 100\]

\[\text{Percentage choosing candy E}=8\%\]

Therefore, the percentage of the class that chose candy E is:

\[8\%\]

Figure 38: An example of vision recognition error.

**Question:** The sums of the all the three numbers on each side of the triangle are equal. Two numbers happened to be stained with ink. How much is the sum of these two numbers?

**Ground Truth:** 2

**GPT4V Answer:**

To solve this problem, we need to realize that the sum of the numbers on each side of the triangle is the same. We will call this sum \(S\).

Let's denote the two unknown numbers, which are obscured by ink, as \(x\) and \(y\).

We can write the sum on each side of the triangle using these variables. For the side with the numbers 7, \(x\), and 1, the sum is \(7+x+1\). **For the side with the numbers 3, 6, and \(y\), the sum is \(3+6+y\).**

Because the sums are equal, we have:

\[7+x+1=3+6+y\]

\[8+x=9+y\]

We want to find the sum of \(x\) and \(y\). Let's add \(y\) to both sides of the equation to move terms involving \(y\) to the left side and terms involving \(x\) to the right side:

\[y+9+y=8+x+y\]

\[2y+9=8+x+y\]

Now subtract \(y\) from both sides:

\[y+9=8+x\]

We know that the other side of the triangle is \(1+6+3=10\), so the sum of the numbers on each side must be 10, which means:

\[y+9=10\]

\[y=10-9\]

\[y=1\]

Now we substitute \(y=1\) into the sum for the other sides to find \(x\):

\[x+8=10\]

\[x=10-8\]

\[x=2\]

So, the two numbers are \(x=2\) and \(y=1\), and their sum is:

\[x+y=2+1=3\]

Thus, the sum of the two numbers that are stained with ink is 3, and so we have:

Figure 39: An example of vision recognition error.

**Vision Recognition Error**

**Question:**

Karin wants to place five bowls on a table so that they are ordered according to their weight. She has already placed the bowls \(Q,R,S\) and \(T\) in order, where \(Q\) is lightest and \(T\) is heaviest. Where does she have to place bowl Z?

(A) to the left of bowl Q

(B) between bowls Q and R

(C) between bowls R and S

(D) between bowls S and T

(E) to the right of bowl T

**Ground Truth:** (B)

**GPT4V Answer:**

The image depicts a series of bowls marked Q, R, S, T, and Z, with balance scales suggesting their comparative weights. To determine where bowl Z should be placed in the sequence, we examine the scale readings:

- Bowl Q is lighter than all other bowls since it's shown on the higher side of all the scales it's compared with.
- Bowl R is heavier than Q but lighter than the other bowls since it is on the lower side when compared with Q and on the higher side compared to S and T.
- Bowl S is heavier than Q and R but lighter than T, as displayed by being on the lower side of the scales versus Q and R, and the higher side versus T.
- Bowl T is the heaviest as it is on the lower side of every comparison.

Bowl Z is shown compared only to bowl S. Z is on the lower side, suggesting it is heavier than S. Given that there's no additional comparison provided, we cannot confirm Z's weight relative to T; however, Z must be placed between S and T, as Z is heavier than S and there is no information to assume it is heavier than T.

Therefore, the correct placement for bowl Z is between bowls S and T:

Figure 40: An example of vision recognition error. Model is experiencing severe hallucinations in this example

**Convergence**

**Question:** We see in the diagram at the right a piece of the graphic of the function

\[f(x)=ax^{3}+bx^{2}+cx+d.\]

What is the value of \(b\)?

**Ground Truth:** -2

**GPT4V Answer:**

To determine the value of \(b\) in the cubic function \(f(x)=ax^{3}+bx^{2}+cx+d\), we need to look at the properties of the function's graph provided in the diagram.

From the graph, we can identify the coordinates of the two local extrema (maximum and minimum). At these points, the first derivative \(f^{\prime}(x)\) of the function must be zero since the slope of the tangent line at these points is zero.

The first derivative of the cubic function \(f(x)\) is given by:

\[f^{\prime}(x)=3ax^{2}+2bx+c.\]

We have two extrema points visible on the graph, at \(x=-1\) and \(x=1\). Let's set up the derivative to be equal to zero at these points:

For \(x=-1\):

\[0=3a(-1)^{2}+2b(-1)+c\]

\[0=3a-2b+c\]

\[2b=3a+c\quad(1)\]

For \(x=1\):

\[0=3a(1)^{2}+2b(1)+c\]

\[0=3a+2b+c\quad(2)\]

Let's solve this system of equations (1) and (2) for \(b\).

Subtract (1) from (2):

\[3a+2b+c-(3a-2b+c)=0\]

\[3a+2b+c-3a+2b-c=0\]

\[4b=0\]

\[b=0\]

Therefore, the value of \(b\) is zero.

Figure 41: An example of knowledge error.

**Knowledge Error**

**Question:** In the accompanying figure, segments \(AB\) and \(CD\) are parallel, the measure of angle \(D\) is twice the measure of angle \(B\), and the measures of segments \(AB\) and \(CD\) are \(a\) and \(b\) respectively. Then the measure of \(AB\) is equal to

(A) \(\frac{1}{2}a+2b\)

(B) \(\frac{3}{2}b+\frac{3}{4}a\)

(C) \(2a-b\)

(D) \(4b-\frac{1}{2}a\)

(E) \(a+b\)

**Ground Truth:** -2

**GPT4V Answer:**

The image you've provided appears to be a quadrilateral with sides \(AB\) and \(CD\) denoted as being parallel and segments \(AB\) labeled as \(a\), and \(CD\) as \(b\). However, without additional information such as angle measures or the relationship between the sides and angles, it is impossible to determine a definitive relationship between \(a\) and \(b\) based on the given options (A) through (E).

Typically, in problems like this, you might apply properties of parallel lines and transversals or properties of similar triangles if any triangle relationships can be determined. But the given information is insufficient to apply these properties directly.

Given the description of the problem, it seems to be expecting the application of a geometric principle or theorem which is not evident from the image alone. It's possible that there is either additional information that has not been provided or there may be an error in the problem's depiction.

With the information currently at hand, it is not feasible to reach a solution, and I cannot provide an answer in the requested format. If you can provide further details or clarify the conditions of the problem, I would be happy to reattempt to solve it.

Figure 42: An example of knowledge error.

**Question:**

Johann stacks \(1\times 1\) cubes on the squares of a \(4\times 4\) grid. The diagram <image1> shows how many cubes were piled on top of each other on each square of the grid. What will Johann see if he looks from behind (hinten) at the tower?

**GPT4V Answer:**

To solve this problem, we need to visualize the grid from the opposite side, which means flipping the given grid horizontally to see it from behind. The numbers represent the height of the towers of cubes. Let's look at the grid and rewrite the cube heights as they would appear from behind:

The original grid from the front is:

\begin{tabular}{l l}  & 4 2 3 2 \\  & 3 3 1 2 \\  & 2 1 3 1 \\  & 1 2 1 2 \\ \end{tabular} When viewed from behind, the grid should be:

\begin{tabular}{l l}  & 2 3 2 4 \\  & 2 1 3 3 \\  & 1 3 1 2 \\  & 2 1 2 1 \\ \end{tabular} Now we need to match this to the options given:

(A) Does not match because the first column (from left to right) should start with two blocks and end with four blocks. (B) Also does not match because the second column should start with three blocks. (C) Does not match because the third column should start with two blocks. (D) Does not match because the first and second columns have incorrect distributions. (E) Matches because it has the suitable pattern when flipped horizontally:

- The first column starts with 2 and ends with 4,
- The second column starts with 3 and ends with 3,
- The third column starts with 2 and ends with 1,
- The fourth column starts with 4 and ends with 1.

Consequently, the correct answer is

Figure 43: An example of question understood error.

**Question Misunderstood Error**

**Question:** Each one of the four keys locks exactly one padlock. Every letter on a padlock stands for exactly one digit. Same letters mean same digits. Which letters must be written on the fourth padlock?

(A) GDA

(B) ADG

(C) GAD

(D) GAG

(E) DAD

**Ground Truth:** (D) GAG

**GPT4V Answer:**

To solve this problem, we need to match each letter (A, D, G) to a unique digit in the keys (1, 4, 7), given that each letter corresponds to exactly one digit and that same letters mean the same digits.

**Let's examine the first two padlooks and their keys:**

**1. ADA corresponds to 141**

**2. DGA corresponds to 717**

From the first padlock/key pair ADA = 141, we can establish that A = 1. Now, let's apply that to the second padlock/key pair:

If A = 1, then DGA = 717 would mean that D = 7 (since it's the only remaining number in the second position where A is not).

Now we know that:

\[\begin{array}{l}A=1\\ D=7\end{array}\]

For the third padlock and key, we have:

\[DAG=417\]

Since we know D = 7 and A = 1, it must mean that G = 4 because it's the remaining number and G has not been paired with a digit yet.

So, we have:

\[\begin{array}{l}A=1\\ D=7\\ G=4\end{array}\]

Now, let's see which combination matches our findings for the fourth padlock:

Considering our assignments of A = 1, D = 7, and G = 4, the fourth padlock should be:

\[\begin{array}{l}GAD=471\end{array}\]

Therefore, the correct answer is: (C) (C) GAD

Figure 44: An example of question understood error. The model mistakenly believes that keys and locks are in one-to-one correspondence, but there is no such condition in the question.

Figure 46: An example of reject to answer error.

Figure 45: An example of reject to answer error.

**Calculation Error**

**Question:** The diagram shows three concentric circles and two perpendicular, common diameters of the three circles. The three grey sections are of equal area, the small circle has radius 1. What is the product of the radii of the three circles?

(A) \(\sqrt{6}\)

(B) 3

(C) \(\frac{3\sqrt{3}}{2}\)

(D) \(2\sqrt{2}\)

(E) 6

**Ground Truth:**(A)\(\sqrt{6}\)

**GPT4V Answer:**

The problem states that the three grey sections are of equal area, and we know the radius of the smallest circle to be 1. Let's denote the radius of the middle circle as \(r\) and the radius of the largest circle as \(R\).

The area of a sector of a circle is given by \(\frac{1}{2}\theta r^{2}\), where \(\theta\) is the central angle in radians, and \(r\) is the radius of the circle. Since the three grey sections are equal in area and shaped as quarter circles, each grey section has a central angle of \(\frac{\pi}{2}\) radians.

Therefore, the area of each grey section can be represented as follows:

For the smallest circle:

\[A_{\text{small}}=\frac{1}{2}\times\frac{\pi}{2}\times 1^{2}=\frac{\pi}{4}\]

For the middle circle, the area of the sector minus the area of the smaller circle's quarter:

\[A_{\text{middle}}=\frac{1}{2}\times\frac{\pi}{2}\times r^{2}-A_{\text{small }}=\frac{\pi r^{2}}{4}-\frac{\pi}{4}\]

For the largest circle, the area of the sector minus the area of the middle circle's quarter:

\[A_{\text{large}}=\frac{1}{2}\times\frac{\pi}{2}\times R^{2}-A_{\text{middle} }-A_{\text{small}}=\frac{\pi R^{2}}{4}-\frac{\pi r^{2}}{4}\]

Since the areas are equal: \(A_{\text{small}}=A_{\text{middle}}=A_{\text{large}}\)

We can set the expressions for \(A_{\text{small}}\) and \(A_{\text{middle}}\) equal to each other:

\[\frac{\pi}{4}=\frac{\pi r^{2}}{4}-\frac{\pi}{4}\]

\[\Rightarrow\frac{\pi r^{2}}{4}=\frac{2\pi}{4}\]

\[\Rightarrow r^{2}=2\]

Now, do the same for \(A_{\text{middle}}\) and \(A_{\text{large}}\):

\[\frac{\pi r^{2}}{4}-\frac{\pi}{4}=\frac{\pi R^{2}}{4}-\frac{\pi r^{2}}{4}\]

\[\Rightarrow\frac{\pi R^{2}}{4}-\frac{2\pi r^{2}}{4}=0\]

\[\Rightarrow\frac{\pi R^{2}}{4}=\frac{2\pi}{4}\]

\[\Rightarrow R^{2}=2\]

\[\Rightarrow R=\sqrt{2}\times r=2\]

The product of the radii of the three circles is: \(1\times r\times R=1\times\sqrt{2}\times 2=2\sqrt{2}\)

Therefore, the answer is:

**Theorem 4.1**.: _The number of circles in the \(\mathbb{R}^{2}\) is given by \(\frac{1}{2}\theta r^{2}\)._

Proof.: The proof is similar to the proof of Theorem 4.1.

Figure 47: Full example of calculation error. The wrong steps are highlighted in red.

**Question:** The diagram shows two adjoining squares with side lengths \(a\) and \(b\) (with \(a<b\)). How big is the area of the grey triangle?

(A) \(\sqrt{ab}\)

(B) \(\frac{1}{2}a^{2}\)

(C) \(\frac{1}{2}b^{2}\)

(D) \(\frac{1}{4}\left(a^{2}+b^{2}\right)\)

(E) \(\frac{1}{2}\left(a^{2}+b^{2}\right)\)

**Ground Truth: (B) \(\frac{1}{2}a^{2}\)**

**GPT4V Answer:**

The grey triangle in the diagram can be seen as the difference between the large square with side \(b\) and the smaller square with side \(a\), minus the two right triangles that are outside of the grey area but inside the larger square.

The area of the larger square is \(b^{2}\), and the area of the smaller square is \(a^{2}\). The two right triangles each have legs of length \(a\) and \(b-a\), thus the area of one such right triangle is \(\frac{1}{2}\times a\times(b-a)\). There are two of these triangles, so the total area of the two triangles is \(\frac{1}{2}\times a\times(b-a)\times 2=a\times(b-a)\). Now, the area of the grey triangle is therefore the area of the large square minus the area of the small square and the two right triangles:

\[b^{2}-a^{2}-a(b-a)=b^{2}-a^{2}-ab+a^{2}=b^{2}-ab\]

We simplify this by factoring out a \(b\):

\[b(b-a)\]

Now, we notice that \(b-a\) is the height of the grey triangle, and, by looking at the diagram, we can see that the base of the grey triangle overlaps exactly with the side length of the larger square, which is \(b\).

Therefore, the area of the grey triangle is \(\frac{1}{2}\times\) base \(\times\) height, which in this case is \(\frac{1}{2}\times b\times(b-a)\): Area of grey triangle = \(\frac{1}{2}\times b\times(b-a)\)

But since we can't simplify this expression using the given variables directly, we must find a match among the options provided. By adding and subtracting \(\frac{1}{2}a^{2}\) within the expression of the larger square area \(b^{2}\), we can rewrite the expression:

\[b^{2}-a(b-a)=b^{2}-ab+a^{2}-\frac{1}{2}a^{2}+\frac{1}{2}a^{2}\] \[=\frac{1}{2}a^{2}+\frac{1}{2}b^{2}-\frac{1}{2}a^{2}-ab+\frac{1}{2 }a^{2}\] \[=\frac{1}{2}(a^{2}+b^{2})-\frac{1}{2}a^{2}\]

Therefore, the area of the grey triangle is equal to half the sum of the areas of the two squares minus half the area of the smaller square:

Area of grey triangle = \(\frac{1}{2}(a^{2}+b^{2})-\frac{1}{2}a^{2}\)

\[=\frac{1}{2}b^{2}\]

This matches option (C), so the area of the grey triangle is \(\frac{1}{2}b^{2}\).

Thus, the answer is:

**Ground Truth: (B) \(\frac{1}{2}a^{2}\)**

**GPT4V Answer:**

The grey triangle in the diagram can be seen as the difference between the large square with side \(b\) and the smaller square with side \(a\), minus the two right triangles that are outside of the grey area but inside the larger square.

The area of the larger square is \(b^{2}\), and the area of the smaller square is \(a^{2}\). The two right triangles each have legs of length \(a\) and \(b-a\), thus the area of one such right triangle is \(\frac{1}{2}\times a\times(b-a)\). There are two of these triangles, so the total area of the two triangles is \(\frac{1}{2}\times a\times(b-a)\times 2=a\times(b-a)\). Now, the area of the grey triangle is therefore the area of the large square minus the area of the small square and the two right triangles:

\[b

**Example of Algebra**

**Question:** The numbers in the five circles around each house add up to 20. Some numbers are missing. Which number does the question mark stand for?

**Ground Truth:** 9

**GPT4V Image Caption:**

The diagram features two houses connected by a series of circles, each containing a number or a question mark. Each house has a large circle on its roof and two smaller circles on the ground, one on each side.

The house on the left has the number 6 in the circle on its roof. The two smaller circles on the ground next to this house contain the numbers 2 and 5, respectively. The house on the right has the number 3 in the circle on its roof. **The circle on the ground to the right of this house contains the number 1, and the circle on the ground to the left of this house contains a question mark.**

The circles are connected by lines, suggesting a relationship or a pattern that needs to be discovered.... **Additionally, there is a line connecting the two houses, specifically from the circle with the number 5 to the circle with the question mark.**

The goal of the problem is likely to determine the number that should replace the question mark, based on the pattern or relationship between the numbers in the connected circles.

**GPT4 Answer:**

To solve this problem, we need to find the pattern or relationship between the numbers in the connected circles, given that the sum of the numbers around each house adds up to 20. For the house on the left: The numbers we know are 6 (on the roof), 2, and 5 (on the ground). Let's add these numbers: \(6+2+5=13\). Since the sum of the numbers around the house must be 20, we need to find the missing number that, when added to 13, equals 20. Let's denote the missing number on the left side as \(x\). So, \(13+x=20\). Solving for \(x\), we get \(x=20-13=7\).

For the house on the right: The numbers we know are 3 (on the roof) and 1 (on the ground). We also know now that the missing number on the left side, which is connected to the house on the right, is 7. Let's add these numbers: \(3+1+7=11\). Since the sum of the numbers around the house must be 20, we need to find the missing number that, when added to 11, equals 20. Let's denote the missing number on the right side as \(y\). So, \(11+y=20\). Solving for \(y\), we get \(y=20-11=9\). Therefore, the number that should replace the question mark is \(9\).

**Gemini Pro Answer:**

Let's assume the missing numbers as x and y. The sum of the numbers in the circles around the first house = 6 + 2 + 5 + x = \(13+x\). **The sum of the numbers in the circles around the second house = 3 + y + 1 = 4 + y.** Since the sum of the numbers in the circles around each house is 20, we have: \(13+x+4+y=20\); \(17+x+y=20\); \(x+y=3\). As the number in each circle is different, the possible values of x and y are (0, 3), (1, 2), and (2, 1). The question mark can be either \(0\) or \(2\).

**Qwen-VL-Max Answer:**

The sum of the numbers in the circles around the left house is \(6\pm 3\pm 2\pm 5=16\), so the number on the top right corner is \(20-16=\)4. The sum of the numbers in the circles around the right house is \(3+4+1+?=20\), so the question mark stands for \(8\).

Figure 49: An example of algebra question of GPT-4 with image caption. Although there are small hallucinations marked in red, The model correctly solves the problem based on the image description. Gemini Pro and Qwen-VL-Max fail to solve it because of weak visual recognition and reasoning ability.

**Question:**

Which of these clouds contain only numbers that are smaller than 7?

**GPT4V Image Caption:**

The diagram consists of five separate cloud-like shapes, each containing a set of numbers arranged in a specific pattern. Each cloud is labeled with a letter from (A) to (E). I will describe the arrangement of numbers within each cloud:

(A) This cloud contains four numbers arranged in a diamond shape. The top number is 1, the number to the left is 6, the number to the right is 8, and the bottom number is 7.

(B) This cloud also contains four numbers in a diamond pattern. The top number is 2, the left number is 5, the right number is 3, and the bottom number is 9.

(C) The third cloud has a similar diamond arrangement with the top number being 2, the left number 6, the right number 4, and the bottom number 7.

(D) This cloud's diamond pattern has the number 3 at the top, 1 on the left, 2 on the right, and 5 at the bottom.

(E) **The final cloud has a slightly different arrangement with five numbers. There are two numbers at the top: 8 on the left and 9 on the right. Below these, in the center, is the number 10. The bottom left number is 8, and the bottom right number is 7.**

Each cloud seems to represent a puzzle or problem where the numbers are related to each other in some way, possibly through mathematical operations. The exact nature of the relationship or the rules governing the arrangement of numbers is not specified in the diagram.

**GPT4 Answer:**

To solve this problem, we need to examine each cloud and determine if all the numbers it contains are smaller than 7.

(A) This cloud contains the numbers 1, 6, 8, and 7. Since it contains 8 and 7, which are not smaller than 7, it does not meet the criteria.

...

(E) This cloud contains the numbers 8, 9, 10, 8, and 7. Since it contains numbers greater than 7, it does not meet the criteria.

Therefore, the cloud that contains only numbers that are smaller than 7 is \(D\)

**Gemini Pro Answer:**

**(A) and (D) (A) 1, 6, 7, 8 (D) 1, 2, 3, 4, 5**

**Qwen-VL-Max Answer:**

To determine which clouds contain only numbers that are smaller than 7, we need to look at each cloud and check if all the numbers inside it are less than 7.

... **The clouds that contain only numbers that are smaller than 7 are \((C),(D)\)**,**

Figure 50: An example of arithmetic question of GPT-4 with image caption. Although there are small hallucinations marked in red, The model correctly solves the problem based on the image description. GPT-4V also does it correct while Gemini Pro and Qwen-VL-Max fail because of weak visual recognition ability.

**Figure 51**: An example of analytic geometry question of GPT-4 with image caption. Although there are small hallucinations marked in red, The model correctly solves the problem based on the image description while GPT-4V fails because of inaccurate image recognition.

**Example of Counting**

**Question:** How many triangles can you find in the picture?

**Ground Truth:** 5

**GPT4V Image Caption:**

The image depicts a stylized, geometric drawing of a humanoid figure that appears to be a representation of a girl or woman, composed of various simple shapes. The figure is not a real person but rather a collection of shapes that could be used in a math problem. Here is a detailed description of the figure:

**Head:** The head is a white circle with a simple, smiling face. The face has two blue dots for eyes and a red, upside-down triangle for a nose. The mouth is a black, curved line like a smile.

**Hair:** The figure has orange hair represented by two shapes on either side of the head, resembling pigtails. Each pigtail is a half-circle, and there is a pink bow on top of the head, consisting of a small circle in the center and two triangles on either side to represent the bow's loops.

**Body:** The body is a green trapezoid, wider at the top and tapering slightly towards the bottom, suggesting a dress or a tunic. There are two yellow circles in the middle of the trapezoid, aligned vertically, representing buttons.

**Arms:** The arms are two white rectangles extending from the upper sides of the trapezoid, each ending in a white triangle to represent hands.

**Skirt:** Below the trapezoid body, there is a green, inverted trapezoid that is narrower at the top and wider at the bottom, representing a skirt or the lower part of the dress.

**Legs:** The legs are two white rectangles extending downward from the bottom of the skirt.

**Shoes:** It has orange shoes, **each represented by a small rectangle at the end of the legs**. The entire figure is outlined in black, giving it a cartoonish, clip-art style appearance. The use of geometric shapes to construct the figure suggests that it could be used in a math problem involving geometry, such as calculating areas, perimeters, or identifying shapes.

**GPT4 Answer:**

To solve the problem, let's identify and count all the triangles in the described figure. The description mentions specific shapes that make up the figure, and we'll focus on those identified as triangles or that can form triangles.

1. **Nose:** The nose is described as a red, upside-down triangle. Count: 1 triangle

2. **Hair Bows:** Each bow on top of the head consists of a small circle in the center and two triangles on either side to represent the bow's loops. Count: 2 triangles (since there's one bow mentioned, it has two triangles as part of its design)

3. **Hands:** Each hand at the end of the arms is represented by a white triangle. Count: 2 triangles (one for each hand)

4. **Implied Triangles:** The description does not explicitly mention other triangles, but we can check for any implied or composite triangles. Given the shapes described (circles for the head and buttons, rectangles for arms and legs, trapezoids for the body and skirt, half-circles for hair, and rectangles for shoes), there are no additional triangles formed by these shapes.

Therefore, the total count of triangles in the figure, based on the description provided, is:

\[1\text{ (nose)}+2\text{ (hair bows)})+2\text{ (hands)})=5\]

So, the answer is \(\framebox{5}\) triangles.

Figure 52: An example of counting question of GPT-4 with image caption. Although there are small hallucinations marked in red, The model correctly solves the problem based on the image description.

**Example of Counting**

**Question:** How many more bricks does the right hand pyramid have than the left hand pyramid?

**Ground Truth:** 5

**GPT4V Image Caption:**

The diagram shows two separate pyramid-like structures composed of blocks. Each pyramid is arranged in a way that resembles steps, with each upper row containing fewer blocks than the row below it. The blocks are depicted in a 3D perspective, giving the appearance of depth.

The pyramid on the left has a total of 4 rows. From top to bottom, the number of blocks in each row increases by one. The top row has 1 block, the second row has 2 blocks, the third row has 3 blocks, and the bottom row has 4 blocks.

The pyramid on the right is larger, with a total of 5 rows. Similar to the left pyramid, the number of blocks in each row increases by one as you move from top to bottom. The top row has 1 block, the second row has 2 blocks, the third row has 3 blocks, the fourth row has 4 blocks, and the bottom row has 5 blocks.

Both pyramids are centered and have a symmetrical arrangement,...

**GPT4 Answer:**

To solve this problem, we need to calculate the total number of blocks in each pyramid and then find the difference between the two totals.

For the left pyramid (4 rows): The number of blocks in each row forms an arithmetic sequence: 1, 2, 3, 4. The total number of blocks is the sum of this sequence: \(1+2+3+4\).

For the right pyramid (5 rows): Similarly, the number of blocks in each row forms an arithmetic sequence: 1, 2, 3, 4, 5. The total number of blocks is the sum of this sequence: \(1+2+3+4+5\).

Now, let's calculate the sums:

For the left pyramid: \(1+2+3+4=10\); For the right pyramid: \(1+2+3+4+5=15\). Finally, to find out how many more blocks the right pyramid has than the left pyramid, we subtract the total number of blocks in the left pyramid from the total number of blocks in the right pyramid: \(15-10=5\). So, the right pyramid has 5 more blocks than the left pyramid.

**GPT-4V Answer:**

For the left hand pyramid: The top row has 1 brick, the second row has 2 bricks, the third row has 3 bricks, and the bottom row has 4 bricks. So, the total number of bricks in the left pyramid is \(1+2+3+4=10\) bricks.

For the right hand pyramid: The top row has 1 brick, the second row has 3 bricks, the third row has 5 bricks, and the bottom row has 7 bricks. So, the total number of bricks in the right hand pyramid is \(1+3+5+7=16\) bricks.

Now we find the difference: \(16-10=6\).

Therefore, the right pyramid has 6 more bricks than the left pyramid.

**Gemini Pro Answer:**

The left hand pyramid has 6 rows of bricks. The right hand pyramid has 7 rows of bricks. There are 7 - 6 = 1 more rows of bricks in the right hand pyramid. Each row of the left hand pyramid has 6 bricks. So there are 6 * 6 = 36 bricks in the left hand pyramid. Each row of the right hand pyramid has 7 bricks. So there are 7 * 7 = 49 bricks in the right hand pyramid. There are 49 - 36 = 13 more bricks in the right hand pyramid., [13]

Figure 53: An example of counting question of GPT-4 with image caption. The model correctly solves the problem based on the accurate and detailed image description while GPT-4V and Gemini Pro fail.

**Question:** The numbers \(1,2,3,4\) and 9 are written into the squares on the following figure. The sum of the three numbers in the horizontal row, should be the same as the sum of the three numbers in the vertical column. Which number is written in the middle?

**Ground Truth:** 9

**GPT4V Image Caption:**

The diagram shows a geometric shape that resembles a cross or a plus sign (+). It is composed of five identical rectangles arranged such that one rectangle is at the center, and the other four are attached to each of its sides. The rectangles are aligned so that the longer sides of the rectangles are connected to the shorter sides of the central rectangle, forming a symmetrical cross shape.

Each rectangle appears to have the same dimensions, suggesting that they could be squares, but without specific measurements, we cannot confirm their exact shape. The lines that form the rectangles are uniform, indicating that the boundaries of each rectangle are clearly defined.

The overall shape could be interpreted as a 2D representation of an unfolded 3D box...

**GPT4 Answer:**

Given the description of the figure and the problem, let's denote the numbers in the squares as follows, assuming the central square is \(X\). And we know there's a number 9 involved, which hasn't been placed yet. Since the sum of the three numbers in the horizontal row should be the same as the sum of the three numbers in the vertical column, and we need to include all numbers \(1,2,3,4,\) and \(9\), let's analyze the possible placements.

Given the numbers, the sum involving \(9\) would be significantly higher than any sum without it, meaning \(9\) must be in either the horizontal row or the vertical column that includes \(X\). Since the figure is symmetrical and the problem specifies that the sums of the numbers in the horizontal row and vertical column must be equal, the only logical placement for \(9\) to balance the equation would be in the center, as \(X\). Thus, the number written in the middle is \(9\).

**Gemini Pro Answer:**

Let's call the numbers in the squares A, B, C, D, and E. We know that A + B + C = D + E. We also know that A + D = B + E. Adding these two equations together, we get 2A + B + C + D = 2B + 2E. Simplifying this equation, we get A + C = B + E. Since A + B + C = D + E, we can substitute A + C for D + E in this equation to get A + C = B + (A + C). Simplifying this equation, we get B = A. Therefore, the number in the middle is \(A\).

**Qwen-VL-Max Answer:**

Let's call the number in the middle square \(x\).

From the given information, we can set up the following equations:

\[1+2+x=9+4,\ 3+4+x=1+2+9\]

**Solving for \(x\) in each equation, we find that \(x=\) 10**.

Figure 54: An example of combinatorics question of GPT-4 with image caption. The model correctly solves the problem based on the accurate and detailed image description while Gemini Pro and Qwen-VL-Max fail.

**Example of Combinatorics**

**Question:** A hen lays white and brown eggs. Lisa takes six of them and puts them in a box as shown. The brown eggs are not allowed to touch each other. What is the maximum number of brown eggs Lisa can place in the box?

**Ground Truth: 3**

**GPT4V Image Caption:**

The diagram consists of two distinct parts:

1. On the left, there is a rectangle that contains six circles arranged in two rows of three. The circles are drawn such that each circle in the top row touches the two circles below it, and each circle in the bottom row touches the two circles above it. The circles are also touching their neighbors within the same row. The arrangement is such that the circles are packed closely together, maximizing the number of circles that can fit within the rectangle without overlapping.

2. On the right, there is an illustration of a chicken. The chicken is...

**GPT4 Answer:**

1. **First Row**: Place a brown egg in the first position of the top row. To prevent any brown eggs from touching each other, the next egg in the top row must be white. Following this pattern, the third egg in the top row can be brown again. So, in the top row, we can place 2 brown eggs without them touching each other.

2. **Second Row**: For the bottom row, directly below a brown egg in the top row, we must place a white egg to prevent brown eggs from touching. This means that if the first and third positions in the top row are brown, the first and third positions in the bottom row must be white. Following the pattern to avoid brown eggs touching, we can place a brown egg in the second position of the bottom row. This ensures no brown eggs are touching vertically or horizontally.

So, in the bottom row, we can place 1 brown egg without it touching any other brown egg. Adding the brown eggs from both rows, we get a total of \(2+1=3\) brown eggs. [3].

**GPT-4V Answer:**

The problem involves arranging a maximum number of brown eggs in a box with six compartments without allowing the brown eggs to touch each other. To maximize the number of brown eggs while adhering to this constraint, we can place a brown egg in every other compartment to ensure no two brown eggs are adjacent to one another.

Placing a brown egg in the first compartment, we then skip the second compartment and place another brown egg in the third compartment. **Repeating this pattern, we can see that we can place brown eggs in the first, third, and fifth compartments in the top row. For the bottom row, we can place brown eggs in the second, fourth, and sixth compartments.**

Hence, the maximum number of brown eggs Lisa can place in the box without any two brown eggs touching each other is **three on the top row and three on the bottom row, which is a total of: 3 (top row) + 3 (bottom row) = 6 brown eggs. Therefore, the answer is:**[6]

**Gemini Pro Answer:**

Lisa can place at most 2 brown eggs in the box. Since the brown eggs are not allowed to touch each other, they can only be placed in the corner spots. There are only two corner spots, so the maximum number of brown eggs Lisa can place in the box is 2.

Figure 55: An example of combinatorics question of GPT-4 with image caption. The model correctly solves the problem based on the accurate and detailed image description while GPT-4V and Gemini Pro fail.

[MISSING_PAGE_FAIL:70]

**GPT4V Image Caption:**

The diagram consists of a series of five images labeled from (A) to (E), each containing a combination of geometric shapes within a rectangular frame. Below each image, there is a single shape corresponding to each label.

Image (A) contains **three blue triangles** and two red circles arranged around a central green square. The triangles are pointing upwards, and the circles are placed near the corners of the rectangle, not touching the edges. The shape below image (A) is a single red circle.

Image (B) shows two blue triangles and **two red circles**. The triangles are pointing upwards, and the circles are placed near the top corners of the rectangle, not touching the edges. The shape below image (B) is a single red circle, larger than the one below image (A).

Image (C) contains **three blue triangles**, with two triangles pointing upwards and one pointing downwards. The triangles are arranged in a horizontal line in the center of the rectangle. The shape below image (C) is a single blue triangle pointing upwards.

Image (D) has **two green squares** and **one blue triangle**. The squares are placed on the left side of the rectangle, one above the other, and the triangle is pointing upwards, located on the right side of the rectangle. The shape below image (D) is a single green square.

Image (E) includes **one green square**, **one blue triangle pointing upwards, and one red circle**. The square is placed on the left side, the triangle in the center, and the circle on the right side of the rectangle. The shape below image (E) is a single blue triangle pointing upwards.

**GPT4 Answer:**

To solve this problem, we need to identify which shape is not present in every image from (A) to (E).

**Image (A) contains: 3 blue triangles**, 2 red circles, and 1 green square.

**Image (B) contains: 2 blue triangles and 2 red circles.

**Image (C) contains: 3 blue triangles.**

**Image (D) contains: 1 blue triangle and 2 green squares.**

**Image (E) contains: 1 blue triangle, 1 red circle, and 1 green square.**

Now, let's identify which shapes are present in each image:

Blue triangles are present in all images (A) to (E). Red circles are present in images (A), (B), and (E).

Green squares are present in images (A), (D), and (E).

From this analysis, we can see that the shape that cannot be seen in every picture is the red circle, as it is missing from images (C) and (D). Therefore, the answer is. Red Circle.

**GPT-4V Answer:**

The images display various combinations of shapes: circles, triangles, and squares in different arrangements. The question is asking which shape is not present in all the images.

After examining each picture, we can see: Picture 1 contains circles, triangles, and a square. Picture 2 contains circles and triangles. Picture 3 contains circles and triangles. Picture 4 contains triangles and squares. Picture 5 contains circles, triangles, and a square.

The square shape does not appear in all pictures; it is absent in Picture 2 and Picture 3.

So, the correct answer is. \(D\)

Figure 57: An example of combinatorial geometry question of GPT-4 with image caption. The model fails to solve the problem because of the inaccurate image description. GPT-4V also misinterprets the image, but gives the correct answer.

**Figure 58**: An example of transformation geometry question of GPT-4 with image caption. The model fails to solve the problem because of the inaccurate image description. GPT-4V also misinterprets the image, but gives the correct answer.

**Question:** Five equally big square pieces of card are placed on a table on top of each other. The picture on the side is created this way. The cards are collected up from top to bottom. In which order are they collected? (A) 5-4-3-2-1 (B) 5-2-3-4-1 (C) 5-4-2-3-1 (D) 5-3-2-1-4 (E) 5-2-3-1-4

**Ground Truth:** E

**GPT4V Image Caption:**

The diagram shows a composite rectangular figure made up of five smaller rectangles, each labeled with a number from 1 to 5. The overall shape is a larger rectangle. 1. Rectangle 1 is in the top left corner. **It is a vertical rectangle.** 2. Rectangle 2 is below Rectangle 1. It is a larger vertical rectangle, taking up the entire left side of the larger rectangle. 3. Rectangle 3 is in the center, bordering the right side of Rectangle 1 and the top side of Rectangle 2. It is a horizontal rectangle.. Rectangle 4 is a small square that is placed to the right of Rectangle 1 and above Rectangle 3. 5. Rectangle 5 is a large square that occupies the bottom right corner of the larger rectangle. It borders the right side of Rectangle 2 and the bottom side of Rectangle 3 and Rectangle 4. The rectangles are shaded in different shades of gray, possibly to distinguish them from one another. The numbers are placed in the center of each corresponding rectangle. The arrangement of the rectangles suggests that the dimensions of some could be inferred if the dimensions of others are known, which is a common setup for problems involving area and perimeter.

**GPT4 Answer:**

To solve this problem, we need to determine the order in which the cards (represented as rectangles in the description) are collected based on their arrangement in the composite figure. The key is to understand that the card on top in the figure will be the last one collected, and the card at the bottom will be the first one collected, as they are collected from top to bottom. 1. **Rectangle 5** is the large square at the bottom right corner. It does not lie underneath any other rectangle, meaning it is on top of all others in its area. Thus, it will be the last one collected. 2. **Rectangle 4** is the small square to the right of Rectangle 1 and above Rectangle 3. **Since it lies on top of Rectangle 3 and does not have any other rectangles on top of it, it will be collected before Rectangle 5 but after the rectangles it covers (Rectangles 1 and 3).**

Therefore, the order in which the cards are collected from top to bottom is: \(\left[\begin{array}{c}5,4,3,1,2\\ \end{array}\right]\)

**GPT-4V Answer:**

To solve the problem, we look at which cards are on top of others. To collect the cards from top to bottom as described, we should look for the one that is not covered by any other card, then the next, and so on. Starting from the top, we can see that card **5 is at the bottom, as all other cards are on top of it in some part. Next, we see that card 2 is on top of card 5 but underneath cards 1 and 3. Then, card 3 is on top of both cards 2 and 5 but below cards 1 and 4. Card 4 is only underneath card 1. Finally, card 1 is at the top with no cards above it......

Therefore, the correct sequence of collecting the cards from top to bottom is represented by option (E), when we reverse that sequence (since they are collected from top to bottom): 4-1-3-2-5. So the answer is:

Figure 59: An example of descriptive geometry question of GPT-4 with image caption. The models fail to correctly interpret the spacial relationships between the cards.

**Example of Logic**

**Question:** Maria wants to write whole numbers in the squares of the figure, so that the sum of the numbers in three consecutive squares is always 10. She has already written a number. What number should she write on the gray square?

**Ground Truth:** 2

**GPT4V Image Caption:**

The diagram shows a horizontal rectangular bar divided into five equal-sized squares. The bar has a border around it, giving it the appearance of a single entity with internal divisions. The first square on the left is filled with the number 2 in a bold font. The second square is empty, the third square is shaded in a solid color, and the fourth and fifth squares are empty. The shaded square suggests it may have a particular significance in the context of the math problem, such as representing a specific value or condition that needs to be determined. The sequence and the values in the squares are likely part of the problem that needs to be solved.

**GPT4 Answer:**

To solve this problem, let's denote the numbers in the squares from left to right as \(a\), \(b\), \(c\), \(d\), and \(e\). According to the description, \(a=2\), and \(c\) is the number in the gray square that we need to find. We are given that the sum of the numbers in any three consecutive squares is always 10. Given this, we can set up **equations based on the sums of three consecutive squares: 1. \(a+b+c=10\) 2**, \(b+c+d=10\) 3**, \(c+d+e=10\) We know that \(a=2\), so we can substitute this into the first equation to find \(b+c\): 1. \(2+b+c=10\) 2. \(b+c=8\) However, we need to find \(c\), and we don't have direct values for \(b\), \(d\), or \(e\). But we can notice that since the sum of any three consecutive numbers is 10, and \(a=2\), the only way to satisfy these conditions is to ensure that the sequence allows for this constant sum across any three consecutive squares. Given \(2+b+c=10\), we have \(b+c=8\).

.....

Given the constraints and the aim to solve for \(c\), the approach taken highlights the need for either a specific pattern to follow or additional given numbers in the sequence to directly calculate \(c\)'s value, pointing towards a limitation in the direct calculation method

**GPT-4V Answer:**

Let's assign variables to represent the numbers in each of the squares:

\[\begin{array}{|c|c|c|c|}\hline 2&a&b&c&d\\ \hline\end{array}\]

Given that the sum of the numbers in three consecutive squares is always 10, we can create the following equations:

\[\begin{cases}2+a+b=10\\ a+b+c=10\\ b+c+d=10\\ \end{cases}\]

We can solve these equations successively. From the first equation: \(a+b=10-2\)\(a+b=8\) Now, we don't have enough information to solve for a or b individually, but we don't need to because we can use the second equation to find c (which is related to the grey square): \(a+b+c=10\) Substitute \(a+b=8\) into the second equation: \(8+c=10\)\(c=10-8\)\(c=2\) So, the number that should be written on the grey square is [2].

Figure 60: An example of logic question of GPT-4 with image caption. GPT-4 fails to correctly find all the necessary equations and does not reach an answer. GPT-4V correctly solves the problems.

**Example of Metric Geometry**

**Question:** A circle \(K\) is inscribed in a quarter circle with radius 6 as shown in the figure.

What is the radius of circle \(K\)?

(A) \(\frac{6-\sqrt{2}}{2}\) (B) \(\frac{3\sqrt{2}}{2}\) (C) 2.5 (D) 3 (E) \(6(\sqrt{2}-1)\)

**Ground Truth:** E

**GPT4V Image Caption:**

The diagram shows a large circle with a smaller circle inside it. The smaller circle is tangent to the larger circle at a single point on the larger circle's interior. The smaller circle is labeled with the letter K. The space between the two circles forms a crescent shape. The larger circle appears to be a boundary that encloses the smaller circle, and there is no indication of any dimensions, angles, or other geometric properties.

**GPT4 Answer:**

To solve this problem, let's denote the radius of the smaller circle (circle \(K\)) as \(r\). Since circle \(K\) is inscribed within a quarter circle of radius 6, we can visualize or draw a diagram where the quarter circle's radius forms a right-angled triangle with the sides being the radius of the quarter circle, the radius of circle \(K\), and the distance from the point where circle \(K\) touches the quarter circle's arc to the corner of the quarter circle (the center of the quarter circle). The distance from the center of circle \(K\) to the corner of the quarter circle is \(r\), and the distance from the corner of the quarter circle to the point where circle \(K\) touches the quarter circle's arc is also \(r\). Therefore, the hypotenuse of the right-angled triangle formed (which is also the radius of the quarter circle) is 6 units. Using the Pythagorean theorem, we can set up the equation: \(r+r=2r\) (this is the base and the height of the right-angled triangle, both equal to the diameter of circle \(K\) since it touches both the arc and the two straight sides of the quarter circle). **The hypotenuse (the radius of the quarter circle) is \(6\) units, so: \((2r)^{2}+(2r)^{2}=6^{2}\)**

....

Therefore, the radius of circle \(K\) is \(\frac{3\sqrt{2}}{2}\) units.

**GPT-4V Answer:**

To solve this problem, we need to use geometric relationships within the quarter circle and the inscribed circle K. Consider a quarter circle of radius 6. If we inscribe a circle K inside it, such that K is tangent to the two sides and the arc of the quarter circle, there will be two points of tangency on the sides of the quarter circle which are at a distance equal to the radius of K from the corners of the quarter circle. Let's denote the radius of circle K as r. Since circle K is tangent to the two sides, we know that the distance from the center of K to each side is equal to r, the radius of K. This means there will be a square of side r formed between the center of circle K and the two sides of the quarter circle. The remaining section between the edge of this square and the quarter circle will be a right-angled isosceles triangle with sides of length \(6-r\) (since the quarter circle has a radius of 6). The hypotenuse of this right-angled isosceles triangle will be the radius of the quarter circle, which is 6. Using the Pythagorean theorem, we get: \((6-r)^{2}+(6-r)^{2}=6^{2}\)....

so our answer is: \(E\)

Figure 61: An example of metric geometry question of GPT-4 with image caption. GPT-4 fails to find the correct relationship between the variables. GPT-4V correctly solves the problems.