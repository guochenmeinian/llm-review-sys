# Precise asymptotics of reweighted least-squares algorithms for linear diagonal networks

Chiraag Kaushik

Electrical and Computer Engineering

Georgia Institute of Technology

Atlanta, GA 30308

ckaushik7@gatech.edu

Justin Romberg

Electrical and Computer Engineering

Georgia Institute of Technology

Atlanta, GA 30308

jrom@ece.gatech.edu

Vidya Muthukumar

Electrical and Computer Engineering,

Industrial & Systems Engineering

Atlanta, GA 30308

vmuthukumar8@gatech.edu

###### Abstract

The classical _iteratively reweighted least-squares_ (IRLS) algorithm aims to recover an unknown signal from linear measurements by performing a sequence of weighted least squares problems, where the weights are recursively updated at each step. Varieties of this algorithm have been shown to achieve favorable empirical performance and theoretical guarantees for sparse recovery and \(\ell_{p}\)-norm minimization. Recently, some preliminary connections have also been made between IRLS and certain types of non-convex linear neural network architectures that are observed to exploit low-dimensional structure in high-dimensional linear models. In this work, we provide a unified asymptotic analysis for a family of algorithms that encompasses IRLS, the recently proposed lin-RFM algorithm (which was motivated by feature learning in neural networks), and the alternating minimization algorithm on linear diagonal neural networks. Our analysis operates in a "batched" setting with i.i.d. Gaussian covariates and shows that, with appropriately chosen reweighting policy, the algorithm can achieve favorable performance in only a handful of iterations. We also extend our results to the case of group-sparse recovery and show that leveraging this structure in the reweighting scheme provably improves test error compared to coordinate-wise reweighting.

## 1 Introduction

Many high-dimensional machine learning and signal processing tasks rely on solving optimization problems with regularizers that explicitly enforce certain structure on the learned parameters. The traditional formulation for such tasks involves a regularized empirical risk minimization (ERM) problem of the form

\[\min_{\bm{\theta}\in\mathbb{R}^{d}}L(\bm{\theta})+\lambda R(\bm{\theta}),\] (1)

where \(L(\cdot)\) is a loss function that encourages fidelity to the observed training data and \(R(\cdot)\) encodes desirable structural properties. In many important applications, it is desirable to obtain a sparsity-seeking solution; in such cases, the regularizer is typically non-smooth, as in the LASSO, group LASSO, and nuclear norm regularizers. As an alternative approach to this non-smooth optimization, several recent works have proposed the "Hadamard over-parameterization" of \(\bm{\theta}\) into the entry-wise product of two factors \(\bm{u}\odot\bm{v}\). While the resulting minimization problem is non-convex, thisparameterization, coupled with a _smooth_ regularizer, has been shown to achieve competitive empirical performance (in terms of numerical stability, robustness, and convergence rate) when compared to traditional sparse recovery algorithms [13, 25]. For example, rather than solving the convex, but non-smooth LASSO (where \(L\) is the squared loss and \(R\) is the \(\ell_{1}\) norm), the Hadamard reparameterization yields the following non-convex and smooth formulation:

\[\min_{\bm{u},\bm{v}\in\mathbb{R}^{d}}L(\bm{u}\odot\bm{v})+\frac{\lambda}{2}( \|\bm{u}\|_{2}^{2}+\|\bm{v}\|_{2}^{2}).\] (2)

In the case where the regression function is linear in \(\bm{\theta}\), solving (2) is equivalent to learning a function of the form

\[f_{\bm{u},\bm{v}}(\bm{x})=\langle\bm{x},\bm{u}\odot\bm{v}\rangle=\langle \operatorname{diag}(\bm{v})\bm{x},\bm{u}\rangle,\]

which can be thought of as a one hidden layer neural network with linear activation function and inner weight matrix \(\operatorname{diag}(\bm{v})\). In this context, this _linear diagonal neural network (LDNN)_ architecture has also been studied as an illustrative case study to improve our understanding of how neural networks perform iterative _"feature learning"_ to leverage low-dimensional structure in high-dimensional settings [34, 23]. We note here that, in the linear model case, feature learning is equivalent to learning which subset of the input's coordinates are relevant for the true predictor (i.e., feature selection).

One way to understand the connection between classical sparse recovery algorithms and the Hadamard product/LDNN form in (2) is to consider the change of variable \(v_{i}\to\sqrt{\eta_{i}}\) and \(u_{i}\to\frac{\theta_{i}}{\sqrt{\eta_{i}}}\)[25]. This yields the following optimization problem, which is jointly convex in \(\bm{\eta}\) and \(\bm{\theta}\):

\[\min_{\bm{\theta}\in\mathbb{R}^{d}}\min_{\bm{\eta}\in\mathbb{R}^{d}_{+}}L(\bm {\theta})+\frac{\lambda}{2}\sum_{i=1}^{d}\biggl{(}\frac{\theta_{i}^{2}}{\eta _{i}}+\eta_{i}\biggr{)}.\] (3)

After solving the minimum over \(\bm{\eta}\) explicitly, the second term becomes exactly \(\lambda\|\bm{\theta}\|_{1}\), and we recover the Lasso objective. This is a special case of the so-called "eta-trick" [1], which can be used to write many common sparsity-inducing penalties as the minimization of a quadratic functional of \(\bm{\theta}\).

A variety of algorithms for learning Hadamard product parameterizations have recently been studied, including alternating minimization [13], bi-level optimization [25], and joint gradient descent on \((\bm{u},\bm{v})\)[34]. The connection to the \((\bm{\theta},\bm{\eta})\) optimization in (3) can also be leveraged to construct algorithms based on classical sparse recovery techniques. In particular, alternating minimization over \(\bm{\theta}\) and \(\bm{\eta}\) in (3) yields the popular _iteratively reweighted least-squares (IRLS)_ algorithm [11, 9]. Translating these updates to the equivalent updates on \((\bm{u},\bm{v})\), we obtain an iterative least-squares algorithm for LDNNs, which alternately sets \(\bm{v}^{(t+1)}=\sqrt{|\bm{u}^{(t)}\odot\bm{v}^{(t)}|}\) and performs a weighted least squares update on \(\bm{u}\). This particular form of reparameterized IRLS was generalized in [28] to a larger family of iterative least-squares algorithms under the name of _linear recursive feature machines (lin-RFM)_.

While several methods for learning Hadamard/LDNN parameterizations have been introduced in the literature, there remain many open questions about how they each perform and how they compare. Theoretical analyses of these algorithms typically assume fixed, possibly worst-case training data, and aim to characterize the properties of the fixed-points [28, 34] or give convergence guarantees to second-order stationary points [25]. However, these worst-case analyses do not readily yield guarantees on the estimation error, which is the principal metric of interest. Indeed, many works have shown that studying the _average-case_, or typical, behavior of non-convex optimization algorithms can allow for estimation guarantees that are more precise and reflective of practice [14, 18, 7].

In this paper, we provide a precise analysis of a general family of iterative algorithms for learning LDNNs that take the form

\[\bm{u}^{(t+1)} =\arg\min_{\bm{u}\in\mathbb{R}^{d}}\frac{1}{n}\biggl{\|}\bm{y}^{( t)}-\frac{1}{\sqrt{d}}\bm{X}^{(t)}(\bm{u}\odot\bm{v}^{(t)})\biggr{\|}_{2}^{2}+ \frac{\lambda}{d}\|\bm{u}\|_{2}^{2}\] \[\bm{v}^{(t+1)} =\psi(\bm{u}^{(t+1)},\bm{v}^{(t)}),\]

for some reweighting function \(\psi\) and batches of training data \((\bm{X}^{(t)},\bm{y}^{(t)})\). As we show in Section 2, this formulation encompasses multiple existing algorithms, including reparameterized IRLS, lin-RFM, and alternating minimization over \(\bm{u}\) and \(\bm{v}\). We consider the common scenario where training is performed with batches of data and characterize the _exact_ distribution of the parameters after each iteration in the high-dimensional limit \((n,d)\to\infty\). This allows us to address questions such as * How do different algorithm choices compare (in terms of convergence and signal recovery) in the high-dimensional regime?
* How many iterations does it take common algorithms to find statistically favorable solutions?
* What is the effect of _model architecture_ in LDNNs? Does leveraging group structure provably improve sample complexity when the ground-truth signal is group-sparse?

Contributions: We define a general class of algorithms which learns LDNNs by alternately performing least-squares and reweighting steps in a sample-split/batched setting, and we show the following.

**(1)** Under mild assumptions on the target signal, initialization, and reweighting function, we provide an exact characterization of the distribution of the entries of the parameters at each iteration in the limit as \(n,d\) approach infinity (Theorem 1).

**(2)** We show that this asymptotic result aligns well with numerical simulations and allows for accurate prediction of the test error at each iteration. This enables rigorous comparison between different algorithms and demonstrates that, with appropriate reweighting schemes, a statistically favorable solution can be obtained in only a handful of iterations.

**(3)** Lastly, we extend our asymptotic framework to a setting of _structured sparsity_, where \(\bm{\theta}^{*}\) has group-sparse structure (Theorem 2). Our results show that using a grouped Hadamard parameterization (i.e., tying together groups of weights in the LDNN) effectively learns such signals, with performance scaling with the number of non-zero groups, rather than the total sparsity level.

### Related work

**IRLS and the \(\eta\)-trick:** The reformulation of non-smooth regularizers in terms of quadratic variational forms (the "\(\eta\)-trick") has been studied in various early works in computer vision and robust statistics [12; 4]. Further analysis and examples of sparsity-promoting norms are provided in [20; 2], and [25] provides a characterization of when a regularizer admits a variational form of this type. The resultant optimization algorithm is iteratively-reweighted least-squares (IRLS), a popular technique for compressive sensing and sparse recovery [11; 9]. These works also consider IRLS algorithms corresponding to \(\ell_{p}\)-norm regularization for \(0<p<1\); in this case, the minimization is no longer convex, but [11] shows that such methods can find sparse solutions with fast local convergence rate. The family of algorithms we consider includes a reparameterized version of each of these IRLS algorithms, but unlike these prior works, we consider a batched setting and the high-dimensional asymptotic regime. Moreover, our results apply to other algorithms which may not be easily expressed as resulting from the \(\eta\)-trick.

Hadamard parameterization and linear diagonal networks: The reparameterization of \(\bm{\theta}\) into the product of factors \(\bm{u}\odot\bm{v}\) has been considered in a variety of recent works. The authors of [31; 35] show that early-stopped joint gradient descent over the two factors can lead to optimal sample complexity for sparse linear regression. The equivalence of this parameterization to LDNNs has also led to a surge of interest in the _implicit bias_ of gradient descent/flow on this parameterization, i.e., a characterization of which solution gradient descent will reach without explicit regularization (corresponding to \(\lambda=0\)). These works typically consider gradient flow run until completion and characterize the solution as a minimizer of a certain sparsity-inducing functional that depends on the initialization [34; 10; 23].

The connection between the LASSO (as well as some non-convex \(\ell_{q}\) penalties) and the Hadamard parameterization was studied in [13], where alternating minimization over the two factors is used instead of first-order methods. More recently, [25] extends these observations by making explicit the connection to the \(\eta\)-trick and showing that saddle points are strict (escapable). These insights lead to global convergence guarantees and a smooth bi-level optimization scheme [25; 26] for non-smooth structured optimization problems that was shown to perform competitively with state-of-the-art solvers. The non-convex landscape of such formulations is further explored in [15], where it is shown that for a large class of parameterizations (including grouped, deep, and fractional Hadamard products), the non-convex problem has no spurious local minima. Motivated by the type of feature learning observed in neural networks, the authors of [28] propose lin-RFM, which updates one of the parameters via weighted least-squares while iteratively updating the other parameter via a reweighting scheme based on the average gradient outer product of the learned function. The authors characterize properties of the fixed-points and show that, for certain reweighting schemes, lin-RFM is equivalent to a reparameterization of IRLS. The family of algorithms we consider is similar, consisting of a weighted least-squares step and a reweighting step; however, it is more general and doesn't require the reweighting function to have the particular form required by lin-RFM. Moreover, our asymptotic characterization of the iterates allows for a precise understanding of how the test error evolves. On the other hand, our analysis relies on batching/sample-splitting of training data while all of the above works reuse the entire batch of training data at each iteration.

We make particular note here of the few works which explicitly consider a "grouped" Hadamard parameterization, which we consider in Section 4. This corresponds to a LDNN with groups of tied weights in the hidden layer. Early stopped gradient flow/descent for this type of architecture was shown in [17] to achieve sample-complexity scaling with the number of non-zero groups (rather than the overall sparsity). The non-convex landscape for this grouped architecture is studied in [36] and [15]. Our results complement these works by studying group-reweighted least-squares algorithms (rather than gradient methods) for learning functions of this form.

**Precise characterization of higher-order non-convex optimization problems:** On a technical level, our work provides a precise deterministic characterization of a family of higher-order optimization algorithms. In this sense, our results are of a similar flavor to [7], where Gaussian comparison inequalities are used to obtain a precise characterization of non-convex optimization problems. However, since the Hadamard parameterization is a re-parameterization of the actual estimator of interest (\(\bm{\theta}\coloneqq\bm{u}\odot\bm{v}\)), the results of [7] are not directly applicable. While our results are asymptotic and do not provide finite-sample guarantees, we provide a _distributional_ characterization of \(\bm{v}\) after each reweighting step, which allows us to characterize the behavior of more complicated functions of the iterates. Precise characterizations of alternating minimization and lin-prox methods for rank-1 matrix sensing are studied in the works [6; 19]. While these works obtain non-asymptotic guarantees, the estimation model and resulting optimization objective are quite different, with each unknown parameter interacting with independent sensing vectors (rather than a single sensing vector interacting with the product of the two parameters).

## 2 Background and formulation

**Notation:** The ones vector of dimension \(d\) is denoted as \(\bm{1}_{d}\). We denote the element-wise multiplication (Hadamard product) of two vectors \(\bm{x}\) and \(\bm{y}\) as \(\bm{x}\odot\bm{y}\). Element-wise division of two vectors is denoted as \(\frac{\bm{x}}{\bm{y}}\). We say a function \(f\colon\mathbb{R}^{p}\to\mathbb{R}\) is _pseudo-Lipschitz_ of order \(2\) if, for all \(\bm{x}\), \(\bm{y}\in\mathbb{R}^{p}\),

\[|f(\bm{x})-f(\bm{y})|\leq C(1+\|\bm{x}\|_{2}+\|\bm{y}\|_{2})\|\bm{x}-\bm{y}\|_ {2}\]

for some constant \(C>0\). The set of such functions is denoted by \(\text{PL}(2)\).

Convergence in probability of a sequence of random variables \(X_{d}\) to a random variable \(X\) is denoted by \(X_{d}\stackrel{{ P}}{{\to}}X\). Convergence in Wasserstein-2 distance of a sequence of probability distributions \(\nu_{d}\) to a limiting distribution \(\nu\) is denoted as \(\nu_{d}\stackrel{{\mathcal{W}_{2}}}{{\to}}\nu\), and this fact is equivalent to the statement \(\mathbb{E}_{X\sim\nu_{d}}g(X)\to\mathbb{E}_{X\sim\nu}g(X)\) for all \(g\in\text{PL}(2)\)[3]. If the \(\nu_{d}\) are _random_ probability measures, we say that \(\nu_{d}\stackrel{{\mathcal{W}_{2}}}{{\to}}\nu\) if the same convergence holds in probability, i.e., \(\mathbb{E}_{X\sim\nu_{d}}g(X)\stackrel{{ P}}{{\to}}\mathbb{E}_{X \sim\nu}g(X)\) for all \(g\in\text{PL}(2)\). The empirical distribution of a vector \(\bm{z}\in\mathbb{R}^{d}\) is defined as \(\frac{1}{d}\sum_{i=1}^{d}\delta(z_{i})\), where \(\delta(z_{i})\) is the Dirac delta distribution centered at \(z_{i}\).

**Formulation:** We consider a batched noisy linear model where, at each time \(t=0,1,\ldots,T\), a user has access to an independent batch of data \((\bm{X}^{(t)},\bm{y}^{(t)})\in\mathbb{R}^{n\times d}\times\mathbb{R}^{n}\) satisfying

\[\bm{y}^{(t)}=\frac{1}{\sqrt{d}}\bm{X}^{(t)}\bm{\theta}^{*}+\bm{\epsilon}^{(t)}.\]

Above, \(\bm{\theta}^{*}\in\mathbb{R}^{d}\) is an unknown signal, \(\bm{X}^{(t)}\) has i.i.d. standard Gaussian entries, and \(\bm{\epsilon}^{(t)}\sim\mathcal{N}(\bm{0},\sigma^{2}\bm{I}_{n})\) is i.i.d. noise in the measurements. Given an initial weight vector \(\bm{v}^{(0)}\in\mathbb{R}^{d}\), we are interested in the behavior of iterative algorithms of the form

\[\begin{split}\bm{u}^{(t+1)}&=\arg\min_{\bm{u}\in \mathbb{R}^{d}}\frac{1}{n}\bigg{\|}\bm{y}^{(t)}-\frac{1}{\sqrt{d}}\bm{X}^{(t)}( \bm{u}\odot\bm{v}^{(t)})\bigg{\|}_{2}^{2}+\frac{\lambda}{d}\|\bm{u}\|_{2}^{2} \\ \bm{v}^{(t+1)}&=\psi(\bm{u}^{(t+1)},\bm{v}^{(t)}), \end{split}\] (4)where \(\psi\colon\mathbb{R}\times\mathbb{R}\to\mathbb{R}\) is a "reweighting" function that acts entry-wise on \((\bm{u}^{(t)},\bm{v}^{(t)})\) and \(\lambda>0\) is a hyperparameter governing the strength of the regularization. We we will study the behavior of the iterates \(\bm{u}^{(t)},\bm{v}^{(t)}\) in the high-dimensional limit where \(n\) and \(d\) both approach infinity with fixed ratio \(\frac{d}{n}=\kappa\). Since our primary interest is to reveal the feature learning capabilities of such algorithms when \(\bm{\theta}^{*}\) is a high-dimensional signal with low-dimensional structure, we will typically focus on the regime where \(\kappa>T\), where \(T\) is the number of total iterations. This ensures that the total number of observed samples \(nT\) is smaller than the ambient dimension \(d\).

Before proceeding to our main results, we note that this formulation encompasses a wide variety of classical and modern algorithms (summarized in Table 1):

* **Alternating minimization:** One perspective on this algorithm is to consider it as a way to perform alternating minimization on the non-convex loss function \[L(\bm{u},\bm{v})=\frac{1}{n}\bigg{\|}\bm{y}-\frac{1}{\sqrt{d}}\bm{X}(\bm{u} \odot\bm{v})\bigg{\|}_{2}^{2}+\frac{\lambda}{d}\|\bm{u}\|_{2}^{2}+\frac{ \lambda}{d}\|\bm{v}\|_{2}^{2}.\] Using the fact that the loss function is symmetric in \(\bm{u}\) and \(\bm{v}\), choosing \(\psi(u,v)=u\) recovers the mini-batched alternating minimization algorithm for this loss. In other words, \(\psi\) simply switches the two parameters \(\bm{u}\) and \(\bm{v}\).
* **IRLS algorithms for sparse recovery:** As shown in [28], classical IRLS reweighting schemes used for sparse recovery and compressed sensing [11; 21] can be reparameterized in the form of (4) \(\psi(u,v)=(u^{2}v^{2}+\epsilon)^{\alpha}\), where different choices of \(\alpha\) correspond to different \(\ell_{p}\) penalties. In particular, the choice \(p=2-4\alpha\) corresponds to the IRLS-p algorithm of [21].
* **Lin-RFM [28]:** Generalizing the reparameterized IRLS update, the authors of [28] propose the choice \(\psi(u,v)=\phi(u^{2}v^{2})\) for some continuous function \(\phi\colon\mathbb{R}\to\mathbb{R}^{+}\). Here, the quantity \(u^{2}v^{2}\) arises from the average outer product of the gradient of the learned regression function, which was shown empirically in [27] to correlate with the features learned in the weight matrices of various common neural network architectures.

Our goal is to understand statistical properties of the iterates for different choices of \(\psi\), and in particular how the test error evolves from iteration to iteration. In the following section, we develop an asymptotic characterization of the iterates that can be used to gain insight into these questions for a large class of reweighting functions and problem settings.

## 3 A precise characterization of the iterates

In this section, we provide a precise characterization of the iterates of the algorithm in Equation 4 with i.i.d. Gaussian covariates. First, we introduce and discuss the two main assumptions needed for our main result. The first assumption is concerned with the distribution of the initialization \(\bm{v}_{0}\) and the target signal \(\bm{\theta}^{*}\):

**Assumption 1**.: _The empirical distribution of the entries of \(\bm{v}^{(0)}\) and \(\bm{\theta}^{*}\) converges in \(\mathcal{W}_{2}\) distance to some joint distribution \(\Pi_{0}\), i.e., \(\frac{1}{d}\sum_{i=1}^{d}\delta(v_{i}^{(0)},\theta_{i}^{*})\overset{\mathcal{ W}_{2}}{\rightarrow}\Pi_{0}\). Additionally, \(v_{i}^{(0)}\neq 0\) for all \(i\) and \(\bm{\theta}^{*}\) has bounded entries almost surely._

Here, the requirement of empirical distribution convergence is easily satisfied by common choices of \(\bm{v}^{(0)}\), including the ones vector and i.i.d. Gaussian entries. For a typical sparse regression setup, we might, for example, consider the \(\Pi_{0}\) induced by choosing \(\bm{v}^{(0)}=\mathbf{1}_{d}\) and letting \(\bm{\theta}^{*}\) have i.i.d. entries

\begin{table}
\begin{tabular}{l l} \hline \hline Algorithm & Reweighting function \\ \hline Alternating minimization (AM) [13] & \(\psi(u,v)=u\) \\ Reparameterized IRLS [9; 11; 28] & \(\psi(u,v)=(u^{2}v^{2}+\epsilon)^{\alpha}\) \\ Linear recursive feature machines (lin-RFM) [28] & \(\psi(u,v)=\phi(u^{2}v^{2})\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Some algorithms taking the form (4)that equal \(0\) with certain probability. The requirement that \(\bm{\theta}^{*}\) has bounded entries appears to be an artifact of the proof, and is used only in the proof of one technical lemma. In our simulations, we find that our asymptotic predictions often remain accurate when \(\bm{\theta}^{*}\) has entries from distributions which are not bounded almost surely (e.g., Gaussian entries).

Secondly, we define the set of reweighting functions \(\psi\) for which our result will apply.

**Assumption 2**.: _The reweighting function \(\psi\colon\mathbb{R}\times\mathbb{R}\to\mathbb{R}\) satisfies the following:_

1. _If_ \(U,V\) _are random variables such that_ \(U,V\neq 0\) _with probability 1, then_ \(\psi(U,V)\neq 0\) _with probability 1._
2. \(\psi\) _is continuous and bounded or_ \(\psi^{2}\) _is pseudo-Lipschitz of order 2._

This family allows us to consider many of the choices of \(\psi\) discussed in the previous section, including \(\psi(u,v)=u\) (AM on linear diagonal networks), \(\psi(u,v)=\sqrt{|uv|}\) (lin-RFM and IRLS), \(\psi(u,v)=\phi(u^{2}v^{2})\) for bounded \(\phi\) (lin-RFM). We note that this does _not_ include some choices of \(\psi\) which apply more "aggressive" weighting, such as \(\psi(u,v)=|uv|\). Nevertheless, we can apply our theoretical predictions for these choices of \(\psi\) after passing the weights through a bounded activation (such as a sigmoid). In Appendix D, we show that our predictions often still show excellent agreement with empirical simulation even when the boundedness assumption is violated.

Our results are stated in terms of the following iteration, for \(t\geq 0\):

\[\begin{split}&\tau_{t+1},\beta_{t+1}=\arg\max_{\tau\geq 0}\min_{ \beta\geq 0}\left\{\frac{\tau\sigma^{2}}{\beta}+\tau\beta(1-\kappa)-\tau^{2}+ \tau\lambda\operatorname{\mathbb{E}}_{(V,\Theta)\sim\Pi_{t}}\!\left[\frac{ \Theta^{2}+\beta^{2}\kappa}{\tau V^{2}+\beta\lambda}\right]\right\}\\ & Q_{t+1}=\frac{\tau_{t+1}V(\Theta+\beta_{t+1}\sqrt{\kappa}G_{t}) }{\tau_{t+1}V^{2}+\beta_{t+1}\lambda},\\ &\Pi_{t+1}=\text{Law}(\psi(Q_{t+1},V),\Theta),\end{split}\] (5)

where \(G_{t}\overset{\text{i.i.d.}}{\sim}\mathcal{N}(0,1)\). In words, given a probability distribution \(\Pi_{t}\) over \(\mathbb{R}\times\mathbb{R}\), \(\tau_{t+1}\) and \(\beta_{t+1}\) are scalars computed as the unique1 solutions to a _deterministic_ optimization problem (this can be solved easily by studying the optimality conditions, as shown in Appendix C). Then, \(Q_{t+1}\) is defined as a random variable that is a function of \((V,\Theta)\sim\Pi_{t}\) and \(G_{t}\sim\mathcal{N}(0,1)\). Lastly, \(\Pi_{t+1}\) is defined as the joint distribution of \(\psi(Q_{t+1},V)\) and \(\Theta\).

Footnote 1: The uniqueness of the solution is shown in the proof of Theorem 1.

Given this iteration, we obtain the following result, which is proved in Appendix A:

**Theorem 1**.: _Suppose Assumptions 1 and 2 are satisfied. Then, for any \(t\geq 0\) and any function \(g\colon\mathbb{R}^{3}\to\mathbb{R}\) such that \(g\in\text{PL}(2)\) or \(g\) is bounded and continuous, we have_

\[\frac{1}{d}\sum_{i=1}^{d}g(u_{i}^{(t+1)},v_{i}^{(t)},\theta_{i}^{*})\overset{P }{\to}\mathbb{E}[g(Q_{t+1},V,\Theta)],\]

_where the expectation is over the independent random variables \((V,\Theta)\sim\Pi_{t}\) and \(G_{t}\sim\mathcal{N}(0,1)\)._

The limit in this theorem should be interpreted as being the limit as \(n,d\to\infty\) with their ratio \(\kappa=\frac{d}{n}\) held as a constant. Applying the above theorem for each \(t\geq 0\), we can get precise asymptotic predictions for a wide variety of test functions of the iterates. One example of particular interest is the test error, which we measure as the normalized \(\ell_{1}\) distance between \(\bm{u}^{(t+1)}\odot\bm{v}^{(t)}\) and \(\bm{\theta}^{*}\), corresponding to \(g(u,v,\theta)=|uv-\theta|\) (we provide a proof that this is PL(2) in Proposition 1 in Appendix B). We note that the limiting expectation can be computed via simple Monte Carlo simulation of a _scalar_ random variable.

From a technical standpoint, our result is obtained by applying the Convex Gaussian Min-Max Theorem (CGMT) [30, 29] to the weighted least-squares objective function in (4). Previous works have obtained a similar distributional characterization for the solution to least-squares with anisotropic covariates (where the "weights" \(\bm{v}\) are the square root of the eigenvalues of the data covariance) [8]. However, while [8] assume that the eigenvalues are uniformly bounded by constants, this is not a reasonable assumption in our setting, since many common choices of \(\psi\) are not bounded and hence \(\bm{v}^{(t)}\) is not necessarily bounded uniformly for \(t>1\). A second key difference is that we need to obtain a distributional characterization which can be applied recursively for all \(t\geq 0\). The analysis in [8] assumes convergence of the initialization in \(\mathcal{W}_{4}\) distance and proves convergence of the estimator in \(\mathcal{W}_{3}\) distance. However, to apply the result recursively in our setting, if assume that the empirical distribution of \((\bm{v}^{(0)},\bm{\theta}^{*})\) converges in \(\mathcal{W}_{k}\) distance, then we need to show that after one iteration, the iterates also converge in \(\mathcal{W}_{k}\) distance (and not in any weaker sense).

To overcome these differences, we use a different technique to show distributional convergence of the iterates. Similar to the approach in [5], we apply the CGMT to a _perturbed_ optimization problem, which ultimately allows us to show convergence of test functions of the solutions to the unperturbed problem. While this approach necessitates the additional assumption that \(\bm{\theta}^{*}\) has bounded entries and we obtain results for a slightly smaller family of test functions \(g\) (note, for example, that the squared loss \(g(u,v,\theta)=(uv-\theta)^{2}\) is not \(\text{PL}(2)\)), we obtain a distributional convergence result that can be applied to a _sequence_ of recursively defined least-squares problems which define the trajectory of an algorithm, rather than to a single optimization problem. Moreover, our simulations in Appendix D suggest that the predictions of Theorem 1 still often apply without these additional assumptions, including in the case of the squared loss, indicating that these additional assumptions could potentially be weakened with a more complicated analysis.

### Application to sparse linear regression

In this subsection, we apply Theorem 1 to a sparse recovery setting and compare the asymptotic predictions to numerical simulations on high-dimensional Gaussian data. First, we consider a setting where \(n=250,d=2000\), and \(\bm{\theta}^{*}\) has \(\text{Bernoulli}(0.01)\) entries (so the expected sparsity level is \(\mathbb{E}[s]=20\)). We run Algorithm 4 with initialization \(\bm{v}^{(0)}=\bm{1}_{d}\) for four different choices of reweighting function and display the test error at each iteration (median over 100 trials) in Figure 1. For each choice of reweighting function \(\psi\), we choose the regularization parameter \(\lambda\) that minimizes the asymptotic test loss achieved within 8 iterations, and we plot the corresponding trajectory. As shown in the figure, the numerical simulations show excellent alignment with the asymptotic predictions even for this moderate choice of \(n\) and \(d\).

The asymptotic predictions show that this family of algorithms can find solutions with low test error within only a few iterations. Our results also reveal fine-grained differences in the convergence

Figure 1: Theoretical predictions and simulations of the test error \(\frac{1}{d}\|\bm{u}\odot\bm{v}-\bm{\theta}^{*}\|_{1}\) (log scale, pluses denote the median over 100 trials and the shaded region indicates the interquartile range) for two different noise levels, where \(n=250,d=2000\), and \(\bm{\theta}^{*}\) has \(\text{Bernoulli}(0.01)\) entries. Here, \(\psi=|uv|^{\frac{1}{2}}\) corresponds to the classical IRLS weighting from [11], \(\psi=\tanh|uv|\) is a version of lin-RFM, \(\psi=u\) corresponds to AM, and \(\psi=\tanh u^{2}\) is a new reweighting scheme we introduce. We note that the \(\psi\) which depend only on \(\bm{u}\) can lead to oscillatory behavior in the test risk.

behavior of the different algorithms. For instance, more aggressive weightings \(\psi=\tanh|uv|\) and \(\psi=\tanh u^{2}\) seem to find better solutions after several iterations. Interestingly, the weighting functions which depend only on \(u\) (like alternating minimization) sometimes display a non-monotonic, oscillatory decay of the test loss, particularly in the low-noise regime. However, we do see a steady decrease in test error after every _pair_ of iterations (e.g., in AM, after both parameters have been updated). Finally, we note that our framework allows for analysis of new algorithms for training LDNNs. In particular, to our knowledge, weighting functions of the form \(\phi(u^{2})\) have not been previously considered for this task, but our results indicate that this small modification to AM is competitive with many existing algorithms in this setting.

## 4 Grouped IRLS and the benefits of structured feature learning

In many scenarios, the unknown signal \(\bm{\theta}^{*}\) is known to possess additional structure that can be leveraged during training. One commonly studied example of this is _structured sparsity_, or group sparsity, where \(\bm{\theta}^{*}\) has many blocks which are zero. In this section, we generalize the results of Theorem 1 to the case where the reweighting function respects this additional structure in the signal, i.e., \(\psi\) acts on blocks of \(\bm{v}\), rather than on individual coordinates.

Concretely, we consider the following modification to our formulation. Let \(b\geq 1\) be a constant and write \(\mathbb{R}^{d}\) as a product space over \(M=\frac{d}{b}\) factors: \(\mathbb{R}^{b}\times\cdots\times\mathbb{R}^{b}\). Then, \(\bm{\theta}^{*},\bm{u}^{(t)},\bm{v}^{(t)}\in\mathbb{R}^{d}\) can all be represented as \(M\) stacked blocks, each in \(\mathbb{R}^{b}\). Under the same linear measurement model, we now let \(\psi:\mathbb{R}^{b}\times\mathbb{R}^{b}\rightarrow\mathbb{R}^{b}\) act on each of the _factors_ of \((\bm{u}^{(t)},\bm{\theta}^{*})\), and consider the same Algorithm 4.

Here, the case \(b=1\) recovers the results of the previous section, but the case \(b>1\) allows us to study the interplay between signal structure and reweighting scheme in a more fine-grained way. For example, suppose \(\bm{\theta}^{*}\) is known to be group-sparse, meaning that many of the factors \(\{\bm{\theta}^{*}_{i}\}_{i=1}^{M}\) are zero. In this case, it might make sense for \(\psi\) to return a vector of the form

\[\psi(\bm{u}^{(t)}_{i},\bm{v}^{(t)}_{i})=\alpha_{i}\mathbf{1}_{b},\]

for some \(\alpha_{i}\in\mathbb{R}\) that is chosen as a function \(\bm{u}^{(t)}_{i}\) and \(\bm{v}^{(t)}_{i}\). This corresponds to a reweighting scheme which acts on blocks, rather than individual entries. Another way to motivate this "grouped reweighting" is to leverage the connection to the \(\eta\)-trick, as in (3). In particular, the group Lasso problem can be written in the following variational form [2]:

\[\min_{\bm{\theta}\in\mathbb{R}^{d}}\min_{\bm{\eta}\in\mathbb{R}^{M}_{+}}L( \bm{\theta})+\frac{\lambda}{2}\sum_{i=1}^{M}\biggl{(}\frac{\|\bm{\theta}_{i} \|_{2}^{2}}{\eta_{i}}+\eta_{i}\biggr{)},\] (6)

where the closed-form solution to the \(\bm{\eta}\) minimization yields the classical group norm regularizer \(\sum_{i=1}^{M}\|\bm{\theta}_{i}\|_{2}\). Here, reparameterizing as \(\alpha_{i}\rightarrow\sqrt{\eta_{i}}\) and \(\bm{u}_{i}\rightarrow\frac{\bm{\theta}_{i}}{\sqrt{\eta_{i}}}\) gives rise naturally to the grouped Hadamard parameterization, with \(\bm{v}_{i}=\alpha_{i}\mathbf{1}_{b}\).

From the perspective of linear diagonal networks, such an approach is equivalent to "tying" together the weights of the hidden layer that correspond to each block. Rather than studying gradient descent/flow for this parameterization (as in [16; 17]), we consider an optimization approach that relies on alternate updates of \(\bm{u}^{(t)}\) and \(\bm{v}^{(t)}\).

We make the same technical assumptions as in Assumptions 1 and 2, with the natural modifications to accommodate \(b\geq 1\):

1. \(\Pi_{0}\) is the limit of the empirical distribution of factors of \((\bm{v}^{(0)},\bm{\theta}^{*})\) and hence is a distribution over \(\mathbb{R}^{b}\times\mathbb{R}^{b}\).
2. Each factor \(\bm{\theta}^{*}_{i}\in\mathbb{R}^{b}\) for \(i=1,\ldots,M\) has bounded \(\ell_{2}\)-norm almost surely.
3. \(\psi\) is bounded and continuous or each of its coordinate projections satisfies \(\psi^{2}_{j}\in\text{PL}(2)\) for \(j=1,\ldots,b\).

A straightforward extension of Theorem 1 yields the following generalization for the grouped algorithm, where \(\bm{V},\bm{\Theta},\bm{G}_{t},\bm{Q}_{t+1}\in\mathbb{R}^{b}\) are now vector-valued random variables: For \(t\geq 0\), let

\[\begin{split}&\tau_{t+1},\beta_{t+1}=\arg\max_{\tau\geq 0}\min_{ \beta\geq 0}\left\{\frac{\tau\sigma^{2}}{\beta}+\tau\beta(1-\kappa)-\tau^{2} +\tau\lambda\operatorname{\mathbb{E}}_{(\bm{V},\bm{\Theta})\sim\Pi_{t}}\!\left[ \frac{1}{b}\sum_{j=1}^{b}\frac{\Theta_{j}^{2}+\beta^{2}\kappa}{\tau V_{j}^{2}+ \beta\lambda}\right]\right\}\\ &\bm{Q}_{t+1}=\frac{\tau_{t+1}\bm{V}\odot(\bm{\Theta}+\beta_{t+1} \bm{G}_{t}\sqrt{\kappa})}{\tau_{t+1}\bm{V}^{\odot 2}+\beta_{t+1}\lambda\bm{1}_{b}},\text{ (entry-wise division)}\\ &\Pi_{t+1}=\text{Law}(\psi(\bm{Q}_{t+1},\bm{V}),\bm{\Theta}). \end{split}\] (7)

Here, \(\bm{G}_{t}\overset{\text{iid}}{\sim}\mathcal{N}(\bm{0},\bm{I}_{b})\). Then, we have the following result, which is proved in Appendix A.

**Theorem 2**.: _[Generalization of Theorem 1 for \(b\geq 1\)] Under the assumptions above, for any \(t\geq 0\) and any function \(g\colon(\mathbb{R}^{b})^{3}\to\mathbb{R}\) such that \(g\in\text{PL}(2)\) or \(g\) is bounded and continuous, we have_

\[\frac{1}{M}\sum_{i=1}^{M}g(\bm{u}_{i}^{(t+1)},\bm{v}_{i}^{(t)},\bm{\theta}_{i }^{*})\overset{P}{\to}\mathbb{E}[g(\bm{Q}_{t+1},\bm{V},\bm{\Theta})].\]

Given a reweighting function \(\psi\), Theorem 2 characterizes the distribution of the factors (blocks) of the iterates. Hence, by choosing \(g(\bm{u},\bm{v},\bm{\theta})=|\bm{u}\odot\bm{v}-\bm{\theta}|\), we can predict the exact limiting test error for this family of algorithms.

Computing these theoretical predictions reveals that choosing \(\psi\) in a group-aware way can lead to significant performance improvements compared to coordinate-wise reweighting. In Figure 2, we fix \(\sigma=0.1,n=500,d=4000\), and set the overall expected sparsity level of \(\bm{\theta}^{*}\) as in Figure 1. We compare the performance of Algorithm 4 for a "group-blind" (\(\psi_{gb}\)) and "group-aware" (\(\psi_{ga}\)) choice of reweighting function:

* \(\psi_{gb}(\bm{u},\bm{v})=\tanh|\bm{u}\odot\bm{v}|\) -- note this is identical to one of the reweightings considered in Section 3.1.
* \(\psi_{\text{ga}}(\bm{u},\bm{v})=\left(\frac{1}{b}\sum_{j=1}^{b}\tanh|u_{j}v_{ j}|\right)\bm{1}_{b}\)

The theoretical predictions align with simulations and show a notable improvement in performance when using the group-aware scheme with \(b>1\). Moreover, as the group size \(b\) increases, the performance of \(\psi_{gb}\) remains approximately the same, indicating that it is not able to take adapt to the

Figure 2: Group-blind (\(\psi_{gb}\)) vs. group-aware (\(\psi_{ga}\)) reweighting when \(\bm{\theta}^{*}\) has group-sparse structure. We set \(n=500,d=4000,\sigma=0.1\), and \(\bm{\theta}_{i}^{*}\overset{\text{iid}}{\sim}\text{Bernoulli}(0.01)\bm{1}_{b}\). For each curve, \(\lambda\) is set to minimize the asymptotic test error achieved. Simulation results are the median/IQR over 100 trials. Left: Comparison of the test error trajectory (log scale) for a fixed block size \(b=8\). Right: \(\ell_{1}\) test error after \(T=4\) iterations, for varying group sizes.

group-structure. By contrast, using \(\psi_{ga}\) leads to a consistent improvement in test error as \(b\) gets larger. Hence, the test error when using the group-aware scheme scales with the size/number of groups, rather than the overall sparsity level.

## 5 Conclusion

In this paper, we derived a precise asymptotic characterization of the iterates of a family of algorithms for learning high-dimensional linear models with linear diagonal networks. We used these predictions to obtain fine-grained predictions of the test error at each iteration for various existing algorithms for this task, and we showed that our framework can also be used as a test bed for new variations on these algorithms that take a similar form. Lastly, we demonstrated the advantage of embedding more structure into the model by using together groups of weights when the ground-truth has structured sparsity. Several interesting open questions about these types of algorithms remain. While our simulations align very well with the predicted asymptotic trajectory, it would be interesting to obtain finite-sample guarantees that hold even for batch sizes that are much smaller than \(d\) (as in the "mini-batch" case studied by [19]). Moreover, seeing as our analysis depends crucially on the independence the covariates at every iteration, developing precise predictions of the trajectory in the non-batched setting remains an interesting direction for future work.

## Acknowledgments and Disclosure of Funding

We thank the anonymous reviewers for their helpful feedback. This work was supported by an NSF Graduate Research Fellowship (DGE-2039655), the NSF AI Institute AI4OPT, NSF grants IIS-2212182, CCF-223915 and 2112533, and gifts from Amazon and Adobe.

## References

* Bach [2019] Francis Bach. The "\(\eta\)-trick" or the effectiveness of reweighted least-squares. https://francisbach.com/the-%CE/B7-trick-or-the-effectiveness-of-reweighted-least-squares/, 2019. Accessed: April 2024.
* Bach et al. [2012] Francis Bach, Rodolphe Jenatton, Julien Mairal, Guillaume Obozinski, et al. Optimization with sparsity-inducing penalties. _Foundations and Trends(r) in Machine Learning_, 4(1):1-106, 2012.
* Bayati and Montanari [2011] Mohsen Bayati and Andrea Montanari. The dynamics of message passing on dense graphs, with applications to compressed sensing. _IEEE Transactions on Information Theory_, 57(2):764-785, 2011.
* Black and Rangarajan [1996] Michael J Black and Anand Rangarajan. On the unification of line processes, outlier rejection, and robust statistics with applications in early vision. _International Journal of Computer Vision_, 19(1):57-91, 1996.
* Bosch et al. [2023] David Bosch, Ashkan Panahi, Ayca Ozcelikkale, and Devdatt Dubhashi. Random features model with general convex regularization: A fine grained analysis with precise asymptotic learning curves. In _International Conference on Artificial Intelligence and Statistics_, pages 11371-11414. PMLR, 2023.
* Chandrasekher et al. [2022] Kabir Aladin Chandrasekher, Mengqi Lou, and Ashwin Pananjady. Alternating minimization for generalized rank one matrix sensing: Sharp predictions from a random initialization. _arXiv preprint arXiv:2207.09660_, 2022.
* Chandrasekher et al. [2023] Kabir Aladin Chandrasekher, Ashwin Pananjady, and Christos Thrampoulidis. Sharp global convergence guarantees for iterative nonconvex optimization with random data. _The Annals of Statistics_, 51(1):179-210, 2023.
* Chang et al. [2021] Xiangyu Chang, Yingcong Li, Samet Oymak, and Christos Thrampoulidis. Provable benefits of overparameterization in model compression: From double descent to pruning neural networks. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 35, pages 6974-6983, 2021.

* Chartrand and Yin [2008] Rick Chartrand and Wotao Yin. Iteratively reweighted algorithms for compressive sensing. In _2008 IEEE International Conference on Acoustics, Speech and Signal Processing_, pages 3869-3872. IEEE, 2008.
* Chou et al. [2023] Hung-Hsu Chou, Johannes Maly, and Holger Rauhut. More is less: inducing sparsity via overparameterization. _Information and Inference: A Journal of the IMA_, 12(3):1437-1460, 2023.
* Daubechies et al. [2010] Ingrid Daubechies, Ronald DeVore, Massimo Fornasier, and C Sinan Gunturk. Iteratively reweighted least squares minimization for sparse recovery. _Communications on Pure and Applied Mathematics: A Journal Issued by the Courant Institute of Mathematical Sciences_, 63(1):1-38, 2010.
* Geman and Reynolds [1992] Donald Geman and George Reynolds. Constrained restoration and the recovery of discontinuities. _IEEE Transactions on Pattern Analysis & Machine Intelligence_, 14(03):367-383, 1992.
* Hoff [2017] Peter D Hoff. Lasso, fractional norm and structured sparse estimation using a Hadamard product parametrization. _Computational Statistics & Data Analysis_, 115:186-198, 2017.
* Jain et al. [2013] Prateek Jain, Praneeth Netrapalli, and Sujay Sanghavi. Low-rank matrix completion using alternating minimization. In _Proceedings of the 45th Annual ACM Symposium on Theory of Computing_, pages 665-674, 2013.
* Kolb et al. [2023] Chris Kolb, Christian L Muller, Bernd Bischl, and David Rugamer. Smoothing the edges: A general framework for smooth optimization in sparse regularization using Hadamard overparametrization. _arXiv preprint arXiv:2307.03571_, 2023.
* Kumar et al. [2023] Akshay Kumar, Akshay Malhotra, and Shahab Hamidi-Rad. Group sparsity via implicit regularization for MIMO channel estimation. In _2023 IEEE Wireless Communications and Networking Conference (WCNC)_, pages 1-6. IEEE, 2023.
* Li et al. [2023] Jiangyuan Li, Thanh V Nguyen, Chinmay Hegde, and Raymond KW Wong. Implicit regularization for group sparsity. _arXiv preprint arXiv:2301.12540_, 2023.
* Loh and Wainwright [2011] Po-Ling Loh and Martin J Wainwright. High-dimensional regression with noisy and missing data: Provable guarantees with non-convexity. _Advances in Neural Information Processing Systems_, 24, 2011.
* Lou et al. [2024] Mengqi Lou, Kabir Aladin Verchand, and Ashwin Pananjady. Hyperparameter tuning via trajectory predictions: Stochastic prox-linear methods in matrix sensing. _arXiv preprint arXiv:2402.01599_, 2024.
* Micchelli et al. [2013] Charles A Micchelli, Jean M Morales, and Massimiliano Pontil. Regularizers for structured sparsity. _Advances in Computational Mathematics_, 38:455-489, 2013.
* Mohan and Fazel [2012] Karthik Mohan and Maryam Fazel. Iterative reweighted algorithms for matrix rank minimization. _The Journal of Machine Learning Research_, 13(1):3441-3473, 2012.
* Newey and McFadden [1994] Whitney K Newey and Daniel McFadden. Large sample estimation and hypothesis testing. _Handbook of Econometrics_, 4:2111-2245, 1994.
* Pesme et al. [2021] Scott Pesme, Loucas Pillaud-Vivien, and Nicolas Flammarion. Implicit bias of sgd for diagonal linear networks: a provable benefit of stochasticity. _Advances in Neural Information Processing Systems_, 34:29218-29230, 2021.
* Pollard [1991] David Pollard. Asymptotics for least absolute deviation regression estimators. _Econometric Theory_, 7(2):186-199, 1991.
* Poon and Peyre [2021] Clarice Poon and Gabriel Peyre. Smooth bilevel programming for sparse regularization. _Advances in Neural Information Processing Systems_, 34:1543-1555, 2021.
* Poon and Peyre [2023] Clarice Poon and Gabriel Peyre. Smooth over-parameterized solvers for non-smooth structured optimization. _Mathematical Programming_, 201(1):897-952, 2023.

* [27] Adityanarayanan Radhakrishnan, Daniel Beaglehole, Parthe Pandit, and Mikhail Belkin. Mechanism for feature learning in neural networks and backpropagation-free machine learning models. _Science_, 383(6690):1461-1467, Mar 2024.
* [28] Adityanarayanan Radhakrishnan, Mikhail Belkin, and Dmitriy Drusvyatskiy. Linear recursive feature machines provably recover low-rank matrices. _arXiv preprint arXiv:2401.04553_, 2024.
* [29] Christos Thrampoulidis, Ehsan Abbasi, and Babak Hassibi. Precise error analysis of regularized \(m\)-estimators in high dimensions. _IEEE Transactions on Information Theory_, 64(8):5592-5628, 2018.
* [30] Christos Thrampoulidis, Samet Oymak, and Babak Hassibi. Regularized linear regression: A precise analysis of the estimation error. In _Conference on Learning Theory_, pages 1683-1709. PMLR, 2015.
* [31] Tomas Vaskevicius, Varun Kanade, and Patrick Rebeschini. Implicit regularization for optimal sparse recovery. _Advances in Neural Information Processing Systems_, 32, 2019.
* [32] Roman Vershynin. _High-dimensional Probability: An Introduction with Applications in Data Science_, volume 47. Cambridge university press, 2018.
* [33] Cedric Villani et al. _Optimal Transport: Old and New_, volume 338. Springer, 2009.
* [34] Blake Woodworth, Suriya Gunasekar, Jason D Lee, Edward Moroshko, Pedro Savarese, Itay Golan, Daniel Soudry, and Nathan Srebro. Kernel and rich regimes in overparametrized models. In _Conference on Learning Theory_, pages 3635-3673. PMLR, 2020.
* [35] Peng Zhao, Yun Yang, and Qiao-Chu He. High-dimensional linear regression via implicit regularization. _Biometrika_, 109(4):1033-1046, 2022.
* [36] Liu Ziyin and Zihao Wang. spred: Solving \(L_{1}\) penalty with SGD. In _International Conference on Machine Learning_, pages 43407-43422. PMLR, 2023.

Proof of main results

In this section, we provide the proofs of our main results.

### Notation and background

For convenience, we first restate the main notation that is used in our proofs.

**Notation** The ones vector of dimension \(d\) is denoted \(\mathbf{1}_{d}\). We write \(a\lesssim b\) when \(a\leq Cb\) for some sufficiently large constant \(C>0\) which does not depend on \(d\). We denote the element-wise multiplication (Hadamard product) of two vectors \(\bm{x}\) and \(\bm{y}\) as \(\bm{x}\odot\bm{y}\). Element-wise division of two vectors is denoted as \(\frac{\bm{x}}{\bm{y}}\). We use the shorthand \((\cdot)_{+}=\max(\cdot,0)\). A function \(f\colon\mathbb{R}^{p}\to\mathbb{R}\) is called _pseudo-Lipschitz_ of order \(2\) if, for all \(\bm{x},\bm{y}\in\mathbb{R}^{p}\),

\[|f(\bm{x})-f(\bm{y})|\leq C(1+\|\bm{x}\|_{2}+\|\bm{y}\|_{2})\|\bm{x}-\bm{y}\|_{2}\]

for some constant \(C>0\). The set of such functions is denoted PL(2).

Convergence in probability of a sequence of random variables \(X_{d}\) to a random variable \(X\) is denoted \(X_{d}\stackrel{{ P}}{{\to}}X\). Convergence in Wasserstein-2 distance of a sequence of probability distributions \(\nu_{d}\) to a limiting distribution \(\nu\) is denoted as \(\nu_{d}\stackrel{{\mathcal{W}_{2}}}{{\to}}\nu\), and this fact is equivalent to the statement \(\mathbb{E}_{X\sim\nu_{d}}g(X)\to\mathbb{E}_{X\sim\nu}\,g(X)\) for all \(g\in\) PL(2) [3]. If the \(\nu_{d}\) are _random_ probability measures, we say that \(\nu_{d}\stackrel{{\mathcal{W}_{2}}}{{\to}}\nu\) if the same convergence holds in probability, i.e., \(\mathbb{E}_{X\sim\nu_{d}}\,g(X)\stackrel{{ P}}{{\to}}\mathbb{E}_{X \sim\nu}\,g(X)\) for all \(g\in\) PL(2). The empirical distribution of a vector \(\bm{z}\in\mathbb{R}^{d}\) is defined as \(\frac{1}{d}\sum_{i=1}^{d}\delta(z_{i})\), where \(\delta(z_{i})\) is the Dirac delta distribution centered at \(z_{i}\). For any random variable (or group of random variables) \(X\), we use the notation \(\text{Law}(X)\) to denote the probability distribution of \(X\).

We also define two key quantities which appear in the analysis.

**Definition 1**.: _The Moreau envelope function of a lower semi-continuous, proper convex function \(\ell\colon\mathbb{R}^{p}\to\mathbb{R}\) with step size \(\tau\) is defined as_

\[\mathcal{M}_{\ell}(\bm{x};\tau)=\min_{\bm{y}\in\mathbb{R}^{p}}\ell(\bm{y})+ \frac{1}{2\tau}\|\bm{y}-\bm{x}\|_{2}^{2}.\]

_The proximal (prox) operator of \(\ell\) with step size \(\tau\), denoted \(\text{prox}_{\ell}(\bm{x},\tau)\) is defined as the \(\arg\min\) of the above optimization problem._

Lastly, we restate the version of the Convex Gaussian Min-Max Theorem (CGMT) that we will use in our proofs.

**Theorem 3** (Convex Gaussian Min-Max Theorem [29]).: _Let \(\bm{G}\in\mathbb{R}^{m\times n},\bm{g}\in\mathbb{R}^{m},\bm{h}\in\mathbb{R}^{n}\) have i.i.d. \(\mathcal{N}(0,1)\) entries. Let \(\mathcal{S}_{w}\subset\mathbb{R}^{n}\) and \(\mathcal{S}_{u}\subset\mathbb{R}^{m}\) be compact, convex sets, and \(f\colon\mathbb{R}^{n}\times\mathbb{R}^{m}\to\mathbb{R}\) be convex-concave on \(\mathcal{S}_{w}\times\mathcal{S}_{u}\). Define the following two min-max problems:_

\[\Phi(\bm{G}) \coloneqq\min_{\bm{w}\in\mathcal{S}_{w}}\max_{\bm{u}\in\mathcal{S} _{u}}\bm{u}^{\top}\bm{G}\bm{w}+f(\bm{w},\bm{u})\] \[\phi(\bm{g},\bm{h}) \coloneqq\min_{\bm{w}\in\mathcal{S}_{w}}\max_{\bm{u}\in\mathcal{S} _{u}}\|\bm{w}\|_{2}\bm{u}^{\top}\bm{g}+\|\bm{u}\|_{2}\bm{w}^{\top}\bm{h}+f( \bm{w},\bm{u})\]

_Then, for all \(c\in\mathbb{R}\) and \(t>0\),_

\[\mathbb{P}\{|\Phi(\bm{G})-c|>t\}\leq 2\,\mathbb{P}\{|\phi(\bm{g},\bm{h})-c|>t\}\]

### Proof of Theorems 1 and 2

Proof of Theorem 1.: Assume that \(\frac{1}{d}\sum_{i=1}^{d}\delta(v_{i}^{(t)},\theta_{i}^{*})\stackrel{{ \mathcal{W}_{2}}}{{\to}}\Pi_{t}\) (note that this holds by assumption at \(t=0\); we will show later that it holds at time \(t+1\)).

First observe that convergence of the joint empirical distribution of \((\bm{u}^{(t+1)},\bm{v}^{(t)},\bm{\theta}^{*})\) to the joint distribution of \((Q_{t+1},V,\Theta)\) in Wasserstein-2 distance implies that

\[\frac{1}{d}\sum_{i=1}^{d}g(u_{i}^{(t+1)},v_{i}^{(t)},\theta_{i}^{*})\stackrel{{ P}}{{\to}}\mathbb{E}[g(Q_{t+1},V,\Theta)],\]for any \(g\in\text{PL}(2)\) or which is bounded and continuous. This is because \(\mathcal{W}_{2}\) convergence implies convergence in expectation of any pseudo-Lipschitz function of order 2 [3, Lemma 5] and of any bounded continuous function (since \(\mathcal{W}_{2}\) convergence is stronger than weak convergence [33, Theorem 6.9]). Hence, it suffices to show that

\[\frac{1}{d}\sum_{i=1}^{d}\delta(u_{i}^{(t+1)},v_{i}^{(t)},\theta_{i}^{*}) \overset{\mathcal{W}_{2}}{\rightarrow}\text{Law}(Q_{t+1},V,\Theta),\] (8)

where \((V,\Theta)\sim\Pi_{t}\) and \(Q_{t+1}\) is defined as in Eq. 5.

Recall that the objective function for the update on \(\bm{u}\) is given by

\[\bm{u}^{(t+1)}=\arg\min_{\bm{u}\in\mathbb{R}^{d}}\frac{1}{n}\norm{\bm{y}^{(t)} -\frac{1}{\sqrt{d}}\bm{X}^{(t)}(\bm{u}\odot\bm{v}^{(t)})}_{2}^{2}+\frac{ \lambda}{d}\norm{\bm{u}}_{2}^{2}.\]

Rather than study this update directly, we first analyze a slightly more general problem (following the approach in [5]). Let \(h\colon\mathbb{R}^{3}\rightarrow\mathbb{R}\) be a continuous test function with \(\norm{\nabla^{2}h}_{2}\leq C\) and that satisfies one of the following:

1. \(h\) is uniformly bounded.
2. \(h(u,v,\theta)=u^{2}\).

Then we consider the following problem (the dependence of \(\bm{X},\bm{y}\), \(\bm{\epsilon}\), and \(\bm{v}\) on \(t\) is dropped to simplify the notation):

\[P_{1}(\mu)=\min_{\bm{u}\in\mathbb{R}^{d}}\frac{1}{n}\norm{\bm{y}-\frac{1}{ \sqrt{d}}\bm{X}(\bm{u}\odot\bm{v})}_{2}^{2}+\frac{\lambda}{d}\norm{\bm{u}}_{2} ^{2}+\frac{\mu}{d}\sum_{i=1}^{d}h(u_{i},v_{i},\theta_{i}^{*}),\] (9)

where \(\mu\in[-\mu^{*},\mu^{*}]\) and \(\mu^{*}=\frac{\lambda}{d}\) is chosen sufficiently small so that the objective function (scaled by \(d\)) is \(\lambda\)-strongly convex for all \(\mu\) in this range. The case \(\mu=0\) recovers the original problem of interest.

Step 1: Convergence of the lossRewriting this in terms of the error vector \(\bm{\Delta}\coloneqq\frac{1}{\sqrt{d}}(\bm{u}\odot\bm{v}-\bm{\theta}^{*})\), we have

\[P_{1}(\mu)=\min_{\bm{\Delta}\in\mathbb{R}^{d}}\frac{1}{n}\norm{\bm{\epsilon}- \bm{X}\bm{\Delta}}_{2}^{2}+\frac{\lambda}{d}\norm{\frac{\sqrt{d}\bm{\Delta}+ \bm{\theta}^{*}}{\bm{v}}}_{2}^{2}+\frac{\mu}{d}\sum_{i=1}^{d}h\Bigg{(}\frac{ \sqrt{d}\Delta_{i}+\theta_{i}^{*}}{v_{i}},v_{i},\theta_{i}^{*}\Bigg{)}.\]

In writing this, we use the fact that \(v_{i}\neq 0\) for all \(i\) with probability \(1\) (and the notation in the second-to-last term indicates entry-wise division). Now, using the identity \(\norm{\cdot}_{2}^{2}=\max_{\bm{q}}\bm{2}\bm{q}^{\top}(\cdot)-\norm{\bm{q}}_{2}^ {2}\), we can write this as

\[P_{1}(\mu)=\min_{\bm{\Delta}\in\mathbb{R}^{d}}\max_{\bm{q}\in \mathbb{R}^{n}}\frac{2}{\sqrt{n}}\bm{q}^{\top}\bm{\epsilon}-\frac{2}{\sqrt{n}} \bm{q}^{\top}\bm{X}\bm{\Delta}-\norm{\bm{q}}_{2}^{2}+\frac{\lambda}{d}\norm{ \frac{\sqrt{d}\bm{\Delta}+\bm{\theta}^{*}}{\bm{v}}}_{2}^{2}+\frac{\mu}{d}\sum_ {i=1}^{d}h\Bigg{(}\frac{\sqrt{d}\Delta_{i}+\theta_{i}^{*}}{v_{i}},v_{i},\theta _{i}^{*}\Bigg{)}.\] (10)

Next, in Lemma 1, we show that there exist Euclidean balls \(B_{\Delta}\) and \(B_{q}\), each of radius \(C_{1}\norm{\bm{v}}_{\infty}\) such that, with probability approaching \(1\), we can constrain the feasible set to lie with these balls without changing the value of \(P_{1}(\mu)\), so we can study

\[\tilde{P}_{1}(\mu)=\min_{\bm{\Delta}\in B_{\Delta}}\max_{\bm{q}\in B_{q}}\frac{ 2}{\sqrt{n}}\bm{q}^{\top}\bm{\epsilon}-\frac{2}{\sqrt{n}}\bm{q}^{\top}\bm{X} \bm{\Delta}-\norm{\bm{q}}_{2}^{2}+\frac{\lambda}{d}\norm{\frac{\sqrt{d}\bm{ \Delta}+\bm{\theta}^{*}}{\bm{v}}}_{2}^{2}+\frac{\mu}{d}\sum_{i=1}^{d}h\Bigg{(} \frac{\sqrt{d}\Delta_{i}+\theta_{i}^{*}}{v_{i}},v_{i},\theta_{i}^{*}\Bigg{)},\] (11)

where \(P_{1}(\mu)=\tilde{P}_{1}(\mu)\) with probability tending to \(1\). We can therefore condition on this event for the remainder of the analysis without changing our asymptotic conclusions.

Now, noting that this is in the correct form to apply Theorem 3, we define the auxiliary optimization problem

\[P_{2}(\mu)=\min_{\bm{\Delta}\in B_{\Delta}}\max_{\bm{q}\in B_{q}} \frac{2}{\sqrt{n}}\bm{q}^{\top}\bm{\epsilon}-\frac{2}{\sqrt{n}}\|\bm{q}\|_{2} \bm{g}^{\top}\bm{\Delta}-\frac{2}{\sqrt{n}}\|\bm{\Delta}\|_{2}\bm{h}^{\top}\bm{ q}-\|\bm{q}\|_{2}^{2}+\frac{\lambda}{d}\Bigg{\|}\frac{\sqrt{d}\bm{\Delta}+\bm{ \theta}^{*}}{\bm{v}}\Bigg{\|}_{2}^{2}\] (12) \[+\frac{\mu}{d}\sum_{i=1}^{d}h\Bigg{(}\frac{\sqrt{d}\Delta_{i}+ \theta_{i}^{*}}{v_{i}},v_{i},\theta_{i}^{*}\Bigg{)},\] (13)

where \(\bm{g}\in\mathbb{R}^{d}\) and \(\bm{h}\in\mathbb{R}^{n}\) have i.i.d. standard normal entries. By Theorem 3, for all \(\delta>0\) and fixed \(\bar{P}(\mu)\in\mathbb{R}\),

\[\mathbb{P}\{|P_{1}(\mu)-\bar{P}(\mu)|>\delta\}\leq 2\,\mathbb{P}\{|P_{2}(\mu)- \bar{P}(\mu)|>\delta\}.\]

In particular, if we can find some \(\bar{P}(\mu)\) such that \(P_{2}(\mu)\overset{P}{\rightarrow}\bar{P}(\mu)\), then we can conclude also that \(P_{1}(\mu)\overset{P}{\rightarrow}\bar{P}(\mu)\).

To accomplish this, we next perform a series of simplifications to \(P_{2}(\mu)\) which will later help us characterize its asymptotic behavior. First, we can decouple the optimization over \(\bm{q}\) into its norm and direction, and the latter can be solved explicitly. Letting \(\tau=\|\bm{q}\|_{2}\), this yields

\[P_{2}(\mu)=\min_{\bm{\Delta}\in B_{\Delta}}\max_{0\leq\tau\leq R }\frac{2\tau}{\sqrt{n}}\|\bm{\epsilon}-\|\bm{\Delta}\|_{2}\bm{h}\|_{2}-\frac{2 \tau}{\sqrt{n}}\bm{g}^{\top}\bm{\Delta}-\tau^{2}+\frac{\lambda}{d}\Bigg{\|} \frac{\sqrt{d}\bm{\Delta}+\bm{\theta}^{*}}{\bm{v}}\Bigg{\|}_{2}^{2}\] (14) \[+\frac{\mu}{d}\sum_{i=1}^{d}h\Bigg{(}\frac{\sqrt{d}\Delta_{i}+ \theta_{i}^{*}}{v_{i}},v_{i},\theta_{i}^{*}\Bigg{)},\]

where \(R\coloneqq C_{1}\|\bm{v}\|_{\infty}\). Next, note that \(\bm{\epsilon}\) and \(\bm{h}\) are independent Gaussian vectors and hence \(\bm{\epsilon}-\|\bm{\Delta}\|_{2}\bm{h}\overset{d}{=}\sqrt{\sigma^{2}+\|\bm{ \Delta}\|_{2}^{2}}\bm{h}\). So, we have

\[P_{2}(\mu)\overset{d}{=}\min_{\bm{\Delta}\in B_{\Delta}}\max_{0 \leq\tau\leq R}\frac{2\tau\|\bm{h}\|_{2}}{\sqrt{n}}\sqrt{\sigma^{2}+\|\bm{ \Delta}\|_{2}^{2}}-\frac{2\tau}{\sqrt{n}}\bm{g}^{\top}\bm{\Delta}-\tau^{2}+ \frac{\lambda}{d}\Bigg{\|}\frac{\sqrt{d}\bm{\Delta}+\bm{\theta}^{*}}{\bm{v}} \Bigg{\|}_{2}^{2}\] (15) \[+\frac{\mu}{d}\sum_{i=1}^{d}h\Bigg{(}\frac{\sqrt{d}\Delta_{i}+ \theta_{i}^{*}}{v_{i}},v_{i},\theta_{i}^{*}\Bigg{)}.\]

Before proceeding further, we rewrite this in terms of a minimization over the variable \(\bm{u}=\frac{\sqrt{d}\bm{\Delta}+\bm{\theta}^{*}}{\bm{v}}\).

\[P_{2}(\mu)\overset{d}{=}\min_{\bm{u}\in B_{u}}\max_{0\leq\tau \leq R}\frac{2\tau\|\bm{h}\|_{2}}{\sqrt{n}}\sqrt{\sigma^{2}+\frac{1}{d}\|\bm{ u}\odot\bm{v}-\bm{\theta}^{*}\|_{2}^{2}}-\frac{2\tau}{\sqrt{nd}}\bm{g}^{\top}( \bm{u}\odot\bm{v}-\bm{\theta}^{*})\] (16) \[-\tau^{2}+\frac{\lambda}{d}\|\bm{u}\|_{2}^{2}+\frac{\mu}{d}\sum_{ i=1}^{d}h(u_{i},v_{i},\theta_{i}^{*})\]

Here, \(B_{u}\coloneqq\Big{\{}\bm{u}\in\mathbb{R}^{d}\colon\Big{\|}\frac{1}{\sqrt{d}}( \bm{u}\odot\bm{v}-\bm{\theta}^{*})\Big{\|}_{2}\leq R\Big{\}}\). After this step, observe that the objective function is strongly concave in \(\tau\) and strongly convex in \(\bm{u}\) (the sum of the last two terms is strongly convex in \(\bm{u}\) based on the assumption that \(h\) has bounded Hessian and \(\mu\) is sufficiently small). Since the objective function is convex-concave over convex and compact sets, we can invoke Sion's minimax theorem to switch the \(\min\) and \(\max\). Furthermore, we use the fact that \(\sqrt{x}=\min_{\beta>0}\frac{x}{2\beta}+\frac{\beta}{2}\) to write this as

\[P_{2}(\mu)\overset{d}{=}\max_{0\leq\tau\leq R}\min_{\begin{subarray} {c}\bm{u}\in B_{u},\\ \sigma\leq\beta\leq\sigma+R\end{subarray}}\frac{\tau\|\bm{h}\|_{2}}{\sqrt{n}} \Bigg{(}\frac{\sigma^{2}}{\beta}+\frac{\|\bm{u}\odot\bm{v}-\bm{\theta}^{*}\|_{2 }^{2}}{\beta d}+\beta\Bigg{)}-\frac{2\tau}{\sqrt{nd}}\bm{g}^{\top}(\bm{u}\odot \bm{v}-\bm{\theta}^{*})\] (17) \[-\tau^{2}+\frac{\lambda}{d}\|\bm{u}\|_{2}^{2}+\frac{\mu}{d}\sum_{ i=1}^{d}h(u_{i},v_{i},\theta_{i}^{*}).\]Here, note that we can add the constraint on \(\beta\) without changing the solution since the optimal value of \(\beta\) will be obtained at \(\sqrt{\sigma^{2}+\frac{1}{d}\norm{\bm{u}\odot\bm{v}-\bm{\theta}^{*}}_{2}^{2}}\in[ \sigma,\sigma+R]\) for all feasible \(\bm{u}\).

Next, we can explicitly solve the inner minimization over \(\bm{u}\). To do this, we first show in Lemma 2 that the optimal solution to the unconstrained minimization is strictly feasible for large enough \(C_{1}\), and hence, the unconstrained and constrained minimizations over \(\bm{u}\) are equivalent. Next, observe that the unconstrained problem is separable over the indices, so we need only to solve the scalar problem

\[\min_{u_{i}\in\mathbb{R}}\frac{\tau\norm{\bm{h}}_{2}(u_{i}v_{i}-\theta_{i}^{*}) ^{2}}{\beta d\sqrt{n}}g_{i}(u_{i}v_{i}-\theta_{i}^{*})+\frac{\lambda}{d}u_{i} ^{2}+\frac{\mu}{d}h(u_{i},v_{i},\theta_{i}^{*})\] (18)

Completing the squares, we obtain that the above problem can be written in terms of the Moreau envelope (Definition 1) of the function \(\ell(u)=\lambda u^{2}+\mu h\):

\[\frac{1}{d}\Bigg{[}\frac{\tau\xi\theta_{i}^{*2}}{\beta}-\frac{\tau}{\beta\xi} \big{(}\beta g_{i}\sqrt{\kappa}-\xi\theta_{i}^{*}\big{)}^{2}+\mathcal{M}_{ \lambda(\cdot)^{2}+\mu h(\cdot,v_{i},\theta_{i}^{*})}\Bigg{(}\frac{\xi\theta_{ i}^{*}-\beta g_{i}\sqrt{\kappa}}{\xi v_{i}};\frac{\beta}{2\xi\tau v_{i}^{(t)2}} \Bigg{)}\Bigg{]},\]

where we have introduced the shorthand notation \(\xi=\frac{\norm{\bm{h}}_{2}}{\sqrt{n}}\). Substituting this into the expression for \(P_{2}\) above, we obtain

\[P_{2}(\mu)\stackrel{{ d}}{{=}}\max_{0\leq\tau\leq R }\min_{\sigma\leq\beta\leq\sigma+R}\frac{\tau\sigma^{2}\xi}{\beta} +\tau\beta\xi-\tau^{2}+\frac{1}{d}\sum_{i=1}^{d}\bigg{[}\frac{ \tau\xi\theta_{i}^{*2}}{\beta}-\frac{\tau}{\beta\xi}\big{(}\beta g_{i}\sqrt{ \kappa}-\xi\theta_{i}^{*}\big{)}^{2}\bigg{]}\] (19) \[+\frac{1}{d}\sum_{i=1}^{d}\Bigg{[}\mathcal{M}_{\lambda(\cdot)^{2} +\mu h(\cdot,v_{i},\theta_{i}^{*})}\Bigg{(}\frac{\xi\theta_{i}^{*}-\beta g_{i} \sqrt{\kappa}}{\xi v_{i}};\frac{\beta}{2\xi\tau v_{i}^{(t)2}}\Bigg{)}\Bigg{]}\] \[\coloneqq\max_{0\leq\tau\leq R}\min_{\sigma\leq\beta\leq\sigma+ R}f_{d}(\tau,\beta)\]

Now that the optimization has been fully "scalarized", we proceed by considering its asymptotic behavior. First, note that the partial minimization over \(\bm{u}\) preserves the concavity/convexity in \((\tau,\beta)\). In Lemma 3, we prove that for any fixed \(\tau\) and \(\beta\), the objective function \(f_{d}(\tau,\beta)\) converges in probability to

\[f(\tau,\beta)=\frac{\tau\sigma^{2}}{\beta}+\tau\beta(1-\kappa)-\tau^{2}+ \mathbb{E}\bigg{[}\mathcal{M}_{\lambda(\cdot)^{2}+\mu h(\cdot,V,\Theta)} \bigg{(}\frac{\Theta-\beta G\sqrt{\kappa}}{V};\frac{\beta}{2\tau V^{2}}\bigg{)} \bigg{]},\] (20)

where the expectation is over \((V,\Theta)\sim\Pi_{t}\) and an independent \(G\sim\mathcal{N}(0,1)\). We note here that Lemma 3 is the only place in our proof which requires the boundedness of the entries of \(\bm{\theta}^{*}\).

Since \(f_{d}(\tau,\beta)\) is strongly concave in \(\tau\) with parameter \(1\) for all feasible \(\beta\), we can conclude that \(f(\tau,\beta)\) is also strongly concave in \(\tau\) with parameter \(1\). Directly taking a derivative with respect to \(\beta\), we also find that \(f\) has a single non-negative critical point, at the point \(\hat{\beta}=\sqrt{\sigma^{2}+\mathbb{E}[(\hat{u}V-\Theta)^{2}]}\), where

\[\hat{u}=\hat{u}(V,\Theta)=\text{prox}_{\lambda(\cdot)^{2}+\mu h(\cdot,V, \Theta)}\bigg{(}\frac{\Theta-\beta G\sqrt{\kappa}}{V};\frac{\beta}{2\tau V^{ 2}}\bigg{)}.\]

So, we can conclude that \(f\) has unique saddle point \((\hat{\tau},\hat{\beta})\). Note \(f\) is a deterministic function that does not depend on \(d\) and hence \((\hat{\tau},\hat{\beta})\) are also deterministic and independent of \(d\).

Now let \(C_{2}\coloneqq\max\{\hat{\tau},\hat{\beta}\}+1\). By the "convexity lemma" (as stated in [24]), pointwise convergence (in probability) of a convex function is uniform over compact sets. So, this result implies that the convergence is uniform over \((\tau,\beta)\in[0,C_{2}]\times[0,C_{2}]\),2 so

Footnote 2: Note we could not have directly applied this to the feasible sets of \(P_{2}(\mu),\) since \(R\) may have a dependence on \(d\).

\[\max_{0\leq\tau\leq C_{2}}\min_{0\leq\beta\leq C_{2}}f_{d}(\tau,\beta) \stackrel{{ P}}{{\to}}\max_{0\leq\tau\leq C_{2}}\min_{0\leq\beta\leq C _{2}}f(\tau,\beta)\]

Let \((\hat{\tau}_{d},\hat{\beta}_{d})\) denote the optimnal solution for the problem on the left. We can also conclude that \((\hat{\tau}_{d},\hat{\beta}_{d})\stackrel{{ P}}{{\to}}(\hat{\tau}, \hat{\beta})\) by [22, Theorem 2.1], which states that uniform convergence in probability of a convex function over a compact set implies convergence of the optimal minimizer. So, with probability approaching 1, \((\hat{\tau}_{d},\hat{\beta}_{d})\) are strictly smaller than \(C_{2}\), and the same solution is also optimal for \(P_{2}(\mu)\). We can therefore conclude

\[P_{2}(\mu)\stackrel{{ P}}{{\to}}\max_{0\leq\tau\leq C_{2}}\min_{0 \leq\beta\leq C_{2}}f(\tau,\beta)=\max_{\tau\geq 0}\min_{\beta\geq 0}f(\tau, \beta)=\colon\bar{P}(\mu)\]

Therefore, by Theorem 3, for any fixed \(\mu\in[-\mu^{*},\mu^{*}]\), we have the convergence

\[P_{1}(\mu)\stackrel{{ P}}{{\to}}\bar{P}(\mu).\]

In the special case \(\mu=0\), we can further simplify the Moreau envelope term to obtain

\[\bar{P}(0)\coloneqq\max_{\tau\geq 0}\min_{\beta\geq 0}\frac{\tau\sigma^{2}}{ \beta}+\tau\beta(1-\kappa)-\tau^{2}+\tau\lambda\,\mathbb{E}\bigg{[}\frac{ \Theta^{2}+\beta^{2}\kappa}{\tau V^{2}+\beta\lambda}\bigg{]}.\] (21)

Step 2: Convergence of the optimal solutionWe next need to extend this result to the desired Wasserstein-2 convergence result (8). Recall here that \(\bm{u}^{(t+1)}\) is the solution of \(P_{1}(0)\).

First, let \(h\colon\mathbb{R}^{3}\to\mathbb{R}\) be any bounded, Lipschitz function, and let \(h^{(k)}\) be a sequence of bounded, twice-differentiable functions that converge uniformly to \(h\) as \(k\to\infty\) (e.g., the convolution of \(h\) with a sequence of mollifiers). Let \(P_{1}^{(k)}(\mu),\bar{P}^{(k)}(\mu)\) be the optimal cost of \(P_{1}\) and \(\bar{P}\) when using test function \(h^{(k)}\) and for \(\mu\in[-\mu^{*},\mu^{*}]\). Note the convergence \(P_{1}^{(k)}(\mu)\stackrel{{ P}}{{\to}}\bar{P}^{(k)}(\mu)\) for any \(\mu\) in a sufficiently small neighborhood around zero holds by Step 1.

By the uniform convergence of the \(h^{(k)}\) to \(h\),

\[\lim_{k\to\infty}P_{1}^{(k)}(\mu) =P_{1}(\mu)\] \[\lim_{k\to\infty}\bar{P}^{(k)}(\mu) =\bar{P}(\mu).\]

Now, fix \(\delta>0\) and choose \(k\) large enough that \(|P_{1}^{(k)}(\mu)-P_{1}(\mu)|<\frac{\delta}{3}\) and \(|\bar{P}^{(k)}(\mu)-\bar{P}(\mu)|<\frac{\delta}{3}\). Then,

\[\mathbb{P}\big{\{}|P_{1}(\mu)-\bar{P}(\mu)|>\delta\big{\}}\leq\mathbb{P}\Big{\{} |P_{1}^{(k)}(\mu)-\bar{P}^{(k)}(\mu)|>\delta/3\Big{\}}\to 0,\]

since \(P_{1}^{(k)}\stackrel{{ P}}{{\to}}\bar{P}_{2}^{(k)}\) for all \(k\). Hence, we can also apply the result of Step 1 to any bounded Lipschitz function \(h\).

Since the convergence result of Step 1 holds for any \(\mu\) in a neighborhood around zero, we can conclude that

\[\frac{1}{d}\sum_{i=1}^{d}h(u_{i}^{(t+1)},v_{i},\theta_{i}^{*})\stackrel{{ P}}{{\to}}\left.\frac{d\bar{P}(\mu)}{d\mu}\right|_{\mu=0},\]

where the derivative is well-defined since \(\bar{P}\) has a unique solution in a neighborhood around zero. The proof of this fact is identical to that of Lemma 7 of [5], so we omit it here. Moreover, using the Dominated Convergence Theorem to differentiate inside the expectation, we can compute this exactly:

\[\frac{d\bar{P}(\mu)}{d\mu}\bigg{|}_{\mu=0}=\mathbb{E}\,h\Bigg{(}\text{prox}_{ \lambda(\cdot)^{2}}\Bigg{(}\frac{\Theta-\hat{\beta}G\sqrt{\kappa}}{V};\frac{ \hat{\beta}}{2\hat{\tau}V^{2}}\Bigg{)},V,\Theta\Bigg{)}=\mathbb{E}\,h\Bigg{(} \frac{\hat{\tau}V(\Theta-\hat{\beta}G\sqrt{\kappa})}{\hat{\tau}V^{2}+\hat{ \beta}\lambda},V,\Theta\Bigg{)},\]

where \((\hat{\beta},\hat{\tau})\) are found in the optimal solution to \(\bar{P}(0)\).

Hence, for all bounded, Lipschitz \(h\), we have

\[\frac{1}{d}\sum_{i=1}^{d}h(u_{i}^{(t+1)},v_{i},\theta_{i}^{*})\stackrel{{ P}}{{\to}}\mathbb{E}\,h\Bigg{(}\frac{\hat{\tau}V(\Theta-\hat{\beta}G \sqrt{\kappa})}{\hat{\tau}V^{2}+\hat{\beta}\lambda},V,\Theta\Bigg{)},\]

so the empirical distribution of the triple \((u_{i}^{(t+1)},v_{i},\theta_{i}^{*})\) converges weakly to the distribution of the random variable \((\frac{\hat{\tau}V(\Theta-\hat{\beta}G\sqrt{\kappa})}{\hat{\tau}V^{2}+\hat{ \beta}\lambda},V,\Theta)\), where \(G\sim\mathcal{N}(0,1)\) and \((V,\Theta)\sim\Pi_{t}\). By choosing \(h(u,v,\theta)=u^{2}\), we also know that second moments of the empirical distribution converge in probability. Hence, the convergence can be strengthened from weak convergence to convergence in \(\mathcal{W}_{2}\) distance (see, e.g. [33, Theorem 6.9]).

Step 3: Verifying the inductive hypothesisLastly, we need to show that

\[\frac{1}{d}\sum_{i=1}^{d}\delta(v_{i}^{(t+1)},\theta_{i}^{*})\stackrel{{ \mathcal{W}_{2}}}{{\rightarrow}}\text{Law}(\psi(Q_{t+1},V),\Theta)\coloneqq \Pi_{t+1}.\]

where \((V,\Theta)\sim\Pi_{t}\). Here, weak convergence follows from the result of Step 2 since \(\psi\) is a continuous map. To show convergence of second moments, we need to show

\[\frac{1}{d}\sum_{i=1}^{d}v_{i}^{(t+1),2}=\frac{1}{d}\sum_{i=1}^{d}\psi(u_{i}^{ (t+1)},v_{i}^{(t)})^{2}\stackrel{{ P}}{{\rightarrow}}\mathbb{E}[ \psi(Q_{t+1},V)^{2}].\]

For \(\psi\) that satisfies Assumption 2, this convergence is immediate from the result of Step 2 (since \(\mathcal{W}_{2}\) convergence implies convergence in expectation of bounded continuous and PL(2) functions). Therefore, the initial inductive assumption made at the beginning of this proof holds at time \(t+1\), and we can apply the result inductively to conclude Theorem 1. 

Proof of Theorem 2.: The proof is an extension of the proof of Theorem 1 to the case where the test function \(h\) acts on blocks rather than individual entries. Much of the proof is identical, so we only sketch the argument and highlight the major differences here. We begin with the inductive hypothesis that \(\frac{1}{M}\sum_{i=1}^{M}\delta(\bm{v}_{i}^{(t)},\bm{\theta}_{i}^{*})\stackrel{{ \mathcal{W}_{2}}}{{\rightarrow}}\Pi_{t}\), for a known distribution \(\Pi_{t}\) over \(\mathbb{R}^{b}\times\mathbb{R}^{b}\). Recall here that \(M\) denotes the number of blocks/factors of size \(b\) (so \(M=\frac{d}{b}\)).

Then, let \(h:(\mathbb{R}^{b})^{3}\rightarrow\mathbb{R}\) be a test function with \(\|\nabla^{2}h\|_{2}\leq C\) and such that either \(h\) is bounded or \(h(\bm{u}_{i},\bm{v}_{i},\bm{\theta}_{i})=\|\bm{u}_{i}\|_{2}^{2}\). We consider a similar perturbed optimization problem:

\[P_{1}(\mu)=\min_{\bm{u}\in\mathbb{R}^{d}}\frac{1}{n}\bigg{\|}\bm{y}-\frac{1}{ \sqrt{d}}\bm{X}(\bm{u}\odot\bm{v})\bigg{\|}_{2}^{2}+\frac{\lambda}{d}\|\bm{u} \|_{2}^{2}+\frac{\mu}{M}\sum_{i=1}^{M}h(\bm{u}_{i},\bm{v}_{i},\bm{\theta}_{i} ^{*}).\] (22)

Again, we consider this for \(|\mu|\leq\frac{\lambda}{b\xi}\), so that the optimization problem is \(\frac{\lambda}{d}\) strongly convex in \(\bm{u}\). Noting that the proof of Lemma 1 still holds in this grouped case, we can constrain \(P_{1}(\mu)\) to be over compact sets and apply the CGMT to obtain the auxiliary problem

\[P_{2}(\mu)=\min_{\bm{\Delta}\in B_{\Delta}}\max_{\bm{q}\in B_{q}}\frac{2}{ \sqrt{n}}\bm{q}^{\top}\bm{\epsilon}-\frac{2}{\sqrt{n}}\|\bm{q}\|_{2}\bm{g}^{ \top}\bm{\Delta}-\frac{2}{\sqrt{n}}\|\bm{\Delta}\|_{2}\bm{h}^{\top}\bm{q}-\| \bm{q}\|_{2}^{2}+\frac{\lambda}{d}\bigg{\|}\frac{\sqrt{d}\bm{\Delta}+\bm{\theta }^{*}}{\bm{v}}\bigg{\|}_{2}^{2}\] (23)

\[+\frac{\mu}{M}\sum_{i=1}^{M}h(\bm{u}_{i},\bm{v}_{i}^{(t)},\bm{\theta}_{i}^{*}).\] (24)

The sequence of "scalarization" steps on \(P_{2}\) is identical to in Theorem 1, until we arrive at

\[P_{2}(\mu)\stackrel{{ d}}{{=}}\max_{0\leq\tau\leq R}\min_{ \begin{subarray}{c}\bm{u}\in B_{u},\\ \sigma\leq\beta\leq\sigma+R\end{subarray}}\frac{\tau\|\bm{h}\|_{2}}{\sqrt{n}} \Bigg{(}\frac{\sigma^{2}}{\beta}+\frac{\|\bm{u}\odot\bm{v}-\bm{\theta}^{*} \|_{2}^{2}}{\beta d}+\beta\Bigg{)}-\frac{2\tau}{\sqrt{nd}}\bm{g}^{\top}(\bm{u} \odot\bm{v}-\bm{\theta}^{*})\] (25)

Here, since the sum of the last two terms in the objective function is \(\frac{\lambda}{d}\) strongly convex by our choice of \(\mu\), the proof of Lemma 2 holds without change and we can consider the unconstrained minimization over \(\bm{u}\). In this case, the minimization is _block-separable_ over each of the \(M\) factors of \(\bm{u}\), so it can be expressed as

\[\frac{1}{d}\sum_{i=1}^{M}\min_{\bm{u}_{i}\in\mathbb{R}^{b}}\bigg{\{}\frac{ \tau\xi}{\beta}\|\bm{u}_{i}\odot\bm{v}_{i}-\bm{\theta}_{i}^{*}\|_{2}^{2}-2 \tau\sqrt{\kappa}\bm{g}_{i}^{\top}(\bm{u}_{i}\odot\bm{v}_{i}-\bm{\theta})+ \lambda\|\bm{u}\|_{2}^{2}+\mu bh(\bm{u}_{i},\bm{v}_{i},\bm{\theta}_{i}^{*}) \bigg{\}}\]

\[=\frac{1}{bM}\sum_{i=1}^{M}\min_{\bm{u}_{i}\in\mathbb{R}^{b}}\bigg{\{}\frac{ \tau\xi}{\beta}\|\bm{u}_{i}\odot\bm{v}_{i}-\bm{\theta}_{i}^{*}\|_{2}^{2}-2\tau \sqrt{\kappa}\bm{g}_{i}^{\top}(\bm{u}_{i}\odot\bm{v}_{i}-\bm{\theta})+\lambda \|\bm{u}\|_{2}^{2}+\mu bh(\bm{u}_{i},\bm{v}_{i},\bm{\theta}_{i}^{*})\bigg{\}}\]

\[\coloneqq\frac{1}{bM}\sum_{i=1}^{M}q(\bm{v}_{i},\bm{\theta}_{i}^{*},\bm{g}_{i}),\]where \(\bm{g}_{i}\in\mathbb{R}^{b}\) denotes the \(i\)th block of \(\bm{g}\) and \(q\coloneqq(\mathbb{R}^{b})^{3}\to\mathbb{R}\) is defined as a shorthand for the quantity inside the summation.

Next, we consider the asymptotic behavior of \(P_{2}(\mu)\). Here, the only term which is different than in Theorem 1 is the term \(\frac{1}{bM}\sum_{i=1}^{M}q(\bm{v}_{i},\bm{\theta}_{i}^{*},\bm{g}_{i})\). By the same argument as in Lemma 3, we can write \(q\) as the Moreau envelope of a convex function and show that

\[\frac{1}{bM}\sum_{i=1}^{M}q(\bm{v}_{i},\bm{\theta}_{i}^{*},\bm{g}_{i}) \overset{P}{\to}\mathbb{E}\,\frac{1}{b}q(\bm{V},\bm{\Theta},\bm{G}),\]

where the expectation is over \((\bm{V},\bm{\Theta})\sim\Pi_{t}\) and \(\bm{G}\sim\mathcal{N}(\bm{0},\bm{I}_{b})\). After the same uniform convergence argument as in the proof of Theorem 1, we can conclude that for all \(\mu\in[-\mu^{*},\mu^{*}]\), \(P_{1}(\mu)\overset{P}{\to}\bar{P}(\mu)\), where

\[\bar{P}(\mu)=\max_{\tau\geq 0}\min_{\beta\geq 0}\frac{\tau\sigma^{2}}{\beta}+ \tau\beta-\tau^{2}+\mathbb{E}\,\frac{1}{b}q(\bm{V},\bm{\Theta},\bm{G}).\]

In particular, when \(\mu=0\), the minimization implicit in the definition of \(q\) can be solved exactly; this yields exactly the optimization problem in 7. Step 2 of the proof (convergence of test functions of the optimal minimizer) is identical to that of Theorem 1, and for the final step (showing the inductive hypothesis holds at the next iteration), we need to argue that the second moment of \(\bm{v}^{(t+1)}=\psi(\bm{u}_{i}^{(t+1)},\bm{v}_{i})\) converges to its expectation under \(\Pi_{t+1}\):

\[\frac{1}{M}\sum_{i=1}^{M}\lVert\psi(\bm{u}_{i}^{(t+1)},\bm{v}_{i})\rVert_{2}^ {2}\overset{P}{\to}\mathbb{E}\lVert\psi(\bm{Q}_{t+1},\bm{V})\rVert_{2}^{2}\]

If \(\psi\) is bounded and continuous or has \(\text{PL}(2)\) coordinate projections (as we have assumed), then the above convergence holds based on the Wasserstein-2 convergence of the joint distribution of \((\bm{u}^{(t+1)},\bm{v})\).

## Appendix B Technical lemmas

**Proposition 1**.: _The function \(g(u,v,\theta)=|uv-\theta|\) is pseudo-Lipschitz of order 2._

Proof.: The result follows from the following series of inequalities:

\[||uv-\theta|-|u^{\prime}v^{\prime}-\theta^{\prime}|| \leq|uv-\theta-(u^{\prime}v^{\prime}-\theta^{\prime})|\] \[\leq|uv-u^{\prime}v^{\prime}|+|\theta-\theta^{\prime}|\] \[\leq|u||v-v^{\prime}|+|v^{\prime}||u-u^{\prime}|+|\theta-\theta^ {\prime}|\] \[\leq(|u|+|v^{\prime}|+1)(|u-u^{\prime}|+|v-v^{\prime}|+|\theta- \theta^{\prime}|)\] \[\leq(1+\lVert\bm{x}\rVert_{1}+\lVert\bm{x}^{\prime}|_{1})\lVert \bm{x}-\bm{x}^{\prime}\rVert_{1}\] \[\leq 3(1+\lVert\bm{x}\rVert_{2}+\lVert\bm{x}^{\prime}\rVert_{2}) \lVert\bm{x}-\bm{x}^{\prime}\rVert_{2},\]

where \(\bm{x},\bm{x}^{\prime}\in\mathbb{R}^{3}\) denote \((u,v,\theta)\) and \((u^{\prime},v^{\prime},\theta^{\prime})\), respectively. 

**Lemma 1**.: _Let \(\bm{\Delta}^{*},\bm{q}^{*}\) be the optimal solution to (10). Then, there exists universal constant \(C_{1}>0\) such that_

\[\lim_{d\to\infty}\mathbb{P}\{\|\bm{\Delta}^{*}\|_{2}\leq C_{1}\|\bm{v}\|_{ \infty}\}=\lim_{d\to\infty}\mathbb{P}\{\|\bm{q}^{*}\|_{2}\leq C_{1}\|\bm{v}\|_{ \infty}\}=1.\]

Proof.: We proceed via a similar argument to Lemma 2 in [5]. First, consider the original expression for \(P_{1}(\mu)\) from (9) :

\[P_{1}(\mu)=\min_{\bm{u}\in\mathbb{R}^{d}}F(\bm{u})+R(\bm{u}),\]

where \(F(\bm{u})\coloneqq\frac{1}{n}\Big{\lVert}\bm{y}-\frac{1}{\sqrt{d}}\bm{X}(\bm {u}\odot\bm{v})\Big{\rVert}_{2}^{2}\) and \(R(\bm{u})\coloneqq\frac{\lambda}{d}\lVert\bm{u}\rVert_{2}^{2}+\frac{\mu}{d} \sum_{i=1}^{d}h(u_{i},v_{i}^{(t)},\theta_{i}^{*})\). Recall here that \(R\) is \(\frac{\lambda}{d}\)-strongly convex for all \(\mu\in[-\mu^{*},\mu^{*}]\), and denote the unique optimal minimizer to this problem as \(\bm{u}^{*}\). Then, the following chain of inequalities holds, by the optimality of \(\bm{u}^{*}\) and the non-negativity of \(F\).

\[\frac{1}{n}\|\bm{y}\|_{2}^{2}+R(\bm{0})=F(\bm{0})+R(\bm{0})\geq F(\bm{u}^{*})+R( \bm{u}^{*})\geq R(\bm{u}^{*}).\]

Moreover, by the strong convexity of \(R\), we have

\[R(\bm{u}^{*})\geq R(\bm{0})+\nabla R(\bm{0})^{\top}\bm{u}^{*}+\frac{\lambda}{2d }\|\bm{u}^{*}\|_{2}^{2}.\]

Combining the above two series of inequalities, we obtain (recall \(\kappa=\frac{d}{n}\))

\[\|\bm{u}^{*}\|_{2}^{2}+\frac{2d}{\lambda}\nabla R(\bm{0})^{\top}\bm{u}^{*}\leq \frac{2\kappa}{\lambda}\|\bm{y}\|_{2}^{2}.\]

After completing the square, this yields

\[\bigg{\|}\bm{u}^{*}+\frac{d}{\lambda}\nabla R(\bm{0})\bigg{\|}_{2}^{2}\leq \frac{2\kappa}{\lambda}\|\bm{y}\|_{2}^{2}+\frac{d^{2}}{\lambda^{2}}\|\nabla R (\bm{0})\|_{2}^{2},\]

whence, by the triangle inequality,

\[\|\bm{u}^{*}\|_{2} \leq\frac{d}{\lambda}\|\nabla R(\bm{0})\|_{2}+\sqrt{\frac{2\kappa }{\lambda}\|\bm{y}\|_{2}^{2}+\frac{d^{2}}{\lambda^{2}}\|\nabla R(\bm{0})\|_{2 }^{2}}\] \[\leq\frac{2d}{\lambda}\|\nabla R(\bm{0})\|_{2}+\sqrt{\frac{2 \kappa}{\lambda}}\|\bm{y}\|_{2}.\]

Here, standard concentration inequalities for Gaussian random variables (e.g., [32, Theorem 5.2.2, Corollary 7.3.3]) imply that, with probability approaching 1, \(\|\bm{X}\|_{2}\lesssim\sqrt{d}\) and \(\|\bm{\epsilon}\|_{2}\lesssim\sqrt{d}\). And Assumption 1 implies that \(\|\bm{\theta}^{*}\|_{2}\lesssim\sqrt{d}\) with probability tending to 1. So,

\[\|\bm{y}\|_{2}\leq\frac{\mu}{\sqrt{d}}\|\bm{X}\|\|\bm{\theta}^{*}\|_{2}+\|\bm {\epsilon}\|_{2}\lesssim\sqrt{d}\]

with probability approaching 1. Next, we bound \(\|\nabla R(\bm{0})\|_{2}\). Recalling the definition of \(R\),

\[\|\nabla R(\bm{0})\|_{2}=\frac{\mu}{d}\sqrt{\sum_{i=1}^{d}\biggl{(}\frac{ \partial}{\partial u}h(u,v_{i},\theta_{i}^{*})\bigg{|}_{u=0}\biggr{)}^{2}}= \frac{1}{\sqrt{d}}\sqrt{\frac{1}{d}\sum_{i=1}^{d}\biggl{(}\frac{\partial}{ \partial u}h(u,v_{i},\theta_{i}^{*})\bigg{|}_{u=0}\biggr{)}^{2}}.\]

Since the function \(g(v,\theta)=\frac{\partial}{\partial u}h(u,v,\theta)\bigg{|}_{u=0}\) is Lipschitz (by the fact that \(h\) has bounded second derivatives), \(g^{2}\) is pseudo-Lipschitz of order 2. So, the quantity under the square root converges in probability to \(\mathbb{E}_{(V,\Theta)\sim\Pi_{t}}\,g^{2}\) by Assumption 1, and, with probability tending to \(1\), we have \(\|\nabla R(\bm{0})\|_{2}\lesssim 1/\sqrt{d}\).

Combining the above bounds on \(\|\bm{y}\|_{2}\) and \(\|\nabla R(\bm{0})\|_{2}\), we can conclude that \(\|\bm{u}^{*}\|_{2}\lesssim\sqrt{d}\). The first part of the lemma follows by noting that

\[\|\bm{\Delta}^{*}\|_{2}=\frac{1}{\sqrt{d}}\|\bm{u}^{*}\odot\bm{v} -\bm{\theta}^{*}\|_{2} \leq\frac{1}{\sqrt{d}}\|\bm{u}^{*}\odot\bm{v}\|_{2}+\frac{1}{ \sqrt{d}}\|\bm{\theta}^{*}\|_{2}\] \[\leq\frac{1}{\sqrt{d}}\|\bm{v}\|_{\infty}\|\bm{u}^{*}\|_{2}+\frac{ 1}{\sqrt{d}}\|\bm{\theta}^{*}\|_{2}\] \[\lesssim\|\bm{v}\|_{\infty},\]

where the last inequality holds with probability approaching 1. Lastly, the optimal \(\bm{q}\) for any \(\bm{\Delta}\) has closed-form \(\bm{q}=\frac{1}{\sqrt{n}}\bm{\epsilon}-\frac{1}{\sqrt{n}}\bm{X}\bm{\Delta}\). By the triangle inequality, we then obtain

\[\|\bm{q}^{*}\|_{2}\leq\frac{1}{\sqrt{n}}\|\bm{\epsilon}\|_{2}+\frac{1}{\sqrt{n} }\|\bm{X}\|\|\bm{\Delta}^{*}\|_{2}\lesssim\|\bm{v}\|_{\infty},\]

with the last inequality holding with probability approaching 1, by the concentration of norms of \(\bm{\epsilon}\) and \(\bm{X}\) as discussed above, and the bound on \(\|\bm{\Delta}^{*}\|_{2}\)

**Lemma 2**.: _Consider the following unconstrained minimization problem over \(\bm{u}\in\mathbb{R}^{d}\):_

\[\min_{\bm{u}\in\mathbb{R}^{d}}\max_{0\leq\tau\leq R}\frac{2\tau\| \bm{h}\|_{2}}{\sqrt{n}}\sqrt{\sigma^{2}+\frac{1}{d}\|\bm{u}\odot\bm{v}-\bm{\theta }^{*}\|_{2}^{2}}-\frac{2\tau}{\sqrt{nd}}\bm{g}^{\top}(\bm{u}\odot\bm{v}-\bm{ \theta}^{*})\] \[-\tau^{2}+\frac{\lambda}{d}\|\bm{u}\|_{2}^{2}+\frac{\mu}{d}\sum_{ i=1}^{d}h(u_{i},v_{i}^{(t)},\theta_{i}^{*}).\]

_With probability approaching 1, the solution \(\bm{u}^{*}\) satisfies \(\left\|\frac{1}{\sqrt{d}}(\bm{u}^{*}\odot\bm{v}-\bm{\theta}^{*})\right\|_{2} \lesssim\|\bm{v}\|_{\infty}\)._

Proof.: First note \(\left\|\frac{1}{\sqrt{d}}(\bm{u}^{*}\odot\bm{v}-\bm{\theta}^{*})\right\|_{2} \leq\frac{1}{\sqrt{d}}\|\bm{u}^{*}\|_{2}\|\bm{v}\|_{\infty}+\frac{1}{\sqrt{d} }\|\bm{\theta}^{*}\|_{2}\), and \(\frac{1}{\sqrt{d}}\|\bm{\theta}^{*}\|_{2}\) is bounded by a constant with probability approaching \(1\) by the assumed \(\mathcal{W}_{2}\) convergence of \(\bm{\theta}^{*}\) to a fixed limit. Hence, it suffices to show that \(\|\bm{u}^{*}\|_{2}\lesssim\sqrt{d}\) with high probability. We begin by noting that the inner maximization over \(\tau\) admits a closed form solution, so the problem becomes

\[\min_{\bm{u}\in\mathbb{R}^{d}}\Biggl{(}\frac{\|\bm{h}\|_{2}}{2\sqrt{n}}\sqrt{ \sigma^{2}+\frac{1}{d}\|\bm{u}\odot\bm{v}-\bm{\theta}^{*}\|_{2}^{2}}-\frac{1 }{2\sqrt{nd}}\bm{g}^{\top}(\bm{u}\odot\bm{v}-\bm{\theta}^{*})\Biggr{)}_{+}^{2} +\frac{\lambda}{d}\|\bm{u}\|_{2}^{2}+\frac{\mu}{d}\sum_{i=1}^{d}h(u_{i},v_{i}^ {(t)},\theta_{i}^{*}).\]

Now, we can proceed similarly to in the proof of Lemma 1. Let

\[F(\bm{u}) \coloneqq\Biggl{(}\frac{\|\bm{h}\|_{2}}{2\sqrt{n}}\sqrt{\sigma^{2 }+\frac{1}{d}\|\bm{u}\odot\bm{v}-\bm{\theta}^{*}\|_{2}^{2}}-\frac{1}{2\sqrt{ nd}}\bm{g}^{\top}(\bm{u}\odot\bm{v}-\bm{\theta}^{*})\Biggr{)}_{+}^{2},\] \[R(\bm{u}) \coloneqq\frac{\lambda}{d}\|\bm{u}\|_{2}^{2}+\frac{\mu}{d}\sum_{ i=1}^{d}h(u_{i},v_{i}^{(t)},\theta_{i}^{*}).\]

Then, noting \(F\) is always non-negative and \(R\) is \(\frac{\lambda}{d}\) strongly-convex, we use the same argument as in Lemma 1 to obtain the inequality

\[\|\bm{u}^{*}\|_{2}^{2}+\frac{2d}{\lambda}\nabla R(\bm{0})^{\top}\bm{u}^{*} \leq\frac{2d}{\lambda}F(\bm{0}).\]

After completing the squares, we obtain

\[\left\|\bm{u}^{*}+\frac{d}{\lambda}\nabla R(\bm{0})\right\|_{2}^{2}\leq\frac{2 d}{\lambda}F(\bm{0})+\frac{d^{2}}{\lambda^{2}}\|\nabla R(\bm{0})\|_{2}^{2},\]

so we can conclude

\[\|\bm{u}^{*}\|_{2}\leq\frac{d}{\lambda}\|\nabla R(\bm{0})\|_{2}+\sqrt{\frac{2 d}{\lambda}F(\bm{0})+\frac{d^{2}}{\lambda^{2}}\|\nabla R(\bm{0})\|_{2}^{2}} \leq\frac{2d}{\lambda}\|\nabla R(\bm{0})\|_{2}+\sqrt{\frac{2d}{\lambda}F(\bm{ 0})}.\]

As shown in Lemma 1, \(\|\nabla R(\bm{0})\|_{2}\lesssim\frac{1}{\sqrt{d}}\) with probability approaching 1. It remains to show that \(\sqrt{F(\bm{0})}\lesssim 1\) with probability approaching 1. To see this, observe that

\[\sqrt{F(\bm{0})} =\Biggl{(}\frac{\|\bm{h}\|_{2}}{2\sqrt{n}}\sqrt{\sigma^{2}+\frac{1 }{d}\|\bm{\theta}^{*}\|_{2}^{2}}-\frac{1}{2\sqrt{nd}}\bm{g}^{\top}\bm{\theta} ^{*}\Biggr{)}_{+}\] \[\leq\frac{\|\bm{h}\|_{2}}{2\sqrt{n}}\sqrt{\sigma^{2}+\frac{1}{d} \|\bm{\theta}^{*}\|_{2}^{2}}-\frac{1}{2\sqrt{nd}}\bm{g}^{\top}\bm{\theta}^{*}\Biggr{|}\] \[\leq\frac{\|\bm{h}\|_{2}}{2\sqrt{n}}\sqrt{\sigma^{2}+\frac{1}{d} \|\bm{\theta}^{*}\|_{2}^{2}}+\frac{1}{2\sqrt{nd}}\|\bm{g}\|_{2}\|\bm{\theta}^{*} \|_{2},\]

where the last line uses the triangle and Cauchy-Schwarz inequalities. By concentration of the norm for Gaussian vectors, there exists a universal constant \(c\) such that \(\frac{\|\bm{h}\|_{2}}{\sqrt{n}}\leq c\) and \(\frac{\|\bm{g}\|_{2}}{\sqrt{n}}\leq c\) with probability approaching 1. Moreover, by Assumption 1, \(\frac{\|\bm{\theta}^{*}\|_{2}}{\sqrt{d}}\leq c\) with probability approaching 1. Hence, \(\sqrt{F(\bm{0})}\lesssim 1\) with probability approaching 1. Substituting this into the bound for \(\|\bm{u}^{*}\|_{2}\) from above completes the proof.

**Lemma 3**.: _Under Assumption 1, the function \(f_{d}(\tau,\beta)\) (Eq. 19) converges pointwise in probability to \(f(\tau,\beta)\) (Eq. 20) as \(d\to\infty\)._

Proof.: We consider the limit of each term in \(f_{d}\) separately. The limit of the terms \(\frac{\tau\sigma^{2}\xi}{\beta}\) and \(\tau\beta\xi\) is found by noting that \(\xi\to 1\) in probability by Gaussian Lipschitz concentration.

The first summation term simplifies as follows:

\[\frac{1}{d}\sum_{i=1}^{d}\biggl{[}\frac{\tau\xi\theta_{i}^{*2}}{ \beta}-\frac{\tau}{\beta\xi}\bigl{(}\beta g_{i}\sqrt{\kappa}-\xi\theta_{i}^{* }\bigr{)}^{2}\biggr{]} =\frac{1}{d}\sum_{i=1}^{d}\biggl{[}-\frac{\tau}{\beta\xi}\bigl{(} \beta^{2}g_{i}^{2}\kappa-2\beta g_{i}\sqrt{\kappa}\xi\theta_{i}^{*}\bigr{)} \biggr{]}\] \[=-\frac{\tau}{\beta\xi}\frac{1}{d}\sum_{i=1}^{d}\bigl{[}\beta^{2 }g_{i}^{2}\kappa-2\beta g_{i}\sqrt{\kappa}\xi\theta_{i}^{*}\bigr{]}^{\frac{P} {d}}\to-\tau\beta\kappa,\]

where the last line follows from the weak law of large numbers since the \(g_{i}\) are i.i.d. standard Gaussian variables.

For the last term, after again using the fact that \(\xi\overset{P}{\to}1\), we need to consider

\[\frac{1}{d}\sum_{i=1}^{d}\biggl{[}\mathcal{M}_{\lambda(\cdot)^{2}+\mu h(\cdot,v_{i}^{(t)},\theta_{i}^{*})}\biggl{(}\frac{\theta_{i}^{*}-\beta g_{i}\sqrt{ \kappa}}{v_{i}^{(t)}};\frac{\beta}{2\tau v_{i}^{(t)2}}\biggr{)}\biggr{]}=: \frac{1}{d}\sum_{i=1}^{d}q(v_{i},\theta_{i}^{*},g_{i})\]

To show convergence in probability of this term, first fix \(\delta>0\). Then, we want to show

\[\mathbb{P}\Biggl{[}\biggl{|}\frac{1}{d}\sum_{i=1}^{d}q(v_{i},\theta_{i}^{*},g _{i})-\mathbb{E}\,q(V,\Theta,G)\biggr{|}>\delta\Biggr{]}\to 0,\]

where the expectation is over \((V,\Theta)\sim\Pi_{t}\) and \(G\sim\mathcal{N}(0,1)\). It suffices to show the following two statements:

\[\mathbb{P}\Biggl{[}\biggl{|}\frac{1}{d}\sum_{i=1}^{d}q(v_{i}, \theta_{i}^{*},g_{i})-\mathbb{E}_{\boldsymbol{g}}\,\frac{1}{d}\sum_{i=1}^{d}q (v_{i},\theta_{i}^{*},g_{i})\Biggr{|}>\frac{\delta}{2}\Biggr{]}\to 0,\] (26) \[\mathbb{P}\Biggl{[}\mathbb{E}_{\boldsymbol{g}}\,\frac{1}{d}\sum_ {i=1}^{d}q(v_{i},\theta_{i}^{*},g_{i})-\mathbb{E}\,q(V,\Theta,G)\Biggr{|}> \frac{\delta}{2}\Biggr{]}\to 0.\] (27)

To show (26), we rely on a concentration inequality for the Moreau envelope of a Gaussian vector plus a bounded vector from [5]. First note that

\[\frac{1}{d}\sum_{i=1}^{d}q(v_{i},\theta_{i}^{*},g_{i}) =\frac{1}{d}\min_{\boldsymbol{u}\in\mathbb{R}^{d}}\Biggl{\{} \lambda\|\boldsymbol{u}\|_{2}^{2}+\mu\sum_{i=1}^{d}h(u_{i},v_{i},\theta_{i}^{* })+\frac{\tau}{\beta}\bigl{\|}\boldsymbol{u}\odot\boldsymbol{v}-\boldsymbol{ \theta}^{*}+\beta\sqrt{\kappa}\boldsymbol{g}\bigr{\|}_{2}^{2}\Biggr{\}}\] \[=\frac{1}{d}\min_{\boldsymbol{u}\in\mathbb{R}^{d}}\Biggl{\{} \lambda\|\boldsymbol{u}\|_{2}^{2}+\mu\sum_{i=1}^{d}h(u_{i},v_{i},\theta_{i}^{* })+\tau\beta\kappa\biggl{\|}\frac{\boldsymbol{u}\odot\boldsymbol{v}}{\beta \sqrt{\kappa}}-\frac{\boldsymbol{\theta}^{*}}{\beta\sqrt{\kappa}}+ \boldsymbol{g}\Bigr{\|}_{2}^{2}\Biggr{\}}\] \[=\frac{1}{d}\mathcal{M}_{\ell}\biggl{(}\frac{\boldsymbol{\theta}^ {*}}{\beta\sqrt{\kappa}}-\boldsymbol{g};\frac{1}{2\tau\beta\kappa}\biggr{)},\]

where the second to last line follows from the change of variable \(\boldsymbol{\theta}=\frac{\boldsymbol{u}\odot\boldsymbol{v}}{\beta\sqrt{ \kappa}}\), and \(\ell(\boldsymbol{\theta})\coloneqq\lambda\left\|\frac{\beta\sqrt{\kappa} \boldsymbol{\theta}}{\boldsymbol{v}}\right\|_{2}^{2}+\mu\sum_{i=1}^{d}h\bigl{(} \frac{\beta\sqrt{\kappa}\boldsymbol{\theta}_{i}}{v_{i}},v_{i},\theta_{i}^{* }\bigr{)}\). Here, for fixed \(\boldsymbol{v}\), \(\ell\) is a proper convex function of \(\boldsymbol{\theta}\). Moreover, by Assumption 1, \(\frac{\boldsymbol{\theta}^{*}}{\beta\sqrt{\kappa}}\) has norm of order \(\sqrt{d}\) with high probability. Hence, by [5, Lemma 8], this quantity concentrates around its expectation (with respect to \(\boldsymbol{g}\)), and we can conclude that there is some \(c>0\) such that

\[\mathbb{P}\Biggl{[}\biggl{|}\frac{1}{d}\sum_{i=1}^{d}q(v_{i},\theta_{i}^{*},g_{ i})-\mathbb{E}_{\boldsymbol{g}}\,\frac{1}{d}\sum_{i=1}^{d}q(v_{i},\theta_{i}^{*},g_{ i})\Biggr{|}>\frac{\delta}{2}\Biggr{]}\leq\frac{c\tau^{2}\beta^{2}\kappa^{2}}{d \delta^{2}}\to 0.\]

[MISSING_PAGE_EMPTY:23]

Substituting this \(\gamma\) back into the first optimality condition in (28), we can express the optimal \(\beta\) in closed-form, in terms of \(\gamma\):

\[\beta=\sqrt{\frac{\sigma^{2}+\lambda^{2}\operatorname{\mathbb{E}}\Bigl{[}\frac{1 }{b}\sum_{i=1}^{b}\frac{\Theta_{i}^{2}}{(\gamma V_{i}^{2}+\lambda)^{2}}\Bigr{]} }{2\gamma+\kappa-1-\lambda^{2}\kappa\operatorname{\mathbb{E}}\Bigl{[}\frac{1 }{b}\sum_{i=1}^{b}\frac{1}{(\gamma V_{i}^{2}+\lambda)^{2}}\Bigr{]}}}.\]

Finally, the optimal \(\tau\) can be found simply as \(\tau=\gamma\beta\).

This yields a simple recipe for solving the min-max problem. First, compute the positive solution \(\hat{\gamma}\) to the fixed point equation (30) (this can be found easily using standard numerical solvers). Then, \((\hat{\beta},\hat{\tau})\) are both given in closed-form as functions of \(\hat{\gamma}\) (where the required expectations can all be approximated via Monte Carlo simulation).

## Appendix D Further simulations

In this section, we demonstrate that our asymptotic predictions can provide accurate estimates of the test error, even when some of our technical assumptions are not satisfied.

First, we compare the two "heavier" weightings considered in Section 3.1, \(\psi(u,v)=\tanh|uv|\) and \(\psi(u,v)=\tanh u^{2}\), to the same weightings without the bounded tanh activation: \(\psi(u,v)=|uv|\) and \(\psi(u,v)=u^{2}\). We note that the reweighting choice \(|uv|\) is considered in [21, 28] as a limit as \(p\to 0\) of the classical IRLS update for \(\ell_{p}\) minimization. In Figure 2(a), we consider the same sparse regression as in Section 3.1, i.e., with \(n=250,d=2000,\sigma=0.1,\theta_{i}^{*\text{ i.i.d.}}\operatorname{Bernoulli}(0.01)\) and \(\lambda\) chosen to minimize the predicted asymptotic loss.

For each choice of \(\psi\), we apply the theoretical predictions of Theorem 1, even if \(\psi\) violates Assumption 2. We find that our predictions remain accurate for all these choices of \(\psi\). The choice \(\tanh|uv|\) performs almost identically without the tanh activation. Interestingly, the choice \(\psi=\tanh u^{2}\) outperforms the variant without the tanh and has a more regular decay of the test loss.

In Figure 2(b), we apply Theorem 1 to predict the asymptotic squared test loss: \(\frac{1}{d}\|\bm{u}\odot\bm{v}-\bm{\theta}^{*}\|_{2}^{2}\) at each iteration. While this function is not \(PL(2)\), as required by the theorem, the asymptotic predictions still align well with simulations. Extending our technical results to hold formally in such scenarios is an interesting direction for future work.

Figure 3: Here, we fix \(n=250,d=2000,\sigma=0.1,\theta_{i}^{*\,\mathrm{i.i.d}}\,\)Bernoulli\((0.01)\) and select \(\lambda\) to minimize the predicted asymptotic loss. Plus marks denote the median over 100 trials, and the shaded region indicates the interquartile range. Left: Predictions and simulations for weighting functions which are not uniformly bounded. Right: Predictions and simulations for the squared error \(\frac{1}{d}\|\bm{u}\odot\bm{v}-\bm{\theta}^{*}\|_{2}^{2}\).

### NeurIPS Paper Checklist

The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: **The papers not including the checklist will be desk rejected.** The checklist should follow the references and follow the (optional) supplemental material. The checklist does NOT count towards the page limit.

Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist:

* You should answer [Yes], [No], or [NA].
* [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.
* Please provide a short (1-2 sentence) justification right after your answer (even for NA).

**The checklist answers are an integral part of your paper submission.** They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper.

The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While "[Yes] " is generally preferable to "[No] ", it is perfectly acceptable to answer "[No] " provided a proper justification is given (e.g., "error bars are not reported because it would be too computationally expensive" or "we were unable to find the license for the dataset we used"). In general, answering "[No] " or "[NA] " is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found.

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: Theorems 1 and 2 directly reflect the claims from the abstract/introduction. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We provide detailed discussion of the strength of our technical assumptions after the statement of Theorem 1 and in Appendix D, and we further elaborate on these areas for improvement in the conclusion. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.

* The authors are encouraged to create a separate "Limitations" section in their paper.
* The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.
* The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.
* The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.
* The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.
* If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.
* While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: We formally state and discuss Assumptions 1 and 2, and we provide a complete proof of our results in the Appendix A of the supplemental material. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We provide all details of hyperparameters used to run simulations in the corresponding figure caption. We also detail the exact method used to compute our asymptotic predictions in Appendix C and provide accompanying code for the simulations. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.

* If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.
* Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general, releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.
* While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: We provide an accompanying file with code for computing the asymptotic predictions and running simulations with high-dimensional Gaussian data. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details**Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: For each simulation, hyperparameter choices (and the method for choosing the ridge parameter \(\lambda\)) are specified either in the caption or in the accompanying discussion in the text. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: We report the median and interquartile range for all simulations over 100 independent trials. These metrics are chosen due to the asymmetry of the distribution across trials (e.g., the mean minus the standard deviation might be negative in low-noise settings). Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors). * It should be clear whether the error bar is the standard deviation or the standard error of the mean. * It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. * For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). * If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [NA] Justification: Due to the small-scale of our included simulations, we do not include this information. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.

* The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.
* The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: The research presented in this work abides by the Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: Our work provides statistical analysis for a generic family of algorithms in an abstract setup, and we do not believe our work has immediate social impacts or an immediate path toward such impacts. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. * The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. * The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. * If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [Yes] Justification: The paper poses no such risks.

Guidelines:

* The answer NA means that the paper poses no such risks.
* Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.
* Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.
* We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.

12. **Licensees for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [NA] Justification: The paper does not use existing assets. Guidelines:

* The answer NA means that the paper does not use existing assets.
* The authors should cite the original paper that produced the code package or dataset.
* The authors should state which version of the asset is used and, if possible, include a URL.
* The name of the license (e.g., CC-BY 4.0) should be included for each asset.
* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.

13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: The provided code includes all necessary information for running simulations. Guidelines:

* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.

14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)?Answer: [NA] Justification: The paper does not involve research with human subjects or crowdsourcing. Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not involve research with human subjects or crowdsourcing. Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.