# Bifurcations and loss jumps in RNN training

Lukas Eisenmann\({}^{1,2,*}\), Zahra Monfared\({}^{1,*}\), Niclas Goring\({}^{1,2}\), and Daniel Durstewitz\({}^{1,2,3}\)

{lukas.eisenmann,zahra.monfared,daniel.durstewitz}@zi-mannheim.de

\({}^{1}\)Department of Theoretical Neuroscience, Central Institute of Mental Health,

Medical Faculty Mannheim, Heidelberg University, Mannheim, Germany

\({}^{2}\)Faculty of Physics and Astronomy, Heidelberg University, Heidelberg, Germany

\({}^{3}\)Interdisciplinary Center for Scientific Computing, Heidelberg University

\({}^{*}\)These authors contributed equally

###### Abstract

Recurrent neural networks (RNNs) are popular machine learning tools for modeling and forecasting sequential data and for inferring dynamical systems (DS) from observed time series. Concepts from DS theory (DST) have variously been used to further our understanding of both, how trained RNNs solve complex tasks, and the training process itself. Bifurcations are particularly important phenomena in DS, including RNNs, that refer to topological (qualitative) changes in a system's dynamical behavior as one or more of its parameters are varied. Knowing the bifurcation structure of an RNN will thus allow to deduce many of its computational and dynamical properties, like its sensitivity to parameter variations or its behavior during training. In particular, bifurcations may account for sudden loss jumps observed in RNN training that could severely impede the training process. Here we first mathematically prove for a particular class of ReLU-based RNNs that certain bifurcations are indeed associated with loss gradients tending toward infinity or zero. We then introduce a novel heuristic algorithm for detecting all fixed points and \(k\)-cycles in ReLU-based RNNs and their existence and stability regions, hence bifurcation manifolds in parameter space. In contrast to previous numerical algorithms for finding fixed points and common continuation methods, our algorithm provides _exact_ results and returns fixed points and cycles up to high orders with surprisingly good scaling behavior. We exemplify the algorithm on the analysis of the training process of RNNs, and find that the recently introduced technique of generalized teacher forcing completely avoids certain types of bifurcations in training. Thus, besides facilitating the DST analysis of trained RNNs, our algorithm provides a powerful instrument for analyzing the training process itself.

## 1 Introduction

Recurrent neural networks (RNNs) are common and powerful tools for learning sequential tasks or modeling and forecasting time series data [31, 65, 2, 43, 9, 26]. Typically, RNNs are "black boxes," whose inner workings are hard to dissect. Techniques from dynamical system theory (DST) can significantly aid in this effort, as RNNs are formally discrete-time dynamical systems (DS) [37]. A better understanding of how RNNs solve their tasks is important for detecting failure modes and designing better architectures and training algorithms. In scientific machine learning, on the other hand, RNNs are often employed for reconstructing unknown DS from sets of time series observations [10, 30, 64, 63, 23, 55], e.g. in climate or disease modeling. In this case, the RNN is supposed to provide an approximation to the flow of the observed system that reproduces all its dynamical properties, e.g., cyclic behavior in climate or epidemiological systems. In such scientific or medicalsettings, a detailed understanding of the RNN's dynamics and its sensitivity to parameter variations is in fact often crucial.

Beyond understanding the dynamical properties of a once trained RNN, it may also be of interest to know how its dynamical repertoire changes with changes in its parameters. The parameter space of any DS is partitioned into regions (or sets) with topologically different dynamical behaviors by bifurcation curves (or, more generally, manifolds). Such bifurcations, qualitative changes in the system dynamics due to parameter variations, may not only be crucial in scientific applications where we use RNNs, for instance, to predict tipping points in climate systems or medical scenarios like sepsis detection [45]. They also pose severe challenges for the training process itself as qualitative changes in RNN dynamics may go in hand with sudden jumps in the loss landscape [16; 44]. Although methods from DST have significantly advanced the field in recent years, especially with regards to algorithms for reconstructing nonlinear DS from data [55; 46; 67], progress is still hampered by the lack of efficient tools for analysing the dynamics of higher-dimensional RNNs and their bifurcations. In particular, methods are needed for exactly locating geometrical objects like fixed points or cycles in an RNN's state space, but current numerical techniques do not scale well to higher-dimensional scenarios and provide only approximate solutions [29; 21; 61].

The contributions of this work are threefold: After first providing an introduction into bifurcations of piecewise-linear (ReLU-based) RNNs (PLRNNs), which have been extensively used for reconstructing DS from empirical data [10; 30], we mathematically prove that certain bifurcations during the training process will indeed cause loss gradients to diverge to infinity, resulting in abrupt jumps in the loss, while others will cause them to vanish. As RNNs are likely to undergo several bifurcations during training on their way from some initial parameter configuration toward a dynamics that successfully implements any given task, this poses severe challenges for RNN training and may be one of the reasons for exploding and vanishing gradients [16; 44; 8; 25]. We then create a novel, efficient heuristic algorithm for _exactly_ locating all fixed points and \(k\)-cycles in PLRNNs, which can be used to delineate bifurcation manifolds in higher-dimensional systems. Our algorithm finds these dynamical objects in many orders of magnitude less time than an exhaustive search would take. Using this algorithm, we demonstrate empirically that steep cliffs in loss landscapes and bifurcation curves indeed tightly overlap, and that bifurcations in the system dynamics are accompanied by sudden loss jumps. Finally, we prove and demonstrate that the recently introduced technique of generalized teacher forcing (GTF) [24] completely eliminates certain types of bifurcation in training, providing an explanation for its efficiency.

## 2 Related Work

DS analysis of trained RNNsIn many areas of science one is interested in identifying the nonlinear DS that explains a set of observed time series [12; 23]. A variety of purely data-driven machine learning approaches have been developed for this purpose [11; 51; 15], but mostly RNNs are used for the goal of reconstructing DS from measured time series [10; 55; 30; 64; 63; 46; 45]. In scientific settings in particular, but also often in engineering applications, we seek a detailed understanding of the system dynamics captured - or the dynamical repertoire produced - by a trained RNN [62; 33; 61; 14; 13]. To analyze an RNN's dynamics, typically its fixed points are determined numerically, for instance by optimizing some cost function [21; 34; 61], by numerically traversing directional fibers [29], or by co-training a switching linear dynamical system [56]. These techniques, however, often scale poorly with dimensionality, provide only approximate solutions, and are not designed for detecting other dynamical objects like \(k\)-cycles. This is, however, crucial for understanding the complete dynamical repertoire of a trained RNN. PLRNNs are of particular interest in this context [10; 55; 30; 38], because their piecewise linear nature makes some of their properties analytically accessible [55; 10], which is a tremendous advantage from the perspective of DST. In this work, we develop an efficient heuristic algorithm for locating a PLRNN's fixed points exactly, as well as its \(k\)-cycles.

Bifurcations and loss jumps in RNN trainingThe idea that bifurcations in RNN dynamics could impede the training process is not new [16; 44]. Doya [16], to our knowledge, was the first to point out that even in simple single-unit RNNs with sigmoid activation function (saddle-node) bifurcations may occur as an RNN parameter is adapted during training. This may not only cause an abrupt jump in training loss, but could lead to situations where it is impossible, even in principle, to reach the training objective (the desired target output), as across the bifurcation point there is a _discrete_ change in network behavior [16; 68; 4]. Pascanu et al. [44] discussed similar associations between steep cliffs in RNN loss functions and bifurcations. Although profound for training success, this topic received surprisingly little attention over the years. Haschke and Steil [22] extended previous work by a more formal treatment of bifurcation boundaries in RNNs, and Marichal et al. [35] examined fold bifurcations in RNNs. The effects of bifurcations and their relation to exploding gradients in gated recurrent units (GRUs) was investigated in Kanai et al. [27]. Ribeiro et al. [50] looked at the connection between dynamics and smoothness of the cost function, but failed to find a link between bifurcations and jumps in performance. In contrast, Rehmer and Kroll [49] observed large gradients at bifurcation boundaries and concluded that bifurcations can indeed cause problems in gradient-based optimization. To the best of our knowledge, we are, however, the first to _formally prove_ a direct link between bifurcations and the behavior of loss gradients, and to derive a systematic and efficient algorithmic procedure for identifying bifurcation manifolds for a class of ReLU-based RNNs.

## 3 Theoretical analysis

In this paper we will focus on PLRNNs as one representative of the wider class of ReLU-based RNNs, but similar derivations and algorithmic procedures could be devised for any type of ReLU-based RNN (in fact, many other types of ReLU-based RNNs could be brought into the same functional form as PLRNNs; e.g. [10]). We will first, in sect. 3.1, provide some theoretical background on bifurcations in PLRNNs, and illustrate how existence and stability regions of fixed points and cycles could be analytically computed for low dimensional (\(2d\)) PLRNNs. In sect. 3.2 we will then state two theorems regarding the association of bifurcations and loss gradients in training. It turns out that for certain types of bifurcations exploding or vanishing gradients are inevitable in gradient-based training procedures like Back-Propagation Through Time (BPTT).

### Bifurcation curves in PLRNN parameter space

The PLRNN, originally introduced as a kind of discrete time neural population model [18], has the general form

\[\bm{z}_{t}=F_{\bm{\theta}}(\bm{z}_{t-1},\bm{s}_{t})=\bm{A}\:\bm{z}_{t-1}+\bm{ W}\phi(\bm{z}_{t-1})+\bm{C}\bm{s}_{t}+\bm{h},\] (1)

where \(\bm{z}_{t}\in\mathbb{R}^{M}\) is the latent state vector and \(\bm{\theta}\) are system parameters consisting of diagonal matrix \(\bm{A}\in\mathbb{R}^{M\times M}\) (auto-regression weights), off-diagonal matrix \(\bm{W}\in\mathbb{R}^{M\times M}\) (coupling weights), \(\phi(\bm{z}_{t-1})=\max(\bm{z}_{t-1},0)\) is the element-wise rectified linear unit (\(ReLU\)) function, \(\bm{h}\in\mathbb{R}^{M}\) a constant bias term, and \(\bm{s}_{t}\in\mathbb{R}^{K}\) represents external inputs weighted by \(\bm{C}\in\mathbb{R}^{M\times K}\). The original formulation of the PLRNN is stochastic [18; 30], with a Gaussian noise term added to eq. (1), but here we will consider the deterministic variant.

Formally, like other ReLU-based RNNs, PLRNNs constitute piecewise linear (PWL) maps, a subclass of piecewise smooth (PWS) discrete-time DS. Define \(\bm{D}_{\Omega(t)}\coloneqq\mathrm{diag}(\bm{d}_{\Omega(t)})\) as a diagonal matrix with indicator vector \(\bm{d}_{\Omega(t)}\coloneqq(d_{1},d_{2},\cdots,d_{M})\) such that \(d_{m}(z_{m,t})\eqeqcolon d_{m}=1\) whenever \(z_{m,t}>0\), and zero otherwise. Then (1) can be rewritten as

\[\bm{z}_{t}\,=\,F_{\bm{\theta}}(\bm{z}_{t-1})\,=\,(\bm{A}+\bm{W}\bm{D}_{\Omega (t-1)})\bm{z}_{t-1}\,+\,\bm{h}\eqqcolon\bm{W}_{\Omega(t-1)}\:\bm{z}_{t-1}\,+\, \bm{h},\] (2)

where we have ignored external inputs \(\bm{s}_{t}\) for simplicity. There are in general \(2^{M}\) different configurations for matrix \(\bm{D}_{\Omega(t-1)}\) and hence for matrix \(\,\bm{W}_{\Omega(t-1)}\), dividing the phase space into \(2^{M}\) sub-regions separated by switching manifolds (see Appx. A.1.1 for more details).

Recall that fixed points of a map \(\bm{z}_{t}=F_{\bm{\theta}}(\bm{z}_{t-1})\) are defined as the set of points for which we have \(\bm{z}^{*}=F_{\bm{\theta}}(\bm{z}^{*})\), and that the type (node, saddle, spiral) and stability of a fixed point can be read off from the eigenvalues of the Jacobian \(\bm{J}_{t}\coloneqq\frac{\partial F_{\bm{\theta}}(\bm{z}_{t-1})}{\partial\bm{ z}_{t-1}}=\frac{\partial\bm{z}_{t}}{\partial\bm{z}_{t-1}}\) evaluated at \(\bm{z}^{*}\)[3; 47]. Similarly, a \(k\)-cycle of map \(F_{\bm{\theta}}\) is a periodic orbit \(\{\bm{z}_{1}^{*},\bm{z}_{2}^{*},\ldots,\bm{z}_{k}^{*}\}\) such that each of the periodic points \(\bm{z}_{i}^{*},i=1\ldots k\), is distinct, and is a solution to the equation \(\bm{z}_{i}^{*}=F_{\bm{\theta}}^{k}(\bm{z}_{i}^{*})\), i.e. the \(k\) times iterated map \(F_{\bm{\theta}}\). Type and stability of a \(k\)-cycle are then determined via the Jacobian \(\prod_{r=1}^{k}\bm{J}_{t+k-r}=\prod_{r=1}^{k}\frac{\partial\bm{z}_{t+k-r}}{ \partial\bm{z}_{t+k-r-1}}=\frac{\partial\bm{z}_{t+k-1}}{\partial\bm{z}_{t-1}}\). Solving these equations and computing the corresponding Jacobians thus allows to determine all existence and stability regions of fixed points and cycles, where the latter are a subset of the former, bounded by bifurcation curves (see Appx. A.1 for more formal details).

To provide a specific example, assume \(M=2\) and fix - for the purpose of this exposition - parameters \(w_{12}=w_{22}=0\), such that we have

\[\bm{W}_{\Omega^{1}}=\bm{W}_{\Omega^{3}}=\begin{pmatrix}a_{11}&0\\ 0&a_{22}\end{pmatrix}\,,\qquad\quad\bm{W}_{\Omega^{2}}=\bm{W}_{\Omega^{4}}= \begin{pmatrix}a_{11}+w_{11}&0\\ w_{21}&a_{22}\end{pmatrix}\,,\] (3)

i.e. only one border which divides the phase space into two distinct sub-regions (see Appx. A.1.1). For this setup, Fig. 1A provides examples of analytically determined stability regions for two low order cycles in the \((a_{11},\,a_{11}+w_{11})\)-parameter plane (see Appx. A.1). Note that there are regions in parameter space where two or more stability regions overlap: In these regions we have _multi-stability_, the co-existence of different attractor states in the PLRNN's state space.

As noted above, bifurcation curves delimit the different stability regions in parameter space and are hence associated with abrupt changes in the topological structure of a system's state space. In general, there are many different types of bifurcations through which dynamical objects can come into existence, disappear, or change stability (see, e.g., [3; 40; 47]), the most common ones being saddle node, transcritical, pitchfork, homoclinic, and Hopf bifurcations. In comparison with smooth systems, bifurcation theory of PWS (or PWL) maps includes additional dynamical phenomena related to the existence of borders in the phase space [6]. _Border-collision bifurcations (BCBs)_ arise when for a PWS map a specific point of an invariant set collides with a border and this collision leads to a qualitative change of dynamics [5; 6; 42]. More specifically, a BCB occurs, if for a PWS map \(\bm{z}_{t}\,=\,F_{\bm{\theta}}(\bm{z}_{t-1})\) a fixed point or \(k\)-cycle either _crosses_ the switching manifold \(\sum_{i}:=\{\bm{z}\in\mathbb{R}^{n}:\bm{e}_{i}^{\intercal}\bm{z}=0\}\) transversely at \(\theta=\theta^{*}\) and its qualitative behavior changes in the event, or if it _collides_ on the border with another fixed point or \(k\)-cycle and both objects disappear [7; 41]. _Degenerate transcritical bifurcations (DTBs)_ occur when a fixed point or a periodic point of a cycle tends to infinity and one of its eigenvalues tends to \(1\) by variation of a parameter. Specifically, let \(\Gamma_{k},k\geq 1,\) be a fixed point or a \(k\)-cycle with the periodic points \(\{\bm{z}_{1}^{*},\bm{z}_{2}^{*},\ldots,\bm{z}_{k}^{*}\}\), and assume \(\,\lambda^{i}\) denotes an eigenvalue of the Jacobian matrix at the periodic point \(\bm{z}_{i}^{*},i\in\{1,2,\cdots,k\}\). Then \(\Gamma_{k}\) undergoes a DTB at \(\theta=\theta^{*}\), if \(\lambda^{i}(\theta^{*})\to+1\) and \(\|\bm{z}_{i}^{*}\|\to\infty\). \(\Gamma_{k}\) undergoes a _degenerate flip bifurcation (DFB)_, iff \(\lambda^{i}(\theta^{*})=-1\) and the map \(F^{k}\) has locally, in some neighborhood of \(\bm{z}_{i}^{*}\), infinitely many \(2\)-cycles at \(\theta=\theta^{*}\). A _center bifurcation (CB)_ occurs, if \(\Gamma_{k}\) has a pair of complex conjugate eigenvalues \(\lambda_{1,2}\) and locally becomes a center at the bifurcation value \(\theta=\theta^{*}\), i.e. if its eigenvalues are complex and lie on the unit circle (\(|\lambda_{1,2}(\theta^{*})|=1\)). Another important class of bifurcations are _multiple attractor bifurcations (MABs)_, discussed in more detail in Appx. A.1.5 (see Fig. S3). In addition to existence and stability regions of fixed points and cycles, in Appx. A.1.2-A.1.4 we also illustrate how to analytically determine the types of bifurcation curves bounding the existence and stability regions of \(k\)-cycles (DTB, DFB, CB and BCB curves; see also Fig. S6 for a \(1d\) example).

### Bifurcations and loss jumps in training

Here we will prove that major types of bifurcations discussed above are always associated with exploding or vanishing gradients in PLRNNs during training, and hence often with abrupt jumps in the loss. For this we may assume any generic loss function \(\mathcal{L}(\bm{\theta})\), like a negative log-likelihood or a mean-squared-error (MSE) loss, and a gradient-based training technique like BPTT [52] or Real-Time-Recurrent-Learning (RTRL) that involves a recursion (via chain rule) through loss terms across time. The first theorem establishes that a DTB inevitably causes exploding gradients.

**Theorem 1**.: _Consider a PLRNN of the form (2) with parameters \(\bm{\theta}=\{\bm{A},\bm{W},\bm{h}\}\). Assume that it has a stable fixed point or \(k\)-cycle \(\Gamma_{k}\,(k\geq 1)\) with \(\mathcal{B}_{\Gamma_{k}}\) as its basin of attraction. If \(\Gamma_{k}\) undergoes a degenerate transcritical bifurcation (DTB) for some parameter value \(\theta=\theta_{0}\in\bm{\theta}\), then the norm of the PLRNN loss gradient, \(\left\|\frac{\partial\mathcal{L}_{t}}{\partial\theta}\right\|\), tends to infinity at \(\theta=\theta_{0}\) for every \(\bm{z}_{1}\in\mathcal{B}_{\Gamma_{k}}\), i.e. \(\lim_{\theta\to\theta_{0}}\left\|\frac{\partial\mathcal{L}_{t}}{\partial \theta}\right\|=\infty\)._

Proof.: See Appx. A.2.1 

However, bifurcations may also cause gradients to suddenly vanish, as it is the case for a BCB as established by our second theorem:

**Theorem 2**.: _Consider a PLRNN of the form (2) with parameters \(\bm{\theta}=\{\bm{A},\bm{W},\bm{h}\}\). Assume that it has a stable fixed point or \(k\)-cycle \(\Gamma_{k}\,(k\geq 1)\) with \(\mathcal{B}_{\Gamma_{k}}\) as its basin of attraction. If \(\Gamma_{k}\) undergoes a border collision bifurcation (BCB) for some parameter value \(\theta=\theta_{0}\in\bm{\theta}\), then the gradient of the loss function, \(\frac{\partial\mathcal{L}_{t}}{\partial\theta}\), vanishes at \(\theta=\theta_{0}\) for every \(\bm{z}_{1}\in\mathcal{B}_{\Gamma_{k}}\), i.e. \(\lim_{\theta\to\theta_{0}}\left\|\frac{\partial\mathcal{L}_{t}}{\partial \theta}\right\|=0\)._Proof.: See Appx. A.2.2. 

**Corollary 1**.: _Assume that the PLRNN (2) has a stable fixed point \(\Gamma_{1}\) with \(\mathcal{B}_{\Gamma_{1}}\) as its basin of attraction. If \(\Gamma_{1}\) undergoes a degenerate flip bifurcation (DFB) for some parameter value \(\theta=\theta_{0}\in\bm{\theta}\), then this will always coincide with a BCB of a \(2\)-cycle, and as a result \(\lim_{\theta\rightarrow\theta_{0}}\left\|\frac{\partial\mathcal{L}_{t}}{ \partial\theta}\right\|=0\) for every \(\bm{z}_{1}\in\mathcal{B}_{\Gamma_{1}}\)._

Proof.: See Appx. A.2.3. 

Hence, certain bifurcations will inevitably cause gradients to suddenly explode or vanish, and often induce abrupt jumps in the loss (see Appx. A.3.2 for when this will happen for a BCB). We emphasize that these results are general and hold _for systems of any dimension_, as well as _in the presence of inputs_. Since inputs do not affect the Jacobians in eqn. (70), (71) and (76), they do not change the theorems (even if they would affect the Jacobians, Theorem 2 would be unaltered, and Theorem 1 could be amended in a straightforward way). Furthermore, since we are addressing bifurcations that occur during model training, from this angle inputs may simply be treated as either additional parameters (if piecewise constant) or states of the system (without changing any of the mathematical derivations). In fact, mathematically, any non-autonomous dynamical system (RNN with inputs) can always and strictly be reformulated as an autonomous system (RNN without inputs), see [3, 47, 66].

## 4 Heuristic algorithm for finding PLRNN bifurcation manifolds

### Searcher for fixed points and cycles (SCYFI): motivation and validation

In sect. 3.1 and Appx. A.1.2-A.1.4 we derived existence and stability regions for fixed points and low order (\(k\leq 3\)) cycles in \(2d\) PLRNNs with specific parameter constraints analytically. For higher-order cycles and higher-dimensional PLRNNs (or any other ReLU-type RNN) this is no longer feasible due to the combinatorial explosion in the number of subregions that need to be considered as \(M\) and \(k\) increase. Here we therefore introduce an efficient search algorithm for finding all \(k\)-cycles of a given PLRNN, which we call _Searcher for **Cycles** and **Fixed points**_: **SCYFI** (Algorithm 1). Once all \(k\)-cycles (\(k\geq 1\)) have been detected on some parameter grid, the stability-/existence regions of these objects and thereby the bifurcation manifolds can be determined. \(k\)-cycles were defined in sect. 3.1,

Figure 1: A) Analytically calculated stability regions for a \(2\)-cycle (\(\mathcal{S}_{\mathcal{R}\mathcal{L}}\), red), a \(3\)-cycle (\(\mathcal{S}_{\mathcal{R}\mathcal{L}^{2}}\), blue), and their intersection (yellow) in the \((a_{11},\,a_{11}+w_{11})\)-parameter plane for the system eq. (3) with \(a_{22}=0.2\), \(w_{21}=0.5\). B) Same as determined by SCYFI, with bifurcation curves bordering the stability regions labeled by the type of bifurcation (DTB = Degenerate Transcritical Bifurcation, BCB= Border Collision Bifurcation, DFB = Degenerate Flip Bifurcation). C) Bifurcation graph (showing the stable cyclic points in the \(z_{1}\) coordinate) along the cross-section in A indicated by the gray line, illustrating the different types of bifurcation encountered when moving in and out of the various stability regions in A. D) State space at the point denoted ’D’ in A (for \(a_{11}=0.253\), \(a_{11}+w_{11}=-2.83\)), where the \(2\)-cycle (red) and \(3\)-cycle (blue) co-exist for the same parameter settings, with their corresponding basins of attraction indicated by lighter colors.

and for the PLRNN, eq. (2), are given by the set of \(k\)-periodic points \(\{\bm{z}_{1}^{*},\ldots,\bm{z}_{l}^{*},\ldots,\bm{z}_{k}^{*}\}\), where

\[\bm{z}_{k}^{*}= \bigg{(}\mathds{1}-\prod_{r=0}^{k-1}\bm{W}_{\Omega(k-r)}\bigg{)}^{ -1}\bigg{[}\sum_{j=2}^{k-1}\prod_{r=0}^{k-j}\bm{W}_{\Omega(k-r)}+\mathds{1} \bigg{]}\bm{h},\] (4)

if \(\big{(}\mathds{1}-\prod_{r=0}^{k-1}\bm{W}_{\Omega(k-r)}\big{)}\) is invertible (if not, we are dealing with a bifurcation or a continuous set of fixed points). The other periodic points are \(\bm{z}_{l}=F^{l}(\bm{z}_{k}^{*}),l=1,\cdots,k-1\), with corresponding matrices \(\bm{W}_{\Omega(l)}\,=\,\bm{A}+\bm{W}\bm{D}_{l}\,\). Now, if the diagonal entries in \(\bm{D}_{l}\) are consistent with the signs of the corresponding states \(z_{ml}^{*}\), i.e. if \(d_{mm}^{(l)}=1\) if \(z_{ml}^{*}>0\) and \(d_{mm}^{(l)}=0\) otherwise for all \(l\), \(\{\bm{z}_{1}^{*},\ldots,\bm{z}_{k}^{*}\}\) is a true cycle of eq. (2), otherwise we call it _virtual_. To find a \(k\)-cycle, since an \(M\)-dimensional PLRNN harbors \(2^{M}\) different linear sub-regions, there are approximately \(2^{Mk}\) different combinations of configurations of the matrices \(\bm{D}_{l},l=1\ldots k\), to consider (strictly, mathematical constraints rule out some of these possibilities, e.g. not all periodic points can lie within the same sub-region/orthant).

Clearly, for higher-dimensional PLRNNs and higher cycle orders exhaustively searching this space becomes unfeasible. Instead, we found that the following heuristic works surprisingly well: First, for some order \(k\) and a random initialization of the matrices \(\bm{D}_{l},l=1\ldots k\), generate a cycle candidate by solving eq. (4). If each of the points \(\bm{z}_{l}^{*},l=1\ldots k\), is consistent with the diagonal entries in the corresponding matrix \(\bm{D}_{l},l=1\ldots k\), and none of them is already in the current library of cyclic points, then a true \(k\)-cycle has been identified, otherwise the cycle is virtual (or a super-set of lower-order cycles). We discovered that the search becomes extremely efficient, without the need to exhaustively consider all configurations, if a new search loop is re-initialized at the last visited virtual cyclic point (i.e., all _inconsistent_ entries \(d_{mm}^{(l)}\) in the matrices \(\bm{D}_{l},l=1\ldots k\), are flipped, \(d_{mm}^{(l)}\to 1-d_{mm}^{(l)}\), to bring them into agreement with the signs of the solution points, \(\bm{z}_{l}^{*}=[z_{ml}^{*}]\), of eq.

(4), thus yielding the next initial configuration). It is straightforward to see that this procedure almost surely converges if \(N_{out}\) is chosen large enough, see Appx. A.2.4. The whole procedure is formalized in Algorithm 1, and the code is available at https://github.com/DurstewitzLab/SCYFI.

To validate the algorithm, we can compare analytical solutions as derived in sect. 3.1 to the output of the algorithm. To delineate all existence and stability regions, the algorithm searches for all \(k\)-cycles up to some maximum order \(K\) along a fine grid across the (\(a_{11},\;\;a_{11}+w_{11}\))-parameter plane. A bifurcation happens whenever between two grid points a cycle appears, disappears, or changes stability (as determined from the eigenvalue spectrum of the respective \(k^{th}\)-order Jacobian). The results of this procedure are shown in Fig. 1B, illustrating that the analytical solutions for existence and stability regions precisely overlap with those identified by Algorithm 1 (see also Fig. S7).

### Numerical and theoretical results on SCYFI's scaling behavior

Because of the combinatorial nature of the problem, it is generally not feasible to obtain ground truth settings in higher dimensions for SCYFI to compare to. To nevertheless assess its scaling behavior, we therefore studied two specific scenarios. For an exhaustive search, the expected and median numbers of linear subregions \(n\) until an object of interest (fixed point or cycle) is found, i.e. the number of \(\{\bm{D}_{1:k}\}\) constellations that need to be inspected until the first hit, are given by

\[E[n]=\frac{N+1}{m+1}=\frac{2^{Mk}+1}{m+1},\quad\overline{n}=\text{min}\Bigg{\{} n\in\mathbb{N}\Bigg{|}\binom{2^{Mk}-n}{m}\leq\frac{1}{2}\binom{2^{Mk}}{m} \Bigg{\}}\] (5)

with \(m\) being the number of existing \(k\)-cycles and \(N\) the total number of combinations, as shown in Ahlgren [1] (assuming no prior knowledge about the mathematical limitations when drawing regions). The median \(\overline{n}\) as well as the actual median number of to-be-searched combinations required by SCYFI to find at least one \(k\)-cycle is given for low-dimensional systems in Fig. 2A as a function of cycle order \(k\), and can be seen to be surprisingly linear as confirmed by linear regression fits to the data (see Fig. 2 legend for details). To assess scaling as a function of dimensionality \(M\), we explicitly constructed systems with one known fixed point (see Appx. A.3.1 for details) and determined the number \(n\) of subregions required by SCYFI to detect this embedded fixed point (Fig. 2B). In general, the scaling depended on the system's eigenspectrum, but for reasonable scenarios was polynomial or even sublinear (Fig. 2B, see also Fig. S8). In either case, the number of required SCYFI iterations scaled much more favorably than would be expected from an exhaustive search.

How could this surprisingly good scaling behavior be explained? As shown numerically in Fig. S9, when we initiate SCYFI in different randomly selected linear subregions, it converges to the subregions including the dynamical objects of interest exponentially fast, offsetting the combinatorial explosion. A more specific and stronger theoretical result about SCYFI's convergence speed can be obtained under certain conditions on the parameters (which agrees nicely with the numerical results in Fig. 2). It rests on the observation that SCYFI is designed to move _only among subregions containing virtual or actual fixed points or cycles_, based on the fact that it is always reinitialized with the next virtual fixed (cyclic) point in case the consistency check fails. The result can be stated as follows:

Figure 2: A) Number of linear subregions \(n\) searched until at least one cycle of order \(k\) was found by SCYFI (blue) vs. the median number \(\overline{n}\) an exhaustive search would take by randomly drawing combinations without replacement (black) as a function of cycle order (\(M=2\) fixed). Each data point represents the median of 50 different initializations across \(5\) different PLRNN models. Error bars = median absolute deviation. Linear regression fit using weighted least-squares (\(R^{2}\approx 0.998,p<10^{-30}\)). B) Number of linear subregions \(n\) searched until a specific fixed point was found as function of dimensionality \(M\) for different eigenvalue spectra (see Appx. A.3.1 for details).

**Theorem 3**.: _Consider a PLRNN of the form (2) with parameters \(\bm{\theta}=\{\bm{A},\bm{W},\bm{h}\}\). Under certain conditions on \(\bm{\theta}\) (for which \(\|\bm{A}\|+\|\bm{W}\|<1\)), SCYFI will converge in at most linear time._

Proof.: See Appx. A.2.5 

## 5 Loss landscapes and bifurcation curves

### Bifurcations and loss jumps in training

Fig. 3 provides a \(2d\) toy example illustrating the tight association between the loss landscape and bifurcation curves, as determined through SCYFI, for a PLRNN trained by BPTT on reproducing a specific \(16\)-cycle. Fig. 3A depicts a contour plot of the gradient norms with overlaid bifurcation curves in yellow, while Fig. 3B shows the MSE loss landscape as a relief for better appreciation of the sharp changes in loss height associated with the bifurcation curves. Shown in green are two trajectories from two different parameter initializations traced out during PLRNN training in parameter space, where training was confined to only those two parameters given in the graphs (i.e., all other PLRNN parameters were kept fixed during training for the purpose of this illustration). As confirmed in Fig. 3C & D, as soon as the training trajectory crosses the bifurcation curves in parameter space, a huge jump in the loss associated with a sudden increase in the gradient norm occurs. This illustrates empirically and graphically the theoretical results derived in sect. 3.

Next we illustrate the application of SCYFI on a real-world example, learning the behavior of a rodent spiking cortical neuron observed through time series measurements of its membrane potential (note that spiking is a highly nonlinear behavior involving fast within-spike and much slower between-spike time scales). For this, we constructed a \(6\)-dimensional delay embedding of the membrane voltage [53, 28], and trained a PLRNN with one hidden layer (cf. eq. 6) using BPTT with sparse teacher forcing (STF) [37] to approximate the dynamics of the spiking neuron (see Appx. A.3.2 for a similar analysis on a biophysical neuron model). With \(M=6\) latent states and \(H=20\) hidden dimensions, the trained PLRNN comprises \(2^{20}\) different linear subregions and \(|\bm{\theta}|=272\) parameters, much higher-dimensional than the toy example considered above. Fig. 4A gives the MAE loss as a function of training epoch (i.e., single SGD updates), while Figs. 4B & C illustrate the well-trained behavior in time (Fig. 4B) and in a \(2\)-dimensional projection of the model's state space obtained by PCA (Fig. 4C). The loss curve exhibits several steep jumps. Zooming into one of these regions (Fig. 4A; indicated by the red box) and examining the transitions in parameter space using SCYFI, we find they are indeed produced by bifurcations, with an example given in Fig. 4D. Note that we are now dealing with high-dimensional state and parameter spaces, such that visualization of results becomes tricky. For the bifurcation diagram in Fig. 4D we therefore projected all extracted \(k\)-cycles (\(k\geq 1\)) onto a line given by the PCA-derived maximum eigenvalue component, and plotted this as a function of training epoch.1 Since SCYFI extracts all \(k\)-cycles and their eigenvalue spectrum, we can also determine the type of bifurcation that caused the jump. While before the loss jump the PLRNN already produced time series quite similar to those of the physiologically recorded cell (Fig. 4E), a

Figure 3: A) Logarithm of gradient norm of the loss in PLRNN parameter space, with ground truth parameters centered at \((0,0)\). Superimposed in yellow are bifurcation curves computed by SCYFI, and in green two examples of training trajectories from different parameter initial conditions (indicated by the stars). Red dots indicate the bifurcation crossing time points shown in C & D. B) Relief plot of the loss landscape from A to highlight the differences in loss altitude associated with the bifurcations. C) Loss during the training run represented by the green trajectory labeled C in A and B. Red dot indicates the time point of bifurcation crossing corresponding to the red dot in A and B. D) Same for trajectory labeled D in A and B.

DTB (cf. Theorem 1) produced catastrophic forgetting of the learned behavior with the PLRNN's states suddenly diverging to minus infinity (Fig. 4F; Fig. S10 also provides an example of a BCB during PLRNN training, and Fig. S11 an example of a DFB). This illustrates how SCYFI can be used to analyze the training process with respect to bifurcation events also for high-dimensional real-world examples, as well as the behavior of the trained model (Fig. 4C).

### Implications for designing training algorithms

What are potential take-homes of the results in sects. 3.2 & 5.1 for designing RNN training algorithms? One possibility is to design smart initialization or training procedures that aim to place or push an RNN into the right topological regime by taking big leaps in parameter space whenever the current regime is not fit for the data, rather than dwelling within a wrong regime for too long. These ideas are discussed in a bit more depth in Appx.A.3.3, with a proof of concept in Fig. S12.

Figure 4: A) Loss across training epochs for a PLRNN with one hidden layer trained on electrophysiological recordings from a cortical neuron. Red box zooms in on one of the training phases with a huge loss jump, caused by a DTB. Letters refer to selected training epochs in other subpanels. B) Time series of true (gray) and PLRNN-simulated (black) membrane potential in the well trained regime (see A). C) All fixed points and cycles discovered by SCYFI for the well-trained model in state space projected onto the first two principle components using PCA. Filled circles represent stable and open circles unstable objects. The stable 39-cycle corresponds to the spiking behavior. D) Bifurcation diagram of the PLRNN as a function of training epoch around the loss peak in A. Locations of stable (filled circles) and unstable (open circles) objects projected onto the first principle component. E) Model behavior as in B shortly before the DTB and associated loss jump (from the epoch indicated in A, D). F) Model behavior as in B right around the DTB (diverging to \(-\infty\)).

Figure 5: Example loss curves during training a PLRNN (\(M=10\)) on a \(2d\) cycle using gradient descent, once without GTF (\(\alpha=0\), blue curve) but gradient clipping, and once with GTF (\(\alpha=0.1\)). Note that without GTF there are several sharp loss jumps associated with bifurcations in the PLRNN parameters, while activating GTF leads to a smooth loss curve avoiding bifurcations. Note: For direct comparability both loss curves were cut off at \(4\) and then scaled to \([0,1]\).The absolute loss is much lower for GTF.

More importantly, however, we discovered that the recently proposed technique of 'generalized teacher forcing (GTF)' [24] tends to circumvent bifurcations in RNN training altogether, leading to much faster convergence as illustrated in Fig. 5. The way this works is that GTF, by trading off forward-iterated RNN latent states with data-inferred states according to a specific annealing schedule during training (see Appx. A.2.6), tends to pull the RNN directly into the right dynamical regime. In fact, for DTBs we can strictly prove these will never occur in PLRNN training with the right adjustment of the GTF parameter:

**Theorem 4**.: _Consider a PLRNN of the form (2) with parameters \(\bm{\theta}=\{\bm{A},\bm{W},\bm{h}\}\). Assume that it has a stable fixed point or \(k\)-cycle \(\Gamma_{k}\) (\(k\geq 1\)) that undergoes a degenerate transcritical bifurcation (DTB) for some parameter value \(\theta=\theta_{0}\in\bm{\theta}\)._

1. _If_ \(\|\bm{A}\|+\|\bm{W}\|\leq 1\)_, then for any GTF parameter_ \(0<\alpha<1\)_, GTF controls the system, avoiding a DTB and, hence, gradient divergence at_ \(\theta_{0}\)_._
2. _If_ \(\|\bm{A}\|+\|\bm{W}\|=r>1\)_, then for any_ \(1-\frac{1}{r}<\alpha<1\)_, GTF prevents a DTB and, hence, gradient divergence at_ \(\theta_{0}\)_._

Proof.: See Appx. A.2.6. 

As this example illustrates, we may be able to amend training procedures such as to avoid specific types of bifurcations.

## 6 Discussion

DS theory [3; 47; 58] is increasingly appreciated in the ML/AI community as a powerful mathematical framework for understanding both the training process of ML models [49; 54; 44; 16] as well as the behavior of trained models [62; 33; 61]. While the latter is generally useful for understanding how a trained RNN performs a given ML task, with prospects of improving found solutions, it is in fact imperative in areas like science or medicine where excavating the dynamical behavior and repertoire of trained models yields direct insight into the underlying physical, biological, or medical processes the model is supposed to capture. However, application of DS theory is often not straightforward, especially when dealing with higher-dimensional systems, and commonly requires numerical routines that may only find some of the dynamical objects of interest, and also only approximate solutions. One central contribution of the present work therefore was the design of a novel algorithm, SCYFI, that can exactly locate fixed points and cycles of a wide class of ReLU-based RNNs. This provides an efficient instrument for the DS analysis of trained models, supporting their interpretability and explainability.

A surprising observation was that SCYFI often finds cycles in only linear time, despite the combinatorial nature of the problem, a feature shared with the famous Simplex algorithm for solving linear programming tasks [36; 32; 57]. While we discovered numerically that SCYFI for empirically relevant scenarios converges surprisingly fast, deriving strict theoretical guarantees is hard, and so far we could establish stronger theoretical results on its convergence properties only under specific assumptions on the RNN parameters. Further theoretical work is therefore necessary to precisely understand why the algorithm works so effectively.

In this work we applied SCYFI to illuminate the training process itself. Since RNNs are themselves DS, they are subject to different forms of bifurcations during training as their parameters are varied under the action of a training algorithm (similar considerations may apply to very deep NNs). It has been recognized for some time that bifurcations in RNN training may give rise to sudden jumps in the loss [44; 16], but the phenomenon has rarely been treated more systematically and mathematically. Another major contribution of this work thus was to formally prove a strict connection between three types of bifurcations and abrupt changes in the gradient norms, and to use SCYFI to further reveal such events during PLRNN training on various example systems. There are numerous other types of bifurcations (e.g., center bifurcations, Hopf bifurcations etc.) that are likely to impact gradients during training, only for a subset of which we could provide formal proofs here. As we have demonstrated, understanding the topological and bifurcation landscape of RNNs could help improve training algorithms and provide insights into their working. Hence, a more general understanding of how various types of bifurcation affect the training process in a diverse range of RNN architectures is a promising future avenue not only for our theoretical understandings of RNNs, but also for guiding future algorithm design.

AcknowledgementsThis work was supported by the German Research Foundation (DFG) through individual grants Du 354/10-1 & Du 354/15-1 to DD, within research cluster FOR-5159 ("Resolving prefrontal flexibility"; Du 354/14-1), and through the Excellence Strategy EXC 2181/1 - 390900948 (STRUCTURE). We also thank Mahasheta Patra for lending us code for graphing analytically derived bifurcation diagrams and state spaces.

## References

* Ahlgren [2014] J. Ahlgren. The probability distribution for draws until first success without replacement. _arXiv preprint arXiv:1404.1161_, 2014.
* Alahi et al. [2016] A. Alahi, K. Goel, V. Ramanathan, A. Robicquet, L. Fei-Fei, and S. Savarese. Social lstm: Human trajectory prediction in crowded spaces. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, June 2016.
* Alligood et al. [1996] Kathleen T. Alligood, Tim D. Sauer, and James A. Yorke. _Chaos: An Introduction to Dynamical Systems_. Springer, New York, NY, 1996.
* Arora et al. [2022] S. Arora, Z. Li, and A. Panigrahi. Understanding gradient descent on the edge of stability in deep learning. In _International Conference on Machine Learning_, pages 948-1024. PMLR, 2022.
* Avrutin et al. [2012] V. Avrutin, M. Schanz, and S. Banerjee. Occurrence of multiple attractor bifurcations in the two-dimensional piecewise linear normal form map. _Nonlinear Dynamics_, 67:293-307, 2012.
* Avrutin et al. [2019] V. Avrutin, L. Gardini, I. Sushko, and F. Tramontana. _Continuous and Discontinuous Piecewise Smooth One-Dimensional Maps: Invariant Sets and Bifurcation structures_, volume 1. World Scientific, Singapore, 2019.
* Banerjee et al. [2000] S. Banerjee, MS. Karthik, G. Yuan, and J. A. Yorke. Bifurcations in one-dimensional piecewise smooth maps-theory and applications in switching circuits. _IEEE Transactions on Circuits and Systems I: Fundamental Theory and Applications_, 47(3):389-394, 2000.
* Bengio et al. [1994] Y. Bengio, P. Simard, and P. Frasconi. Learning long-term dependencies with gradient descent is difficult. _IEEE Transactions on Neural Networks_, 5(2):157-166, 1994. doi: 10.1109/72.279181.
* Bouktif et al. [2020] S.h Bouktif, A. Fiaz, A. Ouni, and M. A. Serhani. Multi-sequence lstm-rnn deep learning and metaheuristics for electric load forecasting. _Energies_, 13(2), 2020. ISSN 1996-1073. doi: 10.3390/en13020391.
* Brenner et al. [2022] M. Brenner, F. Hess, J. M. Mikhaeil, L. F. Bereska, Z. Monfared, P. Kuo, and D. Durstewitz. Tractable Dendritic RNNs for Reconstructing Nonlinear Dynamical Systems. In _Proceedings of the 39th International Conference on Machine Learning_, pages 2292-2320. PMLR, June 2022. ISSN: 2640-3498.
* Brunton et al. [2016] S. L. Brunton, J. L. Proctor, and J. N. Kutz. Discovering governing equations from data by sparse identification of nonlinear dynamical systems. _Proceedings of the National Academy of Sciences_, 113(15):3932-3937, 2016.
* Champion et al. [2019] K. Champion, B. Lusch, J. N. Kutz, and S. L. Brunton. Data-driven discovery of coordinates and governing equations. _Proceedings of the National Academy of Sciences_, 116(45):22445-22451, 2019. doi: 10.1073/pnas.1906995116.
* Chatziafratis et al. [2020] V. Chatziafratis, S. G. Nagarajan, and I. Panageas. Better depth-width trade-offs for neural networks through the lens of dynamical systems. In Hal Daume III and Aarti Singh, editors, _Proceedings of the 37th International Conference on Machine Learning_, volume 119 of _Proceedings of Machine Learning Research_, pages 1469-1478. PMLR, 13-18 Jul 2020. URL https://proceedings.mlr.press/v119/chatziafratis20a.html.
* Chatziafratis et al. [2022] V. Chatziafratis, I. Panageas, and S. Sanford, C.and Stavroulakis. On scrambling phenomena for randomly initialized recurrent networks. _Advances in Neural Information Processing Systems_, 35:18501-18513, 2022.

* de Silva et al. [2020] B. M. de Silva, K. Champion, M. Quade, J. Loiseau, J. N. Kutz, and. L. Brunton. Pysindy: A python package for the sparse identification of nonlinear dynamics from data, 2020. URL https://arxiv.org/abs/2004.08424.
* Doya [1992] K. Doya. Bifurcations in the learning of recurrent neural networks. _Proceedings 1992 IEEE International Symposium on Circuits and Systems_, 6:2777-2780, 1992.
* Durstewitz [2009] D. Durstewitz. Implications of synaptic biophysics for recurrent network dynamics and active memory. _Neural Networks_, 22(8):1189-1200, 2009. ISSN 0893-6080. doi: https://doi.org/10.1016/j.neunet.2009.07.016. URL https://www.sciencedirect.com/science/article/pii/S0893608009001622. Cortical Microcircuits.
* Durstewitz [2017] D. Durstewitz. A state space approach for piecewise recurrent neural networks for reconstructing nonlinear dynamics from neural measurements. _PLoS Computational Biology_, 13(6), 2017.
* Ganguli and Banerjee [2005] A. Ganguli and S. Banerjee. Dangerous bifurcation at border collision: When does it occur? _Physical Review E_, 71, 2005.
* Gardini et al. [2011] L. Gardini, D. Fournier-Prunaret, and P. Charge. Border collision bifurcations in a two-dimensional piecewise smooth map from a simple switching circuit. _Chaos: An Interdisciplinary Journal of Nonlinear Science_, 21, 2011.
* Golub and Sussillo [2018] M. Golub and D. Sussillo. FixedPointFinder: A Tensorflow toolbox for identifying and characterizing fixed points in recurrent neural networks. _Journal of Open Source Software_, 3(31):1003, November 2018. ISSN 2475-9066.
* Haschke and Steil [2005] R. Haschke and J. J. Steil. Input space bifurcation manifolds of recurrent neural networks. _Neurocomputing_, 64:25-38, March 2005. ISSN 0925-2312. doi: 10.1016/j.neucom.2004.11.030.
* Hernandez et al. [2020] D.l Hernandez, A. K. Moretti, Z.g Wei, S. Saxena, J. Cunningham, and L. Paninski. Nonlinear Evolution via Spatially-Dependent Linear Dynamics for Electrophysiology and Calcium Data, June 2020. arXiv:1811.02459 [cs, q-bio, stat].
* Hess et al. [2023] F. Hess, Z. Monfared, M. Brenner, and D. Durstewitz. Generalized teacher forcing for learning chaotic dynamics. In _Proceedings of the 40th International Conference on Machine Learning_, volume 202 of _Proceedings of Machine Learning Research_, pages 13017-13049. PMLR, 23-29 Jul 2023. URL https://proceedings.mlr.press/v202/hess23a.html.
* Hochreiter and Schmidhuber [1997] S. Hochreiter and J. Schmidhuber. Long short-term memory. _Neural Computation_, 9(8):1735-1780, 1997.
* Hossen et al. [2018] T. Hossen, A. S. Nair, R. A. Chinnathambi, and P. Ranganathan. Residential load forecasting using deep neural networks (dnn). In _2018 North American Power Symposium (NAPS)_, pages 1-5, 2018. doi: 10.1109/NAPS.2018.8600549.
* Kanai et al. [2017] S. Kanai, Y. Fujiwara, and S. Iwamura. Preventing Gradient Explosions in Gated Recurrent Units. In _Advances in Neural Information Processing Systems_, volume 30. Curran Associates, Inc., 2017.
* Kantz and Schreiber [2004] H. Kantz and T. Schreiber. _Nonlinear time series analysis_, volume 7. Cambridge university press, 2004.
* Katz and Reggia [2018] G. E. Katz and J. A. Reggia. Using Directional Fibers to Locate Fixed Points of Recurrent Neural Networks. _IEEE Transactions on Neural Networks and Learning Systems_, 29(8):3636-3646, August 2018. ISSN 2162-2388. doi: 10.1109/TNNLS.2017.2733544. Conference Name: IEEE Transactions on Neural Networks and Learning Systems.
* Koppe et al. [2019] G. Koppe, H. Toutounji, P. Kirsch, S. Lis, and D. Durstewitz. Identifying nonlinear dynamical systems via generative recurrent neural networks with applications to fmri. _PLOS Computational Biology_, 15(8):1-35, 2019.

* [31] T. Lang and M. Rettenmeier. Understanding consumer behavior with recurrent neural networks. 2017.
* [32] D. G. Luenberger and Y. Ye. _Linear and Nonlinear Programming_. Springer International Publishing, 2016. doi:10.1007/978-3-319-18842-3.
* [33] N. Maheswaranathan and D. Sussillo. How recurrent networks implement contextual processing in sentiment analysis. In _Proceedings of the 37th International Conference on Machine Learning_, volume 119 of _Proceedings of Machine Learning Research_, pages 6608-6619. PMLR, 13-18 Jul 2020.
* [34] N. Maheswaranathan, AH. Williams, MD. Golub, S. Ganguli, and D. Sussillo. Reverse engineering recurrent networks for sentiment classification reveals line attractor dynamics. _Advances in neural information processing systemsv_, 32:15696-15705, 2019.
* [35] R Marichal, JD Pineiro, and E Gonzalez. Study of fold bifurcation in a discrete recurrent neural network. In _Proceedings of the World Congress on Engineering and Computer Science_, volume 2, 2009.
* [36] N. Megiddo. _On the complexity of linear programming_, page 225-268. Econometric Society Monographs. Cambridge University Press, 1987. doi: 10.1017/CCOL0521340446.006.
* [37] J. M. Mikhaei, Z. Monfared, and D. Durstewitz. On the difficulty of learning chaotic dynamics with RNNs. In _Advances in Neural Information Processing Systems_, 2022.
* [38] Z. Monfared and D. Durstewitz. Existence of n-cycles and border-collision bifurcations in piecewise-linear continuous maps with applications to recurrent neural networks. _Nonlinear Dynamics_, 101(2):1037-1052, 2020.
* [39] Z. Monfared and D. Durstewitz. Transformation of ReLU-based recurrent neural networks from discrete-time to continuous-time. In _International Conference on Machine Learning_, pages 6999-7009. PMLR, 2020. ISSN: 2640-3498.
* [40] Z. Monfared, Z. Dadi, N. Miladi Lari, and Z. Afsharnezhad. Existence and nonexistence of periodic solution and hopf bifurcation of a tourism-based social-ecological system. _Optik_, 127 (22):10908-10918, 2016.
* [41] H. E. Nusse and J. A. Yorke. Border-collision bifurcations including "period two to period three" for piecewise smooth systems. _Physica D: Nonlinear Phenomena_, 57(1-2):39-57, 1992.
* [42] H.E. Nusse and J.A. Yorke. Border-collision bifurcations for piecewise smooth one dimensional maps. _Int. J. Bifurc. Chaos_, 5(1):189-207, 1995.
* [43] S. H. Park, B. Kim, C. M. Kang, C. C. Chung, and J. W. Choi. Sequence-to-sequence prediction of vehicle trajectory via lstm encoder-decoder architecture. In _2018 IEEE Intelligent Vehicles Symposium (IV)_, pages 1672-1678, 2018. doi: 10.1109/IVS.2018.8500658.
* [44] R. Pascanu, T. Mikolov, and Y. Bengio. On the difficulty of training recurrent neural networks. In _Proceedings of the 30th International Conference on Machine Learning_, volume 28 of _Proceedings of Machine Learning Research_, pages 1310-1318, Atlanta, Georgia, USA, 17-19 Jun 2013. PMLR.
* [45] D. Patel and E. Ott. Using machine learning to anticipate tipping points and extrapolate to post-tipping dynamics of non-stationary dynamical systems, 2022. URL https://arxiv.org/abs/2207.00521.
* [46] J. Pathak, B. Hunt, M. Girvan, Z.n Lu, and E. Ott. Model-free prediction of large spatiotemporally chaotic systems from data: A reservoir computing approach. _Phys. Rev. Lett._, 120:024102, Jan 2018. doi: 10.1103/PhysRevLett.120.024102.
* [47] L. Perko. _Differential Equations and Dynamical Systems_. Springer New York, NY, 2001.
* [48] P. Razvan. _On Recurrent and Deep Neural Networks_. PhD thesis, 2014. Universite de Montreal.

* Rehmer and Kroll [2022] A. Rehmer and A. Kroll. The effect of the forget gate on bifurcation boundaries and dynamics in Recurrent Neural Networks and its implications for gradient-based optimization. In _2022 International Joint Conference on Neural Networks (IJCNN)_, pages 01-08, July 2022. doi: 10.1109/IJCNN55064.2022.9892458. ISSN: 2161-4407.
* Ribeiro et al. [2020] A. H. Ribeiro, K. Tiels, Lu. A. Aguirre, and T. Schon. Beyond exploding and vanishing gradients: analysing RNN training using attractors and smoothness. In _Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics_, pages 2370-2380. PMLR, June 2020. URL https://proceedings.mlr.press/v108/ribeiro20a.html. ISSN: 2640-3498.
* Rudy et al. [2017] S. H. Rudy, S. L. Brunton, J. L. Proctor, and J. N. Kutz. Data-driven discovery of partial differential equations. _Science Advances_, 3(4):e1602614, 2017. doi: 10.1126/sciadv.1602614.
* Rumelhart et al. [1986] D. E. Rumelhart, G. E. Hinton, and R. J. Williams. Learning representations by back-propagating errors. _nature_, 323(6088):533-536, 1986.
* Sauer et al. [1991] T. Sauer, J. A. Yorke, and M. Casdagli. Embedology. _Journal of statistical Physics_, 65:579-616, 1991.
* Saxe et al. [2013] A. M. Saxe, J. L. McClelland, and S. Ganguli. Exact solutions to the nonlinear dynamics of learning in deep linear neural networks, 2013. URL https://arxiv.org/abs/1312.6120.
* Schmidt et al. [2021] D. Schmidt, G. Koppe, Z. Monfared, M. Beutelspacher, and D. Durstewitz. Identifying nonlinear dynamical systems with multiple time scales and long-range dependencies. _In proceedings of the Ninth International Conference on Learning Representations, ICLR_, 2021.
* Smith et al. [2021] J. Smith, S. Linderman, and D. Sussillo. Reverse engineering recurrent neural networks with Jacobian switching linear dynamical systems. In _Advances in Neural Information Processing Systems_, volume 34, pages 16700-16713. Curran Associates, Inc., 2021.
* Spielman and Teng [2001] D. A. Spielman and S. Teng. Smoothed analysis of algorithms: Why the simplex algorithm usually takes polynomial time, 2001. URL https://arxiv.org/abs/cs/0111050.
* Strogatz [2015] S. H. Strogatz. _Nonlinear Dynamics and Chaos: Applications to Physics, Biology, Chemistry, and Engineering: With Applications to Physics, Biology, Chemistry and Engineering_. CRC Press, 2015.
* Sushko and Gardini [2006] I. Sushko and L. Gardini. Center bifurcation for a two-dimensional piecewise linear map, in business cycles dynamics. _Models and Tools, eds. Puu, T. & Sushko, I. (Springer-Verlag, NY)_, pages 49--78, 2006.
* Sushko and Gardini [2008] I. Sushko and L. Gardini. Center bifurcation for two-dimensional border-collision normal form. _International Journal of Bifurcation and Chaos_, 18:1029-1050, 2008.
* Sussillo and Barak [2013] D. Sussillo and O. Barak. Opening the Black Box: Low-Dimensional Dynamics in High-Dimensional Recurrent Neural Networks. _Neural Computation_, 25(3):626-649, March 2013. ISSN 0899-7667, 1530-888X. doi: 10.1162/NECO_a_00409.
* Turner et al. [2021] E. Turner, K. V. Dabholkar, and O. Barak. Charting and navigating the space of solutions for recurrent neural networks. In _Advances in Neural Information Processing Systems_, volume 34, pages 25320-25333. Curran Associates, Inc., 2021.
* Vlachas et al. [2018] P. R. Vlachas, W. Byeon, Z. Y. Wan, T. P. Sapsis, and P. Koumoutsakos. Data-driven forecasting of high-dimensional chaotic systems with long short-term memory networks. _Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences_, 474(2213):20170844, 2018. doi: 10.1098/rspa.2017.0844.
* Vlachas et al. [2020] P.R. Vlachas, J. Pathak, B.R. Hunt, T.P. Sapsis, M. Girvan, E. Ott, and P. Koumoutsakos. Backpropagation algorithms and reservoir computing in recurrent neural networks for the forecasting of complex spatiotemporal dynamics. _Neural Networks_, 126:191-217, 2020.

* [65] C. Wang, D. Han, Q. Liu, and S. Luo. A deep learning approach for credit scoring of peer-to-peer lending using attention mechanism lstm. _IEEE Access_, 7:2161-2168, 2019. doi: 10.1109/ACCESS.2018.2887138.
* [66] D. Zhang, H.and Liu and Z. Wang. _Controlling chaos: suppression, synchronization and chaotification_. Springer Science & Business Media, 2009.
* [67] Xun Zheng, Manzil Zaheer, Amr Ahmed, Yuan Wang, Eric P. Xing, and Alexander J. Smola. State space LSTM models with particle MCMC inference. _CoRR_, abs/1711.11179, 2017. URL http://arxiv.org/abs/1711.11179.
* [68] X. Zhu, Z. Wang, X. Wang, M. Zhou, and R. Ge. Understanding edge-of-stability training dynamics with a minimalist example. In _The Eleventh International Conference on Learning Representations_, 2023. URL https://openreview.net/forum?id=p7EagBsMAEO.

Appendix

### Analysis of bifurcation curves

#### a.1.1 Plknns

The standard PLRNN [18], given in eq. (1) in sect. 3.1, was defined by

\[\bm{z}_{t}=F_{\bm{\theta}}(\bm{z}_{t-1},\bm{s}_{t})\,=\,\bm{A}\,\bm{z}_{t-1}\,+ \,\bm{W}\phi(\bm{z}_{t-1})\,+\,\bm{C}\bm{s}_{t}\,+\,\bm{h},\]

where \(\phi(\bm{z}_{t-1})=\max(\bm{z}_{t-1},0)\). There are various extensions of this basic architecture like the dendPLRNN [10] or the'shallow PLRNN' (shPLRNN) [24], as used in sect. 5.1 for training on single cell membrane potentials. The latter is essentially a 1-hidden-layer version of the form

\[\bm{z}_{t}=F_{\bm{\theta}}(\bm{z}_{t-1},\bm{s}_{t})=\bm{A}\,\bm{z}_{t-1}+\bm{W }_{1}\phi(\bm{W}_{2}\bm{z}_{t-1}+\bm{h}_{2})+\bm{C}\bm{s}_{t}+\bm{h}_{1},\] (6)

with \(\bm{W}_{1}\in\mathbb{R}^{M\times L}\) and \(\bm{W}_{2}\in\mathbb{R}^{L\times M}\), \(L\geq M\), connectivity matrices, \(\bm{h}_{1}\in\mathbb{R}^{M}\), \(\bm{h}_{2}\in\mathbb{R}^{L}\) bias terms, and all other parameters and variables as in eq. (1). While this formulation is beneficial for training, the shPLRNN can essentially be rewritten in standard PLRNN form (see [24]).

Assume that \(\bm{D}_{\Omega(t)}\coloneqq\mathrm{diag}(\bm{d}_{\Omega(t)})\) is a diagonal matrix with an indicator vector \(\bm{d}_{\Omega(t)}\coloneqq(d_{1},d_{2},\cdots,d_{M})\) such that \(d_{m}(z_{m,t})=:d_{m}=1\) whenever \(z_{m,t}>0\), and zero otherwise. Then eq. (1) can be rewritten as

\[\bm{z}_{t}\,=\,(\bm{A}+\bm{W}\bm{D}_{\Omega(t-1)})\bm{z}_{t-1}\,+\,\bm{C}\bm{ s}_{t}\,+\,\bm{h}\eqqcolon\bm{W}_{\Omega(t-1)}\,\bm{z}_{t-1}\,+\,\bm{C}\bm{s}_{t}\,+\, \bm{h}.\]

Let us ignore the inputs for simplicity. There are \(2^{M}\) different configurations for matrix \(\bm{D}_{\Omega(t-1)}\) and so \(2^{M}\) different forms for matrix \(\,\bm{W}_{\Omega(t-1)}\,\) in the system

\[\bm{z}_{t}\,=\,F_{\bm{\theta}}(\bm{z}_{t-1})\,=\,\bm{W}_{\Omega(t-1)}\,\bm{z}_ {t-1}\,+\,\bm{h}.\] (7)

Thus, the phase space of the system is divided into \(2^{M}\) sub-regions corresponding to the indexed matrices

\[\bm{W}_{\Omega^{k}}\eqqcolon=\bm{A}+\bm{W}\bm{D}_{\Omega^{k}},\ \ \ \ k=1,2,\cdots,2^{M},\] (8)

see [38; 39] for more details. For \(M=2\), assuming

\[\bm{W}=\begin{pmatrix}w_{11}&0\\ w_{21}&0\end{pmatrix},\] (9)

in (8), we have

\[\bm{W}_{\Omega^{1}} =\bm{W}_{\Omega^{3}}=\begin{pmatrix}a_{11}&0\\ 0&a_{22}\end{pmatrix}=\bm{A},\] \[\bm{W}_{\Omega^{2}} =\bm{W}_{\Omega^{4}}=\begin{pmatrix}a_{11}+w_{11}&0\\ w_{21}&a_{22}\end{pmatrix}.\] (10)

Hence, for this parameter constellation, the map simplifies as there exists only one border which divides the phase space into two distinct sub-regions, such that (7) can be rewritten as a map of the form

\[\begin{pmatrix}z_{1,t}\\ z_{2,t}\end{pmatrix} =\,T(z_{1,t-1},z_{2,t-1})\] \[=\,\begin{cases}T_{\mathcal{L}}(z_{1,t-1},z_{2,t-1})&=\, \underbrace{\begin{pmatrix}a_{l}&c\\ b_{l}&d\end{pmatrix}}_{\bm{A}_{\mathcal{L}}}\begin{pmatrix}z_{1,t-1}\\ z_{2,t-1}\end{pmatrix}+\begin{pmatrix}h_{1}\\ h_{2}\end{pmatrix};\ \ \ z_{1,t-1}\leq 0\\ T_{\mathcal{R}}(z_{1,t-1},z_{2,t-1})&=\,\underbrace{ \begin{pmatrix}a_{r}&c\\ b_{r}&d\end{pmatrix}}_{\bm{A}_{\mathcal{R}}}\begin{pmatrix}z_{1,t-1}\\ z_{2,t-1}\end{pmatrix}+\begin{pmatrix}h_{1}\\ h_{2}\end{pmatrix};\ \ \ z_{1,t-1}\geq 0\end{cases},\] (11)

[MISSING_PAGE_FAIL:17]

\[1\pm(a_{l/r}+d)\,+a_{l/r}\,d-b_{l/r}\,c>0\biggr{\}}.\] (20)

Note that when \(\mathcal{D}_{\mathcal{L}/\mathcal{R}}<0\), all the eigenvalues are real and so there cannot be any spiralling orbit.

**Remark 1**.: _Consider the PLRNN (2) with \(M=2\). For the parameter setting (3), i.e._

\[\boldsymbol{W}_{\Omega^{1}}=\boldsymbol{W}_{\Omega^{3}}=\begin{pmatrix}a_{11} &0\\ 0&a_{22}\end{pmatrix}\,=:\,\boldsymbol{A}_{\mathcal{L}},\qquad\quad\boldsymbol{W }_{\Omega^{2}}=\boldsymbol{W}_{\Omega^{4}}=\begin{pmatrix}a_{11}+w_{11}&0\\ w_{21}&a_{22}\end{pmatrix}\,=:\,\boldsymbol{A}_{\mathcal{R}},\] (21)

_the two fixed points \(\mathcal{O}_{\mathcal{L}/\mathcal{R}}=\left(z_{1}^{\mathcal{L}/\mathcal{R}}, z_{2}^{\mathcal{L}/\mathcal{R}}\right)^{\mathsf{T}}\) are given by_

\[\mathcal{O}_{\mathcal{L}}=\biggl{(}\frac{h_{1}}{1-a_{11}},\,\frac{h_{2}}{1-a_ {22}}\biggr{)}^{\mathsf{T}},\quad\quad\mathcal{O}_{\mathcal{R}}=\biggl{(} \frac{h_{1}}{1-a_{11}-w_{11}},\frac{w_{21}\,h_{1}+\left(1-a_{11}-w_{11}\right) h_{2}}{(1-a_{22})(1-a_{11}-w_{11})}\biggr{)}^{\mathsf{T}}.\] (22)

_Hence, the existence regions of admissible fixed points are_

\[E_{\mathcal{O}_{\mathcal{L}}}=\biggl{\{}(h_{1},a_{11},a_{22})\,\Bigl{|}\, \frac{h_{1}}{1-a_{11}}<0\biggr{\}},\quad\quad E_{\mathcal{O}_{\mathcal{R}}}= \biggl{\{}(h_{1},a_{11},a_{22},w_{11})\bigr{|}\,\frac{h_{1}}{1-a_{11}-w_{11}} >0\biggr{\}},\]

_and their stability regions can be obtained as_

\[\mathcal{S}_{\mathcal{L}}=\biggl{\{}(h_{1},a_{11},a_{22})\in E_{\mathcal{O}_{ \mathcal{L}}}\,|\,a_{11}\,a_{22}<1,\;1\pm(a_{11}+a_{22})\,+a_{11}\,a_{22}>0 \biggr{\}},\] (23)

\[\mathcal{S}_{\mathcal{R}}=\biggl{\{}(h_{1},a_{11},a_{22},w_{11})\in E_{ \mathcal{O}_{\mathcal{R}}}\,|(a_{11}+w_{11})a_{22}<1,\,1\pm(a_{11}+w_{11}+a_{ 22})+(a_{11}+w_{11})a_{22}>0\biggr{\}}.\]

**Remark 2**.: _If \(b_{l/r}\,c=0\), then \(\lambda_{1,2}(\mathcal{O}_{\mathcal{L}/\mathcal{R}})\) are real and the stability regions \(\mathcal{S}_{\mathcal{L}/\mathcal{R}}\) become_

\[\mathcal{S}_{\mathcal{L}/\mathcal{R}}\,=\,\biggl{\{}(h_{1},h_{2},a_{l/r},b_{l/ r},c,d)\in E_{\mathcal{O}_{\mathcal{L}/\mathcal{R}}}\,\bigl{|}\,b_{l/r}\,c\,=\,0, \,-1\leq a_{l/r}\leq 1,\,-1\leq d\leq 1\biggr{\}}.\] (24)

The fixed points are regular saddles for all parameters that belong to

\[\biggl{\{}(h_{1},h_{2},a_{l/r},b_{l/r},c,d)\in E_{\mathcal{O}_{\mathcal{L}/ \mathcal{R}}}\,\bigl{|}\,a_{l/r}+d>1,\,a_{l/r}\,d-a_{l/r}-d+1\,<\,b_{l/r}\,c<a _{l/r}\,d\biggr{\}},\] (25)

and in this case \(\lambda_{1}(\mathcal{O}_{\mathcal{L}/\mathcal{R}})>1,\,0<\lambda_{2}(\mathcal{O }_{\mathcal{L}/\mathcal{R}})<1\). Furthermore, they are flip saddles (i.e., with one negative eigenvalue) if parameters are in

\[\biggl{\{}(h_{1},h_{2},a_{l/r},b_{l/r},c,d)\in E_{\mathcal{O}_{\mathcal{L}/ \mathcal{R}}}\,\bigl{|}\,a_{l/r}+d>1,\,a_{l/r}\,d<b_{l/r}\,c<a_{l/r}\,d+a_{l/r} +\,d\,+\,1\biggr{\}}\,\bigcup\]

\[\biggl{\{}(h_{1},h_{2},a_{l/r},b_{l/r},c,d)\in E_{\mathcal{O}_{\mathcal{L}/ \mathcal{R}}}\,\bigl{|}\,d-a_{l/r}-d+1<b_{l/r}\,c<a_{l/r}\,d+a_{l/r}+d+1,\]

\[0<a_{l/r}+d\leq 1,\,a_{l/r}\biggr{\}},\] (26)

for which \(\lambda_{1}(\mathcal{O}_{\mathcal{L}/\mathcal{R}})>1,\,-1<\lambda_{2}( \mathcal{O}_{\mathcal{L}/\mathcal{R}})<0\), as well as in

\[\biggl{\{}(h_{1},h_{2},a_{l/r},b_{l/r},c,d)\in E_{\mathcal{O}_{\mathcal{L}/ \mathcal{R}}}\,\bigl{|}\,a_{l/r}+d\leq-1,\,a_{l/r}\,d<b_{l/r}\,c<a_{l/r}\,d-a_{ l/r}-d+1\biggr{\}}\,\bigcup\]

\[\biggl{\{}(h_{1},h_{2},a_{l/r},b_{l/r},c,d)\in E_{\mathcal{O}_{\mathcal{L}/ \mathcal{R}}}\,\bigl{|}\,a_{l/r}\,d+a_{l/r}+d+1<b_{l/r}\,c<a_{l/r}\,d-a_{l/r}+d-1,\]

\[-1<a_{l/r}+d<0\biggr{\}}\] (27)such that \(0<\lambda_{1}(\mathcal{O}_{\mathcal{L}/\mathcal{R}})<1,\,\lambda_{2}(\mathcal{O}_{ \mathcal{L}/\mathcal{R}})<-1\).

When \(b_{l/r}\,c<0\) and \(|a_{l/r}-d|<2\sqrt{-b_{l/r}\,c}\), the eigenvalues are complex conjugates and both \(\mathcal{O}_{\mathcal{L}}\) and \(\mathcal{O}_{\mathcal{R}}\) are spirally attracting (attracting focus) if \(a_{l/r}\,d-b_{l/r}\,c<1\). In this case, if \(a_{l/r}+d>0\) then they are clockwise spiral, while for \(a_{l/r}+d<0\) the spiralling motion will be counterclockwise. Moreover, for \(a_{l/r}\,d-b_{l/r}\,c>1\) they are repelling foci. Finally, for \(a_{l/r}\,d-b_{l/r}\,c=1\), the fixed points are locally centers and they undergo a CB at the following boundaries:

\[\mathcal{C}_{\mathcal{L}}\,=\,\bigg{\{}(a_{l},b_{l},c,d)\big{|}\,b_{l}\,c<0,\, |a_{l}-d|<2\sqrt{-b_{l}\,c},\,a_{l}\,d-b_{l}\,c=1\bigg{\}},\]

\[\mathcal{C}_{\mathcal{R}}\,=\,\bigg{\{}(a_{r},b_{r},c,d)\big{|}\,b_{r}\,c<0, \,|a_{r}-d|<2\sqrt{-b_{r}\,c},\,a_{r}\,d-b_{r}\,c=1\bigg{\}}.\] (28)

At these boundaries, the fixed points lose their stability with a pair of complex conjugate eigenvalues crossing the unit circle. For the parameters belonging to \(\mathcal{C}_{\mathcal{L}/\mathcal{R}}\), the Jacobian \(J_{\mathcal{L}/\mathcal{R}}\) is a rotation matrix whose determinant is equal to \(1\). In this case, \(J_{\mathcal{L}/\mathcal{R}}\) can be determined by a rotation number which is either rational (\(\frac{p}{q}\)) or irrational (\(\rho\)). Therefore, in some neighborhood of \(\mathcal{O}_{\mathcal{L}/\mathcal{R}}\), there is a region filled with invariant ellipses such that they are periodic with period \(p\) (if the rotation number is a rational number \(\frac{p}{q}\)) or quasiperiodic (if the rotation number is an irrational number \(\rho\)); for more information see [60, 59]. For \((1-d)\,h_{1}+c\,h_{2}\neq 0\), at the boundary

\[\tau_{\mathcal{L}}\,=\,\bigg{\{}(h_{1},h_{2},a_{l},b_{l},c,d)\big{|}\,1-a_{l} -d+a_{l}\,d-b_{l}\,c=0\bigg{\}},\] (29)

the fixed point \(\mathcal{O}_{\mathcal{L}}\) undergoes a DTB, since, if the parameters tend to \(\tau_{\mathcal{L}}\), then \(\mathcal{O}_{\mathcal{L}}\to\pm\infty\) and \(\lambda(\mathcal{O}_{\mathcal{L}})\to 1\). Similarly, for \((1-d)\,h_{1}+c\,h_{2}\neq 0\), a DTB occurs for the fixed point \(\mathcal{O}_{\mathcal{R}}\) at the boundary

\[\tau_{\mathcal{R}}\,=\,\bigg{\{}(h_{1},h_{2},a_{r},b_{r},c,d)\big{|}\,1-a_{r} -d+a_{r}\,d-b_{r}\,c=0\bigg{\}}.\] (30)

A DTB of a fixed point results in its disappearance, as in this case the fixed point becomes virtual which may lead to changes in the global dynamics [6]. Furthermore, the BCB curves are given by

\[\xi_{\mathcal{L}}\,=\,\bigg{\{}(h_{1},h_{2},a_{l},b_{l},c,d)\,\big{|}\,(1-d)(1 -a_{l})-b_{l}\,c\neq 0,\,(1-d)\,h_{1}+c\,h_{2}=0\bigg{\}},\] (31)

and

\[\xi_{\mathcal{R}}\,=\,\bigg{\{}(h_{1},h_{2},a_{r},b_{r},c,d)\,\big{|}\,(1-d)(1 -a_{r})-b_{r}\,c\neq 0,\,(1-d)\,h_{1}+c\,h_{2}=0\bigg{\}}.\] (32)

In addition, the DFB curves for the fixed points \(\mathcal{O}_{\mathcal{L}}\) and \(\mathcal{O}_{\mathcal{R}}\) are

\[\mathcal{F}_{\mathcal{L}}\,=\,\bigg{\{}(h_{1},h_{2},a_{l},b_{l},c,d)\big{|}\, 1+a_{l}+d+a_{l}\,d-b_{l}\,c=0\bigg{\}},\]

\[\mathcal{F}_{\mathcal{R}}\,=\,\bigg{\{}(h_{1},h_{2},a_{r},b_{r},c,d)\big{|}\, 1+a_{r}+d+a_{r}\,d-b_{r}\,c=0\bigg{\}}.\] (33)

**Remark 3**.: _The existence regions \(E_{\mathcal{O}_{\mathcal{L}}}\) and \(E_{\mathcal{O}_{\mathcal{R}}}\) are bounded by the BCB curves \(\xi_{\mathcal{L}}\) and \(\xi_{\mathcal{R}}\)._

**Remark 4**.: _The stability regions \(\mathcal{S}_{\mathcal{L}/\mathcal{R}}\) of fixed points (eq. (20)) are bounded by the DTB curves \(\tau_{\mathcal{L}/\mathcal{R}}\) (eqn. (29) and (30)), the DFB curves \(\mathcal{F}_{\mathcal{L}/\mathcal{R}}\) (eqn. (33)), and the CB curves \(a_{l/r}\,d-b_{l/r}\,c=1\). For instance, for \(d=1\), \(\mathcal{S}_{\mathcal{L}/\mathcal{R}}\) are illustrated in Fig. S\(1(a)\). In this case, the stability regions only exist for \(b_{l/r}\,c<0\) and \(a_{l/r}-d<-2\sqrt{-b_{l/r}\,c}\). Moreover, as shown in Fig. S\(1(b)\), for \(d=0.01\), these stability regions can exist for both cases \(b_{l/r}\,c<0\), \(a_{l/r}-d<-2\sqrt{-b_{l/r}\,c}\) (in blue), and \(b_{l/r}\,c<0\), \(a_{l/r}-d>2\sqrt{-b_{l/r}\,c}\) (in green), but not for \(b_{l/r}\,c\geq 0\). Furthermore, if \(c=1\), there are stability regions \(\mathcal{S}_{\mathcal{L}/\mathcal{R}}\) for the two cases \(b_{l/r}\,c<0\), \(a_{l/r}-d<-2\sqrt{-b_{l/r}\,c}\) (in blue), and \(b_{l/r}\,c>0\) (in purple); see Fig. S\(2(a)\). Finally, when \(c=0\), as explained in Remark 2, the stability regions have the form (24), i.e._

\[\mathcal{S}_{\mathcal{L}/\mathcal{R}}\,=\,\bigg{\{}(h_{1},h_{2},a_{l/r},b_{l/r},c, d)\in E_{\mathcal{O}_{\mathcal{L}/\mathcal{R}}}\,\big{|}\,c=0,\,b_{l/r}\in\mathbb{R},\,-1 \leq a_{l/r},\,d\leq 1\bigg{\}}.\] (34)

_which are displayed in Fig. S\(2(b)\)._Since the system (11) is a linear map in each sub-region \(\mathcal{L}\) and \(\mathcal{R}\), there cannot be any \(n\)-cycle, \(n\geq 2\), with all periodic points on only one linear side. So, all period-\(n\) orbits have both letters \(\mathcal{L}\) and \(\mathcal{R}\) in their symbolic sequence.

#### a.1.3 2-cycles of the map (11) and their bifurcations

The 2-cycle \(\mathcal{O}_{\mathcal{R}\mathcal{L}}\) of the map (11) is determined by solving the equation \(T_{\mathcal{L}}\circ T_{\mathcal{R}}(z_{1},z_{2})\,=\,(z_{1},z_{2})^{\mathsf{T}}\) where

\[T_{\mathcal{L}}\circ T_{\mathcal{R}}(z_{1},z_{2})\,=\,\begin{pmatrix}a_{l}\;a_ {r}+b_{r}\;c&a_{l}\;c+c\;d\\ a_{r}\;b_{l}+b_{r}\;d&d^{2}+b_{l}\;c\end{pmatrix}\begin{pmatrix}z_{1}\\ z_{2}\end{pmatrix}\,+\,\begin{pmatrix}c\;h_{2}+h_{1}\;\big{(}a_{l}+1\\ b_{l}\;h_{1}+h_{2}\big{(}d+1\end{pmatrix}\right).\] (35)

In this case if \(\big{(}I-J_{\mathcal{L}}\,J_{\mathcal{R}}\big{)}\) is invertible, then the solution \((z_{1},z_{2})^{\mathsf{T}}=\big{(}z_{1}^{(1)},z_{2}^{(1)}\big{)}^{\mathsf{T}}\) is given by

\[\big{(}z_{1}^{(1)},z_{2}^{(1)}\big{)}^{\mathsf{T}}\,=\,\bigg{(}\frac{\big{(}(1 -d)h_{1}+c\;h_{2}\big{)}\big{(}a_{l}+d+a_{l}\;d-b_{l}\;c+1\big{)}}{(a_{r}\;d-b_ {r}\,c)(a_{l}\,d-b_{l}\,c)-c(b_{l}+b_{r})-d^{2}-a_{l}\,a_{r}+1},\]

\[\frac{h_{2}\big{(}1+d-a_{l}\,a_{r}-b_{r}\,c-a_{l}\,a_{r}\;d+a_{r}\,b_{l}\;c \big{)}+h_{1}\big{(}b_{l}+a_{r}\;b_{l}+b_{r}\;d+a_{l}\;b_{r}\;d-b_{l}\;b_{r}\; c\big{)}}{(a_{r}\,d-b_{r}\,c)(a_{l}\,d-b_{l}\,c)-c(b_{l}+b_{r})-d^{2}-a_{l}\,a_{ r}+1}\bigg{)}.\] (36)

Also \(T_{\mathcal{R}}\big{(}z_{1}^{(1)},z_{2}^{(1)}\big{)}=\big{(}z_{1}^{(2)},z_{2 }^{(2)}\big{)}^{\mathsf{T}}\) yields

\[\big{(}z_{1}^{(2)},z_{2}^{(2)}\big{)}^{\mathsf{T}}\,=\,\bigg{(}\frac{\big{(}( 1-d)h_{1}+c\;h_{2}\big{)}\big{(}a_{r}+d+a_{r}\;d-b_{r}\,c+1\big{)}}{(a_{r}\,d- b_{r}\,c)(a_{l}\,d-b_{l}\,c)-c(b_{l}+b_{r})-d^{2}-a_{l}\,a_{r}+1},\]

\[\frac{h_{2}\big{(}1+d-a_{l}\,a_{r}-b_{l}\,c-a_{l}\,a_{r}\,d+a_{l}\;b_{r}\,c \big{)}+h_{1}\big{(}b_{r}+a_{l}\;b_{r}+b_{l}\;d+a_{r}\;b_{l}\;d-b_{l}\;b_{r}\,c \big{)}}{(a_{r}\,d-b_{r}\,c)(a_{l}\,d-b_{l}\,c)-c(b_{l}+b_{r})-d^{2}-a_{l}\,a_ {r}+1}\bigg{)}.\] (37)

Hence, the existence region of the 2-cycle \(\mathcal{O}_{\mathcal{R}\mathcal{L}}\) is

\[E_{\mathcal{O}_{\mathcal{R}\mathcal{L}}}\,=\,\bigg{\{}(h_{1},h_{2},a_{l},b_{l},c,d)\,\big{|}\frac{\big{(}(1-d)h_{1}+c\;h_{2}\big{)}\big{(}a_{l}+d+a_{l}\;d- b_{l}\,c+1\big{)}}{(a_{r}\,d-b_{r}\,c)(a_{l}\,d-b_{l}\,c)-c(b_{l}+b_{r})-d^{2}-a_{l} \,a_{r}+1}\,>\,0,\]\[\frac{\big{(}(1-d)h_{1}+c\,h_{2}\big{)}\big{(}a_{r}+d+a_{r}\,d-b_{r}\,c+1\big{)}}{ (a_{r}\,d-b_{r}\,c)(a_{l}\,d-b_{l}\,c)-c(b_{l}+b_{r})-d^{2}-a_{l}\,a_{r}+1}\,< \,0\Big{\}}.\] (38)

The characteristic polynomial of \(\,J_{\mathcal{R}\mathcal{L}}\,=\,J_{\mathcal{L}}\,J_{\mathcal{R}}\,=\,\begin{pmatrix} a_{l}\,a_{r}+b_{r}\,c&a_{l}\,c+c\,d\\ a_{r}\,b_{l}+b_{r}\,d&d^{2}+b_{l}\,c\end{pmatrix}\) is given by

\[\mathcal{P}_{\mathcal{O}_{\mathcal{R}\mathcal{L}}}(\lambda)\,=\,\lambda^{2}-( d^{2}+a_{l}\,a_{r}+b_{l}\,c+b_{r}\,c)\lambda+(a_{r}\,d-b_{r}\,c)(a_{l}\,d-b_{l} \,c),\] (39)

and

\[\mathcal{D}_{\mathcal{R}\mathcal{L}}\,=\,(a_{r}\,d-b_{r}\,c)(a_{l}\,d-b_{l} \,c),\]

\[\mathcal{P}_{\mathcal{O}_{\mathcal{R}\mathcal{L}}}(1)\,=\,(a_{r}\,d-b_{r}\,c )(a_{l}\,d-b_{l}\,c)-c(b_{l}+b_{r})-d^{2}-a_{l}\,a_{r}+1,\]

\[\mathcal{P}_{\mathcal{O}_{\mathcal{R}\mathcal{L}}}(-1)\,=\,(a_{r}\,d-b_{r}\,c )(a_{l}\,d-b_{l}\,c)+c(b_{l}+b_{r})+d^{2}+a_{l}\,a_{r}+1,\]

\[\lambda_{1,2}(\mathcal{O}_{\mathcal{R}\mathcal{L}})\,=\,\frac{a_{l}\,a_{r}+c( b_{l}+b_{r})+d^{2}}{2}\]

Thus, the stability region of \(\mathcal{O}_{\mathcal{R}\mathcal{L}}\) is

\[\mathcal{S}_{\mathcal{R}\mathcal{L}}\,=\,\bigg{\{}(h_{1},h_{2},a _{l/r},b_{l/r},c,d)\in E_{\mathcal{O}_{\mathcal{R}\mathcal{L}}}\,\big{|}\,-1< (a_{r}\,d-b_{r}\,c)(a_{l}\,d-b_{l}\,c)<1,\] \[-\,(a_{r}\,d-b_{r}\,c)(a_{l}\,d-b_{l}\,c)-1<c(b_{l}+b_{r})+d^{2} +a_{l}\,a_{r}<(a_{r}\,d-b_{r}\,c)(a_{l}\,d-b_{l}\,c)+1\bigg{\}}.\] (41)

In addition, for \(\big{(}(1-d)h_{1}+c\,h_{2}\big{)}\big{(}a_{l}+d+a_{l}\,d-b_{l}\,c+1\big{)}\neq 0\), the set

\[\tau_{\mathcal{R}\mathcal{L}}\,= \bigg{\{}(h_{1},h_{2},a_{l},a_{r},b_{l},b_{r},c,d)\,\big{|}\,a_{ l}\,a_{r}+b_{l}\,c+b_{r}\,c+d^{2}-a_{l}\,a_{r}\,d^{2}-b_{l}\,b_{r}\,c^{2}+a_{l} \,b_{r}\,c\,d\] \[\qquad+a_{r}\,b_{l}\,c\,d-1=0\bigg{\}},\] (42)

is the DTB curve for the \(2\)-cycle \(\mathcal{O}_{\mathcal{R}\mathcal{L}}\). As in this case, for the parameter values belonging to \(\tau_{\mathcal{R}\mathcal{L}}\), the points of the \(2\)-cycle \(\mathcal{O}_{\mathcal{R}\mathcal{L}}\) tend to \(\pm\infty\), and the corresponding eigenvalue tends to one. Moreover, for \((1-d)h_{1}+c\,h_{2}\neq 0\), the BCB curves of \(\mathcal{O}_{\mathcal{R}\mathcal{L}}\) can be computed as

\[\xi^{1}_{\mathcal{R}\mathcal{L}}\,=\,\bigg{\{}(h_{1},h_{2},a_{l},b_{l},c,d)\, \big{|}\,a_{l}+d+a_{l}\,d-b_{l}\,c+1=0\bigg{\}},\]

\[\xi^{2}_{\mathcal{R}\mathcal{L}}\,=\,\bigg{\{}(h_{1},h_{2},a_{r},b_{r},c,d)\, \big{|}\,a_{r}+d+a_{r}\,d-b_{r}\,c+1=0\bigg{\}}.\] (43)

Note that here the condition \((1-d)h_{1}+c\,h_{2}\neq 0\) guarantees a regular BCB in the sense that only one periodic point of \(\mathcal{O}_{\mathcal{R}\mathcal{L}}\) collides with the switching boundary; for more details see [6]. Besides,

\[\mathcal{F}_{\mathcal{R}\mathcal{L}}\,= \bigg{\{}(h_{1},h_{2},a_{l},a_{r},b_{l},b_{r},c,d)\,\big{|}\,a_{l} a_{r}+b_{l}c+b_{r}c+d^{2}+a_{l}\,a_{r}d^{2}+b_{l}\,b_{r}c^{2}\] \[\qquad-a_{l}\,b_{r}\,c\,d-a_{r}\,b_{l}cd+1=0\bigg{\}},\] (44)

is the DFB curve of the 2-cycle \(\mathcal{O}_{\mathcal{R}\mathcal{L}}\).

**Remark 5**.: _One can see that for \((1-d)h_{1}+c\,h_{2}\neq 0\) the DFB curves of the fixed points \(\mathcal{O}_{\mathcal{L}}\) and \(\mathcal{O}_{\mathcal{R}}\) (\(\mathcal{F}_{\mathcal{L}}\) and \(\mathcal{F}_{\mathcal{R}}\)) and the BCB boundaries of the 2-cycle \(\mathcal{O}_{\mathcal{R}\mathcal{L}}\) (\(\xi^{1}_{\mathcal{R}\mathcal{L}}\) and \(\xi^{2}_{\mathcal{R}\mathcal{L}}\)) are the same. In this case, the DFB of the fixed points can lead to the (attracting) 2-cycle \(\mathcal{O}_{\mathcal{R}\mathcal{L}}\)._

#### a.1.4 3-cycles of the map (11) and their bifurcations

Here, we investigate the existence, stability and bifurcation structure of maximal or basic 3-cycles. Note that for the continuous map (11), basic \(n\)-cycles \(\mathcal{O}_{\mathcal{R}\mathcal{L}^{n-1}}\) (\(n\geq 3\)) exist in pairs with their complementary cycles (\(\mathcal{O}_{\mathcal{R}\mathcal{L}^{n-2}\mathcal{R}}\)), and they appear via BCBs such that one of them may be attracting and the other repelling [6, 20, 38]. In this case, a BCB of basic cycles demonstrates a non-smooth fold bifurcation which includes a stable basic orbit and an unstable nonbasic orbit [6, 5, 19]. Furthermore, the complementary orbits can have nonempty stability regions such that, similar to the basic orbits, they are bounded by curves of BCBs, DTBs and DFBs [6, 5].

Basic 3-cycles \(\mathcal{O}_{\mathcal{R}\mathcal{L}^{2}}\) and their complementary cycles \(\mathcal{O}_{\mathcal{R}^{2}\mathcal{L}}\).The basic 3-cycle \(\mathcal{O}_{\mathcal{R}\mathcal{L}^{2}}\) can be obtained from the equation \(T_{\mathcal{L}}\circ T_{\mathcal{L}}\circ T_{\mathcal{R}}(z_{1},z_{2})\,=\,(z_ {1},z_{2})^{\mathsf{T}}\) where

\[T_{\mathcal{L}}\circ T_{\mathcal{L}}\circ T_{\mathcal{R}}(z_{1},z_{2})\,= \,\begin{pmatrix}a_{r}(a_{l}^{2}+b_{l}\,c)+b_{r}(a_{l}\,c+c\,d)&c(a_{l}^{2}+b _{l}\,c)+d(a_{l}\,c+c\,d)\\ b_{r}(d^{2}+b_{l}\,c)+a_{r}(a_{l}\,b_{l}+b_{l}\,d)&d(d^{2}+b_{l}\,c)+c(a_{l}\,b_ {l}+b_{l}\,d)\end{pmatrix}\begin{pmatrix}z_{1}\\ z_{2}\end{pmatrix}\]

\[+\,\begin{pmatrix}h_{1}\big{(}b_{l}\,c+a_{l}(a_{l}+1)+1\big{)}+h_{2}\big{(}a_ {l}\,c+c(d+1)\big{)}\\ h_{1}\big{(}b_{l}\,d+b_{l}(a_{l}+1)\big{)}+h_{2}\big{(}b_{l}\,c+d(d+1)+1\big{)} \end{pmatrix}.\] (45)

If \(\big{(}I-J_{\mathcal{L}}^{2}\,J_{\mathcal{R}}\big{)}\) is invertible, then the solution \((z_{1},z_{2})^{\mathsf{T}}=\big{(}z_{1}^{(1)},z_{2}^{(1)}\big{)}^{\mathsf{T}}\) is

\[(z_{1}^{(1)},z_{2}^{(1)})^{T}\,=\,\Big{(}\frac{\big{(}(1-d)h_{1}+c\,h_{2} \big{)}G_{1}}{G},\,\frac{G_{2}}{G}\Big{)}^{\mathsf{T}},\] (46)

where

\[G_{1}\,=\,a_{l}^{2}d^{2}+a_{l}^{2}d+a_{l}^{2}-2a_{l}\,b_{l}cd-a_ {l}b_{l}c+a_{l}d^{2}+a_{l}d+a_{l}+b_{l}^{2}c^{2}-b_{l}cd+b_{l}c+d^{2}+d+1,\] \[G\,=\,-a_{l}^{2}\,a_{r}-d^{3}-c\big{(}a_{l}\,b_{l}+a_{l}\,b_{r}+a _{r}\,b_{l}+d(2\,b_{l}+b_{r})\big{)}+(a_{r}\,d-b_{r}\,c)(a_{l}\,d-b_{l}\,c)^{ 2}\,+\,1,\] \[G_{2}\,=\,h_{2}+b_{l}h_{1}+dh_{2}+d^{2}h_{2}+a_{l}b_{l}h_{1}+b_ {l}ch_{2}+b_{l}dh_{1}-a_{l}^{2}a_{r}h_{2}+b_{r}d^{2}h_{1}-a_{r}b_{l}^{2}ch_{1}\] \[-\,a_{l}^{2}a_{r}dh_{2}+a_{l}b_{r}d^{2}h_{1}+b_{l}b_{r}c^{2}h_{2 }-a_{r}b_{l}^{2}c^{2}\,h_{2}-a_{l}^{2}a_{r}d^{2}h_{2}+a_{l}^{2}b_{r}\,d^{2}h_{ 1}+b_{l}^{2}\,b_{r}\,c^{2}h_{1}\] \[+\,a_{l}\,a_{r}\,b_{l}\,h_{1}-a_{l}\,b_{r}\,c\,h_{2}-a_{r}\,b_{l} \,c\,h_{2}+a_{r}\,b_{l}\,d\,h_{1}+b_{l}\,b_{r}\,ch_{1}-b_{r}\,c\,dh_{2}+a_{l} \,a_{r}\,b_{l}ch_{2}\] \[+\,a_{l}\,a_{r}\,b_{l}\,dh_{1}-a_{l}\,b_{r}\,c\,d\,h_{2}-b_{l}\,b _{r}\,c\,dh_{1}+2a_{l}\,a_{r}\,b_{l}\,c\,dh_{2}-2a_{l}\,b_{l}\,b_{r}\,c\,dh_{1}.\] (47)

Further

\[T_{\mathcal{R}}(z_{1}^{(1)},z_{2}^{(1)})=\big{(}z_{1}^{(2)},z_{2}^{(2)}\big{)} ^{\mathsf{T}}\,=\,\Big{(}\frac{\big{(}(1-d)h_{1}+c\,h_{2}\big{)}K_{1}}{G},\, \frac{K_{2}}{G}\Big{)}^{T},\]

\[T_{\mathcal{L}}(z_{1}^{(2)},z_{2}^{(2)})=\big{(}z_{1}^{(3)},z_{2}^{(3)}\big{)} ^{\mathsf{T}}\,=\,\Big{(}\frac{\big{(}(1-d)h_{1}+c\,h_{2}\big{)}H_{1}}{G},\, \frac{H_{2}}{G}\Big{)}^{T},\] (48)

where

\[K_{1}\,=a_{r}+d+a_{l}a_{r}+b_{l}c+a_{r}d+a_{r}d^{2}+d^{2}+a_{l}a _{r}d-a_{l}b_{r}c-b_{r}cd+a_{l}a_{r}d^{2}+b_{l}b_{r}c^{2}\] \[-\,a_{l}b_{r}cd-a_{r}b_{l}cd+1,\] \[K_{2}\,=\,h_{2}+b_{r}h_{1}+dh_{2}+d^{2}h_{2}+a_{l}b_{r}h_{1}+b_ {r}ch_{2}+b_{l}dh_{1}-a_{l}^{2}a_{r}h_{2}+a_{l}^{2}b_{r}h_{1}+b_{l}d^{2}h_{1}\] \[+\,a_{l}^{2}b_{r}ch_{2}+a_{r}b_{l}d^{2}h_{1}+b_{l}b_{r}c^{2}h_{2 }-a_{l}^{2}a_{r}d^{2}h_{2}+b_{l}^{2}b_{r}c^{2}h_{1}-a_{l}b_{l}ch_{2}-a_{r}b_{ l}ch_{2}+a_{l}b_{l}dh_{1}\] \[+\,b_{l}\,b_{r}\,ch_{1}-b_{l}\,c\,dh_{2}+a_{l}a_{r}b_{l}dh_{1}-a_ {l}\,b_{l}b_{r}ch_{1}-a_{r}\,b_{l}\,c\,dh_{2}-b_{l}b_{r}c\,dh_{1}+a_{l}\,a_{r}b_ {l}d^{2}h_{1}\] \[-\,a_{l}\,b_{l}b_{r}c^{2}h_{2}-a_{r}b_{l}^{2}cdh_{1}+a_{l}^{2}b_ {r}cdh_{2}+a_{l}\,a_{r}\,b_{l}\,c\,d\,h_{2}-a_{l}\,b_{l}\,b_{r}\,c\,d\,h_{1}-a_{l} ^{2}a_{r}dh_{2},\] \[H_{1}\,=a_{l}+d+a_{l}a_{r}+a_{l}d+b_{r}c+a_{l}d^{2}+d^{2}+a_{l}a_{r }d-a_{r}\,b_{l}c-b_{l}cd+a_{l}a_{r}d^{2}+b_{l}b_{r}c^{2}\] \[-\,a_{l}b_{r}cd-a_{r}b_{l}cd+1,\]\[+\,\,b_{l}d^{2}h_{1}-a_{l}^{2}a_{r}dh_{2}-b_{l}cdh_{2}+a_{l}b_{l}d^{2} h_{1}+a_{l}^{2}b_{r}dh_{1}-b_{l}^{2}cdh_{1}-a_{l}^{2}a_{r}d^{2}h_{2}+b_{l}^{2}b_{r}c^{2}h_{1}\] \[+\,\,a_{l}a_{r}b_{l}h_{1}-a_{l}b_{l}ch_{2}-a_{l}b_{r}ch_{2}+a_{l}b_ {r}dh_{1}+a_{l}a_{r}b_{l}ch_{2}-a_{l}b_{l}b_{r}ch_{1}-a_{l}b_{l}cd\,h_{2}\] \[+\,\,a_{l}a_{r}b_{l}d^{2}h_{1}-a_{l}b_{l}b_{r}c^{2}h_{2}-a_{r}b_{l }^{2}cdh_{1}+a_{l}^{2}b_{r}cdh_{2}+a_{l}a_{r}b_{l}cdh_{2}-a_{l}b_{l}b_{r}cdh_{1}.\] (49)

Therefore, the existence region of the 3-cycle \(\mathcal{O}_{\mathcal{R}\mathcal{L}^{2}}\) is given by

\[E_{\mathcal{O}_{\mathcal{R}\mathcal{L}^{2}}}\,=\,\bigg{\{} \big{(}h_{1},h_{2},a_{l},b_{l},c,d\big{)}\,\big{|}\,\frac{\big{(}(1-d)h_{1}+c \,h_{2}\big{)}G_{1}}{G}\,>\,0,\,\,\,\,\,\frac{\big{(}(1-d)h_{1}+c\,h_{2}\big{)} K_{1}}{G}\,<\,0,\] \[\qquad\qquad\frac{\big{(}(1-d)h_{1}+c\,h_{2}\big{)}H_{1}}{G}\,<\, 0\bigg{\}},\] (50)

where \(G,G_{1},K_{1}\) and \(H_{1}\) are defined in (47) and (49). On the other hand, the characteristic polynomial of

\[J_{\mathcal{R}\mathcal{L}^{2}}\,=\,J_{\mathcal{L}}^{2}\,J_{ \mathcal{R}}\,=\,\begin{pmatrix}a_{r}(a_{l}^{2}+b_{l}\,c)+b_{r}(a_{l}\,c+c\,d) &\qquad c(a_{l}^{2}+b_{l}\,c)+d(a_{l}\,c+c\,d)\\ b_{r}(d^{2}+b_{l}\,c)+a_{r}(a_{l}\,b_{l}+b_{l}\,d)&\qquad d(d^{2}+b_{l}\,c)+c( a_{l}\,b_{l}+b_{l}\,d)\end{pmatrix},\]

is

\[\mathcal{P}_{\mathcal{O}_{\mathcal{R}\mathcal{L}^{2}}}(\lambda)\,=\,\lambda^{2 }-\big{(}a_{l}^{2}a_{r}+d^{3}+c\big{(}a_{l}b_{l}+a_{l}b_{r}+a_{r}b_{l}+d(2b_{l} +b_{r})\big{)}\big{)}\lambda+(a_{r}d-b_{r}c)(a_{l}d-b_{l}c)^{2}.\] (51)

According to

\[\mathcal{D}_{\mathcal{R}\mathcal{L}^{2}}\,=\,(a_{l}\,d-b_{l}\,c)^{ 2}(a_{r}\,d-b_{r}\,c),\] \[\mathcal{P}_{\mathcal{O}_{\mathcal{R}\mathcal{L}^{2}}}(1)=-a_{l}^ {2}a_{r}-d^{3}-c\big{(}a_{l}b_{l}+a_{l}b_{r}+a_{r}b_{l}+d(2b_{l}+b_{r})\big{)}\] \[\mathcal{P}_{\mathcal{O}_{\mathcal{R}\mathcal{L}^{2}}}(-1)=a_{l}^ {2}a_{r}+d^{3}+c\big{(}a_{l}b_{l}+a_{l}b_{r}+a_{r}b_{l}+d(2b_{l}+b_{r})\big{)}+ (a_{r}d-b_{r}c)(a_{l}d-b_{l}c)^{2}+1,\] (52)

the stability region of the 3-cycle \(\mathcal{O}_{\mathcal{R}\mathcal{L}^{2}}\) is given by

\[\mathcal{S}_{\mathcal{R}\mathcal{L}^{2}}\,=\,\bigg{\{} \big{(}h_{1},h_{2},a_{l/r},b_{l/r},c,d\big{)}\,\big{|}\,b_{r}\,a_{l}^{2} cd^{2}-a_{r}\,a_{l}^{2}\,d^{3}+a_{r}a_{l}^{2}-2\,b_{r}\,a_{l}b_{l}c^{2}d+2a_{r}\,a_{l} \,b_{l}\,c\,d^{2}\] \[\qquad+\,a_{l}\,b_{l}\,c+b_{r}\,a_{l}\,c+b_{r}\,b_{l}^{2}c^{3}-a_ {r}\,b_{l}^{2}c^{2}d+2\,b_{l}cd+a_{r}b_{l}\,c+b_{r}\,c\,d+d^{3}-1=0\bigg{\}}.\] (53)

For \((1-d)h_{1}+c\,h_{2}\neq 0\)

\[\xi_{\mathcal{R}\mathcal{L}^{2}}^{1}\,=\,\bigg{\{} \big{(}h_{1},h_{2},a_{l},a_{r},b_{r},c,d\big{)}\,\big{|}\,K_{1}\,=\,a_{r}+d+a _{l}a_{r}+b_{l}\,c+a_{r}d+a_{r}d^{2}+d^{2}+a_{l}a_{r}d\] \[\qquad-\,a_{l}\,b_{r}\,c-b_{r}\,c\,d+a_{l}\,a_{r}\,d^{2}+b_{l}\,b _{r}\,c^{2}-a_{l}\,b_{r}\,c\,d-a_{r}\,b_{l}\,c\,d+1=0\bigg{\}},\] \[\xi_{\mathcal{R}\mathcal{L}^{2}}^{2}\,=\,\bigg{\{} \big{(}h_{1},h_{2},a_{r},b_{r},c,d\big{)}\,\big{|}\,H_{1}\,=\,a_{l}+d+a_{l}\,a _{r}+a_{l}\,d+b_{r}\,c+a_{l}\,d^{2}+d^{2}+a_{l}\,a_{r}\,d\]\[-\,a_{r}\,b_{l}\,c-b_{l}\,c\,d+a_{l}\,a_{r}\,d^{2}+b_{l}\,b_{r}\,c^{2} -a_{l}\,b_{r}\,c\,d-a_{r}\,b_{l}\,c\,d+1=0\biggr{\}},\] (55)

are (regular) BCB curves of \(\mathcal{O}_{\mathcal{R}\mathcal{L}^{2}}\). Furthermore, the set

\[\mathcal{F}_{\mathcal{R}\mathcal{L}^{2}}\,= \bigg{\{}(h_{1},h_{2},a_{l},a_{r},b_{l},b_{r},c,d)\big{|}\,-b_{r} \,a_{l}^{2}cd^{2}+a_{r}a_{l}^{2}d^{3}+a_{r}a_{l}^{2}+2b_{r}a_{l}b_{l}c^{2}d-2a _{r}a_{l}b_{l}cd^{2}\] \[\qquad+\,a_{l}b_{l}c+b_{r}a_{l}c-b_{r}b_{l}^{2}c^{3}+a_{r}\,b_{l} ^{2}\,c^{2}d+2b_{l}cd+a_{r}b_{l}c+b_{r}\,c\,d+d^{3}+1=0\biggr{\}},\] (56)

is the DFB curve of \(\mathcal{O}_{\mathcal{R}\mathcal{L}^{2}}\). As noted, the basic 3-cycle \(\mathcal{O}_{\mathcal{R}\mathcal{L}^{2}}\) exists in a pair with its complementary cycle \(\mathcal{O}_{\mathcal{R}^{2}\mathcal{L}}\). Moreover, the existence region of \(\mathcal{O}_{\mathcal{R}^{2}\mathcal{L}}\) can easily be found by interchanging the letters \(\mathcal{L}\) and \(\mathcal{R}\) in all notations of the equations (45)- (49) and considering

\[z_{1}^{(1)}<0,\quad z_{1}^{(2)}>0,\quad z_{1}^{(3)}>0.\] (57)

Further, the stability region of the 3-cycle \(\mathcal{O}_{\mathcal{R}^{2}\mathcal{L}}\) for the parameter values satisfying (57) is given by

\[\mathcal{S}_{\mathcal{R}^{2}\mathcal{L}}= \bigg{\{}(h_{1},h_{2},a_{l/r},b_{l/r},c,d)\,\big{|}\,-1<(a_{r}\,d- b_{r}\,c)^{2}(a_{l}\,d-b_{l}\,c)<1,\] \[\qquad-(a_{r}d-b_{r}c)^{2}(a_{l}d-b_{l}c)-1<a_{r}^{2}a_{l}+d^{3}+ c\big{(}a_{r}b_{r}+a_{r}b_{l}+a_{l}b_{r}+d(2b_{r}+b_{l})\big{)}\] \[\qquad<\,(a_{r}\,d-b_{r}\,c)^{2}(a_{l}\,d-b_{l}\,c)\,+\,1\bigg{\}}.\] (58)

Notice that whenever the stable 3-cycle \(\mathcal{O}_{\mathcal{R}\mathcal{L}^{2}}\) exists, its complementary orbit \(\mathcal{O}_{\mathcal{R}^{2}\mathcal{L}}\) also exists, but it is unstable. Furthermore, for \((1-d)h_{1}+c\,\,h_{2}\neq 0\) both the 3-cycles \(\mathcal{O}_{\mathcal{R}\mathcal{L}^{2}}\) and \(\mathcal{O}_{\mathcal{R}^{2}\mathcal{L}}\) appear at the same BCB curves (55). On the other hand, the DTB and DFB curves of the 3-cycle \(\mathcal{O}_{\mathcal{R}^{2}\mathcal{L}}\) are given by

\[\tau_{\mathcal{R}^{2}\mathcal{L}}\,= \bigg{\{}(h_{1},h_{2},a_{l},a_{r},b_{l},b_{r},c,d)\,\big{|}\,b_{l }a_{r}^{2}cd^{2}-a_{l}a_{r}^{2}d^{3}+a_{l}a_{r}^{2}-2b_{l}a_{r}b_{r}c^{2}d+2a _{l}a_{r}b_{r}cd^{2}\] \[\qquad+\,a_{r}\,b_{r}\,c+b_{l}a_{r}c+b_{l}\,b_{r}^{2}c^{3}-a_{l} \,b_{r}^{2}\,c^{2}d+2b_{r}cd+a_{l}b_{r}c+b_{l}\,c\,d+d^{3}-1=0\biggr{\}},\] (59)

and

\[\mathcal{F}_{\mathcal{R}^{2}\mathcal{L}}\,= \bigg{\{}(h_{1},h_{2},a_{l},a_{r},b_{l},b_{r},c,d)\,\big{|}\,-b_{l }a_{r}^{2}cd^{2}+a_{l}a_{r}^{2}d^{3}+a_{l}a_{r}^{2}+2b_{l}a_{r}b_{r}c^{2}d-2a _{l}a_{r}b_{r}cd^{2}\] \[\qquad+\,a_{r}b_{r}c+b_{l}\,a_{r}c-b_{l}\,b_{r}^{2}c^{3}+a_{l} \,b_{r}^{2}c^{2}\,d+2b_{r}cd+a_{l}\,b_{r}\,c+b_{l}\,c\,d+d^{3}+1=0\biggr{\}},\] (60)

respectively.

#### a.1.5 Multiple attractor bifurcations (MABs) of the map (11)

To detect multiple attractor bifurcations for the map (11), a straightforward way is to determine the overlapping stability regions of different periodic orbits. For instance, as shown in Fig. 1A, for \(c=0.8\), \(d=0.2\), \(b_{l}=-0.4\), \(b_{r}=0.5\), two stability regions \(\mathcal{S}_{\mathcal{R}\mathcal{L}}\) and \(\mathcal{S}_{\mathcal{R}\mathcal{L}^{2}}\) overlap in the \((a_{l},a_{r})\)-parameter plane (or in the \((a_{11},\,a_{11}+w_{11})\)-parameter space for the \(2d\) PLRNN [7]). Their overlapping region, displayed in yellow, reveals the structure of the \((a_{l},a_{r})\)-parameter plane. This helps us to find various MABs. Assuming \(h_{2}=0\) and varying \(h_{1}\) from a negative value to a positive one, an MAB of the form

\[\mathcal{O}_{\mathcal{L}}^{s}\,\,\stackrel{{ h_{1}}}{{ \longleftrightarrow}}\,\mathcal{O}_{\mathcal{R}\mathcal{L}}^{s}\,+\,\mathcal{O}_{ \mathcal{R}\mathcal{L}^{2}}^{s},\] (61)

occurs in the overlapping region. An example of this kind of bifurcation is illustrated in Fig. S3A.

Moreover, Fig. S4 indicates, for \(c=0.9,\,d=0.3,\,b_{l}=-0.6,\,b_{r}=-1.54\,\), there are nonempty overlapping regions \(\mathcal{S}_{\mathcal{R}\mathcal{L}}\cap\mathcal{S}_{\mathcal{R}\mathcal{L}^{2}}\) and \(\mathcal{S}_{\mathcal{R}\mathcal{L}^{2}}\cap\mathcal{S}_{\mathcal{R}}\). This leads to the occurrence of two different MABs given by (61) and

\[\mathcal{O}_{\mathcal{L}}^{s}\,\stackrel{{ h_{1}}}{{ \longleftrightarrow}}\,\mathcal{O}_{\mathcal{R}}^{s}\,+\,\mathcal{O}_{ \mathcal{R}\mathcal{L}^{2}},\] (62)

for \(h_{2}=0\) and \(h_{1}\) changing from negative to positive values. Both of these bifurcations are shown in Fig. S3A and Fig. S3B, associated with the points \(P_{1}\)and \(P_{2}\) in Fig. S4. Note that in Fig. S4, all the points \(P_{2},\,P_{3}\) and \(P_{4}\) belong to the overlapping region \(\mathcal{S}_{\mathcal{R}\mathcal{L}^{2}}\cap\mathcal{S}_{\mathcal{R}}\) (in sky blue). These points are related to the parameter values \(\,c=0.9,\,d=0.3,\,b_{l}=-0.6,\,b_{r}=-1.54,\,a_{r}=-1.8,\,h_{2}=0\), and they only differ in the parameter \(a_{l}\). In this case, one can see that fixing all parameters and changing only the parameter \(a_{l}\), from \(P_{2}\) to \(P_{4}\), the basins of attraction change. The corresponding basins of attraction for these three points are demonstrated in Fig. S3B (right) and Fig. S5 for \(h_{1}=0.5\) (after the bifurcation).

### Proofs of theorems

#### a.2.1 Proof of theorem 1

Proof.: Let \(\mathcal{L}(\bm{\theta})\) be some loss function employed for PLRNN training that decomposes in time as \(\mathcal{L}=\sum_{t=1}^{T}\mathcal{L}_{t}\). Then

\[\frac{\partial\mathcal{L}}{\partial\theta} = \sum_{t=1}^{T}\frac{\partial\mathcal{L}_{t}}{\partial\theta},\] \[\frac{\partial\mathcal{L}_{t}}{\partial\theta} = \frac{\partial\mathcal{L}_{t}}{\partial\bm{z}_{t}}\,\frac{ \partial\bm{z}_{t}}{\partial\theta}.\] (63)

Denoting the Jacobian of system (2) at time \(t\) by

\[\bm{J}_{t}\,:=\,\frac{\partial F_{\bm{\theta}}(\bm{z}_{t-1})}{\partial\bm{z}_ {t-1}}\,=\,\frac{\partial\bm{z}_{t}}{\partial\bm{z}_{t-1}},\] (64)

we have

\[\frac{\partial\bm{z}_{t}}{\partial\theta}\,=\,\frac{\partial\bm{z}_{t}}{ \partial\bm{z}_{t-1}}\,\frac{\partial\bm{z}_{t-1}}{\partial\theta}\,+\,\frac{ \partial^{+}\bm{z}_{t}}{\partial\theta}\,=\,\bm{J}_{t}\,\frac{\partial\bm{z} _{t-1}}{\partial\theta}\,+\,\frac{\partial^{+}\bm{z}_{t}}{\partial\theta},\] (65)

where \(\partial^{+}\) denotes the immediate partial derivative (see [48] for more details). Assume that \(\Gamma_{k}\) is a \(k\)-cycle \((k\geq 1)\) of (2). Thus, \(\Gamma_{k}\) is a set of temporally successive periodic points

\[P_{k}:=\{\bm{z}_{t^{*k}},\bm{z}_{t^{*k}-1},\cdots,\bm{z}_{t^{*k}-(k-1)}\}=\{ \bm{z}_{t^{*k}},F(\bm{z}_{t^{*k}}),\dots,F_{\bm{\theta}}^{k-1}(\bm{z}_{t^{*k}} )\},\] (66)

such that all of them are fixed points of

\[\bm{z}_{t+k}\,=\,F_{\bm{\theta}}^{k}(\bm{z}_{t})\,=\,F_{\bm{\theta}}(F_{\bm{ \theta}}(...F_{\bm{\theta}}(\bm{z}_{t})...))),\] (67)

and \(k\) is the smallest such positive integer (for \(k=1\), \(\Gamma_{1}\) is a fixed point of \(F_{\bm{\theta}}\)). Similar to (65), the tangent vector \(\frac{\partial\bm{z}_{t+k}}{\partial\theta}\) can be computed as

\[\frac{\partial\bm{z}_{t+k}}{\partial\theta}\,=\,\frac{\partial\bm{z}_{t+k}}{ \partial\bm{z}_{t}}\,\frac{\partial\bm{z}_{t}}{\partial\theta}\,+\,\frac{ \partial^{+}\bm{z}_{t+k}}{\partial\theta}\,=\,\prod_{r=0}^{k-1}\bm{J}_{t+k-r} \,\frac{\partial\bm{z}_{t}}{\partial\theta}\,+\,\frac{\partial^{+}\bm{z}_{t+k} }{\partial\theta}.\] (68)

Thus, for \(\bm{z}_{t^{*k}}\,=\,F_{\bm{\theta}}^{k}(\bm{z}_{t^{*k}})\) we have

\[\frac{\partial\bm{z}_{t^{*k}}}{\partial\theta}\,=\,\prod_{r=0}^{k-1}\bm{J}_{t^{ *k}-r}\,\frac{\partial\bm{z}_{t^{*k}}}{\partial\theta}\,+\,\frac{\partial^{+} \bm{z}_{t^{*k}}}{\partial\theta}.\] (69)

Accordingly

\[\frac{\partial\bm{z}_{t^{*k}}}{\partial\theta}\,=\,\left(\bm{I}-\prod_{r=0}^{k -1}\bm{J}_{t^{*k}-r}\right)^{-1}\,\frac{\partial^{+}\bm{z}_{t^{*k}}}{\partial \theta}\,=\,\frac{adj\!\left(\bm{I}-\prod_{r=0}^{k-1}\bm{J}_{t^{*k}-r}\right) }{P_{\prod_{r=0}^{k-1}\bm{J}_{t^{*k}-r}}(1)}\,\frac{\partial^{+}\bm{z}_{t^{*k} }}{\partial\theta},\] (70)

[MISSING_PAGE_EMPTY:27]

\(\bm{z}_{t^{*k}}\), collides with one border. Therefore, \(z_{m\,t^{*k}}=0\) for some \(1\leq m\leq M\) by the definition of discontinuity boundaries in [38; 39]. Similar to the proof of Theorem 1, for \(\bm{\theta}=\bm{A},\bm{W}\) we have

\[\frac{\partial\bm{z}_{t^{*k}-1}}{\partial\theta}\,=\,\frac{adj\! \left(\bm{I}-\prod_{r=0}^{k-1}\bm{J}_{t^{*k}-1-r}\right)}{P_{\prod_{r=0}^{k-1} \bm{J}_{t^{*k}-1-r}}(1)}\,\frac{\partial^{+}\bm{z}_{t^{*k}-1}}{\partial\theta},\] (75)

in which

\[\frac{\partial^{+}\bm{z}_{t^{*k}-1}}{\partial w_{nm}}\,=\,\bm{1}_ {(n,m)}\,\bm{D}_{\Omega(t^{*k})}\,\bm{z}_{t^{*k}},\] \[\frac{\partial^{+}\bm{z}_{t^{*k}-1}}{\partial a_{mm}}\,=\,\bm{1}_ {(m,m)}\,\bm{z}_{t^{*k}},\] (76)

where \(\bm{1}_{(n,m)}\) is an \(M\times M\) indicator matrix with a \(1\) for the \((n,m)\)'th entry and \(0\) everywhere else. Since \(z_{m\,t^{*k}}=0\) at \(\theta=\theta_{0}\), due to (76) \(\frac{\partial^{+}\bm{z}_{t^{*k}-1}}{\partial\theta}\) becomes the zero vector at \(\theta=\theta_{0}\). Consequently, \(\left\|\frac{\partial\bm{z}_{t^{*k}-1}}{\partial\theta}\right\|\) and so \(\left\|\frac{\partial\mathcal{L}_{t}}{\partial\theta}\right\|\) vanishes at \(\theta=\theta_{0}\). Now it can be shown that at \(\theta=\theta_{0}\) the loss gradient goes to zero for every \(\bm{z}_{1}\in\mathcal{B}_{\Gamma_{k}}\) (the proof is similar to the last part of the proof of Theorem 1). 

#### a.2.3 Proof of corollary 1

Proof.: For \(M=2\), let \(h_{1}\neq 0\). Then the DFB curves of the fixed point \(\Gamma_{1}\) coincide with the BCB curves of the 2-cycle \(\mathcal{O}_{\mathcal{R}\mathcal{L}}\) of the form

\[\mathcal{F}_{1}=\xi_{\mathcal{R}\mathcal{L}}^{1}=\{(h_{1},h_{2},a_{11},a_{22} )|1+a_{11}+a_{22}+a_{11}a_{22}=0\},\] (77)

or

\[\mathcal{F}_{2}=\xi_{\mathcal{R}\mathcal{L}}^{2}=\{(h_{1},h_{2},a_{11},w_{11},w_{21},a_{22})\big{|}1+a_{11}+w_{11}+a_{22}+(a_{11}+w_{11})a_{22}=0\}.\] (78)

For \(M>2\), assume that \(\Gamma_{1}=\{\bm{z}_{1}^{*}\}\) is a fixed point of the system, i.e.

\[\bm{z}_{1}^{*}\,=\,(\bm{I}-\bm{W}_{\Omega(t_{1}^{*})})^{-1}\,\bm{h}=\frac{adj( \bm{I}-\bm{W}_{\Omega(t_{1}^{*})})}{P_{\bm{I}-\bm{W}_{\Omega(t_{1}^{*})}}(1)} \bm{h},\] (79)

where \(\,P_{\bm{I}-\bm{W}_{\Omega(t_{1}^{*})}}(1)\,\) is the characteristic polynomial of \(\,\bm{I}-\bm{W}_{\Omega(t_{1}^{*})}\) at \(1\). Let us denote the first row of the adjoint matrix of \(\bm{I}-\bm{W}_{\Omega(t_{1}^{*})}\) by \(\,adj(\bm{I}-\bm{W}_{\Omega(t_{1}^{*})})_{1}\). If \(adj(\bm{I}-\bm{W}_{\Omega(t_{1}^{*})})_{1}\,\bm{h}\neq 0\), then we can analogously demonstrate that the DFB curves of the fixed point align with the BCB curves of the \(2\)-cycles. This implies that, in accordance with Theorem 2, DFBs of fixed points will also lead to vanishing gradients in the loss function. 

#### a.2.4 Convergence of SCYFI

To ensure that SCYFI almost surely converges, we can simply choose the number of random initializations (i.e., \(N_{out}\) in algorithm 1) large enough such that every linear subregion will have been sampled with probability almost \(1\).More precisely, drawing uniformly from the \(2^{M}\) different \(\bm{D}_{\Omega}\)-matrices (linear subregions) for initialization, the probability that a particular subregion has not been drawn after \(r\) repetitions is \(p=(1-\frac{1}{2^{M}})^{r}\). Hence, in order to ensure that all \(2^{M}\) subregions have been visited with probability \(1-\epsilon\) we need \(r\geq\lceil\frac{\ln(\epsilon)}{\ln(1-\frac{1}{2^{M}})}\rceil\) iterations. Choosing \(N_{out}=r\), we can thus ensure that SCYFI was initialized in each subregion with probability almost \(1\), and thus, in the limit, will have probed all subregions for dynamical objects. This argument extends to \(k\)-cycles by replacing \(2^{M}\) by \(2^{kM}\) above (strictly, a more precise bound for \(k\geq 2\) is given by \(2^{M(k-1)}\times(2^{M}-1)=2^{Mk}-2^{M(k-1)}\), due to the fact that the PLRNN (2) is a linear map in each subregion and, hence, cannot have any \(k\)-cycles with all periodic points in only one subregion).

#### a.2.5 Proof of theorem 3

Proof.: We examine the convergence and scaling behavior of SCYFI for fixed points. A similar argument applies to \(k\)-cycles where \(k>1\).

Let \(\bm{z}_{1}^{*}\) be a fixed point of the system, i.e.

\[\bm{z}_{1}^{*}\,=\,\left(\bm{I}-\bm{W}_{\Omega(t_{1}^{*})}\right)^{-1}\bm{h}.\] (80)

\(\bm{z}_{1}^{*}\) is a true fixed point iff

\[(d_{m}(t_{1}^{*})-a)\cdot z_{m,t_{1}^{*}}\,>\,0,\qquad\quad\forall\,m\in\{1,2, \cdots,M\},\] (81)

where \(\bm{D}_{\Omega(t_{1}^{*})}=diag(d_{1}(t_{1}^{*}),d_{2}(t_{1}^{*}),\cdots,d_{M} (t_{1}^{*}))\) and \(0<a<1\) is a positive real constant.

For examining SCYFI's efficiency, here we focus on two scenarios that impose specific constraints on parameters \(\bm{\theta}\); other cases remain to be investigated.

**Case (I)** : Let \(\bm{R}\) be a randomly generated matrix with uniformly distributed entries in the interval \([0,1)\), and \(\bm{h}\neq 0\,\) be a random vector with all its components being non-negative. For an arbitrary \(\epsilon>0\), we set

\[\bm{A} = \frac{1}{2+\|\bm{R}\|+\epsilon}\,\text{diag}(\bm{R}),\] \[\bm{W} = \frac{1}{2+\|\bm{R}\|+\epsilon}\,\Big{(}\bm{R}-\text{diag}(\bm{R })\Big{)}.\] (82)

Then

\[\|\bm{A}\| = \frac{\|\text{diag}(\bm{R})\|}{2+\|\bm{R}\|+\epsilon}\,<\,\frac{1 }{2+\|\bm{R}\|+\epsilon},\] \[\|\bm{W}\| = \frac{\|\bm{R}-\text{diag}(\bm{R})\|}{2+\|\bm{R}\|+\epsilon}\, \leq\,\frac{\|\bm{R}\|+\|\text{diag}(\bm{R})\|}{2+\|\bm{R}\|+\epsilon}\,<\, \frac{1+\|\bm{R}\|}{2+\|\bm{R}\|+\epsilon},\] (83)

and so \(\|\bm{A}\|+\|\bm{W}\|<1\). Therefore

\[\forall t\ \ \left\|\bm{W}_{\Omega(t)}\right\|=\left\|\bm{A}+\bm{W}\bm{D}_{ \Omega(t)}\right\|\leq\|A\|+\|W\|\left\|\bm{D}_{\Omega(t)}\right\|\leq\|A\|+ \|W\|<1,\] (84)

and so

\[\forall t\ \ \ \rho(\bm{W}_{\Omega(t)})\leq\left\|\bm{W}_{\Omega(t)}\right\|<1.\] (85)

In this case, for any \(n\in\mathbb{N}\), we also have

\[\left\|\prod_{i=1}^{n}\bm{W}_{\Omega(t_{i})}\right\|\,\leq\,\prod_{i=1}^{n} \left\|\bm{W}_{\Omega(t_{i})}\right\|\,\leq\,\Big{(}\,\|A\|+\|W\|\,\Big{)}^{n} \,<\,1.\] (86)

This ensures the stability of all fixed points and \(k\)-cycles of the system.

According to (85), we have

\[\big{(}I-\bm{W}_{\Omega(t_{1}^{*})}\big{)}^{-1}=\sum_{n=0}^{\infty}\bm{W}_{ \Omega(t_{1}^{*})}^{n}\,=\,\bm{I}+\bm{W}_{\Omega(t_{1}^{*})}+\bm{W}_{\Omega(t _{1}^{*})}^{2}+\cdots.\] (87)

Hence, all the elements of \(\big{(}I-\bm{W}_{\Omega(t_{1}^{*})}\big{)}^{-1}\) are positive, and so \(z_{m,t_{1}^{*}}>0\,\) for every \(t_{1}^{*}\). This implies that all true and virtual fixed points exist within a singular sub-region. Additionally, only one fixed point is true, while all the other fixed points are virtual.

**Case (II)** : Let \(\bm{h}=(h_{1},h_{2},\cdots,h_{M})^{\mathsf{T}}\) be a random vector with all \(h_{m}\,\) uniformly distributed in \((0,1]\) and

\[\beta_{min} = \min\big{\{}h_{m}\,:\,h_{m}\in\bm{h},\,1\leq m\leq M\big{\}}>0, \qquad\quad 0<\beta_{min}\leq 1,\] \[\beta_{max} = \max\big{\{}h_{m}\,:\,h_{m}\in\bm{h},\,1\leq m\leq M\big{\}}>0, \qquad\quad 0<\beta_{max}\leq 1.\] (88)Assume further that \(\bm{R}_{1}\) is a randomly generated matrix with uniformly distributed entries in the interval \((-1,0]\), and for \(M\geq 2\)

\[\bm{W}\,=\,\frac{\beta_{min}}{M+\|R_{1}\|+\epsilon}\,\Big{(}\bm{R}_{1}-\text{ diag}(\bm{R}_{1})\Big{)}.\] (89)

Consider

\[\alpha_{max}\,=\,\max\big{\{}|w_{ij}|\,:\,w_{ij}\in\bm{W}\big{\}}, \hskip 28.452756pt0\leq\alpha_{max}<\frac{\beta_{min}}{M+\|R\|+\epsilon}\] (90)

and \(S\subset\{1,2,\cdots,M\}=I\) such that \(K=2^{M-card(S)}\ll 2^{M}\). Suppose that \(\bm{R}_{2}=diag(r_{1},\cdots,r_{M})\) is a randomly chosen diagonal matrix with \(r_{m}\) uniformly distributed in \((-1,1)\) for \(m\in I\setminus S\), and the other elements (\(m\in S\)) uniformly distributed in \((r^{*}-1,0)\) where \(r^{*}=\frac{(M-1)\alpha_{max}\,\beta_{max}}{\beta_{min}}\). Since

\[0\leq\frac{(M-1)\alpha_{max}\,\beta_{max}}{\beta_{min}}\,<\, \frac{(M-1)\,\beta_{max}}{M+\|R\|+\epsilon}\,\leq\,\frac{(M-1)}{M+\|R\|+ \epsilon}\,<\,\frac{(M-1)}{M}<1,\] (91)

so \(-1\leq r^{*}-1<0\).

If

\[\bm{A}\,=\,\frac{1}{2+\|R_{1}\|+\epsilon}\,\bm{R}_{2},\] (92)

then

\[\|\bm{A}\| =\,\frac{\|\bm{R}_{2}\|}{2+\|\bm{R}_{1}\|+\epsilon}\,<\,\frac{1}{ 2+\|\bm{R}_{1}\|+\epsilon},\] \[\|\bm{W}\| =\,\frac{\beta_{min}\,\|\bm{R}_{1}-\text{diag}(\bm{R}_{1})\|}{ M+\|\bm{R}_{1}\|+\epsilon}\,\leq\,\frac{\|\bm{R}_{1}\|+\|\text{diag}(\bm{R}_{1})\|}{ M+\|\bm{R}_{1}\|+\epsilon}\,<\,\frac{1+\|\bm{R}_{1}\|}{2+\|\bm{R}_{1}\|+ \epsilon},\] (93)

which implies \(\|\bm{A}\|+\|\bm{W}\|<1\). We set \(\epsilon>0\) large enough to satisfy the condition

\[\big{(}I-\bm{W}_{\Omega(t_{1}^{*})}\big{)}^{-1}=\sum_{n=0}^{ \infty}\bm{W}_{\Omega(t_{1}^{*})}^{n}\,\approx\,\bm{I}+\bm{W}_{\Omega(t_{1}^{ *})}\hskip 28.452756pt\forall\,t_{1}^{*}.\] (94)

On the other hand, for any \(t\) we have

\[\bm{W}_{\Omega(t)}\,=\,\bm{A}+\bm{W}\bm{D}_{\Omega(t)}=\begin{pmatrix}a_{11}& w_{12}d_{2}(t)&w_{13}d_{3}(t)&\cdots&w_{1M}d_{M}(t)\\ w_{21}d_{1}(t)&a_{22}&w_{23}d_{3}(t)&\cdots&w_{2M}d_{M}(t)\\ w_{31}d_{1}(t)&w_{32}d_{2}(t)&a_{33}&\cdots&w_{3M}d_{M}(t)\\ \vdots&\vdots&\vdots&\ddots&\vdots\\ w_{M1}d_{1}(t)&w_{M2}d_{2}(t)&w_{M3}d_{3}(t)&\cdots&a_{MM}\end{pmatrix}.\] (95)

Hence

\[z_{m,t_{1}^{*}}\,=\,(1+a_{mm})\,h_{m}+\sum_{\begin{subarray}{c}j=1\\ j\neq m\end{subarray}}^{M}w_{mj}\,d_{j}(t_{1}^{*})h_{j}\,=\,(1+a_{mm})\,h_{m}- \sum_{\begin{subarray}{c}j=1\\ j\neq m\end{subarray}}^{M}|w_{mj}|\,d_{j}(t_{1}^{*})h_{j}.\] (96)

Since for every \(t_{1}^{*}\)

\[\sum_{\begin{subarray}{c}j=1\\ j\neq m\end{subarray}}^{M}|w_{mj}|\,d_{j}(t_{1}^{*})h_{j}\,\leq\,\sum_{ \begin{subarray}{c}j=1\\ j\neq m\end{subarray}}^{M}|w_{mj}|\,h_{j},\] (97)

so

\[z_{m,t_{1}^{*}}\,\geq\,(1+a_{mm})\,h_{m}-\sum_{\begin{subarray}{c}j=1\\ j\neq m\end{subarray}}^{M}|w_{mj}|\,h_{j}\hskip 28.452756pt\forall\,m\in I.\] (98)Moreover \(\,a_{ss}\in(r^{*}-1,0)\), for every \(s\in S\), and thus

\[a_{ss}+1\,>\,\frac{(M-1)\alpha_{max}\beta_{max}}{\beta_{min}}\,=\, \frac{\sum_{j=1}^{M}\alpha_{max}\beta_{max}}{\beta_{min}}\,\geq\,\frac{\sum_{j \neq s}^{M}\left|w_{sj}\right|h_{j}}{h_{s}}.\] (99)

Therefore, due to (98) and (99), \(\,z_{s,t_{1}^{*}}>0\,\) for every \(t_{1}^{*}\) and \(s\in S\). This means that all true and virtual fixed points only exist within a relatively small number of sub-regions, denoted as \(K=2^{M-card(S)}\ll 2^{M}\). Given our specific initalization of \(\bm{\theta}\), in both cases **(I)** and **(II)** there is a set of \(K\) different sub-regions, each associated with a unique \(\bm{D}_{\Omega(t)}\) matrix. We refer to the entire set of these matrices as

\[\mathcal{D}_{K}\,=\,\{\bm{D}_{1},...,\bm{D}_{K}\}.\] (100)

SCYFI, by its definition, only moves within the sub-regions that have virtual and true fixed points, continuing until it discovers a true fixed point (or gets stuck in a virtual cycle). Thus, it can iterate between \(J\leq K\) sub-regions

\[\mathcal{D}_{J}=\{\bm{D}_{1},...,\bm{D}_{J}\}\subseteq\mathcal{D}_{K},\] (101)

or within the set of virtual fixed points

\[\mathcal{Z}_{L}=\{\bm{z}_{1},...,\bm{z}_{L}\}.\] (102)

In case **(I)**, there is only one true fixed point. Since all virtual fixed points are located within the same single sub-region, SCYFI's initialization will naturally position it within the correct linear region, requiring no more than \(1\) iteration. Hence, it needs at most \(2\) iterations to find the true fixed point. Consequently, SCYFI's scaling is constant.

For case **(II)**, if we suppose that SCYFI follows the virtual/true fixed point structure of the underlying system in these \(K\) sub-regions, the necessity for the probability of discovering the fixed point to be close to 1, specifically \(1-\epsilon\), is to have

\[N\geq\lceil\frac{\ln(\epsilon)}{\ln(1-\frac{1}{2^{M-card(S)}})}\rceil\,=\, \lceil\frac{\ln(\epsilon)}{\ln(1-\frac{1}{K})}\rceil,\] (103)

iterations. Since \(1\leq card(S)\leq M-1\), so \(K\geq 2\) and \(\ln(1-\frac{1}{K})\approx\frac{-1}{K}\,\). For \(\epsilon^{*}\geq\epsilon\), let \(N=\lceil\frac{\ln(\epsilon^{*})}{\ln(1-\frac{1}{K})}\rceil\geq\lceil\frac{ \ln(\epsilon)}{\ln(1-\frac{1}{K})}\rceil\), then

\[N\,=\,\lceil\frac{\ln(\epsilon^{*})}{\ln(1-\frac{1}{K})}\rceil\,\leq\,\frac{ \ln(\epsilon^{*})}{\ln(1-\frac{1}{K})}\approx\ln(\frac{1}{\epsilon^{*}})K\,:= \,c\,K,\] (104)

which implies the number of iterations is bounded from above. If, for every \(M\), we choose \(K\) small enough, then the upper bound will stay within a linear growth. 

#### a.2.6 Proof of theorem 4

In GTF [24], during training RNN latent states are replaced by a weighted sum of forward propagated states \(\bm{z}_{t}=F_{\bm{\theta}}(\bm{z}_{t-1})\) and data-inferred states \(\bm{\tilde{z}}_{t}=G_{\bm{\phi}}^{-1}(\bm{x}_{t})\) (obtained by inversion of the decoder model \(G_{\bm{\phi}}\)):

\[\bm{\tilde{z}}_{t}:=(1-\alpha)\bm{z}_{t}+\alpha\bm{\tilde{z}}_{t},\] (105)

where \(0\leq\alpha\leq 1\) is the GTF parameter (usually adaptively regulated in training, see [24]). This leads to the following factorization of Jacobians in PLRNN (2) training:

\[\bm{J}_{t}^{GTF}\,=\,\frac{\partial\bm{z}_{t}}{\partial\bm{z}_{t-1}}=\frac{ \partial\bm{z}_{t}}{\partial\bm{\tilde{z}}_{t-1}}\frac{\partial\bm{\tilde{z}}_ {t-1}}{\partial\bm{z}_{t-1}}=\frac{\partial\bm{F_{\theta}}(\tilde{z}_{t-1})}{ \partial\tilde{z}_{t-1}}\frac{\partial\bm{\tilde{z}}_{t-1}}{\partial\bm{z}_{t- 1}}\,=\,(1-\alpha)\bm{\tilde{J}}_{t}\,=\,(1-\alpha)\bm{W}_{\Omega(t)}.\] (106)

Proof.: \((i)\) Since \(\|\bm{A}\|+\|\bm{W}\|\,\leq\,1\), we have

\[\forall t\ \ \left\|\bm{W}_{\Omega(t)}\right\|=\left\|\bm{A}+\bm{W}\bm{D}_{ \Omega(t)}\right\|\leq\|A\|+\|W\|\left\|\bm{D}_{\Omega(t)}\right\|\leq\|A\|+\|W \|\leq 1,\] (107)and so

\[\forall t\;\;\rho(\boldsymbol{W}_{\Omega(t)})\leq\big{\|}\boldsymbol{W}_{\Omega(t) }\big{\|}\leq 1,\] (108)

where \(\rho\) denotes the spectral radius of a matrix. In this case, for any \(n\in\mathbb{N}\), we also have

\[\rho(\prod_{i=1}^{n}\boldsymbol{W}_{\Omega(t_{i})})\,\leq\,\bigg{\|}\prod_{i=1 }^{n}\boldsymbol{W}_{\Omega(t_{i})}\bigg{\|}\,\leq\,\prod_{i=1}^{n}\big{\|} \boldsymbol{W}_{\Omega(t_{i})}\big{\|}\,\leq\,\Big{(}\,\|A\|+\|W\|\,\Big{)}^{n} \,\leq\,1.\] (109)

Now, for any \(0<\alpha<1\), the product of Jacobians under GTF is

\[\prod_{i=1}^{n}\boldsymbol{J}_{t_{i}}^{GTF}\,=\,(1-\alpha)^{n}\prod_{i=1}^{n} \tilde{\boldsymbol{J}}_{t_{i}}\,=\,(1-\alpha)^{n}\prod_{i=1}^{n}\boldsymbol{W }_{\Omega(t_{i})},\] (110)

and

\[\rho\Big{(}\prod_{i=1}^{n}\boldsymbol{J}_{t_{i}}^{GTF}\Big{)} =\,\rho\Big{(}(1-\alpha)^{n}\prod_{i=1}^{n}\boldsymbol{W}_{\Omega (t_{i})}\Big{)}\,=\,(1-\alpha)^{n}\rho\Big{(}\prod_{i=1}^{n}\boldsymbol{W}_{ \Omega(t_{i})}\Big{)}\] \[\leq\,(1-\alpha)^{n}\,<\,1.\] (111)

Hence \(\rho\Big{(}\prod_{i=1}^{n}\boldsymbol{J}_{t_{i}}^{GTF}\Big{)}<1\) which implies for any \(n\in\mathbb{N}\) and \(0<\alpha<1\), the product \(\,\prod_{i=1}^{n}\boldsymbol{J}_{t_{i}}^{GTF}\) has no eigenvalue equal to \(1\) and so no DTB can occur (see definition of DTB in sect. 3).

1. Let \(\|\boldsymbol{A}\|+\|\boldsymbol{W}\|\,=\,r\,>\,1\), then for any \(n\in\mathbb{N}\) we have \[\rho\Big{(}\prod_{i=1}^{n}\tilde{\boldsymbol{J}}_{t_{i}}\Big{)}\leq r^{n},\] (112) and thus \[\rho\Big{(}\prod_{i=1}^{n}\boldsymbol{J}_{t_{i}}^{GTF}\Big{)} =\,(1-\alpha)^{n}\rho\Big{(}\prod_{i=1}^{n}\boldsymbol{W}_{\Omega(t_{i})} \Big{)}\,\leq\,[(1-\alpha)\,r]^{n}.\] (113) Since \(0<1-\frac{1}{r}<1\), inserting \(1-\frac{1}{r}<\alpha=\alpha^{*}<1\) into the r.h.s. of (113) again gives \(\rho\Big{(}\prod_{i=1}^{n}\boldsymbol{J}_{t_{i}}^{GTF}\Big{)}<1\) for any \(n\in\mathbb{N}\), implying that no DTB can occur.

### Additional results

#### a.3.1 Scaling analysis

Although the results presented in Fig. 2 suggest that SCYFI's scaling behavior is much better than theoretically expected, the fact that it is hard to obtain ground truth comparisons for high-dimensional systems (because of the combinatorial explosion) generally makes an extensive empirical analysis difficult. For Fig. 2 we therefore focused on scenarios for which we can also provide analytical curves for an exhaustive search strategy (eq. (5)) and where we then either examined scaling with cycle-order \(k\) for rather low-dimensional systems, or where we explicitly embedded fixed points to search for which allowed us to move to very high dimensionality \(M\). In general we observed that the scaling behavior also depended on the PLRNN's matrix norms and the eigenspectrum of the embedded fixed points, so we constructed different scenarios where we varied these factors as well.

To construct a fairly well behaved case with low matrix norms, we randomly generated matrices \(\bm{R}\) with uniformly distributed entries in the interval \([-1,1]\) and then normalized by its maximal eigenvalue: We set PLRNN parameters \(\bm{A}=\frac{1}{\lambda_{max}}\text{diag}(\bm{R})\) and \(\bm{W}=\frac{1}{\lambda_{max}}(\bm{R}-\text{diag}(\bm{R}))\), and chose \(\bm{h}\) uniformly in the interval \([-50,50]\). For each of \(10\) different such systems, we fixed the number of outer loops and inner loops (\(N_{out}\), \(N_{in}\) in Algorithm 1) such that a fixed point would be detected in at least \(50/75\) independent runs of the algorithm, and then determined the total number \(n\) of linear regions (i.e., across all \(N_{out}\) different initializations) the algorithm needed to cycle throughto detect a stable fixed point. We also ensured that across all different runs this stable fixed point would be the same, in accordance with our assumptions. The resulting scaling behavior was well fitted by a doubly-logarithmic curve of the form \(c_{1}\ln(\ln(M))+c_{2}\) (\(R^{2}\approx 0.913,p<10^{-4}\)). This low-matrix norm scenario with a stable fixed point may be seen as a kind of lower bound on the scaling.

To embed a specific fixed point \(\bm{z}^{*}\), we again start with a matrix \(\bm{R}\) as described above and take \(\bm{A}=\text{diag}(\bm{R})\) and \(\bm{W}=(\bm{R}-\text{diag}(\bm{R}))\). We then minimize

\[\min_{\bm{A},\bm{W},\bm{h}}\mid\bm{z}^{*}-((\bm{A}+\bm{W}\cdot\bm{D}_{\Omega(t ^{*})})\cdot\bm{z}^{*}+\bm{h})\mid,\] (114)

subject to \(\bm{A}\) staying diagonal and \(\bm{W}\) off-diagonal (we observed that adding a small Gaussian noise term to the right appearance of \(\bm{z}^{*}\) in eq. 114 which decayed proportionally to the learning rate improved numerical stability in the optimization process). The such constructed PLRNNs generally have several fixed points, but to compute \(n\) we only search for the inserted fixed point \(\bm{z}^{*}\) (making eq. (5) directly applicable). This way we produced \(5-10\) systems, initializing \(\bm{R}\) with values in \([-0.2,0.2]\) (orange curve in Fig. 2B) or \([-1.0,1.0]\) (blue curve in Fig. 2B), thus effectively restricting the eigenspectrum of the fixed point as well as the matrix norms of the PLRNN to a certain range. However, since matrix norms may change during optimization, eq. (114), our procedure is not strictly guaranteed to produce eigenspectra and matrix norms within a desired range, which is crucial especially for the first scenario where we wanted to keep norms within a 'typical range' (see below). So here, to ensure consistency among drawn systems and with our assumptions, the mean absolute eigenvalue of the embedded fixed points was kept close to \(0.31\pm 0.05\) and the mean maximum absolute eigenvalue close to \(1.25\pm 0.13\). For \(>75\%\) of the resulting systems spectral matrix norms were within the range \([1.0,3.0]\). While this produced matrix spectra typical for trained PLRNNs (\(>95\%\) out of \(361\) PLRNNs trained on various benchmarks and data had spectral matrix norms within \([1.0,3.0]\)), the second initialization range resulted in unnaturally large matrix norms and hence may be seen as providing a kind of upper bound on SCYFI's scaling behavior. Fig. S8 shows the best case (left; purple curve in Fig. 2B) and typical (right; orange curve in Fig. 2B) scaling scenarios on linear scale to better expose the scaling behavior and function fits.

#### a.3.2 Loss jumps & bifurcations in PLRNN training on biophysical model simulations

Here we provide an additional illustration of how SCYFI can be used to dissect bifurcations in model training. For this, we produced time series of membrane voltage and a gating variable from a biophysical neuron model [17], on which we trained a dendPLRNN [10] using BPTT [52] with sparse teacher forcing (STF) [37]. The dendPLRNN used (\(M=9\) latent states, \(B=2\) bases) has \(2^{18}\) different linear subregions and \(|\bm{\theta}|=124\) parameters. Fig. S10A gives the MSE loss as a function of training epoch (i.e., single SGD updates). The loss curve exhibits several steep jumps. Zooming into these points and examining the transitions in parameter space using SCYFI, we find they are indeed produced by bifurcations, with an example given in Fig. S10B. As we had done for Fig. 4 in the main text, since the state and parameter spaces are very high dimensional, for the bifurcation diagram in Fig. S10B all extracted \(k\)-cycles (\(k\geq 1\)), including fixed points, were projected onto a line given by the PCA-derived maximum eigenvalue component, and plotted as a function of training epoch. For the example in Fig. S10B, we found that a BCB (Theorem 2) underlies the transition in the qualitative dynamics of the PLRNN as training progresses. Fig. S10C illustrates the dendPLRNN dynamics just before (left) and right after (right) the bifurcation point highlighted in Fig. S10B, together with time series from the true system.

More generally, whether a bifurcation associated with _vanishing_ gradients produces a loss jump depends on the system's dynamics before and after the bifurcation point. In the case of BCBs, one possible scenario involves a change in stability, as illustrated in Fig. S10. During a BCB, a stable fixed point (or cycle) can loose stability as it passes through the bifurcation point. The maximum Lyapunov exponent of an unstable fixed point (or cycle) is positive, resulting in exploding gradients right after the bifurcation point [37], and consequently to a very steep slope in the loss function near the bifurcation point as in Fig. S10.

#### a.3.3 Dealing with bifurcations in RNN training

Here are some additional thoughts on how RNN training algorithms could possibly be modified to deal with bifurcations. If the algorithm finds itself during training in a parameter regime which does not exhibit the right topological structure, it does not make sense to further dwell within that regime, or possibly anywhere within the vicinity of the current parameter estimate. Unlike standard SGD, the algorithm should therefore perhaps take large leaps in parameter space as soon as it gets stuck in a non-suitable dynamical regime. One possibility to implement this is through a 'look-ahead' mechanism that probes for topological properties of regions not visited so far. While fully fleshing out this idea is beyond this paper, a proof of concept that this may speed up convergence is provided in Fig. S12. Along similar lines, if we knew the model's full bifurcation structure in parameter space ahead of time, we could simply pick a parameter set which corresponds to the right dynamicsdescribing patterns in the data best. While of course it will in general not be feasible to chart the whole bifurcation structure before training (this is in a sense the whole point of a training algorithm), it may be possible to design smart initialization procedures based on this insight, e.g. probing topological regimes at randomly selected points in parameter space before starting training and initializing with parameters that produce a desired type of dynamics (e.g., cyclic behavior) to begin with.

Figure S12: A) Example loss curves for RNNs trained on electrophysiological recordings by BPTT without (blue) vs. with (black) ’look-ahead’ (the look-ahead function checks whether there would be a bifurcation away from a stable fixed point when taking \(10\times\) the current gradient step). Dashed yellow line indicates the epoch at which the look-ahead step was executed. B) Average across 6 loss curves of RNNs trained without (blue) vs. with (black) look-ahead. Error bands = SEM.