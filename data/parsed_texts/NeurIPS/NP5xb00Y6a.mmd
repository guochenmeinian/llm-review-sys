# Fairly Recommending with Social Attributes:

A Flexible and Controllable Optimization Approach

 Jinqiu Jin\({}^{1,}\)1 Haoxuan Li\({}^{2,}\)1 Fuli Feng\({}^{1,}\)2 Sihao Ding\({}^{1}\) Peng Wu\({}^{3}\) Xiangnan He\({}^{1}\)

\({}^{1}\)University of Science and Technology of China

\({}^{2}\)Peking University \({}^{3}\)Beijing Technology and Business University

jjq20021015@mail.ustc.edu.cn, hxli@stu.pku.edu.cn, fulifeng93@gmail.com, dsihao@mail.ustc.edu.cn, pengwu@btbu.edu.cn, xiangnanhe@gmail.com

Equal contribution.Corresponding author.

Footnote 1: footnotemark:

###### Abstract

Item-side group fairness (IGF) requires a recommendation model to treat different item groups similarly, and has a crucial impact on information diffusion, consumption activity, and market equilibrium. Previous IGF notions only focus on the _direct_ utility of the item exposures, _i.e.,_ the exposure numbers across different item groups. Nevertheless, the item exposures also facilitate utility gained from the neighboring users via social influence, called _social_ utility, such as information sharing on the social media. To fill this gap, this paper introduces two social attribute-aware IGF metrics, which require similar user social attributes on the exposed items across the different item groups. In light of the trade-off between the direct utility and social utility, we formulate a new multi-objective optimization problem for training recommender models with flexible trade-off while ensuring controllable accuracy. To solve this problem, we develop a gradient-based optimization algorithm and theoretically show that the proposed algorithm can find Pareto optimal solutions with varying trade-off and guaranteed accuracy. Extensive experiments on two real-world datasets validate the effectiveness of our approach. Our codes are available at https://github.com/mitao-cat/nips23_social_igf.

## 1 Introduction

Developing fair recommendation algorithms is crucial to perform reliable information search and decision making, which prevents users' as well as platforms' interests from being sacrificed [1]. One particular aspect of recommendation fairness is item-side group fairness (IGF), which requires a recommendation model to treat different item groups similarly, and the items are grouped based on attributes such as category or brand [2; 3; 4]. Existing IGF frameworks primarily focus on the _direct_ utility of item exposures, requiring a similar number of exposures for different item groups, and can be broadly categorized into two forms: Statistical Parity (SP) [5; 6] and Equal Opportunity (EO) [7].

Nevertheless, the existing IGF notions overlook the _social_ utility of item exposures, as users with different social attributes may produce varying utilities due to the social influence among users. For example, a user's friends may view a recommended micro-video due to sharing activities or public record in the user's timeline. This can lead to unfairness problem in which some item groups benefit more than others though the existing IGF notions are satisfied. Figure 0(a) illustrates a toy example where item group A receives the same number of exposures as group B, thus satisfying the requirement of the previous SP, while items in group A are recommended to users with greater numbers of friends. This motivates us to consider both direct and social utility for IGF notions.

In this paper, we first introduce two social attribute-aware IGF metrics, namely Neighborhood SP (NSP) and Neighborhood EO (NEO), which require that users exposed to different item groups have similar total social utility. As shown in Figure 0(b), item groups A and B have the same total social utility, which is measured by the number of friends in this toy example. Compared to the previous SP and EO notions, NSP and NEO require similarity in the neighbors of the exposed users, rather than simply requiring the similar number of exposed users across the item groups. However, optimizing only the social attribute-aware IGF metrics may result in varying direct utilities across different item groups, _e.g.,_ item groups A and B in Figure 0(b) have the different number of exposed users. Therefore, it is reasonable to consider both direct and social utility when developing fair recommender models.

To this end, we next formulate a multi-objective optimization problem for training the recommender models with consideration of both direct and social utility, and further require flexibility and controllability of the trade-offs. Specifically, motivated by the previous study [8], we incorporate pre-defined preference regions with varying trade-offs between the direct and social utility into the multi-objective optimization problem, enabling the flexibility to choose the desired trade-offs in practice. Meanwhile, we also incorporate a customizable accuracy constraint for the controllability of the optimization, which leads to a guaranteed recommendation accuracy while satisfying the IGF notions.

We further propose a **S**ocial attribute-aware **F**lexible fair recommendation training algorithm with controllable Accuracy (SoFA), for solving the above multi-objective optimization problem. Given a pre-defined preference region, the proposed gradient-based algorithm updates the model parameters within this region to achieve the desired trade-off. On the other hand, when the accuracy loss exceeds a given threshold, the model parameters update direction is changed to achieve the minimum accuracy guarantee. We also theoretically show that SoFA can find Pareto optimal solutions with varying trade-off, meaning the direct and social utility of the trained model is not dominated by any other solutions. Extensive experiments are conducted on two real-world datasets with user social attributes, and the experimental results validate the effectiveness of the proposed training algorithm.

The main contributions of this paper are summarized as follows:

* We propose two social attribute-aware IGF metrics, named NSP and NEO, to study the item exposure utility gained from the user social network.
* We formalize a multi-objective optimization problem to achieve flexible trade-off between the direct utility and social utility with controllable accuracy sacrifice of the recommendation.
* We further propose a gradient-based optimization algorithm to solve the above problem, called SoFA, and theoretically show that SoFA can find Pareto optimal solutions with varying trade-off.
* We conduct extensive experiments on two real-world datasets, revealing that methods ignoring social influence would lead to unfair exposure, and validating the effectiveness of our proposal.

## 2 Preliminaries

In this section, we present the background about personalized recommendation, item-side group fairness including SP and EO notions, and multi-objective optimization.

**Personalized Recommendation.** Personalized recommendation aims to select a small subset of items to each user \(u\) based on historical data, with users denoted as \(\mathcal{U}=\{u\}\) and items denoted as

Figure 1: A toy example with item groups A and B to illustrate direct and social utilities of the IGF notions, where ”# Exp” is the number of exposed users across the item group, and ”# Friends” is the number of friends of the exposed users. The multi-objective optimization considers both utilities.

\(\mathcal{I}=\{i\}\). In our study, we focus on a common recommendation scenario known as collaborative filtering (CF) with implicit feedback, where the historical data contains the items that each user has interacted with [9, 10, 11, 12]. Bayesian Personalized Ranking (BPR) [9] is one of the representitive CF algorithm modeling the interaction probabilities, by minimizing the pairwise ranking loss

\[\min_{\bm{\theta}}\mathcal{L}_{\mathrm{BPR}}(\bm{\theta})=-\sum_{(u,i,j)\in \mathcal{D}}\ln\sigma\left(\hat{y}_{u,i}-\hat{y}_{u,j}\right)+\frac{\lambda_{ \bm{\theta}}}{2}\|\bm{\theta}\|_{\mathrm{F}}^{2},\] (1)

where \(\mathcal{D}\) denotes the historical data for training, including user \(u\), interacted item \(i\), and no interacted item \(j\). \(\sigma(\cdot)\) is the Sigmoid function, \(\bm{\theta}\) represents the model parameters, and \(\lambda_{\bm{\theta}}\) is the regularization hyper-parameter to prevent overfitting. The predicted user preference scores \(\hat{y}_{u,i}\) and \(\hat{y}_{u,j}\) for positive item \(i\) and negative item \(j\) can be computed by using the widely-adopted matrix factorization [13].

**Item-Side Group Fairness (IGF).** The IGF requires fairness of recommendation holds across a set of item groups \(\mathcal{G}=\{g_{1},g_{2},\ldots,g_{A}\}\), where each item \(i\in\mathcal{I}\) belongs to one or more groups. The group of an item is correspond to attributes such as genre, brand, or other item characteristics, and denote \(G_{g_{a}}(i)\) as the indicator function of whether item \(i\) belongs to group \(g_{a}\), _i.e.,_\(G_{g_{a}}(i)=1\) if \(i\in g_{a}\), otherwise \(G_{g_{a}}(i)=0\). IGF notions aim to ensure fairness across different groups without over-recommending or under-recommending any specific group [14]. The existing IGF notions mainly focus on the exposure levels across different groups, and can be classified into positive rate-based metrics and confusion matrix-based metrics [1]. One particular positive rate-based metric is _Statistical Parity_ (SP) [5, 6], which aims to ensure that each group has an equal likelihood of being recommended and requires the following quantities to be similar across different item groups

\[\mathbb{P}\left(R\mid g=g_{a}\right)=\frac{\sum_{u\in\mathcal{U}}\sum_{i\in \mathcal{I}}G_{g_{a}}(i)\cdot\hat{Y}(u,i)}{\sum_{u\in\mathcal{U}}\sum_{i\in \mathcal{I}}G_{g_{a}}(i)},\quad\text{for all}\quad a=1,\cdots,A,\] (2)

where \(\hat{Y}(u,i)=\sigma(\hat{y}_{u,i})\) estimates the interaction probability for user \(u\) to item \(i\), and \(\sum_{i\in\mathcal{I}}G_{g_{a}}(i)\) is the total number of items belonging to group \(g_{a}\). Instead, confusion matrix-based metrics further consider the ground truth labels. For instance, _Equal Opportunity_ (EO) [7] aims to ensure the same true positive rate across different item groups

\[\mathbb{P}\left(R\mid g=g_{a},y=1\right)=\frac{\sum_{u\in\mathcal{U}}\sum_{i \in\mathcal{I}}G_{g_{a}}(i)\cdot Y(u,i)\cdot\hat{Y}(u,i)}{\sum_{u\in\mathcal{ U}}\sum_{i\in\mathcal{I}}G_{g_{a}}(i)\cdot Y(u,i)},\quad\text{for all}\quad a=1,\cdots,A,\] (3)

where \(Y(u,i)\) is the ground truth label for user \(u\) and item \(i\), and \(\sum_{i\in\mathcal{I}}G_{g_{a}}(i)Y(u,i)\) computes the total number of items that user \(u\) has interacted with in group \(g_{a}\). To keep the same value scale for different metrics, we evaluate the IGF of a recommendation model by computing the relative standard deviation of the group probabilities

\[\begin{split}\mathrm{SP}&=\mathrm{rsd}\left( \mathbb{P}\left(R\mid g=g_{1}\right),\cdots,\mathbb{P}\left(R\mid g=g_{A} \right)\right),\\ \mathrm{EO}&=\mathrm{rsd}\left(\mathbb{P}\left(R\mid g =g_{1},y=1\right),\cdots,\mathbb{P}\left(R\mid g=g_{A},y=1\right)\right), \end{split}\] (4)

where \(\mathrm{rsd}(\cdot)=\mathrm{std}(\cdot)/\mathrm{mean}(\cdot)\) calculates the relative standard deviation. The lower the SP and EO values, the fairer the recommender system satisfies the IGF fairness notions.

**Multi-Objective Optimization.** In many real-world tasks, there might be multiple optimization objectives rather than single one, while these objectives may collaborate or conflict with each other [15, 16]. Multi-objective optimization [17] aims to find a set of solutions that effectively balance the trade-off among these objectives, and it has been widely used in areas such as reinforcement learning [18] and E-commerce [19]. Typically, a multi-objective optimization problem is

\[\min_{\bm{\theta}}\mathcal{L}(\bm{\theta})=\left(\mathcal{L}_{1}(\bm{\theta}), \mathcal{L}_{2}(\bm{\theta}),\cdots,\mathcal{L}_{M}(\bm{\theta})\right)^{ \mathrm{T}},\] (5)

where \(\left(\mathcal{L}_{1}(\bm{\theta}),\cdots,\mathcal{L}_{M}(\bm{\theta})\right)\) states the multiple objectives. To solve the multi-objective optimization problem, several population-based and evolutionary algorithm-based methods [20, 21, 22] have been proposed. Nevertheless, they are not efficient in handling large-scale datasets in recommendation. Alternatively, multi-objective gradient descent approaches [23, 24] address this problem by leveraging Karush-Kuhn-Tucker (KKT) conditions [25] to find a gradient direction that reduces all objective values simultaneously. For example, gradient-based steepest descent methods [23] are proposed to seek a descent direction \(\bm{d}_{t}\) by solving the following optimization problem

\[(\bm{d}_{t},\alpha_{t})=\operatorname*{arg\,min}_{\bm{d},\alpha\in R}\alpha+ \frac{1}{2}\|\bm{d}\|^{2},\text{ s.t. }\nabla\mathcal{L}_{i}(\bm{\theta})^{T}\bm{d}\leq\alpha,\ i=1,\cdots,M,\] (6)then updates the model parameters using gradient descent \(\bm{\theta}_{t+1}=\bm{\theta}_{t}+\eta\bm{d}_{t}\) with step size \(\eta\). Theoretically, the solutions of the descent direction guarantee that all objective values will decrease, thus achieving the Pareto optimality [26]. To obtain these descent direction solutions, one can employ the widely-adopted multiple gradient descent algorithm (MGDA) [24].

## 3 Social Attributes-Aware IGF

Existing IGF notions such as SP and EO define the utility of recommending an item to a user only depends on the exposure and interaction numbers. For example, if item \(i_{1}\) and item \(i_{2}\) are recommended to user \(u_{1}\) and user \(u_{2}\) respectively, then both \(i_{1}\) and \(i_{2}\) have the same utility when \(\hat{Y}(u_{1},i_{1})=\hat{Y}(u_{2},i_{2})\). However, they overlook the social attributes of users. Considering that the user social network plays a crucial role in click or conversion behaviors in real-world recommendation scenarios [27], recommending to users with distinct social attributes may lead to diverse item utilities.

To bridge this gap, we propose social attribute-aware IGF metrics as the extension of previously widely used SP and EO notions. Denote the neighbors (such as friends or followers) of user \(u\) as \(\mathcal{N}_{u}\), for each user \(v\in\mathcal{N}_{u}\), let \(R_{v}(i)>0\) be the utility of item \(i\) gained from user \(v\) via the social influence of user \(u\), when the item \(i\) is recommended to the user \(u\). Therefore, for positive rate-based metrics and confusion matrix-based metrics, the following quantities are required to be similar

\[\mathrm{NR}\left(g=g_{a}\right)=\frac{\sum_{u\in\mathcal{U}}\sum _{i\in\mathcal{I}}G_{g_{a}}(i)\cdot\hat{Y}(u,i)\sum_{v\in\mathcal{N}_{u}}R_{v} (i)}{\sum_{u\in\mathcal{U}}\sum_{i\in\mathcal{I}}G_{g_{a}}(i)},\quad\text{ for all}\quad a=1,\cdots,A,\] (7) \[\mathrm{NR}\left(g=g_{a},y=1\right)=\frac{\sum_{u\in\mathcal{U}} \sum_{i\in\mathcal{I}}G_{g_{a}}(i)\cdot Y(u,i)\cdot\hat{Y}(u,i)\sum_{v\in \mathcal{N}_{u}}R_{v}(i)}{\sum_{u\in\mathcal{U}}\sum_{i\in\mathcal{I}}G_{g_{a }}(i)\cdot Y(u,i)},\quad\text{for all}\quad a=1,\cdots,A,\]

where the sum of utilities from the social network \(\sum_{v\in\mathcal{N}_{u}}R_{v}\) are multiplied by the interaction probability \(\hat{Y}(u,i)\) to measure the total social utility. Similar to Eq. (4), we then compute the _Neighborhood SP_ and _Neighborhood EO_ by the relative standard deviations

\[\mathrm{NSP} =\mathrm{rsd}\left(\mathrm{NR}\left(g=g_{1}\right),\cdots, \mathrm{NR}\left(g=g_{A}\right)\right),\] (8) \[\mathrm{NEO} =\mathrm{rsd}\left(\mathrm{NR}\left(g=g_{1},y=1\right),\cdots, \mathrm{NR}\left(g=g_{A},y=1\right)\right).\]

**Further Discussion.** For the existing and the proposed IGF notions, the former including SP and EO focus on the direct utility obtained through recommendation exposures to users, whereas the latter including SP and EO emphasize the social utility gained from the user social network. Since these metrics emphasize different aspects, optimizing one IGF notion alone does not guarantee the optimality of the other IGF notion. Figures 0(a) and 0(b) illustrate this with a toy example: if we solely optimize SP, it does not guarantee the achievement of the other IGF notion, _i.e.,_ NSP, and vise versa. Since both IGF metrics are important, it is desirable to optimize both SP (EO) and NSP (NEO) simultaneously from the prospective of multi-objective optimization, as illustrated in Figure 0(c).

## 4 Methodology

### Multi-Objective Optimization Problem Formulation

The optimization objective for training a fair recommendation model under IGF notions is to minimize both SP (EO) and NSP (NEO), so as to obtain similar direct utility and similar social utility simultaneously. Building upon prior studies [8; 28], we formulate the fair recommendation training task as a multi-objective optimization problem, where each optimization objective corresponds to a pre-defined IGF metric. Without loss of generality, let the number of IGF metrics used as optimization objectives be \(M\), then we aim to obtain Pareto optimal solutions among these IGF metrics.

**Pareto Dominance.** A solution \(\bm{\theta}_{1}\) dominates another solution \(\bm{\theta}_{2}\) if \(\mathcal{L}_{i}(\bm{\theta}_{1})\leq\mathcal{L}_{i}(\bm{\theta}_{2}),\;\forall i =1,\cdots,M\) and \(\mathcal{L}(\bm{\theta}_{1})=\left(\mathcal{L}_{1}(\bm{\theta}_{1}),\cdots, \mathcal{L}_{M}(\bm{\theta}_{1})\right)^{\mathrm{T}}\neq\mathcal{L}(\bm{ \theta}_{2})=\left(\mathcal{L}_{1}(\bm{\theta}_{2}),\cdots,\mathcal{L}_{M}( \bm{\theta}_{2})\right)^{\mathrm{T}}\).

**Pareto Optimality.** A solution \(\bm{\theta}\) is a Pareto optimal solution if \(\bm{\theta}\) is not dominated by any other solutions. The set of the Pareto optimal solutions is called as Pareto optimal set.

It is now attractive to formulate the trade-off between different IGF metrics as a multi-objective optimization problem, then obtain a Pareto-optimal solution between these metrics. However, due to the well-known accuracy sacrifice for improving fairness [14], directly solving the above problem without considering recommendation accuracy may lead to a significant reduction in recommendation quality. To ensure controllable recommendation accuracy during the optimization process, we introduce an accuracy constraint that penalizes instances with the accuracy loss (_e.g.,_ BPR loss) exceeding a pre-defined threshold. Formally, the multi-objective optimization problem is

\[\min_{\bm{\theta}}\mathcal{L}(\bm{\theta})=(\mathcal{L}_{1}(\bm{ \theta}),\mathcal{L}_{2}(\bm{\theta}),\cdots,\mathcal{L}_{M}(\bm{\theta}))^{ \mathrm{T}}\,,\text{ {s.t. }}\mathcal{L}_{\mathrm{BPR}}(\bm{\theta})\leq\xi,\] (9)

where \(\xi\) is the pre-defined accuracy threshold, and \(\xi\) should be set to exceed the loss of a recommendation model trained using only BPR loss to ensure the existence of feasible solutions.

Furthermore, in real-world recommendation scenarios, the two types of IGF metrics that consider direct and social effects, respectively, may require different proportions of trade-offs. For example, a recommender system designed for social media applications may place more emphasis on the social utility than the direct utility. Therefore, it is meaningful to find flexible Pareto-optimal solutions to the multi-objective optimization problem with varying trade-offs. Following the previous work [8], we decompose the optimization problem into \(N\) subproblems using a set of pre-defined unit preference vectors \(\bm{s}_{1},\bm{s}_{2},\ldots,\bm{s}_{N}\in R_{+}^{M}\), and define the \(N\) preference regions \(\Omega_{1},\Omega_{2},\ldots,\Omega_{N}\) as follows

\[\Omega_{n}=\left\{\mathcal{L}(\bm{\theta})\in R_{+}^{M}\mid\bm{ s}_{j}^{T}\mathcal{L}(\bm{\theta})\leq\bm{s}_{n}^{T}\mathcal{L}(\bm{\theta}), \;\forall j=1,\cdots,N\right\},\] (10)

where \(\mathcal{L}(\bm{\theta})\in\Omega_{n}\) if and only if \(\mathcal{L}(\bm{\theta})\) forms the smallest acute angle with \(\bm{s}_{n}\), resulting in the largest inner product \(\bm{s}_{n}^{T}\mathcal{L}(\bm{\theta})\) among all \(\bm{s}_{1}^{T}\mathcal{L}(\bm{\theta}),\bm{s}_{2}^{T}\mathcal{L}(\bm{\theta}), \ldots,\bm{s}_{N}^{T}\mathcal{L}(\bm{\theta})\). To ensure that the solution \(\mathcal{L}(\bm{\theta})\) obtained from the optimization phase falls within the preference region \(\Omega_{n}\), we further impose a constraint that penalizes the distance of the obtained solution from this preference region

\[\min_{\bm{\theta}}\mathcal{L}(\bm{\theta})=\left(\mathcal{L}_{1} (\bm{\theta}),\mathcal{L}_{2}(\bm{\theta}),\cdots,\mathcal{L}_{M}(\bm{\theta })\right)^{\mathrm{T}}\] (11) \[\text{{s.t. }}\mathcal{G}_{j}(\bm{\theta})=\left(\bm{s}_{j}- \bm{s}_{n}\right)^{T}\mathcal{L}(\bm{\theta})\leq 0,\;\forall j=1,\cdots,N,\] \[\mathcal{L}_{\mathrm{BPR}}(\bm{\theta})\leq\xi.\]

By letting the preference vectors be uniformly distributed in \(R_{+}^{M}\), we can obtain flexible Pareto optimal solutions with varying trade-offs. Each subproblem corresponds to a unique Pareto-optimal solution in a specific preference region, which can reflect different attention to the direct and social utilities. Finally, we can select the desired Pareto optimal solution for diverse recommendation scenarios to satisfy both the flexible trade-offs and the controllable prediction accuracy.

### Gradient-Based Flexible and Controllable Fair Recommendation Training

**Finding the Initial Solution.** For more efficient finding a solution to the multi-objective problem that falls within the specified preference region, we first find a feasible initial solution that satisfies all the constraints of that preference region. Specifically, for a randomly generated initial solution \(\bm{\theta}_{r}\), we define the set of indices that violate the constraints as \(I_{\epsilon}(\bm{\theta}_{r})=\{j=1,\cdots,N\mid\mathcal{G}_{j}(\bm{\theta}_{ r})\geq-\epsilon\}\), where \(\epsilon\) is a small value to deal with solutions near the boundary. Then we can compute a descending direction \(\bm{d}_{r}\) that reduces all the values of \(\{\mathcal{G}_{j}(\bm{\theta}_{r})|j\in I_{\epsilon}(\bm{\theta}_{r})\}\) by solving the optimization problem

\[(\bm{d}_{r},\alpha_{r})=\operatorname*{arg\,min}_{\bm{d},\alpha \in R}\alpha+\frac{1}{2}\|\bm{d}\|^{2},\text{ {s.t. }}\nabla\mathcal{G}_{j}(\bm{\theta}_{r})^{T}\bm{d}\leq\alpha,\;j\in I_{ \epsilon}(\bm{\theta}_{r}).\] (12)

The gradient-based update rule can be expressed as \(\bm{\theta}_{r_{t+1}}=\bm{\theta}_{r_{t}}+\eta_{r}\bm{d}_{r_{t}}\), where \(\eta_{r}\) denotes the step size, and will stop when a feasible solution is found or the maximum iteration number is reached.

**Solving the Subproblem.** Given the feasible initial solution \(\bm{\theta}_{0}\), we next propose a gradient-based learning approach to solve the multi-objective optimization problem in Eq. (11). Specifically, we compute the update direction \(\bm{d}_{t}\) from \(\bm{\theta}_{t}\) to \(\bm{\theta}_{t+1}\) by solving the following optimization problem

\[(\bm{d}_{t},\alpha_{t})= \operatorname*{arg\,min}_{\bm{d},\alpha\in R}\alpha+\frac{1}{2}\| \bm{d}\|^{2}\] (13) \[\text{{s.t. }}\nabla\mathcal{L}_{i}(\bm{\theta}_{t})^{T}\bm{d}\leq \alpha,\;i=1,\cdots,M,\] \[\nabla\mathcal{G}_{j}(\bm{\theta}_{t})^{T}\bm{d}\leq\alpha,\;j\in I _{\epsilon}(\bm{\theta}_{t}),\] \[\nabla\mathcal{L}_{\mathrm{BPR}}(\bm{\theta}_{t})^{T}\bm{d}\leq \alpha,\;\text{if }\mathcal{L}_{\mathrm{BPR}}(\bm{\theta}_{t})\geq\xi.\]The Lagrange function of the above optimization problem is

\[\begin{split}&\mathcal{L}\left(\bm{d},\alpha,\alpha_{i},\beta_{j}, \lambda\right)=\alpha+\frac{1}{2}\|\bm{d}\|^{2}+\sum_{i=1}^{M}\alpha_{i}\left( \nabla_{\bm{\theta}_{t}}\mathcal{L}_{i}(\bm{\theta}_{t})^{T}\bm{d}-\alpha \right)\\ &+\sum_{j\in I_{\epsilon}(\bm{\theta}_{t})}\beta_{j}\left(\nabla_ {\bm{\theta}_{t}}\mathcal{G}_{j}(\bm{\theta}_{t})^{T}\bm{d}-\alpha\right)+ \lambda\cdot\mathbb{I}(\mathcal{L}_{\rm{BPR}}(\bm{\theta}_{t})\geq\xi)\left( \nabla_{\bm{\theta}_{t}}\mathcal{L}_{\rm{BPR}}(\bm{\theta}_{t})^{T}\bm{d}- \alpha\right),\end{split}\] (14)

where \(\alpha_{i}\geq 0,\beta_{j}\geq 0,\lambda\geq 0\) are the Lagrange multipliers. By requiring that the derivatives of \(\mathcal{L}\left(\bm{d},\alpha,\alpha_{i},\beta_{j},\lambda\right)\) with respect to both \(\bm{d}\) and \(\alpha\) be zero, we have the following equations

\[\begin{split}&\bm{d}=-\sum_{i=1}^{M}\alpha_{i}\nabla_{\bm{\theta }_{t}}\mathcal{L}_{i}(\bm{\theta}_{t})-\sum_{j\in I_{\epsilon}(\bm{\theta}_{t })}\beta_{j}\nabla_{\bm{\theta}_{t}}\mathcal{G}_{j}(\bm{\theta}_{t})-\lambda \cdot\mathbb{I}(\mathcal{L}_{\rm{BPR}}(\bm{\theta}_{t})\geq\xi)\nabla_{\bm{ \theta}_{t}}\mathcal{L}_{\rm{BPR}}(\bm{\theta}_{t}),\\ &\sum_{i=1}^{M}\alpha_{i}+\sum_{j\in I_{\epsilon}(\bm{\theta}_{t })}\beta_{j}+\lambda\cdot\mathbb{I}(\mathcal{L}_{\rm{BPR}}(\bm{\theta}_{t}) \geq\xi)=1.\end{split}\] (15)

Next, we compute the minimal value of Eq. (14) by substituting the corresponding terms in Eq. (15), and obtain the dual problem of Eq. (13) as \(\max\limits_{\alpha_{i},\beta_{j},\lambda}\min\limits_{\bm{d},\alpha}\mathcal{ L}\left(\bm{d},\alpha,\alpha_{i},\beta_{j},\lambda\right)\), equals to

\[\begin{split}&\min\limits_{\alpha_{i},\beta_{j},\lambda}\frac{1}{ 2}\left\|\sum_{i=1}^{M}\alpha_{i}\nabla_{\bm{\theta}_{t}}\mathcal{L}_{i}(\bm{ \theta}_{t})+\sum_{j\in I_{\epsilon}(\bm{\theta}_{t})}\beta_{j}\nabla_{\bm{ \theta}_{t}}\mathcal{G}_{j}(\bm{\theta}_{t})+\lambda\cdot\mathbb{I}(\mathcal{ L}_{\rm{BPR}}(\bm{\theta}_{t})\geq\xi)\nabla_{\bm{\theta}_{t}}\mathcal{L}_{\rm{BPR}}(\bm{ \theta}_{t})\right\|^{2},\\ &\text{\it s.t. }\sum_{i=1}^{M}\alpha_{i}+\sum_{j\in I_{ \epsilon}(\bm{\theta}_{t})}\beta_{j}+\lambda\cdot\mathbb{I}(\mathcal{L}_{\rm{ BPR}}(\bm{\theta}_{t})\geq\xi)=1,\end{split}\] (16)

which can be efficiently solved by the gradient-based optimization methods such as MGDA [24].

Denote the solutions of Eq. (13) and Eq. (16) as \((\bm{d}^{*},\alpha^{*})\) and \((\alpha_{i}^{*},\beta_{j}^{*},\lambda^{*})\), respectively. To obtain the solution of the dual problem, according to the KKT conditions, we have

\[\begin{split}&\alpha_{i}^{*}\left(\nabla_{\bm{\theta}_{t}} \mathcal{L}_{i}(\bm{\theta}_{t})^{T}\bm{d}^{*}-\alpha^{*}\right)=0,\;i=1, \cdots,M,\\ &\beta_{j}^{*}\left(\nabla_{\bm{\theta}_{t}}\mathcal{G}_{j}(\bm{ \theta}_{t})^{T}\bm{d}^{*}-\alpha^{*}\right)=0,\;\forall j\in I_{\epsilon}( \bm{\theta}_{t}),\\ &\lambda\cdot\mathbb{I}(\mathcal{L}_{\rm{BPR}}(\bm{\theta}_{t}) \geq\xi)\left(\nabla_{\bm{\theta}_{t}}\mathcal{L}_{\rm{BPR}}(\bm{\theta}_{t}) ^{T}\bm{d}^{*}-\alpha^{*}\right)=0,\;\text{if }\mathcal{L}_{\rm{BPR}}(\bm{\theta}_{t})\geq\xi.\end{split}\] (17)

By adding all equations in Eq. (17) together, we have \(\alpha^{*}=-\|\bm{d}^{*}\|^{2}\). In this way,

* If \(\bm{\theta}_{t}\) is Pareto optimal, then no other solution in its neighborhood can achieve better objective values, and we obtain the solution \(\bm{d}^{*}=\bm{0}\), indicating that no direction can simultaneously improve all objective values.
* If \(\bm{\theta}_{t}\) is not Pareto optimal, we have the following conclusions from Eq. (13) \[\begin{split}&\nabla\mathcal{L}_{i}(\bm{\theta}_{t})^{T}\bm{d}^{ *}\leq\alpha^{*}\leq-\|\bm{d}^{*}\|^{2}<0,\;i=1,\cdots,M,\\ &\nabla\mathcal{G}_{j}(\bm{\theta}_{t})^{T}\bm{d}^{*}\leq\alpha^{ *}\leq-\|\bm{d}^{*}\|^{2}<0,\;j\in I_{\epsilon}(\bm{\theta}_{t}),\\ &\nabla\mathcal{L}_{\rm{BPR}}(\bm{\theta}_{t})^{T}\bm{d}^{*}\leq \alpha^{*}\leq-\|\bm{d}^{*}\|^{2}<0,\;\text{if }\mathcal{L}_{\rm{BPR}}(\bm{\theta}_{t})\geq\xi,\end{split}\] (18) therefore \(\bm{d}^{*}\) will be a descent direction that at least decreases all the IGF losses simultaneously, as well as the recommendation accuracy loss when \(\mathcal{L}_{\rm{BPR}}(\bm{\theta}_{t})\geq\xi\).

Finally, the model parameters are updated as \(\bm{\theta}_{t+1}=\bm{\theta}_{t}+\eta\bm{d}_{t}\), where \(\eta\) denotes the step size. Remarkably, throughout the optimization procedure, the values of all IGF objectives and the distance between the obtained solution and the preference region decrease, indicating the Pareto optimality within the preference region. Moreover, to tackle the problem of high-dimensional parameter \(\bm{\theta}\) in the multi-objective optimization, we propose to optimize its dual problem rather than directly solving the primal problem. As a result, the numbers of parameters \((\alpha_{i},\beta_{j})\) in the dual problem is significantly reduced to be same as the numbers of IGF objectives and constraints, respectively, which greatly increases the scalability of the proposed algorithm to the large-scale recommendation datasets. We summarize the whole gradient-based fair recommendation training process in Algorithm 1.

## 5 Experiments

In this section we aim to answer the following research questions:

* **RQ1:** How does our proposed method perform compared with the existing approaches?
* **RQ2:** Do the preference regions in our method lead to more flexible Pareto solutions? Does the accuracy constraint result in more accurate recommendations while achieving IGF?
* **RQ3:** How does varying the accuracy thresholds affect the performance of our method?

### Experiment Setups

**Datasets and Pre-processing.** The experiments are performed on two benchmark recommendation datasets that include user social attributes and item features:

\(\bullet\)**KuaiRec** is a short-video recommendation dataset on the video-sharing platform Kuaishou3, which contains interactions between users and videos. In our experiments, we select user clicks spanning 10 consecutive days as the outcome of interest [29] and apply 10-core filtering to eliminate users and items with minimal interactions, which results in a pre-processed dataset consisting of 7,010 users, 1,405 items, and 863,819 interactions. For each user, the number of followers of that user is treated as the user social attribute, and we define the social utility derived from the user social attribute as the total fans count of that user. As to videos, we use tags to categorize them into six item groups.

Footnote 3: https://www.kuaishou.com/.

\(\bullet\)**Epinions** is a dataset derived from a trust network, comprising users' rating scores for products. Following previous work [10], we only keep the interactions with ratings greater than 3 and also adopt the 10-core filtering setting. Subsequently, we obtain a dataset consisting of 11,710 users, 13,682 items, and 215,262 interactions. For each user, the dataset includes their trust relations with other users, while for each product, it specifies its category. We let user's social utility equal to the number of trustors, and classify the products into six groups based on their categories.

Throughout our experiment, we randomly split the interactions of each dataset into training set (60%), validation set (20%), and testing set (20%). We use the validation set for tuning hyper-parameters and the testing data for evaluation the prediction and fairness performance of the trained models.

Evaluation Protocols.We evaluate the recommendation accuracy with NDCG@5 [30]. For evaluating IGF, we report both the previous fairness metrics and social attribute-aware metrics, where lower value indicates fairer performance. We study two settings with \((\mathrm{SP},\mathrm{NSP})\) and \((\mathrm{EO},\mathrm{NEO})\) as the IGF objectives, respectively. Under each setting, the overall fairness is assessed using the harmonic-mean of the two IGF metrics denoted as \(\mathrm{F1SP}\). In addition, to quantify the trade-offs between the two IGF metrics, we calculate the angles formed by these metrics via \(\mathrm{deg}=\arctan(\mathrm{NSP}/\mathrm{SP})\), where a lower angle degree signifies a greater emphasis on the social utility.

Hyper-Parameter Settings.For all compared methods, we use a pre-trained BPRMF [9] as the backbone with fixed optimal batch size and regularization coefficient, then fine-tune the model by using the baselines methods for achieving IGF notions. For the proposed SoFA, we set \(N=5\) as the number of preference vectors that equally divide the first quadrant, and tune the learning rate for finding the initial solution within \(\{0.3\mathrm{lr},\mathrm{lr},3\mathrm{lr}\}\), where \(\mathrm{lr}\) denotes the learning rate for the pre-trained BPRMF. This optimization step stops once a feasible solution is found. The learning rate for all methods is searched within \(\{0.03\mathrm{lr},0.1\mathrm{lr},0.3\mathrm{lr},\mathrm{lr},3\mathrm{lr},10 \mathrm{lr}\}\), and the coefficients of IGF terms for regularization-based and post-processing baselines are searched within \(\{0.1,0.2,0.5,1,2,5\}\). The early stopping strategy is employed when the F1SP or F1EO value does not decrease over 5 epochs.

\begin{table}
\begin{tabular}{l c c c c c c c c} \hline \hline  & \multicolumn{4}{c}{KuaiRec} & \multicolumn{4}{c}{Epinions} \\ \cline{2-9}  & N@\(\mathbf{5\uparrow}\) & SP\(\downarrow\) & NSP\(\downarrow\) & F1SP \(\downarrow\) deg & N@\(\mathbf{5\uparrow}\) & SP\(\downarrow\) & NSP\(\downarrow\) & F1SP\(\downarrow\) deg \\ \hline BPRMF & **0.2426** & 0.0966 & 0.1119 & 0.1037 49.2\({}^{\circ}\) & 0.0443 & 0.0252 & 0.0286 & 0.0268 48.6\({}^{\circ}\) \\ \hline + SP Reg & \(0.2389\) & 0.0062 & 0.0168 & 0.0091 69.7\({}^{\circ}\) & 0.0450 & 0.0140 & 0.0196 & 0.0163 54.5\({}^{\circ}\) \\ + NSP Reg & \(0.2279\) & 0.0366 & 0.0142 & 0.0205 21.2\({}^{\circ}\) & 0.0378 & 0.0224 & 0.0188 & 0.0205 40.0\({}^{\circ}\) \\ + SP\&NSP Reg & \(0.2369\) & 0.0090 & 0.0245 & 0.0132 69.8\({}^{\circ}\) & 0.0448 & 0.0154 & 0.0205 & 0.0176 53.2\({}^{\circ}\) \\ \hline + SP Post & \(0.2412\) & 0.0388 & 0.0545 & 0.0454 54.5\({}^{\circ}\) & 0.0445 & 0.0141 & 0.0196 & 0.0164 54.2\({}^{\circ}\) \\ + NSP Post & \(0.2348\) & 0.0844 & 0.0311 & 0.0455 20.3\({}^{\circ}\) & 0.0398 & 0.0212 & 0.0185 & 0.0197 41.2\({}^{\circ}\) \\ + SP\&NSP Post & \(0.2405\) & 0.0817 & 0.0562 & 0.0666 34.5\({}^{\circ}\) & 0.0443 & 0.0152 & 0.0207 & 0.0175 53.7\({}^{\circ}\) \\ \hline MOOMTL & \(0.2229\) & 0.0069 & 0.0238 & 0.0107 73.8\({}^{\circ}\) & 0.0446 & 0.0138 & 0.0193 & 0.0161 54.4\({}^{\circ}\) \\ \hline SoFA region 0 & \(0.2349\) & 0.0296 & **0.0096** & 0.0145 18.0\({}^{\circ}\) & 0.0364 & 0.0909 & 0.0294 & 0.0445 17.9\({}^{\circ}\) \\ SoFA region 1 & \(0.2376\) & 0.0179 & 0.0105 & 0.0133 30.4\({}^{\circ}\) & 0.0441 & 0.0326 & 0.0225 & 0.0266 34.6\({}^{\circ}\) \\ SoFA region 2 & \(0.2329\) & 0.0103 & 0.0146 & 0.0121 54.8\({}^{\circ}\) & **0.0451** & 0.0153 & 0.0210 & 0.0177 53.9\({}^{\circ}\) \\ SoFA region 3 & \(0.2413\) & 0.0074 & 0.0194 & 0.0107 69.1\({}^{\circ}\) & 0.0427 & 0.0118 & **0.0177** & **0.0142** 56.3\({}^{\circ}\) \\ SoFA region 4 & \(0.2402\) & **0.0046** & 0.0227 & **0.0077** 78.5\({}^{\circ}\) & 0.0185 & **0.0095** & 0.0314 & 0.0146 73.1\({}^{\circ}\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Performance comparison using SP and NSP as IGF notions, where SoFA is implemented with five preference regions. The best and second best results are bolded and underlined, respectively.

\begin{table}
\begin{tabular}{l c c c c c c c c} \hline \hline  & \multicolumn{4}{c}{KuaiRec} & \multicolumn{4}{c}{Epinions} \\ \cline{2-9}  & N@\(\mathbf{5\uparrow}\) & EO\(\downarrow\) & NEO\(\downarrow\) & F1EO \(\downarrow\) deg & N@\(\mathbf{5\uparrow}\) & EO\(\downarrow\) & NEO\(\downarrow\) & F1EO\(\downarrow\) deg \\ \hline BPRMF & **0.2426** & 0.0189 & 0.1031 & 0.0319 79.6\({}^{\circ}\) & 0.0443 & 0.1182 & 0.2059 & 0.1502 60.1\({}^{\circ}\) \\ \hline + EO Reg & \(0.2340\) & 0.0044 & 0.1082 & 0.0085 87.7\({}^{\circ}\) & 0.0425 & 0.1126 & 0.1956 & 0.1429 60.1\({}^{\circ}\) \\ + NEO Reg & \(0.2329\) & 0.0147 & 0.0992 & 0.0256 81.6\({}^{\circ}\) & 0.0430 & 0.1289 & 0.1557 & 0.1410 50.4\({}^{\circ}\) \\ + EO\&NEO Reg & \(0.2346\) & 0.0054 & 0.1035 & 0.0102 87.0\({}^{\circ}\) & 0.0432 & 0.1136 & 0.1465 & 0.1280 52.2\({}^{\circ}\) \\ \hline + EO Post & \(0.2371\) & 0.0061 & 0.1082 & 0.0116 86.8\({}^{\circ}\) & 0.0437 & 0.1100 & 0.1850 & 0.1379 59.3\({}^{\circ}\) \\ + NEO Post & \(0.2280\) & 0.0143 & 0.0672 & 0.0236 78.0\({}^{\circ}\) & 0.0427 & 0.1274 & 0.1565 & 0.1405 50.8\({}^{\circ}\) \\ + EO\&NEO Post & \(0.2413\) & 0.0133 & 0.1039 & 0.0235 82.7\({}^{\circ}\) & 0.0436 & 0.1131 & 0.1630 & 0.1335 53.3\({}^{\circ}\) \\ \hline MOOMTL & \(0.2332\) & 0.0031 & 0.1003 & 0.0061 88.2\({}^{\circ}\) & 0.0444 & **0.1093** & 0.1449 & **0.1246** 53.0\({}^{\circ}\) \\ \hline SoFA region 0 & \(0.1635\) & 0.1454 & 0.0557 & 0.0806 21.0\({}^{\circ}\) & 0.0224 & 0.3273 & 0.1494 & 0.2051 24.5\({}^{\circ}\) \\ SoFA region 1 & \(0.2077\) & 0.1135 & 0.0558 & 0.0749 26.2\({}^{\circ}\) & 0.0353 & 0.1752 & **0.1394** & 0.1552 38.

### Performance Comparison (RQ1)

We compare our proposed SoFA with the vanilla BPRMF [9] and several baseline methods to achieve recommendation fairness. These baselines include: (1) Regularization-based method [31], which incorporates IGF metrics as regularization terms into the BPR loss. (2) Post-processing method [32], which reconstructs the recommendation results while imposing fairness objectives as constraints. (3) Multi-objective optimization-based method [28], which optimizes the formulated multi-objective problem without the preference regions and recommendation accuracy constraint. We adopt three versions of regularization-based methods and post-processing methods, which considers only direct utility (SP or EO), only social utility (NSP or NEO), and both utilities, respectively.

Tables 1 and 2 show the overall performance of compared methods under the (SP, NSP) and (EO, NEO) settings, respectively, from which we have the following observations: (1) In all cases, SoFA outperforms other baselines in terms of accuracy and fairness, which validates its effectiveness in fairly recommending with accuracy guarantee. (2) SoFA can effectively find solutions in different preference regions, showing the ability to accommodate customized trade-off between the direct utility and social utility as the IGF metrics. Figure 1(a) visualizes the trade-off between SP and NSP, showing that SoFA is able to find pareto optimal solutions between the IGF metrics. Figure 1(b) suggests that F1SP and NDCG do not always exhibit a conflicting trade-off, meaning that enhancing fairness does not necessarily sacrifice recommendation accuracy in certain scenarios.

\begin{table}
\begin{tabular}{l c c c c c c c c} \hline \hline  & \multicolumn{4}{c}{KuaiRec} & \multicolumn{4}{c}{Epinions} \\ \cline{2-9}  & N@5\(\uparrow\) & SP\(\downarrow\) & NSP\(\downarrow\) & F1SP \(\downarrow\) deg & N@5\(\uparrow\) & SP\(\downarrow\) & NSP\(\downarrow\) & F1SP \(\downarrow\) deg \\ \hline SoFA region 0 & \(0.2349\) & \(0.0296\) & \(0.0096\) & \(0.0145\)\({}_{18.0^{\circ}}\) & \(0.0364\) & \(0.0909\) & \(0.0294\) & \(0.0445\)\({}_{17.9^{\circ}}\) \\ w/o con & \(0.2181\) & \(0.0415\) & \(0.0124\) & \(0.0191\)\({}_{16.6^{\circ}}\) & \(0.0339\) & \(0.0814\) & \(0.0260\) & \(0.0395\)\({}_{17.7^{\circ}}\) \\ \hline SoFA region 1 & \(0.2376\) & \(0.0179\) & \(0.0105\) & \(0.0133\)\({}_{30.4^{\circ}}\) & \(0.0441\) & \(0.0326\) & \(0.0225\) & \(0.0266\)\({}_{34.6^{\circ}}\) \\ w/o con & \(0.2254\) & \(0.0211\) & \(0.0121\) & \(0.0154\)\({}_{29.8^{\circ}}\) & \(0.0429\) & \(0.0371\) & \(0.0267\) & \(0.0311\)\({}_{35.8^{\circ}}\) \\ \hline SoFA region 2 & \(0.2329\) & \(0.0103\) & \(0.0146\) & \(0.0121\)\({}_{54.8^{\circ}}\) & **0.0451** & \(0.0153\) & \(0.0210\) & \(0.0177\)\({}_{53.9^{\circ}}\) \\ w/o con & \(0.2117\) & \(0.0085\) & \(0.0131\) & \(0.0103\)\({}_{57.0^{\circ}}\) & \(0.0445\) & \(0.0163\) & \(0.0224\) & \(0.0189\)\({}_{53.9^{\circ}}\) \\ \hline SoFA region 3 & \(0.2413\) & \(0.0074\) & \(0.0194\) & \(0.0107\)\({}_{69.1^{\circ}}\) & \(0.0427\) & \(0.0118\) & \(0.0177\) & **0.0142**\({}_{66.3^{\circ}}\) \\ w/o con & \(0.2212\) & \(0.0077\) & \(0.0216\) & \(0.0113\)\({}_{70.4^{\circ}}\) & \(0.0417\) & \(0.0131\) & \(0.0222\) & \(0.0165\)\({}_{59.4^{\circ}}\) \\ \hline SoFA region 4 & \(0.2402\) & **0.0046** & \(0.0227\) & \(0.0077\)\({}_{78.5^{\circ}}\) & \(0.0185\) & \(0.0095\) & \(0.0314\) & \(0.0146\)\({}_{73.1^{\circ}}\) \\ w/o con & \(0.2193\) & \(0.0069\) & \(0.0256\) & \(0.0109\)\({}_{74.9^{\circ}}\) & \(0.0169\) & **0.0093** & \(0.0313\) & \(0.01414\)\({}_{73.4^{\circ}}\) \\ \hline w/o SP & \(0.2220\) & \(0.0206\) & **0.0064** & \(0.0098\)\({}_{17.4^{\circ}}\) & \(0.0365\) & \(0.0216\) & **0.0147** & \(0.0175\)\({}_{34.3^{\circ}}\) \\ w/o NSP & **0.2415** & \(0.0050\) & \(0.0173\) & \(0.0077\)\({}_{74.0^{\circ}}\) & \(0.0447\) & \(0.0148\) & \(0.0200\) & \(0.0170\)\({}_{53.5^{\circ}}\) \\ w/o region & \(0.2412\) & \(0.0046\) & \(0.0193\) & **0.0074**\({}_{76.7^{\circ}}\) & \(0.0447\) & \(0.0141\) & \(0.0197\) & \(0.0164\)\({}_{54.4^{\circ}}\) \\ \hline \hline \end{tabular}
\end{table}
Table 3: Ablation studies of SoFA on the accuracy constraint, IGF objectives, and preference region.

Figure 2: The trade-offs on KuaiRec between (a) IGF metrics, _i.e.,_ SP and NSP, and (b) overall fairness and accuracy, _i.e.,_ F1SP and NDCG, where yellow lines refer to the results when only optimizing one IGF metric, and scatters located on the upper right indicate better overall performance.

### In-depth Analysis

**Ablation Study (RQ2).** We conduct the following ablations to evaluate the effectiveness of specific designs in SoFA: (1) _w/o con_, which trains the model without the recommendation accuracy constraint. (2) _w/o region_, which trains the model without specifying the preference region. (3) _w/o SP_, which trains the model without the SP fairness loss, thus does not require the preference region specification. (4) _w/o NSP_, similar to _w/o SP_ but trains the model without the NSP fairness loss. Table 3 shows the performance of SoFA and its ablated versions, we find that training the model without the accuracy constraint encounters significant drop in recommendation accuracy compared to SoFA, indicating the effectiveness of the proposed accuracy constraint. In addition, removing the specification of the preference region may not hurt the performance in terms of accuracy and fairness, because the preference region do not directly contribute to the recommendation accuracy and overall fairness. However, it fails to obtain a recommendation model with flexibility to trade-off these IGF metrics.

**Effects of Accuracy Requirement (RQ3).** To further investigate the effect of recommendation accuracy constraint, we evaluate SoFA with relaxed constraints by increasing the threshold \(\xi\) in Eq. (9). In particular, we compare the default choice of the accuracy threshold with 1.1 times and 1.2 times of that value, as well as the optimization without the recommendation accuracy constraint. We conduct 100 runs for each optimization process on KuaiRec with varying preference regions. Figure 3 shows the accuracy and fairness performance, and we have observations as below: (1) NDCG has a clear decrease trend as the threshold increases, which further reveals that constraining the recommendation loss can control the accuracy of the obtained solution. (2) F1SP improves as the threshold decreases in most regions, which indicates that the accuracy constraint may also benefit the fairness. Therefore, we suggest to use the original BPRMF loss as the accuracy threshold in practice.

## 6 Conclusion

This study addresses a new problem in IGF motivated by the effect of user social influence on item utility. First, we propose two IGF notions, namely NSP and NEO, as extensions to the existing IGF notions that only considers the direct utility of item exposures. Next, we formulate a multi-objective optimization problem with pre-defined preference regions to ensure the flexibility of the trade-off between these IGF metrics. Then, we incorporate a recommendation accuracy constraint to control the accuracy sacrifice for satisfying the fairness requirements. We further propose an algorithm to solve the optimization problem and theoretically demonstrate its Pareto optimality. Extensive experiments on two real-world datasets validate the effectiveness of our method. One interesting future research direction is to generalize the user social influence studies in this paper to ranking-based IGF settings.

## Acknowledgement

This work is supported by the National Key Research and Development Program of China (No. 2022YFB3104701) and the National Natural Science Foundation of China (No. 12301370).

Figure 3: Effects of varying accuracy thresholds in the optimization on KuaiRec with 100 repeats.

## References

* [1] Simon Caton and Christian Haas. Fairness in Machine Learning: A Survey. _ACM Computing Surveys_, 2020.
* [2] Rishabh Mehrotra, James McInerney, Hugues Bouchard, Mounia Lalmas, and Fernando Diaz. Towards a Fair Marketplace: Counterfactual Evaluation of the Trade-Off between Relevance, Fairness & Satisfaction in Recommendation Systems. In _Proceedings of the 27th ACM International Conference on Information and Knowledge Management_, pages 2243-2251, 2018.
* [3] Ziwei Zhu, Xia Hu, and James Caverlee. Fairness-Aware Tensor-Based Recommendation. In _Proceedings of the 27th ACM International Conference on Information and Knowledge Management_, pages 1153-1162, 2018.
* [4] Tao Qi, Fangzhao Wu, Chuhan Wu, Peijie Sun, Le Wu, Xiting Wang, Yongfeng Huang, and Xing Xie. ProFairRec: Provider Fairness-Aware News Recommendation. In _Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval_, pages 1164-1173, 2022.
* [5] Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard Zemel. Fairness through Awareness. In _Proceedings of the 3rd Innovations in Theoretical Computer Science Conference_, pages 214-226, 2012.
* [6] Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez-Rodriguez, and Krishna P Gummadi. Fairness Constraints: A Flexible Approach for Fair Classification. _The Journal of Machine Learning Research_, 20(1):2737-2778, 2019.
* [7] Moritz Hardt, Eric Price, and Nati Srebro. Equality of Opportunity in Supervised Learning. In _Advances in Neural Information Processing Systems_, 2016.
* [8] Xi Lin, Hui-Ling Zhen, Zhenhua Li, Qing-Fu Zhang, and Sam Kwong. Pareto Multi-Task Learning. In _Advances in Neural Information Processing Systems_, 2019.
* [9] Steffen Rendle, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidt-Thieme. Bpr: Bayesian Personalized Ranking from Implicit Feedback. _arXiv preprint arXiv:1205.2618_, 2012.
* [10] Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, and Tat-Seng Chua. Neural Collaborative Filtering. In _Proceedings of the 26th International Conference on World Wide Web_, pages 173-182, 2017.
* [11] Yehuda Koren, Steffen Rendle, and Robert Bell. Advances in Collaborative Filtering. _Recommender Systems Handbook_, pages 91-142, 2021.
* [12] Le Wu, Xiangnan He, Xiang Wang, Kun Zhang, and Meng Wang. A Survey on Accuracy-Oriented Neural Recommendation: From Collaborative Filtering to Information-Rich Recommendation. _IEEE Transactions on Knowledge and Data Engineering_, 35(5):4425-4445, 2022.
* [13] Yehuda Koren, Robert Bell, and Chris Volinsky. Matrix Factorization Techniques for Recommender Systems. _Computer_, 42(8):30-37, 2009.
* [14] Yunqi Li, Hanxiong Chen, Shuyuan Xu, Yingqiang Ge, Juntao Tan, Shuchang Liu, and Yongfeng Zhang. Fairness in Recommendation: Foundations, Methods, and Applications. _ACM Transactions on Intelligent Systems and Technology_, 14(5):1-48, 2023.
* [15] Sebastian Ruder. An Overview of Multi-Task Learning in Deep Neural Networks. _arXiv preprint arXiv:1706.05098_, 2017.
* [16] Yu Zhang and Qiang Yang. A Survey on Multi-Task Learning. _IEEE Transactions on Knowledge and Data Engineering_, 34(12):5586-5609, 2021.
* [17] Kalyanmoy Deb and Kalyanmoy Deb. Multi-Objective Optimization. In _Search Methodologies: Introductory Tutorials in Optimization and Decision Support Techniques_, pages 403-449. Springer, 2013.

* [18] Xiao Lin, Hongjie Chen, Changhua Pei, Fei Sun, Xuanji Xiao, Hanxiao Sun, Yongfeng Zhang, Wenwu Ou, and Peng Jiang. A Pareto-Efficient Algorithm for Multiple Objective Optimization in E-commerce Recommendation. In _Proceedings of the 13th ACM Conference on Recommender Systems_, pages 20-28, 2019.
* [19] Kristof Van Moffaert and Ann Nowe. Multi-Objective Reinforcement Learning Using Sets of Pareto Dominating Policies. _The Journal of Machine Learning Research_, 15(1):3483-3512, 2014.
* [20] Eckart Zitzler. _Evolutionary Algorithms for Multiobjective Optimization: Methods and Applications_, volume 63. Shaker Ithaca, 1999.
* [21] Abdullah Konak, David W Coit, and Alice E Smith. Multi-Objective Optimization Using Genetic Algorithms: A Tutorial. _Reliability Engineering & System Safety_, 91(9):992-1007, 2006.
* [22] Ioannis Giagkiozis, Robin C Purshouse, and Peter J Fleming. An Overview of Population-Based Algorithms for Multi-Objective Optimization. _International Journal of Systems Science_, 46(9):1572-1599, 2015.
* [23] Jorg Fliege and Benar Fux Svaiter. Steepest Descent Methods for Multicriteria Optimization. _Mathematical Methods of Operations Research_, 51:479-494, 2000.
* [24] Jean-Antoine Desideri. Multiple-Gradient Descent Algorithm (MGDA) for Multiobjective Optimization. _Comptes Rendus Mathematique_, 350(5-6):313-318, 2012.
* [25] William Karush. Minima of Functions of Several Variables with Inequalities as Side Constraints. _M. Sc. Dissertation. Dept. of Mathematics, Univ. of Chicago_, 1939.
* [26] Vilfredo Pareto et al. Manual of Political Economy. 1971.
* [27] Wenqi Fan, Yao Ma, Qing Li, Yuan He, Eric Zhao, Jiliang Tang, and Dawei Yin. Graph Neural Networks for Social Recommendation. In _The World Wide Web Conference_, pages 417-426, 2019.
* [28] Ozan Sener and Vladlen Koltun. Multi-Task Learning as Multi-Objective Optimization. In _Advances in Neural Information Processing Systems_, 2018.
* [29] Chongming Gao, Shijun Li, Wenqiang Lei, Jiawei Chen, Biao Li, Peng Jiang, Xiangnan He, Jiaxin Mao, and Tat-Seng Chua. KualRec: A Fully-observed Dataset and Insights for Evaluating Recommender Systems. In _Proceedings of the 31st ACM International Conference on Information & Knowledge Management_, pages 540-550, 2022.
* [30] Kalervo Jarvelin and Jaana Kekalainen. Cumulated Gain-Based Evaluation of IR Techniques. _ACM Transactions on Information Systems_, 20(4):422-446, 2002.
* [31] Dimitris Sacharidis, Carine Pierrette Mukamakuza, and Hannes Werthner. Fairness and Diversity in Social-Based Recommender Systems. In _Adjunct Publication of the 28th ACM Conference on User Modeling, Adaptation and Personalization_, pages 83-88, 2020.
* [32] Preetam Nandy, Cyrus Diciccio, Divya Venugopalan, Heloise Logan, Kinjal Basu, and Noureddine El Karoui. Achieving Fairness via Post-Processing in Web-Scale Recommender Systems. In _Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency_, pages 715-725, 2022.