# UniTS: A Unified Multi-Task Time Series Model

 Shanghua Gao

Harvard University

shanghua_gao@hms.harvard.edu &Teddy Koker

MIT Lincoln Laboratory

tekoker@mit.edu &Owen Queen

Harvard University

owen_queen@hms.harvard.edu &Thomas Hartvigsen

University of Virginia

hartvigsen@virginia.edu &Theodoros Tsiligkaridis

MIT Lincoln Laboratory

ttsili@ll.mit.edu &Marinka Zitnik

Harvard University

marinka@hms.harvard.edu

###### Abstract

Although pre-trained transformers and reprogrammed text-based LLMs have shown strong performance on time series tasks, the best-performing architectures vary widely across tasks, with most models narrowly focused on specific areas, such as time series forecasting. Unifying predictive and generative time series tasks within a single model remains challenging. We introduce UniTS, a unified multi-task time series model that utilizes task tokenization to integrate predictive and generative tasks into a single framework. UniTS employs a modified transformer block to capture universal time series representations, enabling transferability from a heterogeneous, multi-domain pre-training dataset--characterized by diverse dynamic patterns, sampling rates, and temporal scales--to a wide range of downstream datasets with varied task specifications and data domains. Tested on 38 datasets across human activity sensors, healthcare, engineering, and finance, UniTS achieves superior performance compared to 12 forecasting models, 20 classification models, 18 anomaly detection models, and 16 imputation models, including adapted text-based LLMs. UniTS also demonstrates strong few-shot and prompt capabilities when applied to new domains and tasks. In single-task settings, UniTS outperforms competitive task-specialized time series models. Code and datasets are available at https://github.com/mims-harvard/UniTS.

## 1 Introduction

Foundation models, particularly large language models (LLMs), have transformed deep learning by enabling a single pre-trained model to support multiple tasks, eliminating the need for task-specific models. Language and vision models [9, 101, 92, 50, 32] can be adapted to new tasks with minimal additional training through approaches such as multi-task learning [125], few-shot learning [108, 86], and prompting [66]. Beyond language and vision, there is a growing need for similarly versatile models in time series that can accommodate data from diverse domains--including medicine [34], engineering [102], and science [48]--and support a wide range of tasks, such as forecasting, classification, imputation, and anomaly detection.

Developing multi-task time series models that unify predictive and generative tasks under a single framework remains an open challenge. Time series datasets span multiple domains and exhibit varied temporal scales, sampling rates, and dynamic patterns, making them complex to manage [124, 78].

Existing models often fall short in adaptability, as they either struggle to handle samples with varying numbers of variables [112; 67; 14] or treat each variable as independent, overlooking important interdependencies [82]. Time series tasks are also highly diverse, encompassing distinct objectives and specifications across generative and predictive tasks. For example, generative forecasting tasks aim to produce future values within a time series, while predictive tasks may involve making discrete predictions for entire samples. Additionally, task requirements can vary significantly even within the same task type; for instance, generative tasks may involve different forecast lengths, and predictive tasks may feature multiple classification categories. As a result, time series models have mainly remained task-specific, with unique architectures typically designed and trained from scratch for forecasting [67; 82; 119], classification [30; 113], or other specialized tasks [116; 112]. Recent efforts to pre-train unified models [36; 22] or adapt LLMs for time series [118; 12; 129; 47; 97; 100] still heavily depend on extensive fine-tuning or the addition of task- and dataset-specific modules. Some models have explored generative pre-training transformers specifically for time series forecasting [10; 118; 47; 28], reporting strong results but focusing exclusively on forecasting without addressing other types of time series tasks. Consequently, these approaches require users to design and train new modules for each task or limit their application to a single type of tasks. To achieve a versatile, unified time series model--akin to foundational models in vision and language that operate across unified task spaces--a model must accommodate both _generative_ and _predictive_ tasks. Such a unified model would leverage a single set of weights for multiple tasks, removing the need to develop task-specific models from scratch. This approach would support a broad range of tasks and facilitate rapid adaptation to new datasets.

**Present work.** To address these challenges, we introduce UniTS, a unified multi-task time series model capable of handling a broad spectrum of time series tasks. We rigorously compare UniTS against 12 forecasting methods, 20 classification methods, 18 anomaly detection methods, and 16 imputation methods, including transformer-based, LLM-based, RNN-based, and traditional approaches, to highlight UniTS's generalizability to new tasks. This capability is achieved through the following model design: 1) _Task tokenization:_ UniTS encodes task specifications into a unified token representation, enabling universal task specification without post-hoc architectural modifications. 2) _Unified time series architecture:_ UniTS processes heterogeneous time series data with varying numbers of variables and sequence lengths without altering its network structure. To accomplish this, UniTS employs self-attention across time and variable dimensions to adapt to diverse temporal dynamics. We introduce a dynamic linear operator to model complex relationships between data points along the time dimension and a module to reduce interference in the feature space of heterogeneous data. 3) _Support for generative and predictive tasks:_ The combination of universal task specification and a unified time series architecture allows UniTS to share weights across tasks by co-training on multiple datasets. We use a masked reconstruction pre-training approach, enabling UniTS to be jointly optimized for generative and predictive tasks.

In the single-task setting, where models are trained individually for each dataset, UniTS outperforms task-specialized time series models and repurposed LLMs across forecasting, classification, anomaly detection, and imputation. In a challenging multi-domain, multi-task setting, we find that a single shared-weight UniTS model successfully handles 38 tasks, demonstrating its versatility as a multi-task time series model. UniTS surpasses top baselines that rely on data- and task-specific modules, achieving the highest average performance across tasks and excelling in 27 out of 38 tasks. Additionally, UniTS supports prompt-based learning and direct multi-step forecasting with flexible sequence lengths, capabilities not offered by models using task- and data-specific heads. In direct multi-step forecasting, UniTS outperforms the strongest baseline (which uses a sliding-window approach) by 10.5%. UniTS can also adapt to new tasks through parameter-efficient prompting, achieving results comparable to its fully fine-tuned counterpart. For example, across 20 forecasting datasets, prompted UniTS slightly outperforms the fully fine-tuned model, reducing MAE from 0.381

Figure 1: UniTS is a unified multi-task time series model for predictive and generative tasks.

to 0.376. Furthermore, UniTS demonstrates effective few-shot transfer, successfully addressing tasks like imputation, anomaly detection, and out-of-domain forecasting and classification without requiring specialized modules. For instance, UniTS improves on the strongest baseline by 12.4% in MSE on imputation and 2.3% in F1-score on anomaly detection. UniTS paves the way toward unified time series models, offering strong performance and adaptability across tasks and domains.

## 2 Related Work

**Traditional time series modeling.** Time series analysis has been extensively explored in both the statistics and machine learning communities for many years [45; 103; 123; 18; 80]. Numerous neural architectures have been developed for specific time series tasks such as forecasting [114; 65; 68; 67; 107], classification [115; 71; 70], anomaly detection [25; 56; 16], and imputation [17; 49; 3]. Task-specific models are typically trained via supervised learning on individual datasets, necessitating specialized modules. For example, a classification model requires a classification head with a specific number of classes, while data processing modules must handle a predetermined number of variables. In contrast, UniTS aims to unify various tasks into a universal task specification, enabling the handling of diverse data with a single, unified network architecture. This approach facilitates training a multi-task model capable of addressing multiple time series tasks.

**General time series modeling.** Foundation models, including language models [9; 101] and vision models [62; 50], are trained on broad data at scale to address diverse tasks with no or minimal additional training [8]. Recent studies in time series analysis have sought to develop models with similar capabilities. This includes developing novel architectures to capture diverse time series signals. For instance, TimesNet [112] uses multiple frequency-based features obtained through Fourier transform to capture complex time series signals. There have been several efforts to reprogram LLMs for time series tasks [81; 12; 129; 47; 10]. Models such as GPT4TS [129] and Time-LLM [47] adapt LLMs by fine-tuning their embedding layers or aligning time series samples with LLM-based text prototypes (e.g., GPT-2 [89]). Unlike these models, UniTS is trained exclusively on time series data rather than relying on LLM architectures. Another approach, Lag-Llama [90], pre-trains a model on time series data from multiple domains specifically for forecasting tasks. Similarly, the Moment model [36] is pre-trained on a diverse range of time series data. However, these approaches still require task-specific modules and tuning for each task. In contrast, our UniTS model supports generative and predictive tasks without requiring extensive task-specific model adjustments.

**Prompt learning.** Prompt learning has emerged as an efficient method for task adaptation in large models [55; 88; 121; 13; 42]. Some approaches construct prompts directly in the model's input domain, such as text prompts for LLMs [2]. Other methods involve tuning soft token inputs to frozen language models [58]. In time series, PromptCast [118] and LLMTime [81] convert time series data into prompts for LLMs to facilitate forecasting. TEMPO [10] is another prompt-based approach that uses a learned set of prompts for LLM-based forecasting applications, while GPT4MTS [46] integrates both textual and numerical data to fine-tune LLMs for forecasting. In contrast, UniTS is trained exclusively on time series data, eliminating the need for computationally expensive pre-trained LLMs. Moreover, the universal task tokenization enables a frozen UniTS to adapt to new tasks beyond forecasting, such as classification and imputation. Further discussion of related work can be found in Appendix A.

## 3 Problem Formulation

**Notation.** We are given a set of multi-domain datasets \(\mathcal{D}=\{\mathcal{D}_{i}|i=1,\ldots,n\}\), where each dataset \(\mathcal{D}_{i}\) can have a varying number of time series samples; samples can be of varying time lengths and have varying numbers of sensors/variables. Each dataset is described as \(\mathcal{D}_{i}=(\mathcal{X}_{i},\mathcal{Y}_{i})\), where \(\mathcal{X}_{i}\) denotes time series samples and \(\mathcal{Y}_{i}\) specifies a task defined on \(\mathcal{X}_{i}\). Let \(\mathcal{X}\) and \(\mathcal{Y}\) be collections, defined as \(\mathcal{X}=\{\mathcal{X}_{i}|i=1,\ldots,n\}\) and \(\mathcal{Y}=\{\mathcal{Y}_{i}|i=1,\ldots,n\}\), respectively. A time series sample in datasets is denoted as \(\mathbf{x}\in\mathbb{R}^{t\times v}\), where \(t\) and \(v\) are the length of the time series sample and the number of variables, respectively. We use _time dimension_ and _variable dimension_ to indicate the row and column dimensions in \(\mathbf{x}\). \(\mathcal{Y}_{i}\) contains four common time series tasks: forecasting, classification, anomaly detection, and imputation. Further, each task type can be instantiated in numerous ways, e.g., forecasting over different time lengths and classification with varying numbers of classes. We use \(F(\mathcal{X},\theta)\) to denote a multi-task model trained on \(\mathcal{X}\). See Table 12 for notation details.

**Desiderata for a unified multi-task time series model.** Unlike specialized time series models designed and separately trained for each specific dataset \(\mathcal{D}_{i}\), a unified time series model \(F(\mathcal{X},\theta)\) is a single model with weights \(\theta\) that are shared across all types of tasks and satisfies the following three desiderata: 1) _Heterogeneous time series:_ To process time series from all sources, the model \(F\) must be agnostic with any input samples in \(\mathcal{X}\), given the heterogeneity in time series lengths \(t\) and variable counts \(v\) in time series samples \(\mathbf{x}\) from various sources. 2) _Universal task specification_: For easy multi-task support and swift adaptation to new tasks, the model \(F\) should adopt a universal task specification \(F(\mathcal{X},\theta)\rightarrow\mathcal{Y}\) applicable across all type of tasks \(\mathcal{Y}\). 3) _One shared model:_ Sharing weights \(\theta\) across tasks enables the unified model \(F\) to handle multiple tasks simultaneously. It contrasts with existing methods that typically train separate models on task-specific datasets, often involving elaborately tuned training parameters.

To realize the above desiderata, UniTS supports multi-task, prompt-based, and few-shot learning. **Multi-task learning**: UniTS specifies a single model \(F(\mathcal{X},\theta)\rightarrow\mathcal{Y}\) for tasks \(\mathcal{Y}\) defined on datasets \(\mathcal{X}\). Multi-task learning showcases the flexibility of the model to learn across time series domains and tasks. **Prompt learning**: By leveraging prompt tokens, UniTS supports prompt learning, \(\textit{Prompting}\{F(\mathcal{X},\theta),\text{token}\}\rightarrow\mathcal{Y}\), across tasks while keeping the model frozen. Additionally, UniTS can be trained in a single-task manner, following the same setup as used by many existing models. Other settings are described in Appendix C.1.

## 4 UniTS Model

UniTS is a multi-task model with a unified network architecture. It uses a token-based format to describe tasks and time series from different domains. We introduce a novel approach with three distinct token types: sample, prompt, and task tokens, each serving a unique purpose in time series analysis. The input time series sample is tokenized into sample tokens. Prompt tokens provide essential context for the task, guiding the model to accomplish the user-specified task. Task tokens (GEN and CLS) are combined with other tokens and used for generative and predictive tasks. UniTS then converts task tokens into task predictions to produce the final model output. Unlike transformers such as PatchTST [82], UniTS introduces new token types: sample tokens allow for modeling of multivariate time series, prompt tokens enable efficient multi-task and prompt learning [101], and task tokens unify predictive and generative tasks into one format.

### Prompting UniTS with Unified Time Series Data Tokens

We introduce how to use unified tokens to unify different task types and data for inference. Tokens on different network layers have the same shape, so we omit the layer index for simplicity.

**Sample tokens.** We divide time series input sample \(\mathbf{x}\in\mathbb{R}^{t\times v}\) into patches along the time dimension using a non-overlapping patch size of \(k\). A linear layer projects each patch into an embedding vector of length \(d\), obtaining sample tokens \(\mathbf{z_{x}}\in\mathbb{R}^{s\times v\times d}\), where \(s=t/k\). Since \(v\) and \(s\) vary across time

Figure 2: **a) UniTS for forecasting; input is tokenized, and GEN tokens are un-patchified to infer the forecast horizon. b) UniTS for classification; a CLS token is used to represent class information and then compared to class tokens to get prediction class. c) Architecture of UniTS model.**series data domains, we keep the variable and time dimension in tokens. \(\mathbf{z_{x}}\) are then added with learnable positional embeddings.

**Prompt tokens.** Prompt tokens \(\mathbf{z}_{p}\in\mathbb{R}^{p\times v\times d}\) are defined as learnable embeddings, where \(p\) is the number of tokens. In a multi-task setting, each dataset has its own set of prompt tokens. These tokens incorporate the specific context related to the data and the task the model needs to complete. For each sample in the dataset, these prompt tokens are appended to the sample tokens and sent to the network to provide context information about the current sample. For prompt learning, with the pre-trained model weights being frozen, UniTS adapts to new tasks by utilizing prompt tokens learned with the prompt tuning. Prompt learning is more efficient than tuning new data/task-specific heads and achieves comparable performance to full model fine-tuning, as shown by few-shot learning experiments on new tasks (Tables 4 and 5) and new datasets (Table 3).

**Task tokens.** In Figure 1(b), we categorize task tokens into two types: 1) GEN (Generation) tokens used in forecasting, imputation, and anomaly detection, and 2) CLS (Classification) tokens, which are used for classification tasks (in a given task, the number of CLS tokens corresponds to the number of classes in the task). Task tokens define a general format for representing tasks and support flexible adaptation to new tasks. For tasks involving forecasting, in Figure 1(a), the GEN token \(\mathbf{z}_{m}\in\mathbb{R}^{1\times v\times d}\), is replicated \(f\)-times based on desired forecasting length to get \(\mathbf{\hat{z}}_{m}\in\mathbb{R}^{f\times v\times d}\). These tokens \(\mathbf{\hat{z}}_{m}\) are then concatenated with the sample and prompt tokens and fed into the UniTS network:

\[\mathbf{z_{\text{Fore}}}=\text{CA}(\mathbf{z}_{p},\mathbf{z_{x}},\mathbf{\hat {z}}_{m})\in\mathbb{R}^{(p+s+f)\times v\times d},\] (1)

where CA is the concatenation operation along the time dimension. At the output of the model, embedding vectors with length \(d\) in \(\mathbf{\hat{z}}_{m}\) are unpatchified to patches with size \(e\) to obtain the forecasting sample \(\mathbf{\hat{x}}\), i.e. \(\mathbf{\hat{x}}=\text{Proj}(\mathbf{\hat{z}}_{m})\in\mathbb{R}^{(f\times e) \times v}\). This approach allows the UniTS model to perform direct multi-step forecasting [99, 76, 119] over arbitrary time lengths, as illustrated in Figure 3. For classification, in Figure 1(b), CLS token \(\mathbf{z}_{c}\in\mathbb{R}^{1\times v\times d}\) is concatenated along the time dimension with the prompt and sample tokens, resulting in:

\[\mathbf{z_{\text{Pred}}}=\text{CA}(\mathbf{z}_{p},\mathbf{z_{x}},\mathbf{z}_{ c})\in\mathbb{R}^{(p+s+1)\times v\times d},\] (2)

which is then fed into the model. We define class embeddings \(\mathbf{z}_{e}\in\mathbb{R}^{e\times v\times d}\) for each of \(e\) classes in the task. These class embeddings are either trained or generated by averaging CLS tokens of training samples in each class. Finally, the class for sample \(\mathbf{x}\) is predicted by finding the class embedding vector in \(\mathbf{z}_{e}\) that is the closest to the CLS token \(\mathbf{z}_{c}\) from the model output:

\[\text{Class}=\operatorname*{argmin}_{i}\left|\left|\mathbf{z}_{c}-\mathbf{z}_{ e_{i}}\right|\right|^{2},i\in[0,e).\] (3)

For imputation, missing values are imputed using the GEN tokens. For anomaly detection, the model takes a time series sample containing any number of potentially anomalous values, generates the output sample by reading out the sample tokens, and then determines anomalous values based on the reconstruction error between the input sample and the generated sample. Details on using tokens for imputation and anomaly detection are in Appendix C.2. All tokens and embeddings are trained to achieve their functions.

### Unified Network Architecture in UniTS

Time series samples can have varying numbers of variables, temporal dynamics, and time lengths across different domains and types of tasks. UniTS uses a modified transformer architecture [104] to handle heterogeneous multi-domain data with varying dynamics and the number of variables (Figure 1(c)). In the following, we describe key modules of UniTS architecture. Note that UniTS can also be used with other backbones, such as Mamba [38].

**Time and variable self-attention.** We use a two-way self-attention to both variable and time dimensions. This approach contrasts with previous methods that apply self-attention to either time [82] or variable dimension [67], but not to both dimensions. Time and variable self-attention effectively handle time series samples with various numbers of variables \(v\) and different time lengths \(t\).

**DyLinear.** We modify the transformer block by adding a dynamic operator (DyLinear) into the feed-forward network layer (FFN). This modification enables the FFN to capture dependencies between tokens. In contrast to the standard FFN, which processes embedding vectors on a point-wise basis, DyLinear uses weight interpolation to accommodate varying time lengths. Given a sequence of sample tokens \(\mathbf{z}_{t}\in\mathbb{R}^{l_{\text{in}}\times d}\), DyLinear interpolates weights \(\mathbf{w}\in\mathbb{R}^{w_{\text{out}}\times w_{\text{in}}}\) to accommodate varying time lengths as follows:

\[\text{DyLinear}(\mathbf{z}_{t},\mathbf{w})=\mathbf{W}_{\text{Interp}}\mathbf{z }_{t}\ \in\mathbb{R}^{l_{\text{in}}\times d};\mathbf{W}_{\text{Interp}}=\text{Interp}( \mathbf{w})\in\mathbb{R}^{l_{\text{in}}\times l_{\text{in}}},\] (4)

where Interp is a bi-linear interpolation to resize \(\mathbf{w}\) from shape \(w_{\text{out}}\times w_{\text{in}}\) to \(l_{\text{out}}\times l_{\text{in}}\) to match the input and output length. DyLinear captures dependency patterns across time series samples, which leads to improved performance on generative tasks (Table 23).

**Gating module.** We add a gating module after each layer to mitigate interference in the latent representation space caused by multi-domain and multi-task datasets (Figure 2). This module dynamically re-scales features in layer-wise latent spaces and promotes the stability of latent representations.

**Generative and predictive towers.** We design a shared GEN tower (\(H_{\text{GEN}}\)) and CLS tower (\(H_{\text{CLS}}\)) for transferring GEN/CLS tokens to generate time series samples and classification classes, as introduced in Section 4.1. Unlike existing works that use standalone, task-specific heads for individual datasets, our approach leverages GEN tower and CLS tower for all generative and predictive tasks, respectively, ensuring a more unified and efficient model architecture.

The UniTS architecture includes the backbone network composed of \(N\) modified transformer blocks described above, a CLS tower, and a GEN tower. Implementation details are in Appendix C.3. Ablations in Appendix F verify the effectiveness of this architecture.

### UniTS Model Training

**Unified masked reconstruction pre-training.** To enhance UniTS's abilities to 1) learn general features applicable to both generative and predictive tasks and 2) efficiently adapt to downstream tasks via prompt learning, we introduce a unified mask reconstruction pre-training scheme. It leverages the semantics of both prompt and CLS tokens (Section 4.1) for masked reconstruction pre-training, therefore learning representations for both generative and predictive capabilities. This is distinct from pre-training strategies that use either generative [82; 120; 26; 54] or predictive [72; 109; 117; 29; 87] approach. Unlike these approaches that pre-train only the model backbone, our strategy pre-trains all components of UniTS, including the backbone and GEN/CLS towers (Section 4.2), enabling prompt and zero-shot learning over a frozen pre-trained model. For each time-series sample \(\mathbf{x}\), a handful of sample tokens get masked and replaced with GEN tokens. These masked sample tokens is then concatenated with prompt tokens and CLS tokens, sent to the UniTS backbone network. In the unified pre-training loss, tokens from the backbone network output are sent to the CLS/GEN towers to reconstruct the input sample \(\mathbf{x}\), formulating as follows:

\[L_{\text{pretrain}}=L_{\text{MSE}}(H_{\text{GEN}}(\mathbf{z}_{p},\mathbf{z}_{ \mathbf{x}}),\mathbf{x})+L_{\text{MSE}}(\hat{H}_{\text{GEN}}(H_{\text{CLS}}( \mathbf{z}_{\text{pred}}),\ \mathbf{z}_{\mathbf{x}}),\ \mathbf{x}).\] (5)

\(L_{\text{MSE}}\) is the MSE loss to predict the full sample \(\mathbf{x}\). For the left side of the loss, prompt token \(\mathbf{z}_{p}\) is sent along with sample token \(\mathbf{z}_{\mathbf{x}}\) to GEN tower \(H_{\text{GEN}}\) to help with the reconstruction. For the right side of the loss, to leverage the semantics of the CLS token and train the CLS tower \(H_{\text{CLS}}\) for predictive tasks, \(\mathbf{z}_{\text{Pred}}\) (Eq. 2) from the model output is processed by the CLS tower \(H_{\text{CLS}}\) to get classification-related embedding vectors \(\mathbf{\hat{z}}_{\text{pred}}=H_{\text{CLS}}(\mathbf{z}_{\text{pred}})\), and another GEN tower \(\hat{H}_{\text{GEN}}\) takes in \(\mathbf{\hat{z}}_{\text{Pred}}\) and \(\mathbf{z}_{\mathbf{x}}\) to predict the full sample. \(\hat{H}_{\text{GEN}}\) is only used for pre-training and will be removed for downstream tasks. This unified pre-training strategy involves pre-training both tokens, the backbone network, and the GEN/CLS towers for both generative and predictive abilities.

**Training UniTS models.** We implement and evaluate two UniTS models, each trained in a different regime. We start with a pre-trained UniTS that is optimized using self-supervised \(L_{\text{pretrain}}\) in Eq. 5 and trained across a collection of multi-domain datasets. Given a self-supervised pre-trained UniTS whose weights are frozen, we consider a fine-tuned model where only tokens for predictive or generative tasks are fine-tuned (denoted as UniTS-_PMT_ in Experiments). We also consider a standard multi-task supervised learning regime, where a single UniTS model is trained from scratch to simultaneously perform many tasks (denoted as UniTS-_SUP_ in Experiments). These two regimes use a multi-task setup, where a single model is trained and tested on multiple tasks and datasets. During multi-task training, we sample batches of time series samples and aggregate dataset-centric loss values: \(L_{\text{total}}=\sum_{i=1}^{I}\lambda_{i}L_{i}(D_{i})\), where \(L_{i}\) is the loss of batch \(i\), \(\lambda_{i}\) is the weight for each loss, and \(I\) denotes the number of batches. We follow [112] and use the MSE loss for forecasting and cross-entropy loss for classification. For fair comparison with models trained in a single-task manner, we follow the experimental setup of [112; 67] and benchmark UniTS in a single-task setting (denoted as UniTS-_ST_ in Experiments), where the model is trained separately on each dataset/task.

[MISSING_PAGE_FAIL:7]

"\(p\)" is forecasting length. "Class./Num." denotes the "number of classes in each task"/"number of datasets".

**Results.** Table 1 shows the single-task performance for four types of tasks. On forecasting tasks with forecasting lengths of 92, 196, 336, and 720, compared with 11 forecasting methods, UniTS-_ST_ achieves the best results on 28 out of 32 datasets for MSE and 27 out of 32 for MAE, surpassing the previous best method, iTransformer, by a clear margin. In Table 34, we demonstrate that UniTS-_ST_ outperforms the concurrent MOMENT [36] model, which was trained on a large and diverse collection of time series data. Additionally, UniTS-_ST_ achieves stronger performance than LLM-reprogrammed methods that are pre-trained with extensive natural language data, e.g. GPT4TS [129], TEST [97], LLM4TS [12], and TEMPO [10]. On 10 classification datasets, UniTS-_ST_ outperforms 19 classification methods on the average accuracy, such as the transformer/MLP/frequency-based methods. It has a gain of 1.4% compared to the previous best TimesNet model. On 5 anomaly detection datasets, UniTS-_ST_ has a clear gain of 3.95% in F1 score compared to the TimesNet and also beat other 15 anomaly detection methods, such as Anomaly Transformer [116]. On 16 imputation datasets with a mask ratio of 12.5%, 25%, 37.5%, UniTS-_ST_ has the best results on all datasets in terms of MSE and MAE, outperforming 14 baseline methods. UniTS-_ST_ has the SoTA performance on these single-task benchmarks, showing its effectiveness.

### Benchmarking UniTS for Multi-Task Learning

**Setup.** In a multi-task setting, we benchmark a single UniTS model co-trained and evaluated on 38 datasets, comprising 20 forecasting tasks and 18 classification tasks, with variations in the number of variables/sensors, classification classes, and forecasting lengths. We consider two variants of UniTS; the fully supervised UniTS-_SUP_ and the more challenging UniTS-_PMT_ with prompting, as introduced in Section 4.3. Baselines use the same fully supervised multi-task training as our approach but cannot handle differences across data types and task specifications with a single model. To benchmark them, a shared backbone is used for all tasks, augmented by data-specific input modules and task-specific output modules.

**Results: Model benchmarking.** Table 2 shows multi-task learning performance. UniTS consistently outperforms baseline methods, achieving the best results in 17 out of 20 forecasting tasks (MSE) and 10 out of 18 classification tasks (accuracy). Performance gains are especially remarkable because UniTS has one fully shared model, whereas all existing methods require task or dataset-specific modules. We find that baseline methods encounter difficulties performing well across different types of tasks. For example, TimesNet, which excels in classification tasks, underperforms in forecasting tasks. Conversely, iTransformer, the top-performing forecaster, struggles with classification tasks. In contrast, the UniTS model exhibits robust performance across classification and forecasting. On forecasting, UniTS-_SUP_ surpasses the leading baseline, iTransformer, by 5.8% (0.439 vs. 0.466) in MSE and 3.3% (0.381 vs. 0.394) in MAE. On classification, UniTS-_SUP_ has an average gain of 0.7% accuracy (81.6% vs. 80.9%) over the strongest baseline (TimesNet). UniTS shows promising potential to unify data and task diversity across time series domains.

\begin{table}
\begin{tabular}{l|c c c c c c c c c c c c c c c c c c c c c} \hline \multicolumn{13}{c}{**Multi-task**} & \multicolumn{13}{c}{**UNITS-_SUP_} & \multicolumn{13}{c}{**UNITS-_PMT_} & \multicolumn{13}{c}{**UNITS-_PMT_} & \multicolumn{13}{c}{**UNITS-_PMT_} & \multicolumn{13}{c}{**UNITS-_PMT_} & \multicolumn{13}{c}{**UNITS-_PMT_} & \multicolumn{13}{c}{**UNITS-_PMT_} \\ \multicolumn{13}{c}{**POMENT**} & \multicolumn{13}{c}{**UnITS**} & \multicolumn{13}{c}{**UNITS**} & \multicolumn{13}{c}{**UNITS**} & \multicolumn{13}{c}{**UNITS**} & \multicolumn{13}{c}{**UNITS**} & \multicolumn{13}{c}{**UNITS**} & \multicolumn{13}{c}{**UNITS**} & \multicolumn{13}{c}{**UNITS**} & \multicolumn{13}{c}{**UNITS**} \\ \hline NN5 & **All** & 589 & 62.6 & 67.1 & 35.2 & 48.6 & 52.4 & 65.4 & 65.4 & 65.8 & 10.7 & 11.1 & 53.0 & 54.5 & **All** & **UnITS** & **UnITS** & **UnITS** & **UnITS** & **UnITS** & **UnITS** \\ \hline PC\({}_{11}\) & 21 & 21 & 23 & 23 & 23 & 23 & 23 & 23 & 23 & 23 & 23 & 23 & 23 & 39 & 40 & 46 & 35 & 24 & 20 & 23 &Recent research has adapted pre-trained LLMs to time series [47; 12; 129; 37]. Most approaches [47; 12; 129], such as GPT4TS, incorporate additional task-specific modules to align the modalities of time series and natural language. We compare UniTS with GPT4TS that reprograms pre-trained GPT-2 model [89]. Despite the substantial data amount and model scale gap, e.g., GPT4TS is 48\(\times\) larger than UniTS-_SUP_ (164.5dN vs. 3.4M), UniTS-_SUP_ still compares favorably to GPT4TS. On forecasting tasks, UniTS-_SUP_ even outperforms GPT4TS by 2.2% (0.439 vs. 0.449; MSE).

**Results: Prompting is competitive with supervised training.** Using tokens to prompt a frozen UniTS, the SSL-pre-trained UniTS achieves performance comparable to its fully supervised counterpart (Table 2). UniTS-_PMT_ even outperforms the supervised model in forecasting, with a lower MAE score (0.379 vs. 0.381), highlighting the effectiveness of prompt learning in UniTS. Furthermore, prompt learning with UniTS surpasses the performance of supervised baseline methods with separate modules. This indicates that the SSL-pre-trained model captures valuable time series representations and that prompt learning allows the model to efficiently adapt to target tasks.

### UniTS for Direct Multi-Step Forecasting

**Setup.** Direct multi-step forecasting predicts across varying time horizons by adjusting from the original trained length, with offsets ranging from 0 to 384. We use 14 out of 20 forecasting datasets with varying lengths. UniTS achieves this flexibility by repeating the GEN token, as described in Section 4.1, a capability not supported by existing methods. For comparison with baseline models, we implement a sliding-window approach for forecasting. In this method, predictions are made over a fixed window size, which then shifts forward incrementally to cover progressively extended time horizons. This sliding mechanism allows us to adapt the model to forecast over new, unseen time periods while maintaining consistency with the evaluation setup used by baseline methods.

**Results: Direct multi-step inference outperforms sliding window approach.** In Figure 3, UniTS demonstrates improved performance over baseline models across various forecasting lengths when using the sliding-window approach. For example, in the longest forecasting extension of +384, UniTS outperforms the iTransformer by 8.7% in MSE, achieving a score of 0.451 compared to 0.494. When using direct multi-step inference, UniTS gains an even larger advantage over the iTransformer, reducing MSE by 10.5% (0.442 vs. 0.494). This approach also reduces the average number of inference steps from 3.66 to 1, resulting in a 3x speedup.

### UniTS for Few-Shot Learning on New Datasets and Tasks

For transfer learning on new tasks and datasets, we load the model weights pre-trained on 38 datasets and apply them in a multi-task setting. We evaluate two approaches: the fully fine-tuned UniTS-_FT_ model and the prompted UniTS-_PMT_ model, in which task-specific tokens are trained.

**Setup: Few-shot classification and forecasting.** Pre-trained models, undergo fine-tuning using 5%, 15%, and 20% of the 11 training set shown in Table 8. Average performance is reported.

**Results.** UniTS achieves superior performance compared to iTransformer across all training data ratios (Table 3). At the 20% data ratio, UniTS-_FT_ achieves a gain of 8.8% in classification accuracy and a reduction of 5.7% in forecasting MSE. UniTS-_PMT_ surpasses the fully supervised iTrans

Figure 3: Direct multi-step forecasting on new lengths. UniTS achieves any new forecasting length with unified direct multi-step inference. Baseline methods use the sliding windows inference as they do not support direct multi-step inference.

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline Model & Ratio & Acc\(\uparrow\) & MSE\(\downarrow\) & MAE\(\downarrow\) & Best Count & Shared \\ \hline iTransformer-_FT_ & 5\% & 56.4 & 0.598 & 0.487 & 1/24 & \(\times\) \\ UniTS-_PMT_ & 5\% & 55.7 & **0.508** & **0.440** & 16/24 & \(\checkmark\) \\ UniTS-_FT_ & 5\% & **57.4** & 0.530 & 0.448 & 7/24 & \(\checkmark\) \\ \hline iTransformer-_FT_ & 15\% & 56.5 & 0.524 & 0.447 & 4/24 & \(\times\) \\ UniTS-_PMT_ & 15\% & 59.5 & 0.496 & 0.435 & 4/24 & \(\checkmark\) \\ UniTS-_FT_ & 15\% & **61.8** & **0.487** & **0.428** & 16/24 & \(\checkmark\) \\ \hline iTransformer-_FT_ & 20\% & 59.9 & 0.510 & 0.438 & 4/24 & \(\times\) \\ UniTS-_PMT_ & 20\% & 63.6 & 0.494 & 0.435 & 3/24 & \(\checkmark\) \\ UniTS-_FT_ & 20\% & **65.2** & **0.481** & **0.425** & 17/24 & \(\checkmark\) \\ \hline \hline \end{tabular}
\end{table}
Table 3: Few-shot multi-task learning on 9 forecasting and 6 classification tasks on out-of-domain datasets. Ratio is the data ratio of the dataset used for training. Full results in Table 29.

former, leading to 6.2% increase in classification accuracy and 3.1% decrease in forecasting MSE. When trained under a 5% data ratio,UniTS-_PMT_ exceeds UniTS-_FT_ performance for forecasting, suggesting that prompt learning is effective for transfer learning when training data is scarce.

**Setup: Few-shot imputation.** Models are fine-tuned with 10% of 6 imputation training data listed in Table 10, asked to impute 25% and 50% of missing data points.

**Results.** A unified UniTS-_FT_ outperforms models that use separate task-specific modules (Table 4), indicating that UniTS has robust few-shot imputation performance. Specifically, on a 25% masking ratio, UniTS-_FT_ exceeds the top-performing baseline iTransformer by 12.4% in MSE and 7.9% in MAE. The margin remains notable at a 50% masking ratio, where UniTS-_FT_ surpasses iTransformer by 8.8% in MSE and 6.8% in MAE. UniTS-_PMT_, the fixed model with appropriate prompt tokens, outperforms all baseline methods and achieves results comparable to its fully fine-tuned counterpart, suggesting that prompting can adapt UniTS for imputation.

**Setup: Few-shot anomaly detection.** The pre-trained models have been fine-tuned using 5% of five training datasets as listed in Table 10. The average F1-score is used as the metric.

**Results.** UniTS outperforms the top-performing baseline (PathTST) across all metrics (Table 5). UniTS-_FT_ achieves an F1-score of 86.3 compared to PathTST's F1-score of 84.3. UniTS-_PMT_ also outperforms specialized models (Anomaly Transformer) trained from scratch.

**Additional results and ablations.** Zero-shot learning is significantly more challenging than few-shot learning. Our work primarily focuses on few-shot learning, with some initial exploration of zero-shot learning for forecasting tasks of UniTS on new datasets in Appendix G. Additional analysis and ablation results are in Appendix F and Appendix E.

## 6 Conclusion

We have developed UniTS, a unified model for time series that uses a universal specification of time series tasks. UniTS handles multi-domain time series data with heterogeneous representations, outperforming task-specific models and reprogrammed LLMs on 38 multi-domain and multi-task datasets. UniTS also shows strong few-shot and prompt-based performance and can generalize to new domains and tasks. The unified token scheme in UniTS allows it to represent data and tasks in a general manner. UniTS uses a transformer architecture, and we plan to explore other types of backbones, such MLP-based blocks [107; 14] and Mamba [38], to further enhance UniTS. Limitations and future directions are discussed in Appendix M.

## Acknowledgments

S.G., O.Q., and M.Z. gratefully acknowledge the support of NIH R01-HD108794, NSF CAREER 2339524, US DoD FA8702-15-D-0001, awards from Harvard Data Science Initiative, Amazon Faculty Research, Google Research Scholar Program, AstraZeneca Research, Roche Alliance with Distinguished Scientists, Sanofi iDEA-iTECH, Pfizer Research, Chan Zuckerberg Initiative, John and Virginia Kaneb Fellowship at Harvard Medical School, Biswas Computational Biology Initiative in partnership with the Miklen Institute, Harvard Medical School Dean's Innovation Fund for the Use of Artificial Intelligence, and Kempner Institute for the Study of Natural and Artificial Intelligence at Harvard University. T.H. acknowledges the support of the National Security Data & Policy Institute, Contracting Activity 2024-24070100001. Any opinions, findings, conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the funders.

\begin{table}
\begin{tabular}{l c c c c c c c c c} \hline \hline \multirow{2}{*}{**Inputs (MSE)**} & \multirow{2}{*}{Ratio} & \multirow{2}{*}{ECL} & \multicolumn{2}{c}{ETH1} & \multicolumn{2}{c}{ETH2} & \multicolumn{2}{c}{ETH2} & \multicolumn{2}{c}{Wearching} & \multicolumn{1}{c}{Age} & \multicolumn{1}{c}{Bust Shared} \\ \hline \multirow{3}{*}{**TimeNet-FT**} & 25\% & 0.245 & 0.246 & 0.249 & 0.193 & 0.442 & 0.110 & 0.106 & 0.246 & 0.08 \\  & & & & & & & & & & \\ \hline \multirow{3}{*}{**PatchST-FT**} & 25\% & 0.195 & 0.135 & 0.142 & 0.309 & 0.092 & 0.089 & 0.190 & 0.08 \\  & & & & & & & & & \\ \hline \multirow{3}{*}{**PatchST-FT**} & 25\% & 0.179 & 0.353 & 0.142 & 0.309 & 0.092 & 0.089 & 0.190 & 0.08 \\  & & & & & & & & & \\ \hline \multirow{3}{*}{**PatchST-FT**} & 25\% & 0.174 & 0.351 & 0.185 & 0.142 & 0.111 & 0.105 & 0.246 & 0.08 \\  & & & & & & & & & \\ \hline \multirow{3}{*}{**UniTS-_PMT_} & 25\% & **0.117** & 0.281 & **0.177** & 0.247 & 0.095 & 0.075 & 0.165 & 2.08 \\  & & & & & & & & & \\ \hline \multirow{3}{*}{**UNiTS-_PT_} & 25\% & **0.112** & 0.281 & **0.177** & 0.247 & 0.095 & 0.075 & 0.165 & 2.08 \\  & & & & & & & & & \\ \hline \multirow{3}{*}{**UNiTS-_PT_} & 25\% & **0.112** & 0.352 & **0.275** & 0.133 & 0.199 & **0.062** & **0.206** & **0.165** \\  & & & & & & & & & \\ \hline \end{tabular}
\end{table}
Table 4: Few-shot multi-task learning for block-wise imputation on 6 datasets. Full results are in Table 28.

\begin{table}
\begin{tabular}{l c c c c c c c} \hline \hline \multirow{2}{*}{**Anomaly (F1T)**} & \multirow{2}{*}{MSL} & \multirow{2}{*}{PSM} & \multirow{2}{*}{SMAP} & \multirow{2}{*}{SDW} & \multirow{2}{*}{AVg} & \multirow{2}{*}{Bust Shared} \\ \hline \multirow{3}{*}{**Anomaly Trans.**} & 78.0 & 90.2 & 68.3 & 77.8 & 81.5 & 79.2 & 0.05 & \(\times\) \\  & & & & & & & & \\ \hline \multirow{3}{*}{**TimeNet-FT**} & 33.9 & 91.0 & 68.5 & 84.0 & **93.4** & 74.2 & 1.15 & \(\times\) \\  & & & & & & & & \\ \hline \multirow{3}{*}{**PatchST-FT**} & 78.0 & 96.6 & 67.2 & 82.4 & 89.0 & 83.1 & \(\times\) & \(\times\) \\  & & & & & & & & \\ \hline \multirow{3}{*}{**UNiTS-_PT_} & 79.9 & 96.6 & 68.7 & 83.8 & 92.6 & 84.3 & 0.05 & \(\times\) \\ \cline{2-2}  & & & & & & & & \\ \hline \multirow{3}{*}{**UNiTS-_PT_} & 75.4 & 95.5 & 65.8 & 82.3 & 92.5 & 82.3 & 0.05 & \(\checkDISTRIBUTION STATEMENT: Approved for public release. Distribution is unlimited. This material is based upon work supported by the Under Secretary of Defense for Research and Engineering under Air Force Contract No. FA8702-15-D-0001. Any opinions, findings, conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the Under Secretary of Defense for Research and Engineering.

## References

* Abdulaal et al. [2021] Ahmed Abdulaal, Zhuanghua Liu, and Tomer Lancewicki. Practical approach to asynchronous multivariate time series anomaly detection and localization. In _Proceedings of the 27th ACM SIGKDD conference on knowledge discovery & data mining_, pages 2485-2494, 2021.
* Arora et al. [2023] Simran Arora, Avanika Narayan, Mayee F Chen, Laurel Orr, Neel Guha, Kush Bhatia, Ines Chami, and Christopher Re. Ask me anything: A simple strategy for prompting language models. In _The Eleventh International Conference on Learning Representations_, 2023.
* Ashok et al. [2024] Arjun Ashok, Etienne Marcotte, Valentina Zantedeschi, Nicolas Chapados, and Alexandre Drouin. Tactis-2: Better, faster, simpler attentional copulas for multivariate time series. In _International conference on learning representations_, 2024.
* Bagnall et al. [2018] Anthony Bagnall, Hoang Anh Dau, Jason Lines, Michael Flynn, James Large, Aaron Bostrom, Paul Southam, and Eamonn Keogh. The uea multivariate time series classification archive, 2018. _arXiv preprint arXiv:1811.00075_, 2018.
* Bedda and Hammami [2010] Mouldi Bedda and Nacereddine Hammami. Spoken Arabic Digit. UCI Machine Learning Repository, 2010. DOI: https://doi.org/10.24432/C52C9Q.
* Berndt and Clifford [1994] Donald J. Berndt and James Clifford. Using dynamic time warping to find patterns in time series. In _KDD Workshop_, 1994.
* Birbaumer et al. [1999] Niels Birbaumer, Nimr Ghanayim, Thilo Hinterberger, Iver Iversen, Boris Kotchoubey, Andrea Kubler, Juri Perelmouter, Edward Taub, and Herta Flor. A spelling device for the paralysed. _Nature_, 398(6725):297-298, 1999.
* Bommasani et al. [2021] Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportunities and risks of foundation models. _arXiv preprint arXiv:2108.07258_, 2021.
* Brown et al. [2020] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. _Advances in neural information processing systems_, 33:1877-1901, 2020.
* Cao et al. [2024] Defu Cao, Furong Jia, Sercan O Arik, Tomas Pfister, Yixiang Zheng, Wen Ye, and Yan Liu. TEMPO: Prompt-based generative pre-trained transformer for time series forecasting. In _The Twelfth International Conference on Learning Representations_, 2024.
* Illness [2023] CDC. Illness.
* Chang et al. [2023] Ching Chang, Wen-Chih Peng, and Tien-Fu Chen. Llm4ts: Two-stage fine-tuning for time-series forecasting with pre-trained llms. _arXiv preprint arXiv:2308.08469_, 2023.
* Chen et al. [2023] Guangyi Chen, Weiran Yao, Xiangchen Song, Xinyue Li, Yongming Rao, and Kun Zhang. PLOT: Prompt learning with optimal transport for vision-language models. In _The Eleventh International Conference on Learning Representations_, 2023.
* Chen et al. [2023] Si-An Chen, Chun-Liang Li, Nate Yoder, Sercan O Arik, and Tomas Pfister. Tsmixer: An all-mlp architecture for time series forecasting. _arXiv preprint arXiv:2303.06053_, 2023.
* Chen and Guestrin [2016] Tianqi Chen and Carlos Guestrin. Xgboost: A scalable tree boosting system. _KDD_, 2016.
* Chen et al. [2023] Xuanhao Chen, Liwei Deng, Yan Zhao, and Kai Zheng. Adversarial autoencoder for unsupervised time series anomaly detection and interpretation. In _Proceedings of the Sixteenth ACM International Conference on Web Search and Data Mining_, pages 267-275, 2023.

* [17] Yu Chen, Wei Deng, Shikai Fang, Fengpei Li, Nicole Tianjiao Yang, Yikai Zhang, Kashif Rasul, Shandian Zhe, Anderson Schneider, and Yuriy Nevmyvaka. Provably convergent schr\(\backslash\)" odinger bridge with applications to probabilistic time series imputation. In _International Conference on Machine Learning_, 2023.
* [18] Yuqi Chen, Kan Ren, Yansen Wang, Yuchen Fang, Weiwei Sun, and Dongsheng Li. Contiformer: Continuous-time transformer for irregular time series modeling. In _Thirty-seventh Conference on Neural Information Processing Systems_, 2023.
* [19] Kelvin Ortiz Chicaiza and Marco E Benalcazar. A brain-computer interface for controlling iot devices using eeg signals. In _2021 IEEE Fifth Ecuador Technical Chapters Meeting (ETCM)_, pages 1-6. IEEE, 2021.
* [20] Marco Cuturi. Fast global alignment kernels. In _Proceedings of the 28th international conference on machine learning (ICML-11)_, pages 929-936, 2011.
* [21] Abhimanyu Das, Weihao Kong, Andrew Leach, Rajat Sen, and Rose Yu. Long-term forecasting with tide: Time-series dense encoder. _arXiv preprint arXiv:2304.08424_, 2023.
* [22] Abhimanyu Das, Weihao Kong, Rajat Sen, and Yichen Zhou. A decoder-only foundation model for time-series forecasting. _arXiv preprint arXiv:2310.10688_, 2023.
* [23] Hoang Anh Dau, Eamonn Keogh, Kaveh Kamgar, Chin-Chia Michael Yeh, Yan Zhu, Shaghayegh Gharghabi, Chotirat Ann Ratanamahatana, Yanping, Bing Hu, Nurjahan Begum, Anthony Bagnall, Abdullah Mueen, Gustavo Batista, and Hexagon-ML. The ucr time series classification archive, October 2018. https://www.cs.ucr.edu/~eamonn/time_series_data_2018/.
* [24] Angus Dempster, Franccois Petitjean, and Geoffrey I. Webb. Rocket: exceptionally fast and accurate time series classification using random convolutional kernels. _Data Min. Knowl. Discov._, 2020.
* [25] Chaoyue Ding, Shiliang Sun, and Jing Zhao. Mst-gat: A multimodal spatial-temporal graph attention network for time series anomaly detection. _Information Fusion_, 89:527-536, 2023.
* [26] Jiaxiang Dong, Haixu Wu, Haoran Zhang, Li Zhang, Jianmin Wang, and Mingsheng Long. Simmtm: A simple pre-training framework for masked time-series modeling. _arXiv preprint arXiv:2302.00861_, 2023.
* [27] Joy O Egede, Siyang Song, Temitayo A Olugbade, Chongyang Wang, C De C Amanda, Hongying Meng, Min Aung, Nicholas D Lane, Michel Valstar, and Nadia Bianchi-Berthouze. Emopain challenge 2020: Multimodal pain evaluation from facial and bodily expressions. In _2020 15th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2020)_, pages 849-856. IEEE, 2020.
* [28] Vijay Ekambaram, Arindam Jati, Nam H Nguyen, Pankaj Dayama, Chandra Reddy, Wesley M Gifford, and Jayant Kalagnanam. Ttms: Fast multi-level tiny time mixers for improved zeroshot and few-shot forecasting of multivariate time series. _arXiv preprint arXiv:2401.03955_, 2024.
* [29] Archibald Fraikin, Adrien Bennett, and Stephanie Allassonniere. T-rep: Representation learning for time series using time-embeddings. In _The Twelfth International Conference on Learning Representations_, 2024.
* [30] Jean-Yves Franceschi, Aymeric Dieuleveut, and Martin Jaggi. Unsupervised scalable representation learning for multivariate time series. In _NeurIPS_, 2019.
* [31] Shanghua Gao, Ming-Ming Cheng, Kai Zhao, Xin-Yu Zhang, Ming-Hsuan Yang, and Philip Torr. Res2net: A new multi-scale backbone architecture. _IEEE transactions on pattern analysis and machine intelligence_, 43(2):652-662, 2019.
* [32] Shanghua Gao, Zhijie Lin, Xingyu Xie, Pan Zhou, Ming-Ming Cheng, and Shuicheng Yan. Editanything: Empowering unparalleled flexibility in image editing and generation. In _Proceedings of the 31st ACM International Conference on Multimedia_, pages 9414-9416, 2023.

* [33] Rakshitha Godahewa, Christoph Bergmeir, Geoffrey I. Webb, Rob J. Hyndman, and Pablo Montero-Mango. Monash time series forecasting archive. In _Neural Information Processing Systems Track on Datasets and Benchmarks_, 2021.
* [34] A. L. Goldberger, L. A. N. Amaral, L. Glass, J. M. Hausdorff, P. Ch. Ivanov, R. G. Mark, J. E. Mietus, G. B. Moody, C.-K. Peng, and H. E. Stanley. PhysioBank, PhysioToolkit, and PhysioNet: Components of a new research resource for complex physiologic signals. _Circulation_, 101(23):e215-e220, 2000. Circulation Electronic Pages: http://circ.ahajournals.org/content/101/23/e215.full PMID:1085218; doi:10.1161/01.CIR.101.23.e215.
* [35] Ary L Goldberger, Luis AN Amaral, Leon Glass, Jeffrey M Hausdorff, Plamen Ch Ivanov, Roger G Mark, Joseph E Mietus, George B Moody, Chung-Kang Peng, and H Eugene Stanley. Physiobank, physiotoolkit, and physionet: components of a new research resource for complex physiologic signals. _circulation_, 101(23):e215-e220, 2000.
* [36] Mononito Goswami, Konrad Szafer, Arjun Choudhry, Yifu Cai, Shuo Li, and Artur Dubrawski. Moment: A family of open time-series foundation models. _arXiv preprint arXiv:2402.03885_, 2024.
* [37] Nate Gruver, Marc Anton Finzi, Shikai Qiu, and Andrew Gordon Wilson. Large language models are zero-shot time series forecasters. In _Thirty-seventh Conference on Neural Information Processing Systems_, 2023.
* [38] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. _arXiv preprint arXiv:2312.00752_, 2023.
* [39] Albert Gu, Karan Goel, and Christopher Re. Efficiently modeling long sequences with structured state spaces. In _ICLR_, 2022.
* [40] Richard N Henson, Daniel G Wakeman, Vladimir Litvak, and Karl J Friston. A parametric empirical bayesian framework for the eeg/meg inverse problem: generative models for multi-subject and multi-modal integration. _Frontiers in human neuroscience_, 5:76, 2011.
* [41] S. Hochreiter and J. Schmidhuber. Long short-term memory. _Neural Comput._, 1997.
* [42] Qian Huang, Hongyu Ren, Peng Chen, Gregor Krizmanc, Daniel Zeng, Percy Liang, and Jure Leskovec. PRODIGY: Enabling in-context learning over graphs. In _Thirty-seventh Conference on Neural Information Processing Systems_, 2023.
* [43] Kyle Hundman, Valentino Constantinou, Christopher Laporte, Ian Colwell, and Tom Soderstrom. Detecting spacecraft anomalies using lstms and nonparametric dynamic thresholding. In _Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining_, pages 387-395, 2018.
* [44] RJ Hyndman. expsmooth: Data sets from "forecasting with exponential smoothing". _R package version_, 2, 2015.
* [45] Rob J Hyndman and George Athanasopoulos. _Forecasting: principles and practice_. OTexts, 2018.
* [46] Furong Jia, Kevin Wang, Yixiang Zheng, Defu Cao, and Yan Liu. Gpt4mts: Prompt-based large language model for multimodal time-series forecasting. In _Proceedings of the AAAI Conference on Artificial Intelligence_, pages 23343-23351, 2024.
* [47] Ming Jin, Shiyu Wang, Lintao Ma, Zhixuan Chu, James Y Zhang, Xiaoming Shi, Pin-Yu Chen, Yuxuan Liang, Yuan-Fang Li, Shirui Pan, et al. Time-llm: Time series forecasting by reprogramming large language models. _arXiv preprint arXiv:2310.01728_, 2023.
* [48] Julia Kaltenborn, Charlotte Emilie Elektra Lange, Venkatesh Ramesh, Philippe Brouillard, Yaniv Gurwicz, Chandni Nagda, Jakob Runge, Peer Nowack, and David Rolnick. Climateset: A large-scale climate model dataset for machine learning. In _Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track_, 2023.

* [49] SeungHyun Kim, Hyunsu Kim, EungGu Yun, Hwangrae Lee, Jaehun Lee, and Juho Lee. Probabilistic imputation for time-series classification with missing data. In _International Conference on Machine Learning_, pages 16654-16667. PMLR, 2023.
* [50] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. _arXiv preprint arXiv:2304.02643_, 2023.
* [51] Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. In _ICLR_, 2020.
* [52] Mineichi Kudo, Jun Toyama, and Masaru Shimbo. Multidimensional curve classification using passing-through regions. _Pattern Recognition Letters_, 20(11-13):1103-1111, 1999.
* [53] Guokun Lai, Wei-Cheng Chang, Yiming Yang, and Hanxiao Liu. Modeling long-and short-term temporal patterns with deep neural networks. In _The 41st international ACM SIGIR conference on research & development in information retrieval_, pages 95-104, 2018.
* [54] Seunghan Lee, Taeyoung Park, and Kibok Lee. Learning to embed time series patches independently. In _The Twelfth International Conference on Learning Representations_, 2024.
* [55] Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih, editors, _Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing_, pages 3045-3059, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics.
* [56] Gen Li and Jason J Jung. Deep learning for anomaly detection in multivariate time series: Approaches, applications, and challenges. _Information Fusion_, 91:93-102, 2023.
* [57] Shiyang Li, Xiaoyong Jin, Yao Xuan, Xiyou Zhou, Wenhu Chen, Yu-Xiang Wang, and Xifeng Yan. Enhancing the locality and breaking the memory bottleneck of transformer on time series forecasting. In _NeurIPS_, 2019.
* [58] Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli, editors, _Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)_, pages 4582-4597, Online, August 2021. Association for Computational Linguistics.
* [59] Zhe Li, Shiyi Qi, Yiduo Li, and Zenglin Xu. Revisiting long-term time series forecasting: An investigation on linear mapping. _arXiv preprint arXiv:2305.10721_, 2023.
* [60] Jason Lines, Anthony Bagnall, Patrick Caiger-Smith, and Simon Anderson. Classification of household devices by electricity usage profiles. In _Intelligent Data Engineering and Automated Learning-IDEAL 2011: 12th International Conference, Norwich, UK, September 7-9, 2011. Proceedings 12_, pages 403-412. Springer, 2011.
* [61] Chengyu Liu, David Springer, Qiao Li, Benjamin Moody, Ricardo Abad Juan, Francisco J Chorro, Francisco Castells, Jose Millet Roig, Ikaro Silva, Alistair EW Johnson, et al. An open access database for the evaluation of heart sound algorithms. _Physiological measurement_, 37(12):2181, 2016.
* [62] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In _Advances in neural information processing systems_, 2023.
* [63] Jiayang Liu, Lin Zhong, Jehan Wickramasuriya, and Venu Vasudevan. uwave: Accelerometer-based personalized gesture recognition and its applications. _Pervasive and Mobile Computing_, 5(6):657-675, 2009.
* [64] Minhao Liu, Ailing Zeng, Muxi Chen, Zhijian Xu, Qiuxia Lai, Lingna Ma, and Qiang Xu. Scinet: time series modeling and forecasting with sample convolution and interaction. _NeurIPS_, 2022.

* [65] Shizhan Liu, Hang Yu, Cong Liao, Jianguo Li, Weiyao Lin, Alex X Liu, and Schahram Dustdar. Pyraformer: Low-complexity pyramidal attention for long-range time series modeling and forecasting. In _International conference on learning representations_, 2021.
* [66] Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng Tam, Zhengxiao Du, Zhilin Yang, and Jie Tang. P-tuning: Prompt tuning can be comparable to fine-tuning across scales and tasks. In _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)_, pages 61-68, 2022.
* [67] Yong Liu, Tengge Hu, Haoran Zhang, Haixu Wu, Shiyu Wang, Lintao Ma, and Mingsheng Long. intramformer: Inverted transformers are effective for time series forecasting. In _International Conference on Learning Representations_, 2024.
* [68] Yong Liu, Chenyu Li, Jianmin Wang, and Mingsheng Long. Koopa: Learning non-stationary time series dynamics with koopman predictors. In _Advances in neural information processing systems_, 2023.
* [69] Yong Liu, Haixu Wu, Jianmin Wang, and Mingsheng Long. Non-stationary transformers: Rethinking the stationarity in time series forecasting. In _NeurIPS_, 2022.
* [70] Zhen Liu, Peitian Ma, Dongliang Chen, Wenbin Pei, and Qianli Ma. Scale-teaching: Robust multi-scale training for time series classification with noisy labels. In _Thirty-seventh Conference on Neural Information Processing Systems_, 2023.
* [71] Wang Lu, Jindong Wang, Xinwei Sun, Yiqiang Chen, and Xing Xie. Out-of-distribution representation learning for time series classification. In _The Eleventh International Conference on Learning Representations_, 2023.
* [72] Dongsheng Luo, Wei Cheng, Yingheng Wang, Dongkuan Xu, Jingchao Ni, Wenchao Yu, Xuchao Zhang, Yanchi Liu, Yuncong Chen, Haifeng Chen, et al. Time series contrastive learning with information-aware augmentations. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 37, pages 4534-4542, 2023.
* [73] A Ian MacLeod and Hyukjun Gweon. Optimal deseasonalization for monthly and daily geophysical time series. _Journal of Environmental Statistics_, 2012.
* [74] Maggie, Oren Anava, Vitaly Kuznetsov, and Will Cukierski. Web traffic time series forecasting, 2017.
* [75] Mohammad Malekzadeh, Richard G Clegg, Andrea Cavallaro, and Hamed Haddadi. Mobile sensor data anonymization. In _Proceedings of the international conference on internet of things design and implementation_, pages 49-58, 2019.
* [76] Massimiliano Marcellino, James H Stock, and Mark W Watson. A comparison of direct and iterated multistep ar methods for forecasting macroeconomic time series. _Journal of econometrics_, 135(1-2):499-526, 2006.
* [77] Aditya P Mathur and Nils Ole Tippenhauer. Swat: A water treatment testbed for research and training on ics security. In _2016 international workshop on cyber-physical systems for smart water networks (CySWater)_, pages 31-36. IEEE, 2016.
* [78] Mike A Merrill, Mingtian Tan, Vinayak Gupta, Tom Hartvigsen, and Tim Althoff. Language models still struggle to zero-shot reason about time series. In _Empirical Methods for Natural Language Processing_, 2024.
* [79] Matthew Middlehurst, Patrick Schafer, and Anthony Bagnall. Bake off redux: a review and experimental evaluation of recent time series classification algorithms. _arXiv preprint arXiv:2304.13029_, 2023.
* [80] Ilan Naiman, N Benjamin Erichson, Pu Ren, Michael W Mahoney, and Omri Azencot. Generative modeling of regular and irregular time series data via koopman vaes. _International conference on learning representations_, 2024.

* [81] Shikai Qiu Nate Gruver, Marc Finzi and Andrew Gordon Wilson. Large Language Models Are Zero Shot Time Series Forecasters. In _Advances in Neural Information Processing Systems_, 2023.
* [82] Yuqi Nie, Nam H. Nguyen, Phanwadee Sinthong, and Jayant Kalagnanam. A time series is worth 64 words: Long-term forecasting with transformers. In _International Conference on Learning Representations_, 2023.
* [83] NREL. Solar power data for integration studies.
* [84] Robert Thomas Olszewski. _Generalized feature extraction for structural pattern recognition in time-series data_. Carnegie Mellon University, 2001.
* [85] PeMS. Traffic.
* [86] Farhad Pourpanah, Moloud Abdar, Yuxuan Luo, Xinlei Zhou, Ran Wang, Chee Peng Lim, Xi-Zhao Wang, and QM Jonathan Wu. A review of generalized zero-shot learning methods. _IEEE transactions on pattern analysis and machine intelligence_, 2022.
* [87] Owen Queen, Thomas Hartvigsen, Teddy Koker, Huan He, Theodoros Tsiligkaridis, and Marinka Zitnik. Encoding time-series explanations through self-supervised model behavior consistency. In _Thirty-seventh Conference on Neural Information Processing Systems_, 2023.
* [88] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In _International conference on machine learning_, pages 8748-8763. PMLR, 2021.
* [89] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. 2019.
* [90] Kashif Rasul, Arjun Ashok, Andrew Robert Williams, Arian Khorasani, George Adamopoulos, Rishika Bhagwatkar, Marin Bilos, Hena Ghonia, Nadhir Vincent Hassen, Anderson Schneider, et al. Lag-llama: Towards foundation models for time series forecasting. _arXiv preprint arXiv:2310.08278_, 2023.
* [91] Umaa Rebbapragada, Pavlos Protopapas, Carla E Brodley, and Charles Alcock. Finding anomalous periodic time series: An application to catalogs of periodic variable stars. _Machine learning_, 74:281-313, 2009.
* [92] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 10684-10695, 2022.
* [93] Davide Roverso. Plant diagnostics by transient classification: The aladdin approach. _International Journal of Intelligent Systems_, 17(8):767-790, 2002.
* [94] Mohammad Shokoohi-Yekta, Bing Hu, Hongxia Jin, Jun Wang, and Eamonn Keogh. Generalizing dtw to the multi-dimensional case requires an adaptive approach. _Data mining and knowledge discovery_, 31:1-31, 2017.
* [95] Ikaro Silva, Joachim Behar, Reza Sameni, Tingting Zhu, Julien Oster, Gari D Clifford, and George B Moody. Noninvasive fetal ecg: the physionet/computing in cardiology challenge 2013. In _Computing in cardiology 2013_, pages 149-152. IEEE, 2013.
* [96] Ya Su, Youjian Zhao, Chenhao Niu, Rong Liu, Wei Sun, and Dan Pei. Robust anomaly detection for multivariate time series through stochastic recurrent neural network. In _Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining_, pages 2828-2837, 2019.
* [97] Chenxi Sun, Yaliang Li, Hongyan Li, and Shenda Hong. Test: Text prototype aligned embedding to activate llm's ability for time series. _arXiv preprint arXiv:2308.08241_, 2023.

* [98] Souhaib Ben Taieb, Gianluca Bontempi, Amir F Atiya, and Antti Sorjamaa. A review and comparison of strategies for multi-step ahead time series forecasting based on the nn5 forecasting competition. _Expert systems with applications_, 39(8):7067-7083, 2012.
* [99] Souhaib Ben Taieb, Rob J Hyndman, et al. _Recursive and direct multi-step forecasting: the best of both worlds_, volume 19. Department of Econometrics and Business Statistics, Monash Univ., 2012.
* [100] Mingtian Tan, Mike A Merrill, Vinayak Gupta, Tim Althoff, and Thomas Hartvigsen. Are language models actually useful for time series forecasting? In _Advances in Neural Information Processing Systems_, 2024.
* [101] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. _arXiv preprint arXiv:2302.13971_, 2023.
* [102] Artur Trindade. ElectricityLoadDiagrams20112014. UCI Machine Learning Repository, 2015. DOI: https://doi.org/10.24432/C58C86.
* [103] Patara Trirat, Yooju Shin, Junhyeok Kang, Youngeun Nam, Jihye Na, Minyoung Bae, Joeun Kim, Byunghyun Kim, and Jae-Gil Lee. Universal time-series representation learning: A survey. _arXiv preprint arXiv:2401.03717_, 2024.
* [104] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In _NeurIPS_, 2017.
* [105] Jose R Villar, Paula Vergara, Manuel Menendez, Enrique de la Cal, Victor M Gonzalez, and Javier Sedano. Generalized models for the classification of abnormal movements in daily life and its applicability to epilepsy convulsion recognition. _International journal of neural systems_, 26(06):1650037, 2016.
* [106] Huiqiang Wang, Jian Peng, Feihu Huang, Jince Wang, Junhui Chen, and Yifei Xiao. Micn: Multi-scale local and global context modeling for long-term series forecasting. In _The Eleventh International Conference on Learning Representations_, 2022.
* [107] Shiyu Wang, Haixu Wu, Xiaoming Shi, Tengge Hu, Huakun Luo, Lintao Ma, James Y. Zhang, and JUN ZHOU. Timemixer: Decomposable multiscale mixing for time series forecasting. In _The Twelfth International Conference on Learning Representations_, 2024.
* [108] Yaqing Wang, Quanming Yao, James T Kwok, and Lionel M Ni. Generalizing from a few examples: A survey on few-shot learning. _ACM computing surveys (csur)_, 53(3):1-34, 2020.
* [109] Yihe Wang, Yu Han, Haishuai Wang, and Xiang Zhang. Contrast everything: A hierarchical contrastive framework for medical time-series. In _Thirty-seventh Conference on Neural Information Processing Systems_, 2023.
* [110] Wetterstation. Weather.
* [111] Gerald Woo, Chenghao Liu, Doyen Sahoo, Akshat Kumar, and Steven C. H. Hoi. Etsformer: Exponential smoothing transformers for time-series forecasting. _arXiv preprint arXiv:2202.01381_, 2022.
* [112] Haixu Wu, Tengge Hu, Yong Liu, Hang Zhou, Jianmin Wang, and Mingsheng Long. Timesnet: Temporal 2d-variation modeling for general time series analysis. In _International Conference on Learning Representations_, 2023.
* [113] Haixu Wu, Jialong Wu, Jiehui Xu, Jianmin Wang, and Mingsheng Long. Flowformer: Linearizing transformers with conservation flows. In _ICML_, 2022.
* [114] Haixu Wu, Jiehui Xu, Jianmin Wang, and Mingsheng Long. Autoformer: Decomposition transformers with auto-correlation for long-term series forecasting. _Advances in Neural Information Processing Systems_, 34:22419-22430, 2021.

* [115] Qiao Xiao, Bojan Wu, Yu Zhang, Shiwei Liu, Mykola Pechenizkiy, Elena Mocanu, and Decebal Constantin Mocanu. Dynamic sparse network for time series classification: Learning what to "see". In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, _Advances in Neural Information Processing Systems_, 2022.
* [116] Jiehui Xu, Haixu Wu, Jianmin Wang, and Mingsheng Long. Anomaly transformer: Time series anomaly detection with association discrepancy. In _ICLR_, 2021.
* [117] Maxwell A Xu, Alexander Moreno, Hui Wei, Benjamin M Marlin, and James M Rehg. Retrieval-based reconstruction for time-series contrastive learning. In _The Twelfth International Conference on Learning Representations_, 2024.
* [118] Hao Xue and Flora D Salim. Promptcast: A new prompt-based learning paradigm for time series forecasting. _IEEE Transactions on Knowledge and Data Engineering_, 2023.
* [119] Ailing Zeng, Muxi Chen, Lei Zhang, and Qiang Xu. Are transformers effective for time series forecasting? In _Proceedings of the AAAI conference on artificial intelligence_, volume 37, pages 11121-11128, 2023.
* [120] George Zerveas, Srideepika Jayaraman, Dhaval Patel, Anuradha Bhamidipaty, and Carsten Eickhoff. A transformer-based framework for multivariate time series representation learning. In _Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining_, KDD '21, page 2114-2124, New York, NY, USA, 2021. Association for Computing Machinery.
* [121] Haotian Zhang, Pengchuan Zhang, Xiaowei Hu, Yen-Chun Chen, Liunian Harold Li, Xiyang Dai, Lijuan Wang, Lu Yuan, Jenq-Neng Hwang, and Jianfeng Gao. GLIPv2: Unifying localization and vision-language understanding. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, _Advances in Neural Information Processing Systems_, 2022.
* [122] T. Zhang, Yizhuo Zhang, Wei Cao, J. Bian, Xiaohan Yi, Shun Zheng, and Jian Li. Less is more: Fast multivariate time series forecasting with light sampling-oriented mlp structures. _arXiv preprint arXiv:2207.01186_, 2022.
* [123] Xiang Zhang, Marko Zeman, Theodoros Tsiligkaridis, and Marinka Zitnik. Graph-guided network for irregularly sampled multivariate time series. In _International Conference on Learning Representations, ICLR_, 2022.
* [124] Xiang Zhang, Ziyuan Zhao, Theodoros Tsiligkaridis, and Marinka Zitnik. Self-supervised contrastive pre-training for time series via time-frequency consistency. _Advances in Neural Information Processing Systems_, 2022.
* [125] Yu Zhang and Qiang Yang. A survey on multi-task learning. _IEEE Transactions on Knowledge and Data Engineering_, 34(12):5586-5609, 2021.
* [126] Yunhao Zhang and Junchi Yan. Crossformer: Transformer utilizing cross-dimension dependency for multivariate time series forecasting. _ICLR_, 2023.
* [127] Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, and Wancai Zhang. Informer: Beyond efficient transformer for long sequence time-series forecasting. In _Proceedings of the AAAI conference on artificial intelligence_, volume 35, pages 11106-11115, 2021.
* [128] Tian Zhou, Ziqing Ma, Qingsong Wen, Xue Wang, Liang Sun, and Rong Jin. Fedformer: Frequency enhanced decomposed transformer for long-term series forecasting. In _International Conference on Machine Learning_, pages 27268-27286. PMLR, 2022.
* [129] Tian Zhou, Peisong Niu, Xue Wang, Liang Sun, and Rong Jin. One fits all: Power general time series analysis by pretrained LM. In _Thirty-seventh Conference on Neural Information Processing Systems_, 2023.

Extended Related Work

**Comparison of the abilities required by a unified time series model.** We evaluate whether existing works in time series possess the necessary capabilities for constructing a unified time series model, as outlined in Table 6. Most methods fail to support these requirements. For instance, PatchTST [82] processes each variable independently, enabling it to handle multi-domain time series datasets without the need for data-specific heads. However, it still requires task-specific heads for tasks like making forecasts over a fixed length or performing classifications within a predetermined number of classes.

## Appendix B Datasets

**Dataset details.** We introduce the details of the multi-task dataset collection used by our work in Table 7. The dataset collection used for few-shot learning on classification and forecasting are listed in Table 8, the collection used for zero-shot forecasting are listed in Table 9, the collection used for imputation is listed in Table 10, and the collection used for anomaly detection is listed in Table 11. Datasets were aggregated from the Monash Forecasting Repository [33], Time Series Classification Website [79], and Time Series Library [112]. The combined training set consists of over 35 million timesteps and over 6,000 variables. For subsets of a dataset such as ETTh1, we start by splitting the data into training and testing sets based on distinct time intervals of a long time series sequence, following splits in [112]. Within these training and testing intervals, we generate samples using various sliding windows, ensuring that there is no data leakage between the training and testing sets.

**Dataset for direct multi-step forecasting on new forecasting lengths.** For evaluating zero-shot learning capabilities over new forecasting lengths, we initially consider 20 forecasting datasets utilized in the multi-task setting, as detailed in Table 7. However, to adapt to 384 additional forecasting lengths that the model was not trained on, we exclude specific datasets that are incompatible with this requirement. These datasets include NN5\({}_{P112}\), ECL\({}_{P720}\), ETTh1\({}_{P720}\), ILI\({}_{P60}\), Traffic\({}_{P720}\), and Weather\({}_{P720}\). Consequently, our analysis is conducted using 14 remaining forecasting datasets.

## Appendix C Further information on UniTS

### All learning settings supported by UniTS

UniTS incorporates multi-task, prompt, few-shot, and zero-shot learning, as well as the single-task learning same to existing methods. We introduce the multi-task and prompt learning in the manuscript, here we introduce the other settings supported by UniTS.

**Notations for zero-shot/few-shot learning.**\(\mathcal{\hat{X}}\) is an out-of-domain dataset collection not included in \(\mathcal{X}\), and \(\mathcal{\hat{Y}}\) is used to denote a new type of tasks not contained in \(\mathcal{Y}\).

**Zero-shot learning.** UniTS has zero-shot learning ability where model \(F(\mathcal{X},\theta)\) trained on all datasets in \(\mathcal{D}\) is tested on multiple types of new tasks that are not trained for, i.e. \(F(\mathcal{X},\theta)\rightarrow\mathcal{\hat{X}},\mathcal{\hat{X}}\notin \mathcal{X}\). New zero-shot learning tasks include direct multi-step forecasting with a new length and forecasting on out-of-domain datasets with a new number of variables. Zero-shot learning shows the adaptability of UniTS to different time series tasks.

\begin{table}
\begin{tabular}{l c c c} \hline \hline Method & Multi-domain time series & Universal task specification & One model \\ \hline TimesNet [112] & \(\times\) & \(\times\) & \(\times\) \\ PatchTST [82] & \(\check{\check{\vee}}\) & \(\times\) & \(\times\) \\ iTransferner [67] & \(\times\) & \(\times\) & \(\times\) \\ Dlinear [119] & \(\times\) & \(\times\) & \(\times\) \\ FEDForenter [128] & \(\times\) & \(\times\) & \(\times\) \\ MICN [106] & \(\times\) & \(\times\) & \(\times\) \\ Pyraformer [65] & \(\times\) & \(\times\) & \(\times\) \\ Autoformer [114] & \(\times\) & \(\times\) & \(\times\) \\
**UniTS** & \(\check{\check{\vee}}\) & \(\check{\check{\vee}}\) & \(\check{\check{\vee}}\) \\ \hline \hline \end{tabular}
\end{table}
Table 6: Key features of a unified multi-task time series model include the capability to handle heterogeneous time series samples with different numbers of variables and time lengths. Additionally, it should support both generative and predictive time series tasks within the same model.

**Few-shot learning.** UniTS model \(F(\mathcal{X},\theta)\) pre-trained on \(\mathcal{X}\), can be fine-tuned on a few samples on new data \(\hat{\mathcal{X}}\) and new tasks \(\hat{\mathcal{Y}}\), i.e., \(\textit{Few-Shot}\{F(\mathcal{X},\theta),\hat{\mathcal{X}}\}=F(\hat{\mathcal{X} },\hat{\theta})\rightarrow\hat{\mathcal{Y}}\). We verify the few-shot learning ability of UniTS on forecasting and classification tasks on new, out-of-domain datasets and on new types of tasks, including imputation and anomaly detection.

**Single-task learning.** UniTS model can also conduct the single-task learning same as the existing works, where each model is separately trained on each dataset \(\mathcal{D}_{i}=(\mathcal{X}_{i},\mathcal{Y}_{i})\), i.e., \(F(\mathcal{X}_{i},\theta_{i})\rightarrow\mathcal{Y}_{i}\).

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline Name & Train Size & Sequence Length & Variables & Task & Class \\ \hline NN5\({}_{P112}\)[98] & 409 & 112 & 111 & Forecast (112) & Finance \\ ECL\({}_{P96}\)[102] & 18221 & 96 & 321 & Forecast (96) & Electricity \\ ECL\({}_{P192}\)[102] & 18125 & 96 & 321 & Forecast (92) & Electricity \\ ECL\({}_{P336}\)[102] & 17981 & 96 & 321 & Forecast (336) & Electricity \\ ECL\({}_{P202}\)[102] & 17597 & 96 & 321 & Forecast (720) & Electricity \\ ETH\({}_{P36}\)[127] & 8449 & 96 & 7 & Forecast (96) & Electricity \\ ETH\({}_{P192}\)[127] & 8353 & 96 & 7 & Forecast (192) & Electricity \\ ETH\({}_{P336}\)[127] & 8209 & 96 & 7 & Forecast (336) & Electricity \\ ETH\({}_{P202}\)[127] & 7825 & 96 & 7 & Forecast (720) & Electricity \\ Exchange\({}_{P192}\)[53] & 5024 & 96 & 8 & Forecast (192) & Finance \\ Exchange\({}_{P336}\)[53] & 4880 & 96 & 8 & Forecast (336) & Finance \\ IL\({}_{P96}\)[111] & 581 & 36 & 7 & Forecast (60) & Illness \\ Traffic\({}_{P96}\)[85] & 12089 & 96 & 862 & Forecast (96) & Traffic \\ Traffic\({}_{P1928}\)[85] & 11993 & 96 & 862 & Forecast (192) & Traffic \\ Traffic\({}_{P336}\)[85] & 11849 & 96 & 862 & Forecast (336) & Traffic \\ Traffic\({}_{P720}\)[85] & 11465 & 96 & 862 & Forecast (720) & Traffic \\ Weather\({}_{P96}\)[110] & 36696 & 96 & 21 & Forecast (96) & Weather \\ Weather\({}_{P192}\)[110] & 36600 & 96 & 21 & Forecast (92) & Weather \\ Weather\({}_{P336}\)[110] & 36456 & 96 & 21 & Forecast (336) & Weather \\ Weather\({}_{P270}\)[110] & 36072 & 96 & 21 & Forecast (720) & Weather \\ SharePriclofenac [79] & 965 & 60 & 1 & Classification (2) & Finance \\ JapaneseVowels [52] & 270 & 29 & 12 & Classification (9) & Audio \\ SpokenArabicDigits [65] & 6599 & 93 & 13 & Classification (10) & Audio \\ Heartbeat [61] & 204 & 405 & 61 & Classification (2) & Audio \\ ECG5000 [35] & 500 & 140 & 1 & Classification (5) & ECG \\ NonIvasiveFetalECGTbreax1 [95] & 1800 & 750 & 1 & Classification (52) & ECG \\ Blink [19] & 500 & 510 & 4 & Classification (2) & EEG \\ FaceDetection [40] & 5890 & 62 & 144 & Classification (2) & EEG \\ SelfRegulationCYP2 & 200 & 1152 & 7 & Classification (2) & EEG \\ ElectricDevices [60] & 8926 & 96 & 1 & Classification (7) & Sensors \\ Trace [93] & 100 & 275 & 1 & Classification (4) & Sensors \\ FordB [23] & 3636 & 500 & 1 & Classification (2) & Sensors \\ MotionSenseHAR [75] & 966 & 200 & 12 & Classification (6) & Human Activity \\ EMOPain [27] & 968 & 180 & 30 & Classification (3) & Human Activity \\ UWaveGestarLabrary [63] & 120 & 315 & 3 & Classification (8) & Human Activity \\ Chinatou [23] & 20 & 24 & 1 & Classification (2) & Traffic \\ MelbournePedestrian [23] & 1194 & 24 & 1 & Classification (10) & Traffic \\ PEMS-SF [20] & 267 & 144 & 963 & Classification (7) & Traffic \\ \hline \hline \end{tabular}
\end{table}
Table 7: Multi-task datasets for classification and forecasting. Prediction length or number of classes are indicated in parenthesis for Forecast and Classification respectively.

\begin{table}
\begin{tabular}{l c c c c} \hline \hline Name & Train Size & Sequence Length & Variables & Task & Class \\ \hline ECG200 [84] & 100 & 96 & 1 & Classification (2) & ECG \\ SelfRegulationSCPI [7] & 268 & 896 & 6 & Classification (2) & EEG \\ RacketsSports [4] & 151 & 30 & 6 & Classification (4) & Human Activity \\ Handwriting [94] & 150 & 152 & 3 & Classification (26) & Human Activity \\ Epilepsy [105] & 137 & 207 & 3 & Classification (4) & Human Activity \\ StarlightDevics [91] & 1000 & 1024 & 1 & Classification (3) & Sensor \\ ETH2\({}_{P96}\)[127] & 8449 & 96 & 7 & Forecast (96) & Electricity \\ ETH2\({}_{P192}\)[127] & 8353 & 96 & 7 & Forecast (192) & Electricity \\ ETH2\({}_{P336}\)[127] & 8209 & 96 & 7 & Forecast (336) & Electricity \\ ETH2\({}_{P202}\)[127] & 7825 & 96 & 7 & Forecast (720) & Electricity \\ ETH1\({}_{P192}\)[127] & 34369 & 96 & 7 & Forecast (96) & Electricity \\ ETH1\({}_{P192}\)[127] & 34273 & 96 & 7 & Forecast (92) & Electricity \\ ETH1\({}_{P336}\)[127] & 34129 & 96 & 7 & Forecast (336) & Electricity \\ ETH1\({}_{P202}\)[127] & 33745 & 96 & 7 & Forecast (720) & Electricity \\ SaugeenRiverFlow [73] & 18921 & 48 & 1 & Forecast (24) & Weather \\ \hline \hline \end{tabular}
\end{table}
Table 8: Datasets for few-shot learning on classification and forecasting tasks. Prediction length or number of classes are indicated in parenthesis for Forecast and Classification respectively.

### Generalizing Task Tokens to Various Tasks

We introduce how to use tokens for forecasting and classification tasks in the manuscript. Here we present the implementation of using tokens for imputation and anomaly detection tasks.

**Imputation task.** In tasks that require imputation, GEN token \(\mathbf{z}_{m}\) is inserted in the positions where sample tokens \(\mathbf{z_{x}}\) are missing. This process creates an augmented sequence of tokens represented by \(\mathbf{\hat{z}_{x}}\). These augmented tokens are then concatenated along the time dimension with prompt tokens, forming the input tokens for the network:

\[\mathbf{z}_{\text{lmp}}=\text{CA}(\mathbf{z}_{p},\mathbf{\hat{z}_{x}})\in \mathbb{R}^{(p+s)\times v\times d},\] (6)

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline Name & Sequence Length & Variables & Task & Class \\ \hline SMD [96] & 96 & 100 & 38 & Anomaly detection & Machine \\ MSL [43] & 96 & 100 & 55 & Anomaly detection & Spacecraft \\ SMAP [43] & 96 & 100 & 25 & Anomaly detection & Spacecraft \\ SWAT [77] & 96 & 100 & 51 & Anomaly detection & Infrastructure \\ PSM [1] & 96 & 100 & 25 & Anomaly detection & Machine \\ \hline \hline \end{tabular}
\end{table}
Table 11: Datasets for anomaly detection tasks.

\begin{table}
\begin{tabular}{c c c c c} \hline \hline Name & Sequence Length & Variables & Task & Class \\ \hline Solar [83] & 128 & 137 & Forecast (64) & Electricity \\ SuapeenRiverFlow [73] & 256 & 1 & Forecast (128) & Weather \\ Hospital [44] & 32 & 767 & Forecast (16) & Healthcare \\ Web Traffic [74] & 160 & 500 & Forecast (80) & Web \\ Temperature Rain [33] & 96 & 500 & Forecast (48) & Weather \\ \hline \hline \end{tabular}
\end{table}
Table 9: Datasets for zero-shot forecasting. Prediction length is indicated in parenthesis. Note that only the first 500 variables are used for the Web Traffic and Temperature Rain datasets.

\begin{table}
\begin{tabular}{c c c c c} \hline \hline Name & Sequence Length & Variables & Task & Class \\ \hline EMTn1 [127] & 96 & 7 & Imputation & 12.5\%, 25\%, 37.5\%,50\% & Electricity \\ ETTh1 [127] & 96 & 7 & Imputation & 12.5\%, 25\%, 37.5\%,50\% & Electricity \\ ECL[102] & 96 & 321 & Imputation & 12.5\%, 25\%, 37.5\%,50\% & Electricity \\ Weather [110] & 96 & 21 & Imputation & 12.5\%, 25\%, 37.5\%,50\% & Weather \\ \hline \hline \end{tabular}
\end{table}
Table 10: Datasets for imputation tasks.

\begin{table}
\begin{tabular}{c c c c} \hline \hline Name & Sequence Length (Multi-task) & Sequence Length (Single-task) & Variables & Task & Class \\ \hline SMD [96] & 96 & 100 & 38 & Anomaly detection & Machine \\ MSL [43] & 96 & 100 & 55 & Anomaly detection & Spacecraft \\ SMAP [43] & 96 & 100 & 25 & Anomaly detection & Spacecraft \\ SWAT [77] & 96 & 100 & 51 & Anomaly detection & Infrastructure \\ PSM [1] & 96 & 100 & 25 & Anomaly detection & Machine \\ \hline \hline \end{tabular}
\end{table}
Table 11: Datasets for anomaly detection tasks.

where CA denotes the concatenation operation along the time dimension. Similar to the approach in forecasting tasks, the output for augmented sample tokens \(\hat{\mathbf{z}}_{\mathbf{x}}\) are unpatchified to obtain the imputed sample \(\hat{\mathbf{x}}\), i.e. \(\hat{\mathbf{x}}=\text{Proj}(\hat{\mathbf{z}}_{\mathbf{x}})\).

**Anomaly detection task.** For the anomaly detection task, we follow TimesNet [112] to form it as a generative task, where the model is trained to reconstruct the time series sample using reconstruction error as the anomaly criterion. The prompt tokens and the sample tokens are concatenated along the time dimension to form the input tokens for the network:

\[\mathbf{z}_{\text{Ano}}=\text{CA}(\mathbf{z}_{p},\mathbf{z}_{\mathbf{x}})\in \mathbb{R}^{(p+s)\times v\times d}.\] (7)

The output for sample tokens \(\mathbf{z}_{\mathbf{x}}\) is unpatchified to obtain the predicted sample \(\hat{\mathbf{x}}\). During inference, following the approach in [112], we determine a threshold of reconstruction error from the training and testing data, which is then used to detect anomalous time series points. Specifically, we sort the reconstruction errors between the input and output samples from our model across all training and testing sets. A predefined anomaly ratio is then applied to determine the threshold that distinguishes normal from anomalous data points.

### Implementation of UniTS Network Architecture

The UniTS network architecture is composed of \(N\) UniTS blocks, one CLS tower, and one GEN tower. We introduce more implementation details of UniTS network architecture, including the Time MHSA, Variable MHSA, Dynamic FFN, and Gate Module in the UniTS block, as well as the GEN/CLS towers shared for generative and predictive tasks.

**UniTS block: time and variable MHSA.** For attention across the time dimension, the standard MHSA is applied as done by [82]. For variable MHSA, to capture relations among variables across all time points while minimizing the computational overhead associated with long time lengths, we average the \(Q\) and \(K\) over the time dimension to get shared \(\hat{Q}\) and \(\hat{K}\) as follows:

\[\hat{Q},\hat{K}=\text{mean}_{t}(Q,K);Q,K,V=\text{Linear}(\mathbf{z}_{\text{ in}}),\] (8)

where \(\text{mean}_{t}\) is the mean along the time dimension. Then, \(\text{Output}=\text{Attn}_{v}V=\text{Softmax}\left(\frac{\hat{Q}\hat{K}^{ \tau}}{\sqrt{d}}\right)V\) is obtained where \(\text{Attn}_{v}\in\mathbb{R}^{v\times v}\) is the attention map among variables, which is shared for all time points. The notations for multi-head attention are omitted for simplicity. We show the effectiveness of both time and variable MHSA in Table 22.

Figure 4: The network architecture of UniTS. Shared GEN tower and CLS tower transform task tokens to the prediction results of generative and predictive tasks.

**UnITS block: Dynamic FFN.** By argument the FFN layer in transformers with the proposed DyLinear operator, we present the Dynamic FFN module, as shown in Figure 5. In the Dynamic FFN, we replace the first linear layer in the standard FFN layer with a 3-kernel convolution across the time dimension to capture the local details. The second linear layer is kept the same as the standard FFN layer, and the DyLinear is inserted in between the input convolution and the output linear layer. Specifically, after processed by the convolution layer, the embeddings with \(d\) dimension are split into two groups, resulting in \((\mathbf{z}^{1}_{\text{mid}},\mathbf{z}^{2}_{\text{mid}})\in\mathbb{R}^{s \times v\times d/2}\). \(\mathbf{z}^{1}_{\text{mid}}\) and \(\mathbf{z}^{2}_{\text{mid}}\) are processed as follows:

\[\mathbf{z}_{\text{out}}=\text{Linear}(\text{Concat}(\text{DyLinear}_{M}( \mathbf{z}^{1}_{\text{mid}}),\mathbf{z}^{2}_{\text{mid}})),\] (9)

where \(\text{DyLinear}_{M}\) processes the sample and prompt tokens in \(\mathbf{z}^{1}_{\text{mid}}\) with two DyLinear operators, while \(\mathsf{CLS}\) token is skipped to ensure consistency for all tasks. \(\mathbf{z}^{2}_{\text{mid}}\) is kept unprocessed. This separation of routes for \(\mathbf{z}^{1}_{\text{mid}}\) and \(\mathbf{z}^{2}_{\text{mid}}\) leads to a scale combination effect, enhancing multi-scale processing ability [31].

**UnITS block: gate module.** The gate module is placed as the output of each component in the UniTS block, including time MHSA, variable MHSA, and Dynamic FFN. Specifically, given an input \(\mathbf{z}_{\text{in}}\in\mathbb{R}^{s\times v\times d}\), a linear layer maps it to a scaling factor \(\mathbf{x}_{g}\in\mathbb{R}^{s\times v\times 1}\) along the embedding dimension. This is followed by a Sigmoid function to ensure the scaling factor lies between 0 and 1. The final gating operation involves element-wise multiplication of the input by the Sigmoid-activated scaling factor, i.e.,

\[\mathbf{z}_{\text{out}}=\text{Sigmoid}(\mathbf{x}_{g})\cdot\mathbf{z}_{\text {in}},\mathbf{x}_{g}=\text{Linear}(\mathbf{z}_{\text{in}}).\] (10)

**GEN tower.** The GEN tower \(H_{\texttt{GEN}}\) is designed to transform tokens into time points prediction results. One GEN tower is shared by all generative tasks, including forecasting, imputation, and anomaly detection. As shown in Figure 4, take the forecasting task as an example, the \(\mathbf{z}_{\text{fore}}\in\mathbb{R}^{(p+s+f)\times v\times d}\) from Eq. 1 is processed by the GEN tower to get the full time-series sample as follows:

\[\hat{\mathbf{x}}=\text{Proj}(\text{MLP}((\mathbf{z}_{\text{fore}}+\text{DyLinear }(\mathbf{z}_{\text{fore}}))),\] (11)

where the MLP is composed of two linear layers with an activation layer in between, and Proj is the unpatchify operation that transfers the embedding back to the time series patch as introduced in Section 4.1. For imputation and anomaly detection tasks, only the tokens are modified while the GEN tower remains unchanged.

**CLS tower.** The \(\mathsf{CLS}\) tower \(H_{\mathsf{CLS}}\) transforms \(\mathsf{CLS}\) tokens into classification classes. The \(\mathsf{CLS}\) tower is shared across all classification tasks from different datasets. As illustrated in Figure 4, the \(\mathsf{CLS}\) tower processes \(\mathbf{z}_{\text{Pred}}\in\mathbb{R}^{(p+s+1)\times v\times d}\) from Eq. 2, which includes the \(\mathsf{CLS}\) token \(\mathbf{z}_{c}\), to produce the final \(\mathsf{CLS}\) token \(\mathbf{z}_{c}\) as follows:

\[\mathbf{z}_{c}=\mathbf{z}_{c}^{{}^{\prime\prime}}+\text{MLP}(\mathbf{z}_{c}^{{} ^{\prime\prime}}),\quad\mathbf{z}_{c}^{{}^{\prime\prime}}=\mathbf{z}_{c}^{{}^{ \prime}}+\text{CrossAtt}(\text{Query}=\mathbf{z}_{c}^{{}^{\prime}},\text{K}= \text{V}=\mathbf{z}_{\text{Pred}}),\] (12)

where the \(\mathsf{CLS}\) token \(\mathbf{z}_{c}^{{}^{\prime}}\) serves as a query to perform cross-attention with all tokens in \(\mathbf{z}_{\text{Pred}}\). Subsequently, the processed \(\mathsf{CLS}\) token \(\mathbf{z}_{c}\) is matched with class embeddings to determine the predicted class as described in Eq. 3.

## Appendix D Implementation Details

### Model Details

By default, in a multi-task setting, the UniTS network comprises three UniTS blocks, one GEN tower, and one \(\mathsf{CLS}\) tower. For each data source, the prompt tokens and task tokens are defined. Forecasting tasks on the same data source but with different forecast lengths share the same prompt and GEN token. For zero-shot learning on new datasets, we use a shared prompt and GEN token across all data sources to facilitate zero-shot learning. Tokens are trained to achieve their functions. The number of embedding dimensions, \(d\), is set to \(64\) for UniTS-_SUP_ and \(128\) for UniTS-_PMT_. All blocks in UniTS maintain the same feature shape, following the Transformer architecture.

Figure 5: The dynamic FFN in UniTS.

### Training Details

For multi-task settings, all models are jointly trained on multiple tasks following the same training protocol. To match the size of the largest dataset, samples from each dataset are repeated in every training epoch. In each inference step, datasets are randomly sampled with equal probability, utilizing a batch size of 32. Supervised training involves 5 epochs using gradient accumulation for an effective batch size of 1024, starting with a learning rate of 3.2e-2 and adjusted with a multi-step decayed schedule. The \(\lambda_{i}\) in \(L_{\text{total}}\) are all set to 1 in this work. For self-supervised pre-training, the models are trained over 10 epochs with an effective batch size of 4096 and an initial learning rate of 6.4e-3, using a cosine decay schedule. All experiments are conducted using A100-40G GPUs. Each experiment is conducted with one or two GPUs, and the maximum running time is under 48 hours.

Since all models are jointly trained across multiple tasks, we report the average performance for each task type. For tasks involving forecasting and imputation, model performance is assessed using Mean Squared Error (MSE) and Mean Absolute Error (MAE). In classification tasks, accuracy is used as the primary evaluation metric. For anomaly detection tasks, performance is measured using precision, recall, and the F1-score.

**No task-specific hyper-parameter tuning.** UniTS is designed for multi-task settings where tasks share the same model weights. In UniTS, we do not need to perform any task-specific hyper-parameter tuning. The baseline methods follow the same training setting as our method to ensure a fair comparisons.

### Further Information on Pre-training

During the unified pre-training, we introduce two distinct masking schemes: the random masking scheme and the right masking scheme. The time series sample is initially truncated to a length randomly selected within the range of 50% to 100% of its original length. Subsequently, in the random masking scheme, a certain proportion \(p_{\text{rand}}\) of tokens are masked at random positions within the time dimension. For the right masking scheme, designed to enhance the model's forecasting ability, a random proportion \(p_{\text{right}}\) of tokens on the right side of the sample is masked. Both \(p_{\text{rand}}\) and \(p_{\text{right}}\) are set to 70%-80%. Each training step randomly utilizes one of these two schemes with equal probability.

\begin{table}
\begin{tabular}{l c c} \hline \hline Task & Method Types & Method \\ \hline Forecasting & \begin{tabular}{c} LLM-reprogrammed \\ Transformer-based \\ MLP-based \\ Frequency-based \\ Conv-based \\ \end{tabular} & \begin{tabular}{c} TEMPO [10] TIME-LLM [47] LLM4TS [12] TEST [97] GPT4TS [129] \\ MOMENT [36] lfransformer [67] patchST [82] Crosformer [126] \\ FEDformer [128] sationation [69] sationformer [114] \\ MLP-based \\ Frequency-based \\ Conv-based \\ \end{tabular} & \begin{tabular}{c} TEMPO [10] TIME-LLM [47] LLM4TS [12] TEST [97] GPT4TS [129] \\ MOMENT [36] lfransformer [67] patchST [82] Crosformer [126] \\ FEDformer [128] sationation [69] sationformer [114] \\ TSN [30] \\ LSTM [41] lfransformer [13] LSSL [39] \\ DTW [6] XGBoost [15] Rocket [24] \\ \hline Imputation & \begin{tabular}{c} Frequency-based \\ MLP-based \\ Transformer-based \\ RNN-based \\ \end{tabular} & \begin{tabular}{c} TimesNet [112] \\ DLinear [119] lfst [122] \\ iTransformer [67] PatchST [82] Rformer [51] \\ ITM [127] Pyraformer [69] Autoformer [114] Stationformer [69] \\ FEDformer [128] ETSformer [111] LogTransformer [57] \\ TCN [30] \\ LSTM [41] LSSL [39] \\ \end{tabular} \\ \hline Anomaly detection & 
\begin{tabular}{c} Frequency-based \\ MLP-based \\ MLP-based \\ MLP-based \\ Transformer-based \\ Stationformer [69] FEDformer [128] ETSformer [111] LogTransformer [57] \\ TCN [30] \\ LSTM [41] LSSL [39] \\ \end{tabular} \\ \hline \hline \end{tabular}
\end{table}
Table 13: Baseline methods used for comparison in this paper.

### Implementation Details of Baselines

The baseline methods used in this paper are summarized in Table 13. Unlike UniTS, which can handle diverse data and tasks within a single model, baseline methods cannot be directly used for unified training because: 1) To accommodate data with varying numbers of variables, baseline methods typically use a data-specific input head to project features from the variable count to a fixed number of embedding dimensions. 2) Similarly, to manage different tasks, such as classification with various classes and forecasting with different lengths, baseline methods employ task-specific output heads to transform the features into the appropriate task outputs. Since baseline methods are designed for single-task training, in their original setting, data/task-specific heads are used for each data and task. In the multi-task learning setting, to make baseline methods support unified training, we add separate input heads to project data into a shared embedding space and separate output heads to convert the shared model output into task-specific outputs. However, using separate input and output heads makes it hard to generalize to new datasets and tasks. We employ the same fully supervised multi-task training approach as UniTS. In this setting, model networks are stacked with 3 basic building blocks, except for GPT4TS, which utilizes the prescribed setting of 6 GPT blocks. For both the proposed method and patch-based baseline approaches, the patch size and stride are fixed at 16. The input and output heads of baseline methods are duplicated for each task to create data/task-specific heads tailored for each data source and task. For single-task learning settings, we follow the original settings of baseline methods and compare results reported in their papers.

## Appendix E Additional Results: Prompt Learning and Pre-training

We do more analysis on the prompting and pre-training of UniTS. The average performance under 38 datasets with the multi-task setting is reported.

**Prompt learning with model scaling.** In Table 14, we further explore the capabilities of prompt learning in the SSL pre-trained UniTS model across different model sizes. As UniTS model size grows, we observe consistent improvements in performance for both classification and forecasting, suggesting that larger SSL models contain more robust representations for prompt learning.

**Effect of prompt tokens.** Prompt tokens learn the contextual information related to the given data source and task types. By default, we use 10 prompt tokens for each task. We present an ablation

\begin{table}
\begin{tabular}{c c c c} \hline \hline Prompt token Num. & Acc\({}_{Avg}\uparrow\) & MSE\({}_{Avg}\downarrow\) & MAE\({}_{Avg}\downarrow\) \\ \hline Unshared prompt tokens & **81.6** & **0.439** & **0.381** \\ Shared prompt tokens & 81.4 & 0.450 & 0.387 \\ \hline \hline \end{tabular}
\end{table}
Table 16: Ablation on using shared/unshared prompt tokens in UniTS network.

\begin{table}
\begin{tabular}{c c c c} \hline \hline \multirow{2}{*}{Prompt Learning} & \multirow{2}{*}{Par.} & Classification & \multicolumn{2}{c}{Forecasting} \\  & & Acc\(\uparrow\) & MSE\(\downarrow\) & MAE\(\downarrow\) \\ \hline UniTS-\(SUP\times 44\) & 3.41M & 81.6 & 0.439 & 0.381 \\ UniTS-\(PMT\times 32\) & 1.57M & 78.0 & 0.471 & 0.388 \\ UniTS-\(PMT\times 64\) & 3.41M & 79.0 & 0.460 & 0.383 \\ UniTS-\(PMT\times 96\) & 5.67M & 79.2 & 0.458 & 0.382 \\ UniTS-\(PMT\times 128\) & 8.24M & **81.2** & **0.453** & **0.376** \\ \hline \hline \end{tabular}
\end{table}
Table 14: Enhancing prompt learning capability of pre-trained UniTS through model scaling. Average performance on 20 forecasting tasks and 18 classification tasks are reported.

\begin{table}
\begin{tabular}{c c c c} \hline \hline Prompt token Num. & Acc\({}_{Avg}\uparrow\) & MSE\({}_{Avg}\downarrow\) & MAE\({}_{Avg}\downarrow\) \\ \hline No & 81.0 & 0.460 & 0.391 \\
5 & 81.5 & 0.455 & 0.387 \\
10 & **81.6** & **0.439** & **0.381** \\ \hline \hline \end{tabular}
\end{table}
Table 15: Ablation on the number of prompt tokens.

study on the use of different numbers of prompt tokens in Table 15. Utilizing prompt tokens leads to notable improvements in both forecasting and classification tasks. The average classification accuracy improves from 81.0% to 81.6%, and the average MSE and MAE improve from 0.460 to 0.439 and 0.391 to 0.381, respectively. Employing 10 instead of 5 prompt tokens results in greater gains in forecasting tasks and a marginal improvement of 0.1% in classification accuracy, indicating that forecasting tasks benefit more from the contextual information provided by the prompt tokens. We also evaluate the case where all prompt tokens are shared among tasks in Table 16. Using shared prompt tokens across different tasks results in a performance decline, yet this approach still surpasses the performance of models that do not utilize prompt tokens.

**Unified pre-training.** In Equation 5, the proposed unified mask reconstruction pre-training loss is detailed, consisting of two components: the mask reconstruction loss associated with prompt tokens and the mask reconstruction loss related to \(\mathtt{CLS}\) tokens. Table 17 presents the results where either the \(\mathtt{CLS}\) token-based reconstruction loss or the prompt token-based reconstruction loss is omitted. The performance of prompt learning is reported. The results highlight the impact of each loss component on the learning performance.

Specifically, excluding the \(\mathtt{CLS}\) token-based loss resulted in a significant decline in classification performance, dropping sharply from 78.0% to 33.1%. This substantial drop underscores the critical role of the \(\mathtt{CLS}\) token-based pre-training loss in enabling the model's classification capabilities. Conversely, the removal of the prompt token-based loss adversely affected the forecasting performance. For instance, the MSE drops from 0.471 to 0.967. This deterioration in performance demonstrates the importance of prompt token-based pre-training in generative tasks.

**Pre-training with scaled numbers of epochs and data sizes.** To evaluate the effect of scaling effect of pre-training, we conduct experiments of pre-training UniTS by varying the size of the pre-training dataset and the amount of training epochs. As demonstrated in Table 18, increasing the number of pre-training epochs improves performance on both forecasting and classification tasks. Similarly, increasing the size of pre-training dataset improves performance on both forecasting and classification tasks, as shown in Table 19.

**Cross-task pre-training.** We evaluate the effect of cross-task pre-training by pre-training a model using our pre-training strategy on either generative tasks (forecasting) or predictive tasks (classification). Table 20 shows that UniTS, pre-trained solely on forecasting datasets, achieves similar performance to the model pre-trained on both forecasting and classification data. Despite not encountering any

\begin{table}
\begin{tabular}{l c c c c} \hline \hline Pre-training steps & 1 epoch & 3 epochs & 5 epochs & 8 epochs & 10 epochs \\ \hline \(\mathtt{Acc}_{Avg}\uparrow\) (Cls.) & 75.1 & 76.8 & 78.2 & 77.0 & 79.0 \\ \(\mathtt{MSE}_{Avg}\downarrow\) (Fore.) & 0.493 & 0.479 & 0.484 & 0.473 & 0.460 \\ \(\mathtt{MAE}_{Avg}\downarrow\) (Fore.) & 0.410 & 0.391 & 0.389 & 0.386 & 0.383 \\ \hline \hline \end{tabular}
\end{table}
Table 18: Performance of UniTS under different pre-training epochs, average performance on 20 forecasting and 18 classification are reported.

\begin{table}
\begin{tabular}{l c c c} \hline \hline UniTS-_PMT_ & \(\mathtt{Acc}_{Avg}\uparrow\) & \(\mathtt{MSE}_{Avg}\downarrow\) & \(\mathtt{MAE}_{Avg}\downarrow\) \\ \hline Unified Pre-training & **78.0** & **0.471** & **0.388** \\ Without \(\mathtt{CLS}\) token based reconstruction loss & 33.1 & 0.484 & 0.393 \\ Without Prompt token based reconstruction loss & 76.8 & 0.967 & 0.656 \\ \hline \hline \end{tabular}
\end{table}
Table 17: Ablation on the pre-training scheme.

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline Pre-training steps & 1 epoch & 3 epochs & 5 epochs & 8 epochs & 10 epochs \\ \hline \(\mathtt{Acc}_{Avg}\uparrow\) (Cls.) & 75.1 & 76.8 & 78.2 & 77.0 & 79.0 \\ \(\mathtt{MSE}_{Avg}\downarrow\) (Fore.) & 0.493 & 0.479 & 0.484 & 0.473 & 0.460 \\ \(\mathtt{MAE}_{Avg}\downarrow\) (Fore.) & 0.410 & 0.391 & 0.389 & 0.386 & 0.383 \\ \hline \hline \end{tabular}
\end{table}
Table 18: Performance of UniTS under different pre-training epochs, average performance on 20 forecasting and 18 classification are reported.

classification datasets during pre-training, it still performs well on classification tasks. When the model is pre-trained exclusively on classification datasets, performance on both classification and forecasting tasks drops significantly compared to the model pre-trained on both types of data. Given that the data amount of forecasting datasets is larger than classification datasets (22920 vs. 5022 iterations per epoch), this suggests that the larger amount of data plays a more crucial role in pre-training effectiveness than the data type.

**Cross-domain pre-training.** We evaluate the effect of cross-domain data pre-training, where the model is pre-trained on either Weather-domain datasets or Traffic-domain datasets. In Table 21, compared to joint pre-training on both domains, the performance decreases with single-domain pre-training, where pre-training is conducted solely on the downstream dataset's domain, showing the advantage of joint pre-training. For instance, the MSE on Weather datasets goes from 0.253 to 0.259. Compared to single-domain pre-training, cross-domain pre-training leads to larger performance drops, e.g., pre-training on Traffic datasets and then evaluating on Weather datasets results in an MSE increase from 0.259 to 0.289. Interestingly, pre-training on Weather datasets achieves better performance across both domains, suggesting that data from certain domains might be more beneficial for pre-training.

## Appendix F Additional Results: Ablation Studies of UniTS

We conduct an ablation study to verify the effectiveness of the key designs in UniTS. The average performance under 38 datasets with the multi-task setting is reported.

**Effect of time and variable MHSA.** In Table 22, we present an ablation study to assess the impact of both Time and Variable MHSA on the UniTS model. When the Time MHSA is removed from the UniTS model, we observe a decrease in performance, where the average accuracy drops to 80.7%, and the MSE drops to 0.449. Similarly, eliminating the Variable MHSA from the UniTS model results in diminished performance. This scenario yields a decreased accuracy of 80.8%, a decrease in MSE to 0.444, and a reduction in MAE to 0.383. These experimental findings highlight the crucial role that both Time and Variable MHSA play in the efficacy of the UniTS model.

**Effect of Dynamic FFN.** In Table 23, we present an ablation study on the Dynamic FFN layer in the UniTS network. The UniTS, which incorporates the Dynamic FFN, achieves the highest performance with an average accuracy of 81.6%, demonstrating effectiveness in handling classification

\begin{table}
\begin{tabular}{l c c c} \hline \hline  & \multicolumn{3}{c}{Evaluation data} \\ Pre-training data type & Acc\({}_{Avg}\)\(\uparrow\) (Cls.) & MSE\({}_{Avg\)\(\downarrow}\) (Fore.) & MAE\({}_{Avg\)\(\downarrow}\) (Fore.) \\ \hline
20 forecasting datasets & 78.5 & 0.454 & 0.379 \\
18 classification datasets & 74.1 & 0.583 & 0.807 \\ Full 38 datasets & 79.0 & 0.460 & 0.383 \\ \hline \hline \end{tabular}
\end{table}
Table 20: Cross-task pre-training evaluation on UniTS, average performance on 20 forecasting and 18 classification tasks are reported.

\begin{table}
\begin{tabular}{l c c} \hline \hline  & **Weather datasets (4 sets)** & **Traffic datasets (4 sets)** \\
**Pre-training data** & MSE\({}_{Avg}\)/MAE\({}_{Avg}\)\(\downarrow\) (Fore.) & MSE\({}_{Avg}\)/MAE\({}_{Avg\)\(\downarrow}\) (Fore.) \\ \hline Weather domain (4 datasets) & 0.259 / 0.287 & 1.338 / 0.768 \\ Traffic domain (4 datasets) & 0.289 / 0.314 & 0.680 / 0.438 \\ Weather + Traffic domains (8 sets) & 0.253 / 0.282 & 0.511 / 0.320 \\ \hline \hline \end{tabular}
\end{table}
Table 21: Cross-domain pre-training evaluation on UniTS, average performance on 4 Weather or Traffic dataset domains are reported.

\begin{table}
\begin{tabular}{l c c c} \hline \hline  & Acc\({}_{Avg}\)\(\uparrow\) & MSE\({}_{Avg}\)\(\downarrow\) & MAE\({}_{Avg}\)\(\downarrow\) \\ \hline UniTS-_SUP_ & **81.6** & **0.439** & **0.381** \\ Without Time MHSA & 80.7 & 0.449 & 0.380 \\ Without Variable MHSA & 80.8 & 0.444 & 0.383 \\ \hline \hline \end{tabular}
\end{table}
Table 22: Ablation on the MHSA in UniTS.

tasks. It also shows superior results in terms of MSE and MAE in forecasting tasks, with scores of 0.439 and 0.381 respectively. The model variant where the Dynamic FFN is replaced with a standard MLP layer exhibits a decrease in performance. The average accuracy dropped to 81.3%, and MSE and MAE dropped to 0.462 and 0.394, respectively. This variation suggests the effect of Dynamic FFN for the UnITS. The performance is observed when the Dynamic FFN is completely removed from the model, highlighting the importance of Dynamic FFN layers in UnITS network.

**Effect of gate module.** In Table 24, we present a comparison of the UnITS model with and without the inclusion of the gate module. Incorporating the gate module yields consistent enhancements relative to the baseline model that lacks it. Specifically, the addition of the gate module results in an increase in classification accuracy, moving from 81.1% to 81.6%. For the forecasting task, the MSE sees an improvement from 0.459 to 0.439, and the MAE decreases from 0.387 to 0.381. These results show the effectiveness of the gate module in mitigating task interference by adjusting the scaling of embedding vectors.

**Comparison with Transformer.** To verify the effectiveness of UnITS structure, we compare the original Transformer with UnITS. The unified tokenization and co-training strategy are applied to both models. The results shown in Table 26 indicate that UnITS clearly outperforms the Transformer in both classification and forecasting tasks, suggesting that merely using a transformer structure is insufficient for achieving robust multi-task performance on time series datasets.

## Appendix G Additional Results: UnITS for Zero-Shot Forecasting on New Datasets

**Setup.** When UnITS is trained with shared prompt and GEN tokens across all forecasting tasks, it acquires the ability to perform zero-shot forecasting on datasets with new lengths and variable numbers that were not part of its training domain. We evaluate UniTS in a zero-shot setting on five new forecasting tasks as referenced in Table 9. These tasks have varying forecasting lengths and numbers of variables compared to those seen by UniTS during pre-training. We benchmark against LLMTime [81], a model designed for zero-shot forecasting using LLMs. Following LLMTime, we utilize one sample from each dataset to manage the extensive inference costs. We exclude a related method, Time-LLM [47], from experiments. Time-LLM supports zero-shot learning but requires that the forecasting length and the number of variables/sensors for zero-shot prediction are the same as those used for training.

\begin{table}
\begin{tabular}{c c c c} \hline \hline  & Acc\({}_{Avg}\uparrow\) & MSE\({}_{Avg}\downarrow\) & MAE\({}_{Avg}\downarrow\) \\ \hline UnITS-_SUP_ & **81.6** & **0.439** & **0.381** \\ Dynamic FFN \(\rightarrow\) MLP & 81.3 & 0.462 & 0.394 \\ Without Dynamic FFN & 80.8 & 0.465 & 0.396 \\ \hline \hline \end{tabular}
\end{table}
Table 23: Ablation on the MLP layer in UniTS network.

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline  & Var. & Pred. & \begin{tabular}{c} UnITS-_Zero-shot_ \\ MSE\(\downarrow\) \\ \end{tabular} & \begin{tabular}{c} LLMTime \\ MSE\(\downarrow\) \\ \end{tabular} & 
\begin{tabular}{c} LLMTime \\ MSE\(\downarrow\) \\ \end{tabular} \\ \hline Solar & 137 & 64 & 0.030 & \(6.8e^{-3}\) & 0.265 & \(2.0e^{-3}\) \\ River & 1 & 128 & 0.456 & \(1.4e^{-2}\) & 0.832 & \(3.5e^{-3}\) \\ Hospital & 767 & 16 & 1.045 & \(5.9e^{-3}\) & 1.319 & \(2.9e^{-3}\) \\ Web Tr. & 500 & 80 & 1.393 & \(5.9e^{-3}\) & 1.482 & \(9.5e^{-3}\) \\ Temp. Rain & 500 & 48 & 11.51 & \(1.6e^{-1}\) & 5.69 & \(5.3e^{-3}\) \\ \hline \hline \end{tabular}
\end{table}
Table 25: Zero-shot multi-task learning on forecasting tasks on 5 out-of-domain data with new forecasting length and new number of variables. We set shared prompt tokens and GEN tokens for UniTS. One sample from each dataset is used following [81].

[MISSING_PAGE_FAIL:29]

[MISSING_PAGE_FAIL:30]

[MISSING_PAGE_FAIL:31]

\begin{table}
\begin{tabular}{l c c c|c c c c c c|c c c c c|c c c c} \hline \hline \multirow{2}{*}{Datasets} & \multicolumn{3}{c}{SMD} & \multicolumn{3}{c}{MSL} & \multicolumn{3}{c}{SMAP} & \multicolumn{3}{c}{SWaT} & \multicolumn{3}{c}{PSM} & \multicolumn{3}{c}{Arg F1\(\uparrow\)} \\ \cline{2-13} \cline{2-13} \multicolumn{1}{c}{Metrics} & \multicolumn{1}{c}{P\(\uparrow\)} & R\(\uparrow\) & F1\(\uparrow\) & P\(\uparrow\) & R\(\uparrow\) & F1\(\uparrow\) & P\(\uparrow\) & R\(\uparrow\) & F1\(\uparrow\) & P\(\uparrow\) & R\(\uparrow\) & F1\(\uparrow\) & P\(\uparrow\) & R\(\uparrow\) & F1\(\uparrow\) & (\%) \\ \hline LSTM & [41] & 78.52 & 65.47 & 71.41 & 78.04 & 86.22 & 81.93 & 91.06 & 57.49 & 70.48 & 78.06 & 91.72 & 84.34 & 69.24 & 99.53 & 81.67 & 77.97 \\ Transformer [104] & 83.58 & 76.13 & 79.56 & 71.57 & 87.37 & 78.68 & 89.37 & 57.12 & 69.70 & 68.84 & 96.53 & 80.37 & 62.75 & 96.56 & 76.07 & 76.88 \\ LogTrans & [57] & 83.46 & 70.13 & 76.21 & 73.05 & 87.37 & 79.57 & 89.15 & 57.59 & 69.97 & 68.67 & 97.32 & 80.52 & 63.06 & 98.00 & 76.74 & 76.60 \\ TCN & [30] & 84.06 & 79.07 & 81.49 & 75.11 & 82.44 & 78.60 & 86.90 & 59.23 & 70.45 & 76.59 & 95.71 & 85.09 & 54.59 & 99.77 & 70.57 & 77.24 \\ Reformer & [51] & 82.58 & 69.24 & 75.32 & 85.51 & 83.31 & 84.40 & 90.91 & 57.44 & 70.40 & 72.50 & 96.53 & 82.80 & 59.93 & 95.38 & 73.61 & 77.31 \\ Informer & [127] & 86.60 & 77.23 & 81.65 & 81.77 & 86.48 & 84.06 & 90.11 & 57.13 & 69.92 & 70.29 & 96.75 & 81.43 & 64.27 & 96.33 & 77.10 & 78.83 \\ Anomaly* [116] & 88.91 & 82.23 & 85.49 & 79.61 & 87.37 & 83.31 & 91.85 & 58.11 & 71.18 & 72.51 & 97.32 & 83.10 & 68.35 & 94.72 & 79.40 & 80.50 \\ Pyraformer & [65] & 85.61 & 80.61 & 83.04 & 83.81 & 85.93 & 84.86 & 92.54 & 57.71 & 71.09 & 87.92 & 96.00 & 91.78 & 71.67 & 69.02 & 82.08 & 82.57 \\ Autoformer & [114] & 88.06 & 82.35 & 85.11 & 77.72 & 80.92 & 79.05 & 90.40 & 58.62 & 71.12 & 89.85 & 95.81 & 92.74 & 99.08 & 88.15 & 99.32 & 84.26 \\ LSSL & [39] & 78.51 & 65.32 & 71.31 & 77.55 & 88.18 & 82.53 & 89.43 & 53.43 & 69.00 & 79.05 & 93.72 & 85.76 & 96.60 & 29.93 & 77.20 & 76.74 \\ Station. & [89] & 88.31 & 81.21 & 84.62 & 68.55 & 89.14 & 77.50 & 89.37 & 59.02 & 71.09 & 68.03 & 96.75 & 79.88 & 97.82 & 96.76 & 97.29 & 82.08 \\ DLinear & [119] & 83.62 & 71.52 & 77.10 & 84.34 & 85.42 & 84.88 & 92.32 & 55.41 & 69.26 & 80.91 & 95.30 & 87.52 & 98.28 & 89.26 & 93.55 & 82.46 \\ ETSformer & [111] & 87.44 & 79.23 & 83.13 & 85.13 & 84.93 & **85.03** & 92.25 & 55.75 & 69.50 & 90.02 & 80.36 & 84.91 & 99.31 & 85.28 & 91.76 & 82.87 \\ LightTS & [122] & 87.10 & 78.42 & 82.53 & 82.40 & 75.78 & 78.95 & 92.58 & 55.27 & 69.21 & 91.98 & 94.72 & **93.33** & 98.37 & 95.97 & 97.15 & 84.23 \\ FEDformer & [128] & 87.95 & 82.39 & 85.08 & 77.14 & 80.07 & 78.57 & 90.47 & 58.10 & 70.76 & 90.17 & 96.42 & 93.19 & 97.31 & 97.16 & 97.23 & 84.97 \\ TimesNet* [112] & 87.95 & 81.54 & 84.62 & 89.55 & 75.29 & 81.80 & 90.14 & 56.56 & 69.50 & 90.76 & 95.35 & 93.00 & 98.50 & 96.29 & 97.38 & 85.26 \\
**UniTS-ST** & Ours & 89.32 & 86.90 & **88.09** & 89.91 & 77.68 & 83.46 & 93.37 & 76.02 & **83.80** & 92.37 & 94.17 & 93.26 & 98.62 & 96.28 & **97.43** & **89.21** \\ \hline \hline \end{tabular}

* For fair comparisons, we follow the settings of [112] to only use reconstruction error for Anomaly Transformer.
* TimesNet are reproduced from the https://github.com/thuml/Time-Series-Library to ensure fair comparisons.

\end{table}
Table 31: Full results for the single-task classification task. \(*\). in the Transformers indicates the name of \(*\)former. We report the classification accuracy (%) as the result.

\begin{table}
\begin{tabular}{l|c c c c c c c c c c c c c c c} \hline \hline \hline \multirow{2}{*}{Datasets / Models} & \multirow{2}{*}{Classical methods} & \multicolumn{3}{c}{RNN} & \multicolumn{3}{c}{TCN} & \multicolumn{3}{c}{Transformers} & \multicolumn{3}{c}{MLP} & \multicolumn{3}{c}{Freq.} \\ \cline{3-14}  & & & \multicolumn{3}{c}{-} & & & & & & & & & & & & & & \\ \cline{3-14}  & DTWXC/BioRes/Redicat & STML/STNe LSSL & TCN Trans. Re. & In. & Pyra-Auto. Satation & FED. ETS. & Flow.DLinear/Light/TS. TimesNet & **UniTS-ST** \\  & [6] & [15] & [24] & [41] & [53] & [39] & [30] & [104] & [51] & [127] & [65] & [114] & [69] & [128] & [111] & [113] & [19] & [122] & [112] & [112] & **(Ours)** \\ \hline EthanolConcentration & 32.3 & 43.7 & 45.2 & 32.3 & 39.9 & 31.1 & 28.9 & 32.7 & 31.9 & 31.6 & 30.8 & 31.6 & 32.7 & 31.2 & 28.1 & 33.8 & 32.6 & 29.7 & 35.7 & 37.6 \\ FaceDetection & 52.9 & 63.3 & 64.7 & 57.7 & 65.7 & 66.7 & 52.8 & 67.3 & 68.6 & 67.0 & 6

[MISSING_PAGE_EMPTY:33]

## Appendix M Limitations and Future Directions

The datasets collected by this work do not yet cover all available time series datasets, such as some of the univariate datasets in UCR dataset collections [23] and the more physiologic time series signals from PhysioNet [34]. We will explore using larger dataset collections to further improve UniTS.

UniTS primarily aims to unify predictive and generative tasks within a single multi-task model. We demonstrate this by showcasing its adaptability to new data and tasks through prompt learning and few-shot learning. While adapting to new time series data differs fundamentally from generalizing to entirely new data, we will further explore UniTS's generalization ability for zero-shot learning.

\begin{table}
\begin{tabular}{l c c c} \hline \hline UniTS & Acc\({}_{Avg}\uparrow\) (Classification) & MSE\({}_{Avg}\downarrow\) (Forecasting) \\ \hline
**Multi-task** & 81.6\% & 0.439 \\
**Single-task** & 65.3\% & 0.464 \\ \hline \hline \end{tabular}
\end{table}
Table 35: Compare UniTS trained by multi-task learning with that trained by single-task learning under same hyper-parameters.

\begin{table}
\begin{tabular}{l|c|c c c c c c c c c c c c c c} \hline \hline \multirow{2}{*}{Models} & \multirow{2}{*}{Models} & \multirow{2}{*}{\begin{tabular}{c} **UniTS-ST** \\ **(Ours)** \\ \end{tabular} } & \multirow{2}{*}{\begin{tabular}{c} **MOMENT** \\ [36] \\ \end{tabular} } & \multirow{2}{*}{\begin{tabular}{c} **TSMMer** \\ [41] \\ \end{tabular} } & \multirow{2}{*}{\begin{tabular}{c} **TSMMer** \\ [10] \\ \end{tabular} } & \multirow{2}{*}{\begin{tabular}{c} **TSEMO** \\ [47] \\ \end{tabular} } & \multirow{2}{*}{\begin{tabular}{c} **UTEMTS** \\ [12] \\ \end{tabular} } & \multirow{2}{*}{\begin{tabular}{c} **HLTMS** \\ [97] \\ \end{tabular} } & \multirow{2}{*}{\begin{tabular}{c} **TEST** \\ [12] \\ \end{tabular} } & \multirow{2}{*}{\begin{tabular}{c} **GPTATS** \\ [12] \\ \end{tabular} } \\ \cline{4-14}  & Metric & MSE & MSE & MSE & MSE & MSE & MSE & MSE & MSE & MSE & MSE & MSE & MSE & MSE & MSE & MSE \\ \hline \multirow{9}{*}{\begin{tabular}{c} **UniTS** \\ \end{tabular} } & 96 & **0.278** & **0.338** & 0.293 & 0.349 & 0.285 & 0.339 & 0.438 & 0.424 & 0.272 & 0.334 & 0.360 & 0.388 & 0.299 & 0.346 & 0.292 & 0.346 \\  & 192 & **0.319** & **0.346** & - & - & 0.327 & 0.365 & 0.461 & 0.421 & 0.310 & 0.358 & 0.386 & 0.401 & 0.332 & 0.369 & 0.332 & 0.372 \\  & 336 & **0.354** & **0.366** & - & - & 0.356 & **0.382** & 0.515 & 0.476 & 0.352 & 0.384 & 0.415 & 0.471 & 0.368 & 0.392 & 0.366 & 0.394 \\  & 720 & **0.397** & 0.416 & 0.405 & 0.416 & 0.419 & **0.414** & 0.591 & 0.509 & 0.383 & 0.411 & 0.470 & 0.445 & 0.418 & 0.420 & 0.417 & 0.421 \\ \cline{2-14}  & Avg & **0.337** & 0.376 & 0.349 & 0.383 & 0.347 & **0.375** & 0.501 & 0.458 & 0.329 & 0.372 & 0.408 & 0.413 & 0.353 & 0.382 & 0.352 & 0.383 \\ \cline{2-14}  & 96 & 0.167 & 0.285 & 0.181 & 0.26 & **0.163** & **0.252** & 0.185 & 0.267 & 0.161 & 0.538 & 0.265 & - & 0.184 & 0.265 & - & 0.173 & 0.262 \\  & 192 & 0.222 & 0.295 & - & **0.216** & **0.290** & 0.243 & 0.304 & 0.219 & 0.293 & 0.260 & 0.301 & - & - & 0.229 & 0.301 \\  & 336 & 0.270 & 0.325 & - & **0.268** & **0.324** & 0.396 & 0.347 & 0.271 & 0.329 & 0.292 & 0.337 & - & - & 0.286 & 0.341 \\  & 720 & **0.358** & **0.380** & 0.366 & 0.388 & 0.420 & 0.422 & 0.386 & 0.395 & 0.352 & 0.379 & 0.386 & 0.393 & - & - & 0.378 & 0.401 \\ \cline{2-14}  & Avg & **0.254** & **0.315** & 0.274 & 0.329 & 0.267 & 0.322 & 0.281 & 0.328 & 0.251 & 0.314 & 0.276 & 0.324 & - & 0.284 & 0.339 \\ \hline \multirow{9}{*}{
\begin{tabular}{c} **UniTS** \\ \end{tabular} } & 96 & **0.306** & 0.396 & 0.387 & 0.410 & 0.361 & **0.392** & 0.400 & 0.406 & 0.362 & 0.392 & 0.371 & 0.394 & 0.372 & 0.400 & 0.376 & 0.397 \\  & 192 & **0.401** & **0.416** & - & - & **0.404** & **0.418** & 0.420 & 0.413 & 0.403 & 0.412 & 0.412 & 0.422 & 0.416 & 0.418 \\ \cline{2-14}  & 316 & 0.425 & 0.439 & - & **0.420** & **0.431** & 0.441 & 0.450 & 0.430 & 0.427 & 0.422 & 0.422 & 0.423 & 0.437 & 0.442 & 0.433 \\  & 720 & **0.434** & 0.454 & 0.472 & 0.463 & 0.472 & 0.428 & 0.424 & 0.424 & 0.427 & 0.444 & 0.447 & 0.457 & 0.436 \\ \cline{2-14}  & Avg & **0.405** & **0.426** & 0.421 & 0.441 & 0.412 & **0.428** & 0.428 & 0.427 & 0.408 & 0.424 & 0.404 & 0.418 & 0.414 & 0.431 & 0.428 & 0.426 \\ \cline{2-14}  & 96 & 0.277 & 0.346 & 0.288 & 0.345 & **0.274** & **0.341** & 0.301 & 0.353 & 0.268 & 0.328 & 0.269 & 0.332 & 0.275 & 0.338 & 0.285 & 0.342 \\  & 192 & **0.325** & **0.328** & - & 0.339 & 0.338 & 0.355 & 0.392 & 0.325 & 0.375 & 0.337 & 0.307 & 0.370 & 0.379 & 0.354 & 0.389 \\  & 336 & **0.347** & **0.398** & - & - & 0.361 & **0.406** & 0.379 & 0.408 & 0.368 & 0.409 & 0.353 & 0.396 & 0.329 & 0.381 & 0.373 & 0.407 \\  & 720 & **0.373** & **0.420** & **0.403** & 0.439 & 0.445 & 0.470 & 0.409 & 0.409 & 0.407 & 0.422 & 0.303 & 0.342 & 0.308 & 0.423 & 0.406 & 0.441 \\ \cline{2-14}  & Avg & **0.331** & **0.387** & 0.346 & 0.392 & 0.355 & **0.401** & 0

## Appendix N Impact Statement

This paper focuses on analyzing time series sequences from various domains and introduces a versatile machine-learning approach designed for this purpose. While our research has numerous potential societal impacts, we believe none require specific emphasis in this context.

Figure 7: The similarity of prompt tokens among datasets.

Figure 8: UMAP of untrained prompt tokens in UniTS. This plot illustrates that there is no significant organization (clustering) of prompt tokens prior to UniTS training.

Figure 9: UMAP of trained prompt tokens in UniTS. Unlike Figure 8 above, this plot illustrates the meaningful organization (clustering) of prompt tokens by dataset domain category when trained by UniTS.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The abstract and introduction cover the contributions and scope of the paper regarding building a unified time-series model. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: In Section M, we discuss the limitations and future work regarding the size of dataset collections and the exploration of more advanced network architectures. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?Answer: [NA] Justification: This paper does not include theoretical results. Guidelines:

* The answer NA means that the paper does not include theoretical results.
* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We provide the implementation details for experiment settings in Section 5, Section D, and Section K. We also provide the source code and datasets at https://github.com/mims-harvard/UniTS. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code**Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: We provide the source code and datasets at https://github.com/mims-harvard/UniTS. Instructions for downloading data and running experiments are provided inside." Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We provide the details for training and test settings in Section 5, Section D, and Section K. Full details are shown in the provided code. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: We use a fixed seed in experiments to maintain low statistical variance. Additionally, we conduct experiments on diverse datasets to ensure the statistical significance of the results. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.

* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We provide the computer resources we used in Section D.2. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We have read the NeurIPS Code of Ethics and adhere to it. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: We discuss the broader impacts in Section N. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.

* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: This paper poses no such risks. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We use open source datasets and codes based on their licenses. Most baselines are released under mit lisxxx. We use opsource datasets preprocessed xxx we did not crxx build new dataset in this paper. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.

* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: This paper does not release new assets. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: This paper does not involve crowdsourcing nor research with Human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.