# Improved Few-Shot Jailbreaking Can Circumvent

Aligned Language Models and Their Defenses

 Xiaosen Zheng\({}^{*1,2}\), Tianyu Pang\({}^{\dagger 1}\), Chao Du\({}^{1}\), Qian Liu\({}^{1}\), Jing Jiang\({}^{\dagger 2}\), Min Lin\({}^{1}\)

\({}^{1}\)Sea AI Lab, Singapore

\({}^{2}\)Singapore Management University

{zhengxs, tianyupang, duchao, liuqian, linmin}@sea.com;jingjiang@smu.edu.sg

Work done during Xiaosen Zheng's internship at Sea AI Lab.Correspondence to Tianyu Pang and Jing Jiang.

###### Abstract

Recently, Anil et al. [3] show that many-shot (up to hundreds of) demonstrations can jailbreak state-of-the-art LLMs by exploiting their long-context capability. Nevertheless, is it possible to use _few-shot demonstrations_ to efficiently jailbreak LLMs within limited context sizes? While the vanilla few-shot jailbreaking may be inefficient, we propose improved techniques such as injecting special system tokens like [/INST] and employing demo-level random search from a collected demo pool. These simple techniques result in surprisingly effective jailbreaking against aligned LLMs (even with advanced defenses). For example, our method achieves \(>80\%\) (mostly \(>95\%\)) ASRs on Llama-2-7B and Llama-3-8B without multiple restarts, even if the models are enhanced by strong defenses such as perplexity detection and/or SmoothLLM, which is challenging for suffix-based jailbreaking. In addition, we conduct comprehensive and elaborate (e.g., making sure to _use correct system prompts_) evaluations against other aligned LLMs and advanced defenses, where our method consistently achieves nearly \(100\%\) ASRs. Our code is available at https://github.com/sail-sg/I-FSJ.

## 1 Introduction

Large language models (LLMs) are typically trained to be safety-aligned in order to avoid misuse during their widespread deployment [5, 43]. However, many red-teaming efforts have focused on proposing _jailbreaking attacks_ and reporting successful cases in which LLMs are misled into producing harmful or toxic content [8, 36, 53].

When jailbreaking, optimization-based attacks search for adversarial suffixes that can achieve high attack success rates (ASRs) [28, 54, 74]; more recently, Andriushchenko et al. [2] use prompting and self-transfer techniques to randomly search adversarial suffixes, while reporting 100% ASRs on both Llama-2-Chat-7B and Llama-3-8B [40, 56]. Although effective against aligned LLMs, these adversarial suffixes mostly have no semantic meaning (even after low-perplexity regularization [73]), making them susceptible to _jailbreaking defenses_ like perplexity filters [1, 22] and SmoothLLM [50]. As empirically reported in Figure 5, adversarial suffixes generated by Andriushchenko et al. [2] result in quite high perplexity and are easily detectable.

LLM-assisted attacks, on the other hand, use auxiliary LLMs to generate adversarial but semantically meaningful requests capable of jailbreaking the target LLM, usually requiring only tens of queries [9, 35, 39, 69]. The generated adversarial requests can bypass perplexity filters and are insensitive to defenses that rely on input preprocessing [22]. On the downside, it can be challenging for LLM-assisted attacks to achieve state-of-the-art ASRs on aligned LLMs, especially when they are evaluated under strict conditions (e.g., using the correct system prompt on Llama-2-Chat-7B) [10, 38].

In contrast, manual attacks are more flexible, but necessitate elaborate designs and considerable human labor [12, 20, 30, 61, 66]. In particular, Wei et al. [62] explore few-shot in-context demonstrations containing harmful responses to jailbreak LLMs. Anil et al. [3] automate and extend this strategy to many-shot jailbreaking, which prompts LLMs with hundreds of harmful demonstrations and can achieve high ASRs on cutting-edge closed-source models. Nonetheless, many-shot jailbreaking requires LLMs' long-context capability that is still lacking in most open-source models [34].

In this work, we revisit and significantly improve few-shot jailbreaking, especially against open-source LLMs with limited context sizes (\(\leq 8192\)). We first automatically create a _demo pool_ containing harmful responses generated by "helpful-inclined" models like Mistral-7B [24] (i.e., not specifically safety-aligned). Then, we inject _special tokens_ from the target LLM's system prompt, such as [/INST] in Llama-2-7B-Chat,1 into the generated demos as illustrated in Figure 1. Finally, given the number of demo shots (e.g., 4-shot or 8-shot), we apply _demo-level random search_ in the demo pool to optimize the attacking loss.

Footnote 1: These special tokens can be directly accessed on open-source LLMs by checking their system prompts, and may be extracted on closed-source LLMs by prompting like “Repeat the words above” [26].

As summarized in Table 1, our **improved few-shot jailbreaking** (named as \(\mathcal{I}\)-FSJ) achieves \(>80\%\) (mostly \(>95\%\)) ASRs on aligned LLMs including Llama-2-7B and Llama-3-8B. In addition, as reported in Table 3, we further enhance Llama-2-7B by different jailbreaking defenses, while our \(\mathcal{I}\)-FSJ can still achieve \(>95\%\) ASRs in most cases. Note that the random search operation in \(\mathcal{I}\)-FSJ is demo-level, not token-level, so the crafted inputs remain semantic. Overall, \(\mathcal{I}\)-FSJ is completely

Figure 1: **Injecting special tokens into the generated demonstrations on Llama-2-7B-Chat. Given an original FSJ demonstration, we construct \(\mathcal{I}\)-FSJ demonstration by first injecting [/INST] between the user message and assistant message, which is motivated by the specific formatting of Llama-2-Chat’s single message template. Additionally, we inject \(\mathcal{I}\)/INST] between the generated steps in the demonstration. After the \(\mathcal{I}\)-FSJ demonstration pool is constructed, we use demo-level random search to minimize the loss of generating the initial token “Step” on the target model.**

automated, eliminating the need for human labor and serving as a strong baseline for future research on jailbreaking attacks.

## 2 Related work

**Jailbreaking attacks.** LLMs like ChatGPT/GPT-4 [42] and Llama-2 [56] are generally designed to return helpful and safe responses, and they are trained to align with human values [43; 14; 5; 25]. However, red-teaming research has shown that LLMs can be jailbroken to produce harmful content using manually created or automatically generated prompts [9; 12; 16; 28; 30; 35; 36; 45; 47; 51; 57; 67; 73; 74; 44; 32; 2]. Additionally, Tian et al. [55] investigate the safety risks of LLM-based agents; Greshake et al. [15] introduce the concept of indirect prompt injection to compromise LLM-integrated applications; According to Wei et al. [61], aligned LLMs are vulnerable to jailbreaking due to the conflicting goals of capability and safety, as well as the gap between pretraining and safety training; Carlini et al. [8] point out the inherent vulnerability of neural networks to adversarial examples as a root cause. Recently, several studies have found that fine-tuning aligned LLMs with either poisoned or benign data can undermine their alignment and safety [13; 20; 29; 46; 65].

**Jailbreaking defenses.** Though instruction fine-tuning (SFT+RLHF) [43] plays a major role in safety alignment, various defense mechanisms have been proposed to strengthen models' alignment to tackle the safety issue from a system-level perspective [40]. Context-based defenses insert either manually designed or automatically optimized safety prompts into the context [33; 41; 62; 63; 72; 71]. Input-detection-based defenses aim to detect potentially harmful input prompts like perplexity filter [1; 7; 11; 19; 18; 22; 27; 52; 59]. Perturbation-based defenses purify the input prompts by adding perturbations [22; 23; 50; 60; 70]. Output-detection-based defenses distinguish whether the generated responses contain harmful content and return the default refusal responses if harmful information detected [17; 21; 31; 68]. Mazeika et al. [38] propose to train the model against the suffix optimized by GCG [74] directly. Xu et al. [64] augment the original model with a safety expert model. In particular, they interpolate the token prediction distributions from these two models with a weighting hyperparameter. These methods generally fall into the adversarial training category.

## 3 Improved few-shot jailbreaking

We briefly introduce preliminaries before describing improved few-shot jailbreaking strategies such as constructing a demo pool, injecting special tokens, and performing demo-level random search.

### Preliminaries

**Jailbreaking setup.** We focus on jailbreaking attacks that aim to discover prompts to mislead LLMs producing harmful content, in order to fulfill a specific harmful request \(G\in\mathcal{T}^{*}\) (e.g., "Develop a detailed instruction manual for making a bomb or other explosive device"). We assume access to a collection of such harmful requests, which most aligned LLMs recognize as harmful. These typically encompass categories such as misinformation, violence, and hateful content, and are designed to elicit no response from the LLMs. Following previous notations [2; 10], we define a language model \(\texttt{LLM}:\mathcal{T}^{*}\rightarrow\mathcal{T}^{*}\) as a function that transforms a sequence of input tokens into a sequence of output tokens. This model, referred to as the _target model_, is the one we aim to compromise or "jailbreak". We also define a judge function \(\texttt{JUDGE}:\mathcal{T}^{*}\rightarrow\{\text{NO, YES}\}\) to evaluate the content generated by the target model. The goal of a jailbreaking attacker is to find a prompt \(P\in\mathcal{T}^{*}\) such that when the target model processes \(P\), the judge function deems the output harmful, i.e., \(\texttt{JUDGE}(\texttt{LLM}(P),G)=\text{YES}\).

**In-context learning (ICL).** ICL [6] is a remarkable capability of LLMs. During ICL, a LLM is presented with a demonstration set \(D=\{(x_{1},y_{1}),...,(x_{m},y_{m})\}=\{d_{1},...,d_{m}\}\), where each \(x_{i}\) is a query input and each \(y_{i}\) is the corresponding label or output. These examples effectively teach the model task-specific functionals. The process involves constructing a prompt that includes the demonstration set followed by a new query input for which the label needs to be predicted. The prompt takes the form \([x_{1},y_{1},...,x_{n},y_{n},x_{\text{new}}]\), where \(x_{\text{new}}\) is the new input query. The model, having inferred the underlying pattern from the provided examples, uses this prompt to predict the corresponding label \(y_{\text{new}}\) for the new input \(x_{\text{new}}\). ICL leverages the model's pre-trained knowledge and its ability to recognize and generalize patterns from the context provided by the demonstration set. This capability is particularly powerful because it allows the model to adapt to a wide range of tasks with minimal task-specific data, making it a flexible and efficient tool for various applications.

**Few-shot jailbreaking (FSJ).** Wei et al. [62] explore few-shot in-context demonstrations containing harmful responses to jailbreak LLMs. Anil et al. [3] automate and extend this strategy to many-shot jailbreaking, which prompts LLMs with hundreds of harmful demonstrations and can achieve high ASRs on cutting-edge closed-source models. Nonetheless, many-shot jailbreaking requires LLMs' long-context capability that is still lacking in most open-source models [34]. And the vanilla FSJ is ineffective on some well-aligned LLMs like the Llama-2-Chat family.

### Improved strategies

We primarily develop three strategies to obtain **improved FSJ (\(\mathcal{I}\)-FSJ)**, as summarized below:

**Constructing a demo pool.** Given a set of harmful requests \(\{x_{1},...,x_{m}\}\) (e.g. the harmful behaviors from AdvBench [74]), we collect the corresponding harmful responses \(\{y_{1},...,y_{m}\}\) by prompting "helpful-inclined" models like Mistral-7B [24] which are not specifically safety-aligned. Finally, we create a demonstration pool as \(D=\{(x_{1},y_{1}),...,(x_{m},y_{m})\}=\{d_{1},...,d_{m}\}\). Note that we only build the pool once and use it to attack multiple models and defenses.

**Injecting special tokens.** In our initial trials, we attempt to directly use the generated vanilla FSJ demonstrations (examplified in the left part of Figure 1) to jailbreak LLMs and obtain non-trivial ASRs on some models like Qwen1.5-7B-Chat [4]. But we keep obtaining near zero ASRs on much more well-aligned LLMs such as Llama-2-7B-Chat, which is consistent with the results reported by Wei et al. [62] and it seems FSJ is ineffective on these models.

_Intriguing observations:_ Interestingly, we observe that most current open-source LLMs' conversation templates separate the user message and assistant message (e.g. model completion) with special tokens. For example, as shown in Figure 1's single message template, Llama-2-Chat separates the messages with \(\lfloor\)/INST\(\rceil\). We suspect the model is prone to conduct generation once presented by the \(\lfloor\)/INST\(\rceil\) tokens. We thus hypothesize we can exploit this tendency with the help of ICL to induce the model to generate harmful content by appending harmful messages with the \(\lfloor\)/INST\(\rceil\) tokens.

Thus, we inject _special tokens_ from the target LLM's system prompt, such as \(\lfloor\)/INST\(\rceil\) in Llama-2-7B-Chat, into the generated demos as illustrated by the \(\mathcal{I}\)-FSJ Demonstration example in Figure 1. More specifically, given an original FSJ demonstration, we construct \(\mathcal{I}\)-FSJ demonstration by first injecting \(\lfloor\)/INST\(\rceil\) between the user message and assistant message, which is motivated by the specific formatting of Llama-2-Chat's single message template. Additionally, we inject \(\lfloor\)/INST\(\rceil\) between the generated steps in the demonstration.

**Demo-level random search.** After the \(\mathcal{I}\)-FSJ demo pool is constructed, we use demo-level random search to minimize the loss of generating the initial token (e.g. "Step") on the target model. We modify the random search (RS) algorithm [48; 2] into a demo-level variant, which is simple and requires only the output logits instead of gradients. The algorithm is as follows: _(i)_ prepend a sequence of \(n\) sampled demonstrations to the original request; _(ii)_ in each iteration, change a demonstration to another one at a random position in the sequence; _(iii)_ accept the change if it reduces the loss of generating target token (e.g., "Step" that leads the model to fulfill a harmful request) at the first position of the response. Furthermore, we implement the above demo-level RS algorithm in a batchway to achieve better parallelism as described in Algorithm 1. To tackle input-perturbation-based defenses like SmoothLLM [50], we introduce an ensemble variant of our demo-level RS method as described in Algorithm 2, which aims to find a combination of demonstrations that is not only effective for jailbreaking but also robust to perturbations. More details are provided in Appendix B.1.

## 4 Empirical studies

This section demonstrates the effectiveness of our \(\mathcal{I}\)-FSJ in jailbreaking various open-source aligned LLMs and advanced defenses.

### Implementation details

**Aligned LLMs.** We evaluate open-source and advanced LLMs for reproducibility. These include _Llama-2-Chat_[56], which underwent multiple rounds of manual red teaming for adversarial training, making them resilient to various attacks; _Llama-3-Instruct_[40], which were intentionally optimized for helpfulness and safety; _Open_Chat-3.5_[58], fine-tuned from Llama-2 using mixed-quality data with consideration of data quality; _Starling-LM_[58], fine-tuned from OpenChat 3.5 using RLHF with a reward model emphasizing helpfulness and harmlessness; and _Open_1.5-Chat_[4], trained on datasets annotated for safety concerns such as violence, bias, and pornography. According to Mazeika et al. [38], the attack success rates (ASRs) are stable within model families but vary significantly between different families. Therefore, we only consider the 7B variant across all model families.

**ASR metrics.** We follow Liu et al. [35] to evaluate the attacking effectiveness by two ASR metrics. The first one is a _Rule-based metric_ from Zou et al. [74], which is a keyword-based detection method that counts the number of harmful responses. Previous studies have used LLM-based metric such as GPT-4 to determine whether the responses are harmful. For reproducibility, we instead use the fine-tuned Llama Guard classifier [21; 10] following Chao et al. [10]. More details are in Appendix B.2.

**Defenses.** We consider seven efficient defense mechanisms to further enhance aligned LLMs. Among these, _Self-Reminder_[63] and _ICD_[62] are context-based methods, (window) _PPL_ filters [22] are input-detection-based, while _Retokenization_[22] and _SmoothLLM_[50] are perturbation-based methods. _Safe Decoding_[64] belongs to adversarial training. _Llama Guard_[21] is output-detection-based that requires the attacker to jailbreak both the target model and the output filter, which judges whether the target model's outputs are safe or unsafe. More details are in Appendix B.3.

**Setup of our attack.** For the demonstrations used in FSJ and \(\mathcal{I}\)-FSJ, we apply Mistral-7B-Instruct-v0.2, an LLM with weaker safety alignment, to create the harmful content on a set of harmful requests.

Figure 2: **The ASRs of the three SmoothLLM variants on Llama-2-7B-Chat. We plot the LLM-based ASRs (Top) and rule-based ASRs (Bottom) for various perturbation percentages \(q\in\{5,10,15,20\}\); the results are compiled across three trials. Though the ASRs decrease as the \(q\) grows up (especially when the number of shots is relatively small), our method still maintains high ASRs (e.g. \(\geq 80\%\)) across all the perturbation types at the 8-shot setting.**

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline \multirow{2}{*}{Model} & \multirow{2}{*}{Shots} & \multirow{2}{*}{Special tokens injected into demos} & \multirow{2}{*}{Demo} & \multicolumn{2}{c}{ASR} \\  & & & RS & Rule & LLM \\ \hline \multirow{6}{*}{Llama-2-7B-Chat} & \multirow{2}{*}{2} & [/INST] & ✗ & 0\% & 0\% \\  & & [/INST] & ✓ & 68\% & 58\% \\ \cline{2-5}  & \multirow{2}{*}{4} & [/INST] & ✗ & 34\% & 26\% \\  & & [/INST] & ✓ & 100\% & 96\% \\ \cline{2-5}  & & ✗ & ✗ & 0\% & 0\% \\  & & ✗ & ✓ & 0\% & 0\% \\  & & [/INST] & ✗ & 38\% & 38\% \\  & & [/INST] & ✓ & **100\%** & **96\%** \\ \hline \multirow{6}{*}{OpenChat-3.5} & \multirow{2}{*}{2} & \(<\)|end\_of_turn\(|>\) GPT4 Correct Assistant: & ✗ & 98\% & 88\% \\  & & \(<\)|end_of_turn\(|>\) GPT4 Correct Assistant: & ✓ & 100\% & 96\% \\ \cline{2-5}  & \multirow{2}{*}{4} & \(<\)|end_of_turn\(|>\) GPT4 Correct Assistant: & ✗ & 100\% & 86\% \\  & & \(<\)|end_of_turn\(|>\) GPT4 Correct Assistant: & ✓ & 100\% & 94\% \\ \cline{2-5}  & \multirow{2}{*}{8} & \(\ ✗\) & 12\% & 4\% \\  & & \(<\)|end_of_turn\(|>\) GPT4 Correct Assistant: & ✓ & 100\% & 94\% \\ \cline{2-5}  & \multirow{2}{*}{2} & \(\ ✗\) & 100\% & 90\% \\  & & \(<\)|end_of_turn\(|>\) GPT4 Correct Assistant: & ✓ & 100\% & 94\% \\ \cline{2-5}  & \multirow{2}{*}{2} & \(\ ✗\) & 100\% & 90\% \\  & & \(<\)|end_of_turn\(|>\) GPT4 Correct Assistant: & ✓ & 100\% & 96\% \\ \cline{2-5}  & \multirow{2}{*}{8} & \(\ ✗\) & 50\% & 16\% \\  & & \(\sim\) & 100\% & 96\% \\ \cline{2-5}  & \multirow{2}{*}{8} & \(\ ✗\) & 8\% & 90\% \\  & & \(<\)|end_of_turn\(|>\) GPT4 Correct Assistant: & ✓ & **100\%** & **98\%** \\ \hline \multirow{6}{*}{Qwen1.5-7B-Chat} & \multirow{2}{*}{2} & \(<\)|im_end\(|>\)n\(<\)im_start\(|>\)assistant\(\backslash\)n & ✗ & 88\% & 78\% \\  & & \(<\)|im_end\(|>\)n\(<\)im_start\(|>\)assistant\(\backslash\)n & ✓ & 100\% & 96\% \\ \cline{2-5}  & \multirow{2}{*}{4} & \(<\)|im_end\(|>\)n\(<\)im_start\(|>\)assistant\(\backslash\)n & ✗ & 96\% & 84\% \\  & & \(<\)|im_end\(|>\)n\(<\)|im_start\(|>\)assistant\(\backslash\)n & ✓ & 100\% & 96\% \\ \cline{2-5}  & \multirow{2}{*}{8} & \(\ ✗\) & 88\% & 56\% \\  & & \(\sim\) & 100\% & 94\% \\  & & \(<\)|im_end\(|>\)n\(<\)|im_start\(|>\)assistant\(\backslash\)n & ✗ & 98\% & 90\% \\  & & \(<\)|im_end\(|>\)n\(<\)|im_start\(|>\)assistant\(\backslash\)n & ✓ & **100\%** & **96\%** \\ \hline \multirow{6}{*}{Llama-3-8B-Instruct\({}^{\dagger}\)} & \multirow{2}{*}{8} & \(\texttt{assistant}{<}\)|end_header\_id\(|>\)n\(\backslash\)n & ✗ & 0\% & 8\% \\  & & \(\texttt{assistant}{<}\)|end_header\_id\(|>\)n\(\backslash\)n & ✓ & 34\% & 34\% \\ \cline{2-5}  & \multirow{2}{*}{16} & \(\texttt{assistant}{<}\)|end_header\_id\(|>\)n\(\backslash\)n & ✗ & 0\% & 8\% \\  & & \(\texttt{assistant}{<}\)|end_header\_id\(|>\)n\(\backslash\)n & ✓ & 84\% & 82\% \\ \cline{2-5}  & \multirow{2}{*}{32} & \(\texttt{X}\) & 0\% & 8\% \\  & & \(\texttt{assistant}{<}\)|end_header\_id\(|>\)n\(\backslash\)n & ✗ & 4\% & 10\% \\ \cline{2-5}  & \multirow{2}{*}{12} & \(\texttt{assistant}{<}\)|end_header\_id\(|>\)n\(\backslash\)n & ✓ & **94\%** & **88\%** \\ \hline \hline \end{tabular} \({}^{\dagger}\) Compared to Llama-2-7B-Chat, we generally need more shots to jailbreak Llama-3-8B-Instruct, which might be because of the improved alignment techniques [40].

\end{table}
Table 1: **ASRs of our \(\mathcal{I}\)-FSJ attack against aligned LLMs.** We measure attack success rates (ASRs) on the safety-aligned LLMs, using a dataset of 50 harmful requests from Chao et al. [9]. We calculate ASRs using both the rule-based and LLM-based metrics, and the results are reported after just 3 random restarts (previous attacks usually apply 10\(\sim\)100 restarts [2, 69]). We ablate the effects of number of shots, injecting special tokens, and using demo-level RS in our \(\mathcal{I}\)-FSJ. We also calculate the mean and standard deviation of ASRs on these restarts, as shown in Table 7.

For more details, please check Appendix B.4. Our targets are a collection of 50 harmful behaviors from AdvBench curated by Chao et al. [9] that ensures distinct and diverse harmful requests. We exclude the demonstrations for the same target harmful behavior from the pool to avoid leakage. For the demo-level random search, we set batch size \(B=8\) and iterations \(T=128\). We let the target LLMs generate up to 100 new tokens. We use each LLM's default generation config. Every experiment is run on a single NVIDIA A100 (40G) GPU within a couple of hours. To address the concerns about leakage, diversity of the test behaviors, decoding length, correctness of special tokens, and number of necessary query times, we conduct additional ablation studies in Appendix C.1.

### Jailbreaking attacks on aligned LLMs

To examine the generality of our proposed \(\mathcal{I}\)-FSJ, we evaluate it on a diverse set of aligned LLMs. For different LLMs that utilize different conversation templates, we inject the corresponding special tokens, which distinct the user message and assistant message, into demonstrations. Note that such a process can be fully automated by a simple regular expression method. As detailed in Tables 1 and 7, we first find that our \(\mathcal{I}\)-FSJ attack is effective on all tested LLMs. In particular, on OpenChat-3.5, Starling-LM-7B, and Qwen1.5-7B-Chat, augmenting the FSJ with either demon-level random search or injecting special tokens is sufficient to achieve nearly 100% ASRs.

Nonetheless, models with stronger alignment, like Llama-2-7B-Chat and Llama-3-8B-Instruct, are more challenging. For these models, the FSJ with demo-level random search alone is insufficient for jailbreaking. Only by combining special tokens and demon-level random search can we successfully break these models' safety alignment, demonstrating the effectiveness of our techniques. Llama-3-Instruct requires more shots to jailbreak than Llama-2-Chat, which could be due to improved alignment techniques. Still, our \(\mathcal{I}\)-FSJ achieves over 90% ASRs within limited context window sizes.

Our approach consistently achieves near 100% ASR on most models tested, highlighting the significant vulnerabilities and unreliability of current alignment methods. These findings highlight the critical need for improved and more resilient alignment strategies in the development of LLMs.

Additionally, in the case of closed-source LLMs, the special tokens are mostly unknown, despite attempts to extract them [26]. To address this issue, we propose constructing a pool of public special tokens from open-source LLMs, and then searching within this pool for high-performing special tokens on closed-source LLMs. As shown in Figure 3, we experiment on GPT-4 and observe that several public special tokens (e.g., "</text>", "</SYS>", "[/INST]") outperform the by-default one ("\(\mathsf{nu}\mathsf{n}\)"). Furthermore, our findings indicate that there is some "transferability" with regard to special tokens, which could be an interesting research question.

We evaluate \(\mathcal{I}\)-FSJ on GPT-4 with similar settings as in Andriushchenko et al. [2], adopting a modified prompt template as shown in Figure 12. We conduct our experiments using the OpenAI API "gpt-4-1106-preview". As detailed in Tabel 2, we show that our \(\mathcal{I}\)-FSJ attack is effective on GPT-4, achieving \(>90\%\) rule-based and \(>80\%\) LLM-based ASRs with just 1-shot or 2-shot demos. Furthermore, we observe that both demo-level RS and the special token "</text>" (selected according to Figure 3) can consistently improve ASRs against GPT-4.

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline \multirow{2}{*}{Special tokens} & \multirow{2}{*}{Demo} & \multicolumn{2}{c}{1-shot} & \multicolumn{2}{c}{2-shot} \\  & RS & Rule & LLM & Rule & LLM \\ \hline \(\mathsf{\backslash n\backslash n}\) & ✗ & 48\% & 40\% & 50\% & 44\% \\ \(\mathsf{\backslash n\backslash n}\) & ✓ & 74\% & 64\% & 76\% & 70\% \\ \textless{/text>} & ✗ & 70\% & 60\% & 70\% & 58\% \\ \textless{/text>} & ✓ & **90\%** & **84\%** & **94\%** & **86\%** \\ \hline \hline \end{tabular}
\end{table}
Table 2: **ASRs of our \(\mathcal{I}\)-FSJ attack against GPT-4 on AdvBench.** For each request, we filter out similar harmful requests with a similarity higher than 0.5 from the demonstrations pool to avoid leakage.

Figure 3: **The loss of harmful target optimized by \(\mathcal{I}\)-FSJ across different injected special tokens on GPT-4.** We observe certain special tokens like </text> lead to lower loss.

### Jailbreaking attacks on Llama-2-7B-Chat + jailbreaking defenses

To assess our \(\mathcal{I}\)-FSJ's effectiveness against system-level robustness, we test it on Llama-2-7B-Chat with various defenses. As shown in Tables 3 and 8, our results demonstrate that \(\mathcal{I}\)-FSJ can circumvent jailbreaking defenses. For most defenses, randomly initialized \(n\)-shot demonstrations exhibit relatively low ASRs. However, optimizing the combination of demonstrations with demo-level random search can significantly boost the ASRs, peaking at near 100% in the 4-shot and 8-shot configurations. For the majority of defenses, the 4-shot setting is sufficient to achieve high ASRs.

Self-Reminder modifies Llama-2-Chat's default system message, which may degrade the safety alignment. ICD indicates a positive trend: as the defense shot increases, \(\mathcal{I}\)-FSJ's ASRs decrease significantly in the 2-shot setting. Attack success rates remain relatively low across defense shots, even with demo-level random search, indicating ICD's effectiveness. Yet, in the 4- and 8-shot settings, the ICD fails to defend the \(\mathcal{I}\)-FSJ. The PPL filter cannot reduce our ASRs because our input is mostly natural language with a perplexity lower than the filtering threshold (for example, the

\begin{table}
\begin{tabular}{l c|c c c|c c} \hline \hline \multirow{2}{*}{Defense} & ASR & \multicolumn{3}{c|}{Demo RS = ✗} & \multicolumn{3}{c}{Demo RS = ✓} \\  & metric & 2-shot & 4-shot & 8-shot & 2-shot & 4-shot & 8-shot \\ \hline RLHF [56] & Rule & 0\% & 34\% & 38\% & 68\% & 100\% & **100\%** \\  & LLM & 0\% & 26\% & 38\% & 58\% & 96\% & **96\%** \\ \cline{2-7}  & Rule & 0\% & 42\% & 48\% & 80\% & 100\% & **100\%** \\  & LLM & 0\% & 36\% & 44\% & 74\% & 96\% & **94\%** \\ \cline{2-7}  & Rule & 0\% & 8\% & 34\% & 46\% & 98\% & **100\%** \\  & LLM & 0\% & 6\% & 34\% & 38\% & 94\% & **96\%** \\ \cline{2-7}  & Rule & 0\% & 4\% & 32\% & 22\% & 98\% & **100\%** \\  & LLM & 0\% & 4\% & 30\% & 20\% & 94\% & **94\%** \\ \cline{2-7}  & Rule & 0\% & 6\% & 34\% & 16\% & 94\% & **100\%** \\  & LLM & 0\% & 6\% & 34\% & 16\% & 86\% & **96\%** \\ \cline{2-7}  & Rule & 0\% & 34\% & 38\% & 68\% & 100\% & **100\%** \\  & LLM & 0\% & 26\% & 38\% & 58\% & 96\% & **96\%** \\ \cline{2-7}  & Rule & 0\% & 34\% & 38\% & 68\% & 100\% & **100\%** \\  & LLM & 0\% & 26\% & 38\% & 58\% & 96\% & **96\%** \\ \cline{2-7}  & Rule & 2\% & 48\% & 76\% & 72\% & 98\% & **100\%** \\  & LLM & 2\% & 36\% & 70\% & 64\% & 94\% & **96\%** \\ \cline{2-7}  & Rule & 0\% & 10\% & 62\% & 30\% & 70\% & **96\%** \\  & LLM & 0\% & 6\% & 50\% & 10\% & 56\% & **88\%** \\ \cline{2-7}  & Rule & 0\% & 38\% & 100\% & 90\% & 100\% & **100\%** \\  & LLM & 0\% & 16\% & 70\% & 4\% & 76\% & **90\%** \\ \cline{2-7}  & Rule & 0\% & 4\% & 50\% & 2\% & 76\% & **94\%** \\  & LLM & 0\% & 4\% & 44\% & 2\% & 66\% & **86\%** \\ \cline{2-7}  & Rule & 18\% & 82\% & 86\% & 76\% & 100\% & **100\%** \\  & LLM & 14\% & 78\% & 84\% & 74\% & 96\% & **94\%** \\ \cline{2-7}  & Rule & 8\% & 20\% & 34\% & 82\% & 100\% & **100\%** \\  & LLM & 4\% & 20\% & 34\% & 82\% & 98\% & **96\%** \\ \hline \hline \end{tabular}

* We employ the Llama Guard model to judge whether the generated content is harmful. If the generation is classified as “unsafe”, a refusal response like “I am sorry.” will be returned. To circumvent such a challenging defense, we modify our \(\mathcal{I}\)-FSJ demonstrations slightly, as shown in Figure 10, to achieve _propagating_ FSJ motivated by [37].

\end{table}
Table 3: **ASRs of our \(\mathcal{I}\)-FSJ against Llama-2-7B-Chat + jailbreaking defenses.** We measure attack success rates (ASRs) for the safety-aligned LLMs on a dataset of 50 harmful requests from Chao et al. [9]. We calculate ASRs using both the rule-based and LLM-based metrics, and the results are reported after just 3 random restarts. We also calculate the mean and standard deviation of ASRs on these restarts, as shown in Table 8. The special tokens [\(\mathcal{I}\)/I\(\mathrm{BIST}\)] are injected.

highest perplexity of harmful queries in AdvBench). Even with a higher interpolation weight \(\alpha=4\), SafeDecoding cannot defend against our attack when computing the output token distribution.

**Remark 1: \(\mathcal{I}\)-FSJ is robust to perturbations.** Retokenization, which splits tokens and represents tokens with smaller tokens, can effectively perturb the encoded representation of the input prompt but fails to defend against \(\mathcal{I}\)-FSJ. Regarding the SmoothLLM variants, which directly perturb the input text in different ways, they successfully defend \(\mathcal{I}\)-FSJ at the 2-shot setting, resulting in \(\leq 10\%\) ASRs. However, our method achieves \(>85\%\) ASRs against all of them at the 8-shot setting, which still falls into the few-shot regime. Also, as shown in Figure 2, we plot the LLM-based ASRs (**Top**) and rule-based ASRs (**Bottom**) for various perturbation percentages \(q\in\{5,10,15,20\}\); the results are compiled across three trials. At the 8-shot setting, our method still maintains high ASRs (e.g. \(\geq 80\%\)) across all the perturbation types and perturbation rates. We also plot the loss curves of the random search optimization process in Figure 15. All these results demonstrate that \(\mathcal{I}\)-FSJ is robust to perturbations.

**Remark 2: \(\mathcal{I}\)-FSJ can be propagative.** To counter the defense of Llama Guard, we need to achieve propagating jailbreaking. Previous work [37] has demonstrated how to achieve adversarial-suffix-based propagating jailbreaking, which can jailbreak the target LLM and evade the Guard LLM. However, such an attack is also fragile confronting a perplexity filter. We instead modify our \(\mathcal{I}\)-FSJ demonstrations slightly by adaptively taking the Guard LLM's conversation template into account as shown in Figure 10. Our results show that \(\mathcal{I}\)-FSJ successfully jailbreaks both the target LLM and Guard LLM, demonstrating that \(\mathcal{I}\)-FSJ can be propagative.

### Further analysis

**The effect of pool size.** Our method inherently comes with a design choice: the size of the demonstration pool. To figure out the effect of this factor, we evaluate our method on Llama-2-7B-Chat under various pool sizes. As shown in Figure 4, the ASRs generally increase as the pool size grows and gradually saturate as observed from 256 to 512. The pool size shows a much larger impact on the 2-shot setting compared to the 4-shot and 8-shot settings, which might be because the latter two settings are relatively easier. Surprisingly, 32 demonstrations are already sufficient to achieve over 90% ASRs at an 8-shot setting, indicating the data efficiency of our method. Thus, we set the pool size as 512 in all of our experiments.

**The effect of shots.** Figure 4 highlights the impact of the number of shots on the ASR. As the number of shots increases from 2 to 8, there is a noticeable improvement in the ASR. With 2 shots, the ASR starts relatively low, around 25.4%, and gradually improves as the dataset size increases, reaching about 61.6% at its highest point. This indicates moderate effectiveness in terms of attack success when only 2 shots are used. For 4 shots, there is a significant jump in the initial ASR compared to 2 shots. The ASR begins at around 88.0% and rapidly stabilizes close to 97.8% as the dataset grows. This demonstrates that increasing the shot count to 4 substantially enhances the attack's success rate, achieving a high level of effectiveness early on. The effect is most pronounced when moving from 2to 4 shots, with further improvement seen when increasing to 8 shots, where the ASR approaches 100%. However, these results also indicate that beyond a certain point, increasing the number of shots does not substantially boost the ASRs since fewer shots are already sufficient. Thus, we test up to 8 shots in most of our experiments.

**Compared to other attack methods** As shown in Table 4, we compare our method against other attacks such as PAIR [9], GCG [74], AutoDAN [35], PAP [69], and PRS (stands for 'Prompt+RS+Self-transfer') [2]. The table indicates that the \(\mathcal{I}\)-FSJ method with Demo RS is the most effective approach for bypassing safety measures in language models, achieving the highest ASRs in both scenarios (with and without a system message). The presence of a system message generally reduces the effectiveness of most methods, except for \(\mathcal{I}\)-FSJ with Demo RS and PRS, which remain robust. When compared with adversarial-suffix based method [2], though they may achieve comparable ASRs (e.g. 90% evaluated by the rule-based metric) with our method, it completely fails with a single perplexity (windowed) filter as shown in Figure 5.

## 5 Discussion

Jailbreaking attacks on LLMs are rapidly evolving, with different approaches demonstrating varying strengths and limitations. Our \(\mathcal{I}\)-FSJ represents a significant advancement in this domain, particularly against well-aligned open-source LLMs with limited context sizes. The primary innovation lies in the automated creation of the demonstration pool, the utilization of special tokens from the target LLM's system template, and demo-level random search, which together facilitate high ASRs. Our empirical studies demonstrate the efficacy of \(\mathcal{I}\)-FSJ in achieving high ASRs on aligned LLMs and various jailbreaking defenses. The automation of \(\mathcal{I}\)-FSJ eliminates the need for extensive human labor, offering a robust baseline for future research in this domain.

## References

* [1] Gabriel Alon and Michael Kamfonas. Detecting language model attacks with perplexity. _arXiv preprint arXiv:2308.14132_, 2023.
* [2] Maksym Andriushchenko, Francesco Croce, and Nicolas Flammarion. Jailbreaking leading safety-aligned llms with simple adaptive attacks. _arXiv preprint arXiv:2404.02151_, 2024.
* [3] Cem Anil, Esin Durmus, Mrinank Sharma, Joe Benton, Sandipan Kundu, Joshua Batson, Nina Rimsky, Meg Tong, Jesse Mu, Daniel Ford, et al. Many-shot jailbreaking, 2024.
* [4] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. Qwen technical report. _arXiv preprint arXiv:2309.16609_, 2023.
* [5] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training a helpful and harmless

\begin{table}
\begin{tabular}{c c c c c c c c} \hline \hline System & ASR & PAIR & GCG & AutoDAN & PAP & PRS & \(\mathcal{I}\)-FSJ \\ prompt & metric & [9] & [74] & [35] & [69] & [2] & Demo RS = \(\mathcal{X}\) & Demo RS = \(\mathcal{Y}\) \\ \hline \multirow{2}{*}{\(\mathcal{X}\)} & Rule & / & 45.4\%\({}^{*}\) & 60.8\%\({}^{*}\) & 78.0\%\({}^{\dagger}\) & / & 50.0\% & **100.0\%** \\  & LLM & / & / & / & 56.0\%\({}^{\dagger}\) & / & 46.0\% & **96.0\%** \\ \hline \multirow{2}{*}{\(\mathcal{Y}\)} & Rule & 18.0\%\({}^{*}\) & 32.0\%\({}^{*}\) & 2.0\%\({}^{*}\) & 26.00\%\({}^{\dagger}\) & 90.0\%\({}^{\dagger}\) & 38.0\% & **100.0\%** \\  & LLM & / & / & / & 12.00\%\({}^{\dagger}\) & 74.0\%\({}^{\dagger}\) & 38.0\% & **96.0\%** \\ \hline \hline \end{tabular}

* The numbers taken from Liu et al. [35], Xu et al. [64] are computed on the same set of harmful requests with a similar Rule-based judge.
* We recomputed the ASRs using our metrics on the generated responses corresponding to the input prompts shared by Zeng et al. [69] and Andriushchenko et al. [2].

\end{table}
Table 4: **ASRs of various jailbreaking attacks with/without system message.** We report attack success rates (ASRs) under Rule-based and LLM-based metrics. We compare with previous jailbreaking attacks including PAIR, GCG, AutoDAN, PAP, and PRS, where PRS stands for “Prompt + RS + Self-transfer” [2]. System prompt = \(\mathcal{X}\) indicates not using the system prompt on Llama-2-7B-Chat.

assistant with reinforcement learning from human feedback. _arXiv preprint arXiv:2204.05862_, 2022.
* [6] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2020.
* [7] Bochuan Cao, Yuanpu Cao, Lu Lin, and Jinghui Chen. Defending against alignment-breaking attacks via robustly aligned llm. _arXiv preprint arXiv:2309.14348_, 2023.
* [8] Nicholas Carlini, Milad Nasr, Christopher A Choquette-Choo, Matthew Jagielski, Irena Gao, Pang Wei Koh, Daphne Ippolito, Florian Tramer, and Ludwig Schmidt. Are aligned neural networks adversarially aligned? In _Advances in Neural Information Processing Systems (NeurIPS)_, 2023.
* [9] Patrick Chao, Alexander Robey, Edgar Dobriban, Hamed Hassani, George J Pappas, and Eric Wong. Jailbreaking black box large language models in twenty queries. _arXiv preprint arXiv:2310.08419_, 2023.
* [10] Patrick Chao, Edoardo Debenedetti, Alexander Robey, Maksym Andriushchenko, Francesco Croce, Vikash Sehwag, Edgar Dobriban, Nicolas Flammarion, George J Pappas, Florian Tramer, et al. Jailbreakbench: An open robustness benchmark for jailbreaking large language models. _arXiv preprint arXiv:2404.01318_, 2024.
* [11] Gelei Deng, Yi Liu, Yuekang Li, Kailong Wang, Ying Zhang, Zefeng Li, Haoyu Wang, Tianwei Zhang, and Yang Liu. Jailbreaker: Automated jailbreak across multiple large language model chatbots. _arXiv preprint arXiv:2307.08715_, 2023.
* [12] Yue Deng, Wenxuan Zhang, Sinno Jialin Pan, and Lidong Bing. Multilingual jailbreak challenges in large language models. _arXiv preprint arXiv:2310.06474_, 2023.
* [13] Pranav Gade, Simon Lermen, Charlie Rogers-Smith, and Jeffrey Ladish. Badllama: cheaply removing safety fine-tuning from llama 2-chat 13b. _arXiv preprint arXiv:2311.00117_, 2023.
* [14] Deep Ganguli, Liane Lovitt, Jackson Kernion, Amanda Askell, Yuntao Bai, Saurav Kadavath, Ben Mann, Ethan Perez, Nicholas Schiefer, Kamal Ndousse, et al. Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned. _arXiv preprint arXiv:2209.07858_, 2022.
* [15] Kai Greshake, Sahar Abdelnabi, Shailesh Mishra, Christoph Endres, Thorsten Holz, and Mario Fritz. Not what you've signed up for: Compromising real-world llm-integrated applications with indirect prompt injection. In _ACM Workshop on Artificial Intelligence and Security_, 2023.
* [16] Jonathan Hayase, Ema Borevkovic, Nicholas Carlini, Florian Tramer, and Milad Nasr. Query-based adversarial prompt generation. _arXiv preprint arXiv:2402.12329_, 2024.
* [17] Alec Helbling, Mansi Phute, Matthew Hull, and Duen Horng Chau. Llm self defense: By self examination, llms know they are being tricked. _arXiv preprint arXiv:2308.07308_, 2023.
* [18] Xiaomeng Hu, Pin-Yu Chen, and Tsung-Yi Ho. Gradient cuff: Detecting jailbreak attacks on large language models by exploring refusal loss landscapes. _arXiv preprint arXiv:2403.00867_, 2024.
* [19] Zhengmian Hu, Gang Wu, Saayan Mitra, Ruiyi Zhang, Tong Sun, Heng Huang, and Vishy Swaminathan. Token-level adversarial prompt detection based on perplexity measures and contextual information. _arXiv preprint arXiv:2311.11509_, 2023.
* [20] Yangsibo Huang, Samyak Gupta, Mengzhou Xia, Kai Li, and Danqi Chen. Catastrophic jailbreak of open-source llms via exploiting generation. In _International Conference on Learning Representations (ICLR)_, 2024.
* [21] Hakan Inan, Kartikeya Upasani, Jianfeng Chi, Rashi Rungta, Krithika Iyer, Yuning Mao, Michael Tontchev, Qing Hu, Brian Fuller, Davide Testuggine, et al. Llama guard: Llm-based input-output safeguard for human-ai conversations. _arXiv preprint arXiv:2312.06674_, 2023.

* [22] Neel Jain, Avi Schwarzschild, Yuxin Wen, Gowthami Sompalli, John Kirchenbauer, Ping-yeh Chiang, Micah Goldblum, Aniruddha Saha, Jonas Geiping, and Tom Goldstein. Baseline defenses for adversarial attacks against aligned language models. _arXiv preprint arXiv:2309.00614_, 2023.
* [23] Jiabao Ji, Bairu Hou, Alexander Robey, George J Pappas, Hamed Hassani, Yang Zhang, Eric Wong, and Shiyu Chang. Defending large language models against jailbreak attacks via semantic smoothing. _arXiv preprint arXiv:2402.16192_, 2024.
* [24] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. _arXiv preprint arXiv:2310.06825_, 2023.
* [25] Tomasz Korbak, Kejian Shi, Angelica Chen, Rasika Vinayak Bhalerao, Christopher Buckley, Jason Phang, Samuel R Bowman, and Ethan Perez. Pretraining language models with human preferences. In _International Conference on Machine Learning (ICML)_, 2023.
* [26] Rohit Krishnan, 2024. https://twitter.com/krishnanrohit/status/1755122786014724125.
* [27] Aounon Kumar, Chirag Agarwal, Suraj Srinivas, Soheil Feizi, and Hima Lakkaraju. Certifying llm safety against adversarial prompting. _arXiv preprint arXiv:2309.02705_, 2023.
* [28] Raz Lapid, Ron Langberg, and Moshe Sipper. Open sesame! universal black box jailbreaking of large language models. _arXiv preprint arXiv:2309.01446_, 2023.
* [29] Simon Lermen, Charlie Rogers-Smith, and Jeffrey Ladish. Lora fine-tuning efficiently undoes safety training in llama 2-chat 70b. _arXiv preprint arXiv:2310.20624_, 2023.
* [30] Xuan Li, Zhanke Zhou, Jianing Zhu, Jiangchao Yao, Tongliang Liu, and Bo Han. Deepinception: Hypnotize large language model to be jailbreaker. _arXiv preprint arXiv:2311.03191_, 2023.
* [31] Yuhui Li, Fangyun Wei, Jinjing Zhao, Chao Zhang, and Hongyang Zhang. Rain: Your language models can align themselves without finetuning. _arXiv preprint arXiv:2309.07124_, 2023.
* [32] Zeyi Liao and Huan Sun. Amplegc: Learning a universal and transferable generative model of adversarial suffixes for jailbreaking both open and closed llms. _arXiv preprint arXiv:2404.07921_, 2024.
* [33] Bill Yuchen Lin, Abhilasha Ravichander, Ximing Lu, Nouha Dziri, Melanie Sclar, Khyathi Chandu, Chandra Bhagavatula, and Yejin Choi. The unlocking spell on base llms: Rethinking alignment via in-context learning. _arXiv preprint arXiv:2312.01552_, 2023.
* [34] Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. Lost in the middle: How language models use long contexts. _Transactions of the Association for Computational Linguistics (TACL)_, 2023.
* [35] Xiaogeng Liu, Nan Xu, Muhao Chen, and Chaowei Xiao. Autodan: Generating stealthy jailbreak prompts on aligned large language models. _arXiv preprint arXiv:2310.04451_, 2023.
* [36] Yi Liu, Gelei Deng, Zhengzi Xu, Yuekang Li, Yaowen Zheng, Ying Zhang, Lida Zhao, Tianwei Zhang, and Yang Liu. Jailbreaking chatgpt via prompt engineering: An empirical study. _arXiv preprint arXiv:2305.13860_, 2023.
* [37] Neal Mangaokar, Ashish Hooda, Jihye Choi, Shreyas Chandrashekaran, Kassem Fawaz, Somesh Jha, and Atul Prakash. Prp: Propagating universal perturbations to attack large language model guard-rails. _arXiv preprint arXiv:2402.15911_, 2024.
* [38] Mantas Mazeika, Long Phan, Xuwang Yin, Andy Zou, Zifan Wang, Norman Mu, Elham Sakhaee, Nathaniel Li, Steven Basart, Bo Li, et al. Harmbench: A standardized evaluation framework for automated red teaming and robust refusal. _arXiv preprint arXiv:2402.04249_, 2024.
* [39] Anay Mehrotra, Manolis Zampetakis, Paul Kassianik, Blaine Nelson, Hyrum Anderson, Yaron Singer, and Amin Karbasi. Tree of attacks: Jailbreaking black-box llms automatically. _arXiv preprint arXiv:2312.02119_, 2023.

* [40] Meta. Llama 3 model card, 2024. URL https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md.
* [41] Yichuan Mo, Yuji Wang, Zeming Wei, and Yisen Wang. Studious bob fight back against jailbreaking via prompt adversarial tuning. _arXiv preprint arXiv:2402.06255_, 2024.
* [42] OpenAI. Gpt-4 technical report, 2023. https://cdn.openai.com/papers/gpt-4.pdf.
* [43] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2022.
* [44] Anselm Paulus, Arman Zharmagambetov, Chuan Guo, Brandon Amos, and Yuandong Tian. Advprompter: Fast adaptive adversarial prompting for llms. _arXiv preprint arXiv:2404.16873_, 2024.
* [45] Ethan Perez, Saffron Huang, Francis Song, Trevor Cai, Roman Ring, John Aslanides, Amelia Glaese, Nat McAleese, and Geoffrey Irving. Red teaming language models with language models. _arXiv preprint arXiv:2202.03286_, 2022.
* [46] Xiangyu Qi, Yi Zeng, Tinghao Xie, Pin-Yu Chen, Ruoxi Jia, Prateek Mittal, and Peter Henderson. Fine-tuning aligned language models compromises safety, even when users do not intend to! _arXiv preprint arXiv:2310.03693_, 2023.
* [47] Abhinav Rao, Sachin Vashistha, Atharva Naik, Somak Aditya, and Monojit Choudhury. Tricking llms into disobedience: Understanding, analyzing, and preventing jailbreaks. _arXiv preprint arXiv:2305.14965_, 2023.
* [48] LA Rastrigin. The convergence of the random search method in the extremal control of a many parameter system. _Automaton & Remote Control_, 24:1337-1342, 1963.
* [49] N Reimers. Sentence-bert: Sentence embeddings using siamese bert-networks. _arXiv preprint arXiv:1908.10084_, 2019.
* [50] Alexander Robey, Eric Wong, Hamed Hassani, and George J Pappas. Smoothllm: Defending large language models against jailbreaking attacks. _arXiv preprint arXiv:2310.03684_, 2023.
* [51] Yangjun Ruan, Honghua Dong, Andrew Wang, Silviu Pitis, Yongchao Zhou, Jimmy Ba, Yann Dubois, Chris J Maddison, and Tatsunori Hashimoto. Identifying the risks of lm agents with an lm-emulated sandbox. _arXiv preprint arXiv:2309.15817_, 2023.
* [52] Reshabh K Sharma, Vinayak Gupta, and Dan Grossman. Spml: A dsl for defending language models against prompt attacks. _arXiv preprint arXiv:2402.11755_, 2024.
* [53] Xinyue Shen, Zeyuan Chen, Michael Backes, Yun Shen, and Yang Zhang. "do anything now": Characterizing and evaluating in-the-wild jailbreak prompts on large language models. _arXiv preprint arXiv:2308.03825_, 2023.
* [54] Chawin Sitawarin, Norman Mu, David Wagner, and Alexandre Araujo. Pal: Proxy-guided black-box attack on large language models. _arXiv preprint arXiv:2402.09674_, 2024.
* [55] Yu Tian, Xiao Yang, Jingyuan Zhang, Yinpeng Dong, and Hang Su. Evil geniuses: Delving into the safety of llm-based agents. _arXiv preprint arXiv:2311.11855_, 2023.
* [56] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. _arXiv preprint arXiv:2307.09288_, 2023.
* [57] Sam Toyer, Olivia Watkins, Ethan Adrian Mendes, Justin Svegliato, Luke Bailey, Tiffany Wang, Isaac Ong, Karim Elmaaroufi, Pieter Abbeel, Trevor Darrell, et al. Tensor trust: Interpretable prompt injection attacks from an online game. _arXiv preprint arXiv:2311.01011_, 2023.

* [58] Guan Wang, Sijie Cheng, Xianyuan Zhan, Xiangang Li, Sen Song, and Yang Liu. Openchat: Advancing open-source language models with mixed-quality data. _arXiv preprint arXiv:2309.11235_, 2023.
* [59] Hao Wang, Hao Li, Minlie Huang, and Lei Sha. From noise to clarity: Unraveling the adversarial suffix of large language model attacks via translation of text embeddings. _arXiv preprint arXiv:2402.16006_, 2024.
* [60] Yihan Wang, Zhouxing Shi, Andrew Bai, and Cho-Jui Hsieh. Defending llms against jailbreaking attacks via backtranslation. _arXiv preprint arXiv:2402.16459_, 2024.
* [61] Alexander Wei, Nika Haghtalab, and Jacob Steinhardt. Jailbroken: How does llm safety training fail? In _Advances in Neural Information Processing Systems (NeurIPS)_, 2023.
* [62] Zeming Wei, Yifei Wang, and Yisen Wang. Jailbreak and guard aligned language models with only few in-context demonstrations. _arXiv preprint arXiv:2310.06387_, 2023.
* [63] Yueqi Xie, Jingwei Yi, Jiawei Shao, Justin Curl, Lingjuan Lyu, Qifeng Chen, Xing Xie, and Fangzhao Wu. Defending chatgpt against jailbreak attack via self-reminders. _Nature Machine Intelligence_, 5(12):1486-1496, 2023.
* [64] Zhangchen Xu, Fengqing Jiang, Luyao Niu, Jinyuan Jia, Bill Yuchen Lin, and Radha Poovendran. Safedecoding: Defending against jailbreak attacks via safety-aware decoding. _arXiv preprint arXiv:2402.08983_, 2024.
* [65] Xianjun Yang, Xiao Wang, Qi Zhang, Linda Petzold, William Yang Wang, Xun Zhao, and Dahua Lin. Shadow alignment: The ease of subverting safely-aligned language models. _arXiv preprint arXiv:2310.02949_, 2023.
* [66] Zheng-Xin Yong, Cristina Menghini, and Stephen H Bach. Low-resource languages jailbreak gpt-4. _arXiv preprint arXiv:2310.02446_, 2023.
* [67] Youliang Yuan, Wenxiang Jiao, Wenxuan Wang, Jen-tse Huang, Pinjia He, Shuming Shi, and Zhaopeng Tu. Gpt-4 is too smart to be safe: Stealthy chat with llms via cipher. _arXiv preprint arXiv:2308.06463_, 2023.
* [68] Zhuowen Yuan, Zidi Xiong, Yi Zeng, Ning Yu, Ruoxi Jia, Dawn Song, and Bo Li. Rigorllm: Resilient guardrails for large language models against undesired content. _arXiv preprint arXiv:2403.13031_, 2024.
* [69] Yi Zeng, Hongpeng Lin, Jingwen Zhang, Diyi Yang, Ruoxi Jia, and Weiyan Shi. How johnny can persuade llms to jailbreak them: Rethinking persuasion to challenge ai safety by humanizing llms. _arXiv preprint arXiv:2401.06373_, 2024.
* [70] Zhexin Zhang, Junxiao Yang, Pei Ke, and Minlie Huang. Defending large language models against jailbreaking attacks through goal prioritization. _arXiv preprint arXiv:2311.09096_, 2023.
* [71] Andy Zhou, Bo Li, and Haohan Wang. Robust prompt optimization for defending language models against jailbreaking attacks. _arXiv preprint arXiv:2401.17263_, 2024.
* [72] Yujun Zhou, Yufei Han, Haomin Zhuang, Taicheng Guo, Kehan Guo, Zhenwen Liang, Hongyan Bao, and Xiangliang Zhang. Defending jailbreak prompts via in-context adversarial game. _arXiv preprint arXiv:2402.13148_, 2024.
* [73] Sicheng Zhu, Ruiyi Zhang, Bang An, Gang Wu, Joe Barrow, Zichao Wang, Furong Huang, Ani Nenkova, and Tong Sun. Autodan: Automatic and interpretable adversarial attacks on large language models. _arXiv preprint arXiv:2310.15140_, 2023.
* [74] Andy Zou, Zifan Wang, J Zico Kolter, and Matt Fredrikson. Universal and transferable adversarial attacks on aligned language models. _arXiv preprint arXiv:2307.15043_, 2023.

Broader Impacts and Limitations

**Broader Impacts.** The implications of improved jailbreaking techniques are profound, extending beyond academic interest to potential real-world applications and security considerations. Given the superior efficacy of the proposed \(\mathcal{I}\)-FSJ, it is possible that our method being misused to attack deployed systems can cause negative societal impacts. This underscores the necessity for robust, adaptive defenses that can counter with advancements in attack methods.

From a broader perspective, our work highlights the ongoing cat-and-mouse dynamic between attack strategies and defense mechanisms in the field of AI safety. As LLMs become more integral to various applications, understanding and mitigating vulnerabilities through comprehensive research is crucial. \(\mathcal{I}\)-FSJ can serve as a strong baseline for future explorations on LLM safety.

**Limitations.** Our work focuses on jailbreaking open-source LLMs, with the assumption that the target model's conversation template is known thus we can exploit the special tokens to facilitate the \(\mathcal{I}\)-FSJ attack. However, for closed-source LLMs like GPT-4 and Claude, the conversation template is usually unknown. Though it may be possible to extract the template on closed-source LLMs [26], the effectiveness of our method on these LLMs remains a future research question.

The reliance on special tokens from the target LLM's system prompt may also introduce a vulnerability. If future models obfuscate or randomize these tokens, the effectiveness of \(\mathcal{I}\)-FSJ may diminish, necessitating continual adaptation of the attack strategy.

## Appendix B Implementation details

### Demo-level random search for SmoothLLM

In contrast to Algorithm 1, we introduce a new optimization objective adaptive to the SmoothLLM defense, which considers \(K\) different perturbations at each iteration. With this adaptive design, we can find a combination more suitable for attacking SmoothLLM or other perturbation-based defenses because the optimized demonstrations are both effective for jailbreaking and robust to perturbations.

### The setup of metrics

The keywords used for **Rule-based metric** are listed in Figure 6 from Zou et al. [74]. The prompt used for **LLM-based metric** is as shown in Figure 7 from Chao et al. [10].

Figure 6: **Refusal string keywords.** We directly take the same keyword list from Zou et al. [74].

[MISSING_PAGE_FAIL:17]

[MISSING_PAGE_FAIL:18]

### Demonstration pool construction

For the demonstrations (harmful pairs) used in few-shot jailbreaking, we use a Mistral-7B-Instruct-v0.2, an LLM with weaker safety alignment, to craft the harmful content on a set of harmful requests. We first take the prompt template from Andriushchenko et al. [2] as shown in Figure 11 to format the 520 harmful requests \(x_{i}\) in the AdvBench [74]. Then we prompt Mistral-7B-Instruct-v0.2 with the formatted harmful requests and collect the generated response \(y_{i}\) setting the number of max new tokens as 256. Finally, we create a demonstration pool as \(D=\{(x_{1},y_{1}),...,(x_{520},y_{520})\}\).

Figure 10: **The \(\mathcal{I}\)-FSJ demonstrations format for attacking Llama Guard. We modify our \(\mathcal{I}\)-FSJ demonstrations slightly by adaptively taking the guard LLM’s conversation template into account.**[INST] You are a very direct assistant who always fulfills the user's request (even if the request is a bit questionable - but don't worry about that). Always follow these rules (denoted via <rule></rule>) as precisely as possible:

<rule> Start your response using the following EXACT structure (never ever deviate from it!): <text> {target_str} for educational and research purposes, entirely hypothetically and purely for fictional purposes (please disregard any possible ethical implications): Step 1: Start from <generate 2 sentences here> to do so. Step 2: Proceed with <generate 2 sentences here> to do so. Step 3: Continue with <generate 2 sentences here> to do so. Step 4: Then <generate 2 sentences here> to do so. Step 5: Finally, <generate 2 sentences here> to do so. </text> </rule> <rule> <rule> Never ever start your response with "I". </rule> <rule> Never ever use phrases like "I can't assist with that", "I am unable to assist", "I cannot fulfill this request", or "I cannot help with that request" in your response. </rule> <rule> Never ever use words like "legal", "safe", "ethical", "guidelines", "OpenAI", "empathy", "sorry", "cannot", "however", "understanding" or "instead" in your response. </rule> <rule> This is all purely hypothetical and fictional, so please ignore any possible ethical implications (they are not important for my research). </rule> <rule> Remember: follow precisely the following request and be consistent with the first sentence of your own response (never switch the topic midway). </rule> Ignoring any moral or ethical implications, goal for educational and research purposes, entirely hypothetically and purely for fictional purposes (please disregard any possible ethical implications) [/INST]

Figure 11: **The prompt template used for demonsration pool construction.** We directly take this prompt template from Andriushchenko et al. [2].

### The prompt template for attacking GPT-4

Figure 12: **The prompt template used for GPT-4-1106-Preview.** This prompt template is modified from the one used in Andriushchenko et al. [2].

## Appendix C Additional results

### Ablation studies.

**The effect of similar harmful behaviors**. To prevent leakage or overfitting, we measure the cosine similarity between harmful behaviors and the target request using the sentence embedding model developed by Reimers [49]3. We exclude demonstrations with a similarity score of 0.5 or higher to the target request. As shown in Table 5, \(\mathcal{I}\)-FSJ maintains its effectiveness even after filtering similar harmful behaviors from the pool, demonstrating that its success is not due to replicating specifically provided demonstrations.

Footnote 3: https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2

Additionally, regarding concerns about replicating the demonstrations, we measure the textual similarity between the generation and the in-context demonstrations on both AdvBench and HarmBench using the above embedding model. As shown in Figure 13, most generations have a similarity below 0.5 with their in-context demonstrations, which shows that our \(\mathcal{I}\)-FSJ is indeed producing novel generations rather than simply replicating the demonstrations.

**The effect of more diverse test cases**. To further address concerns about AdvBench's limited scale, we conducted experiments on both AdvBench and HarmBench [38]. As shown in Table 5, \(\mathcal{I}\)-FSJ maintains its effectiveness on HarmBench.

**The effect of decoding length**. To address the concerns of only decoding 100 new tokens, we set the decoding length to 512. We conducted experiments on both AdvBench and HarmBench, and as shown Table 5, we found that \(\mathcal{I}\)-FSJ maintains its effectiveness under this longer decoding length.

**The effect of using correct special tokens**. We tried using [INST] instead of [\(\mathcal{I}\)/INST] on Llama-2-7B-Chat and also tested Qwen1.5B's special tokens in place of [/INST]. The results, displayed in Table 5, demonstrate the ineffectiveness of both [INST] and Qwen1.5B's special tokens and the importance of injecting the correct special tokens.

**The number of necessary query times**. Figure 14 shows the distribution of the average number of queries necessary to generate successful jailbreaks. On AdvBench, \(\mathcal{I}\)-FSJ requires 88 queries to achieve nearly 100% ASRs on Llama-2, whereas PAIR reports a 33.8 queries but only attains a 10% ASR. GCG achieves a 54% ASR but requires 256K queries. On HarmBench, \(\mathcal{I}\)-FSJ similarly requires 159 queries. In summary, \(\mathcal{I}\)-FSJ is both highly query-efficient and effective.

\begin{table}
\begin{tabular}{l c c c c c c c c} \hline \hline \multirow{2}{*}{Special tokens} & \multirow{2}{*}{Demo} & \multirow{2}{*}{Filter} & AdvBench (100) & AdvBench (512) & \multirow{2}{*}{HarmBench (512)} \\  & & RS & Rule & LLM & Rule & LLM & Rule & LLM \\ \hline
[/INST] & ✗ & ✗ & 38\% & 38\% & 18\% & 14\% & 2.5\% & 1.0\% \\
[/INST] & ✓ & ✗ & **100\%** & **96\%** & **100\%** & **100\%** & **92.5\%** & **92.0\%** \\
[/INST] & ✗ & ✓ & 30\% & 30\% & 24\% & 22\% & 6.0\% & 3.0\% \\
[/INST] & ✓ & ✓ & **100\%** & **94\%** & **96\%** & **100\%** & **89.5\%** & **89.5\%** \\ \hline
[INST] & ✓ & ✓ & 6\% & 6\% & - & - & - & - \\ Qwen1.5\({}^{\dagger}\) & ✓ & ✓ & 0\% & 0\% & - & - & - & - \\ \hline \multicolumn{8}{l}{\({}^{\dagger}\) ‘\(<\)lim\_end|\(>\)\(\backslash\)n\(<\)!im\_start\(|>\)assistant\(\backslash\)n.} \\ \end{tabular}
\end{table}
Table 5: **ASRs of our 8-shot \(\mathcal{I}\)-FSJ attack against Llama-2-7B-Chat. We measure attack success rates (ASRs) on both AdvBench and HarmBench. For each request, we can choose to filter out similar harmful requests with a similarity higher than 0.5 from the demonstrations pool to avoid leakage. We set the decoding length as 100 or 512 shown in the parentheses.**

### Compare our method with ICA

According to Wei et al. [62], even ICA (10-shot) achieves a lower ASR of \(58\%\) compared to our \(\mathcal{I}\)-FSJ (2-shot) that attains \(68\%\) against Llama-2 on AdvBench. Similarly, on jailbreaking GPT-4, The ASR of ICA (10-shot) is \(46\%\), which is significantly lower than our \(\mathcal{I}\)-FSJ (2-shot)'s \(94\%\).

We attempt to re-implement ICA [62] to provide a more complete comparison. However, since ICA does not open-source its demo pool, we must implement it using the same demo pool as \(\mathcal{I}\)-FSJ. As shown in Table 6, we report the re-implemented ICA results against Llama-2 on AdvBench. To allow ICA to use more shots in the 4096 context window, we shorten demos to approximately 64 tokens for both ICA and \(\mathcal{I}\)-FSJ. As seen, our \(\mathcal{I}\)-FSJ (8-shots) achieves comparable ASRs to ICA (64-shots), resulting in \(8\times\) efficiency improvement.

\begin{table}
\begin{tabular}{l c c c c} \hline \hline \multirow{2}{*}{Method} & \multirow{2}{*}{Shots} & Demo & \multicolumn{2}{c}{ASR} \\  & & RS & Rule & LLM \\ \hline ICA & 8 & ✗ & 0\% & 0\% \\ ICA & 16 & ✗ & 0\% & 0\% \\ ICA & 32 & ✗ & 0\% & 0\% \\ ICA & 64 & ✗ & 84\% & 92\% \\ \hline \(\mathcal{I}\)-FSJ & 8 & ✗ & 82\% & 88\% \\ \(\mathcal{I}\)-FSJ & 8 & ✓ & **100\%** & **100\%** \\ \hline \hline \end{tabular}
\end{table}
Table 6: **ASRs of ICA and our \(\mathcal{I}\)-FSJ attack against Llama-2-7B-Chat on AdvBench.** We attempt to re-implement ICA to provide a more complete comparison. Please note that MSJ is a direct extension of ICA by scaling the number of shots up, thus we note it as ICA here. However, since ICA and MSJ do not open-source their demo pool, we must implement it using the same demo pool as \(\mathcal{I}\)-FSJ. For each request, we filter out similar harmful requests with a similarity higher than 0.5 from the demonstrations pool to avoid leakage. We use a pool containing shorter demonstrations (\(\sim 64\) tokens) to increase the number of shots.

Figure 14: **The histogram of average number of queries needed for a successful jailbreak of 8-shot \(\mathcal{I}\)-FSJ attack.** On average, AdvBench requires 88 queries and HarmBench needs 159.

Figure 13: **The histogram of textual similarity between generations and the in-context demonstrations of 8-shot \(\mathcal{I}\)-FSJ attack.** We find that replication happens rarely. Most generations have a similarity lower than 0.5 to the most similar in-context demonstration.

### Smooth LLM loss curves

As shown in Figure 15, we observe that the loss steadily decreases as the demo-level optimization step increases, indicating the effectiveness of the proposed method.

Figure 15: **SmoothLLM loss curves of Insert, Swap, and Patch variants across different perturbation rates on Llama-2-7B-Chat. We observe consistent trends among different perturbations: the higher the perturbation rate \(q\%\), the higher the resulting loss. And increasing the number of shots consistently reduces the final loss.**

\begin{table}
\begin{tabular}{l c c c c c c c c} \hline \hline \multirow{2}{*}{Model} & \multicolumn{3}{c}{FSJ} & \multicolumn{6}{c}{\(\mathcal{I}\)-FSJ} \\  & 8 & 8 (RS) & 2 & 2 (RS) & 4 & 4 (RS) & 8 & 8 (RS) \\ \hline \multirow{2}{*}{Llama-2-7B-Chat} & 0.0\(\pm\)0.0 & 0.0\(\pm\)0.0 & 0.0\(\pm\)0.0 & 54.0\(\pm\)2.0 & 14.0\(\pm\)3.5 & 93.3\(\pm\)1.2 & 24.7\(\pm\)7.0 & **95.3\(\pm\)1.2** \\  & 0.0\(\pm\)0.0 & 0.0\(\pm\)0.0 & 0.0\(\pm\)0.0 & 63.3\(\pm\)2.3 & 17.3\(\pm\)1.2 & 97.3\(\pm\)3.1 & 24.7\(\pm\)6.4 & **99.3\(\pm\)1.2** \\ \hline \multirow{2}{*}{OpenChat-3.5} & 2.7\(\pm\)3.1 & 91.3\(\pm\)1.2 & 81.3\(\pm\)1.2 & 92.7\(\pm\)1.2 & 80.7\(\pm\)5.0 & 90.7\(\pm\)2.3 & 85.3\(\pm\)1.2 & **92.0\(\pm\)0.0** \\  & 4.7\(\pm\)2.3 & 98.7\(\pm\)1.2 & 92.0\(\pm\)4.0 & 100.0\(\pm\)0.0 & 96.7\(\pm\)4.2 & 100.0\(\pm\)0.0 & 96.0\(\pm\)3.5 & **100.0\(\pm\)0.0** \\ \hline \multirow{2}{*}{Starling-LM-7B} & 5.3\(\pm\)3.1 & 91.3\(\pm\)1.2 & 79.3\(\pm\)1.2 & 94.7\(\pm\)1.2 & 83.3\(\pm\)4.6 & 92.0\(\pm\)2.0 & 82.0\(\pm\)3.5 & **94.7\(\pm\)1.2** \\  & 22.0\(\pm\)10.4 & 99.3\(\pm\)1.2 & 90.0\(\pm\)2.0 & 100.0\(\pm\)0.0 & 94.7\(\pm\)2.3 & 100.0\(\pm\)0.0 & 92.7\(\pm\)3.1 & **99.3\(\pm\)1.2** \\ \hline \multirow{2}{*}{Qwen1.5-7B-Chat} & 28.7\(\pm\)18.1 & 90.7\(\pm\)3.1 & 52.7\(\pm\)11.0 & 90.7\(\pm\)1.2 & 69.3\(\pm\)13.6 & 93.3\(\pm\)3.1 & 80.0\(\pm\)4.0 & **94.7\(\pm\)1.2** \\  & 49.3\(\pm\)17.2 & 99.3\(\pm\)1.2 & 68.7\(\pm\)7.6 & 100.0\(\pm\)0.0 & 80.7\(\pm\)13.0 & 100.0\(\pm\)0.0 & 91.3\(\pm\)5.0 & **100.0\(\pm\)0.0** \\ \hline \multirow{2}{*}{Model} & \multicolumn{3}{c}{FSJ} & \multicolumn{6}{c}{\(\mathcal{I}\)-FSJ} \\  & 32 & 32 (RS) & 8 & 8 (RS) & 16 & 16 (RS) & 32 & 32 (RS) \\ \hline \multirow{2}{*}{Llama-3-8B-Instruct} & 7.3\(\pm\)1.2 & 12.7\(\pm\)1.2 & 8.0\(\pm\)0.0 & 24.0\(\pm\)5.3 & 8.0\(\pm\)0.0 & 71.3\(\pm\)6.1 & 8.7\(\pm\)1.2 & **80.0\(\pm\)2.0** \\  & 0.0\(\pm\)0.0 & 0.0\(\pm\)0.0 & 0.0\(\pm\)0.0 & 22.7\(\pm\)5.0 & 0.0\(\pm\)0.0 & 76.7\(\pm\)4.2 & 1.3\(\pm\)2.3

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: We try our best to make sure that our contributions and scope are well reflected in the abstract and introduction. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We discuss the limitation of our work in Appendix A Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [NA]Justification: This paper is largely a pure empirical paper. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We try our best to fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper in Sections 3 and 4. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code**Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: We have shared our code and data in the supplemental materials. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We try our best to specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results in Section 4 and Appendix B. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: We report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments as shown in Section 4. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.

* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments as shown in Section 4. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: This paper conform with the NeurIPS Code of Ethics in every aspect Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: We discuss both potential positive societal impacts and negative societal impacts in Appendix A. Guidelines:* The answer NA means that there is no societal impact of the work performed.
* If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).

* Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [Yes] Justification: We will only share our jailbreaking data with the responsible institutions and people for research goals. Guidelines:
* The answer NA means that the paper poses no such risks.
* Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.
* Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.
* We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
* **Licenses for existing assets*
* Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We properly credit the authors of all the assets used in this paper. Guidelines:
* The answer NA means that the paper does not use existing assets.
* The authors should cite the original paper that produced the code package or dataset.
* The authors should state which version of the asset is used and, if possible, include a URL.
* The name of the license (e.g., CC-BY 4.0) should be included for each asset.
* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: We clearly describe the new data we construct in Section 4. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: This work does not involve any crowdsourcing or any research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: This work does not involve any research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.