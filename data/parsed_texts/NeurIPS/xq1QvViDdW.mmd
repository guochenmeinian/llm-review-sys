# Beyond Unimodal: Generalising Neural Processes for Multimodal Uncertainty Estimation

 Myong Chol Jung

Monash University

david.jung@monash.edu

&He Zhao

CSIRO's Data61

he.zhao@ieee.org

&Joanna Dipnall

Monash University

jo.dipnall@monash.edu

&Lan Du

Monash University

lan.du@monash.edu

Corresponding author

###### Abstract

Uncertainty estimation is an important research area to make deep neural networks (DNNs) more trustworthy. While extensive research on uncertainty estimation has been conducted with unimodal data, uncertainty estimation for multimodal data remains a challenge. Neural processes (NPs) have been demonstrated to be an effective uncertainty estimation method for unimodal data by providing the reliability of Gaussian processes with efficient and powerful DNNs. While NPs hold significant potential for multimodal uncertainty estimation, the adaptation of NPs for multimodal data has not been carefully studied. To bridge this gap, we propose Multimodal Neural Processes (MNPs) by generalising NPs for multimodal uncertainty estimation. Based on the framework of NPs, MNPs consist of several novel and principled mechanisms tailored to the characteristics of multimodal data. In extensive empirical evaluation, our method achieves state-of-the-art multimodal uncertainty estimation performance, showing its appealing robustness against noisy samples and reliability in out-of-distribution detection with faster computation time compared to the current state-of-the-art multimodal uncertainty estimation method.

## 1 Introduction

Uncertainty estimation of deep neural networks (DNNs) is an essential research area in the development of reliable and well-calibrated models for safety-critical domains [49; 45]. Despite the remarkable success achieved by DNNs, a common issue with these model is their tendency to make overconfident predictions for both in-distribution (ID) and out-of-distribution (OOD) samples [16; 43; 35]. Extensive research has been conducted to mitigate this problem, but most of these efforts are limited to unimodal data, neglecting the consideration of multimodal data [57; 11; 50; 63; 38; 34].

In many practical scenarios, safety-critical domains typically involve the processing of multimodal data. For example, medical diagnosis classification that uses X-ray, radiology text reports and the patient's medical record history data can be considered as a form of multimodal learning that requires trustworthy predictions [8]. Despite the prevalence of such multimodal data, the problem of uncertainty estimation for multimodal data has not been comprehensively studied. Furthermore, it has been demonstrated that using existing unimodal uncertainty estimation techniques directly for multimodal data is ineffective, emphasising the need for a meticulous investigation of multimodal uncertainty estimation techniques [24; 17].

Several studies investigated the problem of multimodal uncertainty estimation with common tasks including 1) assessing calibration performance, 2) evaluating robustness to noisy inputs, and 3) detecting OOD samples. Trusted Multi-view Classification (TMC) [17] is a single-forward deterministic classifier that uses the Dempster's combination rule [7] to combine predictions obtained from different modalities. The current state-of-the-art (SOTA) in this field is Multi-view Gaussian Process (MGP) [24], which is a non-parametric Gaussian process (GP) classifier that utilises the product-of-experts to combine predictive distributions derived from multiple modalities. Although MGP has shown reliability of GPs in this context, it should be noted that the computational cost of a GP increases cubically with the number of samples [19, 65].

Neural processes (NPs) offer an alternative approach that utilises the representation power of DNNs to imitate the non-parametric behaviour of GPs while maintaining a lower computational cost [13, 14]. It has been shown that NPs can provide promising uncertainty estimation for both regression [28, 33, 29] and classification tasks [62, 25] involving unimodal data. Despite the promising potential of NPs for multimodal data, to the best of our knowledge, no research has yet investigated the feasibility of using NPs for multimodal uncertainty estimation.

In this work, we propose a new multimodal uncertainty estimation framework called Multimodal Neural Processes (MNPs) by generalising NPs for multimodal uncertainty estimation. MNPs have three key components: the dynamic context memory (DCM) that efficiently stores and updates informative training samples, the multimodal Bayesian aggregation (MBA) method which enables a principled combination of multimodal latent representations, and the adaptive radial basis function (RBF) attention mechanism that facilitates well-calibrated predictions. Our contributions are:

1. We introduce a novel multimodal uncertainty estimation method by generalising NPs that comprise the DCM, the MBA, and the adaptive RBF attention.
2. We conduct rigorous experiments on seven real-world datasets and achieve the new SOTA performance in classification accuracy, calibration, robustness to noise, and OOD detection.
3. We show that MNPs achieve faster computation time (up to 5 folds) compared to the current SOTA multimodal uncertainty estimation method.

## 2 Background

Multimodal ClassificationThe aim of this research is to investigate uncertainty estimation for multimodal classification. Specifically, we consider a multimodal training dataset \(D_{train}=\{\{x_{i}^{m}\}_{m=1}^{M},y_{i}\}_{i=1}^{N_{train}}\), where \(M\) is the number of input modalities and \(N_{train}\) is the number of training samples. We assume that each \(i^{th}\) sample has \(M\) modalities of input \(x_{i}^{m}\in\mathbb{R}^{d^{m}}\) with the input dimension of \(d^{m}\) and a single one-hot encoded label \(y_{i}\) with the number of classes \(K\). In this study, we consider the input space to be a feature space. The objective is to estimate the labels of test samples \(\{y_{i}\}_{i=1}^{N_{test}}\) given \(D_{test}=\{\{x_{i}^{m}\}_{m=1}^{M}\}_{i=1}^{N_{test}}\), where \(N_{test}\) is the number of test samples.

Neural ProcessesNPs are stochastic processes using DNNs to capture the ground truth stochastic processes that generate the given data [13]. NPs learn the distribution of functions and provide uncertainty of target samples, preserving the property of GPs. At the same time, NPs exploit function approximation of DNNs in a more efficient manner than GPs. For NPs, both training and test datasets have a context set \(C=\{C_{X},C_{Y}\}=\{x_{i}^{C},y_{i}^{C}\}_{i=1}^{N_{C}}\) and a target set \(T=(T_{X},T_{Y})=\{x_{i}^{T},y_{i}^{T}\}_{i=1}^{N_{T}}\) with \(N_{C}\) being the number of context samples and \(N_{T}\) the number of target samples2, and the learning objective of NPs is to maximise the likelihood of \(p(T_{Y}|C,T_{X})\).

Footnote 2: The modality notation is intentionally omitted as the original NPs are designed for unimodal inputs [13, 14].

Conditional Neural Processes (CNPs), the original work of NPs [13], maximise \(p(T_{Y}|r(C),T_{X})\) with \(r(C)=\frac{1}{N_{C}}\sum_{i=1}^{N_{C}}\text{enc}_{\rho}(cat\left[x_{i}^{C};y_{ i}^{C}\right])\in\mathbb{R}^{d_{c}}\) where \(\text{enc}_{\rho}\) is an encoder parameterised by \(\rho\), \(cat[\cdot;\cdot]\) is the concatenation of two vectors along the feature dimension, and \(d_{e}\) is the feature dimension. The mean vector \(r(C)\) is the permutation invariant representation that summarises the context set which is passed to a decoder with \(T_{X}\) to estimate \(T_{Y}\).

The original CNPs use the unweighted mean operation to obtain \(r(C)\) by treating all the context points equally, which has been shown to be underfitting [28]. To improve over this, Attentive NeuralProcesses (ANPs) [28] leveraged the scaled dot-product cross-attention [60] to create target-specific context representations that allocate higher weight on the closer context points:

\[r_{*}(C,T_{X})=\underbrace{\text{Softmax}(T_{X}C_{X}^{T}/\sqrt{d})}_{A(T_{X},C_{ X})\in\mathbb{R}^{N_{T}\times N_{C}}}\underbrace{\text{enc}_{\rho}(cat[C_{X};C_{Y}])}_{ \mathbb{R}^{N_{C}\times d_{e}}}\] (1)

where \(A(T_{X},C_{X})\) is the attention weight with \(T_{X}\) as the query, \(C_{X}\) is the key, the encoded context set is the value, and \(d\) is the input dimension of \(T_{X}\) and \(C_{X}\). The resulted target-specific context representation has been shown to enhance the expressiveness of the task representation [28, 53, 29, 9, 47, 67].

The training procedure of NPs often randomly splits a training dataset to the context and the target sets (i.e., \(N_{C}+N_{T}=N_{train}\)). In the inference stage, a context set is provided for the test/target samples (e.g., a support set in few-shot learning or unmasked patches in image completion). While the context set for the test dataset is given for these tasks, NPs also have been applied to other tasks where the context set is not available (e.g., semi-supervised learning [62] and uncertainty estimation for image classification [25]). In those tasks, existing studies have used a _context memory_ during inference, which is composed of training samples that are updated while training [25, 62]. It is assumed that the context memory effectively represents the training dataset with a smaller number of samples. We build upon these previous studies by leveraging the concept of context memory in a more effective manner, which is suitable for our task.

Note that the existing NPs such as ETP [25] can be applied to multimodal classification by concatenating multimodal features into a single unimodal feature. However, in Section 5.1, we show that this approach results in limited robustness to noisy samples.

## 3 Multimodal Neural Processes

The aim of this study is to generalise NPs to enable multimodal uncertainty estimation. In order to achieve this goal, there are significant challenges that need to be addressed first. Firstly, it is essential to have an efficient and effective context memory for a classification task as described in Section 2. Secondly, a systematic approach of aggregating multimodal information is required to provide unified predictions. Finally, the model should provide well-calibrated predictions without producing overconfident estimates as described in Section 1.

Our MNPs, shown in Figure 1, address these challenges with three key components respectively: 1) the dynamic context memory (DCM) (Section 3.1), 2) the multimodal Bayesian aggregation (MBA) (Section 3.2), and 3) the adaptive radial basis function (RBF) attention (Section 3.3). From Section 3.1 to 3.3, we elaborate detailed motivations and proposed solutions of each challenge, and

Figure 1: Model diagram of MNPs: DCM refers to Dynamic Context Memory, MBA refers to Multimodal Bayesian Aggregation, and RBF refers to radial basis function.

Section 3.4 outlines the procedures for making predictions. Throughout this section, we refer the multimodal target set \(T^{M}=(\{T^{n}_{X}\}_{m=1}^{M},T_{Y})=\{\{x^{m}_{i}\}_{m=1}^{M},y_{i}\}_{i=1}^{N_ {T}}\) to the samples from training and test datasets and the multimodal context memory \(C^{M}=\{C^{m}_{X},C^{m}_{Y}\}_{m=1}^{M}\) to the context set.

### Dynamic Context Memory

MotivationIn NPs, a context set must be provided for a target set, which however is not always possible for non-meta-learning tasks during inference. One simple approach to adapt NPs to these tasks is to randomly select context points from the training dataset, but its performance is suboptimal as randomly sampling a few samples may not adequately represent the entire training distribution. Wang et al. [62] proposed an alternative solution by introducing a first-in-first-out (FIFO) memory that stores the context points with the predefined memory size. Although the FIFO performs slightly better than the random sampling in practice, the performance is still limited because updating the context memory is independent of the model's predictions. Refer to Appendix C.1 for comparisons.

Proposed SolutionTo overcome this limitation of the existing context memory, we propose a simple and effective updating mechanism for the context memory in the training process, which we call Dynamic Context Memory (DCM). We partition context memory \(\{C^{m}_{X},C^{m}_{Y}\}\) of \(m^{th}\) modality into \(K\) subsets (\(K\) as the number of classes) as \(\{C^{m}_{X},C^{m}_{Y}\}=\{C^{m}_{X,k},C^{m}_{Y,k}\}_{k=1}^{K}\) to introduce a class-balance context memory, with each subset devoted to one class. Accordingly, \(N^{m}=N^{m}_{K}\times K\) where \(N^{m}\) is the number of context elements per modality (i.e., the size of \(\{C^{m}_{X},C^{m}_{Y}\}\)) and \(N^{m}_{K}\) is the number of class-specific context samples per modality. This setting resembles the classification setting in [13], where class-specific context representations are obtained for each class.

DCM is initialised by taking \(N^{m}_{K}\) random training samples from each class of each modality and is updated every mini-batch during training by replacing the least "informative" element in the memory with a possibly more "informative" sample. We regard the element in DCM that receives the smallest attention weight (i.e., \(A(T_{X},C_{X})\) in Equation (1)) during training as the least informative one and the target point that is most difficult to classify (i.e., high classification error) as a more informative sample that should be added to DCM to help the model learn the decision boundary. Formally, this can be written as:

\[C^{m}_{X,k}[i^{*},:]=T^{m}_{X}[j^{*},:],\quad k\in\{1,\dots K\},\quad m\in\{1, \dots M\}\] (2)

\[i^{*}=\underset{i\in\{1,\dots,N^{m}_{K}\}}{\text{argmin}}\frac{1}{N_{T}}\sum_ {t=1}^{N_{T}}\underbrace{A(T^{m}_{X},C^{m}_{X,k})}_{\mathbb{R}^{N_{T}\times N^ {m}_{K}}}[t,i]\,,\,j^{*}=\underset{j\in\{1,\dots,N_{T}\}}{\text{argmax}} \frac{1}{K}\sum_{k=1}^{K}\left(T_{Y}[j,k]-\widehat{T}^{m}_{Y}[j,k]\right)^{2}\] (3)

where \([i,:]\) indicates the \(i^{th}\) row vector of a matrix, \([i,j]\) indicates the \(i^{th}\) row and the \(j^{th}\) column element of a matrix, and \(\widehat{T}^{m}_{Y}\) is the predicted probability of the \(m^{th}\) modality input. \(i^{*}\) selects the context memory element that receives the least average attention weight in a mini-batch during training, and \(j^{*}\) selects the target sample with the highest classification error. To measure the error between the predictive probability and the ground truth of a target sample, one can use mean squared error (MSE) (Equation (3)) or cross-entropy loss (CE). We empirically found that the former gives better performance in our experiments. Refer to Appendix C.1 for ablation studies comparing the two and the impact of \(N^{m}\) on its performance.

The proposed updating mechanism is very efficient as the predictive probability and the attention weights in Equation (3) are available with no additional computational cost. Also, in comparison to random sampling which requires iterative sampling during inference, the proposed approach is faster in terms of inference wall-clock time since the updated DCM can be used without any additional computation (refer to Appendix C.1 for the comparison).

### Multimodal Bayesian Aggregation

MotivationWith DCM, we obtain the encoded context representations for the \(m^{th}\) modality input as follows:

\[r^{m}=\text{enc}^{m}_{\psi}(cat[C^{m}_{X};C^{m}_{Y}])\in\mathds{R}^{N^{m} \times d_{e}},\quad s^{m}=\text{enc}^{m}_{\psi}(cat[C^{m}_{X};C^{m}_{Y}])\in \mathds{R}^{N^{m}\times d_{e}}\] (4)where \(\phi\) and \(\psi\) are the encoders' parameters for the \(m^{th}\) modality. Next, given the target set \(T_{X}^{m}\), we compute the target-specific context representations with the attention mechanism:

\[r_{*}^{m}=A(T_{X}^{m},C_{X}^{m})r^{m}\in\mathbbm{R}^{N_{T}\times d_{e}},\quad s_ {*}^{m}=A(T_{X}^{m},C_{X}^{m})s^{m}\in\mathbbm{R}^{N_{T}\times d_{e}}\] (5)

where the attention weight \(A(T_{X}^{m},C_{X}^{m})\in\mathbbm{R}^{N_{T}\times N^{m}}\) can be computed by any scoring function without loss of generality. Note that a single \(i^{th}\) target sample consists of multiple representations from \(M\) modalities \(\{r_{*,i}^{m}=r_{*}^{m}[i,:]\in\mathbbm{R}^{d_{e}}\}_{m=1}^{M}\). It is important to aggregate these multiple modalities into one latent variable/representation \(z_{i}\) for making a unified prediction of the label.

Proposed SolutionInstead of using a deterministic aggregation scheme, we propose Multimodal Bayesian Aggregation (MBA), inspired by [61]. Specifically, we view \(r_{*,i}^{m}\) as a sample from a Gaussian distribution with mean \(z_{i}\): \(p(r_{*,i}^{m}|z_{i})=\mathcal{N}\left(r_{*,i}^{m}|z_{i},\text{diag}(s_{*,i}^{m })\right)\) where \(s_{*,i}^{m}=s_{*}^{m}[i,:]\in\mathbbm{R}^{d_{e}}\) is the \(i^{th}\) sample in \(s_{*}^{m}\). Additionally, we impose an informative prior on \(z_{i}\): \(p(z_{i})=\prod_{m=1}^{M}\mathcal{N}\left(u^{m},\text{diag}(q^{m})\right)\) with the mean context representations of \(u^{m}\in\mathbbm{R}^{d_{e}}\) and \(q^{m}\in\mathbbm{R}^{d_{e}}\), which assign uniform weight across the context set as follows:

\[u^{m}=\frac{1}{N^{m}}\sum_{i=1}^{N^{m}}\text{enc}_{\theta}^{m}(cat\left[C_{X} ^{m}[i,:];C_{Y}^{m}[i,:]\right]),\,q^{m}=\frac{1}{N^{m}}\sum_{i=1}^{N^{m}} \text{enc}_{\omega}^{m}(cat\left[C_{X}^{m}[i,:];C_{Y}^{m}[i,:]\right])\] (6)

where \(\theta\) and \(\omega\) are the encoders' parameters for the \(m^{th}\) modality. Note that the encoders in Equation (4) and (6) are different because they approximate different distribution parameters.

**Lemma 3.1** (Gaussian posterior distribution with factorised prior distribution).: _If we have \(p(x_{i}|\mu)=\mathcal{N}(x_{i}|\mu,\Sigma_{i})\) and \(p(\mu)=\prod_{i=1}^{n}\mathcal{N}(\mu_{0,i},\Sigma_{0,i})\) for \(n\) i.i.d. observations of \(D\) dimensional vectors, then the mean and covariance of posterior distribution \(p(\mu|x)=\mathcal{N}(\mu|\mu_{n},\Sigma_{n})\) are:_

\[\Sigma_{n}=\left[\sum_{i=1}^{n}\left(\Sigma_{i}^{-1}+\Sigma_{0,i}^{-1}\right) \right]^{-1},\quad\mu_{n}=\Sigma_{n}\left[\sum_{i=1}^{n}\left(\Sigma_{i}^{-1} x_{i}+\Sigma_{0,i}^{-1}\mu_{0,i}\right)\right]\] (7)

As both \(p(z_{i})\) and \(p(r_{*,i}^{m}|z_{i})\) are Gaussian distributions, the posterior of \(z_{i}\) is also a Gaussian distribution: \(p(z_{i}|r_{*,i})=\mathcal{N}\left(z_{i}|\mu_{z_{i}},\text{diag}(\sigma_{z_{i}} ^{2})\right)\), whose mean and variance are obtained by using Lemma 3.1 as follows:

\[\sigma_{z_{i}}^{2}=\left[\sum_{m=1}^{M}\left((s_{*,i}^{m})^{\odot}+(q^{m})^{ \odot}\right)\right]^{\odot},\mu_{z_{i}}=\sigma_{z_{i}}^{2}\otimes\left[\sum_ {m=1}^{M}\left(r_{*,i}^{m}\otimes(s_{*,i}^{m})^{\odot}+u^{m}\otimes(q^{m})^{ \odot}\right)\right]\] (8)

where \(\odot\) and \(\otimes\) are element-wise inverse and element-wise product respectively (see Appendix A for proof).

We highlight that if the variance \(s_{*,i}^{m}\) of a modality formed by the target-specific context representation is high in \(\sigma_{z_{i}}^{2}\), \(q^{m}\) formed by the mean context representations dominates the summation of the two terms. Also, if both \(s_{*,i}^{m}\) and \(q^{m}\) are high for a modality, the modality's contribution to the summation over modalities is low. By doing so, we minimise performance degradation caused by uncertain modalities (see Section 5.1 for its robustness to noisy samples). Refer to Appendix C.2 for ablation studies on different multimodal aggregation methods.

### Adaptive RBF Attention

MotivationThe attention weight \(A(T_{X}^{m},C_{X}^{m})\) in Equation (3) and (5) can be obtained by any attention mechanism. The dot-product attention is one of the simple and efficient attention mechanisms, which has been used in various NPs such as [28; 53; 9; 47]. However, we have observed that it is not suitable for our multimodal uncertainty estimation problem, as the dot-product attention assigns excessive attention to context points even when the target distribution is far from the context distribution. The right figure in Figure 1(a) shows the attention weight in Equation (1) with \(A(x_{OOD}^{m},C_{X}^{m})\) where \(x_{OOD}^{m}\) is a single OOD target sample (grey sample) far from the context set (red and blue samples). This excessive attention weight results in overconfident predictive probability for OOD samples (see the left figure of Figure 1(a)), which makes the predictive uncertainty of a classifier hard to distinguish the ID and OOD samples.

Proposed SolutionTo address the overconfident issue of the dot-product attention, we propose an attention mechanism based on RBF. RBF is a stationary kernel function that depends on the relative distance between two points (i.e., \(x-x^{\prime}\)) rather than the absolute locations [65], which we define as \(\kappa^{m}\left(x,x^{\prime}\right)=\exp\left(-\frac{1}{2}||\frac{x-x^{\prime}} {(l^{m})^{2}}||^{2}\right)\) where \(||\cdot||^{2}\) is the squared Euclidean norm, and \(l^{m}\) is the lengthscale parameter that controls the smoothness of the distance in the \(m^{th}\) modality input space. The RBF kernel is one of the widely used kernels in GPs [64, 65], and its adaptation with DNNs has shown well-calibrated and promising OOD detection [40, 41, 58, 24]. Formally, we define the attention weight using the RBF kernel as:

\[A(T_{X}^{m},C_{X}^{m})=\text{Sparsemax}(G(T_{X}^{m},C_{X}^{m}))\] (9)

where the elements of \(G(T_{X}^{m},C_{X}^{m})\in\mathbb{R}^{N_{T}\times N^{m}}\) are \([G]_{ij}=\kappa^{m}(T_{X}^{m}[i,:],C_{X}^{m}[j,:])\), and Sparsemax [39] is an alternative activation function to Softmax. It is defined as \(\text{Sparsemax}(h):=\underset{p\in\Delta^{K-1}}{\text{argmax}}||p-h||^{2}\) where \(\Delta^{K-1}\) is the \(K-1\) dimensional simplex \(\Delta^{K-1}:=\{p\in\mathbb{R}^{K}|\mathbb{1}^{\mathsf{T}}p=1,p\geq\mathbb{0}\}\). Here we use Sparsemax instead of the standard Softmax because Sparsemax allows zero-probability outputs. This property is desirable because the Softmax's output is always positive even when \(\kappa^{m}(x,x^{\prime})=0\) (i.e., \(||x-x^{\prime}||^{2}\rightarrow\infty\)) leading to higher classification and calibration errors (see Appendix C.3 for ablation studies).

The lengthscale \(l^{m}\) is an important parameter that determines whether two points are far away from each other (i.e., \(\kappa^{m}(x,x^{\prime})\to 0\)) or close to each other (i.e., \(0<\kappa^{m}(x,x^{\prime})\leq 1\)). However, in practice, \(l^{m}\) has been either considered as a non-optimisable hyperparameter or an optimisable parameter that requires a complex initialisation [24, 59, 58, 42, 65, 56]. To address this issue, we propose an adaptive learning approach of \(l^{m}\) to form a tight bound to the context distribution by leveraging the supervised contrastive learning [27]. Specifically, we let anchor index \(i\in T_{ind}\equiv\{1,\ldots,N_{T}\}\) with

Figure 2: Predictive probability of upper circle samples (blue) is shown in the left column with (a) the dot-product attention, (b) the RBF attention without \(\mathcal{L}_{RBF}\), (c) and the RBF attention with \(\mathcal{L}_{RBF}\). Upper circle samples (blue) are class 1, lower circle samples (red) are class 2, and grey samples are OOD samples. The attention weight \(A(x_{OOD}^{m},C_{X}^{m})\) of an OOD sample across 100 context points is shown in the right column. The summarised steps of training MNPs are shown in (d). Refer to Appendix B for the experimental settings.

negative indices \(N(i)=T_{ind}\backslash\{i\}\) and positive indices \(P(i)=\{p\in N(i):T_{Y}[p,:]=T_{Y}[i,:]\}\). Given an anchor sample of the target set, the negative samples refer to all samples except for the anchor sample, while the positive samples refer to other samples that share the same label as the anchor sample. We define the multimodal supervised contrastive loss as:

\[\mathcal{L}^{M}_{CL}=\frac{1}{M}\sum_{m=1}^{M}\mathcal{L}^{m}_{CL}=\frac{1}{M }\sum_{m=1}^{M}\sum_{i=1}^{N_{T}}-\frac{1}{|P(i)|}\times\sum_{p\in P(i)}\log \frac{\exp\left(\kappa^{m}(T^{m}_{X}[i,:],T^{m}_{X}[p,:])/\tau\right)}{\sum_{n \in N(i)}\exp\left(\kappa^{m}(T^{m}_{X}[i,:],T^{m}_{X}[n,:])/\tau\right)}\] (10)

where \(|P(i)|\) is the cardinality of \(P(i)\), and \(\tau\) is the temperature scale. This loss encourages higher RBF output of two target samples from the same class and lower RBF output of two target samples from different classes by adjusting the lengthscale. In addition to \(\mathcal{L}^{M}_{CL}\), a \(l_{2}\)-loss is added to form the tighter bound by penalising large lengthscale. Overall, the loss term for our adaptive RBF attention is \(\mathcal{L}_{RBF}=\mathcal{L}^{M}_{CL}+\alpha*\frac{1}{M}\sum_{m=1}^{M}||l^{ m}||\) with the balancing coefficient \(\alpha\).

We show the difference between the RBF attention without \(\mathcal{L}_{RBF}\) (see Figure 2b) and the adaptive RBF attention with \(\mathcal{L}_{RBF}\) (see Figure 2c). It can be seen that the decision boundary modelled by the predictive probability of the adaptive RBF attention is aligned with the data distribution significantly better than the non-adaptive one. For ablation studies with real-world datasets, see Appendix C.4.

### Conditional Predictions

We follow the standard procedures of Gaussian process classification to obtain predictions [65; 24; 42; 19] where we first compute the predictive latent distribution \(p(f(T^{M}_{X})|C^{M},T^{M}_{X})\) as a Gaussian distribution by marginalising \(Z=\{z_{i}\}_{i=1}^{N_{T}}\):

\[p(f(T^{M}_{X})|C^{M},T^{M}_{X})=\int p(f(T^{M}_{X})|Z)p(Z|R^{*}(C^{M},T^{M}_{X }))\,dZ\] (11)

where \(T^{M}_{X}=\{T^{m}_{X}\}_{m=1}^{M}\), \(R^{*}(C^{M},T^{M}_{X})=\{r_{*,i}\}_{i=1}^{N_{T}}\), and \(p(f(T^{M}_{X})|Z)\) is parameterised by a decoder. Then, we obtain the predictive probability \(\widehat{T}_{Y}\) by:

\[\widehat{T}_{Y}=\int\text{Softmax}(p(f(T^{M}_{X})))p(f(T^{M}_{X})|C^{M},T^{M} _{X})\,df(T^{M}_{X})\] (12)

Similarly, we can obtain the unimodal predictive latent distribution \(p(f(T^{m}_{X})|\{C^{m}_{X},C^{m}_{Y}\},T^{m}_{X})\) and the unimodal predictive probability \(\widehat{T}^{m}_{Y}\) for the \(m^{th}\) modality as:

\[\widehat{T}^{m}_{Y}=\int\text{Softmax}(p(f(T^{m}_{X})))p(f(T^{m}_{X})|\{C^{m} _{X},C^{m}_{Y}\},T^{m}_{X})\,df(T^{m}_{X})\] (13)

We minimise the negative log likelihood of the aggregated prediction and the unimodal predictions by:

\[\mathcal{L}_{T_{Y}}=-\mathbb{E}_{T_{Y}}\left[\log\widehat{T}_{Y}\right]-\frac {1}{M}\sum_{m=1}^{M}\mathcal{L}^{m}_{T_{Y}}=-\mathbb{E}_{T_{Y}}\left[\log \widehat{T}_{Y}\right]-\frac{1}{M}\sum_{m=1}^{M}\mathbb{E}_{T_{Y}}\left[\log \widehat{T}^{m}_{Y}\right]\] (14)

Since Equations (11)-(13) are analytically intractable, we approximate the integrals by the Monte Carlo method [44]. The overall loss for MNPs is \(\mathcal{L}=\mathcal{L}_{T_{Y}}+\beta*\mathcal{L}_{RBF}\) where \(\beta\) is the balancing term. Refer to Figure 2d for the summarised steps of training MNPs.

## 4 Related Work

Neural ProcessesCNP and the latent variant of CNP that incorporates a latent variable capturing global uncertainty were the first NPs introduced in the literature [13; 14]. To address the under-fitting issue caused by the mean context representation in these NPs, Kim et al. [28] proposed to leverage the attention mechanism for target-specific context representations. This approach has been shown to be effective in subsequent works [53; 29; 9; 47; 67; 22]. Many other variants, such as SNP [51; 67], CNAP [48], and MPNPs [5] were proposed for common downstream tasks like 1D regression, 2D image completion [28; 14; 15; 10; 61; 26; 20], and image classification [13; 62; 25]. However, none of these studies have investigated the generalisation of NPs to multimodal data. This study is the first to consider NPs for multimodal classification and its uncertainty estimation.

Multimodal LearningThe history of multimodal learning that aims to leverage multiple sources of input can be traced back to the early work of Canonical Correlation Analysis (CCA) [21]. CCA learns the correlation between two variables which was further improved by using feed-forward networks by Deep CCA (DCCA) [2]. With advances in various architectures of DNNs, many studies on multimodal fusion and alignment were proposed [4; 12; 55; 6]. In particular, transformer-based models for vision-language tasks [36; 52; 1] have obtained great attention. Nonetheless, most of these methods were not originally intended for uncertainty estimation, and it has been demonstrated that many of them exhibit inadequate calibration [16; 43].

Multimodal Uncertainty EstimationMultimodal uncertainty estimation is an emerging research area. Its objective is to design robust and calibrated multimodal models. Ma et al. [37] proposed the Mixture of Normal-Inverse Gamma (MoNIG) algorithm that quantifies predictive uncertainty for multimodal regression. However, this work is limited to regression, whereas our work applies to multimodal classification. Han et al. [17] developed TMC based on the Dempster's combination rule to combine multi-view logits. In spite of its simplicity, empirical experiments showed its limited calibration and capability in OOD detection [24]. Jung et al. [24] proposed MGP that combines predictive posterior distributions of multiple GPs by the product of experts. While MGP achieved the current SOTA performance, its non-parametric framework makes it computationally expensive. Our proposed method overcomes this limitation by generalising efficient NPs to imitate GPs.

## 5 Experiments

Apart from measuring the test accuracy, we assessed our method's performance in uncertainty estimation by evaluating its calibration error, robustness to noisy samples, and capability to detect OOD samples. These are crucial aspects that a classifier not equipped to estimate uncertainty may struggle with. We evaluated MNPs on seven real-world datasets to compare the performance of MNPs against four unimodal baselines and three multimodal baselines.

To compare our method against unimodal baselines, we leveraged the early fusion (EF) method [3] that concatenates multimodal input features to single one. The unimodal baselines are (1) **MC Dropout (MCD)**[11] with dropout rate of 0.2, (2) **Deep Ensemble (DE)**[32] with five ensemble models, (3) **SNGP**'s GP layer [35], and (4) **ETP**[25] with memory size of 200 and a linear projector. ETP was chosen as a NP baseline because of its original context memory which requires minimal change of the model to be used for multimodal classification.

The multimodal baselines consist of (1) **Deep Ensemble (DE)** with the late fusion (LF) [3] where a classifier is trained for each modality input, and the final prediction is obtained by averaging predictions from all the modalities, (2) **TMC**[17], and (3) **MGP**[24]. For TMC and MGP, we followed the settings proposed by the original authors. In each experiment, the same feature extractors were used for all baselines. We report mean and standard deviation of results from five random seeds. The bold values are the best results for each dataset, and the underlined values are the second-best ones. See Appendix B for more detailed settings.

### Robustness to Noisy Samples

Experimental SettingsIn this experiment, we evaluated the classification performance of MNPs and the robustness to noisy samples with six multimodal datasets [24; 17]. Following [24] and [17], we normalised the datasets and used a train-test split of 0.8:0.2. To test the robustness to noise, we added zero-mean Gaussian noise with different magnitudes of standard deviation during inference (10 evenly spaced values on a log-scale from \(10^{-2}\) to \(10^{1}\)) to half of the modalities. For each noise level, all possible combinations of selecting half of the modalities (i.e., \(\binom{M}{M/2}\)) were evaluated and averaged. We report accuracy and expected calibration error (ECE) for each experiment. Please refer to Appendix B for details of the datasets and metrics.

ResultsWe provide the test results without noisy samples in Table 1, 2, and 5 and the results with noisy samples in Table 3. In terms of accuracy, MNPs outperform all the baselines in 5 out of 6 datasets. At the same time, MNPs provide the most calibrated predictions in 4 out of 6 datasets, preserving the non-parametric GPs' reliability that a parametric model struggles to achieve. This shows that MNPs bring together the best of non-parametric models and parametric models. Also,MNPs provide the most robust predictions to noisy samples in 5 out of 6 datasets achieved by the MBA mechanism. Both unimodal and multimodal baselines except MGP show limited robustness with a large performance degradation.

In addition to its superior performance, MNPs, as shown in Table 5, are also faster than the SOTA multimodal uncertainty estimator MGP in terms of wall-clock time per epoch (up to \(\times 5\) faster) measured in the identical environment including batch size, GPU, and code libraries (see Appendix B for computational complexity of the two models). This highly efficient framework was made possible

\begin{table}
\begin{tabular}{c c c c c c c} \hline \hline  & \multicolumn{5}{c}{Dataset} \\ \cline{2-7} Method & Handwritten & CUB & PIE & Caltech101 & Scene15 & HMDB \\ \hline MCD & 99.25\(\pm\)0.00 & 92.33\(\pm\)1.09 & 91.32\(\pm\)0.62 & 92.95\(\pm\)0.29 & 71.75\(\pm\)0.25 & 71.68\(\pm\)0.36 \\ DE (EF) & 99.20\(\pm\)0.11 & 93.16\(\pm\)0.70 & 91.76\(\pm\)0.33 & 92.99\(\pm\)0.09 & 72.70\(\pm\)0.39 & 71.67\(\pm\)0.23 \\ SNGP & 98.85\(\pm\)0.22 & 89.50\(\pm\)0.75 & 87.06\(\pm\)1.23 & 91.24\(\pm\)0.46 & 64.68\(\pm\)4.03 & 67.65\(\pm\)1.03 \\ ETP & 98.75\(\pm\)0.25 & 92.33\(\pm\)1.99 & 91.76\(\pm\)0.62 & 92.08\(\pm\)0.33 & 72.58\(\pm\)1.35 & 67.43\(\pm\)0.95 \\ DE (LF) & 99.25\(\pm\)0.00 & 92.33\(\pm\)0.70 & 87.21\(\pm\)0.66 & 92.97\(\pm\)0.13 & 67.05\(\pm\)0.38 & 69.98\(\pm\)0.36 \\ TMC & 98.10\(\pm\)0.14 & 91.17\(\pm\)0.46 & 91.18\(\pm\)1.72 & 91.63\(\pm\)0.28 & 67.68\(\pm\)0.27 & 65.17\(\pm\)0.87 \\ MGP & 98.60\(\pm\)0.14 & 92.33\(\pm\)0.70 & 92.06\(\pm\)0.96 & 93.00\(\pm\)0.33 & 70.00\(\pm\)0.53 & **72.30\(\pm\)0.19** \\ MNPs (Ours) & **99.50\(\pm\)0.00** & **93.50\(\pm\)1.71** & **95.00\(\pm\)0.62** & **93.46\(\pm\)0.32** & **77.90\(\pm\)0.71** & 71.97\(\pm\)0.43 \\ \hline \hline \end{tabular}
\end{table}
Table 1: Test accuracy (\(\uparrow\)).

\begin{table}
\begin{tabular}{c c c c c c} \hline \hline  & \multicolumn{5}{c}{Dataset} \\ \cline{2-7} Method & Handwritten & CUB & PIE & Caltech101 & Scene15 & HMDB \\ \hline MCD & 0.009\(\pm\)0.000 & 0.069\(\pm\)0.017 & 0.299\(\pm\)0.005 & 0.017\(\pm\)0.003 & 0.181\(\pm\)0.003 & 0.388\(\pm\)0.004 \\ DE (EF) & 0.007\(\pm\)0.000 & 0.054\(\pm\)0.010 & 0.269\(\pm\)0.004 & 0.036\(\pm\)0.001 & 0.089\(\pm\)0.003 & 0.095\(\pm\)0.003 \\ SNGP & 0.023\(\pm\)0.004 & 0.200\(\pm\)0.010 & 0.852\(\pm\)0.012 & 0.442\(\pm\)0.004 & 0.111\(\pm\)0.063 & 0.227\(\pm\)0.010 \\ ETP & 0.020\(\pm\)0.002 & 0.051\(\pm\)0.009 & 0.287\(\pm\)0.007 & 0.096\(\pm\)0.002 & 0.045\(\pm\)0.008 & 0.100\(\pm\)0.010 \\ DE (LF) & 0.292\(\pm\)0.001 & 0.270\(\pm\)0.009 & 0.567\(\pm\)0.006 & 0.023\(\pm\)0.002 & 0.319\(\pm\)0.005 & 0.270\(\pm\)0.003 \\ TMC & 0.013\(\pm\)0.002 & 0.141\(\pm\)0.002 & 0.072\(\pm\)0.011 & 0.068\(\pm\)0.002 & 0.180\(\pm\)0.004 & 0.594\(\pm\)0.008 \\ MGP & 0.006\(\pm\)0.004 & **0.038\(\pm\)0.007** & 0.079\(\pm\)0.007 & **0.009\(\pm\)0.003** & 0.062\(\pm\)0.006 & 0.036\(\pm\)0.003 \\ MNPs (Ours) & **0.005\(\pm\)0.001** & 0.049\(\pm\)0.008 & **0.040\(\pm\)0.005** & 0.017\(\pm\)0.003 & **0.038\(\pm\)0.009** & **0.028\(\pm\)0.006** \\ \hline \hline \end{tabular}
\end{table}
Table 2: Test ECE (\(\downarrow\)).

\begin{table}
\begin{tabular}{c c c c c} \hline \hline  & \multicolumn{5}{c}{Dataset} \\ \cline{2-7} Method & Handwritten & CUB & PIE & Caltech101 & Scene15 & HMDB \\ \hline MCD & 82.15\(\pm\)0.17 & 76.08\(\pm\)0.61 & 64.65\(\pm\)0.77 & 73.45\(\pm\)0.11 & 48.97\(\pm\)0.33 & 42.63\(\pm\)0.08 \\ DE (EF) & 82.16\(\pm\)0.18 & 76.94\(\pm\)0.82 & 65.53\(\pm\)0.20 & 73.99\(\pm\)0.19 & 49.45\(\pm\)0.35 & 41.92\(\pm\)0.06 \\ SNGP & 72.46\(\pm\)0.41 & 61.27\(\pm\)1.24 & 56.52\(\pm\)0.69 & 56.57\(\pm\)0.17 & 38.19\(\pm\)1.86 & 37.49\(\pmby DCM that stores a small number of informative context points. It also highlights the advantage of using efficient DNNs to imitate GPs, which a non-parametric model like MGP struggles to achieve.

### OOD Detection

Experimental SettingsFollowing the experimental settings of [24], we trained the models with three different corruption types of CIFAR10-C [18] as a multimodal dataset and evaluated the OOD detection performance using two different test datasets. The first test dataset comprised half CIFAR10-C and half SVHN [46] samples, while the second test dataset comprised half CIFAR10-C and half CIFAR100 [31] samples. We used the Inception v3 [54] pretrained with ImageNet as the backbone of all the baselines. The area under the receiver operating characteristic (AUC) is used as a metric to classify the predictive uncertainty into ID (class 0) and OOD (class 1).

ResultsTable 4 shows test accuracy and ECE with CIFAR10-C and OOD AUC against SVHN and CIFAR100. MNPs outperform all the baselines in terms of ECE and OOD AUC. A large difference in OOD AUC is observed which shows that the proposed adaptive RBF attention identifies OOD samples well. Also, we highlight MNPs outperform the current SOTA MGP in every metric. A marginal difference in test accuracy between DE (LF) and MNPs is observed, but MNPs achieve much lower ECE (approximately \(8.6\) folds) with higher OOD AUC than DE (LF).

## 6 Conclusion

In this study, we introduced a new multimodal uncertainty estimation method by generalising NPs for multimodal uncertainty estimation, namely Multimodal Neural Processes. Our approach leverages a simple and effective dynamic context memory, a Bayesian method of aggregating multimodal representations, and an adaptive RBF attention mechanism in a holistic and principled manner. We evaluated the proposed method on the seven real-world datasets and compared its performance against seven unimodal and multimodal baselines. The results demonstrate that our method outperforms all the baselines and achieves the SOTA performance in multimodal uncertainty estimation. A limitation of this work is that despite the effectiveness of the updating mechanism of DCM in practive, it is not theoretically guaranteed to obtain the optimal context memory. Nonetheless, our method effectively achieves both accuracy and reliability in an efficient manner. We leave developing a better updating mechanism for our future work. The broader impacts of this work are discussed in Appendix D.

## References

* Akbari et al. [2021] H. Akbari, L. Yuan, R. Qian, W. Chuang, S. Chang, Y. Cui, and B. Gong. Vatt: Transformers for multimodal self-supervised learning from raw video, audio and text. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P. Liang, and J. W. Vaughan, editors, _Advances in Neural Information Processing Systems_, volume 34, pages 24206-24221. Curran Associates, Inc., 2021.
* Andrew et al. [2013] G. Andrew, R. Arora, J. Bilmes, and K. Livescu. Deep canonical correlation analysis. In S. Dasgupta and D. McAllester, editors, _Proceedings of the 30th International Conference on Machine Learning_, volume 28 of _Proceedings of Machine Learning Research_, pages 1247-1255, Atlanta, Georgia, USA, 17-19 Jun 2013. PMLR.
* Baltrusaitis et al. [2019] T. Baltrusaitis, C. Ahuja, and L.-P. Morency. Multimodal machine learning: A survey and taxonomy. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 41(2):423-443, 2019. doi: 10.1109/TPAMI.2018.2798607.
* Bhatt et al. [2019] G. Bhatt, P. Jha, and B. Raman. Representation learning using step-based deep multi-modal autoencoders. _Pattern Recognition_, 95:12-23, 2019. ISSN 0031-3203. doi: https://doi.org/10.1016/j.patcog.2019.05.032.
* Cangea et al. [2022] C. Cangea, B. Day, A. R. Jamasb, and P. Lio. Message passing neural processes. In _ICLR 2022 Workshop on Geometrical and Topological Representation Learning_, 2022.
* Dao et al. [2023] S. D. Dao, D. Huynh, H. Zhao, D. Phung, and J. Cai. Open-vocabulary multi-label image classification with pretrained vision-language model. In _2023 IEEE International Conference on Multimedia and Expo (ICME)_, pages 2135-2140. IEEE, 2023.

* Dempster [1967] A. Dempster. Upper and lower probabilities induced by a multi- valued mapping. _Annals of Mathematical Statistics_, 38:325-339, 1967.
* Dipnall et al. [2021] J. F. Dipnall, R. Page, L. Du, M. Costa, R. A. Lyons, P. Cameron, R. de Steiger, R. Hau, A. Bucknill, A. Oppy, E. Edwards, D. Varma, M. C. Jung, and B. J. Gabbe. Predicting fracture outcomes from clinical registry data using artificial intelligence supplemented models for evidence-informed treatment (praise) study protocol. _PLOS ONE_, 16(9):1-12, 09 2021. doi: 10.1371/journal.pone.0257361.
* Feng et al. [2023] L. Feng, H. Hajimirsadeghi, Y. Bengio, and M. O. Ahmed. Latent bottlenecked attentive neural processes. In _The Eleventh International Conference on Learning Representations_, 2023.
* Foong et al. [2020] A. Foong, W. Bruinsma, J. Gordon, Y. Dubois, J. Requeima, and R. Turner. Meta-learning stationary stochastic process prediction with convolutional neural processes. In H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. Lin, editors, _Advances in Neural Information Processing Systems_, volume 33, pages 8284-8295. Curran Associates, Inc., 2020.
* Gal and Ghahramani [2015] Y. Gal and Z. Ghahramani. Bayesian convolutional neural networks with bernoulli approximate variational inference. _arXiv preprint arXiv:1506.02158_, 2015.
* Gao et al. [2019] M. Gao, J. Jiang, G. Zou, V. John, and Z. Liu. Rgb-d-based object recognition using multimodal convolutional neural networks: A survey. _IEEE Access_, 7:43110-43136, 2019. doi: 10.1109/ACCESS.2019.2907071.
* Garnelo et al. [2018] M. Garnelo, D. Rosenbaum, C. Maddison, T. Ramalho, D. Saxton, M. Shanahan, Y. W. Teh, D. Rezende, and S. M. A. Eslami. Conditional neural processes. In J. Dy and A. Krause, editors, _Proceedings of the 35th International Conference on Machine Learning_, volume 80 of _Proceedings of Machine Learning Research_, pages 1704-1713. PMLR, 10-15 Jul 2018.
* Garnelo et al. [2018] M. Garnelo, J. Schwarz, D. Rosenbaum, F. Viola, D. J. Rezende, S. Eslami, and Y. W. Teh. Neural processes. _arXiv preprint arXiv:1807.01622_, 2018.
* Gordon et al. [2020] J. Gordon, W. P. Bruinsma, A. Y. K. Foong, J. Requeima, Y. Dubois, and R. E. Turner. Convolutional conditional neural processes. In _International Conference on Learning Representations_, 2020.
* Guo et al. [2017] C. Guo, G. Pleiss, Y. Sun, and K. Q. Weinberger. On calibration of modern neural networks. In D. Precup and Y. W. Teh, editors, _Proceedings of the 34th International Conference on Machine Learning_, volume 70 of _Proceedings of Machine Learning Research_, pages 1321-1330. PMLR, 06-11 Aug 2017.
* Han et al. [2021] Z. Han, C. Zhang, H. Fu, and J. T. Zhou. Trusted multi-view classification. In _International Conference on Learning Representations_, 2021.
* Hendrycks and Dietterich [2019] D. Hendrycks and T. Dietterich. Benchmarking neural network robustness to common corruptions and perturbations. _Proceedings of the International Conference on Learning Representations_, 2019.
* Hensman et al. [2015] J. Hensman, A. Matthews, and Z. Ghahramani. Scalable Variational Gaussian Process Classification. In G. Lebanon and S. V. N. Vishwanathan, editors, _Proceedings of the Eighteenth International Conference on Artificial Intelligence and Statistics_, volume 38 of _Proceedings of Machine Learning Research_, pages 351-360, San Diego, California, USA, 09-12 May 2015. PMLR.
* Holderrieth et al. [2021] P. Holderrieth, M. J. Hutchinson, and Y. W. Teh. Equivariant learning of stochastic fields: Gaussian processes and steerable conditional neural processes. In M. Meila and T. Zhang, editors, _Proceedings of the 38th International Conference on Machine Learning_, volume 139 of _Proceedings of Machine Learning Research_, pages 4297-4307. PMLR, 18-24 Jul 2021.
* Hotelling [1936] H. Hotelling. Relations between two sets of variates. _Biometrika_, 28(3/4):321-377, 1936. ISSN 00063444.
* Jha et al. [2023] S. Jha, D. Gong, H. Zhao, and Y. Lina. NPCL: Neural processes for uncertainty-aware continual learning. In _Advances in Neural Information Processing Systems_, 2023.

* [23] A. Josang. Belief fusion. In _Subjective Logic_, volume 3, chapter 12, pages 207-236. Springer, 2016.
* [24] M. C. Jung, H. Zhao, J. Dipnall, B. Gabbe, and L. Du. Uncertainty estimation for multi-view data: The power of seeing the whole picture. In A. H. Oh, A. Agarwal, D. Belgrave, and K. Cho, editors, _Advances in Neural Information Processing Systems_, 2022.
* [25] M. Kandemir, A. Akgul, M. Haussmann, and G. Unal. Evidential turing processes. In _International Conference on Learning Representations_, 2022.
* [26] M. Kawano, W. Kumagai, A. Sannai, Y. Iwasawa, and Y. Matsuo. Group equivariant conditional neural processes. In _International Conference on Learning Representations_, 2021.
* [27] P. Khosla, P. Teterwak, C. Wang, A. Sarna, Y. Tian, P. Isola, A. Maschinot, C. Liu, and D. Krishnan. Supervised contrastive learning. In H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. Lin, editors, _Advances in Neural Information Processing Systems_, volume 33, pages 18661-18673. Curran Associates, Inc., 2020.
* [28] H. Kim, A. Mnih, J. Schwarz, M. Garnelo, A. Eslami, D. Rosenbaum, O. Vinyals, and Y. W. Teh. Attentive neural processes. In _International Conference on Learning Representations_, 2019.
* [29] M. Kim, K. R. Go, and S.-Y. Yun. Neural processes with stochastic attention: Paying more attention to the context dataset. In _International Conference on Learning Representations_, 2022.
* [30] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. In Y. Bengio and Y. LeCun, editors, _3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings_, 2015.
* [31] A. Krizhevsky and G. Hinton. Learning multiple layers of features from tiny images. _Master's thesis, Department of Computer Science, University of Toronto_, 2009.
* [32] B. Lakshminarayanan, A. Pritzel, and C. Blundell. Simple and scalable predictive uncertainty estimation using deep ensembles. In _Proceedings of the 31st International Conference on Neural Information Processing Systems_, NIPS'17, page 6405-6416, Red Hook, NY, USA, 2017. Curran Associates Inc. ISBN 9781510860964.
* [33] B.-J. Lee, S. Hong, and K.-E. Kim. Residual neural processes. _Proceedings of the AAAI Conference on Artificial Intelligence_, 34(04):4545-4552, Apr. 2020. doi: 10.1609/aaai.v34i04.5883.
* [34] J. Lee, M. Humt, J. Feng, and R. Triebel. Estimating model uncertainty of neural networks in sparse information form. In H. D. III and A. Singh, editors, _Proceedings of the 37th International Conference on Machine Learning_, volume 119 of _Proceedings of Machine Learning Research_, pages 5702-5713. PMLR, 13-18 Jul 2020.
* [35] J. Liu, Z. Lin, S. Padhy, D. Tran, T. Bedrax Weiss, and B. Lakshminarayanan. Simple and principled uncertainty estimation with deterministic deep learning via distance awareness. In H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. Lin, editors, _Advances in Neural Information Processing Systems_, volume 33, pages 7498-7512. Curran Associates, Inc., 2020.
* [36] J. Lu, D. Batra, D. Parikh, and S. Lee. Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alche-Buc, E. Fox, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 32. Curran Associates, Inc., 2019.
* [37] H. Ma, Z. Han, C. Zhang, H. Fu, J. T. Zhou, and Q. Hu. Trustworthy multimodal regression with mixture of normal-inverse gamma distributions. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P. Liang, and J. W. Vaughan, editors, _Advances in Neural Information Processing Systems_, volume 34, pages 6881-6893. Curran Associates, Inc., 2021.
* [38] A. Malinin and M. Gales. Predictive uncertainty estimation via prior networks. In _Proceedings of the 32nd International Conference on Neural Information Processing Systems_, NIPS'18, page 7047-7058, Red Hook, NY, USA, 2018. Curran Associates Inc.

* Martins and Astudillo [2016] A. Martins and R. Astudillo. From softmax to sparsemax: A sparse model of attention and multi-label classification. In M. F. Balcan and K. Q. Weinberger, editors, _Proceedings of The 33rd International Conference on Machine Learning_, volume 48 of _Proceedings of Machine Learning Research_, pages 1614-1623, New York, New York, USA, 20-22 Jun 2016. PMLR.
* Meronen et al. [2020] L. Meronen, C. Irwanto, and A. Solin. Stationary activations for uncertainty calibration in deep learning. In H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. Lin, editors, _Advances in Neural Information Processing Systems_, volume 33, pages 2338-2350. Curran Associates, Inc., 2020.
* Meronen et al. [2021] L. Meronen, M. Trapp, and A. Solin. Periodic activation functions induce stationarity. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P. Liang, and J. W. Vaughan, editors, _Advances in Neural Information Processing Systems_, volume 34, pages 1673-1685. Curran Associates, Inc., 2021.
* Milios et al. [2018] D. Milios, R. Camoriano, P. Michiardi, L. Rosasco, and M. Filippone. Dirichlet-based gaussian processes for large-scale calibrated classification. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 31. Curran Associates, Inc., 2018.
* Minderer et al. [2021] M. Minderer, J. Djolonga, R. Romijnders, F. A. Hubis, X. Zhai, N. Houlsby, D. Tran, and M. Lucic. Revisiting the calibration of modern neural networks. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. W. Vaughan, editors, _Advances in Neural Information Processing Systems_, 2021.
* Murphy [2022] K. P. Murphy. _Probabilistic machine learning: an introduction_. MIT press, 2022.
* Nair et al. [2020] T. Nair, D. Precup, D. L. Arnold, and T. Arbel. Exploring uncertainty measures in deep networks for multiple sclerosis lesion detection and segmentation. _Medical Image Analysis_, 59:101557, 2020. ISSN 1361-8415. doi: https://doi.org/10.1016/j.media.2019.101557.
* Netzer et al. [2011] Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and A. Y. Ng. Reading digits in natural images with unsupervised feature learning. In _NIPS Workshop on Deep Learning and Unsupervised Feature Learning_, 2011.
* Nguyen and Grover [2022] T. Nguyen and A. Grover. Transformer neural processes: Uncertainty-aware meta learning via sequence modeling. In K. Chaudhuri, S. Jegelka, L. Song, C. Szepesvari, G. Niu, and S. Sabato, editors, _Proceedings of the 39th International Conference on Machine Learning_, volume 162 of _Proceedings of Machine Learning Research_, pages 16569-16594. PMLR, 17-23 Jul 2022.
* Requeima et al. [2019] J. Requeima, J. Gordon, J. Bronskill, S. Nowozin, and R. E. Turner. Fast and flexible multi-task classification using conditional neural adaptive processes. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alche-Buc, E. Fox, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 32. Curran Associates, Inc., 2019.
* Roy et al. [2019] A. G. Roy, S. Conjeti, N. Navab, and C. Wachinger. Bayesian quicknat: Model uncertainty in deep whole-brain segmentation for structure-wise quality control. _NeuroImage_, 195:11-22, 2019. ISSN 1053-8119. doi: https://doi.org/10.1016/j.neuroimage.2019.03.042.
* Sensoy et al. [2018] M. Sensoy, L. Kaplan, and M. Kandemir. Evidential deep learning to quantify classification uncertainty. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 31. Curran Associates, Inc., 2018.
* Singh et al. [2019] G. Singh, J. Yoon, Y. Son, and S. Ahn. Sequential neural processes. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alche-Buc, E. Fox, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 32. Curran Associates, Inc., 2019.
* Sun et al. [2019] C. Sun, A. Myers, C. Vondrick, K. Murphy, and C. Schmid. Videobert: A joint model for video and language representation learning. In _Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)_, October 2019.
* Suresh and Srinivasan [2019] A. Suresh and S. Srinivasan. Improved attentive neural processes. In _Fourth Workshop on Bayesian Deep Learning at the Conference on Neural Information Processing Systems_, volume 14, page 17, 2019.

* [54] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna. Rethinking the inception architecture for computer vision. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, June 2016.
* 2019 IEEE International Geoscience and Remote Sensing Symposium_, pages 923-926, 2019. doi: 10.1109/IGARSS.2019.8898605.
* [56] N. Ulapane, K. Thiyagarajan, and S. Kodagoda. Hyper-parameter initialization for squared exponential kernel-based gaussian process regression. In _2020 15th IEEE Conference on Industrial Electronics and Applications (ICIEA)_, pages 1154-1159, 2020. doi: 10.1109/ICIEA48937.2020.9248120.
* [57] M. Valdenegro-Toro. Deep sub-ensembles for fast uncertainty estimation in image classification. _arXiv preprint arXiv:1910.08168_, 2019.
* [58] J. Van Amersfoort, L. Smith, Y. W. Teh, and Y. Gal. Uncertainty estimation using a single deep deterministic neural network. In H. D. III and A. Singh, editors, _Proceedings of the 37th International Conference on Machine Learning_, volume 119 of _Proceedings of Machine Learning Research_, pages 9690-9700. PMLR, 13-18 Jul 2020.
* [59] J. van Amersfoort, L. Smith, A. Jesson, O. Key, and Y. Gal. On feature collapse and deep kernel learning for single forward pass uncertainty. _arXiv preprint arXiv:2102.11409_, 2021.
* [60] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. u. Kaiser, and I. Polosukhin. Attention is all you need. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 30. Curran Associates, Inc., 2017.
* [61] M. Volpp, F. Flurenbrock, L. Grossberger, C. Daniel, and G. Neumann. Bayesian context aggregation for neural processes. In _International Conference on Learning Representations_, 2021.
* [62] J. Wang, T. Lukasiewicz, D. Massiceti, X. Hu, V. Pavlovic, and A. Neophytou. NP-match: When neural processes meet semi-supervised learning. In K. Chaudhuri, S. Jegelka, L. Song, C. Szepesvari, G. Niu, and S. Sabato, editors, _Proceedings of the 39th International Conference on Machine Learning_, volume 162 of _Proceedings of Machine Learning Research_, pages 22919-22934. PMLR, 17-23 Jul 2022.
* [63] Y. Wen, D. Tran, and J. Ba. Batchensemble: an alternative approach to efficient ensemble and lifelong learning. In _International Conference on Learning Representations_, 2020.
* [64] C. Williams. Computing with infinite networks. In M. Mozer, M. Jordan, and T. Petsche, editors, _Advances in Neural Information Processing Systems_, volume 9. MIT Press, 1996.
* [65] C. K. Williams and C. E. Rasmussen. _Gaussian Processes for Machine Learning_. The MIT Press, 2006.
* [66] B. Xu, N. Wang, T. Chen, and M. Li. Empirical evaluation of rectified activations in convolutional network. _arXiv preprint arXiv:1505.00853_, 2015.
* [67] J. Yoon, G. Singh, and S. Ahn. Robustifying sequential neural processes. In H. D. III and A. Singh, editors, _Proceedings of the 37th International Conference on Machine Learning_, volume 119 of _Proceedings of Machine Learning Research_, pages 10861-10870. PMLR, 13-18 Jul 2020.

Lemma and Proof

For the comprehensiveness of proof, we duplicate Lemma 3.1 here.

**Lemma A.1** (Gaussian posterior distribution with factorised prior distribution).: _If we have \(p(x_{i}|\mu)=\mathcal{N}(x_{i}|\mu,\Sigma_{i})\) and \(p(\mu)=\prod_{i=1}^{n}\mathcal{N}(\mu_{0,i},\Sigma_{0,i})\) for \(n\) i.i.d. observations of \(D\) dimensional vectors, then the mean and covariance of posterior distribution \(p(\mu|x)=\mathcal{N}(\mu|\mu_{n},\Sigma_{n})\) are:_

\[\Sigma_{n}=\left[\sum_{i=1}^{n}\left(\Sigma_{i}^{-1}+\Sigma_{0,i}^{-1}\right) \right]^{-1},\quad\mu_{n}=\Sigma_{n}\left[\sum_{i=1}^{n}\left(\Sigma_{i}^{-1} x_{i}+\Sigma_{0,i}^{-1}\mu_{0,i}\right)\right]\] (15)

Proof.: \[p(\mu|x)\propto \prod_{i=1}^{n}\frac{1}{\sqrt{(2\pi)^{D}|\Sigma_{i}|}}\exp\left(- \frac{1}{2}(x_{i}-\mu)^{T}\Sigma_{i}^{-1}(x_{i}-\mu)\right)\times\] \[\frac{1}{\sqrt{(2\pi)^{D}|\Sigma_{0,i}|}}\exp\left(-\frac{1}{2}( \mu-\mu_{0,i})^{T}\Sigma_{0,i}^{-1}(\mu-\mu_{0,i})\right)\] (16) \[\propto \exp\left[-\frac{1}{2}\left(\sum_{i=1}^{n}(\mu-x_{i})^{T}\Sigma_ {i}^{-1}(\mu-x_{i})+(\mu-\mu_{0,i})^{T}\Sigma_{0,i}^{-1}(\mu-\mu_{0,i})\right)\right]\] (17) \[\propto \exp\left[-\frac{1}{2}\left(\mu^{T}\left(\sum_{i=1}^{n}\left( \Sigma_{i}^{-1}+\Sigma_{0,i}^{-1}\right)\right)\mu-2\mu^{T}\left(\sum_{i=1}^{n }\left(\Sigma_{i}^{-1}x_{i}+\Sigma_{0,i}^{-1}\mu_{0,i}\right)\right)\right)\right]\] (18) \[= \frac{1}{\sqrt{(2\pi)^{D}|\Sigma_{n}|}}\exp\left(-\frac{1}{2}( \mu-\mu_{n})^{T}\Sigma_{n}^{-1}(\mu-\mu_{n})\right)\] (19)

where we dropped constant terms for clarity. From Equation (18) and (19), we can see that:

\[\Sigma_{n}^{-1}=\sum_{i=1}^{n}\left(\Sigma_{i}^{-1}+\Sigma_{0,i}^ {-1}\right),\quad\Sigma_{n}^{-1}\mu_{n}=\sum_{i=1}^{n}\left(\Sigma_{i}^{-1}x_ {i}+\Sigma_{0,i}^{-1}\mu_{0,i}\right)\] (20) \[\Sigma_{n}=\left[\sum_{i=1}^{n}\left(\Sigma_{i}^{-1}+\Sigma_{0,i} ^{-1}\right)\right]^{-1},\quad\mu_{n}=\Sigma_{n}\left[\sum_{i=1}^{n}\left( \Sigma_{i}^{-1}x_{i}+\Sigma_{0,i}^{-1}\mu_{0,i}\right)\right]\] (21)

If we use Lemma A.1 with diagonal covariance matrices for \(p(r_{*,i}^{m}|z_{i})=\mathcal{N}\left(r_{*,i}^{m}|z_{i},\text{diag}(s_{*,i}^{m })\right)\) and \(p(z_{i})=\prod_{m=1}^{M}\mathcal{N}\left(u^{m},\text{diag}(q^{m})\right)\), we can obtain the posterior distribution of \(\mathcal{N}\left(z_{i}|\mu_{z_{i}},\text{diag}(\sigma_{z_{i}}^{2})\right)\) as follows:

\[\sigma_{z_{i}}^{2}=\left[\sum_{m=1}^{M}\left((s_{*,i}^{m})^{\oslash}+(q^{m})^{ \oslash}\right)\right]^{\oslash},\mu_{z_{i}}=\sigma_{z_{i}}^{2}\otimes\left[ \sum_{m=1}^{M}\left(r_{*,i}^{m}\otimes(s_{*,i}^{m})^{\oslash}+u^{m}\otimes(q^ {m})^{\oslash}\right)\right]\] (22)

where \(\oslash\) is the element-wise inversion, and \(\otimes\) is the element-wise product.

## Appendix B Experimental Details

In this section, we outline additional details of the experimental settings including the datasets (Appendix B.1), hyperparameters of the models used (Appendix B.2), metrics (Appendix B.3), and a brief analysis of computational complexity of MGP and MNPs (Appendix B.4). For all the experiments, we used the Adam optimiser [30] with batch size of 200 and the Tensorflow framework. All the experiments were conducted on a single NVIDIA GeForce RTX 3090 GPU.

### Details of Datasets

Synthetic DatasetFigure 2 shows the predictive probability and the attention weight of different attention mechanisms. Here, we describe the dataset and the settings used for the demonstrations.

We generated 1,000 synthetic training samples (i.e., \(N_{train}=1,000\)) for binary classification by using the Scikit-learn's moon dataset 3 with zero-mean Gaussian noise (\(std=0.15\)) added. The test samples were generated as a mesh-grid of 10,000 points (i.e., \(100\times 100\) grid with \(N_{test}=10,000\)). The number of points in the context memory \(N^{m}\) was set to 100. In this demonstration, we simplified the problem by setting \(M=1\) which is equivalent to the unimodal setting and illustrated the difference in attention mechanisms.

Footnote 3: https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_moons.html

Robustness to Noisy Samples DatasetIn Section 5.1, we evaluated the models' robustness to noisy samples with the six multimodal datasets. The details of each dataset are outlined in Table 6. These datasets lie within a feature space where each feature extraction method can be found in [17].

OOD Detection DatasetWe used CIFAR10-C [18] which consists of corrupted images of CIFAR10 [31]. 15 types of corruptions and five levels of corruption for each type are available for the dataset. Following [24], we used the first three types as multimodal inputs with different levels of corruption (1, 3, and 5).

### Details of Models

In our main experiments, four unimodal baselines with the early fusion (EF) method [3] (MC Dropout, Deep Ensemble (EF), SNGP, and ETP) and three multimodal baselines with the late fusion (LF) method [3] (Deep Ensemble (LF), TMC, and MGP) were used. In this section, we describe the details of the feature extractors and each baseline.

Feature ExtractorsWe used the same feature extractor for all the methods to ensure fair comparisons of the models. For the synthetic dataset, the 2D input points were projected to a high-dimensional space (\(d^{m}=128\)) with a feature extractor that has 6 residual fully connected (FC) layers with the ReLU activation. For the OOD detection experiment, the Inception v3 [54] pretrained with ImageNet was used as the feature extractor. Note that the robustness to noisy samples experiment does not require a separate feature extractor as the dataset is already in a feature space.

MC DropoutMonte Carlo (MC) Dropout [11] is a well-known uncertainty estimation method that leverages existing dropout layers of DNNs to approximate Bayesian inference. In our experiments, the dropout rate was set to 0.2 with 100 dropout samples used to make predictions in the inference stage. The predictive uncertainty was quantified based on the original paper [11].

Deep EnsembleDeep ensemble [32] is a powerful uncertainty estimation method that trains multiple independent ensemble members. In the case of the unimodal baseline, we employed five ensemble members, whereas for the multimodal baseline, a single classifier was trained independently for each modality input. In both scenarios, the unified predictions were obtained by averaging the predictions from the ensemble members, while the predictive uncertainty was determined by calculating the variance of those predictions.

\begin{table}
\begin{tabular}{c c c c c c c} \hline \hline  & \multicolumn{6}{c}{Dataset} \\ \cline{2-7}  & Handwritten & CUB & PIE & Caltech101 & Scene15 & HMDB \\ \hline \# of modalities & 6 & 2 & 3 & 2 & 3 & 2 \\ Types of modalities & Images & Image\&Text & Images & Images & Images & Images \\ \# of samples & 2,000 & 11,788 & 680 & 8,677 & 4,485 & 6,718 \\ \# of classes & 10 & 10 & 68 & 101 & 15 & 51 \\ \hline \hline \end{tabular}
\end{table}
Table 6: Multimodal datasets used for evaluating robustness to noisy samples.

SNGPSpectral-normalized Neural Gaussian Process (SNGP) [35] is an effective and scalable uncertainty estimation method that utilises Gaussian process (GP). It consists of a feature extractor with spectral normalisation and a GP output layer. Since we used the identical feature extractor for all the baselines, we only used the GP layer in this work. Following [24], the model's covariance matrix was updated without momentum with \(\lambda=\pi/8\) for the mean-field approximation. As the original authors proposed, we quantified the predictive uncertainty based on the Dempster-Shafer theory [7] defined as \(u(x)=K/(K+\sum_{k=1}^{K}\exp{(\text{logit}_{k}(x))})\) where \(\text{logit}_{k}(\cdot)\) is the \(k^{th}\) class of output logit with the number of classes \(K\).

EtpEvidential Turing Processes (ETP) [25] is a recent variant of NPs for uncertainty estimation of image classification. Since none of the existing NPs can be directly applied to multimodal data, there are several requirements to utilise them for multimodal classification: 1) a context set in the inference stage (e.g., context memory) and 2) a method of processing multimodal data. ETP was selected due to its inclusion of the original context memory, requiring minimal modifications to be applicable to our task. We used the memory size of 200 and quantified the predictive uncertainty with entropy as proposed by the original paper [25].

TmcTrusted Multi-view Classification (TMC) [17] is a simple multimodal uncertainty estimation based on the Subjective logic [23]. We used the original settings of the paper with the annealing epochs of ten for the balancing term. TMC explicitly quantifies its predictive uncertainty based on the Dempster-Shafer theory [7].

MgpMulti-view Gaussian Process (MGP) [24] is the current SOTA multimodal uncertainty estimation method that combines predictive posterior distributions of multiple GPs. We used the identical settings of the original paper with the number of inducing points set to 200 and ten warm-up epochs. Its predictive uncertainty was quantified by the predictive variance as proposed by the original paper [24].

MNPs (Ours)The encoders and decoder in Multimodal Neural Processes (MNPs) consist of two FC layers with the Leaky ReLU activation [66] after the first FC layer. A normalisation layer is stacked on top of the second FC layer for the encoders. For \(\text{enc}^{m}_{\psi}(\cdot)\) and \(\text{enc}^{m}_{\omega}(\cdot)\) that approximate the variance of distributions, we ensure positivity by transforming the outputs as \(h_{+}=0.01+0.99*\text{Softplus}(h)\) where \(h\) is the raw output from the encoders. \(l^{m}\) of the adaptive RBF attention was initialised as \(10*1\in\mathbb{R}^{d^{m}}\), and DCM was initialised by randomly selecting training samples. We used five samples for the Monte Carlo method to approximate the integrals in Equations (11)-(13), which we found enough in practice. Refer to Table 7 for the hyperparameters of MNPs. We provide the impact of \(N^{m}\) on the model performance in Appendix C.1.

### Details of Metrics

Apart from test accuracy, we report the expected calibration error (ECE) [16] and the area under the receiver operating characteristic curve (AUC). ECE is defined as:

\[\text{ECE}=\frac{1}{n}\sum_{i=1}^{b}|B_{i}||\text{acc}(B_{i})-\text{conf}(B_{i})|\]

where \(n\) is the number of testing samples, \(B_{i}\) is a bin with partitioned predictions with the number of bins \(b\), \(|B_{i}|\) is the number of elements in \(B_{i}\), \(\text{acc}(B_{i})\) is the accuracy of predictions in \(B_{i}\), and

\begin{table}
\begin{tabular}{c c c c c c c c} \hline \hline  & \multicolumn{6}{c}{Dataset} \\ \cline{2-9} Parameter & Handwritten & CUB & PIE & Caltech101 & Scene15 & HMDB & CIFAR10-C \\ \hline \(N^{m}\) & 100 & 200 & 300 & 700 & 300 & 400 & 200 \\ \(\alpha\) & 1 & 0.03 & 1 & 1 & 0.0001 & 1 & 1 \\ \(\beta\) & 1 & 1 & 1 & 1 & 1 & 1 & 1 \\ \(\tau\) & 0.25 & 0.01 & 0.1 & 0.01 & 0.5 & 0.01 & 0.01 \\ \hline \hline \end{tabular}
\end{table}
Table 7: Hyperparameters of MNPs.

\(\text{conf}(B_{i})\) is the average predictive confidence in \(B_{i}\). Following [35] and [24], we set \(b=15\). AUC was used for the OOD detection experiment with ground truth labels of class 0 being the ID samples and class 1 being the OOD samples. Each model's predictive uncertainty was used as confidence score to predict whether a test sample is a ID or OD sample.

### Computational Complexity of MGP and MNPs

In addition to the empirical difference of wall-clock time per epoch in Table 5, we provide computational complexity of the two models in Table 8. We assume that the number of inducing points in MGP equals to the number of context points in MNPs. During training of MNPs, each modality requires a cross-attention (\(\mathcal{O}(N^{m}N_{T})\)) and a contrastive learning (\(\mathcal{O}((N_{T})^{2})\)) that sum to \(\mathcal{O}(M(N^{m}N_{T}+(N_{T})^{2}))\) with \(M\) being the number of modalities, whereas during inference, each modality only requires the cross-attention which results in \(\mathcal{O}(MN^{m}N_{T})\).

## Appendix C Ablation Studies

In this section, we analyse MNPs' performance with different settings and show the effectiveness of the proposed framework.

### Context Memory Updating Mechanisms

We compare the updating mechanism of DCM based on MSE in Equation (2)-(3) with three other baselines: random sampling, first-in-first-out (FIFO) [62], and cross-entropy based (CE). Random sampling bypasses DCM and randomly selects training samples during inference. For FIFO, we follow the original procedure proposed by [62] that updates the context memory during training and only uses it during inference. CE-based mechanism replaces \(j^{*}\) in Equation (3) with \(j^{*}=\underset{j\in\{1,...,N_{T}\}}{\text{argmax}}\frac{1}{K}\sum_{k=1}^{K} -T_{Y}[j,k]\log\left(\widehat{T}_{Y}^{m}[j,k]\right)\).

We provide experimental results for all the experiments outlined in Section 5. We highlight that random sampling and FIFO achieve high accuracy both without noise and with noise as shown in Table 9 and 11. However, MSE and CE outperform the others in terms of ECE in Table 10 and OOD AUC in Table 12. As MSE and CE select the new context points based on classification error, the selected context points tend to be close to decision boundary, which is the most difficult region to classify. We believe this may contribute to the lower calibration error, suppressing overconfident predictions. The MSE and CE mechanisms show comparable overall results, but we selected MSE for its lower ECE. In terms of time efficiency, Table 13 shows that random sampling is slower than the other three methods.

For DCM updated by MSE, we also provide difference in performance for a range of number of context points \(N^{m}\) in Figure 3-9. For every figure, the bold line indicates the mean value, and the shaded area indicates 95% confidence interval. Unsurprisingly, the training time and the testing time increase with respect to \(N^{m}\). The general trend in test accuracy across the datasets shows the benefit of increasing the number of context points. However, the performance gain in ECE and OOD AUC is ambivalent as different patterns are observed for different datasets. We leave an in-depth analysis of this behaviour for our future study.

\begin{table}
\begin{tabular}{c c c} \hline \hline  & Training & Inference \\ \hline MGP & \(\mathcal{O}(M(N^{m})^{3})\) & \(\mathcal{O}(M(N^{m})^{3})\) \\ MNPs (Ours) & \(\mathcal{O}(M(N^{m}N_{T}+(N_{T})^{2}))\) & \(\mathcal{O}(MN^{m}N_{T})\) \\ \hline \hline \end{tabular}
\end{table}
Table 8: Computational complexity of MGP and MNPs.

\begin{table}
\begin{tabular}{c c c c c} \hline \hline Updating & \multicolumn{4}{c}{Dataset} \\ Mechanism & Handwritten & CUB & PIE & Caltech101 & Scene15 & HMDB \\ \hline Random & 31.80\(\pm\)3.68 & 8.15\(\pm\)2.80 & 12.14\(\pm\)3.09 & 255.37\(\pm\)13.25 & 33.73\(\pm\)3.56 & 79.19\(\pm\)5.95 & 710.48\(\pm\)8.58 \\ FIFO & 24.91\(\pm\)0.68 & 5.87\(\pm\)3.06 & 7.20\(\pm\)2.77 & 101.02\(\pm\)2.90 & **25.44\(\pm\)3.35** & **41.50\(\pm\)2.74** & 496.23\(\pm\)10.85 \\ CE & 25.00\(\pm\)0.28 & 5.61\(\pm\)1.56 & 6.85\(\pm\)1.04 & 101.10\(\pm\)2.59 & 25.47\(\pm\)3.77 & 43.45\(\pm\)3.85 & 500.79\(\pm\)7.04 \\ MSE & **22.53\(\pm\)1.88** & **5.57\(\pm\)1.58** & **6.70\(\pm\)0.92** & **101.01\(\pm\)2.38** & 26.60\(\pm\)10.37 & 41.87\(\pm\)2.10 & **493.18\(\pm\)9.91** \\ \hline \hline \end{tabular}
\end{table}
Table 13: Wall-clock inference time (ms/epoch) with different context memory updating mechanisms.

\begin{table}
\begin{tabular}{c c c c c c} \hline \hline Updating & \multicolumn{4}{c}{Dataset} \\ Mechanism & Handwritten & CUB & PIE & Caltech101 & Scene15 & HMDB \\ \hline Random & 98.39\(\pm\)0.21 & 83.11\(\pm\)4.08 & 92.55\(\pm\)0.55 & 89.36\(\pm\)1.18 & 72.85\(\pm\)2.30 & 62.10\(\pm\)0.44 \\ FIFO & 98.51\(\pm\)0.11 & 85.86\(\pm\)2.87 & **93.81\(\pm\)0.67** & 89.59\(\pm\)1.02 & 72.59\(\pm\)1.82 & 63.00\(\pm\)0.89 \\ CE & 98.49\(\pm\)0.13 & 88.80\(\pm\)1.57 & 93.75\(\pm\)0.72 & **92.87\(\pm\)0.21** & 73.98\(\pm\)0.41 & 63.97\(\pm\)0.71 \\ MSE & **98.58\(\pm\)0.10** & **88.96\(\pm\)1.98** & 93.80\(\pm\)0.49 & 92.83\(\pm\)0.18 & **74.14\(\pm\)0.35** & **64.11\(\pm\)0.15** \\ \hline \hline \end{tabular}
\end{table}
Table 11: Average test accuracy across 10 noise levels with different context memory updating mechanisms \((\uparrow)\).

\begin{table}
\begin{tabular}{c c c c c c c} \hline \hline Updating & \multicolumn{4}{c}{Dataset} \\ Mechanism & Handwritten & CUB & PIE & Caltech101 & Scene15 & HMDB \\ \hline Random & 0.007\(\pm\)0.001 & 0.069\(\pm\)0.029 & 0.050\(\pm\)0.009 & 0.043\(\pm\)0.005 & 0.059\(\pm\)0.061 & 0.052\(\pm\)0.006 \\ FIFO & 0.007\(\pm\)0.001 & 0.067\(\pm\)0.021 & 0.057\(\pm\)0.016 & 0.027\(\pm\)0.004 & 0.056\(\pm\)0.048 & 0.032\(\pm\)0.007 \\ CE & 0.006\(\pm\)0.001 & 0.050\(\pm\)0.016 & 0.041\(\pm\)0.009 & **0.017\(\pm\)0.003** & **0.038\(\pm\)0.010** & 0.034\(\pm\)0.008 \\ MSE & **0.005\(\pm\)0.001** & **0.049\(\pm\)0.008** & **0.040\(\pm\)0.005** & **0.017\(\pm\)0.003** & **0.038\(\pm\)0.010** & **0.028\(\pm\)0.006** \\ \hline \hline \end{tabular}
\end{table}
Table 10: Test ECE with different context memory updating mechanisms \((\downarrow)\).

\begin{table}
\begin{tabular}{c c c c c c} \hline \hline Updating & \multicolumn{4}{c}{Dataset} \\ Mechanism & Handwritten & CUB & PIE & Caltech101 & Scene15 & HMDB \\ \hline Random & 0.007\(\pm\)0.001 & 0.069\(\pm\)0.029 & 0.050\(\pm\)0.009 & 0.043\(\pm\)0.005 & 0.059\(\pm\)0.061 & 0.052\(\pm\)0.006 \\ FIFO & 0.007\(\pm\)0.001 & 0.067\(\pm\)0.021 & 0.057\(\pm\)0.016 & 0.027\(\pm\)0.004 & 0.056\(\pm\)0.048 & 0.032\(\pm\)0.007 \\ CE & 0.006\(\pm\)0.001 & 0.050\(\pm\)0.016 & 0.041\(\pm\)0.009 & **0.017\(\pm\)0.003** & **0.038\(\pm\)0.010** & 0.034\(\pm\)0.008 \\ MSE & **0.005\(\pm\)0.001** & **0.049\(\pm\)0.008** & **0.040\(\pm\)0.005** & **0.017\(\pm\)0.003** & **0.038\(\pm\)0.010** & **0.028\(\pm\)0.006** \\ \hline \hline \end{tabular}
\end{table}
Table 10: Test ECE with different context memory updating mechanisms \((\downarrow)\).

Figure 4: Test accuracy, ECE, average training time, and average testing time with different \(N^{m}\) for the CUB dataset.

Figure 3: Test accuracy, ECE, average training time, and average testing time with different \(N^{m}\) for the Handwritten dataset.

Figure 5: Test accuracy, ECE, average training time, and average testing time with different \(N^{m}\) for the PIE dataset.

Figure 6: Test accuracy, ECE, average training time, and average testing time with different \(N^{m}\) for the Caltech101 dataset.

Figure 8: Test accuracy, ECE, average training time, and average testing time with different \(N^{m}\) for the HMDB dataset.

Figure 7: Test accuracy, ECE, average training time, and average testing time with different \(N^{m}\) for the Scene15 dataset.

### Multimodal Aggregation Methods

We demonstrate the performance of MBA compared with two other methods namely "Concat" and "Mean". "Concat" bypasses MBA and directly provides \(r_{*}^{m}\) of multiple modalities to the decoder (see Figure 1) by simple concatenation followed by passing to a MLP which lets \(p(f(T_{X}^{M})|C^{M},T_{X}^{M})\) in Equation (12) be parameterised by a decoder where \(\{C^{M},T_{X}^{M}\}=MLP(Concat(\{r_{*}^{m}\}_{r=1}^{M}))\). \(Concat(\cdot)\) represents concatenating multiple vectors along their feature dimension. Similarly, "Mean" also bypasses MBA and simply averages the multiple modalities into single representation. Formally, \(p(f(T_{X}^{M})|C^{M},T_{X}^{M})\) parameterised by a decoder where \(\{C^{M},T_{X}^{M}\}=\frac{1}{M}\sum_{m=1}^{M}r_{*}^{m}\).

The results are shown in Table 14-17. In every case, MBA outperforms both baselines. While similar performance can be observed for Handwritten, Scene15, and Caltech101, large differences are observed in CUB, PIE, and HMDB across different metrics. The test accuracy of CIFAR10 is almost consistent across all methods, but large gaps in ECE and OOD performance are observed. This highlights the importance of MBA, especially in robustness and calibration performance.

\begin{table}
\begin{tabular}{c c c c c c c} \hline \hline Aggregation & \multicolumn{6}{c}{Dataset} \\ \cline{2-7} Methods & Handwritten & CUB & PIE & Caltech101 & Scene15 & HMDB \\ \hline Concat & 99.35\(\pm\)0.22 & 89.00\(\pm\)1.24 & 89.71\(\pm\)2.49 & 92.63\(\pm\)0.18 & 77.18\(\pm\)0.64 & 56.06\(\pm\)2.13 \\ Mean & 99.45\(\pm\)0.11 & 92.50\(\pm\)2.43 & 90.88\(\pm\)2.24 & 93.14\(\pm\)0.25 & 77.60\(\pm\)0.56 & 57.80\(\pm\)1.97 \\ MBA & **99.50\(\pm\)0.00** & **93.50\(\pm\)1.71** & **95.00\(\pm\)0.62** & **93.46\(\pm\)0.32** & **77.90\(\pm\)0.71** & **71.97\(\pm\)0.43** \\ \hline \hline \end{tabular}
\end{table}
Table 14: Test accuracy with different multimodal aggregation methods (\(\uparrow\)).

Figure 9: Test accuracy, ECE, OOD AUC (SVHN), OOD AUC (CIFAR100), average training time, and average testing time with different \(N^{m}\) for the CIFAR10-C dataset.

### Attention Types

We decompose the attention weight \(A(T_{X}^{m},C_{X}^{m})\) in Equation (9) as follows:

\[A(T_{X}^{m},C_{X}^{m})=\text{Norm}(\text{Sim}(T_{X}^{m},C_{X}^{m}))\] (23)

where \(\text{Norm}(\cdot)\) is the normalisation function such as Softmax and Sparsemax, and \(\text{Sim}(\cdot,\cdot)\) as the similarity function such as the dot-product and the RBF kernel. We provide experimental results of four different combinations of normalisation functions and similarity functions in Table 18-21.

Among the four combinations, the RBF function with Sparsemax outperforms the others in most cases. More importantly, Table 20 shows a large difference in robustness to noisy samples between the RBF function with Sparsemax and the dot-product with Sparsemax, even when a marginal difference in accuracy is shown in Table 18. For instance, for the PIE dataset, the difference in accuracy without noisy samples is 0.3, but the difference increases to 6.0 in the presence of noisy samples. The same pattern is observed with OOD AUC in Table 21. This illustrates the strength of RBF attention that is more sensitive to distribution-shift as shown in Figure 2. Lastly, for both similarity functions, Sparsemax results in superior overall performance.

\begin{table}
\begin{tabular}{c c c c c c c} \hline \hline Aggregation & \multicolumn{6}{c}{Dataset} \\ \cline{2-7} Methods & Handwritten & CUB & PIE & Caltech101 & Scene15 & HMDB \\ \hline Concat & 97.71\(\pm\)0.46 & 85.51\(\pm\)1.42 & 85.94\(\pm\)2.48 & 89.84\(\pm\)0.17 & 72.23\(\pm\)0.52 & 45.22\(\pm\)2.86 \\ Mean & 98.42\(\pm\)0.09 & 88.27\(\pm\)1.83 & 88.74\(\pm\)2.33 & 92.07\(\pm\)0.16 & 74.06\(\pm\)0.28 & 49.58\(\pm\)2.24 \\ MBA & **98.58\(\pm\)0.10** & **88.96\(\pm\)1.98** & **93.80\(\pm\)0.49** & **92.83\(\pm\)0.18** & **74.14\(\pm\)0.35** & **64.11\(\pm\)0.15** \\ \hline \hline \end{tabular}
\end{table}
Table 16: Average test accuracy across 10 noise levels with different multimodal aggregation methods (\(\uparrow\)).

\begin{table}
\begin{tabular}{c c c c c c} \hline \hline Aggregation & \multicolumn{6}{c}{OOD AUC \(\uparrow\)} \\ \cline{2-7} Methods & Test accuracy \(\uparrow\) & ECE \(\downarrow\) & SVHN & CIFAR100 \\ \hline Concat & 74.24\(\pm\)0.27 & 0.125\(\pm\)0.005 & 0.781\(\pm\)0.016 & 0.728\(\pm\)0.004 \\ Mean & 74.72\(\pm\)0.24 & 0.109\(\pm\)0.003 & 0.803\(\pm\)0.007 & 0.742\(\pm\)0.003 \\ MBA & **74.92\(\pm\)0.07** & **0.011\(\pm\)0.001** & **0.872\(\pm\)0.002** & **0.786\(\pm\)0.005** \\ \hline \hline \end{tabular}
\end{table}
Table 17: Test accuracy (\(\uparrow\)), ECE (\(\downarrow\)), and OOD detection AUC (\(\uparrow\)) with different multimodal aggregation methods.

\begin{table}
\begin{tabular}{c c c c c c c} \hline \hline \multirow{2}{*}{
\begin{tabular}{c} Aggregation \\ Methods \\ \end{tabular} } & \multicolumn{6}{c}{Dataset} \\ \cline{2-7}  & Handwritten & CUB & PIE & Caltech101 & Scene15 & HMDB \\ \hline Concat & 0.007\(\pm\)0.001 & 0.109\(\pm\)0.008 & 0.092\(\pm\)0.020 & 0.038\(\pm\)0.005 & 0.061\(\pm\)0.005 & 0.060\(\pm\)0.017 \\ Mean & 0.006\(\pm\)0.001 & 0.057\(\pm\)0.012 & 0.059\(\pm\)0.008 & 0.030\(\pm\)0.004 & 0.038\(\pm\)0.005 & 0.117\(\pm\)0.014 \\ MBA & **0.005\(\pm\)0.001** & **0.049\(\pm\)0.008** & **0.040\(\pm\)0.005** & **0.017\(\pm\)0.003** & **0.038\(\pm\)0.009** & **0.028\(\pm\)0.006** \\ \hline \hline \end{tabular}
\end{table}
Table 15: Test ECE with different multimodal aggregation methods (\(\downarrow\)).

### Adaptive Learning of RBF Attention

We have shown that the effectiveness of learning the RBF attention's parameters with the synthetic dataset in Figure 2. We further provide the ablation studies with the real-world datasets in Table 22-25.

\begin{table}
\begin{tabular}{c c c c c c c} \hline \hline \multirow{2}{*}{\begin{tabular}{c} Similarity \\ Function \\ \end{tabular} } & \multicolumn{4}{c}{\begin{tabular}{c} Normalisation \\ Function \\ \end{tabular} } \\ \cline{2-7}  & \begin{tabular}{c} Softmax \\ Sparsemax \\ \end{tabular} & \begin{tabular}{c} 0.019\(\pm\)0.005 \\ **0.001** \\ \end{tabular} & \begin{tabular}{c} 0.084\(\pm\)0.020 \\ **0.049\(\pm\)0.008** \\ \end{tabular} & \begin{tabular}{c} 0.100\(\pm\)0.017 \\ **0.017\(\pm\)0.003** \\ \end{tabular} & \begin{tabular}{c} 0.025\(\pm\)0.007 \\ **0.038\(\pm\)0.009** \\ \end{tabular} & \begin{tabular}{c} 0.202\(\pm\)0.019 \\ **0.028\(\pm\)0.006** \\ \end{tabular} \\ \cline{2-7}  & 
\begin{tabular}{c} Softmax \\ Sparsemax \\ \end{tabular

## Appendix D Broader Impacts

As a long-term goal of this work is to make multimodal classification of DNNs more trustworthy by using NPs, it has many potential positive impacts to our society. Firstly, with transparent and calibrated predictions, more DNNs can be deployed to safety-critical domains such as medical diagnosis. Secondly, this work raises awareness to the machine learning society to evaluate and review reliability of a DNN model. Lastly, our study shows the potential capability of NPs in more diverse applications. Nevertheless, a potential negative impact may exist if the causes of uncertain predictions are not fully understood. To take a step further to reliable DNNs, the source of uncertainty should be transparent to non-experts.