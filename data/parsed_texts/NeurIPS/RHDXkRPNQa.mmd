# Learning Rule-Induced Subgraph Representations for Inductive Relation Prediction

 Tianyu Liu\({}^{1}\)   Qitan Lv\({}^{1}\)   Jie Wang\({}^{1,2}\)1   Shuling Yang\({}^{1}\)   Hanzhu Chen\({}^{1}\)

\({}^{1}\)University of Science and Technology of China

\({}^{2}\)Institute of Artificial Intelligence, Hefei Comprehensive National Science Center

{tianyu_liu, qitanlv, slyang0916, chenhz}@mail.ustc.edu.cn

{jiewangx}@ustc.edu.cn

Footnote 1: The Corresponding Author.

###### Abstract

Inductive relation prediction (IRP)--where entities can be different during training and inference--has shown great power for completing evolving knowledge graphs. Existing works mainly focus on using graph neural networks (GNNs) to learn the representation of the subgraph induced from the target link, which can be seen as an implicit rule-mining process to measure the plausibility of the target link. However, these methods cannot differentiate the target link and other links during message passing, hence the final subgraph representation will contain irrelevant rule information to the target link, which reduces the reasoning performance and severely hinders the applications for real-world scenarios. To tackle this problem, we propose a novel _single-source edge-wise_ GNN model to learn the **R**ule-induc**Ed **S**ubgraph represent**T**ations (**REST**), which encodes relevant rules and eliminates irrelevant rules within the subgraph. Specifically, we propose a _single-source_ initialization approach to initialize edge features only for the target link, which guarantees the relevance of mined rules and target link. Then we propose several RNN-based functions for _edge-wise_ message passing to model the sequential property of mined rules. REST is a simple and effective approach with theoretical support to learn the _rule-induced subgraph representation_. Moreover, REST does not need node labeling, which significantly accelerates the subgraph preprocessing time by up to **11.66\(\times\)**. Experiments on inductive relation prediction benchmarks demonstrate the effectiveness of our REST2.

Footnote 2: Our code is available at https://github.com/smart-lty/REST

## 1 Introduction

Knowledge graphs are a collection of factual triples about human knowledge. In recent years, knowledge graphs have been successfully applied in various fields, such as natural language processing [1], question answering [2] and recommendation systems [3].

However, due to issues such as privacy concerns or data collection costs, many real-world knowledge graphs are far from completion. Moreover, knowledge graphs are continuously evolving with new entities or triples emerging. This dynamic change causes even the large-scale knowledge graphs, e.g., Freebase [4], Wikidata [5] and YAGO3 [6], to still suffer from incompleteness. Most existing knowledge graph completion models, e.g., RotatE [7], R-GCN [8], suffer from handling emerging new entities as they require test entities to be observed in training time. Therefore, inductive relation prediction, which aims at predicting missing links in evolving knowledge graphs, has attracted extensive attention[9, 10].

The key idea of inductive relation prediction on knowledge graphs is to learn _logical rules_, which can capture co-occurrence patterns between relations in an entity-independent manner and can thus naturally generalize to unseen entities [11; 12]. Some existing models, e.g., AMIE+[13], Neural LP[14], explicitly mine logical rules for inductive relation prediction with good interpretability[14], while their performances are limited due to the large searching space and discrete optimization[15; 16]. Recently, some **subgraph-based** methods, e.g., GraIL[11], TACT[12], CoMPILE[17], have been proposed to implicitly mine logical rules by reasoning over the subgraph induced from the target link.

However, there are still some irrelevant rules[18] within the subgraph. Considering the rule body and the rule head as a cycle[19], the relevant rules are cycles that pass the target link. As illustrated in Figure 1, \(u\to v\to e_{4}\to u\) and \(u\to v\to e_{2}\to e_{1}\to u\) are relevant rules as they pass the target link, while \(e_{1}\to e_{3}\to e_{2}\to e_{1}\) are irrelevant rules as they do not contain the target link. Existing methods cannot differentiate the target link and other links during message passing. Thus, they will mine plenty of irrelevant rules and encode them into the final subgraph representation, which makes the model prone to over-fitting and severely hinders reasoning performance.

In this paper, we propose a novel **single-source edge-wise** GNN model to learn the **R**ule-inducEd **S**ubgraph represent**T**ations (**REST**), which encodes relevant rules and eliminates irrelevant rules within the subgraph. Specifically, we observe that the information flow originating from a unique edge and returning to this edge will form a cycle automatically. Consequently, the information flow originating from the target link can encode the relevant logical rules. Inspired by this observation, we propose a **single-source** initialization approach to assign a nonzero initial embedding for the target link according to its relation and zero embeddings for other links. Then we propose several RNN-based functions for **edge-wise** message passing to model the sequential property of mined rules. Finally, we use the representation of the target link as the final subgraph representation.

We theoretically show that with appropriate message passing functions, REST can learn the rule-induced subgraph representation for reasoning. Notably, REST avoids the heavy burden of node labeling in subgraph preprocessing, which significantly accelerates the time of subgraph preprocessing by up to **11.66\(\times\)**. Experiments on inductive relation prediction benchmarks demonstrate the effectiveness of our REST.

## 2 Related Work

Existing works for IRP can be mainly categorized into **rule-based** methods and **subgraph-based** methods. While rule-based methods explicitly learn logical rules in knowledge graphs, subgraph-based methods implicitly mine logical rules by learning the representation of subgraphs. Moreover, we discuss some graph neural network methods that reason over the whole graph.

**Rule-based methods.** Rule-based approaches mine logical rules that are independent of entities and describe co-occurrence patterns of relations to predict missing factual triples. Such a rule consists of a head and a body, where a head is a single atom, i.e., a fact in the form of _Relation(head entity, tail entity)_, and a body is a set of atoms. Given a head \(R(y,x)\) and a body \(\{B_{1},B_{2},\cdots,B_{n}\}\), there is a rule \(R(y,x)\gets B_{1}\wedge B_{2}\wedge\cdots\wedge B_{n}\). The rule-based methods have been studied for a long time in Inductive Logic Programming [20], yet traditional approaches face the challenges of optimization and scalability. Recently, Neural LP [14] presents an end-to-end differentiable framework that enables modern gradient-based optimization techniques to learn the structure and parameters of logical rules. DRUM[21] analyzes Neural LP from the perspective of low-rank tensor approximation and uses bidirectional RNNs to mine more accurate rules. Moreover, for automatically learning rules from large knowledge graphs, RLvLR [22] proposes an effective rule searching and pruning strategy, which shows promising results on both scalability and accuracy for link prediction. However, these explicit rule-based methods lack expressive power due to rule-based nature, and cannot scale to large knowledge graphs as well.

Figure 1: Relevant and irrelevant rules.

**Subgraph-based methods.** Subgraph-based methods extract a local subgraph around the target link and use GNNs to learn subgraph representation to predict link existence. Such a subgraph is usually induced by the neighbor nodes of the target link, which encodes the rules related to the target link. GraIL[11] is the first subgraph-based inductive relation prediction model. It defines the _enclosing subgraph_ as the graph induced by all the nodes in the paths between two target nodes. After labeling all the nodes with _double radius vertex labeling_[23], it employs R-GCNs[8] to learn subgraph representation. CoMPILE[17] extracts directed enclosing subgraphs to handle the asymmetric / anti-symmetric patterns of the target link. TACT[12] converts the original enclosing subgraph into a relational correlation graph, and proposes a relational correlation network to model different correlation patterns between relations. More recently, SNRI[24] extracts enclosing subgraphs with complete neighboring relations to consider neighboring relations for reasoning. ConGLR[25] converts the original enclosing subgraph into a context graph to model relational paths. However, these methods cannot differentiate the target link and other links during message passing. Therefore, the GNNs will mine plenty of irrelevant rules for other links and encode them into subgraph representation, which reduces the accuracy of reasoning.

**Graph neural network for link prediction.** Some methods use GNNs to reason over the whole graph rather than a subgraph for inductive relation prediction. INDIGO[26] converts the KG into a node-annotated graph and fully encodes it into a GNN. NBFNet[27] generalizes Bellman-Ford algorithm and proposes a general GNN framework to learn path representation for link prediction. MorsE [28] considers transferring entity-independent meta-knowledge by GNNs. While these methods share some spirit with subgraph-based methods, they are essentially different with subgraph-based methods. These methods need to reason over the whole graph for a test example, while subgraph-based methods only need to reason over a subgraph. Meanwhile, these methods tend to predict entities rather than relations for a query, while subgraph-based methods tend to predict relations, as the subgraph only needs to be extracted once for relation prediction. As these methods benefit from a larger number of negative sampling, we do not take them into comparison.

## 3 Problem Definition

We define a training graph as \(\mathcal{G}_{\mathrm{tr}}=(\mathcal{E}_{\mathrm{tr}},\mathcal{R}_{\mathrm{tr }},\mathcal{T}_{\mathrm{tr}})\), where \(\mathcal{E}_{\mathrm{tr}}\), \(\mathcal{R}_{\mathrm{tr}}\), and \(\mathcal{T}_{\mathrm{tr}}\subset\mathcal{E}_{\mathrm{tr}}\times\mathcal{R}_{ \mathrm{tr}}\times\mathcal{E}_{\mathrm{tr}}\) are the set of entities, relations and triples during training, respectively. We aim to train a model such that for **any** graph \(\mathcal{G}^{\prime}=(\mathcal{E}^{\prime},\mathcal{R}^{\prime},\mathcal{T}^{ \prime})\) whose relations are **all** seen during training (i.e., \(\mathcal{R}^{\prime}\subseteq\mathcal{R}_{\mathrm{tr}}\)), the model can predict missing triples in \(\mathcal{G}^{\prime}\), i.e., \((?,r_{t},v),(u,?,v),(u,r_{t},?)\), where \(u,v\in\mathcal{E}^{\prime}\) and \(r_{t}\in\mathcal{R}^{\prime}\). We denote the set of any possible entities as \(\{\mathcal{E}\}\) and the set of knowledge graphs whose relation set \(\mathcal{R}\) are subset of \(\mathcal{R}_{\mathrm{tr}}\) as \(\{\mathcal{G}\}_{\mathrm{tr}}\). The model we would like to train is a score function \(f:\{\mathcal{G}\}_{\mathrm{tr}}\times\{\mathcal{E}\}\times\mathcal{R}_{ \mathrm{tr}}\times\{\mathcal{E}\}\rightarrow\mathbb{R}\), \((\mathcal{G}^{\prime},u,r_{t},v)\mapsto f(\mathcal{G}^{\prime},u,r_{t},v)\), where \(\mathcal{G}^{\prime}=(\mathcal{E}^{\prime},\mathcal{R}^{\prime},\mathcal{T}^{ \prime}),\mathcal{R}^{\prime}\subseteq\mathcal{R}_{\mathrm{tr}}\) and \(u,v\in\mathcal{E}^{\prime}\). For a query triple \((u,r_{t},?)\), we enumerate valid candidate tail entities \(v^{\prime}\) and use the model to get the score \(s^{\prime}\) of this triple \((u,r_{t},v^{\prime})\). We call the query triple \((u,r_{t},v)\) as _target link_ and \(u,v\) as _target nodes_, respectively.

## 4 Methodology

In this section, we describe the architecture of the proposed REST in detail. Following existing subgraph-based methods, we first extract a subgraph for each query triple. Then we apply **single-source initialization** and **edge-wise message passing** to update edge features iteratively. Finally, the representation of the target link is used for scoring. REST organizes the two methods in a unified framework to perform inductive relation prediction. Figure 2 gives an overview of REST.

### Subgraph Extraction

For a query triple \((u,r_{t},v)\), the subgraph around it contains the logical rules to infer this query, thus we extract a local subgraph \(\mathcal{SG}_{u,r_{t},v}\) to implicitly learn logical rules for reasoning. Specifically, we first compute the \(k\)-hop neighbors \(\mathcal{N}_{k}(u)\) and \(\mathcal{N}_{k}(v)\) of the target nodes \(u\) and \(v\). Then we define _enclosing subgraph_ as the graph induced by \(\mathcal{N}_{k}(u)\cap\mathcal{N}_{k}(v)\) and _unclosing subgraph_ as the graph induced by \(\mathcal{N}_{k}(u)\cup\mathcal{N}_{k}(v)\). Note that the subgraph extraction process of our REST omits the node labeling, as node features are unnecessary in **edge-wise** message passing, which significantly reduces the time cost of subgraph preprocessing.

### Single-source Initialization

Single-source initialization is a simple and effective initialization approach, which initializes a nonzero embedding to the query triple according to \(r_{t}\) and zero embeddings for other triples. Specifically, the embeddings of links and nodes within \(\mathcal{SG}_{u,r,v}\) are initialized as follows:

\[\mathbf{e}^{0}_{x,y,z}=\underset{(u,r,v)}{\mathbbm{1}}(x,y,z) \odot\mathbf{r}_{y} =\begin{cases}\mathbf{r}_{y},&\quad\text{if }(x,y,z)=(u,r_{t},v)\\ \mathbf{0},&\quad\text{if }(x,y,z)\neq(u,r_{t},v)\end{cases}\] (1) \[\mathbf{h}^{0}_{v} =\mathbf{0}\qquad for\;\forall v\in\mathcal{E},\]

where \(\mathbf{e}^{0}_{x,y,z}\) and \(\mathbf{h}^{0}_{v}\) are the initial representation of edge \((x,y,z)\) and node \(v\), respectively. \(\mathbbm{1}\) is the indicator function to differentiate the target link and other links. \(\odot\) is Hadamard product. Note that the representation of nodes is used as temporary variables in edge-wise message passing. With this initialization approach, we ensure the relevance between mined rules and the target link.

### Edge-wise Message Passing

After initializing all the edges and nodes, we perform edge-wise message passing to encode all relevant rules into the final subgraph representation. Specifically, each iteration of edge-wise message passing consists of three parts, (1) applying message function to every link, (2) updating node features by aggregating message and (3) updating edge features by temporary node features, which are described as follows:

\[\mathbf{m}^{k}_{x,y,z}=\mathbf{Message}(\mathbf{h}^{k-1}_{x},\mathbf{e}^{k-1}_{ x,y,z},\mathbf{r}_{y})=(\mathbf{h}^{k-1}_{x}\otimes^{1}\mathbf{r}_{y})\uplus( \mathbf{e}^{k-1}_{x,y,z}\otimes^{2}\mathbf{r}_{y})\] (2)

\[\mathbf{h}^{k}_{z}=\underset{(x,y,z)\in\mathcal{T}}{\mathbf{Aggragate}}( \mathbf{m}^{k}_{x,y,z})=\bigoplus_{(x,y,z)\in\mathcal{T}}\mathbf{m}^{k}_{x,y,z}\] (3)

\[\mathbf{e}^{k}_{x,y,z}=\mathbf{Update}(\mathbf{h}^{k}_{x},\mathbf{e}^{k-1}_{x, y,z})=\mathbf{h}^{k}_{x}\diamond\mathbf{e}^{k-1}_{x,y,z}\] (4)

Here, \(\uplus,\oplus,\diamond,\otimes^{1},\otimes^{2}\) are binary operators which denote a function to parameterize. \(\bigoplus\) denotes the large size operator of \(\oplus\). \(\mathbf{h}^{k}_{z}\) and \(\mathbf{e}^{k}_{x,y,z}\) respectively represent the feature of node \(z\) and link \((x,y,z)\) after \(k\) iterations of edge-wise message passing. We visualize the comparison between conventional message passing framework developed by GraIL[11] and proposed edge-wise message passing framework in Figure 3.

Figure 2: An overview of REST. REST organizes the _single-source_ initialization method and the _edge-wise_ message passing method in a unified framework to learn relevant rules representations within the subgraph for the target link. Different relevant rules are shown in different colors in part 3.

### RNN-based Functions

Message passing functions in existing works use order-independent binary operators such as ADD and MUL, which cannot model the sequential property of rules and lead to incorrect rules[21]. To tackle this problem, we introduce several RNN-based methods as message passing functions.

**Message Functions.** For the edge-wise message passing process, each iteration REST takes in \(\mathbf{h}_{x}^{k-1},\mathbf{e}_{x,y,z}^{k-1}\) and \(\mathbf{r}_{y}\) to form a message. We modify GRU[29] as message function as follows:

\[\begin{split}\delta_{k}&=\sigma_{g}(\mathbf{W}_{ \delta,1}^{k}\mathbf{r}_{y}\odot\mathbf{e}_{x,y,z}^{k-1}+\mathbf{W}_{\delta,2 }^{k}\mathbf{h}_{x}^{k-1}+\mathbf{b}_{\delta}^{k})\\ \gamma_{k}&=\sigma_{g}(\mathbf{W}_{\gamma,1}^{k} \mathbf{r}_{y}\odot\mathbf{e}_{x,y,z}^{k-1}+\mathbf{W}_{\gamma,2}^{k}\mathbf{ h}_{x}^{k-1}+\mathbf{b}_{\gamma}^{k})\\ c_{k}&=\sigma_{h}(\mathbf{W}_{c,1}^{k}\mathbf{r}_{y} \odot\mathbf{e}_{x,y,z}^{k-1}+\mathbf{W}_{c,2}^{k}(\gamma_{k}\odot\mathbf{h}_ {x}^{k-1}))\\ \mathbf{m}_{x,y,z}^{k}&=\delta_{k}\odot c_{k}+(1- \delta_{k})\odot\mathbf{h}_{x}^{k-1}\end{split}\] (5)

Here, \(\delta_{k}\) is the update gate vector, \(\gamma_{k}\) is the reset gate vector and \(c_{k}\) is the candidate activation vector. The operator \(\odot\) denotes the Hadamard product, \(\sigma_{g}\) denotes Sigmoid activation function and \(\sigma_{h}\) denotes Tanh activation function. During each iteration of message passing, we only use GRU once, therefore \(k\)-layer message passing includes \(k\) GRUs, which can model sequence with length \(l\leq k\).

**Aggregate Functions.** The aggregate function aggregates messages for each node from its neighboring edges. Here, we use simplified PNA[30] to consider different types of aggregation.

\[\begin{split}\mathbf{h}_{z,1}^{k}&=\underset{(x,y,z)\in\mathcal{T}}{mean}(\mathbf{m}_{x,y,z}^{k}),\;\mathbf{h}_{z,2}^{k}= \underset{(x,y,z)\in\mathcal{T}}{max}(\mathbf{m}_{x,y,z}^{k}),\\ \mathbf{h}_{z,3}^{k}&=\underset{(x,y,z)\in\mathcal{T }}{min}(\mathbf{m}_{x,y,z}^{k}),\;\mathbf{h}_{z,4}^{k}=\underset{(x,y,z)\in \mathcal{T}}{std}(\mathbf{m}_{x,y,z}^{k}),\\ \mathbf{h}_{z}^{k}&=\mathbf{W}_{agg}^{k}[\mathbf{h} _{z,1}^{k};\mathbf{h}_{z,2}^{k};\mathbf{h}_{z,3}^{k};\mathbf{h}_{z,4}^{k}; \mathbf{h}_{z}^{k-1}]\end{split}\] (6)

Here, \([;]\) denotes the concatenation of vectors, \(\mathbf{W}_{agg}^{k}\) denotes the linear transformation matrix in the \(k\)-th layer.

**Update Functions.** The update function is used to update the edge feature. We propose to update the edge feature with LSTM[31]. Specifically, LSTM needs three inputs: a hidden vector, a current

Figure 3: Comparison between conventional message passing framework developed by GraIL[11] and our REST. First, REST initializes node and edge features with single-source initialization. Then, REST employs Update function to update edge features. Finally, REST directly uses the embedding of the target link as the final subgraph representation, rather than the pooling of all node embeddings.

input vector and a cell vector. We use \(\mathbf{h}_{x}^{k}\) as the hidden vector and \(\mathbf{e}_{x,y,z}^{k-1}\) as the current input vector. Moreover, we expect that each edge can _differentiate the target link_ during message passing, which requires each edge to specify the query information. Therefore, we initialize each edge with another _query_ feature as the cell vector. All the edges are initialized with the same query embedding \(\mathbf{r}_{r}^{q}\) related to the query relation \(r\).

\[\mathbf{q}_{x,y,z}^{0}=\mathbf{r}_{r}^{q}\] (7)

Then the update function can be described as follows:

\[\mathbf{e}_{x,y,z}^{k},\mathbf{q}_{x,y,z}^{k}=\mathbf{LSTM}(\mathbf{e}_{x,y, z}^{k-1},\mathbf{q}_{x,y,z}^{k-1},\mathbf{h}_{x}^{k})\] (8)

After updating the edge feature by \(k\) iterations of edge-wise message passing, we output \(\mathbf{e}_{u,r,v}^{k}\) as the subgraph representation. Then we use a linear transformation and an activation function to get the score of the target link.

\[f(u,r_{t},v)=\sigma(\mathbf{W}_{s}\mathbf{e}_{u,r_{t},v}^{k}+\mathbf{b}_{s})\] (9)

## 5 Analysis

In this section, we theoretically analyze the effectiveness of our REST. We first define the rule-induced subgraph representation, which utilizes encoded relevant rules to infer the plausibility of the target link. Then we show that our REST is able to learn such a rule-induced subgraph representation for reasoning.

### Rule-induced Subgraph Representation Formulation

Our rule-induced subgraph representation aims to encode all relevant rules into the subgraph representation for reasoning. Therefore, we can define the rule-induced subgraph representation as the aggregation of these relevant rules:

\[\mathcal{S}_{u,r_{t},v}=\bigoplus_{c\in\mathcal{C}}\mathbf{p}_{c},\] (10)

where \(\mathcal{C}\) denotes the set of all possible relevant rules within \(\mathcal{SG}_{u,r_{t},v}\) and \(\mathbf{p}_{c}\) is the representation of a relevant rule \(c\). Following the idea of Neural LP[14] to associate each relation in the rule with a weight, we model the representation of a rule as a function of its relation set. Therefore, we give the definition of rule-induced subgraph representation.

**Definition 1** (Rule-induced subgraph representation.): _Given a subgraph \(\mathcal{SG}_{u,r_{t},v}\), its rule-induced subgraph representation is defined as follows:_

\[\mathcal{S}_{u,r_{t},v}=\bigoplus_{i=1}^{k}\bigoplus_{\begin{subarray}{c}(u, r_{t},v)\end{subarray}}\bigoplus_{\begin{subarray}{c}(v,y_{0},x_{0}) \end{subarray}}...\bigoplus_{\begin{subarray}{c}(x_{i-3},y_{i-2},u)\\ i\end{subarray}}\alpha_{i1}\mathbf{r}_{r_{t}}\otimes\alpha_{i2}\mathbf{r}_{y _{0}}\otimes...\otimes\alpha_{ii}\mathbf{r}_{y_{i-2}}\] (11)

_where \(i\) denotes the length of the cycle, \(\mathbf{r}_{y}\), is the representation of relation \(y_{i}\), \((x_{i},y_{i},x_{i-1})\) is an existing triple in \(\mathcal{SG}_{u,r_{t},v}\). \(\{(u,r_{t},v),(v,y_{0},x_{0}),...,(x_{i-3},y_{i-2},u)\}\) is a cycle at length \(i\)._

Note that \(\oplus\) and \(\otimes\) denote binary aggregation functions. Intuitively, rule-induced subgraph representation captures all relevant rules within the subgraph and is expressive enough for reasoning.

### Rule-induced Subgraph Representation Learning

Here, we show that our REST can learn such a rule-induced subgraph representation. First, we show this in a simple case.

**Theorem 1**: _Single-source edge-wise GNN can learn rule-induced subgraph representation if \(\uplus=+,\oplus=+,\diamond=+\), \(\otimes^{1}=\times,\otimes^{2}=\times\). i.e., there exists nonzero \(\alpha_{i,j}\) such that_

\[\mathbf{e}_{u,r_{t},v}^{k}=\sum_{i=1}^{k}\underbrace{\sum_{(u,r_{t},v)}\sum_ {(v,y_{0},x_{0})}...\sum_{(x_{i-3},y_{i-2},u)}}_{i}\alpha_{i1}\mathbf{r}_{r_{ t}}\times\alpha_{i2}\mathbf{r}_{y_{0}}\times...\times\alpha_{ii}\mathbf{r}_{y_{i-2}}\] (12)We prove this in Appendix A. This theorem states that our REST can learn the rule-induced subgraph representation in the basic condition. Then we generalize this theorem to a general version.

**Theorem 2**: _Single-source edge-wise GNN can learn rule-induced subgraph representation if \(\uplus=\oplus,\oplus=\oplus,\diamond=\oplus,\otimes^{1}=\otimes,\otimes^{2}=\otimes\), where \(\oplus\) and \(\otimes\) are binary operators that satisfy \(0\oplus a=a,0\otimes a=0\). i.e., there exists nonzero \(\alpha_{i,j}\) such that_

\[\mathbf{e}_{u,r_{t},v}^{k}=\bigoplus_{i=1}^{k}\underbrace{\bigoplus_{(u,r_{t},v )}\bigoplus_{(v,y_{0},x_{0})}...\bigoplus_{(x_{i-3},y_{i-2},u)}}_{i}\alpha_{i1} \mathbf{r}_{r_{t}}\otimes\alpha_{i2}\mathbf{r}_{y_{0}}\otimes...\otimes\alpha_ {ii}\mathbf{r}_{y_{i-2}}\] (13)

We prove this in Appendix A. Intuitively, we can get this theorem by replacing \(+,\times\) with \(\oplus,\otimes\). The key step to learn rule-induced subgraph representation is to ensure \(\mathbf{e}_{x,y,z}^{0}\neq 0\) if and only if \((x,y,z)=(u,r_{t},v)\). Existing models[11; 12] do not satisfy this requirement, as they initialize both the target link and the other links with nonzero embeddings. Therefore, their final subgraph representations contain irrelevant rule terms, which leads to suboptimal results. On the contrary, we show that with appropriate message passing functions, REST learns rule-induced subgraph representation. As the rule-induced subgraph representation encodes all relevant rules within the subgraph, REST is expressive enough to infer the plausibility of any reasonable triple, while it eliminates the negative influence of irrelevant rules.

Our analysis gives some insight of IRP methods. First, eliminating noises within the extracted subgraph is crucial for subgraph-based methods. While existing methods focus on data level to extract ad-hoc subgraphs, our model proposes a simple way for denoising at the model level, i.e., single-source edge-wise message passing. Second, labeling tricks such as single-source initialization can effectively improve the model performance. Last but not least, the idea of learning links is especially important in IRP task, as links play a vital role in reasoning.

## 6 Experiments

In this section, we first introduce the experiment setup including datasets and implementation details. Then we show the main results of REST on several benchmark datasets. Finally, we conduct ablation studies, case studies and further experiments.

### Experiment Setup

**Datasets and Implementation Details** We conduct experiments on three inductive benchmark datasets proposed by GraIL[11], which are dervied from WN18RR[32], FB15K-237[33], and NELL-995[34]. For inductive relation prediction, the training set and testing set should have no overlapping entities. Details of the datasets are summarized in Appendix B. We use PyTorch[35] and DGL[36] to implement our REST. Implementation Details of REST are summarized in Appendix C.

### Main Results

We follow GraIL[11] to rank each test triple among 50 other randomly sampled negative triples. We report the Hits@10 metric on the benchmark datasets. Following the standard procedure in prior work [37], we use the filtered setting, which does not take any existing valid triples into account at ranking. We demonstrate the effectiveness of the proposed REST by comparing its performance with both rule-based methods including Neural LP [14], DRUM [21] and RuleN [38] and subgraph-based methods including GraIL [11], CoMPILE [17], TACT[12], SNRI[24] and ConGLR [25]. We run each experiment five times with different random seeds and report the mean results in Table 1.

From the Hits@10 results in Table 1, we make the observation that our model REST significantly outperforms existing methods on 12 versions of 3 datasets. Specifically, our REST can outperform rule-based baselines, including Neural LP, DRUM and RuleN by a large margin. And compared with existing subgraph-based methods, e.g., GraIL, CoMPILE, TACT, SNRI and ConGLR, REST has achieved average improvements of 17.89%, 9.35%, 13.76%; 16.23%, 8.18%, 13.04%; 13.58%, 8.06%, 8.96%; 10.89%, 4.55%,"-" and 5.58%, 5.82%, 5.1% on three datasets respectively. As RESTonly assigns the embedding of the target link, these improvements demonstrate the effectiveness of our REST via distilling all relevant rules within the subgraph.

### Ablation Study

We conduct ablation studies to validate the effectiveness of proposed single-source initialization and edge-wise message passing. We show the main results of ablation studies in Table 2.

**Single-source initialization.** Single-source initialization is vital for learning rule-induced subgraph representation. To demonstrate the effectiveness of single-source initialization, we perform another _full initialization_ method as a comparison, which initializes all edges according to their relations. As illustrated in Table 2, we can find that single-source initialization is significant for capturing relevant rules for reasoning. Without single-source initialization, the performance of REST will exhibit a significant decrease, e.g., from 92.61 to 68.26 in NELL-995 v4. This result exhibits the effectiveness of single-source initialization.

**Edge-wise message passing.** To demonstrate the necessity of proposed RNN-based functions, we conduct ablation studies on various combinations of message functions, including SUM, MUL, and GRU, as well as update functions, including LSTM and MLP. These functions are defined as follows:

\[\begin{split}\textbf{Sum}:&\textbf{m}_{x,y,z}^{k}= \textbf{h}_{x}^{k-1}+\textbf{e}_{x,y,z}^{k-1}+\textbf{r}_{y}\\ \textbf{Mul}:&\textbf{m}_{x,y,z}^{k}=\textbf{h}_{x}^ {k-1}\odot\textbf{e}_{x,y,z}^{k-1}\odot\textbf{r}_{y}\\ \textbf{Mlp}:&\textbf{e}_{x,y,z}^{k}=\textbf{W}_{e }[\textbf{e}_{x,y,z}^{k-1};\textbf{q}_{x,y,z}^{k-1};\textbf{h}_{x}^{k}]\\ &\textbf{q}_{x,y,z}^{k}=\textbf{W}_{q}[\textbf{e}_{x,y,z}^{k-1}; \textbf{q}_{x,y,z}^{k-1};\textbf{h}_{x}^{k}]\end{split}\] (14)

In general, REST benefits from RNN-based functions, as they can capture the sequential properties of rules. Using order-independent binary operators, such as ADD and MUL, leads to a decline in performance across all datasets, as they cannot differentiate correct and incorrect rules.

\begin{table}
\begin{tabular}{c l c c c c c c c c c c c c} \hline \hline  & & \multicolumn{3}{c}{**WN18RR**} & \multicolumn{3}{c}{**FB15k-237**} & \multicolumn{3}{c}{**NELL-995**} \\ \cline{3-13}  & & v1 & v2 & v3 & v4 & v1 & v2 & v3 & v4 & v1 & v2 & v3 & v4 \\ \hline \multirow{3}{*}{Rule-Based} & Neural LP & 74.37 & 68.93 & 46.18 & 67.13 & 52.92 & 58.94 & 52.90 & 55.88 & 40.78 & 78.73 & 82.71 & 80.58 \\  & DRUM & 74.37 & 68.93 & 46.18 & 67.13 & 52.92 & 58.73 & 52.90 & 55.88 & 19.42 & 78.55 & 82.71 & 80.58 \\  & RuleN & 80.85 & 78.23 & 53.39 & 71.59 & 49.76 & 77.82 & 87.69 & 85.60 & 53.50 & 81.75 & 77.26 & 61.35 \\ \hline \multirow{3}{*}{Subgraph-Based} & GraIL & 82.45 & 78.68 & 58.43 & 73.41 & 64.15 & 81.80 & 82.83 & 89.29 & 59.50 & 93.25 & 91.41 & 73.19 \\  & CoMPLLE & 83.60 & 79.82 & 60.69 & 75.49 & 67.64 & 82.89 & 84.67 & 84.44 & 58.38 & 93.87 & 92.77 & 75.19 \\  & TACT & 84.04 & 81.63 & 67.97 & 76.56 & 65.76 & 83.56 & 85.20 & 88.69 & 79.80 & 88.91 & 94.02 & 73.78 \\  & SNRI & 87.23 & 83.10 & 67.31 & 83.32 & 71.79 & 86.50 & 89.59 & 89.39 & - & - & - & - \\  & ConGLR & 85.64 & 92.93 & 70.74 & 92.90 & 68.29 & 85.98 & 88.61 & 89.31 & 81.07 & 94.92 & 94.36 & 81.61 \\ \cline{1-1} \cline{2-13}  & REST(**ours**) & **96.28** & **94.56** & **79.50** & **94.19** & **75.12** & **91.21** & **93.06** & **96.06** & **88.00** & **94.96** & **96.79** & **92.61** \\ \hline \hline \end{tabular}
\end{table}
Table 1: Hits@10 results on the inductive benchmark datasets extracted from WN18RR, FB15k-237 and NELL-995. The results of Neural LP, DURM, RuleN, GraIL, CoMPLE and ConGLR are taken from the paper [25].

\begin{table}
\begin{tabular}{c c c c c c c c c c c c c} \hline \hline  & \multicolumn{3}{c}{**WN18RR**} & \multicolumn{3}{c}{**FB15k-237**} & \multicolumn{3}{c}{**NELL-995**} \\ \cline{2-13}  & v1 & v2 & v3 & v4 & v1 & v2 & v3 & v4 & v1 & v2 & v3 & v4 \\ \hline REST & 96.28 & 94.56 & 79.50 & 94.19 & 75.12 & 91.21 & 93.06 & 96.06 & 88.00 & 94.96 & 96.79 & 92.61 \\ \hline Full Initialization & 92.55 & 90.70 & 68.76 & 79.49 & 71.71 & 79.29 & 89.25 & 91.22 & 83.00 & 86.13 & 94.54 & 68.26 \\ \(\Delta\) & \(\xrightarrow{\text{\text{\text{\text@underline{i}}}}}\)3.73 & \(\xrightarrow{\text{\text{\text{\text@underline{i}}}}}\)3.86 & \(\xrightarrow{\text{\text{\text@underline{i}}}}\)10.74 & \(\xrightarrow{\text{\text{\text@underline{i}}}}\)4.70 & \(\xrightarrow{\text{\text{\text@underline{i}}}}\)4.11 & \(\xrightarrow{\text{\text{\text@underline{i}}}}\)2.3 & \(\xrightarrow{\text{\text{\text@underline{i}}}}\)4.84 & \(\xrightarrow{\text{\text{\text@underline{i}}}}\

[MISSING_PAGE_FAIL:9]

extraction by up to \(11.66\times\), which significantly decreases the time cost of subgraph extraction. Experiments demonstrate that our proposed REST outperforms existing state-of-the-art methods on inductive relation prediction benchmarks.

Future WorkFor future work, we target at enhancing the scalability of our REST to conduct reasoning on large-scale knowledge graphs. Moreover, REST can serve as a complementary reasoning model to help large language models conduct reasoning with promising and interpretable results. Hopefully, REST will facilitate the future development of reasoning ability.

## Acknowledgements

The authors would like to thank all the anonymous reviewers for their insightful comments. This work was supported in part by National Key R&D Program of China under contract 2022ZD0119801, National Nature Science Foundations of China grants U19B2026, U19B2044, 61836011, 62021001, and 61836006.

## References

* [1] Zhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang, Maosong Sun, and Qun Liu. ERNIE: Enhanced language representation with informative entities. In _Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics_, July 2019.
* [2] Xiao Huang, Jingyuan Zhang, Dingcheng Li, and Ping Li. Knowledge graph embedding based question answering. In _Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining_, pages 105-113, 2019.
* [3] Hongwei Wang, Fuzheng Zhang, Jialin Wang, Miao Zhao, Wenjie Li, Xing Xie, and Minyi Guo. Ripplenet: Propagating user preferences on the knowledge graph for recommender systems. In _Proceedings of the 27th ACM International Conference on Information and Knowledge Management_, pages 417-426, 2018.
* [4] Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor. Freebase: a collaboratively created graph database for structuring human knowledge. In _Proceedings of the 2008 ACM SIGMOD international conference on Management of data_, pages 1247-1250, 2008.
* [5] Denny Vrandecic and Markus Krotzsch. Wikidata: a free collaborative knowledgebase. _Communications of the ACM_, 57(10):78-85, 2014.
* [6] Farzaneh Mahdisoltani, Joanna Biega, and Fabian Suchanek. Yago3: A knowledge base from multilingual wikipedias. In _7th biennial conference on innovative data systems research_. CIDR Conference, 2014.
* [7] Zhiqing Sun, Zhi-Hong Deng, Jian-Yun Nie, and Jian Tang. Rotate: Knowledge graph embedding by relational rotation in complex space. _arXiv preprint arXiv:1902.10197_, 2019.
* [8] Michael Schlichtkrull, Thomas N Kipf, Peter Bloem, Rianne Van Den Berg, Ivan Titov, and Max Welling. Modeling relational data with graph convolutional networks. In _European semantic web conference_, pages 593-607. Springer, 2018.
* [9] Mikhail Galkin, Max Berrendorf, and Charles Tapley Hoyt. An open challenge for inductive link prediction on knowledge graphs. _arXiv preprint arXiv:2203.01520_, 2022.
* [10] Zijie Geng, Shufang Xie, Yingce Xia, Lijun Wu, Tao Qin, Jie Wang, Yongdong Zhang, Feng Wu, and Tie-Yan Liu. De novo molecular generation via connection-aware motif mining. In _The Eleventh International Conference on Learning Representations_, 2023.
* [11] Komal Teru, Etienne Denis, and Will Hamilton. Inductive relation prediction by subgraph reasoning. In _International Conference on Machine Learning_, pages 9448-9457. PMLR, 2020.
* [12] Jiajun Chen, Huarui He, Feng Wu, and Jie Wang. Topology-aware correlations between relations for inductive link prediction in knowledge graphs. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 35, pages 6271-6278, 2021.
* [13] Luis Galarraga, Christina Teflioudi, Katja Hose, and Fabian M Suchanek. Fast rule mining in ontological knowledge bases with amie+. _The VLDB Journal_, 24(6):707-730, 2015.
* [14] Fan Yang, Zhilin Yang, and William W Cohen. Differentiable learning of logical rules for knowledge base reasoning. _arXiv preprint arXiv:1702.08367_, 2017.

* [15] Stanley Kok and Pedro Domingos. Statistical predicate invention. In _Proceedings of the 24th international conference on Machine learning_, pages 433-440, 2007.
* [16] William Yang Wang, Kathryn Mazaitis, and William W Cohen. Structure learning via parameter learning. In _Proceedings of the 23rd ACM international conference on conference on information and knowledge management_, pages 1199-1208, 2014.
* [17] Sijie Mai, Shuangjia Zheng, Yuedong Yang, and Haifeng Hu. Communicative message passing for inductive relation reasoning. In _AAAI_, pages 4294-4302, 2021.
* [18] K Rameshkumar, M Sambath, and S Ravi. Relevant association rule mining from medical dataset using new irrelevant rule elimination technique. In _2013 International Conference on Information Communication and Embedded Systems (ICICES)_, pages 300-304. IEEE, 2013.
* [19] Zuoyu Yan, Tengfei Ma, Liangcai Gao, Zhi Tang, and Chao Chen. Cycle representation learning for inductive relation prediction. In _International Conference on Machine Learning_, pages 24895-24910. PMLR, 2022.
* [20] Stephen Muggleton and Luc De Raedt. Inductive logic programming: Theory and methods. _The Journal of Logic Programming_, 19:629-679, 1994.
* [21] Ali Sadeghian, Mohammadreza Armandpour, Patrick Ding, and Daisy Zhe Wang. Drum: End-to-end differentiable rule mining on knowledge graphs. _Advances in Neural Information Processing Systems_, 32, 2019.
* [22] Pouya Ghiasnezhad Omran, Kewen Wang, and Zhe Wang. An embedding-based approach to rule learning in knowledge graphs. _IEEE Transactions on Knowledge and Data Engineering_, 2019.
* [23] Muhan Zhang and Yixin Chen. Link prediction based on graph neural networks. _Advances in neural information processing systems_, 31, 2018.
* [24] Xiaohan Xu, Peng Zhang, Yongquan He, Chengpeng Chao, and Chaoyang Yan. Subgraph neighboring relations infomax for inductive link prediction on knowledge graphs. _arXiv preprint arXiv:2208.00850_, 2022.
* [25] Qika Lin, Jun Liu, Fangzhi Xu, Yudai Pan, Yifan Zhu, Lingling Zhang, and Tianzhe Zhao. Incorporating context graph with logical reasoning for inductive relation prediction. In _Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval_, pages 893-903, 2022.
* [26] Shuwen Liu, Bernardo Grau, Ian Horrocks, and Egor Kostylev. Indigo: Gnn-based inductive knowledge graph completion using pair-wise encoding. _Advances in Neural Information Processing Systems_, 34:2034-2045, 2021.
* [27] Zhaocheng Zhu, Zuobai Zhang, Louis-Pascal Khonneux, and Jian Tang. Neural bellman-ford networks: A general graph neural network framework for link prediction. _Advances in Neural Information Processing Systems_, 34:29476-29490, 2021.
* [28] Mingyang Chen, Wen Zhang, Yushan Zhu, Hongting Zhou, Zonggang Yuan, Changliang Xu, and Huajun Chen. Meta-knowledge transfer for inductive knowledge graph embedding. In _Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval_, pages 927-937, 2022.
* [29] Kyunghyun Cho, Bart Van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical machine translation. _arXiv preprint arXiv:1406.1078_, 2014.
* [30] Gabriele Corso, Luca Cavalleri, Dominique Beaini, Pietro Lio, and Petar Velickovic. Principal neighbourhood aggregation for graph nets. _Advances in Neural Information Processing Systems_, 33:13260-13271, 2020.
* [31] Sepp Hochreiter and Jurgen Schmidhuber. Long short-term memory. _Neural computation_, 9(8):1735-1780, 1997.
* [32] Kristina Toutanova and Danqi Chen. Observed versus latent features for knowledge base and text inference. In _The Workshop on CVSC_, 2015.
* [33] Tim Dettmers, Pasquale Minervini, Pontus Stenetorp, and Sebastian Riedel. Convolutional 2d knowledge graph embeddings. In _AAAI_, 2018.

* [34] Wenhan Xiong, Thien Hoang, and William Yang Wang. Deeppath: A reinforcement learning method for knowledge graph reasoning. In _EMNLP_, 2017.
* [35] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. _Advances in neural information processing systems_, 32, 2019.
* [36] Minjie Yu Wang. Deep graph library: Towards efficient and scalable deep learning on graphs. In _ICLR workshop on representation learning on graphs and manifolds_, 2019.
* [37] Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Oksana Yakhnenko. Translating embeddings for modeling multi-relational data. _Advances in neural information processing systems_, 26, 2013.
* [38] Christian Meilicke, Manuel Fink, Yanjie Wang, Daniel Ruffinelli, Rainer Gemulla, and Heiner Stuckenschmidt. Fine-grained evaluation of rule-and embedding-based systems for knowledge graph completion. In _The Semantic Web-ISWC 2018: 17th International Semantic Web Conference, Monterey, CA, USA, October 8-12, 2018, Proceedings, Part I 17_, pages 3-20. Springer, 2018.
* [39] Diederick P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In _ICLR_, 2015.
* [40] Yuxia Geng, Jiaoyan Chen, Jeff Z. Pan, Mingyang Chen, Song Jiang, Wen Zhang, and Huajun Chen. Relational message passing for fully inductive knowledge graph completion, 2022.
* [41] Mikhail Galkin, Etienne Denis, Jiapeng Wu, and William L. Hamilton. Nodepiece: Compositional and parameter-efficient representations of large knowledge graphs, 2022.

Proof of Theorem 1 and 2

**Theorem 1**: _Single-source edge-wise GNN can learn rule-induced subgraph representation if \(\uplus=+,\oplus=+,\diamond=+\), \(\otimes^{1}=\times,\otimes^{2}=\times\). i.e., there exists nonzero \(\alpha_{i,j}\) such that_

\[\mathbf{e}_{u,r_{t},v}^{k}=\sum_{i=1}^{k}\underbrace{\sum_{(u,r_{t},v)}\sum_{( v,y_{0},x_{0})}...\sum_{(x_{i-3},y_{i-2},u)}}_{i}\alpha_{i1}\mathbf{r}_{r_{t}} \times\alpha_{i2}\mathbf{r}_{y_{0}}\times...\times\alpha_{ii}\mathbf{r}_{y_{i -2}}\] (15)

_Proof_ In this case, the rule-induced subgraph representation is:

\[\mathcal{S}_{u,r_{t},v}=\sum_{i=1}^{k}\underbrace{\sum_{(u,r_{t},v)}\sum_{(v,y _{0},x_{0})}...\sum_{(x_{i-3},y_{i-2},u)}}_{i}\alpha_{i1}\mathbf{r}_{r_{t}} \times\alpha_{i2}\mathbf{r}_{y_{0}}\times...\times\alpha_{ii}\mathbf{r}_{y_{i -2}}\] (16)

Then we will show that single-source edge-wise GNN can learn this rule-induced sugraph representation in **induction**.

\(k=1\). we have

\[\mathbf{e}_{u,r_{t},v}^{1}=\mathbf{h}_{u}^{1}+\mathbf{r}_{r_{t}}=\mathbf{r}_{ r_{t}}+\sum_{(x_{0},y_{0},u)\in\mathcal{T}}(\mathbf{h}_{x_{0}}^{0}+\mathbf{e}_{x_{0},y_{0},u}^{0})\times\mathbf{r}_{y_{0}}\]

Note that \(\mathbf{e}_{x_{0},y_{0},u}^{0}\neq 0\) if and only if \((x_{0},y_{0},u)=(u,r_{t},v)\). However, this is impossible as \(u\neq v\). Thus \(\mathbf{e}_{u,r_{t},v}^{1}\) satisfies the definition of rule-induced subgraph representation.

\(k=2\). we have:

\[\begin{split}\mathbf{e}_{u,r_{t},v}^{2}&=\mathbf{h }_{u}^{2}+\mathbf{e}_{u,r_{t},v}^{1}=\sum_{(x_{0},y_{0},u)\in\mathcal{T}}( \mathbf{h}_{x_{0}}^{1}+\mathbf{e}_{x_{0},y_{0},u}^{1})\times\mathbf{r}_{y_{0} }+\mathbf{r}_{r_{t}}\\ &=\sum_{(x_{0},y_{0},u)\in\mathcal{T}}2\mathbf{h}_{x_{0}}^{1} \times\mathbf{r}_{y_{0}}+\mathbf{r}_{r_{t}}\\ &=\sum_{(x_{0},y_{0},u)}\sum_{(x_{1},y_{1},x_{0})}2(\mathbf{h}_{x _{1}}^{0}+\mathbf{e}_{x_{1},y_{1},x_{0}}^{0})\times\mathbf{r}_{y_{1}}\times \mathbf{r}_{y_{0}}+\mathbf{r}_{r_{t}}\\ &=\sum_{(x_{0},y_{0},u)}\sum_{(x_{1},y_{1},x_{0})}2\mathbf{e}_{x_{ 1},y_{1},x_{0}}^{0}\times\mathbf{r}_{y_{1}}\times\mathbf{r}_{y_{0}}+\mathbf{r} _{r_{t}}\end{split}\] (17)

We can find that \(e_{x_{1},y_{1},x_{0}}^{0}\neq 0\) if and only if \((x_{1},y_{1},x_{0})=(u,r_{t},v)\), i.e. there exists both \((u,r_{t},v)\) and \((v,r_{t},u)\). Obviously, \(\mathbf{e}_{u,r_{t},v}^{2}\) satisfies the definition of rule-induced subgraph representation.

Assume that this conclusion exists for \(n\leq k-1\). Now we check the \(k\)-th term.

\[\mathbf{e}_{u,r_{t},v}^{k}=\mathbf{h}_{u}^{k}+\sum_{i=1}^{k-1}\underbrace{ \sum_{(x_{0},y_{0},u)}\sum_{(x_{1},y_{1},x_{0})}...\sum_{(v,y_{i-2},x_{0})}}_{ i-1}\alpha_{i1}\mathbf{r}_{r_{t}}\otimes\alpha_{i2}\mathbf{r}_{y_{i -2}}\otimes...\otimes\alpha_{ii}\mathbf{r}_{y_{0}}\] (18)First, we consider \(\mathbf{h}_{u}^{k}\).

\[\begin{split}\mathbf{h}_{u}^{k}&=\sum_{(x_{0},y_{0},u) }(\mathbf{h}_{x_{0}}^{k-1}+\mathbf{e}_{x_{0},y_{0},u}^{k-1})\times\mathbf{r}_{y _{0}}\\ &=\sum_{(x_{0},y_{0},u)}\sum_{(x_{1},y_{1},x_{0})}(\mathbf{h}_{x_ {1}}^{k-2}+\mathbf{e}_{x_{1},y_{1},x_{0}}^{k-2})\times\mathbf{r}_{y_{1}}\times \mathbf{r}_{y_{0}}+\sum_{(x_{0},y_{0},u)}\mathbf{e}_{x_{0},y_{0},u}^{k-1} \times\mathbf{r}_{y_{0}}\\ &=\underbrace{\sum_{(x_{0},y_{0},u)}\sum_{(x_{1},y_{1},x_{0})}...\sum_{(x_{k-1},y_{k-1},x_{k-2})}}_{k}\mathbf{e}_{x_{k-1},y_{k-1},x_{k-2}}^ {0}\times\mathbf{r}_{y_{k-1}}\times\mathbf{r}_{y_{k-2}}\times...\times\mathbf{ r}_{y_{0}}\\ &+\underbrace{\sum_{(x_{0},y_{0},u)}\sum_{(x_{1},y_{1},x_{0})}...\sum_{(x_{k-2},y_{k-2},x_{k-3})}}_{k-1}\mathbf{e}_{x_{k-2},y_{k-2},x_{k-3}} ^{1}\times\mathbf{r}_{y_{k-2}}\times...\times\mathbf{r}_{y_{0}}\\ &+...\\ &+\sum_{(x_{0},y_{0},u)}\mathbf{e}_{x_{0},y_{0},u}^{k-1}\times \mathbf{r}_{y_{0}}\end{split}\] (19)

Notice that \(\underbrace{\sum_{(x_{0},y_{0},u)}\sum_{(x_{1},y_{1},x_{0})}...\sum_{(x_{k-1 },y_{k-1},x_{k-2})}}_{k}\mathbf{e}_{x_{k-1},y_{k-1},x_{k-2}}^{0}\times\mathbf{ r}_{y_{k-1}}\times\mathbf{r}_{y_{k-2}}\times...\times\mathbf{r}_{y_{0}}\neq 0\) if and only if \((x_{k-1},y_{k-1},x_{k-2})=(u,r_{t},v)\). In this situation, this term is exactly the \(k\)-th term in the expression of \(\mathbf{e}_{u,r_{t},v}^{k}\). Now we want to prove that:

\[\begin{split}&\underbrace{\sum_{(x_{0},y_{0},u)}\sum_{(x_{1},y_{1},x_ {0})}...\sum_{(x_{k-2},y_{k-2},x_{k-3})}}_{k-1}\mathbf{e}_{x_{k-2},y_{k-2},x_{ k-3}}^{1}\times\mathbf{r}_{y_{k-2}}\times...\times\mathbf{r}_{y_{0}}\\ &+...\\ &+\sum_{(x_{0},y_{0},u)}\mathbf{e}_{x_{0},y_{0},u}^{k-1}\times \mathbf{r}_{y_{0}}\end{split}\] (20)

can be fused in top \(k-1\) term of Equation. 16. Let's check the \(j\)-th term of Equation. 20.

\[\begin{split}&\underbrace{(x_{0},y_{0},u)}_{(x_{1},y_{1},x_{0})}... \sum_{(x_{j-1},y_{j-1},x_{j-2})}\mathbf{e}_{x_{j-1},y_{j-1},x_{j-2}}^{k-j} \times\mathbf{r}_{y_{j-1}}\times...\times\mathbf{r}_{y_{0}}\\ &=\underbrace{(x_{0},y_{0},u)}_{(x_{1},y_{1},x_{0})}...\sum_{(x_{ j-1},y_{j-1},x_{j-2})}(\mathbf{e}_{x_{j-1},y_{j-1},x_{j-2}}^{k-j-1}+\mathbf{h}_{x_{j-1} }^{k-j})\times\mathbf{r}_{y_{j-1}}\times...\times\mathbf{r}_{y_{0}}\\ &=\underbrace{(x_{0},y_{0},u)}_{(x_{1},y_{1},x_{0})}...\sum_{(x_{ j-1},y_{j-1},x_{j-2})}(\mathbf{e}_{x_{j-1},y_{j-1},x_{j-2}}^{0}+\mathbf{h}_{x_{j-1} }^{k-j}+...++\mathbf{h}_{x_{j-1}}^{1})\times\mathbf{r}_{y_{j-1}}\times... \times\mathbf{r}_{y_{0}}\\ &=\underbrace{(x_{0},y_{0},u)}_{(x_{1},y_{1},x_{0})}...\sum_{(x_{ j-1},y_{j-1},x_{j-2})}\mathbf{e}_{x_{j-1},y_{j-1},x_{j-2}}^{0}\times\mathbf{r}_{y_{j-1} }\times...\times\mathbf{r}_{y_{0}}\\ &+\underbrace{\sum_{(x_{0},y_{0},u)}\sum_{(x_{1},y_{1},x_{0})}... \sum_{(x_{j-1},y_{j-1},x_{j-2})}\sum_{(x_{j},y_{j},x_{j-1})}}_{j+1}\\ &(\mathbf{h}_{x_{j}}^{k-j-1}+...+\mathbf{h}_{x_{j}}^{0}+\mathbf{ e}_{x_{j},y_{j},x_{j-1}}^{k-j-1}+...+\mathbf{e}_{x_{j},y_{j},x_{j-1}}^{0})\times \mathbf{r}_{y_{j-1}}\times...\times\mathbf{r}_{y_{0}}\end{split}\] (21)

Note that \(\mathbf{e}_{x_{j-1},y_{j-1},x_{j-2}}^{0}\neq 0\) if and only if \((x_{j-1},y_{j-1},x_{j-2})=(u,r_{t},v)\), thus the term can be fused into the \(j\)-th term of Equation. 16. \(\mathbf{e}_{x_{j},y_{j},x_{j-1}}^{0}\) can be fused into the \((j+1)\)-th term and so on. Therefore, we have:

\[\begin{split}&\underbrace{\sum_{(x_{0},y_{0},u)}\sum_{(x_{1},y_{1},x_{ 0})}...\sum_{(x_{j-1},y_{j-1},x_{j-2})}\mathbf{e}_{x_{j-1},y_{j-1},x_{j-2}}^{k-j }\times\mathbf{r}_{y_{j-1}}\times...\times\mathbf{r}_{y_{0}}}\\ &=\sum_{i=1}^{k}\underbrace{\sum_{(u,r_{t},v)}\sum_{(v,y_{0},x_{ 0})}...\sum_{(x_{i-3},y_{i-2},u)}}_{i}\alpha_{i1}\mathbf{r}_{r_{t}}\times \alpha_{i2}\mathbf{r}_{y_{0}}\times...\times\alpha_{ii}\mathbf{r}_{y_{i-2}} \end{split}\] (22)

There, we prove that single-source edge-wise GNN can learn rule-induced subgraph representation in this case. 

**Theorem 2**_Single-source edge-wise GNN can learn rule-induced subgraph representation if \(\psi=\oplus,\oplus=\oplus,\diamond=\oplus\), \(\diamond=\oplus\), \(\otimes^{1}=\otimes,\otimes^{2}=\otimes\), where \(\oplus\) and \(\otimes\) are binary operators that satisfy \(0\oplus a=a,0\otimes a=0\). i.e., there exists nonzero \(\alpha_{i,j}\) such that_

\[\mathbf{e}_{u,r_{t},v}^{k}=\bigoplus_{i=1}^{k}\underbrace{\bigoplus_{(u,r_{t}, v)}\bigoplus_{(v,y_{0},x_{0})}...\bigoplus_{(x_{i-3},y_{i-2},u)}}_{i}\alpha_{i1} \mathbf{r}_{r_{t}}\otimes\alpha_{i2}\mathbf{r}_{y_{0}}\otimes...\otimes\alpha _{ii}\mathbf{r}_{y_{i-2}}\] (23)

_Proof_ Without loss of generality, we can replace \(+\) with \(\oplus\) and \(\times\) with \(\otimes\) to represent a binary operator, then we directly get this theorem. Note that we should ensure that \(\oplus\) and \(\otimes\) satisfy \(0\oplus a=a,0\otimes a=0\), which we use in the process of proof. \(\Box\)

## Appendix B Details of Datasets

We summarize the details of inductive relation prediction benchmark datasets in Table 5.

## Appendix C Implementation Details

In general, our proposed method is implemented in DGL[36] and PyTorch[35] and trained on single GPU of NVIDIA GeForce RTX 3090. We apply Adam optimizer[39] with an initial learning rate of \(0.0005\). Observing that batch size has little effect on the performance of the model, We adjust batch size as large as possible for different datasets to accelerate training. We use the binary cross entropy loss.The maximum number of training epochs is set to 10. During training, we add reversed edges to fully capture relevant rules. The number of hop \(h\) is set to 3 which is consistent with existing subgraph-based methods. We conduct grid search to obtain optimal hyperparameters, where we search subgraph types in {enclosing, unclosing}, embedding dimensions in {16, 32}, number of GNN layers in {3, 4, 5, 6} and dropout in {0, 0.1, 0.2}. Configuration for the best performance of each dataset is given within the code.

## Appendix D Transductive Results

The transductive results, as discussed in Section 6.2, were obtained using the same methodology as the aforementioned evaluations. Specifically, REST was trained on the training graph and tested in a similar manner. We randomly selected 10% of the links from the training graph as test links. As we can sen in Table 6, REST outperforms GraIL and RuleN significantly across all benchmarks.

\begin{table}
\begin{tabular}{l c c c c c c c c c c c c} \hline Model & wn\_v1 & wn\_v2 & wn\_v3 & wn\_v4 & fb\_v1 & fb\_v2 & fb\_v3 & fb\_v4 & nell\_v1 & nell\_v2 & nell\_v3 & nell\_v4 \\ \hline GraIL & 65.59 & 69.36 & 64.63 & 67.28 & 71.93 & 86.30 & 88.95 & 91.55 & 64.08 & 86.88 & 84.19 & 82.33 \\ RuleN & 63.42 & 68.09 & 63.05 & 65.55 & 67.53 & 88.00 & 91.47 & 92.35 & 62.82 & 82.82 & 80.72 & 58.84 \\ REST & 92.02 & 90.90 & 91.59 & 91.63 & 88.01 & 93.98 & 96.43 & 97.71 & 93.62 & 95.76 & 92.79 & 92.47 \\ \hline \end{tabular}
\end{table}
Table 6: Experiments of the transductive versions of the current benchmarks.

\begin{table}
\begin{tabular}{l l c c c c c c c c c} \hline \hline  & & \multicolumn{3}{c}{**WN18RR**} & \multicolumn{3}{c}{**FB15k-237**} & \multicolumn{3}{c}{**NELL-995**} \\ \cline{3-11}  & & \#R & \#E & \#TR & \#R & \#E & \#TR & \#R & \#E & \#TR \\ \hline \multirow{2}{*}{v1} & train & 9 & 2746 & 6678 & 183 & 2000 & 5226 & 14 & 10915 & 5540 \\  & test & 9 & 922 & 1991 & 146 & 1500 & 2404 & 14 & 225 & 1034 \\ \hline \multirow{2}{*}{v2} & train & 10 & 6954 & 18968 & 203 & 3000 & 12085 & 88 & 2564 & 10109 \\  & test & 10 & 2923 & 4863 & 176 & 2000 & 5092 & 79 & 4937 & 5521 \\ \hline \multirow{2}{*}{v3} & train & 11 & 12078 & 32150 & 218 & 4000 & 22394 & 142 & 4647 & 20117 \\  & test & 11 & 5084 & 7470 & 187 & 3000 & 9137 & 122 & 4921 & 9668 \\ \hline \multirow{2}{*}{v4} & train & 9 & 3861 & 9842 & 222 & 5000 & 33916 & 77 & 2092 & 9289 \\  & test & 9 & 7208 & 15157 & 204 & 3500 & 14554 & 61 & 3294 & 8520 \\ \hline \hline \end{tabular}
\end{table}
Table 5: Statistics of three inductive datasets, which contain four different versions individually. We use #E and #R and #TR to denote the number of entities, relations, and triples.

[MISSING_PAGE_FAIL:17]

[MISSING_PAGE_EMPTY:18]

[MISSING_PAGE_EMPTY:19]