# Should Under-parameterized Student Networks

Copy or Average Teacher Weights?

Berfin Simsek

NYU\({}^{*}\)

bs3736@nyu.edu

&Amire Bendjeddou

EPFL

amire.bendjeddou@epfl.ch

&Wulfram Gerstner

EPFL

wulfram.gerstner@epfl.ch

&Johanni Brea

EPFL

johanni.brea@epfl.ch

Previous address: EPFL.

###### Abstract

Any continuous function \(f^{*}\) can be approximated arbitrarily well by a neural network with sufficiently many neurons \(k\). We consider the case when \(f^{*}\) itself is a neural network with one hidden layer and \(k\) neurons. Approximating \(f^{*}\) with a neural network with \(n<k\) neurons can thus be seen as fitting an under-parameterized "student" network with \(n\) neurons to a "teacher" network with \(k\) neurons. As the student has fewer neurons than the teacher, it is unclear, whether each of the \(n\) student neurons should copy one of the teacher neurons or rather average a group of teacher neurons. For shallow neural networks with erf activation function and for the standard Gaussian input distribution, we prove that "copy-average" configurations are critical points if the teacher's incoming vectors are orthonormal and its outgoing weights are unitary. Moreover, the optimum among such configurations is reached when \(n-1\) student neurons each copy one teacher neuron and the \(n\)-th student neuron averages the remaining \(k-n+1\) teacher neurons. For the student network with \(n=1\) neuron, we provide additionally a closed-form solution of the non-trivial critical point(s) for commonly used activation functions through solving an equivalent constrained optimization problem. Empirically, we find for the erf activation function that gradient flow converges either to the optimal copy-average critical point or to another point where each student neuron approximately copies a different teacher neuron. Finally, we find similar results for the ReLU activation function, suggesting that the optimal solution of underparameterized networks has a universal structure.

## 1 Introduction

A shallow neural network with a single hidden layer of a large number \(k\) of neurons can approximate any continuous function \(f^{*}\) arbitrarily well on a compact subset of the input space [1]. We consider a related problem, where the function \(f^{*}\) itself is a neural network with a large number \(k\) of neurons, and its approximation is a smaller network with \(n<k\) neurons. In other words, we fit an under-parameterized "student" network with \(n\) neurons to a "teacher" network with \(k\) neurons. As the student has fewer neurons than the teacher, it cannot perfectly match the teacher. In the configuration with the lowest loss, where the approximation error is smallest, one may expect that the incoming and outgoing weights of a student neuron are either identical to those of a teacher neuron or that they are aligned with the weights of a group of teacher neurons, but it is unclear what the optimal configuration is.

To answer the question of whether student neurons should "copy" or "average" teacher neurons, and more generally to shed light on the loss landscape of under-parameterized neural networks, we study the theoretically tractable setup with standard Gaussian input data and teacher networks with orthogonal incoming vectors. First, we re-parameterize the loss in terms of interactions between pairs of neurons, similar to [2, 3], and we re-formulate the original optimization problem as a constrained optimization problem. The interactions between neurons can be written as a function expressed in terms of the standard deviation and correlation of two Gaussian random variables, with explicit formulas for the erf and ReLU activation functions [2, 3, 4]. Next, we prove several properties of the most extremely under-parameterized student network with a single neuron \(n=1\), extending thus the important work of [5, 6, 7]. For many commonly used activation functions, we prove for the network with a single hidden neuron that the optimal solution is the only non-trivial critical point of the loss function up to symmetries and is achieved when the incoming vector of the one-neuron student reaches a configuration that can be interpreted as a damped average of all incoming teacher weights.

The proof relies on identifying the critical points of the constrained optimization problem and showing that the common activation functions satisfy the assumptions. We rely in particular on the derivative rule of the interaction function which comes as a pleasant consequence of Stein's Lemma [8] instead of the Hermite basis expansion which is a commonly used technique [9, 10, 11, 12, 13, 14]. For the erf and ReLU activation functions we derive additionally a closed-form solution of the optimization problem for \(n=1\). Next, we investigate "copy-average" configurations of students with \(n>1\) neurons, where some student neurons copy teacher neurons and other student neurons average sub-groups of teacher neurons, in the sense that they are at the optimal one-neuron solution for the given sub-group of teacher neurons. Our particular contributions are:

* We propose a constrained optimization formulation of the standard minimization problem in the weight-space in terms of the _interaction function_ (Section 3). The interaction function is a natural generalization of the dual activation [15].
* Applying the constrained optimization formulation for \(n=1\), we prove that the incoming vector of the student lies in the span of the incoming vectors of an orthogonal teacher network (Proposition 4.1). For a broad class of activation functions, we prove that the incoming vector aligns with the average of the teacher's incoming vectors for the "unit-orthonormal" teacher network (Theorem 5.1). Using the derivative rule of the interaction function (Lemma F.1), we show that common activation functions such as erf, softplus, tanh, and ReLU satisfy this property (Lemma F.2 and Corollary G.5).
* Assuming a unit-orthonormal teacher network and erf activation function, we prove that the concatenation of critical points of single neurons (of the student network) each approximating a teacher subnetwork is a _copy-average_ critical point (Theorem 4.2).
* Assuming a unit-orthonormal teacher network and erf activation function, we prove that the optimal copy-average (CA) configuration is such that \(n-1\) student neurons each copy a teacher neuron and the \(n\)-th student neuron approximates optimally the sum of the remaining teacher neurons (Theorem 5.5; see also Fig. 1, top row). Empirically, we find that the gradient flow converges to an optimal-CA point for all seeds when \(n<\gamma_{1}k\) with a fixed \(\gamma_{1}\) near \(0.46\) (Figure 4).
* Surprisingly, we find empirically three regimes of training via gradient flow (GF)2for under-parameterized networks (Figure 4): (i) for \(n<\gamma_{1}k\), GF converges to an optimal-CA point for all seeds, (ii) for \(n>\gamma_{2}k\) with a fixed \(\gamma_{2}\) near \(0.6\), GF converges to a point that we call _perturbed-\(n\)-copy_ for all seeds, (iii) for \(\gamma_{1}k<n<\gamma_{2}k\), GF converges to either an optimal-CA point or a perturbed-\(n\)-copy point. Therefore, as the under-parameterized network grows larger, the solution found with gradient flow where the weights are initialized randomly with a fixed standard deviation changes qualitatively. The code to reproduce these findings is available on GitHub, and we refer to Appendix C for details. Footnote 2: We use a numerical ODE solver for multi-layer networks [16] to simulate the gradient flow in this paper. All “solutions”, which are the points at which gradient flow converges, have a gradient norm of at most \(5\cdot 10^{-8}\).

### Related Work

The teacher-student setup has been extensively used in the literature to study the evolution of gradient flow trajectories and of the generalization error [2, 3, 17, 18, 19, 20]. This series of work gives insight into the solution found at convergence, however, they rely on numerically integrating the equations of dynamics. Tian [5] gives convergence guarantees for ReLU activation function, however, their method only works for _one_ student and _one_ teacher neuron. Xu and Du [21] recently gave the convergence rates for _multiple_ student neurons for the case of _one_ teacher neuron as a prototypical setup for overparameterization. These convergence guarantees were extended to broad input distributions [7, 22] and finite training data [6, 23]. We give the analytical formula of the optimal solution and its generalization error for one student neuron and unit-orthonormal teacher network with _multiple_ neurons for erf and ReLU activation functions and a partial characterization for a broader class of activation functions without relying on the analytic formula of the loss.

The studies cited above showed positive results for a single-neuron teacher or a unit-orthonormal teacher. However, even for settings where the teacher has only a few neurons, hard teachers can be constructed in the sense that the student fails to find a zero-loss solution for a certain fraction of random initializations [24, 25, 26]. Moreover, for medium-scale problems, gradient flow often converges to 'non-zero loss' solutions [27, 28, 29]. Arjevani and Field [30] characterized some families of local minima using symmetries, for the ReLU activation function and unit-orthonormal teacher network. In this paper, we similarly characterize, for the case that the student has a smaller size than the teacher, a large family of 'copy-average' critical points, but for the erf activation function. Our approach focuses on the important regime of under-parameterized networks which is relevant for the superposition of features [31] and for the distillation of large networks into smaller ones [32, 33].

There is a large history of approximation theory of neural networks that give universal guarantees on the approximation error, e.g. [34, 35, 1, 36]. However, these works focus on rates of convergence and provide neither a formula nor an approximation for the error. In this paper, we make a conjecture for the _exact formula_ of the approximation error of under-parameterized student networks which we support both theoretically and numerically.

Figure 1: _The gradient flow converges to the copy-average optimum point for erf activation (top), or nearby for ReLU activation (bottom): the first \(n-1\) neurons copy one teacher neuron each; the \(n\)-th neuron takes an average of the remaining teacher neurons._ The teacher network is unit-orthonormal, i.e. \(f^{*}(x)\!=\!\sum_{j=1}^{k}\!\sigma(v_{j}\!\cdot\!x)\) where \(v_{j}\!\in\!\mathbb{R}^{d}\)’s are orthonormal, and \(d\!=\!k\!+\!1\). **A1** The gradient flow trajectory is shown in the weight space for \(n\!=\!2,k\!=\!3\): the positions of the circles (red and green) represent incoming vector \(w_{i}\) projected down to the span of \(v_{1},v_{2},v_{3}\) and the sizes of the circles represent outgoing weights \(a_{i}\). The blue circle represents the one-neuron solution (the position shows \(w^{*}\), the size shows \(a^{*}\)). **A2** Same setting, the weight-space parameters at convergence are mapped to the order-parameter space; \(u_{i}=(u_{i1},...,u_{ik})\) where \(u_{ij}\) represents the normalized dot product between \(w_{i}\) and \(v_{j}\) and \(r_{i}=\|w_{i}\|\). **B** Order parameters shown at convergence for \(n\!=\!4,k\!=\!8\). For erf (top) the point at convergence is exactly an \((n-1)\)-copy-\(1\)-average point, whereas for ReLU, it is perturbed away from this configuration. Neurons are reordered for clarity.

## 2 Setup

**Neural network:** Consider a two-layer (student) network function \(f:\mathbb{R}^{d}\rightarrow\mathbb{R}\) with \(n\) neurons

\[f(x)=\sum_{i=1}^{n}a_{i}\sigma\left(w_{i}\cdot x\right)\] (1)

where \(w_{i}\in\mathbb{R}^{d}\) is the incoming vector, \(a_{i}\in\mathbb{R}\) is the outgoing weight of neuron \(i\), and the activation function \(\sigma\) is twice differentiable unless it is specified to be ReLU, i.e. \(\sigma_{\text{relu}}(x)=\max(0,x)\), and the dot marks the scalar product. \(P\!=\!n(d+1)\) is the number of parameters.

**Parameter vector:** The parameter vector is represented as

\[\theta=(w_{1},a_{1})\oplus...\oplus(w_{n},a_{n})\in\mathbb{R}^{P}\] (2)

where \(\oplus\) denotes the concatenation of two vectors into one vector. We use the notation \(\oplus\), since the network function can be seen as a sum of its hidden neurons. Sometimes \(\theta\) is written explicitly in the network function \(f(x|\theta)\!=\!f(x)\).

**Loss function:** We assume that the input distribution is a standard \(d\)-dimensional Gaussian \(\mathcal{D}=\mathcal{N}(0,I_{d})\). The target function is denoted by \(f^{*}:\mathbb{R}^{d}\rightarrow\mathbb{R}\). Using the square cost, the loss function \(L:\mathbb{R}^{P}\rightarrow\mathbb{R}\) (also known as the risk or the generalization error) is defined as

\[L(\theta)=\mathbb{E}_{x\sim\mathcal{D}}\left[(f(x|\theta)-f^{*}(x))^{2}\right].\] (3)

**Orthogonal teacher network:** We assume that the target function is a neural network (also known as the teacher network or a multi-index model)

\[f^{*}(x)=\sum_{j=1}^{k}b_{j}\sigma(v_{j}\cdot x)\] (4)

where its outgoing weights are non-zero and its incoming vectors \(v_{1},\ldots,v_{k}\in\mathbb{R}^{d}\) are orthogonal to each other, that is, \(v_{i}\cdot v_{j}=0\) for \(i\neq j\). This implies that the input dimension satisfies \(d\!\geq\!k\). Following [27; 30; 37], we particularly focus on the _unit-orthonormal_ teacher network where the outgoing weights are all one, that is \(b_{j}=1\), and the incoming vectors have unit norm, i.e. \(v_{i}\cdot v_{j}=\delta_{ij}\).

**Optimal loss:** We study the optimal solution(s) of the following non-convex optimization problem

\[L^{n,k}(\oplus_{i=1}^{n}(a_{i},w_{i}))=\mathbb{E}_{x\sim\mathcal{D}}\left[ \left(\sum_{i=1}^{n}a_{i}\sigma(w_{i}\cdot x)-\sum_{j=1}^{k}b_{j}\sigma(v_{j} \cdot x)\right)^{2}\right].\] (5)

for under-parameterized (student) networks, i.e. \(n<k\), and orthogonal teachers. For \(n\geq k\) neurons, the network can copy all teacher neurons and set the outgoing weights of the remaining neurons to zero, therefore the optimal loss is trivially zero. If the teacher is unit-orthonormal, then all of its neurons contribute equally; hence the optimal loss is determined by \(n\) and \(k\) only and denoted by \(L^{*}(n,k)\). If the student neural network has one neuron we use the notation \(L^{*}(k):=L^{*}(1,k)\).

## 3 Foundations & Constrained Optimization Formulation

In this section, we introduce a constrained optimization problem that is a reformulation of the minimization problem in Eq. 5. This formulation allows us to show that the incoming vector of any non-trivial critical point of the one-neuron network is in the span of the teacher's \(k\) orthogonal (or potentially even non-orthogonal, see Appendix Remark D.1) incoming vectors (see Proposition 4.1). We give the exact solution in the case of a unit-orthonormal teacher (see Corollary 5.2 and Corollary G.5).

Using the linearity of expectation, the loss function in Eq. 5 can be expanded as a weighted sum of the following Gaussian integral terms

\[\mathbb{E}_{x\sim\mathcal{D}}[\sigma(V_{1}\cdot x)\sigma(V_{2}\cdot x)]\]where \(V_{1}\) and \(V_{2}\) represent two arbitrary vectors of student and teacher networks such as \((w_{i},w_{j})\) or \((w_{i},v_{j})\). As both \(V_{1}\cdot x\) and \(V_{2}\cdot x\) are centered Gaussian random variables, the above expectation can be expressed in terms of the covariance of the two-dimensional Gaussian

\[\mathbb{E}_{x\sim\mathcal{D}}\begin{bmatrix}(V_{1}\cdot x)^{2}&(V_{1}\cdot x)( V_{2}\cdot x)\\ (V_{1}\cdot x)(V_{2}\cdot x)&(V_{2}\cdot x)^{2}\end{bmatrix}=\begin{bmatrix}r_{ 1}^{2}&r_{1}r_{2}u\\ r_{1}r_{2}u&r_{2}^{2}\end{bmatrix}\]

where \(r_{i}:=\|V_{i}\|\) for \(i=1,2\) is the \(\ell_{2}\)-norm and, assuming \(r_{i}>0\), \(u:=V_{1}\cdot V_{2}/(r_{1}r_{2})\) is the correlation. The covariance entries \(Q_{ii}=r_{i}^{2},Q_{12}=r_{1}r_{2}u\) have been used to study the gradient flow trajectories [2, 3, 24]. We prefer the parametrization with \(r_{i}\) and \(u\) as it enables us to make the positive definiteness constraint explicit, i.e.

\[|u|=\frac{|V_{1}\cdot V_{2}|}{r_{1}r_{2}}\leq 1,\] (6)

due to the Cauchy-Schwarz inequality. We introduce the _interaction function_\(g_{\sigma}:\mathbb{R}^{2}_{\geq 0}\times[-1,1]\to\mathbb{R}\)

\[g_{\sigma}(r_{1},r_{2},u)=\mathbb{E}_{(x_{1},x_{2})\sim\mathcal{N}(0,\Sigma)}[ \sigma(r_{1}x_{1})\sigma(r_{2}x_{2})]\quad\text{with}\;\;\Sigma=\begin{bmatrix} 1&u\\ u&1\end{bmatrix},\;\;r_{1},r_{2}>0,\] (7)

to express the Gaussian integral terms. Note that \(u\) is not well-defined if one of the norms is zero. Extending the formula above, for the case w.l.o.g. \(r_{2}=0\), we define

\[g_{\sigma}(r_{1},0,u):=\mathbb{E}_{x\sim\mathcal{N}(0,1)}[\sigma(r_{1}x)] \sigma(0)\]

for all \(u\in[-1,1]\). In this paper, we consider the activation functions satisfying the following.

**Assumption 3.1**.: _For all \(r_{1},r_{2}>0\) and \(u\in(-1,1)\), we assume that the interaction function \(g_{\sigma}\) satisfies either the first or both of the following properties_

\[\text{(i)}\;\frac{d}{du}g_{\sigma}(r_{1},r_{2},u)>0,\qquad\text{(ii)}\;\frac{ d^{2}}{du^{2}}g_{\sigma}(r_{1},1,u)u<\frac{d}{du}g_{\sigma}(r_{1},1,u).\] (8)

To check whether a specific activation function satisfies the above properties, we mainly rely on Lemma F.1 which gives us the rule for the partial derivative of \(g_{\sigma}\) with respect to the correlation

\[\frac{d}{du}g_{\sigma}(r_{1},r_{2},u)=r_{1}r_{2}\mathbb{E}[\sigma^{\prime}(r_ {1}x)\sigma^{\prime}(r_{2}y)].\] (9)

Hence, if \(\sigma\) is monotonic (increasing or decreasing)3, the integrand on the right-hand side is positive; satisfying Assumption 3.1 (i). The ReLU activation function \(\sigma_{\text{relu}}(x)=\max(0,x)\) also satisfies it because of the known analytical expression of the interaction [4, 27]

Footnote 3: Increasing (or decreasing) mean strictly increasing (or decreasing) everywhere in this paper.

\[g_{\text{relu}}(r_{1},r_{2},u)=r_{1}r_{2}h(u)\quad\text{where}\;\;h(u)=\frac {1}{2\pi}\left(\sqrt{1-u^{2}}+(\pi-\arccos(u))u\right).\]

Checking Assumption 3.1 (ii) for a given activation function is delicate. We rely on it in Section 5.

Using the interaction function, the loss function can be expressed in terms of the _order parameters:_

* norms of the incoming vectors of the student \(r_{i}=\|w_{i}\|\),
* correlations between the incoming vectors of the student and teacher \(u_{ij}=w_{i}\cdot v_{j}/(r_{i}\|v_{j}\|)\),
* correlations between the incoming vectors of the student \(\rho_{ii^{\prime}}=w_{i}\cdot w_{i^{\prime}}/(r_{i}r_{i^{\prime}})\);

where we assumed \(r_{i}>0\) for all \(i\in[n]\). The constrained optimization formulation is possible for general non-orthogonal teacher networks (see Remark D.1 in the Appendix). For the sake of simplicity, we formulate here the constrained optimization problem for the case of orthogonal teachers and reformulate the objective in Eq. 5 as

minimize \[\sum_{i=1}^{n}a_{i}^{2}g_{\sigma}(r_{i},r_{i},1)+2\sum_{i\neq i^ {\prime}}a_{i}a_{i^{\prime}}g_{\sigma}(r_{i},r_{i^{\prime}},\rho_{ii^{\prime}} )-2\sum_{i=1}^{n}\sum_{j=1}^{k}a_{i}b_{j}g_{\sigma}(r_{i},\|v_{j}\|,u_{ij})+C\] subject to \[\|u_{i}\|\leq 1,\;\;r_{i}\geq 0,\quad\text{for all}\;\;i\in[n],\] \[\left|\rho_{ii^{\prime}}-u_{i}\cdot u_{i^{\prime}}\right|\leq \sqrt{1-\|u_{i}\|^{2}}\sqrt{1-\|u_{i^{\prime}}\|^{2}},\quad\text{for all}\;\;i\neq i^{\prime}\in[n];\] (10)where \(u_{i}=(u_{i1},...,u_{ik})\) and \(C=\mathbb{E}_{x\sim\mathcal{D}}[f^{*}(x)^{2}]\). The constraints in Eq. 10 give tighter bounds than simply bounding correlations with the help of Eq. 6. See Appendix D for the derivation of the constraints and Fig. 2 for a schematic.

The objective above is _exact_ for \(n=1,2\), in the sense that its optimal solution is equivalent to the optimal solution in the weight space, since the mapping from the weight-space to the order space is invertible. However, it is a _relaxation_ for \(n\geq 3\), since there are order-parameter configurations in the domain (see Figure 2) that do not correspond to any weight-space configuration (see Appendix D.3 for a construction). It seems possible to overcome this gap by considering the geometry of the angles between \(n\geq 3\) incoming vectors to tighten the constraints between student-student correlations.

## 4 Copy-Average Critical Points

In this section, we identify a new family of critical points by 'combining' critical points of one-neuron networks for the unit-orthonormal teacher and the erf activation function. We first show that in a network with \(n=1\) student neuron, for any "non-trivial" critical point, that is \(w^{*}\neq 0\) and \(a^{*}\neq 0\), the incoming vector \(w^{*}\) is in the span of the teacher's incoming vectors (Proposition 4.1). Applying this proposition to the special case of the erf activation function, we show that the concatenation of such critical points is also a critical point for multi-neuron networks (Theorem 4.2).

**Proposition 4.1**.: _Assume that \(f^{*}\) is an orthogonal teacher network (Eq. 4) of width \(k\). If the activation function satisfies Assumption 3.1 (i), any non-trivial critical point \(\theta^{*}=(w^{*},a^{*})\), i.e. \(\nabla L^{1,k}(\theta^{*})=0\), \(\|w^{*}\|\neq 0\), \(a^{*}\neq 0\), satisfies that \(w^{*}\) is in the span of the teacher's incoming vectors._

The proof uses the constrained optimization formulation for \(n=1\) (see Appendix G.1). In short, a critical point mapped to the order parameter space satisfies either \(\|u_{1}\|=1\) or \(\partial_{u}g_{\sigma}(r,\|v_{j}\|,u_{1j})=0\) for all \(j\in[k]\). Under the Assumption 3.1 (i) these partial derivatives are non-zero, hence \(\|u_{1}\|=1\). In a recent work [38], the incoming vectors of the student network are also shown to converge to the span of the vectors of the multi-index model using weight-decay.

Finding the optimal solution for the multi-neuron network is challenging. Natural candidates are concatenation of student neurons where each one of them is a critical point of the loss function \(L^{1,\ell_{i}}\) where \(\ell_{i}\) is the number of the subgroup of teacher neurons. More precisely, let us pick a partition \(\ell_{1}+...+\ell_{n}\leq k\) such that \(\ell_{i}\geq 1\), and define \(s_{m}=\sum_{i=1}^{m}\ell_{i}\) for \(m\leq n\) and \(s_{0}=0\). We denote a one-neuron critical point by \(\theta_{i}^{*}=(w_{i}^{*},a_{i}^{*})\), when learning from a part of the teacher network

\[f_{i}^{*}(x)=\sum_{j=s_{i-1}+1}^{s_{i}}\sigma(v_{j}\cdot x).\] (11)

Since \(f_{i}^{*}\) is a unit-orthonormal teacher, \(w_{i}^{*}\) is in the span of \(v_{s_{i-1}+1},...,v_{s_{i}}\) due to Proposition 4.1.

Figure 2: _Cartoon representation of the mapping of a student with three neurons from the weight space **A**\(\mathbb{R}^{nd}\) to order parameter space **B1-B2**. The mapping between the outgoing weights is an identity mapping hence not shown. **A** Each axis shows the direction of weights \(v_{i}\) of one teacher neuron (\(k\geq 3\)). **B1** Each incoming vector \(w_{i}\in\mathbb{R}^{d}\) is first transformed into \((r_{i},w_{i}/r_{i})\) and then \(w_{i}/r_{i}\) is projected onto the span of the teacher’s incoming vectors, yielding the student-teacher correlation vector \(u_{i}=(u_{i1},...,u_{ik})\). **B2** The student-student correlations \(\rho_{ii^{\prime}}\) are in general free parameters bounded in between \(u_{i}\cdot u_{j}\pm\sqrt{1-\|u_{i}\|^{2}}\sqrt{1-\|u_{i^{\prime}}\|^{2}}\) hence the box constraint. An activated constraint, w.l.o.g. \(u_{1}\in\mathbb{S}^{k-1}\), gives a vanishing \(\pm\) term for the interval of correlation \(\rho_{1i}\) for all \(i\neq 1\), hence they are no longer free (shown in red). In the case \(d=k\), all \(u_{i}\) are on the hypersphere due to the problem geometry, hence the correlations \(\rho_{ii^{\prime}}\) are fixed and not free (see Appendix D.1).

We use the term _copy-average_ (CA) point or configuration to refer to the concatenation of such one-neuron critical points in the student network with \(n\) neurons: if \(\ell_{i}=1\), the student neuron copies one of the teacher neurons \((v_{j},1)\); if \(\ell_{i}>1\), it averages a group of teacher neurons in the sense of approximating their sum with one neuron. For odd activation functions, the one-neuron network problems decouple from each other, as the cross-terms \(\mathbb{E}[\sigma(w_{1}\cdot x)\sigma(w_{2}\cdot x)]\) vanish for \(w_{1}\perp w_{2}\). For the specific case of erf, we prove that all the copy-average configurations are critical points.

**Theorem 4.2**.: _Assume that \(\sigma(x)=\sigma_{\text{e}\!\text{f}}(x)=\frac{2}{\sqrt{\pi}}\int_{0}^{\frac{x }{\sqrt{2}}}e^{-t^{2}}dt\). We pick a copy-average parameter_

\[\theta^{*}=(w_{1}^{*},a_{1}^{*})\oplus...\oplus(w_{n}^{*},a_{n}^{*})\] (12)

_where \((w_{i}^{*},a_{i}^{*})\) is a non-trivial critical point when learning from a unit-orthonormal teacher \(f_{i}^{*}\) with the incoming vectors \(v_{s_{i-1}+1},...,v_{s_{i}}\) shown in Eq. 11. Then \(\theta^{*}\) is a critical point of the loss function \(L^{n,k}\) where the target function is \(f^{*}(x)=\sum_{j=1}^{k}\sigma(v_{j}\cdot x)\)._

In particular, all neurons are equivalent to each other in a unit-orthonormal teacher network. Therefore, the copy-average configurations where \(n-1\) student neurons each copy a distinct teacher neuron and the \(n\)-th student neuron takes an average are also equivalent and called \((n-1)\)-copy-\(1\)-average, or \((n-1)\)-C-1-A in short. Another interesting configuration is where \(n\) student neurons each copy a distinct teacher neuron, which is called \(n\)-copy, or \(n\)-C in short.

For general activation functions the copy-average parameter vectors are not critical points (see Eq. 56). Nevertheless, we numerically find that the gradient flow converges to similar configurations for the ReLU activation function (see Figure 1, see Appendix C.3 for more experiments).

## 5 Approximation Error of Underparameterized Networks

The target function is assumed to be a unit-orthonormal teacher network in this section. In Subsection 5.1, we show for the one-neuron network that there is a unique non-trivial critical point up to symmetries, which is necessarily the global minimum (Theorem 5.1). Furthermore, we give the analytic expression of the optimal solution and its loss for erf (Corollary 5.2) and ReLU (Corollary G.5) activation functions. In Subsection 5.2, we provide for the under-parameterized student with \(n>1\) neurons the exact loss of copy-average critical points for the erf activation function and show that the \((n-1)\)-copy-\(1\)-average configurations reach the lowest loss among CA-critical points (see also Appendix E.1 for the combinatorial number of the equivalent copy-average configurations related to the landscape complexity calculations [39]).

### One-Neuron Network

Using the constrained optimization formulation in 10, we first prove that at any non-trivial critical point of the one-neuron network, the incoming vector aligns equally with all teacher's incoming vectors for unit-orthonormal teachers for activation functions satisfying Assumption 3.1 (see Theorem 5.1). This is related to the symmetric solution visited during the learning plateaus studied in Saad and Solla [2] for erf activation and in Tian [5] for ReLU activation (see Appendix B for a detailed comparison). Our proof works for a broad class of activation functions and does not use the analytic expression of the interaction function.

**Theorem 5.1**.: _Assume that the activation function satisfies Assumptions 3.1 (i) and (ii). At any non-trivial critical point \((w^{*},a^{*})\) of the loss \(L^{1,k}\) for the unit-orthonormal teacher network, the incoming vector satisfies_

\[\frac{w^{*}}{\|w^{*}\|}=u^{*}\sum_{j=1}^{k}v_{j}\] (13)

_where \(u^{*}\) is either \(\frac{1}{\sqrt{k}}\) or \(-\frac{1}{\sqrt{k}}\)._

Proof Sketch.: There is no student-student interaction term since we have a single neuron; therefore we write \(u_{j}\) instead of \(u_{1j}\) and the constrained optimization problem in 10 simplifies to

\[\text{minimize }a^{2}g_{\sigma}(r,r,1)-2a\sum_{j=1}^{k}g_{\sigma}(r,1,u_{j} )+\text{const},\quad\text{subject to }\|u\|\leq 1,r\geq 0.\] (14)From Proposition 4.1, we have that \(\|u\|=1\) for any non-trivial critical point. Therefore, the constraint of 14 on the correlations \(u=(u_{1},...,u_{k})\) is satisfied. The mapping of any non-trivial critical point to the order-parameter space is a critical point of the Lagrangian loss (see Appendix Lemma G.2). Hence every \(u_{j}\) satisfies

\[-2a\frac{d}{du_{j}}g_{\sigma}(r,1,u_{j})+2\lambda u_{j}=0\] (15)

for fixed \((r,a)\). Assumption 3.1-(ii) implies that \(\frac{1}{u}\partial_{u}g_{\sigma}(r,1,u)\) is one-to-one hence all \(u_{j}\) are equal. _End of Proof Sketch._

We show in Lemma F.2 that the interactions of the common activation functions such as erf, tanh, sigmoid, and softplus (respectively)

\[\sigma_{\text{erf}}(x)=\frac{2}{\sqrt{\pi}}\!\int_{0}^{\frac{x}{ \sqrt{2}}}e^{-t^{2}}dt,\;\sigma_{\text{tanh}}(x)=\frac{1-e^{-x}}{1+e^{-x}},\; \sigma_{\text{sig}}(x)=\frac{1}{1+e^{-x}},\;\sigma_{\text{soft}}^{\beta}(x)= \frac{1}{\beta}\log(e^{\beta x}+1),\]

with \(\beta\in(0,2]\) satisfy Assumption 3.1-(ii). The interaction of the ReLU activation function, i.e. \(\sigma_{\text{relu}}(x)=\max(0,x)\) also satisfies Assumptions 3.1 (with a slight modification in the domain for (ii); see the proof of Corollary G.5).

Thanks to Theorem 5.1, the loss in Eq. 14 can be reduced to a two-dimensional loss in \(a\) and \(r\), which can be solved explicitly for ReLU (Corollary G.5) and erf.

**Corollary 5.2**.: _Assume that the activation function is \(\sigma_{\text{erf}}\). The optimal solution \((w^{*},a^{*})\) is given by_

\[\|w^{*}\|=\sqrt{\frac{1}{2k-1}},\quad a^{*}=k,\quad\frac{w^{*}}{ \|w^{*}\|}=\frac{1}{\sqrt{k}}\sum_{i=1}^{k}v_{i},\]

_or, equivalently, by \((-w^{*},-a^{*})\). The optimal loss is then given by_

\[L^{*}_{\text{erf}}(k)=\frac{2}{\pi}\Big{(}k\arcsin\!\big{(}\frac {1}{2}\big{)}-k^{2}\arcsin\!\big{(}\frac{1}{2k}\big{)}\Big{)}.\] (16)

The proof of Corollary 5.2 is presented in Section G.3. For general activation functions, the two-dimensional loss does not admit an analytical expression. For this case, from the partial derivatives, we obtain a fixed-point equation in \(r\) which we solve numerically for the activation functions listed above (see Appendix Section G.2.2). For softplus, specifically, we prove in addition the following

**Theorem 5.3**.: _Assume that the activation function is \(\sigma_{\text{soft}}^{\beta}(x)\) with \(\beta\leq 2\). The optimal solution \((w^{*},a^{*})\) satisfies \(\|w^{*}\|\leq\nicefrac{{1}}{{\sqrt{k}}}\) and \(a^{*}\geq k\)._

We use the FKG inequality to prove Theorem 5.3 (see Appendix G.5). Although the proof requires specific properties of softplus, we show that the above bounds hold also for tanh and sigmoid by numerically solving the fixed point equation (Figure 10, also Figure 5). Note that the incoming vector norm is bounded above by \(1/\sqrt{k}\), hence \(w^{*}\) is a _damped_ average of the teacher incoming vectors.

Figure 3: _One-neuron network solutions._ **A** Network output (color coded) as a function of input in \(d=2\) for (left) a unit-orthonormal network with \(k=2\) neurons (incoming vectors \(v_{1}\) and \(v_{2}\) are shown as black dots) and (right) the student network function generated by the optimal solution (incoming vector shown in red) for the erf activation function. **B** Same for the softplus activation function. **C** Approximation error of a student with \(n=1\) neurons as a function of the number of \(k\) teacher neurons. For large \(k\), the approximation error for \(n=1\) grows near-linearly for the differentiable activation functions studied in this paper (erf, sigmoid, tanh, and softplus with \(\beta=1\)); however the growth is quadratic for ReLU (see Appendix Corollary G.5).

**Remark 5.4**.: _We do not impose either of the two reductions that are common in literature: (i) incoming vector \(w\) is constrained to be on the unit sphere [9, 11, 14, 40], (ii) the outgoing weight \(a\) is constrained to be one [7, 18, 20]. An important step in our analysis is related to the norm \(r\) of the incoming vector which we discuss in Appendix Section G.2.2._

### Multi-Neuron Network

In this subsection, we assume that the activation function is erf such that CA-configurations are critical points (see Theorem 4.2). For a student network with \(n=2\) and a partition \((\ell_{1},\ell_{2})\) we can decompose the loss of a CA critical point as

\[L_{\text{erf}}^{*}(\ell_{1})+L_{\text{erf}}^{*}(\ell_{2})+L_{\text{erf}}^{*}(0, k-(\ell_{1}+\ell_{2}))\] (17)

where \(L_{\text{erf}}^{*}(0,\ell_{0}):=\mathbb{E}_{x\sim\mathcal{D}}[f_{\ell_{0}}^{*} (x)^{2}]\) is the error made by a student with vanishing output when representing a reduced unit-orthonormal teacher network with \(\ell_{0}\) neurons. This decomposition is possible because the cross-terms between orthogonal vectors are zero for odd activation functions. Furthermore, for the erf activation function, we show that

\[L_{\text{erf}}^{*}(\ell_{1})+L_{\text{erf}}^{*}(0,\ell_{0})>L_{\text{erf}}^{*} (\ell_{1}+\ell_{0})\] (18)

(see the proof of Lemma E.2). Therefore, we should search for the minimum loss configuration among the partitions with \(\ell_{1}+\ell_{2}=k\). Among such partitions, Lemma E.2 shows that the optimum CA-point has the partition \((1,k-1)\). In words, the optimum is a \(1\)-copy-\(1\)-average point.

For general \(n\), using Lemma E.2 and a small trick, we prove the following.

**Theorem 5.5**.: _Consider a unit-orthonormal teacher network \(f^{*}(x)=\sum_{j=1}^{k}\sigma(v_{j}\cdot x)\) and the erf activation function. For an under-parameterized student network with \(n\) neurons, the minimum-loss copy-average configuration up to permutations (of the student and teacher neurons) is_

\[\theta=(\epsilon_{1}v_{1},\epsilon_{1})\oplus...\oplus(\epsilon_{n-1}v_{n-1}, \epsilon_{n-1})\oplus(\epsilon_{n}w_{n}^{*},\epsilon_{n}a_{n}^{*})\] (19)

_where \(\epsilon_{i}\in\{\pm 1\}\) and \((w_{n}^{*},a_{n}^{*})\) is given by Corollary 5.2 after substituting \(k\) with \(k-n+1\)._

Figure 4: _Under-parameterized student networks of width \(n\) with erf activation function learning (via gradient flow) from a unit-orthonormal teacher network of width \(k\)._ **A** Each dot is the mean error at convergence for \(20\) seeds of random initializations; black-dashed lines are the theory predictions \(L_{\text{erf}}^{*}(k-n+1)\), see Eq. 20. Standard deviations do not show on the figure as they are too small. We identify four regimes indicated by colors (green-gray-blue-red) depending on the type of solution found by gradient flow (GF). In the green regime, GF converges to an optimal \((n-1)\)-C-\(1\)-A solution for all \(20\) initializations (Fig. 4-B1). In the gray regime, GF converges either to \((n-1)\)-C-\(1\)-A solution or to a “Perturbation of the all-copy solution” that we call P-\(n\)-C (Fig. 4-B2). In the blue and red regimes, for \(n>\gamma_{2}k\) where \(n=8,12,16\) the gradient flow converges to a P-\(n\)-C solution from all seeds (Fig. 4-B3). Moreover, in the red regime, for \(n>\gamma_{3}k\) where \(n=8,12,16\) and \(\gamma_{3}\) is near \(0.75\), the P-\(n\)-C solutions achieve lower loss than the \((n-1)\)-C-\(1\)-A solutions (Fig. 4-B4). **B1-B4** Examples of loss at convergence (vertical axis) for all \(20\) different initialization seeds (horizontal axis); theory is shown by the red-dashed horizontal line. Insets show examples of correlation matrices \(u_{ij}\) (\(k\) lines, \(n\) columns) between student and teacher incoming vectors at convergence after reordering neurons. In the gray regime (for ex. B2) the gradient flow converges to either one of the two types of minima with correlations shown in the inset; in the other regimes, it consistently converges to the same minimum up to permutations.

See Appendix E.3 for the proof. Because copy-average critical points are not necessarily the only critical points of the loss function for students with \(n>1\), we investigate in simulations, if they are found by gradient flow where the weights are initialized as Gaussian with a fixed standard deviation (see Fig. 4).

Interestingly, gradient flow converges to the CA-optimal solution for all random seeds in a broad regime of under-parameterization (green in Fig. 4). Only when \(n>\gamma_{1}k\) for \(n=8,12,16\) and \(\gamma_{1}\sim 0.46\), gradient flow converged in some seeds to points close to the \(n\)-copy critical point. We call these newly found points "perturbed \(n\)-copy" (P-\(n\)-C) points. In gray-blue regimes, the P-\(n\)-C points have higher loss than the optimal CA critical point (Fig. 4).

However, this is not always the case: when the student width is close to the teacher width (low compression regime), the P-\(n\)-C point has a slightly lower loss the lowest amongst the CA critical points (red in Fig. 4). When \(k-n\) is fixed, we found that the \((n-1)\)-C-\(1\)-A solution turns from a minimum for small \(n\) to a saddle for large \(n\) (see App. Fig. 6); which explains why the gradient flow escapes it in this regime and converges to another minimum at a lower loss.

Finally, based on our theory and experiments, we conjecture that there exists a \(\gamma_{0}\in(0,\gamma_{3})\) such that when \(n<\gamma_{0}k\) and when the activation function is erf, the global optimum of the non-convex loss in Eq. 5 is a \((n-1)\)-C-\(1\)-A configuration. Therefore, if our conjecture holds, the _exact_ approximation error, i.e. the optimal loss, is identical to that of a one-neuron network approximating a teacher with \(k-n+1\) neurons and is given by

\[L^{*}_{\text{erf}}(n,k)=L^{*}_{\text{erf}}(k\!-\!n\!+\!1).\] (20)

## 6 Conclusion & Future Directions

We studied the learning of under-parameterized student networks from orthogonal teacher networks for standard Gaussian input data and vanishing thresholds. For erf activation function, we introduced a new family of critical points that arise from the decoupling of the problem into one-neuron networks that can be solved separately. Moreover, the exact parameters of copy-average (CA) critical points are given which can be used to study escape behavior near saddles and to determine convergence of first and second-order optimization algorithms [16; 41].

Furthermore, we showed that the optimal CA point is that \(n-1\) neurons copy teacher neurons and the \(n\)-th neuron averages the remaining \(k-n+1\) neurons. In simulations, gradient flow converges to a CA-optimal solution for \(n<\gamma_{1}k\) where \(\gamma_{1}\) is near \(0.46\). However, for \(n>\gamma_{2}k\) where \(\gamma_{2}\) is near \(0.6\), we observe another phase where the gradient flow finds a perturbed copy solution. For the ReLU activation function and the onset of under-parameterization, gradient flow converges to qualitatively similar solutions; however, at the crossing point from under-parameterization to over-parameterization (i.e. \(n=k\)), gradient flow is known to get stuck in spurious local minima [27]. On another note, determining the CA-optimal solution of the two-neuron network plays a critical role in our analysis. Still, there is only little literature on two-neuron networks [5; 42] compared to the well-studied one-neuron case [5; 6; 7; 22; 23]. The two-neuron network is possibly the simplest model with interactions between neurons, hence it is important to understand the global minimum and gradient flow dynamics of this challenging problem.

On the practical side, our analysis of under-parameterized networks gives a recipe for how to warm-start smaller neural networks for distilling unit-orthonormal teacher networks. If one desires low compression (\(n>\gamma_{3}k\)), then we recommend initializing the student network in a configuration where each neuron copies a different teacher neuron, to be close to a P-\(n\)-C point. However, for higher compression, we recommend initializing the student network in a configuration where \(n-1\) neurons are each copied and the \(n\)-th neuron is initialized as an average neuron to be close to a \((n-1)\)-copy-\(1\)-average point. It remains an open question whether this recipe applies to non-idealized scenarios such as non-isotropic input distribution, teacher networks with non-orthogonal incoming vectors, or non-unit outgoing weights. More generally, it is natural to expect that the optimal distillation strategy changes from low compression levels to high compression levels. How exactly and where this change happens is a very intriguing question of theory and practice.

## Acknowledgements

The authors thank Lenka Zdeborova for the discussions and encouragement at the beginning of this project and Clement Hongler for many discussions and valuable feedback. This work was supported by the Swiss National Science Foundation (no. \(200020\_207426\)).

## References

* [1] Ken-Ichi Funahashi. On the approximate realization of continuous mappings by neural networks. _Neural networks_, 2(3):183-192, 1989.
* [2] David Saad and Sara A Solla. On-line learning in soft committee machines. _Physical Review E_, 52(4):4225, 1995.
* [3] Sebastian Goldt, Madhu Advani, Andrew M Saxe, Florent Krzakala, and Lenka Zdeborova. Dynamics of stochastic gradient descent for two-layer neural networks in the teacher-student setup. _Advances in neural information processing systems_, 32, 2019.
* [4] Youngmin Cho and Lawrence Saul. Kernel methods for deep learning. _Advances in neural information processing systems_, 22, 2009.
* [5] Yuandong Tian. An analytical formula of population gradient for two-layered relu network and its applications in convergence and critical point analysis. In _International conference on machine learning_, pages 3404-3413. PMLR, 2017.
* [6] Song Mei, Yu Bai, and Andrea Montanari. The landscape of empirical risk for nonconvex losses. _The Annals of Statistics_, 46(6A):2747-2774, 2018.
* [7] Gilad Yehudai and Shamir Ohad. Learning a single neuron with gradient methods. In _Conference on Learning Theory_, pages 3756-3786. PMLR, 2020.
* [8] Charles M Stein. Estimation of the mean of a multivariate normal distribution. _The annals of Statistics_, pages 1135-1151, 1981.
* [9] Rishabh Dudeja and Daniel Hsu. Learning single-index models in gaussian space. In _Conference On Learning Theory_, pages 1887-1930. PMLR, 2018.
* [10] Gerard Ben Arous, Reza Gheissari, and Aukosh Jagannath. Online stochastic gradient descent on non-convex losses from high-dimensional inference. _The Journal of Machine Learning Research_, 22(1):4788-4838, 2021.
* [11] Alberto Bietti, Joan Bruna, Clayton Sanford, and Min Jae Song. Learning single-index models with shallow neural networks. _Advances in Neural Information Processing Systems_, 35:9768-9783, 2022.
* [12] Raphael Berthier, Andrea Montanari, and Kangjie Zhou. Learning time-scales in two-layers neural networks. _arXiv preprint arXiv:2303.00055_, 2023.
* [13] Yatin Dandi, Florent Krzakala, Bruno Loureiro, Luca Pesce, and Ludovic Stephan. Learning two-layer neural networks, one (giant) step at a time. _arXiv preprint arXiv:2305.18270_, 2023.
* [14] Alex Damian, Eshaan Nichani, Rong Ge, and Jason D Lee. Smoothing the landscape boosts the signal for sgd: Optimal sample complexity for learning single index models. _arXiv preprint arXiv:2305.10633_, 2023.
* [15] Amit Daniely, Roy Frostig, and Yoram Singer. Toward deeper understanding of neural networks: The power of initialization and a dual view on expressivity. _Advances in neural information processing systems_, 29, 2016.
* [16] Johanni Brea, Flavio Martinelli, Berlin Simsek, and Wulfram Gerstner. Mlpgradientflow: going with the flow of multilayer perceptrons (and finding minima fast and accurately). _arXiv preprint arXiv:2301.10638_, 2023.

* [17] Michael Biehl and Holm Schwarze. Learning by on-line gradient descent. _Journal of Physics A: Mathematical and general_, 28(3):643, 1995.
* [18] David Saad and Sara A Solla. Exact solution for on-line learning in multilayer neural networks. _Physical Review Letters_, 74(21):4337, 1995.
* [19] Peter Riegler and Michael Biehl. On-line backpropagation in two-layered neural networks. _Journal of Physics A: Mathematical and General_, 28(20):L507, 1995.
* [20] Rodrigo Veiga, Ludovic Stephan, Bruno Loureiro, Florent Krzakala, and Lenka Zdeborova. Phase diagram of stochastic gradient descent in high-dimensional two-layer neural networks. _arXiv preprint arXiv:2202.00293_, 2022.
* [21] Weihang Xu and Simon Du. Over-parameterization exponentially slows down gradient descent for learning a single neuron. In _The Thirty Sixth Annual Conference on Learning Theory_, pages 1155-1198. PMLR, 2023.
* [22] Gal Vardi, Gilad Yehudai, and Ohad Shamir. Learning a single neuron with bias using gradient descent. _Advances in Neural Information Processing Systems_, 34:28690-28700, 2021.
* [23] Lei Wu. Learning a single neuron for non-monotonic activation functions. In _International Conference on Artificial Intelligence and Statistics_, pages 4178-4197. PMLR, 2022.
* [24] Gerard Ben Arous, Reza Gheissari, and Aukosh Jagannath. High-dimensional limit theorems for sgd: Effective dynamics and critical scaling. _arXiv preprint arXiv:2206.04030_, 2022.
* [25] Flavio Martinelli, Berlin Simsek, Johanni Brea, and Wulfram Gerstner. Expand-and-cluster: Exact parameter recovery of neural networks. _arXiv preprint arXiv:2304.12794_, 2023.
* [26] Alberto Bietti, Joan Bruna, and Loucas Pillaud-Vivien. On learning gaussian multi-index models with gradient flow. _arXiv preprint arXiv:2310.19793_, 2023.
* [27] Itay Safran and Ohad Shamir. Spurious local minima are common in two-layer relu neural networks. In _International Conference on Machine Learning_, pages 4433-4441. PMLR, 2018.
* [28] Mahdi Soltanolkotabi, Adel Javanmard, and Jason D Lee. Theoretical insights into the optimization landscape of over-parameterized shallow neural networks. _IEEE Transactions on Information Theory_, 65(2):742-769, 2018.
* [29] Samet Oymak and Mahdi Soltanolkotabi. Toward moderate overparameterization: Global convergence guarantees for training shallow neural networks. _IEEE Journal on Selected Areas in Information Theory_, 1(1):84-105, 2020.
* [30] Yossi Arjevani and Michael Field. Analytic study of families of spurious minima in two-layer relu neural networks: a tale of symmetry ii. _Advances in Neural Information Processing Systems_, 34:15162-15174, 2021.
* [31] Nelson Elhage, Tristan Hume, Catherine Olsson, Nicholas Schiefer, Tom Henighan, Shauna Kravec, Zac Hatfield-Dodds, Robert Lasenby, Dawn Drain, Carol Chen, et al. Toy models of superposition. _arXiv preprint arXiv:2209.10652_, 2022.
* [32] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. _arXiv preprint arXiv:1503.02531_, 2015.
* [33] Jianping Gou, Baosheng Yu, Stephen J Maybank, and Dacheng Tao. Knowledge distillation: A survey. _International Journal of Computer Vision_, 129:1789-1819, 2021.
* [34] George Cybenko. Approximation by superpositions of a sigmoidal function. _Mathematics of control, signals and systems_, 2(4):303-314, 1989.
* [35] Kurt Hornik, Maxwell Stinchcombe, and Halbert White. Multilayer feedforward networks are universal approximators. _Neural networks_, 2(5):359-366, 1989.
* [36] Andrew R Barron. Universal approximation bounds for superpositions of a sigmoidal function. _IEEE Transactions on Information theory_, 39(3):930-945, 1993.

* Safran et al. [2021] Itay M Safran, Gilad Yehudai, and Ohad Shamir. The effects of mild over-parameterization on the optimization landscape of shallow relu neural networks. In _Conference on Learning Theory_, pages 3889-3934. PMLR, 2021.
* Mousavi-Hosseini et al. [2022] Alireza Mousavi-Hosseini, Sejun Park, Manuela Girotti, Ioannis Mitliagkas, and Murat A Erdogdu. Neural networks efficiently learn low-dimensional representations with sgd. _arXiv preprint arXiv:2209.14863_, 2022.
* Simsek et al. [2021] Berlin Simsek, Francois Ged, Arthur Jacot, Francesco Spadaro, Clement Hongler, Wulfram Gerstner, and Johanni Brea. Geometry of the loss landscape in overparameterized neural networks: Symmetries and invariances. In _International Conference on Machine Learning_, pages 9722-9732. PMLR, 2021.
* Bruna et al. [2023] Joan Bruna, Loucas Pillaud-Vivien, and Aaron Zweig. On single index models beyond gaussian data. _arXiv preprint arXiv:2307.15804_, 2023.
* Frye et al. [2021] Charles G Frye, James Simon, Neha S Wadia, Andrew Ligeralde, Michael R DeWeese, and Kristofer E Bouchard. Critical point-finding methods reveal gradient-flat regions of deep network losses. _Neural computation_, 33(6):1469-1497, 2021.
* ICLR_, 2018.
* Glorot and Bengio [2010] Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks. In _Proceedings of the thirteenth international conference on artificial intelligence and statistics_, pages 249-256. JMLR Workshop and Conference Proceedings, 2010.
* Boyd et al. [2004] Stephen Boyd, Stephen P Boyd, and Lieven Vandenberghe. _Convex optimization_. Cambridge university press, 2004.

* A Summary of Results
* B Further Comparison to Literature
* C Further Experiments
* C.1 One-Neuron Network
* C.2 Eff Experiments
* C.3 ReLU Experiments
* D Constrained Optimization Formulation
* D.1 Equality Constraints in the Case \(d=k\)
* D.2 Binary-Equality Constraints in the Case \(d=k+1\)
* D.3 Three-Neuron Network
* E Copy-Average Critical Points
* E.1 Number of CA Critical Points
* F General Properties of the Interaction Function
* G The One-Neuron Network
* G.1 Any Non-Trivial Critical Point Satisfies the Lagrangian Condition
* G.2 General Activation Functions
* G.3 Closed-Form Solution for Erf Activation
* G.4 Closed-Form Solution for ReLU Activation
* G.5 Bounds on Incoming Vector Norm and Outgoing Weight for Softplus

## Appendix A Summary of Results

In the table above, UO means unit-orthonormal, n.i.g. stands for 'not in general' and yes* follows as a special case from the results with yes on the same row.

## Appendix B Further Comparison to Literature

In this section, we compare the symmetric solutions found in erf [2] and ReLU networks [5] to our one-neuron solution (\(n=1\)). The main difference is that both earlier studies constrain the search space to the symmetric subspace whereas we first prove that the non-trivial critical points are contained in this subspace in Theorem 5.1 for a broad class of activation functions, including erf and ReLU. Solving the low-dimensional loss, we recover the same solution for ReLU and erf as in [2, 5] for unit-orthonormal teachers.

\begin{table}
\begin{tabular}{l l l l} \hline \hline columns=conditions: & Orthogonal & UO \& erf & UO \& \(\sigma\) satisfying \(g_{\sigma}\) assumptions \\ lines=results: & & & & \\ \hline \(n=1\) average is the optimal solution & n.i.g. & yes* & yes \\ \(n>1\) CA points are critical points & n.i.g. & yes & n.i.g. maybe for odd \\ \(n>1\)\((n-1)\)-C-1-A is the optimal-CA solution & n.i.g. & yes & n.i.g. maybe for some odd \\ \(n=1\)\(w^{*}\) is in the span of \(\{v_{1},...,v_{k}\}\) & yes & yes* & yes* \\ \hline \hline \end{tabular}
\end{table}
Table 1: Summary of Results_Symmetric Solution of Saad and Solla [2] for ef activation._ The authors focus on the'symmetric subspace' parameterized as

\[Q_{ii}=r_{i}^{2}=Q,\quad Q_{ij}=p_{ij}r_{i}r_{j}=C,\quad R_{in}=u_{in}r_{i}=R.\] (21)

In this case, the loss is parameterized by three values, that is \(Q,C,R\), hence can be expressed analytically in terms of these values. Solving the fixed point equations, they find the following critical/fixed point (their Eq.22)

\[Q=C=\frac{1}{2k-1},\quad R=\frac{1}{\sqrt{k(2k-1)}}\] (22)

which implies \(r_{i}=\nicefrac{{1}}{{\sqrt{2k-1}}}\) and \(\rho_{ij}=1\) in our parameterization. This selection of parameters forces all student vectors to be equal therefore reducing the system to a one-neuron network. There are two main improvements in our analysis

1. We prove that student-teacher correlations \(u_{ij}\) are equal to each other at a non-trivial critical point, and give necessary conditions on the activation function (Assumption 3.1) to satisfy this property. We show in Lemma F.2 that not only erf but a large class of common activation functions satisfy Assumption 3.1.
2. Our student network has a flexible outgoing weight (shallow neural network) as opposed to a fixed outgoing weight \(+1\) (soft-committee machine) in Saad and Solla [2]. It is instructive to compare the generalization errors of the one-neuron network \[\text{(soft-committee machine)}\;\;L_{\text{erf.\;soft}}^{*}(k)=\frac{k} {3}-k^{2}\frac{2}{\pi}\arcsin(\frac{1}{2k})\] \[\text{(shallow network)}\;\;L_{\text{erf}}^{*}(k)=k\frac{2}{\pi} \arcsin(\frac{1}{2})-k^{2}\frac{2}{\pi}\arcsin(\frac{1}{2k})\approx k(\frac{ 1}{3}-\frac{1}{\pi}),\] which are identical since \(\arcsin(0.5)=\pi/6\) (Saad and Solla [2] uses \(\epsilon_{g}(k)=\frac{1}{2}L_{\text{erf.\;soft}}^{*}(k)\) that's why there is a factor \(0.5\) difference with respect to their Eq. (23)). However, if we set teacher outgoing weights to say \(a_{t}\), the shallow network adapts and reaches the generalization error \(a_{t}^{2}L_{\text{erf}}^{*}(k)\) but the error of the soft-committee machine is \[L_{\text{erf.\;soft}}^{*}(k) =k^{2}g(\frac{1}{\sqrt{2k-1}},\frac{1}{\sqrt{2k-1}},1)-2k^{2}a_{ t}g(\frac{1}{\sqrt{2k-1}},1,\frac{1}{\sqrt{k-1}})+a_{t}^{2}kg(1,1,1)\] \[=O(a_{t})+a_{t}^{2}k\frac{1}{3}.\] which has a worse coefficient \(\frac{1}{3}\) compared to \(\frac{1}{3}-\frac{1}{\pi}\) as expected.

_Symmetric Solution of Tian [5] for ReLU activation._ The authors focus on a particular two-dimensional subspace \((x,y)\) that allows the specialization of student neurons, namely

\[w_{i}=xv_{i}+y\sum_{j\neq i}v_{j}.\] (23)

In particular, they consider the'symmetric subspace' \(x=y\) which is the case when all student neurons collapse to one neuron, and show that the dynamics converge to the following fixed point

\[x=y=\frac{1}{\pi k}(\sqrt{k-1}-\arccos(\frac{1}{\sqrt{k}})+\pi).\] (24)

Summing over \(k\) neurons then produces the following one-neuron due to the positive homogeneity

\[w^{*}=\frac{1}{\pi}(\sqrt{k-1}-\arccos(\frac{1}{\sqrt{k}})+\pi)\sum_{j=1}^{k} v_{j}.\]

Our formula (Corollary G.5) gives the identical result due to

\[w^{*}a^{*}=\frac{k}{h(1)}h(\frac{1}{\sqrt{k}})\sum_{j=1}^{k}\frac{1}{\sqrt{k} }v_{j}=\frac{1}{\pi}(\sqrt{k-1}-\arccos(\frac{1}{\sqrt{k}})+\pi)\sum_{j=1}^{k} v_{j}.\]

In this case, there is no difference between the optimal solution of the soft-committee machine and the shallow network since ReLU is positive-homogeneous as expected.

## Appendix C Further Experiments

All experiments in this paper are implemented using the gradient flow package implemented by Brea et al. [16] which is particularly suited to studying gradient flow on the population loss. For activation functions for which there is an analytic formula, it is already implemented in the package; for the others, we used the approximator option for a speed-up compared to the numerical integration option. This method uses a neural network in the background fitted to approximate Gaussian integrals. We trained for \(10^{5}\) code iterations for erf and relu experiments; \(10^{3}\) ode iterations for softplus, tanh, and sigmoid. For erf experiments, all seeds converged to configurations with gradient norm below \(5\cdot 10^{-8}\). For ReLU experiments, a fraction of seeds failed to converge (large gradient norm at the end of training). In Appendix C.3, we report among the seeds that succeeded in converging. Weights initialized as Gaussians with zero mean and standard deviation \(0.1\) or with Glorot initialization [43]. This is in contrast with Saad and Solla [2], Tian [5] where (order) parameters are initialized with positive values (as opposed to the rotationally symmetric initializations done in practice). For each \((n,k)\) pair, we implemented \(10\) or \(20\) seeds of random initializations.

### One-Neuron Network

Empirically, gradient flow converges to the point where all student-teacher correlations are \(\frac{1}{k}\)4.

Footnote 4: For odd activation functions, there are two solutions that are sign-symmetric: the first one where all correlations are \(\frac{1}{\sqrt{k}}\) and its equivalent where all correlations are \(-\frac{1}{\sqrt{k}}\). The first solution is plotted in Fig. 5 for a fine comparison on the positive scale.

We know from Theorem 5.1 that at the non-trivial critical point all correlations are equal at correlation \(\nicefrac{{1}}{{\sqrt{k}}}\) and there is possibly another critical point at correlation \(-\nicefrac{{1}}{{\sqrt{k}}}\). Depending on the activation function, the point where correlations are \(-\nicefrac{{1}}{{\sqrt{k}}}\) might be

* either an equivalent of the optimum solution (for odd activation functions),
* or a saddle (for ReLU),
* or does not exist (for softplus).

The details can be found in the proofs for individual cases.

### Erf Experiments

In this section, we first numerically investigate whether the CA-optimal critical point is a saddle or minimum in Figure 6. Surprisingly, we find that the point turns from a saddle point to a minimum

Figure 5: _Structure of the optimal solution of the one-neuron network for various activation functions._ We trained \(20\) seeds of one-neuron students learning from the unit-orthonormal teacher networks with \(k=2,...,10\) neurons. All students converge to the same optimal solution up to symmetries (that is, positive-scaling symmetry for ReLU and sign symmetry for odd activation functions such as tanh and erf). **A** For ReLU, the magnitude \(\|w^{*}\|a^{*}\) exactly matches with the result of Corollary G.5. For softplus, the magnitude is very close to \(\sqrt{k}\); for sigmoid, tanh, and erf, it is below \(\sqrt{k}\). **B** The norm of the incoming vector is smaller than \(1/\sqrt{k}\) for softplus, sigmoid, tanh, and erf. **C** The outgoing weight is larger than \(k\) for softplus and tanh, and it is virtually \(k\) for sigmoid and erf.

point in some regimes of \((n,k)\), despite that the configuration has the same structure of copying \(n-1\) teacher neurons and taking an average of the remaining teacher neurons. When \(k-n\) is fixed, and for large \(n\), Figure 6 explains why gradient flow does not converge to the CA-optimal point.

Figure 6: _The minimum eigenvalue of the Hessian at an optimal-CA point._ We numerically investigate whether a CA-optimal critical point is a strict saddle (min. eig. of the Hessian is negative) or a minimum (min. eig. of the Hessian is non-negative). Interestingly, the minimum eigenvalue turns from positive to negative as \(n\) grows for \(k=n+h\) for fixed \(h=1,2,3\) (left panel). Therefore in this regime, the CA-optimal cannot be the optimal solution of the non-convex problem for large \(n\). For \(k\gg n\), for example for \(k=n,2n,3n\) (right panel), the min. eigenvalue is positive and it approaches zero as \(n\) increases.

Figure 7: _Evolution of order parameters during convergence to a \((n-1)\)-\(C\)-\(1\)-A solution: top \(n=2\), middle \(n=4\), bottom \(n=8\); and \(k=64\); representative seeds._ We can distinguish \(3\) phases: before iteration \(20\), after iteration \(20\), and beyond iteration \(40\). We observe that in the first phase of training (less than \(5\) iterations), the student neurons do not specialize into teacher neurons but approach the one-neuron solution. For \(n=2\) (top row), in the second phase, we observe that the first neuron implements an average of teacher neurons and the second neuron implements a copy of the remaining teacher neuron. In the second phase, in general, \(n-1\) neurons specialize to match one teacher neuron each (or its negative equivalent) and the \(n\)-th neuron splits its correlations into two groups: those that correspond to the teacher neurons being matched become negative and the others collapse on each other. Finally, in the third phase, the negative correlations converge to zero correlation, decoupling the student neurons from each other. All student neuron correlation signs can be flipped as long as the corresponding outgoing weight signs are flipped since the erf activation is odd. These examples illustrate the green regime (see Fig. 4 in the main text).

We show some representative trajectories of gradient flow, in the regime when the CA-optimal critical point is a minimum in Figure 7 and in the regime when it is a saddle point in Figure 8.

### ReLU Experiments

In this subsection, we will present the structure of the minimum loss configuration found by gradient flow. In the regime \(n\ll k\), the minimum loss configuration is qualitatively similar to the optimal-CA solution but without perfect decoupling. For \(k\) that is slightly bigger than \(n\), the gradient flow finds an "all-copy" configuration for \(n=4,12\). The overall trend of the minimum loss configuration is

Figure 8: _Evolution of order parameters; P-\(n\)-C solution: for \(n=8\) and \(k=9\). We observe that all student neurons match one teacher neuron in this case, however not perfectly at the end of training. This is an example of the red regime (see Fig. 4 in the main text), where the students converge to a perturbation of the \(n\)-copy configuration._

Figure 9: _Optimal configuration found by gradient flow for ReLU activation function for \(n=2\) (top); \(n=4\) (middle); \(n=12\) (bottom). The last row of the correlation matrices represents \(1-\|u_{i}\|\) which is zero for all student neurons in all cases. Top row, \(n=2\); the optimal point is composed of a copy and an average neuron: the copy neuron is very close to one of the teacher neurons, and the average neuron is close to the average of the remaining teacher neurons while negatively correlating with the copied teacher neuron. The negative correlation increases in magnitude as \(k\) increases. Middle row, \(n=4\); for \(k=5\), the optimal point is a perturbation of the all-copy configuration; for \(k=8,16\), it is close to the \((n-1)\)-copy-\(1\)-average configuration. Bottom row, \(n=12\); for \(k=16,24\), the optimal point is a perturbation of the all-copy configuration; for \(k=32\), it is close to the \((n-1)\)-copy-\(1\)-average configuration. In all regimes, the norms and outgoing weights of all student neurons are close to each other (for the bottom row only the first four neurons are shown)._

[MISSING_PAGE_EMPTY:19]

### Equality Constraints in the Case \(d=k\)

In this case, the teacher incoming vectors \(v_{1},\ldots,v_{k}\) span the input domain \(\mathbb{R}^{d}\). Each incoming vector (of the student) can be expressed as a linear combination of the teacher's incoming vectors

\[\frac{w_{i}}{r_{i}}=\sum_{j=1}^{k}u_{ij}v_{j},\quad\sum_{j=1}^{k}u_{ij}^{2}=1,\] (32)

and the equality constraint pops up since the normalized vector has a unit \(\ell_{2}\) norm. The correlations between the incoming vectors are then expressed in terms of the student-teacher correlations

\[\rho_{ii^{\prime}}=u_{i}\cdot u_{i^{\prime}}.\] (33)

Therefore the optimization problem is equivalent to

\[\min \sum_{i=1}^{n}a_{i}^{2}g_{\sigma}(r_{i},r_{i},1)+2\sum_{i\neq i^{ \prime}}a_{i}a_{i^{\prime}}g_{\sigma}(r_{i},r_{i^{\prime}},\sum_{j=1}^{k}u_{ ij}u_{i^{\prime}j})-2\sum_{i=1}^{n}\sum_{j=1}^{k}a_{i}b_{j}g_{\sigma}(r_{i},\|v_{j} \|,u_{ij})\] subject to \[\sum_{j=1}^{k}u_{ij}^{2}=1,\;\;r_{i}\geq 0,\quad\text{for all}\;\;i\in[n].\] (34)

Since \(\binom{n}{2}\) student-student correlations terms are not free, the problem has only \(n(k+2)\) free parameters and \(k\) equality constraints, yielding \(n(k+1)\) effective parameters, which is the same number as the number of parameters of the original problem in the weight-space.

### Binary-Equality Constraints in the Case \(d=k+1\)

In this case, there is only one direction orthogonal to the span of the teacher's incoming vectors (i.e. \(v_{i}^{\perp}\parallel v_{i^{\prime}}^{\perp}\)). Therefore the general inequality constraint on \(\rho_{ii^{\prime}}\) reduces to

\[\rho_{ii^{\prime}}=u_{i}\cdot u_{i^{\prime}}\pm\sqrt{1-\|u_{i}\|^{2}}\sqrt{1- \|u_{i^{\prime}}\|^{2}}.\] (35)

### Three-Neuron Network

We present a case for the three-neuron network where the optimal solution of the constrained optimization problem may not be projected back to the weight space. In particular, let us consider a positive and monotonic activation function, i.e. \(\sigma(x)>0\). This implies that \(g_{\sigma}>0\) and that \(g_{\sigma}\) is increasing in correlation. It is natural to expect that all outgoing weights are positive since this would bring the network function closer to the target function. If the network is overparameterized, some outgoing weights may be zero or even negative (balanced by a positive outgoing weight corresponding to the same incoming vector). Let us pick three positive outgoing weights \(a_{1},a_{2},a_{3}>0\) and three corresponding student-student interaction terms

\[\text{minimize}\;\;a_{1}a_{2}g_{\sigma}(r_{1},r_{2},\rho_{12})+a _{1}a_{3}g_{\sigma}(r_{1},r_{3},\rho_{13})+a_{2}a_{3}g_{\sigma}(r_{2},r_{3}, \rho_{23})+...,\] \[\text{subject to}\;\;|\rho_{ii^{\prime}}-u_{i}\cdot u_{i^{\prime}}| \leq\sqrt{1-\|u_{i}\|^{2}}\sqrt{1-\|u_{i^{\prime}}\|^{2}}.\] (36)

Note that each \(\rho_{ii^{\prime}}\) is decoupled from each other. Since \(g_{\sigma}\) is increasing in correlation, the minimum of each term above is achieved when

\[\rho_{ii^{\prime}}=u_{i}\cdot u_{i^{\prime}}-\sqrt{1-\|u_{i}\|^{2}}\sqrt{1-\|u _{i^{\prime}}\|^{2}}.\] (37)

This implies that the inequality is tight and therefore \(v_{i}^{\perp}\parallel v_{i^{\prime}}^{\perp}\) moreover,

\[v_{i}^{\perp}=\sqrt{1-\|u_{i}\|^{2}}v^{\perp}\quad\text{and}\quad v_{i^{\prime }}^{\perp}=-\sqrt{1-\|u_{i^{\prime}}\|^{2}}v^{\perp}\] (38)

up to a sign flip. However, it is not possible that the three vectors all have pairwise flipped directions to each other as we would have \((-)\cdot(-)=(+)\). If the optimal solution of the constrained optimization problem verifies \(\|u_{i}\|^{2}<1\), then we conclude that it cannot be mapped back to the weight space; in this case, the optimal \(L_{\text{proj}}\) would only give a lower bound on the optimal loss of the weight-space.

Copy-Average Critical Points

**Theorem E.1**.: _Assume that \(\sigma\) is the erf activation function. We pick a copy-average parameter vector_

\[\theta^{*}=(w_{1}^{*},a_{1}^{*})\oplus...\oplus(w_{n}^{*},a_{n}^{*})\] (39)

_where \((w_{i}^{*},a_{i}^{*})\) is a non-trivial critical point when learning from a unit-orthonormal teacher \(f_{i}^{*}\) with the incoming vectors \(v_{s_{i-1}+1},...,v_{s_{i}}\) shown in Eq. 11. Then \(\theta^{*}\) is a critical point of the loss function \(L^{n,k}\) where the target function is \(f^{*}(x)=\sum_{j=1}^{k}\sigma(v_{j}\cdot x)\)._

Proof.: Let us write down the partial derivatives with respect to the outgoing weights and incoming vectors

\[\frac{d}{da_{i}}L^{n,k}(\theta^{*})=2\mathbb{E}_{x\sim\mathcal{D}}[\sigma(w_{ i}^{*}\cdot x)(\sum_{j=1}^{n}a_{j}^{*}\sigma(w_{j}^{*}\cdot x)-f^{*}(x))],\]

\[\frac{d}{dw_{i}}L^{n,k}(\theta^{*})=2a_{i}^{*}\mathbb{E}_{x\sim\mathcal{D}}[ \sigma^{\prime}(w_{i}^{*}\cdot x)x(\sum_{j=1}^{n}a_{j}^{*}\sigma(w_{j}^{*} \cdot x)-f^{*}(x))].\] (40)

We will show that they are equivalent to the following

\[\frac{d}{da_{i}}L^{n,k}(\theta^{*})=2\mathbb{E}_{x\sim\mathcal{D} }[\sigma(w_{i}^{*}\cdot x)(a_{i}^{*}\sigma(w_{i}^{*}\cdot x)-f_{i}^{*}(x))],\] \[\frac{d}{dw_{i}}L^{n,k}(\theta^{*})=2a_{i}^{*}\mathbb{E}_{x\sim \mathcal{D}}[\sigma^{\prime}(w_{i}^{*}\cdot x)x(a_{i}^{*}\sigma(w_{i}^{*} \cdot x)-f_{i}^{*}(x))],\] (41)

which implies that the partial derivatives are zero, since \((w_{i}^{*},a_{i}^{*})\) is a critical point of the loss

\[L^{1,\ell_{i}}=\mathbb{E}_{x\sim\mathcal{D}}[(a\sigma(w\cdot x)-f_{i}^{*}(x) )^{2}].\] (42)

Since \((w_{i}^{*},a_{i}^{*})\) is the optimal solution of the teacher network generated by \(v_{s_{i-1}+1},...,v_{s_{i}}\), from Theorem 5.1, we have that \(w_{i}^{*}\) is in the span of \(v_{s_{i-1}+1},...,v_{s_{i}}\). We have that

\[w_{i}^{*}\cdot w_{i^{\prime}}^{*}=0\quad\text{and}\quad w_{i}^{*}\cdot v_{j}= 0\quad\text{for}\quad j\in[k]\setminus[s_{i-1}+1,s_{i}]\] (43)

since the two incoming vectors are in the span of two orthogonal subspaces respectively and \(w_{i}^{*}\) is orthogonal to all other teacher incoming vectors that are outside of the span of \(v_{s_{i-1}+1},...,v_{s_{i}}\). For two orthogonal vectors say \(w_{i}^{*}\) and \(v\), we have that

\[\mathbb{E}_{x\sim\mathcal{D}}[\sigma_{1}(w_{i}^{*}\cdot x)\sigma_{2}(v\cdot x) ]=\mathbb{E}_{x\sim\mathcal{D}}[\sigma_{1}(w_{i}^{*}\cdot x)]\mathbb{E}_{x \sim\mathcal{D}}[\sigma_{2}(v\cdot x)]\] (44)

which is zero if at least one of \(\sigma_{1}\) and \(\sigma_{2}\) is odd. This implies the first equation in 41 for the partial derivatives with respect to the outgoing weights. In order to show the second equation for the partial derivatives with respect to the incoming vectors, let us define \(S_{j}=[s_{j-1}+1,s_{j}]\),

\[W_{i,j}=\mathbb{E}_{x\sim\mathcal{D}}[\sigma^{\prime}(w_{i}^{*}\cdot x)x(a_{j }^{*}\sigma(w_{j}^{*}\cdot x)-\sum_{k\in S_{j}}\sigma(v_{k}\cdot x))],\] (45)

and note that it suffices to show that \(W_{i,j}=0\) for all \(i\neq j\). This is true if and only if \(W_{i,j}\cdot\bar{v}_{\ell}=0\) where \(\{\bar{v}_{1},...,\bar{v}_{d}\}\) form an orthogonal basis of \(\mathbb{R}^{d}\). Let us choose \(\bar{v}_{1}=v_{1},...,\bar{v}_{k}=v_{k}\) and that \(\bar{v}_{k+1},...,\bar{v}_{d}\) completes the basis if \(d>k\). One can observe that \(W_{i,j}\cdot\bar{v}_{\ell}=0\) for \(k+1\leq\ell\leq d\) since \(x\cdot\bar{v}_{\ell}\) is an independent Gaussian from the others. Hence, the expectation, i.e. \(W_{i,j}\cdot\bar{v}_{\ell}\), factorizes with a factor of \(\mathbb{E}[x\cdot\bar{v}_{\ell}]\) which is zero.

It remains to check \(W_{i,j}\cdot v_{l}=0\) for all \(v_{l}\)'s. We split the analysis into two cases. If \(v_{l}\notin S_{j}\), then \(x\cdot v_{l}\) is independent from \(x\cdot w_{j}^{*}\) and from \(x\cdot v_{k}\) for \(k\in S_{j}\). Hence \(W_{i,j}\) splits into

\[W_{i,j}\cdot v_{l}=\mathbb{E}_{x\sim\mathcal{D}}[\sigma^{\prime}(w_{i}^{*} \cdot x)x\cdot v_{l}]\mathbb{E}_{x\sim\mathcal{D}}[(a_{j}^{*}\sigma(w_{j}^{*} \cdot x)-\sum_{k\in S_{j}}\sigma(v_{k}\cdot x))]=0,\] (46)where the second term in the product is zero because \(w_{i}^{*}\cdot x\) and \(v_{k}\cdot x\) are centered Gaussian and \(\sigma\) is odd. For the second case, where \(v_{l}\in S_{j}\), using the fact that \(x\cdot v_{l}\) is independent from \(w_{i}^{*}\cdot x\) and from \(x\cdot v_{k}\) for \(l\neq k\), we have

\[W_{i,j}\cdot v_{l} =\mathbb{E}_{x\sim\mathcal{D}}[\sigma^{\prime}(w_{i}^{*}\cdot x) ]\mathbb{E}_{x\sim\mathcal{D}}[x\cdot v_{l}(a_{j}^{*}\sigma(w_{j}^{*}\cdot x)- \sum_{k\in S_{j}}\sigma(v_{k}\cdot x))]\] (47) \[=\mathbb{E}_{x\sim\mathcal{D}}[\sigma^{\prime}(w_{i}^{*}\cdot x) ]\mathbb{E}_{x\sim\mathcal{D}}[x\cdot v_{l}(a_{j}^{*}\sigma(w_{j}^{*}\cdot x)- \sigma(v_{l}\cdot x))]\] (48) \[=\mathbb{E}_{x\sim\mathcal{D}}[\sigma^{\prime}(w_{i}^{*}\cdot x) ]\Big{(}\mathbb{E}_{x\sim\mathcal{D}}[a_{j}^{*}\sigma(w_{j}^{*}\cdot x)x \cdot v_{l}]-\mathbb{E}_{x\sim\mathcal{D}}[\sigma(v_{l}\cdot x)x\cdot v_{l}] \Big{)}.\] (49)

Applying Stein's Lemma to both terms on the right we have

\[W_{i,j}\cdot v_{l}=\mathbb{E}_{x\sim\mathcal{D}}[\sigma^{\prime}(w_{i}^{*} \cdot x)]\Big{(}\mathbb{E}[a_{j}^{*}r_{j}^{*}u^{*}\sigma^{\prime}(r_{j}^{*}Z) ]-\mathbb{E}[\sigma^{\prime}(Z)]\Big{)},\] (50)

where \(Z\) is standard Gaussian and \(r_{j}^{*}=\sqrt{\frac{1}{2k-1}},u^{*}=\sqrt{\frac{1}{k}}\), \(a_{j}^{*}=k\) (parameters of erf). Hence we want to show that

\[a_{j}^{*}r_{j}^{*}u^{*}\mathbb{E}[\sigma^{\prime}(r_{j}^{*}Z)]=\mathbb{E}[ \sigma^{\prime}(Z)].\] (51)

To show this, we use the following relation [2, 3]

\[g_{\text{ef}}(r,r,u)=\mathbb{E}[\sigma(rx)\sigma(ry)]=\frac{2}{\pi}\arcsin \left(\frac{r^{2}u}{r^{2}+1}\right).\] (52)

Differentiating with respect to the correlation \(u\) we have

\[\frac{d}{du}g_{\text{ef}}(r,r,u)=r^{2}\mathbb{E}[\sigma^{\prime}(rx)\sigma^{ \prime}(ry)]=\frac{2}{\pi}\frac{1}{\sqrt{1-u^{2}\left(\frac{r^{2}}{r^{2}+1} \right)^{2}}}\frac{r^{2}}{(r^{2}+1)}.\] (53)

In particular, at correlation zero, we get

\[\mathbb{E}[\sigma^{\prime}(rx)]=\sqrt{\frac{2}{\pi}\frac{1}{(r^{2}+1)}}\] (54)

Therefore, we have

\[\mathbb{E}[\sigma^{\prime}(r_{j}^{*}x)]=\sqrt{\frac{2}{\pi}\left(\frac{2k-1}{ 2k}\right)},\quad\mathbb{E}[\sigma^{\prime}(x)]=\sqrt{\frac{1}{\pi}},\] (55)

which implies 51 and the proof is complete. 

For general activation functions, using the substitution in Eq.41, the first partial derivatives in Eq. 40 reduce to

\[\frac{d}{da_{i}}L^{n,k}(\theta^{*})=2\sum_{i\neq i^{\prime}}a_{i^{\prime}}^{* }g_{\sigma}(\|w_{i}^{*}\|,\|w_{i^{\prime}}^{*}\|,0)-2(k-\ell_{i})g_{\sigma}(\| w_{i}^{*}\|,1,0)\] (56)

which is in general non-zero if \(\sigma\) is not odd.

**Lemma E.2**.: _Assume \(\ell_{2}>\ell_{1}\geq 1\). We have that_

\[L_{\text{ef}}^{*}(\ell_{2}+1)-L_{\text{ef}}^{*}(\ell_{2})<L_{\text{ef}}^{*}( \ell_{1}+1)-L_{\text{ef}}^{*}(\ell_{1}).\] (57)

Proof.: We first show that the function \(x^{2}\arcsin(\frac{1}{2x})\) is increasing for \(x\geq 1\) and convex for \(x>0\). Using the Taylor expansion of \(\arcsin\), we have that

\[f(x)=x^{2}\arcsin\bigl{(}\frac{1}{2x}\bigr{)}=\frac{x}{2}+\frac{1}{2\cdot 3} \frac{1}{2^{3}x}+\frac{1\cdot 3}{2\cdot 4\cdot 5}\frac{1}{2^{5}x^{3}}+...\] (58)

where the higher-order terms all have positive coefficients. The first derivative is

\[f^{\prime}(x)=\frac{1}{2}-\frac{1}{2\cdot 3}\frac{1}{2^{3}x^{2}}-\frac{1\cdot 3 \cdot 3}{2\cdot 4\cdot 5}\frac{1}{2^{5}x^{4}}+...\] (59)which is positive for \(x\geq 1\) since we have

\[\frac{1}{2\cdot 3}\frac{1}{2^{3}x^{2}}+\frac{1\cdot 3\cdot 3}{2\cdot 4\cdot 5}\frac{ 1}{2^{5}x^{4}}+...<\frac{1}{2^{2}x^{2}}+\frac{1}{2^{4}x^{4}}+...\leq\frac{1}{4 }+\frac{1}{4^{2}}+\frac{1}{4^{3}}+...=\frac{1}{3}.\] (60)

The second derivative is

\[f^{\prime\prime}(x)=2\frac{1}{2\cdot 3}\frac{1}{2^{3}x^{3}}+4\frac{1\cdot 3 \cdot 3}{2\cdot 4\cdot 5}\frac{1}{2^{5}x^{3}}+...\] (61)

which is positive for positive \(x\).

First, let us show that Eq. 18 holds. Plugging in the analytic expressions for \(L^{*}_{\text{erf}}(0,\cdot)\) and \(L^{*}_{\text{erf}}(\cdot)\), it is equivalent to

\[\ell_{0}\frac{2}{\pi}\arcsin\bigl{(}\frac{1}{2}\bigr{)}>\ell_{0} \frac{2}{\pi}\arcsin\bigl{(}\frac{1}{2}\bigr{)}-\frac{2}{\pi}\Bigl{(}(\ell_{1} \!+\!\ell_{0})^{2}\arcsin\bigl{(}\frac{1}{2(\ell_{1}\!+\!\ell_{0})}\bigr{)}- \ell_{1}^{2}\arcsin\bigl{(}\frac{1}{2\ell_{1}}\bigr{)}\Bigr{)}.\] (62)

Since \(f(x)\) is increasing for \(x\geq 1\), the second term inside the parenthesis is positive, hence the inequality holds.

We will now prove the statement of the Lemma. It suffices to show the following for all \(\ell=\ell_{2}\geq 2\)

\[L^{*}_{\text{erf}}(\ell+1)-L^{*}_{\text{erf}}(\ell)<L^{*}_{\text{ erf}}(\ell)-L^{*}_{\text{erf}}(\ell-1)\] (63)

since then we can continue to decrease \(\ell\) by one, i.e. \(\ell-1,\ell-2,...\), until we reach \(\ell_{1}\). Eq. 63 is equivalent to

\[L^{*}_{\text{erf}}(\ell+1)-2L^{*}_{\text{erf}}(\ell)+L^{*}_{\text{erf}}(\ell-1 )<0,\] (64)

that is the second-order finite difference, similar to the second-derivative of a continuous function. The proof is completed by observing that \(L^{*}_{\text{erf}}(\cdot)\) is a discrete-concave function since its continuous interpolation

\[L^{*}_{\text{erf}}(x)=x\arcsin(\frac{1}{2})-x^{2}\arcsin(\frac{1}{2x})\] (65)

is concave for \(x>0\) since it can be written as \(L^{*}_{\text{erf}}(x)=\alpha x-f(x)\) where \(f\) is convex for \(x>0\). 

Lemma E.2 tells us that if we add one neuron to the teacher, then it is better to approximate it by the student neuron that already approximates many teacher neurons. Applying Lemma E.2 iteratively, we get

\[L^{*}_{\text{erf}}(k\!-\!1)+L^{*}_{\text{erf}}(1)<L^{*}_{\text{ erf}}(k\!-\!2)+L^{*}_{\text{erf}}(2)<...<L^{*}_{\text{erf}}(\ell_{2}\!+\!1)+L^{*}_{ \text{erf}}(\ell_{1})<L^{*}_{\text{erf}}(\ell_{2})+L^{*}_{\text{erf}}(\ell_{1} \!+\!1)\]

where \(\ell_{2}\!=\!\frac{k}{2},\ell_{1}\!=\!\frac{k}{2}-1\) if \(k\) is even and \(\ell_{2}\!=\!\frac{k+1}{2},\ell_{1}\!=\!\frac{k-3}{2}\) if \(k\) is odd.

**Theorem E.3**.: _Consider a unit-orthonormal teacher network \(f^{*}(x)=\sum_{j=1}^{k}\sigma(v_{j}\cdot x)\) and the erf activation function. For an under-parameterized student network with \(n\) neurons, the minimum-loss copy-average configuration up to permutations (of the student and teacher neurons) is_

\[\theta=(\epsilon_{1}v_{1},\epsilon_{1})\oplus...\oplus(\epsilon_{n-1}v_{n-1}, \epsilon_{n-1})\oplus(\epsilon_{n}w^{*}_{n},\epsilon_{n}a^{*}_{n})\] (66)

_where \(\epsilon_{i}\in\{\pm 1\}\) and \((w^{*}_{n},a^{*}_{n})\) is given by Corollary 5.2 after substituting \(k\) with \(k-n+1\)._

Proof.: We will conclude with a simple argument that the minimum-loss CA configuration for a multi-neuron network with \(n\) neurons is \((n\!-\!1)\)-C-1-A. In particular, if there are two averaging neurons inside the student network, we can redistribute the teacher neurons shared between these two to a lower-loss CA configuration by ensuring that one student neuron copies and the other student neuron averages (see Lemma E.2). The minimum-loss CA point is then achieved among CA configurations where at least \(n\!-\!1\) neurons each copy a single teacher neuron (of the \(k\) possible ones). The remaining student neuron can be treated as a single-neuron network learning from a teacher with \(k\!-\!n\!+\!1\) neurons - for which we know the optimal solution is to average (Theorem 5.1).

### Number of CA Critical Points

There is a combinatorial number of \((\ell_{1},...,\ell_{n})\)-CAC critical points, that is

\[c(\ell_{1},...,\ell_{n})\binom{k}{\ell_{1}}...\binom{k-(\ell_{1}+...+\ell_{n})}{ \ell_{n}}\] (67)

where \(c_{n}\!:=\!c(\ell_{1},...,\ell_{n})\) counts distinguishable permutations between the neurons of the student network, and the binomial coefficients stand for grouping teacher neurons into \(n\) non-empty buckets.

If \(\ell_{1}=...=\ell_{n}\), permutation between the student neurons is already counted when distributing the teacher neurons, hence \(c_{n}=1\). If all \(\ell_{1},...,\ell_{n}\) are distinct from each other, we have that \(c_{n}=n!\) since we swap all pairs of student neurons after assigning groups of teacher neurons. In general, let \(c_{i}\) denote the number of \(i\)'s among \(\ell_{1},...,\ell_{n}\) for all \(i=1,..,k\); the formula for the permutation-factor is given by

\[c(\ell_{1},...,\ell_{n})=\frac{n!}{c_{1}!...c_{k}!}.\] (68)

## Appendix F General Properties of the Interaction Function

In this Section, we introduce some general properties of the interactions. We use these only for the one-neuron network in this paper (see Section G), however, these properties are likely to play a role in studying the networks with two or more neurons.

We first present the partial derivative of a general interaction function, i.e. two activation functions may be different, for example, if the student activation function does not match the teacher, with respect to the correlation in a simple expression in Lemma F.1. In the second part, we present a property of the activation function sufficient for Assumption 3.1 (ii), and show that the differentiable activation functions studied in this paper satisfy this property in Lemma F.2.

**Lemma F.1**.: _Assume that functions \(\sigma_{1}\) and \(\sigma_{2}\) are differentiable. The partial derivative of the following Gaussian integral term \(\mathbb{E}[\sigma_{1}(r_{1}x)\sigma_{2}(r_{2}y)]\) with respect to the correlation \(\mathbb{E}[xy]=u\) is_

\[\frac{d}{du}\mathbb{E}[\sigma_{1}(r_{1}x)\sigma_{2}(r_{2}y)]=r_{1}r_{2} \mathbb{E}[\sigma_{1}^{\prime}(r_{1}x)\sigma_{2}^{\prime}(r_{2}y)].\] (69)

We apply the Lemma for \(\sigma_{1}=\sigma_{2}=\sigma\) in the main text in Eq. 9.

Proof.: We compute the derivative of \(\mathbb{E}[\sigma_{1}(r_{1}x)\sigma_{2}(r_{2}y)]\) by making the correlation \(u\) explicit. Denote \(u^{\prime}=\sqrt{1-u^{2}}\) and \(y=ux+u^{\prime}z\). After the computation, we use Stein's lemma to reach the desired formula.

\[\partial_{u}\mathbb{E}[\sigma_{1}(r_{1}x)\sigma_{2}(r_{2}y)]=r_{2}\mathbb{E}[ \sigma_{1}(r_{1}x)\sigma_{2}^{\prime}(r_{2}y)x]-\frac{r_{2}u}{u^{\prime}} \mathbb{E}[\sigma_{1}(r_{1}x)\sigma_{2}^{\prime}(r_{2}y)z]\] (70)

where \(x\) and \(z\) are independent standard Gaussians. Here is a reminder for Stein's Lemma for a standard Gaussian \(z\)

\[\mathbb{E}[v(z)z]=\mathbb{E}[v^{\prime}(z)].\] (71)

To remove \(x\) in the first term, we apply Stein's formula for \(v(x)=\sigma_{1}(r_{1}x)\sigma_{2}^{\prime}(r_{2}(ux+u^{\prime}z))\) yielding

\[r_{1}r_{2}\mathbb{E}[\sigma_{1}^{\prime}(r_{1}x)\sigma_{2}^{\prime}(r_{2}y)]+ r_{2}^{2}u\mathbb{E}[\sigma_{1}(r_{1}x)\sigma_{2}^{\prime\prime}(r_{2}y)].\] (72)

To remove \(z\) in the second term, we apply Stein's formula for \(v(z)=\sigma_{2}^{\prime}(r_{2}(ux+u^{\prime}z))\) by considering fixed \(x\) which yields

\[-r_{2}^{2}u\mathbb{E}[\sigma_{1}(r_{1}x)\sigma_{2}^{\prime\prime}(r_{2}y)].\] (73)

Summing up the two terms completes the proof. 

For softplus that is increasing and convex, using Lemma F.1 for \(\sigma_{1}\!=\!\sigma_{2}\!=\!\sigma\) twice, we infer that the interaction \(g\) is also increasing and convex in \(u\). Hence, for \(u<0\), Assumption 3.1 (ii) holds for softplus. However, for the other activation functions, using second-order derivatives does not suffice to show the assumption. We will propose a new property of the activation function that implies that the interaction satisfies Assumption 3.1 (ii) and prove that softplus with \(\beta\leq 2\), sigmoid, tanh, and erf satisfy this property.

**Lemma F.2**.: _If the activation function \(\sigma\) is thrice-differentiable and it satisfies_

\[\sigma^{\prime}(x)-x\sigma^{\prime\prime}(x)+\sigma^{\prime\prime\prime}(x)>0,\] (74)

_then its interaction satisfies Assumption 3.1 (ii) for all \(u\in(-1,1)\). Softplus with \(\beta\in(0,2]\), sigmoid, tanh, and erf activation functions satisfy the above inequality._

Proof.: Let us first write out Assumption 3.1 (ii) explicitly using Lemma F.1

\[r_{1}u\mathbb{E}[\bar{\sigma}^{\prime}(r_{1}x)\bar{\sigma}^{\prime}(y)]< \mathbb{E}[\bar{\sigma}(r_{1}x)\bar{\sigma}(y)].\] (75)

where \(\bar{\sigma}(x)=\sigma^{\prime}(x)\). Using Stein's Lemma for \(v(x)=\bar{\sigma}(r_{1}x)\bar{\sigma}^{\prime}(y)\), we get

\[\mathbb{E}[\bar{\sigma}(r_{1}x)\bar{\sigma}^{\prime}(y)x]=\mathbb{E}[\bar{ \sigma}^{\prime}(r_{1}x)\bar{\sigma}^{\prime}(y)]r_{1}+\mathbb{E}[\bar{ \sigma}(r_{1}x)\bar{\sigma}^{\prime\prime}(y)]u.\] (76)

The desired inequality is equivalent to

\[\mathbb{E}[\bar{\sigma}(r_{1}x)(\bar{\sigma}(y)-\bar{\sigma}^{\prime}(y)xu+ \bar{\sigma}^{\prime\prime}(y)u^{2})]>0.\] (77)

Let us introduce \(f(x)=\bar{\sigma}(x)-x\bar{\sigma}^{\prime}(x)+\bar{\sigma}^{\prime\prime}(x)\). For \(y=ux+u^{\prime}z\) where \(u^{\prime}=\sqrt{1-u^{2}}\), we have the conditional average of \(y\) fixing \(x\) (we drop conditioning on the right-hand terms for convenience)

\[\mathbb{E}[f(y)|x] =\mathbb{E}[\bar{\sigma}(y)]-\mathbb{E}[y\bar{\sigma}^{\prime}(y) ]+\mathbb{E}[\bar{\sigma}^{\prime\prime}(y)]\] \[=\mathbb{E}[\bar{\sigma}(y)]-ux\mathbb{E}[\bar{\sigma}^{\prime}( y)]-\mathbb{E}[u^{\prime}z\bar{\sigma}^{\prime}(y)]+\mathbb{E}[\bar{\sigma}^{ \prime\prime}(y)]\] \[=\mathbb{E}[\bar{\sigma}(y)]-ux\mathbb{E}[\bar{\sigma}^{\prime}( y)]-(u^{\prime})^{2}\mathbb{E}[\bar{\sigma}^{\prime\prime}(y)]+\mathbb{E}[\bar{ \sigma}^{\prime\prime}(y)]\] \[=\mathbb{E}[\bar{\sigma}(y)]-ux\mathbb{E}[\bar{\sigma}^{\prime}( y)]+u^{2}\mathbb{E}[\bar{\sigma}^{\prime\prime}(y)],\] (78)

where second last equality comes from Stein's Lemma for \(v(z)=\bar{\sigma}^{\prime}(ux+u^{\prime}z)\). Hence the desired inequality is equivalent to

\[\mathbb{E}[\bar{\sigma}(r_{1}x)f(y)]>0.\] (79)

By straightforward calculus, we will show that \(f(x)>0\), or that \(f(x)\geq 0\) and \(f(x)=0\) if and only if \(x=0\). In the latter case, the expectation in Eq. 79 is positive since \(f(y)>0\) for some \(y\) values of the integrand. First, for the sigmoid and tanh activation functions, for which we have

\[\bar{\sigma}(x)=\frac{e^{x}}{(e^{x}+1)^{2}},\;\bar{\sigma}^{\prime}(x)=\frac{e ^{x}(1-e^{x})}{(e^{x}+1)^{3}},\;\bar{\sigma}^{\prime\prime}(x)=\frac{e^{x}(e^ {2x}-4e^{x}+1)}{(e^{x}+1)^{4}}.\] (80)

Hence, we can explicitly write \(f\) as

\[f(x) =\frac{e^{x}}{(e^{x}+1)^{2}}-x\frac{e^{x}(1-e^{x})}{(e^{x}+1)^{3 }}+\frac{e^{x}(e^{2x}-4e^{x}+1)}{(e^{x}+1)^{4}}\] (81) \[=\frac{e^{x}}{(e^{x}+1)^{4}}((e^{x}+1)^{2}-x(1-e^{x})(e^{x}+1)+( e^{2x}-4e^{x}+1)).\] (82)

Therefore showing \(f(x)>0\) is equivalent to showing that the factor on the right, that is,

\[2e^{x}(e^{x}-1)+2-x(1-e^{2x})\] (83)

is positive. For \(x<0\), we have \(e^{x}<1\) which implies \(-x(1-e^{2x})>0\) and \((1-e^{x})e^{x}\leq 1/4\) due to the inequality of arithmetic and geometric means hence the first term is upper bounded by \(-1/2\) and since we have \(+2\), the whole term is positive. For \(x\geq 0\), we have \(e^{x}\geq 1\), hence we can rewrite the inequality as a sum of non-negative terms

\[2e^{x}(e^{x}-1)+2+x(e^{2x}-1)>0.\] (84)

Let us now handle the case of erf. Its first three derivatives are given by

\[\bar{\sigma}(x)=\frac{2}{\sqrt{\pi}}e^{-x^{2}/2},\bar{\sigma}^{\prime}(x)=- \frac{2}{\sqrt{\pi}}xe^{-x^{2}/2},\bar{\sigma}^{\prime\prime}(x)=\frac{2}{ \sqrt{\pi}}(x^{2}e^{-x^{2}/2}-e^{-x^{2}/2})\] (85)

Hence, we can explicitly write \(f\) as

\[f(x)=\frac{2}{\sqrt{\pi}}e^{-x^{2}/2}(1+xx+x^{2}-1)=\frac{4}{\sqrt{\pi}}e^{-x^ {2}/2}x^{2}\] (86)that is non-negative for all \(x\) and zero iff \(x=0\).

Finally, for the softplus activation function with \(\beta\in(0,2]\), we have the following derivatives

\[\bar{\sigma}(x)=\frac{e^{\beta x}}{(e^{\beta x}+1)},\;\bar{\sigma}^{\prime}(x)= \frac{\beta e^{\beta x}}{(e^{\beta x}+1)^{2}},\;\bar{\sigma}^{\prime\prime}(x)= \frac{\beta^{2}e^{\beta x}(1-e^{\beta x})}{(e^{\beta x}+1)^{3}}.\] (87)

Plugging in the function \(f\), we get

\[f(x) =\frac{e^{\beta x}}{(e^{\beta x}+1)}-x\frac{\beta e^{\beta x}}{(e ^{\beta x}+1)^{2}}+\frac{\beta^{2}e^{\beta x}(1-e^{\beta x})}{(e^{\beta x}+1)^ {3}}\] (88) \[=\frac{e^{\beta x}}{(e^{\beta x}+1)^{3}}((e^{\beta x}+1)^{2}-x \beta(e^{\beta x}+1)+\beta^{2}(1-e^{\beta x}))\] (89)

Therefore showing \(f(x)>0\) is equivalent to showing that the factor on the right, that is,

\[e^{2\beta x}+e^{\beta x}(2-x\beta-\beta^{2})+1-x\beta+\beta^{2}\] (90)

is positive. For \(x\leq 0\), we have that \(-x\beta>0\) and \(2-\beta^{2}\geq-2\) since \(\beta\leq 2\), hence it is sufficient to show that the following is positive

\[e^{2\beta x}-2e^{\beta x}+1+\beta^{2}=(e^{\beta x}-1)^{2}+\beta^{2}\] (91)

which is a sum of squares. For \(x>0\), in the rest of the proof we will show that

\[e^{\beta x}(e^{\beta x}+2-x\beta-\beta^{2})+1-x\beta+\beta^{2}>0,\] (92)

for \(\beta\in(0,2]\). Using \(e^{\beta x}\geq(\beta x)^{2}/2+\beta x+1\), it suffices to show that

\[e^{\beta x}((\beta x)^{2}/2+3-\beta^{2})+1-x\beta+\beta^{2}>0.\] (93)

If \((\beta x)^{2}/2+3-\beta^{2}\geq 1\), then the first term is bigger than \(\beta x+1\) hence the above term is positive. The remaining possibility is that we have

\[\frac{x^{2}}{2}<1-\frac{2}{\beta^{2}}.\] (94)

\(\beta\leq 2\) implies \(x<1\) and \(x^{2}>0\) implies \(\beta>\sqrt{2}\). Hence we have \(-x\beta+\beta^{2}>0\) since \(\beta>x\). Therefore, if we have \((\beta x)^{2}/2+3-\beta^{2}\geq 0\), Eq. 92 is positive. Assuming the opposite, we get

\[\frac{x^{2}}{2}<1-\frac{3}{\beta^{2}},\] (95)

\(\beta\leq 2\) implies \(x<1/\sqrt{2}\) and \(x^{2}>0\) implies \(\beta>\sqrt{3}\).

Going back to Eq. 92, what remains to show is that it is positive in the domain \(x<1/\sqrt{2}\), \(\beta\in(\sqrt{3},2]\). It suffices to show that \(e^{\beta x}+2-x\beta-\beta^{2}>0\). Assuming the contrary implies \(e^{\beta x}<x\beta+2\) since \(\beta\leq 2\). We can then deduce that \(x\beta<c=1.2\) since otherwise we would have

\[e^{\beta x} =1+\beta x+\frac{(\beta x)^{2}}{2!}+\frac{(\beta x)^{3}}{3!}+...\] (96) \[\geq 1+\beta x+\frac{c^{2}}{2!}+\frac{c^{3}}{3!}+...=1+\beta x+(e^ {c}-c-1)>1+\beta x+1\] (97)

which implies a contradiction. \(c\) can be chosen smaller but this will be enough for our purposes.

Assuming \(e^{\beta x}+2-x\beta-\beta^{2}\leq 0\), let us expand Eq. 92

\[e^{\beta x}(e^{\beta x}+2-x\beta-\beta^{2})+1-x\beta+\beta^{2} \geq\;(\text{using }e^{\beta x}<\beta x+2)\] (98) \[(x\beta+2)e^{\beta x}+(x\beta+2)(2-x\beta-\beta^{2})+1-x\beta+ \beta^{2}=\] (99) \[(x\beta+2)e^{\beta x}-(x\beta)^{2}-(1+\beta^{2})x\beta+5-\beta^{ 2}>\;(\text{using }e^{\beta x}>\beta x+1)\] (100) \[7-\beta^{2}+(2-\beta^{2})x\beta\geq 3-2x\beta>0\] (101)

where in the last inequality we used \(x\beta<1.2\). We note that this inequality holds for slightly larger \(\beta\) using the same technique, however, for significantly larger \(\beta\), the property breaks down.

The One-Neuron Network

We study the critical points of the following loss function

\[L^{1,k}(w,a)=\mathbb{E}_{x\sim\mathcal{D}}[a\sigma(w\cdot x)-\sum_{j=1}^{k}b_{j} \sigma(v_{j}\cdot x)],\] (102)

in particular, the optimal solution. For \(n=1\), all configurations of order parameters are realizable in the weight space, therefore, the optimal solution of the following loss (repeating Eq. 14)

\[L^{1,k}_{\text{proj}}=a^{2}g_{\sigma}(r,r,1)-2a\sum_{j=1}^{k}b_{j}g_{\sigma}(r, \|v_{j}\|,u_{j})+\text{const},\quad\text{subject to}\,\sum_{j=1}^{k}u_{j}^{2} \leq 1,r\geq 0,\] (103)

is equivalent to the optimal solution in the weight space. Let us denote the unit ball by \(B=\{(u_{1},...,u_{k})\mid u_{1}^{2}+...+u_{k}^{2}\leq 1\}\). Its interior is denoted by \(\operatorname{int}B\) and its boundary is denoted by \(\partial B\).

We will present the results for the one-neuron network in five parts

1. In Subsection G.1, we give a proof of Proposition 4.1: any non-trivial critical point \((w,a)\) of \(L^{1,k}\) satisfies that \(w\) is in the span of the teacher's incoming vectors if the activation function satisfies Assumption 3.1 (i). Moreover, we show in Lemma G.2 that the corresponding order parameters should satisfy a Lagrangian condition (Eq. 105).
2. In Subsection G.2, we characterize the topology of the loss landscape in terms of its critical points for the activation functions studied in this paper and for the unit-orthonormal teacher. Our results for the one-neuron network are strong in the sense that it gives all possible critical points of the loss landscape. 1. In Subsection G.2.1, we give a proof of Theorem 5.1: for general activation functions satisfying Assumption 3.1, any non-trivial critical point of the one-neuron network attains equal correlations that are either \(\nicefrac{{1}}{{\sqrt{k}}}\) or \(\nicefrac{{-1}}{{\sqrt{k}}}\). 2. In Subsection G.2.2, we study the two-dimensional loss obtained after applying Theorem 5.1. From its derivative constraints, we get a fixed point equation (Eq. 120) that needs to be satisfied by the incoming vector norm \(r\) at any non-trivial critical point with equal correlations \(u\). Numerically, we show that there is a unique solution of the fixed point equation for \(u>0\) for differentiable activation functions studied in this paper (Fig. 10). Finally, we give some sufficient conditions in Eq. 121 to prove uniqueness based on log-concavity; numerically, these are shown to be satisfied by softplus and sigmoid activation functions.
3. In Subsection G.3, we give a proof of Corollary 5.2 by solving the two-dimensional loss for the erf activation function. Moreover, from the proof, we conclude that there are exactly two non-trivial critical points identical up to the mirror symmetry of erf (since it is odd); and these are the optimal solutions for the loss landscape.
4. In Subsection G.4, we present and prove Corollary G.5 by solving the two-dimensional loss for the ReLU activation function. We find that there are two non-trivial critical points of the loss function: a saddle 'point' at correlation \(\nicefrac{{-1}}{{\sqrt{k}}}\) and the optimal'solution' at correlation \(\nicefrac{{1}}{{\sqrt{k}}}\). Due to the positive homogeneity of ReLU, these are not two points but two equal-loss hyperbolas in the loss landscape.
5. In Subsection G.5, we study the two-dimensional loss for the softplus activation function and give a proof of Theorem 5.3. Absence of analytical expression for the Gaussian integral terms make the problem challenging; we use several non-trivial steps in the proof. The proof shows that there is no critical point at correlation \(\nicefrac{{-1}}{{\sqrt{k}}}\); and a non-trivial critical point \((w^{*},a^{*})\) at correlation \(\nicefrac{{1}}{{\sqrt{k}}}\) satisfies the following bounds: \(a^{*}\geq k\) and \(\|w^{*}\|\leq\nicefrac{{1}}{{\sqrt{k}}}\). Numerically, we find that these bounds hold for other activation functions studied in this paper (tanh and sigmoid; Fig. 5).

### Any Non-Trivial Critical Point Satisfies the Lagrangian Condition

We add a reminder here for the definition of the non-trivial critical point \(\theta=(w,a)\): it is a critical point of the loss function that satisfies \(a\neq 0\) and \(\|w\|\neq 0\).

**Proposition G.1**.: _Assume that \(f^{*}\) is an orthogonal teacher network of width \(k\). If the activation function satisfies Assumption 3.1 (i), any non-trivial critical point \(\theta^{*}=(w^{*},a^{*})\), i.e. \(\nabla L^{1,k}(\theta^{*})=0\), satisfies that \(w^{*}\) is in the span of the teacher's incoming vectors._

Proof.: We will prove by contradiction. Let us assume that \(w\) is outside of the span of the teacher's incoming vectors. We will show that \((w,a)\) is not a critical point for any \(a\neq 0\). Mapping \((w,a)\) to the order parameter space, we get that \((r,u,a)\) where \(u=(u_{1},...,u_{k})\in\mathrm{int}\,B\) which implies \(u_{j}\in(-1,1)\). Since \(u\in\mathrm{int}\,B\) and \(r>0\), we have that \((r,u,a)\) is a critical point of \(L^{1,k}_{\text{proj}}\) since the boundaries are not seen near the neighborhood of this point. Therefore the partial derivatives of \(L^{1,k}_{\text{proj}}\) are all zero including

\[b_{j}\frac{d}{du_{j}}g_{\sigma}(r,\|v_{j}\|,u_{j})=0.\] (104)

From Assumption 3.1 (i), we have that \(\partial_{u}g_{\sigma}(r_{1},r_{2},u)>0\) for \(u\in(-1,1)\) which yields a contradiction. Thus, each critical point of the projected loss is on the boundary, i.e. \(u\in\partial B\), which implies that the incoming vector is in the span of the teacher's incoming vectors. 

We will next show that any non-trivial critical point satisfies a Lagrangian condition since it is on the boundary of a constrained optimization problem.

**Lemma G.2**.: _Let \(\theta=(w,a)\) be a non-trivial critical point of \(L^{1,k}\). Then the corresponding order parameters \(p=(r,u,a)\) satisfy the following Lagrangian condition_

\[b_{j}\partial_{u}g_{\sigma}(r,\|v_{j}\|,u_{j})=\lambda u_{j}\quad\text{for all}\;\;j\in[k].\] (105)

Proof.: We will first show that for any differentiable path \((r,\gamma(t),a)\) on the boundary such that \(\gamma(t)\in\partial B\) for \(t\in(-\epsilon,\epsilon)\) for some \(\epsilon>0\) and \(\gamma(0)=u\), the following holds

\[\frac{d}{dt}L^{1,k}_{\text{proj}}(\gamma(t))\big{|}_{t=0}=\nabla_{u}L^{1,k}_{ \text{proj}}(p)\cdot\gamma^{\prime}(0)=0.\] (106)

Let us assume the contrary. We construct the corresponding following path in the weight space

\[\theta(t)=\left(r\left(\sum_{j=1}^{k}u_{j}(t)v_{j}+v_{\perp}\right),a\right).\] (107)

Thanks to the equivalence of the losses along the path, we have that

\[\frac{d}{dt}L^{1,k}(\theta(t))\big{|}_{t=0}=\frac{d}{dt}L^{1,k}_{\text{proj}}( \gamma(t))\big{|}_{t=0}=0,\] (108)

since \(\theta(0)=\theta\) is a critical point in the weight space. Therefore, Eq. 106 holds for any differentiable path on the boundary and implies that \(\nabla_{u}L(p)\) is orthogonal to all \(\gamma^{\prime}(0)\). The vector that is orthogonal to all \(\gamma^{\prime}(0)\) is the gradient of the surface, that is \(2(u_{1},...,u_{k})\). Hence we get \(\nabla_{u}L(p)\parallel u\) which is written explicitly as the Lagrangian condition in Eq. 105. This is equivalent to setting the partial derivatives of the Lagrangian loss with respect to \(u_{j}\) to zero where the Lagrangian loss is given by

\[\mathcal{L}(p,\lambda)=-2a\sum_{j=1}^{k}b_{j}g_{\sigma}(r,\|v_{j}\|,u_{j})+ \lambda^{\prime}(\sum_{j=1}^{k}u_{j}^{2}-1).\] (109)

We set \(\lambda=\nicefrac{{\lambda^{\prime}}}{{a}}\) in Eq. 105. 

### General Activation Functions

Before we present our results, let us take a detour to check the applicability of the convex optimization framework. For a convex and twice-differentiable activation function such as softplus, applying Lemma F.1 twice implies that the interaction \(g_{\sigma}(r_{1},r_{2},\cdot)\) is a convex function of the correlation \(u\in(-1,1)\) for \(r_{1},r_{2}\!>\!0\). Let us consider a fixed \(a\!<\!0\) and \(r\!>\!0\) and consider the loss parameterized by \(u_{j}\)'s. It is convex since its Hessian is a diagonal matrix with entries

\[\frac{d^{2}}{du_{j}^{2}}L=-2a\frac{d^{2}}{du_{j}^{2}}g_{\sigma}(r,\|v_{j}\|,u_ {j})>0.\] (110)Since the constraint on the correlations (Eq. 14) is also convex, we get a convex optimization problem that has a unique global minimum (see Boyd et al. [44], Section 4.2). Swapping a pair of \(u_{j}\) does not change the loss, thus it is permutation symmetric. If any two \(u_{j}\) were distinct from each other at the minimum, then its permutation would also be a minimum which would violate the unicity. We conclude that at the unique minimum point, the correlations are equal to each other. However, for the case \(a>0\), and for other activation functions, the objective is not convex.

We instead use Lagrange multipliers for proving Theorem 5.1.

#### g.2.1 Proof of Theorem 5.1

**Theorem G.3**.: _Assume that the activation function satisfies Assumption 3.1. At any non-trivial critical point \((w^{*},a^{*})\) of the loss \(L^{1,k}\) for the unit-orthonormal teacher network, the incoming vector satisfies_

\[\frac{w^{*}}{\|w^{*}\|}=u\sum_{j=1}^{k}v_{j}\] (111)

_where \(u\) is either \(\nicefrac{{1}}{{\sqrt{k}}}\) or \(\nicefrac{{-1}}{{\sqrt{k}}}\)._

Proof.: From Proposition 4.1 and Lemma G.2, we get that any non-trivial critical point should satisfy the Lagrangian condition in Eq. 105. In particular for unit-orthonormal teacher, setting \(\|v_{j}\|=1\) and \(b_{j}=1\), we get the following Lagrangian condition

\[\partial_{u}g_{\sigma}(r,1,u_{j})=\lambda u_{j}\ \ \forall j\in[k],\quad \sum_{j=1}^{k}u_{j}^{2}=1.\] (112)

If \(u_{j}=0\), we get \(\partial_{u}g_{\sigma}(r,1,0)=0\) which is not possible since \(g_{\sigma}(r,1,u)\) is increasing due to Assumption 3.1 (i). Hence we have

\[\frac{\partial_{u}g_{\sigma}(r,1,u_{j})}{u_{j}}=\lambda.\] (113)

Let us observe that \(\partial_{u}g_{\sigma}(r,1,u)/u\) is decreasing for \(u\in(-1,1)\setminus\{0\}\) if and only if

\[\frac{d}{du}\left(\frac{1}{u}\frac{d}{du}g_{\sigma}(r,1,u)\right)=\frac{1}{u} \frac{d^{2}}{du^{2}}g_{\sigma}(r,1,u)-\frac{1}{u^{2}}\frac{d}{du}g_{\sigma}(r, 1,u)<0,\] (114)

which is equivalent to Assumption 3.1 (ii) for \(u\in(-1,1)\setminus\{0\}\) (we included \(u=0\) in Assumption 3.1 (ii) for a simpler statement which is already implied from Assumption 3.1 (i) at \(u=0\)).

Taken together, we conclude that \(\partial_{u}g_{\sigma}(r,1,u)/u\) is one-to-one in \(u\in(-1,1)\setminus\{0\}\). We need to consider the remaining case \(u_{i}\!\in\!\{-1,1\}\). For \(k\geq 2\), necessarily, we have \(u_{j}=0\) for \(j\neq i\), which is not possible as we have shown. For \(k=1\), \(u_{i}\in\{-1,1\}\) is the only option that satisfies the boundary condition. For \(k\geq 2\), Eq. 113 implies that all correlations are equal. Combining it with the boundary condition, we get \(u_{1}=...=u_{k}=u\) with \(ku^{2}=1\), which completes the proof. 

#### g.2.2 Two-Dimensional Loss, The Derivative Constraints, Uniqueness

At any non-trivial critical point, we proved in Theorem 5.1 that all correlations are equal and denoted by \(u\) that is either \(1/\sqrt{k}\) or \(-1/\sqrt{k}\). The projected loss at a critical point reduces to

\[L=a^{2}g_{\sigma}(r,r,1)-2kag_{\sigma}(r,1,u)+C.\] (115)

Moreover, at a critical point, the partial derivatives with respect to the outgoing weight and norm should also be zero which gives the following two constraints

\[\partial_{a}L =2ag_{\sigma}(r,r,1)-2kg_{\sigma}(r,1,u)=0,\] \[\partial_{r}L =a^{2}\partial_{r}g_{\sigma}(r,r,1)-2ka\partial_{r}g_{\sigma}(r,1,u)=0,\] (116)which can be rearranged into the following (assuming \(g_{\sigma}(r,r,1)\neq 0\) and \(\partial_{r}g_{\sigma}(r,r,1)\neq 0\))

\[\frac{a}{k}=\frac{g_{\sigma}(r,1,u)}{g_{\sigma}(r,r,1)}=\frac{2 \partial_{r}g_{\sigma}(r,1,u)}{\partial_{r}g_{\sigma}(r,r,1)}.\] (117)

The second equality between the two ratios of Gaussian integral terms gives a fixed point equation on the norm \(r\). Writing the interactions in Eq. 117 explicitly and rearranging the ratios, we get

\[f(r,u)=\frac{\mathbb{E}[\sigma^{\prime}(rx)\sigma(rx)x]}{\mathbb{E}[\sigma(rx )^{2}]}-\frac{\mathbb{E}[\sigma^{\prime}(rx)\sigma(y)x]}{\mathbb{E}[\sigma(rx )\sigma(y)]}=0,\] (118)

where \(x\) and \(y\) are standard Gaussians with correlation \(\mathbb{E}[xy]=u\). Let us define the following helper functions

\[G(r)=\frac{\mathbb{E}[\sigma^{\prime}(rx)\sigma(rx)x]}{\mathbb{ E}[\sigma(rx)^{2}]}=\frac{1}{2}\frac{d}{dr}\log(\mathbb{E}[\sigma(rx)^{2}]),\] \[\tilde{G}(u,r)=\frac{\mathbb{E}[\sigma^{\prime}(rx)\sigma(y)x]} {\mathbb{E}[\sigma(rx)\sigma(y)]}=\frac{d}{dr}\log(\mathbb{E}[\sigma(rx) \sigma(y)]),\] (119)

which yields

\[f(r,u)=G(r)-\tilde{G}(u,r)=\frac{d}{dr}\log\left(\frac{\mathbb{E }[\sigma(rx)^{2}]^{\frac{1}{2}}}{\mathbb{E}[\sigma(rx)\sigma(y)]}\right)=0.\] (120)

Let us consider the case \(u>0\). We want to show that for any given \(u\in(0,1]\) there is a unique \(r\in(0,1]\) such that \(f(r,u)=0\). Under the assumption \(\sigma(0)\neq 0\), if the following three conditions are satisfied for all \(u\in(0,1]\),

\[\text{(i)}\ \frac{\sigma^{\prime}(0)}{\sigma(0)}\frac{\mathbb{E}[ \sigma(y)x]}{\mathbb{E}[\sigma(y)]}>0,\] \[\text{(ii)}\ \frac{\mathbb{E}[\sigma^{\prime}(x)\sigma(x)x]}{ \mathbb{E}[\sigma(x)^{2}]}>\frac{\mathbb{E}[\sigma^{\prime}(x)\sigma(y)x]}{ \mathbb{E}[\sigma(x)\sigma(y)]},\] \[\text{(iii)}\ \frac{d^{2}}{dr^{2}}\log\left(\frac{\mathbb{E}[\sigma(rx)^{2 }]^{\frac{1}{2}}}{\mathbb{E}[\sigma(rx)\sigma(y)]}\right)>0,\] (121)

then we have a unique \(r\) solving Eq. 120 as we explain next. Note that the first two conditions are equivalent to \(f(0,u)<0\) and \(f(1,u)>0\), respectively. The tricky part is the third condition which is equivalent to showing that

\[\frac{\mathbb{E}[\sigma(rx)\sigma(y)]}{\mathbb{E}[\sigma(rx)^{2 }]^{\frac{1}{2}}}\] (122)

Figure 10: _The graph of \(f(r,u)=\frac{d}{dr}\left(\frac{1}{2}\log g_{\sigma}(r,r,1)-\log g_{\sigma}(r, 1,u)\right)\) for activation functions erf, softplus with \(\beta=1\), sigmoid, tanh, and gelu, respectively. Zero crossings of \(f\) are shown in red._ For softplus and sigmoid, we observe that \(f\) is negative for \(r=0,u\in(0,1)\), positive for \(r=1,u\in(0,1)\), and increasing in \(r\in[0,1]\) for any fixed \(u\), thus satisfying the sufficient conditions in Eq. 121. However, for tanh and erf, \(f\) shows non-monotonic behavior in \(r\) when \(u\) is close to \(1\). For the GeLU activation function \(\sigma(x)=x\Phi(x)\), which is non-monotonic, we observe that \(f\) does not cross zero for any \((r,u)\) pair in the plotted domain. It approaches zero from below when \(r\to\infty\) thus showing a very different behavior from the other activation functions.

is log-concave in \(r\). We note that marginalization properties of log-concave functions may be helpful here. In this paper, we were not able to prove the sufficient conditions listed above for general activation functions that do not admit an analytic formula of the interaction, even for softplus which we studied in detail (see Subsection G.5). Instead, we present the numerical integration results, which show that for any given \(u\in(0,1]\), there is a unique \(r\in(0,1]\) such that \(f=0\) (see Fig. 10). Once \(r\) is shown to be unique, then the matching outgoing weight \(a\) follows from Eq. 117.

### Closed-Form Solution for Erf Activation

**Corollary G.4**.: _Assume that the activation function is \(\sigma_{\text{ref}}\). The optimal solution \((w^{*},a^{*})\) is given by_

\[\|w^{*}\|=\sqrt{\frac{1}{2k-1}},\quad a^{*}=k,\quad\frac{w^{*}}{\|w^{*}\|}= \frac{1}{\sqrt{k}}\sum_{i=1}^{k}v_{i},\]

_or, equivalently, by \((-w^{*},-a^{*})\). The optimal loss is given by_

\[L^{*}_{\text{eff}}(k)=\frac{2}{\pi}\Big{(}k\arcsin\bigl{(}\frac{1}{2}\bigr{)} -k^{2}\arcsin\bigl{(}\frac{1}{2k}\bigr{)}\Big{)}.\] (123)

Proof.: Since \(\operatorname{erf}\) is an odd activation function, it suffices to find parameters of the non-trivial critical points satisfying \(u\geq 0\). For any such critical point \((w^{*},a^{*})\), its mirror symmetry \((-w^{*},-a^{*})\) is an equivalent critical point due to Eq. 41.

For \(u=0\), we have that \(g_{\text{eff}}(r,1,u)=0\) which implies \(g_{\text{eff}}(r,r,1)=\mathbb{E}[\sigma(rx)^{2}]=0\) due to the first derivative constraint in Eq. 116 which holds if and only if \(r=0\). This gives a possible trivial critical point yielding the zero predictor function.

For a given \(u=\frac{1}{\sqrt{k}}>0\), from Fig. 10, we observe that there is a unique \(r\in(0,1]\) satisfying the fixed point equation in Eq. 120. For uniqueness, we rely on numerical integration. We will find one solution to the derivative constraints given below

\[ag_{\text{eff}}(r,r,1)=kg_{\text{eff}}(r,1,u),\quad a\partial_{r}g_{\text{ eff}}(r,r,1)-2k\partial_{r}g_{\text{eff}}(r,1,u)=0,\] (124)

for a given \(k\), and equivalently \(u=\frac{1}{\sqrt{k}}>0\); and due to uniqueness, conclude that it is the only non-trivial critical point up to symmetries.

In particular, we will use the analytic formula for the interaction function [2, 3]

\[g_{\text{eff}}(r_{1},r_{2},u)=\frac{2}{\pi}\arcsin\Bigl{(}\frac{r_{1}r_{2}u} {\sqrt{r_{1}^{2}+1}\sqrt{r_{2}^{2}+1}}\Bigr{)}.\] (125)

Let us find \(r\) where we have

\[g_{\text{eff}}(r,1,u)=g_{\text{eff}}(r,r,1)\]

that is satisfied if we have that the arguments of \(\arcsin\) match, which happens at

\[\frac{ru}{\sqrt{r^{2}+1}\sqrt{2}}=\frac{r^{2}}{r^{2}+1}\quad\Rightarrow\quad r =\sqrt{\frac{u^{2}}{2-u^{2}}}=\frac{1}{\sqrt{2k-1}}.\] (126)

Interestingly, at this value of \(r\), we also have

\[2\partial_{r}g_{\text{eff}}(r,1,u)=\partial_{r}g_{\text{eff}}(r,r,1)\]

which can be seen by inserting the guessed values in the following equation

\[2\partial_{r}\Bigl{(}\frac{ru}{\sqrt{r^{2}+1}\sqrt{2}}\Bigr{)}\arcsin^{ \prime}\bigl{(}\frac{ru}{\sqrt{r^{2}+1}\sqrt{2}}\bigr{)}=\partial_{r}\Bigl{(} \frac{r^{2}}{r^{2}+1}\Bigr{)}\arcsin^{\prime}\bigl{(}\frac{r^{2}}{r^{2}+1} \bigr{)}.\]

Setting \(a=k\) in Eq. 124 completes the order parameters of the non-trivial critical point. Finally, let us compute the loss at \(r=\nicefrac{{1}}{{\sqrt{2k-1}}},u=\nicefrac{{1}}{{\sqrt{k}}},a=k\);

\[L^{*}_{\text{eff}}(k) :=a^{2}g_{\text{eff}}(r,r,1)-2akg_{\text{eff}}(r,1,u)+kg_{\text{ eff}}(1,1,1),\] \[=-k^{2}g_{\text{eff}}(r,r,1)+kg_{\text{eff}}(1,1,1),\] \[=\frac{2}{\pi}\Bigl{(}k\arcsin\bigl{(}\frac{1}{2}\bigr{)}-k^{2} \arcsin\bigl{(}\frac{1}{2k}\bigr{)}\Bigr{)}.\] (127)

### Closed-Form Solution for ReLU Activation

**Corollary G.5**.: _Assume that the activation function is \(\sigma_{\text{relu}}\). Any optimal solution \((w^{*},a^{*})\) satisfies_

\[\|w^{*}\|a^{*}=\frac{k}{h(1)}h\big{(}\frac{1}{\sqrt{k}}\big{)},\quad \frac{w^{*}}{\|w^{*}\|}=\frac{1}{\sqrt{k}}\sum_{i=1}^{k}v_{i},\] (128)

_forming an equal-loss hyperbola. The optimal loss is given by_

\[L^{*}_{\text{relu}}(k)=k^{2}\Big{(}h(0)-\frac{1}{h(1)}h\big{(} \frac{1}{\sqrt{k}}\big{)}^{2}\Big{)}+k(h(1)-h(0)).\] (129)

We will first show that the interaction of ReLU satisfies

\[\text{(i)}\;\,h^{\prime}(u)\!>\!0\;\;\text{for}\;\;u\!\in\!(-1,1),\] \[\text{(ii)}\;\,h^{\prime\prime}(u)u\!<\!h^{\prime}(u)\;\;\text{ for}\;\;u\!\in\!(-1,u_{0}],\] \[\text{(iii)}\;\,\frac{h^{\prime}(u_{0})}{u_{0}}\!>\!\frac{h^{ \prime}(u)}{u}\;\;\text{for}\;\;u\!\in\!(u_{0},1);\] (130)

where \(u_{0}=1/\sqrt{2}\). Note that property (i) is equivalent to Assumption 3.1 (i), and property (ii) is almost equivalent to Assumption 3.1 (ii) except that it holds in the interval \((-1,u_{0}]\); property (iii) covers up for the missing piece of the interval in the property (ii).

_ReLU interaction satisfies Properties 130; Proof._ Let us write the first two derivatives of \(h\):

\[h^{\prime}(u)=\frac{\pi-\arccos(u)}{2\pi},\quad h^{\prime\prime} (u)=\frac{1}{2\pi\sqrt{1-u^{2}}}.\] (131)

Property (i) easily comes from noting that the derivative of \(h\) is positive for \(u\in(-1,1)\). Property (ii) holds for \(u\in(-1,0]\) since both the first and second derivatives are positive. Let us show that Property (ii) holds for \(u\in(0,u_{0}]\), that is equivalent to

\[\frac{u}{\sqrt{1-u^{2}}}<\pi-\arccos(u)=\frac{\pi}{2}+\arcsin(u).\] (132)

Let us note that the left-hand side is smaller than \(1\) since

\[\frac{u^{2}}{1-u^{2}}\leq 1.\]

Note that \(\arcsin(u)>0\) for \(u>0\); and \(\pi/2>1\). This completes the proof of Property (ii).

For Property (iii), we first show that \(h^{\prime}(u)/u\) is convex in \(u\in(0,1)\). The first two derivatives are

\[\frac{d}{du}\left(\frac{h^{\prime}(u)}{u}\right)=\frac{h^{\prime \prime}(u)}{u}-\frac{h^{\prime}(u)}{u^{2}},\quad\frac{d^{2}}{du^{2}}\left( \frac{h^{\prime}(u)}{u}\right)=\frac{h^{\prime\prime\prime}(u)}{u}-\frac{2h^ {\prime\prime}(u)}{u^{2}}+\frac{2h^{\prime}(u)}{u^{3}}.\]

Thus, it is equivalent to showing

\[h^{\prime\prime\prime}(u)u-2h^{\prime\prime}(u)+\frac{2h^{\prime }(u)}{u}=\frac{u^{2}}{(1-u^{2})^{3/2}}-\frac{2}{(1-u^{2})^{1/2}}+\frac{\pi+2 \arcsin(u)}{u}>0.\]

Using the Taylor series of \(\arcsin\) and \(u>0\), we have that \(\arcsin(u)>u\). Hence, it suffices to show

\[\frac{1}{(1-u^{2})^{1/2}}\big{(}-3+\frac{1}{1-u^{2}}+2(1-u^{2})^{1/2}\big{)} \geq 0;\] (133)

where we dropped the positive term \(\frac{\pi}{u}\) which holds due to the inequality of arithmetic and geometric means

\[\frac{1}{1-u^{2}}+(1-u^{2})^{1/2}+(1-u^{2})^{1/2}\geq 3.\]

Let us assume the contrary of Property (iii), that there exists \(u\in(u_{0},1)\) such that

\[\frac{h^{\prime}(u_{0})}{u_{0}}\leq\frac{h^{\prime}(u)}{u}.\] (134)Note that \(h^{\prime}(u_{0})/u_{0}>h^{\prime}(1)\) because \(\pi(1-u_{0})-\arccos(u_{0})>0\) holds at \(u_{0}=1/\sqrt{2}\). Since \(h^{\prime}(u)/u\) is left-continuous at \(u=1\), there exists \(\epsilon>0\) such that

\[\frac{h^{\prime}(u_{0})}{u_{0}}>\frac{h^{\prime}(1-\epsilon)}{1-\epsilon}.\] (135)

Finally, there exists \(\alpha\in(0,1)\) such that \(u=\alpha(1-\epsilon)+(1-\alpha)u_{0}\) which gives due to the convexity of \(h^{\prime}(u)/u\) the following

\[\alpha\frac{h^{\prime}(1-\epsilon)}{1-\epsilon}+(1-\alpha)\frac{h^{\prime}(u _{0})}{u_{0}}\geq\frac{h^{\prime}(u)}{u}.\] (136)

This yields a contradiction since the left-hand side is strictly smaller than \(h^{\prime}(u_{0})/u_{0}\) hence the proof of Property (iii) is complete. _ReLU interaction satisfies Properties 130; End of Proof._

Proof.: First, we replicate the proof steps of Theorem 5.1 to show that any non-trivial critical point must be on the boundary and attain equal correlations. From Property 130 (i), we get that there is no non-trivial critical point in \(\mathrm{int}\,B\). For \(k=1\), this implies that \(u_{1}=-1\) or \(u_{1}=1\).

For general \(k\), let us recall that we get the Lagrangian condition for non-trivial critical points

\[rh^{\prime}(u_{j})=\lambda u_{j}\ \ \forall j\in[k],\quad\sum_{j=1}^{k}u_{j}^{2}=1.\] (137)

which is equivalent to Eq. 112 for ReLU activation function. \(u_{j}=0\) is not possible since we have \(h^{\prime}(0)\neq 0\). Hence, we get

\[\frac{h^{\prime}(u_{j})}{u_{j}}=\frac{\lambda}{r},\quad\forall j\in[k].\] (138)

Property 130 (ii) implies that \(f(u)=h^{\prime}(u)/u\) is decreasing for \(u\in(-1,u_{0})\backslash\{0\}\). Moreover, \(f\) is negative for \(u<0\) and positive for \(u>0\).

1. If \(\lambda/r<0\), we get that all \(u_{j}\) are equal and negative, hence they are equal to \(-1/\sqrt{k}\) due to the boundary condition.
2. If \(\lambda/r=0\), we get \(u_{j}=-1\) for all \(j\) which implies that \(k=1\) which is already covered above.
3. If \(\lambda/r>0\), Property 130 (iii) gives that \(f(u_{0})>f(u)\) for \(u\!\in\!(u_{0},1)\). Since \(f\) is decreasing we have also \(f(u)>f(u_{0})\) for \(u\in(0,u_{0})\); hence \(f(u_{j})\) are equal only when all \(u_{j}<u_{0}\) or \(u_{j}>u_{0}\); however, the latter case is not possible for \(k\geq 2\) since it breaks the ball constraint, i.e. \(u_{1}^{2}+u_{2}^{2}>1\).

Hence, we get that \(u_{j}\in(0,u_{0}]\) and are equal since \(f\) is decreasing in this interval. This completes the proof of replica of Theorem 5.1 for the ReLU activation function.

For the ReLU activation function, there is at least one non-differentiable critical point at \(a=0\) or \(r=0\). The careful analysis of this critical point is beyond the scope of this work. For any such 'trivial' point, the error of zero-function is equivalent to

\[\mathbb{E}[(\sum_{j=1}^{k}\sigma(v_{j}\cdot x))^{2}]=kh(1)+k(k-1)h(0).\] (139)

We will next show that \((-1/\sqrt{k})_{j=1}^{k}\) and \((1/\sqrt{k})_{j=1}^{k}\) are the global minimum and the global maximum of the following loss function

\[\sum_{j=1}^{k}h(u_{j}),\quad\text{subject to}\ \sum_{j=1}^{k}u_{j}^{2}\leq 1.\] (140)Due to the Lagrange condition, there is no other critical point, hence these are the only two critical points of the constrained objective in Eq. 140. The objective then reduces to \(kh(u)\) which is minimized at \(u=\nicefrac{{-1}}{{\sqrt{k}}}\) and maximized at \(u=\nicefrac{{1}}{{\sqrt{k}}}\).

Next, we will give the closed-form solution of the remaining order parameters. Plugging in the correlation in the loss and using the factorization of the interaction in Eq. 14, we get

\[L=a^{2}r^{2}\cdot h(1)-2kar\cdot h(u)+C.\]

Let us set \(\tilde{a}=ar\). The loss is a second-order polynomial in \(\tilde{a}\)

\[L=h(1)\left(\tilde{a}^{2}-2\tilde{a}k\frac{h(u)}{h(1)}+k+k(k-1)\frac{h(0)}{h(1 )}\right)\]

where we made the constant explicit. Since the coefficient of the leading term is positive, there is a minimizer and it is the only critical point. Taking the derivative, the minimum is attained at

\[\tilde{a}_{*}=k\frac{h(u)}{h(1)}\] (141)

Finally, plugging in \(\tilde{a}_{*}\), we get

\[L(u)=-k^{2}\frac{h(u)^{2}}{h(1)}+kh(1)+k(k-1)h(0).\] (142)

For \(u=1/\sqrt{k}\) and \(u=-1/\sqrt{k}\), \(h(u)\) is non-zero; hence \(l(u)\) is smaller than the loss of the zero function (trivial critical points). The smallest loss is attained at \(u=\nicefrac{{1}}{{\sqrt{k}}}\) which is, therefore, the optimal solution. We conclude that the critical point at \(u=-\nicefrac{{1}}{{\sqrt{k}}}\) is a saddle point since it is a maximum in \(u\) and a minimum in \(\tilde{a}\). 

### Bounds on Incoming Vector Norm and Outgoing Weight for Softplus

Unlike ReLU and erf, the interaction function does not have a known analytic expression for softplus, hence the proof involves some techniques to compare ratios of Gaussian integral terms.

_FKG Inequality._ We will use a special case of the FKG inequality repeatedly, that is,

\[\mathbb{E}[f(x)g(x)]>\mathbb{E}[f(x)]\mathbb{E}[g(x)]\] (143)

if both \(f,g\) are increasing (or decreasing) implying that \(f\) and \(g\) are positively correlated. The inequality changes direction if \(f\) is increasing and \(g\) is decreasing (or vice versa) implying that \(f\) and \(g\) are negatively correlated.

We will rely on some specific properties of the softplus family that are developed in Section G.5.4. Unfortunately, some of these properties do not apply to other activation functions. As a first example of managing interactions that do not have an analytic formula, the proof may inspire generalizations to other activation functions. Below we present the proof sketch for Theorem 5.3. In the following Subsections G.5.1, G.5.2, G.5.3, and G.5.4, the components of the proof are presented in detail.

_Proof Sketch._ We want to characterize the zero(s) of \(f\) introduced in Section G.2.2 that is

\[f(r,u)=G(r)-\tilde{G}(u,r)=\frac{\mathbb{E}[\sigma^{\prime}(rx)\sigma(rx)x]}{ \mathbb{E}[\sigma(rx)^{2}]}-\frac{\mathbb{E}[\sigma^{\prime}(rx)\sigma(y)x]}{ \mathbb{E}[\sigma(rx)\sigma(y)]}.\] (144)

For \(r\in[0,1]\), there is a unique correlation \(u\in[0,1]\) such that \(f(r,u)=0\). Denoting this correlation by \(h(r)\), we have a map \(h:[0,1]\to[0,1]\) with boundary conditions \(h(0)=0\) and \(h(1)=1\). For \(r>1\), there is no solution of \(f\). As a consequence, no \(r\geq 0\) solves \(f(r,u)=0\) for negative \(u\), hence there is no non-trivial critical point at \(u=-1/\sqrt{k}\).

In Section G.5.2, we prove the inequality \(h(r)\geq r\), which gives us the upper bound on the norm since we have that the correlation at a non-trivial critical point is \(h(r)=1/\sqrt{k}\). Using this inequality and Stein's Lemma, we give a lower bound on the outgoing weight, that is \(a\geq k\), in Section G.5.3. In summary, any non-trivial critical point of the loss has equal correlations that are \(u=1/\sqrt{k}\), the norm satisfies \(r\leq u\), and the lower bound on the outgoing weight follows. _End of Proof Sketch._

#### g.5.1 Constraining the Zeros of \(f\)

In this subsection, we will describe all zero-crossings of \(f:[0,\infty)\times[-1,1]\to\mathbb{R}\). We need to check four cases _(i)_\(r=0\), _(ii)_\(r=1\), _(iii)_\(r>1\), and _(iv)_\(r\in(0,1)\).

_(i)_\(r=0\)_:_ Note that \(G(0)=\tilde{G}(0,0)=0\). Since \(\tilde{G}\) is increasing in correlation for \(u\in[0,1]\) and \(\tilde{G}(u,0)<\tilde{G}(0,0)\) for \(u<0\) (Lemma G.6), the only solution is \(u=0\).

_(ii)_\(r=1\)_:_ Note that \(G(1)=\tilde{G}(1,1)\) since \(y=x\) due to correlation one in Eq. 119. Since \(\tilde{G}\) is increasing in correlation for \(u\in[0,1]\) and \(\tilde{G}(u,1)<\tilde{G}(0,1)\) for \(u<0\) (Lemma G.6), the only solution is \(u=1\).

_(iii)_\(r>1\)_:_ We will show that there is no zero in this case. Let us first show that \(G(r)>\tilde{G}(1,r)\) for \(r>1\), which is equivalent to

\[\mathbb{E}[\sigma^{\prime}(rx)\sigma(rx)x]\mathbb{E}[\sigma(rx)\sigma(x)]> \mathbb{E}[\sigma^{\prime}(rx)\sigma(x)x]\mathbb{E}[\sigma(rx)^{2}].\] (145)

Changing the measure of \(x\) from the standard Gaussian \(p(x)\) to \(\tilde{p}(x)=p(x)\sigma(rx)^{2}/\mathbb{E}[\sigma(rx)^{2}]\), we get the following equivalent inequality

\[\mathbb{E}_{x\sim\tilde{p}}\left[\frac{\sigma^{\prime}(rx)x}{\sigma(rx)} \right]\mathbb{E}_{x\sim\tilde{p}}\left[\frac{\sigma(x)}{\sigma(rx)}\right]> \mathbb{E}_{x\sim\tilde{p}}\left[\frac{\sigma^{\prime}(rx)x}{\sigma(rx)} \frac{\sigma(x)}{\sigma(rx)}\right].\] (146)

From the property (iv) of Lemma G.8, we have that \(\sigma^{\prime}(rx)x/\sigma(rx)\) is increasing after a substitution \(x\gets rx\). We need to show \(\sigma(x)/\sigma(rx)\) is decreasing in \(x\) for \(r>1\). We take the derivative

\[\frac{d}{dx}\frac{\sigma(x)}{\sigma(rx)}=\frac{\sigma^{\prime}(x)\sigma(rx)- \sigma(x)\sigma^{\prime}(rx)r}{\sigma(rx)^{2}}.\] (147)

Since \(\sigma^{\prime}(x)x/\sigma(x)\) is increasing \(\forall x\in\mathbb{R}\), we have

\[\frac{\sigma^{\prime}(x)x}{\sigma(x)}<\frac{\sigma^{\prime}(rx)rx}{\sigma(rx )}\text{ for }x>0,\text{ and }\frac{\sigma^{\prime}(x)x}{\sigma(x)}>\frac{\sigma^{\prime}(rx)rx}{\sigma( rx)}\text{ for }x<0\]

which yields \(\sigma^{\prime}(x)\sigma(rx)<\sigma(x)\sigma^{\prime}(rx)r\), hence we conclude that \(\sigma(x)/\sigma(rx)\) is decreasing. Thanks to the FKG inequality, \(\sigma^{\prime}(rx)x/\sigma(rx)\) and \(\sigma(x)/\sigma(rx)\) are negatively correlated which completes the argument. Since from Lemma G.6, \(\tilde{G}\) is increasing in correlation and \(\tilde{G}(0,r)>\tilde{G}(u,r)\) for \(u<0\), we have \(\tilde{G}(1,r)>\tilde{G}(u,r)\) for all \(u\in[-1,1)\), therefore there is no solution of \(f\).

_(iv)_\(r\in(0,1)\)_:_ We want to show that \(\forall r\in(0,1)\), there is a unique \(u\in(0,1)\) such that \(f(r,u)=0\). It suffices to show

\[\tilde{G}(0,r)<G(r)<\tilde{G}(1,r),\]

since \(\tilde{G}\) is continuous and increasing in correlation for \(u\in[0,1]\) (Lemma G.6), it then crosses \(G(r)\) at a unique \(u\in(0,1)\).

_First inequality_; \(\tilde{G}(0,r)<G(r)\). In this case, \(x\) and \(y\) are Gaussians with zero correlation, hence independent. We can expand \(\tilde{G}(0,r)\) by factorizing the integrals

\[\tilde{G}(0,r)=\frac{\mathbb{E}\left[\sigma^{\prime}(rx)x\right]\mathbb{E} \left[\sigma(y)\right]}{\mathbb{E}\left[\sigma(rx)\right]\mathbb{E}\left[ \sigma(y)\right]}=\frac{\mathbb{E}\left[\sigma^{\prime}(rx)x\right]}{\mathbb{E }\left[\sigma(rx)\right]}.\]

We want to show

\[\mathbb{E}\left[\sigma^{\prime}(rx)x\right]\mathbb{E}\left[\sigma(rx)^{2} \right]<\mathbb{E}\left[\sigma^{\prime}(rx)\sigma(rx)x\right]\mathbb{E}\left[ \sigma(rx)\right]\] (148)

which is equivalent to the following inequality after changing the measure from standard Gaussian \(p(x)\) to \(\tilde{p}(x)=p(x)\sigma(rx)/\mathbb{E}[\sigma(rx)]\)

\[\mathbb{E}_{x\sim\tilde{p}}\left[\frac{\sigma^{\prime}(rx)x}{\sigma(rx)} \right]\mathbb{E}_{x\sim\tilde{p}}[\sigma(rx)]<\mathbb{E}_{x\sim\tilde{p}} \left[\frac{\sigma^{\prime}(rx)x}{\sigma(rx)}\sigma(rx)\right].\] (149)

This follows from the FKG inequality since we have that both \(\sigma^{\prime}(x)x/\sigma(x)\) and \(\sigma(x)\) are increasing from the properties (iv) and (i) of softplus (Lemma G.8).

_Second inequality: \(G(r)<\tilde{G}(1,r)\)._ This is equivalent to the Ineq. 145, but the direction is reversed since in this case \(r<1\). We showed that \(\sigma(x)/\sigma(r^{\prime}x)\) is decreasing in \(x\) for all \(r^{\prime}>1\), therefore its reciprocal \(\sigma(r^{\prime}x)/\sigma(x)\) is increasing in \(x\). Substituting \(x\gets rx\) where \(r=1/r^{\prime}<1\), we get that \(\sigma(x)/\sigma(rx)\) is increasing in \(x\) for \(r<1\). This yields a positive correlation between \(\sigma^{\prime}(rx)x/\sigma(rx)\) and \(\sigma(x)/\sigma(rx)\) from the FKG inequality and completes the argument.

Overall, we showed that there are no zeros of \(f\) for \(r>1\). For \(r\in[0,1],\) there is a unique correlation \(u\), that we will denote by \(h(r)\), such that \(f(r,h(r))=0\). Furthermore, \(h:[0,1]\rightarrow[0,1]\) satisfies the following

1. \(h(0)=0\) and \(h(1)=1\),
2. for \(r\in(0,1)\), we have \(h(r)\in(0,1)\).

#### g.5.2 Bound on the Norm

In this subsection, we will show that \(h(r)\geq r\) for all \(r\in(0,1)\). Let us assume the contrary, which implies

\[\tilde{G}(h(r),r)<\tilde{G}(r,r)\]

due to Lemma G.6. It suffices to show that for all \(r\in(0,1)\), we have

\[\tilde{G}(r,r)\leq G(r),\] (150)

which yields a contradiction since \(G(r)=\tilde{G}(h(r),r)\). Showing this is equivalent to

\[\mathbb{E}\left[\sigma^{\prime}(rx)\sigma(rx+r^{\prime}z)x\right]\mathbb{E} \left[\sigma(rx)^{2}\right]\leq\mathbb{E}\left[\sigma^{\prime}(rx)\sigma(rx)x \right]\mathbb{E}\left[\sigma(rx)\sigma(rx+r^{\prime}z)\right]\] (151)

where \(r^{\prime}=\sqrt{1-r^{2}}\). After a change of measure from standard Gaussian \(p(x)\) to

\[\tilde{p}(x)=p(x)\frac{\mathbb{E}[\sigma(rx+r^{\prime}z)|x]\sigma(rx)}{ \mathbb{E}\left[\sigma(rx+r^{\prime}z)\sigma(rx)\right]},\]

this is equivalent to the following inequality

\[\mathbb{E}_{x\sim\tilde{p}}\left[\frac{\sigma^{\prime}(rx)x}{\sigma(rx)} \right]\mathbb{E}_{x\sim\tilde{p}}\left[\frac{\sigma(rx)}{\mathbb{E}[\sigma( rx+r^{\prime}z)|x]}\right]\leq\mathbb{E}_{x\sim\tilde{p}}\left[\frac{ \sigma^{\prime}(rx)x}{\sigma(rx)}\frac{\sigma(rx)}{\mathbb{E}[\sigma(rx+r^{ \prime}z)|x]}\right].\] (152)

What remains to show is that

\[\frac{\mathbb{E}[\sigma(rx+r^{\prime}z)|x]}{\sigma(rx)}\]

is non-increasing in \(x\) since then we can conclude by the FKG inequality. Since \(r>0\) we can drop it up to a change in the standard deviation of \(x\). We want to show that its derivative is non-positive:

\[\sigma(x)\mathbb{E}[\sigma^{\prime}(x+r^{\prime}z)|x]\leq\sigma^{\prime}(x) \mathbb{E}[\sigma(x+r^{\prime}z)|x]\;\Leftrightarrow\;\frac{\sigma(x)}{ \sigma^{\prime}(x)}\leq\frac{\mathbb{E}[\sigma(x+r^{\prime}z)|x]}{\mathbb{E}[ \sigma^{\prime}(x+r^{\prime}z)|x]}.\] (153)

From the property (iii) of softplus (Lemma G.8), we have that \(R(x)=\sigma(x)/\sigma^{\prime}(x)\) is convex. Applying Jensen, we get

\[\frac{\sigma(x)}{\sigma^{\prime}(x)}\leq\mathbb{E}\left[\frac{\sigma(x+r^{ \prime}z)}{\sigma^{\prime}(x+r^{\prime}z)}\Big{|}x\right].\]

What remains to show is that

\[\mathbb{E}\left[\frac{\sigma(x+r^{\prime}z)}{\sigma^{\prime}(x+r^{\prime}z)} \Big{|}x\right]\mathbb{E}\left[\sigma^{\prime}(x+r^{\prime}z)|x\right]\leq \mathbb{E}\left[\sigma(x+r^{\prime}z)|x\right].\] (154)

Note that \(\mathbb{E}[\sigma^{\prime}(x+r^{\prime}z)|x]\) is increasing in \(x\) since \(\sigma^{\prime}\) is increasing. Moreover, the function

\[\mathbb{E}\left[\frac{\sigma(x+r^{\prime}z)}{\sigma^{\prime}(x+r^{\prime}z)} \Big{|}x\right]\]

is increasing in \(x\) since its integrand \(R\) is increasing from the property (ii) of softplus (Lemma G.8). Then we conclude by the FKG inequality that Eq. 154 holds. Therefore, for a solution \((r,u)\) of the fixed point Eq. 117, we have \(r\!\leq\!u=\frac{1}{k}\).

#### g.5.3 Bounding the Outgoing Weight

To get a bound on \(a\), let us analyze the ratio of interactions in Eq. 117

\[\frac{g_{\sigma}(r,1,u)}{g_{\sigma}(r,r,1)}=\frac{a}{k}.\] (155)

Using the convexity of softplus (property (i) of Lemma G.8), we get

\[\frac{\mathbb{E}[\sigma(rx)\sigma(ux+u^{\prime}z)]}{\mathbb{E}[ \sigma(rx)^{2}]} \geq\frac{\mathbb{E}[\sigma(rx)\sigma(rx)]+\mathbb{E}[\sigma(rx)((u -r)x+u^{\prime}z)\sigma^{\prime}(rx)]}{\mathbb{E}[\sigma(rx)^{2}]}\] \[=1+(u-r)\frac{\mathbb{E}[\sigma^{\prime}(rx)\sigma(rx)x]}{ \mathbb{E}[\sigma(rx)^{2}]}.\] (156)

We can transform the numerator using Stein's lemma with \(v(x)=\sigma(rx)\sigma^{\prime}(rx)\)

\[\mathbb{E}[\sigma(rx)\sigma^{\prime}(rx)x]=r\mathbb{E}[\sigma^{ \prime}(rx)^{2}+\sigma(rx)\sigma^{\prime\prime}(rx)]\] (157)

which is positive since softplus is positive, increasing, and convex. Combining it with \(u\geq r\), we get that the ratio is bounded below by \(1\) which yields \(a\geq k\).

#### g.5.4 Helper Lemmas

In this subsection, we provide helper lemmas used in the proof of Theorem 5.3. We present Lemma G.6 which shows that \(\tilde{G}\) is increasing in correlation and Lemma G.7 used in the proof of the former. Finally, we present several properties of the softplus family in Lemma G.8 that are used throughout the proof.

**Lemma G.6**.: _The following function is increasing in \(u\in[0,1]\)_

\[\tilde{G}(u,r)=\frac{\mathbb{E}[\sigma^{\prime}(rx)\sigma(y)x]}{ \mathbb{E}[\sigma(rx)\sigma(y)]}\] (158)

_for any \(r\geq 0\), where \(x\) and \(y\) are standard Gaussians with correlation \(\mathbb{E}[xy]=u\). Moreover, \(\tilde{G}(u,r)<\tilde{G}(0,r)\) for \(u<0\)._

Proof.: Let us assume \(0\leq u_{1}<u_{2}\leq 1\). For the first part of the statement, we want to show

\[\frac{\mathbb{E}[\sigma^{\prime}(rx)\sigma(y_{1})x]}{\mathbb{E}[ \sigma(rx)\sigma(y_{1})]}<\frac{\mathbb{E}[\sigma^{\prime}(rx)\sigma(y_{2})x] }{\mathbb{E}[\sigma(rx)\sigma(y_{2})]}\] (159)

where \(\mathbb{E}[xy_{1}]=u_{1}\) and \(\mathbb{E}[xy_{2}]=u_{2}\). Changing the measure from the standard Gaussian \(p(x)\) to

\[\tilde{p}(x)=p(x)\frac{\sigma(rx)\mathbb{E}[\sigma(y_{2})|x]}{ \mathbb{E}[\sigma(rx)\sigma(y_{2})]},\]

we get the following equivalent inequality

\[\mathbb{E}\left[\frac{\sigma^{\prime}(rx)x}{\sigma(rx)}\frac{ \mathbb{E}[\sigma(y_{1})|x]}{\mathbb{E}[\sigma(y_{2})|x]}\right]<\mathbb{E} \left[\frac{\sigma^{\prime}(rx)x}{\sigma(rx)}\right]\mathbb{E}\left[\frac{ \mathbb{E}[\sigma(y_{1})|x]}{\mathbb{E}[\sigma(y_{2})|x]}\right].\] (160)

Thanks to the property (iv) of softplus (Lemma G.8), we have that \(\sigma^{\prime}(rx)x/\sigma(rx)\) is increasing in \(x\) after a substitution \(x\gets rx\) for \(r>0\). For \(r=0\), the function reduces to \(\gamma x\) with some \(\gamma>0\), hence increasing. We will next show that (all integrations are w.r.t \(z\) hereafter, hence we drop the conditioning on \(x\))

\[\frac{\mathbb{E}[\sigma(y_{1})]}{\mathbb{E}[\sigma(y_{2})]}\] (161)

is decreasing in \(x\). Computing the derivative w.r.t \(x\), we want to show that it is negative

\[\frac{\mathbb{E}[\sigma^{\prime}(y_{1})]u_{1}}{\mathbb{E}[\sigma (y_{2})]}-\frac{\mathbb{E}[\sigma(y_{1})]\mathbb{E}[\sigma^{\prime}(y_{2})]u_{ 2}}{\mathbb{E}[\sigma(y_{2})]^{2}}<0\ \ \Leftrightarrow\ \frac{\mathbb{E}[\sigma^{\prime}(y_{1})]u_{1}}{\mathbb{E}[\sigma(y_{1})]}< \frac{\mathbb{E}[\sigma^{\prime}(y_{2})]u_{2}}{\mathbb{E}[\sigma(y_{2})]}.\] (162)Note that this is equivalent to showing

\[\frac{d}{du}\frac{\mathbb{E}[\sigma^{\prime}(y)]u}{\mathbb{E}[\sigma(y)]}=\frac{ d^{2}}{dudx}\log(\mathbb{E}[\sigma(y)])>0\]

for all \(u\in[0,1)\) and \(x\in\mathbb{R}\). Changing the order of derivatives, it is sufficient to show

\[\frac{d}{dx}\left(\frac{\mathbb{E}[\sigma^{\prime}(y)]x}{\mathbb{E}[\sigma(y)] }-\big{(}\frac{u}{1-u^{2}}\big{)}\frac{\mathbb{E}[u^{\prime}z\sigma^{\prime}( y)]}{\mathbb{E}[\sigma(y)]}\right)>0.\] (163)

The first function

\[s_{1}(x)=\frac{\mathbb{E}[\sigma^{\prime}(y)]x}{\mathbb{E}[\sigma(y)]}\] (164)

is shown to be increasing in \(x\) in Lemma G.7 where we need to substitute \(x\to xu_{1}\) for \(u_{1}>0\), and for \(u_{1}=0\), we have \(s_{1}(x)=\gamma x\) for some \(\gamma>0\) hence it is increasing. The remaining part is to show that the second function

\[s_{2}(x)=\frac{\mathbb{E}[u^{\prime}z\sigma^{\prime}(y)]}{\mathbb{E}[\sigma(y )]}\] (165)

is decreasing. We will consider \(z\gets u^{\prime}z\) and \(x\gets ux\) in what follows. We have

\[\frac{d}{dx}\frac{\mathbb{E}[z\sigma^{\prime}(x+z)]}{\mathbb{E}[\sigma(x+z)] }<0\quad\Leftrightarrow\quad\frac{d}{dx}\frac{\mathbb{E}[\sigma(x+z)]}{ \mathbb{E}[\sigma^{\prime\prime}(x+z)]}>0\]

due to first applying Stein's Lemma to the numerator and then inverting the ratio. Using the chain rule, it is sufficient to show that

\[f_{1}(x)=\frac{\mathbb{E}[\sigma(x+z)]}{\mathbb{E}[\sigma^{\prime}(x+z)]}, \quad\text{and}\quad f_{2}(x)=\frac{\mathbb{E}[\sigma^{\prime}(x+z)]}{ \mathbb{E}[\sigma^{\prime\prime}(x+z)]}\] (166)

are increasing, since both functions are positive.

Interestingly, \(f_{1}\) is increasing in \(x\) if \(\sigma\) is a log-concave function. Because its derivative is positive

\[\frac{d}{dx}f_{1}(x)=1-\frac{\mathbb{E}[\sigma(x+z)]\mathbb{E}[\sigma^{\prime \prime}(x+z)]}{\mathbb{E}[\sigma^{\prime}(x+z)]^{2}}>0\]

if \(\mathbb{E}[\sigma(x+z)]\) is log-concave. This is the case since a centered Gaussian distribution \(p(z)\) is log-concave, therefore \(\sigma(x+z)p(z)\) is jointly log-concave, and marginalization preserves log-concavity.

Similarly, \(f_{2}\) is increasing since \(\sigma^{\prime}\) is also log-concave due to property (v) of softplus (Lemma G.8). Hence we showed that

\[r(u)=\frac{\mathbb{E}[\sigma^{\prime}(y)]x}{\mathbb{E}[\sigma(y)]}\] (167)

is increasing for \(u\in[0,1)\). The derivative of \(r\) explodes at \(1\), however, we can conclude by contradiction that \(r(1)>r(u)\) for \(u<1\): if \(r(u)\geq r(1)\) for some \(0\leq u<1\), then there exists \(u_{0}\in(u,1)\) where the function is decreasing. Therefore, \(r\) is increasing for \(u\in[0,1]\). We can conclude the first part of the proof by the FKG inequality \(\nicefrac{{\sigma^{\prime}(rx)x}}{{\sigma(rx)}}\) and \(\nicefrac{{\mathbb{E}[\sigma(y_{1})]}}{{\mathbb{E}[\sigma(y_{2})]}}\) are negatively correlated.

For the second part of the statement, we need to show

\[\mathbb{E}[\sigma^{\prime}(rx)\sigma(ux+u^{\prime}z)x]\mathbb{E}[\sigma(rx)] <\mathbb{E}[\sigma^{\prime}(rx)x]\mathbb{E}[\sigma(rx)\sigma(ux+u^{\prime}z)]\] (168)

for \(u<0\). Changing the measure from standard Gaussian \(p(x)\) to

\[\tilde{p}(x)=p(x)\frac{\sigma(rx)}{\mathbb{E}[\sigma(rx)]},\] (169)

the above inequality is equivalent to

\[\mathbb{E}_{x\sim\tilde{p}}\left[\frac{\sigma^{\prime}(rx)x}{\sigma(rx)}\sigma (ux+u^{\prime}z)\right]<\mathbb{E}_{x\sim\tilde{p}}\left[\frac{\sigma^{\prime} (rx)x}{\sigma(rx)}\right]\mathbb{E}_{x\sim\tilde{p}}[\sigma(ux+u^{\prime}z)].\] (170)

This holds since \(\sigma^{\prime}(rx)x/\sigma(rx)\) is increasing in \(x\), however, \(\sigma(ux+u^{\prime}z)\) is decreasing in \(x\) since \(u\) is negative which implies a negative correlation due to the FKG inequality.

**Lemma G.7**.: _The following function_

\[\frac{\mathbb{E}[\sigma^{\prime}(x+z)|x]x}{\mathbb{E}[\sigma(x+z)|x]}\]

_is increasing in \(x\) where the integrations are w.r.t a centered Gaussian \(z\)._

Proof.: Since all integrals are w.r.t \(z\), we drop the conditioning with respect to \(x\) in the proof. Taking the derivative w.r.t \(x\), and arranging the terms, it suffices to show

\[\left(\frac{\mathbb{E}[\sigma^{\prime\prime}(x+z)]x}{\mathbb{E}[\sigma^{ \prime}(x+z)]}+1\right)\mathbb{E}[\sigma(x+z)]>\mathbb{E}[\sigma^{\prime}(x+z )]x\] (171)

which is equivalent to the following due to the property \(\sigma^{\prime\prime}(z)=\beta\sigma^{\prime}(z)(1-\sigma^{\prime}(z))\)

\[\left(\beta x\left(1-\frac{\mathbb{E}[\sigma^{\prime}(x+z)^{2}]}{\mathbb{E}[ \sigma^{\prime}(x+z)]}\right)+1\right)\mathbb{E}[\sigma(x+z)]>\mathbb{E}[ \sigma^{\prime}(x+z)]x.\] (172)

In the case \(x\geq 0\), the LHS is bigger than \(\mathbb{E}[\sigma(x+z)]\) since \(\sigma^{\prime}(\cdot)\) is upper bounded by \(1\). Moreover, since \(\sigma(x)>x\) and from the convexity of softplus, we get \(\mathbb{E}[\sigma(x+z)]>x\). This yields the above inequality by again noting that \(\mathbb{E}[\sigma^{\prime}(x+z)]\) is upper bounded by \(1\).

In the case \(x<0\), we need another strategy. We have thanks to Cauchy-Schwartz

\[\frac{\mathbb{E}[\sigma^{\prime}(x+z)^{2}]}{\mathbb{E}[\sigma^{\prime}(x+z)]} \geq\mathbb{E}[\sigma^{\prime}(x+z)],\] (173)

thus it suffices to show

\[\left(\beta x-\beta x\mathbb{E}[\sigma^{\prime}(x+z)]+1\right)\mathbb{E}[ \sigma(x+z)]>\mathbb{E}[\sigma^{\prime}(x+z)]x.\] (174)

We will now show the following

\[\mathbb{E}[\sigma^{\prime}(x+z)]\geq\sigma^{\prime}(x)\] (175)

for which it suffices to show that \(v(z):=\sigma^{\prime}(x+z)+\sigma^{\prime}(x-z)\geq 2\sigma^{\prime}(x)\) for all \(z\) since the centered Gaussian measure \(p(z)\) is even and the integration can be done over the integrand \(v(z)\). We have

\[v^{\prime}(z)=\sigma^{\prime\prime}(x+z)-\sigma^{\prime\prime}(x-z)\] (176)

that is zero iff either \(x+z=x-z\) or \(x+z=-x+z\) where the latter is not possible since \(x<0\). Hence we get that a critical point of \(v(z)\) at \(z=0\) which is a minimizer since \(v^{\prime\prime}(0)=2\sigma^{\prime\prime\prime}(x)>0\) for \(x<0\). Hence \(v(z)\geq v(0)=2\sigma^{\prime}(x)\) for all \(z\) which completes the argument.

Finally, it remains to show

\[\left(\frac{\beta x}{e^{\beta x}+1}+1\right)\mathbb{E}[\sigma(x+z)]>\mathbb{E }[\sigma^{\prime}(x+z)]x.\] (177)

From the proof of Lemma G.8, we have that \(\beta\sigma(x)>\sigma^{\prime}(x)\), which in combination with the following trivial observation for all \(x<0\) (note that \(+1\) is not needed for the following to hold)

\[\frac{\beta x}{e^{\beta x}+1}+1>\beta x\] (178)

shows that Eq. 177 holds, hence the proof is complete. 

**Lemma G.8**.: _The softplus family has the following properties_

1. \(\sigma(x)\) _is increasing and convex,_
2. \(\sigma(x)\) _is log-concave (equivalently,_ \(\sigma(x)/\sigma^{\prime}(x)\) _is increasing),_
3. \(\sigma(x)/\sigma^{\prime}(x)\) _is convex,_
4. \(\sigma^{\prime}(x)x/\sigma(x)\) _is increasing,_
5. \(\sigma^{\prime}(x)\) _is log-concave._Proof.: For the property (i), see the formulas of \(\sigma^{\prime}\) and \(\sigma^{\prime\prime}\) in the proof of Lemma F.2. We next prove each one of the properties one after the other. Let us start with property (ii). First note that \(\sigma(x)\) is log-concave if and only if \(\sigma(x)/\sigma^{\prime}(x)\) is increasing since

\[\frac{d}{dx}\frac{\sigma(x)}{\sigma^{\prime}(x)}=1-\frac{\sigma(x)\sigma^{ \prime\prime}(x)}{\sigma^{\prime}(x)^{2}}>0\ \Leftrightarrow\ \sigma^{\prime}(x)^{2}>\sigma(x)\sigma^{\prime\prime}(x)\] (179)

where the second inequality is a characterization of log-concavity. We will prove that \(R(x):=\sigma(x)/\sigma^{\prime}(x)\) is increasing.

Let us write out the ratio explicitly

\[R(x)=\frac{1}{\beta}\left(\log(e^{\beta x}+1)+\frac{\log(e^{\beta x}+1)}{e^{ \beta x}}\right).\] (180)

The first derivative of \(R\) is given by

\[R^{\prime}(x)=\sigma^{\prime}(x)+\frac{\sigma^{\prime}(x)-\beta\sigma(x)}{e^ {\beta x}}=\frac{e^{\beta x}-\beta\sigma(x)}{e^{\beta x}}.\] (181)

Since \(\log\) is concave, expanding it around \(1\) we get \(\log(y+1)<y\) for all \(y>0\). Substituting \(y=e^{\beta x}\), we get that the numerator of \(R^{\prime}\) is positive, thus \(R\) is increasing. This completes the proof of property (ii). Computing the second derivative of \(R\), we get

\[R^{\prime\prime}(x)=\frac{\sigma^{\prime\prime}(x)(e^{\beta x}+1)-2\beta \sigma^{\prime}(x)+\beta^{2}\sigma(x)}{e^{\beta x}}=\beta\left(\frac{-\sigma^ {\prime}(x)+\beta\sigma(x)}{e^{\beta x}}\right).\] (182)

What remains to show is that \(\beta\sigma(x)>\sigma^{\prime}(x)\). Using the fundamental theorem of calculus, we get

\[\frac{\log(y+1)}{y}=\frac{1}{y}\int_{0}^{y}\frac{1}{t+1}dt>\frac{1}{y+1}\] (183)

since \(1/(y+1)\) is a lower bound of the integrand which completes the proof of the property (iii). Let us prove the property (iv) by taking the derivative of the function of interest

\[\frac{d}{dx}\frac{\sigma^{\prime}(x)x}{\sigma(x)}=\frac{(\sigma^{\prime\prime }(x)x+\sigma^{\prime}(x))\sigma(x)-\sigma^{\prime}(x)^{2}x}{\sigma(x)^{2}}\] (184)

Using \(\sigma^{\prime\prime}(x)=\beta\sigma^{\prime}(x)(1-\sigma^{\prime}(x))\) and dropping the positive term \(\sigma^{\prime}(x)\), the numerator of the derivative is

\[((1-\sigma^{\prime}(x))\beta x+1)\sigma(x)-\sigma^{\prime}(x)x =\left(\frac{\beta x}{e^{\beta x}+1}+1\right)\sigma(x)-\frac{e^{ \beta x}}{e^{\beta x}+1}x\] (185) \[=\frac{e^{\beta x}}{e^{\beta x}+1}\left(\frac{1}{e^{\beta x}}( \beta x+e^{\beta x}+1)\sigma(x)-x\right)\] (186)

For the case \(x\geq 0\), we have \(\sigma(x)>x\) and \((\beta x+1)/e^{\beta x}>0\), hence the derivative is positive. For the case \(x<0\), we want to show

\[\left(e^{\beta x}+\beta x+1\right)\frac{\log(e^{\beta x}+1)}{e^{\beta x}}>\beta x.\] (187)

If \(e^{\beta x}+\beta x+1>0\), it is done since the LHS is positive. If \(e^{\beta x}+\beta x+1\leq 0\), we have

\[\left(e^{\beta x}+\beta x+1\right)\frac{\log(e^{\beta x}+1)}{e^{\beta x}}\geq \left(e^{\beta x}+\beta x+1\right)\sup\frac{\log(e^{\beta x}+1)}{e^{\beta x}}\] (188)

since \(\log(e^{\beta x}+1)/e^{\beta x}\) is positive. We will next show that \(\log(e^{\beta x}+1)/e^{\beta x}\) is a decreasing function therefore its supremum is achieved at \(x\!\to\!-\infty\). From the integral expression in Eq. 183, we deduce that \(\log(y+1)/y\) is a decreasing function since adding smaller terms in the average decreases it. Thus the following limit gives us the supremum using L'Hopital's rule

\[\lim_{y\to 0}\frac{\log(y+1)}{y}=\lim_{y\to 0}\frac{1}{y+1}=1.\] (189)Combining it with the Eq. 188 after the substitution \(y=e^{\beta x}\), we get the desired Ineq. 187 which implies that the derivative is positive in this case too. This completes the proof of property (iv).

For the property (v), we first give a formula for the third derivative of softplus

\[\sigma^{\prime\prime\prime}(x)=\beta\sigma^{\prime\prime}(x)(1-2 \sigma^{\prime}(x)).\] (190)

\(\sigma^{\prime}\) is log-concave if and only if we have

\[\sigma^{\prime\prime\prime}(x)\sigma(x)<\sigma^{\prime\prime}(x) \sigma^{\prime}(x)\Leftrightarrow\] \[\beta\sigma^{\prime\prime}(x)(1-2\sigma^{\prime}(x))\sigma(x)< \sigma^{\prime\prime}(x)\sigma^{\prime}(x)\] (191)

which is equivalent to

\[(1-e^{\beta x})\log(e^{\beta x}+1)<e^{\beta x}.\] (192)

This is equivalent to \((1-y)\log(y+1)<\log(y+1)<y\) where \(y=e^{\beta x}>0\); the second inequality holds due to \(y+1<e^{y}\).