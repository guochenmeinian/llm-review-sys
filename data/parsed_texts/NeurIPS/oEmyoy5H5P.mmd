# Grounding and Validation of Algorithmic Recourse

in Real-World Contexts:

A Systematized Literature Review

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

The aim of algorithmic recourse (AR) is generally understood to be the provision of "actionable" recommendations to individuals affected by algorithmic decision-making systems, in an attempt to offer the capacity for taking actions that may lead to more desirable outcomes in the future. Over the past few years, AR literature has largely focused on theoretical frameworks to generate "actionable" counterfactual explanations that further satisfy various desiderata, such as diversity or robustness. We believe that algorithmic recourse, by its nature, should be seen as a practical problem: real-world socio-technical decision-making systems are complex dynamic entities involving various actors (end users, domain experts, civil servants, system owners, etc.) engaged in social and technical processes. Thus, research needs to account for the specificities of systems where it would be applied. To evaluate how authors envision AR "in the wild", we carry out a systematized review of 127 publications pertaining to the problem and identify the real-world considerations that motivate them. Among others, we look at the ways to make recourse (individually) actionable, the involved stakeholders, the perceived challenges, and the availability of practitioner-friendly open-source codebases. We find that there is a strong disconnect between the existing research and the practical requirements for AR. Most importantly, the grounding and validation of algorithmic recourse in real-world contexts remain underexplored. As an attempt to bridge this gap, we provide other authors with five recommendations to make future solutions easier to adapt to their potential real-world applications.

## 1 Introduction

Algorithmic decision-making (ADM) tools are frequently seen as a way to improve decision processes in a variety of high-stakes domains such as public administration [47; 146] or healthcare [45; 87]. Deep learning models have attracted much attention due to their perceived high performance, but the predictions of such models cannot be interpreted by humans, hence end users - both individuals subjected to algorithmic decisions and decision-makers operating on them - are placed in a position where they are unable to understand the grounds of a prediction, act on it, or trust it [159].

To help address this problem, a variety of explanation methods has been proposed. Of particular interest for this paper are counterfactual explanations (CEs) that attempt to explain the predictions for individual instances of data, taking the form of conditional statements such as "_if the value of feature \(x\) was \(a\) instead of \(b\), the model would have predicted class \(y\) instead of \(z\)"_. They are perceived to be an attractive approach to explanation that does not require "opening the black box" [151] and have been argued to align with the ways that humans naturally reason about events [84].

CEs are also seen as the go-to method for algorithmic recourse (AR), or the generation of actionable recommendations that provide people with the knowledge needed to achieve more desirable predictions in ADM systems. Recourse is distinct from the "explanation" or "justification" of algorithmic decisions, and more closely related to the notion of contestability of Artificial Intelligence [7] in that it aims not only to improve the _trust_ in the algorithm, but also embrace human _agency_[142].

Algorithmic recourse is an inherently practical problem in that it resembles a bureaucratic complaint process: an individual unhappy with some decision engages with a representative of the issuing organization, in an attempt to overturn it. Yet, we observe that much of the existing work is highly theoretical, with little consideration of whether it could be applied in organizational settings [see also 18]. Deploying AR in realistic systems without analyzing its mechanics in a broader context and without knowing what types of dynamics are expected to arise is bound to lead to unanticipated outcomes. Many of them will be undesirable and even potentially unsafe, and impossible to validate with respect to a set of requirements because the requirements for AR are _necessarily_ socio-technical.

**Societal and institutional components of algorithmic recourse are the focal point of our work**, as we look beyond the typical technical considerations to assess the practical aspects of the problem.

To that end, we contribute a _systematized review_ of 127 publications that address the goals of algorithmic recourse and we evaluate to what extent they incorporate such practical considerations. We characterize our approach as _systematized_ because we follow a fully systematic approach to the collection of publications, but their selection is not necessarily exhaustive [46] as many impactful ideas in computer science are published only in the form of pre-prints. Based on our analysis, we also provide other authors with five recommendations on how to improve the practicality of AR research.

The rest of the manuscript is structured as follows. In Section 2 we elaborate on the background of our work. Then, in Section 3 we describe our approach to this review. Next, Section 4 introduces our findings. Section 5 provides a discussion of our results, introduces our recommendations, and addresses the limitations of the current work. Finally, Section 6 forms the conclusion to this paper.

## 2 Background

### On algorithmic recourse

Algorithmic - or actionable, individual - recourse was introduced in [138] as _"the ability of a person to change the decision of the model through actionable input variables"_, building on the earlier work of [151] who argued that CEs are a psychologically-grounded way to (1) help decision-subjects understand an algorithmic decision, (2) provide them with information needed to contest it, and (3) inform about actions that could be taken to overturn it. For instance, consider a person who has unsuccessfully applied for a loan; they may then receive AR such as _"if you requested $5000 less, you would qualify for this loan"_. The key consideration for AR is "actionability", which entails that the recipient of the recommendation should be capable of implementing it. If they had been informed _"if you were 10 years younger, you would qualify for the loan"_, they would have still received a valid CE, _but not_ recourse. More recently [69] has recast the problem as reasoning about minimal interventions on the structural causal model. This formulation (at least theoretically) addresses an important shortcoming of "correlational" recourse. Without accounting for the downstream causal effects of actions, an individual may exert more effort than necessary and still fail to achieve the target outcome. Indeed, counterfactuals are an inherently causal concept [103].

We note that problems similar to AR have been studied under a variety of different names: _actionable knowledge discovery_ [e.g., 2], _action rules mining_ [e.g., 110], _inverse classification_ [e.g., 5], _why not questions_ [e.g., 58], or _actionable feature tweaking_[134]. These alternative formulations have generally focused on "business" knowledge, rather than individual recommendations, but ultimately the goal of all these approaches is to extract information from a (black-box) model that allows the user - an individual or a decision-maker - to act. We highlight them to emphasize that AR does not have to be achieved through the means of CEs. Rather CEs should be seen as _one of the means_ to achieve AR, particularly promising in that they do not require expert-level understanding of the model to be useful. Nonetheless, we decide to distinguish between the literature on AR (commonly equated with actionable CEs), and these alternative formulations in our work.

Existing research has generally considered AR in simplistic settings that are far removed from real-world socio-technical decision-making systems, where it would be implemented as a process.

For example, such systems are dynamic [113; 137], must support the implementation of AR at scale [9; 94], and involve various stakeholders beyond the end users [17; 151]. Moreover, if the intended goal of AR is to help individuals subjected to algorithmic decisions in an effective manner, research must entail a rich understanding of "actionability" to account for the differences between them [142].

### On the position of our review

Several groups of authors have previously surveyed the landscape of counterfactual explanations in general, and algorithmic recourse specifically. Perhaps the most relevant to our work is [71], which discusses five deficits of research on CEs, with a special focus on the (lack of) psychological grounding. Another pertinent publication is [70], which attempts to unify the definitions and formulations of AR in existing literature, but the work primarily focuses on technical aspects. Next, [143] develops a rubric to compare counterfactual explainers (equated with AR) and identifies 21 research challenges. While these also remain mostly technical, several of them are relevant to our work, for instance, CEs _"as an interactive service to the applicants"_ or reinforcing _"the ties between machine learning and regulatory communities"_. More recently, [48] reviewed and benchmarked a number of CE generators, but AR is only a secondary consideration in the work. We also highlight [130], which is the only systematic review of counterfactual and contrastive approaches to date. The authors understand CEs as a way to justify model predictions (i.e., they are different from AR). We agree with this distinction in that CEs can be useful for reasons other than recourse, such as model debugging [e.g., 1; 122]. Finally, although not reviews, [13] and [142] are particularly relevant to our work, offering critical perspectives on AR and addressing multiple shortcomings of recourse literature.

## 3 Methods

In this section, we briefly discuss our approach to the literature review following the SALSA - Search, Appraisal, Synthesis, Analysis - framework introduced in [46]. We also provide a more detailed description to allow for the reproduction of our process in the supplementary materials. Figure 1 presents our process in the form of a PRISMA flow diagram [97].

### Search

We make use of three search engines to collect the initial set of studies: ACM Digital Library, IEEE Xplore, and SCOPUS. Given the previously mentioned blurry distinction between AR and CEs, we consider the papers discussing either problem. In a small scoping review, we identify several keywords common to publications on recourse, as well as several equivalent terms to build the query. We search in titles, abstracts, and keywords, arriving at 3092 records after de-duplication. To facilitate the screening process, we employ the open-source ASReview tool, which makes use of an active learning approach to re-order the set of publications, such that the most relevant ones are always "at the top of the stack" [139]. The researchers behind the tool suggest employing a stopping rule measured in the number of consecutive irrelevant records, which we set to 30, or 1% of the entire dataset. We accept all papers that focus on algorithmic recourse and counterfactual explanations, completing the screening after evaluating 1040 abstracts, leading to 499 relevant records.

We observe that some important publications may be missing from our results. For instance, [151] was published in a legal journal that is not indexed by computer science search engines. Thus, we decide to augment the set of records by applying snowballing, which has been shown as a good alternative to databases in systematic reviews in software engineering [162]. We collect the references for the top 50 (10%) "most impactful" publications, measured by the number of citations. While this introduces several pre-prints into our result set [52; 61; 91; 113; 143; 150], we decide not to exclude them. Our review remains primarily concerned with peer-reviewed work. After adding the snowballed references to our dataset, we are left with 2018 records for the second screening with ASReview. This time, we look for publications that specifically refer to the problem of AR, "actionable" CEs, or modifying outcomes of automated decision-making systems. We employ a stricter stopping rule to minimize the risk of false negatives, completing the screening after 60 consecutive irrelevant records with 203 records considered for full-text appraisal. To allow for complete reproducibility of the search process, we provide an extended discussion (including queries) in the technical Appendix A.

### Appraisal

We were able to retrieve all of the remaining 203 documents. For each document, we require that the authors explicitly cite recourse as the center of interest, or look at **(1)** explanations **(2)** provided for individual instances **(3)** with the goal of acting upon them **(4)** in an attempt to modify the predictions **(5)** of a classification model. We exclude 51 publications as they are not on topic, primarily because they focus on CEs for the sake of explanation. Four works in this category look at (what they call) recourse but extend the problem to settings beyond the scope of this review: recommender systems [31, 43, 145], text classification [37], and anomaly detection [27]. Further 15 publications are duplicates, typically pre-prints of other documents that were included in the review. Next, 8 documents were published before [151] that sparked the research on AR, and thus we exclude them as well. These look at the alternative formulations discussed earlier in Section 2.1. Finally, 2 documents are not publications: one is an abstract of a talk, and the other is a student poster. For each document, we answer a number of questions relating to the practical considerations introduced by the authors.

### Synthesis

To compile the results we carry out a standard thematic content analysis following the approach presented in [40]. First, we explore the data extracted from the set of publications relevant to each question to find the commonalities, which serves as the grounds for creating the initial set of codes. We evaluate the documents against these codes and keep track of any other considerations. If such considerations appear in multiple documents, we create new codes for them. Afterward, we re-evaluate all documents against the new code. As the coding exercise is carried out by one author, they do a third pass over all documents to double-check for potential errors. Finally, where relevant, we cluster the codes into larger themes. In this analysis we only look at the explicit statements provided by the authors, we do not attempt to infer their understanding of the problem. Thus, the numbers provided in Section 4 should be understood as describing how algorithmic recourse is _discussed_ in the literature. For brevity, we focus our discussion on the main themes, but we still highlight specific publications if we observe that the authors introduce novel, highly relevant considerations that do not fit into other themes. Finally, even though we also evaluated the technical aspects of the proposed solutions - requirements for methods and datasets used in evaluations - they are not covered in this review. Instead, we point the interested readers to [48, 70, 143].

Figure 1: Identification of studies via databases and snowballingResults

The following nine sections introduce the results of the thematic analysis. For each question, we explain why it is relevant to the analysis and examine the main themes. We also highlight highly important but underexplored themes. We start with the general points such as contributions and definitions in Sections 4.1 to 4.3. Then, in Sections 4.4 to 4.7 we investigate the societal components of AR research. Finally, in Sections 4.8 and 4.9 we look at the aspects relevant to practitioners.

### What types of contributions do the authors choose to make to the AR research?

We start by looking at the main goals of the collected publications to validate our assumption that AR literature is primarily concerned with technical solutions. We annotate each entry with at most two codes based on the form of contributions. By far the largest group is _propose methods_, which applies to 88 (69.3%) out of the 127 publications. These are primarily generators for individual CEs, but we also find 18 (14.2%) documents that propose other methods. Next, 20 (15.7%) publications _develop theoretical frameworks_, for instance by grounding AR in user studies or providing critical perspectives on the problem. Further, 15 (11.8%) focus on _empirical or theoretical analyses_ of the properties of AR and another 15 publications _apply_ it in a variety of domains. We did not identify any applications evaluated with humans in the loop. Then, 5 (3.9%) publications _benchmark_ existing methods, while 3 (2.4%) _review_ them. We make our annotations available in technical Appendix B.

### What are the criteria covered in the authors' definitions of AR?

We also evaluate what is understood as the problem to be addressed by AR mechanisms. In particular, what are the criteria to satisfy authors' definitions of recourse. A similar question was posed by [70] who combined six definitions into _"recourse can be achieved by an affected individual if they can understand and accordingly act to alleviate an unfavorable situation, thus exercising temporally-extended agency"_, but this approach was far from systematic. Instead, we are interested in the underlying concepts. 74 (58.3%) publications explicitly define AR, 16 (12.6%) mention it but do not include a definition, while 37 (29.1%) do not mention AR, even though they align with its (overall) goals. The most common theme is _overturning undesirable decisions_, present in 47 definitions (63.5% of all definitions), but specifically _overturning algorithmic decisions_ is mentioned only 43 (58.1%) times. It is generally understood that _AR is provided to affected individuals_ (44, or 59.5%) but 4 (5.4%) definitions _consider stakeholders_ more broadly. _Actionability_ as a requirement for recourse is noted in only 39 (52.7%) definitions. Then, 20 (27.0%) publications specifically mention counterfactual explanations as means to AR, while 26 (35.1%) include various other technical considerations in the definitions, such as "changes to actionable input variables" or "desired classes".

We also point to several themes that are, interestingly, underrepresented. Only 18 (24.3%) documents mention _explanation, justification, or understanding of a decision_ as the pre-requisite for AR. Next, 10 (13.5%) highlight _future-orientation or other temporal aspects_ of the provided recommendations. Although _"consequential settings"_, typically bank lending, are given as examples in nine (12.2%) definitions, they are never explicitly mentioned as the scenarios where recourse ought to be provided, which may be akin to the "enjoyment of recourse" as defined by [142] where people are aware that there exists a way to reverse undesirable decisions.18 publications (10.8%) promote _AR as an ability_. Finally, only 2 (2.7%) publications require that recourse accounts for the _preferences_ of its recipients.

Footnote 1: Financial domain dominates the evaluations as well, with 90 of 116 evaluations on non-synthetic data making use of at least one finance-related dataset, most commonly German Credit Data [59] with 51 uses.

### What are the criteria covered in the authors' definitions of actionability?

As we observe, "actionability" is a concept that underpins AR but we discover that, in general, its understanding is limited. 91 (71.6%) publications attempt to define what it means (for a CE) to be actionable. Most commonly, in 48 (52.7%) out of 91 definitions, it is understood as _acting only on directly-mutable features_, 6 (6.6%) distinguish that _features may be indirectly-mutable_ but still not actionable, while 22 (24.2%) also highlight that _feature values may need to be constrained_. Next, 19 (20.9%) definitions rely on a tautology that actionability means _people can take actions_, 11 (12.1%) emphasize that these _actions must be successful or lead to change_, and 3 (3.3%) further require that they are _aligned with people's real-world objectives_. Only 14 (15.4%) definitions put usersat the center stage, indicating that actionability _depends on the user or their preferences_, while 2 (2.2%) highlight the _importance of the context_[144; 156], for instance, that the ability to act on a recommendation may change over time. Importantly, ethical considerations are never mentioned as the pre-requisite for actionability, but we find some broader discussions about this [e.g., 142].

### What is the role of end users? What other stakeholders are envisioned in the AR process?

Given that AR is to be implemented in socio-technical systems that include a variety of actors, we are interested in the types of stakeholders acknowledged in the literature. A total of 105 publications provide explicit consideration of this type. In general, end users subject to algorithmic decisions are envisioned to be the recipients of AR, but this is not always the case: it may also be provided to experts [e.g., 21; 22; 76] or organizations [e.g., 65; 72; 147], which highlights that in some cases AR may be carried out on behalf of the affected individuals. In any case, 47 (44.8%) publications in the subset agree that end users should inform actionability, but it is rarely clear _how_ these preferences should be specified. User-friendly (interactive) interfaces are a consideration in only 14 (13.3%) documents. A total of 29 (27.6%) publications envision domain experts as someone who inform the recourse process. They are either expected to inform actionability in the AR system or provide other forms of knowledge, typically in the form of a causal structure. Besides the experts, authors of 35 (33.3%) papers have discussed a variety of stakeholders. Most commonly system owners [e.g., 20; 34; 38; 89], but also auditors [e.g., 138; 158], data scientists [e.g., 28; 82], developers [e.g., 22; 131], practitioners [e.g., 100; 156], regulators [e.g., 28; 120], or even potential attackers [102].

### What types of real-world considerations motivate existing research?

With the multitude of challenges that stand ahead of real-world AR, we are interested in the considerations that motivate existing work. The main theme we find is _ensuring proper individual actionability_, which is addressed in 46 (37.4%) of 123 publications relevant to this question. This is typically achieved with the encoding of user preferences as constraints, but other means include providing diverse CEs. In fact, _tackling specific desiderata for AR_ (beyond actionability) is the second largest area of research with 28 (22.8%) publications. Various _other technical challenges_ are considered in 24 (19.5%) documents, for example, integrating background knowledge [e.g., 16; 62; 64; 98], or incorporating feature importance [e.g., 4; 6; 96; 116]. We also find 19 (15.4%) publications that discuss the problem of _communicating recourse to the end users_. 16 (13.0%) focus on the _dynamics of real-world systems_, typically addressing the robustness of AR [e.g., 75; 91; 93; 137], while 14 (11.4%) look at recourse in _multi-agent systems_. This also relates to _performance considerations_ emphasized in 15 (12.2%) of documents. _Causality_ drives research in 14 (11.4%) cases. We also find several themes that are under-emphasized: only 9 (7.3%) publications are directly _motivated by research in psychology_, while _ethics of AR_ are emphasized in only 7 (5.7%) documents.

### What types of real-world considerations are seen as challenges for future work?

While the previous section looked at the considerations that drive existing research, in this section we distill the recommendations for _future_ research going beyond the improvement of own work, which are provided in 74 documents. _Causality_ is highlighted as a challenge in 22 (29.7%) of them, while _other technical considerations_ are given in 20 (27.0%) cases. These range from robustness [e.g., 51; 117; 137], support for categorical features [e.g., 36; 157], or distinguishing between valid CEs and adversarial examples [101]. Next, 19 (25.6%) documents highlight the importance of _ensuring proper individual actionability_, which also relates to _communicating recourse to the end users_ (9, or 12.2%) and _supporting realistic cost functions_ (8, or 10.8%). _Ethics of AR_ are highlighted in 11 (14.9%) publications, for example, that AR research may detract from other obligations of model owners [77; 133]. The same number of publications emphasize the need to (1) _ground research in user studies_, and (2) accommodate for the _dynamics of real-world systems_. _Privacy or security_ is highlighted in 10 (13.5%) documents, while the _abuse of recourse_, such as strategic behaviors, surfaces in 7 (9.4%) papers. Other challenges include improving _performance_ (8, or 10.8%), considering _multi-agent systems_ (4, or 5.4%), and developing _legal frameworks_ (4, or 5.4%) for recourse. We also highlight several challenges particularly relevant to our work: (the usefulness of) recourse is perceived as difficult to evaluate in practice [41; 60; 115], it must account for individual, contextual, societal, and even cultural factors [123], which further means that engagement with recourse mechanisms and the likelihood of its implementation are context-dependent [e.g., 6; 42; 128].

### What types of (emergent) group-level dynamics are addressed in the existing research?

Real-world systems entail the implementation of recourse by more than one agent, which may introduce group-level dynamics. Nonetheless, out of 119 documents relevant to this question, 93 (78.2%) seem to understand recourse as a purely individual phenomenon. Among the remaining 26 documents we find considerations for several different group-level effects. Various perspectives on the problem of fair AR, covering both individual and group formulations are addressed by [12, 36, 52, 120, 121, 131, 149, 154]. Next, [9] shows that the implementation of AR on a large scale may lead to domain and model shifts, which introduce unexpected costs for the stakeholders.2 In [42] the authors focus on another negative consequence of AR at scale, showing that it may reinforce social segregation. The impact of the "right to be forgotten", where data deletion requests trigger model retraining that may invalidate existing recourses is addressed in [75]. Then, [94] develop a game-theoretic framework for AR in multi-agent settings, attempting to optimize for "social welfare" rather than the profits of individual agents. We find two further similar perspectives on recourse: [38] proposes auditing and subsidies to minimize the risks of strategic behaviors in a multi-agent setting, while [136] attempts to incentivize actual improvement for a population of agents. Finally, [65] provides a framework that generates transparent and consistent recourses for a sub-population. We also note two other lines of research that account for the remaining documents with group-level considerations. First, in a causal setting [e.g., 68, 73] subpopulations are necessary to estimate the interventional effects on individuals. Second, several works highlight the importance of global insights into the data [22, 41, 44, 78, 108, 112, 152], such as recourse summaries [78, 112].

Footnote 2: Such “endogenous dynamics” were postulated earlier in the first version of [113] dated December 22\({}^{\text{nd}}\)[2020], but this discussion has been completely removed from the subsequent versions of the pre-print.

### What are the approaches to the realistic evaluation of proposed methods?

We now explore the different forms of "real-world" evaluations, going beyond quantitative experiments, which are present in 51 publications. Most commonly, in 28 (54.9%) of those, the authors make use of _case studies_ presenting the methods in an end-to-end manner. Among those, the application of recourse in the Hired.com marketplace goes furthest in simulating real-world conditions for AR [89], but the recommendations are still not evaluated with humans in the loop. Further, 9 (17.6%) documents include other forms of _short walk-through examples_. We also identify 14 (27.5%) papers that evaluate the methods with _user experiments_, 10 of which involve non-expert users and 4 involve expert users. While we do not observe any interviews with non-expert users, we find 1 (2.0%) publication where _experts are interviewed_[22]. _Other involvement of non-experts_ applies to [116], where they inform the development of methods. _Other involvement of experts_ is featured in two documents where they evaluated the outputs of methods [25, 132]. Altogether, end users were involved in 17 publications, which is only 13.3% of all publications covered in our study, even more striking than the 21% of CE methods evaluated with user studies as reported in [71].

### What are the open source and documentation practices in AR research?

Finally, we note that the lack of availability of well-documented open-source code may be an important obstacle to the application of AR in real-world systems. For all 116 publications that involve some form of computational experiments, we verify whether the source code is publicly available. If the authors do not explicitly link to their code in the paper, we attempt to find it independently. Ultimately, we collect open-source implementations for 64 (55.2%) publications. Then, for each of them, we evaluate the quality of documentation. The _instructions on the general usage_ (such as installation and workflow) are provided with 27 (41.5%) repositories, while _instructions on the reproduction of results_ in 23 (35.4%). In 19 (29.2%) cases we find _walk-through tutorials_, typically in the form of Jupyter Notebooks, although we note that they differ in quality. For instance, 5 repositories include code-only notebooks with no further textual explanation that could guide the practitioner. Implementations for 4 papers include more "professionalized" _documentation_[9, 86, 100, 156]. The latter sets a golden standard as it further includes a tutorial video and a live demo. We do not find _any_ additional materials for practitioners for 13 (20.0%) of the available implementations.

Discussion

Regardless of whether AR can be normatively expected or not [77], many systems can genuinely benefit from recourse mechanisms, especially when the interests of the system owner and the end users are aligned [72], such as in the healthcare system to improve the well-being of patients [76; 96; 155], or on the online platforms that attempt to improve the experience of their users [89; 134]. Nonetheless, the values and norms underlying recourse - trust, agency, fairness, safety, and so on - are emergent properties of systems where recourse mechanisms would be introduced. Such norms can only be understood and evaluated when accounting for the technical, social, and institutional components of the system [32], but the latter two remain largely unexplored in the recourse literature.

Recourse is not inherently safe or unsafe, _but_ its (incorrect) implementation may lead to the emergence of unsafe dynamics, such as the unexpected costs to stakeholders as discussed by [9] or the reinforcement of social segregation addressed in [42]. While it may be too challenging to provide accurate system-level evaluations at this stage of research, authors can still expand the boundaries of their analyses to account for global effects or look at the position of recourse mechanisms in the broader context of a complete socio-technical AI system [33]. As AR is a "reality-centric AI" problem [140] by its nature, working towards its integration into existing systems will require a design-oriented approach, potentially with _specific_ systems in mind. The "Abstraction Traps" discussed by [119] in the context of research on fair machine learning apply here: that technical solutions designed for one social context cannot be directly repurposed for another application, that values to which they are expected to adhere to cannot be captured with mathematical formulas, that their insertion into an existing process will impact its behavior, or that the best solutions may not necessarily be technical.

It is perhaps most telling that only 12% of surveyed publications attempt to apply recourse in realistic settings. We will discuss two of these settings to highlight the stark differences in system properties. Most of the applications included in our review focus on the provision of actionable individual recommendations to students [3; 4; 24; 109; 126; 135; 160]. In this relatively low-stakes domain almost any recourse will be actionable in that following a personalized set of learning activities does not require any resources other than time. Even then, the system involves multiple actors - students, teachers, parents - whose interactions will impact the process, for example, because students may fail to benefit from certain learning activities without additional support. Conversely, we find several publications where authors attempt to provide recourse in the high-stakes medical domain [76; 96; 155]. Here, recommendations must be tailored to the preferences, resources, or lifestyles of patients in order to have a chance of being actionable. Moreover, certain aspects of their implementation fully rely on other actors, such as a clinician prescribing the medications. Finally, it may happen that recourse does not exist at all when the outcomes of a patient cannot be improved.

### Recommendations for future research

We distill our findings into five key recommendations. First, in Sections 4.2, 4.3 we observed that _operational_ definitions for recourse are still unavailable. Second, Sections 4.4 and 4.8 underlined little consideration for people involved in recourse processes. Third, Sections 4.5, 4.6 highlighted the overwhelmingly technical approaches to recourse. Fourth, Section 4.7 stressed the lack of group-level analyses. Fifth, from Sections 4.8, and 4.9 we learned about the missing consideration of practitioners.

1. Broadening the scope of research.AR is generally seen as a service for affected individuals, but this formalization may be unnecessarily limiting. In fact, in many systems, these individuals may be unable to directly act on recommendations [see also 142]. Instead, we propose to operationalize the aim of AR as the provision of recommendations _aligned with the preferences of non-expert users_ in an attempt _to help them improve outcomes_ in an _ADM setting_, which emphasizes that providing _easy to understand_ and _individually actionable_ recommendations remains the key research problem.

2. Engaging end users, affected individuals, and communities.AR solutions are rarely evaluated with humans. Instead, they attempt to satisfy a variety of desiderata formulated by authors and assessed in an automated manner. Sparsity, proximity, or mutability of features are far from perfect proxies for individual actionability. For AR to be truly useful, it must be able to satisfy the preferences of its end users. Research is also necessary to learn about the needs of the affected individuals concerning recourse, and to validate its potential contributions and inherent limitations. Authors may also benefit from the rich literature on human-computer interaction [e.g., 11; 23] or psychology.

3. Accepting a socio-technical perspective.A pervasive assumption in the literature is that all challenges of AR require purely technical solutions. For instance, many authors emphasize the importance of causal modeling to guarantee recourse, but the models that aim to be explained are themselves _not_ causal. Similarly, to improve the performance of CE generators many authors turn to deep generative models [35; 42; 61; 67; 81; 90; 99]. Not only do they explain the data rather than the model [10], but more importantly they shift the problem from improving the trust in non-interpretable models, to attempting to trust non-interpretable explainers. Although a socio-technical perspective on AR brings its own challenges, such as accounting for the roles of stakeholders involved in the provision of recourse, it creates important opportunities. For example, developing "recourse contracts" [34; 39] or designing feedback processes to account for imperfect robustness.

4. Accounting for emergent effects.Decision-making systems involve multiple individuals who may be interested in receiving recourse and may have competing interests. Research on AR should, from the onset, explore group-level effects such as external costs or fairness. While this may require expanding the boundaries of analysis, it is necessary to anticipate the emergent outcomes of recourse. These may even occur due to the multi-system dynamics of AR: recommendations implemented by an individual to improve their outcomes in one system will affect them in other contexts [see also 13].

5. Attending to other operational aspects.Finally, the artifacts of AR research should be practitioner-friendly. On the one hand, this requires being explicit about the position of the proposed methods in a broader system, for example, in the form of end-to-end case studies that allow practitioners to better understand the benefits of the proposed solutions. On the other hand, this suggests that authors should attempt to move away from merely providing scripts for experiments, and focus on developing well-documented frameworks that can be adapted to different ADM systems.

### Limitations of our work

Our review is not without shortcomings. Most importantly, for each paper the extraction and coding of data was performed by a single author, which means that the quantitative results may be imperfect. We account for this by focusing the analysis on the _overarching themes_ represented in existing publications, thus, even if another researcher would have carried out the coding in a somewhat different manner, they should arrive at similar results and our analysis remains valid. Additionally, as our review ultimately looks at the authors' perception of recourse, we do not want to misconstrue their views. Thus, we do not infer any considerations unless they are provided explicitly. Our reading may be more strict than intended by the authors and the numbers reported in our results may be underestimated. At the same time, we believe that if certain considerations are deemed important by the researchers, they would choose to be explicit about them. Finally, although we followed a systematic process, we cannot claim that we collected AR literature in an exhaustive manner due to the specificities of computer science publishing. Thus, we acknowledge that there may exist some insightful publications addressing recourse that have not been covered in this literature review.

## 6 Conclusions

Algorithmic recourse concerns the provision of recommendations aligned with the preferences of non-expert users of algorithmic decision-making systems to help them achieve more desirable outcomes in the future. Existing research on the topic is predominantly theoretical, even though recourse, in expectation, is a real-world problem with strong practical implications. To that end, we conducted a systematized literature review of 127 publications that focus on algorithmic recourse, and more generally on actionable counterfactual explanations. We evaluated the practical considerations provided by the authors. Our findings indicate that, indeed, AR tends to be perceived as a (predominantly) technical problem. Although we think highly of fundamental research, we note that for algorithmic recourse to leave computer science labs, it must be more strongly grounded and validated in the real world, and consider the requirements for systems that include not only technical but also social and institutional components. To help bridge this gap, we synthesize a list of five recommendations for other authors that aim to reinforce recourse as a practical problem. We believe that AR should not be seen as only a simple ad-hoc solution to improve the acceptance of black-box models in consequential domains, but rather as a full-fledged socio-technical mechanism that can benefit many systems and improve the agency of affected individuals and decision-makers across a variety of settings.

## References

* Abibak et al. [2022] Abubakar Abid, Mert Yukekgonul, and James Zou. Meaningfully Debugging Model Mistakes using Conceptual Counterfactual Explanations. In _Proceedings of the 39th International Conference on Machine Learning_, volume 162 of _Proceedings of Machine Learning Research_, pages 66-88. PMLR, 17-23 Jul 2022. URL https://proceedings.mlr.press/v162/a bid22a.html.
* Adomavicius and Tuzhilin [1997] Gediminas Adomavicius and Alexander Tuzhilin. Discovery of Actionable Patterns in Databases: The Action Hierarchy Approach. In _Proceedings of the Third International Conference on Knowledge Discovery and Data Mining_, KDD'97, page 111-114. AAAI Press, 1997.
* ICCS 2023_, volume 14074 LNCS, pages 413-420. Springer Nature Switzerland, 2023. doi: 10.1007/978-3-031-36021-3_44.
* Afzaal et al. [2021] Muhammad Afzaal, Jalal Nouri, Aayesha Zia, Panagiotis Papapetrou, Uno Fors, Xiu Wu, Yongchaoand Li, and Rebecka Weegar. Automatic and Intelligent Recommendations to Support Students' Self-Regulation. In _2021 International Conference on Advanced Learning Technologies (ICALT)_, pages 336-338, July 2021. ISBN 2161-377X. doi: 10.1109/ICALT522 72.2021.00107.
* Aggarwal et al. [2010] Charu C. Aggarwal, Chen Chen, and Jiawei Han. The Inverse Classification Problem. _Journal of Computer Science and Technology_, 25:458-468, 2010.
* Albini et al. [2022] Emanuele Albini, Jason Long, Danil Dervovic, and Daniele Magazzeni. Counterfactual Shapley Additive Explanations. In _Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency_, FAccT '22, page 1054-1070, New York, NY, USA, 2022. Association for Computing Machinery. ISBN 9781450393522. doi: 10.1145/3531146.3533168. URL https://doi.org/10.1145/3531146.3533168.
* Alfrink et al. [2022] Kars Alfrink, Ianus Keller, Gerd Kortuem, and Neelke Doorn. Contestable AI by design: Towards a framework. _Minds and Machines_, pages 1-27, 2022.
* Alotaibi and Singh [2023] Hissah Alotaibi and Ronal Singh. Metrics for Evaluating Actionability in Explainable AI. In _PRICAI 2023: Trends in Artificial Intelligence_, pages 481-487. Springer Nature Singapore, 2023. ISBN 978-981-99-7022-3.
* Altmeyer et al. [2023] Patrick Altmeyer, Giovan Angela, Aleksander Buszydlik, Karol Dobiczek, Arie van Deursen, and Cynthia C. S. Liem. Endogenous Macrodynamics in Algorithmic Recourse. In _2023 IEEE Conference on Secure and Trustworthy Machine Learning (SaTML)_, pages 418-431, 2023. doi: 10.1109/SaTML54575.2023.00036.
* Altmeyer et al. [2024] Patrick Altmeyer, Mojtaba Farmanbar, Arie van Deursen, and Cynthia C. S. Liem. Faithful Model Explanations through Energy-Constrained Conformal Counterfactuals. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 38, pages 10829-10837, 2024.
* Amershi et al. [2014] Saleema Amershi, Maya Cakmak, William Bradley Knox, and Todd Kulesza. Power to the People: The Role of Humans in Interactive Machine Learning. _AI Magazine_, 35(4):105-120, 2014.
* Artelt et al. [2021] Andre Artelt, Valerie Vaquet, Riza Velioglu, Fabian Hinder, Johannes Brinkrolf, Malte Schilling, and Barbara Hammer. Evaluating Robustness of Counterfactual Explanations. In _2021 IEEE Symposium Series on Computational Intelligence (SSCI)_, pages 01-09, December 2021. doi: 10.1109/SSCI50451.2021.9660058.
* Barocas et al. [2020] Solon Barocas, Andrew D. Selbst, and Manish Raghavan. The Hidden Assumptions Behind Counterfactual Explanations and Principal Reasons. In _Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency_, FAT* '20, page 80-89, New York, NY, USA, 2020. Association for Computing Machinery. ISBN 9781450369367. doi: 10.1145/3351095.3372830. URL https://doi.org/10.1145/3351095.3372830.

* Barzekar and McRoy [2023] Hosein Barzekar and Susan McRoy. Achievable Minimally-Contrastive Counterfactual Explanations. _Machine Learning and Knowledge Extraction_, 5(3):922-936, 2023. doi: 10.3390/make5030048.
* Beckers [2022] Sander Beckers. Causal Explanations and XAI. In Bernhard Scholkopf, Caroline Uhler, and Kun Zhang, editors, _Proceedings of the First Conference on Causal Learning and Reasoning_, volume 177 of _Proceedings of Machine Learning Research_, pages 90-109. PMLR, 11-13 Apr 2022. URL https://proceedings.mlr.press/v177/beckers22a.html.
* Berman et al. [2022] Alexander Berman, Ellen Breitholtz, Christine Howes, and Jean-Philippe Bernardy. Explaining Predictions with Enthymematic Counterfactuals. In _CEUR Workshop Proceedings_, volume 3319, pages 95-100. CEUR-WS, 2022.
* Bhatt et al. [2020] Umang Bhatt, Alice Xiang, Shubham Sharma, Adrian Weller, Ankur Taly, Yunhan Jia, Joydeep Ghosh, Ruchir Puri, Jose M. F. Moura, and Peter Eckersley. Explainable Machine Learning in Deployment. In _Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency_, FAT* '20, page 648-657, New York, NY, USA, 2020. Association for Computing Machinery. ISBN 9781450369367. doi: 10.1145/3351095.3375624. URL https://doi.org/10.1145/3351095.3375624.
* Birhane et al. [2022] Abeba Birhane, Pratyusha Kalluri, Dallas Card, William Agnew, Ravit Dotan, and Michelle Bao. The Values Encoded in Machine Learning Research. In _Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency_, FAccT '22, page 173-184, New York, NY, USA, 2022. Association for Computing Machinery. ISBN 9781450393522. doi: 10.1145/3531146.3533083. URL https://doi.org/10.1145/3531146.3533083.
* Carreira-Perpinan and Hada [2021] Miguel A. Carreira-Perpinan and Suryabhan Singh Hada. Counterfactual Explanations for Oblique Decision Trees: Exact, Efficient Algorithms. _Proceedings of the AAAI Conference on Artificial Intelligence_, 35:6903-6911, May 2021. doi: 10.1609/aaai.v35i8.16851. URL https://ojs.aaai.org/index.php/AAAI/article/view/16851.
* Chen et al. [2020] Yatong Chen, Jialu Wang, and Yang Liu. Strategic Recourse in Linear Classification. In _Workshop on Consequential Decision Making in Dynamic Environments_, 2020.
* Chen et al. [2024] Ziheng Chen, Fabrizio Silvestri, Gabriele Tolomei, Jia Wang, He Zhu, and Hongshik Ahn. Explain the Explainer: Interpreting Model-Agnostic Counterfactual Explanations of a Deep Reinforcement Learning Agent. _IEEE Transactions on Artificial Intelligence_, 5(4):1443-1457, 2024. doi: 10.1109/TAI.2022.3223892.
* Cheng et al. [2021] Furui Cheng, Yao Ming, and Huamin Qu. DECE: Decision Explorer with Counterfactual Explanations for Machine Learning Models. _IEEE Transactions on Visualization & Computer Graphics_, 27(02):1438-1447, feb 2021. ISSN 1941-0506. doi: 10.1109/TVCG.2020.3030342.
* Cheng et al. [2019] Hao-Fei Cheng, Ruotong Wang, Zheng Zhang, Fiona O'Connell, Terrance Gray, F. Maxwell Harper, and Haiyi Zhu. Explaining Decision-Making Algorithms through UI: Strategies to Help Non-Expert Stakeholders. In _Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems_, CHI '19, page 1-12, New York, NY, USA, 2019. Association for Computing Machinery. ISBN 9781450359702. doi: 10.1145/3290605.3300789. URL https://doi.org/10.1145/3290605.3300789.
* Cohausz [2022] Lea Cohausz. Towards Real Interpretability of Student Success Prediction Combining Methods of XAI and Social Science. In _Proceedings of the 15th International Conference on Educational Data Mining_, pages 361-367, Durham, United Kingdom, July 2022. International Educational Data Mining Society. ISBN 978-1-7336736-3-1. doi: 10.5281/zenodo.6853069.
* Crupi et al. [2022] Riccardo Crupi, Alessandro Castelnovo, Daniele Regoli, and Beatriz San Miguel Gonzalez. Counterfactual Explanations as Interventions in Latent Space. _Data Mining and Knowledge Discovery_, 2022. doi: 10.1007/s10618-022-00889-2.
* PPSN XVI_, pages 448-469, Cham, 2020. Springer International Publishing. ISBN 978-3-030-58112-1. doi: 10.1007/978-3-030-58112-1_3.
* Datta et al. [2022] Debanjan Datta, Feng Chen, and Naren Ramakrishnan. Framing Algorithmic Recourse for Anomaly Detection. In _Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining_, KDD '22, page 283-293, New York, NY, USA, 2022. Association for Computing Machinery. ISBN 9781450393850. doi: 10.1145/3534678.3539344. URL https://doi.org/10.1145/3534678.3539344.
* Davis et al. [2022] Randall Davis, Andrew W. Lo, Sudhanshu Mishra, Arash Nourian, Manish Singh, Nicholas Wu, and Ruixun Zhang. Explainable Machine Learning Models of Consumer Credit Risk. _Journal of Financial Data Science_, 5(4):9-39, 2022. doi: 10.3905/jfds.2023.1.141.
* An agnostic method of counterfactual, selected, and social explanations for classification models. _Expert Systems with Applications_, 228:120373, 2023. ISSN 0957-4174. doi: https://doi.org/10.1016/j.eswa.2023.120373. URL https://www.sciencedirect.com/science/article/pii/S0957417423008758.
* De Toni et al. [2023] Giovanni De Toni, Bruno Lepri, and Andrea Passerini. Synthesizing explainable counterfactual policies for algorithmic recourse with program synthesis. _Machine Learning_, 112(4):1389-1409, 2023. ISSN 0885-6125. doi: 10.1007/s10994-022-06293-7.
* Dean et al. [2020] Sarah Dean, Sarah Rich, and Benjamin Recht. Recommendations and User Agency: The Reachability of Collaboratively-Filtered Information. In _Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency_, FAT* '20, page 436-445, New York, NY, USA, 2020. Association for Computing Machinery. ISBN 9781450369367. doi: 10.1145/3351095.3372866. URL https://doi.org/10.1145/3351095.3372866.
* Dobbe and Wolters [2024] Roel Dobbe and Anouk Wolters. Toward Sociotechnical AI: Mapping Vulnerabilities for Machine Learning in Context. _Minds and Machines_, 34(2):1-51, 2024.
* Dobbe et al. [2021] Roel Dobbe, Thomas Krendl Gilbert, and Yonatan Mintz. Hard choices in artificial intelligence. _Artificial Intelligence_, 300:103555, 2021. ISSN 0004-3702. doi: https://doi.org/10.1016/j.arti nt.2021.103555. URL https://www.sciencedirect.com/science/article/pii/S0004370221001065.
* Dominguez-Olmedo et al. [2022] Ricardo Dominguez-Olmedo, Amir-Hossein Karimi, and Bernhard Scholkopf. On the Adversarial Robustness of Causal Algorithmic Recourse. In _Proceedings of the 39th International Conference on Machine Learning_, volume 162 of _Proceedings of Machine Learning Research_, pages 5324-5342. PMLR, 17-23 2022.
* Downs et al. [2020] Michael Downs, Jonathan L. Chu, Yaniv Yacoby, Finale Doshi-Velez, and Weiwei Pan. CRUDS: Counterfactual Recourse Using Disentangled Subspaces. _ICML Workshop on Human Interpretability in Machine Learning_, pages 1-23, 2020.
* Ehyaei et al. [2023] Ahmad-Reza Ehyaei, Amir-Hossein Karimi, Bernhard Schoelkopf, and Setareh Maghsudi. Robustness Implies Fairness in Causal Algorithmic Recourse. In _Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency_, FAccT '23, page 984-1001, New York, NY, USA, 2023. Association for Computing Machinery. ISBN 9798400701924. doi: 10.1145/3593013.3594057. URL https://doi.org/10.1145/3593013.3594057.
* Zini and Awad [2022] Julia El Zini and Mariette Awad. Beyond Model Interpretability: On the Faithfulness and Adversarial Robustness of Contrastive Textual Explanations. In _Findings of the Association for Computational Linguistics: EMNLP 2022_, pages 1391-1402. Association for Computational Linguistics, 2022. doi: 10.18653/v1/2022.findings-emnlp.100.
* Estornell et al. [2023] Andrew Estornell, Yatong Chen, Sanmay Das, Yang Liu, and Yevgeniy Vorobeychik. Incentivizing Recourse through Auditing in Strategic Classification. In _Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence, IJCAI-23_, pages 400-408. International Joint Conferences on Artificial Intelligence, 8 2023. doi: 10.24963/ijcai.2023/45. URL https://doi.org/10.24963/ijcai.2023/45.
* Ferrario and Loi [2022] Andrea Ferrario and Michele Loi. The Robustness of Counterfactual Explanations Over Time. _IEEE Access_, 10:82736-82750, 2022. ISSN 2169-3536. doi: 10.1109/ACCESS.2022.3196917.

* Friese et al. [2018] Susanne Friese, Jacks Soratto, and Denise Pires de Pires. Carrying out a computer-aided thematic content analysis with ATLAS.ti. _IWMI Working Papers_, 18, 04 2018.
* Galhotra et al. [2021] Sainyam Galhotra, Rornila Pradhan, and Babak Salimi. Explaining Black-Box Algorithms Using Probabilistic Contrastive Counterfactuals. In _Proceedings of the 2021 International Conference on Management of Data_, SIGMOD '21, pages 577-590, New York, NY, USA, 2021. Association for Computing Machinery. ISBN 978-1-4503-8343-1. doi: 10.1145/3448016.3458455.
* Gao and Lakkaraju [2023] Ruijiang Gao and Himabindu Lakkaraju. On the Impact of Algorithmic Recourse on Social Segregation. In _Proceedings of the 40th International Conference on Machine Learning_, ICML'23. JMLR.org, 2023.
* Ghazimatin et al. [2020] Azin Ghazimatin, Oana Balalau, Rishiraj Saha Roy, and Gerhard Weikum. PRINCE: Provider-Side Interpretability with Counterfactual Explanations in Recommender Systems. In _Proceedings of the 13th International Conference on Web Search and Data Mining_, WSDM '20, pages 196-204, New York, NY, USA, 2020. Association for Computing Machinery. ISBN 978-1-4503-6822-3. doi: 10.1145/3336191.3371824.
* Gomez et al. [2020] Oscar Gomez, Steffen Holter, Jun Yuan, and Enrico Bertini. ViCE: Visual Counterfactual Explanations for Machine Learning Models. In _Proceedings of the 25th International Conference on Intelligent User Interfaces_, IUI '20, pages 531-535, New York, NY, USA, 2020. Association for Computing Machinery. ISBN 978-1-4503-7118-6. doi: 10.1145/3377325.3377536.
* Grant [2022] Crystal Grant. Algorithms Are Making Decisions About Health Care, Which May Only Worsen Medical Racism, October 2022. URL https://www.aclu.org/news/privacy-technology/algorithms-in-health-care-may-worsen-medical-racism. Accessed 22.05.2024.
* Grant and Booth [2009] Maria J. Grant and Andrew Booth. A typology of reviews: an analysis of 14 review types and associated methodologies. _Health Information & Libraries Journal_, 26(2):91-108, 2009. doi: https://doi.org/10.1111/j.1471-1842.2009.00848.x.
* Grimmelikhuijsen and Meijer [2022] Stephan Grimmelikhuijsen and Albert Meijer. Legitimacy of Algorithmic Decision-Making: Six Threats and the Need for a Calibrated Institutional Response. _Perspectives on Public Management and Governance_, 5(3):232-242, 03 2022. ISSN 2398-4910. doi: 10.1093/ppmg ov/gvac008. URL https://doi.org/10.1093/ppmgov/gvac008.
* Guidotti [2022] Riccardo Guidotti. Counterfactual Explanations and How to Find Them: Literature Review and Benchmarking. _Data Mining and Knowledge Discovery_, 2022. doi: 10.1007/s10618-022 -00831-6.
* Guidotti and Ruggieri [2021] Riccardo Guidotti and Salvatore Ruggieri. Ensemble of Counterfactual Explainers. In _Discovery Science: 24th International Conference, DS 2021, Halifax, NS, Canada, October 11-13, 2021, Proceedings_, pages 358-368, Berlin, Heidelberg, 2021. Springer-Verlag. ISBN 978-3-030-88941-8. doi: 10.1007/978-3-030-88942-5_28.
* Guidotti et al. [2022] Riccardo Guidotti, Anna Monreale, Salvatore Ruggieri, Francesca Naretto, Franco Turini, Dino Pedreschi, and Fosca Giannotti. Stable and Actionable Explanations of Black-Box Models through Factual and Counterfactual Rules. _Data Mining and Knowledge Discovery_, 2022. doi: 10.1007/s10618-022-00878-5.
* Guo et al. [2023] Hangzhi Guo, Feiran Jia, Jinghui Chen, Anna Squicciarini, and Amulya Yadav. RoCourseNet: Robust Training of a Prediction Aware Recourse Model. In _Proceedings of the 32nd ACM International Conference on Information and Knowledge Management_, CIKM '23, pages 619-628, New York, NY, USA, 2023. Association for Computing Machinery. ISBN 9798400701245. doi: 10.1145/3583780.3615040.
* Gupta et al. [2019] Vivek Gupta, Pegah Nokhiz, Chitradeep Dutta Roy, and Suresh Venkatasubramanian. Equalizing Recourse Across Groups. _arXiv_, 2019.

* Guyomard et al. [2021] Victor Guyomard, Francoise Fessant, Tassadit Bouadi, and Thomas Guyet. Post-hoc Counterfactual Generation with Supervised Autoencoder. In _Communications in Computer and Information Science_, volume 1524 CCIS, pages 105-114. Springer Science and Business Media Deutschland GmbH, 2021. doi: 10.1007/978-3-030-93736-2_10.
* Guyomard et al. [2023] Victor Guyomard, Francoise Fessant, Thomas Guyet, Tassadit Bouadi, and Alexandre Termier. Generating Robust Counterfactual Explanations. In _Machine Learning and Knowledge Discovery in Databases: Research Track. ECML PKDD 2023_, pages 394-409, Berlin, Heidelberg, 2023. Springer-Verlag. ISBN 978-3-031-43417-4. doi: 10.1007/978-3-031-43418-1_24.
* Hada and Carreira-Perpinan [2021] Suryabhan Singh Hada and Miguel A. Carreira-Perpinan. Exploring Counterfactual Explanations for Classification and Regression Trees. In _Communications in Computer and Information Science_, volume 1524 CCIS, pages 489-504. Springer Science and Business Media Deutschland GmbH, 2021. doi: 10.1007/978-3-030-93736-2_37.
* Haldar et al. [2022] Aparajita Haldar, Teddy Cunningham, and Hakan Ferhatosmanoglu. RAGUEL: Recourse-Aware Group Unfairness Elimination. In _Proceedings of the 31st ACM International Conference on Information & Knowledge Management_, CIKM '22, pages 666-675, New York, NY, USA, 2022. Association for Computing Machinery. ISBN 978-1-4503-9236-5. doi: 10.1145/3511808.3557424.
* Hardy et al. [2023] Ian Hardy, Jayanth Yetukuri, and Yang Liu. Adaptive Adversarial Training Does Not Increase Recourse Costs. In _Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society_, AIES '23, pages 432-442, New York, NY, USA, 2023. Association for Computing Machinery. ISBN 9798400702310. doi: 10.1145/3600211.3604704.
* He and Lo [2012] Zhian He and Eric Lo. Answering Why-not Questions on Top-k Queries. _2012 IEEE 28th International Conference on Data Engineering_, pages 750-761, 2012. doi: 10.1109/ICDE.201 2.8.
* Hofmann [1994] Hans Hofmann. Statlog (German Credit Data). UCI Machine Learning Repository, 1994. DOI: https://doi.org/10.24432/C5NC77.
* Hollig et al. [2023] Jacqueline Hollig, Aniek F. Markus, JJef de Slegte, and Prachi Bagave. Semantic Meaningfulness: Evaluating Counterfactual Approaches for Real-World Plausibility and Feasibility. In _Communications in Computer and Information Science_, volume 1902 CCIS, pages 636-659. Springer Science and Business Media Deutschland GmbH, 2023. doi: 10.1007/978-3-031-44067-0_32.
* Joshi et al. [2019] Shalmalmi Joshi, Oluwasanmi Koyejo, Warut Vijitbenjaronk, Been Kim, and Joydeep Ghosh. Towards Realistic Individual Recourse and Actionable Explanations in Black-Box Decision Making Systems. _arXiv_, 2019.
* K et al. [2022] Sarathi K, Shania Mitra, Deepak P, and Sutanu Chakraborti. Counterfactuals as Explanations for Monotonic Classifiers. In _CEUR Workshop Proceedings_, volume 3389, pages 177-188. CEUR-WS, 2022.
* Kanamori et al. [2021] Kentaro Kanamori, Takuya Takagi, Ken Kobayashi, and Hiroki Arimura. Distribution-Aware Counterfactual Explanation by Mixed-Integer Linear Optimization. _Transactions of the Japanese Society for Artificial Intelligence_, 36(6), 2021. doi: 10.1527/TJISA1.36-6_C-L44.
* Kanamori et al. [2021] Kentaro Kanamori, Takuya Takagi, Ken Kobayashi, Yuichi Ike, Kento Uemura, and Hiroki Arimura. Ordered Counterfactual Explanation by Mixed-Integer Linear Optimization. In _35th AAAI Conference on Artificial Intelligence, AAAI 2021_, volume 13A, pages 11564-11574. Association for the Advancement of Artificial Intelligence, 2021.
* Kanamori et al. [2022] Kentaro Kanamori, Takuya Takagi, Ken Kobayashi, and Yuichi Ike. Counterfactual Explanation Trees: Transparent and Consistent Actionable Recourse with Decision Trees. In _Proceedings of The 25th International Conference on Artificial Intelligence and Statistics_, volume 151, pages 1846-1870. PMLR, 2022.

* Karimi et al. [2020] Amir-Hossein Karimi, Gilles Barthe, Borja Balle, and Isabel Valera. Model-Agnostic Counterfactual Explanations for Consequential Decisions. In _Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics_, volume 108, pages 895-905. PMLR, 2020.
* Karimi et al. [2020] Amir-Hossein Karimi, Julius Von Kugelgen, Bernhard Scholkopf, and Isabel Valera. Algorithmic recourse under imperfect causal knowledge: a probabilistic approach. _Advances in Neural Information Processing Systems_, 33:265-277, 2020.
* Beyond Explainable AI: International Workshop, Held in conjunction with ICML 2020, July 18, 2020, Vienna, Austria, Revised and Extended Papers_, pages 139-166, Cham, 2020. Springer International Publishing. ISBN 978-3-031-04082-5. doi: 10.1007/978-3-031-04083-2_8.
* Karimi et al. [2021] Amir-Hossein Karimi, Bernhard Scholkopf, and Isabel Valera. Algorithmic Recourse: From Counterfactual Explanations to Interventions. In _Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency_, FAccT '21, pages 353-362, New York, NY, USA, 2021. Association for Computing Machinery. ISBN 978-1-4503-8309-7. doi: 10.1145/344218 8.3445899.
* Karimi et al. [2022] Amir-Hossein Karimi, Gilles Barthe, Bernhard Scholkopf, and Isabel Valera. A Survey of Algorithmic Recourse: Contrastive Explanations and Consequential Recommendations. _ACM Computing Surveys_, 55(5), December 2022. ISSN 0360-0300. doi: 10.1145/3527848.
* Keane et al. [2021] Mark T. Keane, Eoin M. Kenny, Eoin Delaney, and Barry Smyth. If Only We Had Better Counterfactual Explanations: Five Key Deficits to Rectify in the Evaluation of Counterfactual XAI Techniques. In Zhi-Hua Zhou, editor, _Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI-21_, pages 4466-4474. International Joint Conferences on Artificial Intelligence Organization, 8 2021. doi: 10.24963/ijcai.2021/609. URL https://doi.org/10.24963/ijcai.2021/609. Survey Track.
* Kelechi and Jiao [2023] Nwaike Kelechi and Licheng Jiao. Quantifying Actionability: Evaluating Human-Recipient Models. _IEEE Access_, 11:119811-119823, 2023. ISSN 2169-3536. doi: 10.1109/ACCESS.2 023.3324906.
* Konig et al. [2021] Gunnar Konig, Timo Freiesleben, and Moritz Grosse-Wentrup. Causal Perspective on Meaningful and Robust Algorithmic Recourse. _ICML Workshop on Algorithmic Recourse_, 2021.
* Konig et al. [2023] Gunnar Konig, Timo Freiesleben, and Moritz Grosse-Wentrup. Improvement-Focused Causal Recourse (ICR). In _Proceedings of the Thirty-Seventh AAAI Conference on Artificial Intelligence and Thirty-Fifth Conference on Innovative Applications of Artificial Intelligence and Thirteenth Symposium on Educational Advances in Artificial Intelligence_, AAAI'23/IAAI'23/EAAI'23. AAAI Press, 2023. ISBN 978-1-57735-880-0. doi: 10.1609/aa ai.v37i10.26398.
* Krishna et al. [2023] Satyapriya Krishna, Jiaqi Ma, and Himabindu Lakkaraju. Towards Bridging the Gaps between the Right to Explanation and the Right to Be Forgotten. In _Proceedings of the 40th International Conference on Machine Learning_, ICML'23. JMLR.org, 2023.
* Lacerda et al. [2023] Anisio Lacerda, Claudio Almeida, Leonardo Ferreira, Adriano Pereira, Gisele L. Pappa, Wagner Meira, Debora Miranda, Marco A. Romano-Silva, and Leandro Malloy Diniz. Algorithmic Recourse in Mental Healthcare. In _2023 International Joint Conference on Neural Networks (IJCNN)_, pages 1-8, June 2023. ISBN 2161-4407. doi: 10.1109/IJCNN54540.2023.10191158.
* Leben [2023] Derek Leben. Explainable AI as evidence of fair decisions. _Frontiers in Psychology_, 14, 2023. doi: 10.3389/fpsyg.2023.1069426.
* Ley et al. [2023] Dan Ley, Saumitra Mishra, and Daniele Magazzeni. GLOBE-CE: A Translation Based Approach for Global Counterfactual Explanations. In _Proceedings of the 40th International Conference on Machine Learning_, ICML'23. JMLR.org, 2023.

* Lucic et al. [2022] Ana Lucic, Harrie Oosterhuis, Hinda Haned, and Maarten de Rijke. FOCUS: Flexible Optimizable Counterfactual Explanations for Tree Ensembles. In _Proceedings of the 36th AAAI Conference on Artificial Intelligence, AAAI 2022_, volume 36, pages 5313-5322, 2022.
* Ma et al. [2022] Shucen Ma, Jianqi Shi, Yanhong Huang, Shengchao Qin, and Zhe Hou. Minimal-unsatisfiable-core-driven Local Explainability Analysis for Random Forest. _International Journal of Software and Informatics_, 12(4):355-376, 2022. doi: 10.21655/ijsi.1673-7288.00280.
* Mahajan et al. [2019] Divyat Mahajan, Chenhao Tan, and Amit Sharma. Preserving Causal Constraints in Counterfactual Explanations for Machine Learning Classifiers. In _NeurIPS 2019 Workshop "Do the right thing": machine learning and causal inference for improved decision making_, 2019.
* Mazzine et al. [2021] Raphael Mazzine, Sofie Goethals, Dieter Brughmans, and David Martens. Counterfactual Explanations for Employment Services. _International workshop on AI for Human Resources and Public Employment Services_, 2021.
* Hasan and Talbert [2022] Md Golam Moula Mehedi Hasan and Douglas A. Talbert. Mitigating the Rashomon Effect in Counterfactual Explanation: A Game-theoretic Approach. In _Proceedings of the International Florida Artificial Intelligence Research Society Conference, FLAIRS_, volume 35. Florida Online Journals, University of Florida, 2022. doi: 10.32473/flairs.v35i.130711.
* Miller [2019] Tim Miller. Explanation in artificial intelligence: Insights from the social sciences. _Artificial Intelligence_, 267:1-38, 2019. ISSN 0004-3702. doi: https://doi.org/10.1016/j.artint.2018.07.07. URL https://www.sciencedirect.com/science/article/pii/S0004370218305988.
* Moore et al. [2019] Jonathan Moore, Nils Hammerla, and Chris Watkins. Explaining Deep Learning Models with Constrained Adversarial Examples. In _PRICAI 2019: Trends in Artificial Intelligence: 16th Pacific Rim International Conference on Artificial Intelligence, Cuvu, Yannca Island, Fiji, August 26-30, 2019, Proceedings, Part I_, pages 43-56, Berlin, Heidelberg, 2019. Springer-Verlag. ISBN 978-3-030-29907-1. doi: 10.1007/978-3-030-29908-8_4.
* Mothil et al. [2020] Ramaravind K. Mothil, Amit Sharma, and Chenhao Tan. Explaining Machine Learning Classifiers through Diverse Counterfactual Explanations. In _Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency_, FAT* '20, pages 607-617, New York, NY, USA, 2020. Association for Computing Machinery. ISBN 978-1-4503-6936-7. doi: 10.1145/3351095.3372850.
* Murgia [2023] Madhumita Murgia. Algorithms are deciding who gets organ transplants. Are their decisions fair?, November 2023. URL https://www.ft.com/content/5125c83a-b82b-40c5-8b35-99579e087951. Accessed 22.05.2024.
* Naumann and Ntoutsi [2021] Philip Naumann and Eirini Ntoutsi. Consequence-Aware Sequential Counterfactual Generation. In _Machine Learning and Knowledge Discovery in Databases. Research Track: European Conference, ECML PKDD 2021, Bilbao, Spain, September 13-17, 2021, Proceedings, Part II_, pages 682-698, Berlin, Heidelberg, 2021. Springer-Verlag. ISBN 978-3-030-86519-1. doi: 10.1007/978-3-030-86520-7_42.
* Proceedings of the 14th ACM International Conference on Web Search and Data Mining_, pages 1089-1092. Association for Computing Machinery, 2021. doi: 10.1145/3437963.3441705.
* Nemirovsky et al. [2022] Daniel Nemirovsky, Nicolas Thiebaut, Ye Xu, and Abhishek Gupta. CounteRGAN: Generating Counterfactuals for Real-Time Recourse and Interpretability using Residual GANs. In _Proceedings of the 38th Conference on Uncertainty in Artificial Intelligence, UAI 2022_, pages 1488-1497. Association For Uncertainty in Artificial Intelligence (AUAI), 2022.
* Nguyen et al. [2023] Duy Nguyen, Ngoc Bui, and Viet Anh Nguyen. Distributionally Robust Recourse Action. _arXiv_, 2023.
* Nguyen et al. [2023] Duy Nguyen, Ngoc Bui, and Viet Anh Nguyen. Feasible Recourse Plan via Diverse Interpolation. In _Proceedings of The 26th International Conference on Artificial Intelligence and Statistics_, volume 206, pages 4679-4698. PMLR, 2023.

* Nguyen et al. [2022] Tuan-Duy H. Nguyen, Ngoc Bui, Duy Nguyen, Man-Chung Yue, and Viet Anh Nguyen. Robust Bayesian Recourse. In _Proceedings of the Thirty-Eighth Conference on Uncertainty in Artificial Intelligence_, volume 180, pages 1498-1508. PMLR, 2022.
* O'Brien and Kim [2022] Andrew O'Brien and Edward Kim. Toward Multi-Agent Algorithmic Recourse: Challenges From a Game-Theoretic Perspective. In _Proceedings of the International Florida Artificial Intelligence Research Society Conference, FLAIRS_, volume 35. Florida Online Journals, University of Florida, 2022. doi: 10.32473/flairs.v35i.130614.
* O'Brien et al. [2023] Andrew O'Brien, Edward Kim, and Rosina Weber. Investigating Causally Augmented Sparse Learning as a Tool for Meaningful Classification. In _2023 IEEE Sixth International Conference on Artificial Intelligence and Knowledge Engineering (AIKE)_, pages 33-37, September 2023. ISBN 2831-7203. doi: 10.1109/AIKE59827.2023.00013.
* Ong et al. [2021] Ming Lun Ong, Anthony Li, and Mehul Motani. Explainable and Actionable Machine Learning Models for Electronic Health Record Data. In _IFMBE Proceedings_, volume 79, pages 91-99, Cham, 2021. Springer International Publishing. doi: 10.1007/978-3-030-62045-5_9.
* Page et al. [2021] Matthew J. Page, Joanne E. McKenzie, Patrick M. Bossuyt, Isabelle Boutron, Tammy C. Hoffmann, Cynthia D. Mulrow, Larissa Shamseer, Jennifer M. Tetzlaff, Elie A. Akl, Sue E. Brennan, et al. The PRISMA 2020 statement: an updated guideline for reporting systematic reviews. _Bmj_, 372, 2021.
* Parmentier and Vidal [2021] Axel Parmentier and Thibaut Vidal. Optimal Counterfactual Explanations in Tree Ensembles. In _Proceedings of the 38th International Conference on Machine Learning_, volume 139, pages 8422-8431. PMLR, 2021.
* Proceedings of the World Wide Web Conference, WWW 2020_, pages 3126-3132, 2020. doi: 10.1145/3366423.3380087.
* Pawelczyk et al. [2021] Martin Pawelczyk, Sascha Bielawski, Johannes van den Heuvel, Tobias Richter, and Gjergji Kasneci. CARLA: A Python Library to Benchmark Algorithmic Recourse and Counterfactual Explanation Algorithms. In _Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1 (NeurIPS Datasets and Benchmarks 2021)_, 2021.
* Pawelczyk et al. [2022] Martin Pawelczyk, Chirag Agarwal, Shalmali Joshi, Sohini Upadhyay, and Himabidu Lakkaraju. Exploring Counterfactual Explanations Through the Lens of Adversarial Examples: A Theoretical and Empirical Analysis. In _Proceedings of The 25th International Conference on Artificial Intelligence and Statistics_, volume 151, pages 4574-4594. PMLR, 2022.
* Pawelczyk et al. [2023] Martin Pawelczyk, Himabidu Lakkaraju, and Seth Neel. On the Privacy Risks of Algorithmic Recourse. In _Proceedings of The 26th International Conference on Artificial Intelligence and Statistics_, volume 206, pages 9680-9696. PMLR, 2023.
* Pearl [2009] Judea Pearl. _Causality_. Cambridge University Press, 2 edition, 2009. ISBN 9780511803161.
* Poyiadjzi et al. [2020] Rafael Poyiadjzi, Kacper Sokol, Raul Santos-Rodriguez, Tijl De Bie, and Peter Flach. FACE: Feasible and actionable counterfactual explanations. In _Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society_, AIES '20, pages 344-350, New York, NY, USA, 2020. Association for Computing Machinery. doi: 10.1145/3375627.3375850.
* Qi and Chelmis [2021] Wenting Qi and Charalampos Chelmis. Improving Algorithmic Decision-Making in the Presence of Untrustworthy Training Data. In _2021 IEEE International Conference on Big Data (Big Data)_, pages 1102-1108, 2021. doi: 10.1109/BigData52589.2021.9671677.
* Raimundo et al. [2022] Marcos M. Raimundo, Luis Gustavo Nonato, and Jorge Poco. Mining Pareto-optimal Counterfactual Antecedents with a Branch-and-Bound Model-Agnostic Algorithm. _Data Mining and Knowledge Discovery_, 2022. doi: 10.1007/s10618-022-00906-4.
* Ramakrishnan et al. [2020] Goutham Ramakrishnan, Yun Chan Lee, and Aws Albarghouthi. Synthesizing Action Sequences for Modifying Model Decisions. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 34, pages 5462-5469, 2020.

* Raman et al. [2023] Natraj Raman, Daniele Magazzeni, and Sameena. Shah. Bayesian Hierarchical Models for Counterfactual Estimation. In _Proceedings of The 26th International Conference on Artificial Intelligence and Statistics_, volume 206, pages 1115-1128. PMLR, 2023.
* Ramaswami et al. [2022] Gomathy Ramaswami, Teo Susnjak, and Anuradha Mathrani. Supporting Students' Academic Performance Using Explainable Machine Learning with Automated Prescriptive Analytics. _Big Data and Cognitive Computing_, 6(4), 2022. doi: 10.3390/bdc66040105.
* Ras and Wieczorkowska [2000] Zbigniew W. Ras and Alicja Wieczorkowska. Action-Rules: How to Increase Profit of a Company. In _Principles of Data Mining and Knowledge Discovery_, pages 587-592. Springer Berlin Heidelberg, 2000. ISBN 978-3-540-45372-7.
* Rasouli and Yu [2022] Peyman Rasouli and Ingrid Chieh Yu. CARE: Coherent Actionable Recourse Based on Sound Counterfactual Explanations. _International Journal of Data Science and Analytics_, 17, 2022. doi: 10.1007/s41060-022-00365-6.
* Rawal and Lakkaraju [2020] Kaivalya Rawal and Himabindu Lakkaraju. Beyond Individualized Recourse: Interpretable and Interactive Summaries of Actionable Recourses. In _Proceedings of the 34th International Conference on Neural Information Processing Systems_, NIPS'20, Red Hook, NY, USA, 2020. Curran Associates Inc. ISBN 978-1-71382-954-6.
* Rawal et al. [2021] Kaivalya Rawal, Ece Kamar, and Himabindu Lakkaraju. Algorithmic Recourse in the Wild: Understanding the Impact of Data and Model Shifts. _arXiv_, 2021.
* Redelmeier et al. [2024] Annabelle Redelmeier, Martin Jullum, Kjersti Aas, and Anders Loland. MCCE: Monte Carlo sampling of realistic counterfactual explanations. In _Data Mining and Knowledge Discovery_, pages 421-437. Springer Nature, 2024. doi: 10.1007/s10618-024-01017-y.
* Ross et al. [2021] Alexis Ross, Himabindu Lakkaraju, and Osbert Bastani. Learning models for actionable recourse. In _Advances in Neural Information Processing Systems_, volume 34, pages 18734-18746, 2021.
* Salimi et al. [2023] Pedram Salimi, Nirmalie Wiratunga, David Corsar, and Anjana Wijekoon. Towards Feasible Counterfactual Explanations: A Taxonomy Guided Template-Based NLG Method. In _Frontiers in Artificial Intelligence and Applications_, volume 372, pages 2057-2064. IOS Press BV, 2023. doi: 10.3233/FAIA230499.
* Schleich et al. [2021] Maximilian Schleich, Zixuan Geng, Yihong Zhang, and Dan Suciu. GeCo: Quality Counterfactual Explanations in Real Time. _Proc. VLDB Endow._, 14(9):1681-1693, may 2021. ISSN 2150-8097. doi: 10.14778/3461535.3461555. URL https://doi.org/10.14778/346155.3461555.
* Schoeff et al. [2022] Jakob Schoeff, Niklas Kuehl, and Yvette Machowski. "There Is Not Enough Information": On the Effects of Explanations on Perceptions of Informational Fairness and Trustworthiness in Automated Decision-Making. In _Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency_, FAccT '22, pages 1616-1628, New York, NY, USA, 2022. Association for Computing Machinery. ISBN 978-1-4503-9352-2. doi: 10.1145/3531146.3533218.
* Selbst et al. [2019] Andrew D. Selbst, danah boyd, Sorelle A. Friedler, Suresh Venkatasubramanian, and Janet Vertesi. Fairness and abstraction in sociotechnical systems. In _Proceedings of the Conference on Fairness, Accountability, and Transparency_, FAT* '19, page 59-68, New York, NY, USA, 2019. Association for Computing Machinery. ISBN 9781450361255. doi: 10.1145/3287560.3287598. URL https://doi.org/10.1145/3287560.3287598.
* Sharma et al. [2020] Shubham Sharma, Jette Henderson, and Joydeep Ghosh. CERTIFAI: A Common Framework to Provide Explanations and Analyse the Fairness and Robustness of Black-Box Models. In _Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society_, AIES '20, pages 166-172, New York, NY, USA, 2020. Association for Computing Machinery. ISBN 978-1-4503-7110-0. doi: 10.1145/3375627.3375812.

* Sharma et al. [2021] Shubham Sharma, Alan H. Gee, David Paydarfar, and Joydeep Ghosh. FaiR-N: Fair and Robust Neural Networks for Structured Data. In _Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society_, AIES '21, pages 946-955, New York, NY, USA, 2021. Association for Computing Machinery. ISBN 978-1-4503-8473-5. doi: 10.1145/3461702.3462559.
* Shree et al. [2022] Sunny Shree, Jaganmohan Chandrasekaran, Yu Lei, Raghu N. Kacker, and D. Richard Kuhn. DeltaExplainer: A Software Debugging Approach to Generating Counterfactual Explanations. In _2022 IEEE International Conference On Artificial Intelligence Testing (AITest)_, pages 103-110, 2022. doi: 10.1109/AITest55621.2022.00023.
* Singh et al. [2023] Manan Singh, Sai Srinivas Kancheti, Shivam Gupta, Ganesh Ghalme, Shweta Jain, and Narayanan C. Krishnan. Algorithmic Recourse Based on User's Feature-Order Preference. In _Proceedings of the 6th Joint International Conference on Data Science & Management of Data (10th ACM IKDD CODS and 28th COMAD)_, CODS-COMAD '23, pages 293-294, New York, NY, USA, 2023. Association for Computing Machinery. ISBN 978-1-4503-9797-1. doi: 10.1145/3570991.3571039.
* Singh et al. [2023] Ronal Singh, Tim Miller, Henrietta Lyons, Liz Sonenberg, Eduardo Velloso, Frank Vetere, Piers Howe, and Paul Dourish. Directive Explanations for Actionable Explainability in Machine Learning Applications. _ACM Trans. Interact. Intell. Syst._, 13(4), December 2023. ISSN 2160-6455. doi: 10.1145/3579363.
* Slack et al. [2021] Dylan Slack, Sophie Hilgard, Himabindu Lakkaraju, and Sameer Singh. Counterfactual Explanations Can Be Manipulated. In _Advances in Neural Information Processing Systems_, volume 34, pages 62-75, 2021.
* Smith et al. [2022] Bevan I. Smith, Charles Chimedza, and Jacoba H. Buhrmann. Individualized Help for At-Risk Students Using Model-Agnostic and Counterfactual Explanations. _Education and Information Technologies_, 27(2):1539-1558, March 2022. ISSN 1360-2357. doi: 10.1007/s10639-021-1 0661-6.
* Sohns et al. [2023] Jan-Tobias Sohns, Christoph Garth, and Heike Leitte. Decision Boundary Visualization for Counterfactual Reasoning. _Computer Graphics Forum_, 42(1):7-20, 2023. doi: 10.1111/cgf.14 650.
* Spreitzer et al. [2022] Nina Spreitzer, Hinda Haned, and Ilse van der Linden. Evaluating the Practicality of Counterfactual Explanations. In _CEUR Workshop Proceedings_, volume 3277, pages 31-50. CEUR-WS, 2022.
* State et al. [2023] Laura State, Salvatore Ruggieri, and Franco Turini. Reason to Explain: Interactive Contrastive Explanations (REASONX). In _Explainable Artificial Intelligence_, volume 1901 CCIS, pages 421-437, Cham, 2023. Springer Nature Switzerland. ISBN 978-3-031-44064-9.
* Stepin et al. [2021] Ilia Stepin, Jose M. Alonso, Alejandro Catala, and Martin Pereira-Farina. A Survey of Contrastive and Counterfactual Explanation Generation Methods for Explainable Artificial Intelligence. _IEEE Access_, 9:11974-12001, 2021.
* Suffian and Bogliolo [2022] Muhammad Suffian and Alessandro Bogliolo. Investigation and Mitigation of Bias in Explainable AI. In _CEUR Workshop Proceedings_, volume 3319, pages 89-94. CEUR-WS, 2022.
* Suffian et al. [2022] Muhammad Suffian, Pierluigi Graziani, Jose M. Alonso, and Alessandro Bogliolo. FCE: Feedback Based Counterfactual Explanations for Explainable AI. _IEEE Access_, 10:72363-72372, 2022. ISSN 2169-3536. doi: 10.1109/ACCESS.2022.3189432.
* Sullivan and Verreault-Julien [2022] Emily Sullivan and Philippe Verreault-Julien. From Explanation to Recommendation: Ethical Standards for Algorithmic Recourse. In _Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society_, AIES '22, pages 712-722, New York, NY, USA, 2022. Association for Computing Machinery. ISBN 978-1-4503-9247-1. doi: 10.1145/3514094.3534185.
* Tolomei et al. [2017] Gabriele Tolomei, Fabrizio Silvestri, Andrew Haines, and Mounia Lalmas. Interpretable Predictions of Tree-Based Ensembles via Actionable Feature Tweaking. In _Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining_, pages 465-474, 2017. doi: 10.1145/3097983.3098039.

* Tsiakmaki and Ragos [2021] Maria Tsiakmaki and Omiros Ragos. A Case Study of Interpretable Counterfactual Explanations for the Task of Predicting Student Academic Performance. In _2021 25th International Conference on Circuits, Systems, Communications and Computers (CSCC)_, pages 120-125, July 2021. doi: 10.1109/CSCC53858.2021.00029.
* Tsirtstis and Gomez-Rodriguez [2020] Stratis Tsirtstis and Manuel Gomez-Rodriguez. Decisions, Counterfactual Explanations and Strategic Behavior. In _Proceedings of the 34th International Conference on Neural Information Processing Systems_, NIPS'20, Red Hook, NY, USA, 2020. Curran Associates Inc. ISBN 978-1-71382-954-6.
* Upadhyay et al. [2021] Sohini Upadhyay, Shalmali Joshi, and Himabindu Lakkaraju. Towards Robust and Reliable Algorithmic Recourse. In _Advances in Neural Information Processing Systems_, volume 20, pages 16926-19937, 2021.
* Ustun et al. [2019] Berk Ustun, Alexander Spangher, and Yang Liu. Actionable Recourse in Linear Classification. In _Proceedings of the Conference on Fairness, Accountability, and Transparency_, FAT* '19, pages 10-19, New York, NY, USA, 2019. Association for Computing Machinery. ISBN 978-1-4503-6125-5. doi: 10.1145/3287560.3287566.
* De Schoot et al. [2021] Rens Van De Schoot, Jonathan De Bruin, Raoul Schram, Parisa Zahedi, Jan De Boer, Felix Weijdema, Bianca Kramer, Martijn Huijts, Maarten Hoogerwerf, Gerbrich Ferdinands, Albert Harkema, Joukje Willemsen, Yongchao Ma, Qixiang Fang, Sybren Hindriks, Lars Tummers, and Daniel L. Oberski. An open source machine learning framework for efficient and transparent systematic reviews. _Nature Machine Intelligence_, 3(2):125-133, 2021. doi: 10.1038/s42256-020-00287-7.
* van der Schaar and Rashbass [2023] Mihaela van der Schaar and Andrew Rashbass. The case for Reality-centric AI, Feb 2023. URL https://www.vanderschaar-lab.com/the-case-for-reality-centric-ai/. Accessed 21.05.2024.
* VanNostrand et al. [2023] Peter M. VanNostrand, Huayi Zhang, Dennis M. Hofmann, and Elke A. Rundensteiner. FACET: Robust Counterfactual Explanation Analytics. _Proc. ACM Manag. Data_, 1(4), December 2023. doi: 10.1145/3626729.
* Venkatasubramanian and Alfano [2020] Suresh Venkatasubramanian and Mark Alfano. The Philosophical Basis of Algorithmic Recourse. In _Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency_, FAT* '20, pages 284-293, New York, NY, USA, 2020. Association for Computing Machinery. ISBN 978-1-4503-6936-7. doi: 10.1145/3351095.3372876.
* Verma et al. [2022] Sahil Verma, Varich Boonsanong, Minh Hoang, Keegan E. Hines, John P. Dickerson, and Chirag Shah. Counterfactual Explanations and Algorithmic Recourses for Machine Learning: A Review. _arXiv_, 2022.
* Verma et al. [2022] Sahil Verma, Keegan Hines, and John P. Dickerson. Amortized Generation of Sequential Algorithmic Recourses for Black-Box Models. In _Proceedings of the 36th AAAI Conference on Artificial Intelligence, AAAI 2022_, volume 36, pages 8512-8519. Association for the Advancement of Artificial Intelligence, 2022.
* Verma et al. [2023] Sahil Verma, Ashudeep Singh, Varich Boonsanong, John P. Dickerson, and Chirag Shah. RecRec: Algorithmic Recourse for Recommender Systems. In _Proceedings of the 32nd ACM International Conference on Information and Knowledge Management_, CIKM '23, pages 4325-4329, New York, NY, USA, 2023. Association for Computing Machinery. ISBN 9798400701245. doi: 10.1145/3583780.3615181.
* Vieth-Ditlmann [2024] Kilian Vieth-Ditlmann. The algorithmic administration: automated decision-making in the public sector, May 2024. URL https://algorithmwatch.org/en/algorithmic-administration-explained/. Accessed 22.05.2024.
* Virgolin and Fracaros [2023] Marco Virgolin and Saverio Fracaros. On the Robustness of Sparse Counterfactual Explanations to Adverse Perturbations. _Artificial Intelligence_, 316(C), March 2023. ISSN 0004-3702. doi: 10.1016/j.artint.2022.103840.

* Vo et al. [2023] Vy Vo, Trung Le, Van Nguyen, He Zhao, Edwin V. Bonilla, Gholamreza Haffari, and Dinh Phung. Feature-Based Learning for Diverse and Privacy-Preserving Counterfactual Explanations. In _Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining_, KDD '23, pages 2211-2222, New York, NY, USA, 2023. Association for Computing Machinery. ISBN 9798400701030. doi: 10.1145/3580305.3599343.
* V. Kugelgen et al. [2022] Julius Von Kugelgen, Amir-Hossein Karimi, Umang Bhatt, Isabel Valera, Adrian Weller, and Bernhard Scholkopf. On the Fairness of Causal Algorithmic Recourse. In _Proceedings of the 36th AAAI Conference on Artificial Intelligence, AAAI 2022_, volume 36, pages 9584-9594. Association for the Advancement of Artificial Intelligence, 2022.
* von Kugelgen et al. [2021] Julius von Kugelgen, Nikita Agarwal, Jakob Zeitler, Afsaneh Mastouri, and Bernhard Scholkopf. Algorithmic Recourse in Partially and Fully Confounded Settings Through Bounding Counterfactual Effects. _arXiv_, 2021.
* Wachter et al. [2017] Sandra Wachter, Brent Mittelstadt, and Chris Russell. Counterfactual explanations without opening the black box: Automated decisions and the GDPR. _Harvard Journal of Law & Technology_, 31:841, 2017.
* Wang et al. [2021] Paul Y. Wang, Sainyam Galhotra, Romila Pradhan, and Babak Salimi. Demonstration of Generating Explanations for Black-Box Algorithms Using Lewis. _Proc. VLDB Endow._, 14(12):2787-2790, July 2021. ISSN 2150-8097. doi: 10.14778/3476311.3476345.
* Wang et al. [2021] Yongjie Wang, Qinxu Ding, Ke Wang, Yue Liu, Xingyu Wu, Jinglong Wang, Yong Liu, and Chunyan Miao. The Skyline of Counterfactual Explanations for Machine Learning Decision Models. In _Proceedings of the 30th ACM International Conference on Information & Knowledge Management_, CIKM '21, pages 2030-2039, New York, NY, USA, 2021. Association for Computing Machinery. ISBN 978-1-4503-8446-9. doi: 10.1145/3459637.3482397.
* Wang et al. [2023] Yongjie Wang, Hangwei Qian, Yongjie Liu, Wei Guo, and Chunyan Miao. Flexible and Robust Counterfactual Explanations with Minimal Satisfiable Perturbations. In _Proceedings of the 32nd ACM International Conference on Information and Knowledge Management_, CIKM '23, pages 2596-2605, New York, NY, USA, 2023. Association for Computing Machinery. ISBN 9798400701245. doi: 10.1145/3583780.3614885.
* Wang et al. [2023] Zhendong Wang, Isak Samsten, Vasiliki Kougia, and Panagiotis Papapetrou. Style-Transfer Counterfactual Explanations: An Application to Mortality Prevention of ICU Patients. _Artif. Intell. Med._, 135(C), January 2023. ISSN 0933-3657. doi: 10.1016/j.artmed.2022.102457.
* Wang et al. [2023] Zijie J. Wang, Jennifer Wortman Vaughan, Rich Caruana, and Duen Horng Chau. GAM Coach: Towards Interactive and User-Centered Algorithmic Recourse. In _Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems_, CHI '23, New York, NY, USA, 2023. Association for Computing Machinery. ISBN 978-1-4503-9421-5. doi: 10.1145/3544548.3580816.
* Warren et al. [2022] Greta Warren, Mark T. Keane, and Ruth M. J. Byrne. Features of Explainability: How Users Understand Counterfactual and Causal Explanations for Categorical and Continuous Features in XAI. In _CEUR Workshop Proceedings_, volume 3251. CEUR-WS, 2022.
* Warren et al. [2008] Greta Warren, Barry Smyth, and Mark T. Keane. "Better" Counterfactuals, Ones People Can Understand: Psychologically-Plausible Case-Based Counterfactuals Using Categorical Features for Explainable AI (XAI). In _Case-Based Reasoning Research and Development: 30th International Conference, ICCBR 2022, Nancy, France, September 12-15, 2022, Proceedings_, pages 63-78, Berlin, Heidelberg, 2022. Springer-Verlag. ISBN 978-3-031-14922-1. doi: 10.1007/978-3-031-14923-8_5.
* Weld and Bansal [2019] Daniel S. Weld and Gagan Bansal. The Challenge of Crafting Intelligible Intelligence. _Commun. ACM_, 62(6):70-79, may 2019. ISSN 0001-0782. doi: 10.1145/3282486. URL https://doi.org/10.1145/3282486.
* Wijekoon et al. [2021] Anjana Wijekoon, Nirmalie Wiratunga, Ikechukwu Nikisi-Orji, Kyle Martin, Chamath Palihawadana, and David Corsar. Counterfactual Explanations for Student Outcome Prediction with Moodle Footprints. In _CEUR Workshop Proceedings_, volume 2894, pages 1-8. CEUR-WS, 2021.

* Wiratunga et al. [2021] Nirmalie Wiratunga, Anjana Wijekoon, Ikechukwu Nkisi-Orj, Kyle Martin, Chamath Palihawadana, and David Corsar. DisCERN: Discovering Counterfactual Explanations using Relevance Features from Neighbourhoods. In _2021 IEEE 33rd International Conference on Tools with Artificial Intelligence (ICTAI)_, pages 1466-1473, November 2021. ISBN 2375-0197. doi: 10.1109/ICTAI52525.2021.00233.
* Wohlin [2014] Claes Wohlin. Guidelines for Snowballing in Systematic Literature Studies and a Replication in Software Engineering. _EASE '14: Proceedings of the 18th International Conference on Evaluation and Assessment in Software Engineering_, 2014. doi: 10.1145/2601248.2601268. URL https://doi.org/10.1145/2601248.2601268.
* Yan and Wang [2023] Jingquan Yan and Hao Wang. Self-Interpretable Time Series Prediction with Counterfactual Explanations. In _Proceedings of the 40th International Conference on Machine Learning_, ICML'23. JMLR.org, 2023.
* Yetukuri et al. [2023] Jayanth Yetukuri, Ian Hardy, and Yang Liu. Towards User Guided Actionable Recourse. In _Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society_, AIES '23, pages 742-751, New York, NY, USA, 2023. Association for Computing Machinery. ISBN 9798400702310. doi: 10.1145/3600211.3604708.
* Zhang et al. [2023] Songming Zhang, Xiaofeng Chen, Shiping Wen, and Zhongshan Li. Density-Based Reliable and Robust Explainer for Counterfactual Explanation. _Expert Syst. Appl._, 226(C), September 2023. ISSN 0957-4174. doi: 10.1016/j.eswa.2023.120214.

Extended discussion of the search process

While our discussion of the search process in Section 3.1 in the main body of the document is complete, we also provide an extended version of this discussion to allow for full reproducibility.

We make use of 3 search engines to collect the initial set of studies: ACM Digital Library, IEEE Xplore, and SCOPUS. Given the blurry distinction between AR and CEs, we consider the papers discussing either problem. In a small scoping review, we identify several keywords common to publications on recourse, as well as several equivalent terms to build the query shown below.

("Machine Learning" OR "Artificial Intelligence" OR "Algorithmic Decision*" OR "Consequential Decision*" OR Classif* OR Predict* OR "Explainable AI" OR AI OR XAI) AND (((Counterfactual OR Contrastive OR Actionable) AND Explanation*) OR ((Algorithmic OR Individual* OR Actionable) AND Recourse) OR Counterfactual?)

We modify this query to account for the semantic differences between the search engines.

For ACM Digital Library:

Title:( "Machine Learning" OR "Artificial Intelligence" OR "Algorithmic Decision*" OR "Consequential Decision*" OR classif* OR predict* OR "Explainable AI" OR ai OR xai ) AND ( ( ( counterfactual OR contrastive OR actionable ) AND explanation* ) OR ( ( algorithmic OR individual* OR actionable ) AND recourse ) OR counterfactual? ) OR Abstract:( ( "Machine Learning" OR "Artificial Intelligence" OR "Algorithmic Decision*" OR "Consequential Decision*" OR classif* OR predict* OR "Explainable AI" OR ai OR xai ) AND ( ( ( counterfactual OR contrastive OR actionable ) AND explanation* ) OR ( ( algorithmic OR individual* OR actionable ) AND recourse ) OR counterfactual? ) OR Keyword:( ( "Machine Learning" OR "Artificial Intelligence" OR "Algorithmic Decision*" OR "Consequential Decision*" OR classif* OR predict* OR "Explainable AI" OR ai OR xai ) AND ( ( ( counterfactual OR contrastive OR actionable ) AND explanation* ) OR ( ( algorithmic OR individual* OR actionable ) AND recourse ) OR counterfactual? ))

For IEEE Xplore:

(("All Metadata":"Machine Learning" OR "All Metadata":"Artificial Intelligence" OR "All Metadata":"Algorithmic Decision*" OR "All Metadata":"Consequential Decision*" OR "All Metadata":"classif* OR "All Metadata":predict* OR "All Metadata":"Explainable AI" OR "All Metadata":xi ) AND ((("All Metadata":counterfactual OR "All Metadata":contrastive OR "All Metadata":actionable ) AND "All Metadata":explanation* ) OR ( ("All Metadata":algorithmic OR "All Metadata":individual* OR "All Metadata":actionable ) AND "All Metadata":recourse ) OR "All Metadata":counterfactual? )))* [114] "The "Machine Learning" OR "Artificial Intelligence" OR "Algorithmic Decision*" OR "Consequential Decision*" OR classif* OR predict* OR "Explainable AI" OR ai OR xai ) AND ( ( ( counterfactual OR contrastive OR actionable ) AND explanation* ) OR ( ( algorithmic OR individual* OR actionable ) AND recourse ) OR counterfactual? ) )
* [115] "The search is carried out on January 12th 2024 in titles, abstracts, and keywords, with 1267 results from ACM Digital Library (The ACM Guide to Computing Literature), 513 results from IEEE Xplore, and 2139 results from SCOPUS. This leads to a total of 3919 results, which are imported to the Zotero reference management software for de-duplication. After removing the duplicates, we are left with 3136 results, 44 of which are the meta-data of conference proceedings that we also remove."
* [116] "To facilitate the screening process, we employ the open-source ASReview tool, which makes use of an active learning approach to re-order the set of publications, such that the most relevant ones are always "at the top of the stack" [139]. We run ASReview on the default settings, i.e.: Feature extraction technique: TF-IDF Classifier: Naive Bayes Query strategy: Maximum Balance strategy: Dynamic resampling (Double)
* [117] "The researchers behind the tool suggest employing a stopping rule measured in the number of consecutive irrelevant records, which we set to 30, or 1% of the entire dataset. We accept all papers that focus on algorithmic recourse and counterfactual explanations, completing the screening after evaluating 1040 abstracts (33.67% of the dataset), leading to 504 (16.30%) records among which we identify further 4 duplicates to remove. This results in the reported number of 499 relevant records."
* [118] "We observe that some important publications may be missing from our results. For instance, [118] was published in the Harvard Journal of Law & Technology that is not indexed by computer science search engines. Thus, we decide to augment the set of records by applying snowballing, which has been shown as a good alternative to databases in systematic reviews in software engineering [162]."
* [119] "We decide to make use of citation counts as a proxy for impact. Due to the lack of a suitable tool that would provide unbiased citation counts for _all_ papers in our dataset, we collect them from Google Scholar. Unfortunately, citation counts on Google Scholar tend to be inflated, but as we make use of snowballing purely to enrich the dataset, these does not impact the validity of our study. We manually collect Google Scholar citation counts for all 499 results from the first screening on January 27th and 28th, order them descendingly, and collect references for the top 50 (10%) "most impactful" publications. Snowballing results in a total of 1519 new records. Indeed, we observe that [118] (mentioned above) is referenced by 39 of the 50 publications used for snowballing."
* [119] "While this strategy introduces several pre-prints into our result set [52, 61, 91, 113, 143, 150], we decide not to exclude them. Our review remains primarily concerned with peer-reviewed work. Here, we also note that [114], which we collected as a pre-print has been published between the search and appraisal. As such we decided to evaluate its published version and refer to it in this paper."
* [119] "After adding the snowballed references into our dataset, we are left with 2018 records for the second screening with ASReview, again on the default settings. This time, we look for publications that specifically refer to the problem of AR, "actionable" CEs, or modifying outcomes of automated decision-making systems. We employ a stricter stopping rule to minimize the risk of false negatives, completing the screening after 60 consecutive irrelevant records. We evaluate 538 results (26.71% of the dataset), with 203 (10.06%) relevant results that are considered for full-text appraisal. This concludes the extended discussion of the search process."

[MISSING_PAGE_EMPTY:25]

\begin{table}
\begin{tabular}{l l l l l l l} \hline \hline Year & Reference & \begin{tabular}{l} Propose \\ methods \\ \end{tabular} & 
\begin{tabular}{l} Theoretical \\ frameworks \\ \end{tabular} & Analyses & Apply & Benchmark & Review \\ \hline
2022 & [39] & ✓ & & ✓ & & \\  & [34] & ✓ & & ✓ & & \\  & [6] & ✓ & & & & \\  & [25] & ✓ & & & & \\  & [50] & ✓ & & & & \\  & [62] & ✓ & & & & \\  & [158] & ✓ & & & & \\  & [83] & ✓ & & & & \\  & [56] & ✓ & & & & \\  & [79] & ✓ & & & & \\  & [80] & ✓ & & & & \\  & [90] & ✓ & & & & \\  & [93] & ✓ & & & & \\  & [106] & ✓ & & & & \\  & [111] & ✓ & & & & \\  & [132] & ✓ & & & & \\  & [131] & ✓ & & & & \\  & [144] & ✓ & & & & \\  & [65] & ✓ & & & & \\  & [101] & & ✓ & ✓ & & \\  & [24] & & ✓ & & ✓ & \\  & [70] & & ✓ & & & ✓ \\  & [15] & & ✓ & & & ✓ \\  & [16] & & ✓ & & & \\  & [94] & & ✓ & & & \\  & [118] & ✓ & & & & \\  & [133] & ✓ & & & & \\  & [157] & ✓ & & & & \\  & [128] & ✓ & & & & \\  & [149] & & & ✓ & & \\  & [28] & & & ✓ & & \\  & [109] & & & ✓ & & \\  & [126] & & & ✓ & & \\  & [48] & & & & ✓ & ✓ \\  & [143] & & & & ✓ & ✓ \\ \hline \hline \end{tabular}
\end{table}
Table 2: Evaluation of the collected publications on the types of contributions, 2022.

\begin{table}
\begin{tabular}{l l l l l l l l} \hline \hline Year & Reference & \begin{tabular}{l} Propose \\ methods \\ \end{tabular} & 
\begin{tabular}{l} Theoretical \\ frameworks \\ \end{tabular} & Analyses & Apply & Benchmark & Review \\ \hline
2023 & [36] & ✓ & ✓ & & & & \\  & [29] & ✓ & ✓ & & & & \\  & [116] & ✓ & ✓ & & & & \\  & [9] & ✓ & & ✓ & & & \\  & [42] & ✓ & & ✓ & & & \\  & [75] & ✓ & & ✓ & & & \\  & [147] & ✓ & & ✓ & & & \\  & [156] & ✓ & & & ✓ & & \\  & [155] & ✓ & & & ✓ & \\  & [54] & ✓ & & & & & \\  & [123] & ✓ & & & & & \\  & [14] & ✓ & & & & & \\  & [72] & ✓ & & & & & \\  & [30] & ✓ & & & & & \\  & [51] & ✓ & & & & & \\  & [91] & ✓ & & & & & \\  & [92] & ✓ & & & & \\  & [95] & ✓ & & & & & \\  & [108] & ✓ & & & & \\  & [127] & ✓ & & & & & \\  & [129] & ✓ & & & & \\  & [141] & ✓ & & & & \\  & [154] & ✓ & & & & \\  & [163] & ✓ & & & & \\  & [164] & ✓ & & & & \\  & [165] & ✓ & & & & \\  & [78] & ✓ & & & & \\  & [148] & ✓ & & & & \\  & [74] & ✓ & & & & \\  & [77] & & ✓ & & & \\  & [124] & & ✓ & & & \\  & [38] & & & ✓ & & \\  & [57] & & & ✓ & & \\  & [102] & & & ✓ & & \\  & [3] & & & & ✓ & \\  & [76] & & & & ✓ & \\  & [8] & & & & ✓ & \\  & [60] & & & & & ✓ \\ \hline
2024 & [21] & ✓ & & & & & \\  & [114] & ✓ & & & & \\ \hline \hline \end{tabular}
\end{table}
Table 3: Evaluation of the collected publications on the types of contributions, 2023-2024.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: Our main claim is that existing research on recourse is disconnected from the practical requirements of systems where it would be applied (see Section 4 and Section 5.1). Our claim is supported by a systematized literature review which is the contribution of this work (Section 3). These are reflected in the abstract and the introduction. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We highlight and discuss the three main limitations of our work in Section 5.2. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?Answer: [NA] Justification: Our work, as a literature review, does not rely on theoretical results or proofs. Nonetheless, we are explicit about the "assumptions" in that we discuss our approach to the collection and analysis of results in depth in Section 3. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [NA] Justification: Our work does not rely on any experiments, so this question is not applicable. Nonetheless, we believe that we provide sufficiently in-depth characterization of the review process where other authors should be able to reproduce it (Section 3 and Appendix A). Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.

5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [NA] Justification: Our work does not rely on any experiments, so this question is not applicable. Nonetheless, we provide the complete list of publications covered in this review. We will also release the review data upon acceptance. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/pu blic/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [NA] Justification: Our work does not rely on any experiments, so this question is not applicable. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [NA] Justification: Our work does not rely on any experiments, so this question is not applicable. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.

* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [NA] Justification: Our work does not rely on any experiments, so this question is not applicable. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We have reviewed the NeurIPS Code of Ethics and we confirm that our work conforms to it in every respect. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: Although this is not covered in a separate section, positive and negative societal impacts of our work (and algorithmic recourse in general) are a key consideration throughout this paper. See for instance Section 1 or Section 6. Guidelines:* The answer NA means that there is no societal impact of the work performed.
* If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).

* Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: Our work poses no such risks, so this question is not applicable. We do not introduce any data or models. Guidelines:
* The answer NA means that the paper poses no such risks.
* Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.
* Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.
* We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
* **Licensees for existing assets*
* Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [NA] Justification: Our work does not use existing assets (other than the referenced papers), so this question is not applicable. All papers covered in the review are referenced in sufficient detail, so that the readers can access them. Guidelines:
* The answer NA means that the paper does not use existing assets.
* The authors should cite the original paper that produced the code package or dataset.
* The authors should state which version of the asset is used and, if possible, include a URL.
* The name of the license (e.g., CC-BY 4.0) should be included for each asset.
* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: Our work does not release any new assets, so this question is not applicable. We release the paper with the most permissible license available for NeurIPS submissions. Finally, we will release the review data upon acceptance. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: Our paper does not involve crowdsourcing or research with human subjects, so this question is not applicable. The work was in its entirety carried out by the authors. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: Our work does not involve crowdsourcing or research with human subjects, so this question is not applicable. We did not require an IRB approval or equivalent to carry out this work.

Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.