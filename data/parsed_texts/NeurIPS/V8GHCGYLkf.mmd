# Temporally Disentangled Representation Learning

under Unknown Nonstationarity

 Xiangchen Song\({}^{1}\) Weiran Yao\({}^{2}\) Yewen Fan\({}^{1}\) Xinshuai Dong\({}^{1}\) Guangyi Chen\({}^{1,3}\) Juan Carlos Niebles\({}^{2}\) Eric Xing\({}^{1,3}\) Kun Zhang\({}^{1,3}\)

\({}^{1}\)Carnegie Mellon University

\({}^{2}\)Salesforce Research

\({}^{3}\)Mohamed bin Zayed University of Artificial Intelligence

Part of the work was done while interning at Salesforce Research

###### Abstract

In unsupervised causal representation learning for sequential data with time-delayed latent causal influences, strong identifiability results for the disentanglement of causally-related latent variables have been established in stationary settings by leveraging temporal structure. However, in _nonstationary_ setting, existing work only partially addressed the problem by either utilizing observed auxiliary variables (e.g., class labels and/or domain indexes) as side-information or assuming simplified latent causal dynamics. Both constrain the method to a limited range of scenarios. In this study, we further explored the Markov Assumption under time-delayed causally related process in _nonstationary_ setting and showed that under mild conditions, the independent latent components can be recovered from their nonlinear mixture up to a permutation and a component-wise transformation, _without_ the observation of auxiliary variables. We then introduce NCTRL, a principled estimation framework, to reconstruct time-delayed latent causal variables and identify their relations from measured sequential data only. Empirical evaluations demonstrated the reliable identification of time-delayed latent causal influences, with our methodology substantially outperforming existing baselines that fail to exploit the nonstationarity adequately and then, consequently, cannot distinguish distribution shifts.

## 1 Introduction

Causal reasoning for time-series data is a long-lasting yet fundamental task [1; 2; 3]. The majority of the studies focus on the temporal causal discovery among observed variables [4; 5; 6]. However, in many real-world scenarios, the observed data (e.g., image pixels in videos) instead of having direct causal edges, are generated by some causally related latent temporal processes or confounders. Learning causal relations has practical use cases, which benefit a lot of downstream tasks. However, estimating latent causal structures among those unobserved variables purely from observations without appropriate class of assumptions is an extremely challenging task (i.e. the latent variables are generally not identifiable) [7; 8].

Under the topic of unsupervised representation learning via nonlinear Independent Component Analysis (ICA), some strong identifiability results of the latent variables have been established [9; 10; 11; 12; 13; 14] by introducing side information such as class labels and domain indices. Specifically focusing on time-series data, history information is also widely used as the side information for the identifiability of latent processes [15; 16; 17; 18]. However, existing studies mainly focused on and derived identifiability results in stationary settings [10; 16] (Fig 1 (a)) or nonstationary settings with explicitly observed domain indices [17; 18; 12] (Fig 1 (b)).

One can immediately tell the infeasibility of those two scenarios that general time-series data is usually nonstationary and the side information (class labels and domain indices) is usually unobserved. That is particularly true when considering real-world data such as video or signal sequences. It doesn't make any sense to assume that there exists a stationary transition function that is applied to the whole video clip. Take a very simple video clip of a mouse2[19] as an example, it is fairly clear that such a simple motion example can be divided into at least two phases (1) active phase in which the mouse is moving and (2) inactive phase in which the mouse is laying down. Instead of using a complex transition function to describe the whole video clip, a more reasonable assumption is that the same transition function is shared within the same phase, but across different phases, the transition functions are different, in other words, the transition function can be expressed as a function of the domain index. Also, it is worth mentioning that if such domain or phase indices is latent or unobserved, then we cannot directly utilize the existing framework to learn the latent causal dynamics. That is again a more realistic case that in general, the domain indices within a video are not accessible without expensive human annotation.

Footnote 2: https://dattalab.github.io/moseq2-website/images/sample-extraction.gif.

Recently, HMNLICA [14] attempted to resolve the problem by introducing Markov Assumption on the nonstationary discrete domain variable, they assumed the domain indices follow a first-order Markov Chain and estimated the domain information purely from observed data. However, HMNLICA assumes temporally mutually independent sources in the data-generating process (conditioning on domain indices), i.e. they don't allow latent variables to have time-delayed causal relations in between (Fig 1 (c)). Such an assumption imposed a huge negative impact on the usability of those methods. Considering the video of the little mouse example, the \(\mathbf{x}_{t}\)s are the observed video frames, \(\mathbf{z}_{t}\)s can be the independent motion dynamics or causal process such as position, velocity, (angular) momentum, etc, and \(c_{t}\)s are the phases or actions such as standing up (active) and laying down (inactive). To accommodate for such general sequential data, time-delayed temporal dependence should be considered in the latent \(\mathbf{z}_{t}\) space (Fig 1 (d)), otherwise, it is impossible to model a complex video data's temporal relation purely from discrete, domain indices. Also to make sure that the latent independent components can be recovered, temporally conditional independence should also be enforced, i.e. Each dimension of \(\mathbf{z}_{t}\) is conditionally independent given the history \(\mathbf{z}_{\text{history}}\). To this end, a natural question is:

_How can we establish identifiability of nonlinear ICA for general sequential data with nonstationary causally-related process without observing auxiliary variable?_

To answer this question, we first formulate the latent nonstationary states as a discrete Markov process and further explore the Markov Assumption [20] which is introduced for identifiability of nonlinear ICA in HMNLICA [14] and provided stronger identifiability result corresponding to the conditional emission distribution (i.e. the transition function of different domains) and the transition matrix of

Figure 1: Graphic models for three different settings in causally related time-delayed time series data with a visual illustration. (a) is a _stationary_ setting in which the transition function \(\mathbf{z}_{t+1}=f_{z}(\mathbf{z}_{t})\) stays universally the same. (b) is the setting widely explored in existing work, in which the transition function \(f_{z}\) changes according to different domains (denoted as \(c_{t}\)), and all those domain indices are observed. (c) capture the unobserved domain indices by introducing a Markov chain on \(c_{t}\). (d) is a more general form to model the time series data in this work. It allows nonstationary settings and it doesnâ€™t require the domain indices to be observed.

the Markov process. Specifically, we generalized the identifiability of Hidden Markov Models in [20] to accommodate time-delayed causally-related non-parametric transitions in latent space (Thm. 1). Then we utilize the linear independence (Thm. 2) to further establish the identifiability of \(\mathbf{z}_{t}\).

The main contributions of this work can be summarized as follows:

* To our best knowledge, this is the first identifiability result that can handle the nonstationary time-delayed causally-related latent temporal processes without the auxiliary variable. We formulate the problem, especially the nonstationary states into the Markov process, establish identifiability purely from observed data, and then show strong identifiability of latent independent components.
* We present NCTRL, Nonstationary Causal Temporal Representation Learning, a principled framework to recover time-delayed latent causal variables and identify their relations from measured sequential data under unobserved different distribution shifts.
* Experiments on both synthetic and real-world datasets demonstrate the effectiveness of the proposed method in recovering the latent variables.

## 2 Problem Formulation

### Time Series Generative Model

Assume we observe \(n\)-dimensional time-series data at discrete time steps, \(\mathbf{X}=\{\mathbf{x}_{1},\mathbf{x}_{2},\ldots,\mathbf{x}_{T}\}\), where each \(\mathbf{x}_{t}\in\mathcal{X}\) is generated from time-delayed causally related hidden components \(\mathbf{z}_{t}\in\mathbb{R}^{n}\) by the invertible mixing function:

\[\mathbf{x}_{t}=\mathbf{g}(\mathbf{z}_{t}).\] (1)

In addition to latent components \(\mathbf{z}_{t}\), there is an extra hidden variable \(c_{t}\) which is discrete with cardinality \(|\,c_{t}\,|=C\), it follows a first-order Markov process controlled by a \(C\times C\) transition matrix \(\mathbf{A}\), in which the \(i,j\)-th entry \(A_{i,j}\) is the probability to transit from state \(i\) to \(j\).

\[c_{1},c_{2},\ldots,c_{t}\sim\text{Markov Chain}(\mathbf{A})\] (2)

For \(i\in\{1,\ldots,n\}\), \(z_{it}\), as the \(i\)-th component of \(\mathbf{z}_{t}\), is generated by (some) components of history information \(\mathbf{z}_{t-1}\), discrete nonstationary indicator \(c_{t}\), and noise \(\epsilon_{it}\).

\[z_{it}=f_{i}(\{z_{j,t-1}\,|\,z_{j,t-\tau}\in\mathbf{Pa}(z_{it})\},c_{t}, \epsilon_{it})\quad with\quad\epsilon_{it}\sim p_{\epsilon_{i}}\] (3)

where \(\mathbf{Pa}(z_{it})\) is the set of latent factors that directly cause \(z_{it}\), which can be any subset of \(\mathbf{z}_{\text{Hx}}=\{\mathbf{z}_{t-1},\mathbf{z}_{t-2},\ldots,\mathbf{z}_ {t-L}\}\) up to history information maximum lag \(L\). The components of \(\mathbf{z}_{t}\) are mutually independent conditional on \(\mathbf{z}_{\text{Hx}}\) and \(c_{t}\).

### Identifiability of Latent Causal Processes and Time-Delayed Latent Causal Relations

We define the identifiability of time-delayed latent causal processes in the representation function space in **Definition 1**. Furthermore, if the estimated latent processes can be identified at least up to permutation and component-wise invertible nonlinearities, the latent causal relations are also immediately identifiable because conditional independence relations fully characterize time-delayed causal relations in a time-delayed causally sufficient system, in which there are no latent causal confounders in the (latent) causal processes. Note that invertible component-wise transformations on latent causal processes do not change their conditional independence relations.

**Definition 1** (Identifiable Latent Causal Processes).: _Formally let \(\mathbf{X}=\{\mathbf{x}_{1},\mathbf{x}_{2},\ldots,\mathbf{x}_{T}\}\) be a sequence of observed variables generated by the true temporally causal latent processes specified by \((f_{i},p(\epsilon_{i}),\mathbf{A},\mathbf{g})\) given in Eqs. (1), (2), and (3). A learned generative model \((\hat{f}_{i},\hat{p}(\epsilon_{i}),\hat{\mathbf{A}},\hat{\mathbf{g}})\) is observationally equivalent to \((f_{i},p(\epsilon_{i}),\mathbf{A},\mathbf{g})\) if the model distribution \(p_{\hat{g},\hat{p}_{\epsilon},\hat{\mathbf{A}},\hat{\mathbf{g}}}(\{\mathbf{x}_ {1},\mathbf{x}_{2},\ldots,\mathbf{x}_{T}\})\) matches the data distribution \(p_{f_{i},p_{\epsilon},\mathbf{A},\mathbf{g}}(\{\mathbf{x}_{1},\mathbf{x}_{2},\ldots,\mathbf{x}_{T}\})\) everywhere. We say latent causal processes are identifiable if observational equivalence can lead to identifiability of the latent variables up to permutation \(\pi\) and component-wise invertible transformation \(T\):_

\[p_{\hat{f}_{i},\hat{p}_{\epsilon_{i}},\hat{\mathbf{A}},\hat{ \mathbf{g}}}(\{\mathbf{x}_{1},\mathbf{x}_{2},\ldots,\mathbf{x}_{T}\}) =p_{f_{i},p_{\epsilon_{i}},\mathbf{A},\mathbf{g}}(\{\mathbf{x}_{1}, \mathbf{x}_{2},\ldots,\mathbf{x}_{T}\})\] (4) \[\Rightarrow\hat{\mathbf{g}}^{-1}(\mathbf{x}_{t}) =T\circ\pi\circ\mathbf{g}^{-1}(\mathbf{x}_{t}),\quad\forall \mathbf{x}_{t}\in\mathcal{X},\]

_where \(\mathcal{X}\) is the observation space._Identifiability Theory

In this section, we showed that under mild conditions, the latent variable \(\mathbf{z}_{t}\) is identifiable up to permutation and a component-wise transformation. The theoretical results can be divided into two parts (1) identifiability of the nonstationarity and (2) identifiability of the independent components. As introduced above, the major challenge comes from the unobserved domain indices or nonstationary indicators (\(c_{t}\) in our graphic models). We first establish the identifiability of the different conditional distributions from the observed data and then show that the latent variables \(\mathbf{z}\) are identifiable. The complete proofs can be found in Appendix A.

### Identifiability of Nonstationary Hidden States

Gassiat et al.[20] showed that the conditional emission distributions in Hidden Markov Models and the transition matrix are identifiable up to label swapping. We first generalize it to the autoregressive setting to accommodate for the time-delayed causal relation, i.e. we showed the identifiability of conditional emission distributions \(p(\mathbf{x}_{t}|\mathbf{x}_{t-1},c)\).

**Theorem 1**.: _(identifiability of the nonstationarity with Markov Assumptions) Suppose the observed data is generated following the nonlinear ICA framework as defined in Eqs. (1), (2) and (3). Suppose the following assumptions (Markov Assumptions) hold:_

* _For the Markov process, the number of latent states,_ \(C\)_, is known._
* _The transition matrix_ \(\mathbf{A}\) _is full rank._

_Use \(\mu_{1},\ldots,\mu_{C}\in\mathbb{R}^{n}\) to denote nonparametric probability distributions of the \(C\) emission distributions \(\mu_{c}=p(\mathbf{x}_{t}\,|\,\mathbf{x}_{t-1},c)\). Then the parameters \(\mathbf{A}\) and \(M=(\mu_{1},\ldots,\mu_{C})\) are identifiable given the distribution, \(\mathbb{P}^{(4)}_{\mathbf{A},M}\), of at least 4 consecutive observations \(\mathbf{x}_{t},\mathbf{x}_{t+1},\mathbf{x}_{t+2},\mathbf{x}_{t+3}\), up to label swapping of the hidden states, that is:_

_If \(\widetilde{\mathbf{A}}\) is a \(C\times C\) transition matrix and if \(\widetilde{\pi}(c)\) is a stationary distribution of \(\widetilde{\mathbf{A}}\) with \(\widetilde{\pi}(c)>0\)\(\forall c\in\{1,\ldots,C\}\), and if \(\widetilde{M}=(\tilde{\mu}_{1},\ldots,\tilde{\mu}_{C})\) are \(C\) probability distributions on \(\mathbb{R}^{n}\) that verify the equality of the distribution functions \(\mathbb{P}^{(4)}_{\widetilde{\mathbf{A}},\widetilde{M}}=\mathbb{P}^{(4)}_{ \mathbf{A},M}\), then there exists a permutation \(\sigma\) of the set \(\{1,\ldots,C\}\) such that for all \(k,l=1,\ldots,C\) we have \(\tilde{A}_{k,l}=A_{\sigma(k),\sigma(l)}\) and \(\tilde{\mu}_{k}=\mu_{\sigma(k)}\)._

For notational simplicity, and without loss of generality, we can assume the components are ordered such that \(c=\sigma(c)\). That leads us to the identifiability of the nonstationarity in the system i.e. up to label swapping of the hidden states, the conditional emission distributions \(p(\mathbf{x}_{t}|\mathbf{x}_{t-1},c_{t})\) and transition matrix \(\mathbf{A}\) are identifiable, hence providing us a bridge to further leverage the temporal independence condition in the latent space to establish the identifiability result for demixing function or in other words the latent variables \(\mathbf{z}_{t}\).

### Identifiability of Latent Causal Processes

To incorporate nonlinear ICA into the Markov Assumption we define the emission distribution \(p(\mathbf{x}_{t}\,|\,\mathbf{x}_{t-1},c)\) as a deep latent variable model. First, the latent independent component variables \(\mathbf{z}_{t}\in\mathbb{R}^{n}\) are generated from a factorial prior, given the hidden state \(c_{t}\) and previous \(\mathbf{z}_{t-1}\), as

\[p(\mathbf{z}_{t}\,|\,\mathbf{z}_{t-1},c_{t})=\prod_{k=1}^{n}p(z_{kt}\,|\, \mathbf{z}_{t-1},c_{t}).\] (5)

Second, the observed data \(\mathbf{x}_{t}\) is generated by a nonlinear mixing function as in Eq. (1) which is assumed to be bijective with inverse given by \(\mathbf{z}_{t}=\mathbf{g}(\mathbf{x}_{t})\). Let \(\eta_{kt}(c_{t})\triangleq\log p(z_{kt}|\mathbf{z}_{t-1},c_{t})\), and assume that \(\eta_{kt}(c_{t})\) is twice differentiable in \(z_{kt}\) and is differentiable in \(z_{l,t-1}\), \(l=1,2,...,n\). Note that the parents of \(z_{kt}\) may be only \(c_{t}\) and a subset of \(\mathbf{z}_{t-1}\); if \(z_{l,t-1}\) is not a parent of \(z_{kt}\), then \(\frac{\partial\eta_{k}}{\partial z_{1,t-1}}=0\).

**Theorem 2**.: _(identifiability of the independent components) Suppose there exists an invertible function \(\hat{\mathbf{g}}^{-1}\), which is the estimated demixing function that maps \(\mathbf{x}_{t}\) to \(\hat{\mathbf{z}}_{t}\), i.e.,_

\[\hat{\mathbf{z}}_{t}=\hat{\mathbf{g}}^{-1}(\mathbf{x}_{t})\] (6)_such that the components of \(\hat{\mathbf{z}}_{t}\) are mutually independent conditional on \(\hat{\mathbf{z}}_{t-1}\). Let_

\[\begin{split}\mathbf{v}_{k,t}(c)&\triangleq\Big{(} \frac{\partial^{2}\eta_{kt}(c)}{\partial z_{k,t}\partial z_{1,t-1}},\frac{ \partial^{2}\eta_{kt}(c)}{\partial z_{k,t}\partial z_{2,t-1}},...,\frac{ \partial^{2}\eta_{kt}(c)}{\partial z_{k,t}\partial z_{n,t-1}}\Big{)}^{\intercal},\\ \hat{\mathbf{v}}_{k,t}(c)&\triangleq\Big{(}\frac{ \partial^{3}\eta_{kt}(c)}{\partial z_{k,t}^{2}\partial z_{1,t-1}},\frac{ \partial^{3}\eta_{kt}(c)}{\partial z_{k,t}^{2}\partial z_{2,t-1}},...,\frac{ \partial^{3}\eta_{kt}(c)}{\partial z_{k,t}^{2}\partial z_{n,t-1}}\Big{)}^{ \intercal}.\end{split}\] (7)

_And_

\[\begin{split}\mathbf{s}_{kt}&\triangleq\Big{(} \mathbf{v}_{kt}(1)^{\intercal},...,\mathbf{v}_{kt}(C)^{\intercal},\frac{ \partial^{2}\eta_{kt}(2)}{\partial z_{kt}^{2}}-\frac{\partial^{2}\eta_{kt}(1) }{\partial z_{kt}^{2}},...,\frac{\partial^{2}\eta_{kt}(C)}{\partial z_{kt}^{ 2}}-\frac{\partial^{2}\eta_{kt}(C-1)}{\partial z_{kt}^{2}}\Big{)}^{\intercal},\\ \hat{\mathbf{s}}_{kt}&\triangleq\Big{(}\hat{\mathbf{ v}}_{kt}(1)^{\intercal},...,\hat{\mathbf{v}}_{kt}(C)^{\intercal},\frac{ \partial\eta_{kt}(2)}{\partial z_{kt}}-\frac{\partial\eta_{kt}(1)}{\partial z_ {kt}},...,\frac{\partial\eta_{kt}(C)}{\partial z_{kt}}-\frac{\partial\eta_{ kt}(C-1)}{\partial z_{kt}}\Big{)}^{\intercal}.\end{split}\] (8)

_If for each value of \(\mathbf{z}_{t}\), \(\mathbf{s}_{1t},\hat{\mathbf{s}}_{1t},\mathbf{v}_{2t},\hat{\mathbf{s}}_{2t},...,\mathbf{s}_{nt},\hat{\mathbf{s}}_{nt}\), as \(2n\) function vectors \(\mathbf{s}_{k,t}\) and \(\hat{\mathbf{s}}_{k,t}\), with \(k=1,2,...,n\), are linearly independent, then \(\hat{\mathbf{z}}_{t}\) must be an invertible, component-wise transformation of a permuted version of \(\mathbf{z}_{t}\)._

So far, the identifiability result has been established without observing the nonstationarity indicators such as domain indices. In the next section, a novel Variational Auto-Encoder based method is introduced to estimate the demixing function \(\hat{\mathbf{g}}^{-1}\).

## 4 Nctrl: Nonstationary Causal Temporal Representation Learning

In this section, we present the details of NCTRL to estimate the latent causal processes under unobserved nonstationary distribution shift, given the identifiability results in Sec 3. First, we show that our framework includes three modules, Autoregressive Hidden Markov Module, Prior Network, and Encoder-Decoder Module. Then, we provide the optimization objective of our model training including an HMM free energy lower bound, a reconstruction likelihood loss, and a KL divergence.

### Model Architecture

Our framework extends Sequential Variational Auto-Encoders [21] with tailored modules to model nonstationarity, and enforces the conditions in Sec. 3 as constraints. We give the estimation procedure of the latent causal dynamics model in Eq. (3). The model architecture is showcased in Fig. 2. The framework has three major components (1) Autoregressive Hidden Markov Module (ARHMM), (2) Prior Network Module, and (3) Encoder-Decoder Module.

Autoregressive Hidden Markov Module (ARHMM)The first component of our framework is ARHMM which deals with the nonstationarity with unobserved domains. As discussed in Thm 1, the transition function or the conditional emission distributions across different domains together with the Markov transition matrix \(\mathbf{A}\) are identifiable. This module estimates the transition function of different domains \(p(\mathbf{x}_{t}|\mathbf{x}_{t-1},c_{t})\) and the transition matrix \(\mathbf{A}\) of the Markov process, and ultimately decodes the optimal domain indices \(\{\hat{c}_{1},\hat{c}_{2},\dots,\hat{c}_{T}\}\) via the Viterbi algorithm.

Prior Network ModuleTo better estimate the prior distribution \(p(\hat{z}_{t}|\hat{\mathbf{z}}_{\text{Hx}},c_{t})\), let \(\mathbf{z}_{\text{Hx}}\) denote the lagged latent variables up to maximum time lag \(L\). We evaluate \(p(\hat{z}_{t}|\hat{\mathbf{z}}_{\text{Hx}},c_{t})=p_{\epsilon}\left(\hat{f}_{z}^ {-1}(\hat{z}_{t},\hat{\mathbf{z}}_{\text{Hx}},\hat{\mathbf{\theta}}_{c_{t}}) \right)\left|\frac{\partial\hat{f}_{z}^{-1}}{\partial\hat{z}_{t}}\right|\) by learning a holistic inverse dynamics \(\hat{f}_{z}^{-1}\) that takes the estimated change factors for dynamics \(\hat{\mathbf{\theta}}_{c_{t}}\) as inputs. Conditional independence of the estimated latent variables \(p(\hat{\mathbf{z}}_{t}|\hat{\mathbf{z}}_{\text{Hx}})\) is enforced by summing up all estimated component densities when obtaining the joint \(p(\mathbf{z}_{t}|\mathbf{z}_{\text{Hx}},c_{t})\) in Eq. 9. Given that the Jacobian is lower-triangular, we can compute its determinant as

Figure 2: Illustration of NCTRLwith (1) Autoregressive Hidden Markov Module, (2) Prior Network, and (3) Encoder-Decoder Module.

the product of diagonal terms. The detailed derivations are given in Appendix B.2.

\[\log p\left(\hat{\mathbf{z}}_{t}|\hat{\mathbf{z}}_{\text{Hx}},c_{t}\right)= \underbrace{\sum_{i=1}^{n}\log p(\hat{\epsilon}_{i}|c_{t})}_{\text{ Conditional independence}}+\underbrace{\sum_{i=1}^{n}\log\left|\frac{\partial\hat{f}_{i}^{-1}}{ \partial\hat{z}_{it}}\right|}_{\text{Lower-triangular Jacobian}}\] (9)

Encoder-Decoder ModuleThe third component is a Variational Auto-Encoder based module which utilizes reconstruction loss to enforce the invertibility of learned mixing function \(\hat{\mathbf{g}}\). Specifically, the encoder fits the demixing function \(\hat{\mathbf{g}}^{-1}\) and the decoder fits the mixing function \(\hat{\mathbf{g}}\). The implementation details are in Appendix B.

### Optimization

The first training objective of NCTRL is to maximize the Log-likelihood of the observed data:

\[\log p_{\theta_{\text{HMM}}}(\{\mathbf{x}_{1},\mathbf{x}_{2},\ldots,\mathbf{ x}_{T}\})\] (10)

where \(\boldsymbol{\theta}_{\text{HMM}}\) represents the HMM training parameters. Then the free energy lower bound can be defined as:

\[-\mathcal{L}_{\text{HMM}}=\mathcal{L}(q(\mathbf{c}),\boldsymbol{\theta}_{ \text{HMM}})\triangleq\mathbb{E}_{q(\mathbf{c})}\left[\log p_{\boldsymbol{ \theta}_{\text{HMM}}}(\mathbf{x}_{1},\mathbf{x}_{2},\ldots,\mathbf{x}_{T}, \mathbf{c})\right]-\mathbf{H}(q(\mathbf{c}))\] (11)

Consistent with the theory part, the first training objective is to maximize data log-likelihood in the ARHMM module to get optimal \(q(\mathbf{c}^{\star})\).

\[q(\mathbf{c}^{\star})\triangleq\operatorname*{arg\,max}_{q(\mathbf{c})} \mathcal{L}(q(\mathbf{c}),\boldsymbol{\theta}_{\text{HMM}})\] (12)

which can easily be computed by the Forward-Backward algorithm and luckily all of it is differentiable to the HMM training parameters \(\boldsymbol{\theta}_{\text{HMM}}\)(transition matrix \(\mathbf{A}\) and transition function parameters \(\boldsymbol{\theta}_{f}\)).

Then the second part is to maximize the Evidence Lower BOund (ELBO) for the VAE framework, which can be written as (complete derivation steps are in Appendix B.3):

\[\begin{split}\text{ELBO}&\triangleq\log p_{\text{ data}}(\mathbf{X})-D_{KL}(q_{\phi}(\mathbf{Z}|\mathbf{X})||p_{\text{data}}( \mathbf{Z}|\mathbf{X}))\\ &=\underbrace{\mathbb{E}_{\mathbf{z}_{t}}\sum_{t=1}^{T}\log p_{ \text{data}}(\mathbf{x}_{t}|\mathbf{z}_{t})}_{-\mathcal{L}_{\text{Reson}}}+ \underbrace{\mathbb{E}_{\mathbf{c}}\left[\sum_{t=1}^{T}\log p_{\text{data}}( \mathbf{z}_{t}|\mathbf{z}_{\text{Hx}},c_{t})-\sum_{t=1}^{T}\log q_{\phi}( \mathbf{z}_{t}|\mathbf{x}_{t})\right]}_{-\mathcal{L}_{\text{KLD}}}\end{split}\] (13)

We use mean-squared error (MSE) for the reconstruction likelihood loss \(\mathcal{L}_{\text{Recon}}\). The KL divergence \(\mathcal{L}_{\text{KLD}}\) is estimated via a sampling approach since with a learned nonparametric transition prior, the distribution does not have an explicit form. Specifically, we obtain the log-likelihood of the posterior, evaluate the prior \(\log p\left(\hat{\mathbf{z}}_{t}|\hat{\mathbf{z}}_{\text{Hx}},c_{t}\right)\) in Eq. (9), and compute their mean difference in the dataset as the KL loss: \(\mathcal{L}_{\text{KLD}}=\mathbb{E}_{\hat{\mathbf{z}}_{t}\sim q(\hat{\mathbf{ z}}_{t}|\mathbf{x}_{t})}\log q(\hat{\mathbf{z}}_{t}|\mathbf{x}_{t})-\log p \left(\hat{\mathbf{z}}_{t}|\hat{\mathbf{z}}_{\text{Hx}},c_{t}\right)\).

## 5 Experiments

We evaluate the identifiability results of NCTRL on a number of simulated and real-world temporal datasets. We first introduce the evaluation metrics and baselines and then discuss the datasets we used in our experiments. Lastly, we show the experiment results discuss the performance, and make comparisons.

### Evaluation Metrics

**Mean Correlation Coefficient (MCC)** To evaluate the identifiability of the latent variables, we compute the Mean Correlation Coefficient (MCC) on the test dataset. MCC is a standard metric in the ICA literature for continuous variables which measure the identifiability of the learned latent causal processes. MCC is close to 1.0 when latent variables are identifiable up to permutation and component-wise invertible transformation in the noiseless case.

**Mean Square Error (MSE) for estimating A** As introduced in the theory, the \(\mathbf{A}\) is identifiable in our setting, which means that our proposed method can provide accurate estimation for the transition matrix \(\mathbf{A}\), to valid such a claim, we use mean square error (MSE) to capture the distance between the estimated \(\hat{\mathbf{A}}\) and ground truth \(\mathbf{A}\).

**Accuracy for estimating \(c_{t}\)** We also test the accuracy for estimating the discrete domain indices \(c_{t}\) supplementary to the MSE for \(\mathbf{A}\) since in theory, the \(\mathbf{A}\) is identifiable but the \(c_{t}\) is generally not identifiable, which is relatively easy to understand as an analogy in Hidden Markov Models, the transition matrix is identifiable but we can only "infer" the best possible discrete variables but cannot establish identifiability for it.

It is also worth mentioning that the MSE and Accuracy are influenced by the permutation, which is also true in clustering evaluation problems. Here we explored all permutations and selected the best possible assignment for evaluation.

### Baselines

The following identifiable nonlinear ICA methods are used: (1) BetaVAE [22] which ignores both history and nonstationarity information. (2) i-VAE [12] and TCL [9] which leverage nonstationarity to establish identifiability but assume independent factors. (3) SlowVAE [16], and PCL [10] which exploit temporal constraints but assume independent sources and stationary processes. (4) TDRL [18] which assumes nonstationary, causal processes but with observed domain indices. (5) HMNLICA [14] which considers the unobserved nonstationary part in the data generation process but doesn't allow any causally related time-delayed relations.

### Simulated Results

We generate two synthetic datasets corresponding to different complexity of the nonlinear mixing function \(\mathbf{g}\). Both synthetic datasets satisfy our identifiability conditions in the theorems following the procedures in Appendix B.4. As in Table 1, NCTRL can recover the latent processes under unknown nonstationary distribution shifts with high MCCs (>0.95). The baselines that do not exploit history (i.e., BetaVAE, i-VAE, TCL), with independent source assumptions (Slow-VAE, PCL), consider limited nonstationary cases (TDRL) distort the identifiability results. The only baseline that considers the unknown nonstationarity in the domain indices (HMNLICA) explored the Markov Assumption but doesn't allow a time-delayed causal process and hence suffers a poor result (MCC 0.58).

On the other hand, the difference between dataset A and dataset B is the nonlinearity in the mixing function, dataset A has a relatively simple nonlinear mixing function, on the contrary, dataset B has more complex nonlinearity. Some variability has been observed among the relative performance ranks of different baselines. For example, i-VAE showed a great discrepancy between the two datasets, which revived the weakness of capturing complex nonlinearity in the unknown nonstationary distribution shift environments. Again we also observed that our proposed method can constantly recover the latent independent components with high MCC which indicates on both datasets the model is identifiable and the estimation algorithm is highly effective.

To further validate if NCTRL successfully recovered the Markov transition matrix \(\mathbf{A}\) and inferred the domain indices \(c_{t}\) with high accuracy. We further examine the accuracy for estimating nonstationary domain indices \(c_{t}\) and the mean square error estimating the transition matrix \(\mathbf{A}\). As shown in Table 2 the result is consistent with our theory in which the transition matrix \(\mathbf{A}\) is identifiable and we can estimate it with

\begin{table}
\begin{tabular}{l|c|c|c} \hline \hline \multirow{2}{*}{**Method**} & \multicolumn{3}{c}{**Mean Correlation Coefficient (MCC)**} \\ \cline{2-4}  & **Dataset A** & **Dataset B** & **Ave.** \\ \hline BetaVAE & 44.02 \(\pm\) 3.11 & 47.48 \(\pm\) 10.58 & 45.75 \\ i-VAE & 89.74 \(\pm\) 3.38 & 44.50 \(\pm\) 0.25 & 67.12 \\ TCL & 37.12 \(\pm\) 0.60 & 56.33 \(\pm\) 3.77 & 46.73 \\ SlowVAE & 33.84 \(\pm\) 0.60 & 53.92 \(\pm\) 3.56 & 43.88 \\ PCL & 42.41 \(\pm\) 2.87 & 63.66 \(\pm\) 2.77 & 53.04 \\ HMNLICA & 59.82 \(\pm\) 4.94 & 57.25 \(\pm\) 1.45 & 58.54 \\ TDRL & 83.99 \(\pm\) 1.92 & 72.02 \(\pm\) 2.76 & 78.01 \\ \hline NCTRL & **98.85 \(\pm\) 0.30** & **99.01 \(\pm\) 0.24** & **98.93** \\ \hline \hline \end{tabular}
\end{table}
Table 1: Experiment results of two synthetic datasets on baselines and proposed NCTRL, we run the experiments with five different random seeds and calculate the average with standard derivation. The best results are shown in **bold**.

high accuracy. For the nonstationary domain indices \(c_{t}\) even though there is no identifiability result governing the estimation accuracy, it can still be inferred pretty well since it is nothing but a decoding problem in Hidden Markov Models.

### Real-world Applications

Video data - Modified CartPole EnvironmentWe evaluate NCTRL on the modified CartPole [23] video dataset and compare the performances with the baselines. Modified Cartpole is a nonlinear dynamical system with cart positions \(x_{t}\) and pole angles \(\theta_{t}\) as the true state variables. The dataset descriptions are in Appendix B.5. Similar to the synthetic dataset, we randomly initialize a Markov chain and roll out a series of \(c_{t}\), and configure the CartPole environment with respect to the \(c_{t}\). Specifically, we use five domains with different configurations of cart mass, pole mass, gravity, and noise levels. Together with the two discrete actions (i.e., left and right). By doing so, the nonstationarity is enforced, and since we can control and access all intermediate states in the system, all metrics including MCC and \(c_{t}\) accuracy together with **A** MSE can be easily calculated. We fit NCTRL with two-dimensional causal factors. We set the latent size \(n=2\) and the lag number \(L=2\). In Fig. 3, the latent causal processes are recovered, as seen from (a) high MCC for the latent causal processes; (b) the latent factors are estimated up to component-wise transformation; and (c) the latent traversals confirm the two recovered latent variables correspond to the position and pole angle.

Similar to Table 1 and 2, we compare our NCTRL with baseline methods. In addition, we also compare with SKD [24], a state-of-the-art sequential disentangle representation learning method without identifiability guarantee. In Table 3 and 4 we can see that compared with TDRL, our NCTRL can recover the latent processes under unknown nonstationary distribution shifts with high MCCs (>0.95) with highly accurate estimated transition matrix **A** and high quality inferred \(c_{t}\). Specifically, by comparing the result of SKD, the MCC for SKD is better than a variety of baselines,

\begin{table}
\begin{tabular}{c|c|c} \hline \hline \multicolumn{3}{c|}{**Unknown Nonstationary Metrics**} \\ \hline
**Dataset** & **Accuracy estimating**\(c_{t}\) & **MSE estimating A** \\ \hline A & 89.96 \(\pm\) 0.24 & 1.01 \(\times 10^{-3}\)\(\pm\) 1.67 \(\times 10^{-4}\) \\ B & 89.84 \(\pm\) 0.29 & 1.08 \(\times 10^{-3}\)\(\pm\) 1.89 \(\times 10^{-4}\) \\ \hline \hline \end{tabular}
\end{table}
Table 2: Supplementary experiment results of two synthetic datasets on estimating domain indices \(c_{t}\) and transition matrix **A** in NCTRL, we run the experiments with five different random seeds and calculate the average with standard derivation.

Figure 3: Modified Cartpole results: (a) MCC for causally-related factors; (b) scatterplots between estimated and true factors; and (c) latent traversal on a fixed video frame

\begin{table}
\begin{tabular}{c c c c c c c} \hline \hline \multicolumn{5}{c}{**Mean Correlation Coefficient (MCC)**} \\ \hline
**BetaVAE** & **i-VAE** & **TCL** & **SlowVAE** & **SKD** & **TDRL** & **NCTRL** \\ \hline
57.54 & 60.14 & 65.07 & 63.16 & 73.24 & 85.26 & **96.06** \\ \hline \hline \end{tabular}
\end{table}
Table 3: Experiment results of CartPole dataset. The best results are shown in **bold**.

however, we can see the distinction between well-disentangled models and identifiable models, only the models with identifiability can find the ground truth latent variables with theoretical guarantee.

Video data - MoSeq DatasetWe test NCTRL framework to analyze mouse behavior video data from Wiltschko et al. [19], which represents the original application to clustering mouse behavior3, details of this dataset are available in Appendix B.6. Since there are no ground truth independent components in this particular real-world dataset, we analyze it by several visualizations to see if different domains can be properly identified and if the patterns in the recovered independent components are consistent with the recovered domain indices. We analyze the first video clip of mouse behavior data and visualize the two phases we discovered and segmented in Fig 4. We can clearly see from Fig 4 that there are different phases with the upper one actively moving and the lower one inactive. The recovered independent components showed a consistent pattern with the recovered phase or domain indices as shown in Fig 4.

Footnote 3: Dataset can be accessed via https://dattalab.github.io/moseq2-website/index.html

## 6 Related Work

Causal Discovery from Time SeriesUnderstanding the causal structure in time-series data is pivotal in areas such as machine learning [1], econometrics [2], and neuroscience [3]. A bulk of the research in this realm emphasizes determining the temporal causal links among observed variables. The primary techniques employed are constraint-based methods [25], which use conditional independence tests to ascertain causal structures, and score-based methods [26, 27], where scores are utilized to oversee a search operation. Some researchers also proposed a combination of these two methods [28, 29]. Additionally, Granger causality [30] and its nonlinear adaptations [31, 32] have gained widespread acceptance in this context.

\begin{table}
\begin{tabular}{c|c} \hline \hline \multicolumn{2}{c}{**Unknown Nonstationary Metrics**} \\ \hline
**Accuracy estimating \(c_{t}\)** & **MSE estimating A** \\ \hline
79.23 \(\pm\) 5.33 & 5.01 \(\times 10^{-2}\)\(\pm\) 1.23 \(\times 10^{-2}\) \\ \hline \hline \end{tabular}
\end{table}
Table 4: Supplementary experiment results of CartPole datasets on estimating domain indices \(c_{t}\) and transition matrix \(\mathbf{A}\) in NCTRL, we run the experiments with five different random seeds and calculate the average with standard derivation.

Figure 4: Result visualization of MoSeq dataset. (Active, Inactive) show two representative video frames for the active and inactive phases and (Independent Components) visualize the discovered independent components with corresponding phases tagged with different colors.

Nonlinear ICA for Time SeriesRecently, the significance of temporal structures and nonstationarities has been recognized in achieving identifiability within nonlinear ICA. Time-contrastive learning (TCL [9]) utilizes the independent sources principle, focusing on data segments' variability. On the other hand, Permutation-based contrastive (PCL [10]) offers a learning approach that distinguishes true independent sources from shuffled ones under the uniformly dependent assumption. HMNLICA [14] integrates nonlinear ICA with an HMM to address non-stationarity without segmenting data manually. The i-VAE [12] approach employs VAEs to capture the actual joint distribution between observed and auxiliary non-stationary domains, assuming an exponential families conditional distribution. The recent advancements in nonlinear ICA for time series include LEAP [17], (i-)CITRIS [33; 34], and TDRL [18]. While LEAP introduces a novel condition emphasizing non-stationary noise, TDRL delves deeper into a non-parametric environment within a nonstationary context. In contrast, CITRIS recommends utilizing intervention target data for pinpointing latent causal aspects, avoiding certain constraints but necessitating active intervention access.

Sequential DisentanglementMajority of existing work about sequential disentanglement focuses on architecture based on dynamical variational autoencoder (VAE) [35]. Early works [36; 37] separate dynamic factors from static factors using probabilistic methods. Then auxiliary tasks with self-supervisory signals [38] were introduced. C-DSVAE [39] utilized contrastive penalty terms with data augmentation to introduce additional inductive biases. In R-WAE [40], Wasserstein distance was introduced to replace KL divergence. To deal with video disentanglement [41; 42] explored generative adversarial network (GAN) architectures and [43] introduced a recurrent model with adversarial loss. FAVAE, [44] proposed a factorizing VAE and [45] proposed to learn hierarchical features. Finally, SKD [24] introduced a spectral loss term that leads to structured Koopman matrices and disentanglement.

## 7 Conclusion and Discussion

**Conclusion.** In this paper, we first established an identifiability theory for general sequential data with nonstationary causally-related processes under unknown distribution shifts. Then we presented NCTRL, a principled framework to recover the time-delayed latent causal variable identify their causal relations from measured data, and decode high-quality domain indices under Markov assumption. Experiment results on both synthetic datasets and real-world video datasets showed that our proposed method can recover the latent causal variables and their causal relations purely from measured data with the observation of auxiliary variables or domain indices.

**Limitation.** The basic limitation of this work is that the nonstationary domain indices are assumed to follow a Markov chain. Also, this work highly relies on the latent processes to have no instantaneous causal relations but only time-delayed influences. If the resolution of the time series is much lower, then it is usually violated and one has to find a way to deal with instantaneous causal relations. Extending our theories and framework to address the scenarios when more flexibility in the domain indices transition is allowed (i.e. beyond discrete variables following a Markov chain) and to address instantaneous dependency or instantaneous causal relations will be some of our future work.

**Boarder Impacts.** This work proposes a theoretical analysis and technical methods to learn the causal representation from time-series data, which facilitate the construction of more transparent and interpretable models to understand the causal effect in the real world. This could be beneficial in a variety of sectors, including healthcare, finance, and technology. In contrast, misinterpretations of causal relationships could also have significant negative implications in these fields, which must be carefully done to avoid unfair or biased predictions.

## 8 Acknowledgment

This project has been graciously funded by NGA HM04762010002, NSF IIS1955532, NSF CNS2008248, NIGMS R01GM140467, NSF IIS2123952, NSF BCS2040381, an Amazon Research Award, NSF IIS2311990, and DARPA ECOLE HR00112390063. This project is also partially supported by NSF Grant 2229881, the National Institutes of Health (NIH) under Contract R01HL159805, a grant from Apple Inc., a grant from KDDI Research Inc., and generous gifts from Salesforce Inc., Microsoft Research, and Amazon Research.

## References

* [1] Carlo Berzuini, Philip Dawid, and Luisa Bernardinell. _Causality: Statistical perspectives and applications_. John Wiley & Sons, 2012.
* [2] Eric Ghysels, Jonathan B Hill, and Kaiji Motegi. Testing for granger causality with mixed frequency data. _Journal of Econometrics_, 192(1):207-230, 2016.
* [3] Karl Friston. Causal modelling and brain connectivity in functional magnetic resonance imaging. _PLoS biology_, 7(2):e1000033, 2009.
* [4] Clive WJ Granger. Testing for causality: A personal viewpoint. _Journal of Economic Dynamics and control_, 2:329-352, 1980.
* [5] Mingming Gong, Kun Zhang, Bernhard Schoelkopf, Dacheng Tao, and Philipp Geiger. Discovering temporal causal relations from subsampled data. In _International Conference on Machine Learning_, pages 1898-1906. PMLR, 2015.
* [6] Aapo Hyvarinen, Kun Zhang, Shohei Shimizu, and Patrik O Hoyer. Estimation of a structural vector autoregression model using non-gaussianity. _Journal of Machine Learning Research_, 11(5), 2010.
* [7] Francesco Locatello, Stefan Bauer, Mario Lucic, Gunnar Raetsch, Sylvain Gelly, Bernhard Scholkopf, and Olivier Bachem. Challenging common assumptions in the unsupervised learning of disentangled representations. In _international conference on machine learning_, pages 4114-4124. PMLR, 2019.
* [8] Aapo Hyvarinen and Petteri Pajunen. Nonlinear independent component analysis: Existence and uniqueness results. _Neural networks_, 12(3):429-439, 1999.
* [9] Aapo Hyvarinen and Hiroshi Morioka. Unsupervised feature extraction by time-contrastive learning and nonlinear ica. _Advances in Neural Information Processing Systems_, 29:3765-3773, 2016.
* [10] Aapo Hyvarinen and Hiroshi Morioka. Nonlinear ica of temporally dependent stationary sources. In _Artificial Intelligence and Statistics_, pages 460-469. PMLR, 2017.
* [11] Aapo Hyvarinen, Hiroaki Sasaki, and Richard Turner. Nonlinear ica using auxiliary variables and generalized contrastive learning. In _The 22nd International Conference on Artificial Intelligence and Statistics_, pages 859-868. PMLR, 2019.
* [12] Ilyes Khemakhem, Diederik Kingma, Ricardo Monti, and Aapo Hyvarinen. Variational autoencoders and nonlinear ica: A unifying framework. In _International Conference on Artificial Intelligence and Statistics_, pages 2207-2217. PMLR, 2020.
* [13] Peter Sorrenson, Carsten Rother, and Ullrich Kothe. Disentanglement by nonlinear ica with general incompressible-flow networks (gin). _arXiv preprint arXiv:2001.04872_, 2020.
* [14] Hermanni Halva and Aapo Hyvarinen. Hidden markov nonlinear ica: Unsupervised learning from nonstationary time series. In _Conference on Uncertainty in Artificial Intelligence_, pages 939-948. PMLR, 2020.
* [15] Hermanni Halva, Sylvain Le Corff, Luc Lehericy, Jonathan So, Yongjie Zhu, Elisabeth Gassiat, and Aapo Hyvarinen. Disentangling identifiable features from noisy data with structured nonlinear ica. _arXiv preprint arXiv:2106.09620_, 2021.
* [16] David Klindt, Lukas Schott, Yash Sharma, Ivan Ustyuzhaninov, Wieland Brendel, Matthias Bethge, and Dylan Paiton. Towards nonlinear disentanglement in natural data with temporal sparse coding. _arXiv preprint arXiv:2007.10930_, 2020.
* [17] Weiran Yao, Yuewen Sun, Alex Ho, Changyin Sun, and Kun Zhang. Learning temporally causal latent processes from general temporal data. In _International Conference on Learning Representations_, 2022. URL https://openreview.net/forum?id=RD1LMjLJXdq.

* Yao et al. [2022] Weiran Yao, Guangyi Chen, and Kun Zhang. Temporally disentangled representation learning. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, _Advances in Neural Information Processing Systems_, 2022. URL https://openreview.net/forum?id=Vi-szWNA_Ue.
* Wiltschko et al. [2015] Alexander B. Wiltschko, Matthew J. Johnson, Giuliano Turilli, Ralph E. Peterson, Jesse M. Katon, Stan L. Pashkovski, Victoria E. Abraira, Ryan P. Adams, and Sandeep Robert Datta. Mapping sub-second structure in mouse behavior. _Neuron_, 88(6):1121-1135, 2015. ISSN 0896-6273. doi: https://doi.org/10.1016/j.neuron.2015.11.031. URL https://www.sciencedirect.com/science/article/pii/S0896627315010375.
* Gassiat et al. [2016] Elisabeth Gassiat, Alice Cleynen, and Stephane Robin. Inference in finite state space non parametric hidden markov models and applications. _Statistics and Computing_, 26:61-71, 2016.
* Yingzhen and Mandt [2018] Li Yingzhen and Stephan Mandt. Disentangled sequential autoencoder. In _International Conference on Machine Learning_, pages 5670-5679. PMLR, 2018.
* Higgins et al. [2017] Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick, Shakir Mohamed, and Alexander Lerchner. beta-VAE: Learning basic visual concepts with a constrained variational framework. In _International Conference on Learning Representations_, 2017. URL https://openreview.net/forum?id=Sy2fzU9g1.
* Huang et al. [2021] Biwei Huang, Fan Feng, Chaochao Lu, Sara Magliacane, and Kun Zhang. Adarl: What, where, and how to adapt in transfer reinforcement learning. _arXiv preprint arXiv:2107.02729_, 2021.
* Berman et al. [2023] Nimrod Berman, Ilan Naiman, and Omri Azencot. Multifactor sequential disentanglement via structured koopman autoencoders. In _The Eleventh International Conference on Learning Representations_, 2023. URL https://openreview.net/forum?id=6fuPle9tbnC.
* Entner and Hoyer [2010] Doris Entner and Patrik O Hoyer. On causal discovery from time series data using fci. _Probabilistic graphical models_, pages 121-128, 2010.
* Murphy et al. [2002] Kevin P Murphy et al. Dynamic bayesian networks. _Probabilistic Graphical Models, M. Jordan_, 7:431, 2002.
* Pamfil et al. [2020] Roxana Pamfil, Nisara Sriwattanaworachai, Shaan Desai, Philip Pilgerstorfer, Konstantinos Georgatzis, Paul Beaumont, and Bryon Aragam. Dynotears: Structure learning from time-series data. In _International Conference on Artificial Intelligence and Statistics_, pages 1595-1605. PMLR, 2020.
* Malinsky and Spirtes [2018] Daniel Malinsky and Peter Spirtes. Causal structure learning from multivariate time series in settings with unmeasured confounding. In _Proceedings of 2018 ACM SIGKDD Workshop on Causal Discovery_, pages 23-47. PMLR, 2018.
* Malinsky and Spirtes [2019] Daniel Malinsky and Peter Spirtes. Learning the structure of a nonstationary vector autoregression. In _The 22nd International Conference on Artificial Intelligence and Statistics_, pages 2986-2994. PMLR, 2019.
* Granger [1969] Clive WJ Granger. Investigating causal relations by econometric models and cross-spectral methods. _Econometrica: journal of the Econometric Society_, pages 424-438, 1969.
* Tank et al. [2018] Alex Tank, Ian Covert, Nicholas Foti, Ali Shojaie, and Emily Fox. Neural granger causality. _arXiv preprint arXiv:1802.05842_, 2018.
* Lowe et al. [2020] Sindy Lowe, David Madras, Richard Zemel, and Max Welling. Amortized causal discovery: Learning to infer causal graphs from time-series data. _arXiv preprint arXiv:2006.10833_, 2020.
* Lippe et al. [2022] Phillip Lippe, Sara Magliacane, Sindy Lowe, Yuki M Asano, Taco Cohen, and Stratis Gavves. Citris: Causal identifiability from temporal intervened sequences. In _International Conference on Machine Learning_, pages 13557-13603. PMLR, 2022.
* Lippe et al. [2022] Phillip Lippe, Sara Magliacane, Sindy Lowe, Yuki M Asano, Taco Cohen, and Efstratios Gavves. icitris: Causal representation learning for instantaneous temporal effects. _arXiv preprint arXiv:2206.06169_, 2022.

* Grin et al. [2021] Laurent Grin, Simon Leglaive, Xiaoyu Bie, Julien Diard, Thomas Hueber, and Xavier Alameda-Pineda. _Dynamical Variational Autoencoders: A Comprehensive Review_. 2021.
* Hsu et al. [2017] Wei-Ning Hsu, Yu Zhang, and James Glass. Unsupervised learning of disentangled and interpretable representations from sequential data. _Advances in neural information processing systems_, 30, 2017.
* Li and Mandt [2018] Yingzhen Li and Stephan Mandt. Disentangled sequential autoencoder. _arXiv preprint arXiv:1803.02991_, 2018.
* Zhu et al. [2020] Yizhe Zhu, Martin Renqiang Min, Asim Kadav, and Hans Peter Graf. S3VAE: Self-supervised sequential VAE for representation disentanglement and data generation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 6538-6547, 2020.
* Bai et al. [2021] Junwen Bai, Weiran Wang, and Carla P Gomes. Contrastively disentangled sequential variational autoencoder. _Advances in Neural Information Processing Systems_, 34, 2021.
* Han et al. [2021] Jun Han, Martin Renqiang Min, Ligong Han, Li Erran Li, and Xuan Zhang. Disentangled recurrent wasserstein autoencoder. In _International Conference on Learning Representations_, 2021. URL https://openreview.net/forum?id=07ms4LFdsX.
* Villegas et al. [2017] Ruben Villegas, Jimei Yang, Seunghoon Hong, Xunyu Lin, and Honglak Lee. Decomposing motion and content for natural video sequence prediction. In _International Conference on Learning Representations_, 2017. URL https://openreview.net/forum?id=rkEFLFqee.
* Tulyakov et al. [2018] Sergey Tulyakov, Ming-Yu Liu, Xiaodong Yang, and Jan Kautz. Mocogan: Decomposing motion and content for video generation. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 1526-1535, 2018.
* Denton and Birodkar [2017] Emily L Denton and Vighnesh Birodkar. Unsupervised learning of disentangled representations from video. _Advances in neural information processing systems_, 30, 2017.
* Yamada et al. [2020] Masanori Yamada, Heecheol Kim, Kosuke Miyoshi, Tomoharu Iwata, and Hiroshi Yamakawa. Disentangled representations for sequence data using information bottleneck principle. In _Asian Conference on Machine Learning_, pages 305-320. PMLR, 2020.
* Zhao et al. [2017] Shengjia Zhao, Jiaming Song, and Stefano Ermon. Learning hierarchical features from deep generative models. In Doina Precup and Yee Whye Teh, editors, _Proceedings of the 34th International Conference on Machine Learning_, volume 70 of _Proceedings of Machine Learning Research_, pages 4091-4099. PMLR, 06-11 Aug 2017. URL https://proceedings.mlr.press/v70/zhao17c.html.
* Allman et al. [2008] Elizabeth S. Allman, Catherine Matias, and John A. Rhodes. Identifiability of parameters in latent structure models with many observed variables. _Annals of Statistics_, 37:3099-3132, 2008.
* Spantini et al. [2018] Alessio Spantini, Daniele Bigoni, and Youssef Marzouk. Inference via low-dimensional couplings. _The Journal of Machine Learning Research_, 19(1):2639-2709, 2018.

* **"Temporally Disentangled Representation Learning under Unknown Nonstationarity"**

Appendix organization:

	* 1.1 Identifiability of Nonstationary Hidden States
	* 1.2 Identifiability of Latent Causal Processes
	* 1.3 Discussion on Assumptions in Theorem 2
* **Implementation Details*
	* 1.1 Reproducibility
	* 1.2 Prior Likelihood Derivation
	* 1.3 Derivation of ELBO
	* 1.4 Synthetic Dataset Generation
	* 1.4 Sample \(c_{t}\) from Markov chain
	* 1.4 Generation of latent variables \(\mathbf{z}_{t}\)
	* 1.4 Generation of observations \(\mathbf{x}_{t}\)
	* 1.5 Modified CartPole Dataset Generation
	* 1.6 MoSeq Dataset
	* 1.7 Mean Correlation Coefficient
	* 1.8 Network Architecture
Identifiability

Assume we observe \(n\)-dimensional time-series data at discrete time steps, \(\mathbf{X}=\{\mathbf{x}_{1},\mathbf{x}_{2},\ldots,\mathbf{x}_{T}\}\), where each \(\mathbf{x}_{t}\) is generated from time-delayed causally related hidden components \(\mathbf{z}_{t}\in\mathbb{R}^{n}\) by the invertible mixing function:

\[\mathbf{x}_{t}=\mathbf{g}(\mathbf{z}_{t}).\] (1)

In addition to latent components \(\mathbf{z}_{t}\), there is an extra hidden component \(c_{t}\) which is a discrete variable with cardinality \(\left\lvert\,c_{t}\,\right\rvert=C\), it follows first-order Markov process controlled by a \(C\times C\) transition matrix \(\mathbf{A}\), in which the \(i,j\)-th entry \(A_{i,j}\) is the probability to transit from state \(i\) to \(j\).

\[c_{1},c_{2},\ldots,c_{t}\sim\text{Markov Chain}(\mathbf{A})\] (2)

For \(i\in\{1,\ldots,n\}\), \(z_{it}\), as the \(i\)-th component of \(\mathbf{z}_{t}\), is generated by (some) components of history information \(\mathbf{z}_{t-1}\), discrete nonstationary indicator \(c_{t}\), and noise \(\epsilon_{it}\).

\[z_{it}=f_{i}(\{z_{j,t-1}\,|\,z_{j,t-\tau}\in\mathbf{Pa}(z_{it})\},c_{t}, \epsilon_{it})\quad with\quad\epsilon_{it}\sim p_{\epsilon_{i}|c_{t}}\] (3)

where \(\mathbf{Pa}(z_{it})\) is the set of latent factors that directly cause \(z_{it}\), which can be any subset of \(\mathbf{z}_{\text{Hx}}=\{\mathbf{z}_{t-1},\mathbf{z}_{t-2},\ldots,\mathbf{z}_{ t-L}\}\) up to history information maximum lag \(L\). The components of \(\mathbf{z}_{t}\) are mutually independent conditional on \(\mathbf{z}_{\text{Hx}}\) and \(c_{t}\).

### Identifiability of Nonstationary Hidden States

**Theorem 1**.: _(identifiability of the nonstationarity with Markov Assumptions) Suppose the observed data is generated following the nonlinear ICA framework as defined in Eqs. (1), (2) and (3). And Suppose the following assumptions (Markov Assumptions) hold:_

* _For the Markov process, the number of latent states,_ \(C\)_, is known._
* _The transition matrix_ \(\mathbf{A}\) _is full rank._

_Use \(\mu_{1},\ldots,\mu_{C}\in\mathbb{R}^{n}\) to denote nonparametric probability distributions of the \(C\) emission distributions \(\mu_{c}=p(\mathbf{x}_{t}\,|\,\mathbf{x}_{t-1},c)\). Then the parameters \(\mathbf{A}\) and \(M=(\mu_{1},\ldots,\mu_{C})\) are identifiable given the distribution, \(\mathbb{P}^{(3)}_{\mathbf{A},M}\), of at least 4 consecutive observations \(\mathbf{x}_{t},\mathbf{x}_{t+1},\mathbf{x}_{t+2},\mathbf{x}_{t+3}\), up to label swapping of the hidden states, that is:_

_If \(\widetilde{\mathbf{A}}\) is a \(C\times C\) transition matrix, if \(\widetilde{\pi}(c)\) is a stationary distribution of \(\widetilde{\mathbf{A}}\) with \(\widetilde{\pi}(c)>0\)\(\forall c\in\{1,\ldots,C\}\), and if \(\tilde{M}=(\tilde{\mu}_{1},\ldots,\tilde{\mu}_{C})\) are \(C\) probability distributions on \(\mathbb{R}^{n}\) that verify the equality of the distribution functions \(\mathbb{P}^{(3)}_{\widetilde{\mathbf{A}},\tilde{M}}=\mathbb{P}^{(3)}_{ \mathbf{A},M}\), then there exists a permutation \(\sigma\) of the set \(\{1,\ldots,C\}\) such that for all \(k,l=1,\ldots,C\) we have \(\tilde{A}_{k,l}=A_{\sigma(k),\sigma(l)}\) and \(\tilde{\mu}_{k}=\mu_{\sigma(k)}\)._

Proof.: Suppose we have:

\[\tilde{p}(\mathbf{x}_{1},\ldots,\mathbf{x}_{T})=p(\mathbf{x}_{1},\ldots, \mathbf{x}_{T})\] (4)

where \(p(\mathbf{x}_{1},\ldots,\mathbf{x}_{T})\) has transition matrix \(\mathbf{A}\) and emission distributions \((\mu_{1},\ldots,\mu_{C})\), similarly for \(\tilde{p}(\mathbf{x}_{1},\ldots,\mathbf{x}_{T})\).

We consider four consecutive observations \(\mathbf{x}_{0},\mathbf{x}_{1},\mathbf{x}_{2},\mathbf{x}_{3}\) and corresponding four discrete elements \(c_{0},c_{1},c_{2},c_{3}\).

\[p(\mathbf{x}_{1},\mathbf{x}_{2},\mathbf{x}_{3}\,|\,\mathbf{x}_{0}) =\sum_{c_{1},c_{2},c_{3}}p(c_{1})p(\mathbf{x}_{1}\,|\,\mathbf{x}_{ 0},c_{1})\cdot A_{c_{1},c_{2}}p(\mathbf{x}_{2}\,|\,\mathbf{x}_{1},c_{2})\cdot A _{c_{2},c_{3}}p(\mathbf{x}_{3}\,|\,\mathbf{x}_{2},c_{3})\] \[=\sum_{c_{1},c_{2}}p(c_{1})A_{c_{1},c_{2}}p(\mathbf{x}_{1}\,|\, \mathbf{x}_{0},c_{1})\cdot p(\mathbf{x}_{2}\,|\,\mathbf{x}_{1},c_{2})\cdot \left(\sum_{c_{3}}A_{c_{2},c_{3}}p(\mathbf{x}_{3}\,|\,\mathbf{x}_{2},c_{3})\right)\] \[=\sum_{c_{2}}\pi_{c_{2}}\underbrace{\left(\sum_{c_{1}}\frac{\pi_{ c_{1}}A_{c_{1},c_{2}}}{\pi_{c_{2}}}\mu_{c_{1}}\right)}_{\mu_{c_{2}}}\cdot\mu_{c_{2}} \cdot\underbrace{\left(\sum_{c_{3}}A_{c_{2},c_{3}}\mu_{c_{3}}\right)}_{\mu_{c_{2}}}\] (5)where \(\pi_{c_{i}}=p(c_{i})\). Since \(\mathbf{A}\) has full rank and the probability measures \(\mu_{1},\ldots,\mu_{C}\) are linearly independent, the probability measures \(\{\bar{\mu}_{c_{2}}=\sum_{c_{1}}\frac{x_{c_{1}}A_{c_{1},c_{2}}}{\pi_{c_{2}}}\mu _{c_{1}}\,|\,c_{2}=1,\ldots,C\}\) are linearly independent, and the probability measures \(\{\hat{\mu}_{c_{2}}=\sum_{c_{3}}A_{c_{2},c_{3}}\mu_{c_{3}}\,|\,c_{2}=1,\ldots,C\}\) are also linearly independent. Thus, applying Theorem 9 of [46], there exists a permutation \(\sigma\) of \(\{1,\ldots,C\}\) such that, \(\forall i\in\{1,\ldots,C\}\):

\[\tilde{\mu}_{i} =\mu_{\sigma(i)}\] \[\sum_{j}\tilde{A}_{i,j}\tilde{\mu}_{j} =\sum_{j}A_{\sigma(i),j}\mu_{j}\]

This gives easily \(\forall i\in\{1,\ldots,C\}\):

\[\sum_{j}\tilde{A}_{i,j}\mu_{\sigma(j)}=\sum_{j}A_{\sigma(i),\sigma(j)}\mu_{ \sigma(j)}.\]

Since the conditional distributions \(\mu_{i}\) are linearly independent, we can establish the equivalence between \(\tilde{\mathbf{A}}\) and \(\mathbf{A}\) via permutation \(\sigma\),

\[\tilde{A}_{j,i}=A_{\sigma(j),\sigma(i)},\] (6)

then the theorem is proved. 

For notational simplicity, and without loss of generality, we assume the components are ordered such that \(c=\sigma(c)\). That leads us to the identifiability of the nonstationarity in the system i.e. up to label swapping of the hidden states, the conditional emission distributions \(p(\mathbf{x}_{t}|\mathbf{x}_{t-1},c_{t})\) and transition matrix \(\mathbf{A}\) are identifiable, hence providing us a bridge to further leverage the temporal independence condition in the latent space to establish the identifiability result for demixing function or in other words the latent variables \(\mathbf{z}_{t}\).

### Identifiability of Latent Causal Processes

To incorporate nonlinear ICA into the Markov Assumption we define the emission distribution \(p(\mathbf{x}_{t}\,|\,\mathbf{x}_{t-1},c)\) as a deep latent variable model. First, the latent independent component variables \(\mathbf{z}_{t}\in\mathbb{R}^{n}\) are generated from a factorial prior, given the hidden state \(c_{t}\) and previous \(\mathbf{z}_{t-1}\), as

\[p(\mathbf{z}_{t}\,|\,\mathbf{z}_{t-1},c_{t})=\prod_{k=1}^{n}p(z_{kt}\,|\, \mathbf{z}_{t-1},c_{t}).\] (7)

Second, the observed data \(\mathbf{x}_{t}\in\mathbb{R}^{n}\) is generated by a nonlinear mixing function as in Eq. (1) which is assumed to be bijective with inverse given by \(\mathbf{z}_{t}=\mathbf{g}^{-1}(\mathbf{x}_{t})\). Let \(\eta_{kt}(c_{t})\triangleq\log p(z_{kt}|\mathbf{z}_{t-1},c_{t})\), and assume that \(\eta_{kt}(c_{t})\) is twice differentiable in \(z_{kt}\) and is differentiable in \(z_{l,t-1}\), \(l=1,2,...,n\). Note that the parents of \(z_{kt}\) may be only \(c_{t}\) and a subset of \(\mathbf{z}_{t-1}\); if \(z_{l,t-1}\) is not a parent of \(z_{kt}\), then \(\frac{\partial\eta_{k}}{\partial z_{1,t-1}}=0\).

**Theorem 2**.: _Suppose there exists an invertible function \(\hat{\mathbf{g}}^{-1}\), which is the estimated demixing function that maps \(\mathbf{x}_{t}\) to \(\hat{\mathbf{z}}_{t}\), i.e.,_

\[\hat{\mathbf{z}}_{t}=\hat{\mathbf{g}}^{-1}(\mathbf{x}_{t})\] (8)

_such that the components of \(\hat{\mathbf{z}}_{t}\) are mutually independent conditional on \(\hat{\mathbf{z}}_{t-1}\). Let_

\[\mathbf{v}_{k,t}(c) \triangleq\Big{(}\frac{\partial^{2}\eta_{kt}(c)}{\partial z_{k,t }\partial z_{1,t-1}},\frac{\partial^{2}\eta_{kt}(c)}{\partial z_{k,t}\partial z _{2,t-1}},...,\frac{\partial^{2}\eta_{kt}(c)}{\partial z_{k,t}\partial z_{n,t- 1}}\Big{)}^{\intercal},\] (9) \[\hat{\mathbf{v}}_{k,t}(c) \triangleq\Big{(}\frac{\partial^{3}\eta_{kt}(c)}{\partial z_{k,t }^{2}\partial z_{1,t-1}},\frac{\partial^{3}\eta_{kt}(c)}{\partial z_{k,t}^{2} \partial z_{2,t-1}},...,\frac{\partial^{3}\eta_{kt}(c)}{\partial z_{k,t}^{2} \partial z_{n,t-1}}\Big{)}^{\intercal}.\]

_And_

\[\mathbf{s}_{kt} \triangleq\Big{(}\mathbf{v}_{kt}(1)^{\intercal},...,\mathbf{v}_{ kt}(C)^{\intercal},\frac{\partial^{2}\eta_{kt}(2)}{\partial z_{kt}^{2}}- \frac{\partial^{2}\eta_{kt}(1)}{\partial z_{kt}^{2}},...,\frac{\partial^{2} \eta_{kt}(C)}{\partial z_{kt}^{2}}-\frac{\partial^{2}\eta_{kt}(C-1)}{\partial z _{kt}^{2}}\Big{)}^{\intercal},\] (10) \[\hat{\mathbf{s}}_{kt} \triangleq\Big{(}\hat{\mathbf{v}}_{kt}(1)^{\intercal},...,\hat{ \mathbf{v}}_{kt}(C)^{\intercal},\frac{\partial\eta_{kt}(2)}{\partial z_{kt}}- \frac{\partial\eta_{kt}(1)}{\partial z_{kt}},...,\frac{\partial\eta_{kt}(C)}{ \partial z_{kt}}-\frac{\partial\eta_{kt}(C-1)}{\partial z_{kt}}\Big{)}^{ \intercal}.\]

_If for each value of \(\mathbf{z}_{t}\), \(\mathbf{s}_{1t},\hat{\mathbf{s}}_{1t},\mathbf{v}_{2t},\hat{\mathbf{s}}_{2t},..., \mathbf{s}_{nt},\hat{\mathbf{s}}_{nt}\), as \(2n\) function vectors \(\mathbf{s}_{k,t}\) and \(\hat{\mathbf{s}}_{k,t}\), with \(k=1,2,...,n\), are linearly independent, then \(\hat{\mathbf{z}}_{t}\) must be an invertible, component-wise transformation of a permuted version of \(\mathbf{z}_{t}\)._Proof.: Combining (1) and (6) gives \(\mathbf{z}_{t}=(\mathbf{g}^{-1}\circ\hat{\mathbf{g}})(\hat{\mathbf{z}}_{t})= \mathbf{h}(\hat{\mathbf{z}}_{t})\), where \(\mathbf{h}\triangleq\mathbf{g}^{-1}\circ\hat{\mathbf{g}}\). Since both \(\mathbf{g}\) and \(\hat{\mathbf{g}}\) are invertible, \(\mathbf{h}\) is invertible. Let \(\mathbf{H}_{t}\) be the Jacobian matrix of the transformation \(\mathbf{h}(\hat{\mathbf{z}}_{t})\), and denote by \(\mathbf{H}_{kit}\) its \((k,i)\)th entry.

First, it is straightforward to see that if the components of \(\hat{\mathbf{z}}_{t}\) are mutually independent conditional on previous \(\hat{\mathbf{z}}_{t-1}\) and current \(c_{t}\), then for any \(i\neq j\), \(\hat{z}_{it}\) and \(\hat{\mathbf{z}}_{jt}\) are conditionally independent given \(\hat{\mathbf{z}}_{t-1}\cup(\hat{\mathbf{z}}_{t}\setminus\{\hat{z}_{it},\hat{ z}_{jt}\})\cup\{c_{t}\}\). Mutual independence of the components of \(\hat{\mathbf{z}}_{t}\) conditional on \(\hat{\mathbf{z}}_{t-1}\) implies that \(\hat{z}_{it}\) is independent from \(\hat{\mathbf{z}}_{t}\setminus\{\hat{z}_{it},\hat{z}_{jt}\}\) conditional on \(\hat{\mathbf{z}}_{t-1}\) and \(c_{t}\), i.e.,

\[p(\hat{z}_{it}\,|\,\hat{\mathbf{z}}_{t-1},c_{t})=p(\hat{z}_{it}\,|\,\hat{ \mathbf{z}}_{t-1}\cup(\hat{\mathbf{z}}_{t}\setminus\{\hat{z}_{it},\hat{z}_{jt }\}),c_{t}).\]

At the same time, it also implies \(\hat{z}_{it}\) is independent from \(\hat{\mathbf{z}}_{t}\setminus\{\hat{z}_{it}\}\) conditional on \(\hat{\mathbf{z}}_{t-1}\) and \(c_{t}\), i.e.,

\[p(\hat{z}_{it}\,|\,\hat{\mathbf{z}}_{t-1},c_{t})=p(\hat{z}_{it}\,|\,\hat{ \mathbf{z}}_{t-1}\cup(\hat{\mathbf{z}}_{t}\setminus\{\hat{z}_{it}\}),c_{t}).\]

Combining the above two equations gives

\[p(\hat{z}_{it}\,|\,\hat{\mathbf{z}}_{t-1}\cup(\hat{\mathbf{z}}_{t}\setminus \{\hat{z}_{it}\}),c_{t})=p(\hat{z}_{it}\,|\,\hat{\mathbf{z}}_{t-1}\cup(\hat{ \mathbf{z}}_{t}\setminus\{\hat{z}_{it},\hat{z}_{jt}\}),c_{t}),\]

i.e., for \(i\neq j\), \(\hat{z}_{it}\) and \(\hat{z}_{jt}\) are conditionally independent given \(\hat{\mathbf{z}}_{t-1}\cup(\hat{\mathbf{z}}_{t}\setminus\{\hat{z}_{it},\hat{ z}_{jt}\})\cup\{c_{t}\}\).

We then make use of the fact that if \(\hat{z}_{it}\) and \(\hat{z}_{jt}\) are conditionally independent given \(\hat{\mathbf{z}}_{t-1}\cup(\hat{\mathbf{z}}_{t}\setminus\{\hat{z}_{it},\hat{ z}_{jt}\})\cup\{c_{t}\}\), then

\[\frac{\partial^{2}\log p(\hat{\mathbf{z}}_{t},\hat{\mathbf{z}}_{t-1},c_{t})}{ \partial\hat{z}_{it}\partial\hat{z}_{jt}}=0,\]

assuming the cross second-order derivative exists [47]. Since \(p(\hat{\mathbf{z}}_{t},\hat{\mathbf{z}}_{t-1},c_{t})=p(\hat{\mathbf{z}}_{t} \,|\,\hat{\mathbf{z}}_{t-1},c_{t})p(\hat{\mathbf{z}}_{t-1},c_{t})\) while \(p(\hat{\mathbf{z}}_{t-1},c_{t})\) does not involve \(\hat{z}_{it}\) or \(\hat{z}_{jt}\), the above equality is equivalent to

\[\frac{\partial^{2}\log p(\hat{\mathbf{z}}_{t}\,|\,\hat{\mathbf{z}}_{t-1},c_{t })}{\partial\hat{z}_{it}\partial\hat{z}_{jt}}=0.\] (11)

Then for any \(c_{t}\), the Jacobian matrix of the mapping from \((\mathbf{x}_{t-1},\hat{\mathbf{z}}_{t})\) to \((\mathbf{x}_{t-1},\mathbf{z}_{t})\) is \(\begin{bmatrix}\mathbf{I}&\mathbf{0}\\ *&\mathbf{H}_{t}\end{bmatrix}\), where \(*\) stands for a matrix, and the (absolute value of the) determinant of this Jacobian matrix is \(|\mathbf{H}_{t}|\). Therefore \(p(\hat{\mathbf{z}}_{t},\mathbf{x}_{t-1}|c_{t})=p(\mathbf{z}_{t},\mathbf{x}_{t- 1}|c_{t})\cdot|\mathbf{H}_{t}|\). Dividing both sides of this equation by \(p(\mathbf{x}_{t-1}|c_{t})\) gives

\[p(\hat{\mathbf{z}}_{t}\,|\,\mathbf{x}_{t-1},c_{t})=p(\mathbf{z}_{t}\,|\, \mathbf{x}_{t-1},c_{t})\cdot|\mathbf{H}_{t}|.\] (12)

Since \(p(\mathbf{z}_{t}\,|\,\mathbf{z}_{t-1},c_{t})=p(\mathbf{z}_{t}\,|\,\mathbf{g}( \mathbf{z}_{t-1}),c_{t})=p(\mathbf{z}_{t}\,|\,\mathbf{x}_{t-1},c_{t})\) and similarly \(p(\hat{\mathbf{z}}_{t}\,|\,\hat{\mathbf{z}}_{t-1},c_{t})=p(\hat{\mathbf{z}}_{t} \,|\,\mathbf{x}_{t-1},c_{t})\), Eq. 12 tells us

\[\log p(\hat{\mathbf{z}}_{t}\,|\,\hat{\mathbf{z}}_{t-1},c_{t})=\log p(\mathbf{z }_{t}\,|\,\mathbf{z}_{t-1},c_{t})+\log|\mathbf{H}_{t}|=\sum_{k=1}^{n}\eta_{kt}(c _{t})+\log|\mathbf{H}_{t}|.\] (13)

Its partial derivative w.r.t. \(\hat{z}_{it}\) is

\[\frac{\partial\log p(\hat{\mathbf{z}}_{t}\,|\,\hat{\mathbf{z}}_{t- 1},c_{t})}{\partial\hat{z}_{it}} =\sum_{k=1}^{n}\frac{\partial\eta_{kt}(c_{t})}{\partial z_{kt}} \cdot\frac{\partial z_{kt}}{\partial\hat{z}_{it}}-\frac{\partial\log|\mathbf{H}_{ t}|}{\partial\hat{z}_{it}}\] \[=\sum_{k=1}^{n}\frac{\partial\eta_{kt}(c_{t})}{\partial z_{kt}} \cdot\mathbf{H}_{kit}-\frac{\partial\log|\mathbf{H}_{t}|}{\partial\hat{z}_{it}}.\]

Its second-order cross-derivative is

\[\frac{\partial^{2}\log p(\hat{\mathbf{z}}_{t}\,|\,\hat{\mathbf{z}}_{t-1},c_{t})}{ \partial\hat{z}_{it}\partial\hat{z}_{jt}}=\sum_{k=1}^{n}\Big{(}\frac{\partial^{2} \eta_{kt}(c_{t})}{\partial z_{kt}^{2}}\cdot\mathbf{H}_{kit}\mathbf{H}_{kit}+ \frac{\partial\eta_{kt}(c_{t})}{\partial z_{kt}}\cdot\frac{\partial\mathbf{H}_{kit} }{\partial\hat{z}_{jt}}\Big{)}-\frac{\partial^{2}\log|\mathbf{H}_{t}|}{ \partial\hat{z}_{it}\partial\hat{z}_{jt}}.\] (14)

The above quantity is always 0 according to Eq. (11). Therefore, for each \(l=1,2,...,n\) and each value \(z_{l,t-1}\), its partial derivative w.r.t. \(z_{l,t-1}\) is always 0. That is,

\[\frac{\partial^{3}\log p(\hat{\mathbf{z}}_{t}\,|\,\hat{\mathbf{z}}_{t-1},c_{t})}{ \partial\hat{z}_{it}\partial\hat{z}_{jt}\partial z_{l,t-1}}=\sum_{k=1}^{n} \Big{(}\frac{\partial^{3}\eta_{kt}(c_{t})}{\partial z_{kt}^{2}\partial z_{l,t- 1}}\cdot\mathbf{H}_{kit}\mathbf{H}_{kit}+\frac{\partial^{2}\eta_{kt}(c_{t})}{ \partial z_{kt}\partial z_{l,t-1}}\cdot\frac{\partial\mathbf{H}_{kit}}{ \partial\hat{z}_{jt}}\Big{)}\equiv 0,\] (15)where we have made use of the fact that entries of \(\mathbf{H}_{t}\) do not depend on \(z_{l,t-1}\). Using different values \(r\) for \(c_{t}\) in Eq. (14) take the difference of this equation across them gives

\[\frac{\partial^{2}\log p(\hat{\mathbf{z}}_{t}\,|\,\hat{\mathbf{z}} _{t-1};r+1)}{\partial\hat{z}_{it}\partial\hat{z}_{jt}}-\frac{\partial^{2}\log p (\hat{\mathbf{z}}_{t}\,|\,\hat{\mathbf{z}}_{t-1};r)}{\partial\hat{z}_{it} \partial\hat{z}_{jt}}\] \[= \sum_{k=1}^{n}\left[\left(\frac{\partial^{2}\eta_{kt}(r+1)}{ \partial z_{kt}^{2}}-\frac{\partial^{2}\eta_{kt}(r)}{\partial z_{kt}^{2}} \right)\cdot\mathbf{H}_{kit}\mathbf{H}_{kjt}+\left(\frac{\partial\eta_{kt}(r+1 )}{\partial z_{kt}}-\frac{\partial\eta_{kt}(r)}{\partial z_{kt}}\right)\cdot \frac{\partial\mathbf{H}_{kit}}{\partial\hat{z}_{jt}}\right]\equiv 0.\] (16)

If for any value of \(\mathbf{z}_{t}\), \(\mathbf{s}_{1t},\hat{\mathbf{s}}_{1t},\mathbf{s}_{2t},\hat{\mathbf{s}}_{2t},...,\mathbf{s}_{nt},\hat{\mathbf{s}}_{nt}\) are linearly independent, to make the above equation hold true, one has to set \(\mathbf{H}_{kit}\mathbf{H}_{kjt}=0\) or \(i\neq j\). That is, in each row of \(\mathbf{H}_{t}\) there is only one non-zero entry. Since \(h\) is invertible, then \(\mathbf{z}_{t}\) must be an invertible, component-wise transformation of a permuted version of \(\hat{\mathbf{z}}_{t}\). 

So far, the identifiability result has been established without observing the nonstationarity indicators such as domain indices.

### Discussion on Assumptions in Theorem 2

This condition was initially introduced in GCL [11], namely, "sufficient variability", to extend the modulated exponential families [9] to general modulated distributions. Essentially, the condition says that the nonstationary domains \(c\) must have a sufficiently complex and diverse effect on the transition distributions. In other words, if the underlying distributions are composed of relatively many domains of data, the condition generally holds true. Loosely speaking, the sufficient variability holds if the modulation of by \(c\) on the conditional distribution \(q(z_{it}|\mathbf{z}_{\mathrm{Hx}},c)\) is not too simple in the following sense:

1. Higher order of \(k\) (\(k>1\)) is required. If \(k=1\), the sufficient variability cannot hold;
2. The modulation impacts \(\lambda_{ij}\) by \(\mathbf{u}\) must be linearly independent across domains \(c\). The sufficient statistics functions \(q_{ij}\) cannot be all linear, i.e., we require higher-order statistics.

Further details of this example can be found in Appendix B of [11] and Appendix S1.4.1 of [18]. In summary, we need the domains denoted by \(c\) to have diverse (i.e., distinct influences) and complex impacts on the underlying data generation process.

## Appendix B Implementation Details

### Reproducibility

All experiments are done in a GPU workstation with CPU: Intel i7-13700K, GPU: NVIDIA RTX 4090, Memory: 128 GB. The code can be found via https://github.com/xiangchensong/nctrl.

### Prior Likelihood Derivation

Let us start with an illustrative example of stationary latent causal processes consisting of two time-delayed latent variables, i.e., \(\mathbf{z}_{t}=[z_{1,t},z_{2,t}]\) with maximum time lag \(L=1\), i.e., \(z_{i,t}=f_{i}(\mathbf{z}_{t-1},\epsilon_{i,t})\) with mutually independent noises. Let us write this latent process as a transformation map \(\mathbf{f}\) (note that we overload the notation \(f\) for transition functions and for the transformation map):

\[\begin{bmatrix}z_{1,t-1}\\ z_{2,t-1}\\ z_{1,t}\\ z_{2,t}\end{bmatrix}=\mathbf{f}\left(\begin{bmatrix}z_{1,t-1}\\ z_{2,t-1}\\ \epsilon_{1,t}\\ \epsilon_{2,t}\end{bmatrix}\right).\] (17)

By applying the change of variables formula to the map \(\mathbf{f}\), we can evaluate the joint distribution of the latent variables \(p(z_{1,t-1},z_{2,t-1},z_{1,t},z_{2,t})\) as:

\[p(z_{1,t-1},z_{2,t-1},z_{1,t},z_{2,t})=p(z_{1,t-1},z_{2,t-1},\epsilon_{1,t}, \epsilon_{2,t})/\left|\det\mathbf{J}_{\mathbf{f}}\right|,\] (18)where \(\mathbf{J_{f}}\) is the Jacobian matrix of the map \(\mathbf{f}\), which is naturally a low-triangular matrix:

\[\mathbf{J_{f}}=\begin{bmatrix}1&0&0&0\\ 0&1&0&0\\ \frac{\partial z_{1,t}}{\partial z_{1,t-1}}&\frac{\partial z_{1,t}}{\partial z _{2,t-1}}&\frac{\partial z_{1,t}}{\partial\epsilon_{1,t}}&0\\ \frac{\partial z_{2,t}}{\partial z_{1,t-1}}&\frac{\partial z_{2,t}}{\partial z _{2,t-1}}&0&\frac{\partial z_{2,t}}{\partial\epsilon_{2,t}}\end{bmatrix}.\]

Given that this Jacobian is triangular, we can efficiently compute its determinant as \(\prod_{i}\frac{\partial z_{i,t}}{\partial\epsilon_{i,t}}\). Furthermore, because the noise terms are mutually independent, and hence \(\epsilon_{i,t}\perp\epsilon_{j,t}\) for \(j\neq i\) and \(\epsilon_{t}\perp\mathbf{z}_{t-1}\), we can write the RHS of Eq. 18 as:

\[p(z_{1,t-1},z_{2,t-1},z_{1,t},z_{2,t}) =p(z_{1,t-1},z_{2,t-1})\times p(\epsilon_{1,t},\epsilon_{2,t})/ \left|\det\mathbf{J_{f}}\right|\quad\text{(because $\epsilon_{t}\perp\mathbf{z}_{t-1}$)}\] \[=p(z_{1,t-1},z_{2,t-1})\times\prod_{i}p(\epsilon_{i,t})/\left| \det\mathbf{J_{f}}\right|\quad\text{(because $\epsilon_{1,t}\perp\epsilon_{2,t}$)}\] (19)

Finally, by canceling out the marginals of the lagged latent variables \(p(z_{1,t-1},z_{2,t-1})\) on both sides, we can evaluate the transition prior likelihood as:

\[p(z_{1,t},z_{2,t}|z_{1,t-1},z_{2,t-1})=\prod_{i}p(\epsilon_{i,t})/\left|\det \mathbf{J_{f}}\right|=\prod_{i}p(\epsilon_{i,t})\times\left|\det\mathbf{J_{f} }^{-1}\right|.\] (20)

Now we generalize this example and derive the prior likelihood below.

Let \(\{f_{i}^{-1}\}_{i=1,2,3\ldots}\) be a set of learned inverse transition functions that take the estimated latent causal variables, and output the noise terms, i.e., \(\hat{\epsilon}_{i,t}=f_{i}^{-1}\left(\hat{z}_{i,t},\{\hat{\mathbf{z}}_{t-\tau },c_{t}\}\right)\).

Design transformation \(\mathbf{A}\rightarrow\mathbf{B}\) with low-triangular Jacobian as follows:

\[\underbrace{\begin{bmatrix}\hat{\mathbf{z}}_{t-L},\ldots,\hat{ \mathbf{z}}_{t-1},\hat{\mathbf{z}}_{t}\end{bmatrix}^{\top}}_{\mathbf{A}} \text{ mapped to }\underbrace{\begin{bmatrix}\hat{\mathbf{z}}_{t-L},\ldots,\hat{ \mathbf{z}}_{t-1},\hat{\epsilon}_{i,t}\end{bmatrix}^{\top}}_{\mathbf{B}},\ with\ \mathbf{J_{A\rightarrow B}}=\begin{pmatrix} \mathbb{I}_{nL}&0\\ *&\text{diag}\left(\frac{\partial f_{i-1}^{-1}}{\partial\hat{z}_{jt}}\right) \end{pmatrix}.\] (21)

Similar to Eq. 20, we can obtain the joint distribution of the estimated dynamics subspace as:

\[\log p(\mathbf{A})=\underbrace{\log p\left(\hat{\mathbf{z}}_{t-L},\ldots, \hat{\mathbf{z}}_{t-1}\right)+\sum_{j=1}^{n}\log p(\hat{\epsilon}_{i,t})}_{ \text{Because of mutually independent noise assumption}}+\log\left(\left|\det\left(\mathbf{J_{A \rightarrow B}}\right)\right|\right).\] (22)

### Derivation of ELBO

Then the second part is to maximize the Evidence Lower BOund (ELBO) for the VAE framework, which can be written as:ELBO\(\triangleq\log p_{\text{data}}(\mathbf{X})-D_{KL}(q_{\phi}( \mathbf{Z}|\mathbf{X})||p_{\text{data}}(\mathbf{Z}|\mathbf{X}))\)

\[= \mathbb{E}_{\mathbf{Z}\sim q_{\phi}(\mathbf{Z}|\mathbf{X})}\log p_{ \text{data}}(\mathbf{X}|\mathbf{Z})-D_{KL}(q_{\phi}(\mathbf{Z}|\mathbf{X})||p_ {\text{data}}(\mathbf{Z}|\mathbf{X}))\] \[= \mathbb{E}_{\mathbf{Z}\sim q_{\phi}(\mathbf{Z}|\mathbf{X})}\log p _{\text{data}}(\mathbf{X}|\mathbf{Z})-\mathbb{E}_{\mathbf{Z}\sim q_{\phi}( \mathbf{Z}|\mathbf{X})}\left[\log q_{\phi}(\mathbf{Z}|\mathbf{X})-\log p_{ \text{data}}(\mathbf{Z})\right]\] \[= \mathbb{E}_{\mathbf{Z}\sim q_{\phi}(\mathbf{Z}|\mathbf{X})}\left[ \log p_{\text{data}}(\mathbf{X}|\mathbf{Z})+\underbrace{\log p_{\text{data}}( \mathbf{Z})}_{\mathbb{E}_{\text{e}}\left[\sum_{t=1}^{T}\log p(\mathbf{z}_{t}| \mathbf{z}_{t-1},c_{t})\right]}-\log q_{\phi}(\mathbf{Z}|\mathbf{X})\right]\] (24) \[= \mathbb{E}_{\mathbf{z}_{t}}\left[\underbrace{\sum_{t=1}^{T}\log p _{\text{data}}(\mathbf{x}_{t}|\mathbf{z}_{t})}_{-\mathbb{E}_{\text{Reco}}}+ \underbrace{\mathbb{E}_{\mathbf{c}}\left[\sum_{t=1}^{T}\log p_{\text{data}}( \mathbf{z}_{t}|\mathbf{z}_{t\text{Ix}},c_{t})\right]-\sum_{t=1}^{T}\log q_{ \phi}(\mathbf{z}_{t}|\mathbf{x}_{t})}_{-\mathbb{E}_{\text{RLD}}}\right]\]

### Synthetic Dataset Generation

We generated two synthetic datasets (A and B) with different nonlinear mixing functions. In this section we will introduce the detailed implementation of the generation. The generation can be split into steps (1) sample \(c_{t}\) from a Markov chain, (2) generate \(\mathbf{z}_{t}\) with different transition functions \(f_{c_{t}}\) with respect to \(c_{t}\), and (3) generate observation \(\mathbf{x}_{t}\) via mixing function \(\mathbf{g}\).

#### b.4.1 Sample \(c_{t}\) from Markov chain

We first randomly initialized a Markov chain with transition matrix \(\mathbf{A}\) and sample 20,000 steps.

#### b.4.2 Generation of latent variables \(\mathbf{z}_{t}\)

We first randomly initialized \(|C|=5\) different transition functions \(\{f_{1},f_{2},\dots,f_{|C|}\}\) with different MLPs, and generate \(\mathbf{z}_{t}=f_{c_{t}}(\mathbf{z}_{\text{Hx}})\). The dimensions are set to 8 for fair comparison.

#### b.4.3 Generation of observations \(\mathbf{x}_{t}\)

The difference between datasets A and B is the mixing function. We use a two-layer randomly initialized MLP for dataset A and a three-layer MLP for dataset B. For each linear layer in the MLP, we use condition number of the weight matrix to filter out ones that are not "invertible".

### Modified CartPole Dataset Generation

Similar to the synthetic datasets, we also sample from a Markov chain and get \(c_{t}\). For the modified CartPole, we initialized 5 different environments which have different combinations of hyperparameters such as gravity, pole mass, etc. A detailed comparison is listed in Table 1.

At each time step \(t\) the environment will load the corresponding hyperparameters for given \(c_{t}\) and update the states \(\mathbf{z}_{t}\) according to the configuration given \(c_{t}\). The nonlinear mixing function from states to observations \(\mathbf{x}_{t}\) is fixed by a rendering method in the gym package.

\begin{table}
\begin{tabular}{c c c c} \hline \hline Environment ID & Gravity & Pole Mass & Noise Scale \\ \hline
0 & 9.8 & 0.2 & 0.01 \\
1 & 24.79 & 0.5 & 0.01 \\
2 & 3.7 & 1.0 & 0.01 \\
3 & 11.15 & 1.5 & 0.01 \\
4 & 0.62 & 2.0 & 0.01 \\ \hline \hline \end{tabular}
\end{table}
Table 1: Different configs for different Modified CartPole environments.

### MoSeq Dataset

In the MoSeq dataset, the observations \(\mathbf{x}_{t}\) are taken to be the first 10 principal components of depth camera video data of mice exploring an open field. The dataset consists of 20-minute depth camera recordings of 24 mice. In preprocessing, the videos are cropped and centered around the mouse centroid and then filtered to remove recording artifacts. Finally, the preprocessed video is projected onto the top principal components to obtain a 10-dimensional time series.

### Mean Correlation Coefficient

MCC is a standard metric for evaluating the recovery of latent factors in ICA literature. MCC first calculates the absolute values of the correlation coefficient between every ground-truth factor against every estimated latent variable. Pearson correlation coefficients or Spearman's rank correlation coefficients can be used depending on whether componentwise invertible nonlinearities exist in the recovered factors. The possible permutation is adjusted by solving a linear sum assignment problem in polynomial time on the computed correlation matrix.

### Network Architecture

We summarize our network architecture below and describe it in detail in Table 2 and Table 3.

\begin{table}
\begin{tabular}{|l l l|} \hline
**Configuration** & **Description** & **Output** \\ \hline \hline
**ARHMM** & Autoregressive HMM for Synthetic Data & \\ \hline Input: \(\mathbf{x}_{1:T}\) & Observed time series & BS \(\times\) T \(\times\) i\_dim \\ Emission Module & Compute \(\mu_{\mathbf{z}_{t+1}},\sigma_{\mathbf{z}_{t+1}}\) & BS \(\times\) T \(\times\) 2 \(\times\) z\_dim \\ \hline \hline
**MLP-Encoder** & Encoder for Synthetic Data & \\ \hline Input: \(\mathbf{x}_{1:T}\) & Observed time series & BS \(\times\) T \(\times\) i\_dim \\ Dense & 128 neurons, LeakyReLU & BS \(\times\) T \(\times\) 128 \\ Dense & 128 neurons, LeakyReLU & BS \(\times\) T \(\times\) 128 \\ Dense & 128 neurons, LeakyReLU & BS \(\times\) T \(\times\) 128 \\ Dense & Temporal embeddings & BS \(\times\) T \(\times\) z\_dim \\ \hline \hline
**MLP-Decoder** & Decoder for Synthetic Data & \\ \hline Input: \(\hat{\mathbf{z}}_{1:T}\) & Sampled latent variables & BS \(\times\) T \(\times\) z\_dim \\ Dense & 128 neurons, LeakyReLU & BS \(\times\) T \(\times\) 128 \\ Dense & i\_dim neurons, reconstructed \(\hat{\mathbf{x}}_{1:T}\) & BS \(\times\) T \(\times\) i\_dim \\ \hline \hline
**Factorized Inference Network** & Bidirectional Inference Network & \\ \hline Input & Sequential embeddings & BS \(\times\) T \(\times\) z\_dim \\ Bottleneck & Compute mean and variance of posterior & \(\mu_{1:T},\sigma_{1:T}\) \\ Reparameterization & Sequential sampling & \(\hat{\mathbf{z}}_{1:T}\) \\ \hline \hline
**Prior Network** & Nonlinear Transition Prior Network & \\ \hline Input & Sampled latent variable sequence \(\hat{\mathbf{z}}_{1:T}\) & BS \(\times\) T \(\times\) z\_dim \\ InverseTransition & Compute estimated residuals \(\hat{\epsilon}_{it}\) & BS \(\times\) T \(\times\) z\_dim \\ JacobianCompute & Compute \(\log\left(\left|\det\left(\mathbf{J}\right)\right|\right)\) & BS \\ \hline \end{tabular}
\end{table}
Table 2: Architecture details. BS: batch size, T: length of time series, i\_dim: input dimension, z\_dim: latent dimension, LeakyReLU: Leaky Rectified Linear Unit.

\begin{table}
\begin{tabular}{|l l l|} \hline
**Configuration** & **Description** & **Output** \\ \hline \hline
**CNN-Encoder** & Feature Extractor & \\ \hline Input: \(\mathbf{x}_{1:T}\) & RGB video frames & BS \(\times\) T \(\times\) 3 \(\times\) 64 \(\times\) 64 \\ Conv2D & F: 32, BatchNorm2D, LeakyReLU & BS \(\times\) T \(\times\) 32 \(\times\) 64 \(\times\) 64 \\ Conv2D & F: 32, BatchNorm2D, LeakyReLU & BS \(\times\) T \(\times\) 32 \(\times\) 32 \(\times\) 32 \\ Conv2D & F: 32, BatchNorm2D, LeakyReLU & BS \(\times\) T \(\times\) 32 \(\times\) 16 \(\times\) 16 \\ Conv2D & F: 64, BatchNorm2D, LeakyReLU & BS \(\times\) T \(\times\) 64 \(\times\) 8 \(\times\) 8 \\ Conv2D & F: 64, BatchNorm2D, LeakyReLU & BS \(\times\) T \(\times\) 64 \(\times\) 4 \(\times\) 4 \\ Conv2D & F: 128, BatchNorm2D, LeakyReLU & BS \(\times\) T \(\times\) 128 \(\times\) 1 \(\times\) 1 \\ Dense & F: 2 * z\_dim = dimension of hidden embedding & BS \(\times\) T \(\times\) 2 * z\_dim \\ \hline \hline
**CNN-Decoder** & Video Reconstruction & \\ \hline Input: \(\mathbf{z}_{1:T}\) & Sampled latent variable sequence & BS \(\times\) T \(\times\) z\_dim \\ Dense & F: 128, LeakyReLU & BS \(\times\) T \(\times\) 128 \(\times\) 1 \(\times\) 1 \\ ConvTranspose2D & F: 64, BatchNorm2D, LeakyReLU & BS \(\times\) T \(\times\) 64 \(\times\) 4 \(\times\) 4 \\ ConvTranspose2D & F: 64, BatchNorm2D, LeakyReLU & BS \(\times\) T \(\times\) 64 \(\times\) 8 \(\times\) 8 \\ ConvTranspose2D & F: 32, BatchNorm2D, LeakyReLU & BS \(\times\) T \(\times\) 32 \(\times\) 16 \(\times\) 16 \\ ConvTranspose2D & F: 32, BatchNorm2D, LeakyReLU & BS \(\times\) T \(\times\) 32 \(\times\) 32 \(\times\) 32 \\ ConvTranspose2D & F: 32, BatchNorm2D, LeakyReLU & BS \(\times\) T \(\times\) 32 \(\times\) 64 \(\times\) 64 \\ ConvTranspose2D & F: 3, estimated scene \(\mathbf{\hat{x}}_{1:T}\) & BS \(\times\) T \(\times\) 3 \(\times\) 64 \(\times\) 64 \\ \hline \end{tabular}
\end{table}
Table 3: Architecture details on CNN encoder and decoder. BS: batch size, T: length of time series, h_dim: hidden dimension, z_dim: latent dimension, F: number of filters, (Leaky)ReLU: (Leaky) Rectified Linear Unit.