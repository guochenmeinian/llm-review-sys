# Learning to Augment Distributions for

Out-of-Distribution Detection

Qizhou Wang\({}^{1}\)1 Zhen Fang\({}^{2}\)1 Yonggang Zhang\({}^{1}\) Feng Liu\({}^{3}\) Yixuan Li\({}^{4}\) Bo Han\({}^{1}\)2

\({}^{1}\)Department of Computer Science, Hong Kong Baptist University

\({}^{2}\)Australian Artificial Intelligence Institute, University of Technology Sydney

\({}^{3}\)School of Computing and Information Systems, The University of Melbourne

\({}^{4}\)Department of Computer Sciences, University of Wisconsin-Madison

{csqzwang, csygzhang, bhanml}@comp.hkbu.edu.hk

zhen.fang@uts.edu.au fengliu.ml@gmail.com sharonli@cs.wisc.edu

Equal contributions.Correspondence to Bo Han (bhanml@comp.hkbu.edu.hk).

Footnote 1: footnotemark:

###### Abstract

Open-world classification systems should discern out-of-distribution (OOD) data whose labels deviate from those of in-distribution (ID) cases, motivating recent studies in OOD detection. Advanced works, despite their promising progress, may still fail in the open world, owing to the lack of knowledge about unseen OOD data in advance. Although one can access auxiliary OOD data (distinct from unseen ones) for model training, it remains to analyze how such auxiliary data will work in the open world. To this end, we delve into such a problem from a learning theory perspective, finding that the distribution discrepancy between the auxiliary and the unseen real OOD data is the key to affecting the open-world detection performance. Accordingly, we propose _Distributional-Augmented OOD Learning_ (DAL), alleviating the OOD distribution discrepancy by crafting an _OOD distribution set_ that contains all distributions in a Wasserstein ball centered on the auxiliary OOD distribution. We justify that the predictor trained over the worst OOD data in the ball can shrink the OOD distribution discrepancy, thus improving the open-world detection performance given only the auxiliary OOD data. We conduct extensive evaluations across representative OOD detection setups, demonstrating the superiority of our DAL over its advanced counterparts. The code is publicly available at: https://github.com/tmlr-group/DAL.

## 1 Introduction

Deep learning in the open world often encounters out-of-distribution (OOD) data of which the label space is disjoint with that of the in-distribution (ID) cases Hendrycks and Gimpel (2017); Fang et al. (2022). It leads to the well-known OOD detection problem, where the predictor should make accurate predictions for ID data and detect anomalies from OOD cases Bulusu et al. (2020); Yang et al. (2021). Nowadays, OOD detection has attracted intensive attention in reliable machine learning due to its integral role in safety-critical applications Cao et al. (2020); Shen et al. (2021).

OOD detection remains challenging since predictors can make over-confidence predictions for OOD data Hendrycks et al. (2019), motivating recent studies towards effective OOD detection. Therein, outlier exposure Hendrycks et al. (2019); Ming et al. (2022) is among the most potent ones, learning from _auxiliary OOD data_ to discern ID and OOD patterns. However, due to the openness of the OOD task objective Wang et al. (2023), auxiliary OOD data can arbitrarily differ from the (unseen) real OOD data in the open world. So, to formally understand their consequences, we model the differencebetween auxiliary and real OOD data by their distribution discrepancy, measured by the Wasserstein distance (Villani, 2021, 2008). Then, we reveal the negative impacts of such OOD distribution discrepancy on the real detection power, with a larger distribution discrepancy indicating a lower performance on real OOD data, cf., Eq. (4).

The OOD distribution discrepancy threatens the open-world detection performance for outlier exposure. Therefore, we raise a natural question in _how to alleviate such an OOD distribution discrepancy_. Hence, this paper establishes a promising learning framework named _Distributional-Augmented OOD Learning_ (DAL). Therein, we augment the auxiliary OOD distribution by crafting an _OOD distribution set_ containing all distributions in a Wasserstein ball (Villani, 2021, 2008), centered on the auxiliary OOD distribution. Then, by making the predictor learn from the worst OOD distribution in the set, cf., Eq. (8), one can alleviate the negative impacts of the distribution discrepancy. Moreover, our proposed framework enjoys the learning guarantees towards the expected risk with respect to the real OOD distribution, making OOD detection stay effective when facing unseen data (cf., Theorem 3). Figure 1 provides a conceptual explanation: learning from the worst OOD distribution ensures the uniformly well performance inside the Wasserstein ball, enlarging the influence of the auxiliary OOD distribution. Thus, one can shrink the OOD distribution discrepancy between the auxiliary and the real OOD data and improve OOD detection.

In realization, the primal learning objective in Eq. (8) is generally intractable due to the infinite-dimensional optimization for the worst OOD distribution search. Instead, we adopt the dual form with respect to the original learning problem (cf., Theorem 1), transforming it into a tractable problem of the worst OOD data search in a finite-dimensional space. Furthermore, following Du et al. (2022); Mehra et al. (2022), the data search procedure is conducted in the embedding space, which can benefit the open-world performance of OOD detection with decent costs of additional computation.

We conduct extensive experiments over representative OOD detection setups, revealing the open-world performance of our method toward effective OOD detection. For example, our DAL reduces the average FPR95 by 1.99 to 13.46 on CIFAR benchmarks compared with the conventional outlier exposure (Hendrycks et al., 2019). Overall, we summarize our contributions into three folds:

* We measure the difference between the auxiliary and the real OOD data by the Wasserstein distance, and establish an effective learning framework, named DAL, to mitigate the OOD distribution discrepancy issue. We further guarantee our performance with respect to unseen real OOD data via Theorem 3, which is new to previous works.
* DAL leads to a practical method in Algorithm 1, learning from the worst cases in the Wasserstein ball to improve the open-world detection performance. Overall, our method solves the dual problem, which performs the worst-case search in the embedding space, which is simple to compute yet effective in OOD detection.
* We conduct extensive experiments in Section 5 to evaluate our effectiveness, ranging from the well-known CIFAR benchmarks to the challenging ImageNet settings. The empirical results comprehensively demonstrate our superiority over advanced counterparts, and the improvement is mainly attributed to our distributional-augmented learning framework.

A detailed overview of existing OOD detection methods and theories can be found in Appendix A, and a summary of the important notations can be found in Appendix B.

Figure 1: A heuristic illustration for our DAL. A large distribution discrepancy between the auxiliary and the unseen OOD data will hurt the real detection effectiveness. However, by ensuring uniformly well performance inside the Wasserstein ball, we can mitigate the distribution discrepancy and thus improve the detection power in the open world.

Outlier Exposure

Let \(\mathcal{X}\) denote the feature space and \(\mathcal{Y}=\{1,\ldots,C\}\) denote the label space with respect to the ID distribution. We consider the ID distribution \(D_{X_{1}Y_{1}}\), a joint distribution defined over \(\mathcal{X}\times\mathcal{Y}\), where \(X_{\mathrm{I}}\) and \(Y_{\mathrm{I}}\) are random variables whose outputs are from spaces \(\mathcal{X}\) and \(\mathcal{Y}\). We also have an OOD joint distribution \(D_{X_{0}Y_{0}}\), where \(X_{0}\) is a random variable from \(\mathcal{X}\), but \(Y_{0}\) is a random variable whose outputs do not belong to \(\mathcal{Y}\), i.e., \(Y_{0}\notin\mathcal{Y}\)(Fang et al., 2022).

The classical OOD detection (Hendrycks and Gimpel, 2017; Yang et al., 2021) typically considers an open-world setting, where the real OOD data drawn from \(D_{X_{0}Y_{0}}\) are unseen during training. Recently, Fang et al. (2022) have provided several _strong_ conditions necessary to ensure the success of the classical OOD setting. Furthermore, to increase the possibility of success for OOD detection and weaken the strong conditions proposed by Fang et al. (2022), advanced works (Hendrycks et al., 2019; Chen et al., 2021) introduce a promising approach named _outlier exposure_, where a set of auxiliary OOD data is employed as a surrogate of real OOD data. Here, we provide a formal definition.

**Problem 1** (OOD Detection with Outlier Exposure).: Let \(D_{X_{1}Y_{1}}\), \(D_{X_{0}}\), and \(D_{X_{\mathrm{A}}}\) be the ID joint distribution, the OOD distribution, and the auxiliary OOD distribution, respectively. Given the sets of samples called the ID and the auxiliary OOD data, namely,

\[S=\{(\mathbf{x}_{1}^{1},y_{1}^{1}),...,(\mathbf{x}_{1}^{n},y_{1}^{n})\}\sim D _{X_{1}Y_{1}}^{n},\ i.i.d.,\ \ \ \ T=\{\mathbf{x}_{\mathrm{A}}^{1},...,\mathbf{x}_{\mathrm{A}}^{m}\}\sim D_{X_{ \mathrm{A}}}^{m},\ i.i.d.,\]

outlier exposure trains a predictor \(\mathbf{f}\) by using the training data \(S\) and \(T\), such that for any test data \(\mathbf{x}\): 1) if \(\mathbf{x}\) is an observation from \(D_{X_{\mathrm{I}}}\), the predictor \(\mathbf{f}\) can classify \(\mathbf{x}\) into its correct ID label; otherwise 2) if \(\mathbf{x}\) is an observation from \(D_{X_{0}}\), the predictor \(\mathbf{f}\) can detect \(\mathbf{x}\) as an OOD case.

**OOD Scoring.** Many existing methods detect OOD data by using various score-based strategies (Hendrycks and Gimpel, 2017; Lee et al., 2018; Liu et al., 2020; Sun et al., 2022). In general, given a model \(\mathbf{f}:\mathcal{X}\rightarrow\mathbb{R}^{C}\) and a scoring function \(s(\cdot;\mathbf{f}):\mathcal{X}\rightarrow\mathbb{R}\), the OOD detector \(g_{\lambda}\) is given by:

\[g_{\lambda}(\mathbf{x})=\mathrm{ID},\ \ \text{if }s(\mathbf{x};\mathbf{f}) \geq\lambda;\ \text{otherwise},\ g_{\lambda}(\mathbf{x})=\mathrm{OOD},\]

where \(\lambda\) is a given threshold. For example, as a well-known baseline scoring function, the maximum softmax prediction (MSP) (Hendrycks and Gimpel, 2017) is given by:

\[s_{\text{MSP}}(\mathbf{x};\mathbf{f})=\max_{k\in\mathcal{Y}}\ \texttt{softmax}_{k}\ \mathbf{f}(\mathbf{x}),\] (1)

with \(\texttt{softmax}_{k}(\cdot)\) denoting the \(k\)-th dimension of the softmax output.

**Model and Risks.** We denote \(\mathbf{f}_{\mathbf{w}}:\mathcal{X}\rightarrow\mathbb{R}^{C}\) the predictor with parameters \(\mathbf{w}\in\mathcal{W}\), with \(\mathcal{W}\) the parameter space. We consider the loss functions \(\ell\) and \(\ell_{\mathrm{OE}}\) w.r.t. the ID and the OOD cases, respectively. Then, the expected and the empirical _ID risks_ of the model \(\mathbf{f}_{\mathbf{w}}\) can be written as:

\[R_{\mathrm{I}}(\mathbf{w})=\mathbb{E}_{(\mathbf{x},y)\sim D_{X_{1}Y_{1}}}\ell (\mathbf{f}_{\mathbf{w}};\mathbf{x},y)\ \ \text{and}\ \ \widehat{R}_{\mathrm{I}}(\mathbf{w})=\frac{1}{n}\sum_{i=1}^{n}\ell(\mathbf{f}_{ \mathbf{w}};\mathbf{x}_{\mathrm{I}}^{i},y_{\mathrm{I}}^{i}).\]

The expected and the empirical _auxiliary OOD risks_ are then given by

\[R_{\mathrm{A}}(\mathbf{w})=\mathbb{E}_{\mathbf{x}\sim D_{X_{\mathrm{A}}}}\ell _{\mathrm{OE}}(\mathbf{f}_{\mathbf{w}};\mathbf{x})\ \ \text{and}\ \ \widehat{R}_{\mathrm{A}}(\mathbf{w})=\frac{1}{m}\sum_{i=1}^{m}\ell_{\mathrm{OE}}( \mathbf{f}_{\mathbf{w}};\mathbf{x}_{\mathrm{A}}^{i}),\]

and the expected _real OOD risk_ is given by \(R_{\mathrm{O}}(\mathbf{w})=\mathbb{E}_{\mathbf{x}\sim D_{X_{0}}}\ell_{ \mathrm{OE}}(\mathbf{f}_{\mathbf{w}};\mathbf{x})\). Accordingly, we can define the expected _detection risk_ with respect to real OOD data, following

\[R_{D}(\mathbf{w})=R_{\mathrm{I}}(\mathbf{w})+\alpha R_{\mathrm{O}}(\mathbf{w}),\] (2)

where \(\alpha\) is the trade-off parameter.

**Learning Strategy.** After the scoring function is selected, one can obtain the OOD detector if the model \(\mathbf{f}_{\mathbf{w}}\) is given. Under the Problem 1 of outlier exposure, a common learning strategy is to optimize the empirical ID and auxiliary OOD risk jointly (Hendrycks et al., 2019), namely,

\[\min_{\mathbf{w}\in\mathcal{W}}\ \big{[}\widehat{R}_{\mathrm{I}}(\mathbf{w})+ \alpha\widehat{R}_{\mathrm{A}}(\mathbf{w})\big{]}.\] (3)

Note that the auxiliary OOD data are employed in Eq. (3), which can arbitrarily differ from the real OOD cases. Then, it is generally expected that the predictor \(\mathbf{f}_{\mathbf{w}}\), trained over the auxiliary OOD data, can perform well even on unseen OOD data, i.e., a small value of \(R_{D}(\mathbf{w})\) is expected.

Motivation

To the general learning strategy in Eq. (3), intuitively, if the auxiliary data are sampled from a distribution similar to real ones, the predictor will perform well for real OOD data. However, auxiliary and real OOD data differ in practice, posing us to suspect their open-world detection performance. To formally study the problem, we measure the difference between auxiliary and real OOD data in the distribution level, motivating our discussion of _OOD distribution discrepancy_.

**Distribution Discrepancy.** In this paper, we adopt a classical measurement for the distribution discrepancy--Optimal Transport Cost (Sinha et al., 2018; Mehra et al., 2022).

**Definition 1** (Optimal Transport Cost and Wasserstein-1 Distance (Villani, 2021, 2008)).: Given a cost function \(c:\mathcal{X}\times\mathcal{X}\rightarrow\mathbb{R}_{+}\), the _optimal transport cost_ between two distributions \(D\) and \(D^{\prime}\) is

\[\mathrm{W}_{c}(D,D^{\prime})=\inf_{\pi\in\Pi(D,D^{\prime})}\mathbb{E}_{(\mathbf{ x},\mathbf{x}^{\prime})\sim\pi}c(\mathbf{x},\mathbf{x}^{\prime}),\]

where \(\Pi(D,D^{\prime})\) is the space of all couplings for \(D\) and \(D^{\prime}\). Furthermore, if the cost \(c\) is a _metric_, then the optimal transport cost is also called the _Wasserstein-1_ distance.

Based on Definition 1, we use the distribution discrepancy to measure the difference between the auxiliary and the real OOD data, namely, \(\mathrm{W}_{c}(D_{X_{\mathrm{O}}},D_{X_{\mathrm{A}}})\). Then, we can formally study the impacts of such a discrepancy on the detection performance of the predictor. Under certain assumptions (cf., Corollary 1), we can prove that with high probability, the following generalization bound holds:

\[R_{D}(\widehat{\mathbf{w}})\leq\min_{\mathbf{w}\in\mathcal{W}}\left(R_{\mathrm{ l}}(\mathbf{w})+\alpha R_{\mathrm{A}}(\mathbf{w})\right)+\alpha L_{c}\mathrm{W}_{c}(D_{ X_{\mathrm{O}}},D_{X_{\mathrm{A}}})+\mathcal{O}(1/\sqrt{n})+\mathcal{O}(1/ \sqrt{m}),\] (4)

where \(\widehat{\mathbf{w}}\) is the parameter learned by Eq. (3), i.e., \(\widehat{\mathbf{w}}\in\arg\min_{\mathbf{w}\in\mathcal{W}}\ \widehat{R}_{\mathrm{l}}(\mathbf{w})+\alpha\widehat{R}_{\mathrm{A}}( \mathbf{w})\), \(L_{c}\) is the Lipschitz constant of \(\ell_{\mathrm{O}\mathrm{E}}\) w.r.t. the cost function \(c(\cdot,\cdot)\) (see Theorem 3). In general, the expected detection risk \(R_{D}(\widehat{\mathbf{w}})\) measures the expected performance on unseen OOD data given the predictor trained on the auxiliary OOD data. Then, due to the upper bound, the impacts of the OOD distribution discrepancy are reflected by the Wasserstein-1 distance between the auxiliary and the real OOD data, i.e., \(\mathrm{W}_{c}(D_{X_{\mathrm{O}}},D_{X_{\mathrm{A}}})\). Therefore, although classical outlier exposure can improve OOD detection to some extent, it fails to ensure reliable detection of unseen OOD data, in that a larger distribution discrepancy generally indicates a worse guarantee for open-world OOD detection.

The key to improve the detection performance is mitigating the negative impact induced by the OOD distribution discrepancy. To tackle this problem, a simple lemma inspires us:

**Lemma 1**.: _Let \(d(\cdot,\cdot)\) be the distance to measure the discrepancy between distributions. Given a space \(\mathfrak{D}\) consisting of some OOD distributions, if \(D_{X_{\mathrm{A}}}\in\mathfrak{D}\), then_

\[\inf_{D_{X^{\prime}}\in\mathfrak{D}}d(D_{X^{\prime}},D_{X_{\mathrm{O}}})\leq d (D_{X_{\mathrm{A}}},D_{X_{\mathrm{O}}}).\] (5)

_If \(d(\cdot,\cdot)\) is the Optimal Transport Cost in Definition 1, the cost function \(c\) is a continuous metric, and \(\mathfrak{D}\) is the Wasserstein-1 ball with a radius \(\rho>0\), i.e., \(\mathfrak{D}=\{D_{X^{\prime}}:\mathrm{W}_{c}(D_{X^{\prime}},D_{X_{\mathrm{A}}} )\leq\rho\}\), then_

\[\inf_{D_{X^{\prime}}\in\mathfrak{D}}\mathrm{W}_{c}(D_{X^{\prime}},D_{X_{ \mathrm{O}}})\leq\max\{\mathrm{W}_{c}(D_{X_{\mathrm{A}}},D_{X_{\mathrm{O}}})- \rho,0\}.\] (6)

In the light of Lemma 1, we introduce a specific set of distributions \(\mathfrak{D}\), augmented around the auxiliary OOD distribution. It makes it possible to mitigate the distribution discrepancy, following Eqs. (5) and (6). Therefore, instead of choosing a model \(\mathbf{f_{w}}\) that directly minimizes the empirical risk in Eq. (3), we target augmenting the auxiliary OOD data within the distribution space \(\mathfrak{D}\), namely,

\[\min_{\mathbf{w}\in\mathcal{W}}\big{[}\widehat{R}_{\mathrm{l}}(\mathbf{w})+ \alpha\sup_{D_{X^{\prime}}\in\mathfrak{D}}\mathbb{E}_{\mathbf{x}\sim D_{X^{ \prime}}}\ell_{\mathrm{O}\mathrm{E}}(\mathbf{f_{w}};\mathbf{x})\big{]},\ \text{subject to }\widehat{D}_{X_{\mathrm{A}}}\in\mathfrak{D},\] (7)

where \(\widehat{D}_{X_{\mathrm{A}}}\) is the empirical form of \(D_{X_{\mathrm{A}}}\), i.e., \(\widehat{D}_{X_{\mathrm{A}}}=\frac{1}{m}\sum_{i=1}^{m}\delta_{\mathbf{x}_{ \mathrm{A}}^{\downarrow}}\) and \(\delta_{\mathbf{x}_{\mathrm{A}}^{\downarrow}}\) is the dirac measure.

## 4 Learning Framework

This section proposes a general learning framework to mitigate the OOD distribution discrepancy. As aforementioned, we consider an augmented set of OOD distributions to improve OOD detection, thus named _Distributional-Augmented OOD Learning_ (DAL).

To begin with, we need to select a suitable distribution space \(\mathfrak{D}\) for the tractable solutions of Eq. (7). Generally, the choice of \(\mathfrak{D}\) influences both the richness of the auxiliary data as well as the tractability of the resulting optimization problem. Previous works have developed a series of distribution spaces, e.g., the distribution ball based on \(f\)-divergences (Namkoong and Duchi, 2016; Michel et al., 2021) and maximum mean discrepancy (MMD) (Staib and Jegelka, 2019). However, there are several drawbacks for the distribution balls based on \(f\)-divergences and MMD: 1) any \(f\)-divergence-based space \(\mathfrak{D}\) contains only distributions within the same support set as \(\widehat{D}_{X_{\mathrm{A}}}\); and 2) the effective solutions in the MMD-based space have not been provided (Staib and Jegelka, 2019).

Instead, motivated by Sinha et al. (2018); Mehra et al. (2022); Dai et al. (2023) and Theorem 1, we consider the Wasserstein ball. For any \(\rho>0\), we define the augmented OOD distribution set as

\[\mathfrak{D}=\{D_{X^{\prime}}:\mathrm{W}_{c}(D_{X^{\prime}},\widehat{D}_{X_{ \mathrm{A}}})\leq\rho\},\]

and consider the following optimization problem:

\[\min_{\mathbf{w}\in\mathcal{W}}\widehat{R}_{D}(\mathbf{w};\rho)=\min_{ \mathbf{w}\in\mathcal{W}}\big{[}\widehat{R}_{\mathrm{I}}(\mathbf{w})+\alpha \widehat{R}_{\mathrm{O}}(\mathbf{w};\rho)\big{]},\] (8)

where

\[\widehat{R}_{\mathrm{O}}(\mathbf{w};\rho)=\sup_{\mathrm{W}_{c}(D_{X^{\prime}},\widehat{D}_{X_{\mathrm{A}}})\leq\rho}\mathbb{E}_{\mathbf{x}\sim D_{X^{\prime }}}\ell_{\mathrm{OE}}(\mathbf{f}_{\mathbf{w}};\mathbf{x}).\] (9)

However, the optimization problem in Eq. (8) is intractable due to the infinite-dimensional search for the distribution \(D_{X^{\prime}}\). Fortunately, the following dual theorem provides a solution:

**Theorem 1** (Blanchet and KarthyekRajhahaA. (2016)).: _Let \(c(\cdot,\cdot)\) be a continuous metric and \(\phi_{\gamma}(\mathbf{w};\mathbf{x})=\sup_{\mathbf{x}^{\prime}\in\mathcal{X}} \{\ell(\mathbf{f}_{\mathbf{w}};\mathbf{x}^{\prime})-\gamma c(\mathbf{x}^{ \prime},\mathbf{x})\}\) be the robust surrogate function. Then, for any \(\rho>0\),_

\[\widehat{R}_{D}(\mathbf{w};\rho)=\widehat{R}_{\mathrm{I}}(\mathbf{w})+\alpha \inf_{\gamma\geq 0}\big{\{}\gamma\rho+\frac{1}{m}\sum_{i=1}^{m}\phi_{\gamma}( \mathbf{w};\mathbf{x}_{\mathrm{A}}^{i})\big{\}}.\] (10)

Theorem 1 provides a feasible surrogate for the original optimization problem in Eq. (8), transforming the infinite-dimensional problem to its finite counterpart, i.e., the data feature search. We use Eq. (10) to design our algorithm, cf., Section 4.2.

### Theoretical Supports

This section provides the theoretical support for our DAL. Specifically, 1) Theorem 2 shows that the empirical model given by Eq. (8) can achieve consistent learning performance, and 2) Theorem 3 further demonstrates the expected detection risk estimation, i.e., \(R_{D}(\mathbf{w})\), with respect to the empirical model given by Eq. (8). All the proofs can be found in Appendix C. To state our theoretical results, we use the notation \(R_{D}(\mathbf{w};\rho)\) to represent the ideal form of \(\widehat{R}_{D}(\mathbf{w};\rho)\), which is defined by

\[R_{D}(\mathbf{w};\rho)=R_{\mathrm{I}}(\mathbf{w})+\alpha R_{\mathrm{O}}( \mathbf{w};\rho),\]

where

\[R_{\mathrm{O}}(\mathbf{w};\rho)=\sup_{\mathrm{W}_{c}(D_{X^{\prime}},D_{X_{ \mathrm{A}}})\leq\rho}\mathbb{E}_{\mathbf{x}\sim D_{X^{\prime}}}\ell_{\mathrm{ OE}}(\mathbf{f}_{\mathbf{w}};\mathbf{x}).\]

Similar to Sinha et al. (2018), our results rely on the covering number (cf., Appendix C.1) for the model classes \(\mathcal{F}=\{\ell(\mathbf{f}_{\mathbf{w}};\cdot):\mathbf{w}\in\mathcal{W}\}\) and \(\mathcal{F}_{\mathrm{OE}}=\{\ell_{\mathrm{OE}}(\mathbf{f}_{\mathbf{w}};\cdot ):\mathbf{w}\in\mathcal{W}\}\) to represent their complexity. Intuitively, the covering numbers \(\mathcal{N}(\mathcal{F},\epsilon,L^{\infty})\) and \(\mathcal{N}(\mathcal{F}_{\mathrm{OE}},\epsilon,L^{\infty})\) are the minimal numbers of \(L^{\infty}\) balls of radius \(\epsilon>0\) needed to cover the model classes \(\mathcal{F}\) and \(\mathcal{F}_{\mathrm{OE}}\), respectively. Now, we demonstrate that DAL can achieve consistent performance under mild assumptions.

**Theorem 2** (Excess Generalization Bound).: _Assume that \(0\leq\ell(\mathbf{f}_{\mathbf{w}};\mathbf{x},y)\leq M_{\ell}\), \(0\leq\ell_{\mathrm{OE}}(\mathbf{f}_{\mathbf{w}};\mathbf{x})\leq M_{\ell_{ \mathrm{OE}}}\), and \(c(\cdot,\cdot):\mathcal{X}\times\mathcal{X}\to\mathbb{R}_{+}\) is a continuous metric. Let \(\widehat{\mathbf{w}}\) be the optimal solution of Eq. (8), i.e., \(\widehat{\mathbf{w}}\in\arg\min_{\mathbf{w}\in\mathcal{W}}\widehat{R}_{D}( \mathbf{w};\rho)\). Then with the probability at least \(1-4e^{-t}>0\),_

\[R_{D}(\widehat{\mathbf{w}};\rho)-\min_{\mathbf{w}\in\mathcal{W}}R_{D}(\mathbf{w} ;\rho)\leq\epsilon(n,m;t),\] (11)

_for any \(\rho>0\), where_

\[\epsilon(n,m;t)= \frac{b_{0}M_{\ell}}{\sqrt{n}}\int_{0}^{1}\sqrt{\log\mathcal{N}( \mathcal{F},M_{\ell}\epsilon,L^{\infty})}d\epsilon+2M_{\ell}\sqrt{\frac{2t}{n}}\] \[+ \alpha b_{1}\sqrt{\frac{M_{\ell_{\mathrm{OE}}}^{3}}{\rho^{2}m}}\int_ {0}^{1}\sqrt{\log\mathcal{N}(\mathcal{F}_{\mathrm{OE}},M_{\ell_{\mathrm{OE}}} \epsilon,L^{\infty})}\mathrm{d}\epsilon+\alpha b_{2}M_{\ell_{\mathrm{OE}}} \sqrt{\frac{2t}{m}},\]

_where \(b_{0}\), \(b_{1}\) and \(b_{2}\) are uniform constants._Furthermore, under proper conditions, one can show that the bound in Eq. (11) can attain \(\mathcal{O}(1/\sqrt{n})+\mathcal{O}(1/\sqrt{m})\), i.e., \(R_{D}(\widehat{\mathbf{w}};\rho)-\min_{\mathbf{w}\in\mathcal{W}}R_{D}(\mathbf{ w};\rho)\leq\mathcal{O}(1/\sqrt{n})+\mathcal{O}(1/\sqrt{m})\). Corollary 1 in Appendix C.5 gives an example to support the above claim. Next, we give a learning bound to estimate the expected detection risk in Eq. (2) w.r.t. the model \(\mathbf{f}_{\widehat{\mathbf{w}}}\) given by Eq. (8).

**Theorem 3** (Risk Estimation).: _Given the same conditions in Theorem 2 and let \(\widehat{\mathbf{w}}\) be the solution of Eq. (8), which is given by \(\widehat{\mathbf{w}}\in\arg\min_{\mathbf{w}\in\mathcal{W}}\widehat{R}_{D}( \mathbf{w};\rho)\). If \(\ell_{\mathrm{OE}}(\mathbf{f}_{\mathbf{w}};\mathbf{x})\) is \(L_{c}\)-Lipschitz w.r.t. \(c(\cdot,\cdot)\), i.e., \(|\ell_{\mathrm{OE}}(\mathbf{f}_{\mathbf{w}};\mathbf{x})-\ell_{\mathrm{OE}}( \mathbf{f}_{\mathbf{w}};\mathbf{x}^{\prime})|\leq L_{c}(\mathbf{x},\mathbf{x }^{\prime})\), then with the probability at least \(1-4e^{-t}>0\),_

\[R_{D}(\widehat{\mathbf{w}})-\overbrace{\min_{\mathbf{w}\in\mathcal{W}}R_{D}( \mathbf{w};\rho)}^{\text{approximate risk}}\leq\underbrace{\alpha L_{c}\max\{\mathrm{W }_{c}(D_{X_{\mathrm{O}}},D_{X_{\mathrm{A}}})-\rho,0\}+\epsilon(n,m;t)}_{ \text{estimation error}},\]

_for any \(\rho>0\), where \(\epsilon(n,m;t)\) is defined in Theorem 2._

The bias term \(\alpha L_{c}\max\{\mathrm{W}_{c}(D_{X_{\mathrm{A}}})-\rho,0\}=0\) when \(\rho\) is large enough. Hence, a large \(\rho\) implies a small estimation error. Although a larger \(\rho\) leads to better generalization ability, the approximate risk \(\min_{\mathbf{w}\in\mathcal{W}}R_{D}(\mathbf{w};\rho)\) may become larger. It implies that for practical effectiveness, i.e., small \(R_{D}(\widehat{\mathbf{w}})\), there is a trade-off between the approximate risk \(\min_{\mathbf{w}\in\mathcal{W}}R_{D}(\mathbf{w};\rho)\) and the bias \(\alpha L_{c}\max\{\mathrm{W}_{c}(D_{X_{\mathrm{O}}},D_{X_{\mathrm{A}}})-\rho,0\}\) across different choices of \(\rho\). Hence, we need to choose a proper \(\rho\) for open-world detection with unseen data (cf., Section 5.3).

### Proposed Algorithm

In this section, we introduce the algorithm design for DAL, summarized in Algorithm 1. Due to the space limit, we provide further discussions in Appendix E.

``` Input: ID and OOD samples from \(D_{X_{1}Y_{1}}\) and \(D_{X_{\mathrm{A}}}\); for\(\mathbf{st}=1\)to num_stepdo  Sample \(S_{\mathrm{B}}\) and \(T_{\mathrm{B}}\) from \(D_{X_{1}Y_{1}}\) and \(D_{X_{\mathrm{A}}}\);  Initialize \(\mathbf{p}^{i}\sim\mathcal{N}(\mathbf{0},\sigma I),\ \forall\ i\in\{1,\ldots,|T_{ \mathrm{B}}|\}\); for\(\mathbf{se}=1\)to num_searchdo \(\psi^{i}=\nabla_{\mathbf{p}^{i}}\left[\ell_{\mathrm{OE}}\left(\mathbf{h}( \mathbf{e}(\mathbf{x}^{i}_{\mathrm{A}}+\mathbf{p}^{i});\mathbf{e}(\mathbf{x}^ {i}_{\mathrm{A}}))-\gamma\left\|\mathbf{p}^{i}\right\|_{1}\right],\ \forall\ i\in\{1,\ldots,|T_{ \mathrm{B}}|\}\); \(\mathbf{p}^{i}\leftarrow\mathbf{p}^{i}+\mathfrak{psw}^{i},\ \forall\ i\in\{1,\ldots,|T_{ \mathrm{B}}|\}\) endfor \(\gamma\leftarrow\min\big{(}\max\big{(}\gamma-\beta(\rho-\frac{1}{|T_{ \mathrm{B}}|}\sum_{i=1}^{|T_{\mathrm{B}}|}\|\mathbf{p}^{i}\|,\gamma_{\text{max} }\big{)},0\big{)}\); \(\mathbf{w}\leftarrow\mathbf{w}-\mathbf{1r}\nabla_{\mathbf{w}}\big{[}\frac{1}{|T _{\mathrm{B}}|}\sum_{i=1}^{|T_{\mathrm{B}}|}\ell_{\mathrm{OE}}(\mathbf{h}( \mathbf{g}(\mathbf{x}^{i}_{\mathrm{A}})+\mathbf{p}^{i}))+\alpha\frac{1}{|S_{ \mathrm{B}}|}\sum_{i=1}^{|S_{\mathrm{B}}|}\ell(\mathbf{f}_{\mathbf{w}}; \mathbf{x}^{i}_{\mathrm{I}},y^{i}_{\mathrm{I}}))\big{]}\); endfor Output: model parameter \(\mathbf{w}\). ```

**Algorithm 1** Distribution-Augmented OOD Learning (DAL)

**Losses and Cost Function.** Following Hendrycks et al. (2019), we adopt the cross entropy loss to realize \(\ell\) and the KL-divergence between model predictions and uniform distribution for \(\ell_{\mathrm{OE}}\). We also define the cost function \(c\) by the \(l_{1}\) norm, namely, \(c(\mathbf{x},\mathbf{x}^{\prime})=\|\mathbf{x}-\mathbf{x}^{\prime}\|_{1}\).

**Algorithm Design.** By Theorem 1, we can address the primary problem in Eq. (8) by the dual problem in Eq. (9). Additionally, following Du et al. (2022), we perturb for the worst OOD data in the embedding space. Denote the model \(\mathbf{f}_{\mathbf{w}}=\mathbf{h}\circ\mathbf{e}\) with \(\mathbf{h}\) the classifier and \(\mathbf{e}\) the feature extractor, we find the perturbation \(\mathbf{p}\) for the embedding features, i.e., \(\mathbf{e}(\mathbf{x})\), of the associated data \(\mathbf{x}\). The perturbation \(\mathbf{p}\) should lead to the worst OOD case for the surrogate function in Theorem 1, namely,

\[\phi_{\gamma}(\mathbf{w};\mathbf{e}(\mathbf{x}))=\sup_{\mathbf{p}\in\mathcal{E}} \left\{\ell_{\mathrm{OE}}(\mathbf{h}(\mathbf{e}(\mathbf{x})+\mathbf{p}); \mathbf{e}(\mathbf{x}))-\gamma\|\mathbf{p}\|_{1}\right\},\]

where \(\mathcal{E}\) denotes the space of embedding features. Note that we abuse the definition of \(\ell_{\mathrm{OE}}\), emphasizing that we perturb the embedding features of \(\mathbf{e}(\mathbf{x})\) by \(\mathbf{p}\).

**Training and Inference.** Our definition of \(\phi_{\gamma}(\mathbf{w};\mathbf{e}(\mathbf{x}))\) leads to a particular realization of Eq. (10), which is the learning objective of our DAL. It can be solved by stochastic gradient optimization for deep models, e.g., mini-batch stochastic gradient descent. After training, we use the MSP scoring function by default and discuss the possibility of other scoring functions in Appendix F.3.

**Stochastic Realization.** Algorithm 1 gives a stochastic realization of DAL, where ID and auxiliary OOD mini-batches are randomly sampled in each stochastic iteration, denoted by \(S_{\mathrm{B}}\) and \(T_{\mathrm{B}}\) respectively. Therein, we first find the perturbation \(\mathbf{p}\) that leads to the maximal \(\phi_{\gamma}(\mathbf{w},\mathbf{e}(\mathbf{x}))\). The value of \(\mathbf{p}\) is initialized by random Gaussian noise with the standard deviation \(\sigma\) and updated by gradient ascent for num_search steps with the perturbation strength \(\mathtt{ps}\). Then we update \(\gamma\) by one step of gradient descent with the learning rate \(\beta\), and further clipping between \(0\) and \(\gamma_{\text{max}}\) to avoid extreme values. Finally, given the proper perturbations for the auxiliary OOD data in \(T_{\mathrm{B}}\), we update the model parameter \(\mathbf{w}\) by one step of mini-batch gradient descent.

## 5 Experiments

In this section, we mainly test DAL on the CIFAR (Krizhevsky and Hinton, 2009) benchmarks (as ID datasets). To begin with, we introduce the evaluation setups.

**OOD Datasets.** We adopt the 80 Million Tiny Images (Torralba et al., 2008) as the auxiliary OOD dataset; Textures (Cimpoi et al., 2014), SVHN (Netzer et al., 2011), Places\(365\)(Zhou et al., 2018), LSUN (Yu et al., 2015), and iSUN (Xu et al., 2015) as the (test-time) real OOD datasets. We eliminate those data whose labels coincide with ID cases.

**Pre-training Setups.** We employ Wide ResNet-40-2 (Zagoruyko and Komodakis, 2016) trained for \(200\) epochs via empirical risk minimization, with a batch size \(64\), momentum \(0.9\), and initial learning rate \(0.1\). The learning rate is divided by \(10\) after \(100\) and \(150\) epochs.

**Hyper-parameters Tuning Strategy.** The hyper-parameters are tuned based on the validation data, separated from the training ID and auxiliary OOD data, which is a common strategy in OOD detection with outlier exposure field (Hendrycks et al., 2019; Chen et al., 2021). Specifically, we fix \(\sigma=0.001,\mathtt{num\_search}=10\), and adopt the grid search to choose \(\gamma_{\text{max}}\) from \(\{0.1,0.5,1,5,10,50\}\); \(\beta\) from \(\{1e^{-3},5e^{-3},1e^{-2},5e^{-2},1e^{-\intercal},5e^{-\intercal},1,5\}\); \(\rho\) from \(\{1e^{-2},1e^{-1},1,10,100\}\); \(\mathtt{ps}\) from \(\{1e^{-3},1e^{-2},1e^{-1},1,10,100\}\); \(\alpha\) from \(\{0.1,0.5,1.0,1.5,2.0\}\).

**Hyper-parameters Setups.** For CIFAR-10, DAL is run for \(50\) epochs with the ID batch size \(128\), the OOD batch size \(256\), the initial learning rate \(0.07\), \(\gamma_{\text{max}}=10\), \(\beta=0.01\), \(\rho=10\), \(\mathtt{ps}=1\), and \(\alpha=1\). For CIFAR-100, DAL is run for \(50\) epochs with the ID batch size \(128\), the OOD batch size \(256\), the initial learning rate \(0.07\), \(\gamma_{\text{max}}=10\), \(\beta=0.005\), \(\rho=10\), and \(\mathtt{ps}=1\), and \(\alpha=1\). For both cases, we employ cosine decay (Loshchilov and Hutter, 2017) for the model learning rate.

**Baseline Methods.** We compare DAL with representative methods, including MSP (Hendrycks and Gimpel, 2017), Free Energy (Liu et al., 2020), ASH (Djurisic et al., 2023), ReAct (Sun et al., 2021), Mahalanobis (Lee et al., 2018), KNN (Sun et al., 2022), KNN+ (Sun et al., 2022), CSI (Tack et al., 2020), VOS (Du et al., 2022), Outlier Exposure (OE) (Hendrycks et al., 2019), Energy-OE (Liu et al., 2020), ATOM (Chen et al., 2021), DOE (Wang et al., 2023), and POEM (Ming et al., 2022). We adopt their suggested setups but unify the backbones for fairness.

**Evaluation Metrics.** The detection performance is evaluated via two representative metrics, which are both threshold-independent: the false positive rate of OOD data when the true positive rate of ID data is at \(95\%\) (FPR\(95\)); and the area under the receiver operating characteristic curve (AUROC), which can be viewed as the probability of the ID case having greater score than that of the OOD case.

Due to the space limit, we test our DAL with more advanced scoring strategies in Appendix F.3 and conduct experiments on the more complex ImageNet (Deng et al., 2009) dataset in Appendix F.10.

### Main Results

The main results are summarized in Table 1, where we report the detailed results across the considered real OOD datasets. First, we reveal that using auxiliary OOD data can generally lead to better results than using only ID information, indicating that outlier exposure remains a promising direction worth studying. However, as demonstrated in Section 3, the OOD distribution discrepancy can hurt its open-world detection power, while previous works typically oversee such an important issue. Therefore, our DAL, which can alleviate the OOD distribution discrepancy, reveals a large improvement over the original outlier exposure. Specifically, comparing with the conventional outlier exposure, our method reveals 1.99 and 0.13 average improvements w.r.t. FPR95 and AUROC on the CIFAR-10 dataset, and 13.46 and 3.65 of the average improvements on CIFAR-100 dataset. For advanced works that consider the OOD sampling strategies, e.g., ATOM and POEM, DAL can achieve much better results, especially for the CIFAR-100 case. The reason is that these methods mainly consider the situationswhere the model capacity is not enough to learn from all the auxiliary OOD data, deviating from our considered issue in OOD distribution discrepancy. Moreover, for the previous works that adopt similar concepts in the worst-case OOD learning, e.g., VOS and DOE, DAL also reveals better results, with 1.29 and 30.89 improvements on the CIFAR-10 dataset and 11.27 and 44.12 improvements on the CIFAR-100 dataset w.r.t. FPR95. It indicates that our theoretical-driven scheme can also guide the algorithm designs with practical effectiveness. Note that many previous works use advanced scoring strategies other than MSP, and thus our experiment above is not completely fair to us. Therefore, in Appendix F.3, we also combine DAL with many advanced scoring strategies other than MSP, which can further improve our performance.

### Hard OOD Detection

We further consider hard OOD scenarios (Sun et al., 2022), of which the test OOD data are very similar to that of the ID cases. Following the common setup (Sun et al., 2022) with the CIFAR-\(10\) dataset being the ID case, we evaluate our DAL on three hard OOD datasets, namely, LSUN-Fix (Yu et al., 2015), ImageNet-Resize (Deng et al., 2009), and CIFAR-\(100\). Note that data in ImageNet-Resize (\(1000\) classes) with the same semantic space as Tiny-ImageNet (\(200\) classes) are removed. We select a set of strong baselines that are competent in hard OOD detection, summarizing the experiments in Table 2. As we can see, our method can beat these advanced methods across the considered datasets, even for the challenging

\begin{table}
\begin{tabular}{c|c c c c c c c c c c c} \hline \hline \multirow{2}{*}{Method} & \multicolumn{3}{c}{SUN} & \multicolumn{3}{c}{LSUN} & \multicolumn{3}{c}{Testures} & \multicolumn{3}{c}{Place\({}_{356}\)} & \multicolumn{3}{c}{Average} \\ \cline{2-13}  & FPR95\(\downarrow\) & AUROC \(\uparrow\) & FPR95\(\downarrow\) & AUROC \(\uparrow\) & FPR95\(\downarrow\) & AUROC \(\uparrow\) & FPR95\(\downarrow\) & AUROC \(\uparrow\) & FPR95\(\downarrow\) & AUROC \(\uparrow\) \\ \hline \hline \multicolumn{13}{c}{**CBRAR-10**} \\ \hline  & \multicolumn{13}{c}{Using ID data only} \\ \hline MSP & 48.59 & 91.97 & 25.53 & 96.49 & 56.44 & 98.96 & 59.68 & 88.42 & 60.19 & 88.36 & 50.15 & 91.02 \\ Free Energy & 35.21 & 91.24 & 4.42 & 99.06 & 35.84 & 92.56 & 52.46 & 85.35 & 40.11 & 90.02 & 33.21 & 91.64 \\ Asht & 33.98 & 91.79 & 4.76 & 98.98 & 34.38 & 92.64 & 50.90 & 86.07 & 40.89 & 89.79 & 32.98 & 91.85 \\ Mahalanobis & 12.21 & 97.70 & 57.25 & 93.98 & 79.74 & 77.85 & 15.20 & 95.40 & 68.81 & 82.39 & 46.64 & 88.59 \\ KNN & 26.56 & 95.93 & 27.52 & 95.43 & 33.55 & 93.15 & 37.62 & 93.07 & 41.67 & 91.21 & 33.83 & 93.36 \\ KNN+ & 3.28 & 99.33 & 2.24 & 98.90 & 71.85 & 96.65 & 10.87 & 97.72 & 30.63 & 94.98 & 12.97 & 97.32 \\ CSI & 17.37 & 97.69 & 6.75 & 98.46 & 12.58 & 97.95 & 25.65 & 94.70 & 40.00 & 92.05 & 20.47 & 96.17 \\ VOS & 36.55 & 93.30 & 9.98 & 98.03 & 28.93 & 94.25 & 52.83 & 85.74 & 39.56 & 89.71 & 33.57 & 92.21 \\ \hline  & \multicolumn{13}{c}{Using ID data and auxiliary OOD data} \\ \hline  & 2.36 & 99.72 & 1.15 & **99.08** & 2.48 & 99.34 & 5.35 & 98.88 & 11.99 & 97.23 & 4.67 & 98.88 \\ Energy-OE & 0.97 & 99.54 & 1.00 & 99.15 & 2.32 & 99.37 & 3.42 & **99.18** & 9.57 & 97.44 & 3.46 & 98.91 \\ ATOM & 1.00 & 99.53 & 0.61 & 99.53 & 2.15 & **99.0** & 2.52 & 99.10 & 7.93 & 97.27 & 2.84 & 98.97 \\ DOE & 1.80 & 99.37 & **0.45** & 99.65 & 2.00 & 99.36 & 6.95 & 87.15 & 91.58 & 97.28 & 3.97 & 98.88 \\ POGM & 1.20 & 99.53 & 0.80 & 99.10 & **1.47** & 99.26 & 2.93 & 99.13 & 7.65 & 97.35 & 2.81 & 98.87 \\ DAL & **0.80** & **99.65** & 0.90 & 99.46 & 1.70 & 99.34 & **2.30** & 99.14 & **7.65** & **97.45** & **2.68** & **99.01** \\ \hline \hline \multicolumn{13}{c}{**CBRAR-100**} \\ \hline  & \multicolumn{13}{c}{Using ID data only} \\ \hline MSP & 84.90 & 71.18 & 60.36 & 85.59 & 82.63 & 75.69 & 83.32 & 75.39 & 83.37 & 73.69 & 78.61 & 75.95 \\ Free Energy & 85.24 & 73.71 & 23.05 & 95.89 & 81.11 & 79.02 & 96.73 & 76.35 & 80.18 & 75.65 & 69.94 & 80.12 \\ Asht & 70.09 & 83.56 & 13.20 & 97.71 & 69.87 & 52.56 & 63.69 & 83.59 & 79.70 & 78.47 & 59.31 & 84.46 \\ Mahalanobis & 51.00 & 88.70 & 91.60 & 60.69 & 38.48 & 91.96 & 47.07 & 89.09 & 82.70 & 74.18 & 72.37 & 82.70 \\ KNN & 52.10 & 88.83 & 68.82 & 70.06 & 42.17 & 90.59 & 42.79 & 89.07 & 92.21 & 61.08 & 59.62 & 81.71 \\ KNN+ & 32.50 & 93.66 & 47.41 & 84.43 & 93.92 & 91.12 & 40.50 & 88.55 & 62.76 & 79.28 & 45.20 & 87.55 \\ CSI & 64.50 & 84.62 & 25.88 & 95.93 & 70.62 & 80.83 & 61.50 & 86.74 & 83.08 & 77.11 & 61.12 & 95.05 \\ VOS & 78.06 & 92.59 & 40.40 & 92.90 & 85.77 & 70.20 & 82.46 & 77.22 & 82.31 & 75.47 & 73.80 & 91.67 \\ \hline  & \multicolumn{13}{c}{Using ID data and auxiliary OOD data} \\ \hline  & \multicolumn{13}{c}{Using ID data only} \\ \hline Free Energy & 6.42 & 63.78 & 98.55 & 46.46 & 89.02 & 50.47 & 87.08 \\ Asht & 4.00 & 98.20 & 46.18 & 88.55 & 54.31 & 83.71 \\ KNN+ & 24.88 & 95.75 & 30.52 & 94.85 & 40.00 & 89.11 \\ CSI & 39.79 & 93.63 & 37.47 & 93.93 & 45.64 & 87.64 \\ \hline  & \multicolumn{13}{c}{Using ID data and auxiliary OOD data} \\ \hline OE & 1.75 & 99.47 & 6.76 & 98.85 & 29.40 & 94.20 \\ DOE & 1.97 & 98.71 & 5.98 & 98.75 & 29.75 & 94.24 \\ POGM & **1.24** & 98.93 & 6.56 & 98.37 & 35.11 & 91.80 \\ DAL & 1.39 & **99.47** & **5.60** & **98.80** & **25.45** & **94.34** \\ \hline \hline \end{tabular}
\end{table}
Table 1: Comparison between our method and advanced methodsCIFAR-\(10\) versus CIFAR-\(100\) setting. The reason is that our distributional augmentation directly learns from OOD data close to ID pattern, which can cover hard OOD cases.

### Ablation Study

We further conduct an ablation study to demonstrate two mechanisms that mainly contribute to our open-world effectiveness, namely, OOD data generation and Wasserstein ball constraint.

**OOD Data Generation.** DAL learns from the worst OOD data to mitigate the OOD distribution discrepancy. To understand such an OOD generation scheme, we employ the t-SNE visualization (Van der Maaten and Hinton, 2008) for the ID, the auxiliary OOD, and the worst OOD data. Figure 2 summarizes the results before and after DAL training. Before training, the ID and auxiliary OOD data overlap largely, indicating that the original model is not effective at distinguishing between them. Then, DAL does not directly train the model on auxiliary OOD data but instead perturbs it to further confuse the model beyond the overlap region. After DAL training, the overlap region between ID and auxiliary OOD data shrinks. Additionally, perturbing the original OOD data becomes more difficult, indicating that the model has learned to handle various worst-case OOD scenarios.

**Wasserstein Ball Constraint.** The choice of \(\rho\) determines the radius of the Wasserstein ball. Larger values of \(\rho\) reduce estimation error and improve model generalization, as stated in Theorem 3. However, larger values of \(\rho\) also increase the approximate risk \(\min_{\mathbf{w}\in\mathcal{W}}R_{D}(\mathbf{w};\rho)\) as it becomes more challenging to ensure uniform model performance with increased distributional perturbation. Figure 3 shows the FPR95 curves on the CIFAR-100 dataset for both the real and the surrogate OOD data, revealing the trade-off in selecting \(\rho\). Here, we consider two setups of \(\rho\), i.e., \(\mathtt{ps}=1\) (default) and \(\mathtt{ps}=10\) (large perturbation strength). First, when the perturbation strength is very large (i.e., \(\mathtt{ps}=10\)), the model can easily fail for training if the value of \(\rho\) is also large (e.g., \(\rho=100\)), indicating that large value of \(\rho\) can lead to a large approximation error. However, such an issue can be overcome by selecting a relatively small value of \(\rho\) (e.g., \(\rho=0.5\)).

## 6 Conclusion

Outlier exposure is one of the most powerful methods in OOD detection, but the discrepancy between the auxiliary and (unseen) real OOD data can hinder its practical effectiveness. To address such an issue, we have formalized it as the OOD distribution discrepancy and developed an effective learning framework to mitigate its negative impacts. Specifically, we consider a specific distribution set that contains all distributions in a Wasserstein ball centered on the auxiliary OOD distribution. Then, models trained over worst-case OOD data in the ball can ensure improved performance toward open-world OOD detection. Overall, as pioneers in critically analyzing the open-world setting with theoretical analysis, we are committed to raising attention to the OOD distribution discrepancy issue and encouraging further research in this direction.

## Acknowledgments and Disclosure of Funding

QZW, YGZ, and BH were supported by the NSFC Young Scientists Fund No. 62006202, NSFC General Program No. 62376235, Guangdong Basic and Applied Basic Research Foundation No. 2022A1515011652, HKBU Faculty Nichee Research Areas No. RC-FNRA-IG/22-23/SCI/04, and HKBU CSD Departmental Incentive Scheme. FL was supported by Australian Research Council (ARC) under Award No. DP230101540, and by NSF and CSIRO Responsible AI Program under Award No. 2303037. YXL was supported by the AFOSR Young Investigator Program under award number FA9550-23-1-0184, National Science Foundation (NSF) Award No. IIS-2237037 & IIS-2331669, and Office of Naval Research under grant number N00014-23-1-2643.

## References

* Hendrycks and Gimpel (2017) Dan Hendrycks and Kevin Gimpel. A baseline for detecting misclassified and out-of-distribution examples in neural networks. In _ICLR_, 2017.
* Fang et al. (2022) Zhen Fang, Yixuan Li, Jie Lu, Jiahua Dong, Bo Han, and Feng Liu. Is out-of-distribution detection learnable? In _NeurIPS_, 2022.
* Bulusu et al. (2020) Saikiran Bulusu, Bhavya Kailkhura, Bo Li, P Varshney, and Dawn Song. Anomalous instance detection in deep learning: A survey. Technical report, Lawrence Livermore National Lab., 2020.
* Yang et al. (2021) Jingkang Yang, Kaiyang Zhou, Yixuan Li, and Ziwei Liu. Generalized out-of-distribution detection: a survey. _arXiv preprint arXiv:2110.11334_, 2021.
* Cao et al. (2020) Tianshi Cao, Chin-Wei Huang, David Yu-Tung Hui, and Joseph Paul Cohen. A benchmark of medical out of distribution detection. _arXiv preprint arXiv:2007.04250_, 2020.
* Shen et al. (2021) Zheyan Shen, Jiashuo Liu, Yue He, Xingxuan Zhang, Renzhe Xu, Han Yu, and Peng Cui. Towards out-of-distribution generalization: A survey. _arXiv preprint arXiv:2108.13624_, 2021.
* Hendrycks et al. (2019) Dan Hendrycks, Mantas Mazeika, and Thomas G. Dietterich. Deep anomaly detection with outlier exposure. In _ICLR_, 2019.
* Ming et al. (2022) Yifei Ming, Ying Fan, and Yixuan Li. POEM: Out-of-distribution detection with posterior sampling. In _ICML_, 2022.
* Wang et al. (2023) Qizhou Wang, Junjie Ye, Feng Liu, Quanyu Dai, Marcus Kalander, Tongliang Liu, Jianye Hao, and Bo Han. Out-of-distribution detection with implicit outlier transformation. In _ICLR_, 2023.
* Villani (2021) Cedric Villani. _Topics in Optimal Transportation_. American Mathematical Society, 2021.
* Villani (2008) Cedric Villani. _Optimal Transport: Old and New_. Springer, 2008.
* Du et al. (2022a) Xuefeng Du, Zhaoning Wang, Mu Cai, and Yixuan Li. VOS: Learning what you don't know by virtual outlier synthesis. In _ICLR_, 2022a.
* Mehra et al. (2022) Akshay Mehra, Bhavya Kailkhura, Pin-Yu Chen, and Jihun Hamm. On certifying and improving generalization to unseen domains. _arXiv preprint arXiv:2206.12364_, 2022.
* Chen et al. (2021) Jiefeng Chen, Yixuan Li, Xi Wu, Yingyu Liang, and Somesh Jha. ATOM: Robustifying out-of-distribution detection using outlier mining. In _ECML_, 2021.
* Lee et al. (2018a) Kimin Lee, Kibok Lee, Honglak Lee, and Jinwoo Shin. A simple unified framework for detecting out-of-distribution samples and adversarial attacks. In _NeurIPS_, 2018a.
* Liu et al. (2020) Weitang Liu, Xiaoyun Wang, John D Owens, and Yixuan Li. Energy-based out-of-distribution detection. In _NeurIPS_, 2020.
* Sun et al. (2022) Yiyou Sun, Yifei Ming, Xiaojin Zhu, and Yixuan Li. Out-of-distribution detection with deep nearest neighbors. In _ICML_, 2022.
* Sinha et al. (2018) Aman Sinha, Hongseok Namkoong, and John C. Duchi. Certifying some distributional robustness with principled adversarial training. In _ICLR_, 2018.
* Sun et al. (2019)Hongseok Namkoong and John C. Duchi. Stochastic gradient methods for distributionally robust optimization with f-divergences. In _NIPS_, 2016.
* Michel et al. (2021) Paul Michel, Tatsunori Hashimoto, and Graham Neubig. Modeling the second player in distributionally robust optimization. In _ICLR_, 2021.
* Staib and Jegelka (2019) Matthew Staib and Stefanie Jegelka. Distributionally robust optimization and generalization in kernel methods. In _NeurIPS_, 2019.
* Dai et al. (2023) Rui Dai, Yonggang Zhang, Zhen Fang, Bo Han, and Xinmei Tian. Moderately distributional exploration for domain generalization. In _ICML_, 2023.
* Blanchet and KarthyekRajhaaA (2016) Jose H. Blanchet and M. KarthyekRajhaaA. Quantifying distributional model risk via optimal transport. _Risk Management eJournal_, 2016.
* Krizhevsky and Hinton (2009) Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. _Technical Report TR-2009, University of Toronto_, 2009.
* Torralba et al. (2008) Antonio Torralba, Rob Fergus, and William T Freeman. 80 million tiny images: A large data set for nonparametric object and scene recognition. _IEEE transactions on pattern analysis and machine intelligence_, 30(11):1958-1970, 2008.
* Cimpoi et al. (2014) Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and Andrea Vedaldi. Describing textures in the wild. In _CVPR_, 2014.
* Netzer et al. (2011) Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading digits in natural images with unsupervised feature learning. In _NIPS Workshop_, 2011.
* Zhou et al. (2018) Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva, and Antonio Torralba. Places: A 10 million image database for scene recognition. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 40(6):1452-1464, 2018.
* Yu et al. (2015) Fisher Yu, Ari Seff, Yinda Zhang, Shuran Song, Thomas Funkhouser, and Jianxiong Xiao. LSUN: construction of a large-scale image dataset using deep learning with humans in the loop. _arXiv preprint arXiv:1506.03365_, 2015.
* Xu et al. (2015) Pingmei Xu, Krista A Ehinger, Yinda Zhang, Adam Finkelstein, Sanjeev R Kulkarni, and Jianxiong Xiao. Turkergaze: crowdsourcing saliency with webcam based eye tracking. _arXiv preprint arXiv:1504.06755_, 2015.
* Zagoruyko and Komodakis (2016) Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. In _BMVC_, 2016.
* Loshchilov and Hutter (2017) Ilya Loshchilov and Frank Hutter. SGDR: Stochastic gradient descent with warm restarts. In _ICLR_, 2017.
* Djurisic et al. (2023) Andrija Djurisic, Nebojsa Bozanic, Arjun Ashok, and Rosanne Liu. Extremely simple activation shaping for out-of-distribution detection. In _ICLR_, 2023.
* Sun et al. (2021) Yiyou Sun, Chuan Guo, and Yixuan Li. ReAct: out-of-distribution detection with rectified activations. In _NeurIPS_, 2021.
* Tack et al. (2020) Jihoon Tack, Sangwoo Mo, Jongheon Jeong, and Jinwoo Shin. CSI: novelty detection via contrastive learning on distributionally shifted instances. In _NeurIPS_, 2020.
* Deng et al. (2009) Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: a large-scale hierarchical image database. In _CVPR_, 2009.
* Van der Maaten and Hinton (2008) Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-SNE. _Journal of machine learning research_, 9(11):2579-2605, 2008.
* Liang et al. (2018) Shiyu Liang, Yixuan Li, and R. Srikant. Enhancing the reliability of out-of-distribution image detection in neural networks. In _ICLR_, 2018.
* Wang et al. (2021) Haoran Wang, Weitang Liu, Alex Bocchieri, and Yixuan Li. Can multi-label classification networks know what they don't know? In _NeurIPS_, 2021a.
* Wang et al. (2021)Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive uncertainty estimation using deep ensembles. In _NIPS_, 2017.
* Huang and Li (2021) Rui Huang and Yixuan Li. MOS: towards scaling out-of-distribution detection for large semantic space. In _CVPR_, 2021.
* Sastry and Oore (2020) Chandramouli Shama Sastry and Sageev Oore. Detecting out-of-distribution examples with gram matrices. In _ICML_, 2020.
* Wang et al. (2022a) Haoqi Wang, Zhizhong Li, Litong Feng, and Wayne Zhang. ViM: Out-of-distribution with virtual-logit matching. In _CVPR_, 2022a.
* Lin et al. (2021) Ziqian Lin, Sreya Dutta Roy, and Yixuan Li. MOOD: Multi-level out-of-distribution detection. In _CVPR_, 2021.
* Morteza and Li (2022) Peyman Morteza and Yixuan Li. Provable guarantees for understanding out-of-distribution detection. In _AAAI_, 2022.
* Luo et al. (2023) Yadan Luo, Zijian Wang, Zhuoxiao Chen, Zi Huang, and Mahsa Baktashmotlagh. Source-free progressive graph learning for open-set domain adaptation. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 45(9):11240-11255, 2023.
* Huang et al. (2021) Rui Huang, Andrew Geng, and Yixuan Li. On the importance of gradients for detecting distributional shifts in the wild. In _NeurIPS_, 2021.
* Igoe et al. (2022) Conor Igoe, Youngseog Chung, Ian Char, and Jeff Schneider. How useful are gradients for ood detection really? _arXiv preprint arXiv:2205.10439_, 2022.
* Zhu et al. (2023a) Jianing Zhu, Hengzhuang Li, Jiangchao Yao, Tongliang Liu, Jianliang Xu, and Bo Han. Unleashing mask: Explore the intrinsic out-of-distribution detection capability. In _ICML_, 2023a.
* Sehwag et al. (2021) Vikash Sehwag, Mung Chiang, and Prateek Mittal. SSD: A unified framework for self-supervised outlier detection. In _ICLR_, 2021.
* Wang et al. (2022b) Haotao Wang, Aston Zhang, Yi Zhu, Shuai Zheng, Mu Li, Alex J Smola, and Zhangyang Wang. Partial and asymmetric contrastive learning for out-of-distribution detection in long-tailed recognition. In _ICML_, 2022b.
* Zheng et al. (2023) Haotian Zheng, Qizhou Wang, Zhen Fang, Xiaobo Xia, Feng Liu, Tongliang Liu, and Bo Han. Out-of-distribution detection learning with unreliable out-of-distribution sources. In _NeurIPS_, 2023.
* Du et al. (2022b) Xuefeng Du, Gabriel Gozum, Yifei Ming, and Yixuan Li. SIREN: Shaping representations for detecting out-of-distribution objects. In _NeurIPS_, 2022b.
* Ming et al. (2023) Yifei Ming, Yiyou Sun, Ousmane Dia, and Yixuan Li. How to exploit hyperspherical embeddings for out-of-distribution detection? In _ICLR_, 2023.
* Zaeemzadeh et al. (2021) Alireza Zaeemzadeh, Niccolo Bisagno, Zeno Sambugaro, Nicola Conci, Nazanin Rahnavard, and Mubarak Shah. Out-of-distribution detection using union of 1-dimensional subspaces. In _CVPR_, 2021.
* Wei et al. (2022) Hongxin Wei, Renchunzi Xie, Hao Cheng, Lei Feng, Bo An, and Yixuan Li. Mitigating neural network overconfidence with logit normalization. In _ICML_, 2022.
* Huang et al. (2023) Zhuo Huang, Xiaobo Xia, Li Shen, Bo Han, Mingming Gong, Chen Gong, and Tongliang Liu. Harnessing out-of-distribution examples via augmenting content and style. In _ICLR_, 2023a.
* Zhu et al. (2023b) Jianing Zhu, Geng Yu, Jiangchao Yao, Tongliang Liu, Gang Niu, Masashi Sugiyama, and Bo Han. Diversified outlier exposure for out-of-distribution detection via informative extrapolation. In _NeurIPS_, 2023b.
* Li and Vasconcelos (2020) Yi Li and Nuno Vasconcelos. Background data resampling for outlier-aware classification. In _CVPR_, 2020.
* Li et al. (2020)* Hein et al. (2019) Matthias Hein, Maksym Andriushchenko, and Julian Bitterwolf. Why relu networks yield high-confidence predictions far away from the training data and how to mitigate the problem. In _CVPR_, 2019.
* Jeong and Kim (2020) Taewon Jeong and Heeyoung Kim. OOD-MAMI: Meta-learning for few-shot out-of-distribution detection and classification. In _NeurIPS_, 2020.
* Amersfoort et al. (2020) Joost Van Amersfoort, Lewis Smith, Yee Whye Teh, and Yarin Gal. Uncertainty estimation using a single deep deterministic neural network. In _ICML_, 2020.
* Lee et al. (2018) Kimin Lee, Honglak Lee, Kibok Lee, and Jinwoo Shin. Training confidence-calibrated classifiers for detecting out-of-distribution samples. In _ICLR_, 2018b.
* Vernekar et al. (2019) Sachin Vernekar, Ashish Gaurav, Vahdat Abdelzad, Taylor Denouden, Rick Salay, and Krzysztof Czarnecki. Out-of-distribution detection in classifiers via generation. In _NeurIPS Workshop_, 2019.
* Tao et al. (2023) Leitian Tao, Xuefeng Du, Xiaojin Zhu, and Yixuan Li. Non-parametric outlier synthesis. In _ICLR_, 2023.
* Luo et al. (2020) Yadan Luo, Zijian Wang, Zi Huang, and Mahsa Baktashmotlagh. Progressive graph learning for open-set domain adaptation. In _ICML_, 2020.
* Zhang et al. (2021) Lily H. Zhang, Mark Goldstein, and Rajesh Ranganath. Understanding failures in out-of-distribution detection with deep generative models. In _ICML_, 2021.
* Fang et al. (2021) Zhen Fang, Jie Lu, Anjin Liu, Feng Liu, and Guangquan Zhang. Learning bounds for open-set learning. In _ICML_, 2021.
* Bitterwolf et al. (2022) Julian Bitterwolf, Alexander Meinke, Maximilian Augustin, and Matthias Hein. Breaking down out-of-distribution detection: Many methods based on OOD training data estimate a combination of the same core quantities. In _ICML_, 2022.
* Vershynin (2018) Roman Vershynin. _High-Dimensional Probability_. Springer, 2018.
* Blanchet et al. (2019) Jose Blanchet, Yang Kang, and Karthyek Murthy. Robust wasserstein profile inference and applications to machine learning. _Journal of Applied Probability_, 56(3):830-857, 2019.
* Golowich et al. (2018) Noah Golowich, Alexander Rakhlin, and Ohad Shamir. Size-independent sample complexity of neural networks. In _COLT_, 2018.
* Madry et al. (2018) Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. In _ICLR_, 2018.
* Wang et al. (2021) Qizhou Wang, Feng Liu, Bo Han, Tongliang Liu, Chen Gong, Gang Niu, Mingyuan Zhou, and Masashi Sugiyama. Probabilistic margins for instance reweighting in adversarial training. In _NeurIPS_, 2021b.
* Huang et al. (2023) Zhuo Huang, Miaoxi Zhu, Xiaobo Xia, Li Shen, Jun Yu, Chen Gong, Bo Han, Bo Du, and Tongliang Liu. Robust generalization against photon-limited corruptions via worst-case sharpness minimization. In _CVPR_, 2023b.
* Liu et al. (2021) Risheng Liu, Jiaxin Gao, Jin Zhang, Deyu Meng, and Zhouchen Lin. Investigating bi-level optimization for learning and vision from a unified perspective: A survey and beyond. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 44(12):10045-10067, 2021.
* Zhang et al. (2022) Yonggang Zhang, Mingming Gong, Tongliang Liu, Gang Niu, Xinmei Tian, Bo Han, Bernhard Scholkopf, and Kun Zhang. Adversarial robustness through the lens of causality. In _ICLR_, 2022.
* Ridnik et al. (2021) Tal Ridnik, Emanuel Ben-Baruch, Asaf Noy, and Lihi Zelnik-Manor. ImageNet-21k pretraining for the masses. _arXiv preprint arXiv:2104.10972_, 2021.
* Van Horn et al. (2018) Grant Van Horn, Oisin Mac Aodha, Yang Song, Yin Cui, Chen Sun, Alexander Shepard, Hartwig Adam, Pietro Perona, and Serge J. Belongie. The iNaturalist species classification and detection dataset. In _CVPR_, 2018.
* Hendrycks et al. (2022) Dan Hendrycks, Steven Basart, Mantas Mazeika, Mohammadreza Mostajabi, Jacob Steinhardt, and Dawn Song. Scaling out-of-distribution detection for real-world settings. In _ICML_, 2022.
* Zhu et al. (2021)