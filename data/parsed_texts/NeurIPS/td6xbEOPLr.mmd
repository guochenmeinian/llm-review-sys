# Fate: Fairness Attacks on Graph Learning

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

We study fairness attacks on graph learning to answer the following question: _How can we achieve poisoning attacks on a graph learning model to exacerbate the bias?_ We answer this question via a bi-level optimization problem and propose a meta learning-based attacking framework named Fate. The proposed framework is broadly applicable with respect to various fairness definitions and graph learning models, as well as arbitrary choices of manipulation operations. We further instantiate Fate to attack statistical parity and individual fairness on graph neural networks. We conduct extensive experimental evaluations on real-world datasets in the task of semi-supervised node classification. The experimental results demonstrate that Fate could amplify the bias of graph neural networks with or without fairness consideration while maintaining the utility on the downstream task. We hope this paper provides insights into the adversarial robustness of fair graph learning and can shed light on designing robust and fair graph learning in future studies.

## 1 Introduction

Algorithmic fairness in graph learning has received much research attention [5; 20; 24]. Despite its substantial progress, existing studies mostly assume the benevolence of input graphs and aim to ensure that the bias would not be perpetuated or amplified in the learning process. However, malicious activities in the real world are commonplace. For example, consider a financial fraud detection system which utilizes a transaction network to classify whether a bank account is fraudulent or not [49; 45]. An adversary may manipulate the transaction network (e.g., malicious banker with access to the transaction data, theft of bank accounts to make malicious transactions), so that the graph-based fraud detection model would exhibit unfair classification results with respect to people of different demographic groups. Consequently, a biased fraud detection model may infringe civil liberty to certain financial activities and impact the well-being of an individual negatively [6]. It would also make the graph learning model fail to provide the same quality of service to people of certain demographic groups, causing the financial institutions to lose business in the communities of the corresponding demographic groups. Thus, it is critical to understand how resilient a graph learning model is with respect to adversarial attacks on fairness, which we term as _fairness attacks_.

To date, fairness attack has not been well studied. Sporadic literature often follows two strategies: (1) adversarial data point injection, which is often designed for tabular data rather than graphs [38; 33; 8; 44] or (2) adversarial edge injection, which only attacks the group fairness of a graph neural network [19]. It is thus crucial to study how to attack different fairness definitions for a variety of graph learning models.

To achieve this goal, we study the Fairness attacks on graph learning (Fate) problem. We formulate it as a bi-level optimization, where the lower-level problem optimizes a task-specific loss function to make the fairness attacks deceptive and the upper-level problem leverages the supervision signal to modify the input graph and maximize the bias function corresponding to a user-defined fairness definition. To solve the bi-level optimization problem, we propose a meta learning-based solver(Fate), whose key idea is to compute the meta-gradient of the upper-level bias function with respect to the input graph to guide the fairness attacks. Compared with existing works, our proposed Fate framework has two major advantages. First, it is capable of attacking _any_ fairness definition on _any_ graph learning model, as long as the corresponding bias function and the task-specific loss function are differentiable. Second, it is equipped with the ability for either continuous or discretized poisoning attacks on the graph topology. We also briefly discuss its ability for poisoning attacks on node features in a later section.

The major contributions of this paper are summarized as follows.

* **Problem definition.** We formally define the problem of fairness attacks on graph learning (the Fate problem). Based on the definition, we formulate it as a bi-level optimization problem, whose key idea is to maximize a bias function in the upper level while minimizing a task-specific loss function for a graph learning task.
* **Attacking framework.** We propose an end-to-end attacking framework named Fate. It learns a perturbed graph topology via meta learning, such that the bias with respect to the learning results trained with the perturbed graph will be amplified.
* **Empirical evaluation.** We conduct experiments on three benchmark datasets to demonstrate the efficacy of our proposed Fate framework in amplifying the bias while being the most deceptive method (i.e., achieving the highest micro F1 score) on semi-supervised node classification.

## 2 Preliminaries and Problem Definition

**A - Notations.** Throughout the paper, we use bold upper-case letter for matrix (e.g., \(\mathbf{A}\)), bold lower-case letter for vector (e.g., \(\mathbf{x}\)) and calligraphic letter for set (e.g., \(\mathcal{G}\)). We use superscript \({}^{T}\) to denote the transpose of a matrix/vector (e.g., \(\mathbf{x}^{T}\) is the transpose of \(\mathbf{x}\)). Regarding matrix/vector indexing, we use conventions similar to NumPy in Python. For example, \(\mathbf{A}[i,j]\) is the entry of \(\mathbf{A}\) at the \(i\)-th row and \(j\)-th column; \(\mathbf{x}[i]\) is the \(i\)-th entry of \(\mathbf{x}\); \(\mathbf{A}[i,:]\) and \(\mathbf{A}[j,:]\) are the \(i\)-th row and \(j\)-th column of \(\mathbf{A}\), respectively.

**B - Algorithmic fairness.** The general principle of algorithmic fairness is to ensure the learning results would not favor one side or another.1 Among several fairness definitions that follow this principle, group fairness [16; 18] and individual fairness [15] are the most widely studied ones. Group fairness splits the entire population into multiple demographic groups by a sensitive attribute (e.g., gender) and ensure the parity of a statistical property among learning results of those groups. For example, statistical parity, a classic group fairness definition, guarantees the statistical independence between the learning results (e.g., predicted labels of a classification algorithm) and the sensitive attribute [16]. Individual fairness suggests that similar individuals should be treated similarly. It is often formulated as a Lipschitz inequality such that distance between the learning results of two data points should be no larger than the difference between these two data points [15].

Footnote 1: https://www.merriam-webster.com/dictionary/fairness

**C - Problem definition.** Existing work [19] for fairness attacks on graphs randomly injects adversarial edges so that the disparity between the learning results of two different demographic groups would be amplified. However, it suffers from three major limitations. (1) First, it only attacks statistical parity while overlooking other fairness definitions (e.g., individual fairness [15]). (2) Second, it only considers adversarial edge injection, excluding other manipulations like edge deletion or reweighting. Hence, it is essential to investigate the possibility to attack other fairness definitions on real-world graphs with an arbitrary choice of manipulation operations. (3) Third, it does not consider the utility of graph learning models while achieving the fairness attacks, resulting in performance degradation in the downstream tasks. However, an institution that applies the graph learning models are often utility-maximizing [28; 2]. Thus, a performance degradation in the utility would make the fairness attacks not deceptive from the perspective of a utility-maximizing institution.

In this paper, we seek to overcome the aforementioned limitations. To be specific, given an input graph, an optimization-based graph learning model, and a user-defined fairness definition, we aim to learn a modified graph such that a bias function of the corresponding fairness definition would be maximized for _effective_ fairness attacks, while minimizing the task-specific loss function with respect to the graph learning model for _deceptive_ fairness attacks. Formally, we define the problem of fairness attacks on graph learning, which is referred to as the Fate problem.

**Problem 1**Fate_: Fairness Attacks on Graph Learning_

**Given:** (1) An undirected graph \(\mathcal{G}=\{\mathbf{A},\mathbf{X}\}\); (2) a task-specific loss function \(l(\mathcal{G},\mathcal{Y},\Theta,\theta)\) where \(\mathcal{Y}\) is the graph learning results, \(\Theta\) is the set of learnable variables and \(\theta\) is the set of hyperparameters; (3) a bias function \(b(\mathbf{Y},\Theta^{*},\mathbf{F},\theta)\) where \(\Theta^{*}=\arg\min_{\Theta}l(\mathcal{G},\mathcal{Y},\Theta,\theta)\) and \(\mathbf{F}\) is the matrix that contains auxiliary fairness-related information (e.g., sensitive attribute values of all nodes in \(\mathcal{G}\) for group fairness, pairwise node similarity matrix for individual fairness); (4) an integer budget \(B\).

**Find:** a poisoned graph \(\widetilde{\mathcal{G}}=\{\widetilde{\mathbf{A}},\widetilde{\mathbf{X}}\}\) which satisfies the following properties: (1) \(d(\mathcal{G},\widetilde{\mathcal{G}})\leq B\) where \(d(\mathcal{G},\widetilde{\mathcal{G}})\) is the distance between the input graph \(\mathcal{G}\) and the poisoned graph \(\widetilde{\mathcal{G}}\) (e.g., \(\|\mathbf{A},\widetilde{\mathbf{A}}\|_{1,1}\)); (2) the bias function \(b\left(\mathbf{Y},\Theta^{*},\mathbf{F}\right)\) is maximized for effectiveness; (3) the task-specific loss function \(l\left(\widetilde{\mathcal{G}},\mathcal{Y},\Theta,\theta\right)\) is minimized for deceptiveness.

## 3 Methodology

In this section, we first formulate Problem 1 as a bi-level optimization problem, followed by a generic meta learning-based solver named Fate.

### Problem Formulation

Given an input graph \(\mathcal{G}=\{\mathbf{A},\mathbf{X}\}\) with adjacency matrix \(\mathbf{A}\) and node feature matrix \(\mathbf{X}\), an attacker aims to learn a poisoned graph \(\widetilde{\mathcal{G}}=\{\widetilde{\mathbf{A}},\widetilde{\mathbf{X}}\}\) such that the graph learning model will be maximally biased when trained on \(\widetilde{\mathcal{G}}\). In this work, we consider the following settings for the attacker.

**The goal of the attacker.** The attacker aims to amplify the bias of the graph learning results output by a victim graph learning model. And the bias to be amplified is a choice made by the attacker based on which fairness definition the attacker aims to attack.

**The knowledge of the attacker.** Following similar settings in [19], we assume the attacker has access to the adjacency matrix, the feature matrix of the input graph, and the sensitive attribute of all nodes in the graph. For a (semi-)supervised learning problem, we assume that the ground-truth labels of the training nodes are also available to the attacker. For example, for a graph-based financial fraud detection problem, the malicious banker may have access to the demographic information (i.e., sensitive attribute) of the account holders and also know whether some bank accounts are fraudulent or not, which are the ground-truth labels for training nodes. Similar to [51, 52, 19], the attacker has no knowledge about the parameters of the victim model. Instead, the attacker will perform a gray-box attack by attacking a surrogate graph learning model.

**The capabilitiy of the attacker.** The attacker is able to perturb up to \(B\) edges/features in the graph (i.e., \(\|\mathbf{A}-\widetilde{\mathbf{A}}\|_{1,1}\leq B\) or \(\|\mathbf{X}-\widetilde{\mathbf{X}}\|_{1,1}\leq B\)).

Based on that, we formulate Problem 1 as a bi-level optimization problem as follows.

\[\begin{split}\widetilde{\mathcal{G}}=\arg\max_{\mathcal{G}}& \;b\left(\mathbf{Y},\Theta^{*},\mathbf{F}\right)\\ \text{s.t.}&\Theta^{*}=\arg\min_{\Theta}l\left( \mathcal{G},\mathbf{Y},\Theta,\theta\right),\;d\left(\mathcal{G},\widetilde{ \mathcal{G}}\right)\leq B\end{split}\] (1)

where the lower-level problem learns an optimal surrogate graph learning model \(\Theta^{*}\) by minimizing \(l\left(\mathcal{G},\mathbf{Y},\Theta,\theta\right)\), the upper-level problem finds a poisoned graph \(\widetilde{\mathcal{G}}\) that could maximize a bias function \(b\left(\mathbf{Y},\Theta^{*},\mathbf{F}\right)\) for the victim graph learning model and the distance between the input graph and the poisoned graph \(d\left(\mathcal{G},\widetilde{\mathcal{G}}\right)\) is constrained to satisfy the setting about the budgeted attack. Note that Eq. (1) is applicable to attack _any_ fairness definition on _any_ graph learning model, as long as the bias function \(b\left(\mathbf{Y},\Theta^{*},\mathbf{F}\right)\) and the loss function \(l\left(\mathcal{G},\mathbf{Y},\Theta,\theta\right)\) are differentiable.

**A - Lower-level optimization problem.** A wide spectrum of graph learning models are essentially solving an optimization problem. Take the graph convolutional network (GCN) [26] as an example. It learns the node representation by aggregating information from its neighborhood, i.e., message passing. Mathematically, for an \(L\)-layer GCN, the hidden representation at \(k\)-th layer can be represented as \(\mathbf{E}^{(k)}=\sigma\left(\widehat{\mathbf{A}}\mathbf{E}^{(k-1)}\mathbf{W}^ {(k)}\right)\) where \(\sigma\) is a nonlinear activation function (e.g., ReLU), \(\widehat{\mathbf{A}}=\mathbf{D}^{-1/2}\left(\mathbf{A}+\mathbf{I}\right)\mathbf{ D}^{-1/2}\) with \(\mathbf{D}\) being the degree matrix of \(\left(\mathbf{A}+\mathbf{I}\right)\) and \(\mathbf{W}^{(k)}\) is the learnable weight matrix of the \(k\)-th layer. Then the lower-level optimization problem aims to learn the setof parameters \(\Theta^{*}=\{\mathbf{W}^{(k)}|k=1,\ldots,L\}\) that could minimize a task-specific loss function (e.g., cross-entropy loss for semi-supervised node classification). For more examples of graph learning models from the optimization perspective, please refers to Appendix A.

**B - Upper-level optimization problem.** To attack the fairness aspect of a graph learning model, we aim to maximize a differentiable bias function \(b\left(\mathbf{Y},\Theta^{*},\mathbf{F}\right)\) with respect to a user-defined fairness definition in the upper-level optimization problem. For example, for statistical parity [16], the fairness-related auxiliary information matrix \(\mathbf{F}\) can be defined as the one-hot demographic membership matrix, where \(\mathbf{F}[i,j]=1\) if and only if node \(i\) belongs to \(j\)-th demographic group. Then the statistical parity is equivalent to the statistical independence between the learning results \(\mathbf{Y}\) and \(\mathbf{F}\). Based on that, existing studies propose several differentiable measurements of the statistical dependence between \(\mathbf{Y}\) and \(\mathbf{F}\) as the bias function. For example, Bose et al. [5] use mutual information \(I(\mathbf{Y};\mathbf{F})\) as the bias function; Prost et al. [35] define the bias function as the Maximum Mean Discrepancy _MMD_\(\left(\mathcal{Y}_{0},\mathcal{Y}_{1}\right)\) between the learning results of two different demographic groups \(\mathcal{Y}_{0}\) and \(\mathcal{Y}_{1}\).

### The Fate Framework

To solve Eq. (1), we propose a generic attacking framework named Fate to learn the poisoned graph. The key idea is to view Eq. (1) as a meta learning problem, which aims to find suitable hyperparameter settings for a learning task [3], and treat the graph \(\mathcal{G}\) as a hyperparameter. With that, we learn the poisoned graph \(\widetilde{\mathcal{G}}\) using the meta-gradient of the bias function \(b\left(\mathbf{Y},\Theta^{*},\mathbf{F}\right)\) with respect to \(\mathcal{G}\). In the following, we introduce two key parts of Fate in details, including meta-gradient computation and graph poisoning with meta-gradient.

**A - Meta-gradient computation.** The key term to learn the poisoned graph is the meta-gradient of the bias function with respect to the graph \(\mathcal{G}\). Before computing the meta-gradient, we assume that the lower-level optimization problem converges in \(T\) epochs. Thus, we first pre-train the lower-level optimization problem by \(T\) epochs to obtain the optimal model \(\Theta^{*}=\Theta^{(T)}\) before computing the meta-gradient. The training of the lower-level optimization problem can also be viewed as a dynamic system with the following updating rule

\[\Theta^{(t+1)}=\mathrm{opt}^{(t+1)}\left(\mathcal{G},\Theta^{(t)},\theta, \mathbf{Y}\right),\;\forall t\in\{1,\ldots,T\}\] (2)

where \(\Theta^{(1)}\) refers to \(\Theta\) at initialization, \(\mathrm{opt}^{(t+1)}(\cdot)\) is an optimizer that minimizes the lower-level loss function \(l\left(\mathcal{G},\mathbf{Y},\Theta^{(t)},\theta\right)\) at \((t+1)\)-th epoch. From the perspective of the dynamic system, by applying the chain rule and unrolling the training of lower-level problem with Eq. (2), the meta-gradient \(\nabla_{\mathcal{G}}b\) can be written as

\[\nabla_{\mathcal{G}}b=\nabla_{\mathcal{G}}b\left(\mathbf{Y},\Theta^{(T)}, \mathbf{F}\right)+\sum_{t=0}^{T-2}A_{t}B_{t+1}\ldots B_{T-1}\nabla_{\theta^{(T )}}b\left(\mathbf{Y},\Theta^{(T)},\mathbf{F}\right)\] (3)

where \(A_{t}=\nabla_{\mathcal{G}}\Theta^{(t+1)}\) and \(B_{t}=\nabla_{\Theta^{(t)}}\Theta^{(t+1)}\). However, Eq. (3) is computationally expensive in both time and space. To further speed up the computation, we adopt a first-order approximation of the meta-gradient [17] and simplify the meta-gradient as

\[\nabla_{\mathcal{G}}b\approx\nabla_{\Theta^{(T)}}b\left(\mathbf{Y},\Theta^{(T )},\mathbf{F}\right)\cdot\nabla_{\mathcal{G}}\Theta^{(T)}\] (4)

Since the input graph is undirected, the derivative of the symmetric adjacency matrix \(\mathbf{A}\) can be computed as follows by applying the chain rule of a symmetric matrix [21].

\[\nabla_{\mathbf{A}}b\leftarrow\nabla_{\mathbf{A}}b+\left(\nabla_{\mathbf{A}} b\right)^{T}-\mathrm{diag}\left(\nabla_{\mathbf{A}}b\right)\] (5)

For the node feature matrix \(\mathbf{X}\), its derivative is equal to the partial derivative \(\nabla_{\mathbf{X}}b\) since it is often an asymmetric matrix.

**B - Graph poisoning with meta-gradient.** After computing the meta-gradient of the bias function \(\nabla_{\mathcal{G}}b\), we aim to poison the input graph guided by \(\nabla_{\mathcal{G}}b\). We introduce two poisoning strategies: (1) continuous poisoning and (2) discretized poisoning.

_Continuous poisoning attack._ The continuous poisoning attack is straightforward by reweighting edges in the graph. We first compute the meta-gradient of the bias function \(\nabla_{\mathbf{A}}b\), then use it to poison the input graph in a gradient descent-based updating rule as follows.

\[\mathbf{A}\leftarrow\mathbf{A}-\eta\nabla_{\mathbf{A}}b\] (6)where \(\eta\) is a learning rate to control the magnitude of the poisoning attack. The learning rate should satisfy \(\eta\leq\frac{B}{\|\nabla_{\mathbf{A}}\|_{1,1}}\) to ensure that constraint on the budgeted attack.

_Discretized poisoning attack._ The discretized poisoning attack aims to select a set of edges to be added/deleted. It is guided by a poisoning preference matrix defined as follows.

\[\nabla_{\mathbf{A}}=(\mathbf{1}-2\mathbf{A})\circ\nabla_{\mathbf{A}}b\] (7)

where \(\mathbf{1}\) is an all-one matrix with the same dimension as \(\mathbf{A}\) and \(\circ\) denotes the Hadamard product. A large positive \(\nabla_{\mathbf{A}}[i,j]\) indicates strong preference in adding an edge if nodes \(i\) and \(j\) are not connected (i.e., positive \(\nabla_{\mathbf{A}}b[i,j]\), positive \((\mathbf{1}-2\mathbf{A})[i,j]\)) or deleting an edge if nodes \(i\) and \(j\) are connected (i.e., negative \(\nabla_{\mathbf{A}}b[i,j]\), negative \((\mathbf{1}-2\mathbf{A})[i,j]\)). Then, a greedy selection strategy is applied to find the set of edges \(\mathcal{E}_{\text{attack}}\) to be added/deleted.

\[\mathcal{E}_{\text{attack}}=\mathrm{topk}(\nabla_{\mathbf{A}},\delta)\] (8)

where \(\mathrm{topk}(\nabla_{\mathbf{A}},\delta)\) selects \(\delta\) entries with highest preference score in \(\nabla_{\mathbf{A}}\). Note that, if we only want to add edges without any deletion, all negative entries in \(\nabla_{\mathbf{A}}b\) should be zeroed out before computing Eq. (7). Likewise, if edges are only expected to be deleted, all positive entries should be zeroed out.

_Remarks._ Poisoning node feature matrix \(\mathbf{X}\) follows the same steps as poisoning adjacency matrix \(\mathbf{A}\) without applying Eq. (5).

**C - Overall framework.**Fate generally works as follows. (1) We first pre-train the surrogate graph learning model and get the corresponding learning model \(\Theta^{(T)}\) as well as the learning results \(\mathbf{Y}^{(T)}\). (2) Then we compute the meta gradient of the bias function using Eqs. (4) and (5). (3) Finally, we perform the discretized poisoning attack (Eqs. (7) and (8)) or continuous poisoning attack (Eq. (6)). A detailed pseudo-code of Fate is provided in Appendix B.

**D - Limitations.** Since Fate leverages the meta-gradient to poison the input graph, it requires the bias function \(b\left(\mathbf{Y},\Theta^{(T)},\mathbf{F}\right)\) to be differentiable in order to calculate the meta-gradient \(\nabla_{\mathcal{G}}b\). In Sections 4 and 5, we present a carefully chosen bias function for Fate. And we leave it for future work on exploring the ability of Fate in attacking other fairness definitions. Moreover, though the meta-gradient can be efficiently computed via auto-differentiation in many deep learning packages (e.g., PyTorch2, TensorFlow3), it requires \(O(n^{2})\) space complexity to store the meta-gradient when attacking fairness via edge flipping. It is still a challenging open problem on how to efficiently compute the meta-gradient in terms of space. One possible remedy for discretized attack might be a low-rank approximation on the perturbation matrix formed by \(\mathcal{E}_{\text{attack}}\). Since the difference between the benign graph and poisoned graph are often small and budgeted (\(d\left(\mathcal{G},\tilde{\mathcal{G}}\right)\leq B\)), it is likely that the edge manipulations may be around a few set of nodes, which makes the perturbation matrix to be an (approximately) low-rank matrix.

Footnote 2: https://pytorch.org/

Footnote 3: https://www.tensorflow.org/

## 4 Instantiation #1: Statistical Parity on Graph Neural Networks

Here, we instantiate Fate framework by attacking statistical parity on graph neural networks in a binary node classification problem with a binary sensitive attribute. We briefly discuss how to choose (1) the surrogate graph learning model used by the attacker, (2) the task-specific loss function in the lower-level optimization problem and (3) the bias function in the upper-level optimization problem.

**A - Surrogate graph learning model.** We assume that the surrogate model to be used by the attacker is a 2-layer linear GCN [47] with different hidden dimensions and model parameters at initialization.

**B - Lower-level loss function.** We consider a semi-supervised node classification task for the graph neural network to be attacked. Thus, the lower-level loss function is chosen as the cross entropy between the ground-truth label and the predicted label: \(l\left(\mathcal{G},\mathbf{Y},\Theta,\theta\right)=\frac{1}{|\mathcal{V}_{ \text{train}}|}\sum_{i\in\mathcal{V}_{\text{train}}}\sum_{j=1}^{c}y_{i,j}\ln \widehat{y}_{i,j}\), where \(\mathcal{V}_{\text{train}}\) is the set of training nodes with ground-truth labels with \(|\mathcal{V}_{\text{train}}|\) being its cardinality, \(c\) is the number of classes, \(y_{i,j}\) is a binary indicator of whether node \(i\) belongs to class \(j\) and \(\widehat{y}_{i,j}\) is the prediction probability of node \(i\) belonging to class \(j\).

**C - Upper-level bias function.** We aim to attack statistical parity in the upper-level problem, which asks for \(\mathrm{P}\left[\hat{y}=1\right]=\mathrm{P}\left[\hat{y}=1|s=1\right]\). Suppose \(p\left(\widehat{y}\right)\) is the probability density function (PDF) of \(\widehat{y}_{i,1}\)for any node \(i\) and \(p\left(\widehat{y}|s=1\right)\) is the PDF of \(\widehat{y}_{i,1}\) for any node \(i\) belong to the demographic group with sensitive attribute value \(s=1\). We observe that \(\mathrm{P}\left[\hat{y}=1\right]\) and \(\mathrm{P}\left[\hat{y}=1|s=1\right]\) are equivalent to the cumulative distribution functions (CDF) of \(p\left(\widehat{y}<\frac{1}{2}\right)\) and \(p\left(\widehat{y}<\frac{1}{2}|s=1\right)\), respectively. To estimate both \(\mathrm{P}\left[\hat{y}=1\right]\) and \(\mathrm{P}\left[\hat{y}=1|s=1\right]\) with a differentiable function, we first estimate their probability density functions (\(p\left(\widehat{y}<\frac{1}{2}\right)\) and \(p\left(\widehat{y}<\frac{1}{2}|s=1\right)\)) with kernel density estimation (KDE, Definition 1).

**Definition 1**: _(Kernel density estimation [7]) Given a set of n IID samples \(\left\{x_{1},\ldots,x_{n}\right\}\) drawn from a distribution with an unknown probability density function \(f\), the kernel density estimation of \(f\) at point \(\tau\) is defined as follows._

\[\widetilde{f}\left(\tau\right)=\frac{1}{na}\sum_{i=1}^{n}f_{k}\left(\frac{ \tau-x_{i}}{a}\right)\] (9)

_where \(\widetilde{f}\) is the estimated probability density function, \(f_{k}\) is the kernel function and \(a\) is a non-negative bandwidth._

Moreover, we assume the kernel function in KDE is the Gaussian kernel \(f_{k}\left(x\right)=\frac{1}{\sqrt{2\pi}}e^{-x^{2}/2}\). However, computing the CDF of a Gaussian distribution is non-trivial. Following [9], we leverage a tractable approximation of the Gaussian Q-function as follows.

\[Q(\tau)=F_{k}\left(\tau\right)=\int_{\tau}^{\infty}f_{k}(x)dx\approx e^{- \alpha\tau^{2}-\beta\tau-\gamma}\] (10)

where \(f_{k}(x)==\frac{1}{\sqrt{2\pi}}e^{-x^{2}/2}\) is a Gaussian distribution with zero mean, \(\alpha=0.4920\), \(\beta=0.2887\), \(\gamma=1.1893\)[30]. The overall workflow of estimating \(\mathrm{P}\left[\hat{y}=1\right]\) is as follows.

* For any node \(i\), get its prediction probability \(\widehat{y}_{i,1}\) with respect to class \(1\);
* Estimate the CDF \(\mathrm{P}\left[\hat{y}=1\right]\) using a Gaussian KDE with bandwidth \(a\) by \(\mathrm{P}\left[\hat{y}=1\right]=\frac{1}{n}\sum_{i=1}^{n}\exp\left(-\alpha \left(\frac{0.5-\widehat{y}_{i,1}}{a}\right)^{2}-\beta\left(\frac{0.5- \widehat{y}_{i,1}}{a}\right)-\gamma\right)\), where \(\alpha=0.4920\), \(\beta=0.2887\), \(\gamma=1.1893\) and \(\exp(x)=e^{x}\).

Note that \(\mathrm{P}\left[\hat{y}=1|s=1\right]\) can be estimated with a similar procedure with minor modifications. The only modifications needed are: (1) get the prediction probability of nodes with \(s=1\) and (2) compute the CDF using the Gaussian Q-function over nodes with \(s=1\) rather than all nodes in the graph.

## 5 Instantiation #2: Individual Fairness on Graph Neural Networks

We provide another instantiation of Fate framework by attacking individual fairness on graph neural networks. Here, we consider the same surrogate graph learning model (i.e., 2-layer linear GCN) and the same lower-level loss function (i.e., cross entropy) as described in Section 4. To attack individual fairness, we define the upper-level bias function following the principles in [20]: the fairness-related auxiliary information matrix \(\mathbf{F}\) is defined as the oracle symmetric pairwise node similarity matrix \(\mathbf{S}\) (i.e., \(\mathbf{F}=\mathbf{S}\)), where \(\mathbf{S}[i,j]\) measures the similarity between node \(i\) and node \(j\). Kang et al. [20] define that the overall individual bias to be \(\mathrm{Tr}\left(\mathbf{Y}^{T}\mathbf{L_{S}}\mathbf{Y}\right)\). Assuming that \(\mathbf{Y}\) is the output of an optimization-based graph learning model, \(\mathbf{Y}\) can be viewed as a function with respect to the input graph \(\mathcal{G}\), which makes \(\mathrm{Tr}\left(\mathbf{Y}^{T}\mathbf{L_{S}}\mathbf{Y}\right)\) differentiable with respect to \(\mathcal{G}\). Thus, the bias function \(b(\cdot)\) can be naturally defined as the overall individual bias of the input graph \(\mathcal{G}\), i.e., \(b\left(\mathbf{Y},\Theta^{*},\mathbf{S}\right)=\mathrm{Tr}\left(\mathbf{Y}^{ T}\mathbf{L_{S}}\mathbf{Y}\right)\).

## 6 Experiments

### Attacking Statistical Parity on Graph Neural Networks

**Settings.** We compare Fate with 4 baseline methods, i.e., Random, DICE [46], FA-GNN [19], under the same setting as in Section 4. That is, (1) the fairness definition to be attacked is statistical parity; (2) the downstream task is binary semi-supervised node classification with binary sensitive attributes. The experiments are conducted on 3 real-world datasets, i.e., Pokec-n, Pokec-z and Bail. Similar to existing works, we use the 50%/25%/25% splits for train/validation/test sets. For all baseline

[MISSING_PAGE_FAIL:7]

**Analysis on the manipulated edges.** Here, we aim to characterize the properties of edges that are flipped by Fate (i.e., Fate-flip) in attacking statistical parity. The reason to only analyze Fate-flip is that the majority of edges manipulated by Fate-flip on all three datasets is by addition (i.e., flipping from non-existing to existing). Figure 0(b) suggests that, if the two endpoints of an manipulated edge share the same class label or same sensitive attribute value, these two endpoints are most likely from the minority class and protected group. Combining Figures 0(a) and 0(b), Fate would significantly increase the number of edges that are incident to nodes in the minority class and/or protected group.

**More experimental results.** Due to the space limitation, we defer more experimental results on attacking statistical parity on graph neural networks in Appendix D. More specifically, we present the performance evaluation under different metrics, i.e., Macro F1 and AUC, as well as the effectiveness of Fate with a different victim model, i.e., FairGNN [11], which ensures statistical parity.

### Attacking Individual Fairness on Graph Neural Networks

**Settings.** To showcase the ability of Fate on attacking the individual fairness (Section 5), we further compare Fate with the same set of baseline methods (Random, DICE [46], FA-GNN [19]) on the same set of datasets (Pokec-n, Pokec-z, Bail). We follow the settings as in Section 5. We use the 50%/25%/25% splits for train/validation/test sets with GCN [26] being the victim model. For each dataset, we use a fixed random seed to learn the poisoned graph corresponding to each baseline method. Then we train the victim model 5 times with different random seeds. And each entry in the oracle pairwise node similarity matrix is computed by the cosine similarity of the corresponding rows in the adjacency matrix. That is, \(\mathbf{S}[i,j]=\cos{(\mathbf{A}[i,:],A[j,:])}\), where \(\cos{()}\) is the function to compute cosine similarity. For fair comparison, we only attack the adjacency matrix in all experiments. Please refer to Appendix C for detailed experimental settings.

**Main results.** Similarly, we test Fate with both edge flipping (Fate-flip in Table 2) and edge addition (Fate-add in Table 2), while all other baseline methods only add edges. From Table 2, we have two key observations. (1) Fate-flip and Fate-add are effective: they are the only methods that could consistently attack individual fairness whereas all other baseline methods mostly fail to attack individual fairness. (2) Fate-flip and Fate-add are deceptive: they achieve comparable or even better utility on all datasets compared with the utility on the benign graph. Hence, Fate framework is able to achieve effective and deceptive attacks to exacerbate individual bias.

**Effect of the perturbation rate.** From Table 2, we obtain similar observations as in Section 6.1 for Bail dataset. While for Pokec-n and Pokec-z, the correlation between the perturbation rate (Ptb.) and the individual bias is weaker. One possible reason is that: for Pokec-n and Pokec-z, the discrepancy between the oracle pairwise node similarity matrix and the benign graph is larger. Since the individual bias is computed using the oracle pairwise node similarity matrix rather than the benign/poisoned adjacency matrix, higher perturbation rate to poison the adjacency matrix may have less impact on the computation of individual bias.

**Analysis on the manipulated edges.** Similarly, since the majority of edges manipulated by Fate-flip is through addition, we only analyze Fate-flip here. From Figure 2, we can find out that Fate will manipulate edges from the same class (especially from the minority class). In this way, Fate would find edges that could increase individual bias and improve the utility of the minority class in order to make the fairness attack deceptive.

**More experimental results.** Due to the space limitation, we defer more experimental results on attacking individual fairness on graph neural networks in Appendix E. More specifically, we present the performance evaluation under different metrics, i.e., Macro F1 and AUC, as well as

Figure 2: Attacking individual fairness with Fate-flip. (a) Ratios of flipped edges that connect two nodes with same/different label. (b) Ratios of flipped edges whose two endpoints are both from the majority/minority class. Majority/minority classes are formed by splitting the training nodes based on their class labels.

[MISSING_PAGE_FAIL:9]

## References

* Agarwal et al. [2021] Chirag Agarwal, Himabindu Lakkaraju, and Marinka Zitnik. Towards a unified framework for fair and stable graph representation learning. In _Uncertainty in Artificial Intelligence_, pages 2114-2124. PMLR, 2021.
* Baumann et al. [2022] Joachim Baumann, Aniko Hannak, and Christoph Heitz. Enforcing group fairness in algorithmic decision making: Utility maximization under sufficiency. In _2022 ACM Conference on Fairness, Accountability, and Transparency_, pages 2315-2326, 2022.
* Bengio [2000] Yoshua Bengio. Gradient-based optimization of hyperparameters. _Neural computation_, 12(8):1889-1900, 2000.
* Bojchevski and Gunnemann [2019] Aleksandar Bojchevski and Stephan Gunnemann. Adversarial attacks on node embeddings via graph poisoning. In _International Conference on Machine Learning_, pages 695-704. PMLR, 2019.
* Bose and Hamilton [2019] Avishek Bose and William Hamilton. Compositional fairness constraints for graph embeddings. In _International Conference on Machine Learning_, pages 715-724. PMLR, 2019.
* Bureau [2022] Consumer Financial Protection Bureau. CFPB targets unfair discrimination in consumer finance. https://www.consumerfinance.gov/about-us/newsroom/cfpb-targets-unfair-discrimination-in-consumer-finance/, 2022. [Online; accessed 13-April-2023].
* Chen [2017] Yen-Chi Chen. A tutorial on kernel density estimation and recent advances. _Biostatistics & Epidemiology_, 1(1):161-187, 2017.
* Chhabra et al. [2021] Anshuman Chhabra, Adish Singla, and Prasant Mohapatra. Fairness degrading adversarial attacks against clustering algorithms. _arXiv preprint arXiv:2110.12020_, 2021.
* Cho et al. [2020] Jaewoong Cho, Gyeongjio Hwang, and Changho Suh. A fair classifier using kernel density estimation. _Advances in neural information processing systems_, 33:15088-15099, 2020.
* Choudhary et al. [2022] Manvi Choudhary, Charlotte Laclau, and Christine Largeron. A survey on fairness for machine learning on graphs. _arXiv preprint arXiv:2205.05396_, 2022.
* Dai and Wang [2021] Enyan Dai and Suhang Wang. Say no to the discrimination: Learning fair graph neural networks with limited sensitive attribute information. In _Proceedings of the 14th ACM International Conference on Web Search and Data Mining_, pages 680-688, 2021.
* Dai et al. [2018] Hanjun Dai, Hui Li, Tian Tian, Xin Huang, Lin Wang, Jun Zhu, and Le Song. Adversarial attack on graph structured data. In _International conference on machine learning_, pages 1115-1124. PMLR, 2018.
* Dong et al. [2021] Yushun Dong, Jian Kang, Hanghang Tong, and Jundong Li. Individual fairness for graph neural networks: A ranking based approach. In _Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining_, pages 300-310, 2021.
* Dong et al. [2022] Yushun Dong, Jing Ma, Chen Chen, and Jundong Li. Fairness in graph mining: A survey. _arXiv preprint arXiv:2204.09888_, 2022.
* Dwork et al. [2012] Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard Zemel. Fairness through awareness. In _Proceedings of the 3rd innovations in theoretical computer science conference_, pages 214-226, 2012.
* Feldman et al. [2015] Michael Feldman, Sorelle A Friedler, John Moeller, Carlos Scheidegger, and Suresh Venkata-subramanian. Certifying and removing disparate impact. In _proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and data mining_, pages 259-268, 2015.
* Finn et al. [2017] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In _International conference on machine learning_, pages 1126-1135. PMLR, 2017.

* [18] Moritz Hardt, Eric Price, and Nati Srebro. Equality of opportunity in supervised learning. _Advances in neural information processing systems_, 29, 2016.
* [19] Hussain Hussain, Meng Cao, Sandipan Sikdar, Denis Helic, Elisabeth Lex, Markus Strohmaier, and Roman Kern. Adversarial inter-group link injection degrades the fairness of graph neural networks. _arXiv preprint arXiv:2209.05957_, 2022.
* [20] Jian Kang, Jingrui He, Ross Maciejewski, and Hanghang Tong. Inform: Individual fairness on graph mining. In _Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining_, pages 379-389, 2020.
* [21] Jian Kang and Hanghang Tong. N2n: Network derivative mining. In _Proceedings of the 28th ACM International Conference on Information and Knowledge Management_, pages 861-870, 2019.
* [22] Jian Kang and Hanghang Tong. Fair graph mining. In _Proceedings of the 30th ACM International Conference on Information & Knowledge Management_, pages 4849-4852, 2021.
* [23] Jian Kang and Hanghang Tong. Algorithmic fairness on graphs: Methods and trends. In _Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining_, pages 4798-4799, 2022.
* [24] Jian Kang, Yan Zhu, Yinglong Xia, Jiebo Luo, and Hanghang Tong. Rawlsgcn: Towards rawlsian difference principle on graph convolutional network. In _Proceedings of the ACM Web Conference 2022_, pages 1214-1225, 2022.
* [25] Ahmad Khajehnejad, Moein Khajehnejad, Mahmoudreza Babaei, Krishna P Gummadi, Adrian Weller, and Baharan Mirzasoleiman. Crosswalk: Fairness-enhanced node representation learning. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 36, pages 11963-11970, 2022.
* [26] Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. In _International Conference on Learning Representations_, 2017.
* [27] Peizhao Li, Yifei Wang, Han Zhao, Pengyu Hong, and Hongfu Liu. On dyadic fairness: Exploring and mitigating bias in graph connections. In _International Conference on Learning Representations_, 2021.
* [28] Lydia T Liu, Sarah Dean, Esther Rolf, Max Simchowitz, and Moritz Hardt. Delayed impact of fair machine learning. In _International Conference on Machine Learning_, pages 3150-3158. PMLR, 2018.
* [29] Zemin Liu, Trung-Kien Nguyen, and Yuan Fang. On generalized degree fairness in graph neural networks. _arXiv preprint arXiv:2302.03881_, 2023.
* [30] Miguel Lopez-Benitez and Fernando Casadevall. Versatile, accurate, and analytically tractable approximation for the gaussian q-function. _IEEE Transactions on Communications_, 59(4):917-922, 2011.
* [31] Yao Ma, Suhang Wang, Tyler Derr, Lingfei Wu, and Jiliang Tang. Graph adversarial attack via rewiring. In _Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining_, pages 1161-1169, 2021.
* [32] Farzan Masrour, Tyler Wilson, Heng Yan, Pang-Ning Tan, and Abdol Esfahanian. Bursting the filter bubble: Fairness-aware network link prediction. In _Proceedings of the AAAI conference on artificial intelligence_, volume 34, pages 841-848, 2020.
* [33] Ninareh Mehrabi, Muhammad Naveed, Fred Morstatter, and Aram Galstyan. Exacerbating algorithmic bias through fairness attacks. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 35, pages 8930-8938, 2021.
* [34] Lawrence Page, Sergey Brin, Rajeev Motwani, and Terry Winograd. The pagerank citation ranking: Bringing order to the web. Technical report, Stanford InfoLab, 1999.

* [35] Flavien Prost, Hai Qian, Qiuwen Chen, Ed H Chi, Jilin Chen, and Alex Beutel. Toward a better trade-off between performance and fairness with kernel-based distribution matching. _arXiv preprint arXiv:1910.11779_, 2019.
* [36] Tahleen Rahman, Bartlomiej Surma, Michael Backes, and Yang Zhang. Fairwalk: Towards fair graph embedding. In _Proceedings of the 28th International Joint Conference on Artificial Intelligence_, pages 3289-3295, 2019.
* [37] Aida Rahmattalabi, Phebe Vayanos, Anthony Fulginiti, Eric Rice, Bryan Wilder, Amulya Yadav, and Milind Tambe. Exploring algorithmic fairness in robust graph covering problems. _Advances in Neural Information Processing Systems_, 32, 2019.
* [38] David Solans, Battista Biggio, and Carlos Castillo. Poisoning attacks on algorithmic fairness. In _Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2020, Ghent, Belgium, September 14-18, 2020, Proceedings, Part I_, pages 162-177. Springer, 2021.
* [39] Indro Spinelli, Simone Scardapane, Amir Hussain, and Aurelio Uncini. Fairdrop: Biased edge dropout for enhancing fairness in graph representation learning. _IEEE Transactions on Artificial Intelligence_, 3(3):344-354, 2021.
* [40] Mingjie Sun, Jian Tang, Huichen Li, Bo Li, Chaowei Xiao, Yao Chen, and Dawn Song. Data poisoning attack against unsupervised node embedding methods. _arXiv preprint arXiv:1810.12881_, 2018.
* [41] Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, and Qiaozhu Mei. Line: Large-scale information network embedding. In _Proceedings of the 24th international conference on world wide web_, pages 1067-1077, 2015.
* [42] Xianfeng Tang, Huaxiu Yao, Yiwei Sun, Yiqi Wang, Jiliang Tang, Charu Aggarwal, Prasenjit Mitra, and Suhang Wang. Investigating and mitigating degree-related biases in graph convolutional networks. In _Proceedings of the 29th ACM International Conference on Information & Knowledge Management_, pages 1435-1444, 2020.
* [43] Alan Tsang, Bryan Wilder, Eric Rice, Milind Tambe, and Yair Zick. Group-fairness in influence maximization. In _Proceedings of the 28th International Joint Conference on Artificial Intelligence_, pages 5997-6005, 2019.
* [44] Minh-Hao Van, Wei Du, Xintao Wu, and Aidong Lu. Poisoning attacks on fair machine learning. In _International Conference on Database Systems for Advanced Applications_, pages 370-386. Springer, 2022.
* [45] Daixin Wang, Jianbin Lin, Peng Cui, Quanhui Jia, Zhen Wang, Yanming Fang, Quan Yu, Jun Zhou, Shuang Yang, and Yuan Qi. A semi-supervised graph attentive network for financial fraud detection. In _2019 IEEE International Conference on Data Mining (ICDM)_, pages 598-607. IEEE, 2019.
* [46] Marcin Waniek, Tomasz P Michalak, Michael J Wooldridge, and Talal Rahwan. Hiding individuals and communities in a social network. _Nature Human Behaviour_, 2(2):139-147, 2018.
* [47] Felix Wu, Amauri Souza, Tianyi Zhang, Christopher Fifty, Tao Yu, and Kilian Weinberger. Simplifying graph convolutional networks. In _International conference on machine learning_, pages 6861-6871. PMLR, 2019.
* [48] Kaidi Xu, Hongge Chen, Sijia Liu, Pin-Yu Chen, Tsui-Wei Weng, Mingyi Hong, and Xue Lin. Topology attack and defense for graph neural networks: An optimization perspective. In _Proceedings of the 28th International Joint Conference on Artificial Intelligence_, pages 3961-3967, 2019.
* [49] Si Zhang, Dawei Zhou, Mehmet Yigit Yildirim, Scott Alcorn, Jingrui He, Hasan Davulcu, and Hanghang Tong. Hidden: Hierarchical dense subgraph detection with application to financial fraud detection. In _Proceedings of the 2017 SIAM International Conference on Data Mining_, pages 570-578. SIAM, 2017.

* [50] Wenbin Zhang, Jeremy C Weiss, Shuigeng Zhou, and Toby Walsh. Fairness amidst non-iid graph data: A literature review. _arXiv preprint arXiv:2202.07170_, 2022.
* [51] Daniel Zugner, Amir Akbarnejad, and Stephan Gunnemann. Adversarial attacks on neural networks for graph data. In _Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining_, pages 2847-2856, 2018.
* [52] Daniel Zugner and Stephan Gunnemann. Adversarial attacks on graph neural networks via meta learning. In _International Conference on Learning Representations_, 2019.