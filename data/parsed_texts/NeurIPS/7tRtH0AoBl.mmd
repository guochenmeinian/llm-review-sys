# Randomized Exploration for Reinforcement Learning with Multinomial Logistic Function Approximation

 Wooseong Cho

Seoul National University

Seoul, South Korea

wooseong_cho@snu.ac.kr

&Taehyun Hwang

Seoul National University

Seoul, South Korea

th.hwang@snu.ac.kr

&Joongkyu Lee

Seoul National University

Seoul, South Korea

jklee0717@snu.ac.kr

&Min-hwan Oh

Seoul National University

Seoul, South Korea

minoh@snu.ac.kr

Equal contributionCorresponding author

###### Abstract

We study reinforcement learning with _multinomial logistic_ (MNL) function approximation where the underlying transition probability kernel of the _Markov decision processes_ (MDPs) is parametrized by an unknown transition core with features of state and action. For the finite horizon episodic setting with inhomogeneous state transitions, we propose provably efficient algorithms with randomized exploration having frequentist regret guarantees. For our first algorithm, RRL-MNL, we adapt optimistic sampling to ensure the optimism of the estimated value function with sufficient frequency. We establish that RRL-MNL achieves a \(\widetilde{\mathcal{O}}(\kappa^{-1}d^{\frac{3}{2}}H^{\frac{3}{2}}\sqrt{T})\) frequentist regret bound with constant-time computational cost per episode. Here, \(d\) is the dimension of the transition core, \(H\) is the horizon length, \(T\) is the total number of steps, and \(\kappa\) is a problem-dependent constant. Despite the simplicity and practicality of RRL-MNL, its regret bound scales with \(\kappa^{-1}\), which is potentially large in the worst case. To improve the dependence on \(\kappa^{-1}\), we propose ORRL-MNL, which estimates the value function using the local gradient information of the MNL transition model. We show that its frequentist regret bound is \(\widetilde{\mathcal{O}}(d^{\frac{3}{2}}H^{\frac{3}{2}}\sqrt{T}+\kappa^{-1}d^{ 2}H^{2})\). To the best of our knowledge, these are the first randomized RL algorithms for the MNL transition model that achieve statistical guarantees with constant-time computational cost per episode. Numerical experiments demonstrate the superior performance of the proposed algorithms.

## 1 Introduction

_Reinforcement learning_ (RL) is a sequential decision-making problem in which an agent tries to maximize its expected cumulative reward by interacting with an unknown environment over time. Despite significant empirical progress in RL algorithms for various applications [47, 52, 65, 66, 25], the theoretical understanding of RL algorithms had long been limited to tabular methods [40, 56, 10, 77, 79], which explicitly enumerate the entire state and action spaces and learn the value (or the policy) for each state and action. Recently, there has been an increasing body of research in RL with function approximation to extend beyond the tabular problem setting. In particular, _linear function approximation_ has served as a foundational model [43, 73, 22, 9, 37]. On the other hand,the linear transition model assumption poses significant constraints: 1) the output of the function must be within \([0,1]\), and 2) the sum of the probabilities for all possible next states must be exactly 1. These constraints make it challenging to apply RL with linear function approximation to real-world applications [35]. To overcome such challenges, there has been literature on RL with general function approximation [21, 28, 37, 44, 4, 18]. Despite the guarantee of sample efficiency achieved by their algorithms, this accomplishment might be impeded by computational intractability or the necessity to rely on stronger assumptions. As a result, the resulting methods may not be as general or practical.

On the other hand, Hwang and Oh [35] introduce specific non-linear parametric MDPs called MNL-MDPs (Assumption 1) where the transition probability of MDPs is given by an MNL model. They consider an _upper confidence bound_ (UCB) approach to balance exploration and exploitation. Since it is costly or even intractable to compute UCB explicitly, randomized exploration methods such as _Thompson Sampling_ (TS) are widely studied in RL with linear function approximation as well as tabular MDPs. This is because, in various decision-making problems ranging from multi-armed bandits to RL, randomized exploration algorithms have been shown to perform better than UCB methods in empirical evaluations [16, 57, 64, 49]. Furthermore, randomized exploration can be easily integrated with linear function approximation. This is because the value function in linear MDPs can be linearly parameterized, allowing perturbations of the estimator to directly control the perturbations of the value function. However, although there has been some literature aiming to propose randomized algorithms for general function classes [37, 4, 5, 75], these methods do not discuss how to define the posterior distribution supported by the given function class and how to draw the optimistic sample from the posterior [4, 5, 75], or they require stronger assumptions on stochastic optimism [37], which is one of the most challenging elements in frequentist regret analysis. Thus, the design of a tractable randomized exploration RL algorithm and the feasibility of frequentist regret analysis for randomized exploration remain open challenges. Hence, the following question arises:

_Can we design a provably efficient and tractable randomized algorithm for RL with MNL function approximation?_

We answer the above question by proposing the first randomized algorithm, RRL-MNL, achieving \(\widetilde{\mathcal{O}}(\kappa^{-1}d^{\frac{3}{2}}H^{\frac{3}{2}}\sqrt{T})\) frequentist regret with constant-time computational cost per episode. RRL-MNL is not only the first algorithm with randomized exploration for MNL-MDPs, but also, to the best of our knowledge, it provides the first frequentist regret analysis for a _non-linear model-based_ algorithm with randomized exploration without assuming stochastic optimism [37].

While RRL-MNL is _statistically_ efficient, the current method used to analyze the regret of MNL function approximation introduces a problem-dependent constant \(\kappa\) (Assumption 4), which reflects the level of non-linearity of the MNL transition model. This constant \(\kappa\) originates from the use of generalized linear models (GLMs) for contextual bandit settings [26, 51, 45] and MNL bandit settings [54, 17, 55]. The magnitude of the constant \(\kappa\) can be exponentially small with respect to the size of the decision set, hence the regret bound scaling with \(\kappa^{-1}\) could be prohibitively large in the worst case [23]. Worse yet, the situation is even more challenging in RL, as in the worst case, \(\kappa^{-1}\) can be much larger than in the case of bandits. To overcome the prohibitive dependence on \(\kappa\), algorithms based on new Bernstein-like inequalities and the self-concordant-like property of the log-loss have been proposed for logistic bandits [23, 3, 24] and for MNL bandits [61, 6, 50]. As an extension of these works, the following fundamental question remains open:

_Is it possible for RL algorithms with MNL function approximation to have a sharper dependence on the problem-dependent constant \(\kappa\)?_

For the above question, we propose the second randomized algorithm referred to as ORRL-MNL, which establishes a regret bound of \(\widetilde{\mathcal{O}}(d^{\frac{3}{2}}H^{\frac{3}{2}}\sqrt{T}+\kappa^{-1}d^ {2}H^{2})\) with constant-time computational cost per episode. We summarize our main contributions as follows:

* We propose computationally tractable randomized algorithms for RL with MNL function approximation: RRL-MNL and ORRL-MNL. To the best of our knowledge, these are the first randomized model-based RL algorithms with MNL function approximation that achieve the frequentist regret bounds with constant-time computational cost per episode.
* We establish that RRL-MNL enjoys \(\widetilde{\mathcal{O}}(\kappa^{-1}d^{\frac{3}{2}}H^{\frac{3}{2}}\sqrt{T})\) frequentist regret bound with constant-time computational cost per episode, where \(d\) is the dimension of the transition core, \(H\) is horizon length, \(T\) is the total number of rounds, and \(\kappa\) is a problem-dependent constant. Wederive the stochastic optimism of RRL-MNL, and to our knowledge, this is the first frequentist regret analysis for a non-linear model-based algorithm with randomized exploration without assuming stochastic optimism.
* To achieve a regret bound with improved dependence on \(\kappa\), we introduce \(\mathtt{ORRL}\)-MNL, which constructs the optimistic randomized value functions by taking into account the effects of the local gradient information for the MNL transition model at each reachable state. We prove that \(\mathtt{ORRL}\)-MNL enjoys an \(\widetilde{\mathcal{O}}(d^{\frac{3}{2}}H^{\frac{2}{3}}\sqrt{T}+\kappa^{-1}d^ {2}H^{2})\) regret with constant-time computational cost per episode, significantly improving the regret of RRL-MNL without requiring prior knowledge of \(\kappa\).
* We evaluate our algorithms on tabular MDPs and demonstrate the superior performance of our proposed algorithms compared to the existing state-of-the-art MNL-MDP algorithm [35]. The experiments provide evidence that our proposed algorithms are both computationally and statistically efficient.

Related works on RL with function approximation and MNL contextual bandits are provided in Appendix A.

## 2 Problem Setting

We consider the episodic _Markov decision processes_ (MDPs) denoted by \(\mathcal{M}(\mathcal{S},\mathcal{A},H,\{P\}_{h=1}^{H},r)\), where \(\mathcal{S}\) is the state space, \(\mathcal{A}\) is the action space, \(H\) is the horizon length of each episode, \(\{P\}_{h=1}^{H}\) is the collection of probability distributions, and \(r\) is the reward function. Every episodes start from the initial state \(s_{1}\) and for every step \(h\in[H]:=\{1,...,H\}\) in an episode, the learning agent interacts with the environment represented as \(\mathcal{M}\). The agent observes the state \(s_{h}\in\mathcal{S}\), chooses an action \(a_{h}\in\mathcal{A}\), receives a reward \(r(s_{h},a_{h})\in[0,1]\) and the next state \(s_{h+1}\) is given by the transition probability distribution \(P_{h}(\cdot|s_{h},a_{h})\). Then this process is repeated throughout the episode. A policy \(\pi:\mathcal{S}\times[H]\rightarrow\mathcal{A}\) is a function that determines the action of the agent at state \(s_{h}\), i.e., \(a_{h}=\pi(s_{h},h):=\pi_{h}(s_{h})\).

We define the value function of the policy \(\pi\), denoted by \(V_{h}^{\pi}(s)\), as the expected sum of rewards under the policy \(\pi\) until the end of the episode starting from \(s_{h}=s\), i.e., \(V_{h}^{\pi}(s)=\mathbb{E}_{\pi}\left[\sum_{h^{\prime}=h}^{H}r(s_{h^{\prime}}, \pi_{h^{\prime}}(s_{h^{\prime}}))\mid s_{h}=s\right]\). Similarly, we define the action-value function \(Q_{h}^{\pi}(s,a)=r(s,a)+\mathbb{E}_{s^{\prime}\sim P_{h}(\cdot|s,a)}\left[V_{h +1}^{\pi}(s^{\prime})\right]\). We define an optimal policy \(\pi^{*}\) to be a policy that achieves the highest possible value at every \((s,h)\in\mathcal{S}\times[H]\). We denote the optimal value function by \(V_{h}^{*}(s)=V_{h}^{*^{*}}(s)\) and the optimal action-value function by \(Q_{h}^{*}(s,a)=Q_{h}^{\pi^{*}}(s,a)\). To simplify, we introduce the notation \(P_{h}V_{h+1}(s,a)=\mathbb{E}_{s^{\prime}\sim P_{h}(\cdot|s,a)}[V_{h+1}(s^{ \prime})]\). Recall that the Bellman equations are,

\[Q_{h}^{\pi}(s,a)=r(s,a)+P_{h}V_{h+1}^{\pi}(s,a)\,,\quad Q_{h}^{*}(s,a)=r(s,a)+ P_{h}V_{h+1}^{*}(s,a)\,,\]

where \(V_{H+1}^{\pi}(s)=V_{H+1}^{*}(s)=0\) and \(V_{h}^{*}(s)=\max_{a\in\mathcal{A}}Q_{h}^{*}(s,a)\) for all \(s\in\mathcal{S}\).

The goal of the agent is to maximize the sum of rewards for K episodes. In other words, the goal is to minimize the cumulative regret of the policy \(\pi\) over K episodes where \(\pi=\{\pi^{k}\}_{k=1}^{K}\) is a collection of policies \(\pi^{k}\) at k-th episode. The regret is defined as

\[\mathbf{Regret}_{\pi}(K):=\sum_{k=1}^{K}(V_{1}^{*}-V_{1}^{\pi^{k}})(s_{1}^{k})\]

where \(s_{1}^{k}\) is the initial state at the \(k\)-th episode.

### Multinomial Logistic Markov Decision Processes (MNL-MDPs)

Even though a lot of provable RL algorithms for linear MDPs are proposed, there is a simple but fundamental problem with the linear transition model assumption on the linear MDPs. In other words, the output of a linear function approximating the transition model must be in \([0,1]\) and the probability of all possible following states must sum to \(1\) exactly. Such restrictive assumption can affect the regret performances of algorithm suggested under the linearity assumption. To resolve these challenges, Hwang and Oh [35] propose a setting of a _multinomial logistic Markov decision processes_ (MNL-MDPs), where the state transition model is given by a multinomial logistic model. We introduce the formal definition for MNL-MDP as follows:

**Assumption 1** (MNL-MDPs [35]).: _An MDP \(\mathcal{M}(\mathcal{S},\mathcal{A},H,\{P_{h}\}_{h=1}^{H},r)\) is an MNL-MDP with a feature map \(\bm{\varphi}:\mathcal{S}\times\mathcal{A}\times\mathcal{S}\to\mathbb{R}^{d}\), if for each \(h\in[H]\), there exists \(\bm{\theta}_{h}^{*}\in\mathbb{R}^{d}\), such that for any \((s,a)\in\mathcal{S}\times\mathcal{A}\) and \(s^{\prime}\in\mathcal{S}_{s,a}:=\{s^{\prime}\in\mathcal{S}:\mathbb{P}(s^{ \prime}\mid s,a)\neq 0\}\), the state transition kernel of \(s^{\prime}\) when an action \(a\) is taken at a state \(s\) is given by,_

\[P_{h}(s^{\prime}\mid s,a)=\frac{\exp(\bm{\varphi}(s,a,s^{\prime})^{\top}\bm{ \theta}_{h}^{*})}{\sum_{\widetilde{s}\in\mathcal{S}_{s,a}}\exp(\bm{\varphi}(s,a,\widetilde{s})^{\top}\bm{\theta}_{h}^{*})}\,.\] (1)

_We call each unknown vector \(\bm{\theta}_{h}^{*}\) transition core. Furthermore, we denote the maximum cardinality of the set of reachable states as \(\mathcal{U}\), i.e., \(\mathcal{U}:=\max_{s,a}|\mathcal{S}_{s,a}|\)._

**Remark 1**.: _While Hwang and Oh [35] assume a homogeneous transition kernel, we assume an inhomogeneous transition kernel, in which the probability varies depending on the current time step \(h\) even for the same state transition, which is a more general setting. Also, for notational simplicity, we denote the true transition kernel \(P_{h}\) as \(P_{\bm{\theta}_{h}^{*}}\), and the estimated transition kernel by \(\bm{\theta}\) as \(P_{\bm{\theta}}\)._

### Assumptions

We introduce some standard regularity assumptions.

**Assumption 2** (Boundedness).: _We assume \(\|\bm{\varphi}(s,a,s^{\prime})\|_{2}\leq L_{\bm{\varphi}}\) for all \((s,a,s^{\prime})\in\mathcal{S}\times\mathcal{A}\times\mathcal{S}_{s,a}\), and \(\|\bm{\theta}_{h}^{*}\|_{2}\leq L_{\bm{\theta}}\) for all \(h\in[H]\)._

**Assumption 3** (Known reward).: _We assume that the reward function \(r\) is known to the agent._

**Assumption 4** (Problem-dependent constant).: _Let \(\mathcal{B}_{d}(L_{\bm{\theta}}):=\{\bm{\theta}\in\mathbb{R}^{d}:\|\bm{ \theta}\|_{2}\leq L_{\bm{\theta}}\}\). There exists \(\kappa>0\) such that for any \((s,a)\in\mathcal{S}\times\mathcal{A}\) and \(s^{\prime},\widetilde{s}\in\mathcal{S}_{s,a}\) with \(s^{\prime}\neq\widetilde{s}\),_

\[\inf_{\bm{\theta}\in\mathcal{B}_{d}(L_{\bm{\theta}})}P_{\bm{\theta}}(s^{\prime }\mid s,a)P_{\bm{\theta}}(\widetilde{s}\mid s,a)\geq\kappa\,.\]

Discussion of assumptionsAssumption 2 is common in the literature on RL with function approximation [43; 72; 73; 37; 35] to make the regret bounds scale-free. Assumption 3 is used to focus on the main challenge of model-based RL that learning about \(P\) of the environment is more difficult than learning \(r\). In the model-based RL literature [71; 9; 72; 81; 35], the known reward \(r\) assumption is widely used. Assumption 4 is typical in generalized linear contextual bandit [26; 51; 23; 3; 24] and MNL contextual bandit literature [54; 8; 55; 61; 6; 76; 50] to guarantee non-singular Fisher information matrix.

## 3 Randomized Algorithm for MNL-MDPs having constant-time computational cost

Previous work for MNL-MDPs [35] proposed a UCB-based exploration algorithm. Constructing a UCB-based optimistic value function is not only computationally intractable but also tends to overly optimistically estimate the true optimal value function. Additionally, their algorithm incurs increasing computation costs as episodes progress, as it requires all samples from the previous episode to estimate the transition core. In this section, we present a novel model-based RL algorithm that incorporates _randomized exploration_ and _online parameter estimation_ for MNL-MDPs.

### Algorithm: Rrl-Mnl

Online transition core estimationWhile Hwang and Oh [35] estimate the transition core using maximum likelihood estimation over all samples from previous episodes, we employ an efficient online parameter estimation method by exploiting the particular structure of the MNL transition model. The key insight is that the negative log-likelihood function for the MNL model in each episode is strongly convex over a bounded domain. This property allows us to utilize a variation of the online Newton step [30; 31], which inspired online algorithms for logistic bandits [74] and MNL contextual bandits [55]. Specifically, for \((k,h)\in[K]\times[H]\), we define the response variable \(\left[y_{h}^{k}(s^{\prime})\right]_{s^{\prime}\in\mathcal{S}_{k,h}}\) such that \(y_{h}^{k}(s^{\prime})=\mathds{1}(s_{h+1}^{k}=s^{\prime})\) for \(s^{\prime}\in\mathcal{S}_{k,h}:=\mathcal{S}_{s_{h}^{k},a_{h}^{k}}\). Then, \(y_{h}^{k}\) is sampled from the following multinomial distribution: \(y_{h}^{k}\sim\mathrm{multinomial}(1,\left[P_{\boldsymbol{\theta}_{h}^{*}}(s^{ \prime}\mid s_{h}^{k},a_{h}^{k})\right]_{s^{\prime}\in\mathcal{S}_{k,h}})\), where \(1\) represents that \(y_{h}^{k}\) is a single-trial sample. We define the per-episode loss \(\ell_{k,h}(\boldsymbol{\theta})\) as follows:

\[\ell_{k,h}(\boldsymbol{\theta}):=-\sum_{s^{\prime}\in\mathcal{S}_{k,h}}y_{h}^ {k}(s^{\prime})\log P_{\boldsymbol{\theta}}(s^{\prime}\mid s_{h}^{k},a_{h}^{k} )\,.\]

Then, the estimated transition core for \(\boldsymbol{\theta}_{h}^{*}\) is given by

\[\boldsymbol{\theta}_{h}^{k}=\operatorname*{argmin}_{\boldsymbol{\theta}\in \mathcal{B}_{d}(L_{\boldsymbol{\theta}})}\frac{1}{2}\|\boldsymbol{\theta}- \boldsymbol{\theta}_{h}^{k-1}\|^{2}_{\mathbf{A}_{k,h}}+(\boldsymbol{\theta}- \boldsymbol{\theta}_{h}^{k-1})^{\top}\nabla\ell_{k-1,h}(\boldsymbol{\theta}_{ h}^{k-1})\,,\] (2)

where \(\boldsymbol{\theta}_{h}^{1}\) can be initialized as any point in \(\mathcal{B}_{d}(L_{\boldsymbol{\theta}})\) and \(\mathbf{A}_{k,h}\) is the Gram matrix defined by

\[\mathbf{A}_{k,h}:=\lambda\mathbf{I}_{d}+\frac{\kappa}{2}\sum_{i=1}^{k-1}\sum_ {s^{\prime}\in\mathcal{S}_{i,h}}\boldsymbol{\varphi}(s_{h}^{i},a_{h}^{i},s^{ \prime})\boldsymbol{\varphi}(s_{h}^{i},a_{h}^{i},s^{\prime})^{\top}\,.\] (3)

Stochastically optimistic value functionFirst of all, we introduce the key challenges of regret analysis for randomized algorithms, explain how previous works have overcome these challenges, and then describe why the techniques from previous works cannot be applied to MNL-MDPs. Ensuring that the estimated value function is optimistic with sufficient frequency is a crucial challenge in analyzing the frequentist regret of randomized algorithms. A common way to promote sufficient exploration in randomized algorithms is by perturbing the estimated value function or by performing posterior sampling in the transition model class. Frequentist regret analysis of randomized exploration in an RL setting has been conducted for tabular [59, 7, 62, 60, 67], linear MDPs [73, 37], and general function classes [37, 4, 5, 75]. In the case of linear MDPs [73, 37], since the property that the action-value function is linear in the feature map allows perturbing the estimated parameter directly to control the perturbation of the estimated value function. Also, even though Ishfaq et al. [37] presented a randomized algorithm for the general function class using eluder dimension, they assume stochastic optimism (anti-concentration), which is in fact one of the most challenging aspects of frequentist analysis. Other posterior sampling algorithms in RL for the general function class such as [4, 5, 75], except for very limited examples, do not discuss how to define the posterior distribution supported by the given function class and how to draw the optimistic sample from the posterior. That is why even after there exists a so-called _general function class_-based result, it is often the case that results in specific parametric models are still needed.

Note that in episodic RL, the perturbed estimated value functions are propagated back through horizontal steps, requiring careful adjustment of the perturbation scheme to maintain a sufficient probability of optimism without decaying too quickly with the horizon. For example, if the probability of the estimated value function being optimistic at horizon \(h\) is denoted as \(p\), this would result in the probability that the estimated value function in the initial state is optimistic being on the order of \(p^{H}\), implying that the regret can increase exponentially with the length of the horizon \(H\)Additionally, the non-linearity and substitution effect of the next state transition in the MNL-MDPs make applying the existing TS techniques infeasible to guarantee optimism in MNL-MDPs with sufficient frequency. Instead, we design the _stochastically optimistic value function_ by exploiting the structure of the MNL transition model. In other words, the prediction error of MNL transition model (Definition 1) can be bounded by the weighted norm of the dominant feature \(\hat{\bm{\varphi}}\) (Lemma 4). Based on such dominant feature, we perturb the estimated value function by injecting Gaussian noise whose variance is proportional to the inverse of the Gram matrix to encourage the perturbation with higher variance in less explored directions. To guarantee the optimism with fixed probability, we adapt optimistic sampling technique [7, 54, 37, 36]. For each \(m\in[M]\), sample _i.i.d._ Gaussian noise vector \(\bm{\xi}_{k,h}^{(m)}\sim\mathcal{N}(\mathbf{0}_{d},\sigma_{k}^{2}\mathbf{A}_{ k,h}^{-1})\) where \(\sigma_{k}\) is an exploration parameter, and add the most optimistic inner product value \(\max_{m\in[M]}\hat{\bm{\varphi}}_{k,h}(s,a)^{\top}\bm{\xi}_{k,h}^{(m)}\) to the estimated value function. To summarize for any \((s,a)\in\mathcal{S}\times\mathcal{A}\), set \(Q_{H+1}^{k}(s,a)=0\) and for \(h\in[H]\),

\[Q_{h}^{k}(s,a)=\min\left\{r(s,a)+\sum_{s^{\prime}\in\mathcal{S}_{s,a}}P_{\bm{ \theta}_{h}^{k}}(s^{\prime}\mid s,a)V_{h+1}^{k}(s^{\prime})+\max_{m\in[M]} \hat{\bm{\varphi}}_{k,h}(s,a)^{\top}\bm{\xi}_{k,h}^{(m)},H\right\},\] (4)

where \(V_{h}^{k}(s)=\max_{a^{\prime}}Q_{h}^{k}(s,a^{\prime})\) and \(\hat{\bm{\varphi}}_{k,h}(s,a):=\bm{\varphi}(s,a,\hat{s})\) for \(\hat{s}=\operatorname*{argmax}_{s^{\prime}\in\mathcal{S}_{s,a}}\|\bm{\varphi} (s,a,s^{\prime})\|_{\bm{\Lambda}_{k,h}^{-1}}\). Based on these stochastically optimistic value function, the agent plays a greedy action \(a_{h}^{k}=\operatorname*{argmax}_{a^{\prime}}Q_{h}^{k}(s_{h}^{k},a^{\prime})\). We layout the procedure in Algorithm 1.

**Remark 2**.: _Note that \(\mathtt{RRL}\)-\(\mathtt{MNL}\) only requires constant-time computational cost and storage cost per episode, as it does not require storing all samples from previous episodes, and the Gram matrix \(\mathbf{A}_{k,h}\) can be updated incrementally._

### Regret bound of \(\mathtt{RRL}\)-\(\mathtt{MNL}\)

We present the regret upper bound of \(\mathtt{RRL}\)-\(\mathtt{MNL}\). The complete proof is deferred to Appendix C.

**Theorem 1** (Regret Bound of \(\mathtt{RRL}\)-\(\mathtt{MNL}\)).: _Suppose that Assumption 1- 4 hold. For any \(0<\delta<\frac{\Phi(-1)}{2}\), if we set the input parameters in Algorithm 1 as \(\lambda=L_{\bm{\varphi}}^{2},\sigma_{k}=\widetilde{\mathcal{O}}(H\sqrt{d})\) and \(M=\lceil 1-\frac{\log H}{\log\Phi(1)}\rceil\) where \(\Phi\) is the normal CDF, then with probability at least \(1-\delta\), the cumulative regret of the \(\mathtt{RRL}\)-\(\mathtt{MNL}\) policy \(\pi\) is upper-bounded as follows:_

\[\text{Regret}_{\pi}(K)=\widetilde{\mathcal{O}}\left(\kappa^{-1}d^{\frac{3}{2}} H^{\frac{3}{2}}\sqrt{T}\right),\]

_where \(T=KH\) is the total number of steps._

Discussion of Theorem 1To our best knowledge, this is the first result to provide a frequentist regret bound for the MNL-MDPs. Among the previous RL algorithms using function approximation, the most comparable techniques to our method are _model-free_ algorithms with randomized exploration [73, 37]. To guarantee stochastic optimism, Zanette et al. [73] established a lower bound on the difference between the estimated value and the optimal value by the summation of linear terms with respect to the average feature (Lemma F.1 in [73]). This property is achievable due to the linear expression of the value function in linear MDPs. Instead, we established a lower bound on the difference between value functions by the summation of the Bellman errors (Definition 1) along the sample path obtained through the optimal policy (Lemma 7). Hence, our analysis significantly differs from that of Zanette et al. [73] since the value function in MNL-MDPs is no longer linearly parametrized, and there is no closed-form expression for it.

Compared to [37], they also used an optimistic sampling technique; however, our theoretical sampling size \(M=\mathcal{O}(\log H)\) is much tighter than that of [37], i.e., \(\mathcal{O}(d)\) for the linear function class, \(\mathcal{O}(\log(T|\mathcal{S}||\mathcal{A}|))\) for the general function class. While Ishfaq et al. [37] extend the results of the linear function class to general function class under the assumption of stochastic optimism (Assumption C in [37]), we provide the frequentist regret analysis for a _non-linear model-based_ algorithm with randomized exploration _without assuming stochastic optimism_.

Compared to the optimistic exploration algorithm for MNL-MDPs [35], our randomized exploration requires a more involved proof technique to ensure that the perturbation of the estimated value function has enough variance to maintain optimism with sufficient frequency (Lemma 6). As a result,the established regret of RRL-MNL differs by a factor of \(\sqrt{d}\), which aligns with the difference in the existing bounds of linear bandits between a TS-based algorithm [2] and a UCB-based algorithm [1]. Additionally, we achieve statistical efficiency for the _inhomogeneous transition model_, which is a more general setting than that of Hwang and Oh [35]. Our computation cost per episode is \(\mathcal{O}(1)\) while the computation cost per episode of Hwang and Oh [35] is \(\mathcal{O}(K)\).

Proof Sketch of Theorem 1We provide the proof sketch of Theorem 1. By decomposing the regret into the estimation part and the pessimism part, we have

\[\sum_{k=1}^{K}(V_{1}^{*}-V_{1}^{\pi_{k}})(s_{1}^{k})=\sum_{k=1}^{K}\Big{(} \underbrace{V_{1}^{*}-V_{1}^{k}}_{\text{Pessim}}+\underbrace{V_{1}^{k}-V_{1 }^{\pi_{k}}}_{\text{Estimation}}\Big{)}(s_{1}^{k})\,.\]

We bound these two parts separately. For the estimation part, for each \(k\in[K],h\in[H]\), we first show that the online estimated transition core \(\bm{\theta}_{h}^{k}\) (2) concentrates around the unknown transition core parameter \(\bm{\theta}_{h}^{*}\) with high probability (Lemma 1). Then, we show that the prediction error induced by the estimated transition core can be bounded by the weighted norm of the dominant feature \(\hat{\bm{\varphi}}\), multiplied by the confidence radius of the estimated transition core (Lemma 4). The bounded prediction error, together with the concentration of Gaussian noise, implies the desired bound on the estimation part (Lemma 10). For the pessimism part, we first show that the stochastically optimistic value function \(V_{1}^{k}\) is optimistic than the true optimal value function \(V_{1}^{*}\) with sufficient frequency (Lemma 6). In the next step, we show that the pessimism part is upper bounded by a bound of the estimation part times the inverse probability of being optimistic (Lemma 11). Combining all the results, we can conclude the proof. Refer to Appendix C for detailed proofs.

## 4 Statistically Improved Algorithm for MNL-MDPs

Although RRL-MNL is provably efficient and achieves constant-time computational cost per episode, the current analysis makes its regret bound scale with \(\kappa^{-1}\). Recall that the problem-dependent constant \(\kappa\) introduced in Assumption 4 indicates the curvature of the MNL function, i.e., how difficult it is to learn the true transition core parameter. It is required to ensure the non-singular Fisher information matrix, hence is typically used in GLM or MNL bandit algorithms that use the maximum likelihood estimator. As introduced in Faury et al. [23], \(\kappa^{-1}\) can be exponentially large in the worst case. The appearance of \(\kappa\) in existing bounds originates in the connection between the difference of estimators and the difference of gradients of negative log-likelihood, usually denoted as \(\mathbf{G}\) in Filippi et al. [26]. Without considering local information at all, using a loose lower bound for \(\mathbf{G}\) incurs \(\kappa^{-1}\) in regret bound (see Section 4.1 in Agrawal et al. [6]). Recently, improved dependence on \(\kappa\) has been achieved in bandit literature [23; 3; 61; 6; 76; 50] through the use of generalization of the Bernstein-like tail inequality [23] and the self-concordant-like property of the log loss [11]. However, a direct adaptation of the MNL bandit technique would result in sub-optimal dependence on the assortment size in MNL bandit, which corresponds to the size of the set of reachable states, such as \(\mathcal{U}\). In this section, we introduce a new randomized algorithm for MNL-MDPs, equipped with a tight online parameter estimation and feature centralization technique that achieves a regret bound with improved dependence on \(\kappa\) and \(\mathcal{U}\).

### Algorithms: Orrl-Mnl

Tight online transition core estimationZhang and Sugiyama [76] presented a jointly efficient UCB-based MNL contextual bandit algorithm using online mirror descent algorithm. Adapting the update rule from [76], the estimated transition core run by the online mirror descent is given by

\[\widetilde{\bm{\theta}}_{h}^{k+1}=\operatorname*{argmin}_{\bm{\theta}\in \mathcal{B}_{d}(L_{\bm{\theta}})}\frac{1}{2\eta}\|\bm{\theta}-\widetilde{\bm {\theta}}_{h}^{k}\|_{\widetilde{\mathbf{B}}_{h,h}}^{2}+\bm{\theta}^{\top} \nabla\ell_{k,h}(\widetilde{\bm{\theta}}_{h}^{k})\,,\] (5)

where \(\widetilde{\bm{\theta}}_{h}^{1}\) can be initialized as any point in \(\mathcal{B}_{d}(L_{\bm{\theta}})\), \(\eta\) is a step size, and \(\widetilde{\mathbf{B}}_{k,h}\) is defined as

\[\widetilde{\mathbf{B}}_{k,h}:=\mathbf{B}_{k,h}+\eta\nabla^{2}\ell_{k,h}( \widetilde{\bm{\theta}}_{h}^{k})\,,\quad\mathbf{B}_{k,h}:=\lambda\mathbf{I}_{ d}+\sum_{i=1}^{k-1}\nabla^{2}\ell_{i,h}(\widetilde{\bm{\theta}}_{h}^{i+1})\,.\] (6)Note that the MNL model in Zhang and Sugiyama [76] operates in a _multiple-parameter_ setting, where there are multiple unknown choice parameters and one given context feature. In contrast, our MNL model operates in a _single-parameter_ setting, where there is one unknown transition core and features for up to \(\mathcal{U}\) reachable states. This difference results in variations in applying the self-concordant-like property of the log-loss for the MNL model. For instance, Zhang and Sugiyama [76] utilized the fact that the log-loss for the multiple parameter MNL model is \(\sqrt{6}\)-self-concordant-like (Lemma 2 in Zhang and Sugiyama [76]). On the other hand, Lee and Oh [50] revisit the self-concordant-like property and demonstrate that the log-loss of the single-parameter MNL model is \(3\sqrt{2}\)-self-concordant-like (Proposition B.1 in Lee and Oh [50]). This results in a concentration bound that is independent of \(\kappa\) and \(\mathcal{U}\), introduced in Lemma 12.

**Remark 3**.: _Note that the online estimated parameters \(\bm{\theta}_{h}^{k}\) (2) and \(\widetilde{\bm{\theta}}_{h}^{k}\) (5) do not aim to minimize the sum of negative log-likelihoods, \(\sum_{k^{\prime}=1}^{k}\ell_{k^{\prime},h}(\bm{\theta})\). Instead, we show that the online estimated parameter concentrates around the unknown transition core \(\bm{\theta}_{h}^{*}\) with high probability (Lemma 1 & 12). This online update approach allows us to estimate the transition core with constant-time computational cost per episode, as the agent does not need to store all samples from previous episodes._

Optimistic randomized value functionTo achieve improved dependence on \(\kappa\), a crucial point is to utilize the local gradient information of MNL transition probabilities for each reachable state when constructing the Gram matrix. In MNL bandit problems [61, 76], this can be accomplished by substituting the Hessian of the negative log-likelihood with the Gram matrix using global gradient information \(\kappa\). However, there are fundamental differences between the settings in Perivier and Goyal [61], Zhang and Sugiyama [76] and ours. Perivier and Goyal [61] address the case where the reward for each product is _uniform_ (i.e., all products have a reward of 1), and the reward for not selecting a product from the given assortment (also known as the outside option) is 0. On the other hand, Zhang and Sugiyama [76] deal with _non-uniform_ rewards where the reward for each product may vary; however, the rewards for individual products are known a priori to the agent. In contrast, in MNL-MDPs, the value for each reachable state may vary (non-uniform) and is _not known_ beforehand. Due to these differences, the analysis techniques in MNL bandits [61, 76] cannot be directly applied to our setting. Instead, we adapt the feature centralization technique [50]. Then, the Hessian of the per-round loss \(\ell_{k,h}(\bm{\theta})\) is expressed in terms of the centralized feature as follows:

\[\nabla^{2}\ell_{k,h}(\bm{\theta})=\sum_{s^{\prime}\in\mathcal{S}_{k,h}}P_{ \bm{\theta}}(s^{\prime}\mid s^{k}_{h},a^{k}_{h})\bar{\bm{\varphi}}(s^{k}_{h}, a^{k}_{h},s^{\prime};\bm{\theta})\bar{\bm{\varphi}}(s^{k}_{h},a^{k}_{h},s^{ \prime};\bm{\theta})^{\top}\,.\]

where \(\bar{\bm{\varphi}}(s,a,s^{\prime};\bm{\theta}):=\bm{\varphi}(s,a,s^{\prime})- \mathbb{E}_{\widetilde{s}\sim P_{\bm{\theta}}(\cdot\mid s,a)}[\bm{\varphi}(s, a,\widetilde{s})]\) is the centralized feature by \(\bm{\theta}\). For more details, please refer to Appendix D.2.

Now we introduce the _optimistic randomized value function_\(\widetilde{Q}_{h}^{k}(\cdot,\cdot)\) for ORRL-MNL. The key point is that when perturbing the estimated value function, we use the centralized feature by the estimatedtransition parameter \(\widetilde{\bm{\theta}}_{h}^{k}\). For any \((s,a)\in\mathcal{S}\times\mathcal{A}\), set \(\widetilde{Q}_{H+1}^{k}(s,a)=0\) and for each \(h\in[H]\),

\[\widetilde{Q}_{h}^{k}(s,a):=\min\left\{r(s,a)+\sum_{s^{\prime}\in \mathcal{S}_{s,a}}P_{\widetilde{\bm{\theta}}_{h}^{k}}(s^{\prime}\mid s,a) \widetilde{V}_{h+1}^{k}(s^{\prime})+\nu_{k,h}^{\mathrm{rand}}(s,a)\,,H\right\},\] (7)

where \(\widetilde{V}_{h}^{k}(s):=\max_{a\in\mathcal{A}}\widetilde{Q}_{h}^{k}(s,a)\) and \(\nu_{k,h}^{\mathrm{rand}}(s,a)\) is the _randomized bonus term_ defined by

\[\nu_{k,h}^{\mathrm{rand}}(s,a):=\sum_{s^{\prime}\in\mathcal{S}_{s,a}}P_{ \widetilde{\bm{\theta}}_{h}^{k}}(s^{\prime}\mid s,a)\tilde{\bm{\varphi}}(s,a, s^{\prime};\widetilde{\bm{\theta}}_{h}^{k})^{\top}\bm{\xi}_{k,h}^{s^{\prime}}+3H \beta_{k}^{2}\max_{s^{\prime}\in\mathcal{S}_{s,a}}\|\bm{\varphi}(s,a,s^{ \prime})\|_{\mathbf{B}_{k,h}^{-1}}^{2}\,.\]

Here we sample _i.i.d._ Gaussian noise \(\bm{\xi}_{k,h}^{(m)}\sim\mathcal{N}(\bm{0}_{\bm{\alpha}},\sigma_{x}^{2} \mathbf{B}_{k,h}^{-1})\) for each \(m\in[M]\) and set \(\bm{\xi}_{k,h}^{s^{\prime}}:=\bm{\xi}_{k,h}^{m(s^{\prime})}\) where \(m(s^{\prime}):=\operatorname*{argmax}_{m\in[M]}\tilde{\bm{\varphi}}(s,a,s^{ \prime};\widetilde{\bm{\theta}}_{h}^{k})^{\top}\bm{\xi}_{k,h}^{m}\) is the most optimistic sampling index for a reachable state \(s^{\prime}\). Based on these optimistic randomized value function, at each episode the agent plays a greedy action with respect to \(\widetilde{Q}_{h}^{k}\) as summarized in Algorithm 2.

**Remark 4**.: _Note that the second term in the randomized bonus always has a positive value, but it rapidly decreases as episode proceeds. While due to the randomness of \(\bm{\xi}\), the randomized bonus \(\nu_{k,h}^{\mathrm{rand}}\) itself cannot be guaranteed to always have a positive value. Consequently, the constructed value function \(\widetilde{Q}_{h}^{k}(\cdot,\cdot)\) can be optimistic or pessimistic. However, as shown in Lemma 18, optimistic sampling technique ensures that the optimistic randomized value function \(\widetilde{Q}_{h}^{k}\) has at least a constant probability of being optimistic than the true optimal value function._

**Remark 5**.: _As with_ RRL-MNL_, since the transition core is estimated in an online manner and the Gram matrices with local gradient information \(\mathbf{B}_{k,h}\) and \(\widetilde{\mathbf{B}}_{k,h}\) are updated incrementally,_ ORRL-MNL _also requires constant-time computational cost and storage cost per-episode. Although_ ORRL-MNL _requires an additional \(\mathcal{O}(\mathcal{U})\) computation cost for feature centralization, the computation complexity order is the same as that of_ RRL-MNL _because it also needs to go over reachable states to calculate the dominant feature \(\tilde{\bm{\varphi}}\), which also incurs a \(\mathcal{O}(\mathcal{U})\) computation cost. On the other hand,_ ORRL-MNL _does not require prior knowledge of \(\kappa\) and achieves a regret with a better dependence on \(\kappa\)._

### Regret Bound of **Orrl-Mnl**

We present the regret upper bound of **Orrl-MNL**. The complete proof is deferred to Appendix D.

**Theorem 2** (Regret Bound of **Orrl-Mnl**).: _Suppose that Assumption 1- 4 hold. For any \(0<\delta<\frac{\Phi(-1)}{2}\), if we set the input parameters in Algorithm 2 as \(\lambda=\mathcal{O}(L_{\bm{\varphi}}^{2}d\log\mathcal{U}),\beta_{k}=\mathcal{O} (\sqrt{d}\log\mathcal{U}\log(kH)),\sigma_{k}=H\beta_{k},\,M=\lceil 1-\frac{\log(H \mathcal{U})}{\log\Phi(1)}\rceil\), and \(\eta=\mathcal{O}(\log\mathcal{U})\), then with probability at least \(1-\delta\), the cumulative regret of the_ ORRL-MNL _policy \(\pi\) is upper-bounded as follows:_

\[\textbf{Regret}_{\pi}(K)=\widetilde{\mathcal{O}}\left(d^{3/2}H^{3/2}\sqrt{T}+ \kappa^{-1}d^{2}H^{2}\right)\,,\]

_where \(T=KH\) is the total number of time steps._

Discussion of Theorem 2Theorem 2 establishes that the leading term in the regret bound does not suffer from the problem-dependent constant \(\kappa^{-1}\) and the second term of the regret bound is independent of the size of set of reachable states. To the extent of our knowledge, this is the first algorithm that provides a frequentist regret guarantee with improved dependence on \(\kappa^{-1}\) in MNL-MDPs. Compared to RRL-MNL, the technical challenge lies in ensuring the stochastic optimism of the estimated value for ORRL-MNL. Note that the prediction error (Definition 1) for ORRL-MNL is characterized by two components: one related to the gradient information of the MNL transition model at each reachable state, and the other related to the dominant feature with respect to the Gram matrix \(\mathbf{B}_{k,h}\) (Lemma 16). Hence, the probability of the Bellman error at each horizon, when following the optimal policy, being negative can depend on the size of the reachable states. This implies that the probability of stochastic optimism can be exponentially small, not only in the horizon \(H\) but also in the size of the reachable states \(\mathcal{U}\). However, as shown in Lemma 18, this challenge has been overcome by using a sample size \(M\) that _logarithmically_ increases with \(\mathcal{U}\), effectively addressing the issue.

Proof Sketch of Theorem 2The overall proof pipeline for Theorem 2 is similar to that of Theorem 1. The main differences lie in the concentration of the estimated transition core (Lemma D.2), the bound on the prediction error (Lemma D.2), and the stochastic optimism (Lemma 18). Please refer to Appendix D for detailed proofs.

Optimistic exploration extensionIn general, since TS-based randomized exploration requires a more rigorous proof technique than UCB-based algorithms, our technical ingredients enable the use of optimistic exploration in a straightforward manner. We introduce UCRL-MNL+ (Algorithm 3) in the Appendix E, an optimism-based algorithm for MNL-MDPs. It is both _computationally_ and _statistically_ efficient compared to UCRL-MNL [35], achieving _the tightest regret bound_ for MNL-MDPs.

**Corollary 1**.: _UCRL-MNL+ (Algorithm 3) has \(\widetilde{\mathcal{O}}(dH^{3/2}\sqrt{T}+\kappa^{-1}d^{2}H^{2})\) regret with high probability._

## 5 Numerical Experiments

We perform a numerical evaluation on a variant of RiverSwim [58] to demonstrate practicality of our proposed algorithms. We compare our algorithms (RRL-MNL, ORRL-MNL, UCRL-MNL+) with the state-of-the-art UCRL-MNL [35] for MNL-MDPs. For each configuration, we report the averaged results over 10 independent runs. Figure 0(a) and 0(b) show the episodic return of each algorithm, which is the sum of all the rewards obtained in one episode. First, our proposed algorithms (RRL-MNL, ORRL-MNL, UCRL-MNL+) outperform UCRL-MNL [35] for both cases of \(|\mathcal{S}|=4,8\). Second, ORRL-MNL and UCRL-MNL+ reach the optimal values quickly compared to the other algorithms, demonstrating improved statistical efficiency. Figure 0(c) illustrates the comparison in running time of the algorithms for the first 1,000 episodes. Our proposed algorithms are at least 50 times faster than UCRL-MNL. These differences become more pronounced as the episodes progress because our algorithms have a constant computation cost, whereas the computation cost of UCRL-MNL increases over time.

## 6 Conclusions

We propose randomized algorithms with provable efficiency and constant-time computational cost for MNL-MDPs. For the first algorithm, RRL-MNL, we use an optimistic sampling technique to ensure the stochastic optimism of the estimated value functions and provide the frequentist regret analysis. This is the first frequentist regret analysis for a non-linear model-based algorithm with randomized exploration without assuming stochastic optimism. To achieve a statistically improved regret bound, we propose ORRL-MNL by constructing the optimistic randomized value function using the effects of the local gradient of the MNL transition model equipped with the centralized feature. As a result, we achieve a frequentist regret guarantee with improved dependence on \(\kappa\) in RL with the MNL transition model, which is a significant contribution. The effectiveness and practicality of our methods are supported by numerical experiments.

Figure 1: Riverswim experiment results

## Acknowledgements

We sincerely thank the anonymous reviewers for their constructive feedback. This work was supported by the National Research Foundation of Korea(NRF) grant funded by the Korea government(MSIT) (No. 2022R1C1C1006859, 2022R1A4A1030579, and RS-2023-00222663) and by AI-Bio Research Grant through Seoul National University.

## References

* Abbasi-Yadkori et al. [2011] Yasin Abbasi-Yadkori, David Pal, and Csaba Szepesvari. Improved algorithms for linear stochastic bandits. _Advances in neural information processing systems_, 24:2312-2320, 2011.
* Abeille and Lazaric [2017] Marc Abeille and Alessandro Lazaric. Linear Thompson Sampling Revisited. In Aarti Singh and Jerry Zhu, editors, _Proceedings of the 20th International Conference on Artificial Intelligence and Statistics_, volume 54 of _Proceedings of Machine Learning Research_, pages 176-184. PMLR, PMLR, 20-22 Apr 2017.
* Abeille et al. [2021] Marc Abeille, Louis Faury, and Clement Calauzenes. Instance-wise minimax-optimal algorithms for logistic bandits. In _International Conference on Artificial Intelligence and Statistics_, pages 3691-3699. PMLR, 2021.
* Agarwal and Zhang [2022] Alekh Agarwal and Tong Zhang. Model-based rl with optimistic posterior sampling: Structural conditions and sample complexity. _Advances in Neural Information Processing Systems_, 35:35284-35297, 2022.
* Agarwal and Zhang [2022] Alekh Agarwal and Tong Zhang. Non-linear reinforcement learning in large action spaces: Structural conditions and sample-efficiency of posterior sampling. In _Conference on Learning Theory_, pages 2776-2814. PMLR, 2022.
* Agrawal et al. [2023] Priyank Agrawal, Theja Tulabandhula, and Vashist Avadhanula. A tractable online learning algorithm for the multinomial logit contextual bandit. _European Journal of Operational Research_, 2023.
* Agrawal and Jia [2017] Shipra Agrawal and Randy Jia. Posterior sampling for reinforcement learning: worst-case regret bounds. In _Advances in Neural Information Processing Systems_, pages 1184-1194, 2017.
* Amani and Thrampoulidis [2021] Sanae Amani and Christos Thrampoulidis. Ucb-based algorithms for multinomial logistic regression bandits. _Advances in Neural Information Processing Systems_, 34:2913-2924, 2021.
* Ayoub et al. [2020] Alex Ayoub, Zeyu Jia, Csaba Szepesvari, Mengdi Wang, and Lin Yang. Model-based reinforcement learning with value-targeted regression. In _International Conference on Machine Learning_, pages 463-474. PMLR, 2020.
* Azar et al. [2017] Mohammad Gheshlaghi Azar, Ian Osband, and Remi Munos. Minimax regret bounds for reinforcement learning. In _International Conference on Machine Learning_, pages 263-272. PMLR, 2017.
* 414, 2010.
* Bartlett et al. [2005] Peter L Bartlett, Olivier Bousquet, and Shahar Mendelson. Local rademacher complexities. _The Annals of Statistics_, 2005.
* Bradtke and Barto [1996] Steven J Bradtke and Andrew G Barto. Linear least-squares algorithms for temporal difference learning. _Machine learning_, 22(1):33-57, 1996.
* Cai et al. [2020] Qi Cai, Zhuoran Yang, Chi Jin, and Zhaoran Wang. Provably efficient exploration in policy optimization. In _International Conference on Machine Learning_, pages 1283-1294. PMLR, 2020.
* Campolongo and Orabona [2020] Nicolo Campolongo and Francesco Orabona. Temporal variability in implicit online learning. _Advances in neural information processing systems_, 33:12377-12387, 2020.

* [16] Olivier Chapelle and Lihong Li. An empirical evaluation of thompson sampling. _Advances in neural information processing systems_, 24, 2011.
* [17] Xi Chen, Yining Wang, and Yuan Zhou. Dynamic assortment optimization with changing contextual information. _Journal of machine learning research_, 2020.
* [18] Zixiang Chen, Chris Junchi Li, Huizhuo Yuan, Quanquan Gu, and Michael Jordan. A general framework for sample-efficient function approximation in reinforcement learning. In _The Eleventh International Conference on Learning Representations_, 2023.
* [19] Christoph Dann, Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, John Langford, and Robert E Schapire. On oracle-efficient pac rl with rich observations. In _Advances in Neural Information Processing Systems_, volume 31, 2018.
* [20] Simon Du, Akshay Krishnamurthy, Nan Jiang, Alekh Agarwal, Miroslav Dudik, and John Langford. Provably efficient rl with rich observations via latent state decoding. In _International Conference on Machine Learning_, pages 1665-1674. PMLR, 2019.
* [21] Simon Du, Sham Kakade, Jason Lee, Shachar Lovett, Gaurav Mahajan, Wen Sun, and Ruosong Wang. Bilinear classes: A structural framework for provable generalization in rl. In _International Conference on Machine Learning_, pages 2826-2836. PMLR, 2021.
* [22] Simon S. Du, Sham M. Kakade, Ruosong Wang, and Lin F. Yang. Is a good representation sufficient for sample efficient reinforcement learning? In _8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020_, 2020.
* [23] Louis Faury, Marc Abeille, Clement Calauzenes, and Olivier Fercoq. Improved optimistic algorithms for logistic bandits. In _International Conference on Machine Learning_, pages 3052-3060. PMLR, 2020.
* [24] Louis Faury, Marc Abeille, Kwang-Sung Jun, and Clement Calauzenes. Jointly efficient and optimal algorithms for logistic bandits. In _International Conference on Artificial Intelligence and Statistics_, pages 546-580. PMLR, 2022.
* [25] Alhussein Fawzi, Matej Balog, Aja Huang, Thomas Hubert, Bernardino Romera-Paredes, Mohammad Bernsteinarekatian, Alexander Novikov, Francisco J R Ruiz, Julian Schrittwieser, Grzegorz Swirszcz, et al. Discovering faster matrix multiplication algorithms with reinforcement learning. _Nature_, 610(7930):47-53, 2022.
* Volume 1_, NIPS'10, page 586-594, Red Hook, NY, USA, 2010. Curran Associates Inc.
* [27] Dylan J Foster, Satyen Kale, Haipeng Luo, Mehryar Mohri, and Karthik Sridharan. Logistic regression: The importance of being improper. In _Conference On Learning Theory_, pages 167-208. PMLR, 2018.
* [28] Dylan J Foster, Sham M Kakade, Jian Qian, and Alexander Rakhlin. The statistical complexity of interactive decision making. _arXiv preprint arXiv:2112.13487_, 2021.
* [29] David A Freedman. On tail probabilities for martingales. _the Annals of Probability_, pages 100-118, 1975.
* [30] Elad Hazan, Amit Agarwal, and Satyen Kale. Logarithmic regret algorithms for online convex optimization. _Machine Learning_, 69(2):169-192, 2007.
* [31] Elad Hazan, Tomer Koren, and Kfir Y Levy. Logistic regression: Tight bounds for stochastic and online optimization. In _Conference on Learning Theory_, pages 197-209. PMLR, 2014.
* [32] Elad Hazan et al. Introduction to online convex optimization. _Foundations and Trends(r) in Optimization_, 2(3-4):157-325, 2016.

* He et al. [2021] Jiafan He, Dongruo Zhou, and Quanquan Gu. Logarithmic regret for reinforcement learning with linear function approximation. In _International Conference on Machine Learning_, pages 4171-4180. PMLR, 2021.
* He et al. [2021] Jiafan He, Heyang Zhao, Dongruo Zhou, and Quanquan Gu. Nearly minimax optimal reinforcement learning for linear markov decision processes. In _International Conference on Machine Learning_, pages 12790-12822. PMLR, 2023.
* Hwang and Oh [2023] Taehyun Hwang and Min-hwan Oh. Model-based reinforcement learning with multinomial logistic function approximation. In _Proceedings of the AAAI conference on artificial intelligence_, pages 7971-7979, 2023.
* Hwang et al. [2023] Taehyun Hwang, Kyuwook Chai, and Min-Hwan Oh. Combinatorial neural bandits. In _Proceedings of the 40th International Conference on Machine Learning_. PMLR, 2023.
* Ishfaq et al. [2021] Haque Ishfaq, Qiwen Cui, Viet Nguyen, Alex Ayoub, Zhuoran Yang, Zhaoran Wang, Doina Precup, and Lin Yang. Randomized exploration in reinforcement learning with general value function approximation. In _International Conference on Machine Learning_, volume 139, pages 4607-4616. PMLR, PMLR, 2021.
* Ishfaq et al. [2024] Haque Ishfaq, Qingfeng Lan, Pan Xu, A. Rupam Mahmood, Doina Precup, Anima Anandkumar, and Kamyar Azizzadenesheli. Provable and practical: Efficient exploration in reinforcement learning via langevin monte carlo. In _The Twelfth International Conference on Learning Representations_, 2024. URL https://openreview.net/forum?id=nfIAEJFiBZ.
* Ishfaq et al. [2024] Haque Ishfaq, Yixin Tan, Yu Yang, Qingfeng Lan, Jianfeng Lu, A Rupam Mahmood, Doina Precup, and Pan Xu. More efficient randomized exploration for reinforcement learning via approximate sampling. _Reinforcement Learning Journal_, 3(1), 2024.
* Jaksch et al. [2010] Thomas Jaksch, Ronald Ortner, and Peter Auer. Near-optimal regret bounds for reinforcement learning. _Journal of Machine Learning Research_, 11(4), 2010.
* Jia et al. [2020] Zeyu Jia, Lin Yang, Csaba Szepesvari, and Mengdi Wang. Model-based reinforcement learning with value-targeted regression. In _Learning for Dynamics and Control_, pages 666-686. PMLR, 2020.
* Jiang et al. [2017] Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, John Langford, and Robert E Schapire. Contextual decision processes with low bellman rank are pac-learnable. In _International Conference on Machine Learning_, pages 1704-1713. PMLR, 2017.
* Jin et al. [2020] Chi Jin, Zhuoran Yang, Zhaoran Wang, and Michael I Jordan. Provably efficient reinforcement learning with linear function approximation. In _Conference on Learning Theory_, pages 2137-2143. PMLR, 2020.
* Jin et al. [2021] Chi Jin, Qinghua Liu, and Sobhan Miryoosefi. Bellman eluder dimension: New rich classes of rl problems, and sample-efficient algorithms. _Advances in neural information processing systems_, 34:13406-13418, 2021.
* Jun et al. [2017] Kwang-Sung Jun, Aniruddha Bhargava, Robert Nowak, and Rebecca Willett. Scalable generalized linear bandits: Online computation and hashing. _Advances in Neural Information Processing Systems_, 30, 2017.
* Kim et al. [2022] Yeoneung Kim, Insoon Yang, and Kwang-Sung Jun. Improved regret analysis for variance-adaptive linear bandits and horizon-free linear mixture mdps. _Advances in Neural Information Processing Systems_, 35:1060-1072, 2022.
* Kober et al. [2013] Jens Kober, J Andrew Bagnell, and Jan Peters. Reinforcement learning in robotics: A survey. _The International Journal of Robotics Research_, 32(11):1238-1274, 2013.
* Krishnamurthy et al. [2016] Akshay Krishnamurthy, Alekh Agarwal, and John Langford. Pac reinforcement learning with rich observations. _Advances in Neural Information Processing Systems_, 29:1840-1848, 2016.
* Kveton et al. [2020] Branislav Kveton, Csaba Szepesvari, Mohammad Ghavamzadeh, and Craig Boutilier. Perturbed-history exploration in stochastic linear bandits. In _Uncertainty in Artificial Intelligence_, pages 530-540. PMLR, 2020.

* [50] Joongkyu Lee and Min-hwan Oh. Nearly minimax optimal regret for multinomial logistic bandit. _arXiv preprint arXiv:2405.09831_, 2024.
* [51] Lihong Li, Yu Lu, and Dengyong Zhou. Provably optimal algorithms for generalized linear contextual bandits. In _International Conference on Machine Learning_, pages 2071-2080. PMLR, 2017.
* [52] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. _nature_, 518(7540):529-533, 2015.
* [53] Aditya Modi, Nan Jiang, Ambuj Tewari, and Satinder Singh. Sample complexity of reinforcement learning using linearly combined model ensembles. In _International Conference on Artificial Intelligence and Statistics_, pages 2010-2020. PMLR, 2020.
* [54] Min-hwan Oh and Garud Iyengar. Thompson sampling for multinomial logit contextual bandits. _Advances in Neural Information Processing Systems_, 32:3151-3161, 2019.
* [55] Min-hwan Oh and Garud Iyengar. Multinomial logit contextual bandits: Provable optimality and practicality. In _Proceedings of the AAAI Conference on Artificial Intelligence_, pages 9205-9213, 2021.
* [56] Ian Osband and Benjamin Van Roy. Model-based reinforcement learning and the eluder dimension. In _Advances in Neural Information Processing Systems_, pages 1466-1474, 2014.
* [57] Ian Osband and Benjamin Van Roy. Why is posterior sampling better than optimism for reinforcement learning? In _International conference on machine learning_, pages 2701-2710. PMLR, 2017.
* [58] Ian Osband, Daniel Russo, and Benjamin Van Roy. (more) efficient reinforcement learning via posterior sampling. _Advances in Neural Information Processing Systems_, 26, 2013.
* [59] Ian Osband, Benjamin Van Roy, and Zheng Wen. Generalization and exploration via randomized value functions. In _International Conference on Machine Learning_, pages 2377-2386. PMLR, 2016.
* [60] Aldo Pacchiano, Philip Ball, Jack Parker-Holder, Krzysztof Choromanski, and Stephen Roberts. Towards tractable optimism in model-based reinforcement learning. In _Uncertainty in Artificial Intelligence_, pages 1413-1423. PMLR, 2021.
* [61] Noemie Perivier and Vineet Goyal. Dynamic pricing and assortment under a contextual mnl demand. _Advances in Neural Information Processing Systems_, 35:3461-3474, 2022.
* [62] Daniel Russo. Worst-case regret bounds for exploration via randomized value functions. _Advances in Neural Information Processing Systems_, 32, 2019.
* [63] Daniel Russo and Benjamin Van Roy. Eluder dimension and the sample complexity of optimistic exploration. In _Advances in Neural Information Processing Systems_, pages 2256-2264, 2013.
* [64] Daniel J Russo, Benjamin Van Roy, Abbas Kazerouni, Ian Osband, Zheng Wen, et al. A tutorial on thompson sampling. _Foundations and Trends(r) in Machine Learning_, 11(1):1-96, 2018.
* [65] David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go without human knowledge. _nature_, 550(7676):354-359, 2017.
* [66] David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, et al. A general reinforcement learning algorithm that masters chess, shogi, and go through self-play. _Science_, 362(6419):1140-1144, 2018.
* [67] Daniil Tiapkin, Denis Belomestny, Daniele Calandriello, Eric Moulines, Remi Munos, Alexey Naumov, Mark Rowland, Michal Valko, and Pierre Menard. Optimistic posterior sampling for reinforcement learning with few samples and tight guarantees. _Advances in Neural Information Processing Systems_, 35:10737-10751, 2022.

* [68] Ruosong Wang, Russ R Salakhutdinov, and Lin Yang. Reinforcement learning with general value function approximation: Provably efficient approach via bounded eluder dimension. _Advances in Neural Information Processing Systems_, 33, 2020.
* [69] Yining Wang, Ruosong Wang, Simon Shaolei Du, and Akshay Krishnamurthy. Optimism in reinforcement learning with generalized linear function approximation. In _9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021_, 2021.
* [70] Gellert Weisz, Philip Amortila, and Csaba Szepesvari. Exponential lower bounds for planning in mdps with linearly-realizable optimal action-value functions. In _Algorithmic Learning Theory_, pages 1237-1264. PMLR, 2021.
* [71] Lin Yang and Mengdi Wang. Sample-optimal parametric q-learning using linearly additive features. In _International Conference on Machine Learning_, pages 6995-7004. PMLR, 2019.
* [72] Lin Yang and Mengdi Wang. Reinforcement learning in feature space: Matrix bandit, kernels, and regret bound. In _International Conference on Machine Learning_, pages 10746-10756. PMLR, 2020.
* [73] Andrea Zanette, David Brandfonbrener, Emma Brunskill, Matteo Pirotta, and Alessandro Lazaric. Frequentist regret bounds for randomized least-squares value iteration. In _International Conference on Artificial Intelligence and Statistics_, pages 1954-1964. PMLR, 2020.
* [74] Lijun Zhang, Tianbao Yang, Rong Jin, Yichi Xiao, and Zhi-Hua Zhou. Online stochastic linear optimization under one-bit feedback. In _International Conference on Machine Learning_, pages 392-401. PMLR, 2016.
* [75] Tong Zhang. Feel-good thompson sampling for contextual bandits and reinforcement learning. _SIAM Journal on Mathematics of Data Science_, 4(2):834-857, 2022.
* [76] Yu-Jie Zhang and Masashi Sugiyama. Online (multinomial) logistic bandit: Improved regret and constant computation cost. In _Thirty-seventh Conference on Neural Information Processing Systems_, 2023.
* [77] Zihan Zhang, Yuan Zhou, and Xiangyang Ji. Almost optimal model-free reinforcement learning-via reference-advantage decomposition. In _Advances in Neural Information Processing Systems_, volume 33, pages 15198-15207, 2020.
* [78] Zihan Zhang, Jiaqi Yang, Xiangyang Ji, and Simon S Du. Improved variance-aware confidence sets for linear bandits and linear mixture mdp. _Advances in Neural Information Processing Systems_, 34:4342-4355, 2021.
* [79] Zihan Zhang, Yuan Zhou, and Xiangyang Ji. Model-free reinforcement learning: from clipped pseudo-regret to sample complexity. In _International Conference on Machine Learning_, pages 12653-12662. PMLR, 2021.
* [80] Dongruo Zhou and Quanquan Gu. Computationally efficient horizon-free reinforcement learning for linear mixture mdps. _Advances in neural information processing systems_, 35:36337-36349, 2022.
* [81] Dongruo Zhou, Quanquan Gu, and Csaba Szepesvari. Nearly minimax optimal reinforcement learning for linear mixture markov decision processes. In _Conference on Learning Theory_, pages 4532-4576. PMLR, 2021.
* [82] Dongruo Zhou, Jiafan He, and Quanquan Gu. Provably efficient reinforcement learning for discounted mdps with feature mapping. In _International Conference on Machine Learning_, pages 12793-12802. PMLR, 2021.

###### Contents of Appendix

* A Related Work
* B Notations & Definitions
* C Detailed Regret Analysis for RRL-NNL (Theorem 1)
* C.1 Concentration of Estimated Transition Core \(\bm{\theta}_{h}^{k}\)
* C.2 Bound on Prediction Error
* C.3 Good Events with High Probability
* C.4 Stochastic Optimism
* C.5 Bound on Estimation Part
* C.6 Bound on Pessimism Part
* C.7 Regret Bound of RRL-MNL
* D Detailed Regret Analysis for ORRL-MNL (Theorem 2)
* D.1 Concentration of Estimated Transition Core \(\widetilde{\bm{\theta}}_{h}^{k}\)
* D.2 Bound on Prediction Error
* D.3 Good Events with High Probability
* D.4 Stochastic Optimism
* D.5 Bound on Estimation Part
* D.6 Bound on Pessimism Part
* D.7 Regret Bound of ORRL-MNL
* E Optimistic Exploration Extension
* E.1 Optimism
* F Experiment Details
* G Auxiliary Lemmas
* H Limitations

## Appendix A Related Work

RL with linear function approximationThere has been a growing interest in studies that extend beyond tabular MDPs and focus on function approximation methods with provable guarantees [42, 71, 43, 73, 53, 22, 14, 9, 68, 70, 33, 81, 82, 37, 35, 38]. In particular, for minimizing regret in linear MDPs, Jin et al. [43] propose an optimistic variant of the Least-Squares Value Iteration (LSVI) algorithm [13, 59] under the assumption that the transition model and reward function of the MDPs are linear function of a \(d\)-dimensional feature mapping and they guarantee \(\widetilde{\mathcal{O}}(d^{\frac{3}{2}}H^{\frac{3}{2}}\sqrt{T})\) regret. Zanette et al. [73] propose a randomized LSVI algorithm that incorporates exploration by perturbing the least-square approximation of the action-value function, and this algorithm guarantees \(\widetilde{\mathcal{O}}(d^{2}H^{2}\sqrt{T})\) regret. Ishfaq et al. [37] propose a variant of the randomized LSVI algorithm that combines optimism and TS by perturbing the training data with _i.i.d._ scalar noise, achieving a regret bound of \(\widetilde{\mathcal{O}}(d^{\frac{3}{2}}H^{\frac{3}{2}}\sqrt{T})\). Similarly, Ishfaq et al. [38] introduce a randomized RL algorithm that employs Langevin Monte Carlo (LMC) to approximate the posterior distribution of the action-valuefunction, also ensuring a regret bound of \(\widetilde{\mathcal{O}}(d^{\frac{3}{2}}H^{\frac{3}{2}}\sqrt{T})\). Also, there have been studies on model-based methods with function approximation in linear MDPs, such as Yang and Wang [72], which assume that the transition probability kernel is a bilinear model parametrized by a matrix and propose a UCB-based algorithm with an upper bound of \(\widetilde{\mathcal{O}}(d^{\frac{3}{2}}H^{2}\sqrt{T})\) for regret. He et al. [34] propose an algorithm achieving nearly minimax optimal regret \(\widetilde{\mathcal{O}}(dH\sqrt{T})\). Jia et al. [41] consider a specific type of MDPs called linear mixture MDPs in which the transition probability kernel is a linear combination of different basis kernels. This model encompasses various types of MDPs studied previously in Modi et al. [53], Yang and Wang [72]. For this model, Jia et al. [41] propose a UCB-based RL algorithm with value-targeted model parameter estimation that guarantees an upper bound of \(\widetilde{\mathcal{O}}(dH^{\frac{3}{2}}\sqrt{T})\) for regret. The same linear mixture MDPs have been used in other studies such as Ayoub et al. [9], Zhou et al. [81, 82]. Specifically, in Zhou et al. [81], a variant of the method proposed by Jia et al. [41] is suggested and proved that the algorithm guarantees an upper bound of \(\widetilde{\mathcal{O}}(dH\sqrt{T})\) regret with a matching lower bound of \(\Omega(dH\sqrt{T})\) for linear mixture MDPs. More recently, there are also works achieving horizon-free regret bounds for linear mixture MDPs [78, 46, 80].

RL with non-linear function approximationStudies have been conducted on extending function approximation beyond linear models. Ayoub et al. [9], Wang et al. [68], Ishfaq et al. [37] provide upper bound for regret based on eluder dimension [63]. Also, there has been an effort to develop sample-efficient methods with more "general" function approximation [48, 42, 19, 20, 21, 28, 37, 44, 4, 5, 75, 18, 39] However, these attempts may have been hindered by the difficulty of solving computationally intractable problems [48, 42, 19, 21, 28, 44, 18], the necessity of relying on stronger assumptions [20, 37], or the lack of discussion on how to define the posterior distribution supported by a given function class and how to draw the optimistic sample from the posterior [4, 5, 75]. That is why even after there exists a so-called "general function class"-based result, it is often the case that the results in specific parametric models are still needed. Despite the large number of studies on RL with linear function approximation, there is limited research on extending beyond linear models to other parametric models. Wang et al. [69] use generalized linear function approximation, where the Bellman backup of any value function is assumed to be a generalized linear function of feature mapping. Hwang and Oh [35] discuss the limitations of linear function approximation and propose a UCB-based algorithm for MNL transition model in feature space achieving \(\widetilde{\mathcal{O}}(dH^{\frac{3}{2}}\sqrt{T})\). Ishfaq et al. [39] present TS-based RL algorithms that utilize approximate samplers, such as LMC or Underdamped LMC, to enhance the implementation and computational tractability of TS for RL with general function classes.

Contextual bandits Faury et al. [23] first provide a UCB-based algorithm with \(\kappa\)-independent regret for binary logistic bandit and Abeille et al. [3] present UCB & TS based algorithms achieving nearly minimax optimal regret for the same setting. Faury et al. [24] propose a jointly efficient UCB-based algorithm that achieve \(\kappa\)-independent regret bound with \(\mathcal{O}(\log t)\) computation cost. In the context of MNL model, Oh and Iyengar [54] employ TS approach, while Oh and Iyengar [55] incorporate a combination of UCB exploration and online parameter updates for MNL bandits. Both of the methods have \(\mathcal{O}(\kappa^{-1}\sqrt{T})\) regret. Amani and Thrampoulidis [8] propose an optimistic algorithm with better dependence on \(\kappa\). Agrawal et al. [6] design a UCB-based algorithm with

\begin{table}
\begin{tabular}{c c c c c} \hline Algorithm & Model-based & Transition model & Reward & Computation cost & Regret \\ \hline LSVI-UCB [43] & ✗ & Linear & Linear & \(\mathcal{O}(K)\) & \(\widetilde{\mathcal{O}}(d^{2}H^{2}\sqrt{T})\) \\ OPT-RLSVII [73] & ✗ & Linear & Linear & \(\mathcal{O}(K)\) & \(\widetilde{\mathcal{O}}(d^{2}H^{2}\sqrt{T})\) \\ LSVI-PHE [37] & ✗ & Linear & Linear & \(\mathcal{O}(K)\) & \(\widetilde{\mathcal{O}}(d^{1}H^{1}\sqrt{T})\) \\ UC-MatrixLR [72] & ✓ & Linear & Known & \(\mathcal{O}(K)\) & \(\widetilde{\mathcal{O}}(d^{3}H^{2}\sqrt{T})\) \\ UCRL-VTR [9] & ✓ & Linear mixture & Known & \(\mathcal{O}(K)\) & \(\widetilde{\mathcal{O}}(dH^{2}\sqrt{T})\) \\ UCRL-NML [35] & ✓ & MNL & Known & \(\mathcal{O}(K)\) & \(\widetilde{\mathcal{O}}(\kappa^{-1}dH^{1}\sqrt{T})\) \\ RRL-NML (**this work**) & ✓ & MNL & Known & \(\mathcal{O}(1)\) & \(\widetilde{\mathcal{O}}(\kappa^{-1}d^{2}H^{2}\sqrt{T})\) \\ ORRL-NML (**this work**) & ✓ & MNL & Known & \(\mathcal{O}(1)\) & \(\widetilde{\mathcal{O}}\left(d^{3}H^{3}\sqrt{T}+\kappa^{-1}d^{2}H^{2}\right)\) \\ UCRL-NML+ (**this work**) & ✓ & MNL & Known & \(\mathcal{O}(1)\) & \(\widetilde{\mathcal{O}}\left(dH^{\frac{3}{2}}\sqrt{T}+\kappa^{-1}d^{2}H^{2}\right)\) \\ \hline \end{tabular}
\end{table}
Table 1: This table compares the problem settings, online update, performance of the this paper with those of other methods in provable RL with function approximation. For computation cost, we only keep the dependence on the number of episode \(K\).

\(\mathcal{O}(\sqrt{T})\) regret bound without \(\kappa\) in its leading term, and Perivier and Goyal [61] establish \(\mathcal{O}(\sqrt{T/\kappa_{*}})\) regret for the uniform reward setting. Zhang and Sugiyama [76] develop jointly efficient UCB-based algorithm for non-uniform MNL bandit problem. Lee and Oh [50] propose nearly minimax optimal MNL bandit algorithm for both uniform and non-uniform reward structures.

## Appendix B Notations & Definitions

In this section, we formally summarize some definitions and notations used to analyze the proposed algorithm.

### Inhomogeneous MNL transition model

For \(h\in[H]\), the probability of state transition to \(s^{\prime}\in\mathcal{S}_{s,a}\) when an action \(a\) is taken at a state \(s\) is given by

\[P_{h}(s^{\prime}\mid s,a):=P_{\bm{\theta}_{h}^{*}}(s^{\prime}\mid s,a)=\frac{ \exp(\bm{\varphi}(s,a,s^{\prime})^{\top}\bm{\theta}_{h}^{*})}{\sum_{\widetilde {s}\in\mathcal{S}_{s,a}}\exp(\bm{\varphi}(s,a,\widetilde{s})^{\top}\bm{\theta }_{h}^{*})}\,.\]

The estimated transition probability parameterized by \(\bm{\theta}\) is denoted as

\[P_{\bm{\theta}}(s^{\prime}\mid s,a):=\frac{\exp(\bm{\varphi}(s,a,s^{\prime}) ^{\top}\bm{\theta})}{\sum_{\widetilde{s}\in\mathcal{S}_{s,a}}\exp(\bm{ \varphi}(s,a,\widetilde{s})^{\top}\bm{\theta})}\,.\]

### Feature vector

We abbreviate the feature vector as follows:

\[\bm{\varphi}_{s,a,s^{\prime}}:=\bm{\varphi}(s,a,s^{\prime})\ \ \text{for}\ (s,a,s^{\prime})\in \mathcal{S}\times\mathcal{A}\times\mathcal{S}_{s,a}\,,\] \[\bm{\varphi}_{k,h,s^{\prime}}:=\bm{\varphi}(s_{h}^{k},a_{h}^{k}, s^{\prime})\ \ \text{for}\ (k,h)\in[K]\times[H]\ \text{and}\ s^{\prime}\in\mathcal{S}_{k,h}:=\mathcal{S}_{s_{h}^{k},a_{h}^{ k}}\,,\] \[\hat{\bm{\varphi}}_{k,h}(s,a):=\bm{\varphi}(s,a,\hat{s})\ \ \text{for}\ \hat{s}:=\underset{s^{\prime}\in\mathcal{S}_{s,a}}{\operatorname{argmax}}\| \bm{\varphi}(s,a,s^{\prime})\|_{\mathbf{A}_{h,h}^{-1}}\,,\] \[\bar{\bm{\varphi}}_{s,a,s^{\prime}}(\bm{\theta}):=\bar{\bm{ \varphi}}(s,a,s^{\prime};\bm{\theta})=\bm{\varphi}(s,a,s^{\prime})-\mathbb{E }_{\widetilde{s}\sim P_{\bm{\theta}}(\cdot\mid s,a)}[\bm{\varphi}(s,a, \widetilde{s})]\,,\] \[\bar{\bm{\varphi}}_{k,h,s^{\prime}}(\bm{\theta}):=\bar{\bm{ \varphi}}(s_{h}^{k},a_{h}^{k},s^{\prime};\bm{\theta})\,.\]

### Response variable & per-episode loss

The response variable \(y_{h}^{k}\) is given by

\[y_{h}^{k}:=[y_{h}^{k}(s^{\prime})]_{s^{\prime}\in\mathcal{S}_{k,h}}\ \ \text{where}\ y_{h}^{k}(s^{\prime}):=\mathds{1}(s_{h+1}^{k}=s^{\prime})\ \ \text{for}\ s^{\prime}\in\mathcal{S}_{k,h}\,.\]

The per-episode loss \(\ell_{k,h}(\bm{\theta})\) is given by

\[\ell_{k,h}(\bm{\theta}):=-\sum_{s^{\prime}\in\mathcal{S}_{k,h}}y _{h}^{k}(s^{\prime})\log P_{\bm{\theta}}(s^{\prime}\mid s_{h}^{k},a_{h}^{k})\,,\] \[\mathbf{G}_{k,h}(\bm{\theta}):=\nabla\ell_{k,h}(\bm{\theta})=\sum _{s^{\prime}\in\mathcal{S}_{k,h}}(P_{\bm{\theta}}(s^{\prime}\mid s_{h}^{k},a_ {h}^{k})-y_{h}^{k}(s^{\prime}))\bm{\varphi}_{k,h,s^{\prime}}\,,\] \[\mathbf{H}_{k,h}(\bm{\theta}):=\nabla^{2}\ell_{k,h}(\bm{\theta})\] \[=\sum_{s^{\prime}\in\mathcal{S}_{k,h}}P_{\bm{\theta}}(s^{\prime} \mid s_{h}^{k},a_{h}^{k})\bm{\varphi}_{k,h,s^{\prime}}\bm{\varphi}_{k,h,s^{ \prime}}^{\top}-\sum_{s^{\prime}\in\mathcal{S}_{k,h}}\sum_{\widetilde{s}\in \mathcal{S}_{k,h}}P_{\bm{\theta}}(s^{\prime}\mid s_{h}^{k},a_{h}^{k})P_{\bm{ \theta}}(\widetilde{s}\mid s_{h}^{k},a_{h}^{k})\bm{\varphi}_{k,h,s^{\prime}} \bm{\varphi}_{k,h,\widetilde{s}}^{\top}\,.\]

#### Regularity constants

\[H:\text{Horizon length}\] \[K:\text{Episode number}\] \[T=KH:\text{Total number of interactions}\] \[L_{\bm{\varphi}}:\ell_{2}\text{-norm upper bound of }\bm{\varphi}(s,a,s),\text{ i.e., }\|\bm{\varphi}(s,a,s^{\prime})\|_{2}\leq L_{\bm{\varphi}}\,,\] \[L_{\bm{\theta}}:\ell_{2}\text{-norm upper bound of }\bm{\theta}_{h}^{*},\text{ i.e., }\|\bm{\theta}_{h}^{*}\|_{2}\leq L_{\bm{\theta}}\,,\] \[\kappa:\text{Problem-dependent constant such that }\inf_{\bm{\theta}\in\mathcal{B}_{d}(L_{\bm{ \theta}})}P_{\bm{\theta}}(s^{\prime}\mid s,a)P_{\bm{\theta}}(\widetilde{s} \mid s,a)\geq\kappa\,,\] \[\mathcal{U}:\text{Maximum cardinality of the set of reachable states, i.e., }\mathcal{U}:=\max_{s,a}|\mathcal{S}_{s,a}|\,.\]

#### Estimated transition core

The estimated transition core for RRL-MNL is given by

\[\bm{\theta}_{h}^{k}=\operatorname*{argmin}_{\bm{\theta}\in\mathcal{B}_{d}(L_ {\bm{\theta}})}\frac{1}{2}\|\bm{\theta}-\bm{\theta}_{h}^{k-1}\|_{\mathbf{A}_{ k,h}}^{2}+(\bm{\theta}-\bm{\theta}_{h}^{k-1})^{\top}\nabla\ell_{k-1,h}(\bm{ \theta}_{h}^{k-1})\,,\]

and the estimated transition core for ORRL-MNL is given by

\[\widetilde{\bm{\theta}}_{h}^{k+1}=\operatorname*{argmin}_{\bm{\theta}\in \mathcal{B}_{d}(L_{\bm{\theta}})}\frac{1}{2\eta}\left\|\bm{\theta}-\widetilde{ \bm{\theta}}_{h}^{k}\right\|_{\widetilde{\mathbf{B}}_{h,h}}^{2}+\bm{\theta}^{ \top}\nabla\ell_{k,h}(\widetilde{\bm{\theta}}_{h}^{k})\,.\]

#### Gram matrices

The Gram matrix with global gradient information \(\kappa\) is given by

\[\mathbf{A}_{k,h}:=\lambda\mathbf{I}_{d}+\frac{\kappa}{2}\sum_{i=1}^{k-1}\sum_ {s^{\prime}\in\mathcal{S}_{i,h}}\bm{\varphi}(s_{h}^{i},a_{h}^{i},s^{\prime}) \bm{\varphi}(s_{h}^{i},a_{h}^{i},s^{\prime})^{\top}\,.\]

The Gram matrices with local gradient information are given by

\[\widetilde{\mathbf{B}}_{k,h}:=\mathbf{B}_{k,h}+\eta\nabla^{2}\ell_{k,h}( \widetilde{\bm{\theta}}_{h}^{k})\text{ \ and \ }\mathbf{B}_{k,h}:=\lambda\mathbf{I}_{d}+\sum_{i=1}^{k-1}\nabla^{2}\ell_{i,h}( \widetilde{\bm{\theta}}_{h}^{i+1})\,.\]

#### Confidence radius

For some absolute constants \(C_{\beta},C_{\bm{\xi}}>0\),

\[\alpha_{k} :=\alpha_{k}(\delta)\] \[=\widetilde{\mathcal{O}}(\kappa^{-1/2}d^{1/2})\,,\] \[\beta_{k} :=\beta_{k}(\delta)=C_{\beta}\sqrt{\log\mathcal{U}\left(\lambda \log(\mathcal{U}k)+\log(\mathcal{U}k)\log\left(\frac{H\sqrt{1+2k}}{\delta} \right)+d\log\left(1+\frac{k}{d\lambda}\right)\right)+\lambda L_{\bm{\theta}} ^{2}}\] \[=\mathcal{O}(\sqrt{d}\log\mathcal{U}\log(kH))\,,\] \[\gamma_{k} :=\gamma_{k}(\delta)=C_{\bm{\xi}}\sigma_{k}\sqrt{d\log(Md/\delta)}\,.\]

#### Filtration

For an arbitrary set \(X\), we denote the \(\Sigma\)-algebra generated by \(X\) as \(\Sigma(X)\). Then we define the following filtrations

\[\mathcal{F}_{k}:=\Sigma\left(\left\{s_{j}^{i},a_{j}^{i},r(s_{j}^{i},a_{j}^{i} )\mid i<k,j\leq H\right\}\cup\left\{\bm{\xi}_{i,j}^{(m)}\mid i<k,j\leq H,1\leq m \leq M\right\}\right)\,,\]

\[\mathcal{F}_{k,h}:=\Sigma\left(\mathcal{F}_{k}\cup\left\{s_{j}^{k},a_{j}^{k},r (s_{j}^{k},a_{j}^{k})\mid j\leq h\right\}\cup\left\{\bm{\xi}_{k,j}^{(m)}\mid j \geq h,1\leq m\leq M\right\}\right)\,.\]

#### Pseudo-noise

For RRL-MNL, the pseudo-noise is sampled as

\[\bm{\xi}_{k,h}^{(m)}\sim\mathcal{N}(\bm{0}_{d},\sigma_{k}^{2}\mathbf{A}_{k,h}^{-1 })\,,\]

and for ORRL-MNL, the pseudo-noise is sampled as

\[\bm{\xi}_{k,h}^{(m)}\sim\mathcal{N}(\bm{0}_{d},\sigma_{k}^{2}\mathbf{B}_{k,h}^{- 1})\,,\]

for \(M\) times independently.

#### Estimated value functions

The stochastically optimistic value function for RRL-MNL is defined as follows:

\[Q_{H+1}^{k}(s,a)=0\,,\] \[Q_{h}^{k}(s,a)=\min\left\{r(s,a)+\sum_{s^{\prime}\in\mathcal{S}_{ s,a}}P_{\bm{\theta}_{h}^{k}}(s^{\prime}\mid s,a)V_{h+1}^{k}(s^{\prime})+\max_{m \in[M]}\hat{\bm{\varphi}}_{k,h}(s,a)^{\top}\bm{\xi}_{k,h}^{(m)},H\right\}\, \text{for }h\in[H]\,.\]

The optimistic randomized value function for ORRL-MNL is defined as follows:

\[\widetilde{Q}_{H+1}^{k}(s,a)=0\,,\] \[\widetilde{Q}_{h}^{k}(s,a):=\min\left\{r(s,a)+\sum_{s^{\prime}\in \mathcal{S}_{s,a}}P_{\widetilde{\bm{\theta}}_{h}^{k}}(s^{\prime}\mid s,a) \widetilde{V}_{h+1}^{k}(s^{\prime})+\nu_{k,h}^{\mathrm{rand}}(s,a)\,,H\right\} \,\text{for }h\in[H]\,,\]

where

\[\nu_{k,h}^{\mathrm{rand}}(s,a):=\sum_{s^{\prime}\in\mathcal{S}_{ s,a}}P_{\widetilde{\bm{\theta}}_{h}^{k}}(s^{\prime}\mid s,a)\bar{\bm{\varphi}}(s,a,s ^{\prime};\widetilde{\bm{\theta}}_{h}^{k})^{\top}\bm{\xi}_{k,h}^{s^{\prime}}+3 H\beta_{k}^{2}\max_{s^{\prime}\in\mathcal{S}_{s,a}}\|\bm{\varphi}(s,a,s^{\prime}) \|_{\mathbf{B}_{k,h}^{-1}}^{2}\,,\] \[\bm{\xi}_{k,h}^{s^{\prime}}:=\bm{\xi}_{k,h}^{m(s^{\prime})}\, \text{for }m(s^{\prime}):=\operatorname*{argmax}_{m\in[M]}\bar{\bm{\varphi}}(s,a,s^{ \prime};\widetilde{\bm{\theta}}_{h}^{k})^{\top}\bm{\xi}_{k,h}^{m}\,.\]

#### Prediction error & Bellman error

**Definition 1** (Prediction error & Bellman error).: _For any \((s,a)\in\mathcal{S}\times\mathcal{A}\) and \((k,h)\in[K]\times[H]\), we define the prediction error about \(\bm{\theta}_{h}^{k}\) as_

\[\Delta_{h}^{k}(s,a):=\sum_{s^{\prime}\in\mathcal{S}_{s,a}}\left(P_{\bm{\theta} _{h}^{k}}(s^{\prime}\mid s,a)-P_{\bm{\theta}_{h}^{*}}(s^{\prime}|s,a)\right)V_ {h+1}^{k}(s^{\prime})\,.\]

_Also we define the Bellman error as follows:_

\[\iota_{h}^{k}(s,a):=r(s,a)+P_{h}V_{h+1}^{k}(s,a)-Q_{h}^{k}(s,a)\,.\]

#### Good events

For any \(\delta\in(0,1)\), we define the following good events:

For RRL-MNL,

\[\mathcal{G}_{k,h}^{\Delta}(\delta) :=\left\{|\Delta_{h}^{k}(s,a)|\leq H\alpha_{k}(\delta)\|\hat{\bm{ \varphi}}_{k,h}(s,a)\|_{\mathbf{A}_{k,h}^{-1}}\right\}\,,\] \[\mathcal{G}_{k,h}^{\bm{\xi}}(\delta) :=\left\{\max_{m\in[M]}\|\bm{\xi}_{k,h}^{(m)}\|_{\mathbf{A}_{k,h} }\leq\gamma_{k}(\delta)\right\}\,,\] \[\mathcal{G}_{k,h}(\delta) :=\left\{\mathcal{G}_{k,h}^{\Delta}(\delta)\cap\mathcal{G}_{k,h}^ {\bm{\xi}}(\delta)\right\}\,,\] \[\mathcal{G}_{k}(\delta) :=\bigcap_{h\in[H]}\mathcal{G}_{k,h}(\delta)\,,\] \[\mathcal{G}(K,\delta) :=\bigcap_{k\leq K}\mathcal{G}_{k}(\delta)\,.\]For ORRL-MNL,

\[\mathfrak{G}_{k,h}^{\Delta}(\delta) :=\left\{|\Delta_{h}^{k}(s,a)|\leq H\beta_{k}(\delta)\underset{s^{ \prime}\in\mathcal{S}_{s,a}}{\sum}P_{\boldsymbol{\tilde{\theta}}_{h}^{k}}(s^{ \prime}\mid s,a)\left\|\bar{\boldsymbol{\varphi}}_{s,a,s^{\prime}}(\widehat{ \boldsymbol{\theta}}_{h}^{k})\right\|_{\mathbf{B}_{h,h}^{-1}}\] \[\qquad\qquad+3H\beta_{k}(\delta)^{2}\max_{s^{\prime}\in\mathcal{S }_{s,a}}\|\boldsymbol{\varphi}_{s,a,s^{\prime}}\|_{\mathbf{B}_{h,h}^{-1}}^{2} \right\},\] \[\mathfrak{G}_{k,h}^{\boldsymbol{\xi}}(\delta) :=\left\{\underset{m\in[M]}{\max}\|\boldsymbol{\xi}_{k,h}^{(m)} \|_{\mathbf{B}_{h,h}}\leq\gamma_{k}(\delta)\right\}\,,\] \[\mathfrak{G}_{k,h}(\delta) :=\left\{\mathfrak{G}_{k,h}^{\Delta}(\delta)\cap\mathfrak{G}_{k, h}^{\boldsymbol{\xi}}(\delta)\right\}\,,\] \[\mathfrak{G}_{k}(\delta) :=\bigcap_{h\in[H]}\mathfrak{G}_{k,h}(\delta)\,,\] \[\mathfrak{G}(K,\delta) :=\bigcap_{k\leq K}\mathfrak{G}_{k}(\delta)\,.\]

Derivative of MNL transition model**Proposition 1** (Derivative of MNL transition model).: _The gradient and Hessian of \(P_{\boldsymbol{\theta}}(\cdot\mid\cdot,\cdot)\) can be calculated as follows:_

\[\nabla P_{\boldsymbol{\theta}}(s^{\prime}\mid s,a) =P_{\boldsymbol{\theta}}(s^{\prime}\mid s,a)\left(\boldsymbol{ \varphi}_{s,a,s^{\prime}}-\underset{s^{\prime\prime}\in\mathcal{S}_{s,a}}{ \sum}P_{\boldsymbol{\theta}}(s^{\prime\prime}\mid s,a)\boldsymbol{ \varphi}_{s,a,s^{\prime\prime}}\right)\] (8) \[=P_{\boldsymbol{\theta}}(s^{\prime}\mid s,a)\bar{\boldsymbol{ \varphi}}_{s,a,s^{\prime}}(\boldsymbol{\theta})\,,\]

_and_

\[\begin{split}&\nabla^{2}P_{\boldsymbol{\theta}}(s^{\prime}\mid s,a)\\ &=P_{\boldsymbol{\theta}}(s^{\prime}\mid s,a)\boldsymbol{\varphi}_{ s,a,s^{\prime}}\boldsymbol{\varphi}_{s,a,s^{\prime}}^{\top}\\ &\quad-P_{\boldsymbol{\theta}}(s^{\prime}\mid s,a)\sum_{s^{ \prime\prime}\in\mathcal{S}_{s,a}}P_{\boldsymbol{\theta}}(s^{\prime\prime} \mid s,a)\left(\boldsymbol{\varphi}_{s,a,s^{\prime}}\boldsymbol{\varphi}_{s,a,s^{\prime\prime}}^{\top}+\boldsymbol{\varphi}_{s,a,s^{\prime\prime}} \boldsymbol{\varphi}_{s,a,s^{\prime}}^{\top}+\boldsymbol{\varphi}_{s,a,s^{ \prime\prime}}\boldsymbol{\varphi}_{s,a,s^{\prime\prime}}^{\top}\right)\\ &\quad+2P_{\boldsymbol{\theta}}(s^{\prime}\mid s,a)\left(\underset {s^{\prime\prime}\in\mathcal{S}_{s,a}}{\sum}P_{\boldsymbol{\theta}}(s^{\prime \prime}\mid s,a)\boldsymbol{\varphi}_{s,a,s^{\prime\prime}}\right)\left( \underset{s^{\prime\prime}\in\mathcal{S}_{s,a}}{\sum}P_{\boldsymbol{\theta}}(s ^{\prime\prime}\mid s,a)\boldsymbol{\varphi}_{s,a,s^{\prime\prime}}\right)^{ \top}\,.\end{split}\] (9)

Proof of Proposition 1.: Let \(\boldsymbol{\theta}=(\theta_{1},\ldots,\theta_{d})\) and \([\boldsymbol{\varphi}_{s,a,s^{\prime}}]_{i}\) be the \(i\)-th component of \(\boldsymbol{\varphi}_{s,a,s^{\prime}}\). Then, we have

\[\frac{\partial}{\partial\theta_{j}}P_{\boldsymbol{\theta}}(s^{ \prime}\mid s,a)\] \[=\frac{\exp\left(\boldsymbol{\varphi}_{s,a,s^{\prime}}^{\top} \boldsymbol{\theta}\right)[\boldsymbol{\varphi}_{s,a,s^{\prime}}]_{j}}{\sum_{ s^{\prime\prime}\in\mathcal{S}_{s,a}}\exp\left(\boldsymbol{\varphi}_{s,a,s^{ \prime\prime}}^{\top}\boldsymbol{\theta}\right)}-\frac{\exp\left(\boldsymbol{ \varphi}_{s,a,s^{\prime}}^{\top}\boldsymbol{\theta}\right)\sum_{s^{\prime \prime}\in\mathcal{S}_{s,a}}\exp\left(\boldsymbol{\varphi}_{s,a,s^{\prime \prime}}^{\top}\boldsymbol{\theta}\right)[\boldsymbol{\varphi}_{s,a,s^{\prime \prime}}]_{j}}{\left(\sum_{s^{\prime\prime}\in\mathcal{S}_{s,a}}\exp\left( \boldsymbol{\varphi}_{s,a,s^{\prime\prime}}^{\top}\boldsymbol{\theta}\right) \right)^{2}}\] \[=P_{\boldsymbol{\theta}}(s^{\prime}\mid s,a)\left([\boldsymbol{ \varphi}_{s,a,s^{\prime}}]_{j}-\underset{s^{\prime\prime}\in\mathcal{S}_{s,a}}{ \sum}P_{\boldsymbol{\theta}}(s^{\prime\prime}\mid s,a)[\boldsymbol{\varphi}_{ s,a,s^{\prime\prime}}]_{j}\right)\,.\]

Then, the gradient of \(P_{\boldsymbol{\theta}}(s^{\prime}\mid s,a)\) is given by

\[\nabla P_{\boldsymbol{\theta}}(s^{\prime}\mid s,a) =P_{\boldsymbol{\theta}}(s^{\prime}\mid s,a)\boldsymbol{\varphi} _{s,a,s^{\prime}}-P_{\boldsymbol{\theta}}(s^{\prime}\mid s,a)\underset{s^{ \prime\prime}\in\mathcal{S}_{s,a}}{\sum}P_{\boldsymbol{\theta}}(s^{\prime \prime}\mid s,a)\boldsymbol{\varphi}_{s,a,s^{\prime\prime}}\] \[=P_{\boldsymbol{\theta}}(s^{\prime}\mid s,a)\left(\boldsymbol{ \varphi}_{s,a,s^{\prime}}-\underset{s^{\prime\prime}\in\mathcal{S}_{s,a}}{ \sum}P_{\boldsymbol{\theta}}(s^{\prime\prime}\mid s,a)\boldsymbol{\varphi}_{s,a,s ^{\prime\prime}}\right)\] \[=P_{\boldsymbol{\theta}}(s^{\prime}\mid s,a)\bar{\boldsymbol{ \varphi}}_{s,a,s^{\prime}}(\boldsymbol{\theta})\,.\]On the other hand, the second derivative \(\frac{\partial}{\partial\theta_{i}\partial\theta_{j}}P_{\bm{\theta}}(s^{\prime}\mid s,a)\) can be obtained as follows:

\[\frac{\partial}{\partial\theta_{i}\partial\theta_{j}}P_{\bm{\theta} }(s^{\prime}\mid s,a)\] \[=P_{\bm{\theta}}(s^{\prime}\mid s,a)\left([\bm{\varphi}_{s,a,s^{ \prime}}]_{i}-\sum_{s^{\prime\prime}\in\mathcal{S}_{s,a}}P_{\bm{\theta}}(s^{ \prime\prime}\mid s,a)[\bm{\varphi}_{s,a,s^{\prime\prime}}]_{i}\right)\] \[\quad\cdot\left([\bm{\varphi}_{s,a,s^{\prime}}]_{j}-\sum_{s^{ \prime\prime}\in\mathcal{S}_{s,a}}P_{\bm{\theta}}(s^{\prime\prime}\mid s,a)[ \bm{\varphi}_{s,a,s^{\prime\prime}}]_{j}\right)\] \[+P_{\bm{\theta}}(s^{\prime}\mid s,a)\left(-\sum_{s^{\prime\prime }\in\mathcal{S}_{s,a}}P_{\bm{\theta}}(s^{\prime\prime}\mid s,a)\left([\bm{ \varphi}_{s,a,s^{\prime\prime}}]_{i}-\sum_{\widetilde{z}\in\mathcal{S}_{s,a}}P _{\bm{\theta}}(\widetilde{s}\mid s,a)[\bm{\varphi}_{s,a,s^{\prime}}]_{i} \right)[\bm{\varphi}_{s,a,s^{\prime\prime}}]_{j}\right)\] \[=P_{\bm{\theta}}(s^{\prime}\mid s,a)\bigg{\{}[\bm{\varphi}_{s,a, s^{\prime}}]_{i}[\bm{\varphi}_{s,a,s^{\prime}}]_{j}\] \[\quad-\sum_{s^{\prime\prime}\in\mathcal{S}_{s,a}}P_{\bm{\theta}} (s^{\prime\prime}\mid s,a)\left([\bm{\varphi}_{s,a,s^{\prime\prime}}]_{i}[\bm{ \varphi}_{s,a,s^{\prime}}]_{j}+[\bm{\varphi}_{s,a,s^{\prime}}]_{i}[\bm{\varphi }_{s,a,s^{\prime\prime}}]_{j}\right)\] \[\quad+\left(\sum_{s^{\prime\prime}\in\mathcal{S}_{s,a}}P_{\bm{ \theta}}(s^{\prime\prime}\mid s,a)[\bm{\varphi}_{s,a,s^{\prime\prime}}]_{i} \right)\left(\sum_{s^{\prime\prime}\in\mathcal{S}_{s,a}}P_{\bm{\theta}}(s^{ \prime\prime}\mid s,a)[\bm{\varphi}_{s,a,s^{\prime\prime}}]_{j}\right)\] \[\quad-\sum_{s^{\prime\prime}\in\mathcal{S}_{s,a}}P_{\bm{\theta}} (s^{\prime\prime}\mid s,a)[\bm{\varphi}_{s,a,s^{\prime\prime}}]_{i}[\bm{ \varphi}_{s,a,s^{\prime\prime}}]_{j}\] \[\quad+\left(\sum_{s^{\prime\prime}\in\mathcal{S}_{s,a}}P_{\bm{ \theta}}(s^{\prime\prime}\mid s,a)[\bm{\varphi}_{s,a,s^{\prime\prime}}]_{j} \right)\left(\sum_{\widetilde{z}\in\mathcal{S}_{s,a}}P_{\bm{\theta}}(\widetilde {s}\mid s,a)[\bm{\varphi}_{s,a,\widetilde{z}}]_{i}\right)\right\}\] \[=P_{\bm{\theta}}(s^{\prime}\mid s,a)\left\{[\bm{\varphi}_{s,a,s^ {\prime}}]_{i}[\bm{\varphi}_{s,a,s^{\prime}}]_{j}\right.\] \[\quad-\sum_{s^{\prime\prime}\in\mathcal{S}_{s,a}}P_{\bm{\theta}} (s^{\prime\prime}\mid s,a)\left([\bm{\varphi}_{s,a,s^{\prime\prime}}]_{i}[\bm{ \varphi}_{s,a,s^{\prime}}]_{j}+[\bm{\varphi}_{s,a,s^{\prime}}]_{i}[\bm{\varphi }_{s,a,s^{\prime\prime}}]_{j}\right)\] \[\quad\left.-\sum_{s^{\prime\prime}\in\mathcal{S}_{s,a}}P_{\bm{ \theta}}(s^{\prime\prime}\mid s,a)[\bm{\varphi}_{s,a,s^{\prime\prime}}]_{i}[ \bm{\varphi}_{s,a,s^{\prime\prime}}]_{j}\right.\] \[\quad\left.+2\left(\sum_{s^{\prime\prime}\in\mathcal{S}_{s,a}}P_{ \bm{\theta}}(s^{\prime\prime}\mid s,a)[\bm{\varphi}_{s,a,s^{\prime\prime}}]_{i }\right)\left(\sum_{s^{\prime\prime}\in\mathcal{S}_{s,a}}P_{\bm{\theta}}(s^{ \prime\prime}\mid s,a)[\bm{\varphi}_{s,a,s^{\prime\prime}}]_{j}\right)\right\}\,.\]

Thus, we get the desired result as follows:

\[\nabla^{2}P_{\bm{\theta}}(s^{\prime}\mid s,a)\] \[=P_{\bm{\theta}}(s^{\prime}\mid s,a)\bm{\varphi}_{s,a,s^{\prime }}\bm{\varphi}_{s,a,s^{\prime}}^{\top}\] \[\quad-P_{\bm{\theta}}(s^{\prime}\mid s,a)\sum_{s^{\prime\prime} \in\mathcal{S}_{s,a}}P_{\bm{\theta}}(s^{\prime\prime}\mid s,a)\left(\bm{ \varphi}_{s,a,s^{\prime}}\bm{\varphi}_{s,a,s^{\prime\prime}}^{\top}+\bm{\varphi}_ {s,a,s^{\prime\prime}}\bm{\varphi}_{s,a,s^{\prime}}^{\top}+\bm{\varphi}_{s,a,s ^{\prime\prime}}\bm{\varphi}_{s,a,s^{\prime\prime}}^{\top}\right)\] \[\quad+2P_{\bm{\theta}}(s^{\prime}\mid s,a)\left(\sum_{s^{\prime \prime}\in\mathcal{S}_{s,a}}P_{\bm{\theta}}(s^{\prime\prime}\mid s,a)\bm{ \varphi}_{s,a,s^{\prime\prime}}\right)\left(\sum_{s^{\prime\prime}\in \mathcal{S}_{s,a}}P_{\bm{\theta}}(s^{\prime\prime}\mid s,a)\bm{\varphi}_{s,a,s^ {\prime\prime}}\right)^{\top}\,.\]

## Appendix C Detailed Regret Analysis for RRL-Mnl (Theorem 1)

In this section, we provide the complete proof of Theorem 1. First, we introduce all the technical lemmas needed to prove Theorem 1 along with their proofs. At the end of this section, we present the proof of Theorem 1.

### Concentration of Estimated Transition Core \(\bm{\theta}_{h}^{k}\)

In this section, we provide the concentration inequality for the estimated transition core run by the approximate online Newton step. The proof is similar to that given by Oh and Iyengar [55]. For completeness, we provide the detailed proof.

**Lemma 1** (Concentration of online estimated transition core).: _For each \(h\in[H]\), if \(\lambda\geq L_{\bm{\varphi}}^{2}\), then we have_

\[\mathbb{P}\left(\forall k\geq 1,\|\bm{\theta}_{h}^{k}-\bm{\theta}_{h}^{*}\|_{ \mathbf{A}_{k,h}}\leq\alpha_{k}(\delta)\right)\geq 1-\delta\,.\]

_where \(\alpha_{k}(\delta)\) is given by_

\[\alpha_{k}(\delta)\] \[:=\sqrt{\frac{8d}{\kappa}\log\left(1+\frac{k4L_{\bm{\varphi}}^{2} }{d\lambda}\right)+\left(\frac{32L_{\bm{\varphi}}L_{\bm{\theta}}}{3}+\frac{16 }{\kappa}\right)\log\frac{\left(1+\left\lceil 2\log_{2}k4L_{\bm{\varphi}}L_{\bm{ \theta}}\right\rceil\right)k^{2}}{\delta}+2\sqrt{2}+2\lambda L_{\bm{\theta}} ^{2}}\,.\]

Proof of lemma 1.: Recall that the per-round loss \(\ell_{k,h}(\bm{\theta})\) and its gradient \(\mathbf{G}_{k,h}(\bm{\theta})\) is defined as follows:

\[\ell_{k,h}(\bm{\theta}):=-\sum_{s^{\prime}\in\mathcal{S}_{k,h}}y_{h}^{k}(s^{ \prime})\log P_{\bm{\theta}}(s^{\prime}\mid s_{h}^{k},a_{h}^{k})\,,\quad \mathbf{G}_{k,h}(\bm{\theta}):=\nabla_{\bm{\theta}}\ell_{k,h}(\bm{\theta})\,.\]

For the analysis, we define the conditional expectations of \(\ell_{k,h}(\bm{\theta})\) & \(\mathbf{G}_{k,h}(\bm{\theta})\) as follows:

\[\bar{\ell}_{k,h}(\bm{\theta}):=\mathbb{E}_{y_{h}^{k}}\left[\ell_{k,h}(\bm{ \theta})\mid\mathcal{F}_{k,h}\right]\,,\quad\bar{\mathbf{G}}_{k,h}(\bm{\theta} ):=\mathbb{E}_{y_{h}^{k}}[\mathbf{G}_{k,h}(\bm{\theta})\mid\mathcal{F}_{k,h} ]\,.\]

By Taylor expansion with \(\bar{\bm{\theta}}=\nu\bm{\theta}_{h}^{k}+(1-\nu)\bm{\theta}_{h}^{*}\) for some \(\nu\in(0,1)\), we have

\[\ell_{k,h}(\bm{\theta}_{h}^{*})=\ell_{k,h}(\bm{\theta}_{h}^{k})+\mathbf{G}_{k,h}(\bm{\theta}_{h}^{h})^{\top}(\bm{\theta}_{h}^{*}-\bm{\theta}_{h}^{k})+ \frac{1}{2}(\bm{\theta}_{h}^{*}-\bm{\theta}_{h}^{k})^{\top}\mathbf{H}_{k,h}( \bar{\bm{\theta}})(\bm{\theta}_{h}^{*}-\bm{\theta}_{h}^{k})\,,\] (10)

where \(\mathbf{H}_{k,h}(\bm{\theta})\) is the Hessian of the per-round loss evaluated at \(\bm{\theta}\), i.e.,

\[\mathbf{H}_{k,h}(\bm{\theta}) :=\nabla^{2}\ell_{k,h}(\bm{\theta})\] (11) \[=\sum_{s^{\prime}\in\mathcal{S}_{k,h}}P_{\bm{\theta}}(s^{\prime} \mid s_{h}^{k},a_{h}^{k})\bm{\varphi}_{k,h,s^{\prime}}\bm{\varphi}_{k,h,s^{ \prime}}^{\top}\] \[\quad-\sum_{s^{\prime},\widetilde{s}\in\mathcal{S}_{k,h}}P_{\bm{ \theta}}(s^{\prime}\mid s_{h}^{k},a_{h}^{k})P_{\bm{\theta}}(\widetilde{s}\mid s _{h}^{k},a_{h}^{k})\bm{\varphi}_{k,h,s^{\prime}}\bm{\varphi}_{k,h,\widetilde{s} }^{\top}\,.\]

Note that for \(\bar{\bm{\theta}}=\nu\bm{\theta}_{h}^{k}+(1-\nu)\bm{\theta}_{h}^{*}\) with \(\nu\in(0,1)\), we have

\[\mathbf{H}_{k,h}(\bar{\bm{\theta}}) =\sum_{s^{\prime}\in\mathcal{S}_{k,h}}P_{\bar{\bm{\theta}}}(s^{ \prime}\mid s_{h}^{k},a_{h}^{k})\bm{\varphi}_{k,h,s^{\prime}}\bm{\varphi}_{k,h,s^{\prime}}^{\top}\] \[\quad-\sum_{s^{\prime}\in\mathcal{S}_{k,h}}\sum_{\widetilde{s}\in \mathcal{S}_{k,h}}P_{\bar{\bm{\theta}}}(s^{\prime}\mid s_{h}^{k},a_{h}^{k})P_{ \bar{\bm{\theta}}}(\widetilde{s}\mid s_{h}^{k},a_{h}^{k})\bm{\varphi}_{k,h,s ^{\prime}}^{\top}\bm{\varphi}_{k,h,\widetilde{s}}^{\top}\] \[=\sum_{s^{\prime}\in\mathcal{S}_{k,h}}P_{\bar{\bm{\theta}}}(s^{ \prime}\mid s_{h}^{k},a_{h}^{k})\bm{\varphi}_{k,h,s^{\prime}}\bm{\varphi}_{k,h,s^{\prime}}^{\top}\] \[\quad-\frac{1}{2}\sum_{s^{\prime}\in\mathcal{S}_{k,h}}\sum_{ \widetilde{s}\in\mathcal{S}_{k,h}}P_{\bar{\bm{\theta}}}(s^{\prime}\mid s_{h}^{k}, a_{h}^{k})P_{\bar{\bm{\theta}}}(\widetilde{s}\mid s_{h}^{k},a_{h}^{k})(\bm{\varphi}_{k,h,s^{ \prime}}\bm{\varphi}_{k,h,\widetilde{s}}^{\top}+\bm{\varphi}_{k,h,\widetilde{s} }\bm{\varphi}_{k,h,\widetilde{s}}^{\top})\] \[\quad-\sum_{s^{\prime}\in\mathcal{S}_{k,h}}\sum_{\widetilde{s}\in \mathcal{S}_{k,h}}P_{\bar{\bm{\theta}}}(s^{\prime}\mid s_{h}^{k},a_{h}^{k})P_{ \bar{\bm{\theta}}}(\widetilde{s}\mid s_{h}^{k},a_{h}^{k})\bm{\varphi}_{k,h,s^{ \prime}}^{\top}\bm{\varphi}_{k,h,s^{\prime}}^{\top}\,,\]where the inequality utilizes the fact that \(\mathbf{x}\mathbf{x}^{\top}+\mathbf{y}\mathbf{y}^{\top}\succeq\mathbf{x}\mathbf{y}^ {\top}+\mathbf{y}\mathbf{x}^{\top}\) for any \(\mathbf{x},\mathbf{y}\in\mathbb{R}^{d}\). Therefore, we have

\[\mathbf{H}_{k,h}(\bar{\bm{\theta}}) \succeq\sum_{s^{\prime}\in\mathcal{S}_{k,h}}P_{\bm{\theta}}(s^{ \prime}\mid s^{k}_{h},a^{k}_{h})\bm{\varphi}_{k,h,s^{\prime}}\bm{\varphi}_{k,h,s^{\prime}}^{\top}\] \[\quad-\sum_{s^{\prime}\in\mathcal{S}_{k,h}}\sum_{\widetilde{s} \in\mathcal{S}_{k,h}}P_{\bm{\theta}}(s^{\prime}\mid s^{k}_{h},a^{k}_{h})P_{ \bm{\theta}}(\widetilde{s}\mid s^{k}_{h},a^{k}_{h})\bm{\varphi}_{k,h,s^{ \prime}}\bm{\varphi}_{k,h,s^{\prime}}^{\top}\] \[=\sum_{s^{\prime}\neq\hat{s}_{k,h}}P_{\bm{\theta}}(s^{\prime}\mid s ^{k}_{h},a^{k}_{h})\bm{\varphi}_{k,h,s^{\prime}}\bm{\varphi}_{k,h,s^{\prime}} ^{\top}\] \[\quad-\sum_{s^{\prime}\neq\hat{s}_{k,h}}\sum_{\widetilde{s} \neq\hat{s}_{k,h}}P_{\bm{\theta}}(s^{\prime}\mid s^{k}_{h},a^{k}_{h})P_{\bm{ \theta}}(\widetilde{s}\mid s^{k}_{h},a^{k}_{h})\bm{\varphi}_{k,h,s^{\prime}} \bm{\varphi}_{k,h,s^{\prime}}^{\top}\] \[=\sum_{s^{\prime}\neq\hat{s}_{k,h}}P_{\bm{\theta}}(s^{\prime}\mid s ^{k}_{h},a^{k}_{h})\left(1-\sum_{\widetilde{s}\neq\hat{s}_{k,h}}P_{\bm{\theta }}(\widetilde{s}\mid s^{k}_{h},a^{k}_{h})\right)\bm{\varphi}_{k,h,s^{\prime}} \bm{\varphi}_{k,h,s^{\prime}}^{\top}\] \[=\sum_{s^{\prime}\neq\hat{s}_{k,h}}P_{\bm{\theta}}(s^{\prime}\mid s ^{k}_{h},a^{k}_{h})P_{\bm{\theta}}(\hat{s}_{k,h}\mid s^{k}_{h},a^{k}_{h})\bm{ \varphi}_{k,h,s^{\prime}}\bm{\varphi}_{k,h,s^{\prime}}^{\top}\] \[\succeq\sum_{s^{\prime}\neq\hat{s}_{k,h}}\kappa\bm{\varphi}_{k,h,s ^{\prime}}\bm{\varphi}_{k,h,s^{\prime}}^{\top}\] \[=\sum_{s^{\prime}\in\mathcal{S}_{k,h}}\kappa\bm{\varphi}_{k,h,s^{ \prime}}\bm{\varphi}_{k,h,s^{\prime}}^{\top}\,,\]

where \(\hat{s}_{k,h}\) is the state satisfying \(\bm{\varphi}(s^{k}_{h},a^{k}_{h},\hat{s}_{k,h})=\mathbf{0}_{d}\) and the last inequality comes from the Assumption 4.

Using the lower bound of the Hessian of the per-round loss evaluated at \(\bar{\bm{\theta}}\), from (10) we have

\[\ell_{k,h}(\bm{\theta}_{h}^{*})\geq\ell_{k,h}(\bm{\theta}_{h}^{k})+\mathbf{G} _{k,h}(\bm{\theta}_{h}^{k})^{\top}(\bm{\theta}_{h}^{*}-\bm{\theta}_{h}^{k})+ \frac{\kappa}{2}(\bm{\theta}_{h}^{*}-\bm{\theta}_{h}^{k})^{\top}\left(\sum_{s^ {\prime}\in\mathcal{S}_{k,h}}\bm{\varphi}_{k,h,s^{\prime}}\bm{\varphi}_{k,h,s ^{\prime}}^{\top}\right)(\bm{\theta}_{h}^{*}-\bm{\theta}_{h}^{k})\,.\]

By rearranging, we have

\[\ell_{k,h}(\bm{\theta}_{h}^{k})\leq\ell_{k,h}(\bm{\theta}_{h}^{*})+\mathbf{G} _{k,h}(\bm{\theta}_{h}^{k})^{\top}(\bm{\theta}_{h}^{k}-\bm{\theta}_{h}^{*})- \frac{\kappa}{2}(\bm{\theta}_{h}^{*}-\bm{\theta}_{h}^{k})^{\top}\mathbf{W}_{k,h }(\bm{\theta}_{h}^{*}-\bm{\theta}_{h}^{k})\,,\]

where we denote \(\mathbf{W}_{k,h}:=\sum_{s^{\prime}\in\mathcal{S}_{k,h}}\bm{\varphi}_{k,h,s^{ \prime}}\bm{\varphi}_{k,h,s^{\prime}}^{\top}\). By taking expectation over \(y_{h}^{k}\), we have

\[\bar{\ell}_{k,h}(\bm{\theta}_{h}^{k})\leq\bar{\ell}_{k,h}(\bm{\theta}_{h}^{*})+ \bar{\mathbf{G}}_{k,h}(\bm{\theta}_{h}^{k})^{\top}(\bm{\theta}_{h}^{k}-\bm{ \theta}_{h}^{*})-\frac{\kappa}{2}(\bm{\theta}_{h}^{*}-\bm{\theta}_{h}^{k})^{\top} \mathbf{W}_{k,h}(\bm{\theta}_{h}^{*}-\bm{\theta}_{h}^{k})\,.\] (12)

On the other hand, for any \(\bm{\theta}\in\mathbb{R}^{d}\), since we have

\[\bar{\ell}_{k,h}(\bm{\theta})-\bar{\ell}_{k,h}(\bm{\theta}_{h}^{ *})\] \[=-\sum_{s^{\prime}\in\mathcal{S}_{k,h}}P_{\bm{\theta}_{h}^{*}}(s^{ \prime}\mid s^{k}_{h},a^{k}_{h})\log P_{\bm{\theta}}(s^{\prime}\mid s^{k}_{h},a ^{k}_{h})+\sum_{s^{\prime}\in\mathcal{S}_{k,h}}P_{\bm{\theta}_{h}^{*}}(s^{ \prime}\mid s^{k}_{h},a^{k}_{h})\log P_{\bm{\theta}_{h}^{*}}(s^{\prime}\mid s^{k} _{h},a^{k}_{h})\] \[=\sum_{s^{\prime}\in\mathcal{S}_{k,h}}P_{\bm{\theta}_{h}^{*}}(s^{ \prime}\mid s^{k}_{h},a^{k}_{h})\left(\log P_{\bm{\theta}_{h}^{*}}(s^{\prime} \mid s^{k}_{h},a^{k}_{h})-\log P_{\bm{\theta}}(s^{\prime}\mid s^{k}_{h},a^{k}_{h })\right)\] \[=\sum_{s^{\prime}\in\mathcal{S}_{k,h}}P_{\bm{\theta}_{h}^{*}}(s^{ \prime}\mid s^{k}_{h},a^{k}_{h})\log\frac{P_{\bm{\theta}_{h}^{*}}(s^{\prime}\mid s ^{k}_{h},a^{k}_{h})}{P_{\bm{\theta}}(s^{\prime}\mid s^{k}_{h},a^{k}_{h})}\] \[=D_{\text{KL}}(P_{\bm{\theta}_{h}^{*}}\parallel P_{\bm{\theta}})\] \[\geq 0\,,\]

where \(D_{\text{KL}}(P\parallel Q)\) is the Kullback-Leibler divergence of \(P\) from \(Q\), from (12) we have

\[0 \leq\bar{\ell}_{k,h}(\bm{\theta}_{h}^{k})^{\top}(\bm{\theta}_{h}^{ k}-\bm{\theta}_{h}^{*})-\frac{\kappa}{2}\|\bm{\theta}_{h}^{*}-\bm{\theta}_{h}^{k}\|_{ \mathbf{W}_{k,h}}^{2}\] \[=\mathbf{G}_{k,h}(\bm{\theta}_{h}^{k})^{\top}(\bm{\theta}_{h}^{k}-\bm{ \theta}_{h}^{*})-\frac{\kappa}{2}\|\bm{\theta}_{h}^{*}-\bm{\theta}_{h}^{k}\|_{ \mathbf{W}_{k,h}}^{2}+\left(\bar{\mathbf{G}}_{k,h}(\bm{\theta}_{h}^{k})-\mathbf{G} _{k,h}(\bm{\theta}_{h}^{k})\right)^{\top}(\bm{\theta}_{h}^{k}-\bm{\theta}_{h}^{*})\,.\] (13)To get an upper bound of \(\mathbf{G}_{k,h}(\bm{\theta}_{h}^{k})^{\top}(\bm{\theta}_{h}^{k}-\bm{\theta}_{h}^{*})\), recall that the estimated transition core is given by

\[\bm{\theta}_{h}^{k+1}=\operatorname*{argmin}_{\bm{\theta}\in\mathcal{B}_{d}(L_{ \bm{\theta}})}\frac{1}{2}\|\bm{\theta}-\bm{\theta}_{h}^{k}\|_{\mathbf{A}_{k+1,h }}^{2}+(\bm{\theta}-\bm{\theta}_{h}^{k})^{\top}\mathbf{G}_{k,h}(\bm{\theta}_{ h}^{k})\,.\] (14)

Since the objective function in (14) is convex, by the first-order optimality condition for any \(\bm{\theta}\in\mathcal{B}_{d}(L_{\bm{\theta}})\), we have

\[\left(\mathbf{G}_{k,h}(\bm{\theta}_{h}^{k})+\mathbf{A}_{k+1,h}(\bm{\theta}_{h }^{k+1}-\bm{\theta}_{h}^{k})\right)^{\top}(\bm{\theta}-\bm{\theta}_{h}^{k+1}) \geq 0,\]

which gives

\[\bm{\theta}^{\top}\mathbf{A}_{k+1,h}(\bm{\theta}_{h}^{k+1}-\bm{\theta}_{h}^{ k})\geq(\bm{\theta}_{h}^{k+1})^{\top}\mathbf{A}_{k+1,h}(\bm{\theta}_{h}^{k+1}- \bm{\theta}_{h}^{k})-\mathbf{G}_{k,h}(\bm{\theta}_{h}^{k})^{\top}(\bm{\theta} -\bm{\theta}_{h}^{k+1})\,.\] (15)

Then, we have

\[\|\bm{\theta}_{h}^{k}-\bm{\theta}_{h}^{*}\|_{\mathbf{A}_{k+1,h}}^ {2}-\|\bm{\theta}_{h}^{k+1}-\bm{\theta}_{h}^{*}\|_{\mathbf{A}_{k+1,h}}^{2}\] \[=(\bm{\theta}_{h}^{k})^{\top}\mathbf{A}_{k+1,h}\bm{\theta}_{h}^{ k}-(\bm{\theta}_{h}^{k+1})^{\top}\mathbf{A}_{k+1,h}\bm{\theta}_{h}^{k+1}+2(\bm{ \theta}_{h}^{*})^{\top}\mathbf{A}_{k+1,h}(\bm{\theta}_{h}^{k+1}-\bm{\theta}_{h }^{k})\] \[\geq(\bm{\theta}_{h}^{k})^{\top}\mathbf{A}_{k+1,h}\bm{\theta}_{h} ^{k}-(\bm{\theta}_{h}^{k+1})^{\top}\mathbf{A}_{k+1,h}\bm{\theta}_{h}^{k+1}+2( \bm{\theta}_{h}^{k+1})^{\top}\mathbf{A}_{k+1,h}(\bm{\theta}_{h}^{k+1}-\bm{ \theta}_{h}^{k})\] \[\quad-2\mathbf{G}_{k,h}(\bm{\theta}_{h}^{k})^{\top}(\bm{\theta}_{h }^{k}-\bm{\theta}_{h}^{k+1})\] (by ( 15 )) \[=(\bm{\theta}_{h}^{k})^{\top}\mathbf{A}_{k+1,h}\bm{\theta}_{h}^{ k}+(\bm{\theta}_{h}^{k+1})^{\top}\mathbf{A}_{k+1,h}\bm{\theta}_{h}^{k+1}-2(\bm{ \theta}_{h}^{k+1})^{\top}\mathbf{A}_{k+1,h}\bm{\theta}_{h}^{k}-2\mathbf{G}_{k,h}(\bm{\theta}_{h}^{k})^{\top}(\bm{\theta}_{h}^{*}-\bm{\theta}_{h}^{k+1})\] \[=\|\bm{\theta}_{h}^{k}-\bm{\theta}_{h}^{k+1}\|_{\mathbf{A}_{k+1,h} }^{2}+2\mathbf{G}_{k,h}(\bm{\theta}_{h}^{k})^{\top}(\bm{\theta}_{h}^{k}-\bm{ \theta}_{h}^{k+1})\] \[=\|\bm{\theta}_{h}^{k}-\bm{\theta}_{h}^{k+1}\|_{\mathbf{A}_{k+1,h} }^{2}+2\mathbf{G}_{k,h}(\bm{\theta}_{h}^{k})^{\top}(\bm{\theta}_{h}^{k+1}-\bm{ \theta}_{h}^{k})+2\mathbf{G}_{k,h}(\bm{\theta}_{h}^{k})^{\top}(\bm{\theta}_{h}^ {k}-\bm{\theta}_{h}^{*})\] \[\geq-\|\mathbf{G}_{k,h}(\bm{\theta}_{h}^{k})\|_{\mathbf{A}_{k+1,h} ^{-1}}^{2}+2\mathbf{G}_{k,h}(\bm{\theta}_{h}^{k})^{\top}(\bm{\theta}_{h}^{k}- \bm{\theta}_{h}^{*})\,,\] (16)

where the last inequality follows by the fact that

\[\|\bm{\theta}_{h}^{k}-\bm{\theta}_{h}^{k+1}\|_{\mathbf{A}_{k+1,h }}^{2}+2\mathbf{G}_{k,h}(\bm{\theta}_{h}^{k})^{\top}(\bm{\theta}_{h}^{k+1}-\bm {\theta}_{h}^{k}) \geq\min_{\bm{\theta}\in\mathcal{B}_{d}(L_{\bm{\theta}})}\Big{\{} \|\bm{\theta}\|_{\mathbf{A}_{k+1,h}}^{2}+2\mathbf{G}_{k,h}(\bm{\theta}_{h}^{k}) ^{\top}\bm{\theta}\Big{\}}\] \[=-\|\mathbf{G}_{k,h}(\bm{\theta}_{h}^{k})\|_{\mathbf{A}_{k+1,h}^{ -1}}^{2}\,.\]

Therefore, from (16) we have

\[\mathbf{G}_{k,h}(\bm{\theta}_{h}^{k})^{\top}(\bm{\theta}_{h}^{k}-\bm{\theta}_{h }^{*})\leq\frac{1}{2}\|\mathbf{G}_{k,h}(\bm{\theta}_{h}^{k})\|_{\mathbf{A}_{k+1,h}^{-1}}^{2}+\frac{1}{2}\|\bm{\theta}_{h}^{k}-\bm{\theta}_{h}^{*}\|_{\mathbf{A}_{ k+1,h}}^{2}-\frac{1}{2}\|\bm{\theta}_{h}^{k+1}-\bm{\theta}_{h}^{*}\|_{ \mathbf{A}_{k+1,h}}^{2}\,.\] (17)

By substituting (17) into (13), we have

\[0 \leq\frac{1}{2}\|\mathbf{G}_{k,h}(\bm{\theta}_{h}^{k})\|_{\mathbf{A }_{k+1,h}^{-1}}^{2}+\frac{1}{2}\|\bm{\theta}_{h}^{k}-\bm{\theta}_{h}^{*}\|_{ \mathbf{A}_{k+1,h}}^{2}-\frac{1}{2}\|\bm{\theta}_{h}^{k+1}-\bm{\theta}_{h}^{*}\|_{ \mathbf{A}_{k+1,h}}^{2}\] \[\quad-\frac{\kappa}{2}\|\bm{\theta}_{h}^{*}-\bm{\theta}_{h}^{k}\|_{ \mathbf{W}_{k,h}}+\left(\bar{\mathbf{G}}_{k,h}(\bm{\theta}_{h}^{k})-\mathbf{G}_{k,h}(\bm{\theta}_{h}^{k})\right)^{\top}(\bm{\theta}_{h}^{k}-\bm{\theta}_{h}^{*})\,.\] (18)Note that since we have

\[\|\mathbf{G}_{k,h}(\bm{\theta}_{h}^{k})\|_{\bm{\Lambda}_{k+1,h}^{-1}}^ {2}\] \[=\sum_{s^{\prime},\vec{s}\in\mathcal{S}_{k,h}}\left(P_{\bm{\theta}_{ h}^{k}}(s^{\prime}\mid s_{h}^{k},a_{h}^{k})-y_{h}^{k}(s^{\prime})\right)\left(P_{\bm{ \theta}_{h}^{k}}(\widetilde{s}\mid s_{h}^{k},a_{h}^{k})-y_{h}^{k}(\widetilde{s })\right)\bm{\varphi}_{k,h,s^{\prime}}^{\top}\mathbf{A}_{k+1,h}^{-1}\bm{\varphi }_{k,h,\widetilde{s}}\] \[=\frac{1}{2}\sum_{s^{\prime},\vec{s}\in\mathcal{S}_{k,h}}\left(P_ {\bm{\theta}_{h}^{k}}(s^{\prime}\mid s_{h}^{k},a_{h}^{k})-y_{h}^{k}(s^{\prime })\right)\left(P_{\bm{\theta}_{h}^{k}}(\widetilde{s}\mid s_{h}^{k},a_{h}^{k})- y_{h}^{k}(\widetilde{s})\right)\] \[\qquad\qquad\qquad\qquad\cdot(\bm{\varphi}_{k,h,s^{\prime}}^{ \top}\mathbf{A}_{k+1,h}^{-1}\bm{\varphi}_{k,h,\widetilde{s}}+\bm{\varphi}_{k, h,\widetilde{s}}^{\top}\mathbf{A}_{k+1,h}^{-1}\bm{\varphi}_{k,h,s^{\prime}})\] \[\leq\frac{1}{2}\sum_{s^{\prime},\vec{s}\in\mathcal{S}_{k,h}}\bigg{[} \left(P_{\bm{\theta}_{h}^{k}}(s^{\prime}\mid s_{h}^{k},a_{h}^{k})-y_{h}^{k}(s^ {\prime})\right)^{2}\bm{\varphi}_{k,h,s^{\prime}}^{\top}\mathbf{A}_{k+1,h}^{- 1}\bm{\varphi}_{k,h,s^{\prime}}\] \[\qquad\qquad\qquad\qquad+\left(P_{\bm{\theta}_{h}^{k}}( \widetilde{s}\mid s_{h}^{k},a_{h}^{k})-y_{h}^{k}(\widetilde{s})\right)^{2}\bm {\varphi}_{k,h,\widetilde{s}}^{\top}\mathbf{A}_{k+1,h}^{-1}\bm{\varphi}_{k,h, \widetilde{s}}\bigg{]}\] \[=\sum_{s^{\prime}\in\mathcal{S}_{k,h}}\left(P_{\bm{\theta}_{h}^{ k}}(s^{\prime}\mid s_{h}^{k},a_{h}^{k})-y_{h}^{k}(s^{\prime})\right)^{2}\bm{ \varphi}_{k,h,s^{\prime}}^{\top}\mathbf{A}_{k+1,h}^{-1}\bm{\varphi}_{k,h,s^{ \prime}}\] \[\leq\sum_{s^{\prime}\in\mathcal{S}_{k,h}}\left|P_{\bm{\theta}_{h}^ {k}}(\widetilde{s}\mid s_{h}^{k},a_{h}^{k})-y_{h}^{k}(\widetilde{s})\right| \bm{\varphi}_{k,h,s^{\prime}}^{\top}\mathbf{A}_{k+1,h}^{-1}\bm{\varphi}_{k,h,s ^{\prime}}\] \[\leq\sum_{s^{\prime}\in\mathcal{S}_{k,h}}\left(P_{\bm{\theta}_{h} ^{k}}(\widetilde{s}\mid s_{h}^{k},a_{h}^{k})+y_{h}^{k}(s^{\prime})\right)\bm{ \varphi}_{k,h,s^{\prime}}^{\top}\mathbf{A}_{k+1,h}^{-1}\bm{\varphi}_{k,h,s^{ \prime}}\] \[=\sum_{s^{\prime}\in\mathcal{S}_{k,h}}P_{\bm{\theta}_{h}^{k}}( \widetilde{s}\mid s_{h}^{k},a_{h}^{k})\bm{\varphi}_{k,h,s^{\prime}}^{\top} \mathbf{A}_{k+1,h}^{-1}\bm{\varphi}_{k,h,s^{\prime}}+\sum_{s^{\prime}\in \mathcal{S}_{k,h}}y_{h}^{k}(s^{\prime})\bm{\varphi}_{k,h,s^{\prime}}^{\top} \mathbf{A}_{k+1,h}^{-1}\bm{\varphi}_{k,h,s^{\prime}}\] \[\leq 2\max_{s^{\prime}\in\mathcal{S}_{k,h}}\|\bm{\varphi}_{k,h,s^{ \prime}}\|_{\mathbf{A}_{k+1,h}^{-1}}^{2}\,,\] (19)

where the first inequality utilizes the inequality \(\mathbf{x}^{\top}\mathbf{A}\mathbf{y}+\mathbf{y}^{\top}\mathbf{A}\mathbf{x} \leq\mathbf{x}^{\top}\mathbf{A}\mathbf{x}+\mathbf{y}^{\top}\mathbf{A}\mathbf{y}\) for any positive-semidefinite matrix \(\mathbf{A}\), and the last inequality holds since \(0\leq P_{\bm{\theta}_{h}^{k}}(s^{\prime}\mid s_{h}^{k},a_{h}^{k})\leq 1\) and \(\sum_{s^{\prime}}P_{\bm{\theta}_{h}^{k}}(s^{\prime}\mid s_{h}^{k},a_{h}^{k})=1\).

Combining the results of (18) and (19), we have

\[0 \leq\max_{s^{\prime}\in\mathcal{S}_{k,h}}\|\bm{\varphi}_{k,h,s^{ \prime}}\|_{\mathbf{A}_{k+1,h}^{-1}}^{2}+\frac{1}{2}\|\bm{\theta}_{h}^{k}-\bm{ \theta}_{h}^{*}\|_{\mathbf{A}_{k+1,h}}^{2}-\frac{1}{2}\|\bm{\theta}_{h}^{k+1}- \bm{\theta}_{h}^{*}\|_{\mathbf{A}_{k+1,h}}^{2}\] \[\quad-\frac{\kappa}{2}\|\bm{\theta}_{h}^{*}-\bm{\theta}_{h}^{k}\| _{\mathbf{W}_{h,h}}+\left(\bar{\mathbf{G}}_{k,h}(\bm{\theta}_{h}^{k})-\mathbf{G }_{k,h}(\bm{\theta}_{h}^{k})\right)^{\top}(\bm{\theta}_{h}^{k}-\bm{\theta}_{h}^{*})\] \[=\max_{s^{\prime}\in\mathcal{S}_{k,h}}\|\bm{\varphi}_{k,h,s^{ \prime}}\|_{\mathbf{A}_{k+1,h}^{-1}}^{2}+\frac{1}{2}\|\bm{\theta}_{h}^{k}-\bm{ \theta}_{h}^{*}\|_{\mathbf{A}_{k,h}}^{2}+\frac{\kappa}{4}\|\bm{\theta}_{h}^{k}- \bm{\theta}_{h}^{*}\|_{\mathbf{W}_{h,h}}^{2}-\frac{1}{2}\|\bm{\theta}_{h}^{k+1}- \bm{\theta}_{h}^{*}\|_{\mathbf{A}_{k+1,h}}^{2}\] \[\quad-\frac{\kappa}{2}\|\bm{\theta}_{h}^{*}-\bm{\theta}_{h}^{k}\| _{\mathbf{W}_{h,h}}+\left(\bar{\mathbf{G}}_{k,h}(\bm{\theta}_{h}^{k})-\mathbf{G }_{k,h}(\bm{\theta}_{h}^{k})\right)^{\top}(\bm{\theta}_{h}^{k}-\bm{\theta}_{h}^{*})\] \[=\max_{s^{\prime}\in\mathcal{S}_{k,h}}\|\bm{\varphi}_{k,h,s^{ \prime}}\|_{\mathbf{A}_{k+1,h}^{-1}}^{2}+\frac{1}{2}\|\bm{\theta}_{h}^{k}-\bm{ \theta}_{h}^{*}\|_{\mathbf{A}_{k,h}}^{2}-\frac{\kappa}{4}\|\bm{\theta}_{h}^{k}- \bm{\theta}_{h}^{*}\|_{\mathbf{W}_{k,h}}^{2}-\frac{1}{2}\|\bm{\theta}_{h}^{k+1}- \bm{\theta}_{h}^{*}\|_{\mathbf{A}_{k+1,h}}^{2}\] \[\quad+\left(\bar{\mathbf{G}}_{k,h}(\bm{\theta}_{h}^{k})-\mathbf{G }_{k,h}(\bm{\theta}_{h}^{k})\right)^{\top}(\bm{\theta}_{h}^{k}-\bm{\theta}_{h}^{*})\,,\]

where for the first equality we use \(\mathbf{A}_{k+1,h}=\mathbf{A}_{k,h}+\frac{\kappa}{2}\mathbf{W}_{h,h}\). By rearranging the terms, we have

\[\|\bm{\theta}_{h}^{k+1}-\bm{\theta}_{h}^{*}\|_{\mathbf{A}_{k+1,h}}^{2} \leq\|\bm{\theta}_{h}^{k}-\bm{\theta}_{h}^{*}\|_{\mathbf{A}_{k,h}}^{2}+2 \max_{s^{\prime}\in\mathcal{S}_{k,h}}\|\bm{\varphi}_{k,h,s^{\prime}}\|_{ \mathbf{A}_{k+1,h}^{-1}}^{2}-\frac{\kappa}{2}\|\bm{\theta}_{h}^{k}-\bm{\theta}_{h}^ {*}\|_{\mathbf{W}_{k,hThen summing over \(k\) gives

\[\|\bm{\theta}_{h}^{k+1}-\bm{\theta}_{h}^{*}\|_{\mathbf{A}_{k+1,h}}^{2} \leq\|\bm{\theta}_{1,h}-\bm{\theta}_{h}^{*}\|_{\mathbf{A}_{1,h}}^{2} +2\sum_{i=1}^{k}\max_{s^{\prime}\in\mathcal{S}_{i,h}}\|\bm{\varphi}_{i,h,s^{ \prime}}\|_{\mathbf{A}_{i+1,h}^{-1}}^{2}-\frac{\kappa}{2}\sum_{i=1}^{k}\|\bm{ \theta}_{h}^{i}-\bm{\theta}_{h}^{*}\|_{\mathbf{W}_{i,h}}^{2}\] \[\quad+2\sum_{i=1}^{k}\left(\mathbf{G}_{i,h}(\bm{\theta}_{h}^{i})- \mathbf{G}_{i,h}(\bm{\theta}_{h}^{i})\right)^{\top}\left(\bm{\theta}_{h}^{i}- \bm{\theta}_{h}^{*}\right)\] \[\leq 2\lambda L_{\theta}^{2}+2\sum_{i=1}^{k}\max_{s^{\prime}\in \mathcal{S}_{i,h}}\|\bm{\varphi}_{i,h,s^{\prime}}\|_{\mathbf{A}_{i+1,h}^{-1}} ^{2}-\frac{\kappa}{2}\sum_{i=1}^{k}\|\bm{\theta}_{h}^{i}-\bm{\theta}_{h}^{*}\| _{\mathbf{W}_{i,h}}^{2}\] \[\quad+2\sum_{i=1}^{k}\left(\bar{\mathbf{G}}_{i,h}(\bm{\theta}_{h} ^{i})-\mathbf{G}_{i,h}(\bm{\theta}_{h}^{i})\right)^{\top}\left(\bm{\theta}_{h} ^{i}-\bm{\theta}_{h}^{*}\right).\]

For the final step, note that \(\left(\bar{\mathbf{G}}_{i,h}(\bm{\theta}_{h}^{i})-\mathbf{G}_{i,h}(\bm{\theta} _{h}^{i})\right)^{\top}\left(\bm{\theta}_{h}^{i}-\bm{\theta}_{h}^{*}\right)\) is a martingale difference sequence. To bound this term, we invoke the following lemmas:

**Lemma 2**.: _For \(\delta\in(0,1)\) and \((k,h)\in[K]\times[H]\), with a probability at least \(1-\delta\) we have_

\[\sum_{i=1}^{k}\left(\bar{\mathbf{G}}_{i,h}(\bm{\theta}_{h}^{i})- \mathbf{G}_{i,h}(\bm{\theta}_{h}^{i})\right)^{\top}(\bm{\theta}_{h}^{i}-\bm{ \theta}^{*})\] \[\leq\frac{\kappa}{4}\sum_{i=1}^{k}\|\bm{\theta}_{h}^{i}-\bm{ \theta}_{h}^{*}\|_{\mathbf{W}_{i,h}}^{2}+\left(\frac{16L_{\bm{\varphi}}L_{\bm {\theta}}}{3}+\frac{8}{\kappa}\right)\log\frac{\left(1+\lceil 2\log_{2}k \mathcal{U}L_{\bm{\varphi}}L_{\bm{\theta}}\rceil\right)k^{2}}{\delta}+\sqrt{2}\,.\]

**Lemma 3** (Generalized elliptical potential).: _Let \(S_{t}:=\{\mathbf{x}_{t,1},\ldots,\mathbf{x}_{t,K}\}\subset\mathbb{R}^{d}\). For any \(1\leq t\leq T\) and \(i\in[K]\), suppose \(\|\mathbf{x}_{t,i}\|_{2}\leq L\). Let \(\mathbf{V}_{t}:=\lambda\mathbf{I}_{d}+\sum_{\tau=1}^{t-1}\sum_{i\in S_{\tau}} \mathbf{x}_{\tau,i}\mathbf{x}_{\tau,i}^{\top}\) for some \(\lambda>0\). If \(\lambda\geq L^{2}\), then we have_

\[\sum_{t=1}^{T}\max_{i\in[K]}\|\mathbf{x}_{t,i}\|_{\mathbf{V}_{t}^{-1}}^{2}\leq 2 d \log\left(1+\frac{TKL}{d\lambda}\right)\,.\]

By Lemma 2, with probability at least \(1-\delta\), we have

\[\|\bm{\theta}_{h}^{k+1}-\bm{\theta}_{h}^{*}\|_{\mathbf{A}_{k+1,h}}^ {2}\] \[\leq 2\lambda L_{\bm{\theta}}^{2}+2\sum_{i=1}^{k}\max_{s^{\prime} \in\mathcal{S}_{i,h}}\|\bm{\varphi}_{i,h,s^{\prime}}\|_{\mathbf{A}_{i+1,h}^{-1}} ^{2}\] \[\quad+\left(\frac{32L_{\bm{\varphi}}L_{\bm{\theta}}}{3}+\frac{16} {\kappa}\right)\log\frac{\left(1+\lceil 2\log_{2}k\mathcal{U}L_{\bm{\varphi}}L_{\bm{ \theta}}\rceil\right)k^{2}}{\delta}+2\sqrt{2}\] \[\leq 2\lambda L_{\bm{\theta}}^{2}+\frac{8}{\kappa}d\log\left(1+ \frac{k\mathcal{U}L_{\bm{\varphi}}^{2}}{d\lambda}\right)+\left(\frac{32L_{\bm {\varphi}}L_{\bm{\theta}}}{3}+\frac{16}{\kappa}\right)\log\frac{\left(1+ \lceil 2\log_{2}k\mathcal{U}L_{\bm{\varphi}}L_{\bm{\theta}}\rceil\right)k^{2}}{ \delta}+2\sqrt{2}\,,\]

where the second inequality comes from Lemma 3. Note that the Gram matrix \(\mathbf{A}_{k,h}\) in Algorithm 1 and the Gram matrix \(\mathbf{V}\) in Lemma 3 are different by the factor of \(\frac{\kappa}{2}\), which results in additional \(\frac{2}{\kappa}\) factor for the bound of \(\sum_{i=1}^{k}\max_{s^{\prime}\in\mathcal{S}_{i,h}}\|\bm{\varphi}_{i,h,s^{ \prime}}\|_{\mathbf{A}_{i+1,h}^{-1}}^{2}\). 

In the following, we provide all the proofs of the lemmas used to prove Lemma 1.

#### c.1.1 Proof of Lemma 2

Proof of Lemma 2.: Note that \(\left(\bar{\mathbf{G}}_{i,h}(\bm{\theta}_{h}^{i})-\mathbf{G}_{i,h}(\bm{\theta}_{h }^{i})\right)^{\top}(\bm{\theta}_{h}^{i}-\bm{\theta}_{h}^{*})\) is a martingale difference sequence, i.e.,

\[\mathbb{E}\left[\left(\bar{\mathbf{G}}_{i,h}(\bm{\theta}_{h}^{i}) -\mathbf{G}_{i,h}(\bm{\theta}_{h}^{i})\right)^{\top}(\bm{\theta}_{h}^{i}-\bm{ \theta}_{h}^{*})\mid\mathcal{F}_{i,h}\right]\] \[=\left(\bar{\mathbf{G}}_{i,h}(\bm{\theta}_{h}^{i})-\mathbb{E} \left[\mathbf{G}_{i,h}(\bm{\theta}_{h}^{i})\mid\mathcal{F}_{i,h}\right]\right) ^{\top}(\bm{\theta}_{h}^{i}-\bm{\theta}_{h}^{*})\] \[=0\,.\]

On the other hand, for any \(\bm{\theta}\in\mathbb{R}^{d}\), since we have

\[\|\mathbf{G}_{i,h}(\bm{\theta})\|_{2} =\left\|\sum_{s^{\prime}\in\mathcal{S}_{i,h}}\left(P_{\bm{\theta }}(s^{\prime}\mid s_{h}^{i},a_{h}^{i})-y_{h}^{i}(s^{\prime})\right)\bm{\varphi }_{i,h,s^{\prime}}\right\|_{2}\] \[\leq\sum_{s^{\prime}\in\mathcal{S}_{i,h}}\left|P_{\bm{\theta}}(s ^{\prime}\mid s_{h}^{i},a_{h}^{i})-y_{h}^{i}(s^{\prime})\right|\|\bm{\varphi }_{i,h,s^{\prime}}\|_{2}\] \[\leq L_{\bm{\varphi}}\left(\sum_{s^{\prime}\in\mathcal{S}_{i,h}}P _{\bm{\theta}}(s^{\prime}\mid s_{h}^{i},a_{h}^{i})+\sum_{s^{\prime}\in\mathcal{ S}_{i,h}}y_{h}^{i}(s^{\prime})\right)\] \[=2L_{\bm{\varphi}}\,,\]

then, it follows by

\[\left|\left(\bar{\mathbf{G}}_{i,h}(\bm{\theta}_{h}^{i})-\mathbf{G }_{i,h}(\bm{\theta}_{h}^{i})\right)^{\top}(\bm{\theta}_{h}^{i}-\bm{\theta}_{h }^{*})\right|\] \[\leq\left|\left(\bar{\mathbf{G}}_{i,h}(\bm{\theta}_{h}^{i})\right) ^{\top}(\bm{\theta}_{h}^{i}-\bm{\theta}_{h}^{*})\right|+\left|\left(\mathbf{G }_{i,h}(\bm{\theta}_{h}^{i})\right)^{\top}(\bm{\theta}_{h}^{i}-\bm{\theta}_{h }^{*})\right|\] \[\leq\|\bar{\mathbf{G}}_{i,h}(\bm{\theta}_{h}^{i})\|_{2}\|\bm{ \theta}_{h}^{i}-\bm{\theta}_{h}^{*}\|_{2}+\|\mathbf{G}_{i,h}(\bm{\theta}_{h}^{ i})\|_{2}\|\bm{\theta}_{h}^{i}-\bm{\theta}_{h}^{*}\|_{2}\] \[\leq 4L_{\bm{\varphi}}\|\bm{\theta}_{h}^{i}-\bm{\theta}_{h}^{*}\|_ {2}\] \[\leq 8L_{\bm{\varphi}}L_{\bm{\theta}}\,,\] (20)

where the last inequality follows by \(\|\bm{\theta}_{h}^{i}-\bm{\theta}_{h}^{*}\|_{2}\leq\|\bm{\theta}_{h}^{i}\|_{2 }+\|\bm{\theta}_{h}^{*}\|_{2}\leq 2L_{\bm{\theta}}\). Hence, if we denote \(M_{k,h}:=\sum_{i=1}^{k}\left(\bar{\mathbf{G}}_{i,h}(\bm{\theta}_{h}^{i})- \mathbf{G}_{i,h}(\bm{\theta}_{h}^{i})\right)^{\top}(\bm{\theta}_{h}^{i}-\bm{ \theta}_{h}^{*})\), then \(M_{k,h}\) is a martingale. Note that we alsohave

\[\Sigma_{k,h} =\sum_{i=1}^{k}\mathbb{E}_{y_{h}^{i}}\left[\left(\left[\mathbf{G}_{i,h}(\boldsymbol{\theta}_{h}^{i})-\mathbf{G}_{i,h}(\boldsymbol{\theta}_{h}^{i}) \right]^{\top}(\boldsymbol{\theta}_{h}^{i}-\boldsymbol{\theta}_{h}^{*})\right)^ {2}\right]\] \[=\sum_{i=1}^{k}\mathbb{E}_{y_{h}^{i}}\Bigg{[}\left(\left[\mathbf{G }_{i,h}(\boldsymbol{\theta}_{h}^{i})\right]^{\top}(\boldsymbol{\theta}_{h}^{i} -\boldsymbol{\theta}_{h}^{*})\right)^{2}\Bigg{]}-\mathbb{E}_{y_{h}^{i}}\Bigg{[} \left(\left[\mathbf{G}_{i,h}(\boldsymbol{\theta}_{h}^{i})\right]^{\top}( \boldsymbol{\theta}_{h}^{i}-\boldsymbol{\theta}_{h}^{*})\right)^{2}\Bigg{]}\] \[\leq\sum_{i=1}^{k}\mathbb{E}_{y_{h}^{i}}\left[\left(\left[ \mathbf{G}_{i,h}(\boldsymbol{\theta}_{h}^{i})\right]^{\top}(\boldsymbol{ \theta}_{h}^{i}-\boldsymbol{\theta}_{h}^{*})\right)^{2}\right]\] \[=\sum_{i=1}^{k}\mathbb{E}_{y_{h}^{i}}\left[\left(\sum_{s^{\prime }\in\mathcal{S}_{i,h}}\left(P_{\boldsymbol{\theta}_{h}^{i}}(s^{\prime}\mid s _{h}^{i},a_{h}^{i})-y_{h}^{i}(s^{\prime})\right)^{2}\right]\left(\sum_{s^{ \prime}\in\mathcal{S}_{i,h}}\left(\boldsymbol{\varphi}_{i,h,s^{\prime}}^{\top }(\boldsymbol{\theta}_{h}^{i}-\boldsymbol{\theta}_{h}^{*})\right)^{2}\right)\] \[\leq 2\sum_{i=1}^{k}\sum_{s^{\prime}\in\mathcal{S}_{i,h}}\left( \boldsymbol{\varphi}_{i,h,s^{\prime}}^{\top}(\boldsymbol{\theta}_{h}^{i}- \boldsymbol{\theta}_{h}^{*})\right)^{2}\] (22) \[=2\sum_{i=1}^{k}\|\boldsymbol{\theta}_{h}^{i}-\boldsymbol{\theta} _{h}^{*}\|_{\mathbf{W}_{i,h}}^{2}=:B_{k,h}\,,\]

where (21) holds by the Cauchy-Schwarz inequality, (22) holds because

\[\sum_{s^{\prime}\in\mathcal{S}_{i,h}}\left(P_{\boldsymbol{\theta} _{h}^{i}}(s^{\prime}\mid s_{h}^{i},a_{h}^{i})-y_{h}^{i}(s^{\prime})\right)^{2}\] \[=\sum_{s^{\prime}\in\mathcal{S}_{i,h}}\left\{P_{\boldsymbol{ \theta}_{h}^{i}}(s^{\prime}\mid s_{h}^{i},a_{h}^{i})\right\}^{2}-2P_{ \boldsymbol{\theta}_{h}^{i}}(s^{\prime}\mid s_{h}^{i},a_{h}^{i})y_{h}^{i}(s^{ \prime})+\left\{y_{h}^{i}(s^{\prime})\right\}^{2}\] \[\leq 2\,.\]

However, if we denote \(B_{k,h}:=2\sum_{i=1}^{k}\|\boldsymbol{\theta}_{h}^{i}-\boldsymbol{\theta}_{h}^ {*}\|_{\mathbf{W}_{i,h}}^{2}\), since \(B_{k,h}\) is itself a random variable, to apply Freedman's inequality to \(M_{k,h}\), we consider two cases depending on the values of \(B_{k,h}\).

**Case 1 : \(B_{k,h}\leq\frac{4}{k\mathcal{U}}\)**Suppose that \(B_{k,h}=2\sum_{i=1}^{k}\|\bm{\theta}_{h}^{i}-\bm{\theta}_{h}^{*}\|_{\mathbf{W}_{i,h}}^{2}\leq\frac{4}{k\mathcal{U}}\). Then we have

\[M_{k,h} =\sum_{i=1}^{k}\left(\mathbf{G}_{i,h}(\bm{\theta}_{h}^{i})-\mathbf{ G}_{i,h}(\bm{\theta}_{h}^{i})\right)^{\top}(\bm{\theta}_{h}^{i}-\bm{\theta}_{h}^{*})\] \[=\sum_{i=1}^{k}\sum_{s^{\prime}\in\mathcal{S}_{i,h}}\left(y_{h}^{ i}(s^{\prime})-\mathbb{E}[y_{h}^{i}(s^{\prime})]\right)\bm{\varphi}_{i,h,s^{ \prime}}^{\top}(\bm{\theta}_{h}^{i}-\bm{\theta}_{h}^{*})\] \[=\sum_{i=1}^{k}\sum_{s^{\prime}\in\mathcal{S}_{i,h}}\left(y_{h}^{ i}(s^{\prime})-P_{\bm{\theta}_{h}^{*}}(s^{\prime}\mid s_{h}^{i},a_{h}^{i}) \right)\bm{\varphi}_{i,h,s^{\prime}}^{\top}(\bm{\theta}_{h}^{i}-\bm{\theta}_{ h}^{*})\] \[\leq\sum_{i=1}^{k}\sum_{s^{\prime}\in\mathcal{S}_{i,h}}|\bm{ \varphi}_{i,h,s^{\prime}}^{\top}(\bm{\theta}_{h}^{i}-\bm{\theta}_{h}^{*})|\] \[\leq\sqrt{k\mathcal{U}\sum_{i=1}^{k}\sum_{s^{\prime}\in\mathcal{ S}_{i,h}}\left(\bm{\varphi}_{i,h,s^{\prime}}^{\top}(\bm{\theta}_{h}^{i}-\bm{ \theta}_{h}^{*})\right)^{2}}\] \[=\sqrt{k\mathcal{U}\frac{B_{k,h}}{2}}\] \[\leq\sqrt{2}\,.\]

**Case 2 : \(B_{k,h}>\frac{4}{k\mathcal{U}}\)**

Suppose that \(B_{k,h}=2\sum_{i=1}^{k}\|\bm{\theta}_{h}^{i}-\bm{\theta}_{h}^{*}\|_{\mathbf{W} _{i,h}}^{2}>\frac{4}{k\mathcal{U}}\). Then, we have both a lower and upper bound for \(B_{k,h}\) as follows:

\[\frac{4}{k\mathcal{U}}<B_{k,h}\leq 2\sum_{i=1}^{k}\sum_{s^{\prime}\in \mathcal{S}_{i,h}}\|\bm{\varphi}_{i,h,s^{\prime}}\|_{2}^{2}\|\bm{\theta}_{h}^ {i}-\bm{\theta}_{h}^{*}\|_{2}^{2}\leq 8k\mathcal{U}L_{\bm{\varphi}}^{2}L_{\bm{ \theta}}^{2}\,.\]

Then by the peeling process from Bartlett et al. [12], for any \(\eta_{k}>0\), we have

\[\mathbb{P}\left(M_{k,h}\geq 2\sqrt{\eta_{k}B_{k,h}}+\frac{16 \eta_{k}L_{\bm{\varphi}}L_{\bm{\theta}}}{3}\right)\] \[=\mathbb{P}\left(M_{k,h}\geq 2\sqrt{\eta_{k}B_{k,h}}+\frac{16 \eta_{k}L_{\bm{\varphi}}L_{\bm{\theta}}}{3},\frac{4}{k\mathcal{U}}<B_{k,h}\leq 8 k\mathcal{U}L_{\bm{\varphi}}^{2}L_{\bm{\theta}}^{2}\right)\] \[=\mathbb{P}\left(M_{k,h}\geq 2\sqrt{\eta_{k}B_{k,h}}+\frac{16 \eta_{k}L_{\bm{\varphi}}L_{\bm{\theta}}}{3},\frac{4}{k\mathcal{U}}<B_{k,h}\leq 8 k\mathcal{U}L_{\bm{\varphi}}^{2}L_{\bm{\theta}}^{2},\Sigma_{k,h}\leq B_{k,h}\right)\] \[\leq\sum_{j=1}^{m}\mathbb{P}\left(M_{k,h}\geq 2\sqrt{\eta_{k}B_{k,h} }+\frac{16\eta_{k}L_{\bm{\varphi}}L_{\bm{\theta}}}{3},\frac{4\cdot 2^{j-1}}{k \mathcal{U}}<B_{k,h}\leq\frac{4\cdot 2^{j}}{k\mathcal{U}},\Sigma_{k,h}\leq B_{k,h}\right)\] \[\leq\sum_{j=1}^{m}\underbrace{\mathbb{P}\left(M_{k,h}\geq\sqrt{ \eta_{k}\frac{8\cdot 2^{j}}{k\mathcal{U}}}+\frac{16\eta_{k}L_{\bm{\varphi}}L_{\bm{ \theta}}}{3},\Sigma_{k,h}\leq\frac{4\cdot 2^{j}}{k\mathcal{U}}\right)}_{I_{j}},\] (23)

where \(m=1+\lceil 2\log_{2}k\mathcal{U}L_{\bm{\varphi}}L_{\bm{\theta}}\rceil\). For \(I_{j}\), note that from (20) we have

\[\left|\left(\bar{\mathbf{G}}_{i,h}(\bm{\theta}_{h}^{i})-\mathbf{G}_{i,h}(\bm{ \theta}_{h}^{i})\right)^{\top}(\bm{\theta}_{h}^{i}-\bm{\theta}_{h}^{*})\right| \leq 8L_{\bm{\varphi}}L_{\bm{\theta}}\,.\]By Freedman's inequality (Lemma 29), we have

\[\mathbb{P}\left(M_{k,h}\geq\sqrt{\eta_{k}\frac{8\cdot 2^{j}}{k \mathcal{U}}}+\frac{16\eta_{k}L_{\bm{\varphi}}L_{\bm{\theta}}}{3},\Sigma_{k,h} \leq\frac{4\cdot 2^{j}}{k\mathcal{U}}\right)\] \[\leq\exp\left(\frac{-\left(\sqrt{\eta_{k}\frac{8\cdot 2^{j}}{k \mathcal{U}}}+\frac{16\eta_{k}L_{\bm{\varphi}}L_{\bm{\theta}}}{3}\right)^{2}}{ \frac{8\cdot 2^{j}}{k\mathcal{U}}+\frac{2}{3}\cdot 8L_{\bm{\varphi}}L_{\bm{ \theta}}\left(\sqrt{\eta_{k}\frac{8\cdot 2^{j}}{k\mathcal{U}}}+\frac{16\eta_{k}L_{\bm{ \varphi}}L_{\bm{\theta}}}{3}\right)}\right)\] \[=\exp\left(\frac{-\eta_{k}\left(\sqrt{\frac{8\cdot 2^{j}}{k \mathcal{U}}}+\frac{16\eta_{k}L_{\bm{\varphi}}L_{\bm{\theta}}}{3}\right)^{2}}{ \frac{8\cdot 2^{j}}{k\mathcal{U}}+\frac{16L_{\bm{\varphi}}L_{\bm{\theta}}}{3} \sqrt{\eta_{k}\frac{8\cdot 2^{j}}{k\mathcal{U}}}+\frac{16^{2}\eta_{k}L_{\bm{ \varphi}}^{2}L_{\bm{\theta}}^{2}}{3^{2}}}\right)\] \[\leq\exp\left(\frac{-\eta_{k}\left(\sqrt{\frac{8\cdot 2^{j}}{k \mathcal{U}}}+\frac{16\sqrt{\eta_{k}}L_{\bm{\varphi}}L_{\bm{\theta}}}{3} \right)^{2}}{\frac{8\cdot 2^{j}}{k\mathcal{U}}+\frac{32L_{\bm{\varphi}}L_{\bm{ \theta}}}{3}\sqrt{\eta_{k}\frac{8\cdot 2^{j}}{k\mathcal{U}}}+\frac{16^{2}\eta_{k}L_{ \bm{\varphi}}^{2}L_{\bm{\theta}}^{2}}{3^{2}}}\right)\] \[=\exp(-\eta_{k})\,.\] (24)

By substituting Eq. (24) into Eq. (23), we have

\[\mathbb{P}\left(M_{k,h}\geq 2\sqrt{\eta_{k}B_{k,h}}+\frac{16\eta_{k}L_{ \bm{\varphi}}L_{\bm{\theta}}}{3}\right)\leq m\exp(-\eta_{k})\,.\]

Then, combining with the result of Case 1 & 2, letting \(\eta_{k}=\log\frac{m}{\delta/k^{2}}=\log\frac{(1+\lceil 2\log_{2}k \mathcal{U}L_{\bm{\theta}}\rceil)k^{2}}{\delta}\) and taking union bound over \(k\), with probability at least \(1-\delta\), we have

\[M_{k,h}\leq 2\sqrt{2\eta_{k}\sum_{i=1}^{k}\|\bm{\theta}_{h}^{i}-\bm{ \theta}_{h}^{*}\|_{\mathbf{W}_{i,h}}^{2}}+\frac{16\eta_{k}L_{\bm{\varphi}}L_{ \bm{\theta}}}{3}+\sqrt{2}\,.\] (25)

By applying \(2\sqrt{ab}\leq a+b\) to the first term on the right hand side, we have

\[2\sqrt{2\eta_{k}\sum_{i=1}^{k}\|\bm{\theta}_{h}^{i}-\bm{\theta }_{h}^{*}\|_{\mathbf{W}_{i,h}}^{2}}\leq\frac{8\eta_{k}}{\kappa}+\frac{\kappa} {4}\sum_{i=1}^{k}\|\bm{\theta}_{h}^{i}-\bm{\theta}_{h}^{*}\|_{\mathbf{W}_{i,h }}^{2}\,.\] (26)

Combining the results of Eq. (25) & Eq. (26), we have

\[M_{k,h} =\sum_{i=1}^{k}\left(\bar{\mathbf{G}}_{i,h}(\bm{\theta}_{h}^{i}) -\mathbf{G}_{i,h}(\bm{\theta}_{h}^{i})\right)^{\top}(\bm{\theta}_{h}^{i}-\bm{ \theta}_{h}^{*})\] \[\leq\frac{\kappa}{4}\sum_{i=1}^{k}\|\bm{\theta}_{h}^{i}-\bm{ \theta}_{h}^{*}\|_{\mathbf{W}_{i,h}}^{2}+\left(\frac{16L_{\bm{\varphi}}L_{\bm {\theta}}}{3}+\frac{8}{\kappa}\right)\log\frac{\left(1+\lceil 2\log_{2}k \mathcal{U}L_{\bm{\varphi}}L_{\bm{\theta}}\rceil\right)k^{2}}{\delta}+\sqrt{2 }\,.\]

#### c.1.2 Proof of Lemma 3

Proof of Lemma 3.: By definition of \(\mathbf{V}_{t}\), we have

\[\det(\mathbf{V}_{t+1}) =\det\left(\mathbf{V}_{t}+\sum_{i\in S_{t}}\mathbf{x}_{t,i}\mathbf{ x}_{t,i}^{\top}\right)\] \[=\det(\mathbf{V}_{t})\det\left(\mathbf{I}_{d}+\sum_{i\in S_{t}} \mathbf{V}_{t}^{-\frac{1}{2}}\mathbf{x}_{t,i}\mathbf{x}_{t,i}^{\top}\mathbf{V }_{t}^{-\frac{1}{2}}\right)\] \[=\det(\mathbf{V}_{t})\left(1+\sum_{i\in S_{t}}\|\mathbf{x}_{t,i} \|_{\mathbf{V}_{t}^{-1}}^{2}\right)\] \[=\det(\lambda\mathbf{I}_{d})\prod_{\tau=1}^{t}\left(1+\sum_{i\in S _{\tau}}\|\mathbf{x}_{\tau,i}\|_{\mathbf{V}_{\tau}^{-1}}^{2}\right)\] \[\geq\det(\lambda\mathbf{I}_{d})\prod_{\tau=1}^{t}\left(1+\max_{i \in S_{\tau}}\|\mathbf{x}_{\tau,i}\|_{\mathbf{V}_{t}^{-1}}^{2}\right)\,.\] (27)

Since \(\lambda\geq L^{2}\), we have

\[\max_{i\in S_{\tau}}\|\mathbf{x}_{\tau,i}\|_{\mathbf{V}_{\tau}^{-1}}^{2}\leq \frac{L^{2}}{\lambda}\leq 1\,.\]

Since for any \(z\in[0,1]\), it follows that \(z\leq 2\log(1+z)\). Hence, we have

\[\sum_{t=1}^{T}\max_{i\in S_{t}}\|\mathbf{x}_{t,i}\|_{\mathbf{V}_{ t}^{-1}}^{2} \leq 2\sum_{t=1}^{T}\log\left(1+\max_{i\in S_{t}}\|\mathbf{x}_{t,i} \|_{\mathbf{V}_{t}^{-1}}^{2}\right)\] \[=2\log\prod_{t=1}^{T}\left(1+\max_{i\in S_{t}}\|\mathbf{x}_{t,i} \|_{\mathbf{V}_{t}^{-1}}^{2}\right)\] \[\leq 2\log\frac{\det(\mathbf{V}_{T+1})}{\det(\lambda\mathbf{I}_{ d})}\] \[\leq 2d\log\left(1+\frac{TKL^{2}}{d\lambda}\right)\,,\]

where the second inequality comes from Eq. (27) and the last inequality follows by the determinant-trace inequality (Lemma 28). 

### Bound on Prediction Error

In this section, we provide the bound on the prediction error induced by estimated transition core \(\bm{\theta}_{h}^{k}\).

**Lemma 4** (Bound on Prediction Error).: _For any \(\delta\in(0,1)\), suppose that Lemma 1 holds. Then for any \((s,a)\in\mathcal{S}\times\mathcal{A}\), we have_

\[|\Delta_{h}^{k}(s,a)|\leq H\alpha_{k}(\delta)\|\hat{\bm{\varphi}}_{k,h}(s,a)\| _{\mathbf{A}_{h,h}^{-1}}\,.\]

Proof of Lemma 4.: Recall that

\[\Delta_{h}^{k}(s,a) =\sum_{s^{\prime}\in\mathcal{S}_{s,a}}\left(P_{\bm{\theta}_{h}^ {k}}(s^{\prime}\mid s,a)-P_{\bm{\theta}_{h}^{*}}(s^{\prime}\mid s,a)\right)V_ {h+1}^{k}(s^{\prime})\] \[=\sum_{s^{\prime}\in\mathcal{S}_{s,a}}\frac{\exp(\bm{\varphi}_{s,a,s^{\prime}}^{\top}\bm{\theta}_{h}^{k})V_{h+1}^{k}(s^{\prime})}{\sum_{\widetilde {s}\in\mathcal{S}_{s,a}}\exp(\bm{\varphi}_{s,a,\widetilde{s}}^{\top}\bm{ \theta}_{h}^{k})}-\sum_{s^{\prime}\in\mathcal{S}_{s,a}}\frac{\exp(\bm{\varphi }_{s,a,s^{\prime}}^{\top}\bm{\theta}_{h}^{*})V_{h+1}^{k}(s^{\prime})}{\sum_{ \widetilde{s}\in\mathcal{S}_{s,a}}\exp(\bm{\varphi}_{s,a,\widetilde{s}}^{\top }\bm{\theta}_{h}^{*})}\,.\]Then by the mean value theorem, there exists \(\bar{\bm{\theta}}=\rho\bm{\theta}_{h}^{k}+(1-\rho)\bm{\theta}_{h}^{*}\) for some \(\rho\in[0,1]\) satisfying that

\[\Delta_{h}^{k}(s,a) =\frac{\left(\sum_{s^{\prime}\in\mathcal{S}_{s,a}}\exp(\bm{\varphi }_{s,a,s^{\prime}}^{\top}\bar{\bm{\theta}})V_{h+1}^{k}(s^{\prime})\bm{\varphi} _{s,a,s^{\prime}}^{\top}(\bm{\theta}_{h}^{k}-\bm{\theta}_{h}^{*})\right)\left( \sum_{\widetilde{s}\in\mathcal{S}_{s,a}}\exp(\bm{\varphi}_{s,a,\widetilde{s}}^ {\top}\bar{\bm{\theta}})\right)}{\left(\sum_{\widetilde{s}\in\mathcal{S}_{s,a} }\exp(\bm{\varphi}_{s,a,\widetilde{s}}^{\top}\bar{\bm{\theta}})\right)^{2}}\] \[\quad-\frac{\left(\sum_{s^{\prime}\in\mathcal{S}_{s,a}}\exp(\bm{ \varphi}_{s,a,s^{\prime}}^{\top}\bar{\bm{\theta}})V_{h+1}^{k}(s^{\prime}) \right)\left(\sum_{\widetilde{s}\in\mathcal{S}_{s,a}}\exp(\bm{\varphi}_{s,a, \widetilde{s}}^{\top}\bar{\bm{\theta}})\bm{\varphi}_{s,a,\widetilde{s}}^{ \top}\left(\bm{\theta}_{h}^{k}-\bm{\theta}_{h}^{*}\right)\right)}{\left(\sum_ {\widetilde{s}\in\mathcal{S}_{s,a}}\exp(\bm{\varphi}_{s,a,\widetilde{s}}^{\top }\bar{\bm{\theta}})\right)^{2}}\] \[=\sum_{s^{\prime}\in\mathcal{S}_{s,a}}P_{\bar{\bm{\theta}}}(s^{ \prime}\mid s,a)V_{h+1}^{k}(s^{\prime})\bm{\varphi}_{s,a,s^{\prime}}^{\top}( \bm{\theta}_{h}^{k}-\bm{\theta}_{h}^{*})\] \[\quad-\left(\frac{\sum_{s^{\prime}\in\mathcal{S}_{s,a}}\exp(\bm{ \varphi}_{s,a,s^{\prime}}^{\top}\bar{\bm{\theta}})V_{h+1}^{k}(s^{\prime})}{ \sum_{\widetilde{s}\in\mathcal{S}_{s,a}}\exp(\bm{\varphi}_{s,a,\widetilde{s}}^ {\top}\bar{\bm{\theta}})}\right)\sum_{s^{\prime}\in\mathcal{S}_{s,a}}P_{\bar{ \bm{\theta}}}(s^{\prime}\mid s,a)\bm{\varphi}_{s,a,s^{\prime}}^{\top}(\bm{ \theta}_{h}^{k}-\bm{\theta}_{h}^{*})\] \[=\sum_{s^{\prime}\in\mathcal{S}_{s,a}}\left(V_{h+1}^{k}(s^{\prime })-\frac{\sum_{s^{\prime}\in\mathcal{S}_{s,a}}\exp(\bm{\varphi}_{s,a, \widetilde{s}}^{\top}\bar{\bm{\theta}})V_{h+1}^{k}(s^{\prime})}{\sum_{ \widetilde{s}\in\mathcal{S}_{s,a}}\exp(\bm{\varphi}_{s,a,\widetilde{s}}^{\top }\bar{\bm{\theta}})}\right)P_{\bar{\bm{\theta}}}(s^{\prime}\mid s,a)\bm{ \varphi}_{s,a,s^{\prime}}^{\top}(\bm{\theta}_{h}^{k}-\bm{\theta}_{h}^{*})\,.\]

Since \(V_{h}^{k}(s^{\prime})\leq H\) for all \(s^{\prime}\in\mathcal{S},k\in[K]\), and \(h\in[H]\), we have

\[\Delta_{h}^{k}(s,a) \leq H\sum_{s^{\prime}\in\mathcal{S}_{s,a}}P_{\bar{\bm{\theta}}}(s ^{\prime}\mid s,a)\bm{\varphi}_{s,a,s^{\prime}}^{\top}(\bm{\theta}_{h}^{k}- \bm{\theta}_{h}^{*})\] \[\leq H\max_{s^{\prime}\in\mathcal{S}_{s,a}}|\bm{\varphi}_{s,a,s^{ \prime}}^{\top}(\bm{\theta}_{h}^{k}-\bm{\theta}_{h}^{*})|\] \[\leq H\max_{s^{\prime}\in\mathcal{S}_{s,a}}\|\bm{\varphi}_{s,a,s^{ \prime}}\|_{\mathbf{A}_{h,h}^{-1}}\|\bm{\theta}_{h}^{k}-\bm{\theta}_{h}^{*}\|_ {\mathbf{A}_{h,h}}\] \[\leq H\alpha_{k}(\delta)\|\hat{\bm{\varphi}}_{k,h}(s,a)\|_{ \mathbf{A}_{k,h}^{-1}}\,,\]

where the second inequality comes from the fact that \(P_{\bar{\bm{\theta}}}(s^{\prime}\mid s,a)\leq 1\) is a multinomial probability, the third inequality holds due to the Cauchy-Schwarz inequality, and the last inequality follows from Lemma 1 and the definition of \(\hat{\bm{\varphi}}_{k,h}\), i.e., \(\hat{\bm{\varphi}}_{k,h}(s,a):=\bm{\varphi}(s,a,\hat{s})\) for \(\hat{s}=\operatorname*{argmax}_{s^{\prime}\in\mathcal{S}_{s,a}}\|\bm{\varphi} (s,a,s^{\prime})\|_{\mathbf{A}_{k,h}^{-1}}\). 

### Good Events with High Probability

**Lemma 5** (Good event probability).: _For any \(K\in\mathbb{N}\) and \(\delta\in(0,1)\), the good event \(\mathcal{G}(K,\delta^{\prime})\) holds with probability at least \(1-\delta\) where \(\delta^{\prime}=\delta/(2KH)\)._

Proof of Lemma 5.: For any \(\delta^{\prime}\in(0,1)\), we have

\[\mathcal{G}(K,\delta^{\prime})=\bigcap_{k\leq K}\bigcap_{h\leq H}\mathcal{G}_{k, h}(\delta^{\prime})=\bigcap_{k\leq K}\bigcap_{h\leq H}\left\{\mathcal{G}_{k,h}^{ \Delta}(\delta^{\prime})\cap\mathcal{G}_{k,h}^{\bm{\xi}}(\delta^{\prime}) \right\}\,.\]

On the other hand, for any \((k,h)\in[K]\times[H]\), by Lemma 30, \(\mathcal{G}_{k,h}^{\bm{\xi}}(\delta^{\prime})\) holds with probability at least \(1-\delta^{\prime}\). Then, for \(\delta^{\prime}=\delta/(2KH)\) by taking union bound, we have the desired result as follows:

\[\mathbb{P}(\mathcal{G}(K,\delta^{\prime}))\geq(1-\delta^{\prime})^{2KH}\geq 1-2 KH\delta^{\prime}=1-\delta\,.\]

### Stochastic Optimism

**Lemma 6** (Stochastic optimism).: _For any \(\delta\) with \(0<\delta<\Phi(-1)/2\), let \(\sigma_{k}=H\alpha_{k}(\delta)=\widetilde{\mathcal{O}}(H\sqrt{d})\). If we take multiple sample size \(M=\lceil 1-\frac{\log H}{\log\Phi(1)}\rceil\), then for any \(k\in[K]\), we have_

\[\mathbb{P}\left((V_{1}^{k}-V_{1}^{*})(s_{1}^{k})\geq 0\mid s_{1}^{k},\mathcal{F}_{k} \right)\geq\Phi(-1)/2\,.\]Proof of lemma 6.: Before presenting the proof, we introduce the following lemmas.

**Lemma 7**.: _For any \(k\in[K]\), it holds_

\[V_{1}^{k}(s_{1}^{k})-V_{1}^{*}(s_{1}^{k})\geq\mathbb{E}_{\pi^{*}}\left[\sum_{h=1} ^{H}-\iota_{h}^{k}(x_{h},a_{h})\mid x_{1}=s_{1}^{k}\right]\,,\]

_where \(\iota_{h}^{k}(s,a):=r(s,a)+P_{h}V_{h+1}^{k}(s,a)-Q_{h}^{k}(s,a)\)._

**Lemma 8**.: _Let \(\delta\in(0,1)\) be given. For any \((k,h)\in[K]\times[H]\), let \(\sigma_{k}=H\alpha_{k}(\delta)\). If we define the event \(\mathcal{G}_{k,h}^{\Delta}(\delta)\) as_

\[\mathcal{G}_{k,h}^{\Delta}(\delta):=\left\{\Delta_{h}^{k}(s,a)\leq H\alpha_{k} (\delta)\|\hat{\boldsymbol{\varphi}}_{k,h}(s,a)\|_{\mathbf{A}_{k,h}^{-1}} \right\}\,,\]

_then conditioned on \(\mathcal{G}_{k,h}^{\Delta}(\delta)\), for any \((s,a)\in\mathcal{S}\times\mathcal{A}\), we have_

\[\mathbb{P}\left(-\iota_{h}^{k}(s,a)\geq 0\mid\mathcal{G}_{k,h}^{\Delta}( \delta)\right)\geq 1-\Phi(1)^{M}\,.\]

**Lemma 9**.: _Let \(\delta\in(0,1)\) be given. For any \((h,k)\in[H]\times[K]\), let \(\sigma_{k}=H\alpha_{k}(\delta)\). If we take multiple sample size \(M=\lceil 1-\frac{\log H}{\log\Phi(1)}\rceil\), then conditioned on the event \(\mathcal{G}_{k}^{\Delta}(\delta):=\bigcap_{h\in[H]}\mathcal{G}_{k,h}^{\Delta}(\delta)\), we have_

\[\mathbb{P}\left(-\iota_{h}^{k}(s_{h},a_{h})\geq 0,\forall h\in[H]\mid\mathcal{G}_ {k}^{\Delta}(\delta)\right)\geq\Phi(-1)\,.\]

Now, we define the event of the estimated value function being optimistic at the start of the \(k\)-th episode as

\[\mathcal{X}_{k}:=\left\{(V_{1}^{k}-V_{1}^{*})(s_{1}^{k})\geq 0\right\}\,.\]

Then for the event \(\mathcal{G}_{k}(\delta)=:\mathcal{G}_{k}\), we have

\[\mathbb{P}(\mathcal{X}_{k}) =1-\mathbb{P}(\mathcal{X}_{k}^{\mathsf{c}})\] \[=1-\mathbb{P}(\mathcal{X}_{k}^{\mathsf{c}}\cap\mathcal{G}_{k})- \mathbb{P}(\mathcal{X}_{k}^{\mathsf{c}}\cap\mathcal{G}_{k}^{\mathsf{c}})\] \[\geq 1-\mathbb{P}(\mathcal{X}_{k}^{\mathsf{c}}\cap\mathcal{G}_{k})- \mathbb{P}(\mathcal{G}_{k}^{\mathsf{c}})\] \[\geq 1-\mathbb{P}(\mathcal{X}_{k}^{\mathsf{c}}\cap\mathcal{G}_{k})-\delta\]

where the last inequality comes from lemma 5.

On the other hand, by Lemma 7, we have

\[V_{1}^{k}(s_{1}^{k})-V_{1}^{*}(s_{1}^{k}) \geq\mathbb{E}_{\pi^{*}}\left[\sum_{h=1}^{H}-\iota_{h}^{k}(x_{h}, a_{h})\mid x_{1}=s_{1}^{k}\right]\] \[=\sum_{h=1}^{H}\mathbb{E}_{\pi^{*}}\left[-\iota_{h}^{k}(x_{h},a_{h })\mid x_{1}=s_{1}^{k}\right]\,.\]

If we define an event

\[\mathcal{Y}_{k}=\left\{\sum_{h=1}^{H}\mathbb{E}_{\pi^{*}}\left[-\iota_{h}^{k}( x_{h},a_{h})\mid x_{1}=s_{1}^{k}\right]\geq 0\right\}\,,\]

then, by Lemma 9, we have

\[\mathbb{P}(\mathcal{Y}_{k}\mid\mathcal{G}_{k})\geq\Phi(-1) \iff\mathbb{P}(\mathcal{Y}_{k}^{\mathsf{c}}\mid\mathcal{G}_{k})\leq 1- \Phi(-1)\] \[\implies\mathbb{P}(\mathcal{Y}_{k}^{\mathsf{c}}\cap\mathcal{G}_{k}) \leq(1-\Phi(-1))\,\mathbb{P}(\mathcal{G}_{k})\leq 1-\Phi(-1)\]

Note that since \(\mathcal{X}_{k}^{\mathsf{c}}\cap\mathcal{G}_{k}\subset\mathcal{Y}_{k}^{\mathsf{ c}}\cap\mathcal{G}_{k}\), we can conclude that

\[\mathbb{P}(\mathcal{X}_{k}) \geq 1-\mathbb{P}(\mathcal{X}_{k}^{\mathsf{c}}\cap\mathcal{G}_{k})-\delta\] \[\geq 1-\mathbb{P}(\mathcal{Y}_{k}^{\mathsf{c}}\cap\mathcal{G}_{k})-\delta\] \[\geq 1-(1-\Phi(-1))-\delta\] \[=\Phi(-1)-\delta\] \[\geq\Phi(-1)/2\]

where the last inequality comes from the choice of \(\delta\). 

In the following, we provide all the proofs of the lemmas used to prove Lemma 6.

#### c.4.1 Proof of Lemma 7

Proof of lemma 7.: In this proof, we use \(x_{h}^{k}\) as the states sampled under the \(\pi^{*}\) to distinguish with \(s_{h}^{k}\). Since we have,

\[V_{1}^{k}(s_{1}^{k})-V_{1}^{*}(s_{1}^{k})\] \[\geq Q_{1}^{k}(s_{1}^{k},\pi^{*}(s_{1}^{k}))-Q_{1}^{*}(s_{1}^{k}, \pi^{*}(s_{1}^{k}))\] \[=r(s_{1}^{k},\pi^{*}(s_{1}^{k}))+P_{1}V_{2}^{k}(s_{1}^{k},\pi^{*}( s_{1}^{k}))-\iota_{1}^{k}(s_{1}^{k},\pi^{*}(s_{1}^{k}))-\big{(}r(s_{1}^{k},\pi^{*}( s_{1}^{k}))+P_{1}V_{2}^{*}(s_{1}^{k},\pi^{*}(s_{1}^{k}))\big{)}\] \[=P_{1}(V_{2}^{k}-V_{2}^{*})(s_{1}^{k},\pi^{*}(s_{1}^{k}))-\iota_{1 }^{k}(s_{1}^{k},\pi^{*}(s_{1}^{k}))\] \[=\mathbb{E}_{x|s_{1}^{k},\pi^{*}(s_{1}^{k})}\left[(V_{2}^{k}-V_{2 }^{*})(x)\right]-\iota_{1}^{k}(s_{1}^{k},\pi^{*}(s_{1}^{k}))\] \[\geq\mathbb{E}_{x_{2}^{k}|s_{1}^{k},\pi^{*}(s_{1}^{k})}\left[(Q_{2 }^{k}-Q_{2}^{*})(x_{2}^{k},\pi^{*}(x_{2}^{k}))\right]-\iota_{1}^{k}(s_{1}^{k}, \pi^{*}(s_{1}^{k}))\] \[=\mathbb{E}_{x_{2}^{k}\sim s_{1}^{k},\pi^{*}(s_{1}^{k})}\left[ \mathbb{E}_{x|x_{2}^{k},\pi^{*}(x_{2}^{k})}\left[(V_{3}^{k}-V_{3}^{*})(x) \right]-\iota_{2}^{k}(x_{2}^{k},\pi^{*}(x_{2}^{k}))\right]-\iota_{1}^{k}(s_{1} ^{k},\pi^{*}(s_{1}^{k}))\] \[=\underbrace{\mathbb{E}_{x_{2}^{k}\sim s_{1}^{k},\pi^{*}(s_{1}^{k })}\left[\mathbb{E}_{x|x_{2}^{k},\pi^{*}(x_{2}^{k})}\left[(V_{3}^{k}-V_{3}^{*} )(x)\right]\right]}_{\mathbb{E}_{x_{3}^{k}\sim\pi^{*}|s_{1}^{k}}\left[(V_{3}^{ k}-V_{3}^{*})(x_{3}^{k})\right]}\] \[\quad-\mathbb{E}_{x_{2}^{k}\sim s_{1}^{k},\pi^{*}(s_{1}^{k})} \left[\iota_{2}^{k}(x_{2}^{k},\pi^{*}(x_{2}^{k}))\right]-\iota_{1}^{k}(s_{1}^{ k},\pi^{*}(s_{1}^{k}))\]

then by applying this argument recursively, we finally have

\[V_{1}^{k}(s_{1}^{k})-V_{1}^{*}(s_{1}^{k})\geq\mathbb{E}_{\pi^{*}}\left[\sum_{h =1}^{H}-\iota_{h}^{k}(x_{h},a_{h})\mid x_{1}=s_{1}^{k}\right]\,.\]

#### c.4.2 Proof of Lemma 8

Proof of Lemma 8.: Since we have

\[-\iota_{h}^{k}(s,a) =Q_{h}^{k}(s,a)-\big{(}r(s,a)+P_{h}V_{h+1}^{k}(s,a)\big{)}\] \[=\min\left\{r(s,a)+\sum_{s^{\prime}\in\mathcal{S}_{s,a}}P_{ \boldsymbol{\theta}_{h}^{k}}(s^{\prime}\mid s,a)V_{h+1}^{k}(s^{\prime})+\max_{ m\in[M]}\hat{\boldsymbol{\varphi}}_{k,h}(s,a)^{\top}\xi_{k,h}^{(m)},H\right\}\] \[\quad-\big{(}r(s,a)+P_{h}V_{h+1}^{k}(s,a)\big{)}\] \[\geq\min\left\{\sum_{s^{\prime}\in\mathcal{S}_{s,a}}P_{ \boldsymbol{\theta}_{h}^{k}}(s^{\prime}\mid s,a)V_{h+1}^{k}(s^{\prime})+\max_{ m\in[M]}\hat{\boldsymbol{\varphi}}_{k,h}(s,a)^{\top}\xi_{k,h}^{(m)}-P_{h}V_{h+1}^{k}(s,a),0 \right\}\,,\]

it is enough to show that

\[\sum_{s^{\prime}\in\mathcal{S}_{s,a}}P_{\boldsymbol{\theta}_{h}^{k}}(s^{\prime }\mid s,a)V_{h+1}^{k}(s^{\prime})+\max_{m\in[M]}\hat{\boldsymbol{\varphi}}_{k,h }(s,a)^{\top}\xi_{k,h}^{(m)}-P_{h}V_{h+1}^{k}(s,a)\geq 0\]

at least with constant probability.

On the other hand, under the event \(\mathcal{G}_{k,h}(\delta)\), by Lemma 4 we have

\[\sum_{s^{\prime}\in\mathcal{S}_{s,a}}P_{\boldsymbol{\theta}_{h}^{k}} (s^{\prime}\mid s,a)V_{h+1}^{k}(s^{\prime})+\max_{m\in[M]}\hat{\boldsymbol{ \varphi}}_{k,h}(s,a)^{\top}\xi_{k,h}^{(m)}-P_{h}V_{h+1}^{k}(s,a)\] \[\geq\max_{m\in[M]}\hat{\boldsymbol{\varphi}}_{k,h}(s,a)^{\top} \xi_{k,h}^{(m)}-H\alpha_{k}(\delta)\|\hat{\boldsymbol{\varphi}}_{k,h}(s,a)\|_{ \boldsymbol{A}_{k,h}^{-1}}\,.\]

Now, for \(\forall m\in[M]\), since \(\xi_{k,h}^{(m)}\sim\mathcal{N}(\boldsymbol{0}_{d},\sigma_{k}^{2}\mathbf{A}_{k,h}^{-1})\), we have

\[\hat{\boldsymbol{\varphi}}_{k,h}(s,a)^{\top}\xi_{k,h}^{(m)}\sim\mathcal{N}(0, \sigma_{k}^{2}\|\hat{\boldsymbol{\varphi}}_{k,h}(s,a)\|_{\mathbf{A}_{k,h}^{-1}}^{ 2})\,,\]which means,

\[\mathbb{P}\left(\hat{\bm{\varphi}}_{k,h}(s,a)^{\top}\xi_{k,h}^{(m)}\geq H\alpha_{ k}(\delta)\|\hat{\bm{\varphi}}_{k,h}(s,a)\|_{\mathbf{A}_{k,h}^{-1}}\right)\geq\Phi(-1)\,,\]

by setting \(\sigma_{k}=H\alpha_{k}(\delta)\). Then, finally we have the desired results as follows:

\[\mathbb{P}\left(-\iota_{h}^{k}(s,a)\geq 0\mid\mathcal{G}_{k,h}^{ \Delta}(\delta)\right)\] \[\geq\mathbb{P}\left(\max_{m\in[M]}\hat{\bm{\varphi}}_{k,h}(s,a)^{ \top}\xi_{k,h}^{(m)}\geq H\alpha_{k}(\delta)\|\hat{\bm{\varphi}}_{k,h}(s,a)\|_ {\mathbf{A}_{k,h}^{-1}}\mid\mathcal{G}_{k,h}^{\Delta}(\delta)\right)\] \[=1-\mathbb{P}\left(\hat{\bm{\varphi}}_{k,h}(s,a)^{\top}\xi_{k,h}^ {(m)}<H\alpha_{k}(\delta)\|\hat{\bm{\varphi}}_{k,h}(s,a)\|_{\mathbf{A}_{k,h}^ {-1}},\forall m\in[M]\mid\mathcal{G}_{k,h}^{\Delta}(\delta)\right)\] \[\geq 1-\left(1-\Phi(-1)\right)^{M}\] \[=1-\Phi(1)^{M}\,.\]

#### c.4.3 Proof of Lemma 9

Proof of Lemma 9.: For each \(h\in[H]\) and \(k\in[K]\), define an event \(\mathcal{E}_{h}^{k}:=\{-\iota_{h}^{k}(s_{h},a_{h})\geq 0\}\) Then it holds

\[\mathbb{P}\left(-\iota_{h}^{k}(s_{h},a_{h})\geq 0,\forall h\in[H] \mid\mathcal{G}_{k}^{\Delta}(\delta)\right) =\mathbb{P}\left(\bigcap_{h=1}^{H}\mathcal{E}_{h}^{k}\mid\mathcal{G }_{k}^{\Delta}(\delta)\right)\] \[=1-\mathbb{P}\left(\bigcup_{h=1}^{H}(\mathcal{E}_{h}^{k})^{ \mathsf{c}}\mid\mathcal{G}_{k}^{\Delta}(\delta)\right)\] \[\geq 1-\sum_{h=1}^{H}\mathbb{P}\left((\mathcal{E}_{h}^{k})^{ \mathsf{c}}\mid\mathcal{G}_{k,h}^{\Delta}(\delta)\right)\] \[\geq 1-H\Phi(1)^{M}\] \[\geq\Phi(-1)\]

where the first inequality uses the union bound, the second inequality comes from the Lemma 8 and the last inequality holds due to the choice of \(M=\lceil 1-\frac{\log H}{\log\Phi(1)}\rceil\). 

### Bound on Estimation Part

We decompose the regret into the estimation part and the pessimism part as follows:

\[\sum_{k=1}^{K}(V_{1}^{*}-V_{1}^{\pi^{k}})(s_{1}^{k})=\sum_{k=1}^{K}\Big{(} \underbrace{V_{1}^{*}-V_{1}^{k}}_{\text{pessimism}}+\underbrace{V_{1}^{k}-V_ {1}^{\pi^{k}}}_{\text{pessimism}}\Big{)}(s_{1}^{k})\,,\]

and we bound these two parts in the following sections, respectively.

**Lemma 10** (Bound on estimation part).: _For any \(\delta\in(0,1)\), if \(\lambda\geq L_{\bm{\varphi}}^{2}\), then with probability at least \(1-\delta/2\), we have_

\[\sum_{k=1}^{K}(V_{1}^{k}-V_{1}^{\pi^{k}})(s_{1}^{k})=\widetilde{\mathcal{O}} \left(\kappa^{-1}d^{\frac{3}{2}}H^{\frac{3}{2}}\sqrt{T}\right)\,.\]

Proof of lemma 10.: For any given \(k\in[K]\),

\[(V_{1}^{k}-V_{1}^{\pi^{k}})(s_{1}^{k}) =(Q_{1}^{k}-Q_{1}^{\pi^{k}})(s_{1}^{k},a_{1}^{k})+\iota_{1}^{k}(s _{1}^{k},a_{1}^{k})-\iota_{1}^{k}(s_{1}^{k},a_{1}^{k})\] \[=(Q_{1}^{k}-Q_{1}^{\pi^{k}})(s_{1}^{k},a_{1}^{k})+P_{1}(V_{2}^{k} -V_{2}^{\pi^{k}})(s_{1}^{k},a_{1}^{k})\] (28) \[\quad+(Q_{1}^{\pi^{k}}-Q_{1}^{k})(s_{1}^{k},a_{1}^{k})-\iota_{1}^ {k}(s_{1}^{k},a_{1}^{k})\] \[=\underbrace{P_{1}(V_{2}^{k}-V_{2}^{\pi^{k}})(s_{1}^{k},a_{1}^{k })-(V_{2}^{k}-V_{2}^{\pi^{k}})(s_{2}^{k})}_{\hat{\zeta}_{1}^{k}}+(V_{2}^{k}-V_ {2}^{\pi^{k}})(s_{2}^{k})-\iota_{1}^{k}(s_{1}^{k},a_{1}^{k})\]where the second equality holds due to the variant of \(t_{h}^{k}(s_{h}^{k},a_{h}^{k})\) as follows:

\[t_{h}^{k}(s_{h}^{k},a_{h}^{k}) =r(s_{h}^{k},a_{h}^{k})+P_{h}V_{h+1}^{k}(s_{h}^{k},a_{h}^{k})-Q_{h}^ {k}(s_{h}^{k},a_{h}^{k})+Q_{h}^{\pi^{k}}(s_{h}^{k},a_{h}^{k})-Q_{h}^{\pi^{k}}(s_{ h}^{k},a_{h}^{k})\] \[=r(s_{h}^{k},a_{h}^{k})+P_{h}V_{h+1}^{k}(s_{h}^{k},a_{h}^{k})-Q_{h} ^{k}(s_{h}^{k},a_{h}^{k})\] \[\quad+Q_{h}^{\pi^{k}}(s_{h}^{k},a_{h}^{k})-\left(r(s_{h}^{k},a_{h} ^{k})+P_{h}V_{h+1}^{\pi^{k}}(s_{h}^{k},a_{h}^{k})\right)\] \[=P_{h}(V_{h+1}^{k}-V_{h+1}^{\pi^{k}})(s_{h}^{k},a_{h}^{k})+(Q_{h} ^{\pi^{k}}-Q_{h}^{k})(s_{h}^{k},a_{h}^{k})\,.\]

Then, by applying this argument recursively for whole horizon, we have

\[(V_{1}^{k}-V_{1}^{\pi^{k}})(s_{1}^{k})=\sum_{h=1}^{H}-\iota_{h}^{k}(s_{h}^{k}, a_{h}^{k})+\sum_{h=1}^{H}\dot{\zeta}_{h}^{k}\,,\] (29)

where \(\dot{\zeta}_{h}^{k}:=P_{h}(V_{h+1}^{k}-V_{h+1}^{\pi^{k}})(s_{h}^{k},a_{h}^{k}) -(V_{h+1}^{k}-V_{h+1}^{\pi^{k}})(s_{h+1}^{k})\).

Let \(\delta^{\prime}=\delta/(8KH)\). By Lemma 5, the good event \(\mathcal{G}(K,\delta^{\prime})\) holds with probability at least \(1-\delta/4\). Then under the event \(\mathcal{G}(K,\delta^{\prime})\), for any \(h\in[H]\) we have

\[-\iota_{h}^{k}(s_{h}^{k},a_{h}^{k})\] \[=Q_{h}^{k}(s_{h}^{k},a_{h}^{k})-\left(r(s_{h}^{k},a_{h}^{k})+P_{ h}V_{h+1}^{k}(s_{h}^{k},a_{h}^{k})\right)\] \[=\min\left\{r(s_{h}^{k},a_{h}^{k})+\sum_{s^{\prime}\in\mathcal{S} _{h,h}}P_{\boldsymbol{\theta}_{h}^{k}}(s^{\prime}\mid s_{h}^{k},a_{h}^{k})V_{h +1}^{k}(s^{\prime})+\max_{m\in[M]}\hat{\boldsymbol{\varphi}}_{k,h}(s_{h}^{k}, a_{h}^{k})^{\top}\xi_{k,h}^{(m)},H\right\}\] \[\quad-\left(r(s_{h}^{k},a_{h}^{k})+P_{h}V_{h+1}^{k}(s_{h}^{k},a_{h }^{k})\right)\] \[\leq\sum_{s^{\prime}\in\mathcal{S}_{k,h}}P_{\boldsymbol{\theta}_{ h}^{k}}(s^{\prime}\mid s_{h}^{k},a_{h}^{k})V_{h+1}^{k}(s^{\prime})+\max_{m\in[M]} \hat{\boldsymbol{\varphi}}_{k,h}(s_{h}^{k},a_{h}^{k})^{\top}\xi_{k,h}^{(m)}-P_ {h}V_{h+1}^{k}(s_{h}^{k},a_{h}^{k})\] \[\leq\left|\sum_{s^{\prime}\in\mathcal{S}_{k,h}}P_{\boldsymbol{ \theta}_{h}^{k}}(s^{\prime}\mid s_{h}^{k},a_{h}^{k})V_{h+1}^{k}(s^{\prime})-P_ {h}V_{h+1}^{k}(s_{h}^{k},a_{h}^{k})\right|+\max_{m\in[M]}\left|\hat{\boldsymbol {\varphi}}_{k,h}(s_{h}^{k},a_{h}^{k})^{\top}\xi_{k,h}^{(m)}\right|\] \[\leq|\Delta_{h}^{k}(s_{h}^{k},a_{h}^{k})|+\max_{m\in[M]}\|\hat{ \boldsymbol{\varphi}}_{k,h}(s_{h}^{k},a_{h}^{k})\|_{\boldsymbol{\Lambda}_{k, h}^{-1}}\|\xi_{k,h}^{(m)}\|_{\boldsymbol{\Lambda}_{k,h}}\] (30) \[\leq\left(H\alpha_{k}(\delta^{\prime})+\gamma_{k}(\delta^{\prime })\right)\|\hat{\boldsymbol{\varphi}}_{k,h}(s_{h}^{k},a_{h}^{k})\|_{\boldsymbol {\Lambda}_{k,h}^{-1}}\,,\] (31)

where (30) comes from the Cauchy-Schwarz inequality and (31) holds due the the Lemma 4 & 30. Then, with probability at least \(1-\delta/4\), we have

\[\sum_{h=1}^{H}-\iota_{h}^{k}(s_{h}^{k},a_{h}^{k})\leq\sum_{h=1}^{H}\left(H \alpha_{k}(\delta^{\prime})+\gamma_{k}(\delta^{\prime})\right)\|\hat{ \boldsymbol{\varphi}}_{k,h}(s_{h}^{k},a_{h}^{k})\|_{\boldsymbol{\Lambda}_{k,h }^{-1}}\,.\] (32)

On the other hand, for \(\dot{\zeta}_{h}^{k}\), we have \(|\dot{\zeta}_{h}^{k}|\leq 2H\) and \(\mathbb{E}[\dot{\zeta}_{h}^{k}\mid\mathcal{F}_{k,h}]=0\), which means \(\{\dot{\zeta}_{h}^{k}\mid\mathcal{F}_{k,h}\}_{k,h}\) is a martingale difference sequence for any \(k\in[K]\) and \(h\in[H]\). Hence, by applying the Azuma-Hoeffding inequality with probability at least \(1-\delta/4\), we have

\[\sum_{k=1}^{K}\sum_{h=1}^{H}\dot{\zeta}_{h}^{k}\leq 2H\sqrt{2KH\log(4/\delta)}\,.\] (33)Combining the results of (32) and (33), with probability at least \(1-\delta/2\), we have

\[(V_{1}^{k}-V_{1}^{\pi^{k}})(s_{1}^{k})\] \[\leq 2H\sqrt{2T\log(4/\delta)}+\sum_{k=1}^{K}\sum_{h=1}^{H}\left(H \alpha_{k}(\delta^{\prime})+\gamma_{k}(\delta^{\prime})\right)\|\hat{\bm{\varphi }}_{k,h}(s_{h}^{k},a_{h}^{k})\|_{\mathbf{A}_{h,h}^{-1}}\] \[\leq 2H\sqrt{2T\log(4/\delta)}+\left(H\alpha_{K}(\delta^{\prime}) +\gamma_{K}(\delta^{\prime})\right)\sum_{k=1}^{K}\sum_{h=1}^{H}\|\hat{\bm{ \varphi}}_{k,h}(s_{h}^{k},a_{h}^{k})\|_{\mathbf{A}_{k,h}^{-1}}\] (34) \[\leq 2H\sqrt{2T\log(4/\delta)}+\left(H\alpha_{K}(\delta^{\prime}) +\gamma_{K}(\delta^{\prime})\right)\sum_{h=1}^{H}\sqrt{K\sum_{k=1}^{K}\|\hat{ \bm{\varphi}}_{k,h}(s_{h}^{k},a_{h}^{k})\|_{\mathbf{A}_{k,h}^{-1}}^{2}}\] (35) \[\leq 2H\sqrt{2T\log(4/\delta)}+\left(H\alpha_{K}(\delta^{\prime}) +\gamma_{K}(\delta^{\prime})\right)\sum_{h=1}^{H}\sqrt{4\kappa^{-1}Kd\log \left(1+\frac{K\mathcal{U}L_{\bm{\varphi}}^{2}}{d\lambda}\right)}\] (36) \[=2H\sqrt{2T\log(4/\delta)}+\left(H\alpha_{K}(\delta^{\prime})+ \gamma_{K}(\delta^{\prime})\right)\sqrt{4\kappa^{-1}THd\log\left(1+\frac{K \mathcal{U}L_{\bm{\varphi}}^{2}}{d\lambda}\right)}\,,\] \[=\widetilde{\mathcal{O}}\left(\kappa^{-1}d^{\frac{3}{2}}H^{\frac {3}{2}}\sqrt{T}+H\sqrt{T}\right)\,,\]

where (34) follows from the fact that both \(\alpha_{k}(\delta)\) and \(\gamma_{k}(\delta)\) are increasing in \(k\), (35) comes from Cauchy-Schwarz inequality and (36) holds by the generalized elliptical potential lemma (Lemma 3). 

### Bound on Pessimism Part

**Lemma 11** (Bound on pessimism).: _For any \(\delta\) with \(0<\delta<\Phi(-1)/2\), let \(\sigma_{k}=H\alpha_{k}(\delta)\). If \(\lambda\geq L_{\bm{\varphi}}^{2}\) and we take multiple sample size \(M=\lceil 1-\frac{\log H}{\log\Phi(1)}\rceil\), then with probability at least \(1-\delta/2\), we have_

\[\sum_{k=1}^{K}(V_{1}^{*}-V_{1}^{k})(s_{1}^{k})=\widetilde{\mathcal{O}}\left( \kappa^{-1}d^{\frac{3}{2}}H^{\frac{3}{2}}\sqrt{T}\right)\,.\]

Proof of lemma 11.: Similar to the techniques used in [73], we show that the difference between the optimal value function \(V_{1}^{*}\) and the estimated value function \(V_{1}^{k}\) can be controlled by constructing an upper bound on \(V_{1}^{*}\) and a lower bound on \(V_{1}^{k}\). In this proof, we consider three kinds of pseudo-noises, \(\bm{\xi},\bm{\xi}\) and \(\bm{\xi}\) that we define later in the proof. Also, for \(\delta^{\prime}=\delta/10\), we denote \(\mathcal{G}(K,\delta^{\prime}),\mathcal{\bar{G}}(K,\delta^{\prime})\) and \(\mathcal{\underline{G}}(K,\delta^{\prime})\) as the good events induced by \(\bm{\xi},\bm{\bar{\xi}}\) and \(\bm{\xi}\) respectively. From now on, we denote \(G(K,\delta^{\prime})\) by the event \(\mathcal{G}(K,\delta^{\prime})\cap\mathcal{\bar{G}}(K,\delta^{\prime})\cap \mathcal{\underline{G}}(K,\delta^{\prime})\). Then, by Lemma 5, the event \(G(K,\delta^{\prime})\) holds with high probability at least \(1-3\delta/10\).

First, we construct the lower bound of \(V_{1}^{k}\). For any given \(k\in[K]\), let \(\widetilde{\bm{\xi}}:=\{\widetilde{\bm{\xi}}_{k,h}^{(m)}\}_{m\in[M]}\subset \mathbb{R}^{d}\) be a set of vectors for \(h\in[H]\) and \(V_{h}^{k}(\cdot;\bm{\bar{\xi}})\) be the value function obtained by the Algorithm 1 with non-random \(\widetilde{\bm{\xi}}_{k,h}^{(m)}\) in place of \(\bm{\xi}_{k,h}^{(m)}\). Then consider the following minimization problem:

\[\min_{\begin{subarray}{c}\{\widetilde{\bm{\xi}}_{k,h}^{(m)}\}_{h \in[H],m\in[M]}\end{subarray}} V_{1}^{k}(s_{1}^{k};\widetilde{\bm{\xi}})\] \[\operatorname{s.t.} \max_{m\in[M]}\|\widetilde{\bm{\xi}}_{k,h}^{(m)}\|_{\mathbf{A}_{k,h}}\leq\gamma_{k}(\delta),\quad\forall h\in[H]\]

And we denote \(\bm{\underline{\xi}}:=\{\bm{\xi}_{k,h}^{(m)}\}_{h\in[H],m\in[M]}\) by a minimizer and \(\underline{V}_{1}^{k}(s_{1}^{k})\) by the minimum of the above minimization problem, i.e., \(\underline{V}_{h}^{k}(\cdot):=V_{h}^{k}(\cdot;\bm{\underline{\xi}})\). Then, under the event \(\mathcal{G}(K,\delta^{\prime})\), since \(\{\bm{\xi}_{k,h}^{(m)}\}_{h\in[H],m\in[M]}\) is also a feasible solution of the above optimization problem, and since \(V_{h}^{k}=V_{h}^{k}(\cdot;\bm{\xi})\), thus we have

\[\underline{V}_{1}^{k}(s_{1}^{k})\leq V_{1}^{k}(s_{1}^{k})\,.\] (37)Second, to find an upper bound for \(V^{*}\), considering i.i.d copies \(\{\tilde{\bm{\xi}}_{k,h}^{(m)}\}_{h\in[H],m\in[M]}\) of \(\{\bm{\xi}_{k,h}^{(m)}\}_{h\in[H],m\in[M]}\) and run Algorithm 1 to get a corresponding value function \(\bar{V}_{h}^{k}\) and \(\bar{Q}_{h}^{k}\) for all \(h\in[H]\). Define the event that \(\bar{V}_{1}^{k}(s_{1}^{k})\) is optimistic in the \(k\)-th episode as

\[\bar{\mathcal{X}}_{k}=\{(\bar{V}_{1}^{k}-V_{1}^{*})(s_{1}^{k})\geq 0\}\,.\]

Then by Lemma 6, for given \(\delta\), we have

\[\mathbb{P}(\bar{\mathcal{X}}_{k}\mid s_{1}^{k},\mathcal{F}_{k})\geq\Phi(-1)/2\,.\]

Then by the definition of optimism, under the event \(\mathcal{G}(K,\delta^{\prime})\), we have

\[(V_{1}^{*}-V_{1}^{k})(s_{1}^{k}) \leq\mathbb{E}_{\tilde{\bm{\xi}}|\bar{\mathcal{X}}_{k}}\left[( \bar{V}_{1}^{k}-V_{1}^{k})(s_{1}^{k})\right]\] \[\leq\mathbb{E}_{\tilde{\bm{\xi}}|\bar{\mathcal{X}}_{k}}\left[( \bar{V}_{1}^{k}-\underline{V}_{1}^{k})(s_{1}^{k})\right]\,,\] (38)

where the expectations are over the \(\tilde{\bm{\xi}}\)'s conditioned on the event \(\bar{\mathcal{X}}_{k}\) and the second inequality comes from (37). On the other hand, under the event \(\bar{\mathcal{G}}(K,\delta^{\prime})\) by the law of the total expectation, we have

\[\mathbb{E}_{\tilde{\bm{\xi}}}\left[(\bar{V}_{1}^{k}-\underline{V} _{1}^{k})(s_{1}^{k})\right] =\mathbb{E}_{\tilde{\bm{\xi}}|\bar{\mathcal{X}}_{k}}\left[(\bar{ V}_{1}^{k}-\underline{V}_{1}^{k})(s_{1}^{k})\right]\mathbb{P}(\bar{\mathcal{X}}_{k} )+\mathbb{E}_{\tilde{\bm{\xi}}|\bar{\mathcal{X}}_{k}^{*}}\left[(\bar{V}_{1}^{ k}-\underline{V}_{1}^{k})(s_{1}^{k})\right]\mathbb{P}(\bar{\mathcal{X}}_{k}^{ *})\] \[\geq\mathbb{E}_{\tilde{\bm{\xi}}|\bar{\mathcal{X}}_{k}}\left[(\bar {V}_{1}^{k}-\underline{V}_{1}^{k})(s_{1}^{k})\right]\mathbb{P}(\bar{\mathcal{ X}}_{k})\,,\] (39)

where (39) comes from the fact that \(\{\tilde{\bm{\xi}}_{k,h}^{(m)}\}_{h\in[H],m\in[M]}\) is also a feasible solution of the above optimization problem under the event \(\bar{\mathcal{G}}(K,\delta^{\prime})\), i.e., \(\bar{V}_{1}^{k}(s_{1}^{k})\geq\underline{V}_{1}^{k}(s_{1}^{k})\). Then, by combining the results of (39) and (38), under the event \(G(K,\delta^{\prime})\), we have

\[(V_{1}^{*}-V_{1}^{k})(s_{1}^{k}) \leq\mathbb{E}_{\tilde{\bm{\xi}}|\bar{\mathcal{X}}_{k}}\left[( \bar{V}_{1}^{k}-\underline{V}_{1}^{k})(s_{1}^{k})\right]\] \[\leq\mathbb{E}_{\tilde{\bm{\xi}}}\left[(\bar{V}_{1}^{k}-\underline {V}_{1}^{k})(s_{1}^{k})\right]/\mathbb{P}(\bar{\mathcal{X}}_{k})\] \[\leq\frac{2}{\Phi(-1)}\mathbb{E}_{\tilde{\bm{\xi}}}\left[(\bar{V} _{1}^{k}-V_{1}^{k}+V_{1}^{k}-\underline{V}_{1}^{k})(s_{1}^{k})\right]\] \[=\frac{2}{\Phi(-1)}\left((V_{1}^{k}-\underline{V}_{1}^{k})(s_{1}^ {k})\right)+\ddot{\zeta}_{k}\,,\] (40)

where we denote

\[\ddot{\zeta}_{k}:=\frac{2}{\Phi(-1)}\left(\mathbb{E}_{\tilde{\bm{\xi}}}\left[ \bar{V}_{1}^{k}(s_{1}^{k})\right]-V_{1}^{k}(s_{1}^{k})\right)\,.\]

Note that since \(\bar{\bm{\xi}}\) is the i.i.d copy of \(\bm{\xi}\), therefore \(\bar{V}_{k,1}\) and \(V_{k,1}\) are independent, which means \(\{\ddot{\zeta}_{k}\mid\mathcal{F}_{k-1}\}_{k=1}^{K}\) is a martingale difference sequence with \(|\ddot{\zeta}_{k}|\leq\frac{2H}{\Phi(-1)}\). Therefore by applying Azuma-Hoeffding inequality under the event \(G(K,\delta^{\prime})\), with probability at least \(1-\delta^{\prime}\), we have

\[\sum_{k=1}^{K}\ddot{\zeta}_{k}\leq\frac{2H}{\Phi(-1)}\sqrt{2K\log(1/\delta^{ \prime})}\,.\] (41)

On the other hand, by dividing the first term in (40) into two terms we have

\[(V_{1}^{k}-\underline{V}_{1}^{k})(s_{1}^{k})=\underbrace{(V_{1}^{k}-V_{1}^{ \pi^{k}})(s_{1}^{k})}_{I_{1}}+\underbrace{(V_{1}^{\pi^{k}}-\underline{V}_{1}^ {k})(s_{1}^{k})}_{I_{2}}\,.\]

For \(I_{1}\), note that since it is related to the estimation error, under the event \(G(K,\delta^{\prime})\) we can bound the sum of \(I_{1}\) for the total episode number using Lemma 10 as follows:

\[\sum_{k=1}^{K}(V_{1}^{k}-V_{1}^{\pi^{k}})(s_{1}^{k}) \leq(H\alpha_{K}(\delta^{\prime})+\gamma_{K}(\delta^{\prime})) \,\sqrt{4\kappa^{-1}THd\log\left(1+\frac{K\mathcal{U}L_{\bm{\varphi}}^{2}}{d \lambda}\right)}\] \[\quad+2H\sqrt{2T\log(1/\delta^{\prime})}\,.\] (42)For \(I_{2}\), since we have

\[I_{2} =Q_{1}^{\pi^{k}}(s_{1}^{k},a_{1}^{k})-\underline{V}_{1}^{k}(s_{1}^{ k})\] \[\leq Q_{1}^{\pi^{k}}(s_{1}^{k},a_{1}^{k})-\underline{Q}_{1}^{k}(s_ {1}^{k},a_{1}^{k})\] (43) \[=Q_{1}^{\pi^{k}}(s_{1}^{k},a_{1}^{k})-\underline{Q}_{1}^{k}(s_{1} ^{k},a_{1}^{k})-\iota_{1}^{k}(s_{1}^{k},a_{1}^{k})+\iota_{1}^{k}(s_{1}^{k},a_{1 }^{k})\] \[=P_{1}(V_{2}^{\pi^{k}}-\underline{V}_{2}^{k})(s_{1}^{k},a_{1}^{k}) +\iota_{1}^{k}(s_{1}^{k},a_{1}^{k})\] (44) \[=\underline{P_{1}(V_{2}^{\pi^{k}}-\underline{V}_{2}^{k})(s_{1}^{k },a_{1}^{k})-(V_{2}^{\pi^{k}}-\underline{V}_{2}^{k})(s_{2}^{k})}+(V_{2}^{\pi^{ k}}-\underline{V}_{2}^{k})(s_{2}^{k})+\iota_{1}^{k}(s_{1}^{k},a_{1}^{k})\]

where (43) comes from \(a_{1}^{k}=\operatorname*{argmax}_{a}Q_{1}^{k}(s_{1}^{k},a)\) and (44) holds by the following definition of \(\iota_{h}^{k}(s_{h}^{k},a_{h}^{k})\):

\[\iota_{h}^{k}(s_{h}^{k},a_{h}^{k}) :=r(s_{h}^{k},a_{h}^{k})+P_{h}\underline{V}_{h+1}^{k}(s_{h}^{k},a_ {h}^{k})-\underline{Q}_{h}^{k}(s_{h}^{k},a_{h}^{k})\] \[=r(s_{h}^{k},a_{h}^{k})+P_{h}\underline{V}_{h+1}^{k}(s_{h}^{k},a_ {h}^{k})-\underline{Q}_{h}^{k}(s_{h}^{k},a_{h}^{k})+Q_{h}^{\pi^{k}}(s_{h}^{k},a _{h}^{k})-Q_{h}^{\pi^{k}}(s_{h}^{k},a_{h}^{k})\] \[=P_{h}(\underline{V}_{h+1}^{k}-V_{h+1}^{\pi^{k}})(s_{h}^{k},a_{h} ^{k})+(Q_{h}^{\pi^{k}}-\underline{Q}_{h}^{k})(s_{h}^{k},a_{h}^{k})\,.\]

Then by applying the same argument recursively for the whole horizon, we have

\[I_{2}\leq\sum_{h=1}^{H}\iota_{h}^{k}(s_{h}^{k},a_{h}^{k})+\sum_{h=1}^{H} \,\widetilde{\zeta}_{h}^{\,k}\,,\]

where we denote

\[\widetilde{\zeta}_{h}^{\,k}:=P_{h}(V_{h+1}^{\pi^{k}}-\underline{V}_{h+1}^{k}) (s_{h}^{k},a_{h}^{k})-(V_{h+1}^{\pi^{k}}-\underline{V}_{h+1}^{k})(s_{h+1}^{k} )\,.\]

Note that \(\left\{\,\widetilde{\zeta}_{h}^{\,k}\mid\mathcal{F}_{k,h}\right\}_{k,h}\) is a martingale difference sequence with \(|\,\widetilde{\zeta}_{h}^{\,k}|\leq 2H\). Then, under the event \(G(K,\delta^{\prime})\) by applying the Azuma-Hoeffding inequality with probability at least \(1-\delta^{\prime}\), we have

\[\sum_{k=1}^{K}\sum_{h=1}^{H}\,\widetilde{\zeta}_{h}^{\,k}\leq 2H\sqrt{2T\log(1/ \delta^{\prime})}\,.\] (45)

To bound \(\sum_{h=1}^{H}\iota_{h}^{k}(s_{h}^{k},a_{h}^{k})\), we divide the whole horizon index set into two groups as follows:

\[H^{+}\] \[=\left\{j\in[H]:r(s_{j}^{k},a_{j}^{k})+\sum_{s^{\prime}\in \mathcal{S}_{k,j}}P_{\boldsymbol{\theta}_{h}^{k}}(s^{\prime}\mid s_{j}^{k},a_{ j}^{k})\underline{V}_{j+1}^{k}(s^{\prime})+\max_{m\in[M]}\hat{\boldsymbol{ \varphi}}_{k,j}(s_{j}^{k},a_{j}^{k})^{\top}\underline{\boldsymbol{\xi}}_{k,j}^ {(m)}>H\right\}\] \[H^{-}=[H]\backslash H^{+}\,.\]

Then, for \(j\in H^{+}\) since \(\underline{Q}_{j}^{k}(s_{j}^{k},a_{j}^{k})=H-j+1\), \(\underline{V}_{j+1}^{k}\leq H-j\) and \(r(s_{j}^{k},a_{j}^{k})\leq 1\), we have

\[\iota_{j}^{k}(s_{j}^{k},a_{j}^{k}) =r(s_{j}^{k},a_{j}^{k})+P_{j}\underline{V}_{j+1}^{k}(s_{j}^{k},a_ {j}^{k})-(H-j+1)\leq 0\,.\] (46)

On the other hand, for \(j\in H^{-}\), under the event \(G(K,\delta^{\prime})\) we have

\[\iota_{j}^{k}(s_{j}^{k},a_{j}^{k}) =P_{j}\underline{V}_{j+1}^{k}(s_{j}^{k},a_{j}^{k})-\sum_{s^{\prime }\in\mathcal{S}_{k,j}}P_{\boldsymbol{\theta}_{h}^{k}}(s^{\prime}\mid s_{j}^{k},a _{j}^{k})\underline{V}_{j+1}^{k}(s^{\prime})-\max_{m\in[M]}\hat{\boldsymbol{ \varphi}}_{k,j}(s_{j}^{k},a_{j}^{k})^{\top}\underline{\boldsymbol{\xi}}_{k,j}^{(m)}\] \[\leq\left|P_{j}\underline{V}_{j+1}^{k}(s_{j}^{k},a_{j}^{k})-\sum_ {s^{\prime}\in\mathcal{S}_{k,j}}P_{\boldsymbol{\theta}_{h}^{k}}(s^{\prime}\mid s_{j }^{k},a_{j}^{k})\underline{V}_{j+1}^{k}(s^{\prime})\right|+\left|\max_{m\in[M]} \hat{\boldsymbol{\varphi}}_{k,j}(s_{j}^{k},a_{j}^{k})^{\top}\underline{ \boldsymbol{\xi}}_{k,j}^{(m)}\right|\] \[\leq H\alpha_{k}(\delta^{\prime})\|\hat{\boldsymbol{\varphi}}_{k,j}(s _{j}^{k},a_{j}^{k})\|_{\boldsymbol{\Lambda}_{k,j}^{-1}}+\max_{m\in[M]}\hat{ \boldsymbol{\varphi}}_{k,j}(s_{j}^{k},a_{j}^{k})^{\top}\underline{\boldsymbol{ \xi}}_{k,j}^{(m)}\big{|}\] (47) \[\leq H\alpha_{k}(\delta^{\prime})\|\hat{\boldsymbol{\varphi}}_{k,j}(s _{j}^{k},a_{j}^{k})\|_{\boldsymbol{\Lambda}_{k,j}^{-1}}+\max_{m\in[M]}\hat{ \boldsymbol{\varphi}}_{k,j}(s_{j}^{k},a_{j}^{k})\|_{\boldsymbol{\Lambda}_{k,j}^{-1}} \|\underline{\boldsymbol{\xi}}_{k,j}^{(m)}\|_{\boldsymbol{\Lambda}_{k,j}}\] \[\leq\left(H\alpha_{k}(\delta^{\prime})+\gamma_{k}(\delta^{ \prime})\right)\|\hat{\boldsymbol{\varphi}}_{k,j}(s_{j}^{k},a_{j}^{k})\|_{ \boldsymbol{\Lambda}_{k,j}^{-1}}\,,\] (48)where (47) holds by Lemma 4.

By combining the result of (46) and (48), we have

\[I_{2} \leq\sum_{j\in H^{-}}\left(H\alpha_{k}(\delta^{\prime})+\gamma_{k}( \delta^{\prime})\right)\|\hat{\bm{\varphi}}_{k,j}(s_{j}^{k},a_{j}^{k})\|_{ \mathbf{A}_{k,h}^{-1}}+\sum_{h=1}^{H}\,\dddot{\zeta}_{\,h}^{k}\] \[\leq\sum_{h=1}^{H}\left(H\alpha_{k}(\delta^{\prime})+\gamma_{k}( \delta^{\prime})\right)\|\hat{\bm{\varphi}}_{k,h}(s_{h}^{k},a_{h}^{k})\|_{ \mathbf{A}_{k,h}^{-1}}+\sum_{h=1}^{H}\,\dddot{\zeta}_{\,h}^{k}\,.\]

Then summing \(I_{2}\) over the total number of episodes, under the event \(G(K,\delta^{\prime})\), we have

\[\sum_{k=1}^{K}(V_{1}^{\pi^{k}}-\underline{V}_{1}^{k})(s_{1}^{k}) \leq\sum_{k=1}^{K}\sum_{h=1}^{H}\left(H\alpha_{k}(\delta^{\prime} )+\gamma_{k}(\delta^{\prime})\right)\|\hat{\bm{\varphi}}_{k,h}(s_{h}^{k},a_{h }^{k})\|_{\mathbf{A}_{k,h}^{-1}}+\sum_{k=1}^{K}\sum_{h=1}^{H}\,\dddot{\zeta}_{ \,h}^{k}\] \[\leq\left(H\alpha_{K}(\delta^{\prime})+\gamma_{K}(\delta^{\prime })\right)\sum_{k=1}^{K}\sum_{h=1}^{H}\|\hat{\bm{\varphi}}_{k,h}(s_{h}^{k},a_{h }^{k})\|_{\mathbf{A}_{k,h}^{-1}}+\sum_{k=1}^{K}\sum_{h=1}^{H}\,\dddot{\zeta}_{ \,h}^{k}\] \[\leq\left(H\alpha_{K}(\delta^{\prime})+\gamma_{K}(\delta^{\prime })\right)\sqrt{4\kappa^{-1}THd\log\left(1+\frac{K\mathcal{U}L_{\bm{\varphi}}^{ 2}}{d\lambda}\right)}\] (49) \[\quad+2H\sqrt{2T\log(1/\delta^{\prime})}\,,\] (50)

where the last inequality holds due to the Lemma 3 and (45).

Finally, by summing (40) over \(k\) and plugging the results of (42), (50) and (41) then, we have

\[\sum_{k=1}^{K}(V_{1}^{*}-V_{1}^{k})(s_{1}^{k})\] \[\leq\frac{4}{\Phi(-1)}\left[\left(H\alpha_{K}(\delta^{\prime})+ \gamma_{K}(\delta^{\prime})\right)\sqrt{4\kappa^{-1}THd\log\left(1+\frac{K \mathcal{U}L_{\bm{\varphi}}^{2}}{d\lambda}\right)}+2H\sqrt{2T\log(1/\delta^{ \prime})}\right]\] \[\quad+\frac{2H}{\Phi(-1)}\sqrt{2K\log(1/\delta^{\prime})}\] \[\leq\widetilde{\mathcal{O}}\left(\kappa^{-1}d^{3/2}H^{3/2}\sqrt{ T}+H\sqrt{T}+H\sqrt{K}\right)\,.\]

To conclude the proof, by setting \(\delta^{\prime}=\delta/10\) and we take a union bound over the two applications of Azuma-Hoeffding (\(\dddot{\zeta}_{k},\dddot{\zeta}_{\,h}^{k}\)) and the event \(G(K,\delta^{\prime})\), we get the desired result with probability at least \(1-\delta/2\). 

### Regret Bound of RRL-Mnl

Proof of Theorem 1.: We can decompose the regret with estimation part and pessimism part as follows:

\[\textbf{Regret}_{\pi}(K) =\sum_{k=1}^{K}(V_{1}^{*}-V_{1}^{\pi^{k}})(s_{1}^{k})\] \[=\sum_{k=1}^{K}(V_{1}^{*}-V_{1}^{k})(s_{1}^{k})+\sum_{k=1}^{K}(V_ {1}^{k}-V_{1}^{\pi^{k}})(s_{1}^{k})\,.\]

Since both Lemma 10 and Lemma 11 holds with probability at least \(1-\delta/2\) respectively, by taking the union bound the following holds with probability at least \(1-\delta\):

\[\textbf{Regret}_{\pi}(K) =\widetilde{\mathcal{O}}\left(\kappa^{-1}d^{\frac{3}{2}}H^{\frac{ 3}{2}}\sqrt{T}+H\sqrt{T}+H\sqrt{K}\right)+\widetilde{\mathcal{O}}\left(\kappa^ {-1}d^{\frac{3}{2}}H^{\frac{3}{2}}\sqrt{T}+H\sqrt{T}\right)\] \[=\widetilde{\mathcal{O}}\left(\kappa^{-1}d^{\frac{3}{2}}H^{\frac{ 3}{2}}\sqrt{T}\right)\,.\]Detailed Regret Analysis for ORRl-Mnl (Theorem 2)

### Concentration of Estimated Transition Core \(\widetilde{\bm{\theta}}_{h}^{k}\)

In this section, we provide the detailed proof of Lemma 12, which demonstrates the concentration result for \(\widetilde{\bm{\theta}}_{h}^{k}\) independently of \(\kappa\) and \(\mathcal{U}\). Note that we adapt the proof provided by Zhang and Sugiyama [76] in the MNL contextual bandit setting to MNL-MDPs and improve the result, making it independent of \(\mathcal{U}\). We provide the lemmas for the concentration of the online transition core for completeness, noting that there are slight differences compared to their work, which stem from the different problem setting.

**Lemma 12** (Concentration of online estimated transition core).: _Let \(\eta=\mathcal{O}(\log\mathcal{U})\) and \(\lambda=\mathcal{O}(d\log\mathcal{U})\). Then, for any \(\delta\in(0,1]\) and for any \(h\in[H]\), we have_

\[\mathbb{P}\left(\forall k\geq 1,\left\|\widetilde{\bm{\theta}}_{h}^{k}-\bm{ \theta}_{h}^{*}\right\|_{\mathbf{B}_{k,h}}\leq\beta_{k}(\delta)\right)\geq 1- \delta\,,\]

_where \(\beta_{k}(\delta)=\mathcal{O}(\sqrt{d}\log\mathcal{U}\log(kH))\)._

Proof of Lemma 12.: Recall that the transition core updated by the online mirror descent is represented by

\[\widetilde{\bm{\theta}}_{h}^{k+1}=\operatorname*{argmin}_{\bm{\theta}\in \mathcal{B}(L_{\bm{\theta}})}\widetilde{\ell}_{k,h}(\bm{\theta})+\frac{1}{2 \eta}\left\|\bm{\theta}-\widetilde{\bm{\theta}}_{h}^{k}\right\|_{\mathbf{B}_{ k,h}}^{2},\]

where \(\widetilde{\ell}_{k,h}(\bm{\theta})=\ell_{k,h}(\widetilde{\bm{\theta}}_{h}^{k} )+(\bm{\theta}-\widetilde{\bm{\theta}}_{h}^{k})^{\top}\nabla\ell_{k,h}( \widetilde{\bm{\theta}}_{h}^{k})+\frac{1}{2}\left\|\bm{\theta}-\widetilde{\bm{ \theta}}_{h}^{k}\right\|_{\nabla^{2}\ell_{k,h}(\widetilde{\bm{\theta}}_{h}^{k} )}\). We introduce the following lemma providing that the estimation error of the online estimator \(\widetilde{\bm{\theta}}_{h}^{k}\) can be bounded by the regret.

**Lemma 13** (Lemma 12 in [76]).: _Let \(\alpha=\log\mathcal{U}+2(1+L_{\bm{\theta}}L_{\bm{\varphi}})\) and \(\lambda>0\). If we set the step size \(\eta=\alpha/2\), then we have_

\[\begin{split}\left\|\widetilde{\bm{\theta}}_{h}^{k}-\bm{\theta} _{h}^{*}\right\|_{\mathbf{B}_{k,h}}^{2}\leq&\alpha\sum_{i=1}^{k} \left(\ell_{i,h}(\bm{\theta}_{h}^{*})-\ell_{i,h}(\widetilde{\bm{\theta}}_{h}^{i +1})\right)+\lambda L_{\bm{\theta}}^{2}\\ &+3\sqrt{2}L_{\bm{\varphi}}^{3}\alpha\sum_{i=1}^{k}\left\| \widetilde{\bm{\theta}}_{h}^{i+1}-\widetilde{\bm{\theta}}_{h}^{i}\right\|_{2}^ {2}-\sum_{i=1}^{k}\left\|\widetilde{\bm{\theta}}_{h}^{i+1}-\widetilde{\bm{ \theta}}_{h}^{i}\right\|_{\mathbf{B}_{i,h}}^{2}\,.\end{split}\] (51)

Now, we bound the first term of (51). To simplify the presentation, for all \((k,h)\in[K]\times[H]\), we define the softmax function \(\bm{\sigma}_{k,h}:\mathbb{R}^{|\mathcal{S}_{k,h}|}\to[0,1]^{|\mathcal{S}_{k,h }|}\) as follows:

\[[\bm{\sigma}_{k,h}(\mathbf{z})]_{s^{\prime}}=\frac{\exp([\mathbf{z}]_{s^{ \prime}})}{\sum_{s^{\prime\prime}\in\mathcal{S}_{k,h}}\exp([\mathbf{z}]_{s^{ \prime\prime}})},\]

where \([.]_{s^{\prime}}\) denote the element corresponding to \(s^{\prime}\in\mathcal{S}\) of the input vector. We also define the pseudo-inverse of the softmax function \(\bm{\sigma}_{k,h}\) via \([\bm{\sigma}_{k,h}^{+}(\mathbf{p})]_{s^{\prime}}=\log([\mathbf{p}]_{s^{\prime}})\) which has the property that for all \(\mathbf{p}\in\Delta_{|\mathcal{S}_{k,h}|}\), we have \(\bm{\sigma}_{k,h}(\bm{\sigma}_{k,h}^{+}(\mathbf{p}))=\mathbf{p}\) and \(\sum_{s\in\mathcal{S}_{k,h}}\exp\left([\bm{\sigma}_{k,h}^{+}(\mathbf{p})]_{s }\right)=1\).

We denote \(\bm{\Phi}_{k,h}=[\bm{\varphi}_{k,h,s^{\prime}}]_{s^{\prime}\in\mathcal{S}_{k,h }}\in\mathbb{R}^{d\times|\mathcal{S}_{k,h}|}\) for simplicity. Then, the transition model can also be written as \(P_{\bm{\theta}}(s^{\prime}\mid s_{h}^{k},a_{h}^{k})=[\bm{\sigma}_{k,h}(\bm{ \Phi}_{k,h}^{\top}\bm{\theta}_{h}^{*})]_{s^{\prime}}\). We further define \(\widetilde{\mathbf{z}}_{i,h}=\bm{\sigma}_{i,h}^{+}\left(\mathbb{E}_{\bm{ \theta}\sim\mathcal{N}(\widetilde{\bm{\theta}}_{h}^{i},c\mathbf{B}_{i,h}^{-1} )}[\bm{\sigma}_{i,h}(\bm{\Phi}_{i,h}^{\top}\bm{\theta})]\right)\) for our analysis. Then, we have

\[\sum_{i=1}^{k}\left(\ell_{i,h}(\bm{\theta}_{h}^{*})-\ell_{i,h}(\widetilde{\bm{ \theta}}_{h}^{i+1})\right)=\sum_{i=1}^{k}\left(\ell_{i,h}(\bm{\theta}_{h}^{*})- \ell(\widetilde{\mathbf{z}}_{i,h},y_{h}^{i})\right)+\sum_{i=1}^{k}\left(\ell( \widetilde{\mathbf{z}}_{i,h},y_{h}^{i})-\ell_{i,h}(\widetilde{\bm{\theta}}_{h}^{ i+1})\right).\] (52)

We can bound the first term of (52) by the following lemma.

**Lemma 14**.: _Let \(\delta\in(0,1]\). Then, for all \((k,h)\in[K]\times[H]\), with probability at least \(1-\delta\), we have_

\[\sum_{i=1}^{k}\left(\ell_{i,h}(\bm{\theta}_{h}^{*})-\ell(\widetilde{\mathbf{z}} _{i,h},y_{h}^{i})\right)\leq\Gamma_{k}^{A}(\delta),\]

_where \(\Gamma_{k}^{A}(\delta)=\frac{5}{4}(3\log(\mathcal{U}k)+L_{\bm{\varphi}}L_{\bm {\theta}})\lambda+4(3\log(\mathcal{U}k)+L_{\bm{\varphi}}L_{\bm{\theta}})\log \left(\frac{H\sqrt{1+2k}}{\delta}\right)+2\)._

Furthermore, we can bound the second term of (52) by the following lemma.

**Lemma 15**.: _Let \(\lambda\geq 72L_{\bm{\varphi}}^{2}cd\). Then, for any \(c>0\) and all \((k,h)\in[K]\times[H]\), we have_

\[\sum_{i=1}^{k}\left(\ell(\widetilde{\mathbf{z}}_{i,h},y_{h}^{i})-\ell_{i,h}( \widetilde{\bm{\theta}}_{h}^{i+1})\right)\leq\frac{1}{2c}\sum_{i=1}^{k}\left \|\widetilde{\bm{\theta}}_{h}^{i+1}-\widetilde{\bm{\theta}}_{h}^{i}\right\|_ {\mathbf{B}_{i,h}}^{2}+\Gamma_{k}^{B}(\delta).\]

_where \(\Gamma_{k}^{B}(\delta)=\sqrt{6}cd\log\left(1+\frac{2kL_{\bm{\varphi}}^{2}}{d \lambda}\right)\)._

Combining Lemma 13, Lemma 14, and Lemma 15, and by setting \(\eta=\alpha/2,c=2\alpha/3\) and \(\lambda\geq\max\{12\sqrt{2}L_{\bm{\varphi}}^{3}\alpha,48L_{\bm{\varphi}}^{2}d\alpha\}\), we derive that

\[\left\|\widetilde{\bm{\theta}}_{h}^{k+1}-\bm{\theta}_{h}^{*} \right\|_{\mathbf{B}_{k,h}}^{2}\] \[\leq\alpha\Gamma_{k}^{A}(\delta)+\alpha\Gamma_{k}^{B}(\delta)+ \lambda L_{\bm{\theta}}^{2}+3\sqrt{2}L_{\bm{\varphi}}^{3}\alpha\sum_{i=1}^{k} \left\|\widetilde{\bm{\theta}}_{h}^{i+1}-\widetilde{\bm{\theta}}_{h}^{i} \right\|_{2}^{2}+\left(\frac{\alpha}{2c}-1\right)\sum_{i=1}^{k}\left\| \widetilde{\bm{\theta}}_{h}^{i+1}-\widetilde{\bm{\theta}}_{h}^{i}\right\|_{ \mathbf{B}_{i,h}}^{2}\] \[\leq\alpha\Gamma_{k}^{A}(\delta)+\alpha\Gamma_{k}^{B}(\delta)+ \lambda L_{\bm{\theta}}^{2}\] \[\leq C\log\mathcal{U}\left(\lambda\log(\mathcal{U}k)+\log( \mathcal{U}k)\log\left(\frac{H\sqrt{1+2k}}{\delta}\right)+d\log\left(1+\frac{ k}{d\lambda}\right)\right)+\lambda L_{\bm{\theta}}^{2}\] \[=:\beta_{k}(\delta)^{2}\] (53)

where \(C>0\) is an absolute constant. In the above, we choose \(\lambda=\mathcal{O}(d\log\mathcal{U})\), \(\alpha=\mathcal{O}(\log\mathcal{U})\). The second inequality of (53) is derived from the fact that

\[3\sqrt{2}L_{\bm{\varphi}}^{3}\alpha\sum_{i=1}^{k}\left\| \widetilde{\bm{\theta}}_{h}^{i+1}-\widetilde{\bm{\theta}}_{h}^{i}\right\|_{2} ^{2}+\left(\frac{\alpha}{2c}-1\right)\sum_{i=1}^{k}\left\|\widetilde{\bm{ \theta}}_{h}^{i+1}-\widetilde{\bm{\theta}}_{h}^{i}\right\|_{\mathbf{B}_{i,h}}^ {2}\] \[=3\sqrt{2}L_{\bm{\varphi}}^{3}\alpha\sum_{i=1}^{k}\left\| \widetilde{\bm{\theta}}_{h}^{i+1}-\widetilde{\bm{\theta}}_{h}^{i}\right\|_{2} ^{2}-\frac{1}{4}\sum_{i=1}^{k}\left\|\widetilde{\bm{\theta}}_{h}^{i+1}- \widetilde{\bm{\theta}}_{h}^{i}\right\|_{\mathbf{B}_{i,h}}^{2}\] \[\leq 3\sqrt{2}L_{\bm{\varphi}}^{3}\alpha\sum_{i=1}^{k}\left\| \widetilde{\bm{\theta}}_{h}^{i+1}-\widetilde{\bm{\theta}}_{h}^{i}\right\|_{2} ^{2}-\frac{\lambda}{4}\sum_{i=1}^{k}\left\|\widetilde{\bm{\theta}}_{h}^{i+1}- \widetilde{\bm{\theta}}_{h}^{i}\right\|_{2}^{2}\] \[\leq 0.\]

The first inequality holds from \(\mathbf{B}_{i,h}\succeq\lambda\mathbf{I}_{d}\), and the second inequality is obvious from our setting of \(\lambda\). Therefore, we can conclude that

\[\left\|\widetilde{\bm{\theta}}_{h}^{k}-\bm{\theta}_{h}^{*}\right\|_{\mathbf{B} _{k,h}}\leq\beta_{k}(\delta)=\mathcal{O}(\sqrt{d}\log\mathcal{U}\log(kH))\,.\]

In the following section, we provide the proofs of the lemmas used in Lemma 12.

#### d.1.1 Proof of Lemma 13

Proof of Lemma 13.: Let \(\widetilde{\ell}_{i,h}(\bm{\theta})=\ell_{i,h}(\widetilde{\bm{\theta}}_{h}^{i})+ \nabla\ell_{i,h}(\widetilde{\bm{\theta}}_{h}^{i})^{\top}\left(\bm{\theta}- \widetilde{\bm{\theta}}_{h}^{i}\right)+\frac{1}{2}\left\|\bm{\theta}- \widetilde{\bm{\theta}}_{h}^{i}\right\|_{\nabla^{2}\ell_{i,h}(\widetilde{\bm{ \theta}}_{h}^{i})}^{2}\) be a second-order Taylor expansion of \(\ell_{i,h}(\bm{\theta})\) at \(\widetilde{\bm{\theta}}_{h}^{i}\). Since we have

\[\widetilde{\bm{\theta}}_{h}^{k+1}=\operatorname*{argmin}_{\bm{\theta}\in\mathcal{ B}_{d}(L_{\bm{\theta}})}\frac{1}{2\eta}\left\|\bm{\theta}-\widetilde{\bm{\theta}}_{h}^{ k}\right\|_{\widetilde{\mathbf{B}}_{k,h}}+\nabla\ell_{k,h}(\widetilde{\bm{ \theta}}_{h}^{k})^{\top}\bm{\theta}=\operatorname*{argmin}_{\bm{\theta}\in \mathcal{B}(\bm{0}_{d},L_{\bm{\theta}})}\widetilde{\ell}_{k,h}(\bm{\theta})+ \frac{1}{2\eta}\left\|\bm{\theta}-\widetilde{\bm{\theta}}_{h}^{k}\right\|_{ \widetilde{\mathbf{B}}_{k,h}}^{2},\]

by Lemma 31, if we define \(\psi(\bm{\theta})=\frac{1}{2}\|\bm{\theta}\|_{\widetilde{\mathbf{B}}_{i,h}}^{2}\) we obtain

\[\nabla\widetilde{\ell}_{i,h}(\widetilde{\bm{\theta}}_{h}^{i+1})^{\top}( \widetilde{\bm{\theta}}_{h}^{i+1}-\bm{\theta}_{h}^{*})\leq\frac{1}{2\eta}\left( \left\|\widetilde{\bm{\theta}}_{h}^{i}-\bm{\theta}_{h}^{*}\right\|_{\widetilde {\mathbf{B}}_{i,h}}^{2}-\left\|\widetilde{\bm{\theta}}_{h}^{i+1}-\bm{\theta}_ {h}^{*}\right\|_{\widetilde{\mathbf{B}}_{i,h}}^{2}-\left\|\widetilde{\bm{ \theta}}_{h}^{i+1}-\widetilde{\bm{\theta}}_{h}^{i}\right\|_{\widetilde{ \mathbf{B}}_{i,h}}\right).\] (54)

By applying Lemma 33, we have

\[\ell_{i,h}(\widetilde{\bm{\theta}}_{h}^{i+1})-\ell_{i,h}(\bm{\theta}_{h}^{*}) \leq\left\langle\nabla\ell_{i,h}(\widetilde{\bm{\theta}}_{h}^{i+1}), \widetilde{\bm{\theta}}_{h}^{i+1}-\bm{\theta}_{h}^{*}\right\rangle-\frac{1}{ \alpha_{i,h}}\left\|\widetilde{\bm{\theta}}_{h}^{i+1}-\bm{\theta}_{h}^{*} \right\|_{\nabla^{2}\ell_{i,h}(\widetilde{\bm{\theta}}_{h}^{i+1})},\] (55)

where \(\alpha_{i,h}=\log|\mathcal{S}_{i,h}|+2(1+L_{\bm{\varphi}}L_{\bm{\theta}})\).

By setting \(\eta=\alpha_{i,h}/2\) and merging equations (54) and (55), we arrive at

\[\ell_{i,h}(\widetilde{\bm{\theta}}_{h}^{i+1})-\ell_{i,h}(\bm{\theta }_{h}^{*}) \leq\left\langle\nabla\ell_{i,h}(\widetilde{\bm{\theta}}_{h}^{i+1})- \nabla\widetilde{\ell}_{i,h}(\widetilde{\bm{\theta}}_{h}^{i+1}),\widetilde{ \bm{\theta}}_{h}^{i+1}-\bm{\theta}_{h}^{*}\right\rangle\] \[+\frac{1}{\alpha_{i,h}}\left(\left\|\widetilde{\bm{\theta}}_{h}^{ i}-\bm{\theta}_{h}^{*}\right\|_{\widetilde{\mathbf{B}}_{i,h}}^{2}-\left\| \widetilde{\bm{\theta}}_{h}^{i+1}-\bm{\theta}_{h}^{*}\right\|_{\widetilde{ \mathbf{B}}_{i+1,h}}^{2}-\left\|\widetilde{\bm{\theta}}_{h}^{i+1}-\widetilde{ \bm{\theta}}_{h}^{i}\right\|_{\widetilde{\mathbf{B}}_{i,h}}\right).\] (56)

Meanwhile, we obtain

\[\nabla\widetilde{\ell}_{i,h}(\bm{\theta})=\nabla\ell_{i,h}(\widetilde{\bm{ \theta}}_{h}^{i})+\nabla^{2}\ell_{i,h}(\widetilde{\bm{\theta}}_{h}^{i})(\bm{ \theta}-\widetilde{\bm{\theta}}_{h}^{i})\] (57)

by taking the gradient over both sides of the Taylor approximation of \(\ell_{i,h}(\bm{\theta})\). Using (57), we proceed to bound the first term of (56) as follows:

\[\left\langle\nabla\ell_{i,h}(\widetilde{\bm{\theta}}_{h}^{i+1})- \nabla\widetilde{\ell}_{i,h}(\widetilde{\bm{\theta}}_{h}^{i+1}),\widetilde{\bm {\theta}}_{h}^{i+1}-\bm{\theta}_{h}^{*}\right\rangle\] \[=\left\langle\nabla\ell_{i,h}(\widetilde{\bm{\theta}}_{h}^{i+1})- \nabla\ell_{i,h}(\widetilde{\bm{\theta}}_{h}^{i})-\nabla^{2}\ell_{i,h}( \widetilde{\bm{\theta}}_{h}^{i})(\widetilde{\bm{\theta}}_{h}^{i+1}-\widetilde{ \bm{\theta}}_{h}^{i}),\widetilde{\bm{\theta}}_{h}^{i+1}-\bm{\theta}^{*}\right\rangle\] \[=\left\langle D^{3}\ell_{i,h}(\widetilde{\bm{\theta}}_{h}^{i+1}) \left[\widetilde{\bm{\theta}}_{h}^{i+1}-\widetilde{\bm{\theta}}_{h}^{i}\right] (\widetilde{\bm{\theta}}_{h}^{i+1}-\widetilde{\bm{\theta}}_{h}^{i}), \widetilde{\bm{\theta}}_{h}^{i+1}-\bm{\theta}^{*}\right\rangle\] \[\leq 3\sqrt{2}L_{\bm{\varphi}}\left\|\widetilde{\bm{\theta}}_{h}^{i+1 }-\bm{\theta}_{h}^{*}\right\|_{2}\left\|\widetilde{\bm{\theta}}_{h}^{i+1}- \widetilde{\bm{\theta}}_{h}^{i}\right\|_{\nabla^{2}\ell_{i,h}(\widetilde{\bm{ \theta}}_{h}^{i+1})}^{2}\] \[\leq 3\sqrt{2}L_{\bm{\varphi}}\left\|\widetilde{\bm{\theta}}_{h}^{i+1 }-\widetilde{\bm{\theta}}_{h}^{i}\right\|_{\nabla^{2}\ell_{i,h}(\widetilde{\bm{ \theta}}_{h}^{i+1})}^{2}\] \[\leq 3\sqrt{2}L_{\bm{\varphi}}^{3}\left\|\widetilde{\bm{\theta}}_{h}^{ i+1}-\widetilde{\bm{\theta}}_{h}^{i}\right\|_{2}^{2}\]

where \(\widetilde{\bm{\theta}}_{h}^{i+1}\) is a convex combination of \(\widetilde{\bm{\theta}}_{h}^{i}\) and \(\widetilde{\bm{\theta}}_{h}^{i+1}\). The second equality arises from the Taylor expansion, the first inequality is due to the self-concordant property, and the final inequality is justified by the following:

\[\nabla^{2}\ell_{i,h}(\bm{\theta}_{h}^{i+1})\] \[=\sum_{s^{\prime}\in\mathcal{S}_{i,h}}P_{\widetilde{\bm{ \theta}}_{h}^{i+1}}(s^{\prime}\mid s_{h}^{i},a_{h}^{i})\bm{\varphi}_{i,h,s^{ \prime}}\bm{\varphi}_{i,h,s^{\prime}}^{\top}\] \[\quad-\sum_{s^{\prime}\in\mathcal{S}_{i,h}}\sum_{s^{\prime}\in \mathcal{S}_{i,h}}P_{\widetilde{\bm{\theta}}_{h}^{i+1}}(s^{\prime}\mid s_{h}^{i},a_ {h}^{i})P_{\widetilde{\bm{\theta}}_{h}^{i+1}}(s^{\prime\prime}\mid s_{h}^{i},a_ {h}^{i})\bm{\varphi}_{i,h,s^{\prime}}\bm{\varphi}_{i,h,s^{\prime\prime}}^{\top}\] \[\preceq\sum_{s^{\prime}\in\mathcal{S}_{i,h}}P_{\widetilde{\bm{ \theta}}_{h}^{i+1}}(s^{\prime}\mid s_{h}^{i},a_{h}^{i})\bm{\varphi}_{i,h,s^{ \prime}}\bm{\varphi}_{i,h,s^{\prime}}^{\top}\] \[\preceq L_{\bm{\varphi}}^{2}\mathbf{I}_{d}.\]By summing over \(i\) and reorganizing the terms, we arrive at the final result as follows:

\[\left\|\widetilde{\bm{\theta}}_{h}^{k+1}-\bm{\theta}_{h}^{*}\right\| _{\mathbf{B}_{k+1,h}}^{2}\] \[\leq\sum_{i=1}^{k}\alpha_{i,h}\left(\ell_{i,h}(\bm{\theta}_{h}^{*} )-\ell_{i,h}(\widetilde{\bm{\theta}}_{h}^{i+1})\right)+\left\|\widetilde{\bm{ \theta}}_{h}^{1}-\bm{\theta}_{h}^{*}\right\|_{\mathbf{B}_{1,h}}^{2}\] \[\quad+3\sqrt{2}L_{\bm{\varphi}}^{3}\sum_{i=1}^{k}\alpha_{i,h} \left\|\widetilde{\bm{\theta}}_{h}^{i+1}-\widetilde{\bm{\theta}}_{h}^{i} \right\|_{2}^{2}-\sum_{i=1}^{k}\left\|\widetilde{\bm{\theta}}_{h}^{i+1}- \widetilde{\bm{\theta}}_{h}^{i}\right\|_{\mathbf{B}_{i,h}}^{2}\] \[\leq\alpha\sum_{i=1}^{k}\left(\ell_{i,h}(\bm{\theta}_{h}^{*})- \ell_{i,h}(\widetilde{\bm{\theta}}_{h}^{i+1})\right)+\lambda L_{\bm{\theta}}^ {2}+3\sqrt{2}L_{\bm{\varphi}}^{3}\alpha\sum_{i=1}^{k}\left\|\widetilde{\bm{ \theta}}_{h}^{i+1}-\widetilde{\bm{\theta}}_{h}^{i}\right\|_{2}^{2}-\sum_{i=1}^ {k}\left\|\widetilde{\bm{\theta}}_{h}^{i+1}-\widetilde{\bm{\theta}}_{h}^{i} \right\|_{\mathbf{B}_{i,h}}^{2}.\]

where the first inequality holds by Assumption 2 and the last inequality holds since \(\alpha=\log\mathcal{U}+2(1+L_{\bm{\varphi}}L_{\bm{\theta}})\geq\alpha_{i,h}\) for all \(i\in[k]\). 

#### d.1.2 Proof of Lemma 14

Proof of Lemma 14.: The norm of \(\widetilde{\mathbf{z}}_{i,h}=\bm{\sigma}_{i,h}^{+}\left(\mathbb{E}_{\bm{ \theta}\sim\mathcal{N}(\widetilde{\bm{\theta}}_{h}^{i},c\mathbf{B}_{i,h}^{-1 })}[\bm{\sigma}_{i,h}(\bm{\Phi}_{i,h}^{\top}\bm{\theta})]\right)\) is generally unbounded [27]. In this proof, we utilize the smoothed version of \(\widetilde{\mathbf{z}}_{i,h}\), defined as follows:

\[\widetilde{\mathbf{z}}_{i,h}^{u}=\bm{\sigma}_{i,h}^{+}\left(\mathrm{smooth}_{ i,h}^{u}\,\mathbb{E}_{\bm{\theta}\sim\mathcal{N}(\widetilde{\bm{\theta}}_{h}^{i },c\mathbf{B}_{i,h}^{-1})}[\bm{\sigma}_{i,h}(\bm{\Phi}_{i,h}^{\top}\bm{\theta} )]\right)\]

where the smooth function \(\mathrm{smooth}_{i,h}^{u}(\mathbf{p})=(1-u)\mathbf{p}+(u/\mathcal{U})\mathbf{1}\) with \(u\in[0,1/2]\), and \(\mathbf{1}\in\mathbb{R}^{|\mathcal{S}_{i,h}|}\) is an all-one vector.

Exploiting the property of \(\bm{\sigma}_{i,h}^{+}\) such that \(\bm{\sigma}_{i,h}(\bm{\sigma}_{i,h}^{+}(\mathbf{p}))=\mathbf{p}\) for any \(\mathbf{p}\in\Delta_{|\mathcal{S}_{i,h}|}\), it is straightforward to show that \(\widetilde{\mathbf{z}}_{i,h}^{u}=\bm{\sigma}_{i,h}^{+}(\mathrm{smooth}_{i,h}^ {u}(\bm{\sigma}_{i,h}(\widetilde{\mathbf{z}}_{i,h})))\). Then, by Lemma 34, we have

\[\sum_{i=1}^{k}\ell(\widetilde{\mathbf{z}}_{i,h}^{u},y_{h}^{i})- \sum_{i=1}^{k}\ell(\widetilde{\mathbf{z}}_{i,h},y_{h}^{i})\leq 2uk,\quad\text{and} \quad\|\widetilde{\mathbf{z}}_{i,h}^{u}\|_{\infty}\leq\log(\mathcal{U}/u).\] (58)

Given the definition of \(\ell_{i,h}\), we know that \(\ell(\mathbf{z}_{i,h}^{*},y_{h}^{i})=\ell_{i,h}(\bm{\theta}_{h}^{*})\), where \(\mathbf{z}_{i,h}^{*}=\bm{\Phi}_{i,h}^{\top}\bm{\theta}_{h}^{*}\). We can bound the gap between the loss of \(\bm{\theta}_{h}^{*}\) and \(\widetilde{\mathbf{z}}_{i,h}^{u}\) as follows:

\[\sum_{i=1}^{k}\left(\ell_{i,h}(\bm{\theta}_{h}^{*})-\ell(\widetilde {\mathbf{z}}_{i,h}^{u},y_{h}^{i})\right)\] \[=\sum_{i=1}^{k}\left(\ell(\mathbf{z}_{i,h}^{*},y_{h}^{i})-\ell( \widetilde{\mathbf{z}}_{i,h}^{u},y_{h}^{i})\right)\] \[\leq\sum_{i=1}^{k}\langle\nabla_{z}\ell(\mathbf{z}_{i,h}^{*},y_{h }^{i}),\mathbf{z}_{i,h}^{*}-\widetilde{\mathbf{z}}_{i,h}^{u}\rangle-\sum_{i=1}^ {k}\frac{1}{M_{i,h}}\|\mathbf{z}_{i,h}^{*}-\widetilde{\mathbf{z}}_{i,h}^{u}\|_ {\nabla_{z}^{2}\ell(\mathbf{z}_{i,h}^{*},y_{h}^{i})}^{2}\] \[=\sum_{i=1}^{k}\langle\nabla_{z}\ell(\mathbf{z}_{i,h}^{*},y_{h}^{ i}),\mathbf{z}_{i,h}^{*}-\widetilde{\mathbf{z}}_{i,h}^{u}\rangle-\sum_{i=1}^{k}\frac{1}{M_{ i,h}}\|\mathbf{z}_{i,h}^{*}-\widetilde{\mathbf{z}}_{i,h}^{u}\|_{\nabla \bm{\sigma}_{i,h}(\mathbf{z}_{i,h}^{*})}^{2},\] (59)

where \(M_{i,h}=\log(|\mathcal{S}_{i,h}|+2\log(\mathcal{U}/u)\), and the second equality holds by a direct calculation of the first order and Hessian of the logistic loss.

Now, we first bound the first term of the right-hand side. Let \(\mathbf{d}_{i,h}=(\mathbf{z}_{i,h}^{*}-\widetilde{\mathbf{z}}_{i,h}^{u})/(M+L_{ \bm{\varphi}}L_{\bm{\theta}})\), where \(M=\log\mathcal{U}+2\log(\mathcal{U}/u)\). Then, one can check that \(\|\mathbf{d}_{i,h}\|_{\infty}\leq 1\) since \(\|\mathbf{z}_{i,h}^{*}\|_{\infty}\leq\max_{s^{\prime}\in\mathcal{S}_{i,h}}\| \bm{\varphi}_{i,h,s^{\prime}}\|_{2}\|\bm{\theta}_{h}^{*}\|_{2}\leq L_{\bm{ \varphi}}L_{\bm{\theta}}\) and \(\|\widetilde{\mathbf{z}}_{i,h}^{u}\|_{\infty}\leq\log(\mathcal{U}/u)\). Moreover, since \(\mathbf{z}_{i,h}^{*}\) and \(\widetilde{\mathbf{z}}_{i,h}^{u}\) are independent of \(y_{h}^{i}\), \(\mathbf{d}_{i,h}\) is \(\mathcal{F}_{i,h}\)-measurable. Since \(\mathbb{E}[(\bm{\sigma}_{i,h}(\mathbf{z}_{i,h}^{*})-y_{h}^{i})(\bm{\sigma}_{i,h} (\mathbf{z}_{i,h}^{*})-y_{h}^{i})^{\top}\mid\mathcal{F}_{i,h}]=\nabla\bm{ \sigma}_{i,h}(\mathbf{z}_{i,h}^{*})\) and \(\|\bm{\sigma}_{i,h}(\mathbf{z}_{i,h}^{*})-y_{h}^{i}\|_{1}\leq 2\), we can apply Lemma 32. For any \(k\) and \(\delta\in(0,1]\)with probability at least \(1-\delta/H\), we have

\[\sum_{i=1}^{k}\langle\nabla_{z}\ell(\mathbf{z}_{i,h}^{*},y_{h}^{i}), \mathbf{z}_{i,h}^{*}-\widetilde{\mathbf{z}}_{i,h}^{u}\rangle\] \[=(M+L_{\boldsymbol{\varphi}}L_{\boldsymbol{\theta}})\sum_{i=1}^{k }\langle\nabla_{z}\ell(\mathbf{z}_{i,h}^{*},y_{h}^{i}),\mathbf{d}_{i,h}\rangle\] \[\leq(M+L_{\boldsymbol{\varphi}}L_{\boldsymbol{\theta}})\sqrt{ \lambda+\sum_{i=1}^{k}\|\mathbf{d}_{i,h}\|_{\nabla\boldsymbol{\sigma}_{i,h}( \mathbf{z}_{i,h}^{*})}^{2}}\] \[\quad\cdot\sqrt{\frac{\sqrt{\lambda}}{4}+\frac{4}{\sqrt{ \lambda}}\log\left(\frac{H\sqrt{1+\frac{1}{\lambda}\sum_{i=1}^{k}\|\mathbf{d}_ {i,h}\|_{\nabla\boldsymbol{\sigma}_{i,h}(\mathbf{z}_{i,h}^{*})}^{2}}}{\delta} \right)}\] \[\leq(M+L_{\boldsymbol{\varphi}}L_{\boldsymbol{\theta}})\sqrt{ \lambda+\sum_{i=1}^{k}\|\mathbf{d}_{i,h}\|_{\nabla\boldsymbol{\sigma}_{i,h}( \mathbf{z}_{i,h}^{*})}^{2}}\sqrt{\frac{\sqrt{\lambda}}{4}+4\log\left(\frac{H \sqrt{1+2k}}{\delta}\right)},\] (60)

where the second inequality holds since \(\|\mathbf{d}_{i,h}\|_{\nabla\boldsymbol{\sigma}_{i,h}(\mathbf{z}_{i,h}^{*})}^ {2}=\mathbf{d}_{i,h}^{\top}\nabla\boldsymbol{\sigma}_{i,h}(\mathbf{z}_{i,h}^{ *})\mathbf{d}_{i,h}\leq 2\) and \(\lambda\geq 1\). Plugging (60) into (59) and rearranging the term, we get

\[\sum_{i=1}^{k}\left(\ell_{i,h}(\boldsymbol{\theta}^{*})-\ell( \widetilde{\mathbf{z}}_{i,h}^{u},y_{h}^{i})\right)\] \[\leq(M+L_{\boldsymbol{\varphi}}L_{\boldsymbol{\theta}})\sqrt{ \lambda+\sum_{i=1}^{k}\|\mathbf{d}_{i,h}\|_{\nabla\boldsymbol{\sigma}_{i,h}( \mathbf{z}_{i,h}^{*})}^{2}}\sqrt{\frac{\sqrt{\lambda}}{4}+4\log\left(H\frac{ \sqrt{1+2k}}{\delta}\right)}\] \[\quad-\sum_{i=1}^{k}\frac{1}{M_{i,h}}\|\mathbf{z}_{i,h}^{*}- \widetilde{\mathbf{z}}_{i,h}^{u}\|_{\nabla\boldsymbol{\sigma}_{i,h}(\mathbf{ z}_{i,h}^{*})}^{2}\] \[\leq(M+L_{\boldsymbol{\varphi}}L_{\boldsymbol{\theta}})\sqrt{ \lambda+\sum_{i=1}^{k}\|\mathbf{d}_{i,h}\|_{\nabla\boldsymbol{\sigma}_{i,h}( \mathbf{z}_{i,h}^{*})}^{2}}\sqrt{\frac{\sqrt{\lambda}}{4}+4\log\left(\frac{H \sqrt{1+2k}}{\delta}\right)}\] \[\quad-(M+L_{\boldsymbol{\varphi}}L_{\boldsymbol{\theta}})\sum_{i= 1}^{k}\|\mathbf{d}_{i,h}\|_{\nabla\boldsymbol{\sigma}_{i,h}(\mathbf{z}_{i,h}^ {*})}^{2}\] \[\leq(M+L_{\boldsymbol{\varphi}}L_{\boldsymbol{\theta}})\left( \lambda+\sum_{i=1}^{k}\|\mathbf{d}_{i,h}\|_{\nabla\boldsymbol{\sigma}_{i,h}( \mathbf{z}_{i,h}^{*})}^{2}\right)+(M+L_{\boldsymbol{\varphi}}L_{\boldsymbol{ \theta}})\left(\frac{\sqrt{\lambda}}{4}+4\log\left(\frac{H\sqrt{1+2k}}{\delta} \right)\right)\] \[\quad-(M+L_{\boldsymbol{\varphi}}L_{\boldsymbol{\theta}})\sum_{i= 1}^{k}\|\mathbf{d}_{i,h}\|_{\nabla\boldsymbol{\sigma}_{i,h}(\mathbf{z}_{i,h}^{ *})}^{2}\] \[\leq\frac{5}{4}(M+L_{\boldsymbol{\varphi}}L_{\boldsymbol{\theta} })\lambda+4(M+L_{\boldsymbol{\varphi}}L_{\boldsymbol{\theta}})\log\left(\frac{H \sqrt{1+2k}}{\delta}\right).\] (61)

Finally, combining (58) and (61), by setting \(u=1/k\), we derive that

\[\sum_{i=1}^{k}\left(\ell_{i,h}(\boldsymbol{\theta}_{h}^{*})-\ell( \widetilde{\mathbf{z}}_{i,h},y_{h}^{i})\right)\] \[\leq\frac{5}{4}(M+L_{\boldsymbol{\varphi}}L_{\boldsymbol{\theta} })\lambda+4(M+L_{\boldsymbol{\varphi}}L_{\boldsymbol{\theta}})\log\left(\frac{H \sqrt{1+2k}}{\delta}\right)+2uk\] \[\leq\frac{5}{4}(3\log(\mathcal{U}k)+L_{\boldsymbol{\varphi}}L_{ \boldsymbol{\theta}})\lambda+4(3\log(\mathcal{U}k)+L_{\boldsymbol{\varphi}}L_{ \boldsymbol{\theta}})\log\left(\frac{H\sqrt{1+2k}}{\delta}\right)+2\]

where the last inequality holds by the definition of \(M=\log\mathcal{U}+2\log(\mathcal{U}/u)\). Taking the union bound over \(h\in[H]\), we conclude the proof.

#### d.1.3 Proof of Lemma 15

_Proof of Lemma 15._ We start the proof from the observation of Proposition 2 in Foster et al. [27], stating that \(\widetilde{\mathbf{z}}_{i,h}\) represents the mixed prediction, which adheres to the following property:

\[\ell(\widetilde{\mathbf{z}}_{i,h},y_{h}^{i})\leq-\log\left(\mathbb{ E}_{\boldsymbol{\theta}\sim\mathcal{N}\left(\widetilde{\boldsymbol{\theta}}_{h,c}^{i}\mathbf{B}_{i,h}^{-1}\right)}\left[\exp\left(-\ell_{i,h}(\boldsymbol{ \theta})\right)\right]\right)=-\log\left(\frac{1}{Z_{i,h}}\int_{\mathbb{R}^{d }}\exp\left(-L_{i,h}(\boldsymbol{\theta})\right)\mathrm{d}\boldsymbol{\theta} \right),\] (62)

where \(L_{i,h}(\boldsymbol{\theta}):=\ell_{i,h}(\boldsymbol{\theta})+\frac{1}{2c} \left\|\boldsymbol{\theta}-\widetilde{\boldsymbol{\theta}}_{h}^{i}\right\|_{ \mathbf{B}_{i,h}}^{2}\) and \(Z_{i,h}:=\sqrt{(2\pi)^{d}c|\mathbf{B}_{i,h}^{-1}|}\).

Consider the quadratic approximation

\[\widetilde{L}_{i,h}(\boldsymbol{\theta})=L_{i,h}(\widetilde{ \boldsymbol{\theta}}_{h}^{i+1})+\left\langle\nabla L_{i,h}(\widetilde{ \boldsymbol{\theta}}_{h}^{i+1}),\boldsymbol{\theta}-\widetilde{\boldsymbol{ \theta}}_{h}^{i+1}\right\rangle+\frac{1}{2c}\left\|\boldsymbol{\theta}- \widetilde{\boldsymbol{\theta}}_{h}^{i+1}\right\|_{\mathbf{B}_{i,h}}^{2}.\]

Using the property that \(\ell_{i,h}\) is \(3\sqrt{2}L_{\boldsymbol{\varphi}}\)-self-concordant-like function as asserted by Proposition B.1 in [50], and applying Lemma 35, we obtain

\[L_{i,h}(\boldsymbol{\theta})\leq\widetilde{L}_{i,h}(\boldsymbol{ \theta})+\exp\left(18L_{\boldsymbol{\varphi}}^{2}\left\|\boldsymbol{\theta}- \widetilde{\boldsymbol{\theta}}_{h}^{i+1}\right\|_{2}^{2}\right)\left\| \boldsymbol{\theta}-\widetilde{\boldsymbol{\theta}}_{h}^{i+1}\right\|_{\nabla \ell_{i,h}(\widetilde{\boldsymbol{\theta}}_{h}^{i+1})}^{2}.\]

Also, we have

\[\frac{1}{Z_{i,h}}\int_{\mathbb{R}^{d}}\exp(-L_{i,h}(\boldsymbol{ \theta}))\,\mathrm{d}\boldsymbol{\theta}\] \[\geq\frac{1}{Z_{i,h}}\int_{\mathbb{R}^{d}}\exp\left(-\widetilde{ L}_{i,h}(\boldsymbol{\theta})-\exp\left(18L_{\boldsymbol{\varphi}}^{2} \left\|\boldsymbol{\theta}-\widetilde{\boldsymbol{\theta}}_{h}^{i+1}\right\|_ {2}^{2}\right)\left\|\boldsymbol{\theta}-\widetilde{\boldsymbol{\theta}}_{h}^ {i+1}\right\|_{\nabla\ell_{i,h}(\widetilde{\boldsymbol{\theta}}_{h}^{i+1})}^{2 }\right)\mathrm{d}\boldsymbol{\theta}\] \[=\frac{\exp\left(-L_{i,h}(\widetilde{\boldsymbol{\theta}}_{h}^{i +1})\right)}{Z_{i,h}}\int_{\mathbb{R}^{d}}\widetilde{f}_{i+1,h}(\boldsymbol{ \theta})\cdot\exp\left(-\left\langle\nabla L_{i,h}(\widetilde{\boldsymbol{ \theta}}_{h}^{i+1}),\boldsymbol{\theta}-\widetilde{\boldsymbol{\theta}}_{h}^{i+ 1}\right\rangle\right)\mathrm{d}\boldsymbol{\theta},\] (63)

where we define the function \(\widetilde{f}_{i,h}:\mathcal{B}(\mathbf{0}_{d},1)\to\mathbb{R}\) as

\[\widetilde{f}_{i+1,h}(\boldsymbol{\theta})=\exp\left(-\frac{1}{ 2c}\left\|\boldsymbol{\theta}-\widetilde{\boldsymbol{\theta}}_{h}^{i+1} \right\|_{\mathbf{B}_{i,h}}^{2}-\exp\left(18L_{\boldsymbol{\varphi}}^{2} \left\|\boldsymbol{\theta}-\widetilde{\boldsymbol{\theta}}_{h}^{i+1}\right\|_ {2}^{2}\right)\left\|\boldsymbol{\theta}-\widetilde{\boldsymbol{\theta}}_{h}^ {i+1}\right\|_{\nabla^{2}\ell_{i,h}(\widetilde{\boldsymbol{\theta}}_{h}^{i+1} )}^{2}\right).\]

We denote \(\widetilde{Z}_{i+1,h}=\int_{\mathbb{R}^{d}}\widetilde{f}_{i+1,h}(\boldsymbol{ \theta})\,\mathrm{d}\boldsymbol{\theta}\leq+\infty\) and define \(\widetilde{\Theta}_{i+1,h}\) as the distribution whose density function is \(\widetilde{f}_{i+1,h}(\boldsymbol{\theta})/\widetilde{Z}_{i+1,h}\). Then, we can rewrite (63) as follows:

\[\frac{1}{Z_{i,h}}\int_{\mathbb{R}^{d}}\exp(-L_{i,h}(\boldsymbol{ \theta}))\,\mathrm{d}\boldsymbol{\theta}\] \[\geq\frac{\exp\left(-L_{i,h}(\widetilde{\boldsymbol{\theta}}_{h}^ {i+1})\right)\widetilde{Z}_{i+1,h}}{Z_{i,h}}\mathbb{E}_{\boldsymbol{\theta} \sim\widetilde{\Theta}_{i+1,h}}\left[\exp\left(-\left\langle\nabla L_{i,h}( \widetilde{\boldsymbol{\theta}}_{h}^{i+1}),\boldsymbol{\theta}-\widetilde{ \boldsymbol{\theta}}_{h}^{i+1}\right\rangle\right)\right]\] \[\geq\frac{\exp\left(-L_{i,h}(\widetilde{\boldsymbol{\theta}}_{h}^ {i+1})\right)\widetilde{Z}_{i+1,h}}{Z_{i,h}}\exp\left(-\mathbb{E}_{\boldsymbol{ \theta}\sim\widetilde{\Theta}_{i+1,h}}\left[\left\langle\nabla L_{i,h}( \widetilde{\boldsymbol{\theta}}_{h}^{i+1}),\boldsymbol{\theta}-\widetilde{ \boldsymbol{\theta}}_{h}^{i+1}\right\rangle\right]\right)\] \[=\frac{\exp\left(-L_{i,h}(\widetilde{\boldsymbol{\theta}}_{h}^{i+1} )\right)\widetilde{Z}_{i+1,h}}{Z_{i,h}},\] (64)

where the second inequality is by Jensen's inequality and the last inequality holds because \(\widetilde{\Theta}_{i+1,h}\) is symmetric around \(\widetilde{\boldsymbol{\theta}}_{h}^{i+1}\) and thus \(\mathbb{E}_{\boldsymbol{\theta}\sim\widetilde{\Theta}_{i+1,h}}\left[\left \langle\nabla L_{i,h}(\widetilde{\boldsymbol{\theta}}_{h}^{i+1}),\boldsymbol{ \theta}-\widetilde{\boldsymbol{\theta}}_{h}^{i+1}\right\rangle\right]=0\).

Combining (62) and (64), we get

\[\ell_{i,h}(\widetilde{\mathbf{z}})\leq L_{i,h}(\widetilde{\boldsymbol{\theta}}_{h }^{i+1})+\log Z_{i,h}-\log\widetilde{Z}_{i+1,h}.\] (65)Moreover, we have

\[-\log\widetilde{Z}_{i+1,h}\] \[=-\log\left(\int_{\mathbb{R}^{d}}\exp\left(-\frac{1}{2c}\left\| \boldsymbol{\theta}-\widetilde{\boldsymbol{\theta}}_{h}^{i+1}\right\|_{\mathbf{ B}_{i,h}}^{2}\right.\right.\] \[\qquad\qquad\left.-\exp\left(18L_{\boldsymbol{\varphi}}^{2} \left\|\boldsymbol{\theta}-\widetilde{\boldsymbol{\theta}}_{h}^{i+1}\right\|_{ 2}^{2}\right)\left\|\boldsymbol{\theta}-\widetilde{\boldsymbol{\theta}}_{h}^{i+ 1}\right\|_{\nabla^{2}\ell_{i,h}(\widetilde{\boldsymbol{\theta}}_{h}^{i+1})}^{2 }\right)\mathrm{d}\boldsymbol{\theta}\right)\] \[=-\log\left(\widehat{Z}_{i+1,h}\cdot\mathbb{E}_{\boldsymbol{ \theta}\sim\widehat{\Theta}_{i+1,h}}\left[\exp\left(-\exp\left(18L_{ \boldsymbol{\varphi}}^{2}\left\|\boldsymbol{\theta}-\widetilde{\boldsymbol{ \theta}}_{h}^{i+1}\right\|_{2}^{2}\right)\left\|\boldsymbol{\theta}- \widetilde{\boldsymbol{\theta}}_{h}^{i+1}\right\|_{\nabla^{2}\ell_{i,h}( \widetilde{\boldsymbol{\theta}}_{h}^{i+1})}^{2}\right)\right]\right)\] \[\leq-\log\widehat{Z}_{i+1,h}+\mathbb{E}_{\boldsymbol{\theta}\sim \widehat{\Theta}_{i+1,h}}\left[\exp\left(18L_{\boldsymbol{\varphi}}^{2}\left\| \boldsymbol{\theta}-\widetilde{\boldsymbol{\theta}}_{h}^{i+1}\right\|_{2}^{2} \right)\left\|\boldsymbol{\theta}-\widetilde{\boldsymbol{\theta}}_{h}^{i+1} \right\|_{\nabla^{2}\ell_{i,h}(\widetilde{\boldsymbol{\theta}}_{h}^{i+1})}^{2 }\right]\] \[=-\log Z_{i,h}+\mathbb{E}_{\boldsymbol{\theta}\sim\widehat{ \Theta}_{i+1,h}}\left[\exp\left(18L_{\boldsymbol{\varphi}}^{2}\left\| \boldsymbol{\theta}-\widetilde{\boldsymbol{\theta}}_{h}^{i+1}\right\|_{2}^{2} \right)\left\|\boldsymbol{\theta}-\widetilde{\boldsymbol{\theta}}_{h}^{i+1} \right\|_{\nabla^{2}\ell_{i,h}(\widetilde{\boldsymbol{\theta}}_{h}^{i+1})}^{2 }\right],\] (66)

where \(\widehat{\Theta}_{i+1,h}=\mathcal{N}(\widetilde{\boldsymbol{\theta}}_{h}^{i+1 },c\mathbf{B}_{i,h}^{-1})\) and \(\widehat{Z}_{i+1,h}=\int_{\mathbb{R}^{d}}\exp\left(-\frac{1}{2c}\left\| \boldsymbol{\theta}-\widetilde{\boldsymbol{\theta}}_{h}^{i+1}\right\|_{ \mathbf{B}_{i,h}}^{2}\right)\mathrm{d}\boldsymbol{\theta}\), and the last inequality holds because \(\widehat{Z}_{i+1,h}\) and \(Z_{i,h}\) are identical normalizing factors. Integrating (65) and (66) and summing over \(k\), yields

\[\sum_{i=1}^{k}\ell(\widetilde{\mathbf{z}}_{i,h},y_{h}^{i})\] \[=\sum_{i=1}^{k}L_{i,h}(\widetilde{\boldsymbol{\theta}}_{h}^{i+1}) +\sum_{i=1}^{k}\mathbb{E}_{\boldsymbol{\theta}\sim\widehat{\Theta}_{i+1,h}} \left[\exp\left(18L_{\boldsymbol{\varphi}}^{2}\left\|\boldsymbol{\theta}- \widetilde{\boldsymbol{\theta}}_{h}^{i+1}\right\|_{2}^{2}\right)\left\| \boldsymbol{\theta}-\widetilde{\boldsymbol{\theta}}_{h}^{i+1}\right\|_{\nabla ^{2}\ell_{i,h}(\widetilde{\boldsymbol{\theta}}_{h}^{i+1})}^{2}\right]\,.\]

Moreover, we can further bound the second term on the right-hand side of (66). By Cauchy-Schwarz inequality, we get

\[\mathbb{E}_{\boldsymbol{\theta}\sim\widehat{\Theta}_{i+1,h}} \left[\exp\left(18L_{\boldsymbol{\varphi}}^{2}\left\|\boldsymbol{\theta}- \widetilde{\boldsymbol{\theta}}_{h}^{i+1}\right\|_{2}^{2}\right)\left\| \boldsymbol{\theta}-\widetilde{\boldsymbol{\theta}}_{h}^{i+1}\right\|_{\nabla ^{2}\ell_{i,h}(\widetilde{\boldsymbol{\theta}}_{h}^{i+1})}^{2}\right]\] \[\leq\underbrace{\sqrt{\mathbb{E}_{\boldsymbol{\theta}\sim\widehat{ \Theta}_{i+1,h}}\left[\exp\left(36L_{\boldsymbol{\varphi}}^{2}\left\| \boldsymbol{\theta}-\widetilde{\boldsymbol{\theta}}_{h}^{i+1}\right\|_{2}^{2} \right)\right]}}_{\mathrm{(I)}}\underbrace{\sqrt{\mathbb{E}_{\boldsymbol{ \theta}\sim\widehat{\Theta}_{i+1,h}}\left[\left\|\boldsymbol{\theta}- \widetilde{\boldsymbol{\theta}}_{h}^{i+1}\right\|_{\nabla^{2}\ell_{i,h}( \widetilde{\boldsymbol{\theta}}_{h}^{i+1})}^{4}\right]}}_{\mathrm{(II)}}.\]

Since \(\widehat{\Theta}_{i+1,h}=\mathcal{N}\left(\widetilde{\boldsymbol{\theta}}_{h} ^{i+1},c\mathbf{B}_{i,h}^{-1}\right)\), \(\boldsymbol{\theta}-\widetilde{\boldsymbol{\theta}}_{h}^{i+1}\) follows the same distribution as

\[\sum_{j=1}^{d}\sqrt{c\lambda_{j}\left(\mathbf{B}_{i,h}^{-1}\right)}X_{j} \mathbf{e}_{j},\quad\text{where }X_{j}\overset{i.i.d.}{\sim}\mathcal{N}(0,1),\forall j\in[d],\] (67)

where \(\lambda_{j}\left(\mathbf{B}_{i,h}^{-1}\right)\) denotes the \(j\)-th largest eigenvalue of \(\mathbf{B}_{i,h}^{-1}\) and \(\{\mathbf{e}_{1},\ldots,\mathbf{e}_{d}\}\) are orthogonal basis of \(\mathbb{R}^{d}\). Furthermore, since we know that \(\mathbf{B}_{i,h}^{-1}\leq\lambda^{-1}\mathbf{I}_{d}\), we can bound the term \(\mathrm{(I)}\) by

\[\mathrm{(I)} \leq\sqrt{\mathbb{E}_{X_{j}}\left[\prod_{j=1}^{d}\exp\left(36L_{ \boldsymbol{\varphi}}^{2}c\lambda^{-1}X_{j}^{2}\right)\right]}=\sqrt{\prod_{j=1 }^{d}\mathbb{E}_{X_{j}}\left[\exp\left(36L_{\boldsymbol{\varphi}}^{2}c\lambda^{- 1}X_{j}^{2}\right)\right]}\] \[\leq\left(\mathbb{E}_{W\sim\chi^{2}}\left[\exp\left(36L_{ \boldsymbol{\varphi}}^{2}c\lambda^{-1}W\right)\right]\right)^{\frac{d}{2}}\leq \mathbb{E}_{W\sim\chi^{2}}\left[\exp\left(18L_{\boldsymbol{\varphi}}^{2}c \lambda^{-1}Wd\right)\right]\]

where \(\chi^{2}\) is the chi-square distribution and the last inequality holds due to Jensen's inequality. By choosing \(\lambda\geq 72L_{\boldsymbol{\varphi}}^{2}cd\), we arrive that

\[\mathrm{(I)}\leq\mathbb{E}_{W\sim\chi^{2}}\left[\exp\left(\frac{W}{4}\right) \right]\leq\sqrt{2},\] (68)where the last inequality holds because the moment-generating function for -distribution is bounded by. Now, we bound the term.

where. Let be the -th largest eigenvalue of the matrix. Then, a similar analysis as (67) gives that

where the last inequality holds due to when considering the case where and the last equality is derived from the fact that. Here, we denote as the trace of the matrix.

We define matrix. Under the condition, we have. Then, we can bound the trace by

(69)

Combining (68) and (69), we get

(70)

Plugging (66) and (70) into (65), and taking summation over, we derive that

where the last inequality holds because. By rearranging the terms, we conclude the proof.

### Bound on Prediction Error

In this section, we present the bound on the prediction error of parameters updated by ORRL-MNL. First, we compare the problem setting of MNL contextual bandits with ours and introduce the challenges of applying their analysis to our setting.

**MNL dynamic assortment optimization (single-parameter & uniform reward) [61]**  Perivier and Goyal [61] consider an assortment selection problem where the user choice is given by a MNL choice model with the single-parameter. At each time \(t\), the agent observes context features \(\{\mathbf{x}_{t,i}\}_{i=1}^{M}\subset\mathbb{R}^{d}\). Then the agent decides on the set \(S_{t}\subset[M]\) to offer to a user, with \(|S_{t}|\leq N\). Without loss of generality, we may assume \(|S_{t}|=N\). Then the user purchases one single product \(j\in S_{t}\cup\{0\}\) and the probability of each product \(j\) is purchased by a user follows the MNL model parametrized by a unknown fixed parameter \(\bm{\theta}^{*}\in\mathbb{R}^{d}\),

\[q_{t,j}(S_{t},\bm{\theta}^{*}):=\begin{cases}\frac{\exp(\mathbf{x}_{t,i}^{ \top},\bm{\theta}^{*})}{1+\sum_{k\in S_{t}}\exp(\mathbf{x}_{k}^{\top}\bm{ \theta}^{*})}&\text{if }j\in S_{t}\\ \frac{1}{1+\sum_{k\in S_{t}}\exp(\mathbf{x}_{k}^{\top}\bm{\theta}^{*})}& \text{if }j=0\,.\end{cases}\]

Then the difference between the revenue induced by \(\bm{\theta}^{*}\) and that by an estimator \(\bm{\theta}\) in Perivier and Goyal [61] is expressed as follows:

\[\sum_{j\in S_{t}}q_{t,j}(S_{t},\bm{\theta}^{*})-\sum_{j\in S_{t}}q_{t,j}(S_{t},\bm{\theta})\,.\] (71)

If we define \(Q:\mathbb{R}^{N}\to\mathbb{R}\), such that for all \(\mathbf{u}=(u_{1},\ldots,u_{N})\in\mathbb{R}^{N}\), \(Q(\mathbf{u}):=\sum_{i=1}^{N}\frac{\exp(u_{i})}{1+\sum_{j=1}^{N}\exp(u_{j})}\) and let \(\mathbf{v}^{*}=(\mathbf{x}_{t,i_{1}}^{\top}\bm{\theta}^{*},\ldots,\mathbf{x}_ {t,i_{N}}^{\top}\bm{\theta}^{*})\) and \(\mathbf{v}=(\mathbf{x}_{t,i_{1}}^{\top}\bm{\theta},\ldots,\mathbf{x}_{t,i_{N} }^{\top}\bm{\theta})\), then Eq. (71) can be expressed as follows:

\[\sum_{j\in S_{t}}q_{t,j}(S_{t},\bm{\theta}^{*})-\sum_{j\in S_{t}} q_{t,j}(S_{t},\bm{\theta}) =Q(\mathbf{v}^{*})-Q(\mathbf{v})\] \[=\nabla Q(\mathbf{v}^{*})^{\top}(\mathbf{v}^{*}-\mathbf{v})+ \frac{1}{2}(\mathbf{v}^{*}-\mathbf{v})^{\top}\nabla^{2}Q(\bar{\mathbf{v}})( \mathbf{v}^{*}-\mathbf{v})\,,\] (72)

where \(\bar{\mathbf{v}}\) is a convex combination of \(\mathbf{v}^{*}\) and \(\mathbf{v}\). For the first term in Eq. (72), we have

\[\nabla Q(\mathbf{v}^{*})^{\top}(\mathbf{v}^{*}-\mathbf{v})\] \[=\frac{\sum_{i\in S_{t}}\exp(\mathbf{x}_{t,j}^{\top}\bm{\theta}^ {*})(v_{j}-v_{j}^{*})}{1+\sum_{j\in S_{t}}\exp(\mathbf{x}_{t,j}^{\top}\bm{ \theta}^{*})}-\frac{\sum_{i\in S_{t}}\exp(\mathbf{x}_{t,j}^{\top}\bm{\theta}^ {*})\sum_{i\in S_{t}}\exp(\mathbf{x}_{t,i}^{\top}\bm{\theta}^{*})(v_{j}-v_{j}^ {*})}{\left(1+\sum_{j\in S_{t}}\exp(\mathbf{x}_{t,j}^{\top}\bm{\theta}^{*}) \right)^{2}}\] \[=\sum_{j\in S_{t}}q_{t,j}(S_{t},\bm{\theta}^{*})\mathbf{x}_{t,j}^{ \top}(\bm{\theta}^{*}-\bm{\theta})-\sum_{j\in S_{t}}\sum_{i\in S_{t}}q_{t,j}(S _{t},\bm{\theta}^{*})q_{t,j}(S_{t},\bm{\theta}^{*})\mathbf{x}_{t,i}^{\top}( \bm{\theta}^{*}-\bm{\theta})\] \[=\sum_{j\in S_{t}}q_{t,j}(S_{t},\bm{\theta}^{*})\left(1-\sum_{i\in S _{t}}q_{t,i}(S_{t},\bm{\theta}^{*})\right)\mathbf{x}_{t,j}^{\top}(\bm{\theta}^ {*}-\bm{\theta})\] \[=\sum_{j\in S_{t}}q_{t,j}(S_{t},\bm{\theta}^{*})q_{t,0}(S_{t},\bm {\theta}^{*})\mathbf{x}_{t,j}^{\top}(\bm{\theta}^{*}-\bm{\theta})\] \[\leq\sum_{j\in S_{t}}q_{t,j}(S_{t},\bm{\theta}^{*})q_{t,0}(S_{t}, \bm{\theta}^{*})\|\mathbf{x}_{t,j}\|_{\mathbf{H}_{t}^{-1}(\bm{\theta}^{*})}\| \bm{\theta}^{*}-\bm{\theta}\|_{\mathbf{H}_{t}(\bm{\theta}^{*})}\,,\] (73)

where \(\mathbf{H}_{t}(\bm{\theta})\) is the Gram matrix used in [61] defined by

\[\mathbf{H}_{t}(\bm{\theta}^{*}):=\sum_{\tau=1}^{t-1}\sum_{j\in S_{\tau}}q_{\tau, j}(S_{\tau},\bm{\theta}^{*})\mathbf{x}_{\tau,j}\mathbf{x}_{\tau,j}^{\top}-\sum_{j\in S _{\tau}}\sum_{i\in S_{\tau}}q_{\tau,j}(S_{\tau},\bm{\theta}^{*})q_{\tau,i}(S_{ \tau},\bm{\theta}^{*})\mathbf{x}_{\tau,j}\mathbf{x}_{\tau,i}^{\top}\,.\]

Note that the term \(\|\bm{\theta}^{*}-\bm{\theta}\|_{\mathbf{H}_{t}(\bm{\theta}^{*})}\) can be bounded by the concentration result of the estimated parameter. On the other hand, to apply the elliptical potential lemma to the term\(\sum_{j\in S_{t}}q_{t,j}(S_{t},\bm{\theta}^{*})q_{t,0}(S_{t},\bm{\theta}^{*})||{\bf x }_{t,j}\|_{{\bf H}_{t}^{-1}(\bm{\theta}^{*})}\), note that \({\bf H}_{t}(\bm{\theta}^{*})\) can be bounded as follows:

\[{\bf H}_{t}(\bm{\theta}^{*})\] \[={\bf H}_{t-1}(\bm{\theta}^{*})+\sum_{j\in S_{t}}q_{t,j}(S_{t}, \bm{\theta}^{*}){\bf x}_{t,j}{\bf x}_{t,j}^{\top}-\frac{1}{2}\sum_{i,j\in S_{t }}q_{t,j}(S_{t},\bm{\theta}^{*})q_{t,i}(S_{t},\bm{\theta}^{*})\left({\bf x}_{t,j}{\bf x}_{t,i}^{\top}+{\bf x}_{t,i}{\bf x}_{t,j}^{\top}\right)\] \[\succeq{\bf H}_{t-1}(\bm{\theta}^{*})+\sum_{j\in S_{t}}q_{t,j}(S_ {t},\bm{\theta}^{*}){\bf x}_{t,j}{\bf x}_{t,j}^{\top}-\frac{1}{2}\sum_{i,j\in S _{t}}q_{t,j}(S_{t},\bm{\theta}^{*})q_{t,i}(S_{t},\bm{\theta}^{*})\left({\bf x} _{t,j}{\bf x}_{t,j}^{\top}+{\bf x}_{t,i}{\bf x}_{t,i}^{\top}\right)\] \[={\bf H}_{t-1}(\bm{\theta}^{*})+\sum_{j\in S_{t}}q_{t,j}(S_{t}, \bm{\theta}^{*})\left(1-\sum_{i\in S_{t}}q_{t,i}(S_{t},\bm{\theta}^{*})\right){ \bf x}_{t,j}{\bf x}_{t,j}^{\top}\] \[={\bf H}_{t-1}(\bm{\theta}^{*})+\sum_{j\in S_{t}}q_{t,j}(S_{t}, \bm{\theta}^{*})q_{t,0}(S_{t},\bm{\theta}^{*}){\bf x}_{t,j}{\bf x}_{t,j}^{\top}\,.\] (74)

Now since the coefficient \(q_{t,j}(S_{t},\bm{\theta}^{*})q_{t,0}(S_{t},\bm{\theta}^{*})\) of \(\|{\bf x}\|_{{\bf H}_{t}^{-1}(\bm{\theta}^{*})}\) in Eq. (73) aligns with the coefficients of the lower bound of \({\bf H}_{t}(\bm{\theta}^{*})\) in Eq. (74), the elliptical potential lemma can be applied. Note that such a lower bound in Eq. (74) holds since Perivier and Goyal [61] deals with the uniform reward, i.e., \(1-\sum_{i\in S_{t}}q_{t,i}(S_{t},\bm{\theta}^{*})=q_{t,0}(S_{t},\bm{\theta}^{ *})\).

**Multinomial logistic bandit problem [76]** Zhang and Sugiyama [76] address the multiple-parameter MNL contextual bandit problem where at each time step \(t\) the agent selects an action \({\bf x}_{t}\in\mathbb{R}^{d}\) and receives response feedback \(y_{t}\in\{0\}\cup[N]\) with \(N+1\) possible outcomes. Each outcome \(i\in[N]\) is associated with a ground-truth parameter \(\bm{\theta}_{i}^{*}\in\mathbb{R}^{d}\), and the probability of the outcome \(\mathbb{P}(y_{t}=i\mid{\bf x}_{t})\) follows the MNL model,

\[\mathbb{P}(y_{t}=i\mid{\bf x}_{t})=\frac{\exp({\bf x}_{t}^{\top}\bm{\theta}_{ i}^{*})}{1+\sum_{j=1}^{N}\exp({\bf x}_{t}^{\top}\bm{\theta}_{j}^{*})}\,, \quad\mathbb{P}(y_{t}=0\mid{\bf x}_{t})=1-\sum_{j=1}^{N}\mathbb{P}(y_{t}=j \mid{\bf x}_{t})\,.\]

In this model, there are \(N\) unknown choice parameter \(\bm{\Theta}^{*}:=[\bm{\theta}_{1}^{*},\dots,\bm{\theta}_{N}^{*}]\in\mathbb{R}^ {d\times N}\) and the agent chooses one context feature \({\bf x}_{t}\), that is why we call multiple-parameter MNL model. Then, the expected revenue of an action \({\bf x}_{t}\) in [76] is given by

\[\sum_{i=1}^{N}\frac{\exp({\bf x}_{t}^{\top}\bm{\theta}_{i}^{*})\rho_{i}}{1+\sum_ {j=1}^{N}\exp({\bf x}_{t}^{\top}\bm{\theta}_{j}^{*})}:=\bm{\rho}^{\top}\bm{ \sigma}(\bm{\Theta}^{*}{\bf x}_{t})\,,\]

where we define the softmax function \(\bm{\sigma}:\mathbb{R}^{N}\rightarrow[0,1]^{N}\) by

\[[\bm{\sigma}({\bf z})]_{k}=\frac{\exp([{\bf z}]_{i})}{1+\sum_{j=1}^{N}\exp([{ \bf z}]_{j})}\quad\forall k\in[N]\quad\text{and}\quad[\bm{\sigma}({\bf z})]_{0 }=\frac{1}{1+\sum_{j=1}^{N}\exp([{\bf z}]_{j})}\quad\forall k\in[N]\,,\]

and \(\bm{\rho}:=[\rho_{1},\dots,\rho_{N}]\in\mathbb{R}_{+}^{N+1}\) represents the reward for each outcome \(j\in[N]\) with \(\rho_{0}=0\). Then, the difference between the revenue induced by \(\bm{\Theta}^{*}\) and that by an estimator \(\hat{\bm{\Theta}}\) in [76] is expressed by

\[\bm{\rho}^{\top}\left(\bm{\sigma}(\bm{\Theta}^{*}{\bf x}_{t})-\bm{ \sigma}(\hat{\bm{\Theta}}{\bf x}_{t})\right)\] \[=\sum_{k=1}^{N}\rho_{k}\left([\bm{\sigma}(\bm{\Theta}^{*}{\bf x}_{ t})]_{k}-[\bm{\sigma}(\hat{\bm{\Theta}}{\bf x}_{t})]_{k}\right)\] \[=\sum_{k=1}^{N}\rho_{k}\left(\nabla[\bm{\sigma}(\hat{\bm{\Theta}}{ \bf x}_{t})]_{k}\right)^{\top}(\bm{\Theta}^{*}-\hat{\bm{\Theta}}){\bf x}_{t}+ \sum_{k=1}^{N}\rho_{k}\|(\bm{\Theta}^{*}-\bm{\Theta}){\bf x}_{t}\|_{\bm{\Xi}_{k}}\,,\] (75)where \(\bm{\Xi}_{k}=\int_{0}^{1}(1-\nu)\nabla^{2}[\bm{\sigma}(\hat{\bm{\Theta}}\mathbf{x} _{t}+\nu(\bm{\Theta}^{*}-\hat{\bm{\Theta}})\mathbf{x}_{t})]_{k}d\nu\). Then for the first term in Eq. (75), we have

\[\sum_{k=1}^{N}\rho_{k}\left(\nabla[\bm{\sigma}(\hat{\bm{\Theta}} \mathbf{x}_{t})]_{k}\right)^{\top}(\bm{\Theta}^{*}-\hat{\bm{\Theta}})\mathbf{x} _{t}\] \[\leq\left|\bm{\rho}^{\top}\nabla\bm{\sigma}(\hat{\bm{\Theta}} \mathbf{x}_{t})(\bm{\Theta}^{*}-\hat{\bm{\Theta}})\mathbf{x}_{t}\right|\] \[=\left|\bm{\rho}^{\top}\nabla\bm{\sigma}(\hat{\bm{\Theta}} \mathbf{x}_{t})(\mathbf{I}_{N}\otimes\mathbf{x}_{t}^{\top})(\mathrm{vec}(\bm {\Theta}^{*})-\mathrm{vec}(\hat{\bm{\Theta}}))\right|\] \[\leq\|\mathrm{vec}(\bm{\Theta}^{*})-\mathrm{vec}(\hat{\bm{ \Theta}})\|_{\mathbf{H}_{t}}\|\mathbf{H}_{t}^{-\frac{1}{2}}(\mathbf{I}_{N} \otimes\mathbf{x}_{t}^{\top})\nabla\bm{\sigma}(\hat{\bm{\Theta}}\mathbf{x}_{ t})\bm{\rho}\|_{2}\] (76)

where \(\mathbf{H}_{t}\) is the Gram matrix used in [76] defined by

\[\mathbf{H}_{t}:=\lambda\mathbf{I}_{N}+\sum_{s=1}^{t-1}\nabla\bm{\sigma}(\hat{ \bm{\Theta}}_{s+1}\mathbf{x}_{s})\otimes\mathbf{x}_{s}\mathbf{x}_{s}^{\top}\,.\]

Note that the term \(\|\mathrm{vec}(\bm{\Theta}^{*})-\mathrm{vec}(\hat{\bm{\Theta}})\|_{\mathbf{H }_{t}}\) in Eq. (76) can be bounded by the concentration result of the estimated parameter, and the term \(\|\mathbf{H}_{t}^{-\frac{1}{2}}(\mathbf{I}_{N}\otimes\mathbf{x}_{t}^{\top}) \nabla\bm{\sigma}(\hat{\bm{\Theta}}\mathbf{x}_{t})\bm{\rho}\|_{2}\) also can be bounded as follows:

\[\|\mathbf{H}_{t}^{-\frac{1}{2}}(\mathbf{I}_{N}\otimes\mathbf{x}_{t}^{\top}) \nabla\bm{\sigma}(\hat{\bm{\Theta}}\mathbf{x}_{t})\bm{\rho}\|_{2}\leq\|\bm{ \rho}\|_{2}\|\mathbf{H}_{t}^{-\frac{1}{2}}(\mathbf{I}_{N}\otimes\mathbf{x}_{t }^{\top})\nabla\bm{\sigma}(\hat{\bm{\Theta}}\mathbf{x}_{t})\|_{2}\,.\]

Here Zhang and Sugiyama [76] bound the term \(\|\mathbf{H}_{t}^{-\frac{1}{2}}(\mathbf{I}_{N}\otimes\mathbf{x}_{t}^{\top}) \nabla\bm{\sigma}(\hat{\bm{\Theta}}\mathbf{x}_{t})\|_{2}\) using a matrix version of elliptical lemma. However, they assume \(\|\bm{\rho}\|_{2}\leq R\) (Assumption 2 in [76]).

Now, regarding the prediction error in our setting, the estimated values (\(\widetilde{V}_{h+1}^{k}(\cdot)\)) for each reachable state are typically distinct, and we do not assume a constant upper bound on the \(\ell_{2}\)-norm of the estimated value vector for all reachable states. Instead, we can bound the \(\ell_{2}\)-norm of the estimated value vector for all reachable states as follows:

\[\|\widetilde{\mathbf{V}}_{h+1}^{k}(s,a)\|_{2}\leq\max_{s^{\prime}\in\mathcal{ S}_{s,a}}\left|\widetilde{V}_{h+1}^{k}(s^{\prime})\right|\sqrt{|\mathcal{S}_{s,a} |}\leq H\sqrt{\mathcal{U}}\,,\]

where \(\widetilde{\mathbf{V}}_{h+1}^{k}(s,a):=\left[\widetilde{V}_{h+1}^{k}(s^{ \prime})\right]_{s^{\prime}\in\mathcal{S}_{s,a}}\in\mathbb{R}^{|\mathcal{S}_{s,a}|}\). However, such a bound leads to a looser regret by a factor of \(\sqrt{\mathcal{U}}\). To address, we adapt the _feature centralization technique_[50] to bound the prediction error independently of \(\mathcal{U}\), without making any additional assumptions. The key point is that the Hessian of per-round loss \(\ell_{k,h}(\bm{\theta})\) is expressed in terms of the centralized feature as follows:

\[\nabla^{2}\ell_{k,h}(\bm{\theta})=\sum_{s^{\prime}\in\mathcal{S}_{k,h}}P_{\bm {\theta}}(s^{\prime}\mid s_{h}^{k},a_{h}^{k})\bm{\varphi}(s_{h}^{k},a_{h}^{k}, s^{\prime};\bm{\theta})\bm{\varphi}(s_{h}^{k},a_{h}^{k},s^{\prime};\bm{\theta})^{ \top}\,.\]

where \(\bm{\varphi}(s,a,s^{\prime};\bm{\theta}):=\bm{\varphi}(s,a,s^{\prime})-\mathbb{ E}_{\widetilde{s}\sim P_{\bm{\theta}}(\cdot\mid s,a)}[\bm{\varphi}(s,a,\widetilde{s})]\) is the centralized feature by \(\bm{\theta}\). Now, we provide the bound on prediction error of the estimated parameter updated by ORRL-MNL.

**Lemma 16** (Bound on the prediction error).: _For any \(\delta\in(0,1)\), suppose that Lemma 12 holds. Let us denote the prediction error about \(\bm{\widehat{\theta}}_{h}^{k}\) by_

\[\Delta_{h}^{k}(s,a):=\sum_{s^{\prime}\in\mathcal{S}_{s,a}}\left(P_{\bm{ \widehat{\theta}}_{h}^{k}}(s^{\prime}\mid s,a)-P_{\bm{\theta}_{h}^{*}}(s^{ \prime}\mid s,a)\right)\widetilde{V}_{h+1}^{k}(s^{\prime})\,.\]

_Then, for any \((s,a)\in\mathcal{S}\times\mathcal{A}\), we have_

\[|\Delta_{h}^{k}(s,a)|\leq H\beta_{k}(\delta)\sum_{s^{\prime}\in\mathcal{S}_{s,a }}P_{\bm{\widehat{\theta}}_{h}^{k}}(s^{\prime}\mid s,a)\left\|\bm{\varphi}_{s,a, s^{\prime}}(\bm{\widehat{\theta}}_{h}^{k})\right\|_{\mathbf{B}_{h,h}^{-1}}+3H \beta_{k}(\delta)^{2}\max_{s^{\prime}\in\mathcal{S}_{s,a}}\|\bm{\varphi}_{s,a,s^{ \prime}}\|_{\mathbf{B}_{h,h}^{-1}}^{2}\,.\]

Proof of Lemma 16.: Let us define \(F(\bm{\theta}):=\sum_{s^{\prime}\in\mathcal{S}_{s,a}}P_{\bm{\theta}}(s^{\prime} \mid s,a)\widetilde{V}_{h+1}^{k}(s^{\prime})\). Then, by Taylor expansion we have

\[F(\bm{\theta}_{h}^{*})=F(\bm{\widehat{\theta}}_{h}^{k})+\nabla F(\bm{\widehat{ \theta}}_{h}^{k})^{\top}(\bm{\theta}_{h}^{*}-\bm{\widehat{\theta}}_{h}^{k})+ \frac{1}{2}(\bm{\theta}_{h}^{*}-\bm{\widehat{\theta}}_{h}^{k})^{\top}\nabla^{2}F( \bm{\bar{\theta}})(\bm{\theta}_{h}^{*}-\bm{\widehat{\theta}}_{h}^{k})\,,\]where \(\bar{\bm{\theta}}=(1-v)\bm{\theta}_{h}^{*}+v\bar{\bm{\theta}}_{h}^{k}\) for some \(v\in(0,1)\). By Proposition 1, we have

\[\nabla F(\bm{\theta}) =\sum_{s^{\prime}\in\mathcal{S}_{s,a}}\nabla P_{\bm{\theta}}(s^{ \prime}\mid s,a)\widetilde{V}_{h+1}^{k}(s^{\prime})\] \[=\sum_{s^{\prime}\in\mathcal{S}_{s,a}}P_{\bm{\theta}}(s^{\prime} \mid s,a)\left(\bm{\varphi}_{s,a,s^{\prime}}-\sum_{\widetilde{x}\in\mathcal{S} _{s,a}}P_{\bm{\theta}}(\widetilde{s}\mid s,a)\bm{\varphi}_{s,a,\widetilde{s}} \right)\widetilde{V}_{h+1}^{k}(s^{\prime})\] \[=\sum_{s^{\prime}\in\mathcal{S}_{s,a}}P_{\bm{\theta}}(s^{\prime} \mid s,a)\bar{\bm{\varphi}}_{s,a,s^{\prime}}(\bm{\theta})\widetilde{V}_{h+1}^ {k}(s^{\prime})\,,\]

and

\[\nabla^{2}F(\bm{\theta})=\sum_{s^{\prime}\in\mathcal{S}_{s,a}} \nabla^{2}P_{\bm{\theta}}(s^{\prime}\mid s,a)\widetilde{V}_{h+1}^{k}(s^{\prime})\] \[=\sum_{s^{\prime}\in\mathcal{S}_{s,a}}P_{\bm{\theta}}(s^{\prime} \mid s,a)\widetilde{V}_{h+1}^{k}(s^{\prime})\bm{\varphi}_{s,a,s^{\prime}}\bm{ \varphi}_{s,a,s^{\prime}}^{\top}\] \[\quad\quad-\sum_{s^{\prime}\in\mathcal{S}_{s,a}}P_{\bm{\theta}}(s ^{\prime}\mid s,a)\widetilde{V}_{h+1}^{k}(s^{\prime})\] \[\quad\quad\quad\cdot\sum_{s^{\prime\prime}\in\mathcal{S}_{s,a}}P_ {\bm{\theta}}(s^{\prime\prime}\mid s,a)\left(\bm{\varphi}_{s,a,s^{\prime}} \bm{\varphi}_{s,a,s^{\prime\prime}}^{\top}+\bm{\varphi}_{s,a,s^{\prime\prime}} \bm{\varphi}_{s,a,s^{\prime}}^{\top}+\bm{\varphi}_{s,a,s^{\prime\prime}}\bm{ \varphi}_{s,a,s^{\prime\prime}}^{\top}\right)\] \[\quad\quad+2\sum_{s^{\prime}\in\mathcal{S}_{s,a}}P_{\bm{\theta}}(s ^{\prime}\mid s,a)\widetilde{V}_{h+1}^{k}(s^{\prime})\] \[\quad\quad\quad\cdot\left(\sum_{s^{\prime\prime}\in\mathcal{S}_{ s,a}}P_{\bm{\theta}}(s^{\prime\prime}\mid s,a)\bm{\varphi}_{s,a,s^{\prime\prime}} \right)\left(\sum_{s^{\prime\prime}\in\mathcal{S}_{s,a}}P_{\bm{\theta}}(s^{ \prime\prime}\mid s,a)\bm{\varphi}_{s,a,s^{\prime\prime}}\right)^{\top}.\]

Then, the prediction error can be bounded as follows:

\[|\Delta_{h}^{k}(s,a)| =|F(\bm{\theta}_{h}^{*})-F(\widetilde{\bm{\theta}}_{h}^{k})|\] \[\leq\left|\nabla F(\widetilde{\bm{\theta}}_{h}^{k})^{\top}( \widetilde{\bm{\theta}}_{h}^{k}-\bm{\theta}_{h}^{*})\right|+\frac{1}{2}\left|( \widetilde{\bm{\theta}}_{h}^{k}-\bm{\theta}_{h}^{*})^{\top}\nabla^{2}F(\bar{ \bm{\theta}})(\widetilde{\bm{\theta}}_{h}^{k}-\bm{\theta}_{h}^{*})\right|\,.\] (77)

For the first term in Eq. (77),

\[\left|\nabla F(\widetilde{\bm{\theta}}_{h}^{k})^{\top}( \widetilde{\bm{\theta}}_{h}^{k}-\bm{\theta}_{h}^{*})\right| =\left|\sum_{s^{\prime}\in\mathcal{S}_{s,a}}P_{\widetilde{\bm{ \theta}}_{h}^{k}}(s^{\prime}\mid s,a)\bar{\bm{\varphi}}_{s,a,s^{\prime}}( \widetilde{\bm{\theta}}_{h}^{k})^{\top}(\widetilde{\bm{\theta}}_{h}^{k}-\bm{ \theta}_{h}^{*})\widetilde{V}_{h+1}^{k}(s^{\prime})\right|\] \[\leq H\sum_{s^{\prime}\in\mathcal{S}_{s,a}}P_{\widetilde{\bm{ \theta}}_{h}^{k}}(s^{\prime}\mid s,a)\left\|\bar{\bm{\varphi}}_{s,a,s^{\prime}}( \widetilde{\bm{\theta}}_{h}^{k})\right\|_{\mathbf{B}_{h,h}^{-1}}\left\| \widetilde{\bm{\theta}}_{h}^{k}-\bm{\theta}_{h}^{*}\right\|_{\mathbf{B}_{h,h}}\] \[\leq H\beta_{k}(\delta)\sum_{s^{\prime}\in\mathcal{S}_{s,a}}P_{ \widetilde{\bm{\theta}}_{h}^{k}}(s^{\prime}\mid s,a)\left\|\bar{\bm{\varphi}}_ {s,a,s^{\prime}}(\widetilde{\bm{\theta}}_{h}^{k})\right\|_{\mathbf{B}_{h,h}^{-1} }\,,\] (78)

where in the first inequality we use \(\widetilde{V}_{h+1}^{k}(s^{\prime})\leq H\) and Cauchy-Scharwz inequality, and the second inequality follows by the concentration result of Lemma 12.

For the second term in Eq. (77), since \(0\leq\widetilde{V}_{h+1}^{k}(s^{\prime})\leq H\),

\[\left|(\widetilde{\bm{\theta}}_{h}^{k}-\bm{\theta}_{h}^{*})^{\top} \nabla^{2}F(\bar{\bm{\theta}})(\widetilde{\bm{\theta}}_{h}^{k}-\bm{\theta}_{h}^{* })\right|\] \[\leq H\sum_{s^{\prime}\in\mathcal{S}_{s,a}}P_{\bar{\bm{\theta}}}(s^ {\prime}\mid s,a)\left((\widetilde{\bm{\theta}}_{h}^{k}-\bm{\theta}_{h}^{*})^{ \top}\bm{\varphi}_{s,a,s^{\prime}}\right)^{2}\] \[\quad+H\sum_{s^{\prime}\in\mathcal{S}_{s,a}}P_{\bar{\bm{\theta}}}( s^{\prime}\mid s,a)\] \[\quad\cdot\sum_{s^{\prime\prime}\in\mathcal{S}_{s,a}}P_{\bar{\bm{ \theta}}}(s^{\prime\prime}\mid s,a)\left|(\widetilde{\bm{\theta}}_{h}^{k}-\bm {\theta}_{h}^{*})^{\top}\left(\bm{\varphi}_{s,a,s^{\prime}}\bm{\varphi}_{s,a, s^{\prime\prime}}^{\top}+\bm{\varphi}_{s,a,s^{\prime\prime}}\bm{\varphi}_{s,a,s^{ \prime}}^{\top}\right)(\widetilde{\bm{\theta}}_{h}^{k}-\bm{\theta}_{h}^{*})\right|\] \[\quad+H\sum_{s^{\prime}\in\mathcal{S}_{s,a}}P_{\bar{\bm{\theta}}}( s^{\prime}\mid s,a)\sum_{s^{\prime\prime}\in\mathcal{S}_{s,a}}P_{\bar{\bm{ \theta}}}(s^{\prime\prime}\mid s,a)\left((\widetilde{\bm{\theta}}_{h}^{k}-\bm {\theta}_{h}^{*})^{\top}\bm{\varphi}_{s,a,s^{\prime\prime}}\right)^{2}\] \[\quad+2H\left((\widetilde{\bm{\theta}}_{h}^{k}-\bm{\theta}_{h}^{* })^{\top}\bigg{(}\sum_{s^{\prime\prime}\in\mathcal{S}_{s,a}}P_{\bar{\bm{ \theta}}}(s^{\prime\prime}\mid s,a)\bm{\varphi}_{s,a,s^{\prime\prime}}\bigg{)} \right)^{2}\] \[\leq H\sum_{s^{\prime}\in\mathcal{S}_{s,a}}P_{\bar{\bm{\theta}}}( s^{\prime}\mid s,a)\|\bm{\varphi}_{s,a,s^{\prime}}\|_{\mathbf{B}_{k,h}^{-1}}^{2}\left\| \widetilde{\bm{\theta}}_{h}^{k}-\bm{\theta}_{h}^{*}\right\|_{\mathbf{B}_{k,h}} ^{2}\] \[\quad+H\sum_{s^{\prime}\in\mathcal{S}_{s,a}}P_{\bar{\bm{\theta}}}( s^{\prime}\mid s,a)\] \[\quad\cdot\sum_{s^{\prime\prime}\in\mathcal{S}_{s,a}}P_{\bar{\bm{ \theta}}}(s^{\prime\prime}\mid s,a)\left|(\widetilde{\bm{\theta}}_{h}^{k}-\bm {\theta}_{h}^{*})^{\top}\left(\bm{\varphi}_{s,a,s^{\prime}}\bm{\varphi}_{s,a, s^{\prime}}^{\top}+\bm{\varphi}_{s,a,s^{\prime\prime}}\bm{\varphi}_{s,a,s^{ \prime\prime}}^{\top}\right)(\widetilde{\bm{\theta}}_{h}^{k}-\bm{\theta}_{h}^{ *})\right|\] \[\quad+H\sum_{s^{\prime\prime}\in\mathcal{S}_{s,a}}P_{\bar{\bm{ \theta}}}(s^{\prime\prime}\mid s,a)\|\bm{\varphi}_{s,a,s^{\prime\prime}}\|_{ \mathbf{B}_{k,h}^{-1}}^{2}\left\|\widetilde{\bm{\theta}}_{h}^{k}-\bm{\theta}_{h }^{*}\right\|_{\mathbf{B}_{k,h}}\] \[\quad+2H\bigg{(}\sum_{s^{\prime\prime}\in\mathcal{S}_{s,a}}P_{ \bar{\bm{\theta}}}(s^{\prime\prime}\mid s,a)\|\bm{\varphi}_{s,a,s^{\prime\prime }}\|_{\mathbf{B}_{k,h}^{-1}}\left\|\widetilde{\bm{\theta}}_{h}^{k}-\bm{\theta}_{ h}^{*}\right\|_{\mathbf{B}_{k,h}}\bigg{)}^{2}\,,\] (79)

where for the second inequality we use Cauchy-Schwarz inequality, \(\mathbf{xx}^{\top}+\mathbf{yy}^{\top}\succeq\mathbf{xy}^{\top}+\mathbf{yx}^{\top}\) for any \(\mathbf{x},\mathbf{y}\in\mathbb{R}^{d}\), and triangle inequality. Note that

\[H\sum_{s^{\prime}\in\mathcal{S}_{s,a}}P_{\bar{\bm{\theta}}}(s^{ \prime}\mid s,a)\] \[\quad\cdot\sum_{s^{\prime\prime}\in\mathcal{S}_{s,a}}P_{\bar{\bm{ \theta}}}(s^{\prime\prime}\mid s,a)\left|(\widetilde{\bm{\theta}}_{h}^{k}-\bm{ \theta}_{h}^{*})^{\top}\left(\bm{\varphi}_{s,a,s^{\prime}}\bm{\varphi}_{s,a,s^ {\prime}}^{\top}+\bm{\varphi}_{s,a,s^{\prime\prime}}\bm{\varphi}_{s,a,s^{ \prime\prime}}^{\top}\right)(\widetilde{\bm{\theta}}_{h}^{k}-\bm{\theta}_{h}^{*})\right|\] \[=H\sum_{s^{\prime}\in\mathcal{S}_{s,a}}P_{\bar{\bm{\theta}}}(s^{ \prime}\mid s,a)\left((\widetilde{\bm{\theta}}_{h}^{k}-\bm{\theta}_{h}^{*})^{ \top}\bm{\varphi}_{s,a,s^{\prime}}\right)^{2}\] \[\quad+H\sum_{s^{\prime\prime}\in\mathcal{S}_{s,a}}P_{\bar{\bm{ \theta}}}(s^{\prime\prime}\mid s,a)\left((\widetilde{\bm{\theta}}_{h}^{k}-\bm{ \theta}_{h}^{*})^{\top}\bm{\varphi}_{s,a,s^{\prime\prime}}\right)^{2}\] \[\leq 2H\sum_{s^{\prime}\in\mathcal{S}_{s,a}}P_{\bar{\bm{\theta}}}(s^{ \prime}\mid s,a)\|\bm{\varphi}_{s,a,s^{\prime}}\|_{\mathbf{B}_{k,h}^{-1}}^{2} \left\|\widetilde{\bm{\theta}}_{h}^{k}-\bm{\theta}_{h}^{*}\right\|_{\mathbf{B}_ {k,h}}^{2}\,.\] (80)By substituting Eq. (80) into Eq. (79) we have

\[\left|(\widetilde{\bm{\theta}}_{h}^{k}-\bm{\theta}_{h}^{*})^{\top} \nabla^{2}F(\bar{\bm{\theta}})(\widetilde{\bm{\theta}}_{h}^{k}-\bm{\theta}_{h}^{* })\right|\] \[\leq 4H\sum_{s^{\prime}\in\mathcal{S}_{s,a}}P_{\tilde{\bm{\theta}}}(s^ {\prime}\mid s,a)\|\bm{\varphi}_{s,a,s^{\prime}}\|_{\mathbf{B}_{h,h}^{-1}}^{2} \left\|\widetilde{\bm{\theta}}_{h}^{k}-\bm{\theta}_{h}^{*}\right\|_{\mathbf{B }_{h,h}}^{2}\] \[\quad+2H\bigg{(}\sum_{s^{\prime\prime}\in\mathcal{S}_{s,a}}P_{ \tilde{\bm{\theta}}}(s^{\prime\prime}\mid s,a)\|\bm{\varphi}_{s,a,s^{\prime \prime}}\|_{\mathbf{B}_{h,h}^{-1}}\left\|\widetilde{\bm{\theta}}_{h}^{k}-\bm{ \theta}_{h}^{*}\right\|_{\mathbf{B}_{h,h}}\bigg{)}^{2}\] \[\leq 4H\beta_{k}^{2}\max_{s^{\prime}\in\mathcal{S}_{s,a}}\|\bm{ \varphi}_{s,a,s^{\prime}}\|_{\mathbf{B}_{h,h}^{-1}}^{2}+2H\left(\beta_{k}\max_ {s^{\prime}\in\mathcal{S}_{s,a}}\|\bm{\varphi}_{s,a,s^{\prime}}\|_{\mathbf{ B}_{h,h}^{-1}}\right)^{2}\] \[\leq 6H\beta_{k}^{2}\max_{s^{\prime}\in\mathcal{S}_{s,a}}\|\bm{ \varphi}_{s,a,s^{\prime}}\|_{\mathbf{B}_{h,h}^{-1}}^{2}\,,\] (81)

where for the second inequality follows by Lemma 12 and \(\sum_{s^{\prime}\in\mathcal{S}_{s,a}}P_{\tilde{\bm{\theta}}}(s^{\prime}\mid s,a)=1\). Combining the results of Eq. (78) and Eq. (81) and, we conclude the proof. 

### Good Events with High Probability

In this section, we introduce the good events used to prove Theorem 2 and show that the good events happen with high probability.

**Lemma 17** (Good event probability).: _For any \(K\in\mathbb{N}\) and \(\delta\in(0,1)\), the good event \(\mathfrak{G}(K,\delta^{\prime})\) holds with probability at least \(1-\delta\) where \(\delta^{\prime}=\delta/(2KH)\)._

Proof of Lemma 17.: For any \(\delta^{\prime}\in(0,1)\), we have

\[\mathfrak{G}(K,\delta^{\prime})=\bigcap_{k\leq K}\bigcap_{h\leq H}\mathfrak{G }_{k,h}(\delta^{\prime})=\bigcap_{k\leq K}\bigcap_{h\leq H}\left\{\mathfrak{G }_{k,h}^{\Delta}(\delta^{\prime})\cap\mathfrak{G}_{k,h}^{\bm{\xi}}(\delta^{ \prime})\right\}\,.\]

On the other hand, for any \((k,h)\in[K]\times[H]\), by Lemma 30\(\mathfrak{G}_{k,h}^{\bm{\xi}}(\delta^{\prime})\) holds with probability at least \(1-\delta^{\prime}\). Then, for \(\delta^{\prime}=\delta/(2KH)\) by taking union bound, we have the desired result as follows:

\[\mathbb{P}(\mathfrak{G}(K,\delta^{\prime}))\geq(1-\delta^{\prime})^{2KH}\geq 1 -2KH\delta^{\prime}=1-\delta\,.\]

### Stochastic Optimism

**Lemma 18** (Stochastic optimism).: _For any \(\delta\) with \(0<\delta<\Phi(-1)/2\), let \(\sigma_{k}=H\beta_{k}(\delta)\). If we take multiple sample size \(M=\lceil 1-\frac{\log(H\mathcal{U})}{\log\Phi(1)}\rceil\), then for any \(k\in[K]\), we have_

\[\mathbb{P}\left((\widetilde{V}_{1}^{k}-V_{1}^{*})(s_{1}^{k})\geq 0\mid s_{1}^{k },\mathcal{F}_{k}\right)\geq\Phi(-1)/2\,.\]

Proof of Lemma 18.: First, we introduce the following lemmas.

**Lemma 19**.: _Let \(\delta\in(0,1)\) be given. For any \((k,h)\in[K]\times[H]\), let \(\sigma_{k}=H\beta_{k}(\delta)\). If we define the event \(\mathfrak{G}_{k,h}^{\Delta}(\delta)\) as_

\[\mathfrak{G}_{k,h}^{\Delta}(\delta) :=\left\{\left|\Delta_{h}^{k}(s,a)\right|\leq H\beta_{k}(\delta) \sum_{s^{\prime}\in\mathcal{S}_{s,a}}P_{\tilde{\bm{\theta}}_{h}^{k}}(s^{ \prime}\mid s,a)\left\|\hat{\bm{\varphi}}_{s,a,s^{\prime}}(\widetilde{\bm{ \theta}}_{h}^{k})\right\|_{\mathbf{B}_{h,h}^{-1}}\] \[\quad+3H\beta_{k}(\delta)^{2}\max_{s^{\prime}\in\mathcal{S}_{s,a}} \|\bm{\varphi}_{s,a,s^{\prime}}\|_{\mathbf{B}_{h,h}^{-1}}^{2}\right\},\]

_then conditioned on \(\mathfrak{G}_{k,h}^{\Delta}(\delta)\), for any \((s,a)\in\mathcal{S}\times\mathcal{A}\), we have_

\[\mathbb{P}\left(-t_{h}^{k}(s,a)\geq 0\mid\mathfrak{G}_{k,h}^{\Delta}(\delta) \right)\geq 1-\Phi(1)^{M}\,.\]

**Lemma 20**.: _Let \(\delta\in(0,1)\) be given. For any \((h,k)\in[H]\times[K]\), let \(\sigma_{k}=H\beta_{k}(\delta)\). If we take multiple sample size \(M=\lceil 1-\frac{\log(H\ell)}{\log\Phi(1)}\rceil\), then conditioned on the event \(\mathfrak{G}_{k}^{\Delta}(\delta):=\cap_{h\in[H]}\mathfrak{G}_{k,h}^{\Delta}(\delta)\), we have_

\[\mathbb{P}\left(-t_{h}^{k}(s_{h},a_{h})\geq 0,\forall h\in[H]\mid\mathfrak{G}_{k}^ {\Delta}(\delta)\right)\geq\Phi(-1)\,.\]

Based on the result of Lemma 20, using the same argument as in Lemma 6 we obtain the desired result. 

In the following section, we provide the proofs of the lemmas used in Lemma 18.

#### d.4.1 Proof of Lemma 19

Proof of Lemma 19.: Recall the definition of Bellman error (Definition 1), we have

\[-t_{h}^{k}(s,a)\] \[=\widetilde{Q}_{h}^{k}(s,a)-\Big{(}r(s,a)+P_{h}\widetilde{V}_{h+1 }^{k}(s,a)\Big{)}\] \[=\min\left\{r(s,a)+\sum_{s^{\prime}\in\mathcal{S}_{s,a}}P_{ \widetilde{\boldsymbol{\theta}}_{h}^{k}}(s^{\prime}\mid s,a)\widetilde{V}_{h+ 1}^{k}(s^{\prime})+\nu_{k,h}^{\mathrm{rand}}(s,a)\right\}-\Big{(}r(s,a)+P_{h} \widetilde{V}_{h+1}^{k}(s,a)\Big{)}\] \[\geq\min\left\{\sum_{s^{\prime}\in\mathcal{S}_{s,a}}P_{ \widetilde{\boldsymbol{\theta}}_{h}^{k}}(s^{\prime}\mid s,a)\widetilde{V}_{h+ 1}^{k}(s^{\prime})-P_{h}\widetilde{V}_{h+1}^{k}(s,a)+\nu_{k,h}^{\mathrm{rand} }(s,a),0\right\}.\]

Then, it is enough to show that

\[\sum_{s^{\prime}\in\mathcal{S}_{s,a}}P_{\widetilde{\boldsymbol{\theta}}_{h}^{ k}}(s^{\prime}\mid s,a)\widetilde{V}_{h+1}^{k}(s^{\prime})-P_{h}\widetilde{V}_{h+ 1}^{k}(s,a)+\nu_{k,h}^{\mathrm{rand}}(s,a)\geq 0\]

at least with constant probability. On the other hand, under the event \(\mathfrak{G}_{k,h}^{\Delta}(\delta)\), by Lemma 16 we have

\[\sum_{s^{\prime}\in\mathcal{S}_{s,a}}P_{\widetilde{\boldsymbol{ \theta}}_{h}^{k}}(s^{\prime}\mid s,a)\widetilde{V}_{h+1}^{k}(s^{\prime})-P_{h} \widetilde{V}_{h+1}^{k}(s,a)+\nu_{k,h}^{\mathrm{rand}}(s,a)\] \[=\Delta_{h}^{k}(s,a)+\nu_{k,h}^{\mathrm{rand}}(s,a)\] \[\geq-H\beta_{k}(\delta)\sum_{s^{\prime}\in\mathcal{S}_{s,a}}P_{ \widetilde{\boldsymbol{\theta}}_{h}^{k}}(s^{\prime}\mid s,a)\left\|\bar{ \boldsymbol{\varphi}}_{s,a,s^{\prime}}(\widetilde{\boldsymbol{\theta}}_{h}^{k })\right\|_{\mathbf{B}_{k,h}^{-1}}\] \[\quad-3H\beta_{k}(\delta)^{2}\max_{s^{\prime}\in\mathcal{S}_{s,a }}\left\|\boldsymbol{\varphi}_{s,a,s^{\prime}}\right\|_{\mathbf{B}_{k,h}^{-1} }^{2}+\nu_{k,h}^{\mathrm{rand}}(s,a)\] \[=\sum_{s^{\prime}\in\mathcal{S}_{s,a}}P_{\widetilde{\boldsymbol{ \theta}}_{h}^{k}}(s^{\prime}\mid s,a)\bar{\boldsymbol{\varphi}}_{s,a,s^{ \prime}}(\widetilde{\boldsymbol{\theta}}_{h}^{k})^{\top}\boldsymbol{\xi}_{k,h }^{s^{\prime}}-H\beta_{k}(\delta)\sum_{s^{\prime}\in\mathcal{S}_{s,a}}P_{ \widetilde{\boldsymbol{\theta}}_{h}^{k}}(s^{\prime}\mid s,a)\left\|\bar{ \boldsymbol{\varphi}}_{s,a,s^{\prime}}(\widetilde{\boldsymbol{\theta}}_{h}^{k })\right\|_{\mathbf{B}_{k,h}^{-1}}\,.\]

Note that since \(\boldsymbol{\xi}_{k,h}^{(m)}\sim\mathcal{N}(\boldsymbol{0},\sigma_{k}^{2} \mathbf{B}_{k,h}^{-1})\), it follows that

\[\bar{\boldsymbol{\varphi}}_{s,a,s^{\prime}}(\widetilde{\boldsymbol{\theta}}_{h }^{k})^{\top}\boldsymbol{\xi}_{k,h}^{(m)}\sim\mathcal{N}\left(0,\sigma_{k}^{2} \left\|\bar{\boldsymbol{\varphi}}_{s,a,s^{\prime}}(\widetilde{\boldsymbol{ \theta}}_{h}^{k})\right\|_{\mathbf{B}_{k,h}^{-1}}^{2}\right)\,,\quad\gamma m \in[M]\,.\]

Therefore, by setting \(\sigma_{k}=H\beta_{k}(\delta)\), we have for \(m\in[M]\) and \(s^{\prime}\in\mathcal{S}_{s,a}\),

\[\mathbb{P}\left(\bar{\boldsymbol{\varphi}}_{s,a,s^{\prime}}(\widetilde{ \boldsymbol{\theta}}_{h}^{k})^{\top}\boldsymbol{\xi}_{k,h}^{(m)}\geq H\beta _{k}(\delta)\left\|\bar{\boldsymbol{\varphi}}_{s,a,s^{\prime}}(\widetilde{ \boldsymbol{\theta}}_{h}^{k})\right\|_{\mathbf{B}_{k,h}^{-1}}\right)=\Phi(-1)\,.\]Recall that \(\bm{\xi}_{k,h}^{s^{\prime}}:=\bm{\xi}_{k,h}^{m(s^{\prime})}\) where \(m(s^{\prime}):=\operatorname*{argmax}_{m\in[M]}\bar{\bm{\varphi}}_{s,a,s^{\prime}} (\widetilde{\bm{\theta}}_{h}^{k})^{\top}\bm{\xi}_{k,h}^{(m)}\). Then, we can deduce

\[\mathbb{P}\left(\bar{\bm{\varphi}}_{s,a,s^{\prime}}(\widetilde{ \bm{\theta}}_{h}^{k})^{\top}\bm{\xi}_{k,h}^{s^{\prime}}\geq H\beta_{k}(\delta) \left\|\bar{\bm{\varphi}}_{s,a,s^{\prime}}(\widetilde{\bm{\theta}}_{h}^{k}) \right\|_{\mathbf{B}_{k,h}^{-1}}\right)\] \[=\mathbb{P}\left(\max_{m\in[M]}\bar{\bm{\varphi}}_{s,a,s^{\prime} }(\widetilde{\bm{\theta}}_{h}^{k})^{\top}\bm{\xi}_{k,h}^{(m)}\geq H\beta_{k}( \delta)\left\|\bar{\bm{\varphi}}_{s,a,s^{\prime}}(\widetilde{\bm{\theta}}_{h}^ {k})\right\|_{\mathbf{B}_{k,h}^{-1}}\right)\] \[=1-\mathbb{P}\left(\bar{\bm{\varphi}}_{s,a,s^{\prime}}(\widetilde {\bm{\theta}}_{h}^{k})^{\top}\bm{\xi}_{k,h}^{(m)}<H\beta_{k}(\delta)\left\| \bar{\bm{\varphi}}_{s,a,s^{\prime}}(\widetilde{\bm{\theta}}_{h}^{k})\right\| _{\mathbf{B}_{k,h}^{-1}},\forall m\in[M]\right)\] \[\geq 1-(1-\Phi(-1))^{M}\] \[=1-\Phi(1)^{M}\,.\] (82)

Consequently, we arrive at the conclusion as follows:

\[\mathbb{P}(-t_{h}^{k}(s,a)\geq 0\mid\bm{\Theta}_{k,h}^{\Delta}( \delta))\] \[\geq\mathbb{P}\left(\sum_{s^{\prime}\in\mathcal{S}_{s,a}}P_{ \widetilde{\bm{\theta}}_{h}^{k}}(s^{\prime}\mid s,a)\widetilde{V}_{h+1}^{k}(s^ {\prime})-P_{h}\widetilde{V}_{h+1}^{k}(s,a)+\nu_{k,h}^{\operatorname{rand}}(s,a )\geq 0\mid\bm{\Theta}_{k,h}^{\Delta}(\delta)\right)\] \[\geq\mathbb{P}\left(\sum_{s^{\prime}\in\mathcal{S}_{s,a}}P_{ \widetilde{\bm{\theta}}_{h}^{k}}(s^{\prime}\mid s,a)\bar{\bm{\varphi}}_{s,a,s^ {\prime}}(\widetilde{\bm{\theta}}_{h}^{k})^{\top}\bm{\xi}_{k,h}^{s^{\prime}}\right.\] (83) \[\qquad\qquad\geq H\beta_{k}(\delta)\sum_{s^{\prime}\in\mathcal{S} _{s,a}}P_{\widetilde{\bm{\theta}}_{h}^{k}}(s^{\prime}\mid s,a)\left\|\bar{\bm {\varphi}}_{s,a,s^{\prime}}(\widetilde{\bm{\theta}}_{h}^{k})\right\|_{\mathbf{ B}_{k,h}^{-1}}\mid\bm{\Theta}_{k,h}^{\Delta}(\delta)\right)\] \[\geq\mathbb{P}\left(\bar{\bm{\varphi}}_{s,a,s^{\prime}}( \widetilde{\bm{\theta}}_{h}^{k})^{\top}\bm{\xi}_{k,h}^{s^{\prime}}\geq H\beta _{k}(\delta)\left\|\bar{\bm{\varphi}}_{s,a,s^{\prime}}(\widetilde{\bm{\theta}}_ {h}^{k})\right\|_{\mathbf{B}_{k,h}^{-1}}\mid\bm{\Theta}_{k,h}^{\Delta}( \delta)\right)\] \[=1-\mathbb{P}\left(\exists s^{\prime}\in\mathcal{S}_{s,a}\text{ s.t. }\bar{\bm{\varphi}}_{s,a,s^{\prime}}(\widetilde{\bm{\theta}}_{h}^{k})^{\top}\bm{ \xi}_{k,h}^{s^{\prime}}<H\beta_{k}(\delta)\left\|\bar{\bm{\varphi}}_{s,a,s^{ \prime}}(\widetilde{\bm{\theta}}_{h}^{k})\right\|_{\mathbf{B}_{k,h}^{-1}} \mid\bm{\Theta}_{k,h}^{\Delta}(\delta)\right)\] \[\geq 1-\mathcal{U}\mathbb{P}\left(\bar{\bm{\varphi}}_{s,a,s^{ \prime}}(\widetilde{\bm{\theta}}_{h}^{k})^{\top}\bm{\xi}_{k,h}^{s^{\prime}}<H \beta_{k}(\delta)\left\|\bar{\bm{\varphi}}_{s,a,s^{\prime}}(\widetilde{\bm{ \theta}}_{h}^{k})\right\|_{\mathbf{B}_{k,h}^{-1}}\mid\bm{\Theta}_{k,h}^{ \Delta}(\delta)\right)\] (84) \[\geq 1-\mathcal{U}\Phi(1)^{M}\,,\] (85)

where (84) comes from the fact that \(\max_{s,a}\left|\mathcal{S}_{s,a}\right|=\mathcal{U}\) and the union bound, and (85) follows by (82). 

#### d.4.2 Proof of Lemma 20

Proof of Lemma 20.: It holds

\[\mathbb{P}\left(-t_{h}^{k}(s_{h},a_{h})\geq 0,\forall h\in[H]\right) =1-\mathbb{P}\left(\exists h\in[H]\text{ s.t. }-t_{h}^{k}(s_{h},a_{h})<0\right)\] \[\geq 1-H\mathbb{P}\left(-t_{h}^{k}(s_{h},a_{h})<0\right)\] \[\geq 1-H\mathcal{U}\Phi(1)^{M}\] \[\geq\Phi(-1)\]

where the first inequality uses the Bernoulli's inequality, the second inequality follows by Lemma 19, and the last inequality holds due to the choice of \(M=\lceil 1-\frac{\log(\mathcal{U}H)}{\log\Phi(1)}\rceil\). 

### Bound on Estimation Part

In this section, we provide the upper bound on the estimation part of the regret: \(\sum_{k=1}^{K}(\widetilde{V}_{1}^{k}-V_{1}^{*})(s_{1}^{k})\).

**Lemma 21** (Bound on estimation).: _For any \(\delta\in(0,1)\), if \(\lambda=\mathcal{O}(L_{\bm{\varphi}}^{2}d\log\mathcal{U})\), then with probability at least \(1-\delta/2\), we have_

\[\sum_{k=1}^{K}(\widetilde{V}_{1}^{k}-V_{1}^{\pi^{k}})(s_{1}^{k})=\widetilde{ \mathcal{O}}\left(d^{3/2}H^{3/2}\sqrt{T}+\kappa^{-1}d^{2}H^{2}\right)\,.\]Proof of Lemma 21.: With the same argument in Lemma 10, we have

\[(\widetilde{V}_{1}^{k}-V_{1}^{\pi^{k}})(s_{1}^{k})=\sum_{h=1}^{H}-\iota_{h}^{k}(s _{h}^{k},a_{h}^{k})+\sum_{h=1}^{H}\dot{\zeta}_{h}^{k}\,,\] (86)

where \(\dot{\zeta}_{h}^{k}:=P_{h}(\widetilde{V}_{h+1}^{k}-V_{h+1}^{\pi^{k}})(s_{h}^{k },a_{h}^{k})-(\widetilde{V}_{h+1}^{k}-V_{h+1}^{\pi^{k}})(s_{h+1}^{k})\). Note that

\[-\iota_{h}^{k}(s_{h}^{k},a_{h}^{k}) =\widetilde{Q}_{h}^{k}(s_{h}^{k},a_{h}^{k})-\left(r(s_{h}^{k},a_{h }^{k})+P_{h}\widetilde{V}_{h+1}^{k}(s_{h}^{k},a_{h}^{k})\right)\] \[\leq\sum_{s^{\prime}\in\mathcal{S}_{k,h}}P_{\widetilde{\bm{ \theta}}_{h}^{k}}(s^{\prime}\mid s_{h}^{k},a_{h}^{k})\widetilde{V}_{h+1}^{k}( s^{\prime})-P_{h}\widetilde{V}_{h+1}^{k}(s_{h}^{k},a_{h}^{k})+\nu_{k,h}^{\rm rand }(s_{h}^{k},a_{h}^{k})\] \[\leq\left|\Delta_{h}^{k}(s_{h}^{k},a_{h}^{k})\right|+\nu_{k,h}^{ \rm rand}(s_{h}^{k},a_{h}^{k})\] \[\leq H\beta_{k}\sum_{s^{\prime}\in\mathcal{S}_{k,h}}P_{\widetilde{ \bm{\theta}}_{h}^{k}}(s^{\prime}\mid s_{h}^{k},a_{h}^{k})\left\|\bar{\bm{ \varphi}}_{k,h,s^{\prime}}(\widetilde{\bm{\theta}}_{h}^{k})\right\|_{\mathbf{ B}_{k,h}^{-1}}+3H\beta_{k}^{2}\max_{s^{\prime}\in\mathcal{S}_{k,h}}\left\|\bm{ \varphi}_{k,h,s^{\prime}}\right\|_{\mathbf{B}_{k,h}^{-1}}^{2}\] \[\quad+\nu_{k,h}^{\rm rand}(s_{h}^{k},a_{h}^{k})\,,\] (87)

where the last inequality follows by Lemma 16. Now we introduce the following lemma.

**Lemma 22**.: _For any \((k,h)\in[K]\times[H]\) and \((s,a)\in\mathcal{S}\times\mathcal{A}\), it holds_

\[\sum_{s^{\prime}\in\mathcal{S}_{s,a}}P_{\widetilde{\bm{\theta}}_ {h}^{k}}(s^{\prime}\mid s,a)\left\|\bar{\bm{\varphi}}_{s,a,s^{\prime}}( \widetilde{\bm{\theta}}_{h}^{k})\right\|_{\mathbf{B}_{k,h}^{-1}}\] \[\leq\sum_{s^{\prime}\in\mathcal{S}_{s,a}}P_{\widetilde{\bm{ \theta}}_{h}^{k+1}}(s^{\prime}\mid s,a)\left\|\bar{\bm{\varphi}}_{s,a,s^{ \prime}}(\widetilde{\bm{\theta}}_{h}^{k+1})\right\|_{\mathbf{B}_{k,h}^{-1}}+ \frac{16\eta L_{\bm{\varphi}}}{\sqrt{\lambda}}\max_{s^{\prime}\in\mathcal{S}_ {s,a}}\left\|\bar{\bm{\varphi}}_{s,a,s^{\prime}}(\widetilde{\bm{\theta}}_{h}^{k +1})\right\|_{\mathbf{B}_{k,h}^{-1}}^{2}\,.\]

By plugging the result of Lemma 22 into Eq. (87), we have

\[-\iota_{h}^{k}(s_{h}^{k},a_{h}^{k})\] \[\leq H\beta_{k}\sum_{s^{\prime}\in\mathcal{S}_{k,h}}P_{\widetilde{ \bm{\theta}}_{h}^{k}+1}(s^{\prime}\mid s_{h}^{k},a_{h}^{k})\left\|\bar{\bm{ \varphi}}_{k,h,s^{\prime}}(\widetilde{\bm{\theta}}_{h}^{k+1})\right\|_{\mathbf{ B}_{k,h}^{-1}}\] \[\quad+H\beta_{k}\frac{16\eta L_{\bm{\varphi}}}{\sqrt{\lambda}}\max _{s^{\prime}\in\mathcal{S}_{k,h}}\left\|\bar{\bm{\varphi}}_{k,h,s^{\prime}}( \widetilde{\bm{\theta}}_{h}^{k+1})\right\|_{\mathbf{B}_{k,h}^{-1}}^{2}+3H \beta_{k}^{2}\max_{s^{\prime}\in\mathcal{S}_{k,h}}\left\|\bm{\varphi}_{k,h,s^{ \prime}}\right\|_{\mathbf{B}_{k,h}^{-1}}^{2}+\nu_{k,h}^{\rm rand}(s_{h}^{k},a_ {h}^{k})\] \[\leq H\beta_{k}\sum_{s^{\prime}\in\mathcal{S}_{k,h}}P_{\widetilde{ \bm{\theta}}_{h}^{k}+1}(s^{\prime}\mid s_{h}^{k},a_{h}^{k})\left\|\bar{\bm{ \varphi}}_{k,h,s^{\prime}}(\widetilde{\bm{\theta}}_{h}^{k+1})\right\|_{ \mathbf{B}_{k,h}^{-1}}\] \[\quad+\sum_{s^{\prime}\in\mathcal{S}_{k,h}}P_{\widetilde{\bm{ \theta}}_{h}^{k}}(s^{\prime}\mid s_{h}^{k},a_{h}^{k})\bar{\bm{\varphi}}_{k,h,s ^{\prime}}(\widetilde{\bm{\theta}}_{h}^{k})^{\top}\bm{\xi}_{k,h}^{s^{\prime}}\] \[\quad+H\beta_{k}\frac{16\eta L_{\bm{\varphi}}}{\sqrt{\lambda}}\max _{s^{\prime}\in\mathcal{S}_{k,h}}\left\|\bar{\bm{\varphi}}_{k,h,s^{\prime}}( \widetilde{\bm{\theta}}_{h}^{k+1})\right\|_{\mathbf{B}_{k,h}^{-1}}^{2}+6H \beta_{k}^{2}\max_{s^{\prime}\in\mathcal{S}_{k,h}}\left\|\bm{\varphi}_{k,h,s^{ \prime}}\right\|_{\mathbf{B}_{k,h}^{-1}}^{2}.\]

By letting us denote

\[\Upsilon_{h}^{k}(s,a):=H\beta_{k}\frac{16\eta L_{\bm{\varphi}}}{\sqrt{\lambda}} \max_{s^{\prime}\in\mathcal{S}_{s,a}}\left\|\bar{\bm{\varphi}}_{s,a,s^{\prime}}( \widetilde{\bm{\theta}}_{h}^{k+1})\right\|_{\mathbf{B}_{k,h}^{-1}}^{2}+6H\beta_{k }^{2}\max_{s^{\prime}\in\mathcal{S}_{s,a}}\left\|\bm{\varphi}_{s,a,s^{\prime}} \right\|_{\mathbf{B}_{k,h}^{-1}}^{2},\] (88)

[MISSING_PAGE_EMPTY:59]

**Lemma 24**.: _Let \(\delta\in(0,1)\) be given. For any \((k,h)\in[K]\times[H]\) and \((s,a)\in\mathcal{S}\times\mathcal{A}\), with probability at least \(1-\delta\), it holds_

\[\sum_{s^{\prime}\in\mathcal{S}_{s,a}}P_{\tilde{\bm{\theta}}_{h}^{k} }(s^{\prime}\mid s,a)\bar{\bm{\varphi}}_{s,a,s^{\prime}}(\widetilde{\bm{ \theta}}_{h}^{k})^{\top}\bm{\xi}_{k,h}^{s^{\prime}}\] \[\leq\gamma_{k}(\delta)\bigg{(}\sum_{s^{\prime}\in\mathcal{S}_{k,h }}P_{\tilde{\bm{\theta}}_{h}^{k+1}}(s^{\prime}\mid s_{h}^{k},a_{h}^{k})\left\| \bar{\bm{\varphi}}_{k,h,s^{\prime}}(\widetilde{\bm{\theta}}_{h}^{k+1})\right\| _{\mathbf{B}_{k,h}^{-1}}\] \[\qquad\qquad\qquad+\frac{16\eta L_{\bm{\varphi}}}{\sqrt{\lambda }}\max_{s^{\prime}\in\mathcal{S}_{k,h}}\left\|\bar{\bm{\varphi}}_{k,h,s^{ \prime}}(\widetilde{\bm{\theta}}_{h}^{k+1})\right\|_{\mathbf{B}_{k,h}^{-1}}^ {2}\bigg{)}\,,\]

_where \(\gamma_{k}(\delta):=C_{\bm{\xi}}\sigma_{k}\sqrt{d\log(Md/\delta)}\) for an absolute constant \(C_{\bm{\xi}}>0\)._

By Lemma 24, we have

\[\sum_{k,h,s^{\prime}}P_{\tilde{\bm{\theta}}_{h}^{k}}(s^{\prime} \mid s_{h}^{k},a_{h}^{k})\bar{\bm{\varphi}}_{k,h,s^{\prime}}(\widetilde{\bm{ \theta}}_{h}^{k})^{\top}\bm{\xi}_{k,h}^{s^{\prime}}\] \[\leq\gamma_{K}(\delta)\bigg{(}\sum_{k,h,s^{\prime}}P_{\tilde{\bm {\theta}}_{h}^{k+1}}(s^{\prime}\mid s_{h}^{k},a_{h}^{k})\left\|\bar{\bm{ \varphi}}_{k,h,s^{\prime}}(\widetilde{\bm{\theta}}_{h}^{k+1})\right\|_{ \mathbf{B}_{k,h}^{-1}}\] \[\qquad\qquad\qquad+\frac{16\eta L_{\bm{\varphi}}}{\sqrt{\lambda }}\sum_{k=1}^{K}\sum_{h=1}^{H}\max_{s^{\prime}\in\mathcal{S}_{k,h}}\left\| \bar{\bm{\varphi}}_{k,h,s^{\prime}}(\widetilde{\bm{\theta}}_{h}^{k+1})\right\| _{\mathbf{B}_{k,h}^{-1}}^{2}\bigg{)}\] \[\leq\gamma_{K}(\delta)\bigg{(}\sqrt{T}\sqrt{2Hd\log\left(1+\frac{ K\mathcal{U}L_{\bm{\varphi}}^{2}}{d\lambda}\right)}+\frac{16\eta L_{\bm{ \varphi}}}{\sqrt{\lambda}}\sum_{k=1}^{K}\sum_{h=1}^{H}\max_{s^{\prime}\in \mathcal{S}_{k,h}}\left\|\bar{\bm{\varphi}}_{k,h,s^{\prime}}(\widetilde{\bm{ \theta}}_{h}^{k+1})\right\|_{\mathbf{B}_{k,h}^{-1}}^{2}\bigg{)}\,,\] (92)

where the last inequality follows by Eq. (90). Note that

\[\sum_{k=1}^{K}\sum_{h=1}^{H}\max_{s^{\prime}\in\mathcal{S}_{k,h}} \left\|\bar{\bm{\varphi}}_{k,h,s^{\prime}}(\widetilde{\bm{\theta}}_{h}^{k+1}) \right\|_{\mathbf{B}_{h,h}^{-1}}^{2}\] \[\leq\sum_{k=1}^{K}\sum_{h=1}^{H}\max_{s^{\prime}\in\mathcal{S}_{ k,h}}\left\|\bar{\bm{\varphi}}_{k,h,s^{\prime}}(\widetilde{\bm{\theta}}_{h}^{k+1}) \right\|_{\mathbf{A}_{h,h}^{-1}}^{2}\] \[=\sum_{k=1}^{K}\sum_{h=1}^{H}\max_{s^{\prime}\in\mathcal{S}_{k,h} }\left\|\bm{\varphi}_{k,h,s^{\prime}}-\sum_{\widetilde{s}\in\mathcal{S}_{k,h}}P _{\widetilde{\bm{\theta}}_{h}^{k+1}}(\widetilde{s}\mid s_{h}^{k},a_{h}^{k}) \bm{\varphi}_{k,h,\widetilde{s}}\right\|_{\mathbf{A}_{h,h}^{-1}}^{2}\] \[\leq\sum_{k=1}^{K}\sum_{h=1}^{H}\max_{s^{\prime}\in\mathcal{S}_{ k,h}}\left(2\|\bm{\varphi}_{k,h,s^{\prime}}\|_{\mathbf{A}_{h,h}^{-1}}^{2}+2\left\| \sum_{\widetilde{s}\in\mathcal{S}_{k,h}}P_{\widetilde{\bm{\theta}}_{h}^{k+1}}( \widetilde{s}\mid s_{h}^{k},a_{h}^{k})\bm{\varphi}_{k,h,\widetilde{s}}\right\| _{\mathbf{A}_{h,h}^{-1}}^{2}\right)\] \[\leq 2\sum_{k=1}^{K}\sum_{h=1}^{H}\max_{s^{\prime}\in\mathcal{S}_{ k,h}}\left\|\bm{\varphi}_{k,h,s^{\prime}}\right\|_{\mathbf{A}_{h,h}^{-1}}^{2}+2\sum_{k=1}^{ K}\sum_{h=1}^{H}\sum_{\widetilde{s}\in\mathcal{S}_{k,h}}P_{\widetilde{\bm{\theta}}_{h}^{k+1}}( \widetilde{s}\mid s_{h}^{k},a_{h}^{k})\|\bm{\varphi}_{k,h,\widetilde{s}}\|_{ \mathbf{A}_{h,h}^{-1}}^{2}\] \[\leq 4\sum_{k=1}^{K}\sum_{h=1}^{H}\max_{s^{\prime}\in\mathcal{S}_{ k,h}}\left\|\bm{\varphi}_{k,h,s^{\prime}}\right\|_{\mathbf{A}_{h,h}^{-1}}^{2}\] \[\leq 16\kappa^{-1}dH\log\left(1+\frac{K\mathcal{U}L_{\bm{\varphi}}^{ 2}}{d\lambda}\right)\,,\] (93)

where the first inequality holds since \(\mathbf{B}_{k,h}^{-1}\preceq\mathbf{A}_{k,h}^{-1}\), the second inequality follows from \((x+y)^{2}\leq 2x^{2}+2y^{2}\), and the third inequality uses the triangle inequality, and the fourth inequality uses \(\sum_{\widetilde{s}\in\mathcal{S}_{k,h}}P_{\widetilde{\bm{\theta}}_{h}^{k+1}}( \widetilde{s}\mid s_{h}^{k},a_{h}^{k})=1\), and the last inequality follows by Lemma 3. By substitutingEq. (93) into Eq. (92), we have

\[\text{(ii)} \leq\gamma_{K}(\delta)\bigg{(}\sqrt{T}\sqrt{2Hd\log\left(1+K\mathcal{ U}L_{\bm{\varphi}}^{2}/(d\lambda)\right)}+\frac{256\eta L_{\bm{\varphi}}}{ \sqrt{\lambda}}\kappa^{-1}dH\log\left(1+K\mathcal{U}L_{\bm{\varphi}}^{2}/(d \lambda)\right)\bigg{)}\] \[=\widetilde{\mathcal{O}}(d^{3/2}H^{3/2}\sqrt{T}+\kappa^{-1}d^{3/ 2}H^{2})\,.\] (94)

For term \(\text{(iii)}\),

\[\sum_{k=1}^{K}\sum_{h=1}^{H}\Upsilon_{h}^{k}(s_{h}^{k},a_{h}^{k})\] \[=\sum_{k=1}^{K}\sum_{h=1}^{H}\bigg{(}H\beta_{k}\frac{16\eta L_{ \bm{\varphi}}}{\sqrt{\lambda}}\max_{s^{\prime}\in\mathcal{S}_{k,h}}\left\| \bm{\bar{\varphi}}_{\bm{k},h,s^{\prime}}(\bm{\widetilde{\theta}}_{h}^{k+1}) \right\|_{\mathbf{B}_{k,h}^{-1}}^{2}+6H\beta_{k}^{2}\max_{s^{\prime}\in \mathcal{S}_{k,h}}\left\|\bm{\varphi}_{k,h,s^{\prime}}\right\|_{\mathbf{B}_{k, h}^{-1}}^{2}\bigg{)}\] \[\leq H\beta_{K}\frac{16\eta L_{\bm{\varphi}}}{\sqrt{\lambda}} \sum_{k=1}^{K}\sum_{h=1}^{H}\max_{s^{\prime}\in\mathcal{S}_{k,h}}\left\|\bm{ \bar{\varphi}}_{\bm{k},h,s^{\prime}}(\bm{\widetilde{\theta}}_{h}^{k+1}) \right\|_{\mathbf{B}_{k,h}^{-1}}^{2}+6H\beta_{K}^{2}\sum_{k=1}^{K}\sum_{h=1}^ {H}\max_{s^{\prime}\in\mathcal{S}_{k,h}}\left\|\bm{\varphi}_{k,h,s^{\prime}} \right\|_{\mathbf{A}_{k,h}^{-1}}^{2}\] \[\leq\beta_{K}\frac{256\eta L_{\bm{\varphi}}}{\sqrt{\lambda}} \kappa^{-1}dH^{2}\log\left(1+K\mathcal{U}L_{\bm{\varphi}}^{2}/(d\lambda) \right)+24\kappa^{-1}dH^{2}\beta_{K}^{2}\log\left(1+K\mathcal{U}L_{\bm{ \varphi}}^{2}/(d\lambda)\right)\] \[=\widetilde{\mathcal{O}}(\kappa^{-1}d^{2}H^{2})\,,\] (95)

where for the second inequality we use the same argument used to derive Eq. (93) and Lemma 3.

For term \(\text{(iv)}\), since we have \(|\dot{\zeta}_{h}^{k}|\leq 2H\) and \(\mathbb{E}[\dot{\zeta}_{h}^{k}\mid\mathcal{F}_{k,h}]=0\), which means \(\{\dot{\zeta}_{h}^{k}\mid\mathcal{F}_{k,h}\}_{k,h}\) is a martingale difference sequence for any \(k\in[K]\) and \(h\in[H]\). Hence, by applying the Azuma-Hoeffding inequality with probability at least \(1-\delta/4\), we have

\[\sum_{k=1}^{K}\sum_{h=1}^{H}\dot{\zeta}_{h}^{k}\leq 2H\sqrt{2KH\log(4/\delta)}\,.\] (96)

Combining all results of Eq. (91), (94), (95), and (96), we have the desired result.

\[\sum_{k=1}^{K}(\widetilde{V}_{1}^{k}-V_{1}^{\pi^{k}})(s_{1}^{k}) =\widetilde{\mathcal{O}}(dH^{3/2}\sqrt{T}+d^{3/2}H^{3/2}\sqrt{T}+ \kappa^{-1}d^{3/2}H^{2}+\kappa^{-1}d^{2}H^{2}+H\sqrt{T})\] \[=\widetilde{\mathcal{O}}(d^{3/2}H^{3/2}\sqrt{T}+\kappa^{-1}d^{2}H ^{2})\,.\]

In the following, we provide the proof of the lemmas used in Lemma 21.

#### d.5.1 Proof of Lemma 22

Proof of Lemma 22.: Note that

\[\sum_{s^{\prime}\in\mathcal{S}_{s,a}}P_{\widetilde{\bm{\theta}}_{h}^ {k}}(s^{\prime}\mid s,a)\left\|\widetilde{\bm{\varphi}}_{s,a,s^{\prime}}( \widetilde{\bm{\theta}}_{h}^{k})\right\|_{\mathbf{B}_{k,h}^{-1}}\] \[\leq\sum_{s^{\prime}\in\mathcal{S}_{s,a}}P_{\widetilde{\bm{ \theta}}_{h}^{k}}(s^{\prime}\mid s,a)\left\|\widetilde{\bm{\varphi}}_{s,a,s^{ \prime}}(\widetilde{\bm{\theta}}_{h}^{k+1})\right\|_{\mathbf{B}_{k,h}^{-1}}\] \[\quad+\sum_{s^{\prime}\in\mathcal{S}_{s,a}}P_{\widetilde{\bm{ \theta}}_{h}^{k}}(s^{\prime}\mid s,a)\left\|\widetilde{\bm{\varphi}}_{s,a,s^{ \prime}}(\widetilde{\bm{\theta}}_{h}^{k})-\widetilde{\bm{\varphi}}_{s,a,s^{ \prime}}(\widetilde{\bm{\theta}}_{h}^{k+1})\right\|_{\mathbf{B}_{k,h}^{-1}}\] \[\leq\sum_{s^{\prime}\in\mathcal{S}_{s,a}}P_{\widetilde{\bm{ \theta}}_{h}^{k+1}}(s^{\prime}\mid s,a)\left\|\widetilde{\bm{\varphi}}_{s,a,s^ {\prime}}(\widetilde{\bm{\theta}}_{h}^{k+1})\right\|_{\mathbf{B}_{k,h}^{-1}}\] \[\quad+\underbrace{\sum_{s^{\prime}\in\mathcal{S}_{s,a}}\left(P_{ \widetilde{\bm{\theta}}_{h}^{k}}(s^{\prime}\mid s,a)-P_{\widetilde{\bm{\theta} }_{h}^{k+1}}(s^{\prime}\mid s,a)\right)\left\|\widetilde{\bm{\varphi}}_{s,a,s ^{\prime}}(\widetilde{\bm{\theta}}_{h}^{k+1})\right\|_{\mathbf{B}_{k,h}^{-1}}}_ {(\mathrm{i})}\] \[\quad+\underbrace{\sum_{s^{\prime}\in\mathcal{S}_{s,a}}P_{ \widetilde{\bm{\theta}}_{h}^{k}}(s^{\prime}\mid s,a)\left\|\widetilde{\bm{ \varphi}}_{s,a,s^{\prime}}(\widetilde{\bm{\theta}}_{h}^{k})-\widetilde{\bm{ \varphi}}_{s,a,s^{\prime}}(\widetilde{\bm{\theta}}_{h}^{k+1})\right\|_{ \mathbf{B}_{k,h}^{-1}}}_{(\mathrm{ii})},\]

where the first inequality holds by triangle inequality.

For \((\mathrm{i})\), we have

\[(\mathrm{i}) =\sum_{s^{\prime}\in\mathcal{S}_{s,a}}\nabla P_{\bm{\theta}_{h} ^{k}}(s^{\prime}\mid s,a)^{\top}(\widetilde{\bm{\theta}}_{h}^{k}-\widetilde{ \bm{\theta}}_{h}^{k+1})\left\|\widetilde{\bm{\varphi}}_{s,a,s^{\prime}}( \widetilde{\bm{\theta}}_{h}^{k+1})\right\|_{\mathbf{B}_{k,h}^{-1}}\] \[\leq\sum_{s^{\prime}\in\mathcal{S}_{s,a}}\left\|\nabla P_{\bm{ \theta}_{h}^{k}}(s^{\prime}\mid s,a)\right\|_{\mathbf{B}_{k,h}^{-1}}\left\| \widetilde{\bm{\theta}}_{h}^{k}-\widetilde{\bm{\theta}}_{h}^{k+1}\right\|_{ \mathbf{B}_{k,h}}\left\|\widetilde{\bm{\varphi}}_{s,a,s^{\prime}}(\widetilde{ \bm{\theta}}_{h}^{k+1})\right\|_{\mathbf{B}_{k,h}^{-1}}\] (97)

where in the equality we apply the mean value theorem with \(\bm{\vartheta}_{h}^{k}=v\widetilde{\bm{\theta}}_{h}^{k}+(1-v)\widetilde{\bm{ \theta}}_{h}^{k+1}\) for some \(v\in[0,1]\), and the inequality follows by Cauchy-Schwarz inequality. Meanwhile, since we have

\[P_{\bm{\vartheta}_{h}^{k}}(s^{\prime}\mid s,a)\bigg{(}\widetilde{\bm{\varphi} }_{s,a,s^{\prime}}(\widetilde{\bm{\theta}}_{h}^{k+1})-\sum_{s^{\prime\prime}\in \mathcal{S}_{s,a}}P_{\bm{\vartheta}_{h}^{k}}(s^{\prime\prime}\mid s,a) \widetilde{\bm{\varphi}}_{s,a,s^{\prime\prime}}(\widetilde{\bm{\theta}}_{h}^{k+ 1})\bigg{)}\] (98) \[=P_{\bm{\vartheta}_{h}^{k}}(s^{\prime}\mid s,a)\Bigg{(}\bm{ \varphi}_{s,a,s^{\prime}}-\sum_{\widetilde{s}\in\mathcal{S}_{s,a}}P_{ \widetilde{\bm{\theta}}_{h}^{k+1}}(\widetilde{s}\mid s,a)\bm{\varphi}_{s,a, \widetilde{s}}\] \[\quad-\sum_{s^{\prime\prime}\in\mathcal{S}_{s,a}}P_{\bm{ \vartheta}_{h}^{k}}(s^{\prime\prime}\mid s,a)\left[\bm{\varphi}_{s,a,s^{ \prime\prime}}-\sum_{\widetilde{s}}P_{\widetilde{\bm{\theta}}_{h}^{k+1}}( \widetilde{s}\mid s,a)\bm{\varphi}_{s,a,\widetilde{s}}\right]\Bigg{)}\] \[=P_{\bm{\vartheta}_{h}^{k}}(s^{\prime}\mid s,a)\bm{\varphi}_{s,a,s ^{\prime}}-P_{\bm{\vartheta}_{h}^{k}}(s^{\prime}\mid s,a)\sum_{\widetilde{s}\in \mathcal{S}_{s,a}}P_{\bm{\vartheta}_{h}^{k+1}}(\widetilde{s}\mid s,a)\bm{ \varphi}_{s,a,\widetilde{s}}\] \[\quad-P_{\bm{\vartheta}_{h}^{k}}(s^{\prime}\mid s,a)\sum_{s^{ \prime\prime}\in\mathcal{S}_{s,a}}P_{\bm{\vartheta}_{h}^{k}}(s^{\prime\prime} \mid s,a)\bm{\varphi}_{s,a,s^{\prime\prime}}\] \[\quad+P_{\bm{\vartheta}_{h}^{k}}(s^{\prime}\mid s,a)\bigg{(} \underbrace{\sum_{s^{\prime\prime}\in\mathcal{S}_{s,a}}P_{\bm{\vartheta}_{h}^{k}}(s^ {\prime\prime}\mid s,a)}_{\mathrm{I}}\bigg{)}\sum_{\widetilde{s}}P_{\widetilde{ \bm{\theta}}_{h}^{k+1}}(\widetilde{s}\mid s,a)\bm{\varphi}_{s,a,\widetilde{s}}\] \[=P_{\bm{\vartheta}_{h}^{k}}(s^{\prime}\mid s,a)\bm{\varphi}_{s,a,s^ {\prime}}-P_{\bm{\vartheta}_{h}^{k}}(s^{\prime}\mid s,a)\sum_{s^{\prime\prime}\in \mathcal{S}_{s,a}}P_{\bm{\vartheta}_{h}^{k}}(s^{\prime\prime}\mid s,a)\bm{ \varphi}_{s,a,s^{\prime\prime}}\] \[=\nabla P_{\bm{\vartheta}_{h}^{k}}(s^{\prime}\mid s,a)\,,\]by substituting (98) into (97) we have

\[\mathrm{(i)}\] \[\leq\sum_{s^{\prime}\in\mathcal{S}_{s,a}}P_{\boldsymbol{\theta}_{h}^ {k}}(s^{\prime}\mid s,a)\left\|\bar{\boldsymbol{\varphi}}_{s,a,s^{\prime}}( \widetilde{\boldsymbol{\theta}}_{h}^{k+1})\right\|_{\mathbf{B}_{k,h}^{-1}}\] \[\quad\quad\quad\quad\cdot\left\|\widetilde{\boldsymbol{\theta}}_{ h}^{k}-\widetilde{\boldsymbol{\theta}}_{h}^{k+1}\right\|_{\mathbf{B}_{k,h}} \left\|\bar{\boldsymbol{\varphi}}_{s,a,s^{\prime}}(\widetilde{\boldsymbol{ \theta}}_{h}^{k+1})\right\|_{\mathbf{B}_{k,h}^{-1}}\] \[\leq\sum_{s^{\prime}\in\mathcal{S}_{s,a}}P_{\boldsymbol{\theta}_{ h}^{k}}(s^{\prime}\mid s,a)\left\|\bar{\boldsymbol{\varphi}}_{s,a,s^{\prime}}( \widetilde{\boldsymbol{\theta}}_{h}^{k+1})\right\|_{\mathbf{B}_{k,h}^{-1}} \left\|\widetilde{\boldsymbol{\theta}}_{h}^{k}-\widetilde{\boldsymbol{\theta} }_{h}^{k+1}\right\|_{\mathbf{B}_{k,h}}\] \[\quad+\bigg{(}\sum_{s^{\prime}\in\mathcal{S}_{s,a}}P_{ \boldsymbol{\theta}_{h}^{k}}(s^{\prime}\mid s,a)\left\|\bar{\boldsymbol{ \varphi}}_{s,a,s^{\prime}}(\widetilde{\boldsymbol{\theta}}_{h}^{k+1})\right\| _{\mathbf{B}_{k,h}^{-1}}\bigg{)}^{2}\left\|\widetilde{\boldsymbol{\theta}}_{ h}^{k}-\widetilde{\boldsymbol{\theta}}_{h}^{k+1}\right\|_{\mathbf{B}_{k,h}}\,.\] (99)

Note that by Jensen's inequality, we have

\[\bigg{(}\sum_{s^{\prime}\in\mathcal{S}_{s,a}}P_{\boldsymbol{ \theta}_{h}^{k}}(s^{\prime}\mid s,a)\left\|\bar{\boldsymbol{\varphi}}_{s,a,s^ {\prime}}(\widetilde{\boldsymbol{\theta}}_{h}^{k+1})\right\|_{\mathbf{B}_{k,h}^ {-1}}\bigg{)}^{2} =\bigg{(}\mathbb{E}_{s^{\prime}\sim P_{\boldsymbol{\theta}_{h}^ {k}}(\cdot\mid s,a)}\left[\left\|\bar{\boldsymbol{\varphi}}_{s,a,s^{\prime}}( \widetilde{\boldsymbol{\theta}}_{h}^{k+1})\right\|_{\mathbf{B}_{k,h}^{-1}} \right]\bigg{)}^{2}\] \[\leq\mathbb{E}_{s^{\prime}\sim P_{\boldsymbol{\theta}_{h}^{k}}( \cdot\mid s,a)}\left[\left\|\bar{\boldsymbol{\varphi}}_{s,a,s^{\prime}}( \widetilde{\boldsymbol{\theta}}_{h}^{k+1})\right\|_{\mathbf{B}_{k,h}^{-1}}^{2}\right]\] \[=\sum_{s^{\prime}\in\mathcal{S}_{s,a}}P_{\boldsymbol{\theta}_{h}^ {k}}(s^{\prime}\mid s,a)\left\|\bar{\boldsymbol{\varphi}}_{s,a,s^{\prime}}( \widetilde{\boldsymbol{\theta}}_{h}^{k+1})\right\|_{\mathbf{B}_{k,h}^{-1}}^{2}\,.\] (100)

Also, we introduce the following lemma:

**Lemma 25**.: _For any \(k\in[K]\) and \(h\in[H]\), the following holds:_

\[\left\|\widetilde{\boldsymbol{\theta}}_{h}^{k+1}-\widetilde{\boldsymbol{\theta }}_{h}^{k}\right\|_{\mathbf{B}_{k,h}}\leq\frac{4\eta L_{\boldsymbol{ \varphi}}}{\sqrt{\lambda}}\,.\]

Then, substituting (100) into (99), we have

\[\mathrm{(i)} \leq 2\sum_{s^{\prime}\in\mathcal{S}_{s,a}}P_{\boldsymbol{\theta} _{h}^{k}}(s^{\prime}\mid s,a)\left\|\bar{\boldsymbol{\varphi}}_{s,a,s^{\prime}} (\widetilde{\boldsymbol{\theta}}_{h}^{k+1})\right\|_{\mathbf{B}_{k,h}^{-1}}^{2} \left\|\widetilde{\boldsymbol{\theta}}_{h}^{k}-\widetilde{\boldsymbol{\theta} }_{h}^{k+1}\right\|_{\mathbf{B}_{k,h}}\] \[\leq\frac{8\eta L_{\boldsymbol{\varphi}}}{\sqrt{\lambda}}\sum_{s^ {\prime}\in\mathcal{S}_{s,a}}P_{\boldsymbol{\theta}_{h}^{k}}(s^{\prime}\mid s,a )\left\|\bar{\boldsymbol{\varphi}}_{s,a,s^{\prime}}(\widetilde{\boldsymbol{ \theta}}_{h}^{k+1})\right\|_{\mathbf{B}_{k,h}^{-1}}^{2}\] \[\leq\frac{8\eta L_{\boldsymbol{\varphi}}}{\sqrt{\lambda}}\max_{s^ {\prime}\in\mathcal{S}_{s,a}}\left\|\bar{\boldsymbol{\varphi}}_{s,a,s^{\prime}} (\widetilde{\boldsymbol{\theta}}_{h}^{k+1})\right\|_{\mathbf{B}_{k,h}^{-1}}^{2}\,,\] (101)

where the second inequality comes from Lemma 25, and the last inequality holds due to \(\sum_{s^{\prime}\in\mathcal{S}_{s,a}}P_{\boldsymbol{\theta}_{h}^{k}}(s^{\prime} \mid s,a)=1\).

For (ii), we have

\[\text{(ii)} =\sum_{s^{\prime}\in\mathcal{S}_{s,a}}P_{\widetilde{\bm{\theta}}_{h}^ {k}}(s^{\prime}\mid s,a)\left\|\widetilde{\bm{\varphi}}_{s,a,s^{\prime}}( \widetilde{\bm{\theta}}_{h}^{k})-\widetilde{\bm{\varphi}}_{s,a,s^{\prime}}( \widetilde{\bm{\theta}}_{h}^{k+1})\right\|_{\mathbf{B}_{k,h}^{-1}}\] \[=\sum_{s^{\prime}\in\mathcal{S}_{s,a}}P_{\widetilde{\bm{\theta}}_{h }^{k}}(s^{\prime}\mid s,a)\left\|\mathbb{E}_{\widetilde{s}\sim P_{\widetilde{ \bm{\theta}}_{h}^{k}}(\cdot\mid s,a)}\left[\bm{\varphi}_{s,a,\widetilde{s}} \right]-\mathbb{E}_{\widetilde{s}\sim P_{\widetilde{\bm{\theta}}_{h}^{k+1}}( \cdot\mid s,a)}\left[\bm{\varphi}_{s,a,\widetilde{s}}\right]\right\|_{ \mathbf{B}_{k,h}^{-1}}\] \[=\left\|\sum_{\widetilde{s}\in\mathcal{S}_{s,a}}\left(P_{ \widetilde{\bm{\theta}}_{h}^{k}}(\widetilde{s}\mid s,a)-P_{\widetilde{\bm{ \theta}}_{h}^{k+1}}(\widetilde{s}\mid s,a)\right)\bm{\varphi}_{s,a,\widetilde{ s}}\right\|_{\mathbf{B}_{k,h}^{-1}}\] \[=\left\|\sum_{\widetilde{s}\in\mathcal{S}_{s,a}}\left(P_{ \widetilde{\bm{\theta}}_{h}^{k}}(\widetilde{s}\mid s,a)-P_{\widetilde{\bm{ \theta}}_{h}^{k+1}}(\widetilde{s}\mid s,a)\right)\left(\bm{\varphi}_{s,a, \widetilde{s}}-\mathbb{E}_{s^{\prime}\sim P_{\widetilde{\bm{\theta}}_{h}^{k+1} }(\cdot\mid s,a)}\left[\bm{\varphi}_{s,a,s^{\prime}}\right]\right)\right\|_{ \mathbf{B}_{k,h}^{-1}}\] \[=\left\|\sum_{\widetilde{s}\in\mathcal{S}_{s,a}}\left(P_{ \widetilde{\bm{\theta}}_{h}^{k}}(\widetilde{s}\mid s,a)-P_{\widetilde{\bm{ \theta}}_{h}^{k+1}}(\widetilde{s}\mid s,a)\right)\widetilde{\bm{\varphi}}_{s,a,\widetilde{s}}(\widetilde{\bm{\theta}}_{h}^{k+1})\right\|_{\mathbf{B}_{k,h}^ {-1}}\] \[\leq\frac{8\eta L_{\bm{\varphi}}}{\sqrt{\lambda}}\max_{s^{\prime} \in\mathcal{S}_{s,a}}\left\|\widetilde{\bm{\varphi}}_{s,a,s^{\prime}}( \widetilde{\bm{\theta}}_{h}^{k+1})\right\|_{\mathbf{B}_{k,h}^{-1}}^{2}\,,\] (102)

where the last inequality is obtained through the same argument as used to bound \((\mathrm{i})\). Combining the results of Eq. (101) and Eq. (102), we have

\[\sum_{s^{\prime}\in\mathcal{S}_{s,a}}P_{\widetilde{\bm{\theta}}_{ h}^{k}}(s^{\prime}\mid s,a)\left\|\widetilde{\bm{\varphi}}_{s,a,s^{\prime}}( \widetilde{\bm{\theta}}_{h}^{k})\right\|_{\mathbf{B}_{k,h}^{-1}}\] \[\leq\sum_{s^{\prime}\in\mathcal{S}_{s,a}}P_{\widetilde{\bm{ \theta}}_{h}^{k+1}}(s^{\prime}\mid s,a)\left\|\widetilde{\bm{\varphi}}_{s,a,s^{ \prime}}(\widetilde{\bm{\theta}}_{h}^{k+1})\right\|_{\mathbf{B}_{k,h}^{-1}}+ \frac{16\eta L_{\bm{\varphi}}}{\sqrt{\lambda}}\max_{s^{\prime}\in\mathcal{S}_ {s,a}}\left\|\widetilde{\bm{\varphi}}_{s,a,s^{\prime}}(\widetilde{\bm{\theta}} _{h}^{k+1})\right\|_{\mathbf{B}_{k,h}^{-1}}^{2}\]

#### d.5.2 Proof of Lemma 23

Proof of Lemma 23.: Note that

\[\mathbf{B}_{k+1,h} =\mathbf{B}_{k,h}+\sum_{s^{\prime}\in\mathcal{S}_{k,h}}P_{ \widetilde{\bm{\theta}}_{h}^{k+1}}(s^{\prime}\mid s_{h}^{k},a_{h}^{k}) \widetilde{\bm{\varphi}}_{k,h,s^{\prime}}(\widetilde{\bm{\theta}}_{h}^{k+1}) \widetilde{\bm{\varphi}}_{k,h,s^{\prime}}(\widetilde{\bm{\theta}}_{h}^{k+1})^ {\top}\] \[=\mathbf{B}_{k,h}+\sum_{s^{\prime}\in\mathcal{S}_{k,h}}\widetilde{ \bm{\varphi}}_{k,h,s^{\prime}}(\widetilde{\bm{\theta}}_{h}^{k+1})\widetilde{\bm{ \varphi}}_{k,h,s^{\prime}}(\widetilde{\bm{\theta}}_{h}^{k+1})^{\top}\,,\]

where we define \(\widetilde{\bm{\varphi}}_{k,h,s^{\prime}}(\widetilde{\bm{\theta}}_{h}^{k+1}):= \sqrt{P_{\widetilde{\bm{\theta}}_{h}^{k+1}}(s^{\prime}\mid s_{h}^{k},a_{h}^{k}) \widetilde{\bm{\varphi}}_{k,h,s^{\prime}}(\widetilde{\bm{\theta}}_{h}^{k+1})}\). Then, we have

\[\det(\mathbf{B}_{k+1,h}) =\det(\mathbf{B}_{k,h})\det\left(\mathbf{I}_{d}+\mathbf{B}_{k,h}^ {-\frac{1}{2}}\sum_{s^{\prime}\in\mathcal{S}_{k,h}}\widetilde{\bm{\varphi}}_{k,h,s^{\prime}}(\widetilde{\bm{\theta}}_{h}^{k+1})\widetilde{\bm{\varphi}}_{k,h,s^{ \prime}}(\widetilde{\bm{\theta}}_{h}^{k+1})^{\top}\mathbf{B}_{k,h}^{-\frac{1}{2} }\right)\] \[=\det(\mathbf{B}_{k,h})\left(1+\sum_{s^{\prime}\in\mathcal{S}_{k,h }}\left\|\widetilde{\bm{\varphi}}_{k,h,s^{\prime}}(\widetilde{\bm{\theta}}_{h}^{k+1} )\right\|_{\mathbf{B}_{k,h}^{-1}}^{2}\right)\] \[=\det(\lambda\mathbf{I}_{d})\,\prod_{k=1}^{K}\left(1+\sum_{s^{ \prime}\in\mathcal{S}_{k,h}}\left\|\widetilde{\bm{\varphi}}_{k,h,s^{\prime}}( \widetilde{\bm{\theta}}_{h}^{k+1})\right\|_{\mathbf{B}_{k,h}^{-1}}^{2}\right)\,.\]

Taking the logarithm on both sides yields

\[\log\frac{\det(\mathbf{B}_{k+1,h})}{\det(\lambda\mathbf{I}_{d})}=\sum_{k=1}^{K} \log\left(1+\sum_{s^{\prime}\in\mathcal{S}_{k,h}}\left\|\widetilde{\bm{\varphi}}_{k,h,s^{\prime}}(\widetilde{\bm{\theta}}_{h}^{k+1})\right\|_{\mathbf{B}_{k,h}^{-1}}^{2} \right)\,.\]On the other hand, since \(\lambda\geq L_{\bm{\varphi}}^{2}\),

\[\sum_{s^{\prime}\in\mathcal{S}_{\bm{k},h}}\left\|\widetilde{\bm{ \varphi}}_{k,h,s^{\prime}}(\widetilde{\bm{\theta}}_{h}^{k+1})\right\|_{\mathbf{ B}_{h,h}^{-1}}^{2} \leq\sum_{s^{\prime}\in\mathcal{S}_{\bm{k},h}}\frac{1}{\lambda} \left\|\widetilde{\bm{\varphi}}_{k,h,s^{\prime}}(\widetilde{\bm{\theta}}_{h}^{ k+1})\right\|_{2}^{2}\] \[=\sum_{s^{\prime}\in\mathcal{S}_{\bm{k},h}}\frac{1}{\lambda}P_{ \widetilde{\bm{\theta}}_{h}^{k+1}}(s^{\prime}\mid s_{h}^{k},a_{h}^{k})\left\| \widetilde{\bm{\varphi}}_{k,h,s^{\prime}}(\widetilde{\bm{\theta}}_{h}^{k+1}) \right\|_{2}^{2}\] \[\leq\frac{L_{\bm{\varphi}}^{2}}{\lambda}\sum_{s^{\prime}\in \mathcal{S}_{\bm{k},h}}P_{\widetilde{\bm{\theta}}_{h}^{k+1}}(s^{\prime}\mid s _{h}^{k},a_{h}^{k})\] \[\leq 1\,,\]

where the last inequality uses \(\sum_{s^{\prime}\in\mathcal{S}_{\bm{k},h}}P_{\widetilde{\bm{\theta}}_{h}^{k+1} }(s^{\prime}\mid s_{h}^{k},a_{h}^{k})=1\). From the fact that \(z\leq 2\log(1+z)\) for any \(z\in[0,1]\), it follows that

\[\sum_{k=1}^{K}\log\left(1+\sum_{s^{\prime}\in\mathcal{S}_{\bm{k},h}}\left\| \widetilde{\bm{\varphi}}_{k,h,s^{\prime}}(\widetilde{\bm{\theta}}_{h}^{k+1}) \right\|_{\mathbf{B}_{h,h}^{-1}}^{2}\right)\geq\sum_{k=1}^{K}\frac{1}{2}\sum_{ s^{\prime}\in\mathcal{S}_{\bm{k},h}}\left\|\widetilde{\bm{\varphi}}_{k,h,s^{ \prime}}(\widetilde{\bm{\theta}}_{h}^{k+1})\right\|_{\mathbf{B}_{\bm{k},h}^{-1 }}^{2}\,.\]

Finally, we obtain

\[\sum_{k=1}^{K}\sum_{s^{\prime}\in\mathcal{S}_{\bm{k},h}}\left\| \widetilde{\bm{\varphi}}_{k,h,s^{\prime}}(\widetilde{\bm{\theta}}_{h}^{k+1}) \right\|_{\mathbf{B}_{\bm{k},h}^{-1}}^{2} \leq 2\sum_{k=1}^{K}\log\left(1+\sum_{s^{\prime}\in\mathcal{S}_{\bm{ k},h}}\left\|\widetilde{\bm{\varphi}}_{k,h,s^{\prime}}(\widetilde{\bm{\theta}}_{h}^{k+1}) \right\|_{\mathbf{B}_{\bm{k},h}^{-1}}^{2}\right)\] \[=2\log\frac{\det(\mathbf{B}_{K+1,h})}{\det(\lambda\mathbf{I}_{d})}\] \[\leq 2d\log\left(1+\frac{K\mathcal{U}L_{\bm{\varphi}}^{2}}{d \lambda}\right)\,,\]

where the last inequality follows by the determinant-trace inequality (Lemma 28). 

#### d.5.3 Proof of Lemma 24

Proof of Lemma 24.: Since \(\bm{\xi}_{k,h}^{(m)}\sim\mathcal{N}(\mathbf{0},\sigma_{k}^{2}\mathbf{B}_{k,h} ^{-1})\), by Lemma 30 for each \(m\in[M]\), we have

\[\left\|\bm{\xi}_{k,h}^{(m)}\right\|_{\mathbf{B}_{\bm{k},h}} \leq C_{\bm{\xi}}\sigma_{k}\sqrt{d\log(Md/\delta)}\,.\]

Following the result of Lemma 22, we have

\[\sum_{s^{\prime}\in\mathcal{S}_{\bm{k},h}}P_{\widetilde{\bm{ \theta}}_{h}^{k}}(s^{\prime}\mid s_{h}^{k},a_{h}^{k})\left\|\tilde{\bm{\varphi}} _{k,h,s^{\prime}}(\widetilde{\bm{\theta}}_{h}^{k})\right\|_{\mathbf{B}_{\bm{k}, h}^{-1}} \leq\sum_{s^{\prime}\in\mathcal{S}_{\bm{k},h}}P_{\widetilde{\bm{ \theta}}_{h}^{k+1}}(s^{\prime}\mid s_{h}^{k},a_{h}^{k})\left\|\tilde{\bm{ \varphi}}_{k,h,s^{\prime}}(\widetilde{\bm{\theta}}_{h}^{k+1})\right\|_{ \mathbf{B}_{\bm{k},h}^{-1}}\] \[\quad+\frac{16\eta L_{\bm{\varphi}}}{\sqrt{\lambda}}\max_{s^{ \prime}\in\mathcal{S}_{\bm{k},h}}\left\|\tilde{\bm{\varphi}}_{k,h,s^{\prime}}( \widetilde{\bm{\theta}}_{h}^{k+1})\right\|_{\mathbf{B}_{\bm{k},h}^{-1}}^{2}\,.\]

Then, we obtain

\[\sum_{s^{\prime}\in\mathcal{S}_{\bm{k},h}}P_{\widetilde{\bm{\theta }}_{h}^{k}}(s^{\prime}\mid s_{h}^{k},a_{h}^{k})\tilde{\bm{\varphi}}_{k,h,s^{ \prime}}(\widetilde{\bm{\theta}}_{h}^{k})^{\top}\bm{\xi}_{k,h}^{s^{\prime}}\] \[\leq\sum_{s^{\prime}\in\mathcal{S}_{\bm{k},h}}P_{\widetilde{\bm{ \theta}}_{h}^{k}}(s^{\prime}\mid s_{h}^{k},a_{h}^{k})\left\|\tilde{\bm{\varphi}}_ {k,h,s^{\prime}}(\widetilde{\bm{\theta}}_{h}^{k})\right\|_{\mathbf{B}_{\bm{k},h} ^{-1}}\left\|\bm{\xi}_{k,h}^{s^{\prime}}\right\|_{\mathbf{B}_{\bm{k},h}}\] \[\leq C_{\bm{\xi}}\sigma_{k}\sqrt{d\log(Md/\delta)}\sum_{s^{ \prime}\in\mathcal{S}_{\bm{k},h}}P_{\widetilde{\bm{\theta}}_{h}^{k}}(s^{\prime} \mid s_{h}^{k},a_{h}^{k})\left\|\tilde{\varphi}_{k,h,s^{\prime}}(\widetilde{\bm{ \theta}}_{h}^{k})\right\|_{\mathbf{B}_{\bm{k},h}^{-1}}\] \[\leq\gamma_{k}(\delta)\bigg{(}\sum_{s^{\prime}\in\mathcal{S}_{\bm{ k},h}}P_{\widetilde{\bm{\theta}}_{h}^{k+1}}(s^{\prime}\mid s_{h}^{k},a_{h}^{k})\left\| \tilde{\bm{\varphi}}_{k,h,s^{\prime}}(\widetilde{\bm{\theta}}_{h}^{k+1}) \right\|_{\mathbf{B}_{\bm{k},h}^{-1}}\] \[\qquad\qquad+\frac{16\eta L_{\bm{\varphi}}}{\sqrt{\lambda}}\max_{s^ {\prime}\in\mathcal{S}_{\bm{k},h}}\left\|\tilde{\bm{\varphi}}_{k,h,s^{\prime}}( \widetilde{\bm{\theta}}_{h}^{k+1})\right\|_{\mathbf{B}_{\bm{k},h}^{-1}}^{2} \bigg{)}\,.\]

#### d.5.4 Proof of Lemma 25

Proof of Lemma 25.: We provide a proof for Lemma 25 since it is slight modification of Lemma 20 of [76]. From the definition, we know that

\[\left(\widetilde{\bm{\theta}}_{h}^{k+1}\right)^{\top}\nabla\ell_{k,h}( \widetilde{\bm{\theta}}_{h}^{k})+\frac{1}{2\eta}\left\|\widetilde{\bm{\theta}} _{h}^{k+1}-\widetilde{\bm{\theta}}_{h}^{k}\right\|_{\widetilde{\mathbf{B}}_{k,h}}^{2}\leq\left(\widetilde{\bm{\theta}}_{h}^{k}\right)^{\top}\nabla\ell_{k,h} (\widetilde{\bm{\theta}}_{h}^{k})\,.\]

By rearranging the terms, the following holds:

\[\frac{1}{2\eta}\left\|\widetilde{\bm{\theta}}_{h}^{k+1}-\widetilde {\bm{\theta}}_{h}^{k}\right\|_{\widetilde{\mathbf{B}}_{k,h}}^{2} \leq\left(\widetilde{\bm{\theta}}_{h}^{k}-\widetilde{\bm{\theta} }_{h}^{k+1}\right)^{\top}\nabla\ell_{k,h}(\widetilde{\bm{\theta}}_{h}^{k})\] \[\leq\left\|\widetilde{\bm{\theta}}_{h}^{k}-\widetilde{\bm{ \theta}}_{h}^{k+1}\right\|_{\widetilde{\mathbf{B}}_{k,h}}\left\|\nabla\ell_{k, h}(\widetilde{\bm{\theta}}_{h}^{k})\right\|_{\widetilde{\mathbf{B}}_{k,h}^{-1}}\]

Thus, we get

\[\left\|\widetilde{\bm{\theta}}_{h}^{k+1}-\widetilde{\bm{\theta}}_{h}^{k}\right\| _{\widetilde{\mathbf{B}}_{k,h}}\leq 2\eta\left\|\nabla\ell_{k,h}(\widetilde{\bm{ \theta}}_{h}^{k})\right\|_{\widetilde{\mathbf{B}}_{k,h}^{-1}}\,.\]

Since \(\mathbf{B}_{k,h}\preceq\widetilde{\mathbf{B}}_{k,h}\) and \(\widetilde{\mathbf{B}}_{k,h}^{-1}\preceq\lambda^{-1}\mathbf{I}_{d}\), we obtain

\[\left\|\widetilde{\bm{\theta}}_{h}^{k+1}-\widetilde{\bm{\theta}}_{h}^{k} \right\|_{\widetilde{\mathbf{B}}_{k,h}}\leq\left\|\widetilde{\bm{\theta}}_{h}^ {k+1}-\widetilde{\bm{\theta}}_{h}^{k}\right\|_{\widetilde{\mathbf{B}}_{k,h}} \leq 2\eta\left\|\nabla\ell_{k,h}(\widetilde{\bm{\theta}}_{h}^{k})\right\| _{\widetilde{\mathbf{B}}_{k,h}^{-1}}\leq\frac{2\eta}{\sqrt{\lambda}}\left\| \nabla\ell_{k,h}(\widetilde{\bm{\theta}}_{h}^{k})\right\|_{2}\leq\frac{4\eta L _{\bm{\varphi}}}{\sqrt{\lambda}}\,.\] (103)

For the last inequality of (103), we provide the upper bound of \(l_{2}\)-norm of \(\nabla\ell_{k,h}(\bm{\theta})\). Since

\[\ell_{k,h}(\bm{\theta})=-\sum_{s^{\prime}\in\mathcal{S}_{k,h}}y_{h}^{k}(s^{ \prime})\log P_{\bm{\theta}}(s^{\prime}\mid s_{h}^{k},a_{h}^{k})\,,\]

the gradient of the loss function is given by

\[\nabla\ell_{k,h}(\bm{\theta}) =-\sum_{s^{\prime}\in\mathcal{S}_{k,h}}y_{h}^{k}(s^{\prime})\left( \bm{\varphi}_{s,a,s^{\prime}}-\sum_{s^{\prime\prime}\in\mathcal{S}_{k,h}}P_{ \bm{\theta}}(s^{\prime\prime}\mid s_{h}^{k},a_{h}^{k})\bm{\varphi}_{s,a,s^{ \prime\prime}}\right)\] \[=\sum_{s^{\prime}\in\mathcal{S}_{k,h}}y_{h}^{k}(s^{\prime})\sum_{ s^{\prime\prime}\in\mathcal{S}_{k,h}}P_{\bm{\theta}}(s^{\prime\prime}\mid s_{h}^{k},a_{h}^ {k})\bm{\varphi}_{s,a,s^{\prime\prime}}-\sum_{s^{\prime}\in\mathcal{S}_{k,h}}y _{h}^{k}(s^{\prime})\bm{\varphi}_{s,a,s^{\prime}}\] \[=\sum_{s^{\prime\prime}\in\mathcal{S}_{k,h}}P_{\bm{\theta}}(s^{ \prime\prime}\mid s_{h}^{k},a_{h}^{k})\bm{\varphi}_{s,a,s^{\prime\prime}}- \sum_{s^{\prime}\in\mathcal{S}_{k,h}}y_{h}^{k}(s^{\prime})\bm{\varphi}_{s,a,s^ {\prime}}\] \[=\sum_{s^{\prime}\in\mathcal{S}_{k,h}}\left(P_{\bm{\theta}}(s^{ \prime}\mid s_{h}^{k},a_{h}^{k})-y_{h}^{k}(s^{\prime})\right)\bm{\varphi}_{s,a,s^{\prime}}\,.\]

Therefore, we have

\[\left\|\nabla\ell_{k,h}(\bm{\theta})\right\|_{2} =\left\|\sum_{s^{\prime}\in\mathcal{S}_{k,h}}\left(P_{\bm{\theta}} (s^{\prime}\mid s_{h}^{k},a_{h}^{k})-y_{h}^{k}(s^{\prime})\right)\bm{\varphi}_ {s,a,s^{\prime}}\right\|_{2}\] \[\leq\sum_{s^{\prime}\in\mathcal{S}_{k,h}}\left|P_{\bm{\theta}}(s^{ \prime}\mid s_{h}^{k},a_{h}^{k})-y_{h}^{k}(s^{\prime})\right|\|\bm{\varphi}_{s,a,s^{\prime}}\|_{2}\] \[\leq 2L_{\bm{\varphi}}\]

and this concludes the proof. 

### Bound on Pessimism Part

In this section, we provide the upper bound on the pessimism part of the regret: \(\sum_{k=1}^{K}(V_{1}^{*}-\widetilde{V}_{1}^{k})(s_{1}^{k})\).

**Lemma 26** (Bound on pessimism).: _For any \(\delta\) with \(0<\delta<\Phi(-1)/2\), let \(\sigma_{k}=H\beta_{k}\). If \(\lambda=\mathcal{O}(L_{\bm{\varphi}}^{2}d\log\mathcal{U})\) and we take multiple sample size \(M=\lceil 1-\frac{\log(H\mathcal{U})}{\log\Phi(1)}\rceil\), then with probability at least \(1-\delta/2\), we have_

\[\sum_{k=1}^{K}(V_{1}^{*}-V_{1}^{k})(s_{1}^{k})=\widetilde{\mathcal{O}}\left(d^{3/ 2}H^{3/2}\sqrt{T}+\kappa^{-1}d^{2}H^{2}\right)\,.\]Proof of Lemma 26.: As seen in Lemma 18, by using multiple sampling technique we show that the optimistic randomized value function \(\tilde{V}\) of \(\mathtt{ORRL}\)-\(\mathtt{MNL}\) is optimistic than the true optimal value with constant probability Hence, with the same argument used in Lemma 11, we can show that the pessimism term of \(\mathtt{ORRL}\)-\(\mathtt{MNL}\) is upper bounded by a bound of the estimation term times the inverse probability of being optimistic, i.e.,

\[\sum_{k=1}^{K}\left(V_{1}^{*}-V_{1}^{k}\right)(s_{1}^{k})\leq\widetilde{ \mathcal{O}}\left(\frac{1}{\Phi(-1)}\sum_{k=1}^{K}\left(V_{1}^{k}-{V_{1}^{ \pi^{k}}}\right)(s_{1}^{k})\right)\,.\]

### Regret Bound of \(\mathtt{ORRL}\)-\(\mathtt{NNL}\)

Proof of Theorem 2.: Since both Lemma 21 and Lemma 26 holds with probability at least \(1-\delta/2\) respectively, by taking the union bound we conclude the proof. 

## Appendix E Optimistic Exploration Extension

In this section, we introduce \(\mathtt{UCRL}\)-\(\mathtt{MNL}\)+ (Algorithm 3), which is both _computationally_ and _statistically_ efficient for \(\mathtt{MNL}\)-MDPs with \(\mathtt{UCB}\)-based exploration. The main difference compared to \(\mathtt{ORRL}\)-\(\mathtt{MNL}\) is that \(\mathtt{UCRL}\)-\(\mathtt{MNL}\)+ constructs an _optimistic value function_ that is greater than the optimal value function with high probability. At each episode \(k\in[K]\), with the estimated transition core parameter \(\widetilde{\bm{\theta}}_{h}^{k}\) (5), for \((s,a)\in\mathcal{S}\times\mathcal{A}\), set \(\hat{Q}_{H+1}^{k}(s,a)=0\). For each \(h\in[H]\),

\[\hat{Q}_{h}^{k}(s,a):=r(s,a)+\sum_{s^{\prime}\in\mathcal{S}_{s,a}}P_{ \widetilde{\bm{\theta}}_{h}^{k}}(s^{\prime}\mid s,a)\hat{V}_{h+1}^{k}(s^{ \prime})+\nu_{k,h}^{\mathrm{opt}}(s,a)\,,\] (104)

where \(\hat{V}_{h}^{k}(s):=\min\{\max_{a\in\mathcal{A}}\hat{Q}_{h}^{k}(s,a),H\}\) and \(\nu_{k,h}^{\mathrm{opt}}(s,a)\) is the _optimistic bonus term_ defined by

\[\nu_{k,h}^{\mathrm{opt}}(s,a):=H\beta_{k}\sum_{s^{\prime}\in\mathcal{S}_{s,a} }P_{\widetilde{\bm{\theta}}_{h}^{k}}(s^{\prime}\mid s,a)\|\tilde{\bm{\varphi} }(s,a,s^{\prime};\widetilde{\bm{\theta}}_{h}^{k})\|_{\mathbf{B}_{k,h}^{-1}}+3H \beta_{k}^{2}\max_{s^{\prime}\in\mathcal{S}_{s,a}}\|\bm{\varphi}(s,a,s^{\prime })\|_{\mathbf{B}_{k,h}^{-1}}^{2}\,.\]

Based on these _optimistic value function_\(\hat{Q}_{h}^{k}\), at each episode the agent plays a greedy action with respect to \(\hat{Q}_{h}^{k}\) as summarized in Algorithm 3.

```
1:Inputs: Episodic MDP \(\mathcal{M}\), Feature map \(\bm{\varphi}:\mathcal{S}\times\mathcal{A}\times\mathcal{S}\rightarrow\mathbb{R} ^{d}\), Number of episodes \(K\), Regularization parameter \(\lambda\), Confidence radius \(\{\beta_{k}\}_{k=1}^{K}\), Step size \(\eta\)
2:Initialize:\(\widetilde{\bm{\theta}}_{h}^{1}=\bm{0}_{d}\), \(\bm{\mathrm{B}}_{1,h}=\lambda\bm{\mathrm{I}}_{d}\) for all \(h\in[H]\)
3:for episode \(k=1,2,\cdots,K\)do
4: Observe \(s_{1}^{k}\) and set \(\left\{\hat{Q}_{h}^{k}(\cdot,\cdot)\right\}_{h\in[H]}\) as described in (104)
5:for horizon \(h=1,2,\cdots,H\)do
6: Select \(a_{h}^{k}=\operatorname*{argmax}_{a\in\mathcal{A}}\hat{Q}_{h}^{k}(s_{h}^{k},a)\) and observe \(s_{h+1}^{k}\)
7: Update \(\widetilde{\bm{\mathrm{B}}}_{k,h}=\bm{\mathrm{B}}_{k,h}+\eta\nabla^{2}\ell_{k, h}(\widetilde{\bm{\theta}}_{h}^{k})\) and \(\widetilde{\bm{\theta}}_{h}^{k+1}\) as in (5)
8: Update \(\bm{\mathrm{B}}_{k+1,h}=\bm{\mathrm{B}}_{k,h}+\nabla^{2}\ell_{k,h}(\widetilde{ \bm{\theta}}_{h}^{k+1})\)
9:endfor
10:endfor ```

**Algorithm 3**\(\mathtt{UCRL}\)-\(\mathtt{MNL}\)+ (Upper Confidence RL for \(\mathtt{MNL}\)-MDPs)

The main difference in regret analysis lies in ensuring the optimism of the estimated value function \(\hat{Q}_{h}^{k}\) (Lemma 27). In the following statement (formal statement of Corollary 1), we provide a regret guarantee for \(\mathtt{UCRL}\)-\(\mathtt{MNL}\)+, which enjoys the tightest regret bound for \(\mathtt{MNL}\)-MDPs.

**Theorem 3** (Regret Bound of \(\mathtt{UCRL}\)-\(\mathtt{MNL}\)+).: _Suppose that Assumption 1- 4 hold. For any \(\delta\in(0,1)\), if we set the input parameters in Algorithm 3 as \(\lambda=\mathcal{O}(L_{\bm{\varphi}}^{2}d\log\mathcal{U}),\beta_{k}=\mathcal{O}( \sqrt{d}\log\mathcal{U}\log(kH))\)\(\eta=\mathcal{O}(\log\mathcal{U})\), then with probability at least \(1-\delta\), the cumulative regret of the UCRL-MNL+ policy \(\pi\) is upper-bounded by_

\[\textbf{Regret}_{\pi}(K)=\widetilde{\mathcal{O}}\left(dH^{3/2}\sqrt{T}+\kappa^{ -1}d^{2}H^{2}\right)\,,\]

_where \(T=KH\) is the total number of time steps._

Proof of Theorem 3.: By Lemma 17, suppose that the good event \(\mathfrak{G}(K,\delta^{\prime})\) holds with probability at least \(1-\delta\). Then, we show that the optimistic value function \(\hat{Q}_{h}^{k}\) is deterministically greater than the true optimal value function as follows:

**Lemma 27** (Optimism).: _Suppose that the event \(\mathfrak{G}_{k,h}^{\Delta}(\delta)\) holds for all \(k\in[K]\) and \(h\in[H]\). Then for any \((s,a)\in\mathcal{S}\times\mathcal{A}\), we have_

\[Q_{h}^{*}(s,a)\leq\hat{Q}_{h}^{k}(s,a)\,.\]

Conditioned on \(\mathfrak{G}(K,\delta^{\prime})\), by Lemma 27 we have

\[(V_{1}^{*}-V_{1}^{\pi^{k}})(s_{1}^{k}) =Q_{1}^{*}(s_{1}^{k},\pi^{*}(s_{1}^{k}))-Q_{1}^{\pi^{k}}(s_{1}^{k },a_{1}^{k})\] \[\leq\hat{Q}_{1}^{k}(s_{1}^{k},\pi^{*}(s_{1}^{k}))-Q_{1}^{\pi^{k}}( s_{1}^{k},a_{1}^{k})\] \[\leq\hat{Q}_{1}^{k}(s_{1}^{k},a_{1}^{k})-Q_{1}^{\pi^{k}}(s_{1}^{k},a_{1}^{k})=\nu_{k,1}^{\mathrm{opt}}(s_{1}^{k},a_{1}^{k})+P_{1}(\hat{V}_{2}^{k }-V_{2}^{\pi^{k}})(s_{1}^{k},a_{1}^{k})\,.\]

Note that

\[P_{1}(\hat{V}_{2}^{k}-V_{2}^{\pi^{k}})(s_{1}^{k},a_{1}^{k})=\mathbb{E}_{\bar{ \jmath}|s_{1}^{k},a_{1}^{k}}\left[(\hat{V}_{2}^{k}-V_{2}^{\pi^{k}})(\bar{s}) \right]=(\hat{V}_{2}^{k}-V_{2}^{\pi^{k}})(s_{2}^{k})+\dot{\zeta}_{1}^{k}\,,\]

where we denote \(\zeta_{h}^{k}:=(\hat{V}_{h+1}^{k}-V_{h+1}^{\pi^{k}})(s_{h+1}^{k})-\mathbb{E}_{ \bar{\jmath}|s_{h}^{k},a_{h}^{k}}\left[(\hat{V}_{h+1}^{k}-V_{h+1}^{\pi^{k}})( \bar{s})\right]\). Then, with the same argument, we have

\[(V_{1}^{*}-V_{1}^{\pi^{k}})(s_{1}^{k})\leq\sum_{h=1}^{H}\nu_{k,h}^{\mathrm{opt }}(s_{h}^{k},a_{h}^{k})+\sum_{h=1}^{H}\dot{\zeta}_{h}^{k}\,.\]

By summing over all episodes, we have

\[\textbf{Regret}_{\pi}(K)\leq\sum_{k=1}^{K}\sum_{h=1}^{H}\nu_{k,h}^{\mathrm{opt }}(s_{h}^{k},a_{h}^{k})+\sum_{k=1}^{K}\sum_{h=1}^{H}\dot{\zeta}_{h}^{k}\,.\] (105)

On the other hand, note that

\[\sum_{k=1}^{K}\sum_{h=1}^{H}\nu_{k,h}^{\mathrm{opt}}(s_{h}^{k},a_ {h}^{k})\] \[=\sum_{k=1}^{K}\sum_{h=1}^{H}H\beta_{k}\sum_{s^{\prime}\in \mathcal{S}_{k,h}}P_{\boldsymbol{\tilde{\theta}}_{h}^{k}}(s^{\prime}\mid s_{h} ^{k},a_{h}^{k})\|\boldsymbol{\bar{\varphi}}_{k,h,s^{\prime}}(\boldsymbol{ \widetilde{\theta}}_{h}^{k})\|_{\mathbf{B}_{k,h}^{-1}}+\sum_{k=1}^{K}\sum_{h=1 }^{H}3H\beta_{k}^{2}\max_{s^{\prime}\in\mathcal{S}_{k,h}}\|\boldsymbol{\varphi} _{k,h,s^{\prime}}\|^{2}_{\mathbf{B}_{k,h}^{-1}}\] \[\leq H\beta_{K}\sum_{k=1}^{K}\sum_{h=1}^{H}\sum_{s^{\prime}\in \mathcal{S}_{k,h}}P_{\boldsymbol{\widetilde{\theta}}_{h}^{k}}(s^{\prime}\mid s _{h}^{k},a_{h}^{k})\|\boldsymbol{\bar{\varphi}}_{k,h,s^{\prime}}(\boldsymbol{ \widetilde{\theta}}_{h}^{k})\|_{\mathbf{B}_{k,h}^{-1}}\] \[\quad+3H\beta_{K}^{2}\sum_{k=1}^{K}\sum_{h=1}^{H}\max_{s^{\prime }\in\mathcal{S}_{k,h}}\|\boldsymbol{\varphi}_{k,h,s^{\prime}}\|^{2}_{\mathbf{ B}_{k,h}^{-1}}\] \[\leq\underbrace{H\beta_{K}\sum_{k=1}^{K}\sum_{h=1}^{H}\sum_{s^{ \prime}\in\mathcal{S}_{k,h}}P_{\boldsymbol{\widetilde{\theta}}_{h}^{k+1}}(s^{ \prime}\mid s_{h}^{k},a_{h}^{k})\left\|\boldsymbol{\bar{\varphi}}_{k,h,s^{ \prime}}(\boldsymbol{\widetilde{\theta}}_{h}^{k+1})\right\|_{\mathbf{B}_{k,h}^{- 1}}}_{(\mathrm{ii})}\] \[\quad+\underbrace{\frac{16\eta L_{\boldsymbol{\varphi}}}{\sqrt{ \lambda}}H\beta_{K}\sum_{k=1}^{K}\sum_{h=1}^{H}\max_{s^{\prime}\in\mathcal{S} _{k,h}}\left\|\boldsymbol{\bar{\varphi}}_{k,h,s^{\prime}}(\boldsymbol{ \widetilde{\theta}}_{h}^{k+1})\right\|^{2}_{\mathbf{B}_{k,h}^{-1}}}_{(\mathrm{ ii})}+\underbrace{3H\beta_{K}^{2}\sum_{k=1}^{K}\sum_{h=1}^{H}\max_{s^{\prime}\in \mathcal{S}_{k,h}}\|\boldsymbol{\varphi}_{k,h,s^{\prime}}\|^{2}_{\mathbf{B}_{k,h }^{-1}}}_{(\mathrm{iii})}\,,\]where the last inequality follows by Lemma 22.

Term \((\mathrm{i})\) can be bounded as in Eq. (91):

\[H\beta_{K}\sum_{k=1}^{K}\sum_{h=1}^{H}\sum_{s^{\prime}\in\mathcal{S}_{k,h}}P_{ \widetilde{\bm{\theta}}_{h}^{k+1}}(s^{\prime}\mid s_{h}^{k},a_{h}^{k})\left\| \widetilde{\bm{\varphi}}_{k,h,s^{\prime}}(\widetilde{\bm{\theta}}_{h}^{k+1}) \right\|_{\mathbf{B}_{k,h}^{-1}}=\widetilde{\mathcal{O}}(dH^{3/2}\sqrt{T})\,.\] (106)

For term \((\mathrm{ii})\), recall that as in Eq. (93) we have

\[\sum_{k=1}^{K}\sum_{h=1}^{H}\max_{s^{\prime}\in\mathcal{S}_{k,h}}\left\| \widetilde{\bm{\varphi}}_{k,h,s^{\prime}}(\widetilde{\bm{\theta}}_{h}^{k+1}) \right\|_{\mathbf{B}_{k,h}^{-1}}^{2}\leq 16\kappa^{-1}dH\log\left(1+\frac{K \mathcal{U}L_{\bm{\varphi}}^{2}}{d\lambda}\right)\,.\]

Then, we have

\[\frac{16\eta L_{\bm{\varphi}}}{\sqrt{\lambda}}H\beta_{K}\sum_{k=1}^{K}\sum_{h= 1}^{H}\max_{s^{\prime}\in\mathcal{S}_{k,h}}\left\|\widetilde{\bm{\varphi}}_{ k,h,s^{\prime}}(\widetilde{\bm{\theta}}_{h}^{k+1})\right\|_{\mathbf{B}_{k,h}^{-1}}^ {2}=\widetilde{\mathcal{O}}(\kappa^{-1}dH^{2})\,.\] (107)

For term \((\mathrm{iii})\), since we have

\[3H\beta_{K}^{2}\sum_{k=1}^{K}\sum_{h=1}^{H}\max_{s^{\prime}\in \mathcal{S}_{k,h}}\left\|\bm{\varphi}_{k,h,s^{\prime}}\right\|_{\mathbf{B}_{k,h}^{-1}}^{2} \leq 3H\beta_{K}^{2}\sum_{k=1}^{K}\sum_{h=1}^{H}\max_{s^{\prime} \in\mathcal{S}_{k,h}}\left\|\bm{\varphi}_{k,h,s^{\prime}}\right\|_{\mathbf{A }_{k,h}^{-1}}^{2}\] \[\leq 12\kappa^{-1}dH^{2}\beta_{K}^{2}\log\left(1+K\mathcal{U}L_{ \bm{\varphi}}^{2}/(d\lambda)\right)\] \[=\widetilde{\mathcal{O}}(\kappa^{-1}d^{2}H^{2})\,.\] (108)

Combining the results of Eq. (106), (107), and (108), we have

\[\sum_{k=1}^{K}\sum_{h=1}^{H}\nu_{k,h}^{\mathrm{opt}}(s_{h}^{k},a_{h}^{k})= \widetilde{\mathcal{O}}(dH^{3/2}\sqrt{T}+\kappa^{-1}d^{2}H^{2})\,.\]

Finally, by Azuma-Hoeffiding inequality as in Eq. (96) we have

\[\sum_{k=1}^{K}\sum_{h=1}^{H}\dot{\zeta}_{h}^{k}=\widetilde{\mathcal{O}}(H \sqrt{T})\,.\]

This concludes the proof. 

In the following, we provide the proof of Lemma 27.

### Optimism

Proof of Lemma 27.: We prove this by backwards induction on \(h\). For the base case \(h=H\), since \(V_{H+1}^{*}(s)=\dot{V}_{H+1}^{k}(s)=0\) for all \(s\in\mathcal{S}\), we have

\[\hat{Q}_{H}^{k}(s,a)=r(s,a)=Q_{H}^{*}(s,a)\,.\]Suppose that the statement holds for \(h+1\) where \(h\in[H-1]\). Then, for \(h\) and for any \((s,a)\in\mathcal{S}\times\mathcal{A}\),

\[\hat{Q}_{h}^{k}(s,a)\] \[=r(s,a)+\sum_{s^{\prime}\in\mathcal{S}_{s,a}}P_{\vec{\theta}_{h}^{ k}}(s^{\prime}\mid s,a)\hat{V}_{h+1}^{k}(s^{\prime})+\nu_{k,h}^{\mathrm{opt}}(s,a)\] \[\geq r(s,a)+\sum_{s^{\prime}\in\mathcal{S}_{s,a}}P_{\vec{\theta}_ {h}^{k}}(s^{\prime}\mid s,a)V_{h+1}^{*}(s^{\prime})+\nu_{k,h}^{\mathrm{opt}}(s,a)\] \[=r(s,a)+\sum_{s^{\prime}\in\mathcal{S}_{s,a}}P_{\vec{\theta}_{h}^ {*}}(s^{\prime}\mid s,a)V_{h+1}^{*}(s^{\prime})\] \[\quad+\sum_{s^{\prime}\in\mathcal{S}_{s,a}}\left(P_{\vec{\theta} _{h}^{k}}(s^{\prime}\mid s,a)-P_{\vec{\theta}_{h}^{*}}(s^{\prime}\mid s,a) \right)V_{h+1}^{*}(s^{\prime})+\nu_{k,h}^{\mathrm{opt}}(s,a)\] \[\geq r(s,a)+\sum_{s^{\prime}\in\mathcal{S}_{s,a}}P_{\vec{\theta}_ {h}^{*}}(s^{\prime}\mid s,a)V_{h+1}^{*}(s^{\prime})\] \[=Q_{h}^{*}(s,a)\,,\]

where the first inequality follows from the induction hypothesis and the second inequality holds by Lemma 16. 

## Appendix F Experiment Details

The RiverSwim environment (Figure 2) consists of \(n\) states that are arranged in a chain. The agent starts in the leftmost state with a relatively small reward of \(0.005\) and aims to reach the rightmost state, which has a relatively large reward of \(1\). Choosing to swim to the left moves the agent deterministically to the left, while swimming to the right has a probability of transitioning the agent toward the right state, but also a high chance of remaining in the current state or even moving left due to the strong current of river. Therefore, efficient exploration is crucial in order to learn the optimal policy for this environment.

We fine-tuned the hyperparameters for each algorithm within specific ranges. Figures 0(a) and 0(b) show the episodic returns in the RiverSwim environment over 10 independent runs with \(|\mathcal{S}|=4,H=12\), and \(K=10,000\) and \(|\mathcal{S}|=8,H=24\), and \(K=10,000\), respectively. The shaded areas represent the standard deviations (1-sigma error). Figure 0(c) compares the running time of the algorithms over the first 1,000 episodes. All experiments were conducted on a Xeon(R) Gold 6226R CPU @ 2.90GHz (16 cores).

## Appendix G Auxiliary Lemmas

**Lemma 28** (Determinant-trace inequality [1]).: _Suppose \(\mathbf{x}_{1},\ldots,\mathbf{x}_{t}\in\mathbb{R}^{d}\) and for any \(1\leq\tau\leq t\), \(\|\mathbf{x}_{\tau}\|_{2}\leq L\). Let \(\mathbf{V}_{t}=\lambda\mathbf{I}_{d}+\sum_{\tau=1}^{t}\mathbf{x}_{\tau} \mathbf{x}_{\tau}^{\top}\) for some \(\lambda>0\). Then,_

\[\det(\mathbf{V}_{t})\leq(\lambda+tL^{2}/d)^{d}\,.\]

**Lemma 29** (Freedman's inequality [29]).: _Consider a real-valued martingale \(\{Y_{k}:k=0,1,2,\ldots\}\) with difference sequence \(\{X_{k}:k=0,1,2,3,\ldots\}\). Assume that the difference sequence is uniformly

Figure 2: The “RiverSwim” environment with \(n\) states [58]

bounded, \(X_{k}\leq R\) almost surely for \(k=1,2,3,\ldots\). Define the predictable quadratic variation process of the martingale:_

\[W_{k}:=\sum_{j=1}^{k}\mathbb{E}_{j-1}[X_{j}^{2}]\quad\text{for }k=1,2,3,\ldots.\]

_Then, for all \(t\geq 0\) and \(\sigma^{2}>0\),_

\[\mathbb{P}\left(\exists k\geq 0:Y_{k}\geq t\text{ and }\,W_{k}\leq\sigma^{2} \right)\leq\exp\left(-\frac{-t^{2}/2}{\sigma^{2}+Rt/3}\right)\,.\]

**Lemma 30** (Gaussian noise concentration (Lemma D.2 in [37])).: _Let \(\bm{\xi}^{(1)},\bm{\xi}^{(2)},\ldots,\bm{\xi}^{(M)}\) be \(M\) independent \(d\)-dimensional multivariate normal distributed vector with mean \(\mathbf{0}_{d}\) and covariance \(\sigma^{2}\mathbf{A}^{-1}\) for some \(\sigma>0\) and a positive definite matrix \(\mathbf{A}^{-1}\), i.e., \(\bm{\xi}^{(m)}\sim\mathcal{N}(\mathbf{0}_{d},\sigma^{2}\mathbf{A}^{-1})\) for \(m\in[M]\). Then for any \(\delta\in(0,1)\), with probability at least \(1-\delta\), we have_

\[\max_{m\in[M]}\|\bm{\xi}^{(m)}\|_{\mathbf{A}}\leq C_{\bm{\xi}}\sigma\sqrt{d \log(Md/\delta)}:=\gamma(\delta)\,,\]

_where \(C_{\bm{\xi}}\) is an absolute constant._

**Lemma 31** (Proposition 4.1 of 15).: _Let the \(w_{t+1}\) be the solution of the update rule_

\[w_{t+1}=\arg\min_{w\in\mathcal{V}}\eta\ell_{t}(w)+D_{\psi}(w,w_{t}),\]

_where \(\mathcal{V}\subseteq\mathcal{W}\subseteq\mathbb{R}^{d}\) is a non-empty convex set and \(D_{\psi}(w_{1},w_{2})=\psi(w_{1})-\psi(w_{2})-\langle\nabla\psi(w_{2}),w_{1} -w_{2}\rangle\) is the Bregman Divergence w.r.t. a strictly convex and continuously differentiable function \(\psi:\mathcal{W}\rightarrow\mathbb{R}\). Further supposing \(\psi(w)\) is \(1\)-strongly convex w.r.t. a certain norm \(\|\cdot\|\) in \(\mathcal{W}\), then there exists a \(g_{t}\in\partial\ell_{t}(w_{t+1})\) such that_

\[\langle\eta g_{t}^{\prime},w_{t+1}-u\rangle\leq\langle\nabla\psi(w_{t})- \nabla\psi(w_{t+1}),w_{t+1}-u\rangle\]

_for any \(u\in\mathcal{W}\)._

**Lemma 32**.: _Let \(\{\mathcal{F}_{t}\}_{t=1}^{\infty}\) be a filtration. Let \(\{\mathbf{z}_{t}\}_{t=1}^{\infty}\) be a stochastic process in \(\mathcal{B}_{2}(\mathcal{U})=\{\mathbf{z}\in\mathbb{R}^{\mathcal{U}}\mid\| \mathbf{z}\|_{\infty}\leq 1\}\) such that \(\mathbf{z}_{t}\) is \(\mathcal{F}_{t}\) measurable. Let \(\{\bm{\varepsilon}_{t}\}_{t=1}^{\infty}\) be a martingale difference sequence such that \(\bm{\varepsilon}_{t}\in\mathbb{R}^{\mathcal{U}}\) is \(\mathcal{F}_{t+1}\) measurable. Furthermore, assume that conditional on \(\mathcal{F}_{t}\), we have \(\|\bm{\varepsilon}_{t}\|_{1}\leq 2\) almost surely, and denote by \(\Sigma_{t}=\mathbb{E}[\bm{\varepsilon}_{t}\bm{\varepsilon}_{t}^{\top}| \mathcal{F}_{t}]\). Let \(\lambda>0\) and for any \(t\geq 1\) define_

\[U_{t}=\sum_{i=1}^{t-1}\langle\bm{\varepsilon}_{i},\mathbf{z}_{i}\rangle\quad \text{and}\quad\mathbf{B}_{t}=\lambda+\sum_{i=1}^{t-1}\|\mathbf{z}_{i}\|_{ \Sigma_{i}}^{2},\]

_Then, for any \(\delta\in(0,1]\), we have_

\[\Pr\left[\exists t\geq 1,U_{t}\geq\sqrt{\mathbf{B}_{t}}\left(\frac{\sqrt{ \lambda}}{4}+\frac{4}{\sqrt{\lambda}}\log\left(\sqrt{\frac{\mathbf{B}_{t}}{ \lambda}}\right)+\frac{4}{\sqrt{\lambda}}\log\left(\frac{2}{\delta}\right) \right)\right]\leq\delta.\]

**Lemma 33** (Lemma 1 of 76).: _Let \(\ell(\mathbf{z},y)=\sum_{k=0}^{K}\mathbf{1}\{y=k\}\cdot\log\left(\frac{1}{[ \sigma(\bm{\alpha})]_{k}}\right)\), \(\mathbf{a}\in[-C,C]^{K}\), \(y\in\{0\}\cup[K]\) and \(\mathbf{b}\in\mathbb{R}^{K}\) where \(C>0\). Then, we have_

\[\ell(\mathbf{a},y)\geq\ell(\mathbf{b},y)+\nabla\ell(\mathbf{b},y)^{\top}( \mathbf{a}-\mathbf{b})+\frac{1}{\log(K+1)+2(C+1)}(\mathbf{a}-\mathbf{b})^{\top }\nabla^{2}\ell(\mathbf{b},y)(\mathbf{a}-\mathbf{b}).\]

**Lemma 34** (Lemma 17 of 76).: _Let \(\ell(\mathbf{z},y)=\sum_{k=0}^{K}\mathbf{1}\{y=k\}\cdot\log\left(\frac{1}{[ \sigma(\bm{\alpha})]_{k}}\right)\) and \(\mathbf{z}\in\mathbb{R}^{K}\) be a \(K\)-dimensional vector. Define \(\mathbf{z}^{\mu}\triangleq\sigma^{+}\left(\mathrm{smooth}_{\mu}(\sigma(\mathbf{ z}))\right)\), where \(\mathrm{smooth}_{\mu}(\mathbf{p})=(1-\mu)\mathbf{p}+\mu\mathbf{1}/(K+1)\). Then, for \(\mu\in[0,1/2]\), we have_

\[\ell(\mathbf{z}^{\mu},y)-\ell(\mathbf{z},y)\leq 2\mu\]

_for any \(y\in\{0\}\cup[K]\). We also have \(\|\mathbf{z}^{\mu}\|_{\infty}\leq\log(K/\mu)\)._

**Lemma 35** (Lemma 18 of 76).: _Let \(L_{i,h}(\bm{\theta}):=\ell_{i,h}(\bm{\theta})+\frac{1}{2c}\|\bm{\theta}-\bm{ \theta}_{h}^{i}\|_{\mathbf{B}_{i,h}}^{2}\). Assume that \(\ell_{i,h}\) is a \(\sqrt{N}\)-self-concordant-like function. Then, for any \(\bm{\theta},\bm{\theta}_{h}^{i}\in\mathcal{B}(\mathbf{0}_{d},1)\), the quadratic approximation \(\widetilde{L}_{i,h}(\bm{\theta})=L_{i,h}(\widetilde{\bm{\theta}}_{h}^{i+1})+ \langle\nabla L_{i,h}(\widetilde{\bm{\theta}}_{h}^{i+1}),\bm{\theta}-\widetilde{ \bm{\theta}}_{h}^{i+1}\rangle+\frac{1}{2c}\left\|\bm{\theta}-\widetilde{\bm{ \theta}}_{h}^{i+1}\right\|_{\mathbf{B}_{i,h}}^{2}\) satisfies_

\[L_{i,h}(\bm{\theta})\leq\widetilde{L}_{i,h}(\bm{\theta})+\exp\left(N\left\|\bm{ \theta}-\widetilde{\bm{\theta}}_{h}^{i+1}\right\|_{2}^{2}\right)\left\|\bm{ \theta}-\widetilde{\bm{\theta}}_{h}^{i+1}\right\|_{\nabla\ell_{i,h}( \widetilde{\bm{\theta}}_{h}^{i})}^{2}.\]Limitations

We make an assumption about the transition model of MDPs by using the MNL model, which is a specific parametric model. This assumption implies that we assume the realizability of the MNL model. It's worth noting that the realizability assumption has also been commonly made in previous literature on provable reinforcement learning with function approximation, including works such as [72, 43, 73, 53, 22, 14, 9, 68, 70, 33, 81, 82, 37, 35]. However, we hope that this condition can be relaxed in the future work.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The main claims made in the abstract is to propose provably efficient randomized algorithms for MNL-MDPs. In Section 1 (Introduction), we provide the motivation and main contributions of this paper. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We discuss the limitation of this work in Appendix H. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?Answer: [Yes] Justification: We provide the full set of assumptions in Section 2.2 and a complete proof of main results in Appendix C and D. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We provide numerical experiments that support our main claims in Section 5 and the detailed information of experiments in Appendix F. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code**Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: We have attached the data and code with sufficient instructions to reproduce the main experimental results in the supplementary material. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We provide the detailed explanation for the experimental setting in Appendix F. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: We report error bars (standard deviation) in our numerical experiment results shown in Section 5. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).

* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We provide the detailed information on the computer resources used to conduct numerical experiments in Appendix F. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: The research conducted in this paper adheres to the NeurIPS Code of Ethics in all aspects. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: There is no negative societal impacts of the work performed because this research focuses on theoretical aspects. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.

* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: The research conducted in this paper does not pose any such risks. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [NA] Justification: This paper does not use any external assets such as code, data, or models. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.

* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: This paper does not release new assets. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.