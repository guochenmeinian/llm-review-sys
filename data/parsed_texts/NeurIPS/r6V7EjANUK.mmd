# GSDF: 3DGS Meets SDF for Improved Neural

Rendering and Reconstruction

Mulin Yu \({}^{1}\)1  Tao Lu\({}^{1}\)1  Linning Xu\({}^{2}\)2  Lihan Jiang\({}^{4,1}\)2  Yuanbo Xiangli\({}^{3}\)2  Bo Dai\({}^{5,1}\)

\({}^{1}\)Shanghai Artificial Intelligence Laboratory, \({}^{2}\)The Chinese University of Hong Kong,

\({}^{3}\)Cornell University, \({}^{4}\) University of Science and Technology of China, \({}^{5}\) The University of Hong Kong

Footnote 1: Equal contribution

Footnote 2: Corresponding author

###### Abstract

Representing 3D scenes from multiview images remains a core challenge in computer vision and graphics, requiring both reliable rendering and reconstruction, which often conflicts due to the mismatched prioritization of image quality over precise underlying scene geometry. Although both neural implicit surfaces and explicit Gaussian primitives have advanced with neural rendering techniques, current methods impose strict constraints on density fields or primitive shapes, which enhances the affinity for geometric reconstruction at the sacrifice of rendering quality. To address this dilemma, we introduce GSDF, a dual-branch architecture combining 3D Gaussian Splatting (3DGS) and neural Signed Distance Fields (SDF). Our approach leverages mutual guidance and joint supervision _during the training process_ to mutually enhance reconstruction and rendering. Specifically, our method guides the Gaussian primitives to locate near potential surfaces and accelerates the SDF convergence. This implicit mutual guidance ensures robustness and accuracy in both synthetic and real-world scenarios. Experimental results demonstrate that our method boosts the SDF optimization process to reconstruct more detailed geometry, while reducing floaters and blurry edge artifacts in rendering by aligning Gaussian primitives with the underlying geometry.

Figure 1: **Conceptual Illustration of GSDF. Rendering and reconstruction tasks have traditionally involved trade-offs in neural representation methods. While 3D-GS achieves high-fidelity view-dependent rendering, it often compromises on geometric accuracy. Recent approaches [13, 10] use explicit regularization to align Gaussian primitives near surfaces, but this can reduce model capacity for high-fidelity visuals. Our GSDF introduces a dual-branch framework with specialized GS- and SDF-branches for rendering and geometry tasks. We propose three mutual guidances (detailed in Sec. 3.2) to enhance the quality of both tasks.**Introduction

Recent advancements in neural scene representations have showcased superior rendering capabilities [23; 24; 17], these advancements have sparked significant interest in neural surface reconstruction [39; 31; 19; 32; 10; 13]. They seek to develop unified representations, which simultaneously support high-fidelity rendering and accurate geometric reconstruction, to better support downstream applications such as robotics [16; 27], physical simulations [35; 8], and XR applications [36; 15].

Although both neural implicit surfaces and explicit Gaussian primitives have advanced with neural rendering techniques, current methods often suffer from a mismatched prioritization between appearance and geometry. Imposing regularization or constraints on the unified representation may boost one task, while unavoidably deteriorating the performance of the other. Recent approaches [10; 5], have explored using flat Gaussian primitives for surface modeling. Enforcing binary opacity [10] and jointly learned NeuS models [5] to regularize attributes have led to degraded rendering quality due to primitive constraints. However, works such as Adaptive Shell [34], Binary Occupancy Field [26], and Scaffold-GS [21] have demonstrated that incorporating geometry guidance significantly enhances rendering quality by producing well-regularized spatial structures with hybrid representations.

Building on these insights, we propose a synchronously optimized _dual-branch_ system, addressing rendering and reconstruction with hybrid representations, to bypass the conflicts and further achieve mutual enhancements, as illustrated in Fig. 1. Our method leverages mutual guidance and joint supervision to balance rendering and reconstruction without compromising their intrinsic advantages. Specifically, our system features a 3D Gaussian Splitting (3DGS) branch for rendering and a Signed Distance Field (SDF) branch for surface reconstruction. The key innovations of our approach include: (1) Utilizing rasterized depth from the GS-branch to guide ray sampling in the SDF-branch, enhancing volume rendering efficiency and avoiding local minima. (2) Applying SDF-guidance for density control in 3DGS, directing the growth of 3D Gaussians in near-surface regions and pruning elsewhere. (3) Aligning geometry properties (depth and normal) estimated from both branches. This unified system overcomes the limitations inherent in each method due to differences in rendering techniques ( rasterization vs. dense ray sampling) and scene representation (discrete primitives vs. continuous fields). Moreover, our framework is designed to accommodate future advancements in each branch.

Extensive experiments demonstrate that our dual-branch design allows: 1) The GS-branch to generate structured primitives closely aligned with the surface, reducing floaters and improving detail and edge quality in view synthesis. 2) Accelerated convergence in the SDF-branch, resulting in superior geometric accuracy and enhanced surface details. Our results confirm that an integrated blend of both reconstruction and rendering is achievable, enhancing overall robustness and performance.

## 2 Related work

Advanced Neural Rendering Techniques.Neural Radiance Fields (NeRFs) [23] have achieved remarkable photorealistic rendering with view-dependent effects. They use Multi-Layer Perceptrons (MLPs) to map 3D spatial locations to color and density, which are then aggregated into pixel colors through neural volumetric rendering. This approach excels in novel view synthesis but is slow due to the need for extensive point sampling along each ray and the global MLP architecture's scalability limitations. Recent research [24; 37; 4; 9] has shifted the learning burden to locally optimized spatial features, enhancing scalability. Alternative methods like rasterizing geometric primitives (e.g., point clouds) [2; 41] offer efficiency and flexibility but struggle with discontinuities and outliers. Augmenting points with neural features and incorporating volumetric rendering [38] improves quality but adds computational overhead. Recently, 3D Gaussian Splatting (3DGS) [17] revolutionized neural rendering by using anisotropic 3D Gaussians as primitives, sorted by depth and rasterized onto a 2D screen with \(\alpha\)-blending. This method achieves high-quality, detailed results at real-time frame rates. Scaffold-GS [21] enhanced 3DGS by introducing a hierarchical structure that aligns anchors with scene geometry, improving rendering quality and memory efficiency. However, these methods prioritize view synthesis over accurate scene geometry, often resulting in fuzzy volumetric density fields that hinder the extraction of high-quality surfaces

Neural Surface Reconstruction.The success of neural rendering has sparked significant interest in neural surface reconstruction [25; 39; 31; 19; 32; 40]. These methods typically use coordinate-based networks to encode scene geometry through occupancy fields or Signed Distance Field (SDF) values.

While MLPs with volume rendering produce smooth and complete surfaces, they often lack high-fidelity details and are slow to optimize. Recent works [19; 34; 26] have leveraged multi-resolution hashed feature grids from iNGP [24] to enhance representation power, achieving state-of-the-art results. Hybrid approaches combining surface and volume rendering [30; 34; 26] have also emerged to maintain rendering speed and quality. More recently, methods have explored integrating 3D Gaussian Splitting for surface learning. For example, SuGaR [10] aligns 3D Gaussians with potential surfaces by approximating them to 2D planar primitives with binary opacity. Similarly, 2D Gaussian Splitting [13] replaces 3D Gaussians with 2D Gaussians for more accurate ray-splat intersection and regularizes depth maps to enforce a more condensed Gaussian primitive distribution. Another approach [7] optimizes Gaussian surfels constrained by a normal prior from a pre-trained monocular estimator, enhancing surface representation. The concurrent work NeuSG [5] jointly optimizes 3D Gaussian Splatting with NeuS [31] by encouraging flat 3D Gaussians as well, with normals aligned with NeuS predictions. 3DGSR [22] also aligns the geometry from 3DGS with a SDF field. Despite advances in neural surface reconstruction, a fidelity gap persists due to the explicit regularization of Gaussian primitives, which hampers rendering quality compared to 3DGS. Our method optimizes primitives aligned with surfaces using geometric clues, enhancing structural integrity without losing representational power.

## 3 Method

We present a dual-branch framework with GS- and SDF-branches, jointly optimized to provide mutual guidance, as shown in Fig. 2. Our approach aligns Gaussian primitives with surfaces using geometric clues rather than constraining their shapes, enhancing both structure and representational power. These improved primitives yield more accurate and detailed scene surfaces.

In Sec.3.1, we briefly review neural rendering methods[17; 21] and neural implicit surface representations [31; 19]. Sec.3.2 details our framework and the three proposed mutual guidance techniques. Sec.3.3 outlines our training strategy and loss design.

Figure 2: **Overview of Dual-branch Guidance**. Our dual-branch framework includes a GS-branch for rendering and an SDF-branch for learning neural surfaces. This design preserves the efficiency and fidelity of Gaussian primitive for rendering [17; 21] while accurately approximating scene surfaces from an SDF field adapted from NeuS [31]. Specifically: (1) The GS-branch renders depth maps to guide SDF-branch ray sampling, querying absolute SDF values \(|s|\) and sampling points within \(2k|s|\) (e.g., \(k=4\)). (2) Predicted SDF values guide GS-branch density control, growing Gaussians near surfaces and pruning deviated ones. (3) Mutual geometry consistency is enforced by comparing depth and normal maps from both branches, ensuring coherent alignment between Gaussians and surfaces.

### Preliminary

3D Gaussian Splatting.3D Gaussian Splatting (3DGS) [17] represents scenes using 3D Gaussian primitives, achieving state-of-the-art rendering quality and speed with a tile-based rasterizer. Each Gaussian is defined by its mean \(\mu\in\mathbb{R}^{3}\) and covariance \(\Sigma\in\mathbb{R}^{3\times 3}\) with \(G(x)=e^{-\frac{1}{2}(x-\mu)^{T}\Sigma^{-1}(x-\mu)}\), where \(x\) is a 3D position in the scene. The rasterizer efficiently sorts and \(\alpha\)-blends Gaussians onto the 2D image plane [43]. The adaptive control of Gaussians based on gradients is also critical for improving scene representation accuracy, reducing redundancy, and refining Gaussians to better match the underlying geometry and appearance.

Scaffold-GS [21] improves 3DGS by enhancing scene structure fidelity and robustness to view-dependent effects. It uses a hierarchical 3D Gaussian representation, with anchor points encoding local scene information and generating local neural Gaussians. Each anchor, optimized with a feature vector, predicts the color, center, variance, and opacity of the neural Gaussians.

Neural Implicit SDFs.NeuS [31] integrates signed distance functions (SDFs) with NeRF's volumetric rendering, converting SDF values into volume densities to maintain geometric accuracy. SDF values are converted to opacities using a logistic function \(\alpha_{i}=\max\left(\frac{\Phi_{s}(f(\mathbf{x}_{i}))-\Phi_{s}(f(\mathbf{x}_{ i+1}))}{\Phi_{s}(f(\mathbf{x}_{i}))},0\right).\) The color of a ray \(r\) is also calculated by accumulating weighted colors of the sample points: \(\mathbf{C}(\mathbf{r})=\sum_{i=1}^{P}T_{i}\alpha_{i}\mathbf{c}_{i}\), and \(T_{i}=\exp\left(-\sum_{j=1}^{i-1}\alpha_{j}\delta_{j}\right)\), where \(\delta_{j}\) is the interval between sampled points. Inspired by [24], [11] introduces a multi-resolution hash grid to enhance representation power and accelerate rendering and training.

### GSDF: Dual-branch for Rendering and Reconstruction

To bridge the gap between rendering and geometric accuracy, we propose a novel approach that combines the strengths of Gaussian-based and SDF-based methods. By leveraging the high-quality rendering capabilities of 3DGS and the precise geometric representation of SDFs, our method aims to achieve superior results in both tasks.

As illustrated in Fig. 2, our dual-branch design integrates a GS-branch and an SDF-branch. We use Scaffold-GS [21] and NeuS [31] with adapted hash-encoding implementation [11] as the backbones, chosen for their effectiveness and simplicity. Importantly, our framework is versatile and can be easily adapted to incorporate future advanced methods for each branch.

#### 3.2.1 Gs \(\rightarrow\) SDF: Depth Guided Ray Sampling

To address the computational expenses of ray-sampling, techniques such as hierarchical sampling [23], occupancy grids [20; 4; 24], early stopping [24], and proposal networks [3; 26] are widely used with CUDA accelerations. When depth maps are available, either from sensors or monocular estimation, samples can be strategically placed around surface regions, which is crucial for effective optimization of the Signed Distance Field (SDF). Unlike neural implicit SDF-based methods that rely on their own predicted SDF values for ray sampling [31; 19; 28], we employ the GS-branch to provide surface proximity, avoiding the chicken-and-egg dilemma. Inspired by [37], where a more efficient branch provides coarse geometry guidance, we leverage depth maps from the GS-branch to refine the ray sampling range for the SDF-branch. While Gaussian primitives may be less precise, they are efficient and flexible, offering sufficient geometric clues without significant overhead.

Concretely, for a ray emitted from a camera center \(\vec{o}\) in the direction \(\vec{v}\), the depth value \(D\) from the GS-branch is:

\[D=\sum_{i\in N}d_{i}\sigma_{i}\prod_{j=1}^{i-1}(1-\sigma_{j}),\] (1)

where \(N\) is the number of 3D Gaussians encountered, \(\sigma_{i}\) is the opacity, and \(d_{i}\) is the distance from the \(i\)-th Gaussian to the camera. For SDF-branch optimization, points are sampled around \(\vec{o}+D\cdot\vec{v}\). The sampling range adapts based on predicted SDF values \(s\) at various depths:

\[s=\mathcal{F}_{s}(\vec{o}+D\cdot\vec{v}),\] (2)

where \(\mathcal{F}_{s}\) is a two-layer MLP predicting the SDF value at a given position. The sampling range is defined as \(r=[\vec{o}+(D-k|s|)\cdot\vec{v},\vec{o}+(D+k|s|)\cdot\vec{v}]\). As shown in Fig. 2, inspired from NeRF'shierarchical sampling strategy [23], we use coarse and fine ranges with \(k=3\) and \(k=1\), respectively, we uniformly sample \(M\) points along the ray within each range.

#### 3.2.2 SDF \(\rightarrow\) GS: Geometry-aware Gaussian Density Control

Many previous and concurrent methods [13; 10; 7] have attempted to learn scene surfaces from 3DGS by flattening 3D Gaussians along the normal direction and enforcing nearly binary opacity, treating them like surface primitives similar to mesh triangles. However, this approach often leads to degraded rendering quality and incomplete surfaces. Our approach, instead of putting additional regularization on the 3D Gaussian primitives, enhances the distribution of Gaussian primitives using a geometry-aware density control strategy. Building on the original gradient-based density control, we leverage the zero-level set of the SDF-branch to determine the proximity of Gaussian primitives to the surface. By querying the SDF-branch with the positions of Gaussian primitives, we are able to identify close-to-surface Gaussian primitives (i.e. with smaller absolute SDF values), allowing for more precise control over the placement and density of Gaussians.

**Growing Operator.** For each Gaussian primitive at position \(c\), we obtain its SDF value \(s=\mathcal{F}_{s}(c)\) from the SDF-branch. The growth criteria for Gaussians are then defined as:

\[\epsilon_{g}=\nabla_{g}+\omega_{g}\mu(s),\] (3)

where \(\nabla_{g}\) is the averaged gradient of Gaussian primitives accumulated over \(K\) training iterations, as in Scaffold-GS [21]. \(\mu(s)=\exp\left(-s^{2}/(2\sigma^{2})\right)\) is a Gaussian function converting the SDF value to a positive factor, decreasing monotonically with distance from the zero level set. \(\omega_{g}\) controls the influence of geometric guidance. New Gaussian primitives are added if \(\epsilon_{g}\) are greater than a predefined \(\tau_{g}\).

**Pruning Operator.** Beyond the original opacity-based criteria, we prune Gaussian primitives far from the surface, indicated by large SDF values. The pruning criteria are:

\[\epsilon_{p}=\sigma_{a}-\omega_{p}(1-\mu(s)),\] (4)

where \(\sigma_{a}\) is the aggregated opacity over \(K\) iterations. The weight \(\omega_{p}\) balances the contributions of transparency and SDF. Primitives with \(\epsilon_{p}\) less than a predefined \(\tau_{p}\) are pruned.

#### 3.2.3 GS \(\leftrightarrow\) SDF: Mutual Geometry Supervision

To enhance both rendering and reconstruction outcomes, our framework incorporates mutual supervision between the two branches, using depth and normal maps as key geometric features to facilitate this interconnection.

For the SDF-branch, a per-view depth map \(D_{s}\) is rendered using volumetric rendering principles, and a normal map \(N_{s}\) is derived by rendering the gradients of the SDF volumetrically. For the GS-branch, we compute a per-view depth map \(D_{g}\) following Eq. 1. The normal of each 3D Gaussian is determined by the direction of its smallest scaling factor, as in [10; 5; 6]. To render the normal map for each camera view, we accumulate the normals of the 3D Gaussians using \(\alpha\)-blending \(N_{g}=\sum_{i\in N}n_{i}\sigma_{i}\prod_{j=1}^{i-1}(1-\sigma_{j})\), where \(n_{i}\) is the estimated normal of the \(i\)-th 3D Gaussian.

This mutual geometry supervision ensures that the depth and normal maps from both branches are consistently aligned. This consistent alignment is crucial for maintaining coherence between the rendering and reconstruction processes, as it helps to prevent discrepancies that could lead to artifacts and inaccuracies when performing mutual guidance. This consistency is especially important for achieving high-quality results in complex scenes, where precise geometric alignment can significantly enhance the visual and structural integrity of the output.

### Training Strategy and Loss Design

The GS-branch is supervised by rendering losses \(\mathcal{L}_{1}\) and \(\mathcal{L}_{\text{SSIM}}\), which measure the difference between the rendered RGB images and ground truth images, and a volume regularization term \(\mathcal{L}_{vol}\) as in [21]. The complete loss function for GS-branch is:

\[\mathcal{L}_{\text{g}}=\lambda_{1}\mathcal{L}_{1}+(1-\lambda_{1})\mathcal{L}_ {\text{SSIM}}+\lambda_{vol}\mathcal{L}_{vol},\] (5)where \(\lambda_{1}\) and \(\lambda_{vol}\) are weighting coefficients. Similarly, the SDF-branch is supervised by the \(\mathcal{L}_{1}\) rendering loss, supplemented with Eikonal and curvature penalties to ensure accurate geometry reconstruction. The loss function for the SDF-branch is:

\[\mathcal{L}_{\text{s}}=\mathcal{L}_{1}+\lambda_{\text{eik}}\mathcal{L}_{\text {eik}}+\lambda_{\text{curv}}\mathcal{L}_{\text{curv}},\] (6)

where \(\mathcal{L}_{\text{eik}}\) represents the Eikonal loss, ensuring that the gradients of the predicted SDF field are normalized, and \(\mathcal{L}_{\text{curv}}\) denotes the curvature loss, promoting surface smoothness as described in [19; 28]. The coefficients \(\lambda_{\text{eik}}\) and \(\lambda_{\text{curv}}\) balance the influence of these loss terms.

The mutual geometry supervision includes depth and normal consistency losses applied to both branches. For unbounded scenes with distant backgrounds, we only apply the mutual geometric supervision to foreground regions, as these geometries are more reliable and of primary interest [11]. The mutual loss is formulated as:

\[\mathcal{L}_{\text{mutual}}=\lambda_{d}\mathcal{L}_{d}+\lambda_{n}\mathcal{ L}_{n}=\lambda_{d}\|D_{gs}-D_{s}\|+\lambda_{n}\left(1-\frac{\left|N_{gs}\cdot N _{s}\right|}{\left\|N_{gs}\right\|\left\|N_{s}\right\|}\right),\] (7)

where \(\mathcal{L}_{d}\) and \(\mathcal{L}_{n}\) represent the depth and normal discrepancies between the two branches, and \(\lambda_{d}\) and \(\lambda_{n}\) balance their importance. Finally, the total loss for joint learning is defined as:

\[\mathcal{L}=\mathcal{L}_{\text{g}}+\mathcal{L}_{\text{s}}+\mathcal{L}_{\text {mutual}}.\] (8)

The hyper-parameter settings are detailed in the supplementary material.

## 4 Experiment

### Experimental Setup

**Datasets.** We evaluated results using 26 real-world and synthetic scenes from various datasets: 7 from Mip-NeRF360 [3], 2 from DeepBlending [12], 2 from Tanks&Temples [18], and 15 from DTU [14], featuring a wide range of indoor, outdoor, and object-centric scenarios.

**Implementation Details.** We implemented our dual-branch model based on 1) Scaffold-GS [21] and 2) an enhanced version of NeuS [31] with a hash-grid variant [11], following the practice of [19]. The hash grid resolution spans from \(2^{5}\) to \(2^{11}\) with 16 levels, each entry having a feature dimension of 4 and a maximum of \(2^{21}\) entries per level. The coarsest 4 layers were activated initially for the DTU [1], and 8 layers for other datasets, with finer levels added every \(2k\) iterations. We trained the GS-branch for 15k iterations, followed by joint training of both branches for 30k iterations. The SDF-branch was warmed up for 2k iterations on the DTU and 5k on other datasets without depth-guided ray sampling. Note that our system is adaptable with various existing or future rendering and reconstruction models.

**Evaluations.** We evaluated our method against state-of-the-art rendering and reconstruction approaches. For rendering comparison, we compared with Scaffold-GS [21], 3D-GS [17], NeuS [31], 2D-GS [13], and SuGaR [10] using PSNR, SSIM [33], and LPIPS [42] for quantitative comparisons. We trained 3D-GS and Scaffold-GS for 45\(k\) iterations to align with our configurations. For reconstruction, we compared with NeuS [31], Instant-NSR [11] (our representative SDF-branch), SuGaR [10] and 2D-GS [13]. We use Chamfer distance for quantitative evaluation. For all datasets, we used \(1/8\) of the images as test sets and the other \(7/8\) as training sets. All experiments were conducted on a single NVIDIA A100 GPU with 80G memory.

### Results Analysis

#### 4.2.1 Rendering Comparisons

Our GSDF retained high rendering quality compared to state-of-the-art 3D-GS and SDF-based methods, as shown in Table 1. Quantitative metrics against 3D-GS [17], 2D-GS [13], and Scaffold-GS [21] show that GSDF consistently outperformed these methods across all four datasets, showcasing improvements in all metrics. Notably, the prominent improvements in the LPIPS metric indicate that our method effectively captures high-frequency scene details and renders perceptually superior results compared to baselines.

**Enchanced Detail and Fidelity.** Notably, our method excelled in achieving _high rendering quality_ in texture-less areas, as showcased in Fig.3, which is aligned with our design motivation for geometry-aware density control. Specifically, in texture-less areas where vanilla 3D Gaussians struggled due to small accumulated gradients, our method overcame this limitation by growing anchors in surface regions which brings enhanced accuracy and scene details.

**Robustness with Noisy Gaussians.** To test the robustness, we experimented with randomly initialized Gaussian primitives on 2D-GS, Scaffold-GS, and our method. Quantitative results in Table 2 highlight the advantages of our geometric guidance, demonstrating superior performance and stability even with random input. Further visual results are provided in the supplementary material.

\begin{table}
\begin{tabular}{c|c|c c c|c c c|c c c|c c c} \hline \hline Dataset & \multicolumn{3}{c|}{MIp-NeRF360 [3]} & \multicolumn{3}{c|}{Tanks\&Temples [18]} & \multicolumn{3}{c}{Deep Blending [12]} \\ Method \& Metrics & PSNR \(\uparrow\) & SSIM \(\uparrow\) & LPIPS \(\downarrow\) & PSNR \(\uparrow\) & SSIM \(\uparrow\) & LPIPS \(\downarrow\) & PSNR \(\uparrow\) & SSIM \(\uparrow\) & LPIPS \(\downarrow\) \\ \hline
2D-GS (rand) & 26.94 & 0.793 & 0.300 & 25.16 & 0.817 & 0.275 & 28.76 & 0.892 & 0.274 \\ Scaffold-GS (rand) & 27.84 & 0.817 & 0.259 & 26.37 & 0.838 & 0.230 & 29.12 & 0.895 & 0.260 \\
**GSDF** (rand) & **28.05** & **0.830** & **0.229** & **26.55** & **0.852** & **0.197** & **29.57** & **0.899** & **0.244** \\ \hline \hline \end{tabular}
\end{table}
Table 2: **Rendering comparisons with random initialization against baselines over three benchmark scenes. Scaffold-GS (rand) [21], 2D-GS (rand) [13] and GSDF (rand) initialized the Gaussian primitives with random points.**

\begin{table}
\begin{tabular}{c|c|c c c|c c c|c c c} \hline \hline Dataset & \multicolumn{3}{c|}{MIp-NeRF360 [3]} & \multicolumn{3}{c|}{Tanks\&Temples [18]} & \multicolumn{3}{c}{Deep Blending [12]} \\ Method \& Metrics & PSNR \(\uparrow\) & SSIM \(\uparrow\) & LPIPS \(\downarrow\) & PSNR \(\uparrow\) & SSIM \(\uparrow\) & LPIPS \(\downarrow\) & PSNR \(\uparrow\) & SSIM \(\uparrow\) & LPIPS \(\downarrow\) \\ \hline
2D-GS (rand) & 26.94 & 0.793 & 0.300 & 25.16 & 0.817 & 0.275 & 28.76 & 0.892 & 0.274 \\ Scaffold-GS (rand) & 27.84 & 0.817 & 0.259 & 26.37 & 0.838 & 0.230 & 29.12 & 0.895 & 0.260 \\
**GSDF** (rand) & **28.05** & **0.830** & **0.229** & **26.55** & **0.852** & **0.197** & **29.57** & **0.899** & **0.244** \\ \hline \hline \end{tabular}
\end{table}
Table 2: **Rendering comparisons with random initialization against baselines over three benchmark scenes. Scaffold-GS (rand) [21], 2D-GS (rand) [13] and GSDF (rand) initialized the Gaussian primitives with random points.**

Figure 3: **Qualitative comparisons of GSDF against popular Gaussian-based baselines [17; 21; 13] across diverse 3D scene datasets [18; 3; 12]. As highlighted, GSDF excels in modeling delicate geometries (1st & 2nd rows) and handling texture-less and sparsely observed regions (3rd & 4th rows), which are commonly presented in larger scenes where baseline approaches struggle to address.**

#### 4.2.2 Reconstruction Comparisons

We quantitatively evaluated the reconstruction ability on the DTU dataset, where our method achieved the best Chamfer distance as shown in Table 1.

**Enhanced Geometry Accuracy and Completeness.** As illustrated in Fig.4, our method reconstructed more complete and detailed meshes compared to baseline methods. Our approach effectively bypassed local minima, preventing holes in the meshes. Notably, our method outperformed Instant-NSR, delivering smooth, continuous meshes with well-preserved high-frequency details. In contrast, meshes extracted from SuGaR [10] were non-manifold with broken topological relationships. 2D-GS [13] extracts mesh from fused depth, which leads to over-smooth geometry.

**Accelerated Convergence and Rendering.** Benefited from the GS-branch, our method optimized the SDF field significantly faster in terms of training iterations than previous methods with accurate sample placements, which often required slow and exhaustive training and rendering. For instance, NeuS [31] required about 8 hours of optimization on a single GPU for the DTU dataset, whereas our method achieved comparable or better results within just 2 hours on a single GPU.

Figure 4: **Reconstruction Comparison**. We visualize the reconstructed meshes from Instant-NSR [11] (our SDF-branch), SuGaR [10], 2D-GS [13], and Ours.

### Ablation Studies

In this section, we examine the effectiveness of each individual module. Quantitative metrics and qualitative visualizations are provided in Table 3 and Fig. 5.

**Depth-Guided Ray Sampling.** To evaluate the effectiveness of our depth-guided ray sampling detailed in Sec 3.2, we conducted an ablation on depth-guided ray sampling using the original stratified ray sampling approach [23]. The results show that this ablated setting produced overly smoothed surfaces, failing to capture finer geometry details such as the heater and doors (Fig. 5, column 2, patches 2 & 3). The resulting less accurate SDF also impacted rendering quality, making details like the sticker on the wall appear more blurred compared to our full model.

**Geometry-aware Gaussian Density Control.** We replaced our geometry-aware Gaussian density control with the pruning and growing strategy from Scafffold-GS [21] to evaluate its efficacy. As shown in Fig. 5 (3rd column), this change led to missed details like the sticker on the wall (patch 4) due to small accumulated gradients on the texture-less surface. The reconstruction results also showed the absence of thin objects such as table legs and the chandelier (patch 1), highlighting the limitations of the vanilla strategy.

**Mutual Geometric Supervision.** Lastly, we ablated the proposed mutual geometric supervision by setting both \(\lambda_{d}\) and \(\lambda_{n}\) to 0 (Eq. 7). This resulted in a significant decay in surface quality and omission of details in rendering. These findings indicate that neural rendering is more tolerant of deviations than neural surface reconstruction. Without explicitly aligning Gaussians with SDF-derived geometry during optimization, the two branches can diverge, leading to sub-optimal results for both.

### Limitations and Future Works

Currently, our method is not tailored to handle challenging scenes with reflections and intense lighting variations, such as those found in indoor environments. However, we have observed that employing more structured and surface-aligned Gaussian primitives holds promise in capturing such view-dependent appearance changes with improved scene geometry. Our framework requires more memory usage because it simultaneously considers two representations. Furthermore, the performance of our SDF-branch significantly lags behind the GS-branch, leading to extended training durations compared to Scafffold-GS [21] only. Hence, improving the efficiency of the MLP-based

Figure 5: **Ablation results**. Visualizations of reconstructed meshes and rendered images from 1) our full method, 2) ours w/o depth-guided ray sampling, 3) ours w/o geometry-aware density control, and 4) ours w/o geometric supervision. We highlight the degradation of quality using numbered patches.

\begin{table}
\begin{tabular}{c|c c c|c c c|c c} \hline \hline Dataset & \multicolumn{3}{c|}{Mip-NeRF360[3]} & \multicolumn{3}{c|}{Tanks\&Temples[18]} & \multicolumn{3}{c}{Deep Blending [12]} \\ Method \(|\) Metrics & PSNR \(\uparrow\) & SSIM \(\uparrow\) & LPIPS \(\downarrow\) & PSNR \(\uparrow\) & SSIM \(\uparrow\) & LPIPS \(\downarrow\) & PSNR \(\uparrow\) & SSIM \(\uparrow\) & LPIPS \(\downarrow\) \\ \hline GSDF (Full) & **29.38** & **0.865** & **0.185** & **27.37** & **0.875** & **0.156** & **30.38** & **0.909** & **0.223** \\ \hline w/o geometric supervision & 29.27 & 0.862 & 0.196 & 27.18 & 0.865 & 0.191 & 30.21 & 0.907 & 0.234 \\ w/o depth-guided sampling & 29.26 & 0.863 & 0.196 & 27.30 & 0.873 & 0.159 & 30.17 & 0.908 & 0.228 \\ w/o geometry-aware densification & 29.29 & 0.863 & 0.196 & 27.29 & 0.870 & 0.179 & 30.25 & 0.908 & 0.236 \\ \hline \hline \end{tabular}
\end{table}
Table 3: **Quantitative Results on Ablation Studies. We separately listed the rendering metrics for each ablation described in Sec. 4.3.**SDF-branch is a crucial direction for future research. Still, training time is not the highest priority compared to inference, where primitives are stored and can be accessed efficiently.

## 5 Conclusion

In this work, we introduced a dual-branch framework that leverages the strengths of both 3D-GS and SDF, showcasing its potential to achieve both enhanced rendering and reconstruction quality. The inherent differences in two representations, rendering approaches, and supervision loss pose a challenge to the seamless integration. We therefore integrate a bidirectional mutual guidance approach during the training to circumvent these restrictions. Three types of guidance have been introduced and validated in our framework, namely: 1) depth guided sampling (GS\(\rightarrow\)SDF), 2) geometry-aware Gaussian density control (SDF\(\rightarrow\)GS); and 3) mutual geometry supervision (GS\(\leftrightarrow\)SDF). Our extensive results demonstrate the efficiency and joint performance improvement on both tasks. As the two branches maintain their original architectures, we keep their efficiency during inference, allowing room for potential enhancements by substituting each branch with more advanced models in the future. We envision our model benefiting applications demanding high-quality rendering and geometry, including embodied environments, physical simulation, and immersive VR experiences.

## 6 Acknowledgements

This work is funded in part by the National Key R&D Program of China (2022ZD0160201), and Shanghai Artificial Intelligence Laboratory.

## References

* [1] Henrik Aanaes, Rasmus Ramsbol Jensen, George Vogiatzis, Engin Tola, and Anders Bjorholm Dahl. Large-scale data for multiple-view stereopsis. _International Journal of Computer Vision_, pages 1-16, 2016.
* [2] Kara-Ali Aliev, Artem Sevastopolsky, Maria Kolos, Dmitry Ulyanov, and Victor Lempitsky. Neural point-based graphics. In _Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part XXII 16_, pages 696-712. Springer, 2020.
* [3] Jonathan T Barron, Ben Mildenhall, Dor Verbin, Pratul P Srinivasan, and Peter Hedman. Mipnerf 360: Unbounded anti-aliased neural radiance fields. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 5470-5479, 2022.
* [4] Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, and Hao Su. TensoRF: Tensorial radiance fields. 2022.
* [5] Hanlin Chen, Chen Li, and Gim Hee Lee. Neusg: Neural implicit surface reconstruction with 3d gaussian splatting guidance. _arXiv preprint arXiv:2312.00846_, 2023.
* [6] Kai Cheng, Xiaoxiao Long, Kaizhi Yang, Yao Yao, Wei Yin, Yuexin Ma, Wenping Wang, and Xuejin Chen. Gaussianpro: 3d gaussian splatting with progressive propagation. _arXiv preprint arXiv:_, 2024.
* [7] Pinxuan Dai, Jiamin Xu, Wenxiang Xie, Xinguo Liu, Huamin Wang, and Weiwei Xu. High-quality surface reconstruction using gaussian surfels. 2024.
* [8] Yutao Feng, Xiang Feng, Yintong Shang, Ying Jiang, Chang Yu, Zeshun Zong, Tianjia Shao, Hongzhi Wu, Kun Zhou, Chenfanfu Jiang, et al. Gaussian splashing: Dynamic fluid synthesis with gaussian splatting. _arXiv preprint arXiv:2401.15318_, 2024.
* [9] Sara Fridovich-Keil, Alex Yu, Matthew Tancik, Qinhong Chen, Benjamin Recht, and Angjoo Kanazawa. Plenoxels: Radiance fields without neural networks. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 5501-5510, 2022.
* [10] Antoine Guedon and Vincent Lepetit. Sugar: Surface-aligned gaussian splatting for efficient 3d mesh reconstruction and high-quality mesh rendering. _arXiv preprint arXiv:2311.12775_, 2023.

* Guo [2022] Yuan-Chen Guo. Instant neural surface reconstruction, 2022. https://github.com/bennyguo/instant-nsr-pl.
* Hedman et al. [2018] Peter Hedman, Julien Philip, True Price, Jan-Michael Frahm, George Drettakis, and Gabriel Brostow. Deep blending for free-viewpoint image-based rendering. 37(6):257:1-257:15, 2018.
* Huang et al. [2024] Binbin Huang, Zehao Yu, Anpei Chen, Andreas Geiger, and Shenghua Gao. 2d gaussian splatting for geometrically accurate radiance fields. _ArXiv_, abs/2403.17888, 2024.
* Jensen et al. [2014] Rasmus Jensen, Anders Dahl, George Vogiatzis, Engin Tola, and Henrik Aanaes. Large scale multi-view stereopsis evaluation. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 406-413, 2014.
* Jiang et al. [2024] Ying Jiang, Chang Yu, Tianyi Xie, Xuan Li, Yutao Feng, Huamin Wang, Minchen Li, Henry Lau, Feng Gao, Yin Yang, et al. Vr-gs: A physical dynamics-aware interactive gaussian splatting system in virtual reality. _arXiv preprint arXiv:2401.16663_, 2024.
* Keetha et al. [2023] Nikhil Keetha, Jay Karhade, Krishna Murthy Jatavallabhula, Gengshan Yang, Sebastian Scherer, Deva Ramanan, and Jonathon Luiten. Splatam: Splat, track & map 3d gaussians for dense rgb-d slam. _arXiv preprint arXiv:2312.02126_, 2023.
* Kerbl et al. [2023] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuhler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. _ACM Transactions on Graphics_, 42(4), 2023.
* Knapitsch et al. [2017] Arno Knapitsch, Jaesik Park, Qian-Yi Zhou, and Vladlen Koltun. Tanks and temples: Benchmarking large-scale scene reconstruction. _ACM Transactions on Graphics_, 36(4), 2017.
* Li et al. [2023] Zhaoshuo Li, Thomas Muller, Alex Evans, Russell H Taylor, Mathias Unberath, Ming-Yu Liu, and Chen-Hsuan Lin. Neuralangelo: High-fidelity neural surface reconstruction. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 8456-8465, 2023.
* Liu et al. [2020] Lingjie Liu, Jiatao Gu, Kyaw Zaw Lin, Tat-Seng Chua, and Christian Theobalt. Neural sparse voxel fields. _Advances in Neural Information Processing Systems_, 33:15651-15663, 2020.
* Lu et al. [2023] Tao Lu, Mulin Yu, Liming Xu, Yuanbo Xiangli, Limin Wang, Dahua Lin, and Bo Dai. Scaffolds: Structured 3d gaussians for view-adaptive rendering, 2023.
* Lyu et al. [2024] Xiaoyang Lyu, Yang-Tian Sun, Yi-Hua Huang, Xiuzhe Wu, Ziyi Yang, Yilun Chen, Jiangmiao Pang, and Xiaojuan Qi. 3dgsr: Implicit surface reconstruction with 3d gaussian splatting. _arXiv preprint arXiv:2404.00409_, 2024.
* Mildenhall et al. [2020] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. In _ECCV_, 2020.
* Muller et al. [2022] Thomas Muller, Alex Evans, Christoph Schied, and Alexander Keller. Instant neural graphics primitives with a multiresolution hash encoding. _ACM Transactions on Graphics (ToG)_, 41(4):1-15, 2022.
* Oechsle et al. [2021] Michael Oechsle, Songyou Peng, and Andreas Geiger. Unisurf: Unifying neural implicit surfaces and radiance fields for multi-view reconstruction. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 5589-5599, 2021.
* Reiser et al. [2024] Christian Reiser, Stephan Garbin, Pratul P Srinivasan, Dor Verbin, Richard Szeliski, Ben Mildenhall, Jonathan T Barron, Peter Hedman, and Andreas Geiger. Binary opacity grids: Capturing fine geometric detail for mesh-based view synthesis. _arXiv preprint arXiv:2402.12377_, 2024.
* Rosinol et al. [2023] Antoni Rosinol, John J Leonard, and Luca Carlone. Nerf-slam: Real-time dense monocular slam with neural radiance fields. In _2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)_, pages 3437-3444. IEEE, 2023.

* [28] Radu Alexandru Rosu and Sven Behnke. Permutosdf: Fast multi-view reconstruction with implicit surfaces using permutohedral lattices. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 8466-8475, 2023.
* [29] Johannes L Schonberger and Jan-Michael Frahm. Structure-from-motion revisited. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 4104-4113, 2016.
* [30] Haithem Turki, Vasu Agrawal, Samuel Rota Bulo, Lorenzo Porzi, Peter Kontschieder, Deva Ramanan, Michael Zollhofer, and Christian Richardt. Hybriderf: Efficient neural rendering via adaptive volumetric surfaces. _arXiv preprint arXiv:2312.03160_, 2023.
* [31] Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku Komura, and Wenping Wang. Neus: Learning neural implicit surfaces by volume rendering for multi-view reconstruction. _arXiv preprint arXiv:2106.10689_, 2021.
* [32] Yiming Wang, Qin Han, Marc Habermann, Kostas Daniilidis, Christian Theobalt, and Lingjie Liu. Neus2: Fast learning of neural implicit surfaces for multi-view reconstruction. _2023 IEEE/CVF International Conference on Computer Vision (ICCV)_, pages 3272-3283, 2022.
* [33] Zhou Wang, A.C. Bovik, H.R. Sheikh, and E.P. Simoncelli. Image quality assessment: from error visibility to structural similarity. _IEEE Transactions on Image Processing_, 13(4):600-612, 2004.
* [34] Zian Wang, Tianchang Shen, Merlin Nimier-David, Nicholas Sharp, Jun Gao, Alexander Keller, Sanja Fidler, Thomas Muller, and Zan Gojcic. Adaptive shells for efficient neural radiance field rendering. _arXiv preprint arXiv:2311.10091_, 2023.
* [35] Tianyi Xie, Zeshun Zong, Yuxin Qiu, Xuan Li, Yutao Feng, Yin Yang, and Chenfanfu Jiang. Physgaussian: Physics-integrated 3d gaussians for generative dynamics. _arXiv preprint arXiv:2311.12198_, 2023.
* [36] Linning Xu, Vasu Agrawal, William Laney, Tony Garcia, Aayush Bansal, Changil Kim, Samuel Rota Bulo, Lorenzo Porzi, Peter Kontschieder, Aljaz Bozic, et al. Vr-nerf: High-fidelity virtualized walkable spaces. In _SIGGRAPH Asia 2023 Conference Papers_, pages 1-12, 2023.
* [37] Linning Xu, Yuanbo Xiangli, Sida Peng, Xingang Pan, Nanxuan Zhao, Christian Theobalt, Bo Dai, and Dahua Lin. Grid-guided neural radiance fields for large urban scenes. 2023.
* [38] Qiangeng Xu, Zexiang Xu, Julien Philip, Sai Bi, Zhixin Shu, Kalyan Sunkavalli, and Ulrich Neumann. Point-nerf: Point-based neural radiance fields. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 5438-5448, 2022.
* [39] Lior Yariv, Jiatao Gu, Yoni Kasten, and Yaron Lipman. Volume rendering of neural implicit surfaces. _Advances in Neural Information Processing Systems_, 34:4805-4815, 2021.
* [40] Lior Yariv, Peter Hedman, Christian Reiser, Dor Verbin, Pratul P Srinivasan, Richard Szeliski, Jonathan T Barron, and Ben Mildenhall. Bakedsf: Meshing neural sdfs for real-time view synthesis. _arXiv preprint arXiv:2302.14859_, 2023.
* [41] Wang Yifan, Felice Serena, Shihao Wu, Cengiz Oztireli, and Olga Sorkine-Hornung. Differentiable surface splatting for point-based geometry processing. _ACM Transactions on Graphics (TOG)_, 38(6):1-14, 2019.
* [42] Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, June 2018.
* [43] Matthias Zwicker, Hanspeter Pfister, Jeroen Van Baar, and Markus Gross. Ewa volume splatting. In _Proceedings Visualization, 2001. VIS'01._, pages 29-538. IEEE, 2001.

Supplementary Material

The following sections are organized as follows: 1) The first section elaborates on the implementation details of _GSDF_, covering hyper-parameters and curvature loss. 2) We then show additional experimental results. 3) Lastly, we also delve into limitations and future directions. More results and video demos can be found in supplementary webpage.

### Implementation details

#### a.1.1 Configurations.

Below, we enumerate the hyper-parameters used in our experiments:

* The variance \(\sigma^{2}\) for the Gaussian function in Eq. 3 and 4 is set to 0.005.
* For the rendering loss discussed in Sec. 3.3, we set \(\lambda_{1}=0.2\) and \(\lambda_{vol}=0.01\), in consistency with the configurations specified in Scaffold-GS [21].
* For the SDF-branch, we set \(\lambda_{eik}=0.1\) and implement an adaptive scheme for \(\lambda_{curv}\). Specifically, \(\lambda_{curv}\) increases linearly from \(0\) to \(1\) (\(0.5\) on DTU) over the first \(2000\) iterations, after which it remains at \(0.05\) (\(0.01\) on DTU) for subsequent iterations. This strategy is based on our observation that increasing the weight of the curvature loss significantly encourages the convergence of the SDF to a geometric outline.
* For the mutual geometry loss, we typically assign \(\lambda_{d}=0.5\) and \(\lambda_{n}=0.01\).

#### a.1.2 Curvature Loss.

While Instant-NSR [11] derives the curvature loss from a discrete Laplacian, we follow a more robust and explicit method as described in PermutoSDF [28]. For any given point, we randomly perturb it within the tangent plane orthogonal to its normal. The curvature loss is then measured by the cosine similarity of normals between the original point and its perturbed counterpart. Note that, we applied the same curvature loss for both Instant-NSR and GSDF.

### More experiments

#### a.2.1 Reconstruction with 500\(k\) iterations.

As discussed in Sec. 4, optimizing the SDF is a time-intensive process. While our GS-branch assists in the convergence of the SDF-branch, we conjecture that further improvements could be made through additional iterations. Hence, we train our method for 500\(k\) iterations. As shown in Fig.6, our method achieves superior reconstruction quality compared to Instant-NSR [11] (our SDF-branch) with the same training iterations.

#### a.2.2 Rendering comparison with random initialization.

In Sec. 3.2, we discussed how the SDF-branch can improve the rendering quality of the GS-branch. This enhancement is particularly noticeable when Gaussian primitives are randomly initialized, as shown in Fig. 7 and Fig. 8.

#### a.2.3 Per-scene Results.

We list the per-scene quality metrics (PSNR, SSIM, LPIPS, and Chamfer Distance) used in our rendering and reconstruction evaluation in Sec. 4 for all considered methods, as shown from Tab. 4 to Tab. 6.

[MISSING_PAGE_EMPTY:14]

\begin{table}
\begin{tabular}{c|c c c|c c c|c c c|c c c} \hline \hline Dataset & \multicolumn{4}{c|}{Tanks\&Temple [18]} & \multicolumn{4}{c}{Deep Blending [12]} \\ \hline Method \& Metrics & \multicolumn{3}{c|}{Barn} & \multicolumn{3}{c|}{Track} & \multicolumn{3}{c|}{Dr Johnson} & \multicolumn{3}{c}{Plavroom} \\  & PSNR \(\uparrow\) & SSIM \(\uparrow\) & LPIPS \(\downarrow\) & PSNR \(\uparrow\) & SSIM \(\uparrow\) & LPIPS \(\downarrow\) & PSNR \(\uparrow\) & SSIM \(\uparrow\) & LPIPS \(\downarrow\) & PSNR \(\uparrow\) & SSIM \(\uparrow\) & LPIPS \(\downarrow\) \\ \hline Instant-NSR [11] & 20.86 & 0.600 & 0.475 & 17.87 & 0.622 & 0.514 & 24.59 & 0.791 & 0.476 & 23.42 & 0.799 & 0.448 \\ \hline
3D-GS [17] & 28.21 & 0.855 & 0.223 & 25.24 & 0.861 & 0.197 & 20.94 & 0.897 & 0.248 & 29.84 & 0.900 & 0.248 \\ Scaffold-GS [21] & 28.72 & 0.869 & 0.192 & 25.80 & 0.869 & 0.172 & 29.86 & 0.908 & 0.236 & 0.899 & 0.908 & 0.239 \\ Scaffold-GS (4) & 27.82 & 0.832 & 0.243 & 24.93 & 0.845 & 0.217 & 25.85 & 0.892 & 0.262 & 29.66 & 0.889 & 0.258 \\
2D-GS (13) & 25.74 & 0.828 & 0.267 & 24.75 & 0.852 & 0.224 & 28.65 & 0.959 & 0.262 & 29.88 & 0.899 & 0.260 \\
2D-GS (and) & 26.48 & 0.810 & 0.288 & 23.84 & 0.824 & 0.262 & 28.13 & 0.889 & 0.276 & 29.40 & 0.895 & 0.273 \\ SuGAR [10] & 26.67 & 0.837 & 0.233 & 22.69 & 0.817 & 0.227 & 28.77 & 0.889 & 0.271 & 30.30 & 0.900 & 0.258 \\ \hline GSDF & **289.39** & **0.877** & **0.126** & **25.81** & **0.873** & **0.135** & **29.87** & **0.909** & **0.225** & **3.089** & **0.099** & **0.222** \\ GSDF (rand) & 28.11 & 0.551 & 0.210 & 24.99 & 0.853 & 0.184 & 29.09 & 0.898 & 0.247 & 30.04 & 0.899 & 0.241 \\ \hline \hline \end{tabular}
\end{table}
Table 6: Quantitative results for Tanks&Temples [18] and Deep Blending [12] scenes.

Figure 6: **Reconstruction comparison with \(500\)k training iterations**.

Figure 7: **Rendering Comparison with random initialization (Part 1)**. We compare rendering results between Scaffold-GS [21] and our method. The highlighted patches indicate that our method is superior in expressing finer details in both geometry and appearance.

Figure 8: **Rendering Comparison with random initialization (Part 2) - Cont**.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: We introduce GSDF, a dual-branch structure combining the strengths of 3D-GS and SDF, for improving both rendering and reconstruction. Please see our abstract and introduction for further details. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: Our framework still struggles with the dilemma like most SDF-based reconstruction methods. Please see the limitation section (Sec. 4.4) for further details. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?Answer: [Yes] Justification: In the method and experiment sections, we provide a detailed discussion and substantial evidence to demonstrate each result. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: GSDF is a robust framework that comprises a GS-branch dedicated to rendering and an SDF-branch focusing on learning neural surfaces. By leveraging the mutual guidance between these two branches, our method achieves state-of-the-art performance on both rendering and reconstruction tasks. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.

5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [No] Justification: We will release the code after acceptance. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: In the Experimental Setup section (Sec. 4.1) and Implementation details section (Sec. A.1), we provide comprehensive details, including the data splits and hyperparameters used in our experiments. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [No] Justification: The experimental result for redering and reconstruction tasks from dense views are very stable, the standard deviation is negligible, so we follow the previous work's practice and not report the error bar. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.

* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: In Experiment section (Sec. 4), We provide sufficient information on the computer resources, including the type of compute workers GPU and training time. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We have carefully reviewed the NeurIPS Code of Ethics and are committed to strictly adhering to its guidelines. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: Neural rendering and surface reconstruction are crucial tasks with important downstream applications in areas like robotics, physical simulations, and augmented reality. We believe our GSDF system has the potential to advance progress in this domain and support the development of these applications.

Guidelines:

* The answer NA means that there is no societal impact of the work performed.
* If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).

11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: There is no such risks in the rendering and reconstruction tasks. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: In the Experimental Setup section (Sec. 4.1), we cite each used code package and public datasets. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset.

* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [No] Justification: We plan to release the code upon acceptance of the paper. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: This paper does not involve any crowdsourcing or research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: This paper does not involve any crowdsourcing or research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.

* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.