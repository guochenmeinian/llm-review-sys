# Parsel \(\mathbb{C}\): Algorithmic Reasoning with Language Models by Composing Decompositions

 Eric Zelikman, Qian Huang, Gabriel Poesia, Noah D. Goodman, Nick Haber

Stanford University

{ezelikman, qhwang, poesia, ngoodman, nhaber}@stanford.edu

###### Abstract

Despite recent success in large language model (LLM) reasoning, LLMs struggle with hierarchical multi-step reasoning tasks like generating complex programs. For these tasks, humans often start with a high-level algorithmic design and implement each part gradually. We introduce Parsel, a framework enabling automatic implementation and validation of complex algorithms with code LLMs. With Parsel, we automatically decompose algorithmic tasks into hierarchical natural language function descriptions and then search over combinations of possible function implementations using tests. We show that Parsel can be used across domains requiring hierarchical reasoning, including program synthesis and robotic planning. We find that, using Parsel, LLMs solve more competition-level problems in the APPS dataset, resulting in pass rates over 75% higher than prior results from directly sampling AlphaCode and Codex, while often using a smaller sample budget. Moreover, with automatically generated tests, we find that Parsel can improve the state-of-the-art pass@1 performance on HumanEval from 67% to 85%. We also find that LLM-generated robotic plans using Parsel are more than twice as likely to be considered accurate than directly generated plans. Lastly, we explore how Parsel addresses LLM limitations and discuss how Parsel may be useful for human programmers. We release our code at https://github.com/ezelikman/parsel.

## 1 Introduction

To a language model for code (as for a human), each new token is a new chance to break the program. Chen et al. [12] highlighted this issue in a simple synthetic experiment: when asked to generate a program with a series of simple string transformations, the performance of their code language model, Codex, drops dramatically with the number of steps. As they pointed out, a human who can implement a few building blocks should be able to compose these blocks with arbitrary length.

Unlike other token generators, human programmers have (mostly) learned to break down complex tasks into manageable parts that work alone (i.e., are modular) and work together (i.e., are compositional). And, when human-generated tokens cause functions to break, the functions can ideally be rewritten independently of the rest of the program. In contrast, naively, we expect code language models to generate token sequences that are correct in their entirety. Motivated by this, we study how to leverage language models to decompose problems and assemble their compositional solutions.

We introduce Parsel, a framework that enables the decomposition, implementation, then composition of complex programs from natural language. The process consists of three main steps: 1) A language model generates a Parsel program by decomposing a natural language task into natural language function descriptions. 2) For each function description, a language model generates modular implementations. 3) The Parsel synthesizer then builds up programs by testing minimal, combinatorial groups of implementations against sets of constraints (e.g. input-output examples).

Thus, generating and implementing Parsel mirrors a pattern in human reasoning - decomposing an abstract plan until it can be solved automatically [55] - and this compositional structure also benefits language models. We show that LLMs can generate Parsel with only few-shot prompting. Using their solutions as input to the Parsel synthesizer, we outperform prior work on competition-level problems in the APPS dataset [27], including AlphaCode [35] and both versions of Codex [12; 11]. We also show that with GPT-4 [44], not only can we generate Parsel _zero-shot_ just by describing its format, but this allows it to substantially outperform GPT-4 alone on HumanEval [12], with and without generated tests. By automatically generating and implementing Parsel, LLMs also propose robotic plans that are more accurate than in prior work on language models as robotic planners [28]. Broadly, we formulate Parsel as a general-purpose algorithmic reasoning framework.

## 2 Methods

In this section, we provide an overview of the Parsel framework. First, we show how programs can be specified in Parsel. Afterward, we explain how language models can generate Parsel from task descriptions. Finally, we present the Parsel synthesizer that allows us to actually implement Parsel programs into runnable code. Specifically, we further discuss how we handle various challenges in solving the constraints, from recursion to test underspecification. There are several steps to synthesizing a Parsel program, which we explicate in this section. We visualize the details in Fig.1 and provide a high-level pseudocode in Fig. A.11.

With Parsel, we balance two challenges. On one hand, function implementations must be modular to fully leverage combinatoriality. By requiring function signatures and descriptions to be specified (or generated), we ensure functions can call others without specific implementation information. On the other hand, it must be possible to avoid combinatorial explosions. By factoring the dependency graph into strongly-connected components (SCCs), we ensure the complexity grows exponentially only with the size of the largest strongly-connected component but not the total number of functions.

### Specifying Parsel Programs

To formally specify a high-level algorithmic design, we develop a simple intermediate language, also called Parsel. It is designed to be accessible to programmers, code LLMs, and students, as discussed further in Appendix A, and inspired by many works (Section 4). In Parsel, each line contains a function description, a constraint, or a reference to a description. They all obey scoping rules, with some nuances per target language. In short: **descriptions** specify what a function does in natural language, optionally preceded by a function name and its arguments (if not included, they are inferred); **references** indicate to the language model that a function should call another function described elsewhere; **constraints** validate that a function is implemented correctly (if not provided, they may be generated automatically using the language model). We elaborate on syntactic details in Appendix R. We provide some examples of Parsel programs in Figures 2 and 3. For example, in Figure 2, task_plan(): return a list of strings... represents a description, while the call to collatz_recursion by recursion_rule is a reference.

Figure 1: **Parsel overview**. A human or LLM writes a task decomposition (in Parsel), which is split into its strongly-connected components (SCC), and then the Parsel synthesizer uses a code LLM and a constraint solver to implement and compose each SCC. When all functions have constraints and there is no recursion, each function is its own SCC. We provide a more detailed figure in Appendix H.

### Generating Parsel Programs

We use a language model to generate Parsel programs from arbitrary natural language task descriptions as a few-shot translation task (or zero-shot with GPT4 [44]). Explicitly writing out a preliminary plan can be helpful, so we first ask the model to (zero-shot) generate a plan in natural language by thinking step-by-step (motivated by Kojima et al. [32]) and then prompt the model to translate the plan to Parsel. These steps correspond to the first and second arrows in Figure 3, respectively. In some domains, in particular for robotic planning, we find that this intermediate step is not necessary (while increasing cost) and can ask the language model to directly translate the task description to a Parsel solution.

### Implementing Parsel Programs

Given the Parsel program, a core technical challenge is how to implement it in a modular way (to fully leverage the advantage of decomposition). Here we propose the Parsel synthesizer: At a high level, to implement Parsel programs with the Parsel synthesizer, we first use a language model to generate a set of candidate implementations of each of the functions based on their descriptions and then search over minimal sets of combinations of the functions to find ones that satisfy the provided constraints. The step described in this section corresponds to the rightmost arrow in Figure 3.

#### 2.3.1 Implementing Functions

A function is implemented by first aggregating descriptions and function signatures of its (potentially zero) children to form an LLM prompt (for Python, we generate a prompt as if the child functions are imported and use their descriptions as comments). Crucially, this facilitates easily changing child implementations. A code LLM is then queried using the description's text as a docstring and the description's function name and arguments for the signature; full prompts shown in Appendix K.

#### 2.3.2 Searching Over Function Combinations

**Sequential case.** The most straightforward case is when all functions have constraints (e.g., unit tests), and no functions have recursive dependencies (e.g., Fig. A.9). We start by considering this case. This defines a clear topological order on functions so they can be implemented sequentially. In this situation, Parsel implements functions with post-order traversal from function at the root, generating implementations and finding one passing the specified constraints for each function. In

Figure 2: Parsel to Python

Figure 1: Parsel to VirtualHome (robotic planning)other words, without any cycles in the call graph, we can start by implementing the leaf functions first, then their parents, etc., until the program is implemented. However, in practice, many programs have more complex structures and constraints.

**Recursion.** Permitting mutual recursion (e.g. a function \(f\) that depends on \(g\) while \(g\) also calls \(f\)) introduces the need for joint implementation of functions. However, naively considering all possible implementations is intractable for large programs. Specifically, the number of possible implementations of a program with \(k\) functions and \(n\) implementations per function is \(O(n^{k})\), exponential in the number of functions. We propose a more efficient solution, inspired by Cocke and Kennedy [16]. We reference the function call graph, identify its strongly-connected components (SCCs), and for each SCC consider all possible sets of function implementations until one set satisfies all constraints. For example, for possible implementations of functions \(f,g,h\) forming an SCC of a call graph, with \(I(f)\) corresponding to the language-model-generated implementations of \(f\), we evaluate uniformly random samples without replacement from \(I(f)\times I(g)\times I(h)\).

**Functions without constraints.** For functions with no constraints, we can conveniently use the same approach as above by reformulating the call graph as a "test dependency graph" and adding an edge from a test-less child to its parent. That is, if a function has no constraints, it depends on its parents to enforce constraints on its implementations. This could also allow us to automatically introduce new children via automatic decomposition without needing to also generate constraints for those children (see Subsection R.3). Alternatively, we support automatic test generation, prompting the language model to generate tests. For generated tests, we select the best implementation based on CodeT score [11], as some generated tests could be incorrect. In practice, we use the first (dependency) approach for inner functions and the second (test generation) approach when the top-level function has no tests. Note an opposite extreme from the sequential case: if no function or only the top-level function has constraints, we simply implement each function some number of times and then test all combinations. More generally, if \(n\) functions with \(m\) implementations where the sizes of the SCCs are at most \(c\), partitioning reduces necessary evaluations from O(\(n^{m}\)) to O(\(\frac{n^{c}m}{c}\)).

## 3 Experiments

We explore the ability of Parsel to generate programs for various tasks, including competitive coding, a standard coding benchmark, and robotic task planning. These three categories represent related but distinct kinds of algorithmic reasoning, with varying levels of abstractness and generalizability. Competitive programming represents one of the few benchmarks for evaluating code generation tools that benefits greatly from decomposition.1 By evaluating Parsel on this breadth of tasks, we hope to better understand its generality as a framework for algorithmic reasoning.

Footnote 1: Unfortunately, benchmarks that evaluate language models on complex real-world programming tasks requiring thousands of lines of code do not currently exist. Developing such a dataset is an important direction for future work.

Figure 3: **Parsel pipeline. Overview of the full Parsel pipeline for an APPS example. Each step is performed by a language model. We include the complete task, sketch, and program in Appendix S.**

### Python Code Generation

**Solving competition-level problems.** We evaluated Parsel on the competition-level subset of the APPS [27] dataset as follows: first, we zero-shot prompt GPT-3 (text-davinci-002) with the problem statement, requiring it to first propose a high-level solution and explain why it is correct, asking it to "think step by step to come up with a clever algorithm" [32]. We then prompt Codex [12] to translate the generated solution into Parsel code, providing three examples of valid code. We then attempt to synthesize the Parsel code and evaluate implementations to measure their pass rate, i.e., the proportion of solved problems. We report "pass@\(n\times k\)", where \(n\) is the number of Parsel (i.e. intermediate, semi-formal) programs generated and \(k\) is the number of implementations per function. Prior work has emphasized performance as a function of the LLM sample/inference budget measured in complete programs generated [12; 35]; for Parsel, we use the effective number of complete programs generated from the LLM for comparison, which is \(n\times k\). We compare to directly generating solutions from Codex [12; 11] and AlphaCode [35]. We visualize the pass rate in Figure 4, finding that Parsel substantially improves over prior work [11], from 14.5% to 25.5%. We include full prompts and further evaluations in Appendix N and discuss the evaluation runtime costs in depth in Appendix T.8.

Since Parsel will "mix-and-match" function implementations, the number of distinct candidate programs available to test grows exponentially with \(k\), while the generation cost for the components is linear. Generating each program likely requires on the order of petaFLOPs of compute [21], dwarfing the computational cost of testing complete programs. Yet when few functions have constraints (as in this evaluation) or there are complex recursive dependencies, it could become expensive to exhaustively evaluate all possible programs. Thus, we also explore the performance of Parsel as a function of the number of evaluations, as shown in Fig. 4. We find that the performance improves log-linearly with the number of evaluations, justifying spending compute to evaluate all programs implied by generated function implementations. One can interpret this to suggest the number of Parsel programs and combined implementations play similar and important roles. Thus, one may compensate for implementation ability with Parsel-generation ability and vice-versa (to a limit).

**Ablating the Parsel synthesizer.** We perform an ablation to explore the impact of the Parsel intermediate language: we provide the same high-level plans to Codex, but instead of translating them to Parsel and synthesizing programs as decribed, we translate them to Python directly. We match the code generation budget (effectively complete programs inferenced), generating 16 Python implementations2 per high-level plan on 100 randomly sampled problems, and find that the pass@\(8\times 16\)

Figure 4: **Left: Competition-level problem pass rate. Comparison of Parsel’s pass rate on competition-level APPS problems [27] against direct code generation with Codex [12; 11] and AlphaCode [35] Labels correspond to pass@\(n\) and pass@\(n\times k\). Sample budget is measured in effective number of complete programs generated by the code LLM. (These measures are further explained in Subsection 3.1.) Right: Pass rate vs number of evaluations. Parsel generates and evaluates many programs with a small inference budget, by combinatorial composition. Though evaluation is cheap compared to generation, we examine to understand the effect of evaluation number. For evaluation budget analysis, we evaluate pass@\(8\times 16\) on a random 200-problem competition-level APPS subset, and sample combinations of function implementations at random for a given budget.**performance drops to 6% from 25.5% for the full pipeline. Not only did this approach solve substantially fewer problems, but the problems that it did solve were a strict subset of those which we were able to solve with Parsel. This indicates that for Parsel with Codex, the step-by-step decomposition provided by the high-level plan is not by itself responsible for the observed improvements by Parsel.

We additionally perform an ablation to understand how Parsel's performance without a high-level plan. On a 200-problem competition-level APPS sample, we found that, without a high-level plan, the accuracy falls to 13% (from 25.5%, with the same configuration). This is better than the ablation where Parsel wasn't used, suggesting that Parsel plays a larger role than the high-level plan on APPS, but both are necessary. Note part of this improvement is from the challenge of prompting Codex to generate Parsel, given long APPS problem statements.

**HumanEval.** We next tested Parsel on HumanEval[12]. Because these problems are substantially simpler, they provide an opportunity to evaluate pass@1 by generating tests automatically. Furthermore, this smaller and simpler set of problems enables us to assess the (much more costly) GPT-4. We found a significant improvement over the state-of-the-art pass@1 performance--from 67% to 85%. We found that by describing the Parsel format (see Appendix O for details), noting the role of indentation in providing hierarchy and explaining the format of a description, GPT-4 was able to translate natural language plans into Parsel zero-shot. Moreover, the combination of Parsel and GPT-4 surpassed GPT-4 alone on HumanEval, both with and without generated tests, further emphasizing the importance of leveraging intermediate languages like Parsel for improved code generation performance in complex tasks.

The 85.1% pass rate with generated tests, evaluated based on the CodeT score [11], was obtained by sampling until three Parsel programs had one implementation passing a generated test and then selecting the one passing the most tests. On average, this required 3.5 Parsel programs per problem, with 8 implementations each (i.e. \(<32\) complete implementations on average). Parsel also outperforms GPT-4 with CodeT alone on the same set of tests, and using a comparable 32 samples, which passes 81.1% of the problems.

Additionally, when evaluating performance in terms of pass@any (i.e., not filtering attempts by CodeT), we discovered that by allowing up to 8 Parsel programs (still with 8 implementations each), the pass rate increased to 91.5%. In contrast, when allowing up to 64 directly generated programs, the pass rate was only 82.3%. This difference becomes more pronounced when generating 128 programs (128 directly vs. \(16\times 8\) with Parsel), improving from 84.1% to 93.9% (See Fig. 5).

While there are still eight tasks that are never solved with or without Parsel, GPT-4 solves eighteen problems with Parsel that it cannot solve alone. For eight of these, the solution has six or more functions (excluding defined but unused functions). Of the problems Parsel solved, the ones GPT-4 could not solve itself required 4.2 functions on average, while those GPT-4 could solve required only 2.8 functions (sampled over 50 problems). We find GPT-4 alone solves two problems pass@any that GPT-4 with Parsel cannot: decimal_to_binary and decode_cyclic. The decode_cyclic solutions match Chen et al. [12]'s Figure 2 (but not the dataset's canonical solution) with minor comment differences, indicating possible contamination. In decimal_to_binary, the generated Parsel solutions often unsuccessfully rely on statefulness: for example, divide_by_2_with_remainder calls calculate_remainder and update_decimal, but update_decimal cannot mutate an integer.

Most crucially, these experiments indicate that, even when given the same set of generated tests and program generation budget and selecting only one best solution, Parsel significantly increases the probability that the solution is correct on the ground-truth set of tests.

Figure 5: **Pass rates on HumanEval.** We analyze the effect of using Parsel with GPT-4 [44] on HumanEval [12]. Given the same program inference budget, the pass rate with Parsel improves regardless of whether one filters by generated tests.

**Comparison to a human expert.** In our fully automated pipeline, an LLM generates a Parsel program, which Parsel attempts to synthesize into a solution. We also evaluated how well an expert Parsel user would perform by directly writing Parsel and interacting with the Parsel synthesizer. As a case study, one of our authors with significant prior experience in competitive programming was presented with 10 randomly-selected competition-level Codeforces problems from the APPS dataset, writing Parsel from scratch in order to solve the problems The participant successfully solved 5 of the problems within a 6-hour time frame.

In comparison, when GPT-3 generated Parsel solutions to these problems, it had a success rate of 2 out of 10 problems with 8 attempts per problem (of course, in a much shorter time-frame). This result suggests that, given a suitable decomposition, the Parsel synthesizer can effectively generate complete correct solutions to hard problems - thus, a major point for improvement in the fully automated pipeline lies in the first stage (Parsel generation). In other words, improving the ability of models to decompose tasks into Parsel programs is a key bottleneck and is an important direction for future work.

While the participant could solve problems that GPT-3 could not, the participant found some aspects of working with Parsel counterintuitive. For example, one of the most effective ways to provide additional details to ensure a working solution was to write additional tests rather than to clarify the meaning in the function descriptions. Given the stochasticity of language model generation, there was concern that changing the description of a child function could break its parent - however, in practice, this seemed rare. We include the participant's solutions and the associated problems in Appendix 0.D.

**Chained components**. Chen et al. [12] highlight in their discussion of limitations that language models fail to produce code that requires the simple composition of building blocks. We replicate their experiment with Parsel, using the same docstring and providing the same descriptions as function descriptions. In Fig. 6, we visualize how Parsel solves this. We find that as the number of required components grows, even interchanging the components of only two complete programs (1x2) solves more problems than Codes solves with 16 programs. This illustrates a key point: assuming any independence, the complete set of combinatorial combinations of \(n\) implementations of a program's functions is clearly more likely to be correct than \(n\) samples from that same set. Moreover, this suggests that a framework like Parsel may support increasingly complex and abstracted programs as language models improve.

### Robotic Planning in VirtualHome

We perform a study on VirtualHome [50] to demonstrate that Parsel can also be used for complex robotic planning. VirtualHome is a simulation environment consisting of households with various interactive objects and agents. VirtualHome includes a small set of permissible actions with a strict grammar - tasks like "paint ceiling" may require multiple levels of decomposition, e.g., finding and collecting specific painting supplies, finding and using a ladder, and using the painting supplies, each requiring further decomposition.

To test the effectiveness of Parsel in this domain, we investigate whether Parsel could generate programs to solve tasks in the VirtualHome environment, while using the environment to provide feedback on whether the plan is executable. Specifically, we use Parsel to generate a Python program that can generate action plans in natural language similar to ones used in Huang et al. [28]. In each specified constraint, the produced natural language action plan is translated to formal VirtualHome instructions with minimal regex matching and tested executability. If the instructions can be successfully executed, they are considered valid - however, a potential enhancement could be the inclusion of object-relational constraints on the subsequent state of the world. We include an example of a Parsel program that successfully executed and decomposed a task in VirtualHome in Figure 2. Note we also used a header describing a valid action plan, shown in Figure 45.

Figure 6: **Pass rate vs number of chained components.** Parsel performance is shown as 1x\(k\) where \(k\) is the number of complete programs sampled. Orig and Curr are the original [12] and current Codex results, respectively. Codex (@16) corresponds to the pass rate with 16 programs.

However, as pointed out by [28], while it is easy to check plan executability, it is harder to check correctness, because there are generally many valid ways of accomplishing realistic tasks. Thus, like Huang et al. [28], in order to evaluate the accuracy of the executable plans, we perform a human study. Specifically, we ran two surveys on Prolific [45] where we asked 20 participants to make five rankings each, for a total of 100 rankings per survey. In one survey, we asked participants to rank a set of solutions by how accurately each solution accomplishes the task while in another we asked about how understandable the solution was. Two versions of distinct Parsel solutions were shown (where multiple executable Parsel solutions were available), one which included the indented function names used to solve the tasks alongside their step-by-step solutions, and one which only included step-by-step output. We compared both Parsel solutions to an also-executable baseline solution, based on Huang et al. [28]. We include more details about the survey format and executability in Appendix P.

Humans ranked the Parsel solutions as more accurate than the baseline. In each ranking, both the indented and non-indented Parsel solutions were consistently ranked higher than the baseline. In accuracy, standard Parsel solutions (as well as indented solutions) were identified as more accurate than the baseline solutions in 69% of comparisons, more than twice as often as the baseline. In clarity, the standard Parsel solution was 70% more likely to be preferred over the baseline while the indented Parsel solution was 50% more likely to be preferred compared to the baseline. There was no notable difference between indented and non-indented Parsel solutions in either accuracy or clarity. Specifically, note that all of these comparisons are reported pairwise, and Figure 7 compares the rankings given to the (non-hierarchical) Parsel-generated plan and the Codex-generated solution.

In essence, participants did not appear to prefer either Parsel plan format over the other, but given a choice between one Parsel-generated plan and its corresponding Codex-generated plan, they typically considered the Parsel-generated plan to be more accurate and clearer.

## 4 Related Works

**Step-by-step problem solving with LLMs.** Many works show that step-by-step reasoning benefits LLM performance [51; 54; 42; 60; 37; 34] and correspondingly, that this performance can be improved with guidance and tool use [71; 68; 63; 59; 22]. Acquaviva et al. [2] encouragingly showed that humans, when asked to explain how to solve problems in the Abstract Reasoning Corpus [14], tended to provide step-by-step hierarchical descriptions with many verification steps. Moreover, Wies et al. [61] presents a theoretical argument showing problems that can be learned efficiently if decomposed but require exponentially many examples w.r.t. length if not decomposed.

**Program synthesis.** Program synthesis is the long-standing challenge of generating programs from high-level specifications [26], such as input-output examples [7; 25] and/or natural language descriptions [52; 65; 19]. Program synthesizers typically search the exponentially large space of programs. Consequently, synthesizing large, complex programs remains an open challenge. Recently, _library learning_ has shown a way to make progress: even complex programs can be short in terms of the right high-level library. In turn, this library can be progressively induced from solutions to simpler synthesis problems. This idea is embodied in DreamCoder [23; 9]. Library learning requires a rich distribution of related tasks so that patterns emerge from solutions to simple problems. Patterns are abstracted into useful library functions, enabling short solutions to more complex problems. Parsel also aims to synthesize complex programs by decomposing them. However, Parsel programs specify decompositions, so a family of related tasks is not required.

Figure 7: **Robotic plan comparison.** Parsel-generated robotic plans were consistently selected as more accurate and clear when compared to a direct generation approach like in Huang et al. [28].

**LLMs for formal environment multi-step planning.** Also encouragingly, several existing works can be expressed in Parsel. For example, Huang et al. [28] and Brohan et al. [10] showed that language models can automatically generate step-by-step algorithms for robotic agents in language. In both cases, the generated language corresponds directly to pre-implemented low-level robotic abilities. This could be expressed by providing a task description and constraints evaluating that the high-level task was completed successfully. In addition, Jiang et al. [30] proposed a framework to generate formal proofs in formal theorem-proving languages from informal proofs by first generating an intermediate natural language proof sketch. This could be expressed in Parsel by generating each sketch step as a function and using formal verification for each lemma as the Parsel validation step.

**Programming languages and frameworks incorporating language models.** Other works have explored programming languages that incorporate language models. For example, Cheng et al. [13] explored the introduction of a language-model-based evaluation function, which would allow f('North America?, 'U.S.') to automatically return 'yes' by referencing the knowledge of the language model, and showed that they could also generate programs using this tool with a language model and Beurer-Kellner et al. [8] explored a related SQL-style LLM-querying language. In addition, Dohan et al. [20] presents an inference-focused framework for language models more broadly for probabilistic graphical models composed of language model actions. Unlike LM Cascades [20], we primarily focus on the constrained generation of programs, instead of leveraging language models as functions within a particular program.

**Testing code language model outputs.** Related works have explored the capacity of assert statements to constrain the generation space of code LLMs on functions [6, 12, 35]. The automatic generation of unit tests in Chen et al. [11] allowed them to filter many programs before performing final evaluations, significantly improving the pass rate per evaluation. CodeT [11] applies a language model to generate a large number of tests, presuming that some of them will be incorrect. However, instead of simply selecting the program passing the most tests as correct, CodeT groups together generated programs by the subset of the tests that they pass - it then returns any program from the group with the most tests passed in total [11]. Chen et al. [11] intuitively motivates this by the Anna Karenina principle: "Happy families are all alike; every unhappy family is unhappy in its own way."

Notably, both the number of final evaluations and the sample budget are important, and we find that combining Parsel with automatic test generation improves performance given a fixed sample and evaluation budget. In addition, Merrill et al. [38] proves essential constraints on what can be learned from assertions alone and, more crucially, what cannot.

## 5 Discussion and Limitations

We note that Parsel has important limitations, largely stemming from its dependence on closed LLMs. First, LLMs naturally underperform on languages underrepresented in their training data, like Parsel, affecting the quality and reliability of the generated code, and these closed models do not offer a training API. In addition, reliance on Codex and GPT-4 introduces vulnerabilities due to potential abrupt changes in behavior - since this project's start, access to Codex (which is free) has become much more limited and the much-better GPT-4 is also much more expensive. We unfortunately found a 2.7B CodeGen model [39], evaluated like Codex on APPS, solved no problems. However, we anticipate that improvements in open-source code LLMs will mitigate these issues.

Figure 8: **Scaling performance. The probability of passing an APPS competition-level problem increases quadratically with respect to the log of the number of Parsel programs sampled (left) and the number of implementations sampled (right).**Despite the limitations, elaborated in Appendix B, and the challenges of using a language model to generate a language that it has never seen before, the generated Parsel was able to implement robust algorithmic reasoning. There are a few natural questions that arise from this work. How robust is Parsel to the language used to specify the problem? When solving hard problems, is it better to increase the number of Parsel programs sampled or the number of program implementations sampled? We visualize Parsel's performance on the explored subset of APPS in Figure 8 to try to investigate some of these questions. In general, it appears that the probability of passing increased quadratically (R\({}^{2}\geq\)0.99 for all curves) with respect to the log of both the number of Parsel programs sampled and the number of implementations sampled. Clearly, since the pass rate is bounded between 0 and 1, this trend cannot continue indefinitely. However, given the rate limit associated with calls to the Codex API (officially, 40,000 tokens or 20 requests per minute, but in practice, we consistently ran into the rate limit with \(\approx\)10% of that usage), we were not able to identify the inflection point. Li et al. [35] indicated the pass rate curves become log-linear, but we cannot guarantee this holds for Parsel.

Furthermore, we highlight that the question we (and, in our view, most prior work) were looking at in our APPS experiments is, given hard problems and limited by one's generation budget, what is the chance of finding a correct solution? However, if one instead views each test as a successive submission, where any failure is comparable to a crash in production, one would likely take an approach like the one in our HumanEval test generation experiments, one that uses interpreter feedback to revise solutions, or perhaps even one that uses the language model to simulate the result of tests. Both interpretations are important, but they highlight different concerns: even if one can guarantee success upon submission given a trillion generated tokens, this has limited practical applicability. A method's ability to efficiently generate reasonable solutions in the first place is one key bottleneck, and assuming an imperfect model, the ability to identify them is another.

Finally, we highlight that Parsel is already capable of synthesizing larger, more complex programs. For reference, we include a working Lisp interpreter in Appendix I, based on Norvig [41], which we translated to Parsel with 21 functions. We make sure to include no explicit references to Lisp in the Parsel code and provide a header to tell Parsel to keep track of environments as dictionaries. However, this is clearly an imperfect substitute for a true object-oriented programming implementation, and we believe adding better object-oriented features to Parsel would be a valuable future direction.

## 6 Conclusion

We hope that Parsel provides a broadly useful framework for several groups: for programmers, Parsel should provide a language for robust code generation without the need to evaluate the underlying code; for students, Parsel should allow the teaching of algorithmic reasoning with less emphasis on syntax and more emphasis on problem-solving, similarly to a mathematics curriculum; for language models, Parsel should facilitate hierarchical task decomposition.

Ultimately, as we discuss in detail in Appendix C, Parsel has many natural extensions building on prior work in code synthesis from automatic test case generation [17; 12], reranking solutions by model-judged quality [69], more clever recursive decomposition [49], bootstrapping increasingly complex programs [4; 68; 43], to generating language model prompts as part of a target language [20; 31]. As we discuss in Appendix D, we also envision that Parsel may be useful for theorem proving. Thus, Parsel helps fill the gap between reasoning and execution, providing a new approach to complex, hierarchical reasoning tasks for language models.

## Acknowledgements

We would like to thank Jesse Mu, Tianyi Zhang, Rose E. Wang, Allen Nie, Ben Prystawski, Xindi Wu, Fan-Yun Sun, Isaac Kauvar, Xi Jia Zhou, John Thickstun, and Shikhar Murty for their helpful feedback, as well as Rishi Bommasani for highlighting highly relevant works. We also thank the Stanford HAI-Google collaboration for their Cloud Credit grant. Qian Huang is supported by an Open Philanthropy AI fellowship. Gabriel Poesia is supported by a Stanford Interdisciplinary Graduate Fellowship. In addition, this work was partially supported by National Science Foundation Grant No. 2302701 and National Science Foundation Expeditions Grant No. 1918771.

## References

* Abadi et al. [2015] M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen, C. Citro, G. S. Corrado, A. Davis, J. Dean, M. Devin, S. Ghemawat, I. Goodfellow, A. Harp, G. Irving, M. Isard, Y. Jia, R. Jozefowicz, L. Kaiser, M. Kudlur, J. Levenberg, D. Mane, R. Monga, S. Moore, D. Murray, C. Olah, M. Schuster, J. Shlens, B. Steiner, I. Sutskever, K. Talwar, P. Tucker, V. Vanhoucke, V. Vasudevan, F. Viegas, O. Vinyals, P. Warden, M. Wattenberg, M. Wicke, Y. Yu, and X. Zheng. TensorFlow: Large-scale machine learning on heterogeneous systems, 2015. URL https://www.tensorflow.org/. Software available from tensorflow.org.
* Acquaviva et al. [2022] S. Acquaviva, Y. Pu, M. Kryven, T. Sechopoulos, C. Wong, G. E. Ecanow, M. Nye, M. H. Tessler, and J. B. Tenenbaum. Communicating natural programs to humans and machines. _NeurIPS_, 2022.
* Anil et al. [2017] C. Anil, Y. Wu, A. J. Andreassen, A. Lewkowycz, V. Misra, V. V. Ramasesh, A. Slone, G. Gurri, E. Dyer, and B. Neyshabur. Exploring length generalization in large language models. In _Advances in Neural Information Processing Systems_.
* Anthony et al. [2017] T. Anthony, Z. Tian, and D. Barber. Thinking fast and slow with deep learning and tree search. _Advances in Neural Information Processing Systems_, 30, 2017.
* Athiwaratkun et al. [2022] B. Athiwaratkun, S. K. Gouda, Z. Wang, X. Li, Y. Tian, M. Tan, W. U. Ahmad, S. Wang, Q. Sun, M. Shang, et al. Multi-lingual evaluation of code generation models. _arXiv preprint arXiv:2210.14868_, 2022.
* Austin et al. [2021] J. Austin, A. Odena, M. Nye, M. Bosma, H. Michalewski, D. Dohan, E. Jiang, C. Cai, M. Terry, Q. Le, et al. Program synthesis with large language models. _arXiv preprint arXiv:2108.07732_, 2021.
* Bauer [1979] M. A. Bauer. Programming by examples. _Artificial Intelligence_, 12(1):1-21, 1979.
* Beurer-Kellner et al. [2022] L. Beurer-Kellner, M. Fischer, and M. Vechev. Prompting is programming: A query language for large language models. _arXiv preprint arXiv:2212.06094_, 2022.
* Bowers et al. [2023] M. Bowers, T. X. Olaussson, C. Wong, G. Grand, J. B. Tenenbaum, K. Ellis, and A. Solar-Lezama. Top-down synthesis for library learning. _POPL_, 2023.
* Brohan et al. [2022] A. Brohan, Y. Chebotar, C. Finn, K. Hausman, A. Herzog, D. Ho, J. Ibarz, A. Irpan, E. Jang, R. Julian, et al. Do as i can, not as i say: Grounding language in robotic affordances. In _6th Annual Conference on Robot Learning_, 2022.
* Chen et al. [2022] B. Chen, F. Zhang, A. Nguyen, D. Zan, Z. Lin, J.-G. Lou, and W. Chen. Codel: Code generation with generated tests. _arXiv preprint arXiv:2207.10397_, 2022.
* Chen et al. [2021] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. d. O. Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman, et al. Evaluating large language models trained on code. _arXiv preprint arXiv:2107.03374_, 2021.
* Cheng et al. [2022] Z. Cheng, T. Xie, P. Shi, C. Li, R. Nadkarni, Y. Hu, C. Xiong, D. Radev, M. Ostendorf, L. Zettlemoyer, et al. Binding language models in symbolic languages. _arXiv preprint arXiv:2210.02875_, 2022.
* Chollet [2019] F. Chollet. On the measure of intelligence. _arXiv preprint arXiv:1911.01547_, 2019.
* Cobbe et al. [2021] K. Cobbe, V. Kosaraju, M. Bavarian, J. Hilton, R. Nakano, C. Hesse, and J. Schulman. Training verifiers to solve math word problems. _arXiv preprint arXiv:2110.14168_, 2021.
* Cocke and Kennedy [1977] J. Cocke and K. Kennedy. An algorithm for reduction of operator strength. _Communications of the ACM_, 20(11):850-856, 1977.
* Daka and Fraser [2014] E. Daka and G. Fraser. A survey on unit testing practices and problems. In _2014 IEEE 25th International Symposium on Software Reliability Engineering_, pages 201-211. IEEE, 2014.
* Denny et al. [2022] P. Denny, V. Kumar, and N. Giacaman. Conversing with copilot: Exploring prompt engineering for solving cs1 problems using natural language. _SIGCSE_, 2022.
* Desai et al. [2016] A. Desai, S. Gulwani, V. Hingorani, N. Jain, A. Karkare, M. Marron, and S. Roy. Program synthesis using natural language. In _Proceedings of the 38th International Conference on Software Engineering_, pages 345-356, 2016.
* Dohan et al. [2022] D. Dohan, W. Xu, A. Lewkowycz, J. Austin, D. Bieber, R. G. Lopes, Y. Wu, H. Michalewski, R. A. Saurous, J. Sohl-Dickstein, et al. Language model cascades. _arXiv preprint arXiv:2207.10342_, 2022.

* [21] N. Du, Y. Huang, A. M. Dai, S. Tong, D. Lepikhin, Y. Xu, M. Krikun, Y. Zhou, A. W. Yu, O. Firat, et al. Glam: Efficient scaling of language models with mixture-of-experts. In _International Conference on Machine Learning_, pages 5547-5569. PMLR, 2022.
* [22] D. Dua, S. Gupta, S. Singh, and M. Gardner. Successive prompting for decomposing complex questions. _arXiv preprint arXiv:2212.04092_, 2022.
* [23] K. Ellis, C. Wong, M. Nye, M. Sable-Meyer, L. Morales, L. Hewitt, L. Cary, A. Solar-Lezama, and J. B. Tenenbaum. Dreamcoder: Bootstrapping inductive program synthesis with wake-sleep library learning. In _Proceedings of the 42nd acm sigplan international conference on programming language design and implementation_, pages 835-850, 2021.
* [24] M. Games. The fantastic combinations of john conway's new solitaire game "life" by martin gardner. _Scientific American_, 223:120-123, 1970.
* [25] S. Gulwani. Programming by examples. _Dependable Software Systems Engineering_, 45(137):3-15, 2016.
* [26] S. Gulwani, O. Polozov, R. Singh, et al. Program synthesis. _Foundations and Trends(r) in Programming Languages_, 4(1-2):1-119, 2017.
* [27] D. Hendrycks, S. Basart, S. Kadavath, M. Mazeika, A. Arora, E. Guo, C. Burns, S. Puranik, H. He, D. Song, et al. Measuring coding challenge competence with apps. _Advances in Neural Information Processing Systems_, 2021.
* [28] W. Huang, P. Abbeel, D. Pathak, and I. Mordatch. Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. _arXiv preprint arXiv:2201.07207_, 2022.
* [29] B. Ibarz, V. Kurin, G. Papamakarios, K. Nikiforou, M. Bennani, R. Csordas, A. Dudzik, M. Bosnjak, A. Vit Vitvitskyi, Y. Rubanova, et al. A generalist neural algorithmic learner. _LoG_, 2022.
* [30] A. Q. Jiang, S. Welleck, J. P. Zhou, W. Li, J. Liu, M. Jamnik, T. Lacroix, Y. Wu, and G. Lample. Draft, sketch, and prove: Guiding formal theorem provers with informal proofs. _arXiv preprint arXiv:2210.12283_, 2022.
* [31] O. Khattab, K. Santhanam, X. L. Li, D. Hall, P. Liang, C. Potts, and M. Zaharia. Demonstrate-search-predict: Composing retrieval and language models for knowledge-intensive nlp. _arXiv preprint arXiv:2212.14024_, 2022.
* [32] T. Kojima, S. S. Gu, M. Reid, Y. Matsuo, and Y. Iwasawa. Large language models are zero-shot reasoners. _arXiv preprint arXiv:2205.11916_, 2022.
* [33] S. Kulal, P. Pasupat, K. Chandra, M. Lee, O. Padon, A. Aiken, and P. S. Liang. Spoc: Search-based pseudocode to code. _Advances in Neural Information Processing Systems_, 32, 2019.
* [34] A. K. Lampinen, I. Dasgupta, S. C. Chan, K. Matthewson, M. H. Tessler, A. Creswell, J. L. McClelland, J. X. Wang, and F. Hill. Can language models learn from explanations in context? _arXiv preprint arXiv:2204.02329_, 2022.
* [35] Y. Li, D. Choi, J. Chung, N. Kushman, J. Schrittwieser, R. Leblond, T. Eccles, J. Keeling, F. Gimeno, A. Dal Lago, et al. Competition-level code generation with alphacode. _Science_, 378 (6624):1092-1097, 2022.
* [36] P. Liang, R. Bommasani, T. Lee, D. Tsipras, D. Soylu, M. Yasunaga, Y. Zhang, D. Narayanan, Y. Wu, A. Kumar, et al. Holistic evaluation of language models. _arXiv preprint arXiv:2211.09110_, 2022.
* [37] A. Marasovic, I. Beltagy, D. Downey, and M. E. Peters. Few-shot self-rationalization with natural language prompts. _arXiv preprint arXiv:2111.08284_, 2021.
* [38] W. Merrill, Y. Goldberg, R. Schwartz, and N. A. Smith. Provable limitations of acquiring meaning from ungrounded form: What will future language models understand? _Transactions of the Association for Computational Linguistics_, 9:1047-1060, 2021.
* [39] E. Nijkamp, B. Pang, H. Hayashi, L. Tu, H. Wang, Y. Zhou, S. Savarese, and C. Xiong. Codegen: An open large language model for code with multi-turn program synthesis. _arXiv preprint arXiv:2203.13474_, 2022.
* [40] E. Nijkamp, B. Pang, H. Hayashi, L. Tu, H. Wang, Y. Zhou, S. Savarese, and C. Xiong. A conversational paradigm for program synthesis. _arXiv preprint arXiv:2203.13474_, 2022.

* Norvig [2010] P. Norvig. How to write a Lisp interpreter (in Python), 2010. URL http://norvig.com/lispy.html.
* Nye et al. [2021] M. Nye, A. J. Andreassen, G. Gur-Ari, H. Michalewski, J. Austin, D. Bieber, D. Dohan, A. Lewkowycz, M. Bosma, D. Luan, et al. Show your work: Scratchpads for intermediate computation with language models. _arXiv preprint arXiv:2112.00114_, 2021.
* Odena et al. [2021] A. Odena, K. Shi, D. Bieber, R. Singh, C. Sutton, and H. Dai. Bustle: Bottom-up program synthesis through learning-guided exploration. _ICLR_, 2021.
* OpenAI [2023] OpenAI. Gpt-4 technical report. _arXiv_, 2023.
* Palan and Schitter [2018] S. Palan and C. Schitter. Prolific. ac--a subject pool for online experiments. _Journal of Behavioral and Experimental Finance_, 17:22-27, 2018.
* Paszke et al. [2019] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. _Advances in neural information processing systems_, 32, 2019.
* Perry et al. [2022] N. Perry, M. Srivastava, D. Kumar, and D. Boneh. Do users write more insecure code with ai assistants? _arXiv preprint arXiv:2211.03622_, 2022.
* Poesia et al. [2022] G. Poesia, A. Polozov, V. Le, A. Tiwari, G. Soares, C. Meek, and S. Gulwani. Synchromesh: Reliable code generation from pre-trained language models. In _International Conference on Learning Representations_, 2022.
* Polu and Sutskever [2020] S. Polu and I. Sutskever. Generative language modeling for automated theorem proving. _arXiv preprint arXiv:2009.03393_, 2020.
* Puig et al. [2018] X. Puig, K. Ra, M. Boben, J. Li, T. Wang, S. Fidler, and A. Torralba. Virtualhome: Simulating household activities via programs. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition_, pages 8494-8502, 2018.
* Rajani et al. [2019] N. F. Rajani, B. McCann, C. Xiong, and R. Socher. Explain yourself! leveraging language models for commonsense reasoning. _ACL_, 2019.
* Raza et al. [2015] M. Raza, S. Gulwani, and N. Milic-Frayling. Compositional program synthesis from natural language and examples. In _Twenty-Fourth International Joint Conference on Artificial Intelligence_, 2015.
* Sandoval et al. [2022] G. Sandoval, H. Pearce, T. Nys, R. Karri, B. Dolan-Gavitt, and S. Garg. Security implications of large language model code assistants: A user study. _arXiv preprint arXiv:2208.09727_, 2022.
* Shwartz et al. [2020] V. Shwartz, P. West, R. L. Bras, C. Bhagavatula, and Y. Choi. Unsupervised commonsense question answering with self-talk. _EMNLP 2020_, 2020.
* Simon and Newell [1971] H. A. Simon and A. Newell. Human problem solving: The state of the theory in 1970. _American psychologist_, 26(2):145, 1971.
* Singh et al. [2022] I. Singh, V. Blukis, A. Mousavian, A. Goyal, D. Xu, J. Tremblay, D. Fox, J. Thomason, and A. Garg. Progprompt: Generating situated robot task plans using large language models. _arXiv preprint arXiv:2209.11302_, 2022.
* Stuhlmuller et al. [2022] A. Stuhlmuller, J. Reppert, and L. Stebbing. Factored cognition primer. https://primer.ought.org, 2022.
* Tabachnyk and Nikolov [2022] M. Tabachnyk and S. Nikolov. Ml-enhanced code completion improves developer productivity, 2022.
* Uesato et al. [2022] J. Uesato, N. Kushman, R. Kumar, F. Song, N. Siegel, L. Wang, A. Creswell, G. Irving, and I. Higgins. Solving math word problems with process-and outcome-based feedback. _arXiv preprint arXiv:2211.14275_, 2022.
* Wei et al. [2022] J. Wei, X. Wang, D. Schuurmans, M. Bosma, E. Chi, Q. Le, and D. Zhou. Chain of thought prompting elicits reasoning in large language models. _arXiv preprint arXiv:2201.11903_, 2022.
* Wies et al. [2022] N. Wies, Y. Levine, and A. Shashua. Sub-task decomposition enables learning in sequence to sequence tasks. _arXiv preprint arXiv:2204.02892_, 2022.
* Xu et al. [2022] F. F. Xu, U. Alon, G. Neubig, and V. J. Hellendoorn. A systematic evaluation of large language models of code. In _Proceedings of the 6th ACM SIGPLAN International Symposium on Machine Programming_, pages 1-10, 2022.

* Yao et al. [2022] S. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. Narasimhan, and Y. Cao. React: Synergizing reasoning and acting in language models. _arXiv preprint arXiv:2210.03629_, 2022.
* Ye et al. [2022] A. Ye, C. Cui, T. Shi, and M. O. Riedl. Neural story planning, 2022. URL https://arxiv.org/abs/2212.08718.
* Yin and Neubig [2017] P. Yin and G. Neubig. A syntactic neural model for general-purpose code generation. _ACL_, 2017.
* Yin et al. [2018] P. Yin, B. Deng, E. Chen, B. Vasilescu, and G. Neubig. Learning to mine aligned code and natural language pairs from stack overflow. In _2018 IEEE/ACM 15th international conference on mining software repositories (MSR)_, pages 476-486. IEEE, 2018.
* Yin et al. [2022] P. Yin, W.-D. Li, K. Xiao, A. Rao, Y. Wen, K. Shi, J. Howland, P. Bailey, M. Catasta, H. Michalewski, A. Polozov, and C. Sutton. Natural language to code generation in interactive data science notebooks, 2022. URL https://arxiv.org/abs/2212.09248.
* Zelikman et al. [2022] E. Zelikman, Y. Wu, J. Mu, and N. Goodman. STar: Bootstraping reasoning with reasoning. In A. H. Oh, A. Agarwal, D. Belgrave, and K. Cho, editors, _Advances in Neural Information Processing Systems_, 2022. URL https://openreview.net/forum?id=_3ELRdg2sgI.
* Zhang et al. [2022] T. Zhang, T. Yu, T. B. Hashimoto, M. Lewis, W.-t. Yih, D. Fried, and S. I. Wang. Coder reviewer reranking for code generation. _arXiv preprint arXiv:2211.16490_, 2022.
* Zheng et al. [2022] K. Zheng, J. M. Han, and S. Polu. Minif2f: a cross-system benchmark for formal olympiad-level mathematics. _ICLR_, 2022.
* Zhou et al. [2022] D. Zhou, N. Scharli, L. Hou, J. Wei, N. Scales, X. Wang, D. Schuurmans, O. Bousquet, Q. Le, and E. Chi. Least-to-most prompting enables complex reasoning in large language models. _arXiv preprint arXiv:2205.10625_, 2022.
* Zhou et al. [2022] H. Zhou, A. Nova, H. Larochelle, A. Courville, B. Neyshabur, and H. Sedghi. Teaching algorithmic reasoning via in-context learning. _arXiv preprint arXiv:2211.09066_, 2022.

* select_airport_cities(city_road_cost, city_airport_cost): given a matrix representing the -- cost of building a road between any two cities, and a list representing the cost of -- building an airport in a city (where any two cities with airports are connected), return -- a list of the cities that should have airports built in them to minimize the total cost -- of building roads and airports such that all cities are connected. The list should be -- sorted in ascending order.
* [[0,3,3],[3,0,3],[3,3,0]],[0,0,0] -> [0,1,2]
* [[0,3,3],[3,0,3],[3,3,0]],[10,10,10] -> []
* [[0,10,3],[10,0,11],[3,11,0]],[1,4,5] -> [0,1]
* sky_city_cost(city_road_cost, city_airport_cost): given a list of lists representing the -- cost of building a road between any two cities, and a list representing the cost of -- building an airport in a city, return a new cost matrix with a new node corresponding -- to the sky.
* [[1,2,3],[1,2,3]],[4,5,6] -> [[1,2,3,4],[1,2,3,5],[1,2,3,6],[4,5,6,0]]
* minimum_spanning_tree(cost_matrix): given a list of lists representing the cost of each -- edge, return an adjacency matrix corresponding to the minimum spanning true.
* [[0,1,3,4],[1,0,2,100],[3,2,0,5],[4,100,5,0]] -> [[0,1,0,1],[1,0,1,0],[0,1,0,0],[1,0,0,0]]
* final_node_connectors(adjacency_matrix): given a list of lists representing an adjacency -- matrix, return a list of the nodes connected to the final node. However, if only one -- node is connected to the final node, return an empty list.
* [[0,1,0,1],[1,0,1,0],[0,1,0,0],[1,0,0,0]] -> []
* [[0,1,0,1],[1,0,1,0],[0,1,0,1],[1,0,1,0]] -> [0,2]

Figure 10: A potential programming assignment focused on problem-solving rather than implementation. The top-level function and asserts would be the assigned problem (which Codex [12] does not seem to be able to solve directly), while the other functions would be the student solution.

## Appendix A Implications

Parsel is a natural language compiler framework that bridges the gap between natural language and programming language by allowing programmers to write high-level algorithmic designs in natural language and automatically compiling them into valid code. This has potential benefits for programmers, students, and code language models.

### For Programmers

#### a.1.1 Current Limitations

First, programming generation language models like Codex continue to be constrained primarily to individual functions, rarely exceeding a few dozen lines in practice [12, 58]. This is still a dramatic shift from foundational earlier works, which focused on the association between one line of natural language pseudocode with one line of code [33] or a line of text to a StackOverflow snippet [66]. Yet, these models perform worse the more unusual the desired functions are, and recent research suggests that people using these language models are more likely to introduce buggy code [47], although this is not yet conclusive [53].

#### a.1.2 Potential Benefits

On the other hand, results from Google and others indicate that professionals can write code more efficiently with large language models, and the benefits will likely only improve as they improve [58]. Since Parsel requires constraints that ensure functions behave as expected, this should encourage bug-free programs and avoid the need for manually checking that specific underlying functions are correct. Furthermore, a function written in Parsel is likely to be more resilient to breaking changes in the target language, especially syntactic changes (e.g. Python2 to Python3). In addition, a natural extension would draw on work on automatic unit testing [17] to suggest additional constraints where behavior is ambiguous between implementations of a function. However, for Parsel to be practically useful for real-world programmers, we expect multiple improvements to be necessary, including additional object-oriented features.

### For Students

#### a.2.1 Current Limitations

In addition, these language models pose serious challenges for programming pedagogy - existing introductory programming classes rely extensively on teaching syntax and how to implement algorithms over how to solve problems with them. Free language model-based tools like Copilot can essentially solve many of these introductory assignments directly, function by function. Those which cannot be solved currently will be increasingly solved [18].

#### a.2.2 Potential Benefits

Many students currently introduced to programming struggle with learning syntax and debugging unclear compiler or interpreter errors. However, abstracting away these details with a natural-language coding language will likely make learning to code more accessible to students who are just beginning to code. In addition, stepping away from implementation-focused assignments will allow a focus on higher-level problem-solving assignments earlier. These will allow for assignments that are more like those in mathematics. For example, for a problem like Figure A.9, instead of choosing between requiring students to manually implement a problem-solving focused question like the top-level description of, or requiring teaching assistants to manually evaluate the reasoning for correctness, one could ask them to implement a solution in Parsel.

### For Code Language Models

#### a.3.1 Current Limitations

Traditional programming languages result in some unique challenges for language models. For example, unlike natural languages, traditional programming languages are far less robust to slight variations in wording. In addition, traditional programming languages require many tokens for syntactic details and in some cases, may take many lines to express what can be expressed far more simply in language. For example, referring to a shortest-path algorithm or Conway's game of life takes far fewer tokens than actually implementing them. However, even with fairly nonstandard problems, LLMs have shown remarkable algorithmic generalization ability [36; 62; 3; 72]. One alternative that has been explored is conversational code generation [40; 67]. However, these approaches have primarily focused on highly imperative programming structures. Moreover, they still require having the full program in context and do not clearly generalize to complex hierarchical programs with many functions.

#### a.3.2 Potential Benefits

Parsel allows code language models to stay closer to natural language when generating code, which corresponds more closely to their primary source of training data. Moreover, it allows complex but standard methods to be described concisely, requiring fewer tokens to generate. One exciting additional benefit is the potential to generate solutions recursively: if the Parsel compiler is unable to find a solution for a set of functions, it should be possible to prompt the model to define new helper functions. In fact, we find that often the model attempts to reference undefined auxiliary functions when defining complex functions (e.g. "count_living_neighbors(grid, i, j)" in Conway's game of life), and as a result support an optional argument where the model can attempt to resolve NameErrors automatically by attempting to implement functions.

## Appendix B Limitations

There are several limitations to the current implementation of Parsel. First, Parsel relies on a code LLM to generate implementations of individual functions, and the quality of these implementations can vary depending on the specific model used and the complexity of the function descriptions. In particular, Parsel may struggle to generate correct code for individual functions with complex behavior (i.e. functions that Codex cannot implement). However, this can be mitigated by decomposing the complex functions into simpler ones that can be implemented more easily.

The current implementation of Parsel may struggle to generate correct code when there are many functions with complex dependencies or without constraints. This is because the number of implementation combinations to consider grows exponentially with the size of the largest strongly connected components. As discussed, this can limit Parsel's performance on some programs. However, approaches like Chen et al. [11] may be able to mitigate this.

Code LLMs, unfortunately, do not perform well on languages underrepresented in their training data - with few examples to learn from, LLMs may struggle to generate correct code in these languages [5]. However, some LLMs can learn new languages in context, allowing them to generate code in languages not in their training data [5]. These limitations can impact the quality and reliability of the code generated with Parsel. In addition, because code LLMs have never been trained on Parsel, this harms their ability to generate it. While we could wait for Parsel to gain widespread adoption, it should also be possible to translate many existing codebases to Parsel. We include a proof-of-concept backtranslation/decompilation study in Appendix L.

In addition, the best open-source code LLMs currently available e.g. PolyCoder [62] substantially underperform Codex, while Codex is competitive with other traditional LLMs on reasoning tasks [36]. However, this dependence on closed models creates a vulnerability, as the providers of closed LLMs can change behavior (e.g. rate limits or model implementations) without warning. Indeed, between the time we started working on Parsel and this version of the paper, OpenAI ended widespread access to Codex, now available only by request.

Because of this, we evaluated a 2.7B CodeGen model from Nijkamp et al. [39] with Parsel in the same configuration we used when evaluating APPS on Codex (in the 8x16 configuration). We found that it could solve none of the random 25 problems which we evaluated it on. However, despite these limitations, the current Parsel implementation has shown promising results in generating correct code for a variety of functions and languages. Many limitations will likely be ameliorated as code LLMs improve.

## Appendix C Future Work

In the future, we hope to more deeply integrate automatic unit test generation, especially in combination with user-provided tests [11, 17]. One method would be to identify edge cases and check whether the set of functions that successfully solve all existing tests disagree on any new tests. This could permit automatic decomposition without exponential growth in implementation combinations. Techniques like those proposed in Zhang et al. [69], which would allow us to rerank a set of solutions, could also allow us to search the combinatorial space of solutions more quickly. Relatedly, for the robotic task planning, incorporating asserts at the execution level (e.g. checking whether the agent is close to the microwave, as in Singh et al. [56]) is a promising research direction. Furthermore, evaluating the examples in this paper, we found that using the minimum CodeT score across all generated functions was a consistently effective heuristic to identify good sets of functions. However, generating unit tests for all functions when generating Parsel programs instead of generating unit tests for a shared top-level function increases the inference cost from linear in the number of tasks to also being linear in the number of functions and Parsel programs generated. Finding a way to balance this tradeoff would likely be valuable.

In addition, we plan to incorporate ways of varying the "confidence threshold" of the language model. Ensuring that the descriptions are straightforward and unambiguous is important for more critical programs and parts of programs. In addition, when teaching students simpler concepts, requiring them to decompose the task further may be useful.

We would like to integrate value functions to allow decomposition to be done more methodically where no verification is possible. Specifically, automatically decomposing all functions that have not yet been implemented in an SCC is suboptimal and could be improved with a model of expected improvement due to expansion, as done for proof expansion in Polu and Sutskever [49]. In addition, when decomposing functions, we would like to permit the model to reference already-defined functions (rather than to just define new ones). We might even use the code language model to determine which function to evaluate next. Further, we aim to support more general reward functions for function implementations where multiple may be valid but we rank implementations based on a desired feature. These "soft" constraints may also allow new Parsel uses, e.g. planning stories in natural language [64].

Finally, we hope it would be possible to use Parsel as a framework for bootstrapping increasingly complex program generation (e.g. Anthony et al. [4], Zelikman et al. [68], Odena et al. [43]). That is, by 1) generating Parsel examples from a purely natural language specification and then reinforcing those which successfully compile, and 2) by reinforcing the model with each successfully compiled component, we would likely be able to iteratively improve performance with an arbitrarily large dataset of examples.

Another feature that would be valuable would be the ability to incorporate multiple base tools with different kinds of specialized models, inspired by Ibarz et al. [29] and Dohan et al. [20]. That is, it would be valuable to allow a model to determine which target language to use, possibly combining them. For example, for large parts of the Tensorflow and PyTorch libraries, while their interfaces are written in Python, they depend heavily on large C++ codebases [46, 1]. Relatedly, Cobbe et al. [15] showed that giving language models access to a calculator allowed them to solve more complex math word problems. This, combined with the observation that Parsel could also compile programs by generating language model prompts to be used as part of the program, may potentially allow the automatic generation of task-specific language model cascades [20].

Another noteworthy addition would be the integration of Synchromesh [48], ensuring that each new word or token generated by the model is actually possible within the grammar of the given formal language and does not violate other semantic constraints.

Ultimately, we hope that this specification for Parsel is a jumping-off point for a new way of thinking about programming and reasoning.

## Appendix D Theorem Proving in Lean

With the same framework, we can generate proofs in formal theorem-proving languages such as Lean, as in Figure 10. We include the translated version in the appendix. Note a nuance of Lean and theorem-proving languages is that the ability to run Lean on proof with no errors/warnings indicates the proof is correct (but is not a guarantee that the proof statement matches our claim in language). Thus, each function in a Lean Parsel proof has an "implicit constraint." This makes it straightforward to identify which informal parts of a proof are most difficult to explicate. Generally, we believe Parsel can be a powerful tool for theorem proving.

Yet, we observed important challenges in this context, which we believe are avenues for future work and can be resolved. For example, in datasets such as MiniF2F [70], many proofs require explicit calculations in intermediate steps. That is, many proofs are similar to "Find the minimum value of \(\frac{9x^{2}\sin^{2}x+4}{x\sin^{2}x}\) for \(0<x<\pi\). Show that it is 012." (from the informal MiniF2F introduced by Jiang et al. [30]). We believe that a dataset of proof statements (in natural and formal language), requiring complex proofs that are more abstract and less dependent on explicit calculations would allow us to better measure progress towards solving difficult theorems - we leave this to future work.

Figure 10: Parsel to Lean (theorem proving)

[MISSING_PAGE_FAIL:19]

[MISSING_PAGE_EMPTY:20]

```

**Algorithm 1** Parsel program synthesis - corresponds to short Parsel pseudocode.

``` Input: A string \(program\) specifying a Parsel program and a \(target\_language\) Output: Synthesized program Function Parsel(program,target\_language): \(function\_graph\leftarrow\) ParseProgram(\(program\)) \(dependency\_graph\leftarrow\) GetDependencyGraph(\(function\_graph,target\_language\)) \(scc\_scc\_graph\leftarrow\) IdentifyScc(\(dependency\_graph\)) \(implementation\leftarrow\) \(\leftarrow\) foreach soc \(\in\)WordInterval(\(scc\))do \(implementation\leftarrow\) \(implementation\) + SynthesizeSCC(\(scc\_scc\_graph\)) returnimplementation FunctionParseProgram(\(program\)): \(root\leftarrow CreateRootNode()\) \(current\_node\gets root\) \(current\_indentation\gets 0\) foreach line \(in\)\(program\)do \(line\_indentation\leftarrow\) IndentationOf(\(line\)) while\(line\_indentation\in current\_indentation\)do \(current\_node\leftarrow current\_node\_parent\) \(current\_ind\_indentation\_=1\) \(current\_node\leftarrow\) ParseLine(\(line\_current\_node\)) return\(root\) FunctionParseLine(\(line\_current\_node\)): if\(is\)then  current_node_children\(\leftarrow\) ParseDefinition(\(line\)); // Creates a new function node else if\(\mathit{IfReference(line)}\)then  current_node_children\(\leftarrow\)ParseReference(\(line\)); // Adds reference to current node else current_node_constraints\(\leftarrow\) ParseConstraint(\(line\)); // Adds constraint to current node Function GetDependencyGraph(\(function\_graph,target\_language\)): //Cancers/function call graph independency graph \(dependency\_graph\leftarrow\) CopyOf(\(function\_graph\)): foreach node \(dependency\_graph\)do foreach child \(\in\)\(node\_children\)do if\(child\_asserts=\emptyset\)then  AddDependency(\(child\_node\)); // Child without asserts depends on parent return\(dependency\_graph\) Function SynthesizeSCC(\(scc\_scc\_graph\)): \(implementation\_str\leftarrow\) SynthesizeChildren(\(scc\_scc\_graph\)) \(fn\_appendements\leftarrow\) GenerateImplementations(\(scc,n,implementation\_str\)) \(implementation\leftarrow\) SolveConstraints(\(scc\_fn\_implementations\)) if\(implementation\) is validthen return\(implementation\) Function SynthesizeChildren(\(scc\_scc\_graph\)): \(child\_implementation\leftarrow\) \(\leftarrow\) 'foreach \(child\_scc\)\(scc\_graph\).\(GetChildren(scc)\)do \(child\_implementations\leftarrow\) \(child\_implementations\) + SynthesizeSCC(\(child\_scc\_scc\_graph\)) return\(child\_implementations\) Function GenerateImplementations(\(scc,n,implementation\_str\)): \(implementation\leftarrow\) [] foreach \(fn\in\)\(\mathit{score}\)do \(fn\_implementations\leftarrow\) GenerateImplementations(\(fn,n,implementation\_str\)) \(implementation\_appendfn\_implementations\)) return\(implementations\) FunctionSolveConstraints(\(scc\_fn\_implementations\)): \(combinations\leftarrow\) DirectProductInplementations(\(fn\_implementations\)) \(constraints\leftarrow\) GenerateConstraints(\(scc\)); // Translate each of the constraints into an evaluation string foreach implementation\(\_str\in\)\(combinations\)do \(\mathit{return}implementation\_str\); // Return if implementation passes constraints \(\mathit{OnAll(ccc,scc\_graph)}\); // If none are found, raise error FunctionGenerateConstraints(\(scc\)); \(constraints\leftarrow\) [] foreach \(fn\in\)\(\mathit{score}\)do \(fn\_constraints\leftarrow\) GetConstraints(\(fn\)) \(constraints.extend(fn\_constraints)\) return\(constraints\) ```

**Algorithm 2** Parsel program synthesis - corresponds to short Parsel pseudocode.

Parsel Overview (Detailed)

We include a more detailed figure outlining Parsel.

## Appendix I Lisp Interpreter

We include the Parsel code for a minimal Lisp interpreter.

Figure A.13: **Parsel overview (detailed)**.

* [Lanewisdictionaryof{'var':val}pairs,withalinktoitsouterenvironmentinenv{'outer'}.
* [AProcedureisuainlambdaexpression,withparms,body,andenvwhichcalleseval,exponthebody.

[MISSING_PAGE_POST]

[MISSING_PAGE_FAIL:24]

* [1] # Reviewer:
* [2] # Please explain the above function in one sentence with as much detail as possible.
* [3] # In your one-sentence description, specify the range and domain of your function precisely.
* [4] # Your description should be clear enough that someone could reimplement the function from it.
* [5] # Author:
* [6] # Sounds good, here's my one-sentence explanation of {name}:
* [7] # {name}

Figure A.18: Prompt format to generate descriptions for backtranslation

## Appendix L APPS Backtranslation

### Backtranslation / decompiling.

We anticipate that there are many programs that LLMs can implement by first generating Parsel code. But, as Parsel is a new framework, while language models can sometimes generate Parsel programs with few-shot prompts, it is not a syntax they have previously encountered. Thus, we may want to use existing code in other languages to construct datasets of Parsel programs from other languages. This requires us to first extract the call graph from the code, generate descriptions for each of the functions, and then generate Parsel programs from the graph. This call graph representation is convenient, so it is useful to have a bidirectional method to produce a graph from Parsel code and to produce Parsel code from the graph.

We filter the dataset to problems with starter code (providing the name of the evaluated function) and unit tests (provided as input-output pairs). For those tasks, we select solutions that define and call at least three functions, with at least one over 4 lines long and none over 15 lines.

As a proof of concept, we show 10 Parsel solutions which we could automatically generate from the APPS solutions. We generated the descriptions by prompting Codex to explain each function and its inputs and outputs. From this, we use backtranslation to attempt to implement these solutions in Python. We then verify that they are correct by applying the original unit tests as constraints on the root function. As mentioned in Section 1, the Parsel code is substantially shorter in terms of lines of code. We include these in Appendix L.

### Examples

We exclude the asserts in these examples for brevity - they correspond to those in the original dataset.

```
1longest_palindrome(s):longest_palindrometakesastringsandreturnsthelongestpalindromeins.
2is_palindrome(s):is_palindromereturnsTrueifthestringsisthesameforwardsandbackwards,andFalseotherwise.
3check(1i,ri,s):checktakesastrings,aleftindex1i,andarightindexri,andreturnsthelongestpalindromethatstartsatorbefore1iandendsatorafterri.
4is_palindrome

Figure 19: Train Problem 1638, Solution 2* [1] # longest_palindrome takes a string s and returns the longest palindrome in s.
* [2] def longest_palindrome(s):
* [3] if len(s) <= 1:
* [4] returns
* [5] else:
* [6] longest = s[0]
* [7] for i in range(len(s)):
* [8] for j in range(len(s)):
* [9] if is_palindrome(check(i, j, s)) and len(check(i, j, s)) > len(longest):
* [10] longest = check(i, j, s)
* [11] return longest
* [12]
* [13] # is_palindrome returns True if the strings is the same forwards and backwards, and False otherwise.
* [14] def is_palindrome(s):
* [15] if len(s) <= 1:
* [16] return True
* [17] else:
* [18] return s[0] == s[-1] and is_palindrome(s[1:-1])
* [19]
* [20] # check takes a string s, a left index li, and a right index ri, and returns the longest
* [21] palindrome that starts at or before li and ends at or after ri.
* [22] def check(li, ri, s):
* [23] while li >= 0 and ri < len(s) and s[li] == s[ri]:
* [24] ri += 1
* [25] returns[li+1:ri]

Figure 19: Train Problem 1638, Solution 2

Figure 20: Train Problem 1638, Solution 2

* #case_id takes a string and returns a string that is either "kebab", "snake", "camel", or " -- none" depending on whether the input string is in kebab case, snake case, camel case, or -- none of the above.
* 2def case_id(c_str):
* 3ifis_snake(c_str) == True:
* 4return "snake"
* 5elifis_kebab(c_str) == True:
* 6return "kebab"
* 7elifis_camel(c_str) == True:
* 8return "camel"
* 9else:
* 10return "none"
* 11
* 12#is_snake takes a string and returns True if the string is snake_case and False otherwise.
* 13defis_snake(s):
* 14ifs[0].isalpha() ands[0].islower() and len(s) > 1:
* 15for char in s:
* 16if char.isalpha():
* 17ifchar.isupper():
* 18return False
* 19elifchar == '_':
* 20pass
* 21else:
* 22return False
* 23return True
* 24else:
* 25return False
* 26
* 27#is_kebab takes a string and returns True if the string is a kebab-case string, and False otherwise.
* 28defis_kebab(s):
* 29#ifsisempty, False
* 30ifs== '':
* 31return False
* 32#ifsisnotastring, False
* 33iftype(s)!=str:
* 34return False
* 35#ifsisnotlowevercase, False
* 36ifs!=s.lower():
* 37return False
* 38#ifscontainsanythingotherthana-zor-, False
* 39forcins:
* 40ifnot(c.isalpha()orc=='_'):
* 41returnFalse
* 42#ifscontainsa-athebeginningorend, False
* 43ifs[0]=='_'ors[-1]=='_':
* 44returnFalse
* 45#ifscontainsmorethanone-inarow, False
* 46foriinrange(len(s)-1):
* 47ifs[i]=='_'ands[i+1]=='_':
* 48returnFalse
* 49#otherwise, True
* 50return True
* 51
* 52
* 53#is_camelreturnsTrueifthestringsisnotlowercase,doesnotcontaindashes,anddoesnotcontainunderscores.
* 54defis_camel(s):
* 55returns!=s.lower()ands.find('_')==-1ands.find('_')==-1

Figure.22: Train Problem 2892, Solution 7

[MISSING_PAGE_FAIL:29]

* 1# happy_numberstakesapositiveintegernandreturnsalistofallthehappynumbers
* 2defhappy_numbers(n): return[iforiinrange(1,n+1)if_is_happy_number(i)]
* 4
* 5#_is_happy_numbertakesapositiveintegerandreturnsTrueifthenumberisahappynumber,Falseotherwise.
* 6def_is_happy_number(number):
* 7#Wewanttomakesurethatthenumberispositive
* 8ifnumber<0: returnFalse
* 10#Wewanttomakesurethatthenumberisnot1
* 11ifnumber==1: returnTrue
* 13#Wewanttomkeeptrackofthenumberswehaveused
* 14used_numbers=[]
* 15#Wewanttomologthroughthenumber
* 16whilenumbernotinused_numbers:
* 17#Wewanttomakesurethattheallofusednumbers
* 18used_numbers.append(number)
* 19#Wewanttomofindensumofthesquaresofthedigitsofthenumber
* 20number=_sum_squares(number)
* 21#Wewanttomchecksifthenumberisone
* 22ifnumber==1:
* 23#WewanttomreturnTrue
* 24returnTrue
* 25#WewanttomreturnFalse
* 26returnFalse
* 27
* 8#_sum_squarestakesanon-negativeintegerandreturnsthesumofthesquaresofitsdigits.
* 29def_sum_squares(number):
* 30ifnumber<0: raiseValueError
* 32else: number=str(number)
* 33sum=0
* 34foriinnumber:
* 35sum+=int(i)**2
* 36returnsum

* 11am_i_wilson(n):am_i_wilson(n)returnsTrueifnisaprimenumberbetween2and563,inclusive,andFalseotherwise.
* 2is_prime(n):is_primetakesapositiveintegernandreturnsTrueifnisprimeandFalse
* 3factorial(n):factorial(n)returnstheproductofallintegersfrom1ton,inclusive.

Figure 10: Train Problem 3140, Solution 7

Figure 11: Train Problem 3140, Solution 7

* #am_i_wilson(n) returns True if n is a prime number between 2 and 563, inclusive, and False otherwise.
* defam_i_wilson(n): ifis_prime(n) and 2 <= n and n <= 563:
* return (factorial(n-1) + 1) % (n**2) == 0
* else:
* return False
* #is_prime takes a positive integer n and returns True if n is prime and False otherwise.
* defis_prime(n): ifn == 2:
* return True
* ifn == 3: return True
* ifn % 2 == 0: return False
* ifn % 3 == 0: return False
* i = 5
* w = 2
* while i
* i <= n: ifn % i == 0: return False
* i += w

* return True
*
*

Figure A.28: Train Problem 3229, Solution 26

Figure A.29: Train Problem 3229, Solution 71

[MISSING_PAGE_FAIL:32]

* 1#circular_primetakesanumberandreturnsTrueifitisacircularprime,andFalse
* 1#otherwise.
* 2defcircular_prime(number):
* 3ifis_prime(number)==False:
* 4returnFalse
* 5else:
* 6foriinrange(len(str(number))-1):
* 7number=rotate(str(number),1)
* 8ifis_prime(int(number))==False:
* 9returnFalse
* 10returnTrue
* 11
* 12#rotatetakesalistandanintegerandreturnsanewlistwiththelastnelementsoftheoriginallistmovedothefront.
* 13defrotate(1,n):
* 14returnl[-n:]+l[:-n]
* 15
* 16#is_primereturnsTrueifnisaprimenumber,andFalseotherwise.
* 17defis_prime(n):
* 18ifn<2:
* 19returnFalse
* 20foriinrange(2,int(n**0.5)+1):
* 21ifn%i==0:
* 22returnFalse
* 23returnTrue

Figure A.34: Train Problem 3368, Solution 5

Figure A.35: Train Problem 3368, Solution 6

[MISSING_PAGE_FAIL:34]

* [1] # list_squared(m, n) returns a list of lists of integers, where each sublist contains an integer and its sum of squared divisors, and the integers are in the range [m, n] ( inclusive).
* [2] def list_squared(m, n):
* [3] result = []
* [4] for num in range(m, n + 1):
* [5] divisors = divisors_list(num) sum_of_squares = sum_squares(divisors)
* [6] if isSquarable(sum_of_squares):
* [7] result.append([num, sum_of_squares])
* [8] return result
* [10] # divisors_list takes a number and returns a list of all the divisors of that number.
* [11] def divisors_list(num):
* [12] divisors = []
* [13] for i in range(1, num + 1):
* [14] if num % i == 0:
* [15] divisors.append(i)
* [16] return divisors
* [17]
* [18]
* [19] # sum_squares takes a list of numbers and returns the sum of the squares of those numbers.
* [20] def sum_squares(nums):
* [21] sum = 0
* [22] for i in range(len(nums)):
* [23] sum += nums[i]**2
* [24] return sum
* [25]
* [26] # isSquarable takes a number and returns True if the number is a perfect square, and False otherwise.
* [27] def isSquarable(num):
* [28] root = num ** (1/2)
* [29] if root % 1 == 0:
* [30] return True
* [31] else:
* [32] return False

Figure A.38: Train Problem 3393, Solution 35

Compiled Examples
* 2def game_of_life_inversion_iteration(array_at_time_t):
* 3#Your code here
* 4#return game_of_life_iteration(invert_array(array_at_time_t))
* 5return invert_array(game_of_life_iteration(array_at_time_t))
* 6
* 7#Takes a board and returns the next iteration of the game of life
* 8der invert_array(array_at_time_t):
* 9return [list(map(lambda x: 1-x, row)) for row in array_at_time_t]
* 10
* 11#Takes a board and returns the board with all values flipped
* 2def game_of_life_iteration(array_at_time_t):
* 3#The array that will be returned
* array_at_time_t_plus_1 = []
* 15
* 16#Iterate through the rows of the array
* 17for i in range(0, len(array_at_time_t)):
* 18#The array that will contain the next row
* 19next_row = []
* 20
* 21#Iterate through the columns of the array
* 22for j in range(0, len(array_at_time_t[i])):
* 23#The number of neighbors
* 24num_neighbors = 0
* 25
* 26#Iterate through the neighbors of the cell
* 27for k in range(-1, 2):
* 28for l in range(-1, 2):
* 29#Don't count the cell itself
* 30if k == 0 and l == 0:
* continue
* 31
* 32
* 33#Check if the neighbor is valid if i + k >= 0 and i + k < len(array_at_time_t) and j + 1 >= 0 and j + 1 < len(array_at_time_t[i]):
* 34#If the neighbor is alive, increment the number of neighbors
* 35if array_at_time_t[i + k][j + 1] == 1:
* 36num_neighbors == 1
* 37
* 38
* 39#If the cell is alive, check if it should die
* 40if array_at_time_t[i][j] == 1:
* 41if num_neighbors < 2 or num_neighbors > 3:
* 42next_row.append(0)
* 43else:
* 44next_row.append(1)
* 45#If the cell is dead, check if it should become alive
* 46else:
* 47if num_neighbors == 3:
* 48next_row.append(1)
* 49else:
* 50next_row.append(0)
* 51
* 52#Add the next row to the array
* 53array_at_time_t_plus_1.append(next_row)
* 54
* 55#Return the next array
* 56return array_at_time_t_plus_1
* 57
* 58assert game_of_life_inversion_iteration([[0, 0, 1], [1, 0, 0], [1, 0, 0]]) == [[1, 1, 1], [1, 0, 1], [1, 1, 1]]
* 59assert game_of_life_inversion_iteration([[0, 1, 0, 0], [1, 0, 1, 0], [1, 0, 0, 1], [0, 1, 1, 0]) == [[1, 0, 1, 1], [0, 1, 1, 0], [1, 0, 0, 1]]

Figure 19: Compilation of the program taking Conway’s game of life [24] and returning the next state, with the values inverted. In this case, we use automatic decomposition to generate the two subfunctions of game_of_life_iteration.

* [1]#Takes a board with active and inactive cells and returns the next iteration of the game of
* [2]life, but with all the values flipped
* [3]returnarray_inversion(game_of_life_iteration(array_at_time_t))
* [4]
* [5]#Takes a board with active and inactive cells as a list of lists and returns the next iteration of the game of life
* [6]def game_of_life_iteration(array_at_time_t):
* [7]array_at_time_t_plus_1 = []
* [8]for row in range(len(array_at_time_t)):
* [9]array_at_time_t_plus_1.append([])
* [10]for column in range(len(array_at_time_t[row])):
* [11]array_at_time_t_plus_1[row].append(0)
* [12]for row in range(len(array_at_time_t)):
* [13]for column in range(len(array_at_time_t[row])):
* [14]if array_at_time_t[row][column] == 1:
* [15]if count_neighbors(array_at_time_t, row, column) < 2:
* [16]array_at_time_t_plus_1[row][column] = 0
* [17]elif count_neighbors(array_at_time_t, row, column) > 3:
* [18]array_at_time_t_plus_1[row][column] = 0
* [19]else:
* [20]array_at_time_t_plus_1[row][column] = 1
* [21]else:
* [22]if count_neighbors(array_at_time_t, row, column) == 3:
* [23]array_at_time_t_plus_1[row][column] = 1
* [24]returnarray_at_time_t_plus_1
* [25]
* [26]#Invertasquarearraybyreplacingall0'swith1'sandviceversa
* [27]defarray_inversion(array):
* [28]inverted_array=[]
* [29]foriinrange(len(array)):
* [30]inverted_array.append([])
* [31]forjinrange(len(array[i])):
* [32]inverted_array[i].append(1-array[i][j])
* [33]returninverted_array
* [34]
* [35]#
* [36]defcount_neighbors(array_at_time_t, row, column):
* [37]count=0
* [38]foriinrange(row-1, row+2):
* [39]forjinrange(column-1, column+2):
* [40]ifi=rowandj==column:
* [41]ifi<0orj<0:
* [42]continue
* [43]ifi>=len(array_at_time_t)orj>=len(array_at_time_t[0]):
* [44]continue
* [45]ifarray_at_time_t[i][j]==1:
* [46]count=1
* [47]returncount
* [48]assertgame_of_life_inversion_iteration([0, 0, 1], [1, 0, 0], [1, 0, 0]) == [[1, 1, 1], [1, 0, 1], [1, 0, 1], [1, 1, 1]]
* [49]assertgame_of_life_inversion_iteration([0, 0, 1], [1, 0, 0], [1, 0, 0]) == [[0, 0, 0], [0, 1, 0], [0, 0, 0]]
* [50]assertgame_of_life_iteration([0, 1, 0, 0], [1, 0, 1], 0], [1, 0, 0, 1], [0, 1, 1, 0]) ==
* [60]assertarray_inversion([[1]]) == [[0]]
* [71]assertarray_inversion([[0, 1], [1, 0]) == [[1, 0], [0, 1]]

Figure 40: Compilation of the program taking Conway’s game of life [24] and returning the next state, with the values inverted. In this case, we usggautomatic infilling to generate the count_neighbors function.

[MISSING_PAGE_FAIL:39]

* [1] -- if p \(\wedge\) q, then q \(\wedge\) p
* [2] lemma p_q_implies_q_p(p q: Prop):
* [3] p \(\wedge\) q \(\rightarrow\) q \(\wedge\) p :=
* [4] begin
* [5] intro h,
* [6] cases h with hp hq,
* [7] split,
* [8] exact hq,
* [9] exact hp,
* [10]
* [11] end
* [12]
* [13] -- Description: if p \(\vee\) q, then q \(\vee\) p
* [14] -- if q \(\wedge\) p, then p \(\wedge\) q
* [15] lemma q_p_implies_p_q(p q: Prop):
* [16]
* [17] (q \(\wedge\) p) \(\rightarrow\) (p \(\wedge\) q) :=
* [18] begin
* [19] intro h,
* [20] split,
* [21] exact h.right,
* [22] exact h.left,
* [23] end
* [24]
* [25] /-
* [26] Theorem:
* [27] If q \(\wedge\) p, then p \(\wedge\) q
* [28] -/
* [29] -- the and operator is commutative
* [30] lemma and_commute(p q: Prop):
* [31] (p \(\wedge\) q \(\rightarrow\) q \(\wedge\) p) \(\wedge\) (q \(\wedge\) p \(\rightarrow\) p \(\wedge\) q) :=
* [32] begin
* [33] apply and.intro,
* [34] { apply p_q_implies_q_p },
* [35] { apply q_p_implies_p_q }
* [36] end
* [37]
* [38] -- Description: if p \(\wedge\) q, then p
* [39] -- Signature: p_and_q_implies_p(p q: Prop)
* [40] -- show (p \(\wedge\) q \(\rightarrow\) q \(\wedge\) p) \(\wedge\) (q \(\wedge\) p \(\rightarrow\) p \(\wedge\) q)

Figure A.42: Generated proof of and_commute. We trim the post-proof comments elsewhere.

[MISSING_PAGE_EMPTY:41]

[MISSING_PAGE_FAIL:42]

APPS Decomposition Prompts and Evaluation Hyperparameters

We slightly loosen the requirements for Parsel programs generated by language models, treating redundant function definitions as references instead of raising errors. We sample everything with temperature=0.6, except the translations which we sample with temperature=0.2, a presence penalty of 0.1, and a logit bias to prevent it from generating the text "def", as Codex has a tendency to degenerate to producing Python even when prompted with Parsel examples. We allow at most 500 tokens per function, but in practice found that they typically used less than half of them.

For evaluation, we use a timeout of 0.04 seconds per solution and evaluate at most 100,000 implementations per generated Parsel program.

For the Codex-only ablation, we allow it to generate at most 1000 tokens, in large part due to the rate limit. In particular, there is a heuristic rate limit that rejects any calls requesting more than 4,000 tokens. As a result, any larger number of samples per problem would prevent batching more than 3 samples at a time.

```
1"""Anactionplanisalistofstringsthatdescribesasequenceofstepstoaccomplisha-task,Tobeccorrectlypared,anactionplanmustbesyntacticallycorrectandcontain-onlyallowedactionsandrecognizableableobjects.Allowedactions:"close'<arg1>,'.cut><arg1>,'drink'<arg1>,'drop'<arg1>,'eat'<arg1>,'find'<arg1>,'grab'>arg1>,'.get>'arg1>,'ileon'<arg1>,'lookat'<arg1>,'open'<arg1>,'plugin'<arg1>,'plugout'<arg1>,'pointat'<arg1>,'pour'<arg1>'into'<arg2>,'pull'<arg1>,'push'<arg1>,'put'<arg1>'on'<arg2>,'put'<arg1>'in'<arg2>,'putback'<arg1>,'takeoff'<arg1>,'paton'<arg1>,'read'<arg1>,'release','rinse'<arg1>,'runto'<arg1>,'send'<arg1>,'sileon'<arg1>,'sleep'<arg1>,'standup','switchoff'<arg1>,'switchon'<arg1>,'touch'<arg1>,'turnto'<arg1>,'typeon'<arg1>,'wakeup','walkto'<arg1>,'wash'<arg1>,'watch'<arg1>,'wipe'<arg1>.Toastastlythecommon-senseconstraints,eachactionstepinthisactionplanmustnotviolatethesetofits-pre-conditions(e.g.theagentcannotgraphmilkfromthefridgebeforeopeningit)and-post-conditions(e.g.thestateofthefridgechangesfrom"closed"to"open"afterthe-agentopensit)."""
2#**
3task_plan():returnalistofstringsthatrepresentsanactionplantoputaumongonthe-stallandbreadonthedesk.
4->"executable"
5put_object_on(object,place):returnalistofstringsthatrepresentsanactionplanto-putanobjectinapace.
6"mug","stall"->"executable" ```

Figure 45: Full Parsel program including header for Fig. 2 example, with the #### as the header separator. Note that we essentially just took the executability definition in [28] and added the list of available actions.

[MISSING_PAGE_EMPTY:44]

[MISSING_PAGE_FAIL:45]

HumanEval Prompts

We use the same zero-shot prompt to encourage the high-level sketch as in APPS. For translation we use:

```
1YouwillaimtosolvethefollowingprobleminParsel:
2{question}
3
4TranslatethefollowingsolutionplanintoParsel:
5{solution_start}{solution_text}
6
7YouwilltranslateasolutionplanforaproblemintoParsel.Eachlineshouldcontaineither
8
9Afunctiondescriptionshouldbeoftheform:
10"""
11functionname(arg1,arg2):Descriptionofthefunction
12"""
13
14Afunctionreferenceshouldbeoftheform:
15"""
16functionname
17"""
18
19Useindentationtoindicatedependenciesbetweenfunctions.Forexample,iffunctionAcalls
20Mathemsthatthetop-levelfunctionmatchesthenameofthefunctioninthesolutionplan. ```

Figure A.48: Translation prompt for GPT-4

Robotic Plan Evaluation Details

### Questionnaire

Our questionnaire closely follows that of Huang et al. [28]. We provide a figure with the directions for the accuracy version of the survey in the first image of Fig A.49. We include an example question in the second image. Note that each participant was shown a random 5 questions with their answers in random order. The clarity survey instead asks "For every question below, evaluate how easy it is to understand how the provided steps accomplish the task. Please rank the planned steps for each question from most understandable to least understandable (with 1 as the best and 3 as the worst)." In addition, for the clarity survey, each question text instead said "Rank the following plans based on which is the most understandable (1 = most understandable, 3 = least understandable)."

### Executability

We find that the automated executability check is a less insightful metric than human evaluation, as it doesn't meaningfully reflect the plan's likelihood of successfully completing a task. Unfortunately, the code to replicate the executability measure from Huang et al. [28] is unavailable. As an alternative, we developed our own executability checker using example code found on VirtualHome's GitHub repository, which evaluates if a proposed plan is syntactically accurate and can be executed within the VirtualHome environment. By leveraging Codex to generate eight Parsel programs for each of the 88 tasks and subsequently compiling them using the Parsel compiler, our method successfully produced executable solutions for all tasks. Conversely, Huang et al. [28] managed executable plans for only 86 tasks. However, it is worth noting that our Parsel compiler explicitly incorporates this executability measure as a constraint, which explains the higher executability rates observed in our approach.

Figure A.49: Screenshot of survey directions and example survey question. In this figure, the first answer was generated by the baseline, the second was the indented Parsel version, and the third was the unindented Parsel version. However, note that the order is randomized for each participant.

## Appendix Q Human Case Study

```
1main(input_string):Takesaninputline.First,splitsitatnewline,andstoresthesecondlineinavalraiblesandthethirdint.Splitssintwopartsattheasterisk,andcheckswhetherthestringtstartswiththefirstpartofsandendswiththesecond.part.Returnsthestring"YES"ifthandconditionismet,otherwise"N0".Also,itshouldalwaysreturn"N0"ifthelengthoftissmallerthanthesumoftthelengthofthepartsofs.
2"610\ncode#s\ncode#s\ncodeforces"->"YES"
3"610\ncodeforces\ncodeforces"->"YES"
4"610\ncode#s\ncodeforces"->"N0"
5"610\n*a\n*a\n*a\n*->"N0" ```

Figure A.51: Solution to https://codeforces.com/problemset/problem/529/B1main(input): Converts the input to an integer and returns the value of f of n.
2"1"->1
3"2"->3
4"3"->10
5f(n): First pre-computes the Pascal triangle up to n+1 using compute_pascal_triangle. - Then, returns the value of dp(n, pascal_triangle)
61->1
72->3
83->10
9compute_pascal_triangle(N): returns a matrix with N + 1 rows where m[i][j] corresponds - to "i choose k", i.e., the Pascal triangle. It is computed using dynamic - programming: m[i][j] = m[i-1][j] + m[i-1][j-1]. All elements are modulo (10**9 + )?). The i-th row has only i columns.
102->[[1],[1,1],[1,2,1]]
113->[[1],[1,1],[1,2,1],[1,3,3,1]]
12dp(n, pascal_triangle): first creates a list with (n + 1) zeros called L. Then fills - it in with the following dynamic programming relation: base case is L[0] = 1; - then, L[i] = sum (j in [1, i]) pascal_triangle[i-1][j-1] * L[i - j]. Finally, - returns the following answer: sum (k in [1, n]) pascal_triangle[n][k] * L[n - k]. - After each of these assignments, take modulo 10**9 + 7 to avoid big numbers.
131,[[1],[1,1],[1,2,1]] ->1
142,[[1],[1,1],[1,2,1]] ->3
153,[[1],[1,1],[1,2,1],[1,3,3,1]] ->10 ```

Figure A.52: Solution to https://codeforces.com/problemset/problem/568/B

Figure A.53: Solution to https://codeforces.com/problemset/problem/420/C

[MISSING_PAGE_EMPTY:51]

Parsel Language Nuances

### Syntax

**Descriptions.** A function description is represented as a function name followed by comma-separated input arguments in parentheses, and optionally an arrow followed by what the function returns4, then a colon and text describing the function to be implemented. For example, as part of Conway's Game of Life, one might write

Footnote 4: Note that in Parsel, for Python one can also indicate in the description that a function should yield a value or is a generator.

count_living_neighbors(grid, i, j): count the number of living neighbors of the cell at the -- index (i, j)

A function generated from a description can call either the functions defined directly below the description in the function graph (indicated with indentation) or references directly below the description5, both shown in Fig. 0(i).

Footnote 5: A nuance here is the optional ability for undefined/out-of-scope functions which are generated by the code LLM to also be implemented automatically.

**Constraints.** A constraint is represented as a function's input values comma-separated, optionally followed by an arrow and the expected output of the function. Constraints are provided at the same indentation as the preceding description. For example, after the definition of count_living_neighbors one can write

[[i, 0], [0, 1]], 0, 0 -> 1

[[i, 0], [0, 1]], 0, 1 -> 2

This indicates that count_living_neighbors should return 1 when called with the arguments [[1, 0], [0, 1]], 0, 0 and 2 when called with [[1, 0], [0, 1]], 0, 1. Notably, to apply complex constraints on functions, one can describe and constrain higher-order functions. For example:

type_fn_output(fn, args): returns the type of the output of a function called with args count_living_neighbors, ([[1, 0], [0, 1]], 0, 0) -> int

This indicates that the function count_living_neighbors should return an integer when called with the input arguments [[1, 0], [0, 1]], 0, 0.

What it means to satisfy constraints to validate a program varies from language to language: in Python, one can check that a program passes certain assert statements by evaluating them; however, in a theorem-proving language like Lean, where the ability to run a program (without skipping steps by using sorry or oops lines) shows that a proof holds, one would instead represent the formal proof statement as the specified constraint (that is, that you are actually proving what you set out to prove). For languages where correctness can be checked without any unit tests, their functions can be treated as also having implicit constraints.

**References.** A reference is simply the name of a function defined in the current scope (see the next paragraph for details) within the function graph. A reference allows and encourages (via prompt) the parent function to use the referenced function. This allows for recursive function definitions and functions called by multiple functions. For example, one can define an (overly verbose) version of the Collatz conjecture (a well-known recursive open question in mathematics) as shown in Figure 0(i), where the final line is a reference. We visualize the corresponding call graph and its strongly connected components (SCC) in Figure 0(i). In the Collatz functions, base_case is implemented first as the collatz_recursion SCC depends on it.

Figure 0(i).55: collatz_recursion call graph. There is a strongly connected component formed by collatz_recursion and recursion_rule.

### Headers

We also support program headers, allowing global contexts, used when implementing all new functions within a program. This is indicated by a line containing an optional string of special characters (e.g. "**#**#") separating the body and the text and is passed as a prefix to all prompts.

### Repeated Automatic Decomposition

As indicated by a rapidly growing number of papers [10; 28], the task of decomposing a task into steps in natural language is one that language models are surprisingly capable of. As explored in concurrent work [57], using language models to automatically and recursively decompose difficult open-ended problems to arbitrary depth is a powerful tool. Thus, we treat the ability to automatically decompose a Parsel function as a key feature of Parsel. This is an optional flag that prompts a language model to generate Parsel code corresponding to any additional subfunctions necessary when Parsel fails to implement a function. These proposed subfunctions are then added as child nodes to the decomposed function node. However, an additional consequence is that Parsel can thus be used to recursively decompose tasks into steps, by repeatedly identifying descriptions that cannot be directly implemented and attempting to decompose them.

### Scope.

Scope in Parsel is defined by indentation. The scope \(S\) of a function \(f\) includes the set of functions that can be used as a reference for a given function - that is, all functions where the indentations between the current function to the referenced function are strictly decreasing.

### Variations due to target language requirements.

Certain aspects of the implementation are still target-language specific. As discussed above, the meaning and representation of a constraint may vary by language. Moreover, every language has a different evaluation function: executing Python is different than compiling and running C++ code, which is different than checking a proof with Lean. Further, every language will likely require a different prompt for the language model. Thus, we detail these particularities in language-specific configuration files.

[MISSING_PAGE_EMPTY:54]

* [1] First, we need to figure out how to calculate the maximum sum of values of jewels for each size of knapsack. To do this, we can use a dynamic programming approach.
* [2]
* [3] We can create a two-dimensional array of size $n \times k$ where each row represents the different jewels and each column represents the different sizes of knapsack. The value of each cell in the array will represent the maximum sum of values of jewels that can be stolen for a given knapsack size.
* [4]
* [5] We can then iterate through the array, starting at the beginning. For each row, we can calculate the maximum sum of values of jewels that can be stolen for each knapsack size by taking the maximum of the value of the current cell and the value of the previous cell plus the value of the current jewel.
* [6]
* [7] At the end, we will have an array that contains the maximum sum of values of jewels that can be stolen for each knapsack size. We can then output these values as our answer.
* [8]
* [9] This algorithm is correct because we are calculating the maximum sum of values of jewels that can be stolen for each knapsack size by comparing the values of the current cell and the previous cell plus the value of the current jewel. This ensures that we are taking the maximum value for each knapsack size.

Figure A.57: Pipeline Figure Sketch

- jewels on the next lines, and returns the number of jewels, the knapsack sizes, and the
* [2]def parse_input(input_str):
* [3]lines = input_str.split('\n')
* [4]first_line = lines[0].split(' ')
* [5]num_jewels = int(first_line[0])
* [6]knapsack_sizes = int(first_line[1])
* [7]jewels = [[0, 0]]*num_jewels
* [8]for i in range(1, num_jewels + 1):
* [9]jewel = lines[1].split(' ')
* 1]= [int[jewel[0]], int(jewel[1])]
* [11]return num_jewels, knapsack_sizes, jewels
* [12]
* [13]#Returns the maximum sum of values of jewels that can be stolen for each knapsack size.
* [14]def max_sum_of_jewels_for_size_and_jewel(jewels, size, jewel):
* [15]if size < 0:
* [16]return 0
* [17]elif jewel < 0:
* [18]return 0
* [19]elif jewel == 0:
* [20]return jewels[jewel][1]if jewels[jewel][0]<= size else 0
* [21]elif jewels[jewel][0]>size:
* [22]return max_sum_of_jewels_for_size_and_jewel(jewels, size, jewel-1)
* [23]else:
* [24]return max(max_sum_of_jewels_for_size_and_jewel(jewels, size, jewel-1),
* [25]max_sum_of_jewels_for_size_and_jewel(jewels, size-jewels[jewel][0], jewel-1) + jewels[jewel][1])
* [26]
* [27]#Returns the maximum sum of values of jewels that can be stolen for each knapsack size.
* [28]def max_sum_of_jewels_for_size(jewels, size):
* [29]result = [] for s in range(1, size + 1):

* [31]return result
* [32]#Returns the string of the maximum sum of values of jewels that can be stolen for each knapsack size.
* [33]def to_output_str(max_sum_of_jewels_for_size):
* [34]return " ".join(map(str, max_sum_of_jewels_for_size))
* [35]#Returns the maximum sum of values of jewels that can be stolen for each knapsack size.
* [36]def max_sum_of_jewels(input_str):
* [37] ".
* [38] ".
* [39]def max_sum_of_jewels(input_str):
* [39] ".
* [40] ".
* [41] ".
* [42] ".
* [43] ".
* [44]number_of_jewels, knapsack_sizes, jewels = parse_input(input_str)
* [45]return to_output_str(max_sum_of_jewels_for_size(jewels, knapsack_sizes))
* 109 109')
* [47]assert compare_output(max_sum_of_jewels, '5 7\n2 2\n3 8\n2 7\n2 4\n3 8', '0 7 8 11 15 16 19')
* [48]assert compare_output(max_sum_of_jewels, '2 6\n300 1\n300 2', '0 0 0 0 0 0')

Figure 10: Pipeline Figure Sketch

Other Details

### Test Generation in HumanEval

For our test generation in our HumanEval experiments, we generated a set of tests by prompting GPT:

4 to "Generate an assert-based test for the following function. Answer with only a code block, and no other text. Do not wrap your asserts in a function.\(\backslash\)n" + question and then collecting and set-aggregating 100 completions.

### Backtracking

We also support backtracking for the Parsel implementation step, where we re-implement descendants by sampling new solutions for dependencies if a correct solution is not found for a parent. This is necessary to improve the robustness of some of the Appendix examples such as Figure A.14.

### Training Details

Although we do not train any models, all models used are discussed throughout the paper. See Appendix N for more details about sampling hyperparameters.

### Error Bars

We estimate a standard deviation of \(\pm 1.4\%\) for the best APPS result, given 1000 sampled problems.

### Compute

The most computationally intensive part of this research, by far (in terms of FLOPS), was the ablation using an open-source CodeGen model, which required several-hundred A100 hours. The rest of the inference was done through API calls.

### Generated Tokens

For the APPS evaluation, in terms of tokens generated, it is hard to compare the models directly: The CodeT paper does not specify the number of tokens decoded for their evaluation. Without more detail about their evaluations, it is impossible to confidently estimate the tokens generated per program for the CodeT evaluation. The AlphaCode results sample at most 768 tokens per solution, but they do not report average statistics directly - based on Figure 11 in the AlphaCode paper [35], the majority of its generated solutions are 150 to 350 tokens long after removing dead code and comments. The competition-level problems (that we evaluate on) require more tokens on average. For their best reported results, their figures indicate they sample at least 20 billion tokens for the competition-level subset of APPS. On the other hand, for our best results, Parsel generates (on average) 491 tokens of Python code per program implementation, and because we implement each high-level sketch in Python sixteen times (i.e. \(k=16\) in our best \(n\times k\)), we also sample on average 22 sketch tokens and 43 translation tokens per Python program implementation. Correspondingly, we sample roughly 7 million tokens for our APPS evaluation.

### Reproducibility

While our contribution is not a model or dataset, we have released our code.

### Evaluation Time

For a similar API rate limit and evaluation time, we expect our longest-running evaluation runtime would be fairly comparable to the 1,000-program AlphaCode evaluation (despite performing 3x better) and several times faster than the 50,000-program AlphaCode evaluation. In other words, we believe that generating 1,000 programs and evaluating them is not meaningfully faster than generating the 128 Parsel ones and then evaluating their combinations, even in the worst case. Our reasoning is as follows:_Generation time._ Because AlphaCode has no public-facing API, let us use the advertised rate limit for Codex (40,000 tokens per minute) as the generation speed. (This ignores the request per-minute limit and the number of input tokens in context.) For AlphaCode, assuming roughly 400 tokens per solution, generation would take 10 minutes plus the completion time (i.e., the latency from an API request to the result) of roughly 30 seconds. For Parsel, in our largest-scale experiment, we generate eight plans, their corresponding Parsel programs, and then each Parsel program's corresponding 16 implementations. At the same rate limit and program length, these generations would take about 1.5 minutes, plus three times the completion time. In other words, generation should take about 3 minutes for Parsel vs. 10.5 minutes for AlphaCode.

_Evaluation time._ Let's consider three evaluation scenarios, maintaining the 0.04-second implementation eval timeout used in the paper, run on a single 16-core machine. 1,000 evaluations. If we run 1,000 programs for each method, under the same constraints, their evaluation runtime should be the same. As indicated by Fig. 4-right, the Parsel performance would be substantially higher despite being over 3x faster overall. 10,000 evaluations. If we assume all programs take the maximum allowable runtime (the worst case for Parsel), then 10,000 evaluations takes 6.7 minutes for Parsel, compared to 40 seconds for 1,000 AlphaCode program evals. Parsel is still slightly faster, yet solves multiple times more problems. More evaluations. After this point, we run into the 2-minute per Parsel program limit (discussed previously), making Parsel slower than AlphaCode in the worst case -- but also solving over 3x as many problems.

_Other nuances._ With all this being said, this is a difficult (potentially impossible) comparison to make fairly for a few reasons. First, AlphaCode is worse than Codex, but likely faster, so it isn't clear which Parsel data point and which rate limit would make an appropriate comparison. Second, runtimes and problem lengths can vary substantially. For instance, we haven't considered input tokens so far, but these penalize AlphaCode more than Parsel, as AlphaCode inferences many more programs. Note that APPS problems are fairly long (the longest competition-level problem requires over 1800 tokens) so the effect of re-processing the problem statement at each program inference is substantial. Since the rate limit includes input tokens and there is a hard limit of 4,000 generated tokens per query, this decreases the effective rate limit by a third!