# Variational Imbalanced Regression: Fair Uncertainty Quantification via Probabilistic Smoothing

 Ziyan Wang

Georgia Institute of Technology

wzy@gatech.edu

&Hao Wang

Rutgers University

hw488@cs.rutgers.edu

###### Abstract

Existing regression models tend to fall short in both accuracy and uncertainty estimation when the label distribution is imbalanced. In this paper, we propose a probabilistic deep learning model, dubbed variational imbalanced regression (VIR), which not only performs well in imbalanced regression but naturally produces reasonable uncertainty estimation as a byproduct. Different from typical variational autoencoders assuming I.I.D. representations (a data point's representation is not directly affected by other data points), our VIR borrows data with similar regression labels to compute the latent representation's variational distribution; furthermore, different from deterministic regression models producing point estimates, VIR predicts the entire normal-inverse-gamma distributions and modulates the associated conjugate distributions to impose probabilistic reweighting on the imbalanced data, thereby providing better uncertainty estimation. Experiments in several real-world datasets show that our VIR can outperform state-of-the-art imbalanced regression models in terms of both accuracy and uncertainty estimation. Code will soon be available at https://github.com/Wang-ML-Lab/variational-imbalanced-regression.

## 1 Introduction

Deep regression models are currently the state of the art in making predictions in a continuous label space and have a wide range of successful applications in computer vision [50], natural language processing [22], healthcare [43, 45], recommender systems [16, 42], etc. However, these models fail however when the label distribution in training data is imbalanced. For example, in visual age estimation [30], where a model infers the age of a person given her visual appearance, models are typically trained on imbalanced datasets with overwhelmingly more images of younger adults, leading to poor regression accuracy for images of children or elderly people [48, 49]. Such unreliability in imbalanced regression settings motivates the need for both _improving performance for the minority_ in the presence of imbalanced data and, more importantly, _providing reasonable uncertainty estimation_ to inform practitioners on how reliable the predictions are (especially for the minority where accuracy is lower).

Existing methods for deep imbalanced regression (DIR) only focus on improving the accuracy of deep regression models by smoothing the label distribution and reweighting data with different labels [48, 49]. On the other hand, methods that provide uncertainty estimation for deep regression models operates under the balance-data assumption and therefore do not work well in the imbalanced setting [1, 8, 29].

To simultaneously cover these two desiderata, we propose a probabilistic deep imbalanced regression model, dubbed variational imbalanced regression (VIR). Different from typical variational autoencoders assuming I.I.D. representations (a data point's representation is not directly affected by other data points), our VIR assumes Neighboring and Identically Distributed (N.I.D.) and borrowsdata with similar regression labels to compute the latent representation's variational distribution. Specifically, VIR first encodes a data point into a probabilistic representation and then mix it with neighboring representations (i.e., representations from data with similar regression labels) to produce its final probabilistic representation; VIR is therefore particularly useful for minority data as it can borrow probabilistic representations from data with similar labels (and naturally weigh them using our probabilistic model) to counteract data sparsity. Furthermore, different from deterministic regression models producing point estimates, VIR predicts the entire Normal Inverse Gamma (NIG) distributions and modulates the associated conjugate distributions by the importance weight computed from the smoothed label distribution to impose probabilistic reweighting on the imbalanced data. This allows the negative log likelihood to naturally put more focus on the minority data, thereby balancing the accuracy for data with different regression labels. Our VIR framework is compatible with any deep regression models and can be trained end to end.

We summarize our contributions as below:

* We identify the problem of probabilistic deep imbalanced regression as well as two desiderata, balanced accuracy and uncertainty estimation, for the problem.
* We propose VIR to simultaneously cover these two desiderata and achieve state-of-the-art performance compared to existing methods.
* As a byproduct, we also provide strong baselines for benchmarking high-quality uncertainty estimation and promising prediction performance on imbalanced datasets.

## 2 Related Work

Variational Autoencoder.Variational autoencoder (VAE) [25] is an unsupervised learning model that aims to infer probabilistic representations from data. However, as shown in Figure 1, VAE typically assumes I.I.D. representations, where a data point's representation is not directly affected by other data points. In contrast, our VIR borrows data with similar regression labels to compute the latent representation's variational distribution.

Imbalanced Regression.Imbalanced regression is under-explored in the machine learning community. Most existing methods for imbalanced regression are direct extensions of the SMOTE algorithm [9], a commonly used algorithm for imbalanced classification, where data from the minority classes is over-sampled. These algorithms usually synthesize augmented data for the minority regression labels by either interpolating both inputs and labels [40] or adding Gaussian noise [5; 6] (more discussion on augmentation-based methods in the Appendix).

Such algorithms fail to measure the distance in continuous label space and fall short in handling high-dimensional data (e.g., images and text). Recently, DIR [49] addresses these issues by applying kernel density estimation to smooth and reweight data on the continuous label distribution, achieving state-of-the-art performance. However, DIR only focuses on improving the accuracy, especially for the data with minority labels, and therefore does not provide uncertainty estimation, which is crucial to assess the predictions' reliability. [32] focuses on re-balancing the mean squared error (MSE) loss for imbalanced regression, and [13] introduces ranking similarity for improving deep imbalanced regression. In contrast, our VIR provides a principled probabilistic approach to simultaneously achieve these two desiderata, not only improving upon DIR in terms of performance but also producing reasonable uncertainty estimation as a much-needed byproduct to assess model reliability. There is also related work on imbalanced classification [11], which is related to our work but focusing on classification rather than regression.

Figure 1: Comparing inference networks of typical VAE [25] and our VIR. In VAE (**left**), a data point’s latent representation (i.e., **z**) is affected only by itself, while in VIR (**right**), neighbors participate to modulate the final representation.

**Uncertainty Estimation in Regression.** There has been renewed interest in uncertainty estimation in the context of deep regression models [1; 12; 18; 23; 26; 29; 36; 37; 38; 51]. Most existing methods directly predict the variance of the output distribution as the estimated uncertainty [1; 23; 52], rely on post-hoc confidence interval calibration [26; 37; 51], or using Bayesian neural networks [44; 46; 47]; there are also training-free approaches, such as Infer Noise and Infer Dropout [29], which produce multiple predictions from different perturbed neurons and compute their variance as uncertainty estimation. Closest to our work is Deep Evidential Regression (DER) [1], which attempts to estimate both aleatoric and epistemic uncertainty [20; 23] on regression tasks by training the neural networks to directly infer the parameters of the evidential distribution, thereby producing uncertainty measures. DER [1] is designed for the data-rich regime and therefore fails to reasonably estimate the uncertainty if the data is imbalanced; for data with minority labels, DER [1] tends produce unstable distribution parameters, leading to poor uncertainty estimation (as shown in Sec. 5). In contrast, our proposed VIR explicitly handles data imbalance in the continuous label space to avoid such instability; VIR does so by modulating both the representations and the output conjugate distribution parameters according to the imbalanced label distribution, allowing training/inference to proceed as if the data is balance and leading to better performance as well as uncertainty estimation (as shown in Sec. 5).

## 3 Method

In this section we introduce the notation and problem setting, provide an overview of our VIR, and then describe details on each of VIR's key components.

### Notation and Problem Setting

Assuming an imbalanced dataset in continuous space \(\{\mathbf{x}_{i},y_{i}\}_{i=1}^{N}\) where \(N\) is the total number of data points, \(\mathbf{x}_{i}\in\mathbb{R}^{d}\) is the input, and \(y_{i}\in\mathcal{Y}\subset\mathbb{R}\) is the corresponding label from a continuous label space \(\mathcal{Y}\). In practice, \(\mathcal{Y}\) is partitioned into B equal-interval bins \([y^{(0)},y^{(1)}),[y^{(1)},y^{(2)}),...,[y^{(B-1)},y^{(B)})\), with slight notation overload. To directly compare with baselines, we use the same grouping index for target value \(b\in\mathcal{B}\) as in [49]. We denote representations as \(\mathbf{z}_{i}\), and use \((\bar{\mathbf{z}}_{i}^{\mu},\bar{\mathbf{z}}_{i}^{\bar{\sigma}})=q_{\phi}( \mathbf{z}|\mathbf{x}_{i})\) to denote the probabilistic representations for input \(\mathbf{x}_{i}\) generated by a probabilistic encoder parameterized by \(\phi\); furthermore, we denote \(\bar{\mathbf{z}}\) as the mean of representation \(\mathbf{z}_{i}\) in each bin, i.e., \(\bar{\mathbf{z}}=\frac{1}{N_{b}}\sum_{i=1}^{N_{b}}\mathbf{z}_{i}\) for a bin with \(N_{b}\) data points. Similarly we use \((\widehat{y}_{i},\widehat{s}_{i})\) to denote the mean and variance of the predictive distribution generated by a probabilistic predictor \(p_{\theta}(y_{i}|\mathbf{z})\).

### Method Overview

In order to achieve both desiderata in probabilistic deep imbalanced regression (i.e., performance improvement and uncertainty estimation), our proposed variational imbalanced regression (VIR) operates on both the encoder \(q_{\phi}(\mathbf{z}_{i}|\{\mathbf{x}_{i}\}_{i=1}^{N})\) and the predictor \(p_{\theta}(y_{i}|\mathbf{z}_{i})\).

Typical VAE [25] lower-bounds input \(\mathbf{x}_{i}\)'s marginal likelihood; in contrast, VIR lower-bounds the marginal likelihood of input \(\mathbf{x}_{i}\) and labels \(y_{i}\):

\[\log p_{\theta}(\mathbf{x}_{i},y_{i})=\mathcal{D}_{\mathcal{KL}}\big{(}q_{\phi }(\mathbf{z}_{i}|\{\mathbf{x}_{i}\}_{i=1}^{N})||p_{\theta}(\mathbf{z}_{i}| \mathbf{x}_{i},y_{i})\big{)}+\mathcal{L}(\theta,\phi;\mathbf{x}_{i},y_{i}).\]

Note that our variational distribution \(q_{\phi}(\mathbf{z}_{i}|\{\mathbf{x}_{i}\}_{i=1}^{N})\) (1) does not condition on labels \(y_{i}\), since the task is to predict \(y_{i}\) and (2) conditions on all (neighboring) inputs \(\{\mathbf{x}_{i}\}_{i=1}^{N}\) rather than just \(\mathbf{x}_{i}\). The second term \(\mathcal{L}(\theta,\phi;\mathbf{x}_{i},y_{i})\) is VIR's evidence lower bound (ELBO), which is defined as:

\[\mathcal{L}(\theta,\phi;\mathbf{x}_{i},y_{i})=\underbrace{\mathbb{E}_{q}\big{[} \log p_{\theta}(\mathbf{x}_{i}|\mathbf{z}_{i})\big{]}}_{\mathcal{L}_{i}^{ \mathcal{P}}}+\underbrace{\mathbb{E}_{q}\big{[}\log p_{\theta}(y_{i}|\mathbf{z} _{i})\big{]}}_{\mathcal{L}_{i}^{\mathcal{P}}}-\underbrace{\mathcal{D}_{ \mathcal{KL}}(q_{\phi}(\mathbf{z}_{i}|\{\mathbf{x}_{i}\}_{i=1}^{N})||p_{\theta }(\mathbf{z}_{i}))}_{\mathcal{L}_{i}^{\mathcal{KL}}}.\] (1)

Figure 2: Overview of our VIR method. **Left:** The inference model infers the latent representations given input \(\mathbf{x}\)’s in the neighborhood. **Right:** The generative model reconstructs the input and predicts the label distribution (including the associated uncertainty) given the latent representation.

where the \(p_{\theta}(\mathbf{z}_{i})\) is the standard Gaussian prior \(\mathcal{N}(\mathbf{0},\mathbf{I})\), following typical VAE [25], and the expectation is taken over \(q_{\phi}(\mathbf{z}_{i}|\{\mathbf{x}_{i}\}_{i=1}^{N})\), which infers \(\mathbf{z}_{i}\) by borrowing data with similar regression labels to produce the balanced probabilistic representations, which is beneficial especially for the minority (see Sec. 3.3 for details).

Different from typical regression models which produce only point estimates for \(y_{i}\), our VIR's predictor, \(p_{\theta}(y_{i}|\mathbf{z}_{i})\), directly produces the parameters of the entire NIG distribution for \(y_{i}\) and further imposes probabilistic reweighting on the imbalanced data, thereby producing balanced predictive distributions (more details in Sec. 3.4).

### Constructing \(q(\mathbf{z}_{i}|\{\mathbf{x}_{i}\}_{i=1}^{N})\)

To cover both desiderata, one needs to (1) produce _balanced_ representations to improve performance for the data with minority labels and (2) produce _probabilistic_ representations to naturally obtain reasonable uncertainty estimation for each model prediction. To learn such _balanced probabilistic representations_, we construct the encoder of our VIR (i.e., \(q_{\phi}(\mathbf{z}_{i}|\{\mathbf{x}_{i}\}_{i=1}^{N})\)) by (1) first encoding a data point into a **probabilistic representation**, (2) computing **probabilistic statistics** from neighboring representations (i.e., representations from data with similar regression labels), and (3) producing the final representations via **probabilistic whitening and recoloring** using the obtained statistics.

**Intuition on Using Probabilistic Representation.** DIR uses deterministic representations, with one vector as the final representation for each data point. In contrast, our VIR uses probabilistic representations, with one vector as the mean of the representation and another vector as the variance of the representation. Such dual representation is more robust to noise and therefore leads to better prediction performance. Therefore, We first encode each data point into a probabilistic representation. Note that this is in contrast to existing work [49] that uses deterministic representations. We assume that each encoding \(\mathbf{z}_{i}\) is a Gaussian distribution with parameters \(\{\mathbf{z}_{i}^{\mu},\mathbf{z}_{i}^{\Sigma}\}\), which are generated from the last layer in the deep neural network.

**From I.I.D. to Neighboring and Identically Distributed (N.I.D.).** Typical VAE [25] is an unsupervised learning model that aims to learn a variational representation from latent space to reconstruct the original inputs under the I.I.D. assumption; that is, in VAE, the latent value (i.e., \(\mathbf{z}_{i}\)) is generated from its own input \(\mathbf{x}_{i}\). This I.I.D. assumption works well for data with majority labels, but significantly harms performance for data with minority labels. To address this problem, we replace the I.I.D. assumption with the N.I.D. assumption; specifically, VIR's variational latent representations still follow Gaussian distributions (i.e., \(\mathcal{N}(\mathbf{z}_{i}^{\mu},\mathbf{z}_{i}^{\Sigma})\), but these distributions will be first calibrated using data with neighboring labels. For a data point \((\mathbf{x}_{i},y_{i})\) where \(y_{i}\) is in the \(b^{\prime}\)th bin, i.e., \(y_{i}\in[y^{(b-1)},y^{(b)})\), we compute \(q(\mathbf{z}_{i}|\{\mathbf{x}_{i}\}_{i=1}^{N})\triangleq\mathcal{N}(\mathbf{ z}_{i};\widetilde{\mathbf{z}}_{i}^{\mu},\widetilde{\mathbf{z}}_{i}^{\Sigma})\) with the following four steps.

**(1)** Mean and Covariance of Initial \(\mathbf{z}_{i}\):

\[\mathbf{z}_{i}^{\mu},\mathbf{z}_{i}^{\Sigma}=\mathcal{I}(\mathbf{x}_{i}),\]

**(2)** Statistics of Bin \(b\)'s Statistics:

\[\mathbf{z}_{b}^{\mu},\mathbf{\widetilde{\mu}}_{b}^{\Sigma},\mathbf{ \widetilde{\Sigma}}_{b}^{\mu},\mathbf{\widetilde{\Sigma}}_{b}^{\Sigma}= \mathcal{S}(\{\mathbf{z}_{b}^{\mu},\mathbf{\mu}_{b}^{\Sigma},\mathbf{\Sigma}_ {b}^{\mu},\mathbf{\Sigma}_{b}^{\Sigma}\}_{b=1}^{N}),\]

**(3)** Smoothed Statistics of Bin \(b\)'s Statistics:

\[\mathbf{\widetilde{\mu}}_{b}^{\mu},\mathbf{\widetilde{\mu}}_{b}^{\Sigma}, \mathbf{\widetilde{\Sigma}}_{b}^{\mu},\mathbf{\widetilde{\Sigma}}_{b}^{\Sigma}= \mathcal{S}(\{\mathbf{\mu}_{b}^{\mu},\mathbf{\mu}_{b}^{\Sigma},\mathbf{\Sigma} _{b}^{\mu},\mathbf{\Sigma}_{b}^{\Sigma}\}_{b=1}^{N}),\]

**(4)** Mean and Covariance of Final \(\mathbf{z}_{i}\):

\[\mathbf{\widetilde{z}}_{i}^{\mu},\mathbf{\widetilde{z}}_{i}^{\Sigma}= \mathcal{F}(\mathbf{z}_{i}^{\mu},\mathbf{z}_{i}^{\Sigma},\mathbf{\mu}_{b}^{\mu },\mathbf{\mu}_{b}^{\Sigma},\mathbf{\Sigma}_{b}^{\Sigma},\mathbf{\widetilde{ \Sigma}}_{b}^{\mu},\mathbf{\widetilde{\mu}}_{b}^{\Sigma},\mathbf{\widetilde{ \Sigma}}_{b}^{\mu},\mathbf{\widetilde{\Sigma}}_{b}^{\mu},\mathbf{\widetilde{ \Sigma}}_{b}^{\mu},\mathbf{\widetilde{\Sigma}}_{b}^{\Sigma}),\]

where the details of functions \(\mathcal{I}(\cdot)\), \(\mathcal{A}(\cdot)\), \(\mathcal{S}(\cdot)\), and \(\mathcal{F}(\cdot)\) are described below.

**(1) Function \(\mathcal{I}(\cdot)\): From Deterministic to Probabilistic Statistics.** Different from deterministic statistics in [49], our VIR's encoder uses _probabilistic statistics_, i.e., _statistics of statistics_. Specifically, VIR treats \(\mathbf{z}_{i}\) as a distribution with the mean and covariance \((\mathbf{z}_{i}^{\mu},\mathbf{z}_{i}^{\Sigma})=\mathcal{I}(\mathbf{x}_{i})\) rather than a deterministic vector.

As a result, all the deterministic statistics for bin \(b\), \(\bm{\mu}_{b}\), \(\bm{\Sigma}_{b}\), \(\widetilde{\bm{\mu}}_{b}\), and \(\widetilde{\bm{\Sigma}}_{b}\) are replaced by distributions with the means and covariances, \((\bm{\mu}_{b}^{\mu},\bm{\mu}_{b}^{\Sigma})\), \((\bm{\Sigma}_{b}^{\mu},\bm{\Sigma}_{b}^{\Sigma})\), \((\widetilde{\bm{\mu}}_{b}^{\mu},\widetilde{\bm{\mu}}_{b}^{\Sigma})\), and \((\widetilde{\bm{\Sigma}}_{b}^{\mu},\widetilde{\bm{\Sigma}}_{b}^{\Sigma})\), respectively (more details in the following three paragraphs on \(\mathcal{A}(\cdot)\), \(\mathcal{S}(\cdot)\), and \(\mathcal{F}(\cdot)\)).

**(2) Function \(\mathcal{A}(\cdot)\): Statistics of the Current Bin \(b\)'s Statistics.** In VIR, the _deterministic overall mean_ for bin \(b\) (with \(N_{b}\) data points), \(\bm{\mu}_{b}=\mathbf{\widetilde{z}}=\frac{1}{N_{b}}\sum_{i=1}^{N_{b}}\mathbf{z}_{i}\), becomes the _probabilistic overall mean_, i.e., a distribution of \(\bm{\mu}_{b}\) with the mean \(\bm{\mu}_{b}^{\mu}\) and covariance \(\bm{\mu}_{b}^{\Sigma}\) (assuming diagonal covariance) as follows:

\[\bm{\mu}_{b}^{\mu}\triangleq\mathbb{E}[\bar{\mathbf{z}}]=\frac{1}{N_{b}} \sum\nolimits_{i=1}^{N_{b}}\mathbb{E}[\mathbf{z}_{i}]=\frac{1}{N_{b}}\sum \nolimits_{i=1}^{N_{b}}\mathbf{z}_{i}^{\mu},\] \[\bm{\mu}_{b}^{\Sigma}\triangleq\mathbb{V}[\bar{\mathbf{z}}]=\frac {1}{N_{b}^{\Sigma}}\sum\nolimits_{i=1}^{N_{b}}\mathbb{V}[\mathbf{z}_{i}]= \frac{1}{N_{b}^{\Sigma}}\sum\nolimits_{i=1}^{N_{b}}\mathbf{z}_{i}^{\Sigma}.\]

Similarly, the _deterministic overall covariance_ for bin \(b\), \(\bm{\Sigma}_{b}=\frac{1}{N_{b}}\sum\nolimits_{i=1}^{N_{b}}(\mathbf{z}_{i}-\bar {\mathbf{z}})^{2}\), becomes the _probabilistic overall covariance_, i.e., a matrix-variate distribution [15] with the mean:

\[\bm{\Sigma}_{b}^{\mu}\triangleq\mathbb{E}[\bm{\Sigma}_{b}]=\frac{1}{N_{b}} \sum\nolimits_{i=1}^{N_{b}}\mathbb{E}[(\mathbf{z}_{i}-\bar{\mathbf{z}})^{2}] =\frac{1}{N_{b}}\sum\nolimits_{i=1}^{N_{b}}\Big{[}\mathbf{z}_{i}^{\Sigma}+( \mathbf{z}_{i}^{\mu})^{2}-\Big{(}\big{[}\bm{\mu}_{b}^{\Sigma}]_{i}+([\bm{\mu }_{b}^{\mu}]_{i})^{2}\Big{)}\Big{]},\]

since \(\mathbb{E}[\bar{\mathbf{z}}]=\bm{\mu}_{b}^{\mu}\) and \(\mathbb{V}[\bar{\mathbf{z}}]=\bm{\mu}_{b}^{\Sigma}\). Note that the covariance of \(\bm{\Sigma}_{b}\), i.e., \(\bm{\Sigma}_{b}^{\Sigma}\triangleq\mathbb{V}[\bm{\Sigma}_{b}]\), involves computing the fourth-order moments, which is computationally prohibitive. Therefore in practice, we directly set \(\bm{\Sigma}_{b}^{\Sigma}\) to zero for simplicity; empirically we observe that such simplified treatment already achieves promising performance improvement upon the state of the art. More discussions on the idea of the hierarchical structure of the statistics of statistics for smoothing are in the Appendix.

**(3) Function \(\mathcal{S}(\cdot)\): Neighboring Data and Smoothed Statistics.** Next, we can borrow data from neighboring label bins \(b^{\prime}\) to compute the smoothed statistics of the current bin \(b\) by applying a symmetric kernel \(k(\cdot,\cdot)\) (e.g., Gaussian, Laplacian, and Triangular kernels). Specifically, the _probabilistic smoothed mean and covariance_ are (assuming diagonal covariance):

\[\widetilde{\bm{\mu}}_{b}^{\mu}=\sum\nolimits_{b^{\prime}\in\mathcal{B}}k(y_{b },y_{b^{\prime}})\bm{\mu}_{b^{\prime}}^{\mu},\quad\widetilde{\bm{\mu}}_{b}^{ \Sigma}=\sum\nolimits_{b^{\prime}\in\mathcal{B}}k^{2}(y_{b},y_{b^{\prime}}) \bm{\mu}_{b^{\prime}}^{\Sigma},\quad\widetilde{\bm{\Sigma}}_{b}^{\mu}=\sum \nolimits_{b^{\prime}\in\mathcal{B}}k(y_{b},y_{b^{\prime}})\bm{\Sigma}_{b^{ \prime}}.\]

**(4) Function \(\mathcal{F}(\cdot)\): Probabilistic Whitening and Recoloring.** We develop a probabilistic version of the whitening and re-coloring procedure in [39; 49]. Specifically, we produce the final probabilistic representation \(\{\widetilde{\mathbf{z}}_{i}^{\mu},\widetilde{\mathbf{z}}_{i}^{\Sigma}\}\) for each data point as:

\[\widetilde{\mathbf{z}}_{i}^{\mu}=(\mathbf{z}_{i}^{\mu}-\bm{\mu}_{b}^{\mu}) \cdot\sqrt{\frac{\widetilde{\bm{\Sigma}}_{b}^{\mu}}{\widetilde{\bm{\Sigma}}_{ b}^{\mu}}}+\widetilde{\bm{\mu}}_{b}^{\mu},\quad\quad\widetilde{\mathbf{z}}_{i}^{ \Sigma}=(\mathbf{z}_{i}^{\Sigma}+\bm{\mu}_{b}^{\Sigma})\cdot\sqrt{\frac{ \widetilde{\bm{\Sigma}}_{b}^{\mu}}{\widetilde{\bm{\mu}}_{b}^{\mu}}}+\widetilde {\bm{\mu}}_{b}^{\Sigma}.\] (2)

During training, we keep updating the probabilistic overall statistics, \(\{\bm{\mu}_{b}^{\mu},\bm{\mu}_{b}^{\Sigma},\bm{\Sigma}_{b}^{\mu}\}\), and the probabilistic smoothed statistics, \(\{\widetilde{\bm{\mu}}_{b}^{\mu},\widetilde{\bm{\mu}}_{b}^{\Sigma},\widetilde{ \bm{\Sigma}}_{b}^{\mu}\}\), across different epochs. The probabilistic representation \(\{\widetilde{\mathbf{z}}_{i}^{\mu},\widetilde{\mathbf{z}}_{i}^{\Sigma}\}\) are then re-parameterized [25] into the final representation \(\mathbf{z}_{i}\), and passed into the final layer (discussed in Sec. 3.4) to generate the prediction and uncertainty estimation. Note that the computation of statistics from multiple \(\mathbf{x}\)'s is only needed during training. During testing, VIR directly uses these statistics and therefore does not need to re-compute them.

### Constructing \(p(y_{i}|\mathbf{z}_{i})\)

Our VIR's predictor \(p(y_{i}|\mathbf{z}_{i})\triangleq\mathcal{N}(y_{i};\widehat{y}_{i},\widehat{ s}_{i})\) predicts both the mean and variance for \(y_{i}\) by first predicting the NIG distribution and then marginalizing out the latent variables. It is motivated by the following observations on label distribution smoothing (LDS) in [49] and deep evidential regression (DER) in [1], as well as intuitions on effective counts in conjugate distributions.

**LDS's Limitations in Our Probabilistic Imbalanced Regression Setting.** The motivation of LDS [49] is that the empirical label distribution can not reflect the real label distribution in an imbalanced dataset with a continuous label space; consequently, reweighting methods for imbalanced regression fail due to these inaccurate label densities. By applying a smoothing kernel on the empirical label distribution, LDS tries to recover the effective label distribution, with which reweighting methods can obtain 'better' weights to improve imbalanced regression. However, in our probabilistic imbalanced regression, one needs to consider both (1) prediction accuracy for the data with minority labels and (2) uncertainty estimation for each model. Unfortunately, LDS only focuses on improving the accuracy, especially for the data with minority labels, and therefore does not provide uncertainty estimation, which is crucial to assess the predictions' reliability.

**DER's Limitations in Our Probabilistic Imbalanced Regression Setting.** In DER [1], the predicted labels with their corresponding uncertainties are represented by the approximate posterior parameters \((\gamma,\nu,\alpha,\beta)\) of the NIG distribution \(NIG(\gamma,\nu,\alpha,\beta)\). A DER model is trained via minimizing the negative log-likelihood (NLL) of a Student-t distribution:

\[\mathcal{L}_{i}^{DER}=\tfrac{1}{2}\log(\tfrac{\pi}{\nu})+(\alpha+\tfrac{1}{2}) \log((y_{i}-\gamma)^{2}\nu+\Omega)-\alpha\log(\Omega)+\log(\tfrac{\Gamma( \alpha)}{\Gamma(\alpha+\tfrac{1}{2})}),\] (3)where \(\Omega=2\beta(1+\nu)\). It is therefore nontrivial to properly incorporate a reweighting mechanism into the NLL. One straightforward approach is to directly reweight \(\mathcal{L}_{i}^{DER}\) for different data points \((\mathbf{x}_{i},y_{i})\). However, this contradicts the formulation of NIG and often leads to poor performance, as we verify in Sec. 5.

**Intuition of Pseudo-Counts for VIR.** To properly incorporate different reweighting methods, our VIR relies on the intuition of pseudo-counts (pseudo-observations) in conjugate distributions [4]. Assuming Gaussian likelihood, the _conjugate distributions_ would be an NIG distribution [4], i.e., \((\mu,\Sigma)\sim NIG(\gamma,\nu,\alpha,\beta)\), which means:

\[\mu\sim\mathcal{N}(\gamma,\Sigma/\nu),\;\;\;\Sigma\sim\Gamma^{-1}(\alpha,\beta),\]

where \(\Gamma^{-1}(\alpha,\beta)\) is an inverse gamma distribution. With an NIG prior distribution \(NIG(\gamma_{0},\nu_{0},\alpha_{0},\beta_{0})\), the posterior distribution of the NIG after observing \(n\) real data points \(\{u_{i}\}_{i=1}^{n}\) are:

\[\gamma_{n}=\tfrac{\gamma_{0}\iota_{0}+n\Psi}{\nu_{n}},\quad\nu_{n}=\nu_{0}+n, \quad\alpha_{n}=\alpha_{0}+\tfrac{n}{2},\quad\beta_{n}=\beta_{0}+\tfrac{1}{2 }(\gamma_{0}^{2}\nu_{0})+\Phi,\] (4)

where \(\Psi=\bar{u}\) and \(\Phi=\tfrac{1}{2}(\sum_{i}u_{i}^{2}-\gamma_{n}^{2}\nu_{n})\). Here \(\nu_{0}\) and \(\alpha_{0}\) can be interpreted as virtual observations, i.e., _pseudo-counts or pseudo-observations_ that contribute to the posterior distribution. Overall, the mean of posterior distribution above can be interpreted as an estimation from \((2\alpha_{0}+n)\) observations, with \(2\alpha_{0}\) virtual observations and \(n\) real observations. Similarly, the variance can be interpreted an estimation from \((\nu_{0}+n)\) observations. This intuition is crucial in developing our VIR's predictor.

**From Pseudo-Counts to Balanced Predictive Distributions.** Based on the intuition above, we construct our predictor (i.e., \(p(y_{i}|\mathbf{z}_{i})\)) by (1) generating the parameters in the posterior distribution of NIG, (2) computing re-weighted parameters by imposing the importance weights obtained from LDS, and (3) producing the final prediction with corresponding uncertainty estimation.

Based on Eqn. 4, we feed the final representation \(\{\mathbf{z}_{i}\}_{i=1}^{N}\) generated from the Sec. 3.3 (Eqn. 2) into a linear layer to output the intermediate parameters \(n_{i},\Psi_{i},\Phi_{i}\) for data point \((\mathbf{x}_{i},y_{i})\):

\[n_{i},\Psi_{i},\Phi_{i}=\mathcal{G}(\mathbf{z}_{i}),\quad\mathbf{z}_{i}\sim q (\mathbf{z}_{i}|\{\mathbf{x}_{i}\}_{i=1}^{N})=\mathcal{N}(\mathbf{z}_{i}; \widetilde{\mathbf{z}}_{i}^{\mu},\widetilde{\mathbf{z}}_{i}^{\Sigma})\]

We then apply the importance weights \(\big{(}\sum_{b^{\prime}\in\mathcal{B}}k(y_{b},y_{b^{\prime}})p(y_{b^{\prime}} )\big{)}^{-1/2}\) calculated from the smoothed label distribution to the _pseudo-count_\(n_{i}\) to produce the re-weighted parameters of posterior distribution of NIG, where \(p(y)\) denotes the marginal distribution of \(y\). Along with the pre-defined prior parameters \((\gamma_{0},\nu_{0},\alpha_{0},\beta_{0})\), we are able to compute the parameters of posterior distribution \(NIG(\gamma_{i},\nu_{i},\alpha_{i},\beta_{i})\) for \((\mathbf{x}_{i},y_{i})\):

\[\gamma_{i}^{*} =\tfrac{\gamma_{0}\iota_{0}+\big{(}\sum_{b^{\prime}\in\mathcal{B }}k(y_{b},y_{b^{\prime}})p(y_{b^{\prime}})\big{)}^{-1/2}\cdot n_{i}\Psi_{i}}{ \nu_{n}^{*}}, \nu_{i}^{*}=\nu_{0}+\big{(}\sum_{b^{\prime}\in\mathcal{B}}k(y_{b},y_{b^{ \prime}})p(y_{b^{\prime}})\big{)}^{-1/2}\cdot n_{i},\] \[\alpha_{i}^{*} =\alpha_{0}+\big{(}\sum_{b^{\prime}\in\mathcal{B}}k(y_{b},y_{b^{ \prime}})p(y_{b^{\prime}})\big{)}^{-1/2}\cdot\tfrac{n_{i}}{2}, \beta_{i}^{*}=\beta_{0}+\tfrac{1}{2}(\gamma_{0}^{2}\nu_{0})+\Phi_{i}.\]

Based on the NIG posterior distribution, we can then compute final prediction and uncertainty estimation as

\[\widehat{y}_{i}=\gamma_{i}^{*},\;\;\;\widehat{s}_{i}=\tfrac{\beta_{i}^{*}}{\nu _{i}^{*}(\alpha_{i}^{*}-1)}.\]

We use an objective function similar to Eqn. 3, but with different definitions of \((\gamma,\nu,\alpha,\beta)\), to optimize our VIR model:

\[\mathcal{L}_{i}^{\mathcal{P}}=\mathbb{E}_{q_{0}(\mathbf{z}_{i}|\{\mathbf{x}_{i }\}_{i=1}^{N})}[\tfrac{1}{2}\log(\tfrac{\pi}{\nu_{i}^{*}})+(\alpha_{i}^{*}+ \tfrac{1}{2})\log((y_{i}-\gamma_{i}^{*})^{2}\nu_{n}^{*}+\Omega)-\alpha_{i}^{*} \log(\omega_{i}^{*})+\log(\tfrac{\Gamma(\alpha_{i}^{*})}{\Gamma(\alpha_{i}^{*}+ \tfrac{1}{2})})],\] (5)

where \(\omega_{i}^{*}=2\beta_{i}^{*}(1+\nu_{i}^{*})\). Note that \(\mathcal{L}_{i}^{\mathcal{P}}\) is part of the ELBO in Eqn. 1. Similar to [1], we use an additional regularization term to achieve better accuracy :

\[\mathcal{L}_{i}^{\mathcal{R}}=(\nu+2\alpha)\cdot|y_{i}-\widehat{y}_{i}|.\]

\(\mathcal{L}_{i}^{\mathcal{P}}\) and \(\mathcal{L}_{i}^{\mathcal{R}}\) together constitute the objective function for learning the predictor \(p(\mathbf{y}_{i}|\mathbf{z}_{i})\).

### Final Objective Function

Putting together Sec. 3.3 and Sec. 3.4, our final objective function (to minimize) for VIR is:

\[\mathcal{L}^{\mathcal{VIR}}=\sum\nolimits_{i=1}^{N}\mathcal{L}_{i}^{\mathcal{ VIR}},\quad\mathcal{L}_{i}^{\mathcal{VIR}}=\lambda\mathcal{L}_{i}^{\mathcal{R}}- \mathcal{L}(\theta,\phi;\mathbf{x}_{i},y_{i})=\lambda\mathcal{L}_{i}^{\mathcal{ R}}-\mathcal{L}_{i}^{\mathcal{P}}-\mathcal{L}_{i}^{\mathcal{D}}+\mathcal{L}_{i}^{ \mathcal{KL}},\]

where \(\mathcal{L}(\theta,\phi;\mathbf{x}_{i},y_{i})=\mathcal{L}_{i}^{\mathcal{P}}+ \mathcal{L}_{i}^{\mathcal{D}}-\mathcal{L}_{i}^{\mathcal{KL}}\) is the ELBO in Eqn. 1. \(\lambda\) adjusts the importance of the additional regularizer and the ELBO, and thus lead to a better result both on accuracy and uncertainty estimation.

## 4 Theory

**Notation.** As mentioned in Sec. 3.1, we partitioned \(\{Y_{j}\}_{j=1}^{N}\) into \(|\mathcal{B}|\) equal-interval bins (denote the set of bins as \(\mathcal{B}\)), and \(\{Y_{j}\}_{j=1}^{N}\) are sampled from the label space \(\mathcal{Y}\). In addition, We use the binary set \(\{P_{i}\}_{i=1}^{|\mathcal{B}|}\) to represent the label distribution (frequency) for each bin \(i\), i.e., \(P_{i}\triangleq\mathbb{P}(Y_{j}\in\mathcal{B}_{i})\). We also use the binary set \(\{O_{i}\}_{j=1}^{N}\) to represent whether the data point \((\mathbf{x}_{j},y_{j})\) is observed (i.e., \(O_{j}\sim\mathbb{P}(O_{j}=1)\propto P_{B(Y_{j})}\), and \(\mathbb{E}_{O}[O_{j}]\sim P_{B(Y_{j})}\)), where \(B(Y_{j})\) represents the bin which \((x_{j},y_{j})\) belongs to. For each bin \(i\in\mathcal{B}\), we denote the associated set of data points as

\[\mathcal{U}_{i}\triangleq\{j:Y_{j}=i\}.\]

When the imbalanced dataset is partially observed, we denote the observation set as:

\[\mathcal{S}_{i}\triangleq\{j:O_{j}=1~{}~{}\&~{}~{}B(Y_{j})=i\}.\]

**Definition 4.1** (**Expectation over Observability \(\mathbb{E}_{O}\)**).: _We define the expectation over the observability variable \(O\) as \(\mathbb{E}_{O}[\cdot]\equiv\mathbb{E}_{O_{j}\sim\mathbb{P}(O_{j}=1)}[\cdot]\)._

**Definition 4.2** (**True Risk**).: _Based on the previous definitions, the true risk is defined as:_

\[R(\widehat{Y})=\frac{1}{|\mathcal{B}|}\sum_{i=1}^{|\mathcal{B}|}\frac{1}{| \mathcal{U}_{i}|}\sum_{j\in\mathcal{U}_{i}}\delta_{j}(Y,\widehat{Y}),\]

_where \(\delta_{j}(Y,\widehat{Y})\) refers to some loss function (e.g. MAE, MSE). In this paper, we assume the loss is upper bounded by \(\Delta\), i.e., \(0\leq\delta_{j}(Y,\widehat{Y})\leq\Delta\)._

Below we define the Naive Estimator.

**Definition 4.3** (**Naive Estimator**).: _Given the observation set, the Naive Estimator is defined as:_

\[\widehat{R}_{\mathrm{NAIVE}}(\widehat{Y})=\frac{1}{\sum_{i=1}^{|\mathcal{B}|}| \mathcal{S}_{i}|}\sum_{i=1}^{|\mathcal{B}|}~{}\sum_{j\in\mathcal{S}_{i}}\delta _{j}(Y,\widehat{Y}).\]

\begin{table}
\begin{tabular}{l|c c c c c c c} \hline \hline Metrics & \multicolumn{3}{c|}{MAR \(\downarrow\)} & \multicolumn{3}{c}{GM \(\downarrow\)} \\ \hline Shot & all & many & medium & few & all & many & medium & few \\ \hline VANILA [49] & 7.77 & 6.62 & 9.55 & 13.67 & 5.05 & 4.23 & 7.01 & 10.75 \\ VANIL [25] & 7.63 & 6.58 & 9.21 & 13.45 & 48.16 & 6.61 & 10.24 \\ Deep Ens. [27] & 7.33 & 6.62 & 9.37 & 13.09 & 48.7 & 47.37 & 6.91 & 11.15 \\ Inte Noise [29] & 8.53 & 7.62 & 9.13 & 13.82 & 5.57 & 49.5 & 6.88 & 10.86 \\ Smooth [40] & 8.16 & 7.39 & 8.65 & 12.82 & 45.62 & 5.69 & 8.49 \\ SMOON [5] & 8.26 & 7.64 & 9.01 & 6.50 & 4.99 & 6.14 & 8.44 \\ SMOON [49] & 7.81 & 7.16 & 8.90 & 12.19 & 4.99 & 4.57 & 5.73 & 7.77 \\ DER [11] & 8.09 & 7.31 & 8.99 & 12.66 & 4.99 & 4.69 & 8.40 & 10.49 \\ LDS [49] & 7.67 & 6.98 & 8.86 & 10.89 & 4.85 & 4.39 & 5.8 & 7.45 \\ GPS [49] & 7.69 & 7.10 & 8.86 & 9.98 & 4.84 & 4.91 & 5.97 & 6.29 \\ LDS + FDS [49] & 7.55 & 7.01 & 8.24 & 10.99 & 4.22 & 4.36 & 5.48 & 6.79 \\ RANSIM [13] & 7.02 & 6.49 & 7.84 & 6.85 & 4.13 & 5.37 & 6.89 \\ LDS + FDS - DER [11] & 8.18 & 7.44 & 9.52 & 11.53 & 5.30 & 5.47 & 6.74 & 7.68 \\ VIR (Ours) & **6.99** & **6.39** & **7.47** & **9.51** & **4.41** & **4.97** & **5.08** & **6.23** \\ \hline
**Ours vs. VANILA** & **4.03** & **4.03** & **4.04** & **4.04** & **4.16** & **4.16** & **4.36** & **4.52** \\
**Ours vs. Traffic Noise [29] & **4.51** & **4.23** & **4.23** & **4.31** & **4.16** & **4.08** & **4.13** & **4.53** \\
**Ours vs. DER** [11] & **4.19** & **4.21** & **4.25** & **4.31** & **4.16** & **4.08** & **4.13** & **4.26** \\
**Ours vs. DER** [11] & **4.19** & **4.52** & **4.31** & **4.16** & **4.08** & **4.13** & **4.26** & **4.35** \\
**Ours vs. DER** [11] & **4.25** & **4.31** & **4.17** & **4.12** & **4.06** & **4.32** & **4.46** & **4.14** \\
**Ours vs. DER** [11] & **4.32** & **4.31** & **4.32** & **4.17** & **4.12** & **4.06** & **4.32** & **4.46** \\
**Ours vs. RANSIM [4] & **4.03** & **4.10** & **4.37** & **4.17** & **4.12** & **4.06** & **4.32** & **4.46** \\ \hline \hline \end{tabular}
\end{table}
Table 1: Accuracy on ZOB-DIR.

\begin{table}
\begin{tabular}{l|c c c c c c c} \hline \hline Metrics & \multicolumn{3}{c|}{MAR \(\downarrow\)} & \multicolumn{3}{c}{GM \(\downarrow\)} \\ \hline Shot & all & many & medium & few & all & many & medium & few \\ \hline VANILA [49] & 8.06 & 7.23 & 15.12 & 26.33 & 4.57 & 4.17 & 10.59 & 20.46 \\ VANIL [25] & 8.04 & 7.20 & 15.06 & 28.30 & 45.7 & 4.22 & 10.66 & 20.72 \\ Deep Ens. [27] & 8.08 & 7.31 & 5.09 & 26.47 & 4.99 & 4.92 & 10.61 & 21.13 \\ Deep Noise [29] & 8.11 & 7.36 & 15.23 & 26.29 & 4.68 & 4.33 & 10.65 & 20.31 \\ Smooth [40] & 8.14 & 7.44 & 24.15 & 21.58 & 24.64 & 4.30 & 9.05 & 19.46 \\ SMOON [5] & 8.03 & 7.

It is easy to verify that the expectation of this naive estimator is not equal to the true risk, as \(\mathbb{E}_{O}[\widehat{R}_{\text{NAIVE}}(\widehat{Y})]\neq R(\widehat{Y})\).

Considering an imbalanced dataset as a subset of observations from a balanced one, we contrast it with the Inverse Propensity Score (IPS) estimator [34].

**Definition 4.4** (**Inverse Propensity Score Estimator**).: _The inverse propensity score (IPS) estimator (an unbiased estimator) is defined as_

\[\widehat{R}_{\text{IPS}}(\widehat{Y}|P)=\frac{1}{|\mathcal{B}|}\sum_{i=1}^{| \mathcal{B}|}\frac{1}{|\mathcal{U}_{i}|}\sum_{j\in\mathcal{S}_{i}}\frac{ \delta_{j}(Y,\widehat{Y})}{P_{i}}.\]

The IPS estimator is an unbiased estimator, as we can verify by taking the expectation value over the observation set:

\[\mathbb{E}_{O}[\widehat{R}_{\text{IPS}}(\widehat{Y}|P)] =\frac{1}{|\mathcal{B}|}\sum_{i=1}^{|\mathcal{B}|}\frac{1}{| \mathcal{U}_{i}|}\sum_{j\in\mathcal{U}_{i}}\frac{\delta_{j}(Y,\widehat{Y})}{P _{i}}\cdot\mathbb{E}_{O}[O_{j}]\] \[=\frac{1}{|\mathcal{B}|}\sum_{i=1}^{|\mathcal{B}|}\frac{1}{| \mathcal{U}_{i}|}\sum_{j\in\mathcal{U}_{i}}\delta_{j}(Y,\widehat{Y})=R( \widehat{Y}).\]

Finally, we define our VIR/DIR estimator below.

**Definition 4.5** (**VIR Estimator**).: _The VIR estimator, denoted by \(\widehat{R}_{\text{VIR}}(\widehat{Y}|\widetilde{P})\), is defined as:_

\[\widehat{R}_{\text{VIR}}(\widehat{Y}|\widetilde{P})=\frac{1}{|\mathcal{B}|} \sum_{i=1}^{|\mathcal{B}|}\frac{1}{|\mathcal{U}_{i}|}\sum_{j\in\mathcal{S}_{i} }\frac{\delta_{j}(Y,\widehat{Y})}{\widetilde{P}_{i}},\] (6)

_where \(\{\widetilde{P}_{i}\}_{i=1}^{|\mathcal{B}|}\) represents the smoothed label distribution used in our VIR's objective function. It is important to note that our VIR estimator is biased._

For multiple predictions, we select the "best" estimator according to the following definition.

**Definition 4.6** (**Empirical Risk Minimizer**).: _For a given hypothesis space \(\mathcal{H}\) of predictions \(\widehat{Y}\), the Empirical Risk Minimization (ERM) identifies the prediction \(\widehat{Y}\in\mathcal{H}\) as_

\[\widehat{Y}^{\text{ERM}}=\text{argmin}_{\widehat{Y}\in\mathcal{H}}\Big{\{} \widehat{R}_{\text{VIR}}(\widehat{Y}|\widetilde{P})\Big{\}}\]

With all the aforementioned definitions, we can derive the generalization bound for the VIR estimator.

**Theorem 4.1** (Generalization Bound of VIR).: _In imbalanced regression with bins \(\mathcal{B}\), for any finite hypothesis space of predictions \(\mathcal{H}=\{\widehat{Y}_{1},\dots,\widehat{Y}_{\mathcal{H}}\}\), the transductive prediction error of the empirical risk minimizer \(\widehat{Y}^{ERM}\) using the VIR estimator with estimated propensities \(\widetilde{P}\) (\(P_{i}>0\)) and given training observations \(O\) from \(\mathcal{Y}\) with independent Bernoulli propensities \(P\), is bounded by:_

\[R(\widehat{Y}^{ERM})\leq\widehat{R}_{\text{VIR}}(\widehat{Y}^{ERM}|\widetilde{ P})+\underbrace{\frac{\Delta}{|\mathcal{B}|}\sum_{i=1}^{|\mathcal{B}|}\left|1- \frac{P_{i}}{\widetilde{P}_{i}}\right|}_{\text{Bias }\overline{\text{Term}}}+\underbrace{\frac{\Delta}{|\mathcal{B}|}\sqrt{\frac{ \log(2|\mathcal{H}|/\eta)}{2}}\sqrt{\sum_{i=1}^{|\mathcal{B}|}\frac{1}{ \widetilde{P}_{i}^{2}}}}_{\text{Variance Term}}.\] (7)

**Remark.** The naive estimator (i.e., Definition 4.3) has large bias and large variance. If one directly uses the original label distribution in the training objective (i.e., Definition 4.4), i.e., \(\widetilde{P}_{i}=P_{i}\), the "bias" term will be \(0\). However, the "variance" term will be extremely large for minority data because \(\widetilde{P}_{i}\) is very close to \(0\). In contrast, under VIR's N.I.D., \(\widetilde{P}_{i}\) used in the training objective function will be smoothed. Therefore, the minority data's label density \(\widetilde{P}_{i}\) will be smoothed out by its neighbors and becomes larger (compared to the original \(P_{i}\)), leading to smaller "variance" in the generalization error bound. Note that \(\widetilde{P}_{i}\neq P_{i}\), VIR (with N.I.D.) essentially increases bias, but **significantly reduces** its variance in the imbalanced setting, thereby leading to a lower generalization error.

[MISSING_PAGE_FAIL:9]

upon the state-of-the-art method RankSim by \(9.6\%\) and \(7.9\%\) on AgeDB-DIR and IMDB-WIK-DIR, respectively, in terms of few-shot GM. This verifies the effectiveness of our methods in terms of overall performance. More accuracy results on different metrics are included in the Appendix. Besides the main results, we also include ablation studies for VIR in the Appendix, showing the effectiveness of VIR's encoder and predictor.

### Imbalanced Regression Uncertainty Estimation

Different from DIR [49] which only focuses on accuracy, we create a new benchmark for uncertainty estimation in imbalanced regression. Table 4, Table 5, and Table 6 show the results on uncertainty estimation for three datasets AgeDB-DIR, IMDB-WIK-DIR, and STS-B-DIR, respectively. Note that most baselines from Table 1, Table 2, and Table 3 are _deterministic_ methods (as opposed to probabilistic methods like ours) and _cannot provide uncertainty estimation_; therefore they are not applicable here. To show the superiority of our VIR model, we create a strongest baseline by concatenating the DIR variants (LDS + FDS) with the DER [1].

Results show that our VIR consistently outperforms all baselines across different metrics, especially in the few-shot metrics. Note that our proposed methods mainly focus on the imbalanced setting, and therefore naturally places more emphasis on the few-shot metrics. Notably, on AgeDB-DIR, IMDB-WIKI-DIR, and STS-B-DIR, our VIR improves upon the strongest baselines, by \(14.2\%\sim 17.1\%\) in terms of few-shot AVSE.

### Limitations

Although our methods successfully improve both accuracy and uncertainty estimation on imbalanced regression, there are still several limitations. Exactly computing _variance of the variances_ in Sec. 3.3 is challenging; we therefore resort to fixed variance as an approximation. Developing more accurate and efficient approximations would also be interesting future work.

## 6 Conclusion

We identify the problem of probabilistic deep imbalanced regression, which aims to both improve accuracy and obtain reasonable uncertainty estimation in imbalanced regression. We propose VIR, which can use any deep regression models as backbone networks. VIR borrows data with similar regression labels to produce the probabilistic representations and modulates the conjugate distributions to impose probabilistic reweighting on imbalanced data. Furthermore, we create new benchmarks with strong baselines for uncertainty estimation on imbalanced regression. Experiments show that our methods outperform state-of-the-art imbalanced regression models in terms of both accuracy and uncertainty estimation. Future work may include (1) improving VIR by better approximating _variance of the variances_ in probability distributions, and (2) developing novel approaches that can achieve stable performance even on imbalanced data with limited sample size, and (3) exploring techniques such as mixture density networks [3] to enable multi-modality in the latent distribution, thereby further improving the performance.

\begin{table}
\begin{tabular}{l|c c c c|c c c c} \hline \hline Metrics & \multicolumn{4}{c|}{NLL \(\downarrow\)} & \multicolumn{4}{c}{AVSE \(\downarrow\)} \\ \hline Shot & All & Many & Medium & Few & All & Many & Medium & Few \\ \hline Deep Exs. [27] & 3.913 & 3.911 & 4.223 & 4.106 & 0.709 & 0.621 & 0.676 & 0.663 \\ DIRR Jonge [29] & 3.748 & 3.753 & 3.753 & 3.568 & 0.673 & 0.631 & 0.644 & 0.639 \\ DER [1] & 2.667 & 2.661 & 3.013 & 2.401 & 0.682 & 0.583 & 0.613 & 0.624 \\ LDS + FDS + DER [1] & 2.561 & 2.514 & 2.880 & 2.398 & 0.672 & 0.581 & 0.609 & 0.615 \\
**VIR (Overs)** & **1.996** & **1.810** & **2.754** & **2.152** & **0.591** & **0.575** & **0.602** & **0.510** \\ \hline \hline
**Overs** \(\backslash\) DER & **4.671** & **4.791** & **4.259** & **4.204** & **4.091** & **4.098** & **4.091** & **4.011** \\
**Overs** \(\backslash\) LDS + FDS + DER & **4.556** & **4.704** & **4.125** & **4.206** & **4.091** & **4.098** & **4.097** & **4.105** \\ \hline \hline \end{tabular}
\end{table}
Table 6: Uncertainty estimation on STS-B-DIR.

\begin{table}
\begin{tabular}{l|c c c|c c c} \hline \hline Metrics & \multicolumn{4}{c|}{NLL \(\downarrow\)} & \multicolumn{4}{c}{AVSE \(\downarrow\)} \\ \hline Shot & All & Many & Medium & Few & All & Many & Medium & Few \\ \hline Deep Exs. [27] & 5.219 & 4.122 & 5.823 & 6.824 & 6.082 & 0.764 & 0.743 \\ DERs Note [29] & 4.162 & 4.143 & 4.243 & 5.338 & 0.262 & 0.732 & 0.671 & 0.746 \\ DER [1] & 3.595 & 3.699 & 4.997 & 4.957 & 0.541 & 0.631 & 0.630 & 0.634 \\ LDS + FDS + DRR [1] & 3.872 & 3.897 & 3.951 & 4.224 & 0.681 & 0.640 & 0.599 & 0.643 \\
**VIR (Overs)** & **3.598** & **3.506** & **4.082** & **4.040** & **4.042** & **4.041** & **4.047** \\ \hline \hline
**Overs** \(\backslash\) DER & **4.115** & **4.104** & **4.104** & **4.243** & **4.455** & **4.098** & **4.098** & **4.094** \\
**Overs** \(\backslash\) LDS + FDS + DER & **4.403** & **4.107** & **4.107** & **4.038** & **4.147** & **4.041** & **4.095** & **4.151** \\ \hline \hline \end{tabular}
\end{table}
Table 5: Uncertainty estimation on IW-DIR.

## Acknowledgement

The authors thank Pei Wu for help with figures, the reviewers/AC for the constructive comments to improve the paper, and Amazon Web Service for providing cloud computing credit. Most work is done when ZW is a master student at UMich. HW is partially supported by NSF Grant IIS-2127918 and an Amazon Faculty Research Award. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of the sponsors.

## References

* [1] A. Amini, W. Schwarting, A. Soleimany, and D. Rus. Deep evidential regression. _Advances in Neural Information Processing Systems_, 33:14927-14937, 2020.
* [2] J. Bergstra and Y. Bengio. Random search for hyper-parameter optimization. _Journal of machine learning research_, 13(2), 2012.
* [3] C. M. Bishop. Mixture density networks. 1994.
* [4] C. M. Bishop. _Pattern recognition and machine learning_. springer, 2006.
* [5] P. Branco, L. Torgo, and R. P. Ribeiro. Smogn: a pre-processing approach for imbalanced regression. In _First international workshop on learning with imbalanced domains: Theory and applications_, pages 36-50. PMLR, 2017.
* [6] P. Branco, L. Torgo, and R. P. Ribeiro. Rebagg: Resampled bagging for imbalanced regression. In _Second International Workshop on Learning with Imbalanced Domains: Theory and Applications_, pages 67-81. PMLR, 2018.
* [7] D. Cer, M. Diab, E. Agirre, I. Lopez-Gazpio, and L. Specia. Semeval-2017 task 1: Semantic textual similarity multilingual and crosslingual focused evaluation. In _Proceedings of the 11th International Workshop on Semantic Evaluation_, pages 1-14, 2017.
* [8] B. Charpentier, O. Borchert, D. Zugner, S. Geisler, and S. Gunnemann. Natural posterior network: Deep bayesian predictive uncertainty for exponential family distributions. In _The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022_. OpenReview.net, 2022.
* [9] N. V. Chawla, K. W. Bowyer, L. O. Hall, and W. P. Kegelmeyer. Smote: synthetic minority over-sampling technique. _Journal of artificial intelligence research_, 16:321-357, 2002.
* [10] T. Chen, S. Kornblith, M. Norouzi, and G. Hinton. A simple framework for contrastive learning of visual representations. In _International conference on machine learning_, pages 1597-1607. PMLR, 2020.
* [11] Z. Deng, H. Liu, Y. Wang, C. Wang, Z. Yu, and X. Sun. PML: progressive margin loss for long-tailed age classification. In _IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual, June 19-25, 2021_, 2021.
* [12] Y. Gal and Z. Ghahramani. Dropout as a bayesian approximation: Representing model uncertainty in deep learning. In _Proceedings of the 33nd International Conference on Machine Learning, ICML 2016, New York City, NY, USA, June 19-24, 2016_, volume 48 of _JMLR Workshop and Conference Proceedings_, pages 1050-1059. JMLR.org, 2016.
* [13] Y. Gong, G. Mori, and F. Tung. Ranksim: Ranking similarity regularization for deep imbalanced regression. In _International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA_, volume 162 of _Proceedings of Machine Learning Research_, pages 7634-7649. PMLR, 2022.
* [14] C. Guo, G. Pleiss, Y. Sun, and K. Q. Weinberger. On calibration of modern neural networks. In _International conference on machine learning_, pages 1321-1330. PMLR, 2017.
* [15] A. K. Gupta and D. K. Nagar. _Matrix variate distributions_, volume 104. CRC Press, 2018.

* [16] S. Gupta, H. Wang, Z. Lipton, and Y. Wang. Correcting exposure bias for link recommendation. In _ICML_, 2021.
* [17] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In _2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016_, pages 770-778. IEEE Computer Society, 2016.
* [18] J. Heiss, J. Weissteiner, H. Wutte, S. Seuken, and J. Teichmann. Nomu: Neural optimization-based model uncertainty. _arXiv preprint arXiv:2102.13640_, 2021.
* [19] Z. Huang, W. Xu, and K. Yu. Bidirectional LSTM-CRF models for sequence tagging. _CoRR_, 2015.
* [20] E. Hullermeier and W. Waegeman. Aleatoric and epistemic uncertainty in machine learning: A tutorial introduction. _CoRR_, abs/1910.09457, 2019.
* ECCV 2018
- 15th European Conference, Munich, Germany, September 8-14, 2018, Proceedings, Part VII_, volume 11211 of _Lecture Notes in Computer Science_, pages 677-693. Springer, 2018.
* [22] H. Jiang, P. He, W. Chen, X. Liu, J. Gao, and T. Zhao. SMART: robust and efficient fine-tuning for pre-trained natural language models through principled regularized optimization. In D. Jurafsky, J. Chai, N. Schluter, and J. R. Tetreault, editors, _Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020_, pages 2177-2190. Association for Computational Linguistics, 2020.
* [23] A. Kendall and Y. Gal. What uncertainties do we need in bayesian deep learning for computer vision? _arXiv preprint arXiv:1703.04977_, 2017.
* [24] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. In _3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings_, 2015.
* [25] D. P. Kingma and M. Welling. Auto-encoding variational bayes. In _2nd International Conference on Learning Representations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings_, 2014.
* [26] V. Kuleshov, N. Fenner, and S. Ermon. Accurate uncertainties for deep learning using calibrated regression. In _International Conference on Machine Learning_, pages 2796-2804. PMLR, 2018.
* [27] B. Lakshminarayanan, A. Pritzel, and C. Blundell. Simple and scalable predictive uncertainty estimation using deep ensembles. In _Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA_, pages 6402-6413, 2017.
* [28] Z. Liu, Z. Miao, X. Zhan, J. Wang, B. Gong, and S. X. Yu. Large-scale long-tailed recognition in an open world. In _IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019_, pages 2537-2546. Computer Vision Foundation / IEEE, 2019.
* [29] L. Mi, H. Wang, Y. Tian, and N. Shavit. Training-free uncertainty estimation for neural networks. In _AAAI_, 2022.
* [30] S. Moschoglou, A. Papaioannou, C. Sagonas, J. Deng, I. Kotsia, and S. Zafeiriou. Agedb: The first manually collected, in-the-wild age database. In _2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops, CVPR Workshops 2017, Honolulu, HI, USA, July 21-26, 2017_, pages 1997-2005, 2017.
* [31] J. Pennington, R. Socher, and C. D. Manning. Glove: Global vectors for word representation. In _Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)_, pages 1532-1543, 2014.

* [32] J. Ren, M. Zhang, C. Yu, and Z. Liu. Balanced mse for imbalanced visual regression. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022_, pages 7916-7925. IEEE, 2022.
* [33] R. Rothe, R. Timofte, and L. V. Gool. Deep expectation of real and apparent age from a single image without facial landmarks. _Int. J. Comput. Vis._, 126(2-4):144-157, 2018.
* [34] T. Schnabel, A. Swaminathan, A. Singh, N. Chandak, and T. Joachims. Recommendations as treatments: Debiasing learning and evaluation. In _international conference on machine learning_, pages 1670-1679. PMLR, 2016.
* [35] N. Silberman, D. Hoiem, P. Kohli, and R. Fergus. Indoor segmentation and support inference from rgbd images. _ECCV (5)_, 7576:746-760, 2012.
* [36] J. Snoek, Y. Ovadia, E. Fertig, B. Lakshminarayanan, S. Nowozin, D. Sculley, J. V. Dillon, J. Ren, and Z. Nado. Can you trust your model's uncertainty? evaluating predictive uncertainty under dataset shift. In _Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada_, pages 13969-13980, 2019.
* [37] H. Song, T. Diethe, M. Kull, and P. Flach. Distribution calibration for regression. In _International Conference on Machine Learning_, pages 5897-5906. PMLR, 2019.
* [38] M. Stadler, B. Charpentier, S. Geisler, D. Zugner, and S. Gunnemann. Graph posterior network: Bayesian predictive uncertainty for node classification. In _Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual_, pages 18033-18048, 2021.
* [39] B. Sun, J. Feng, and K. Saenko. Return of frustratingly easy domain adaptation. In _Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence, February 12-17, 2016, Phoenix, Arizona, USA_, pages 2058-2065. AAAI Press, 2016.
* 16th Portuguese Conference on Artificial Intelligence, EPIA 2013, Angra do Heroismo, Azores, Portugal, September 9-12, 2013. Proceedings_, volume 8154 of _Lecture Notes in Computer Science_, pages 378-389. Springer, 2013.
* [41] A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. R. Bowman. Glue: A multi-task benchmark and analysis platform for natural language understanding. _EMNLP 2018_, page 353, 2018.
* [42] H. Wang, Y. Ma, H. Ding, and Y. Wang. Context uncertainty in contextual bandits with applications to recommender systems. In _AAAI_, 2022.
* [43] H. Wang, C. Mao, H. He, M. Zhao, T. S. Jaakkola, and D. Katabi. Bidirectional inference networks: A class of deep bayesian networks for health profiling. In _AAAI_, volume 33, pages 766-773, 2019.
* [44] H. Wang, S. Xingjian, and D.-Y. Yeung. Natural-parameter networks: A class of probabilistic neural networks. In _NIPS_, pages 118-126, 2016.
* [45] H. Wang and J. Yan. Self-interpretable time series prediction with counterfactual explanations. In _ICML_, 2023.
* [46] H. Wang and D.-Y. Yeung. Towards bayesian deep learning: A framework and some existing methods. _TDKE_, 28(12):3395-3408, 2016.
* [47] H. Wang and D.-Y. Yeung. A survey on bayesian deep learning. _CSUR_, 53(5):1-37, 2020.
* [48] Y. Yang, H. Wang, and D. Katabi. On multi-domain long-tailed recognition, generalization and beyond. In _ECCV_, 2022.
* [49] Y. Yang, K. Zha, Y. Chen, H. Wang, and D. Katabi. Delving into deep imbalanced regression. In _International Conference on Machine Learning_, pages 11842-11851. PMLR, 2021.

* [50] W. Yin, J. Zhang, O. Wang, S. Niklaus, L. Mai, S. Chen, and C. Shen. Learning to recover 3d scene shape from a single image. In _IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual, June 19-25, 2021_, pages 204-213. Computer Vision Foundation / IEEE, 2021.
* [51] E. Zelikman, C. Healy, S. Zhou, and A. Avati. Crude: calibrating regression uncertainty distributions empirically. _arXiv preprint arXiv:2005.12496_, 2020.
* [52] Z. Zhang, A. Romero, M. J. Muckley, P. Vincent, L. Yang, and M. Drozdzal. Reducing uncertainty in undersampled mri reconstruction with active acquisition. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 2049-2058, 2019.

Theory

**Notation.** As defined in Sec.3 in main paper, we partitioned \(\{Y_{j}\}_{j=1}^{N}\) into \(|\mathcal{B}|\) equal-interval bins (denote the set of bins as \(\mathcal{B}\)), and \(\{Y_{j}\}_{j=1}^{N}\) are sampled from the label space \(\mathcal{Y}\). In addition, We denote the binary set \(\{P_{i}\}_{i=1}^{|\mathcal{B}|}\) as the label distribution (frequency) for each bin, i.e., \(P_{i}\triangleq\mathbb{P}(Y_{j}\in\mathcal{B}_{i})\). We also denote the binary set \(\{O_{i}\}_{j=1}^{N}\) to represent whether the data \(\{\mathbf{x}_{j},y_{j}\}\) are observed (i.e., \(O_{j}\sim\mathbb{P}(O_{j}=1)\propto P_{B(Y_{j})}\), and \(\mathbb{E}_{O}[O_{j}]\sim P_{B(Y_{j})}\)), where \(B(Y_{j})\) represents the bin which \((x_{j},y_{j})\) belongs to. For each bin \(i\in\mathcal{B}\), we denote the global set of samples as

\[\mathcal{U}_{i}\triangleq\{j:Y_{j}=i\}.\]

When the imbalanced dataset is partially observed, we denote the observation set as:

\[\mathcal{S}_{i}\triangleq\{j:O_{j}=1\ \ \&\ \ B(Y_{j})=i\}.\]

**Definition A.1** (Expectation over Observability \(\mathbb{E}_{O}\)).: _We define the expectation over the observability variable \(O\) as \(\mathbb{E}_{O}[\cdot]\equiv\mathbb{E}_{O_{j}\sim\mathbb{P}(O_{j}=1)}[\cdot]\)_

**Definition A.2** (**True Risk**).: _Based on the previous definitions, the true risk for our model is defined as:_

\[R(\widehat{Y})=\frac{1}{|\mathcal{B}|}\sum_{i=1}^{|\mathcal{B}|}\frac{1}{| \mathcal{U}_{i}|}\sum_{j\in\mathcal{U}_{i}}\delta_{j}(Y,\widehat{Y}),\]

_where \(\delta_{j}(Y,\widehat{Y})\) refers to some loss function (e.g. MAE, MSE). In this paper we assume these loss is upper bounded by \(\Delta\), i.e., \(0\leq\delta_{j}(Y,\widehat{Y})\leq\Delta\)._

Then in next step we define the Naive Estimator.

**Definition A.3** (**Naive Estimator**).: _Given the observation set, the Naive Estimator is defined as:_

\[\widehat{R}_{\mathrm{NAIVE}}(\widehat{Y})=\frac{1}{\sum_{i=1}^{|\mathcal{B}|}| \mathcal{S}_{i}|}\sum_{i=1}^{|\mathcal{B}|}\ \sum_{j\in\mathcal{S}_{i}}\delta_{j}(Y,\widehat{Y})\]

_It is easy to verify that the expectation of this naive estimator is not equal to the true risk, as \(\mathbb{E}_{O}[\widehat{R}_{\mathrm{NAIVE}}(\widehat{Y})]\neq R(\widehat{Y})\)._

Considering an imbalanced dataset as a subset of observations from a balanced one, we contrast it with the Inverse Propensity Score (IPS) estimator [34], underscoring the superiorities of our approach.

**Definition A.4** (**Inverse Propensity Score Estimator**).: _The inverse propensity score (IPS) estimator (an unbiased estimator) is defined as_

\[\widehat{R}_{\mathrm{IPS}}(\widehat{Y}|P)=\frac{1}{|\mathcal{B}|}\sum_{i=1}^{ |\mathcal{B}|}\frac{1}{|\mathcal{U}_{i}|}\sum_{j\in\mathcal{S}_{i}}\frac{ \delta_{j}(Y,\widehat{Y})}{P_{i}}.\]

IPS estimator is an unbiased estimator, as we can verify by taking the expectation value over the observation set:

\[\mathbb{E}_{O}[\widehat{R}_{\mathrm{IPS}}(\widehat{Y}|P)] =\frac{1}{|\mathcal{B}|}\sum_{i=1}^{|\mathcal{B}|}\frac{1}{| \mathcal{U}_{i}|}\sum_{j\in\mathcal{U}_{i}}\frac{\delta_{j}(Y,\widehat{Y})}{P _{i}}\cdot\mathbb{E}_{O}[O_{j}]\] \[=\frac{1}{|\mathcal{B}|}\sum_{i=1}^{|\mathcal{B}|}\frac{1}{| \mathcal{U}_{i}|}\sum_{j\in\mathcal{U}_{i}}\delta_{j}(Y,\widehat{Y})=R( \widehat{Y}).\]

Finally, we define our VIR/DIR estimator below.

**Definition A.5** (**VIR Estimator**).: _The VIR estimator, denoted by \(\widehat{R}_{\mathrm{VIR}}(\widehat{Y}|\widetilde{P})\), is defined as:_

\[\widehat{R}_{\mathrm{VIR}}(\widehat{Y}|\widetilde{P})=\frac{1}{|\mathcal{B}|} \sum_{i=1}^{|\mathcal{B}|}\frac{1}{|\mathcal{U}_{i}|}\sum_{j\in\mathcal{S}_{i}} \frac{\delta_{j}(Y,\widehat{Y})}{\widetilde{P}_{i}},\] (8)

_where \(\{\widetilde{P}_{i}\}_{i=1}^{|\mathcal{B}|}\) represents the smoothed label distribution utilized in our VIR's objective function (see Eqn.5 in the main paper). It's important to note that our VIR estimator is biased._For multiple predictions, we select the "best" estimator according to the following definition.

**Definition A.6** (**Empirical Risk Minimizer**).: _For a given hypothesis space \(\mathcal{H}\) of predictions \(\widehat{Y}\), the Empirical Risk Minimization (ERM) identifies the prediction \(\widehat{Y}\in\mathcal{H}\) as_

\[\widehat{Y}^{\text{ERM}}=\text{argmin}_{\widehat{Y}\in\mathcal{H}}\Big{\{} \widehat{R}_{\text{VIR}}(\widehat{Y}|\widetilde{P})\Big{\}}\]

**Lemma A.1** (**Tail Bound for VIR Estimator**).: _For any given \(\widehat{Y}\) and \(Y\), with probability \(1-\eta\), the VIR estimator \(\widehat{R}_{\text{VIR}}(\widehat{Y}|\widetilde{P})\) does not deviate from its expectation \(\mathbb{E}_{O}[\widehat{R}_{\text{VIR}}(\widehat{Y}^{ERM}|\widetilde{P})]\) by more than_

\[\left|\widehat{R}_{\text{VIR}}(\widehat{Y}^{ERM}|\widetilde{P})-\mathbb{E}_{O }[\widehat{R}_{\text{VIR}}(\widehat{Y}^{ERM}|\widetilde{P})]\right|\leq\frac {\Delta}{|\mathcal{B}|}\sqrt{\frac{\log(2|\mathcal{H}|/\eta)}{2}}\sqrt{\sum_{i =1}^{|\mathcal{B}|}\frac{1}{\widehat{P}_{i}^{2}}}.\]

Proof.: For independent bounded random variables \(X_{1},\cdots,X_{n}\) that takes values in intervals of sizes \(\rho_{1},\cdots,\rho_{n}\) with probability \(1\), and for any \(\epsilon>0\),

\[\mathbb{P}\Bigg{(}\bigg{|}\sum_{i}^{n}X_{i}-\mathbb{E}\Big{[}\sum_{i}^{n}X_{i }\Big{]}\bigg{|}\geq\epsilon\Bigg{)}\leq 2\exp\Big{(}\frac{-2\epsilon^{2}}{ \sum_{i}^{n}\rho_{i}^{2}}\Big{)}\]

Consider the error term for each bin \(i\) in \(\widehat{R}_{\text{VIR}}(\widehat{Y}^{ERM}|\widetilde{P})\) as \(X_{i}\). Using Hoeffding's inequality, define \(\mathbb{P}(X_{i}=\frac{\delta_{j}(Y,\widehat{Y})}{\widehat{P}_{i}})= \widetilde{P}_{i}\) and \(\mathbb{P}(X_{i}=0)=1-\widetilde{P}_{i}\). Then, by setting \(\epsilon_{0}=|\mathcal{B}|\cdot\epsilon\), we are then able to show that:

\[\mathbb{P}\Bigg{(}\bigg{|}|\mathcal{B}|\cdot\widehat{R}_{\text{ VIR}}(\widehat{Y}^{ERM}|\widetilde{P})-|\mathcal{B}|\cdot\mathbb{E}_{O}[ \widehat{R}_{\text{VIR}}(\widehat{Y}^{ERM}|\widetilde{P})]\bigg{|}\geq\epsilon _{0}\Bigg{)}\leq 2\exp\Big{(}\frac{-2\epsilon_{0}^{2}}{\Delta^{2}\sum_{i=1}^{| \mathcal{B}|}\frac{1}{|\mathcal{U}_{i}|}\sum_{j\in\mathcal{U}_{i}}\frac{1}{ \widetilde{P}_{i}^{2}}}\Big{)}\] \[\qquad\Longleftrightarrow\ \mathbb{P}\Bigg{(}\bigg{|}\widehat{R}_{\text{ VIR}}(\widehat{Y}^{ERM}|\widetilde{P})-\mathbb{E}_{O}[\widehat{R}_{\text{ VIR}}(\widehat{Y}^{ERM}|\widetilde{P})]\bigg{|}\geq\epsilon\Bigg{)}\leq 2\exp\Big{(} \frac{-2\epsilon^{2}|\mathcal{B}|^{2}}{\Delta^{2}\sum_{i=1}^{|\mathcal{B}|} \frac{1}{\widetilde{P}_{i}^{2}}}\Big{)}.\]

We can then solve for \(\epsilon\), completing the proof. 

With all the aforementioned definitions, we can derive the generalization bound for the VIR estimator.

**Theorem A.1** (Generalization Bound of VIR).: _In imbalanced regression with bins \(\mathcal{B}\), for any finite hypothesis space of predictions \(\mathcal{H}=\{\widehat{Y}_{1},...,\widehat{Y}_{\mathcal{H}}\}\), the transductive prediction error of the empirical risk minimizer \(\widehat{Y}^{ERM}\) using the VIR estimator with estimated propensities \(\widetilde{P}\) (\(P_{i}>0\)) and given training observations \(O\) from \(\mathcal{Y}\) with independent Bernoulli propensities \(P\), is bounded by:_

\[R(\widehat{Y}^{ERM})\leq\widehat{R}_{\text{VIR}}(\widehat{Y}^{ERM}|\widetilde {P})+\frac{\Delta}{|\mathcal{B}|}\sum_{i=1}^{|\mathcal{B}|}\bigg{|}1-\frac{P_{i }}{\widetilde{P}_{i}}\bigg{|}+\frac{\Delta}{|\mathcal{B}|}\sqrt{\frac{\log(2| \mathcal{H}|/\eta)}{2}}\sqrt{\sum_{i=1}^{|\mathcal{B}|}\frac{1}{\widetilde{P} _{i}^{2}}}\] (9)

Proof.: We first re-state the generalization bound for our VIR estimator: with probability \(1-\eta\), we have

\[R(\widehat{Y}^{ERM})\leq\widehat{R}_{\text{VIR}}(\widehat{Y}^{ERM}|\widetilde {P})+\frac{\Delta}{|\mathcal{B}|}\sum_{i=1}^{|\mathcal{B}|}\bigg{|}1-\frac{P_{i }}{\widetilde{P}_{i}}\bigg{|}+\frac{\Delta}{|\mathcal{B}|}\sqrt{\frac{\log(2| \mathcal{H}|/\eta)}{2}}\sqrt{\sum_{i=1}^{|\mathcal{B}|}\frac{1}{\widetilde{P} _{i}^{2}}}\]

We start to prove it from the LHS:

\[R(\widehat{Y}^{ERM}) =R(\widehat{Y}^{ERM})-\mathbb{E}_{O}[\widehat{R}_{\text{VIR}}( \widehat{Y}^{ERM}|\widetilde{P})]+\mathbb{E}_{O}[\widehat{R}_{\text{VIR}}( \widehat{Y}^{ERM}|\widetilde{P})]\] \[=\underbrace{\text{bias}(\widehat{R}_{\text{VIR}}(\widehat{Y}^{ ERM}|\widetilde{P}))}_{Bias}+\underbrace{\mathbb{E}_{O}[\widehat{R}_{\text{VIR}}(\widehat{Y}^{ERM}| \widetilde{P})]-\widehat{R}_{\text{VIR}}(\widehat{Y}^{ERM}|\widetilde{P})}_{ Variance\ Term}+\widehat{R}_{\text{VIR}}(\widehat{Y}^{ERM}|\widetilde{P})\] \[=\underbrace{\text{bias}(\widehat{R}_{\text{VIR}}(\widehat{Y}^{ ERM}|\widetilde{P}))}_{Bias}+\Big{|}\underbrace{\widehat{R}_{\text{VIR}}(\widehat{Y}^{ERM}| \widetilde{P})-\mathbb{E}_{O}[\widehat{R}_{\text{VIR}}(\widehat{Y}^{ERM}| \widetilde{P})]}_{ Variance\ Term}\Big{|}+\widehat{R}_{\text{VIR}}(\widehat{Y}^{ERM}| \widetilde{P})\]Below we derive each term:

_Variance Term._ With probability \(1-\eta\), the variance term is derived as

\[\mathbb{P}\Bigg{(}\bigg{|}\widehat{R}_{\text{VIR}}(\widehat{Y}^{ ERM}|\widetilde{P})-\mathbb{E}_{O}[\widehat{R}_{\text{VIR}}(\widehat{Y}^{ ERM}|\widetilde{P})]\bigg{|}\leq\epsilon\Bigg{)}\geq 1-\eta\] \[\Longleftrightarrow\mathbb{P}\Bigg{(}\bigvee_{\widehat{Y}_{j}} \bigg{|}\widehat{R}_{\text{VIR}}(\widehat{Y}|\widetilde{P})-\mathbb{E}_{O}[ \widehat{R}_{\text{VIR}}(\widehat{Y}|\widetilde{P})]\bigg{|}\geq\epsilon \Bigg{)}<\eta\] \[\Longleftrightarrow\mathbb{P}\Bigg{(}\bigvee_{\widehat{Y}_{j}} \bigg{|}\widehat{R}_{\text{VIR}}(\widehat{Y}|\widetilde{P})-\mathbb{E}_{O}[ \widehat{R}_{\text{VIR}}(\widehat{Y}|\widetilde{P})]\bigg{|}\geq\epsilon \Bigg{)}<\eta\] \[\Longleftrightarrow|\mathcal{H}|\cdot 2\exp(\frac{-2\epsilon^{2}| \mathcal{B}|^{2}}{\Delta^{2}\sum_{i=1}^{|\mathcal{B}|}\frac{1}{|\mathcal{U}_ {i}|}\sum_{j\in\mathcal{U}_{i}}\frac{1}{\widetilde{P}_{i}^{2}}})<\eta\] (11) \[\Longleftrightarrow|\mathcal{H}|\cdot 2\exp(\frac{-2\epsilon^{2}| \mathcal{B}|^{2}}{\Delta^{2}\sum_{i=1}^{|\mathcal{B}|}\frac{1}{\widetilde{P}_ {i}^{2}}})<\eta,\]

where inequality (10) is by Boole's inequality (Union bound), and inequality (11) holds by Lemma A.1. Then, by solving for \(\epsilon\), we can derive Variance Term that with probability \(1-\eta\),

\[\mathbb{E}_{O}[\widehat{R}_{\text{VIR}}(\widehat{Y}^{ ERM}|\widetilde{P})]-\widehat{R}_{\text{VIR}}(\widehat{Y}^{ERM}|\widetilde{P}) \leq\bigg{|}\widehat{R}_{\text{VIR}}(\widehat{Y}^{ERM}|\widetilde{P})- \mathbb{E}_{O}[\widehat{R}_{\text{VIR}}(\widehat{Y}^{ERM}|\widetilde{P})] \bigg{|}\] \[\leq\frac{\Delta}{|\mathcal{B}|}\sqrt{\frac{\log(2|\mathcal{H}|/ \eta)}{2}}\sqrt{\sum_{i=1}^{|\mathcal{B}|}\frac{1}{\widetilde{P}_{i}^{2}}}.\]

_Bias Term._ By definition, we can derive:

\[\text{bias}(\widehat{R}_{\text{VIR}}(\widehat{Y}^{ ERM}|\widetilde{P})) =R(\widehat{Y}^{ ERM})-\mathbb{E}_{O}[\widehat{R}_{\text{VIR}}(\widehat{Y}^{ ERM}|\widetilde{P})]\] \[=\frac{1}{|\mathcal{B}|}\sum_{i=1}^{|\mathcal{B}|}\frac{1}{| \mathcal{U}_{i}|}\sum_{j\in\mathcal{U}_{i}}\delta_{j}(Y,\widehat{Y}^{ ERM})-\frac{1}{|\mathcal{B}|}\sum_{i=1}^{|\mathcal{B}|}\frac{1}{|\mathcal{U}_{i}|} \sum_{j\in\mathcal{U}_{i}}\frac{P_{i}}{\widetilde{P}_{i}}\delta_{j}(Y,\widehat {Y}^{ERM})\] \[\leq\frac{\Delta}{|\mathcal{B}|}\sum_{i=1}^{|\mathcal{B}|}\frac{1 }{|\mathcal{U}_{i}|}\sum_{j\in\mathcal{U}_{i}}\bigg{|}1-\frac{P_{i}}{\widetilde {P}_{i}}\bigg{|}\] \[=\frac{\Delta}{|\mathcal{B}|}\sum_{i=1}^{|\mathcal{B}|}\bigg{|}1- \frac{P_{i}}{\widetilde{P}_{i}}\bigg{|},\]

concluding the proof for the Bias Term, hence completing the proof for the whole generalization bound. 

## Appendix B Details for Experiments

**Datasets.** In this work, we evaluate our methods in terms of prediction accuracy and uncertainty estimation on four imbalanced datasets2, AgeDB [30], IMDB-WIKI [33], STS-B [7], and NYUD2-DIR [35]. Due to page limit, the results for NYUD2-DIR [35] are in the supplementary. We follow 

[MISSING_PAGE_FAIL:18]

[MISSING_PAGE_FAIL:19]

[MISSING_PAGE_FAIL:20]

* For example, if we use the equal-interval bins \([0,1),[1,2),...\), VIR will naturally compute \(k(y,y^{\prime})\) for \(y=1,2,3,4,5,...\) and \(y^{\prime}=1,2,3,4,5,...\).
* In contrast, if we use equal-size bins, VIR may end up with **large intervals** and may lead to inaccurate kernel values for \(k(y,y^{\prime})\). To see this, consider a case where equal-size bins are \([0,1),[1,2),[2,3.1),[3.1,8.9),...\); the kernel value \(k(y,y^{\prime})\) between bins \([2,3.1)\) and \([3.1,8.9)\) is \(k(2,3.1)\), which is very inaccurate since \(3.1\) is very far away from the mean of the bin \([3.1,8.9)\) (i.e., \(6\)). Using small and equal-interval bins can naturally address such issues.

### The Number of Bins

Our preliminary results indicate that the performance of our VIR remains consistent regardless of the number of bins, as shown in the Sec. E.3 of the Supplement. Thus in our paper, we chose to use the same number of bins as the imbalanced regression literature [49, 13] for fair comparison with prior work. For example, in the AgeDB dataset where the regression labels are people's "age" in the range of 0 99, we use 100 bins, with each year as one bin.

### Reweighting Methods and Stronger Augmentations

Our method focus on reweighting methods, and using augmentation (e.g., the SimCLR pipeline [10]) is an orthogonal direction to our work. However, we expect that data augmentation could further improve our VIR's performance. This is because one could perform data augmentation only on minority data to improve accuracy in the minority group, but this is sub-optimal; the reason is that one could potentially further perform data augmentation on majority data to improve accuracy in the majority group without sacrificing too much accuracy in the minority group. However, performing data augmentation on both minority and majority groups does not transform an imbalanced dataset to an balanced dataset. This is why our VIR is still necessary; VIR could be used on top of any data augmentation techniques to address the imbalance issue and further improve accuracy.

### Discussion on I.I.D. and N.I.D. Assumptions

**Generalization Error, Bias, and Variance.** We could analyze the generalization error of our VIR by bounding the generalization with the sum of three terms: (a) the bias of our estimator, (2) the variance of our estimator, (3) model complexity. Essentially VIR uses the N.I.D. assumption increases our estimator's bias, but significantly reduces its variance in the imbalanced setting. Since the model complexity is kept the same (using the same backbone neural network) as the baselines, N.I.D. will lead to a lower generalization error.

**Variance of Estimators in Imbalanced Settings.** In the imbalanced setting, one typically use inverse weighting (i.e., the IPS estimator in Definition A.4) to produced an unbiased estimator (i.e., making the first term of the aforementioned bound zero). However, for data with extremely low density, its inverse would be extremely large, therefore leading to a very large variance for the estimator. Our VIR replaces I.I.D. with N.I.D. to "smooth out" such singularity, and therefore significantly lowers the variance of the estimator (i.e., making the second term of the aforementioned bound smaller), and ultimately lowers the generalization error.

\begin{table}
\begin{tabular}{l|c|c c c c|c c c|c c c|c c} \hline \hline Metrics & Bins & \multicolumn{4}{c|}{MSE \(\downarrow\)} & \multicolumn{4}{c|}{MAE \(\downarrow\)} & \multicolumn{4}{c}{GM \(\downarrow\)} \\ \hline Shot & \# & All & Many & Med. & Few & All & Many & Med. & Few & All & Many & Med. & Few \\ \hline \hline Ranssim & 100 & 83.51 & 71.99 & 99.14 & 149.05 & 7.02 & 6.49 & 7.84 & 9.68 & 4.53 & 4.13 & 5.37 & 6.89 \\ VIR (Ours) & 100 & **81.76** & **70.61** & **91.47** & **142.36** & **6.99** & **6.39** & **7.47** & **9.51** & **4.41** & **4.07** & **5.05** & **6.23** \\ \hline \hline Ranssim & 33 & 109.45 & 91.78 & 128.10 & 187.13 & 7.46 & 6.94 & 8.42 & 10.66 & 5.13 & 4.70 & 5.23 & 8.21 \\ VIR (Ours) & 33 & **84.77** & **77.29** & **95.66** & **125.33** & **7.01** & **6.70** & **7.45** & **8.74** & **4.36** & **4.20** & **4.73** & **4.94** \\ \hline \hline Ranssim & 20 & 98.71 & 84.38 & 107.89 & 171.04 & 7.32 & 6.78 & 8.35 & 10.57 & 5.33 & 4.51 & 5.69 & 7.92 \\ VIR (Ours) & 20 & **84.05** & **72.12** & **100.49** & **151.25** & **7.06** & **6.50** & **7.90** & **10.06** & **4.49** & **4.05** & **5.34** & **7.28** \\ \hline \hline \end{tabular}
\end{table}
Table 13: **Comparison for different numbers of Bins. ”Med.” is short for ”Medium”.**

### Why We Need Statistics of Statistics for Smoothing

Compared with DIR [49], which only considers the **statistics** for _deterministic representations_, our VIR considers the **statistics of statistics** for _probabilistic representations_, this is because the requirement to perform feature smoothing to get the representation \(z_{i}\) necessitates the computation of mean and variance of \(z_{i}\)'s neighboring data (i.e., data with neighboring labels). Here \(z_{i}\) contains the **statistics** of neighboring data. In contrast, our VIR also needs to generate uncertainty estimation, which requires a stochastic representation for \(z_{i}\), e.g., the mean and variance of \(z_{i}\) (note that \(z_{i}\) itself is already a form of statistics). This motivates the hierarchical structure of the **statistics of statistics**. Here the variance measures the uncertainty of the representation.

### The Choice of Kernel Function

The DIR paper shows that a simple Gaussian kernel with inverse square-root weighting (i.e., SQINV) achieves the best performance. Therefore, we use exactly the same parameter configuration as the DIR paper [49]. Specifically, we set \(\sigma=2\); for label \(y_{b}\) in bin \(b\), we define neighboring labels as \(y_{b^{\prime}}\) such that \(|y_{b^{\prime}}-y_{b}|\leq 2\), i.e., \(B\) contains \(5\) bins. For example, if \(y_{b}=23\), its neighboring labels are \(21\), \(22\), \(23\), \(24\), and \(25\).

Besides, our preliminary results also show that the performance is not very sensitive to the choice of kernels, as long as the kernel \(k(a,b)\) reflects the distance between \(a\) and \(b\), i.e., larger distance between \(a\) and \(b\) leads to smaller \(k(a,b)\).

### Why VIR Solves the Imbalanced Regression Problem

Our training objective function (Eqn.5 in the main paper) is the **negative log likelihood** for the Normal Inverse Gaussian (NIG) distribution, and each posterior parameter (\(\nu_{i}^{*},\gamma_{i}^{*},\alpha_{i}^{*}\)) of the NIG distribution is reweighted by importance weights, thereby assigning higher weights to minority data during training and allowing minority data points to benefit more from their neighboring information.

Take \(\nu_{i}^{*}\) as an example. Assume a minority data point \((x_{i},y_{i})\) that belongs to bin \(b\), i.e., its label \(y_{i}=y_{b}\). Note that there is **a loss term**\((y_{i}-\gamma_{i}^{*})^{2}\nu_{i}^{*}\) in **Eqn.5**, where \(\gamma_{i}^{*}\) is the model prediction, \(y_{i}\) is the label, and \(\nu_{i}^{*}\) is the _importance weight_ for this data point.

Here \(\nu_{i}^{*}=\nu_{0}+(\sum_{b^{\prime}\in\mathcal{B}}k(y_{b},y_{b^{\prime}})p(y _{b^{\prime}}))^{-1/2}\cdot n_{i}\) where \(n_{i}\) represents the pseudo-count for the NIG distribution. Since \((x_{i},y_{i})\) is a minority data point, data from its neighboring bins has smaller frequency \(p(y_{b^{\prime}})\) and therefore smaller \(\sum_{b^{\prime}\in\mathcal{B}}k(y_{b},y_{b^{\prime}})p(y_{b^{\prime}})\), leading to **a larger _importance weight_\(\nu_{i}^{*}\) for this minority data point in Eqn.5.

This allows VIR to naturally put more focus on the minority data, thereby alleviating the imbalance problem.

### Difference from DIR, VAE and DER

From a technical perspective, VIR is substantially different from any combinations of DIR [49], VAE [25], and DER [1]. Specifically,

* VIR is a deep generative model to define how imbalanced data are generated, which is learned by a principled variational inference algorithm. In contrast, DIR is a simply discriminative model (without any principled generative model formulation) that directly predict the labels from input. It is more prone to overfitting.
* DIR uses deterministic representations, with one vector as the final representation for each data point. In contrast, our VIR uses probabilistic representations, with one vector as the mean of the representation and another vector as the variance of the representation. Such dual representation is more robust to noise and therefore leads to better prediction performance.
* DIR is a deterministic model, while our VIR is a Bayesian model. Essentially VIR is equivalent to sampling infinitely many predictions for each input data point and averaging these predictions. Therefore intuitively it makes sense that VIR could lead to better prediction performance.

[MISSING_PAGE_FAIL:23]

[MISSING_PAGE_FAIL:24]