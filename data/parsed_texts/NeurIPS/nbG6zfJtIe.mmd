# Learning Curves for

Deep Structured Gaussian Feature Models

 Jacob A. Zavatone-Veth\({}^{1,2}\) and **Cengiz Pehlevan\({}^{3,2,4}\)**

\({}^{1}\)Department of Physics, \({}^{2}\)Center for Brain Science,

\({}^{3}\)John A. Paulson School of Engineering and Applied Sciences,

\({}^{4}\)Kempner Institute for the Study of Natural and Artificial Intelligence,

Harvard University

Cambridge, MA 02138, USA

jzavatoneveth@g.harvard.edu, cpehlevan@seas.harvard.edu

###### Abstract

In recent years, significant attention in deep learning theory has been devoted to analyzing when models that interpolate their training data can still generalize well to unseen examples. Many insights have been gained from studying models with multiple layers of Gaussian random features, for which one can compute precise generalization asymptotics. However, few works have considered the effect of weight anisotropy; most assume that the random features are generated using independent and identically distributed Gaussian weights, and allow only for structure in the input data. Here, we use the replica trick from statistical physics to derive learning curves for models with many layers of structured Gaussian features. We show that allowing correlations between the rows of the first layer of features can aid generalization, while structure in later layers is generally detrimental. Our results shed light on how weight structure affects generalization in a simple class of solvable models.

## 1 Introduction

Characterizing how data structure and model architecture affect generalization performance is among the foremost goals of deep learning theory [1; 2]. A fruitful line of inquiry has focused on the properties of a class of simplified models that are asymptotically solvable: neural networks in which only the readout layer is trained and other weights are random, which are known as random feature models (RFMs) [3; 4; 5; 6; 7; 8; 9; 10; 11; 12; 13; 14; 15; 16; 17; 18; 19; 20; 21]. Though RFMs cannot capture the effects of representation learning on generalization in richly-trained neural networks [13; 22; 23], they have substantially advanced our understanding of how data structure and model architecture interact to give rise to a wide array of generalization phenomena observed in deep learning [1; 2; 3; 4; 5; 7; 8; 9; 10; 24; 25].

Of particular interest is the question of when models overfit benignly, that is, when they generalize well despite having been trained to perfectly interpolate their training data. Here, much intuition has been gained by studying minimum-norm kernel interpolation--that is, the ridgeless limit of kernel ridge regression--with RFM kernels, for which precise generalization asymptotics can be computed using tools from random matrix theory. These asymptotics lead to a precise picture of how the spectrum of the random feature kernel and the structure of the task interact to determine generalization. These analyses are facilitated by universality results, often termed Gaussian equivalence theorems, that state that the generalization error of a nonlinear RFM is asymptotically equal to that of a linear Gaussian model with an effective noise term resulting from nonlinearity [3; 7; 10; 25; 26]. In the past few years, Gaussian equivalence theorems for ever more general classes of RFMs have been established: within this year Schroder et al. [20] and Bosch et al. [21] have established Gaussian equivalence theoremsfor deep nonlinear RFMs with unstructured feature weights, while Cui et al. [27] have extended some of these results to the setting of deep Bayesian neural networks when the target is of the same architecture.

However, these analyses consider the effect only of correlations in the data, and do not address the possibility of correlations between the random weights. It is standard to assume that the elements of the weight matrices at each layer are independent and identically distributed Gaussian random variables, and to our knowledge all existing Gaussian equivalence theorems make use of this assumption [3; 4; 5; 6; 7; 8; 9; 10; 11; 12; 13; 14; 15; 19; 20; 21]. As a result, how weight anisotropy affects generalization in deep RFMs--in particular, if it can affect the asymptotic scaling of generalization error with dataset size and network width [16; 28; 19]--remains unclear.

In this note, we take the first step towards filling that gap in our theoretical understanding of RFMs by computing the asymptotic generalization error of the simplest class of deep RFMs with anisotropic weight correlations: models with linear activations. Our primary contributions are as follows:

* Using the replica method from statistical mechanics [29], we compute the asymptotic generalization error of deep linear random feature models with weights drawn from general matrix Gaussian distributions. This computation is closely related to prior replica approches to product random matrix problems [30; 13].
* We show that, in the ridgeless limit, structure in the weights beyond the first layer is detrimental for generalization.
* We next consider the special case of power-law spectra in the weights and in the data, which was classically studied in kernel interpolation in the form of source-capacity conditions [31], and has recently attracted substantial interest in deep learning due to approximate power-law spectra present in real data [16; 19; 28; 32]. Using approximations for required spectral statistics derived in past works [19], we show that altering the power laws of the weight covariance spectra do not affect the scaling laws of generalization.
* We finally show how our results can be extended from the ridge regression estimator to the Bayesian Gibbs estimator, an object of classic study in the statistical physics of learning [33; 13; 34]. For sufficiently large prior variance, structure can be beneficial for generalization with this estimator.

Taken together, these results are consistent with the intuition that representation learning at only the first layer of a deep linear model is sufficient to recover a single teacher weight vector [35; 36; 13; 37].

## 2 Preliminaries

We consider depth-\(L\) linear RFMs with input \(\mathbf{x}\in\mathbb{R}^{n_{0}}\) and scalar output given by

\[g(\mathbf{x};\mathbf{v},\mathbf{F})=\frac{1}{\sqrt{n_{0}}}(\mathbf{F}\mathbf{v })^{\top}\mathbf{x},\] (1)

where the feature matrix \(\mathbf{F}\in\mathbb{R}^{n_{0}\times n_{L}}\) is fixed and the vector \(\mathbf{v}\in\mathbb{R}^{n_{L}}\) is trainable. If \(L=0\), corresponding to standard linear regression, the feature matrix is simply the identity: \(\mathbf{F}=\mathbf{I}_{n_{0}}\). If \(L>0\), we take the feature matrix to be defined by a product of \(L\) factors \(\mathbf{U}_{\ell}\in\mathbb{R}^{n_{\ell-1}\times n_{\ell}}\):

\[\mathbf{F}=\frac{1}{\sqrt{n_{1}\cdots n_{L}}}\mathbf{U}_{1}\cdots\mathbf{U}_{ L}.\] (2)

We draw the random feature matrices independently from matrix Gaussian distributions

\[\mathbf{U}_{\ell}\sim\mathcal{MN}_{n_{\ell-1}\times n_{\ell}}(\mathbf{0}, \mathbf{\Gamma}_{\ell},\mathbf{\Sigma}_{\ell})\] (3)

for input covariance matrices \(\mathbf{\Gamma}_{\ell}\in\mathbb{R}^{n_{\ell-1}\times n_{\ell-1}}\) and output covariance matrices \(\mathbf{\Sigma}_{\ell}\in\mathbb{R}^{n_{\ell}\times n_{\ell}}\), such that \(\mathbb{E}[(U_{\ell})_{ij}(U_{\ell^{\prime}})_{i^{\prime}j^{\prime}}]=\delta_ {\ell\ell^{\prime}}(\Gamma_{\ell})_{ii^{\prime}}(\Sigma_{\ell})_{jj^{\prime}}\). Subject to the constraints of layer-wise independence and separability--which are required for the factors to be matrix-Gaussian distributed--this is the most general covariance structure one could consider. One might wish to relax this to include non-separable covariance tensors \(\mathbb{E}[(U_{\ell})_{ij}(U_{\ell^{\prime}})_{i^{\prime}j^{\prime}}]=\delta _{\ell\ell^{\prime}}(\chi_{\ell})_{ii^{\prime},jj^{\prime}}\), but this would spoil the matrix-Gaussianity of the factors, and to our knowledge does not appear to be addressable using standard methods [30; 38]. We generate training datasets according to a structured Gaussian covariate model, with \(p\) i.i.d. training examples \((\mathbf{x}_{\mu},y_{\mu})\) generated as

\[\mathbf{x}_{\mu}\sim_{\text{i.i.d.}}\mathcal{N}(\mathbf{0},\mathbf{\Sigma}_{0}),\qquad y_{\mu}=\frac{1}{\sqrt{n_{0}}}\mathbf{w}_{*}^{\top}\mathbf{x}_{\mu}+ \xi_{\mu},\] (4)

where the teacher weight vector \(\mathbf{w}_{*}\) is fixed and the label noise follows

\[\xi_{\mu}\sim_{\text{i.i.d.}}\mathcal{N}(0,\eta^{2}).\] (5)

We collect the covariates into a matrix \(\mathbf{X}\in\mathbb{R}^{p\times n_{0}}\), and the targets into a vector \(\mathbf{y}\in\mathbb{R}^{p}\).

As in most works on RFMs [3; 4; 5; 8; 9; 10; 11; 12; 13; 14; 15; 16; 17; 18; 19; 20; 21; 25], our focus is on the ridge regression estimator

\[\mathbf{v}=\operatorname*{arg\,min}_{\mathbf{v}}L\quad\text{for}\quad L=\frac{ 1}{2}\left\|\frac{1}{\sqrt{n_{0}}}\mathbf{X}\mathbf{F}\mathbf{v}-\mathbf{y} \right\|^{2}+\frac{\lambda}{2}\|\boldsymbol{\Gamma}_{L+1}^{-1/2}\mathbf{v}\|_ {2}^{2},\] (6)

where the positive-definite matrix \(\boldsymbol{\Gamma}_{L+1}\in\mathbb{R}^{n_{L}\times n_{L}}\) controls the anisotropy of the norm and the ridge parameter \(\lambda>0\) sets the regularization strength. This minimization problem has the well-known closed form solution

\[\hat{\mathbf{v}}=\frac{1}{\sqrt{n_{0}}}\left(\lambda\boldsymbol{\Gamma}_{L+1} ^{-1}+\frac{1}{n_{0}}\mathbf{F}^{\top}\mathbf{X}^{\top}\mathbf{X}\mathbf{F} \right)^{-1}\mathbf{F}^{\top}\mathbf{X}^{\top}\mathbf{y}.\] (7)

As motivated in the Introduction, we are chiefly interested in the ridgeless limit \(\lambda\downarrow 0\), in which the ridge regression solution gives the minimum \(\ell_{2}\) norm interpolant of the training data. We measure performance of this estimator by the generalization error

\[\epsilon_{p,n_{0},\dots,n_{L}}=\mathbb{E}_{\mathbf{x}}\left(g(\mathbf{x}; \hat{\mathbf{v}},\mathbf{F})-\mathbb{E}_{\xi}[y(\mathbf{x})]\right)^{2}=\frac {1}{n_{0}}\|\boldsymbol{\Sigma}_{0}^{1/2}(\mathbf{F}\hat{\mathbf{v}}-\mathbf{ w}_{*})\|^{2},\] (8)

which is a random variable with distribution induced by the training data and feature weights.

This leads us to a simple, but important observation: including structured input-input covariances is equivalent to transforming the feature-feature covariances. We state this formally as:

**Lemma 2.1**.: _Fix sets of matrices \(\{\boldsymbol{\Gamma}_{\ell}\}_{\ell=1}^{L+1}\) and \(\{\boldsymbol{\Sigma}_{\ell}\}_{\ell=0}^{L}\), and a target vector \(\mathbf{w}_{*}\). Let \(\epsilon_{p,n_{0},\dots,n_{L}}\) be the resulting generalization error as defined in (8). Let_

\[\tilde{\boldsymbol{\Gamma}}_{\ell} =\mathbf{I}_{n_{\ell-1}} \text{for }\ell=1,\dots,L+1,\] (9) \[\tilde{\boldsymbol{\Sigma}}_{\ell} =\boldsymbol{\Gamma}_{\ell+1}^{1/2}\boldsymbol{\Sigma}_{\ell} \boldsymbol{\Gamma}_{\ell+1}^{1/2} \text{for }\ell=0,\dots,L,\text{ and}\] (10) \[\tilde{\mathbf{w}}_{*} =\boldsymbol{\Gamma}_{1}^{-1/2}\mathbf{w}_{*}.\] (11)

_Let \(\tilde{\epsilon}_{p,n_{0},\dots,n_{L}}\) be the generalization error for these transformed covariance matrices and target. Then, for any \(\lambda>0\), we have the equality in distribution \(\epsilon_{p,n_{0},\dots,n_{L}}\stackrel{{ d}}{{=}}\tilde{ \epsilon}_{p,n_{0},\dots,n_{L}}\)._

Proof of Lemma 2.1.: As the features and data are Gaussian, we can write \(\mathbf{X}\stackrel{{ d}}{{=}}\boldsymbol{\Sigma}_{0}^{1/2} \mathbf{Z}_{0}\) and \(\mathbf{U}_{\ell}\stackrel{{ d}}{{=}}\mathbf{I}_{\ell}^{1/2} \mathbf{Z}_{\ell}\boldsymbol{\Sigma}_{\ell}^{1/2}\) for unstructured Gaussian matrices \((Z_{\ell})_{ij}\sim_{\text{i.i.d.}}\mathcal{N}(0,1)\). Substituting these representations the ridge regression solution (7) and the generalization error (8), the claim follows. 

Therefore, we may take \(\boldsymbol{\Gamma}_{\ell}=\mathbf{I}_{n_{\ell-1}}\) without loss of generality. Moreover, thanks to the rotation-invariance of the isotropic Gaussian factors \(\mathbf{Z}_{\ell}\), we may in fact take the remaining covariance matrices \(\boldsymbol{\Sigma}_{\ell}\) to be diagonal without loss of generality, so long as we then express \(\tilde{\mathbf{w}}_{*}\) in the basis of eigenvectors of \(\boldsymbol{\Sigma}_{0}\). An important qualitative takeaway of this result is that changing the covariance matrix of the inputs of the first layer \(\boldsymbol{\Gamma}_{1}\) is equivalent to modifying the data covariance matrix, which was in a simpler form observed in the shallow setting (\(L=1\)) by Pandey et al. [39].

## 3 Asymptotic learning curves

Having defined the setting of our problem, we can define our concrete objective and state our main results, deferring their interpretation to the following section. We consider the standard proportional asymptotic limit

\[p,n_{0},\dots,n_{L}\rightarrow\infty,\quad\text{with}\quad n_{\ell}/p \rightarrow\alpha_{\ell}\in(0,\infty),\] (12)which we will refer to as the thermodynamic limit. Our goal is to compute the limiting generalization error:

\[\epsilon=\lim_{p,n_{0},\ldots,n_{L}\rightarrow\infty}\mathbb{E}_{ \mathcal{D}}\frac{1}{n_{0}}\|\bm{\Sigma}_{0}^{1/2}(\mathbf{F}\mathbf{v}-\mathbf{ w}_{*})\|^{2},\] (13)

where \(\mathbb{E}_{\mathcal{D}}\) denotes expectation over all sources of quenched disorder in the problem, i.e., the training data and the random feature weights. In the thermodynamic limit, we expect the generalization error to concentrate, which is why we compute its average in (13) [3, 4, 5, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21].

To have a well-defined thermodynamic limit, the covariances \(\tilde{\bm{\Sigma}}_{\ell}\) and the teacher \(\tilde{\mathbf{w}}_{\ell}\) must be in some sense sufficiently well-behaved. We consider the following conditions, which are the generalization to our setting of those assumed in previous work [4, 5, 6, 7, 16, 17, 18, 40]:

**Assumption 3.1**.: _We assume that we are given deterministic sequences of positive-definite matrices \(\tilde{\bm{\Sigma}}_{\ell}(n_{\ell})\) and vectors \(\tilde{\mathbf{w}}_{*}(n_{0})\) indexed by the system size, such that the limiting (weighted) spectral moment generating functions_

\[M_{\tilde{\bm{\Sigma}}_{\ell}}(z)=\lim_{n_{\ell}\rightarrow\infty} \frac{1}{n_{\ell}}\operatorname{tr}[\tilde{\bm{\Sigma}}_{\ell}(z\mathbf{I}_{n_ {\ell}}-\tilde{\bm{\Sigma}}_{\ell})^{-1}]\quad\text{and}\quad\psi(z)=\lim_{n_ {0}\rightarrow\infty}\frac{1}{n_{0}}\tilde{\mathbf{w}}_{*}^{\top}\tilde{\bm{ \Sigma}}_{0}(z\mathbf{I}_{n_{0}}+\tilde{\bm{\Sigma}}_{0})^{-1}\tilde{\mathbf{w} }_{*}\] (14)

_are well-defined, for all \(\ell=0,\ldots,L\)._

We can now state our results. As a preliminary step, we first give an expression for the generalization error for a fixed teacher \(\tilde{\mathbf{w}}_{*}\) at finite ridge \(\lambda\). Then, we pass to the ridgeless limit, on which we focus for the remainder of the paper. At finite ridge, we have the following:

**Proposition 3.1**.: _Assume Assumption 3.1 holds. For \(\lambda>0\), let \(\zeta\) solve the self-consistent equation_

\[\lambda=\frac{1-\zeta}{\zeta}\prod_{\ell=0}^{L}\frac{-\zeta}{ \alpha_{\ell}}M_{\tilde{\bm{\Sigma}}_{\ell}}^{-1}\left(-\frac{\zeta}{\alpha_ {\ell}}\right).\] (15)

_In terms of \(\zeta\), let \(\kappa_{\ell}(\zeta)\) solve_

\[\mathbb{E}_{\tilde{\sigma}_{\ell}}\left[\frac{\tilde{\sigma}_{ \ell}}{\kappa_{\ell}(\zeta)+\tilde{\sigma}_{\ell}}\right]=-M_{\tilde{\bm{ \Sigma}}_{\ell}}(-\kappa_{\ell}(\zeta))=\frac{\zeta}{\alpha_{\ell}}\] (16)

_for \(\ell=0,\ldots,L\), where \(\mathbb{E}_{\tilde{\sigma}_{\ell}}[h(\tilde{\sigma}_{\ell})]=\lim_{n_{\ell} \rightarrow\infty}n_{\ell}^{-1}\sum_{j=1}^{n_{\ell}}h(\tilde{\sigma}_{\ell,j})\) denotes expectation of a function \(h\) with respect to the limiting spectral distribution of \(\tilde{\bm{\Sigma}}_{\ell}\), for \(\tilde{\sigma}_{\ell,j}\) its eigenvalues at finite size, and let_

\[\mu_{\ell}(\zeta)=-\frac{\alpha_{\ell}}{\zeta}\kappa_{\ell}(\zeta )M_{\tilde{\bm{\Sigma}}_{\ell}}^{\prime}\left(-\kappa_{\ell}(\zeta)\right)=1- \frac{\alpha_{\ell}}{\zeta}\mathbb{E}_{\tilde{\sigma}_{\ell}}\left[\left(\frac {\tilde{\sigma}_{\ell}}{\kappa_{\ell}(\zeta)+\tilde{\sigma}_{\ell}}\right)^{2 }\right].\] (17)

_Then, the learning curve (13) at finite ridge for a fixed target is given by_

\[\big{[}1+\big{(}\sum_{\ell=0}^{L}\frac{1-\mu_{\ell}}{\mu_{\ell}} \big{)}(1-\zeta)\big{]}\epsilon=\big{(}\sum_{\ell=1}^{L}\frac{1-\mu_{\ell}}{ \mu_{\ell}}\big{)}\kappa_{0}\psi(\kappa_{0})-\frac{\kappa_{0}^{2}}{\mu_{0}} \psi^{\prime}(\kappa_{0})+\big{(}\sum_{\ell=0}^{L}\frac{1-\mu_{\ell}}{\mu_{\ell} }\big{)}\zeta\eta^{2}.\] (18)

Proof of Proposition 3.1.: We defer the derivation of (18) to Appendix A. To compute the disorder average in (13), we express the minimization problem in (6) as the zero-temperature limit \(\beta\rightarrow\infty\) of an auxiliary Gibbs distribution \(p(\mathbf{v})\propto e^{-\beta L}\), and evaluate the average over the random data random feature weights using the non-rigorous replica method from the statistical mechanics of disordered systems [29, 33]. This computation is lengthy but standard, and is closely related to the approach used in our previous works on deep linear models [13, 30]. All of our results are obtained under a replica-symmetric _Ansatz_; as the ridge regression problem (6) is convex, we expect replica symmetry to be unbroken [29, 41, 42]. 

From the self-consistent equation (15), we recognize that \(\zeta\) is is up to a sign the spectral moment generating function of the feature Gram matrix \(\mathbf{K}=\mathbf{X}\mathbf{F}\mathbf{F}^{\top}\mathbf{X}^{\top}/n_{0}\), which is a product-Wishart random matrix [30]:

\[\zeta(\lambda)=-M_{\mathbf{K}}(-\lambda).\] (19)

This dependence falls out of the replica computation of the generalization error using an auxiliary Gibbs distribution; we emphasize that one could take an alternative approach in which the generalization error is first expressed in terms of \(M_{\mathbf{K}}\)--as, for instance, in Gerace et al. [25] or Hastie et al.

[5]--and then use results on the spectra of product-Wishart matrices to conclude the claimed result [30]. This approach would potentially have the advantage of giving a fully rigorous proof, rather than one that depends on the replica trick. However, one would still then be faced with the task of solving the self-consistent equation for the spectral moment generating function, and therefore would end up in the same place insofar as quantitative predictions are concerned.

In principle, we could now directly proceed to study how weight structure affects (18) for some fixed ridge \(\lambda\). However, as long as there is structure in the weights and/or the data, the self-consistent equation (15) must generally be solved numerically [30, 14]. To allow us to make analytical progress, we therefore focus on the ridgeless limit \(\lambda\downarrow 0\) for the remainder of the present paper, and leave careful analysis of the \(\lambda>0\) case to future work. This follows the path of most recent studies of models with linear random features, and also the fundamental interest in interpolating models [3, 4, 5, 6, 7, 19, 20, 21, 17, 18, 3, 10, 11, 12, 13, 14, 15, 16, 17, 19]. We therefore emphasize that we state Proposition 3.1 merely as a preliminary result.

Before giving our result for the generalization error in the ridgeless limit, we warn the reader of an impending, somewhat severe abuse of notation: in Proposition 3.2 and for the remainder of the paper, we will re-define \(\kappa_{\ell}\) to be given by its value for the solution for \(\zeta\) appropriate in the regime of interest. Moreover, we will simply write \(\epsilon\) for \(\lim_{\lambda\downarrow 0}\epsilon\).

**Proposition 3.2**.: _Assume Assumption 3.1 holds, and let \(\alpha_{\min}=\min\{\alpha_{1},\cdots,\alpha_{L}\}\). For \(\ell=0,\ldots,L\), in the regime \(\alpha_{\ell}>1\), let \(\kappa_{\ell}\) be given by the unique non-negative solution to the implicit equation_

\[\frac{1}{\alpha_{\ell}}=-M_{\mathbf{\tilde{\Sigma}}_{\ell}}(-\kappa_{\ell})= \mathbb{E}_{\tilde{\sigma}_{\ell}}\left[\frac{\tilde{\sigma}_{\ell}}{\kappa_{ \ell}+\tilde{\sigma}_{\ell}}\right].\] (20)

_In terms of \(\kappa_{\ell}\), let_

\[\mu_{\ell}=-\alpha_{\ell}\kappa_{\ell}M^{\prime}_{\mathbf{\tilde{\Sigma}}_{ \ell}}(-\kappa_{\ell})=1-\alpha_{\ell}\mathbb{E}_{\tilde{\sigma}_{\ell}}\left[ \left(\frac{\tilde{\sigma}_{\ell}}{\kappa_{\ell}+\tilde{\sigma}_{\ell}}\right) ^{2}\right].\] (21)

_In the regime \(\alpha_{\min}<\alpha_{0}\), let \(\kappa_{\min}\) be the unique non-negative solution to the implicit equation_

\[\frac{\alpha_{\min}}{\alpha_{0}}=-M_{\mathbf{\tilde{\Sigma}}_{0}}(-\kappa_{ \min})=\mathbb{E}_{\tilde{\sigma}_{0}}\left[\frac{\tilde{\sigma}_{0}}{\kappa_ {\min}+\tilde{\sigma}_{0}}\right].\] (22)

_Then, the learning curve (13) for a fixed target in the ridgeless limit \(\lambda\downarrow 0\) is given by_

\[\epsilon=\begin{cases}\big{(}\sum_{\ell=1}^{L}\frac{1-\mu_{\ell}}{\mu_{\ell}} \big{)}\kappa_{0}\psi(\kappa_{0})-\frac{\kappa_{0}^{2}}{\mu_{0}}\psi^{\prime} (\kappa_{0})+\big{(}\sum_{\ell=0}^{L}\frac{1-\mu_{\ell}}{\mu_{\ell}}\big{)} \eta^{2},&\alpha_{0},\alpha_{\min}>1\\ \frac{\alpha_{\min}\psi(\kappa_{\min})}{1-\alpha_{\min}}+\frac{\alpha_{\min}} {1-\alpha_{\min}}\eta^{2},&\alpha_{\min}<1,\alpha_{\min}<\alpha_{0}\\ \frac{\alpha_{0}}{1-\alpha_{0}}\eta^{2},&\alpha_{0}<1,\alpha_{0}<\alpha_{\min}.\end{cases}\] (23)

Proof of Proposition 3.2.: We derive (23) as the zero-ridge limit of Proposition 3.1 in Appendix A. 

Before we analyze the effect of weight anisotropy in detail in Section 4, we note several simplifying special cases of Proposition 3.2 which recover the results of prior works. To facilitate this comparison, we provide a notational dictionary in Appendix D. The first important special case is

**Corollary 3.1**.: _If \(L=0\), we have_

\[\epsilon=\begin{cases}-\frac{\kappa_{0}^{2}}{\mu_{0}}\psi^{\prime}(\kappa_{0} )+\frac{1-\mu_{0}}{\mu_{0}}\eta^{2},&\alpha_{0}>1\\ \frac{\alpha_{0}}{1-\alpha_{0}}\eta^{2},&\alpha_{0}<1.\end{cases}\] (24)

This recovers the known, rigorously proved result for linear ridgeless regression [4, 5, 6, 7, 16, 17, 18]. For larger depths, an important simplifying case of Proposition 3.2 is that in which the data and features are unstructured, in which case the generalization error is given by

**Corollary 3.2**.: _If \(\tilde{\mathbf{\Sigma}}_{\ell}=\mathbf{I}_{n_{\ell}}\) for \(\ell=0,\ldots,L\), we have, for any target satisfying \(\|\tilde{\mathbf{w}}_{*}\|^{2}=n_{0}\),_

\[\epsilon=\begin{cases}\big{(}1+\sum_{\ell=1}^{L}\frac{1}{\alpha_{\ell}}\big{)} \big{(}1-\frac{1}{\alpha_{0}}\big{)}+\big{(}\sum_{\ell=0}^{L}\frac{1}{\alpha_{ \ell}-1}\big{)}\eta^{2},&\alpha_{0},\alpha_{\min}>1\\ \frac{1-\alpha_{\min}/\alpha_{0}}{1-\alpha_{\min}}+\frac{\alpha_{\min}}{1- \alpha_{\min}}\eta^{2},&\alpha_{\min}<1,\alpha_{\min}<\alpha_{0}\\ \frac{\alpha_{0}}{1-\alpha_{0}}\eta^{2},&\alpha_{0}<1,\alpha_{0}<\alpha_{\min}.\end{cases}\] (25)Proof of Corollary 3.2.: We have \(M_{\mathbf{I}_{n_{\ell}}}(z)=1/(z-1)\), hence \(\kappa_{\ell}=\alpha_{\ell}-1\), \(\mu_{\ell}=1-1/\alpha_{\ell}\), and \(\kappa_{\min}=\alpha_{0}/\alpha_{\min}-1\). Finally, for any fixed teacher vector satisfying \(\|\tilde{\mathbf{w}}_{*}\|^{2}=n_{0}\), we have \(\psi(z)=1/(z+1)\) if \(\tilde{\mathbf{\Sigma}}_{0}=\mathbf{I}_{n_{0}}\). Substituting these results into (23), we obtain (25). 

This recovers the result obtained in our previous work [13], and in the single-layer case \(L=1\) recovers results obtained by Rocks and Mehta [14, 15], and by Hastie et al. [5] (see Appendix D). In the slightly more general case of unstructured weights but structured features, we have

**Corollary 3.3**.: _If \(\tilde{\mathbf{\Sigma}}_{\ell}=\mathbf{I}_{n_{\ell}}\) for \(\ell=1,\ldots,L\), but \(\tilde{\mathbf{\Sigma}}_{0}\neq\mathbf{I}_{n_{0}}\), we have, for any target satisfying \(\|\tilde{\mathbf{w}}_{*}\|^{2}=n_{0}\)._

\[\epsilon=\begin{cases}\big{(}\sum_{\ell=1}^{L}\frac{1}{\alpha_{\ell}-1}\big{)} \kappa_{0}\psi(\kappa_{0})-\frac{\kappa_{0}^{2}}{\mu_{0}}\psi^{\prime}(\kappa_ {0})+\big{(}\frac{1-\mu_{0}}{\mu_{0}}+\sum_{\ell=1}^{L}\frac{1}{\alpha_{\ell}- 1}\big{)}\eta^{2},&\alpha_{0},\alpha_{\min}>1\\ \frac{\kappa_{0\min}\psi(\kappa_{\min})}{1-\alpha_{\min}}+\frac{\alpha_{\min} }{1-\alpha_{\min}}\eta^{2},&\alpha_{\min}<1,\alpha_{\min}<\alpha_{0}\\ \frac{\alpha_{0}}{1-\alpha_{0}}\eta^{2},&\alpha_{0}<1,\alpha_{0}<\alpha_{\min}.\end{cases}\] (26)

Proof of Corollary 3.3.: (26) follows from substituting the results of Corollary 3.2 into (23). 

In the special case \(L=1\), this recovers the result obtained using rigorous methods in contemporaneous work by Bach [40], posted to the arXiv one day after the first version of our work [43]. Here, as the data spectrum and target vector enter the generalization error in nearly the same way as in the case of linear regression, all of the intuitions developed in that case can be carried over [16, 17, 18, 4, 5, 6, 7].

Another useful simplification can be obtained by further averaging over isotropically-distributed teachers \(\tilde{\mathbf{w}}_{*}\sim\mathcal{N}(\mathbf{0},\mathbf{I}_{n_{0}})\), which gives

**Corollary 3.4**.: _Let \(\bar{\epsilon}=\mathbb{E}_{\tilde{\mathbf{w}}_{*}\sim\mathcal{N}(\mathbf{0}, \mathbf{I}_{n_{0}})}[\epsilon]\). Then, we have_

\[\bar{\epsilon}=\begin{cases}\big{(}1+\sum_{\ell=1}^{L}\frac{1-\mu_{\ell}}{ \mu_{\ell}}\big{)}\frac{\kappa_{0}}{\alpha_{0}}+\big{(}\sum_{\ell=0}^{L}\frac {1-\mu_{\ell}}{\mu_{\ell}}\big{)}\eta^{2},&\alpha_{0},\alpha_{\min}>1\\ \frac{\alpha_{\min}\kappa_{\min}/\alpha_{0}}{1-\alpha_{\min}}+\frac{\alpha_{ \min}}{1-\alpha_{\min}}\eta^{2},&\alpha_{\min}<1,\alpha_{\min}<\alpha_{0}\\ \frac{\alpha_{0}}{1-\alpha_{0}}\eta^{2},&\alpha_{0}<1,\alpha_{0}<\alpha_{\min}.\end{cases}\] (27)

Proof of Corollary 3.4.: Observing that \(\mathbb{E}_{\tilde{\mathbf{w}}_{*}}\psi(z)=-M_{\tilde{\mathbf{\Sigma}}_{0}}( -z)\), the claim follows from (23). 

In the special case of a single layer of unstructured feature weights (\(L=1\), \(\tilde{\mathbf{\Sigma}}_{1}=\mathbf{I}_{n_{1}}\)), this recovers the result of recent work by Maloney et al. [19], who used a planar diagram method to the generalization error of single-hidden-layer linear RFMs with unstructured weights (see Appendix D).

Another important simplifying case of Proposition 3.2 is the limit in which the hidden layer widths are large, in which the generalization error of the deep RFM reduces to that of a shallow model, as given by Corollary 3.1. More precisely, we have a large-width expansion given by:

Figure 1: Phase diagram of generalization in deep linear RFMs. For simplicity, we consider a model with a single hidden layer (\(L=1\)); the picture for deeper models is identical if one considers the narrowest hidden layer [13]. (a). Generalization error \(\epsilon\) for unstructured data and features from (25) as a function of training data density \(1/\alpha_{0}\) and hidden layer width \(\alpha_{1}/\alpha_{0}\) in the absence of label noise (\(\eta=0\); _left_) and in the presence of label noise (\(\eta=0.5\); _right_). (b). As in (a), but for power law structured data and weights, with \(\omega_{0}=\omega_{1}=1\), and \(\bar{\epsilon}\) given by (31). See Appendix F for numerical methods.

**Corollary 3.5**.: _In the large-width regime \(\alpha_{1},\ldots,\alpha_{L}\gg 1\), assuming that the weight spectra have finite moments, the generalization error (23) expands as_

\[\epsilon=-\tfrac{\kappa_{0}^{2}}{\mu_{0}}\psi^{\prime}(\kappa_{0})+\tfrac{1-\mu _{0}}{\mu_{0}}\eta^{2}+\big{(}\sum_{\ell=1}^{L}\tfrac{\mathbb{E}_{\delta_{ \ell}}[\hat{\sigma}_{\ell}^{2}]}{\mathbb{E}_{\delta_{\ell}}[\hat{\sigma}_{\ell }]^{2}}\tfrac{1}{\alpha_{\ell}}\big{)}(\kappa_{0}\psi(\kappa_{0})+\eta^{2})+ \mathcal{O}(\alpha_{1}^{-2},\ldots,\alpha_{L}^{-2})\] (28)

_in the regime \(\alpha_{0}>1\); if \(\alpha_{0}<1\) the generalization error does not depend on the hidden layer widths so long as they are greater than 1._

Proof of Corollary 3.5.: See Appendix E. 

## 4 How does weight structure affect generalization?

The first salient feature of the learning curves given by Proposition 3.2 is that the addition of weight structure does not alter the phase diagram of generalization, which is illustrated in Figure 1. There are three qualitatively distinct phases present, depending on the data density and minimum layer width: the overparameterized regime \(\alpha_{0},\alpha_{\min}>1\), the bottlenecked regime \(\alpha_{\min}<1\), \(\alpha_{\min}<\alpha_{0}\), and the overdetermined regime \(\alpha_{0}<1\), \(\alpha_{0}<\alpha_{\min}\). This dependence on the narrowest hidden layer matches our previous work on models with unstructured weights [13]1, and can be observed in the solutions to the ridge regression problem for fixed data (Appendix C). As \(\alpha_{\ell}\downarrow 1\), \(\kappa_{\ell}\downarrow 0\) and \(\mu_{\ell}\downarrow 0\), and the generalization error diverges. Similarly, the generalization error diverges as \(\alpha_{\min}\uparrow 1\), or \(\alpha_{0}\uparrow 1\) in the presence of label noise. However, there are not multiple descents in these deep linear models, consistent with the qualitative picture of the effect of nonlinearity given by previous works [9; 10].

Footnote 1: Previous works on deep RFMs have used several different parameterizations of the thermodynamic limit [3; 4; 5; 6; 7; 8; 9; 10; 11; 12; 13; 14; 15; 16; 17; 19; 20; 21]. We detail the conversion between these conventions in Appendix D.

The second salient feature of Proposition 3.2 is that the matrices \(\tilde{\bm{\Sigma}}_{\ell}\) enter the generalization error independently; there are no 'interaction' terms involving products of the correlation matrices for different layers. This decoupling is expected given that the features are Gaussian and independent across layers [30]. Moreover, under the rescaling \(\tilde{\bm{\Sigma}}_{\ell}^{\prime}=\tau_{\ell}\tilde{\bm{\Sigma}}_{\ell}\) for \(\tau_{\ell}>0\), we have \(\kappa_{\ell}^{\prime}=\tau_{\ell}\kappa_{\ell}\) and \(\mu_{\ell}^{\prime}=\mu_{\ell}\) (we show this explicitly in Appendix B). Therefore, (23) is sensitive only to the overall scale of \(\tilde{\bm{\Sigma}}_{0}\), not to the scales of \(\tilde{\bm{\Sigma}}_{1},\ldots,\tilde{\bm{\Sigma}}_{L}\). This scale-invariance can be observed directly from the ridgeless limit of the ridge regression estimator (7).

We can gain intuition for the effect of having \(\tilde{\bm{\Sigma}}_{\ell}\not\propto\mathbf{I}_{n_{\ell}}\) for \(\ell\geq 1\) through the following argument:

**Lemma 4.1**.: _Under the conditions of Proposition 3.2, in the regime \(\alpha_{0},\alpha_{\min}>1\), we have_

\[\epsilon\geq\big{(}\sum_{\ell=1}^{L}\tfrac{1}{\alpha_{\ell}-1}\big{)}\kappa_{0 }\psi(\kappa_{0})-\tfrac{\kappa_{0}^{2}}{\mu_{0}}\psi^{\prime}(\kappa_{0})+ \big{(}\tfrac{1-\mu_{0}}{\mu_{0}}+\sum_{\ell=1}^{L}\tfrac{1}{\alpha_{\ell}-1} \big{)}\eta^{2}.\] (29)

_That is, the generalization error for a given \(\tilde{\bm{\Sigma}}_{1},\cdots,\tilde{\bm{\Sigma}}_{L}\) is bounded from below by the generalization error for \(\tilde{\bm{\Sigma}}_{\ell}=\mathbf{I}_{n_{\ell}}\) for \(\ell=1,\ldots,L\)._

Proof of Lemma 4.1.: In Appendix B, we show that \(\mu_{\ell}\leq 1-1/\alpha_{\ell}\) for any weight spectrum, which implies that \((1-\mu_{\ell})/\mu_{\ell}\geq 1/(\alpha_{\ell}-1)\). Substituting these bounds in to the general expression for the generalization error in this regime from (23), the claim follows. 

Therefore, having \(\tilde{\bm{\Sigma}}_{\ell}\neq\mathbf{I}_{n_{\ell}}\) for \(\ell=1,\ldots,L\) cannot improve generalization in the \(\alpha_{0},\alpha_{\min}>1\) regime. This is consistent with the large-width expansion in Corollary 3.5, where we can apply Jensen's inequality to bound the weight-dependence of the correction as \(\mathbb{E}_{\delta_{\ell}}[\hat{\sigma}_{\ell}^{2}]/\mathbb{E}_{\delta_{\ell} }[\hat{\sigma}_{\ell}]^{2}\geq 1\), with equality only when the weights are unstructured. In other regimes, \(\tilde{\bm{\Sigma}}_{1},\cdots,\tilde{\bm{\Sigma}}_{L}\) do not affect the generalization error. In contrast, a similar argument shows that anisotropy in \(\tilde{\bm{\Sigma}}_{0}\) can be beneficial in the target-averaged case, at least in the absence of label noise. We formalize this as:

**Lemma 4.2**.: _Under the conditions of Corollary 3.4, in the absence of label noise (\(\eta=0\)), we have_

\[\bar{\epsilon}\leq\begin{cases}\big{(}1+\sum_{\ell=1}^{L}\tfrac{1-\mu_{\ell} }{\mu_{\ell}}\big{)}\big{(}1-\tfrac{1}{\alpha_{0}}\big{)}\mathbb{E}[\tilde{ \sigma}_{0}],&\alpha_{0},\alpha_{\min}>1\\ \frac{(1-\alpha_{\min}/\alpha_{0})}{1-\alpha_{\min}}\mathbb{E}[\tilde{\sigma}_ {0}],&\alpha_{\min}<1,\alpha_{\min}<\alpha_{0}\\ 0,&\alpha_{0}<1,\alpha_{0}<\alpha_{\min}.\end{cases}\] (30)

_That is, \(\bar{\epsilon}\) for a given \(\tilde{\bm{\Sigma}}_{0}\) is bounded from above by the generalization error for a flat spectrum \(\tilde{\bm{\Sigma}}_{0}=\mathbb{E}[\tilde{\sigma}_{0}]\mathbf{I}_{n_{0}}\)._Proof of Lemma 4.2.: In Appendix B, we show that \(\kappa_{0}\leq(\alpha_{0}-1)\mathbb{E}[\tilde{\sigma}_{0}]\). As its defining equation (22) is of the same form as (20), the corresponding bound for \(\kappa_{\min}\) follows immediately: \(\kappa_{\min}\leq(\alpha_{0}/\alpha_{\min}-1)\mathbb{E}[\tilde{\sigma}_{0}]\). Substituting these bounds into (27) with \(\eta=0\), the claim follows. 

If \(\mathbb{E}[\tilde{\sigma}_{0}]\) is not finite, then this bound is entirely vacuous: \(\bar{\epsilon}\leq\infty\). If we do not average over isotropically-distributed targets, then the effect of anisotropy in \(\tilde{\bm{\Sigma}}_{0}\) is harder to analyze. Previous works have, however, analyzed the interaction of data structure with a fixed target in great detail for models with \(L=0\) or \(L=1\), showing that targets that align with the top eigenvectors of \(\tilde{\bm{\Sigma}}_{0}\) are easier to learn [5; 16; 42; 44].

## 5 Power law spectra

We can gain further intuition for the effect of weight structure by considering an approximately solvable model for anisotropic spectra: power laws [16; 19; 28]. Power law data spectra have recently attracted considerable attention as a possible model for explaining the scaling laws of generalization observed in large language models [16; 19; 28; 32]. Maloney et al. [19] proposed a single-hidden-layer (\(L=1\)) linear RFM with power-law-structured data and unstructured weights as a model for neural scaling laws. Does introducing power law structure into the weights affect the scaling laws predicted by deep linear RFMs? We have the following result:

**Corollary 5.1**.: _At finite size, define each covariance matrix \(\tilde{\bm{\Sigma}}_{\ell}\) such that its \(j\)-th eigenvalue is \(\tilde{\sigma}_{\ell,j}=\tilde{\varsigma}_{\ell}(n_{\ell}/j)^{1+\omega_{\ell}}\) for some fixed scale factor \(\tilde{\varsigma}_{\ell}>0\) and exponent \(\omega_{\ell}>0\). Then, the limiting target-averaged generalization error is approximately_

\[\bar{\epsilon}\simeq\begin{cases}(1+\Omega_{L}+\sum_{\ell=1}^{L}\frac{1}{ \alpha_{\ell}-1})\chi(\alpha_{0})+\big{(}\omega_{0}+\Omega_{L}+\sum_{\ell=0}^ {L}\frac{1}{\alpha_{\ell}-1}\big{)}\eta^{2},&\alpha_{0},\alpha_{\min}>1\\ \frac{\chi(\alpha_{0}/\alpha_{\min})}{1-\alpha_{\min}}+\frac{\alpha_{\min}}{1 -\alpha_{\min}}\eta^{2},&\alpha_{\min}<1,\alpha_{\min}<\alpha_{0}\\ \frac{\alpha_{0}}{1-\alpha_{0}}\eta^{2},&\alpha_{0}<1,\alpha_{0}<\alpha_{\min},\end{cases}\] (31)

_where \(\Omega_{L}=\sum_{\ell=1}^{L}\omega_{\ell}\) and for \(z>1\) we have \(\chi(z)\simeq-M_{\tilde{\bm{\Sigma}}_{0}}^{-1}(z)/z\) given by \(\chi(z)=\tilde{\varsigma}_{0}\left\{k(z^{\omega_{0}}-1)+\left[2+\omega_{0}(1- k)\right](1-1/z)\right\}\) for \(k=\operatorname{sinc}[\pi/(1+\omega_{0})]^{-(1+\omega_{0})}\)._

Proof of Corollary 5.1.: Using the dictionary of notation in Appendix D, we can plug the approximate solutions for \(\kappa_{\ell}\) and \(\mu_{\ell}\) derived by Maloney et al. [19] into (27) to obtain (31). 

Therefore, the power law exponents \(\omega_{1},\cdots,\omega_{L}\) of the weight covariances beyond the first layer, which enter only through their sum \(\Omega_{L}\), do not affect the scaling laws of the generalization error with the dataset size and network widths. In particular, in the absence of label noise (\(\eta=0\)) we can approximate the scaling of (31) in the regimes of large or small hidden layer width by

\[\bar{\epsilon}\sim\begin{cases}\alpha_{0}^{\omega_{0}},&\alpha_{\min}>1, \alpha_{0}\gg 1,\\ (\alpha_{0}/\alpha_{\min})^{\omega_{0}},&\alpha_{\min}<1,\alpha_{0}/\alpha_{ \min}\gg 1,\end{cases}\] (32)

which recovers the results found by Maloney et al. [19] for \(L=1\) with unstructured weights. This behavior, and the agreement of (31) with numerical experiments, is illustrated in Figure 2. Consistent with Lemma 4.1, generalization with power-law weight structure is never better than with unstructured weights, as can be seen by comparing (31) with (25).

## 6 Bayesian inference and the Gibbs estimator at large prior variance

Thus far, we have focused on ridge regression (6). Though this is the most commonly-considered estimator in studies of random feature models [3; 4; 5; 6; 7; 19; 20; 17; 18; 17; 19], one might ask whether our qualitative findings--in particular, that feature weight structure beyond the first layer is generally harmful for generalization--carry over to other estimators. Our approach to Proposition 3.2 is easily extensible to the setting of _zero-temperature Bayesian inference_, which has recently attracted substantial interest [13; 27; 34; 45; 46; 47], sparked by work from Li and Sompolinsky [34]. In this case, we take seriously the Gibbs distribution \(p(\mathbf{v})\propto e^{-\beta L}\), which in the ridge regression case was simply a convenient tool, and interpret it as the Bayes posterior for a Gaussian likelihood of variance \(1/\beta\) and a Gaussian prior with covariance \(\bm{\Gamma}_{L+1}/(\beta\lambda)\). It is in this context conventional to fix \(\lambda=1/\beta\), such that the prior variance does not scale with \(\beta\). We can then study the average of the generalization error (13) under this posterior in the zero-temperature limit \(\beta\to\infty\), which we refer to as the generalization error of the Gibbs estimator. We emphasize that this is not identical to the Bayesian minimum mean squared error (MMSE) estimator given by the posterior mean, which would coincide with the ridgeless estimator in the zero-temperature limit (see Appendix A).

For a deep RFM, this simply has the effect of adding a "thermal" variance term to the generalization error of the ridgeless estimator, which we describe in detail in Appendices A and C. We have:

**Proposition 6.1**.: _With the same setup as in Proposition 3.2, the generalization error of the Gibbs estimator for a RFM is_

\[\epsilon_{\mathrm{BRF}}=\epsilon_{\mathrm{ridgeless}}+\begin{cases}\prod_{ \ell=0}^{L}\frac{\kappa_{\ell}}{\alpha_{\ell}},&\alpha_{0},\alpha_{\min}>1\\ 0,&\text{otherwise},\end{cases}\] (33)

_where \(\epsilon_{\mathrm{ridgeless}}\) is given by Proposition 3.2, and \(\kappa_{\ell}\) is defined as in (20)._

Proof of Proposition 6.1.: We derive (33) alongside Proposition 3.2 in Appendix A. 

The Gibbs estimator is sensitive to the scale of the random feature weight distributions through \(\kappa_{\ell}\), while as noted above the ridgeless estimator is not sensitive to their overall scale. This direct dependence on \(\kappa_{\ell}\) means that the simple argument of Lemma 4.1 cannot be applied. Indeed, in the limit of large prior variance, where the thermal variance term dominates, structure can improve the performance of the Gibbs estimator. We make this result precise in the following lemma:

**Lemma 6.1**.: _In the setting of Proposition 6.1, consider Bayesian RFMs with weight covariances scaled as \(\tau_{\bar{\epsilon}}\tilde{\bm{\Sigma}}_{\ell}\) for \(\ell=1,\ldots,L\). Then, in the non-trivial regime \(\alpha_{0},\alpha_{\min}>1\) where the thermal variance is non-vanishing, we have_

\[\lim_{\tau_{1},\ldots,\tau_{L}\to\infty}\frac{\epsilon_{\mathrm{BRF}}}{\prod_{ \ell=1}^{L}\tau_{\ell}}=\prod_{\ell=0}^{L}\frac{\kappa_{\ell}}{\alpha_{\ell} }\leq\frac{\kappa_{0}}{\alpha_{0}}\varsigma^{2}\prod_{\ell=1}^{L}\big{(}1- \frac{1}{\alpha_{\ell}}\big{)},\] (34)

_where the scalars \(\kappa_{\ell}\) are defined in terms of the un-scaled covariances \(\tilde{\bm{\Sigma}}_{\ell}\) as in (20) and \(\varsigma^{2}\equiv\prod_{\ell=1}^{L}\mathbb{E}_{\tilde{\sigma}_{\ell}}[\tilde {\sigma}_{\ell}]\). Therefore, in the limit of large prior variance, including structure in the weight priors is generically advantageous for generalization. If \(\mathbb{E}_{\tilde{\sigma}_{\ell}}[\tilde{\sigma}_{\ell}]\) is not finite, then the bound is vacuous._

Proof of Lemma 6.1.: The first part of (34) follows from (33) using the scaling properties of \(\kappa_{\ell}\), while the bound follows from the bounds on \(\kappa_{\ell}\) derived as part of Lemma 4.2. 

In contrast, weight structure is generally harmful for the Bayesian RFM in the limit of small prior variance, as its performance then coincides with the ridgeless RFM, as can be seen from the scaling

Figure 2: Generalization for power-law spectra. (a). Target-averaged generalization error \(\bar{\epsilon}\) as a function of training data density \(1/\alpha_{0}\) for shallow models (\(L=1\)) of varying hidden layer width \(\alpha_{1}/\alpha_{0}\) in the absence of label noise (\(\eta=0\)). Here, the data and weight spectra have identical power law decay \(\omega_{0}=\omega_{1}=1\). (b). As in (a), but in the presence of label noise (\(\eta=1/2\)). (c). As in (b), but for fixed hidden layer width \(\alpha_{1}/\alpha_{0}=4\), fixed data exponent \(\omega_{0}=1\), and varying weight exponents \(\omega_{1}\). In all cases, solid lines show the predictions of (31), while dots with error bars show the mean and standard error over 100 realizations of numerical experiments with \(n_{0}=1000\). See Appendix F for details of our numerical methods.

of \(\kappa_{\ell}\). This example illustrates that there are cases in which, depending on the estimator used, weight structure in deeper layers can sometimes be helpful for generalization. However, whereas the ridgeless estimator is commonly used in practice, the Gibbs estimator is less standard, and the limit of large prior variance is certainly artificial. Therefore, we emphasize that we give this example to show that the behavior of the ridgeless estimator is not entirely general, not to show that weight structure can be helpful in practical settings.

## 7 Discussion

We have computed learning curves for models with many layers of structured Gaussian random features learning a linear target function, showing that structure beyond the first layer is generally detrimental for generalization. This result is consistent with the intuition that in deep linear models learning a single target direction it is sufficient to modify the representation only at the first layer [13; 36]. It will be interesting to investigate whether this intuition carries over to nonlinear networks learning complex tasks, particularly including multi-index targets [35; 48]. Moreover, we have considered only linear, Gaussian models. As mentioned in the Introduction, past works have established Gaussian equivalence theorems for nonlinear RFMs with unstructured Gaussian feature weights. It will be important to investigate the effect of feature weight structure on Gaussian equivalence in future work, and determine whether our qualitative results carry over to nonlinear RFMs in the proportional limit.

Though our results are obtained using the replica trick, and we do not address the possibility of replica symmetry breaking, they should be rigorously justifiable given the convexity of the ridge regression problem [29; 33; 41]. We note that the replica approach makes it straightforward to handle models of any finite depth [30]. The relevant averages could of course be computed with alternative random matrix theory techniques, which could allow for a fully rigorous proof [5; 19; 20; 21]. Another more challenging setting to study with either the replica trick or rigorous techniques would be that in which one allows for correlations between weights in different layers. This setting could qualitatively capture aspects of feature learning in deep networks, which induces couplings across depth [45].

In closing, we note that RFMs with structured weights may also have relevance for biological neural networks. A recent study by Pandey et al. [39] considered RFMs with a single layer of random features (\(L=1\)) with correlated rows (\(\mathbf{\Gamma}_{1}\neq\mathbf{I}_{n_{0}}\)). In several biologically-inspired settings, they showed that introducing this structure could improve generalization, consistent with our results. More broadly, biological neural networks are imbued with rich priors [49]; investigating what insights deep structured models can afford for neuroscience will be an interesting subject for further study.

## Acknowledgments and Disclosure of Funding

We thank Alexander Atanasov, Blake Bordelon, Benjamin S. Ruben, and James B. Simon for helpful discussions and comments on a draft of our manuscript. JAZ-V and CP were supported by NSF Award DMS-2134157 and NSF CAREER Award IIS-2239780. CP received additional support from a Sloan Research Fellowship. This work has been made possible in part by a gift from the Chan Zuckerberg Initiative Foundation to establish the Kempner Institute for the Study of Natural and Artificial Intelligence.

## References

* Belkin et al. [2019] Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal. Reconciling modern machine-learning practice and the classical bias-variance trade-off. _Proceedings of the National Academy of Sciences_, 116(32):15849-15854, 2019. doi:https://doi.org/10.1073/pnas.1903070116.
* Zhang et al. [2021] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning (still) requires rethinking generalization. _Communications of the ACM_, 64(3):107-115, 2021. doi: 10.1145/3446776.
* Mei and Montanari [2019] Song Mei and Andrea Montanari. The generalization error of random features regression: Precise asymptotics and the double descent curve. _Communications on Pure and Applied Mathematics_, 75, 2019. doi: 10.1002/cpa.22008.
* Bartlett et al. [2020] Peter L. Bartlett, Philip M. Long, Gabor Lugosi, and Alexander Tsigler. Benign overfitting in linear regression. _Proceedings of the National Academy of Sciences_, 117(48):30063-30070, 2020. doi: 10.1073/pnas.1907378117. URL https://www.pnas.org/doi/abs/10.1073/pnas.1907378117.
* 986, 2022. doi: 10.1214/21-AOS2133. URL https://doi.org/10.1214/21-AOS2133.
* 1347, 2020. doi: 10.1214/19-AOS1849. URL https://doi.org/10.1214/19-AOS1849.
* Hu and Lu [2023] Hong Hu and Yue M. Lu. Universality laws for high-dimensional learning with random features. _IEEE Transactions on Information Theory_, 69(3):1932-1964, 2023. doi: 10.1109/TIT.2022.3217698.
* d'Ascoli et al. [2020] Stephane d'Ascoli, Maria Refinetti, Giulio Biroli, and Florent Krzakala. Double trouble in double descent: Bias and variance(s) in the lazy regime. In _International Conference on Machine Learning_, pages 2280-2290. PMLR, 2020. URL https://proceedings.mlr.press/v119/d-ascoli20a.html.
* d'Ascoli et al. [2020] Stephane d'Ascoli, Levent Sagun, and Giulio Biroli. Triple descent and the two kinds of overfitting: where & why do they appear? In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, _Advances in Neural Information Processing Systems_, volume 33, pages 3058-3069. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/1fd09c5f59a8ff35d499c0ee25a1d47e-Paper.pdf.
* Adlam and Pennington [2020] Ben Adlam and Jeffrey Pennington. The neural tangent kernel in high dimensions: Triple descent and a multi-scale theory of generalization. In _International Conference on Machine Learning_, pages 74-84. PMLR, 2020. URL https://proceedings.mlr.press/v119/adlam20a.html.
* Adlam and Pennington [2020] Ben Adlam and Jeffrey Pennington. Understanding double descent requires a fine-grained bias-variance decomposition. In _Advances in Neural Information Processing Systems_, volume 33, pages 11022-11032, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/7d420e2b2939762031eed0447a9be19f-Abstract.html.
* Mel and Pennington [2022] Gabriel Mel and Jeffrey Pennington. Anisotropic random feature regression in high dimensions. In _International Conference on Learning Representations_, 2022. URL https://openreview.net/forum?id=JfAwaw28BmX.
* Zavatone-Veth et al. [2022] Jacob A. Zavatone-Veth, William L. Tong, and Cengiz Pehlevan. Contrasting random and learned features in deep Bayesian linear regression. _Physical Review E_, 105:064118, Jun 2022. doi: 10.1103/PhysRevE.105.064118. URL https://link.aps.org/doi/10.1103/PhysRevE.105.064118.
* Rocks and Mehta [2022] Jason W. Rocks and Pankaj Mehta. Memorizing without overfitting: Bias, variance, and interpolation in overparameterized models. _Physical Review Research_, 4:013201, Mar 2022. doi: 10.1103/PhysRevResearch.4.013201. URL https://link.aps.org/doi/10.1103/PhysRevResearch.4.013201.

* Rocks and Mehta [2022] Jason W. Rocks and Pankaj Mehta. Bias-variance decomposition of overparameterized regression with random linear features. _Physical Review E_, 106:025304, Aug 2022. doi: 10.1103/PhysRevE.106.025304. URL https://link.aps.org/doi/10.1103/PhysRevE.106.025304.
* Bordelon et al. [2020] Blake Bordelon, Abdulkadir Canatar, and Cengiz Pehlevan. Spectrum dependent learning curves in kernel regression and wide neural networks. In Hal Daume III and Aarti Singh, editors, _Proceedings of the 37th International Conference on Machine Learning_, volume 119 of _Proceedings of Machine Learning Research_, pages 1024-1034. PMLR, 13-18 Jul 2020. URL https://proceedings.mlr.press/v119/bordelon20a.html.
* Canatar et al. [2021] Abdulkadir Canatar, Blake Bordelon, and Cengiz Pehlevan. Spectral bias and task-model alignment explain generalization in kernel regression and infinitely wide neural networks. _Nature Communications_, 12(1):2914, May 2021. ISSN 2041-1723. doi: 10.1038/s41467-021-23103-1. URL https://doi.org/10.1038/s41467-021-23103-1.
* Simon et al. [2022] James B. Simon, Madeline Dickens, Dhruva Karkada, and Michael R. DeWeese. The Eigen-learning framework: A conservation law perspective on kernel regression and wide neural networks. _arXiv_, 2022. doi: https://doi.org/10.48550/arXiv.2110.03922.
* Maloney et al. [2022] Alexander Maloney, Daniel A. Roberts, and James Sully. A solvable model of neural scaling laws. _arXiv_, 2022. doi: 10.48550/ARXIV.2210.16859.
* Schroder et al. [2023] Dominik Schroder, Hugo Cui, Daniil Dmitriev, and Bruno Loureiro. Deterministic equivalent and error universality of deep random features learning. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, _Proceedings of the 40th International Conference on Machine Learning_, volume 202 of _Proceedings of Machine Learning Research_, pages 30285-30320. PMLR, 23-29 Jul 2023. URL https://proceedings.mlr.press/v202/schroder23a.html.
* Bosch et al. [2023] David Bosch, Ashkan Panahi, and Babak Hassibi. Precise asymptotic analysis of deep random feature models. _arXiv_, 2023. doi: 10.48550/ARXIV.2302.06210. URL https://arxiv.org/abs/2302.06210.
* Lee et al. [2020] Jaehoon Lee, Samuel Schoenholz, Jeffrey Pennington, Ben Adlam, Lechao Xiao, Roman Novak, and Jascha Sohl-Dickstein. Finite versus infinite neural networks: an empirical study. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, _Advances in Neural Information Processing Systems_, volume 33, pages 15156-15172. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/ad086f59924ffe0773f8d0ca22ea712-Paper.pdf.
* Atanasov et al. [2023] Alexander Atanasov, Blake Bordelon, Sabarish Sainathan, and Cengiz Pehlevan. The onset of variance-limited behavior for networks in the lazy and rich regimes. In _International Conference on Learning Representations_, 2023. URL https://openreview.net/forum?id=JLINxPOVTh7.
* Nakkiran et al. [2021] Preetum Nakkiran, Gal Kaplan, Yamini Bansal, Tristan Yang, Boaz Barak, and Ilya Sutskever. Deep double descent: where bigger models and more data hurt. _Journal of Statistical Mechanics: Theory and Experiment_, 2021(12):124003, 12 2021. doi: 10.1088/1742-5468/ac3a74. URL https://dx.doi.org/10.1088/1742-5468/ac3a74.
* Gerace et al. [2020] Federica Gerace, Bruno Loureiro, Florent Krzakala, Marc Mezard, and Lenka Zdeborova. Generalisation error in learning with random features and the hidden manifold model. In Hal Daume III and Aarti Singh, editors, _Proceedings of the 37th International Conference on Machine Learning_, volume 119 of _Proceedings of Machine Learning Research_, pages 3452-3462. PMLR, 13-18 Jul 2020. URL https://proceedings.mlr.press/v119/gerace20a.html.
* Montanari and Saeed [2022] Andrea Montanari and Basil N. Saeed. Universality of empirical risk minimization. In Po-Ling Loh and Maxim Raginsky, editors, _Proceedings of Thirty Fifth Conference on Learning Theory_, volume 178 of _Proceedings of Machine Learning Research_, pages 4310-4312. PMLR, 02-05 Jul 2022. URL https://proceedings.mlr.press/v178/montanari22a.html.

* Cui et al. [2021] Hugo Cui, Florent Krzakala, and Lenka Zdeborova. Bayes-optimal learning of deep random networks of extensive-width. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, _Proceedings of the 40th International Conference on Machine Learning_, volume 202 of _Proceedings of Machine Learning Research_, pages 6468-6521. PMLR, 23-29 Jul 2023. URL https://proceedings.mlr.press/v202/cui23b.html.
* Bahri et al. [2021] Yasaman Bahri, Ethan Dyer, Jared Kaplan, Jaehoon Lee, and Utkarsh Sharma. Explaining neural scaling laws. _arXiv_, 2021. doi:10.48550/ARXIV.2102.06701. URL https://arxiv.org/abs/2102.06701.
* Mezard et al. [1987] Marc Mezard, Giorgio Parisi, and Miguel Angel Virasoro. _Spin Glass Theory and Beyond: An Introduction to the Replica Method and Its Applications_. World Scientific Publishing Company, 1987. doi:https://doi.org/10.1142/0271.
* Zavatone-Veth and Pehlevan [2023] Jacob A. Zavatone-Veth and Cengiz Pehlevan. Replica method for eigenvalues of real Wishart product matrices. _SciPost Physics Core_, 6:026, 2023. doi:10.21468/SciPostPhysCore.6.2.026. URL https://scipost.org/10.21468/SciPostPhysCore.6.2.026.
* Caponnetto and De Vito [2007] A. Caponnetto and E. De Vito. Optimal rates for the regularized least-squares algorithm. _Foundations of Computational Mathematics_, 7(3):331-368, 07 2007. ISSN 1615-3383. doi:10.1007/s10208-006-0196-8. URL https://doi.org/10.1007/s10208-006-0196-8.
* Kaplan et al. [2020] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. _arXiv_, 2020. doi:10.48550/ARXIV.2001.08361. URL https://arxiv.org/abs/2001.08361.
* Engel and van den Broeck [2001] Andreas Engel and Christian van den Broeck. _Statistical Mechanics of Learning_. Cambridge University Press, 2001. doi:https://doi.org/10.1017/CBO9781139164542.
* Li and Sompolinsky [2021] Qianyi Li and Haim Sompolinsky. Statistical mechanics of deep linear neural networks: The backpropagating kernel renormalization. _Physical Review X_, 11:031059, 09 2021. doi:10.1103/PhysRevX.11.031059.
* Radhakrishnan et al. [2022] Adityanarayanan Radhakrishnan, Daniel Beaglehole, Parthe Pandit, and Mikhail Belkin. Feature learning in neural networks and kernel machines that recursively learn features. _arXiv_, 2022. doi:10.48550/ARXIV.2212.13881. URL https://arxiv.org/abs/2212.13881.
* Shan and Sompolinsky [2022] Haozhe Shan and Haim Sompolinsky. Minimum perturbation theory of deep perceptual learning. _Physical Review E_, 106:064406, Dec 2022. doi:10.1103/PhysRevE.106.064406. URL https://link.aps.org/doi/10.1103/PhysRevE.106.064406.
* Hanin and Zlokapa [2023] Boris Hanin and Alexander Zlokapa. Bayesian interpolation with deep linear networks. _Proceedings of the National Academy of Sciences_, 120(23):e2301345120, 2023. doi:10.1073/pnas.2301345120. URL https://www.pnas.org/doi/abs/10.1073/pnas.2301345120.
* Burda et al. [2005] Zdzislaw Burda, Jerzy Jurkiewicz, and Bartlomiej Waclaw. Spectral moments of correlated Wishart matrices. _Physical Review E_, 71:026111, Feb 2005. doi:10.1103/PhysRevE.71.026111. URL https://link.aps.org/doi/10.1103/PhysRevE.71.026111.
* Pandey et al. [2022] Biraj Pandey, Marius Pachitariu, Bingni W. Brunton, and Kameron Decker Harris. Structured random receptive fields enable informative sensory encodings. _PLOS Computational Biology_, 18(10):1-28, 10 2022. doi:10.1371/journal.pcbi.1010484. URL https://doi.org/10.1371/journal.pcbi.1010484.
* Bach [2023] Francis Bach. High-dimensional analysis of double descent for linear regression with random projections. _arXiv_, 2023. doi:https://doi.org/10.48550/arXiv.2303.01372.
* Barbier et al. [2021] Jean Barbier, Dmitry Panchenko, and Manuel Saenz. Strong replica symmetry for high-dimensional disordered log-concave Gibbs measures. _Information and Inference: A Journal of the IMA_, 12 2021. ISSN 2049-8772. doi:10.1093/imaiai/iaab027. URL https://doi.org/10.1093/imaiai/iaab027. iaab027.

* Loureiro et al. [2021] Bruno Loureiro, Cedric Gerbelot, Hugo Cui, Sebastian Goldt, Florent Krzakala, Marc Mezard, and Lenka Zdeborova. Learning curves of generic features maps for realistic datasets with a teacher-student model. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan, editors, _Advances in Neural Information Processing Systems_, volume 34, 2021. URL https://proceedings.neurips.cc/paper/2021/hash/9704a4fc48ae88598dcbdcdf5f7f3fdef-Abstract.html.
* Zavatone-Veth and Pehlevan [2023] Jacob A. Zavatone-Veth and Cengiz Pehlevan. Learning curves for deep structured Gaussian feature models. _arXiv_, 2023. doi:https://doi.org/10.48550/arXiv.2303.00564.
* Canatar et al. [2021] Abdulkadir Canatar, Blake Bordelon, and Cengiz Pehlevan. Out-of-distribution generalization in kernel regression. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan, editors, _Advances in Neural Information Processing Systems_, volume 34, pages 12600-12612. Curran Associates, Inc., 2021. URL https://proceedings.neurips.cc/paper/2021/file/691dcb1d65f31967a874d18383b9da75-Paper.pdf.
* Zavatone-Veth et al. [2022] Jacob A Zavatone-Veth, Abdulkadir Canatar, Benjamin S Ruben, and Cengiz Pehlevan. Asymptotics of representation learning in finite Bayesian neural networks. _Journal of Statistical Mechanics: Theory and Experiment_, 2022(11):114008, nov 2022. doi:10.1088/1742-5468/ac98a6. URL https://dx.doi.org/10.1088/1742-5468/ac98a6.
* Zavatone-Veth and Pehlevan [2021] Jacob A Zavatone-Veth and Cengiz Pehlevan. Depth induces scale-averaging in overparameterized linear Bayesian neural networks. In _Asilomar Conference on Signals, Systems, and Computers_, volume 55, 2021. doi:10.1109/IEEECONF53345.2021.9723137.
* Ariosto et al. [2022] S. Ariosto, R. Pacelli, M. Pastore, F. Ginelli, M. Gherardi, and P. Rotondo. Statistical mechanics of deep learning beyond the infinite-width limit. _arXiv_, 2022. doi:10.48550/ARXIV.2209.04882. URL https://arxiv.org/abs/2209.04882.
* Zavatone-Veth et al. [2023] Jacob A. Zavatone-Veth, Sheng Yang, Julian A. Rubinfien, and Cengiz Pehlevan. Neural networks learn to magnify areas near decision boundaries. _arXiv_, 2023. doi:10.48550/ARXIV.2301.11375. URL https://arxiv.org/abs/2301.11375.
* Braun et al. [2022] Lukas Braun, Clementine Carla Juliette Domine, James E Fitzgerald, and Andrew M Saxe. Exact learning dynamics of deep linear networks with prior knowledge. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, _Advances in Neural Information Processing Systems_, volume 35, 2022. URL https://openreview.net/forum?id=1Jx2vng-KiC.
* Muirhead [2009] Robb J Muirhead. _Aspects of Multivariate Statistical Theory_. John Wiley & Sons, 2009. doi:10.1002/9780470316559.

Derivation of Proposition 3.2

In this Appendix, we sketch our replica-theory approach to computing the learning curves, which leads to Proposition 3.2. Many of the steps of this calculation are all but identical to our previous works on replica approaches to the spectra of product Wishart random matrices [30], and on unstructured deep Gaussian random feature models [13], so we will sketch the major steps rather than spelling out all the details of the algebra.

### Gibbs distribution and replica free energy

We start by introducing a Gibbs distribution at fictitious inverse temperature \(\beta\) associated with the ridge regression loss

\[L=\frac{1}{2}\left\|\frac{1}{\sqrt{n_{0}}}\mathbf{X}\mathbf{F} \mathbf{v}-\mathbf{y}\right\|^{2}+\frac{\lambda}{2}\|\mathbf{\Gamma}_{L+1}^{- 1/2}\mathbf{v}\|_{2}^{2},\] (A.1)

with partition function

\[Z(\beta,\mathcal{D})\propto\int d\mathbf{v}\,e^{-\beta L(\mathbf{ v},\mathcal{D})},\] (A.2)

where we denote by \(\mathcal{D}\) all randomness in the problem. For any \(\lambda>0\), in the zero-temperature limit \(\beta\to\infty\), this Gibbs distribution concentrates around the unique minimum of the loss \(E\)[29, 33].

For the purpose of the replica computation, it is convenient to consider instead the partition function of the posterior of a related Bayesian model, which corresponds to absorbing \(\beta\lambda\) into a redefinition of \(\mathbf{\Gamma}_{L+1}\), and treating the ridge penalty as a Gaussian prior

\[\mathbf{v}\sim_{\text{prior}}\mathcal{N}(\mathbf{0},\mathbf{ \Gamma}_{L+1}).\] (A.3)

We can then recover the partition function of the ridge regression model by undoing the rescaling: \(\mathbf{\Gamma}_{L+1}\leftarrow\mathbf{\Gamma}_{L+1}/(\beta\lambda)\). Without this re-scaling--i.e., in the case in which the prior variance is held fixed as the temperature goes to zero--this is the Gibbs estimator in the zero-temperature limit, i.e., a Bayesian model with Gaussian likelihood of vanishing variance [13, 34, 37, 45, 46].

This gives us the partition function

\[Z=\mathbb{E}_{\mathbf{v}\sim\mathcal{N}(\mathbf{0},\mathbf{ \Gamma}_{L+1})}\exp\left[-\frac{\beta}{2}\sum_{\mu=1}^{p}[g(\mathbf{x}_{\mu}; \mathbf{v},\mathbf{F})-y_{\mu}]^{2}\right],\] (A.4)

which is the extension to structured priors of the Gibbs estimator partition function considered in [13]. By standard arguments, we expect the quenched free energy

\[f=-\lim_{p,n_{0},\ldots,n_{L}\to\infty}\frac{1}{p}\log Z,\] (A.5)

to be self-averaging in the thermodynamic limit, i.e., \(f=\mathbb{E}_{\mathcal{D}}f\) almost surely [29, 33]. To compute the limiting quenched average, we use the replica trick, and write

\[f=-\lim_{m\to 0}\lim_{p,n_{0},\ldots,n_{L}\to\infty}\frac{1}{ pm}\log\mathbb{E}_{\mathcal{D}}Z^{m},\] (A.6)

where we evaluate the moments \(\mathbb{E}_{\mathcal{D}}Z^{m}\) for positive integer \(m\), and assume that they can be analytically continued to \(m\to 0\).

Following previous work [13, 30], we can compute the quenched averages and integrate out the weights by introducing order parameters

\[(C_{0})^{ab}=\frac{1}{n_{0}}(\mathbf{F}\mathbf{v}^{a}-\mathbf{w} _{*})^{\top}\mathbf{\Sigma}_{0}(\mathbf{F}\mathbf{v}^{b}-\mathbf{w}_{*}),\] (A.7)

for \(\ell=0\),

\[(C_{\ell})^{ab}=\frac{1}{n_{\ell}\cdots n_{L}}(\mathbf{v}^{a}) ^{\top}\mathbf{U}_{L}^{\top}\cdots\mathbf{U}_{\ell+1}^{\top}\mathbf{\Sigma}_ {\ell}\mathbf{U}_{\ell+1}\cdots\mathbf{U}_{L}\mathbf{v}^{b}\] (A.8)for \(\ell=1,\ldots,L-1\) and

\[(C_{L})^{ab}=\frac{1}{n_{L}}(\mathbf{v}^{a})^{\top}\mathbf{\Sigma}_{L}\mathbf{v}^{b},\] (A.9)

along with corresponding Lagrange multipliers \(\mathbf{\hat{C}}_{\ell}\), which yields

\[\mathbb{E}_{\mathcal{D}}Z^{m}=\int\frac{d\mathbf{C}_{0}\,d\hat{ \mathbf{C}}_{0}}{(4\pi i/n_{0})^{m(m+1)/2}}\int\frac{d\mathbf{C}_{1}\,d\hat{ \mathbf{C}}_{1}}{(4\pi i/n_{1})^{m(m+1)/2}}\cdots\int\frac{d\mathbf{C}_{L}\,d \hat{\mathbf{C}}_{L}}{(4\pi i/n_{L})^{m(m+1)/2}}\exp\left[-\frac{pm}{2}S\right]\] (A.10)

for

\[mS =\log\det(\mathbf{I}_{m}+\beta\mathbf{C}_{0}+\beta\eta^{2} \mathbf{1}_{m}\mathbf{1}_{m}^{\top})\] \[\quad-\alpha_{0}\frac{1}{n_{0}}\,\mathrm{v}(\tilde{\mathbf{w}}_{ \ast}\mathbf{1}_{m}^{\top})^{\top}[\hat{\mathbf{C}}_{0}\otimes\tilde{ \mathbf{\Sigma}}_{0}][\mathbf{I}_{mn_{0}}-\mathbf{C}_{1}\hat{\mathbf{C}}_{0} \otimes\tilde{\mathbf{\Sigma}}_{0}]^{-1}\,\mathrm{v}(\tilde{\mathbf{w}}_{ \ast}\mathbf{1}_{m}^{\top})\] \[\quad+\sum_{\ell=0}^{L}\alpha_{\ell}\left[\mathrm{tr}(\mathbf{C} _{\ell}\hat{\mathbf{C}}_{\ell})+\frac{1}{n_{\ell}}\log\det(\mathbf{I}_{mn_{ \ell}}-\mathbf{C}_{\ell+1}\hat{\mathbf{C}}_{\ell}\otimes\tilde{\mathbf{\Sigma }}_{\ell})\right],\] (A.11)

where we let \(\mathbf{C}_{L+1}=\mathbf{I}_{m}\) and

\[\tilde{\mathbf{\Sigma}}_{\ell}=\mathbf{\Gamma}_{\ell+1}^{1/2} \mathbf{\Sigma}_{\ell}\mathbf{\Gamma}_{\ell}^{1/2}\] (A.12)

for \(\ell=0,\ldots,L\). We note that \(\otimes\) here denotes the Kronecker product, and we use the convention that the standard matrix product has higher precedence than the Kronecker product, i.e., \(\mathbf{AB}\otimes\mathbf{C}=(\mathbf{AB})\otimes\mathbf{C}\). Importantly, the quantity of interest--the generalization error--is simply given by the diagonal elements of \(\mathbf{C}_{0}\), i.e., \(\epsilon=(C_{0})^{aa}\). Therefore, if we can solve for the order parameters at zero temperature, we will obtain the generalization error.

In the thermodynamic limit, the integral over these order parameters can be evaluated using the method of steepest descent. We make a replica symmetric _Ansatz_, and seek saddle points of the form

\[\mathbf{C}_{\ell} =q_{\ell}\mathbf{I}_{m}+c_{\ell}\mathbf{I}_{m}\mathbf{1}_{m}^{ \top},\] (A.13) \[\hat{\mathbf{C}}_{\ell} =\hat{q}_{\ell}\mathbf{I}_{m}+\hat{c}_{\ell}\mathbf{I}_{m} \mathbf{1}_{m}^{\top}.\] (A.14)

Under this _Ansatz_, we have

\[S =\log(1+\beta q_{0})+\frac{\beta(c_{0}+\eta^{2})}{1+\beta q_{0}}\] \[\quad-\alpha_{0}\frac{1}{n_{0}}(\tilde{\mathbf{w}}_{\ast}^{\top} \tilde{\mathbf{\Sigma}}_{0}(\mathbf{I}_{n_{0}}-q_{1}\hat{q}_{0}\tilde{ \mathbf{\Sigma}}_{0})^{-1}\tilde{\mathbf{w}}_{\ast})\hat{q}_{0}\] \[\quad+\sum_{\ell=0}^{L}\alpha_{\ell}\bigg{(}q_{\ell}\hat{q}_{ \ell}+q_{\ell}\hat{c}_{\ell}+c_{\ell}\hat{q}_{\ell}+\mathbb{E}_{\hat{\sigma}_{ \ell}}\log(1-q_{\ell+1}\hat{q}_{\ell}\tilde{\sigma}_{\ell})\] \[\qquad\qquad\qquad-(q_{\ell+1}\hat{c}_{\ell}+c_{\ell+1}\hat{q}_{ \ell})\mathbb{E}_{\hat{\sigma}_{\ell}}\left[\frac{\tilde{\sigma}_{\ell}}{1-q_ {\ell+1}\hat{q}_{\ell}\tilde{\sigma}_{\ell}}\right]\bigg{)}\] \[\quad+\mathcal{O}(m)\] (A.15)

to leading order in \(m\), where we recall the boundary condition \(q_{L+1}=1\), \(c_{L+1}=0\)[30]. The resulting saddle point equations can be simplified to give a closed system for the replica non-uniform components,

\[\alpha_{0}\hat{q}_{0} =-\frac{\beta}{1+\beta q_{0}}\] (A.16) \[\alpha_{\ell}\hat{q}_{\ell} =\alpha_{\ell-1}\hat{q}_{\ell-1}\mathbb{E}_{\tilde{\sigma}_{\ell-1} }\left[\frac{\tilde{\sigma}_{\ell-1}}{1-q_{\ell}\hat{q}_{\ell-1}\tilde{\sigma }_{\ell-1}}\right] (\ell=1,\ldots,L)\] (A.17) \[q_{\ell} =q_{\ell+1}\mathbb{E}_{\tilde{\sigma}_{\ell}}\left[\frac{\tilde{ \sigma}_{\ell}}{1-q_{\ell+1}\hat{q}_{\ell}\tilde{\sigma}_{\ell}}\right] (\ell=0,\ldots,L)\] (A.18)with the boundary condition \(q_{L+1}=1\), and a linear system for the replica uniform components,

\[\alpha_{0}\hat{c}_{0} =\frac{\beta^{2}(c_{0}+\eta^{2})}{(1+\beta q_{0})^{2}}\] (A.19) \[\alpha_{1}\hat{c}_{1} =\alpha_{0}\frac{1}{n_{0}}(\tilde{\mathbf{w}}_{*}^{\top}\tilde{ \mathbf{\Sigma}}_{0}^{2}(\mathbf{I}_{n_{0}}-q_{1}\hat{q}_{0}\tilde{\mathbf{ \Sigma}}_{0})^{-2}\tilde{\mathbf{w}}_{*})\hat{q}_{0}^{2}\] \[\quad+\alpha_{0}\left(\hat{c}_{0}\mathbb{E}_{\hat{\sigma}_{0}} \left[\frac{\tilde{\sigma}_{0}}{1-q_{1}\hat{q}_{0}\tilde{\sigma}_{0}}\right]+ (q_{1}\hat{c}_{0}+c_{1}\hat{q}_{0})\hat{q}_{0}\mathbb{E}_{\hat{\sigma}_{0}} \left[\left(\frac{\tilde{\sigma}_{0}}{1-q_{1}\hat{q}_{0}\tilde{\sigma}_{0}} \right)^{2}\right]\right)\] (A.20) \[\frac{\alpha_{\ell}}{\alpha_{\ell-1}}\hat{c}_{\ell} =\hat{c}_{\ell-1}\mathbb{E}_{\hat{\sigma}_{\ell-1}}\left[\frac{ \tilde{\sigma}_{\ell-1}}{1-q_{\ell}\hat{q}_{\ell-1}\tilde{\sigma}_{\ell-1}}\right]\] \[\quad+(q_{\ell}\hat{c}_{\ell-1}+c_{\ell}\hat{q}_{\ell-1})\hat{q} _{\ell-1}\mathbb{E}_{\hat{\sigma}_{\ell-1}}\left[\left(\frac{\tilde{\sigma}_{ \ell-1}}{1-q_{\ell}\hat{q}_{\ell-1}\tilde{\sigma}_{\ell-1}}\right)^{2}\right] (\ell=2,\ldots,L)\] (A.21) \[c_{0} =\frac{1}{n_{0}}(\tilde{\mathbf{w}}_{*}^{\top}\tilde{\mathbf{ \Sigma}}_{0}(\mathbf{I}_{n_{0}}-q_{1}\hat{q}_{0}\tilde{\mathbf{\Sigma}}_{0})^{ -2}\tilde{\mathbf{w}}_{*})\] \[\quad+\left(c_{1}\mathbb{E}_{\hat{\sigma}_{0}}\left[\frac{ \tilde{\sigma}_{0}}{1-q_{1}\hat{q}_{0}\tilde{\sigma}_{0}}\right]+(q_{1}\hat{c }_{0}+c_{1}\hat{q}_{0})q_{1}\mathbb{E}\left[\left(\frac{\tilde{\sigma}_{0}}{1 -q_{1}\hat{q}_{0}\tilde{\sigma}_{0}}\right)^{2}\right]\right)\] (A.22) \[c_{\ell} =c_{\ell+1}\mathbb{E}_{\hat{\sigma}_{\ell}}\left[\frac{\tilde{ \sigma}_{\ell}}{1-q_{\ell+1}\hat{q}_{\ell}\tilde{\sigma}_{\ell}}\right]\] \[\quad+(q_{\ell+1}\hat{c}_{\ell}+c_{\ell+1}\hat{q}_{\ell})q_{\ell +1}\mathbb{E}_{\hat{\sigma}_{\ell}}\left[\left(\frac{\tilde{\sigma}_{\ell}}{1 -q_{\ell+1}\hat{q}_{\ell}\tilde{\sigma}_{\ell}}\right)^{2}\right]\qquad\qquad (\ell=1,\ldots,L)\] (A.23)

with the boundary condition \(c_{L+1}=0\).

### Converting between the Gibbs and maximum-likelihood estimators

As our primary aim is to study ridge regression, we must now account for the fact that the prior over the readout weights scales with the inverse temperature \(\beta\). In particular, we have a prior with scaled covariance \(\boldsymbol{\Gamma}_{L+1}/(\beta\lambda)\), where \(\boldsymbol{\Gamma}_{L+1}\) does not scale with \(\beta\). If we perform this rescaling in (A.16) and (A.16), we can see that the re-scaled order parameters

\[\bar{q}_{\ell} =\beta\lambda q_{\ell}\] (A.24) \[\bar{\hat{q}}_{\ell} =\frac{1}{\beta\lambda}\hat{q}_{\ell}\] (A.25) \[\bar{c}_{\ell} =c_{\ell}\] (A.26) \[\bar{\hat{\varepsilon}}_{\ell} =\frac{1}{(\beta\lambda)^{2}}\hat{c}_{\ell}\] (A.27)

obey an identical system of equations to the original order parameters in the Bayesian case at inverse temperature

\[\beta=\frac{1}{\lambda}.\] (A.28)

Therefore, if we can solve the saddle point equations for the Gibbs estimator in the zero-temperature limit, we can simply read off the corresponding result for the ridge regression estimator in the ridgeless limit. The important difference is that the replica nonuniform component \(q_{0}\) of \(\mathbf{C}_{0}\) is \(\mathcal{O}(1/\beta)\) in the ridge regression case, hence only the replica uniform component \(c_{0}\) contributes to the generalization error. We note that this allows one to read off the generalization error of a deep linear RFM with unstructured features from the results of our previous work [13] simply by setting the Bayesian prior variance \(\sigma^{2}\) to zero.

### Solutions for the generalization error

The replica-symmetric saddle point equations in (A.16) and (A.19) are nearly identical to those analyzed our computation of the maximum eigenvalue of a structured Wishart product matrix [30],which in turn are related to those in our original paper on unstructured deep linear RFMs [13] by the replacement of the spectral moment generating function of the identity matrix with the appropriate spectral generating functions. Given this similarity, and the fact that we have provided extensive exposition of how to solve such systems in those previous works, we will merely state the results for the order parameters relevant to the computation of the generalization error.

Let

\[M_{\mathbf{\tilde{\Sigma}}_{\ell}}(z)=\lim_{n_{\ell}\to\infty}\frac{1}{n_{\ell }}\operatorname{tr}[\mathbf{\tilde{\Sigma}}_{\ell}(z\mathbf{I}_{n_{\ell}}- \mathbf{\tilde{\Sigma}}_{\ell})^{-1}]\] (A.29)

be the moment generating function of \(\mathbf{\tilde{\Sigma}}_{\ell}\), with functional inverse \(M_{\mathbf{\tilde{\Sigma}}_{\ell}}^{-1}(z)\). Then, at finite temperature, after eliminating the Lagrange multipliers, the replica nonuniform components of the order parameters are given by

\[q_{\ell}=\prod_{j=\ell}^{L}\frac{A}{\alpha_{j}}M_{\mathbf{\tilde{\Sigma}}_{j}} ^{-1}\left(\frac{A}{\alpha_{j}}\right)\] (A.30)

for \(\ell=0,\ldots,L\), where

\[A=q_{0}\dot{q}_{0}=-\frac{\beta q_{0}}{1+\beta q_{0}}\] (A.31)

satisfies the closed equation

\[-\frac{1}{\beta}=\frac{A+1}{A}\prod_{\ell=0}^{L}\frac{A}{\alpha_{\ell}}M_{ \mathbf{\tilde{\Sigma}}_{\ell}}^{-1}\left(\frac{A}{\alpha_{\ell}}\right).\] (A.32)

From [30], we recognize this as the self-consistent equation for the moment generating function \(M=A\) of the feature kernel \(\mathbf{K}=\mathbf{XFF}^{\top}\mathbf{X}^{\top}/n_{0}\), evaluated at \(-1/\beta\). Even in the unstructured case, this equation must in general be solved numerically at finite temperature [30].

Given a solution to this equation, we can solve the system of linear equations (A.19) for \(c_{0}\), mirroring the computation of the extremal eigenvalues of structured product Wishart matrices in [30]. After eliminating the Lagrange multipliers, this calculation boils down to solving a three-term recurrence relation, which is detailed in previous works [13, 30]. We therefore simply state the result of this computation here. Let \(\zeta=-A\), which then satisfies

\[\lambda=\frac{1-\zeta}{\zeta}\prod_{\ell=0}^{L}\frac{-\zeta}{\alpha_{\ell}}M_ {\mathbf{\tilde{\Sigma}}_{\ell}}^{-1}\left(-\frac{\zeta}{\alpha_{\ell}} \right).\] (A.33)

For \(\ell=0,\ldots,L\), let

\[\kappa_{\ell}=-M_{\mathbf{\tilde{\Sigma}}_{\ell}}^{-1}\left(-\frac{\zeta}{ \alpha_{\ell}}\right)\] (A.34)

so that \(\kappa_{\ell}\) satisfies

\[\frac{\zeta}{\alpha_{\ell}}=\mathbb{E}_{\tilde{\sigma}_{\ell}}\left[\frac{ \tilde{\sigma}_{\ell}}{\kappa_{\ell}+\tilde{\sigma}_{\ell}}\right].\] (A.35)

Viewing \(\kappa_{\ell}\) as a function of \(\zeta\), we may alternatively write the self-consistent equation for \(\zeta\) as

\[\frac{1}{\beta}=\frac{1-\zeta}{\zeta}\prod_{\ell=0}^{L}\frac{\zeta}{\alpha_{ \ell}}\kappa_{\ell}(\zeta)\] (A.36)

In terms of \(\kappa_{\ell}\), let

\[\mu_{\ell} =-\frac{\alpha_{\ell}}{\zeta}\kappa_{\ell}M_{\mathbf{\tilde{ \Sigma}}_{\ell}}^{\prime}\left(-\kappa_{\ell}\right)\] (A.37) \[=1-\frac{\alpha_{\ell}}{\zeta}\mathbb{E}_{\tilde{\sigma}_{\ell}} \left[\left(\frac{\tilde{\sigma}_{\ell}}{\kappa_{\ell}+\tilde{\sigma}_{\ell}} \right)^{2}\right]\] (A.38)We then finally have

\[\left[1+\left(\sum_{\ell=0}^{L}\frac{1-\mu_{\ell}}{\mu_{\ell}} \right)(1-\zeta)\right]c_{0} =\frac{1}{\mu_{0}}\frac{1}{n_{0}}(\tilde{\mathbf{w}}_{*}^{\top} \tilde{\boldsymbol{\Sigma}}_{0}(\kappa_{0}\mathbf{I}_{n_{0}}+\tilde{\boldsymbol {\Sigma}}_{0})^{-2}\tilde{\mathbf{w}}_{*})\kappa_{0}^{2}\] \[\quad+\left(\sum_{\ell=1}^{L}\frac{1-\mu_{\ell}}{\mu_{\ell}} \right)\frac{1}{n_{0}}(\tilde{\mathbf{w}}_{*}^{\top}\tilde{\boldsymbol{\Sigma}} _{0}(\kappa_{0}\mathbf{I}_{n_{0}}+\tilde{\boldsymbol{\Sigma}}_{0})^{-1}\tilde{ \mathbf{w}}_{*})\kappa_{0}\] \[\quad+\bigg{(}\sum_{\ell=0}^{L}\frac{1-\mu_{\ell}}{\mu_{\ell}} \bigg{)}\zeta\eta^{2}.\] (A.39)

Using the mapping of Appendix A.2 and again defining the weighted generating function

\[\psi(z)=\lim_{n_{0}\to\infty}\frac{1}{n_{0}}\tilde{\mathbf{w}}_{*}^{\top} \tilde{\boldsymbol{\Sigma}}_{0}(z\mathbf{I}_{n_{0}}+\tilde{\boldsymbol{\Sigma} }_{0})^{-1}\tilde{\mathbf{w}}_{*}.\] (A.40)

this yields Proposition 3.1.

We now want to extract the zero-temperature/ridgeless limit. As \(\beta\to\infty\), the self-consistent equation for \(\zeta\) admits the solution

\[\zeta=1,\] (A.41)

valid for \(\alpha_{\ell}>1\) for all \(\ell\), which gives \(q_{0}\sim\mathcal{O}(1)\), the solution

\[\zeta=\alpha_{0},\] (A.42)

valid for \(\alpha_{0}<1\), \(\alpha_{0}<\alpha_{1},\ldots,\alpha_{L}\), which gives \(q_{0}\sim\mathcal{O}(1/\beta)\), and, for \(\ell_{*}=0,\ldots,L\), the solutions

\[\zeta=\alpha_{\ell_{*}},\] (A.43)

valid for \(\alpha_{\ell_{*}}<1\), \(\alpha_{\ell_{*}}<\alpha_{0}\), \(\alpha_{\ell_{*}}<\alpha_{\ell}\) for all \(\ell\neq\ell_{*}\), which also give \(q_{0}\sim\mathcal{O}(1/\beta)\). These solutions mirror those found in the unstructured setting [13]. We remark that, as in [13], we can determine the regimes in which each solution is physical by demanding that the order parameters \(q_{\ell}\) are non-negative.

For the \(\zeta\to 1\) solution, we immediately have

\[c_{0} =\frac{1}{\mu_{0}}\frac{1}{n_{0}}(\tilde{\mathbf{w}}_{*}^{\top} \tilde{\boldsymbol{\Sigma}}_{0}(\kappa_{0}\mathbf{I}_{n_{0}}+\tilde{ \boldsymbol{\Sigma}}_{0})^{-2}\tilde{\mathbf{w}}_{*})\kappa_{0}^{2}\] \[\quad+\left(\sum_{\ell=1}^{L}\frac{1-\mu_{\ell}}{\mu_{\ell}} \right)\frac{1}{n_{0}}(\tilde{\mathbf{w}}_{*}^{\top}\tilde{\boldsymbol{ \Sigma}}_{0}(\kappa_{0}\mathbf{I}_{n_{0}}+\tilde{\boldsymbol{\Sigma}}_{0})^{-1 }\tilde{\mathbf{w}}_{*})\kappa_{0}\] \[\quad+\bigg{(}\sum_{\ell=0}^{L}\frac{1-\mu_{\ell}}{\mu_{\ell}} \bigg{)}\zeta\eta^{2},\] (A.44)

where by a minor abuse of notation we simply write \(\kappa_{\ell}\) and \(\mu_{\ell}\) for the corresponding quantities evaluated at \(\zeta=1\).

If \(\zeta\to\alpha_{\ell}\), then \(\kappa_{\ell}\downarrow 0\) and \(\mu_{\ell}\downarrow 0\). We can then apply L'Hopital's rule to evaluate the limit in A.39, which corresponds to extracting the most divergent terms on each side of A.39. For the \(\zeta=\alpha_{0}\) solution, one finds that

\[c_{0}=\frac{\alpha_{0}}{1-\alpha_{0}}\eta^{2}.\] (A.45)

Finally, for the solutions with \(\zeta=\alpha_{\ell_{*}}\) for \(\ell_{*}=1,\ldots,L\), one finds that

\[c_{0} =\frac{1}{1-\alpha_{\ell_{*}}}\frac{1}{n_{0}}(\tilde{\mathbf{w}}_{* }^{\top}\tilde{\boldsymbol{\Sigma}}_{0}(\kappa_{0}\mathbf{I}_{n_{0}}+\tilde{ \boldsymbol{\Sigma}}_{0})^{-1}\tilde{\mathbf{w}}_{*})\kappa_{0}\] \[\quad+\frac{\alpha_{\ell_{*}}}{1-\alpha_{\ell_{*}}}\eta^{2},\] (A.46)

where we must be careful to recall that \(\kappa_{0}\) now satisfies

\[\frac{\alpha_{\ell_{*}}}{\alpha_{0}}=\mathbb{E}_{\tilde{\sigma}_{0}}\left[ \frac{\tilde{\sigma}_{0}}{\kappa_{0}+\tilde{\sigma}_{0}}\right].\] (A.47)But, we recognize that \(\alpha_{\ell_{*}}=\alpha_{\text{min}}=\min\{\alpha_{1},\ldots,\alpha_{L}\}\), so we will write

\[\kappa_{\text{min}}=\kappa_{0}\bigg{|}_{\zeta=\alpha_{\text{min}}}\] (A.48)

to avoid clashing with our notation for the \(\zeta=1\) solution.

Therefore, recalling from Appendix A.2 that the generalization error for the ridge regression estimator in the ridgeless limit is simply given by \(c_{0}\), we have

\[\epsilon_{\text{ridgeless}}=\begin{cases}\big{(}\sum_{\ell=1}^{L} \frac{1-\mu_{\ell}}{\mu_{\ell}}\big{)}\kappa_{0}\psi(\kappa_{0})-\frac{\kappa _{0}^{2}}{\mu_{0}}\psi^{\prime}(\kappa_{0})+\big{(}\sum_{\ell=0}^{L}\frac{1- \mu_{\ell}}{\mu_{\ell}}\big{)}\eta^{2},&\alpha_{0},\alpha_{\text{min}}>1\\ \frac{\kappa_{\text{min}}\kappa(\kappa_{\text{min}})}{1-\alpha_{\text{min}}}+ \frac{\alpha_{\text{min}}}{1-\alpha_{\text{min}}}\eta^{2},&\alpha_{\text{min} }<1,\alpha_{\text{min}}<\alpha_{0}\\ \frac{\alpha_{0}}{1-\alpha_{0}}\eta^{2},&\alpha_{0}<1,\alpha_{0}<\alpha_{ \text{min}},\end{cases}\] (A.49)

as reported in (23), where we again define the weighted generating function

\[\psi(z)=\lim_{n_{0}\to\infty}\frac{1}{n_{0}}\tilde{\mathbf{w}}_{ *}^{\top}\tilde{\mathbf{\Sigma}}_{0}(z\mathbf{I}_{n_{0}}+\tilde{\mathbf{ \Sigma}}_{0})^{-1}\tilde{\mathbf{w}}_{*}.\] (A.50)

To obtain the average generalization error for the Gibbs estimator in the zero-temperature limit, we must account for the effect of \(q_{0}\) in the regime \(\alpha_{\ell}>1\), as in all other regimes it is \(q_{0}\sim\mathcal{O}(1/\beta)\). But, we recognize that

\[q_{0}=\prod_{j=0}^{L}\frac{-1}{\alpha_{j}}M_{\tilde{\mathbf{\Sigma}}_{j}}^{-1} \left(\frac{-1}{\alpha_{j}}\right)=\prod_{\ell=0}^{L}\frac{\kappa_{\ell}}{ \alpha_{\ell}}\] (A.51)

from the definition above, hence we conclude that

\[\epsilon_{\text{BBFM}}=\epsilon_{\text{ridgeless}}+\begin{cases}\prod_{\ell=0} ^{L}\frac{\kappa_{\ell}}{\alpha_{\ell}},&\alpha_{0},\alpha_{\text{min}}>1\\ 0,&\alpha_{\text{min}}<1,\alpha_{\text{min}}<\alpha_{0}\\ 0,&\alpha_{0}<1,\alpha_{0}<\alpha_{\text{min}}.\end{cases}\] (A.52)

### Physical interpretation of the order parameters and thermal bias-variance decomposition

With these results in hand, we now comment on the interpretation of the replica uniform and replica non-uniform contributions to

\[\mathbf{C}_{0}=q_{0}\mathbf{I}_{m}+c_{0}\mathbf{1}_{m}\mathbf{1}_{m}^{\top}.\] (A.53)

At the saddle point, we have

\[(C_{0})^{ab}=\mathbb{E}_{\mathcal{D}}\left\langle\frac{1}{n_{0}}(\mathbf{F} \mathbf{v}^{a}-\mathbf{w}_{*})^{\top}\mathbf{\Sigma}_{0}(\mathbf{F}\mathbf{v} ^{b}-\mathbf{w}_{*})\right\rangle_{\beta},\] (A.54)

where \(\langle\cdot\rangle_{\beta}\) denotes the expectation with respect to the replicated Gibbs measure at inverse temperature \(\beta\). Under the replica-symmetric _Ansatz_, considering off-diagonal elements \(a\neq b\), we can use the fact that the replicas are initially uncoupled and identical to write

\[c_{0} =C_{0}^{ab}\] (A.55) \[=\mathbb{E}_{\mathcal{D}}\frac{1}{n_{0}}(\mathbf{F}\langle \mathbf{v}\rangle_{\beta}-\mathbf{w}_{*})^{\top}\mathbf{\Sigma}_{0}(\mathbf{F} \langle\mathbf{v}\rangle_{\beta}-\mathbf{w}_{*})\] (A.57) \[=\mathbb{E}_{\mathcal{D}}\frac{1}{n_{0}}\|\mathbf{\Sigma}_{0}^{1/ 2}(\mathbf{F}\langle\mathbf{v}\rangle_{\beta}-\mathbf{w}_{*})\|^{2}.\] (A.58)Similarly, we have

\[q_{0} =C_{0}^{aa}-C_{0}^{ab}\] (A.59) \[=\mathbb{E}_{\mathcal{D}}\left\langle\frac{1}{n_{0}}(\mathbf{F} \mathbf{v}^{a}-\mathbf{w}_{*})^{\top}\mathbf{\Sigma}_{0}(\mathbf{F}\mathbf{v}^{ a}-\mathbf{w}_{*})\right\rangle_{\beta}-c_{0}\] (A.60) \[=\mathbb{E}_{\mathcal{D}}\left\langle\frac{1}{n_{0}}(\mathbf{F} \delta\mathbf{v}+\mathbf{F}\langle\mathbf{v}\rangle_{\beta}-\mathbf{w}_{*})^{ \top}\mathbf{\Sigma}_{0}(\mathbf{F}\delta\mathbf{v}+\mathbf{F}\langle\mathbf{v} \rangle_{\beta}-\mathbf{w}_{*})\right\rangle_{\beta}-c_{0}\] (A.61) \[=\mathbb{E}_{\mathcal{D}}\left\langle\frac{1}{n_{0}}(\mathbf{F} \delta\mathbf{v})^{\top}\mathbf{\Sigma}_{0}(\mathbf{F}\delta\mathbf{v}) \right\rangle_{\beta}\] (A.63) \[=\mathbb{E}_{\mathcal{D}}\left\langle\frac{1}{n_{0}}\|\mathbf{ \Sigma}_{0}^{1/2}\mathbf{F}\delta\mathbf{v}\|^{2}\right\rangle_{\beta},\] (A.64)

where we write \(\delta\mathbf{v}=\mathbf{v}-\langle\mathbf{v}\rangle_{\beta}\). Therefore, at the saddle point, \(c_{0}\) and \(q_{0}\) correspond exactly to the bias and variance terms in the thermal bias-variance decomposition of the generalization error:

\[\mathbb{E}_{\mathcal{D}}\left\langle\frac{1}{n_{0}}\|\mathbf{ \Sigma}_{0}^{1/2}(\mathbf{F}\mathbf{v}-\mathbf{w}_{*})\|^{2}\right\rangle_{ \beta}=\mathbb{E}_{\mathcal{D}}\frac{1}{n_{0}}\|\mathbf{\Sigma}_{0}^{1/2}( \mathbf{F}\langle\mathbf{v}\rangle_{\beta}-\mathbf{w}_{*})\|^{2}+\mathbb{E}_{ \mathcal{D}}\left\langle\frac{1}{n_{0}}\|\mathbf{\Sigma}_{0}^{1/2}\mathbf{F} \delta\mathbf{v}\|^{2}\right\rangle_{\beta}.\] (A.65)

This makes concrete an argument which was presented only intuitively in [13]. As a result, if one considered the Bayesian MMSE estimator \(\hat{\mathbf{v}}=\langle\mathbf{v}\rangle_{\beta}\), the zero-temperature generalization error would simply coincide with that for the ridgeless estimator.

## Appendix B Properties of the inverse generating functions

Here, we record a few useful properties of the inverse spectral generating functions

\[\frac{1}{\alpha_{\ell}}=-M_{\mathbf{\tilde{\Sigma}}_{\ell}}(- \kappa_{\ell})=\mathbb{E}_{\tilde{\sigma}_{\ell}}\left[\frac{\tilde{\sigma}_{ \ell}}{\kappa_{\ell}+\tilde{\sigma}_{\ell}}\right]\] (B.1)

and their relatives

\[\mu_{\ell}=-\alpha_{\ell}\kappa_{\ell}M_{\mathbf{\tilde{\Sigma}}_{\ell}}^{ \prime}(-\kappa_{\ell})=1-\alpha_{\ell}\mathbb{E}_{\tilde{\sigma}_{\ell}} \left[\left(\frac{\tilde{\sigma}_{\ell}}{\kappa_{\ell}+\tilde{\sigma}_{\ell}} \right)^{2}\right].\] (B.2)

These results are used in the proofs of Lemmas

### Dependence on width

Implicitly differentiating the self-consistent equation defining \(\kappa_{\ell}\), we have

\[\frac{d\kappa_{\ell}}{d(1/\alpha_{\ell})}=-\frac{1}{\mathbb{E}_{ \tilde{\sigma}_{\ell}}\left[\frac{\tilde{\sigma}_{\ell}}{(\kappa_{\ell}+\tilde {\sigma}_{\ell})^{2}}\right]},\] (B.3)

showing that \(\kappa_{\ell}\) is a decreasing function of \(1/\alpha_{\ell}\). As \(1/\alpha_{\ell}\downarrow 0\), we should have \(\kappa_{\ell}\uparrow\infty\), while as \(1/\alpha_{\ell}\uparrow 1\), we should have \(\kappa_{\ell}\downarrow 0\).

### Behavior under rescaling

Consider the re-scaling \(\mathbf{\tilde{\Sigma}}_{\ell}^{\prime}=\tau_{\ell}\mathbf{\tilde{\Sigma}}_{\ell}\) for \(\tau_{\ell}>0\). Then, we have \(\kappa_{\ell}\) and \(\tilde{\kappa}_{\ell}^{\prime}\) given by

\[\frac{1}{\alpha_{\ell}}=-M_{\mathbf{\tilde{\Sigma}}_{\ell}}(- \kappa_{\ell})=\mathbb{E}_{\tilde{\sigma}_{\ell}}\left[\frac{\tilde{\sigma}_{ \ell}}{\kappa_{\ell}+\tilde{\sigma}_{\ell}}\right]\] (B.4)

and

\[\frac{1}{\alpha_{\ell}}=-M_{\mathbf{\tilde{\Sigma}}_{\ell}^{\prime}}(-\kappa_{ \ell}^{\prime})=\mathbb{E}_{\tilde{\sigma}_{\ell}}\left[\frac{\tau_{\ell} \tilde{\sigma}_{\ell}}{\kappa_{\ell}^{\prime}+\tau_{\ell}\tilde{\sigma}_{\ell}}\right]\] (B.5)

respectively. We can then see that we should have

\[\kappa_{\ell}^{\prime}=\tau_{\ell}\kappa_{\ell}.\] (B.6)

### Bound on \(\kappa_{\ell}\) in terms of isotropic spectrum

We now prove that

\[\kappa_{\ell}\leq(\alpha_{\ell}-1)\mathbb{E}[\tilde{\sigma}_{\ell}]\] (B.7)

in the relevant regime \(\alpha_{\ell}>1\). For any \(z>0\),

\[\tilde{\sigma}_{\ell}\mapsto\frac{\tilde{\sigma}_{\ell}}{(z+\tilde{\sigma}_{ \ell})}\] (B.8)

is a concave function of \(\tilde{\sigma}_{\ell}\geq 0\), hence Jensen's inequality implies that

\[\mathbb{E}_{\tilde{\sigma}_{\ell}}\left[\frac{\tilde{\sigma}_{\ell}}{(z+ \tilde{\sigma}_{\ell})}\right]\leq\frac{\mathbb{E}[\tilde{\sigma}_{\ell}]}{z+ \mathbb{E}[\tilde{\sigma}_{\ell}]}.\] (B.9)

Then, note that

\[z\mapsto\mathbb{E}_{\tilde{\sigma}_{\ell}}\left[\frac{\tilde{\sigma}_{\ell}}{ z+\tilde{\sigma}_{\ell}}\right]\] (B.10)

and

\[z\mapsto\frac{\mathbb{E}[\tilde{\sigma}_{\ell}]}{z+\mathbb{E}[\tilde{\sigma}_ {\ell}]}\] (B.11)

are both decreasing functions of \(z\geq 0\), and both are equal to 1 when \(z=0\). Thus, if \(\kappa_{\ell}>0\) solves

\[\frac{1}{\alpha_{\ell}}=\mathbb{E}_{\tilde{\sigma}_{\ell}}\left[\frac{\tilde{ \sigma}_{\ell}}{\kappa_{\ell}+\tilde{\sigma}_{\ell}}\right]\] (B.12)

as specified by its definition and \(\bar{\kappa}_{\ell}>0\) solves

\[\frac{1}{\alpha_{\ell}}=\frac{\mathbb{E}[\tilde{\sigma}_{\ell}]}{\bar{\kappa} _{\ell}+\mathbb{E}[\tilde{\sigma}_{\ell}]},\] (B.13)

we must have

\[\kappa_{\ell}\leq\bar{\kappa}_{\ell}.\] (B.14)

But, we can easily see that \(\bar{\kappa}_{\ell}=(\alpha_{\ell}-1)\mathbb{E}[\tilde{\sigma}_{\ell}]\), hence the claim follows.

### Bound on \(\mu_{\ell}\) terms of isotropic spectrum

We next prove that

\[\mu_{\ell}\leq 1-\frac{1}{\alpha_{\ell}}\] (B.15)

in the relevant regime \(\alpha_{\ell}>1\). By definition, we have

\[\mu_{\ell}=1-\alpha_{\ell}\mathbb{E}_{\tilde{\sigma}_{\ell}}\left[\left(\frac {\tilde{\sigma}_{\ell}}{\kappa_{\ell}+\tilde{\sigma}_{\ell}}\right)^{2}\right].\] (B.16)

By Jensen's inequality and the definition of \(\kappa_{\ell}\), we have

\[\mathbb{E}_{\tilde{\sigma}_{\ell}}\left[\left(\frac{\tilde{ \sigma}_{\ell}}{\kappa_{\ell}+\tilde{\sigma}_{\ell}}\right)^{2}\right] \geq\mathbb{E}_{\tilde{\sigma}_{\ell}}\left[\frac{\tilde{\sigma}_ {\ell}}{\kappa_{\ell}+\tilde{\sigma}_{\ell}}\right]^{2}\] (B.17) \[=\frac{1}{\alpha_{\ell}^{2}}.\] (B.18)

As \(\alpha_{\ell}>1\) by assumption, this bound is always positive. Therefore, we conclude the desired claim.

Simplifying the generalization error for fixed data

In this appendix, we show how the ridgeless generalization error can be simplified in each regime for fixed data. Using the solution to the ridge regression problem (6),

\[\hat{\mathbf{v}}=\frac{1}{\sqrt{n_{0}}}\left(\lambda\mathbf{\Gamma}_{L+1}^{-1}+ \frac{1}{n_{0}}\mathbf{F}^{\top}\mathbf{X}^{\top}\mathbf{X}\mathbf{F}\right)^{- 1}\mathbf{F}^{\top}\mathbf{X}^{\top}\mathbf{y},\] (C.1)

we have

\[\epsilon =\lim_{\lambda\downarrow 0}\lim_{p,n_{0},\dots,n_{L}\rightarrow\infty} \mathbb{E}_{\mathcal{D}}\frac{1}{n_{0}}\|\mathbf{\Sigma}_{0}^{1/2}(\mathbf{F} \hat{\mathbf{v}}-\mathbf{w}_{*})\|^{2}\] (C.2) \[=\lim_{\lambda\downarrow 0}\lim_{p,n_{0},\dots,n_{L}\rightarrow\infty} \mathbb{E}_{\mathcal{D}}\frac{1}{n_{0}}\left\|\frac{1}{\sqrt{n_{0}}}\mathbf{ \Sigma}_{0}^{1/2}\mathbf{F}\left(\lambda\mathbf{\Gamma}_{L+1}^{-1}+\frac{1}{n _{0}}\mathbf{F}^{\top}\mathbf{X}^{\top}\mathbf{X}\mathbf{F}\right)^{-1} \mathbf{F}^{\top}\mathbf{X}^{\top}\mathbf{y}-\mathbf{\Sigma}_{0}^{1/2} \mathbf{w}_{*}\right\|^{2}.\] (C.3)

Following our discussion in the main text, we may set \(\mathbf{\Gamma}_{L+1}=\mathbf{I}_{n_{L}}\) without loss of generality, as otherwise we may re-define \(\mathbf{\Sigma}_{L}\). Then, we have

\[\epsilon=\lim_{\lambda\downarrow 0}\lim_{p,n_{0},\dots,n_{L} \rightarrow\infty}\mathbb{E}_{\mathcal{D}}\frac{1}{n_{0}}\left\|\frac{1}{ \sqrt{n_{0}}}\mathbf{\Sigma}_{0}^{1/2}\mathbf{F}\left(\lambda\mathbf{I}_{n_{L} }+\frac{1}{n_{0}}\mathbf{F}^{\top}\mathbf{X}^{\top}\mathbf{X}\mathbf{F}\right) ^{-1}\mathbf{F}^{\top}\mathbf{X}^{\top}\mathbf{y}-\mathbf{\Sigma}_{0}^{1/2} \mathbf{w}_{*}\right\|^{2}.\] (C.4)

In the subsequent sections, we will simplify this expression in each regime.

For the Gibbs estimator, we must account for the additional contribution to the generalization error from thermal variance. Following our previous work [13], we may compute the bias and variance terms directly from the posterior moment generating function of the readout weight vector,

\[\mathcal{Z}(\mathbf{j}) \propto\int d\mathbf{v}\,\exp\left(-\frac{\beta}{2}\|n_{0}^{-1/2 }\mathbf{X}\mathbf{F}\mathbf{v}-\mathbf{y}\|^{2}-\frac{1}{2}\|\mathbf{\Gamma }_{L+1}^{-1/2}\mathbf{v}\|^{2}+\mathbf{j}^{\top}\mathbf{v}\right)\] (C.5) \[\propto\exp\left(\beta n_{0}^{-1/2}\mathbf{y}^{\top}\mathbf{X} \mathbf{F}(\mathbf{\Gamma}_{L+1}^{-1}+\beta n_{0}^{-1}\mathbf{F}^{\top} \mathbf{X}^{\top}\mathbf{X}\mathbf{F})^{-1}\mathbf{j}\right.\] \[\qquad\qquad+\frac{1}{2}\mathbf{j}^{\top}(\mathbf{\Gamma}_{L+1} ^{-1}+\beta n_{0}^{-1}\mathbf{F}^{\top}\mathbf{X}^{\top}\mathbf{X}\mathbf{F}) ^{-1}\mathbf{j}\right),\] (C.6)

yielding

\[\langle\mathbf{v}\rangle_{\beta}=\frac{1}{\sqrt{n_{0}}}\left(\frac{1}{\beta} \mathbf{\Gamma}_{L+1}^{-1}+\frac{1}{n_{0}}\mathbf{F}^{\top}\mathbf{X}^{\top} \mathbf{X}\mathbf{F}\right)^{-1}\mathbf{F}^{\top}\mathbf{X}^{\top}\mathbf{y}\] (C.7)

and

\[\langle\mathbf{v}\mathbf{v}^{\top}\rangle_{\beta}-\langle\mathbf{v}\rangle_{ \beta}\langle\mathbf{v}\rangle_{\beta}^{\top}=\left(\mathbf{\Gamma}_{L+1}^{- 1}+\frac{\beta}{n_{0}}\mathbf{F}^{\top}\mathbf{X}^{\top}\mathbf{X}\mathbf{F} \right)^{-1}.\] (C.8)

We then can see that

\[\langle\mathbf{v}\rangle_{\beta}=\hat{\mathbf{v}}\bigg{|}_{\lambda =1/\beta},\] (C.9)

which is precisely in agreement with the conversion in Appendix A.2. Considering the thermal bias-variance decomposition of the generalization error for the Gibbs estimator,

\[\mathbb{E}_{\mathcal{D}}\left\langle\frac{1}{n_{0}}\|\mathbf{ \Sigma}_{0}^{1/2}(\mathbf{F}\mathbf{v}-\mathbf{w}_{*})\|^{2}\right\rangle_{ \beta}=\mathbb{E}_{\mathcal{D}}\frac{1}{n_{0}}\|\mathbf{\Sigma}_{0}^{1/2}( \mathbf{F}\langle\mathbf{v}\rangle_{\beta}-\mathbf{w}_{*})\|^{2}+\mathbb{E}_{ \mathcal{D}}\left\langle\frac{1}{n_{0}}\|\mathbf{\Sigma}_{0}^{1/2}\mathbf{F} \delta\mathbf{v}\|^{2}\right\rangle_{\beta},\] (C.10)

we can then see that the bias term at zero temperature coincides exactly with the generalization error of the ridgeless estimator, as we found in Appendix A. The variance term is

\[\lim_{\beta\rightarrow\infty}\mathbb{E}_{\mathcal{D}}\left\langle\frac{1}{n_{0 }}\|\mathbf{\Sigma}_{0}^{1/2}\mathbf{F}\delta\mathbf{v}\|^{2}\right\rangle_{ \beta}=\lim_{\beta\rightarrow\infty}\mathbb{E}_{\mathcal{D}}\frac{1}{n_{0}} \operatorname{tr}\left[\mathbf{\Sigma}_{0}\mathbf{F}\left(\mathbf{\Gamma}_{L+1} ^{-1}+\frac{\beta}{n_{0}}\mathbf{F}^{\top}\mathbf{X}^{\top}\mathbf{X} \mathbf{F}\right)^{-1}\mathbf{F}^{\top}\right].\] (C.11)In both the bias and variance terms, we can see that we may set \(\bm{\Gamma}_{L+1}=\mathbf{I}_{n_{L}}\) without loss of generality, as otherwise we may simply re-scale \(\bm{\Sigma}_{L}\) as discussed in Lemma 2.1. Then, we need only consider the thermal variance term

\[\lim_{\beta\to\infty}\mathbb{E}_{\mathcal{D}}\left\langle\frac{1}{n_{0}}\|\bm{ \Sigma}_{0}^{1/2}\mathbf{F}\delta\mathbf{v}\|^{2}\right\rangle_{\beta}=\lim_{ \beta\to\infty}\mathbb{E}_{\mathcal{D}}\frac{1}{n_{0}}\operatorname{tr} \left[\bm{\Sigma}_{0}\mathbf{F}\left(\mathbf{I}_{n_{L}}+\frac{\beta}{n_{0}} \mathbf{F}^{\top}\mathbf{X}^{\top}\mathbf{X}\mathbf{F}\right)^{-1}\mathbf{F}^{ \top}\right].\] (C.12)

Here, we leave the thermodynamic limit implicit to allow the expression to fit on a single line.

### The overparameterized regime

First, consider the regime \(p<\min\{n_{0},\ldots,n_{L}\}\). Here, we expect the kernel

\[\mathbf{K}=\frac{1}{n_{0}}\mathbf{X}\mathbf{F}\mathbf{F}^{\top}\mathbf{X}^{\top}\] (C.13)

to be invertible with probability one in the thermodynamic limit, and with overwhelming probability at large but finite size [50]. Applying the push-through identity and passing to the ridgeless limit, we have

\[\epsilon =\lim_{\lambda\downarrow 0}\lim_{p,n_{0},\ldots,n_{L}\to\infty} \mathbb{E}_{\mathcal{D}}\frac{1}{n_{0}}\left\|\frac{1}{\sqrt{n_{0}}}\bm{ \Sigma}_{0}^{1/2}\mathbf{F}\mathbf{F}^{\top}\mathbf{X}^{\top}\left(\lambda \mathbf{I}_{p}+\frac{1}{n_{0}}\mathbf{X}\mathbf{F}\mathbf{F}^{\top}\mathbf{X} ^{\top}\right)^{-1}\mathbf{y}-\bm{\Sigma}_{0}^{1/2}\mathbf{w}_{*}\right\|^{2}\] (C.14) \[=\lim_{p,n_{0},\ldots,n_{L}\to\infty}\mathbb{E}_{\mathcal{D}} \frac{1}{n_{0}}\left\|\sqrt{n_{0}}\bm{\Sigma}_{0}^{1/2}\mathbf{F}\mathbf{F}^{ \top}\mathbf{X}^{\top}\left(\mathbf{X}\mathbf{F}\mathbf{F}^{\top}\mathbf{X}^ {\top}\right)^{-1}\mathbf{y}-\bm{\Sigma}_{0}^{1/2}\mathbf{w}_{*}\right\|^{2}.\] (C.15)

Averaging over label noise, we have

\[\epsilon =\lim_{p,n_{0},\ldots,n_{L}\to\infty}\mathbb{E}_{\mathcal{D}} \frac{1}{n_{0}}\left\|\bm{\Sigma}_{0}^{1/2}\mathbf{F}\mathbf{F}^{\top} \mathbf{X}^{\top}\left(\mathbf{X}\mathbf{F}\mathbf{F}^{\top}\mathbf{X}^{\top }\right)^{-1}\mathbf{X}\mathbf{w}_{*}-\bm{\Sigma}_{0}^{1/2}\mathbf{w}_{*} \right\|^{2}\] \[\quad+\eta^{2}\lim_{p,n_{0},\ldots,n_{L}\to\infty}\mathbb{E}_{ \mathcal{D}}\left\|\bm{\Sigma}_{0}^{1/2}\mathbf{F}\mathbf{F}^{\top}\mathbf{X} ^{\top}\left(\mathbf{X}\mathbf{F}\mathbf{F}^{\top}\mathbf{X}^{\top}\right)^{- 1}\right\|^{2}.\] (C.16)

Turning our attention to the Gibbs estimator, we can use the Woodbury identity to write the thermal variance term as

\[\mathbb{E}_{\mathcal{D}}\left\langle\frac{1}{n_{0}}\|\bm{\Sigma} _{0}^{1/2}\mathbf{F}\delta\mathbf{v}\|^{2}\right\rangle_{\beta}\] \[=\mathbb{E}_{\mathcal{D}}\frac{1}{n_{0}}\operatorname{tr}\left[ \bm{\Sigma}_{0}\mathbf{F}\left(\mathbf{I}_{n_{L}}+\frac{\beta}{n_{0}}\mathbf{ F}^{\top}\mathbf{X}^{\top}\mathbf{X}\mathbf{F}\right)^{-1}\mathbf{F}^{\top}\right]\] (C.17) \[=\mathbb{E}_{\mathcal{D}}\frac{1}{n_{0}}\operatorname{tr}\left[ \bm{\Sigma}_{0}\mathbf{F}\mathbf{F}^{\top}\right]-\mathbb{E}_{\mathcal{D}} \frac{1}{n_{0}}\operatorname{tr}\left[\bm{\Sigma}_{0}\mathbf{F}\mathbf{F}^{ \top}\mathbf{X}^{\top}\left(\beta^{-1}\mathbf{I}_{n_{L}}+\frac{1}{n_{0}} \mathbf{X}\mathbf{F}\mathbf{F}^{\top}\mathbf{X}^{\top}\right)^{-1}\mathbf{X} \mathbf{F}\mathbf{F}^{\top}\right]\] (C.18) \[=\mathbb{E}_{\mathcal{D}}\frac{1}{n_{0}}\operatorname{tr}\left[ \bm{\Sigma}_{0}\mathbf{F}\mathbf{F}^{\top}\right]-\mathbb{E}_{\mathcal{D}} \frac{1}{n_{0}}\operatorname{tr}\left[\bm{\Sigma}_{0}\mathbf{F}\mathbf{F}^{ \top}\mathbf{X}^{\top}\left(\frac{1}{n_{0}}\mathbf{X}\mathbf{F}\mathbf{F}^{ \top}\mathbf{X}^{\top}\right)^{-1}\mathbf{X}\mathbf{F}\mathbf{F}^{\top} \right]+\mathcal{O}(\beta^{-1}),\] (C.19)

where the thermodynamic limit is implied [13]. Therefore, in this regime we do not expect the thermal variance term to vanish, consistent with Proposition 6.1.

### The bottlenecked regime

If \(\min\{n_{1},\ldots,n_{L}\}<\min\{n_{0},p\}\), then the situation is slightly more complicated. Let

\[\ell_{\min}=\operatorname*{arg\,min}_{\ell}n_{\ell}\] (C.20)be the index of the narrowest hidden layer. Then, let

\[\mathbf{F}_{1}=\frac{1}{\sqrt{n_{1}\cdots n_{\ell_{\min}}}}\mathbf{U}_{1}\cdots \mathbf{U}_{\ell_{\min}}\in\mathbb{R}^{n_{0}\times n_{\min}}\] (C.21)

and

\[\mathbf{F}_{2}=\frac{1}{\sqrt{n_{\ell_{\min}+1}\cdots n_{L}}}\mathbf{U}_{\ell_ {\min}+1}\cdots\mathbf{U}_{L}\in\mathbb{R}^{n_{\min}\times n_{L}}\] (C.22)

such that

\[\mathbf{F}=\mathbf{F}_{1}\mathbf{F}_{2}.\] (C.23)

Then, the matrices \(\mathbf{F}_{1}^{\top}\mathbf{X}^{\top}\mathbf{X}\mathbf{F}_{1}\) and \(\mathbf{F}_{2}\mathbf{F}_{2}^{\top}\) are invertible with probability one, and upon passing to the ridgeless limit we have

\[\epsilon=\lim_{p,n_{0},\ldots,n_{L}\rightarrow\infty}\mathbb{E}_{\mathcal{D}} \frac{1}{n_{0}}\|\sqrt{n_{0}}\mathbf{\Sigma}_{0}^{1/2}\mathbf{F}_{1}(\mathbf{ F}_{1}^{\top}\mathbf{X}^{\top}\mathbf{X}\mathbf{F}_{1})^{-1}\mathbf{F}_{1}^{ \top}\mathbf{X}^{\top}\mathbf{y}-\mathbf{\Sigma}_{0}^{1/2}\mathbf{w}_{*}\|^{2}.\] (C.24)

Averaging over the label noise,

\[\epsilon= \lim_{p,n_{0},\ldots,n_{L}\rightarrow\infty}\mathbb{E}_{\mathcal{ D}}\frac{1}{n_{0}}\|\mathbf{\Sigma}_{0}^{1/2}\mathbf{F}_{1}(\mathbf{F}_{1}^{ \top}\mathbf{X}^{\top}\mathbf{X}\mathbf{F}_{1})^{-1}\mathbf{F}_{1}^{\top} \mathbf{X}^{\top}\mathbf{X}\mathbf{w}_{*}-\mathbf{\Sigma}_{0}^{1/2}\mathbf{w}_ {*}\|^{2}\] \[+\eta^{2}\lim_{p,n_{0},\ldots,n_{L}\rightarrow\infty}\mathbb{E}_ {\mathcal{D}}\|\mathbf{\Sigma}_{0}^{1/2}\mathbf{F}_{1}(\mathbf{F}_{1}^{\top} \mathbf{X}^{\top}\mathbf{X}\mathbf{F}_{1})^{-1}\mathbf{F}_{1}^{\top}\mathbf{ X}^{\top}\|^{2}\] (C.25)

Focusing on the label noise term, we have

\[\mathbb{E}_{\mathcal{D}}\|\mathbf{\Sigma}_{0}^{1/2}\mathbf{F}_{1}(\mathbf{F} _{1}^{\top}\mathbf{X}^{\top}\mathbf{X}\mathbf{F}_{1})^{-1}\mathbf{F}_{1}^{ \top}\mathbf{X}^{\top}\|^{2}=\mathbb{E}_{\mathcal{D}}\operatorname{tr}[ \mathbf{F}_{1}^{\top}\mathbf{\Sigma}_{0}\mathbf{F}_{1}(\mathbf{F}_{1}^{\top} \mathbf{X}^{\top}\mathbf{X}\mathbf{F}_{1})^{-1}].\] (C.26)

Then, using the fact that

\[\mathbf{X}^{\top}\mathbf{X}\sim\mathcal{W}_{n_{0}}(\mathbf{\Sigma}_{0},p),\] (C.27)

we have

\[\mathbf{F}_{1}^{\top}\mathbf{X}^{\top}\mathbf{X}\mathbf{F}_{1}\sim\mathcal{W} _{n_{\min}}(\mathbf{F}_{1}^{\top}\mathbf{\Sigma}_{0}\mathbf{F}_{1},p).\] (C.28)

Then, as we expect the matrix \(\mathbf{F}_{1}^{\top}\mathbf{\Sigma}_{0}\mathbf{F}_{1}\) to be invertible with overwhelming probability, the standard formula for the mean of an inverse-Wishart distribution [50] gives

\[\mathbb{E}_{\mathcal{D}}(\mathbf{F}_{1}^{\top}\mathbf{X}^{\top}\mathbf{X} \mathbf{F}_{1})^{-1}=\frac{1}{p-n_{\min}-1}(\mathbf{F}_{1}^{\top}\mathbf{ \Sigma}_{0}\mathbf{F}_{1})^{-1},\] (C.29)

so

\[\lim_{p,n_{0},\ldots,n_{L}\rightarrow\infty}\mathbb{E}_{\mathcal{ D}}\operatorname{tr}[\mathbf{F}_{1}^{\top}\mathbf{\Sigma}_{0}\mathbf{F}_{1}( \mathbf{F}_{1}^{\top}\mathbf{X}^{\top}\mathbf{X}\mathbf{F}_{1})^{-1}] =\lim_{p,n_{0},\ldots,n_{L}\rightarrow\infty}\frac{n_{\min}}{p-n_{ \min}-1}\] (C.30) \[=\frac{\alpha_{\min}}{1-\alpha_{\min}}.\] (C.31)

This proves that, in this regime, the label noise term does not depend on data structure, matching the result of our replica computation.

Considering the Gibbs estimator, we can see immediately that the thermal variance term is \(\mathcal{O}(\beta^{-1})\) because of the fact that \(\mathbf{F}_{1}^{\top}\mathbf{X}^{\top}\mathbf{X}\mathbf{F}_{1}\) and \(\mathbf{F}_{2}\mathbf{F}_{2}^{\top}\) are invertible with probability one. This is consistent with Proposition 6.1.

### The overdetermined regime

Finally, consider the regime in which \(n_{0}<\min\{p,n_{1},\ldots,n_{L}\}\). Then, both \(\mathbf{X}^{\top}\mathbf{X}\) and \(\mathbf{F}\mathbf{F}^{\top}\) are invertible with probability one, and we can easily compute

\[\epsilon =\lim_{p,n_{0},\ldots,n_{L}\rightarrow\infty}\lim_{\lambda \downarrow 0}\mathbb{E}_{\mathcal{D}}\frac{1}{n_{0}}\|\mathbf{\Sigma}_{0}^{1/2}( \lambda\mathbf{I}_{n_{L}}+\frac{1}{n_{0}}\mathbf{F}\mathbf{F}^{\top}\mathbf{X} ^{\top}\mathbf{X})^{-1}\frac{1}{\sqrt{n_{0}}}\mathbf{F}\mathbf{F}^{\top} \mathbf{X}^{\top}\mathbf{y}-\mathbf{w}_{*}\|^{2}\] (C.32) \[=\lim_{p,n_{0},\ldots,n_{L}\rightarrow\infty}\mathbb{E}_{ \mathcal{D}}\|\mathbf{\Sigma}_{0}^{1/2}(\mathbf{X}^{\top}\mathbf{X})^{-1} \mathbf{X}^{\top}\boldsymbol{\xi}\|^{2}\] (C.33) \[=\eta^{2}\lim_{p,n_{0},\ldots,n_{L}\rightarrow\infty}\mathbb{E}_{ \mathcal{D}}\operatorname{tr}[\mathbf{\Sigma}_{0}(\mathbf{X}^{\top}\mathbf{X})^{-1}].\] (C.34)Then,

\[(\mathbf{X}^{\top}\mathbf{X})^{-1}\sim\mathcal{W}_{n_{0}}^{-1}(\mathbf{\Sigma}_{0 }^{-1},p),\] (C.35)

so using the formula for the mean of the inverse-Wishart [50] we have

\[\epsilon =\eta^{2}\lim_{p,n_{0},\ldots,n_{L}\to\infty}\mathbb{E}_{\mathcal{ D}}\operatorname{tr}[\mathbf{\Sigma}_{0}(\mathbf{X}^{\top}\mathbf{X})^{-1}]\] (C.36) \[=\eta^{2}\lim_{p,n_{0},\ldots,n_{L}\to\infty}\frac{n_{0}}{p-n_{0}-1}\] (C.37) \[=\frac{\alpha_{0}}{1-\alpha_{0}}\eta^{2},\] (C.38)

as we found using replicas.

Here, again, we can see that the thermal variance term for the Gibbs estimator is \(\mathcal{O}(\beta^{-1})\), matching Proposition 6.1.

## Appendix D A notational dictionary

In this appendix, we show that special cases of our general result recover the results reported in previous works. This is largely a matter of translating notation, as the conventions used in different communities are often at odds with each other.

### Shallow ridgeless regression

In the shallow case \(L=0\), our general result for a fixed target (23) reduces to

\[\epsilon=\begin{cases}-\frac{\kappa_{0}^{2}}{\mu_{0}}\psi^{\prime}( \kappa_{0})+\frac{1-\mu_{0}}{\mu_{0}}\eta^{2},&\alpha_{0}>1\\ \frac{\alpha_{0}}{1-\alpha_{0}}\eta^{2},&\alpha_{0}<1\end{cases}\] (D.1)

where, writing expectation with respect to the limiting spectral distribution of \(\mathbf{\Sigma}_{0}\) as \(\mathbb{E}_{\sigma_{0}}\), we recall that \(\kappa_{0}\) is determined by the implicit equation

\[\frac{1}{\alpha_{0}}=-M_{\mathbf{\Sigma}_{0}}(-\kappa_{0})=\mathbb{E}_{\sigma _{0}}\left[\frac{\sigma_{0}}{\kappa_{0}+\sigma_{0}}\right],\] (D.2)

in terms of which we have

\[\mu_{0}=1-\alpha_{0}\mathbb{E}_{\sigma_{0}}\left[\left(\frac{\sigma_{0}}{ \kappa_{0}+\sigma_{0}}\right)^{2}\right],\] (D.3)

and that

\[\psi(z)=\lim_{n_{0}\to\infty}\frac{1}{n_{0}}\mathbf{w}_{*}^{\top}\mathbf{ \Sigma}_{0}(z\mathbf{I}_{n_{0}}+\mathbf{\Sigma}_{0})^{-1}\mathbf{w}_{*}.\] (D.4)

Working in the eigenbasis of \(\mathbf{\Sigma}_{0}\) and assuming that \(\|\mathbf{w}_{*}\|^{2}=n_{0}\), we introduce the weighted density

\[\rho(\sigma_{0})=\lim_{n_{0}\to\infty}\frac{1}{n_{0}}\sum_{j=1}^{n_{0}}(w_{*} )_{j}^{2}\delta(\sigma_{0}-\sigma_{j})\] (D.5)

in terms of which we have

\[\psi(z)=\mathbb{E}_{\sigma_{0}\sim\rho}\left[\frac{\sigma_{0}}{z+\sigma_{0}}\right]\] (D.6)

and

\[-\psi^{\prime}(z)=\mathbb{E}_{\sigma_{0}\sim\rho}\left[\frac{\sigma_{0}}{(z+ \sigma_{0})^{2}}\right].\] (D.7)

We can now make contact with the result of Hastie et al. [5]. We note that those authors use an opposite definition for \(p\) and \(n\): following the convention in the statistics literature, they use \(p\) for the dimensionality and for the number of examples, while we follow the convention in the physics literature of using for the dimensionality and for the number of examples. Then, Hastie et al. [5]'s, defined such that, in our terms,, is precisely our. Moreover, they use to denote the limiting spectral law of, and to denote the law corresponding to the weighted density we define above as. We note also that their is our. In these terms, their Theorem 2 gives the generalization error in the overparameterized regime as

(D.8)

where is defined by the implicit equation

(D.9)

Subtracting one from both sides, the implicit equation for gives

(D.10)

from which we can see that

(D.11)

Then, we have

(D.12) (D.13)

and

(D.14) (D.15) (D.16) (D.17) (D.18)

which proves the equivalence of our results. This also shows that we recover the results of other works on ridgeless kernel interpolation [4, 6, 7, 16, 17] that are in this setting equivalent to the results of Hastie et al. [5].

### Two-layer linear random feature models with unstructured weights and isotropic targets

Another special case in which we can make contact with prior work is that of a single hidden layer () and with target averaging. In this case, our general result (27) reduces to

(D.19)where in this case we find it convenient to write \(\kappa_{0}/\alpha_{0}\) and \(\alpha_{0}\kappa_{\min}/\alpha_{\min}\) in terms of \(\chi(z)\), which solves

\[1 =-zM_{\tilde{\mathbf{\Sigma}}_{0}}[-z\chi(z)]\] (D.20) \[=\mathbb{E}_{\tilde{\sigma}_{0}}\left[\frac{\tilde{\sigma}_{0}}{ \chi(z)+z^{-1}\tilde{\sigma}_{0}}\right].\] (D.21)

It is then easy to show that our result agrees with that of Maloney et al. [19]. Their notation is:

\[M =n_{0}\] (D.22) \[N =n_{1}\] (D.23) \[T =p.\] (D.24)

When \(M>N,T\), their result is, in the absence of label noise,

\[\bar{\epsilon}=\frac{1}{M}\begin{cases}\frac{1}{1-N/T}\Delta_{-1 }(N,M),&N<T\\ \frac{1}{1-T/N}\Delta_{-1}(T,M),&N>T,\end{cases}\] (D.25)

where \(\Delta_{-1}(N,M)\) solves

\[1=\operatorname{tr}[\mathbf{\Sigma}_{0}(\Delta_{-1}(N,M)\mathbf{ I}_{M}+N\mathbf{\Sigma}_{0})^{-1}]\] (D.26)

and similarly for \(\Delta_{-1}(T,M)\). To map this to our results, let us re-define

\[\bar{\Delta}_{-1}(N,M)\equiv\frac{1}{M}\Delta_{-1}(N,M),\] (D.27)

which then satisfies

\[1=\frac{1}{M}\operatorname{tr}[\mathbf{\Sigma}_{0}(\bar{\Delta}_{-1}(N,M) \mathbf{I}_{M}+(N/M)\mathbf{\Sigma}_{0})^{-1}],\] (D.28)

or

\[\frac{N}{M}=-M_{\mathbf{\Sigma}_{0}}\left(-\frac{M}{N}\bar{\Delta}_{-1}(N,M)\right)\] (D.29)

Then, we can see that, in our notation,

\[\bar{\Delta}_{-1}(N,M)=\chi\left(\frac{M}{N}\right)=\chi\left( \frac{\alpha_{0}}{\alpha_{1}}\right),\] (D.30)

while

\[\bar{\Delta}_{-1}(T,M)=\chi\left(\frac{M}{T}\right)=\chi(\alpha_{0}).\] (D.31)

Then, noting that

\[\frac{1}{1-T/N}=\frac{1}{1-1/\alpha_{1}}=\frac{\alpha_{1}}{\alpha _{1}-1}=1+\frac{1}{\alpha_{1}-1},\] (D.32)

we can see that we recover their result in these regimes. We can also map their \(\Delta_{0}\) to our \(\mu_{0}\). For \(T<M\), they let

\[\frac{\Delta_{0}}{1+\Delta_{0}} =\sum_{j=1}^{M}\frac{T\sigma_{j}^{2}}{(T\sigma_{j}+\Delta_{-1})^{ 2}}\] (D.33) \[=\frac{1}{T}\sum_{j=1}^{M}\frac{\sigma_{j}^{2}}{(\sigma_{j}+M/T \bar{\Delta}_{-1})^{2}},\] (D.34)

hence we can see that

\[\frac{\Delta_{0}}{1+\Delta_{0}}=1-\mu_{0}.\] (D.35)This mapping also enables our application of their interpolating approximate solutions for \(\Delta_{-1}\) and \(\Delta_{0}\) in the case of power law spectra. For a finite-size spectrum

\[\sigma_{j}=\frac{\sigma_{+}}{j^{1+\omega}}\qquad(j=1,\dots,M),\] (D.36)

with

\[\sigma_{+}=M^{1+\omega}\sigma_{-},\] (D.37)

where we denote the exponent by \(\omega\) rather than \(\alpha\) as Maloney et al. [19] do to avoid clashing with our notation elsewhere, they obtain the approximate solution

\[\frac{1}{M}\Delta_{-1}(N,M)=\begin{cases}\sigma_{-}\left\{k\left[\left(\frac{ M}{N}\right)^{\omega}-1\right]+\left[2+\omega(1-k)\right]\left(1-\frac{N}{M} \right)\right\},&N<M\\ 0&N>M\end{cases}\] (D.38)

for

\[k=\left[\frac{\frac{\pi}{1+\omega}}{\sin\left(\frac{\pi}{1+\omega}\right)} \right]^{1+\omega}=\left[\frac{1}{\operatorname{sinc}\left(\frac{\pi}{1+ \omega}\right)}\right]^{1+\omega},\] (D.39)

which leads to the expression

\[\chi(z)=\begin{cases}\sigma_{-}\left\{k(z^{\omega}-1)+\left[2+\omega(1-k) \right]\left(1-\frac{1}{z}\right)\right\},&z>1\\ 0&z<1.\end{cases}\] (D.40)

Moreover, for \(T<M\), they give the approximate solution

\[\Delta_{0}(T,M)=\omega+\frac{1}{M/T-1}.\] (D.41)

By applying these results, we obtain the result claimed in the main text, (31). We note that we fix \(\sigma_{-}\) to be constant rather than \(\sigma_{+}\) as Maloney et al. [19] do, which ensures normalizability of the limiting eigenvalue distribution at the expense of diverging moments.

### Deep linear models with unstructured weights and data

In [13], we studied deep Bayesian linear models with unstructured features and data.2 There, and in very recent work by Schroder et al. [20], a different parameterization for the thermodynamic limit

Figure D.1: Phase diagrams in different parameterizations of the thermodynamic limit. (a). Phase diagram in the \((\alpha_{0},\alpha_{1})\) plane. Region 1 (_orange_) is the overparameterized regime, Region 2 (_yellow_) is the bottlenecked regime, and Region 3 (_green_) is the overdetermined regime. (b). As in (a), but in the \((1/\alpha_{0},\alpha_{1}/\alpha_{0})\) plane, matching the parameterization used in our previous work [13]. Note that the plane is divided identically, but the locations of the phases are swapped.

was used:

\[p,n_{0},\dots,n_{L}\to\infty,\quad\text{with}\quad\frac{p}{n_{0}}\to \tilde{\alpha},\quad\frac{n_{\ell}}{n_{0}}\to\tilde{\gamma}_{\ell}\quad(\ell=1, \dots,L),\] (D.42)

where we decorate \(\tilde{\alpha}\) and \(\tilde{\gamma}_{\ell}\) with tildes to avoid confusion with parameters used elsewhere in the present work. The conversion to the parameterization used in the present work and in [30] is then given by

\[\tilde{\alpha} =\frac{1}{\alpha_{0}},\] (D.43) \[\tilde{\gamma}_{\ell} =\frac{\alpha_{\ell}}{\alpha_{0}},\qquad(\ell=1,\dots,L).\] (D.44)

Though these parameterizations are mathematically equivalent, it is important to distinguish between them as they give phase diagrams that divide the plane identically but swap the locations of the phases, as is shown in Figure D.1. Moreover, though the parameterization used here is more convenient for the replica computation [30], that given in (D.42) is conceptually useful, as it is closer to what one does in practical machine learning settings: the input dimension \(n_{0}\) is fixed by the task, and one can vary the dataset size \(p\) and the network widths \(n_{\ell}\). This is why we plot the phase diagrams in Figure 1 in the \((1/\alpha_{0},\alpha_{1}/\alpha_{0})\) plane.

## Appendix E Large-width expansions

In this appendix, we consider the limit of large width, i.e., the limit in which \(\alpha_{1},\dots,\alpha_{L}\to\infty\) for fixed \(\alpha_{0}\). Our first task is to determine how the quantities \(\kappa_{\ell}\) behave in this limit, as it is through these inverse generating functions that the hidden layer widths enter the generalization error.

Starting from the defining equation

\[\frac{1}{\alpha_{\ell}}=\mathbb{E}_{\tilde{\sigma}_{\ell}}\left[ \frac{\tilde{\sigma}_{\ell}}{\kappa_{\ell}+\tilde{\sigma}_{\ell}}\right]\] (E.1)

we can see that \(\kappa_{\ell}\) should tend to infinity linearly with \(\alpha_{\ell}\) as \(\alpha_{\ell}\to\infty\). In particular, we should have

\[\frac{\kappa_{\ell}}{\alpha_{\ell}}\to\mathbb{E}_{\tilde{\sigma}_{\ell}}[ \tilde{\sigma}_{\ell}]\] (E.2)

at large widths. Then, \(\mu_{\ell}\) has limiting behavior

\[\mu_{\ell} =1-\alpha_{\ell}\mathbb{E}_{\tilde{\sigma}_{\ell}}\left[\left( \frac{\tilde{\sigma}_{\ell}}{\kappa_{\ell}+\tilde{\sigma}_{\ell}}\right)^{2}\right]\] (E.3) \[\to 1\] (E.4)

From this, we can see that in the infinite-width limit the generalization error of the random feature model in Proposition 3.2 reduces to that of shallow ridgeless regression as in Corollary 3.1, as we would expect.

We now want to compute the leading correction to this result. In the unstructured case, this is easy, because we have \(\kappa_{\ell}=(\alpha_{\ell}-1)\tilde{\sigma}_{\ell}\), hence there is an \(\mathcal{O}(1)\) correction and nothing else. More generally, we assume Laurent series behavior of the form

\[\kappa_{\ell}=\alpha_{\ell}\kappa_{\ell}^{1}+\kappa_{\ell}^{0}+ \frac{1}{\alpha_{\ell}}\kappa_{\ell}^{-1}+\dots.\] (E.5)

Expanding, we have

\[\frac{\tilde{\sigma}_{\ell}}{\kappa_{\ell}+\tilde{\sigma}_{\ell}} =\frac{\tilde{\sigma}_{\ell}}{\alpha_{\ell}\kappa_{\ell}^{1}}- \frac{\tilde{\sigma}_{\ell}(\tilde{\sigma}_{\ell}+\kappa_{\ell}^{0})}{\alpha _{\ell}^{2}(\kappa_{\ell}^{1})^{2}}+\mathcal{O}(\alpha_{\ell}^{-3})\] (E.6)

hence, if we integrate term-by-term, we have

\[\frac{1}{\alpha_{\ell}}=\frac{\mathbb{E}_{\tilde{\sigma}_{\ell}}[ \tilde{\sigma}_{\ell}]}{\alpha_{\ell}\kappa_{\ell}^{1}}-\frac{\mathbb{E}_{ \tilde{\sigma}_{\ell}}[\tilde{\sigma}_{\ell}^{2}]+\mathbb{E}_{\tilde{\sigma}_ {\ell}}[\tilde{\sigma}_{\ell}]\kappa_{\ell}^{0}}{\alpha_{\ell}^{2}(\kappa_{ \ell}^{1})^{2}}+\mathcal{O}(\alpha_{\ell}^{-3}).\] (E.7)If we solve order-by-order, we again find that

\[\kappa^{1}_{\ell}=\mathbb{E}_{\tilde{\sigma}_{\ell}}[\tilde{\sigma}_{\ell}]\] (E.8)

while the coefficients of all higher-order terms in \(1/\alpha_{\ell}\) must vanish. In particular, this gives

\[\kappa^{0}_{\ell}=-\frac{\mathbb{E}_{\tilde{\sigma}_{\ell}}[\tilde{\sigma}_{ \ell}^{2}]}{\mathbb{E}_{\tilde{\sigma}_{\ell}}[\tilde{\sigma}_{\ell}]}.\] (E.9)

This computation assumes that the spectrum has finite moments, which is not the case for the heavy-tailed power law spectra considered in Corollary 5.1.

Then, we have

\[\mu_{\ell} =1-\alpha_{\ell}\mathbb{E}_{\tilde{\sigma}_{\ell}}\left[\left( \frac{\tilde{\sigma}_{\ell}}{\kappa_{\ell}+\tilde{\sigma}_{\ell}}\right)^{2}\right]\] (E.10) \[=1-\frac{\mathbb{E}_{\tilde{\sigma}_{\ell}}[\tilde{\sigma}_{\ell }^{2}]}{\alpha_{\ell}(\kappa^{1}_{\ell})^{2}}+2\frac{\mathbb{E}_{\tilde{ \sigma}_{\ell}}[\tilde{\sigma}_{\ell}^{3}]+\mathbb{E}_{\tilde{\sigma}_{\ell}} [\tilde{\sigma}_{\ell}^{2}]\kappa^{0}_{\ell}}{\alpha^{2}_{\ell}(\kappa^{1}_{ \ell})^{3}}+\mathcal{O}(\alpha^{-3}_{\ell})\] (E.11) \[=1-\frac{\mathbb{E}_{\tilde{\sigma}_{\ell}}[\tilde{\sigma}_{\ell }^{2}]}{\mathbb{E}_{\tilde{\sigma}_{\ell}}[\tilde{\sigma}_{\ell}]^{2}}\frac{1 }{\alpha_{\ell}}+\mathcal{O}(\alpha^{-2}_{\ell}).\] (E.12)

Collecting our results, we have

\[\kappa_{\ell}=\mathbb{E}_{\tilde{\sigma}_{\ell}}[\tilde{\sigma}_{\ell}]\alpha _{\ell}\left(1-\frac{\mathbb{E}_{\tilde{\sigma}_{\ell}}[\tilde{\sigma}_{\ell} ^{2}]}{\mathbb{E}_{\tilde{\sigma}_{\ell}}[\tilde{\sigma}_{\ell}]^{2}}\frac{1 }{\alpha_{\ell}}+\mathcal{O}(\alpha^{-2}_{\ell})\right)\] (E.13)

and

\[\mu_{\ell}=1-\frac{\mathbb{E}_{\tilde{\sigma}_{\ell}}[\tilde{\sigma}_{\ell}^ {2}]}{\mathbb{E}_{\tilde{\sigma}_{\ell}}[\tilde{\sigma}_{\ell}]^{2}}\frac{1 }{\alpha_{\ell}}+\mathcal{O}(\alpha^{-2}_{\ell}).\] (E.14)

Each term in these expansions has the expected behavior under rescaling: if we let \(\tilde{\bm{\Sigma}}^{\prime}_{\ell}=\tau_{\ell}\tilde{\bm{\Sigma}}_{\ell}\) for \(\tau_{\ell}>0\), we have \(\kappa^{\prime}_{\ell}=\tau_{\ell}\kappa_{\ell}\) and \(\mu^{\prime}_{\ell}=\mu_{\ell}\).

Then, substituting these expansions into (23), we find that the generalization error of an RFM in the ridgeless limit expands at large widths as

\[\epsilon =-\frac{\kappa^{2}_{0}}{\mu_{0}}\psi^{\prime}(\kappa_{0})+\frac{ 1-\mu_{0}}{\mu_{0}}\eta^{2}\] \[\quad+\left(\sum_{\ell=1}^{L}\frac{\mathbb{E}_{\tilde{\sigma}_{ \ell}}[\tilde{\sigma}_{\ell}^{2}]}{\mathbb{E}_{\tilde{\sigma}_{\ell}}[\tilde{ \sigma}_{\ell}]^{2}}\frac{1}{\alpha_{\ell}}\right)(\kappa_{0}\psi(\kappa_{0})+ \eta^{2})\] \[\quad+\mathcal{O}(\alpha^{-2}_{1},\dots,\alpha^{-2}_{L})\] (E.15)

in the regime \(\alpha_{0}>1\); if \(\alpha_{0}<1\) the generalization error does not depend on the hidden layer widths so long as they are greater than 1.

For an RFM trained using the Gibbs estimator, as considered in Proposition 6.1, we find that

\[\epsilon_{\mathrm{BRFM}} =-\frac{\kappa^{2}_{0}}{\mu_{0}}\psi^{\prime}(\kappa_{0})+\frac{ 1-\mu_{0}}{\mu_{0}}\eta^{2}+\frac{\kappa_{0}}{\alpha_{0}}\varsigma^{2}\] \[\quad+\left(\sum_{\ell=1}^{L}\frac{\mathbb{E}_{\tilde{\sigma}_{ \ell}}[\tilde{\sigma}_{\ell}^{2}]}{\mathbb{E}_{\tilde{\sigma}_{\ell}}[\tilde{ \sigma}_{\ell}]^{2}}\frac{1}{\alpha_{\ell}}\right)\left(\kappa_{0}\psi(\kappa_ {0})+\eta^{2}-\frac{\kappa_{0}}{\alpha_{0}}\varsigma^{2}\right)\] \[\quad+\mathcal{O}(w^{-2})\] (E.16)

where we have defined

\[\varsigma^{2}\equiv\prod_{\ell=1}^{L}\mathbb{E}_{\tilde{\sigma}_{\ell}}[ \tilde{\sigma}_{\ell}],\] (E.17)upon expanding the thermal variance term

\[\prod_{\ell=0}^{L}\frac{\kappa_{\ell}}{\alpha_{\ell}} =\frac{\kappa_{0}}{\alpha_{0}}\left[\prod_{\ell=1}^{L}\mathbb{E}_{ \tilde{\sigma}_{\ell}}[\tilde{\sigma}_{\ell}]\right]\] \[\quad-\frac{\kappa_{0}}{\alpha_{0}}\left[\prod_{\ell=1}^{L} \mathbb{E}_{\tilde{\sigma}_{\ell}}[\tilde{\sigma}_{\ell}]\right]\sum_{\ell=1}^ {L}\frac{\mathbb{E}_{\tilde{\sigma}_{\ell}}[\tilde{\sigma}_{\ell}^{2}]}{ \mathbb{E}_{\tilde{\sigma}_{\ell}}[\tilde{\sigma}_{\ell}]^{2}}\frac{1}{\alpha_ {\ell}}\] \[\quad+\mathcal{O}(w^{-2}).\] (E.18)

Here, we denote by \(\mathcal{O}(w^{-2})\) all terms of \(\mathcal{O}(\alpha_{\ell}^{-2})\) for a given layer \(\ell=1,\dots,L\) or terms of \(\mathcal{O}(\alpha_{\ell}^{-1}\alpha_{\ell^{\prime}}^{-1})\) for two different layers \(\ell,\ell^{\prime}\).

## Appendix F Numerical methods

In this appendix, we describe the numerical methods used to produce Figures 1, 2. All simulations were performed using Matlab 9.13 (R2022b; The MathWorks, Natick MA, USA; https://www.mathworks.com/products/matlab.html) on a desktop workstation (CPU: Intel Xeon W-2145, 64GB RAM). They were not computationally intensive, and required less than an hour of compute time in total. Code to reproduce the figures is archived as part of the online supplemental material. Numerical computation of the solution to the ridgeless regression problem--the minimum-norm interpolant--was performed using the lsqminnorm solver (https://www.mathworks.com/help/matlab/ref/lsqminnorm.html), which uses an algorithm based on the complete orthogonal decomposition of the design matrix.