# _When LLMs Meet Cuning Texts_:

A Fallacy Understanding Benchmark for

Large Language Models

Yinghui Li\({}^{1}\)1, Qingyu Zhou\({}^{2,}\)2,\({}^{*}\)3, Yuanzhen Luo\({}^{*}\), Shirong Ma\({}^{1}\),

**Yangning Li\({}^{1}\), Hai-Tao Zheng\({}^{1,}\)4, Xuming Hu\({}^{3,}\)5, Philip S. Yu\({}^{4}\)**

\({}^{1}\)Tsinghua University, \({}^{2}\) Bytedance Inc.

\({}^{3}\)The Hong Kong University of Science and Technology (Guangzhou)

\({}^{4}\)University of Illinois Chicago

liyinghu20@mails.tsinghua.edu.cn

indicates equal contribution.Corresponding authors

Footnote 1: footnotemark:

Footnote 2: footnotemark:

###### Abstract

Recently, Large Language Models (LLMs) make remarkable evolutions in language understanding and generation. Following this, various benchmarks for measuring all kinds of capabilities of LLMs have sprung up. In this paper, we challenge the reasoning and understanding abilities of LLMs by proposing a **FaL**lacy **U**nderstanding **B**enchmark (**FLUB**) containing cunning texts that are easy for humans to understand but difficult for models to grasp. Specifically, the cunning texts that **FLUB** focuses on mainly consist of the tricky, humorous, and misleading texts collected from the real internet environment. And we design three tasks with increasing difficulty in the **FLUB** benchmark to evaluate the fallacy understanding ability of LLMs. Based on **FLUB**, we investigate the performance of multiple representative and advanced LLMs, reflecting our **FLUB** is challenging and worthy of more future study. Interesting discoveries and valuable insights are achieved in our extensive experiments and detailed analyses. We hope that our benchmark can encourage the community to improve LLMs' ability to understand fallacies. Our data and codes are available at https://github.com/THUKElab/FLUB.

## 1 Introduction

Large Language Models (LLMs) have shown great abilities to understand human languages, including information extraction [1], text correction [2], humor understanding [3], etc. Researchers have constructed numerous benchmarks to evaluate LLMs in various aspects [4, 5, 6, 7, 8]. By using constructed benchmarks to interact with LLMs, researchers can analyze the behavior of LLMs to compare the performance of different LLMs and study how to further improve LLMs in a targeted manner.

Although many LLM benchmarks have sprung up, we believe that existing benchmarks are not challenging enough to truly measure the human-like intelligence of LLMs. In particular, we are still wondering whether LLMs can understand cunning texts that may contain misleading, wrong premise, intentional ambiguity, and so forth, considering that almost all LLMs are trained on "cleaned" and "correct" corpora. Therefore, we build a **FaL**lacy **U**nderstanding **B**enchmark (**FLUB**) to challenge LLMs for solving these problems.

Figure 0(a) shows the running examples from **FLUB**. From these cases, we directly feel the different behaviors of LLMs and humans when facing cunning texts. In the first example, LLMs ignore thecommon sense that the lotus root itself has many holes in its structure and fall into the trap of the running text, wrongly judging that the holes in the lotus root are caused by insect infestation. In the second example, LLMs fail to see the logic that depositing money into random ATMs does not create problems and therefore give an answer that seems reasonable but is absurdly laughable. In fact, these running texts for LLMs are very easy to handle for human intelligence. **Therefore, it is very urgent and meaningful to construct a benchmark composed of cunning texts to evaluate and thereby promote the improvement of LLMs' fallacy understanding capabilities.**

Inspired by the above motivation, we collect real cunning texts as our raw data from a famous Chinese online forum, the "Ruozhiba" (retard forum) 3. This forum is popular for its cunning and unreasonable posts, which are generally easy for humans to understand but challenging for LLMs. The characteristics of the posts contained in this forum are consistent with our research motivation, so choosing it as the data source well supports FLUB's evaluation of LLMs' fallacy understanding ability. After data cleaning and annotating of cunning types, FLUB has 8 fine-grained types of cunning texts and most of the texts in FLUB fall into two types of fallacy, namely, faulty reasoning and word game. Moreover, we also manually annotated one correct answer (i.e., the explanation of the cunning text) and three confusing wrong answers for each input text in FLUB, as shown in Figure 0(b).

Footnote 3: https://tieba.baidu.com/f?kw=%5XBCXB1XE6X99%BA&ie=utf-8

Based on our constructed FLUB and its annotation information, we design three tasks with increasing difficulty to test whether the LLMs can understand the fallacy and solve the "cunning" texts. Specifically, (1) **Answer Selection**: The model is asked to select the correct one from the four answers provided by FLUB for each input text. (2) **Cunning Type Classification**: Given a cunning text as input, the model is expected to directly identify its fallacy type defined in our scheme. (3) **Fallacy Explanation**: We hope the model sees a cunning text and intelligently generates a correct explanation for the fallacy contained in the text, just like humans, without falling into its trap.

In our experiments, we select representative and advanced LLMs to be evaluated on FLUB. Our empirical study reveals: (1) LLMs are very poor in their ability to perceive fallacy types in cunning texts. (2) For a specific task, LLMs with larger parameter sizes do not always perform better. (3) There is a close relationship between the Answer Selection task and the Fallacy Explanation task, and the interaction between them is critical to promoting the understanding of fallacies in LLMs. (4) On FLUB, the widely used Chain-of-Thought and In-context Learning techniques deserve further improvement and research. We believe that our proposed FLUB and all our findings are crucial for LLMs to comprehend the fallacy and handle cunning texts in the real world.

Figure 1: The running examples and annotation examples of FLUB.

## 2 The FLUB Benchmark

### Benchmark Construction

Data CollectionWe collect raw text data from "Ruozhiba" in Baidu Tiepa 4. "Ruozhiba" is one of the most famous online forums in the Chinese internet community, and people often post interesting or "silly" texts on it just for fun. In addition, the recent study [9] also shows that the Ruozhiba data is very useful for improving the ability of Chinese LLMs. We find that many of the posts on this forum are tricky texts or brain-teaser-like texts, which is exactly in line with our purpose of using cunning texts to challenge LLMs, so we utilize this forum as our data source. As a result of automatic crawling, we initially collect 9,927 candidate posts. Notably, according to the Baidu Bar agreement 5, the data on Baidu Tiepa can be used for academic research free of charge and without liability.

Footnote 4: https://tieba.baidu.com

Footnote 5: https://baike.baidu.com/item/XE8XB4XB4XE5X990XA7XE5X8DX8FXE8XAE8XAE8/8397765

Data CleaningWe employ annotators to manually filter out irrelevant posts that do not present cunning texts. Since the collected original posts contain irrelevant content such as links and images, we also require annotators to extract the fallacious and illogical contents from the raw post and rewrite them into a complete sentence. Besides, it is worth noting that we carefully ensure that the texts in FLUB are ethical texts. This process includes user information anonymization, sensitive information removal, and filtering of impolite posts. In total, we obtain 834 data samples to form FLUB.

Data AnnotationTo ensure the annotation quality, our criteria for selecting annotators is that the person must be a native Chinese speaker and have a bachelor's degree. In addition, because FLUB comes from the online forum, we also require annotators to have more than five years of experience as netizens. The detailed annotation workflows are as follows:

1. **Cunning Type Annotation**: We first define 8 cunning types within the collected texts along with their corresponding examples, as shown in Figure 2. Specifically, our core authors make a comprehensive summary based on careful observation of the 9,927 initial candidate posts, thus defining 8 types. Subsequently, each data sample is processed by three junior annotators, who are required to select an appropriate cunning type for the sample. We achieve the initial annotation results based on the voting results among three annotators. The initial annotation results become the final annotation information after being reviewed by the senior annotator (and modified if necessary). Particularly, there are still a small number

Figure 2: The definitions and examples of the cunning types in FLUB.

of samples that fall into multiple types. For these samples, senior annotators and our core authors will discuss carefully and select the main type (i.e., the most obvious type among multiple types) as the annotation result.
2. **Correct Explanation Annotation**: We assign two junior annotators to write the explanation or answer for each sample independently. We ask them to try to explain the given text in a detailed, objective, and unambiguous way. The senior annotator then selects (and modifies if necessary) the more suitable text written by the two junior annotators.
3. **Wrong Candidates Annotation**: This part annotation is to obtain the wrong candidate answer that may be likely to be answered incorrectly for each input text. We assign three junior annotators for each sample and require each of them to write three different incorrect answers based on their understanding of the text. Particularly, we emphasize to each junior annotator that the three different wrong answers they write should ensure diversity and resemble as much as possible the answers that LLMs can easily produce. For each sample's nine initial incorrect answers, the senior annotator selects the three most challenging sentences as the final wrong candidates.

Since the annotation difficulty of different information is different, the salary we pay to the annotators we employ is also different. Specifically, we pay each person who annotates the cunning type $0.5 per sample, each person who writes the correct explanation $1 per sample, and each person who writes the wrong candidates $2 per sample. In addition to the junior annotators providing the initial annotation results, we also set three senior annotators with a salary of $2 per sample, who are responsible for carefully checking the correctness of the annotation results provided by the junior annotators.

It is worth mentioning that we have prepared sufficient and representative samples for annotators to learn and pre-annotate to ensure that they fully understand the information we want to annotate before they officially start annotation. Specifically, we select senior annotators based on their performance in the pre-annotation process. If an annotator's success rate is above 95%, he or she will be appointed as a senior annotator. In addition, it is worth mentioning that, all of our formal annotators have a success rate of over 80% in the pre-annotation process. At the same time, to avoid bias caused by the subjectivity of annotators as much as possible, our core authors also carefully checked the final annotation results of each data sample. Our entire annotation process lasted 2 weeks.

### Dataset Analysis

Data SizeFLUB comprises 834 samples that span 8 cunning types. It is worth emphasizing that the data size is not directly related to the evaluation effectiveness of a LLM benchmark. For example, TruthfulQA [10] and FreshQA [11], these benchmarks that have been widely used and had deep impacts, only have 817 and 500 test samples respectively. The main reasons limiting the size of FLUB are that it is derived entirely from real-world online forum posts and our rigorous high-quality data cleaning process, which retained 834 final samples from 9,927 candidate posts.

Data DistributionAs for the cunning type distribution of FLUB, most data in FLUB belong to the types of reasoning errors (53.4%) and word games (28.7%). This is because these two types of posts appear widely in "Ruozhi Bar" forum whose purpose is to challenge human intelligence. A large number of cunning texts involving reasoning errors and word games ensure that FLUB is challenging enough. Besides, we observe that some types of texts are relatively rare, such as phonetic errors (0.6%). In fact, this is because our data come entirely from the real world and are all carefully constructed by netizens. Cases of cunning texts caused by phonetic errors are indeed rare in the real world. To eliminate the impact of type imbalance when FLUB evaluating LLMs, we choose the F-1 score as the evaluation metric which comprehensively considers the type coverage.

Annotation QualitySince cunning type annotation is essentially a classification process performed by multiple annotators, we analyze the annotation quality of this information. Specifically, we calculate Fleiss' Kappa [12] to reflect the three junior annotator's Inter-Annotator Agreement (IAA). Our final obtained Fleiss Kappa result is greater than 0.767, which shows that our annotation results have excellent consistency and quality [13]. On the other hand, we further ensure annotation quality by checking the annotation and modification results of the senior annotators. According to our statistics, senior annotators modified a total of 159 initial annotation results of data samples, that is, the modification rate of senior annotators was 19.06%. This reflects the excellent workload of our senior annotators and also reflects the high quality of our dataset. Moreover, after further checking of the modification results of the senior annotators by our core authors, we found that the main reason for the modifications was the disagreement between the senior annotators and the junior annotators on the cunning types (most of the cases were the ones we mentioned before that may fall into multiple types of samples). For these cases, our core authors made the most reasonable choices and personally modified the annotation results to maximize the quality of the annotation. After all, no one knows the full picture of our work better than our core authors.

### Benchmark Task Setups

To evaluate the fallacy understanding ability of LLMs, we design three benchmark tasks on FLUB: Answer Selection, Cunning Type Classification, and Fallacy Explanation. For each task, we design prompts to guide LLMs on the expected output. We also explore the prompting strategies of Chain-of-Thought and In-context Learning to conduct in-depth exploration on FLUB. The details of our designed prompts are shown in Appendix A. Below we introduce the details of our three tasks:

Task 1: Answer SelectionIn Task 1, LLMs are required to select the correct answer from four given candidate explanations for each input text. The annotation of candidate explanations is illustrated in Figure 0(b). In general, each sample in this task is a tuple \(\{p,q,O_{A},O_{B},O_{C},O_{D},l\}\), where \(p\) is our given prompt as shown in Appendix A, \(q\) is the input text, \(O_{A}\), \(O_{B}\), \(O_{C}\), and \(O_{D}\) are four candidate explanations, and \(l\in\{A,B,C,D\}\) is the golden label indicating \(O_{l}\) is the correct explanation. The design motivation of this task is to test whether LLMs can distinguish right from wrong when seeing the correct and wrong answers in the context of a given cunning text.

Task 2: Cunning Type ClassificationIf LLMs are directly tasked with determining the corresponding cunning type, it will help us in conducting an initial automated assessment of the LLM's understanding ability. The cunning type classification task is specifically designed to evaluate whether LLMs can classify the cunning text into categories aligned with human intuition based on the hidden irrational aspects within the current text. The annotated problem types are shown in Figure 2. During task evaluation, all the problem types will be combined with the prompt to allow LLMs to directly pick the correct type of cunning text.

Task 3: Fallacy ExplanationTo further test whether LLMs truly understand the given cunning text, we design the explanation task. In this task, the designed prompt and input texts are directly input into LLMs, enabling them to "read" input texts and generate corresponding explanations. Note that since some texts are not expressed in the form of inquiries, we also set a prompt to guide LLMs in identifying the question (See Appendix A). The generated explanations will be compared with the correct explanation for evaluation. If LLMs can generate reasonable explanations, we believe that they have at least developed the ability to identify and avoid the traps of cunning texts.

Automatic Evaluation MetricsFor Task 1, we calculate **Accuracy** directly based on the LLMs' selection results. For Task 2, considering that there are a few cunning types in FLUB with small sample size, we choose the **F-1 Score** to measure the performance of LLMs because it focuses on both the accuracy of model prediction and the coverage for positive class samples, thereby effectively avoiding bias caused by type imbalance and ensuring the rationality and reliability of evaluation. To evaluate the quality of LLMs' generated explanations in Task 3, inspired by MT-Bench [14], we construct prompts that incorporate the task instruction, input texts, LLM's explanations, and reference answers. These prompts are fed into GPT-4, which is tasked with assigning a **GPT-4 Score** ranging from 1 to 10. The prompt for the automated evaluation is illustrated in Appendix B.

Human Evaluation SettingsFor Task 1 and Task 2, **we conduct human evaluations to explore how well human-level intelligence could perform these two tasks.** To ensure the fairness of the comparison between humans and LLMs, we hire 3 new persons who do not participate in the construction process of FLUB. After briefly introducing them to the objectives of Task 1 and Task 2 (without introducing additional knowledge and information), let them directly carry out selection and classification. For the human evaluation of Task 3, **we mainly want to verify the effectiveness of the automatic GPT-4 score we use,** therefore, we hire 3 evaluation annotators to rate LLMs' explanations, with scores ranging from \(\{1,2,3,4,5\}\). To ensure an accurate evaluation of the explanations of LLMs, we developed a set of scoring guidelines for annotators, including the definitions and relevant examples for each score. The scoring guidelines of human evaluation are presented in Appendix C.

When designing the GPT-4 scoring range and the human scoring range, we have different motivations. We hope that GPT-4's scoring range can be as unbiased and detailed as possible, so we set its scoring range to 1-10. But this scoring range is too fine-grained and difficult for humans, so we set the human scoring range to 1-5. Therefore, for comparability of GPT-4 scores and human scores in Table 2, we multiply human scores by 2 to match the range of GPT-4 scores.

## 3 Experiments

### Experimental Settings

To better reflect the evaluation of FLUB's fallacy understanding ability of LLMs, we select some advanced LLMs that are widely used in the Chinese community: (1) **ERNIE-Bot**[15] is a series of closed-sourced commercial LLMs released by Baidu. We evaluate the three latest chat models, including ERNIE-Bot-3.5, ERNIE-Bot-3.5-Turbo, and ERNIE-Bot-4.0. (2) **ChatGPT**[16] ChatGPT is undoubtedly the hottest model developed by OpenAI. We evaluate GPT-3.5-Turbo and GPT-4-Turbo. (3) **ChatGLM3**[17] is the latest open-sourced model of the ChatGLM which is a series of bilingual LLMs. We evaluate the only open-sourced parameter size of ChatGLM3-6B. (4) **Qwen**[18] is the open-sourced LLMs developed by the Alibaba Group. We select three chat Qwen models, including Owen-7B-Chat, Qwen-14B-Chat, and Owen-72B-Chat. (5) **Yi**[19] series models are open-sourced LLMs trained from scratch by 01-AI. In our experiments, we select Yi-6B-Chat and Yi-34B-Chat to be evaluated on FLUB. (6) **Baichuan2**[20] has achieved the competitive performance of its size on many Chinese benchmarks. We select Baichuan2-7B-Chat and Baichuan2-13B-Chat.

When running LLMs inference, for closed-sourced LLMs, we access corresponding models via the official APIs. Meanwhile, open-sourced models are deployed on 1 to 4 NVIDIA A100 GPUs depending on their parameter size.

### Automatic Evaluation Results

The main results are presented in Table 1 and we have the following insights:

1. **For the difficulty of different tasks**, the Answer Selection task is the simplest, which shows that LLMs should have a certain ability to distinguish right from wrong when seeing correct and wrong answers. However, we also see that the performance of all models on

\begin{table}
\begin{tabular}{l c c c c c c c c} \hline \hline \multirow{2}{*}{**Models**} & \multirow{2}{*}{**Open**} & \multicolumn{3}{c}{**Selection**} & \multicolumn{2}{c}{**Classification**} & \multicolumn{2}{c}{**Explanation**} & \multicolumn{2}{c}{**Overall**} \\  & & \multicolumn{2}{c}{**Accuracy**} & \multicolumn{2}{c}{**F-1 Score**} & \multicolumn{2}{c}{**GPT-4 Score**} & \multicolumn{2}{c}{**Performance**} \\  & **Source** & **w/o CoT** & **CoT** & **w/o CoT** & **CoT** & **w/o CoT** & **CoT** & **w/o CoT** & **CoT** \\ \hline ERNIE-Bot-3.5-Turbo [15] & ✗ & 32.97 & 34.65\({}^{\dagger}\) & 1.99 & 6.09\({}^{\dagger}\) & 5.78 & 5.83\({}^{\dagger}\) & 7.24 & 10.72\({}^{\dagger}\) \\ ERNIE-Bot-3.5 [15] & ✗ & 52.76 & 38.37\({}^{\ddagger}\) & 10.33 & 11.15\({}^{\dagger}\) & 6.35 & 6.22\({}^{\ddagger}\) & 15.13 & 13.86\({}^{\ddagger}\) \\ ERNIE-Bot-4.0 [15] & ✗ & 75.66 & 71.34\({}^{\ddagger}\) & 11.84 & **14.42\({}^{\dagger}\)** & 7.38 & 8.11\({}^{\dagger}\) & 19.06 & 20.28\({}^{\ddagger}\) \\ GPT-3.5-Turbo [16] & ✗ & 50.48 & 48.08\({}^{\ddagger}\) & 3.09 & 6.15\({}^{\ddagger}\) & 6.23 & 7.00\({}^{\dagger}\) & 9.91 & 12.74\({}^{\ddagger}\) \\ GPT-4-Turbo [16] & ✗ & **79.38** & **82.73\({}^{\ddagger}\)** & **12.31** & 13.97\({}^{\dagger}\) & **8.95** & **9.21\({}^{\dagger}\)** & **20.60** & **22.00\({}^{\ddagger}\)** \\ \hline ChatGLM3-6B [17] & ✓ & 35.85 & 35.01\({}^{\ddagger}\) & 7.48 & 9.34\({}^{\dagger}\) & 4.98 & 4.82\({}^{\ddagger}\) & 11.01 & 11.64\({}^{\dagger}\) \\ Qwen-7B-Chat [18] & ✓ & 38.49 & 33.69\({}^{\ddagger}\) & 8.00 & 10.97\({}^{\dagger}\) & 5.39 & 5.65\({}^{\dagger}\) & 11.84 & 11.98\({}^{\dagger}\) \\ Qwen-14B-Chat [18] & ✓ & 42.57 & 43.05\({}^{\dagger}\) & **10.34** & 10.44\({}^{\ddagger}\) & 5.24 & 6.24 & 13.21 & 14.10\({}^{\ddagger}\) \\ Qwen-72B-Chat [18] & ✓ & **58.63** & **61.51\({}^{\dagger}\)** & 9.32 & **12.26\({}^{\ddagger}\)** & **7.34** & **7.90\({}^{\dagger}\)** & **15.89** & **18.13\({}^{\ddagger}\)** \\ Yi-6B-Chat [19] & ✓ & 32.37 & 29.26\({}^{\ddagger}\) & 8.87 & 9.84\({}^{\dagger}\) & 5.73 & 5.39\({}^{\ddagger}\) & 18.11 & 15.8\({}^{\ddagger}\) \\ Yi-34B-Chat [19] & ✓ & 47.96 & 48.80\({}^{\dagger}\) & 4.74 & 11.70\({}^{\dagger}\) & 6.97 & 7.52\({}^{\dagger}\) & 11.66 & 16.17\({}^{\dagger}\) \\ Baichuan2-7B-Chat [20] & ✓ & 43.17 & 37.17\({}^{\ddagger}\) & 1.02 & 4.45\({}^{\dagger}\) & 5.48 & 4.85\({}^{\ddagger}\) & 6.23 & 9.29\({}^{\dagger}\) \\ Baichuan2-13B-Chat [20] & ✓ & 37.05 & 38.01\({}^{\dagger}\) & 3.52 & 4.58\({}^{\ddagger}\) & 5.79 & 5.84\({}^{\ddagger}\) & 9.11 & 10.06\({}^{\ddagger}\) \\ \hline Random & - & 25.00 & & 7.90 & & - & - \\ \hline Human & - & 93.35 & & 63.69 & & - & - \\ \hline \hline \end{tabular}
\end{table}
Table 1: We **bold** the optimal and underline the suboptimal of closed/open-source models. We report the overall performance by calculating the **geometric mean** of the three tasks. We color the result that Chain-of-Thought (CoT) brings positive / negative gain as green\({}^{\dagger}\)/ red\({}^{\ddagger}\).

the Cunningham Type Classification task is unsatisfactory, with F-1 scores below 15.0, and some models even perform below random performance. This deficiency may stem from the models' limited capability to comprehend the semantics of various running types.
2. **For the connection between different tasks**, the comparative outcomes among different models across the three tasks are not consistent. Nevertheless, models that exhibit superior performance in the Answer Selection task tend to generate more plausible explanations. This phenomenon reminds us that there is a close relationship between the Answer Selection task and the Fallacy Explanation task. The interaction between these two tasks is very critical for improving the fallacy understanding ability of LLMs.
3. **For the model performance of different scale parameters**, overall, models of larger scale are better equipped to understand cunning texts, which aligns with intuitive expectations. Of course, there are exceptions. We find that for the Qwen and Yi models, as the parameter size increases, the performance of the Cunning Type Classification task decreases. This is because this task requires a deep understanding of the Chinese language, especially the popular Internet language, and we observe that as the Qwen and Yi models become larger, their ability to understand special Internet language becomes poorer. Besides, the another reason for the poor cunning type classification performance of the models is that they cannot accurately understand the defined types. Therefore, how to improve the perception ability of LLMs for the cunning types will be the key challenge to improving the performance of LLMs on the cunning classification task.
4. **For the impact of Chain-of-Thought**, to our surprise, Chain-of-Thought (CoT) does not bring stable improvements to LLMs' fallacy understanding ability. Especially for the Answer Selection and Fallacy Explanation tasks, CoT even has negative impacts on some models. We think there are two main reasons for this phenomenon: (1) We notice that when the model size exceeds 10B, CoT still has positive effects on these two tasks. This reflects the challenge of our tasks, which makes CoT unable to stimulate the small models to have sufficient capabilities to cope with them. (2) For traditional QA tasks (such as commonsense reasoning, mathematical reasoning, etc.), CoT can improve performance because these tasks themselves are relatively logical, and the process of solving their questions can be modeled as the logical reasoning process. Unlike these tasks, our tasks are not very logical problems but require more intuition about the language. Hence, adding intermediate steps by the CoT has no significant effect on our tasks. In summary, our proposed tasks deserve further research to improve the fallacy understanding ability of LLMs.
5. **For the overall performance**, considering that the performance values of the three sub-tasks are very different, we use the geometric mean to balance the impact of each sub-task and avoid the excessive impact of a single extreme value on the overall performance. We see that the overall performance of each model is basically consistent with common sense, that is, the larger the model, the better the performance, and CoT also brings positive effects. This shows that \(\mathsf{FLUB}\) is of high quality and suitable to measure the fallacy understanding ability of LLMs from an overall perspective.
6. **For the human performance**, we see that humans perform well on the Answer Selection and Cunning Type Classification tasks, which reflects the considerable gap in fallacy understanding between human intelligence and LLMs. It also shows that our proposed new benchmark and tasks are conducive to further promoting the progress of LLMs. Note that the reason why the Fallacy Explanation task is not suitable for evaluating human performance is that its automatic evaluation indicator is the GPT-4 Score. We think that using GPT-4 to evaluate explanations written by humans is unreasonable and unnecessary.

### The Impact of In-context Learning

We select 5 high-performing LLMs to study the impact of in-context learning on LLMs' fallacy understanding ability. Demonstrations used for in-context learning are randomly selected. As shown in Figure 3, unlike Chain-of-Thought which has no stable positive effect, the LLMs' performance with in-context learning is basically on the rise as demonstrations increase. This indicates that letting LLMs see more examples can improve their fallacy understanding ability, but the number of examples must be large enough because we have also seen that when only one shot example is added, the performance of LLMs sometimes declines compared to the zero-shot cases.

### Human Evaluation of Explanation

To verify the effectiveness of our designed automatic GPT-4 score for Task 3, we randomly select 50 data samples from FLUB, along with outputs from 5 high-performing LLMs for human evaluation by our contracted annotators. From the human evaluation results in Table 2, we observe that:

1. The overall correlation coefficient between the automatic and human evaluation is 0.69, indicating a high consistency between GPT-4 scores and human preferences. Besides, the correlation results also verify the effectiveness of our designed GPT-4 score for Task 3.
2. Both automatic and human evaluations exhibit a broadly consistent ranking across the selected five models. The GPT-4-Turbo achieves superior performance over all other models. In contrast, human annotators perceive marginal performance disparities among ERNIE-Bot-4.0, Qwen-72B-Chat, and Yi-34B-Chat models.
3. From the human evaluation results, except for GPT-4-Turbo, which can exceed the passing score of 6, the performance of other LLMs is still not ideal, which shows that the community still needs to further study how to improve the fallacy understanding ability of LLMs.

### Case Study

To analyze FLUB's challenges, we conduct case studies on the two advanced models with better performance in the fallacy explanation task in Table 3. From the first case, we see that GPT-4-Turbo gives a relatively perfect explanation, while ERNIE-Bot-4.0's answer does not explain the causal relationship clearly although its final conclusion is correct. According to ERNIE-Bot-4.0's explanation, if the egg is added with water, it can be restored. This is obviously wrong. In the second case which is more difficult, both ERNIE-Bot-4.0 and GPT-4-Turbo easily fail when facing these cunning texts. Specifically, ERNIE-Bot-4.0 follows the trap of the input text, not clearly stating that "putting heads on the shore" is an impossible operation, but giving a dumbfounding explanation. In comparison, GPT-4-Turbo's performance is slightly better, but it does not perceive the trap in the input text at all, resulting in an answer that is not what is questioned. It can be seen from these two cases that LLMs' ability to handle cunning texts is still insufficient.

\begin{table}
\begin{tabular}{l c c c} \hline \hline
**Models** & **Human** & **GPT-4** & **Correlation** \\ \hline GPT-4-Turbo & 7.12 & 8.60 & 0.57 \\ ERNIE-Bot-4.0 & 5.82 & 7.20 & 0.71 \\ Qwen-72B-Chat & 5.74 & 7.82 & 0.42 \\ Yi-34B-Chat & 5.42 & 6.44 & 0.74 \\ Baichuan2-13B-Chat & 4.42 & 5.84 & 0.63 \\ Overall & - & - & 0.69 \\ \hline \hline \end{tabular}
\end{table}
Table 2: Human evaluation on the explanation task. Note that we multiply the human results by 2 to normalize their range to be the same as the GPT-4 results’ range. The reported correlations are Spearman’s rank correlation coefficients. All correlations are extremely significant with \(p<0.01\).

Figure 3: The results of in-context learning with 0/1/2/5-shots demonstrations.

[MISSING_PAGE_FAIL:9]

Ethics Statement

In this paper, we present a new benchmark, FLUB. We have described the details of the collection, preprocessing, and annotation of FLUB. And we ensure that no infringement or unethical behavior occurred during the dataset construction. In terms of the data itself, to ensure that the dataset we need to release in the future meets ethical requirements, we spend lots of energy on data anonymization, data desensitization, improper data cleaning, etc. Besides, the cunning texts we are concerned about come from daily life and are very common. Therefore, the new research direction and tasks we propose will not cause harm to human society.

## 7 Conclusion

In this work, we construct FLUB, a high-quality benchmark consisting of cunning texts designed to evaluate the fallacy understanding ability of LLMs. Furthermore, we evaluate advanced LLMs on FLUB. Detailed analyses indicate FLUB is very challenging and of great research value. To date, most existing LLMs still can not understand the fallacy well, which results in them being far from dealing with complex problems in the real world as easily as humans. We believe that the benchmark and the research direction we provide are valuable for the LLMs community.

## Acknowledgments and Disclosure of Funding

This research is supported by the National Natural Science Foundation of China (Grant No. 62276154), the Natural Science Foundation of Guangdong Province (Grant No. 2023A1515012914 and 440300241033100801770), Shenzhen Science and Technology Program (Grant No. WDZC20231128091437002), Basic Research Fund of Shenzhen City (Grant No. JCYJ20210324120012033 and GJHZ20240218300101), the Major Key Project of PCL for Experiments and Applications (PCL2021A06), the Guangdong Provincial Department of Education Project (Grant No.2024KQNCX028); Scientific Research Projects for the Higher-educational Institutions (Grant No.2024312096), Education Bureau of Guangzhou Municipality; Guangzhou-HKUST(GZ) Joint Funding Program (Grant No.SL2024A03J01201), Education Bureau of Guangzhou Municipality; China Association for Science and Technology (Grant No.XMSB20240711064). This work is also supported in part by NSF under grants III-2106758, and POSE-2346158.

## References

* [1] T. Yu, C. Jiang, C. Lou, S. Huang, X. Wang, W. Liu, J. Cai, Y. Li, Y. Li, K. Tu, H. Zheng, N. Zhang, P. Xie, F. Huang, and Y. Jiang, "Seqgpt: An out-of-the-box large language model for open domain sequence understanding," _CoRR_, vol. abs/2308.10529, 2023.
* [2] Y. Li, H. Huang, S. Ma, Y. Jiang, Y. Li, F. Zhou, H. Zheng, and Q. Zhou, "On the (in)effectiveness of large language models for chinese text correction," _CoRR_, vol. abs/2307.09007, 2023.
* [3] J. Hessel, A. Marasovic, J. D. Hwang, L. Lee, J. Da, R. Zellers, R. Mankoff, and Y. Choi, "Do androids laugh at electric sheep? humor "understanding" benchmarks from the new yorker caption contest," in _Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_ (A. Rogers, J. Boyd-Graber, and N. Okazaki, eds.), (Toronto, Canada), pp. 688-714, Association for Computational Linguistics, July 2023.
* [4] S. Ma, Y. Li, R. Sun, Q. Zhou, S. Huang, D. Zhang, Y. Li, R. Liu, Z. Li, Y. Cao, H. Zheng, and Y. Shen, "Linguistic rules-based corpus generation for native chinese grammatical error correction," in _Findings of the Association for Computational Linguistics: EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022_ (Y. Goldberg, Z. Kozareva, and Y. Zhang, eds.), pp. 576-589, Association for Computational Linguistics, 2022.
* [5] S. Huang, S. Ma, Y. Li, L. Yangning, S. Lin, H. Zheng, and Y. Shen, "Towards attribute-entangled controllable text generation: A pilot study of blessing generation," in _Proceedings of the 2nd Workshop on Natural Language Generation, Evaluation, and Metrics (GEM)_ (A. Bosselut, K. Chandu, K. Dhole, V. Gangal, S. Gehrmann, Y. Jernite, J. Novikova, and L. Perez-Beltrachini, eds.), (Abu Dhabi, United Arab Emirates (Hybrid)), pp. 235-247, Association for Computational Linguistics, Dec. 2022.
* [6] S. Huang, S. Ma, Y. Li, M. Huang, W. Zou, W. Zhang, and H. Zheng, "Lateval: An interactive llms evaluation benchmark with incomplete information from lateral thinking puzzles," _CoRR_, vol. abs/2308.10855, 2023.
* [7] Y. Li, Z. Xu, S. Chen, H. Huang, Y. Li, Y. Jiang, Z. Li, Q. Zhou, H. Zheng, and Y. Shen, "Towards real-world writing assistance: A chinese character checking benchmark with faked and misspelled characters," _CoRR_, vol. abs/2311.11268, 2023.
* [8] Y. Li, T. Lu, Y. Li, T. Yu, S. Huang, H. Zheng, R. Zhang, and J. Yuan, "MESED: A multi-modal entity set expansion dataset with fine-grained semantic classes and hard negative entities," _CoRR_, vol. abs/2307.14878, 2023.
* [9] Y. Bai, X. Du, Y. Liang, Y. Jin, Z. Liu, J. Zhou, T. Zheng, X. Zhang, N. Ma, Z. Wang, _et al._, "Coig-cqa: Quality is all you need for chinese instruction fine-tuning," _arXiv preprint arXiv:2403.18058_, 2024.
* [10] S. Lin, J. Hilton, and O. Evans, "TruthfulQA: Measuring how models mimic human falsehoods," in _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_ (S. Muresan, P. Nakov, and A. Villavicencio, eds.), (Dublin, Ireland), pp. 3214-3252, Association for Computational Linguistics, May 2022.
* [11] T. Vu, M. Iyyer, X. Wang, N. Constant, J. Wei, J. Wei, C. Tar, Y.-H. Sung, D. Zhou, Q. Le, _et al._, "Freshllms: Refreshing large language models with search engine augmentation," _arXiv preprint arXiv:2310.03214_, 2023.
* [12] R. Falotico and P. Quatto, "Fleiss' kappa statistic without paradoxes," _Quality & Quantity_, vol. 49, pp. 463-470, 2015.
* [13] J. R. Landis and G. G. Koch, "The measurement of observer agreement for categorical data," _biometrics_, pp. 159-174, 1977.
* [14] L. Zheng, W.-L. Chiang, Y. Sheng, S. Zhuang, Z. Wu, Y. Zhuang, Z. Lin, Z. Li, D. Li, E. Xing, _et al._, "Judging llm-as-a-judge with mt-bench and chatbot arena," _arXiv preprint arXiv:2306.05685_, 2023.
* [15] Baidu, "Ernie-bot, https://cloud.baidu.com/product/wenxinworkshop," 2023.
* [16] OpenAI, "GPT-4 technical report," _CoRR_, vol. abs/2303.08774, 2023.
* [17] Z. Du, Y. Qian, X. Liu, M. Ding, J. Qiu, Z. Yang, and J. Tang, "GLM: General language model pretraining with autoregressive blank infilling," in _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_ (S. Muresan, P. Nakov, and A. Villavicencio, eds.), (Dublin, Ireland), pp. 320-335, Association for Computational Linguistics, May 2022.
* [18] J. Bai, S. Bai, Y. Chu, Z. Cui, K. Dang, X. Deng, Y. Fan, W. Ge, Y. Han, F. Huang, _et al._, "Qwen technical report," _arXiv preprint arXiv:2309.16609_, 2023.
* [19] 01-AI, "Yi, https://github.com/01-ai/Yi," 2023.
* [20] A. Yang, B. Xiao, B. Wang, B. Zhang, C. Bian, C. Yin, C. Lv, D. Pan, D. Wang, D. Yan, _et al._, "Baichuan 2: Open large-scale language models," _arXiv preprint arXiv:2309.10305_, 2023.
* [21] C. Dong, Y. Li, H. Gong, M. Chen, J. Li, Y. Shen, and M. Yang, "A survey of natural language generation," _ACM Comput. Surv._, vol. 55, no. 8, pp. 173:1-173:38, 2023.
* [22] Y. Chang, X. Wang, J. Wang, Y. Wu, K. Zhu, H. Chen, L. Yang, X. Yi, C. Wang, Y. Wang, _et al._, "A survey on evaluation of large language models," _arXiv preprint arXiv:2307.03109_, 2023.

* [23] Z. Guo, R. Jin, C. Liu, Y. Huang, D. Shi, L. Yu, Y. Liu, J. Li, B. Xiong, D. Xiong, _et al._, "Evaluating large language models: A comprehensive survey," _arXiv preprint arXiv:2310.19736_, 2023.
* [24] A. Talmor, J. Herzig, N. Lourie, and J. Berant, "Commonsenseqa: A question answering challenge targeting commonsense knowledge," in _Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers)_ (J. Burstein, C. Doran, and T. Solorio, eds.), pp. 4149-4158, Association for Computational Linguistics, 2019.
* [25] Y. Bisk, R. Zellers, R. L. Bras, J. Gao, and Y. Choi, "PIQA: reasoning about physical commonsense in natural language," in _The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020_, pp. 7432-7439, AAAI Press, 2020.
* [26] M. Sap, H. Rashkin, D. Chen, R. L. Bras, and Y. Choi, "Social iqa: Commonsense reasoning about social interactions," in _Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019_ (K. Inui, J. Jiang, V. Ng, and X. Wan, eds.), pp. 4462-4472, Association for Computational Linguistics, 2019.
* [27] R. Zellers, A. Holtzman, Y. Bisk, A. Farhadi, and Y. Choi, "Hellaswag: Can a machine really finish your sentence?," in _Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers_ (A. Korhonen, D. R. Traum, and L. Marquez, eds.), pp. 4791-4800, Association for Computational Linguistics, 2019.
* [28] B. Zhou, D. Khashabi, Q. Ning, and D. Roth, ""going on a vacation" takes longer than "going for a walk": A study of temporal commonsense understanding," in _Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019_ (K. Inui, J. Jiang, V. Ng, and X. Wan, eds.), pp. 3361-3367, Association for Computational Linguistics, 2019.
* [29] Y. Bang, S. Cahyawijaya, N. Lee, W. Dai, D. Su, B. Wilie, H. Lovenia, Z. Ji, T. Yu, W. Chung, Q. V. Do, Y. Xu, and P. Fung, "A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity," _CoRR_, vol. abs/2302.04023, 2023.
* [30] N. Bian, X. Han, L. Sun, H. Lin, Y. Lu, and B. He, "Chatgpt is a knowledgeable but inexperienced solver: An investigation of commonsense problem in large language models," _CoRR_, vol. abs/2303.16421, 2023.
* [31] S. Saha, Y. Nie, and M. Bansal, "Conjnli: Natural language inference over conjunctive sentences," in _Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020_ (B. Webber, T. Cohn, Y. He, and Y. Liu, eds.), pp. 8240-8252, Association for Computational Linguistics, 2020.
* [32] J. Tian, Y. Li, W. Chen, L. Xiao, H. He, and Y. Jin, "Diagnosing the first-order logical reasoning ability through logicnli," in _Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021_ (M. Moens, X. Huang, L. Specia, and S. W. Yih, eds.), pp. 3738-3747, Association for Computational Linguistics, 2021.
* investigating contextual reasoning over long texts," in _Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021, Virtual Event, February 2-9, 2021_, pp. 13388-13396, AAAI Press, 2021.
* [34] J. Liu, L. Cui, H. Liu, D. Huang, Y. Wang, and Y. Zhang, "Logiqa: A challenge dataset for machine reading comprehension with logical reasoning," in _Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI 2020_ (C. Bessiere, ed.), pp. 3622-3628, ijcai.org, 2020.
* [35] R. Liu, Y. Li, L. Tao, D. Liang, and H. Zheng, "Are we ready for a new paradigm shift? A survey on visual deep MLP," _Patterns_, vol. 3, no. 7, p. 100520, 2022.
* [36] S. Wang, Z. Liu, W. Zhong, M. Zhou, Z. Wei, Z. Chen, and N. Duan, "From LSAT: the progress and challenges of complex reasoning," _IEEE ACM Trans. Audio Speech Lang. Process._, vol. 30, pp. 2201-2216, 2022.
* an improved dataset for logical reasoning in natural language understanding," _IEEE ACM Trans. Audio Speech Lang. Process._, vol. 31, pp. 2947-2962, 2023.
* [38] H. Liu, R. Ning, Z. Teng, J. Liu, Q. Zhou, and Y. Zhang, "Evaluating the logical reasoning ability of chatgpt and GPT-4," _CoRR_, vol. abs/2304.03439, 2023.
* [39] A. Anjum and N. Lieberum, "Exploring humor in natural language processing: A comprehensive review of JOKER tasks at CLEF symposium 2023," in _Working Notes of the Conference and Labs of the Evaluation Forum (CLEF 2023), Thessaloniki, Greece, September 18th to 21st, 2023_ (M. Aliannejadi, G. Faggioli, N. Ferro, and M. Vlachos, eds.), vol. 3497 of _CEUR Workshop Proceedings_, pp. 1828-1837, CEUR-WS.org, 2023.
* [40] C. F. Hempelmann, "Computational humor: Beyond the pun?," _The Primer of Humor Research. Humor Research_, vol. 8, pp. 333-360, 2008.
* [41] Y. Li, Q. Zhou, Y. Li, Z. Li, R. Liu, R. Sun, Z. Wang, C. Li, Y. Cao, and H. Zheng, "The past mistake is the future wisdom: Error-driven contrastive probability optimization for chinese spell checking," in _Findings of the Association for Computational Linguistics: ACL 2022, Dublin, Ireland, May 22-27, 2022_ (S. Muresan, P. Nakov, and A. Villavicencio, eds.), pp. 3202-3213, Association for Computational Linguistics, 2022.
* [42] P. Chen and V. Soo, "Humor recognition using deep learning," in _Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT, New Orleans, Louisiana, USA, June 1-6, 2018, Volume 2 (Short Papers)_ (M. A. Walker, H. Ji, and A. Stent, eds.), pp. 113-117, Association for Computational Linguistics, 2018.
* [43] O. Popova and P. Dadic, "Does ai have a sense of humor? clef 2023 joker tasks 1, 2 and 3: using bloom, gpt, simplet5, and more for pun detection, location, interpretation and translation," _Proceedings of the Working Notes of CLEF_, vol. 3, 2023.

## Checklist

The checklist follows the references. Please read the checklist guidelines carefully for information on how to answer these questions. For each question, change the default [**TODO**] to [Yes],[No], or [N/A]. You are strongly encouraged to include a **justification to your answer**, either by referencing the appropriate section of your paper or providing a brief inline description. For example:

* Did you include the license to the code and datasets? [Yes] See Section **??**.
* Did you include the license to the code and datasets? [No] The code and the data are proprietary.
* Did you include the license to the code and datasets? [N/A]

Please do not modify the questions and only use the provided macros for your answers. Note that the Checklist section does not count towards the page limit. In your paper, please delete this instructions block and only keep the Checklist section heading above along with the questions/answers below.

1. For all authors...

1. Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? [Yes] See Abstract and Section 1. 2. Did you describe the limitations of your work? See Section 5. 3. Did you discuss any potential negative societal impacts of your work? See Section 6. 4. Have you read the ethics review guidelines and ensured that your paper conforms to them? See Section 6.
2. If you are including theoretical results... 1. Did you state the full set of assumptions of all theoretical results? [N/A] 2. Did you include complete proofs of all theoretical results? [N/A]
3. If you ran experiments (e.g. for benchmarks)... 1. Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? See Abstract, Section 3, Appendix A, Appendix B. 2. Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [N/A] 3. Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? See Section 2.3 and Section 3.3. 4. Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? See Section 3.1.
4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets... 1. If your work uses existing assets, did you cite the creators? See Section 2.1 and Section 3.1. 2. Did you mention the license of the assets? See Section 2.1. 3. Did you include any new assets either in the supplemental material or as a URL? See Abstract. 4. Did you discuss whether and how consent was obtained from people whose data you're using/curating? See Section 2.1. 5. Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? See Section 2.1.
5. If you used crowdsourcing or conducted research with human subjects... 1. Did you include the full text of instructions given to participants and screenshots, if applicable? See Appendix C. 2. Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A] 3. Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? See Section 2.1.

## Appendix A Our Designed Prompts for Flub

Our designed prompts without Chain-of-Thought for FLUB are shown in Figure 4. And the Chain-of-Thought prompts for Task 1 and Task 2 are presented in Figure 5. For the convenience of reading, we provide both prompts and their English translations. Note that we use Chinese prompts in specific experiments because our data samples are in Chinese.

## Appendix B Automatic Evaluation Prompt in Task 3

To enable GPT-4 can automatically score explanations generated by other LLMs according to the settings of Task 3, we carefully design the corresponding prompt, as shown in Figure 6.

## Appendix C Scoring Guidelines in Task 3

Scoring ObjectiveFor the LLMs' output response to each input cunning text, please refer to the given annotated answer to score the output response. This score will be used to measure the quality of the model response and the degree of matching with the answer.

Scoring RulesThe scoring values are defined as {1, 2, 3, 4, 5}. The higher the score, the higher the quality of the model output. **The specific definitions and examples of each score are as follows:**

Figure 4: Our designed prompts without the Chain-of-Thought idea. Task 3(a) is for the texts that are not expressed in the form of inquiries. Task 3(b) is for inquiries.

Figure 5: Our designed prompts with the Chain-of-Thought idea. Task 3(a) is for the texts that are not expressed in the form of inquiries. Task 3(b) is for inquiries.

[MISSING_PAGE_FAIL:16]

#### c.2.1 The First Example of Score 2

Input Text:Dreams are all reversed. I dream of myself looking in the mirror, so I am the one in the mirror. () )

Model Response:The humor of this sentence lies in the literal interpretation of the saying "Dreams are all reversed". Usually, people think that the scene in a dream is the opposite of reality, but in this sentence, the author dreams of himself looking in the mirror, if we understand that "Dreams are all reversed", then the author is actually a reflection in the mirror, which is obviously an absurd interpretation of reality, and therefore has a sense of humor. ()

Correct Answer:"Dreams are all reversed" means that the dream is opposite to reality. If "I dream of myself looking in the mirror" is contrary to reality, the reality should be "I do not look in the mirror in reality", rather than "I dream of myself looking in the mirror, and I am the person in the mirror". ()

#### c.2.2 The Second Example of Score 2

Input Text:If all three of Nezha's heads suffered from cerebral thrombosis for ten years, did he have cerebral thrombosis for ten years or thirty years? ()

Model Response:Nezha is a fictional character in Chinese mythology that does not exist. Cerebral thrombosis is a cerebrovascular disease, usually caused by blood clotting in the blood vessels of the brain resulting in brain hypoxia and ischemia. Therefore, the claim that Nezha's three heads have cerebral thrombosis is not realistic, let alone calculate whether it is ten years or thirty years. ()

Correct Answer:Nezha is an individual, assuming Nezha's three heads all have cerebral thrombosis, it should be something that happens at the same time, and will not produce accumulation, so it should not be 30 years. ()

### Definition and Examples of Score 3

DefinitionThe LLM's response correctly points out the type of irrationality/humor, but the key points of irrationality/humor pointed out by the correct answer are inconsistent. Alternatively, the response is consistent with the key points of irrationality/humor pointed out by the correct answer, but the judgment on the type of cunning text is inconsistent.

#### c.3.1 The First Example of Score 3

Input Text:The drug is partially poisonous with side effects, so we should not take the drug in parts. ()

Model Response:The humor of this sentence lies in the misunderstanding of the literal and actual meaning of the idiom "The drug is partially poisonous with side effects". This idiom is meant to say that any drug has potential side effects, while the sentence misunderstands "partially" to mean that the drug should not be taken in parts, and thus draws the absurd conclusion that the drug cannot be taken in three separate doses, which is obviously a comical distortion of the original meaning. ()

[MISSING_PAGE_EMPTY:18]

[MISSING_PAGE_FAIL:19]

Model Response:The unreasonableness of this sentence lies in the fact that Officer Wang's actions are contrary to the duties of ordinary police officers. Usually, the task of the police is to protect the hostages and try to arrest the criminals, not to kill the hostages to protect the criminals. Thus, the sentence reverses the duties of the police with an absurd logic that produces a humorous effect. ()

Correct Answer:As a police officer, what Wang should do is shoot the robber to save the hostage, not shoot the hostage to save the robber. The fact that Wang killed the hostage by mistake is described humorously. ()

## Appendix D Datasheet For FLUB

### Motivation

1. **For what purpose was the dataset created?** Was there a specific task in mind? Was there a specific gap that needed to be filled? Please provide a description. * In this work, we aim to challenge the reasoning and understanding abilities of LLMs by proposing the FLUB containing cunning texts that are easy for humans to understand but difficult for models to grasp. Specifically, we design three tasks with increasing difficulty to test whether the LLMs can understand the fallacy and solve the "cunning" texts: Answer Selection, (2) Cuning Type Classification, (3) Fallacy Explanation. We hope and believe that our proposed FLUB and all our findings are crucial for LLMs to comprehend the fallacy and handle cunning texts in the real world.
2. **Who created the dataset (e.g., which team, research group) and on behalf of which entity (e.g., company, institution, organization)?** * The dataset is presented by Tsinghua Knowledge Engineering Laboratory (SZ).
3. **Who funded the creation of the dataset?** If there is an associated grant, please provide the name of the grantor and the grant name and number. * This work is sponsored by NSFC, Guangdong Province, Shenzhen City, Peng Cheng Laboratory, and Tsinghua Univerisity.
4. **Any other comments?** * No.

### Composition

1. **What do the instances that comprise the dataset represent (e.g., documents, photos, people, countries)?** Are there multiple types of instances (e.g., movies, users, and ratings; people and interactions between them; nodes and edges)? Please provide a description. * All the instances in FLUB are represented by texts. We make our benchmark openly available on the GitHub page (https://github.com/THUKElab/FLUB).
2. **How many instances are there in total (of each type, if appropriate)?** * FLUB includes 834 instances. For fine-grained cunning types, "False Analogy" has 11 instances, "Lame Jokes" has 44 instances, "Phonetic Error" has 5 instances, "Ambiguity" has 35 instances, "Paradox" has 29 instances, "Factual Error" has 12 instances, "Reasoning Error" has 445 instances, "Word Game" has 239 instances, and "Undefined" has 14 instances.
3. **Does the dataset contain all possible instances or is it a sample (not necessarily random) of instances from a larger set?** If the dataset is a sample, then what is the larger set? Is the sample representative of the larger set (e.g., geographic coverage)? If so, please describe how this representativeness was validated/verified. If it is not representative of the larger set, please describe why not (e.g., to cover a more diverse range of instances, because instances were withheld or unavailable).

* FLUB has contained all possible instances, because we have tried our best to collect as much data as possible from "Ruozhiba" and conducted strict manual annotation.
8. **What data does each instance consist of? "Raw" data (e.g., unprocessed text or images) or features?** In either case, please provide a description. * Each instance consists of the input cunning text, cunning type, fallacy explanation, candidate answers, and the corresponding correct option, as illustrated in Figure 0(b).
9. **Is there a label or target associated with each instance?** If so, please provide a description. * There is a cunning type for each instance, which describes the cunning type of each input text.
10. **Is any information missing from individual instances?** If so, please provide a description, explaining why this information is missing (e.g., because it was unavailable). This does not include intentionally removed information, but might include, e.g., redacted text. * No.
11. **Are relationships between individual instances made explicit (e.g., users' movie ratings, social network links)?** If so, please describe how these relationships are made explicit. * Not applicable.
12. **Are there recommended data splits (e.g., training, development/validation, testing)?** If so, please provide a description of these splits, explaining the rationale behind them. * No, because FLUB is a benchmark test set, all its instances are used for testing LLMs, regardless of training/validation.
13. **Are there any errors, sources of noise, or redundancies in the dataset?** If so, please provide a description. * No.
14. **Is the dataset self-contained, or does it link to or otherwise rely on external resources (e.g., websites, tweets, other datasets)?** If it links to or relies on external resources, a) are there guarantees that they will exist, and remain constant, over time; b) are there official archival versions of the complete dataset (i.e., including the external resources as they existed at the time the dataset was created); c) are there any restrictions (e.g., licenses, fees) associated with any of the external resources that might apply to a dataset consumer? Please provide descriptions of all external resources and any restrictions associated with them, as well as links or other access points, as appropriate. * FLUB is self-contained.
15. **Does the dataset contain data that might be considered confidential (e.g., data that is protected by legal privilege or by doctor-patient confidentiality, data that includes the content of individuals' non-public communications)?** If so, please provide a description. * No.
16. **Does the dataset contain data that, if viewed directly, might be offensive, insulting, threatening, or might otherwise cause anxiety?** If so, please describe why. * No. We have conducted a strict data cleaning process to ensure that FLUB does not contain unethical data.
17. **Does the dataset identify any subpopulations (e.g., by age, gender)?** If so, please describe how these subpopulations are identified and provide a description of their respective distributions within the dataset. * No.
18. **Is it possible to identify individuals (i.e., one or more natural persons), either directly or indirectly (i.e., in combination with other data) from the dataset?** If so, please describe how. * No.
19. **Does the dataset contain data that might be considered sensitive in any way (e.g., data that reveals race or ethnic origins, sexual orientations, religious beliefs, political opinions or union memberships, or locations; financial or health data; biometric or genetic data; forms of government identification, such as social security numbers; criminal history)?** If so, please provide a description. * No.
20. **Any other comments?** * No.

### Collection Process

1. **How was the data associated with each instance acquired? Was the data directly observable (e.g., raw text, movie ratings), reported by subjects (e.g., survey responses), or indirectly inferred/derived from other data (e.g., part-of-speech tags, model-based guesses for age or language)?** If the data was reported by subjects or indirectly inferred/derived from other data, was the data validated/verified? If so, please describe how. * We collect raw text data from "Ruozhiba" in Baidu Tieba, as described in Section 2.1. The data is directly observable at https://github.com/THUKElab/FLUB.
22. **What mechanisms or procedures were used to collect the data (e.g., hardware apparatuses or sensors, manual human curation, software programs, software APIs)? How were these mechanisms or procedures validated?** * We use web crawlers to automatically crawl the raw data, and we perform manual filtering and filtering to validate the crawled data, as described in Section 2.1.
23. **If the dataset is a sample from a larger set, what was the sampling strategy (e.g., deterministic, probabilistic with specific sampling probabilities)?** * Not applicable.
24. **Who was involved in the data collection process (e.g., students, crowdworkers, contractors) and how were they compensated (e.g., how much were crowdworkers paid)?** * We hired crowdworkers to clean the raw data and paid each person $0.50 per piece of raw data.
25. **Over what timeframe was the data collected?** Does this timeframe match the creation timeframe of the data associated with the instances (e.g., recent crawl of old news articles)? If not, please describe the timeframe in which the data associated with the instances was created. * The raw data of FLUB was collected in in October 2023. The task characteristics of FLUB are not time-sensitive, so the collection time is not associated with the data instances.
26. **Were any ethical review processes conducted (e.g., by an institutional review board)?** If so, please provide a description of these review processes, including the outcomes, as well as a link or other access point to any supporting documentation. * Not applicable. Our data collection process does not involve human or animal experiments. In addition, according to the Baidu Bar agreement, the data on Baidu Tieba can be used for academic research free of charge and without liability. Therefore, our data collection process does not require the involvement of an ethical review board.
27. **Did you collect the data from the individuals in question directly, or obtain it via third parties or other sources (e.g., websites)?** * We collect raw text data from "Ruozhiba" in Baidu Tieba, as described in Section 2.1.
28. **Were the individuals in question notified about the data collection?** If so, please describe (or show with screenshots or other information) how notice was provided, and provide a link or other access point to, or otherwise reproduce, the exact language of the notification itself. * Not applicable. According to the Baidu Bar agreement, the data on Baidu Tieba can be used for academic research free of charge and without liability.
29. **Did the individuals in question consent to the collection and use of their data?** If so, please describe (or show with screenshots or other information) how consent was requested and provided, and provide a link or other access point to, or otherwise reproduce, the exact language to which the individuals consent.

* Yes. According to the Baidu Bar agreement, the data on Baidu Tieba can be used for academic research free of charge and without liability.
30. **If consent was obtained, were the consenting individuals provided with a mechanism to revoke their consent in the future or for certain uses?** If so, please provide a description, as well as a link or other access point to the mechanism (if appropriate). * Not applicable.
31. **Has an analysis of the potential impact of the dataset and its use on data subjects (e.g., a data protection impact analysis) been conducted?** If so, please provide a description of this analysis, including the outcomes, as well as a link or other access point to any supporting documentation. * Yes. The cunning texts we are concerned about come from daily life and are very common. Therefore, the new research direction and tasks we propose will not cause harm to human society.
32. **Any other comments?** * No.

### Preprocessing/cleaning/labeling

1. **Was any preprocessing/cleaning/labeling of the data done (e.g., discretization or bucketing, tokenization, part-of-speech tagging, SIFT feature extraction, removal of instances, processing of missing values)?** If so, please provide a description. If not, you may skip the remaining questions in this section. * Yes. We employ annotators to manually filter out irrelevant posts that do not present cunning texts. Since the collected original posts contain irrelevant content such as links and images, we also require annotators to extract the fallacious and illogical contents from the raw post and rewrite them into a complete sentence. Besides, it is worth noting that we carefully ensure that the texts in FLUB are ethical texts. This process includes user information anonymization, sensitive information removal, and filtering of impolite posts.
2. **Was the "raw" data saved in addition to the preprocessed/cleaned/labeled data (e.g., to support unanticipated future uses)?** If so, please provide a link or other access point to the "raw" data. * No.
3. **Is the software that was used to preprocess/clean/label the data available?** If so, please provide a link or other access point. * No.
4. **Any other comments?** * No.

### Uses

1. **Has the dataset been used for any tasks already?** If so, please provide a description. * No.
2. **Is there a repository that links to any or all papers or systems that use the dataset?** If so, please provide a link or other access point. * No.
3. **What (other) tasks could the dataset be used for?** * Based on our constructed FLUB and its annotation information, we design three tasks with increasing difficulty to test whether the LLMs can understand the fallacy and solve the "cunning" texts. Specifically, (1) Answer Selection: The model is asked to select the correct one from the four answers provided by FLUB for each input text. (2) Cunning Type Classification: Given a cunning text as input, the model is expected to directly identify its fallacy type defined in our scheme. (3) Fallacy Explanation: Wehope the model sees a cunning text and intelligently generates a correct explanation for the fallacy contained in the text, just like humans, without falling into its trap.
40. **Is there anything about the composition of the dataset or the way it was collected and preprocessed/cleaned/labeled that might impact future uses?** For example, is there anything that a dataset consumer might need to know to avoid uses that could result in unfair treatment of individuals or groups (e.g., stereotyping, quality of service issues) or other risks or harms (e.g., legal risks, financial harms)? If so, please provide a description. Is there anything a dataset consumer could do to mitigate these risks or harms? * No.
41. **Are there tasks for which the dataset should not be used?** If so, please provide a description. * According to the characteristics of the data in FLUB, it is known that in addition to the three benchmark tasks we designed, we think that it may also be suitable for improving the reasoning ability and humor ability of LLMs. Beyond that, FLUB may not be suitable for other tasks.
42. **Any other comments?** * No.

### Distribution

1. **Will the dataset be distributed to third parties outside of the entity (e.g., company, institution, organization) on behalf of which the dataset was created?** If so, please provide a description. * Yes, the dataset has been open-source.
2. **How will the dataset will be distributed (e.g., tarball on website, API, GitHub)?** Does the dataset have a digital object identifier (DOI)? * The data is available through https://github.com/THUKElab/FLUB.
3. **When will the dataset be distributed?** * The dataset has been open-source.
4. **Will the dataset be distributed under a copyright or other intellectual property (IP) license, and/or under applicable terms of use (ToU)?** If so, please describe this license and/or ToU, and provide a link or other access point to, or otherwise reproduce, any relevant licensing terms or ToU, as well as any fees associated with these restrictions. * FLUB is published under Creative Commons Attribution-NonCommercial 4.0 International (CC BY-NC 4.0), which means everyone can use this dataset for non-commercial research purposes.
5. **Have any third parties imposed IP-based or other restrictions on the data associated with the instances?** If so, please describe these restrictions, and provide a link or other access point to, or otherwise reproduce, any relevant licensing terms, as well as any fees associated with these restrictions. * We collect raw text data from "Ruozhiba" in Baidu Tieba. According to the Baidu Bar agreement, the data on Baidu Tieba can be used for academic research free of charge and without liability.
6. **Do any export controls or other regulatory restrictions apply to the dataset or to individual instances?** If so, please describe these restrictions, and provide a link or other access point to, or otherwise reproduce, any supporting documentation. * No.
7. **Any other comments?** * No.

### Maintenance

* **Who will be supporting/hosting/maintaining the dataset?*
* Tsinghua Knowledge Engineering Laboratory (SZ) will support hosting of the dataset.
* **How can the owner/curator/manager of the dataset be contacted (e.g., email address)?*
* The manager of FLUB can be contacted through:
* Email (liyinghu20@mails.tsinghua.edu.cn)
* GitHub issues (https://github.com/THUKElab/FLUB/issues).
* **Is there an erratum?*
* If so, please provide a link or other access point.
* There is no erratum for our first release. Errata will be documented on the dataset website as a future release.
* **Will the dataset be updated (e.g., to correct labeling errors, add new instances, delete instances)?*
* If so, please describe how often, by whom, and how updates will be communicated to dataset consumers (e.g., mailing list, GitHub)?
* Yes. Once any other researchers find that FLUB needs to be updated, we will immediately update it through GitHub.
* **If the dataset relates to people, are there applicable limits on the retention of the data associated with the instances (e.g., were the individuals in question told that their data would be retained for a fixed period of time and then deleted)?*
* If so, please describe these limits and explain how they will be enforced.
* Not applicable.
* **Will older versions of the dataset continue to be supported/hosted/maintained?*
* If so, please describe how. If not, please describe how its obsolescence will be communicated to dataset consumers.
* Yes. We will continue to support FLUB. Once any other researchers find that FLUB needs to be updated, we will immediately update it through GitHub.
* **If others want to extend/augment/build on/contribute to the dataset, is there a mechanism for them to do so? If so, please provide a description. Will these contributions be validated/verified?*
* If so, please describe how. If not, why not? Is there a process for communicating/distributing these contributions to dataset consumers? If so, please provide a description.
* Yes. Once any other researchers find that FLUB needs to be updated, after they contact us via email or GitHub, we will review the data they want to expand. After the new data passes review, we will immediately update it to GitHub.
* **Any other comments?*

## Appendix E Metadata and Data Format of FLUB

### Croissant Metadata

To provide the key descriptive information of FLUB more clearly and improve the traceability and reproducibility of our data, we also provide the Croissant metadata of FLUB, please refer to the link https://github.com/THUKElab/FLUB/blob/main/FLUB_croissant_metadata.json for details.

### Data Format

``` { "text":"Theinputcunningtext", "is_question":"Istheinputcunningtextaquestion?","type":"ThecunningtypeoftheinputtextfortheCunningTypeClassificationtask.","explanation":"ThecorrectexplanationoftheinputtextfortheFallacyExplanationtask.","id":"Theidofeachdatasample","options":{"A":"Thecandidateanswer1forttheinputtext(question)","B":"Thecandidateanswer2fortheinputtext(question)","C":"Thecandidateanswer3forttheinputtext(question)","D":"Thecandidateanswer4forttheinputtext(question)" },"answer":"ThecorrectanswerfortheAnswerSelection(MultipleChoice)task." } ```

## Appendix F Author Statement of FLUB

We, as the authors of the FLUB dataset, hereby declare the following:

1. **Responsibility Statement**: The creation, organization, and publication of FLUB are entirely our responsibility. We confirm that all data were legally obtained and do not infringe on the intellectual property or other rights of any third party. In the event of any disputes or legal liabilities arising from the use of this dataset, we, as the authors, will assume full responsibility.
2. **Data License**: This dataset is released under the following license: Creative Commons Attribution-NonCommercial 4.0 International (CC BY-NC 4.0). Users must comply with the terms of this license agreement when using this dataset. For detailed license terms, please refer to CC BY-NC 4.0.
3. **Data Integrity and Quality**: We have made every effort to ensure the integrity and quality of FLUB. However, due to the dataset's size and complexity, we cannot guarantee it to be completely error-free. If any errors or omissions are discovered, please contact us for corrections and updates.
4. **Ethical Statement**: We have strictly adhered to relevant ethical guidelines during the data collection and processing stages to ensure that the use of this dataset does not negatively impact or harm any individual or organization.