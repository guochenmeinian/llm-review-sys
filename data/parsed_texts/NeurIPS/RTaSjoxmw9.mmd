# Outlier-Robust Phase Retrieval in Nearly-Linear Time

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Phase retrieval is a fundamental problem in signal processing, where the goal is to recover a (complex-valued) signal from phaseless intensity measurements. It is known that natural non-convex formulations of phase retrieval do not have spurious local optima. However, the theoretical analyses of such landscape results often rely on strong assumptions, such as the sampling vectors being Gaussian distributed.

In this paper, we propose and study the problem of outlier robust phase retrieval. We seek to recover a vector \(x\in\mathbb{R}^{d}\) from \(n\) intensity measurements \(y_{i}=(a_{i}^{\top}x)^{2}\), where the sampling vectors \(a_{i}\)'s are initially i.i.d. Gaussian, but a small fraction of the \((a_{i},y_{i})\) pairs are adversarially corrupted.

Our main result is a near-sample-optimal and nearly-linear-time algorithm that provably recovers the ground-truth \(x\) in the presence of adversarial corruptions. We first solve a lightweight convex program to find a vector close to the ground truth. We then run robust gradient descent starting from this initial solution, leveraging recent advances in high-dimensional robust statistics. Our approach is conceptually simple and provides a framework for developing robust algorithms for other tractable non-convex problems.

## 1 Introduction

Phase retrieval is a fundamental problem in signal processing with applications in various fields, including electron microscopy [32], crystallography [33; 36], astronomy [11], and optical imaging [37]. In these applications, one often has access to only the magnitudes of the Fourier transforms of a complex signal. This is because measuring magnitude (e.g., by aggregating energy over time) is much easier than measuring phase (which requires detecting rapid changes). We refer the reader to the survey articles [37; 26] for more details about the theory and applications of phase retrieval.

In this paper, we focus on the real-valued generalized phase retrieval problem, where the Fourier transform is replaced by a general linear operator. We first give a formal definition of this problem.

**Definition 1.1** (Phase Retrieval).: Let \(x\in\mathbb{R}^{d}\) be the ground-truth vector. Let \(a_{1}\ldots a_{n}\in\mathbb{R}^{d}\) be \(n\) sampling vectors and let \(y_{i}=\langle a_{i},x\rangle^{2}\in\mathbb{R}\) be the corresponding intensity measurements. Given \((a_{i},y_{i})_{i=1}^{n}\) as input, the task is to recover \(x\).

Note that it is impossible to distinguish between \(x\) and \(-x\), so it is sufficient to recover either one. Under certain assumptions (e.g., when the \(a_{i}\)'s are Gaussian distributed), the phase retrieval problem in Definition 1.1 can be solved in polynomial time with provable recovery guarantees. This was first achieved via approaches based on semidefinite programming (SDP) relaxations (see, e.g., Candes et al. [5]). In practice, the problem is often solved using first-order optimization algorithms such as gradient descent. It is well-established that, although many natural formulations of phase retrieval have nonconvex objectives, all local optima are globally optimal under certain assumptions [34; 3; 40]. An example of such objective function is the following:

\[\text{minimize}\quad f(z)=\sum_{i=1}^{n}(y_{i}-\langle a_{i},z\rangle^{2})^{ 2}\quad\text{subject to}\quad z\in\mathbb{R}^{d}.\]However, existing analyses of such landscape results often rely on strong assumptions, such as the sampling vectors \(a_{i}\)'s are i.i.d. Gaussian. Our work is motivated by the following questions: Can we relax the assumptions used in proving landscape results in many tractable nonconvex problems? In the context of phase retrieval, what happens if a small fraction of the \((a_{i},y_{i})\)'s are changed adversarially? We focus on the following strong contamination model (see, e.g., [13]).

**Definition 1.2** (\(\epsilon\)-Corruption).: An algorithm first specifies the number of samples \(n\), and \(n\) samples are drawn independently from some unknown distribution \(D\). The adversary is allowed to replace up to \(\epsilon n\) samples with arbitrary points. The modified set of \(n\) samples is then given to the algorithm as input. We say that a set of samples is \(\epsilon\)-corrupted if it is generated by the above process. 1

Footnote 1: We write \(G\subseteq[n]\) for the (remaining) good samples, and \(B=[n]\setminus G\) for the corrupted samples.

Under the \(\epsilon\)-corruption model for high-dimensional data, a common goal is to design efficient algorithms that can achieve dimension-independent error guarantees. Early work in robust statistics [42, 23, 25] provided sample-efficient estimators for various tasks, but with runtimes exponential in the dimension. A recent line of work, initiated by [13, 28], has developed computationally efficient robust algorithms for many fundamental high-dimensional tasks. There has been significant progress in the algorithmic aspects of robust high-dimensional statistics (see, e.g., [12]).

We now formally define the main problem that we pose and study in this paper.

**Problem 1.3** (Outlier-Robust Phase Retrieval).: Let \(\epsilon>0\). Let \(x\in\mathbb{R}^{d}\) be the ground-truth vector with \(\left\|x\right\|_{2}=1\). First, \(n\) sampling vectors \((a_{i})_{i=1}^{n}\) are drawn i.i.d. from \(\mathcal{N}(0,I)\in\mathbb{R}^{d}\). Let \(y_{i}=\left\langle a_{i},x\right\rangle^{2}\) be the corresponding intensity measurements. Then, an adversary arbitrarily corrupts an \(\epsilon\)-fraction of the \((a_{i},y_{i})\)'s. Finally, the corrupted \((a_{i},y_{i})\)'s are given to the algorithm as input. The task is to find a vector \(z\in\mathbb{R}^{d}\) such that \(\min\{\left\|z-x\right\|_{2},\left\|z+x\right\|_{2}\}\leq\Delta\) for some precision parameter \(\Delta>0\).

Note that we allow corruption in both the sampling vectors \(a_{i}\in\mathbb{R}^{d}\) and the intensity measurements \(y_{i}\in\mathbb{R}\). We would like to answer the following algorithmic question:

_Can we design a provably robust and near sample-optimal algorithm for the \(\epsilon\)-corrupted phase retrieval problem (Problem 1.3) that runs in nearly-linear time?_

### Our Results and Contributions

In this paper, we answer the above question affirmatively. We first state the main result of our paper.

**Theorem 1.4** (Main, Informal).: _Consider the outlier-robust phase retrieval problem (Problem 1.3). Let \(\Delta>0\). Given an \(\epsilon\)-corrupted set of \(n=\widetilde{\Omega}(d\log^{2}(1/\Delta))\) samples, we can compute \(z\in\mathbb{R}^{d}\) in time \(\widetilde{O}(nd)\) such that \(\min(\left\|z-x\right\|_{2},\left\|z+x\right\|_{2})\leq\Delta\) with probability at least \(0.8\)._

Our algorithm has near-optimal sample complexity, because even without corruption, recovering the ground-truth vector \(x\) in general requires \(\Omega(d)\) samples because there are \(d\) degrees of freedom in \(x\). Moreover, our algorithm runs in time nearly-linear in the size of the input, and provably recovers the ground-truth vector \(x\) with arbitrary precision \(\Delta\). The formal version of Theorem 1.4 is stated as Theorem 3.1 in Section 3.

We remark that the success probability of Theorem 1.4 can be boosted to \(1-\delta\) for any \(\delta>0\) by incurring an additional factor of \(T=O(\log(1/\delta))\) in the sample complexity and runtime. We can randomly partition the input into \(T\) equal-sized disjoint sets and run our algorithm on each set to obtain \(T\) solutions \(Z=\{z_{1},\ldots,z_{T}\}\). If we output a solution \(z^{*}\) that has the maximum number of points in \(Z\) within distance \(2\Delta\), we can show that \(r(z^{*})\leq 3\Delta\) with probability at least \(1-\delta\).

Our main conceptual contribution is to propose and study the outlier-robust phase retrieval problem, where a small fraction of the input data is adversarially corrupted. Note that we allow arbitrary corruption in both the sampling vectors \(a_{i}\in\mathbb{R}^{d}\) and the intensity measurements \(y_{i}\in\mathbb{R}\). The nonconvex optimization landscape of phase retrieval is well understood when the \(a_{i}\)'s are Gaussian distributed, but the adversarial robustness of such landscape results is largely unexplored.

Our main technical contributions include the design and analysis of a near sample-optimal and nearly-linear time algorithm that provably solves the phase retrieval problem in the presence of outliers. Our approach provides a conceptually simple two-step framework for developing outlier-robust algorithms for tractable nonconvex problems that combines the robustness of spectral initialization and the efficiency of the subsequent robust gradient descent.

### Our Approach and Techniques

When there are infinite samples and no corruption, the objective function \(f(z)\) can be simplified as

\[f(z)=\mathop{\mathbf{E}}_{a\sim\mathcal{N}(0,I_{d})}\left[\left\langle\left\langle a _{i},z\right\rangle^{2}-y_{i}\right\rangle^{2}\right]=3\left\|x\right\|_{2}^{4 }+3\left\|z\right\|_{2}^{4}-2\left\|x\right\|_{2}^{2}\left\|z\right\|_{2}^{2}- 4\left\langle x,z\right\rangle^{2}.\] (1)

Even though \(f(z)\) is nonconvex, we know that it has no spurious local optima [34, 3, 40].

Our approach follows the general structure of Candes et al. [3], which uses a two-step procedure. The first step uses spectral techniques to find an initial guess that is close enough to the ground truth. The second step applies gradient descent to converge to the final solution. However, both steps are susceptible to adversarial corruption. We develop nearly-linear time and provably robust algorithms for both steps and combine them to get our main result.

**Step 1: Robust Spectral Initialization.** When there is no adversarial corruption, the empirical second-moment matrix \(Y=(1/n)\sum_{i=1}^{n}y_{i}a_{i}a_{i}^{\top}\) has expectation \(\mathop{\mathbf{E}}\left[Y\right]=I+2xx^{\top}\), so its top eigenvector is close to \(x\). However, the adversary can arbitrarily change the top eigenvector.

To circumvent this issue, we assign a (nonnegative) weight \(w_{i}\) to each sample, and let \(Y_{w}\) denote the weighted intensity-based second-moment matrix \(Y_{w}=\sum_{i=1}^{n}w_{i}y_{i}a_{i}a_{i}^{\top}\). Ideally, if the weights \(w\) are uniformly distributed on the remaining clean samples, the top eigenvector of \(Y_{w}\) will align with \(x\). We propose a novel optimization problem that can be used to find a weighting \(w\) such that \(Y_{w}\) must be close to the unknown unbiased expectation \(I+2xx^{\top}\). Moreover, we show that such a weight \(w\) can be computed in nearly-linear time.

**Step 2: Approximate Gradient Descent.** Starting with the initial guess \(z_{1}\in\mathbb{R}^{d}\) produced by the robust spectral initialization, we want to apply gradient descent to recover the ground truth \(x\in\mathbb{R}^{d}\). Without corruption, if the initialization is close enough to \(x\), each iteration will bring \(z\) closer to \(x\) by a constant factor. This convergence guarantee can be compromised by the corrupted samples.

At a high level, approximating the gradient at a specific point amounts to a robust mean estimation problem (for the underlying distribution of the gradients). When the input data is \(\epsilon\)-corrupted, the gradients of the \(n\) samples can be viewed as an \(\epsilon\)-corrupted set of vectors. We can approximate the true gradient using this \(\epsilon\)-corrupted set of \(n\) gradients using robust mean estimation algorithms.

### Related and Prior Works

**Phase Retrieval.** The problem of phase retrieval arises in many areas of science and engineering [11, 33]. Early research on this problem proposes error-reduction algorithms [22, 17, 18]. Convex and nonconvex optimization with various objective functions were later proposed and achieved exact recovery [43, 3, 4, 5, 38]. Follow-up works generalize to robust phase retrieval where the observations are subject to perturbations [45, 27, 7, 6, 31].

**Nonconvex Optimization.** Even though optimizing a nonconvex function is NP-Hard in general, recent works showed that many nonconvex functions are locally optimizable due to discrete or rotational symmetry. Besides phase retrieval, it is known that all local optima are globally optimal for natural nonconvex formulations of a wide range of machine learning problems, such as matrix completion [21], matrix sensing [2], phase synchronization [1], dictionary learning [39], and tensor decomposition [20] (see also Chapter 7 of [44]). Closely related to our work, a recent line of work explored the robustness of these landscape results: [30] studied matrix sensing in the \(\epsilon\)-corrupted model and [8, 19] studied matrix completion and matrix sensing in semi-random models.

**High-Dimensional Robust Statistics.** Recent works in high-dimensional robust statistics developed nearly-linear time algorithms for the problem of robust mean estimation [9, 16, 29]. Prior works [35, 14] developed meta-algorithms for finding _first-order_ stationary points with dimension-independent accuracy guarantees, which is closely related to the robust gradient descent procedure that we use.

### Roadmap

We first introduce notations and background in Section 2. Then we give an overview of our approach in Section 3. Next, we focus on how to get an initialization that is close enough to the ground truth \(x\) in Section 4. After the initialization, we use robust mean algorithms to estimate gradients to converge to the desired accuracy in Section 5. Finally, we conclude in Section 6 and discuss open problems.

## 2 Preliminary and Background

**Notation.** We write \([n]\) for the set of integers \(\{1,\ldots,n\}\). We use \(\{e_{1},\ldots,e_{d}\}\) for the standard unit vector basis in \(\mathbb{R}^{d}\) and \(I\) for the identity matrix. For a vector \(x\), we denote its \(\ell_{1}\), \(\ell_{2}\) and \(\ell_{\infty}\) norm as \(\left\|x\right\|_{1}\), \(\left\|x\right\|_{2}\) and \(\left\|x\right\|_{\infty}\), respectively, and write the \(i^{\text{th}}\) coordinate in \(x\) as \(x_{i}\). For vectors \(x,y\in\mathbb{R}^{d}\), we denote its inner product as \(\left\langle x,y\right\rangle=x^{\top}y\). For a matrix \(A\), we use \(\left\|A\right\|_{2}\), \(\left\|A\right\|_{*}\), and \(\left\|A\right\|_{F}\) as its operator norm, nuclear norm, and Frobenius norm, respectively. We write \(\lambda_{k}(A)\) as the \(k\)th-largest eigenvalues of \(A\), and \(\overline{\lambda}_{k}(A)\) as the sum of the \(k\) largest eigenvalues. A symmetric \(n\times n\) matrix \(A\) is said to be positive semidefinite (PSD) if for all vectors \(x\in\mathbb{R}^{n}\), \(x^{\top}Ax\geq 0\). For two symmetric matrices \(A\) and \(B\), we write \(A\preceq B\) when \(B-A\) is positive semidefinite.

**Packing SDP.** We will use nearly-linear time solvers for the following packing SDP.

\[\max_{w}\ \left\|w\right\|_{1}\qquad\mathrm{subject\ to}\quad\sum_{i=1}^{n}w_{i}A _{i}\preceq I,\quad\overline{\lambda}_{k}\left(\sum_{i=1}^{n}w_{i}B_{i}\right) \leq k,\quad w_{i}\geq 0,\forall i.\] (*)

**Lemma 2.1** ([10]).: _Given an instance of optimization (*) with semi-positive definite matrices \(A_{i}\in\mathbb{R}^{d_{1}\times d_{1}}\) and \(B_{i}\in\mathbb{R}^{d_{2}\times d_{2}}\) with \(A_{i}=C_{i}C_{i}^{\top}\), \(B_{i}=D_{i}D_{i}^{\top}\) for all \(i=1,2,\cdots,m\), together with integer \(k>0\), error tolerance \(\epsilon_{0}\geq 1/m^{2}\), and failure probability \(\delta_{0}\), there is an algorithm that runs in time \(\widetilde{O}((t_{C}+t_{D}+d_{1}+d_{2})\operatorname{poly}(1/\epsilon_{0}, \log 1/\delta_{0}))\), where \(t_{C_{i}}\) and \(t_{D_{i}}\) are the time take to perform a matrix product with \(C_{i}\) and \(D_{i}\) respectively and \(t_{C}=\sum_{i=1}^{n}t_{C_{i}}\) and \(t_{D}=\sum_{i=1}^{n}t_{D_{i}}\), and outputs \(w^{\prime}\) with \(\left\|w^{\prime}\right\|_{1}\geq(1-\epsilon_{0})\mathsf{OPT}\) where \(\mathsf{OPT}\) is optimal value, with probability at least \(1-\delta_{0}\)._

**Computing the Top Eigenvector.** We use power method to compute the top eigenvector of a matrix.

**Lemma 2.2** (Power Method for Top Eigenvector, e.g., [41]).: _Let \(A\in\mathbb{R}^{d\times d}\) and let \(\lambda_{1}\) be its largest eigenvalue. For any \(\overline{\delta}\in(0,1)\), there exists an algorithm that takes \(A\) and outputs a unit vector \(x\in\mathbb{R}^{d}\) in time \(O(t\log(d)/\overline{\delta})\) such that \(x^{T}Ax\geq(1-\overline{\delta})\lambda_{1}\) with probability at least \(0.99\), where \(t\) is the time required to compute \(Av\) for an arbitrary \(v\in\mathbb{R}^{d}\)._

**Robust Mean Estimation.** Another tool we use is robust mean estimation in the \(\epsilon\)-corruption model for distributions with bounded covariance. We use robust mean estimation algorithms to approximate the true gradient under adversarial corruption.

**Lemma 2.3** (Robust Mean Estimation, e.g., [15]).: _Let \(\mathcal{D}\) be a distribution on \(\mathbb{R}^{d}\) with unknown mean \(\mu\) and unknown covariance matrix \(\Sigma\) where \(\Sigma\preceq\sigma^{2}I\). Let \(\epsilon_{0}\) be a sufficiently small universal constant. Let \(0<\epsilon<\epsilon_{0}\) and \(\delta>0\). Given an \(\epsilon\)-corrupted set of \(n\) samples drawn from \(\mathcal{D}\), we can output a vector \(\widehat{\mu}\in\mathbb{R}^{d}\) in time \(\widetilde{O}(nd\log(1/\delta))\) such that, with probability at least \(1-\delta-\exp(-n\epsilon)\), we have \(\left\|\widehat{\mu}-\mu\right\|_{2}=O\bigg{(}\sqrt{\epsilon}+\sqrt{\frac{d} {nd}}+\sqrt{\frac{d(\log{d}+\log{1}/\delta)}{n}}\bigg{)}\,\sigma\)._

## 3 Overview

We first state a formal version of our main result.

**Theorem 3.1** (Main).: _Consider the setting of Problem 1.3. Let \(0<\epsilon<\epsilon^{\prime}\) for some universal constant \(\epsilon^{\prime}\) and let \(\Delta>0\). Given an \(\epsilon\)-corrupted set of \(n=\widetilde{\Omega}(d\log^{2}(1/\Delta))\) samples, we can compute a vector \(z\in\mathbb{R}^{d}\) in time \(\widetilde{O}(nd\log(1/\Delta))\) such that \(r(z)=\min\{\left\|z-x\right\|_{2},\left\|z+x\right\|_{2}\}\leq\Delta\) with probability at least \(0.8\)._

Theorem 3.1 requires two key technical lemmas: the robust spectral initialization (Lemma 3.2) and the approximate gradient descent (Lemma 3.3).

We first show that the spectral initialization can be done in nearly linear time with high probability, the proof of which can be found in Section 4.

**Lemma 3.2** (Robust Spectral Initialization).: _Under the setting of Problem 1.3, for any \(0<\epsilon<\epsilon^{\prime}\) for some universal constant \(\epsilon^{\prime}>0\), given an \(\epsilon\)-corrupted set of \(n=\widetilde{\Omega}(d)\) samples, we can compute a vector \(z_{0}\in\mathbb{R}^{d}\) of the ground truth \(x\) in time \(\widetilde{O}(nd)\) such that \(r(z_{1})=\min\{\left\|z_{1}-x\right\|_{2},\left\|z_{1}+x\right\|_{2}\}\leq \frac{1}{8}\) with probability at least \(0.95\)._Then, with such initialization results, we can proceed to show that an approximate gradient descent algorithm can be used to find an arbitrary approximation of the ground truth in Section 5.

**Lemma 3.3** (Robust Gradient Descent).: _Consider the setting of Problem 1.3. Let \(\Delta>0\) be the desired precision. Let \(0<\epsilon<\epsilon_{0}\) for some sufficiently small universal constant \(\epsilon_{0}\). Given an \(\epsilon\)-corrupted set of \(n=\widetilde{\Omega}(d\log^{2}(1/\Delta))\) samples and an initial guess \(z_{1}\) such that \(r(z_{1})=\min(\left\|z_{1}-x\right\|_{2},\left\|z_{1}+x\right\|_{2})\leq 1/8\), we can compute a vector \(z\in\mathbb{R}^{d}\) in time \(\widetilde{O}(nd)\) such that \(r(z)\leq\Delta\) with probability at least \(0.95\)._

For technical reasons, we cannot use the same set of samples for both the robust spectral initialization and the approximate gradient descent. Therefore, we partition the \(\epsilon\)-corrupted set of \(2n\) samples into two equally sized disjoint sets, using one set for each algorithm.

Proof of Theorem 3.1.: Let \(2n=\widetilde{\Omega}(d\log^{2}(1/\Delta))\) be a set of \(\epsilon/2\)-corrupted samples. We partition the input into two disjoint sets of \(n\) samples. Both sets are \(\epsilon\)-corrupted. By Lemmas 3.2 and 3.3, for any \(\epsilon\in[0,\epsilon^{\prime}]\) and \(\Delta>0\), our algorithm takes the first set of samples and output a vector \(z^{\prime}\) in time \(\widetilde{O}(nd)\) such that \(r(z^{\prime})\leq 1/8\) with probability at least \(0.95\). Then, using \(z^{\prime}\) and the second set of samples, our algorithm can output \(z\in\mathbb{R}^{d}\) in time \(\widetilde{O}(nd)\) such that \(r(z)\leq\Delta\) with probability at least \(0.95\). The overall success probability is at least \(0.8\), and the combined running time is \(\widetilde{O}(nd)\). 

## 4 Robust Spectral Initialization

We dedicate this section to proving Lemma 3.2: Given an \(\epsilon\)-corrupted set of \((a_{i},y_{i})\)'s, we can compute an initial guess \(z_{1}\in\mathbb{R}^{d}\) that is close to the ground truth \(x\), where \(\min(\left\|z_{1}-x\right\|_{2},\left\|z_{1}+x\right\|_{2})\leq 1/8\). To build some intuition, consider the following intensity-based covariance matrix \(\tilde{Y}=\frac{1}{n}\sum_{i=1}^{n}y_{i}a_{i}a_{i}^{\top}\), where each \(a_{i}\) is drawn independently from \(\mathcal{N}(0,I)\) and \(y_{i}=\langle a_{i},x\rangle^{2}\). The expectation of this matrix is \(\mathbb{E}[Y]=I+2xx^{\top}\). In other words, when there are enough samples and no adversarial corruption, we can obtain a good guess of the ground truth \(x\) (or \(-x\)) by computing the top eigenvector of \(Y\). However, we cannot rely on this approach in adversarial settings.

To tackle this issue, we propose a nearly-linear time preprocessing step (Algorithm 1) that can recover the true expectation of \(\tilde{Y}\) under adversarial corruptions. Algorithm 1 assigns a non-negative weight to each sample. For a weight vector \(w\in\mathbb{R}^{n}\) and a set of indices \(S\subseteq[n]\), the weighted intensity-based covariance matrix is defined as \(Y_{S,w}\doteq\sum_{i\in S}w_{i}y_{i}a_{i}a_{i}^{\top}\), and we omit \(S\) when \(S=[n]\). The feasible region for the weight vector is: \(\Delta_{n,\epsilon}:=\left\{w\in\mathbb{R}^{n}:\left\|w\right\|_{1}=1\text{ and }\forall i\in[n],0\leq w_{i}\leq\frac{1}{(1-\epsilon)n}\right\}.\)

A weight \(w\) defines an empirical distributions over the samples \((a_{i},y_{i})_{i=1}^{n}\), where the largest probability assigned to any point is \(\frac{1}{(1-\epsilon)n}\). Ideally, we would like to find a weight vector \(w^{*}\in\Delta_{n,\epsilon}\) that assigns its weight uniformly to all the uncorrupted samples, i.e., \(w_{i}^{*}=\frac{1}{(1-\epsilon)n}\cdot 1_{i\in G}\). To find a suitable weighting \(w\), we use the following optimization problem (**) in which \(\overline{\lambda}_{2}\) returns the sum of the top two eigenvalues (commonly known as the Ky Fan \(k\) norm for \(k=2\)).

\[\min_{w} \overline{\lambda}_{2}\bigg{(}\sum_{i=1}^{n}w_{i}y_{i}a_{i}a_{i}^{ \top}\bigg{)}\qquad\mathrm{subject\ to}\quad 0\leq w_{i}\leq\tfrac{1}{(1- \epsilon)n},\forall i\in[n],\quad\sum_{i=1}^{n}w_{i}=1\.\] (**)

At a high level, our main observation is that \(y_{i}a_{i}a_{i}^{\top}\) is always a positive semidefinite matrix as \(y_{i}\geq 0\). Consequently, the adversary can only _add_ extra directions with large eigenvalues, but will not be able to remove the eigendirection of \(x\). By minimizing the Ky Fan \(2\) norm, we can remove any directions added by the adversary and make sure that the only remaining large eigendirection is close to \(x\).

Let \(\delta\geq 0\) be some constant to be determined. We show that we can obtain a robust spectral initialization by solving the packing SDP problem (*), which can be solved efficiently using Lemma 2.1. In particular, to fit the reweighting problem of (**) into the framework of the generalized packing problem (*), we define the following constraint matrices for all \(i\in[n]\) :

\[A_{i}:=(1-\epsilon)n\cdot e_{i}e_{i}^{\top},\quad\ B_{i}:=\tfrac{1}{2}(1- \delta)y_{i}a_{i}a_{i}^{\top}.\] (2)

The matrices \((A_{i})_{i=1}^{n}\) are used to implement the constraint that \(w\in\Delta_{n,\epsilon}\). The matrices \((B_{i})_{i=1}^{n}\) help make sure the sum of the top two eigenvalues of \(Y_{w}\) must be at most roughly \(4\), because \(\overline{\lambda}_{2}(Y_{w^{*}})\approx 4\).

First, we show that the weight \(w^{\prime}\) computed by Algorithm 1 can ensure the weighted intensity-based covariance matrix \(Y_{w^{\prime}}\) is close enough to the unbiased expectation \(I+2xx^{\top}\).

**Lemma 4.1**.: _With probability at least \(0.98\), the \(w^{\prime}\) outputted by Algorithm 1 satisfies:_

\[\left\|Y_{w^{\prime}}-\left(I+2xx^{\top}\right)\right\|_{2}=O(\delta)\] (3)

In order to show Lemma 4.1, we need the following auxiliary Lemma 4.2, the proof of which can be found in Section A. Intuitively, Lemma 4.2 suggests that any weight \(w\) in the feasible region \(\Delta_{n,2\epsilon}\) will not have a huge impact on the properties of uncorrupted measurements.

**Lemma 4.2**.: _For any \(\delta_{0}>0\), and sufficiently small \(\epsilon\geq 0\), given a set of \(n\)\(\epsilon\)-corrupted samples with \(n>\widetilde{\Omega}(d)\), with probability at least \(0.98\), we have \(\left\|Y_{G,w}-\left(I+2xx^{\top}\right)\right\|_{2}\leq\delta_{0}\) for all \(w\in\Delta_{n,2\epsilon}\)._

Using Lemma 4.2, we provide a proof sketch for Lemma 4.1, and defer the details to Section A.

Proof.: We condition on the fact that the event of Lemma 4.2 holds (with probability at least \(0.98\)) for \(\delta_{0}=\delta\). Thus, for the remaining of the proof, we assume that for all \(w\in\Delta_{n,2\epsilon}\), it holds that \(\left\|Y_{G,w}-\left(I+2xx^{\top}\right)\right\|_{2}\leq\delta\).

Let \(\lambda_{1}\) and \(\lambda_{2}\) be the top two eigenvalues of \(Y_{w^{\prime}}\), with \(v_{1}\) and \(v_{2}\) to be their corresponding eigenvectors.

Note that the largest eigenvalue of \(I+2xx^{\top}\) is \(3\), and the rest of the eigenvalues are all \(1\). In the proof, we show that the eigenvalues of \(Y_{w^{\prime}}\) are also close to the ones of \(I+2xx^{\top}\). Our proof consists of two parts. We first establish lower bounds for \(\lambda_{1}\) and \(\lambda_{2}\), and then find an upper bound for \(\lambda_{1}+\lambda_{2}\).

Lower Bound.Since \(y_{i}a_{i}a_{i}^{\top}\succeq 0\) for any \(i\in[n]\), for any positive weight vector \(w\in\Delta_{n,\epsilon}\), we have \(Y_{G,w}\preceq Y_{w}\). Thus a lower bound on eigenvalues of \(Y_{G,w^{\prime}}\) will also be a lower bound on \(Y_{w^{\prime}}\).

For the top eigenvalue \(\lambda_{1}\) of \(Y_{w^{\prime}}\), it holds

\[\lambda_{1}=v_{1}^{\top}Y_{w^{\prime}}v_{1}\geq x^{\top}Y_{w^{\prime}}x\geq x ^{\top}Y_{G,w^{\prime}}x\geq x^{\top}(I+2xx^{\top})x-\delta=3-\delta.\] (4)

Similarly, for the second largest eigenvalue \(\lambda_{2}\) of \(Y_{w^{\prime}}\), we have:

\[\lambda_{2}=v_{2}^{\top}Y_{w^{\prime}}v_{2}\geq v_{2}^{\top}Y_{G,w^{\prime}}v _{2}\geq v_{2}^{\top}(I+2xx^{\top})v_{2}-\delta=1+2\left\langle v_{2},x\right\rangle ^{2}-\delta\geq 1-\delta.\] (5)

Upper Bound.Through the optimization problem (*), a weight \(w^{\prime\prime}\) is calculated such that \(Y_{w^{\prime\prime}}\) are operator-norm upper-bounded by the constraint parameters. Let \(\mathsf{OPT}\) be the value of the optimal solution of the optimization problem (*). The desired uniform weight vector over the good samples \(w^{*}\in\Delta_{n,2\epsilon}\) is also a feasible solution to this optimization problem because \(Y_{w^{*}}\) satisfy the optimization constraints due to Lemma 4.2. Since \(w^{\prime\prime}\) is an \(\epsilon_{0}\)-approximation to the problem, we have

\[\left\|w^{\prime\prime}\right\|_{1}\geq(1-\epsilon_{0})\mathsf{OPT}\geq(1- \epsilon_{0})\left\|w^{*}\right\|_{1}=1-\epsilon_{0}\]

By optimization constraints, the Ky Fan \(2\)-norm of \(\sum_{i}w_{i}^{\prime\prime}B_{i}=\frac{1}{2}(1-\delta)Y_{w^{\prime\prime}}\leq 2\), and consequently,

\[\lambda_{1}+\lambda_{2}=\frac{1}{\left\|w^{\prime\prime}\right\|_{1}}\overline {\lambda}_{2}(Y_{w^{\prime\prime}})\leq\tfrac{4}{(1-\epsilon_{0})(1-\delta)}\enspace.\] (6)

By combining inequalities (4), (5), and (6), we have shown that the top two eigenvalues of \(Y_{w^{\prime}}\) are close to \(3\) and \(1\). Since the rest of the eigenvalues of \(Y_{w^{\prime}}\) can also be bounded, we can conclude that \(\left\|Y_{w^{\prime}}-\left(I+2xx^{\top}\right)\right\|_{2}=O(\delta)\).

We can now show the closeness between the top eigenvector of \(Y_{w^{\prime}}\) and the ground truth.

**Lemma 4.3**.: _There exists an universal constant \(\epsilon^{\prime}\) such that if \(0\leq\epsilon\leq\epsilon^{\prime}\), and Algorithm 1 receives in input an \(\epsilon\)-corrupted set of samples, then it outputs \(z_{1}\in\mathbb{R}^{d}\) such that with probability at least \(0.95\) it holds \(r(z_{1})\leq\frac{1}{8}\)._

Proof.: We condition on the fact that the event of Lemma 4.1 holds (with probability at least \(0.98\)). Let the eigendecomposition of \(Y_{w^{\prime}}\) be \(Y_{w^{\prime}}=\sum_{i\in[d]}\lambda_{i}v_{i}v_{i}^{\top}\), where \(\lambda_{1}\geq\ldots\geq\lambda_{d}\). Under the basis \(\{v_{1},\cdots,v_{d}\}\), the ground truth \(x\) can be represented as \(x=\sum_{i\in[d]}\alpha_{i}v_{i}\). Note that \(\left\|x\right\|_{2}^{2}=\sum_{i\in[d]}\alpha_{i}^{2}=1\). By Lemma 4.1, we have \(\left\|Y_{w^{\prime}}-(I+2xx^{\top})\right\|_{2}=O(\delta)\). Thus, we have

\[x^{\top}Y_{w^{\prime}}x \geq 3-O(\delta)\qquad\text{and}\] \[x^{\top}Y_{w^{\prime}}x \leq\lambda_{1}\alpha_{1}^{2}+\lambda_{2}(1-\alpha_{1}^{2})\leq (3+O(\delta))\alpha_{1}^{2}+(1+O(\delta))(1-\alpha_{1}^{2})\leq 1+2 \alpha^{2}+O(\delta).\]

This implies \(\alpha_{1}^{2}\geq 1-O(\delta)\). As a result,

\[r^{2}(v_{1}) =\left(\min\{\left\|v_{1}-x\right\|_{2}^{2},\left\|v_{1}+x\right\| _{2}^{2}\}\right)=\min\{(\alpha_{1}-1)^{2},(\alpha_{1}+1)^{2}\}+\sum_{i=2}^{d }\alpha_{i}^{2}\] \[=\min\{2-2\alpha_{1},2+2\alpha_{1}\}=O(\delta).\]

The last inequality holds as long as \(\delta\) is sufficiently small. Let \(z_{1}=\sum_{i\in[d]}\beta_{i}v_{i}\) be the unit vector approximating \(v_{1}\) returned by the algorithm. By Lemma 2.2, we have that \(z_{1}^{\top}Y_{w^{\prime}}z_{1}\geq(1-\overline{\delta})\lambda_{1}\) with probability at least \(0.99\). Thus, we have:

\[z_{1}^{\top}Y_{w^{\prime}}z_{1} \geq(1-\overline{\delta})\lambda_{1}\geq(1-\overline{\delta})(3-O (\delta))\geq 3-O(\delta+\overline{\delta})\qquad\text{and}\] \[z_{1}^{\top}Y_{w^{\prime}}z_{1} \leq\lambda_{1}\alpha_{1}^{2}+\lambda_{2}(1-\alpha_{1}^{2})\leq 1+2 \beta_{1}^{2}+O(\delta).\]

Again, this implies that \(\beta_{1}^{2}\geq 1-O(\delta+\overline{\delta})\). We can show that \(\min\{\left\|v_{1}-z_{1}\right\|_{2}^{2},\left\|v_{1}+z_{1}\right\|_{2}^{2}\}= O(\overline{\delta}+\delta)\). By the triangle inequality, we can conclude that \(r^{2}(z_{1})=O(\overline{\delta}+\delta)\leq 1/64\), where the last inequality is obtained by choosing sufficiently small \(\delta\) and \(\overline{\delta}\). Therefore, there exists an universal constant \(\epsilon^{\prime}\geq 0\) such that for all \(0\leq\epsilon\leq\epsilon^{\prime}\), Algorithm 1 takes \(n=\widetilde{\Omega}(d)\) samples and outputs \(z_{1}\) such that \(r(z_{1})\leq 1/8\) with probability at least \(0.95\). 

**Lemma 4.4**.: _Algorithm 1 runs in time \(\widetilde{O}(nd)\)._

Proof of Lemma 4.4.: Since we have the factorization of the rank-two matrices \(A_{i}\) and rank-one matrices \(B_{i}\) for all \(i=1,2,\ldots,n\), and the time to perform a matrix-vector product with \(C_{i}\) and \(D_{i}\) is \(O(d)\). Therefore, by Lemma 2.1, with \(t_{C}\) and \(t_{D}\) to be \(\widetilde{O}(nd)\), Line 3 runs in \(\widetilde{O}(nd)\) time. In Line 5, by Lemma 2.2, the top eigenvector of \(Y_{w^{\prime}}\) can be computed in \(\widetilde{O}(n\log d)\) time using power method. Scaling in Line 4 runs in \(O(n)\) time. As a result, Algorithm 1 runs in \(\widetilde{O}(nd)\) time. 

We can directly combine Lemma 4.3 and Lemma 4.4 to finish the proof of Lemma 3.2.

## 5 Robust Gradient Descent

After the robust spectral initialization in Section 4, we have an initial guess \(z_{1}\in\mathbb{R}^{d}\) that is close to the ground truth \(x\) or \(-x\). Without loss of generality, we can assume that \(z_{1}\) is closer to \(x\) than to \(-x\). In this section, we prove Lemma 3.3: Given an initial guess \(z_{1}\) with \(\left\|z_{1}-x\right\|_{2}\leq 1/8\), we can use a robust gradient descent algorithm (Algorithm 2) to recover \(x\) to any desire precision \(\Delta>0\). It is well-known that gradient descent can achieve geometric convergence rates in non-adversarial settings. We show that Algorithm 2 achieves a similar convergence rate even when the input is \(\epsilon\)-corrupted.

Consider the natural nonconvex formulation: \(\min_{z\in\mathbb{R}^{d}}\,\sum_{i=1}^{n}f_{i}(z)\) where \(f_{i}(z)\doteq\left(\langle a_{i},z\rangle^{2}-y_{i}\right)^{2}.\) Let \(g_{i}\) denote the gradient of \(f_{i}\) with respect to \(z\). Let \(D_{z}\) denote the distribution of \(g_{i}(z)\in\mathbb{R}^{d}\) when there is no adversarial corruption. Formally, \(g(z)\sim\mathcal{D}_{z}\) is distributed as

\[g(z)=\frac{\partial}{\partial z}\left[\left(\langle a,z\rangle^{2}-\langle a,x \rangle^{2}\right)^{2}\right]=-4\left(\langle a,z\rangle^{2}-\langle a,x \rangle^{2}\right)\langle a,z\rangle a\text{ where }\ a\sim\mathcal{N}(0,I)\enspace.\] (7)To run gradient descent, we want to estimate the _expected true gradient_\(\mu_{z}\doteq\mathbb{E}\,g(z)\) using samples. The challenge is that the input samples \(\{(a_{i},y_{i})\}_{i\in[n]}\) are \(\epsilon\)-corrupted, and consequently the gradients \(\{g_{i}(z)\}_{i\in[n]}\) is an \(\epsilon\)-corrupted set of vectors drawn from \(\mathcal{D}_{z}\). To address this, we use robust mean estimation algorithms (e.g., [16]) to approximate \(\mu_{z}\), the true mean of \(\mathcal{D}_{z}\).

```
0:\(\epsilon>0\), an \(\epsilon\)-corrupted set of \(n\) samples \(\{(a_{i},y_{i})\}_{i\in[n]}\), initial guess \(z_{1}\in\mathbb{R}^{d}\) with \(\left\|z_{1}-x\right\|\leq 1/8\), and desired precision \(\Delta>0\).
0:\(z\in\mathbb{R}^{d}\) such that \(\left\|z-x\right\|_{2}\leq\Delta\) where \(x\) is the ground truth.
1:procedureRobustGD(\(\epsilon,\{(a_{i},y_{i})\}_{i\in[n]},z_{1},\Delta\))
2:\(T\gets O(\log(1/\Delta))\), \(\eta\gets 1/300\)
3:\(\{N_{1},N_{2},\cdots,N_{T}\}\leftarrow\) a random disjoint partition of \([n]\) such that \(\left|N_{t}\right|=n/T\) for all \(t\in[T]\)
4:for\(t=1,2,\ldots,T\)do
5:\(\widehat{\mu}_{z_{t}}\leftarrow\) Robust mean estimation on input \(\{g_{i}(z_{t})\}_{i\in N_{t}}\) using Lemma 5.2
6:\(z_{t+1}\gets z_{t}-\eta\,\widehat{\mu}_{z_{t}}\)
7:endfor
8:return\(z_{T+1}\)
9:endprocedure ```

**Algorithm 2** Robust Gradient Descent

The error guarantee of robust mean estimation algorithms depends on the covariance matrix \(\Sigma_{z}\) of the distribution \(\mathcal{D}_{z}\). The next lemma upper bounds the spectral norm of \(\Sigma_{z}\).

**Lemma 5.1**.: _Let \(\mathcal{D}_{z}\) be the distribution of gradients at \(z\) as defined in Equation (7). For any \(z\in\mathbb{R}^{d}\) with \(\left\|z-x\right\|_{2}\leq 1\), the covariance matrix \(\Sigma_{z}\) of \(\mathcal{D}_{z}\) satisfies \(\Sigma_{z}\preceq O(\left\|z-x\right\|_{2}^{2})I\)._

The proof of Lemma 5.1 is deferred to Appendix B. Given Lemma 5.1, we can show that robust mean estimation algorithms can approximate \(\mu_{z}\) with small error. For technical reasons, we randomly partition the input samples \((a_{i},y_{i})\) into \(T\) subsets, and use one subset in each iteration. With high probability, each partition has at most \((2\epsilon)\)-fraction of corrupted samples

**Lemma 5.2**.: _Consider any \(z\in\mathbb{R}^{d}\) with \(\left\|z-x\right\|_{2}\leq 1\). Let \(\mu_{z}\) be the mean of \(\mathcal{D}_{z}\) as defined in Equation (7). Fix universal constants \(c>0\) and \(\epsilon_{0}=\Theta(c^{2})\). Let \(2\epsilon<\epsilon_{0}\) and \(\delta>0\). Given a \((2\epsilon)\)-corrupted set of \(m=\Omega(d\log d/\delta)\) samples drawn from \(\mathcal{D}_{z}\), we can compute \(\widehat{\mu}_{z}\) in time \(\widetilde{O}(md\log(1/\delta))\) such that \(\left\|\widehat{\mu}_{z}-\mu_{z}\right\|_{2}\leq c\left\|z-x\right\|_{2}\) with probability at least \(1-O(\delta)\)._

Proof of Lemma 5.2.: Since \(2\epsilon<\epsilon_{0}\), we can view the \((2\epsilon)\)-corrupted set of \(m\) samples as \(\epsilon_{0}\)-corrupted. We need to replace \(2\epsilon\) with \(\epsilon_{0}\) to reduce the failure probability of Lemma 2.3. This weakens the error guarantee of Lemma 2.3, but the resulting \(\widehat{\mu}_{z}\) is still accurate enough for our algorithm.

We apply Lemma 2.3 to the \(\epsilon_{0}\)-corrupted set of \(m\) vectors drawn from \(\mathcal{D}_{z}\). By Lemma 5.1, the covariance matrix of \(\mathcal{D}_{z}\) satisfies \(\Sigma_{z}\preceq O(\left\|z-x\right\|_{2}^{2})I\). Consequently, for sufficiently large \(m=\Theta(d\log d/\delta)\) and sufficiently small \(\epsilon_{0}=O(c^{2})\), the error guarantee of Lemma 2.3 is \(O\left(\sqrt{\epsilon_{0}}+\sqrt{\frac{d}{m\delta}}+\sqrt{\frac{d(\log d+\log(1 /\delta))}{m}}\right)\left\|z-x\right\|_{2}\leq c\left\|z-x\right\|_{2}\). The success probability is at least \(1-\delta-\exp(-\epsilon_{0}m)=1-O(\delta)\). 

Lemma 5.2 shows that even with a \((2\epsilon)\)-corrupted set of gradients, the true gradient \(\mu_{z}\) can be estimated up to an additive error proportional to the distance between \(z\) and \(x\). The next lemma shows that such an approximate gradient is sufficient for gradient descent to converge, reducing the distance to the ground truth \(x\) by a constant factor in each iteration.

**Lemma 5.3**.: _Suppose in iteration \(t\) of Algorithm 2, the current solution \(z_{t}\) satisfies \(\left\|z_{t}-x\right\|_{2}\leq 1/8\), and the estimated gradient \(\widehat{\mu}_{z_{t}}\in\mathbb{R}^{d}\) satisfies \(\left\|\widehat{\mu}_{z_{t}}-\mu_{z_{t}}\right\|_{2}\leq c\left\|z_{t}-x\right\|_ {2}\) for \(c=4\). Then, we have \(\left\|z_{t+1}-x\right\|_{2}^{2}\leq 0.99\left\|z_{t}-x\right\|_{2}^{2}\)._

Proof Sketch of Lemma 5.3.: We provide a proof sketch and defer the full proof to Appendix B. Our objective function is nonconvex (even with infinitely many samples and no corruption). However, when the starting point \(z_{1}\) is close to a global optimum, it is well-known that gradient descent is well-behaved. More specifically, for any \(z\) close to the ground truth \(x\), we can show that the (expected) true gradient \(\mu_{z}\) aligns with the direction toward \(x\):

\[\left\langle\mu_{z},z-x\right\rangle\geq 7.5\left\|z-x\right\|_{2}^{2}\text{ and } \left\|\mu_{z}\right\|_{2}\leq 29\left\|z-x\right\|_{2}\text{,}\]

which is sufficient for proving geometric convergence. We can immediately see that this argument is robust to additive error in \(\mu_{z}\) that is proportional to \(\left\|z-x\right\|_{2}\). When \(\left\|\widehat{\mu}_{z}-\mu_{z}\right\|_{2}\leq c\left\|z-x\right\|_{2}\),

\[\left\langle\widehat{\mu}_{z},z-x\right\rangle\geq(7.5-c)\left\|z_{t}-x\right\| _{2}^{2}\text{ and }\text{ }\left\|\widehat{\mu}_{z_{t}}\right\|_{2}\leq(29+c)\left\|z_{t}-x\right\|_{2 }\text{.}\]

When \(c<7.5\), we can choose an appropriate step size \(\eta\) such that the distance between \(z_{t}\) and \(x\) decreases by a constant factor in each iteration. 

We are now ready to prove Lemma 3.3, which states the performance guarantee, sample complexity, runtime, and success probability of Algorithm 2. We restate Lemma 3.3 before proving it.

**Lemma 3.3** (Robust Gradient Descent).: _Consider the setting of Problem 1.3. Let \(\Delta>0\) be the desired precision. Let \(0<\epsilon<\epsilon_{0}\) for some sufficiently small universal constant \(\epsilon_{0}\). Given an \(\epsilon\)-corrupted set of \(n=\widetilde{\Omega}(d\log^{2}(1/\Delta))\) samples and an initial guess \(z_{1}\) such that \(r(z_{1})=\min(\left\|z_{1}-x\right\|_{2},\left\|z_{1}+x\right\|_{2})\leq 1/8\), we can compute a vector \(z\in\mathbb{R}^{d}\) in time \(\widetilde{O}(nd)\) such that \(r(z)\leq\Delta\) with probability at least \(0.95\)._

Proof of Lemma 3.3.: First, we prove the success probability of Algorithm 2. Algorithm 2 can fail in two ways: _(i)_ if some \(N_{t}\) has more than \((2\epsilon)\)-fraction of corrupted samples, or _(ii)_ if Lemma 5.2 fails in some iteration \(t\). The probability of event _(i)_ is at most \(0.01\) for our choice of \(n\), which follows from a standard application of Hoeffding's inequality and a union bound over \(T\) iterations. For event _(ii)_, we choose a sufficiently small \(\delta=O(1/T)\) in Lemma 5.2, so each robust gradient estimation fails with probability at most \(O(\delta)=0.01/T\), and overall the probability of event _(ii)_ is at most \(0.01\). For the rest of the proof, we assume these bad events do not happen.

Next, we show the correctness of Algorithm 2. Without loss of generality, we can assume that \(z_{1}\) is closer to the ground truth \(x\) than to \(-x\), which implies \(\left\|z_{1}-x\right\|_{2}\leq 1/8\). By Lemma 5.2, we can obtain an approximation \(\widehat{\mu}_{z_{1}}\) of the true gradient \(\mu_{z_{1}}\) at \(z_{1}\) such that \(\left\|\widehat{\mu}_{z_{1}}-\mu_{z_{1}}\right\|_{2}\leq c\left\|z_{1}-x \right\|_{2}\). Then by Lemma 5.3, we know that \(\left\|z_{2}-x\right\|_{2}\leq 0.99\left\|z_{1}-x\right\|_{2}\) after one step of gradient descent. We can iteratively apply these two lemmas to show that, after \(T=O(\log(1/\Delta))\) iterations, we have \(\left\|z_{T+1}-x\right\|_{2}\leq\Delta\). One technical issue is that in iteration \(t\), we need to use a fresh subset of samples \(N_{t}\). By the principle of deferred decisions, we can view \((a_{i},y_{i})_{i\in N_{t}}\) as being generated (and corrupted) after \(z_{t}\) is chosen, which forms a \((2\epsilon)\)-corrupted set of gradients at \(z_{t}\).

Finally, we analyze the sample complexity and runtime of Algorithm 2. Algorithm 2 requires in total \(n=mT=\Omega(d\log d\log^{2}(1/\Delta))\) samples. A random partition can be computed in \(O(n)\) time by shuffling the input. In each iteration, the \(m\) gradients in \(N_{t}\) can be computed using Equation (7) in time \(O(md)\), and \(z_{t}\) can be updated in time \(O(d)\). By Lemma 5.2, the true gradient can be robustly estimated in time \(\widetilde{O}(md\log T)\) = \(\widetilde{O}(md\log\log(1/\Delta))\). The overall runtime of the algorithm is \(\widetilde{O}(n+(md\log\log(1/\Delta))T)=\widetilde{O}(nd\log\log(1/\Delta))= \widetilde{O}(nd)\). 

## 6 Conclusions and Future Directions

In this paper, our main conceptual contribution is to propose and study the outlier-robust phase retrieval problem, where a constant fraction of the input data is corrupted. Notably, we allow corruption in both the sampled frequencies \(a_{i}\in\mathbb{R}^{d}\) and the intensity measurements \(y_{i}\in\mathbb{R}\). Our main technical contribution is the design and analysis of a near-sample-optimal and nearly-linear-time algorithm that solves this problem with provably guarantees.

An immediate technical question is whether our sample complexity can be tightened by removing some \(\log(1/\Delta)\) factors. One potential approach is to open robust mean estimation algorithms instead of using them in a black-box manner. One could examine the stability conditions that these algorithms require, and see if these stability conditions can be proved without partitioning the samples and using fresh samples in each iteration. More broadly, we believe our framework can be used to develop outlier-robust algorithms for other tractable nonconvex problems, by first finding an initial solution in a saddle-free region near a global optimum and then converging to this global optimum using robust gradient descent.

## References

* Bandeira et al. [2016] A. S. Bandeira, N. Boumal, and V. Voroninski. On the low-rank approach for semidefinite programs arising in synchronization and community detection. In _Conference on learning theory_, pages 361-382. PMLR, 2016.
* Bhojanapalli et al. [2016] S. Bhojanapalli, B. Neyshabur, and N. Srebro. Global optimality of local search for low rank matrix recovery. _Advances in Neural Information Processing Systems_, 29, 2016.
* Candes et al. [2015a] E. J. Candes, X. Li, and M. Soltanolkotabi. Phase retrieval via wirtinger flow: Theory and algorithms. _IEEE Trans. Inf. Theory_, 61(4):1985-2007, 2015a.
* Candes et al. [2015b] E. J. Candes, X. Li, and M. Soltanolkotabi. Phase retrieval from coded diffraction patterns. _Applied and Computational Harmonic Analysis_, 39(2):277-299, Sept. 2015b. ISSN 1063-5203.
* Candes et al. [2015c] E. J. Candes, Y. C. Eldar, T. Strohmer, and V. Voroninski. Phase retrieval via matrix completion. _SIAM Rev._, 57(2):225-251, 2015c.
* Chen et al. [2018] J. Chen, L. Wang, X. Zhang, and Q. Gu. Robust Wirtinger Flow for Phase Retrieval with Arbitrary Corruption, Jan. 2018.
* Chen and Candes [2015] Y. Chen and E. Candes. Solving Random Quadratic Systems of Equations Is Nearly as Easy as Solving Linear Systems. In _Advances in Neural Information Processing Systems_, volume 28. Curran Associates, Inc., 2015.
* Cheng and Ge [2018] Y. Cheng and R. Ge. Non-convex matrix completion against a semi-random adversary. In _Conference On Learning Theory_, pages 1362-1394. PMLR, 2018.
* Cheng et al. [2019] Y. Cheng, I. Diakonikolas, and R. Ge. High-dimensional robust mean estimation in nearly-linear time. In _Proceedings of the 30th ACM-SIAM Symposium on Discrete Algorithms (SODA)_, pages 2755-2771. SIAM, 2019.
* Cherapanamjeri et al. [2020] Y. Cherapanamjeri, S. Mohanty, and M. Yau. List decodable mean estimation in nearly linear time. In S. Irani, editor, _61st IEEE Annual Symposium on Foundations of Computer Science, FOCS_, pages 141-148. IEEE, 2020.
* Dainty and Fienup [1987] C. Dainty and J. R. Fienup. Phase retrieval and image reconstruction for astronomy. _Image recovery: theory and application_, 231:275, 1987.
* Diakonikolas and Kane [2023] I. Diakonikolas and D. M. Kane. _Algorithmic High-Dimensional Robust Statistics_. Cambridge University Press, 2023.
* Diakonikolas et al. [2016] I. Diakonikolas, G. Kamath, D. M. Kane, J. Li, A. Moitra, and A. Stewart. Robust estimators in high dimensions without the computational intractability. In _57th Annual IEEE Symposium on Foundations of Computer Science--FOCS 2016_, pages 655-664. IEEE Computer Soc., Los Alamitos, CA, 2016.
* Diakonikolas et al. [2019] I. Diakonikolas, G. Kamath, D. Kane, J. Li, J. Steinhardt, and A. Stewart. Sever: A robust meta-algorithm for stochastic optimization. In K. Chaudhuri and R. Salakhutdinov, editors, _Proceedings of the 36th International Conference on Machine Learning_, volume 97 of _Proceedings of Machine Learning Research_, pages 1596-1606. PMLR, 09-15 Jun 2019.
* Dong et al. [2019] Y. Dong, S. Hopkins, and J. Li. Quantum Entropy Scoring for Fast Robust Mean Estimation and Improved Outlier Detection. In _Advances in Neural Information Processing Systems_, volume 32. Curran Associates, Inc., 2019.
* Dong et al. [2019] Y. Dong, S. B. Hopkins, and J. Li. Quantum entropy scoring for fast robust mean estimation and improved outlier detection. In _Proc. 33rd Advances in Neural Information Processing Systems (NeurIPS)_, 2019.
* Fienup [1978] J. R. Fienup. Reconstruction of an object from the modulus of its Fourier transform. _Optics Letters_, 3(1):27-29, July 1978. ISSN 1539-4794.
* Fienup [1982] J. R. Fienup. Phase retrieval algorithms: A comparison. _Applied Optics_, 21(15):2758-2769, Aug. 1982. ISSN 2155-3165.

* [19] X. Gao and Y. Cheng. Robust matrix sensing in the semi-random model. _Proceedings of the 37th Conference on Neural Information Processing Systems (NeurIPS)_, 2023.
* [20] R. Ge, F. Huang, C. Jin, and Y. Yuan. Escaping from saddle points--online stochastic gradient for tensor decomposition. In _Conference on Learning Theory_, pages 797-842, 2015.
* [21] R. Ge, J. D. Lee, and T. Ma. Matrix completion has no spurious local minimum. In _Advances in Neural Information Processing Systems_, pages 2973-2981, 2016.
* [22] R. W. Gerchberg. A practical algorithm for the determination of phase from image and diffraction plane pictures. _Optik_, Jan. 1972.
* [23] F. R. Hampel, E. M. Ronchetti, P. J. Rousseeuw, and W. A. Stahel. _Robust statistics. The approach based on influence functions_. Wiley New York, 1986.
* [24] P. Hand and V. Voroninski. Corruption robust phase retrieval via linear programming. _CoRR_, abs/1612.03547, 2016.
* [25] P. J. Huber and E. M. Ronchetti. _Robust statistics_. Wiley New York, 2009.
* [26] K. Jaganathan, Y. C. Eldar, and B. Hassibi. Phase retrieval: An overview of recent developments. _Optical Compressive Imaging_, pages 279-312, 2016.
* [27] R. Kolte and A. Ozgur. Phase Retrieval via Incremental Truncated Wirtinger Flow, June 2016.
* [28] K. A. Lai, A. B. Rao, and S. Vempala. Agnostic estimation of mean and covariance. In _focs2016_, pages 665-674, 2016.
* [29] G. Lecue, M. Lerasle, and T. Mathieu. Robust classification via MOM minimization. _Mach. Learn._, 109(8):1635-1665, 2020.
* [30] S. Li, Y. Cheng, I. Diakonikolas, J. Diakonikolas, R. Ge, and S. Wright. Robust second-order nonconvex optimization and its application to low rank matrix sensing. In _Proc. 37th Advances in Neural Information Processing Systems (NeurIPS)_, 2023.
* [31] J.-W. Liu, Z.-J. Cao, J. Liu, X.-L. Luo, W.-M. Li, N. Ito, and L.-C. Guo. Phase Retrieval via Wirtinger Flow Algorithm and Its Variants. In _2019 International Conference on Machine Learning and Cybernetics (ICMLC)_, pages 1-9, July 2019.
* [32] J. Miao, T. Ishikawa, B. Johnson, E. H. Anderson, B. Lai, and K. O. Hodgson. High resolution 3D X-ray diffraction microscopy. _Physical review letters_, 89(8):088303, 2002.
* [33] R. P. Millane. Phase retrieval in crystallography and optics. _JOSA A_, 7(3):394-411, 1990.
* [34] P. Netrapalli, P. Jain, and S. Sanghavi. Phase retrieval using alternating minimization. In _Proc. 27th Advances in Neural Information Processing Systems (NeurIPS)_, pages 2796-2804, 2013.
* [35] A. Prasad, A. S. Suggala, S. Balakrishnan, and P. Ravikumar. Robust estimation via robust gradient estimation. _Journal of the Royal Statistical Society. Series B. Statistical Methodology_, 82(3):601-627, 2020.
* [36] W. H. Robert. Phase problem in crystallography. _JOSA a_, 10(5):1046-1055, 1993.
* [37] Y. Shechtman, Y. C. Eldar, O. Cohen, H. N. Chapman, J. Miao, and M. Segev. Phase retrieval with application to optical imaging: a contemporary overview. _IEEE signal processing magazine_, 32(3):87-109, 2015.
* [38] M. Soltanolkotabi. Structured signal recovery from quadratic measurements: Breaking sample complexity barriers via nonconvex optimization. _IEEE Trans. Inf. Theory_, 65(4):2374-2400, 2019.
* [39] J. Sun, Q. Qu, and J. Wright. Complete dictionary recovery over the sphere i: Overview and the geometric picture. _IEEE Trans. Inf. Theor._, 63(2):853-884, 2 2017.
* [40] J. Sun, Q. Qu, and J. Wright. A geometric analysis of phase retrieval. _Found. Comput. Math._, 18(5):1131-1198, 2018.

* Trevisan [2017] L. Trevisan. Lecture notes on graph partitioning, expanders and spectral methods. _University of California, Berkeley, https://people. eecs. berkeley. edu/luca/books/expanders-2016. pdf_, 2017.
* Tukey [1975] J. Tukey. Mathematics and picturing of data. In _Proceedings of ICM_, volume 6, pages 523-531, 1975.
* Voroninski [2013] V. Voroninski. _PhaseLift: A Novel Methodology for Phase Retrieval_. PhD thesis, UC Berkeley, 2013.
* Wright and Ma [2022] J. Wright and Y. Ma. _High-dimensional data analysis with low-dimensional models: Principles, computation, and applications_. Cambridge University Press, 2022.
* Zhang et al. [2016] H. Zhang, Y. Chi, and Y. Liang. Provable non-convex phase retrieval with outliers: Median truncated wirtinger flow. In _International conference on machine learning_, pages 1022-1031. PMLR, 2016.

Omitted Proofs in Section 4

**Lemma A.1**.: _[Lemma 4.2, Formal]. For any \(\delta_{0}>0\), there exists constants \(\epsilon_{0},c>0\) such that when \(n>c\cdot d\log d\) and we are given a set of \(n\)\(\epsilon\)-corrupted samples, where \(0\leq\epsilon\leq\epsilon_{0}\), then with probability at least \(0.98\), it holds_

\[\forall w\in\Delta_{n,2\epsilon},\big{\|}Y_{G,w}-(I+2xx^{\top})\big{\|}_{2} \leq\delta_{0}\,.\] (8)

Proof of Lemma a.1.: We recall the definition of \(Y_{G,w}=\sum_{i\in G}w_{i}y_{i}a_{i}a_{i}^{\top}\). Let \(\ell=\epsilon\cdot n\) and let \(\{(a_{n+i},y_{n+i})\}_{i=1}^{\ell}\) be the set of samples that were removed by the \(\epsilon\)-corruption adversary. Let \(G^{\prime}=G\cup\{n+1,\ldots,n+\ell\}\), \(n^{\prime}=n+\ell\), and \(\epsilon^{\prime}=\epsilon/(1+\epsilon)\). Note that without loss of generality, we can assume that \(|G|=(1-\epsilon)n\) and \(|G^{\prime}|=(1-\epsilon^{\prime})n^{\prime}=n\).

We define a mapping \(\sigma:\Delta_{n,2\epsilon}\rightarrow\Delta_{n^{\prime},3\epsilon^{\prime}}\) such that

\[\sigma(w)_{i}=\begin{cases}w_{i}&i\in[n]\\ 0&\text{otherwise}\end{cases}.\] (9)

In other words, all the weights are the same for the samples with index in the set \([n]\), and are equal to \(0\) for the samples removed by the adversary. We can verify that \(\sigma(w)\in\Delta_{n^{\prime},3\epsilon^{\prime}}\) for all \(w\in\Delta_{n,2\epsilon}\) since \(\sigma(w)_{i}\leq w_{i}\leq 1/(1-2\epsilon)n=1/(1-3\epsilon^{\prime})n^{\prime}\) for all \(i\in[n^{\prime}]\), and \(\left\lVert\sigma(w)\right\rVert_{1}=\left\lVert w\right\rVert_{1}=1\). Furthermore, we have \(Y_{G,w}=Y_{G^{\prime},\sigma(w)}\) for all \(w\in\Delta_{n,2\epsilon}\). We denote with \(w^{*}\in\Delta_{n^{\prime},3\epsilon^{\prime}}\) the desired uniform weighting of the samples with index in \(G^{\prime}\), i.e., \(w_{i}^{*}=\frac{1}{(1-\epsilon^{\prime})n^{\prime}}\mathbb{I}_{i\in G^{\prime}}\).

It suffices to show both \(\big{\|}Y_{G^{\prime},w^{*}}-(I+2xx^{\top})\big{\|}_{2}\leq\delta_{0}/2\) and \(\big{\|}Y_{G^{\prime},\sigma(w)-w^{*}}\big{\|}_{2}\leq\delta_{0}/2\).

By triangle inequality, for any \(w\in\Delta_{n,2\epsilon}\), it holds

\[\big{\|}Y_{G^{\prime},\sigma(w)}-(I+2xx^{\top})\big{\|}_{2}\leq\big{\|}Y_{G^{ \prime},w^{*}}-(I+2xx^{\top})\big{\|}_{2}+\big{\|}Y_{G^{\prime},\sigma(w)-w^{ *}}\big{\|}_{2}\,,\]

Thus, it suffices to show both \(\big{\|}Y_{G^{\prime},w^{*}}-(I+2xx^{\top})\big{\|}_{2}\leq\delta_{0}/2\) and \(\big{\|}Y_{G^{\prime},\sigma(w)-w^{*}}\big{\|}_{2}\leq\delta_{0}/2\).

We upper bound the first term. By using the definition of \(w^{*}\), note that

\[\big{\|}Y_{G^{\prime},\sigma(w)}-(I+2xx^{\top})\big{\|}_{2} =\left\lVert\sum_{i\in G^{\prime}}w_{i}^{*}y_{i}a_{i}a_{i}^{\top} -(I+2xx^{\top})\right\rVert_{2}\] \[=\left\lVert\sum_{i\in G^{\prime}}\frac{1}{|G^{\prime}|}y_{i}a_{i} a_{i}^{\top}-(I+2xx^{\top})\right\rVert_{2}\,.\]

Since \(\mathbb{E}(y_{i}a_{i}a_{i}^{\top})=I+2xx^{\top}\) for any \(i\in G^{\prime}\), we can use a concentration inequality to upper bound this term. By Lemma A.2, as long as \(n\geq c_{1}(\delta_{0}/2)\cdot d\log d\), with probability at least \(0.995\), we have

\[\big{\|}Y_{G^{\prime},w^{*}}-(I+2xx^{\top})\big{\|}_{2}\leq\delta_{0}/2\enspace.\] (10)

It remains to show a high-probability upper bound to the second term \(\big{\|}Y_{G^{\prime},w^{*}-\sigma(w)}\big{\|}_{2}\leq\delta_{0}/2\). The first observation is that the weighting \(w^{*}\) and \(\sigma(w)\) for any \(w\in\Delta_{n,2\epsilon}\) cannot too different. In particular, we can show the following upper bound:

\[\sum_{i\in G^{\prime}}|w_{i}^{*}-\sigma(w)_{i}|\leq\sum_{i=1}^{n^{\prime}}|w_{ i}^{*}-\sigma(w)_{i}|\leq\sup_{w,w^{\prime}\in\Delta_{n^{\prime},3\epsilon^{ \prime}}}\sum_{i=1}^{n^{\prime}}|w_{i}-w_{i}^{\prime}|\] (11)

We observe that \(\Delta_{n^{\prime},3\epsilon^{\prime}}\) can be seen as the convex combination of all possible uniform weighting over subsets of \(n^{\prime}(1-3\epsilon^{\prime})\) samples. Thus, the maximum distance will be between two points of the convex hull, and we can upper bound (11) as:

\[\sum_{i\in G^{\prime}}|w_{i}^{*}-\sigma(w)_{i}|\leq\sup_{w,w^{\prime}\in\Delta_{ n^{\prime},3\epsilon^{\prime}}}\sum_{i=1}^{n^{\prime}}|w_{i}-w_{i}^{\prime}|\leq \frac{6\epsilon^{\prime}n}{n^{\prime}(1-3\epsilon^{\prime})}\leq 6\epsilon\enspace.\] (12)For a fixed unit vector \(z\in\mathbb{S}^{d-1}\) with \(z=px+qu\) where \(u\in\mathbb{S}^{d-1}\) and \(\left\langle u,x\right\rangle=0\), we have:

\[\max_{w\in\Delta_{n,2\epsilon}}\left|z^{\top}Y_{G^{\prime},w^{*}- \sigma(w)}z\right| =\max_{w}\left|\sum_{i\in G^{\prime}}(w_{i}^{*}-\sigma(w)_{i}) \left\langle a_{i},x\right\rangle^{2}\left\langle a_{i},z\right\rangle^{2}\right|\] \[=\max_{w}\left|\sum_{i\in G^{\prime}}(w_{i}^{*}-\sigma(w)_{i}) \left\langle a_{i},x\right\rangle^{2}(p\left\langle a_{i},x\right\rangle+q \left\langle a_{i},u\right\rangle)^{2}\right|\] \[\leq 2\max_{w}\left|\sum_{i\in G^{\prime}}(w_{i}^{*}-\sigma(w)_{i })(\left\langle a_{i},x\right\rangle^{4}+\left\langle a_{i},x\right\rangle^{2} \left\langle a_{i},u\right\rangle^{2})\right|\] \[\leq 2\max_{w}\sum_{i\in G^{\prime}}\left|w_{i}^{*}-\sigma(w)_{i }\right|\left\langle a_{i},x\right\rangle^{4}\] \[\quad+2\max_{w}\sum_{i\in G^{\prime}}\left|w_{i}^{*}-\sigma(w)_{ i}\right|\left\langle a_{i},x\right\rangle^{2}\left\langle a_{i},u\right\rangle^{2} \enspace.\]

For ease of notation, let \(\beta_{i}\doteq|w_{i}^{*}-\sigma(w)_{i}|\). Observe that \(0\leq\beta_{i}\leq\frac{1}{(1-2\epsilon)n}\) for all \(i\), and \(\sum_{i}\beta_{i}\leq 6\epsilon\) due to (11). We have:

\[\max_{w\in\Delta_{n,2\epsilon}}\left|z^{\top}Y_{G^{\prime},w^{*}- \sigma(w)}z\right| \leq 2\max_{\beta:\sum_{i\in G^{\prime}}\beta_{i}\leq 6\epsilon}\max_{0\leq\beta_{i}\leq \frac{1}{(1-2\epsilon)n}}\sum_{i\in G^{\prime}}\beta_{i}\left\langle a_{i},x \right\rangle^{4}\] \[\leq \frac{2}{(1-2\epsilon)n}\max_{L\subseteq G^{\prime},|L|=6 \epsilon n}\sum_{i\in L}\left\langle a_{i},x\right\rangle^{4}\] \[\quad+\frac{2}{(1-2\epsilon)n}\max_{L\subseteq G^{\prime},|L|=6 \epsilon n}\sum_{i\in L}\left\langle a_{i},x\right\rangle^{2}\left\langle a_{ i},u\right\rangle^{2}.\] (13)

Inequality (13) follows by assigning the maximum possible \(\beta_{i}\) to the largest entries of the sum until we hit the budget \(6\epsilon\) due to (11).

We bound \(\max_{L}\sum_{i\in L}\left\langle a_{i},x\right\rangle^{4}\) first. Let \(X_{i}=\left\langle a_{i},x\right\rangle\sim\mathcal{N}(0,1)\) for \(i\in G^{\prime}\), and define the threshold function \(h_{r}(z)=\begin{cases}0,&z\leq r\\ z,&z>r\end{cases}\) for \(r=C^{2}\cdot\ln^{2}(1/\epsilon)\) with constant \(C>0\) to be determined. Note that \(z\leq r+h_{r}(z)\) for all \(z>0\). Therefore,

\[\max_{L\subseteq G^{\prime},|L|=6\epsilon n}\frac{1}{n}\sum_{i\in L }X_{i}^{4} \leq\max_{L}\frac{1}{n}\sum_{i\in L}r+\max_{L}\frac{1}{n}\sum_{i\in L}h_{r} (X_{i}^{4})\] \[\leq 6\epsilon r+\frac{1}{n}\sum_{i\in G^{\prime}}h_{r}(X_{i}^{4}).\]

Then, we consider to bound \(\exp\left(\sum_{i\in G^{\prime}}c\cdot h_{r}(X_{i}^{4})\right)\) for some \(c>0\) to be determined. For any \(i\in G^{\prime}\) and \(z\geq 1\), with \(C=6\), for all \(\epsilon>0\), we have

\[\mathbf{Pr}\left[\exp\left(c\cdot h_{r}(X_{i}^{4})\right)\geq z \right]\leq\mathbf{Pr}\left[h_{r}(X_{i}^{4})\geq 0\right]=\mathbf{Pr}\left[X_{i} \geq r^{1/4}\right] \leq\exp(-\sqrt{r}/2)\] \[\leq\exp(\ln\epsilon\cdot C/2)\] \[\leq\epsilon^{3}.\]

At the same time, with \(c<1/200\), for all \(z\geq 1\), we have

\[\mathbf{Pr}\left[\exp\left(c\cdot h_{r}(X_{i}^{4})\right)\geq z \right] \leq\mathbf{Pr}\left[\exp(c\cdot X_{i}^{4})\geq z\right] \leq\mathbf{Pr}\left[X_{i}^{4}\geq\frac{\ln z}{c}\right]\] \[\leq\exp\left(-\sqrt{\frac{\ln z}{c}}/2\right)\] \[\leq z^{-3}.\]Therefore, \(\mathbf{Pr}\left[\exp\left(c\cdot h_{r}(X_{i}^{4})\right)\geq z\right]\leq\min\{z^{- 3},\epsilon^{3}\}\), and for all \(\epsilon<1/2\), we have

\[\mathbf{E}\left[\exp\left(c\cdot h_{r}(X_{i}^{4})\right)\right] =\int_{0}^{\infty}\mathbf{Pr}\left[\exp\left(c\cdot h_{r}(X_{i}^{ 4})\right)\geq z\right]\mathrm{d}z\] \[\leq\int_{0}^{1}1\mathrm{d}z+\int_{1}^{1/\epsilon}\epsilon^{3} \mathrm{d}z+\int_{1/\epsilon}^{\infty}z^{-3}\mathrm{d}z\] \[=1+\epsilon^{2}-\epsilon^{3}+\frac{1}{2}\epsilon^{2}\] \[\leq 1+\epsilon^{2}\] \[\leq\exp(\epsilon^{2}).\]

Since \(\{X_{i}\}_{i\in G^{\prime}}\) are independent, we have

\[\mathbf{E}\left[\exp\left(\sum_{i\in G^{\prime}}c\cdot h_{r}(X_{i}^{4})\right) \right]=\mathbf{E}\left[\prod_{i\in G^{\prime}}\exp\left(c\cdot h_{r}(X_{i}^{4 })\right)\right]\leq\exp(\epsilon^{2}n).\]

Finally, by Markov's inequality, for any constant \(\delta_{1}>0\), as long as \(\epsilon\leq\sqrt{\delta_{1}c}\), we have

\[\mathbf{Pr}\left[\sum_{i\in G^{\prime}}h_{r}(X_{i}^{4})\geq 2 \delta_{1}n\right] =\mathbf{Pr}\left[\exp\left(\sum_{i\in G^{\prime}}c\cdot h_{r}(X_{i }^{4})\right)\geq\exp(2\delta_{1}cn)\right]\] \[\leq\exp(\epsilon^{2}n-2\delta_{1}cn)\leq\exp(-\delta_{1}cn).\]

As a result, with sufficiently large \(n\geq c_{2}(\delta_{1})\cdot d\log d\) and sufficiently small \(\epsilon\) such that \(6er\leq\delta_{1}\),

\[\mathbf{Pr}\left[\epsilon r+\frac{1}{n}\sum_{i\in G^{\prime}}h_{r}(X_{i}^{4}) \geq 3\delta_{1}\right]\leq\exp(-\delta_{1}cn)\leq 0.995\cdot 9^{-d}.\]

Also, \(\max_{L}\sum_{i\in L}\left\langle a_{i},x\right\rangle^{2}\left\langle a_{i},u \right\rangle^{2}\) has a similar tail bound and therefore can be bounded in the same way. We can then bound the operator norm via an epsilon-net argument. Set \(\delta_{1}=\delta_{0}/24\). By an \(1/4\)-net on \(\mathcal{S}^{d-1}\) with \(\left|\mathcal{N}\right|\leq 9^{d}\), we have that

\[\mathbf{Pr}\left[\max_{z\in\mathcal{S}^{d-1}}\left|z^{\top}Y_{G^{\prime},w^{ *}-\sigma(w)}z\right|\geq\delta_{0}/2\right]\leq 9^{d}\cdot 0.99\cdot 9^{-d} \leq 0.99.\]

By combining the above inequality with Equation (10), we can conclude that, for any \(\delta_{0}>0\), there exists \(\epsilon_{0}>0\), such that when \(n\geq\max\{c_{1}(\delta_{0}),c_{2}(\delta_{0})\}\cdot d\log d\) and \(0\leq\epsilon\leq\epsilon_{0}\), with probability at least \(0.98\),

\[\forall w\in\Delta_{n,2\epsilon},\left\|Y_{G,w}-(I+2xx^{\top})\right\|_{2}= \left\|Y_{G^{\prime},\sigma(w)}-(I+2xx^{\top})\right\|_{2}\leq\delta_{0}.\]

Therefore, with probability at least \(0.98\), we have \(\left\|Y_{G,w}-(I+2xx^{\top})\right\|_{2}\leq\delta_{0}\) for all \(w\in\Delta_{n,2\epsilon}\). 

### Concentration Inequalities

For the undisturbed samples, we have the following concentration result.

**Lemma A.2** ([3] Section A.4.2).: _Let \(x\in\mathbb{R}^{d}\). For any \(\delta>0\), there exists a constant \(C(\delta)>0\) such that when \(n>C\cdot d\log d\) and we are given a set of \(n\) samples \(\{(a_{i},y_{i})\}_{i=1}^{n}\) with \(a_{i}\sim\mathcal{N}(0,I)\) independently and \(y_{i}=\left\langle a_{i},x\right\rangle^{2}\) for all \(i\in[n]\), then with probability at least \(0.99\), it holds_

\[\left\|\frac{1}{n}\sum_{i=1}^{n}y_{i}a_{i}a_{i}^{\top}-(I+2xx^{\top})\right\|_ {2}\leq\delta.\]Omitted Proofs in Section 5

**Lemma 5.1**.: _Let \(\mathcal{D}_{z}\) be the distribution of gradients at \(z\) as defined in Equation (7). For any \(z\in\mathbb{R}^{d}\) with \(\left\|z-x\right\|_{2}\leq 1\), the covariance matrix \(\Sigma_{z}\) of \(\mathcal{D}_{z}\) satisfies \(\Sigma_{z}\preceq O(\left\|z-x\right\|_{2}^{2})I\)._

Proof of Lemma 5.1.: Recall that \(g\sim\mathcal{D}_{z}\) is distributed as

\[g =\frac{\partial}{\partial z}\left[\left(\left\langle a,z\right\rangle ^{2}-\left\langle a,x\right\rangle^{2}\right)^{2}\right]\] \[=-4\left(\left\langle a,z\right\rangle^{2}-\left\langle a,x \right\rangle^{2}\right)\langle a,z\rangle a\quad\text{where }\;a\sim\mathcal{N}(0,1)\;.\]

Because \(\mathbf{E}_{g\sim\mathcal{D}_{z}}\left[g\right]=\mu_{z}\), the spectral norm of \(\Sigma_{z}\) can be upper bounded as follows:

\[\left\|\Sigma_{z}\right\|_{2}=\left\|\underset{g\sim\mathcal{D}_{z}}{\mathbf{ E}}\left[gg^{\top}\right]-\mu_{z}\mu_{z}^{\top}\right\|_{2}\leq\left\| \underset{g\sim\mathcal{D}_{z}}{\mathbf{E}}\left[gg^{\top}\right]\right\|_{2}\;.\]

Consequently, it suffices to upper bound \(\left\|\mathbf{E}_{g\sim\mathcal{D}_{z}}\left[gg^{\top}\right]\right\|_{2}\). Let \(h=z-x\).

\[\left\|\underset{g\sim D_{z}}{\mathbf{E}}\left[gg^{\top}\right]\right\|_{2} =\max_{\left\|v\right\|_{2}=1}v^{\top}\underset{g\sim\mathcal{D}_ {z}}{\mathbf{E}}\left[gg^{\top}\right]v=\max_{\left\|v\right\|_{2}=1} \underset{g}{\mathbf{E}}\left[\left\langle g,v\right\rangle^{2}\right]\] \[=16\max_{\left\|v\right\|_{2}=1}\left(105^{4}\left\|h\right\|_{2}^ {8}\left\|2x+h\right\|_{2}^{8}\left\|x+h\right\|_{2}^{8}\left\|v\right\|_{2}^{ 8}\right)^{1/4}\] \[=(16\cdot 105)\left\|h\right\|_{2}^{2}\left\|2x+h\right\|_{2}^{2} \left\|x+h\right\|_{2}^{2}=O(\left\|h\right\|_{2}^{2})\;\;.\]

The first inequality is due to Cauchy-Schwarz inequality. The last step uses the fact that \(\left\|x\right\|_{2}=1\) and \(\left\|h\right\|_{2}\leq 1\). 

**Lemma 5.3**.: _Suppose in iteration \(t\) of Algorithm 2, the current solution \(z_{t}\) satisfies \(\left\|z_{t}-x\right\|_{2}\leq 1/8\), and the estimated gradient \(\widehat{\mu}_{z_{t}}\in\mathbb{R}^{d}\) satisfies \(\left\|\widehat{\mu}_{z_{t}}-\mu_{z_{t}}\right\|_{2}\leq c\left\|z_{t}-x\right\| _{2}\) for \(c=4\). Then, we have \(\left\|z_{t+1}-x\right\|_{2}^{2}\leq 0.99\left\|z_{t}-x\right\|_{2}^{2}\)._

Proof of Lemma 5.3.: Recall that \(g\sim\mathcal{D}_{z}\) is distributed as

\[g=\frac{\partial}{\partial z}\left[\left(\langle a,z\rangle^{2}-\langle a,x \rangle^{2}\right)^{2}\right]\;\text{ where }\;a\sim\mathcal{N}(0,I)\;\;.\]

We can compute the mean of \(\mathcal{D}_{z}\) using moments of Gaussian:

\[\mu_{z}=\underset{g\sim\mathcal{D}_{z}}{\mathbf{E}}g=\left(12\left\|z\right\| _{2}^{2}-4\left\|x\right\|_{2}^{2}\right)z-8\langle x,z\rangle x\;.\] (14)

Consider one step of gradient descent in Algorithm 2: \(z_{t+1}=z_{t}-\eta\widehat{\mu}_{z_{t}}\), where \(\widehat{\mu}_{z_{t}}\) is an approximate gradient. We have

\[\left\|z_{t+1}-x\right\|_{2}^{2}=\left\|z_{t}-\eta\widehat{\mu}_{z_{t}}-x \right\|_{2}^{2}=\left\|z_{t}-x\right\|_{2}^{2}-2\eta\langle\widehat{\mu}_{z_{ t}},z_{t}-x\rangle+\eta^{2}\langle\widehat{\mu}_{z_{t}},\widehat{\mu}_{z_{t}}\rangle\]

To prove convergence, we need to lower bound \(\langle\widehat{\mu}_{z_{t}},z_{t}-x\rangle\) and upper bound \(\langle\widehat{\mu}_{z_{t}},\widehat{\mu}_{z_{t}}\rangle\).

We write \(z=z_{t}\) and \(h=z-x\) to simplify notation. We can substitute \(z=x+h\) in Equation (14):

\[\mu_{z}=\left(16\langle x,h\rangle+12\left\|h\right\|_{2}^{2}\right)x+\left(8 \left\|x\right\|_{2}^{2}+24\langle x,h\rangle+12\left\|h\right\|_{2}^{2} \right)h\;.\]Recall the assumptions of this lemma: \(\left\|x\right\|_{2}=1\), \(\left\|h\right\|_{2}\leq 1/8\), and \(\left\|\widehat{\mu}_{z}-\mu_{z}\right\|_{2}\leq c\left\|h\right\|_{2}\).

First we lower bound \(\left\langle\widehat{\mu}_{z},h\right\rangle\).

\[\left\langle\widehat{\mu}_{z},h\right\rangle =\left\langle\mu_{z},h\right\rangle+\left\langle\widehat{\mu}_{z }-\mu_{z},h\right\rangle\] \[=16\langle x,h\rangle^{2}+36\langle x,h\rangle\left\|h\right\|_{ 2}^{2}+8\left\|x\right\|_{2}^{2}\left\|h\right\|_{2}^{2}+12\left\|h\right\|_{2 }^{4}+\left\langle\widehat{\mu}_{z}-\mu_{z},h\right\rangle\] \[\geq-\tfrac{81}{4}\left\|h\right\|_{2}^{4}+8\left\|x\right\|_{2}^ {2}\left\|h\right\|_{2}^{2}+12\left\|h\right\|_{2}^{4}-\left\langle\widehat{ \mu}_{z}-\mu_{z},h\right\rangle\] \[\geq(8-\tfrac{33}{256}-c)\left\|h\right\|_{2}^{2}\] \[\geq(7.5-c)\left\|h\right\|_{2}^{2}\.\]

The first inequality uses the fact that \(16\langle x,h\rangle^{2}+36\langle x,h\rangle\left\|h\right\|_{2}^{2}\) is a second-order polynomial of \(\langle x,h\rangle\), which has minimum value \(-\tfrac{81}{4}\left\|h\right\|_{2}^{4}\) for all \(\langle x,h\rangle\in\mathbb{R}\).

Next we upper bound \(\left\langle\widehat{\mu}_{z},\widehat{\mu}_{z}\right\rangle\) using the triangle inequality.

\[\left\|\widehat{\mu}_{z}\right\|_{2} \leq\left\|\mu_{z}\right\|_{2}+\left\|\widehat{\mu}_{z}-\mu_{z} \right\|_{2}\] \[\leq\left(16\langle x,h\rangle+12\left\|h\right\|_{2}^{2}\right) \left\|x\right\|_{2}+\left(8\left\|x\right\|_{2}^{2}+24\langle x,h\rangle+12 \left\|h\right\|_{2}^{2}\right)\left\|h\right\|_{2}+c\left\|h\right\|_{2}\] \[\leq\left(16+\tfrac{12}{8}+8+\tfrac{24}{8}+\tfrac{12}{64}+c \right)\left\|h\right\|_{2}\] \[\leq(29+c)\left\|h\right\|_{2}\.\]

Putting everything together, we have

\[\left\|z_{t+1}-x\right\|_{2}^{2} =\left\|z_{t}-x\right\|_{2}^{2}-2\eta\langle\widehat{\mu}_{z_{t}},z_{t}-x\rangle+\eta^{2}\langle\widehat{\mu}_{z_{t}},\widehat{\mu}_{z_{t}}\rangle\] \[\leq\left[1-2(7.5-c)\eta+(29+c)^{2}\eta^{2}\right]\left\|z_{t}-x \right\|_{2}^{2}\.\]

Choosing \(c=4\) and \(\eta=1/300\) gives that \(\left\|z_{t+1}-x\right\|_{2}^{2}\leq 0.99\left\|z_{t}-x\right\|_{2}^{2}\). 

## Appendix C Counter-examples

Prior robust phase retrieval algorithms [24, 45] focus on the setting where the observations \(y_{i}\)'s are subject to adversarial perturbation while the measuring vectors \(a_{i}\)'s are independently sampled from the Gaussian distribution. The Median Truncated Wirtinger Flow Algorithm [45] first initialize \(z^{(0)}\) by the spectral method, calculating \(z^{(0)}\) as the top eigenvector of \(Y:=\frac{1}{m}\sum_{i=1}^{m}y_{i}a_{i}a_{i}^{\top}\mathbb{1}_{\left\|y_{i} \right\|\leq\alpha^{2}\operatorname{\mathrm{med}}\left(\{y_{i}\}_{i=1}^{m}\right)\) using a truncated set of samples, where the threshold is determined by \(\operatorname{\mathrm{med}}\left(\{y_{i}\}_{i=1}^{m}\right)\), the median over all \(y_{i}\)'s. As long as the fraction of of outliers is not too large and the sample complexity is large enough, the initialization is guaranteed to be within a small neighborhood of the ground truth.

In this section, we present a counter-example where robust phase retrieval algorithms [24, 45] can be insufficient when directly applied to the \(\epsilon\)-corruption phase retrieval problem.

Let \(x\in\mathbb{S}^{d-1}\) be the ground truth unit vector. Here we construct an \(\epsilon\)-corruption adversary that can manipulate the top eigenvector of the empirical covariance matrix \(Y=\sum_{i=1}^{n}y_{i}a_{i}a_{i}^{\top}\), even when \(y_{i}\) are accurately calculated as \(y_{i}=(a_{i}^{\top}x)^{2}\).

Let \(u\in\mathbb{S}^{d-1}\) be a unit vector such that \(x^{\top}u=0\). Suppose the adversary changes \(1\%\) of the \(a_{i}\)'s to \(a_{i}=\sqrt{d-1/25}\cdot u+1/5x\), and suppose all the \(y_{i}\)'s are accurate. In particular, the length of the corrupted \(a_{i}\)'s is comparable to Gaussian vectors, and the corresponding \(y_{i}=(a_{i}^{\top}x)^{2}=1/25\). Consequently, the median-truncated initialization in [45] will not be able to filter out such \(y_{i}\). However, the top eigenvector of \(\mathbf{E}\left[Y\right]=\mathbf{E}\left[\sum_{i=1}^{n}y_{i}a_{i}a_{i}^{\top} \right]=O(d)uu^{\top}+O(\sqrt{d})(ux^{\top}+xu^{\top})+O(1)(I+2xx^{\top})\) will be manipulated to \(u\), which is far from the ground truth \(x\).

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: We provide a formal discussion of our theoretical claim in the first section of the paper, and include a formal statement of our result in Section 3. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We point out to all the assumptions used in our paper. There are a discussion of the limitations in the conclusion. Our algorithm can only handle a corruption \(\epsilon<\epsilon_{0}\) for some universal constant \(\epsilon_{0}\). Additionally, it is possible that the dependency on \(\Delta\) for the sample complexity can be removed. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs**Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?

Answer: [Yes]

Justification: We formally describe our problem setting and assumption used. All the proofs are in the paper, either in the main content pages or in the appendix.

Guidelines:

* The answer NA means that the paper does not include theoretical results.
* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [NA] Justification: The paper is a theory paper, and it has no experiments. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.

5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [NA] Justification: The paper is a theory paper, and it has no experiments. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [NA] Justification: The paper is a theory paper, and it has no experiments. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [NA] Justification: The paper is a theory paper, and it has no experiments. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).

* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [NA] Justification: The paper is a theory paper, and it has no experiments. Guidelines: The answer NA means that the paper does not include experiments.
* The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.
* The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.
* The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: The paper conforms with the NeurIPS Code of Ethics. Guidelines: The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
* If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.
* The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: The paper provides a theoretical result. We do not believe there are potential societal consequences of our work, aside from advancing the field of Machine Learning. Guidelines: The answer NA means that there is no societal impact of the work performed. If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.

* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).

11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: The paper only contains theoretical results. Guidelines:

* The answer NA means that the paper poses no such risks.
* Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.
* Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.
* We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.

12. **Licensees for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [NA] Justification: The paper only contains theoretical results. Previous work is properly cited. Guidelines:

* The answer NA means that the paper does not use existing assets.
* The authors should cite the original paper that produced the code package or dataset.
* The authors should state which version of the asset is used and, if possible, include a URL.
* The name of the license (e.g., CC-BY 4.0) should be included for each asset.
* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.

* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: The paper does not introduce any new asset. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: This point does not apply to our paper. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: This point does not apply to our paper. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. * We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. * For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.