# Gaussian Process Bandits for Top-k Recommendations

 Mohit Yadav

University of Massachusetts Amherst

ymohit@cs.umass.edu &Daniel Sheldon

University of Massachusetts Amherst

sheldon@cs.umass.edu &Cameron Musco

University of Massachusetts Amherst

cmusco@cs.umass.edu

###### Abstract

Algorithms that utilize bandit feedback to optimize top-k recommendations are vital for online marketplaces, search engines, and content platforms. However, the combinatorial nature of this problem poses a significant challenge, as the possible number of ordered top-k recommendations from \(n\) items grows exponentially with \(k\). As a result, previous work often relies on restrictive assumptions about the reward or bandit feedback models, such as assuming that the feedback discloses rewards for each recommended item rather than a single scalar feedback for the entire set of top-k recommendations. We introduce a novel contextual bandit algorithm for top-k recommendations, leveraging a Gaussian process with a Kendall kernel to model the reward function. Our algorithm requires only scalar feedback from the top-k recommendations and does not impose restrictive assumptions on the reward structure. Theoretical analysis confirms that the proposed algorithm achieves sub-linear regret in relation to the number of rounds and arms. Additionally, empirical results using a bandit simulator demonstrate that the proposed algorithm outperforms other baselines across various scenarios.

## 1 Introduction

The top-\(k\) recommendation problem involves providing a ranked list of k items, such as news articles or products, from a pool of \(n\) items [34, 13]. Online algorithms must adapt to dynamic user preferences, making bandit algorithms suitable due to their use of limited feedback [1]. Developing bandit algorithms is challenging due to limited feedback and the need for computational efficiency in real-time recommendation environments. Recent research on user interfaces for recommendations highlights that the overall layout of the recommendation page is crucial for user appeal, as modern UI designs have evolved from simple dropdown lists to complex, visually engaging layouts [17, 13, 18]. Consequently, bandit algorithms must jointly select and display all top-\(k\) items, rather than simply choosing the most relevant \(k\) items and ordering them by decreasing user relevance [31].

The joint consideration of top-\(k\) items makes the number of arms (possible actions for the bandit algorithm) combinatorially large, i.e., \(\Theta(n^{k})\). Previous research on bandit algorithms often imposes strict assumptions on feedback models [30, 21]. For instance, _semi-bandit_ feedback provides a scalar reward for each of the top \(k\) items, thus decomposing the combinatorial feedback into item-level feedback. However, this type of feedback is frequently unavailable [32]. Another common feedback model is _cascade_ browsing [16], which assumes that users examine items in a predetermined order and cease browsing once a desirable item is found, offering item-specific scalar feedback but failing to capture potential non-linear interactions among items [26]. Figure 1 illustrates the limitations of the cascade model in capturing user interactions within modern top-\(k\) recommendation interfaces.

These limitations motivate us to adopt a more general _full-bandit_ feedback setting, where only a single scalar value is provided for the entire top-\(k\) set of recommendations [24].

Beyond feedback assumptions, the reward structure in bandit algorithms must be decomposable into scalar values for individual items to prevent a combinatorial explosion of arms--something that is not always feasible. For example, modern e-commerce platforms value and track metrics such as diversity and fairness [1], which cannot be captured by focusing solely on individual items [15]. This necessitates algorithms for full-bandit feedback settings that operate without specific assumptions about the objectives or reward structures [24].

This work introduces a bandit algorithm that uses Gaussian processes (GPs) to model rewards under full-bandit feedback (i.e., a single scalar value). GPs are selected for their flexibility in accommodating feedback across discrete, continuous, and mixed domains, such as continuous contexts and discrete rankings [33]. Additionally, unlike parametric models that require optimization incorporating accumulated feedback from previous rounds, GP updates are computationally efficient, involving only data updates [24]. Although GP inference may face computational limits, we will develop efficient inference methods tailored to our proposed algorithm. A further challenge in designing GP-based bandit algorithms for top-\(k\) recommendations is constructing expressive positive-definite kernels that capture similarities between top-\(k\) recommendations [9]. This work mitigates these computational and modeling challenges, as illustrated in the following sections.

Broadly speaking, GPs have been previously explored for bandit algorithms [27; 19]. Krause et al. [14] employed GPs for contextual bandits in continuous domains; we focus on the discrete domain of top-\(k\) recommendations. Vanchinathan et al. [28] used GPs with a position-based feedback model, and Wang et al. [31] used GPs with semi-bandit feedback for recommending top-\(k\) items. In contrast, our work does not rely on a specific reward model or feedback assumption, and develops an an efficient GP-based bandit algorithm for top-\(k\) recommendations.

### Contributions

Our primary contribution is the GP-TopK algorithm, a contextual bandit algorithm for recommending top-\(k\) items. This algorithm operates in a full-bandit feedback setting without relying on assumptions on reward, making it broadly applicable compared to prior works. We leverage GPs with variants of the _Kendall_ kernel [12] to model the reward function and optimize the upper confidence bound (UCB) [27] acquisition function for selecting the next arm. Additionally, we introduce a novel weighted convolutional Kendall kernel for top-\(k\) recommendations that address pathologies in existing variants of the Kendall kernel when applied to top-\(k\) recommendations.

Our second key contribution is enhancing the scalability of the GP-TopK algorithm for longer time horizons. Initially, the computational cost for top-\(k\) recommendations using the GP-TopK algorithm is \(\mathcal{O}(T^{4})\) for \(T\) rounds. We first reduce this to \(\mathcal{O}(T^{3})\) by leveraging iterative algorithms from numerical linear algebra [25]. Next, we derive sparse feature representations for the novel weighted

\begin{table}
\begin{tabular}{l|c|c} \hline \hline Tasks & _kernel approach_ & _feature approach_ \\ \hline _compute_ & \(\mathcal{O}(\mathbf{T^{3}})\) & \(\mathcal{O}(c\cdot k^{2}\cdot\mathbf{T^{2}})\) \\ _memory_ & \(\mathcal{O}(\mathbf{T^{2}})\) & \(\mathcal{O}(c\cdot k^{2}\cdot\mathbf{T})\) \\ \(\mathbf{mvm}(K_{X_{t}})\) & \(\mathcal{O}(t^{2})\) & \(\mathcal{O}(c\cdot k^{2}\cdot t)\) \\ _compute_\(K_{X_{t}}\) & \(\mathcal{O}((c+k^{2})\cdot t)\) & \(\mathcal{O}(c\cdot k^{2})\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Compute and memory analysis for the proposed GP-TopK bandit algorithm. Rows represent different costs: total _compute_ and _memory_ of the GP-TopK algorithm for \(\mathbf{T}\) rounds, time for matrix-vector multiplication (\(\mathbf{mvm}\)) with the kernel matrix \(K_{X_{t}}\) for \(t^{\text{th}}\) round, and time to update \(K_{X_{t}}\). Columns represent different approaches: the _kernel approach_, which uses full kernel matrices, and our novel _feature approach_, which performs the same operations through feature expansions and scales more efficiently with respect to \(\mathbf{T}\). The symbols \(c\), \(k\), and \(\mathbf{T}\) denote the embedding size for contexts, the number of items, and the number of rounds, respectively.

Figure 1: A snapshot from Etsy showcases Fatherâ€™s Day shopping recommendations. The lack of an obvious linear search order challenges the assumptions of the cascade model. Additionally, the proximity and arrangement of items are likely to influence clicks, indicating complex interaction patterns and supporting the need for full-bandit feedback without assumptions about user interactions with recommended items.

convolutional Kendall kernel, further reducing the compute requirements from \(\mathcal{O}(T^{4})\) to \(\mathcal{O}(T^{2})\) and memory requirements from \(\mathcal{O}(T^{2})\) to \(\mathcal{O}(T)\). Table 1 summarizes these improvements in time and memory requirement, including their dependence on other parameters.

We provide a theoretical analysis showing that GP-TopK's regret is sub-linear in \(T\), benefiting from the feature representations of the Kendall kernels introduced in this work. Specifically, we establish an upper bound on regret that is nearly quadratic in \(n\), significantly improving over the naive \(\Theta(n^{k})\) bound for top-\(k\) recommendations without using feature representations [27]. Finally, we empirically validate GP-TopK's regret on real-world datasets, demonstrating improvement over baseline methods.

### Organization

The remainder of this paper is organized as follows: Section 2 introduces Kendall kernels for full and top-\(k\) rankings, including the novel weighted convolutional Kendall kernel. Section 3 presents faster matrix-vector multiplication algorithms for Kendall kernels, enhancing the efficiency of the proposed bandit algorithm, which is further detailed along with the regret analysis in Section 4. Finally, Sections 5 and 6 present empirical results and concluding discussion, respectively.

## 2 Kendall Kernels for Full and Top-k Rankings

This section introduces Kendall kernels and their extensions for top-\(k\) recommendations, forming the foundation of our approach. We first establish key notations and them present Sections 2.1 and 2.2, which introduce Kendall kernels for full rankings and top-\(k\) rankings, respectively.

Notations:Let \([n]=1,2,\ldots,n\), with \(\pi\) representing a top-\(k\) ranking--an ordered tuple of \(k\) distinct elements from \([n]\). For a full ranking (\(k=n\)), we use \(\sigma\) and denote the set of all possible top-\(k\) rankings by \(\Pi^{k}\), with cardinality \(|\Pi^{k}|=\Theta(n^{k})\). To capture ranking positions, the vector \(\mathbf{p}^{\sigma}\in\mathbb{R}^{n}\) corresponds to a full ranking \(\sigma\) with entry \(\mathbf{p}_{i}^{\sigma}\) gives the rank of item \(i\). For top-k rankings, \(\mathbf{p}^{\pi}\in\mathbb{R}^{n}\) is similarly constructed by arbitrarily assigning distinct ranks to items not in the top \(k\). For relative ranks, indicator functions \(\mathbf{p}_{i<j}^{\sigma}\) and \(\mathbf{p}_{i>j}^{\sigma}\) denote whether item \(i\) is ranked before or after item \(j\), respectively in \(\sigma\). Also, \(\mathbf{p}_{i<j}^{\pi}\) and \(\mathbf{p}_{i>j}^{\pi}\) are similar indicator functions defined for top-k rankings.

### Kendall Kernels for Full Rankings

Jiao et al. [9] showed that the Kendall tau rank correlation coefficient [12] is a positive definite (p.d.) kernel for full rankings, which we refer to as the standard Kendall (SK) kernel. The weighted Kendall (WK) kernel generalizes the SK kernel by differentially weighting item pairs [10]. Specifically, the SK and WK kernels for full rankings \(\sigma_{1},\sigma_{2}\) are defined as:

\[k^{sk}(\sigma_{1},\sigma_{2}) \coloneqq\frac{1}{\binom{n}{2}}\sum_{i<j}\eta_{i,j}(\sigma_{1}, \sigma_{2})\] (1) \[k^{wk}(\sigma_{1},\sigma_{2}) \coloneqq\frac{1}{\binom{n}{2}}\sum_{i<j}w((\mathbf{p}_{i}^{ \sigma_{1}},\mathbf{p}_{j}^{\sigma_{1}}),(\mathbf{p}_{i}^{\sigma_{2}}, \mathbf{p}_{j}^{\sigma_{2}}))\cdot\eta_{i,j}(\sigma_{1},\sigma_{2}),\] (2)

where \(\eta_{i,j}\) is 1 if the pair \((i,j)\) is _concordant_ (ordered the same in both rankings) and \(-1\) otherwise; concretely, \(\eta_{i,j}(\sigma_{1},\sigma_{2})\coloneqq\mathbf{p}_{i<j}^{\sigma_{1}} \cdot\mathbf{p}_{i<j}^{\sigma_{2}}+\mathbf{p}_{i>j}^{\sigma_{1}}\cdot\mathbf{p }_{i>j}^{\sigma_{2}}-\mathbf{p}_{i<j}^{\sigma_{1}}\cdot\mathbf{p}_{i>j}^{ \sigma_{2}}-\mathbf{p}_{i>j}^{\sigma_{1}}\cdot\mathbf{p}_{i<j}^{\sigma_{2}}\); and \(w((\mathbf{p}_{i}^{\sigma_{1}},\mathbf{p}_{j}^{\sigma_{1}}),(\mathbf{p}_{i}^ {\sigma_{2}},\mathbf{p}_{j}^{\sigma_{2}}))\) is the value of a positive definite weighting kernel \(w(\cdot,\cdot):[n]^{2}\times[n]^{2}\mapsto\mathbb{R}\) that operates on pairs of ranks. The \(w_{i,j}\) adds flexibility and can assign varying importance to ranks, similar to the discounted cumulative gain (DCG) metric [7]. Note that both SK and WK kernels are p.d. and right-invariant with respect to \(\Pi^{n}\)[10]. In other words, they compute similarity based only on the relative ranks of pairs, not on the labels of items, as clearly evident from Equations 1 and 2.

### Kendall Kernels for Top-k Rankings

Weighted Kendall and Convolutional Kendall (CK) kernels.To adapt the WK kernel from full rankings to top-\(k\) rankings, Jiao et al. [10] set the weighting function \(w(i,j,\sigma_{1},\sigma_{2})\) to zero if either item is not in the top-k of either ranking. While this approach yields a p.d. kernel, it disregardsitems outside the intersection of top-\(k\) rankings. In contrast, the convolutional operation provides an alternative for adapting the standard Kendall kernel to top-\(k\) rankings.

Let \(B_{\pi}\) denote the set of full rankings consistent with the top-k ranking \(\pi\) (i.e., for every item \(i\) in \(\pi\), \(\forall\sigma\in B_{\pi},\mathbf{p}_{i}^{\pi}=\mathbf{p}_{i}^{\sigma}\)). The convolutional Kendall kernel can be defined as follows:

\[k^{ck}(\pi_{1},\pi_{2})=\frac{1}{|B_{\pi_{1}}|\cdot|B_{\pi_{2}}|}\sum_{\sigma_ {1}\in B_{\pi_{1}},\ \sigma_{2}\in B_{\pi_{2}}}k^{sk}(\sigma_{1},\sigma_{2}),\] (3)

where \(k^{sk}\) is the standard Kendall kernel. Since the CK kernel is a convolution of another p.d. kernel, it is also a p.d. kernel [5]. Unlike the WK kernel for top-k rankings, the CK kernel accounts for items not in both top-k rankings. However, computing the CK kernel is expensive, requiring exponentially many evaluations of the kernel \(k^{sk}\) in the double summation. Therefore, Jiao et al. [9] developed an efficient algorithm to bypass this double summation, reducing compute to \(\mathcal{O}(k\log k)\) time.

Proposed Weighted Convolutional Kendall (WCK) Kernel.To combine the strengths of the WK and CK kernels for top-k rankings, we propose the weighted convolutional Kendall kernel for top-k rankings \(\pi_{1}\) and \(\pi_{2}\in\Pi^{k}\):

\[k^{wck}(\pi_{1},\pi_{2})\coloneqq\frac{1}{|B_{\pi_{1}}|\cdot|B_{\pi_{2}}|}\sum _{\sigma_{1}\in B_{\pi_{1}},\sigma_{2}\in B_{\pi_{2}}}k^{wk}(\sigma_{1},\sigma _{2}),\] (4)

where \(k^{wk}\) represents the weighted Kendall kernel for full rankings \(\sigma_{1},\sigma_{2}\in\Pi^{n}\).

The proposed WCK kernel combines the flexibility of differentially weighting ranks among the top-\(k\) items (as in the WK kernel) with the ability to account for items outside the intersection of both top-\(k\) rankings (as in the CK kernel). Additionally, as a convolution of a p.d. kernel, it is also a p.d. kernel. However, computing the WCK kernel remains challenging, as it requires exponentially many evaluation of the \(k^{wk}\) kernel, as given in the RHS of Equation 4. To address this, we focus on a specific form of rank weights of the \(k^{wk}\) kernel, called as _product-symmetric_ rank weights:

\[w_{ps}((i_{1},j_{1}),(i_{2},j_{2}))\coloneqq w_{s}(i_{1},j_{1})\cdot w_{s}(i_{ 2},j_{2}),\] (5)

where, \(w_{s}(i,j):[n]\times[n]\mapsto\mathbb{R}\) is a symmetric function, i.e., \(w_{s}(i,j)=w_{s}(j,i)\). Notably, the WCK kernel can be computed efficiently for the case of these weights (see Claim 1 below).

The WCK kernel, even with the relatively simple \(w_{ps}\) weights, exhibits notable properties, as shown in Table 2. In this table, we use \(w_{s}(i,j)=\frac{1}{\log(i+1)}\cdot\frac{1}{\log(j+1)}\), inspired by the DCG metric commonly applied in recommendation systems [7]. Notably, the WK kernel ranks two rankings with no overlap (\(\pi_{0}\) and \(\pi_{1}\)) as more similar than two rankings with the same items in reversed order (\(\pi_{0}\) and \(\pi_{2}\)), indicating a clear pathology. Further, the CK kernel fails to distinguish between reversed pairs at different ranks (\(k^{ck}(\pi_{0},\pi_{3})=k^{ck}(\pi_{0},\pi_{4})\)), presenting another limitation if known variants of Kendall kernels for top-k rankings. By using product-symmetric ranking weights, the WCK kernel addresses these shortcomings, providing a more nuanced similarity comparison for top-\(k\) rankings.

**Claim 1**.: _The weighted convolutional Kendall kernel (Equation 4) with product-symmetric rank weights (Equation 5) can be computed in \(\mathcal{O}(k^{2})\) time._

Appendix A provides the proof that leverages the structure of product-symmetric rank weights \(w_{ps}\) to establish the existence of a feature representation for the WCK kernel, as formally stated below in

\begin{table}
\begin{tabular}{l|c|c|c|c} \hline Top-k & \(\pi_{1}\) & \(\pi_{2}\) & \(\pi_{3}\) & \(\pi_{4}\) \\ Kernels & \([4,5,6]\) & \([3,2,1]\) & \([2,1,3]\) & \([1,3,2]\) \\ \hline WK & \(0.00\) & \(-1.00\) & \(0.33\) & \(0.33\) \\ CK & \(-0.60\) & \(0.60\) & \(0.87\) & \(0.87\) \\ WCK & \(-0.38\) & \(0.09\) & \(0.46\) & \(0.87\) \\ \hline \end{tabular}
\end{table}
Table 2: Comparison of Kendall kernel similarities for top-k rankings. The table shows kernel values \(k(\pi_{0},\cdot)\) for the top-k ranking \(\pi_{0}=[1,2,3]\) with other rankings (\(\pi_{1}\), \(\pi_{2}\), \(\pi_{3}\), \(\pi_{4}\)) for \(n=7\) and \(k=3\). Rankings are arranged left to right by increasing similarity to \(\pi_{0}\). The similarity values provided by the proposed kernel increase from left to right as expected, demonstrating the desirable behavior of the WCK kernel with DCG rank weights, unlike other variants. All kernels are unit-normalized. See text for further details.

Claim 3 below. We then demonstrate that the inner product of these features, and hence the WCK kernel, can be computed in \(\mathcal{O}(k^{2})\) time (Algorithm 2 in the appendix). Similar to the result of Jiao et al. [9] for the CK kernel, this approach avoid exponentially many evaluations of \(k^{wk}\) on the RHS of Equation (4) by enabling a direct computation of the WCK kernel.

## 3 Fast Matrix-Vector Multiplication with Kendall Kernel Matrices

In Gaussian processes, inference can be accelerated by using iterative algorithms that take advantage of fast matrix-vector-multiplications (MVMs) with the kernel matrix [3]. This section introduces fast algorithms for kernel MVMs by exploiting the implicit structure of Kendall kernel matrices.

Let \(\mathbf{mvm}(K_{X_{t}})\) denote the runtime required to multiply the \(t\times t\) kernel matrix \(K_{X_{t}}=(k(x_{i},x_{j}))_{x_{i},x_{j}\in X_{t}}\) by any admissible vector. In the naive approach, this runtime is \(\mathbf{mvm}(KX_{t})=\mathcal{O}(t^{2})\). However, if \(k(x_{i},x_{j})=\phi^{a}(x_{i})^{T}\phi^{b}(x_{j})\) for any arbitrary \(x_{i}\) and \(x_{j}\), where the vectors \(\phi^{a}(x_{i})\) and \(\phi^{b}(x_{j})\) are sparse and contain only \(z\) non-zero entries, then \(\mathbf{mvm}(K_{X_{t}})\) reduces to \(\mathcal{O}(z\cdot t)\), which is a significant improvement over \(\mathcal{O}(t^{2})\) when \(z\ll t\). When \(\phi^{a}=\phi^{b}\), we refer to \(\phi^{a}\) as the _linear feature vector_ for the kernel \(k\). Before focusing on top-k ranking kernels, we provide a linear feature vector for the WK kernel on full rankings (given earlier in Equation 2).

**Claim 2**.: _Let \(\phi^{wk}(\sigma):\Pi^{n}\mapsto\mathbb{R}^{\binom{n}{2}}\) be a vector indexed by unique item pairs \((i,j)\), defined as:_

\[\phi^{wk}_{i,j}(\sigma)\coloneqq\frac{1}{\sqrt{\binom{n}{2}}}\cdot w_{s}( \mathbf{p}^{\sigma}_{i},\mathbf{p}^{\sigma}_{j})\cdot\left(\mathbf{p}^{\sigma }_{i<j}-\mathbf{p}^{\sigma}_{i>j}\right),\]

_where \(w_{s}\) is the symmetric weighting function in product-symmetric weights. Then, \(\phi^{wk}\) is a linear feature vector for the weighted Kendall kernel with product-symmetric weights \(w_{ps}\)._

Using Claim 2, the linear feature vector for the WK kernel can be extended to the WK top-\(k\) ranking kernel by utilizing the structure of product-symmetric weights, which allows weights to be set to zero for items outside of the top-\(k\) rankings, as described in Section 2.2. Precisely, such a feature vector for the top-\(k\) ranking kernel is sparse; specifically, the feature vector \(\phi^{wk}(\pi)\) contains only \(\mathcal{O}(k^{2})\) non-zero entries due to the WK kernel's focus on item pairs within the top-\(k\). Consequently, the runtime for \(\mathbf{mvm}(K_{X_{t}})\) in the WK kernel matrix is reduced to \(\mathcal{O}(k^{2}\cdot t)\).

Moving forward, we focus on deriving a sparse feature vector for the WCK kernel, enabling fast MVMs with the WCK kernel, which includes the CK kernel as a special case. Notably, any convolutional kernel inherits linear features from its constituent kernel. Specifically, \(\sum_{\sigma\in B_{s}}\phi^{wk}_{i,j}(\sigma)\) forms a feature vector for the WCK kernel, which follows from Equation 4 and However, computing this feature vector explicitly is computationally challenging, as it requires summing over all \(\sigma\in B_{\pi}\), which includes an exponential number of terms, i.e., \(\Theta(n^{k})\).

In response to this challenge, Claim 3 shows that the summation can be computed analytically and provides explicit linear feature vectors for the WCK and CK kernels. It also shows that \(\phi^{wck}\) has only \(\mathcal{O}(k^{2}+2nk)\) non-zero entries among its \(\mathcal{O}(n^{2})\) total entries. Consequently, \(\mathbf{mvm}(K_{X_{t}})\) for the WCK kernel requires \(\mathcal{O}((k^{2}+2nk)\cdot t)\) operations, which improves from \(\mathcal{O}(t^{2})\) to linear in \(t\). However, this introduces a dependence on \(n\), the number of items, which poses a serious limitation and is beneficial only when \(n\leq t\). In the following theorem, we leverage redundancy in \(\phi^{wck}\) to eliminate this dependence on \(n\), leading to the following main theorem about the \(\mathbf{mvm}(K_{X_{t}})\).

```
1:For the WCK kernel with product-symmetric weights \(w_{ps}\), the computational complexity of multiplying the kernel matrix \(K_{X_{t}}\) with any admissible vector is \(\mathcal{O}(k^{2}t)\), i.e., \(\mathbf{mvm}(K_{X_{t}})=\mathcal{O}(k^{2}t)\), where \(X_{t}\) is any arbitrary set of \(t\) top-k rankings. ```

**Theorem 1**.: _For the WCK kernel with product-symmetric weights \(w_{ps}\), the computational complexity of multiplying the kernel matrix \(K_{X_{t}}\) with any admissible vector is \(\mathcal{O}(k^{2}t)\), i.e., \(\mathbf{mvm}(K_{X_{t}})=\mathcal{O}(k^{2}t)\), where \(X_{t}\) is any arbitrary set of \(t\) top-k rankings._

Appendix A provides the proof in two steps. First, we utilize the values of \(\phi^{wck}\) from Claim 3 and categorize \(\phi^{wck}(\pi_{1})^{T}\phi^{wck}(\pi_{2})\) based on item pairs, as summarized in Table 4. Next, we show that only five combinations yield non-zero values, i.e., \(\phi^{wck}(\pi_{1})^{T}\phi^{wck}(\pi_{2})=\sum_{i=1}^{5}s_{i}(\pi_{1},\pi_{2})\). Each term \(s_{i}(\pi_{1},\pi_{2})\) is a dot product of vectors \(\phi^{a_{i}}(\pi_{1})^{T}\phi^{b_{i}}(\pi_{2})\), which contains at most \(\mathcal{O}(k^{2})\) non-zero entries. Thus, for the WCK and CK kernels, \(\mathbf{mvm}(K_{X_{t}})=\mathcal{O}(k^{2}t)\), since these vectors across all five terms include only \(\mathcal{O}(k^{2})\) non-zero entries. Consequently, Theorem 1 demonstrates that employing these vector representations for top-k rankings leads to faster MVMs, i.e., \(\mathbf{mvm}(K_{X_{t}})=\mathcal{O}(k^{2}t)\ll\mathcal{O}(t^{2})\).

## 4 Proposed GP-TopK Bandit Algorithm

In this section, we begin by formally defining the top-\(k\) recommendation problem within a bandit framework and introduce a generic contextual bandit algorithm, detailed in Algorithm 1. We then explain how the components of the algorithm are instantiated using the proposed GP approach, followed by an analysis of its computational complexity and cumulative regret.

Let \(T\) denote the number of rounds. Contexts \(\mathcal{C}\) are represented in a finite \(c\)-dimensional space, i.e., \(\mathcal{C}\subseteq\mathbb{R}^{c}\). In the \(t^{th}\) round, we receive a context \(\mathbf{c}_{t}\in\mathcal{C}\) and select a top-\(k\) ranking \(\pi_{t}\in\Pi^{k}\). Subsequently, a noisy reward \(y_{t}=\hat{f}(\mathbf{c}_{t},\pi_{t})+\epsilon_{t}\) is observed, where \(\hat{f}\) is the true reward function and \(\epsilon_{t}\) is round-independent noise. The regret is defined as \(r_{t}\coloneqq\max\pi^{{}^{\prime}}\in\Pi^{k}\hat{f}(\mathbf{c}_{t},\pi^{{}^{ \prime}})-\hat{f}(\mathbf{c}_{t},\pi_{t})\), with cumulative regret \(R_{T}\coloneqq\sum_{t=1}^{T}r_{t}\). The accumulated data at the \(t^{th}\) round is \(\mathcal{D}_{t}=(\mathbf{c}_{i},\pi_{i},y_{i})_{i=1}^{t}\). Below, the Algorithm 1 provides provides a generic schematic of the bandit algorithm.

```
1:Total rounds \(T\), initial reward model \(\mathcal{M}_{0}\), and acquisition function \(\mathcal{AF}\).
2:for\(t=1,\cdots,T\)do
3: Observe a context \(\mathbf{c}_{t}\) from the context space \(\mathcal{C}\).
4: Select a top-\(k\) ranking \(\pi_{t}\) that maximizes \(\mathcal{AF}(\mathcal{M}_{t-1}(\mathbf{c}_{t},\pi))\) for the context \(\mathbf{c}_{t}\).
5: Obtain the scalar reward \(y_{t}\).
6: Update the reward model \(\mathcal{M}_{t}\) using the accumulated feedback \(\mathcal{D}_{t}\).
7:endfor ```

**Algorithm 1** Contextual Bandit Algorithm for Top-k Recommendations

We aim to design the components of above Algorithm 1 with the objectives of minimizing cumulative regret and ensuring computational efficiency. It requires two key components: (a) a reward model \(\mathcal{M}_{t}\) that estimates the reward for any context and top-\(k\) ranking utilizing the accumulated data \(\mathcal{D}_{t}\) and (b) an acquisition function \(\mathcal{AF}\) for selecting \(\pi_{t}\) given the reward model \(\mathcal{M}_{t}\) and observed context \(\mathbf{c}_{t}\).

Reward model \(\mathcal{M}\) and acquisition function \(\mathcal{AF}\).The proposed GP-TopK bandit algorithm leverages GP regression to model the reward function over the domain of contexts and top-k rankings. Section B.1 briefs GP regression for the completeness. Essentially, the reward model \(\mathcal{M}\) maintains a distribution over functions \(f\), i.e., \(f\sim\mathcal{N}(0,k(\cdot,\cdot))\), where \(k\) is a product kernel function over both contexts and top-k rankings (\(\mathcal{C}\bigotimes\Pi^{k}\)). Specifically, the kernel function \(k\) is defined as follows:

\[k((\mathbf{c}_{1},\pi_{1}),(\mathbf{c}_{2},\pi_{2}))\coloneqq k^{c}(\mathbf{c}_ _{1},\mathbf{c}_{2})\cdot k^{r}(\pi_{1},\pi_{2}),\] (6)

where \(k^{c}(\mathbf{c}_{1},\mathbf{c}_{2})=\mathbf{c}_{1}^{T}\mathbf{c}_{2}\) is the dot-product kernel and \(k^{r}\) is a kernel for top-k rankings. We use variants of the Kendall kernel for \(k^{r}\) from Section 2. Updating the reward model \(\mathcal{M}_{t}\) at the round involves adding new data points to our GP regression, which is computationally inexpensive compared to the fine-tuning steps required by parametric models to incorporate the latest feedback.

We use the UCB function as the acquisition function, balancing exploration and exploitation by selecting actions that maximize the upper confidence bound of the estimated reward [27]. The UCB acquisition function is \(\mathcal{AF}(\mathcal{M}_{t}(\mathbf{c}_{t},\pi))\coloneqq\mu_{f\mid\mathcal{D}} ((\mathbf{c}_{t},\pi))+\beta^{\frac{1}{2}}.\sigma_{f\mid\mathcal{D}}((\mathbf{ c}_{t},\pi))\), where \(\sigma_{f\mid\mathcal{D}}((\mathbf{c}_{t},\pi))=\sqrt{k_{f\mid\mathcal{D}}(( \mathbf{c}_{t},\pi),(\mathbf{c}_{t},\pi))}\) and \(\beta\) controls the trade-off between exploration and exploitation. Here, \(\mu_{f\mid\mathcal{D}}\) and \(k_{f\mid\mathcal{D}}\) are the GP posterior mean and covariance functions, as detailed in Section B.1. At the \(t^{\text{th}}\) round, the algorithm selects the top-k ranking \(\pi\in\Pi^{k}\) that maximizes \(\mathcal{AF}(\mathcal{M}_{t}(\mathbf{c}_{t},\pi))\), which is performed using local search [19], as detailed further in Appendix B.

Computational complexity.The GP-TopK bandit algorithm does not require compute for model updates. In other words, updating \(\mathcal{M}_{t}\), i.e., in the Line 5 of the Algorithm 1 requires only updating the list of accumulated feedback data \(\mathcal{D}_{t}\). The GP-TopK relies on local search to optimize \(\mathcal{AF}\), so the computational demands stem solely from \(\mathcal{AF}\) evaluations within the local search. As shown in Section B.1, computing the GP variance term for evaluating \(\mathcal{AF}\), i.e, \(\sigma_{f\mid\mathcal{D}}((\mathbf{c}_{t},\pi))\) involves solving \(\left[K_{X_{t}}+\sigma^{2}I\right]^{-1}\mathbf{v}\) for a vector \(\mathbf{v}\), where \(X_{t}=[(\mathbf{c}_{1},\pi_{1}),\cdots,(\mathbf{c}_{t},\pi_{t})]\). Naively, this operation requires \(\mathcal{O}(t^{3})\) time per round, amounting to total \(\mathcal{O}(T^{4})\) over \(T\) rounds. Iterative algorithms, however, can expedite the process by leveraging fast MVMs with kernel matrices, as discussed in Section 3. Below, Theorem 2 formalizes the computational demands of the GP-TopK algorithm.

**Theorem 2**.: _Assuming a fixed number of iterations required by the iterative algorithms, the total computational time for running the GP-TopK bandit algorithm for \(T\) rounds of top-\(k\) recommendations, using the contextual product kernel (Equation 6), is \(\mathcal{O}(k^{2}c\ell T^{2})\). This applies to WK, CK, and WCK top-k ranking kernels, where \(\ell\) is the number of local search evaluations._

The proof of Theorem 2, provided in Appendix B, demonstrates efficiency gains from combining feature representations with iterative algorithms, reducing computational time from \(\mathcal{O}(T^{4})\) to \(\mathcal{O}(T^{2})\). This is a substantial improvement, as even a single MVM with the matrix \(K_{X_{t}}\) using the full kernel matrix at each round would require \(\mathcal{O}(T^{3})\) compute time. Additionally, the theorem shows that the running time of the GP-TopK algorithm does not explicitly depend on the number of items \(n\).

Regret analysis.The cumulative regret is \(R_{T}=\sum_{t=1}^{T}\max_{\pi^{{}^{\prime}}\in\Pi^{k}}\hat{f}(\mathbf{c}_{t},\pi^{{}^{\prime}})-\hat{f}(\mathbf{c}_{t},\pi_{t})\), where \(\pi_{t}\) is the ranking chosen at round \(t\). Optimizing cumulative regret for top-\(k\) recommendations is challenging, as it requires learning the context-arm relationship and matching the best possible mapping. To bound cumulative regret, regularity assumptions are essential, as noted in prior works [27, 14]. _We consider the following two assumptions, either of which suffices_. Also, \(\mathcal{X}\coloneqq\mathcal{C}\bigotimes\Pi^{k}\) for below assumptions.

**Assumption 1**.: \(\mathcal{X}\) _is finite, meaning that only finite contexts are considered (\(|\mathcal{C}|<\infty\)), and the reward function \(\hat{f}\) is sampled from the GP prior with a noise variance of \(\xi^{2}\)._

**Assumption 2**.: \(\mathcal{X}\) _is arbitrary and the reward function \(\hat{f}\) has a bounded RKHS norm for the kernel \(k\), i.e., \(\left\|f\right\|_{k}\leq B\). The reward noises \(\epsilon_{t}\) form an arbitrary martingale difference sequence (i.e., reward noise does not systematically depend on its past values) and are uniformly bounded by \(\xi\)._

The following theorem proves the regret bound for the GP-TopK algorithm under Assumption 1 or 2.

**Theorem 3**.: _If either Assumptions 1 or 2 hold, setting \(\beta_{t}\) as \(2\log\left(\frac{|\mathcal{C}|\cdot\|\Pi^{k}\cdot|^{2}\cdot\pi^{2}}{6\delta}\right)\) and \(300\gamma_{t}\ln^{3}\left(\frac{t}{\delta}\right)\) respectively, the cumulative regret \(\mathcal{R}_{T}\) of the GP-TopK bandit algorithm for top-\(k\) recommendations can, with at least \(1-\delta\) probability, be bounded by \(\tilde{\mathcal{O}}(n\sqrt{C_{1}Tc(\log[\mathcal{C}]+k+\log(T^{2}\pi^{2}/6 \delta))})\) under Assumption 1, and \(\tilde{\mathcal{O}}(n\sqrt{C_{1}(2B^{2}c+300n^{2}c^{2}\ln^{3}(T/\delta))T})\) under Assumption 2. Here, \(C_{1}=\frac{8}{\log(1+\xi^{-2})}\), and \(\tilde{\mathcal{O}}\) excludes logarithmic factors related to \(n\), \(k\), and \(T\)._

Appendix B.4 provides the proof, leveraging the insight that \(\log\det\lvert I+\xi^{-2}\cdot K_{X_{T}}\rvert\) for any set \(X_{T}\) can be effectively bounded using the finite-dimensional feature vectors introduced in this work.

Specifically, Proposition 2 utilizes the feature vectors from Section 2. Building on Proposition 2, Theorem 3 establishes that the cumulative regret of the GP-TopK bandit algorithm grows sublinearly in \(T\) with high probability for both assumptions. Furthermore, this result also underscore the importance of using top-k ranking kernels, which improve the asymptotic order in terms of \(n\) by factors of \(n^{k/2-1}\) and \(n^{k-1}\) under Assumptions 1 and 2, respectively, compared to Srinivas et al. (2017). This improvement is substantial even for small values of \(k\), such as \(k=6\), as shown in Table 3.

## 5 Experiments

This section empirically evaluates the proposed GP-TopK bandit algorithms for the top-k recommendations using a simulation based on the MovieLens dataset (Bengio et al., 2010). The reliance on simulation for evaluating bandit algorithms is prevalent in the literature. It stems from the difficulty of conducting online evaluations in real-world bandit scenarios, mainly when there are combinatorial arms (Kirsh and Kirsh, 2017). Next, we provide details of the simulation setup and considered reward settings. Following that, we present results for the empirical regret for small and large numbers of arms below, respectively.

Simulation setup and reward settings.The bandit simulation setup follows the framework outlined by Jeunen et al. (2010), utilizing real-world datasets on user-item interactions. Specifically, we train user and item embeddings using a collaborative filtering approach (Bengio et al., 2010). The user embeddings are accessed by the bandit algorithms as context embeddings, while the item embeddings remain hidden. In the non-contextual setup, the first user from the dataset is chosen as a fixed context throughout the bandit algorithm run, allowing us to use the same reward functions as the contextual bandit algorithm.

For setting up the reward functions, we utilize a similarity function \(s(\mathbf{c},\theta)\coloneqq\varsigma(a\cdot(\mathbf{c}^{T}\theta)-b)\) to measure similarity between any user and item embeddings, where \(a\) and \(b\) are similarity score and shift scalars, respectively. The sigmoid function \(\varsigma\) maps similarity scores to a range between \(0\) and \(1\), enhancing the interpretability of the reward signal (Kirsh and Kirsh, 2017). We set \(a\) and \(b\) to \(6\) and \(0.3\), respectively, to fully utilize the range of the similarity function, as assessed by evaluating its value for many arms.

We set up two preliminary reward functions based on the similarity function \(s\). The first is the DCG metric, \(\hat{f}_{\text{dcg}}(\mathbf{c},\pi)=\sum_{i=1}^{k}\frac{1}{\log_{2}(i+1)}s( \mathbf{c},\theta_{\pi_{i}})\), where \(\mathbf{c}\) and \(\theta_{\pi_{i}}\) represent the context and item embeddings, respectively. The second is the diversity measure, \(\hat{f}_{\text{div}}(\pi)=\frac{1}{k^{2}}\sum_{i=1}^{k}\sum_{j=1}^{k}\theta_{ \pi_{j}}^{T}\theta_{\pi_{i}}\). These metrics quantify the relevance and diversity of top-k recommendations, respectively.

We use these functions in two contextual reward settings. The first setting focuses on normalized-DCG (n-DCG), \(\hat{f}_{\text{ndcg}}(\mathbf{c},\pi)=\frac{\hat{f}_{\text{drg}}(\mathbf{c}, \pi)}{\max_{s^{\prime}}\hat{f}_{\text{dz}}(\mathbf{c},\pi^{\prime})}\)(Bengio et al., 2010). The second setting combines \(\hat{f}_{\text{ndcg}}\) and \(\hat{f}_{\text{div}}\) as \(\hat{f}_{\text{ndcgdiv}}(\mathbf{c},\pi)=\lambda\cdot\hat{f}_{\text{ndcg}}( \mathbf{c},\pi)+(1-\lambda)\cdot\hat{f}_{\text{div}}(\pi)\), evaluating the aggregate effect of relevance and diversity. We set \(\lambda=0.75\) to emphasize relevance over diversity.

Evaluation for small arm space.This section presents empirical results for the cumulative regret of bandit algorithms with a limited number of arms. Specifically, with \(n=20\) and \(k=3\), there are \(6,840\) top-k rankings, allowing for an exhaustive search to optimize the acquisition function. All bandit algorithms run in batch mode, updating every five rounds. We consider both reward settings for contextual and non-contextual scenarios, using a subset of five users for the contextual setting.

\begin{table}
\begin{tabular}{c|c} \hline \hline \multicolumn{2}{c}{**Assumption 1**} \\ \hline
**Srinivas et al. (2010)** & **Proposed GP-TopK Algorithm** \\ \hline \(\tilde{O}\left(n^{\frac{k}{2}}\sqrt{C_{1}Tc\left(\log\lvert\mathcal{C}\rvert+k +\log\left(\frac{T^{2}\cdot 2}{68}\right)\right)}\right)\) & \(\tilde{O}\left(n\sqrt{C_{1}Tc\left(\log\lvert\mathcal{C}\rvert+k+\log\left( \frac{T^{2}\cdot 2}{68}\right)\right)}\right)\) \\ \hline \multicolumn{2}{c}{**Assumption 2**} \\ \hline
**Srinivas et al. (2010)** & **This work** \\ \hline \(\tilde{O}\left(n^{\frac{k}{2}}\sqrt{C_{1}Tc\left(2B^{2}+300n^{k}c\ln^{3}\left( \frac{T}{\delta}\right)\right)}\right)\) & \(\tilde{O}\left(n\sqrt{C_{1}Tc\left(2B^{2}+300n^{2}c\ln^{3}\left(\frac{T}{ \delta}\right)\right)}\right)\) \\ \hline \hline \end{tabular}
\end{table}
Table 3: Comparison with Srinivas et al. (2010) on regret bounds for the bandit algorithm under both assumptions. Definitions of notations are provided in the main text.

Several baselines are set to assess the benefits of ranking (Kendall) kernels. Section C details the remaining hyper-parameter configurations and details of other baseline bandit algorithms.

The _Random_ algorithm randomly recommends any k items. The _\(\epsilon\)-greedy_ algorithm alternates between recommending a random top-k ranking with a probability of \(\epsilon\) and choosing the top-k ranking with the highest observed mean reward. In contextual settings, _\(\epsilon\)-greedy_ differentiates arms for each unique context. Similarly, _MAB-UCB_ conceptualizes each ranking as an independent arm, an equivalent of using a direct delta kernel approach for GPs along with UCB \(\mathcal{AF}\). In contextual scenarios, _MAB-UCB_ also treats arms distinctly per context. Each variant of the top-k ranking kernel yields one variation of the proposed GP-TopK algorithm, namely, WK, CK, and WCK. Figure 2 presents empirical values of the cumulative regrets for the above baseline and the proposed GP-TopK algorithms. In all cases, across both reward settings and in both contextual and non-contextual setups, the variants of the proposed GP-TopK algorithm outperform baselines that do not use Kendall kernels, highlighting the significance of top-k ranking kernels for full bandit feedback. Specifically, the CK and WCK kernels significantly outperform the WK kernel regarding the converged values of the regret, with the WCK kernel further improving on the CK kernel variant.

**Evaluation for large arm space.** We evaluate bandit algorithms in a large arm space scenario with \(n=50\) and \(k=3\) and \(k=5\), resulting in \(1.1\times 10^{5}\) and \(1.1\times 10^{10}\) possible top-k rankings, respectively. Using local search, we focus on the nDCG reward. The remaining configuration is consistent with the small arm space setup. We use \(10\) restarts and \(5\) steps in each search direction for the local search, starting with \(1000\) initial candidates.

Figure 3 shows that the regret for the GP-TopK variants remains consistently lower even with a large arm space, despite the use of local search. The WCK approach significantly outperforms the CK, especially for \(k=5\), as illustrated in the right plot of Figure 3. Additional empirical results on the effectiveness of local search in a large arm space and other rewards are given in Appendix C.

## 6 Discussion

This work develops a contextual bandit algorithm for top-\(k\) recommendations using Gaussian processes with Kendall kernels in a full-bandit feedback setting, without restrictive assumptions about feedback or reward models. Gaussian processes provide computationally efficient model updates

Figure 3: Comparative evaluation of bandit algorithms for large arm spaces, with \(>1.1\times 10^{5}\) arms for the left plot and \(>1.1\times 10^{10}\) arms for the right plot. Cumulative regret with respect to the rounds of the bandit algorithm is depicted. Results are averaged over six trials. In both settings, the WCK approach outperforms other baselines. For more details, see the textual description.

Figure 2: Comparative evaluation of bandit algorithms: The cumulative regret \(R_{T}\) over \(T\) rounds is shown. Lower values indicate better performance. Plots (a) and (b) represent non-contextual settings for nDCG (\(\hat{f}_{\text{dkg}}\)) and nDCG + diversity (\(\hat{f}_{\text{dkgkg}}\)) rewards, respectively. Plots (c) and (d) show results for contextual settings for five users using the same rewards. The y-axis for (a) and (b) is on the left, and for (c) and (d) on the right. The GP-TopK algorithm with Kendall kernels, especially the weighted convolutional Kendall (WCK) kernel, outperforms others. Details on other algorithms are in the text. Results are averaged over six trials.

for accumulated feedback data, although inference can be challenging. We address this by deriving features for Kendall kernels tailored to top-\(k\) rankings, resulting in a faster inference algorithm that reduces complexity from \(\mathcal{O}(T^{4})\) to \(\mathcal{O}(T^{2})\). While demonstrated here for the product kernel between contexts and top-\(k\) rankings, these computational improvements extend naturally to other kernel types, such as additive kernels. Additionally, we address limitations of known variants and propose a more expressive Kendall kernel for top-\(k\) recommendations. Finally, we provide both theoretical and empirical results demonstrating the improved performance of the proposed GP-TopK algorithm.

Future Directions and Limitations.This work opens several research avenues. Efficient matrix-vector multiplication with Kendall kernel matrices can enable faster bandit algorithms with various acquisition functions, such as Thompson sampling and expected improvement. Exploring other kernels, like Mallow kernels, for top-k rankings and developing efficient algorithms for them is an intriguing direction, especially since the effectiveness of our algorithm depends on the function space induced by the RKHS of the underlying kernel. Assessing how well these kernels approximate various reward functions for top-k recommendations would provide valuable insights.

Exploring other bandit problem settings, such as stochastic item availability or delayed feedback, would enhance the applicability of this work to more complex scenarios. Extending the finite-dimensional GP framework to other acquisition functions using local search is another promising direction. One limitation of our regret analysis is that it does not account for approximations in the arm selection step due to local search [20]. This limitation is common in continuous domains, where optimizing acquisition functions often involves non-convex optimization [27].

Impact.This research advances bandit algorithms for top-k item recommendations. By improving recommendation efficiency and accuracy, our algorithms can enhance user experiences across platforms, promoting content relevancy and engagement. However, they may reinforce implicit biases in training data, limiting content diversity and entrenching prejudices. Therefore, monitoring over time is essential when deploying these algorithms in real-world environments.

## References

* Busa-Fekete et al. [2017] Robert Busa-Fekete, Balazs Szorenyi, Paul Weng, and Shie Mannor. Multi-objective bandits: Optimizing the generalized gini index. In _Proceedings of the 34th International Conference on Machine Learning (ICML)_, pages 625-634. PMLR, 2017.
* Deshwal et al. [2022] Aryan Deshwal, Syrine Belakaria, Janardhan Rao Doppa, and Dae Hyun Kim. Bayesian optimization over permutation spaces. In _Proceedings of the 25th Conference on Artificial Intelligence (AAAI)_, pages 6515-6523, 2022.
* Gardner et al. [2018] Jacob R. Gardner, Geoff Pleiss, David Bindel, Kilian Q. Weinberger, and Andrew Gordon Wilson. GPyTorch: Blackbox matrix-matrix Gaussian process inference with GPU acceleration. _Advances in the 31st Neural Information Processing Systems (NeurIPS)_, pages 7576-7586, 2018.
* Harper and Konstan [2015] F Maxwell Harper and Joseph A Konstan. The Movielens datasets: History and context. In _Transactions on Interactive Intelligent Systems_, volume 5, pages 1-19. ACM, 2015.
* Haussler [1999] David Haussler. Convolution kernels on discrete structures. Technical report, Technical report, Department of Computer Science, University of California Santa Cruz, 1999.
* Hu et al. [2008] Yifan Hu, Yehuda Koren, and Chris Volinsky. Collaborative filtering for implicit feedback datasets. In _8th IEEE International Conference on Data Mining (ICDM)_, pages 263-272. IEEE, 2008.
* Jarvelin and Kekalainen [2002] Kalervo Jarvelin and Jaana Kekalainen. Cumulated gain-based evaluation of ir techniques. In _ACM Transactions of Information Systems_, volume 20, pages 422-446, 2002.
* Jeunen and Goethals [2021] Olivier Jeunen and Bart Goethals. Top-k contextual bandits with equity of exposure. In _Proceedings of the 15th ACM Conference on Recommender Systems (RecSys)_, pages 310-320, 2021.
* Jiao and Vert [2015] Yunlong Jiao and Jean-Philippe Vert. The Kendall and Mallows kernels for permutations. In _Proceedings of the 32nd International Conference on Machine Learning (ICML)_, pages 1935-1944. PMLR, 2015.

* [10] Yunlong Jiao and Jean-Philippe Vert. The weighted Kendall and high-order kernels for permutations. In _Proceedings of the 35th International Conference on Machine Learning (ICML)_, volume 80, pages 2314-2322. PMLR, 2018.
* [11] Michael N. Katehakis and Arthur F. Veinott. The multi-armed bandit problem: Decomposition and computation. In _Mathematics of Operations Research_, volume 12, pages 262-268, 1987.
* [12] Maurice G Kendall. A new measure of rank correlation. _Biometrika_, 30(1/2):81-93, 1938.
* [13] Bart P. Knijnenburg, Martijn C. Willemsen, Zeno Gantner, Hakan Soncu, and Chris Newell. Explaining the user experience of recommender systems. In _User Modeling and User-Adapted Interaction_, volume 22, pages 441-504, 2011.
* [14] Andreas Krause and Cheng Ong. Contextual gaussian process bandit optimization. _Advances in the 24th Neural Information Processing Systems (NeurIPS)_, page 2447-2455, 2011.
* a survey. In _Knowledge Based Systems_, volume 123, pages 154-162, 2017.
* [16] Branislav Kveton, Csaba Szepesvari, Zheng Wen, and Azin Ashkan. Cascading bandits: Learning to rank in the cascade model. In _Proceedings of the 32nd International Conference on Machine Learning (ICML)_, pages 767-776. PMLR, 2015.
* [17] E. Lex, Dominik Kowald, Paul Setilinger, Thi Ngoc Trang Tran, Alexander Felfernig, and Markus Schedl. Psychology-informed recommender systems. In _Foundations and Trends in Information Retrieval_, volume 15, pages 134-242, 2021.
* [18] Zeyang Liu, Yiqun Liu, Ke Zhou, Min Zhang, and Shaoping Ma. Influence of vertical result in web search examination. In _Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval_, pages 193-202, 2015.
* [19] Changyong Oh, Jakub Tomczak, Efstratios Gavves, and Max Welling. Combinatorial bayesian optimization using the graph Cartesian product. _Advances in the 32nd Neural Information Processing Systems (NeurIPS)_, 2019.
* [20] My Phan, Yasin Abbasi Yadkori, and Justin Domke. Thompson sampling and approximate inference. _Advances in the 32nd Neural Information Processing Systems (NeurIPS)_, 32, 2019.
* [21] Lijing Qin, Shouyuan Chen, and Xiaoyan Zhu. Contextual combinatorial bandit and its application on diversified online recommendation. In _In Proceedings of the 2014 SIAM International Conference on Data Mining_, pages 461-469, 2014.
* [22] Carl Edward Rasmussen. _Evaluation of Gaussian processes and other methods for non-linear regression_. PhD thesis, University of Toronto Toronto, Canada, 1997.
* [23] Carl Edward Rasmussen. Gaussian Processes in Machine Learning. In _Advanced Lectures on Machine Learning_, pages 63-71. Springer, 2004.
* [24] Idan Rejwan and Yishay Mansour. Top-\(k\) combinatorial bandits with full-bandit feedback. In _Algorithmic Learning Theory_, pages 752-776. PMLR, 2020.
* [25] Yunus Saatci. _Scalable inference for structured Gaussian process models_. PhD thesis, University of Cambridge, 2012.
* [26] Dong-Her Shih, Kuan-Chu Lu, and Po-Yuan Shih. Exploring shopper's browsing behavior and attention level with an eeg biosensor cap. In _Brain Sciences_, volume 9, page 301, 2019.
* [27] Niranjan Srinivas, Andreas Krause, Sham M Kakade, and Matthias Seeger. Gaussian process optimization in the bandit setting: No regret and experimental design. In _Proceedings of the 27th International Conference on Machine Learning (ICML)_, pages 1015-1022, 2010.
* [28] Hastagiri P Vanchinathan, Isidor Nikolic, Fabio De Bona, and Andreas Krause. Explore-exploit in top-n recommender systems via Gaussian processes. In _Proceedings of the 8th ACM Conference on Recommender Systems (RecSys)_, pages 225-232, 2014.
* [29] Richard S. Varga. Gersgorin and his circles. 2004.
* [30] Siwei Wang and Wei Chen. Thompson sampling for combinatorial semi-bandits. In _Proceedings of the 35th International Conference on Machine Learning (ICML)_, pages 5114-5122, 2018.
* [31] Yingfei Wang, Hua Ouyang, Chu Wang, Jianhui Chen, Tsvetan Asamov, and Yi Chang. Efficient ordered combinatorial semi-bandits for whole-page recommendation. In _Proceedings of the 20th Conference on Artificial Intelligence (AAAI)_, volume 31, page 2746-2753, 2017.

* [32] Yue Wang, Dawei Yin, Luo Jie, Pengyuan Wang, Makoto Yamada, Yi Chang, and Qiaozhu Mei. Beyond ranking: Optimizing whole-page presentation. _Proceedings of the Ninth ACM International Conference on Web Search and Data Mining_, pages 103-112, 2016.
* [33] Christopher K. I. Williams and Carl Edward Rasmussen. Gaussian Processes for Regression. _Advances in the 9th Neural Information Processing Systems (NeurIPS)_, pages 514-520, 1996.
* [34] Dong Xin, Jiawei Han, and Kevin Chen-Chuan Chang. Progressive and selective merge: computing top-k with ad-hoc ranking functions. In _ACM SIGMOD Conference_, pages 103-114, 2007.

Kendall Kernels for Full and Top-k Rankings - Omitted Details

This section includes the proofs that were omitted from Section 2, presented in the following order:

* In Section A.1, we present proofs for Claims 2 and 3, which concern the feature representations of Kendall kernels.
* In Section A.2, we provide Algorithms 2 and a proof of its correctness for computing the WCK kernel in \(\mathcal{O}(k^{2})\) time, thereby proving Claim 1. Additionally, we extend this proof to cover the proof of correctness for Algorithm 3, which can compute the CK kernel in \(\mathcal{O}(k\log k)\), initially introduced by Jiao et al. [9]. The original paper presented the algorithm without a formal proof of correctness, a gap we address and fill in this section.
* Section A.3 details the proof for Theorem 1, discussing the matrix-vector multiplications with the Kendall kernel matrix for top-k rankings. This proof builds on the Algorithm 2 given for computing the WCK kernel for top-k rankings.

### Feature Representation for Kendall Kernels for Top-k Rankings

This section revisits the claims regarding the feature representations of the weighted Kendall kernel and the weighted convolutional Kendall kernel, subsequently providing the proofs for these claims mentioned earlier.

**Claim 2**.: _Let \(\phi^{wk}(\sigma):\Pi^{n}\mapsto\mathbb{R}^{\binom{n}{2}}\) be a vector indexed by unique item pairs \((i,j)\), defined as:_

\[\phi^{wk}_{i,j}(\sigma)\coloneqq\frac{1}{\sqrt{\binom{n}{2}}}\cdot w_{s}( \mathbf{p}_{i}^{\sigma},\mathbf{p}_{j}^{\sigma})\cdot\left(\mathbf{p}_{i<j}^ {\sigma}-\mathbf{p}_{i>j}^{\sigma}\right),\]

_where \(w_{s}\) is the symmetric weighting function in product-symmetric weights. Then, \(\phi^{wk}\) is a linear feature vector for the weighted Kendall kernel with product-symmetric weights \(w_{ps}\)._

Proof.: Following the definition of linear feature representation, we need to prove that \(k^{wk}(\sigma_{1},\sigma_{2})=\phi(\sigma_{1})^{T}\phi(\sigma_{2})\) for the product-symmetric weight kernel as given in Equation 5. Recalling from Equation 2, we have \(k^{wk}(\sigma_{1},\sigma_{2})\) as follows:

\[k^{wk}(\sigma_{1},\sigma_{2}) =\frac{1}{\binom{n}{2}}\cdot\sum_{i<j}w((\mathbf{p}_{i}^{\sigma_ {1}},\mathbf{p}_{j}^{\sigma_{1}}),(\mathbf{p}_{i}^{\sigma_{2}},\mathbf{p}_{j }^{\sigma_{2}}))\cdot\eta_{i,j}(\sigma_{1},\sigma_{2}),\] \[=\frac{1}{\binom{n}{2}}\cdot\sum_{i<j}w_{s}(\mathbf{p}_{i}^{ \sigma_{1}},\mathbf{p}_{j}^{\sigma_{1}})\cdot w_{s}(\mathbf{p}_{i}^{\sigma_{ 2}},\mathbf{p}_{j}^{\sigma_{2}})\cdot\eta_{i,j}(\sigma_{1},\sigma_{2}),\] (7)

where the second line incorporates the use of the product-symmetric weight kernel. Next, our focus shifts to the simplification of \(\eta_{i,j}(\sigma_{1},\sigma_{2})\), which is elaborated as follows:

\[\eta_{i,j}(\sigma_{1},\sigma_{2}) =\mathbf{p}_{i<j}^{\sigma_{1}}\cdot\mathbf{p}_{i<j}^{\sigma_{2} }+\mathbf{p}_{i>j}^{\sigma_{1}}\cdot\mathbf{p}_{i>j}^{\sigma_{2}}-\mathbf{p}_ {i<j}^{\sigma_{1}}\cdot\mathbf{p}_{i>j}^{\sigma_{2}}-\mathbf{p}_{i>j}^{\sigma _{1}}\cdot\mathbf{p}_{i<j}^{\sigma_{2}},\] \[=\mathbf{p}_{i<j}^{\sigma_{1}}\cdot(\mathbf{p}_{i<j}^{\sigma_{2} }-\mathbf{p}_{i>j}^{\sigma_{2}})+\mathbf{p}_{i>j}^{\sigma_{1}}\cdot(\mathbf{p }_{i>j}^{\sigma_{2}}-\mathbf{p}_{i<j}^{\sigma_{2}}),\] \[=(\mathbf{p}_{i<j}^{\sigma_{1}}-\mathbf{p}_{i>j}^{\sigma_{1}}) \cdot(\mathbf{p}_{i<j}^{\sigma_{2}}-\mathbf{p}_{i>j}^{\sigma_{2}}).\]

Combining the above factorization of \(\eta_{i,j}\) with Equation 7, we get:

\[k^{wk}(\sigma_{1},\sigma_{2})=\frac{1}{\binom{n}{2}}\cdot\sum_{i<j}w_{s}( \mathbf{p}_{i}^{\sigma_{1}},\mathbf{p}_{j}^{\sigma_{1}})\cdot w_{s}(\mathbf{ p}_{i}^{\sigma_{2}},\mathbf{p}_{j}^{\sigma_{2}})\cdot(\mathbf{p}_{i<j}^{\sigma_{1}}- \mathbf{p}_{i>j}^{\sigma_{1}})\cdot(\mathbf{p}_{i<j}^{\sigma_{2}}-\mathbf{p}_ {i>j}^{\sigma_{2}})\]\[=\frac{1}{\binom{n}{2}}\cdot\sum_{i<j}\phi_{i,j}^{wk}(\sigma_{1})\cdot \phi_{i,j}^{wk}(\sigma_{2})\] \[=\phi(\sigma_{1})^{T}\phi(\sigma_{2}).\]

Proof.: The main idea revolves around leveraging the feature representation of the Weighted Kendall kernel for a full ranking and the linearity of the convolution operation. It is already established that \(k^{wk}(\sigma_{1},\sigma_{2})=\phi^{wk}(\sigma_{1})^{T}\phi^{wk}(\sigma_{2})\), as demonstrated in Claim 2. Recall that the WCK kernel requires a double summation over pairs of rankings from \(B_{\pi_{1}}\) and \(B_{\pi_{2}}\), which represent the sets of full rankings consistent with their respective top-k rankings, as described in Equation 4. We simplify the WCK kernel as follows:

\[k^{wck}(\pi_{1},\pi_{2}) =\frac{1}{|B_{\pi_{1}}|}\cdot\frac{1}{|B_{\pi_{2}}|}\cdot\sum_{ \sigma_{1}\in B_{\pi_{1}}}\sum_{\sigma_{2}\in B_{\pi_{2}}}\phi^{wk}(\sigma_{1 })^{T}\phi^{wk}(\sigma_{2})\] \[=\left(\frac{1}{|B_{\pi_{1}}|}\cdot\sum_{\sigma_{1}\in B_{\pi_{1 }}}\phi^{wk}(\sigma_{1})^{T}\right)\cdot\underbrace{\left(\frac{1}{|B_{\pi_{2 }}|}\cdot\sum_{\sigma_{2}\in B_{\pi_{2}}}\phi^{wk}(\sigma_{2})\right)}_{:= \phi^{wck}(\pi_{2})}\] \[=\phi^{wck}(\pi_{1})^{T}\phi^{wck}(\pi_{2}).\]

The simplification above reveals that the feature representation, \(\phi^{wck}\), for the WCK kernel, is a \(\binom{n}{2}\) dimensional vector and can be indexed by unique pairs of items \((i,j)\), much like the \(\phi^{wk}\). However, the double summation is over an exponentially large number of pairs of rankings. Moving forward, we shift our focus to the individual entries of this representation involving this summation, elucidating the analytical values within the summation by exploring four unique cases, each dependent on whether these specific items fall within the top-k rankings.

In Case 1, we examine the scenario where items \(i\) and \(j\) are within the top-k ranking \(\pi\). Here, the focus is on the feature representation of the pair, specifically when both elements are ranked among the top-k positions.

Case 1: \(\mathbf{p}_{i}^{\pi}\in[k]\) and \(\mathbf{p}_{j}^{\pi}\in[k]\).

\[\phi_{i,j}^{wck}(\pi) =\frac{1}{|B_{\pi}|}\cdot\sum_{\sigma\in B_{\pi}}\frac{1}{\sqrt{ \binom{n}{2}}}\cdot w_{s}(\mathbf{p}_{i}^{\pi},\mathbf{p}_{j}^{\sigma})\cdot \left(\mathbf{p}_{i<j}^{\sigma}-\mathbf{p}_{i>j}^{\sigma}\right)\] \[=\frac{1}{|B_{\pi}|}\cdot\frac{1}{\sqrt{\binom{n}{2}}}\cdot w_{s }(\mathbf{p}_{i}^{\pi},\mathbf{p}_{j}^{\pi})\cdot\left(\sum_{\sigma\in B_{ \pi}}\mathbf{p}_{i<j}^{\sigma}-\sum_{\sigma\in B_{\pi}}\mathbf{p}_{i>j}^{ \sigma}\right)\]\[=\frac{1}{|B_{\pi}|}\cdot\frac{1}{\sqrt{\binom{n}{2}}}\cdot w_{s}( \mathbf{p}_{i}^{\pi},\mathbf{p}_{j}^{\pi})\cdot\left(|B_{\pi}|\cdot\mathfrak{p}_{ i<j}^{\pi}-|B_{\pi}|\cdot\mathfrak{p}_{i>j}^{\pi}\right)\] \[=\frac{1}{\sqrt{\binom{n}{2}}}\cdot w_{s}(\mathbf{p}_{i}^{\pi}, \mathbf{p}_{j}^{\pi})\cdot\left(\mathfrak{p}_{i<j}^{\pi}-\mathfrak{p}_{i>j}^{ \pi}\right).\] (8)

The simplification in lines 3rd and 4th follows from the fact that any full ranking \(\sigma\in B_{\pi}\), consistent with the top-k ranking \(\pi\), the relative ranks and weights of items \(i\) and \(j\) remains unchanged, given \(\mathbf{p}_{i}^{\pi}\in[k]\) and \(\mathbf{p}_{j}^{\pi}\in[k]\). Concretely, this implies \(\mathfrak{p}_{i<j}^{\sigma}=\mathfrak{p}_{i<j}^{\pi}\) for all \(\sigma\in B_{\pi}\) and similar with the other term.

In Case 2, we analyze when item \(i\) is in the top-k ranking while item \(j\) is not.

Case 2: \(\mathbf{p}_{i}^{\pi}\in[k]\) and \(\mathbf{p}_{j}^{\pi}\notin[k]\).

\[\phi_{i,j}^{wck}(\pi) =\frac{1}{|B_{\pi}|}\cdot\sum_{\sigma\in B_{\pi}}\frac{1}{\sqrt{ \binom{n}{2}}}\cdot w_{s}(\mathbf{p}_{i}^{\sigma},\mathbf{p}_{j}^{\sigma}) \cdot\left(\mathfrak{p}_{i<j}^{\sigma}-\mathfrak{p}_{i>j}^{\sigma}\right)\] \[=\frac{1}{|B_{\pi}|}\cdot\frac{1}{\sqrt{\binom{n}{2}}}\cdot\sum_ {\sigma\in B_{\pi}}w_{s}(\mathbf{p}_{i}^{\sigma},\mathbf{p}_{j}^{\sigma}) \cdot(1-0)\ \ (\text{since}\ \mathbf{p}_{i}^{\pi}\in[k]\ \ \text{and}\ \mathbf{p}_{j}^{\pi}\notin[k])\] \[=\frac{1}{|B_{\pi}|}\cdot\frac{1}{\sqrt{\binom{n}{2}}}\cdot\sum_ {\sigma\in B_{\pi}}w_{s}(\mathbf{p}_{i}^{\pi},\mathbf{p}_{j}^{\sigma}).\]

Next, every possible consistent ranking is considered jointly while fixating on a specific rank outside top-k elements, leading to \((n-k-1)!\) different rankings. Given that \(|B_{\pi}|=(n-k)!\), we can refine the above expression as follows:

\[\phi_{i,j}^{wck}(\pi) =\frac{1}{|B_{\pi}|}\cdot\frac{1}{\sqrt{\binom{n}{2}}}\cdot\sum_ {l=k+1}^{n}w_{s}(\mathbf{p}_{i}^{\pi},l)\cdot(n-k-1)!\] \[=\frac{(n-k-1)!}{|B_{\pi}|}\cdot\frac{1}{\sqrt{\binom{n}{2}}} \cdot\sum_{l=k+1}^{n}w_{s}(\mathbf{p}_{i}^{\pi},l)\] \[=\frac{1}{\sqrt{\binom{n}{2}}}\cdot\frac{1}{n-k}\cdot\sum_{l=k+1 }^{n}w_{s}(\mathbf{p}_{i}^{\pi},l)\] \[=\frac{1}{\sqrt{\binom{n}{2}}}\cdot w_{s}(\mathbf{p}_{i}^{\pi}, \cdot).\] (9)

In Case 3, we analyze when item \(i\) is not in the top-k ranking while item \(j\) is.

Case 3: \(\mathbf{p}_{i}^{\pi}\notin[k]\) and \(\mathbf{p}_{j}^{\pi}\in[k]\). Similar to case 2, the simplification follows analogously, with the only change being \(\mathbf{1}_{\mathbf{p}_{i<j}^{\pi}}-\mathbf{1}_{\mathbf{p}_{i>j}^{\pi}}=-1\) instead of \(1\). Thus, by symmetry between \(i\) and \(j\), we have the following:

\[\phi_{i,j}^{wck}(\pi)=\frac{-1}{\sqrt{\binom{n}{2}}}\cdot w_{s}( \cdot,\mathbf{p}_{j}^{\pi})=\frac{-1}{\sqrt{\binom{n}{2}}}\cdot w_{s}(\mathbf{ p}_{j}^{\pi},\cdot)\qquad\text{(using symmetry of $w_{s}$)}.\] (10)

Lastly, in Case 4, we analyze when items \(i\) and \(j\) are not in the top-k ranking.

Case 4: \(\mathbf{p}_{i}^{\sigma}\notin[k]\) and \(\mathbf{p}_{j}^{\sigma}\notin[k]\).

\[\phi_{i,j}^{wck}(\pi)=\frac{1}{|B_{\pi}|}\cdot\sum_{\sigma\in B_{\pi}}\phi_{i,j} ^{wk}(\sigma)\]\[=\frac{1}{|B_{\pi}|}\cdot\frac{1}{\sqrt{\binom{n}{2}}}\cdot\sum_{ \sigma\in B_{\pi}}w_{s}(\mathbf{p}_{i}^{\sigma},\mathbf{p}_{j}^{\sigma})\cdot \left(\mathbf{p}_{i<j}^{\sigma}-\mathbf{p}_{i>j}^{\sigma}\right)\] \[=0\quad\text{(by symmetry)}.\] (11)

The result of zero arises from symmetry. Since \(\mathbf{p}_{i}^{\sigma}\) and \(\mathbf{p}_{j}^{\sigma}\) are not in the top-\(k\) ranking, they are treated symmetrically in the summation overall rankings in \(B_{\pi}\). For any ranking \(\sigma\), suppose \(\mathbf{p}_{i}^{\sigma}=l\) and \(\mathbf{p}_{j}^{\sigma}=m\), there exists a corresponding ranking \(\sigma^{\prime}\) such that only the items \(i\) and \(j\) are swapped. Therefore, jointly, these two rankings yield \(w_{s}(l,m)\) and \(-w_{s}(l,m)\). Since \(w_{s}\) is symmetric, the overall contribution from each pair of such rankings is zero. Hence, the entire summation nets to zero.

Thus, with the explanation provided for each case and combining results from Equations 8, 9, 10 and 11, it's trivial to validate the Claim 3, i.e., \(\phi_{i,j}^{vck}(\pi)=\frac{1}{\sqrt{\binom{n}{2}}}\cdot\mathbf{w}_{i,j}^{ vck}(\pi)\cdot\left(\mathbf{p}_{i<j}^{\pi}-\mathbf{p}_{i>j}^{\pi}\right)\) for all unique pair of items. From Case 4, we have \(\mathcal{O}((n-k)^{2})\) entries leaving at max only \(\mathcal{O}(k^{2}+2nk)\) non-zero entries. 

### Algorithms for Computing Kendall Kernels for top-k Rankings

In this section, we provide and delve into the proofs of Algorithms 2 and 3 for the weighted convolutional Kendall kernel and the convolutional Kendall kernel, as previously discussed in Section 2. Section A.2.1 for valid both the correctness and computational complexity of Algorithm 2 as given earlier in Claim 1. Following this, Section A.2.2 revisits Algorithm 3, initially introduced by Jiao et al. [10]. The original publication presented the algorithm without formal proof of its correctness, which we rectify and offer in Section A.2.2.

#### a.2.1 Efficiently Computing the Weighted Convolutional Kendall Kernel

This section provides a proof to Claim 1 to establish the efficiency and accuracy of Algorithm 2 in computing the weighted convolutional Kendall kernel, as specified in Equation 4, with a focus on its computational complexity.

**Claim 1**.: _The weighted convolutional Kendall kernel (Equation 4) with product-symmetric rank weights (Equation 5) can be computed in \(\mathcal{O}(k^{2})\) time._

Proof.: The claim is proven through Algorithm 2, where we establish its correctness and demonstrate its computation requirement is \(O(k^{2})\). The essence of our proof centers on analyzing the feature representation of the WCK kernel, \(\phi^{wck}\), as outlined in Claim 3. The feature vectors of \(\phi^{wck}\) reside in a \(\binom{n}{2}\) dimensional space, indexed by pairs of items. Our approach is to demonstrate that Algorithm 2 accurately computes the right-hand side (RHS) of the equation \(k^{wck}(\pi_{1},\pi_{2})=\phi^{wck}(\pi_{1})^{T}\phi^{wck}(\pi_{2})\). This involves a summation over item pairs, expressed as \(k^{wck}(\pi_{1},\pi_{2})=\sum_{l<m}\phi^{wck}_{l,m}(\pi_{1})^{T}\phi^{wck}_{l, m}(\pi_{2})\).

Our proof analyzes various scenarios: cases where pairs of items, namely \(l\) and \(m\), fall within the top-k, scenarios with one item within the top-k and the other outside, and situations where neither item is within the top-k. Each of these cases contributes distinctively to the computation of the overall kernel, resulting in different terms in the algorithmic computation. This is encapsulated in Algorithm 2, where \(k^{wck}(\pi_{1},\pi_{2})=\sum_{i=1}^{5}s_{i}(\pi_{1},\pi_{2})\), and each \(s_{i}\) corresponds to the terms given earlier in Algorithm 2 from Section 2.

Before proceeding with the cases of this summation as given in Table 4, we recall the notations utilized by Algorithm 3 in Definition 1. Also, remember that we will be proving for product-symmetric weights as given in Equation 5, where, \(w_{s}:[n]\times[n]\mapsto\mathbb{R}^{n}\) and its one-dimensional marginals are \(w_{s}(\ell,\cdot)=\frac{1}{n-k}\sum_{j=k+1}^{n}w_{s}(\ell,j)\) Table 4 shows how these cases are organized and relate to different \(s_{i}\) terms required for computing the WCK kernel. The key strategy involves breaking down the kernel's computation into cases based on the positioning of item pairs within the top-k rankings. In case 1, we consider all the scenarios when both indices are within the set of items in both top-k rankings, i.e., all items in the set \(I_{1}\cup I_{2}\).

**Definition 1**.: _Algorithm 2 and 3 and utilize following notations._

* \(I_{1}\) _and_ \(I_{2}\) _are the sets of items in rankings_ \(\pi_{1}\) _and_ \(\pi_{2}\)_, respectively._
* \(\sigma_{1}\in\Pi^{|I_{1}|}\) _and_ \(\tau_{1}\in\Pi^{|I_{1}\cap I_{2}|}\) _are the full rankings of_ \(I_{1}\) _and_ \(I_{1}\cap I_{2}\)_, both consistent with the input top-k ranking_ \(\pi_{1}\)_. I.e., relative ranks of items is same yielding_ \(\forall l,m\in I_{1}\cap I_{2},\ \mathfrak{p}_{\cdot>j}^{\tau_{1}}=\mathfrak{p}_{ \cdot>j}^{\tau_{1}}\)_._
* _Analogously,_ \(\sigma_{2}\) _and_ \(\tau_{2}\) _are constructed utilizing the set_ \(I_{2}\) _and ranking_ \(\pi_{2}\)_._

```
1:Two permutations \(\pi_{1},\pi_{2}\in\Pi^{k}\). Ranking weighting function \(w_{s}:[n]\times[n]\mapsto\mathbb{R}^{n}\) and its one dimensional marginals are \(w_{s}(\ell,\cdot)=\frac{1}{n-k}\sum_{j=k+1}^{n}w_{s}(\ell,j)\).
2:Convolutional Weighted Kendall kernel \(k^{\text{wck}}(\pi_{1},\pi_{2})\). - Let \(I_{1}\) and \(I_{2}\) be the sets of items in rankings \(\pi_{1}\) and \(\pi_{2}\), respectively.
3:if\(|I_{1}\cap I_{2}|\geq 2\)then
4:\(s_{1}(\pi_{1},\pi_{2})=\frac{1}{\binom{n}{2}}\sum\limits_{1\leq l<m\leq n|l,m \in I_{1}\cap I_{2}}w_{s}(\mathfrak{p}_{l}^{\pi_{1}},\mathfrak{p}_{m}^{\pi_{1}} )\cdot w_{s}(\mathfrak{p}_{l}^{\pi_{2}},\mathfrak{p}_{m}^{\pi_{2}})\cdot\eta_{ l,m}(\pi_{1},\pi_{2})\)
5:endif
6:if\(|I_{1}\cap I_{2}|\geq 1\) and \(|I_{1}\setminus I_{2}|\geq 1\)then
7:\(s_{2}(\pi_{1},\pi_{2})=\frac{1}{\binom{n}{2}}\cdot\sum\limits_{l\in I_{1} \cap I_{2}|m\in I_{1}\setminus I_{2}}w_{s}(\mathfrak{p}_{l}^{\pi_{1}}, \mathfrak{p}_{m}^{\pi_{1}})\cdot w_{s}(\mathfrak{p}_{l}^{\pi_{2}},\cdot) \left(\mathfrak{p}_{l<m}^{\pi_{1}}-\mathfrak{p}_{l>m}^{\pi_{1}}\right)\)
8:endif
9:if\(|I_{1}\cap I_{2}|\geq 1\) and \(|I_{2}\setminus I_{1}|\geq 1\)then
10:if\(|I_{1}\setminus I_{2}|\geq 1\) and \(|I_{2}\setminus I_{1}|\geq 1\)then
11:\(s_{4}(\pi_{1},\pi_{2})=-\frac{1}{\binom{n}{2}}\cdot\sum\limits_{l\in I_{1} \setminus I_{2}|m\in I_{2}\setminus I_{1}}w_{s}(\mathfrak{p}_{l}^{\pi_{1}}, \cdot)\cdot w_{s}(\mathfrak{p}_{m}^{\pi_{2}},\cdot)\)
12:endif
13:if\(|I_{1}\cap I_{2}|\geq 1\) and \(|[n]\setminus(I_{1}\cup I_{2})|\geq 1\)then
14:\(s_{5}(\pi_{1},\pi_{2})=\frac{1}{\binom{n}{2}}\cdot(n-|I_{1}\cup I_{2}|)\cdot \sum\limits_{l\in I_{1}\cap I_{2}}w_{s}(\mathfrak{p}_{l}^{\pi_{1}},\cdot) \cdot w_{s}(\mathfrak{p}_{l}^{\pi_{2}},\cdot)\)
15:endif
16:\(k^{\text{wck}}(\pi_{1},\pi_{2})=s_{1}(\pi_{1},\pi_{2})+s_{2}(\pi_{1},\pi_{2}) +s_{3}(\pi_{1},\pi_{2})+s_{4}(\pi_{1},\pi_{2})+s_{5}(\pi_{1},\pi_{2})\) ```

**Algorithm 2** Computing Weighted Convolutional Kendall Kernel

Case 1:The pair \((l,m)\in I_{1}\cup I_{2}\) falls within the top-k, leading to three distinct cases. Below, we provide \(s_{i}\) terms for each case as given in Table 4.

\begin{table}
\begin{tabular}{|c|l|} \hline
**Case** & **Description** \\ \hline
1 & Both items \((l,m)\) in \(I_{1}\cup I_{2}\). Branches into the following three sub-cases based on the presence of items in \(I_{1}\cap I_{2}\): \\  & 1-a: Both items in \(I_{1}\cap I_{2}\). The concerned term is \(s_{1}\). \\  & 1-b: One item in \(I_{1}\cap I_{2}\). Subdivided into 1-b-i (other in \(I_{1}\setminus I_{2}\)) and 1-b-ii (other in \(I_{2}\setminus I_{3}\)), concerned terms are \(s_{2}\) and \(s_{3}\). \\  & 1-c: No item in \(I_{1}\cap I_{2}\). Addresses cases where \(l\) and \(m\) are in different sets (\(I_{1}\setminus I_{2}\) and \(I_{2}\setminus I_{1}\)); concerned term is \(s_{4}\). \\ \hline
2 & One item in \(I_{1}\cup I_{2}\). I.e., either \(l\) is \(I_{1}\cup I_{2}\) or \(m\) is in \(I_{1}\cup I_{2}\), leading to sub-cases \\  & 2-a and 2-b; concerned term is \(s_{5}\). \\ \hline
3 & No item in \(I_{1}\cup I_{2}\). Addresses the scenario where neither \(l\) nor \(m\) is in \(I_{1}\cup I_{2}\); \\  & value trivially zero. \\ \hline \end{tabular}
\end{table}
Table 4: Case categorization for the proof of Algorithms 2 and 3 based on item pair ranks, where \(I_{1}\) and \(I_{2}\) are the sets of items for top-k rankings \(\pi_{1}\) and \(\pi_{2}\), respectively.

Case 1-a:Two items in \(I_{1}\cap I_{2}\), meaning both \(l\) and \(m\) belong to \(I_{1}\cap I_{2}\). Using Claim 3 regarding the feature vector \(\phi^{wck}\), we simplify \(s_{1}\) as follows:

\[s_{1}(\pi_{1},\pi_{2}) =\sum_{1\leq l<m\leq n|l,m\in I_{1}\cap I_{2}}\phi^{wck}_{l,m}(\pi_ {1})\cdot\phi^{wck}_{l,m}(\pi_{2})\] \[=\sum_{1\leq l<m\leq n|l,m\in I_{1}\cap I_{2}}\frac{1}{\sqrt{ \binom{n}{2}}}\cdot w_{s}(\mathbf{p}_{l}^{\pi_{1}},\mathbf{p}_{m}^{\pi_{1}}) \cdot\left(\mathbf{p}_{l<m}^{\pi_{1}}-\mathbf{p}_{l>m}^{\pi_{1}}\right)\] \[\qquad\qquad\qquad\qquad\qquad\cdot\frac{1}{\sqrt{\binom{n}{2}}} \cdot w_{s}(\mathbf{p}_{l}^{\pi_{2}},\mathbf{p}_{m}^{\pi_{2}})\cdot\left( \mathbf{p}_{l<m}^{\pi_{2}}-\mathbf{p}_{l>m}^{\pi_{2}}\right)\] \[=\frac{1}{\binom{n}{2}}\sum_{1\leq l<m\leq n|l,m\in I_{1}\cap I_{2 }}w_{s}(\mathbf{p}_{l}^{\pi_{1}},\mathbf{p}_{m}^{\pi_{1}})\cdot w_{s}(\mathbf{ p}_{l}^{\pi_{2}},\mathbf{p}_{m}^{\pi_{2}})\cdot\eta_{l,m}(\pi_{1},\pi_{2}).\] (12)

Case 1-b:When one item is in \(I_{1}\cap I_{2}\), the other must reside either in \(I_{1}\setminus I_{2}\) or \(I_{2}\setminus I_{1}\), thus leading to two distinct sub-cases. This is specified in Table 4. Concretely, if the other item is in \(I_{1}\setminus I_{2}\), it contributes to the \(s_{2}\) terms, whereas if it's in \(I_{2}\setminus I_{1}\), it contributes to the \(s_{3}\) terms.

Corresponding to Case 1-b-i, when the other item is in \(I_{1}\cap I_{2}\), i.e., \(s_{2}\) is the term corresponding to indices where \(l\) is in \(I_{1}\cap I_{2}\) and \(m\) in \(I_{1}\setminus I_{2}\), or the reverse, represented by partial sums \(u\) and \(v\). For the partial sum \(u\), with \(l\) in \(I_{1}\cap I_{2}\) and \(m\) in \(I_{1}\setminus I_{2}\), we find that \(\mathbf{p}_{l}^{\pi_{2}}\) is in \([k]\), while \(\mathbf{p}_{m}^{\pi_{2}}\) is not. The simplification of \(u\) proceeds using Claim 3 as follows:

\[u =\sum_{1\leq l<m\leq n|l\in I_{1}\cap I_{2}|m\in I_{1}\setminus I _{2}}\frac{1}{\sqrt{\binom{n}{2}}}\cdot w_{s}(\mathbf{p}_{l}^{\pi_{1}}, \mathbf{p}_{m}^{\pi_{1}})\cdot\left(\mathbf{p}_{l<m}^{\pi_{1}}-\mathbf{p}_{l> m}^{\pi_{1}}\right)\] \[\qquad\qquad\qquad\qquad\qquad\cdot\frac{1}{\sqrt{\binom{n}{2}}} \cdot w_{s}(\mathbf{p}_{l}^{\pi_{2}},\cdot)\left(\mathbf{p}_{l<m}^{\pi_{2}}- \mathbf{p}_{l>m}^{\pi_{2}}\right)\] \[=\frac{1}{\binom{n}{2}}\sum_{1\leq l<m\leq n|l\in I_{1}\cap I_{2 }|m\in I_{1}\setminus I_{2}}w_{s}(\mathbf{p}_{l}^{\pi_{1}},\mathbf{p}_{m}^{ \pi_{1}})\cdot w_{s}(\mathbf{p}_{l}^{\pi_{2}},\cdot)\left(\mathbf{p}_{l<m}^{ \pi_{1}}-\mathbf{p}_{l>m}^{\pi_{1}}\right).\]

Similarly, the partial sum \(v\) can be simplified as follows:

\[v =\sum_{1\leq l<m\leq n|m\in I_{1}\cap I_{2}|l\in I_{1}\setminus I _{2}}\frac{1}{\sqrt{\binom{n}{2}}}\cdot w_{s}(\mathbf{p}_{l}^{\pi_{1}}, \mathbf{p}_{m}^{\pi_{1}})\cdot\left(\mathbf{p}_{l<m}^{\pi_{1}}-\mathbf{p}_{l> m}^{\pi_{1}}\right)\] \[\qquad\qquad\qquad\qquad\qquad\cdot\frac{-1}{\sqrt{\binom{n}{2}}} \cdot w_{s}(\mathbf{p}_{l}^{\pi_{2}},\cdot)\cdot\left(\mathbf{p}_{l<m}^{\pi_{2 }}-\mathbf{p}_{l>m}^{\pi_{2}}\right)\] \[=\frac{-1}{\binom{n}{2}}\sum_{1\leq l<m\leq n|m\in I_{1}\cap I_{2 }|l\in I_{1}\setminus I_{2}}w_{s}(\mathbf{p}_{l}^{\pi_{1}},\mathbf{p}_{m}^{ \pi_{1}})\cdot w_{s}(\mathbf{p}_{l}^{\pi_{2}},\cdot)\cdot\left(\mathbf{p}_{l< m}^{\pi_{1}}-\mathbf{p}_{l>m}^{\pi_{1}}\right)\] \[=\frac{-1}{\binom{n}{2}}\sum_{1\leq m<l\leq n|l\in I_{1}\cap I_{2 }|m\in I_{1}\setminus I_{2}}w_{s}(\mathbf{p}_{m}^{\pi_{1}},\mathbf{p}_{l}^{ \pi_{1}})\cdot w_{s}(\mathbf{p}_{m}^{\pi_{2}},\cdot)\cdot\left(\mathbf{p}_{m<l }^{\pi_{1}}-\mathbf{p}_{l>m}^{\pi_{1}}\right)\] \[=\frac{1}{\binom{n}{2}}\sum_{1\leq m<l\leq n|l\in I_{1}\cap I_{2 }|m\in I_{1}\setminus I_{2}}w_{s}(\mathbf{p}_{l}^{\pi_{1}},\mathbf{p}_{m}^{ \pi_{1}})\cdot w_{s}(\mathbf{p}_{l}^{\pi_{2}},\cdot)\left(\mathbf{p}_{l<m}^{ \pi_{1}}-\mathbf{p}_{l>m}^{\pi_{1}}\right).\]

In the above, the first two lines use results from Claim 3 and use similarity of \(w_{s}\). In the following line, \(l\) and \(m\) are exchanged. Lastly, the negative sign is pushed into the indicator functions to make the summand function of this partial sum \(v\) similar to the partial sum \(u\), and the similarity of the \(w_{s}\) is utilized. The above partial sums simplify \(s_{2}\) as follows:

\[s_{2}(\pi_{1},\pi_{2})=\frac{1}{\binom{n}{2}}\cdot\sum_{l\in I_{1}\cap I_{2}|m \in I_{1}\setminus I_{2}}w_{s}(\mathbf{p}_{l}^{\pi_{1}},\mathbf{p}_{m}^{\pi_{1}}) \cdot w_{s}(\mathbf{p}_{l}^{\pi_{2}},\cdot)\left(\mathbf{p}_{l<m}^{\pi_{1}}- \mathbf{p}_{l>m}^{\pi_{1}}\right).\] (13)Analogously, in Case 1-b-ii, we deduce the corresponding term \(s_{3}\) for the pair of indices as described in Table 4 through symmetry. Specifically, the term \(s_{3}\) can be outlined as follows:

\[s_{3}(\pi_{1},\pi_{2})=\frac{1}{\binom{n}{2}}\cdot\sum_{l\in I_{1}\cap I_{2}|m\in I _{2}\setminus I_{1}}w_{s}(\mathbf{p}_{l}^{\pi_{1}},\cdot)\cdot w_{s}(\mathbf{p }_{l}^{\pi_{2}},\mathbf{p}_{m}^{\pi_{2}})\cdot\left(\mathbf{p}_{l<m}^{\pi_{2}}- \mathbf{p}_{l>m}^{\pi_{2}}\right).\] (14)

Case 1-c:Both items are outside \(I_{1}\cap I_{2}\), specifically, \(l\in I_{1}\setminus I_{2}\) and \(m\in I_{2}\setminus I_{1}\) or the reverse. Like Case 1-b-i, we divide \(s_{4}\) into partial summations \(u\) and \(v\). Now, we calculate \(u\) under the condition that \(l\in I_{1}\setminus I_{2}\) and \(m\in I_{2}\setminus I_{1}\).

\[u =\sum_{1\leq l<m\leq n|l\in I_{1}\setminus I_{2}|m\in I_{2} \setminus I_{1}}\frac{1}{\sqrt{\binom{n}{2}}}\cdot w_{s}(\mathbf{p}_{l}^{\pi_ {1}},\cdot)\cdot\left(\mathbf{p}_{l<m}^{\pi_{1}}-\mathbf{p}_{l>m}^{\pi_{1}}\right)\] \[\cdot\frac{1}{\sqrt{\binom{n}{2}}}\cdot w_{s}(\mathbf{p}_{m}^{\pi _{2}},\cdot)\cdot\left(\mathbf{p}_{l<m}^{\pi_{2}}-\mathbf{p}_{l>m}^{\pi_{2}} \right),\] \[=\frac{1}{\binom{n}{2}}\cdot\sum_{1\leq l<m\leq n|l\in I_{1} \setminus I_{2}|m\in I_{2}\setminus I_{1}}w_{s}(\mathbf{p}_{l}^{\pi_{1}}, \cdot)\cdot\left(1-0\right)\cdot w_{s}(\mathbf{p}_{m}^{\pi_{2}},\cdot)\cdot \left(0-1\right),\] \[=\frac{-1}{\binom{n}{2}}\cdot\sum_{1\leq l<m\leq n|l\in I_{1} \setminus I_{2}|m\in I_{2}\setminus I_{1}}w_{s}(\mathbf{p}_{l}^{\pi_{1}}, \cdot)\cdot w_{s}(\mathbf{p}_{m}^{\pi_{2}},\cdot).\]

Similarly, we can estimate partial sum \(v\) for the set \(l\in I_{2}\setminus I_{1}\quad\&\quad m\in I_{1}\setminus I_{2}\). Using calculations similar to Case-1-b-i for summing \(u\) and \(v\), we have:

\[s_{4}(\pi_{1},\pi_{2})=\frac{-1}{\binom{n}{2}}\cdot\sum_{l\in I_{1}\setminus I _{2}|m\in I_{2}\setminus I_{1}}w_{s}(\mathbf{p}_{l}^{\pi_{1}},\cdot)\cdot w_{ s}(\mathbf{p}_{m}^{\pi_{2}},\cdot).\] (15)

Case 2:One item exists in \(I_{1}\cap I_{2}\), the other in \([n]\setminus(I_{1}\cap I_{2})\). It branches into two sub-cases: Case 2-a with one item in \(I_{1}\cup I_{2}\), and Case 2-b, where one item outside \(I_{1}\cap I_{2}\) but is in \(I_{1}\cup I_{2}\). Focusing on Case 2-a, represented by \(s_{5}\), we simplify as follows. This involves two index scenarios, either \(l\in I_{1}\cap I_{2}\) and \(m\notin I_{1}\cup I_{2}\) or vice versa, represented by partial sums \(u\) and \(v\). We now simplify \(u\) below:

\[u =\frac{1}{\binom{n}{2}}\sum_{1\leq l<m\leq n|l\in I_{1}\cap I_{2} |m\notin I_{1}\cup I_{2}}\frac{1}{\sqrt{\binom{n}{2}}}\cdot w_{s}(\mathbf{p} _{l}^{\pi_{1}},\cdot)\cdot\left(\mathbf{p}_{l<m}^{\pi_{1}}-\mathbf{p}_{l>m}^{ \pi_{1}}\right)\] \[\cdot\frac{1}{\sqrt{\binom{n}{2}}}\cdot w_{s}(\mathbf{p}_{l}^{ \pi_{2}},\cdot)\cdot\left(\mathbf{p}_{l<m}^{\pi_{2}}-\mathbf{p}_{l>m}^{\pi_{2} }\right),\] \[=\frac{1}{\binom{n}{2}}\sum_{1\leq l<m\leq n|l\in I_{1}\cap I_{2} |m\notin I_{1}\cup I_{2}}w_{s}(\mathbf{p}_{l}^{\pi_{1}},\cdot)\cdot w_{s}( \mathbf{p}_{l}^{\pi_{2}},\cdot),\] \[=\frac{1}{\binom{n}{2}}\sum_{1\leq l<m\leq n|l\in I_{1}\cap I_{2} }w_{s}(\mathbf{p}_{l}^{\pi_{1}},\cdot)\cdot w_{s}(\mathbf{p}_{l}^{\pi_{2}}, \cdot)\cdot(n-|I_{1}\cup I_{2}|).\]

Using steps similar to the previous case, we get the following value for \(s_{5}\):

\[s_{5}(\pi_{1},\pi_{2})=\frac{1}{\binom{n}{2}}\cdot(n-|I_{1}\cup I_{2}|)\cdot \sum_{l\in I_{1}\cap I_{2}}w_{s}(\mathbf{p}_{l}^{\pi_{1}},\cdot)\cdot w_{s}( \mathbf{p}_{l}^{\pi_{2}},\cdot).\] (16)

For Case 2-b, \(l\) or \(m\) are absent from \(I_{1}\) or \(I_{2}\), leading to two sub-scenarios. Consequently, either \(\phi_{l,m}^{wck}(\pi_{1})\) is zero or \(\phi_{l,m}^{wck}(\pi_{2})\) is zero. Therefore, these terms don't contribute to the overall WCK kernel value.

[MISSING_PAGE_EMPTY:20]

\[s_{1}(\pi_{1},\pi_{2}) =\frac{1}{\binom{n}{2}}\sum_{1\leq l<m\leq n|l,m\in I_{1}\cap I_{2}}w _{s}(\mathbf{p}_{l}^{\pi_{l}},\mathbf{p}_{m}^{\pi_{1}})\cdot w_{s}(\mathbf{p}_{l }^{\pi_{2}},\mathbf{p}_{m}^{\pi_{2}})\cdot\eta_{l,m}(\pi_{1},\pi_{2})\] \[=\frac{1}{\binom{n}{2}}\sum_{1\leq l<m\leq n|l,m\in I_{1}\cap I_{2 }}\eta_{l,m}(\pi_{1},\pi_{2})\] \[=\frac{1}{\binom{n}{2}}\sum_{1<l^{\prime}<m^{\prime}\leq|I_{1} \cap I_{2}|}\eta_{l^{\prime},m^{\prime}}(\tau_{1},\tau_{2})=\frac{\binom{|I_{1} \cap I_{2}|}{2}}{\binom{n}{2}}k^{sk}(\tau_{1},\tau_{2}).\] (17)

The simplification process begins by assigning unit rank weights in the first line, i.e., \(\mathbf{w}_{i}=1\). Following this, by relabeling the items in \(I_{1}\cap I_{2}\) and using \(\tau_{1}\) and \(\tau_{2}\), which are the rankings of \(\pi_{1}\) and \(\tau_{2}\) limited to the set \(I_{1}\cap I_{2}\) as defined in Definition 1, it is established that \(\eta_{l^{\prime},m^{\prime}}(\tau_{1},\tau_{2})=\eta_{l,m}(\pi_{1},\pi_{2})\). This is because the relative order of any pair of items is maintained in \(\tau_{1}\) and \(\tau_{2}\). Consequently, this leads to the final simplification to a scaled value of the standard Kendall kernel \(k^{sk}\), as given in Equation 1.

Simplifying the \(s_{2}\) and \(s_{3}\) Terms:The \(s_{2}\) and \(s_{3}\) terms are obtained for Case 1-b, which is for case when one item is in \(I_{1}\cap I_{2}\) and the other item is either in \(I_{1}\setminus I_{2}\) or \(I_{2}\setminus I_{1}\). We divide this into two sub-cases. Case 1-b-i: The other item is in \(I_{1}\setminus I_{2}\), with \(s_{2}\) representing the summation terms derived from the CK's inner product. Case 1-b-ii: The other item is \(I_{2}\setminus I_{1}\), where \(s_{3}\) denotes the summation terms. We simplify the \(s_{2}\) term for the CK kernel as follows:

\[s_{2}(\pi_{1},\pi_{2}) =\frac{1}{\binom{n}{2}}\sum_{l\in I_{1}\cap I_{2}|m\in I_{1} \setminus I_{2}}w_{s}(\mathbf{p}_{l}^{\pi_{1}},\mathbf{p}_{m}^{\pi_{1}})\cdot w _{s}(\mathbf{p}_{m}^{\pi_{2}},\cdot)\left(\mathfrak{p}_{l<m}^{\pi_{1}}- \mathfrak{p}_{l>m}^{\pi_{1}}\right)\] \[=\frac{1}{\binom{n}{2}}\sum_{l\in I_{1}\cap I_{2}|m\in I_{1} \setminus I_{2}}\left(\mathfrak{p}_{l<m}^{\pi_{1}}-\mathfrak{p}_{l>m}^{\pi_{1 }}\right)\] \[=\frac{1}{\binom{n}{2}}\underbrace{\sum_{l\in I_{1}\cap I_{2}|m \in I_{1}\setminus I_{2}}}_{\coloneqq u}\mathfrak{p}_{l<m}^{\pi_{1}}-\frac{1 }{\binom{n}{2}}\underbrace{\sum_{l\in I_{1}\cap I_{2}|m\in I_{1} \setminus I_{2}}}_{\coloneqq v}\mathfrak{p}_{l>m}^{\pi_{1}}\cdot\]

Next, we examine the terms \(u\) and \(v\) in detail, starting with \(u\). The term \(u\), which corresponds to \(\mathfrak{p}_{l<m}^{\pi_{1}}\), signifies instances where item \(l\) is ranked before item \(m\) in the top-k ranking \(\pi_{1}\). This can be derived from the observation that \(\sigma_{1}(l)-1\) items are positioned before item \(l\) in the set \(I_{1}\). Out of these items, \(\tau_{1}(l)-1\) also belong to the intersection \(I_{1}\cap I_{2}\). This follows from the definition of the full rankings \(\sigma_{1}\) and \(\tau_{1}\) on the set \(I_{1}\) and the intersection \(I_{1}\cap I_{2}\), respectively. Consequently, it can be concluded that \(\sigma_{1}(l)-\tau_{1}(l)\) items from the set difference \(I_{1}\setminus I_{2}\) are ranked before item \(l\). The second term, \(v\), corresponds to \(\mathfrak{p}_{l>m}^{\pi_{1}}\) and involves a calculation that takes into account the items ranked after the \(l\)-th item in the set \(I\). Specifically, there are \(k-\sigma_{1}(l)\) items following the \(l\)-th item. Within the intersection \(I_{1}\cap I_{2}\), the number of items before \(l\) is given by \(|I_{1}\cap I_{2}|-\tau_{1}(l)\). Therefore, the expression \((k-\sigma_{1}(l))-(|I_{1}\cap I_{2}|-\tau_{1}(l))\) represents the count of elements that are positioned after \(l\) in the set difference \(I_{1}\setminus I_{2}\).

Combining the above calculations for both terms \(u\) and \(v\), the \(s_{2}\) term for the CK kernel can be simplified as follows:

\[s_{2}(\pi_{1},\pi_{2})=\frac{1}{\binom{n}{2}}\sum_{l\in I_{1}\cap I_{2}}2 \cdot(\sigma_{1}(l)-\tau_{1}(l))-k+|I_{1}\cap I_{2}|.\] (18)

Using the symmetry between Case 1-b-i and Case 1-b-ii, we can simplify \(s_{3}\) for the CK kernel as follows:

\[s_{3}(\pi_{1},\pi_{2})=\frac{1}{\binom{n}{2}}\sum_{l\in I_{1}\cap I_{2}}2 \cdot(\sigma_{2}(l)-\tau_{2}(l))-k+|I_{1}\cap I_{2}|.\] (19)Simplifying the \(s_{4}\) and \(s_{5}\) Terms:We simplify the \(s_{4}\) and \(s_{5}\) terms for the CK kernel starting from Equation 15 and Equation 16, respectively, as follows:

\[s_{4}(\pi_{1},\pi_{2}) =\frac{-1}{\binom{n}{2}}\cdot\sum_{l\in I_{1}\setminus I_{2}|m\in I _{2}\setminus I_{1}}w_{s}(\mathbf{p}_{l}^{\pi_{1}},\cdot)\cdot w_{s}(\mathbf{ p}_{m}^{\pi_{2}},\cdot)=\frac{-|I_{1}\setminus I_{2}|\cdot|I_{2}\setminus I_{1}|}{ \binom{n}{2}}\] (20) \[s_{5}(\pi_{1},\pi_{2}) =\frac{1}{\binom{n}{2}}\cdot(n-|I_{1}\cup I_{2}|)\cdot\sum_{l\in I _{1}\cap I_{2}}w_{s}(\mathbf{p}_{l}^{\pi_{1}},\cdot)\cdot w_{s}(\mathbf{p}_{l} ^{\pi_{2}},\cdot)=\frac{|I_{1}\cap I_{2}|\cdot|[n]\setminus(I_{1}\cup I_{2})| }{\binom{n}{2}}.\] (21)

We have obtained the values of all the simplified \(s_{i}\) terms for the CK kernel in Equations 17, 18, 19, 20, and 21. By combining these terms, we get \(k^{ck}(\pi_{1},\pi_{2})=\sum_{i=1}^{5}s_{i}(\pi_{1},\pi_{2})\), where each term \(s_{i}\) precisely matches its corresponding expression in Algorithm 3. This completes the proof of the correctness of Algorithm 3. Regarding its time complexity, each term \(s_{i}\) sums at most \(k^{2}\) quantities, and each quantity can be computed in \(\mathcal{O}(1)\) time. Therefore, the time required for Algorithm 3 to compute the CK kernel is \(\mathcal{O}(k^{2})\). 

### Fast Matrix-Vector Multiplication with Kendall Kernel Matrix on Top-k Rankings

This section revisits Theorem 1 about the fact matrix-vector multiplication time for the Kendall kernel matrix for top-k rankings. Specifically, we aim to eliminate the \(\mathbf{mvm}(K_{X})\)'s dependence on the number of items, i.e., \(n\) on and linear dependence in the number of rounds, i.e., \(T\), as claimed in Theorem 1.

**Theorem 1**.: _For the WCK kernel with product-symmetric weights \(w_{ps}\), the computational complexity of multiplying the kernel matrix \(K_{X_{t}}\) with any admissible vector is \(\mathcal{O}(k^{2}t)\), i.e., \(\mathbf{mvm}(K_{X_{t}})=\mathcal{O}(k^{2}t)\), where \(X_{t}\) is any arbitrary set of \(t\) top-k rankings._

Proof.: The cornerstone of this proof lies in the computation of the WCK kernel, as delineated in Algorithm 2. This algorithm requires only \(\mathcal{O}(k^{2})\) computation. For brevity, we write \(X\) to represent \(X_{T}\), and the proof follows for any \(X_{t}\), i.e., any value of \(t\), not just \(T\).

As also suggested previously, we will demonstrate through the equation \(K_{X}=(\Phi_{X}^{a})^{T}\Phi_{X}^{b}\), where both matrices \(\Phi_{X}^{a}\) and \(\Phi_{X}^{b}\) have columns with only \(\mathcal{O}(k^{2})\) non-zero entries. Consequently, this leads to the computational complexity of matrix-vector multiplication, denoted as \(\mathbf{mvm}(K_{X})\), being \(\mathcal{O}(k^{2}\cdot T)\).

From Algorithm 2, we know that each entry of the kernel matrix \(k(\pi_{1},\pi_{2})\), can be expressed as a sum \(\sum_{i=1}^{5}s_{i}(\pi_{1},\pi_{2})\). Assuming each \(s_{i}(\pi_{1},\pi_{2})\) equals \(\phi^{a_{i}}(\pi_{1})^{T}\phi^{b_{i}}(\pi_{2})\), and considering that all vectors \(\phi^{a_{i}}\) and \(\phi^{b_{i}}\) exhibit this property, we can express \(K_{X}\) as \((\Phi_{X}^{b})^{T}\Phi_{X}^{b}\). Here, the \(i^{th}\) row of \((\Phi_{X}^{a})^{T}\) and the \(j^{th}\) column of \(\Phi_{X}^{b}\) are represented by \([\phi^{a_{1}}(\pi_{i})^{T},\cdots,\phi^{a_{5}}(\pi_{i})^{T}]\) and \([\phi^{b_{1}}(\pi_{j}),\cdots,\phi^{b_{5}}(\pi_{j})]\), respectively. Therefore, the overall mvm complexity can be characterized by the sparsity of the vectors \(\phi^{a_{i}}\) and \(\phi^{b_{i}}\), as is formalized in the claim presented below.

**Claim 5**.: _Consider a kernel matrix \(K_{X}\) corresponding to any set \(X\) of cardinality \(T\). Each entry of \(K_{X}\), denoted as \(k(x_{1},x_{2})\), is defined by the sum \(\sum_{i=1}^{5}s_{i}(x_{1},x_{2})\), where each \(s_{i}(x_{1},x_{2})\) is the result of the dot product \(\phi^{a_{i}}(x_{1})^{T}\phi^{b_{i}}(x_{2})\), where, \(\phi^{a_{i}}\) and \(\phi^{b_{i}}\) are vectors characterized by having \(\mathcal{O}(z)\) non-zero entries. Given this structure, the matrix-vector multiplication complexity for \(K_{X}\) is \(O(nnz\cdot T)\), i.e., \(\mathbf{mvm}(K_{X})=O(z\cdot T)\)._

Proof.: We will demonstrate this in the following discussion by concentrating on the \(k^{\text{th}}\) entry of the output vector, specifically \(K_{X}\mathbf{v}\), for any arbitrary vector \(\mathbf{v}\):

\[(K_{X}\mathbf{v})_{k}=\sum_{j}K_{X}(k,j)v_{j}=\sum_{j}\left(\sum_{i=1}s_{i}( \pi_{k},\pi_{j})\right)v_{j}=\sum_{j}\left(\sum_{i=1}^{5}\phi^{a_{i}}(\pi_{k})^{ T}\phi^{b_{i}}(\pi_{j})\right)v_{j},\]\[=\sum_{i=1}^{5}\left(\sum_{j}\phi^{a_{i}}(\pi_{k})^{T}\phi^{b_{i}}( \pi_{j})v_{j}\right)=\sum_{i=1}^{5}\phi^{a_{i}}(\pi_{k})^{T}\left(\sum_{j}\phi^{b _{i}}(\pi_{j})v_{j}\right).\]

Given that for all \(i\), \(\phi^{b_{i}}\) possesses only \(\mathcal{O}(z)\) non-zero entries for any \(\pi_{j}\), the computation of \(\sum_{j}\phi^{b_{i}}(\pi_{j})v_{j}\) requires \(\mathcal{O}(z)\) operations. This implies that the expression \(\sum_{j}\phi^{b_{i}}(\pi_{j})v_{j}\) also necessitates \(\mathcal{O}(z)\) computation. Applying a similar rationale to \(\phi^{a_{i}}\), it follows that computing \((K_{X}v)_{k}\) demands only \(\mathcal{O}(z)\) operations. Extending this argument to all entries of the output vector, it is evident that computing \(K_{X}\mathbf{v}\) requires only \(\mathcal{O}(z\cdot T)\) computation 

Utilizing Claim 5, it suffices to complete the proof by showcasing that these exist vectors \(\phi^{a_{i}}\) and \(\phi^{b_{i}}\), each with only \(\mathcal{O}(k^{2})\) non-zero elements, corresponding to each \(s_{i}\) as specified in Algorithm 2. Additionally, these vectors ensure that \(s_{i}(\pi_{1},\pi_{2})=\phi^{a_{i}}(\pi_{1})^{T}\phi^{b_{i}}(\pi_{2})\). We will next establish such vectors for all \(s_{i}\) terms. Starting with the \(s_{1}\) term below.

Showcasing \(s_{1}(\pi_{1},\pi_{2})=\phi^{a_{1}}(\pi_{1})^{T}\phi^{b_{1}}(\pi_{2})\) for sparse \(\phi^{a_{1}}(\pi_{1})\) and \(\phi^{b_{1}}(\pi_{2})\) vectors. We begin by manipulating \(s_{1}\), as defined in Equation 12. For the sake of brevity, their scalar factors will be omitted in the following explanation.

\[s_{1}(\pi_{1},\pi_{2}) =\sum_{1\leq l<m\leq n|l,m\in I_{1}\cap I_{2}}w_{s}(\mathbf{p}_{l }^{\pi_{1}},\mathbf{p}_{m}^{\pi_{1}})\cdot w_{s}(\mathbf{p}_{l}^{\pi_{2}}, \mathbf{p}_{m}^{\pi_{2}})\cdot\eta_{l,m}(\pi_{1},\pi_{2}),\] \[=\sum_{1\leq l<m\leq n|l,m\in I_{1}\cap I_{2}}w_{s}(\mathbf{p}_{l }^{\pi_{1}},\mathbf{p}_{m}^{\pi_{1}})\cdot w_{s}(\mathbf{p}_{l}^{\pi_{2}}, \mathbf{p}_{m}^{\pi_{2}})\cdot(\mathbf{p}_{l<m}^{\pi_{1}}-\mathbf{p}_{l>m}^{ \pi_{1}})\cdot(\mathbf{p}_{l<m}^{\pi_{2}}-\mathbf{p}_{l>m}^{\pi_{2}}),\] \[=\sum_{1\leq l<m\leq n|l,m\in I_{1}\cap I_{2}}w_{s}(\mathbf{p}_{l }^{\pi_{1}},\mathbf{p}_{m}^{\pi_{1}})\cdot(\mathbf{p}_{i<j}^{\pi_{1}}-\mathbf{ p}_{l>m}^{\pi_{1}})\cdot w_{s}(\mathbf{p}_{l}^{\pi_{2}},\mathbf{p}_{m}^{\pi_{2}}) \cdot(\mathbf{p}_{l<m}^{\pi_{2}}-\mathbf{p}_{l>m}^{\pi_{2}}),\] \[=\sum_{1\leq l<m\leq n} \underbrace{w_{s}(\mathbf{p}_{l}^{\pi_{1}},\mathbf{p}_{m}^{\pi_{1}}) \cdot(\mathbf{p}_{i<j}^{\pi_{1}}-\mathbf{p}_{l>m}^{\pi_{1}})\cdot\mathbf{1}_{ \mathbf{p}_{l}^{\pi_{1}},\mathbf{p}_{m}^{\pi_{1}}\in[k]}}_{:=\phi^{a_{1}}_{l,m }(\pi_{1})}\] \[\cdot\underbrace{w_{s}(\mathbf{p}_{l}^{\pi_{2}},\mathbf{p}_{m}^{ \pi_{2}})\cdot(\mathbf{p}_{l<m}^{\pi_{2}}-\mathbf{p}_{l>m}^{\pi_{2}})\cdot \mathbf{1}_{\mathbf{p}_{l}^{\pi_{2}},\mathbf{p}_{m}^{\pi_{2}}\in[k]}}_{:=\phi^{b _{1}}_{l,m}(\pi_{2})},\] \[=(\phi^{a_{1}}(\pi_{1})^{T}\phi^{b_{1}}(\pi_{2}).\] (22)

Both \(\phi^{a_{1}}\) and \(\phi^{b_{1}}\) are sparse by design, taking non-zero values only when \(l\) and \(m\) appear in the top-k rankings. This demonstrates the existence of sparse vectors for the \(s_{1}\) term. Next, we will establish the same for the \(s_{2}\) and \(s_{3}\) terms.

Showcasing sparse vectors for \(s_{2}\) and \(s_{3}\).We begin by manipulating \(s_{2}\), as defined in Equation 13, while ignoring its scalar factor. We will exploit symmetry between \(s_{2}\) and \(s_{3}\) terms.

\[s_{2}(\pi_{1},\pi_{2})\] \[=\sum_{l\in I_{1}\cap I_{2}|m\in I_{1}\setminus I_{2}}w_{s}( \mathbf{p}_{l}^{\pi_{1}},\mathbf{p}_{m}^{\pi_{1}})\cdot w_{s}(\mathbf{p}_{l}^{ \pi_{2}},\cdot)\left(\mathbf{p}_{l<m}^{\pi_{1}}-\mathbf{p}_{l>m}^{\pi_{1}} \right),\] \[=\sum_{l\in I_{1}\cap I_{2}}w_{s}(\mathbf{p}_{l}^{\pi_{2}},\cdot )\sum_{m\in I_{1}\setminus I_{2}}w_{s}(\mathbf{p}_{l}^{\pi_{1}},\mathbf{p}_{m}^{ \pi_{1}})\left(\mathbf{p}_{l<m}^{\pi_{1}}-\mathbf{p}_{l>m}^{\pi_{1}}\right),\] \[=\sum_{l\in I_{1}\cap I_{2}}w_{s}(\mathbf{p}_{l}^{\pi_{2}},\cdot )\left(\sum_{m\in I_{1}}w_{s}(\mathbf{p}_{l}^{\pi_{1}},\mathbf{p}_{m}^{\pi_{1}} )\left(\mathbf{p}_{l<m}^{\pi_{1}}-\mathbf{p}_{l>m}^{\pi_{1}}\right)-\sum_{m\in I _{1}\cap I_{2}}w_{s}(\mathbf{p}_{l}^{\pi_{1}},\mathbf{p}_{m}^{\pi_{1}})\left( \mathbf{p}_{l<m}^{\pi_{1}}-\mathbf{p}_{l>m}^{\pi_{1}}\right)\right),\] \[=\sum_{l\in[n]} \underbrace{\mathbf{1}_{\mathbf{p}_{l}^{\pi_{2}}\in[k]}w_{s}( \mathbf{p}_{l}^{\pi_{2}},\cdot)}_{:=\phi^{b_{1}}_{l}(\pi_{2})}\underbrace{ \mathbf{1}_{p_{l}^{\pi_{1}}\in[k]}\sum_{m\in I_{1}}w_{s}(\mathbf{p}_{l}^{\pi_{1}}, \mathbf{p}_{m}^{\pi_{1}})\left(\mathbf{p}_{l<m}^{\pi_{1}}-\mathbf{p}_{l>m}^{ \pi_{1}}\right)}_{:=\phi^{a_{1}}_{l}(\pi_{1})}\]\[= -\sum_{l,m\in I_{1}\cap I_{2}}w_{s}(\mathbf{p}_{l}^{\pi_{1}},\cdot) \cdot w_{s}(\mathbf{p}_{m}^{\pi_{2}},\cdot)-\sum_{m\in I_{1}\cap I_{2}}w_{s}( \mathbf{p}_{m}^{\pi_{2}},\cdot)\Bigg{)}\,.\]

Observing that \(\overline{w}\coloneqq\sum\limits_{m\in I_{2}}w_{s}(\mathbf{p}_{m}^{\pi_{2}}, \cdot)\) represents a constant value that does not depend on \(I_{2}\), we can further simplify the above expression for \(s_{4}\) as follows:

\[s_{4}(\pi_{1},\pi_{2}) = -\sum_{l\in I_{1}\setminus I_{2}}w_{s}(\mathbf{p}_{l}^{\pi_{1}}, \cdot)\cdot\left(\overline{w}-\sum_{m\in I_{1}\cap I_{2}}w_{s}(\mathbf{p}_{m}^ {\pi_{2}},\cdot)\right),\] (27) \[= -\left(\overline{w}-\sum_{l\in I_{1}\cap I_{2}}w_{s}(\mathbf{p}_ {l}^{\pi_{1}},\cdot)\right)\cdot\left(\overline{w}-\sum_{m\in I_{1}\cap I_{2 }}w_{s}(\mathbf{p}_{m}^{\pi_{2}},\cdot)\right),\] \[= -\overline{w}^{2}+\overline{w}\left(\sum_{l\in I_{1}\cap I_{2}}w _{s}(\mathbf{p}_{l}^{\pi_{1}},\cdot)+\sum_{m\in I_{1}\cap I_{2}}+w_{s}( \mathbf{p}_{m}^{\pi_{2}},\cdot)\right)\] \[- \sum_{l\in I_{1}\cap I_{2}}w_{s}(\mathbf{p}_{l}^{\pi_{1}},\cdot )\sum_{m\in I_{1}\cap I_{2}}w_{s}(\mathbf{p}_{m}^{\pi_{2}},\cdot).\]

Next, to simplify the above equation, we first focus on the second term and have the following:

\[\overline{w}\left(\sum_{l\in I_{1}\cap I_{2}}w_{s}(\mathbf{p}_{l}^{\pi_{1}}, \cdot)+\sum_{m\in I_{1}\cap I_{2}}w_{s}(\mathbf{p}_{m}^{\pi_{2}},\cdot)\right)\]\[=\sum_{l\in[n]}\underbrace{\mathbf{1}_{\mathbf{p}_{l}^{\pi_{1}}\in[k]} w_{s}(\mathbf{p}_{l}^{\pi_{1}},\cdot)}_{:=\phi_{l}^{\omega_{1}}(\pi_{1})} \underbrace{\mathbf{1}_{\mathbf{p}_{l}^{\pi_{2}}\in[k]}\overline{w}}_{=\phi_{l }^{\omega_{1}}(\pi_{2})}+\sum_{m\in I_{1}\cap I_{2}}\overline{w}\cdot w_{s}( \mathbf{p}_{m}^{\pi_{2}},\cdot),\] \[=\phi^{4a_{1}}(\pi_{1})^{T}\phi^{4b_{1}}(\pi_{2})+\sum_{m\in[k]} \underbrace{\mathbf{1}_{\mathbf{p}_{m}^{\pi_{1}}\in[k]}\overline{w}}_{:=\phi_{ m}^{\omega_{m}}(\pi_{1})}\underbrace{w_{s}(\mathbf{p}_{m}^{\pi_{1}},\cdot)}_{:= \phi_{l}^{\omega_{2}}(\pi_{2})}\] \[=\phi^{4a_{1}}(\pi_{1})^{T}\phi^{4b_{1}}(\pi_{2})+\phi^{4a_{2}}( \pi_{1})^{T}\phi^{4b_{2}}(\pi_{2}).\] (28)

Next, we simplify the third and last term in the Equation 27 as follows:

\[\sum_{l\in I_{1}\cap I_{2}}w_{s}(\mathbf{p}_{l}^{\pi_{1}},\cdot )\sum_{m\in I_{1}\cap I_{2}}w_{s}(\mathbf{p}_{m}^{\pi_{2}},\cdot) =\sum_{l\in[n],m\in[n]}\underbrace{w_{s}(\mathbf{p}_{l}^{\pi_{1}}, \cdot)\mathbf{1}_{\mathbf{p}_{l}^{\pi_{1}},\mathbf{p}_{m}^{\pi_{1}}\in[k]}}_{: =\phi_{l,m}^{4\omega_{3}}(\pi_{1})}\underbrace{w_{s}(\mathbf{p}_{m}^{\pi_{2}}, \cdot)\mathbf{1}_{\mathbf{p}_{l}^{\pi_{2}},\mathbf{p}_{m}^{\pi_{2}}\in[k]}}_{:= \phi_{l,m}^{4\omega_{3}}(\pi_{2})},\] \[=\phi^{4a_{3}}(\pi_{1})^{T}\phi^{4b_{3}}(\pi_{2}).\] (29)

Next, combining the results from Equations 27, 28, and 29, we obtain the following:

\[s_{4}(\pi_{1},\pi_{2}) =\underbrace{[\overline{w},\phi^{4a_{1}}(\pi_{1});\phi^{4a_{1}}( \pi_{1});\phi^{4a_{3}}(\pi_{1})]^{T}}_{:=\phi^{4a_{5}}(\pi_{1})^{T}}[- \overline{w};\phi^{4b_{1}}(\pi_{2});\phi^{4b_{2}}(\pi_{2});-\phi^{4b_{3}}(\pi _{2})]_{:=\phi^{4b}(\pi_{2})}\] \[=\phi^{4a}(\pi_{1})^{T}\phi^{4b}(\pi_{2}).\] (30)

Equation 30 showcases both \(\phi^{4a}\) and \(\phi^{4b}\) has three components with having only \(\mathcal{O}(k^{2})\) non-zero entries, thus fulfilling the requirements for the \(s_{4}\) term. Next, we focus on the \(s_{5}\) term.

Showcasing sparse vectors \(s_{5}(\pi_{1},\pi_{2})=\phi^{5a}(\pi_{1})^{T}\phi^{5b}(\pi_{2})\).We begin by examining the \(s_{5}\) term, excluding its scalar component, as outlined in Equation 16.

\[s_{5}(\pi_{1},\pi_{2}) =(n-|I_{1}\cup I_{2}|)\cdot\sum_{l\in I_{1}\cap I_{2}}w_{s}( \mathbf{p}_{l}^{\pi_{1}},\cdot)\cdot w_{s}(\mathbf{p}_{l}^{\pi_{2}},\cdot),\] \[=(n-(2k-|I_{1}\cap I_{2}|))\cdot\sum_{l\in I_{1}\cap I_{2}}w_{s}( \mathbf{p}_{l}^{\pi_{1}},\cdot)\cdot w_{s}(\mathbf{p}_{l}^{\pi_{2}},\cdot),\] \[=(n-2k)\cdot\sum_{l\in I_{1}\cap I_{2}}w_{s}(\mathbf{p}_{l}^{\pi_ {1}},\cdot)\cdot w_{s}(\mathbf{p}_{l}^{\pi_{2}},\cdot)+|I_{1}\cap I_{2}|\cdot \sum_{l\in I_{1}\cap I_{2}}w_{s}(\mathbf{p}_{l}^{\pi_{1}},\cdot)\cdot w_{s}( \mathbf{p}_{l}^{\pi_{2}},\cdot),\] \[=\sum_{l\in I_{1}\cap I_{2}}\sqrt{n-2k}\cdot w_{s}(\mathbf{p}_{l} ^{\pi_{1}},\cdot)\cdot\sqrt{n-2k}\cdot w_{s}(\mathbf{p}_{l}^{\pi_{2}},\cdot)\] \[+\sum_{l\in I_{1}\cap I_{2}}w_{s}(\mathbf{p}_{l}^{\pi_{1}},\cdot )\cdot w_{s}(\mathbf{p}_{l}^{\pi_{2}},\cdot)\cdot|I_{1}\cap I_{2}|,\] (31) \[=\sum_{l\in[n]}\underbrace{\sqrt{n-2k}\cdot w_{s}(\mathbf{p}_{l} ^{\pi_{1}},\cdot)\cdot\mathbf{1}_{\mathbf{p}_{l}^{\pi_{1}}[k]}}_{:=\phi_{l}^{ \omega_{1}}(\pi_{1})}\cdot\underbrace{\sqrt{n-2k}\cdot w_{s}(\mathbf{p}_{l}^{ \pi_{2}},\cdot)\cdot\mathbf{1}_{\mathbf{p}_{l}^{\pi_{2}}[k]}}_{:=\phi_{l}^{ \omega_{1}}(\pi_{2})}\] \[+\sum_{l\in I_{1}\cap I_{2}}w_{s}(\mathbf{p}_{l}^{\pi_{1}},\cdot )\cdot w_{s}(\mathbf{p}_{l}^{\pi_{2}},\cdot)\cdot|I_{1}\cap I_{2}|,\] \[=\phi^{5a_{1}}(\pi_{1})^{T}\phi^{5b_{1}}(\pi_{2})+\sum_{l\in I_{1} \cap I_{2}}w_{s}(\mathbf{p}_{l}^{\pi_{1}},\cdot)\cdot w_{s}(\mathbf{p}_{l}^{ \pi_{2}},\cdot)\cdot\sum_{m\in I_{1}\cap I_{2}}1,\] \[=(\phi^{5a_{1}}(\pi_{1})^{T}\phi^{5b_{1}}(\pi_{2})+\sum_{l\in I_{1} \cap I_{2},m\in I_{1}\cap I_{2}}w_{s}(\mathbf{p}_{l}^{\pi_{1}},\cdot)\cdot w_{s}( \mathbf{p}_{l}^{\pi_{2}},\cdot),\] \[=\phi^{5a_{1}}(\pi_{1})^{T}\phi^{5b_{1}}(\pi_{2})+\sum_{l\in[n],m \in[n]}\underbrace{w_{s}(\mathbf{p}_{l}^{\pi_{1}},\cdot)\cdot\mathbf{1}_{ \mathbf{p}_{l}^{\pi_{1}},\mathbf{p}_{m}^{\pi_{1}}\in[k]}}_{:=\phi_{l,m}^{5 \omega_{2}}(\pi_{1})}\cdot\underbrace{w_{s}(\mathbf{p}_{l}^{\pi_{2}},\cdot) \cdot\mathbf{1}_{\mathbf{p}_{l}^{\pi_{2}},\mathbf{p}_{m}^{\pi_{2}}\in[k]}}_{:= \phi_{l,m}^{5\omega_{2}}(\pi_{2})},\] (32)\[=\phi^{5a_{1}}(\pi_{1})^{T}\phi^{5b_{1}}(\pi_{2})+\phi^{5a_{2}}(\pi_{ 1})^{T}\phi^{5b_{2}}(\pi_{2}),\] \[=\underbrace{[\phi^{5a_{1}}(\pi_{1});\phi^{5a_{2}}(\pi_{1})]^{T}}_{ :=\phi^{5a}(\pi_{1})^{T}}\underbrace{[\phi^{5b_{1}}(\pi_{2})+\phi^{5b_{2}}(\pi_ {2})]}_{:=\phi^{5b}(\pi_{2})}=\phi^{5a}(\pi_{1})^{T}\phi^{5b}(\pi_{2}).\] (33)

The equation shows that \(s_{5}(\pi_{1},\pi_{2})=\phi^{5a}(\pi_{1})^{T}\phi^{5b}(\pi_{2})\), where both \(\phi^{5a}\) and \(\phi^{5b}\) possess components with a maximum number of non-zero entries, as indicated in Equations 31 and 32. This completes the proof requirements for the \(s_{5}\) term.

By combining the results from Equations 22, 25, 26, 30, and 33, we have demonstrated the existence of vectors \(\phi^{a_{i}}\) and \(\phi^{b_{i}}\), each containing only \(\mathcal{O}(k^{2})\) non-zero elements, and have established that \(s_{i}(\pi_{1},\pi_{2})=\phi^{a_{i}}(\pi_{1})^{T}\phi^{b_{i}}(\pi_{2})\) for each \(i\in 1,2,3,4,5\). In conjunction with Claim 5, this completes the proof. 

## Appendix B Proposed GP-TopK Bandit Algorithm- Omitted Details

This section includes the proofs that were omitted from Section 4, presented in the following order:

* Section B.1 outlines a brief of Gaussian process regression for any domain.
* Section B.2 summarizes the committed details about the local search utilized for optimizing the UCB function.
* Section B.3 provides the removed proof for the Theorem 2 concerning the overall time for the bandit algorithm.
* Section B.4 provides the proof for Theorem 3 concerning regret analysis of the proposed bandit algorithm.

### Gaussian Process Regression

In GP regression [22], the training data are modeled as noisy measurements of a random function \(f\) drawn from a GP prior, denoted \(f\sim\mathcal{N}(0,k(\cdot,\cdot))\), where \(k:\mathcal{X}\times\mathcal{X}\to\mathbb{R}\) is a kernel function over any domain \(\mathcal{X}\). The observed training pairs \((\mathbf{x}_{i},y_{i})\) are collected as \(X=[\mathbf{x}_{1},\ldots,\mathbf{x}_{T}]\) and \(\mathbf{y}=[y_{1},\ldots,y_{T}]\in\mathbb{R}^{T}\), where, for an input \(\mathbf{x}_{i}\), the observed value is modeled as \(\mathbf{y}_{i}=f(\mathbf{x}_{i})+\epsilon\), with \(\epsilon_{i}\sim\mathcal{N}(0,\sigma^{2})\). The kernel matrix on data is \(K_{X}=[k(\mathbf{x}_{i},\mathbf{x}_{j})]_{i,j=1}^{T}\in\mathbb{R}^{T\times T}\). The posterior mean \(\mu_{f|\mathcal{D}}\) and variance \(\sigma_{f|\mathcal{D}}\) functions for GPs are:

\[\mu_{f|\mathcal{D}}(\mathbf{x}) \coloneqq\mathbf{k}_{\mathbf{x}}^{T}\mathbf{z}\] (34) \[\sigma_{f|\mathcal{D}}(\mathbf{x}) \coloneqq k(\mathbf{x},\mathbf{x})-\mathbf{k}_{\mathbf{x}}^{T}(K_{X}+ \sigma^{2}I)^{-1}\mathbf{k}_{\mathbf{x}}\] (35)

where \(\mathbf{k}_{\mathbf{x}}\in\mathbb{R}^{T}\) has as its \(i^{th}\) entry \(k(\mathbf{x},\mathbf{x}_{i})\), \(\mathbf{z}=(K_{X}+\sigma^{2}I)^{-1}\mathbf{y}\), and \(I\) is an identity matrix. For GP regression on an arbitrary domain \(\mathcal{X}\), the kernel function must be a p.d. kernel [23].

Naive approaches rely on the Cholesky decomposition of the matrix \(K_{X}+\sigma^{2}I\), which takes \(\Theta(T^{3})\) time [23]. To circumvent the \(\Theta(T^{3})\) runtime, recent works use iterative algorithms such as the conjugate gradient algorithm, which facilitate GP inference by exploiting fast kernel matrix-vector multiplication (MVM) algorithms, i.e., \(\mathbf{v}\mapsto\mathbf{K}_{\mathbf{X}}\mathbf{v}\)[3]. In practice, these methods yield highly accurate approximations for GP posterior functions with a complexity of \(\Theta(p\cdot T^{2})\) for \(p\) iterations of the conjugate gradient algorithm, as \(\mathbf{mvm}(K_{X})=T^{2}\), and \(\mathbf{mvm}(M)\) is the operation count for multiplying matrix \(M\) by a vector. \(p\ll T\) proves to be efficient in practical application [3].

### Contextual GP Reward Model

Optimizing the \(\mathcal{AF}\), i.e., UCB function, poses a significant challenge due to its enormous size of \(\Pi^{k}\). Drawing inspiration from prior research on Bayesian optimization within combinatorial spaces, we employ a breadth-first local search (BFLS) to optimize the UCB acquisition function [2, 19]. The BFLS begins with the selection of several random top-k rankings. Subsequently, each specific top-k ranking is compared with the UCB values of its neighboring rankings, proceeding to the one with the highest UCB value.

The neighbors of a top-k ranking include all its permutations and the permutations of modified top-k rankings obtained by swapping one item with any of the remaining items. For any top-k ranking, there are \((n-k)\cdot k!+k!\) neighbors, which is often not huge as \(k\) is often \(\leq 6\). This search continues until no neighboring top-k ranking with a higher value is discovered. Although BELS is a local search, the initial random selection and multiple restart points help it evade local minima, a strategy that previous studies have corroborated [19].

### Assessing GP-TopK Compute Requirements

[backgroundcolor=gray!10,linewidth=0.4]

**Theorem 2**.: _Assuming a fixed number of iterations required by the iterative algorithms, the total computational time for running the GP-TopK bandit algorithm for \(T\) rounds of top-k recommendations, using the contextual product kernel (Equation 6), is \(\mathcal{O}(k^{2}c\epsilon T^{2})\). This applies to WK, CK, and WCK top-k ranking kernels, where \(\ell\) is the number of local search evaluations for selecting the next arm in every round._

Proof.: The proof can be straightforwardly derived by combining the results presented in Table 1, which succinctly summarizes the time complexities for each step of computing the UCB using both feature and kernel approaches. It is important to emphasize that iterative algorithms enhance results from \(\mathcal{O}(T^{4})\) to \(\mathcal{O}(T^{3})\) in computational complexity. Furthermore, these algorithms can further reduce complexity to \(\mathcal{O}(T^{2})\) when used with the feature approach.

The results presented in Table 1 can be validated through straightforward observations and by leveraging findings from previous Sections 2. Specifically, Section 2 offers proof for the \(\mathbf{mvm}(K_{X})\) row explicitly. For the _compute_\(K_{X_{t}}\) row, the complexity of kernel approaches is deduced from Algorithms 2 and 3. For feature approaches, the _compute_\(K_{X_{t}}\) row is inferred from the sparsity of the feature representations as stated in Claim 3. Lastly, the _memory_ row is straightforwardly deduced for the kernel approach by counting its entries. For the feature approach, it is derived from the sparsity of the feature representations. 

### Regret Analysis

In this section, we revisit Theorem 3 and provide its proof. The proofs build on the work by Krause et al. [14], delivering results for bounding the contextual regret in the context of the top-k ranking problem. To set the stage for our regret analysis, let's first define the critical term _maximum mutual information_, denoted by \(\gamma_{t}\), is given below:

\[\gamma_{t}\coloneqq\max_{X\subseteq X:|X|=t}I(y_{X};f),\qquad I(y_{X};f)=H(y_ {X})-H(y_{X}|f),\]

where \(I(y_{X};f)\) quantifies the reduction in uncertainty (measured in terms of differential Shannon entropy) about \(f\) achieved by revealing \(y_{A}\)[27]. In Gaussian observation case, the entropy can be computed in closed form: \(H(N(\mu,\Sigma))=\frac{1}{2}\log|2\pi e\Sigma|\), so that \(I(y_{X};f)=\frac{1}{2}\log|I+\xi^{-2}K_{X}|\), where \(K_{X}=[k(x,x^{\prime})]_{x,x^{\prime}\in X}\) is the Gram matrix of \(k\) evaluated on set \(X\subseteq\mathcal{X}\). For the contextual bandit algorithm, \(X\) represents contexts and arms considered until round \(t\).

Before proving Theorem 3, we align the Krause et al. [14] results with our notation for consistency. Furthermore, we modify \(\beta_{t}\) to accommodate embeddings encompassing negative values, aligning with the fact that contextual embeddings may exhibit negative dimensions.

[backgroundcolor=gray!10,linewidth=0.4]
**Proposition 1** (Theorem 1, [14]).: _Let \(\delta\in(0,1)\), and the unknown reward function \(\hat{f}\) be sampled from the known GP prior with known noise variance \(\sigma^{2}\). Suppose one of the following holds:_

1. _Assumption_ 1 _holds and set_ \(\beta_{t}=2\log(|\mathcal{X}|t^{2}\pi^{2}/6\delta)\)_._
2. _Assumption_ 2 _holds and set_ \(\beta_{t}=2B^{2}+300\gamma_{t}\ln^{3}(t/\delta)\)_._

_Then the cumulative regret \(\mathcal{R}_{T}\) of the contextual GP bandit algorithm with the UCB acquisition function is bounded by \(\tilde{\mathcal{O}}(\sqrt{C_{1}T\gamma_{T}\beta_{T}})\) w.h.p. Precisely,\(\Pr\left\{R_{T}\leq\sqrt{C_{1}T\gamma_{T}\beta_{t}})+2\quad\forall T\geq 1 \right\}\geq 1-\delta\), where, \(C_{1}=8/\log(1+\sigma^{-2})\) and the notation \(\tilde{\mathcal{O}}\) hides logarithmic factors in \(n\), \(\frac{1}{\delta}\) and \(T\)._

Proposition 1 shows that the regret \(\mathcal{R}_{T}\) for the contextual GP bandit algorithm, utilizing the UCB acquisition function is bounded with high probability within \(\tilde{\mathcal{O}}(\sqrt{C_{1}T\gamma_{T}\beta_{T}})\), where the notation \(\tilde{\mathcal{O}}\) hides logarithmic factors in \(n\), \(\frac{1}{\delta}\) and \(T\). To ascertain the \(\tilde{\mathcal{O}}\) order for \(\mathcal{R}_{T}\), it is imperative to first bound the \(\tilde{\mathcal{O}}\) order of \(\gamma_{T}\beta_{t}\). We begin by examining the \(\gamma_{T}\) term in the subsequent proposition.

**Proposition 2**.: _Under the assumptions of Theorem 3, \(\gamma_{T}\) can be succinctly characterized as \(\gamma_{T}=\mathcal{O}(n^{2}c\log(n^{2}T)+c\log T)\), which also simplifies to \(\tilde{\mathcal{O}}(n^{2}c)\), where the \(\tilde{\mathcal{O}}\) notation omits logarithmic factors in \(n\) and \(T\)._

Proof.: For the GP bandit algorithm with the UCB acquisition function, \(\gamma_{T}=C\cdot\log\left(|I+\sigma^{-2}K_{X_{T}}|\right)\), where \(C\) equals \((1/2)\cdot(1-1/e)^{-1}\) and \(K_{X_{T}}\) represents the kernel matrix computed over contexts and arms across \(T\) rounds [27, 14]. Precisely, \(K_{X_{T}}\) is calculated using the contextual kernel defined in Equation 6. It is applied to contexts and top-k ratings from the feedback data \(\mathcal{D}_{t}\), corresponding to Line 6 of the generic contextual bandit Algorithm 1.

Next, we leverage the characteristic of the contextual kernel being a product kernel. Consequently, the maximum mutual information term for the joint kernel, \(\gamma_{T}\), can be upper bounded by \(c\cdot(\gamma_{T}^{\pi}+\log T)\), where \(c\) denotes the dimensionality of contexts and \(\gamma_{T}^{\pi}\) represents the maximum information gain in a non-contextual setting [14]. Specifically, \(\gamma_{T}^{\pi}\) is computed similarly but is confined to top-k rankings. That is, \(\gamma_{T}^{\pi}=C\cdot\log\left(|I+\sigma^{-2}K_{X_{T}}|\right)\), with \(K_{X_{T}^{\pi}}\) being calculated exclusively using the top-k kernels on the top-k rankings as selected by the bandit algorithm. \(X_{T}^{\pi}\) represents the top-k rankings selected by the bandit algorithm, i.e., excluding the contexts from the collected feedback.

Recalling the formulation for top-k rankings kernels, we have \(K_{X_{T}}=\Phi_{X_{T}}^{T}\Phi_{X_{T}^{\pi}}\), where \(\Phi_{X^{\pi}}\in\mathbb{R}^{\binom{n}{2}}\times T\) comprises feature columns pertinent to the top-\(k\) ranking kernels, as elucidated in Section A. Utilizing the Weinstein-Aronszajn identity, \(\gamma_{T}^{\pi}\) is expressed as \(C\cdot\log\left(|I+\sigma^{-2}\Phi_{X_{T}^{\pi}}\Phi_{X_{T}^{\pi}}^{T}|\right)\). Further, we deduce that \(\gamma_{T}^{\pi}\leq C\cdot\sum_{i=1}^{\binom{n}{2}}\log\left(|1+\sigma^{-2} \lambda_{i}|\right)\), where \(\lambda_{i}\) is an eigenvalue of \(\Phi_{X_{T}^{\pi}}\Phi_{X_{T}^{\pi}}^{T}\). Given the Gershgorin circle theorem, which bounds all eigenvalues of a matrix by the maximum absolute sum of its rows, therefore we can conclude that \(\gamma_{T}^{\pi}=\mathcal{O}(n^{2}\log(n^{2}T))\), as for all the columns of the \(\Phi_{X^{\pi}}\) have bounded normed as given in Claims 2 and 3, i.e., \(||\phi(\pi)||_{2}^{2}\leq 1\)[29].

By combining \(\gamma_{T}^{\pi}=\mathcal{O}(n^{2}\log(n^{2}T))\) with the contextual product kernel, we obtain \(\gamma_{T}=\mathcal{O}(n^{2}c\log(n^{2}T)+c\log T)\), thereby providing the claimed bound in the proposition. 

Next, we build on Propositions 1 and 2 to prove the main theorem regarding the regret of the proposed GP-TopK bandit algorithm for top-k recommendations.

**Theorem 3**.: _If either Assumptions 1 or 2 hold, setting \(\beta_{t}\) as \(2\log\left(\frac{|\mathcal{C}|\cdot|\Pi^{k}|\cdot|^{2}\cdot\pi^{2}}{6\delta}\right)\) and \(300\gamma_{t}\ln^{3}\left(\frac{t}{\delta}\right)\) respectively, the cumulative regret \(\mathcal{R}_{T}\) of the GP-TopK bandit algorithm for top-k recommendations can, with at least \(1-\delta\) probability, be bounded by \(\tilde{\mathcal{O}}(n\sqrt{C_{1}Tc(\log|\mathcal{C}|+k+\log(T^{2}\pi^{2}/6 \delta))})\) under Assumption 1, and \(\tilde{\mathcal{O}}(n\sqrt{C_{1}(2B^{2}c+300n^{2}c^{2}\ln^{3}(T/\delta))T})\) under Assumption 2. Here, \(C_{1}=\frac{8}{\log(1+\xi^{-2})}\), and \(\tilde{\mathcal{O}}\) excludes logarithmic factors related to \(n\), \(k\), and \(T\)._

Proof.: We will prove the above theorem for both cases separately.

**For Assumption-1.** Given \(|\mathcal{C}|\) is finite and \(\beta_{T}=2\log(|\mathcal{D}|T^{2}\pi^{2}/6\delta)\). First, we focus on bounding \(\beta_{T}\) as follows:

\[\beta_{T}=2\log(|\mathcal{D}|T^{2}\pi^{2}/6\delta)\]\[=\mathcal{O}\left(\log\lvert\mathcal{C}\rvert+\log\lvert\Pi^{k}\rvert+ \log(T^{2}\pi^{2}/6\delta)\right)\]

As \(\binom{n}{k}\leq n^{k}\) and \(k!\leq k^{k}\), we also have \(\log\lvert\Pi^{k}\rvert=\log\left(\binom{n}{k}k!\right)\leq\log\left(n^{k}k^{k} \right)=\mathcal{O}(k\log(nk))\), which implies that \(\beta_{T}=\mathcal{O}(\log\lvert\mathcal{C}\rvert+k\log(nk)+\log(T^{2}\pi^{2}/ 6\delta))\). Combining this with Proposition 2, we have following:

\[\mathcal{O}(\gamma_{T}\beta_{T}) =\mathcal{O}\left((n^{2}c\log(n^{2}T)+c\log T)(\log\lvert \mathcal{C}\rvert+k\log(nk)+\log(T^{2}\pi^{2}/6\delta)\right)\] \[=\mathcal{O}\left(n^{2}c\log(n^{2}T)(\log\lvert\mathcal{C}\rvert +k\log(nk)+\log(T^{2}\pi^{2}/6\delta)\right)\quad\text{(Ignoring $c\log T$ term)}\] \[=\tilde{\mathcal{O}}\left(n^{2}c\left(\log\lvert\mathcal{C} \rvert+k+\log(T^{2}\pi^{2}/6\delta)\right)\right).\]

Thus, we showcase the asserted bound for the regret \(\mathcal{R}_{T}\) as \(\tilde{\mathcal{O}}\left(\sqrt{C_{1}T\gamma_{T}\beta_{T}}\right)=\tilde{ \mathcal{O}}\left(n\sqrt{C_{1}Tc(\log\lvert\mathcal{C}\rvert+k+\log(T^{2}\pi^ {2}/6\delta))}\right)\).

For Assumption-2.Given \(\left\lVert f\right\rVert_{k}\leq B\) and \(\beta_{t}=2B^{2}+300\gamma_{t}\ln^{3}(t/\delta)\). First, we bound the \(\beta_{T}\) term using Proposition 2 as follows:

\[\beta_{T} =2B^{2}+300\cdot\gamma_{T}\cdot\ln^{3}(T/\delta),\] \[=2B^{2}+300\cdot\left(n^{2}c\log(n^{2}T)+c\log T\right)\cdot\ln^ {3}(T/\delta).\]

Using the above result, we have the following:

\[\mathcal{O}(\sqrt{C_{1}T\gamma_{T}\beta_{T}}) =\mathcal{O}\left(\sqrt{C_{1}T\gamma_{T}\cdot\left(2B^{2}+300 \cdot\gamma_{T}\cdot\ln^{3}(T/\delta)\right)}\right),\] \[=\mathcal{O}\left(\sqrt{C_{1}Tn^{2}c\log(n^{2}T)\cdot\left(2B^{2} +300\cdot n^{2}c\log(n^{2}T)\cdot\ln^{3}(T/\delta)\right)}\right),\] \[=\tilde{\mathcal{O}}\left(n\sqrt{C_{1}Tc(2B^{2}+300n^{2}c\ln^{3} (T/\delta))}\right).\]

Comparison with Srinivas et al. (2010).Using the identity kernel for top-k rankings, we can develop a finite-dimensional feature for the contextual kernel and apply Theorem 5 by Srinivas et al. (2010). Given that \(\gamma_{T}=O(n^{k}c\log T)\), the regret bounds are as follows under both assumptions. For instance, the calculations for the \(\tilde{\mathcal{O}}(\sqrt{C_{1}T\gamma_{T}\beta_{T}})\) under the Assumption 2 are as follows:

\[\mathcal{O}(\sqrt{C_{1}T\gamma_{T}\beta_{T}}) =\mathcal{O}\left(\sqrt{C_{1}T\gamma_{T}\cdot\left(2B^{2}+300 \cdot\gamma_{T}\cdot\ln^{3}(T/\delta)\right)}\right),\] \[=\mathcal{O}\left(\sqrt{C_{1}T\left(n^{k}c\log T\right)\cdot \left(2B^{2}+300\cdot(n^{k}c\log T)\cdot\ln^{3}(T/\delta)\right)}\right),\] \[=\tilde{\mathcal{O}}\left(n^{\frac{k}{2}}\sqrt{C_{1}Tc(2B^{2}+300 n^{k}c\ln^{3}(T/\delta))}\right).\]

Similarly, we can analogously perform the analysis for Assumption 1 and combine it with Proposition 1 to obtain the regret bounds mentioned in the Table 3.

## Appendix C Experiments - Omitted Details

This section presents omitted details from the main body of the text.

### Compute resources

We utilized multiple NVIDIA Tesla M40 GPUs with \(40\) GB RAM on our in-house cluster for our experiments. The experiments in Section 5 required approximately 5 GPU-hours for small arm space and 24 GPU-hours per iteration for large arm space. We conducted about 50 to 100 iterations throughout the project. The results reported in Section C.3 required the same computational resources as the large arm space experiments.

### Bandit Simulation and Hyper-parameter Configurations - Omitted Details

To set up the simulation, we utilized embeddings trained on the MovieLens dataset using a collaborative filtering approach [6]. We consider a \(1M\) variant of the MovieLens dataset, which contains \(1\) million ratings from \(6040\) users for \(3677\) items. Specifically, we train user embeddings \(\mathbf{c}_{u}\) and item embeddings \(\theta_{i}\) such that the user's attraction to the items are captured by the inner product of the user embedding with the item embeddings, respectively. Both context and item embeddings, i.e., \(\mathbf{c}_{u}\) and \(\theta_{i}\), are \(5\)-dimensional, optimized by considering the \(5\)-fold performance on this dataset. The reward provided in our experiments is contaminated with zero mean and standard deviation equals \(0.05\).

For the _\(\epsilon\)-greedy_ baselines, we considered various values of \(\epsilon\) are considered, specifically \(\epsilon=\{0.01,0.05,0.1\}\). The outcomes are presented for the configuration that demonstrates optimal performance. For _MAB-UCB_ baseline, the algorithm has an upper confidence score \(ucb(i)=\overline{\mu}_{i}+\beta_{mab}\sqrt{\frac{2\ln(t+1)}{n_{i}}}\)[11]. Here, \(\overline{\mu}_{i}\) represents the average reward, \(n\) denotes the total number of rounds, and \(n_{i}\) signifies the frequency of arm \(i\) being played. \(\beta_{mab}\) is a hyper-parameter. We evaluate \(\beta_{mab}\) values within the set \(\{0.1,0.25,0.5\}\) and disclose results for the best-performing configuration. For the parameters of proposed GP-TopK bandit algorithms, we set \(\beta_{t}=\beta_{gp}\cdot\log(|\mathcal{X}|\cdot t^{2}\cdot\pi^{2})\) with \(\beta_{gp}\in\{0.05,0.1,0.5\}\), reporting results the value that yields the best performance. The choice of \(\beta_{t}\) is informed by prior work in GP bandits [27]. The selection of \(\sigma\) for all variants is determined by optimizing the log-likelihood of the observed after every \(10\) rounds by considering values in the set \(\{0.01,0.05,0.1\}\).

### Additional results

**Local search** results for optimizing combinatorial objectives in \(\Pi^{k}\) for \(n=50\) and \(k=6\). Specifically, \(\pi^{\star}=\max_{\pi}\phi^{r}(\pi)^{T}\phi^{r}(\pi^{{}^{\prime}})\), where \(\phi^{r}(\pi^{{}^{\prime}}\) represents the feature vector for Kendall kernels on top-k rankings. Notably, for this optimization problem, it is known that the optimal value is \(1\) obtained by only \(\pi^{{}^{\prime}}\). Figure 4 shows results for this optimization problem when applied to WK, CK, and WCK kernels.

**Reward results** for large arm space for the nDCG + diversity reward. Similar to Figure 3, a large setup with \(n=50\) for \(k=3\) and \(k=6\), is considered. For \(k=6\), the possible arms are over \(1.1\times 10^{10}\) possible top-k rankings. Given the vastness of this arm space, computing the optimal arm for the diversity reward is not straightforward. Therefore, we focus on reporting the cumulative reward in Figure 5. We implement this setup using a Local search in batch mode, updating every \(5\) round and considering a substantial horizon of \(T=100\) rounds. Specifically, we use \(5\) restarts, \(5\) steps in every search direction, and start with \(1000\) initial candidates. Figure 5 shows that the WCK approach demonstrates superior performance, continuing to learn effectively even after extensive rounds.

Figure 4: Local search results for optimizing combinatorial objectives in \(\Pi^{k}\) for \(n=50\) and \(k=6\). For details, see the textual description. Left (a) shows how many times out of \(100\) trials the local search recovers the exact maximizer, i.e., \(\pi^{{}^{\prime}}\), and right plot (b) shows the average value of the objective for the returned maximizer. These results indicate that the local search utilized in this work is effective.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: Section 1 briefs both contributions and scope of this work.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: Section 6 reflects on the limitations of this work.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: Proofs of all Claims and Theorems are provided in the Appendix.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: Section 5 provides necessary details of bandit simulator and experimental setups considered in this work.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: Our code can be accessed using this hyper-link.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyper-parameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: Section 5 provides experimental details and a few remaining details are given in the Appendix C.

Figure 5: Comparative evaluation of bandit algorithms for large arm spaces for the nDCG + diversity reward, with \(>1.1\times 10^{5}\) for the left plot and \(>1.1\times 10^{10}\) for the right plot, respectively. Cumulative reward with respect to the rounds of the bandit algorithm is depicted. Results are averaged over \(6\) trials. In both settings, the WCK approach outperforms other baselines. For more details, see the textual description.

7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: All figures reported in this work have errorbars with them.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: Section C provides relevant information about compute resources.
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes]
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: Section 6 reflects on the impact of this work.
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: this work does not release any such resource or asset.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes]
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: this work release only code with instructions for its usage.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA]
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA]