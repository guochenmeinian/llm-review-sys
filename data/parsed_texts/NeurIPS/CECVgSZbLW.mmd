# Distributional Monte-Carlo Planning with Thompson Sampling in Stochastic Environments

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

We focus on a class of reinforcement learning algorithms, Monte-Carlo Tree Search (MCTS), in stochastic settings. While recent advancements combining MCTS with deep learning have excelled in deterministic environments, they face challenges in highly stochastic settings, leading to suboptimal action choices and decreased performance. Distributional Reinforcement Learning (RL) addresses these challenges by extending the traditional Bellman equation to consider value distributions instead of a single mean value, showing promising results in Deep Q Learning. In this paper, we bring the concept of Distributional RL to MCTS, focusing on modeling value functions as categorical and particle distributions. Consequently, we propose two novel algorithms: Categorical Thompson Sampling for MCTS (CATS), which uses categorical distributions for Q values, and Particle Thompson Sampling for MCTS (PATS), which models Q values with particle-based distributions. Both algorithms employ Thompson Sampling to handle action selection randomness. Our contributions are threefold: We introduce a distributional framework for Monte-Carlo Planning to model uncertainty in return estimation. We prove the effectiveness of our algorithms by achieving a non-asymptotic problem-dependent upper bound on simple regret of order \(O(n^{-1})\), where \(n\) is the number of trajectories. We provide empirical evidence demonstrating the efficacy of our approach compared to baselines in both stochastic and deterministic environments.

## 1 Introduction

_Online planning_ in Markov decision processes (MDPs) involves making real-time decisions based on the current state of the environment. It requires balancing exploration and exploitation while handling uncertainty and partial observability. Monte Carlo Tree Search (MCTS) is a highly effective online planning method for tackling complex MDPs. MCTS has shown impressive performance in various tasks, including traditional board games like Chess and Go, video games, and real-world challenges. Notable successes include advancements in Chess [(35)] and Go [(34; 36; 30)], video game strategy [(28)], robot assembly [(16)], robot path planning [(15; 13)], and autonomous driving [(24)].

Despite these achievements, current MCTS methods are primarily effective in deterministic environments, often overlooking the significant impact of randomness in real-world scenarios. In highly stochastic and partially observable environments, conventional MCTS approaches face substantial challenges due to widespread randomness and limited observability. This leads to compromised value estimates, suboptimal decisions, and diminished overall performance. Therefore, there is a clear need for improved methods capable of navigating the complexities of randomness and partial observability in value estimation.

We now review related works to understand the advancements and limitations in these areas.

**Related work** In MCTS, value estimation methods and action selection rules are critical factors for algorithm performance. Traditional value estimation methods, such as using empirical average meanfor value backup as in the Upper Confidence bounds applied to Trees method (UCT) (21), suffer from underestimation of optimal values while maximum backup suffers from overestimation of optimal values (9). The power mean estimator (12) offers a balanced solution by computing a mean between the average and maximum values. In our approach, we also use power mean for value operator as each V node stores the power mean of empirical means of succeeding Q-value nodes, eliminating the need for V to be modeled as a distribution.

For action selection in MCTS, strategies from Multi-Armed Bandits (MAB) are commonly employed. For instance, UCT extends the UCB1 strategy from bandits to the tree by computing confidence intervals at each step. However, original UCT's performance is hindered by the incorrect choice of logarithmic bonus constant (32). Shah et al. (32) propose an adapted version of UCT incorporating a polynomial bonus term instead of the "logarithmic" bonus term in UCT and show the non-asymptotic convergence of rate \(O(n^{-1/2})\), with \(n\) is the number of rollout trajectories. On the other hand, our method improves over this rate with theoretical guarantee of \(O(n^{-1})\). Although Thompson sampling has been less explored in MCTS, some approaches like those by Bai et al. (1) and Bai et al. (2) incorporate it for exploration. However, these methods lack convergence rate analysis. Furthermore, in the article Bai et al. (1), authors model value functions as a mixture of Normal distributions, which may lack the generality of complex real-world scenarios. Our approach adopts Thompson sampling for action selection but introduces a novelty by modeling the uncertainty of action value estimates over the tree as arbitrary categorical and particle-based distributions. This modification enhances our ability to handle more generality in highly stochastic environments effectively.

_Entropy regularization_ techniques in RL modify value and action selection functions to balance exploration and exploitation, leading to improved value estimation (25; 17; 31; 18). Several works have applied these techniques in MCTS. Maximum Entropy Tree Search (MENTS) (40) emphasizes exploration by integrating MCTS with maximum entropy policy optimization. MENTS aims to maximize cumulative rewards and policy entropy concurrently, regulated by a temperature parameter. Dam et al. (14) extend MENTS by incorporating Relative and Tsallis entropy, leading to the RESTS and TENTS algorithms. However, the effectiveness of MENTS/RENTS/TENTS hinges on the temperature parameter, which may impede convergence. Furthermore, the value estimation converges exponentially to the regularized value not the optimal one. In contrast, Painter et al. (27) utilize a similar action selection approach but employ a maximum backup operator for value estimation. Although their method exhibits exponential decay of simple regret, it heavily relies on the sensitivity of the temperature parameter for Boltzmann Exploration, limiting its practicality.

_Distributional Reinforcement Learning_ (RL) (6; 11; 22) addresses the randomness of the value estimation by introducing a distributional perspective to the traditional Bellman equation. This approach views the value function as a distribution rather than a single mean, providing a comprehensive understanding of uncertainties in rewards and the stochasticity from environments. Through discretization (26), parameterization (6), and quantization (10), it allows for efficient and effective approximation of value distributions, leading to improved performance in various RL tasks. However, these results are only for _learning_ not for _planning_.

**Outline and contribution** In this work, we integrate the distributional approach from reinforcement learning (RL) into the _planning_ framework to tackle the challenges of planning in stochastic environments. We focus on modeling value functions as categorical and particle distributions. Consequently, we propose two novel algorithms: Categorical Thompson Sampling for MCTS (CATS) and Particle Thompson Sampling for MCTS (PATS). CATS represents each Q value function as a categorical distribution and uses Thompson Sampling for action selection to manage uncertainty. PATS models each Q value function with a particle-based distribution, using a nuanced Thompson Sampling approach to handle action selection randomness.

Our contributions are threefold:

1. In section 3, we introduce a distributional framework for _planning_ to model uncertainty in return estimation, enhancing the robustness of value estimation in stochastic environments.
2. In section 4 Theorem 5 and Theorem 6, we prove the effectiveness of our algorithms by achieving a non-asymptotic problem-dependent upper bound on simple regret of \(O(n^{-1})\), which significantly improves upon the current state-of-the-art theoretical analysis of regret, previously established at \(O(n^{-1/2})\) by Shah et al. (33).

3. In section 5, we provide comprehensive empirical evidence demonstrating the efficacy of our approach compared to baselines, showcasing competitive performance in stochastic settings and the Atari benchmark.

In the next section, we describe the problem setting addressed in this paper.

## 2 Setting

In our study, We address the dynamics of an agent navigating an infinite-horizon discounted Markov decision process (MDP), defined formally as \(\mathcal{M}=\langle\mathcal{S},\mathcal{A},\mathcal{R},\mathcal{P},\gamma\rangle\). Here, \(\mathcal{S}\) represents the state space, \(\mathcal{A}\) denotes the set of actions, and \(\mathcal{R}\) quantifies the Reward function of the MDP (\(\mathcal{R}:\mathcal{S}\times\mathcal{A}\times\mathcal{S}\rightarrow\mathbb{R}\)). Transition dynamics are governed by \(\mathcal{P}(\mathcal{S}\times\mathcal{A}\rightarrow\mathcal{S})\), with \(\gamma\in(0,1]\) as the discount factor. The agent interacts with the environment via a policy \(\pi\in\Pi:\mathcal{S}\rightarrow\mathcal{A}\), guiding action selection based on observed states. This yields an action-value function \(Q^{\star}\), indicating the expected cumulative discounted reward from a state-action pair under \(\pi\). The agent seeks the optimal policy maximizing the action-value function, adhering to the Bellman equation (7), given by \(Q(s,a)\triangleq\int_{\mathcal{S}}\mathcal{P}(s^{\prime}|s,a)[\mathcal{R}(s,a,s^{\prime})+\gamma\max_{a^{\prime}}Q(s^{\prime},a^{\prime})]ds\) for all states \(s\) and actions \(a\). Upon acquiring the optimal action-value function, we derive the optimal value function \(V(s)\triangleq\max_{a\in\mathcal{A}}Q(s,a)\) for all states \(s\) in \(\mathcal{S}\).

**Monte-Carlo tree search** (MCTS) (20; 8) is a planning approach for complex Markov decision processes (MDPs). It employs an iterative approach:

_Selection_: It begins by selecting an action using a specified strategy, followed by executing this action through Monte Carlo simulation.

_Expansion_: Subsequently, it assesses the resulting state, either by recursively evaluating if it already exists in the search tree or by inserting it into the tree.

_Simulation_: Or employing a rollout policy via simulations. This iterative process continues until certain termination criteria are met, allowing traversal through the search tree.

_Backpropagation_: Finally, the outcomes of the simulations are propagated backward through the chosen nodes to update their statistical metrics.

**Simple Regret** An MCTS algorithm dynamically gathers trajectories within an MDP starting from an initial state \(s_{0}\). After processing \(t\) trajectories, it provides two outputs:

* \(\tilde{a}_{t}\), a guess for the best action to take at state \(s_{0}\)
* \(\tilde{V}_{t}(s_{0})\) an estimator of the optimal value in \(s_{0}\),

where \(s_{0}\) is the state at the root node. The algorithm's performance can be assessed by its convergence rate \(r(t)\) of the simple regret, formulated as:

\[\mathbb{E}\left[R(s_{0},t)\right]=\mathbb{E}\left[V^{\star}(s_{0})-\widehat{V }_{t}\left(s_{0}\right)\right]\leq r(t),\]

Here, \(R(s_{0},t)=V^{\star}(s_{0})-\widehat{V}_{t}(s_{0})\) is the simple regret of the algorithm at the root node with \(V^{\star}(s_{0})\) representing the optimal value at state \(s_{0}\).

In this article, we analyze an MCTS algorithm employing a maximal planning horizon \(H\) and a playout policy \(\pi_{0}\) with value \(V_{0}\). We define \(\widetilde{V}(s_{H})=V_{0}(s_{H})\) recursively as follows: for all \(h\leq H-1\),

\[\widetilde{Q}(s_{h},a)=r(s_{h},a)+\gamma\sum_{s_{h+1}\in\mathcal{A}_{s_{h}}} \mathbb{P}(s_{h+1}|s_{h},a)\widetilde{V}(s_{h+1}),\widetilde{V}(s_{h})=\max_{ a}\widetilde{Q}(s_{h},a),\] (1)

where \(r(s_{h},a)\) defined formally as the mean intermediate reward at state \(s_{h}\) after taking action \(a\). The primary objective of an MCTS algorithm is to estimate a tied rate \(r(t)\) by constructing estimates of \(\widetilde{Q}(s_{h},a)\) and \(\widetilde{V}(s_{h})\) to ultimately estimate \(\widetilde{Q}(s_{0},a)\) and consequently \(Q^{\star}(s_{0},a)\). In practical implementations of the MCTS algorithm, the maximal depth \(H\) can sometimes be set to \(+\infty\). However, for theoretical analysis, the maximal depth \(H\) is crucial as we will analyze the algorithm that always collects trajectories of length H.

**Distribution Reinforcement Learning** The mathematical framework used in reinforcement learning is based on the Bellman equation (37), which aims to find an agent to maximize the expected utility Q value. However, the single expected value function cannot encapsulate the stochasticity in the reward function and the dynamic of the environments. Recently, in the article (5), authors shed light on the distributional perspective of the Bellman equation by modeling each Q value function as a distribution instead of a single expected value. The main objective is to study the random return \(\mathcal{Q}\) at the state \(s\), action \(a\), and is defined recursively as

\[\mathcal{Q}(s,a)\overset{D}{=}\mathcal{X}(s,a)+\gamma\mathcal{Q}(s^{{}^{ \prime}},a^{{}^{\prime}}),\mathcal{V}(s^{{}^{\prime}})\overset{D}{=}\mathbb{E}_ {\pi}\mathcal{Q}(s^{{}^{\prime}},\pi(\cdot|s^{{}^{\prime}})),\] (2)where \(\mathcal{X}(s,a)\) is the reward distribution at the state \(s\), action \(a\), \(\mathcal{Q}(s,a)\) is the Q value distribution at state \(s\), action \(a\), and \(\mathcal{Q}(s^{{}^{\prime}},a^{{}^{\prime}})\) is the Q value distribution at state \(s^{{}^{\prime}}\), action \(a^{{}^{\prime}}\). \(s^{{}^{\prime}}\) distributed according to \(\mathbb{P}(\cdot|s,a)\), \(a^{{}^{\prime}}\) distributed according to a policy \(\pi(\cdot|s^{{}^{\prime}})\). \(A\stackrel{{ D}}{{=}}B\) denotes that two random variables \(A\) and \(B\) have equal probability laws.

This distributional approach offers a deeper understanding of uncertainty and variability, especially in complex, stochastic systems where traditional expected value representations may fail to capture the true dynamics of the problem. which has been successfully used in Deep Q Learning (5).

**Categorical Value Distribution** Based on the distributional Bellman equation, In the article (5), authors approximate the Q value distribution \(\mathcal{Q}(s,a)\) as a discrete categorical distribution parametrized by \(\mathrm{N}\in\mathbb{N}\), which denotes the number of atoms (N+1) at fixed-sized locations. This method effectively divides the Q value function into a set of equally spaced atoms \(z_{i}(s,a)=Q_{min}+i\triangle z:0\leq i\leq\mathrm{N}\), where \(Q_{min}\) and \(Q_{max}\) are respectively the minimum and maximum values at state \(s\), action \(a\). The size of each atom is set as \(\triangle z:=\frac{Q_{max}-Q_{min}}{\mathrm{N}}\).

This discrete distribution approach is highly expressive and computationally efficient, making it ideal for practical applications. For instance, in the article (5), authors successfully used this representation in Deep Q Learning (C51), showing promising results in several Atari games. In the next section, we demonstrate how to apply this idea to MCTS.

## 3 Distributional Thompson Sampling in Tree Search

In this section, we introduce two novel distributional approaches for MCTS based on Thompson sampling. The first method represents each Q-value node as a categorical distribution, while the second uses particle-based distributions for greater flexibility. Both methods integrate Thompson sampling for improved exploration and performance.

### Distributional Monte-Carlo Tree Search

We leverage the success of distributional reinforcement learning [4; 3; 6] and apply this concept to MCTS. In MCTS, there are two types of nodes: V-nodes and Q-value nodes. Instead of treating each V value and Q value as a single expected value, we model these functions as distributions.

Based on equation (2), we can derive

\[\mathcal{Q}(s,a)\stackrel{{ D}}{{=}}\mathcal{X}(s,a)+\gamma \mathcal{V}(s^{{}^{\prime}}),\mathcal{V}(s^{{}^{\prime}})\stackrel{{ \prime}}{{=}}\sum_{a^{{}^{\prime}}\sim\pi(\cdot|s^{{}^{\prime}})} \mathcal{Q}(s^{{}^{\prime}},a^{{}^{\prime}}),\] (3)

with \(s^{{}^{\prime}}\sim\mathbb{P}(\cdot|s,a)\), where \(\pi(\cdot|s^{{}^{\prime}})\) is formally defined as the tree policy at state \(s^{\prime}\). We can model any Q distribution with equal law distributed as the sum of the distributions of the next reward and the Q distributions of the next states actions. We further model each V distribution, having equal probability law to the expectation of the chosen policy of the next Q-value distributions (3).

Our method follows the same four basic steps of MCTS but is different in Value Backup and Action selection steps. We introduce two distinct methodologies: categorical-based and particle-based. In the categorical based approach, we parameterize each V value and Q value function in the tree as a categorical distribution. In contrast, in the particle-based approach, we model each value distribution as a set of sampling particles, representing the values observed during the tree planning. We provide a detailed explanation for the value backup and action selection of each method in the next section.

### Value Backup

In this work, we employ two approaches to represent the Q value distribution.

**Categorical distribution**: we represent each node in the tree as a categorical distribution. In each Q-value node, we: (1) store the empirical mean value of that Q-value node (same as in UCT), and (2) maintain a categorical distribution of the Q value function. To define a categorical distribution Q function, we require three essential pieces of information:

* The number of atoms (\(\mathrm{N}+1\)): We choose a consistent number of atoms (\(\mathrm{N}+1\)) that remains the same for all Q distributions along the tree.
* Minimum and maximum values (_min_ and _max_): Each node in the tree may have different ranges for its minimum (\(Q_{min}\))1 and maximum (\(Q_{max}\)) values, depending on its state/action in the environment. When a new Q-value node is added to the tree, we initially set \(Q_{min}\) to 0 (assuming we have scaled the reward range to [0, R]) and initialize \(Q_{max}\) to a small number, e.g., \(Q_{max}=0.001\). Since the min and max values are unknown, we start with a small range, that will get updated accordingly to the scale of the observed values.
* Probabilistic parameterization: The probability of each atom (\(p_{i}(s,a)\)) is determined based on the visitation count ratio. In detail, each atom stores statistical information about the visitation count, and the probability of that atom will be calculated as the visitation count divide with the total visitation count of that Q-value node. When we backpropagate the \(r_{t}(s,a)+\gamma\widehat{V}_{t}(s^{\prime})\) value to a specific node, we identify the atom whose value range includes the \(r_{t}(s,a)+\gamma\widehat{V}_{t}(s^{\prime})\) value. At this point, we increase its visitation count.

Additionally, as we backpropagate Monte-Carlo Q values over time, we empirically adjust the \(Q_{min}\) and \(Q_{max}\) values to account for the dynamic range of Q values observed in the tree. This dynamic scaling ensures that the atom locations are effectively rescaled to adapt to the changing conditions. This representation method allows us to encapsulate the knowledge gained through exploration in the form of categorical distributions, which helps in making informed decisions during the tree search.

**Particle based distribution**: We represent each Q value distribution as a collection of sampling particles, which encapsulate the observed values during tree planning. Initially, we maintain an empty set of particles for the Q value distribution, denoted as \(\mathcal{S}(s,a)\). At time step \(t\), upon receiving an intermediate reward \(\overline{Q}_{t}(s,a)=r_{t}(s,a)+\gamma\widehat{V}_{t}(s^{\prime})\), with \(s^{\prime}\sim\mathbb{P}(\cdot|s,a)\), we add \(\overline{Q}_{t}(s,a)\) to the set \(\mathcal{S}(s,a)\) if the particle does not already exist within it. If the particle \(\overline{Q}_{t}(s,a)\) already exists in \(\mathcal{S}(s,a)\), we increase the visitation count ratio associated with that particle.

**Value function:** The Q-value node is crucial in the tree because its representation influences action selection, as detailed in the next section. We now discuss modeling each V-value node. The V-value distribution is based on the expected outcomes of the chosen policy and the subsequent Q-distributions. Thus, the mean of the V-function corresponds to the tree policy's expectation of the means of all succeeding Q-value nodes. The common approach is to use empirical average mean for the value backup, as in UCT [(21)]. However, this approach underestimates the optimal value, while using the maximum value overestimates it [(9)]. The power mean estimator [(12)] provides a balanced solution, falling between the average and maximum values. In our methods, each V node stores the power mean of the empirical means of all succeeding Q-value nodes, eliminating the need to model V as a distribution.

\[\widehat{V}(s)=\left(\sum_{a}^{T_{s,a}(n)}\widehat{Q}^{p}(s,a)\right)^{\frac{1} {p}},p\geq 1,\]

where \(T_{s}(n),T_{s,a}(n)\) are the number of visitations at \(s\) and \(s,a\) at timestep \(n\) respectively. Next, we show how to select actions in the tree based on the categorical distribution of Q-value nodes.

### Action Selection

Thompson sampling has shown promising results in real bandit scenarios due to the randomness of action selection. Taking advantage of the established categorical based distribution and particle based distribution, we use the Thompson sampling method for action selection. We maintain a Dirichlet distribution of parameter of the Q value distribution. We denote the Dirichlet distribution of parameters \((\alpha^{0},\alpha^{1},\ldots,\alpha^{N})\) by \(\text{Dir}(\alpha^{0},\alpha^{1},\ldots,\alpha^{N})\), whose density function is given by \(\frac{\Gamma(\sum_{i=0}^{N}\alpha^{i})}{\prod_{i=0}^{N}\Gamma(\alpha^{i})} \Pi_{i=0}^{N}x_{i}^{\alpha^{i}-1}\) for \((x_{0},\ldots,x_{N})\in[0,1]^{N+1}\) such that \(\sum_{i=0}^{N}x_{i}=1\).

**Categorical distribution**: The probability mass function of the discrete categorical distribution at each Q-value node at state \(s\), action \(a\): \(p(s,a)=[p_{0}(s,a),p_{1}(s,a),\ldots,p_{N}(s,a)]\), where \(p_{i}(s,a)\) represents the probability of selecting the \(i\)-th atom \(z_{i}(s,a),N+1\) is the number of atoms. We maintain a Dirichlet distribution \(\text{Dir}(\alpha^{0}(s,a),\alpha^{1}(s,a),\ldots,\alpha^{N}(s,a))\) as the prior for the Q-value node at state \(s\), action \(a\). At each time step \(t\) we sample \(L_{t}(s,a)\sim\text{Dir}(\alpha^{0}(s,a),\alpha^{1}(s,a),\ldots,\alpha^{N}(s,a))\) and compute \(\overline{\phi}_{t}(s,a)=[z_{0}(s,a),z_{1}(s,a),\ldots,z_{N}(s,a)]^{\top}L_{t} (s,a)\). Then, the action \(a_{t}\) is selected as follows:

\[a_{t}=\operatorname*{arg\,max}_{a}\left\{\overline{\phi}_{t}(s,a)\right\}\]

After taking action \(a_{t}\) and get an intermediate reward \(\overline{Q}_{t}(s,a_{t})=r_{t}(s,a_{t})+\gamma\widehat{V}_{t}(s^{\prime})\). The posterior is also a Dirichlet: \(\text{Dir}(\alpha^{0}(s,a),\ldots,\alpha^{t}(s,a)+1,\ldots,\alpha^{N}(s,a))\) with the intermediate reward at time step \(t\): \(\overline{Q}_{t}(s,a_{t})\) is in the range of the atom \(z_{t}(s,a)\). We denote this mechanism as Categorical Thompson sampling for Tree Search (CATS) method.

**Particle based distribution**: In the particle-based approach, the prior Dirichlet distribution of the Q-value node at state \(s\), action \(a\) is \(\text{Dir}(\alpha(s,a))\), with \(\alpha(s,a)\) is initiated as [1]. Considering each Q value distribution at state \(s\), action \(a\) has a set of particle \(\{\overline{Q}_{t}(s,a)\}\) with the corresponding weighted \(\alpha(s,a)=\{\alpha^{t}(s,a)\}\) At each time step \(t\) we also sample \(L_{t}(s,a)\sim\text{Dir}(\alpha(s,a))\) and compute \(\overline{\phi}_{t}(s,a)=[1,\overline{Q}_{0}(s,a),\overline{Q}_{1}(s,a),\ldots,\overline{Q}_{N}(s,a)]^{\top}L_{t}(s,a)\). Then the action \(a_{t}\) is chosen as

\[a_{t}=\operatorname*{arg\,max}_{a}\left\{\overline{\phi}_{t}(s,a)\right\}.\]

After taking action \(a_{t}\) and get an intermediate reward \(\overline{Q}_{t}(s,a_{t})=r_{t}(s,a_{t})+\gamma\widehat{V}_{t}(s^{\prime})\). We update \(\alpha^{t}(s,a)=\alpha^{t}(s,a)+1\) if \(\overline{Q}_{t}(s,a_{t})\) is in the set \(\{\overline{Q}_{t}(s,a)\}\). If not, we add \(\overline{Q}_{t}(s,a_{t})\) to the set \(\{\overline{Q}_{t}(s,a)\}\) and add \(1\) to the set \(\{\alpha^{t}(s,a)\}=\{\alpha^{t}(s,a),1\}\).

We call this method as Particle Thompson sampling for Tree Search (PATS) method. Detailed pseudocode and a comparison of CATS and PATS can be seen in Fig 1. The two methods are identical in all procedures except for the Q value function backup (**SimulateQ**) and the action selection function (**SelectAction**).

**Remark 1**.: _CATS and PATS both use similar action selection strategies within a bandit setting, specifically referring to Multinomial Thompson Sampling and Non-Parametric Thompson Sampling, respectively [(29)]. While CATS action selection heavily depends strictly on Thompson Sampling by maintaining parameters of posterior Q-value distribution, PATS is not based on the posterior sampling in the strict sense. At each step, it computes an average of the observed rewards with random weight and is a Non-Parametric approach. Furthermore, CATS maintains a fixed set of atoms, whereas in PATS, the number of particles increases depending on the observed Q values._

In the next section, we provide a theoretical analysis of the convergence of simple regret for CATS and PATS.

## 4 Theoretical analysis

Planning in MCTS involves making a sequence of decisions along the tree, where each internal node functions as a non-stationary bandit, with the empirical mean drifting due to the action selection strategy. Therefore, we first study the non-stationary multi-armed bandit settings using the action selections of CATS and PATS, examining the concentration properties of the power mean backup for each arm relative to the optimal arm. We then apply these results to MCTS.

### Non-stationary multi-armed bandit

We consider a class of non-stationary multi-armed bandit (MAB) problems with \(K\geq 1\) arms. Let \(R_{a,t}\) denote the random reward obtained by playing arm \(a\in[K]\) at the time step \(t\) bounded in \([0,R]\). We consider \(\widehat{\mu}_{a,n}=\frac{1}{n}\sum_{t=1}^{n}R_{a,t}\) as the average rewards collected at arm \(a\) after n plays. We first define:

**Definition 1**.: _A sequence of estimators \((\widehat{V}_{n})_{n\geq 1}\) is concentrated and convergent towards some limit \(V\) if the following two properties hold:_

1. _Concentration: For all_ \(n\geq 1\)_, for all_ \(\varepsilon>0\)_,_ \(\exists c>0\) _that_ \(\mathbb{P}\left(|\widehat{V}_{n}-V|>\varepsilon\right)\leq cn^{-1}\varepsilon^ {-1}\)_._
2. _Convergence:_ \(\lim\limits_{n\to\infty}\mathbb{E}[\widehat{V}_{n}]=V\)_._

_In that case, we write \(\underset{n\to\infty}{\text{plim}}\widehat{V}_{n}=V\)._

We assume that the reward sequence \(\left\{R_{a,t}\right\},t\geq 1\) is a non-stationary process satisfying the convergence and concentration properties from Definition 1, by making the following assumption:

**Assumption 1**.: _Consider K arms that for \(a\in[K]\), let \((\widehat{\mu}_{a,n})_{n\geq 1}\) be a sequence of estimator satisfying_

\[\underset{n\to\infty}{\text{plim}}\widehat{\mu}_{a,n}=\mu_{a}.\]

The action selection of CATS and PATS follows closely as in Section 3.3 and pseudocode are shown in Fig. 2. Let us define \(\widehat{\mu}_{n}(p)=\left(\sum_{a=1}^{K}\frac{T_{a}(n)}{n}\widehat{\mu}_{a,T_ {a}(n)}^{p}\right)^{\frac{1}{p}}\) as the power mean value backup operator after \(n\) rounds. Here \(1\leq p<\infty\) is a constant. We denote \(T_{a}(n)\) is the number of visitations of the arm \(a\).

We define \(\mu_{\star}=\max_{a\in[K]}\{\mu_{a}\}\) and assume that \(\mu_{\star}\) is unique. Then, we establish the concentration and convergence properties of the power mean backup operator \(\widehat{\mu}_{n}(p)\) towards the optimal value \(\mu_{\star}\), as shown in Theorem 1 and Theorem 2, respectively for CATS and PATS.

**Theorem 1**.: _For \(a\in[K]\), let \((\widehat{\mu}_{a,n})_{n\geq 1}\) be a sequence of estimator satisfying \(\underset{n\to\infty}{\text{plim}}\widehat{\mu}_{a,n}=\mu_{a}\) and let \(\mu_{\star}=\max\limits_{a}\{\mu_{a}\}\). Assume that all the estimators are bounded in \([0,R]\). We consider a bandit algorithm that selects each arm according to CATS once in each round \(n\geq K\). Then, \(\underset{n\to\infty}{\text{plim}}\widehat{\mu}_{n}(p)=\mu_{\star}\)._

**Theorem 2**.: _For \(a\in[K]\), let \((\widehat{\mu}_{a,n})_{n\geq 1}\) be a sequence of estimator satisfying \(\underset{n\to\infty}{\text{plim}}\widehat{\mu}_{a,n}=\mu_{a}\) and let \(\mu_{\star}=\max\limits_{a}\{\mu_{a}\}\). Assume that all the estimators are bounded in \([0,R]\). We consider a bandit algorithm that selects each arm according to PATS once in each round \(n\geq K\). Then, \(\underset{n\to\infty}{\text{plim}}\widehat{\mu}_{n}(p)=\mu_{\star}\)._

Figure 2: Comparing CATS (left) and PATS (right) in Non-stationary bandits.

Detailed proofs of the two Theorems can be found in the appendix. Based upon these results we analyse the concentration properties for any internal node and convergence of the simple regret in the MCTS in the next section.

### Monte-Carlo Tree Search

Before presenting the main results (Theorem 3 Theorem 4), we first show an important Lemma

**Lemma 1**.: _Let \((\widehat{V}_{m,n})_{n\geq 1}\), \(m\in[M]\), be a sequence of estimator satisfying \(\underset{n\rightarrow\infty}{\text{plim}}\widehat{V}_{m,n}=V_{m}\). Assume that there exists a constant \(L>0\) such that \(L=\text{supremum}(\widehat{V}_{m,n})_{n\geq 1}\). Let \(R_{i}\) be an iid sequence with mean \(\mu\) and \(S_{i}\) be an iid sequence from a distribution \(p=(p_{1},\ldots,p_{M})\) supported on \(\{1,\ldots,M\}\). Introducing the random variables \(N_{m}^{n}=\#|\{i\leq n:S_{i}=s_{m}\}|\), we define the sequence of estimator_

\[\widehat{Q}_{n}=\tfrac{1}{n}\sum_{i=1}^{n}R_{i}+\gamma\sum_{m=1}^{M}\tfrac{N_ {m}^{n}}{n}\widehat{V}_{m,N_{m}^{n}}.\]

_Then \(\underset{n\rightarrow\infty}{\text{plim}}\widehat{Q}_{n}=\mu+\sum_{m=1}^{M} p_{m}V_{m}\)._

The significance of Lemma 1 lies in demonstrating the concentration and convergence of an estimated Q value, conditioned on the concentration and convergence of a child V-value node. Here, \(\widehat{V}_{\cdot,n}\) represents the value estimation at time step \(n\), and \(R_{i}\) denotes an intermediate reward received by taking a specific action at a particular state.

Next, we first start with Theorem 3 to show the convergence and concentration of any V-Node and Q-node in the tree for CATS.

**Theorem 3**.: _When we apply the CATS algorithm, we have_

1. _For any node_ \(s_{h}\) _at the depth_ \(h^{\text{th}}\) _in the tree,_ \(\underset{n\rightarrow\infty}{\text{plim}}\widehat{Q}_{n}(s_{h},a_{k})= \widetilde{Q}(s_{h},a_{k})\)_._
2. _For any node_ \(s_{h}\) _at the depth_ \(h^{\text{th}}\) _in the tree,_ \(\underset{n\rightarrow\infty}{\text{plim}}\widehat{V}_{n}(s_{h})=\widetilde{V} (s_{h})\)_._

We can derive a similar result for PATS as shown in Theorem 4.

**Theorem 4**.: _When we apply the PATS algorithm, we have_

1. _For any node_ \(s_{h}\) _at the depth_ \(h^{\text{th}}\) _in the tree,_ \(\underset{n\rightarrow\infty}{\text{plim}}\widehat{Q}_{n}(s_{h},a_{k})= \widetilde{Q}(s_{h},a_{k})\)_._
2. _For any node_ \(s_{h}\) _at the depth_ \(h^{\text{th}}\) _in the tree,_ \(\underset{n\rightarrow\infty}{\text{plim}}\widetilde{V}_{n}(s_{h})= \widetilde{V}(s_{h})\)_._

The results of Theorems 4 and 4 demonstrate that, at any node in the tree, both the V-value and Q-value nodes are convergent and concentrated. These results are applicable to any power mean backup operator of V-value nodes with \(p\in[1,+\infty)\). Finally, we show important results in Theorem 5, and Theorem 6, since they show the convergence of simple regret of CATS and PATS, respectively.

**Theorem 5**.: _(Convergence of Simple Regret of CATS) We have at the root node \(s_{0}\),_

\[\left|\mathbb{E}\left[V^{\star}(s_{0})-\widehat{V}_{n}\left(s_{0}\right) \right]\right|\leq O(n^{-1}).\]

**Theorem 6**.: _(Convergence of Simple Regret of PATS) We have at the root node \(s_{0}\),_

\[\left|\mathbb{E}\left[V^{\star}(s_{0})-\widehat{V}_{n}\left(s_{0}\right) \right]\right|\leq O(n^{-1}).\]

**Remark 2**.: _These results demonstrate that both CATS and PATS share the same convergence rate for value estimation at the root node of \(\mathcal{O}(n^{-1})\), which improves over the rate \(\mathcal{O}(n^{-1/2})\) of Fixed-Depth-MCTS (33). Furthermore, Our finding more broadly applies to the power mean estimator with \(p\in[1,+\infty)\)._

## 5 Experiments

We compare our methods with UCT (21), Fixed-Depth-MCTS (33), MENTS (40), REITS, TENTS (14), BTS (27) and DNG (1) in a stochastic setting (_SyntheticTree_) to highlight the benefits of CATS and PATS in stochastic environments. Additionally, we test on 17 Atari games, comparing our algorithms with DQN (base network without planning) and other non-distributional planning methods (Power-UCT (12), MENTS (40), TENTS (14)) to demonstrate CATS and PATS' competitiveness and put results in Appendix. In all settings, we use 100 atoms for CATS, and set the discount factor \(\gamma\) to 0.99 for Atari, and \(\gamma\) to 1 for _SyntheticTree_.

**SyntheticTree**: We evaluate CATS and PATS using the synthetic tree toy problem (14). This problem involves a tree with depth \(d\) and branching factor \(k\). Each tree edge has a random value between 0 and 1. Returns at the leaf nodes are simulated using Gaussian distributions with means equal to the sum of edge values from the root to the leaf, and a standard deviation of \(0.5\). Means are normalized between 0 and 1. An agent traverses the tree from the root, aiming to find the leaf node with the highest mean value. Internal nodes give zero reward, while leaf nodes provide a reward sampled from their Gaussian distribution. We introduce stochasticity into the environment by altering the transition probabilities: there is a \(50\%\) chance of moving to the intended node and a \(50\%\) chance of moving to a different node with equal probability. We conduct 25 experiments on five trees with five runs each, covering all combinations of branching factors \(k=\{2,4,6,8,10,12,14,16,100,200\}\) and depths \(d=\{1,2,3,4\}\). We compute the value estimation error at the root node. Fig. 3 shows the convergence of the value estimations of CATS and PATS at the root node in the Synthetic Tree environment which shows they archives faster convergence compared to other methods.

## 6 Conclusion

To conclude, our work introduces Categorical Thompson Sampling for MCTS (CATS) and Particle Thompson Sampling for MCTS (PATS), distributional planning approaches specifically designed to tackle complexities arising from stochasticity. CATS uses a categorical distribution, while PATS uses a particle-based distribution to represent and model the uncertainty inherent in return outcomes. We also propose exploration strategies based on Thompson Sampling that leverage this distributional modeling. Our methods come with a rigorous theoretical convergence guarantee, achieving a simple regret polynomial decay of the order \(O(n^{-1})\), which improves over the \(O(n^{-1/2})\) rate of the fixed version of UCT (32). Empirical findings conclusively demonstrate the effectiveness of our approach in stochastic environments.

## References

* Bai et al. [2013] A. Bai, F. Wu, and X. Chen. Bayesian mixture modelling and inference based thompson sampling in monte-carlo tree search. _Advances in neural information processing systems_, 26, 2013.
* Bai et al. [2014] A. Bai, F. Wu, Z. Zhang, and X. Chen. Thompson sampling based monte-carlo planning in pomdps. _the International Conference on Automated Planning and Scheduling_, 24(1), 2014.
* Bellemare et al. [2016] M. Bellemare, S. Srinivasan, G. Ostrovski, T. Schaul, D. Saxton, and R. Munos. Unifying count-based exploration and intrinsic motivation. In _Advances in neural information processing systems_, pages 1471-1479, 2016.
* Bellemare et al. [2013] M. G. Bellemare, Y. Naddaf, J. Veness, and M. Bowling. The arcade learning environment: An evaluation platform for general agents. _Journal of Artificial Intelligence Research_, 47:253-279, 2013.
* Bellemare et al. [2016] M. G. Bellemare, W. Dabney, and R. Munos. A distributional perspective on reinforcement learning. In _International Conference on Machine Learning_, 2016.

Figure 3: Performance of CATS and PATS in SyntheticTree.

* [6] M. G. Bellemare, W. Dabney, and R. Munos. A distributional perspective on reinforcement learning. In _Proceedings of the 34th International Conference on Machine Learning-Volume 70_, pages 449-458. JMLR. org, 2017.
* [7] R. Bellman. The theory of dynamic programming. Technical report, Rand corp santa monica ca, 1954.
* [8] C. B. Browne, E. Powley, D. Whitehouse, S. M. Lucas, P. I. Cowling, P. Rohlfshagen, S. Tavener, D. Perez, S. Samothrakis, and S. Colton. A survey of monte carlo tree search methods. _IEEE Transactions on Computational Intelligence and AI in games_, 4(1):1-43, 2012.
* [9] R. Coulom. Efficient selectivity and backup operators in monte-carlo tree search. In _International conference on computers and games_. Springer, 2006.
* [10] W. Dabney, G. Ostrovski, D. Silver, and R. Munos. Implicit quantile networks for distributional reinforcement learning. In _International conference on machine learning_, pages 1096-1105. PMLR, 2018.
* [11] W. Dabney, M. Rowland, M. G. Bellemare, and R. Munos. Distributional reinforcement learning with quantile regression. In _Thirty-Second AAAI Conference on Artificial Intelligence_, 2018.
* [12] T. Dam, P. Klink, C. D'Eramo, J. Peters, and J. Pajarinen. Generalized mean estimation in monte-carlo tree search. _arXiv preprint arXiv:1911.00384_, 2019.
* [13] T. Dam, G. Chalvatzaki, J. Peters, and J. Pajarinen. Monte-carlo robot path planning. _IEEE Robotics and Automation Letters_, 7(4):11213-11220, 2022.
* [14] T. Q. Dam, C. D'Eramo, J. Peters, and J. Pajarinen. Convex regularization in monte-carlo tree search. In _International Conference on Machine Learning_, pages 2365-2375. PMLR, 2021.
* [15] S. Eiffert, H. Kong, N. Pirmarzdashti, and S. Sukkarieh. Path planning in dynamic environments using generative rnns and monte carlo tree search. In _2020 IEEE International Conference on Robotics and Automation (ICRA)_, pages 10263-10269. IEEE, 2020.
* [16] N. Funk, G. Chalvatzaki, B. Belousov, and J. Peters. Learn2assemble with structured representations and search for robotic architectural construction. In _Conference on Robot Learning_, pages 1401-1411. PMLR, 2022.
* [17] M. Geist, B. Scherrer, and O. Pietquin. A theory of regularized markov decision processes. In _International Conference on Machine Learning_, pages 2160-2169, 2019.
* [18] T. Haarnoja, A. Zhou, P. Abbeel, and S. Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. In _International Conference on Machine Learning_, pages 1861-1870, 2018.
* [19] J. Honda and A. Takemura. An asymptotically optimal bandit algorithm for bounded support models. In _COLT_, pages 67-79. Citeseer, 2010.
* [20] L. Kocsis and C. Szepesvari. Bandit based monte-carlo planning. In _Proceedings of the 17th European Conference on Machine Learning_, ECML'06, page 282-293, Berlin, Heidelberg, 2006. Springer-Verlag. ISBN 354045375X. doi: 10.1007/11871842_29. URL https://doi.org/10.1007/11871842_29.
* [21] L. Kocsis, C. Szepesvari, and J. Willemson. Improved monte-carlo search. _Univ. Tartu, Estonia, Tech. Rep_, 1, 2006.
* [22] B. Mavrin, H. Yao, L. Kong, K. Wu, and Y. Yu. Distributional reinforcement learning for efficient exploration. In _International conference on machine learning_, pages 4424-4434. PMLR, 2019.
* [23] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, et al. Human-level control through deep reinforcement learning. _Nature_, 518(7540):529-533, 2015.

* Mo et al. [2021] S. Mo, X. Pei, and C. Wu. Safe reinforcement learning for autonomous vehicle using monte carlo tree search. _IEEE Transactions on Intelligent Transportation Systems_, 23(7):6766-6773, 2021.
* Neu et al. [2017] G. Neu, A. Jonsson, and V. Gomez. A unified view of entropy-regularized markov decision processes. _arXiv preprint arXiv:1705.07798_, 2017.
* Nguyen-Tang et al. [2021] T. Nguyen-Tang, S. Gupta, and S. Venkatesh. Distributional reinforcement learning via moment matching. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 35, pages 9144-9152, 2021.
* Painter et al. [2024] M. Painter, M. Baioumy, N. Hawes, and B. Lacerda. Monte carlo tree search with boltzmann exploration. _Advances in Neural Information Processing Systems_, 36, 2024.
* Perez et al. [2014] D. Perez, S. Samothrakis, and S. Lucas. Knowledge-based fast evolutionary mcts for general video game playing. In _2014 IEEE Conference on Computational Intelligence and Games_, pages 1-8. IEEE, 2014.
* Riou and Honda [2020] C. Riou and J. Honda. Bandit algorithms based on thompson sampling for bounded reward distributions. In _Algorithmic Learning Theory_, pages 777-826. PMLR, 2020.
* Schrittwieser et al. [2020] J. Schrittwieser, I. Antonoglou, T. Hubert, K. Simonyan, L. Sifre, S. Schmitt, A. Guez, E. Lockhart, D. Hassabis, T. Graepel, et al. Mastering atari, go, chess and shogi by planning with a learned model. _Nature_, 588(7839):604-609, 2020.
* Schulman et al. [2015] J. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz. Trust region policy optimization. In _International Conference on Machine Learning_, pages 1889-1897, 2015.
* Shah et al. [2020] D. Shah, Q. Xie, and Z. Xu. Non-asymptotic analysis of monte carlo tree search. In _Abstracts of the 2020 SIGMETRICS/Performance Joint International Conference on Measurement and Modeling of Computer Systems_, pages 31-32, 2020.
* Shah et al. [2022] D. Shah, Q. Xie, and Z. Xu. Nonasymptotic analysis of monte carlo tree search. _Operation Research_, 70(6):3234-3260, 2022.
* Silver et al. [2016] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. van den Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam, M. Lanctot, S. Dieleman, D. Grewe, J. Nham, N. Kalchbrenner, I. Sutskever, T. Lillicrap, M. Leach, K. Kavukcuoglu, T. Graepel, and D. Hassabis. Mastering the game of Go with deep neural networks and tree search. _Nature_, 529(7587):484-489, Jan. 2016. doi: 10.1038/nature16961.
* Silver et al. [2017] D. Silver, T. Hubert, J. Schrittwieser, I. Antonoglou, M. Lai, A. Guez, M. Lanctot, L. Sifre, D. Kumaran, T. Graepel, et al. Mastering chess and shogi by self-play with a general reinforcement learning algorithm. _arXiv preprint arXiv:1712.01815_, 2017.
* Silver et al. [2017] D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang, A. Guez, T. Hubert, L. Baker, M. Lai, A. Bolton, Y. Chen, T. Lillicrap, F. Hui, L. Sifre, G. van den Driessche, T. Graepel, and D. Hassabis. Mastering the game of go without human knowledge. _Nature_, 550:354-, Oct. 2017. URL http://dx.doi.org/10.1038/nature24270.
* Sutton and Barto [2018] R. S. Sutton and A. G. Barto. _Reinforcement learning: An introduction_. MIT press, 2018.
* Van Hasselt et al. [2016] H. Van Hasselt, A. Guez, and D. Silver. Deep reinforcement learning with double q-learning. In _Thirtieth AAAI conference on artificial intelligence_, 2016.
* Weissman et al. [2003] T. Weissman, E. Ordentlich, G. Seroussi, S. Verdu, and M. J. Weinberger. Inequalities for the l1 deviation of the empirical distribution. _Hewlett-Packard Labs, Tech. Rep_, 2003.
* Xiao et al. [2019] C. Xiao, R. Huang, J. Mei, D. Schuurmans, and M. Muller. Maximum entropy monte-carlo planning. In _Advances in Neural Information Processing Systems_, pages 9516-9524, 2019.

Outline
* Notations will be described in Section B.
* Supporting Lemmas are presented in Section C.
* The Convergence of CATS and PATS in Non-stationary multi-armed bandits is shown in Section D.
* Section E presents the concentration and convergence guarantee of CATS and PATS in MCTS.
* Section F discusses about Limitations and possible improvements.
* Experimental setup is provided in Section G.
* Additional Experimental results are shown in Section H.

## Appendix B Notations

## Appendix C Supporting Lemmas

We start with a result of the following lemma which plays an important role in the analysis of our MCTS algorithm.

**Lemma 1**.: _For \(m\in[M]\), let \((\widehat{V}_{m,n})_{n\geq 1}\) be a sequence of estimator satisfying \(\underset{n\rightarrow\infty}{\text{plim}}\widehat{V}_{m,n}=V_{m}\). Assume that there exists a constant \(L>0\) such that \(L=\text{supremum}(\widehat{V}_{m,n})_{n\geq 1}\). Let \(R_{i}\) be an iid sequence with mean \(\mu\) and \(S_{i}\) be an iid sequence from a distribution \(p=(p_{1},\ldots,p_{M})\) supported on \(\{1,\ldots,M\}\). Introducing the random variables \(N_{m}^{n}=\#|\{i\leq n:S_{i}=s_{m}\}|\), we define the sequence of estimator_

\[\widehat{Q}_{n}=\frac{1}{n}\sum_{i=1}^{n}R_{i}+\gamma\sum_{m=1}^{M}\frac{N_{m} ^{n}}{n}\widehat{V}_{m,N_{m}^{n}}.\]

_Then there exists some constant \(c^{\prime}\) (which depends on \(p_{i}\) (i=1,2,...,M), \(\gamma\), \(\mu\)) such that_

\[\underset{n\rightarrow\infty}{\text{plim}}\widehat{Q}_{n}=\mu+\sum_{m=1}^{M}p _{m}V_{m}.\]

Proof.: Let \(p=(p_{1},p_{2},...p_{M}),p\in\triangle^{M}\) where \(\triangle^{M}=\{x\in\mathbb{R}^{M}:\sum_{i=1}^{M}R_{i}=1,R_{i}\geq 0\}\) is the \((M-1)\)-dimensional simplex. Let us study a random vector \(\widehat{p}_{n}=(\frac{N_{m}^{n}}{n},\frac{N_{m}^{n}}{n},...,\frac{N_{M}^{n}}{ n})\). Let us define

\begin{table}
\begin{tabular}{c c c} \hline \hline
**Notation** & **Type** & **Description** \\ \hline \hline \(K\) & \(\mathbb{N}\) & Number of arms \\ \hline \(T_{a}(t)\) & \(\mathbb{N}\) & Number of visitations at arm \(a\) after \(t\) timesteps \\ \hline \(\mu_{a}\) & \(\mathbb{R}\) & mean value of arm \(a\) \\ \hline \(a_{\star}\) & \(\mathcal{A}\) & optimal action \\ \hline \(\mu_{\star}\) & \(\mathbb{R}\) & mean value of an optimal arm. We assume it is unique. \\ \hline \(\widehat{\mu}_{n}(p)\) & \(\mathbb{R}\) & power mean estimator, with a constant \(p\in[1,+\infty)\) \\ \hline \(\widehat{\mu}_{a,n}\) & \(\mathbb{R}\) & mean estimator of arm \(a\) after \(n\) visitations \\ \hline \hline \end{tabular}
\end{table}
Table 1: List of all notations for Non-stationary Multi-arms bandit.

\(V=(V_{1},V_{2},...V_{M})\). Let \(\widehat{R}_{n}=\frac{1}{n}\sum_{i=1}^{n}R_{i},\widehat{V}_{n}=(\widehat{V}_{1,N_{1 }^{n}},\widehat{V}_{2,N_{2}^{n}},...,\widehat{V}_{M,N_{M}^{n}})\), \(\sum_{i=1}^{M}N_{i}^{n}=n\), \(N_{i}^{n}\) is the number of times that population \(i\) was observed. We have \(\widehat{Q}_{n}=\widehat{R}_{n}+\gamma\left<\widehat{p}_{n},\widehat{V}_{n}\right>\). Therefore,

\[\mathbb{P}\bigg{(}\widehat{Q}_{n}-\big{(}\mu+\gamma\left<p,V \right>\big{)}\geq\epsilon\bigg{)} \leq\mathbb{P}\bigg{(}\widehat{R}_{n}-\mu\geq\frac{1}{2}\epsilon \bigg{)}+\mathbb{P}\bigg{(}\gamma\left<\widehat{p}_{n},\widehat{V}_{n} \right>-\gamma\left<p,Y\right>\geq\frac{1}{2}\epsilon\bigg{)}\] \[\leq\exp\{-2n\frac{\epsilon^{2}}{4}\}+\underbrace{\mathbb{P} \bigg{(}\left<\widehat{p}_{n},\widehat{V}_{n}\right>-\left<p,Y\right>\geq \frac{1}{2\gamma}\epsilon\bigg{)}}_{\mathcal{A}}.\]

To upper bound A, let us consider \(\left<\widehat{p}_{n},\widehat{V}\right>-\left<p,V\right>=\left<(\widehat{p}_{ n}-p),\widehat{V}_{n}\right>+\left<p,(\widehat{V}-V)\right>\). Then,

\[A\leq\underbrace{\mathbb{P}\bigg{(}\left<(\widehat{p}_{n}-p),\widehat{V}_{n} \right>\geq\frac{1}{4\gamma}\epsilon\bigg{)}}_{\mathcal{A}_{1}}+\underbrace{ \mathbb{P}\bigg{(}\left<p,(\widehat{V}_{n}-V)\right>\geq\frac{1}{4\gamma} \epsilon\bigg{)}}_{\mathcal{A}_{2}}.\]

\begin{table}
\begin{tabular}{c c c} \hline \hline
**Notation** & **Type** & **Description** \\ \hline \hline \(\gamma\) & \(\mathbb{R}\) & Discount factor \\ \hline N & \(\mathbb{N}\) & Number of atoms \\ \hline \(s_{h}\) & \(\mathcal{S}\) & state at depth \(h\) \\ \hline \(\widehat{V}_{t}(s)\) & \(\mathbb{R}\) & Estimated Value function at state \(s\) after \(t\) visitations \\ \hline \(T_{s}(t)\) & \(\mathbb{N}\) & Number of visitations at state \(s\) after \(t\) timesteps \\ \hline \(T_{s,a}(t)\) & \(\mathbb{N}\) & Number of visitations at \((s,a)\) after \(t\) timesteps \\ \hline \(T_{s,a}^{s^{\prime}}(t)\) & \(\mathbb{N}\) & Number of visitations at \((s,a)\) that goes to \(s^{\prime}\) after \(t\) timesteps \\ \hline \(\widehat{Q}_{t}(s,a)\) & \(\mathbb{R}\) & Estimated Q Value function at state \(s\) action \(a\) after \(t\) visitations \\ \hline \(Q_{\min}(s,a)\) & \(\mathbb{R}\) & Minimum value for the Q value distribution at state \(s\), action \(a\) \\ \hline \(Q_{\max}(s,a)\) & \(\mathbb{R}\) & Maximum value for the Q value distribution at state \(s\), action \(a\) \\ \hline \(\mathcal{R}(s,a)\) & & Reward distribution at state \(s\) action \(a\) \\ \hline \(\mathcal{V}(s)\) & & Value distribution at state \(s\) \\ \hline \(\mathcal{Q}(s,a)\) & & Q Value distribution at state \(s\) action \(a\) \\ \hline \(p_{i}(s,a)\) & \(\mathbb{R}\) & Probability of the \(i_{th}\) atom at the Q Value distribution at state \(s\) action \(a\) \\ \hline \(\triangle z\) & \(\mathbb{R}\) & Size of each atom \\ \hline \(z_{i}(s,a)\) & \(\mathbb{R}\) & value of the atom \(i^{th}\) at state \(s\), action \(a\). \\ \hline \(\overline{Q}_{t}(s,a)\) & \(\mathbb{R}\) & intermediate Q value at time \(t\) at \((s,a)\) \\ \hline \hline \end{tabular}
\end{table}
Table 2: List of all notations for Monte-Carlo Tree Search.

By applying a Holder inequality to \(\widehat{p}_{n}-p\) and \(\widehat{V}\), we obtain

\[\Big{\langle}(\widehat{p}_{n}-p),\widehat{V}_{n}\Big{\rangle}\leq \parallel\widehat{p}_{n}-p\parallel_{1}\parallel\widehat{V}_{n} \parallel_{\infty}=\parallel\widehat{p}_{n}-p\parallel_{1}L,\]

with \(L\) is the supremum of \(\widehat{V}\). Then we can derive

\[A_{1} =\mathbb{P}\bigg{(}\;\big{\langle}(\widehat{p}_{n}-p),\widehat{V }_{n}\Big{\rangle}\geq\frac{1}{4\gamma}\epsilon\bigg{)}\leq\mathbb{P}\bigg{(} \parallel\widehat{p}_{n}-p\parallel_{1}L\geq\frac{1}{4\gamma}\epsilon\bigg{)}\] \[=\mathbb{P}\bigg{(}\parallel\widehat{p}_{n}-p\parallel_{1} \geq\frac{1}{4\gamma L}\epsilon\bigg{)}.\]

According to (39), we have for any \(M\geq 2\) and \(\delta\in[0,1]\)

\[\mathbb{P}\bigg{(}\parallel\widehat{p}_{n}-p\parallel_{1} \geq\sqrt{\frac{2M\ln(2/\delta)}{n}}\bigg{)}\leq\delta.\]

Define \(\epsilon=\sqrt{\frac{2M\ln(2/\delta)}{n}}\), therefore \(\delta=2\exp\{\frac{-n\epsilon^{2}}{2M}\}\), we have

\[\mathbb{P}\bigg{(}\parallel\widehat{p}_{n}-p\parallel_{1} \geq\epsilon\bigg{)}\leq 2\exp\{\frac{-n\epsilon^{2}}{2M}\}.\]

Therefore,

\[A_{1}\leq\mathbb{P}\bigg{(}\parallel\widehat{p}_{n}-p\parallel_ {1}\geq\epsilon\bigg{)}\leq 2\exp\{\frac{-n\epsilon^{2}}{32M\gamma^{2}L^{2}}\}.\]

We also have

\[A_{2} =\mathbb{P}\bigg{(}\sum_{m=1}^{M}p_{m}(\widehat{V}_{m,N_{m}^{n}}- V_{m})\geq\frac{1}{4\gamma}\epsilon\bigg{)}\] \[\leq\sum_{m=1}^{M}\mathbb{E}\bigg{[}\mathbb{P}\bigg{(}\frac{1}{N _{m}^{n}}\sum_{t=1}^{N_{m}^{n}}V_{m,t}-V_{m}\geq\frac{1}{4\gamma p_{m}} \epsilon\big{|}N_{m}^{n}\bigg{)}\bigg{]}\] \[\leq\sum_{m=1}^{M}\mathbb{E}\bigg{[}c(N_{m}^{n})^{-1}(\frac{ \epsilon}{4\gamma p_{m}})^{-1}\bigg{]}.\]

Let us define an event \(\mathcal{E}=\bigg{\{}N_{m}^{n}\geq\frac{np_{m}}{2}\bigg{\}}\). Therefore,

\[A_{2}\leq\sum_{m=1}^{M}\mathbb{E}\bigg{[}c(\frac{np_{m}}{2})^{-1 }(\frac{\epsilon}{4\gamma p_{m}})^{-1}\bigg{]}\] \[+\sum_{m=1}^{M}\mathbb{E}\bigg{[}\mathbb{P}(N_{m}^{n}<\frac{np_{m }}{2})\bigg{]}=\sum_{m=1}^{M}(c2^{1+2}\gamma^{1}p_{m}^{-1+1})n^{-1}\epsilon^{-1}\] \[+\sum_{m=1}^{M}\mathbb{E}\bigg{[}\mathbb{P}(N_{m}^{n}-p_{m}n\leq- \frac{p_{m}n}{2})\bigg{]}\] \[\leq\sum_{m=1}^{M}(c2^{3}\gamma)n^{-1}\epsilon^{-1}+\sum_{m=1}^{M }\exp\bigg{\{}-2n(\frac{p_{m}n}{2})^{2}\bigg{\}}\]

We consider \(p_{m}>0\) only since if \(p_{m}=0,p_{m}(\widehat{V}_{m,N_{m}^{n}}-V_{m})=0\), and has been eliminated. Therefore,

\[A\leq A_{1}+A_{2}\leq 2\exp\{\frac{-n\epsilon^{2}}{32M\gamma^{2}L^{2}} \}+\sum_{m=1}^{M}(c2^{3}\gamma)n^{-1}\epsilon^{-1}+\sum_{m=1}^{M}\exp\bigg{\{}- 2n(\frac{p_{m}n}{2})^{2}\bigg{\}}.\]That leads to

\[\mathbb{P}\bigg{(}\widehat{Q}_{n}-\left(\mu+\gamma\left\langle p,V \right\rangle\right)\geq\epsilon\bigg{)}\leq\exp\{-2n\frac{\epsilon^{2}}{4}\}\] \[+2\exp\{\frac{-n\epsilon^{2}}{32M\gamma^{2}L^{2}}\}+\sum_{m=1}^{M }(c2^{3}\gamma)n^{-1}\epsilon^{-1}+\sum_{m=1}^{M}\exp\bigg{\{}-2n(\frac{p_{m}n} {2})^{2}\bigg{\}}\leq c^{{}^{\prime}}n^{-1}\epsilon^{-1},\]

with \(c^{{}^{\prime}}>0\) depends on \(c,M,p_{i}\). So that

\[\mathbb{P}\bigg{(}\widehat{Q}_{n}-\left(\mu+\gamma\left\langle p,V\right\rangle \right)\geq\epsilon\bigg{)}\leq c^{{}^{\prime}}n^{-1}\epsilon^{-1},\]

By following the same steps, we can derive

\[\mathbb{P}\bigg{(}\widehat{Q}_{n}-\left(\mu+\gamma\left\langle p,V\right\rangle \right)\leq-\epsilon\bigg{)}\leq c^{{}^{\prime}}n^{-1}\epsilon^{-1}.\]

Therefore, with \(n\geq 1,\epsilon>0\),

\[\mathbb{P}\bigg{(}\left|\widehat{Q}_{n}-\left(\mu+\gamma\left\langle p,V \right\rangle\right)\right|\geq\epsilon\bigg{)}\leq c^{{}^{\prime}}n^{-1} \epsilon^{-1}.\]

Furthermore,

\[\widehat{Q}_{n}-\left(\mu+\gamma\left\langle p,V\right\rangle \right) =(\widehat{R}_{n}-\mu)+\left(\gamma\left\langle\widehat{p}_{n}, \widehat{V}_{n}\right\rangle-\gamma\left\langle p,Y\right\rangle\right)\] \[=(\widehat{R}_{n}-\mu)+\gamma\bigg{(}\left\langle(\widehat{p}_{n }-p),\widehat{V}_{n}\right\rangle+\left\langle p,(\widehat{V}-V)\right\rangle \bigg{)}\]

Therefore,

\[\Rightarrow\left|\mathbb{E}[\widehat{Q}_{n}]-\left(\mu+\gamma \left\langle p,V\right\rangle\right)\right|\leq\left|\mathbb{E}[(\widehat{R}_{ n}-\mu)]\right|+\gamma\bigg{(}\left|\mathbb{E}[\widehat{p}_{n}-p]\right| \left|\widehat{V}_{n}\right|+p\left|\mathbb{E}[\widehat{V}-V]\right|\bigg{)}\] \[\Rightarrow\left|\mathbb{E}[\widehat{Q}_{n}]-\left(\mu+\gamma \left\langle p,V\right\rangle\right)\right|\leq\left|\mathbb{E}[(\widehat{R}_{ n}-\mu)]\right|+\gamma\bigg{(}L\left|\mathbb{E}[\widehat{p}_{n}-p]\right|+p \left|\mathbb{E}[\widehat{V}-V]\right|\bigg{)}\]

Also because \(\lim\limits_{n\rightarrow\infty}\mathbb{E}[\widehat{V}_{m,n}]=V_{m}\), \(\lim\limits_{n\rightarrow\infty}\frac{\widehat{N}_{n}^{n}}{n}=p_{m}\), and \(\mathbb{E}[(\widehat{R}_{n}-\mu)]=0\) so that,

\[\lim\limits_{n\rightarrow\infty}\mathbb{E}[\widehat{Q}_{n}]=\mu+\gamma\sum_ {m=1}^{M}p_{m}V_{m}.\]

That mean

\[\underset{n\rightarrow\infty}{\text{plim}}\widehat{Q}_{n}=\mu+\gamma\sum_{m= 1}^{M}p_{m}V_{m},\]

which concludes the proof. 

Results from Lemma 1 is important as it shows the concentration for the Q value estimation given the concentration of V value of the children nodes.

**Lemma 2**.: _Let consider non-negative variables \(x,y\in\mathbb{R}^{+}\), and a constant m that \(0\leq m\leq 1\). Then_

\[(x+y)^{m}\leq x^{m}+y^{m}.\]

Proof.: With \(y=0\), or \(x=0\), the inequality (2) becomes correct. Let consider the case where \(x>0,y>0\), the inequality (2) can be written as

\[(\frac{x}{y}+1)^{m}\leq\left(\frac{x}{y}\right)^{m}+1.\]

Let us define a function

\[f(t)=(t+1)^{m}-t^{m}-1,(t>0).\]We can see that

\[f^{{}^{\prime}}(t)=m(t+1)^{m-1}-mt^{m-1}=m\left((t+1)^{m-1}-t^{m-1}\right)\leq 0 \text{ with }m\in[0,1],t>0,\]

because \(g(x)=x^{m-1}\) is a decreasing function with \(m\in[0,1],x>0\). Therefore,

\[f(t)\leq f(0)=0\text{ with }t>0.\]

So that,

\[(t+1)^{m}-t^{m}-1\leq 0,(t>0).\]

with \(t=\frac{x}{y}\geq 0\), we can derive the inequality (2). 

We use Minkowski's inequality as shown below

**Lemma 3**.: _(**Minkowski's inequality**) Given \(p\geq 1,\{x_{i},y_{i}\}\in\mathbb{R},i=1,2,...,n\), then we have the following inequality_

\[\left(\sum_{i}(|x_{i}+y_{i}|)^{p}\right)^{\frac{1}{p}}\leq\left(\sum_{i}(|x_{i }|)^{p}\right)^{\frac{1}{p}}+\left(\sum_{i}(|y_{i}|)^{p}\right)^{\frac{1}{p}}.\]

Proof.: This is a basic result. 

**Lemma 4**.: _(**Markov's inequality**) If \(X\) is a nonnegative random variable and \(a>0\), then the probability that X is at least a is at most the expectation of X divided by a:_

\[\mathbf{Pr}(X>a)\leq\frac{\mathbb{E}[X]}{a}.\]

Proof.: This is a well-known result. 

## Appendix D Convergence of CATS and PATS in Non-stationary multi-armed bandits

We note that in an MCTS tree, each node is considered a non-stationary multi-armed bandit where the average mean drifts due to the given action selection strategy. Therefore, we first study the convergence of CATS and PATS in non-stationary multi-armed bandits where the action selection is Thompson sampling, with the power mean backup operator at the root node. Detailed descriptions of the CATS and PATS in Non-stationary multi-armed bandits settings can be found in the main article in the Theoretical Analysis section.

We first establish the convergence and concentration properties for the power mean backup operator in non-stationary bandits, detailed in Theorem 1 for CATS and Theorem 2 for PATS.

To achieve these results, we demonstrate that the expected payoff of the power mean backup operator decays polynomially at a rate of \(O(\frac{\log n}{n})\). This is supported by Lemma 7 for CATS and Lemma 8 for PATS. Critical to this analysis are Lemma 5 and Lemma 6, which establish an upper bound of \(\log(n)\) for the expected number of suboptimal arm pulls.

We introduce some important definitions. \(F_{a}^{n}\) represents the empirical cumulative distribution function of arm \(a\) after \(n\) visitations, and \(F_{a}\) represents the cumulative distribution function of arm \(a\). We employ the following distance measure: If \(P\) and \(Q\) are two distributions characterized by parameters \(p=(p_{0},p_{1},\cdots,p_{N})\) and \(q=(q_{0},q_{1},\cdots,q_{N})\) respectively, then the distance is defined as

\[d(P,Q):=\parallel p-q\parallel_{\infty}=\sup_{i\in[0,N]}|p_{i}-q_{i}|\]

This represents the \(L^{\infty}\) distance between \(p\) and \(q\) in \(\mathbb{R}^{N+1}\). We also denotes \(\text{KL}(P\parallel Q)\) as the Kullback-Leibler divergence between \(P\) and \(Q\), and denote \(\mathcal{K}_{\text{inf}}(F_{a},\mu_{\star})=\text{inf}_{G:\mathbb{E}[G]>\mu_{ \star}}\text{KL}(F_{a}\parallel G)\). In addition, we denote \(\mathcal{K}_{\text{inf}}^{(N)}(F_{a},\mu_{\star})=\inf\left\{\text{KL}(F_{a} \parallel G)\right|\text{ the support of }\text{G}\in\left\{0,\frac{R}{N},\frac{2R}{N},\cdots,R \right\},\mathbb{E}[G]>\mu_{\star}\right\}\).

We see that the definition of \(\mathcal{K}_{\text{inf}}(F_{a},\mu_{\star})\) and \(\mathcal{K}_{\text{inf}}^{(N)}(F_{a},\mu_{\star})\) is only difference in the support set.

We denote the true parameter of arm \(a\) by \(p_{a}=(p_{a}^{0},p_{a}^{1},\ldots,p_{a}^{N})\) with \(p_{a}^{i}=\mathbf{Pr}_{X\sim F_{a}}[X=\frac{i}{N}]\). We denote the parameter of the posterior distribution of arm \(a\) as \(\alpha_{a}=(\alpha_{a}^{0},\alpha_{a}^{1},\ldots,\alpha_{a}^{N})\). Since each arm \(a\) is non-stationary, we also denote the parameter of arm \(a\) after \(n\) visitations by \(p_{a}(n)=(p_{a}^{0}(n),p_{a}^{1}(n),\ldots,p_{a}^{N}(n))\) with \(p_{a}^{i}(n)=\mathbf{Pr}_{X\sim F_{a}^{n}}[X=\frac{i}{N}]\). The parameter of the posterior distribution of arm \(a\) denoted as \(\alpha_{a}(n)=(\alpha_{a}^{0}(n),\alpha_{a}^{1}(n),\ldots,\alpha_{a}^{N}(n))\) We first show the results of an important Lemma 5. The proof follows closely to the Proof of Proposition 7 (29). The only difference is that in our settings, we study non-stationary bandits.

**Lemma 5**.: _Consider Categorical Thompson Sampling(CATS) strategy applied to a non-stationary problem where the pay-off sequence satisfies Assumption 1. Let \(T_{a}(n)\) denote the number of plays of arm \(a\) up to timestep \(n\)._

_If \(a\) is the index of a suboptimal arm, Then for any \(\epsilon_{0},\epsilon_{1}\geq 0\), each sub-optimal arm \(a\) is played in expectation at most_

\[\mathbb{E}[T_{a}(n)]\leq\frac{(1+\epsilon_{0})\log n}{\mathcal{K}_{\text{inf} }^{(N)}(F_{a},\mu_{\star})-\epsilon_{1}}+o(\log n)+O(1),\]

Proof.: We have \(\overline{\phi}_{a,t}=[0,\frac{R}{N},\frac{2R}{N},\cdots,R]^{\top}L_{a,t}\), with \(L_{a,t}\sim\text{Dir}(\alpha_{a}^{0}(t),\ldots,\alpha_{a}^{N}(t))\).

To analyze the expectation associated with selecting a suboptimal arm \(a\), we decompose it into two components:

\[\mathbb{E}\left[\sum_{t=1}^{n}\mathbbm{1}(I(t)=a)\right]= \underbrace{\mathbb{E}\left[\sum_{t=1}^{n}\mathbbm{1}(I(t)=a), \overline{\phi}_{a,t}\geq\mu_{\star}-\epsilon_{1},d(\widehat{F}_{I(t)},F_{I(t) })\leq\epsilon_{2})\right]}_{A1}\] \[+\underbrace{\mathbb{E}\left[\sum_{t=1}^{n}\mathbbm{1}(I(t)=a), \overline{\phi}_{a,t}<\mu_{\star}-\epsilon_{1},d(\widehat{F}_{I(t)},F_{I(t)}) >\epsilon_{2})\right]}_{A2}\]

We first find an upper bound for \(A_{1}\):

\[A1=\sum_{t=1}^{n}\sum_{m=1}^{n}\mathbbm{1}\left(I(t)=a,\overline{\theta}_{k}( t)\geq\mu_{\star}-\epsilon_{1};\parallel\frac{\alpha_{a}(t)}{T_{k}(t)+N+1}-p_{a}(t) \parallel_{\infty}\leq\epsilon_{2},T_{k}(t)=m\right)\]

We see that if the event

\[\left\{I(t)=a,\overline{\theta}_{k}(t)\geq\mu_{\star}-\epsilon_{1};\parallel \frac{\alpha_{a}(t)}{T_{k}(t)+N+1}-p_{a}(t)\parallel_{\infty}\leq\epsilon_{2}, T_{k}(t)=m\right\}\]

occurs at step t for a certain \(m\in[1,n]\), then \(T_{k}(t^{\prime})>T_{k}(t)=m\) for any \(t^{\prime}>t\). Therefore, for any \(m\in[n]\)

\[\sum_{t=1}^{n}\mathbbm{1}\left(I(t)=a,\overline{\theta}_{k}(t)\geq\mu_{\star} -\epsilon_{1};\parallel\frac{\alpha_{a}(t)}{T_{k}(t)+N+1}-p_{a}(t)\parallel_{ \infty}\leq\epsilon_{2},T_{k}(t)=m\right)\leq 1\]

We can bound for any \(m_{0}\in[n]\)

\[A1 \leq m_{0}+\sum_{t=1}^{n}\sum_{m=m_{0}}^{n}\mathbb{E}\left[1 \left(I(t)=a,\overline{\theta}_{k}(t)\geq\mu_{\star}-\epsilon_{1};\parallel \frac{\alpha_{a}(t)}{T_{k}(t)+N+1}-p_{a}(t)\parallel_{\infty}\leq\epsilon_{2}, T_{k}(t)=m\right)\right]\] \[\leq m_{0}+\sum_{t=1}^{n}\sum_{m=m_{0}}^{n}\mathbf{Pr}\left( \overline{\theta}_{k}(t)\geq\mu_{\star}-\epsilon_{1}\right|\parallel\frac{ \alpha_{a}(t)}{T_{k}(t)+N+1}-p_{a}(t)\parallel_{\infty}\leq\epsilon_{2},T_{k}(t )=m\right)\] \[\times\mathbf{Pr}\left(\parallel\frac{\alpha_{a}(t)}{T_{k}(t)+N+1 }-p_{a}(t)\parallel_{\infty}\leq\epsilon_{2},T_{k}(t)=m\right)\] (4)By applying results of Lemma 13 Appendix F (29), we have

\[\mathbf{Pr}\left(\overline{\theta}_{k}(t)\geq\mu_{\star}-\epsilon_{1} \bigg{|}\alpha_{a},T_{k}(t)=m\right)\] \[\leq C(m+N+1)^{N/2}\exp\{-(m+N+1)\text{KL}(P_{\alpha_{a}(t)}\parallel P _{\mu_{\star}-\epsilon_{1}}^{*})\}\]

where \(P_{\mu_{\star}-\epsilon_{1}}^{*}=\arg\min_{x:u^{\star}x\geq\mu_{\star}- \epsilon_{1}}\text{KL}(P_{\alpha_{a}}\parallel x)\) and \(P_{\alpha_{a}(t)}=\frac{1}{n+N+1}\alpha_{a}(t)\). And by definition \(\text{KL}(P_{\alpha_{a}(t)}\parallel P_{\mu_{\star}-\epsilon_{1}}^{*})= \mathcal{K}_{\text{inf}}(P_{\alpha_{a}(t)},\mu_{\star}-\epsilon_{1})\), therefore

\[\mathbf{Pr}\left(\overline{\theta}_{k}(t)\geq\mu_{\star}-\epsilon_{1} \bigg{|}\alpha_{a}(t),T_{k}(t)=m\right)\] \[\leq C(m+N+1)^{N/2}\exp\{-(m+N+1)\mathcal{K}_{\text{inf}}(P_{ \alpha_{a}(t)},\mu_{\star}-\epsilon_{1})\},\]

where \(C=\frac{\exp\{1/12\}}{\Gamma(N+1)}\left(\frac{1}{\sqrt{2\pi}}\right)^{N}\). On the other hand, \(\mathcal{K}_{\text{inf}}(x,\mu_{\star}-\epsilon_{1})\) is continuous in \(x\in[0,1]^{N+1}\) on the probability simplex with respect to the \(L^{\infty}\) distance from ((19), Theorem 7) and Lemma 18 in Appendix H (29). Therefore, for any \(\epsilon_{3}>0\), there exists \(\epsilon_{2}>0\) and constant \(C^{\prime}>0\) such that

\[\mathbf{Pr}\left(\overline{\theta}_{k}(t)\geq\mu_{\star}-\epsilon _{1}\bigg{|}\parallel\frac{\alpha_{a}(t)}{T_{k}(t)+N+1}-p_{a}(t)\parallel_{ \infty}\leq\epsilon_{2},T_{k}(t)=m\right)\] \[\leq C^{\prime}\exp\{-(m+N+1)(\mathcal{K}_{\text{inf}}(p_{a},\mu_ {\star}-\epsilon_{1})-\epsilon_{3})\}\]

And because \(\mathbf{Pr}\left(\parallel\frac{\alpha_{a}(t)}{T_{k}(t)+N+1}-p_{a}(t) \parallel_{\infty}\leq\epsilon_{2},T_{k}(t)=m\right)\leq 1\). Therefore,

\[A1 \leq m_{0}+C_{1}^{\prime}\sum_{t=1}^{n}\exp\{-(m+N+1)(\mathcal{K} _{\text{inf}}(p_{a},\mu_{\star}-\epsilon_{1})-\epsilon_{3})\}\] \[\leq m_{0}+C_{1}^{\prime}T\exp\{-(m+N+1)(\mathcal{K}_{\text{inf} }(p_{a},\mu_{\star}-\epsilon_{1})-\epsilon_{3})\}\] (5)

Choosing \(m_{0}=\frac{\log n}{\mathcal{K}_{\text{inf}}(p_{a},\mu_{\star}-\epsilon_{1})- \epsilon_{3}}-N-1\), we have

\[A1\leq\frac{\log n}{\mathcal{K}_{\text{inf}}(p_{a},\mu_{\star}-\epsilon_{1})- \epsilon_{3}}-N-1+C_{1}^{\prime}\]

Furthermore, as from ((19), Theorem 7), it is proven that \(\mu\rightarrow\mathcal{K}_{\text{inf}}(F,\mu)\) is continuous for \(\mu<1\), when we scale reward from [0,1] to \([0,R]\) therefore \(\mu\) from [0,1] to \([0,R]\). We have \(\mu\rightarrow\mathcal{K}_{\text{inf}}(F,\mu)\) is continuous for \(\mu<R\). Therefore, \(\forall\epsilon_{4}>0,\exists\epsilon_{1}>0\), such that

\[|\mathcal{K}_{\text{inf}}(p_{a},\mu^{*}-\epsilon_{1})-\mathcal{K}_{\text{inf }}(p_{a},\mu^{*})|\leq\epsilon_{4}\]

\[\Rightarrow\mathcal{K}_{\text{inf}}(p_{a},\mu^{*}-\epsilon_{1})-\epsilon_{3} \geq\mathcal{K}_{\text{inf}}(p_{a},\mu^{*})-\epsilon_{3}-\epsilon_{4}\]

Therefore, \(\forall\epsilon_{0}>0\)

\[A1\leq\frac{(\epsilon_{0}+1)\log n}{\mathcal{K}_{\text{inf}}(p_{a},\mu_{ \star})}-N-1+C_{1}^{\prime}\]

Also According to Proposition 8 (29), for any \(\epsilon_{0}>0\) we have

\[A2\leq O(1)\] (6)

Combining inequality (5) and inequality (6) leads us to

\[\mathbb{E}[T_{a}(n)]\leq\frac{(1+\epsilon_{0})\log n}{\mathcal{K}_{\text{inf} }^{(N)}(F_{a},\mu_{\star})}+o(\log n)+O(1).\]

Therefore which concludes the proof. 

**Lemma 6**.: _Consider Particle Thompson Sampling(PATS) strategy applied to a non-stationary problem where the pay-off sequence satisfies Assumption 1. Then for any \(\epsilon_{0}\geq 0\). Let \(T_{a}(n)\) denote the number of plays of arm \(a\) up to timestep \(n\). Then if \(a\) is the index of a suboptimal arm, then each sub-optimal arm \(a\) is played in expectation at most_

\[\mathbb{E}[T_{a}(n)]\leq\frac{\log n}{\mathcal{K}_{\text{inf}}(F_{a},\mu_{ \star})-\epsilon_{0}}+o(\log n)+O(1).\]Proof.: In this Theorem, we use the Levy distance. Recall that the Levy distance between two cumulative distribution functions \(F\) and \(G\) on \([0,1]\) is defined as

\[D_{L}(F,G)=\inf\{\epsilon>0:\forall x\in[0,1],F(x-\epsilon)-\epsilon\leq G(x)\leq F (x+\epsilon)+\epsilon\}.\]

The proof follows the same steps as in Lemma 5. We also can derive

\[\mathbb{E}\left[\sum_{t=1}^{n}\mathbbm{1}(I(t)=a)\right]= \underbrace{\mathbb{E}\left[\sum_{t=1}^{n}\mathbbm{1}(I(t)=a), \overline{\phi}_{a,t}\geq\mu_{*}-\epsilon_{1},D_{L}(\widehat{F}_{I(t)},F_{I(t )})\leq\epsilon_{2})\right]}_{B1}\] \[+\underbrace{\mathbb{E}\left[\sum_{t=1}^{n}\mathbbm{1}(I(t)=a), \overline{\phi}_{a,t}<\mu_{*}-\epsilon_{1},D_{L}(\widehat{F}_{I(t)},F_{I(t)}) >\epsilon_{2})\right]}_{B2}\]

We can use the same ways of derivations as in Lemma 5, equation (4) to have the same bound

\[B1 \leq m_{0}+\sum_{t=1}^{n}\sum_{m=m_{0}}^{n}\mathbf{Pr}\left( \overline{\theta}_{k}(t)\geq\mu_{*}-\epsilon_{1}\bigg{|}D_{L}\left(\widehat{ F}_{a}(t),F_{a}(t)\right)\leq\epsilon_{2},T_{k}(t)=m\right)\] \[\times\mathbf{Pr}\left(D_{L}\left(\widehat{F}_{a}(t),F_{a}(t) \right)\leq\epsilon_{2},T_{k}(t)=m\right)\] (7)

According to Lemma 15 in Appendix G.1 (29) on conditional probabilities, for any \(\nu\in(0,1)\) we have

\[\mathbf{Pr}\left(\overline{\theta}_{k}(t)\geq\mu_{*}-\epsilon_{1 }\bigg{|}D_{L}\left(\widehat{F}_{a}(t),F_{a}(t)\right)\leq\epsilon_{2},T_{k}(t )=m\right)\] \[\leq\frac{1}{\nu}\exp\left\{-n\left(\mathcal{K}_{\text{inf}}( \widehat{F}_{a}(t),\mu_{*}-\epsilon_{1})-\nu\frac{\mu_{*}-\epsilon_{1}}{1-(\mu _{*}-\epsilon_{1})}\right)\right\}\]

Because \(\mathcal{K}_{\text{inf}}(F,\mu)\) is continuous in \(F\) with respect to the Levy distance from (19), Theorem 7, for any \(\epsilon_{3}>0\) there exists \(\epsilon_{2}>0\) such that

\[D_{L}(\widehat{F}_{a}(t),F_{a})\leq\epsilon_{2}\Rightarrow\Big{|}\mathcal{K} _{\text{inf}}(\widehat{F}_{a}(t),\mu_{*}-\epsilon_{1})-\mathcal{K}_{\text{ inf}}(F_{a},\mu_{*}-\epsilon_{1})\Big{|}\leq\epsilon_{3}\]

Therefore, \(\forall\nu\in(0,1)\) and for any \(\epsilon_{5}>0\), there exists \(\epsilon_{1},\epsilon_{2}>0\) such that

\[\mathbf{Pr}\left(\overline{\theta}_{k}(t)\geq\mu_{*}-\epsilon_{1 }\bigg{|}D_{L}\left(\widehat{F}_{a}(t),F_{a}(t)\right)\leq\epsilon_{2},T_{k}(t )=m\right)\] \[\leq\frac{1}{\nu}\left(-m\left(\mathcal{K}_{\text{inf}}(F_{a},\mu _{*}-\epsilon_{1})-\epsilon_{3}-\nu\frac{\mu_{*}-\epsilon_{1}}{1-(\mu_{*}- \epsilon_{1})}\right)\right)\] \[\stackrel{{\text{(Theorem \ref{thm})}}}{{\leq}}\frac{1}{\nu}\left(-m \left(\mathcal{K}_{\text{inf}}(F_{a},\mu_{*})\frac{\epsilon_{1}}{1-\mu_{*}}- \epsilon_{3}-\nu\frac{\mu_{*}-\epsilon_{1}}{1-(\mu_{*}-\epsilon_{1})}\right)\right)\]

This implies that \(\forall\epsilon_{0}>0\), there exists \(\nu\in(0,1),\epsilon_{1}>0\) and \(\epsilon_{2}>0\) such that

\[\mathbf{Pr}\left(\overline{\theta}_{k}(t)\geq\mu_{*}-\epsilon_{1 }\bigg{|}D_{L}\left(\widehat{F}_{a}(t),F_{a}(t)\right)\leq\epsilon_{2},T_{k}(t )=m\right)\leq\frac{1}{\nu}\exp\left\{-m(\mathcal{K}_{\text{inf}}(F_{a},\mu_{ *})-\epsilon_{0})\right\}\]

Therefore, according to inequality (7) and the fact that

\[\mathbf{Pr}\left(D_{L}\left(\widehat{F}_{a}(t),F_{a}(t)\right)\leq\epsilon_{2 },T_{k}(t)=m\right)\leq 1\]

we have

\[B1 \leq m_{0}+\sum_{t=1}^{n}\frac{1}{\nu}\exp\left\{-m(\mathcal{K}_{ \text{inf}}(F_{a},\mu_{*})-\epsilon_{0})\right\}\] \[\leq m_{0}+\frac{1}{\nu}T\exp\left\{-m_{0}(\mathcal{K}_{\text{ inf}}(F_{a},\mu_{*})-\epsilon_{0})\right\}\]Choose \(m_{0}=\frac{\log n}{\mathcal{K}_{\text{inf}}(F_{a},\mu_{\star})-\epsilon_{0}}\) we have

\[B1\leq\frac{\log n}{\mathcal{K}_{\text{inf}}(F_{a},\mu_{\star})-\epsilon_{0}}+ \frac{1}{\nu}\]

Also According to Proposition 10 (29), for any \(\epsilon_{0}>0\) we have

\[B2\leq O(1)\]

That leads us to

\[\mathbb{E}[T_{a}(n)]\leq\frac{\log n}{\mathcal{K}_{\text{inf}}(F_{a},\mu_{ \star})-\epsilon_{0}}+o(\log n)+O(1),\]

which concludes the proof. 

**Lemma 7**.: _Consider Categorical Thompson Sampling(CATS) strategy applied to a non-stationary problem where the pay-off sequence satisfies Assumption 1. Let us define the power mean estimator \(\widehat{\mu}_{n}(p)\) as \(\widehat{\mu}_{n}(p)=\left(\sum_{a=1}^{K}\frac{T_{a}(n)}{n}\widehat{\mu}_{a,T_ {a}(n)}^{p}\right)^{\frac{1}{p}}\), and \(\delta_{\star,n}=\mu_{\star}-\mu_{\star,n}\) For any \(p\geq 1,\epsilon_{0}>0\), we have_

\[|\mathbb{E}[\widehat{\mu}_{n}(p)]-\mu_{\star}|\leq|\delta_{\star,n}|+\frac{R} {n}\sum_{a=1,a\neq a_{\star}}^{K}\left\{\frac{(1+\epsilon_{0})\log n}{\mathcal{ K}^{(N)}(F_{a},\mu^{\star})}+o(\log n)+O(1)\right\}\]

Proof.: We observe that

\[|\widehat{\mu}_{n}(p)-\mu_{\star}|\leq|\widehat{\mu}_{n}(p)-\mu_{\star,n}|+| \mu_{\star}-\mu_{\star,n}|=|\widehat{\mu}_{n}(p)-\mu_{\star,n}|+|\delta_{\star,n}|\]

Furthermore,

\[\widehat{\mu}_{a,T_{a}(n)}\leq\mu_{a,n}+\left|\widehat{\mu}_{a,T_{a}(n)}-\mu_ {a,n}\right|.\] (8)

Since \(\mu_{\star,n}=\max_{a\in[K]}\{\mu_{a,n}\}\), we have

\[\widehat{\mu}_{n}(p)-\mu_{\star,n}=\widehat{\mu}_{n}(p)-\sum_{a=1 }^{K}T_{a}(n)\mu_{\star,n} \leq\left(\sum_{a=1}^{K}\frac{T_{a}(n)}{n}\left(\widehat{\mu}_{a,T _{a}(n)}\right)^{p}\right)^{\frac{1}{p}}-\left(\sum_{a=1}^{K}\frac{T_{a}(n)}{ n}\left(\mu_{a,n}\right)^{p}\right)^{\frac{1}{p}}\] \[=\frac{\left(\sum_{a=1}^{K}T_{a}(n)\left(\widehat{\mu}_{a,T_{a}( n)}\right)^{p}\right)^{\frac{1}{p}}-\left(\sum_{a=1}^{K}T_{a}(n)\left(\mu_{a,n} \right)^{p}\right)^{\frac{1}{p}}}{n^{\frac{1}{p}}}\]

Applying Minkowski's inequality from Lemma 3, and the result of (8), we have

\[\widehat{\mu}_{n}(p)-\mu_{\star,n} \leq\frac{\left(\sum_{a=1}^{K}T_{a}(n)\left(\mu_{a}+\left|\widehat {\mu}_{a,T_{a}(n)}-\mu_{a,n}\right|\right)^{p}\right)^{\frac{1}{p}}-\left( \sum_{a=1}^{K}T_{a}(n)\left(\mu_{a,n}\right)^{p}\right)^{\frac{1}{p}}}{n^{ \frac{1}{p}}}\] \[\leq\frac{\left(\sum_{a=1}^{K}T_{a}(n)\left(\left|\widehat{\mu} _{a,T_{a}(n)}-\mu_{a,n}\right|\right)^{p}\right)^{\frac{1}{p}}}{n^{\frac{1}{p}}}\]

On the other hand,

\[\mu_{\star,n}-\widehat{\mu}_{n}(p) =\frac{n\mu_{\star,n}-n\widehat{\mu}_{n}(p)}{n}=\frac{n\mu_{\star,n}-(\sum_{a=1}^{K}T_{a}(n)\mu_{a,n})+\sum_{a=1}^{K}T_{a}(n)\mu_{a,n}-n \widehat{\mu}_{n}(p)}{n}\] \[=\frac{\sum_{a=1,a\neq a_{\star}}^{K}T_{a}(n)\left|\mu_{\star,n}- \mu_{a,n}\right|+\sum_{a=1}^{K}T_{a}(n)\mu_{a,n}-n\widehat{\mu}_{n}(p)}{n}\] \[\leq R\sum_{a=1,a\neq a_{\star}}^{K}\frac{T_{a}(n)}{n}+\sum_{a=1}^ {K}\frac{T_{a}(n)}{n}\mu_{a,n}-\widehat{\mu}_{n}(p)\] (9)Because power mean is an increasing function of \(p\), so that

\[\sum_{a=1}^{K}\frac{T_{a}(n)}{n}\mu_{a,n}\leq\left(\sum_{a=1}^{K}\frac{T_{a}(n)}{ n}\left(\mu_{a,n}\right)^{p}\right)^{1/p}.\]

Furthermore, we observe that

\[\mu_{a,n}\leq\widehat{\mu}_{a,T_{a}(n)}+\left|\widehat{\mu}_{a,T_{a}(n)}-\mu_{ a,n}\right|.\]

So that, from equation (9) we have

\[\mu_{\star,n}-\widehat{\mu}_{n}(p) \leq R\sum_{a=1,a\neq a_{\star}}^{K}\frac{T_{a}(n)}{n}+\left(\sum_ {a=1}^{K}\frac{T_{a}(n)}{n}\left(\mu_{a,n}\right)^{p}\right)^{1/p}-\widehat{\mu }_{n}(p)\] \[\leq R\sum_{a=1,a\neq a_{\star}}^{K}\frac{T_{a}(n)}{n}\] \[+\frac{\left(\sum_{a=1}^{K}T_{a}(n)\left(\widehat{\mu}_{a,T_{a}(n )}+\left|\widehat{\mu}_{a,T_{a}(n)}-\mu_{a,n}\right|\right)^{p}\right)^{\frac{ 1}{p}}-\left(\sum_{a=1}^{K}T_{a}(n)\left(\widehat{\mu}_{a,T_{a}(n)}\right)^{p }\right)^{\frac{1}{p}}}{n^{\frac{1}{p}}}\]

\[\stackrel{{\text{(Minkovski's inequality)}}}{{ \leq}}R\sum_{a=1,a\neq a_{\star}}^{K}\frac{T_{a}(n)}{n}+\frac{\left(\sum_{a=1} ^{K}T_{a}(n)\left(\left|\widehat{\mu}_{a,T_{a}(n)}-\mu_{a,n}\right|\right)^{p }\right)^{\frac{1}{p}}}{n^{\frac{1}{p}}}\] \[\stackrel{{\text{(Properties of $L^{\prime}$ norm)}}}{{\leq}}R\sum_{a=1,a\neq a_{\star}}^{K}\frac{T_{a}(n)}{n}+\frac{\left(\sum_{a=1} ^{K}T_{a}(n)\left(\left|\widehat{\mu}_{a,T_{a}(n)}-\mu_{a,n}\right|\right) \right)}{n^{\frac{1}{p}}}\] \[=R\sum_{a=1,a\neq a_{\star}}^{K}\frac{T_{a}(n)}{n}+\frac{\sum_{a=1 }^{K}\left(\left|\sum_{t}^{T_{a}(n)}R_{a,t}-T_{a}(n)\mu_{a,n}\right|\right)}{ n^{\frac{1}{p}}}\]

Therefore

\[\left|\mathbb{E}[\widehat{\mu}_{n}(p)-\mu_{\star,n}]\right| \leq R\sum_{a=1,a\neq a_{\star}}^{K}\frac{\mathbb{E}[T_{a}(n)]}{n }+\frac{\mathbb{E}\left[\left(\left|\sum_{a=1}^{K}\sum_{t}^{T_{a}(n)}R_{a,t}- T_{a}(n)\mu_{a,n}\right|\right)\right]}{n^{\frac{1}{p}}}\] \[=R\sum_{a=1,a\neq a_{\star}}^{K}\frac{\mathbb{E}[T_{a}(n)]}{n}\]

Please note that because we study non-stationary bandits, \(\mathbb{E}[\sum_{t}^{n}R_{a,t}]=n\mu_{a,n}\), therefore,

\[\frac{\mathbb{E}\left[\left(\left|\sum_{a=1}^{K}\sum_{t}^{T_{a}(n)}R_{a,t}- T_{a}(n)\mu_{a,n}\right|\right)\right]}{n^{\frac{1}{p}}}=0\]

According to Lemma 5, we have

\[\left|\mathbb{E}[\widehat{\mu}_{n}(p)-\mu_{\star,n}]\right|\leq R\sum_{a=1,a \neq a_{\star}}^{K}\frac{\mathbb{E}[T_{a}(n)]}{n}\leq\frac{R}{n}\sum_{a=1,a \neq a_{\star}}^{K}\left\{\frac{(1+\epsilon_{0})\log n}{\mathcal{K}^{(N)}(F_{ a},\mu^{\star})}+o(\log n)+O(1)\right\},\]

which concludes the proof. 

**Lemma 8**.: _Consider Particle Thompson Sampling(PATS) strategy applied to a non-stationary problem where the pay-off sequence satisfies Assumption 1. Let us define the power mean estimator \(\widehat{\mu}_{n}(p)\) as \(\widehat{\mu}_{n}(p)=\left(\sum_{a=1}^{K}\frac{T_{a}(n)}{n}\widehat{\mu}_{a,T_ {a}(n)}^{p}\right)^{\frac{1}{p}}\), and \(\delta_{\star,n}=\mu_{\star}-\mu_{\star,n}\) For any \(p\geq 1,\epsilon_{0}>0\), we have_

\[\left|\mathbb{E}[\widehat{\mu}_{n}(p)]-\mu_{\star}\right|\leq\left|\delta_{ \star,n}\right|+\frac{R}{n}\sum_{a=1,a\neq a_{\star}}^{K}\left\{\frac{\log n} {\mathcal{K}_{\text{inf}}(F_{a},\mu^{\star})-\epsilon_{0}}+o(\log n)+O(1)\right\}\]Proof.: Similar to Lemma 7, we can derive

\[|\mathbb{E}[\widehat{\mu}_{n}(p)-\mu_{\star,n}]|\leq|\delta_{\star,n}|+R\sum_{a=1,a\neq a_{\star}}^{K}\frac{\mathbb{E}[T_{a}(n)]}{n}.\]

And according to Lemma 6, we have

\[|\mathbb{E}[\widehat{\mu}_{n}(p)-\mu_{\star,n}]|\leq R\sum_{a=1,a\neq a_{\star} }^{K}\frac{\mathbb{E}[T_{a}(n)]}{n}\leq\frac{R}{n}\sum_{a=1,a\neq a_{\star}}^{ K}\left\{\frac{\log n}{\mathcal{K}_{\text{inf}}(F_{a},\mu^{\star})-\epsilon_{0}}+o( \log n)+O(1)\right\},\]

which concludes the proof. 

**Theorem 1**.: _For \(a\in[K]\), let \((\widehat{\mu}_{a,n})_{n\geq 1}\) be a sequence of estimator satisfying \(\underset{n\to\infty}{\text{lim}}\widehat{\mu}_{a,n}=\mu_{a}\) and let \(\mu_{\star}=\max\limits_{a}\{\mu_{a}\}\). Assume that all the estimators are bounded in \([0,R]\). We consider a bandit algorithm that selects each arm according to CATS once in each round \(n\geq K\). Then, for all \(p\in[1,\infty)\), the sequence of estimators_

\[\widehat{\mu}_{n}(p)=\left(\sum_{a=1}^{K}\frac{T_{a}(n)}{n}\widehat{\mu}_{a,T _{a}(n)}^{p}\right)^{\frac{1}{p}},\]

_where \(T_{a}(n)=\sum_{t=1}^{n-1}\mathds{1}(a_{t}=a)\) is the number of selections of a prior to round \(n\) satisfies_

\[\underset{n\to\infty}{\text{lim}}\widehat{\mu}_{n}(p)=\mu_{\star}.\]

Proof.: We first prove that \(\underset{n\to\infty}{\text{lim}}\mathbb{E}[\widehat{\mu}_{n}(p)]=\mu_{\star}\). According to the result of Lemma 7, we have

\[|\mathbb{E}[\widehat{\mu}_{n}(p)]-\mu_{\star}| \leq|\delta_{\star,n}|+R\sum_{a=1,a\neq a_{\star}}^{K}\frac{ \mathbb{E}[T_{a}(n)]}{n}\] \[\leq|\delta_{\star,n}|+\frac{R}{n}\sum_{a=1,a\neq a_{\star}}^{K} \left\{\frac{(1+\epsilon_{0})\log n}{\mathcal{K}^{(N)}(F_{a},\mu^{\star})}+o( \log n)+O(1)\right\}\]

with \(\delta_{\star,n}=\mu_{\star}-\mu_{\star,n}\), and because \(\underset{n\to\infty}{\text{lim}}\mu_{\star,n}=\mu_{\star}\), we can concludes that

\[\underset{n\to\infty}{\text{lim}}\mathbb{E}[\widehat{\mu}_{n}(p)]=\mu_{\star}.\]

Second, we prove that

\[\forall n\geq 1,\forall\varepsilon>0,\exists c>0\text{ that }\mathbb{P}\left(|\widehat{\mu}_{n}(p)-\mu_{\star}|>\varepsilon\right)\leq cn ^{-1}\varepsilon^{-1}.\]

We observe that

\[|\widehat{\mu}_{n}(p)-\mu_{\star}|\leq|\widehat{\mu}_{n}(p)-\mu_{ \star,n}|+|\mu_{\star}-\mu_{\star,n}|=|\widehat{\mu}_{n}(p)-\mu_{\star,n}|+| \delta_{\star,n}|\] \[\Longrightarrow \mathbb{P}(|\widehat{\mu}_{n}(p)-\mu_{\star}|\geq\epsilon)\leq \mathbb{P}(|\widehat{\mu}_{n}(p)-\mu_{\star,n}|\geq\epsilon/2)+\mathbb{P}(| \delta_{\star,n}|\geq\epsilon/2).\]

Because \(\underset{n\to n}{\lim}|\delta_{\star,n}|=0\), therefore, \(\exists N_{0}>0\) such that \(\forall n\geq N_{0}\), we have \(|\delta_{\star,n}|<\epsilon/2\) that means

\[\forall n>N_{0},\mathbb{P}(|\delta_{\star,n}|\geq\epsilon/2)=0.\]

Next, according to Lemma 7,

\[|\mathbb{E}[\widehat{\mu}_{n}(p)]-\mu_{\star,n}|\leq\frac{R}{n}\sum_{a=1,a\neq a _{\star}}^{K}\left\{\frac{(1+\epsilon_{0})\log n}{\mathcal{K}^{(N)}(F_{a},\mu^ {\star})}+o(\log n)+O(1)\right\}=O(n^{-1}),\]

that leads to

\[\mathbb{P}(|\widehat{\mu}_{n}(p)-\mu_{\star,n}|\geq\epsilon/2)\leq\frac{| \mathbb{E}[\widehat{\mu}_{n}(p)]-\mu_{\star,n}|}{\epsilon/2}=\frac{O(n^{-1})} {\epsilon/2}.\]Therefore, \(\exists c>0\) such that

\[\mathbb{P}(|\widehat{\mu}_{n}(p)-\mu_{\star,n}|\geq\epsilon/2)\leq cn^{-1} \epsilon^{-1},\]

which means

\[\forall n\geq N_{0},\forall\varepsilon>0,\exists c>0\text{ that }\mathbb{P}\left(| \widehat{\mu}_{n}(p)-\mu_{\star}|>\varepsilon\right)\leq cn^{-1}\varepsilon^{ -1}.\]

Now we see that \(|\widehat{\mu}_{n}(p)-\mu_{\star}|\leq R\). With \(\epsilon\geq R\), we have \(|\widehat{\mu}_{n}(p)-\mu_{\star}|>\epsilon\Leftrightarrow|\widehat{\mu}_{n}( p)-\mu_{\star}|>R\), therefore the inequality holds as

\[\mathbb{P}\left(|\widehat{\mu}_{n}(p)-\mu_{\star}|>\varepsilon\right)=0\leq cn ^{-1}\varepsilon^{-1}.\]

with \(0<\epsilon<R,1\leq n<N_{0}\Rightarrow n\epsilon<RN_{0}\Rightarrow n^{-1} \varepsilon^{-1}>1/RN_{0}\). Therefore

\[\forall C>1/RN_{0}\Rightarrow\mathbb{P}\left(|\widehat{\mu}_{n}(p)-\mu_{ \star}|>\varepsilon\right)\leq 1<Cn^{-1}\varepsilon^{-1},\]

which means

\[\forall n\geq 1,\forall\varepsilon>0,\exists C>0\text{ that }\mathbb{P}\left(| \widehat{\mu}_{n}(p)-\mu_{\star}|>\varepsilon\right)\leq Cn^{-1}\varepsilon^{ -1}.\]

That concludes the proof. 

**Theorem 2**.: _For \(a\in[K]\), let \((\widehat{\mu}_{a,n})_{n\geq 1}\) be a sequence of estimator satisfying \(\underset{n\rightarrow\infty}{\text{plim}}\widehat{\mu}_{a,n}=\mu_{a}\) and let \(\mu_{\star}=\max\limits_{a}\{\mu_{a}\}\). Assume that all the estimators are bounded in \([0,R]\). We consider a bandit algorithm that selects each arm according to PATS once in each round \(n\geq K\)._

_Then, for all \(p\in[1,\infty)\), the sequence of estimators_

\[\widehat{\mu}_{n}(p)=\left(\sum_{a=1}^{K}\frac{T_{a}(n)}{n}\widehat{\mu}_{a,T_ {a}(n)}^{p}\right)^{\frac{1}{p}},\]

_where \(T_{a}(n)=\sum_{t=1}^{n-1}\mathds{1}(a_{t}=a)\) is the number of selections of \(a\) prior to round \(n\) satisfies_

\[\underset{n\rightarrow\infty}{\text{plim}}\widehat{\mu}_{n}(p)=\mu_{\star}.\]

Proof.: The proof follows the same steps as Theorem 1. We first prove that \(\underset{n\rightarrow\infty}{\text{lim}}\mathbb{E}[\widehat{\mu}_{n}(p)]= \mu_{\star}\). According to the result of Lemma 8, we have

\[|\mathbb{E}[\widehat{\mu}_{n}(p)]-\mu_{\star}| \leq|\delta_{\star,n}|+R\sum_{a=1,a\neq a_{\star}}^{K}\frac{ \mathbb{E}[T_{a}(n)]}{n}\] \[\leq|\delta_{\star,n}|+\frac{R}{n}\sum_{a=1,a\neq a_{\star}}^{K} \left\{\frac{\log n}{\mathcal{K}_{\text{inf}}(F_{a},\mu^{\star})-\epsilon_{0}} +o(\log n)+O(1)\right\}\]

with \(\delta_{\star,n}=\mu_{\star}-\mu_{\star,n}\), and because \(\underset{n\rightarrow\infty}{\text{lim}}\mu_{\star,n}=\mu_{\star}\), we can concludes that

\[\underset{n\rightarrow\infty}{\text{lim}}\mathbb{E}[\widehat{\mu}_{n}(p)]= \mu_{\star}.\]

Second, we prove that

\[\forall n\geq 1,\forall\varepsilon>0,\exists c>0\text{ that }\mathbb{P}\left(|\widehat{\mu}_{n}(p)-\mu_{\star}|>\varepsilon\right)\leq cn ^{-1}\varepsilon^{-1}.\]

We observe that

\[|\widehat{\mu}_{n}(p)-\mu_{\star}|\leq|\widehat{\mu}_{n}(p)-\mu_{ \star,n}|+|\mu_{\star}-\mu_{\star,n}|=|\widehat{\mu}_{n}(p)-\mu_{\star,n}|+| \delta_{\star,n}|\] \[\Longrightarrow \mathbb{P}(|\widehat{\mu}_{n}(p)-\mu_{\star}|\geq\epsilon)\leq \mathbb{P}(|\widehat{\mu}_{n}(p)-\mu_{\star,n}|\geq\epsilon/2)+\mathbb{P}(| \delta_{\star,n}|\geq\epsilon/2).\]

Because \(\underset{n\to n}{\lim}|\delta_{\star,n}|=0\), therefore, \(\exists N_{0}>0\) such that \(\forall n\geq N_{0}\), we have \(|\delta_{\star,n}|<\epsilon/2\) that means

\[\forall n>N_{0},\mathbb{P}(|\delta_{\star,n}|\geq\epsilon/2)=0.\]Next, according to Lemma 8,

\[|\mathbb{E}[\widehat{\mu}_{n}(p)]-\mu_{\star,n}|\leq\frac{R}{n}\sum_{a=1,a\neq a _{\star}}^{K}\left\{\frac{\log n}{\mathcal{K}_{\text{inf}}(F_{a},\mu^{\star})- \epsilon_{0}}+o(\log n)+O(1)\right\}=O(n^{-1}),\]

that leads to

\[\mathbb{P}(|\widehat{\mu}_{n}(p)-\mu_{\star,n}|\geq\epsilon/2)\leq\frac{| \mathbb{E}[\widehat{\mu}_{n}(p)]-\mu_{\star,n}|}{\epsilon/2}=\frac{O(n^{-1})} {\epsilon/2}.\]

Therefore, \(\exists c>0\) such that

\[\mathbb{P}(|\widehat{\mu}_{n}(p)-\mu_{\star,n}|\geq\epsilon/2)\leq cn^{-1} \epsilon^{-1},\]

which means

\[\forall n\geq N_{0},\forall\varepsilon>0,\exists c>0\text{ that }\mathbb{P} \left(|\widehat{\mu}_{n}(p)-\mu_{\star}|>\varepsilon\right)\leq cn^{-1} \varepsilon^{-1}.\]

Now we see that \(|\widehat{\mu}_{n}(p)-\mu_{\star}|\leq R\). With \(\epsilon\geq R\), we have \(|\widehat{\mu}_{n}(p)-\mu_{\star}|>\epsilon\Leftrightarrow|\widehat{\mu}_{n} (p)-\mu_{\star}|>R\), therefore the inequality holds as

\[\mathbb{P}\left(|\widehat{\mu}_{n}(p)-\mu_{\star}|>\varepsilon\right)=0\leq cn ^{-1}\varepsilon^{-1}.\]

with \(0<\epsilon<R,1\leq n<N_{0}\Rightarrow n\epsilon<RN_{0}\Rightarrow n^{-1} \varepsilon^{-1}>1/RN_{0}\). Therefore

\[\forall C>1/RN_{0}\Rightarrow\mathbb{P}\left(|\widehat{\mu}_{n}(p)-\mu_{ \star}|>\varepsilon\right)\leq 1<Cn^{-1}\varepsilon^{-1},\]

which means

\[\forall n\geq 1,\forall\varepsilon>0,\exists C>0\text{ that }\mathbb{P} \left(|\widehat{\mu}_{n}(p)-\mu_{\star}|>\varepsilon\right)\leq Cn^{-1} \varepsilon^{-1}.\]

That concludes the proof. 

## Appendix E Convergence of CATS and PATS in Monte-Carlo Tree Search

Based upon the results of CATS and PATS using power mean as the value backup operator on the described non-stationary multi-armed bandit problem, we derive theoretical results for CATS in an MCTS tree.

We derive Theorem 3 for CATS and Theorem 4 for PATS, which show concentration and convergence for any internal node in the tree. These proofs utilize induction, leveraging the results of Lemma 7 for CATS and Lemma 8 for PATS, and Lemma 5 for CATS and Lemma 6 for PATS. Additionally, we use Lemma 1, which demonstrates the concentration and convergence of an estimated Q-value based on the child V-value node, applying it recursively throughout the tree.

Our main results, Theorem 5 for CATS and Theorem 5 for PATS, show that the simple regret converges non-asymptotically at a rate of \(O(n^{-1})\).

**Theorem 3**.: _When we apply the CATS algorithm, we have_

1. _For any node_ \(s_{h}\) _at the depth_ \(h^{\text{th}}\) _in the tree,_ \[\underset{n\rightarrow\infty}{\text{plim}}\widehat{Q}_{n}(s_{h},a_{k})= \widetilde{Q}(s_{h},a_{k}).\] 2. _For any node_ \(s_{h}\) _at the depth_ \(h^{\text{th}}\) _in the tree,_ \[\underset{n\rightarrow\infty}{\text{plim}}\widehat{V}_{n}(s_{h})=\widetilde{ V}(s_{h}).\]

Proof.: We will prove this by induction on the depth \(D\) of the tree. If the tree only has depth \((1)\).

The state at the root node is \(s_{0}\), let us assume that at time step \(t\), after taking action \(a_{k}\), the MCTS tree gets an intermediate reward \(r_{t}(s_{0},a_{k})\) and traverses to the next state \(s_{1}\). Let us assume that \(R(s_{0},a_{k})\) is the mean of the intermediate reward at state \(s_{0}\), after taking action \(a_{k}\). We recall the definition of \(\widetilde{Q}(s_{0},a_{k})\), with \(\pi_{0}\) is the rollout policy to estimate the newly added node at the leaf,

\[\widetilde{Q}(s_{0},a_{k})=R(s_{0},a_{k})+\gamma\sum_{s_{1}\in\mathcal{A}_{s_{ 0}}}\mathbb{P}(s_{1}|s_{0},a_{k})\widetilde{V}(s_{1})\]where \(\widetilde{V}(s_{1})\) is the value of the policy \(\pi_{0}\) at state \(s_{1}\), \(\mathcal{A}_{s_{0}}\) is the set of feasible actions at state \(s_{0}\), \(|\mathcal{A}_{s_{0}}|=M\), \(\mathbb{P}(s_{1}|s_{0},a_{k})\) is the probability transition of taking action \(a_{k}\) at state \(s_{0}\) to state \(s_{1}\). From ((1)), we have

\[\widehat{Q}_{n}(s_{0},a_{k})=\frac{1}{n}\sum_{t=1}^{n}r_{t}(s_{0},a_{k})+ \gamma\sum_{s_{1}\sim\tau(s_{0},a_{k})}\frac{T_{s_{0},a_{k}}^{s_{1}}(n)}{n} \widehat{V}_{T_{s_{0},a_{k}}^{s_{1}}(n)}(s_{1})\]

\((i)\) is a direct result of Lemma 1 with \(X_{t}\) is the intermediate reward \(r_{t}(s_{0},a_{k})\) at time \(t\), \(p=(p_{1},p_{2},...p_{M})\sim\mathbb{P}(\cdot|s_{0},a_{k})\), where \(\mathbb{P}(\cdot|s_{0},a_{k})\) is the probability transition dynamic of taking action \(a_{k}\) at state \(s_{0}\). For \(m\in[M]\), each \((\widehat{V}_{m,t})_{t\geq 1}\) at time step t is the deterministic initial Value function \(\widetilde{V}(s_{1})\). We have

\[\underset{n\rightarrow\infty}{\text{plim}}\widehat{V}_{m,n}(s_{1})= \widetilde{V}(s_{1}),\text{ with }s_{1}\in\{s_{m}\},m=1,2,3...M,\text{ where }s_{m}\sim\tau(\cdot|s_{0},a_{k})\]

\((ii)\) Direct results from Theorem 1. In detail, we have from \((i)\),

\[\underset{n\rightarrow\infty}{\text{plim}}\widehat{Q}_{n}(s_{0},a_{k})= \widetilde{Q}(s_{0},a_{k}),\text{ with }a_{k}\in\mathcal{A}_{s_{0}}\]

Because by definition:

\[\widetilde{V}(s_{0})=\max_{a_{k}\in\mathcal{A}_{s_{0}}}\widetilde{Q}(s_{0},a_{ k})\]

\[\widehat{V}_{n}(s_{0})=\left(\sum_{a\in\mathcal{A}_{s_{0}}}\frac{T_{s_{0},a}( n)}{n}\left(\widehat{Q}_{T_{s_{0},a}(n)}(s_{0},a)\right)^{p}\right)^{\frac{1}{p}} \text{ for some }p\in[1,+\infty)\]

Then we have

\[\underset{n\rightarrow\infty}{\text{plim}}\widehat{V}_{n}(s_{0})=\widetilde{ V}(s_{0})\]

that concludes for \((ii)\)

Let us assume that with the tree of depth \(D\), the theorem holds for all its children.

Now let's consider the tree with depth \((D+1)\). When we take one action at the root node at the state \(s_{0}\), it comes to a subtree with depth \((D)\). According to the induction assumption, the results hold for any internal node in the tree after we take the first action. We have \(s_{1}\sim\tau(s_{0},a_{k})\). By the definition, \(\widetilde{V}(s_{H})=V_{0}(s_{H})\) and, for all \(h\leq H-1\),

\[\widetilde{Q}(s_{h},a) = R(s_{h},a)+\gamma\sum_{s_{h+1}\in\mathcal{A}_{s}}\mathbb{P}(s_{h +1}|s_{h},a)\widetilde{V}(s_{h+1})\] \[\widetilde{V}(s_{h}) = \max_{a}\widetilde{Q}(s_{h},a)\]

By the assumption of the induction the root node of a subtree with depth \((D)\) at state \(s_{1}\) we have

\[\underset{n\rightarrow\infty}{\text{plim}}\widehat{V}_{n}(s_{1})=\widetilde{ V}(s_{1})\]

\((i)\) Let's apply Lemma 1 with \(\{X_{t}\}\) is the intermediate reward \(\{r_{t}(s_{0},a_{k})\}\), \(p=(p_{1},p_{2},...p_{M})\sim\mathbb{P}(\cdot|s_{0},a_{k})\). For \(m\in[M]\), each \((\widehat{V}_{m,t})_{t\geq 1}\) at time step t is the empirical Value function \(\widehat{V}_{t}(s_{1})\). We will have

\[\underset{n\rightarrow\infty}{\text{plim}}\widehat{Q}_{n}(s_{0},a_{k})= \widetilde{Q}(s_{0},a_{k}),\text{ with }a_{k}\in\mathcal{A}_{s_{0}}\]

\((ii)\) follows the results of Theorem 1 as at the root node \(s_{0}\) of depth \(D+1\), with

\[\widetilde{V}(s_{0})=\max_{a_{k}\in\mathcal{A}_{s_{0}}}\widetilde{Q}(s_{0},a_{ k})\]

\[\widehat{V}_{n}(s_{0})=\left(\sum_{a\in\mathcal{A}_{s}}\frac{T_{s_{0},a}(n)}{n} \left(\widehat{Q}_{T_{s_{0},a}(n)}(s_{0},a)\right)^{p}\right)^{\frac{1}{p}} \text{ for some }p\in[1,+\infty)\]

[MISSING_PAGE_FAIL:26]

Limitations

**Computational Demands**: The CATS distributional Monte Carlo Tree Search (MCTS) faces challenges in managing computational demands while maintaining and updating probability distributions, leading to a slightly increased complexity.

**Fixed precision**: The PATS set of particles can increase in size if the observed value are different. We prevent this in the implementation by fixing the float precision.

**Number of atoms**: Our approach's performance is slightly influenced by hyperparameters, with the number of atoms being a critical factor. Suboptimal choices may affect performance.

## Appendix G Experimental setup

All the experiments were done on 8 Intel Xeon Gold 6130 (Skylake), x86_64, 2.10GHz, 2 CPUs/node, 16 cores/CPU. Whenever feasible, we opted for open-source implementations of algorithms and environments.

**Parameters selection** We search the number of atoms from {10,20,...,100} and choose the results with best performances. We set the discount factor \(\gamma=.99\) for MDPs, and \(\gamma=.95\) for POMDPs. For UCT, we use the exploration constant \(C=\sqrt{2}\times(R_{\max}-R_{\min})\).

**Atari hyperparameters** We run CATS in Atari with 10 random seeds, where each seed with 512 samples and collect the average score. We found that only 512 simulations were necessary due to the utilization of a pretrained neural network. We run CATS with 100 atoms. The temperature parameter \(\tau\) of MENTS and TENTS is tuned from {0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0}. The selected parameter \(\tau\) are shown in Table 4. The exploration constant \(\epsilon\) for MENTS and TENTS are set to \(0.01\). For Power-UCT, we select the power mean \(p=2\).

**Atari**

\begin{table}
\begin{tabular}{l|l|l|l|l|l|l|l|l|l}  & \multicolumn{1}{c|}{CATS} & \multicolumn{1}{c|}{FATS} & \multicolumn{1}{c|}{UCT} & \multicolumn{1}{c|}{DQN} & \multicolumn{1}{c|}{Favor-CCT} & \multicolumn{1}{c|}{TENTS} & \multicolumn{1}{c|}{MENTS} \\ \cline{2-10} \multicolumn{1}{c|}{\multirow{-2}{*}{
\begin{tabular}{} \end{tabular} }} & \multicolumn{1}{c|}{200.02} & \multicolumn{1}{c|}{200.01} & \multicolumn{1}{c|}{300.197} & \multicolumn{1}{c|}{240.04} & \multicolumn{1}{c|}{350.00} & \multicolumn{1}{c|}{500.00} & \multicolumn{1}{c|}{402.042} & \multicolumn{1}{c|}{350.00} & \multicolumn{1}{c|}{350.

### NeurIPS Paper Checklist

The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: **The papers not including the checklist will be desk rejected.** The checklist should follow the references and precede the (optional) supplemental material. The checklist does NOT count towards the page limit.

Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist:

* You should answer [Yes], [No], or [NA].
* [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.
* Please provide a short (1-2 sentence) justification right after your answer (even for NA).

### The checklist answers are an integral part of your paper submission.

They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper.

The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While "[Yes] " is generally preferable to "[No] ", it is perfectly acceptable to answer "[No] " provided a proper justification is given (e.g., "error bars are not reported because it would be too computationally expensive" or "we were unable to find the license for the dataset we used"). In general, answering "[No] " or "[NA] " is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found.

IMPORTANT, please:

* **Delete this instruction block, but keep the section heading "NeurIPS paper checklist"**,
* **Keep the checklist subsection headings, questions/answers and guidelines below.**
* **Do not modify the questions and only use the provided macros for your answers**.

* **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes], Justification: We discuss the problem of planning in stochastic environments and we present a method to tackle problem with clear contributions.

\begin{table}
\begin{tabular}{|l|c|c|} \cline{2-3} \multicolumn{1}{c|}{} & MENTS & TENTS \\ \hline Phoenix & 0.07 & 0.6 \\ \hline MSpaceman & 0.09 & 0.03 \\ \hline Alien & 0.1 & 0.03 \\ \hline SpaceInvaders & 0.02 & 0.06 \\ \hline BeamRider & 0.02 & 0.03 \\ \hline Asterix & 0.02 & 0.1 \\ \hline Robotank & 0.01 & 0.05 \\ \hline Seaquest & 0.02 & 0.03 \\ \hline Solaris & 0.03 & 0.06 \\ \hline Asteroids & 0.08 & 0.2 \\ \hline Obert & 0.02 & 0.4 \\ \hline Enduro & 0.02 & 0.1 \\ \hline Atlants & 0.08 & 0.03 \\ \hline Hero & 0.4 & 0.03 \\ \hline Frostbite & 0.01 & 0.02 \\ \hline WizardOfWor & 0.1 & 0.01 \\ \hline Breakout & 0.02 & 0.04 \\ \hline \end{tabular}
\end{table}
Table 4: The hyperparameter \(\tau\) (temperature) for MENTS and TENTS in Atari.

Guidelines:

* The answer NA means that the abstract and introduction do not include the claims made in the paper.
* The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.
* The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.
* It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.

* Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We discuss the limitation in Section 6 Guidelines:
* The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.
* The authors are encouraged to create a separate "Limitations" section in their paper.
* The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.
* The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.
* The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.
* The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.
* If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.
* While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
* **Theory Assumptions and Proofs*
* Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: We provide the main theorems in the main paper and proofs in the appendix. Guidelines:
* The answer NA means that the paper does not include theoretical results.
* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: Code and reproducibility steps are provided in supplementary material. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: Full code is available in supplementary material. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.

* While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
* The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
* **Experimental Setting/Details*
* Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: The experimental setting is detailed in the appendix. Guidelines:
* The answer NA means that the paper does not include experiments.
* The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.
* The full details can be provided either with the code, in appendix, or as supplemental material.
* **Experiment Statistical Significance*
* Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: We provide error bars for the plots. For Atari, we report the standard deviation. Guidelines:
* The answer NA means that the paper does not include experiments.
* The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.
* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
* **Experiments Compute Resources*
* Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We provide the details about the computer resources used (CPU and number of cores). Guidelines:
* The answer NA means that the paper does not include experiments.
* The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.
* The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.
* The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
* **Code Of Ethics*
* Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: The research conducted in the paper conforms the Code of Ethics. Guidelines:
* The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
* If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.
* The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
* **Broader Impacts*
* Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: The research conducted in the paper has no societal impact. Guidelines:
* The answer NA means that there is no societal impact of the work performed.
* If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).

* Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: The research proposed in this paper poses no such risks. Guidelines:
* The answer NA means that the paper poses no such risks.
* Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.
* Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.
* We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
* **Licensees for existing assets*
* Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [NA] Justification: We do not use existing assets. Guidelines:
* The answer NA means that the paper does not use existing assets.
* The authors should cite the original paper that produced the code package or dataset.
* The authors should state which version of the asset is used and, if possible, include a URL.
* The name of the license (e.g., CC-BY 4.0) should be included for each asset.
* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: The provided code is well documented.

Guidelines:

* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve crowdsourcing. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.