# Universality laws for Gaussian mixtures

in generalized linear models

 Yatin Dandi, Ludovic Stephan, Florent Krzakala

Idephics, EPFL, Switzerland

 Bruno Loureiro

DI, Ecole Normale Superieure, Paris, France

&Lenka Zdeborova

SPOC, EPFL, Switzerland

###### Abstract

A recent line of work in high-dimensional statistics working under the Gaussian mixture hypothesis has led to a number of results in the context of empirical risk minimization, Bayesian uncertainty quantification, separation of kernel methods and neural networks, ensembling and fluctuation of random features. We provide rigorous proofs for the applicability of these results to a general class of datasets \((\bm{x}_{i},y_{i})_{i=1,\ldots,n}\) containing independent samples from mixture distribution \(\sum_{c\in C}\rho_{c}P_{c}^{\bm{x}}\). Specifically, we consider the hypothesis class of generalized linear models \(\hat{y}=F(\bm{\Theta}^{\top}\bm{x})\) and investigate the asymptotic joint statistics of a family of generalized linear estimators \((\bm{\Theta}^{(1)},\ldots,\bm{\Theta}^{(M)})\), obtained either from (a) minimizing an empirical risk \(\hat{R}_{m}^{(m)}(\bm{\Theta}^{(m)};\bm{X},\bm{y})\) or (b) sampling from the associated Gibbs measure \(\exp(-\beta n\hat{R}_{n}^{(m)}(\bm{\Theta}^{(m)};\bm{X},\bm{y}))\). Our main contribution is to characterize under which conditions the asymptotic joint statistics of this family depend (on a weak sense) only on the means and covariances of the class conditional feature distribution \(P_{c}^{\bm{x}}\). This allows us to prove the universality of different quantities of interest, including training and generalization errors, as well as the geometrical properties and correlations of the estimators.

A recurrent topic in high-dimensional statistics is the investigation of the typical properties of signal processing and machine learning methods on synthetic, _i.i.d._ Gaussian data, a scenario often known under the umbrella of _Gaussian design_[1, 2, 3, 4, 5]. A less restrictive assumption arises when considering that many machine learning tasks deal with data partitioned into a fixed number of classes. In these cases, the data distribution is naturally described by a _mixture model_, where each sample is generated _conditionally_ on the class. In other words: data is generated by first sampling the class assignment and _then_ generating the input conditioned on the class. Arguably the simplest example of such distributions is that of a _Gaussian mixture_, which shall be our focus in this work.

Gaussian mixtures are a popular model in high-dimensional statistics since, besides being an universal approximator, they often lead to mathematically tractable problems. Indeed, a recent line of work has analyzed the asymptotic performance of a large class of machine learning problems in the proportional high-dimensional limit under the Gaussian mixture data assumption, see e.g. [6, 7, 8, 9, 10, 11, 12]. The goal of the present work is to show that this assumption, and the conclusions derived therein, are far more general than previously anticipated.

A recent line of work [13, 14], initiated by the work of [15] for Kernel matrices, posits that for generalized linear estimation on non-linear feature maps satisfying certain regularity conditions and a "one-dimensional CLT", the data distribution can be replaced by equivalent Gaussian data without affecting the training and generalization errors. This was recently proven by [16] for empiricalrisk minimization under the setup of strongly convex objectives, and extended to a larger class of objectives by [17].

However, there is strong empirical evidence that Gaussian universality holds in a more general sense [18]. First, existing results rely on the assumption of a target function depending on linear projections in the latent or feature space. This excludes the rich class of classification on mixture distributions, where the target function is given by the label. For such distributions, a more appropriate equivalent distribution is given by a mixture of Gaussians. Such a "Gaussian mixture equivalence" has been conjectured and used in existing works, such as [11, 12] and was found to closely approximately classification on real datasets.

Furthermore, equivalence with mixtures of Gaussians has been observed to hold not only for training, generalization errors but other quantities of the estimators such as overlaps, variance, etc. For instance, [19, 20] empirically observed that the equivalence holds even while considering the joint distribution of multiple uncertainty estimators or ensembles of multiple random feature mappings. This suggests the equivalence of the distributions of the minimizers themselves.

Our results fill these gaps and provide rigorous justification for the universality in all the aforementioned works. Namely, we show that the joint statistics of multiple generalized estimators obtained either from ERM or sampling on a mixture model asymptotically agrees (in a weak sense) with the statistics of estimators from the same class trained on a Gaussian mixture model with matching first and second order moments. Our **main contributions** are as follows:

* Through a generalization of recent developments in the Gaussian equivalence principle [14, 17, 16], we prove the universality of empirical risk minimization and sampling for a generic mixture distribution and an equivalent mixture of Gaussians. In particular, we show that a Gaussian mixture observed through a random feature map is also a Gaussian mixture in the high-dimensional limit, a fact used for instance (without rigorous justification) in [11, 12, 21, 22].
* A consequence of our results is that, with conditions on the matrix weights, data generated by conditional Generative Adversarial Networks (cGAN) behave as a Gaussian mixture when observed through the prism of generalized linear models (kernels, feature maps, etc...), as illustrated in Figs 1 and 2. This further generalizes the work of [23] that only considered the universality of Gram matrices for GAN generated data through the prism of random matrix theory.
* We construct a unified framework involving multiple sets of parameters arising from simultaneous minimization of different objectives as well as sampling from Gibbs distributions defined by the empirical risk. Through the design of suitable reductions and a convexity-based argument, we establish conditions for the asymptotic universality of arbitrary functions of the set of minimizers or samples from different Gibbs distributions (Theorem 4). For instance, it includes ensembling [20]), and the uncertainty quantification and Bayesian setting assumed (without proof) in [19, 24].
* Finally, we show that for multi-class classification, the conditions leading to universality hold for a large class of functions of the minimizers, such as overlaps and sparsity measures, leading to the equivalence between their distributions of themselves, and provide a theorem for their weak convergence (Theorem 5).
* As a technical contribution of independent interest, our proof of Theorem 5 demonstrates a principled approach for leveraging existing results on the exact asymptotics for simple data distributions (such as for Gaussian mixture models in [12]) to prove the weak convergence and universality of the joint-empirical measure of the estimators and parameters (means, covariances) of the data-distribution.

**Related works --** Universality is an important topic in applied mathematics, as it motivates the scope of tractable mathematical models. It has been extensively studied in the context of random matrix theory [25, 26], signal processing problems [1, 2, 3, 27, 28, 29, 30, 31] and kernel methods [15, 32, 33]. Closer to us is the recent stream of works that investigated the Gaussian universality of the asymptotic error of generalized linear models trained on non-linear features, starting from single-layer random feature maps [34, 35, 16, 36, 37] and later extended to single-layer NTK [17] and deep random features [38]. These results, together with numerical observations that Gaussian universality holds for more general classes of features, led to the formulation of different _Gaussian equivalence_ conjectures [13, 14, 18]. Our results build on the line of works on the proofs of these conjectures, through the use of the one-dimensional CLT (Central Limit Theorem), stated formally in [14] who proved it for random features of Gaussian data. We generalize this principle to a Gaussian equivalence conditioned on the cluster assignment in a mixture-model with a corresponding conditional 1d-CLT (Assumption 10).

A complementary line of research has investigated cases in which the distribution of the features is multi-modal, suggesting a Gaussian mixture universality class instead [39, 23, 40]. A bridge between these two lines of work has been recently investigated with random labels and teachers in [21, 22]. Our results provide rigorous extensions of Gaussian universality to the setups of mixture models as well as uncertainty quantificatio and ensembling.

## 1 Setting and models

Consider a supervised learning problem where the training data \((\bm{x}_{i},y_{i})\in\mathbb{R}^{p}\times\mathcal{Y}\), \(i\in[n]\coloneqq\{1,\cdots,n\}\) is drawn _i.i.d._ from a mixture distribution:

\[\bm{x}_{i} \sim\sum_{c\in\mathcal{C}}\rho_{c}P_{c}^{\bm{x}}, \mathbb{P}(c_{i}=c)=\rho_{c},\] (1)

with \(c_{i}\) a categorical random variable denoting the cluster assignment for the \(i_{th}\) example \(\mathbf{x}_{i}\). Let \(\bm{\mu}_{c}\), \(\bm{\Sigma}_{c}\) denote the mean and covariance of \(P_{c}^{\bm{x}}\), and \(k=|\mathcal{C}|\). Further, assume that the labels \(y_{i}\) are generated from the following target function:

\[y_{i}(\bm{X})=\eta(\bm{\Theta}_{\star}^{\top}\bm{x}_{i},\varepsilon_{i},c_{i}),\] (2)

where \(\eta:\mathbb{R}^{3}\to\mathbb{R}\) is a general label generating function, \(\bm{\Theta}_{\star}\in\mathbb{R}^{k\times p}\) and \(\varepsilon_{i}\) is an i.i.d source of randomness. It is important to stress that the class labels (2) are themselves not constrained to arise from a simple function of the inputs \(\bm{x}_{i}\). For instance, the functional form in (2) includes the case where the labels are exclusively given by a function of the mixture index \(y_{i}=\eta(c_{i})\). This will allow us to handle complex targets, such as data generated using conditional Generative Adversarial Networks (cGANs).

In this manuscript, we will be interested in hypothesis classes defined by parametric predictors of the form \(y_{\bm{\Theta}}(\bm{x})=F(\bm{\Theta}^{\top}\bm{x})\), where \(\bm{\Theta}\in\mathbb{R}^{k\times p}\) are the parameters and \(F:\mathbb{R}^{k}\to\mathcal{Y}\) a possibly non-linear function. For a given loss function \(\ell:\mathbb{R}^{k}\times\mathcal{Y}\to\mathbb{R}_{+}\) and regularization term \(r:\mathbb{R}^{k\times p}\to\mathbb{R}_{+}\), define the (regularized) empirical risk over the training data:

\[\widehat{\mathcal{R}}_{n}(\bm{\Theta};\bm{X},\bm{y}):=\frac{1}{n}\sum_{i=1}^ {n}\ell(\bm{\Theta}^{\top}\bm{x}_{i},y_{i})+r(\bm{\Theta}),\] (3)

where we have defined the feature matrix \(\bm{X}\in\mathbb{R}^{p\times n}\) by stacking the features \(\bm{x}_{i}\) column-wise and the labels \(y_{i}\) in a vector \(\bm{y}\in\mathcal{Y}^{n}\). In what follows, we will be interested in the following two tasks:

1. Minimization: in a minimization task, the statistician's goal is to find a good predictor by minimizing the empirical risk (3), possibly over a constraint set \(\mathcal{S}_{p}\): \[\hat{\bm{\Theta}}_{\text{erm}}(\bm{X},\bm{y})\in\operatorname*{arg\,min}_{ \bm{\Theta}\in\mathcal{S}_{p}}\widehat{\mathcal{R}}_{n}(\bm{\Theta};\bm{X}, \bm{y}),\] (4)

This encompasses diverse settings such as generalized linear models with noise, two-layer networks with a constant number of neurons and fixed second layer, mixture classification, but also the random label setting (with \(\eta(\bm{\Theta}_{\star}^{\top}\bm{x}_{i},\varepsilon_{i},c_{i})=\varepsilon_{i}\)). In the following, we denote \(\widehat{\mathcal{R}}_{n}^{\star}(\bm{X},\bm{y})\coloneqq\min_{\bm{\Theta}} \widehat{\mathcal{R}}_{n}(\bm{\Theta};\bm{X},\bm{y})\)
2. Sampling: here, instead of minimizing the empirical risk (3), the statistician's goal is to sample from a Gibbs distribution that weights different hypothesis according to their empirical error: \[\bm{\Theta}_{\text{Bayes}}(\bm{X},\bm{y})\sim P_{\text{Bayes}}(\bm{\Theta}) \propto\exp\left(-\beta n\widehat{\mathcal{R}}_{n}(\bm{\Theta};\bm{X},\bm{y}) \right)\mathrm{d}\mu(\bm{\Theta})\] (5)

where \(\mu\) is reference prior measure and \(\beta>0\) is a parameter known as the _inverse temperature_. Note that minimization can be seen as a particular example of sampling when \(\beta\to\infty\), since in this limit the above measure peaks on the global minima of (4).

Applications of interest--So far, the setting defined above is quite generic, and the motivation to study this problem might not appear evident to the reader. Therefore, we briefly discuss a few scenarios of interest which are covered by this model.

1. _Conditional GANs (cGANs):_ These were introduced by [41] as a generative model to learn mixture distributions. Once trained in samples from the target distribution, they define a function \(\Psi\) that maps Gaussian mixtures (defining the latent space) to samples from the target mixture that preserve the label structure. In other words, conditioned on the label: \[\forall c\in\mathcal{C},\qquad\bm{z}\sim\mathcal{N}(\bm{\mu}_{c},\bm{\Sigma}_{ c})\mapsto\bm{x}_{c}=\Psi(\bm{z},c)\sim P_{c}^{\bm{x}}\] (6)

The connection to model (1) is immediate. This scenario was extensively studied by [39, 23, 40], and is illustrated in Fig. 1. In Fig. 2 we report on a concrete experiment with a cGAN trained on the fashion-MNIST dataset.
2. _Multiple objectives:_ Our framework also allows to characterize the joint statistics of estimators \((\bm{\Theta}_{1},\dots,\bm{\Theta}_{M})\) obtained from empirical risk minimization and/or sampling from different objective functions \(\hat{R}_{n}^{m}\) defined on the same training data \((\bm{X},\bm{y})\). This can be of interest in different scenarios. For instance, [19, 24] has characterized the correlation in the calibration of different uncertainty measures of interest, e.g. last-layer scores and Bayesian training of last-layer weights. This crucially depends on the correlation matrix \(\hat{\bm{\Theta}}_{\text{erm}}\bm{\Theta}_{\text{Bayes}}^{\top}\in\mathbb{R}^{ k\times k}\) which fits our framework.
3. _Ensemble of features:_ Another example covered by the multi-objective framework above is that of ensembling. Let \((\bm{z}_{i},y_{i})\in\mathbb{R}^{d}\times\mathcal{Y}\) denote some training data from a mixture model akin to (1). A popular ensembling scheme often employed in the context of deep learning [42] is to take a family of \(M\) feature maps \(\bm{z}_{i}\mapsto\bm{x}_{i}^{(m)}\!=\!\varphi_{m}(\bm{z}_{i})\) (e.g. neural network features trained from different random initialization) and train \(M\) independent learners: \[\hat{\bm{\Theta}}_{\text{erm}}^{(m)}\in\operatorname*{arg\,min}_{\bm{\Theta} \in\mathcal{S}_{p}}\frac{1}{n}\sum_{i=1}^{n}\ell(\bm{\Theta}^{\top}\bm{x}_{i}^ {(m)},y_{i})+r(\bm{\Theta})\] (7)

Prediction on a new sample \(\bm{z}\) is then made by ensembling the independent learners, e.g. by taking their average \(\hat{\bm{y}}=\nicefrac{{1}}{{M}}\sum_{m=1}^{M}\hat{\bm{\Theta}}_{\text{erm}} ^{(m)\top}\varphi_{m}(\bm{z})\). A closely related model was studied in [43, 44, 20].

Note that in all the applications above, having the labels depending on the features \(\bm{X}\) would not be natural, since they are either generated from a latent space, as in \((i)\), or chosen by the statistician, as in \((ii)\), \((iii)\). Indeed, in these cases the most natural label model is given by the mixture index \(y=c\) itself, which is a particular case of (2). This highlights the flexibility of our target model with respect to prior work [17]. Instead, [16] assumes that the target is a function of a _latent variable_, which would correspond to a mismatched setting. The discussion here can be generalized also to this case, but require an additional assumption discussed in Appendix B.

Universality --Given these tasks, the goal of the statistician is to characterize different statistical properties of these predictors. These can be, for instance, point performance metrics such as the empirical and population risks, or uncertainty metrics such as the calibration of the predictor or moments of the posterior distribution (5). These examples, as well as many different other quantities of interest, are functions of the joint statistics of the pre-activations \((\bm{\Theta}_{\bm{x}}^{\top}\bm{x},\bm{\Theta}^{\top}\bm{x})\), for \(\bm{x}\) either a test or

Figure 1: Illustration of Corollary 2: high-dimensional data generated by generative neural networks starting from a mixture of Gaussian in latent space (\(\bm{z}\in\mathbb{R}^{H}\)) are (with conditions on the weights matrices) equivalent, in high-dimension and for generalized linear models, to data sampled from a Gaussian mixture. A concrete example is shown in Fig. 2.

training sample from (1). For instance, in a Gaussian mixture model, where \(\bm{x}\sim\sum_{c\in\mathcal{C}}\rho_{c}\,\mathcal{N}(\bm{\mu}_{c},\bm{\Sigma}_{c})\), the sufficient statistics are simply given by the first two moments of these pre-activations. However, for a general mixture model (1), the sufficient statistics will generically depend on all moments of these pre-activations. Surprisingly, our key result in this work is to show that in the high-dimensional limit this is not the case. In other words, under some conditions which are made precise in Section 2, we show that expectations with respect to (1) can be exchanged by expectations over a Gaussian mixture with matching moments. This can be formalized as follows. Define an _equivalent Gaussian data set_\((\bm{g}_{i},y_{i})_{i=1}^{n}\in\mathbb{R}^{p}\times\mathcal{Y}\) with samples drawn _i.i.d._ from the _equivalent Gaussian mixture model_:

\[\bm{g}_{i}\sim\sum_{c\in\mathcal{C}}\rho_{c}\,\mathcal{N}(\bm{\mu}_{c},\bm{ \Sigma}_{c}), y_{i}(\bm{G})=\eta(\bm{\Theta}_{+}^{\top}\bm{g}_{i},\varepsilon_{i},c _{i}).\] (8)

We recall that \(\bm{\mu}_{c}\), \(\bm{\Sigma}_{c}\) denotes the mean and covariance of \(P_{c}^{\bm{x}}\) from (1). Consider a family of estimators \((\bm{\Theta}_{1},\cdots,\bm{\Theta}_{M})\) defined by minimization (3) and/or sampling (5) over the training data \((\bm{X},\bm{y})\) from the mixture model (1). Let \(h\) be a statistical metric of interest. Then, in the proportional high-dimensional limit where \(n,p\!\to\!\infty\) at a fixed \(\alpha\!=\!\nicefrac{{n}}{{d}}\!>0\), and where \(\langle\cdot\rangle\) denote the expectation with respect to the Gibbs distribution (5), we define universality as:

\[\mathbb{E}_{\bm{X}}\left[\langle h(\bm{\Theta}_{1},\cdots,\bm{\Theta}_{M}) \rangle_{\bm{X}}\right]\underset{n\to\infty}{\simeq}\mathbb{E}_{\bm{G}}\left[ \langle h(\bm{\Theta}_{1},\cdots,\bm{\Theta}_{M})\rangle_{\bm{G}}\right]\] (9)

The goal of the next section is to make this statement precise.

## 2 Main results

We now present the main theoretical contributions of the present work and discuss its consequences. Our proofs for Theorems 4 and 6 build upon existing results on the universality of empirical risk minimization for uni-model distributions [16; 17] and therefore rely on similar technical regularity and concentration assumptions. Concretely, our work relies on the following assumptions:

**Assumption 1** (Loss and regularization).: _The loss function \(\ell:\mathbb{R}^{k+1}\to\mathbb{R}\) is nonnegative and Lipschitz, and the regularization function \(r:\mathbb{R}^{p\times k}\to\mathbb{R}\) is locally Lipschitz, with constants independent from \(p\)._

Figure 2: Illustration of the universality scenario described in Fig.1. Logistic (left) & ridge (right) regression test (up) and training (bottom) errors are shown versus the sample complexity \(\alpha=\nicefrac{{n}}{{d}}\) for an odd vs. even binary classification task on two data models: Blue dots data generated from a conditional GAN [41] trained on the fashion-MNIST dataset [45] and pre-processed with a random features map \(\bm{x}\mapsto\tanh(\mathrm{W}\bm{x})\) with Gaussian weights \(W\!\in\!\mathbb{R}^{1176\times 784}\); Red dots are the \(10\)- clusters Gaussian mixture model with means and covariances matching each fashion-MNIST cluster conditioned on labels (\(\ell_{2}\) regularization is \(\lambda=10^{-4}\)). Details on the simulations are discussed in Appendix D.

**Assumption 2** (Boundedness and concentration).: _The constraint set \(\mathcal{S}_{p}\) is a compact subset of \(\mathbb{R}^{k\times p}\). Further, there exists a constant \(M>0\) such that for any \(c\geq 0\),_

\[\sup_{\bm{\theta}\in\mathcal{K}_{p},\|\bm{\theta}\|_{2}\leq 1}\|\bm{\theta}^{ \top}\bm{x}\|_{\psi_{2}}\leq M,\quad\sup_{\bm{\theta}\in\mathcal{K}_{p},\|\bm{ \theta}\|_{2}\leq 1}\|\bm{\Sigma}_{c}^{1/2}\bm{\theta}\|_{2}\leq M,\quad\text{ and }\quad\|\bm{\mu}_{c}\|_{2}\leq M\] (10)

_where \(\|\cdot\|_{\psi_{2}}\) is the sub-gaussian norm, and \(\mathcal{K}_{p}\subseteq\mathbb{R}^{p}\) is such that \(\mathcal{S}_{p}\subseteq\mathcal{K}_{p}^{k}\)._

**Assumption 3** (Labels).: _The labeling function \(\eta\) is Lipschitz, the teacher vector \(\bm{\Theta}\) belongs to \(\mathcal{S}_{p}\), and the noise variables \(\varepsilon_{i}\) are i.i.d sub-gaussian with \(\|\varepsilon_{i}\|_{\psi_{2}}\leq M\) for some constant \(M>0\)._

Those three assumptions are fairly technical, and it is possible that the universality properties proven in this article hold irrespective of these conditions. The crucial assumption in our theorems is that of a _conditional one-dimensional CLT_:

**Assumption 4**.: _For any Lipschitz function \(\varphi:\mathbb{R}\to\mathbb{R}\),_

\[\lim_{n,p\to\infty}\sup_{\bm{\theta}\in\mathcal{K}_{p}}\left|\mathbb{E}\left[ \varphi(\bm{\theta}^{\top}\bm{x})\,\big{|}\,c_{\bm{x}}=c\right]-\mathbb{E} \left[\varphi(\bm{\theta}^{\top}\bm{g})\,\big{|}\,c_{\bm{g}}=c\right]\right|=0, \quad\forall c\in\mathcal{C}\] (11)

where \(\bm{x}\) and \(\bm{g}\) denote samples from the given mixture distribution and the equivalent gaussian mixture distribution in equations (1) and (8) respectively.

The above assumption is a generalization of the "one-dimensional CLT" underlying the line of work based on the Gaussian equivalence (GE) Principle [13; 14; 17; 16]. The above assumption splits the proof of universality for a general mixture distribution into two parts. First, one shows that asymptotic universality of an observable \(h\) can be reduced to the proof of a one-dimensional CLT. Second, one proves this CLT holds for the particular class of features of interest. This proof scheme streamlines universality proofs. Our work provides a general proof of the first step in Theorem 4, conditioned on the second, later showing that Assumption 4 holds for some natural feature maps of interest, i.e. random features applied to a Gaussian mixture (Theorem 6). However, Assumption 4 is expected to hold for a large class of features, as supported by our empirical observations in Figure 2 and arguments in Appendix C.

### Universality of Mixture Models

We start by proving the universality of the free energy for a Gibbs distribution defined through the objective \(\widehat{\mathcal{R}}_{n}(\bm{\Theta};\bm{X},\bm{y})\) for the data distribution (1) and its equivalent Gaussian mixture (8).

**Theorem 1** (Universality of Free Energy).: _Let \(\mu_{p}(\bm{\Theta})\) be a sequence of Borel probability measures with compact supports \(\mathcal{S}_{p}\). Define the following free energy function:_

\[f_{\beta,n}(\bm{X})=-\frac{1}{\beta n}\log\int\exp\left(-\beta n\widehat{ \mathcal{R}}_{n}(\bm{\Theta};\bm{X},\bm{y}(\bm{X}))\right)d\mu_{p}(\bm{\Theta})\] (12)

_Under Assumptions 1-4 on \(\bm{X}\) and \(\mathcal{S}_{p}\), for any bounded differentiable function \(\Phi\) with bounded Lipschitz derivative, we have:_

\[\lim_{n,p\to\infty}\left|\mathbb{E}\left[\Phi\left(f_{\beta,n}(\bm{X})\right) \right]-\mathbb{E}\left[\Phi\left(f_{\beta,n}(\bm{G})\right)\right]\right|=0.\]

When \(\mu_{p}\) corresponds to discrete measures supported on an \(\epsilon\)-net in \(\mathcal{S}_{p}\), using the reduction from Lemma 1 to Theorem 1 in [17], we obtain the following corollary:

**Corollary 2** (Universality of Training Error).: _For any bounded Lipschitz function \(\Phi:\mathbb{R}\to\mathbb{R}\):_

\[\lim_{n,p\to\infty}\left|\mathbb{E}\left[\Phi\left(\widehat{\mathcal{R}}_{n}^{ \star}(\bm{X},\bm{y}(\bm{X}))\right)\right]-\mathbb{E}\left[\Phi\left(\widehat {\mathcal{R}}_{n}^{\star}(\bm{G},\bm{y}(\bm{G}))\right)\right]\right|=0\]

_In particular, for any \(\mathcal{E}\in\mathbb{R}\), and denoting \(\stackrel{{\mathbb{P}}}{{\longrightarrow}}\) the convergence in probability:_

\[\widehat{\mathcal{R}}_{n}^{\star}(\bm{X},\bm{y}(\bm{X}))\stackrel{{ \mathbb{P}}}{{\longrightarrow}}\mathcal{E}\quad\text{if and only if}\quad\widehat{ \mathcal{R}}_{n}^{\star}(\bm{G},\bm{y}(\bm{G}))\stackrel{{ \mathbb{P}}}{{\longrightarrow}}\mathcal{E},\] (13)

The full theorem, as well as its proof, is presented in Appendix A, along with additional remarks and an intuitive sketch. The proof combines the conditional 1d-CLT in Assumption 4 with theinterpolation of the free-energy in [17]. For strongly-convex losses, one may alternatively use the Lindeberg's method as in [16].

In a nutshell, this theorem shows that the multi-modal data generated by any generative neural network is equivalent to a _finite_ mixture of Gaussian in high-dimensions: in other words, a _finite_ mixture of Gaussians leads to the same loss as for data generated by (for instance) a cGAN. Since the function \(\ell:\mathbb{R}^{k+1}\to\mathbb{R}\) need not be convex, we can take

\[\ell(\bm{x}_{\text{out}},y)=\ell^{\prime}(\bm{\Psi}(\bm{x}_{\text{out}}),y),\]

where \(\bm{\Psi}\) is an already pretrained neural network. In particular, if \(\bm{x}\) is the output of all but the last layer of a neural net, we can view \(\bm{\Psi}\) as the averaging procedure for a small committee machine.

Note that Corollary 2 depends crucially on Assumption 4 (the one-dimensional CLT), which is by no means evident. We discuss the conditions on the weights matrix for which it can be proven in Section 2.4. However, one can observe empirically that the validity of Corollary 2 goes well beyond what can be currently proven. A number of numerical illustrations of this property can be found in the work of [23; 39; 40], who already derived similar (albeit more limited) results using random matrix theory. Additionally, we observed that even with trained GANs, when we observed data through a random feature map [46], the Gaussian mixture universality is well obeyed. This scenario is illustrated in Fig. 1, with a concrete example in Fig. 2. Even though we did not prove the one-dimensional CLT for arbitrary learned matrices, and worked with finite moderate sizes, the realistic data generated by our cGAN behaves extremely closely to those generated by the corresponding Gaussian mixture.

A second remark is that the interest of Corollary 2 lies in the fact that it requires only a _finite_ mixture to approximate the loss. Indeed, while we could use the standard approximation results (e.g. the Stone-Weierstrass theorem) to approximate the data density to arbitrary precision by Gaussian mixtures, this would require a diverging number of Gaussian in the mixture. The fact that loss is captured with finite \(\mathcal{C}\) is key to our approach.

### Convergence of expectations for Joint Minimization and Sampling

Our next result establishes a general relationship between the differentiability of the limit of expected training errors or free energies for empirical risk minimization or free energies for sampling and the universality of expectations of a given function of a set of parameters arising from multiple objectives. As a motivating example, consider the uncertainty quantification in Section 1 that uses both Bayesian and ERM estimators [19; 24]. The parameters \(\hat{\bm{\Theta}}_{\text{erm}}\) and \(\bm{\Theta}_{\text{Bayes}}\) are obtained through empirical risk minimization and posterior sampling respectively on the same sequence of training data. In general, the inputs used in different objectives could be different but have some correlation structure. In the setup of ensembling (Equation 7), they are correlated through the feature mapping \(\bm{z}_{i}\mapsto\bm{x}_{i}^{(m)}=\varphi_{m}(\bm{z}_{i})\). In light of these considerations, we present the following general setup: Consider a sequence of \(M\) risks:

\[\widehat{\mathcal{R}}_{n}^{(m)}(\bm{\Theta};\bm{X}^{(m)},\bm{y}^{(m)}):=\frac {1}{n}\sum_{i=1}^{n}\ell_{m}(\bm{\Theta}^{\top}\bm{x}_{i}^{(m)},y_{i}^{(m)})+ r_{m}(\bm{\Theta}),\quad m\in[M]\] (14)

with possibly different losses, regularizers and datasets. For simplicity, we assume that the objectives are defined on parameters having the same dimension \(\bm{\Theta}\in\mathbb{R}^{p\times k}\). We aim to minimize \(M_{1}\) of them:

\[\hat{\bm{\Theta}}^{(m)}(\bm{X})\in\operatorname*{arg\,min}_{\bm{\Theta}\in \mathcal{S}_{p}^{(m)}}\ \widehat{\mathcal{R}}_{n}^{(m)}(\bm{\Theta};\bm{X}^{(m)},\bm{y}^{(m)}),\quad m \in[M_{1}]\] (15)

and the \(M_{2}\) remaining parameters are independently sampled from a family of Gibbs distributions:

\[\bm{\Theta}^{(m)}\sim P_{m}(\bm{\Theta})\propto\exp\Big{(}-\beta_{m}\widehat {\mathcal{R}}_{n}^{(m)}\Big{(}\bm{\Theta};\bm{X}^{(m)},\bm{y}^{(m)}\Big{)} \Big{)}d\mu_{m}(\bm{\Theta}),\quad m\in[M_{1}+1,M],\] (16)

where \(M=M_{1}+M_{2}\). The joint distribution of the \(\bm{x}_{i}=(\bm{x}_{i}^{(1)},\dots,\bm{x}_{i}^{(M)})\) is assumed to be a mixture of the form (1). However, we assume that the labels \(\bm{y}_{i}^{(m)}\) only depend on the vectors \(\bm{x}_{i}^{(m)}\):

\[y_{i}^{(m)}(\bm{X}^{(m)})=\eta(\bm{\Theta}_{\star}^{(m)\top}\bm{x}_{i}^{(m)}, \varepsilon_{i}^{(m)},c_{i}).\] (17)

The equivalent Gaussian inputs \(\bm{g}_{i}=(\bm{g}_{i}^{(1)},\dots,\bm{g}_{i}^{(M)})\) and their labels \(\bm{y}(\bm{G})\) are defined as in (8).

Statistical metric and free energy --Our goal is to study statistical metrics for some function \(h:\mathbb{R}^{M\times k\times p}\rightarrow\mathbb{R}\) of the form \(h(\bm{\Theta}^{(1)},\cdots,\bm{\Theta}^{(M)})\). For instance, the metric \(h\) could be the population risk (a.k.a. generalization error), or some overlap between \(\bm{\Theta}\) and \(\bm{\Theta}_{\star}\). We define the following coupling free energy function:

\[f_{n,s}(\bm{\Theta}[1:M_{1}],\bm{X},\bm{y})=-\frac{1}{n}\mathrm{ log}\int\!e^{-sn\;h(\bm{\Theta}^{(1)},\cdots,\bm{\Theta}^{(M)})}dP^{(M_{1}+1):M},\] (18)

where \(P^{(M_{1}+1):M}\) denotes the product measure of the \(P_{m}\) defined in (16). This gives rise to the following joint objective:

\[\widehat{\mathcal{R}}_{n,s}(\bm{\Theta}[1:M_{1}],\bm{X},\bm{y})= \sum_{m=1}^{M_{1}}\widehat{\mathcal{R}}_{n}^{(m)}(\bm{\Theta}^{(m)};\bm{X}^{( m)},\bm{y}^{(m)})+f_{n,s}(\bm{\Theta}[1:M_{1}],\bm{X},\bm{y}).\] (19)

In particular, when \(s=0\) we have \(f_{n,0}=0\) and the problem reduces to the joint minimization problem in (15). Our first result concerns the universality of the minimum of the above problem:

**Proposition 3** (Universality for joint minimization and sampling).: _Under Assumptions 1-4, for any \(s>0\) and bounded Lipschitz function \(\Phi:\mathbb{R}\rightarrow\mathbb{R}\), and denoting \(\widehat{\mathcal{R}}_{n,s}^{\star}(\bm{X},\bm{y}):=\min\widehat{\mathcal{R}} _{n,s}(\bm{\Theta};\bm{X},\bm{y})\):_

\[\lim_{n,p\rightarrow\infty}\left|\mathbb{E}\left[\Phi\left(\widehat{\mathcal{R }}_{n,s}^{\star}(\bm{X},\bm{y}(\bm{X}))\right)\right]-\mathbb{E}\left[\Phi \left(\widehat{\mathcal{R}}_{n,s}^{\star}(\bm{G},\bm{y}(\bm{G}))\right) \right]\right|=0\]

The proof uses a reduction to Corollary 2, and can be found in App. A.5. The next result concerns the value of \(h\) at the minimizers point \((\hat{\bm{\Theta}}^{(1)},\ldots,\hat{\bm{\Theta}}^{(M)})\). We make the following additional assumptions:

**Assumption 5** (Differentiable Limit).: _There exists a neighborhood of \(0\) such that the function \(q_{n}(s)=\mathbb{E}\left[\widehat{\mathcal{R}}_{n,s}^{\star}(\bm{G},\bm{y}( \bm{G}))\right]\) converges pointwise to a function \(q(s)\) that is differentiable at \(0\)._

The above assumption stems from the convexity based argument used to prove Theorem 4.

For a fixed realization of the dataset \(\bm{X}\), we denote by \(\left\langle h(\bm{\Theta}^{(1)},\cdots,\bm{\Theta}^{(M)})\right\rangle_{\bm{X}}\) the expected value of \(h\) when \((\hat{\bm{\Theta}}^{(1)},\ldots,\hat{\bm{\Theta}}^{(M_{1})})\) are obtained through the minimization of (15) and \((\bm{\Theta}^{(M_{1}+1)},\ldots,\bm{\Theta}^{(M)})\) are sampled according to the Boltzmann distributions (16).

**Assumption 6**.: _With high probability on \(\bm{X},\bm{G}\), the value \(\left\langle h(\bm{\Theta}^{(1)},\cdots,\bm{\Theta}^{(M)})\right\rangle_{\bm{X}}\) (resp. the same for \(\bm{G}\)) is independent from the chosen minimizers in (15)._

The above assumption is motivated by the fact that commonly non-convex problems contain minima exhibiting a specific symmetry. For example, all the global minima for a two-layer neural network are permutation invariant. Assumption 6 reflects that the quantity \(h\) respects these symmetries by taking the same value at each global minimum. This can be replaced by the stronger condition of a unique minimizer. Then the following holds:

**Theorem 4**.: _Under Assumptions 1-6, we have:_

\[\lim_{n,p\rightarrow\infty}\left|\mathbb{E}\left[\left\langle h\left(\bm{ \Theta}^{(1)},\cdots,\bm{\Theta}^{(M)}\right)\right\rangle_{\bm{X}}\right]- \mathbb{E}\left[\left\langle h\left(\bm{\Theta}^{(1)},\cdots,\bm{\Theta}^{(M) }\right)\right\rangle_{\bm{G}}\right]\right|=0,\] (20)

Proof Sketch:Our proof relies on the observation that \(q_{n}(s)\) is a concave function of \(s\). Further:

\[q_{n}^{\prime}(0)=\mathbb{E}\left[\left\langle h\left(\bm{\Theta}^{(1)}, \cdots,\bm{\Theta}^{(M)}\right)\right\rangle_{\bm{G}}\right].\] (21)

This allows us to leverage a result of convex analysis relating the convergence of a sequence of convex or concave functions to the convergence of the corresponding derivatives, bypassing the more involved probabilistic arguments in [16, 17]. Our approach also generalizes in a straightforward manner to the setup of multiple objectives.

The above result shows that the expected value of \(h\left(\bm{\Theta}^{(1)},\cdots,\bm{\Theta}^{(M)}\right)\) for a multi-modal data satisfying the 1d CLT is equivalent to that of a mixture of Gaussians. The full theorem is presented and proven in Appendix A.

### Universal Weak Convergence

Theorem 4 provides a general framework for proving the equivalence of arbitrary functions of parameters obtained by minimization/sampling on a given mixture dataset and the equivalent gaussian mixture distribution. However, it relies on the assumption of a differentiable limit of the free energy (Assumption 5). If the assumption holds for a sequence of functions belonging to dense subsets of particular classes of functions, it allows us to prove convergence of minimizers themselves, in a weak sense. We illustrate this through a simple setup considered in [12], which precisely characterized the asymptotic distribution of the minimizers of empirical risk with GMM data in the strictly convex case. Consider the following setup:

\[\left(\hat{\bm{W}}^{\bm{X}},\hat{\bm{b}}^{\bm{X}}\right)=\ \operatorname*{arg \,min}_{\bm{W},\bm{b}}\ \sum_{i=1}^{n}\ell\left(\frac{\bm{W}\bm{x}_{i}}{\sqrt{d}}+\bm{b},\bm{y}_{i} \right)+\lambda r(\bm{W}),\] (22)

where \(\bm{W}\in\mathbb{R}^{|\mathcal{C}|\times d}\), \(\bm{b}\in\mathbb{R}^{|\mathcal{C}|}\) and \(\bm{y}_{i}\in\mathbb{R}^{|\mathcal{C}|}\) is the one-hot encoding of the class index \(c_{i}\). We make the following assumptions:

**Assumption 7**.: _All of the covariance matrices \(\Sigma_{c}\) are diagonal, with strictly positive eigenvalues \((\sigma_{c,i})_{i\in[d]}\), and there exists a constant \(M>0\) such that for any \(c\in\mathcal{C}\) we have \(\sigma_{c,i}\leq M\quad\text{ and }\quad\|\bm{\mu}_{c}\|_{2}\leq M.\)_

Secondly, since we aim at obtaining a result on the weak convergence of the estimators, we assume the same weak convergence for the means and covariances, and that the regularization only depends on the empirical measure of \(\bm{W}\).

**Assumption 8**.: _The empirical distribution of the \(\bm{\mu}_{c}\) and \(\bm{\Sigma}_{c}\) converges weakly as follows:_

\[\frac{1}{d}\sum_{i=1}^{d}\prod_{c\in\mathcal{C}}\delta(\mu_{c}-\sqrt{d}\mu_{c,i })\delta(\sigma_{c}-\sigma_{c,i})\quad\xrightarrow[d\to\infty]{\mathcal{L}} \quad p(\bm{\sigma},\bm{\mu})\] (23)

**Assumption 9**.: _The regularizer \(r(\cdot)\) is a pseudo-Lipshitz function of finite-order having the following form: \(r(\bm{W})=\sum_{i=1}^{d}\psi_{r}(\bm{W}_{i}),\) for some convex, differentiable function \(\psi_{r}:\mathbb{R}\to\mathbb{R}\). This includes, in particular the squared regularization \(r(\bm{W})=\sum_{i=1}^{d}\bm{W}_{i}^{2}\)._

We briefly comment on the choice of the above assumptions. The boundedness of the \(\Sigma_{c}\) and \(\mu\) in Assumption 7 guarantees that we are in a case covered both by [12] and by the assumptions of Theorem 4. The diagonal property of the \(\Sigma_{c}\) in 7, as well as the joint convergence in Assumption 8, ensure that we can view the minimization problem 22 ensures that \(W^{\star},b^{\star}\) converge towards a well-defined limit. Finally, the separability assumption on \(r\) in assumption 9 responds to the fact that we aim for a result on the empirical coordinate distribution of \(W^{\star},b^{\star}\)

Under these conditions, the joint empirical measure of the minimizers and of the data moments converges weakly to a fixed limit, indepdendent of the data-distribution:

**Theorem 5**.: _Assume that conditions 1-9 hold, and further that the function \(\ell(\bullet,y)+r(\bullet)\) is convex, coercive and differentiable. Then, for any bounded-Lipschitz function: \(\Phi:\mathbb{R}^{3|\mathcal{C}|}\to\mathbb{R}\), we have:_

\[\mathbb{E}\left[\frac{1}{d}\sum_{i=1}^{d}\Phi(\{(\hat{\bm{W}}^{\bm{X}})_{c,i }\}_{c\in\mathcal{C}},\{\mu_{c,i}\}_{c\in\mathcal{C}},\{\sigma_{c,i}\}_{c\in \mathcal{C}})\right]\xrightarrow[n/d=\infty>0]{n,d\to+\infty}\mathbb{E}_{ \tilde{p}}\left[\Phi(\bm{w},\bm{\mu},\bm{\sigma})\right],\] (24)

_where \(\tilde{p}\) is a measure on \(\mathbb{R}^{3|\mathcal{C}|}\), that is determined by the so-called replica equations._

Proof SketchThe proof starts with the observation that the nonlinear system of (replica) equations in [12] describes the joint-empirical measure of the parameters, means and covariances of the mixtures in a self-consistent manner. Furthermore, for \(h(\bm{W})\) having bounded second derivatives, the perturbation term \(sh(\bm{W})\) can be absorbed into the regularizer. We then utilize topological and analytical arguments to relate the weak convergence to the differentiability Assumption 5 for functions that can be expressed as expectations w.r.t the joint empirical measure in 24. More details can be found in Appendix A.8.

In particular, the above result implies the universality of the overlaps of the minimizers with means, covariances, as well as their geometrical properties such as \(L^{p}\) norms.

### One-dimensional CLT for Random Features

We finally show a conditional one-dimensional CLT for a random features map applied to a mixture of gaussians, in the vein of those shown in [14, 16, 17]. Concretely, we consider the following setup:

\[\bm{x}_{i}=\sigma(\bm{F}\bm{z}_{i}),\quad\bm{z}_{i}\sim\sum_{c\in\mathcal{C}} \mathcal{N}(\bm{\mu}_{c}^{\bm{z}},\bm{\Sigma}_{c}^{\bm{z}}),\] (25)

where the feature matrix \(\bm{F}\in\mathbb{R}^{p\times d}\) has i.i.d \(\mathcal{N}(0,\nicefrac{{1}}{{d}})\) entries. This setup is much more permissive than the ones in [16, 17], that restrict the samples \(\bm{z}\) to standard normal vectors. However, we do require some technical assumptions:

**Assumption 10**.: _The activation function \(\sigma\) is thrice differentiable, with \(\|\sigma^{(i)}\|\leq M\) for some \(M>0\), and we have \(\mathbb{E}_{g\sim\mathcal{N}(0,1)}\left[\sigma(g)\right]=0\). Additionally, the cluster means and covariances of \(\bm{z}\) satisfy for all \(c\in\mathcal{C}\)\(\|\bm{\mu}_{c}^{\bm{z}}\|\leq M,\|\bm{\Sigma}_{c}^{\bm{z}}\|_{\mathrm{op}}\leq M\) for some constant \(M>0\)._

We also place ourselves in the proportional regime, i.e. a regime where \(p/d\in[\gamma^{-1},\gamma]\) for some \(\gamma>0\). For simplicity, we will consider the case \(k=1\); and the constraint set \(\mathcal{S}_{p}\) as follows:

\[\mathcal{S}_{p}=\left\{\bm{\theta}\in\mathbb{R}^{d}\;\big{|}\;\|\bm{\theta}\|_ {2}\leq R,\quad\|\bm{\theta}\|_{\infty}\leq Cp^{-\eta}\right\}\] (26)

for a given \(\eta>0\). We show in the appendix the following theorem:

**Theorem 6**.: _Under Assumption 10, and with high probability on the feature matrix \(\bm{F}\), the data \(\bm{X}\) satisfy the concentration assumption 2, as well as the one-dimensional CLT of Assumption 4. Consequently, the results of Theorems 1 and 4 apply to \(\bm{X}\) and their Gaussian equivalent \(\bm{G}\)._

Proof SketchOur proof proceeds by defining the following neuron-wise activation functions:

\[\sigma_{i,c}(u)=\sigma(u+\bm{f}_{i}^{\top}\bm{\mu}_{c}).\] (27)

We subsequently control the effects of the means, covariances and the dimensions of the inputs to prove a result analogous to the one-dimensional CLT for random features in [16, 17, 14]. While we prove the above result for random weights, we note, however that the non-asymptotic results in [16, 14] also hold for deterministic matrices satisfying approximate orthogonality conditions. Therefore, we expect the one-dimensional CLT to approximately hold for a much larger class of feature maps. Finally, we also note that the above extension of the one-dimensional CLT to mixture of gaussians also provides a proof for the asymptotic error for random features in [11].

**Conclusions --** We demonstrate the universality of the Gaussian mixture assumption in high-dimension for various machine learning tasks such as empirical risk minimization, sampling and ensembling, in a variety of settings including random features or GAN generated data. We also show that universality holds for a large class of functions, and provide a weak convergence theorem. These results, we believe, vindicate the classical theoretical line of works on the Gaussian mixture design. We hope that our results will stimulate further research in this area. We also believe it crucial to understand the limitations of our extended universality framework, for instance in the cases of data with low-dimensional structure or sparsity.

## Acknowledgements

We acknowledge funding from the ERC under the European Union's Horizon 2020 Research and Innovation Program Grant Agreement 714608-SMiLe, as well as by the Swiss National Science Foundation grant SNFS OperaGOST, \(200021\_200390\).

## References

* [1] David Donoho and Jared Tanner. Observed universality of phase transitions in high-dimensional geometry, with implications for modern data analysis and signal processing. _Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences_, 367(1906):4273-4293, 2009.
* [2] Satish Babu Korada and Andrea Montanari. Applications of the lindeberg principle in communications and statistical learning. _IEEE Transactions on Information Theory_, 57(4):2440-2450, 2011.
* [3] Hatef Monajemi, Sina Jafarpour, Matan Gavish, null null, David L. Donoho, Sivaram Ambikasaran, Sergio Bacallado, Dinesh Bharadia, Yuxin Chen, Young Choi, Mainak Chowdhury, Soham Chowdhury, Anil Damle, Will Fithian, Georges Goetz, Logan Grosenick, Sam Gross, Gage Hills, Michael Hornstein, Milinda Lakkam, Jason Lee, Jian Li, Linxi Liu, Carlos Sing-Long, Mike Marx, Akshay Mittal, Hatef Monajemi, Albert No, Reza Omrani, Leonid Pekelis, Junjie Qin, Kevin Raines, Ernest Ryu, Andrew Saxe, Dai Shi, Keith Siilats, David Strauss, Gary Tang, Chaojun Wang, Zoey Zhou, and Zhen Zhu. Deterministic matrices matching the compressed sensing phase transitions of gaussian random matrices. _Proceedings of the National Academy of Sciences_, 110(4):1181-1186, 2013.
* [4] Emmanuel J Candes, Pragya Sur, et al. The phase transition for the existence of the maximum likelihood estimate in high-dimensional logistic regression. _The Annals of Statistics_, 48(1):27-42, 2020.
* [5] Peter L. Bartlett, Philip M. Long, Gabor Lugosi, and Alexander Tsigler. Benign overfitting in linear regression. _Proceedings of the National Academy of Sciences_, 117(48):30063-30070, 2020.
* [6] Xiaoyi Mai and Zhenyu Liao. High dimensional classification via empirical risk minimization: Improvements and optimality. _arXiv: 1905.13742_, 2019.
* [7] Francesca Mignacco, Florent Krzakala, Yue Lu, Pierfrancesco Urbani, and Lenka Zdeborova. The role of regularization in classification of high-dimensional noisy gaussian mixture. In _International Conference on Machine Learning_, pages 6874-6883. PMLR, 2020.
* [8] Hossein Taheri, Rantin Pedarsani, and Christos Thrampoulidis. Optimality of least-squares for classification in gaussian-mixture models. In _2020 IEEE International Symposium on Information Theory (ISIT)_, pages 2515-2520. IEEE, 2020.
* [9] Ganesh Ramachandra Kini and Christos Thrampoulidis. Phase transitions for one-vs-one and one-vs-all linear separability in multiclass gaussian mixtures. In _ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)_, pages 4020-4024. IEEE, 2021.
* [10] Ke Wang and Christos Thrampoulidis. Benign overfitting in binary classification of gaussian mixtures. In _ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)_, pages 4030-4034. IEEE, 2021.
* [11] Maria Refinetti, Sebastian Goldt, Florent Krzakala, and Lenka Zdeborova. Classifying high-dimensional gaussian mixtures: Where kernel methods fail and neural networks succeed. In _International Conference on Machine Learning_, pages 8936-8947. PMLR, 2021.
* [12] Bruno Loureiro, Gabriele Sicuro, Cedric Gerbelot, Alessandro Pacco, Florent Krzakala, and Lenka Zdeborova. Learning gaussian mixtures with generalized linear models: Precise asymptotics in high-dimensions. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan, editors, _Advances in Neural Information Processing Systems_, volume 34, pages 10144-10157. Curran Associates, Inc., 2021.
* [13] Sebastian Goldt, Marc Mezard, Florent Krzakala, and Lenka Zdeborova. Modelling the influence of data structure on learning in neural networks: the hidden manifold model. _Physical Review X_, 10:041044, 2019.

* [14] Sebastian Goldt, Bruno Loureiro, Galen Reeves, Florent Krzakala, Marc Mezard, and Lenka Zdeborova. The gaussian equivalence of generative models for learning with shallow neural networks. In Joan Bruna, Jan Hesthaven, and Lenka Zdeborova, editors, _Proceedings of the 2nd Mathematical and Scientific Machine Learning Conference_, volume 145 of _Proceedings of Machine Learning Research_, pages 426-471. PMLR, 16-19 Aug 2022.
* [15] Noureddine El Karoui. The spectrum of kernel random matrices. _The Annals of Statistics_, 38(1):1-50, 2010.
* [16] Hong Hu and Yue M. Lu. Universality laws for high-dimensional learning with random features. _IEEE Transactions on Information Theory_, pages 1-1, 2022.
* [17] Andrea Montanari and Basil N. Saeed. Universality of empirical risk minimization. In Po-Ling Loh and Maxim Raginsky, editors, _Proceedings of Thirty Fifth Conference on Learning Theory_, volume 178 of _Proceedings of Machine Learning Research_, pages 4310-4312. PMLR, 02-05 Jul 2022.
* [18] Bruno Loureiro, Cedric Gerbelot, Hugo Cui, Sebastian Goldt, Florent Krzakala, Marc Mezard, and Lenka Zdeborova. Learning curves of generic features maps for realistic datasets with a teacher-student model. _Advances in Neural Information Processing Systems_, 34, 2021.
* [19] Lucas Clarke, Bruno Loureiro, Florent Krzakala, and Lenka Zdeborova. Theoretical characterization of uncertainty in high-dimensional linear classification. _arXiv: 2202.03295_, 2022.
* [20] Bruno Loureiro, Cedric Gerbelot, Maria Refinetti, Gabriele Sicuro, and Florent Krzakala. Fluctuations, bias, variance & ensemble of learners: Exact asymptotics for convex losses in high-dimension. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, _Proceedings of the 39th International Conference on Machine Learning_, volume 162 of _Proceedings of Machine Learning Research_, pages 14283-14314. PMLR, 17-23 Jul 2022.
* [21] Federica Gerace, Florent Krzakala, Bruno Loureiro, Ludovic Stephan, and Lenka Zdeborova. Gaussian universality of linear classifiers with random labels in high-dimension. _arXiv: 2205.13303_, 2022.
* [22] Luca Pesce, Florent Krzakala, Bruno Loureiro, and Ludovic Stephan. Are gaussian data all you need? extents and limits of universality in high-dimensional generalized linear estimation. _arXiv preprint arXiv:2302.08923_, 2023.
* [23] Mohamed El Amine Seddik, Cosme Louart, Mohamed Tamaazousti, and Romain Couillet. Random matrix theory proves that deep learning representations of GAN-data behave as Gaussian mixtures. In _Proceedings of the 37th International Conference on Machine Learning_, ICML'20, pages 8573-8582. JMLR.org, July 2020.
* [24] Lucas Clarke, Bruno Loureiro, Florent Krzakala, and Lenka Zdeborova. A study of uncertainty quantification in overparametrized high-dimensional models. _arXiv: 2210.12760_, 2022.
* 204, 2011.
* [26] Terence Tao and Van Vu. Random matrices: universal properties of eigenvectors. _Random Matrices: Theory and Applications_, 01(01):1150001, 2012.
* [27] Ashkan Panahi and Babak Hassibi. A universal analysis of large-scale regularized least squares solutions. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 30. Curran Associates, Inc., 2017.
* [28] Andrea Montanari and Phan-Minh Nguyen. Universality of the elastic net error. In _2017 IEEE International Symposium on Information Theory (ISIT)_, pages 2338-2342, 2017.
* [29] Ehsan Abbasi, Fariborz Salehi, and Babak Hassibi. Universality in learning from linear measurements. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alche-Buc, E. Fox, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 32. Curran Associates, Inc., 2019.

* [30] Alia Abbara, Antoine Baker, Florent Krzakala, and Lenka Zdeborova. On the universality of noiseless linear estimation with respect to the measurement matrix. _Journal of Physics A: Mathematical and Theoretical_, 53(16):164001, mar 2020.
* [31] Rishabh Dudeja, Subhabrata Sen, and Yue M. Lu. Spectral universality of regularized linear regression with nearly deterministic sensing matrices. _arXiv: 2208.02753_, 2022.
* [32] Yue M. Lu and Horng-Tzer Yau. An equivalence principle for the spectrum of random inner-product kernel matrices. _arXiv: 2205.06308_, 2022.
* [33] Theodor Misiakiewicz. Spectrum of inner-product kernel matrices in the polynomial regime and multiple descent phenomenon in kernel ridge regression. _arXiv: 2204.10425_, 2022.
* [34] Andrea Montanari, Feng Ruan, Youngtak Sohn, and Jun Yan. The generalization error of max-margin linear classifiers: High-dimensional asymptotics in the overparametrized regime. _arXiv: 1911.01544_, 2019.
* [35] Federica Gerace, Bruno Loureiro, Florent Krzakala, Marc Mezard, and Lenka Zdeborova. Generalisation error in learning with random features and the hidden manifold model. In _International Conference on Machine Learning_, pages 3452-3462. PMLR, 2020.
* [36] Oussama Dhifallah and Yue M. Lu. A precise performance analysis of learning with random features. _arXiv: 2008.11904_, 2020.
* 1695, 2022.
* [38] Dominik Schroder, Hugo Cui, Daniil Dmitriev, and Bruno Loureiro. Deterministic equivalent and error universality of deep random features learning. _arXiv: 2302.00401_, 2023.
* [39] Cosme Louart, Zhenyu Liao, and Romain Couillet. A random matrix approach to neural networks. _The Annals of Applied Probability_, 28(2):1190-1248, 2018.
* [40] Mohamed El Amine Seddik, Cosme Louart, Romain Couillet, and Mohamed Tamaazousti. The unexpected deterministic and universal behavior of large softmax classifiers. In _International Conference on Artificial Intelligence and Statistics_, pages 1045-1053. PMLR, 2021.
* [41] Mehdi Mirza and Simon Osindero. Conditional generative adversarial nets. _arXiv: 1411.1784_, 2014.
* [42] Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive uncertainty estimation using deep ensembles. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 30. Curran Associates, Inc., 2017.
* [43] Mario Geiger, Arthur Jacot, Stefano Spigler, Franck Gabriel, Levent Sagun, Stephane d'Ascoli, Giulio Biroli, Clement Hongler, and Matthieu Wyart. Scaling description of generalization with number of parameters in deep learning. _Journal of Statistical Mechanics: Theory and Experiment_, 2020(2):023401, feb 2020.
* [44] Stephane d'Ascoli, Maria Refinetti, Giulio Biroli, and Florent Krzakala. Double trouble in double descent: Bias and variance (s) in the lazy regime. In _International Conference on Machine Learning_, pages 2280-2290. PMLR, 2020.
* [45] Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms. _arXiv: 1708.07747_, 2017.
* [46] Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. In J. Platt, D. Koller, Y. Singer, and S. Roweis, editors, _Advances in Neural Information Processing Systems_, volume 20. Curran Associates, Inc., 2007.
* [47] Francesco Guerra. Broken replica symmetry bounds in the mean field spin glass model. _Communications in mathematical physics_, 233(1):1-12, 2003.

* [48] R Tyrrell Rockafellar. _Convex analysis_, volume 18. Princeton university press, 1970.
* [49] Michael Celentano, Andrea Montanari, and Yuting Wei. The lasso with general gaussian designs with applications to hypothesis testing. _arXiv: 2007.13716_, 2020.
* [50] J. W. Lindeberg. Eine neue Herleitung des Exponentialgesetzes in der Wahrscheinlichkeitsrechnung. _Mathematische Zeitschrift_, 15(1):211-225, December 1922.
* [51] Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and generalization in neural networks. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 31. Curran Associates, Inc., 2018.
* [52] S. G. Bobkov. On concentration of distributions of random weighted sums. _The Annals of Probability_, 31(1):195-215, January 2003.
* [53] Michel Ledoux. _The concentration of measure phenomenon_, volume 89 of _Math. Surv. Monogr._ American Mathematical Society (AMS), Providence, RI, 2001.
* [54] Karl Pearson. On lines and planes of closest fit to systems of points in space. _The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science_, 2(11):559-572, November 1901.
* [55] Mark A. Kramer. Nonlinear principal component analysis using autoassociative neural networks. _AIChE Journal_, 37(2):233-243, 1991.
* [56] Elena Facco, Maria d'Errico, Alex Rodriguez, and Alessandro Laio. Estimating the intrinsic dimension of datasets by a minimal neighborhood information. _Scientific Reports_, 7(1):12140, 2017.
* [57] Stefano Spigler, Mario Geiger, and Matthieu Wyart. Asymptotic learning curves of kernel methods: empirical data versus teacher-student paradigm. _Journal of Statistical Mechanics: Theory and Experiment_, 2020(12):124001, dec 2020.
* [58] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence, and K. Q. Weinberger, editors, _Advances in Neural Information Processing Systems_, volume 27. Curran Associates, Inc., 2014.
* [59] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. _arXiv: 1312.6114_, 2013.
* [60] Danilo Rezende and Shakir Mohamed. Variational inference with normalizing flows. In Francis Bach and David Blei, editors, _Proceedings of the 32nd International Conference on Machine Learning_, volume 37 of _Proceedings of Machine Learning Research_, pages 1530-1538, Lille, France, 07-09 Jul 2015. PMLR.
* [61] Peter L Bartlett, Dylan J Foster, and Matus J Telgarsky. Spectrally-normalized margin bounds for neural networks. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 30. Curran Associates, Inc., 2017.
* [62] Jens Behrmann, Paul Vicol, Kuan-Chieh Wang, Roger Grosse, and Joern-Henrik Jacobsen. Understanding and mitigating exploding inverses in invertible neural networks. In Arindam Banerjee and Kenji Fukumizu, editors, _Proceedings of The 24th International Conference on Artificial Intelligence and Statistics_, volume 130 of _Proceedings of Machine Learning Research_, pages 1792-1800. PMLR, 13-15 Apr 2021.
* [63] Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization for generative adversarial networks. In _International Conference on Learning Representations_, 2018.

* [64] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. In _Advances in Neural Information Processing Systems 32_, pages 8024-8035. Curran Associates, Inc., 2019.
* [65] Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. Infogan: Interpretable representation learning by information maximizing generative adversarial nets. _arXiv: 1606.03657_, 2016.
* [66] Pauli Virtanen, Ralf Gommers, Travis E. Oliphant, Matt Haberland, Tyler Reddy, David Cournapeau, Evgeni Burovski, Pearu Peterson, Warren Weckesser, Jonathan Bright, Stefan J. van der Walt, Matthew Brett, Joshua Wilson, K. Jarrod Millman, Nikolay Mayorov, Andrew R. J. Nelson, Eric Jones, Robert Kern, Eric Larson, C J Carey, Ilhan Polat, Yu Feng, Eric W. Moore, Jake VanderPlas, Denis Laxalde, Josef Perktold, Robert Cimrman, Ian Henriksen, E. A. Quintero, Charles R. Harris, Anne M. Archibald, Antonio H. Ribeiro, Fabian Pedregosa, Paul van Mulbregt, and SciPy 1.0 Contributors. SciPy 1.0: Fundamental Algorithms for Scientific Computing in Python. _Nature Methods_, 17:261-272, 2020.