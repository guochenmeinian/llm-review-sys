# On the Trade-off of Intra-/Inter-class Diversity

for Supervised Pre-training

Jieyu Zhang\({}^{1}\), Bohan Wang\({}^{2}\), Zhengyu Hu\({}^{3}\), Pang Wei Koh\({}^{1}\), Alexander Ratner\({}^{1,4}\)

\({}^{1}\) University of Washington \({}^{2}\) USTC \({}^{3}\) HKUST(GZ) \({}^{4}\) Snorkel AI, Inc.

{jieyuz2,pangwei,ajratner}@cs.washington.edu

bhwangfy@gmail.com

zhu021@connect.hkust-gz.edu.cn

These authors contributed equally to this work.

###### Abstract

Pre-training datasets are critical for building state-of-the-art machine learning models, motivating rigorous study on their impact on downstream tasks. In this work, we study the impact of the trade-off between the intra-class diversity (the number of samples per class) and the inter-class diversity (the number of classes) of a supervised pre-training dataset. Empirically, given a fixed pre-training dataset size, we find that the best downstream performance comes with a balance on the intra-/inter-class diversity. To understand the underlying mechanism, we show theoretically that downstream performance depends monotonically on both types of diversity. Notably, our theory reveals that the optimal class-to-sample ratio (\(\frac{\text{\#classes}}{\text{\#samples per class}}\)), _i.e._, the ratio of the number of pre-training classes to the number of samples per class, is invariant to the size of the pre-training dataset, enabling the prediction of the optimal number of pre-training classes. We demonstrate the effectiveness of this application by an improvement of approximately 2 points on average on downstream tasks when pre-training on ImageNet.

## 1 Introduction

Many state-of-the-art deep neural network models are pre-trained on large datasets before being finetuned for downstream tasks [13; 17; 23; 1]. While the composition of their pre-training dataset has been shown to be a key factor in the performance of these models [7; 9; 14; 8; 12; 26], how best to design these pre-training datasets still remains underexplored. In this work, we focus on supervised pre-training, one of the most popular pre-training paradigms, and study two key quantities of a supervised pre-training dataset: intra-class diversity (the number of different samples within each pre-training class) and inter-class diversity (the number of different pre-training classes). Intuitively, both diversities are beneficial for supervised pre-training [13]. Yet when the size of the pre-training dataset is fixed, these diversities trade off, since increasing one will decrease the other. Our work studies the impact of this dataset diversity trade-off on downstream performance, as well as how to balance them to design a supervised pre-training dataset with the best downstream performance.

Empirically, with ImageNet [24] as the pre-training dataset and the pre-training dataset size fixed, we show that the optimal performance on the downstream tasks occurs when a balance on the intra-/inter-class diversity is achieved. We then offer a theoretical explanation for this effect by first modeling the dataset generation process through a two-step sampling framework, and then demonstrating that the test error of the downstream task displays a rational relationship with respect to the class-to-sample ratio, _i.e._, the ratio of the number of pre-training classes to the number of samples per class, or, in other words, the ratio between inter-/intra-class diversity. The established analytical relationshipbetween downstream performance and the class-to-sample ratio can serve as a guiding principle in designing a supervised pre-training dataset by estimating the optimal class-to-sample ratio rather than the grid search.

Notably, our theory shows that given a source of a pre-training dataset and a downstream task, the optimal class-to-sample ratio is invariant to the size of the pre-training dataset. Based on such an invariance, one could estimate the optimal class-to-sample ratio with small pre-training datasets and then leverage it to build a large-scale pre-training dataset. In particular, the optimal number of pre-training classes \(\bar{K}\) and the number of examples per class \(n\) are proportional to the square root of the size of the pre-training dataset \(N\), _i.e._, \(\bar{K}\propto\sqrt{N}\), which leads to an invariant optimal class-to-sample ratio. We empirically verify our theoretical findings on ImageNet [24] and present the effectiveness of its application in predicting the optimal number of classes for pre-training datasets with different sizes. In addition, we conducted experiments with different pre-trained datasets, different model backbones, and downstream tasks of different domains to demonstrate that our findings are consistent across many scenarios.

Our major findings and contributions are as follows:

* In supervised pre-training, we observe that with a fixed pre-training dataset size, there exists a trade-off between intra-class and inter-class diversities. This balance between diversities plays a crucial role in shaping the downstream performance, underscoring the significance of considering both aspects when designing the pre-training dataset;
* We then theoretically explain this effect by first modeling the dataset generation process through a two-step sampling framework and then showing that the test error of the downstream task displays a convex relationship with respect to the class-to-sample ratio, serving as a guiding principle in designing a supervised pre-training dataset.;
* Our theory also uncovers the invariance of the optimal class-to-sample ratio with respect to the size of the pre-training dataset, allowing us to predict the optimal number of classes with a small number of pre-training data before building a larger pre-training dataset for a downstream task.

## 2 Empirical Observations

The goal of this work is to study the trade-off of intra-/inter-class diversity in a supervised pre-training dataset and its impact on the pre-trained model's performance on downstream tasks. Specifically, the inter-class diversity refers to the diversity of classes in pre-training dataset, _i.e._, how many different classes we have (\(K\)); while the intra-class diversity refers to the diversity of samples within each class, _i.e._, how many different samples in each class (\(n\)). When the size of the pre-training dataset is fixed, increasing either type of diversity will by definition decrease the other, leading to a dataset diversity trade-off. To study the impact of such dataset diversity trade-off, we experiment with pre-training datasets with varying numbers of classes and number of samples per class. The experimental details can be found in Appendix A.2.

**Evaluation protocol.** Following common practice [13], we use the ImageNet [24] as the dataset for supervised pre-training. In this work, we mainly use ResNet-18 [10] as the backbone model. For evaluating the performance of the pre-trained model on downstream tasks, we perform linear probing (tuning the head but freezing the lower layers). We repeat each individual experiment five times and report the averaged top-1 accuracy.

**Downstream tasks.** We adopt the following six datasets as the downstream classification tasks: Stanford40 dataset [25] for action recognition, StanfordDogs [15] for fine-grained object recognition, MIT67 [22] for scene classification, CIFAR10 [16] for image recognition datasets, Flowers102 [20] for image classification dataset, FGVCAircraft [18] for aircraft classification dataset.

While having all the other configurations fixed, during pre-training, we vary the number of classes and the number of samples per class. Specifically, given \(N\) as the size of the pre-training dataset and \(K\) as the number of classes, we randomly sample \(K\) classes from ImageNet and then uniformly sample \(n=\frac{N}{K}\) samples from each class to compose the dataset. We experiment with the following \(N\) and \(K\) values: {1K, 2K, 5K, 10K, 20K, 50K, 100K} and {2, 5, 10, 20, 50, 100, 200, 500, 1000} respectively. Note that with larger \(N\) (_e.g._, \(N=10\)K), we cannot evaluate smaller values of \(K\) (_e.g._, \(K=2\)), since in ImageNet each class has at most 1300 samples.

**Results and observations.** We visualize the results in Figure 1. In the contour plot, the z-value is the error rate on the test set, thus lower is better. The x-axis and y-axis are inter-class diversity (\(\log K\)) and intra-class diversity (\(\log n\)) in the log space, respectively. The values on anti-diagonal lines (\(y=-x+c\)) share the same pre-training dataset size as \(\log N=\log K+\log n\). From the results, we have two observations: 1) **Both intra-/inter-class diversity are beneficial for downstream tasks:** We can see that increasing either inter-class diversity ((\(\log K\)) or intra-class diversity (\(\log n\)), given the other is fixed, would lead to a better test error rate. This is intuitive and as expected, since the size of the pre-training dataset \(N\) would increase accordingly, which is known to be beneficial for downstream tasks [11; 7; 13]. 2) **A trade-off of intra-/inter-class diversity on downstream task performance:** More importantly, by looking at the anti-diagonal lines where \(\log N\) is fixed and equals \(\log K+\log n\), we can see a trade-off between intra-/inter-class diversity on the test error rate of downstream tasks: either cases of 1) high inter-class diversity, low intra-class diversity and 2) low inter-class diversity and high intra-class diversity would not render the best performance. Instead, some point in the middle of the anti-diagonal line leads to the lowest test error rate.

## 3 Theoretical Understanding

In this section, we first present the theoretical setup and notations and then provide a theory on the impact of the pre-training dataset diversity on downstream performance. We also show that the optimal class-to-sample ratio (\(\frac{K}{n}\)) is invariant to the size of the pre-training dataset; such a property can be leveraged to predict the optimal number of classes for building a large pre-training dataset with a small number of data samples first.

### Setup and notations

**Dataset.** To be consistent with our experimental setup, we consider the supervised pre-training task. Specifically, we can access two datasets, one for the pre-training task (denoted as \(S^{p}\)) and another for the downstream task (denoted as \(S^{d}\)). Each example in the _pre-training_ dataset consists of input features \(x\in\mathcal{X}=\mathbb{R}^{d_{1}}\) (where \(d_{1}\) is the dimension of data) and a label \(y\in[K]\) (where \(K\) is the number of classes). Specifically, we denote \(S^{p}=\{(x_{1},y_{1}),\cdots,(x_{N},y_{N})\}\), where \(N\) is the size of \(S^{p}\), and assume that \(S^{p}\) is sampled according to some underlying distribution \(\mathcal{P}\) (we do

Figure 1: Test error rate (the darker, the better) as a function of intra-class diversity on the y-axis (\(\log n\)) and inter-class diversity on the x-axis (\(\log K\)). Each plot represents a different dataset. The red dashed anti-diagonal lines indicate fixed pre-training dataset sizes (\(\log K+\log n=\log N\) is constant), from which we can see that obtaining the best downstream performance given a fixed pre-training dataset size requires balancing both diversities.

not specify \(\mathcal{P}\) here because we will analyze cases with different \(\mathcal{P}\) latter). Every example in the _downstream_ dataset consists of input features \(\tilde{x}\in\mathcal{X}\) and a label \(\tilde{y}\in[\tilde{K}]\) (note that \(\tilde{K}\) does not necessarily equal-to \(K\)), and is sampled _i.i.d._ according to an underlying distribution \(\tilde{\mathcal{P}}\). We denote \(S^{d}=\{(\tilde{x}_{1},\tilde{y}_{1}),\cdots,(\tilde{x}_{N},\tilde{y}_{\tilde{ N}})\}\) and thus \(S^{d}\sim\tilde{\mathcal{P}}^{\tilde{N}}\).

**Model.** The models for both pre-training and downstream tasks consist of two components: the feature extractor and the classifier. Specifically, the model for the pre-training task is given as \(f_{S^{p}}\circ h_{S^{p}}\), where \(f_{S^{p}}:\mathbb{R}^{d_{2}}\rightarrow\mathbb{R}^{K}\) is the pre-training classifier (\(d_{2}\) is the dimension of feature) and \(h_{S^{p}}:\mathbb{R}^{d_{1}}\rightarrow\mathbb{R}^{d_{2}}\) is the feature extractor. We denote the set of all possible \(f_{S^{p}}\) as \(\mathcal{F}\), and the set of all possible \(h\) as \(\mathcal{H}\). The model for the downstream task is given as \(f_{S^{d}}\circ h_{S^{d}}\), where \(f_{S^{d}}:\mathbb{R}^{d_{2}}\rightarrow\mathbb{R}^{\tilde{K}}\) is the downstream classifier and \(h\) is the feature extractor shared with the pre-training task. We set all possible \(f_{S^{p}}\) as \(\tilde{\mathcal{F}}\).

**Loss.** To measure the correctness of model's predictions, we use the cross-entropy loss. Specifically, given an example \((x,y)\) and pre-training/downstream model \(f\circ h\), the corresponding cross-entropy loss is defined as

\[\ell(f\circ h(x),y)=-\log\frac{e^{f_{y}\circ h(x)}}{\sum_{i}e^{f_{i}\circ h(x )}},\]

where \(f_{i}\circ h(x)\) is the \(i\)-th coordinate of \(f\circ h(x)\). We make the following assumption about the complexity of the model.

**Assumption 1**.: _There exist a positive constant \(M_{\ell}\), such that \(\forall i\), \(\forall f\in\mathcal{F}\cup\tilde{\mathcal{F}},h\in\mathcal{H},x\in\mathcal{X}\),_

\[\ell(f\circ h(x),i)\leq M_{\ell}.\]

_Furthermore, for any distribution \(Q\) over the feature space \(\mathbb{R}^{d_{1}}\), any \(m\in\mathbb{N}\), and any \(\mathcal{F}^{\prime}\in\{\mathcal{F},\tilde{\mathcal{F}}\}\), define the Gaussian complexity of function class \(\mathcal{F}^{\prime}\circ\mathcal{H}\) over the marginal distribution \(Q\) with sample number \(m\) as_

\[G^{Q}_{m}(\mathcal{F}^{\prime}\circ\mathcal{H})\triangleq\mathbb{E}_{(x_{i})_ {i=1}^{m}\sim Q^{m}}\mathbb{E}_{(\sigma_{i,j})_{i\in[N],j\in\mathcal{K}^{ \prime}}\sim\mathcal{N}(0,\mathbf{1}_{n\times K^{\prime}})}\sup_{f\in\mathcal{ F},h\in\mathcal{H}}\sum_{i=1}^{m}\sum_{j=1}^{K^{\prime}}\sigma_{i,j}f_{j} \circ h(x_{i}).\]

_Here \(K^{\prime}=\dim(\mathcal{F}^{\prime})\). We assume that \(G^{Q}_{m}(\mathcal{F}\circ\mathcal{H})\leq G\sqrt{m}\), where \(G\) is independent of \(Q\) and \(m\). 2_

Footnote 2: This inequality holds for a wide range of models, including deep neural networks [2]. Assumption 1 constrains the dependence of model complexity over the number of sample \(N\), which is a standard assumption in generalization analysis [19].

**Risks and corresponding minimizers.** We first define the empirical risk \(\bar{\mathcal{R}}_{p}(f\circ h,S^{p})\) and population risk \(\mathcal{R}_{p}(f\circ h,\mathcal{P})\) over the pre-training task as

\[\bar{\mathcal{R}}_{p}(f\circ h,S^{p})\triangleq\frac{1}{N}\sum_{i=1}^{N}[\ell( f\circ h(x_{i}),y_{i})],\mathcal{R}_{p}(f\circ h,\mathcal{P})\triangleq \mathbb{E}_{S^{p}\sim\mathcal{P}}\bar{\mathcal{R}}_{p}(f\circ h,S^{p}).\]

The corresponding feature extractor and classifier of the empirical risk minimizer over the pre-training task as

\[h_{S^{p}}\triangleq\arg\min_{h\in\mathcal{H}}\left(\min_{f\in\mathcal{F}} \bar{\mathcal{R}}_{p}(f\circ h,S^{p})\right),f_{S^{p}}\triangleq\arg\min_{f \in\mathcal{F}}\left(\min_{h\in\mathcal{H}}\bar{\mathcal{R}}_{p}(f\circ h,S^{ p})\right).\]

Given a feature extractor \(h\in\mathcal{H}\), we measure its performance over the pre-training task through a classifier agnostic approach by considering

\[\mathcal{R}_{p}(h,\mathcal{P})\triangleq\min_{f\in\mathcal{F}}\mathcal{R}_{p }(f\circ h,\mathcal{P}).\]

Note here we slightly abuse the notation of \(\mathcal{R}_{p}\) without causing confusion as \(f\circ h\) and \(h\) have different image spaces. The representation error of \(h\) over \(\mathcal{P}\) is then defined as the gap between the risk of \(h\) and the smallest possible risk

\[\mathcal{E}_{p}(h,\mathcal{P})\triangleq\mathcal{R}_{p}(h,\mathcal{P})-\min_{h \in\mathcal{H}}\mathcal{R}_{p}(\tilde{h},\mathcal{P}).\]Similarly, the empirical risk \(\bar{\mathcal{R}}_{d}(f\circ h,S^{d})\) and population risk \(\mathcal{R}_{d}(f\circ h,\bar{\mathcal{P}})\) over the downstream task are defined as

\[\bar{\mathcal{R}}_{d}(f\circ h,S^{d})\triangleq\sum_{i=1}^{\tilde{N}}\ell(\bar{ f}\circ h(\tilde{x}_{i}),\tilde{y}_{i}),\ \mathcal{R}_{d}(f\circ h,\bar{\mathcal{P}}) \triangleq\mathbb{E}_{S^{d}\sim\tilde{\mathcal{P}}^{\tilde{N}}}\bar{\mathcal{ R}}_{d}(f\circ h,S^{d}).\]

The learned classifier from the downstream task can then be defined as

\[f_{S^{d}}\triangleq\arg\min_{f\in\bar{\mathcal{F}}}\left(\min_{h\in\mathcal{H}} \bar{\mathcal{R}}_{d}(f\circ h_{S^{p}},S^{d})\right).\]

The corresponding performance and excess risk of \(h\) over the downstream task can then be defined as

\[\mathcal{R}_{d}(h,\bar{\mathcal{P}})\triangleq\min_{\bar{f}\in\bar{\mathcal{F} }}\mathcal{R}_{d}(\bar{f}\circ h,\bar{\mathcal{P}}),\ \mathcal{E}_{d}(h,\bar{ \mathcal{P}})\triangleq\mathcal{R}_{d}(h,\bar{\mathcal{P}})-\min_{\tilde{h} \in\mathcal{H}}\mathcal{R}_{d}(\tilde{h},\bar{\mathcal{P}}).\]

Finally, we are interested in the excess risk of the obtained model \(f_{S^{d}}\circ h_{S^{p}}\) over the downstream task, i.e.,

\[\mathcal{E}_{d}(f_{S^{d}}\circ h_{S_{p}},\bar{\mathcal{P}})\triangleq\mathcal{ R}_{d}(f_{S^{d}}\circ h_{S_{p}},\bar{\mathcal{P}})-\min_{\tilde{h}\in\mathcal{H}} \mathcal{R}_{d}(\tilde{h},\bar{\mathcal{P}}).\]

### A theory on the impact of intra-/inter-class diversity trade-off

In this work, we focus on a common practice of collecting the pre-training dataset: one first sample \(K\) classes and then collect \(n\) samples for each class. We start with a detailed characterization of such data generation process, followed by assumptions and the main result.

**Data generation process 1.** We assume the pre-training data is generated through a two-step sampling process. Specifically, suppose that there is a distribution \(\mathcal{D}\) over \(\Delta(\mathcal{X})\) (the set consisting of all distributions on \(\mathcal{X}\)). Then, \(S^{p}\) is generated by the following procedure (and \(\mathcal{P}\) is naturally induced)

* Sample \(K\) classes by i.i.d. sampling \(K\) distributions \(\{\mathcal{P}_{i}\}_{i=1}^{K}\) according to \(\mathcal{D}\). These are respectively the underlying distributions of \(K\) classes;
* For each \(i\in[K]\), i.i.d. sample \(n\) data \(\{x_{i,1},\cdots,x_{i,n}\}\) according to \(\mathcal{P}_{i}\) and denote \(S_{i}=\{(x_{i,1},i),\cdots,(x_{i,n},i)\}\). Note here \(x_{i,j}\) does not contain the information of label, as its label information is already contained in \(i\). The whole dataset is obtained by putting all \(S_{i}\) together, i.e., \(S^{p}=\{S_{1},\cdots,S_{K}\}\).

We make the following assumption on the correlation between the representation powers of the pre-training and the downstream task.

**Assumption 2**.: _Given \(\mathcal{P}\), there exists non-negative coefficients \(\nu_{0}^{\tilde{\mathcal{P}}}(\mathcal{P})\) and \(\nu_{1}^{\tilde{\mathcal{P}}}(\mathcal{P})\), such that \(\forall h\in\mathcal{H}\),_

\[\mathcal{E}_{d}(h,\bar{\mathcal{P}})\leq\nu_{1}^{\tilde{\mathcal{P}}}(\mathcal{ P})\mathcal{E}_{p}(h,\mathcal{P})+\nu_{0}^{\tilde{\mathcal{P}}}(\mathcal{P}).\]

_We further assume that \(\nu_{0}^{\tilde{\mathcal{P}}}(\mathcal{P})\) and \(\nu_{1}^{\tilde{\mathcal{P}}}(\mathcal{P})\) are stable, that is, there exist two \(\tilde{\mathcal{P}}\)-dependent positive constants \(M_{0}^{\tilde{\mathcal{P}}}\) and \(M_{1}^{\tilde{\mathcal{P}}}\), such that for any \(\mathcal{P}=\Pi_{i=1}^{K}(\mathcal{P}_{i},i)^{n}\) and \(\mathcal{P}^{\prime}=\Pi_{i=1}^{K-1}(\mathcal{P}_{i},i)^{n}\times(\mathcal{P} _{K}^{\prime},K)^{n}\) which differ by only one component, we have that \(|\nu_{0}^{\tilde{\mathcal{P}}}(\mathcal{P})-\nu_{0}^{\tilde{\mathcal{P}}}( \mathcal{P}^{\prime})|\leq\frac{M_{0}^{\tilde{\mathcal{P}}}}{K}\) and \(|\nu_{1}^{\tilde{\mathcal{P}}}(\mathcal{P})-\nu_{1}^{\tilde{\mathcal{P}}}( \mathcal{P}^{\prime})|\leq\frac{M_{0}^{\tilde{\mathcal{P}}}}{K}\). Moreover, we assume that \(\nu_{0}^{\tilde{\mathcal{P}}}(\mathcal{P})\) and \(\nu_{1}^{\tilde{\mathcal{P}}}(\mathcal{P})\) concentrate around their means, i.e., there exist \(\nu_{0}^{\tilde{\mathcal{P}}}(\mathcal{D})\), \(\nu_{1}^{\tilde{\mathcal{P}}}(\mathcal{D})\), \(C_{0}^{\tilde{\mathcal{P}}}\) and \(C_{1}^{\tilde{\mathcal{P}}}\), such that \(|\mathbb{E}_{\{\mathcal{P}_{i}\}_{i=1}^{K}\sim\mathcal{D}^{K}}\nu_{0}^{ \tilde{\mathcal{P}}}(\mathcal{P})-\nu_{0}^{\tilde{\mathcal{P}}}(\mathcal{D})| \leq\frac{C_{0}^{\tilde{\mathcal{P}}}}{\sqrt{K}}\) and \(|\mathbb{E}_{\{\mathcal{P}_{i}\}_{i=1}^{K}\sim\mathcal{D}^{K}}\nu_{1}^{ \tilde{\mathcal{P}}}(\mathcal{P})-\nu_{1}^{\tilde{\mathcal{P}}}(\mathcal{D})| \leq\frac{C_{0}^{\tilde{\mathcal{P}}}}{\sqrt{K}}\)._

Assumption 2 assumes that the pre-training representation error can bound the downstream representation error, which is a common assumption in existing works [6, 3]. Also, as \(\mathcal{P}\) is derived by sampling \(K\) distributions according to \(\mathcal{D}\), we make mild assumptions that the coefficients \(\nu_{0}^{\tilde{\mathcal{P}}}(\mathcal{P})\) and \(\nu_{1}^{\tilde{\mathcal{P}}}(\mathcal{P})\) is robust when changing the underlying distribution of only one class, and when \(K\) grows, the expectation of \(\nu_{0}^{\tilde{\mathcal{P}}}(\mathcal{P})\) and \(\nu_{1}^{\tilde{\mathcal{P}}}(\mathcal{P})\) converge to some limits.

**Theorem 3.1**.: _Let Assumptions 1 and 2 hold. For **data generation process 1**, with probability over the sampling of the datasets at least \(1-\delta\), we have_

\[\mathcal{E}_{d}(f_{S^{4}}\circ h_{S_{p}},\tilde{\mathcal{P}})\leq \left(\nu_{1}^{\tilde{\mathcal{P}}}(\mathcal{D})+M_{1}\sqrt{\frac{\log\frac{6}{ \delta}}{2K}}+\frac{C_{1}}{\sqrt{K}}\right)\left(5M_{\ell}\sqrt{\frac{\log\frac {6}{\delta}}{2n}}+\frac{2G\sqrt{2}}{\sqrt{n}}\right)+\nu_{0}^{\tilde{\mathcal{P }}}(\mathcal{D})\] \[+M_{0}\sqrt{\frac{\log\frac{6}{\delta}}{2K}}+\frac{C_{0}}{\sqrt{K} }+5M_{\ell}\sqrt{\frac{\log\frac{6}{\delta}}{2N}}+2\sqrt{2}G\frac{1}{\sqrt{N}}. \tag{1}\]

The detailed proof is deferred to Appendix A.8.1. Below we simplify the right-hand-side of Equation 1 and show that the empirically observed downstream performance trade-off can be explained by such a result.

Simplifying the Theorem 3.1.Denote the RHS of Equation 1 as \(U\), we have

\[U= \nu_{1}^{\tilde{\mathcal{P}}}(\mathcal{D})\left(5M_{\ell}\sqrt{ \frac{\log\frac{6}{\delta}}{2}}+2G\sqrt{2}\right)\frac{1}{\sqrt{n}}+\left(M_{ 0}\sqrt{\frac{\log\frac{6}{\delta}}{2}}+C_{0}\right)\frac{1}{\sqrt{K}}+\left(M _{1}\sqrt{\frac{\log\frac{6}{\delta}}{2}}+C_{1}\right)\] \[\times\left(5M_{\ell}\sqrt{\frac{\log\frac{6}{\delta}}{2}}+2G \right)\frac{1}{\sqrt{N}}+\nu_{0}^{\tilde{\mathcal{P}}}(\mathcal{D})+5M_{\ell }\sqrt{\frac{\log\frac{6}{\delta}}{2N}}+2\sqrt{2}G\frac{1}{\sqrt{N}}.\]

We can see that the above equation can be simplified as

\[U=\frac{A}{\sqrt{n}}+\frac{B}{\sqrt{K}}+\frac{C}{\sqrt{N}}+D, \tag{2}\]

where \(A,B,C,D\) do not depend on \(N,K,n\), but instead only depend on the properties of the underlying pre-training and the downstream task data distribution.

Explaining downstream performance trade-off given a fixed \(N\).From Equation 2 we can see that the performance on the target task would increase when we increase 1) intra-class diversity \(n\), 2) inter-class diversity \(K\), and 3) the size of pre-training dataset \(N\). When \(N\) is fixed, however, increasing either intra-class diversity or inter-class diversity would decrease the other (since \(N=n\times K\)) and therefore eventually lead to a performance drop. Another way to see this is to parameterize \(U\) as a function of \(K\) without \(n\):

\[U(K)=\frac{A\sqrt{K}}{\sqrt{N}}+\frac{B}{\sqrt{K}}+\frac{C}{\sqrt{N}}+D, \tag{3}\]

From this we can clearly see that both extremes of \(K\) (too large or too small) would not lead to optimal performance. A similar conclusion can be drawn regarding \(n\) when parametrizing \(U\) as a function of \(n\) without \(K\).

### Balancing intra-/inter-class diversity: the optimal class-to-sample ratio

When \(N\) is fixed, by leveraging the fact that \(N=n\times K\), we can express \(U\) as

\[U=\frac{1}{N^{\frac{1}{4}}}\left(Ax^{\frac{1}{4}}+B\frac{1}{x^{\frac{1}{4}}} \right)+c, \tag{4}\]

where \(c=\frac{C}{\sqrt{N}}+D\) is a constant and \(x=\frac{K}{n}\) is the class-to-sample ratio. To minimize \(U\), we have the optimal class-to-sample ratio \(\bar{x}=\frac{B^{2}}{A^{2}}\). Notably, because both \(A\) and \(B\) have no dependency on \(N\), **the optimal class-to-sample ratio for a specific downstream task is invariant to the size of the pre-training dataset.** Motivated by this, one could estimate the optimal class-to-sample ratio using a small \(N\) and then use it to predict the optimal number of classes for building a large pre-training dataset. In particular, given the optimal class-to-sample ratio \(\bar{x}\), the optimal number of classes is \(\bar{K}=\frac{B}{A}\sqrt{N}\). Based on Equation 4, one only needs three (class-to-sample ratio, performance) tuples to estimate the constants (\(A\), \(B\), \(c\)) with a fixed \(N\) for computing the optimal class-to-sample ratio.

[MISSING_PAGE_FAIL:7]

### Are the trade-off curves for different \(N\) aligned?

According to our findings in Section 3.3, the test error on a downstream task is a convex function with respect to the class-to-sample ratio (Equation 4) and the optimal class-to-sample ratio \(\bar{x}=\frac{B^{2}}{A^{2}}\) is invariant to the size of pre-training dataset \(N\). To empirically verify this, we visualize the performance on downstream tasks as a function of the class-to-sample ratio with different \(N\) in Figure 2.

From the figures, we can see that the curves of different \(N\) for a specific downstream task are aligned, as well as the optimal class-to-ratios, which follows our theoretical findings. This indicates that empirically, one could extrapolate the optimal class-to-ratio estimated with a small \(N\) for building a large-scale pre-training dataset. Note that the rightmost point of each curve corresponds to using all the classes in ImageNet, _i.e._, \(K=1000\), and we can see that such a standard design choice does not lead to optimal downstream performance, especially with small pre-training datasets. In addition, we conducted experiments with different pre-trained datasets (Appendix A.4), different model backbones (Appendix A.6), and downstream tasks of different domains (Appendix A.5) to demonstrate that our findings are consistent across many scenarios.

### Predicting the optimal number of pre-training classes

As a direct application of our theoretical and empirical findings, one could estimate the optimal class-to-sample ratio with a small \(N\) and use it to decide the optimal number of classes when building a larger pre-training dataset. In particular, denoted by \(\bar{x}\) the optimal class-to-sample ratio, the optimal number of classes for a given \(N\) is \(\bar{K}=\sqrt{\bar{x}N}\). We refer to this approach as **Extrapolation**. We empirically compare it against the following methods of deciding the number of classes when building a pre-training dataset: 1) **Standard:** the number of classes equals 1000 as the standard design choice of ImageNet; 2) **Grid Search:** the number of classes corresponding to the data point with the lowest error rate for each curve in Figure 2; 3) **Fitting:** given the target size, we use the corresponding data points in Figure 2 to fit the theoretically-derived performance function (Equation 4), and then analytically calculate the optimal number of classes. We use the calculated number of classes to build a pre-training dataset and measure the performance of a model trained with it. In reality, the latter two baselines require repeatedly training models on pre-training datasets with the target size yet different numbers of classes and are therefore time-consuming and data-intensive.

We set the target size of pre-training dataset as {50K, 100K} and round the number of classes to an integer if needed. For our Extrapolation method, we only use three data points with \(N\) being much smaller than the target size to estimate the optimal class-to-sample ratio: \(N=5000\) and \(K=\{10,50,200\}\). We use the estimated optimal class-to-sample ratio for both target sizes. The results as well as the number of classes selected by the above methods can be found in Table 1. From

Figure 2: Test error rate across class-to-sample ratio. The vertical bar is the standard deviation.

the results, we can see that although the ImageNet dataset is widely used, its number of classes (\(K=1000\)) is not optimal for building a pre-training dataset of 50K/100K samples, since the Standard underperforms other methods. Besides, methods except for the Standard all render similar test error rate even though their number of classes are different, which reveals that the performance is not sensitive to the number of classes as long as we pick a reasonable number. Thus, our Extrapolation method is superior to Grid Search and Fitting, since it needs much fewer samples to estimate the number of classes, while both Grid Search and Fitting require building the pre-training dataset of target size multiple times.

### Extra data needed for estimating the optimal class-to-sample ratio

One advantage of the Standard method is that it does not require extra data for estimating the optimal number of classes. In contrast, other methods would introduce extra data unused in the final pre-training dataset. For example, when estimating the optimal class-to-sample ratio, one may sample data from 1000 classes but eventually find the optimal number of classes is 600, then the data of the additional 400 classes would not be used in the final pre-training dataset. We then investigate how much data is needed by different methods to build the final pre-training dataset. We list the total number of samples used by different methods in Table 2. From the table, we can see that Grid Search and Fitting require much more samples than the target size of the pre-training dataset, since they involve building the pre-training dataset of the target size multiple times, while the number of extra data needed by Extrapolation is relatively small, because it estimates the optimal number of classes using a small \(N\) of 5000. In addition, using Extrapolation, one only needs to estimate the optimal class-to-sample ratio once and then use it for building pre-training datasets with different sizes without re-estimation.

As the Extrapolation requires more data than the Standard, a fairer comparison between them needs to ensure the total number of data used is similar, and the Standard would have a slightly larger pre-training dataset since it does not spend any data budget for estimation. In Figure 3, we compare

\begin{table}
\begin{tabular}{c|c|c c c c c c} \hline \hline \multirow{2}{*}{\(N\)} & \multirow{2}{*}{**Method**} & \multicolumn{6}{c}{**Target Dataset**} \\ \cline{3-8}  & & **CIFAR10** & **FGVC Aircraft** & **Flowers102** & **MITG7** & **Stanford40** & **StanfordDogs** \\ \hline \multirow{8}{*}{50K} & Standard (\(K\)=1000) & 29.19\({}_{0.34}\) & 77.80\({}_{0.38}\) & 34.08\({}_{0.33}\) & 57.51\({}_{0.11}\) & 62.45\({}_{0.38}\) & 64.96\({}_{0.38}\) \\ \cline{2-8}  & Grid Search & 26.24\({}_{0.44}\) & 75.96\({}_{0.43}\) & 32.70\({}_{0.31}\) & 54.10\({}_{0.34}\) & 57.05\({}_{0.44}\) & 59.12\({}_{0.22}\) \\ \cline{2-8}  & & (200) & (200) & (500) & (200) & (100) & (200) \\ \cline{2-8}  & Fitting & 26.25\({}_{1.04}\) & 76.00\({}_{10.44}\) & 32.13\({}_{0.10}\) & 53.60\({}_{0.19}\) & 57.10\({}_{0.13}\) & 59.76\({}_{0.38}\) \\ \cline{2-8}  & & (169) & (293) & (415) & (161) & (138) & (260) \\ \cline{2-8}  & Extrapolation & 26.27\({}_{1.02}\) & 76.18\({}_{1.04}\) & 32.60\({}_{0.106}\) & 53.01\({}_{0.123}\) & 57.25\({}_{0.14}\) & 60.15\({}_{0.17}\) \\ \cline{2-8}  & & (190) & (168) & (296) & (163) & (134) & (158) \\ \hline \multirow{8}{*}{100K} & Standard (\(K\)=1000) & 25.04\({}_{0.38}\) & 75.21\({}_{0.14}\) & 27.69\({}_{0.40}\) & 52.79\({}_{0.36}\) & 54.19\({}_{0.39}\) & 54.30\({}_{0.44}\) \\ \cline{2-8}  & Grid Search & 23.13\({}_{0.08}\) & 73.01\({}_{0.50}\) & 27.15\({}_{0.44}\) & 50.30\({}_{0.10.44}\) & 51.45\({}_{0.33}\) & 51.98\({}_{0.38}\) \\ \cline{1-1} \cline{2-8}  & & (500) & (500) & (500) & (200) & (200) & (500) \\ \cline{1-1} \cline{2-8}  & Fitting & 22.67\({}_{1.03}\) & 73.45\({}_{0.33}\) & 26.67\({}_{0.31}\) & 50.82\({}_{0.13}\) & 52.24\({}_{0.33}\) & 52.24\({}_{0.33}\) \\ \cline{1-1} \cline{2-8}  & & (276) & (372) & (655) & (249) & (207) & (392) \\ \cline{1-1} \cline{2-8}  & Extrapolation & 23.12\({}_{0.13}\) & 73.30\({}_{0.17}\) & 26.98\({}_{0.14}\) & 50.42\({}_{0.23}\) & 52.32\({}_{0.008}\) & 53.17\({}_{0.38}\) \\ \cline{1-1} \cline{2-8}  & & (269) & (238) & (418) & (231) & (190) & (233) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Test error rate on downstream tasks (the first row of each task, lower is better), and the number of classes in the pre-training dataset (the second row).

\begin{table}
\begin{tabular}{c|c|c c c c c c} \hline \hline \multirow{2}{*}{\(N\)} & \multirow{2}{*}{**Method**} & \multicolumn{6}{c}{**Target Dataset**} \\ \cline{3-8}  & & & **CIFAR10** & **FGVC Aircraft** & **Flowers102** & **MITG7** & **Stanford40** & **StanfordDogs** \\ \hline \multirow{8}{*}{50K} & Standard (\(K\)=1000) & \multicolumn{6}{c}{50K} \\ \cline{2-8}  & Grid Search & & 150K (5 trials) & & & & \\ \cline{2-8}  & Fitting & 158.164K & 161.570K & 159.403K & 158.694K & 159.268K & 160.538K \\ \cline{2-8}  & Extrapolation & 55.418K & 55.183K & 55.830K & 55.117K & 54.598K & 55.045K \\ \hline \multirow{8}{*}{100K} & Standard (\(K\)=1000) & \multicolumn{6}{c}{100K} \\ \cline{2-8}  & Grid Search & & & 260K (4 trials) & & & \\ \cline{1-1} \cline{2-8}  & Fitting & 272.336K & 271.836K & 268.164K & 269.878K & 261.981K & 270.579K \\ \cline{1-1} \cline{2-8}  & Extrapolation & 103.937K & 103.608K & 104.517K & 103.515K & 103.049K & 103.401K \\ \hline \hline \end{tabular}
\end{table}
Table 2: Total number of samples used for building the pre-training dataset.

the Extrapolation to the Standard whose pre-training dataset size is slightly larger than the total number of data used by the Extrapolation. Each bar plot represents a specific downstream task, and the number in parentheses indicates the size (\(N\)) of the corresponding pre-training dataset. We can observe that in most cases, the Extrapolation demonstrates improved performance over the Standard even when the latter uses a larger pre-training dataset, which further justifies the effectiveness of the Extrapolation method.

## 5 Related Work

We briefly review recent studies on supervised pre-training from data-centric perspectives. First, on the composition of the pre-training dataset, [9] presents a scaling law that predicts the test loss on downstream tasks under varying source dataset compositions, while [14] studies the performance on downstream tasks when subsets of the pre-training dataset are removed. Second, on the label space of supervised pre-training, [26] offers a statistical analysis explaining pre-training techniques' success in NLP, showing that class diversity in pre-training tasks substantially enhances sample efficiency in downstream tasks, while the study by [12] explores the impact of pre-training label granularity on downstream tasks, emphasizing the importance of selecting an appropriate level of label granularity. Lastly, [7] explores the impact of pre-training data distribution on transfer performance, finding the choice of the pre-training dataset to be crucial. In contrast, we dive into the trade-off of the intra-/inter-class diversity in the supervised pre-training dataset. The study related the most to ours is [13], where the authors empirically examined the importance of pre-training data characteristics on downstream performance. While covering a wide range of pre-training data characteristics, this study only briefly explores the trade-off of intra-/inter-class diversity in the pre-training dataset (Section 5.5 in [13]). Specifically, the authors only considered two different cases of intra-/inter-class diversity, _i.e._, \(K=\{500,1000\}\) for ImageNet. In contrast, we, both empirically and theoretically, show how such a trade-off would impact the downstream performance and our theory uncovers a surprising property of the optimal class-to-sample ratio: it is invariant to the size of the pre-training dataset.

## 6 Conclusion

In this study, we explore the trade-off of the intra-/inter-class diversity in supervised pre-training datasets of fixed size. We discovered that the optimal downstream performance is achieved through a balance of intra-/inter-class diversity. Our theory demonstrates that downstream performance depends on both diversities, and the optimal class-to-sample ratio remains constant regardless of the dataset size. We apply this finding to predict the optimal number of classes in pre-training datasets and provide evidence of its effectiveness across many scenarios.

Figure 3: Each bar plot visualizes the error rates of different methods on a specific downstream task. The number in parentheses is \(N\), i.e. the size of the corresponding pre-training dataset.

## References

* [1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for few-shot learning. _Advances in neural information processing systems_, 2022.
* [2] Peter L Bartlett, Dylan J Foster, and Matus J Telgarsky. Spectrally-normalized margin bounds for neural networks. _Advances in neural information processing systems_, 30, 2017.
* [3] Shuxiao Chen, Koby Crammer, Hangfeng He, Dan Roth, and Weijie J Su. Weighted training for cross-task learning. In _International Conference on Learning Representations_.
* [4] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A Large-Scale Hierarchical Image Database. In _CVPR09_, 2009.
* [5] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. _arXiv preprint arXiv:2010.11929_, 2020.
* [6] Simon Shaolei Du, Wei Hu, Sham M. Kakade, Jason D. Lee, and Qi Lei. Few-shot learning via learning the representation, provably. In _International Conference on Learning Representations_, 2021.
* [7] Rahim Entezari, Mitchell Wortsman, Olga Saukh, M. Moein Shariatnia, Hanie Sedghi, and Ludwig Schmidt. The role of pre-training data in transfer learning, 2023.
* [8] Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, et al. Datacomp: In search of the next generation of multimodal datasets. _arXiv preprint arXiv:2304.14108_, 2023.
* [9] Tatsunori Hashimoto. Model performance scaling with multiple data sources. In _International Conference on Machine Learning_, 2021.
* [10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 770-778, 2016.
* [11] Danny Hernandez, Jared Kaplan, T. J. Henighan, and Sam McCandlish. Scaling laws for transfer. _ArXiv_, abs/2102.01293, 2021.
* [12] Guan Zhe Hong, Yin Cui, Ariel Fuxman, Stanley H Chan, and Enming Luo. Towards understanding the effect of pretraining label granularity. _arXiv preprint arXiv:2303.16887_, 2023.
* [13] Minyoung Huh, Pulkit Agrawal, and Alexei A. Efros. What makes imagenet good for transfer learning? _ArXiv_, abs/1608.08614, 2016.
* [14] Saachi Jain, Hadi Salman, Alaa Khaddaj, Eric Wong, Sung Min Park, and Aleksander Madry. A data-based perspective on transfer learning. _2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, 2023.
* [15] Aditya Khosla, Nityananda Jayadevaprakash, Bangpeng Yao, and Fei-Fei Li. Novel dataset for fine-grained image categorization: Stanford dogs. In _Proc. CVPR workshop on fine-grained visual categorization (FGVC)_, volume 2. Citeseer, 2011.
* [16] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.
* ECCV 2018
- 15th European Conference, Munich, Germany, September 8-14, 2018, Proceedings, Part II_, volume 11206 of _Lecture Notes in Computer Science_, pages 185-201. Springer, 2018.
* [18] S. Maji, J. Kannala, E. Rahtu, M. Blaschko, and A. Vedaldi. Fine-grained visual classification of aircraft. Technical report, 2013.
* [19] Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. _Foundations of machine learning_. MIT press, 2018.
* [20] Maria-Elena Nilsback and Andrew Zisserman. Automated flower classification over a large number of classes. In _2008 Sixth Indian Conference on Computer Vision, Graphics & Image Processing_, pages 722-729. IEEE, 2008.
* [21] Xingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate Saenko, and Bo Wang. Moment matching for multi-source domain adaptation. In _Proceedings of the IEEE/CVF international conference on computer vision_, pages 1406-1415, 2019.
* [22] Ariadna Quattoni and Antonio Torralba. Recognizing indoor scenes. In _2009 IEEE conference on computer vision and pattern recognition_, pages 413-420. IEEE, 2009.
* [23] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In _International Conference on Machine Learning_, pages 8748-8763. PMLR, 2021.
* [24] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet Large Scale Visual Recognition Challenge. _International Journal of Computer Vision (IJCV)_, 115(3):211-252, 2015.
* [25] Bangpeng Yao, Xiaoye Jiang, Aditya Khosla, Andy Lai Lin, Leonidas Guibas, and Li Fei-Fei. Human action recognition by learning bases of action attributes and parts. In _2011 International conference on computer vision_, pages 1331-1338. IEEE, 2011.
* [26] Yulai Zhao, Jianshu Chen, and Simon S Du. Blessing of class diversity in pre-training. _International Conference on Artificial Intelligence and Statistics_, 2022.
* [27] Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva, and Antonio Torralba. Places: A 10 million image database for scene recognition. _IEEE transactions on pattern analysis and machine intelligence_, 40(6):1452-1464, 2017.

Appendix

### Limitation and Potential Negative Social Impact

Some potential negative societal impacts might arise from this research, such as:

**Bias Amplification**: When the research discusses the optimal class-to-sample ratio in pre-training datasets, it does not discuss how these classes are determined. There's a risk that the choice of classes and the samples within these classes could reflect and perpetuate existing biases in society. For instance, if the classes are determined by stereotypical or biased criteria, models trained on these datasets could amplify these biases in their predictions or recommendations.

**Overemphasis on Quantity over Quality**: This research might also create an overemphasis on the quantity (size and diversity) of the data at the expense of its quality. Poor data quality could lead to the development of inaccurate or unreliable machine learning models.

### Experimental Details

#### a.2.1 Training Details

We build our code on Python and Pytorch. We fix the model to be the ResNet-18 [10]. For pre-training, we set the number of epochs to be 100 and the batch size to be 64. We use the Adam optimizer for training with a learning rate of 0.1, a momentum of 0.9, and a weight decay of 1e-4. We repeat each experiment 3 times with different seeds and report the mean and variance of the results. All experiments ran on a machine with an Intel(R) Xeon(R) CPU E5-2678 v3 with 512G memory and two 48G NVIDIA RTX A6000 GPUs.

#### a.2.2 Details of Dataset

**The Pre-training Dataset**:

* **ImageNet**[4]. It is an image dataset organized according to the WordNet hierarchy. Each meaningful concept in WordNet, possibly described by multiple words or word phrases, is called a "synonym set" or "synset". There are more than 100,000 synsets in WordNet; the majority of them are nouns (80,000+).
* **Place365**[25]. It has 1,803,460 training images with the image number per class varying from 3,068 to 5,000. The validation set has 50 images per class and the test set has 900 images per class.

**The Downstream Tasks Dataset**

* **Stanford Actions 40**[25]. It contains images of humans performing 40 actions. There are about 180-300 images per class. We do not use bounding boxes and other annotation information for training. There are a total of 9,532 images, making it the smallest dataset in our benchmark experiments.
* **Stanford Dogs 120**[15]. It contains images of 120 breeds of dogs worldwide. There are precisely 100 examples per category in the training set. It is used for the task of fine-grained image categorization. We do not use the bounding box annotations. There are a total of 20,580 images.
* **MIT Indoors 67**[22]. It is a scene classification dataset containing 67 indoor scene categories, each consisting of 80 images for training and 20 for testing. Indoor scene recognition is challenging because spatial properties, background information, and object characters are expected to be extracted. There are 15,620 images in total.
* **CIFAR10**[16]. It is a collection of images commonly used to train machine learning and computer vision algorithms. It contains 60,000 32x32 color images in 10 different classes. The ten classes represent airplanes, cars, birds, cats, deer, dogs, frogs, horses, ships, and trucks. There are 6,000 images of each class.
* **Flowers102**[20]. It is an image classification dataset consisting of 102 flower categories. The flowers are chosen to be flowers commonly occurring in UK. Each class consists of between 40 and 258 images. The images have large scale, pose and light variations. In addition, some categories have significant variations within the category and several very similar categories.
* **FGVCAircraft**[18]. It contains 10,200 images of aircraft, with 100 images for each of 102 different aircraft model variants, most of which are airplanes. Each image's (main) aircraft is annotated with a tight bounding box and a hierarchical airplane model label. Aircraft models are organized in a four-level hierarchy.
* **DomainNet-real and DomainNet-painting**[21]. DomainNet-real and DomainNet-painting belong to real and painting domains, respectively, and both comprise 345 categories. DomainNet-real contains over 170k images, while DomainNet-painting has more than 70k images.

### Empirical Verification of Theorem 3.2

As shown in Figure 4, we can find that, across all datasets, the results generally showed a decreasing trend in test error rate as the class-to-sample ratio increased. This empirically supports the theoretical assertion that there is no downstream performance trade-off caused by the inter-class diversity \(K\) and the intra-class diversity \(n\). Instead, the downstream performance improves with increasing \(K\), as suggested in Theorem 3.2.

### Experiments on Places365 as Pre-training Dataset

We use the Places3653[27] for our pre-training and the result is represented in Figure 5. We then performed evaluations on the same batch of downstream tasks as in the main body of the paper to demonstrate the trade-off between intra- and inter-class diversity is consistent with our main findings.

Figure 4: Test error rate across class-to-sample ratio. The vertical bar at each point is the standard deviation.

### Experiments on Downstream Tasks of Different Domains

As illustrated in Figure 6, we test models trained on ImageNet on two distinct domains (painting and real images) sourced from the DomainNet benchmark4[21] and the result indicates that our conclusions do not change with different downstream task datasets.

Footnote 4: [https://github.com/facebookresearch/domainNet](https://github.com/facebookresearch/domainNet)

### Experiments on ViT as Model Backbone

To ensure our conclusions are not restricted to specific model architecture, we use ViT-B-16 model [5] as the backbone model. The results in Figure 7 show that different model backbone does not change the conclusions we derived.

Figure 5: Test error rate across class-to-sample ratio. ResNet-18 pre-trained on the Places365 dataset.

Figure 6: Test error rate across class-to-sample ratio. ResNet-18 pre-trained on ImageNet and tested on two datasets of different domains from the DomainNet benchmark.

### Actual v.s. Predicted Performance of Fitting Equation 4

In Section 4.2, we study the performance of the Fitting method, _i.e._, using the optimal number of classes derived from the function fitting Equation 4. Given the fitted function, one can also predict the test error given a specific value of the number of classes. Here, we compare the actual test error of the derived number of classes (the Fitting entry of Table 1) with the one predicted by the fitted function, in order to see whether our theory offers an accurate prediction of performance. From the results presented in Table 3, we can see that the actual test error (Actual) is similar to the test error predicted by the fitted function (Predicted), this further proves the utility of our theory.

### Proofs of Theoretical Results

#### a.8.1 Proof of Theorem 3.1

Before we formally state the proof of Theorem 3.1, we define several notations as follows:

\[h_{\mathcal{P}}\triangleq\arg\min_{h\in\mathcal{H}}\mathcal{R}_{ p}(h,\mathcal{P}),\] \[f_{\mathcal{P}}\triangleq\arg\min_{f}\mathbb{E}_{i\sim\mathrm{ Unif}[K],x\sim\mathcal{P}_{i}}[\ell(f\circ h_{\mathcal{P}}(x),i)].\]

Proof of Theorem 3.1.: **Step I. Bound \(\mathcal{E}_{p}(h_{S^{p}},\mathcal{P})\).**

\begin{table}
\begin{tabular}{c|c|c c c c c c} \hline \hline \multirow{2}{*}{\(N\)} & \multirow{2}{*}{**Method**} & \multicolumn{6}{c}{**Target Dataset**} \\ \cline{3-8}  & & **CIFAR10** & **FGVCaircraft** & **Flowers102** & **MIT67** & **Stanford40** & **StanfordDogs** \\ \hline \multirow{3}{*}{50K} & Predicted & 26.13\(\pm\)0.1 & 75.95\(\pm\)0.4 & 32.96\(\pm\)0.48 & 53.99\(\pm\)0.43 & 56.72\(\pm\)0.16 & 58.87\(\pm\)0.35 \\ \cline{2-8}  & Actual & 26.25\(\pm\)0.47 & 76.00\(\pm\)0.44 & 32.13\(\pm\)0.10 & 53.60\(\pm\)0.39 & 57.

\(\mathcal{E}_{p}(h_{S^{p}},\mathcal{P})\) can be decomposed into

\[\mathcal{E}_{p}(h_{S^{p}},\mathcal{P})= \mathcal{R}_{p}(h_{S^{p}},\mathcal{P})-\min_{\hat{h}\in\mathcal{H} }\mathcal{R}_{p}(\tilde{h},\mathcal{P})\] \[=\left(\mathcal{R}_{p}(h_{S^{p}},\mathcal{P})-\frac{1}{nK}\sum_{i =1}^{K}\sum_{j=1}^{n}\ell(f_{S^{p}}\circ h_{S^{p}}(x_{i,j}),i)\right)\] \[+\left(\frac{1}{nK}\sum_{i=1}^{K}\sum_{j=1}^{n}\ell(f_{S^{p}} \circ h_{S^{p}}(x_{i,j}),i)-\frac{1}{nK}\sum_{i=1}^{K}\sum_{j=1}^{n}\left(\ell( f_{\mathcal{P}}\circ h_{\mathcal{P}}(x_{i,j}),i)\right)\right)\] \[+\left(\frac{1}{nK}\sum_{i=1}^{K}\sum_{j=1}^{n}\left(\ell(f_{ \mathcal{P}}\circ h_{\mathcal{P}}(x_{i,j}),i)\right)-\mathcal{R}_{p}(h_{ \mathcal{P}},\mathcal{P})\right).\]

We tackle the three terms of the RHS of the above inequality respectively. As for the first term, since

\[\mathcal{R}_{p}(h_{S^{p}},\mathcal{P})= \min_{f\in\mathcal{F}}\mathbb{E}_{i\sim\text{Unif}[K],x\sim\mathcal{ P}_{i}}[\ell(f\circ h_{S^{p}}(x),i)]\] \[\leq \mathbb{E}_{i\sim\text{Unif}[K],x\sim\mathcal{P}_{i}}[\ell(f_{S^ {p}}\circ h_{S^{p}}(x),i)],\]

we have that the first term can be bounded by

\[\mathcal{R}_{p}(h_{S^{p}},\mathcal{P})-\frac{1}{nK}\sum_{i=1}^{K} \sum_{j=1}^{n}\ell(f_{S^{p}}\circ h_{S^{p}}(x_{i,j}),i)\] \[\leq \mathbb{E}_{i\sim\text{Unif}[K],x\sim\mathcal{P}_{i}}[\ell(f_{S^ {p}}\circ h_{S^{p}}(x),i)]-\frac{1}{nK}\sum_{i=1}^{K}\sum_{j=1}^{n}\ell(f_{S^ {p}}\circ h_{S^{p}}(x_{i,j}),i)\] \[\leq \sup_{f\in\mathcal{F},h\in\mathcal{H}}\left[\mathbb{E}_{i\sim\text {Unif}[K],x\sim\mathcal{P}_{i}}[\ell(f\circ h(x),i)]-\frac{1}{nK}\sum_{i=1}^{ K}\sum_{j=1}^{n}\ell(f\circ h(x_{i,j}),i)\right].\]

Applying Gaussian complexity to \(\frac{1}{n}\sum_{j=1}^{n}(\frac{1}{K}\sum_{i=1}^{K}\ell(f\circ h(x_{i,j}),i))\), we obtain that with probability at least \(1-\delta\), the RHS of the above inequality is smaller than

\[M_{\ell}\sqrt{\frac{9\log\frac{1}{\delta}}{2n}}+2\mathbb{E}_{((x _{i,j})_{i=1}^{K})_{j=1}^{n}\sim(\Pi_{i=1}^{K}\mathcal{P}_{i}^{n})}\mathbb{E}_ {(\sigma_{i})_{i=1}^{n}\sim\mathcal{N}(0,\mathbf{1}_{n\times n})}\sup_{f\in \mathcal{F},h\in\mathcal{H}}\frac{1}{n}\sum_{j=1}^{n}\sigma_{j}\left(\frac{1} {K}\sum_{i=1}^{K}\ell(f\circ h(x_{i,j}),i)\right)\] \[\leq M_{\ell}\sqrt{\frac{9\log\frac{1}{\delta}}{2n}}+\frac{2}{K}\sum_{ i=1}^{K}\mathbb{E}_{(x_{i,j})_{j=1}^{n}\sim\mathcal{P}_{i}^{n}}\mathbb{E}_{( \sigma_{i})_{i=1}^{n}\sim\mathcal{N}(0,\mathbf{1}_{n\times n})}\sup_{f\in \mathcal{F},h\in\mathcal{H}}\frac{1}{n}\sum_{j=1}^{n}\sigma_{j}\ell(f\circ h (x_{i,j}),i)\] \[\leq M_{\ell}\sqrt{\frac{9\log\frac{1}{\delta}}{2n}}+\frac{2\sqrt{2}}{K} \sum_{i=1}^{K}\mathbb{E}_{(x_{i,j})_{j=1}^{n}\sim\mathcal{P}_{i}^{n}}\mathbb{E }_{(\sigma_{j,i})_{j\in[n],i\in[K]}\sim\mathcal{N}(0,\mathbf{1}_{n\times K})} \sup_{f\in\mathcal{F},h\in\mathcal{H}}\frac{1}{n}\sum_{j=1}^{n}\sum_{l=1}^{K} \sigma_{j,l}f_{l}\circ h(x_{i,j})\] \[\leq M_{\ell}\sqrt{\frac{9\log\frac{1}{\delta}}{2n}}+2G\sqrt{2}\frac{1 }{\sqrt{n}}.\]

Here the second inequality is due to Slepian's Lemma, and the last inequality is due to Assumption 1.

All in all, with probability at least \(1-\delta\), the first term can be bounded as

\[\mathcal{R}_{p}(h_{S^{p}},\mathcal{P})-\frac{1}{nK}\sum_{i=1}^{K}\sum_{j=1}^{n }\ell(f_{S^{p}}\circ h_{S^{p}}(x_{i,j}),i)\leq M_{\ell}\sqrt{\frac{9\log\frac{1 }{\delta}}{2n}}+2G\sqrt{2}\frac{1}{\sqrt{n}}.\]

Meanwhile, the second term is non-positive due to the optimality of \(f_{S^{p}}\) and \(h_{S^{p}}\).

Finally, the third term can be bounded as

\[\left(\frac{1}{nK}\sum_{i=1}^{K}\sum_{j=1}^{n}\left(\ell(f_{\mathcal{ P}}\circ h_{\mathcal{P}}(x_{i,j}),i)\right)-\mathcal{R}_{p}(h_{\mathcal{P}}, \mathcal{P})\right)\] \[=\left(\frac{1}{nK}\sum_{i=1}^{K}\sum_{j=1}^{n}\left(\ell(f_{ \mathcal{P}}\circ h_{\mathcal{P}}(x_{i,j}),i)\right)-\mathbb{E}_{i\sim\mathrm{ Unif}[K],x\sim\mathcal{P}_{i}}[\ell(f_{\mathcal{P}}\circ h_{\mathcal{P}}(x),i)]\right)\] \[\stackrel{{ w.p.1-\delta}}{{\leq}}M_{\ell}\sqrt{\frac {2\log\frac{1}{\delta}}{n}},\]

where the last inequality is due to Hoeffding's inequality. As a conclusion of Stage I, we obtain that with probability at least \(1-2\delta\),

\[\mathcal{E}_{p}(h_{S^{p}},\mathcal{P})\leq 5M_{\ell}\sqrt{\frac{\log\frac{1}{ \delta}}{2n}}+\frac{2G\sqrt{2}}{\sqrt{n}}.\]

**Step II. Bound \(\mathcal{E}_{d}(h_{S^{p}})\).** Applying Assumption 2, we obtain that with probability at least \(1-2\delta\),

\[\mathcal{E}_{d}(h_{S^{p}},\tilde{\mathcal{P}})\leq\nu_{1}^{\tilde{ \mathcal{P}}}(\mathcal{P})\mathcal{E}_{p}(h_{S^{p}},\mathcal{P}_{i_{1}}, \mathcal{P}_{i_{2}})+\nu_{0}^{\tilde{\mathcal{P}}}(\mathcal{P})\leq\nu_{1}^{ \tilde{\mathcal{P}}}(\mathcal{P})\left(5M_{\ell}\sqrt{\frac{\log\frac{1}{ \delta}}{2n}}+\frac{2G\sqrt{2}}{\sqrt{n}}\right)+\nu_{0}^{\tilde{\mathcal{P}}} (\mathcal{P}).\]

By McDiarmid's inequality, we obtain that with probability at least \(1-\delta\),

\[\nu_{0}^{\tilde{\mathcal{P}}}(\mathcal{P})\leq\mathbb{E}_{\mathcal{P}\sim \mathcal{D}^{K}}\nu_{0}^{\tilde{\mathcal{P}}}(\mathcal{P})+M_{0}\sqrt{\frac{ \log\frac{1}{\delta}}{2K}}\leq\nu_{0}^{\tilde{\mathcal{P}}}(\mathcal{D})+M_{0} \sqrt{\frac{\log\frac{1}{\delta}}{2K}}+\frac{C_{0}}{\sqrt{K}}.\]

Similarly, with probability at least \(1-\delta\),

\[\nu_{1}^{\tilde{\mathcal{P}}}(\mathcal{P})\leq\mathbb{E}_{\mathcal{P}\sim \mathcal{D}^{K}}\nu_{1}^{\tilde{\mathcal{P}}}(\mathcal{P})+M_{1}\sqrt{\frac{ \log\frac{1}{\delta}}{2K}}\leq\nu_{1}^{\tilde{\mathcal{P}}}(\mathcal{D})+M_{1} \sqrt{\frac{\log\frac{1}{\delta}}{2K}}+\frac{C_{1}}{\sqrt{K}}.\]

As a conclusion, we obtain that with probability at least \(1-4\delta\),

\[\mathcal{E}_{d}(h_{S^{p}},\tilde{\mathcal{P}})\] \[\leq \left(\nu_{1}^{\tilde{\mathcal{P}}}(\mathcal{D})+M_{1}\sqrt{\frac {\log\frac{1}{\delta}}{2K}}+\frac{C_{1}}{\sqrt{K}}\right)\left(5M_{\ell}\sqrt {\frac{\log\frac{1}{\delta}}{2n}}+\frac{2G\sqrt{2}}{\sqrt{n}}\right)+\nu_{0}^{ \tilde{\mathcal{P}}}(\mathcal{D})+M_{0}\sqrt{\frac{\log\frac{1}{\delta}}{2K}} +\frac{C_{0}}{\sqrt{K}}.\]

**Step III. Bound \(\bar{\mathcal{R}}_{d}(f_{S^{d}}\circ h_{S_{p}},S_{p})-\mathcal{R}_{d}(h_{S_{p }},\tilde{\mathcal{P}})\)** Finally, denote \(f_{\tilde{\mathcal{P}}}\triangleq\arg\min_{\tilde{f}}\min_{\tilde{f}\in \tilde{\mathcal{F}}}\mathcal{R}_{d}(\tilde{f}\circ h,\tilde{\mathcal{P}})\) based on Hoeffding's inequality, we obtain that with probability at least \(1-\delta\),

\[\bar{\mathcal{R}}_{d}(f_{S^{d}}\circ h_{S_{p}},S_{p})-\mathcal{R}_ {d}(h_{S_{p}},\tilde{\mathcal{P}})\leq \mathcal{R}_{d}(f_{\tilde{\mathcal{P}}}\circ h_{S_{p}},\tilde{ \mathcal{P}})-\mathcal{R}_{d}(h_{S_{p}},\tilde{\mathcal{P}})\] \[\leq M_{\ell}\sqrt{\frac{2\log\frac{1}{\delta}}{\tilde{N}}}.\]

Meanwhile, applying Gaussian's inequality, we have that with probability at least \(1-\delta\),

\[\mathcal{R}_{d}(f_{S^{d}}\circ h_{S_{p}},\tilde{\mathcal{P}})- \bar{\mathcal{R}}_{d}(f_{S^{d}}\circ h_{S_{p}},S_{p})\] \[\leq 2\mathbb{E}_{(x_{i},y_{i})_{i=1}^{\tilde{N}}\sim\beta^{\tilde{ N}}}\mathbb{E}_{(\sigma_{i})_{i=1}^{\tilde{N}}\sim\mathcal{N}(0,1_{\tilde{N}\times \tilde{N}})}\sup_{f\in\tilde{\mathcal{F}},h\in\mathcal{H}}\frac{1}{\tilde{N}} \sum_{i=1}^{\tilde{N}}\sigma_{i}\ell(f\circ h(x_{i}),y_{i})+M_{\ell}\sqrt{ \frac{9\log\frac{1}{\delta}}{2\tilde{N}}}\] \[\leq 2\sqrt{2}\mathbb{E}_{(x_{i},y_{i})_{i=1}^{\tilde{N}}\sim\beta^{ \tilde{N}}}\mathbb{E}_{(\sigma_{i})_{i\in[\tilde{N}],j\in[\tilde{K}]}\sim \mathcal{N}(0,1_{\tilde{N}\tilde{K}\times\tilde{N}\tilde{K}})}\sup_{f\in\tilde {\mathcal{F}},h\in\mathcal{H}}\frac{1}{\tilde{N}}\sum_{i=1}^{\tilde{N}}\sum_{j =1}^{\tilde{K}}\sigma_{i,j}f_{j}\circ h(x_{i})+M_{\ell}\sqrt{\frac{9\log \frac{1}{\delta}}{2\tilde{N}}}\] \[\leq 2G\sqrt{2}\frac{1}{\sqrt{\tilde{N}}}+M_{\ell}\sqrt{\frac{9\log \frac{1}{\delta}}{2\tilde{N}}},\]where the second inequality is due to Slepian's Lemma, and the last inequality is due to Assumption 1.

All in all, we have with probability at least \(1-6\delta\),

\[\mathcal{E}_{d}(f_{S^{d}}\circ h_{S_{p}},\tilde{\mathcal{P}})\] \[= \mathcal{E}_{d}(h_{S^{p}},\tilde{\mathcal{P}})+\mathcal{R}_{d}(f_{ S^{d}}\circ h_{S_{p}},\tilde{\mathcal{P}})-\bar{\mathcal{R}}_{d}(f_{S^{d}}\circ h_{S_{p }},S_{p})+\bar{\mathcal{R}}_{d}(f_{S^{d}}\circ h_{S_{p}},S_{p})-\mathcal{R}_{d} (h_{S_{p}},\tilde{\mathcal{P}})\] \[\leq \left(\nu_{1}^{\tilde{\mathcal{P}}}(\mathcal{D})+M_{1}\sqrt{\frac {\log\frac{1}{\delta}}{2K}}+\frac{C_{1}}{\sqrt{K}}\right)\left(5M_{\ell}\sqrt{ \frac{\log\frac{1}{\delta}}{2n}}+\frac{2G\sqrt{2}}{\sqrt{n}}\right)+\nu_{0}^{ \tilde{\mathcal{P}}}(\mathcal{D})+M_{0}\sqrt{\frac{\log\frac{1}{\delta}}{2K}}+ \frac{C_{0}}{\sqrt{K}}\] \[+5M_{\ell}\sqrt{\frac{\log\frac{1}{\delta}}{2\tilde{N}}}+2\sqrt{ 2}G\frac{1}{\sqrt{\tilde{N}}}.\]

The proof is completed. 

#### a.8.2 Proof of Theorem 3.2

Proof.: Denote

\[h_{\mathcal{P}}\triangleq\arg\min_{h\in\mathcal{H}}\mathcal{R}_{ p}(h),\] \[f_{\mathcal{P}}\triangleq\arg\min_{f}\mathbb{E}_{(x,y)\sim\mathcal{ P}}[\ell(f\circ h_{\mathcal{P}}(x),y)].\]

Step I. Bound \(\mathcal{E}_{p}(h_{S^{p}})\).

Denote \(\mathcal{E}_{p}(h_{S^{p}})\) can be decomposed into

\[\mathcal{E}_{p}(h_{S^{p}})= \mathcal{R}_{p}(h_{S^{p}})-\min_{h\in\mathcal{H}}\mathcal{R}_{p}( \tilde{h})\] \[= \left(\mathcal{R}_{p}(h_{S^{p}})-\frac{1}{N}\sum_{i=1}^{N}\ell(f_{ S^{p}}\circ h_{S^{p}}(x_{i}),y_{i})\right)\] \[+ \left(\frac{1}{N}\sum_{i=1}^{N}\ell(f_{S^{p}}\circ h_{S^{p}}(x_{i }),y_{i})-\frac{1}{N}\sum_{i=1}^{N}\left(\ell(f_{\mathcal{P}}\circ h_{ \mathcal{P}}(x_{i}),y_{i})\right)\right.\] \[+ \left(\frac{1}{N}\sum_{i=1}^{N}\left(\ell(f_{\mathcal{P}}\circ h _{\mathcal{P}}(x_{i}),y_{i})\right)-\mathcal{R}_{p}(h_{\mathcal{P}})\right).\]

We tackle the three terms of the RHS of the above inequality respectively. As for the first term, since

\[\mathcal{R}_{p}(h_{S^{p}})=\min_{f\in\mathcal{F}}\mathbb{E}_{(x,y )\sim\mathcal{P}}[\ell(f\circ h_{S^{p}}(x),y)]\leq\mathbb{E}_{(x,y)\sim \mathcal{P}}[\ell(f_{S^{p}}\circ h_{S^{p}}(x),y)],\]

we have that the first term can be bounded by

\[\mathcal{R}_{p}(h_{S^{p}})-\frac{1}{N}\sum_{i=1}^{N}\ell(f_{S^{p} }\circ h_{S^{p}}(x_{i}),y_{i})\] \[\leq \mathbb{E}_{(x,y)\sim\mathcal{P}}[\ell(f_{S^{p}}\circ h_{S^{p}}( x),i)]-\frac{1}{N}\sum_{i=1}^{N}\ell(f_{S^{p}}\circ h_{S^{p}}(x_{i}),y_{i})\] \[\leq \sup_{f\in\mathcal{F},h\in\mathcal{H}}\left[\mathbb{E}_{(x,y)\sim \mathcal{P}}[\ell(f\circ h(x),i)]-\frac{1}{N}\sum_{i=1}^{N}\ell(f\circ h(x_{ i}),y_{i})\right].\]Applying Gaussian complexity to \(\frac{1}{N}\sum_{i=1}^{N}\ell(f\circ h(x_{i}),y_{i})\), we obtain that with probability at least \(1-\delta\), the RHS of the above inequality is smaller than

\[M_{\ell}\sqrt{\frac{9\log\frac{1}{\delta}}{2N}}+2\mathbb{E}_{(x_{ i})_{i=1}^{N}\sim P^{N}}\mathbb{E}_{(\sigma_{i})_{i=1}^{N}\sim\mathcal{N}(0, \mathds{1}_{N})}\sup_{f\in\mathcal{F},h\in\mathcal{H}}\frac{1}{N}\sum_{i=1}^{n }\sigma_{i}\ell(f\circ h(x_{i}),y_{i})\] \[\leq M_{\ell}\sqrt{\frac{9\log\frac{1}{\delta}}{2N}}+2\sqrt{2}\mathbb{ E}_{(x_{i})_{i=1}^{N}\sim P^{N}}\mathbb{E}_{(\sigma_{i,j})_{i\in[N],j\in[\dim( \mathcal{Y})]}\sim\mathcal{N}(0,\mathds{1}_{N\text{\tiny dim}(\mathcal{Y})})} \sup_{f\in\mathcal{F},h\in\mathcal{H}}\frac{1}{N}\sum_{i=1}^{n}\sum_{j=1}^{K} \sigma_{i,j}f_{j}\circ h(x_{i})\] \[\leq M_{\ell}\sqrt{\frac{9\log\frac{1}{\delta}}{2N}}+2G\sqrt{2}\frac{ 1}{\sqrt{N}}.\]

Here the first inequality is due to Slepian's lemma, and the last inequality is due to Assumption 1. All in all, with probability at least \(1-\delta\), the first term can be bounded as

\[\mathcal{R}_{p}(h_{S^{p}})-\frac{1}{N}\sum_{i=1}^{N}\ell(f_{S^{p} }\circ h_{S^{p}}(x_{i}),y_{i})\leq M_{\ell}\sqrt{\frac{9\log\frac{1}{\delta}}{ 2N}}+2G\sqrt{2}\frac{1}{\sqrt{N}}.\]

Meanwhile, the second term is non-positive due to the optimality of \(f_{S^{p}}\) and \(h_{S^{p}}\).

Finally, the third term can be bounded as

\[\frac{1}{N}\sum_{i=1}^{N}\ell(f_{\mathcal{P}}\circ h_{\mathcal{P} }(x_{i}),y_{i})-\mathcal{R}_{p}(h_{\mathcal{P}})\] \[= \frac{1}{N}\sum_{i=1}^{N}\ell(f_{\mathcal{P}}\circ h_{\mathcal{P} }(x_{i}),y_{i})-\mathbb{E}_{(x,y)\sim\mathcal{P}}[\ell(f_{\mathcal{P}}\circ h _{\mathcal{P}}(x),y)]\] \[\stackrel{{ w.p.1-\delta}}{{\leq}} M_{\ell}\sqrt{\frac{2\log\frac{1}{\delta}}{N}},\]

where the last inequality is due to Hoeffding's inequality. As a conclusion of Stage I, we obtain that with probability at least \(1-2\delta\),

\[\mathcal{E}_{p}(h_{S^{p}},\mathcal{P})\leq 5M_{\ell}\sqrt{\frac{\log\frac{1}{ \delta}}{2N}}+\frac{2G\sqrt{2}}{\sqrt{N}}.\]

**Step II. Bound \(\mathcal{E}_{d}(h_{S^{p}})\).** Applying Assumption 3, we obtain that with probability at least \(1-2\delta\),

\[\mathcal{E}_{d}(h_{S^{p}},\tilde{\mathcal{P}})\leq\nu_{1}^{\tilde{ \mathcal{P}}}(\mathcal{P})\mathcal{E}_{p}(h_{S^{p}})+\nu_{0}^{\tilde{\mathcal{ P}}}(\mathcal{P})\leq\nu_{1}^{\tilde{\mathcal{P}}}(\mathcal{P})\left(5M_{ \ell}\sqrt{\frac{\log\frac{1}{\delta}}{2N}}+\frac{2G\sqrt{2}}{\sqrt{N}}\right) +\nu_{0}^{\tilde{\mathcal{P}}}(\mathcal{P}).\]

Furthermore, as \(|\nu_{0}^{\tilde{\mathcal{P}}}(\mathcal{P})-\nu_{0}^{\tilde{\mathcal{P}}}( \mathcal{P}_{\mathcal{X}})|\leq\frac{C_{0}^{\tilde{\mathcal{P}}}}{\sqrt{K}}\) and \(|\nu_{1}^{\tilde{\mathcal{P}}}(\mathcal{P})-\nu_{1}^{\tilde{\mathcal{P}}}( \mathcal{P}_{\mathcal{X}})|\leq\frac{C_{1}^{\tilde{\mathcal{P}}}}{\sqrt{K}}\), we obtain that with probability at least \(1-2\delta\)

\[\mathcal{E}_{d}(h_{S^{p}},\tilde{\mathcal{P}})\leq\left(\nu_{0}^{\tilde{ \mathcal{P}}}(\mathcal{P}_{\mathcal{X}})+\frac{C_{0}^{\tilde{\mathcal{P}}}}{ \sqrt{K}}\right)\left(5M_{\ell}\sqrt{\frac{\log\frac{2}{\delta}}{2N}}+\frac{2 G\sqrt{2}}{\sqrt{N}}\right)+\nu_{1}^{\tilde{\mathcal{P}}}(\mathcal{P}_{ \mathcal{X}})+\frac{C_{1}^{\tilde{\mathcal{P}}}}{\sqrt{K}}.\]

The rest of the proof flows exactly the same as that of Theorem 3.1.