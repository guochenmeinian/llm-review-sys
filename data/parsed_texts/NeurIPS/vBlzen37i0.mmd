[MISSING_PAGE_FAIL:1]

Let \(\mu\) be a probability measure on \(\mathcal{X}\). Given noisy training data

\[\left\{(X_{i},F(X_{i})+E_{i})\right\}_{i=1}^{m},\] (1.2)

where \(X_{1},\ldots,X_{m}\sim_{\mathrm{i.i.d.}}\mu\) and \(E_{i}\) is noise, a typical operator learning methodology consists of three objects: an approximate encoder \(\mathcal{E}_{\mathcal{X}}:\mathcal{X}\to\mathbb{R}^{d_{X}}\), an approximate decoder \(\mathcal{D}_{\mathcal{Y}}:\mathbb{R}^{d_{\mathcal{Y}}}\to\mathcal{Y}\) and a DNN \(\widehat{N}:\mathbb{R}^{d_{X}}\to\mathbb{R}^{d_{\mathcal{Y}}}\). It then approximates \(F\) as

\[F\approx\widehat{F}:=\mathcal{D}_{\mathcal{Y}}\circ\widehat{N}\circ\mathcal{E} _{\mathcal{X}}.\] (1.3)

The encoder and decoder are either specified by the problem, learned separately from data, or learned concurrently with \(\widehat{N}\). The goal, as in all supervised learning problems, is to ensure good generalization via the learned operator \(\widehat{F}\) from as little training data \(m\) as possible.

### Contributions

As noted in, e.g., [16; 66], the theory of deep operator learning is still in its infancy. We contribute to this growth in the following ways. We consider learning classes of _holomorphic_ operators (Assumption 2.2), with arbitrary approximate encoders \(\mathcal{E}_{\mathcal{X}}\) and decoders \(\mathcal{D}_{\mathcal{Y}}\). As we explain in SS2.3 (see also [52; SS5.2], [53; SS3.4] and [41]) these operators are relevant in many applications, notably those involving _parametric_ PDEs. The main contributions of this work are as follows.

1. We consider operators taking values in general Banach spaces. As noted, the vast majority of existing work (with the notable exception of [16]) considers Hilbert spaces.
2. We consider standard feedforward DNN architectures (constant width, width exceeds depth) and training procedures (\(\ell^{2}\)-loss minimization).
3. (Theorem 3.1) We construct a family of DNNs such that any approximate minimizer of the corresponding training problem satisfies a generalization bound that is explicit in the various error sources: namely, an _approximation error_, which decays algebraically in the amount of training data \(m\); _encoding-decoding errors_, which depend on the accuracy of the learned encoders and decoders; an _optimization error_, and; a _sampling error_, which depends on the noise \(E_{i}\) in (1.2).
4. These DNN architectures are _problem agnostic_; they depend on \(m\) only. In particular, the architectures are completely independent on the regularity assumptions of target operator.
5. (Theorem 3.2) We show that training problems based on _any_ family of fully-connected DNNs possess uncountably many minimizers that achieve the same generalization bounds.
6. (Theorems 3.1-3.2) We provide bounds in both the \(L^{2}_{\mu}\)- and \(L^{\infty}_{\mu}\)-norms that hold in high probability, rather than just expectation.
7. (Theorems 4.1-4.2) We show that the generalization bound is optimal with respect to \(m\): no learning procedure (not necessarily DL-based) can achieve better rates in \(m\) up to log terms.
8. Finally, we present a series of experiments demonstrating the efficacy of DL on challenging problems such as the parametric diffusion, Navier-Stokes-Brinkman and Boussinesq PDEs, the latter two of which involve operators whose codomains are Banach, as opposed to Hilbert, spaces.

### Relation to previous work

Approximating an operator between function spaces with training data obtained through numerical PDEs solves presents a formidable challenge. Nevertheless, in recent years, significant advances have been made through the development of DL techniques, leading to the field of _operator learning_[51; 40; 53; 56; 58; 60; 62; 69; 63; 103; 15]. These approaches often leverage intricate DNN architectures to approximate the complex mappings inherent in physical modelling scenarios. Many works have also focused on the practical aspects of operator learning in real-world applications [13; 24; 35; 36; 37; 42; 45; 48; 49; 59; 61; 64; 65; 70; 73; 77; 80; 81; 84; 96; 98; 99; 100; 101; 105].

On the theoretical side, universal approximation theorems for operator learning have been developed in [50; 55; 68; 69] and elsewhere. Such bounds are typically not quantitative in the size of the DNN needed to achieve a certain error. For this, one typically either restricts to specific operators (e.g., certain PDEs) or imposes regularity conditions. One such assumption is Lipschitz regularity - see [10; 16; 55; 66; 87] and references therein. However, learning Lipschitz operators suffers from a _curse of parametric complexity_[54], meaning that algebraic rates may not be achievable. Anothercommon assumption is holomorphy. While stronger, it is, as noted, very relevant to operator learning problems involving parametric PDEs. Quantitative approximation results for holomorphic operators have been shown in [26; 31; 41; 55; 71] and elsewhere.

However, these works do not consider the generalization error, i.e., the error incurred when learning the approximation (1.3) from the finite training data (1.2). This is particularly important in applications of operator learning where data is obtained through expensive numerical PDE solves, since such problems are highly _data-starved_. Several works have tackled this question from the perspective of statistical learning theory and nonparametric estimation [16; 55; 66], but only for Lipschitz operators. As observed in [6; SS9.5], this approach generally leads to a best \(\mathcal{O}(m^{-1/2})\) decay of the \(L^{2}_{\mu}\)-norm error with respect to \(m\). Theorem 4.1 shows that such a rate is strictly suboptimal for learning the classes of holomorphic operators we consider. Our generalization bounds in Theorems 3.1-3.2 do not use such techniques, and yield near-optimal rates in both the \(L^{2}_{\mu}\)_- and \(L^{\infty}_{\mu}\)-norms. See also [30] for some related work in this direction for reduced-order modelling with convolutional autoencoders.

Our work is inspired by recent research on learning holomorphic, Banach-valued functions [2; 5; 6]. We extend both these works, in particular, [5], to learning holomorphic operators. We also significantly improve the error decay rates in [5] with respect to \(m\) and show they can be achieved using substantially smaller DNNs with standard training (i.e., \(\ell^{2}\)-loss minimization). See Remarks C.1-C.2. Our theoretical guarantees fall into the category of _encoder-decoder-nets_[52], which includes the well-known _PCA-Net_[10] and _DeepONet_[68] frameworks. As in other recent works [16; 30; 55; 66], in Theorems 3.1-3.2 we assume the encoder-decoder pair \((\mathcal{E}_{\mathcal{X}},\mathcal{D}_{\mathcal{Y}})\) in (1.3) have been learned, and focus on the generalization error when training the DNN \(\widehat{N}\).

## 2 Notation, assumptions, setup and examples

### Notation

Let \((\mathcal{X},\|\cdot\|_{\mathcal{X}})\) and \((\mathcal{Y},\|\cdot\|_{\mathcal{Y}})\) be Banach spaces and \(\mu\) be a probability measure on \(\mathcal{X}\). Let \((\mathcal{Y}^{*},\|\cdot\|_{\mathcal{Y}^{*}})\) be the dual of \(\mathcal{Y}\) and \(B(\mathcal{Y}^{*})\) be its unit ball. The _Bochner_ and _Pettis_\(L^{p}\)-norms of a (strongly and weakly, respectively) measurable operator \(F:\mathcal{X}\to\mathcal{Y}\) are defined as

\[\|F\|_{L^{p}_{\mu}(\mathcal{X};\mathcal{Y})} =\left(\int_{\mathcal{X}}\|F(X)\|^{p}_{\mathcal{Y}}\,\mathrm{d} \mu(X)\right)^{1/p}\] \[\|F\|_{L^{p}_{\mu}(\mathcal{X};\mathcal{Y})} =\sup_{y^{*}\in B(\mathcal{Y}^{*})}\left(\int_{\mathcal{X}}|y^{* }(F)|^{p}\,\mathrm{d}\mu(X)\right)^{1/p},\]

respectively, for \(1\leq p<\infty\), and analogously for \(p=\infty\) (see, e.g., [8; 44]). Notice that \(\|\!\|F\|_{L^{p}_{\mu}(\mathcal{X};\mathcal{Y})}\leq\|F\|_{L^{p}_{\mu}( \mathcal{X};\mathcal{Y})}\) for \(1\leq p<\infty\), while \(\|\!\|F\|_{L^{\infty}_{\mu}(\mathcal{X};\mathcal{Y})}=\|F\|_{L^{\infty}_{\mu} (\mathcal{X};\mathcal{Y})}\).

Throughout this work, \(\ell^{p}(\mathbb{N})\), \(0<p\leq\infty\) denotes the standard \(\ell^{p}\) space with (quasi-)norm \(\|\cdot\|_{p}\). We also define the _monotone \(\ell^{p}\) space_\(\ell^{p}_{\mathcal{M}}(\mathbb{N})\) as the space of all sequences \(\bm{z}=(z_{i})_{i=1}^{\infty}\in\mathbb{R}^{\mathbb{N}}\) whose minimal monotone majorant \(\tilde{\bm{z}}\in\ell^{p}(\mathbb{N})\). Here \(\bm{z}=(\tilde{z}_{i})_{i=1}^{\infty}\) is defined as \(\tilde{z}_{i}=\sup_{j\geq i}|z_{j}|\).

Given a (componentwise) activation function \(\sigma\), we consider feedforward DNNs of the form

\[N:\mathbb{R}^{n}\to\mathbb{R}^{k},\ \bm{z}\mapsto N(\bm{z})=\mathcal{A}_{L+1}( \sigma(\mathcal{A}_{L}(\sigma(\cdots\sigma(\mathcal{A}_{0}(\bm{z})))\cdots))),\] (2.1)

where \(\mathcal{A}_{l}:\mathbb{R}^{N_{l}}\to\mathbb{R}^{N_{l+1}}\) are affine maps, and \(N_{0}=n\) and \(N_{L+2}=k\). We define \(\mathrm{width}(N)=\max\{N_{1},\ldots,N_{L+1}\}\) and \(\mathrm{depth}(N)=L\). We denote a class of DNNs of the form (2.1) with a fixed architecture (i.e., fixed activation function, depth and widths) as \(\mathcal{N}\), and write \(\mathrm{width}(\mathcal{N})=\max\{N_{1},\ldots,N_{L+1}\}\) and \(\mathrm{depth}(\mathcal{N})=L\).

### Assumptions and setup

Let \(F:\mathcal{X}\to\mathcal{Y}\) be the unknown operator we seek to learn and

\[\widetilde{\mathcal{E}}_{\mathcal{X}}:\mathcal{X}\to\mathbb{R}^{d_{X}},\ \widetilde{\mathcal{D}}_{\mathcal{X}}:\mathbb{R}^{d_{X}}\to\mathcal{X},\qquad \mathcal{E}_{\mathcal{Y}}:\mathcal{Y}\to\mathbb{R}^{d_{\mathcal{Y}}},\ \mathcal{D}_{\mathcal{Y}}:\mathbb{R}^{d_{\mathcal{Y}}}\to\mathcal{Y}\]

be approximate encoders and decoders for \(\mathcal{X}\) and \(\mathcal{Y}\), respectively. As mentioned, we assume that these maps have already been learned, and focus on the training of the DNN \(\widehat{N}\) in (1.3). Our main results allow for arbitrary encoders and decoders (subject to the assumptions detailed below), and provide generalization bounds that are explicit in these terms: specifically, they depend on how well each encoder-decoder pair approximates the respective identity map on \(\mathcal{X}\) or \(\mathcal{Y}\).

In order to formulate the precise notion holomorphy for the operator \(F\), we require the following. Let \(D=[-1,1]^{\mathbb{N}}\) and \(\varrho\) be the uniform probability measure on \(D\). Given \(\rho>1\), we define the _Bernstein ellipse_\(\mathcal{E}(\rho)=\big{\{}(x+x^{-1})/2:x\in\mathbb{C},\;1\leq|x|\leq\varrho \big{\}}\subset\mathbb{C}\), and, for convenience, we let \(\mathcal{E}(1)=[-1,1]\). Next, for \(\bm{\rho}=(\rho_{i})_{i\in\mathbb{N}}\geq\bm{1}\), we define the _Bernstein polyellipse_ as the product \(\mathcal{E}(\bm{\rho})=\mathcal{E}(\rho_{1})\times\mathcal{E}(\rho_{2})\times \cdots\subset\mathbb{C}^{\mathbb{N}}\).

**Definition 2.1** (Holomorphic map).: Let \(\varepsilon>0\), \(\bm{b}\in\ell^{1}(\mathbb{N})\) with \(\bm{b}\geq\bm{0}\). A Banach-valued function \(f:D\to\mathcal{Y}\) is \((\bm{b},\varepsilon)\)_-holomorphic_ if it is holomorphic in the region

\[\mathcal{R}(\bm{b},\varepsilon)=\bigcup\Big{\{}\mathcal{E}(\bm{\rho}):\bm{ \rho}\geq\bm{1},\;\sum_{j=1}^{\infty}\big{(}(\rho_{j}+\rho_{j}^{-1})/2-1\big{)} \,b_{j}\leq\varepsilon\Big{\}}\subset\mathbb{C}^{\mathbb{N}},\quad\bm{b}=(b_{ j})_{j\in\mathbb{N}}.\] (2.2)

See, e.g., [17; 85]. As noted in [7] we can, by rescaling \(\bm{b}\), assume that \(\varepsilon=1\). For convenience, we define the following unit ball, consisting of all such functions of norm at most one over \(\mathcal{R}(\bm{b},1)\):

\[\mathcal{H}(\bm{b})=\{f:D\to\mathcal{V}\;(\bm{b},1)\text{-holomorphic}:\|f(\bm{x })\|_{\mathcal{Y}}\leq 1,\;\forall\bm{x}\in\mathcal{R}(\bm{b},1)\}.\] (2.3)

**Assumption 2.2**.: _Let \(D=[-1,1]^{\mathbb{N}}\) and \(\varrho\) be the uniform probability measure on \(D\). (A.1) There is a measurable mapping \(\iota:\mathcal{X}\to\mathbb{R}^{\mathbb{N}}\) such that pushforward \(\varsigma:=\iota\sharp\mu\) is a quasi-uniform measure supported on \(D\) and \(\iota|_{\mathrm{supp}(\mu)}:\mathcal{X}\to\ell^{\infty}(\mathbb{N})\) is Lipschitz with constant \(L_{\iota}\geq 0\). (A.2) The operator \(F\) has the form \(F=f\circ\iota\), where \(f\in\mathcal{H}(\bm{b})\) for some \(\bm{b}\in\ell^{p}_{\mathsf{M}}(\mathbb{N})\) and \(0<p<1\). (A.3) The map \(\mathcal{E}_{\mathcal{X}}:=\iota_{d_{\mathcal{X}}}\circ\widetilde{\mathcal{D}} _{\mathcal{X}}\circ\widetilde{\mathcal{E}}_{\mathcal{X}}\) is measurable (here \(\iota_{d_{\mathcal{X}}}:\mathcal{X}\to\mathbb{R}^{d_{\mathcal{X}}}\) is the restriction of \(\iota\), i.e., \(\iota_{d_{\mathcal{X}}}(X)=(\iota(X)_{i=1}^{d_{\mathcal{X}}}\)) and the pushforward \(\tilde{\varsigma}:=\mathcal{E}_{\mathcal{X}}\sharp\mu\) is absolutely continuous with respect to \(\varrho\). (A.4) The maps \(\mathcal{D}_{\mathcal{Y}}\) and \(\mathcal{E}_{\mathcal{Y}}\) are linear and bounded._

Now let \(X_{1},\ldots,X_{m}\sim_{\mathrm{i.i.d.}}\mu\) and consider the training data

\[\{(X_{i},Y_{i})\}_{i=1}^{m}\subset(\mathcal{X}\times\mathcal{Y})^{m},\quad \text{where }Y_{i}=F(X_{i})+E_{i}\in\mathcal{Y}\] (2.4)

and \(E_{i}\in\mathcal{Y}\) represents noise. Let \(\mathcal{N}\) be a class of DNNs \(N:\mathbb{R}^{d_{\mathcal{X}}}\to\mathbb{R}^{d_{\mathcal{Y}}}\), and define

\[F\approx\widehat{F}:=\mathcal{D}_{\mathcal{Y}}\circ\widehat{N}\circ\mathcal{ E}_{\mathcal{X}},\quad\text{where }\widehat{N}\in\operatorname*{argmin}_{N\in\mathcal{N}}\frac{1}{m}\sum_{i=1}^{m }\|Y_{i}-\mathcal{D}_{\mathcal{Y}}\circ N\circ\mathcal{E}_{\mathcal{X}}(X_{i}) \|_{\mathcal{Y}}^{2}.\] (2.5)

### Discussion of assumptions

We now discuss (A.1)-(A.4). In SS6 we describe future work on relaxing these assumptions.

(A.1) is a weak assumption. It asserts that there is a Lipschitz map \(\iota\) under which the pushforward of \(\mu\) is a quasi-uniform measure supported in \(D\). As we discuss in Example 2.3, this is notably the case when \(\mu\) is the law of some random field with an affine parametrization involving bounded random variables - a situation that occurs frequently in parametric and stochastic PDE problems. (A.2) describes the specific holomorphy of the operator \(F\) - see Remark 2.4 for details. Note that we require \(\bm{b}\in\ell^{p}_{\mathsf{M}}(\mathbb{N})\), not just \(\bm{b}\in\ell^{p}(\mathbb{N})\). It is known [7] that one cannot learn holomorphic functions (and hence operators) from finite data if \(\bm{b}\in\ell^{p}(\mathbb{N})\) only. (A.3) is a relatively weak assumption. In view of (A.1), we expect it to hold as long as the \(\widetilde{\mathcal{D}}_{\mathcal{X}}\circ\widetilde{\mathcal{E}}_{\mathcal{X} }\approx\mathcal{I}_{\mathcal{X}}\) sufficiently well. Finally, (A.4) is a standard assumption, which holds for instance in the case of PCA-Net and DeepONet. The former also enforces the learned encoder \(\widetilde{\mathcal{E}}_{\mathcal{X}}\) to be linear, which is not needed in our setup. Moreover, both approaches usually only deal with the case where both \(\mathcal{X}\) and \(\mathcal{Y}\) are Hilbert spaces.

**Example 2.3** (Parametric PDEs).: A common operator learning problem involves learning the map

\[F:a\in\mathcal{X}\mapsto u(a)\in\mathcal{Y},\quad\text{where }u(a)\text{ satisfies } \mathcal{F}_{a}u=0\] (2.6)

and \(\mathcal{F}_{a}\) specifies a certain PDE depending on a parameter or function \(a\). A standard example is the elliptic diffusion equation over a domain \(\Omega\subset\mathbb{R}^{n}\). Here \(a=a(\bm{x})\in\mathrm{L}^{\infty}(\Omega)=:\mathcal{X}\) is the diffusion coefficient and \(u=u(\cdot;a)\) is the solution of the PDE

\[-\nabla\cdot(a\nabla u(\bm{z};a))=g,\;\bm{z}\in\Omega,\quad u(\bm{z};a)=0,\;\bm {z}\in\partial\Omega.\] (2.7)Problems such as (2.6) are ubiquitous in scientific computing, with many applications in engineering, biology, physics, finance and beyond. In many such applications, it is common to assume that the measure \(\mu\) on \(\mathcal{X}\) is the law of a random field

\[a(\bm{x})=a(\cdot;\bm{x})=a_{0}(\cdot)+\sum_{i=1}^{\infty}c_{i}x_{i}\phi_{i}( \cdot),\] (2.8)

for functions \(a_{0},\phi_{i}\in\mathcal{X}\), where the \(x_{i}\) are random variables and \(c_{i}\geq 0\) are scalars that ensure that \(a\in\mathrm{L}^{\infty}(\Omega)\). Under some mild assumptions, (2.8) is then the Karhunen-Loeve (KL) expansion of the measure \(\mu\). See, e.g., [91] (see also [55, SS3.5.1]). The \(x_{i}\) are typically independent. While in some settings, they may have infinite support, it is also common in practice to assume they range between finite maxima and minima. After rescaling, one may therefore assume that \(\bm{x}\in D=[-1,1]^{\mathbb{N}}\).

Problems of this type fits into our framework. Suppose that \(\bm{x}=(x_{1},x_{2},\ldots)\sim\varrho\). The measure \(\mu\) is then given as the pushforward \(\mu=a\sharp\varrho\) and \(f:D\to\mathcal{Y}\) is the parametric solution map \(f:\bm{x}\in D\mapsto u(\cdot;a(\bm{x}))\in\mathcal{Y}\). If needed, the map \(\iota\) can be defined in a number of different ways. Suppose, for instance, that \(\mathcal{X}\) is a Hilbert space, e.g., \(\mathcal{X}=\mathrm{L}^{2}(\Omega)\), and \(\{\phi_{i}\}_{i=1}^{\infty}\) is a Riesz system (this holds, for instance, in the case of a KL expansion, in which case \(\{\phi_{i}\}_{i=1}^{\infty}\) is an orthonormal basis). Then \(\{\phi_{i}\}_{i=1}^{\infty}\) has a unique biorthogonal dual Riesz system \(\{\psi_{i}\}_{i=1}^{\infty}\). We may therefore define \(\iota:a\mapsto\left((a-a_{0},\psi_{i})_{1^{2}(\Omega)}/c_{i}\right)_{i=1}^{\infty}\). Notice that \(\iota\) is a bounded linear map and \(F(X)=f\circ\iota(X)=f(\bm{x})\) for \(X=a(\bm{x})\sim\mu\). However, evaluating \(\iota\) is often not required for computations (see SSA.1).

This example considers an affine parametrization (2.8) inducing the measure \(\mu\). Note that other parametrizations can be considered. Common examples include the _quadratic_\(a(z;\bm{x})=a_{0}(z)+\left(\sum_{i=1}^{\infty}c_{i}x_{i}\phi_{i}(z)\right)^{2}\) and _log-transformed_\(a(z,\bm{x})=\exp\left(\sum_{i=1}^{\infty}c_{i}x_{i}\phi_{i}(z)\right)\) parametrizations [17].

**Remark 2.4** (Holomorphy assumption): In the previous example, the operator \(F\) stems from the solution map \(f:D\to\mathcal{Y}\) of a parametric PDE. The regularity of solution maps of parametric PDEs has been intensively studied, and it is known that many such maps are \((\bm{b},\varepsilon)\)-holomorphic (hence the resulting operator satisfies (A.II)). Consider, for instance, the affine diffusion problem (2.7)-(2.8). Under a mild _uniform ellipticity_ condition, the solution map of the standard weak form of the PDE \(f:\bm{x}\in D\mapsto u(a(\cdot;\bm{x}))\in\mathrm{H}_{0}^{1}(\Omega)\) is \((\bm{b},\varepsilon)\)-holomorphic with \(\bm{b}=(b_{i})_{i=1}^{\infty}\) and \(b_{i}=c_{i}\|\phi_{i}\|_{\mathrm{L}^{\infty}(\Omega)}\). See, e.g., [3, Prop. 4.9], as well as SSB.3. Similar results are known for other parametric PDEs. This includes parabolic PDEs, various types of nonlinear, elliptic PDEs, PDEs over parametrized domains, parametric hyperbolic problems and parametric control problems. See [19] or [3, Chpt. 4] for reviews.

## 3 Main results I: upper bounds

We now present our first two main results. In these results, given an optimization problem \(\min_{t}f(t)\), we say that \(\hat{t}\) is a \(\tau\)_-approximate minimizer_ for some \(\tau\geq 0\) if \(f(\hat{t})\leq\min_{t}f(t)+\tau^{2}\).

**Theorem 3.1** (Existence of good DNN architectures).: _Let \(m\geq 3\), \(\delta>0\), \(0<\epsilon<1\) and \(L=L(m,\epsilon)=\log^{4}(m)+\log(1/\epsilon)\). Then there exists a class \(\mathcal{N}\) of hyperbolic tangent (tanh) DNNs \(N:\mathbb{R}^{d_{\mathcal{X}}}\to\mathbb{R}^{d_{\mathcal{Y}}}\) depending on \(m\) and \(\epsilon\) only with_

\[\mathrm{width}(\mathcal{N})\lesssim(m/L)^{1+\delta},\quad\mathrm{depth}( \mathcal{N})\lesssim\log(m/L),\] (3.1)

_such that following holds. Suppose that Assumption 2.2 holds and_

\[d_{\mathcal{X}}\geq\lceil m/L\rceil,\qquad L_{\iota}\cdot\|\mathcal{I}_{ \mathcal{X}}-\widetilde{\mathcal{D}}_{\mathcal{X}}\circ\widetilde{\mathcal{E}} _{\mathcal{X}}\|_{L_{\bm{\nu}}^{2}(\mathcal{X};\mathcal{X})}\leq c\cdot(m/L)^{ -1/2},\] (3.2)

_where \(\mathcal{I}_{\mathcal{X}}:\mathcal{X}\to\mathcal{X}\) is the identity map and \(c>0\) is a universal constant. Let \(X_{1},\ldots,X_{m}\sim_{\mathrm{i.i.d.}}\mu\) and consider the noisy training data (2.4) with arbitrary noise \(E_{i}\in\mathcal{Y}\). Then, with probability at least \(1-\epsilon\), every \(\tau\)-minimizer \(\widehat{N}\) of (2.5), where \(\tau\geq 0\) is arbitrary, yields an approximation \(\widehat{F}\) that satisfies_

\[\left|\!\left|F-\widehat{F}\right|\!\right|\!\right|_{L_{\mu}^{2}( \mathcal{X};\mathcal{Y})} \lesssim E_{\mathsf{app},2}+E_{\mathcal{X},2}+E_{\mathcal{Y},2}+E_{ \mathsf{opt},2}+E_{\mathsf{samp},2},\] (3.3) \[\left|\!\left|F-\widehat{F}\right|\!\right|_{L_{\mu}^{\infty}( \mathcal{X};\mathcal{Y})} \lesssim E_{\mathsf{app},\infty}+E_{\mathcal{X},\infty}+E_{ \mathcal{Y},\infty}+E_{\mathsf{opt},\infty}+E_{\mathsf{samp},\infty},\] (3.4)

_and, if \(\mathcal{Y}\) is a Hilbert space,_

\[\left|\!\left|F-\widehat{F}\right|\!\right|_{L_{\mu}^{2}(\mathcal{X}; \mathcal{Y})} \lesssim E_{\mathsf{app},2}+E_{\mathcal{X},2}+E_{\mathcal{Y},2}+E_{ \mathsf{opt},2}+E_{\mathsf{samp},2}.\] (3.5)_Here, the approximation error terms \(E_{\mathsf{app},q}\), \(q=2,\infty\), are given by_

\[E_{\mathsf{app},q}=a_{\mathcal{Y}}\cdot C(\bm{b},p,\xi)\cdot(m/L)^{\theta+1-1/q-1 /p},\] (3.6)

_where \(a_{\mathcal{Y}}=\|\mathcal{D}_{\mathcal{Y}}\circ\mathcal{E}_{\mathcal{Y}}\|_{ \mathcal{Y}\to\mathcal{Y}}\), \(C(\bm{b},p,\xi)>0\) depends on \(\bm{b}\), \(p\) and \(\xi\) only and \(\theta=0\) if \(\mathcal{Y}\) is a Hilbert space (as in (3.5)) or \(\theta=1/2\) otherwise (as in (3.3)-(3.4)). The other terms are given by_

\[\begin{split}& E_{\mathcal{X},2}=a_{\mathcal{Y}}\cdot L_{\iota} \cdot\sqrt{m/(L\epsilon)}\cdot\|\mathcal{I}_{\mathcal{X}}-\widetilde{ \mathcal{D}}_{\mathcal{X}}\circ\widetilde{\mathcal{E}}_{\mathcal{X}}\|_{L^{2} _{\mu}(\mathcal{X};\mathcal{X})}\\ & E_{\mathcal{X},\infty}=a_{\mathcal{Y}}\cdot L_{\iota}\cdot\sqrt {m/L}\cdot\left(\sqrt{m/L}\cdot\|\mathcal{I}_{\mathcal{X}}-\widetilde{ \mathcal{D}}_{\mathcal{X}}\circ\widetilde{\mathcal{E}}_{\mathcal{X}}\|_{L^{2} _{\mu}(\mathcal{X};\mathcal{X})}+\|\mathcal{I}_{\mathcal{X}}-\widetilde{ \mathcal{D}}_{\mathcal{X}}\circ\widetilde{\mathcal{E}}_{\mathcal{X}}\|_{L^{ \infty}_{\mu}(\mathcal{X};\mathcal{X})}\right)\\ & E_{\mathcal{Y},2}=\|\mathcal{I}_{\mathcal{Y}}-\mathcal{D}_{ \mathcal{Y}}\circ\mathcal{E}_{\mathcal{Y}}\|_{L^{2}_{\mathcal{Y}\mu}( \mathcal{Y};\mathcal{Y})}/\sqrt{\epsilon}\\ & E_{\mathcal{Y},\infty}=\|\mathcal{I}_{\mathcal{Y}}-\mathcal{D}_{ \mathcal{Y}}\circ\mathcal{E}_{\mathcal{Y}}\|_{L^{\infty}_{\mathcal{Y}\mu}( \mathcal{Y};\mathcal{Y})}+\sqrt{m/L}\cdot\|\mathcal{I}_{\mathcal{Y}}-\mathcal{D }_{\mathcal{Y}}\circ\mathcal{E}_{\mathcal{Y}}\|_{L^{2}_{\mathcal{Y}\mu}( \mathcal{Y};\mathcal{Y})},\end{split}\] (3.7)

_where \(\mathcal{I}_{\mathcal{Y}}:\mathcal{Y}\to\mathcal{Y}\) is the identity map and, if \(\left\|\bm{E}\right\|_{2;\mathcal{Y}}^{2}=\sum_{i=1}^{m}\left\|E_{i}\right\|_{ \mathcal{Y}}^{2}\),_

\[E_{\mathsf{opt}}=\begin{cases}\tau+2^{-m}&q=2\\ \sqrt{m/L}\tau+2^{-m}&q=\infty\end{cases},\quad E_{\mathsf{samp},q}=\begin{cases} \left\|\bm{E}\right\|_{2;\mathcal{Y}}/\sqrt{m}&q=2\\ \left\|\bm{E}\right\|_{2;\mathcal{Y}}/\sqrt{L}&q=\infty\end{cases}.\] (3.8)

(Proofs of this and all other theorems are in SSC-G of the supplemental material.) This theorem shows that there is a family of tanh DNNs that yield provable bounds for learning holomorphic operators. The error (3.3)-(3.5) decomposes into an _approximation error_ (3.6), which decays algebraically in the amount of training data \(m\). Later, in Theorems 4.1-4.2, we show that these rates are optimal when \(\mathcal{Y}\) is a Hilbert space, up to log factors. Next, are the _encoding-decoding errors_ (3.7), which depend on how well the approximate encoder-decoder pairs \((\widetilde{\mathcal{E}}_{\mathcal{X}},\widetilde{\mathcal{D}}_{\mathcal{X}})\) and \((\mathcal{E}_{\mathcal{Y}},\mathcal{D}_{\mathcal{Y}})\) approximate the identity maps on \(\mathcal{X}\) and \(\mathcal{Y}\), respectively. Observe that these terms are increasing in \(m\) for fixed encoders and decoders. Therefore, as one expects, the accuracy of the encoder-decoder approximations \(\widetilde{\mathcal{D}}_{\mathcal{X}}\circ\widetilde{\mathcal{E}}_{\mathcal{X }}\approx\mathcal{I}_{\mathcal{X}}\) and \(\mathcal{D}_{\mathcal{Y}}\circ\mathcal{E}_{\mathcal{Y}}\approx\mathcal{I}_{ \mathcal{Y}}\) should increase with increasing \(m\) to ensure decay to zero of the generalization error as \(m\to\infty\). The specific terms in (3.7) (for \(q=2\)) are quite standard in operator learning. See, e.g., [53, 55]. When the encoders and decoders are computed via PCA, as in PCA-Net, standard bounds can be derived for these terms [53]. For similar analysis in the case of DeepONNets, see [55]. Finally, the error (3.3)-(3.5) involves an _optimization error_\(E_{\mathsf{opt}}\), which primarily depends on how accurately the optimization problem (2.5) is solved (i.e., the term \(\tau\)), and a _sampling error_\(E_{\mathsf{samp}}\), which depends on the error in the training data (2.4).

Theorem 3.1 allows \(\mathcal{Y}\) to be a Banach or a Hilbert space. Overall, when \(\mathcal{Y}\) is only a Banach space, we obtain a weaker \(L^{2}_{\mu}\)-norm bound involving the Pettis norm (3.3) and, moreover, the approximation error \(E_{\mathsf{app},q}\) is worse by a factor of \(1/2\) than when \(\mathcal{Y}\) is a Hilbert space. (Note that one can establish a bound for the Bochner \(L^{2}_{\mu}\)-norm error when \(\mathcal{Y}\) is a Banach space via (3.4) and the inequality \(\|\cdot\|_{L^{2}_{\mu}(\mathcal{X};\mathcal{Y})}\leq\|\cdot\|_{L^{\infty}_{ \mu}(\mathcal{X};\mathcal{Y})}\). However, we do not believe the resulting bound is sharp). As we discuss in Remark D.18, the discrepancies between the two cases stem from the lack of an inner product structure and, in particular, the absence of Parseval's identity when \(\mathcal{Y}\) is a Banach space.

Observe that the DNN architecture in Theorem 3.1 is independent of the smoothness of the operator being learned. We term such an architecture _problem agnostic_. This theorem considers tanh activations only. However, as we discuss in Remark D.11, other activations can be readily used instead. Other key facets of Theorem 3.1 are the width and depth bounds (3.1). Qualitatively, these agree with empirical practice: namely, better performing DNNs tend to be wider than they are deep, and relatively shallow DNNs perform well in practice (see [24, 25] and references therein). We also see this later in SS5.

On the other hand, the family \(\mathcal{N}\) is not fully connected. As we describe in SSC.2.1, while the weights on the final layer can be arbitrary real numbers, the weights and biases in the hidden layers come from a finite (but large) set: they are _handcrafted_ to approximately emulate certain multivariate orthogonal polynomials. Since fully-connected DNNs are typically used in practice, Theorem 3.1 is essentially a theoretical contribution. In our next result, we consider the more practical scenario of fully-connected DNNs.

**Theorem 3.2** (Fully-connected DNN architectures are good).: _There are universal constants \(c_{1},c_{2},c_{3},c_{4}\geq 1\) such that the following holds. Let \(m\), \(\delta\), \(\epsilon\) and \(L\) be as in Theorem 3.1,_

\[d_{\mathcal{X}}\geq c_{1}(m+\log(1/\epsilon)),\quad L_{\iota}\cdot\|\mathcal{I}_{ \mathcal{X}}-\widetilde{\mathcal{D}}_{\mathcal{X}}\circ\widetilde{\mathcal{E}}_{ \mathcal{X}}\|_{L^{2}_{\mu}(\mathcal{X};\mathcal{X})}\leq c(\delta)\cdot(m+\log(1/ \epsilon))^{-1/2},\] (3.9)_where \(c(\delta)>0\) depends on \(\delta\) only, consider any class \(\mathcal{N}\) of fully-connected DNNs satisfying_

\[(n_{0},n_{L+2})=(d_{\mathcal{X}},d_{\mathcal{Y}}),\quad N_{1},\ldots,N_{L+1}\geq c _{2}\cdot(m+\log(1/\epsilon))\cdot(m/L)^{\delta},\quad L\geq c_{3}\cdot\log(m/L).\] (3.10)

_Suppose that Assumption 2.2 holds and that the pushforward \(\varsigma\) in (A.1) is the tensor-product of a univariate probability distribution with mean zero and variance \(\omega\gtrsim 1\). Let \(X_{1},\ldots,X_{m}\sim_{\mathrm{i.i.d.}}\mu\) and consider (2.4) with arbitrary \(E_{i}\in\mathcal{Y}\). Then the following hold with probability at least \(1-\epsilon\)._

1. _Uncountably many 'good' minimizers._ _The problem (_2.5_) has uncountably many minimizers that satisfy (_3.3_) with_ \(\tau=0\) _or (_3.5_) with_ \(\tau=0\) _if_ \(\mathcal{Y}\) _is a Hilbert space. They also satisfy (_3.4_) with_ \(\tau=0\) _and the modified right-hand side_ \(\sqrt{LE}_{\text{app},\infty}+LE_{\mathcal{X},\infty}+\sqrt{LE}_{\mathcal{Y}, \infty}+E_{\text{opt},\infty}+\sqrt{LE}_{\text{samp},\infty}\)_._
2. _Good minimizers are stable._ _Suppose that_ \(\mathcal{E}_{\mathcal{X}}\in L^{\infty}_{\mu}(\mathcal{X};\mathbb{R}^{d_{X}})\) _and let_ \(\tau_{o}>0\) _be arbitrary. Then there is a neighbourhood of DNN parameters around the parameters of each minimizer in (A) for which the approximation corresponding to any parameters in this neigbourhood also satisfies the same bounds as in (A) with_ \(\tau=\tau_{o}\)_._
3. _Good minimizers can be far apart in parameter space._ _For sufficiently large_ \(m\)_, there are at least_ \((m/(c_{4}L))^{2\delta m}\) _minimizers satisfying the bounds in (A) such that, for any two such minimizers, their parameters satisfy_ \(\|\bm{\theta}^{\prime}\|=\|\bm{\theta}\|\) _and_ \(\|\bm{\theta}^{\prime}-\bm{\theta}\|\gtrsim 1\)_._

This theorem states that DL with fully-connected DNN architectures of sufficient width and depth (3.10) can succeed, since there are minimizers that yield the optimal bounds of Theorem 3.1. Such minimizers are uncountably many in number (A), stable to perturbations (B) and many of them (exponentially in \(m\)) have sufficiently distinct and nonvanishing/nonexploding parameters (C). This theorem does not imply that _all_ minimizers are 'good' - an issue we discuss further in SS6 - but our numerical results in SS5 suggest that (approximate) minimizers obtained through training do, at least for the experiments considered, achieve the rates specified in Theorem 3.1.

## 4 Main results II: lower bounds

We now show that the various approximation errors are nearly optimal. For this, we ignore the encoding-decoding, optimization and sampling errors and proceed as follows. Let \(C(\mathcal{X};\mathcal{Y})\) be the Banach space of continuous operators. We term an _(adaptive) sampling map_ as any map

\[\mathcal{L}:C(\mathcal{X},\mathcal{Y})\rightarrow\mathcal{Y}^{m},\quad F \mapsto\mathcal{L}(F)=(F(X_{i}))_{i=1}^{m}\,,\] (4.1)

where \(X_{1}\in\mathcal{X}\), \(X_{2}=X_{2}(F(X_{1}))\in\mathcal{X}\) potentially depends on the previous evaluation \(F(X_{1})\), \(X_{3}=X_{3}(F(X_{1}),F(X_{2}))\in\mathcal{X}\), and so forth. Next, we term a _reconstruction map_ as any map \(\mathcal{R}:\mathcal{Y}^{m}\to L^{2}_{\mu}(\mathcal{X};\mathcal{Y})\). Given this, we let \(\mathcal{H}(\bm{b},\iota)=\{F=f\circ\iota:f\in\mathcal{H}(\bm{b})\}\) and define

\[\theta_{m}(\bm{b})=\inf_{\mathcal{L},\mathcal{R}}\sup_{F\in\mathcal{H}(\bm{b},\iota)}\left\|\!\left\|F-\mathcal{R}\circ\mathcal{L}(F)\right\|\!\right\|_{ L^{2}_{\mu}(\mathcal{X};\mathcal{Y})},\] (4.2)

where the infimum is taken over all such \(\mathcal{L}\) and \(\mathcal{R}\). In other words, \(\theta_{m}(\bm{b})\) measures how well one can learn holomorphic operators using _arbitrary_ training data and an _arbitrary_ reconstruction procedure.

**Theorem 4.1** (Optimal \(L^{2}\) error rates).: _Suppose that (A.1) holds. Then, for any \(0<p<1\) there is a constant \(c(p)>0\) such that the following hold._

1. _For each_ \(m\in\mathbb{N}\)_, there is a_ \(\bm{b}\in\ell^{p}_{\mathsf{M}}(\mathbb{N})\)_,_ \(\bm{b}\geq\bm{0}\)_,_ \(\|\bm{b}\|_{p,\mathsf{M}}=1\) _such that_ \(\theta_{m}(\bm{b})\geq c(p)\cdot m^{1/2-1/p}\)_._
2. _There is a_ \(\bm{b}\in\ell^{p}_{\mathsf{M}}(\mathbb{N})\)_,_ \(\bm{b}\geq\bm{0}\)_,_ \(\|\bm{b}\|_{p,\mathsf{M}}=1\) _such that_ \(\theta_{m}(\bm{b})\geq c(p)\cdot\frac{m^{1/2-1/p}}{\log^{2/p}(2m)}\)_,_ \(\forall m\in\mathbb{N}\)_._

This theorem shows that the error \(E_{\text{app},2}\) in Theorems 3.1-3.2 is optimal, up to log terms, whenever \(\mathcal{Y}\) is a Hilbert space: there does not exist a reconstruction map surpasses the rate \(m^{1/2-1/p}\) for learning holomorphic operators. Note that this result applies not only to DL-based procedures, but _any_ procedure that learns such operators from \(m\) samples. Another consequence of this result is that adaptive sampling, i.e., _active learning_, is of no benefit. As shown by Theorems 3.1-3.2, the optimal rate \(m^{1/2-1/p}\) can, up to log terms, be achieved through inactive learning, i.e., i.i.d. sampling from \(\mu\).

Theorem 4.1 considers \(L^{2}\)-norm. For the \(L^{\infty}\)-norm, we present a somewhat weaker result. Let

\[\tilde{\theta}_{m}(\bm{b})=\inf_{\mathcal{R}}\{\mathbb{E}_{X_{1},\ldots,X_{m} \sim\mu}\sup_{F\in\mathcal{H}(\bm{b},\iota)}\left\|F-\mathcal{R}(\{X_{i},F(X_{ i})\})\right\|_{L^{\infty}_{\mu}(\mathcal{X};\mathcal{Y})}\},\] (4.3)where the infimum is taken over all reconstruction maps \(\mathcal{R}:(\mathcal{X}\times\mathcal{Y})^{m}\to L^{\infty}_{\mu}(\mathcal{X}; \mathcal{Y})\) only.

**Theorem 4.2** (Optimal \(L^{\infty}\) error rates).: _Suppose that (A.1) holds and that the pushforward \(\varsigma\) is the tensor-product of a univariate probability distribution with mean zero and variance \(\omega\gtrsim 1\). Then, for any \(0<p<1\) there is a constant \(c(p)>0\) such that the following hold._

1. _For each_ \(m\in\mathbb{N}\)_, there is a_ \(\boldsymbol{b}\in\ell^{p}_{\mathsf{M}}(\mathbb{N})\)_,_ \(\boldsymbol{b}\geq\boldsymbol{0}\)_,_ \(\left\lVert\boldsymbol{b}\right\rVert_{p,\mathsf{M}}=1\) _such that_ \(\tilde{\theta}_{m}(\boldsymbol{b})\geq c(p)\cdot\frac{m^{1-1/p}}{\log(m)}\)_._
2. _There is a_ \(\boldsymbol{b}\in\ell^{p}_{\mathsf{M}}(\mathbb{N})\)_,_ \(\boldsymbol{b}\geq\boldsymbol{0}\)_,_ \(\left\lVert\boldsymbol{b}\right\rVert_{p,\mathsf{M}}=1\) _such that_ \(\tilde{\theta}_{m}(\boldsymbol{b})\geq c(p)\cdot\frac{m^{1-1/p}}{\log^{2/p+1}( 2m)}\)_,_ \(\forall m\in\mathbb{N}\)_._

As with the previous theorem, this result asserts that the rate \(m^{1-1/p}\) is optimal in the \(L^{\infty}\)-norm when \(\mathcal{Y}\) is a Hilbert space. However, it is strictly weaker than Theorem 4.1 as it only considers i.i.d. random sampling from \(\mu\), as opposed to arbitrary (adaptive) samples. Note that Theorem 4.1 is an extension of [7, Thm. 4.4]. Theorem 4.2 is new, and is of independent interest since it partially addresses an open problem of [7] about deriving lower bounds in the \(L^{\infty}_{\mu}\)-norm, as opposed to just the \(L^{2}_{\mu}\)-norm. See SSC.2.3-C.2.4 for more discussion.

## 5 Numerical experiments

We now present numerical results for DL applied to various different parametric PDE problems, as in Example 2.3. For a full description of our experimental setup, see SSA-B.

Since the main objective of this work is to examine the approximation error, we follow a standard setup and fix the encoder and decoders for each experiment, so that \(\mathcal{E}_{\mathcal{X}}\) and \(\mathcal{D}_{\mathcal{Y}}\) in (1.3) do not change for different choices of \(\widehat{N}\). We also set up our experiments so that encoding-decoding (3.7) and sampling (3.8) errors are zero. We do this in a standard way. To ensure that \(E_{\mathcal{X},q}=0\), we truncate the parametric expansions (2.8) after \(d\) terms (henceforth termed the _parametric dimension_) and define the encoder \(\mathcal{E}_{\mathcal{X}}\) accordingly. This means we effectively consider a parametric PDE depending on finitely-many parameters. We use Finite Element Methods (FEMs) to both solve the PDE (for generating training and testing data) and define the decoder \(\mathcal{D}_{\mathcal{Y}}\) (see (A.2)). To ensure that \(E_{\mathcal{Y},q}=0\), we compute errors with respect to the Bochner \(L^{2}_{\mu}(\mathcal{X};\widetilde{\mathcal{Y}})\)-norm, where \(\widetilde{\mathcal{Y}}=\mathcal{D}_{\mathcal{Y}}(\mathbb{R}^{d_{\mathcal{Y}}})\) is the FEM discretization of \(\mathcal{Y}\). In other words, we use the same FEM code to generate test data and compute the errors as we do to construct the operator approximation \(\widehat{F}\). See SSA.1 for further details.

The DNNs in our experiments are fully-connected and of the form (2.1). We denote by \(\sigma\)\(L\times N\) DNN a DNN \(\widehat{N}\) with activation function \(\sigma\), width \(N\) and depth \(L\). To solve (2.5) we use Adam [47] with early stopping and an exponentially decaying learning rate. We train our DNN architectures for 60,000 epochs and results are averaged over a number of trials. See SSA.2 for further details.

**Parametric elliptic diffusion equation.** Our first example is the parametric elliptic diffusion equation (2.7). This PDE arises in many scientific computing applications, such as groundwater flow modelling, see, e.g., [95]. We describe the full PDE and its FE discretization in SSB.3. In our experiments, we consider both affine (B.1) and log-transformed (B.2) diffusion coefficients. The latter is particularly useful in the groundwater flow problem as the permeability of various layers of sediment can vary on logarithmic scales. Differing from most prior work, we consider a novel _mixed variational formulation_[32] of (2.7), which has a number of key practical benefits (see SSB.3.1). In this case, \(\mathcal{Y}=\mathrm{L}^{2}(\Omega)\) is a Hilbert space. Fig. 1 compares the error versus the amount of training data \(m\) for various DNN architectures for learning the solution map of this PDE in \(d=4\) and \(d=8\) parametric dimensions with these two diffusion coefficients. We observe that architectures with the Exponential Linear Unit (ELU) or hyperbolic tangent (tanh) activation generally outperform similar architectures with the Rectified Linear Unit (ReLU) activation (as we discuss in Remark D.11, this difference is in agreement with our theoretical analysis). Overall, the best performing DNNs appear to roughly match the plotted rate \(m^{-1}\). As we explain further in SSB.3.2, this rate is precisely that predicted by our theory. In particular, the parametric solution map (recall Remark 2.4) is \((\boldsymbol{b},\varepsilon)\)-holomorphic with \(\boldsymbol{b}\in\ell^{p}_{\mathsf{M}}(\mathbb{N})\) for any \(p<2/3\), giving an effective convergent rate \(m^{1/2-1/p}\) that is arbitrarily close to \(m^{-1}\). Another important fact that we observe is that despite the parametric dimension doubling from 4 to 8, there is little change in the error behaviour.

**Parametric Navier-Stokes-Brinkman equations.** We next consider the parametric Navier-Stokes-Brinkman (NSB) equations. See SSB.4 and (B.14) for the full definition. Here the solution is a pair \((\bm{u},p)\), where \(\bm{u}\) is the velocity field and \(p\) is the pressure. These equations describe the dynamics of a viscous fluid flowing through porous media with random viscosity. See, e.g., [28; 43; 46; 94]. We use a mixed variational formulation [34] to discretize the PDE. This formulation is more sophisticated that standard variational formulations, but conveys various practical advantages. Unlike the previous example, it leads to \(\mathcal{Y}\) being either \(\mathcal{Y}=\mathbf{L}^{4}(\Omega)\) for \(\bm{u}\) or \(\mathcal{Y}=\mathrm{L}^{2}(\Omega)\) for \(p\). See SSB.4.1 for details. Fig. 2 compares a variety of DNN architectures for approximating the velocity field component in \(d=4\) and \(d=8\) parametric dimensions. Here again we observe the ELU and tanh DNN architectures outperform similar sized ReLU architectures. We also observe a rate close to \(m^{-1}\). Note that it is currently unknown whether this or the next example possess the same \((\bm{b},\varepsilon)\)-holomorphy guarantee as that of the previous example. Yet we observe the same rate, and therefore conjecture that such a property does indeed hold in these cases. Similar to the previous example, there is also no deterioration of the rate when moving from \(d=4\) to \(d=8\).

**Parametric stationary Boussinesq equation.** Our final example is a parametric stationary Boussinesq PDE. See SSB.5 and (B.16) for the full definition. Here the solution is a triplet \((\bm{u},\varphi,p)\), where \(\bm{u}\) is the velocity field, \(\varphi\) is the temperature and \(p\) is the pressure of the solution. The Boussinesq model arises in a variety of engineering, fluid dynamics and natural convection problems where changes in temperature affect the velocity of a fluid [14; 22; 39]. Similar to the previous example, we consider a fully mixed variational formulation (see SSB.5.1), which leads to \(\mathcal{Y}=\mathbf{L}^{4}(\Omega)\) (for \(\bm{u}\)), \(\mathcal{Y}=\mathrm{L}^{4}(\Omega)\) (for \(\varphi\)) or \(\mathcal{Y}=\mathrm{L}^{2}_{0}(\Omega)\) (for \(p\)). Fig. 3 provides numerical results. Our observations are in line with the previous two examples, with the ELU and the smaller tanh networks being most often the best performers in this problem. Once more, the errors roughly correspond to the rate \(m^{-1}\) and there is no deterioration with increasing \(d\).

## 6 Conclusions and limitations

The purpose of this work was to derive near-optimal generalization bounds for learning certain classes of holomorphic operators that arise frequently in operator learning tasks involving PDEs. Complementing and extending previous works [26; 31; 41; 55; 71] on the approximation of such operators via DNNs, we showing sharp algebraic rates of convergence in \(m\), thus confirming that

Figure 1: **Elliptic diffusion equation.** Average relative \(L^{2}_{\mu}(\mathcal{X};\widetilde{\mathcal{Y}})\)-norm error versus \(m\) for different DNNs approximating the solution operator for the elliptic diffusion equation (B.9). The first two plots use the affine coefficient \(a_{1,d}\) (B.1) with \(d=4,8\), respectively. The rest use the log-transformed coefficient \(a_{2,d}\) (B.2).

Figure 2: **NSB equations.** Average relative \(L^{2}_{\mu}(\mathcal{X};\widetilde{\mathcal{Y}})\)-norm error versus \(m\) for different DNNs approximating the velocity field \(\bm{u}\) of the NSB problem in (B.14). See Fig. 7 for results for the pressure component \(p\). The diffusion coefficients \(a_{1,d},a_{2,d}\) and \(d=4,8\) are as in Fig. 1.

such operators can be learned efficiently and without the _curse of dimensionality_. It is notable that the sizes of the various DNNs in Theorems 3.1-3.2 also do not succumb to the so-called _curse of parametric complexity_[54], since the width and depth bounds are at most algebraic in \(m\).

We end by discussing a number of limitations. First, assumption (A.I) may not hold in some applications. The domain \(D\) can easily be replaced by bounded hyperrectangle through rescaling and the condition that \(\varsigma\) be quasi-uniform relaxed to quasi-ultraspherical (by considering ultraspherical polynomials). However, it is currently an open problem whether our results can be extended to the case where \(\mu\) is Gaussian, in which case \(\varsigma\) would typically be a tensor-product Gaussian measure on \(\mathbb{R}^{\mathbb{N}}\) and the relevant polynomials would be the Hermite polynomials. Second, the reader may have noticed that the encoder \(\mathcal{E}_{\mathcal{X}}\) defined in (A.III) and used to construct the approximation (2.5) involves the pair \((\widetilde{\mathcal{E}}_{\mathcal{X}},\widetilde{\mathcal{D}}_{\mathcal{X}})\) and the map \(\iota_{d_{\mathcal{X}}}\). This is a technical requirement - also found in other theoretical works on operator learning - needed to obtain encoding-decoding errors of the form \(E_{\mathcal{X},q}\), \(q=2,\infty\). It is unknown whether it can be relaxed. It is also unknown whether the assumption on \(\tilde{\varsigma}\) in (A.III) can be relaxed. We believe this can be done, at least if the \(L^{2}_{\mu}\)-norm in (3.2) is replaced by the \(L^{\infty}_{\mu}\)-norm. Whether this is possible without modifying (3.2) is currently unknown.

Third, a limitation of Theorem 3.2 is that it only asserts that some minimizers are 'good', not all. Techniques from statistical learning theory can provide stronger bounds that hold for all minimizers. Yet, as noted in SS1.2, these tools typically produce slower rates of decay in \(m\). Overcoming this limitation - e.g., by refining these tools for the holomorphic setting or showing that the 'good' minimizers can indeed be obtained via standard training - is a topic of future work.

Finally, as noted, our theorems provided worse generalization bounds when \(\mathcal{Y}\) is a Banach space than when \(\mathcal{Y}\) is a Hilbert space. Our numerical results in Figs. 2-3 suggest that this factor is an artefact of the proofs. Whether it can be removed is an interesting open problem.

Figure 3: **Boussinesq equation.** Average relative \(L^{2}_{\mu}(\mathcal{X};\widetilde{\mathcal{Y}})\)-norm error versus \(m\) for different DNNs approximating the temperature \(\varphi\) of the Boussinesq problem in (B.16) (see Fig. 9 for \(\bm{u}\) and \(p\)). The diffusion coefficients \(a_{1,d},a_{2,d}\) and \(d=4,8\) are as in Fig. 1. In this example, we also consider an additional parametric dependence in the tensor \(\mathbb{K}=\mathbb{K}_{d}\) describing the thermal conductivity of the fluid. See §B.5 and (B.17).

## Acknowledgments and Disclosure of Funding

BA acknowledges the support of the Natural Sciences and Engineering Research Council of Canada of Canada (NSERC) through grant RGPIN-2021-611675. ND acknowledges the support of Florida State University through the CRC 2022-2023 FYAP grant program. The authors would like to thank Gregor Maier for helpful comments and feedback.

## References

* [1] B. Adcock and N. Dexter. The gap between theory and practice in function approximation with deep neural networks. _SIAM J. Math. Data Sci._, 3(2):624-655, 2021.
* [2] B. Adcock, S. Brugiapaglia, N. Dexter, and S. Moraga. Deep neural networks are effective at learning high-dimensional Hilbert-valued functions from limited data. In J. Bruna, J. S. Hesthaven, and L. Zdeborova, editors, _Proceedings of The Second Annual Conference on Mathematical and Scientific Machine Learning_, volume 145 of _Proc. Mach. Learn. Res. (PMLR)_, pages 1-36. PMLR, 2021.
* [3] B. Adcock, S. Brugiapaglia, and C. G. Webster. _Sparse Polynomial Approximation of High-Dimensional Functions_. Comput. Sci. Eng. Society for Industrial and Applied Mathematics, Philadelphia, PA, 2022.
* [4] B. Adcock, N. Dexter, and S. Moraga. Optimal approximation of infinite-dimensional holomorphic functions II: recovery from i.i.d. pointwise samples. _arXiv:2310.16940_, 2023.
* [5] B. Adcock, S. Brugiapaglia, N. Dexter, and S. Moraga. Near-optimal learning of Banach-valued, high-dimensional functions via deep neural networks. _Neural Networks (in press)_, 2024.
* [6] B. Adcock, S. Brugiapaglia, N. Dexter, and S. Moraga. Learning smooth functions in high dimensions: from sparse polynomials to deep neural networks. In S. Mishra and A. Townsend, editors, _Numerical Analysis Meets Machine Learning_, volume 25 of _Handbook of Numerical Analysis_, pages 1-52. Elsevier, 2024.
* [7] B. Adcock, N. Dexter, and S. Moraga. Optimal approximation of infinite-dimensional holomorphic functions. _Calcolo_, 61(1):12, 2024.
* [8] C. D. Aliprantis and K. C. Border. _Infinite Dimensional Analysis: A Hitchhiker's Guide_. Springer-Verlag, Berlin, Heidelberg, 3rd edition, 2006.
* [9] S. Alnaes, J. Blechta, J. Hake, A. Johansson, B. Kehlet, A. Logg, C. Richardson, J. Ring, M. E. Rognes, and G. N. Wells. The FEniCS Project Version 1.5. _Archive of Numerical Software_, 3 (100), 2015.
* [10] K. Bhattacharya, B. Hosseini, N. B. Kovachki, and A. M. Stuart. Model reduction and neural networks for parametric PDEs. _SMAI J. Comput. Math._, 7:121-157, 2021.
* [11] D. Boffi, F. Brezzi, and M. Fortin. _Mixed Finite Element Methods and Applications_. Springer, Berlin, Heidelberg, 1st edition, 2013.
* [12] S. Brugiapaglia, S. Dirksen, H. C. Jung, and H. Rauhut. Sparse recovery in bounded Riesz systems with applications to numerical methods for PDEs. _Appl. Comput. Harmon. Anal._, 53:231-269, 2021.
* [13] S. Cai, Z. Wang, L. Lu, T. A. Zaki, and G. E. Karniadakis. DeepM&Mnet: Inferring the electroconvection multiphysics fields based on operator approximation by neural networks. _J. Comput. Phys._, 436:110296, 2021.
* [14] C. Cao and J. Wu. Global regularity for the two-dimensional anisotropic Boussinesq equations with vertical dissipation. _Arch. Rational Mech. Anal._, 208:985-1004, 2013.
* [15] Q. Cao, S. Goswami, and G. E. Karniadakis. LNO: Laplace neural operator for solving differential equations. _arXiv:2303.10528_, 2023.

* [16] K. Chen, C. Wang, and H. Yang. Deep operator learning lessens the curse of dimensionality for pdes. _arXiv:2301.12227_, 2023.
* [17] A. Chkifa, A. Cohen, and C. Schwab. Breaking the curse of dimensionality in sparse polynomial approximation of parametric PDEs. _J. Math. Pures Appl._, 103(2):400-428, 2015.
* [18] P. G. Ciarlet. _The Finite Element Method for Elliptic Problems_. Society for Industrial and Applied Mathematics, Philadelphia, PA, 2002.
* [19] A. Cohen and R. A. DeVore. Approximation of high-dimensional parametric PDEs. _Acta Numer._, 24:1-159, 2015.
* [20] E. Colmenares, G. N. Gatica, and S. Moraga. A Banach spaces-based analysis of a new fully-mixed finite element method for the Boussinesq problem. _ESAIM Math. Model. Numer. Anal._, 54(5):1525-1568, 2020.
* [21] D. Dung, V. K. Nguyen, and D. T. Pham. Deep ReLU neural network approximation in Bochner spaces and applications to parametric PDEs. _J. Complexity_, 79:101779, 2023.
* [22] I. Danaila, R. Moglan, F. Hecht, and S. Le Masson. A newton method with adaptive finite elements for solving phase-change problems with natural convection. _J. Comput. Phys._, 274:826-840, 2014.
* [23] J. Daws and C. Webster. Analysis of deep neural networks with quasi-optimal polynomial approximation rates. _arXiv:1912.02302_, 2019.
* [24] M. De Hoop, D. Z. Huang, E. Qian, and A. Stuart. The cost-accuracy trade-off in operator learning with neural networks. _J. Mach. Learn._, 1:299-341, 2022.
* [25] T. De Ryck, S. Lanthaler, and S. Mishra. On the approximation of functions by tanh neural networks. _Neural Networks_, 143:732-750, 2021.
* [26] B. Deng, Y. Shin, L. Lu, Z. Zhang, and G. E. Karniadakis. Approximation rates of DeepONets for learning operators arising from advection-diffusion equations. _Neural Networks_, 153:411-426, 2022.
* [27] N. Dexter, H. Tran, and C. Webster. A mixed \(\ell_{1}\) regularization approach for sparse simultaneous approximation of parameterized PDEs. _ESAIM Math. Model. Numer. Anal._, 53:2025-2045, 2019.
* [28] Y. Dutil, D. R. Rousse, N. B. Salah, S. Lassue, and L. Zalewski. A review on phase-change materials: Mathematical modeling and simulations. _Renew. Sustain. Energy Rev._, 15(1):112-130, 2011.
* [29] S. Foucart and H. Rauhut. _A Mathematical Introduction to Compressive Sensing_. Appl. Numer. Harmon. Anal. Birkhauser, New York, NY, 2013.
* [30] N. R. Franco and S. Brugiapaglia. A practical existence theorem for reduced order models based on convolutional autoencoders. _arXiv:2402.00435_, 2024.
* [31] N. R. Franco, S. Fresca, A. Manzoni, and P. Zunino. Approximation bounds for convolutional neural networks in operator learning. _Neural Networks_, 161:129-141, 2023.
* [32] G. N. Gatica. _A Simple Introduction to the Mixed Finite Element Method_. SpringerBriefs Math. Springer, Cham, Switzerland, 2014.
* [33] G. N. Gatica, R. Oyarzua, R. Ruiz-Baier, and Y. D. Sobral. Banach spaces-based analysis of a fully-mixed finite element method for the steady-state model of fluidized beds. _Comput. Math. Appl._, 84:244-276, 2021.
* [34] G. N. Gatica, N. Nunez, and R. Ruiz-Baier. New non-augmented mixed finite element methods for the Navier-Stokes-Brinkman equations using Banach spaces. _J. Numer. Math._, 31(4):343-373, 2022.

* [35] S. Goswami, K. Kontolati, M. D. Shields, and G. E. Karniadakis. Deep transfer operator learning for partial differential equations under conditional shift. _Nat. Mach. Intell._, 4(12):1155-1164, 2022.
* [36] S. Goswami, D. S. Li, B. V. Rego, M. Latorre, J. D. Humphrey, and G. E. Karniadakis. Neural operator learning of heterogeneous mechanobiological insults contributing to aortic aneurysms. _J. R. Soc. Interface_, 19(193):20220410, 2022.
* [37] S. Goswami, A. D. Jagtap, H. Babaee, B. T. Susi, and G. E. Karniadakis. Learning stiff chemical kinetics using extended deep neural operators. _arXiv:2302.12645_, 2023.
* [38] I. Guhring and M. Raslan. Approximation rates for neural networks with encodable weights in smoothness spaces. _Neural Networks_, 134:107-130, 2021.
* [39] Z. Guo, B. Shi, and C. Zheng. A coupled lattice BGK model for the Boussinesq equations. _Int. J. Numer. Meth. Fluids_, 39:325-342, 2002.
* [40] Z. Hao, Z. Wang, H. Su, C. Ying, Y. Dong, S. Liu, Z. Cheng, J. Song, and J. Zhu. GNOT: A General Neural Operator Transformer for Operator Learning. In _International Conference on Machine Learning_, pages 12556-12569. PMLR, 2023.
* [41] L. Herrmann, C. Schwab, and J. Zech. Neural and spectral operator surrogates: unified construction and expression rate bounds. _Adv. Comput. Math._, 50:72, 2024.
* [42] A. A. Howard, M. Perego, G. E. Karniadakis, and P. Stinis. Multifidelity deep operator networks for data-driven and physics-informed problems. _J. Comput. Phys._, 493:112462, 2024.
* [43] W. R. Hwang and S. G. Advani. Numerical simulations of Stokes-Brinkman equations for permeability prediction of dual scale fibrous porous media. _Phys. Fluids._, 22:113101, 2010.
* [44] T. Hytonen, J. van Neerven, M. Veraar, and L. Weis. _Analysis in Banach Spaces, Volume I: Martingales and Littlewood-Paley Theory_. Ergebnisse der Mathematik und ihrer Grenzgebiete. 3. Folge. Springer, Cham, 2016.
* [45] G. E. Karniadakis, I. G. Kevrekidis, L. Lu, P. Perdikaris, S. Wang, and L. Yang. Physics-informed machine learning. _Nature Reviews Physics_, 3(6):422-440, 2021.
* [46] K. Khadra, P. Angot, S. Parneix, and J. P. Caltagirone. Fictitious domain approach for numerical modelling of Navier-Stokes equations. _Int. J. Numer. Meth. Fluids_, 34:651-684, 2000.
* [47] D. P. Kingma and J. Ba. Adam: a method for stochastic optimization. _arXiv:1412.6980_, 2017.
* [48] K. Kontolati, S. Goswami, M. D. Shields, and G. E. Karniadakis. On the influence of over-parameterization in manifold based surrogates and deep neural operators. _J. Comput. Phys._, 479:112008, 2023.
* [49] S. Koric, A. Viswantah, D. W. Abueidda, N. A. Sobh, and K. Khan. Deep learning operator network for plastic deformation with variable loads and material properties. _Eng. Comput._, pages 1-13, 2023.
* [50] N. Kovachki, S. Lanthaler, and S. Mishra. On universal approximation and error bounds for Fourier neural operators. _J. Mach. Learn. Res._, 22(1):13237-13312, 2021.
* [51] N. B. Kovachki, Z. Li, B. Liu, K. Azizzadenesheli, K. Bhattacharya, A. M. Stuart, and A. Anandkumar. Neural operator: learning maps between function spaces with applications to PDEs. _J. Mach. Learn. Res._, 24(89):1-97, 2023.
* [52] N. B. Kovachki, S. Lanthaler, and A. M. Stuart. Operator learning: algorithms and analysis. In S. Mishra and A. Townsend, editors, _Numerical Analysis Meets Machine Learning_, volume 25 of _Handbook of Numerical Analysis_, pages 419-467. Elsevier, 2024.
* [53] S. Lanthaler. Operator learning with PCA-Net: upper and lower complexity bounds. _arXiv:2303.16317_, 2023.

* [54] S. Lanthaler and A. M. Stuart. The parametric complexity of operator learning. _arXiv:2306.15924_, 2024.
* [55] S. Lanthaler, S. Mishra, and G. E. Karniadakis. Error estimates for DeepONets: A deep learning framework in infinite dimensions. _Trans. Math. Appl._, 6(1):tnac001, 2022.
* [56] S. Lanthaler, Z. Li, and A. M. Stuart. The nonlocal neural operator: universal approximation. _arXiv:2304.13221_, 2023.
* [57] B. Li, S. Tang, and H. Yu. Better approximations of high dimensional smooth functions by deep neural networks with rectified power units. _Commun. Comput. Phys._, 27:379-411, 2020.
* [58] Z. Li, N. Kovachki, K. Azizzadenesheli, B. Liu, K. Bhattacharya, A. Stuart, and A. Anandkumar. Neural operator: graph kernel network for partial differential equations. In _ICLR_, 2020.
* [59] Z. Li, N. Kovachki, K. Azizzadenesheli, B. Liu, A. Stuart, K. Bhattacharya, and A. Anandkumar. Multipole graph neural operator for parametric partial differential equations. _Advances in Neural Information Processing Systems_, 33:6755-6766, 2020.
* [60] Z. Li, N. Kovachki, K. Azizzadenesheli, B. Liu, K. Bhattacharya, A. Stuart, and A. Anandkumar. Fourier neural operator for parametric partial differential equations. In _ICLR_, 2021.
* [61] Z. Li, M. Liu-Schiaffini, N. Kovachki, K. Azizzadenesheli, B. Liu, K. Bhattacharya, A. Stuart, and A. Anandkumar. Learning chaotic dynamics in dissipative systems. _Advances in Neural Information Processing Systems_, 35:16768-16781, 2022.
* [62] Z. Li, N. B. Kovachki, C. Choy, B. Li, J. Kossaifi, S. P. Ota, M. A. Nabian, M. Stadler, C. Hundt, K. Azizzadenesheli, and A. Anandkumar. Geometry-informed neural operator for large-scale 3D PDEs. _arXiv:2309.00583_, 2023.
* [63] S. Liang and R. Srikant. Why deep neural networks for function approximation? In _ICLR_, 2017.
* [64] C. Lin, Z. Li, L. Lu, S. Cai, M. Maxey, and G. E. Karniadakis. Operator learning for predicting multiscale bubble growth dynamics. _J. Chem. Phys._, 154(10), 2021.
* [65] C. Lin, M. Maxey, Z. Li, and G. E. Karniadakis. A seamless multiscale operator neural network for inferring bubble dynamics. _J. Fluid Mech._, 929:A18, 2021.
* [66] H. Liu, H. Yang, M. Chen, T. Zhao, and W. Liao. Deep nonparametric estimation of operators between infinite dimensional spaces. _J. Mach. Learn. Res._, 25:1-67, 2024.
* [67] J. Lu, Z. Shen, H. Yang, and S. Zhang. Deep network approximation for smooth functions. _SIAM J. Math. Anal._, 53(5):5465-5506, 2021.
* [68] L. Lu, P. Jin, and G. E. Karniadakis. DeepONet: Learning nonlinear operators for identifying differential equations based on the universal approximation theorem of operators. _arXiv:1910.03193_, 2019.
* [69] L. Lu, P. Jin, G. Pang, Z. Zhang, and G. E. Karniadakis. Learning nonlinear operators via DeepONet based on the universal approximation theorem of operators. _Nat. Mach. Intell._, 3 (3):218-229, 2021.
* [70] L. Lu, X. Meng, S. Cai, Z. Mao, S. Goswami, Z. Zhang, and G. E. Karniadakis. A comprehensive and fair comparison of two neural operators (with practical extensions) based on fair data. _Comput. Methods Appl. Mech. Engrg._, 393:114778, 2022.
* [71] C. Marcati and C. Schwab. Exponential convergence of deep operator networks for elliptic partial differential equations. _SIAM J. Numer. Anal._, 61(3):1513-1545, 2023.
* [72] H. Mhaskar. Neural networks for optimal approximation of smooth and analytic functions. _Neural Comput._, 8(1):164-177, 1996.

* [73] K. Michalowska, S. Goswami, G. E. Karniadakis, and S. Riemer-Sorensen. Neural operator learning for long-time integration in dynamical systems with recurrent neural networks. _arXiv:2303.02243_, 2023.
* [74] S. Moraga. _Optimal and efficient algorithms for learning high-dimensional, Banach-valued functions from limited samples_. PhD thesis, Simon Fraser University, 2024.
* [75] H. H. Nguyen and V. H. Vu. Normal vector of a random hyperplane. _Int. Math. Res. Not. IMRN_, 2018(6):1754-1778, 2018.
* [76] F. Nobile, R. Tempone, and C. G. Webster. A sparse grid stochastic collocation method for partial differential equations with random input data. _SIAM J. Numer. Anal._, 46(5):2309-2345, 2008.
* [77] V. Oommen, K. Shukla, S. Goswami, R. Dingreville, and G. E. Karniadakis. Learning two-phase microstructure evolution using neural operators and autoencoder architectures. _npj Comput. Mater._, 8(1):190, 2022.
* [78] J. A. A. Opschoor and C. Schwab. Deep ReLU networks and high-order finite element methods II: Chebyshev emulation. _arXiv:2310.07261_, 2023.
* [79] J. A. A. Opschoor, C. Schwab, and J. Zech. Exponential ReLU DNN expression of holomorphic maps in high dimension. _Constr. Approx._, 55(1):537-582, 2022.
* [80] J. D. Osorio, Z. Wang, G. Karniadakis, S. Cai, C. Chryssostomidis, M. Panwar, and R. Hovsapian. Forecasting solar-thermal systems performance under transient operation using a data-driven machine learning approach based on the deep operator network architecture. _Energy Conversion and Management_, 252:115063, 2022.
* [81] W. Peng, Z. Yuan, Z. Li, and J. Wang. Linear attention coupled fourier neural operator for simulation of three-dimensional turbulence. _Physics of Fluids_, 35(1), 2023.
* [82] M. Pinkus. \(N\)_-widths in Approximation Theory_. Springer-Verlag, Berlin, 1968.
* [83] B. Raonic, R. Molinaro, T. Rohner, S. Mishra, and E. de Bezenac. Convolutional neural operators. In _ICLR_, 2023.
* [84] P. I. Renn, C. Wang, S. Lale, Z. Li, A. Anandkumar, and M. Gharib. Forecasting subcritical cylinder wakes with Fourier neural operators. _arXiv:2301.08290_, 2023.
* [85] C. Schwab and J. Zech. Deep learning in high dimension: neural network expression rates for generalized polynomial chaos expansions in UQ. _Anal. Appl. (Singap.)_, 17(1):19-55, 2019.
* [86] C. Schwab and J. Zech. Deep learning in high dimension: neural network expression rates for analytic functions in \(L^{2}(\mathbb{R}^{d},\gamma_{d})\). _SIAM/ASA J. Uncertain. Quantif._, 11(1):199-234, 2023.
* [87] C. Schwab, A. Stein, and J. Zech. Deep operator network approximation rates for Lipschitz operators. _arXiv:2307.09835_, 2023.
* [88] M. Stoyanov. User manual: Tasmanian sparse grids. Technical Report ORNL/TM-2015/596, Oak Ridge National Laboratory, One Bethel Valley Road, Oak Ridge, TN, 2015.
* [89] M. Stoyanov, D. Lebrun-Grandie, J. Burkardt, and D. Munster. Tasmanian, 9 2013. URL https://github.com/ORNL/Tasmanian.
* [90] M. K. Stoyanov and C. G. Webster. A dynamically adaptive sparse grid method for quasi-optimal interpolation of multidimensional functions. _Comput. Math. Appl._, 71(11):2449-2465, 2016.
* [91] A. M. Stuart. Inverse problems: a Bayesian perspective. _Acta Numer._, 19:451-559, 2010.
* [92] S. Tang, B. Li, and Y. Haijun. ChebNet: efficient and stable constructions of deep neural networks with rectified power units via Chebyshev approximation. _Commun. Math. Stat. (In press)_, 2024.

* [93] T. Tripura and S. Chakraborty. Wavelet neural operator for solving parametric partial differential equations in computational mechanics problems. _Comput. Methods Appl. Mech. Engrg._, 404:115783, 2023.
* [94] S. Wang, A. Faghri, and T. L. Bergman. A comprehensive numerical model for melting with natural convection. _Int. J. Heat Mass Transfer._, 53(9-10):1986-200, 2010.
* [95] C. L. Winter and D. M. Tartakovsky. Groundwater flow in heterogeneous composite aquifers. _Water Resour: Res._, 38(8), 2002.
* [96] K. Wu, T. O'Leary-Roseberry, P. Chen, and O. Ghattas. Large-scale Bayesian optimal experimental design with derivative-informed projected neural network. _J. Sci. Comput._, 95(1):30, 2023.
* [97] D. Yarotsky. Error bounds for approximations with deep ReLU networks. _Neural Networks_, 94:103-114, 2017.
* [98] E. Yeung, S. Kundu, and N. Hodas. Learning deep neural network representations for Koopman operators of nonlinear dynamical systems. In _2019 American Control Conference (ACC)_, pages 4832-4839. IEEE, 2019.
* [99] M. Yin, E. Ban, B. V. Rego, E. Zhang, C. Cavinato, J. D. Humphrey, and G. Em Karniadakis. Simulating progressive intramural damage leading to aortic dissection using DeepONet: an operator-regression neural network. _Journal of the Royal Society Interface_, 19(187):20210670, 2022.
* [100] M. Yin, E. Zhang, Y. Yu, and G. E. Karniadakis. Interfacing finite elements with deep neural operators for fast multiscale modeling of mechanics problems. _Comput. Methods Appl. Mech. Engrg._, 402:115027, 2022.
* [101] E. Zappala, A. H. d. O. Fonseca, A. H. Moberly, M. J. Higley, C. Abdallah, J. A. Cardin, and D. van Dijk. Neural integro-differential equations. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 37, pages 11104-11112, 2023.
* [102] J. Zech. _Sparse-grid approximation of high-dimensional parametric PDEs_. PhD thesis, ETH Zurich, 2018.
* [103] Z. Zhang, L. Wing Tat, and H. Schaeffer. Belnet: basis enhanced learning, a mesh-free neural operator. _Proc. R. Soc. A_, 479(2276):20230043, 2023.
* [104] M. Zhu, H. Zhang, A. Jiao, G. E. Karniadakis, and L. Lu. Reliable extrapolation of deep neural operators informed by physics or sparse observations. _Comput. Methods Appl. Mech. Engrg._, 412:116064, 2023.
* [105] Z. Zou, X. Meng, A. F. Psaros, and G. E. Karniadakis. NeuralUQ: A comprehensive library for uncertainty quantification in neural differential equations and operators. _SIAM Rev._, 66(1):161-190, 2024.

Experimental setup

In this section, we describe our experimental setup.

### Formulation of the learning problems

We first explain how all our experiments are formulated as operator learning problems. We do this in a standard way, by first truncating the random field which generates the measure \(\mu\), and then using an FEM to discretize the output space.

All our examples follow Example (2.3) and involve operators of the form (2.6), where \(\mathcal{F}_{a}\) represents a different type of PDE in each case (specifically, either an elliptic diffusion, Navier-Stokes-Brinkman or Boussinesq PDE). We consider both affine (2.8) and log-transformed parametrizations of the random field \(a(\bm{x})\). Thus, in general we write

\[a(\bm{x})=a(\cdot;\bm{x})=g\left(a_{0}(\cdot)+\sum_{i=1}^{\infty}c_{i}x_{i} \phi_{i}(\cdot)\right),\] (A.1)

where \(g:\mathbb{R}\to\mathbb{R}\) is a (measurable) map. In the affine case \(g(t)=1\). In the log-transformed case \(g(t)=\exp(t)\).

As discussed, since the main objective of this work is to examine the approximation error, we set up our experiments so that encoding-decoding errors are zero. We do this as follows. First, we fix a parametric dimension \(d\) and truncate the expansion in (A.1) after \(d\) terms, giving a map \(a_{d}:[-1,1]^{d}\to\mathcal{X}\) and measure \(\mu=a_{d}\sharp_{d}\), where \(\varrho_{d}\) is the uniform probability measure on \([-1,1]^{d}\). We then define the operator \(F\) as \(F(a_{d}(\bm{x}))=f(\bm{x})=u(\cdot;a_{d}(\bm{x}))\), where \(u(\cdot;a)\) is the solution of the PDE \(\mathcal{F}_{a}u=0\).

In alignment with our theorems, we focus on the _in-distribution_ performance of the learned approximation \(\widehat{F}\). This means we define the encoder only on \(\operatorname{supp}(\mu)\), as \(\mathcal{E}_{\mathcal{X}}(X)=\bm{x}\) when \(X=a_{d}(\bm{x})\) with \(\bm{x}\in[-1,1]^{d}\). As a result, the encoding-decoding error \(E_{\mathcal{X},q}\) in (3.7) satisfies \(E_{\mathcal{X},q}=0\).

To perform our simulations, we use FEMs to solve the PDE and discretize the output space \(\mathcal{Y}\). Let \(\{\varphi_{i}\}_{i=1}^{K}\subset\mathcal{Y}\) be a FEM basis and set \(d_{\mathcal{Y}}=K\). Then we define the decoder as

\[\mathcal{D}_{\mathcal{Y}}:\mathbb{R}^{K}\to\mathcal{Y},\quad\mathcal{D}_{ \mathcal{Y}}(\bm{c})=\sum_{i=1}^{K}c_{i}\varphi_{i}\] (A.2)

and set \(\widetilde{\mathcal{Y}}=\mathcal{D}_{\mathcal{Y}}(\mathbb{R}^{K})\) as the discretization of \(\mathcal{Y}\).

With this in hand, we now describe the simulation of training data in general terms. First, we draw \(\bm{x}_{1},\dots,\bm{x}_{m}\sim_{\mathrm{i.i.d.}}\varrho_{d}\). Then, for each training sample \(\bm{x}_{i}\), we compute \(Y_{i}\) by using the FEM to solve the PDE with parameter \(X_{i}=a_{d}(\bm{x}_{i})\). Notice that \(Y_{i}\in\widetilde{\mathcal{Y}}\) in this setup.

We consider a DNN architecture with input dimension \(n_{0}=d\) and output dimension \(n_{L\pm 2}=K\). After training, we evaluate the learned approximation \(\widehat{F}=\mathcal{D}_{\mathcal{Y}}\circ\widehat{N}\circ\mathcal{E}_{ \mathcal{X}}\) over \(\operatorname{supp}(\mu)\) as \(\widehat{F}(X)=\mathcal{D}_{\mathcal{Y}}\circ\widehat{N}\circ\mathcal{E}_{ \mathcal{X}}(X)=\mathcal{D}_{\mathcal{Y}}\circ\widehat{N}(\bm{x})\) for \(X=a_{d}(\bm{x})\) with \(\bm{x}\in[-1,1]^{d}\). Finally, as noted, we us the same FEM discretization to generate testing data, which allows us to measure the error with respect to the \(L^{2}_{\mu}(\mathcal{X};\widetilde{\mathcal{Y}})\)-norm. This effectively means that the encoding-decoding error \(E_{\mathcal{Y},q}\) in (3.7) satisfies \(E_{\mathcal{Y},q}=0\) as well.

### Computational setup for the numerical experiments

In this work, we investigate the trade-off between the accuracy of the learned operator and the number of samples \(m\) used in training. Our methodology is summarized as follows.

1. **Implementation**. We use the open-source finite element library FEniCS, specifically version 2019.1.0 [9], and Google's TensorFlow version 2.12.0. More information about TensorFlow can be found at https://www.tensorflow.org/.
2. **Hardware**. We train the DNN models in single precision on the Digital Research Alliance of Canada's Cedar compute cluster (see https://docs.allianeccan.ca/wiki/Cedar), usingIntel Xenon Processor E5-2683 v4 CPUs with either 125GB or 250GB per node. The setup for each of our PDEs is as follows. For each experiment we consider training with 14 sets of points of size \(m\in\{10,20,30,40,50,60,70,80,90,100,200,300,400,500\}\) and for 6 different architectures (4 x 40 and 10 x 100 with ReLU, ELU, and tanh activations) over two parametric dimensions (\(d=4\) and \(d=8\)) and two coefficients (\(a_{1,d}\) and \(a_{2,d}\) from (B.2)), giving 336 DNNs to be trained for each trial. For the Poisson PDE and Navier-Stokes-Brinkman PDEs we run 12 trials. For the Boussinesq PDE we run 8 trials due to the larger problem size. For the Poisson PDE, we allocate 336 nodes with \(1\times 32\) core CPUs running 4 threads (totalling 1344 threads, 6.8 GB RAM per node) each running 3 trials, taking approximately 4 hours and 15 minutes to complete. For the Navier-Stokes-Brinkman PDE, we use the same setup allocating 9.88 GB RAM per node and the runs take approximately 9 hours and 13 minutes for each of the two components of the solution to complete. For the Boussinesq PDE, we allocate 336 nodes with \(1\times 32\) core CPUs running 4 threads per node (totaling 1344 threads, 10 GB RAM per node) each running 2 trials, taking approximately 12 hours and 32 minutes for each of the 3 components to complete. Given this, the total time required to reproduce the results in parallel with the above setup is approximately 60 hours or 2.5 days. Results were stored locally on the cluster and the estimated total space used to store the data for testing and training and results from computation is approximately 50 GB. Trained models were not retained due to space limitations on the cluster.
3. **Choice of architectures and initialization**. Based on the strategies in [1], we fix the number of nodes per layer \(N\) and depth \(L\) such that the ratio \(\beta:=L/N\) is \(\beta=0.5\). In addition, we initialize the weights and biases using the HeUniform initializer from keras setting the seed to the trial number. We consider the Rectified Linear Unit (ReLU) \[\sigma_{1}(z):=\max\{0,z\},\] hyperbolic tangent (tanh) \[\sigma_{2}(z):=\frac{\mathrm{e}^{z}-\mathrm{e}^{-z}}{\mathrm{e}^{z}+\mathrm{e }^{-z}},\] or Exponential Linear Unit (ELU) \[\sigma_{3}(z)=\begin{cases}z&z>0,\\ \mathrm{e}^{z}-1&z\leq 0\end{cases}\] activation functions in our experiments.
4. **Optimizers for training and parametrization**. To train the DNNs, we use the Adam optimizer [47], incorporating an exponentially-decaying learning rate. We train our models for 60,000 epochs or until converging to a tolerance level of \(\epsilon_{\text{tol}}=5\cdot 10^{-7}\) in single precision. In light of the nonmonotonic convergence behavior observed during the minimization of the nonconvex loss (see, e.g., [1, 2]), we implement early stopping. More precisely, we save the weights and biases of the partially trained network once the ratio between the current loss and the last checkpoint loss is reduced below \(1/8\), or if the current weights and biases produce the best loss value observed in training. We then restore these weights after training only if the loss value of the current weights is larger than that of the saved checkpoint.
5. **Training data and design of experiments**. First, we define a 'trial' as a complete training run for a DNN approximating a specific function, initialized as mentioned above. Following the setup of SSA.1, we run several trials solving the problem: Given training data \(\{(X_{i},Y_{i})\}_{i=1}^{m}\subset(\mathcal{X}\times\mathcal{Y})^{m},\ \ X_{i}\sim_{\mathrm{i.i.d.}}\mu,\ \ Y_{i}=F(X_{i})+E_{i}\in\mathcal{Y},\) approximate \(F\in L_{\mu}^{2}(\mathcal{X};\mathcal{Y})\). We generate the measurements \(Y_{i}\) using mixed variational formulations of the parametric elliptic, Navier-Stokes-Brinkman and Boussinesq PDEs discretized using FEniCS with input data \(X_{i}\). The noise \(E_{i}\in\mathcal{Y}\) encompasses the discretization errors from numerical solution. Further details of the discretization can be found in SSB. Each of our architectures is trained across a range of datasets with increasing sizes. This involves using a set of training data consisting of values \(\{(X_{i},Y_{i})\}_{i=1}^{m}\), where \(m\) denotes the size of the training data and belongs to the set \(\{10,20,30,40,50,60,70,80,90,100,200,300,400,500\}\). After training we calculate the testing error for each trial and run statistics across all trials for each dataset.
6. **Testing data and error metric**. The testing data is generated similarly to the training data, obtaining solutions at different points \(X_{i}\in\mathcal{X}\) for \(i=1,\ldots,m_{\text{test}}\). However, the testing data \(\{(X_{i},Y_{i}=F(X_{i})+E_{i})\}_{i=1}^{m_{\rm test}}\) is generated using a deterministic high-order sparse grid collocation method [76]. In particular, we use sparse grid quadrature rules to compute approximations to the Bochner norms \[\|F\|_{L_{\mu}^{2}(\mathcal{X};\widetilde{\mathcal{Y}})}=\left(\int_{\mathcal{X }}\|F(X)\|_{\widetilde{\mathcal{Y}}}^{2}\,\mathrm{d}\mu(X))\right)^{1/2} \approx\left(\sum_{i=1}^{m_{\rm test}}\|F(X_{i})\|_{\widetilde{\mathcal{Y}}}^{2 }w_{i}\right)^{1/2},\] where \(\mu=a_{d}\sharp_{\varrho d}\) is the pushforward measure defined in (A.1) and \(w_{i}\), \(i=1,\ldots,m_{\rm test}\) are the quadrature weights. We use these approximations to compute the relative \(L_{\mu}^{2}(\mathcal{X};\widetilde{\mathcal{Y}})\) error \[e_{F}^{\rm test}=\frac{\left(\sum_{i=1}^{m_{\rm test}}\|F(X_{i})-\widehat{F}( X_{i})\|_{\widetilde{\mathcal{Y}}}^{2}w_{i}\right)^{1/2}}{\left(\sum_{i=1}^{m_{ \rm test}}\|F(X_{i})\|_{\widetilde{\mathcal{Y}}}^{2}w_{i}\right)^{1/2}}.\] We use a high order isotropic Clenshaw Curtis sparse grid quadrature rule to evaluate \(e_{F}^{\rm test}\), as described in [2]. This method shows superior convergence over Monte Carlo integration to evaluate the global Bochner error. The sparse grid rule gives \(m_{\rm test}\) points at a level \(\ell\) for \(d\) dimensions. We rely on the TASMANIAN sparse grid toolkit [88, 89, 90] for the generation of the isotropic rule to study the generalization performance of the DNN.
* **Visualization**. The graphs in Figs. 1-3 show the geometric mean (the main curve) and plus/minus one (geometric) standard deviation (the shaded region). We use the geometric mean because our errors are plotted in logarithmic scale on the \(y\)-axis. See [3, Sec. A.1] for further discussion about this choice.

Appendix B Description of the parametric PDEs used in the numerical experiments and their discretization

In this section, we provide full details of the parametric PDEs considered in our numerical experiments. We also describe their variational formulations and numerical solution using FEM.

### Parametric coefficients

We consider two parametric coefficients of the form (A.1) in our numerical experiments. Our first coefficient is

\[a_{1}(\bm{z},\bm{x})=2.62+\sum_{j=1}^{\infty}x_{j}\frac{\sin(\pi z_{1}j)}{j^{3 /2}},\quad\forall\bm{z}\in\Omega,\] (B.1)

where \(x_{j}\in[-1,1]\), \(\forall j\). Our second example involves a log-transformed coefficient, which is a rescaling of an example from [76] of a diffusion coefficient with one-dimensional (layered) spatial dependence given by

\[\begin{split} a_{2}(\bm{z},\bm{x})&=\exp\left(1+x_{ 1}\left(\frac{\sqrt{\pi}\beta}{2}\right)^{1/2}+\sum_{j=2}^{\infty}\zeta_{j} \vartheta_{j}(\bm{z})x_{j}\right),\quad\forall\bm{z}\in\Omega,\\ \zeta_{j}&:=(\sqrt{\pi}\beta)^{1/2}\exp\left(\frac{- \left(\lfloor j/2\rceil\pi\beta\right)^{2}}{8}\right),\\ \vartheta_{j}(\bm{z})&:=\begin{cases}\sin\left( \lfloor j/2\rfloor\pi z_{1}/\beta_{p}\right)&\text{ if $j$ is even}\\ \cos\left(\lfloor j/2\rfloor\pi z_{1}/\beta_{p}\right)&\text{ if $j$ is odd}\end{cases},\end{split}\] (B.2)

where \(x_{j}\in[-1,1]\), \(\forall j\). Here we let \(\beta_{c}=1/8\), and \(\beta_{p}=\max\{1,2\beta_{c}\}\), \(\beta=\beta_{c}/\beta_{p}\).

In both cases we consider truncation of the expansion after \(d\) terms, giving the map \(a_{j,d}:[-1,1]^{d}\to\mathcal{X}\), with \(j=1\) corresponding to (B.1) and \(j=2\) corresponding to (B.2). Our input samples are then \(\{X_{i}=a_{j,d}(\bm{x}_{i})\}_{i=1}^{m}\subset\mathcal{X}\) with \(\bm{x}_{i}\in[-1,1]^{d}\) drawn identically and independently from \(\varrho_{d}\) and \(j\in\{1,2\}\). Note for the Boussinesq problem we also consider an additional parametric dependence in the tensor \(\mathbb{K}\) describing the thermal conductivity of the fluid. See SSB.5 and (B.17).

### Relevant spaces

We require several function space definitions for the development of the mixed variation formulations of the Poisson, Navier-Stokes-Brinkman and Boussinesq PDEs. Let \(\Omega\subset\mathbb{R}^{n}\), \(n\in\{2,3\}\), be the physical domain of a PDE. We write \(\mathrm{L}^{p}(\Omega)\), \(1\leq p\leq\infty\), for the \(L^{p}\)-space of scalar-valued functions with respect to the Lebesgue measure (to avoid confusion with the Bochner space \(L^{2}_{\mu}(\mathcal{X};\mathcal{Y})\)). We denote the standard Sobolev spaces as \(\mathrm{W}^{s,p}(\Omega)\) for \(s\in\mathbb{R}\) and \(p>1\), and write \(\mathrm{H}^{k}(\Omega)\) when \(p=2\) and \(s=k\). Additionally, we consider the space of traces of functions in \(\mathrm{H}^{1}(\Omega)\), denoted by \(\mathrm{H}^{1/2}(\partial\Omega)\), and its dual, \(\mathrm{H}^{-1/2}(\partial\Omega)\) (see, e.g., [11, Sec. 1.2] for further details). We also define the following closed subspace of \(\mathrm{H}^{1}(\Omega)\):

\[\mathrm{H}^{1}_{0}(\Omega):=\overline{C_{0}^{\infty}(\Omega)}^{\|\cdot\|_{1, \Omega}}.\]

Here \(\overline{C_{0}^{\infty}(\Omega)}^{\|\cdot\|_{1,\Omega}}\) denotes the closure of \(C_{0}^{\infty}(\Omega)\) (i.e., the space of \(C^{\infty}(\Omega)\) functions with compact support) with respect to the norm \(\|\cdot\|_{1,\Omega}\), which, for any \(v\in\mathrm{H}^{1}(\Omega)\), is given by

\[\|v\|_{1,\Omega}:=\left\{|v|_{1,\Omega}^{2}+\|v\|_{\mathrm{L}^{2}(\Omega)}^{2 }\right\}^{1/2},\quad\text{where }|v|_{1,\Omega}:=\|\nabla v\|_{\mathrm{L}^{2}( \Omega)}.\]

For scalar functions \(u\) and vector fields \(\bm{v}\), we use \(\nabla u\) and \(\mathrm{div}(\bm{v})\) to denote their gradient and divergence, respectively. For tensor fields \(\bm{\sigma}\) and \(\bm{\tau}\), represented by \((\sigma_{i,j})_{i,j=1}^{n}\) and \((\tau_{i,j})_{i,j=1}^{n}\), respectively, we define \(\mathbf{div}(\bm{\sigma})\) as the divergence operator \(\mathrm{div}\) acting along the rows of \(\bm{\sigma}\), and we define the trace and the tensor inner-product as

\[\mathrm{tr}(\bm{\sigma})=\sum_{i=1}^{n}\sigma_{i,i},\text{ and }\bm{\tau}:\bm{ \sigma}=\sum_{i,j=1}^{n}\tau_{i,j}\sigma_{i,j},\]

respectively. Furthermore, we introduce the notation \(\mathbf{L}^{p}(\Omega)\) and \(\mathbb{L}^{p}(\Omega)\) to represent the vectorial and tensorial counterparts of \(\mathrm{L}^{p}(\Omega)\), respectively, and \(\mathbf{H}^{1}(\Omega)\) and \(\mathbb{H}^{1}(\Omega)\) for the vectorial and tensorial counterparts of \(\mathrm{H}^{1}(\Omega)\), respectively. Keeping this in mind, we introduce the Banach spaces

\[\begin{split}\mathbf{H}(\mathrm{div}_{q};\Omega)&: =\Big{\{}\bm{v}\in\mathbf{L}^{2}(\Omega):\mathrm{div}(\bm{v})\in \mathrm{L}^{q}(\Omega)\Big{\}},\\ \mathbb{H}(\mathbf{div}_{q};\Omega)&:=\Big{\{}\bm{ \tau}\in\mathbb{L}^{2}(\Omega):\mathbf{div}(\bm{\tau})\in\mathbf{L}^{q}( \Omega)\Big{\}}\end{split}\] (B.3)

with norms

\[\begin{split}\|\bm{v}\|_{\mathbf{H}(\mathrm{div}_{q};\Omega)}& :=\,\|\bm{v}\|_{\mathbf{L}^{2}(\Omega)}\,+\,\|\mathrm{div}(\bm{v})\|_{ \mathrm{L}^{q}(\Omega)},\\ \|\bm{\tau}\|_{\mathrm{Bi}(\mathbf{div}_{q};\Omega)}& :=\,\|\bm{\tau}\|_{\mathbb{L}^{2}(\Omega)}\,+\,\|\mathbf{div}(\bm{ \tau})\|_{\mathrm{L}^{q}(\Omega)}\,.\end{split}\]

The cases of \(q=4/3\) and \(q=2\) appear in the mixed variational formulations of the considered PDEs, and for the latter we simply write \(\mathbf{H}(\mathrm{div};\Omega)\).

Often, under certain conditions, such as incompressibility conditions [33, eq.(2.4)], it is convenient to define variants of these spaces. For example, we define

\[\mathbb{L}^{2}_{\mathrm{tr}}(\Omega)\,:=\,\Big{\{}\bm{\tau}\in\mathbb{L}^{2}( \Omega):\mathrm{tr}(\bm{\tau})\,=\,0\Big{\}}\,,\] (B.4)

which represents the space of integrable functions with zero trace over \(\Omega\). Furthermore, given the decomposition (see, e.g., [32])

\[\mathbb{H}(\mathbf{div}_{4/3};\Omega)\,=\,\mathbb{H}_{0}(\mathbf{div}_{4/3}; \Omega)\,\oplus\,\mathrm{R}\,\mathbb{I}\,,\] (B.5)

we may also consider

\[\mathbb{H}_{0}(\mathbf{div}_{4/3};\Omega)\,:=\,\Big{\{}\bm{\tau}\in\mathbb{H}( \mathbf{div}_{4/3};\Omega):\int_{\Omega}\mathrm{tr}(\bm{\tau})=0\Big{\}}\,,\] (B.6)

as the space of elements in \(\mathbb{H}(\mathbf{div}_{4/3};\Omega)\) with zero mean trace. Finally, we define

\[\mathbb{L}^{2}_{\text{skew}}(\Omega)=\{\bm{\eta}\in\mathbb{L}^{2}(\Omega):\ \bm{\eta}+\bm{\eta}^{t}=0\},\] (B.7)

and the space of \(\mathrm{L}^{2}(\Omega)\) functions with zero integral over \(\Omega\) as

\[\mathrm{L}^{2}_{0}(\Omega)=\left\{\nu\in\mathrm{L}^{2}(\Omega):\ \int_{\Omega}\nu=0\right\}.\] (B.8)

### The parametric diffusion equation

We now describe our first example, which is the parametric elliptic diffusion equation. Let \(\Omega\subset\mathbb{R}^{2}\) be a bounded Lipschitz domain, \(\partial\Omega\) be the boundary of \(\Omega\), \(f\in\mathrm{L}^{2}(\Omega)\) and \(g\in\mathrm{H}^{1/2}(\partial\Omega)\). Given \(\bm{x}\in[-1,1]^{d}\), we consider the PDE

\[-\mathrm{div}(a(\bm{x})\nabla u(\bm{x})) =f,\quad\text{ in }\Omega\] (B.9) \[u(\bm{x}) =g,\quad\text{ on }\partial\Omega.\]

Here \(a(\bm{x})=a(\cdot,\bm{x})\in\mathrm{L}^{\infty}(\Omega)=:\mathcal{X}\) is the parametric diffusion coefficient. The terms \(f\) and \(g\) are not parametric.

#### b.3.1 Mixed variational formulation

Our first step in precisely defining the problem is to identify sufficient conditions for the solution map \(\bm{x}\mapsto u(\bm{x})\) to be well-defined. To do this, we diverge from the standard variational formulation involving the space \(\mathcal{Y}=\mathrm{H}^{1}_{0}(\Omega)\) (see Remark B.3) and instead consider a _mixed_ variational formulation of (B.9). Using a mixed formulation to study the solution of PDEs offers several benefits over the standard variational formulation. One key advantage is that it allows us to introduce additional variables that can be of physical interest. Additionally, mixed formulations can naturally accommodate different types of boundary conditions and introduce Dirichlet boundary conditions directly into the formulation rather than imposing them on the search space. For further details on mixed formulations, we refer to [32] and references within.

Assume that there exists \(r,M>0\) such that, for all \(\bm{x}\in[-1,1]^{d}\),

\[0<r\leq\mathrm{essinf}_{\bm{z}\in\Omega}a(\bm{z},\bm{x})=:a_{\min}(\bm{x})\text { and }a_{\max}(\bm{x}):=\mathrm{esssup}_{\bm{z}\in\Omega}a(\bm{z},\bm{x})\leq M.\] (B.10)

Then the problem can be recast as a first-order system: given \(\bm{x}\in[-1,1]^{d}\), find \((\bm{\sigma},u)(\bm{x})\in\mathbf{H}(\mathrm{div};\Omega)\times\mathrm{L}^{2} (\Omega)\) such that

\[d_{a(\bm{x})}(\bm{\sigma}(\bm{x}),\bm{\tau})+b(\bm{\tau},u(\bm{x })) =G(\bm{\tau}),\quad\forall\bm{\tau}\in\mathbf{H}(\mathrm{div};\Omega),\] (B.11) \[b(\bm{\sigma},v) =F(v),\quad\forall v\in\mathrm{L}^{2}(\Omega).\]

Here \(d\) and \(b\) are the bilinear forms defined by

\[d_{a(\bm{x})}(\bm{\sigma},\bm{\tau}) =\int_{\Omega}\frac{\bm{\sigma}\cdot\bm{\tau}}{a(\bm{x})},\quad \forall(\bm{\tau},\bm{\sigma})\in\mathbf{H}(\mathrm{div};\Omega)\times\mathbf{ H}(\mathrm{div};\Omega),\] (B.12) \[b(\bm{\tau},v) =\int_{\Omega}\mathrm{div}(\bm{\tau})v,\quad\forall(\bm{\tau},v) \in\mathbf{H}(\mathrm{div};\Omega)\times\mathrm{L}^{2}(\Omega)\]

and the functionals \(G\in(\mathbf{H}(\mathrm{div};\Omega))^{\prime}\) and \(J\in(\mathrm{L}^{2}(\Omega))^{\prime}\) are defined by

\[J(v)=-\int_{\Omega}fv,\quad\forall v\in\mathrm{L}^{2}(\Omega)\text{ and }G(\bm{\tau})=\langle\gamma(\bm{\tau})\cdot\bm{n},g\rangle_{1/2,\partial\Omega}, \quad\forall\bm{\tau}\in\mathbf{H}(\mathrm{div};\Omega).\] (B.13)

For the experiments in this work, we consider \(\Omega=(0,1)^{2}\) and \(f=10\). For the boundary condition, we consider a constant value \(u(\bm{z},\bm{x})=0.5\) on the bottom of the boundary \((0,1)\times\{0\}\), and zero boundary conditions on the remainder of the boundary.

#### b.3.2 Holomorphy assumption

Consider the affine parametrization (B.1). Setting \(M=2.7\) and observing that

\[\left|\sum_{j\in\mathbb{N}}x_{j}\frac{\sin(\pi z_{1}j)}{j^{3/5}}\right|\leq \sum_{j\in\mathbb{N}}\frac{1}{j^{3/5}}\approx 2.61238=2.62-r,\] (B.14)

for some \(r<0.00762\), we deduce that (B.10) holds, which makes the mixed variational formulation (B.11) well defined, i.e., for each \(\bm{x}\in[-1,1]^{\infty}\), there exists a unique solution \((\bm{\sigma},u)(\bm{x})\in\mathbf{H}(\mathrm{div};\Omega)\times\mathrm{L}^{2} (\Omega)\). Moreover, one can show that the parametric solution map \(\bm{x}\mapsto(\sigma,u)(\bm{x})\) is \((\bm{b},\varepsilon)\)-holomorphic for \(0<\varepsilon<0.00762\) and where \(\bm{b}=(b_{i})_{i=1}^{\infty}\) is given by \(b_{j}=\|\sin(\pi j\cdot)/j^{3/2}\|_{\mathrm{L}^{\infty}(\Omega)}=j^{-3/2}\). See [74, Prop. A.3.2].

In view of this property, this example falls within our theory. Note that \(\bm{b}\in\ell_{\mathsf{M}}^{p}(\mathbb{N})\) for every \(p<2/3\). Thus, we expect a theoretical rate of convergence with respect to the amount of training data that is arbitrarily close to \(m^{1/2-3/2}=m^{-1}\). This holomorphy result applies to the affine diffusion (B.1), not the log-transformed diffusion (B.2). However, we expect that it is possible to extend [74, Prop. A.3.2] to the latter case.

#### b.3.3 Finite element discretization

We use so-called _conforming_ Finite Element (FE) discretizations [18, Chp. 3]. Given a number of Degrees of Freedom (DoF) \(K\), this results in finite-dimensional spaces \(H_{K}\subseteq\mathbf{H}(\operatorname{div};\Omega)\) and \(Q_{K}\subseteq\operatorname{L}^{2}(\Omega)\). Specifically, we consider a regular triangulation \(\mathcal{T}_{K}\) of \(\overline{\Omega}\) made up of triangles of minimum diameter \(h_{\text{min}}=0.0844\) and maximum diameter \(h_{\text{max}}=0.1146\). This corresponds to a total number of DoF \(K=2622\). See Fig. 4 for an illustration of the FE mesh.

Fig. 5 shows a comparison between a reference solution computed by the FEniCS FEM solver and the approximation obtained by an ELU \(4\times 40\) DNN.

### Parametric Navier-Stokes-Brinkman equations

We next consider a parametric model describing the dynamics of a viscous fluid through porous media. Consider a bounded and Lipschitz physical domain \(\Omega\subseteq\mathbb{R}^{2}\). Given \(\bm{x}\in[-1,1]^{d}\), we consider the incompressible nonlinear stationary Navier-Stokes-Brinkman (NSB) equations: find

Figure 4: The domain \(\Omega\) and FE mesh for the parametric diffusion equation.

Figure 5: The solution \(\bm{u}(\bm{x})\) of the parametric Poisson problem in (B.9) for a given parameter \(\bm{x}=(1,0,0,0)^{\top}\) with affine coefficient \(a_{1,d}\) and \(d=4\), using a total of \(K=2622\) DoF. The left plot shows the solution given by the FEM solver. The right plot show the ELU \(4\times 40\) DNN approximation after \(60,000\) epochs of training with \(m=500\) sample points for training.

\(\bm{u}:[-1,1]^{d}\times\Omega\to\mathbb{R}^{2}\) and \(p:[-1,1]^{d}\times\Omega\to\mathbb{R}\) such that

\[\eta\bm{u}-\lambda\mathbf{div}(a(\bm{x})\bm{e}(\bm{u}(\bm{x})))+( \bm{u}(\bm{x})\cdot\nabla)\bm{u}(\bm{x})+\nabla p(\bm{x}) =f \text{in }\Omega\] (B.14) \[\operatorname{div}(\bm{u}(\bm{x})) =0 \text{in }\Omega\] \[\bm{u} =\begin{cases}\bm{u}_{D}&\text{on }\partial\Omega_{\mathrm{in}}\\ 0&\text{on }\partial\Omega_{\mathrm{wall}}\end{cases}\] \[(a(\bm{x})\nabla\bm{e}(\bm{u})-p\mathbb{I})\nu =0 \text{on }\partial\Omega_{\mathrm{out}}\] \[\int_{\Omega}p =0,\]

where \(\lambda=\text{Re}^{-1}\) and Re is the Reynolds number, \(a(\bm{x})=a(\cdot;\bm{x})\in\mathcal{X}:=\mathrm{L}^{\infty}(\Omega)\) and \(a:[-1,1]^{d}\times\Omega\to\mathbb{R}^{+}\) is the random viscosity of the fluid, \(\eta\in\mathbb{R}^{+}\) is the scaled inverse permeability of the porous media, \(\bm{u}\) is the velocity of the fluid, \(\bm{e}(\bm{u})=\frac{1}{2}(\nabla\bm{u}+(\nabla\bm{u})^{t})\) is the symmetric part of the gradient, \(p\) is the pressure of the fluid and \(f:\Omega\to\mathbb{R}\) is an external force independent of the parameters. Here, the fourth condition imposes a zero normal Cauchy stress

\[(a(\bm{x})\nabla\bm{e}(\bm{u})-p\mathbb{I})\nu=0\]

for the output boundary \(\partial\Omega_{\mathrm{out}}\).In addition, the incompressibility of the fluid imposes the following compatibility condition on \(\bm{u}_{D}\):

\[\int_{\Gamma}\bm{u}_{D}\cdot\bm{n}=0\quad\text{ on }\partial\Omega_{\mathrm{in}}.\]

The third condition also imposes a no-slip condition on the walls \(\Omega_{\mathrm{wall}}\)[34, eq.(2.3)].

#### b.4.1 Mixed variational formulation

The analysis of the detailed mixed formulation used for this problem in the nonparametric case can be found in [34]. Over the last decade, many works have used a mixed formulation employing a Banach space framework, allowing one to solve different PDEs in continuum mechanics in suitable Banach spaces. The advantage of this formulation is that no augmentation is required, the spaces are simpler and closer to the original model, and it allows one to obtain more direct approximations of the variables of physical interest [34, Sec. 1].

Based on the analysis in [34], the mixed variational formulation of the parametric NSB equations in (B.14) becomes: given \(\bm{x}\in[-1,1]^{d}\), find \((\bm{u},\bm{t},\bm{\sigma},\bm{\gamma})(\bm{x})\in\mathbf{L}^{4}(\Omega) \times\mathbb{L}^{2}_{\bm{u}}(\Omega)\times\mathbb{H}_{0}(\mathbf{div}_{4/3}; \Omega)\times\mathbb{L}^{2}_{\mathrm{skew}}(\Omega)\) such that

\[\lambda\int_{\Omega}a_{i}(\bm{x})\bm{t}(\bm{x}):\bm{s} \,-\,\int_{\Omega}\bm{s}:\bm{\sigma}(\bm{x})\,-\,\int_{\Omega}( \bm{u}\otimes\bm{u})(\bm{x}):\bm{s} = 0,\] \[\int_{\Omega}\bm{t}(\bm{x}):\bm{\tau}\,+\,\int_{\Omega}\bm{ \gamma}(\bm{x}):\bm{\tau}\,+\,\int_{\Omega}\bm{u}(\bm{x})\cdot\mathbf{div}( \bm{\tau}) = \langle\bm{\tau}\bm{n},\bm{u}_{D}\rangle_{\partial\Omega_{\mathrm{ in}}},\] (B.15) \[\int_{\Omega}\bm{\delta}:\bm{\sigma}(\bm{x})\,+\,\int_{\Omega} \bm{v}\cdot\mathbf{div}(\bm{\sigma}(\bm{x}))\,-\,\int_{\Omega}\eta\bm{u}(\bm{ x})\cdot\bm{v} = \int_{\Omega}\bm{f}\cdot\bm{v},\]

for all \((\bm{v},\bm{s},\bm{\tau},\bm{\delta})\in\mathbf{L}^{4}(\Omega)\times\mathbb{L} ^{2}_{\mathrm{tr}}(\Omega)\times\mathbb{H}_{0}(\mathbf{div}_{4/3};\Omega)\times \mathbb{L}^{2}_{\mathrm{skew}}(\Omega)\). Numerically, the skew-symmetry of \(\bm{\gamma}\) is imposed by searching for \(\gamma\in\mathrm{L}^{2}(\Omega)\) and setting

\[\bm{\gamma}=\begin{bmatrix}0&\gamma\\ -\gamma&0\end{bmatrix}.\]

Moreover, we impose the Neumann boundary condition via a Nietsche method as in [34, Sec. 5.2]. Specifically, we add

\[\kappa\langle(\bm{\sigma}+\bm{u}\otimes\bm{u})\bm{n}),\bm{\tau}\bm{n}\rangle_ {\partial\Omega_{\mathrm{out}}}=0\]

to the second equation where \(\kappa\gg 1\) is a large constant (e.g., \(\kappa=10^{4}\)). As usual in this formulation, the pressure \(p\in\mathrm{L}^{2}(\Omega)\) can be computed according to the post-processing formula

\[p=-\frac{1}{2}\mathrm{tr}(\bm{\sigma}+(\bm{u}\otimes\bm{u})).\]Note that above we omitted the term \(\bm{x}\) for simplicity.

In our experiments, we consider approximating solutions to the parametric NSB problem with \(\lambda=0.1\), a scaled inverse permeability of \(\eta=10+z_{1}^{2}+z_{2}^{2}\), an external force \(\bm{f}=(0,-1)^{\top}\) and random viscosity \(a_{j,d}\) as in (B.1)-(B.2) with \(j\in\{1,2\}\).

We consider the unit square \(\Omega=(0,1)^{2}\) as the domain, an inlet boundary \(\partial\Omega_{\mathrm{in}}=(0,1)\times\{1\}\), an outlet boundary \(\partial\Omega_{\mathrm{out}}=\{1\}\times(0,1)\) and walls \(\partial\Omega_{\mathrm{wall}}=\{0\}\times(0,1)\cup(0,1)\times\{0\}\). For simplicity, we use the same mesh as that of the previous example. See Fig. 4. On the Neumann boundary \(\partial\Omega_{\mathrm{out}}\) we consider a zero normal Cauchy stress, a Dirichlet condition \(\bm{u}_{D}=(0.0625)^{-1}((z_{2}-0.5)(1-z_{2}),0)\) on \(\partial\Omega_{\mathrm{in}}\) and a no-slip velocity on \(\partial\Omega_{\mathrm{wall}}\).

Fig. 6 provides a comparison between a reference solution of the vector field \(\bm{u}\) and pressure \(p\) computed by the FEniCS FEM solver and the approximation generated by a ELU \(4\times 40\) DNN.

**Remark B.1** (Other auxiliary variables): We report the performance of the DNNs approximating \((\bm{u},p)(\bm{x})\in\mathds{L}^{4}(\Omega)\times\mathds{L}^{2}(\Omega)\). Note that any solver based on the above formulation outputs several other variables, e.g., \((\bm{t},\bm{\sigma},\bm{\gamma})(\bm{x})\in\mathbb{L}_{\bm{u}}^{2}(\Omega) \times\mathbb{H}_{0}(\mathbf{div}_{4/3};\Omega)\times\mathbb{L}_{\text{skew}}^ {2}(\Omega)\). One could also approximate these auxiliary variables using DNNs. However, we restrict our experiments to \((\bm{u},p)\) as these are the primary variables of interest in the problem.

To conclude this discussion, in Fig. 7 we plot the numerical results for the approximation of the pressure \(p\) in the above problem. This complements Fig. 2, which showed results for the velocity field \(\bm{u}\). We once more observe similar results: ELU and tanh DNNs outperform ReLU DNNs, the rate of convergence appears to be close to \(\mathcal{O}(m^{-1})\) and there is no degradation with increasing parametric dimension \(d\).

### Parametric stationary Boussinesq equation

To recap, in our first example, we considered a mixed formulation of a parametric diffusion equation that provably satisfies the \((\bm{b},\varepsilon)\)-holomorphy assumption. Using this formulation, we considered

Figure 6: The solution \((\bm{u},p)(\bm{x})\) of the parametric NSB problem (B.14) for a given parameter \(\bm{x}=(1,0,0,0)^{\top}\) with affine coefficient \(a_{1,d}\) and \(d=4\), using a total of \(1464\) DoF for \(\bm{u}\) and \(244\) DoF for \(p\). The top row shows the solution given by the FEM solver, and the bottom row shows the ELU \(4\times 40\) DNN approximation after \(60,000\) epochs of training with \(m=500\) sample points. The left plots show the vector field \(\bm{u}\). The right plots show the points of highest pressure \(p\).

problems with nonzero Dirichlet boundary conditions, whereas previous works [2; 19; 27] study the more restrictive case of homogeneous Dirichlet boundary conditions, where \(u\in\mathrm{H}^{1}_{0}(\Omega)\). In our next example, we studied a more complicated parametric PDE, namely, the parametric NSB equations. While this example currently lacks a holomorphy guarantee, we observe a convergence rate that aligns with what we expect. We conjecture that the \(m^{-1}\) rate holds both for this and for even more complicated problems. To illustrate this claim with an example, we now consider a parametric coupled partial differential equation in three dimensions (\(\Omega\subset\mathbb{R}^{3}\)) with two random coefficients affecting different parts of the coupled problem. The nonparametric version of this problem is based on [20].

Specifically, we consider the Boussinesq formulation in [20] that combines a parametric incompressible Navier-Stokes equation with a parametric heat equation. The parametric dependence affects both equations. The Navier-Stokes equation is affected by a parametric variable multiplying the temperature-dependent viscosity, and the equation for heat flow is affected directly by the thermal conductivity of the fluid. To be more precise, given \(\bm{x}\in[-1,1]^{d}\), our goal is to find the velocity \(\bm{u}:[-1,1]^{d}\times\Omega\to\mathbb{R}^{2}\), pressure \(p:[-1,1]^{d}\times\Omega\to\mathbb{R}\) and temperature \(\varphi:[-1,1]^{d}\times\Omega\to\mathbb{R}\) of a fluid such that

\[-\mathbf{div}(2a(\bm{x})\varpi(\varphi(\bm{x}))\bm{e}(\bm{u}(\bm{ x})))+(\bm{u}(\bm{x})\cdot\nabla)\bm{u}(\bm{x})+\nabla p(\bm{x}) =\varphi(\bm{x})\bm{g}\quad\text{ in }\Omega,\] (B.16) \[\operatorname{div}(\bm{u}(\bm{x})) =0\quad\text{ in }\Omega,\] \[-\operatorname{div}(\mathbb{K}(\bm{x})\nabla\varphi(\bm{x}))+ \bm{u}(\bm{x})\cdot\nabla\varphi(\bm{x}) =0\quad\text{ in }\Omega,\] \[\bm{u} =\bm{u}_{D}\quad\text{ on }\partial\Omega,\] \[\varphi =\varphi_{D}\quad\text{ on }\partial\Omega,\] \[\int_{\Omega}p(\bm{x}) =0.\]

Here \(\bm{g}=(0,0,-1)^{\top}\) is a gravitational force and \(\mathbb{K}(\bm{x})=\mathbb{K}(\cdot;\bm{x})\in\mathbb{L}^{\infty}(\Omega)\), where \(\mathbb{K}:[-1,1]^{d}\times\Omega\to\mathbb{R}^{3\times 3}\) is a parametric uniformly-positive tensor describing the thermal conductivity of the fluid. It is given explicitly by

\[\mathbb{K}(\bm{z},\bm{x})=\left(1.89+\sum_{j\in\mathbb{N}}x_{j}\frac{\sin( \pi z_{3}j)}{j^{9/5}}\right)\begin{bmatrix}\exp(-z_{1})&0&0\\ 0&\exp(-z_{2})&0\\ 0&0&\exp(-z_{3})\end{bmatrix},\;\forall\bm{z}\in\Omega,\] (B.17)

for \(\bm{x}\in[-1,1]^{d}\). The term \(\varpi:\mathbb{R}\to\mathbb{R}^{+}\) is a temperature-dependent viscosity given by \(\varpi(\varphi)=0.1+\exp(-\varphi)\) and the term \(a(\bm{x})=a(\cdot;\bm{x})\in\mathbb{L}^{\infty}(\Omega)\), where \(a:[-1,1]^{d}\times\Omega\to\mathbb{R}\) is a parametric variable affecting the viscosity of the fluid. As in the previous example \(\bm{e}(\bm{u})\) is the symmetric part of \(\nabla\bm{u}\). Note that in this case, we have \((a(\bm{x}),\mathbb{K}(\bm{x}))\in\mathcal{X}:=\mathbb{L}^{\infty}(\Omega) \times\mathbb{L}^{\infty}(\Omega)\).

#### b.5.1 Fully mixed variational formulation

The complete derivation of a fully-mixed variational formulation for the non-parametric Boussinesq equation in Banach spaces can be found in [20, Sec. 3.1]. To make the presentation simpler we rewrite it for the parametric case. Given \(\bm{x}\in[-1,1]^{d}\), find \((\bm{u},\bm{t},\bm{\sigma},\varphi,\tilde{\bm{t}},\tilde{\bm{\sigma}})(\bm{x} )\in\mathbf{L}^{4}(\Omega)\times\mathbb{L}^{2}_{\mathrm{tr}}(\Omega)\times \mathbb{L}^{2}_{\mathrm{tr}}(\Omega)\times\mathbb{L}^{2}_{\mathrm{tr}}(\Omega)\).

Figure 7: The same as Fig. 2, except showing results for the pressure \(p\).

\[\mathbb{H}_{0}(\mathbf{div}_{4/3};\Omega)\times\mathrm{L}^{4}(\Omega) \times\mathbf{L}^{2}(\Omega)\times\mathbf{H}(\mathrm{div}_{4/3};\Omega)\text{ such that}\] \[-\int_{\Omega}\boldsymbol{v}\cdot\mathbf{div}(\boldsymbol{\sigma}( \boldsymbol{x}))\,+\,\frac{1}{2}\int_{\Omega}\boldsymbol{t}(\boldsymbol{x}) \boldsymbol{u}(\boldsymbol{x})\cdot\boldsymbol{v}\,-\,\int_{\Omega}\varphi( \boldsymbol{x})\boldsymbol{g}\cdot\boldsymbol{v} = 0\] \[\int_{\Omega}2a(\boldsymbol{x})\varpi(\varphi(\boldsymbol{x})) \boldsymbol{t}_{\mathrm{sym}}(\boldsymbol{x}):\boldsymbol{s}\,-\,\frac{1}{2} \int_{\Omega}(\boldsymbol{u}\otimes\boldsymbol{u})(\boldsymbol{x}): \boldsymbol{s} = \int_{\Omega}\boldsymbol{\sigma}(\boldsymbol{x}):\boldsymbol{s}\] \[\int_{\Omega}\boldsymbol{\tau}:\boldsymbol{t}(\boldsymbol{x})\,+ \,\int_{\Omega}\boldsymbol{u}(\boldsymbol{x})\cdot\mathbf{div}(\boldsymbol{ \tau}) = \langle\boldsymbol{\tau}\boldsymbol{\nu},\boldsymbol{u}_{D}\rangle_{\Gamma}\] \[-\int_{\Omega}\psi\,\mathrm{div}(\tilde{\boldsymbol{\sigma}}( \boldsymbol{x}))\,+\,\frac{1}{2}\int_{\Omega}\psi(\boldsymbol{x}) \boldsymbol{u}(\boldsymbol{x})\cdot\tilde{\boldsymbol{t}} = 0\] (B.18) \[\int_{\Omega}\mathbb{K}(\boldsymbol{x})\tilde{\boldsymbol{t}}( \boldsymbol{x})\cdot\tilde{\boldsymbol{s}}-\frac{1}{2}\int_{\Omega}\varphi( \boldsymbol{x})\boldsymbol{u}(\boldsymbol{x})\cdot\tilde{\boldsymbol{s}} = \int_{\Omega}\tilde{\boldsymbol{\sigma}}(\boldsymbol{x})\cdot \tilde{\boldsymbol{s}}\] \[\int_{\Omega}\tilde{\boldsymbol{\tau}}\cdot\tilde{\boldsymbol{t} }(\boldsymbol{x})\,+\,\int_{\Omega}\varphi(\boldsymbol{x})\,\mathrm{div}( \tilde{\boldsymbol{\tau}}) = \langle\tilde{\boldsymbol{\tau}}\cdot\boldsymbol{\nu},\varphi_{D}\rangle_ {\Gamma}\] \[\int_{\Omega}\mathrm{tr}(2\boldsymbol{\sigma}+\boldsymbol{u} \otimes\boldsymbol{u})(\boldsymbol{x}) = 0,\]

for all \((\boldsymbol{v},\boldsymbol{s},\boldsymbol{\tau},\psi,\tilde{\boldsymbol{\sigma }},\tilde{\boldsymbol{\tau}})\in\mathbf{L}^{4}(\Omega)\times\mathbb{L}^{2}_{ \mathrm{tr}}(\Omega)\times\mathbb{H}_{0}(\mathbf{div}_{4/3};\Omega)\times \mathrm{L}^{4}(\Omega)\times\mathbf{L}^{2}(\Omega)\times\mathbf{H}(\mathrm{ div}_{4/3};\Omega)\). Here \(p\in\mathrm{L}^{2}_{0}(\Omega)\) can be recovered as

\[p=-\frac{1}{6}\mathrm{tr}(2\sigma+2c\mathbb{I}+\boldsymbol{u}\otimes \boldsymbol{u}),\quad\text{where }c=-\frac{1}{6|\Omega|}\int_{\Omega}\mathrm{tr}( \boldsymbol{u}\otimes\boldsymbol{u}).\] (B.19)

As in the previous example, we omitted the term \(\boldsymbol{x}\) from this equation for simplicity. For further details on this formulation we refer to [20] and references within.

Given \(\boldsymbol{x}\in[-1,1]^{d}\), we approximate the solution \((\boldsymbol{u},p,\varphi)(\boldsymbol{x})\in(\mathbf{L}^{4}(\Omega)\times \mathrm{L}^{2}_{0}(\Omega)\times\mathrm{L}^{4}(\Omega))\) of (B.18) by using DNNs and study the approximation capabilities as we increase the number of training samples \(m\). As in the previous example (see Remark B.1), we do not aim to approximate the other variables \((\boldsymbol{t},\boldsymbol{\sigma},\tilde{\boldsymbol{t}},\tilde{\boldsymbol {\sigma}})(\boldsymbol{x})\in\mathbb{L}^{2}_{\mathrm{tr}}(\Omega)\times \mathbb{H}_{0}(\mathbf{div}_{4/3};\Omega)\times\mathbf{L}^{2}(\Omega)\times \mathbf{H}(\mathrm{div}_{4/3};\Omega)\).

In our experiments, we consider the unit cube \(\Omega=(0,1)^{3}\) as the domain in \(\mathbb{R}^{3}\). We consider a nonzero boundary condition \(u_{D}=(1,1,0)\) on the bottom face of the cube \(\partial\Omega_{\mathrm{bottom}}=(0,1)\times(0,1)\times\{0\}\), and zero on the other faces. We set \(\varphi_{D}=\exp(4(-(z_{1}-0.5)^{2}-(z_{2}-0.5)^{2}))\) on \(\partial\Omega_{\mathrm{bottom}}\) and zero otherwise. For simplicity, we consider the same parametric coefficients \(a_{1,d}\) and \(a_{2,d}\) given by (B.1) and (B.2), respectively. See Fig. 8 for an example of the solution \((\boldsymbol{u},p,\varphi)(\boldsymbol{x})\) for a given \(\boldsymbol{x}\in[-1,1]^{4}\).

To conclude this section, we provide a comparison of the performance of the DNN architectures in approximating the velocity field \(\boldsymbol{u}\) and pressure \(p\) for the Boussinesq PDE. This complements Fig. 3, which showed results for the temperature \(\varphi\). As with \(\varphi\), the convergence rate for \(p\) agrees roughly with the rate \(m^{-1}\), and does not appear to deteriorate with the parametric dimension \(d\). On the other hand, the convergence rate for the velocity field \(\boldsymbol{u}\) is somewhat slower.

## Appendix C Overview of the proofs

In this section, we first introduce additional notation that is needed for the proofs of the main results. We then give a brief overview of the proofs.

### Additional notation

#### c.1.1 Lipschitz constants

Let \((\mathcal{X},\|\cdot\|_{\mathcal{X}})\) and \((\mathcal{Y},\|\cdot\|_{\mathcal{Y}})\) be Banach spaces, \(G:\mathcal{X}\to\mathcal{Y}\) and \(\mathcal{B}\subseteq\mathcal{X}\). We define the Lipschitz constant as \(L=\mathrm{Lip}(G;\mathcal{B},\mathcal{Y})\) as the smallest constant \(L\geq 0\) such that

\[\left\|G(X^{\prime})-G(X)\right\|_{\mathcal{Y}}\leq L\|X^{\prime}-X\|_{ \mathcal{X}},\quad\forall X,X^{\prime}\in\mathcal{B}.\]

#### c.1.2 Sequence spaces

We require some notation for sequences. Let \((\mathcal{Z},\left\|\cdot\right\|_{\mathcal{Z}})\) be a Banach space, \(d=\mathbb{N}\cup\{\infty\}\) and write \(\boldsymbol{\nu}=(\nu_{k})_{k=1}^{d}\) for an arbitrary multi-index in \(\mathbb{N}_{0}^{d}\). If \(\Lambda\subseteq\mathbb{N}_{0}^{d}\) is a finite or countable set of multi-indices and \(0<p\leq\infty\) we define the space \(\ell^{p}(\Lambda;\mathcal{Z})\) as the set of all \(\mathcal{Z}\)-valued sequences \(\boldsymbol{c}=(c_{\boldsymbol{\nu}})_{\boldsymbol{\nu}\in\Lambda}\) for which \(\left\|\boldsymbol{c}\right\|_{p;\mathcal{Z}}<\infty\), where

\[\left\|\boldsymbol{c}\right\|_{p;\mathcal{Z}}=\begin{cases}\left(\sum_{ \boldsymbol{\nu}\in\Lambda}\left\|c_{\boldsymbol{\nu}}\right\|_{\mathcal{Z}} ^{p}\right)^{1/p}&0<p<\infty,\\ \sup_{\boldsymbol{\nu}\in\Lambda}\left\|c_{\boldsymbol{\nu}}\right\|_{\mathcal{Z }}&p=\infty.\end{cases}\]

Figure 8: The solution \((\boldsymbol{u},\varphi,p)(\boldsymbol{x})\) to the parametric Boussinesq problem in (B.18) for a given parameter \(\boldsymbol{x}=(1,0,0,0)^{\top}\) with affine coefficient \(a_{1,d}\) and \(d=4\), using a total of \(18,480\) DoF for \(\boldsymbol{u}\) and \(528\) DoF for both \(\varphi\) and \(p\). The top row shows the solution given by the FEM solver and the bottom row shows the \(4\times 40\) ELU–DNN approximation after \(60,000\) epochs of training with \(m=500\) sample points. The left plots show streamlines of the vector field \(\boldsymbol{u}\) and their directions indicated with coloured arrows. The middle plots visualize the temperature distribution inside the cube using coloured spheres, with the hottest region at the centre of the cube. The right plots illustrate the points of highest pressure \(p\).

[MISSING_PAGE_EMPTY:28]

#### c.1.5 Miscellaneous

Given an optimization problem \(\min_{t}f(t)\), we say that \(\hat{t}\) is a _\((\sigma,\tau)\)-approximate minimizer_ for some \(\sigma\geq 1\) and \(\tau\geq 0\) if \(f(\hat{t})\leq\sigma^{2}\min_{t}f(t)+\tau^{2}\). (In the main paper, we consider only \(\sigma=1\), but for the proofs it is useful to allow \(\sigma>1\)).

Finally, for convenience, given \(X_{1},\ldots,X_{m}\in\mathcal{X}\), we define the semi-norm

\[\left|\kern-1.075pt\left|\kern-1.075pt\left|G\right|\kern-1.075pt\right|\kern-1.075pt \right|_{\text{disc},\mu}=\sqrt{\frac{1}{m}\sum_{i=1}^{m}\left\|G(X_{i})\right\| _{\mathcal{Y}}^{2}}\]

of an operator \(G:\mathcal{X}\to\mathcal{Y}\) and

\[\left|\kern-1.075pt\left|g\right|\kern-1.075pt\right|_{\text{disc},\tilde{ \varsigma}}=\left|\kern-1.075pt\left|g\circ\mathcal{E}_{\mathcal{X}}\right| \kern-1.075pt\right|\kern-1.075pt\right|_{\text{disc},\mu}=\sqrt{\frac{1}{m} \sum_{i=1}^{m}\left\|g\circ\mathcal{E}_{\mathcal{X}}(X_{i})\right\|_{ \mathcal{Y}}^{2}}\]

of a function \(g:\mathbb{R}^{\mathbb{N}}\to\mathcal{Y}\). Here, as in (A.III), \(\tilde{\varsigma}\) denotes the pushforward measure \(\mathcal{E}_{\mathcal{X}}\#\mu\).

#### c.2 Overview of the proofs

#### c.2.1 Theorem 3.1

Theorems 3.1-3.2 are based on polynomials, and specifically, procedures for learning efficient Legendre polynomial approximations to holomorphic operators. As observed, these results are based on recent work on learning holomorphic, Banach-valued functions [2, 5, 6]. See also Remark C.1 below.

The proof of Theorem 3.1 involves three mains steps.

1. Formulation (SSD.1) and analysis (SSD.2-D.4) of a suitable polynomial learning procedure.
2. Construction of a family of DNNs that approximately emulates the Legendre polynomials (SSD.5).
3. Analysis of the corresponding training problem (2.5) (SSD.6-D.8).

Since our goal in this work is 'agnostic' DNN architectures (i.e., independent of the smoothness of the underlying operator), in step (a) we first define a nonlinear set (D.2) spanned by Legendre polynomials with nonzero indices in certain sets of bounded weighted cardinality. This is effectively a form of _sparse polynomial approximation_, and the analysis of the resulting learning procedure (D.3) relies heavily on techniques from compressed sensing. In order to bound the encoding-decoding error, we also require several results on Lipschitz continuity (Lemma D.2) and norm equivalences (Lemma D.4) for multivariate polynomials.

Step (b) relies on what have now become fairly standard results in DNN approximation theory: namely, the approximate emulation of orthogonal polynomials via DNNs of given width and depth. We present such a result in Lemma D.9, then use this to define the DNN family \(\mathcal{N}\) in (D.19).

We then analyze the DNN training problem (2.5) in step (c). Using the emulation result of step (b), we first show that any approximate minimizer \(\widetilde{N}\) of (2.5) yields a polynomial that is also an approximate minimizer of the polynomial training problem (D.3) (Lemma D.12). We may then apply the results shown in Step (a) to prove a generalization bound (Theorem D.13). Up to this point, we have not used the holomorphy assumption. We now use this assumption to bound the various best polynomial approximation errors that arise in the previously-derived generalization bound (SSD.7). Finally, in SSD.8 we put all these estimates together to complete the proof.

**Remark C.1**: Theorem 3.1 is a generalization and improvement of [5, Thms. 4.1 & 4.2], which deals with the case of learning Banach-valued functions rather than operators. Specifically, the setting of [5] can be considered a special case of this paper where \(\mathcal{X}=\ell^{\infty}(\mathbb{N})\) and the encoding error \(E_{\mathcal{X},q}=0\). Moreover, Theorem 3.1 improves the main results of [5] in three key ways. First, the the DNN architectures bounds are much narrower: \(\operatorname{width}(\mathcal{N})\lesssim(m/L)^{1+\delta}\) versus \(\operatorname{width}(\mathcal{N})\lesssim m^{3+\log_{2}(m)}\) in the latter. Second, Theorem 3.1 considers standard training, i.e., \(\ell^{2}\)-loss minimization, whereas [5, Thms. 4.1 & 4.2] requires regularization. Third, for Banach spaces, the error decay rate with respectto \(m\) is roughly doubled: \(E_{\text{app},2}=\mathcal{O}(m^{1-1/p})\) in Theorem 3.1 versus \(\mathcal{O}(m^{1/2(1-1/p)})\) in [5, Thm. 4.1]. Finally, Theorem 3.1 also provides bounds in the \(L_{\mu}^{\infty}\)-norm, whereas the results in [5] only consider the \(L_{\mu}^{2}\)-norm.

#### c.2.2 Theorem 3.2

The proof of Theorem 3.2 relies of three key steps.

1. Using a minimizer of the polynomial training problem (D.3) to construct a family of minimizers of the DNN training problem (2.5).
2. Analysis of the corresponding minimizers using the previously-derived bound for polynomial minimizers.
3. Using the Lipschitz continuity of DNNs to show stability of the DNN minimizer and a permutation argument to show the existence of many parameters that lead to equally 'good' minimizers.

Given any minimizer of the polynomial training problem (D.3), it is straightforward to define a DNN with the desired generalization bound. Unfortunately, this will generally not be a minimizer of (2.5). To achieve the aims of step (a) we proceed as follows. First, we note that a DNN will be a minimizer if the corresponding approximation satisfies \(\widehat{F}(X_{i})=\widetilde{Y}_{i}\), where \(\widetilde{Y}_{i}\) are the closest points to the \(Y_{i}\) from \(\widetilde{\mathcal{Y}}=\mathcal{D}_{\mathcal{Y}}(\mathbb{R}^{d_{\mathcal{Y}}})\). To achieve this, we take the existing DNN then add on a suitable number of additional terms corresponding to the first \(r>m\) order-one Legendre polynomials (SSE.1). We show that by doing this, we can construct a DNN for which \(\widehat{F}(X_{i})=\widetilde{Y}_{i}\) (Lemma E.1).

In Step (b), we first bound this DNN minimizer in terms of the polynomial minimizer plus the contributions of these additional terms (Lemma E.2). The latter involves the minimal singular value of a certain \(m\times r\) matrix \(\boldsymbol{B}\), which is the matrix of the linear system that enforces the condition \(\widehat{F}(X_{i})=\widetilde{Y}_{i}\). We bound this minimal singular value in SSE.3.

We then use this to complete the proof of part (A) of Theorem 3.2 in SSE.4. In this section, we also complete step (c) to establish parts (B) and (C).

**Remark C.2**: Like Theorem 3.1, Theorem 3.2 also relies on ideas from [5]. However, [5] does not address fully-connected DNN architectures. To address this challenge, the proof of Theorem 3.2 involves the technical construction described above.

#### c.2.3 Theorem 4.1

Theorems 4.1 is based on [7, Thm. 4.4]. The basic idea is to consider a family of affine, holomorphic operators (F.4). This allows us to lower bound the quantity \(\theta_{m}(\boldsymbol{b})\) by the so-called _Gelfand width_ (F.1) of a certain weighted unit ball in a finite-dimensional space. Bounds for such Gelfand widths are known, and this allows us to derive the corresponding result. The main difference between this and [7, Thm. 4.4] is the setup leading to the construction in (F.4).

#### c.2.4 Theorem 4.2

Theorems 4.2 employs similar ideas, but in a more technical manner. We consider a family of linear, holomorphic operators (G.2), which involves a sum over \(r\) groups of \(m+1\) coefficients. We restrict to coefficients that lie in the null space of the corresponding sampling operator. Then, through a series of inequalities, we can lower bound \(\tilde{\theta}_{m}(\boldsymbol{b})\) by a sum over \(r\) terms, each involving the \(\ell^{1}\)-norms of certain vectors in the null space of the matrix of the corresponding sampling operator (G.3). This matrix is a subgaussian random matrix. We now use a technical estimate from [75] for vectors in the null space of subgaussian random matrices, which shows that they cannot be'spiky'. Applying this and a series of further inequalities yields the result.

## Appendix D Proof of Theorem 3.1

### Formulation of an approximate polynomial training problem

Let \(\Lambda\subset\mathcal{F}\) with \(\operatorname{supp}(\boldsymbol{\nu})\subseteq\{1,\ldots,d_{\mathcal{X}}\}\), \(\forall\boldsymbol{\nu}\in\Lambda\), and write \(N=|\Lambda|\). Let \(k>0\) and define the set

\[\mathcal{S}=\mathcal{S}_{\Lambda,k}=\{S\subseteq\Lambda:|S|_{\boldsymbol{v}} \leq k\}.\] (D.1)Both \(\Lambda\) and \(k\) will be chosen later in the proof. For any Banach space \((\mathcal{Z},\|\cdot\|_{\mathcal{Z}})\) define the space

\[\mathcal{P}_{\mathcal{S};\mathcal{Z}}=\left\{\sum_{\boldsymbol{\nu}\in S}c_{ \boldsymbol{\nu}}\Psi_{\boldsymbol{\nu}}:c_{\boldsymbol{\nu}}\in\mathcal{Z},S \in\mathcal{S}\right\}.\] (D.2)

Then, given the training data (2.4), consider the problem

\[\min_{p\in\mathcal{P}_{\mathcal{S};\mathcal{Y}}}\frac{1}{m}\sum_{i=1}^{m}\|Y_{ i}-p\circ\mathcal{E}_{\mathcal{X}}(X_{i})\|_{\mathcal{Y}}^{2},\] (D.3)

where \(\widetilde{\mathcal{Y}}=\mathcal{D}_{\mathcal{Y}}(\mathbb{R}^{d_{\mathcal{Y} }})\). Here and throughout the proofs, we slightly abuse notation: the polynomial \(p:\mathbb{R}^{\mathbb{N}}\to\mathbb{R}\), whereas \(\mathcal{E}_{\mathcal{X}}\) has codomain \(\mathbb{R}^{d_{\mathcal{X}}}\). However, by construction, \(p\) is independent of all but the first \(d_{\mathcal{X}}\) variables. Hence we may consider \(p\) as a function \(\mathbb{R}^{d_{\mathcal{X}}}\to\mathbb{R}\). This aside, if \(\hat{p}\) is an approximate minimizer of (D.3), then we define the approximation to \(F\) as

\[F\approx\widehat{F}=\hat{p}\circ\mathcal{E}_{\mathcal{X}}.\] (D.4)

Notice that \(p\circ\mathcal{E}_{\mathcal{X}}=\mathcal{D}_{\mathcal{Y}}\circ P\circ\mathcal{ E}_{\mathcal{X}}\), where \(P:\mathbb{R}^{d_{\mathcal{X}}}\to\mathbb{R}^{d_{\mathcal{Y}}}\) is a vector-valued polynomial, since, by (A.IV), the map \(\mathcal{D}_{\mathcal{Y}}\) is linear. The idea exploited in this proof is to construct a class of DNNs \(\mathcal{N}\) such that (i) all such polynomials \(P\) are approximated by members of \(\mathcal{N}\) and (ii) the polynomial training problem (D.3) is approximated by the DNN training problem (2.5). The first step in this analysis is therefore to analyze the polynomial training problem (D.3).

### Supporting lemmas

We require several lemmas. The first relates the \(L_{\varrho}^{\infty}\)-norm of a polynomial to its Pettis \(L_{\varrho}^{2}\)-norm.

**Lemma D.1** (Nikolskii inequality for polynomials).: _Let \((\mathcal{Z},\|\cdot\|_{\mathcal{Z}})\) be any Banach space and \(p=\sum_{\boldsymbol{\nu}\in S}c_{\boldsymbol{\nu}}\Psi_{\boldsymbol{\nu}}\) for some finite set \(S\subset\mathcal{F}\), where \(c_{\boldsymbol{\nu}}\in\mathcal{Z}\). Then_

\[\|p\|_{L_{\varrho}^{\infty}(D;\mathcal{Z})}\leq\sqrt{|S|_{\boldsymbol{u}}}\|p \|_{L_{\varrho}^{2}(D;\mathcal{Z})}.\]

Proof.: By definition

\[\|p\|_{L_{\varrho}^{\infty}(D;\mathcal{Z})}=\|p\|_{L_{\varrho}^{\infty}(D; \mathcal{Z})}=\sup_{z^{*}\in B(\mathcal{Z}^{*})}\|z^{*}(p)\|_{L_{\varrho}^{ \infty}(D)}.\]

Fix \(z^{*}\in B(\mathcal{Z}^{*})\) and write \(z^{*}(p)=\sum_{\boldsymbol{\nu}\in S}z^{*}(c_{\boldsymbol{\nu}})\Psi_{ \boldsymbol{\nu}}\). By the triangle inequality and the definition of the weights (C.3), we have

\[\|z^{*}(p)\|_{L_{\varrho}^{\infty}(D)}\leq\sum_{\boldsymbol{\nu}\in S}|z^{*}( c_{\boldsymbol{\nu}})|\|\Psi_{\boldsymbol{\nu}}\|_{L_{\varrho}^{\infty}(D)}= \sum_{\boldsymbol{\nu}\in S}u_{\boldsymbol{\nu}}|z^{*}(c_{\boldsymbol{\nu}})|.\]

We now apply the Cauchy-Schwarz inequality, (C.1) and Parseval's identity to get

\[\|z^{*}(p)\|_{L_{\varrho}^{\infty}(D)}\leq\sqrt{|S|_{\boldsymbol{u}}}\sqrt{ \sum_{\boldsymbol{\nu}\in S}|z^{*}(c_{\boldsymbol{\nu}})|^{2}}=\sqrt{|S|_{ \boldsymbol{u}}}\|z^{*}(p)\|_{L_{\varrho}^{2}(D)}.\]

Since \(z^{*}\) was arbitrary, we deduce that

\[\|p\|_{L_{\varrho}^{\infty}(D;\mathcal{Z})}\leq\sqrt{|S|_{\boldsymbol{u}}} \sup_{z^{*}\in B(\mathcal{Z}^{*})}\|z^{*}(p)\|_{L_{\varrho}^{2}(D)}=\sqrt{|S|_ {\boldsymbol{u}}}\|p\|_{L_{\varrho}^{2}(D;\mathcal{Z})},\]

as required. 

Next, we require the following bound on the Lipschitz constant of a multivariate polynomial.

**Lemma D.2** (Lipschitz continuity for polynomials).: _Let \((\mathcal{Z},\|\cdot\|_{\mathcal{Z}})\) be any Banach space and suppose that \(p=\sum_{\boldsymbol{\nu}\in S}c_{\boldsymbol{\nu}}\Psi_{\boldsymbol{\nu}}\) for some finite \(S\subset\mathcal{F}\), where \(c_{\boldsymbol{\nu}}\in\mathcal{Z}\). Then satisfies_

\[\operatorname{Lip}(p;B^{\infty}(\mathbb{N}),\mathcal{Z})\leq\frac{1}{2}\sqrt{|S| _{\boldsymbol{\nu}}}\cdot\|p\|_{L_{\varrho}^{2}(D;\mathcal{Z})},\]

_where \(B^{\infty}(\mathbb{N})=\{\boldsymbol{x}\in\mathbb{R}^{\mathbb{N}}:\| \boldsymbol{x}\|_{\infty}\leq 1\}\) is the unit ball of \(\ell^{\infty}(\mathbb{N})\)._Proof.: Fix \(z^{*}\in B(\mathcal{Z}^{*})\) and let \(\tilde{p}=z^{*}(p)=\sum_{\bm{\nu}\in S}z^{*}(c_{\bm{\nu}})\Psi_{\bm{\nu}}=:\sum_{ \bm{\nu}\in S}\tilde{c}_{\bm{\nu}}\Psi_{\bm{\nu}}\) be the corresponding scalar-valued polynomial. Let \(\bm{x},\bm{x}^{\prime}\in D\). Then the mean value theorem gives that

\[\tilde{p}(\bm{x}^{\prime})-\tilde{p}(\bm{x})=\sum_{i=1}^{\infty}(x_{i}^{\prime }-x_{i})\frac{\partial\tilde{p}}{\partial x_{i}}(t\bm{x}+(1-t)\bm{x}^{\prime})\]

for some \(0\leq t\leq 1\). Since \(B^{\infty}(\mathbb{N})\equiv D\), this and the fact that \(D\) is convex give that

\[|\tilde{p}(\bm{x}^{\prime})-\tilde{p}(\bm{x})|\leq\|\bm{x}^{\prime}-\bm{x}\|_{ \infty}\sum_{i=1}^{\infty}\left\|\frac{\partial\tilde{p}}{\partial x_{i}} \right\|_{L^{\infty}_{\varrho}(D)}.\] (D.5)

We now consider the terms in the sum separately. Using the definition of the Legendre polynomials (see SSC.1.4), we have

\[\frac{\partial\tilde{p}}{\partial x_{i}}=\sum_{\bm{\nu}\in S}\tilde{c}_{\bm{ \nu}}u_{\bm{\nu}}P^{\prime}_{\nu_{i}}(x_{i})\prod_{j\neq i}P_{\nu_{j}}(x_{j}).\]

The unnormalized Legendre polynomials satisfy \(1=P_{n}(1)=\left\|P_{n}\right\|_{L^{\infty}([-1,1])}\) and \(n(n+1)/2=P^{\prime}_{n}(1)=\left\|P^{\prime}_{n}\right\|_{L^{\infty}([-1,1])}\). Hence

\[\left\|\frac{\partial\tilde{p}}{\partial x_{i}}\right\|_{L^{\infty}_{\varrho} (D)}\leq\sum_{\bm{\nu}\in S}|\tilde{c}_{\bm{\nu}}|u_{\bm{\nu}}\nu_{i}(\nu_{i}+ 1)/2.\]

We deduce that

\[\sum_{i=1}^{\infty}\left\|\frac{\partial\tilde{p}}{\partial x_{i}}\right\|_{ L^{\infty}_{\varrho}(D)}\leq\sum_{\bm{\nu}\in S}|\tilde{c}_{\bm{\nu}}|u_{\bm{ \nu}}\sum_{i=1}^{\infty}\nu_{i}(\nu_{i}+1)/2.\]

Now, by definition of the weights \(u_{\bm{\nu}}\) and \(v_{\bm{\nu}}\) (see (C.3) and (C.4)), we have

\[(\nu_{i}+1)\leq\prod_{j\in\mathbb{N}}(2\nu_{j}+1)=u_{\bm{\nu}}^{2}\]

and

\[\sum_{i=1}^{\infty}\nu_{i}\leq\prod_{j\in\mathbb{N}}(2\nu_{j}+1)=u_{\bm{\nu}} ^{2}.\]

Hence

\[\sum_{i=1}^{\infty}u_{\bm{\nu}}\nu_{i}(\nu_{i}+1)\leq u_{\bm{\nu}}^{5}\leq v_{ \bm{\nu}}.\]

Here we also used the fact that \(\bm{u}\geq\bm{1}\). We now apply this, the Cauchy-Schwarz inequality and Parseval's identity to get

\[\sum_{i=1}^{\infty}\left\|\frac{\partial\tilde{p}}{\partial x_{i}}\right\|_{L^ {\infty}_{\varrho}(D)}\leq\frac{1}{2}\|\tilde{p}\|_{L^{2}_{\varrho}(D)}\sqrt{ |S|_{\bm{\nu}}}.\]

Substituting this into (D.5) now gives

\[|\tilde{p}(\bm{x}^{\prime})-\tilde{p}(\bm{x})|\leq\frac{1}{2}\|\tilde{p}\|_{L^ {2}_{\varrho}(D)}\sqrt{|S|_{\bm{\nu}}}\|\bm{x}^{\prime}-\bm{x}\|_{\infty}.\]

We now recall that \(\tilde{p}=z^{*}(p)\) and \(z^{*}\in B(\mathcal{Z}^{*})\) was arbitrary to get

\[\left\|p(\bm{x}^{\prime})-p(\bm{x})\right\|_{\mathcal{Z}} =\sup_{z^{*}\in B(\mathcal{Z}^{*})}|z^{*}(p(\bm{x}^{\prime})-p( \bm{x}))|\] \[\leq\frac{1}{2}\sup_{z^{*}\in B(\mathcal{Z}^{*})}\left\|z^{*}(p) \right\|_{L^{2}_{\varrho}(D)}\sqrt{|S|_{\bm{\nu}}}\|\bm{x}^{\prime}-\bm{x}\|_{\infty}\] \[=\frac{1}{2}\|p\|_{L^{2}_{\varrho}(D;\mathcal{Z})}\sqrt{|S|_{\bm{ \nu}}}\|\bm{x}^{\prime}-\bm{x}\|_{\infty},\]

as required.

We now show that this result can be used to imply a norm equivalence for polynomials. For this we first require the following lemma. Note that in this and subsequent results, we abuse notation and write \(\tilde{\varsigma}\) for both the measure on \([-1,1]^{d_{\mathcal{X}}}\) defined in (A.III) and the measure on \(D=[-1,1]^{\mathbb{N}}\) defined by tensoring this measure (corresponding to the first \(d_{\mathcal{X}}\) variables \(x_{1},\ldots,x_{d_{\mathcal{X}}}\)) with the uniform measure on \(D\backslash[-1,1]^{d_{\mathcal{X}}}\) (corresponding to the remaining variables \(x_{d_{\mathcal{X}}+1},x_{d_{\mathcal{X}}+2},\ldots\)).

**Lemma D.3** (Closeness of \(L^{2}\) norms).: _Let \((\mathcal{Z},\|\cdot\|_{\mathcal{Z}})\) be any Banach space, \(\varsigma\) be the measure defined in (A.I) and \(\tilde{\varsigma}\) be the measure defined in (A.III). Suppose that \(f\in L^{2}_{\varsigma}(D;\mathcal{Z})\) is Lipschitz continuous with constant \(L=\operatorname{Lip}(f;B^{\infty}(\mathbb{N},\mathcal{Z})<\infty\) and that \(f\) depends only on its first \(d_{\mathcal{X}}\) variables. Then \(f\in L^{2}_{\varsigma}(D;\mathcal{Z})\) and_

\[\|\!\|f\|\!\|_{L^{2}_{\varsigma}(D;\mathcal{Z})}-\delta\leq|\!\|f|\!\|\!\|_{L^ {2}_{\varsigma}(D;\mathcal{Z})}\leq|\!|f|\!\|\!|_{L^{2}_{\varsigma}(D; \mathcal{Z})}+\delta,\]

_where_

\[\delta=L\cdot\|\iota_{d_{\mathcal{X}}}-\iota_{d_{\mathcal{X}}}\circ\widetilde {\mathcal{D}}_{\mathcal{X}}\circ\widetilde{\mathcal{E}}_{\mathcal{X}}\|_{L^{ 2}_{\mu}(\mathcal{X};\ell^{\infty}(\mathbb{N}))}.\]

Proof.: Fix \(z^{*}\in B(\mathcal{Z}^{*})\), let \(g=z^{*}(f)\in L^{2}_{\varsigma}(D)\) and set

\[a=\|g\|_{L^{2}_{\varsigma}(D)},\qquad b=\|g\|_{L^{2}_{\varsigma}(D)}.\]

Notice that

\[|g(\bm{x})-g(\bm{x}^{\prime})|\leq\|z^{*}\|_{\mathcal{Z}^{*}}\|f(\bm{x})-f(\bm {x}^{\prime})\|_{\mathcal{Z}}\leq L\|\bm{x}-\bm{x}^{\prime}\|_{\infty},\quad \forall\bm{x},\bm{x}^{\prime}\in B^{\infty}(\mathbb{N}).\]

Now, with slight abuse of notation, \(g(\iota(X))=g(\iota_{d_{\mathcal{X}}}(X))\) due to the assumption on \(f\). Therefore, using this and the Cauchy-Schwarz inequality,

\[a^{2}-b^{2} =\int_{D}(g(\iota_{d_{\mathcal{X}}}\circ\widetilde{\mathcal{D}}_ {\mathcal{X}}\circ\widetilde{\mathcal{E}}_{\mathcal{X}}(X)))^{2}-(g(\iota_{d _{\mathcal{X}}}(X)))^{2}\,\mathrm{d}\mu(X)\] \[\leq L\int_{D}\|\iota_{d_{\mathcal{X}}}(X)-\iota_{d_{\mathcal{X} }}\circ\widetilde{\mathcal{D}}_{\mathcal{X}}\circ\widetilde{\mathcal{E}}_{ \mathcal{X}}(X)\|_{\infty}\left(|g(\iota_{d_{\mathcal{X}}}(X))|+|g(\iota_{d_{ \mathcal{X}}}\circ\widetilde{\mathcal{D}}_{\mathcal{X}}\circ\widetilde{ \mathcal{E}}_{\mathcal{X}})|\right)\,\mathrm{d}\mu(X)\] \[\leq L\|\iota_{d_{\mathcal{X}}}-\iota_{d_{\mathcal{X}}}\circ \widetilde{\mathcal{D}}_{\mathcal{X}}\circ\widetilde{\mathcal{E}}_{\mathcal{ X}}\|_{L^{2}_{\mu}(\mathcal{X};\ell^{\infty}(\mathbb{N}))}\,(a+b)\,.\]

We deduce that \(a-b\leq\delta\). Since \(z^{*}\) was arbitrary, we get

\[|\!|\!|f|\!|\!|\!|_{L^{2}_{\varsigma}(D;\mathcal{Z})}=\sup_{z^{*}\in B( \mathcal{Z}^{*})}\|z^{*}(f)\|_{L^{2}_{\varsigma}(D)}\leq\sup_{z^{*}\in B( \mathcal{Z}^{*})}\|z^{*}(f)\|_{L^{2}_{\varsigma}(D)}+\delta=|\!|\!|f|\!|\!|\!|_{ L^{2}_{\varsigma}(D;\mathcal{Z})}+\delta,\]

which gives the upper bound. The same argument applied to \(b^{2}-a^{2}\) also gives the lower bound. 

**Lemma D.4** (Norm equivalences for polynomials).: _Let \((\mathcal{Z},\|\cdot\|_{\mathcal{Z}})\) be any Banach space, \(\varsigma\) be the measure defined in (A.I), \(\tilde{\varsigma}\) be the measure defined in (A.III) and \(S\subset\mathcal{F}\) with \(\operatorname{supp}(\bm{\nu})\in\{1,\ldots,d_{\mathcal{X}}\}\), \(\forall\bm{\nu}\in S\). Suppose that_

\[\sqrt{|S|_{\bm{\nu}}}\cdot\|\iota_{d_{\mathcal{X}}}-\iota_{d_{\mathcal{X}}} \circ\widetilde{\mathcal{D}}_{\mathcal{X}}\circ\widetilde{\mathcal{E}}_{ \mathcal{X}}\|_{L^{2}_{\mu}(\mathcal{X};\ell^{\infty}(\mathbb{N}))}\leq c,\] (D.6)

_for some sufficiently small universal constant \(c>0\). Then the norm equivalence_

\[|\!|\!|p\!|\!|\!|_{L^{2}_{\varrho}(D;\mathcal{Z})}\lesssim|\!|\!|p\!|\!|\!|_{L^{ 2}_{\varrho}(D;\mathcal{Z})}\lesssim|\!|\!|p\!|\!|\!|_{L^{2}_{\varrho}(D; \mathcal{Z})}\]

_holds for all \(p=\sum_{\bm{\nu}\in S}c_{\bm{\nu}}\Psi_{\bm{\nu}}\), where \(c_{\bm{\nu}}\in\mathcal{Z}\)._

Proof.: Combining Lemmas D.2 and D.3, we see that

\[|\!|\!|p\!|\!|\!|_{L^{2}_{\varsigma}(D;\mathcal{Z})}-\delta\leq|\!|\!|p\!|\!|\!| \!|_{L^{2}_{\varsigma}(D;\mathcal{Z})}\leq|\!|\!|p|\!|\!|_{L^{2}_{\varsigma}(D; \mathcal{Z})}+\delta,\]

where \(\delta=\frac{1}{2}|\!|\!|p\!|\!|\!|_{L^{2}_{\varrho}(D;\mathcal{Z})}\sqrt{|S|_{ \bm{\nu}}}\!|\!|_{d_{\mathcal{X}}}-\iota_{d_{\mathcal{X}}}\circ\widetilde{ \mathcal{D}}_{\mathcal{X}}\circ\widetilde{\mathcal{E}}_{\mathcal{X}}\|_{L^{2}_{ \mu}(\mathcal{X};\ell^{\infty}(\mathbb{N}))}\leq c|\!|\!|p|\!|\!|\!|_{L^{2}_{ \varrho}(D;\mathcal{Z})}/2\). By (A.I), there are constants \(c_{1}\geq c_{2}>0\) such that

\[c_{1}\|\!|p\!|\!|\!|_{L^{2}_{\varrho}(D;\mathcal{Z})}\leq|\!|\!|p|\!|\!|\!|_{L^{2}_ {\varsigma}(D;\mathcal{Z})}\leq c_{2}|\!|\!|p|\!|\!|\!|_{L^{2}_{\varrho}(D; \mathcal{Z})}.\]

We now take \(c=c_{1}/2\)

### Analysis of (D.3)

We now analyze (D.3). Our analysis relies on the following result, which shows an error bound subject to a certain discrete metric inequality (D.7).

**Lemma D.5** (Discrete metric inequality implies error bounds).: _Let \(\mathcal{Q}:\mathcal{Y}\to\widetilde{\mathcal{Y}}=\mathcal{D}_{\mathcal{Y}}( \mathbb{R}^{d_{\mathcal{Y}}})\) be a bounded linear operator and \(\pi_{\mathcal{Q}}=\|\mathcal{Q}\|_{\mathcal{Y}\to\mathcal{Y}}\). Suppose that_

\[\|p-q\|_{\mathsf{disc},\tilde{\zeta}}\geq\alpha\max\{\|\!\|p-q\|_{L^{2}_{ \mu}(D;\mathcal{Y})},|\!\|p-q\|\!\|_{L^{2}_{\tilde{\zeta}}(D;\mathcal{Y})}\}, \quad\forall p,q\in\mathcal{P}_{\mathcal{S};\widetilde{\mathcal{Y}}}\] (D.7)

_for some \(\alpha>0\). Then, for any \(F\in L^{\infty}_{\mu}(\mathcal{X};\mathcal{Y})\), \(p\in\mathcal{P}_{\mathcal{S};\widetilde{\mathcal{Y}}}\) and \(q\in\mathcal{P}_{\mathcal{S};\mathcal{Y}}\), we have_

\[|\!|\!|F-p\circ\mathcal{E}_{\mathcal{X}}|\!|\!|_{L^{2}_{\mu}( \mathcal{X};\mathcal{Y})}\leq \;\|\!|F-\mathcal{Q}\circ F\|\!|\!|_{L^{2}_{\mu}(\mathcal{X}; \mathcal{Y})}+\alpha^{-1}\|p-\mathcal{Q}\circ q\|_{\mathsf{disc},\tilde{\zeta}}\] \[+\pi_{\mathcal{Q}}|\!|\!|F-q\circ\mathcal{E}_{\mathcal{X}}|\!|\!| _{L^{2}_{\mu}(\mathcal{X};\mathcal{Y})}\]

_and_

\[|\!|\!|F-p\circ\mathcal{E}_{\mathcal{X}}|\!|\!|_{L^{\infty}_{\mu} (\mathcal{X};\mathcal{Y})}\leq \;\|\!|\!|F-\mathcal{Q}\circ F|\!|\!|_{L^{\infty}_{\mu}(\mathcal{X };\mathcal{Y})}+\sqrt{2k}/\alpha\|p-\mathcal{Q}\circ q\|_{\mathsf{disc},\tilde{ \zeta}}\] \[+\pi_{\mathcal{Q}}|\!|\!|F-q\circ\mathcal{E}_{\mathcal{X}}|\!|\!| _{L^{\infty}_{\mu}(\mathcal{X};\mathcal{Y})}.\]

Proof.: By the triangle inequality and properties of \(\mathcal{Q}\), we have

\[|\!|\!|F-p\circ\mathcal{E}_{\mathcal{X}}|\!|\!|_{L^{2}_{\mu}( \mathcal{X};\mathcal{Y})}\] \[\leq \;|\!|\!|\!|F-\mathcal{Q}\circ F|\!|\!|\!|_{L^{2}_{\mu}( \mathcal{X};\mathcal{Y})}+|\!|\!|\!|p\circ\mathcal{E}_{\mathcal{X}}-\mathcal{ Q}\circ q\circ\mathcal{E}_{\mathcal{X}}|\!|\!|\!|_{L^{2}_{\mu}(\mathcal{X}; \mathcal{Y})}+|\!|\!|\!|\mathcal{Q}\circ F-\mathcal{Q}\circ q\circ\mathcal{E}_{ \mathcal{X}}|\!|\!|\!|_{L^{2}_{\mu}(\mathcal{X};\mathcal{Y})}\] \[\leq \;|\!|\!|\!|F-\mathcal{Q}\circ F|\!|\!|_{L^{2}_{\mu}(\mathcal{X} ;\mathcal{Y})}+|\!|\!|\!|p\circ\mathcal{E}_{\mathcal{X}}-\mathcal{Q}\circ q \circ\mathcal{E}_{\mathcal{X}}|\!|\!|_{L^{2}_{\mu}(\mathcal{X};\mathcal{Y})}+ \pi_{\mathcal{Q}}|\!|\!|F-q\circ\mathcal{E}_{\mathcal{X}}|\!|\!|_{L^{2}_{\mu} (\mathcal{X};\mathcal{Y})}.\]

Now, since \(p,Q\circ q\in\mathcal{P}_{\mathcal{S};\widetilde{\mathcal{Y}}}\), the second term can be bounded by

\[|\!|\!|p\circ\mathcal{E}_{\mathcal{X}}-\mathcal{Q}\circ q\circ\mathcal{E}_{ \mathcal{X}}|\!|\!|_{L^{2}_{\mu}(\mathcal{X};\mathcal{Y})}=|\!|\!|\!|p- \mathcal{Q}\circ q|\!|\!|_{L^{2}_{\tilde{\zeta}}(D;\mathcal{Y})}\leq\alpha^{- 1}\|p-\mathcal{Q}\circ q\|\!|\!|_{\text{disc},\tilde{\zeta}}.\] (D.8)

This yields the first result.

For the second result, we once more write

\[|\!|\!|F-p\circ\mathcal{E}_{\mathcal{X}}|\!|\!|\!|_{L^{\infty}_{ \mu}(\mathcal{X};\mathcal{Y})}\leq \;\|\!|\!|F-\mathcal{Q}\circ F|\!|\!|\!|_{L^{\infty}_{\mu}( \mathcal{X};\mathcal{Y})}+|\!|\!|\!|p\circ\mathcal{E}_{\mathcal{X}}-\mathcal{ Q}\circ q\circ\mathcal{E}_{\mathcal{X}}|\!|\!|_{L^{\infty}_{\mu}(\mathcal{X}; \mathcal{Y})}\] \[+\pi_{\mathcal{Q}}|\!|\!|F-q\circ\mathcal{E}_{\mathcal{X}}|\!|\!| _{L^{\infty}_{\mu}(\mathcal{X};\mathcal{Y})}.\]

For the second term, we use (A.III) to write

\[|\!|\!|p\circ\mathcal{E}_{\mathcal{X}}-\mathcal{Q}\circ q\circ\mathcal{E}_{ \mathcal{X}}|\!|\!|_{L^{\infty}_{\mu}(\mathcal{X};\mathcal{Y})}=|\!|\!|\!|p- \mathcal{Q}\circ q|\!|\!|_{L^{\infty}_{\tilde{\zeta}}(D;\mathcal{Y})}\leq\|\!| \!|p-\mathcal{Q}\circ q|\!|\!|_{L^{\infty}_{\tilde{\zeta}}(D;\mathcal{Y})}.\]

Notice that \(p-\mathcal{Q}\circ q\) is a polynomial supported in a set \(S\) with \(|S|_{\mathbf{u}}\leq 2k\). We now apply Lemma D.1 and (D.7) to obtain

\[|\!|\!|p\circ\mathcal{E}_{\mathcal{X}}-\mathcal{Q}\circ q\circ\mathcal{E}_{ \mathcal{X}}|\!|\!|_{L^{\infty}_{\mu}(\mathcal{X};\mathcal{Y})}\leq\sqrt{2k}|\!| \!|p-\mathcal{Q}\circ q|\!|\!|_{L^{2}_{\theta}(D;\mathcal{Y})}\leq\sqrt{2k}/ \alpha|\!|\!|p-\mathcal{Q}\circ q|\!|\!|_{\mathsf{disc},\tilde{\zeta}}.\] (D.9)

This gives the result. 

**Theorem D.6** (Error bound for polynomial minimizers).: _Let \(\mathcal{Q}:\mathcal{Y}\to\widetilde{\mathcal{Y}}=\mathcal{D}_{\mathcal{Y}}( \mathbb{R}^{d_{\mathcal{Y}}})\) be a bounded linear operator, \(\pi_{\mathcal{Q}}=\|\mathcal{Q}\|_{\mathcal{Y}\to\mathcal{Y}}\) and suppose that (D.7) holds. Let \(\tilde{F}\) be as in (D.4) for some \((\sigma,\tau)\)-approximate minimizer \(\hat{p}\) of (D.3). Then_

\[|\!|\!|F-\widehat{F}|\!|\!|_{L^{2}_{\mu}(\mathcal{X};\mathcal{Y})} \leq \;|\!|\!|F-\mathcal{Q}\circ F|\!|\!|_{L^{2}_{\mu}(\mathcal{X}; \mathcal{Y})}+\frac{\sigma+1}{\alpha}|\!|\!|F-\mathcal{Q}\circ F|\!|\!|_{\mathsf{ disc},\mu}\] \[+\pi_{\mathcal{Q}}\left(|\!|\!|F-q\circ\mathcal{E}_{\mathcal{X}}|\!| \!|_{L^{2}_{\mu}(\mathcal{X};\mathcal{Y})}+\frac{\sigma+1}{\alpha}|\!|\!|F-q \circ\mathcal{E}_{\mathcal{X}}|\!|\!|_{\mathsf{disc},\mu}\right)\] \[+\frac{\tau}{\alpha}+\frac{\sigma+1}{\alpha\sqrt{m}}|\!|\!|E|\!| _{2;\mathcal{Y}}\]_and_

\[\|F-\widehat{F}\|_{L^{\infty}_{\mu}(\mathcal{X};\mathcal{Y})} \leq\|F-\mathcal{Q}\circ F\|_{L^{\infty}_{\mu}(\mathcal{X};\mathcal{ Y})}+\frac{\sqrt{2k}(\sigma+1)}{\alpha}\|F-\mathcal{Q}\circ F\|_{\mathsf{disc},\mu}\] \[\quad+\pi_{\mathcal{Q}}\left(\|F-q\circ\mathcal{E}_{\mathcal{X}} \|_{L^{\infty}_{\mu}(\mathcal{X};\mathcal{Y})}+\frac{\sqrt{2k}(\sigma+1)}{ \alpha}\|F-q\circ\mathcal{E}_{\mathcal{X}}\|_{\mathsf{disc},\mu}\right)\] \[\quad+\frac{\sqrt{2k}\tau}{\alpha}+\frac{\sqrt{2k}(\sigma+1)}{ \alpha\sqrt{m}}\|\bm{E}\|_{2;\mathcal{Y}}\]

_for all \(q\in\mathcal{P}_{\mathcal{S};\mathcal{Y}}\), where \(\bm{E}=(E_{i})_{i=1}^{m}\in\mathcal{Y}^{m}\) is the (Banach-valued) vector of noise terms._

Proof.: We apply the previous lemma with \(p=\hat{p}\). This gives

\[\begin{split}\|\!\|F-\widehat{F}\|\!\|_{L^{2}_{\mu}(\mathcal{X}; \mathcal{Y})}&\leq\|\!|F-\mathcal{Q}\circ F\|\!\|_{L^{2}_{\mu}( \mathcal{X};\mathcal{Y})}+\alpha^{-1}\|\hat{p}-\mathcal{Q}\circ q\|_{\mathsf{ disc},\tilde{c}}\\ &\quad+\pi_{\mathcal{Q}}\|\!|F-q\circ\mathcal{E}_{\mathcal{X}}\|_{ L^{2}_{\mu}(\mathcal{X};\mathcal{Y})}\\ \|F-\widehat{F}\|_{L^{\infty}_{\mu}(\mathcal{X};\mathcal{Y})}& \leq\|F-\mathcal{Q}\circ F\|_{L^{\infty}_{\mu}(\mathcal{X}; \mathcal{Y})}+\sqrt{2k}/\alpha\|\hat{p}-\mathcal{Q}\circ q\|_{\mathsf{disc}, \tilde{c}}\\ &\quad+\pi_{\mathcal{Q}}\|F-q\circ\mathcal{E}_{\mathcal{X}}\|_{L^ {\infty}_{\mu}(\mathcal{X};\mathcal{Y})}.\end{split}\] (D.10)

Consider the second term. We have

\[\|\hat{p}-\mathcal{Q}\circ q\|_{\mathsf{disc},\tilde{c}}=\|\!|\hat{p}\circ \mathcal{E}_{\mathcal{X}}-\mathcal{Q}\circ q\circ\mathcal{E}_{\mathcal{X}}\| \!\|_{\mathsf{disc},\mu}\leq\|\!|F-\mathcal{Q}\circ q\circ\mathcal{E}_{ \mathcal{X}}\|\!|_{\mathsf{disc},\mu}+\|\!|F-\hat{p}\circ\mathcal{E}_{ \mathcal{X}}\|\!|_{\mathsf{disc},\mu}.\]

Consider the second term of this expression. By the triangle inequality and the facts that \(\hat{p}\) is a \((\sigma,\tau)\)-minimizer and \(\mathcal{Q}\circ q\) is feasible, we obtain

\[\begin{split}\|\!|F-\hat{p}\circ\mathcal{E}_{\mathcal{X}}\|\!|_{ \mathsf{disc},\mu}&\leq\sqrt{\frac{1}{m}\sum_{i=1}^{m}\|Y_{i}- \hat{p}\circ\mathcal{E}_{\mathcal{X}}\|_{\mathcal{Y}}^{2}}+\frac{1}{\sqrt{m}} \|\bm{E}\|_{2;\mathcal{Y}}\\ &\leq\sigma\sqrt{\frac{1}{m}\sum_{i=1}^{m}\|Y_{i}-\mathcal{Q} \circ q\circ\mathcal{E}_{\mathcal{X}}\|_{\mathcal{Y}}^{2}}+\tau+\frac{1}{\sqrt{ m}}\|\bm{E}\|_{2;\mathcal{Y}}\\ &\leq\sigma\|\!|F-\mathcal{Q}\circ q\circ\mathcal{E}_{\mathcal{X} }\|\!|_{\mathsf{disc},\mu}+\tau+\frac{\sigma+1}{\sqrt{m}}\|\bm{E}\|_{2; \mathcal{Y}}\end{split}\]

Therefore, we get

\[\|\hat{p}-\mathcal{Q}\circ q\|_{\mathsf{disc},\tilde{c}}\leq(\sigma+1)\|\!|F- \mathcal{Q}\circ q\circ\mathcal{E}_{\mathcal{X}}\|\!|_{\mathsf{disc},\mu}+ \tau+\frac{\sigma+1}{\sqrt{m}}\|\bm{E}\|_{2;\mathcal{Y}}.\]

We now estimate the first term in this expression as follows:

\[\begin{split}\|\!|F-\mathcal{Q}\circ q\circ\mathcal{E}_{ \mathcal{X}}\|\!|_{\mathsf{disc},\mu}&\leq\|\!|F-\mathcal{Q} \circ F\|\!|_{\mathsf{disc},\mu}+\|\!|\mathcal{Q}\circ F-\mathcal{Q}\circ q \circ\mathcal{E}_{\mathcal{X}}\|\!|_{\mathsf{disc},\mu}\\ &\leq\|\!|F-\mathcal{Q}\circ F\|\!|_{\mathsf{disc},\mu}+\pi_{ \mathcal{Q}}\|\!|F-q\circ\mathcal{E}_{\mathcal{X}}\|\!|_{\mathsf{disc},\mu}. \end{split}\]

Therefore, we conclude that

\[\|\hat{p}-\mathcal{Q}\circ q\|_{\mathsf{disc},\tilde{c}}\leq(\sigma+1)\|\!|F- \mathcal{Q}\circ F\|\!|_{\mathsf{disc},\mu}+(\sigma+1)\pi_{\mathcal{Q}}\|\!|F -q\circ\mathcal{E}_{\mathcal{X}}\|\!|_{\mathsf{disc},\mu}+\tau+\frac{\sigma+1 }{\sqrt{m}}\|\bm{E}\|_{2;\mathcal{Y}}.\]

Combining this with (D.10) now gives the result. 

### Ensuring (D.7) holds with high probability

For the proof of the next lemma and subsequent steps of the proof, we let \(\Lambda=\{\bm{\nu}_{1},\ldots,\bm{\nu}_{N}\}\) and define the matrix

\[\bm{A}=\left(\frac{\Psi_{\bm{\nu}_{j}}(\mathcal{E}_{\mathcal{X}}(X_{i}))}{ \sqrt{m}}\right)_{i,j=1}^{m,N}\in\mathbb{R}^{m\times N}.\]

**Lemma D.7**.: _Let \(0<\epsilon<1\), \(0<\delta<1\), \(k>0\) and suppose that_

\[m\geq c_{0}\cdot\delta^{-2}\cdot k\cdot(\log(\mathrm{e}N)\cdot\log^{2}(k/\delta) +\log(2/\epsilon))\] (D.11)

_for some universal constant \(c_{0}>0\). Let \(T=\left\{\bm{c}\in\mathbb{R}^{N}:\left\|\bm{c}\right\|_{2}=1,\ \left\|\bm{c}\right\|_{0,\bm{v}}\leq k\right\}\) and_

\[\theta_{+}=\sup\left\{\mathbb{E}\|\bm{Ac}\|_{2}^{2}:\bm{c}\in T\right\},\quad \theta_{-}=\inf\left\{\mathbb{E}\|\bm{Ac}\|_{2}^{2}:\bm{c}\in T\right\}.\]

_Then_

\[\left\|\bm{Ac}\right\|_{2}^{2}\geq(\theta_{-}-(1+\theta_{+})c_{1}\delta) \left\|\bm{c}\right\|_{2}^{2},\quad\forall\bm{c}\in\mathbb{R}^{N},\ \left\|\bm{c}\right\|_{0,\bm{v}}\leq k.\]

_with probability at least \(1-\epsilon\), where \(c_{1}>0\) is a universal constant._

Proof.: The proof is based on [12, Thm. 2.13]. Since \(\left\|\bm{c}\right\|_{1,\bm{v}}\leq\sqrt{\left\|\bm{c}\right\|_{0,\bm{v}}} \left\|\bm{c}\right\|_{2}\leq\sqrt{k}\), we see that

\[T\subseteq\left\{\bm{c}\in\mathbb{R}^{N}:\left\|\bm{c}\right\|_{1,\bm{v}}\leq \sqrt{k}\right\}.\]

Define the random vector \(\bm{X}\in\mathbb{R}^{N}\) by \(\bm{X}=(\Psi_{\bm{\nu}_{i}}(\mathcal{E}_{\mathcal{X}}(X)))_{i=1}^{N}\) for \(X\sim\mu\). Observe that

\[\left|\langle\bm{X},\bm{e}_{j}\rangle\right|=\left|\Psi_{\bm{\nu}_{i}}( \mathcal{E}_{\mathcal{X}}(X))\right|\leq u_{\bm{\nu}_{i}}\leq v_{\bm{\nu}_{i}}\]

almost surely, by (A.III) and the definition of the weights. Therefore, by [12, Thm. 2.13], if

\[m\geq c_{0}\cdot\delta^{-2}\cdot k\cdot\log(\mathrm{e}N)\cdot\log^{2}(k/(c_{1 }\delta)),\] (D.12)

it holds that

\[\sup_{\bm{c}\in T}\left|\frac{1}{m}\sum_{i=1}^{m}|\langle\bm{c},\bm{X}_{i} \rangle|^{2}-\mathbb{E}|\langle\bm{c},\bm{X}\rangle|^{2}\right|\leq c_{1} \delta\left(1+\sup_{\bm{c}\in T}\mathbb{E}|\langle\bm{c},\bm{X}\rangle|^{2}\right)\]

with probability at least \(1-2\exp(-c_{2}\delta^{2}m/k)\). Here \(c_{0},c_{1},c_{2}>0\) are universal constants. Now observe that

\[\frac{1}{m}\sum_{i=1}^{m}|\langle\bm{c},\bm{X}_{i}\rangle|^{2}=\|\bm{Ac}\|_{2 }^{2},\qquad\mathbb{E}|\langle\bm{c},\bm{X}\rangle|^{2}=\mathbb{E}(\|\bm{Ac} \|_{2}^{2}).\]

Therefore, we have shown that

\[\left\|\bm{Ac}\right\|_{2}^{2}\geq\theta_{-}-(1+\theta_{+})c_{1}\delta,\quad \forall\bm{c}\in T,\]

with probability at least \(1-2\exp(-c_{2}\delta^{2}m/k)\), provided \(m\) satisfies (D.12). To conclude the result, we observe that (D.11) implies (D.12), up to a possible change in the universal constant \(c_{0}\). Moreover, it also implies that

\[2\exp(-c_{2}\delta^{2}m/k)\leq\epsilon.\]

Hence we obtain the result. 

**Lemma D.8**.: _There exist universal constants \(c_{0},c_{1},c_{2}>0\) such that the following holds. Suppose that_

\[m\geq c_{0}\cdot k\cdot(\log(\mathrm{e}N)\cdot\log^{2}(k)+\log(2/\epsilon))\] (D.13)

_and_

\[\sqrt{k}\cdot\left\|\iota_{d_{\mathcal{X}}}-\iota_{d_{\mathcal{X}}}\circ \widetilde{\mathcal{D}}_{\mathcal{X}}\circ\widetilde{\mathcal{E}}_{\mathcal{ X}}\right\|_{L_{\mu}^{2}(\mathcal{X};\ell^{\infty}(\mathbb{N}))}\leq c_{1}.\] (D.14)

_Then (D.7) holds with probability at least \(1-\epsilon\) and constant \(\alpha\geq c_{2}\)._

Proof.: We shall apply the previous lemma with \(k\) replaced by \(2k\). First, we estimate \(\theta_{+}\) and \(\theta_{-}\). Let \(\bm{c}\in T\), with \(T\) as defined therein with \(2k\) in place of \(k\). Write \(p=\sum_{\bm{\nu}\in\Lambda}c_{\bm{\nu}}\Psi_{\bm{\nu}}\) for the corresponding (scalar-valued) polynomial. Then

\[\left\|\bm{Ac}\right\|_{2}^{2}=\frac{1}{m}\sum_{i=1}^{m}|p\circ\mathcal{E}_{ \mathcal{X}}(X_{i})|^{2},\]

and therefore

\[\mathbb{E}\|\bm{Ac}\|_{2}^{2}=\left\|p\right\|_{L_{\mu}^{2}(D)}^{2}.\]Since \(\left\|c\right\|_{0,\mathbf{v}}\leq 2k\), (D.14) implies (D.6). We now apply Lemma D.4 and the fact that \(\left\|p\right\|_{L^{2}_{\varrho}(D)}=\left\|c\right\|_{2}=1\) to get

\[c_{3}\leq\mathbb{E}\|\bm{A}\bm{c}\|_{2}^{2}\leq c_{4}\]

for universal constants \(c_{4}\geq c_{3}>0\). Since \(\bm{c}\in T\) was arbitrary to deduce that

\[c_{3}\leq\theta_{-}\leq\theta_{+}\leq c_{4}.\] (D.15)

We now show that (D.7) holds with the desired probability. Let \(p,q\in\mathcal{P}_{\mathcal{S};\widetilde{\mathcal{Y}}}\) be arbitrary. Then their difference \(h=p-q\) can be expressed as

\[h=\sum_{\bm{\nu}\in\Lambda}d_{\bm{\nu}}\Psi_{\bm{\nu}}\] (D.16)

where \(\bm{d}=(d_{\bm{\nu}})_{\bm{\nu}\in\Lambda}\in\widetilde{\mathcal{Y}}^{N}\) satisfies

\[\left\|\bm{d}\right\|_{0,\mathbf{u}}\leq\left\|\bm{d}\right\|_{0,\mathbf{v}} \leq 2k.\]

Now, this, (D.14) and Lemma D.4 imply that

\[\left|\kern-1.075pt\left|\kern-1.075pt\left|h\right|\kern-1.075pt\right| \kern-1.075pt\right|\kern-1.075pt\right|\kern-1.075pt\left|h\right|\kern-1.075pt \right|\kern-1.075pt\left|\kern-1.075pt\left|h\right|\kern-1.075pt\right|\kern-1.075pt \right|\kern-1.075pt\left|h\right|\kern-1.075pt\right|\kern-1.075pt\left|h\right| \kern-1.075pt\left|\kern-1.075pt\right|\kern-1.075pt\left|h\right|\kern-1.075pt \right|\kern-1.075pt\left|h\right|\kern-1.075pt\right|\kern-1.075pt\left|h \right|\kern-1.075pt\left|h\right|\kern-1.075pt\right|\kern-1.075pt\]

Therefore, in order to prove (D.7), it suffices to show that \(\left|\kern-1.075pt\left|\kern-1.075pt\left|h\right|\kern-1.075pt\right| \kern-1.075pt\right|\kern-1.075pt\left|h\right|\kern-1.075pt\right|\kern-1.075pt \left|h\right|\kern-1.075pt\left|h\right|\kern-1.075pt\right|\kern-1.075pt \left|h\right|\kern-1.075pt\left|h\right|\kern-1.075pt\left|\kern-1.075pt \left|h\right|\kern-1.075pt\right|\kern-1.075pt\).

Observe that

\[\left\|h\right\|_{\text{disc},\widetilde{\zeta}}=\left\|\bm{A}\bm{d}\right\| _{2;\mathcal{Y}}\geq\left\|\bm{A}\bm{d}\right\|_{2;\mathcal{Y}}=\sup_{y^{*} \in B(\mathcal{Y}^{*})}\left\|y^{*}(\bm{A}\bm{d})\right\|_{2},\]

where we recall that for a vector \(\bm{z}=(z_{i})_{i=1}^{N}\in\mathcal{Y}^{N}\), we write \(y^{*}(\bm{z})=(y^{*}(z_{i}))_{i=1}^{N}\in\mathbb{R}^{N}\). By linearity, we have \(y^{*}(\bm{A}\bm{d})=\bm{A}y^{*}(\bm{d})\). Therefore, by Lemma D.7,

\[\left\|h\right\|_{\text{disc},\widetilde{\zeta}}^{2} =\left\|\bm{A}\bm{d}\right\|_{2;\mathcal{Y}}^{2}\] \[\geq\sup_{y^{*}\in B(\mathcal{Y}^{*})}\left\|\bm{A}y^{*}(\bm{d}) \right\|_{2}^{2}\] \[\geq\sup_{y^{*}\in B(\mathcal{Y}^{*})}\left(\theta_{-}-(1+\theta_{ +})c_{5}\delta\right)\left\|y^{*}(\bm{d})\right\|_{2}^{2}\] \[\geq(c_{3}-(1+c_{4})c_{5}\delta)\sup_{y^{*}\in B(\mathcal{Y}^{*}) }\left\|y^{*}(h)\right\|_{L^{2}_{\varrho}(D)}^{2}\] \[=(c_{3}-(1+c_{4})c_{5}\delta)\left\|h\right\|_{L^{2}_{\varrho}(D; \mathcal{Y})}^{2}.\]

for some universal constant \(c_{5}>0\), provided \(m\) satisfies (D.11). We now set \(\delta=c_{3}/(2(1+c_{4})c_{5})\) to get

\[\left\|h\right\|_{\text{disc},\widetilde{\zeta}}^{2}\geq c_{3}/2\left\|h\right\| _{L^{2}_{\varrho}(D;\mathcal{Y})}^{2},\]

provided \(m\) satisfies (D.11) with this value of \(\delta\). However, this is implied by the condition (D.13). We deduce the result. 

### Construction of the DNN family \(\mathcal{N}\)

Recall that \(\Lambda\subset\mathcal{F}\), \(|\Lambda|=N\) is an arbitrary set and \(\mathcal{S}\) is defined by (D.1). In this and what follows, we slightly abuse notation and consider a DNN \(N:\mathbb{R}^{d}\to\mathbb{R}\) as a function \(\mathbb{R}^{\mathbb{N}}\to\mathbb{R}\) which depends on only the first \(d\) variables.

**Lemma D.9** (Approximating Legendre polynomials with tanh DNNs).: _Let \(\Gamma\subset\Lambda\) with \(\operatorname{supp}(\bm{\nu})\subseteq\{1,\dots,d\}\), \(\forall\bm{\nu}\in\Gamma\), and \(m(\Gamma)=\max_{\bm{\nu}\in\Gamma}\left\|\bm{\nu}\right\|_{1}<\infty\). There exists a fully-connected family \(\mathcal{N}_{o}\) of tanh DNNs \(N:\mathbb{R}^{d}\to\mathbb{R}\) with_

\[\operatorname{width}(\mathcal{N}_{o})\lesssim m(\Gamma),\quad\operatorname{ depth}(\mathcal{N}_{o})\lesssim\log(m(\Gamma)),\]

_such that, for any \(0<\delta<1\) and \(\bm{\nu}\in\Gamma\), there is a DNN \(N_{\bm{\nu}}\in\mathcal{N}_{o}\) with_

\[\left\|N_{\bm{\nu}}-\Psi_{\bm{\nu}}\right\|_{L^{\infty}_{\varrho}(D)}\leq\delta.\] (D.17)

_Moreover, the zero network \(0:\bm{x}\mapsto 0\) also belongs to \(\mathcal{N}_{o}\) (trivially, since \(\tanh(0)=0\))._Proof.: We follow the argument of [5, Thm. 7.4], which is based on [79, Prop. 2.6]. Let \(\boldsymbol{\nu}\in\Gamma\). Then via the fundamental theorem of algebra we can write \(\Psi_{\boldsymbol{\nu}}(\boldsymbol{x})\) as a product of \(\left\|\boldsymbol{\nu}\right\|_{1}\) numbers as

\[\Psi_{\boldsymbol{\nu}}(\boldsymbol{x})=\prod_{i\in\operatorname{supp}( \boldsymbol{\nu})}\prod_{j=1}^{\nu_{i}}d_{i}(x_{i}-r_{ij}).\]

Here \(\{r_{ij}\}_{j=1}^{\nu_{i}}\) are the roots of the univariate Legendre polynomial \(P_{\nu_{i}}\). We append \(m(\Gamma)-\left\|\boldsymbol{\nu}\right\|_{1}\) ones and write \(\Psi_{\boldsymbol{\nu}}(\boldsymbol{x})\) as a product of exactly \(m(\Gamma)\) numbers. Now define the affine map

\[\mathcal{A}_{\boldsymbol{\nu}}:\mathbb{R}^{d}\to\mathbb{R}^{m(\Gamma)},\] (D.18)

so that \(\mathcal{A}_{\boldsymbol{\nu}}(\boldsymbol{x})\) is the vector consisting of the values \(d_{i}(x_{i}-r_{ij})\) for \(j=1,\ldots,\nu_{i}\) and \(i\in\operatorname{supp}(\boldsymbol{\nu})\) and \(1\) otherwise. To complete the proof, we need to construct a tanh DNN mapping \(\mathbb{R}^{m(\Gamma)}\to\mathbb{R}\) that approximately multiplies these numbers. To do this, we argue as in the proof of [5, Thm. 7.4] to see that there is a tanh DNN \(N_{\boldsymbol{\nu}}\) with

\[\operatorname{width}(N_{\boldsymbol{\nu}})\lesssim m(\Gamma),\quad \operatorname{depth}(N_{\boldsymbol{\nu}})\lesssim\log(m(\Gamma))\]

that satisfies the desired bound (D.17). Since these width and depth bounds are independent of \(\boldsymbol{\nu}\), we deduce the result. 

Fix \(\delta>0\), let \(\Gamma=\cup_{S\in\mathcal{S}}S\), \(d=d_{\mathcal{X}}\) and consider the corresponding family \(\mathcal{N}_{o}\) and DNNs \(N_{\boldsymbol{\nu}}\), \(\boldsymbol{\nu}\in\Gamma\), asserted by this lemma. For any \(\boldsymbol{\nu}\in\Gamma\), we have \(\boldsymbol{\nu}\in S\) for some \(S\in\mathcal{S}\), and any such \(S\) satisfies \(|S|_{\boldsymbol{\nu}}\leq k\). Therefore \(u_{\boldsymbol{\nu}}^{2(5+\xi)}=v_{\boldsymbol{\nu}}^{2}\leq k\) for any \(\boldsymbol{\nu}\in S\) and we deduce that

\[\left\|\boldsymbol{\nu}\right\|_{1}\leq\prod_{j=1}^{d}(2\nu_{j}+1)=u_{ \boldsymbol{\nu}}^{2}\leq k^{1/(5+\xi)}.\]

This implies that \(m(\Gamma)\leq k^{1/(5+\xi)}\) and therefore

\[\operatorname{width}(\mathcal{N}_{o})\lesssim k^{1/(5+\xi)},\quad \operatorname{depth}(\mathcal{N}_{o})\lesssim\log(k).\]

With this in hand, we now define the family \(\mathcal{N}\) of DNNs \(N:\mathbb{R}^{d_{\mathcal{X}}}\to\mathbb{R}^{d_{\mathcal{Y}}}\) by

\[\mathcal{N}=\left\{N=\boldsymbol{C}\left[\begin{array}{c}N_{\boldsymbol{ \nu}_{1}}\\ \vdots\\ N_{\boldsymbol{\nu}_{|S|}}\\ 0\\ \vdots\\ 0\end{array}\right]:S=\{\boldsymbol{\nu}_{1},\ldots,\boldsymbol{\nu}_{|S|} \}\in\mathcal{S},\ \boldsymbol{C}\in\mathbb{R}^{d_{\mathcal{Y}}\times\lfloor k \rfloor}\right\}.\] (D.19)

Here we also use the fact that \(|S|\leq|S|_{\boldsymbol{\nu}}\leq k\). Notice that this family satisfies

\[\operatorname{width}(\mathcal{N})\leq k^{1+1/(5+\xi)},\quad \operatorname{depth}(\mathcal{N})\lesssim\log(k),\] (D.20)

due to the bounds for \(\mathcal{N}_{o}\).

Now let \(N\in\mathcal{N}\) and write \(\boldsymbol{C}=[\boldsymbol{c}_{1}|\cdots|\boldsymbol{c}_{\lfloor k \rfloor}]\), where \(\boldsymbol{c}_{i}\in\mathbb{R}^{d_{\mathcal{Y}}}\). Then

\[\mathcal{D}_{\mathcal{Y}}\circ N=\sum_{i=1}^{|S|}\mathcal{D}_{\mathcal{Y}}( \boldsymbol{c}_{i})N_{\boldsymbol{\nu}_{i}}=\sum_{i=1}^{|S|}c_{i}N_{ \boldsymbol{\nu}_{i}},\quad\text{where }c_{i}\in\widetilde{\mathcal{Y}}.\]

Therefore, we can associate \(\mathcal{N}\) with the space of functions \(\widetilde{P}_{\mathcal{S};\widetilde{\mathcal{Y}}}\), where, for any arbitrary Banach space \((\mathcal{Z},\left\|\cdot\right\|_{\mathcal{Z}})\),

\[\widetilde{\mathcal{P}}_{\mathcal{S};\mathcal{Z}}=\left\{\sum_{\boldsymbol{ \nu}\in S}c_{\boldsymbol{\nu}}N_{\boldsymbol{\nu}}:c_{\boldsymbol{\nu}}\in \mathcal{Z},\ S\in\mathcal{S}\right\}.\]

We now require the following lemma, which relates the distance between a function in \(\widetilde{\mathcal{P}}_{\mathcal{S};\mathcal{Z}}\) and the corresponding polynomial in \(\mathcal{P}_{\mathcal{S};\mathcal{Z}}\).

**Lemma D.10** (Discrete norms of polynomials and their approximating DNNs).: _Let \(S\subset\mathcal{F}\), \((\mathcal{Z},\|\cdot\|_{\mathcal{Z}})\) be any Banach space and suppose that \(p=\sum_{\boldsymbol{\nu}\in S}c_{\boldsymbol{\nu}}\Psi_{\boldsymbol{\nu}}\), where \(c_{\boldsymbol{\nu}}\in\mathcal{Z}\). Define_

\[\tilde{p}=\sum_{\boldsymbol{\nu}\in S}c_{\boldsymbol{\nu}}N_{\boldsymbol{\nu}}.\]

_Then_

\[\|p-\tilde{p}\|_{\text{disc},\tilde{\zeta}}\leq\|p-\tilde{p}\|_{L^{\infty}_{ \tilde{\sigma}}(D;\mathcal{Z})}\leq\delta\sqrt{|S|}\|p\|_{L^{2}_{\tilde{\sigma }}(D;\mathcal{Z})}.\] (D.21)

_Moreover, if \(\mathcal{Z}=\widetilde{\mathcal{Y}}\), \(S\in\mathcal{S}\) and (D.7) holds with \(\alpha\) satisfying \(\delta\sqrt{|S|_{\boldsymbol{u}}}/\alpha<1\) then_

\[\|p\|_{L^{2}_{\tilde{\sigma}}(D;\mathcal{Y})}\leq\frac{1}{\alpha-\delta\sqrt{ |S|}}\|\tilde{p}\|_{\text{disc},\tilde{\zeta}}.\]

Proof.: By (A.III) and the definition of the \(N_{\boldsymbol{\nu}}\) we have

\[\|p-\tilde{p}\|_{\text{disc},\tilde{\zeta}} \leq\|p-\tilde{p}\|_{L^{\infty}_{\tilde{\sigma}}(D;\mathcal{Z})}\] \[=\sup_{z^{*}\in B(Z^{*})}\|z^{*}(p-\tilde{p})\|_{L^{\infty}_{ \tilde{\sigma}}(D)}\] \[\leq\sup_{z^{*}\in B(Z^{*})}\sum_{\boldsymbol{\nu}\in S}|z^{*}(c _{\boldsymbol{\nu}})|\|\Psi_{\boldsymbol{\nu}}-N_{\boldsymbol{\nu}}\|_{L^{ \infty}_{\tilde{\sigma}}(D)}\] \[\leq\delta\|\boldsymbol{c}\|_{1;\mathcal{Z}}\]

We now apply the Cauchy-Schwarz inequality and Parseval's identity to obtain

\[\|p-\tilde{p}\|_{\text{disc},\tilde{\zeta}}\leq\delta\sqrt{|S|}\|\boldsymbol{ c}\|_{2;\mathcal{Z}}=\delta\sqrt{|S|_{\boldsymbol{u}}}\|p\|_{L^{2}_{\tilde{ \sigma}}(D;\mathcal{Z})},\]

which gives the first result.

For the second result, we apply (D.7) with \(q=0\) to get

\[\|p\|_{L^{2}_{\tilde{\sigma}}(D;\mathcal{Y})}\leq\alpha^{-1}\|p\|_{\text{disc },\tilde{\zeta}}\leq\alpha^{-1}\left(\|p-\tilde{p}\|_{\text{disc},\tilde{ \zeta}}+\|\tilde{p}\|_{\text{disc},\tilde{\zeta}}\right).\]

Using (D.21) and the fact that \(\delta\sqrt{|S|}/\alpha<1\) now gives the result. 

**Remark D.11** (Other activation functions): As seen in this section, a key step in our proofs is emulating the polynomials via DNNs of quantifiable width and depth. There is an extensive literature on this topic. See, e.g., [5, 21, 23, 25, 38, 57, 63, 67, 72, 78, 79, 85, 86, 92, 97] and references therein. The proof of Lemma D.9 reduces this to the task of emulating the multiplication operation \((x_{1},\ldots,x_{d})\in\mathbb{R}^{d}\mapsto x_{1}\cdots x_{d}\in\mathbb{R}\) via a DNN. As shown in the proof of [5, Thm. 7.4] (which is based on [79, Prop. 2.6]), this can in turn be achieved using a binary tree of \(\lceil\log_{2}(d)\rceil\) DNNs that approximately compute the multiplication of two numbers \((x,y)\in\mathbb{R}^{2}\mapsto xy\in\mathbb{R}\). Further, this task can be achieved via the identity \(xy=((x+y)^{2}-(x-y)^{2})/4\) by using a DNN that approximately computes the squaring function \(x\in\mathbb{R}\mapsto x^{2}\in\mathbb{R}\). To summarize, provide a DNN of quantifiable width and depth can approximately compute the squaring function, it can also approximately emulate the multivariate Legendre polynomials.

In view of this, we can adapt our main theorems to various other activation functions without change. This includes Rectified Polynomial Units (RePUs), where the emulation is, in fact, exact (see, e.g., [57, Lem. 2.1]). It also includes the Exponential Linear Unit (ELU) used in our numerical experiments and many others. See, for instance, Proposition 4.7 of [38] and the ensuing discussion. Rectified Linear Units (ReLUs) are slightly different, as in this case the depth of the DNN that performs the approximation multiplication depends on the desired accuracy (see, e.g., [79, Prop. 2.6]). One could modify Theorem 3.1 to consider ReLU DNNs, with the result being a worse depth bound than that presented for tanh DNNs.

### Analysis of (2.5)

**Lemma D.12** (Approximate minimizers of (2.5) yield approximate minimizers of (D.3)).: _Suppose that (D.7) holds, let \(\mathcal{N}\) be the family of DNNs defined in SSD.5 and \(\widehat{N}\) be any \((\sigma,\tau)\)-approximate minimizer of (2.5). Let_

\[\mathcal{D}_{\mathcal{Y}}\circ\widehat{N}=\sum_{\boldsymbol{\nu}\in S}\hat{c}_ {\boldsymbol{\nu}}N_{\boldsymbol{\nu}}\in\widetilde{\mathcal{P}}_{\mathcal{S} ;\widehat{\mathcal{Y}}},\]_where \(S\in\mathcal{S}\) and \(\hat{c}_{\boldsymbol{\nu}}\in\widehat{\mathcal{Y}}\), and define_

\[\hat{p}=\sum_{\boldsymbol{\nu}\in S}\hat{c}_{\boldsymbol{\nu}}\Psi_{\boldsymbol {\nu}}.\]

_Then \(\hat{p}\) is a \((\sigma^{\prime},\tau^{\prime})\)-approximate minimizer of (D.3), where_

\[\sigma^{\prime} \leq\sigma(1+\delta\sqrt{k}/\alpha)\] (D.22) \[\tau^{\prime} \leq\tau+\sigma\delta\sqrt{k}/\alpha\left(\|F\|_{L^{\infty}_{ \mu}(\mathcal{X};\mathcal{Y})}+\frac{1}{\sqrt{m}}\|\boldsymbol{E}\|_{2; \mathcal{Y}}\right)+\delta\sqrt{k}\|\hat{p}\|_{L^{2}_{\varrho}(D;\mathcal{Y})}.\]

Proof.: Let \(p=\sum_{\boldsymbol{\nu}\in S}c_{\boldsymbol{\nu}}\Psi_{\boldsymbol{\nu}}\in P _{S;\widehat{\mathcal{Y}}}\) be arbitrary and \(N\in\mathcal{N}\) be the corresponding DNN so that \(\mathcal{D}_{\mathcal{Y}}\circ N=\sum_{\boldsymbol{\nu}\in S}c_{\boldsymbol{ \nu}}N_{\boldsymbol{\nu}}\). Then by the triangle inequality and the fact that \(\widehat{N}\) is an approximate minimizer, we have

\[\sqrt{\frac{1}{m}\sum_{i=1}^{m}\|Y_{i}-\hat{p}\circ\mathcal{E}_{ \mathcal{X}}(X_{i})\|^{2}_{\mathcal{Y}}}\] \[\leq\sqrt{\frac{1}{m}\sum_{i=1}^{m}\|Y_{i}-\mathcal{D}_{\mathcal{ Y}}\circ\widehat{N}\circ\mathcal{E}_{\mathcal{X}}(X_{i})\|^{2}_{\mathcal{Y}}}+ \|\hat{p}-\mathcal{D}_{\mathcal{Y}}\circ\widehat{N}\|_{\text{disc},\widehat{ \varsigma}}\] \[\leq\sigma\sqrt{\frac{1}{m}\sum_{i=1}^{m}\|Y_{i}-\mathcal{D}_{ \mathcal{Y}}\circ N\circ\mathcal{E}_{\mathcal{X}}(X_{i})\|^{2}_{\mathcal{Y}}} +\tau+\|\hat{p}-\mathcal{D}_{\mathcal{Y}}\circ\widehat{N}\|_{\text{disc}, \widehat{\varsigma}}\] \[\leq\sigma\sqrt{\frac{1}{m}\sum_{i=1}^{m}\|Y_{i}-p\circ\mathcal{ E}_{\mathcal{X}}(X_{i})\|^{2}_{\mathcal{Y}}}+\tau+\sigma\|p-\mathcal{D}_{ \mathcal{Y}}\circ N\|_{\text{disc},\widehat{\varsigma}}+\|\hat{p}-\mathcal{D}_ {\mathcal{Y}}\circ\widehat{N}\|_{\text{disc},\widehat{\varsigma}}.\]

We now apply Lemma D.10 to the last two terms, noting that \(|S|\leq|S|_{\boldsymbol{\nu}}\leq k\) since \(S\in\mathcal{S}\), to obtain

\[\sqrt{\frac{1}{m}\sum_{i=1}^{m}\|Y_{i}-\hat{p}\circ\mathcal{E}_{ \mathcal{X}}(X_{i})\|^{2}_{\mathcal{Y}}} \leq\sigma\sqrt{\frac{1}{m}\sum_{i=1}^{m}\|Y_{i}-p\circ\mathcal{ E}_{\mathcal{X}}(X_{i})\|^{2}_{\mathcal{Y}}}\] \[\quad+\tau+\sigma\delta\sqrt{k}\|p\|_{L^{2}_{\varrho}(D;\mathcal{ Y})}+\delta\sqrt{k}\|\hat{p}\|_{L^{2}_{\varrho}(D;\mathcal{Y})}.\]

Consider the third term. We first apply (D.7) with \(q=0\) to get

\[\|p\|_{L^{2}_{\varrho}(D;\mathcal{Y})}\leq\alpha^{-1}\|p\|_{\text{disc}, \widehat{\varsigma}}=\alpha^{-1}\|p\circ\mathcal{E}_{\mathcal{X}}\|_{\text{disc },\mu}.\]

We then use the triangle inequality to get

\[\|p\circ\mathcal{E}_{\mathcal{X}}\|_{\text{disc},\mu} \leq\|F-p\circ\mathcal{E}_{\mathcal{X}}\|_{\text{disc},\mu}+\|F \|_{\text{disc},\mu}\] \[\leq\sqrt{\frac{1}{m}\sum_{i=1}^{m}\|Y_{i}-p\circ\mathcal{E}_{ \mathcal{X}}(X_{i})\|^{2}_{\mathcal{Y}}}+\|F\|_{L^{\infty}_{\mu}(\mathcal{X}; \mathcal{Y})}+\frac{1}{\sqrt{m}}\|\boldsymbol{E}\|_{2;\mathcal{Y}}.\]

Therefore, we obtain

\[\sqrt{\frac{1}{m}\sum_{i=1}^{m}\|Y_{i}-\hat{p}\circ\mathcal{E}_{ \mathcal{X}}(X_{i})\|^{2}_{\mathcal{Y}}} \leq\sigma(1+\delta\sqrt{k}/\alpha)\sqrt{\frac{1}{m}\sum_{i=1}^{ m}\|Y_{i}-p\circ\mathcal{E}_{\mathcal{X}}(X_{i})\|^{2}_{\mathcal{Y}}}+\tau\] \[\quad+\sigma\delta\sqrt{k}/\alpha\left(\|F\|_{L^{\infty}_{\mu}( \mathcal{X};\mathcal{Y})}+\frac{1}{\sqrt{m}}\|\boldsymbol{E}\|_{2;\mathcal{Y}} \right)+\delta\sqrt{k}\|\hat{p}\|_{L^{2}_{\varrho}(D;\mathcal{Y})}.\]

Since \(p\in P_{\mathcal{S},\widehat{\mathcal{Y}}}\) was arbitrary we get the result.

[MISSING_PAGE_EMPTY:41]

\[\|F-\widehat{F}\|_{L^{\infty}_{\mu}(\mathcal{X};\mathcal{Y})} \lesssim\|F-\mathcal{Q}\circ F\|_{L^{\infty}_{\mu}(\mathcal{X}; \mathcal{Y})}+\sqrt{k}\sigma\|\!\|F-\mathcal{Q}\circ F\|\!\|_{\text{disc},\mu}\] \[\quad+\pi_{\mathcal{Q}}\left(\|F-q\circ\mathcal{E}_{\mathcal{X}} \|_{L^{\infty}_{\mu}(\mathcal{X};\mathcal{Y})}+\sqrt{k}\sigma\|\!\|F-q\circ \mathcal{E}_{\mathcal{X}}\|\!\|_{\text{disc},\mu}\right)\] \[\quad+\sqrt{k}\tau+\sigma\delta k\|F\|_{L^{\infty}_{\mu}( \mathcal{X};\mathcal{Y})}+\frac{\sqrt{k}\sigma}{\sqrt{m}}\|\!\|E\|_{2; \mathcal{Y}}+\delta k\|\!\|\hat{p}\|_{L^{2}_{\varrho}(D;\mathcal{Y})}.\]

We now bound the final term. Using Lemma D.10 once more, in combination with the fact that \(\alpha-\delta\sqrt{k}\gtrsim 1\), we see that

\[\leq\sqrt{\frac{1}{m}\sum_{i=1}^{m}\|Y_{i}-\mathcal{D}_{\mathcal{Y }}\circ\widehat{N}\circ\mathcal{E}_{\mathcal{X}}(X_{i})\|^{2}_{\mathcal{Y}}}+ \frac{1}{\sqrt{m}}\|\!\|Y\|_{2;\mathcal{Y}}\] \[\leq\sigma\sqrt{\frac{1}{m}\sum_{i=1}^{m}\|Y_{i}-0\|^{2}_{ \mathcal{Y}}}+\tau+\frac{1}{\sqrt{m}}\|\!\|Y\|_{2;\mathcal{Y}}\] \[\leq(1+\sigma)\left(\|F\|_{L^{\infty}_{\mu}(\mathcal{X};\mathcal{ Y})}+\frac{1}{\sqrt{m}}\|\!\|E\|_{2;\mathcal{Y}}\right)+\tau.\]

Here, we also used the fact that \(\widehat{N}\) is an approximate minimizer in the fourth step, as well as the facts that the zero network \(0\in\mathcal{N}\) and that \(\mathcal{D}_{\mathcal{Y}}\) is a linear map. Plugging this into the previous expressions now gives the result. 

### Bounding the best polynomial approximation error terms

When \(\sigma=1\) (as will be the case when we come to prove Theorem 3.1), the error bounds in Theorem D.13 involve best polynomial approximation error terms of the form

\[\|\!\|F-q\circ\mathcal{E}_{\mathcal{X}}\|\!\|_{L^{2}_{\mu}(\mathcal{X}; \mathcal{Y})}+\|\!|F-q\circ\mathcal{E}_{\mathcal{X}}\|\!\|_{\text{disc},\mu}, \quad\|F-q\circ\mathcal{E}_{\mathcal{X}}\|\!\|_{L^{\infty}_{\mu}(\mathcal{X}; \mathcal{Y})}+\sqrt{k}\|\!|F-q\circ\mathcal{E}_{\mathcal{X}}|\!\|_{\text{disc },\mu}\]

for arbitrary \(q\in\mathcal{P}_{\mathcal{S};\mathcal{Y}}\). By the triangle inequality, these are bounded by

\[E_{2}(F,q) :=\|\!|F-q\circ\iota|\!\|_{L^{2}_{\mu}(\mathcal{X};\mathcal{Y})}+ \|\!|F-q\circ\iota|\!\|_{\text{disc},\mu}\] (D.24) \[\quad+\|\!|q\circ\iota-q\circ\mathcal{E}_{\mathcal{X}}\|\!\|_{L^{ \infty}_{\mu}(\mathcal{X};\mathcal{Y})}+\|q\circ\iota-q\circ\mathcal{E}_{ \mathcal{X}}\|\!\|_{\text{disc},\mu},\] \[E_{\infty}(F,q) :=\|F-q\circ\iota|\!\|_{L^{\infty}_{\mu}(\mathcal{X};\mathcal{Y}) }+\sqrt{k}\|\!|F-q\circ\iota|\!\|_{\text{disc},\mu}\] \[\quad+\|q\circ\iota-q\circ\mathcal{E}_{\mathcal{X}}\|_{L^{\infty}_ {\mu}(\mathcal{X};\mathcal{Y})}+\sqrt{k}\|\!|q\circ\iota-q\circ\mathcal{E}_{ \mathcal{X}}\|\!\|_{\text{disc},\mu}.\]

In this section, we construct a suitable polynomial \(q\) in the case where (A.II) holds and thereby derive a bound for these term. We first require the following lemma.

**Lemma D.14**.: _Let \(G\in L^{\infty}_{\mu}(\mathcal{X};\mathcal{Y})\) and \(0<\epsilon<1\). Then the following hold._

1. _With probability at least_ \(1-\epsilon\) _on the draw of the_ \(X_{i}\)_, we have_ \[\|\!|G|\!|\!|_{\text{disc},\mu}\leq\|G\|_{L^{2}_{\mu}(\mathcal{X};\mathcal{Y}) }/\sqrt{\epsilon}.\]
2. _Suppose that_ \(m\geq 2r\log(2/\epsilon)\) _for some_ \(r>0\)_. Then, with probability at least_ \(1-\epsilon\) _on the draw of the_ \(X_{i}\)_, we have_ \[\|\!|G|\!|\!|_{\text{disc},\mu}\leq\sqrt{2}\left(\|G\|_{L^{\infty}_{\mu}( \mathcal{X};\mathcal{Y})}/\sqrt{r}+\|G\|_{L^{2}_{\mu}(\mathcal{X};\mathcal{Y}) }\right).\]

Proof.: Observe that the random variable \(\|\!|G|\!|\!|_{\text{disc},\mu}^{2}\) satisfies

\[\mathbb{E}\|\!|G|\!|\!|_{\text{disc},\mu}^{2}=\|G\|_{L^{2}_{\mu}(\mathcal{X}; \mathcal{Y})}^{2}.\]For (a), we use Markov's inequality to get

\[\mathbb{P}\left(\|G\|_{\mathsf{disc},\mu}\geq\|G\|_{L^{2}_{\mu}(\mathcal{X}; \mathcal{Y})}/\sqrt{\epsilon}\right)\leq\frac{\mathbb{E}\|G\|_{\mathsf{disc}, \mu}^{2}}{\|G\|_{L^{2}_{\mu}(\mathcal{X};\mathcal{Y})}^{2}/\epsilon}=\epsilon,\]

as required.

The proof of (b) is based on [3, Lem. 7.11]. We repeat it here for convenience. Define the random variable \(Z_{i}=\|G(X_{i})\|_{\mathcal{Y}}^{2}\) and observe that

\[\mathbb{E}(Z_{i})=\mathbb{E}_{X\sim\mu}\|G(X)\|_{\mathcal{Y}}^{2}=\|G\|_{L^{2} _{\mu}(\mathcal{X};\mathcal{Y})}^{2}=:a.\]

Let \(X_{i}=Z_{i}-\mathbb{E}(Z_{i})\) so that

\[\|G\|_{\mathsf{disc},\mu}^{2}=\frac{1}{m}\sum_{i=1}^{m}Z_{i}=\frac{1}{m}\sum_{ i=1}^{m}X_{i}+a.\]

Now let \(b=\|G\|_{L^{\infty}_{\mu}(\mathcal{X};\mathcal{Y})}^{2}\) and observe that

\[X_{i}\leq Z_{i}\leq b,\quad-X_{i}\leq\mathbb{E}(Z_{i})\leq b,\quad\text{a.s.}.\]

We also have

\[\sum_{i=1}^{m}\mathbb{E}(X_{i}^{2})\leq\sum_{i=1}^{m}\mathbb{E}(Z_{i}^{2})\leq b \sum_{i=1}^{m}\mathbb{E}(Z_{i})=abm.\]

Therefore, Bernstein's inequality for bounded random variables (see, e.g., [29, Cor. 7.31]) implies that

\[\mathbb{P}\left(\left|\frac{1}{m}\sum_{i=1}^{m}X_{i}\right|\geq t\right)\leq 2 \exp\left(-\frac{t^{2}m/2}{ab+bt/3}\right)\]

for any \(t>0\). We now set \(t=a+b/r\) and notice that \(\frac{t^{2}m/2}{ab+bt/3}\geq\frac{3m}{5r}\geq\log(2/\epsilon)\). Therefore,

\[\left|\frac{1}{m}\sum_{i=1}^{m}X_{i}\right|<a+b/r\]

with probability at least \(1-\epsilon\). It follows that

\[\|G\|_{\mathsf{disc},\mu}\leq\sqrt{2a+b/r}\leq\sqrt{2}\left(\sqrt{a}+\sqrt{b/ r}\right)\]

with the same probability. Substituting the values for \(a\) and \(b\) now gives the result. 

**Lemma D.15**.: _Let \(F\in L^{\infty}_{\mu}(\mathcal{X};\mathcal{Y})\), \(q\in\mathcal{P}_{\mathcal{S};\mathcal{Y}}\) be arbitrary and \(m\geq 2r\log(6/\epsilon)\) for some \(r>0\) and \(0<\epsilon<1\). Then_

\[E_{2}(F,q) \lesssim\|F-q\circ\iota\|_{L^{\infty}_{\mu}(\mathcal{X};\mathcal{ Y})}/\sqrt{r}+\|F-q\circ\iota\|_{L^{2}_{\mu}(\mathcal{X};\mathcal{Y})}\] \[\quad+\sqrt{k/\epsilon}\|q\|_{L^{2}_{\mu}(D;\mathcal{Y})}\|t_{d \mathcal{X}}-\iota_{d_{\mathcal{X}}}\circ\widetilde{\mathcal{D}}_{\mathcal{X }}\circ\widetilde{\mathcal{E}}_{\mathcal{X}}\|_{L^{2}_{\mu}(\mathcal{X}; \ell^{\infty}(\mathbb{R}^{d_{\mathcal{X}}}))}.\]

_and_

\[E_{\infty}(F,q)\lesssim (1+\sqrt{k/r})\|F-q\circ\iota\|_{L^{\infty}_{\mu}(\mathcal{X}; \mathcal{Y})}+\sqrt{k}\|F-q\circ\iota\|_{L^{2}_{\mu}(\mathcal{X};\mathcal{Y})}\] \[+k\|q\|_{L^{2}_{\mu}(D;\mathcal{Y})}\|t_{d\mathcal{X}}-\iota_{d_{ \mathcal{X}}}\circ\widetilde{\mathcal{D}}_{\mathcal{X}}\circ\widetilde{ \mathcal{E}}_{\mathcal{X}}\|_{L^{2}_{\mu}(\mathcal{X};\ell^{\infty}(\mathbb{R}^ {d_{\mathcal{X}}}))}\] \[+\sqrt{k}(1+\sqrt{k/r})\|q\|_{L^{2}_{\mu}(D;\mathcal{Y})}\|t_{d \mathcal{X}}-\iota_{d_{\mathcal{X}}}\circ\widetilde{\mathcal{D}}_{\mathcal{X }}\circ\widetilde{\mathcal{E}}_{\mathcal{X}}\|_{L^{\infty}_{\mu}(\mathcal{X}; \ell^{\infty}(\mathbb{R}^{d_{\mathcal{X}}}))}\]

_with probability at least \(1-\epsilon\)._Proof.: We apply part (b) of Lemma D.14 with \(\epsilon\) replaced by \(\epsilon/3\) to the term \(\|F-q\circ\iota\|_{\mathsf{disc},\mu}\) and parts (a) and (b) of Lemma D.14 with \(\epsilon\) replaced by \(\epsilon/3\) to the term \(\left\|\!\left|\!\left|q\circ\iota-q\circ\mathcal{E}_{\mathcal{X}}\right|\! \right|\!\right|_{\mathsf{disc},\mu}\). Using the union bound, this gives that

\[\left\|\!\left|\!\left|F-q\circ\iota\right|\!\right|\!\right|_{ \mathsf{disc},\mu} \lesssim\left\|F-q\circ\iota\right\|_{L^{2}_{\mu}(\mathcal{X}; \mathcal{Y})}+\left\|F-q\circ\iota\right\|_{L^{\infty}_{\mu}(\mathcal{X}; \mathcal{Y})}/\sqrt{r}\] \[\left\|\!\left|\!\left|q\circ\iota-q\circ\mathcal{E}_{\mathcal{X }}\right|\!\right|\!\right|_{\mathsf{disc},\mu} \lesssim\left\|q\circ\iota-q\circ\mathcal{E}_{\mathcal{X}}\right\| _{L^{2}_{\mu}(\mathcal{X};\mathcal{Y})}/\sqrt{\epsilon}\] \[\left\|\!\left|\!\left|\!\left|q\circ\iota-q\circ\mathcal{E}_{ \mathcal{X}}\right|\!\right|\!\right|_{\mathsf{disc},\mu} \lesssim\left\|q\circ\iota-q\circ\mathcal{E}_{\mathcal{X}}\right\| _{L^{\infty}_{\mu}(\mathcal{X};\mathcal{Y})}/\sqrt{r}+\left\|q\circ\iota-q \circ\mathcal{E}_{\mathcal{X}}\right\|_{L^{2}_{\mu}(\mathcal{X};\mathcal{Y})}\]

with probability at least \(1-\epsilon\). This yields

\[E_{2}(F,q) \lesssim\|F-q\circ\iota\|_{L^{2}_{\mu}(\mathcal{X};\mathcal{Y})}+ \|F-q\circ\iota\|_{L^{\infty}_{\mu}(\mathcal{X};\mathcal{Y})}/\sqrt{r}\] \[\quad+\|q\circ\iota-q\circ\mathcal{E}_{\mathcal{X}}\|_{L^{2}_{ \mu}(\mathcal{X};\mathcal{Y})}/\sqrt{\epsilon}\] \[E_{\infty}(F,q) \lesssim(1+\sqrt{k/r})\|F-q\circ\iota\|_{L^{\infty}_{\mu}( \mathcal{X};\mathcal{Y})}+\sqrt{k}\|F-q\circ\iota\|_{L^{2}_{\mu}(\mathcal{X}; \mathcal{Y})}\] \[\quad+(1+\sqrt{k/r})\|q\circ\iota-q\circ\mathcal{E}_{\mathcal{X }}\|_{L^{\infty}_{\mu}(\mathcal{X};\mathcal{Y})}+\sqrt{k}\|q\circ\iota-q \circ\mathcal{E}_{\mathcal{X}}\|_{L^{2}_{\mu}(\mathcal{X};\mathcal{Y})}\]

with the same probability. It remains to bound the terms involving \(q\circ\iota-q\circ\mathcal{E}_{\mathcal{X}}\). Using (A.III), Lemma D.2 and the fact that \(q\) depends on its first \(d_{\mathcal{X}}\) variables only, we see that

\[\left\|q\circ\iota(X)-q\circ\mathcal{E}_{\mathcal{X}}(X)\right\|_{\mathcal{Y}} \leq\frac{1}{2}\sqrt{k}\left\|\!\left|\!\left|q\right|\!\right|\! \right|_{L^{2}_{\theta}(D;\mathcal{Y})}\|\iota_{d_{\mathcal{X}}}(X)- \mathcal{E}_{\mathcal{X}}(X)\|_{\infty}.\]

Therefore

\[\left\|q\circ\iota-q\circ\mathcal{E}_{\mathcal{X}}\right\|_{L^{2}_{ \mu}(\mathcal{X};\mathcal{Y})} \lesssim\sqrt{k}\left\|\!\left|\!\left|q\right|\!\right|\!\right|_{L^{2 }_{\theta}(D;\mathcal{Y})}\|\iota_{d_{\mathcal{X}}}-\mathcal{E}_{\mathcal{X} }\|_{L^{2}_{\mu}(\mathcal{X};\ell^{\infty}(\mathbb{N}))}\] \[\left\|q\circ\iota-q\circ\mathcal{E}_{\mathcal{X}}\right\|_{L^{ \infty}_{\mu}(\mathcal{X};\mathcal{Y})} \lesssim\sqrt{k}\left\|\!\left|\!\left|q\right|\!\right|\!\right|_{L^{2 }_{\theta}(D;\mathcal{Y})}\|\iota_{d_{\mathcal{X}}}-\mathcal{E}_{\mathcal{X} }\|_{L^{\infty}_{\mu}(\mathcal{X};\ell^{\infty}(\mathbb{N}))}\]

Substituting this into the previous bounds and using the definition of \(\mathcal{E}_{\mathcal{X}}\) from (A.III) now gives the result. 

We are now ready to choose the polynomial \(q\). First, we require the following technical lemma, which shows the existence of multi-index sets of weighted cardinality \(k\) which achieve the desired algebraic rates of convergence.

**Lemma D.16**.: _Let \(f=\sum_{\boldsymbol{\nu}\in\mathcal{F}}c_{\boldsymbol{\nu}}\Psi_{\boldsymbol{ \nu}}\) satisfy \(f\in\mathcal{H}(\boldsymbol{b})\) for some \(\boldsymbol{b}\in\ell^{p}(\mathbb{N})\) with \(\boldsymbol{b}\geq\boldsymbol{0}\). Then there are index sets \(S_{1},S_{2}\subset\mathcal{F}\) with \(|S_{1}|_{\boldsymbol{v}},|S_{2}|_{\boldsymbol{v}}\leq k\) such that_

\[\left\|\!\left|\boldsymbol{c}-\boldsymbol{c}_{S_{1}}\right\|\!\right|_{2; \mathcal{Y}}\leq C(\boldsymbol{b},p,\xi)\cdot k^{1/2-1/p},\quad\left\| \boldsymbol{c}-\boldsymbol{c}_{S_{2}}\right\|_{1,\boldsymbol{v};\mathcal{Y}} \leq C(\boldsymbol{b},p,\xi)\cdot k^{1-1/p},\]

_where \(C(\boldsymbol{b},p,\xi)\geq 0\) depends on \(\boldsymbol{b}\), \(p\) and \(\xi\) only._

Proof.: We first show that \(\boldsymbol{c}\in\ell^{p}_{\boldsymbol{v}}(\mathcal{F};\mathcal{Y})\). By definition of \(\mathcal{H}(\boldsymbol{b})\) (see (2.3)), \(f\) is holomorphic in every Bernstein polyellipse \(\mathcal{E}(\boldsymbol{\rho})\) for which \(\boldsymbol{\rho}\) satisfies

\[\boldsymbol{\rho}\geq\boldsymbol{1},\quad\sum_{j=1}^{\infty}\left(\frac{\rho_{ j}+\rho_{j}^{-1}}{2}-1\right)b_{j}\leq 1.\] (D.25)

Using [4, Lem. 5.3] (which is based on [102, Cor. B.2.7]) with \(\boldsymbol{\alpha}=\boldsymbol{\beta}=\boldsymbol{0}\), we get that \(\left\|c_{\boldsymbol{0}}\right\|_{\mathcal{Y}}\leq 1\) and

\[\left\|c_{\boldsymbol{\nu}}\right\|_{\mathcal{Y}}\leq\prod_{k\in I(\boldsymbol{ \nu},\boldsymbol{\rho})}\frac{\rho_{k}^{-\nu_{k}+1}}{(\rho_{k}-1)^{2}}(\nu_{k}+1 ),\quad\boldsymbol{\nu}\in\mathcal{F}\backslash\{\boldsymbol{0}\}\]

for all such \(\boldsymbol{\rho}\), where \(I(\boldsymbol{\nu},\boldsymbol{\rho})=\operatorname{supp}(\boldsymbol{\nu}) \cap\{k:\rho_{k}>1\}\). Define the sequence \(d_{\boldsymbol{0}}=1\) and

\[d_{\boldsymbol{\nu}}=v_{\boldsymbol{\nu}}^{2/p-1}\cdot\inf\left\{\prod_{k\in I( \boldsymbol{\nu},\boldsymbol{\rho})}\frac{\rho_{k}^{-\nu_{k}+1}}{(\rho_{k}-1)^{2 }}(\nu_{k}+1):\boldsymbol{\rho}\text{ satisfies (D.25)}\right\},\quad\boldsymbol{\nu}\in\mathcal{F} \backslash\{\boldsymbol{0}\}.\]Using (C.3)-(C.4), we see that

\[v_{\boldsymbol{\nu}}=\prod_{k\in\operatorname{supp}(\boldsymbol{\nu})}(2\nu_{k}+1 )^{(5+\xi)/2}\]

and therefore

\[d_{\boldsymbol{\nu}}\leq\prod_{k\in I(\boldsymbol{\nu},\boldsymbol{\rho})} \frac{\rho_{k}^{-\nu_{k}+1}}{(\rho_{k}-1)^{2}}(2\nu_{k}+1)^{\gamma}\]

for all \(\boldsymbol{\rho}\) satisfying (D.25) and some \(\gamma=\gamma(p,\xi)\geq 0\). We now use [4, Lem. 5.4] to deduce that \(\boldsymbol{d}=(d_{\boldsymbol{\nu}})_{\boldsymbol{\nu}\in\mathcal{F}}\in \ell^{p}(\mathbb{N})\) with \(\left\|\boldsymbol{d}\right\|_{p}\leq C(\boldsymbol{b},p,\xi)\) for some \(C(\boldsymbol{b},p,\xi)\geq 0\) depending on \(\boldsymbol{b}\), \(p\) and \(\xi\) only. Returning to \(\boldsymbol{c}\), this gives

\[\left\|\boldsymbol{c}\right\|_{p,\boldsymbol{v};\mathcal{Y}}^{p}=\sum_{ \boldsymbol{\nu}\in\mathcal{F}}v_{\boldsymbol{\nu}}^{2-p}\|\boldsymbol{c}_{ \boldsymbol{\nu}}\|_{\mathcal{Y}}^{p}\leq\sum_{\boldsymbol{\nu}\in\mathcal{F }}|d_{\boldsymbol{\nu}}|^{p}\leq C(\boldsymbol{b},p,\xi)^{p}.\]

Hence \(\boldsymbol{c}\in\ell_{\boldsymbol{v}}^{p}(\mathcal{F};\mathcal{Y})\) with \(\left\|\boldsymbol{c}\right\|_{p,\boldsymbol{v};\mathcal{Y}}\leq C( \boldsymbol{b},p,\xi)\), as required.

The second step involves the application of the weighted Stechkin's inequality (see [3, Lem. 3.12]). This gives that

\[\min\left\{\left\|\boldsymbol{c}-\boldsymbol{c}_{S}\right\|_{q,\boldsymbol{v} ;\mathcal{Y}}:S\subset\mathcal{F},\;\left|S\right|_{\boldsymbol{v}}\leq k \right\}=:\sigma_{k}(\boldsymbol{c})_{q,\boldsymbol{v};\mathcal{Y}}\leq\left\| \boldsymbol{c}\right\|_{p,\boldsymbol{v};\mathcal{Y}}k^{1/q-1/p},\]

for any \(q\in(p,2]\) and \(k>0\). Applying this result with \(q=2\) implies the existence of the set \(S_{1}\) (recall that \(\left\|\cdot\right\|_{2,\boldsymbol{v};\mathcal{Y}}=\left\|\cdot\right\|_{2, \mathcal{Y}}\)) and applying it with \(q=1\) implies the existence of the set \(S_{2}\). 

We now define the set

\[\Lambda_{n}^{\text{HCI}}=\left\{\boldsymbol{\nu}=(\nu_{k})_{k=1}^{\infty}\in \mathcal{F}:\prod_{k:\nu_{k}\neq 0}(\nu_{k}+1)\leq n,\;\nu_{k}=0,\;k>n\right\} \subset\mathcal{F}.\] (D.26)

Notice that \(\Lambda_{n}^{\text{HCI}}\) is isomorphic to an index set in \(\mathbb{N}_{n}^{n}\) by the natural restriction map.

**Lemma D.17**.: _Let \(k>0\) and suppose that \(\Lambda\supseteq\Lambda_{n}^{\text{HCI}}\) for some \(n\in\mathbb{N}\). Let \(f=\sum_{\boldsymbol{\nu}\in\mathcal{F}}c_{\boldsymbol{\nu}}\Psi_{\boldsymbol{\nu}}\) satisfy \(f\in\mathcal{H}(\boldsymbol{b})\) for some \(\boldsymbol{b}\in\ell_{\boldsymbol{b}}^{p}(\mathbb{N})\) with \(\boldsymbol{b}\geq\boldsymbol{0}\). Then there exists an index set \(S\in\mathcal{S}\) such that_

\[\left\|f-f_{S}\right\|_{L_{\varrho}^{\infty}(D;\mathcal{Y})}\leq C( \boldsymbol{b},p,\xi)\cdot\left(k^{1-1/p}+n^{1-1/p}\right),\] (D.27)

_where \(f_{S}=\sum_{\boldsymbol{\nu}\in S}c_{\boldsymbol{\nu}}\Psi_{\boldsymbol{\nu}}\). Moreover, if \(\mathcal{Y}\) is a Hilbert space, then we also have that_

\[\left\|f-f_{S}\right\|_{L_{\varrho}^{2}(D;\mathcal{Y})}\leq C( \boldsymbol{b},p,\xi)\cdot\left(k^{1/2-1/p}+n^{1/2-1/p}\right).\] (D.28)

_Here \(C(\boldsymbol{b},p,\xi)>0\) is a constant depending on \(\boldsymbol{b}\), \(p\) and \(\xi\) only._

Proof of Lemma D.17.: The previous lemma implies that there exist index sets \(S_{1},S_{2}\subset\mathcal{F}\) with \(\left|S_{1}\right|_{v},\left|S_{2}\right|_{\boldsymbol{v}}\leq k/2\) such that

\[\left\|\boldsymbol{c}-\boldsymbol{c}_{S_{1}}\right\|_{2,\boldsymbol{y}}\leq C( \boldsymbol{b},p,\xi)\cdot k^{1/2-1/p},\quad\left\|\boldsymbol{c}-\boldsymbol{ c}_{S_{2}}\right\|_{1,\boldsymbol{v};\mathcal{Y}}\leq C(\boldsymbol{b},p,\xi) \cdot k^{1-1/p}.\]

Now define \(S=S_{1}\cup S_{2}\cap\Lambda\) and notice that \(S\subseteq\Lambda\) and \(\left|S\right|_{\boldsymbol{v}}\leq\left|S_{1}\right|_{\boldsymbol{v}}+\left|S_ {2}\right|_{\boldsymbol{v}}\leq k\). Hence \(S\in\mathcal{S}\). Since \(v_{\boldsymbol{\nu}}\geq u_{\boldsymbol{\nu}}=\left\|\Psi_{\boldsymbol{\nu}} \right\|_{L_{\varrho}^{\infty}(D)}\), we have (using [5, Lem. 5.1])

\[\left\|f-f_{S}\right\|_{L_{\varrho}^{\infty}(D;\mathcal{Y})}\leq\left\| \boldsymbol{c}-\boldsymbol{c}_{S}\right\|_{1,\boldsymbol{u};\mathcal{Y}}\leq \left\|\boldsymbol{c}-\boldsymbol{c}_{S_{2}}\right\|_{1,\boldsymbol{v}; \mathcal{Y}}+\left\|\boldsymbol{c}-\boldsymbol{c}_{\Lambda}\right\|_{1, \boldsymbol{u};\mathcal{Y}}.\]

Hence, to complete the proof of the first result, we need only show that

\[\left\|\boldsymbol{c}-\boldsymbol{c}_{\Lambda}\right\|_{1,\boldsymbol{u}; \mathcal{Y}}\leq C(\boldsymbol{b},p)\cdot n^{1-1/p}\]

for some constant \(C(\boldsymbol{b},p)\geq 0\). First, by construction, \(\Lambda\supseteq\Lambda_{n}^{\text{HCI}}\) contains every anchored set of size at most \(n\). See, e.g.. Therefore \(\left\|\boldsymbol{c}-\boldsymbol{c}_{\Lambda}\right\|_{1,\boldsymbol{u}; \mathcal{Y}}\leq\left\|\boldsymbol{c}-\boldsymbol{c}_{S}\right\|_{1,\boldsymbol{ u};\mathcal{Y}}\) for any such set \(S\). The result now follows from [5, Cor. 8.2].

Now suppose that \(\mathcal{Y}\) is a Hilbert space. Then Parseval's identity gives that

\[\left\|f-f_{S}\right\|_{L_{\varrho}^{2}(D;\mathcal{Y})}=\left\|\boldsymbol{c}- \boldsymbol{c}_{S}\right\|_{2,\mathcal{Y}}\leq\left\|\boldsymbol{c}-\boldsymbol{ c}_{S_{1}}\right\|_{2,\mathcal{Y}}+\left\|\boldsymbol{c}-\boldsymbol{c}_{ \Lambda}\right\|_{2,\mathcal{Y}}.\]

As before, it suffices to show that

\[\left\|\boldsymbol{c}-\boldsymbol{c}_{\Lambda}\right\|_{2,\mathcal{Y}}\leq C( \boldsymbol{b},p)\cdot n^{1/2-1/p}.\]

This follows from the same approach and [5, Cor. 8.2] once more.

### Final arguments

We are now, finally, ready to prove Theorem 3.1. We first consider the case where \(\mathcal{Y}\) is a Banach space, then treat the case where \(\mathcal{Y}\) is a Hilbert space afterwards.

Proof of Theorem 3.1 when \(\mathcal{Y}\) is a Banach space.: We divide the proof into a series of steps.

_Step 1: Setup and DNN width/depth bounds._ Let \(m\), \(\delta\), \(\epsilon\) and \(L\) be as in the theorem statement. We may without loss of generality assume that \(\delta\leq 1/5\). Now let

\[n=\left\lceil\frac{m}{L}\right\rceil\leq d_{\mathcal{X}}\] (D.29)

and

\[\Lambda=\Lambda_{n}^{\text{HCI}},\]

where \(\Lambda_{n}^{\text{HCI}}\) is as in (D.26). We also set

\[\xi=1/\delta-5\geq 0\] (D.30)

and

\[k=\frac{m}{cL},\] (D.31)

where \(c\geq 1\) is a universal constant that will be chosen in the next step. Finally, let \(\delta=2^{-m}/m\) and \(\mathcal{N}\) by the \(\tanh\) DNN family (D.19), where the \(N_{\boldsymbol{\nu}}\) are as in Lemma D.9 for this \(\Lambda\) and value of \(\delta\).

By (D.20) and the definition of \(\xi\) and \(k\), we immediately see that

\[\operatorname{width}(\mathcal{N})\lesssim(m/L)^{1+\delta},\qquad \operatorname{depth}(\mathcal{N})\lesssim\log(m/L).\]

This yields the width and depth bounds (3.1). The rest of the proof is therefore devoted to showing the error bounds (3.3)-(3.4).

_Step 2: Ensuring (D.7) holds with probability at least \(1-\epsilon/2\)._ A standard bound (see, e.g., the proof of Lemma 6.4 in [5]) gives that \(N=|\Lambda|\) satisfies

\[\log(\mathrm{e}N)\leq 4\log^{2}(\mathrm{e}n)\lesssim\log^{2}(m).\] (D.32)

Here, in the final step we used the fact that \(m\geq 3\) and \(L(m,\epsilon)\geq 1\). This and the fact that \(c\geq 1\) also implies that \(k\leq m\). Therefore, the right-hand side of (D.13) with \(\epsilon\) replaced by \(\epsilon/2\) satisfies

\[c_{0}\cdot k\cdot(\log(\mathrm{e}N)\cdot\log^{2}(k)+\log(4/\epsilon))\lesssim k \cdot L(m,\epsilon).\] (D.33)

Hence, for sufficiently large \(c\geq 1\), we deduce that (D.13) holds with \(\epsilon/2\). Using (A.I), we see that

\[\|\iota_{d_{\mathcal{X}}}-\iota_{d_{\mathcal{X}}}\circ\widetilde{\mathcal{D} }_{\mathcal{X}}\circ\widetilde{\mathcal{E}}_{\mathcal{X}}\|_{L^{\infty}_{ \mathcal{X}}(\mathcal{X};\ell^{\infty}(\mathbb{N}))}\leq L_{i}\|\mathcal{I}_ {\mathcal{X}}-\widetilde{\mathcal{D}}_{\mathcal{X}}\circ\widetilde{\mathcal{E }}_{\mathcal{X}}\|_{L^{\infty}_{\mathcal{X}}(\mathcal{X};\mathcal{X})},\quad q =2,\infty.\] (D.34)

Hence (D.14) is implied by (3.2). We conclude from Lemma D.8 that (D.7) holds with probability at least \(1-\epsilon/2\) and \(\alpha\gtrsim 1\).

_Step 3: Error analysis._ Let \(f\in\mathcal{H}(\boldsymbol{b})\) be the function asserted by (A.II) and define \(q=f_{S}\) as the polynomial asserted by Lemma D.17 with \(n\) as in (D.29). We also observe that

\[\|F\|_{L^{\infty}_{\mu}(\mathcal{X};\mathcal{Y})}=\|f\|_{L^{\infty}_{\epsilon} (D;\mathcal{Y})}\lesssim\|f\|_{L^{\infty}_{\epsilon}(D;\mathcal{Y})}\lesssim 1,\] (D.35)

since \(f\in\mathcal{H}(\boldsymbol{b})\). We now apply Theorem D.13 with \(\sigma=1\) and use (D.31), the definition of \(\delta\) and (3.8) to see that

\[\begin{split}&\left|\!\left|\!\left|F-\widehat{F}\right|\!\right|\! \right|_{L^{2}_{\mu}(\mathcal{X};\mathcal{Y})}\lesssim&\left|\! \left|\!\left|F-\mathcal{Q}\circ F\right|\!\right|\!\right|_{L^{2}_{\mu}( \mathcal{X};\mathcal{Y})}+\left|\!\left|\!\left|F-\mathcal{Q}\circ F\right|\! \right|\!\right|\!\right|_{\text{disc},\mu}\\ &+\pi_{\mathcal{Q}}E_{2}(F,q)+E_{\text{opt},2}+E_{\text{samp},2} \\ \left|\!\left|\!\left|F-\widehat{F}\right|\!\right|_{L^{\infty}_{ \mu}(\mathcal{X};\mathcal{Y})}\lesssim&\left|\!\left|F-\mathcal{Q} \circ F\right|\!\right|\!\right|_{L^{\infty}_{\mu}(\mathcal{X};\mathcal{Y})}+ \sqrt{k}\!\left|\!\left|\!\left|F-\mathcal{Q}\circ F\right|\!\right|\!\right| \!\right|_{\text{disc},\mu}\\ &+\pi_{\mathcal{Q}}E_{\infty}(F,q)+E_{\text{opt},\infty}+E_{\text {samp},\infty}\end{split}\] (D.36)

with probability at least \(1-\epsilon/2\), where \(E_{2}(F,q)\) and \(E_{\infty}(F,q)\) are as in (D.24), \(\mathcal{Q}:\mathcal{Y}\to\widetilde{\mathcal{Y}}=\mathcal{D}_{\mathcal{Y}}( \mathbb{R}^{d_{\mathcal{Y}}})\) is any bounded linear operator and \(\pi_{\mathcal{Q}}=\|\mathcal{Q}\|_{\mathcal{Y}\to\mathcal{Y}}\).

Notice that Parseval's identity and (D.35) imply that

\[\left|\kern-1.075pt\left|\kern-1.075pt\left|q\right|\kern-1.075pt\right|\kern-1.075pt \right|\kern-1.075pt\right|_{L^{2}_{\varphi}(D;\mathcal{Y})}\leq\left|\kern-1.075pt \left|\kern-1.075pt\left|f\right|\kern-1.075pt\right|\kern-1.075pt\right|_{L^{ \infty}_{\mu}(\mathcal{D};\mathcal{Y})}\leq 1.\]

Hence, this, the previous bounds, Lemma D.15 with \(\epsilon\) replaced by \(\epsilon/4\) and the union bound yield

\[\left|\kern-1.075pt\left|\kern-1.075pt\left|F-\widehat{F}\right| \kern-1.075pt\right|\kern-1.075pt\right|_{L^{2}_{\mu}(\mathcal{X};\mathcal{Y})} \lesssim\left|\kern-1.075pt\left|F-\mathcal{Q}\circ F\right|\kern-1.075pt \right|\kern-1.075pt\right|_{L^{2}_{\mu}(\mathcal{X};\mathcal{Y})}+\left| \kern-1.075pt\left|F-\mathcal{Q}\circ F\right|\kern-1.075pt\right|\kern-1.075pt \right|_{\text{disc},\mu}\] \[\quad+\pi_{\mathcal{Q}}\left(\|F-q\circ\iota\right|\kern-1.075pt \right|\kern-1.075pt\right|_{L^{\infty}_{\mu}(\mathcal{X};\mathcal{Y})}/ \sqrt{r}+\|F-q\circ\iota\|\kern-1.075pt\left|\kern-1.075pt\left|L^{2}_{\mu}( \mathcal{X};\mathcal{Y})\right)\] \[\quad+\pi_{\mathcal{Q}}\sqrt{k/\epsilon}\|\iota_{d_{\mathcal{X} }}-\iota_{d_{\mathcal{X}}}\circ\widetilde{\mathcal{D}}_{\mathcal{X}}\circ \widetilde{\mathcal{E}}_{\mathcal{X}}\|_{L^{2}_{\mu}(\mathcal{X};\epsilon^{ \infty}(\mathbb{R}^{d_{\mathcal{X}}}))}\] \[\quad+E_{\text{opt},2}+E_{\text{samp},2}\]

and

\[\|F-\widehat{F}\|\kern-1.075pt\left|\kern-1.075pt\right|\kern-1.075pt \right|_{L^{\infty}_{\mu}(\mathcal{X};\mathcal{Y})} \lesssim\|F-\mathcal{Q}\circ F\|\kern-1.075pt\left|\kern-1.075pt \left|\kern-1.075pt\left|F-\mathcal{Q}\circ F\right|\kern-1.075pt\right| \kern-1.075pt\right|_{\text{disc},\mu}\] \[\quad+\pi_{\mathcal{Q}}\left((1+\sqrt{k/r})\|F-q\circ\iota\| \kern-1.075pt\right|\kern-1.075pt\right|_{L^{\infty}_{\mu}(\mathcal{X}; \mathcal{Y})}+\sqrt{k}\|F-q\circ\iota\|\kern-1.075pt\left|\kern-1.075pt\left| \kern-1.075pt\left|L^{2}_{\mu}(\mathcal{X};\mathcal{Y})\right)\] \[\quad+\pi_{\mathcal{Q}}k\|\iota_{d_{\mathcal{X}}}-\iota_{d_{ \mathcal{X}}}\circ\widetilde{\mathcal{D}}_{\mathcal{X}}\circ\widetilde{ \mathcal{E}}_{\mathcal{X}}\|_{L^{2}_{\mu}(\mathcal{X};\epsilon^{\infty}( \mathbb{R}^{d_{\mathcal{X}}}))}\] \[\quad+\pi_{\mathcal{Q}}\sqrt{k}(1+\sqrt{k/r})\|\iota_{d_{ \mathcal{X}}}-\iota_{d_{\mathcal{X}}}\circ\widetilde{\mathcal{D}}_{\mathcal{ X}}\circ\widetilde{\mathcal{E}}_{\mathcal{X}}\|_{L^{\infty}_{\mu}(\mathcal{X}; \epsilon^{\infty}(\mathbb{R}^{d_{\mathcal{X}}}))}\] \[\quad+E_{\text{opt},\infty}+E_{\text{samp},\infty}\]

with probability at least \(1-3\epsilon/4\), for any \(r\) such that \(m\geq 2r\log(24/\epsilon)\). In particular, we may choose \(r=k\) due the definition of \(k\) (D.31). We next bound the discrete error \(\left|\kern-1.075pt\left|\kern-1.075pt\left|F-\mathcal{Q}\circ F\right| \kern-1.075pt\right|\kern-1.075pt\right|_{\text{disc},\mu}\). Applying Lemma D.14 with \(\epsilon\) replaced by \(\epsilon/8\) and the union bound, we see that

\[\left|\kern-1.075pt\left|\kern-1.075pt\left|F-\mathcal{Q}\circ F \right|\kern-1.075pt\right|\kern-1.075pt\right|\kern-1.075pt\right|_{\text{disc },\mu} \lesssim\|F-\mathcal{Q}\circ F\|\kern-1.075pt\right|_{L^{2}_{\mu}( \mathcal{X};\mathcal{Y})}/\sqrt{\epsilon}\] \[\left|\kern-1.075pt\left|\kern-1.075pt\left|F-\mathcal{Q}\circ F \right|\kern-1.075pt\right|\kern-1.075pt\right|\kern-1.075pt\right|_{\text{ disc},\mu} \lesssim\|F-\mathcal{Q}\circ F\|\kern-1.075pt\left|\kern-1.075pt\right|_{L^{\infty}_{ \mu}(\mathcal{X};\mathcal{Y})}/\sqrt{r}+\|F-\mathcal{Q}\circ F\|_{L^{2}_{\mu}( \mathcal{X};\mathcal{Y})}\]

with probability at least \(1-\epsilon/4\), provided \(m\geq 2r\log(16/\epsilon)\). In particular, we may take \(r=k\) once more. Substituting this into the previous expressions, setting \(r=k\) throughout, using the union bound once more and recalling (3.7), (D.31) and (D.34), we deduce that

\[\left|\kern-1.075pt\left|\kern-1.075pt\left|F-\widehat{F}\right| \kern-1.075pt\right|\kern-1.075pt\right|\kern-1.075pt\right|_{L^{2}_{\mu}( \mathcal{X};\mathcal{Y})} \lesssim\|F-\mathcal{Q}\circ F\|\kern-1.075pt\left|\kern-1.075pt\right| \kern-1.075pt\right|_{L^{2}_{\mu}(\mathcal{X};\mathcal{Y})}/\sqrt{\epsilon}\] \[\quad+\pi_{\mathcal{Q}}\left(\|F-q\circ\iota\|\kern-1.075pt\right| \kern-1.075pt\right|_{L^{\infty}_{\mu}(\mathcal{X};\mathcal{Y})}/\sqrt{m/L}+\|F-q \circ\iota\|\kern-1.075pt\left|\kern-1.075pt\left|\kern-1.075pt\left|L^{2}_{\mu} (\mathcal{X};\mathcal{Y})\right)\] \[\quad+(\pi_{\mathcal{Q}}/a_{\mathcal{Y}})\cdot E_{\mathcal{X},2}+E _{\text{opt},2}+E_{\text{samp},2}\]

and

\[\|F-\widehat{F}\|\kern-1.075pt\left|\kern-1.075pt\left|\kern-1.075pt \left|\kern-1.075pt\left|\kern-1.075pt\left|F-\mathcal{Q}\circ F\right|\kern-1.075pt \right|\kern-1.075pt\right|\kern-1.075pt\right|_{L^{\infty}_{\mu}(\mathcal{X}; \mathcal{Y})}+\sqrt{m/L}\|F-\mathcal{Q}\circ F\|\kern-1.075pt\right|_{L^{2}_{ \mu}(\mathcal{X};\mathcal{Y})}\] \[\quad+\pi_{\mathcal{Q}}\left(\|F-q\circ\iota\|\kern-1.075pt\right| \kern-1.075pt\right|_{L^{\infty}_{\mu}(\mathcal{X};\mathcal{Y})}+\sqrt{m/L} \|F-q\circ\iota\|\kern-1.075pt\right|_{L^{2}_{\mu}(\mathcal{X};\mathcal{Y})}\right)\] \[\quad+(\pi_{\mathcal{Q}}/a_{\mathcal{Y}})\cdot E_{\mathcal{X}, \infty}+E_{\text{opt},\infty}+E_{\text{samp},\infty}\]

with probability at least \(1-\epsilon\). This holds for any bounded linear operator \(\mathcal{Q}:\mathcal{Y}\to\widetilde{\mathcal{Y}}=D_{\mathcal{Y}}(\mathbb{R}^{d_{ \mathcal{Y}}})\). We now set \(\mathcal{Q}=\mathcal{D}_{\mathcal{Y}}\circ\mathcal{E}_{\mathcal{Y}}\), which is linear and bounded by (A.IV) with \(\pi_{\mathcal{Q}}=\left|\kern-1.075pt\left|\kern-1.075pt\left|\kern-1.075pt\left| \mathcal{D}_{\mathcal{Y}}\circ\mathcal{E}_{\mathcal{Y}}\right|\kern-1.075pt \right|\kern-1.075pt\right|_{\mathcal{Y}\to\mathcal{Y}}=a_{\mathcal{Y}}\) by definition. Using this, we observe that

\[_Step 4: Bounding the polynomial error terms._ It remains to bound the error terms \(F-q\circ\iota\) in (D.37). Using (A.1), (A.II) and Lemma D.17, we now notice that

\[\|F-q\circ\iota\|_{L^{\infty}_{\mu}(\mathcal{X};\mathcal{Y})}=\|f-q\|_{L^{ \infty}_{\mu}(D;\mathcal{Y})}\lesssim\|f-q\|_{L^{\infty}_{\omega}(D;\mathcal{Y })}\lesssim C(\boldsymbol{b},p,\xi)\cdot(k^{1-1/p}+n^{1-1/p}).\]

Recall that \(n\geq k\). Therefore, using this, (D.31) and (3.6), we see that

\[a_{\mathcal{Y}}\left(\|F-q\circ\iota\|_{L^{\infty}_{\mu}(\mathcal{ X};\mathcal{Y})}/\sqrt{m/L}+\|F-q\circ\iota\|_{L^{2}_{\mu}(\mathcal{X};\mathcal{Y})}\right) \lesssim a_{\mathcal{Y}}\|F-q\circ\iota\|_{L^{\infty}_{\mu}( \mathcal{X};\mathcal{Y})}\] \[\lesssim a_{\mathcal{Y}}\cdot C(\boldsymbol{b},p,\xi)\cdot(m/L) ^{1-1/p}\] \[=E_{\mathsf{app},2}\]

and likewise for the \(L^{\infty}_{\mu}\)-norm bound. Combining this with (D.37) now completes the proof. 

Proof of Theorem 3.1 when \(\mathcal{Y}\) is a Hilbert space.: The two differences in theorem statement when \(\mathcal{Y}\) is a Hilbert space are: (i) the \(L^{2}\)-norm error is with respect to the stronger Bochner norm, and (ii) the approximation error terms \(E_{\mathsf{app},q},\)\(q=2,\infty\), are smaller by a factor of \(1/2\) in the exponent (recall (3.6)). We treat both issues separately.

For the (i), we commence with the supporting results in SSD.2. First, we note that Lemmas D.1 and D.2 also hold in the Bochner \(L^{2}\)-norm, since these results already give upper bounds involving the Pettis \(L^{2}\)-norm. Next, we observe that the proof of Lemma D.3 is readily adapted to yield an equivalent result in the Bochner \(L^{2}\)-norm with the same constant \(\delta\). The same therefore applies to Lemma D.4.

We next consider the analysis of (D.3) in SSD.3. If we replace (D.7) by the condition

\[\|p-q\|_{\mathsf{disc},\tilde{\zeta}}\geq\alpha\max\{\|p-q\|_{L^{2}_{\varrho}( D;\mathcal{Y})},\|p-q\|_{L^{2}_{\varrho}(D;\mathcal{Y})}\},\quad\forall p,q\in \mathcal{P}_{\mathcal{S};\widetilde{\mathcal{Y}}}\] (D.38)

then the proof of Lemma D.5 yields the same error bounds, except with the Pettis \(L^{2}\)-norm replaced by the Bochner \(L^{2}\)-norm. Theorem D.6 is likewise modified to provide a bound in the Bochner \(L^{2}\)-norm.

Up to this point, we have not used the fact that \(\mathcal{Y}\) is a Hilbert space. We now need this property. As in SSD.4, the next step is to establish that (D.38) holds with high probability. Lemma D.7 is unchanged, therefore our focus is on Lemma D.8. We now describe the steps needed to modify the proof of this lemma to assert (D.38) subject to the same conditions (D.13)-(D.14). First, let \(\{\varphi_{i}\}_{i=1}^{d_{\mathcal{Y}}}\) be an orthonormal basis of \(\widetilde{\mathcal{Y}}\) and write each coefficient \(c_{\boldsymbol{\nu}_{i}}\) of the function \(h\) in (D.16) as \(c_{\boldsymbol{\nu}_{i}}=\sum_{j=1}^{d_{\mathcal{Y}}}b_{ij}\varphi_{j}\) for scalars \(b_{ij}\). Then it is a short exercise to write

\[\|h\|_{\mathsf{disc},\tilde{\zeta}}^{2}=\|\boldsymbol{A}\boldsymbol{c}\|_{2; \mathcal{Y}}^{2}=\sum_{j=1}^{d_{\mathcal{Y}}}\|\boldsymbol{A}\boldsymbol{b}_{ j}\|_{2}^{2},\]

where \(\boldsymbol{b}_{j}=(b_{ij})_{i=1}^{N}\). Since \(b_{ij}=0,\forall j\), whenever \(c_{\boldsymbol{\nu}_{i}}=0\), this vector also satisfies \(\|\boldsymbol{b}_{j}\|_{0,\boldsymbol{v}}\leq 2k\). Hence, by Lemma D.7 and Parseval's identity twice,

\[\|h\|_{\mathsf{disc},\tilde{\zeta}}^{2} \geq\sum_{j=1}^{d_{\mathcal{Y}}}(\theta_{-}-(1+\theta_{+}c_{5} \delta)\|\boldsymbol{b}_{j}\|_{2}^{2}\] \[=(\theta_{-}-(1+\theta_{+}c_{5}\delta)\sum_{i=1}^{N}\|c_{ \boldsymbol{\nu}_{i}}\|_{2}^{2}\] \[=(\theta_{-}-(1+\theta_{+}c_{5}\delta)\|h\|_{L^{2}_{\varrho}(D; \mathcal{Y})}^{2}.\]

We now use the bounds (D.15) (which are unchanged) and set \(\delta=c_{3}/(2(1+c_{4})c_{5})\) once more to get

\[\|h\|_{\mathsf{disc},\tilde{\zeta}}^{2}\geq c_{3}/2\|h\|_{L^{2}_{\varrho}(D; \mathcal{Y})}^{2}.\]

This gives the desired result.

This completes the changes needed in order to analyze the polynomial training problem (D.3). We next consider the DNN training problem (2.5). SSD.5 remains unchanged. After reviewing their proofs,we see that Lemma D.12 and Theorem D.13 both hold with Pettis norms replaced by Bochner norms whenever (D.38) holds instead of (D.7). Finally, we also observe that Lemma D.15 also holds with Pettis norms replaced by Bochner norms, both in the bound and in the definition (D.24) of \(E_{2}(F,q)\) and \(E_{\infty}(F,q)\).

Having completed the changes needed in all the preparatory results, we now follow the same steps as above in the Banach space case. Steps 1 and 2 are unchanged. For Step 3, we go through and replace Pettis norms by Bochner norms throughout. Using this, we obtain (D.37), except with the Bochner norm on the left-hand side in the first inequality, i.e.,

\[\begin{split}\|F-\widehat{F}\|_{L^{2}_{\mu}(\mathcal{X}; \mathcal{Y})}&\lesssim a_{\mathcal{Y}}\left(\|F-q\circ\iota\|_{L^ {\infty}_{\nu}(\mathcal{X};\mathcal{Y})}/\sqrt{m/L}+\|F-q\circ\iota\|_{L^{2}_ {\mu}(\mathcal{X};\mathcal{Y})}\right)\\ &\quad+E_{\mathcal{X},2}+E_{\mathcal{Y},2}+E_{\text{opt},2}+E_{ \text{samp},2}\\ \|F-\widehat{F}\|_{L^{\infty}_{\nu}(\mathcal{X};\mathcal{Y})}& \lesssim a_{\mathcal{Y}}\left(\|F-q\circ\iota\|_{L^{\infty}_{ \nu}(\mathcal{X};\mathcal{Y})}+\sqrt{m/L}\|F-q\circ\iota\|_{L^{2}_{\mu}( \mathcal{X};\mathcal{Y})}\right)\\ &\quad+E_{\mathcal{X},\infty}+E_{\mathcal{Y},\infty}+E_{\text{ opt},\infty}+E_{\text{samp},\infty}.\end{split}\] (D.39)

This concludes the changes needed to address (i). To address (ii), we bound the terms \(F-q\circ\iota\). Using (A.I), (A.II), Lemma D.17, the fact that \(\mathcal{Y}\) is a Hilbert space and the definitions (D.31) and (D.29) of \(k\) and \(n\), we see that

\[\|F-q\circ\iota\|_{L^{\infty}_{\mu}(\mathcal{X};\mathcal{Y})}=\|f-q\|_{L^{ \infty}_{\nu}(D;\mathcal{Y})}\lesssim\|f-q\|_{L^{\infty}_{\varrho}(D;\mathcal{ Y})}\lesssim C(\boldsymbol{b},p,\xi)\cdot(m/L)^{1-1/p}\]

and

\[\|F-q\circ\iota\|_{L^{2}_{\mu}(\mathcal{X};\mathcal{Y})}=\|f-q\|_{L^{2}_{\varsigma }(D;\mathcal{Y})}\lesssim\|f-q\|_{L^{2}_{\varrho}(D;\mathcal{Y})}\lesssim C( \boldsymbol{b},p,\xi)\cdot(m/L)^{1/2-1/p}\]

Substituting this into (D.39) and recalling (3.6) now completes the proof. 

**Remark D.18** (Differences between the Banach and Hilbert space case): Having seen the proof, we now summarize these differences as follows. First, the matter of whether the Pettis versus Bochner norm can be used reduces to the choice of such norm in the discrete metric inequality (D.7). When \(\mathcal{Y}\) is a Banach space, we are able to establish this in terms of the Pettis norm subject to a log-linear scaling between \(m\) and \(k\) (see Lemma D.8 and (D.13)). However, when \(\mathcal{Y}\) is also a Hilbert space, we can establish the stronger version (D.38) of this inequality by exploiting the additional structure. This, in short, is what leads to the stronger norm bound in this case.

Second, in the Hilbert space case, we get an improved approximation error. This stems from (D.17) and, specifically, the fact that when \(\mathcal{Y}\) is a Hilbert space we may use Parseval's identity in the Bochner space \(L^{2}_{\varrho}(D;\mathcal{Y})\) to bound the \(L^{2}_{\varrho}\)-norm error term via (D.28). This is not possible when \(\mathcal{Y}\) is a Banach space, so we settle for bounding this term via (D.27) instead.

## Appendix E Proof of Theorem 3.2

### Setup

As in SSD.1, let \(\Lambda\subset\mathcal{F}\) with \(\operatorname{supp}(\boldsymbol{\nu})\subseteq\{1,\ldots,d_{\mathcal{X}}\}\), \(\forall\boldsymbol{\nu}\in\Lambda\), and write \(N=|\Lambda|\). Let \(\mathcal{S}\) be as in (D.1), \(r\in\mathbb{N}\), \(r>\max\{m,k\}\) (its precise value will be chosen later in the proof) and define the set

\[\Gamma=\Gamma_{o}\cup\bigcup_{S\in\mathcal{S}}S\subset\mathcal{F},\quad \text{where }\Gamma_{o}=\{\boldsymbol{e}_{i}:i=1,\ldots,r\}.\] (E.1)

Finally, let \(0<\delta<1\) and consider the family \(\mathcal{N}_{o}\) and the tanh DNNs \(\{N_{\boldsymbol{\nu}}\}_{\boldsymbol{\nu}\in\Gamma}\) whose existence is implied by Lemma D.9. We will specify \(\Lambda\), \(k\), \(r\) and \(\delta\) later in the proof.

Next, let

\[\hat{p}=\sum_{\boldsymbol{\nu}\in S}\hat{\boldsymbol{c}}_{\boldsymbol{\nu}} \Psi_{\boldsymbol{\nu}}\]

be any minimizer of (D.3), where \(|S|_{\boldsymbol{\nu}}\leq k\) and define

\[\tilde{p}=\sum_{\boldsymbol{\nu}\in S}\hat{c}_{\boldsymbol{\nu}}N_{ \boldsymbol{\nu}}.\]Let

\[\bm{B}=\frac{1}{\sqrt{r}}\left(N_{\bm{e}_{j}}\circ\mathcal{E}_{\mathcal{X}}(X_{i}) \right)_{i,j=1}^{m,r}.\]

Now, \(\widetilde{\mathcal{Y}}:=\mathcal{D}_{\mathcal{Y}}(\mathbb{R}^{d_{\mathcal{Y}}})\) is a finite-dimensional subspace of the Banach space \(\mathcal{Y}\). Hence, for any \(Y\in\mathcal{Y}\) there exists a closest point \(\widetilde{Y}\in\widetilde{\mathcal{Y}}\), i.e., a point satisfying

\[\|Y-\widetilde{Y}\|_{\mathcal{Y}}=\inf\left\{\|Y-Z\|_{\mathcal{Y}}:Z\in \widetilde{\mathcal{Y}}\right\}.\]

Given \(Y_{1},\ldots,Y_{m}\), let \(\widetilde{Y}_{1},\ldots,\widetilde{Y}_{m}\in\widetilde{\mathcal{Y}}\) be the corresponding closest points. Now define

\[\bm{e}=\frac{1}{\sqrt{r}}\left(\widetilde{Y}_{i}-\tilde{p}\circ\mathcal{E}_{ \mathcal{X}}(X_{i})\right)_{i=1}^{m}\in\widetilde{\mathcal{Y}}^{m}\]

and

\[\bar{p}=\tilde{p}+\sum_{i=1}^{r}(\bm{B}^{\dagger}\bm{e}+y\bm{z})_{i}N_{\bm{e} _{i}},\] (E.2)

where \(\bm{z}\in N(\bm{B})\backslash\{\bm{0}\}\) and \(y\in\widetilde{\mathcal{Y}}\), \(\left\|y\right\|_{\mathcal{Y}}=1\), are arbitrary. Note that such a \(\bm{z}\) exists, since \(r>m\) by assumption. Notice that

\[\bar{p}=\sum_{\bm{\nu}\in\mathbb{S}\cup\Gamma_{o}}\bar{c}_{\bm{\nu}}N_{\bm{\nu}}\]

for coefficients \(\bar{c}_{\bm{\nu}}\in\widetilde{\mathcal{Y}}\). Therefore, we can write

\[\bar{p}=\mathcal{D}_{\mathcal{Y}}\circ\widehat{N},\] (E.3)

where

\[\widehat{N}=\bm{C}\begin{bmatrix}N_{\bm{\nu}_{1}}\\ \vdots\\ N_{\bm{\nu}_{[S\cup\Gamma_{o]}}}\\ 0\\ \vdots\\ 0\end{bmatrix}\] (E.4)

and \(\bm{C}\in\mathbb{R}^{d_{\mathcal{Y}}\times(\lfloor k\rfloor+r)}\). Finally, we define the approximation

\[\widehat{F}=\mathcal{D}_{\mathcal{Y}}\circ\widehat{N}\circ\mathcal{E}_{ \mathcal{X}}\] (E.5)

and, for convenience,

\[\widetilde{F}=\hat{p}\circ\mathcal{E}_{\mathcal{X}}.\]

### Estimation of the DNN minimizer

**Lemma E.1** (\(\widehat{N}\) is a minimizer).: _If \(\bm{B}\) is full rank, then_

\[\widehat{F}(X_{i})=\widetilde{Y}_{i},\quad\forall i=1,\ldots,m.\]

_Therefore, \(\widehat{N}\) is a minimizer of (2.5)._

Proof.: Observe that

\[\widehat{F}(X_{i}) =\bar{p}\circ\mathcal{E}_{\mathcal{X}}(X_{i})\] \[=\tilde{p}\circ\mathcal{E}_{\mathcal{X}}(X_{i})+\sqrt{r}\sum_{i=1 }^{r}(\bm{B})_{ij}(\bm{B}^{\dagger}\bm{e}+y\bm{z})_{j}\] \[=\tilde{p}\circ\mathcal{E}_{\mathcal{X}}(X_{i})+\sqrt{r}(\bm{B}( \bm{B}^{\dagger}\bm{e}+y\bm{z}))_{i}\] \[=\tilde{p}\circ\mathcal{E}_{\mathcal{X}}(X_{i})+\sqrt{r}(\bm{e}) _{i}\] \[=\widetilde{Y}_{i}.\]Here, in the penultimate step we use the facts that \(\bm{B}y\bm{z}=y\bm{B}\bm{z}=\bm{0}\) since \(\bm{z}\in N(\bm{B})\) and \(\bm{B}\bm{B}^{\dagger}=\bm{I}\) since \(r\geq m\) and \(\bm{B}\) is full rank by assumption. This gives the first result.

For the second result, we recall that \(\widetilde{Y}_{i}\) is a closest point to \(Y_{i}\) from \(\widetilde{\mathcal{Y}}=\mathcal{D}_{\mathcal{Y}}(\mathbb{R}^{d_{\mathcal{Y}}})\). Therefore, for any DNN \(N\),

\[\frac{1}{m}\sum_{i=1}^{m}\left\|Y_{i}-\mathcal{D}_{\mathcal{Y}}\circ N\circ \mathcal{E}_{\mathcal{X}}(X_{i})\right\|_{\mathcal{Y}}^{2}\geq\frac{1}{m}\sum_ {i=1}^{m}\left\|Y_{i}-\widetilde{Y}_{i}\right\|_{\mathcal{Y}}^{2}=\frac{1}{m} \sum_{i=1}^{m}\left\|Y_{i}-\mathcal{D}_{\mathcal{Y}}\circ\widehat{N}\circ \mathcal{E}_{\mathcal{X}}(X_{i})\right\|_{\mathcal{Y}}^{2}\]

as required. 

**Lemma E.2** (Bounding \(\widehat{F}\) in terms of \(\widetilde{F}\)).: _Suppose that (D.7) holds with \(\alpha\geq c_{0}\) and \(\alpha-\delta\sqrt{k}\geq c_{1}\), and also that_

\[\sqrt{r}\|_{\iota_{d_{\mathcal{X}}}}-\iota_{d_{\mathcal{X}}}\circ\widetilde{ \mathcal{D}}_{\mathcal{X}}\circ\widetilde{\mathcal{E}}_{\mathcal{X}}\|_{L^{2} _{\mu}(\mathcal{X};\ell^{\infty}(\mathbb{N}))}\leq c_{2},\]

_where \(c_{0},c_{1},c_{2}>0\) are suitable universal constants. Then the approximation \(\widehat{F}\) satisfies_

\[\left\|\!\left\|F-\widehat{F}\right\|\!\right\|\!\right\|_{L^{2}_{ \mu}(\mathcal{X};\mathcal{Y})}\lesssim \left\|\!\left\|F-\widetilde{F}\right\|\!\right\|_{L^{2}_{\mu}( \mathcal{X};\mathcal{Y})}\] \[+(1+\delta\sqrt{r})\delta\sqrt{k}\left(1+\frac{\sqrt{m}}{\sqrt{r} \sigma_{\min}(\bm{B})}\right)\left(\|F\|_{L^{\infty}_{\mu}(\mathcal{X}; \mathcal{Y})}+\frac{1}{\sqrt{m}}\|\bm{E}\|_{2;\mathcal{Y}}\right)\] \[+(1+\delta\sqrt{r})\|\bm{z}\|_{2}\]

_and_

\[\left\|F-\widehat{F}\right\|_{L^{\infty}_{\mu}(\mathcal{X}; \mathcal{Y})}\lesssim \left\|F-\widetilde{F}\right\|_{L^{\infty}_{\mu}(\mathcal{X}; \mathcal{Y})}\] \[+(1+\delta)\sqrt{r}\delta\sqrt{k}\left(1+\frac{\sqrt{m}}{\sqrt{r} \sigma_{\min}(\bm{B})}\right)\left(\|F\|_{L^{\infty}_{\mu}(\mathcal{X}; \mathcal{Y})}+\frac{1}{\sqrt{m}}\|\bm{E}\|_{2;\mathcal{Y}}\right)\] \[+\frac{\sqrt{m}(1+\delta)}{\sigma_{\min}(\bm{B})}\left(\sqrt{ \frac{1}{m}\sum_{i=1}^{m}\left\|\widetilde{Y}_{i}-\widetilde{F}(X_{i})\right\| _{\mathcal{Y}}^{2}}+\frac{1}{\sqrt{m}}\|\bm{E}\|_{2;\mathcal{Y}}\right)\] \[+(1+\delta)\sqrt{r}\|\bm{z}\|_{2}.\]

Proof.: By the triangle inequality,

\[\left\|\!\left\|F-\widehat{F}\right\|\!\right\|_{L^{2}_{\mu}( \mathcal{X};\mathcal{Y})}\leq \left\|\!\left\|F-\widetilde{F}\right\|\!\right\|_{L^{2}_{\mu}(\mathcal{X}; \mathcal{Y})}+\left\|\!\left\|\widetilde{F}-\widehat{F}\right\|\!\right\|_{L^{2 }_{\mu}(\mathcal{X};\mathcal{Y})}\] (E.6) \[\left\|F-\widehat{F}\right\|_{L^{\infty}_{\mu}(\mathcal{X}; \mathcal{Y})}\leq\left\|F-\widetilde{F}\right\|_{L^{\infty}_{\mu}(\mathcal{X}; \mathcal{Y})}+\left\|\!\left\|F-\widehat{F}\right\|\!\right\|_{L^{\infty}_{\mu} (\mathcal{X};\mathcal{Y})}.\]

Consider the second term. We have

\[\left\|\!\left\|\widetilde{F}-\widehat{F}\right\|\!\right\|_{L^{2}_{ \mu}(\mathcal{X};\mathcal{Y})}=\left\|\!\left\|\hat{p}-\mathcal{D}_{\mathcal{Y }}\circ\widehat{N}\right\|\!\right\|_{L^{2}_{\xi}(D;\mathcal{Y})}\leq\left\|\! \left\|\hat{p}-\tilde{p}\right\|\!\right\|_{L^{2}_{\xi}(D;\mathcal{Y})}+ \left\|\!\left\|q\right\|\!\right\|_{L^{2}_{\xi}(D;\mathcal{Y})},\] (E.7) \[\left\|\widetilde{F}-\widehat{F}\right\|_{L^{\infty}_{\mu}(\mathcal{ X};\mathcal{Y})}=\left\|\hat{p}-\mathcal{D}_{\mathcal{Y}}\circ\widehat{N}\right\|_{L^{ \infty}_{\mu}(D;\mathcal{Y})}\leq\left\|\hat{p}-\tilde{p}\right\|_{L^{\infty}_ {\lambda}(D;\mathcal{Y})}+\left\|q\right\|_{L^{\infty}_{\xi}(D;\mathcal{Y})},\]

where \(q=\bar{p}-\tilde{p}=\sum_{i=1}^{r}(\bm{B}^{\dagger}\bm{e}+y\bm{z})_{i}N_{\bm{e}_ {i}}\). Lemma D.10 and (A.III) give that

\[\left\|\!\left\|\hat{p}-\tilde{p}\right\|\!\right\|_{L^{2}_{\xi}(D;\mathcal{Y} )}=\left\|\hat{p}-\tilde{p}\right\|_{L^{\infty}_{\xi}(D;\mathcal{Y})}\leq \delta\sqrt{k}\|\hat{p}\|\!\right\|_{L^{2}_{\xi}(D;\mathcal{Y})}.\] (E.8)

Now consider the other term in (E.7). Define \(\tilde{q}=\sum_{i=1}^{r}(\bm{B}^{\dagger}\bm{e}+y\bm{z})_{i}\bm{\Psi}_{\bm{e}_ {i}}\). Then Lemma D.10 and (A.III) once more give that

\[\left\|\!\left\|q\right\|\!\right\|_{L^{2}_{\xi}(D;\mathcal{Y})} \leq\left\|\!\left\|\tilde{q}\right\|\!\right\|_{L^{2}_{\xi}(D; \mathcal{Y})}+\left\|\!\left\|q-\tilde{q}\right\|\!\right\|_{L^{2}_{\xi}(D; \mathcal{Y})}\leq\left\|\!\left\|\tilde{q}\right\|\!\right\|_{L^{2}_{\xi}(D; \mathcal{Y})}+\delta\sqrt{r}\|\!\left\|\tilde{q}\right\|\!\right\|_{L^{2}_{ \xi}(D;\mathcal{Y})}.\]Here, we also used the fact that \(|\Gamma_{o}|=r\). We now apply Lemmas D.1 and D.4 and (A.3) once more to get

\[\left|\kern-1.075pt\left|\kern-1.075pt\left|q\right|\kern-1.075pt\right|\kern-1.075pt \right|\kern-1.075pt\right|_{L^{2}_{\mathfrak{e}}(D;\mathcal{Y})}\lesssim(1+ \delta\sqrt{r})\|\tilde{q}\|_{L^{2}_{\mathfrak{e}}(D;\mathcal{Y})},\qquad\|q \|_{L^{\infty}_{\mathfrak{L}}(D;\mathcal{Y})}\lesssim(1+\delta)\sqrt{r}\| \tilde{q}\|_{L^{2}_{\mathfrak{e}}(D;\mathcal{Y})}.\] (E.9)

We next analyze the term \(\|\tilde{q}\|_{L^{2}_{\mathfrak{e}}(D;\mathcal{Y})}\). Let \(y^{*}\in\mathcal{Y}^{*}\). Since \(y^{*}((\boldsymbol{B}^{\dagger}\boldsymbol{e}+y\boldsymbol{z})_{i})=( \boldsymbol{B}^{\dagger}y^{*}(\boldsymbol{e})+y^{*}(y)\boldsymbol{z})_{i}\), we have

\[y^{*}(\tilde{q})=\sum_{i=1}^{r}(\boldsymbol{B}^{\dagger}y^{*}(\boldsymbol{e}) +y^{*}(y)\boldsymbol{z})_{i}\Psi_{\boldsymbol{e}_{i}}.\]

Hence, by Parseval's identity,

\[\left|\kern-1.075pt\left|\kern-1.075pt\left|\kern-1.075pt\left| \tilde{q}\right|\kern-1.075pt\right|\kern-1.075pt\right|_{L^{2}_{\mathfrak{e }}(D;\mathcal{Y})} =\sup_{y^{*}\in B(\mathcal{Y}^{*})}\|y^{*}(\tilde{q})\|_{L^{2}(D)}\] \[=\sup_{y^{*}\in B(\mathcal{Y}^{*})}\left|\boldsymbol{B}^{\dagger }y^{*}(\boldsymbol{e})+y^{*}(y)\boldsymbol{z}\right|_{2}\] \[\leq\frac{1}{\sigma_{\min}(\boldsymbol{B})}\sup_{y^{*}\in B( \mathcal{Y}^{*})}\left\|y^{*}(\boldsymbol{e})\right\|_{2}+\sup_{y^{*}\in B( \mathcal{Y}^{*})}\left|y^{*}(y)\right|\|\boldsymbol{z}\right\|_{2}\] \[=\frac{1}{\sigma_{\min}(\boldsymbol{B})}\left|\kern-1.075pt\left| \kern-1.075pt\left|\boldsymbol{e}\right|\kern-1.075pt\right|\kern-1.075pt \right|_{2;\mathcal{Y}}+\left|\kern-1.075pt\left|\boldsymbol{z}\right|\kern-1.0 75pt\right|_{2}.\]

We now use the definition of \(\boldsymbol{e}\) and the inequality to obtain

\[\left|\kern-1.075pt\left|\kern-1.075pt\left|\tilde{q}\right|\kern-1.075pt\right| \kern-1.075pt\right|_{L^{2}_{\mathfrak{e}}(D;\mathcal{Y})}\leq\frac{\sqrt{m} }{\sqrt{r}\sigma_{\min}(\boldsymbol{B})}\sqrt{\frac{1}{m}\sum_{i=1}^{m}\left| \widetilde{Y}_{i}-\tilde{p}\circ\mathcal{E}_{\mathcal{X}}(X_{i})\right|_{ \mathcal{Y}}^{2}}+\left|\kern-1.075pt\left|\boldsymbol{z}\right|\kern-1.075pt \right|_{2}.\]

We next apply the triangle inequality and Lemma D.10 once more to get

\[\left|\kern-1.075pt\left|\kern-1.075pt\left|\tilde{q}\right| \kern-1.075pt\right|\kern-1.075pt\right|_{L^{2}_{\mathfrak{e}}(D;\mathcal{Y})} \leq\frac{\sqrt{m}}{\sqrt{r}\sigma_{\min}(\boldsymbol{B})}\left(\sqrt{\frac{1 }{m}\sum_{i=1}^{m}\left|\widetilde{Y}_{i}-\hat{p}\circ\mathcal{E}_{\mathcal{X} }(X_{i})\right|_{\mathcal{Y}}^{2}}+\left|\kern-1.075pt\left|\hat{p}-\tilde{p} \right|\kern-1.075pt\right|_{\text{disc},\tilde{\varsigma}}\right)+\left| \kern-1.075pt\left|\kern-1.075pt\left|\boldsymbol{z}\right|\kern-1.075pt\right| _{2}\] \[\leq\frac{\sqrt{m}}{\sqrt{r}\sigma_{\min}(\boldsymbol{B})}\left( \sqrt{\frac{1}{m}\sum_{i=1}^{m}\left|\widetilde{Y}_{i}-\hat{p}\circ\mathcal{E} _{\mathcal{X}}(X_{i})\right|_{\mathcal{Y}}^{2}}+\delta\sqrt{k}\|\kern-1.075pt \left|\hat{p}\right|\kern-1.075pt\right|_{L^{2}_{\mathfrak{e}}(D;\mathcal{Y}) }\right)+\left|\kern-1.075pt\left|\boldsymbol{z}\right|\kern-1.075pt\right|_{2}.\]

Combining this with (E.7) and (E.9), we deduce that

\[\left|\kern-1.075pt\left|\kern-1.075pt\left|\tilde{F}-\widehat{F} \right|\kern-1.075pt\right|\kern-1.075pt\right|_{L^{2}_{\mathfrak{e}}( \mathcal{X};\mathcal{Y})} \lesssim(1+\delta\sqrt{r})\Bigg{[}\delta\sqrt{k}\left(1+\frac{ \sqrt{m}}{\sqrt{r}\sigma_{\min}(\boldsymbol{B})}\right)\left|\kern-1.075pt \left|\kern-1.075pt\left|\hat{p}\right|\kern-1.075pt\right|\kern-1.075pt\right| _{L^{2}_{\mathfrak{e}}(D;\mathcal{Y})}+\left|\kern-1.075pt\left|\boldsymbol{z} \right|\kern-1.075pt\right|_{2}\] \[\quad+\frac{\sqrt{m}}{\sqrt{r}\sigma_{\min}(\boldsymbol{B})} \left(\sqrt{\frac{1}{m}\sum_{i=1}^{m}\left|\widetilde{Y}_{i}-\hat{p}\circ \mathcal{E}_{\mathcal{X}}(X_{i})\right|_{\mathcal{Y}}^{2}}+\frac{1}{\sqrt{m}} \|\kern-1.075pt\left|\boldsymbol{E}\right|\kern-1.075pt\right|_{2;\mathcal{ Y}}\right)\Bigg{]}\]

and

\[\left|\kern-1.075pt\left|\kern-1.075pt\left|\tilde{F}-\widehat{F} \right|\kern-1.075pt\right|_{L^{\infty}_{\mathfrak{e}}(\mathcal{X};\mathcal{Y})} \lesssim(1+\delta)\sqrt{r}\Bigg{[}\delta\sqrt{k}\left(1+\frac{\sqrt{m}}{ \sqrt{r}\sigma_{\min}(\boldsymbol{B})}\right)\left|\kern-1.075pt\left|\kern-1.075pt \left|\hat{p}\right|\kern-1.075pt\right|\kern-1.075pt\right|_{L^{2}_{\mathfrak{e }}(D;\mathcal{Y})}+\left|\kern-1.075pt\left|\boldsymbol{z}\right|\kern-1.075pt \right|_{2}\] \[\quad+\frac{\sqrt{m}}{\sqrt{r}\sigma_{\min}(\boldsymbol{B})}\left( \sqrt{\frac{1}{m}\sum_{i=1}^{m}\left|\widetilde{Y}_{i}-\hat{p}\circ\mathcal{E}_{ \mathcal{X}}(X_{i})\right|_{\mathcal{Y}}^{2}}+\frac{1}{\sqrt{m}}\|\kern-1.075pt \left|\boldsymbol{E}\right|\kern-1.075pt\right|_{2;\mathcal{Y}}\right)\Bigg{]}.\]It remains to bound the term \(\|\hat{p}\|_{L^{2}_{\theta}(D;\mathcal{Y})}\). Using (D.7) and the fact that \(\hat{p}\) is a minimizer of (D.3) and that the zero polynomial is feasible for (D.3), we get

\[\|\hat{p}\|_{L^{2}_{\theta}(D;\mathcal{Y})} \lesssim\|\hat{p}\|_{\text{disc},\hat{\zeta}}\] \[\leq\sqrt{\frac{1}{m}\sum_{i=1}^{m}\left\|Y_{i}-\hat{p}\circ \mathcal{E}_{\mathcal{X}}(X_{i})\right\|_{\mathcal{Y}}^{2}}+\frac{1}{\sqrt{m} }\|\boldsymbol{Y}\|_{2;\mathcal{Y}}\] \[\leq\frac{2}{\sqrt{m}}\|\boldsymbol{Y}\|_{2;\mathcal{Y}}\] \[\leq 2\|F\|_{L^{\infty}_{\mu}(\mathcal{X};\mathcal{Y})}+\frac{2}{ \sqrt{m}}\|\boldsymbol{F}\|_{2;\mathcal{Y}}\]

Combining this with the previous bound and (E.6) now completes the proof. 

### Estimation of \(\sigma_{\min}(\boldsymbol{B})\)

Recall that a matrix \(\boldsymbol{A}\in\mathbb{R}^{m\times n}\) is a _subgaussian random matrix_ if its entries are i.i.d. subgaussian random variables with mean zero and variance one (see, e.g., [29, Def. 9.1]). The following result can be found in, e.g., [29, Ex. 9.3].

**Lemma E.3** (Smallest singular value of a subgaussian random matrix).: _Let \(\boldsymbol{A}\in\mathbb{R}^{m\times n}\) be a subgaussian random matrix and \(\sigma_{\min}\) be the smallest singular value of \(\frac{1}{\sqrt{m}}\boldsymbol{A}\). Then, for all \(0<t<1\),_

\[\mathbb{P}\left(\sigma_{\min}\leq 1-c_{1}\sqrt{n/m}-t\right)\leq 2\exp(-c_{2}mt^{2}),\]

_where \(c_{1},c_{2}>0\) are universal constants._

**Lemma E.4** (Bounding \(\sigma_{\min}(\boldsymbol{B})\)).: _Suppose that \(\sqrt{m}\delta\leq\sqrt{\omega}/8\),_

\[\frac{3^{(5+\xi)/2}\sqrt{r}}{2}\|_{\iota_{d_{\mathcal{X}}}}-\iota_{d_{ \mathcal{X}}}\circ\widetilde{\mathcal{D}}_{\mathcal{X}}\circ\widetilde{ \mathcal{E}}_{\mathcal{X}}\|_{L^{\infty}_{\mu}(\mathcal{X},\ell^{\infty}( \mathbb{N}))}\leq\frac{\sqrt{\omega}}{8},\] (E.10)

_where \(\omega\) is the variance of the univariate probability measure as specified in Theorem 3.2 and_

\[d_{\mathcal{X}}\geq r\geq c\left(m+\log(2/\epsilon)\right)\] (E.11)

_for some universal constant \(c\geq 1\). Then, with probability at least \(1-\epsilon\), the matrix \(\boldsymbol{B}\) is full rank and_

\[\sigma_{\min}(\boldsymbol{B})\geq\sqrt{\omega}/4.\]

Proof.: Define the matrices

\[\boldsymbol{B}^{\prime}=\frac{1}{\sqrt{r}}\left(\Psi_{\boldsymbol{e}_{j}} \circ\mathcal{E}_{\mathcal{X}}(X_{i})\right)_{i,j=1}^{m,r},\quad\boldsymbol{B} ^{\prime\prime}=\frac{1}{\sqrt{r}}\left(\Psi_{\boldsymbol{e}_{j}}\circ\iota_{d _{\mathcal{X}}}(X_{i})\right)_{i,j=1}^{m,r}.\]

Then, since \(r\geq m\),

\[\sigma_{\min}(\boldsymbol{B}) =\inf\left\{\left\|\boldsymbol{B}^{\top}\boldsymbol{d}\right\|_{ 2}:\boldsymbol{d}\in\mathbb{C}^{m},\;\left\|\boldsymbol{d}\right\|_{2}=1\right\}\] \[\geq\sigma_{\min}(\boldsymbol{B}^{\prime})-\left\|(\boldsymbol{B}- \boldsymbol{B}^{\prime})^{\top}\right\|_{2}\] \[=\sigma_{\min}(\boldsymbol{B}^{\prime})-\left\|\boldsymbol{B}- \boldsymbol{B}^{\prime}\right\|_{2}\] \[\geq\sigma_{\min}(\boldsymbol{B}^{\prime\prime})-\left\| \boldsymbol{B}-\boldsymbol{B}^{\prime}\right\|_{2}-\left\|\boldsymbol{B}^{ \prime}-\boldsymbol{B}^{\prime\prime}\right\|_{2}.\]

Now, for any \(\boldsymbol{c}\in\mathbb{C}^{r}\),

\[\left\|(\boldsymbol{B}-\boldsymbol{B}^{\prime})\boldsymbol{c}\right\| _{2}^{2} =\frac{1}{r}\sum_{i=1}^{m}\left(\sum_{j=1}^{r}\left(N_{\boldsymbol{ e}_{j}}(\boldsymbol{x}_{i})-\Psi_{\boldsymbol{e}_{j}}(\boldsymbol{x}_{i}) \right)c_{j}\right)^{2}\] \[\leq\frac{1}{r}\sum_{i=1}^{m}\delta^{2}\|\boldsymbol{c}\|_{1}^{2}\] \[\leq m\delta^{2}\|\boldsymbol{c}\|_{2}^{2}.\]We deduce that \(\left\|\bm{B}-\bm{B}^{\prime}\right\|_{2}\leq\sqrt{m}\delta\leq\sqrt{\omega}/8\). Hence

\[\sigma_{\min}(\bm{B})\geq\sigma_{\min}(\bm{B}^{\prime\prime})-\sqrt{\omega}/8- \left\|\bm{B}^{\prime}-\bm{B}^{\prime\prime}\right\|_{2}.\]

Now let \(\bm{c}\in\mathbb{C}^{r}\) and \(p=\sum_{i=1}^{r}c_{i}\Psi_{e_{i}}\) be the corresponding polynomial. Then, by (A.I), (A.III) and Lemma D.2,

\[\left\|(\bm{B}^{\prime}-\bm{B}^{\prime\prime})\bm{c}\right\|_{2} =\sqrt{\frac{1}{r}\sum_{i=1}^{m}\left|p\circ t_{d_{X}}(X_{i})-p \circ t_{d_{X}}\circ\widetilde{\mathcal{D}}_{\mathcal{X}}\circ\widetilde{ \mathcal{E}}_{\mathcal{X}}(X_{i})\right|^{2}}\] \[\leq\mathrm{Lip}(p,B^{\infty}(\mathbb{N}),\mathbb{R})\|t_{d_{X}} -t_{d_{\mathcal{X}}}\circ\widetilde{\mathcal{D}}_{\mathcal{X}}\circ\widetilde{ \mathcal{E}}_{\mathcal{X}}\|_{L^{\infty}_{\mu}(\mathcal{X},\ell^{\infty}( \mathbb{N}))}\] \[\leq\frac{3^{(5+\xi)/2}\sqrt{r}}{2}\|p\|_{L^{2}_{\varrho}(D)}\|t_ {d_{X}}-t_{d_{\mathcal{X}}}\circ\widetilde{\mathcal{D}}_{\mathcal{X}}\circ \widetilde{\mathcal{E}}_{\mathcal{X}}\|_{L^{\infty}_{\mu}(\mathcal{X},\ell^{ \infty}(\mathbb{N}))}\] \[\leq\frac{\sqrt{\omega}}{8}\|\bm{c}\|_{2}.\]

Here, in the third step we used the fact that \(|\Gamma_{o}|_{\bm{v}}=3^{5+\xi}r\), since \(u_{\bm{e}_{i}}=\sqrt{3}\). We deduce that \(\left\|\bm{B}^{\prime}-\bm{B}^{\prime\prime}\right\|_{2}\leq\sqrt{\omega}/8\) and therefore

\[\sigma_{\min}(\bm{B})\geq\sigma_{\min}(\bm{B}^{\prime\prime})-\sqrt{\omega}/4.\]

It remains to show that \(\sigma_{\min}(\bm{B}^{\prime\prime})\geq\sqrt{\omega}/2\) with high probability. By construction, \(\Psi_{\bm{e}_{j}}(\bm{x})=\sqrt{3}x_{j}\). Now recall that the pushforward \(\varsigma\) is a tensor-product of a univariate probability measure supported in \([-1,1]\) with mean zero and variance \(\omega>0\). Therefore

\[(\bm{B}^{\prime\prime})^{\top}=\sqrt{3}\frac{\sqrt{\omega}}{\sqrt{r}}\bm{A},\]

where \(\bm{A}=(\iota(X_{j})_{i}/\sqrt{\omega})_{i,j=1}^{r,m}\in\mathbb{R}^{r\times m}\). By construction, the entries of \(\bm{A}\) are i.i.d. subgaussian random variables with mean zero and variance one. Hence \(\bm{A}\) is a subgaussian random matrix. We now apply Lemma E.3. Let \(t=1/4\) and observe that

\[r\geq 4c_{1}^{2}m,\quad r\geq\frac{16}{c_{2}}\log(2/\epsilon),\]

by assumption. Therefore,

\[\mathbb{P}(\sigma_{\min}(\bm{B}^{\prime\prime})\leq\sqrt{\omega}/2)\leq \mathbb{P}(\sigma_{\min}(\bm{A}/\sqrt{r})\leq 1/(2\sqrt{3}))\leq\epsilon.\]

This gives the result. 

### Final arguments

We are now ready to complete the proof of Theorem 3.2.

Proof of Theorem 3.2, Statement (A).: We divide the proof into a series of steps.

_Step 1: Setup._ Let \(m\), \(\delta\), \(\epsilon\) and \(L\) be as in the theorem statement. We once more assume without loss of generality that \(\delta\leq 1/5\). Let \(n\) be as in (D.29) and \(\Lambda=\Lambda_{n}^{\text{HCI}}\), let \(\xi\) be as in (D.30) and

\[k=\frac{m}{c_{1}L},\] (E.12)

where \(c_{1}\geq 1\) is a constant that will be chosen in the next step. Let

\[\delta=\min\left\{2^{-m}/r^{2},\sqrt{\omega}/(8\sqrt{m})\right\},\]

where \(\omega\) is the variance of the univariate probability measure and

\[r=\lceil c_{2}(m+\log(1/\epsilon))\rceil,\] (E.13)

where \(c_{2}\geq 2\) will also be chosen in the next step. Note that \(d_{X}\geq r>m\) by assumption. Finally, let \(\mathcal{S}\) be as in (D.1), \(\Gamma\) be as in (E.1) and \(\{N_{\bm{\nu}}\}_{\bm{\nu}\in\Gamma}\) be the corresponding family of \(\tanh\) DNNs ensured by Lemma D.9. Finally, let \(\hat{F}\) be given by (E.5), with DNN \(\widehat{N}\) as in (E.4).

[MISSING_PAGE_FAIL:55]

and, since \(\delta\sqrt{r}\lesssim 1\),

\[\begin{split}\|F-\widehat{F}\|_{L^{\infty}_{\mu}(\mathcal{X}; \mathcal{Y})}&\lesssim\|F-\mathcal{Q}\circ F\|_{L^{\infty}_{\mu}( \mathcal{X};\mathcal{Y})}+\sqrt{k}\|F-\mathcal{Q}\circ F\|_{\mathsf{disc},\mu} \\ &\quad+\pi_{\mathcal{Q}}\left(\|F-q\circ\mathcal{E}_{\mathcal{X} }\|_{L^{\infty}_{\mu}(\mathcal{X};\mathcal{Y})}+\sqrt{k}\|F-q\circ\mathcal{E }_{\mathcal{X}}\|_{\mathsf{disc},\mu}\right)\\ &\quad+\|\bm{E}\|_{2;\mathcal{Y}}+\sqrt{r}\delta\sqrt{k}\|F\|_{L^ {\infty}_{\mu}(\mathcal{X};\mathcal{Y})}\\ &\quad+\sqrt{r}\|\bm{z}\|_{2}+\sqrt{m}\sqrt{\frac{1}{m}\sum_{i=1 }^{m}\left|\widetilde{Y}_{i}-\widetilde{F}(X_{i})\right|^{2}_{\mathcal{Y}}} \end{split}\]

for any \(q\in\mathcal{P}_{\mathcal{S};\mathcal{Y}}\) and linear operator \(\mathcal{Q}:\mathcal{Y}\to\widetilde{\mathcal{Y}}=\mathcal{D}_{\mathcal{Y}}( \mathbb{R}^{d_{\mathcal{Y}}})\) with \(\pi_{\mathcal{Q}}=\|\mathcal{Q}\|_{\mathcal{Y}\to\mathcal{Y}}\). Consider the final term. Since \(\widetilde{Y}_{i}\in\widetilde{\mathcal{Y}}\) is the closest point to \(Y_{i}=F(X_{i})+E_{i}\) we have

\[\|\widetilde{Y}_{i}-Y_{i}\|_{\mathcal{Y}}\leq\|F(X_{i})-\mathcal{Q}\circ F(X _{i})\|_{\mathcal{Y}}+\|E_{i}\|_{\mathcal{Y}}.\]

We now use this, the fact that \(\hat{p}\) is a minimizer and \(\mathcal{Q}\circ q\) is feasible in combination with triangle inequality to get

\[\begin{split}\sqrt{\frac{1}{m}\sum_{i=1}^{m}\left\|\widetilde{Y}_{ i}-\widetilde{F}(X_{i})\right\|^{2}_{\mathcal{Y}}}&\leq\sqrt{ \frac{1}{m}\sum_{i=1}^{m}\left\|Y_{i}-\mathcal{Q}\circ q\circ\mathcal{E}_{ \mathcal{X}}(X_{i})\right\|^{2}_{\mathcal{Y}}}+\left\|F-\mathcal{Q}\circ F \right\|_{\mathsf{disc},\mu}+\frac{1}{\sqrt{m}}\|\bm{E}\|_{2;\mathcal{Y}}\\ &\leq\|F-\mathcal{Q}\circ q\circ\mathcal{E}_{\mathcal{X}}\|_{ \mathsf{disc},\mu}+\left\|F-\mathcal{Q}\circ F\right\|_{\mathsf{disc},\mu}+ \frac{2}{\sqrt{m}}\|\bm{E}\|_{2;\mathcal{Y}}\\ &\leq\pi_{\mathcal{Q}}\|\!\left\|F-q\circ\mathcal{E}_{\mathcal{X} }\right\|_{\mathsf{disc},\mu}+2\|F-\mathcal{Q}\circ F\|_{\mathsf{disc},\mu}+ \frac{2}{\sqrt{m}}\|\bm{E}\|_{2;\mathcal{Y}}.\end{split}\]

Now let \(f\in\mathcal{H}(\bm{b})\) be the function asserted by (A.2) and \(q=f_{S}\) be the polynomial asserted by Lemma D.17. Substituting this into the previous expression, recalling (D.35) and using the definition of \(\delta\), we deduce that

\[\begin{split}\left|\!\left|\!\left|F-\widehat{F}\right|\!\right|_{ L^{2}_{\mu}(\mathcal{X};\mathcal{Y})}\right\|&\lesssim\left|\!\left|\! \left|F-\mathcal{Q}\circ F\right|\!\right|\!\right|_{L^{2}_{\mu}(\mathcal{X}; \mathcal{Y})}+\left|\!\left|\!\left|F-\mathcal{Q}\circ F\right|\!\right|\! \right|_{\mathsf{disc},\mu}\\ &\quad+\pi_{\mathcal{Q}}E_{2}(F,q)+\left\|\!\left|\bm{z}\right|\! \right|_{2}+2^{-m}+E_{\mathsf{samp},2}\\ \left|\!\left|F-\widehat{F}\right|\!\right|_{L^{\infty}_{\mu}( \mathcal{X};\mathcal{Y})}&\lesssim\left|\!\left|F-\mathcal{Q}\circ F \right|\!\right|\!\right|_{L^{\infty}_{\mu}(\mathcal{X};\mathcal{Y})}+\sqrt{ m}\|\!\left|\!\left|F-\mathcal{Q}\circ F\right|\!\right|\!\right|_{\mathsf{disc},\mu}\\ &\quad+\pi_{\mathcal{Q}}\widetilde{E}_{\infty}(F,q)+\sqrt{r}\|\bm{ z}\|_{2}+2^{-m}+E^{\prime}_{\mathsf{samp},\infty}\end{split}\]

with probability at least \(1-\epsilon/2\). Here \(E_{2}(F,q)\) is as in (D.24), \(\widetilde{E}_{\infty}(F,q)\) is as in (D.24) with \(k\) replaced by \(m\) and \(E^{\prime}_{\mathsf{samp},\infty}=\left\|\bm{E}\right\|_{2;\mathcal{Y}}=\sqrt{ L}E_{\mathsf{samp},\infty}\).

Now observe that the first bound is identical to the corresponding bound in (D.36), except with \(E_{\mathsf{opt},2}\) replaced by \(\left\|\bm{z}\right\|_{2}+2^{-m}\). Following the same arguments as in Step 3 of the proof of Theorem 3.1, this gives the corresponding bound in (D.37), which is

\[\begin{split}\left|\!\left|\!\left|F-\widehat{F}\right|\!\right|\! \right|_{L^{2}_{\mu}(\mathcal{X};\mathcal{Y})}&\lesssim a_{\mathcal{Y}}\left(\|F-q\circ\iota\|_{L^{\infty}_{ \mu}(\mathcal{X};\mathcal{Y})}/\sqrt{m/L}+\|F-q\circ\iota\|_{L^{2}_{\mu}( \mathcal{X};\mathcal{Y})}\right)\\ &\quad+E_{\mathcal{X},2}+E_{\mathcal{Y},2}+\left\|\bm{z}\right\|_{ 2}+2^{-m}+E_{\mathsf{samp},2}.\end{split}\] (E.14)

The second bound above is identical to the corresponding bound in (D.36), except with \(E_{\mathsf{opt},\infty}\) replaced by \(\sqrt{r}\|\bm{z}\|_{2}+2^{-m}\), \(E_{\mathsf{samp},\infty}\) and \(E_{\infty}(F,q)\) replaced by \(E^{\prime}_{\mathsf{samp},\infty}\) and \(\widetilde{E}_{\infty}(F,q)\), respectively, and with \(\sqrt{k}\) replaced by \(\sqrt{m}\). We once more follow the same arguments as in Step 3, with these changes. This yields the corresponding version of (D.37), which is

\[\begin{split}\left\|F-\widehat{F}\right\|_{L^{\infty}_{\mu}( \mathcal{X};\mathcal{Y})}&\lesssim a_{\mathcal{Y}}\left(\sqrt{L}\|F-q\circ\iota\|_{L^{ \infty}_{\mu}(\mathcal{X};\mathcal{Y})}+\sqrt{m}\|F-q\circ\iota\|_{L^{2}_{\mu}( \mathcal{X};\mathcal{Y})}\right)\\ &\quad+E^{\prime}_{\mathcal{X},\infty}+E^{\prime}_{\mathcal{Y}, \infty}+\sqrt{r}\|\bm{z}\|_{2}+2^{-m}+E^{\prime}_{\mathsf{samp},\infty}.\end{split}\] (E.15)

Here \(E^{\prime}_{\mathcal{X},\infty}=LE_{\mathcal{X},\infty}\) and \(E^{\prime}_{\mathcal{Y},\infty}=\sqrt{L}E_{\mathcal{Y},\infty}\).

Having done this, we then use the bounds from Step 4 of the proof of Theorem 3.1, to get

\[\left\|\!\left|F-\widehat{F}\right|\!\right\|_{L^{2}_{\mu}(\mathcal{X };\mathcal{Y})} \lesssim E_{\mathsf{app},2}+E_{\mathcal{X},2}+E_{\mathcal{Y},2}+ \left\|\!\left|\bm{z}\right|\!\right|_{2}+2^{-m}+E_{\mathsf{samp},2}\] \[\left\|F-\widehat{F}\right\|_{L^{\infty}_{\mu}(\mathcal{X}; \mathcal{Y})} \lesssim E^{\prime}_{\mathsf{app},\infty}+E^{\prime}_{\mathcal{X}, \infty}+E^{\prime}_{\mathcal{Y},\infty}+\sqrt{r}\left\|\!\left|\bm{z}\right|\! \right|_{2}+2^{-m}+E^{\prime}_{\mathsf{samp},\infty},\]

where \(E^{\prime}_{\mathsf{app},\infty}=\sqrt{L}E_{\mathsf{app},\infty}\).

_Step 6: Existence of uncountably many minimizers._ Let \(\bm{z}=(z_{i})_{i=1}^{r}\in N(\bm{B})\backslash\{\bm{0}\}\) be any vector and consider \(\bm{z}_{1}=\theta_{1}\bm{z}\) and \(\bm{z}_{2}=\theta_{2}\bm{z}\) for \(\theta_{1},\theta_{2}\in[-1,1]\) with \(\theta_{1}\neq\theta_{2}\). Then these vectors define functions \(\bar{p}_{1}\) and \(\bar{p}_{2}\) as in (E.2) and DNNs \(\widehat{N}_{1}\) and \(\widehat{N}_{2}\) as in (E.3). Suppose that \(\widehat{N}_{1}=\widehat{N}_{2}\). Then, since \(\mathcal{D}_{\mathcal{Y}}\) is linear, we have that \(\bar{p}_{1}=\bar{p}_{2}\). But then, by definition and the fact that \(y\in\widehat{\mathcal{Y}}\backslash\{0\}\), we must have

\[0=\sum_{i=1}^{r}(\bm{z}_{1}-\bm{z}_{2})_{i}N_{\bm{e}_{i}}=(\theta_{1}-\theta_{ 2})\sum_{i=1}^{r}z_{i}N_{\bm{e}_{i}}.\]

Suppose that \(\theta_{1}>\theta_{2}\) without loss of generality and let \(\bm{x}\in D\) be the vector \((\mathrm{sign}(z_{i}))_{i=1}^{\infty}\). Then, \(\Psi_{\bm{e}_{i}}(\bm{x})=\sqrt{3}\mathrm{sign}(z_{i})\) and therefore

\[0\geq(\theta_{1}-\theta_{2})\left(\sqrt{3}\|\bm{z}\|_{1}-\delta r\right)\geq( \theta_{1}-\theta_{2})\left(\sqrt{3}\|\bm{z}\|_{2}-\delta r\right)=(\theta_{1 }-\theta_{2})\left(\sqrt{3}\|\bm{z}\|_{2}-2^{-m}/r\right),\]

since \(\|N_{\bm{e}_{i}}-\Psi_{\bm{e}_{i}}\|_{L^{\infty}_{\mathcal{X}}(D)}\leq\delta\) and \(\delta\leq 2^{-m}/r^{2}\) by definition. We now choose \(\bm{z}\) with \(\left\|\bm{z}\right\|_{2}=2^{-m}/r\leq 2^{-m}\). Note that this choice of \(\bm{z}\) does not change the error bound, except for a constant. It also yields

\[0\geq(\theta_{1}-\theta_{2})(\sqrt{3}2^{-m}-2^{-m})>0\]

which is a contradiction. Hence \(\widehat{N}_{1}\neq\widehat{N}_{2}\). Thus, we have shown that any \(\theta\in[-1,1]\) leads to a distinct DNN minimizer that satisfies the desired bounds. We get the result.

_Step 7: Modifications when \(\mathcal{Y}\) is a Hilbert space._ The modifications required when \(\mathcal{Y}\) is a Hilbert space are identical to those needed in the proof of Theorem 3.1 (see SSD.8). We omit the details. 

Proof of Theorem 3.2, Statement (B).: Let \(N=N_{\bm{\theta}}:\mathbb{R}^{d_{X}}\to\mathbb{R}^{d_{\mathcal{Y}}}\) be a tanh DNN, where \(\bm{\theta}\in\mathbb{R}^{D}\) are the network parameters (weights and biases). Let \(\|\cdot\|_{(d_{\mathcal{X}})}\), \(\|\cdot\|_{(d_{\mathcal{Y}})}\) and \(\|\cdot\|_{(D)}\) be arbitrary norms on \(\mathbb{R}^{d_{X}}\), \(\mathbb{R}^{d_{\mathcal{Y}}}\) and \(\mathbb{R}^{D}\), respectively. Then, since the activation function is a Lipschitz function, we have

\[\left\|N_{\bm{\theta}^{\prime}}(\bm{x})-N_{\bm{\theta}}(\bm{x})\right\|_{(d_{ \mathcal{Y}})}\leq c_{\bm{\theta}}\|\bm{\theta}^{\prime}-\bm{\theta}\|_{(D)}( \|\bm{x}\|_{(d_{\mathcal{X}})}+1),\]

where \(c_{\bm{\theta}}>0\) is a constant depending on \(\bm{\theta}\).

Now let \(\widehat{N}=\widehat{N}_{\bm{\hat{\theta}}}\) and \(\widehat{F}=\mathcal{D}_{\mathcal{Y}}\circ\widehat{N}\circ\mathcal{E}_{ \mathcal{X}}\) and consider \(F=\mathcal{D}_{\mathcal{Y}}\circ N\circ\mathcal{E}_{\mathcal{X}}\) for \(N=N_{\bm{\theta}}\). Since \(\mathcal{D}_{\mathcal{Y}}:\mathbb{R}^{d_{\mathcal{Y}}}\to\mathcal{Y}\) is linear and therefore bounded, we have

\[\left\|\widehat{F}(X)-F(X)\right\|_{\mathcal{Y}}\leq c_{\bm{\theta}}\|\mathcal{ D}_{\mathcal{Y}}\|_{(\mathbb{R}^{d_{\mathcal{Y}}},\|\cdot\|_{(d_{\mathcal{Y}})}) \to\mathcal{Y}}\|\hat{\bm{\theta}}-\bm{\theta}\|_{(D)}\left(\left\|\mathcal{E}_ {\mathcal{X}}(X)\right\|_{(d_{\mathcal{X}})}+1\right).\]

We deduce that

\[\|\widehat{F}-F\|_{L^{2}_{\mu}(\mathcal{X};\mathcal{Y})} \leq\|\widehat{F}-F\|_{L^{\infty}_{\mu}(\mathcal{X};\mathcal{Y})}\] \[\leq c_{\bm{\theta}}\|\mathcal{D}_{\mathcal{Y}}\|_{(\mathbb{R}^{d_{ \mathcal{Y}}},\|\cdot\|_{(d_{\mathcal{Y}})})\to\mathcal{Y}}\|\hat{\bm{\theta}}- \bm{\theta}\|_{(D)}\left(\left\|\mathcal{E}_{\mathcal{X}}\right\|_{L^{\infty}_{ \mu}(\mathcal{X};(\mathbb{R}^{d_{\mathcal{X}}},\|\cdot\|_{(d_{\mathcal{X}})})}+1 \right).\]

Therefore, there exists a neighbourhood around \(\bm{\hat{\theta}}\) for which

\[\left|\!\left|\widehat{F}-F\right|\!\right|_{L^{2}_{\mu}(\mathcal{X}; \mathcal{Y})}\leq\|\widehat{F}-F\|_{L^{\infty}_{\mu}(\mathcal{X};\mathcal{Y})} \leq\tau_{o}\]

for all parameters \(\bm{\theta}\) in the neighbourhood. The result now follows. 

Proof of Theorem 3.2, Statement (C).: By construction, the DNN \(\widehat{N}\) defined in (E.4) contains subnetworks that compute the DNNs \(N_{\bm{e}_{i}}\), \(i=1,\ldots,r\), which themselves are approximations to the Legendre polynomials \(\Psi_{\bm{e}_{i}}\). The construction of these subnetworks was described in the proof of Lemma D.9 as the composition of an affine map defined by the fundamental theorem of algebra and a tanh DNN that approximately multiplies \(m(\Gamma)\) numbers. In this specific case, we have

\[\Psi_{\bm{e}_{i}}(\bm{x})=x_{i}.\]

Therefore, the corresponding affine map (D.18) \(\mathcal{A}_{e_{i}}:\mathbb{R}^{d_{\mathcal{X}}}\to\mathbb{R}^{m(\Gamma)}\) has a bias vector that is all ones, except for a single entry that has value zero. We deduce that the bias vector \(\bm{b}\) of the full DNN (E.4) contains a subblock of size \(rm(\Gamma)\) that is all ones, except for \(r\) zeroes. There are \(\binom{rm(\Gamma)}{r}\) rearrangements of this subblock, each of which leading to a bias vector \(\bm{b}^{\prime}\) with \(\left\|\bm{b}-\bm{b}^{\prime}\right\|\gtrsim 1\). Moreover, \(\bm{b}^{\prime}\) leads to the same DNN, after permuting the various weight matrices in the corresponding way. Indeed, if \(\bm{P}\) is a permutation matrix, then \(\sigma(\bm{W}\bm{x}+\bm{b})=\bm{P}^{-1}\sigma(\bm{PW}\bm{x}+\bm{Pb})\), since \(\sigma\) acts componentwise.

It remains to bound \(\binom{rm(\Gamma)}{r}\) from below. We have

\[\binom{rm(\Gamma)}{r}\geq\frac{(rm(\Gamma))^{r}}{r^{r}}=m(\Gamma)^{r}.\]

By (E.13), we have that \(r\geq 2m\). Now, by the definition (E.1) of \(\Gamma\),

\[m(\Gamma)\geq\max_{S\in\mathcal{S}}m(S),\]

where \(\mathcal{S}\) is as in (D.1) for \(k\) as in (E.12) and \(\Lambda=\Lambda_{n}^{\mathsf{HCI}}\) as in (D.26) with \(n\) as in (D.29). Now consider the set \(S=\{\bm{le}_{1}\}\) for some \(l\in\mathbb{N}\). Then

\[|S|_{\bm{v}}=v_{\bm{\nu}}^{2}=(2l+1)^{5+\xi}=(2l+1)^{1/\delta}.\]

Therefore, \(S\in\mathcal{S}\) provided \((2l+1)^{1/\delta}\leq k\) and \(l\leq n\). Set \(l=\lfloor(k^{\delta}-1)/2\rfloor\) and observe that \(l\leq n\) since \(k\leq n\) and \(\delta\leq 1/5\) by assumption. Therefore \(S\in\mathcal{S}\). Using the definition of \(k\), we get that

\[m(\Gamma)\geq m(S)=l\geq(m/(c_{4}L))^{\delta}\]

for all sufficiently large \(m\). The result now follows. 

## Appendix F Proof of Theorem 4.1

The proof of Theorem 4.1 will follow as a consequence of the following result. For this, we require the following notation. Given \(0<p\leq\infty\), \(s\in\mathbb{N}\) and a sequence \(\bm{c}=(c_{i})_{i=1}^{\infty}\), we let

\[\sigma_{s}(\bm{c})_{p}=\min\{\left\|\bm{c}-\bm{z}\right\|_{p}:\bm{z}\in\ell^{2 },\left|\operatorname{supp}(\bm{z})\right|\leq s\},\]

where \(\operatorname{supp}(\bm{z})=\{i\in\mathbb{N}:z_{i}\neq 0\}\) for \(\bm{z}=(z_{i})_{i\in\mathbb{N}}\in\mathbb{R}^{\mathbb{N}}\).

**Theorem F.1**.: _For any \(0<p<1\) then term \(\theta_{m}(\bm{b})\) defined in (4.2) satisfies_

\[\theta_{m}(\bm{b})\gtrsim\sigma_{m}(\bm{b})_{2},\quad\forall\bm{b}\in\ell^{1} (\mathbb{N}),\ \bm{b}\geq\bm{0},\left\|\bm{b}\right\|_{1}\leq 1.\]

As noted, the proof of this theorem is based on [7, Thm. 4.4]. We recap the details as they will also be needed in the proof of the next result. First, we recall some basic definitions. See [82] or [29, Ch. 10] for more details. Let \(\mathcal{K}\) be a subset of a normed space \((\mathcal{X},\left\|\cdot\right\|_{\mathcal{X}})\). Then its _Gelfand \(m\)-width_ is

\[d^{m}(\mathcal{K},\mathcal{X})=\inf\left\{\sup_{x\in\mathcal{K}\cap L^{m}} \left\|x\right\|_{\mathcal{X}},\ L^{m}\text{ a subspace of }\mathcal{X}\text{ with }\operatorname{ codim}(L^{m})\leq m\right\}.\] (F.1)

An equivalent representation is

\[d^{m}(\mathcal{K},\mathcal{X})=\inf\left\{\sup_{x\in\mathcal{K}\cap \operatorname{Ker}(A)}\left\|x\right\|_{\mathcal{X}},\ A:\mathcal{X}\to \mathbb{R}^{m}\text{ linear}\right\}.\]

The Gelfand width is related to the following quantity:

\[E^{m}_{\mathsf{adia}}(\mathcal{K},\mathcal{X})=\inf\left\{\sup_{x\in\mathcal{ K}}\left\|x-\Delta(\Gamma(x))\right\|_{\mathcal{X}},\ \Gamma:\mathcal{X}\to\mathbb{R}^{m}\text{ adaptive},\ \Delta:\mathbb{R}^{m}\to\mathcal{X}\right\},\] (F.2)where \(\Delta\) is an arbitrary (potentially nonlinear) reconstruction map and \(\Gamma\) is an _adaptive_ sampling map. By this, we mean that

\[\Gamma(x)=\begin{bmatrix}\Gamma_{1}(x)\\ \Gamma_{2}(x,\Gamma_{1}(x))\\ \vdots\\ \Gamma_{m}(x,\Gamma_{1}(x),\ldots,\Gamma_{m-1}(x))\end{bmatrix},\]

where \(\Gamma_{1}:\mathcal{X}\to\mathbb{R}\) is linear and, for \(i=2,\ldots,m\), \(\Gamma_{i}:\mathcal{X}\times\mathbb{R}^{i-1}\to\mathbb{R}\) is linear in its first component.

Proof of Theorem F.1.: We proceed in a series of steps.

_Step 1: Setup._ Define the functions

\[\phi_{i}(\bm{x})=\sqrt{3}x_{i},\quad\bm{x}\in D,\qquad i=1,2,\ldots.\]

Notice that these functions form an orthonormal system in \(L^{2}_{\varrho}(D)\). (A.I) implies that these functions form a Riesz system in \(L^{2}_{\varsigma}(D)\), with Riesz constants that are \(\asymp 1\). Hence they have a (unique) biorthogonal Riesz system \(\{\psi_{i}\}_{i=1}^{\infty}\subset L^{2}_{\varsigma}(D)\). Now define \(\Phi_{i}=\phi_{i}\circ\iota\) and \(\Psi_{i}=\psi_{i}\circ\iota\). Let \(G\in L^{2}_{\mu}(\mathcal{X};\mathbb{R})\) be arbitrary. Then

\[\|G\|_{L^{2}_{\mu}(\mathcal{X};\mathbb{R})}\geq\frac{\langle G,\sum_{i=1}^{ \infty}\langle G,\Psi_{i}\rangle_{L^{2}_{\mu}(\mathcal{X};\mathbb{R})}\Psi_{i }\rangle_{L^{2}_{\mu}(\mathcal{X};\mathbb{R})}}{\|\sum_{i=1}^{\infty}\langle G,\Psi_{i}\rangle_{L^{2}(\mathcal{X};\mathbb{R})}\Psi_{i}\|_{L^{2}_{\mu}( \mathcal{X};\mathbb{R})}}=\frac{\sum_{i=1}^{\infty}|\langle G,\Psi_{i}\rangle_ {L^{2}(\mathcal{X};\mathbb{R})}\Psi_{i}\|_{L^{2}_{\mu}(\mathcal{X};\mathbb{R} )}}{\|\sum_{i=1}^{\infty}\langle G,\Psi_{i}\rangle_{L^{2}(\mathcal{X};\mathbb{ R})}\Psi_{i}\|_{L^{2}_{\mu}(\mathcal{X};\mathbb{R})}}.\]

Consider the denominator. Using (A.II) and fact that the \(\psi_{i}\) form a Riesz system, we see that

\[\left\|\sum_{i=1}^{\infty}\langle G,\Psi_{i}\rangle_{L^{2}(\mathcal{X}; \mathbb{R})}\Psi_{i}\right\|_{L^{2}_{\mu}(\mathcal{X};\mathbb{R})}=\left\| \sum_{i=1}^{\infty}\langle G,\Psi_{i}\rangle_{L^{2}(\mathcal{X};\mathbb{R})} \psi_{i}\right\|_{L^{2}_{\epsilon}(D)}\lesssim\sqrt{\sum_{i=1}^{\infty}| \langle G,\Psi_{i}\rangle_{L^{2}(\mathcal{X};\mathbb{R})}|^{2}}.\]

We deduce that

\[\|G\|_{L^{2}(\mathcal{X};\mathbb{R})}\gtrsim\sqrt{\sum_{i=1}^{\infty}|\langle G,\Psi_{i}\rangle_{L^{2}(\mathcal{X};\mathbb{R})}|^{2}},\quad\forall G\in L^{2 }_{\mu}(\mathcal{X};\mathbb{R}).\] (F.3)

Now let \(\bm{b}\geq\bm{0}\) with \(\bm{b}\in\ell^{1}(\mathbb{N})\) and \(I\subset\mathbb{N}\) with \(|I|=N\). Using [7, Lem. 5.2] we see that the function

\[f=c\sum_{i\in I}c_{i}y\phi_{i}\in\mathcal{H}(\bm{b}),\] (F.4)

for any \(y\in\mathcal{Y}\), \(\left\|y\right\|_{\mathcal{Y}}=1\) and \(\bm{c}=(c_{i})_{i\in\mathbb{N}}\subset\mathbb{R}^{\mathbb{N}}\) with \(|\bm{c}|\leq\bm{b}\) (i.e., \(|c_{i}|\leq b_{i}\), \(\forall i\)), where \(c>0\) is a universal constant.

_Step 2: Reduction to a discrete problem._ Let \(\mathcal{L}\) and \(\mathcal{R}\) be arbitrary sampling and reconstruction maps as in (4.2). Following [7, Lem. 5.3], let \(F=f\circ\iota\) and observe that

\[F(X)=cy\sum_{i\in I}c_{i}\Phi_{i}(X)\]

and therefore

\[\mathcal{L}(F)=y\Gamma(\bm{c}),\]

where \(\Gamma:\mathbb{R}^{|I|}\to\mathbb{R}^{m}\) is given by

\[\Gamma(\bm{z})=\begin{bmatrix}c\sum_{i\in I}z_{i}\Phi_{i}(X_{1})\\ \vdots\\ c\sum_{i\in I}z_{i}\Phi_{i}(X_{m})\end{bmatrix},\]

due to (4.1). Notice that \(\Gamma\) is an adaptive sampling map of the form defined above. Now let \(y^{*}\in B(\mathcal{Y}^{*})\) be such that \(|y^{*}(y)|=\left\|y\right\|_{\mathcal{Y}}\). Then, by (F.3),

\[\left\|F-\mathcal{R}\circ\mathcal{L}(F)\right\|_{L^{2}_{\mu}( \mathcal{X};\mathcal{Y})}^{2} \geq\left\|y^{*}(F-\mathcal{R}\circ\mathcal{L}(F))\right\|_{L^{2}_{\mu}( \mathcal{X};\mathbb{R})}^{2}\] \[\gtrsim\sum_{i\in I}|\langle y^{*}(F-\mathcal{R}\circ\mathcal{L}(F )),\Psi_{i}\rangle_{L^{2}_{\mu}(\mathcal{X};\mathbb{R})}|^{2}.\]By biorthogonality, \(\langle y^{*}(F),\Psi_{i}\rangle_{L^{2}_{\mu}(\mathcal{X};\mathbb{R})}=c\|y\|_{ \mathcal{Y}}c_{i}=c\cdot c_{i}\), which implies that

\[\left\|F-\mathcal{R}\circ\mathcal{L}(F)\right\|\!\!\!\right\|_{L^{2}_{\mu}( \mathcal{X};\mathcal{Y})}\gtrsim\left\|\mathbf{c}-\Delta\circ\Gamma(\mathbf{c} )\right\|_{2},\] (F.5)

where

\[\Delta:\mathbb{R}^{m}\to\mathbb{R}^{N},\quad\bm{y}\mapsto\Delta(\bm{y})= \left(\langle y^{*}(\mathcal{R}(y\cdot\bm{y})),\Psi_{i}\rangle_{L^{2}_{\mu}( \mathcal{X};\mathbb{R})}/c\right)_{i\in I}.\]

Therefore,

\[\sup_{F\in\mathcal{H}(\bm{b},\iota)}\left\|\!\!\left|F-\mathcal{R}\circ \mathcal{L}(F)\right|\!\!\!\right\|_{L^{2}_{\mu}(\mathcal{X};\mathcal{Y})} \gtrsim\inf_{\Gamma,\Delta}\sup_{\begin{subarray}{c}\bm{c}\in\mathbb{R}^{N}\\ \text{supp}(\bm{c})\subseteq I\\ |\bm{c}|\leq\bm{b}\end{subarray}}\left\|\bm{c}-\Delta\circ\Gamma(\bm{c}) \right\|_{2},\]

where the infimum is taken over all adaptive sampling maps \(\Gamma\) and reconstruction maps \(\Delta\). Since \(\mathcal{L}\) and \(\mathcal{R}\) were arbitrary, we get

\[\theta_{m}(\bm{b})\gtrsim E_{m}^{\mathsf{ada}}(B(\bm{b},I),\ell_{N}^{2}),\]

where \(B(\bm{b},I)=\{\bm{z}\in\mathbb{R}^{|I|}:|z_{i}|\leq b_{i},\ i\in I\}\) and \(\ell_{N}^{2}=(\mathbb{R}^{N},\|\cdot\|_{2})\).

_Step 3: Derivation of the lower bounds._ The next step is identical to the proof of Theorem 4.4 in [7]. This gives (F.1). 

Proof of Theorem 4.1.: We use Theorem F.1. For (i), we let \(\bm{b}=(b_{i})_{i=1}^{\infty}\) by defined by

\[b_{i}=(2m)^{-1/p},\ i=1,\ldots,2m,\qquad b_{i}=0,\ i>2m.\]

This sequence \(\bm{b}\in\ell_{\mathsf{M}}^{p}(\mathbb{N})\) with \(\left\|\bm{b}\right\|_{p,\mathsf{M}}=\left\|\bm{b}\right\|_{p}=1\). Moreover, we have

\[\sigma_{m}(\bm{b})_{2}=2^{-1/p}m^{1/2-1/p}.\]

For (ii) we let \(\bm{b}=(b_{i})_{i=1}^{\infty}\) be defined by \(b_{i}=c_{p}(i\log^{2}(i))^{-1/p}\), where \(c_{p}=\left(\sum_{i=1}^{\infty}1/(i\log^{2}(i))\right)^{-1/p}\). This sequence \(\bm{b}\in\ell_{\mathsf{M}}^{p}(\mathbb{N})\) with \(\left\|\bm{b}\right\|_{p,\mathsf{M}}=\left\|\bm{b}\right\|_{p}=1\). Moreover,

\[\sigma_{m}(\bm{b})_{2}^{2}\geq c_{p}^{2}\sum_{i=m+1}^{2m}(i\log^{2}(i))^{-2/p }\geq c_{p}^{2}\cdot m\cdot(2m\log^{2}(2m))^{-2/p},\]

as required. 

## Appendix G Proof of Theorem 4.2

Much as in the previous section, the proof of Theorem 4.2 is a consequence of the following result.

**Theorem G.1**.: _Suppose that the pushforward \(\varsigma\) in (A.1) is a tensor-product of a univariate probability measure. Then the term \(\tilde{\theta}_{m}(\bm{b})\) defined in (4.3) satisfies_

\[\tilde{\theta}_{m}(\bm{b})\gtrsim\sigma_{m}(\bm{b})_{1}/\log(m),\quad\forall \bm{b}\in\ell^{1}(\mathbb{N}),\ \bm{b}\geq\bm{0},\left\|\bm{b}\right\|_{1}\leq 1.\]

Proof of Theorem G.1.: We proceed in a similar series of steps to those of the last proof.

_Step 1: Setup._ Let \(\pi:\mathbb{N}\to\mathbb{N}\) be a bijection that gives a nonincreasing rearrangement of \(\bm{b}\), i.e., \(b_{\pi(1)}\geq b_{\pi(2)}\geq\cdots\). Now, let \(r\in\mathbb{N}\) be arbitrary and consider the index set

\[I=I_{1}\cup\cdots\cup I_{r},\quad I_{l}=\{\pi((l-1)(m+1)+1),\ldots,\pi(l(m+1))\}.\]

Notice that \(|I|=r(m+1)\). Define the matrix

\[\bm{A}=\left((\iota(X_{i}))_{\pi(j)}\right)_{i,j=1}^{m,r(m+1)}\]

and notice that we may write

\[\bm{A}=\left[\bm{A}_{1}\quad\cdots\quad\bm{A}_{r}\right],\quad\text{where }\bm{A}_{l}=\left((\iota(X_{i}))_{\pi((l-1)(m+1)+j)}\right)_{i,j=1}^{m,m+1}.\]

Let \(\sigma\) be the one-dimensional probability measure associated with \(\varsigma\). Then notice that each \(\bm{A}_{l}\) is a random matrix whose entries are drawn i.i.d. from \(\sigma\). Since \(\sigma\) is supported in \([-1,1]\), we deduce that the \(\bm{A}_{l}\) are independent subgaussian random matrices with the same distribution. Write \(\gamma\) for this distribution. Let \(t_{1},\ldots,t_{r}>0\) and \(E_{l,t_{l}}\) be the event

\[E_{l,t_{l}}=\left\{\exists\bm{u}\in N(\bm{A}_{l}):\left\|\bm{u}\right\|_{2}=1, \ \left\|\bm{u}\right\|_{\infty}<\sqrt{t_{l}/(m+1)}\right\},\quad l=1,\ldots,r.\] (G.1)

We will make a suitable choice of \(t_{1},\ldots,t_{r}\) later.

_Step 2: Reduction to a discrete problem._ Let

\[\mathcal{C}=\left\{\bm{c}\in\mathbb{R}^{|I|}:|c_{i}|\leq b_{i},\ \forall i\in I,\ \bm{c}=\begin{bmatrix}\bm{c}_{1}\\ \vdots\\ \bm{c}_{r}\end{bmatrix},\bm{c}_{l}\in N(\bm{A}_{l}),\ l=1,\ldots,r\right\}.\]

Notice that any \(\bm{c}\in\mathcal{C}\) also satisfies \(\bm{c}\in N(\bm{A})\). Now let \(f=f_{\bm{e}}\) be as in (F.4) (we make the dependence on \(\bm{c}\) explicit now for convenience). Let \(\bm{x}\in D\). Then

\[f_{\bm{e}}(\bm{x})=\sqrt{3}cy\sum_{i\in I}c_{i}x_{i}.\] (G.2)

We deduce that

\[(F_{\bm{e}}(X_{i}))_{i=1}^{m}=\sqrt{3}cy\bm{A}\bm{c}=\bm{0},\]

where \(F_{\bm{e}}=f_{\bm{e}}\circ\iota\). This implies that

\[\|F-\mathcal{R}\circ\mathcal{L}(F)\|_{L_{\mu}^{\infty}(\mathcal{X};\mathcal{Y })}=\|F-\mathcal{R}(\{X_{i},0\}_{i=1}^{m})\|_{L_{\mu}^{\infty}(\mathcal{X}; \mathcal{Y})},\]

where, for convenience, we let \(\mathcal{L}:F\mapsto\{X_{i},F(X_{i})\}_{i=1}^{m}\). Therefore,

\[\sup_{F\in\mathcal{H}(\bm{b},\iota)}\|F-\mathcal{R}\circ\mathcal{L}(F)\|_{L_{ \mu}^{\infty}(\mathcal{X};\mathcal{Y})}\geq\sup_{\bm{c}\in\mathcal{C}}\|F_{ \bm{e}}-\mathcal{R}(\{X_{i},0\}_{i=1}^{m})\|_{L_{\mu}^{\infty}(\mathcal{X}; \mathcal{Y})}.\]

Now observe that \(F_{\bm{0}}=0\) and \(\bm{0}\in\mathcal{C}\). Hence

\[\sup_{F\in\mathcal{H}(\bm{b},\iota)} \|F-\mathcal{R}\circ\mathcal{L}(F)\|_{L_{\mu}^{\infty}(\mathcal{X} ;\mathcal{Y})}\geq\frac{1}{2}\sup_{\bm{c}\in\mathcal{C}}\|F_{\bm{c}}\|_{L_{\mu }^{\infty}(\mathcal{X};\mathcal{Y})}.\]

Now, by (A.I),

\[\|F_{\bm{e}}\|_{L_{\mu}^{\infty}(\mathcal{X};\mathcal{Y})}=\|f_{\bm{e}}\|_{L_ {\bm{e}}^{\infty}(D;\mathcal{Y})}\gtrsim\|f_{\bm{e}}\|_{L_{\bm{e}}^{\infty}(D ;\mathcal{Y})}=\sqrt{3}c\sup_{\left\|\bm{x}\right\|_{\infty}\lesssim 1}\left|\sum_{i\in I}c_{i}x_{i} \right|=\sqrt{3}c\|\bm{c}\|_{1}.\]

With this in hand, we conclude that

\[\mathbb{E}_{X_{1},\ldots,X_{m}\sim\mu}\sup_{F\in\mathcal{H}(\bm{b},\iota)}\|F -\mathcal{R}(\{X_{i},F(X_{i})\}_{i=1}^{m})\|_{L_{\mu}^{\infty}(\mathcal{X}; \mathcal{Y})}\gtrsim\mathbb{E}_{\bm{A}_{1},\ldots,\bm{A}_{r}\sim\gamma}\sup _{\bm{c}\in\mathcal{C}}\left\|\bm{c}\right\|_{1}.\]

We now use the definition of \(\mathcal{C}\) to write

\[\mathbb{E}_{X_{1},\ldots,X_{m}\sim\mu}\sup_{F\in\mathcal{H}(\bm{b },\iota)} \|F-\mathcal{R}(\{X_{i},F(X_{i})\}_{i=1}^{m})\|_{L_{\mu}^{\infty}( \mathcal{X};\mathcal{Y})}\] (G.3) \[\gtrsim\sum_{l=1}^{r}\mathbb{E}_{\bm{A}_{l}\sim\gamma}\sup_{ \begin{subarray}{c}\bm{c}_{l}\in N(\bm{A}_{l})\\ \left|(\bm{c}_{l})_{i}\right|\leq b_{i},\forall i\in I_{l}\end{subarray}}\left\| \bm{c}_{l}\right\|_{1}.\]

_Step 3: Bounding the expected error._ Fix \(l=1,\ldots,r\) and suppose the event \(E_{l,t_{l}}\) defined (G.1) occurs. Let \(\bm{u}_{l}\) be the corresponding vector and define

\[\bm{c}_{l}=\frac{b_{\pi(l(m+1))}}{\left\|\bm{u}_{l}\right\|_{\infty}}\bm{u}_{l},\ l=1,\ldots,r.\]By construction, we have that \(\bm{c}_{l}\in N(\bm{A}_{l})\) and \(|(\bm{c}_{l})_{i}|\leq b_{i}\), \(\forall i\in I_{l}\). We deduce that

\[\sup_{\begin{subarray}{c}\bm{c}_{l}\in N(\bm{A}_{l})\\ |(\bm{c}_{l})_{i}|\leq b_{i},\forall i\in I_{l}\end{subarray}}\left\|\bm{c}_{l} \right\|_{1}\geq\frac{b_{\pi(l(m+1))}\left\|\bm{u}_{l}\right\|_{1}}{\left\|\bm {u}_{l}\right\|_{\infty}}.\]

Now observe that

\[1=\left\|\bm{u}_{l}\right\|_{2}^{2}\leq\left\|\bm{u}_{l}\right\|_{1}\left\|\bm {u}_{l}\right\|_{\infty}.\]

Therefore, we get that

\[\sup_{\begin{subarray}{c}\bm{c}_{l}\in N(\bm{A}_{l})\\ |(\bm{c}_{l})_{i}|\leq b_{i},\forall i\in I_{l}\end{subarray}}\left\|\bm{c}_{l }\right\|_{1}\geq\frac{b_{\pi(l(m+1))}(m+1)}{\left\|\bm{u}_{l}\right\|_{\infty }^{2}}\geq\frac{b_{\pi(l(m+1))}(m+1)}{t_{l}}\]

whenever the event \(E_{l,t_{l}}\) occurs. Using the law of total expectation, we deduce that

\[\mathbb{E}_{\bm{A}_{l}\sim\gamma}\sup_{\begin{subarray}{c}\bm{c}_{l}\in N( \bm{A}_{l})\\ |(\bm{c}_{l})_{i}|\leq b_{i},\forall i\in I_{l}\end{subarray}}\left\|\bm{c}_{l }\right\|_{1}\geq\frac{b_{\pi(l(m+1))}(m+1)}{t_{l}}\mathbb{P}(E_{l,t_{l}})\]

for any fixed \(t_{l}>0\). We now appeal to [75, Thm. 1.4]. This shows that

\[\mathbb{P}(E_{l,t_{l}}^{c})\leq c_{2}m^{2}\exp(-t_{l}/c_{2}),\quad\forall t_{ l}\geq c_{1}\log(m+1),\]

where \(c_{1},c_{2}>0\) are universal constants. We may without loss of generality assume that \(c_{2}\geq c_{1}\geq 1\). Now set \(t_{l}=c_{2}\log(2c_{2}m^{2})\geq c_{1}\log(m+1)\). Hence

\[\mathbb{P}(E_{l,t_{l}}^{c})\leq 1/2.\]

We deduce that \(\mathbb{P}(E_{l,t_{l}})>1/2\) and therefore

\[\mathbb{E}_{\bm{A}_{l}\sim\gamma}\sup_{\begin{subarray}{c}\bm{c}_{l}\in N( \bm{A}_{l})\\ |(\bm{c}_{l})_{i}|\leq b_{i},\forall i\in I_{l}\end{subarray}}\left\|\bm{c}_{l }\right\|_{1}\geq\frac{b_{\pi(l(m+1))}(m+1)}{2c_{2}\log(2c_{2}m^{2})}.\]

Now observe that

\[b_{\pi(l(m+1))}(m+1)\geq b_{\pi(l(m+1))}+\cdots+b_{\pi(l(m+1)+m)}\]

Substituting this into (G.3), we deduce that

\[\mathbb{E}_{X_{1},\dots,X_{m}\sim\mu}\sup_{F\in\mathcal{H}(\bm{b},t)}\left\|F -\mathcal{R}(\{X_{i},F(X_{i})\}_{i=1}^{m})\right\|_{L^{c}_{\mu}(\mathcal{X}; \mathcal{Y})}\gtrsim\frac{1}{\log(2m)}\sum_{i=m+1}^{r(m+1)+m}b_{\pi(i)}.\]

Since \(r\) was arbitrary, we may take the limit \(r\to\infty\). We now use the fact that

\[\sigma_{m}(\bm{b})_{1}=b_{\pi(m+1)}+b_{\pi(m+2)}+\cdots.\]

to obtain the result. 

Proof of Theorem 4.2.: Using Theorem G.1, statements (i) and (ii) are derived in exactly the same way as in the proof of Theorem 4.1

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: We thoroughly discuss the main claims made in the abstract and introduction and the necessary assumptions to show them. Our main theoretical contributions directly address these claims. We also have several further remarks after these results to provide further context for our work. We also provide detailed numerical experiments showing that DNN architectures compatible with the setup for the theoretical results actually achieve the presented rates of approximation for challenging operator learning problems posed in Banach spaces in terms of the number of samples needed to achieve a given tolerance.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We discuss our main assumptions and their relative strengths in detail in a separate section, SS2.3. We also end the paper with a section discussing limitations. See SS6.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: We have a detailed discussion of the assumptions needed to show our theoretical results in SS2.2-2.3. We provide further discussion of the results themselves in SS3-4 to place the theoretical advancements in this work in the broader context of the operator learning literature. We provide full proofs of our results in the supplemental material.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We provide all of the necessary source code in the supplemental material. Furthermore, we provide a detailed discussion of the setup for the numerical experiments in SSA-B of the supplemental material. Given the code and description of the experiments, reproducing the experiments is straightforward.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: We provide all the necessary software to reproduce our experiments along with instructions for running the code to generate the results.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We specify all of the necessary hyperparameters to obtain our experimental results as well as the optimizers used for training and the software used to generate our training and testing data. All of these are open source, no proprietary data was used in this work.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: We plot not just the average error over all of the trials but also the (corrected) sample standard deviation of the transformed sample with shaded plots to provide an estimate of the variability in the runs.

8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: The experimental details are provided in the Appendix SSA.2. All computational resources are reported, including the type of workers, memory requirements, storage requirements, and time of execution.
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We have complied with the NeurIPS Code of Ethics in the preparation of this manuscript. No human subject data was used to generate the results for our numerical experiments and data-related concerns are not relevant to this work.
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: This work is primarily foundational, and the examples considered do not directly impact society.
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: This work does not release any data or models that have a high risk for misuse. All code is open source and no proprietary data was used in this work.
12. **Licensees for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: The code submitted in the supplemental material to both generate the data for training and testing our models and generate the experimental results was written by the authors. No other code or datasets were used in the production of this work.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: New code and data is included as a zip file as supplemental material.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: We do not use crowdsourcing for our experiments and no research was conducted with human subjects.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained?Answer: [NA]

Justification: We do not use crowdsourcing for our experiments and no research was conducted with human subjects.