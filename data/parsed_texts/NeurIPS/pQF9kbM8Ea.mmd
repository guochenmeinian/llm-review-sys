# Leveraging Vision-Centric Multi-Modal Expertise

for 3D Object Detection

 Linyan Huang\({}^{1}\) Zhiqi Li\({}^{2}\) Chonghao Sima\({}^{1}\) Wenhai Wang\({}^{3}\)

Jingdong Wang\({}^{4}\) Yu Qiao\({}^{1}\) Hongyang Li\({}^{1}\)

\({}^{1}\)Shanghai AI Lab \({}^{2}\)Nanjing University \({}^{3}\)CUHK \({}^{4}\)Baidu

###### Abstract

Current research is primarily dedicated to advancing the accuracy of camera-only 3D object detectors (apprentice) through the knowledge transferred from LiDAR- or multi-modal-based counterparts (expert). However, the presence of the domain gap between LiDAR and camera features, coupled with the inherent incompatibility in temporal fusion, significantly hinders the effectiveness of distillation-based enhancements for apprentices. Motivated by the success of uni-modal distillation, an apprentice-friendly expert model would predominantly rely on camera features, while still achieving comparable performance to multi-modal models. To this end, we introduce **VCD**, a framework to improve the camera-only apprentice model, including an apprentice-friendly multi-modal expert and temporal-fusion-friendly distillation supervision. The multi-modal expert **VCD-E** adopts an identical structure as that of the camera-only apprentice in order to alleviate the feature disparity, and leverages LiDAR input as a depth prior to reconstruct the 3D scene, achieving the performance on par with other heterogeneous multi-modal experts. Additionally, a fine-grained trajectory-based distillation module is introduced with the purpose of individually rectifying the motion misalignment for each object in the scene. With those improvements, our camera-only apprentice **VCD-A** sets new state-of-the-art on nuScenes with a score of 63.1% NDS. The code will be released at https://github.com/OpenDriveLab/Birds-eye-view-Perception.

## 1 Introduction

The camera-only 3D perception has garnered increasing attention in autonomous driving perception tasks [31, 21, 47, 50]. Although camera-only models possess the advantages of low deployment cost and ease of widespread application, they still fall behind state-of-the-art models that leverage LiDAR sensors regarding perception accuracy. Researchers have recently employed distillation methods to transfer knowledge from a powerful expert model into a camera-only apprentice model, with the expectation of leveraging the expertise of these stronger expert models to enhance the capability of the camera-only models. Existing 3D perception distillation methods often adopt expert models with the best performance, such as LiDAR-based models [60] or multi-modal fusion models [2, 59, 33]. However, the presence of the domain gap between LiDAR and camera features hampers knowledge transfer during distillation, resulting in limited improvements in practical applications. An alternative expert model is the large-scale camera-only model [32, 61]. Despite eliminating the domain gap between the camera-only expert model and the apprentice model, the expert model falls short in terms of effectiveness due to the inherent lack of precise geometry information. Likewise, it fails to yield a satisfactory improvement to the apprentice model. Hence, a desirable expert should meet two essential requirements: attaining state-of-the-art performance and minimizing the domain gap.

Furthermore, current distillation methods fall short in compatibility with long-term temporal fusion, which is an essential component in cutting-edge camera-only 3D detectors [18, 43]. Long-termtemporal modeling has shown considerable potential in enhancing the accuracy of depth estimation and detection performance, but it introduces the issue of motion misalignment. Previous methods for BEV distillation have followed two distinct approaches, either distilling the entire BEV space without sufficient attention to the foreground objects [44] or exclusively distilling the foreground object regions [11; 25], thereby overlooking the motion misalignment issue resulting from the long-term temporal fusion. As shown in Fig. 1 (b), this misalignment occurs when past scenes are transformed into the current scene coordinates based solely on ego-motion, assuming all objects are stationary. While in reality, dynamic objects will cause the misalignment, thus interfering with the temporal fusion features. This is more challenging in the case of long-term temporal fusion. Existing methods, such as StreamPETR [51], introduce LayerNorm [1] for dynamic object modeling, but the effects of incorporating velocity and time variables in the model are relatively minor.

To address the aforementioned challenges, we first propose a vision-centric expert, termed as **VCD-E**, which incorporates LiDAR information to enhance the accuracy of depth input. In this context, the term "vision-centric" refers to the utilization of prominent features derived from camera input, distinguishing it from approaches that heavily rely on LiDAR-based features. This model is distinct from conventional multi-modality fusion techniques by eliminating LiDAR backbone. By solely integrating LiDAR depth and long-term temporal fusion under bird's-eye-view (BEV), our model achieves comparable performance to state-of-the-art multi-modal fusion methods [38] by only encoding image modality. As illustrated in Fig. 1 (a), different from previous fusion methods that adopt two modality-specific backbones, we only leverage a single branch to generate semantic features based on image input, and point clouds only provide depth information. Compared to previous fusion methods, our approach eliminates the need for intricate training strategies or specialized fusion module designs while reaching a comparable performance to current fusion methods [38; 33]. More importantly, the vision-centric multi-modal model has the exact same architecture as the camera-only apprentice model, and the generated BEV spatial representation is solely from image features. The domain gap is significantly alleviated through the distillation of knowledge from the proposed multi-modal expert model to the camera-only apprentice model. Due to the advantageous characteristic of domain consistency, our apprentice model acquires substantial benefits from the expert, surpassing previous distillation methods [11; 46].

To mitigate the incompatibility arising from the motion misalignment of dynamic objects, we further propose a trajectory-based distillation module. In this paper, our primary focus revolves around foreground objects, while simultaneously incorporating a meticulous consideration of their historical trajectories. Specifically, by warping dynamic targets from history to the current frame, we derive the motion trajectory associated with each individual object. Then we use the trajectory of each object to query BEV features of the apprentice and expert models respectively. By leveraging the trajectory features of the expert model to optimize the corresponding features of the apprentice model, the latter can acquire the ability to mitigate the interference arising from motion misalignment. In addition, to

Figure 1: (a) The existing pipelines require camera and LiDAR backbones, while our pipeline eliminates the need for LiDAR. Using point cloud depth, we directly transform image features into BEV space to create a vision-centric expert. (b) The warping of an object in the historical frame into the current timestamp results in a false position in the current frame due to assuming the object is stationary. The green rectangle represents true positives, while the pink rectangle indicates false positives. \(x_{i}\) denotes the various positions of the object in the historical timestamp.

enhance the depth perception ability, we diffuse the depth of the foreground part into the 3D space, modeling occupancy to obtain grid-based supervision to assist in depth prediction for objects.

In summary, we propose the multi-modality expert and camera-only apprentice models, termed as **VCD-E** and **VCD-A**, respectively. Our contributions are summarized as follows:

\(\bullet\) We construct a vision-centric multi-modal expert that solely encodes the image modality, eliminating the need for a LiDAR backbone. For the first time, we demonstrate that the expert can deliver performance on par with other state-of-the-art multi-modal methods while being significantly simpler.

\(\bullet\) Due to its homogeneous characteristics and superior performance, the vision-centric expert has been proven effective in distilling knowledge to vision-based models. The effects are significant across a range of model sizes, from compact to more extensive architectures.

\(\bullet\) We propose trajectory-based distillation and occupancy reconstruction modules, which supervise both static and dynamic objects to alleviate misalignment during long-term temporal fusion. Combined with the constructed expert model, we enhance the performance of the vision-based models and achieve state-of-the-art on the nuScenes val and test leaderboard.

## 2 Related Work

In this section, we review previous studies in the areas of 3D object detection, multi-modality fusion, knowledge distillation, focusing on the techniques and methods most relevant to our research.

3D Object Detection.3D object detection has recently gained significant popularity in the context of autonomous driving and robotics. Detection methods generally fall into two categories: vision-based 3D object detection [64; 65; 5; 37; 52; 36; 53; 58; 22; 56] and LiDAR-based 3D detection [30; 60; 45]. Vision-based approaches [34; 54; 15; 35] exploit image information and frequently involve deep learning techniques to estimate depth. In contrast, LiDAR-based methods capitalize on precise geometric information from LiDAR sensors to achieve superior object detection accuracy. Our proposed vision-centric expert shares the same modality as vision-based detectors, while exhibiting superior performance compared to LiDAR-based detectors.

Long-term modeling has been employed to improve the performance of 3D object detection models [18; 43]. SOLOFusion [43] utilizes long-term temporal modeling to achieve excellent performance. VideoBEV [18] maintains comparable performance with SOLOFusion while being more efficient, using long-term recurrent temporal modeling. However, previous research [34; 51] has highlighted that long-term temporal fusion can lead to inadequate detection of dynamic objects. Although StreamPETR [51] proposes the propagation transformer [48; 7; 9; 8] to conduct object-centric temporal modeling, the improvement of dynamic object modeling remains relatively modest. Our proposed trajectory-based distillation module alleviates this limitation, enabling accurate detection of both static and dynamic objects by camera-based 3D object detection models that utilize long-term modeling.

Multi-modality Fusion.Multi-modality fusion techniques [49; 2; 33; 16] have been extensively investigated to enhance 3D object detection performance by integrating complementary information from different sensor modalities, such as cameras and LiDAR sensors. These methods [38; 59] typically require complex training strategies and the development of specialized fusion modules to effectively merge the distinct sources of information. In contrast, we propose a streamlined architecture that utilizes an image backbone for feature extraction, obviating the need for a LiDAR backbone. This efficient approach augments vision-based models by incorporating LiDAR information, maintaining homogeneity with vision-based models while preserving exceptional performance.

Knowledge Distillation.Knowledge distillation [44; 46] is a technique facilitating the transfer of knowledge from a larger, more complex model (expert) to a smaller, more efficient model (apprentice). This approach has been successfully applied in various domains, including image classification [14], natural language processing [48], and policy learning [55; 27; 26; 6]. Therefore, recent distillation methods [17; 25; 32; 24; 29] build upon 3D object detection aims to transfer the accurate geometry knowledge from LiDAR to camera. MonoDistill [12] projects the LiDAR points into the image plane to serve as the expert to transfer knowledge. BEVSimDet [62] simulates fusion-based methods to alleviate the domain gap between the two different modalities. BEVDistill [11] projects the LiDAR points and images into the BEV space to align the LiDAR feature and image feature. Due to the non-homogenous nature between the LiDAR and camera, transferring knowledge from LiDAR to images is challenging. Instead, our work constructs a vision-centric expert, which possesses a homogeneous modality with vision-based models. The vision-centric expert can leverage knowledge distillation to transfer the geometric perception capabilities to various vision-based models, hence enhancing performance accordingly.

## 3 Method

In this section, we present our approach in detail. The overall architecture is presented in Sec. 3.1. Our method involves two main components: (1) the vision-centric expert in Sec. 3.2, and (2) the trajectory-based distillation and occupancy reconstruction modules as elaborated in Sec. 3.3. The pipeline of our method is depicted in Fig. 2.

### Overall Architecture

In this paper, we construct a pair of harmonious expert and apprentice models. The expert and apprentice models adopt the consistent model architecture. The only difference is that the expert additionally leverages the accurate depth map generated from the point cloud, while the apprentice model predicts the depth map from the image. Although our expert model only uses an image backbone to encode high-level scene information, it is on par with state-of-the-art multi-model fusion methods that use several modality-specific backbones and complex interaction strategies. More importantly, we eliminate the domain gap between the multi-model expert and the camera-only apprentice model, which is deemed as one of the most challenging topics in the cross-modality distillation literature.

As illustrated in Fig. 2, we construct a distillation framework between the expert network and the apprentice network. The vision-centric expert fuses feature \(\bm{I}^{E}\) extracted from the image backbone and the temporal depth map \(\bm{D}\) projected from LiDAR points to create a unified BEV representation \(\bm{F}^{E}\) used for 3D object detection. Therefore, although we adopt a cross-modality approach for 3D object detection, the resulting representation remains homogeneous with image modality features.

After obtaining the pretrained vision-centric expert and corresponding apprentice network, we freeze the expert network and leverage its intermediate features as auxiliary supervision for the apprentice network. Since current advanced vision-based detectors employ long-term temporal modeling to attain state-of-the-art performance, we utilize a standard long-term temporal vision-based detector based on BEVDepth as the apprentice model in our context. The expert model also utilizes long-term temporal modeling to ensure consistency and achieve higher performance.

### The Generation of Expert Model

We construct a vision-centric expert by integrating LiDAR information as accurate depth input into a vision-based model. Nevertheless, given the sparsity of LiDAR depth data, we rely on the predicted depth values obtained from images for pixels lacking LiDAR depth information. We also utilize future frames to further improve the performance of the expert model in the offline 3D detection setting. The vision-based detector serves as the primary model, and LiDAR information complements it by providing precise depth information. This approach eliminates the need for complex training strategies or custom-designed fusion modules, streamlining the fusion process.

For the expert model, we project the last sweep LiDAR frame with the current LiDAR frame onto images to obtain corresponding depth maps \(\bm{D}\). Since the depth maps \(\bm{D}\) generated from point clouds can not cover every pixel of the images, we also predict depth distribution for each pixel based on the image features. Then we will project the image features onto the BEV space to obtain BEV features \(\bm{F}^{E}\) based on their depth. Furthermore, we transform the BEV features \(\bm{F}^{E}_{T-N}\) from previous timestamps to the current BEV features \(\bm{F}^{E}_{T}\), modeling the long-term relationship. \(N\) denotes the time interval between the current frame and the history frame. The unified BEV features \(\bm{F}^{E}\) are then combined to produce 3D object detection predictions \(\bm{P}\). The model is trained using a multi-task loss function that considers both 3D detection loss \(\mathcal{L}_{Det}\) and depth estimation loss \(\mathcal{L}_{Depth}\).

### The Procedure of Distillation

In this section, we elucidate the methodology adopted to overcome the constraints inherent in long-term modeling for multi-camera 3D object detection. This is achieved through the incorporation of two innovative modules within the distillation process: the trajectory-based distillation module and the occupancy reconstruction module.

Trajectory-based Distillation.The trajectory-based distillation module aims to improve the detection of dynamic objects by focusing on the inconsistent portion of the objects' motion. For \(i\)-th historical frame at timestamp \(t_{i}\) that contains \(K\) objects, extract the \(j\)-th ground truth object position \(\bm{P}_{i}^{j}=(x_{i}^{j},y_{i}^{j},z_{i}^{j},1)^{T}\) in the ego coordinate system. Determine the actual ego motion matrix \(\bm{M}_{i}\) between the current frame \(t_{0}\) and each historical frame \(t_{i}\). Apply the ego-motion transformation matrix \(\bm{M}_{i}\) to the ground truth object positions \(\bm{P}_{i}^{j}\) to obtain the transformed positions \(\bm{P}_{i}^{j^{\prime}}\) in the current frame's coordinate system:

\[\bm{P}_{i}^{j^{\prime}}=\bm{M}_{i}\times\bm{P}_{i}^{j}.\] (1)

We amalgamate the transformed ground truth object positions, \(\bm{P}_{i}^{j^{\prime}}\), from all historical frames to construct the motion trajectories. The trajectory of \(j\)-th object can thus be represented as a sequence of object positions within the current frame as \((\bm{P}_{1}^{j^{\prime}},\bm{P}_{2}^{j^{\prime}},\dots,\bm{P}_{N}^{j^{\prime}})\), where \(N\) is the number of points on each motion trajectory. Let \(\bm{F}_{ij}^{E}\), \(\bm{F}_{ij}^{A}\) represent the sampled features on identical point \(\bm{P}_{i}^{j^{\prime}}\) from the expert BEV feature \(\bm{F}^{E}\) and apprentice BEV feature \(\bm{F}^{A}\), respectively. They are sampled via bilinear interpretation and then are normalized as :

\[\bm{F}_{ij}^{E}=\text{norm}(\bm{F}^{E}(P_{i}^{j^{\prime}})),\quad\bm{F}_{ij}^{ A}=\text{norm}(\bm{F}^{A}(P_{i}^{j^{\prime}})).\] (2)

Finally, the trajectory-based distillation loss \(\mathcal{L}_{TD}\) is computed between the normalized key sampled features:

\[\mathcal{L}_{TD}=\frac{1}{K\times N}\sum_{j=1}^{K}\sum_{i=1}^{N}L_{2}(\bm{F}_ {ij}^{A},\bm{F}_{ij}^{E}).\] (3)

Ultimately, by using the motion trajectory as queries, we conduct trajectory-based distillation on these representative positions. This approach enables the expert to rectify the motion misalignment in the apprentice.

Figure 2: **Algorithm Overview.** Expert utilizes LiDAR data to enhance depth estimation accuracy before view transformation in BEV pipeline. Apprentice represents a standard long-term vision-based detection model. The occupancy is built using the depth information from the expert model, which serves as the supervision for the apprentice model. Motion trajectory is constructed by warping the time series GT query into the current timestamp. Projecting the motion trajectory of each object into BEV space can rectify the misalignment of object motion. With the knowledge transferred from the expert, the apprentice can deliver higher performance than before.

[MISSING_PAGE_FAIL:6]

[MISSING_PAGE_FAIL:7]

Effectiveness of the General Framework.We first validate the effectiveness of the general framework for various temporal lengths. In Tab. 4, we examine different timestamps for temporal modeling and find that our method significantly outperforms the baseline. By employing the general framework, we achieve 5.0% NDS and 5.7% mAP performance gains with 8 temporal modeling instances. For other temporal modeling scenarios, our method continues to exhibit substantial performance improvements, ranging from 3.6% to 5.0% NDS. As the duration of temporal fusion extends, the advantages of the model become increasingly pronounced, suggesting that this framework exhibits higher compatibility with extended sequences. This indicates the effectiveness of our proposed trajectory-based distillation module, which alleviates the issue of motion misalignment.

Effectiveness of VCD-E.In this study, it is crucial to verify the effectiveness of our proposed vision-centric experts. We select various expert models for a fair comparison, including LiDAR-based expert Centerpoint [60], vision-based expert BEVDepth [35], and fusion-based expert Transfusion, based on our proposed distillation strategy. In Tab. 5, we observe that homogeneous expert models significantly influence the success of knowledge transfer. The camera-based expert BEVDepth demonstrates superior performance gains compared to the other two heterogeneous expert models. Our advanced model, VCD-E, operates within the same modality as vision-based models, and it is equipped with precise geometric information. Consequently, the distillation effect outperforms the Transfusion [2] expert by a significant margin, achieving a 6.2% increase in mAP. Moreover, it surpasses the BEVDepth expert by an additional 1.7% in NDS. This demonstrates the superior performance and effectiveness of our model.

Comparison to SOTA.To further demonstrate the effectiveness of our proposed distillation strategy, we compare our method with other state-of-the-art methods. To ensure a fair comparison, we conduct all experiments based on our vision-centric expert VCD-E. We find that our method consistently outperforms other distillation methods by a significant margin. In 2D detection, FitNet [44] and CWD [46] are two classic distillation methods, we adapt them for 3D object detection to facilitate a fair comparison. BEVDistill [11] represents a state-of-the-art distillation method for multi-view 3D object detection, and we compare our results with this approach as well. As shown in Tab. 6, our method achieves the best results when compared to these state-of-the-art distillation strategies, demonstrating the effectiveness of our approach. Our method surpasses BEVDistill by 2% NDS and 3.8% mAP.

\begin{table}
\begin{tabular}{l|c|c} \hline Methods & mAP & NDS \\ \hline Baseline [35] & 0.297 & 0.409 \\ FitNet [44] & 0.318 & 0.421 \\ CWD [46] & 0.311 & 0.412 \\ BEVDistill [11] & 0.316 & 0.439 \\ VCD-A & **0.354** & **0.459** \\ \hline \end{tabular}
\end{table}
Table 6: **Effect of different distillation methods. All models are trained with VCD-E as expert. Our proposal significantly surpasses previous SOTA methods.**

\begin{table}
\begin{tabular}{c|c|c|c|c|c c} \hline Temporal Length & Distill & mAP (\%)\(\uparrow\) & NDS (\%) \(\uparrow\) & mATE\(\downarrow\) & mAOE\(\downarrow\) & mAVE\(\downarrow\) \\ \hline \multirow{2}{*}{1} & ✗ & 26.6 & 37.9 & 0.815 & 0.645 & 0.556 \\  & ✓ & 30.1 (+3.5) & 41.5(+3.6) & 0.732 & 0.629 & 0.476 \\ \hline \multirow{2}{*}{2} & ✗ & 26.9 & 38.4 & 0.804 & 0.706 & 0.461 \\  & ✓ & 31.3 (+4.4) & 43.2 (+4.8) & 0.717 & 0.615 & 0.403 \\ \hline \multirow{2}{*}{4} & ✗ & 28.4 & 39.8 & 0.748 & 0.739 & 0.432 \\  & ✓ & 33.0 (+4.6) & 44.1 (+4.3) & 0.707 & 0.632 & 0.389 \\ \hline \multirow{2}{*}{8} & ✗ & 29.7 & 40.9 & 0.762 & 0.714 & 0.415 \\  & ✓ & 35.4 (+5.7) & 45.9 (+5.0) & 0.690 & 0.625 & 0.370 \\ \hline \end{tabular}
\end{table}
Table 4: **Ablation study of the proposed distillation framework on different temporal lengths.** The design works with different length of time windows and the gain grows with the temporal length.

\begin{table}
\begin{tabular}{l|c|c|c} \hline Expert & Paradigm & mAP & NDS \\ \hline - & - & 0.297 & 0.409 \\ CenterPoint [60] & CM & 0.281 & 0.420 \\ Transfusion [2] & CM & 0.292 & 0.435 \\ BEVDepth [35] & UM & 0.341 & 0.442 \\ VCD-E & UM & **0.354** & **0.459** \\ \hline \end{tabular}
\end{table}
Table 5: **The performance gains of the apprentice** which benefit from different experts. CM denotes cross-modal and UM represents Uni-modal. It indicates the success of uni-modal distillation remains.

Gains of the Image Backbone.To verify that VCD-E benefits from contemporary image backbones, we conducted experiments outlined in Tab. 7. We selected ResNet-50 and ConvNext-B as two distinct modern image backbones for BEVFusion and VCD-E. Our findings indicate that VCD-E achieves substantial improvements of 5.3% mAP and 3.7% NDS when using ConvNext-B compared to ResNet-50, demonstrating that VCD-E can indeed benefit from modern image backbones. However, existing fusion-based methods, such as BEVFusion, which primarily rely on the capabilities of LiDAR backbones, show limited gains from employing modern image backbones.

Fusion Strategy of Expert.To determine the optimal fusion strategy for LiDAR points and images, we propose four distinct depth fusion approaches, including (1) Predicted depth that directly uses predicted depth based on image features. (2) LiDAR depth that utilizes the projected depth from LiDAR points. (3) Fusion depth that employs the projected depth when corresponding LiDAR points exist for each pixel, otherwise using the predicted scores based on image features, and (4) Weighted depth. Applying the weighted average depth of the predicted depth and LiDAR depth. As demonstrated in Tab. 8, fusion depth yields the best results which indicates that using corresponding predicted scores where LiDAR depth is unavailable is advantageous.

The Effectiveness of Trajectory-based DistillationThe results presented in Table 9 indicate that as the trajectory length increases, the benefits derived from the distillation process become more pronounced. The temporal fusion length for this experiment is set at eight. The first row denotes the baseline model which does not use the distillation method. The second row depicts that the VCD-A model directly conducts distillation under the full BEV feature without the Trajectory-based distillation module. When the trajectory length is set to 1, we only distill ground truth locations in the current frame. However, when the trajectory length exceeds five, there is a noticeable decrease in accuracy. We hypothesize that this decrease may be attributed to the model's distracted attention towards distant motions. The density of traffic can lead to distant motion locations being occupied by other objects, which may not necessarily require additional trajectory supervision. This suggests that the application of excessive trajectory supervision in such scenarios could be unnecessary and inefficient.

Ablation Study of Each Component.In Tab. 10, we conduct an ablation study on the components employed in VCD to verify their contributions to the final result. These components include (1) Long-term temporal fusion, which allows our model to benefit from historical information, thereby enhancing its performance. (2) Trajectory-based distillation, which transfers knowledge from VCD-E to VCD-A in areas with motion misalignment to mitigate this issue. (3) Occupancy reconstruction,

\begin{table}
\begin{tabular}{c|c|c|c} \hline \hline Methods & Backbone & mAP & NDS \\ \hline BEVFusion & ResNet-50 & 0.598 & 0.662 \\ BEVFusion & ConvNext-B & 0.597 & 0.665 \\ VCD-E & ResNet-50 & 0.611 & 0.656 \\ VCD-E & ConvNext-B & **0.664** & **0.693** \\ \hline \hline \end{tabular}
\end{table}
Table 7: **Gains of different image backbone** on multi-modal models [38]. The stronger backbone still demonstrates better performance in this case.

\begin{table}
\begin{tabular}{c|c|c|c} \hline \hline Trajectory Length & Distill & mAP (\%) & NDS (\%) \\ \hline - & ✗ & 29.7 & 40.9 \\
0 & ✓ & 31.8 & 42.1 \\
1 & ✓ & 33.1 & 44.5 \\
3 & ✓ & 34.6 & 45.6 \\
5 & ✓ & **35.4** & **45.9** \\
9 & ✓ & 33.9 & 44.7 \\ \hline \hline \end{tabular}
\end{table}
Table 8: **Gains of different depth fusion strategy.** The proposed fusion depth is optimal among different methods.

\begin{table}
\begin{tabular}{c|c|c|c} \hline \hline Methods & mAP & NDS \\ \hline Predicted depth & 0.495 & 0.585 \\ LiDAR depth & 0.638 & 0.687 \\ Weighted depth & 0.644 & **0.690** \\ Fusion depth & **0.646** & **0.690** \\ \hline \hline \end{tabular}
\end{table}
Table 8: **Gains of different depth fusion strategy.** The proposed fusion depth is optimal among different methods.

which serves as dense depth supervision in the perspective view, improving the performance of VCD-A. Long-term temporal modeling enables our model to reference prior information from history, assisting in object velocity estimation and resulting in a significantly lower mATE. We also evaluate the impact of the trajectory-based distillation module when all other components are incorporated. As shown in Tab. 10, trajectory-based distillation increases NDS by 3.3% and mAP by 3.3%, contributing to the majority of the improvement.

## 5 Conclusion

In this paper, we presented a novel vision-centric model as the expert, which leverages the strengths of both modalities, our proposed model achieves exceptional performance, rivaling that of state-of-the-art multimodal fusion models, while also mitigating domain gap issues, making it suitable for distilling vision-based models. Furthermore, we introduced two innovative modules, the trajectory-based distillation and occupancy reconstruction modules. These modules enhance the geometric perception capabilities of multi-camera 3D object detection models and improve the detection of both static and dynamic objects in the scene. Our experiments demonstrate the effectiveness of our proposed methods on the widely-used nuScenes 3D object detection benchmark.

Limitation.In this work, we have not delved into more details of the vision-centric expert, which may hold significant potential for improvement. Additionally, we have not explored further applications, such as automatic dimension estimation based on VCD-E.

## Acknowledgements

This work was supported by National Key R&D Program of China (2022ZD0160104) and NSFC (62206172). We would like to thank anonymous reviewers for active discussions.

## References

* [1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. _arXiv preprint arXiv:1607.06450_, 2016.
* [2] Xuyang Bai, Zeyu Hu, Xinge Zhu, Qingqiu Huang, Yilun Chen, Hongbo Fu, and Chiew-Lan Tai. TransFusion: Robust lidar-camera fusion for 3d object detection with transformers. In _CVPR_, 2022.
* [3] Holger Caesar, Varun Bankiti, Alex H Lang, Sourabh Vora, Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Giancarlo Baldan, and Oscar Beijbom. nuScenes: A multimodal dataset for autonomous driving. In _CVPR_, 2020.
* [4] Yuxuan Cai, Yizhuang Zhou, Qi Han, Jianjian Sun, Xiangwen Kong, Jun Li, and Xiangyu Zhang. Reversible column networks. In _ICLR_, 2023.
* [5] Li Chen, Chonghao Sima, Yang Li, Zehan Zheng, Jiajie Xu, Xiangwei Geng, Hongyang Li, Conghui He, Jianping Shi, Yu Qiao, et al. Persformer: 3d lane detection via perspective transformer and the openlane benchmark. In _ECCV_, 2022.
* [6] Li Chen, Penghao Wu, Kashyap Chitta, Bernhard Jaeger, Andreas Geiger, and Hongyang Li. End-to-end autonomous driving: Challenges and frontiers. _arXiv preprint arXiv:2306.16927_, 2023.

\begin{table}
\begin{tabular}{l|c c c|c|c c} \hline \hline Methods & Long & Occ & Motion & mAP\(\uparrow\) & NDS\(\uparrow\) & mATE\(\downarrow\) & mAVE\(\downarrow\) \\ \hline Baseline & & & & 0.271 & 0.319 & 0.767 & 1.065 \\ Longer Temporal Fusion & ✓ & & & 0.297 & 0.409 & 0.762 & 0.415 \\ Trajectory-based Distillation & & & ✓ & 0.283 & 0.356 & 0.772 & 0.870 \\ All but Longer Temporal & & ✓ & ✓ & 0.290 & 0.367 & 0.724 & 0.834 \\ Occupancy Reconstruction & ✓ & ✓ & & 0.308 & 0.416 & 0.734 & **0.379** \\ VCD & ✓ & ✓ & ✓ & **0.341** & **0.449** & **0.715** & 0.389 \\ \hline \hline \end{tabular}
\end{table}
Table 10: **Ablation study of important components in VCD-A.** It shows that the motion module contributes most to the performance improvement, indicating the potential of future research here.

* [7] Mengzhao Chen, Mingbao Lin, Ke Li, Yunhang Shen, Yongjian Wu, Fei Chao, and Rongrong Ji. Cf-vit: A general coarse-to-fine method for vision transformer. In _AAAI_, 2023.
* [8] Mengzhao Chen, Mingbao Lin, Zhihang Lin, Yuxin Zhang, Fei Chao, and Rongrong Ji. Smmix: Self-motivated image mixing for vision transformers. In _ICCV_, 2023.
* [9] Mengzhao Chen, Wenqi Shao, Peng Xu, Mingbao Lin, Kaipeng Zhang, Fei Chao, Rongrong Ji, Yu Qiao, and Ping Luo. Diffrate: Differentiable compression rate for efficient vision transformers. _arXiv preprint arXiv:2305.17997_, 2023.
* [10] Xuanyao Chen, Tianyuan Zhang, Yue Wang, Yilun Wang, and Hang Zhao. Futr3d: A unified sensor fusion framework for 3d detection. _arXiv preprint arXiv:2203.10642_, 2022.
* [11] Zehui Chen, Zhenyu Li, Shiquan Zhang, Liangji Fang, Qinhong Jiang, and Feng Zhao. Bevdistill: Cross-modal bev distillation for multi-view 3d object detection. _arXiv preprint arXiv:2211.09386_, 2022.
* [12] Zhiyu Chong, Xinzhu Ma, Hong Zhang, Yuxin Yue, Haojie Li, Zhihui Wang, and Wanli Ouyang. MonoDistill: Learning spatial features for monocular 3d object detection. _arXiv preprint arXiv:2201.10830_, 2022.
* [13] MMDetection3D Contributors. MMDetection3D: OpenMMLab next-generation platform for general 3D object detection. https://github.com/open-mmlab/mmdetection3d, 2020.
* [14] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: A large-scale hierarchical image database. In _CVPR_, 2009.
* [15] Chengjian Feng, Zequn Jie, Yujie Zhong, Xiangxiang Chu, and Lin Ma. Aedet: Azimuth-invariant multi-view 3d object detection. _arXiv preprint arXiv:2211.12501_, 2022.
* [16] Yulu Gao, Chonghao Sima, Shaoshuai Shi, Shangzhe Di, Si Liu, and Hongyang Li. Sparse dense fusion for 3d object detection. In _IROS_, 2023.
* [17] Xiaoyang Guo, Shaoshuai Shi, Xiaogang Wang, and Hongsheng Li. LIGA-Stereo: Learning lidar geometry aware representations for stereo-based 3d detector. In _ICCV_, 2021.
* [18] Chunrui Han, Jianjian Sun, Zheng Ge, Jinrong Yang, Runpei Dong, Hongyu Zhou, Weixin Mao, Yuang Peng, and Xiangyu Zhang. Exploring recurrent long-term temporal fusion for multi-view 3d perception. _arXiv preprint arXiv:2303.05970_, 2023.
* [19] Chunrui Han, Jianjian Sun, Zheng Ge, Jinrong Yang, Runpei Dong, Hongyu Zhou, Weixin Mao, Yuang Peng, and Xiangyu Zhang. Exploring recurrent long-term temporal fusion for multi-view 3d perception. _arXiv preprint arXiv:2303.05970_, 2023.
* [20] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _CVPR_, 2016.
* [21] Yihan Hu, Jiazhi Yang, Li Chen, Keyu Li, Chonghao Sima, Xizhou Zhu, Siqi Chai, Senyao Du, Tianwei Lin, Wenhai Wang, et al. Planning-oriented autonomous driving. In _CVPR_, 2023.
* [22] Junjie Huang and Guan Huang. BEVDet4D: Exploit temporal cues in multi-camera 3d object detection. _arXiv preprint arXiv:2203.17054_, 2022.
* [23] Junjie Huang, Guan Huang, Zheng Zhu, and Dalong Du. BEVDet: High-performance multi-camera 3d object detection in bird-eye-view. _arXiv preprint arXiv:2112.11790_, 2021.
* [24] Linyan Huang, Huijie Wang, Jia Zeng, Shengchuan Zhang, Liujuan Cao, Rongrong Ji, Junchi Yan, and Hongyang Li. Geometric-aware pretraining for vision-centric 3d object detection. _arXiv preprint arXiv:2304.03105_, 2023.
* [25] Peixiang Huang, Li Liu, Renrui Zhang, Song Zhang, Xinli Xu, Baichao Wang, and Guoyi Liu. Tig-bev: Multi-view bev 3d object detection via target inner-geometry learning. _arXiv preprint arXiv:2212.13979_, 2022.
* [26] Xiaosong Jia, Yulu Gao, Li Chen, Junchi Yan, Patrick Langechuan Liu, and Hongyang Li. Driveadapter: Breaking the coupling barrier of perception and planning in end-to-end autonomous driving. In _ICCV_, 2023.
* [27] Xiaosong Jia, Penghao Wu, Li Chen, Jiangwei Xie, Conghui He, Junchi Yan, and Hongyang Li. Think twice before driving: Towards scalable decoders for end-to-end autonomous driving. In _CVPR_, 2023.

* [28] Yanqin Jiang, Li Zhang, Zhenwei Miao, Xiatian Zhu, Jin Gao, Weiming Hu, and Yu-Gang Jiang. Polarformer: Multi-camera 3d object detection with polar transformers. _arXiv preprint arXiv:2206.15398_, 2022.
* [29] Marvin Klingner, Shubhankar Borse, Varun Ravi Kumar, Behnaz Rezaei, Venkatraman Narayanan, Senthil Yogamani, and Fatih Porikli. X\({}^{3}\)kd: Knowledge distillation across modalities, tasks and stages for multi-camera 3d object detection. _arXiv preprint arXiv:2303.02203_, 2023.
* [30] Alex H Lang, Sourabh Vora, Holger Caesar, Lubing Zhou, Jiong Yang, and Oscar Beijbom. PointPillars: Fast encoders for object detection from point clouds. In _CVPR_, 2019.
* [31] Hongyang Li, Chonghao Sima, Jifeng Dai, Wenhai Wang, Lewei Lu, Huijie Wang, Enze Xie, Zhiqi Li, Hanming Deng, Hao Tian, et al. Delving into the devils of bird's-eye-view perception: A review, evaluation and recipe. _arXiv preprint arXiv:2209.05324_, 2022.
* [32] Jianing Li, Ming Lu, Jiaming Liu, Yandong Guo, Li Du, and Shanghang Zhang. Bev-lgkd: A unified lidar-guided knowledge distillation framework for bev 3d object detection. _arXiv preprint arXiv:2212.00623_, 2022.
* [33] Yanwei Li, Yilun Chen, Xiaojuan Qi, Zeming Li, Jian Sun, and Jiaya Jia. Unifying voxel-based representation with transformer for 3d object detection. _arXiv preprint arXiv:2206.00630_, 2022.
* [34] Yinhao Li, Han Bao, Zheng Ge, Jinrong Yang, Jianjian Sun, and Zeming Li. Bevstereo: Enhancing depth estimation in multi-view 3d object detection with dynamic temporal stereo. _arXiv preprint arXiv:2209.10248_, 2022.
* [35] Yinhao Li, Zheng Ge, Guanyi Yu, Jinrong Yang, Zengran Wang, Yukang Shi, Jianjian Sun, and Zeming Li. BEVDepth: Acquisition of reliable depth for multi-view 3d object detection. _arXiv preprint arXiv:2206.10092_, 2022.
* [36] Zhiqi Li, Wenhai Wang, Hongyang Li, Enze Xie, Chonghao Sima, Tong Lu, Qiao Yu, and Jifeng Dai. BEVFormer: Learning bird's-eye-view representation from multi-camera images via spatiotemporal transformers. _arXiv preprint arXiv:2203.17270_, 2022.
* [37] Zhuoling Li, Chuanrui Zhang, Wei-Chiu Ma, Yipin Zhou, Linyan Huang, Haoqian Wang, SerNam Lim, and Hengshuang Zhao. Voxelformer: Bird's-eye-view feature generation based on dual-view attention for multi-view 3d object detection. _arXiv preprint arXiv:2304.01054_, 2023.
* [38] Tingting Liang, Hongwei Xie, Kaicheng Yu, Zhongyu Xia, Zhiwei Lin, Yongtao Wang, Tao Tang, Bing Wang, and Zhi Tang. BEV Fusion: A simple and robust lidar-camera fusion framework. _arXiv preprint arXiv:2205.13790_, 2022.
* [39] Yingfei Liu, Tiancai Wang, Xiangyu Zhang, and Jian Sun. PETR: Position embedding transformation for multi-view 3d object detection. _arXiv preprint arXiv:2203.05625_, 2022.
* [40] Yingfei Liu, Junjie Yan, Fan Jia, Shuailin Li, Qi Gao, Tiancai Wang, Xiangyu Zhang, and Jian Sun. PETRv2: A unified framework for 3d perception from multi-camera images. _arXiv preprint arXiv:2206.01256_, 2022.
* [41] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In _ICCV_, 2021.
* [42] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. A convnet for the 2020s. In _CVPR_, 2022.
* [43] Jinhyung Park, Chenfeng Xu, Shijia Yang, Kurt Keutzer, Kris Kitani, Masayoshi Tomizuka, and Wei Zhan. Time will tell: New outlooks and a baseline for temporal multi-view 3d object detection. _arXiv preprint arXiv:2210.02443_, 2022.
* [44] Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou, Antoine Chassang, Carlo Gatta, and Yoshua Bengio. Fitnets: Hints for thin deep nets. _arXiv preprint arXiv:1412.6550_, 2014.
* [45] Shaoshuai Shi, Li Jiang, Jiajun Deng, Zhe Wang, Chaoxu Guo, Jianping Shi, Xiaogang Wang, and Hongsheng Li. PV-RCNN++: Point-voxel feature set abstraction with local vector representation for 3d object detection. _arXiv preprint arXiv:2102.00463_, 2021.

* [46] Changyong Shu, Yifan Liu, Jianfei Gao, Zheng Yan, and Chunhua Shen. Channel-wise knowledge distillation for dense prediction. In _ICCV_, 2021.
* [47] Wenwen Tong, Chonghao Sima, Tai Wang, Li Chen, Silei Wu, Hamming Deng, Yi Gu, Lewei Lu, Ping Luo, Dahua Lin, and Hongyang Li. Scene as occupancy. In _ICCV_, 2023.
* [48] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In _NeurIPS_, 2017.
* [49] Sourabh Vora, Alex H Lang, Bassam Helou, and Oscar Beijbom. PointPainting: Sequential fusion for 3d object detection. In _CVPR_, 2020.
* [50] Huijie Wang, Tianyu Li, Yang Li, Li Chen, Chonghao Sima, Zhenbo Liu, Yuting Wang, Shengyin Jiang, Peijin Jia, Bangjun Wang, Feng Wen, Hang Xu, Ping Luo, Junchi Yan, Wei Zhang, and Hongyang Li. Openlane-v2: A topology reasoning benchmark for scene understanding in autonomous driving, 2023.
* [51] Shihao Wang, Yingfei Liu, Tiancai Wang, Ying Li, and Xiangyu Zhang. Exploring object-centric temporal modeling for efficient multi-view 3d object detection. _arXiv preprint arXiv:2303.11926_, 2023.
* [52] Tai Wang, Xinge Zhu, Jiangmiao Pang, and Dahua Lin. FCOS3D: Fully convolutional one-stage monocular 3d object detection. In _ICCV_, 2021.
* [53] Yue Wang, Vitor Campagnolo Guizilini, Tianyuan Zhang, Yilun Wang, Hang Zhao, and Justin Solomon. DETR3D: 3d object detection from multi-view images via 3d-to-2d queries. In _CoRL_, 2022.
* [54] Zengran Wang, Chen Min, Zheng Ge, Yinhao Li, Zeming Li, Hongyu Yang, and Di Huang. STS: Surround-view temporal stereo for multi-view 3d detection. _arXiv preprint arXiv:2208.10145_, 2022.
* [55] Penghao Wu, Xiaosong Jia, Li Chen, Junchi Yan, Hongyang Li, and Yu Qiao. Trajectory-guided control prediction for end-to-end autonomous driving: A simple yet strong baseline. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2022.
* [56] Kaixin Xiong, Shi Gong, Xiaoqing Ye, Xiao Tan, Ji Wan, Errui Ding, Jingdong Wang, and Xiang Bai. Cape: Camera view position embedding for multi-view 3d object detection. In _CVPR_, pages 21570-21579, June 2023.
* [57] Junjie Yan, Yingfei Liu, Jianjian Sun, Fan Jia, Shuailin Li, Tiancai Wang, and Xiangyu Zhang. Cross modal transformer via coordinates encoding for 3d object dectection. _arXiv preprint arXiv:2301.01283_, 2023.
* [58] Chenyu Yang, Yuntao Chen, Hao Tian, Chenxin Tao, Xizhou Zhu, Zhaoxiang Zhang, Gao Huang, Hongyang Li, Yu Qiao, Lewei Lu, et al. Bevformer v2: Adapting modern image backbones to bird's-eye-view recognition via perspective supervision. _arXiv preprint arXiv:2211.10439_, 2022.
* [59] Zeyu Yang, Jiaqi Chen, Zhenwei Miao, Wei Li, Xiatian Zhu, and Li Zhang. DeepInteraction: 3d object detection via modality interaction. _arXiv preprint arXiv:2208.11112_, 2022.
* [60] Tianwei Yin, Xingyi Zhou, and Philipp Krahenbuhl. Center-based 3d object detection and tracking. In _CVPR_, 2021.
* [61] Jia Zeng, Li Chen, Hanming Deng, Lewei Lu, Junchi Yan, Yu Qiao, and Hongyang Li. Distilling focal knowledge from imperfect expert for 3d object detection. In _CVPR_, 2023.
* [62] Haimei Zhao, Qiming Zhang, Shanshan Zhao, Jing Zhang, and Dacheng Tao. Bevsimdet: Simulated multi-modal distillation in bird's-eye view for multi-view 3d object detection. _arXiv preprint arXiv:2303.16818_, 2023.
* [63] Xingyi Zhou, Dequan Wang, and Philipp Krahenbuhl. Objects as points. _arXiv preprint arXiv:1904.07850_, 2019.
* [64] Yunsong Zhou, Yuan He, Hongzi Zhu, Cheng Wang, Hongyang Li, and Qinhong Jiang. Monocular 3d object detection: An extrinsic parameter free approach. In _CVPR_, 2021.
* [65] Yunsong Zhou, Yuan He, Hongzi Zhu, Cheng Wang, Hongyang Li, and Qinhong Jiang. Monoef: Extrinsic parameter free monocular 3d object detection. _IEEE TPAMI_, 2022.

Experiment Details

### Dataset and Evaluation Metrics

We conduct our experiments on the nuScenes dataset [3], a widely used benchmark for autonomous driving tasks. The dataset encompasses diverse driving scenarios captured using cameras and LiDAR sensors, offering rich information for both visual and LiDAR-based 3D object detection. The dataset comprises 700 training scenes, 150 validation scenes, and 150 testing scenes. Each scene spans approximately 20 seconds, with key frames annotated at a 2 Hz frequency.

The two dominant metrics for the nuScenes detection task are the nuScenes Detection Score (NDS) and mean Average Precision (mAP). The mAP for nuScenes is computed based on the center distance between predictions and ground truth annotations on the ground plane. Moreover, the nuScenes dataset defines five true positive metrics (mATE, mASE, mAOE, mAVE, mAAE) for measuring translation, scale, orientation, velocity, and attribute, respectively. The NDS for nuScenes is a weighted sum of mAP and the five true positive metrics, defined as \(NDS=\frac{1}{10}[5mAP+\sum_{mTP}(1-\min(1,mTP))]\).

### Implementation Details

We conduct experiments on BEVDepth [35]. The codebase is developed upon MMDetection3D [13]. Main experiments are trained on 8 NVIDIA A100 GPUs, while ablation experiments are conducted on 8 NVIDIA V100 GPUS. For BEVDepth, the model is trained for 20 epochs with an initial learning rate of 2e-4. In the distillation process, the per-GPU batch size is set to 4, whereas during the training of the baseline model, it is set to 8. Normal data augmentations are introduced in the training process such as flip and rotate. In our apprentice models, future frames are not incorporated into the long-term temporal fusion throughout the training phase to ensure a fair comparison. As for ablation study, we conduct experiments with an online training strategy, and we have employed the ResNet-50 configuration in the absence of CBGS.

In our research, we implement distinct temporal modeling strategies for both apprentice and expert models. For the apprentice models, we incorporate a sequence of eight frames into the temporal modeling process. In contrast, the expert models integrate four future frames into the temporal modeling as demonstrated in our primary results. However, in our ablation study, we deviate from this approach and instead employ eight historical frames for temporal modeling. Besides, We use the last sweep LiDAR frame with the current LiDAR frame to create the depth map.

### Experiments Settings

The setting of adopted expert-apprentice pairs is depicted in Tab. 11. We categorize the distillation setting into two distinct groups. The primary group is engaged in training on the main results, whereas the second group is utilized for the ablation study. Both our baseline and VCD-A adopt 8 history frames for temporal fusion.

\begin{table}
\begin{tabular}{c|c|c|c|c|c} \hline \hline Method & Backbone & Image Size & Frames & mAP (\%) & NDS (\%) \\ \hline VCD-E & ConvNext-B [42] & 512 \(\times\) 1408 & 8+1 & 67.7 & 71.1 \\ VCD-A & Res-50 [20] & 256 \(\times\) 704 & 8+1 & 41.8 & 54.2 \\ \hline VCD-E\({}^{*}\) & ConvNext-B [42] & 256 \(\times\) 704 & 8+1 & 54.2 & 58.8 \\ VCD-A & Res-50 [20] & 256 \(\times\) 704 & 8+1 & 29.7 & 40.9 \\ \hline \hline \end{tabular}
\end{table}
Table 11: Experiment settings. \({}^{*}\) denotes that the training schedule for VCD-E is approximately one-fourth of the original schedule. This reduction was implemented to expedite the training process during the ablation study. The first group is engaged in training on the main results, whereas the second group is utilized in the ablation study.

## Appendix B The Analysis of Temporal Fusion

### The Misalignment of Motion Objects

As highlighted in preceding studies [43], long-term temporal fusion may face misalignment issues in motion estimation, which can be discerned through a reduction in performance on metrics like mATE. Let's consider a moving object and analyze the impact of inaccurate motion estimation on its position in the fused frame. We will assume that the environment is static, except for the moving object. Let the position of the moving object in the world coordinate system be represented by \(\bm{P}_{i}^{w}=(x_{i}^{w},y_{i}^{w},z_{i}^{w},1)^{T}\) in each of the \(N\) frames captured at times \(t_{1},t_{2},\dots,t_{N}\). The actual motion of the moving object between frames is represented by \(\bm{M}_{i}^{obj}\), and the estimated motion is represented by \(\bm{\hat{M}_{i}^{obj}}\). The difference between the estimated and actual motion of the object can be denoted as:

\[\Delta\bm{M_{i}}^{obj}=\bm{M_{i}}^{obj}-\bm{\hat{M}_{i}}^{obj}.\] (7)

As we have already computed the transformation matrix \(\bm{T}_{i}\) based on the estimated ego motion, we can calculate the transformed object position in the current frame, considering its actual motion, as:

\[\bm{P}_{i}^{w^{\prime}}=\bm{T}_{i}\times\bm{M}_{i}^{obj}\times\bm{P}_{i}^{w}.\] (8)

The error in the transformed object position can be computed as:

\[\bm{e}_{i}^{obj}=\bm{P}_{i}^{w^{\prime}}-\bm{\hat{P}_{i}}^{w^{\prime}}.\] (9)

In the long-term fusion process, we integrate the information from all \(N\) frames. Assuming we use a fusion function \(F\), the fused position in the current frame can be represented as:

\[\bm{P}_{fusion}^{obj}=F(\bm{P}_{1}^{w^{\prime}},\bm{P}_{2}^{w^{\prime}}.\dots,\bm{P}_{N}^{w^{\prime}}).\] (10)

The inaccuracies in the motion estimation of the moving object for each frame can propagate through the fusion function and result in a misaligned object in the fused frame. The overall error in the fused position can be represented as a function of the errors in each frame:

\[\bm{e}_{fusion}^{obj}=G(\bm{e}_{1}^{obj},\bm{e}_{2}^{obj}.\dots,\bm{e}_{N}^ {obj}),\] (11)

where \(G\) represents a function that combines the errors from each frame. The fused position of the moving object will be less accurate due to these motion estimation errors, leading to a decline in object detection performance in the long-term setting. To address the issue mentioned earlier, we introduce the trajectory-based distillation module, which compensates for the misalignment of moving objects. We will provide further details in the subsequent discussion.

### The Improvements of Static and Dynamic Objects

In this section, we present visualizations to demonstrate the improvements achieved in dynamic objects. Particularly noteworthy is the significant enhancement in the representation of dynamic objects through trajectory-based distillation, thereby highlighting the effectiveness of the trajectory-based module. As depicted in Fig. 3 and Fig. 4, our distillation framework consistently enhances static and dynamic objects across various metrics.

Figure 3: Effects of VCD on static objects. Our distillation framework VCD can still consistently improves static objects, demonstrating 3.2% and 7.6% improvements in precision-recall (mAP) and localization quality (mATE), respectively.

## Appendix C Visualization

We have performed several visualizations in Fig. 5 to showcase the advancements achieved by our distillation framework. Our findings indicate that our models excel in accurately predicting 3D bounding boxes for the target objects.

## Appendix D Broader Impact

Our research introduces a novel perspective for multi-modal methodologies and a fresh distillation paradigm for camera-only techniques. We believe that it can establish a robust baseline for the broader scientific community. However, while our methods contribute to the enhancement of autonomous driving, they are not yet capable of addressing more complex corner cases. Consequently, these limitations could potentially introduce risks in real-world autonomous systems.

Figure 4: Effects of VCD on movable objects. Our distillation framework VCD consistently improves dynamic objects across a range of metrics.

Figure 5: Visualization of the predictions for 3D object detection generated by the VCD-A.