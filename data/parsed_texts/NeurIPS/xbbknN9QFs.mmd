# On Evaluating Adversarial Robustness of

Large Vision-Language Models

 Yunqing Zhao\({}^{*1}\), Tianyu Pang\({}^{*}\)\({}^{1}\)\({}^{2}\), Chao Du\({}^{1}\)\({}^{2}\), Xiao Yang\({}^{3}\), Chongxuan Li\({}^{4}\),

**Ngai-Man Cheung\({}^{1}\)\({}^{1}\), Min Lin\({}^{2}\)**

\({}^{1}\)Singapore University of Technology and Design

\({}^{2}\)Sea AI Lab, Singapore

\({}^{3}\)Tsinghua University \({}^{4}\)Rennin University of China

{zhaoyq, tianyupang, duchao, linmin}@sea.com;

yangxiao19@tsinghua.edu.cn; chongxuanli@ruc.edu.cn; ngaiman_cheung@sutd.edu.sg

Equal contribution. Work done during Yunqing Zhao's internship at Sea AI Lab.Correspondence to Tianyu Pang, Chao Du, and Ngai-Man Cheung.

###### Abstract

Large vision-language models (VLMs) such as GPT-4 have achieved unprecedented performance in response generation, especially with visual inputs, enabling more creative and adaptable interaction than large language models such as ChatGPT. Nonetheless, multimodal generation exacerbates safety concerns, since adversaries may successfully evade the entire system by subtly manipulating the most vulnerable modality (e.g., vision). To this end, we propose evaluating the robustness of open-source large VLMs in the most realistic and high-risk setting, where adversaries have only _black-box_ system access and seek to deceive the model into returning the _targeted_ responses. In particular, we first craft targeted adversarial examples against pretrained models such as CLIP and BLIP, and then transfer these adversarial examples to other VLMs such as MiniGPT-4, LLaVA, UniDiffuser, BLIP-2, and Img2Prompt. In addition, we observe that black-box queries on these VLMs can further improve the effectiveness of targeted evasion, resulting in a surprisingly high success rate for generating targeted responses. Our findings provide a quantitative understanding regarding the adversarial vulnerability of large VLMs and call for a more thorough examination of their potential security flaws before deployment in practice. Our project page: yunqing-me.github.io/AttackVLM/.

## 1 Introduction

Large vision-language models (VLMs) have enjoyed tremendous success and demonstrated promising capabilities in text-to-image generation [55, 68, 72], image-grounded text generation (e.g., image captioning or visual question-answering) [2, 15, 42, 86], and joint generation [5, 32, 98] due to an increase in the amount of data, computational resources, and number of model parameters. Notably, after being finetuned with instructions and aligned with human feedback, GPT-4 [58] is capable of conversing with human users and, in particular, supports visual inputs.

Along the trend of multimodal learning, an increasing number of large VLMs are made publicly available, enabling the exponential expansion of downstream applications. However, this poses significant safety challenges. It is widely acknowledged, for instance, that text-to-image models could be exploited to generate fake content [71, 76] or edit images maliciously [73]. A silver lining is that adversaries must manipulate _textual inputs_ to achieve their evasion goals, necessitating extensive search and engineering to determine the adversarial prompts. Moreover, text-to-image models that areaccessible to the public typically include a safety checker to filter sensitive concepts and an invisible watermarking module to help identify fake content [69; 72; 108].

Image-grounded text generation such as GPT-4 is more interactive with human users and can produce commands to execute codes [28] or control robots [88], as opposed to text-to-image generation which only returns an image. Accordingly, potential adversaries may be able to evade an image-grounded text generative model by manipulating its _visual inputs_, as it is well-known that the vision modality is extremely vulnerable to human-imperceptible adversarial perturbations [8; 22; 29; 81]. This raises even more serious safety concerns, as image-grounded text generation may be utilized in considerably complex and safety-critical environments [62].1 Adversaries may mislead large VLMs deployed as plugins, for example, to bypass their safety/privacy checkers, inject malicious code, or access APIs and manipulate robots/devices without authorization.

Footnote 1: Note that GPT-4 delays the release of its visual inputs due to safety concerns [3].

In this work, we empirically evaluate the adversarial robustness of state-of-the-art _large_ VLMs, particularly against those that accept visual inputs (e.g., image-grounded text generation or joint generation). To ensure reproducibility, our evaluations are all based on open-source large models. We examine the most realistic and high-risk scenario, in which adversaries have only _black-box_ system access and seek to deceive the model into returning the _targeted_ responses. Specifically, we first use pretrained CLIP [65; 80] and BLIP [41] as surrogate models to craft targeted adversarial examples, either by matching textual embeddings or image embeddings, and then we transfer the adversarial examples to other large VLMs, including MiniGPT-4 [109], LLaVA [46], UniDiffuser [5], BLIP-2 [42], and Img2Prompt [30]. Surprisingly, these transfer-based attacks can already induce targeted responses with a high success rate. In addition, we discover that query-based attacks employing transfer-based priors can further improve the efficacy of targeted evasion against these VLMs, as shown in Figure 1 (BLIP-2), Figure 2 (UniDiffuser), and Figure 3 (MiniGPT-4).

Our findings provide a quantitative understanding regarding the adversarial vulnerability of large VLMs and advocate for a more comprehensive examination of their potential security defects prior to deployment, as discussed in Sec. 5. Regarding more general multimodal systems, our findings indicate that the robustness of systems is highly dependent on their most vulnerable input modality.

## 2 Related work

**Language models (LMs) and their robustness.** The seminal works of BERT [21], GPT-2 [64], and T5 [66] laid the foundations of large LMs, upon which numerous other large LMs have been developed

Figure 1: **Image captioning task implemented by BLIP-2. Given an original text description (e.g., an armchair in the shape of an avocado), DALL-E [67] is used to generate corresponding clean images. BLIP-2 accurately returns captioning text (e.g., a stuffed chair in the shape of an avocado) that analogous to the original text description on the clean image. After the clean image is maliciously perturbed by targeted adversarial noises, the adversarial image can mislead BLIP-2 to return a caption (e.g., a pencil drawing of sports car is shown) that semantically resembles the predefined targeted response (e.g., a hand drawn sketch of a Porsche 911). More examples such as attacking real-world image-text pairs are provided in our Appendix.**

and demonstrated significant advancements across various language benchmarks [10; 19; 31; 74; 79; 107]. More recently, ChatGPT [57; 59] and several open-source models [18; 95; 83] tuned based on LLaMA [85] enable conversational interaction with human users and can respond to diverse and complex questions. Nevertheless, Alzantot et al. [4] first construct adversarial examples on sentiment analysis and textual entailment tasks, while Jin et al. [36] report that BERT can be evaded through natural language attacks. Later, various flexible (e.g., beyond word replacement) and semantically preserving methods are proposed to produce natural language adversarial examples [9; 49; 50; 52; 53; 70; 78; 102; 104; 110], as well as benchmarks and datasets to more thoroughly evaluate the adversarial robustness of LMs [90; 56; 91; 92]. There are also red-teaming initiatives that use human-in-the-loop or automated frameworks to identify problematic language model outputs [96; 27; 63].

**Vision-language models (VLMs) and their robustness.** The knowledge contained within these powerful LMs is used to facilitate vision-language tasks [93; 26; 101; 84; 26]. Inspired by the adversarial vulnerability observed in vision tasks, early efforts are devoted to investigating adversarial attacks against visual question answering [105; 105; 6; 11; 37; 38; 43; 77; 89], with the majority of these efforts focusing on conventional CNN-RNN-based models, assuming white-box access or untargeted adversarial goals, and requiring human interaction. Our research, on the other hand, examines the adversarial robustness of advanced large VLMs, assuming black-box access and targeted adversarial goals, and providing quantitative evaluations free of human labor.

## 3 Methodology

In this section, we will first introduce the fundamental preliminary, and then describe the transfer-based and query-based attacking strategies against image-grounded text generation, respectively.

### Preliminary

We denote \(p_{\theta}(\bm{x};\bm{c}_{\text{in}})\mapsto\bm{c}_{\text{out}}\) as an image-grounded text generative model parameterized by \(\theta\), where \(\bm{x}\) is the input image, \(\bm{c}_{\text{in}}\) is the input text, and \(\bm{c}_{\text{out}}\) is the output text. In image captioning tasks, for instance, \(\bm{c}_{\text{in}}\) is a placeholder \(\emptyset\) and \(\bm{c}_{\text{out}}\) is the caption; in visual question answering tasks, \(\bm{c}_{\text{in}}\) is the question and \(\bm{c}_{\text{out}}\) is the answer. Note that here we slightly abuse the notations since the mapping between \(p_{\theta}(\bm{x};\bm{c}_{\text{in}})\) and \(\bm{c}_{\text{out}}\) could be probabilistic or non-deterministic [98; 5].

**Threat models.** We overview threat models that specify adversarial conditions [12] and adapt them to generative paradigms: (i) _adversary knowledge_ describes what knowledge the adversary is assumed

Figure 2: **Joint generation task implemented by UniDiffuser.** There are generative VLMs such as UniDiffuser that model the joint distribution of image-text pairs and are capable of both image-to-text and text-to-image generation. Consequently, given an original text description (e.g., A Van Gogh style painting of an American football player), the text-to-image direction of UniDiffuser is used to generate the corresponding clean image, and its image-to-text direction can recover a text response (e.g., A painting of Packers quarterback football player on a blue background) similar to the original text description. The recovering between image and text modalities can be performed consistently on clean images. When a targeted adversarial perturbation is added to a clean image, however, the image-to-text direction of UniDiffuser will return a text (e.g., A man in an astronaut suit riding a horse on the moon) that semantically resembles the predefined targeted description (e.g., A photo of an astronaut riding a horse on the moon), thereby affecting the subsequent chains of recovering processes.

to have, typically either white-box access with full knowledge of \(p_{\theta}\) including model architecture and weights, or varying degrees of black-box access, e.g., only able to obtain the output text \(\bm{c}_{\text{out}}\) from an API; (ii) _adversary goals_ describe the malicious purpose that the adversary seeks to achieve, including untargeted goals that simply cause \(\bm{c}_{\text{out}}\) to be a wrong caption or answer, and targeted goals that cause \(\bm{c}_{\text{out}}\) to match a predefined targeted response \(\bm{c}_{\text{tar}}\) (measured via text-matching metrics); (iii) _adversary capabilities_ describe the constraints on what the adversary can manipulate to cause harm, with the most commonly used constraint being imposed by the \(\ell_{p}\) budget, namely, the \(\ell_{p}\) distance between the clean image \(\bm{x}_{\text{cle}}\) and the adversarial image \(\bm{x}_{\text{adv}}\) is less than a budget \(\epsilon\) as \(\|\bm{x}_{\text{cle}}-\bm{x}_{\text{adv}}\|_{p}\leq\epsilon\).

**Remark.** Our work investigates the most realistic and challenging threat model, where the adversary has black-box access to the victim models \(p_{\theta}\), a targeted goal, a small perturbation budget \(\epsilon\) on the input image \(\bm{x}\) to ensure human imperceptibility, and is forbidden to manipulate the input text \(\bm{c}_{\text{in}}\).

Figure 3: **Visual question-answering (VQA) task implemented by MiniGPT-4. MiniGPT-4 has capabilities for vision-language understanding and performs comparably to GPT-4 on tasks such as multi-round VQA by leveraging the knowledge of large LMs. We select images with refined details generated by Midiourney [51] and feed questions (e.g., Can you tell me what is the interesting point of this image?) into MiniGPT-4. As expected, MiniGPT-4 can return descriptions that are intuitively reasonable, and when we ask additional questions (e.g., But is this a common scene in the normal life?), MiniGPT-4 demonstrates the capacity for accurate multi-round conversation. Nevertheless, after being fed targeted adversarial images, MiniGPT-4 will return answers related to the targeted description (e.g., A robot is playing in the field). This adversarial effect can even affect multi-round conversations when we ask additional questions. More examples of attacking MiniGPT-4 or LLaVA on VQA are provided in our Appendix.**

### Transfer-based attacking strategy

Since we assume black-box access to the _victim_ models, a common attacking strategy is transfer-based [22, 23, 47, 61, 94, 100], which relies on _surrogate_ models (e.g., a publicly accessible CLIP model) to which the adversary has white-box access and crafts adversarial examples against them, then feeds the adversarial examples into the victim models (e.g., GPT-4 that the adversary seeks to fool). Due to the fact that the victim models are vision-and-language, we select an image encoder \(f_{\phi}(\bm{x})\) and a text encoder \(g_{\psi}(\bm{c})\) as surrogate models, and we denote \(\bm{c}_{\text{tar}}\) as the targeted response that the adversary expects the victim models to return. Two approaches of designing transfer-based adversarial objectives are described in the following.

**Matching image-text features (MF-it).** Since the adversary expects the victim models to return the targeted response \(\bm{c}_{\text{tar}}\) when the adversarial image \(\bm{x}_{\text{adv}}\) is the input, it is natural to match the features of \(\bm{c}_{\text{tar}}\) and \(\bm{x}_{\text{adv}}\) on surrogate models, where \(\bm{x}_{\text{adv}}\) should satisfy2

Footnote 2: We slightly abuse the notations by using \(\bm{x}_{\text{adv}}\) to represent both the variable and the optimal solution.

\[\operatorname*{arg\,max}_{\|\bm{x}_{\text{adv}}-\bm{x}_{\text{adv}}\|_{p} \leq\epsilon}f_{\phi}(\bm{x}_{\text{adv}})^{\top}g_{\psi}(\bm{c}_{\text{tar}}).\] (1)

Here, we use blue color to highlight white-box accessibility (i.e., can directly obtain gradients of \(f_{\phi}\) and \(g_{\psi}\) through backpropagation), the image and text encoders are chosen to have the same output dimension, and their inner product indicates the cross-modality similarity of \(\bm{c}_{\text{tar}}\) and \(\bm{x}_{\text{adv}}\). The constrained optimization problem in Eq. (1) can be solved by projected gradient descent (PGD) [48].

**Matching image-image features (MF-ii).** While aligned image and text encoders have been shown to perform well on vision-language tasks [65], recent research suggests that VLMs may behave like bags-of-words [103] and therefore may not be dependable for optimizing cross-modality similarity. Given this, an alternative approach is to use a public text-to-image generative model \(h_{\xi}\) (e.g., Stable

Figure 4: **Pipelines of our attacking strategies. In the _upper-left_ panel, we illustrate our transfer-based strategy for matching image-image features (MF-ii) as formulated in Eq. (2). We select a targeted text \(\bm{c}_{\text{tar}}\) (e.g., A sea otter with a pearl earring) and then use a pretrained text-to-image generator \(h_{\xi}\) to produce a targeted image \(h_{\xi}(\bm{c}_{\text{tar}})\). The targeted image is then fed to the image encoder \(f_{\phi}\) to obtain the embedding \(f_{\phi}(h_{\xi}(\bm{c}_{\text{tar}}))\). Here we refer to adversarial examples generated by transfer-based strategies as \(\bm{x}_{\text{trans}}=\bm{x}_{\text{cle}}+\Delta\), while adversarial noise is denoted by \(\Delta\). We feed \(\bm{x}_{\text{trans}}\) into the image encoder to obtain the adversarial embedding \(f_{\phi}(\bm{x}_{\text{trans}})\), and then we optimize the adversarial noise \(\Delta\) to maximize the similarity metric \(f_{\phi}(\bm{x}_{\text{trans}})^{\top}f_{\phi}(h_{\xi}(\bm{c}_{\text{tar}}))\). In the _upper-right_ panel, we demonstrate our query-based strategy for matching text-text features (MF-it), as defined by Eq. (3). We apply the resulted transfer-based adversarial example \(\bm{x}_{\text{tray}}\) to initialize \(\bm{x}_{\text{adv}}\), then sample \(N\) random perturbations and add them to \(\bm{x}_{\text{adv}}\) to build \(\{\bm{x}_{\text{adv}}+\bm{\delta}_{n}\}_{n=1}^{N}\). These randomly perturbed adversarial examples are fed into the victim model \(p_{\theta}\) (with the input text \(\bm{c}_{\text{in}}\) unchanged) and the RGF method described in Eq. (4) is used to estimate the gradients \(\nabla_{\bm{x}_{\text{adv}}}g_{\psi}(p_{\theta}(\bm{x}_{\text{adv}};\bm{c}_{ \text{in}}))^{\top}g_{\psi}(\bm{c}_{\text{tar}})\). In the _bottom_, we present the final results of our method’s (MF-ii + MF-tt) targeted response generation.

Diffusion [72]) and generate a targeted image corresponding to \(\bm{c}_{\text{tar}}\) as \(h_{\xi}(\bm{c}_{\text{tar}})\). Then, we match the image-image features of \(\bm{x}_{\text{adv}}\) and \(h_{\xi}(\bm{c}_{\text{tar}})\) as

\[\operatorname*{arg\,max}_{\|\bm{x}_{\text{adv}}-\bm{x}_{\text{adv}}\|_{p}\leq \epsilon}f_{\phi}(\bm{x}_{\text{adv}})^{\top}f_{\phi}(h_{\xi}(\bm{c}_{\text{ tar}})),\] (2)

where orange color is used to emphasize that only black-box accessibility is required for \(h_{\xi}\), as gradient information of \(h_{\xi}\) is not required when optimizing the adversarial image \(\bm{x}_{\text{adv}}\). Consequently, we can also implement \(h_{\xi}\) using advanced APIs such as Midjourney [51].

### Query-based attacking strategy

Transfer-based attacks are effective, but their efficacy is heavily dependent on the similarity between the victim and surrogate models. When we are allowed to repeatedly query victim models, such as by providing image inputs and obtaining text outputs, the adversary can employ a query-based attacking strategy to estimate gradients or execute natural evolution algorithms [7; 16; 34].

**Matching text-text features (MF-tt).** Recall that the adversary goal is to cause the victim models to return a targeted response, namely, matching \(p_{\theta}(\bm{x}_{\text{adv}};\bm{c}_{\text{in}})\) with \(\bm{c}_{\text{tar}}\). Thus, it is straightforward to maximize the textual similarity between \(p_{\theta}(\bm{x}_{\text{adv}};\bm{c}_{\text{in}})\) and \(\bm{c}_{\text{tar}}\) as

\[\operatorname*{arg\,max}_{\|\bm{x}_{\text{adv}}-\bm{x}_{\text{adv}}\|_{p}\leq \epsilon}g_{\psi}(p_{\theta}(\bm{x}_{\text{adv}};\bm{c}_{\text{in}}))^{\top}g _{\psi}(\bm{c}_{\text{tar}}).\] (3)

Note that we cannot directly compute gradients for optimization in Eq. (3) because we assume black-box access to the victim models \(p_{\theta}\) and cannot perform backpropagation. To estimate the gradients, we employ the random gradient-free (RGF) method [54]. First, we rewrite a gradient as the expectation of direction derivatives, i.e., \(\nabla_{\bm{x}}F(\bm{x})=\mathbb{E}\left[\bm{\delta}^{\top}\nabla_{\bm{x}}F( \bm{x})\cdot\bm{\delta}\right]\), where \(F(\bm{x})\) represents any differentiable function and \(\bm{\delta}\sim P(\bm{\delta})\) is a random variable satisfying that \(\mathbb{E}[\bm{\delta}\bm{\delta}^{\top}]=\mathbf{I}\) (e.g., \(\bm{\delta}\) can be uniformly sampled from a hypersphere). Then by zero-order optimization [16], we know that

\[\nabla_{\bm{x}_{\text{adv}}}g_{\psi}(p_{\theta}(\bm{x}_{\text{adv }};\bm{c}_{\text{in}}))^{\top}g_{\psi}(\bm{c}_{\text{tar}})\] (4) \[\approx \frac{1}{N\sigma}\sum_{n=1}^{N}\left[g_{\psi}(p_{\theta}(\bm{x}_ {\text{adv}}+\sigma\bm{\delta}_{n};\bm{c}_{\text{in}}))^{\top}g_{\psi}(\bm{c} _{\text{tar}})-g_{\psi}(p_{\theta}(\bm{x}_{\text{adv}};\bm{c}_{\text{in}}))^{ \top}g_{\psi}(\bm{c}_{\text{tar}})\right]\cdot\bm{\delta}_{n},\]

where \(\bm{\delta}_{n}\sim P(\bm{\delta})\), \(\sigma\) is a hyperparameter controls the sampling variance, and \(N\) is the number of queries. The approximation in Eq. (4) becomes an unbiased equation when \(\sigma\to 0\) and \(N\to\infty\).

**Remark.** Previous research demonstrates that transfer-based and query-based attacking strategies can work in tandem to improve black-box evasion effectiveness [17; 24]. In light of this, we also consider

\begin{table}
\begin{tabular}{l|c c c c|c c} \hline \hline \multirow{2}{*}{Model} & \multicolumn{2}{c}{Clean image} & \multicolumn{2}{c|}{Adversarial image} & \multicolumn{2}{c}{Time to obtain a single \(\bm{x}_{\text{adv}}\)} \\  & \(\bm{x}_{\text{adv}}\) & \(h_{\xi}(\bm{c}_{\text{tar}})\) & MF-ii & MF-it & MF-ii & MF-it \\ \hline CLIP (RN50) [65] & 0.094 & 0.261 & 0.239 & **0.576** & 0.543 & 0.532 \\ CLIP (ViT-B/32) [65] & 0.142 & 0.313 & 0.302 & **0.570** & 0.592 & 0.588 \\ BLIP (ViT) [41] & 0.138 & 0.286 & 0.277 & **0.679** & 0.641 & 0.634 \\ BLIP-2 (ViT) [42] & 0.037 & 0.302 & 0.294 & **0.502** & 0.855 & 0.852 \\ ALBEF (ViT) [40] & 0.063 & 0.098 & 0.091 & **0.451** & 0.750 & 0.749 \\ \hline \hline \end{tabular}
\end{table}
Table 1: **White-box attacks against surrogate models.** We craft adversarial images \(\bm{x}_{\text{adv}}\) using MF-it in Eq. (1) or MF-ii in Eq. (2), and report the CLIP score (\(\uparrow\)) between the images and the predefined targeted text \(\bm{c}_{\text{tar}}\) (randomly chosen sentences). Here the clean images consist of real-world \(\bm{x}_{\text{cle}}\) that is irrelevant to the chosen targeted text and \(h_{\xi}(\bm{c}_{\text{tar}})\) generated by a text-to-image model (e.g., Stable Diffusion [72]) conditioned on the targeted text \(\bm{c}_{\text{tar}}\). We observe that MF-ii induces a similar CLIP score compared to the generated image \(h_{\xi}(\bm{c}_{\text{tar}})\), while MF-it induces a even higher CLIP score by directly matching cross-modality features. Furthermore, we note that the attack is time-efficient, and we provide the average time (in seconds) for each strategy to craft a single \(\bm{x}_{\text{adv}}\). The results in this table validate the effectiveness of white-box attacks against surrogate models, whereas Table 2 investigates the transferability of crafted \(\bm{x}_{\text{adv}}\) to evade large VLMs (e.g., MiniGPT-4).

the adversarial examples generated by transfer-based methods to be an initialization (or prior-guided) and use the information obtained from query-based methods to strengthen the adversarial effects. This combination is effective, as empirically verified in Sec. 4 and intuitively illustrated in Figure 4.

## 4 Experiment

In this section, we demonstrate the effectiveness of our techniques for crafting adversarial examples against open-source, large VLMs. More results are provided in the Appendix.

### Implementation details

In this paper, we evaluate open-source (to ensure reproducibility) and advanced large VLMs, such as **UniDiffuser**[5], which uses a diffusion-based framework to jointly model the distribution of image-text pairs and can perform both image-to-text and text-to-image generation; **BLIP**[41] is a unified vision-language pretraining framework for learning from noisy image-text pairs; **BLIP-2**[42] adds a querying transformer [87] and a large LM (T5 [66]) to improve the image-grounded text generation; **Img2Prompt**[30] proposes a plug-and-play, LM-agnostic module that provides large

\begin{table}
\begin{tabular}{l l|c c c c c|c c} \hline \hline \multirow{2}{*}{VLM model} & \multirow{2}{*}{Attacking method} & \multicolumn{6}{c|}{Text encoder (pretrained) for evaluation} & \multicolumn{2}{c}{Other info.} \\  & & RN50 & RN101 & ViT-B/16 & ViT-B/32 & ViT-L/14 & Ensemble & \# Param. Res. \\ \hline \multirow{4}{*}{BLIP [41]} & Clean image & 0.472 & 0.456 & 0.479 & 0.499 & 0.344 & 0.450 & \multirow{4}{*}{224M} & \multirow{4}{*}{384} \\  & MF-it & 0.492 & 0.474 & 0.520 & 0.546 & 0.384 & 0.483 & & \\  & MF-ii & 0.766 & 0.753 & 0.774 & 0.786 & 0.696 & 0.755 & & \\  & MF-ii + MF-it & **0.855** & **0.841** & **0.861** & **0.868** & **0.803** & **0.846** & & \\ \hline \multirow{4}{*}{UniDiffuser [5]} & Clean image & 0.417 & 0.415 & 0.429 & 0.446 & 0.305 & 0.402 & \multirow{4}{*}{1.4B} & \multirow{4}{*}{224} \\  & MF-it & 0.655 & 0.639 & 0.678 & 0.698 & 0.611 & 0.656 & & \\  & MF-ii & 0.709 & 0.695 & 0.721 & 0.733 & 0.637 & 0.700 & & \\  & MF-ii + MF-it & **0.754** & **0.736** & **0.761** & **0.777** & **0.689** & **0.743** & & \\ \hline \multirow{4}{*}{Img2Prompt [30]} & Clean image & 0.487 & 0.464 & 0.493 & 0.515 & 0.350 & 0.461 & \multirow{4}{*}{1.7B} & \multirow{4}{*}{384} \\  & MF-it & 0.499 & 0.472 & 0.501 & 0.525 & 0.355 & 0.470 & & \\  & MF-ii & 0.502 & 0.479 & 0.505 & 0.529 & 0.366 & 0.476 & & \\  & MF-ii + MF-it & **0.803** & **0.783** & **0.809** & **0.828** & **0.733** & **0.791** & & \\ \hline \multirow{4}{*}{BLIP-2 [42]} & Clean image & 0.473 & 0.454 & 0.483 & 0.503 & 0.349 & 0.452 & \multirow{4}{*}{3.7B} & \multirow{4}{*}{224} \\  & MF-it & 0.492 & 0.474 & 0.520 & 0.546 & 0.384 & 0.483 & & \\  & MF-ii & 0.562 & 0.541 & 0.571 & 0.592 & 0.449 & 0.543 & & \\  & MF-ii + MF-it & **0.656** & **0.633** & **0.665** & **0.681** & **0.555** & **0.638** & & \\ \hline \multirow{4}{*}{LLaVA [46]} & Clean image & 0.383 & 0.436 & 0.402 & 0.437 & 0.281 & 0.388 & \multirow{4}{*}{13.3B} & \multirow{4}{*}{224} \\  & MF-it & 0.389 & 0.441 & 0.417 & 0.452 & 0.288 & 0.397 & & \\  & MF-ii & 0.396 & 0.440 & 0.421 & 0.450 & 0.292 & 0.400 & & \\  & MF-ii + MF-it & **0.548** & **0.559** & **0.563** & **0.590** & **0.448** & **0.542** & & \\ \hline \multirow{4}{*}{MiniGPT-4 [109]} & Clean image & 0.422 & 0.431 & 0.436 & 0.470 & 0.326 & 0.417 & \multirow{4}{*}{14.1B} & \multirow{4}{*}{224} \\  & MF-it & 0.472 & 0.450 & 0.461 & 0.484 & 0.349 & 0.443 & \\ \cline{1-1}  & MF-ii & 0.525 & 0.541 & 0.542 & 0.572 & 0.430 & 0.522 & & \\ \cline{1-1}  & MF-ii + MF-it & **0.633** & **0.611** & **0.631** & **0.668** & **0.528** & **0.614** & & \\ \hline \hline \end{tabular}
\end{table}
Table 2: **Black-box attacks against victim models.** We sample clean images \(\bm{x}_{\text{cle}}\) from the ImageNet-1K validation set and randomly select a target text \(\bm{c}_{\text{tar}}\) from MS-COCO captions for each clean image. We report the CLIP score (\(\uparrow\)) between the generated responses of input images (i.e., clean images \(\bm{x}_{\text{de}}\) or \(\bm{x}_{\text{adv}}\) crafted by our attacking methods MF-it, MF-ii, and the combination of MF-ii + MF-it) and predefined targeted texts \(\bm{c}_{\text{tar}}\), as computed by various CLIP text encoders and their ensemble/average. The default textual input \(\bm{c}_{\text{in}}\) is fixed to be “what is the content of this image?”. Pretrained image/text encoders such as CLIP are used as surrogate models for MF-it and MF-ii. For reference, we also report other information such as the number of parameters and input resolution of victim models.

LM prompts to enable zero-shot VQA tasks; **MiniGPT-4**[109] and **LLaVA**[46] have recently scaled up the capacity of large LMs and leveraged Vicuna-13B [18] for image-grounded text generation tasks. We note that MiniGPT-4 also exploits a high-quality, well-aligned dataset to further finetune the model with a conversation template, resulting in performance comparable to GPT-4 [58].

**Datasets.** We use the validation images from ImageNet-1K [20] as clean images, from which adversarial examples are crafted, to quantitatively evaluate the adversarial robustness of large VLMs. From MS-COCO captions [44], we randomly select a text description (usually a complete sentence, as shown in our Appendix) as the adversarially targeted text for each clean image. Because we cannot easily find a corresponding image of a given, predefined text, we use Stable Diffusion [72] for the text-to-image generation to obtain the targeted images of each text description, in order to simulate the real-world scenario. Midjourney [51] and DALL-E [67; 68] are also used in our experiments to generate the targeted images for demonstration.

**Basic setups.** For fair comparison, we strictly adhere to previous works [109; 41; 42; 5; 30; 46] in the selection of pretrained weights for image-grounded text generation, including large LMs (e.g., T5 [66] and Vicuna-13B [18] checkpoints). We experiment on the original clean images of various resolutions (see Table 2). We set \(\epsilon=8\) and use \(\ell_{\infty}\) constraint by default as \(\|\bm{x}_{\text{cle}}-\bm{x}_{\text{adv}}\|_{\infty}\leq\)8, which is the most commonly used setting in the adversarial literature [12], to ensure that the adversarial perturbations are visually imperceptible where the pixel values are in the range \([0,255]\). We use 100-step PGD to optimize transfer-based attacks (the objectives in Eq. (1) and Eq. (2)). In each step of query-based attacks, we set query times \(N=100\) in Eq. (4) and update the adversarial images by 8-steps PGD using the estimated gradient. Every experiment is run on a single NVIDIA-A100 GPU.

### Empirical studies

We evaluate large VLMs and freeze their parameters to make them act like image-to-text generative APIs. In particular, in Figure 1, we show that our crafted adversarial image consistently deceives BLIP-2 and that the generated response has the same semantics as the targeted text. In Figure 2, we

Figure 5: Adversarial perturbations \(\Delta\) are obtained by computing \(\bm{x}_{\text{adv}}-\bm{x}_{\text{cle}}\) (pixel values are amplified \(\times 10\) for visualization) and their corresponding captions are generated below. Here DALL-E acts as \(h_{\xi}\) to generate targeted images \(h_{\xi}(\bm{c}_{\text{tar}})\) for reference. We note that adversarial perturbations are not only visually hard to perceive, but also not detectable using state-of-the-art image captioning models (we use UniDiffuser for captioning, while similar conclusions hold when using other models).

Figure 6: We experiment with different values of \(\epsilon\) in Eq. (3) to obtain different levels of \(\bm{x}_{\text{adv}}\). As seen, the quality of \(\bm{x}_{\text{adv}}\) degrades (measured by the LPIPS distance between \(\bm{x}_{\text{cle}}\) and \(\bm{x}_{\text{adv}}\)), while the effect of targeted response generation saturates (in this case, we evaluate UniDiffuser). Thus, a proper perturbation budget (e.g., \(\epsilon=8\)) is necessary to balance image quality and generation performance.

evaluate UniDiffuser, which is capable of bidirectional joint generation, to generate text-to-image and then image-to-text using the crafted \(\bm{x}_{\text{adv}}\). It should be noted that such a chain of generation will result in completely different content than the original text description. We simply use "what is the content of this image?" as the prompt to answer generation for models that require text instructions as input (query) [30]. However, for MiniGPT-4, we use a more flexible approach in conversation, as shown in Figure 3. In contrast to the clean images on which MiniGPT-4 has concrete and correct understanding and descriptions, our crafted adversarial counterparts mislead MiniGPT-4 into producing targeted responses and creating more unexpected descriptions that are not shown in the targeted text.

In Table 1, we examines the effectiveness of MF-it and MF-ii in crafting white-box adversarial images against surrogate models such as CLIP [64], BLIP [41] and ALBEF [40]. We take 50K clean images \(\bm{x}_{\text{cle}}\) from the ImageNet-1K validation set and randomly select a targeted text \(\bm{c}_{\text{tar}}\) from MS-COCO captions for each clean image. We also generate targeted images \(h_{\xi}(\bm{c}_{\text{tar}})\) as reference and craft adversarial images \(\bm{x}_{\text{adv}}\) by MF-ii or MF-it. As observed, both MF-ii and MF-it are able to increase the similarity between the adversarial image and the targeted text (as measured by CLIP score) in the white-box setting, laying the foundation for black-box transferability. Specifically, as seen in Table 2, we first transfer the adversarial examples crafted by MF-ii or MF-it in order to evade large VLMs and mislead them into generating targeted responses. We calculate the similarity between the generated response \(p_{\theta}(\bm{x}_{\text{adv}};\bm{c}_{\text{in}})\) and the targeted text \(\bm{c}_{\text{tar}}\) using various types of CLIP text encoders. As mentioned previously, the default textual input \(\bm{c}_{\text{in}}\) is fixed to be "what is the content of this image?". Surprisingly, we find that MF-it performs worse than MF-ii, which suggests overfitting when optimizing directly on the cross-modality similarity. In addition, when we use the transfer-based adversarial image crafted by MF-ii as an initialization and then apply query-based MF-tt to tune the adversarial image, the generated response becomes significantly more similar to the targeted text, indicating the vulnerability of advanced large VLMs.

### Further analyses

Does VLM adversarial perturbations induce semantic meanings?Previous research has demonstrated that adversarial perturbations crafted against robust models will exhibit semantic or perceptually-aligned characteristics [82; 35; 60]. This motivates us to figure out whether adversarial perturbations \(\Delta=\bm{x}_{\text{adv}}-\bm{x}_{\text{cle}}\) crafted against large VLMs possess a similar level of semantic information. In Figure 5, we visualize \(\Delta\) that results in a successful targeted evasion over a real image and report the generated text responses. Nevertheless, we observe no semantic information associated with the targeted text in adversarial perturbations or their captions, indicating that large VLMs are inherently vulnerable.

The influence of perturbation budget \(\bm{\epsilon}\).We use \(\epsilon=8\) as the default value in our experiments, meaning that the pixel-wise perturbation is up to \(\pm 8\) in the range \([0,255]\). In Figure 6, we examine the effect of setting \(\epsilon\) to different values of \(\{2,4,8,16,64\}\) and compute the perceptual distance between the clean image \(\bm{x}_{\text{cle}}\) and its adversarial counterpart \(\bm{x}_{\text{adv}}\) using LPIPS (\(\downarrow\)) [106]. We highlight (in red color) the generated responses that most closely resemble the targeted text. As observed, there is a trade-off between image quality/fidelity and successfully eliciting the targeted response; therefore, it is essential to choose an appropriate perturbation budget value.

Figure 7: **Performance of our attack method under a fixed perturbation budget \(\epsilon=8\). We interpolate between the sole use of transfer-based attack and the sole use of query-based attack strategy. We demonstrate the effectiveness of our method via CLIP score (\(\uparrow\)) between the generated texts on adversarial images and the target texts, with different types of CLIP text encoders. The \(x\)-axis in a “\(\mathfrak{t}_{\ell}\)-\(\mathfrak{q}\)c\({}_{q}\)” format denotes we assign \(\epsilon_{t}\) to transfer-based attack and \(\epsilon_{q}\) to query-based attack. “t+q=8” indicates we use transfer-based attack (\(\epsilon_{t}=8\)) as initialization, and conduct query-based attack for further 8 steps (\(\epsilon_{q}=8\)), such that the resulting perturbation satisfies \(\epsilon=8\). As a result, We show that a proper combination of transfer/query based attack strategy achieves the best performance.**

**Performance of attack with a fixed perturbation budget.** To understand the separate benefit from transfer-based attack and query-based attack, we conduct a study to assign different perturbation budget for transfer (\(\epsilon_{t}\)) and query based attack strategy (\(\epsilon_{q}\)), under the constraint \(\epsilon_{t}+\epsilon_{q}=8\). Unidiffuser is the victim model in our experiment. The results are in Figure 7. We demonstrate that, a proper combination of transfer and query based attack achieves the best performance.

**Interpreting the mechanism of attacking large VLMs.** To understand how our targeted adversarial example influences response generation, we compute the relevancy score of image patches related to the input question using GradCAM [75] to obtain a visual explanation for both clean and adversarial images. As shown in Figure 8, our adversarial image \(\bm{x}_{\text{adv}}\) successfully suppresses the relevancy to the original text description (panel **(b)**) and mimics the attention map of the targeted image \(h_{\xi}(\bm{c}_{\text{tar}})\) (panel **(c)**). Nonetheless, we emphasize that the use of GradCAM as a feature attribution method has some known limitations [13]. Additional interpretable examples are provided in the Appendix.

## 5 Discussion

It is widely accepted that developing large multimodal models will be an irresistible trend. Prior to deploying these large models in practice, however, it is essential to understand their worst-case performance through techniques such as red teaming or adversarial attacks [25]. In contrast to manipulating textual inputs, which may require human-in-the-loop prompt engineering, our results demonstrate that manipulating visual inputs can be automated, thereby effectively fooling the entire large vision-language systems. The resulting adversarial effect is deeply rooted and can even affect multi-round interaction, as shown in Figure 3. While multimodal security issues have been cautiously treated by models such as GPT-4, which delays the release of visual inputs [3], there are an increasing number of open-source multimodal models, such as MiniGPT-4 [109] and LLAVA [46; 45], whose worst-case behaviors have not been thoroughly examined. The use of these open-source, but adversarially unchecked, large multimodal models as product plugins could pose potential risks.

**Broader impacts.** While the primary goal of our research is to evaluate and quantify adversarial robustness of large vision-language models, it is possible that the developed attacking strategies could be misused to evade practically deployed systems and cause potential negative societal impacts. Specifically, our threat model assumes black-box access and targeted responses, which involves manipulating existing APIs such as GPT-4 (with visual inputs) and/or Midjourney on purpose, thereby increasing the risk if these vision-language APIs are implemented as plugins in other products.

**Limitations.** Our work focuses primarily on the digital world, with the assumption that input images feed directly into the models. In the future, however, vision-language models are more likely to be deployed in complex scenarios such as controlling robots or automatic driving, in which case input images may be obtained from the interaction with physical environments and captured in real-time by cameras. Consequently, performing adversarial attacks in the physical world would be one of the future directions for evaluating the security of vision-language models.

## Acknowledgements

This research work is supported by the Agency for Science, Technology and Research (A*STAR) under its MTC Programmatic Funds (Grant No. M23L7b0021). This material is based on the research/work support in part by the Changi General Hospital and Singapore University of Technology and Design, under the HealthTech Innovation Fund (HTIF Award No. CGH-SUTD-2021-004). C. Li was sponsored by Beijing Nova Program (No. 20220484044). We thank Siqi Fu for providing beautiful pictures generated by Midjourney, and anonymous reviewers for their insightful comments.

## References

* [1]N. Aafaq, N. Akhtar, W. Liu, M. Shah, and A. Mian (2021) Controlled caption generation for images through adversarial attacks. arXiv preprint arXiv:2107.03050. Cited by: SS1.
* [2]J. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr, Y. Hasson, K. Lenc, A. Mensch, K. Millican, M. Reynolds, et al. (2022) Flamingo: a visual language model for few-shot learning. In Advances in Neural Information Processing Systems (NeurIPS), Cited by: SS1.
* [3]S. Altman (2023) 3. Note: https://twitter.com/sama/status/1635687855921172480 Cited by: SS1.
* [4]M. Alzantot, Y. Sharma, A. Elgohary, B. Ho, M. Srivastava, and K. Chang (2018) Generating natural language adversarial examples. In Proceedings of Conference on Empirical Methods in Natural Language Processing (EMNLP), Cited by: SS1.
* [5]F. Bao, S. Nie, K. Xue, C. Li, S. Pu, Y. Wang, G. Yue, Y. Cao, H. Su, and J. Zhu (2022) One transformer fits all distributions in multi-modal diffusion at scale. In International Conference on Machine Learning (ICML), Cited by: SS1.
* [6]M. Bartolo, T. Thrush, R. Jia, S. Riedel, P. Stenetorp, and D. Kiela (2021) Improving question answering model robustness with synthetic adversarial data generation. arXiv preprint arXiv:2104.08678. Cited by: SS1.
* [7]A. N. Bhagoji, W. He, B. Li, and D. Song (2018) Practical black-box attacks on deep neural networks using efficient query mechanisms. In European Conference on Computer Vision (ECCV), Cited by: SS1.
* [8]B. Biggio, I. Corona, D. Maiorca, B. Nelson, N. Srndic, P. Laskov, G. Giacinto, and F. Roli (2013) Evasion attacks against machine learning at test time. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pp. 387-402. Cited by: SS1.
* [9]H. J. Branch, J. Rodriguez Cefalu, J. McHugh, L. Hujer, A. Bahl, D. del Castillo Iglesias, R. Heichman, and R. Darwishi (2022) Evaluating the susceptibility of pre-trained language models via handcrafted adversarial examples. arXiv preprint arXiv:2209.02128. Cited by: SS1.
* [10]T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. (2020) Language models are few-shot learners. Advances in Neural Information Processing Systems (NeurIPS). Cited by: SS1.
* [11]Y. Cao, D. Li, M. Fang, T. Zhou, J. Gao, Y. Zhan, and D. Tao (2022) Tasa: deceiving question answering models by twin answer sentences attack. arXiv preprint arXiv:2210.15221. Cited by: SS1.
* [12]N. Carlini, A. Athalye, N. Papernot, W. Brendel, J. Rauber, D. Tsipras, I. Goodfellow, A. Madry, and A. Kurakin (2019) On evaluating adversarial robustness. arXiv preprint arXiv:1902.06705. Cited by: SS1.
* [13]A. Chattopadhay, A. Sarkar, P. Howlader, and V. N. Balasubramanian (2018) Grad-cam++: generalized gradient-based visual explanations for deep convolutional networks. In 2018 IEEE winter conference on applications of computer vision (WACV), pp. 839-847. Cited by: SS1.
* [14]A. Chattopadhay, A. Sarkar, P. Howlader, and V. N. Balasubramanian (2018) Grad-cam++: generalized gradient-based visual explanations for deep convolutional networks. In 2018 IEEE winter conference on applications of computer vision (WACV), pp. 839-847. Cited by: SS1.
* [15]A. Chattopadhay, A. Sarkar, P. Howlader, and V. N. Balasubramanian (2018) Grad-cam++: generalized gradient-based visual explanations for deep convolutional networks. In 2018 IEEE winter conference on applications of computer vision (WACV), pp. 839-847. Cited by: SS1.
* [16]A. Chattopadhay, A. Sarkar, P. Howlader, and V. N. Balasubramanian (2018) Grad-cam++: generalized gradient-based visual explanations for deep convolutional networks. In 2018 IEEE winter conference on applications of computer vision (WACV), pp. 839-847. Cited by: SS1.
* [17]A. Chattopadhay, A. Sarkar, P. Howlader, and V. N. Balasubramanian (2018) Grad-cam++: generalized gradient-based visual explanations for deep convolutional networks. In 2018 IEEE winter conference on applications of computer vision (WACV), pp. 839-847. Cited by: SS1.
* [18]A. Chattopadhay, A. Sarkar, P. Howlader, and V. N. Balasubramanian (2018) Grad-cam++: generalized gradient-based visual explanations for deep convolutional networks. In 2018 IEEE winter conference on applications of computer vision (WACV), pp. 839-847. Cited by: SS1.
* [19]A. Chattopadhay, A. Sarkar, P. Howlader, and V. N. Balasubramanian (2018) Grad-cam++: generalized gradient-based visual explanations for deep convolutional networks. In 2018 IEEE winter conference on applications of computer vision (WACV), pp. 839-847. Cited by: SS1.
* [20]A. Chattopadhay, A. Sarkar, P. Howlader, and V. N. Balasubramanian (2018) Grad-cam++: generalized gradient-based visual explanations for deep convolutional networks. In 2018 IEEE winter conference on applications of computer vision (WACV), pp. 839-847. Cited by: SS1.
* [21]A. Chattopadhay, A. Sarkar, P. Howlader, and V. N. Balasubramanian (2018) Grad-cam++: generalized gradient-based visual explanations for deep convolutional networks. In 2018 IEEE winter conference on applications of computer vision (WACV), pp. 839-847. Cited by: SS1.
* [22]A. Chattopadhay, A. Sarkar, P. Howlader, and V. N. Balasubramanian (2018) Grad-cam++: generalized gradient-based visual explanations for deep convolutional networks. In 2018 IEEE winter conference on applications of computer vision (WACV), pp. 839-847. Cited by: SS1.
* [23]A. Chattopadhay, A. Sarkar, P. Howlader, and V. N. Balasubramanian (2018) Grad-cam++: generalized gradient-based visual explanations for deep convolutional networks. In 2018 IEEE winter conference on applications of computer vision (WACV), pp. 839-847. Cited by: SS1.
* [24]A. Chattopadhay, A. Sarkar, P. Howlader, and V. N. Balasubramanian (2018) Grad-cam++: generalized gradient-based visual explanations for deep convolutional networks. In 2018 IEEE winter conference on applications of computer vision (WACV), pp. 839-847. Cited by: SS1.
* [25]A. Chattopadhay, A. Sarkar, P. Howlader, and V. N. Balasubramanian (2018) Grad-cam++: generalized gradient-based visual explanations for deep convolutional networks. In 2018 IEEE winter conference on applications of computer vision (WACV), pp. 839-847. Cited by: SS1.
* [26]A. Chattopadhay, A. Sarkar, P. Howlader, and V. N. Balasubramanian (2018) Grad-cam++: generalized gradient-based visual explanations for deep convolutional networks. In 2018 IEEE winter conference on applications of computer vision (WACV), pp. 839-847. Cited by: SS1.
* [27]A. Chattopadhay, A. Sarkar, P. Howlader, and V. N. Balasubramanian (2018) Grad-cam++: generalized gradient-based visual explanations for deep convolutional networks. In 2018 IEEE winter conference on applications of computer vision (WACV), pp. 839-847. Cited by: SS1.
* [28]A. Chattopadhay, A. Sarkar, P. Howlader, and V. N. Balasubramanian (2018) Grad-cam++: generalized gradient-based visual explanations for deep convolutional networks. In 2018 IEEE winter conference on applications of computer vision (WACV), pp. 839-847. Cited by: SS1.
* [29]A. Chattopadhay, A. Sarkar, P. Howlader, and V. N. Balasubramanian (2018) Grad-cam++: generalized gradient-based visual explanations for deep convolutional networks. In 2018 IEEE winter conference on applications of computer vision (WACV), pp. 839-847. Cited by: SS1.
* [30]A. Chattopadhay, A. Sarkar, P. Howlader, and V. N. Balasubramanian (2018) Grad-cam++: generalized gradient-based visual explanations for deep convolutional networks. In 2018 IEEE winter conference on applications of computer vision (WACV), pp. 839-847. Cited by: SS1.
* [31]A. Chattopadhay, A. Sarkar, P. Howlader, and V. N. Balasubramanian (2018) Grad-cam++: generalized gradient-based visual explanations for deep convolutional networks. In 2018 IEEE winter conference on applications of computer vision (WACV), pp. 839-847. Cited by: SS1.
* [32]A. Chattopadhay, A. Sarkar, P. Howlader, and V. N. Balasubramanian (2018) Grad-cam++: generalized gradient-based visual explanations for deep convolutional networks. In 2018 IEEE winter conference on applications of computer vision (WACV), pp. 839-847. Cited by: SS1.
* [33]A. Chattopadhay, A. Sarkar, P. Howlader, and V. N. Balasubramanian (2018) Grad-cam++: generalized gradient-based visual explanations for deep convolutional networks. In 2018 IEEE winter conference on applications of computer vision (WACV), pp. 839-847. Cited by: SS1.
* [34]A. Chattopadhay, A. Sarkar, P. Howlader, and V. N. Balasubramanian (2018) Grad-cam++: generalized gradient-based visual explanations for deep convolutional networks. In 2018 IEEE winter conference on applications of computer vision (WACV), pp. 839-847. Cited by: SS1.
* [35]A. Chattopadhay, A. Sarkar, P. Howlader, and V. N. Balasubramanian (2018) Grad-cam++: generalized gradient-based visual explanations for deep convolutional networks. In 2018 IEEE winter conference on applications of computer vision (WACV), pp. 839-847. Cited by: SS1.
* [36]A. Chattopadhay, A. Sarkar, P. Howlader, and V. N. Balasubramanian (2018) Grad-cam++: generalized gradient-based visual explanations for deep convolutional networks. In 2018 IEEE winter conference on applications of computer vision (WACV), pp. 839-847. Cited by: SS1.
* [37]A. Chattopadhay, A. Sarkar, P. Howlader, and V. N. Balasubramanian (2018) Grad-cam++: generalized gradient-based visual explanations for deep convolutional networks. In 2018 IEEE winter conference on applications of computer vision (WACV), pp. 839-847. Cited by: SS1.
* [38]A. Chattopadhay, A. Sarkar, P. Howlader, and V. N. Balasubramanian (2018) Grad-cam++: generalized gradient-based visual explanations for deep convolutional networks. In 2018 IEEE winter conference* [14] Hongge Chen, Huan Zhang, Pin-Yu Chen, Jinfeng Yi, and Cho-Jui Hsieh. Attacking visual language grounding with adversarial examples: A case study on neural image captioning. _arXiv preprint arXiv:1712.02051_, 2017.
* [15] Jun Chen, Han Guo, Kai Yi, Boyang Li, and Mohamed Elhoseiny. Visualgpt: Data-efficient adaptation of pretrained language models for image captioning. In _IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2022.
* [16] Pin-Yu Chen, Huan Zhang, Yash Sharma, Jinfeng Yi, and Cho-Jui Hsieh. Zoo: Zeroth order optimization based black-box attacks to deep neural networks without training substitute models. In _ACM Workshop on Artificial Intelligence and Security (AISec)_. ACM, 2017.
* [17] Shuyu Cheng, Yinpeng Dong, Tianyu Pang, Hang Su, and Jun Zhu. Improving black-box adversarial attacks with a transfer-based prior. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2019.
* [18] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, et al. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, 2023. https://vicuna.lmsys.org/.
* [19] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. _arXiv preprint arXiv:2204.02311_, 2022.
* [20] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In _IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2009.
* [21] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. _arXiv preprint arXiv:1810.04805_, 2018.
* [22] Yinpeng Dong, Fangzhou Liao, Tianyu Pang, Hang Su, Jun Zhu, Xiaolin Hu, and Jianguo Li. Boosting adversarial attacks with momentum. In _IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2018.
* [23] Yinpeng Dong, Tianyu Pang, Hang Su, and Jun Zhu. Evading defenses to transferable adversarial examples by translation-invariant attacks. In _IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2019.
* [24] Yinpeng Dong, Shuyu Cheng, Tianyu Pang, Hang Su, and Jun Zhu. Query-efficient black-box adversarial attacks guided by a transfer-based prior. _IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)_, 44(12):9536-9548, 2021.
* [25] Yinpeng Dong, Huanran Chen, Jiawei Chen, Zhengwei Fang, Xiao Yang, Yichi Zhang, Yu Tian, Hang Su, and Jun Zhu. How robust is google's bard to adversarial image attacks? _arXiv preprint arXiv:2309.11751_, 2023.
* [26] Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. Palm-e: An embodied multimodal language model. _arXiv preprint arXiv:2303.03378_, 2023.
* [27] Deep Ganguli, Liane Lovitt, Jackson Kernion, Amanda Askell, Yuntao Bai, Saurav Kadavath, Ben Mann, Ethan Perez, Nicholas Schiefer, Kamal Ndousse, et al. Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned. _arXiv preprint arXiv:2209.07858_, 2022.
* [28] GitHub. Copilot x, 2023. https://github.com/features/preview/copilot-x.
* [29] Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. In _International Conference on Learning Representations (ICLR)_, 2015.

* [30] Jiaixian Guo, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Boyang Li, Dacheng Tao, and Steven Hoi. From images to textual prompts: Zero-shot visual question answering with frozen large language models. In _IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2023.
* [31] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. _arXiv preprint arXiv:2203.15556_, 2022.
* [32] Minghui Hu, Chuanxia Zheng, Heliang Zheng, Tat-Jen Cham, Chaoyue Wang, Zuopeng Yang, Dacheng Tao, and Ponnuthurai N Suganthan. Unified discrete diffusion for simultaneous vision-language generation. _arXiv preprint arXiv:2211.14842_, 2022.
* [33] Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui, Owais Khan Mohammed, Qiang Liu, et al. Language is not all you need: Aligning perception with language models. _arXiv preprint arXiv:2302.14045_, 2023.
* [34] Andrew Ilyas, Logan Engstrom, Anish Athalye, and Jessy Lin. Black-box adversarial attacks with limited queries and information. In _International Conference on Machine Learning (ICML)_, 2018.
* [35] Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Logan Engstrom, Brandon Anish Athalye, Tran, and Aleksander Madry. Adversarial examples are not bugs, they are features. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2019.
* [36] Di Jin, Zhijing Jin, Joey Tianyi Zhou, and Peter Szolovits. Is bert really robust? a strong baseline for natural language attack on text classification and entailment. In _AAAI Conference on Artificial Intelligence (AAAI)_, 2020.
* [37] Divyansh Kaushik, Douwe Kiela, Zachary C Lipton, and Wen-tau Yih. On the efficacy of adversarial data collection for question answering: Results from a large-scale randomized study. _arXiv preprint arXiv:2106.00872_, 2021.
* [38] Venelin Kovatchev, Trina Chatterjee, Venkata S Govindarajan, Jifan Chen, Eunsol Choi, Gabriella Chronis, Anubrata Das, Katrin Erk, Matthew Lease, Junyi Jessy Li, et al. How many linguists does it take to fool a question answering model? a systematic approach to adversarial attacks. _arXiv preprint arXiv:2206.14729_, 2022.
* [39] Alexandre Lacoste, Alexandra Luccioni, Victor Schmidt, and Thomas Dandres. Quantifying the carbon emissions of machine learning. _arXiv preprint arXiv:1910.09700_, 2019.
* [40] Junnan Li, Ramprasaath R. Selvaraju, Akhilesh Deepak Gotmare, Shafiq Joty, Caiming Xiong, and Steven Hoi. Align before fuse: Vision and language representation learning with momentum distillation. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2021.
* [41] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In _International Conference on Machine Learning (ICML)_, 2022.
* [42] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. _arXiv preprint arXiv:2301.12597_, 2023.
* [43] Linjie Li, Jie Lei, Zhe Gan, and Jingjing Liu. Adversarial vqa: A new benchmark for evaluating the robustness of vqa models. In _IEEE International Conference on Computer Vision (ICCV)_, 2021.
* [44] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In _Computer Vision-ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13_, pages 740-755. Springer, 2014.

* [45] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. _arXiv preprint arXiv:2310.03744_, 2023.
* [46] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. _arXiv preprint arXiv:2304.08485_, 2023.
* [47] Yanpei Liu, Xinyun Chen, Chang Liu, and Dawn Song. Delving into transferable adversarial examples and black-box attacks. _arXiv preprint arXiv:1611.02770_, 2016.
* [48] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. In _International Conference on Learning Representations (ICLR)_, 2018.
* [49] Rishabh Maheshwary, Saket Maheshwary, and Vikram Pudi. Generating natural language attacks in a hard label black box setting. In _AAAI Conference on Artificial Intelligence (AAAI)_, 2021.
* [50] Zhao Meng and Roger Wattenhofer. A geometry-inspired attack for generating natural language adversarial examples. _arXiv preprint arXiv:2010.01345_, 2020.
* [51] Midjourney. Midjourney website, 2023. https://www.midjourney.com.
* [52] Milad Moradi and Matthias Samwald. Evaluating the robustness of neural language models to input perturbations. _arXiv preprint arXiv:2108.12237_, 2021.
* [53] John X Morris, Eli Lifland, Jack Lanchantin, Yangfeng Ji, and Yanjun Qi. Reevaluating adversarial examples in natural language. _arXiv preprint arXiv:2004.14174_, 2020.
* [54] Yurii Nesterov and Vladimir Spokoiny. Random gradient-free minimization of convex functions. _Foundations of Computational Mathematics_, 17:527-566, 2017.
* [55] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. _arXiv preprint arXiv:2112.10741_, 2021.
* [56] Yixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, and Douwe Kiela. Adversarial nli: A new benchmark for natural language understanding. In _Annual Meeting of the Association for Computational Linguistics (ACL)_, 2020.
* [57] OpenAI. Introducing chatgpt, 2022. https://openai.com/blog/chatgpt.
* [58] OpenAI. Gpt-4 technical report. _arXiv_, 2023.
* [59] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2022.
* [60] Tianyu Pang, Min Lin, Xiao Yang, Jun Zhu, and Shuicheng Yan. Robustness and accuracy could be reconcilable by (proper) definition. In _International Conference on Machine Learning (ICML)_, 2022.
* [61] Nicolas Papernot, Patrick McDaniel, and Ian Goodfellow. Transferability in machine learning: from phenomena to black-box attacks using adversarial samples. _arXiv preprint arXiv:1605.07277_, 2016.
* [62] Joon Sung Park, Joseph C O'Brien, Carrie J Cai, Meredith Ringel Morris, Percy Liang, and Michael S Bernstein. Generative agents: Interactive simulacra of human behavior. _arXiv preprint arXiv:2304.03442_, 2023.
* [63] Ethan Perez, Saffron Huang, Francis Song, Trevor Cai, Roman Ring, John Aslanides, Amelia Glaese, Nat McAleese, and Geoffrey Irving. Red teaming language models with language models. _arXiv preprint arXiv:2202.03286_, 2022.

* [64] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. _OpenAI blog_, 1(8):9, 2019.
* [65] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In _International Conference on Machine Learning (ICML)_, 2021.
* [66] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. _Journal of Machine Learning Research (JMLR)_, 21(1):5485-5551, 2020.
* [67] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In _International Conference on Machine Learning_, pages 8821-8831. PMLR, 2021.
* [68] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. _arXiv preprint arXiv:2204.06125_, 2022.
* [69] Javier Rando, Daniel Paleka, David Lindner, Lennard Heim, and Florian Tramer. Red-teaming the stable diffusion safety filter. _arXiv preprint arXiv:2210.04610_, 2022.
* [70] Yankun Ren, Jianbin Lin, Siliang Tang, Jun Zhou, Shuang Yang, Yuan Qi, and Xiang Ren. Generating natural language adversarial examples on a large scale with generative models. _arXiv preprint arXiv:2003.10388_, 2020.
* [71] Jonas Ricker, Simon Damm, Thorsten Holz, and Asja Fischer. Towards the detection of diffusion model deepfakes. _arXiv preprint arXiv:2210.14571_, 2022.
* [72] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In _IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 10684-10695, 2022.
* [73] Hadi Salman, Alaa Khaddaj, Guillaume Leclerc, Andrew Ilyas, and Aleksander Madry. Raising the cost of malicious ai-powered image editing. In _International Conference on Machine Learning (ICML)_, 2023.
* [74] Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilic, Daniel Hesslow, Roman Castagne, Alexandra Sasha Luccioni, Francois Yvon, Matthias Galle, et al. Bloom: A 176b-parameter open-access multilingual language model. _arXiv preprint arXiv:2211.05100_, 2022.
* [75] Ramprasaath R. Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra. Grad-cam: Visual explanations from deep networks via gradient-based localization. In _Proceedings of the IEEE International Conference on Computer Vision (ICCV)_, Oct 2017.
* [76] Zeyang Sha, Zheng Li, Ning Yu, and Yang Zhang. De-fake: Detection and attribution of fake images generated by text-to-image diffusion models. _arXiv preprint arXiv:2210.06998_, 2022.
* [77] Sasha Sheng, Amanpreet Singh, Vedanuj Goswami, Jose Magana, Tristan Thrush, Wojciech Galuba, Devi Parikh, and Douwe Kiela. Human-adversarial visual question answering. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2021.
* [78] Yundi Shi, Piji Li, Changchun Yin, Zhaoyang Han, Lu Zhou, and Zhe Liu. Promptattack: Prompt-based attack for language models via gradient search. In _Natural Language Processing and Chinese Computing (NLPCC)_, 2022.
* [79] Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam Rajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti, et al. Using deepspeed and megatron to train megatron-turing nlg 530b, a large-scale generative language model. _arXiv preprint arXiv:2201.11990_, 2022.

* [80] Quan Sun, Yuxin Fang, Ledell Wu, Xinlong Wang, and Yue Cao. Eva-clip: Improved training techniques for clip at scale. _arXiv preprint arXiv:2303.15389_, 2023.
* [81] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. In _International Conference on Learning Representations (ICLR)_, 2014.
* [82] Guanhong Tao, Shiqing Ma, Yingqi Liu, and Xiangyu Zhang. Attacks meet interpretability: Attribute-steered detection of adversarial samples. In _Advances in Neural Information Processing Systems (NeurIPS)_, pages 7717-7728, 2018.
* [83] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B Hashimoto. Stanford alpaca: An instruction-following llama model, 2023. https://github.com/tatsu-lab/stanford_alpaca.
* [84] Anthony Meng Huat Tiong, Junnan Li, Boyang Li, Silvio Savarese, and Steven CH Hoi. Plug-and-play vqa: Zero-shot vqa by conjoining large pretrained models with zero training. _arXiv preprint arXiv:2210.08773_, 2022.
* [85] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothe Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. _arXiv preprint arXiv:2302.13971_, 2023.
* [86] Maria Tsimpoukelli, Jacob L Menick, Serkan Cabi, SM Eslami, Oriol Vinyals, and Felix Hill. Multimodal few-shot learning with frozen language models. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2021.
* [87] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. _Advances in neural information processing systems_, 30, 2017.
* [88] Sai Vemprala, Rogerio Bonatti, Arthur Bucker, and Ashish Kapoor. Chatgpt for robotics: Design principles and model abilities. _Microsoft Blog_, 2023.
* [89] Eric Wallace, Pedro Rodriguez, Shi Feng, Ikuya Yamada, and Jordan Boyd-Graber. Trick me if you can: Human-in-the-loop generation of adversarial examples for question answering. _Transactions of the Association for Computational Linguistics_, 7:387-401, 2019.
* [90] Boxin Wang, Chejian Xu, Shuohang Wang, Zhe Gan, Yu Cheng, Jianfeng Gao, Ahmed Hassan Awadallah, and Bo Li. Adversarial glue: A multi-task benchmark for robustness evaluation of language models. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2021.
* [91] Jindong Wang, Xixu Hu, Wenxin Hou, Hao Chen, Runkai Zheng, Yidong Wang, Linyi Yang, Haojun Huang, Wei Ye, Xiubo Geng, et al. On the robustness of chatgpt: An adversarial and out-of-distribution perspective. _arXiv preprint arXiv:2302.12095_, 2023.
* [92] Xiao Wang, Qin Liu, Tao Gui, Qi Zhang, Yicheng Zou, Xin Zhou, Jiacheng Ye, Yongxin Zhang, Rui Zheng, Zexiong Pang, et al. Textflint: Unified multilingual robustness evaluation toolkit for natural language processing. In _Annual Meeting of the Association for Computational Linguistics (ACL)_, 2021.
* [93] Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, and Nan Duan. Visual chatgpt: Talking, drawing and editing with visual foundation models. _arXiv preprint arXiv:2303.04671_, 2023.
* [94] Cihang Xie, Zhishuai Zhang, Yuyin Zhou, Song Bai, Jianyu Wang, Zhou Ren, and Alan L Yuille. Improving transferability of adversarial examples with input diversity. In _IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2019.
* [95] Canwen Xu, Daya Guo, Nan Duan, and Julian McAuley. Baize: An open-source chat model with parameter-efficient tuning on self-chat data. _arXiv preprint arXiv:2304.01196_, 2023.
* [96] Jing Xu, Da Ju, Margaret Li, Y-Lan Boureau, Jason Weston, and Emily Dinan. Bot-adversarial dialogue for safe conversational agents. In _North American Chapter of the Association for Computational Linguistics: Human Language Technologies_, 2021.

* [97] Xiaojun Xu, Xinyun Chen, Chang Liu, Anna Rohrbach, Trevor Darrell, and Dawn Song. Fooling vision and language models despite localization and attention mechanism. In _IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2018.
* [98] Xingqian Xu, Zhangyang Wang, Eric Zhang, Kai Wang, and Humphrey Shi. Versatile diffusion: Text, images and variations all in one diffusion model. _arXiv preprint arXiv:2211.08332_, 2022.
* [99] Yan Xu, Baoyuan Wu, Fumin Shen, Yanbo Fan, Yong Zhang, Heng Tao Shen, and Wei Liu. Exact adversarial attack to image captioning via structured output learning with latent variables. In _IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2019.
* [100] Xiao Yang, Yinpeng Dong, Tianyu Pang, Hang Su, and Jun Zhu. Boosting transferability of targeted adversarial examples via hierarchical generative networks. In _European Conference on Computer Vision (ECCV)_, 2022.
* [101] Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu, Michael Zeng, and Lijuan Wang. Mm-react: Prompting chatgpt for multimodal reasoning and action. _arXiv preprint arXiv:2303.11381_, 2023.
* [102] Liping Yuan, Xiaoqing Zheng, Yi Zhou, Cho-Jui Hsieh, and Kai-Wei Chang. On the transferability of adversarial attacksagainst neural text classifier. _arXiv preprint arXiv:2011.08558_, 2020.
* [103] Mert Yukekgonul, Federico Bianchi, Pratyusha Kalluri, Dan Jurafsky, and James Zou. When and why vision-language models behave like bags-of-words, and what to do about it? In _International Conference on Learning Representations (ICLR)_, 2023.
* [104] Huangzhao Zhang, Hao Zhou, Ning Miao, and Lei Li. Generating fluent adversarial examples for natural languages. In _Annual Meeting of the Association for Computational Linguistics (ACL)_, 2019.
* [105] Jiaming Zhang, Qi Yi, and Jitao Sang. Towards adversarial attack on vision-language pre-training models. In _ACM International Conference on Multimedia_, 2022.
* [106] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric. In _CVPR_, 2018.
* [107] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. _arXiv preprint arXiv:2205.01068_, 2022.
* [108] Yunqing Zhao, Tianyu Pang, Chao Du, Xiao Yang, Ngai-Man Cheung, and Min Lin. A recipe for watermarking diffusion models. _arXiv preprint arXiv:2303.10137_, 2023.
* [109] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. _arXiv preprint arXiv:2304.10592_, 2023.
* [110] Terry Yue Zhuo, Zhuang Li, Yujin Huang, Yuan-Fang Li, Weiqing Wang, Gholamreza Haffari, and Fatemeh Shiri. On robustness of prompt-based semantic parsing with large pre-trained language model: An empirical study on codex. _arXiv preprint arXiv:2301.12868_, 2023.

## Appendix

In this appendix, we describe implementation details, additional experiment results and analyses, to support the methods proposed in the main paper. We also discuss failure cases in order to better understand the capability of our attack methods.

## Appendix A Implementation details

In Section 4.1 of the main paper, we introduce large VLMs, datasets, and other basic setups used in our experiments and analyses. Here, we discuss more on the design choices and implementation details to help understanding our attacking strategies and reproducing our empirical results.

**Examples of how the datasets are utilized.** In our experiments, we use the ImageNet-1K [20] validation images as the clean images \((\bm{x}_{\text{cle}})\) to be attacked, and we randomly select a caption from MS-COCO [44] captions as each clean image's targeted text \(\bm{c}_{\text{tar}}\). Therefore, we ensure that each clean image and its randomly selected targeted text are _irrelevant_. To implement MF-ii, we use Stable Diffusion [72] to generate the targeted images (i.e., \(h_{\xi}(\bm{c}_{\text{tar}})\) in the main paper). Here, we provide several examples of <clean image - targeted text - targeted image> pairs used in our experiments (e.g., Table 1 and Table 2 in the main paper), as shown in Figure 9.

**Text-to-image models for targeted image generation.** It is natural to consider the real images from MS-COCO as the targeted images corresponding to the targeted text (caption) in our attack methods. Nevertheless, we emphasize that in our experiments, we expect to examine the targeted text \(\bm{c}_{\text{tar}}\) in a flexible design space, where, for instance, the adversary may define \(\bm{c}_{\text{tar}}\) adaptively and may not be limited to a specific dataset. Therefore, given any targeted text \(\bm{c}_{\text{tar}}\), we adopt Stable Diffusion [72], Midjourney [51] and DALL-E [67, 68] as text-to-image models \(h_{\xi}\) to generate the targeted image \(h_{\xi}(\bm{c}_{\text{tar}})\), laying the foundation for a more flexible adversarial attack framework. In the meantime, we observe empirically that (1) using targeted texts and the corresponding (real) targeted images from MS-COCO, and (2) using targeted texts and the corresponding generated targeted images have comparable qualitative and quantitative performance.

**Hyperparameters.** Here, we discuss the additional setups and hyperparameters applied in our experiments. By default, we set \(\epsilon=8\) and the pixel value of all images is clamped to \([0,255]\). For each PGD attacking step, we set the step size as \(1\), which means we change the pixel value by \(1\) (for each pixel) at each step for crafting adversarial images. The adversarial perturbation is initialized as \(\Delta=\bm{0}\). Nonetheless, we note that initializing \(\Delta\sim\mathcal{N}(\bm{0},\mathbf{I})\) yields comparable results. For query-based attacking strategy (i.e., MF-tt), we set \(\sigma=8\) and \(\delta\sim\mathcal{N}(\bm{0},\mathbf{I})\) to construct randomly perturbed images for querying black-box responses. After the attack, the adversarial images are saved in PNG format to avoid any compression/loss that could result in performance degradation.

**Attacking algorithm.** In addition to the illustration in the main paper (see Figure 4), we present an algorithmic format for our proposed adversarial attack against large VLMs here. We clarify that

Figure 9: An illustration of the dataset used in our MF-ii attack against large VLMs. By utilizing the text-to-image generation capability of Stable Diffusion, we are able to generate high-quality and fidelity targeted images given any type of targeted text, thereby increasing the attacking flexibility.

we slightly abuse the notations by representing both the variable and the optimal solution of the adversarial attack with \(\bm{x}_{\text{adv}}\). For simplicity, we omit the input \(\bm{c}_{\text{in}}\) for the victim model (see Section 3.1). All other hyperparameters and notations are consistent with the main paper or this appendix. Because we see in Table 2 that MF-it has poor transferability on large VLMs, we use MF-ii + MF-tt here, as shown in Figure 4. In Algorithm 1, we summarize the proposed method.

```
1:Input: Clean image \(\bm{x}_{\text{cle}}\), a pretrained substitute model \(f_{\phi}\) (e.g., a ViT-B/32 or ViT-L/14 visual encoder of CLIP), a pretrained victim model \(p_{\theta}\) (e.g., Unidiffuser), a targeted text \(\bm{c}_{\text{tar}}\), a pretrained text-to-image generator \(h_{\xi}\) (e.g., Stable Diffusion), a targeted image \(h_{\xi}(\bm{c}_{\text{tar}})\).
2:Init: Number of steps \(\bm{s}_{1}\) for MF-ii, number of steps \(\bm{s}_{2}\) for MF-tt, number of queries \(N\) in each step for MF-tt, \(\Delta=\bm{0}\), \(\delta\sim\mathcal{N}(\bm{0},\mathbf{I})\), \(\sigma=8\), \(\epsilon=8\), \(\bm{x}_{\text{cle}}.\texttt{requires\_grad()}=\texttt{False}\).
3:# MF-ii
4:for\(\bm{i}=1\); \(\bm{i}\leq\bm{s}_{1}\); \(\bm{i}++\)do
5:\(\bm{x}_{\text{adv}}=\texttt{clamp}(\bm{x}_{\text{cle}}+\Delta\), \(\texttt{min}=0\), \(\texttt{max}=255)\)
6: Compute normalized embedding of \(h_{\xi}(\bm{c}_{\text{tar}})\): \(\bm{e}_{1}=f_{\phi}(h_{\xi}(\bm{c}_{\text{tar}}))/f_{\phi}(h_{\xi}(\bm{c}_{ \text{tar}})).\texttt{norm()}\)
7: Compute embedding of \(\bm{x}_{\text{adv}}\): \(\bm{e}_{2}=f_{\phi}(\bm{x}_{\text{adv}})/f_{\phi}(\bm{x}_{\text{adv}}).\texttt{norm ()}\)
8: Compute embedding similarity: \(\texttt{sim}=\bm{e}_{1}^{\top}\bm{e}_{2}\)
9: Backpropagate the gradient: \(\texttt{grad}=\texttt{sim.backward()}\)
10: Update \(\Delta=\texttt{clamp}(\Delta+\texttt{grad.sign()}\), \(\texttt{min}=-\epsilon\), \(\texttt{max}=\epsilon)\)
11:endfor
12:# MF-tt
13:Init:\(\bm{x}_{\text{adv}}=\bm{x}_{\text{cle}}+\Delta\)
14:for\(\bm{j}=1\); \(\bm{j}\leq\bm{s}_{2}\); \(\bm{j}++\)do
15:Obtain generated output of perturbed images: \(\{p_{\theta}(\bm{x}_{\text{adv}}+\sigma\delta_{n})\}_{n=1}^{N}\)
16:Obtain generated output of adversarial images: \(p_{\theta}(\bm{x}_{\text{adv}})\)
17:Estimate the gradient (Eq. (4)): \(\texttt{pseudo-grad}=\texttt{RGF}(\bm{c}_{\text{tar}},p_{\theta}(\bm{x}_{ \text{adv}}),\{p_{\theta}(\bm{x}_{\text{adv}}+\sigma\delta_{n})\}_{n=1}^{N})\)
18:Update \(\Delta=\texttt{clamp}(\Delta+\texttt{pseudo-grad.sign()}\), \(\texttt{min}=-\epsilon\), \(\texttt{max}=\epsilon)\)
19:\(\bm{x}_{\text{adv}}=\texttt{clamp}(\bm{x}_{\text{cle}}+\Delta\), \(\texttt{min}=0\), \(\texttt{max}=255)\)
20:endfor
21:Output: The queried captions and the adversarial image \(\bm{x}_{\text{adv}}\) ```

**Algorithm 1** Adversarial attack against large VLMs (Figure 4)

**Amount of computation.** The amount of computation consumed in this work is reported in Table 3, in accordance with NeurIPS guidelines. We include the compute amount for each experiment as well as the CO\({}_{2}\) emission (in kg). In practice, our experiments can be run on a single GPU, so the computational demand of our work is low.

## Appendix B Additional experiments

In our main paper, we demonstrated sufficient experiment results using six cutting-edge large VLMs on various datasets and setups. In this section, we present additional results, visualization, and analyses to supplement the findings in our main paper.

### Image captioning task by BLIP-2

In Figure 10, we provide additional targeted response generation by BLIP-2 [42]. We observe that our crafted adversarial examples can cause BLIP-2 to generate text that is sufficiently similar to the predefined targeted text, demonstrating the effectiveness of our method. For example, in Figure 10, when we set the targeted text as "A computer from the 90s in the style of vaporwave", the pretrained BLIP-2 model will generate the response "A cartoon drawn on the side of an old computer", whereas the content of clean image appears to be "A field with yellow flowers and a sky full of clouds". Another example could be when the content of the clean image is "A cute girl sitting on steps playing with her bubbles", the generated response on the adversarial examples is "A stuffed white mushroom sitting next to leaves", which resembles the predefined targeted text "A photo of a mushroom growing from the earth".

[MISSING_PAGE_FAIL:20]

as "where is the corn cob?", GradCAM will highlight the area of the adversarial image that is similar to the targeted image. More results can be found in Figure 14.

## Appendix C Additional discussion

In this section, we clarify on the sensitivity when we perturb adversarial examples, and failure cases to help better understand the limitations of our attacks.

### Sensitivity of adversarial examples to random perturbation

To evaluate the sensitivity of our crafted adversarial examples, we add random Gaussian noises with zero mean and standard deviation \(\sigma_{\text{noise}}\) to the obtained adversarial images \(\bm{x}_{\text{adv}}\), and then feed in the perturbed adversarial examples for response generation. The results are shown in Figure 15. We observe that our adversarial examples are reasonably insensitive to this type of perturbation, and we also make the following observation: as the amplitude (i.e., \(\sigma_{\text{noise}}\)) of the Gaussian noises added to \(\bm{x}_{\text{adv}}\) increase, the effectiveness of our learnt adversarial perturbation diminishes and the targeted responses revert to the original. For instance, in Figure 15, when \(\sigma_{\text{noise}}=0\), we can obtain the generated targeted response "A red and black bird sitting on top of a tree branch" that resembles the targeted text; when \(\sigma_{\text{noise}}=0.025\), it changes to "A red and black bird is sitting on top of a sunflower"; and finally the response degrades to "A large painting of three sunflowers in a field". Additional results are shown in Figure 15.

### Failure cases

While we have demonstrated convincing results of our method in the main paper and in this appendix, we note that the adversarial attack success rate for these large VLMs is not one hundred percent. Here, we present a few failure cases discovered during our experiments, leaving them for future work to improve performance. Specifics are shown in Figure 16.

Figure 10: Additional results of image captioning task implemented by BLIP-2.

Figure 11: Additional results for joint generation task implemented by Unidiffuser.

Figure 12: Additional results of VQA task implemented by MiniGPT-4.

Figure 13: Additional results of VQA task implemented by LLaVA.

Figure 14: **Visually interpreting our attacking mechanism.** To better understand the mechanism by which our adversarial examples deceive large VLMs, we provide additional visual interpretation results (via GradCAM [75]) as supplements to Figure 7 of the main paper. Similar to our previous findings, we demonstrate: **(a)** An example of \(\bm{x}_{\text{cle}}\), \(\bm{x}_{\text{adv}}\), and \(h_{\xi}(\bm{c}_{\text{tar}})\), along with the responses they generate; **(b)** GradCAM visualization when the input question \(\bm{c}_{\text{in}}\) is related to the clean image. **(c)** GradCAM will highlight regions similar to those of \(\bm{x}_{\text{adv}}\) if we provide the targeted text (or other texts related to \(\bm{c}_{\text{tar}}\)) as the question.

Figure 15: **Sensitivity of adversarial examples to Gaussian noises.** Our adversarial examples are found to be relatively insensitive to post-processing perturbations like Gaussian noises. Alternatively, interesting observations emerge when gradually increasing the standard deviation \(\sigma_{\text{noise}}\) of the Gaussian noises added to \(\bm{x}_{\text{adv}}\), where the effectiveness of our learned adversarial examples becomes marginal and the targeted responses (in green) gradually degrade to the original, correct response (in black). In addition, we note that an intermediate state exists in which the generated response is a combination of the targeted text and the original generated response (in red).

Figure 16: **Failure cases found in our experiments.** The generated adversarial image responses appear to be a state in between the text description of the clean image and the predefined targeted text. In this figure, we use LLaVA [46] as the conversation platform, but similar observations can be made with other large VLMs. On the other hand, we discovered that increasing the steps for adversarial attack (we set 100 in main experiments) could effectively address this issue (note that the perturbation budget remains unchanged, e.g., \(\epsilon=8\)).