# The Power of Hard Attention Transformers on Data Sequences: A Formal Language Theoretic Perspective

 Pascal Bergstrafser

RPTU Kaiserslautern-Landau

67663 Kaiserslautern, Germany

bergstraesser@cs.uni-kl.de &Chris Kocher

MPI-SWS

67663 Kaiserslautern, Germany

ckoecher@mpi-sws.org &Anthony Widjaja Lin

MPI-SWS and RPTU Kaiserslautern-Landau

67663 Kaiserslautern, Germany

awlin@mpi-sws.org &Georg Zetzsche

MPI-SWS

67663 Kaiserslautern, Germany

georg@mpi-sws.org

###### Abstract

Formal language theory has recently been successfully employed to unravel the power of transformer encoders. This setting is primarily applicable in Natural Language Processing (NLP), as a token embedding function (where a bounded number of tokens is admitted) is first applied before feeding the input to the transformer. On certain kinds of data (e.g. time series), we want our transformers to be able to handle _arbitrary_ input sequences of numbers (or tuples thereof) without _a priori_ limiting the values of these numbers. In this paper, we initiate the study of the expressive power of transformer encoders on sequences of data (i.e. tuples of numbers). Our results indicate an increase in expressive power of hard attention transformers over data sequences, in stark contrast to the case of strings. In particular, we prove that Unique Hard Attention Transformers (UHAT) over inputs as data sequences no longer lie within the circuit complexity class AC0 (even without positional encodings), unlike the case of string inputs, but are still within the complexity class TC0 (even with positional encodings). Over strings, UHAT without positional encodings capture only regular languages. In contrast, we show that over data sequences UHAT can capture non-regular properties. Finally, we show that UHAT capture languages definable in an extension of linear temporal logic with unary numeric predicates and arithmetics.

## 1 Introduction

Recent years have witnessed the success of transformers [42] in different applications, including natural language processing [12], computer vision [14], speech recognition [13], and time series analysis [45, 46]. In the quest to better understand the ability and limitation of transformers, theoretical investigations have actively been undertaken in the last few years. Among others, _formal language theory_ has been successfully applied to reveal deep insights into the expressive power of transformers, e.g., see the recent survey [40] and [2, 3, 8, 15, 20, 21, 30, 32, 38, 39]. In particular, relevant questions pertain to the power of various attention mechanisms, bounded/unbounded precision, positional encoding functions, and interplay between encoders and decoders, among many others.

One common assumption in the formal language theoretic approach to transformers is that the input sequence ranges over a _finite_ set \(\Sigma\) (called alphabet), which is then to be fed into a transformer after applying a token embedding function of the form \(f:\Sigma\rightarrow\mathbb{R}^{d}\). As a by-product, the number of tokens is finite. In certain applications (e.g. time series forecasting [28]), we want our transformers tobe able to handle _arbitrary_ input sequences of numbers (or tuples thereof) without _a priori_ limiting the values of these numbers. Moreover, numbers could be compared using arithmetic and (in)equality, which is not the case for elements of alphabets considered in formal language theory. For this reason, we propose to investigate the expressive power of _transformers over data sequences_, which takes us to the setting of formal language theory over the alphabet \(\Sigma=\mathbb{Q}^{d}\), for some \(d\in\mathbb{Z}_{>0}\), e.g., see [4, 11]. That is, _what properties of a sequence of (tuples of) numbers can be recognized by transformers?_

**Connections to circuit complexity.** Existing work has revealed intimate connections between transformers and circuit complexity. In particular, let us consider the following class of transformer encoders that has been the main focus of many recent papers: _Unique Hard Attention Transformers (UHAT)_. Among others, UHAT allows arbitrary positional encoding and an attention mechanism that picks a vector at a unique minimum position in the sequence that maximizes the attention score. It is known that the class of formal languages recognized by UHAT is a strict subset of the circuit complexity class \(\text{AC}^{0}\) (cf. [3, 20, 21]), i.e., each UHAT can be simulated by a family of boolean circuits of constant depth. More concretely, this entails among others that UHAT cannot compute the parity (even/oddness) of the number of occurrences of any given letter \(a\) in the input string (for strings over an alphabet containing at least two letters).

**Our first contribution** is that UHAT over data sequences (even without positional encodings) is _no longer_ contained in \(\text{AC}^{0}\), unlike the case of finite number of tokens. Instead, we show that UHAT can be captured by the circuit complexity class \(\text{TC}^{0}\), which extends \(\text{AC}^{0}\) circuits with majority gates.

**Theorem 1**.: _UHAT with positional encoding over data sequences is in \(\text{TC}^{0}\) but not in \(\text{AC}^{0}\)._

This complexity upper bound allows us to deduce the expressive power of UHAT over data sequences by using complexity theory. For example, since UHAT accepts only \(\text{TC}^{0}\) languages, successfully constructing a UHAT (e.g. through learning) for

\[\mathsf{SQRTSUM}:=\left\{(a_{1},b_{1}),\ldots,(a_{n},b_{n})\left|\sum_{i=1}^{ n}\sqrt{a_{i}}\leq\sum_{i=1}^{n}\sqrt{b_{i}},\text{and each }(a_{i},b_{i})\in\mathbb{Z}_{>0}\times\mathbb{Z}_{>0},\right.\right\},\]

would constitute a major breakthrough in complexity theory (cf. [1, 18]), i.e., showing that \(\mathsf{SQRTSUM}\) is in the complexity class \(\text{TC}^{0}\subseteq\text{P}/\text{poly}\). A byproduct of our proof is that for each length, the set of accepted sequences is a semialgebraic set. This implies, e.g., that the graph \(\{(x,e^{x})\mid x\in\mathbb{R}\}\subseteq\mathbb{R}^{2}\) of \(x\mapsto e^{x}\) (viewed as a set of length-\(1\) sequences) is not accepted by UHAT.

**Connection to regular languages over data sequences.** Recent results have revealed surprising connections between regular languages and formal languages recognizable by transformer encoders. In particular, it was proven (cf. [2]) that languages recognizable by UHAT (even _with no positional encodings_) form a strict subset of regular languages, namely those that are "star-free" or, equivalently, definable in First-Order Logic (FO), or Linear Temporal Logic (LTL). With positional encodings, similar connections hold, by extending the logics with _unary numerical predicates_ (cf. [2, 3]).

To investigate whether such connections extend to data sequences, we bring forth _formal languages theory over infinite alphabets_ (cf., [4, 11]), which has been an active research field in the last decade or so with applications to programming languages and databases (to name a few), e.g., see [11, 26, 43]. **Our second contribution** is a language over data sequences recognizable by UHAT without positional encodings that lies beyond existing formal models over infinite alphabets (in particular, "regular" ones). This shows the strength of UHAT over data sequences even without positional encodings, in stark contrast to the case of finite alphabets.

**Theorem 2**.: _There is a non-regular language over \(\Sigma=\mathbb{Q}^{d}\) that is accepted by masked UHAT with no positional encoding._

Finally, to better understand languages over data sequences recognizable by UHAT, **our third contribution** is to show how UHAT can recognize languages definable by the so-called _Locally Testable LTL_ (\((\text{LT})^{2}\text{L}\)), which extends LTL with unary numerical predicates and _local arithmetic tests_ for fixed-size windows over the input sequence. Using \((\text{LT})^{2}\text{L}\), we can see _at a glance_ what can be expressed by UHAT over data sequences. For one, this includes the well-known _Simple Moving Averages_. As another example, using \((\text{LT})^{2}\text{L}\) it can be easily shown that UHAT can capture linear recurrence sequences considered in the famous Skolem problem and discrete linear dynamical systems [25, 27, 29], i.e., sequences of the form \(\bm{x},A\bm{x},\ldots,A^{n}\bm{x}\) such that \(n\geq 0\) is minimal with \(\bm{y}A^{n}\bm{x}=0\) where \(\bm{y}\in\mathbb{Q}^{1\times d}\) and \(A\in\mathbb{Q}^{d\times d}\) are fixed and \(\bm{x}\in\mathbb{Q}^{d}\).

**Theorem 3**.: _Every \((LT)^{2}L\)-definable language is accepted by UHAT with positional encoding._

**Technical challenges.** Obtaining our results poses several challenges. First, for the TC\({}^{0}\) upper bound, we need to use Boolean (constant depth) circuits to simulate UHATs, in which real constants can occur (in affine transformations or positional encodings). While in TC\({}^{0}\), it is known that majority gates can be used to perform multiplication of rationals [7], arithmetic with reals requires infinite precision and cannot be done with Boolean circuits. To this end, we compute rational approximations of reals accurate enough to preserve the acceptance condition for inputs up to a particular length \(n\).

Here, a naive attempt would be to replace each real occurring in the UHAT in affine transformations and the positional encoding by some rational approximation. However, this is not possible, meaning _any_ rational approximation would change the behavior on input sequences of length \(n=3\), even for low-dimensional vectors with entries in \(\{0,1\}\). Indeed, there is a UHAT involving real numbers \(\alpha,\beta\) that accepts a simple sequence of \(\{0,1\}\)-vectors if and only if \(\alpha\beta=2\) and \(\alpha=\beta\), i.e. \(\alpha=\beta=\sqrt{2}\). Thus, \(\alpha\) and \(\beta\) cannot be replaced by rationals, even for very short inputs (see Appendix A for details).

Instead, we show that a UHAT can be translated into a small Boolean combination of polynomial inequalities. This format has the advantage that--as we show using convex geometry--the real coefficients of those polynomials _can_ be replaced by suitably chosen rational numbers. In turn, the layer-by-layer construction of these polynomial inequalities requires a carefully chosen data structure to encode the function computed by a sequence of transformer layers. For example, we show that the resulting Boolean combinations of polynomial inequalities have a bound on the number of alternations between conjunctions and disjunctions, which is crucial for constructing TC\({}^{0}\)-circuits.

Another key challenge occurs in the translation from \((\text{LT})^{2}\text{L}\) to UHATs: In the inductive construction, we need to represent truth values using reals in \([0,1]\). To implement negation, we use a UHAT gadget (with positional encodings) that can normalize these truth values to \(\{0,1\}\).

**Notation.** In the sequel, we assume some background from computational complexity, in particular circuit complexity (see the book [44]). In particular, we use the circuit complexity class AC\({}^{0}\), which defines a class of problems that are computable by a nonuniform family of constant-depth boolean circuits, where each gate permits an unbounded fan-in (i.e. arbitrary many inputs). Similarly, the complexity class TC\({}^{0}\) is an extension of AC\({}^{0}\), where majority gates are allowed. It is well-known that AC\({}^{0}\subsetneq\) TC\({}^{0}\). Assuming _uniformity_, both AC\({}^{0}\) and TC\({}^{0}\) are contained in the class of problems solvable in polynomial-time. For _nonuniformity_, these classes are contained in the complexity class P/poly, which admits (nonuniform) polynomial-size circuits. It is a long-standing open problem whether numerical analysis (e.g. square-root-sum) is in \(\text{P}/\text{poly}\), e.g., see [1].

## 2 Transformer encoders and their languages

In the following, we adapt the setting of formal language theory (see [3, 21, 40]) to data sequences. For a vector \(\bm{a}=(a_{1},\dots,a_{d})\) we write \(\bm{a}[i,j]:=(a_{i},\dots,a_{j})\) for all \(1\leq i\leq j\leq d\) and if \(i=j\), we simply write \(\bm{a}[i]\). For a set \(S\) we denote the set of (potentially empty) sequences of elements from \(S\) by \(S^{*}\). We write \(S^{+}\) for the restriction to non-empty sequences. We consider languages \(L\) over the infinite alphabet \(\Sigma=\mathbb{Q}^{d}\), for some integer \(d>0\). That is, \(L\) is a set of sequences of \(d\)-tuples over rational numbers. We will define a UHAT (similarly as in previous papers that study formal language theoretic perspectives) as a length preserving map \((\mathbb{Q}^{d})^{*}\rightarrow(\mathbb{R}^{e})^{*}\).

**Standard encoder layer with unique hard attention.** A standard encoder layer is defined by three affine transformations \(A,B\colon\mathbb{R}^{d}\rightarrow\mathbb{R}^{d}\) and \(C\colon\mathbb{R}^{2d}\rightarrow\mathbb{R}^{e}\). For a sequence \(\bm{v}_{1},\dots,\bm{v}_{n}\in\mathbb{R}^{d}\) with \(n\geq 1\) we define the _attention vector_ at position \(i\in[1,n]\) as \(\bm{a}_{i}:=\bm{v}_{j}\) with \(j\in[1,n]\) minimal such that the _attention score_\(\langle A\bm{v}_{i},B\bm{v}_{j}\rangle\) is maximized. The layer outputs the sequence \(C(\bm{v}_{1},\bm{a}_{1}),\dots,C(\bm{v}_{n},\bm{a}_{n})\).

**ReLU encoder layer.** A ReLU layer for some \(k\in[1,d]\) on input \(\bm{v}_{1},\dots,\bm{v}_{n}\in\mathbb{R}^{d}\) applies the ReLU function to the \(k\)-th coordinate of each \(\bm{v}_{i}\), i.e. it outputs the sequence \(\bm{v}^{\prime}_{1},\dots,\bm{v}^{\prime}_{n}\) where \(\bm{v}^{\prime}_{i}:=(\bm{v}_{i}[1,k-1],\max\{0,\bm{v}_{i}[k]\},\bm{v}_{i}[k+1,n])\). [Equivalently, one could instead allow a feed-forward network at the end of an encoder layer (see [3, 21, 40]).]

**Transformer encoder.** A _unique hard attention transformer encoder_ (UHAT) is a repeated application of standard encoder layers with unique hard attention and ReLU encoder layers. Clearly, using analternation of standard layers and ReLU layers, we can assume that the output of a UHAT layer is an arbitrary composition of affine transformations and component-wise ReLU application. In particular, these compositions may use the functions \(\max\) and \(\min\).

**Languages accepted by UHATs.** The notion of "languages" accepted by a UHAT (i.e. a set of accepted sequences) can be defined, depending on whether a _positional encoding_ is permitted. If it is permitted, a language \(L\subseteq(\mathbb{Q}^{d})^{+}\) is accepted by a UHAT \(T\) if and only if there exists a positional encoding function \(p\colon\mathbb{N}\times\mathbb{N}\to\mathbb{R}^{s}\) and an acceptance vector \(\bm{t}\in\mathbb{R}^{c}\) such that on every sequence

\[(p(1,n+1),\bm{v}_{1}),\ldots,(p(n,n+1),\bm{v}_{n}),(p(n+1,n+1),\bm{0})\] (1)

\(T\) outputs a sequence \(\bm{v}^{\prime}_{1},\ldots,\bm{v}^{\prime}_{n+1}\in\mathbb{R}^{e}\) with \(\langle\bm{t},\bm{v}^{\prime}_{1}\rangle>0\) if and only if \((\bm{v}_{1},\ldots,\bm{v}_{n})\in L\). Note that if \(T_{1}\) and \(T_{2}\) are UHATs with positional encoding that realize functions \(f_{1}\colon(\mathbb{Q}^{d})^{*}\to(\mathbb{Q}^{c})^{*}\) and \(f_{2}\colon(\mathbb{Q}^{c})^{*}\to(\mathbb{Q}^{c})^{*}\), then there is a UHAT \(T_{2}\circ T_{1}\) with positional encoding that realizes the composition \(f_{2}\circ f_{1}\) by using a positional encoding that combines the positional encodings of \(T_{1},T_{2}\).

In the above definition we appended an additional zero vector to the end of the input. Over finite alphabets it is often assumed that the input sequence is extended with a special unique end-of-input marker (e.g. see [21, 40]). When the input is a sequence of (tuples of) numbers, if we allow positional encoding, then the zero vector at the end of the input can be turned into a unique vector marking the end of the input (see Section 5). Without positional encoding, however, we have to explicitly make the zero vector at the end of the input unique. That is, a UHAT without positional encoding is initialized with the sequence \((1,\bm{v}_{1}),\ldots,(1,\bm{v}_{n}),\bm{0}\in\mathbb{Q}^{d+1}\) instead; this ensures, among others, that the end-of-input marker does not appear in the actual input.

In the definition of a standard encoder layer the attention vector at position \(i\in[1,n]\) can be any vector in the sequence \(\bm{v}_{1},\ldots,\bm{v}_{n}\). Using _masking_, one can restrict the attention vector to vectors of certain positions. A UHAT with _past masking_ restricts the attention vector \(\bm{a}_{i}\) at position \(1\leq i<n\) to be contained in the subsequence \(\bm{v}_{i+1},\ldots,\bm{v}_{n}\) and at position \(n\) to \(\bm{a}_{n}:=\bm{v}_{n}\).

## 3 UHAT and TC\({}^{0}\)

In this section, we prove Theorem 1. First, we show that all languages of UHAT (even with positional encoding) belong to the class TC\({}^{0}\). Then, we show that there is a UHAT (even without positional encoding) whose language is TC\({}^{0}\)-hard under AC\({}^{0}\)-reductions. We begin with the proof that all UHAT languages belong to TC\({}^{0}\).

Input encodingA language \(L\subseteq\Sigma^{*}\) over a finite alphabet \(\Sigma\) belongs to TC\({}^{0}\) if for every input length \(n\), there is a circuit of size polynomial \(n\), such that the circuit consists of input gates (for each input position \(i\), and each letter \(a\in\Sigma\), there is a gate that evaluates to "true" if and only if position \(i\) of the input words carries an \(a\)), Boolean gates (computing the AND, OR, or NOT function) and majority gates (evaluating to true if more than half of their input wires carry true). Here, AND, OR, and majority gates can have arbitrary fan-in. In order to define what it means that a language \(L\subseteq(\mathbb{Q}^{d})^{+}\) belongs to TC\({}^{0}\), we need to specify an encoding as finite-alphabet words. To this end, we encode a sequence \(\bm{u}_{1},\ldots,\bm{u}_{n}\) with \(\bm{u}_{i}\in\mathbb{Q}^{d}\) as a string \(v_{1}\#\cdots\#v_{n}\), where \(v_{i}=p_{1}/q_{1}\square\cdots\square p_{d}/q_{d}\) with \(p_{j},q_{j}\in\{-,0,1\}^{*}\). Here, \(v_{i}\in\{-,/,\square,0,1\}^{*}\) represents the vector \(\bm{u}_{i}\in\mathbb{Q}^{d}\) such that \(\bm{u}_{i}[j]=\frac{a_{j}}{b_{j}}\), \(a_{j},b_{j}\in\mathbb{Z}\), and \(p_{j},q_{j}\in\{-,0,1\}^{*}\) are the binary expansions of \(a_{j}\) and \(b_{j}\).

**Remark 4**.: _The main challenge in proving Theorem 1 is that the constants appearing in a UHAT can be real numbers. These can in general not be avoided: There are UHAT languages over \(\Sigma=\mathbb{Q}\) (even without positional encoding) that cannot be accepted by UHAT with rational constants (even with positional encoding). For example, for every real number \(r>0\), one can1 construct a UHAT for the language \(L_{r}\) of all sequences over \(\Sigma=\mathbb{Q}\) where the first letter is \(>r\). Note that \(L_{r}\neq L_{s}\) for any \(r,s>0\), \(r\neq s\). However, there are clearly only countably many languages over \(\Sigma=\mathbb{Q}\) accepted by UHAT with rational constants where membership only depends on the first letter2. Thus, there are uncountably many real \(r>0\) such that \(L_{r}\) is not accepted by a UHAT with rational constants._

Footnote 1: First, use a standard encoding layer to transform the sequence \((x_{1},\ldots,x_{n})\in\Sigma^{n}\) into \((r^{-1}x_{1}-1,\ldots,r^{-1}x_{n}-1)\in\Sigma^{n}\). Then, accept if and only if the left-most vector is \(>0\).

The construction of TC\({}^{0}\) circuits comprises three steps. In Step I, we show that the set of accepted length-\(n\) sequences can be represented by a Boolean combination of polynomial inequalities. Importantly, (i) this representation, called "polynomial constraints" is polynomial-sized in \(n\), and (ii) the number of alternations between conjunction and disjunction is bounded (i.e. independent of \(n\)). The polynomials in this representations can still contain real coefficients. In Step II, we show that if we restrict the input not only to length-\(n\) sequences, but to rational numbers of size \(\leq m\), then we can replace all real coefficients of our polynomials by rationals of size polynomial in \(m\) and \(n\), without changing the language (among vectors of size \(\leq m\)). In Step III, we implement a TC\({}^{0}\) circuit. Here, it is important that the number of alternations between conjunctions and disjunctions in our polynomial constraints is bounded, because the depth of the circuit is proportional to this number of alternations.

Step I: UHAT as polynomialsWe first consider a formalism to describe a set of sequences over \(\mathbb{Q}^{d}\). We consider such sequences \((\bm{x}_{1},\dots,\bm{x}_{n})\) of length \(n\), where \(\bm{x}_{i}\in\mathbb{Q}^{d}\) for each \(i\). In this case, we also abbreviate \(\bar{\bm{x}}=(\bm{x}_{1},\dots,\bm{x}_{n})\). A _polynomial constraint_ (PC) is a positive Boolean combination (i.e., without negation) of constraints of the form \(p(\bar{\bm{x}})>0\) or \(p(\bar{\bm{x}})\geq 0\), where \(p\in\mathbb{R}[X_{1},\dots,X_{d\cdot n}]\) is a polynomial with real coefficients. Here, plugging \(\bar{\bm{x}}\in(\mathbb{Q}^{d})^{n}\) into \(p\) is defined by assigning the \(d\cdot n\) rational numbers in \(\bar{\bm{x}}\) to the \(d\cdot n\) variables \(X_{1},\dots,X_{d\cdot n}\). The PC \(\alpha\)_accepts_ a sequence of vectors \(\bar{\bm{x}}\in(\mathbb{Q}^{d})^{n}\), if the Boolean formula evaluates to true when plugging \(\bar{\bm{x}}\) into the polynomials \(p\) in \(\alpha\). The set of accepted sequences is denoted by \([\alpha]\). Now let \(a\in\mathbb{N}\). A PC has _a alternations_ if the positive Boolean combination has \(a\) alternations between disjunctions and conjunctions.

In the following, a _constrained polynomial representation (CPR)_ can be used to compute from a sequence of inputs \((\bm{x}_{1},\dots,\bm{x}_{n})\) with \(\bm{x}_{1},\dots,\bm{x}_{n}\in\mathbb{R}^{d^{\prime}}\) a new sequence of outputs \((\bm{y}_{1},\dots,\bm{y}_{n})\) with \(\bm{y}_{1},\dots,\bm{y}_{n}\in\mathbb{R}^{d}\). Formally, a CPR comprises for each \(i\in\{1,\dots,n\}\) a sequence \((\varphi_{1},D_{1}),\dots,(\varphi_{s_{i}},D_{s_{i}})\) of pairs \((\varphi_{j},D_{j})\), where each pair \((\varphi_{j},D_{j})\) is a "conditional assignment": each \((\varphi_{j},D_{j})\) tells us that if the condition \(\varphi_{j}\) is satisfied, then we return \(D_{j}(\bar{\bm{x}})\). More precisely: (i) each \(\varphi_{j}\) is a polynomial constraint where all polynomials have degree \(\leq 2\), (ii) for any \(j\neq m\), the constraints \(\varphi_{j}\) and \(\varphi_{m}\) are mutually exclusive, and (iii) each \(D_{j}\colon\mathbb{R}^{d\cdot n}\to\mathbb{R}^{d}\) is an affine transformation. Because of their role as conditional assignments, we also write \(\varphi_{j}\to D_{j}\) for such pairs. For \(a\in\mathbb{N}\), we say that the CPR is _\(a\)-alternation-bounded_ if each of the formulas \(\varphi_{j}\) has at most \(a\) alternations. A CPR as above computes a function \(\mathbb{R}^{d^{\prime}\cdot n}\to\mathbb{R}^{d\cdot n}\): Given \(\bar{\bm{x}}=(\bm{x}_{1},\dots,\bm{x}_{n})\) with \(\bm{x}_{1},\dots,\bm{x}_{n}\in\mathbb{R}^{d^{\prime}}\), it computes the sequence \((\bm{y}_{1},\dots,\bm{y}_{n})\) if for every \(i\in\{1,\dots,n\}\), we have \(\bm{y}_{i}=D_{j}(\bar{\bm{x}})\), provided that \(j\) is the (in case of existence uniquely determined) index for which \(\varphi_{j}(\bar{\bm{x}})\) is satisfied. The _size_ of PCs and CPRs are their bit lengths (see Appendix B for details).

**Proposition 5**.: _Fix a UHAT with positional encoding and \(\ell\) layers. For any given sequence length \(n\), there exists a polynomial-sized PC \(\alpha\) with \(O(\ell)\) alternations such that \([\alpha]\) equals the set of accepted sequences of length \(n\)._

Note that Proposition 5 implies that the set of sequences of each length \(n\) is a semialgebraic set [31]. The proof is by induction on the number of layers, which requires a slight strengthening:

**Lemma 6**.: _Fix a UHAT with positional encoding and \(\ell\) layers. For any given sequence length \(n\), one can construct in polynomial time an \(O(\ell)\)-alternation-bounded CPR computing the function \(\mathbb{R}^{d\cdot(n+1)}\to\mathbb{R}^{e\cdot(n+1)}\) computed by the UHAT._

Proof.: We prove the statement by induction on the number of layers. First, we consider the positional encoding \(p\colon\mathbb{N}\times\mathbb{N}\to\mathbb{R}^{d}\) as some affine transformations \(P_{i}\colon\mathbb{R}^{d\cdot(n+1)}\to\mathbb{R}^{d}\) mapping the input sequence \(\bar{\bm{x}}\) to \(\bm{x}_{i}+p(i,n+1)\). Then we obtain a CPR with \(\top\to P_{i}\) for each \(1\leq i\leq n+1\). Now, suppose the statement is shown for \(\ell\) layers and consider a UHAT with \(\ell+1\) layers. Suppose that the first \(\ell\) layers of our UHAT compute a function \(\mathbb{R}^{d^{\prime}\cdot(n+1)}\to\mathbb{R}^{d\cdot(n+1)}\), and the last layer computes a function \(\mathbb{R}^{d\cdot(n+1)}\to\mathbb{R}^{e\cdot(n+1)}\). By induction, we have a polynomial size CPR consisting of conditional assignments \(\varphi_{i,k}\to D_{i,k}\) for every \(i\in\{1,\dots,n+1\}\) and \(1\leq k\leq s_{i}\). Here, each \(D_{i,j}\) is an affine transformation \(\mathbb{R}^{d^{\prime}\cdot(n+1)}\to\mathbb{R}^{d}\).

Let us first consider the case that the last layer of our UHAT is a standard encoder layer. For each \((i,I,j,J)\in\{1,\dots,n+1\}^{4}\), we build the conditional assignment using the formula \(\psi_{i,I,j,J}\):

\[\bigwedge_{m=1}^{j-1}\left(\bigvee_{M=1}^{s_{m}}\varphi_{m,M}\wedge p_{i,I,j,J, m,M}(\bar{\bm{x}})>0\right)\wedge\bigwedge_{m=j+1}^{n+1}\left(\bigvee_{M=1}^{s_{m}} \varphi_{m,M}\wedge p_{i,I,j,J,m,M}(\bar{\bm{x}})\geq 0\right)\]where \(p_{i,I,j,J,m,M}(\bar{\bm{x}})\) is the polynomial \(\langle AD_{i,I}\bar{\bm{x}},\ BD_{j,J}\bar{\bm{x}}-BD_{m,M}\bar{\bm{x}}\rangle\). Then, the conditional assignment is \(\varphi_{i,I}\wedge\varphi_{j,J}\wedge\psi_{i,I,j,J}\to C(D_{i,I}\bar{\bm{x}},D_ {j,J}\bar{\bm{x}})\). Here, the idea is that (i) \(\varphi_{i,I}\) expresses that the \(I\)-th conditional assignment was used to produce the \(i\)-th cell in the previous layer, (ii) \(\varphi_{j,J}\) says the \(J\)-th conditional assignment was used to produce the \(j\)-th vector in the previous layer, and (iii) \(\psi_{i,I,j,J}\) says the vector \(\bm{x}_{j}\) yields the maximal attention score for the input \(\bm{x}_{i}\), meaning (iii-a) for all positions \(m<j\), \(\bm{x}_{m}\) has a lower score than \(\bm{x}_{j}\) (left parenthesis), and (iii-b) for all positions \(m>j\), \(\bm{x}_{m}\) has at most the score of \(\bm{x}_{j}\) (right parenthesis). In (iii-a) and (iii-b), we first find the index \(M\) of the conditional assignment used to produce the \(m\)-th cell in the previous layer. Note that then indeed, all the PCs \(\psi_{i,I,j,I}\) are mutually exclusive. Moreover, the polynomials \(\langle AD_{i,I}\bar{\bm{x}},\ BD_{j,J}\bar{\bm{x}}-BD_{m,M}\bar{\bm{x}}\rangle\) have indeed degree \(2\) and are of size polynomial in \(n\). Furthermore, if the assignments \(\varphi_{i,k}\to D_{i,k}\) had at most \(a\) alternations, then the new assignments have at most \(a+3\) alternations. Finally, the case of ReLU layers is straightforward (see Appendix B). 

Finally, the proof of Proposition 5 is straightforward: from the constructed CPR in Lemma 6 we obtain the polynomial constraint \(\bigvee_{J=1}^{s_{1}}\varphi_{1,J}\wedge(\bm{t},D_{1,J}\bar{\bm{x}})>0\) with a bounded number of alternations.

Step II: Replace real coefficients by rationalsIn our proof, the key step is to replace the real coefficients in the PC by rational coefficients so that the rational PC will define the same set of rational sequences, up to some given size. Let us make this precise. We denote by \(\mathbb{Q}_{\leq m}=\{a\in\mathbb{Q}\mid\|a\|_{2}\leq m\}\) the set of all rational numbers of size at most \(m\). A polynomial constraint is _rational_ if all the polynomials occurring in it have rational coefficients.

**Proposition 7**.: _For every \(m\in\mathbb{N}\) and every PC \(\alpha\) with polynomials having \(n\) variables, there exists a rational PC \(\alpha^{\prime}\) of polynomial size such that \(\llbracket\alpha\rrbracket\cap\mathbb{Q}_{\leq m}^{n}=\llbracket\alpha^{\prime} \rrbracket\cap\mathbb{Q}_{\leq m}^{n}\)._

Proving Proposition 7 requires the following technical lemma, for which we introduce some notation. For two vectors \(\bm{u},\bm{v}\in\mathbb{R}^{t}\) and \(m\in\mathbb{N}\), we write \(\bm{u}\sim_{m}\bm{v}\) if for every \(\bm{w}\in\mathbb{Q}^{t}\) and every \(z\in\mathbb{Q}\) with \(\lVert\bm{w}\rVert_{2},\lVert z\rVert_{2}\leq m\), we have (i) \(\langle\bm{w},\bm{u}\rangle\geq z\) if and only if \(\langle\bm{w},\bm{v}\rangle>z\). In other words, we have \(\bm{u}\sim_{m}\bm{v}\) if and only if \(\bm{u}\) and \(\bm{v}\) satisfy exactly the same inequalities with rational coefficients of size at most \(m\).

**Lemma 8**.: _For every \(\bm{c}\in\mathbb{R}^{t}\) and \(m\in\mathbb{N}\), there is a \(\bm{c}^{\prime}\in\mathbb{Q}^{t}\) with \(\lVert\bm{c}^{\prime}\rVert_{2}\leq(mt)^{O(1)}\) and \(\bm{c}\sim_{m}\bm{c}^{\prime}\)._

**Remark 9**.: _For proving Lemma 8, it is not sufficient to pick a rational \(\bm{c}^{\prime}\) with \(\lVert\bm{c}^{\prime}-\bm{c}\rVert<\varepsilon\) for some small enough \(\varepsilon\). For example, note that if in some coordinate, \(\bm{c}\) contains a rational number of size \(\leq m\), then in this coordinate, \(\bm{c}^{\prime}\) and \(\bm{c}\) must agree exactly for \(\bm{c}\sim_{m}\bm{c}^{\prime}\) to hold._

Before we prove Lemma 8, let us see how to deduce Proposition 7: in a PC \(\alpha\) we understand each polynomial \(p(X_{1},\dots,X_{n})\) as a scalar product \(\langle\bm{w},\bm{u}\rangle\) where \(\bm{w}\) constrains only variables and \(\bm{u}\) consists of all coefficients. Then Lemma 8 yields a vector \(\bm{v}\sim_{2m}\bm{u}\) containing only rationals with the same behavior as \(\bm{u}\). From this we finally obtain polynomials having only rational coefficients, which also proves Proposition 7. A detailed proof of Proposition 7 can be found in Appendix B.

In the proof of Lemma 8, we use the following fact about solution sizes to systems of inequalities.

**Lemma 10**.: _Let \(A\in\mathbb{Q}^{k\times n}\), \(A^{\prime}\in\mathbb{Q}^{\ell\times n}\), \(\bm{z}\in\mathbb{Q}^{k}\), \(\bm{z}^{\prime}\in\mathbb{Q}^{\ell}\) with \(\lVert A\rVert_{2},\lVert A^{\prime}\rVert_{2},\lVert\bm{z}\rVert_{2}, \lVert\bm{z}^{\prime}\rVert_{2}\leq m\). If the inequalities \(A\bm{x}\gg\bm{z}\) and \(A^{\prime}\bm{x}\geq\bm{z}^{\prime}\) have a solution in \(\mathbb{R}^{n}\), then they have a solution \(\bm{r}\in\mathbb{Q}^{n}\) with \(\lVert\bm{r}\rVert_{2}\leq(mn)^{O(1)}\)._

We prove Lemma 10 in the appendix. The proof idea is the following. By standard results about polyhedra, the set of vectors \(\bm{x}\) satisfying \(A\bm{x}\geq\bm{z}\) and \(A^{\prime}\bm{x}\geq\bm{z}^{\prime}\) can be written as the convex hull of some finite set \(X=\{\bm{x}_{1},\dots,\bm{x}_{s}\}\), plus the cone generated by some finite set \(\{\bm{y}_{1},\dots,\bm{y}_{t}\}\). Here, the vectors in \(X\cup Y\) are all rational and of polynomial size. By the Caratheodory Theorem, the real solution \(\bm{s}\in\mathbb{R}^{n}\) to \(A\bm{s}\gg\bm{z}\) and \(A^{\prime}\bm{s}\geq\bm{z}^{\prime}\) belongs to the convex hull of \(n\) elements of \(X\), plus a conic combination of \(n\) elements of \(Y\). We then argue that by taking the barycenter of those \(n\) elements of \(X\), plus the sum of the \(n\) elements of \(Y\) gives a rational vector \(\bm{r}\in\mathbb{Q}^{n}\) with \(A\bm{r}\gg\bm{z}\) and \(A^{\prime}\bm{r}\geq\bm{z}^{\prime}\). The full proof of Lemma 10 is in Appendix B.4. To prove Lemma 8, given \(\bm{c}\in\mathbb{R}^{n}\), we set up a system of (exponentially many) inequalities of polynomial size so that the solutions are exactly the vectors \(\bm{d}\) with \(\bm{d}\sim_{m}\bm{c}\). The solution provided by Lemma 10 is the desired \(\bm{c}^{\prime}\) (see Appendix B.5).

Step III: Constructing \(TC^{0}\) circuitsIt is now straightforward to translate a polynomial-sized CPR with rational coefficients and bounded alternations into a TC\({}^{0}\) circuit:

**Proposition 11**.: _Every language accepted by a UHAT with positional encoding is recognized by a family of circuits in TC\({}^{0}\)._

We now show that the TC\({}^{0}\) upper bound is tight: There is a UHAT whose language is TC\({}^{0}\)-hard under AC\({}^{0}\) reductions. In particular, this language is not in AC\({}^{0}\), since AC\({}^{0}\) is strictly included in TC\({}^{0}\).

**Proposition 12**.: _There is a TC\({}^{0}\)-complete language that is accepted by a UHAT, even without positional encoding and masking, but is not recognized by any family of circuits in AC\({}^{0}\)._

Proof.: As shown by Buss [6, Corollary 3], the problem of deciding whether \(ab=c\) for given binary encoded integers \(a,b,c\in\mathbb{Z}\) is TC\({}^{0}\)-complete under AC\({}^{0}\)-reductions. Since \(ab=c\) if and only if \(ab>c-1\) and \(-ab>-(c+1)\), the problem of deciding \(ab>c\) is also TC\({}^{0}\)-complete. We exibit a UHAT such that the problem of deciding \(ab>c\) can be AC\({}^{0}\)-reduced to membership in the language.

It suffices to define a UHAT \(T\) that accepts a language \(L\subseteq(\mathbb{Q}^{2})^{+}\) such that for all \(r,s\in\mathbb{Q}\) we have that \((r,s)\in L\) if and only if \(r>s\). Then we can reduce the test \(ab>c\) to checking whether \((a,\frac{c}{b})\in L\). Note that formally, \(\frac{c}{b}\) is represented as a string containing the binary encodings of \(c\) and \(b\) separated by a special symbol. The UHAT \(T\) is by definition initialized with the sequence \((1,r,s),(0,0,0)\in\mathbb{Q}^{3}\) since we only have to consider the accepted language restricted to sequences of length \(1\). It can directly check that \(r-s>0\) using the acceptance vector \(\bm{t}:=(0,1,-1)\). 

## 4 UHAT and regular languages over infinite alphabets

It was shown by Angluin et al. [2] that UHATs with no positional encoding on binary input strings accept only regular languages, even if masking is allowed. We show that UHATs with masking over data sequences can recognize "non-regular" languages over infinite alphabet (Theorem 2). More precisely, a standard notion of regularity over the alphabet \(\Sigma=\mathbb{Q}^{d}\) is that of _symbolic automata_ (see the CACM article [11]), since it extends and shares all nice closure and algorithmic properties of finite automata over finite alphabets, while at the same time permitting arithmetics. Intuitively, a transition rule in a symbolic automaton is of the form \(p\to_{\varphi}q\), where \(\varphi\) represents the (potentially infinite) set \(S\subseteq\mathbb{Q}^{d}\) of solutions to an arithmetic constraint \(\varphi\) (e.g. \(2x=y\) represents \(\{(n,2n):n\in\mathbb{Q}\}\)). The meaning of such a transition rule is: move from state \(p\) to state \(q\) by reading any \(a\in S\).

To prove Theorem 2, we define the language

\[\mathsf{Double}:=\{(r_{1},\ldots,r_{n})\in\mathbb{Q}^{n}\mid n\geq 1\text{ and }2r_{i}<r_{i+1}\text{ for all }1\leq i<n\}\]

of sequences of rational numbers where each number is more than double the preceding number.

**Lemma 13**.: _UHAT with past masking and without positional encoding can recognize Double._

Proof.: Given an input sequence \(\bm{u}_{1},\ldots,\bm{u}_{n+1}=(1,r_{1}),\ldots,(1,r_{n}),(0,0)\in\mathbb{Q}^{2}\), we need to check that for all pairs \(1\leq i<j<n+1\), we have \(2\cdot r_{i}<r_{j}\). To this end, a first standard encoder layer uses the differences \(2r_{i}-r_{j}\) as attention scores--except for \(j=n+1\), where the attention score will be \(0\). This is achieved by setting the attention score for positions \(i,j\) to \(2\bm{u}_{i}[2]\cdot\bm{u}_{j}[1]-\bm{u}_{j}[2]\). Indeed, this evaluates to \(2r_{i}-r_{j}\) for \(i<j<n+1\), and to \(0\) for \(i<j=n+1\). In particular, for a position \(i\in[1,n]\), the attention score is maximized at \(j=n+1\) if and only if \(2r_{i}<r_{j}\) for all \(j\in[i+1,n]\).

The output vector \(\bm{v}_{i}\) at position \(i\) is then set to \(\bm{a}_{i}[1]\), where \(\bm{a}_{i}\) is the attention vector at position \(i\). Thus, the output vector has dimension \(1\), and for each \(i\in[1,n+1]\), we have \(\bm{v}_{i}=0\) if and only if \(2\cdot r_{i}<r_{j}\) holds for all \(j\in[i+1,n]\).

In a second standard encoder layer we now check whether all \(\bm{v}_{i}\) have value \(0\). To this end, we choose for \(i<j\leq n+1\) the attention score \(\bm{v}_{j}\). Let \(\bm{b}_{i}\) denote the attention vector at position \(i\). Then \(\bm{b}_{i}=0\) iff \(\bm{v}_{i+1},\ldots,\bm{v}_{n}\) are all \(0\). We then output \(\bm{w}_{i}=1-(\bm{v}_{i}+\bm{b}_{i})\), which is positive if and only if \(\bm{v}_{i}=\cdots=\bm{v}_{n+1}=0\). Finally, with the acceptance vector \(\bm{t}=1\) we accept if and only if \(\bm{w}_{1}>0\), which is equivalent to \(\bm{v}_{1}=\cdots=\bm{v}_{n+1}=0\). As we saw above, the latter holds if and only if \(2r_{i}<r_{j}\) for all \(i,j\) with \(1\leq i<j<n+1\). 

The proof of non-regularity of Double is easy (see Appendix C). One could also easily show that Double cannot be recognized by other existing models in the literature of formal language theory over infinite alphabets, e.g., register automata [4, 9, 24, 36, 41], variable/parametric automata [16, 19, 23],and data automata variants [5; 17]. For example, for register automata over \((\mathbb{Q};<)\) (see the book [4]), one could use the result therein that data sequences accepted by such an automaton are closed under any order-preserving map of the elements in the sequence (e.g., if \(1,2,3\) is accepted, then so is \(10,11,20\)), which is not satisfied by Double.

## 5 Logical languages accepted by UHAT

In this section we show that an extension of linear temporal logic (LTL) with linear rational arithmetic (LRA) and unary numerical predicates is expressible in UHAT over data sequences (Theorem 3). A formula of dimension \(d>0\) in _locally testable LTL_ (\(\mathrm{(LT)^{2}L}\)) has the following syntax:

\[\varphi\mathrel{\mathop{:}}=\psi_{k}(\bm{x}_{1},\ldots,\bm{x}_{k+1})\mid \Theta\mid\neg\varphi\mid\varphi\lor\varphi\mid\chi\varphi\mid\varphi U\varphi\]

Here, \(\psi_{k}\) for \(k\geq 0\) is an atom in LRA over the \(d\)-dimensional vectors of variables \(\bm{x}_{i}\) of the form \(\langle\bm{a},(\bm{x}_{1},\ldots,\bm{x}_{k+1})\rangle+b>0\) where \(\bm{a}\in\mathbb{Q}^{d(k+1)}\) and \(b\in\mathbb{Q}\). Intuitively, \(\psi_{k}\) allows one to check the values in the sequence with \(k\) "lookaheads". Furthermore, \(\Theta\) is a _unary numerical predicate_, i.e. a family of functions \(\theta_{n}\colon\{1,\ldots,n\}\to\{0,1\}\) for all \(n\geq 1\). We define the satisfaction of an \(\mathrm{(LT)^{2}L}\) formula \(\varphi\) over a sequence \(\bar{\bm{v}}=(\bm{v}_{1},\ldots,\bm{v}_{n})\) of vectors in \(\mathbb{Q}^{d}\) at position \(i\in[1,n]\), written \((\bar{\bm{v}},i)\models\varphi\), inductively as follows (omitting negation and disjunction):

* \((\bar{\bm{v}},i)\models\psi_{k}(\bm{x}_{1},\ldots,\bm{x}_{k+1})\) iff \(i\leq n-k\) and \(\psi_{k}(\bm{v}_{i},\ldots,\bm{v}_{i+k})\)
* \((\bar{\bm{v}},i)\models\Theta\) iff \(\theta_{n}(i)=1\)
* \((\bar{\bm{v}},i)\models X\varphi\) iff \(i<n\) and \((\bar{\bm{v}},i+1)\models\varphi\)
* \((\bar{\bm{v}},i)\models\varphi U\psi\) iff there is \(j\in[i,n]\) with \((\bar{\bm{v}},j)\models\psi\) and \((\bar{\bm{v}},k)\models\varphi\) for all \(k\in[i,j-1]\)

We write \(L(\varphi)\mathrel{\mathop{:}}=\{\bar{\bm{v}}\in(\mathbb{Q}^{d})^{+}\mid(\bar {\bm{v}},1)\models\varphi\}\) for the language of \(\varphi\).

**Example 14**.: _Consider sequences of the form \(\bm{x},A\bm{x},A^{2}\bm{x},\ldots,A^{n}\bm{x}\) such that \(\bm{y}A^{n}\bm{x}=0\) and \(n\geq 0\) is minimal with this property where \(\bm{y}\in\mathbb{Q}^{1\times d}\) and \(A\in\mathbb{Q}^{d\times d}\) are fixed and \(\bm{x}\in\mathbb{Q}^{d}\). Theorem 3 implies that this language is accepted by a UHAT since it is defined by the \((LT)^{2}L\) formula \(G[(\neg Last\to(\bm{y}\bm{x}_{1}\neq 0\wedge A\bm{x}_{1}=\bm{x}_{2}))\wedge( Last\to\bm{y}\bm{x}_{1}=0)]\), where \(Last\mathrel{\mathop{:}}=\neg X\top\)._

**Example 15**.: _Take the standard notion of 7-day Simple Moving Average (7-SMA); this can be generalized to larger sliding windows of 50-days, or 100 days, which are often used in finance. Using \((LT)^{2}L\), it is easy to show that the following notion of "uptrend" can be captured using UHAT: sequences of numbers such that the value at each time \(t\) is greater than the 7-SMA value at time \(t\). The formula for this is:_

\[G(X^{7}\top\to\varphi(x_{1},\ldots,x_{7}))\]

_where \(\varphi(\bar{x})\) is the formula \(7x_{7}\succ\sum_{i=1}^{7}x_{i}\). Note here that \(G\psi\) means (as usual for LTL) "globally" \(\psi\), which can be written as \(\neg(\top U\neg\psi)\). Similarly, \(X^{i}\) means that \(X\) is repeated \(i\) times._

We assume UHATs with positional encoding and a zero vector at the end of the input sequence (see Section 2). In the following we always assume that the components from the positional encoding are implicitly given and are not changed by any UHAT. So we write the sequence in Eq. (1) as \(\bm{v}_{1},\ldots,\bm{v}_{n},\bm{0}\). We use the following results from [3] that also hold for UHATs over data sequences.

**Lemma 16**.: _Let \(d>0\) and \(\ell\in[1,d]\)._

1. _For every_ \(b\in\{0,1\}\) _there is a UHAT with positional encoding that on every sequence_ \(\bm{v}_{1},\ldots,\bm{v}_{n}\in\mathbb{Q}^{d}\) _with_ \(v_{i}[\ell]\in\{0,1\}\) _for all_ \(i\in[1,n]\) _outputs the sequence_ \(\bm{v}_{1},\ldots,\bm{v}_{n-1},(\bm{v}_{n}[1,\ell-1],b,\bm{v}_{n}[\ell+1,d])\)_._
2. _There is a UHAT layer with positional encoding that on every sequence_ \(\bm{v}_{1},\ldots,\bm{v}_{n}\in\mathbb{Q}^{d}\) _and for every_ \(i\in[1,n-1]\) _picks attention vector_ \(\bm{a}_{i}=\bm{v}_{i+1}\)_._
3. _There is a UHAT layer with positional encoding that on every sequence_ \(\bm{v}_{1},\ldots,\bm{v}_{n}\in\mathbb{Q}^{d}\)_, for every_ \(\ell\in[1,d]\) _with_ \(\bm{v}_{1}[\ell],\ldots,\bm{v}_{n-1}[\ell]\in\{0,1\}\) _and_ \(\bm{v}_{n}[\ell]=0\)_, and for every_ \(i\in[1,n]\) _picks attention vector_ \(\bm{a}_{i}=\bm{v}_{j}\) _with minimal_ \(j\in[i,n]\) _such that_ \(\bm{v}_{j}[\ell]=0\)_._

Here, 2) and 3) directly follow from [3]. For 1) we remark that in [3] only the case \(b=0\) is shown. On input \(\bm{v}_{1},\ldots,\bm{v}_{n}\) as in 1), the UHAT uses positional encoding function \(p(i,n)\mathrel{\mathop{:}}=(i,n)\)and a composition of affine transformations and ReLU to output at position \(i\in[1,n]\) the vector \((\bm{v}_{i}[1,\ell-1],b_{i},\bm{v}_{i}[\ell+1,d])\) where \(b_{i}:=\min\{\bm{v}_{i}[\ell],n-i\}\) if \(b=0\) and \(b_{i}:=\max\{\bm{v}_{i}[\ell],i-n+1\}\) if \(b=1\).

Using Lemma16, we show that a UHAT can transform rational values \(>0\) to 1 and values \(\leq 0\) to 0. This will be used to evaluate inequalities by outputting 1 for true and 0 for false.

**Lemma 17**.: _Let \(d>0\) and \(\ell\in[1,d]\). There is a UHAT with positional encoding that on every sequence \(\bm{v}_{1},\ldots,\bm{v}_{n+1}\in\mathbb{Q}^{d}\) outputs \(\bm{v}^{\prime}_{1},\ldots,\bm{v}^{\prime}_{n+1}\in\mathbb{Q}^{d}\) with \(\bm{v}^{\prime}_{i}:=(\bm{v}_{i}[1,\ell-1],b_{i},\bm{v}_{i}[\ell+1,d])\) for all \(i\in[1,n+1]\) where \(b_{i}:=1\) if \(\bm{v}_{i}[\ell]>0\) and \(b_{i}:=0\) otherwise._

Proof.: On input \(\bm{v}_{1},\ldots,\bm{v}_{n+1}\in\mathbb{Q}^{d}\), the first layer outputs at position \(i\in[1,n+1]\) the vector \(\bm{w}_{i}:=(\bm{v}_{i}[1,\ell-1],r_{i},\bm{v}_{i}[\ell+1,d])\) where \(r_{i}:=\max\{\bm{v}_{i}[\ell],0\}\). Thus, \(r_{i}=0\) if \(\bm{v}_{i}[\ell]\leq 0\) and \(r_{i}>0\) otherwise. The second layer turns the sequence \(\bm{w}_{1},\ldots,\bm{w}_{n+1}\) into \((0,\bm{w}_{1}),\ldots,(0,\bm{w}_{n+1})\). We then apply 1) of Lemma16 to obtain the sequence \((0,\bm{w}_{1}),\ldots,(0,\bm{w}_{n}),(1,\bm{w}_{n+1})\), i.e. the last vector has first component 1, and all other vectors have first component 0. Let \(\bm{u}_{1},\ldots,\bm{u}_{n+1}\in\mathbb{Q}^{d+1}\) be the resulting sequence. The final layer uses attention score \(\langle A\bm{u}_{i},B\bm{u}_{j}\rangle\) for all \(1\leq i,j\leq n+1\) where the affine transformations \(A,B\colon\mathbb{Q}^{d+1}\to\mathbb{Q}^{d+1}\) yield \(A\bm{u}_{i}=(\bm{u}_{i}[\ell],0,\ldots,0)\) and \(B\bm{u}_{j}=(\bm{u}_{j}[1],0,\ldots,0)\). Let \(\bm{a}_{i}\) be the attention vector of position \(i\in[1,n+1]\). Since \(\bm{u}_{i}[\ell]\geq 0\), we have \(\bm{a}_{i}[1]=0\) if \(\bm{u}_{i}[\ell]=0\) and \(\bm{a}_{i}[1]=1\) if \(\bm{u}_{i}[\ell]>0\). The layer outputs \(\bm{v}^{\prime}_{i}:=(\bm{u}_{i}[2,\ell],\bm{a}_{i}[1],\bm{u}_{i}[\ell+2,d+1])\) at position \(i\in[1,n+1]\). 

We now prove Theorem3. We claim that for every \((\text{LT})^{2}\text{L}\) formula \(\varphi\) of dimension \(d\) and every \(m\geq d\) there exists a UHAT \(T_{\varphi,m}\) with positional encoding that on every sequence \(\bm{w}_{1},\ldots,\bm{w}_{n},\bm{0}\in\mathbb{Q}^{m}\) outputs a sequence \(\bm{w}^{\prime}_{1},\ldots,\bm{w}^{\prime}_{n},\bm{0}\in\mathbb{Q}^{m+1}\) such that for all \(i\in[1,n]\) we have \(\bm{w}^{\prime}_{i}[1,m]=\bm{w}_{i}\) and \(\bm{w}^{\prime}_{i}[m+1]=1\) if \((\bar{\bm{v}},i)\models\varphi\) and \(\bm{w}^{\prime}_{i}[m+1]=0\) otherwise, where \(\bar{\bm{v}}:=(\bm{w}_{1}[1,d],\ldots,\bm{w}_{n}[1,d])\). Then the theorem follows since for every \((\text{LT})^{2}\text{L}\) formula \(\varphi\) of dimension \(d\) the UHAT \(T_{\varphi,d}\) outputs on every sequence \(\bar{\bm{v}}=(\bm{v}_{1},\ldots,\bm{v}_{n})\) of vectors in \(\mathbb{Q}^{d}\) extended with the vector \(\bm{0}\in\mathbb{Q}^{d}\) a sequence \(\bm{v}^{\prime}_{1},\ldots,\bm{v}^{\prime}_{n},\bm{0}\in\mathbb{Q}^{d+1}\) such that \(\bm{v}^{\prime}_{1}[d+1]>0\) if and only if \((\bar{\bm{v}},1)\models\varphi\). Thus, \(T_{\varphi,d}\) accepts \(L(\varphi)\) by taking the acceptance vector \(\bm{t}:=(0,\ldots,0,1)\in\mathbb{Q}^{d+1}\).

We prove the claim by induction on the structure of \((\text{LT})^{2}\text{L}\) formulas. If the formula is a unary numerical predicate \(\Theta\), then we can use the positional encoding \(p(i,n+1):=\theta_{n}(i)\) for all \(i\in[1,n]\) and \(p(n+1,n+1):=0\) to output on every sequence \(\bm{w}_{1},\ldots,\bm{w}_{n},\bm{0}\in\mathbb{Q}^{m}\) the sequence \((\bm{w}_{1},p(1,n+1)),\ldots,(\bm{w}_{n},p(n,n+1)),(\bm{0},p(n+1,n+1))\in \mathbb{Q}^{m+1}\).

If the formula is an atom \(\psi_{k}(\bm{x}_{1},\ldots,\bm{x}_{k+1})\) of the form \(\langle\bm{a},(\bm{x}_{1},\ldots,\bm{x}_{k+1})\rangle+b>0\), the UHAT \(T_{\psi_{k},m}\) adds in its first layer a component that is set to 1 to the top of every vector, outputting on every sequence \(\bm{w}_{1},\ldots,\bm{w}_{n},\bm{0}\in\mathbb{Q}^{m}\) the sequence \((1,\bm{w}_{1}),\ldots,(1,\bm{w}_{n}),(1,\bm{0})\). Then we apply 1) of Lemma16 to turn this sequence into \((1,\bm{w}_{1}),\ldots,(1,\bm{w}_{n}),\bm{0}\). Next, \(T_{\psi_{k},m}\) uses \(k\) layers to allow each position to gather the first \(d+1\) components of its \(k\) right neighbors. More precisely, the \(\ell\)-th layer, for \(\ell\in[1,k]\), on sequence \(\bm{u}_{1},\ldots,\bm{u}_{n+1}\) uses 2) of Lemma16 to get for every position \(i\in[1,n]\) the attention vector \(\bm{a}_{i}=\bm{u}_{i+1}\) and the attention vector \(\bm{a}_{n+1}\) is arbitrary. Note that if \(\ell=1\), then \(\bm{a}_{n}=\bm{0}\). Then it applies an affine transformation to output at position \(i\in[1,n]\) the vector \((\bm{a}_{i}[1,d+1],\bm{u}_{i})\) and using 1) at position \(n+1\) the vector \((0,\bm{a}_{n+1}[2,d+1],\bm{u}_{n+1})\). Let \(\bm{u}_{1},\ldots,\bm{u}_{n+1}\in\mathbb{Q}^{k(d+1)+m+1}\) be the output of the \(k\)-th of those layers. We add another layer that using a composition of ReLU and affine functions outputs at every position \(i\in[1,n+1]\) the vector \(\bm{u}^{\prime}_{i}:=(\bm{u}_{i}[k(d+1)+2,k(d+1)+m+1],r_{i})\in\mathbb{Q}^{m+1}\) where \(r_{i}:=\min\{\bm{u}_{i}[1],(\bm{a},\hat{\bm{u}}_{i})+b\}\) and \(\hat{\bm{u}}_{i}:=(\bm{u}_{i}[2,d+1],\bm{u}_{i}[d+3,2(d+1)],\ldots,\bm{u}_{i}[ kd+k+2,(k+1)(d+1)])\),

which contains the first \(d\) components of the initial input vector and its \(k\) right neighbors. That is, for all \(i\in[1,n]\) we have that \(r_{i}=0\) if \(\langle\bm{a},\hat{\bm{u}}_{i}\rangle+b\leq 0\) or \(i+k>n\) since \(\bm{u}_{i}[1]\) can only be 0 if it was gathered from the vector at position \(n+1\) using attention. Furthermore, \(r_{i}>0\) if \(\langle\bm{a},\hat{\bm{u}}_{i}\rangle+b>0\) and \(i+k\leq n\). Note that \(\bm{u}^{\prime}_{i}[1,m]\) is equal to the input vector \(\bm{w}_{i}\) from the beginning if \(i\in[1,n]\) and \(\bm{0}\) if \(i=n+1\). Finally, we apply Lemma17 followed by 1) to output at position \(i\in[1,n]\) the vector \(\bm{w}^{\primeFor the cases \(\neg\varphi\), \(\varphi\lor\psi\), and \(X\varphi\) we refer to Appendix D. For \(\varphi U\psi\) define the UHAT \(T_{\varphi U\psi,m}\) that first applies \(T_{\psi,m+1}\circ T_{\varphi,m}\) outputting a sequence \(\bm{u}_{1},\dots,\bm{u}_{n},\bm{0}\in\mathbb{Q}^{m+2}\). Observe that \((\tilde{\bm{v}},i)\models\varphi U\psi\) for \(i\in[1,n]\) and \(\tilde{\bm{v}}:=(\bm{u}_{1}[1,d],\dots,\bm{u}_{n}[1,d])\) if and only if for the minimal \(j\in[i,n]\) with \((\tilde{\bm{v}},j)\models\neg\varphi\lor\psi\) we have \((\tilde{\bm{v}},j)\models\psi\) and such a \(j\) exists. Equivalently, for the minimal \(j\in[i,n+1]\) with \(\min\{\bm{u}_{j}[m+1],1-\bm{u}_{j}[m+2]\}=0\) it holds that \(\bm{u}_{j}[m+2]=1\). To check this, we first add a layer that outputs at position \(i\in[1,n+1]\) the vector \(\bm{u}^{\prime}_{i}:=(\bm{u}_{i},\min\{\bm{u}_{i}[m+1],1-\bm{u}_{i}[m+2]\}) \in\mathbb{Q}^{m+3}\). Finally, we add a layer that uses 3) of Lemma 16 to get attention vector \(\bm{a}_{i}=\bm{u}^{\prime}_{j}\) with \(j\in[i,n+1]\) minimal such that \(\bm{u}^{\prime}_{j}[m+3]=0\). The layer then outputs at position \(i\in[1,n+1]\) the vector \(\bm{w}^{\prime}_{i}:=(\bm{u}^{\prime}_{i}[1,m],\bm{a}_{i}[m+2])\in\mathbb{Q}^{ m+1}\).

## 6 Concluding remarks

We initiated the study of the expressive power of transformers, when the input is a sequence of (tuples of) numbers, which is the setting for applications like time series analysis/forecasting. Our results indicate an increased expressiveness of transformers on such input data, in comparison to the previous formal language theoretic setting (see survey [40]), i.e., when a token embedding function (with a bounded number of tokens) is first applied before feeding the input to a transformer. More precisely, this represents for Unique Hard Attention Transformers (UHAT) a jump from the complexity class \(\text{AC}^{0}\) to \(\text{TC}^{0}\) (since \(\text{AC}^{0}\subseteq\text{TC}^{0}\)), and the jump from regular to non-regular languages (when position encoding is not allowed). On the positive side, we successfully developed an expressive class of logical languages recognized by UHAT in terms of a logic called locally testable LTL, which extends previously identified logic for UHAT for strings over finite alphabets [2; 3].

**Limitations.** While we follow the standard formalization of transformer encoders in Formal Languages and Neural Networks (e.g. [3; 20; 21; 40]), limitations of the models are known (see [40] for a thorough discussion). For example, used real numbers could be of _unbounded_ precision, which allow one to precisely represent values of \(\sin\) and \(\cos\) functions (actually used in practice for positional encoding). In addition, the positional encoding used by the model could be _uncomputable_. Three answers can be given. First, an _upper bound complexity_ on the model with unbounded precision and arbitrary positional encodings (e.g. in \(\text{TC}^{0}\)) still applies in the case of bounded precision. Second, limiting the power of UHAT (e.g. allow only rational numbers, and assuming efficient (i.e. uniform \(\text{TC}^{0}\)) computability of the positional encoding \(p:\mathbb{N}\times\mathbb{N}\rightarrow\mathbb{Q}^{d}\)), our proof in fact yields _uniformity_ of our \(\text{TC}^{0}\) upper bound. Third, our lower bound for non-regularity of UHAT (cf. Theorem 2) holds even with only rational numbers and no positional encodings. Finally, to alleviate these issues, we have always made an explicit distinction between UHAT with and without positional encodings.

**Future directions.** Our paper opens up a plethora of research avenues on the expressive power of transformers on data sequences. In particular, one could consider other transformer encoder models that have been considered in the formal language theoretic setting to transformers (see the survey [40]). For example, instead of unique hard attention mechanism, we could consider the expressive power of transformers on data sequences using _average hard attention_ mechanism. Similar question could be asked if we use a softmax function instead of a hard attention, which begs the question of which numerical functions could be computed in different circuit complexity classes like \(\text{TC}^{0}\). Another important question concerns a logical characterization for UHAT over sequences of numbers. This is actually still an open question even for the case of finite alphabets. Barcelo et al. [3] showed that first-order logic (equivalently, LTL) with monadic numerical predicates (called LTL(Mon)) is subsumed in UHAT with arbitrary position encodings, i.e., the transformer model that we are generalizing in this paper to sequences of numbers. There are UHAT languages (e.g. the set of palindromes) that are not captured by this. As remarked in [3], LTL(Mon) can be extended with arbitrary linear orders on the positions (parameterized by lengths), which then can define the palindromes. [An analogous extension for \((\text{LT})^{2}\text{L}\) can define palindromes over an infinite alphabet.] However, it is possible to show that the resulting logic is still not expressive enough to capture the full generality of UHAT. That said, although our logic does not capture the full UHAT, it can still be used to see at a glance what languages can be recognized by UHAT (e.g. Simple Moving Averages). There could perhaps be a hope of obtaining a precise logical characterization if we restrict the model of UHAT. The recent paper [2] showed that LTL(Mon) captures precisely the languages of masked UHAT with position encodings with "finite image". It is interesting to study similar restrictions for UHAT in the case of sequences of numbers.

#### Acknowledgments

Funded by the European Union (ERC, LASD, 101089343 and FINABIS, 101077902). Views and opinions expressed are however those of the authors only and do not necessarily reflect those of the European Union or the European Research Council Executive Agency. Neither the European Union nor the granting authority can be held responsible for them.

## References

* Allender et al. [2006] Allender, E., Burgisser, P., Kjeldgaard-Pedersen, J., and Miltersen, P. B. (2006). On the complexity of numerical analysis. In _21st Annual IEEE Conference on Computational Complexity (CCC 2006), 16-20 July 2006, Prague, Czech Republic_, pages 331-339. IEEE Computer Society.
* Angluin et al. [2023] Angluin, D., Chiang, D., and Yang, A. (2023). Masked hard-attention transformers and boolean RASP recognize exactly the star-free languages. _CoRR_, abs/2310.13897.
* Barcelo et al. [2024] Barcelo, P., Kozachinskiy, A., Lin, A. W., and Podolskii, V. (2024). Logical languages accepted by transformer encoders with hard attention. In _ICLR_.
* Bojanczyk [2019] Bojanczyk, M. (2019). _Slightly Infinite Sets_. See https://www.mimuw.edu.pl/~bojan/upload/main-10.pdf.
* Bojanczyk et al. [2011] Bojanczyk, M., David, C., Muscholl, A., Schwentick, T., and Segoufin, L. (2011). Two-variable logic on data words. _ACM Trans. Comput. Log._, 12(4):27:1-27:26.
* Buss [1992] Buss, S. R. (1992). The graph of multiplication is equivalent to counting. _Inf. Process. Lett._, 41(4):199-201.
* Chandra et al. [1984] Chandra, A. K., Stockmeyer, L., and Vishkin, U. (1984). Constant depth reducibility. _SIAM Journal on Computing_, 13(2):423-439.
* Chiang et al. [2023] Chiang, D., Cholak, P., and Pillay, A. (2023). Tighter bounds on the expressivity of transformer encoders. In _ICML_, volume 202, pages 5544-5562. PMLR.
* 31st International Conference, CAV 2019, New York City, NY, USA, July 15-18, 2019, Proceedings, Part I_, volume 11561 of _Lecture Notes in Computer Science_, pages 3-21. Springer.
* 29th International Conference, CAV 2017, Heidelberg, Germany, July 24-28, 2017, Proceedings, Part I_, volume 10426 of _Lecture Notes in Computer Science_, pages 47-67. Springer.
* D'Antoni and Veanes [2021] D'Antoni, L. and Veanes, M. (2021). Automata modulo theories. _Commun. ACM_, 64(5):86-95.
* Devlin et al. [2019] Devlin, J., Chang, M., Lee, K., and Toutanova, K. (2019). BERT: pre-training of deep bidirectional transformers for language understanding. In Burstein, J., Doran, C., and Solorio, T., editors, _Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers)_, pages 4171-4186. Association for Computational Linguistics.
* Dong et al. [2018] Dong, L., Xu, S., and Xu, B. (2018). Speech-transformer: A no-recurrence sequence-to-sequence model for speech recognition. In _2018 IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2018, Calgary, AB, Canada, April 15-20, 2018_, pages 5884-5888. IEEE.
* Dosovitskiy et al. [2021] Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., and Houlsby, N. (2021). An image is worth 16x16 words: Transformers for image recognition at scale. In _9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021_. OpenReview.net.
* DuSell and Chiang [2024] DuSell, B. and Chiang, D. (2024). Stack attention: Improving the ability of transformers to model hierarchical patterns. In _Proc. ICLR_, Vienna, Austria.

* Faran and Kupferman [2018] Faran, R. and Kupferman, O. (2018). LTL with arithmetic and its applications in reasoning about hierarchical systems. In Barthe, G., Sutcliffe, G., and Veanes, M., editors, _LPAR-22. 22nd International Conference on Logic for Programming, Artificial Intelligence and Reasoning, Awassa, Ethiopia, 16-21 November 2018_, volume 57 of _EPiC Series in Computing_, pages 343-362. EasyChair.
* Figueira and Lin [2022] Figueira, D. and Lin, A. W. (2022). Reasoning on data words over numeric domains. In _LICS_, pages 37:1-37:13. ACM.
* Garey et al. [1976] Garey, M. R., Graham, R. L., and Johnson, D. S. (1976). Some NP-complete geometric problems. In Chandra, A. K., Wotschke, D., Friedman, E. P., and Harrison, M. A., editors, _Proceedings of the 8th Annual ACM Symposium on Theory of Computing, May 3-5, 1976, Hershey, Pennsylvania, USA_, pages 10-22. ACM.
* Grumberg et al. [2010] Grumberg, O., Kupferman, O., and Sheinvalid, S. (2010). Variable automata over infinite alphabets. In Dediu, A., Fernau, H., and Martin-Vide, C., editors, _Language and Automata Theory and Applications, 4th International Conference, LATA 2010, Trier, Germany, May 24-28, 2010. Proceedings_, volume 6031 of _Lecture Notes in Computer Science_, pages 561-572. Springer.
* Hahn [2020] Hahn, M. (2020). Theoretical limitations of self-attention in neural sequence models. _Trans. Assoc. Comput. Linguistics_, 8:156-171.
* Hao et al. [2022] Hao, Y., Angluin, D., and Frank, R. (2022). Formal language recognition by hard attention transformers: Perspectives from circuit complexity. _Trans. Assoc. Comput. Linguistics_, 10:800-810.
* Hesse et al. [2002] Hesse, W., Allender, E., and Barrington, D. A. M. (2002). Uniform constant-depth threshold circuits for division and iterated multiplication. _J. Comput. Syst. Sci._, 65(4):695-716.
* Jez et al. [2023] Jez, A., Lin, A. W., Markgraf, O., and Rummer, P. (2023). Decision procedures for sequence theories. In _CAV (2)_, volume 13965 of _Lecture Notes in Computer Science_, pages 18-40. Springer.
* Kaminski and Francez [1994] Kaminski, M. and Francez, N. (1994). Finite-memory automata. _Theor. Comput. Sci._, 134(2):329-363.
* Essays Dedicated to Thomas A. Henzinger on the Occasion of His 60th Birthday_, volume 13660 of _Lecture Notes in Computer Science_, pages 21-38. Springer.
* Libkin et al. [2016] Libkin, L., Martens, W., and Vrgoc, D. (2016). Querying graphs with data. _J. ACM_, 63(2):14:1-14:53.
* 5, 2022_, pages 5:1-5:9. ACM.
* Liu et al. [2023] Liu, Y., Hu, T., Zhang, H., Wu, H., Wang, S., Ma, L., and Long, M. (2023). itransformer: Inverted transformers are effective for time series forecasting. _CoRR_, abs/2310.06625. To appear in ICLR'24.
* Luca et al. [2023] Luca, F., Maynard, J., Noubissie, A., Ouaknine, J., and Worrell, J. (2023). Skolem meets Bateman-Horn. _CoRR_, abs/2308.01152.
* Merrill et al. [2022] Merrill, W., Sabharwal, A., and Smith, N. A. (2022). Saturated transformers are constant-depth threshold circuits. _Trans. Assoc. Comput. Linguistics_, 10:843-856.
* Mishra [1993] Mishra, B. (1993). _Algorithmic Algebra_. Texts and Monographs in Computer Science. Springer.
* Perez et al. [2021] Perez, J., Barcelo, P., and Marinkovic, J. (2021). Attention is turing-complete. _J. Mach. Learn. Res._, 22:75:1-75:35.

* Reif (1987) Reif, J. H. (1987). On threshold circuits and polynomial computation. In _Proceedings of the Second Annual Conference on Structure in Complexity Theory, Cornell University, Ithaca, New York, USA, June 16-19, 1987_, pages 118-123. IEEE Computer Society.
* Reif and Tate (1992) Reif, J. H. and Tate, S. R. (1992). On threshold circuits and polynomial computation. _SIAM J. Comput._, 21(5):896-908.
* Schrijver (1986) Schrijver, A. (1986). _Theory of linear and integer programming_. John Wiley & Sons.
* Leibniz-Zentrum fur Informatik.
* Sipser (1997) Sipser, M. (1997). _Introduction to the Theory of Computation_. PWS Publishing Company.
* Strobl (2023) Strobl, L. (2023). Average-hard attention transformers are constant-depth uniform threshold circuits. _CoRR_, abs/2308.03212.
* Strobl et al. (2024) Strobl, L., Angluin, D., Chiang, D., Rawski, J., and Sabharwal, A. (2024). Transformers as transducers. _CoRR_, abs/2404.02040.
* Strobl et al. (2023) Strobl, L., Merrill, W., Weiss, G., Chiang, D., and Angluin, D. (2023). Transformers as recognizers of formal languages: A survey on expressivity.
* Torunczyk and Zeume (2022) Torunczyk, S. and Zeume, T. (2022). Register automata with extrema constraints, and an application to two-variable logic. _Log. Methods Comput. Sci._, 18(1).
* Vaswani et al. (2017) Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I. (2017). Attention is all you need. In _NeurIPS_, pages 5998-6008.
* Veanes et al. (2012) Veanes, M., Hooimeijer, P., Livshits, B., Molnar, D., and Bjorner, N. S. (2012). Symbolic finite state transducers: algorithms and applications. In Field, J. and Hicks, M., editors, _Proceedings of the 39th ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages, POPL 2012, Philadelphia, Pennsylvania, USA, January 22-28, 2012_, pages 137-150. ACM.
* Vollmer (1999) Vollmer, H. (1999). _Introduction to Circuit Complexity_. Springer.
* Wen et al. (2023) Wen, Q., Zhou, T., Zhang, C., Chen, W., Ma, Z., Yan, J., and Sun, L. (2023). Transformers in time series: A survey. In _IJCAI_, pages 6778-6786. ijcai.org.
* Zhou et al. (2021) Zhou, H., Zhang, S., Peng, J., Zhang, S., Li, J., Xiong, H., and Zhang, W. (2021). Informer: Beyond efficient transformer for long sequence time-series forecasting. In _Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021, Virtual Event, February 2-9, 2021_, pages 11106-11115. AAAI Press.

Example UHAT with real parameters

We present here an example UHAT in which two real numbers \(\alpha\) and \(\beta\) occur (each occurs once in a matrix associated to a particular layer) such that the simple sequence

\[(1,0,\bm{e}_{1})(1,0,\bm{e}_{2})(1,0,\bm{e}_{3}),\] (2)

where \(\bm{e}_{i}\in\mathbb{R}^{3}\) is the \(i\)-th unit vector, is accepted if and only if \(\alpha\beta=2\) and \(\alpha=\beta\). This shows that even if we restrict the input sequence to a particular number \(B\) of bits (e.g. the number of bits to represent the input sequence Eq. (2)), it is not possible to replace the real constants \(\alpha\) and \(\beta\) by rational numbers without changing the accepted sequences of up to \(B\) bits: The sequence Eq. (2) is only accepted if \(\alpha=\beta=\sqrt{2}\). And if we change \(\alpha\) or \(\beta\) in any way, the sequence Eq. (2) will not be accepted anymore.

1. Input layer: \[(1,0,\bm{e}_{1})(1,0,\bm{e}_{2})(1,0,\bm{e}_{3}).\]
2. Using a standard encoding layer, we multiply the first component in each position by \(\alpha\). Result: \[(\alpha,0,\bm{e}_{1})(\alpha,0,\bm{e}_{2})(\alpha,0,\bm{e}_{3}).\]
3. Using attention, we can apply distinct affine transformations to the first three positions. We choose the following affine transformations. The first position is unchanged. The second position is mapped to \((1,0,\bm{e}_{2})\), and the third position is mapped to \((0,\alpha,\bm{e}_{3})\), using a matrix that flips the first and second component. Result: \[(\alpha,0,\bm{e}_{1})(1,0,\bm{e}_{2})(0,\alpha,\bm{e}_{3}).\]
4. Using a standard encoding layer, we multiply the first component in each position with \(\beta\). Result: \[(\alpha\beta,0,\bm{e}_{1})(\beta,0,\bm{e}_{2})(0,\alpha,\bm{e}_{3}).\]
5. Finally, by using our result on \((\text{LT})^{2}\text{L}\), we can build further layers so that we accept if and only if (i) the first component of the first position equals \(2\) and (ii) the first component of the second position equals the second component of the third position. Thus, we accept our original input if and only if \(\alpha\beta=2\) and \(\alpha=\beta\).

## Appendix B Omitted definitions and proofs in Section 3

### Descriptional size

Let \(x\in\mathbb{R}\) be some real number. The _(descriptional) size_ of \(x\) is \(\operatorname{size}(x)=1+\lceil\log_{2}(|p|+1)\rceil+\lceil\log_{2}(|q|+1)\rceil\) if \(x=\frac{p}{q}\) is a rational number (where \(p\) and \(q\) are relatively prime integers) and \(\operatorname{size}(x)=1\) if \(x\) is an irrational number. Note that in the latter case we use the number \(1\) as some placeholder, since we do not have to represent irrational numbers in any algorithm. However, for analysis of the sizes in our constructions we still need some value.

Let \(\bm{v}\in\mathbb{R}^{d}\) be a vector. The _size_ of \(\bm{v}\) is \(\operatorname{size}(\bm{v})=n+\sum_{i=1}^{d}\operatorname{size}(v_{i})\) where \(\bm{v}=(v_{1},\ldots,v_{d})^{T}\).

Let \(M\in\mathbb{R}^{m\times n}\) be a matrix. The _size_ of \(M\) is \(\operatorname{size}(M)=mn+\sum_{1\leq i\leq m,1\leq j\leq n}\operatorname{ size}(a_{ij})\) where \(M=(a_{ij})_{1\leq i\leq m,1\leq j\leq n}\).

Let \(A\colon\mathbb{R}^{d}\to\mathbb{R}^{e}\) be an affine transformation, i.e., we have \(A(\bm{x})=B\bm{x}+\bm{c}\) with \(B\in\mathbb{R}^{d\times e}\) and \(\bm{c}\in\mathbb{R}^{e}\). Then the _size_ of \(A\) is \(\operatorname{size}(A)=\operatorname{size}(B)+\operatorname{size}(c)+1\).

Now, let \(p\in\mathbb{R}[X_{1},\ldots,X_{n}]\) be a polynomial. Then we have \(p(X_{1},\ldots,X_{n})=\sum_{0\leq r_{1},\ldots,r_{n}\leq k}c_{r_{1},\ldots,r_{ n}}X_{1}^{r_{1}}\cdots X_{n}^{r_{n}}\) for some numbers \(k\in\mathbb{N}\) and \(c_{r_{1},\ldots,r_{n}}\in\mathbb{R}\). The _size_ of \(p\) is

\[\operatorname{size}(p)=\sum_{0\leq r_{1},\ldots,r_{n}\leq k}\operatorname{ size}(c_{r_{1},\ldots,r_{n}})+\operatorname{size}(r_{1})+\cdots+\operatorname{size}(r_{n})+n\,.\]

Let \(\alpha\) be a polynomial constraint. We define the _size_ of \(\alpha\) inductively on the structure of the formula as follows:* if \(\alpha=(p(X_{1},\ldots,X_{n})\sim 0)\) with \(\sim\in\{>,\geq\}\) is an atom. Then \(\operatorname{size}(\alpha)=\operatorname{size}(p)+1\).
* if \(\alpha=\bigwedge_{1\leq i\leq k}\beta_{i}\) or \(\alpha=\bigvee_{1\leq i\leq k}\beta_{i}\) is a formula with PCs \(\beta_{1},\ldots,\beta_{k}\), then \(\operatorname{size}(\alpha)=k+\sum_{1\leq i\leq k}\operatorname{size}(() \beta_{i})\).

Let \(R=(\phi_{1},D_{1}),\ldots,(\phi_{k},D_{k})\) be a CPR. Then the _size_ of this CPR is \(\operatorname{size}(R)=k+\sum_{i=1}^{k}\operatorname{size}(\phi_{i})+ \operatorname{size}(D_{k})\).

### ReLU case in Lemma 6

Let us now consider a ReLU layer. Assume that we compute the ReLU-value for the \(j\)-th component, i.e., we compute \(\max\{0,x_{i,j}\}\) for each \(i\in\{1,\ldots,n+1\}\) where \(x_{i,j}\) is the \(j\)-th component of \(\bm{x}_{i}\). From each conditional assignment \(\varphi_{i,k}\to D_{i,k}\) with \(i\in\{1,\ldots,n+1\}\) and \(1\leq k\leq s_{i}\) we construct two new conditional assignments:

1. \(\langle\bm{e}_{(i-1)\cdot n+j},D_{i,k}\bar{\bm{x}}\rangle\geq 0\to D_{i,k}\) where \(\bm{e}_{h}\) is the \(h\)-th unit vector.
2. \(\langle-\bm{e}_{(i-1)\cdot n+j},D_{i,k}\bar{\bm{x}}\rangle>0\to MD_{i,k}\) where \(M=(m_{gh})_{1\leq g,h\leq d^{\prime}\cdot(n+1)}\) is the matrix with \(m_{gh}=1\) if \(g=h\neq(i-1)\cdot n+j\) and \(m_{gh}=0\) otherwise (i.e., \(M\) is the unit matrix except for the \((i-1)\cdot j\)-th entry).

Now, if the \(j\)-th component of \(\bm{x}_{i}\) is non-negative, only the first conditional assignment is satisfied and the value of this component is left untouched. Otherwise, the \(j\)-th component is negative. But then the second conditional assignment is satisfied and the value of this component is set to \(0\) (while the others stay unchanged). So, we obtain again some polynomial sized CPR with the same number of alternations as before.

### Omitted proofs

Proof of Proposition 5.: Let \(\bm{t}\in\mathbb{R}^{e}\) be the acceptance criterion of the UHAT and \(f\colon\mathbb{R}^{d\cdot(n+1)}\to\mathbb{R}^{e\cdot(n+1)}\) be the computed function for inputs of length \(n\). By Lemma 6, we can construct in polynomial time an \(O(\ell)\)-alternation-bounded CPR computing \(f\). So, let \(\varphi_{i,k}\to D_{i,k}\) be the conditional assignments in this CPR (for \(1\leq i\leq n+1\) and \(1\leq k\leq s_{i}\)). Then we obtain a PC from this CPR as follows:

\[\bigvee_{J=1}^{s_{1}}\varphi_{1,J}\wedge\langle\bm{t},D_{1,J}\bar{\bm{x}} \rangle>0\,.\]

Note that this PC has still polynomial size, accepts an input sequence \((\bm{x}_{1},\ldots,\bm{x}_{n},\bm{0})\) if, and only if, the UHAT accepts \((\bm{x}_{1},\ldots,\bm{x}_{n})\), and -- if the CPR is \(a\)-alternation-bounded -- then it has at most \(a+2\) alternations of disjunctions and conjunctions. 

Proof of Proposition 7.: Consider a constraint \(p(X_{1},\ldots,X_{n})>0\) (or \(\geq 0\) resp.) in \(\alpha\). Then \(p\in\mathbb{R}[X_{1},\ldots,X_{n}]\) is a polynomial of degree at most \(2\), i.e., there are real numbers \(c_{i,j,r,s}\in\mathbb{R}\) such that

\[p(X_{1},\ldots,X_{n})=\sum_{0\leq r+s\leq 2}\sum_{1\leq i\leq j\leq n}c_{i,j,r,s }X_{i}^{r}X_{j}^{s}\,.\]

Now, construct two vectors \(\bm{u}\) and \(\bm{w}\) with a component for each tuple \((i,j,r,s)\): \(u_{i,j,r,s}=c_{i,j,r,s}\) and \(w=i,j,r,s=X_{i}^{r}X_{j}^{s}\). Then it is clear that \(p(X_{1},\ldots,X_{n})=\langle\bm{u},\bm{w}(X_{1},\ldots,X_{n})\rangle\) holds.

Application of Lemma 8 yields a vector \(\bm{v}\in\mathbb{Q}^{t}\) with \(\|\bm{v}\|_{2}\leq(2mt)^{O(1)}\) and \(\bm{u}\sim_{2m}\bm{v}\). Note that we need to consider rational numbers up to size \(2m\) due to the fact that substitution of the variables in \(\bm{w}\) by rational numbers \(\bm{x}\in\mathbb{Q}_{\leq m}^{n}\) yields a rational vector \(\bm{w}(\bm{x})\in Q_{\leq 2m}^{\prime}\). Let \(p^{\prime}(X_{1},\ldots,X_{n})\) be the polynomial obtained from \(p\) be replacing the coefficients \(c_{i,j,r,s}\in\mathbb{R}\) by \(v_{i,j,r,s}\in\mathbb{Q}\). Then for each \(\bm{x}\in\mathbb{Q}_{\leq m}^{n}\) we have \(p(\bm{x})=\langle\bm{u},\bm{w}(\bm{x})\rangle>0\) (resp. \(\geq 0\)) if, and only if, \(p^{\prime}(\bm{x})=\langle\bm{v},\bm{w}(\bm{x})\rangle>0\) (resp. \(\geq 0\)).

Replacing each real polynomial \(p\) in \(\alpha\) by the constructed rational polynomial \(p^{\prime}\) results in a rational PC \(\alpha^{\prime}\) with \(\llbracket\alpha\rrbracket\cap\mathbb{Q}_{\leq m}^{n}=\llbracket\alpha\rrbracket \cap\mathbb{Q}_{\leq m}^{n}\).

### Proof of Lemma 10

The rest of this subsection is devoted to proving Lemma 8, for which we rely on results from convex geometry, which requires some terminology. For a set \(S\subseteq\mathbb{R}^{n}\), we define the _convex hull of \(S\)_ as

\[\mathsf{conv.hull}\ S=\{\lambda_{1}\boldsymbol{s}_{1}+\dots+ \lambda_{m}\boldsymbol{s}_{m}\mid m>0,\ \boldsymbol{s}_{1},\dots,\boldsymbol{s}_{m}\in S,\\ \lambda_{1},\dots,\lambda_{m}\in[0,1],\ \lambda_{1}+\dots+ \lambda_{m}=1\}\]

and the _cone generated by \(S\)_ as

\[\mathsf{cone}\ S\ =\ \{\lambda_{1}\boldsymbol{s}_{1}\,+\,\dots\,+\,\lambda_{m} \boldsymbol{s}_{m}\ \mid\ m>0,\ \boldsymbol{s}_{1},\dots,\boldsymbol{s}_{m}\ \in\ S,\ \lambda_{1},\dots,\lambda_{m}\ \geq\ 0\}.\]

We will also rely on Caratheodory's theorem, which says that a points in cones and convex sets can be obtained from at most \(n\) points. See, for example, [35, Theorems 7.1i and 7.1j].

**Theorem 18** (Caratheodory's Theorem).: _Let \(S\subseteq\mathbb{R}^{n}\). For every \(\boldsymbol{x}\in\mathsf{conv.hull}\ S\), there are \(\boldsymbol{x}_{1},\dots,\boldsymbol{x}_{n}\in S\) with \(\boldsymbol{x}\in\mathsf{conv.hull}\ \{\boldsymbol{x}_{1},\dots, \boldsymbol{x}_{n}\}\). Moreover, for every \(\boldsymbol{y}\in\mathsf{cone}\ S\), there are \(\boldsymbol{y}_{1},\dots,\boldsymbol{y}_{n}\in S\) with \(\boldsymbol{y}\in\mathsf{cone}\ \{\boldsymbol{y}_{1},\dots, \boldsymbol{y}_{n}\}\)._

A _polyhedron_ is a set of the form \(\{\boldsymbol{x}\in\mathbb{R}^{n}\mid A\boldsymbol{x}\geq\boldsymbol{b}\}\), where \(A\in\mathbb{R}^{m\times n}\) and \(\boldsymbol{b}\in\mathbb{R}^{m}\) for some \(m\in\mathbb{N}\). If the matrix \(A\) and the vector \(\boldsymbol{b}\) are rational, then the polyhedron is called a _rational polyhedron_. It is a standard result about polyhedra that if \(A\) and \(\boldsymbol{b}\) are rational of size at most \(m\), then the polyhedron \(P=\{\boldsymbol{x}\in\mathbb{R}^{n}\mid A\boldsymbol{x}\geq\boldsymbol{b}\}\) can be written as

\[P=\mathsf{conv.hull}\ \{\boldsymbol{x}_{1},\dots,\boldsymbol{x}_{s}\}+\mathsf{ cone}\ \{\boldsymbol{y}_{1},\dots,\boldsymbol{y}_{t}\},\]

with rational vectors \(\boldsymbol{x}_{1},\dots,\boldsymbol{x}_{s},\boldsymbol{y}_{1},\dots, \boldsymbol{y}_{t}\), where each vector has size polynomial in \(mn\). See, for example, [35, Theorem 10.2].

Proof.: We define the matrix \(B\in\mathbb{Q}^{(k+\ell)\times n}\) and the vector \(\boldsymbol{b}\in\mathbb{Q}^{k+\ell}\) as

\[B=\begin{bmatrix}A\\ A^{\prime}\end{bmatrix} \boldsymbol{b}=\begin{bmatrix}\boldsymbol{z}\\ \boldsymbol{z}^{\prime}\end{bmatrix}.\]

and consider the polyhedron \(P=\{\boldsymbol{x}\in\mathbb{R}^{n}\mid B\boldsymbol{x}\geq\boldsymbol{b}\}\). As mentioned above, we can write

\[P=\mathsf{conv.hull}\ \{\boldsymbol{x}_{1},\dots,\boldsymbol{x}_{s}\}+\mathsf{ cone}\ \{\boldsymbol{y}_{1},\dots,\boldsymbol{y}_{t}\},\]

where \(\boldsymbol{x}_{1},\dots,\boldsymbol{x}_{s},\boldsymbol{y}_{1},\dots, \boldsymbol{y}_{t}\) are rational vectors of size polynomial in \(mn\). By our assumption, there exists an \(\boldsymbol{s}\in\mathbb{R}^{n}\) with \(A\boldsymbol{s}\gg\boldsymbol{z}\) and \(A^{\prime}\boldsymbol{s}\geq\boldsymbol{z}^{\prime}\). By Caratheodory's Theorem, wlog, \(\boldsymbol{s}\) belongs to the smaller polyhedron

\[Q=\mathsf{conv.hull}\ \{\boldsymbol{x}_{1},\dots,\boldsymbol{x}_{n}\}+\mathsf{ cone}\ \{\boldsymbol{y}_{1},\dots,\boldsymbol{y}_{n}\}.\]

Now note that we have \(A\boldsymbol{u}\geq\boldsymbol{z}\) and \(A^{\prime}\boldsymbol{u}\geq\boldsymbol{z}^{\prime}\) for every \(\boldsymbol{u}\in U:=\{\boldsymbol{x}_{1},\dots,\boldsymbol{x}_{n},\boldsymbol {y}_{1},\dots,\boldsymbol{y}_{n}\}\). We claim that the vector

\[\boldsymbol{r}=\tfrac{1}{n}(\boldsymbol{x}_{1}+\dots+\boldsymbol{x}_{n})+ \boldsymbol{y}_{1}+\dots+\boldsymbol{y}_{n}\]

satisfies \(A\boldsymbol{r}\gg\boldsymbol{z}\) and \(A^{\prime}\boldsymbol{r}\geq\boldsymbol{z}^{\prime}\). Indeed, it clearly belongs to \(Q\subseteq P\) and thus satisfies \(A^{\prime}\boldsymbol{r}\geq\boldsymbol{z}^{\prime}\). Moreover, for every row \(\boldsymbol{a}^{\top}\boldsymbol{x}>z\) of \(A\boldsymbol{x}\gg\boldsymbol{z}\), there must be a \(\boldsymbol{u}\in U\) with \(\boldsymbol{a}^{\top}\boldsymbol{u}>z\)--otherwise, we would would have \(\boldsymbol{a}^{\top}\boldsymbol{u}=z\) for every \(\boldsymbol{u}\in U\) and thus \(\boldsymbol{a}^{\top}\boldsymbol{s}=z\). In particular, we have \(A\boldsymbol{r}\gg\boldsymbol{z}\). Finally, the vector \(\boldsymbol{r}\) has size at most \(\|\boldsymbol{x}_{1}\|_{2}+\dots+\|\boldsymbol{x}_{n}\|_{2}+\|\boldsymbol{y}_ {1}\|_{2}+\dots+\|\boldsymbol{y}_{n}\|_{2}\), which is polynomial in \(mn\), since each \(\boldsymbol{x}_{i}\) and each \(\boldsymbol{y}_{i}\) has size polynomial in \(mn\). 

### Proof of Lemma 8

Proof of Lemma 8.: Collect the set of all inequalities \(\langle\boldsymbol{w},\boldsymbol{u}\rangle>z\) or \(\langle\boldsymbol{w},\boldsymbol{u}\rangle\geq z\) with \(\boldsymbol{w}\in\mathbb{Q}^{\ell}_{\leq mn}\) and \(z\in\mathbb{Q}_{\leq m}\) that are satisfied for \(\boldsymbol{u}\). This results in two large matrices \(A\in\mathbb{Q}^{k\times t}_{\leq m}\) and \(A^{\prime}\in\mathbb{Q}^{\ell\times t}_{\leq m}\) and vectors \(\boldsymbol{z}\in\mathbb{Q}^{k}_{\leq m}\) and \(\boldsymbol{z}^{\prime}\in\mathbb{Q}^{\ell}_{\leq m}\) such that we have \(\boldsymbol{u}\sim_{m}\boldsymbol{v}\) if and only if \(A\boldsymbol{v}\gg\boldsymbol{z}\) and \(A^{\prime}\boldsymbol{v}\geq\boldsymbol{z}^{\prime}\). Thus, we can construct \(\boldsymbol{c}^{\prime}\) using Lemma 10. Observe that the bound from Lemma 10 does not depend on the (exponentially large) \(k\) and \(\ell\).

Proof of Proposition 11.: Let \(T\) be some UHAT with positional encoding and \(n,m\in\mathbb{N}\) be two natural numbers. In the following, we consider input sequences of \(T\) having the length \(n\) and size \(m\). By Propositions 5 and 7 there is polynomial sized, \(O(\ell)\)-alternation-bounded, and rational PC \(\alpha\) (where \(\ell\) is the number of layers in \(T\)) such that the set of sequences of size \(m\) accepted by the UHAT \(T\) equals \(\llbracket\alpha\rrbracket\cap\mathbb{Q}_{\leq m}^{a_{\cdot}d_{\cdot}}\).

We finally show that the PC \(\alpha\) can be realized as a circuit of constant depth and polynomial size. So, consider a constraint of the form \(p(\bar{\bm{x}})\sim 0\) where \(\sim\in\{\geq,>\}\), \(p\) is a polynomial of degree at most \(2\) and \(\bar{\bm{x}}\) represents the input sequence. Since addition and multiplication of rational numbers are realizable in TC\({}^{0}\)[7], it is clear that the computation of the value \(p(\bar{\bm{x}})\) is also realizable. Additionally, checking whether this value is \(\geq 0\) (or \(>0\), resp.) is a simple check of the bit representing the signum (and checking that the numerator has at least one non-zero bit).

Finally, we have to connect all the atoms of the form \(p(\bar{\bm{x}})\sim 0\) to the Boolean formula \(\alpha\). Since \(\alpha\) alternates only a bounded number of times between disjunctions and conjunctions, we can realize the complete formula \(\alpha\) in a circuit of constant depth and with polynomial size. Since \(\alpha\) is equivalent (up to the input size \(m\)), the UHAT \(T\) is realizable in TC\({}^{0}\). Note that here, we need to perform iterated addition of rational numbers, which requires iterated multiplication of integers represented in binary. The latter is well-known to be possible in TC\({}^{0}\)[33, 34] (even uniformly [22], but this is not needed in our setting). 

## Appendix C Proof of non-regularity in Section 4

Recall that

\[\mathsf{Double}:=\{(r_{1},\ldots,r_{n})\in\mathbb{Q}^{n}\mid n\geq 1\text{ and }2r_{i}<r_{i+1}\text{ for all }1\leq i<n\}.\]

We next define the notion of symbolic automata [10, 11, 43]. A _symbolic automaton_ is a tuple \((Q,\delta,q_{0},F)\), where \(Q\) is a finite set of states, \(q_{0}\in Q\) is an initial state, \(F\subseteq Q\) is a set of final states, and \(\delta\) is a set of transition rules of the form \((p,S,q)\), where \(S\subseteq\mathbb{Q}\). For \(a\in\mathbb{Q}\), we write \(p\rightarrow_{a}q\) (read "there is a transition from \(p\) to \(q\) reading \(a\)) if there is a transition rule \((p,S,q)\) such that \(a\in S\). Slightly abusing notation, for a set \(S\subseteq\mathbb{Q}\), we also write \(p\rightarrow_{S}q\) to mean that \((p,S,q)\) is a transition rule in \(\delta\). The notion of a run, and an accepting run can then be defined in exactly the same way as for finite automata (e.g. see [37]); namely, it is a sequence of transitions \(q_{0}\rightarrow_{a_{1}}\cdots\rightarrow_{a_{n}}q_{n}\), where \(q_{n}\in F\), reading the sequence \(w=a_{1}\cdots a_{n}\) over \(\mathbb{Q}\).

To prove that there is no symbolic automaton recognizing \(\mathsf{Double}\), let us assume to the contrary that such an automaton \(A\) exists, say, with \(n\) states. Consider a sufficiently long \(w=a_{1}\cdots a_{m}\in\mathsf{Double}\) (i.e. of length at least \(n\)), and an accepting run of \(A\):

\[q_{0}\rightarrow_{S_{1}}\cdots\rightarrow_{S_{m}}q_{m}\]

where each \(a_{i}\in S_{i}\) and \(q_{m}\in F\). Since \(m+1>n\), by pigeonhole principle, it must be the case that \(q_{r}=q_{s}\) for some \(r<s\). Thus, also the sequence

\[a_{1}\cdots a_{r}(a_{r+1}\cdots a_{s})^{2}a_{s+1}\cdots a_{m}\]

is accepted by \(A\) and therefore contained in \(\mathsf{Double}\). This is a contradiction since \(\mathsf{Double}\) imposes \(a_{s}<a_{s}\).

## Appendix D Omitted cases in proof of Theorem 3

For \(\neg\varphi\) the UHAT \(T_{\neg\varphi,m}\) first applies \(T_{\varphi,m}\) and on the obtained sequence \(\bm{u}_{1},\ldots,\bm{u}_{n},\bm{0}\in\mathbb{Q}^{m+1}\) uses a further layer followed by 1) to output \(\bm{w}^{\prime}_{i}:=(\bm{u}_{i}[1,m],1-\bm{u}_{i}[m+1])\) at position \(i\in[1,n]\) and \(\bm{0}\in\mathbb{Q}^{m+1}\) at position \(n+1\).

For \(\varphi\vee\psi\) we define the UHAT \(T_{\varphi\vee\psi,m}\) that first applies \(T_{\psi,m+1}\circ T_{\varphi,m}\) followed by a layer that on sequence \(\bm{u}_{1},\ldots,\bm{u}_{n+1}\in\mathbb{Q}^{m+2}\) outputs \(\bm{w}^{\prime}_{1},\ldots,\bm{w}^{\prime}_{n+1}\in\mathbb{Q}^{m+1}\) with

\[\bm{w}^{\prime}_{i}:=(\bm{u}_{i}[1,m],\max\{\bm{u}_{i}[m+1],\bm{u}_{i}[m+2]\})\]

for all \(i\in[1,n+1]\). Note that if \(\bm{u}_{n+1}=\bm{0}\), then also \(\bm{w}^{\prime}_{n+1}=\bm{0}\).

For \(X\varphi\) the UMAT \(T_{X\varphi,m}\) first applies \(T_{\varphi,m}\) to output a sequence \(\bm{u}_{1},\ldots,\bm{u}_{n+1}\in\mathbb{Q}^{m+1}\) with \(\bm{u}_{n+1}=\bm{0}\). With an additional layer that uses 2) to get attention vector \(\bm{a}_{i}=\bm{u}_{i+1}\) for all \(i\in[1,n]\) it then outputs at position \(i\in[1,n]\) the vector \(\bm{w}_{i}^{\prime}:=(\bm{u}_{i}[1,m],\bm{a}_{i}[m+1])\) and at position \(n+1\) the vector \(\bm{w}_{n+1}^{\prime}:=(\bm{u}_{n+1}[1,m],0)\) after applying 1).

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: This paper contains theoretical results on the expressive power of transformers on data sequences. As standard in theoretical results, we try to mathematically formulate the results as as precisely as they can be. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We have discussed assumptions we made in this paper (e.g., regarding the formal model of transformers) and what consequences they have (see Limitations in Concluding Remarks). Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs**Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: Since this is a theory paper, we try to be explicit in all the assumptions that we made for our results. We have also included all the proofs for the claims we made in the paper. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [NA] Justification: The paper contains no experiments. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.

5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [NA] Justification: The paper contains no experiments and no code. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [NA] Justification: The paper contains no experiments. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [NA] Justification: The paper contains no experiments. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).

* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [NA] Justification: The paper contains no experiments. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: This is a theory paper with no reference to human subjects and datasets. Our paper does not have neither direct societal impact nor potential harmful consequences. We have also carefully checked the impact mitigation measures, which do not directly apply to our paper, which involves neither experiments nor code. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: This is a theory paper with no direct societal impacts. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.

* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: This is a theory paper with neither experiments, codes, nor the use of datasets. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [NA] Justification: This is a theory paper that uses no code, data, and models. We do not use assets of others in any form. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.

* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: We do not release new assets. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: This is a theory paper with no crowdsourcing and no human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: This is a theory paper with no crowdsourcing and no human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.