# Variational Annealing on Graphs for Combinatorial Optimization

 Sebastian Sanokowski\({}^{1,2}\)&Wilhelm Berghammer\({}^{2}\)&Sepp Hochreiter\({}^{1,2}\)

Sebastian Lehner\({}^{1,2}\)

\({}^{1}\) ELLIS Unit Linz and LIT AI Lab

\({}^{2}\) Institute for Machine Learning, Johannes Kepler University Linz, Austria

sanokowski@ml.jku.at

###### Abstract

Several recent unsupervised learning methods use probabilistic approaches to solve combinatorial optimization (CO) problems based on the assumption of statistically independent solution variables. We demonstrate that this assumption imposes performance limitations in particular on difficult problem instances. Our results corroborate that an autoregressive approach which captures statistical dependencies among solution variables yields superior performance on many popular CO problems. We introduce subgraph tokenization in which the configuration of a set of solution variables is represented by a single token. This tokenization technique alleviates the drawback of the long sequential sampling procedure which is inherent to autoregressive methods without sacrificing expressivity. Importantly, we theoretically motivate an annealed entropy regularization and show empirically that it is essential for efficient and stable learning. 1

Footnote 1: Our code is available at: https://github.com/ml-jku/VAG-CO.

## 1 Introduction

Combinatorial optimization (CO) problems are of central interest to a wide range of fields, including operations research, physics, and computational complexity (Papadimitriou and Steiglitz, 1998). Since CO problems are typically NP-hard one would not expect that one can find the solutions to all arbitrarily large CO problem instances in polynomial time. However, these restrictions are concerned with worst case scenarios over entire problem families. For instance finding the Minimum Independent Set of _any_ graph. Consequently, it is not surprising that a large body of work is focused on the design of solution strategies that yield a particularly good performance on a restricted subset of all possible instances. There is growing interest in exploring deep learning approaches to this restricted CO setting (Bello et al., 2017). These methods aim to learn how to efficiently generate high quality solutions for CO problems and rely primarily on learned algorithmic components rather than on problem-specific, hand-crafted ones (Bengio et al., 2021). Importantly, practicable methods should not rely on supervised training with solutions from other solvers since this limits the achievable performance of the learned model to that of the solver used to generate the training data (Yehuda et al., 2020). As a result the development of unsupervised methods (Karalias and Loukas, 2020; Wang et al., 2022; Qiu et al., 2022; Karalias et al., 2022; Min et al., 2022; Wang and Li, 2023) and of Reinforcement Learning (RL) methods (Bello et al., 2017; Khalil et al., 2017; Kool et al., 2019; Ahn et al., 2020; Bother et al., 2022; Mazyavkina et al., 2021) for CO is an active field of research. In this work we are interested in training models that generate solutions at inference in contrast to approachesthat require problem instance specific training. A popular approach is to take a representation of the CO problem instance as input to a deep learning model and to output the parameters of a distribution over solutions that has its probability mass concentrated on regions with high solution quality. This probabilistic optimization approach to CO is used in the recent works of e.g. (Karalias and Loukas, 2020; Min et al., 2022; Qiu et al., 2022; Sun et al., 2022). In these works the distribution over solutions is built on the assumption of mutual statistical independence of the individual solution parameters and can, consequently, be represented by a product of Bernoulli distributions for the the individual solution variables. This simplification is typically refereed to as a _mean-field approximation_ (MFA) and is frequently used in various fields including e.g. statistical mechanics (Parisi, 1988), Bayesian statistics (Wainwright and Jordan, 2008), and the analysis of neural networks (Mei et al., 2019). However, the simplifying assumption of the MFA restricts the expressivity of the corresponding distributions which limits its applicability when the target distribution represents strong correlations (Jaakkola and Jordan, 1998). However, replacing MFA with more expressive approaches is computationally expensive and requires careful regularization to ensure efficient and stable training. Based on these consideration our contributions can be summarized as follows.

We demonstrate that the frequently used MFA imposes limits on the attainable solution quality for CO problems. Importantly, we introduce _Variational Annealing on Graphs for Combinatorial Optimization (VAG-CO)_ a method that achieves new state-of-the-art performances on popular CO problems by combining expressive autoregressive models with annealed entropy regularization. We provide a theoretical motivation for this entropy regularization via considerations of the sample complexity of related density estimation problems. By introducing _sub-graph tokenization_ VAG-CO drastically reduces the number of necessary steps to generate solutions and therein significantly improves the training and inference efficiency.

## 2 Problem Description

**Ising Formulation of CO.** To introduce VAG-CO it is convenient to adopt a point of view on CO that is motivated by statistical mechanics. As shown in Lucas (2014) many frequently encountered NP-complete CO problems can be reformulated in terms of a particular class of models known as Ising models which therefore allow the representation of different CO problem types in a unified manner. In this context the CO problem is equivalent to finding the minimum of an energy function \(E:\{-1,1\}^{N}\mapsto\mathbb{R}\). This function assigns an energy \(E(\bm{\sigma})\) to an N-tuple of discrete solution variables \(\bm{\sigma}=(\sigma_{1}\dots\sigma_{N})\in\{-1,1\}^{N}\) which are often referred to as spins. The family of aforementioned Ising models is characterized by the following form of the energy function:

\[E(\bm{\sigma})=\sum_{i<j}J_{ij}\sigma_{i}\sigma_{j}+\sum_{i}^{N}B_{i}\sigma_{i},\] (1)

where the first term represents the interaction between spins \(\sigma_{i}\in\{-1,1\}\) through couplings \(J_{ij}\in\mathbb{R}\), while the second term determines the magnitude \(B_{i}\in\mathbb{R}\) of the contribution of an individual spin to the energy of the system. For brevity we will denote the parameters of the energy function \((J_{ij},B_{i})\) simply as \(E\). Hence, \(E\) will represent a CO problem instance while \(E(\bm{\sigma})\) will denote the energy of a solution \(\bm{\sigma}\) under an energy function with parameters \(E\). The Ising formulation for the four CO problem types studied in this work is given in Tab. 1.

**Variational Learning.** Next, we specify how we approach the learning problem of approximating the optimal solution for a given problem instance. We follow the frequently taken variational learning approach of using a neural network with parameters \(\theta\) to obtain for a given CO problem instance \(E\) as associated probability distribution \(p_{\theta}(\bm{\sigma}|E)\) over the space of possible solutions \(\{-1,1\}^{N}\). The problem instances \(E\) are assumed to be independently sampled from a probability distribution \(q(E)\) with \(\Omega=\operatorname{supp}(q)\). In this setting the learning problem can be formulated as finding \(\operatorname{argmin}_{\theta}\sum_{\Omega}q(E)\sum_{\bm{\sigma}}p_{\theta}( \bm{\sigma}|E)E(\bm{\sigma})\). Here \(\sum_{\bm{\sigma}}\) is a shorthand for the sum over all \(\bm{\sigma}\in\{-1,1\}^{N}\). In practice both sums are approximated by empirical averages over samples from the corresponding distributions \(q\) and \(p_{\theta}\). In CO \(E(\bm{\sigma})\) is typically characterized by many local minima. Therefore, directly approaching the learning problem described above via gradient descent methods is prone to getting stuck at sub-optimal parameters. As we demonstrate in Fig. 1 (Right) this problem can be alleviated by adding a regularization term to the optimization objective Eq. 1 which encourages \(p_{\theta}\) to avoid a collapse to local minima. The resulting loss function \(L:\{-1,1\}^{N}\rightarrow\mathbb{R}\) associated to the learning problem is given by:

\[L(\theta;\beta)=\frac{1}{M}\sum_{m=1}^{M}\hat{F}(\theta;\beta,E_{m}),\] (2)

where \(M\) is the number of CO problem instances \(E_{m}\) in one batch and \(\beta\) plays the role of a regularization coefficient which is frequently referred to as the inverse temperature \(1/T\). The loss for an individual problem instance \(E_{m}\) is based on the so-called free-energy \(F(\theta;\beta,E_{m})\):

\[F(\theta;\beta,E_{m})=\sum_{\bm{\sigma}}p_{\theta}(\bm{\sigma}|E_{m})\big{(}E_ {m}(\bm{\sigma})+1/\beta\log p_{\theta}(\bm{\sigma}|E_{m})\big{)}.\] (3)

In practice, we approximate the expectation value \(F(\theta;\beta,E_{m})\) with an empirical sample average \(\hat{F}(\theta;\beta,E_{m})\) that is based on \(n_{S}\) samples \(\bm{\sigma}\sim p_{\theta}(\bm{\sigma}|E_{m})\). The term \(E_{m}(\bm{\sigma})\) on the right-hand side of Eq. 3 encourages the concentration of \(p_{\theta}(\bm{\sigma}|E_{m})\) on low-energy solutions. Small values of the regularization parameter \(\beta\in\mathbb{R}_{>0}\) encourage \(p_{\theta}(\bm{\sigma}|E_{m})\) to have a large entropy. For a given value of \(\beta\) the free-energy in Eq. 3 is known to be minimized by the Boltzmann distribution associated to \(E_{m}\) (see e.g. [10]):

\[p_{B}(\bm{\sigma}|E_{m},\beta)=\frac{\exp-\beta E_{m}(\bm{\sigma})}{\sum_{\bm {\sigma}^{\prime}}\exp-\beta E_{m}(\bm{\sigma}^{\prime})}.\] (4)

Thus by minimizing \(L\) the model learns to approximate \(p_{B}(\bm{\sigma}|E_{m},\beta)\) for a given \(E_{m}\) and \(\beta\). For \(\beta\to\inf\) the \(p_{B}(\bm{\sigma}|E_{m},\beta)\) has its mass concentrated on the global minima of \(E_{m}\) and for \(\beta\to 0\) it approaches the uniform distribution [11]. It is, therefore, to be expected that the minimization of \(L\) becomes harder at low temperatures, i.e. as \(\beta\to\inf\), which opens the opportunity to a principled curriculum learning approach (Sec. 4). Based on these considerations we reformulate CO problems as the variational problem of approximating \(p_{B}(\bm{\sigma}|E_{m},\beta)\) in the limit \(\beta\to\inf\) with a variational ansatz \(p_{\theta}\) that has the variational parameters \(\theta\). This problem can be formalized as \(\operatorname*{argmin}_{\theta}\lim_{\beta\to\inf}L(\theta;\beta)\).

## 3 Variational Annealing on Graphs

Our method VAG-CO addresses CO as a variational learning problem on graphs. In particular, given a set of CO problem instances it learns to approximate the Boltzmann distribution of the corresponding Ising models (Sec. 2) with an autoregressive distribution. To obtain efficient training with this expressive model we apply an annealed entropy regularization. We formulate this learning problem in an RL setting and use PPO to train our model. To alleviate the lengthy sampling process of autoregressive approaches we introduce sub-graph tokenization which allows us to generate multiple solution variables at in a single step without loosing expressive power. We further improve the memory efficiency of our approach by dynamically pruning the graph which represents the CO problem instance.

**Autoregressive Solution Generation.** In the following we specify how we represent \(p_{\theta}(\bm{\sigma}|E_{i})\) and how to sample it, i.e. how to generate solutions \(\bm{\sigma}\).

1. Draw a problem instance \(E=(B_{i},J_{ij})\sim q(E)\) from the data distribution \(q\) and construct a graph \(G=(V,\mathcal{E})\) based on \(E\). The nodes \(\nu_{i}\in V\) correspond to the spins \(\sigma_{i}\) and the set of edges \(\mathcal{E}\) represents the non-zero components of \(J_{ij}\). The graph \(G\) is equipped with node features \(x_{i}=[B_{i},t_{i}]\) that are associated to its nodes \(\nu_{i}\). Here \(t_{i}\) is a four dimensional one-hot encoding which indicates the four possible states (I-IV) of a node \(\nu_{i}\) namely whether the corresponding spin \(\sigma_{i}\) is set to the value \(+1\) (I) or \(-1\) (II) or whether it is to be generated in the current step (III) or afterwards (IV). The edges between nodes \(\nu_{i}\) and \(\nu_{j}\) are associated with the scalar edge features \(J_{ij}\).
2. Order the graph nodes according to the breadth-first search (BFS) algorithm. The \(i\)-th node in this ordering is denoted as \(\sigma_{i}\). Now \(i=1\) and \(t_{i}\) is set to the state (III) and all \(t_{>i}\) are set to (IV).
3. A GNN is used to parameterize a Bernoulli distribution \(p_{\theta}(\sigma_{i}|G)=\operatorname*{GNN}_{\theta}(G)\) from which the value \(\pm 1\) of \(\sigma_{i}\) is sampled. The state encoding variables of \(G\) are updated by setting \(t_{i}\) associated to the graph is now updated accordingly and \(t_{i+1}\) is set to (III). Now \(i\) is incremented.
4. The previous step is repeated until the values of all \(\sigma_{i}\) are set.

We note that at each step \(i\) the graph \(G\) depends on the problem instance \(E\) and the already generated spins \(\sigma_{<i}\). Therefore this procedure represents an autoregressive parametrization of a distribution over the space of possible solutions \(\{-1,1\}^{N}\). By denoting the graph \(G\) at step \(i\) as \(G_{i}\) we get:

\[p_{\theta}(\bm{\sigma}|E)=\prod_{i=1}^{N}p_{\theta}(\sigma_{i}| \sigma_{<i},E)=\prod_{i=1}^{N}p_{\theta}(\sigma_{i}|G_{i}).\] (5)

This approach is more expressive than MFA and can therefore be expected to be better suited to approximate the typically complex Boltzmann distributions (Eq. 4) encountered in CO. Next we specify how we realize a stable training procedure by employing RL methods. The model architecture and hyperparameters are detailed in App. A.10.

**Reinforcement Learning Setting.** To explain how we train our model it is convenient to adopt an RL perspective where an episode corresponds to the solution generation procedure described above. For this we consider a Markov Decision Process (MDP) that is given by \((S,A,P,R)\). Here, \(S\) is the set of possible states and the state at step \(i\) is denoted by \(s_{i}\in S\). At each step \(s_{i}\) is represented by the graph \(G_{i}\). Given a state \(s_{i}\) an action \(a_{i}\) that represents the assignments of the spin value \(\sigma_{i}\in\{-1,1\}\) is sampled from a probability distribution which is parameterised by the policy \(p_{\theta}(\sigma_{i}|G_{i})\). After sampling an action the reward \(R_{i}(G_{i},\beta)\) is computed according to an objective that is based on Eq. 3. We use the relation \(F(\theta;\beta,E)=-\operatorname{\mathbb{E}}_{\bm{\sigma}\sim p_{\theta}}[\sum _{i=1}^{N}R_{i}(G_{i},\beta)]\) and define the reward at step \(i\) as

\[R_{i}(G_{i},\beta)=-\left[\Delta E_{i}+\frac{1}{\beta}\,\log p_{ \theta}(\sigma_{i}|\sigma_{<i},E)\right],\] (6)

where \(\Delta E_{i}=\sigma_{i}\left[\sum_{j<i}J_{ij}\sigma_{j}+B_{i}\right]\). With this definition maximising the reward is equivalent to minimizing the free-energy in Eq. 3. We approximate \(F(\theta;\beta,E)\) with the empirical mean of \(-\sum_{i=1}^{N}R_{i}(G_{i},\beta)\) based on \(n_{S}\) samples \(\bm{\sigma}\sim p_{\theta}(\sigma_{i}|\sigma_{<i},E)\). Finally, the state is changed deterministically, i.e. the spin \(\sigma_{i}\) is set according to the sampled action \(a_{i}\). The state update corresponds to the updated of \(t_{i}\) and \(t_{i+1}\) as described in step 3 of the solution generation procedure. Our model learns to solve this MDP via the popular PPO algorithm (Schulman et al., 2017) which is described in more detail in App. A.9.

**Annealing.** As discussed in Sec. 2 our training objective is based on Eq. 3 which contains an entropy regularization term. The strength of this regularization is determined by temperature \(T\). At first, the temperature is kept constant at \(T>0\) for \(N_{\mathrm{warmup}}\) steps. Then, the temperature is slowly adapted for \(N_{\mathrm{anneal}}\) steps by following a predefined annealing schedule (see App. A.14) that converges to \(T=0\) as the end of training is reached. This reduction of the temperature throughout training is motivated from a theoretical point of view in Sec. 4.

**Subgraph Tokenization.** By introducing subgraph tokenization we decrease the number of forward passes per CO problem instance without sacrificing expressivity. Instead of modelling the probability for the two possible values \(\pm 1\) of a single spin, we let the policy represent a probability distribution \(p_{\theta}(\sigma_{i:i+k}|\sigma_{<i},E)\) over all \(2^{k}\) possible configurations of \(k\) consecutive spins in the BFS ordering (step 2 of the solution generation procedure). We represent \(p_{\theta}(\sigma_{i:i+k}|\sigma_{<i},E)\) with a softmax function of a \(2^{k}\) dimensional vector that is output by the GNN (App. A.10). Subgraph tokenization represents a modification of step 3 of the solution generation procedure and yields an improvement of the performance (Sec. 6).

**Dynamic Graph Pruning.** We note that once the spin value of \(\sigma_{i}\) is sampled its interaction with adjacent spins \(\sigma_{j}\) is fixed. Therefore, assuming that we have sampled the first spin \(\sigma_{1}\), its interaction with an adjacent spin \(\sigma_{j}\), that is yet to be generated, can be expressed as \(B(1\mapsto j)_{j}\sigma_{j}\), where we introduce \(B(1\mapsto j)_{j}=J_{1j}\sigma_{1}\). Therefore, we can immediately remove generated spins from the graph and update the node embeddings \(x_{j}=[B_{j},t_{j}]\) of adjacent nodes with \(B_{j}\gets B_{j}+B(i\mapsto j)_{j}\). Reducing the graph size during the solution generation process has the benefit of reducing the memory requirements when processing graphs with a GNN as in step 3 of the solution generation procedure.

Theoretical Motivation of Annealing

In the following we show that the concept of an annealed entropy regularization in Eq. 3 can be motivated based the sample complexity of density estimation for Boltzmann distributions.

Consider the problem of approximating the probability density function of a Boltzmann distribution \(p_{B}(s;E,\beta)\), where \(E:\mathbb{R}^{\mathbb{N}}\rightarrow\left[0,1\right]\) is a suitably normalized energy function. To solve this task we are given a set of samples \(\mathcal{S}=\{s_{1},\ldots,s_{m}\}\) which are independently drawn from the corresponding target Boltzmann distribution \(p_{B}(s;E,\beta^{*})\) at a fixed inverse temperature \(\beta^{*}\). For brevity we will denote the distribution associated to \(p_{B}(s;E,\beta)\) from now on as \(p(\beta)\) where \(E\) is suppressed since it is a fixed function. The empirical distribution corresponding to \(\mathcal{S}\) will be denoted as \(\hat{p}\). Further assume that we can evaluate \(E(s_{i})\) for all \(s_{i}\in\mathcal{S}\).

In the present context a natural feasibility criterion for the estimated distribution is that the expectation value of the energy function for the estimated distribution should be compatible with the corresponding value \(\mathbb{E}_{p(\beta^{*})}E(s)\) of the target distribution. We approach this problem with the maximum entropy principle (Jaynes, 1957). Informally, this principle prescribes that among all feasible distributions one should prefer the one that yields the highest entropy. Since \(\mathcal{S}\) contains only a finite number of samples we cannot determine \(\mathbb{E}_{p(\beta^{*})}E(s)\) with arbitrary accuracy. As already stated in a similar form in (Dudik et al., 2007) one obtains with Hoeffding's inequality that with a probability of \(1-\delta\):

\[|\mathbb{E}_{p(\beta^{*})}E(s)-\mathbb{E}_{\hat{p}}E(s)|<\sqrt{\ln(2/\delta)/ (2m)}.\] (7)

As shown in (Kazama and Tsujii, 2003) the resulting maximum entropy optimization problem with the inequality constraint Eq. (7) is equivalent to the following regularized maximum likelihood problem over the family of Boltzmann distributions: \(\min_{\beta}\mathcal{L}_{\hat{p}}(\beta)+\lambda\beta\), where \(\mathcal{L}_{\hat{p}}(\beta)=-\mathbb{E}_{\hat{p}}\log p(\beta)\) is the cross-entropy between \(\hat{p}\) and \(p(\beta)\). Based on a closely related result in (Dudik et al., 2007) we obtain the following bound for the approximation of maximum entropy distributions:

**Remark 1**.: _Assume a bounded energy function \(E:\mathbb{R}^{\mathbb{N}}\rightarrow\left[0,1\right]\) and let \(\hat{\beta}\) minimize \(\mathcal{L}_{\hat{p}}(\beta)+\lambda|\beta|\) where \(\lambda=\sqrt{\ln(2/\delta)/(2m)}\). Then with a probability at least \(1-\delta\):_

\[D_{KL}\big{(}p(\beta^{*})||p(\hat{\beta})\big{)}\leq\frac{|\beta^{*}|}{\sqrt{ m}}\sqrt{2\ln(2/\delta)}.\] (8)

See App. A.15.2 for further details. According to Eq. 8 the sample complexity of approximating a maximum entropy distribution at an inverse temperature \(\beta\) is in \(O(\beta^{2})\).

Curriculum learning is a machine learning paradigm in which the difficulty of training tasks is gradually increased as training proceeds (Bengio et al., 2009). In view of Theorem 1 the entropy annealing in VAG-CO (Sec. 3) can be regarded as a principled curriculum learning approach if an increased sample complexity is regarded as an indication of a more difficult learning problem. In supervised learning tasks Wu et al. (2021) find that the application of curriculum learning results in more resource efficient training but not necessarily in better performance. As we demonstrate in Sec. 6 our entropy annealing does actually yield better performing models.

## 5 Related Work

As pointed out in (Yehuda et al., 2020) supervised training of CO solvers faces the problem of expensive data generation and that data augmentation cannot circumvent this problem. Consequently, there is a growing interest in RL and unsupervised methods. In the following we focus on methods that attempt to learn how to generate solutions rather than how to improve existing ones.

**Unsupervised Learning and Reinforcement Learning.** The work of (Bello et al., 2017) proposes to use an actor-critic approach to learn how to solve CO problems. They were the first to show that deep RL is a promising approach for CO. Their method represents an autoregressive distribution over solutions. However, in their setting rewards are sparse since they are only available for complete solutions. In our approach rewards are dense since they are available after every state transition. Another influential work is (Khalil et al., 2017) who were the first to exploit the graph structure of CO problems by learning problem representations with GNNs. Applying GNNs to CO problem has become a common choice since then (Cappart et al., 2021). In (Tonshoff et al., 2020) the method RUN-CSP is introduced. The work of (Karalias and Loukas, 2020) proposes Erdos Goes Neural (EGN) in which the goal is to minimize a loss that can be regarded as a certificate of the existence of a CO problem solution whose cost is upper bounded by the loss. By relying on an MFA they can calculate this loss efficiently. Also (Qiu et al., 2022) use an MFA to optimize a distribution over the solution space and optimize the corresponding parameters with gradient estimates that are obtained by REINFORCE. The work of (Min et al., 2022) uses the same MFA-based concept as EGN and focuses on alleviating the oversmoothing problem of GNNs by using modified Graph Convolutional Networks (Kipf and Welling, 2017). An approach based on an entry-wise concave continuous relaxation of the discrete CO loss and a subsequent rounding procedure for integral solution generation is introduced in (Wang et al., 2022). Here the concept of (Karalias and Loukas, 2020) is generalized to a wider class of problems and rather simple rounding procedure is introduced to generate solutions efficiently. This approach is further extended by (Wang and Li, 2023) to a meta-learning setting called Meta-EGN in which network parameters are updated for individual CO problem instances. They also argue that RL based CO methods suffer from unstable training. We call this claim into question by finding no stability issues with our RL-based method. Further VAG-CO outperforms EGN and even Meta-EGN despite not updating any parameters on test problem instances. The approach of extending functions on sets to continuous supports is taken in Neural Set Function Extensions (NSFE) (Karalias et al., 2022). In NSFE the discrete CO loss function is replaced with a convex combination of the discrete loss function values at certain integral solutions. These extensions can be regarded as expectation values that can be calculated in closed form without sampling. Whether the lack of sampling noise in NSFE is beneficial for the optimization procedure is not obvious.

**Variational Annealing.** The concept of using autoregressive models in the variational problem of approximating Boltzmann distributions of Ising models was introduced by (Wu et al., 2019). They show that variational annealing (VA), i.e. the combination of the variational approach and temperature annealing, is a highly performant method. The work of (Hibat-Allah et al., 2021) compares VA to other ones like Simulated Annealing (SA) (Kirkpatrick et al., 1983) and confirms its strong performance on problems related to spin glasses. In (Khandoker et al., 2023) the strong performance of VA compared to SA is confirmed on CO problems. A crucial aspect of these works on VA with respect to ours and the ones in the previous paragraph is that they optimize the parameters of their models only for individual problem instances. They do not attempt to learn how to generalize over a family of problems. The work of (Sun et al., 2022) aims at a generalizing application of VA in CO by using an MFA. Our experiments indicate that the simplicity of MFA-based methods leads to performance limitations in particular on hard CO problem instances.

## 6 Experiments

We evaluate VAG-CO on various CO problems that are studied in the recent literature. Additionally, we evaluate VAG-CO on synthetic datasets where solving the corresponding CO problem is known to be particularly hard. Finally, we discuss experiments on the impact of entropy regularization and subgraph tokenization. Our result tables also include inference runtimes. A quantitative runtime comparison is, however, difficult since the runtimes reported from other works and for Gurobi 10.0.0 (Gurobi Optimization, LLC, 2023) were obtained with differen setups. See App. A.12 for details on the runtime measurements. For Gurobi we report results for various different runtime limits.

**Maximum Independent Set.** In the following we will compare VAG-CO on the Maximum Independent Set (MIS) problem to recently published results from (Karalias et al., 2022), where the graph datasets COLLAB, ENZYMES and PROTEINS Morris et al. (2020) are used. In the MIS problem the task is to find the largest subset of independent, i.e. unconnected, nodes in a graph. As an optimization objective for VAG-CO we use Eq. 2 with the Ising energy function for MIS (see Tab. 1). Here, the energy function \(E(\bm{q})\) consists of two terms \(E_{A}(\bm{q})\) and \(E_{B}(\bm{q})\), that depend on the binary representation of the solution \(\bm{q}=\frac{\bm{\sigma}+1}{2}\) with \(\bm{q}\in\{0,1\}^{N}\). When \(q_{i}=1\) the corresponding node is defined to be included in the set and it is excluded otherwise. The first term \(E_{A}(\bm{q})\) is proportional to the number of nodes that are in the set, whereas \(E_{B}(\bm{q})\) is proportional to the number of independence violations, i.e. the number of connected nodes within the set. By selecting \(A,B\in\mathbb{R}_{+}\) such that \(A<B\) we ensure that all minima satisfy \(E_{B}=0\) since in this case excluding a violating node from the set always reduces the energy. In our experiments, we choose \(A=1.0\) and \(B=1.1\).

We follow the procedure of (Karalias et al., 2022) and use a \(0.6/0.1/0.3\) train/val/test split on the aforementioned datasets and use only the first 1000 graphs on the COLLAB dataset. The resultsare shown in Tab. 2 where the test set average of the best approximation ratio \(\mathrm{AR}^{*}\) out of \(n_{S}=8\) sampled solutions per graph is reported (see App. A.1). This metric was originally proposed in (Karalias and Loukas, 2020). We also report results of our own implementation of an MFA-based method that is trained with REINFORCE. This method is used with (MFA-Anneal) and without (MFA) annealing (App. A.5). As in Wang and Li (2023) and Karalias and Loukas (2020) we use the conditional expectation procedure (CE, App. A.5.5) to sample solutions and report the corresponding results as (MFA: CE) and (MFA-Anneal: CE). We also add results obtained by the Degree Based Greedy (DB-Greedy) algorithm (Wormald, 1995) that is proposed by (Angelini and Ricci-Tersenghi, 2023) as a baseline for machine learning algorithms on MIS. For VAG-CO we greedily sample different states for a given problem instance by using different BSF orderings of the corresponding graph nodes. Our results show that VAG-CO significantly outperforms all competitors including the MFA on ENZYMES and PROTEINS. On IMDB-BINARY and MUTAG the MFA-based approaches and VAG-CO outperform all other machine learning methods and achieve an optimal \(\mathrm{AR}^{*}\). On COLLAB MFA-Anneal and VAG-CO exhibit the best results with insignificantly better results for MFA-Anneal. We also report results based on the test set average of the approximation ratio with \(n_{S}=30\)\(\widehat{\mathrm{AR}}\) (App. A.1). This metric shows the performance of the learned probability distribution for each model, when no post processing is applied. Our results show that VAG-CO always achieves a significantly better performance in terms of \(\widehat{\mathrm{AR}}\) than MFA-based approaches.

**Minimum Vertex Cover.** We compare VAG-CO to results from Wang and Li (2023) where the Minimum Vertex Cover (MVC) problem is solved on the TWITTER (Leskovec and Krevl, 2014), COLLAB and IMDB-BINARY Morris et al. (2020) graph datasets. The MVC problem is the task of finding the smallest subset of vertices such that each edge has at least one node in the subset. As for the MIS problem we formulate this CO problem in terms using the Ising energy function that is defined in Tab. 1 and set \(A=1.0\) and \(B=1.1\). We follow the procedure of Wang and Li (2023) and use a \(0.7/0.1/0.2\) train/val/test split on these datasets. Their method Meta-EGN uses meta

\begin{table}
\begin{tabular}{c|c} Problem Type & Ising formulation: \(\min_{q}E(\bm{q})\) where \(E(\bm{q})=E_{A}(\bm{q})+E_{B}(\bm{q})\) \\ \hline MVC & \(E(\bm{q})=A\)\(\sum_{i}^{N}q_{i}+B,\sum_{(i,j)\in\mathcal{E}}(1-q_{i})\cdot(1-q_{j})\) \\ \hline MIS & \(E(\bm{q})=-A\sum_{i}^{N}q_{i}+B\sum_{(i,j)\in\mathcal{E}}q_{i}\cdot q_{j}\) \\ \hline MaxCl & \(E(\bm{q})=-A\sum_{i}^{N}q_{i}+B\sum_{(i,j)\notin\mathcal{E}}q_{i}\cdot q_{j}\) \\ \hline MaxCut & \(E(\bm{\sigma})=-\sum_{(i,j)\notin\mathcal{E}}\frac{1-\sigma_{i}\sigma_{j}}{2}\)  where \(\sigma_{i}=2\,q_{i}-1\) \\ \end{tabular}
\end{table}
Table 1: Ising formulations of the MVC, MIS, MaxCl and MaxCut problem ((Lucas, 2014)). The term that includes the constant \(A\) corresponds to \(E_{A}(q)\) and the term that includes the constant \(B\) corresponds to \(E_{B}(q)\).

\begin{table}
\begin{tabular}{c c c c c c}  & ENZYMES & PROTEINS & IMDB-BINARY & COLLAB & MUTAG \\ \hline Evaluation metric A.1 & \(AR^{*}\) (\(\mathrm{s/graph}\) ) & \(AR^{*}\) (\(\mathrm{s/graph}\) ) & \(AR^{*}\) (\(\mathrm{s/graph}\) ) & \(AR^{*}\) (\(\mathrm{s/graph}\) ) \\ \hline BGN (\(\mathrm{r}\)) & \(0.821\pm 0.124\) (\(\mathrm{NA}\)) & \(0.963\pm 0.114\) (\(\mathrm{NA}\)) & \(0.515\pm 0.310\) (\(\mathrm{NA}\)) & \(0.886\pm 0.198\) (\(\mathrm{NA}\)) & \(0.539\pm 0.063\) (\(\mathrm{NA}\)) \\ NSFG (\(\mathrm{r}\)) & \(0.725\pm 0.155\) (\(\mathrm{NA}\)) & \(0.729\pm 0.205\) (\(\mathrm{NA}\)) & \(0.679\pm 0.287\) (\(\mathrm{NA}\)) & \(0.836\pm 0.253\) (\(\mathrm{NA}\)) & \(0.854\pm 0.132\) (\(\mathrm{NA}\)) \\ REINFORCE (\(\mathrm{r}\)) & \(0.751\pm 0.301\) (\(\mathrm{NA}\)) & \(0.725\pm 0.285\) (\(\mathrm{NA}\)) & \(0.881\pm 0.240\) (\(\mathrm{NA}\)) & \(1.000\pm 0.031\) (\(\mathrm{NA}\)) & \(0.781\pm 0.016\) (\(\mathrm{NA}\)) \\ Straight-through (\(\mathrm{r}\)) & \(0.725\pm 0.268\) (\(\mathrm{NA}\)) & \(0.722\pm 0.26\) (\(\mathrm{NA}\)) & \(0.917\pm 0.253\) (\(\mathrm{NA}\)) & \(0.856\pm 0.221\) (\(\mathrm{NA}\)) & \(0.965\pm 0.162\) (\(\mathrm{NA}\)) \\ \hline DB-Greedy & \(0.980\pm 0.009\) (\(\mathrm{0.008}\)) & \(0.9848\pm 0.003\) (\(\mathrm{0.008}\)) & \(1.000\pm 0.007\) & \(0.9988\pm 0.0001\) (\(\mathrm{0.03}\)) & \(0.994\pm 0.001\) (\(\mathrm{0.005}\)) \\ MFA-CE & \(0.9875\pm 0.001\) (\(\mathrm{0.036}\)) & \(0.9883\pm 0.0007\) (\(\mathrm{0.037}\)) & \(1.000\pm 0.022\) & \(0.9995\pm 0.0004\) (\(\mathrm{0.054}\)) & \(1.000\pm 0.019\) \\ MFA-Anneal & CE & \(0.9880\pm 0.0014\) (\(\mathrm{0.036}\)) & \(0.981\pm 0.0007\) (\(\mathrm{0.037}\)) & \(1.000\pm 0.022\) & \(0.9991\pm 0.0004\) (\(\mathrm{0.054}\)) & \(1.000\pm 0.019\) \\ VAG-CO (\(\mathrm{r}\)) & \(0.9960\pm 0.0007\) (\(\mathrm{0.026}\)) & \(0.9977\pm 0.0052\) (\(\mathrm{0.0032}\)) & \(1.000\pm 0.017\) & \(0.9988\pm 0.0009\) (\(\mathrm{0.064}\)) & \(1.000\pm 0.015\) \\ \hline Gurbin (\(t_{max}=0.01\)) & \(1.000\pm 0.001\) & \(0.9996\pm 0

[MISSING_PAGE_FAIL:8]

**Evaluation on Synthetic Problems for MIS.** On many of the benchmarks above nearly optimal results are obtained. Therefore, we conduct additional experiments on synthetic graph datasets that yield hard CO problems.

For graphs with a degree \(d\) larger than \(16\) the MIS problem on random regular graphs (RRGs) is known to be particularly hard [1]. Therefore, we generate RRGs with an average size of 100 nodes (RRG-100), where we sample \(4100\) RRGs between the size of 80 to 120 with different degrees \(d\in[3,7,10,20]\). For training, validation and testing we use \(3000/100/1000\) graphs. Results for the MIS problem on the RRG-100 dataset are shown in Fig. 1 (Left), where we plot the test set average of the best relative error \(\epsilon_{\mathrm{rel}}^{*}\) out of \(n_{S}=8\) sampled solutions per problem instance. Here the relative error is calculated with respect to exact solutions that are obtained with the commercial solver Gurobi. Error bars indicate the standard error over graphs with the corresponding node degree. As expected we find that for all methods \(\epsilon_{\mathrm{rel}}^{*}\) increases for larger degrees. MFA and MFA-Anneal outperform the DB-Greedy method. Furthermore, VAG-CO is the best performing method for all graph degrees. We also show Gurobi performance for each \(d\) at different time limits \(t_{\mathrm{max}}\). On this dataset VAG-CO outperforms Gurobi with the given compute budged (see App. A.12) by a high margin on hard instances.

**Evaluation on Synthetic Problems for MVC.** We also conduct experiments on graphs that are generated by the so-called RB method [21]. This method allows the generation of graphs that are known to yield hard MVC problem instances [23]. The RB model has three distinct generation parameters: \(n,k^{\prime}\), and \(p\). Adjusting the values of \(n\) and \(k^{\prime}\) allows us to control the expected size of the generated graphs, while \(p\) serves as a parameter to regulate the hardness of the graph instances. Specifically, when \(p\) is close to one, the generated graphs tend to be easier, whereas reducing \(p\) leads to increased difficulty. To ensure diversity in the RB dataset, we generate graphs with varying levels of hardness randomly sampling \(p\) from the range of \(0.25\) to \(1.0\). For our experiments, we utilize the RB-200 dataset, consisting of RB graphs with an average size of \(200\) nodes with a train/val/test split of \(2000/500/1000\) graphs. Following [23] each graph in this dataset is generated by randomly selecting values for \(n\) from the range of \(20\) to \(25\), and \(k^{\prime}\) from the range of \(9\) to \(10\). Figure 1 (Middle) shows the corresponding results in terms of \(\AR

\begin{table}
\begin{tabular}{c c c} \hline \multicolumn{2}{c}{**BNZYMES**} & **MDB-NNX** \\ \hline Evaluation metric A.1 & _AR*_ (r.g/graph) & _ARP*_ (r.g/graph) \\ \hline EGN (r) & 0.83 \(\pm\) 0.145 (N/A) & 0.936 \(\pm\) 0.175 (N/A) \\ NSFE (r) & 0.933 \(\pm\) 0.148 (N/A) & 0.961 \(\pm\) 0.143 (N/A) \\ REINFORCE (r) & 0.751 \(\pm\) 0.301 (N/A) & 0.881 \(\pm\) 0.240 (N/A) \\ Straight-Graph (r) & 0.725 \(\pm\) 0.208 (N/A) & 0.917 \(\pm\) 0.253 (N/A) \\ \hline DB-Greedy & 0.945 \(\pm\) 0.0034 (0.02) & 0.9875 \(\pm\) 0.0038 (0.010) \\ MFA. CE & 0.976 \(\pm\) 0.001 (0.04) & **0.999 \(\pm\) 0.000** (0.025) \\ MFA-Anneal: CE & **0.9921 \(\pm\) 0.0017** (0.04) & **0.999 \(\pm\) 0.0001** (0.025) \\ VAG-CO (ours) & **0.987 \(\pm\) 0.0042** (0.029) & **0.9981 \(\pm\) 0.0013** (0.017) \\ \hline Gurobi (\(t_{\mathrm{max}}=0.1\)) & 0.963 \(\pm\) 0.0020 (0.04) & 0.996 \(\pm\) 0.0020 (0.01) \\ Gurobi (\(t_{\mathrm{max}}=0.1\)) & 1.000 (0.005) & 1.003 (0.002) \\ \hline Evaluation metric A.1 & \(\widehat{\mathrm{M}}\) (time \(\mathrm{s}\)/s/mpl) & \(\widehat{\mathrm{M}}\) (time \(\mathrm{s}\)/s/mpl) \\ \hline MFA & 0.964 \(\pm\) 0.0133 (0.001) & 0.983 \(\pm\) 0.0094 (0.001) \\ MFA-Anneal & 0.806 \(\pm\) 0.004 (0.001) & 0.974 \(\pm\) 0.0041 (0.017) \\ VAG-CO (ours) & **0.943 \(\pm\) 0.012** (0.029) & **0.993 \(\pm\) 0.0013** (0.017) \\ \hline \end{tabular} 
\begin{tabular}{c c} \hline \multicolumn{2}{c}{**BA \(200-300\)**} \\ \hline \multicolumn{2}{c}{\(\widehat{\mathrm{M}}\)Cut (s/graph)} \\ \hline Gurobi (\(t\)) & 732.47 (1.57) \\ SDP (\(t\)) & 700.36 (4.29) \\ Greedy (\(t\)) & 688.31 (0.026) \\ EGN (\(t\)) & 693.45 (0.092) \\ Anneal (\(t\)) & 696.73 (0.09) \\ GFlowNet (\(t\)) & 704.30 (0.35) \\ \hline VAG-CO (ours) & **722.31** (0.17) \\ \hline Gurobi (\(t_{\mathrm{max}}=0.1\)) & 731.34 (0.1) \\ Gurobi (\(t_{\mathrm{max}}=1.)\) & 731.79 (1.) \\ Gurobi (\(t_{\mathrm{max}}=2.)\) & 732.00 (2.) \\ Gurobi (\(t_{\mathrm{max}}=10.)\) & 732.01 (10.) \\ \hline \end{tabular}
\end{table}
Table 4: Left: Results on the Maximum Clique Problem (see Tab. 1). We show test set results on the best approximation ratio \(AR\)* and the average approximation ratio \(\widehat{\mathrm{AR}}\) across different methods and datasets. Values that are closer to one are better. (r) indicates that these are results as reported in [1]. The time for each algorithm is reported in round brackets as seconds per graph. Right: Results on the Maximum Cut Problem (see Tab. 1) on BA \(200-300\) graphs. We show test set results on the average maximum cut value \(\widehat{\mathrm{MCut}}\) across different methods. Values that are larger are better. (r) indicates that these are results as reported in [11].

Figure 2: Study on the advantages of larger ST configuration sizes \(k\) for VAG-CO on the RB-100 MVC dataset. On the left y-axis we plot \(AR\)* (lower values are better) for VAG-CO models that are trained for values of \(k\). The left y-axis shows the inference time per graph.

of \(p\). We find that the DB-Greedy algorithm outperforms both MFA approaches. The MFA with annealing tends to perform worse than the MFA without annealing on this dataset. Here, we also add our own implementation of EGN and EGN-Anneal (Sun et al., 2022) and find that EGN performs similarly to MFA and EGN-Anneal achieves a similar performance as DB-Greedy. VAG-CO outperforms all other methods for all values of \(p\) on this dataset. We also show Gurobi performance for each \(p\) at different time limits \(t_{\max}\). Here, we see that at high \(p\) values Gurobi achieves close to optimal results within a very small time budged and is the best performing method. Whereas for low p values Gurobi performance drops by a high margin and VAG-CO achieves similar results within a comparable time.

**Ablation on Annealing.** In the following we investigate whether the annealed entropy regularization in Eq. 3 is indeed beneficial. These experiment are conducted for the MVC problem on RB-100 graphs that are generated with \(n\in[9,15]\), \(k^{{}^{\prime}}\in[8,11]\) and \(p\in[0.25,1]\). We compare VAG-CO test set learning curves with three different initial temperatures \(T_{0}\in\{5\cdot 10^{-2},5\cdot 10^{-3},0\}\). The run with \(T_{0}=0\) is equivalent to VAG-CO without annealing (App. A.14). Figure 1 (right) shows the average relative error \(\hat{\epsilon}_{\mathrm{rel}}\) for \(n_{S}=30\) sampled solutions per problem instance on the test set over the number of gradient update steps. We find that the run without annealing starts to converge rather quickly but at a comparably bad relative energy. In contrast to that, the runs with annealing (\(T_{0}\in\{5\cdot 10^{-2},5\cdot 10^{-3}\}\)) keep improving over more steps and achieve a better final performance.

**Ablation on Subgraph Tokenization.** Next we study the impact of subgraph tokenization (see Sec. 3) on the performance of VAG-CO. The same problem setting as in the annealing ablation above is used. Figure 1 (right) compares VAG-CO with the default \(k=5\) subgraph tokenization (w/ ST) to VAG-CO without subgraph tokenization (w/o ST). For a fair comparison the numbers of gradient update steps and the annealing schedules are set to be equal. We find that with subgraph tokenization we achieve a considerably better \(\hat{\epsilon}_{\mathrm{rel}}\). These results underpin that subgraph tokenization does indeed yield an improved efficiency in terms of gradient update steps.

We additionally provide a more detailed investigation on ST, where we compare VAG-CO with four different values of \(k\in\{1,2,5,10\}\) on the RB-100 MVC dataset. In our experiments we keep the number of gradient update steps constant and iteratively tune for the best learning rate and the best initial temperature in each setting of \(k\). Results are shown in Fig. 2, where \(AR^{\text{*}}\) and the average time per graph are plotted over \(k\). As \(k\) increases the performance in terms of \(AR^{\text{*}}\) does improve and the inference time is reduced considerably. This experiment underscores the performance and scalability improvement due to ST.

## 7 Limitations

While MFA-based approaches typically generate a solution to a CO problem in a single forward pass VAG-CO requires a number of forward passes that is proportional the problem size \(N\). However, methods with solution generation in a single forward pass frequently rely on post-processing procedures like CE that have a time complexity which is also linear in \(N\). In our approach the nodes in the graph are always processed in an order that is determined by a BFS. Therefore studying the impact of other node orderings or making the algorithm invariant to such orderings might be an interesting future research direction. For the annealing schedule we report comparisons for VAG-CO with three different initial temperatures but a comprehensive study on the optimization of the annealing schedule is left to future investigations.

## 8 Conclusion

Our results show that learning to solve hard CO problems requires sufficiently expressive approaches like autoregressive models. With VAG-CO we introduce a method that enables stable and efficient training of such a model through an annealed entropy regularization. We provide a theoretical motivation for this regularization method and demonstrate its practical benefit in experiments. In addition, we demonstrate that by systematically grouping solution variables VAG-CO achieves an improved performance with respect to a naive autoregressive model. Importantly VAG-CO outperforms recent approaches on numerous benchmarks and exhibits superior performance on synthetic CO problems that are designed to be hard.

Acknowledgements

We thank Gunter Klambauer for useful discussions and comments. The ELLIS Unit Linz, the LIT AI Lab, the Institute for Machine Learning, are supported by the Federal State Upper Austria. We thank the projects AI-MOTION (LIT-2018-6-YOU-212), DeepFlood (LIT-2019-8-YOU-213), Medical Cognitive Computing Center (MC3), INCONTROL-RL (FFG-81064), PRIMAL (FFG-873979), S3AI (FFG-872172), DL for GranularFlow (FFG-871302), EPILEPSIA (FFG-892171), AIRI FG 9-N (FWF-36284, FWF-36235), AL4GreenHent eatingGrids(FFG-89943), INTEGRATE (FFG-892418), ELISE (H2020-ICT-2019-3 ID: 951847), Stars4Waters (HORIZON-CL6-2021-CLIMATE-01-01). We thank Audi.JKU Deep Learning Center, TGW LOGISTICS GROUP GMBH, Silicon Austria Labs (SAL), FILL Gesellschaft mbH, Anyline GmbH, Google, ZF Friedrichshafen AG, Robert Bosch GmbH, UCB Biopharma SRL, Merck Healthcare KGaA, Verbund AG, GLS (Univ. Waterloo) Software Competence Center Hagenberg GmbH, TUV Austria, Frauscher Sensonic, TRUMPF and the NVIDIA Corporation.

## References

* Abboud et al. (2021) Ralph Abboud, Ismail Ilkan Ceylan, Martin Grohe, and Thomas Lukasiewicz. The surprising power of graph neural networks with random node initialization. In Zhi-Hua Zhou, editor, _Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI 2021, Virtual Event / Montreal, Canada, 19-27 August 2021_, pages 2112-2118. ijcai.org, 2021. doi: 10.24963/ijcai.2021/291. URL https://doi.org/10.24963/ijcai.2021/291.
* Agarap (2018) Abien Fred Agarap. Deep learning using rectified linear units (relu). _CoRR_, abs/1803.08375, 2018. URL http://arxiv.org/abs/1803.08375.
* Ahn et al. (2020) Sungsoo Ahn, Younggyo Seo, and Jinwoo Shin. Learning what to defer for maximum independent sets. In _Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event_, volume 119 of _Proceedings of Machine Learning Research_, pages 134-144. PMLR, 2020. URL http://proceedings.mlr.press/v119/ahn20a.html.
* Angelini and Ricci-Tersenghi (2023) Maria Chiara Angelini and Federico Ricci-Tersenghi. Modern graph neural networks do worse than classical greedy algorithms in solving combinatorial optimization problems like maximum independent set. _Nat. Mac. Intell._, 5(1):29-31, 2023. doi: 10.1038/s42256-022-00589-y. URL https://doi.org/10.1038/s42256-022-00589-y.
* Ba et al. (2016) Lei Jimmy Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization. _CoRR_, abs/1607.06450, 2016. URL http://arxiv.org/abs/1607.06450.
* Barabasi and Albert (1999) Albert-Laszlo Barabasi and Reka Albert. Emergence of scaling in random networks. _Science_, 286(5439):509-512, 1999. doi: 10.1126/science.286.5439.509. URL https://www.science.org/doi/abs/10.1126/science.286.5439.509.
* Barbier et al. (2013) Jean Barbier, Florent Krzakala, Lenka Zdeborova, and Pan Zhang. The hard-core model on random graphs revisited. _Journal of Physics: Conference Series_, 473(1):012021, dec 2013. doi: 10.1088/1742-6596/473/1/012021. URL https://dx.doi.org/10.1088/1742-6596/473/1/012021.
* Battaglia et al. (2018) Peter W. Battaglia, Jessica B. Hamrick, Victor Bapst, Alvaro Sanchez-Gonzalez, Vinicius Flores Zambaldi, Mateusz Malinowski, Andrea Tacchetti, David Raposo, Adam Santoro, Ryan Faulkner, Caglar Gulcehre, H. Francis Song, Andrew J. Ballard, Justin Gilmer, George E. Dahl, Ashish Vaswani, Kelsey R. Allen, Charles Nash, Victoria Langston, Chris Dyer, Nicolas Heess, Daan Wierstra, Pushmeet Kohli, Matthew M. Botvinick, Oriol Vinyals, Yujia Li, and Razvan Pascanu. Relational inductive biases, deep learning, and graph networks. _CoRR_, abs/1806.01261, 2018. URL http://arxiv.org/abs/1806.01261.
* Bello et al. (2017) Irwan Bello, Hieu Pham, Quoc V. Le, Mohammad Norouzi, and Samy Bengio. Neural combinatorial optimization with reinforcement learning. In _5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Workshop Track Proceedings_. OpenReview.net, 2017. URL https://openreview.net/forum?id=Bk9mrlSFx.
* Bengio et al. (2009) Yoshua Bengio, Jerome Louradour, Ronan Collobert, and Jason Weston. Curriculum learning. In _Proceedings of the 26th Annual International Conference on Machine Learning_, ICML '09, page 41-48, New York, NY, USA, 2009. Association for Computing Machinery. ISBN 9781605585161. doi: 10.1145/1553374.1553380. URL https://doi.org/10.1145/1553374.1553380.
* Bengio et al. (2021) Yoshua Bengio, Andrea Lodi, and Antoine Prouvost. Machine learning for combinatorial optimization: A methodological tour d'horizon. _Eur. J. Oper. Res._, 290(2):405-421, 2021. doi: 10.1016/j.ejor.2020.07.063. URL https://doi.org/10.1016/j.ejor.2020.07.063.
* Bother et al. (2022) Maximilian Bother, Otto Kligis, Martin Taraz, Sarel Cohen, Karen Seidel, and Tobias Friedrich. What's wrong with deep learning in tree search for combinatorial optimization. In _The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022_. OpenReview.net, 2022. URL https://openreview.net/forum?id=mk0Hzdq7Yi1.
* Cappart et al. (2021) Quentin Cappart, Didier Chetelat, Elias B. Khalil, Andrea Lodi, Christopher Morris, and Petar Velickovic. Combinatorial optimization and reasoning with graph neural networks. In Zhi-Hua Zhou, editor, _Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI 2021, Virtual Event / Montreal, Canada, 19-27 August 2021_, pages 4348-4355. ijcai.org, 2021. doi: 10.24963/ijcai.2021/595. URL https://doi.org/10.24963/ijcai.2021/595.
* Cortes et al. (2015) Corinna Cortes, Vitaly Kuznetsov, Mehryar Mohri, and Umar Syed. Structural maxent models. In Francis R. Bach and David M. Blei, editors, _Proceedings of the 32nd International Conference on Machine Learning, ICML 2015, Lille, France, 6-11 July 2015_, volume 37 of _JMLR Workshop and Conference Proceedings_, pages 391-399. JMLR.org, 2015. URL http://proceedings.mlr.press/v37/cortes15.html.
* C. C.

Miroslav Dudik, Steven J. Phillips, and Robert E. Schapire. Maximum entropy density estimation with generalized regularization and an application to species distribution modeling. _J. Mach. Learn. Res._, 8:1217-1260, 2007. doi: 10.5555/1314498.1314540. URL https://dl.acm.org/doi/10.5555/1314498.1314540.
* Optimization [2023] Gurobi Optimization, LLC. Gurobi Optimizer Reference Manual, 2023. URL https://www.gurobi.com.
* Hibat-Allah et al. [2021] Mohamed Hibat-Allah, Estelle M. Inack, Roeland Wiersema, Roger G. Melko, and Juan Carrasquilla. Variational neural annealing. _Nat. Mach. Intell._, 3(11):952-961, 2021. doi: 10.1038/s42256-021-00401-3. URL https://doi.org/10.1038/s42256-021-00401-3.
* Jaakkola and Jordan [1998] Tommi S. Jaakkola and Michael I. Jordan. Improving the mean field approximation via the use of mixture distributions. In Michael I. Jordan, editor, _Learning in Graphical Models_, volume 89 of _NATO ASI Series_, pages 163-173. Springer Netherlands, 1998. doi: 10.1007/978-94-011-5014-9_6. URL https://doi.org/10.1007/978-94-011-5014-9_6.
* Jaynes [1957] E. T. Jaynes. Information theory and statistical mechanics. _Phys. Rev._, 106:620-630, May 1957. doi: 10.1103/PhysRev.106.620. URL https://link.aps.org/doi/10.1103/PhysRev.106.620.
* Karalias and Loukas [2020] Nikolaos Karalias and Andreas Loukas. Erdos goes neural: an unsupervised learning framework for combinatorial optimization on graphs. In Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, _Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual_, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/49f85a9ed090b20c8bed85a5923c669f-Abstract.html.
* Karalias et al. [2022] Nikolaos Karalias, Joshua Robinson, Andreas Loukas, and Stefanie Jegelka. Neural set function extensions: Learning with discrete functions in high dimensions. In _NeurIPS_, 2022. URL http://papers.nips.cc/paper_files/paper/2022/hash/6294a235c0b80f0a2b224375c546c750-Abstract-Conference.html.
* Kazama and Tsuji [2003] Jun'ichi Kazama and Jun'ichi Tsuji. Evaluation and extension of maximum entropy models with inequality constraints. In _Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP 2003, Sapporo, Japan, July 11-12, 2003_, 2003. URL https://aclanthology.org/W03-1018/.
* Khalil et al. [2017] Elias B. Khalil, Hanjun Dai, Yuyu Zhang, Bistra Dilkina, and Le Song. Learning combinatorial optimization algorithms over graphs. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett, editors, _Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA_, pages 6348-6358, 2017. URL https://proceedings.neurips.cc/paper/2017/hash/d9896106ca98d305b8cbdf4fd8b13a1-Abstract.html.
* Khandoker et al. [2023] Shoummo Ahsan Khandoker, Jawaril Munshad Abedin, and Mohamed Hibat-Allah. Supplementing recurrent neural networks with annealing to solve combinatorial optimization problems. _Mach. Learn. Sci. Technol._, 4(1):15026, 2023. doi: 10.1088/2632-2153/acb895. URL https://doi.org/10.1088/2632-2153/acb895.
* Kipf and Welling [2017] Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. In _International Conference on Learning Representations_, 2017. URL https://openreview.net/forum?id=SJU4ayYgl.
* Kirkpatrick et al. [1983] S. Kirkpatrick, C. D. Gelatt, and M. P. Vecchi. Optimization by simulated annealing. _Science_, 220(4598):671-680, 1983. doi: 10.1126/science.220.4598.671. URL https://www.science.org/doi/abs/10.1126/science.220.4598.671.
* Kool et al. [2019] Wouter Kool, Herke van Hoof, and Max Welling. Attention, learn to solve routing problems! In _7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019_. OpenReview.net, 2019. URL https://openreview.net/forum?id=ByxBFsRqYm.
* Leskovec and Krevl [2014] Jure Leskovec and Andrej Krevl. SNAP Datasets: Stanford large network dataset collection. http://snap.stanford.edu/data, June 2014.
* Liu et al. [2021] Fenglin Liu, Xuancheng Ren, Zhiyuan Zhang, Xu Sun, and Yuexian Zou. Rethinking skip connection with layer normalization in transformers and resnets. _CoRR_, abs/2105.07205, 2021. URL https://arxiv.org/abs/2105.07205.
* Loshchilov and Hutter [2017] Ilya Loshchilov and Frank Hutter. SGDR: Stochastic gradient descent with warm restarts. In _International Conference on Learning Representations_, 2017. URL https://openreview.net/forum?id=Skq89Scxx.
* Liu et al. [2021]* Lucas (2014) Andrew Lucas. Ising formulations of many np problems. _Frontiers in Physics_, 2, 2014. ISSN 2296-424X. doi: 10.3389/fphy.2014.00005. URL https://www.frontiersin.org/articles/10.3389/fphy.2014.00005.
* MacKay (2003) David J. C. MacKay. _Information theory, inference, and learning algorithms_. Cambridge University Press, 2003. ISBN 978-0-521-64298-9.
* Mazyavkina et al. (2021) Nina Mazyavkina, Sergey Sviridov, Sergei Ivanov, and Evgeny Buraev. Reinforcement learning for combinatorial optimization: A survey. _Comput. Oper. Res._, 134:105400, 2021. doi: 10.1016/j.cor.2021.105400. URL https://doi.org/10.1016/j.cor.2021.105400.
* Mei et al. (2019) Song Mei, Theodor Misiakiewicz, and Andrea Montanari. Mean-field theory of two-layers neural networks: dimension-free bounds and kernel limit. In Alina Beygelzimer and Daniel Hsu, editors, _Conference on Learning Theory, COLT 2019, 25-28 June 2019, Phoenix, AZ, USA_, volume 99 of _Proceedings of Machine Learning Research_, pages 2388-2464. PMLR, 2019. URL http://proceedings.mlr.press/v99/mei19a.html.
* Min et al. (2022) Yimeng Min, Frederik Wenkel, Michael Perlmutter, and Guy Wolf. Can hybrid geometric scattering networks help solve the maximum clique problem? In _NeurIPS_, 2022. URL http://papers.nips.cc/paper_files/paper/2022/hash/8ec88961d36d9a87ac24baf45402744f-Abstract-Conference.html.
* Morris et al. (2020) Christopher Morris, Nils M. Kriege, Franka Bause, Kristian Kersting, Petra Mutzel, and Marion Neumann. Tudataset: A collection of benchmark datasets for learning with graphs. _CoRR_, abs/2007.08663, 2020. URL https://arxiv.org/abs/2007.08663.
* Mezard and Montanari (2009) Marc Mezard and Andrea Montanari. _Information, Physics, and Computation_. Oxford University Press, 01 2009. ISBN 9780198570837. doi: 10.1093/acprof:oso/9780198570837.001.0001. URL https://doi.org/10.1093/acprof:oso/9780198570837.001.0001.
* Papadimitriou and Steiglitz (1998) Christos H Papadimitriou and Kenneth Steiglitz. _Combinatorial optimization: algorithms and complexity_. Courier Corporation, 1998.
* Parisi (1988) G Parisi. Statistical field theory, vol. 66 in. _Frontiers in Physics_, page 343, 1988.
* Qiu et al. (2022) Ruizhong Qiu, Zhiqing Sun, and Yiming Yang. DIMES: A differentiable meta solver for combinatorial optimization problems. In _NeurIPS_, 2022. URL http://papers.nips.cc/paper_files/paper/2022/hash/a3a7387e49f4de290c23beeea2dfcd75-Abstract-Conference.html.
* Raghavan (1988) Prabhakar Raghavan. Probabilistic construction of deterministic algorithms: approximating packing integer programs. _Journal of Computer and System Sciences_, 37(2):130-143, 1988.
* Schulman et al. (2016) John Schulman, Philipp Moritz, Sergey Levine, Michael I. Jordan, and Pieter Abbeel. High-dimensional continuous control using generalized advantage estimation. In Yoshua Bengio and Yann LeCun, editors, _4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings_, 2016. URL http://arxiv.org/abs/1506.02438.
* Schulman et al. (2017) John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. _CoRR_, abs/1707.06347, 2017. URL http://arxiv.org/abs/1707.06347.
* Sun et al. (2022) Haoran Sun, Etash Kumar Guha, and Hanjun Dai. Annealed training for combinatorial optimization on graphs. In _OPT 2022: Optimization for Machine Learning (NeurIPS 2022 Workshop)_, 2022. URL https://openreview.net/forum?id=fo3b0XjTKU.
* Tonshoff et al. (2020) Jan Tonshoff, Martin Ritzert, Hinikus Wolf, and Martin Grohe. Graph neural networks for maximum constraint satisfaction. _Frontiers Artif. Intell._, 3:580607, 2020. doi: 10.3389/frai.2020.580607. URL https://doi.org/10.3389/frai.2020.580607.
* Wainwright and Jordan (2008) Martin J. Wainwright and Michael I. Jordan. Graphical models, exponential families, and variational inference. _Found. Trends Mach. Learn._, 1(1-2):1-305, 2008. doi: 10.1561/2200000001. URL https://doi.org/10.1561/2200000001.
* Wang et al. (2022) Haoyu Wang, Nan Wu, Hang Yang, Cong Hao, and Pan Li. Unsupervised learning for combinatorial optimization with principled objective relaxation. In _NeurIPS_, 2022. URL http://papers.nips.cc/paper_files/paper/2022/hash/cbc1ad2066f0afebbcea930c5688fc1f-Abstract-Conference.html.
* Wang and Li (2023) Haoyu Peter Wang and Pan Li. Unsupervised learning for combinatorial optimization needs meta learning. In _The Eleventh International Conference on Learning Representations_, 2023. URL https://openreview.net/forum?id=-ENYHEC8zBp.

* Wormald (1995) Nicholas C Wormald. Differential equations for random processes and random graphs. _The annals of applied probability_, pages 1217-1235, 1995.
* Wu et al. (2019) Dian Wu, Lei Wang, and Pan Zhang. Solving statistical mechanics using variational autoregressive networks. _Phys. Rev. Lett._, 122:080602, Feb 2019. doi: 10.1103/PhysRevLett.122.080602. URL https://link.aps.org/doi/10.1103/PhysRevLett.122.080602.
* Wu et al. (2021) Xiaoxia Wu, Ethan Dyer, and Behnam Neyshabur. When do curricula work? In _9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021_. OpenReview.net, 2021. URL https://openreview.net/forum?id=tW4QEInpni.
* August 5, 2005_, pages 337-342. Professional Book Center, 2005. URL http://ijcai.org/Proceedings/05/Papers/0989.pdf.
* Yehuda et al. (2020) Gal Yehuda, Moshe Gabel, and Assaf Schuster. It's not what machines can learn, it's what we cannot teach. In _Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event_, volume 119 of _Proceedings of Machine Learning Research_, pages 10831-10841. PMLR, 2020. URL http://proceedings.mlr.press/v119/yehuda20a.html.
* Zhang et al. (2023) Dinghuai Zhang, Hanjun Dai, Nikolay Malkin, Aaron C. Courville, Yoshua Bengio, and Ling Pan. Let the flows tell: Solving graph combinatorial optimization problems with gflownets. _CoRR_, abs/2305.17010, 2023. doi: 10.48550/arXiv.2305.17010. URL https://doi.org/10.48550/arXiv.2305.17010.

Appendix

### Evaluation Metrics

We use different metrics to evaluate the performance of models on CO problems. We assess the quality of an individual solution \(\bm{\sigma}\) by the associated value of the energy function \(E_{m}(\bm{\sigma})\) which represents the size of the solution sets in MIS and MVC. Here \(m\) refers to the problem instance under consideration. Optimal solutions \(\bm{\sigma}_{\mathrm{opt}}\) are obtained with the Gurobi solver [Gurobi Optimization, LLC, 2023]. For models that generate solutions indeterministically we sample \(n_{S}\) different solutions \(\bm{\sigma}_{j}\) per problem instance and calculate the test dataset average of the best relative error \(\epsilon_{\mathrm{rel}}^{\text{*}}\):

\[\epsilon_{\mathrm{rel}}^{\text{*}}=\frac{1}{M}\sum_{m=1}^{M}\min_{j}\frac{|E_{ m}(\bm{\sigma}_{opt})-E_{m}(\bm{\sigma}_{j})|}{|E_{m}(\bm{\sigma}_{\mathrm{opt}})|},\] (9)

where \(M\) denotes the number of problem instances in the test dataset. In case of deterministic algorithms only one sample is generated, i.e. \(n_{S}=1\). In several experiments it is insightful to investigate the average relative error \(\hat{\epsilon}_{\mathrm{rel}}\) for which we take the average instead of the minimum in Eq. 9. Analogously, we also define the best approximation ratio AR* by

\[\text{AR}^{\text{*}}=\frac{1}{M}\sum_{m=1}^{M}\min_{j}\frac{|E_{m}(\bm{\sigma}_ {j})|}{|E_{m}(\bm{\sigma}_{\mathrm{opt}})|},\] (10)

The average approximation ratio \(\widehat{\text{AR}}\) is defined by taking the average instead of the minimum operation in Eq. 10. For VAG-CO we calculate the average over the ordered greedy (OG) sampling method (See. App A.7). We always report the evaluation metric together with the standard error over three different seeds except for the experiment on RRG-100 MIS, where only results on one seed are reported.

### Linear Integer Program Formulations

For Gurobi we formulate the CO Problems studied in this paper in Linear Integer Program Formulation (LIP) (see Tab. 5).

### Ensuring Feasible Solutions

Since there is no rigorous guarantee that the model samples only feasible solutions that satisfy the constraints, we use a fast post processing procedure to make sure that only feasible solutions are sampled. Here, we make use of our choice of the relative weighting of the energy terms \(A\) and \(B\) (see Tab. 1) in the Ising formulation, which ensures that only feasible solutions are minima in the energy landscape. Therefore, we can detect violations when \(E_{B}>0\) and search for the spin that causes the largest amount of violations. Subsequently, we change the spin value of the node with the highest number of violations to satisfy the constraint and repeat the process until \(E_{B}=0\). We observe in our experiments that this post-processing step is typically unnecessary, since only in rare cases violating solutions are sampled.

\begin{table}
\begin{tabular}{c|c c c} Problem Type & \multicolumn{3}{c}{ILP formulation \(q\in\{0,1\}^{N}\)} \\ \hline MVC & \(\min\limits_{\bm{q}}\) & \(\sum\limits_{i=1}^{N}q_{i}\) & s.t. \(q_{i}+q_{j}\geqslant 1\) if \((i,j)\in\mathcal{E}\) \\ \hline MIS & \(\min\limits_{\bm{q}}\) & \(-\sum\limits_{i=1}^{N}q_{i}\) & s.t. \(q_{i}+q_{j}\leqslant 1\) if \((i,j)\in\mathcal{E}\) \\ \hline MaxCl & \(\min\limits_{\bm{q}}\) & \(\sum\limits_{i=1}^{N}q_{i}\) & s.t. \(q_{i}+q_{j}\leqslant 1\) if \((i,j)\notin\mathcal{E}\) \\ \hline MaxCut & \(\max\limits_{e\in\{0,1\}^{|\mathcal{E}|},q}\) & \(\sum\limits_{(i,j)\in\mathcal{E}}^{N}e_{ij}\) \\  & s.t. \(e_{ij}\leqslant q_{i}+q_{j}\) if \((i,j)\in\mathcal{E}\) \\  & & \(e_{ij}\leqslant 2-(q_{i}+q_{j})\) if \((i,j)\in\mathcal{E}\) \\ \end{tabular}
\end{table}
Table 5: Integer Linear Program (ILP) formulations of MIS, MVC, MaxCl and MaxCut.

### Standardization of the Energy Scale

Since the energy scale of CO problems can vary significantly, a good choice of hyperparameters like the initial temperature \(T_{0}\), learning rate and relative weighting between the policy and value loss can vary between different CO problem instances. Therefore, we standardize the energy scale that makes the choice of good hyperparameters easier. For this purpose we first express the binary energy function \(E(q)\) (see. Tab 1) in terms of spins, i.e. as \(E(\bm{\sigma})\) by substituting \(q_{i}\) with \(\frac{\sigma_{i}+1}{2}\). We then sample states for CO problem instances from the training set by using a Random Greedy Algorithm (RGA, see App. A.6.2). Since we use only a few RGA steps (see App. A.6.2), this algorithm performs only slightly better than a completely random algorithm. From these states the mean energy \(\hat{E}_{\mathrm{RGA}}\) and the standard deviation \(\mathrm{std}(E_{\mathrm{RGA}})\) over the training dataset is computed. Subsequently, we standardize the energy scale by:

\[\hat{E}(\bm{\sigma})=\frac{E(\bm{\sigma})-\hat{E}_{\mathrm{RGA}}}{\mathrm{std} (E_{\mathrm{RGA}})}.\] (11)

### Baseline Methods

We compare VAG-CO with our own implementation of a Mean Field Approximation (MFA) and to Erdos Goes Neural (EGN). In both of these approaches we consider the case with and without annealing. In both methods the probability of a state \(\sigma\) factorizes into a product of independent Bernoulli probabilities. Therefore, the state probability is given by:

\[p_{\theta}(\bm{\sigma}|E)=\prod_{i=1}^{N}p_{\theta}(\sigma_{i}|E).\] (12)

While EGN and MFA both use the same probability distribution they differ in how the energy, or in case of annealing the free energy, is calculated. This is described in the following.

#### a.5.1 Mean Field Approximation

We estimate the energy by sampling states from the parameterized probability distribution and apply REINFORCE with variance reduction to update the network parameters. The network parameter gradients for one CO Problem Instance are then calculated with

\[\Delta\theta(E)=\mathop{\mathbb{E}}_{\bm{\sigma}\sim p_{\theta}(\bm{\sigma}|E )}\left[(E(\bm{\sigma})-b)\nabla_{\theta}\log p_{\theta}(\bm{\sigma}|E)\right],\]

where \(b=\mathop{\mathbb{E}}_{\bm{\sigma}\sim p_{\theta}(\bm{\sigma}|E)}[E(\bm{\sigma})\). For a batch of CO Problem Instances \(\Delta\theta(E)\) is averaged over multiple \(E\).

#### a.5.2 MFA with Annealing

In MFA-Anneal we estimate the free energy instead and again update the network parameters with REINFORCE with variance reduction as explained above.

#### a.5.3 Egn

Since we consider cases, where the energy function is known in Ising formulation (see Sec. 1) the expectation \(\mathop{\mathbb{E}}_{\bm{\sigma}\sim p_{\theta}(\bm{\sigma}|E)}[E(\bm{\sigma})\) can be written down in closed form by substituting \(\sigma_{i}\to 2\cdot p_{\theta}(\sigma_{i}|E)-1\)(Karalias and Loukas, 2020). Therefore, the loss function for one CO Problem Instance can be computed with

\[L(E)=E(p_{\theta}(\bm{\sigma}|E))\]

and the network parameters can be updated directly by computing the gradients of this loss function.

#### a.5.4 EGN with Annealing

In case of EGN-Anneal (Sun et al., 2022) the entropy of the parameterized probability distribution is calculated in closed form with the following formula:

\[H(p_{\theta}(\bm{\sigma}|E))=-\sum_{i}^{N}[p_{\theta}(\bm{\sigma}_{i}|E)\cdot \log p_{\theta}(\bm{\sigma}_{i}|E)+(1-p_{\theta}(\bm{\sigma}_{i}|E))\cdot\log( 1-p_{\theta}(\bm{\sigma}_{i}|E)]\]

#### a.5.5 Conditional Expectation

To obtain good samples from the MFA approach in a deterministic way, we adopt the Conditional Expectation (CE) method (Raghavan, 1988) as described in Karalias and Loukas (2020).

To introduce randomness into the CE-based solution generation procedure we initialize the random node features (which are processed by the GNN) of every node with a random vector with six binary entries that are drawn from a uniform distribution. Unless stated otherwise, we follow the procedure in (Wang and Li, 2023) and report the best CE result of eight different random node feature initializations.

### Greedy Algorithms

#### a.6.1 Degree Based Greedy Algorithm

The Degree Based Greedy (DB-Greedy) algorithm (Wormald, 1995) is a polynomial time algorithm for the MIS problem. The DB-Greedy algorithm works in the following way: At first all nodes are sorted according to their degrees. Then, starting from the smallest degree the node is chosen to be part of the independent set. In the next step this node, its neighbors and their corresponding edges are deleted from the graph. These steps are repeated until the graph is empty.

This algorithm can also be applied to the MVC problem by using the fact that the complement of an independent set is a vertex cover. In other words, nodes in the independent set are excluded from the vertex cover and nodes that are not part of the independent set are included into the vertex cover.

#### a.6.2 Random Greedy Algorithm

The Random Greedy Algorithm (RGA) is a general approach that can be utilized for solving a broad range of CO problems that can be mapped to the Ising model. Initially, the algorithm randomly samples spin values with uniform probability. Then, for a fixed number of iterations a spin is randomly selected and its value is changed if it decreases the energy value. We set the number of iterations to \(N\cdot n_{R}\), where \(N\) is the number of nodes in the graph and \(n_{R}\) is the number of repetitions per node.

For the purpose of the standardization the energy scale (see. App. A.4), we employ this algorithm with \(n_{R}\) set to one.

### Study on Sampling Methods

In order to find out, how the best samples can be obtained with VAG-CO we study in Fig. 3 three different sampling methods on the RRG-100 MIS and RB-200 MVC dataset. Here we evaluate \(\epsilon_{\text{rel}}^{*}\) on the test dataset and plot it over the number of samples \(n_{S}\) that are used for each graph in the dataset. We also add MFA and DB-Greedy results, where we show for MFA how the method improves when more solutions are sampled. Since DB-Greedy is a deterministic algorithm, we draw a horizontal line that indicates the solution quality of the algorithm. To draw a relation to results that are presented in Fig. 1 (Left, Middle) we also add a vertical dotted line at \(n_{S}=8\) samples. For VAG-CO we denote the first method as _sampling_ (S), where for each graph \(n_{S}\) solutions are sampled according to the corresponding probability distribution. In the second VAG-CO sampling method called _ordered sampling_ (OS), we use for each graph \(n_{O}\) different BFS node orderings and sample \(n_{S}/n_{O}\) states per graph ordering. Finally, we sample VAG-CO solutions with a method called _ordered greedy_ (OG), where we generate solutions greedily for each BFS ordering with \(n_{O}=n_{S}\). Results in Fig. 3 show that the sampling strategy OG always outperforms all other sampling strategies that we proposed for VAG-CO. Additionally, we see that as we increase \(n_{O}\) in the OS sampling strategy, the performance improves consistently. Remarkably, DB-Greedy exhibits the best solution quality when MFA and VAG-CO are allowed only one sample (\(n_{S}=1\)). However, DB-Greedy is outperformed by VAG-CO OG already with a modest amount of \(n_{S}>1\) samples.

### Experimental Details

In this section we provide additional details to experiments that are presented in Sec. 6.

**Parameter Checkpointing.** With VAG-CO we checkpoint over the course of training the parameters that obtain the best \(\epsilon_{\text{rel}}^{*}\) and the best \(\hat{\epsilon}_{\text{rel}}\) on the validation set. For testing we always use the checkpoint with the best \(\hat{\epsilon}_{\text{rel}}\), except for the results in Fig. 3 when the sampling strategies (S) and (OS) are used. Here we use the checkpoints with the best \(\epsilon_{\text{rel}}^{*}\).

**Hyperparameter Tuning.** For MFA-Anneal, the learning rate and initial temperature are tuned on the validation dataset of Enzymes MIS via a grid search. We considered learning rates (\(\text{lr}\in[5\times 10^{-4},1\times 10^{-4},5\times 10^{-5}]\)) and initial temperatures (\(T_{0}\in[0.5,0.25,0.1,0.05]\)). With that, the number of GNN layers (\(L\in[6,8,12]\)) are tuned. Finally, the annealing duration (\(N_{\text{anneal}}\)) is increased until we observe that longer annealing does not leadto improvements. For MFA, we tested on ENZYMES MIS the learning rates (\(\mathrm{lr}\in[5\times 10^{-4},1\times 10^{-4},5\times 10^{-5},1\times 10^{-5}]\)) and with that tuned the number of GNN layers (\(L\in[6,8,10,12]\)) and stop training when no significant improvement on the validation set is observed. In MFA and MFA-Anneal, when training on the other datasets we started with the optimal ENZYMES MIS parameters and further tested different learning rates (lr), initial temperatures (\(T_{0}\)) and number of GNN layers (\(L\)) individually on each dataset. For experiments on the RRG-100 MIS and RB-200 MVC we make a more extensive hyperparameter search. In RRG-100 MIS for MFA-Anneal we first tuned the initial temperature in the range of \(T_{0}\in[0.25,0.1,0.05,0.035,0.025,0.015,0.01]\) and in RB-200 MVC we search for the best initial temperature within the range of \(T_{0}\in[0.5,0.25,0.1,0.05]\). For MFA and MFA-Anneal, we then iteratively search for the best learning rate in the range of \(\mathrm{lr}\in[5\times 10^{-4},1\times 10^{-4},5\times 10^{-5}]\) and then for the best number of GNN layers \(L\in[6,8,12]\).

In the experiments with EGN on RB-200 MVC we also performed a extensive hyperparameter search, where first the best temperature \(T_{0}\) was tuned within the range of \(T_{0}\in[0.25,0.1,0.05]\). Then we tuned the best learning rate within the range of \(lr\in[5\cdot 10^{-4},1\cdot 10^{-1},5\cdot 10^{-5},1\cdot 10^{-5}]\). Then we tuned for the best number of GNN layers with the range of \(L\in[8,10,12,14]\). In EGN without annealing the same hyperparameter search was performed but at \(T_{0}=0\). The best hyperparameters of all methods are listed in App. A.13.

On VAG-CO we iteratively tuned the learning rate (\(\mathrm{lr}\in[5\cdot 10^{-4},1\cdot 10^{-3}]\)), initial temperature (\(T_{0}\in[0.05,0.7,0.1]\)), number of GNN layers (\(L\in[3,4]\)) and number of annealing steps (\(N_{\mathrm{anneal}}\in[3000,6000]\)) on the ENZYMES MIS validation dataset. We then initially test these hyperparameters on other datasets and adapt the number of annealing steps and the initial temperature. The choice of all hyperparameters in VAG-CO is listed in Tab. 7.

**Ablation on Subgraph Tokenization.** In the ablation on subgraph tokenization in Fig. 1 we keep all hyperparameters the same,except for the time horizon \(\mathcal{T}\) (see Sec. A.9) and the hyperparameter \(\lambda\) in the PPO algorithm (see App. A.9). For the subgraph tokenization run we chose \(\mathcal{T}=20\), \(\lambda=0.95\) and \(k=5\). Therefore, the time horizon includes the generation of \(\mathcal{T}\cdot k=100\) spins per graph during the data collection phase (see App. A.9). If we would keep \(\mathcal{T}\) the same, when we chose \(k=1\) for the run without subgraph tokenization only \(\mathcal{T}\cdot k=20\) spins would be generated for each graph. Therefore, we use \(\mathcal{T}=100\) when no subgraph tokenization is used. As we always set \(\lambda=1-\frac{1}{\mathcal{T}}\) this hyperparameter is adapted accordingly.

**Experiments on Random Regular Graphs.** Since GNNs suffer from node ambiguity on Random Regular Graphs (RRGs) Wang and Li (2023) using random node features Abboud et al. (2021) can resolve this issue. Therefore, for the VAG-CO experiment in Fig. 1 (Left) we sample standard Gaussian random node features with the dimension of six and concatenate them to the graph representation of VAG-CO. For MFA and MFA-Anneal random node features are already used (see App. A.5.5) and had not to be added for the RRG experiments.

**Averaged results on hard instances.** In Fig. 1 (Left, Middle) only show results for specific generation parameters \(p\) on the RB-200 dataset and for specific values of \(d\) on the RRG-100 dataset. Here we report the corresponding averaged results on the RRG-100 MIS and RB-200 MVC dataset in Tab. 6. VAG-CO significantly outperforms all other methods for \(n_{S}=8\).

### Proximal Policy Optimization

Proximal Policy Optimization (PPO) (Schulman et al., 2017) is a popular RL algorithm that has two main components. The policy is represented by the network \(p_{\theta}(\sigma_{i}|G_{i})\) with parameters \(\theta\). The expected future reward is estimated by the value network \(V_{\phi}(G_{i})\) which is parameterized by \(\phi\).

Figure 3: Ablation on different sampling strategies for VAG-CO on the RRG-100 MIS dataset (Left) and on the RB-200 MVC Dataset (Right). The averaged best relative error \(\epsilon_{\mathrm{rel}}^{*}\) on the test dataset is plotted over the number of solutions \(n_{S}\) per graph. Error bars indicate the standard error over the test dataset.

[MISSING_PAGE_FAIL:20]

### Model Details and GNN Architectures

Before processing the graph representation with GNNs, we encode its node features with an encoder MLP. To obtain node embeddings \(\mathcal{N}_{i}^{l}\in\mathbb{R}^{d(l)}\) with \(l\in[1,\ldots,L]\) that depend on the graph structure, the encoded node features are processed by \(L\) layers of GNNs. Afterwards, we apply a global sum aggregation to compute a global graph embedding. The global graph embedding is then concatenated to the node embedding \(\mathcal{N}_{i}^{L}\), where \(i\) is the index of the spin whose value is to be generated (see Sec. 3). We then use this embedding to calculate the policy and value function outputs (see App. A.9) with two separate MLPs.

In our experiments, we employ two distinct message passing architectures. The first one is a Message-passing Neural Network (MPNN) [Battaglia et al., 2018] but with linear message functions. The second architecture also includes skip connections (MPNN-skip) [Battaglia et al., 2018] which we use when more than three GNN layers are used.

**Message Passing Neural Network.** Our MPNN layer is defined by the following update for node embeddings:

\[\mathcal{N}_{i}^{L+1}=\ln\left[\Psi\left(\mathcal{N}_{i}^{L},\sum_{j\in N(i) }\Phi(\mathcal{N}_{j}^{l},\mathcal{N}_{i}^{l},\mathcal{E}_{ij})\right)+W_{ \mathrm{skip}}\,\mathcal{N}_{i}^{L}\right],\] (15)

where \(\Phi(\mathcal{N}_{j}^{l},\mathcal{N}_{i}^{l},\mathcal{E}_{ij})\) is a message update MLP that computes messages with pairwise interactions between nodes. Skip connection are implemented by adding the term \(W_{\mathrm{skip}}\,\mathcal{N}_{i}^{L}\) after the node update, where \(W_{\mathrm{skip}}\in\mathbb{R}^{d(l+1)\times d(l)}\) is a weight matrix. Finally, layer normalization \(\ln[\cdot]\)[Ba et al., 2016] is applied as it is commonly done in Neural Network architectures when skip connections are used [Liu et al., 2021].

In VAG-CO we use a policy and value MLP with three layers. Both MLPs have 120 neurons in the first two layers. The value MLP has only one neuron in the output layer and the policy MLP has \(2^{k}\) output neurons. For the encoder MLP we use a two layer MLP with 40 neurons in each layer. In case of the node update MLP \(\Psi\) and the message update MLP \(\Phi\) we use 2 layers with 64 neurons in each layer. Layer normalization \(\ln[\cdot]\) is applied after every layer within a MLP except in the output layer of the policy and value MLP. We use Rectified Linear Unit (ReLU) activation functions Agarap [2018] except in the output of the policy MLP, where a softmax activation is used and except of the output of the value MLP where no activation is used.

In MFA with and without annealing the model uses an encoder MLP with one layer and \(64\) neurons. The node update MLP \(\Psi\) and the message update MLP \(\Phi\) have the same number of neurals as in VAG-CO. The output MLP that is applied on each node after \(n\) GNN message passing steps the has three layers with \(64\) neurons each and the final output layer has \(2\) neurons with a softmax activation.

#### a.10.1 Subgraph Tokenization

For subgraph tokenization we have to make changes to the one-hot encoding as described in Sec. 3 and also to the model architecture as described in App. A.10. The one-hot encoding in the graph representation is adapted so that spins \(\sigma_{i:i+k}\) that are going to be generated receive an enumeration that indicates their position in the sliced list of BFS-ordered (see Sec. 3) indices \(i:i+k\). Then, the graph \(G_{i}\) is processed by a GNN that provides a node embedding \(GNN_{\theta}(x_{i})\) for each node. Along with a global sum aggregation, the node embeddings that are going to be generated are then concatenated according to their BFS order (Sec. 3) and further processed by the policy MLP that calculates the probability for each of the \(2^{k}\) spin configurations using a softmax output layer.

When the number of nodes \(N\) in the graph is not dividable by \(k\), the number vertices in the CO problem instance description has to be increased without changing the inherent optimization objective. This can be realised by adding a sufficient amount of spins into the Ising model (see Sec. 2) with zero spin weight \(B\) and no connections to other spins.

### Training Time and Computational Resources

All runs with VAG-CO were conducted either on A100 Nvidia GPU with 40 GB Memory or an A40 Nvidia GPUs with 48 GB Memory. In case of COLLAB MVC an A100 with 80 GB Memory is used.

The training time of our algorithm depends hyperparameters like the number of annealing steps \(N_{\mathrm{anneal}}\) (see App. A.14), the number of edges and nodes of graphs in the dataset, on the GPUs that are used during training, on the time horizon and on the minibatch size that is used for gradient updates in PPO (see App. A.9). For example the TWITTER MVC run with \(N_{\mathrm{anneal}}=4000,\mathcal{T}=30,H_{\mathrm{minib}}=N_{\mathrm{ minib}}=S_{\mathrm{minib}}=10\) takes one day, when trained on an A100 40 GB Nvidia GPU. The run on ENZYMES MIS trained on a A40 GPU takes ten hours, when trained with \(N_{\mathrm{anneal}}=6000,\mathcal{T}=20,H_{\mathrm{minib}}=N_{\mathrm{ minib}}=15\) and \(S_{\mathrm{minib}}=10\).

### Time measurement

**Gurobi.** All Gurobi results are conducted on Intel Xeon Platinum 8168 @ 2.70GHz CPU with 24 cores. All Problems expect for MaxCut are formualted as LIP and we set the number of threads for Gurobi to 26.

**Learned Methods.** For all learned methods (EGN, MFA, VAG-CO) we present the time per graph as if the algorithm are implemented in a parallelized manner, where each sampling process runs on a separate thread. We always report runtimes by using the jit compiled performance of the Graph Neural Network.

### Hyperparameters

**VAG-CO Hyperparameters.**

All hyperparameters that change across datasets are listed in Tab. 7, whereas hyperparameters that stay the same are specified in the corresponding section (e.g. App. A.10, A.14).

**MFA Hyperparameters.**

All runs with MFA used a batch size \(H\) of \(32\) and we sampled \(n_{S}=30\) solutions. Furthermore, we used \(8\) GNN layers, and 6 random node features per node. In the RB-200 MVC experiment 10 GNN layers were used. For MFA-Anneal we used a learning rate of \(1\times 10^{-4}\), \(2000\) annealing steps \(N_{\mathrm{anneal}}\) and a start temperature \(T_{0}\) of \(0.1\) except for RRG-100 MIS and for RB-200 MVC. In RRG-100 MIS a \(T_{0}\) of \(0.015\) is used and in RB-200 MVC \(T_{0}=0.05\). For MFA with and without annealing we trained for \(2000\) epochs and used a learning rate of \(5\times 10^{-5}\) except for ENZYMS MIS and RB-200 MVC, where a learning rate of \(1\times 10^{-4}\) is used.

**EGN Hyperparameters.**

All runs with EGN on RB-200 MVC have a batch size of \(H=32\). Here, in EGN-Anneal we use \(12\) GNN layers and in EGN without annealing \(10\) GNN layers performes best. In EGN-Anneal the initial temperature of \(T_{0}=0.1\) with the learning rate of \(5\times 10^{-5}\) obtained the best results. With EGN without annealing the same learning rate is used. Both methods are trained for \(2000\) epochs.

### Annealing Schedule

As described in Sec. 3, we change the temperature in the reward Eq. 6 according to a predefined annealing schedule. During the warm-up phase of \(N_{\mathrm{warmup}}\) epochs the temperature is held constant at the initial temperature \(T_{0}\). Afterwards the following temperature schedule is applied:

\[T(N_{\mathrm{epoch}})=T_{\mathrm{anneal}}(N_{\mathrm{epoch}})\cdot\left[\cos \left(2\pi(\lambda+\frac{1}{2})\frac{N_{\mathrm{epoch}}-N_{\mathrm{warmup}} }{N_{\mathrm{anneal}}}\right)+1\right].\] (16)

Here, \(T_{\mathrm{anneal}}(N_{\mathrm{epoch}})\) is the gradually decreasing amplitude of the temperature oscillations:

\[T_{\mathrm{anneal}}(N_{\mathrm{epoch}})=\frac{T_{0}}{1+c\cdot\frac{N_{\mathrm{ epoch}}-N_{\mathrm{warmup}}}{N_{\mathrm{anneal}}}}.\] (17)

\begin{table}
\begin{tabular}{c|c|c|c|c|c|c|c|c|c|c} Dataset & CO Problem Instance & learning rate & \(N_{\mathrm{anneal}}\) & \(T_{0}\) & \(L\) & \(N_{\mathrm{minib}}\) & \(H_{\mathrm{minib}}\) & \(S_{\mathrm{minib}}\) & \(\mathcal{T}\) & \(k\) \\ \hline TWITTER & MVC & \(1\times 10^{-3}\) & 4000 & 0.05 & 3 & 10 & 10 & 10 & 30 & 5 \\ \hline COLLAB & MVC & \(1\times 10^{-3}\) & 12000 & 0.05 & 3 & 10 & 10 & 6 & 35 & 5 \\  & MIS & \(1\times 10^{-3}\) & 4000 & 0.05 & 3 & 10 & 10 & 10 & 30 & 5 \\ \hline IMDB-BINARY & MVC & \(1\times 10^{-3}\) & 2000 & 0.05 & 3 & 10 & 10 & 10 & 5 & 5 \\  & MIS & \(5\times 10^{-4}\) & 2000 & 0.05 & 3 & 10 & 10 & 10 & 5 & 5 \\  & MaxCl & \(5\times 10^{-4}\) & 0.03 & 6000 & 3 & 6 & 10 & 7 & 20 & 6 \\ \hline ENZYMES & MIS & \(5\times 10^{-4}\) & 6000 & 0.07 & 4 & 15 & 15 & 15 & 25 & 5 \\  & MaxCl & \(5\times 10^{-4}\) & 6000 & 0.03 & 3 & 6 & 10 & 7 & 20 & 6 \\ \hline PROTEINS & MIS & \(5\times 10^{-4}\) & 10000 & 0.1 & 3 & 15 & 15 & 10 & 35 & 5 \\ \hline MUTAG & MIS & \(1\times 10^{-3}\) & 2000 & 0.05 & 3 & 10 & 10 & 10 & 7 & 5 \\ \hline RB-100 & MVC & \(1\times 10^{-3}\) & 2000 & \(5\cdot 10^{-2},5\cdot 10^{-3},0.0\) & 3 & 10 & 10 & 5 & 20 & 5 \\ \hline RB-200 & MVC & \(5\times 10^{-4}\) & 20000 & 0.05 & 3 & 15 & 10 & 10 & 30 & 5 \\ \hline RRG-100 & MIS & \(5\times 10^{-4}\) & 20000 & 0.1 & 4 & 10 & 10 & 10 & 20 & 5 \\ \hline BA 200-300 & MaxCut & \(1\times 10^{-3}\) & 7000 & 0.08 & 4 & 10 & 10 & 10 & 30 & 8 \\ \end{tabular}
\end{table}
Table 7: Hyperparameters that are used in VAG-CO on different datasets.

Here \(c\) is a scaling factor that determines the slope of \(T_{\rm anneal}(N_{\rm epoch})\). The parameter \(\lambda\) determines the number temperature oscillations in the schedule and \(N_{\rm anneal}\) is the total number of epochs that follow after the warmup phase. We use \(\lambda=3\), \(N_{\rm warmup}=400\) and \(c=6\). The course of the annealing schedule is illustrated in Fig. 4. One reason for the usage of cosine modulation function is rather practical, namely that \(T=0\) is reached multiple times during training, which allows an assessment of the training success at an earlier stage for a given set of hyperparameters. In the absence of cosine modulation, we found it harder to assess the final performance before the entire annealing was finished. Similar periodic schedules for learning rates have been proposed as variants of Stochastic Gradient Descent (Loshchilov and Hutter, 2017).

### Derivations

#### a.15.1 Free-Energy Decomposition into Rewards

In the following we show that using the reward defined in Eq. 6 is consistent with the goal of minimizing the free-energy defined in Eq. 2.

The right-hand side of Eq. 2 contains the expectation of the energy \(E(\bm{\sigma})\) and a term that is proportional to the entropy of \(p_{\theta}(\bm{\sigma})\). For the energy we obtain the following decomposition into individual steps \(i\) of the solution generation process (Sec. 3):

\[E(\bm{\sigma})=\sum_{i=1}^{N}\left[\sum_{j<i}J_{ij}\sigma_{j}\sigma_{i}+B_{i} \sigma_{i}\right]=\sum_{i=1}^{N}\sigma_{i}\left[\sum_{j<i}J_{ij}\sigma_{j}+B_{ i}\right]=\sum_{i=1}^{N}\Delta E_{i}\] (18)

By using an autoregressive factorization the entropy can also be decomposed in the following way:

\[\begin{split} S\big{(}p_{\theta}(\bm{\sigma}|E)\big{)}=-\sum_{ \bm{\sigma}}p_{\theta}(\bm{\sigma}|E)\log p_{\theta}(\bm{\sigma}|E)& =-\sum_{\bm{\sigma}}p_{\theta}(\bm{\sigma}|E)\left[\sum_{i=1}^{N} \log p_{\theta}(\sigma_{i}|\sigma_{<i},E)\right]\\ &=-\operatorname*{\mathbb{E}}_{\bm{\sigma}\sim p_{\theta}}\left[ \sum_{i=1}^{N}\log p_{\theta}(\sigma_{i}|\sigma_{<i},E)\right]\end{split}\] (19)

Therefore, we can use this decomposition by using the reward \(R_{i}(G_{i},\beta)=-\left[\Delta E_{i}+\frac{1}{\beta}\,\log p(\sigma_{i}| \sigma_{<i},E)\right]\). By the relation \(F(\theta;\beta,E)=-\operatorname*{\mathbb{E}}_{\bm{\sigma}\sim p_{\theta}}[ \sum_{i}R_{i}(G_{i},\beta)]\), maximizing this reward will then be equivalent to minimizing the free-energy.

#### a.15.2 Details on Remark 1

We first restate Corollary 5 of (Dudrik et al., 2007) in our notation. In the context of this Corollary our energy function \(E\) can be regarded as a single feature and, therefore, we use their result for \(n=1\). In addition, we consider the case in which the samples \(\mathcal{S}\) are drawn from the target distribution, i.e. \(\pi=p(\beta^{\text{*}})\). Since the terms Boltzmann distribution and Gibbs distribution can be used interchangeably we obtain:

Figure 4: Illustration of the cosine modulated annealing schedule. The plot depicts the annealing schedule with \(N_{\rm warmup}=200\) and \(N_{\rm anneal}=1000\) steps. The vertical red line marks the end of the warmup phase.

**Corollary 1** (Corollary 5 of [4]).: _Assume that \(E\) is bounded in \([0,1]\). Let \(\hat{\beta}\) minimize \(\mathcal{L}_{\hat{\rho}}(\beta)+\lambda|\beta|\) with \(\lambda=\sqrt{\ln(2/\delta)/(2m)}\). Then with probability at least \(1-\delta\) for every Boltzmann distribution \(p(\beta)\),_

\[\mathcal{L}_{p(\beta\ast)}(\hat{\beta})\leq\mathcal{L}_{p(\beta\ast)}(\beta)+ \frac{|\beta|}{\sqrt{m}}\sqrt{2\ln(2/\delta)}.\]

Now we consider the result for the case \(\beta=\beta^{\ast}\) and subtract \(\mathcal{L}_{p(\beta\ast)}(\beta\ast)\). Remark 1 follows by applying the definition of the Kullback-Leibler divergence \(D_{KL}\) to the left-hand side.

We note that an equivalent statement, however, with a consideration the Rademacher complexity of \(E\) can be derived from Theorem 2 in Cortes et al. (2015).