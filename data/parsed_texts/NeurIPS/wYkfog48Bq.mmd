# Optimal Block-wise Asymmetric Graph Construction

for Graph-based Semi-supervised Learning

Zixing Song

The Chinese University of Hong Kong

New Territories, Hong Kong SAR

zxsong@cse.cuhk.edu.hk

&Yifei Zhang

The Chinese University of Hong Kong

New Territories, Hong Kong SAR

yfzhang@cse.cuhk.edu.hk

&Irwin King

The Chinese University of Hong Kong

New Territories, Hong Kong SAR

king@cse.cuhk.edu.hk

###### Abstract

Graph-based semi-supervised learning (GSSL) serves as a powerful tool to model the underlying manifold structures of samples in high-dimensional spaces. It involves two phases: constructing an affinity graph from available data and inferring labels for unlabeled nodes on this graph. While numerous algorithms have been developed for label inference, the crucial graph construction phase has received comparatively less attention, despite its significant influence on the subsequent phase. In this paper, we present an optimal asymmetric graph structure for the label inference phase with theoretical motivations. Unlike existing graph construction methods, we differentiate the distinct roles that labeled nodes and unlabeled nodes could play. Accordingly, we design an efficient block-wise graph learning algorithm with a global convergence guarantee. Other benefits induced by our method, such as enhanced robustness to noisy node features, are explored as well. Finally, we perform extensive experiments on synthetic and real-world datasets to demonstrate its superiority to the state-of-the-art graph construction methods in GSSL.

## 1 Introduction

Graph-based semi-supervised learning (GSSL) is a burgeoning research field [52, 9, 14, 63, 65, 62]. As a subclass of semi-supervised learning (SSL), GSSL exhibits promise since it encapsulates the smoothness or manifold assumption, where samples with similar features are likely to share the same label. GSSL methods begin by constructing an affinity graph, wherein nodes represent samples, and weighted edges denote similarity between pairs of nodes. This process aligns with the manifold assumption, implying that nodes connected by edges with large weights tend to have the same label. Upon obtaining the affinity graph, various label inference algorithms such as label propagation [85, 82, 25, 65] can be executed to predict labels for the unlabeled nodes.

Preliminary empirical studies [16] suggest that the quality of the affinity graph significantly influences label prediction performance. However, the graph construction phase in GSSL has received less scrutiny compared to the subsequent label inference phase. Constructing a high-quality graph presents a challenge as its quality can only be assessed indirectly through postmortem verification via label inference performance. Classic solutions such as the Radial Basis Function (RBF) Kernel [85], kNN graph [17], and \(b\)-matching [26], while simple, may exhibit low robustness against noise due to their simplicity. More recent and complex methods, typically framed as optimization problems to findthe optimal graph under the smoothness assumption, suffer from inefficient optimization techniques without fast convergence guarantees [18; 29; 63] or lack a solid theoretical foundation [58; 14; 23].

Consequently, a series of research questions arise: _1) What is the optimal graph structure for label inference algorithms in GSSL? 2) How can we optimize this optimal graph efficiently? 3) What kinds of benefits can this optimal graph bring?_

Most existing GSSL graph construction methods treat all nodes equally, disregarding label information, which results in a symmetric, undirected graph. However, we contend that an asymmetric, directed graph structure might be more beneficial for subsequent label inference due to the distinct roles labeled and unlabeled nodes can play. Intuitively, edges from labeled to unlabeled nodes would naturally facilitate supervision information propagation, while edges from unlabeled to labeled nodes may introduce inconsistency, potentially undermining the prediction accuracy for the labeled nodes.

Motivated by this key observation, we revisit a unified optimization framework that encompasses most label inference algorithms, assuming the affinity graph is already constructed. We then fix the downstream label inference result and position the graph weight matrix (or adjacency matrix) as the optimization variable. This shift allows us to define optimality in the graph construction step of GSSL concisely, addressing our first research question. We subsequently present a tailored optimal solution featuring an asymmetric graph structure that aligns with our proposed intuition. In response, we introduce the **B**lock-wise **A**ffinity **G**raph **L**earning (BAGL) algorithm by leveraging duality and the fast proximal gradient method, addressing our second research question. Finally, we demonstrate that BAGL ensures a sub-linear global convergence rate of \(O(1/k)\) and can alleviate issues of noisy node features, addressing our third research question.

In summary, this work offers four main contributions. First, we provide a succinct definition of the optimality of the affinity graph in GSSL, and through rigorous derivation, propose an ad-hoc solution with an asymmetric structure. Second, we design a block-wise graph learning framework, BAGL, to infer the weights in the optimal graph structure. Third, we prove that a global sub-linear convergence rate is guaranteed for BAGL and analyze other benefits. Fourth, we perform extensive experiments on synthetic and real-world datasets to demonstrate the effectiveness and efficiency of BAGL.

## 2 Preliminary

### Problem Formulation

We present a formulation for the GSSL problem, comprising two steps: graph construction and label inference [52]. Our study primarily investigates the optimal construction of the affinity graph in the first phase to facilitate enhanced performance in the second label inference phase.

Given a set of data points \(\{\bm{x}_{i}\}_{i=1}^{n}\), where each \(\bm{x}_{i}\in\mathbb{R}^{d}\) is sampled from a \(d\) dimensional feature space. In this paper, we interchangeably use the terms _node_, _point_, and _sample_ to refer to \(\bm{x}_{i}\). Each sample \(\bm{x}_{i}\) has a label \(y_{i}\in\mathbb{N}_{c}\), where \(\mathbb{N}_{c}=\{i\in\mathbb{N}^{+}\mid 1\leq i\leq c\}\) with \(c\) being the number of classes. Given the labels of the \(l\) samples \(\{\bm{x}_{i}\}_{i=1}^{l}\) as \(\{y_{i}\}_{i=1}^{l}\), the ultimate goal of general transductive SSL is to infer the labels \(\{y_{i}\}_{i=l+1}^{l+u}\) for the remaining \(u\) unlabeled samples \(\{\bm{x}_{i}\}_{i=l+1}^{l+u}\) (\(n=l+u\)). As a subcategory of general SSL, GSSL methods first construct a graph \(\mathcal{G}=\{\mathcal{V},\mathcal{E},\bm{W}\}\) based on all the training samples \(\{\bm{x}_{i}\}_{i=1}^{n}\) and partially given labels \(\{y_{i}\}_{i=1}^{l}\). Here, \(\mathcal{V}\) is the node set with \(|\mathcal{V}|=n\). Each node represents each sample \(\bm{x}_{i}\). \(\mathcal{E}\) is the edge set in which each edge \((i,j)\) is assigned with a weight \(W_{ij}\) (the \(i\)-th row, \(j\)-th column entry in \(\bm{W}\in\mathbb{R}^{n\times n}\)) to reflect the affinity or similarity between the sample pair \((\bm{x}_{i},\bm{x}_{j})\). Generally, a larger weight indicates a higher level of similarity. \(W_{ij}=0\) indicates no edge between node \(i\) and node \(j\). Therefore, the key challenge in the first graph construction step is to generate \(\bm{W}\) based on \(\{\bm{x}_{i}\}_{i=1}^{n}\) and \(\{y_{i}\}_{i=1}^{l}\) so that the underlying manifold of the data is properly encoded. In the second step, various label inference algorithms can be performed on \(\mathcal{G}\) to propagate the given labels \(\{y_{i}\}_{i=1}^{l}\) and make predictions \(\{y_{i}\}_{i=l+1}^{l+u}\) for unlabeled nodes.

However, the performance of the label inference step is significantly contingent on the quality of the weight matrix \(\bm{W}\) from the first graph construction step. This paper aims to address three critical research questions pertaining to the graph construction step in GSSL. First, what constitutes the overall structure of the optimal weight matrix, \(\bm{W}^{*}\)? (Sect. 3.1). We define the optimal weight matrix, \(\bm{W}^{*}\), as the one that gives the best prediction results when used with the same label inference method across different graphs \(\bm{W}\). Second, how to find an efficient method for optimizing the entries in \(\bm{W}^{*}\)? (Sect. 3.2). Third, what kinds of benefits can \(\bm{W}^{*}\) bring from the theoretical perspective? (Sect. 3.3)

### Recap on the Unified Framework for Label Inference Step in GSSL

Before we delve into the proposed structure for the optimal affinity graph, we first revisit the unified framework of the label inference step, allowing for a seamless definition of the optimal affinity graph.

If we assume the affinity graph has been constructed, then numerous influential label inference methods [85; 82; 83; 4; 5; 9] can be performed over this graph to infer the labels for the unlabeled nodes. However, the majority of them can be incorporated into the following framework.

We first define the node feature matrix \(\bm{X}\in\mathbb{R}^{n\times d}\) as \(\bm{X}=[\bm{x}_{1},\cdots,\bm{x}_{n}]^{\intercal}\) and the predicted soft label matrix \(\bm{F}\in\mathbb{R}^{n\times c}\) as \(\bm{F}=[\bm{f}_{1},\cdots,\bm{f}_{n}]^{\intercal}\) with \(\bm{f}_{i}\in\mathbb{R}^{c}(1\leq i\leq n)\). Ground-truth label matrix is given as \(\bm{Y}=[\bm{y}_{1},\cdots,\bm{y}_{n}]^{\intercal}\in\{0,1\}^{n\times c}\) with \(\bm{y}_{i}\in\{0,1\}^{c}\) (\(1\leq i\leq n\)). Here, for the labeled nodes \(\{\bm{x}_{i}\}_{i=1}^{l}\), \(\bm{y}_{i}\) is a one-hot vector in which \(Y_{ij}=1\) if \(\bm{x}_{i}\) belongs to class \(j\) (\(y_{i}=j\)), and \(Y_{ij}=0\), otherwise. For the unlabeled nodes \(\{\bm{x}_{i}\}_{i=l+1}^{l+u}\), \(\bm{y}_{i}\) is an all-zero vector for initialization.

Driven by the geometry of the affinity graph, we can unify these label inference algorithms as a minimizer of the optimization problem as Problem (1).

\[\bm{F}^{*}=\operatorname*{arg\,min}_{\bm{F}}Q(\bm{F})=\operatorname*{arg\, min}_{\bm{F}}\{\operatorname{Tr}\left(\bm{F}^{\intercal}\bm{S}\bm{F} \right)+\operatorname{Tr}\left((\bm{F}-\bm{Y})^{\intercal}\bm{\Lambda}(\bm{F }-\bm{Y})\right).\] (1)

Note that the loss function \(Q(\bm{F})\) consists of a quadratic variation term \(\operatorname{Tr}\left(\bm{F}^{\intercal}\bm{S}\bm{F}\right)\) as the graph smoothness regularizer, and a quadratic Frobenius error norm \(\|\bm{F}-\bm{Y}\|_{F}^{2}=\operatorname{Tr}((\bm{F}-\bm{Y})^{\intercal}(\bm{ F}-\bm{Y}))\), both of which should ideally be small subject to a trade-off parameter \(\bm{\Lambda}\) between them. Here the smoothing matrix \(\bm{S}=s(\bm{W})\in\mathbb{S}_{+}^{n}\) in the first term, with \(s\colon\mathbb{R}^{n\times n}\to\mathbb{S}_{+}^{n\times n}\), is positive semidefinite and determined by the weight matrix \(\bm{W}\) of the graph to ensure the adjacent nodes share similar predictions. The second term measures the distance between predicted results and initial assignments, and restricts the output for labeled nodes from deviating too much compared with the ground-truth. \(\bm{\Lambda}\) is a diagonal matrix with \(\Lambda_{ii}\geq 0\) and \(\bm{S}+\bm{\Lambda}\) must be invertible to avoid trivial solutions.

By the first-order optimality condition, we can easily obtain the optimal solution for Problem (1).

\[\bm{F}^{*}=(\bm{S}+\bm{\Lambda})^{-1}\bm{\Lambda}\bm{Y}=(s(\bm{W})+\bm{ \Lambda})^{-1}\bm{\Lambda}\bm{Y}.\] (2)

Based on the optimal soft label matrix \(\bm{F}^{*}\), the final predicted label for each node is given as \(\hat{y}_{i}=\hat{y}(\bm{f}_{i})=\operatorname*{arg\,max}_{1\leq j\leq c}F_{ij}^ {*}\). It is worth noting that the unified optimization framework in Problem (1) can admit most of the mainstream GSSL methods. For instance, if we set the smoothing matrix as the normalized graph Laplacian matrix \(\bm{S}=\bm{\mathcal{L}}=s(\bm{W})=\bm{D}^{-\frac{1}{2}}(\bm{D}-\bm{W})\bm{D}^{ -\frac{1}{2}}\) and \(\bm{\Lambda}=\lambda\bm{I}\), we can easily recover one of the most popular label inference methods for GSSL, Local and Global Consistency (LGC) [82], as \(\bm{F}^{*}=\operatorname*{arg\,min}_{\bm{F}}\{\frac{1}{2}\sum_{i,j=1}^{n} \left\|\frac{\bm{f}_{i}}{\sqrt{D_{ii}}}-\frac{\bm{f}_{j}}{\sqrt{D_{jj}}} \right\|_{2}^{2}W_{ij}+\lambda\sum_{i=1}^{n}\left\|\bm{f}_{i}-\bm{y}_{i}\right\| _{2}^{2}\}\). Here, \(\bm{\mathcal{L}}\in\mathbb{S}_{+}^{n}\) is defined as \(\bm{\mathcal{L}}=\bm{D}^{-\frac{1}{2}}\bm{L}\bm{D}^{-\frac{1}{2}}\), where the combinatorial Laplacian matrix \(\bm{L}\in\mathbb{S}_{+}^{n}\) is given as \(\bm{L}=\bm{D}-\bm{W}\), and the degree matrix \(\bm{D}\in\mathbb{R}^{n\times n}\) is a diagonal matrix defined as \(D_{ii}=\sum_{j=1}^{n}W_{ij}\). We summarize several representative works under this unified framework in Table 5 with explicit forms of \(\bm{S}\) (or \(s(\bm{W})\)) and \(\bm{\Lambda}\) in Appendix B.2.

## 3 Methodology

### Motivation: Optimal Affinity Graph Structure

#### 3.1.1 Definition of the Optimality of the Affinity Graph

If we perform the same label inference algorithm from the above-mentioned generalized framework on all possible affinity graphs \(\bm{W}\), the optimal graph \(\bm{W}^{*}\) will enable the label inference step to obtain the most accurate predictions. Under the above-mentioned unified framework for label inference (Problem (1)), the weight matrix \(\bm{W}\) is fixed while the soft label matrix \(\bm{F}\) is the optimization variable. This is due to our goal of executing label inference to attain the optimal \(\bm{F}^{*}\) given the affinity graph \(\bm{W}\). Similarly, when we want to construct the optimal graph \(\bm{W}^{*}\) given the label inference framework,we consider the weight matrix \(\bm{W}\) as the optimization variable, keeping the soft label matrix \(\bm{F}\) fixed as the solution in Eq. (2). The form of \(Q(\cdot)\) remains unchanged as it is related to the generalization bound for GSSL that would be discussed in Appendix D.2. This approach aids in identifying the "optimal" affinity graph for the label inference algorithms under the unified framework.

**Definition 1** (Optimality of the Affinity Graph).: _The affinity graph \(\bm{G}\) is optimal for label inference under the unified GSSL framework if its weight matrix \(\bm{W}\) is the minimizer of Problem (3)._

\[\min_{\bm{W}\in\mathbb{R}^{n\times n}}\left\{\operatorname{Tr}(\bm{F}^{\intercal }s(\bm{W})\bm{F})+\operatorname{Tr}\left((\bm{F}-\bm{Y})^{\intercal}\bm{ \Lambda}(\bm{F}-\bm{Y})\right)\right\}\quad\text{s.t.}\quad\bm{F}=\left(s(\bm{ W})+\bm{\Lambda}\right)^{-1}\bm{\Lambda}\bm{Y},\] (3)

Although the optimization Problem (3) is intractable in general due to the various forms of \(s(\bm{W})\) in the constraint, we can present an ad-hoc optimal structure of \(\bm{W}^{*}\) that is independent of \(s(\bm{W})\), which motivates our proposed method in Sect. 3.2 from a theoretical perspective.

#### 3.1.2 Structure of the Optimal Affinity Graph

We then present an equivalent proposition in Theorem 1, which provides a necessary and sufficient condition for the optimality of \(\bm{W}^{*}\) in Problem (3). Motivated by some classic graph sharpening techniques [47, 15, 46], Theorem 1 helps to circumvent the challenges of directly dealing with Problem (3) and holds regardless of the forms of \(s(\bm{W})\) (Appendix E.1).

**Theorem 1**.: \(\bm{W}^{*}\) _obtained by Definition 1 is optimal if and only if \(\bm{Y}_{l}=\bm{F}_{l}\) holds. \(\bm{Y}_{l}\in\{0,1\}^{l\times c}\), \(\bm{F}_{l}\in\mathbb{R}^{l\times c}\) are the ground-truth label matrix, and the soft label matrix for labeled nodes._

To put it in a simpler way, Theorem 1 tells us that if the affinity graph is optimal, then after we perform the label inference algorithm for GSSL, the predicted soft label for the labeled nodes would coincide with the ground truth precisely. The converse of this observation also holds true. Unfortunately, it remains an open question to solve \(\bm{Y}_{l}=\bm{F}_{l}\) by listing all possible classes of solutions due to the complex interconnection of \(\bm{F}\) and \(\bm{W}\). However, we can provide a simple ad-hoc solution for \(\bm{Y}_{l}=\bm{F}_{l}\) in Proposition 1. For one thing, this asymmetric graph structure, in the theoretical sense, conforms to the intuition of the better affinity graph we discussed earlier. For another, it also sheds some light on the proposed optimization framework to infer the weights in this graph structure later.

**Proposition 1**.: _If \(\bm{W}^{*}\) can be expressed as (4), \(\bm{W}^{*}\) is an optimal solution given by (3) in Definition 1._

\[\bm{W}^{*}=\left(\begin{array}{cc}\bm{O}&\bm{O}\\ \bm{W}_{ul}&\bm{W}_{uu}\end{array}\right),\] (4)

_where \(\bm{W}_{ul}\in\mathbb{R}^{u\times l}\), and \(\bm{W}_{uu}\in\mathbb{R}^{u\times u}\) can be non-zero submatrices with arbitrary entries._

Proposition 1 provides an ad-hoc solution to Problem (3) (Appendix E.2). If the constructed graph is asymmetrical with edges from unlabeled nodes to labeled nodes only, then it is optimal by Definition 1. This asymmetrical optimal graph structure answers the first research question in Sect. 2.1.

The optimal asymmetric graph structure presented in Proposition 1 offers meaningful interpretations and notable advantages. It effectively eliminates the influence from unlabeled nodes to labeled nodes by enforcing \(\bm{W}_{ll}\) and \(\bm{W}_{lu}\) to be zero matrices. This aligns with the intuition that label information should be propagated from labeled nodes to unlabeled nodes in the GSSL methods, rather than the other way around. More technically, when applying the classic label inference algorithm LGC [82] on this optimal graph structure, the soft label matrix for unlabeled nodes now becomes \(\bm{F}_{u}=(\bm{I}-\mu\bm{W}_{uu})^{-1}\bm{W}_{ul}\bm{Y}_{L}\) with some constant \(0\leq\mu\leq 1\). This formulation indicates that the LGC algorithm spreads supervision information from labeled nodes to unlabeled nodes once through \(\bm{W}_{ul}\bm{Y}_{l}\), followed by propagating this information solely among unlabeled nodes through \((\bm{I}-\mu\bm{W}_{uu})^{-1}=\bm{I}+\mu\bm{W}_{uu}+\mu^{2}\bm{W}_{uu}^{2}+\cdots\). By Theorem 1, this optimal asymmetric graph guarantees zero empirical risk on the labeled nodes. Moreover, many existing graph construction methods in GSSL primarily focus on node features while disregarding label information. Consequently, these methods may produce heterophilous edges that connect nodes with similar features but different labels. Such edges violate the manifold assumption in GSSL, where nodes with the same label tend to be linked. During the label inference step, the label information of these nodes connected by heterophilous edges confuses each other during propagation, resulting in misleading predictions. By setting \(\bm{W}_{ll}=\bm{O}\), our method eliminates these heterophilous edges completely. As a result, it increases the edge homophily ratio of the constructed graph and enhances the robustness of subsequent label inference algorithms, as validated in Appendix D.1.

### Implementation: Block-wise Graph Learning Algorithm

#### 3.2.1 Framework

By differentiating the roles that labeled and unlabeled nodes would play, we arrive at an optimal structure for the affinity graph in Proposition 1. However, an efficient algorithm to infer the exact weights or entries for blocks in Eq. (4) is still in demand. Motivated by previous works [29; 30; 19], we make the following reasonable restrictions or assumptions on \(\bm{W}\) to obtain a more meaningful graph for GSSL. First, the features for two directly connected nodes should not vary too much, no matter whether they are labeled or not. This agrees with the manifold assumption in GSSL, where links tend to form between similar nodes. Violation of this fundamental assumption usually causes a significant performance drop in the label inference step. Second, the constructed graph should be well-connected in terms of both \(\bm{W}_{\mathit{ul}}\) and \(\bm{W}_{\mathit{uu}}\) to ensure the supervision information could propagate freely among all nodes. Otherwise, some disconnected nodes may never receive any label information. Third, it is desirable to control the sparsity of the graph, avoiding an overly sparse or overly dense graph. We will empirically demonstrate the effects of the sparsity control later. We propose two similar optimization frameworks for \(\bm{W}_{\mathit{ul}}\) and \(\bm{W}_{\mathit{uu}}\) in Eq. (4) as Problem (5) and (6).

\[\min_{\bm{W}_{\mathit{ul}}}\|\bm{W}_{\mathit{ul}}\odot\bm{Z}_{ \mathit{ul}}\|_{1}-\alpha_{1}\bm{1}^{\intercal}\log(\bm{W}_{\mathit{ul}}\bm{1 })-\alpha_{1}\bm{1}^{\intercal}\log(\bm{W}_{\mathit{ul}}\bm{1})+\beta_{1}\| \bm{W}_{\mathit{ul}}\|_{F}^{2},\text{s.t.}\,\bm{W}_{\mathit{ul}}\geq 0.\] (5) \[\min_{\bm{W}_{\mathit{uu}}}\|\bm{W}_{\mathit{uu}}\odot\bm{Z}_{ \mathit{uu}}\|_{1}-\alpha_{2}\bm{1}^{\intercal}\log(\bm{W}_{\mathit{uu}}\bm{1 })-\alpha_{2}\bm{1}^{\intercal}\log(\bm{W}_{\mathit{uu}}^{\intercal}\bm{1})+ \beta_{2}\|\bm{W}_{\mathit{uu}}\|_{F}^{2},\text{s.t.}\bm{W}_{\mathit{uu}}\geq 0.\] (6)

Here, we define the pairwise distance matrix \(\bm{Z}=\left(\begin{array}{cc}\bm{Z}_{\mathit{ll}}&\bm{Z}_{\mathit{ul}}^{ \intercal}\\ \bm{Z}_{\mathit{ul}}&\bm{Z}_{\mathit{uu}}\end{array}\right)\in\mathbb{R}_{+}^{n \times n}\) with \(Z_{ij}=\|\bm{x}_{i}-\bm{x}_{j}\|_{2}^{2}\).

\(\odot\) is the Hadamard product and \(\log(\cdot)\) is the element-wise logarithm operator. Since Problem (5) and (6) share the same form, we will only focus on Problem (5) as an example. The first term \(\|\bm{W}_{\mathit{ul}}\odot\bm{Z}_{\mathit{ul}}\|_{1}=\sum_{1\leq i\leq u,1 \leq j\leq l}W_{ij}\|\bm{x}_{i}-\bm{x}_{j}\|_{2}^{2}\) encourages similar nodes to be connected with larger weights, meeting the manifold assumption. The second and third logarithmic barrier term act on the out-degree and in-degree vectors to make sure that each unlabeled node is connected by at least one labeled node and vice versa, improving the overall connectivity of the graph. The last Frobenius norm term measures the sparsity of the graph. The parameters \(\alpha_{1},\alpha_{2},\beta_{1},\beta_{2}\) are positive.

#### 3.2.2 Optimization

For convenience of presentation, we view the entries in \(\bm{W}_{\mathit{ul}}\) (\(\bm{W}_{\mathit{uu}}\)) as a new vector \(\bm{w}=\operatorname{vec}[\bm{W}_{\mathit{ul}}]\in\mathbb{R}_{+}^{\mathit{ul}}\). Similarly, we have \(\bm{z}=\operatorname{vec}[\bm{Z}_{\mathit{ul}}]\in\mathbb{R}_{+}^{\mathit{ul}}\). Accordingly, a linear mapping matrix \(\bm{T}_{1}\in\{0,1\}^{\mathit{u}\times\mathit{ul}}\) transforms the edge weights to the corresponding out-degree vector (i.e. \(\bm{T}_{1}\bm{w}=\bm{W}_{\mathit{ul}}\bm{1}\)). Similarly, we have \(\bm{T}_{2}\bm{w}=\bm{W}_{\mathit{ul}}^{\intercal}\bm{1}\) for the in-degree vector. Further, if we let \(\bm{T}^{\intercal}=(\bm{T}_{1}^{\intercal},\bm{T}_{2}^{\intercal})\in\{0,1\}^ {n\times\mathit{ul}}\), \(\alpha=\alpha_{1}=\alpha_{2}\), and \(\beta=\beta_{1}\) we can easily transform Problem (5) into Problem (7) (primal) in a more compact way with an extra linear constraint.

\[\min_{\bm{w},\bm{v}}\quad f(\bm{w})+g(\bm{v})\quad\text{s.t.}\quad\bm{v}=\bm{T} \bm{w},\] (7)

with \(f(\bm{w})=\bm{w}^{\intercal}\bm{z}+\beta\|\bm{w}\|_{2}^{2}+\mathbb{I}_{(\bm{w }\geq 0)},\quad g(\bm{v})=-\alpha\bm{1}^{\intercal}\log(\bm{v})\).

The state-of-the-art method [61] applies the linearized alternating direction method of multipliers (ADMM) algorithm [8] directly to the primal problem with a similar structure in Problem (7), which lacks the theoretical guarantee on its convergence rate since the objective function in Problem (7) has no Lipschitz gradient. Motivated by the recent work [45], we circumvent this critical issue by applying the FISTA algorithm [2], a proximal gradient method, to the dual problem of (7) instead. Motivated by recent advances [3; 61], we can now provide a better convergence rate with rigorous theoretical analysis so that our proposed graph construction method is much more efficient with guarantees.

Dual Problem FormationWe construct the Lagrangian function by introducing the Lagrangian multipliers \(\bm{\lambda}\in\mathbb{R}^{n}\) as \(\mathcal{L}(\bm{w},\bm{v},\bm{\lambda})=f(\bm{w})+g(\bm{v})-\langle\bm{ \lambda},\bm{T}\bm{w}-\bm{v}\rangle\). We establish the corresponding dual problem as Problem (8) by introducing the conjugate functions \(f^{*},g^{*}\) for simpler notation (Appendix E.3).

\[\min_{\bm{\lambda}}\quad F(\bm{\lambda})+G(\bm{\lambda}),\] (8)with \(F(\bm{\lambda})=f^{*}(\bm{T}^{\intercal}\bm{\lambda}),\quad G(\bm{\lambda})=g^{*}( -\bm{\lambda})\). By Slater's condition, we know that strong duality holds as long as \(\bm{v}\) resides in the range of \(\bm{T}\). Therefore, the optimal values for Problem (7) and (8) are identical, and the optimal solution for Problem (8) can be attained. Consequently, we can apply the FISTA algorithm to the dual problem to generate the dual sequence that converges to the optimal solution, and construct the corresponding optimal solution back for the primal problem.

Dual Problem with FISTAIt is not hard to show that \(F(\bm{\lambda})\) is differentiable and \(\nabla F(\bm{\lambda})=\bm{T}\bm{x}^{*}\), where \(\bm{x}^{*}=\arg\max_{x}\{(\bm{T}^{\intercal}\bm{\lambda})^{\intercal}\bm{x}- f(\bm{x})\}\) (Appendix E.4). Therefore, the dual problem minimizes the sum of a differentiable convex function \(F\) and a closed proper convex function \(G\). This structure of the dual Problem (8) immediately paves the way for applying proximal gradient methods. Here, for the sake of a better convergence rate, we apply the FISTA algorithm with a fixed step size to the dual Problem (8), and the following iteration schemes are performed. We first choose any \(\bm{\lambda}^{0}=\bm{\lambda}^{-1}\) and fix the step size as \(t=\frac{2\beta}{l+u}\). Henceforth, \(k=1,2,\cdots\), we repeat the following two steps as

\[\bm{\mu}^{k} =\bm{\lambda}^{k-1}+\frac{k-2}{k+1}(\bm{\lambda}^{k-1}-\bm{ \lambda}^{k-2}),\] (9) \[\bm{\lambda}^{k} =\operatorname{prox}_{tG}\left(\bm{\mu}^{k}-t\nabla F(\bm{\mu}^{ k})\right),\] (10)

Hence, based on the above-mentioned properties of \(\nabla F(\bm{\mu}^{k})\), we have \(\nabla F(\bm{\mu}^{k})=\bm{T}\bar{\bm{w}}^{k}\) with \(\bar{\bm{w}}^{k}\) set as \(\bar{\bm{w}}^{k}=\arg\max_{\bm{w}}\left\{(\bm{T}^{\intercal}\bm{\mu}^{k})^{ \intercal}\bm{w}-f(\bm{w})\right\}=\left[\frac{\bm{T}^{\intercal}\bm{\mu}^{k} -\bm{z}}{2\beta}\right]_{+}\) (Appendix E.5).

Let \(\bm{p}^{k}=\bm{\mu}^{k}-t\bm{T}\bar{\bm{w}}^{k}\). By the extended Moreau decomposition [43], \(\operatorname{prox}_{\gamma h}(z)+\gamma\operatorname{prox}_{\gamma^{-1}h^{* }}(z/\gamma)=z,\forall z\). We have \(\operatorname{prox}_{tG}(\bm{p}^{k})=\bm{p}^{k}-t\operatorname{prox}_{t^{-1}G^ {*}}(t^{-1}\bm{p}^{k})=\bm{\mu}^{k}-t(\bm{T}\bar{\bm{w}}^{k}-\bar{\bm{u}}^{k})\). Here, \(\bar{\bm{u}}^{k}=\operatorname{prox}_{t^{-1}g}(\bm{T}\bar{\bm{w}}^{k}-t^{-1} \bm{\mu}^{k})\). Note that \(g(\bm{v})=-\alpha\bm{1}^{\intercal}\log(\bm{v})\) and by the definition of proximal mapping, it is easy to prove that \(\operatorname{prox}_{t^{-1}g}(\bm{v})=\frac{1}{2}(\bm{v}+\sqrt{\bm{v}\odot\bm{v }+4\alpha t^{-1}}\bm{1})\). Therefore, we can simplify the updating step (10) as the following three steps (Appendix E.6).

\[\bar{\bm{w}}^{k} =\left[\frac{\bm{T}^{\intercal}\bm{\mu}^{k}-\bm{z}}{2\beta} \right]_{+},\] (11) \[\bar{\bm{u}}^{k} =\frac{1}{2}(\bm{T}\bar{\bm{w}}^{k}-t^{-1}\bm{\mu}^{k})+\frac{1}{ 2}\sqrt{(\bm{T}\bar{\bm{w}}^{k}-t^{-1}\bm{\mu}^{k})\odot(\bm{T}\bar{\bm{w}}^{k }-t^{-1}\bm{\mu}^{k})+4\alpha t^{-1}\bm{1}},\] (12) \[\bm{\lambda}^{k} =\bm{\mu}^{k}-t(\bm{T}\bar{\bm{w}}^{k}-\bar{\bm{u}}^{k}),\] (13)

Finally, we arrive at Procedure GWBI, where the optimal graph weights are inferred for one block like \(\bm{W}_{uu}\) in the optimal graph structure suggested in Proposition 1. Here, instead of directly optimizing the primal variable of the block graph weight vector \(\bm{w}\), we consider its corresponding dual variable \(\bm{\lambda}\) for a better convergence rate. In each iteration step, we first find an extrapolated point \(\bm{\mu}\) based on the points \(\bm{\lambda}\) from two previous steps (line 3). We then perform the proximal gradient update on this extrapolated point (lines 4-6) to obtain \(\bm{\lambda}\) for the next iteration. Note that lines 4-6 are the detailed instantiation of Eq. (10). Finally, we can convert the dual variable \(\bm{\lambda}\) back to the desired primal variable \(\bm{w}\) based on line 4 after its convergence. With this core procedure in hand, we plug it into the proposed Algorithm 1, Block-wise Affinity Graph Learning (BAGL) algorithm, in Appendix C.

### Theoretical Analysis

We prove that our proposed optimization method for Problem (7) in Procedure GWBI enjoys the guarantee of the global convergence rate, where the generated primal sequence converges to the global optimal solution at a rate of \(O(\frac{1}{k})\) with a fixed step size. To begin with, it is well known that the FISTA algorithm enjoys the global convergence rate of \(O(\frac{1}{k^{2}})\)[2]. For simplicity, we focus on the results when optimizing Problem (5), which can be viewed as a general case of Problem (6) when \(l\neq u\). Therefore, \(Q(\bm{\lambda})\equiv F(\bm{\lambda})+G(\bm{\lambda})\) converges to the dual optimal value \(Q(\bm{\lambda}^{*})\) at a rate of \(O(\frac{1}{k^{2}})\) due to this well-known fact [2], which yields Theorem 2.

**Theorem 2**.: _Let \(\{\bm{\lambda}^{k}\}\) be the dual sequence generated by Procedure GWBI, then_

\[Q(\bm{\lambda}^{k})-Q(\bm{\lambda}^{*})\leq\frac{l+u}{\beta(k+1)^{2}}\|\bm{ \lambda}^{0}-\bm{\lambda}^{*}\|_{2}^{2}.\] (14)We then consider the corresponding primal sequence \(\{\bm{w}^{k}\}\) and its convergence rate. Since the strong duality holds and a dual optimal solution \(\bm{\lambda}^{*}\) exists, any primal optimal point \((\bm{w}^{*},\bm{v}^{*})\) is also a minimizer of \(\mathcal{L}(\bm{w},\bm{v},\bm{\lambda}^{*})\). This motivates us to construct the primal sequence \(\{\bm{w}^{k}\}\) based on the dual sequence \(\{\bm{\lambda}^{k}\}\) as \(\bm{w}^{k}=\operatorname*{arg\,min}_{\bm{w}}\mathcal{L}(\bm{w},\bm{v},\bm{ \lambda}^{k})=\operatorname*{arg\,max}_{\bm{w}}\{\langle\bm{T}^{\intercal}\bm{ \lambda}^{k},\bm{w}\rangle-f(\bm{w})\}\). Thanks to Theorem 3, this primal sequence \(\{\bm{w}^{k}\}\) is guaranteed to converge to the optimal primal solution \(\bm{w}^{*}\) of Problem (7) at the rate of \(O(\frac{1}{k})\).

**Theorem 3**.: _Let \(\bm{w}^{k}=\operatorname*{arg\,max}_{\bm{w}}\langle\langle\bm{T}^{\intercal} \bm{\lambda}^{k},\bm{w}\rangle-f(\bm{w})\rangle\) with the dual sequence \(\{\bm{\lambda}^{k}\}\) given by Procedure GWBI. \(\bm{w}^{*}\) and \(\bm{\lambda}^{*}\) are the optimal solution of Problem (7) and the optimal solution of Problem (8), respectively. We have,_

\[\|\bm{w}^{k}-\bm{w}^{*}\|_{2}\leq\frac{\sqrt{l+u}}{\beta(k+1)}\|\bm{\lambda}^{ 0}-\bm{\lambda}^{*}\|_{2}.\] (15)

Motivated by [45], Theorem 3 establishes that the proposed method exhibits a sub-linear convergence rate, which represents a state-of-the-art result for optimization-based graph construction methods, accompanied by a global convergence guarantee. This improved convergence rate is primarily attributed to the utilization of the FISTA algorithm applied to the dual problem. Time complexity analysis is included in Appendix G.5. More results regarding the robustness of our method are discussed in Appendix D.

## 4 Experiments

In this section, we conduct numerical experiments on both synthetic and real-world datasets to demonstrate the advantages of our proposed BAGL method in terms of efficacy and convergence. Robustness analysis (Appendix G.3) and more experimental results are included in Appendix G.

### Baseline Models

We choose the following graph construction methods in GSSL for comparison. Radial basis function kernel (RBF) [85] and kNN graph [17] are two classic methods. Smooth graph learning (SGL) [29] is a popular method in the graph signal processing domain. RGCLI [7] is another label-informed graph construction method. Anchor Graph Regularization (AGR) [40] deals with large-scale graph construction. GraphEBM [14] and BCAN [63] are two state-of-the-art methods. The former exploits the energy-based model while the latter constructs a bipartite graph.

All the hyper-parameters are fine-tuned with the grid search method. We repeat the experiment 20 times for each case and report the average result with optimal parameter setting in the efficacy analysis. Unless otherwise specified, the default label inference algorithm is LGC, and the label rate is ten labeled samples per class. More details on the experimental settings can be found in Appendix F.

### Synthetic Dataset

We generate a synthetic dataset as shown in Fig. 1 (a). The constructed dataset contains two clusters, a dense Gaussian cluster surrounded by a sparse ring-like cluster. With only one labeled sample given in each cluster, we compare the result of our proposed BAGL method (Fig. 1 (c)) with the result of the most popular method RBF (Fig. 1 (b)). We use the coordinates as the node feature and set the width in the RBF kernel as \(0.7\). For visualization purposes, we show the adjacency matrix with yellow line segments connecting the node pairs if the weight associated with the edge is greater than 0.5 after normalization. The direction of the edge is ignored in Fig. 1 (c). For a fair comparison, we perform label propagation [85] on both constructed affinity graphs. We can see that BAGL can recover two ground-truth clusters much better. Unlike RBF, BAGL can improve the connectivity by the logarithm penalty term and reduce the inter-cluster links by the block-wise design.

### Real-world Datasets

Classification tasks are implemented to assess the performance of BAGL against all graph construction baseline methods on six real-world datasets, listed in Table 1. **ORHD** (Optical Recognition of Handwritten Digits Data Set), **USPS**, **MNIST**, and **EMNIST Letters** are four popular digits image datasets. **COLL100** is an object image dataset. **TDT2** is a text dataset. We fix the number of anchor nodes as 1000 in four datasets (COIL100, USPS, ORHD and TDT2), while for the rest two datasets (MNIST, EMNIST-Letters), the number of anchors is fixed as 2000. We perform tf-idf and principal component analysis (PCA) as the pre-processing step on TDT2 dataset. The default label inference algorithm is LGC, with ten labels per class. Further details can be found in Appendix F.1.

#### 4.3.1 Efficacy

We fix the number of labeled samples per class to ten and select three label inference methods for the second phase of GSSL. We report average results by performing 20 trials for each algorithm over all the settings in Table 2. Our algorithm outperforms all methods in the USPS, TDT2, and EMNIST-Letters datasets, indicating that BAGL can learn an optimal graph for label inference algorithms in the unified framework. Moreover, we perform the Friedman test with the Bonferroni-Dunn post hoc test for statistical significance analysis. Fig. 2 illustrates the critical difference (CD) diagram on the accuracy, where the average rank is marked along the axis with lower (better) ranks to the left. If the average rank difference between two models is greater than one CD, the relative performance is believed to be different. Accordingly, BAGL significantly outperforms all other baselines by a large margin. We also conduct experiments under low label rates with LGC fixed as the label inference method. Fig. 3 (a) and (b) demonstrate that BAGL performs relatively well with low label rates. This phenomenon can be attributed to the utilization of label information in BAGL. (Appendix G.1)

\begin{table}
\begin{tabular}{c c c c} \hline \hline Dataset & \#Samples \(n\) & \#Features \(d\) & \#Classes \(c\) \\ \hline ORHD & 5,620 & 64 & 10 \\ USPS & 9,298 & 256 & 10 \\ COIL100 & 7,200 & 1,024 & 100 \\ TDT2 & 9,394 & 36,71 & 30 \\ MNIST & 70,000 & 784 & 10 \\ EMNIST Letters & 145,600 & 784 & 20 \\ \hline \hline \end{tabular}
\end{table}
Table 1: Description of datasets

#### 4.3.2 Convergence

As an essential part of the overhead for BAGL, Procedure GWBI needs to be efficient for practice. Compared with two other optimization-based methods sharing a similar objective, SGL and its accelerated version by ADMM [61], BAGL enjoys the fastest convergence rate on both datasets. We sample 1% nodes in each dataset for the convenience of presentation in Fig. 3 (c) and (d). Its outstanding performance confirms the theoretical analysis in Theorem 3. (Appendix G.2).

#### 4.3.3 Ablation Study

To obtain a better understanding of why the proposed BAGL works, we perform some ablation studies to empirically show how the key design of BAGL will potentially affect performance. We create three variants based on the original version of BAGL. First, to demonstrate the significance of the optimal asymmetric structure, we now do not differentiate labeled nodes and unlabeled nodes, and let any graph structure be the potential optimal structure without the constraints of \(\bm{W}_{ul}=\bm{O}\) and \(\bm{W}_{uu}=\bm{O}\). We call this variant _BAGL w/o optimal structure_. Second, to reveal the importance of the connectivity regularization term, we set \(\alpha=0\) in Procedure GWBI to remove the connectivity consideration. This variant is termed _BAGL w/o connectivity_. Third, to investigate the effects of sparsity control, we set \(\beta=10e-5\approx 0\) in Procedure GWBI to allow the sparsity of the constructed graph to vary arbitrarily. The last variant of BAGL is abbreviated as _BAGL w/o sparsity_. We conduct the experiments on the ORHD dataset with the same setting as Table 2 and report the classification accuracy results in Table 3. The proposed optimal asymmetric graph structure contributes most to the success of BAGL. The connectivity regularization term and the sparsity control term also matter since they together encourage a more sparse graph (the latter) but without disconnected components (the former), which is a more favorable graph for the second label inference step.

\begin{table}
\begin{tabular}{c c c c c c c c c c} \hline \hline  & & RBF & kNN & SGL & RGCLI & AGR & GraphEBM & BCAN & BAGL \\ \hline \multirow{3}{*}{ORHD} & GRF & 97.46\(\pm\)0.36 & 86.59\(\pm\)1.26 & 94.68\(\pm\)0.66 & 88.24\(\pm\)3.11 & 97.63\(\pm\)0.53 & 95.13\(\pm\)0.41 & 97.49\(\pm\)0.47 & **97.88\(\pm\)0.40** \\  & LGPC & 97.65\(\pm\)0.59 & 87.61\(\pm\)2.30 & 97.55\(\pm\)0.74 & 89.32\(\pm\)2.58 & 96.90\(\pm\)0.47 & 97.85\(\pm\)0.53 & 97.27\(\pm\)0.63 & **98.04\(\pm\)0.71** \\  & GCN & 98.91\(\pm\)0.44 & 96.04\(\pm\)3.54 & 95.80\(\pm\)0.59 & 89.37\(\pm\)0.29 & 98.02\(\pm\)0.40 & **98.02\(\pm\)0.50** & 98.08\(\pm\)0.70 & 98.15\(\pm\)0.62 \\ \hline \multirow{3}{*}{USPS} & GRF & 94.53\(\pm\)0.65 & 81.42\(\pm\)0.98 & 87.67\(\pm\)0.40 & 84.15\(\pm\)1.85 & 93.62\(\pm\)0.62 & 94.26\(\pm\)0.36 & 93.98\(\pm\)0.60 & **96.56\(\pm\)0.93** \\  & LGPC & 94.75\(\pm\)0.42 & 85.13\(\pm\)1.18 & 86.40\(\pm\)0.51 & 88.63\(\pm\)1.63 & 95.29\(\pm\)0.51 & 94.30\(\pm\)0.30 & 95.07\(\pm\)0.75 & **96.77\(\pm\)0.66** \\  & GCN & 94.98\(\pm\)0.21 & 86.02\(\pm\)0.23 & 90.31\(\pm\)0.32 & 86.89\(\pm\)1.72 & 95.78\(\pm\)0.49 & 95.81\(\pm\)0.48 & 95.79\(\pm\)0.55 & **97.20\(\pm\)0.64** \\ \hline \multirow{3}{*}{COIL100} & GRF & 94.40\(\pm\)0.19 & 81.24\(\pm\)1.64 & 92.65\(\pm\)0.82 & 87.48\(\pm\)2.30 & 86.54\(\pm\)0.40 & 85.22\(\pm\)0.57 & 84.51\(\pm\)0.59 & **94.78\(\pm\)0.53** \\  & LGPC & **95.13\(\pm\)0.37** & 83.66\(\pm\)1.35 & 93.27\(\pm\)1.03 & 87.92\(\pm\)2.47 & 87.18\(\pm\)0.57 & 85.04\(\pm\)0.77 & 87.06\(\pm\)0.63 & 94.93\(\pm\)0.45 \\  & GCN & 94.31\(\pm\)0.25 & 87.64\(\pm\)1.27 & 93.52\(\pm\)0.91 & 99.80\(\pm\)1.65 & 94.63\(\pm\)0.51 & 99.05\(\pm\)0.39 & 90.02\(\pm\)0.48 & **94.99\(\pm\)0.88** \\ \hline \multirow{3}{*}{TDT2} & GRF & 89.22\(\pm\)0.79 & 80.09\(\pm\)2.69 & 92.13\(\pm\)0.99 & 86.51\(\pm\)3.42 & 94.47\(\pm\)0.79 & 93.63\(\pm\)0.74 & 95.95\(\pm\)0.60 & **96.01\(\pm\)0.91** \\  & LGPC & 89.67\(\pm\)0.46 & 82.53\(\pm\)3.04 & 92.96\(\pm\)1.24 & 87.60\(\pm\)2.84 & 94.15\(\pm\)0.67 & 93.97\(\pm\)0.61 & 94.13\(\pm\)0.79 & **95.42\(\pm\)0.71** \\  & GCN & 92.89\(\pm\)0.68 & 85.77\(\pm\)2.41 & 94.39\(\pm\)0.83 & 89.94\(\pm\)3.15 & 95.36\(\pm\)0.84 & 95.36\(\pm\)0.89 & 96.30\(\pm\)0.77 & **96.33\(\pm\)0.85** \\ \hline \multirow{3}{*}{MNIST} & GRF & 83.60\(\pm\)0.24 & 64.20\(\pm\)1.82 & 95.03\(\pm\)0.77 & 87.65\(\pm\)2.07 & 91.02\(\pm\)0.31 & 95.39\(\pm\)0.31 & 92.41\(\pm\)0.47 & **95.40\(\pm\)0.62** \\  & LGPC & 41.42\(\pm\)1.07 & 61.86\(\pm\)1.63 & 94.40\(\pm\)0.52 & 82.22\(\pm\)3.36 & 94.79\(\pm\)0.37 & **95.43\(\pm\)0.47** & 93.55\(\pm\)0.58 & 95.42\(\pm\)0.51 \\  & GCN & 87.03\(\pm\)0.32 & 74.93\(\pm\)1.77 & 95.18\(\pm\)0.47 & 90.47\(\pm\)2.11 & 95.30\(\pm\)0.21 & 95.51\(\pm\)**0.40** & 94.84\(\pm\)0.37 & 95.47\(\pm\)0.43 \\ \

## 5 Conclusion

In this paper, we propose a novel approach to graph construction for graph-based semi-supervised learning. Building upon the optimal asymmetric graph structure derived from theoretical insights, we develop an efficient block-wise graph construction method that guarantees faster convergence. Our approach combines theoretical insights with practical considerations to provide a more effective and reliable framework for the graph construction step in graph-based semi-supervised learning.

## 6 Limitations

BAGL is an optimization-based method for the graph construction step in graph-based semi-supervised learning. Graph Neural Networks (GNNs) excel in learning representations for graph-structured data [64; 54; 12; 49; 39; 78; 41; 75; 13; 53; 77; 51; 73; 52; 50; 38; 76; 11]. More recent graph structure learning methods aim to learn a clean graph structure from the given noisy graph so that the subsequent GNNs trained on this learned clean graph can obtain better performance. In GSSL, however, there is no given graph structure, and we need to learn the graph structure based on the node features only. Therefore, it is a more challenging task compared to graph structure learning. Therefore, we do not compare our method with other graph structure learning methods since their settings and goals are slightly different. We leave the investigation of graph structure learning for GSSL as future work since it is currently out of the scope of this work.

The other limitation of BAGL is it is only suitable for the transductive setting. If we have nodes or samples unseen in the training set, we have to construct the affinity graph again by executing BAGL again to infer their labels, which is often time-consuming and troublesome regarding efficiency. This is not desirable in real-world applications since we often come across new training samples after we build the affinity graph. We also leave the investigation of the inductive extension of BAGL as future work since this lack of inductive generalization is a well-known challenge in graph-based semi-supervised learning.

Even though BAGL is quite efficient in terms of convergence rate, it may still have computational issues when dealing with extremely large-scale datasets with billions of samples because the time spent on finishing one iteration during the optimization would increase dramatically when the number of training samples is extremely large. We leave the exploration of graph construction methods for extremely large-scale datasets as future work. Other potential applications of our method can be explored in the hyperbolic space [67; 70; 68; 71; 69; 66; 72] or in the natural language processing domain [32; 34; 33; 21; 20; 55; 42; 80; 81; 79].

## Acknowledgements

The work described in this paper was partially supported by the National Key Research and Development Program of China (No. 2018AAA0100204) and RGC General Research Funding Scheme (GRF) 14222922 (CUHK 2151185).

## References

* [1] Ahmed El Alaoui, Xiang Cheng, Aaditya Ramdas, Martin J. Wainwright, and Michael I. Jordan. Asymptotic behavior of \(\ell_{p}\)-based laplacian regularization in semi-supervised learning. _CoRR_, abs/1603.00564, 2016.
* [2] Amir Beck and Marc Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse problems. _SIAM J. Imaging Sci._, 2(1):183-202, 2009.

\begin{table}
\begin{tabular}{c c c c c c} \hline \hline  & \multicolumn{2}{c}{BAGL w/o} & BAGL w/o & BAGL w/o & BAGL \\ \cline{2-6}  & \multicolumn{2}{c}{optimal structure} & connectivity & sparsity & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} \\ \hline  & GRF & 95.71\(\pm\)0.84 & 96.71\(\pm\)0.62 & 96.05\(\pm\)0.73 & **97.88\(\pm\)0.40** \\ \cline{2-6} ORHD & LGC & 96.34\(\pm\)0.66 & 96.60\(\pm\)0.57 & 97.19\(\pm\)0.48 & **98.04\(\pm\)0.71** \\ \cline{2-6}  & GCN & 97.29\(\pm\)0.59 & 97.89\(\pm\)0.64 & 97.74\(\pm\)0.53 & **98.15\(\pm\)0.62** \\ \hline \hline \end{tabular}
\end{table}
Table 3: Ablation study of BAGL on the ORHD dataset.

* [3] Amir Beck and Marc Teboulle. A fast dual proximal gradient algorithm for convex minimization and applications. _Operations Research Letters_, 42(1):1-6, 2014.
* [4] Mikhail Belkin, Irina Matveeva, and Partha Niyogi. Tikhonov regularization and semi-supervised learning on large graphs. In _ICASSP (3)_, pages 1000-1003. IEEE, 2004.
* [5] Mikhail Belkin, Partha Niyogi, and Vikas Sindhwani. Manifold regularization: A geometric framework for learning from labeled and unlabeled examples. _J. Mach. Learn. Res._, 7:2399-2434, 2006.
* [6] Lilian Berton and Alneu de Andrade Lopes. Graph construction based on labeled instances for semi-supervised learning. In _ICPR_, pages 2477-2482. IEEE Computer Society, 2014.
* [7] Lilian Berton, Thiago de Paulo Faleiros, Alan Valejo, Jorge Carlos Valverde-Rebaza, and Alneu de Andrade Lopes. RGCLI: robust graph that considers labeled instances for semi-supervised learning. _Neurocomputing_, 226:238-248, 2017.
* [8] Stephen P. Boyd, Neal Parikh, Eric Chu, Borja Peleato, and Jonathan Eckstein. Distributed optimization and statistical learning via the alternating direction method of multipliers. _Found. Trends Mach. Learn._, 3(1):1-122, 2011.
* [9] Jeff Calder, Brendan Cook, Matthew Thorpe, and Dejan Slepcev. Poisson learning: Graph based semi-supervised learning at very low label rates. In _ICML_, volume 119 of _Proceedings of Machine Learning Research_, pages 1306-1316. PMLR, 2020.
* [10] Hongxu Chen, Hongzhi Yin, Tong Chen, Quoc Viet Hung Nguyen, Wen-Chih Peng, and Xue Li. Exploiting centrality information with graph convolutions for network representation learning. In _ICDE_, pages 590-601. IEEE, 2019.
* [11] Yankai Chen, Yixiang Fang, Yifei Zhang, and Irwin King. Bipartite graph convolutional hashing for effective and efficient top-n search in hamming space. _arXiv preprint arXiv:2304.00241_, 2023.
* [12] Yankai Chen, Yifei Zhang, Menglin Yang, Zixing Song, Chen Ma, and Irwin King. WSFE: wasserstein sub-graph feature encoder for effective user segmentation in collaborative filtering. _CoRR_, abs/2305.04410, 2023.
* [13] Yankai Chen, Yifei Zhang, Menglin Yang, Zixing Song, Chen Ma, and Irwin King. WSFE: wasserstein sub-graph feature encoder for effective user segmentation in collaborative filtering. In _SIGIR_, pages 2521-2525. ACM, 2023.
* [14] Zhijie Chen, Hongtai Cao, and Kevin Chen-Chuan Chang. Graphebm: Energy-based graph construction for semi-supervised learning. In _ICDM_, pages 62-71. IEEE, 2020.
* [15] Inae Choi and Hyunjung Shin. Semi-supervised learning with ensemble learning and graph sharpening. In _IDEAL_, volume 5326 of _Lecture Notes in Computer Science_, pages 172-179. Springer, 2008.
* [16] Celso Andre R. de Sousa, Solange O. Rezende, and Gustavo E. A. P. A. Batista. Influence of graph construction on semi-supervised learning. In _ECML/PKDD (3)_, volume 8190 of _Lecture Notes in Computer Science_, pages 160-175. Springer, 2013.
* [17] Cheng-Hao Deng and Wan-Lei Zhao. Fast k-means based on k-nn graph. In _ICDE_, pages 1220-1223. IEEE Computer Society, 2018.
* [18] Xiaowen Dong, Dorina Thanou, Pascal Frossard, and Pierre Vandergheynst. Laplacian matrix learning for smooth graph signal representation. In _ICASSP_, pages 3736-3740. IEEE, 2015.
* [19] Xiaowen Dong, Dorina Thanou, Michael G. Rabbat, and Pascal Frossard. Learning graphs from data: A signal representation perspective. _IEEE Signal Process. Mag._, 36(3):44-63, 2019.
* [20] Yifan Gao, Jingjing Li, Michael R Lyu, and Irwin King. Open-retrieval conversational machine reading. _arXiv preprint arXiv:2102.08633_, 2021.

* [21] Yifan Gao, Chien-Sheng Wu, Jingjing Li, Shafiq Joty, Steven CH Hoi, Caiming Xiong, Irwin King, and Michael Lyu. Discern: Discourse-aware entailment reasoning network for conversational machine reading. In _Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)_, pages 2439-2449, 2020.
* [22] Aditya Gemawat. Graphgem: Optimized scalable system for graph convolutional networks. In _SIGMOD Conference_, pages 2920-2922. ACM, 2021.
* [23] Fang He, Feiping Nie, Rong Wang, Haojie Hu, Weimin Jia, and Xuelong Li. Fast semi-supervised learning with optimal bipartite graph. _IEEE Trans. Knowl. Data Eng._, 33(9):3245-3257, 2021.
* [24] Fang He, Feiping Nie, Rong Wang, Xuelong Li, and Weimin Jia. Fast semisupervised learning with bipartite graph for large-scale data. _IEEE Transactions on Neural Networks and Learning Systems_, 31(2):626-638, 2020.
* [25] Qian Huang, Horace He, Abhay Singh, Ser-Nam Lim, and Austin R. Benson. Combining label propagation and simple models out-performs graph neural networks. In _ICLR_. OpenReview.net, 2021.
* [26] Tony Jebara, Jun Wang, and Shih-Fu Chang. Graph construction and \(b\)-matching for semi-supervised learning. In _ICML_, volume 382 of _ACM International Conference Proceeding Series_, pages 441-448. ACM, 2009.
* [27] Junteng Jia and Austin R. Benson. A unifying generative model for graph learning algorithms: Label propagation, graph convolutions, and combinations. _SIAM Journal on Mathematics of Data Science_, 4(1):100-125, 2022.
* [28] Rie Johnson and Tong Zhang. On the effectiveness of laplacian normalization for graph semi-supervised learning. _J. Mach. Learn. Res._, 8:1489-1517, 2007.
* [29] Vassilis Kalofolias. How to learn a graph from smooth signals. In _AISTATS_, volume 51 of _JMLR Workshop and Conference Proceedings_, pages 920-929. JMLR.org, 2016.
* [30] Vassilis Kalofolias and Nathanael Perraudin. Large scale graph learning from smooth signals. In _ICLR (Poster)_. OpenReview.net, 2019.
* [31] Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. In _ICLR (Poster)_. OpenReview.net, 2017.
* [32] Jingjing Li, Yifan Gao, Lidong Bing, Irwin King, and Michael R Lyu. Improving question generation with to the point context. In _Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)_, pages 3216-3226, 2019.
* [33] Jingjing Li, Zichao Li, Tao Ge, Irwin King, and Michael R Lyu. Text revision by on-the-fly representation optimization. pages 10956-10964, 2022.
* [34] Jingjing Li, Zichao Li, Lili Mou, Xin Jiang, Michael Lyu, and Irwin King. Unsupervised text generation by learning from search. _Advances in Neural Information Processing Systems_, 33:10820-10831, 2020.
* [35] Qimai Li, Xiao-Ming Wu, Han Liu, Xiaotong Zhang, and Zhichao Guan. Label efficient semi-supervised learning via graph filtering. In _CVPR_, pages 9582-9591. Computer Vision Foundation / IEEE, 2019.
* [36] Yu-Feng Li, Shao-Bo Wang, and Zhi-Hua Zhou. Graph quality judgement: A large margin expedition. In _IJCAI_, pages 1725-1731. IJCAI/AAAI Press, 2016.
* [37] Jiye Liang, Junbiao Cui, Jie Wang, and Wei Wei. Graph-based semi-supervised learning via improving the quality of the graph dynamically. _Mach. Learn._, 110(6):1345-1388, 2021.
* [38] Langzhang Liang, Zenglin Xu, Zixing Song, Irwin King, Yuan Qi, and Jieping Ye. Tackling long-tailed distribution issue in graph neural networks via normalization. _IEEE Transactions on Knowledge and Data Engineering_, pages 1-11, 2023.

* [39] Langzhang Liang, Zenglin Xu, Zixing Song, Irwin King, and Jieping Ye. Resnorm: Tackling long-tailed degree distribution issue in graph neural networks via normalization. _CoRR_, abs/2206.08181, 2022.
* [40] Wei Liu, Junfeng He, and Shih-Fu Chang. Large graph construction for scalable semi-supervised learning. In _ICML_, pages 679-686. Omnipress, 2010.
* [41] Yueen Ma, Zixing Song, Xuming Hu, Jingjing Li, Yifei Zhang, and Irwin King. Graph component contrastive learning for concept relatedness estimation. In _AAAI_, pages 13362-13370. AAAI Press, 2023.
* [42] Yueen Ma, Zixing Song, Xuming Hu, Jingjing Li, Yifei Zhang, and Irwin King. Graph component contrastive learning for concept relatedness estimation. _Proceedings of the AAAI Conference on Artificial Intelligence_, 37(11):13362-13370, Jun. 2023.
* [43] Neal Parikh and Stephen Boyd. Proximal algorithms. _Found. Trends Optim._, 1(3):127-239, jan 2014.
* [44] Michael G. Rabbat. Inferring sparse graphs from smooth signals with theoretical guarantees. In _ICASSP_, pages 6533-6537. IEEE, 2017.
* [45] Seyed Saman Saboksayr and Gonzalo Mateos. Accelerated graph learning from smooth signals. _IEEE Signal Process. Lett._, 28:2192-2196, 2021.
* [46] Hyunjung Shin, N. Jeremy Hill, Andreas Martin Lisewski, and Joon-Sang Park. Graph sharpening. _Expert Syst. Appl._, 37(12):7870-7879, 2010.
* [47] Hyunjung Shin, N. Jeremy Hill, and Gunnar Ratsch. Graph based semi-supervised learning with sharper edges. In _ECML_, volume 4212 of _Lecture Notes in Computer Science_, pages 401-412. Springer, 2006.
* [48] Anthony Man-Cho So. Moment inequalities for sums of random matrices and their applications in optimization. _Math. Program._, 130(1):125-151, 2011.
* [49] Zixing Song and Irwin King. Hierarchical heterogeneous graph attention network for syntax-aware summarization. In _AAAI_, pages 11340-11348. AAAI Press, 2022.
* [50] Zixing Song, Yueen Ma, and Irwin King. Individual fairness in dynamic financial networks. In _NeurIPS 2022 Workshop: New Frontiers in Graph Learning_, 2022.
* [51] Zixing Song, Ziqiao Meng, Yifei Zhang, and Irwin King. Semi-supervised multi-label learning for graph-structured data. In _CIKM_, pages 1723-1733. ACM, 2021.
* [52] Zixing Song, Xiangli Yang, Zenglin Xu, and Irwin King. Graph-based semi-supervised learning: A comprehensive review. _IEEE Transactions on Neural Networks and Learning Systems_, pages 1-21, 2022.
* [53] Zixing Song, Yifei Zhang, and Irwin King. Towards an optimal asymmetric graph structure for robust semi-supervised node classification. In _KDD_, pages 1656-1665. ACM, 2022.
* [54] Zixing Song, Yuji Zhang, and Irwin King. Towards fair financial services for all: A temporal GNN approach for individual fairness on transaction networks. In _CIKM_, pages 2331-2341. ACM, 2023.
* [55] Xin Sun, Tao Ge, Shuming Ma, Jingjing Li, Furu Wei, and Houfeng Wang. A unified strategy for multilingual grammatical error correction with pre-trained cross-lingual language model. In Lud De Raedt, editor, _Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, IJCAI-22_, pages 4367-4374. International Joint Conferences on Artificial Intelligence Organization, 7 2022. Main Track.
* [56] Roman Vershynin. _Introduction to the non-asymptotic analysis of random matrices_, page 210-268. Cambridge University Press, 2012.
* [57] Fei Wang and Changshui Zhang. Label propagation through linear neighborhoods. In _ICML_, volume 148 of _ACM International Conference Proceeding Series_, pages 985-992. ACM, 2006.

* [58] Fei Wang and Changshui Zhang. Label propagation through linear neighborhoods. _IEEE Trans. Knowl. Data Eng._, 20(1):55-67, 2008.
* [59] Hongwei Wang and Jure Leskovec. Combining graph convolutional neural networks and label propagation. _ACM Trans. Inf. Syst._, 40(4), nov 2021.
* [60] Xiaolu Wang, Yuen-Man Pun, and Anthony Man-Cho So. Distributionally robust graph learning from smooth signals under moment uncertainty. _IEEE Trans. Signal Process._, 70:6216-6231, 2022.
* [61] Xiaolu Wang, Chaorui Yao, Haoyu Lei, and Anthony Man-Cho So. An efficient alternating direction method for graph learning from smooth signals. In _ICASSP_, pages 5380-5384. IEEE, 2021.
* [62] Yiwei Wang, Wei Wang, Yuxuan Liang, Yujun Cai, Juncheng Liu, and Bryan Hooi. Nodeaug: Semi-supervised node classification with data augmentation. In _KDD_, pages 207-217. ACM, 2020.
* [63] Zhen Wang, Long Zhang, Rong Wang, Feiping Nie, and Xuelong Li. Semi-supervised learning via bipartite graph construction with adaptive neighbors. _IEEE Transactions on Knowledge and Data Engineering_, pages 1-1, 2022.
* [64] Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and Philip S. Yu. A comprehensive survey on graph neural networks. _IEEE Trans. Neural Networks Learn. Syst._, 32(1):4-24, 2021.
* [65] Tian Xie, Bin Wang, and C.-C. Jay Kuo. Graphhop: An enhanced label propagation method for node classification. _IEEE Transactions on Neural Networks and Learning Systems_, pages 1-15, 2022.
* [66] Menglin Yang, Zhihao Li, Min Zhou, Jiahong Liu, and Irwin King. Hicf: Hyperbolic informative collaborative filtering. In _Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining_, pages 2212-2221, 2022.
* [67] Menglin Yang, Ziqiao Meng, and Irwin King. FeatureNorm: L2 feature normalization for dynamic graph embedding. In _2020 IEEE International Conference on Data Mining (ICDM)_, pages 731-740. IEEE, 2020.
* [68] Menglin Yang, Min Zhou, Marcus Kalander, Zengfeng Huang, and Irwin King. Discrete-time temporal network embedding via implicit hierarchical learning in hyperbolic space. In _Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining_, pages 1975-1985, 2021.
* [69] Menglin Yang, Min Zhou, Jiahong Liu, Defu Lian, and Irwin King. Hrcf: Enhancing collaborative filtering via hyperbolic geometric regularization. In _Proceedings of the ACM Web Conference 2022_, pages 2462-2471, 2022.
* [70] Menglin Yang, Min Zhou, Lujia Pan, and Irwin King. \(\kappa\)hgcn: Tree-likeness modeling via continuous and discrete curvature learning. In _Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining_, pages 2965-2977, 2023.
* [71] Menglin Yang, Min Zhou, Hui Xiong, and Irwin King. Hyperbolic temporal network embedding. _IEEE Transactions on Knowledge and Data Engineering_, 2022.
* [72] Menglin Yang, Min Zhou, Rex Ying, Yankai Chen, and Irwin King. Hyperbolic representation learning: Revisiting and advancing. _arXiv preprint arXiv:2306.09118_, 2023.
* [73] Xiangli Yang, Zixing Song, Irwin King, and Zenglin Xu. A survey on deep semi-supervised learning. _IEEE Transactions on Knowledge and Data Engineering_, pages 1-20, 2022.
* [74] Yuan Yuan, Xin Li, Qi Wang, and Feiping Nie. A semi-supervised learning algorithm via adaptive laplacian graph. _Neurocomputing_, 426:162-173, 2021.
* [75] Yifei Zhang, Yankai Chen, Zixing Song, and Irwin King. Contrastive cross-scale graph knowledge synergy. In _KDD_, pages 3422-3433. ACM, 2023.

* [76] Yifei Zhang, Hao Zhu, Ziqiao Meng, Piotr Koniusz, and Irwin King. Graph-adaptive rectified linear unit for graph neural networks. In _Proceedings of the ACM Web Conference 2022_, 2022.
* [77] Yifei Zhang, Hao Zhu, Zixing Song, Piotr Koniusz, and Irwin King. COSTA: covariance-preserving feature augmentation for graph contrastive learning. In _KDD_, pages 2524-2534. ACM, 2022.
* [78] Yifei Zhang, Hao Zhu, Zixing Song, Piotr Koniusz, and Irwin King. Spectral feature augmentation for graph contrastive learning and beyond. In _AAAI_, pages 11289-11297. AAAI Press, 2023.
* [79] Yuji Zhang and Jing Li. Time will change things: An empirical study on dynamic language understanding in social media classification. _arXiv e-prints_, pages arXiv-2210, 2022.
* [80] Yuji Zhang, Jing Li, and Wenjie Li. Vibe: Topic-driven temporal adaptation for twitter classification. In _Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing (EMNLP)_, Singapore, December 2023. Association for Computational Linguistics.
* [81] Yuji Zhang, Yubo Zhang, Chunpu Xu, Jing Li, Ziyan Jiang, and Baolin Peng. #HowYouTagTweets: Learning user hashtagging preferences via personalized topic attention. In _Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing_, pages 7811-7820, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics.
* [82] Dengyong Zhou, Olivier Bousquet, Thomas Navin Lal, Jason Weston, and Bernhard Scholkopf. Learning with local and global consistency. In _NIPS_, pages 321-328. MIT Press, 2003.
* [83] Dengyong Zhou, Jiayuan Huang, and Bernhard Scholkopf. Learning from labeled and unlabeled data on a directed graph. In _Proceedings of the 22nd International Conference on Machine Learning_, ICML '05, page 1036-1043, New York, NY, USA, 2005. Association for Computing Machinery.
* [84] Dengyong Zhou, Bernhard Scholkopf, and Thomas Hofmann. Semi-supervised learning on directed graphs. In _NIPS_, pages 1633-1640, 2004.
* [85] Xiaojin Zhu, Zoubin Ghahramani, and John D. Lafferty. Semi-supervised learning using gaussian fields and harmonic functions. In _ICML_, pages 912-919. AAAI Press, 2003.
* [86] Liansheng Zhuang, Zihan Zhou, Shenghua Gao, Jingwen Yin, Zhouchen Lin, and Yi Ma. Label information guided graph construction for semi-supervised learning. _IEEE Trans. Image Process._, 26(9):4182-4192, 2017.