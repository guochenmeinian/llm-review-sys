# Solving Inverse Problems via Diffusion Optimal Control

Henry Li

Yale University

henry.li@yale.edu

&Marcus Pereira

Bosch Center for Artificial Intelligence

marcus.pereira@us.bosch.com

Work partially completed during an internship at Bosch AI.

###### Abstract

Existing approaches to diffusion-based inverse problem solvers frame the signal recovery task as a probabilistic sampling episode, where the solution is drawn from the desired posterior distribution. This framework suffers from several critical drawbacks, including the intractability of the conditional likelihood function, strict dependence on the score network approximation, and poor \(\mathbf{x}_{0}\) prediction quality. We demonstrate that these limitations can be sidestepped by reframing the generative process as a discrete optimal control episode. We derive a diffusion-based optimal controller inspired by the iterative Linear Quadratic Regulator (iLQR) algorithm. This framework is fully general and able to handle any differentiable forward measurement operator, including super-resolution, inpainting, Gaussian deblurring, nonlinear deblurring, and even highly nonlinear neural classifiers. Furthermore, we show that the idealized posterior sampling equation can be recovered as a special case of our algorithm. We then evaluate our method against a selection of neural inverse problem solvers, and establish a new baseline in image reconstruction with inverse problems1.

Footnote 1: Code is available at https://github.com/lihenryhfl/diffusion_optimal_control.

## 1 Introduction

Diffusion models Song and Ermon (2019); Ho et al. (2020) have been shown to be remarkably adept at conditional generation tasks Dhariwal and Nichol (2021); Ho and Salimans (2022), in part due to their iterative sampling algorithm, which allows the dynamics of an uncontrolled prior score function \(\nabla_{\mathbf{x}}\log p_{t}(\mathbf{x})\) to be directed towards an arbitrary posterior distribution by introducing an additive guidance term \(\mathbf{u}\). When this guidance term is the conditional score \(\nabla_{\mathbf{x}}\log p_{t}(\mathbf{y}|\mathbf{x})\), the resulting sample is provably drawn from the desired conditional distribution \(p(\mathbf{x}|\mathbf{y})\)Song et al. (2020).

A central obstacle to this framework is the general difficulty of obtaining the conditional score function \(\nabla_{\mathbf{x}}\log p_{t}(\mathbf{y}|\mathbf{x}_{t})\) due to its dependence on the _noisy_ diffusion variate \(\mathbf{x}_{t}\) rather than just the final sample \(\mathbf{x}_{0}\)Chung et al. (2023). In large-scale conditional generation tasks such as class- or text-conditional sampling the computational overhead of training a time-dependent conditional score function from scratch is deemed acceptable, and is indeed the approach taken by Rombach et al. (2022); Saharia et al. (2022), and many others. However, this solution is not acceptable in inverse problems where the goal is to design a generalized solver that will work in a zero-shot capacity for an arbitrary forward model.

This bottleneck has spawned a flurry of recent research dedicated to approximating the conditional score \(\nabla_{\mathbf{x}}\log p_{t}(\mathbf{y}|\mathbf{x}_{t})\) as a simple function of the _noiseless_ likelihood \(\log p(\mathbf{y}|\mathbf{x}_{0})\)Choi et al. (2021); Chung et al. (2022); Rout et al. (2024); Chung et al. (2023); Kawar et al. (2022); Chung et al. (2023). However, as we will demonstrate in this work, these approximations impose a significant cost to the performance of the resulting algorithm.

To address these issues, we propose a novel framework built from optimal control theory where such approximations are no longer necessary. By framing the reverse diffusion process as an optimal control episode, we are able to detach the inverse problem solver from the strict requirements of the conditional sampling equation given by Song et al. (2020), while still leveraging the exceptionally powerful prior of the unconditional diffusion process. Moreover, we find that the desired score function directly arises as the Jacobian of the value function.

We summarize our contributions as follows:

* We present diffusion optimal control, a framework for solving inverse problems via the lens of optimal control theory, using pretrained unconditional off-the-shelf diffusion models.
* We show that this perspective overcomes many core obstacles present in existing diffusion-based inverse problem solvers. In particular, the idealized posterior sampling score Song et al. (2021) -- approximated by existing methods -- can be recovered exactly as a specific case of our method.
* We showcase the advantages of our model empirically with quantitative experiments and qualitative examples, and demonstrate state-of-the-art performance on the FFHQ \(256\times 256\) dataset.

## 2 Background

NotationWe use lowercase letters for denoting scalars \(a\in\mathbb{R}\), lowercase bold letters for vectors \(\mathbf{a}\in\mathbb{R}^{n}\) and uppercase bold letters for matrices \(\mathbf{A}\in\mathbb{R}^{m\times n}\). Subscripts indicate Jacobians and Hessians of scalar functions, e.g. \(l_{\mathbf{x}}\in\mathbb{R}^{n}\) and \(l_{\mathbf{x}\mathbf{x}}\in\mathbb{R}^{n\times n}\) for \(l(\mathbf{x}):\mathbb{R}^{n}\rightarrow\mathbb{R}\), respectively. We overload notation for time-dependent variables, where subscripts imply dependence rather than derivatives w.r.t. time, e.g., \(\mathbf{x}_{t}=\mathbf{x}(t)\). Furthermore, \(V(\mathbf{x}_{t})\) and \(Q(\mathbf{x}_{t},\mathbf{u}_{t})\) are scalar functions despite being uppercase, in line with existing optimal control literature Betts (1998).

### Diffusion Models

The diffusion modeling literature uses the following reverse-time Ito SDE to generate samples Song et al. (2021),

\[\mathrm{d}\mathbf{x}_{t}=\big{[}\mathbf{f}(\mathbf{x}_{t})-g(t)^{2}\nabla_{ \mathbf{x}_{t}}\log p_{t}(\mathbf{x}_{t})\big{]}\mathrm{d}t+g(t)\mathrm{d} \mathbf{w}_{t},\] (1)

where \(\mathbf{x}_{t}\in\mathbb{R}^{n}\) is the state vector, \(\mathbf{f}:\mathbb{R}^{n}\rightarrow\mathbb{R}^{n}\) and \(g:\mathbb{R}\rightarrow\mathbb{R}\) are drift and diffusion terms that can take different functional forms (e.g., Variance-Preserving SDEs (VPSDEs) and Variance-Exploding SDEs (VESDEs) in Song et al. (2021)), \(\nabla_{\mathbf{x}_{t}}\log p_{t}(\mathbf{x}_{t})\) is the score-function and \(\mathbf{w}_{t}\in\mathbb{R}^{n}\) is a vector of mutually independent Brownian motions. The above SDE has an associated ODE called the

Figure 1: **Conceptual illustration comparing a probabilistic posterior sampler to our proposed optimal control-based sampler.** In a probabilistic sampler, the model relies on an approximation \(\tilde{\mathbf{x}}_{0}\approx\mathbf{x}_{0}\) to guide each step **(left)**. We are able to compute \(\mathbf{x}_{0}\)**exactly** on each step, resulting in much higher quality gradients \(\nabla\log p(\mathbf{y}|\tilde{\mathbf{x}}_{0})\) and an improved trajectory update (**right**).

probability-flow (PF) ODE given by

\[\mathrm{d}\mathbf{x}_{t}=\mathrm{d}\mathbf{x}_{t}+\big{[}\mathbf{f}(\mathbf{x}_{t })-\frac{1}{2}g(t)^{2}\nabla_{\mathbf{x}_{t}}\log p_{t}(\mathbf{x}_{t})\big{]} \mathrm{d}t,\] (2)

with the same marginals \(p_{t}(\mathbf{x}_{t})\) as the SDE, which allow for likelihood computation (Song et al., 2021; Li et al., 2024). All practical implementations of diffusion samplers require a time-discretization of the PF-ODE. One such discretization is the well-known Euler-discretization which gives,

\[\mathbf{x}_{t-1}=\mathbf{x}_{t}-[\mathbf{f}(\mathbf{x}_{t})-\frac{1}{2}g(t)^{2 }\nabla_{\mathbf{x}}\log p_{t}(\mathbf{x}_{t})]\Delta t\] (3)

where, \(\Delta t\) is the length of the discretization interval and we have reversed the time evolution by changing the sign of the drift. We are not restricted to only using the Euler-discretization and any high-order discretization techniques can also be employed. More concisely, we have,

\[\mathbf{x}_{t-1}=\mathbf{h}(\mathbf{x}_{t}),\ \ \text{where}\ \mathbf{h}: \mathbb{R}^{n}\rightarrow\mathbb{R}^{n}\] (4)

which describes the general non-linear dynamics of the corresponding discrete-time diffusion sampler.

### Posterior Sampling for Inverse Problems

Inverse problems are a general class of problems where an unknown signal is reconstructed from observations obtained by a forward measurement process Ongie et al. (2020). The forward process is usually lossy, resulting in an ill-posed signal recovery task where a _unique_ solution does not exist. The forward model can generally be written as

\[y=\mathcal{A}(\mathbf{x}_{0})+\eta,\] (5)

where \(\mathcal{A}:\mathbb{R}^{n}\rightarrow\mathbb{R}^{d}\) is the forward operator, \(y\in\mathbb{R}^{d}\) the measured signal, \(\mathbf{x}_{0}\in\mathbb{R}^{n}\) the unknown signal to be recovered, and \(\eta\sim\mathcal{N}(0,\sigma\mathbf{I}_{d})\) the noise (with variance \(\sigma^{2}\)) in the measurement process.

Given the forward model Eq. (5) and a measurement \(\mathbf{y}\), sampling from the posterior distribution \(p_{\theta}(\mathbf{x}|\mathbf{y})\) can then be performed by solving the corresponding _conditional_ Ito SDE

\[\mathrm{d}\mathbf{x}=[\mathbf{f}(\mathbf{x})-g(t)^{2}\nabla_{\mathbf{x}}\log p _{t}(\mathbf{x}|\mathbf{y})]\mathrm{d}t+g(t)\mathrm{d}\mathbf{w},\] (6)

where, invoking Bayes rule,

\[\nabla_{\mathbf{x}}\log p_{t}(\mathbf{x}|\mathbf{y})=\nabla_{\mathbf{x}}\log p _{t}(\mathbf{x})+\nabla_{\mathbf{x}}\log p_{t}(\mathbf{y}|\mathbf{x}).\] (7)

As with the unconditional dynamics, Eq. (6) has a corresponding ODE

\[\mathrm{d}\mathbf{x}=[\mathbf{f}(\mathbf{x})-\frac{1}{2}g(t)^{2}\nabla_{ \mathbf{x}}\log p_{t}(\mathbf{x}|\mathbf{y})]\mathrm{d}t,\] (8)

which has an approximate solution obtained by the Euler discretization

\[\mathbf{x}_{t-1}=\mathbf{x}_{t}+[f(\mathbf{x}_{t})-\frac{1}{2}g(t)^{2}\nabla_{ \mathbf{x}_{t}}\log p_{t}(\mathbf{x}_{t}|\mathbf{y})]\Delta t.\] (9)

Figure 2: **Predicted \(\mathbf{x}_{0}\) used in a probabilistic framework (above) compared to ours (below) for a general diffusion trajectory.** The full forward rollout in our proposed framework allows for the predicted \(\mathbf{x}_{0}\) (and therefore \(\nabla_{\mathbf{x}_{t}}\log p(\mathbf{y}|\mathbf{x}_{0})\)) to be efficiently computed for all \(t=0,\dots,T\).

### Optimal Control

Optimal control is the structured and principled approach to the guidance of dynamical systems over time. Many methods have been developed in the optimal control literature and are popularly referred to as _trajectory optimization_ algorithms Betts (1998). Perhaps the most well-known is the Iterative Linear Quadratic Regulator (iLQR) algorithm which uses a first-order approximation of the dynamics and second-order approximations of the value-function Li and Todorov (2004).

Formally, let us define an arbitrary user-defined global cost function

\[J_{T}=\sum_{t=T}^{1}\ell_{t}(\mathbf{x}_{t},\,\mathbf{u}_{t})+\ell_{0}( \mathbf{x}_{0}),\] (10)

composed of a sum over scalar-valued _running_ and _terminal_ cost functions \(\ell_{t}\) and \(\ell_{0}\). Optimal control theory dictates that the value function \(V(\mathbf{x}_{t},\,t):=\min_{\{\mathbf{u}_{t}\}_{n=1}^{n=1}}J_{t}\) satisfies the following recursive relation also known as _Bellman's Principle of Optimality_

\[V(\mathbf{x}_{t},\,t)=\min_{\mathbf{u}_{t}}\Big{[}\ell_{t}(\mathbf{x}_{t},\, \mathbf{u}_{t})+V(\mathbf{x}_{t-1},\,t-1)\Big{]}.\] (11)

The iLQR algorithm centers around approximating the state-action value function,

\[Q(\mathbf{x}_{t},\,\mathbf{u}_{t}):=\ell_{t}(\mathbf{x}_{t},\,\mathbf{u}_{t}) +V(\mathbf{x}_{t-1},\,t-1),\] (12)

from which the value function can be recovered as \(V(\mathbf{x}_{t},\,t)=\min_{\mathbf{u}_{t}}Q(\mathbf{x}_{t},\,\mathbf{u}_{t})\).

Then given a state transition function \(\mathbf{x}_{t}=\mathbf{h}(\mathbf{x}_{t+1},\mathbf{u}_{t+1})\) where we crucially note that we have defined time to flow _backwards_ from \(t=T,\ldots,0\), the iLQR algorithm has feedforward and feedback gains

\[\mathbf{k}=-Q_{\mathbf{u}\mathbf{u}}^{-1}Q_{\mathbf{u}}\qquad\text{and} \qquad\mathbf{K}=-Q_{\mathbf{u}\mathbf{u}}^{-1}Q_{\mathbf{u}\mathbf{x}}\] (13)

The update equations can be written as

\[V_{\mathbf{x}}=Q_{\mathbf{x}}-\mathbf{K}^{T}Q_{\mathbf{u}\mathbf{u}}\mathbf{ k}\qquad\text{and}\qquad V_{\mathbf{x}\mathbf{x}}=Q_{\mathbf{x}\mathbf{x}}- \mathbf{K}^{T}Q_{\mathbf{u}\mathbf{u}}\mathbf{K}.\] (14)

Given the feedforward and feedback gains \(\{(\mathbf{K}_{t},\mathbf{k}_{t})\}_{t=0}^{T}\) and \(\bar{\mathbf{x}}_{0}:=\mathbf{x}_{0}\), we can recursively obtain the locally optimal control at time \(t\) as a function of the present states \(\mathbf{x}_{t}\) and controls \(\mathbf{u}_{t}\) as

\[\bar{\mathbf{x}}_{t} =\mathbf{h}(\bar{\mathbf{x}}_{t+1},\mathbf{u}_{t+1}^{*}),\] (15) \[\mathbf{u}_{t}^{*} =\mathbf{u}_{t}+\lambda\mathbf{k}+\mathbf{K}(\bar{\mathbf{x}}_{t }-\mathbf{x}_{t}).\] (16)

For a more detailed treatment of iLQR as well as a derivation of the equations, please see Appendix B.

## 3 Diffusion Optimal Control

We motivate our framework by observing that the reverse diffusion process Eq. (1) is an uncontrolled non-linear dynamical system that evolves from some initial state (at time \(t=T\)) to some terminal state (at time \(t=0\)). By injecting control vectors \(\mathbf{u}_{t}\) into this system we can influence its behavior and hence its terminal state (i.e., the generated data) to sample from a desired \(p(\mathbf{x}|\mathbf{y})\). There are two obvious ways to inject control into this process:

Figure 3: **Inverse problem solution as a function of total diffusion timesteps \(T\) for the \(4\times\) super-resolution task.** Compared to DPS (**top row**), our method (**bottom row**) produces solutions that are higher quality, in greater agreement with the inverse problem contraint \(\mathcal{A}\mathbf{x}=\mathbf{y}\), and more stable across \(T\).

1. In **input perturbation control**, we apply the \(\mathbf{u}_{t}\)_before_ the diffusion step: \[\mathbf{x}_{t-1}=(\mathbf{x}_{t}+\mathbf{u}_{t})-\big{[}\mathbf{f}(\mathbf{x}_{ t}+\mathbf{u}_{t})-\frac{1}{2}g(t)^{2}\nabla_{\mathbf{x}}\log p_{t}(\mathbf{x}_{t}+ \mathbf{u}_{t})\big{]}\Delta t.\] (17)
2. In **output perturbation control**, \(\mathbf{u}_{t}\) is applied _after_ the diffusion step: \[\mathbf{x}_{t-1}=\mathbf{x}_{t}-\big{[}\mathbf{f}(\mathbf{x}_{t})-\frac{1}{2}g (t)^{2}\nabla_{\mathbf{x}}\log p_{t}(\mathbf{x}_{t})\big{]}\Delta t+\mathbf{u }_{t}.\] (18)

Observe that iLQR is formulated for general discrete-time dynamic processes. When applied specifically to the reverse diffusion dynamics of diffusion models, we are able to make several simplifications. First, we assume that we do not have access to any guidance except at time \(t=0\) -- i.e., \(\ell_{t}(\mathbf{x}_{t},\mathbf{u}_{t})\) does not depend on \(\mathbf{x}_{t}\).

In the case of **input perturbation control**, we observe from Eq. (17) that \(\mathbf{h}_{\mathbf{x}}=\mathbf{h}_{\mathbf{u}}\), whereas **output perturbation control** implies that \(\mathbf{h}_{\mathbf{u}}=\mathbf{I}\), resulting in the left and right equations, respectively:

\[Q_{\mathbf{x}} =\mathbf{h}_{\mathbf{x}}^{T}V_{\mathbf{x}}^{\prime} Q_{\mathbf{x}} =\mathbf{h}_{\mathbf{x}}^{T}V_{\mathbf{x}}^{\prime}\] (19) \[Q_{\mathbf{u}} =\ell_{\mathbf{u}}+\mathbf{h}_{\mathbf{x}}^{T}V_{\mathbf{x}}^{\prime} Q_{\mathbf{u}} =\ell_{\mathbf{u}}+V_{\mathbf{x}}^{\prime}\] (20) \[Q_{\mathbf{x}\mathbf{x}} =\mathbf{h}_{\mathbf{x}}^{T}V_{\mathbf{x}\mathbf{x}}^{\prime} \mathbf{h}_{\mathbf{x}} Q_{\mathbf{x}\mathbf{x}} =\mathbf{h}_{\mathbf{x}}^{T}V_{\mathbf{x}\mathbf{x}}^{\prime}\mathbf{h }_{\mathbf{x}}\] (21) \[Q_{\mathbf{u}\mathbf{x}} =Q_{\mathbf{x}\mathbf{u}} =\mathbf{h}_{\mathbf{x}}^{T}V_{\mathbf{x}\mathbf{x}}^{\prime} \mathbf{h}_{\mathbf{x}} Q_{\mathbf{u}\mathbf{u}} =\ell_{\mathbf{u}\mathbf{u}}+V_{\mathbf{x}\mathbf{x}}^{\prime}.\] (22)

The derivatives of \(V\) can then be backpropagated using the following equations:

\[V_{\mathbf{x}} =Q_{\mathbf{x}}-\mathbf{K}^{T}Q_{\mathbf{u}\mathbf{u}}\mathbf{k} =Q_{\mathbf{x}\mathbf{x}}-\mathbf{K}^{T}Q_{\mathbf{u}\mathbf{u}}\mathbf{K}\] \[=Q_{\mathbf{x}}+Q_{\mathbf{u}\mathbf{x}}^{T}Q_{\mathbf{u}\mathbf{ u}}^{-1}Q_{\mathbf{u}}\] (24) \[V_{\mathbf{x}\mathbf{x}} =Q_{\mathbf{x}\mathbf{x}}-\mathbf{K}^{T}Q_{\mathbf{u}\mathbf{u}} \mathbf{K}\] \[=Q_{\mathbf{x}\mathbf{x}}-Q_{\mathbf{u}\mathbf{x}}^{T}Q_{\mathbf{ u}\mathbf{u}}^{-1}Q_{\mathbf{u}\mathbf{x}}.\] (25)

In high dimensional systems such as Eq. 3, matrices may be singular. Therefore, a Tikhonov regularized variant of iLQR is often employed, where matrix inverses are regularized by a diagonal matrix \(\alpha\mathbf{I}\)Tassa et al. (2014).

### High Dimensional Control

Compared to the dynamics in traditional application areas of optimal control, those we consider in Eqs. (17- 18) are much higher dimensional in the state \(\mathbf{x}\) and control \(\mathbf{u}\) variates. Therefore, iLQR faces several unique computational bottlenecks when applied to such control problems.

In particular, the Jacobian matrices \(\mathbf{h}_{\mathbf{x}},\mathbf{h}_{\mathbf{u}}\) and the second-order derivative matrices \(V_{\mathbf{x}\mathbf{x}},Q_{\mathbf{x}\mathbf{x}},Q_{\mathbf{u}\mathbf{x}},Q_ {\mathbf{x}\mathbf{u}}\), and \(Q_{\mathbf{u}\mathbf{u}}\) are particularly expensive to compute, store, and perform downstream operations against. For example, in a three-channel \(256\times 256\) image, these matrices naively contain \((256\times 256\times 3)^{2}\approx 39B\) parameters.

In Appendix D.1 we propose and analyze three modifications to the standard iLQR algorithm: **randomized low rank approximations**, **matrix-free evaluations**, and action updates via an **adaptive optimizer**, that significantly reduce runtime and memory constraints while introducing minimal deterioration to performance on inverse problem solving tasks.

## 4 Improved Posterior Sampling

We demonstrate that our optimal control-based sampler overcomes several practical obstacles that plague existing diffusion-based methods for inverse problem solvers.

Brittleness to DiscretizationIn a probabilistic framework, solutions to inverse problems incur a discretization error from the numerical solution of Eq. (8) that decays poorly with the total diffusion steps \(T\) of the diffusion process. While much research has been conducted on the acceleration of unconditional diffusion processes Song et al. (2020); Jolicoeur-Martineau et al. (2021); Karras et al. (2022); Meng et al. (2023), sample quality appears to decay much more aggressively in diffusion-based inverse problem solvers (Figure 3).

We theorize that this is due to two reasons: 1) the posterior sampler Eq. (9) is only correct in the limit of infinitely small time steps, and 2) the quality of the approximated conditional score term \(\nabla_{\mathbf{x}}\log p(\mathbf{y}|\mathbf{x}_{t})\) decays quickly with time (Figure 2), and so fewer timesteps lead to fewer chances at low \(t\) to correct errors made at high \(t\). On the other hand, since optimal control directly casts the **discretized** process as an end-to-end control episode, it produces a feasible solution for any number of discretization steps \(T\).

Intractability of \(\nabla_{\mathbf{x}_{t}}\log p(\mathbf{y}|\mathbf{x}_{t})\)When the forward model \(\mathcal{A}\) is known and \(\eta\) comes from a simple distribution, the conditional likelihood \(p(\mathbf{y}|\mathbf{x}_{t})\) can be derived in closed form for \(t=0\). On the other hand, the dependence of \(y\) on \(\mathbf{x}_{t}\) for \(t>0\) is generally not known without explicitly computing \(\mathbf{x}_{0}\), which requires sampling from the diffusion process. Ultimately, obtaining the conditional score term \(\nabla_{\mathbf{x}_{t}}\log p(\mathbf{y}|\mathbf{x}_{t})\) is a highly nontrivial task Song et al. (2021).

To sidestep this issue, many works Meng and Kabashima (2022); Song et al. (2022); Chung et al. (2023) factorize this term as the integral

\[p(\mathbf{y}|\mathbf{x}_{t})=\int p(\mathbf{y}|\mathbf{x}_{0})p(\mathbf{x}_{0 }|\mathbf{x}_{t})d\mathbf{x}_{0}\] (26)

and then apply a series of approximations to recover a computationally feasible estimate of the conditional score. First, the marginal \(p(\mathbf{x}_{0}|\mathbf{x}_{t})\) is replaced by the marginal conditioned on \(\mathbf{x}_{0}\), i.e.

Figure 4: **Examples from inverse problem tasks on FFHQ \(256\times 256\). From left to right each column contains ground truth, measurement, Diffusion Posterior Sampling (DPS), and ours.**

\(p(\mathbf{x}_{0}|\mathbf{x}_{t},\mathbf{x}_{0})=\mathcal{N}(\mathbf{x}_{0},\sigma^ {2}\mathbf{I})\)Kim and Ye (2021). Next, the \(\mathbf{x}_{0}\)-centered marginal is replaced by the posterior mean \(\mathbb{E}[\mathbf{x}_{0}|\mathbf{x}_{t}]\) given by Tweedie's formula Efron (2011). Finally, the true score is replaced by the learned score network.

While these approximations are necessary in a probabilistic framework, we show that they are not required in our method. Intuitively, this is because the linear quadratic regulator backpropagates the control cost \(\log p(\mathbf{y}|\mathbf{x})\) through a forward trajectory rollout, which naturally computes the true conditional score at each time \(t\). Moreover, our model always estimates \(\mathbf{x}_{0}|\mathbf{x}_{t}\) exactly (up to the discretization error induced by solving Eq. 3), rather than forming an approximation \(\tilde{\mathbf{x}}_{0}\approx\mathbf{x}_{0}\) (Figure 2). We formalize this observation with the following statement.

**Theorem 4.1**.: _Let Eq. 3 be the discretized sampling equation for the diffusion model with **output perturbation mode** control (Eq. 18). Moreover, let the terminal cost_

\[\ell_{0}(\mathbf{x}_{0})=-\log p(\mathbf{y}|\mathbf{x}_{0})\] (27)

_be twice-differentiable and the running costs_

\[\ell_{t}(\mathbf{x}_{t},\mathbf{u}_{t})=0.\] (28)

_Then the iterative linear quadratic regulator with Tikhonov regularizer \(\alpha\) produces the control_

\[\mathbf{u}_{t}=\alpha\nabla_{\mathbf{x}_{t}}\log p(\mathbf{y}|\mathbf{x}_{0}).\] (29)

In other words, by framing the inverse problem as an unconditional diffusion process with controls \(\mathbf{u}_{t}\), our proposed method produces controls that coincide precisely with the desired conditional scores \(\nabla_{\mathbf{x}_{t}}\log p(\mathbf{y}|\mathbf{x}_{0})\).

Let us further assume that \(\log p(\mathbf{y}|\mathbf{x}_{t})=\log p(\mathbf{y}|\mathbf{x}_{0})\), i.e., \(\mathbf{x}_{t}\) contains no additional information about \(y\) than \(\mathbf{x}_{0}\). This assumption results in the posterior mean approximation in Chung et al. (2023) under stochastic dynamics (Eq. 1), where we additionally obtain _exact_ computation of \(\mathbf{x}_{0}\), rather than \(\tilde{\mathbf{x}}_{0}\approx\mathbf{x}_{0}\)via Tweedie's formula Kim and Ye (2021). Under the deterministic ODE dynamics (Eq. 2), we recover the **true posterior sampler** under appropriate choice of Tikhonov regularization constant \(\alpha\).

**Lemma 4.2**.: _Under the deterministic sampler with **output perturbation mode** control, \(\alpha=\frac{1}{g(t)^{2}\Delta t}\) recovers posterior sampling (Eq. 9)._

We demonstrate a similar result with **input mode perturbation**.

**Theorem 4.3**.: _Let Eq. 3 be the discretized sampling equation for the diffusion model with **input perturbation mode** control (Eq. 17). Moreover, let_

\[\ell_{0}(\mathbf{x}_{0})=\log p(\mathbf{y}|\mathbf{x}_{0}),\] (30)

_and the running costs_

\[\ell_{t}(\mathbf{x}_{t},\mathbf{u}_{t})=0.\] (31)

_Then the iterative linear quadratic regulator with Tikhonov regularizer \(\alpha=\frac{1}{g(t)^{2}\Delta t}\) produces the dynamical sytem_

\[\widetilde{\mathbf{x}}_{t}=\widetilde{\mathbf{x}}_{t}+[f(\widetilde {\mathbf{x}}_{t})-\frac{1}{2}g(t)^{2}(\nabla_{\mathbf{x}}\log p_{t}( \widetilde{\mathbf{x}}_{t})\] \[+\nabla_{\mathbf{x}}\log p_{t}(\mathbf{y}|\mathbf{x}_{t}))]\Delta t,\] (32)

_where \(\widetilde{\mathbf{x}}_{t}:=\mathbf{x}_{t}+\mathbf{u}_{t}\)._

Observe that Eq. (32) can be understood as a predictor-corrector sampling method, where the predictor produces an unconditional reverse diffusion update and the corrector produces a conditional correction step on the intermediary variable \(\mathbf{x}_{t}=\tilde{\mathbf{x}}_{t}-\mathbf{u}_{t}\).

Ultimately, these results demonstrate that our proposed method is able to recover the idealized sampling procedure under mild assumptions on the diffusion optimal control algorithm.

Dependence on the Approximate ScoreWhile our theoretical results require that the learned score function \(s_{\theta}(\mathbf{x}_{t},t)\) approximates the true data score \(\log p_{t}(\mathbf{x}_{t},t)\), we emphasize that the performance of our method does not necessitate this condition. In fact, we find that reconstruction performance is theoretically and empirically robust to the accuracy of the approximated prior score \(s_{\theta}(\mathbf{x}_{t},t)\approx\nabla_{\mathbf{x}_{t}}\log p_{t}(\mathbf{x }_{t})\) or conditional score \(\nabla_{\mathbf{x}_{t}}\log p_{t}(\mathbf{y}|\mathbf{x}_{0})\approx\nabla_{ \mathbf{x}_{t}}\log p_{t}(\mathbf{y}|\mathbf{x}_{t})\) terms. This is because the optimal control-based solution is formulated for the optimization of generalized dynamical systems, and thus agnostic to the diffusion sampling process.

Certainly, improved approximation of the score terms result in a better-informed prior and usually higher sample quality. However, we demonstrate that our sampler produces remarkably reasonable solutions even in the case of randomly initialized diffusion models. Conversely, probabilistic posterior samplers can only sample from \(p(\mathbf{y}|\mathbf{x}_{0})\) when the terms composing the posterior sampling equation (Eq. (8)) are well approximated (Figure 6). Modeling errors can occur even in foundation models. For example, this scenario may arise in models trained on regions where there are underrepresented examples in the data. When these arise from existing social or ethical biases, they can further perpetuate or amplify biases to the resulting model if left unaddressedBolukbasi et al. (2016); Birhane et al. (2021); Srivastava et al. (2022).

There exist several methods that seek to alleviate the errors incurred by Tweedie's formula (being a mean approximation of the diffusion process), including Song et al. (2024) which imposes a hard data consistency optimization loop at various points in the diffusion process, and Rout et al. (2023) which includes a stochastic averaging loop in each step of the diffusion process. However, these methods still rely on Tweedie's formula for the error reduction scheme, which assumes access to a ground truth score function. Ultimately, the aforementioned problems in the present section are exacerbated in existing samplers, and relatively less consequential in our solver.

## 5 Related Work

The recent success of diffusion models in image generation Song and Ermon (2019); Ho et al. (2020); Song et al. (2021); Rombach et al. (2022) has spawned a surge of research in deep learning-based solvers to inverse problems. Song et al. (2021) demonstrated a strategy for provably sampling from the solution set \(p(\mathbf{x}|\mathbf{y})\) of a general inverse problem \(\mathbf{y}=\mathcal{A}(\mathbf{x})\) using only an unconditional prior score model \(\nabla_{\mathbf{x}}\log p_{t}(\mathbf{x})\) and a forward probabilistic model \(\log p(\mathbf{y}|\mathbf{x}_{t})\). However, a crucial problem arises in the intractability of forward probabilistic model, which depends on the noisy \(\mathbf{x}_{t}\) rather than the final \(\mathbf{x}_{0}\). This has resulted in a series of approximation algorithms Choi et al. (2021); Kawar et al. (2022); Chung et al. (2022, 2023, 2022); Kawar et al. (2023) for the true conditional diffusion dynamics.

Topics in control theory have been applied to deep learning Liu et al. (2020); Pereira et al. (2020) as well as diffusion modeling Berner et al. (2022). Optimal control can also be connected to diffusion processes via forward-backward SDEs Chen et al. (2021). However, these ideas have not been applied to guided conditional diffusion processes solely at inference time, nor for guided conditional sampling. Our proposed optimal control-based algorithm is, to our knowledge, the first such framework for deep inverse problem solvers.

\begin{table}
\begin{tabular}{c c c c c c c c c c c} \hline \hline  & \multicolumn{2}{c}{SR \(\times\)4} & \multicolumn{2}{c}{Random Impiunite} & \multicolumn{2}{c}{Box Impiunite} & \multicolumn{2}{c}{Gaussian Delburring} & \multicolumn{2}{c}{Motion Delburring} \\ \cline{2-11}  & FID \(\downarrow\) & LPIPS \(\downarrow\) & FID \(\downarrow\) & LPIPS \(\downarrow\) & FID \(\downarrow\) & LPIPS \(\downarrow\) & FID \(\downarrow\) & LPIPS \(\downarrow\) & FID \(\downarrow\) & LPIPS \(\downarrow\) \\ \hline Ours (NFE = 2500) & **32.47** & **0.171** & **15.93** & **0.053** & **20.22** & **0.122** & **31.80** & **0.189** & **39.40** & **0.217** \\ Ours (NFE = 1000) & 37.53 & 0.189 & 20.75 & 0.108 & 23.88 & 0.164 & 35.24 & 0.191 & 45.99 & 0.233 \\ \hline PSLD (NFE = 1000) & 34.28 & 201.21 & 3.049 & 0.431 & 0.167 & 41.53 & 0.221 & - & - \\ Flash-Diffusion*(NFE = _varies_) & - & - & 53.95 & 0.195 & - & - & 65.35 & 0.280 & 64.57 & 0.267 \\ DDNM (NFE = 1000) & 68.94 & 0.328 & 105.38 & 0.802 & 72.28 & 0.483 & 126.0 & 0.995 & - & - \\ DPS (NFE = 1000) & 39.35 & 0.214 & 33.12 & 0.168 & 21.19 & 0.212 & 44.05 & 0.257 & 39.92 & 0.242 \\ DDRM (NFE = 1000) & 62.15 & 0.294 & 42.93 & 0.204 & 69.71 & 0.587 & 74.92 & 0.332 & - & - \\ MCG (NFE = 1000) & 87.64 & 0.520 & 40.11 & 0.309 & 29.26 & 0.286 & 101.2 & 0.340 & 310.5 & 0.702 \\ PP-ADMM & 65.52 & 0.353 & 151.9 & 0.406 & 123.6 & 0.692 & 90.42 & 0.441 & 89.08 & 0.405 \\ Score-SDE (NFE = 1000) & 96.72 & 0.563 & 60.06 & 0.331 & 76.54 & 0.612 & 109.0 & 0.403 & 292.2 & 0.657 \\ ADMM-TV & 110.6 & 0.428 & 68.94 & 0.322 & 181.5 & 0.463 & 186.7 & 0.507 & 152.3 & 0.508 \\ \hline \hline \end{tabular}
\end{table}
Table 1: Quantitative evaluation (FID, LPIPS) of model performance on inverse problems on the FFHQ \(256\)x\(256\)-1K dataset.

## 6 Experiments

Following previous work Chung et al. (2023); Meng and Kabashima (2022); Kawar et al. (2022), we consider five inverse problems. 1) In \(4\times\) image super-resolution, we use the bicubic downsampling operator. 2) In randomized inpainting, we uniformly omit 92% of all pixels (across all channels). 3) In box inpainting, we mask out a \(128\times 128\) block uniformly sampled from a 16 pixel margin from each side of the image, as in Chung et al. (2022). 4) In Gaussian deblurring, we use a kernel of size \(61\times 61\) and standard deviation \(3.0\). In motion deblurring, we generate images according to a library2 of point spread functions with kernel size \(61\times 61\) and intensity \(0.5\). Following the experimental design in Chung et al. (2023), we apply Gaussian noise with standard deviation \(0.05\) to all measurements of the forward model.

Footnote 2: https://github.com/LeviBorodenko/motionblur

We compare against a generalized diffusion inverse sampler (Score-SDE) proposed in Song et al. (2021), Diffusion Posterior Sampling (DPS) Chung et al. (2023), Denoising Diffusion Restoration Models Kawar et al. (2022), Manifold Constrained Gradients (MCG) Chung et al. (2022), as well as two recent latent diffusion-based methods Fabian et al. (2023) (Flash-Diffusion3) and Rout et al. (2024) (PSLD). For non-diffusion baselines, we compare against Plug-and-Play Alternating Direction Method of Multipliers (PnP-ADMM) with neural proximal maps Chan et al. (2016); Zhang et al. (2017), and a total-variation based alternating direction method of multipliers (TV-ADMM) baseline proposed in Chung et al. (2023).

Footnote 3: For Gaussian blur and random inpainting, Flash-Diffusion uses randomly sampled, but less severe degradation operators than our experimental setup.

We validate our results on the high resolution human face dataset FFHQ \(256\times 256\)Karras et al. (2019). Several methods are model agnostic (DPS, DDRM, MCG, and thus evaluated with the same pre-trained diffusion models. To fairly compare between all models, all methods use the model weights from Chung et al. (2023), which are trained on 49K FFHQ images, with 1K images left as a held-out set for evaluation. We compare our algorithm against competing frameworks on these last 1K images. We report our results on FFHQ \(256\times 256\) in Table 1, and demonstrate improvements on all tasks against previous methods. Finally, we demonstrate the performance of our algorithm on the nonlinear inverse problem of class-conditional generation. Namely, let \(\mathcal{A}(\mathbf{x})=\mathtt{classifier}(\mathbf{x})\) and \(p(\mathbf{y}|\mathbf{x})\) be its associated probability. We compare our method to DPS on the inverse task of generating an MNIST digit given a label \(\mathbf{y}\). Compared to images generated by DPS, images from our method exhibit more pronounced class alignment and higher overall sample quality (Figure 5).

## 7 Conclusion

In this paper we presented a novel perspective on tackling inverse problems with diffusion models - framing the discretized reverse diffusion process as a discrete time optimal control episode. We demonstrate that this framework alleviates several core problems in probabilistic solvers: its dependence on the approximation quality of the underlying terms in the diffusion process, its sensitivity to the temporal discretization scheme, its inherent inaccuracy due to the intractability of the conditional score function. We also show that the diffusion posterior sampler can be seen as a specific case of

Figure 5: **Examples from the class-conditional inverse problem.** DPS (left) is compared against ours (right). Each row is a different target MNIST class.

Figure 6: **Robustness to approximation quality of the score function.** We consider the \(4\times\) super-resolution task with a _randomly initialized_ diffusion model. Since the reverse diffusion process is no longer well approximated, DPS cannot produce a feasible solution, while our method still can.

our optimal control-based sampler. Finally, leveraging the improvements granted by our solver, we validate the performance of our algorithm on several inverse problem tasks across several datasets, and demonstrate highly competitive results.

## References

* Berner et al. (2022) Julius Berner, Lorenz Richter, and Karen Ullrich. An optimal control perspective on diffusion-based generative modeling. _arXiv preprint arXiv:2211.01364_, 2022.
* Betts (1998) John T Betts. Survey of numerical methods for trajectory optimization. _Journal of guidance, control, and dynamics_, 21(2):193-207, 1998.
* Birhane et al. (2021) Abeba Birhane, Vinay Uday Prabhu, and Emmanuel Kahembwe. Multimodal datasets: misogyny, pornography, and malignant stereotypes. _arXiv preprint arXiv:2110.01963_, 2021.
* Bolukbasi et al. (2016) Tolga Bolukbasi, Kai-Wei Chang, James Y Zou, Venkatesh Saligrama, and Adam T Kalai. Man is to computer programmer as woman is to homemaker? debiasing word embeddings. _Advances in neural information processing systems_, 29, 2016.
* Chan et al. (2016) Stanley H Chan, Xiran Wang, and Omar A Elgendy. Plug-and-play admm for image restoration: Fixed-point convergence and applications. _IEEE Transactions on Computational Imaging_, 3(1):84-98, 2016.
* Chen et al. (2021) Tianrong Chen, Guan-Horng Liu, and Evangelos A Theodorou. Likelihood training of schr\(\backslash\)" odinger bridge using forward-backward sdes theory. _arXiv preprint arXiv:2110.11291_, 2021.
* Choi et al. (2021) Jooyoung Choi, Sungwon Kim, Yonghyun Jeong, Youngjune Gwon, and Sungroh Yoon. Ivr: Conditioning method for denoising diffusion probabilistic models. _arXiv preprint arXiv:2108.02938_, 2021.
* Chung et al. (2022) Hyungjin Chung, Byeongsu Sim, Dohoon Ryu, and Jong Chul Ye. Improving diffusion models for inverse problems using manifold constraints. _Advances in Neural Information Processing Systems_, 35:25683-25696, 2022.
* Chung et al. (2023a) Hyungjin Chung, Jeongsol Kim, Michael T Mccann, Marc L Klasky, and Jong Chul Ye. Diffusion posterior sampling for general noisy inverse problems. _International Conference on Learning Representations_, 2023a.
* Chung et al. (2023b) Hyungjin Chung, Jeongsol Kim, and Jong Chul Ye. Direct diffusion bridge using data consistency for inverse problems. _arXiv preprint arXiv:2305.19809_, 2023b.
* Dhariwal and Nichol (2021) Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. _Advances in neural information processing systems_, 34:8780-8794, 2021.
* Efron (2011) Bradley Efron. Tweedie's formula and selection bias. _Journal of the American Statistical Association_, 106(496):1602-1614, 2011.
* Fabian et al. (2023) Zalan Fabian, Berk Tinaz, and Mahdi Soltanolkotabi. Adapt and diffuse: Sample-adaptive reconstruction via latent diffusion models. _arXiv preprint arXiv:2309.06642_, 2023.
* Halko et al. (2011) Nathan Halko, Per-Gunnar Martinsson, and Joel A Tropp. Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions. _SIAM review_, 53(2):217-288, 2011.
* Ho and Salimans (2022) Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. _arXiv preprint arXiv:2207.12598_, 2022.
* Ho et al. (2020) Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. _Advances in neural information processing systems_, 33:6840-6851, 2020.
* Houghton et al. (2022) Matthew D Houghton, Alexander B Oshin, Michael J Acheson, Evangelos A Theodorou, and Irene M Gregory. Path planning: Differential dynamic programming and model predictive path integral control on vtol aircraft. In _AIAA SCITECH 2022 Forum_, page 0624, 2022.
* Held et al. (2020)David H Jacobson. New second-order and first-order algorithms for determining optimal control: A differential dynamic programming approach. _Journal of Optimization Theory and Applications_, 2:411-440, 1968.
* Jolicoeur-Martineau et al. (2021) Alexia Jolicoeur-Martineau, Ke Li, Remi Piche-Taillefer, Tal Kachman, and Ioannis Mitliagkas. Gotta go fast when generating data with score-based models. _arXiv preprint arXiv:2105.14080_, 2021.
* Karras et al. (2019) Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial networks. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 4401-4410, 2019.
* Karras et al. (2022) Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models. _Advances in Neural Information Processing Systems_, 35:26565-26577, 2022.
* Kawar et al. (2022) Bahjat Kawar, Michael Elad, Stefano Ermon, and Jiaming Song. Denoising diffusion restoration models. _Advances in Neural Information Processing Systems_, 35:23593-23606, 2022.
* Kawar et al. (2023) Bahjat Kawar, Noam Elata, Tomer Michaeli, and Michael Elad. Gsure-based diffusion model training with corrupted data. _arXiv preprint arXiv:2305.13128_, 2023.
* Kim and Ye (2021) Kwanyoung Kim and Jong Chul Ye. Noise2score: tweedie's approach to self-supervised image denoising without clean images. _Advances in Neural Information Processing Systems_, 34:864-874, 2021.
* Kingma and Ba (2014) Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. _arXiv preprint arXiv:1412.6980_, 2014.
* Knoll and Keyes (2004) Dana A Knoll and David E Keyes. Jacobian-free newton-krylov methods: a survey of approaches and applications. _Journal of Computational Physics_, 193(2):357-397, 2004.
* Li et al. (2024) Henry Li, Ronen Basri, and Yuval Kluger. Likelihood training of cascaded diffusion models via hierarchical volume-preserving maps. In _The Twelfth International Conference on Learning Representations_, 2024. URL https://openreview.net/forum?id=sojpn00o8z.
* Li and Todorov (2004) Weiwei Li and Emanuel Todorov. Iterative linear quadratic regulator design for nonlinear biological movement systems. In _First International Conference on Informatics in Control, Automation and Robotics_, volume 2, pages 222-229. SciTePress, 2004.
* Liu et al. (2020) Guan-Horng Liu, Tianrong Chen, and Evangelos A Theodorou. Ddnpopt: Differential dynamic programming neural optimizer. _arXiv preprint arXiv:2002.08809_, 2020.
* Meng et al. (2023) Chenlin Meng, Robin Rombach, Ruiqi Gao, Diederik Kingma, Stefano Ermon, Jonathan Ho, and Tim Salimans. On distillation of guided diffusion models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 14297-14306, 2023.
* Meng and Kabashima (2022) Xiangming Meng and Yoshiyuki Kabashima. Diffusion model based posterior sampling for noisy linear inverse problems. _arXiv preprint arXiv:2211.12343_, 2022.
* Ongie et al. (2020) Gregory Ongie, Ajil Jalal, Christopher A Metzler, Richard G Baraniuk, Alexandros G Dimakis, and Rebecca Willett. Deep learning techniques for inverse problems in imaging. _IEEE Journal on Selected Areas in Information Theory_, 1(1):39-56, 2020.
* Oymak et al. (2019) Samet Oymak, Zalan Fabian, Mingchen Li, and Mahdi Soltanolkotabi. Generalization guarantees for neural networks via harnessing the low-rank structure of the jacobian. _arXiv preprint arXiv:1906.05392_, 2019.
* Pereira et al. (2020) Marcus Pereira, Ziyi Wang, Tianrong Chen, Emily Reed, and Evangelos Theodorou. Feynman-kac neural network architectures for stochastic control using second-order fbsde theory. In _Learning for Dynamics and Control_, pages 728-738. PMLR, 2020.
* Petersen et al. (2008) Kaare Brandt Petersen, Michael Syskind Pedersen, et al. The matrix cookbook. _Technical University of Denmark_, 7(15):510, 2008.
* Pogogor et al. (2019)Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 10684-10695, 2022.
* Rout et al. (2023) Litu Rout, Yujia Chen, Abhishek Kumar, Constantine Caramanis, Sanjay Shakkottai, and Wen-Sheng Chu. Beyond first-order tweedie: Solving inverse problems using latent diffusion. _arXiv preprint arXiv:2312.00852_, 2023.
* Rout et al. (2024) Litu Rout, Negin Raoof, Giannis Daras, Constantine Caramanis, Alex Dimakis, and Sanjay Shakkottai. Solving linear inverse problems provably via posterior sampling with latent diffusion models. _Advances in Neural Information Processing Systems_, 36, 2024.
* Sagun et al. (2017) Levent Sagun, Utku Evci, V Ugur Guney, Yann Dauphin, and Leon Bottou. Empirical analysis of the hessian of over-parametrized neural networks. _arXiv preprint arXiv:1706.04454_, 2017.
* Saharia et al. (2022) Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. _Advances in Neural Information Processing Systems_, 35:36479-36494, 2022.
* Sasaki et al. (2022) Tomohiro Sasaki, Koki Ho, and E Glenn Lightsey. Nonlinear spacecraft formation flying using constrained differential dynamic programming. In _Proceedings of AAS/AIAA Astrodynamics Specialist Conference_, 2022.
* Song et al. (2024) Bowen Song, Soo Min Kwon, Zecheng Zhang, Xinyu Hu, Qing Qu, and Liyue Shen. Solving inverse problems with latent diffusion models via hard data consistency. _arXiv preprint arXiv:2307.08123_, 2024.
* Song et al. (2020) Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. _arXiv preprint arXiv:2010.02502_, 2020.
* Song et al. (2022) Jiaming Song, Arash Vahdat, Morteza Mardani, and Jan Kautz. Pseudoinverse-guided diffusion models for inverse problems. In _International Conference on Learning Representations_, 2022.
* Song and Ermon (2019) Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. _Advances in neural information processing systems_, 32, 2019.
* Song et al. (2021) Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In _International Conference on Learning Representations_, 2021. URL https://openreview.net/forum?id=PxTIG12RRHS.
* Srivastava et al. (2022) Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adria Garriga-Alonso, et al. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. _arXiv preprint arXiv:2206.04615_, 2022.
* Tassa et al. (2007) Yuval Tassa, Tom Erez, and William Smart. Receding horizon differential dynamic programming. _Advances in neural information processing systems_, 20, 2007.
* Tassa et al. (2014) Yuval Tassa, Nicolas Mansard, and Emo Todorov. Control-limited differential dynamic programming. In _2014 IEEE International Conference on Robotics and Automation (ICRA)_, pages 1168-1175. IEEE, 2014.
* Todorov and Li (2005) Emanuel Todorov and Weiwei Li. A generalized iterative lqg method for locally-optimal feedback control of constrained nonlinear stochastic systems. In _Proceedings of the 2005, American Control Conference, 2005._, pages 300-306. IEEE, 2005.
* Zhang et al. (2017) Kai Zhang, Wangmeng Zuo, Yunjin Chen, Deyu Meng, and Lei Zhang. Beyond a gaussian denoiser: Residual learning of deep cnn for image denoising. _IEEE transactions on image processing_, 26(7):3142-3155, 2017.
* Zhang et al. (2018)

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: We demonstrate our results through rigorous analysis of our algorithm and extensive experiments on multiple inverse problem settings over several datasets. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: Yes, the paper discusses the runtime cost of the work, and provides an equivalent budget analysis, where it still demonstrates competitive performance on each benchmark.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: The paper provides full proofs for all theory in the appendix.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: The paper discloses all hyperparameters and implementation details in the appendix.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: The paper provides open access to the data, which is publicly available. The authors will release code upon acceptance.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: The paper provides all details in the appendix.
7. **Experiment Statistical Significance**Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [NA] Justification: Experiments for other works do not provide error bars, therefore error bars would not benefit the analysis in this paper.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: Experiments can be run on any GPU A4000 or later.
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We confirm to the NeurIPS Code of Ethics in every respect.
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: The paper discusses this in the appendix.
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: The results in this paper paper do not have high risk for misuse.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We credit all creators and original owners of assets.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: No new assets are introduced.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: No research is performed with human subjects.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects**Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained?

Answer: [NA]

Justification: No research is performed with human subjects.

Impact Statement

This paper builds on a large body of existing work and presents an improved technique for solving generic nonlinear inverse problems, which can be seen as a generalization of guided diffusion modeling. Controlling the diffusion process in a generative model has many societal applications, and thus a broad range of downstream impacts. We believe that understanding the capabilities and limitations of such models in a public forum and open community is essential for practical and responsible integration of these technologies with society. However, the ideas presented in this work, as well as any other work in this field, must be deployed with caution to the inherent dangers of these technologies.

## Appendix B Deriving the Iterative Linear Quadratic Regulator (iLQR)

Differential Dynamic Programming (DDP) is a very popular trajectory optimization algorithm that has a rich history of theoretical results Jacobson (1968) as well as successful practical applications in robotics Tassa et al. (2007, 2014), aerospace Houghton et al. (2022), Sasaki et al. (2022) and biomechanics Todorov and Li (2005). It falls under the class of _indirect methods_ for trajectory optimization, wherein Bellman's principle of optimality defines the so-called optimal value-function which in turn can be used to determine the optimal control. This is in contrast to so-called _direct methods_ which cast the problem at hand into a nonlinear constrained optimization problem.

To formulate an optimal control algorithm we first define the state transition function of a dynamical system as

\[\mathbf{x}_{t-1}=\mathbf{h}(\mathbf{x}_{t},\,\mathbf{u}_{t}).\] (33)

The next ingredient that we need for our optimal control approach is a cost function \(J(\mathbf{x}_{t},\,\mathbf{u}_{t})\in\mathbb{R}\). This is used to define a performance criterion that iLQR can optimize with respect to the set of controls \(\{\mathbf{u}_{t}\}_{t=T}^{t=1}\) (i.e., the control trajectory going backwards from time \(t=T\) to \(t=1\)). The cost-function is defined as follows:

\[J_{T}=\sum_{t=T}^{1}\ell_{t}(\mathbf{x}_{t},\,\mathbf{u}_{t})+\ell_{0}( \mathbf{x}_{0}),\] (34)

where, \(\ell_{t}\) and \(\ell_{0}\) are scalar-valued functions which are commonly referred to as the _running_ cost-function and the _terminal_ cost-function respectively.

To obtain the sequence of optimal controls, we employ the dynamic programming principle. To do so, we first introduce the notion of the Value-function defined as follows:

\[V(\mathbf{x}_{t},\,t)=\min_{\{\mathbf{u}_{n}\}_{n=t}^{n=1}}J_{t}=\min_{\{ \mathbf{u}_{n}\}_{n=t}^{n=1}}\big{[}\sum_{n=t}^{1}\ell_{n}(\mathbf{x}_{n},\, \mathbf{u}_{n})+\ell_{0}(\mathbf{x}_{0})\big{]}\] (35)

Intuitively, the Value-function resembles the _optimal cost-to-go_ starting from time step \(t\) and state \(\mathbf{x}_{t}\) until the end of the time horizon (i.e., \(t=0\)). Using this definition, one can easily derive the following recursive relation also known as _Bellman's Principle of Optimality_:

\[V(\mathbf{x}_{t},\,t)=\min_{\mathbf{u}_{t}}\Big{[}\ell_{t}(\mathbf{x}_{t},\, \mathbf{u}_{t})+V(\mathbf{x}_{t-1},\,t-1)\Big{]}.\] (36)

A often useful defintion used in the derivation of the iLQR Riccati equations is that of the State-Action Value-Function \(Q(\mathbf{x}_{t},\,\mathbf{u}_{t})\) given by,

\[Q(\mathbf{x}_{t},\,\mathbf{u}_{t})=\ell_{t}(\mathbf{x}_{t},\, \mathbf{u}_{t})+V(\mathbf{x}_{t-1},\,t-1)\] (37) \[\text{Therefore, }V(\mathbf{x}_{t},\,t)=\min_{\mathbf{u}_{t}}Q( \mathbf{x}_{t},\,\mathbf{u}_{t})\] (38)

A sketch of the derivation of the Riccati equations is as follows: we take second-order Taylor expansions of both \(Q(\mathbf{x}_{t},\,\mathbf{u}_{t})\) and \(V(\mathbf{x}_{t},\,t)\) around _nominal_ state and action trajectories of \(\{\tilde{\mathbf{x}}_{t}\}_{t=T}^{t=0}\) and \(\{\tilde{\mathbf{u}}_{t}\}_{t=T}^{t=1}\) respectively. Next, we substitute these into Eq.(37) and equate the first- and second order terms to yield the following relations between the derivatives of \(Q,\,\ell\) and \(V\):

\[Q_{\mathbf{x}} =\ell_{\mathbf{x}}+\mathbf{h}_{\mathbf{x}}^{T}V_{\mathbf{x}}^{ \prime}\] (39) \[Q_{\mathbf{u}} =\ell_{\mathbf{u}}+\mathbf{h}_{\mathbf{u}}^{T}V_{\mathbf{x}}^{ \prime}\] (40) \[Q_{\mathbf{x}\mathbf{x}} =\ell_{\mathbf{x}\mathbf{x}}+\mathbf{h}_{\mathbf{x}}^{T}V_{ \mathbf{x}\mathbf{x}}^{\prime}\mathbf{h}_{\mathbf{x}}\] (41) \[Q_{\mathbf{x}\mathbf{u}} =\ell_{\mathbf{x}\mathbf{u}}+\mathbf{h}_{\mathbf{x}}^{T}V_{ \mathbf{x}\mathbf{x}}^{\prime}\mathbf{h}_{\mathbf{u}}\] (42) \[Q_{\mathbf{u}\mathbf{x}} =\ell_{\mathbf{u}\mathbf{x}}+\mathbf{h}_{\mathbf{u}}^{T}V_{ \mathbf{x}\mathbf{x}}^{\prime}\mathbf{h}_{\mathbf{x}}\] (43) \[Q_{\mathbf{u}\mathbf{u}} =\ell_{\mathbf{u}\mathbf{u}}+\mathbf{h}_{\mathbf{u}}^{T}V_{ \mathbf{x}\mathbf{x}}^{\prime}\mathbf{h}_{\mathbf{u}},\] (44)

where \(\mathbf{h}_{\mathbf{x}_{t}}\) and \(\mathbf{h}_{\mathbf{u}_{t}}\) are the Jacobians of the dynamics function \(\mathbf{h}(\mathbf{x}_{t},\,\mathbf{u}_{t})\), evaluated at time step \(t\), w.r.t the state and the control vectors respectively. For ease of notation, we have dropped the subscript \(t\) and therefore all derivatives above should be considered to be evaluated at time step \(t\), while we use \(V_{\mathbf{x}}^{\prime}\) and \(V_{\mathbf{x}\mathbf{x}}^{\prime}\) above to indicate the gradient and hessian of the Value-function evaluated at the next time step (i.e., at time step \(t-1\)).

Next, we substitute for the second-order approximation of \(Q(\mathbf{x}_{t},\,\mathbf{u}_{t})\) into Eq. (38) and note that \(\mathbf{u}_{t}\) can be written in terms of the nominal control as follows:

\[\mathbf{u}_{t}=\bar{\mathbf{u}}_{t}+\delta\mathbf{u}_{t}.\]

This results in a quadratic objective w.r.t \(\delta\mathbf{u}_{t}\) and the minimization in Eq. (38) can be performed exactly resulting in the following optimal perturbation from the nominal control trajectory:

\[\delta\mathbf{u}_{t}^{*}=\mathbf{k}_{t}+\mathbf{K}_{t}\delta\mathbf{x}_{t}\] (45)

where, the feedforward and feedback gains are given by the following expressions:

\[\mathbf{k} =-Q_{\mathbf{u}1}^{-1}Q_{\mathbf{u}}\] (46) \[\mathbf{K} =-Q_{\mathbf{u}\mathbf{u}}^{-1}Q_{\mathbf{u}\mathbf{x}}\] (47)

Finally, by substituting for the optimal \(\delta\mathbf{u}_{t}^{*}\) back into Eq.(38), we can drop the \(\min\) operator and equate the first- and second-order terms on both sides. This results the following Riccati equations:

\[V_{\mathbf{x}} =Q_{\mathbf{x}}-\mathbf{K}^{T}Q_{\mathbf{u}\mathbf{u}}\mathbf{k}\] (48) \[V_{\mathbf{x}\mathbf{x}} =Q_{\mathbf{x}\mathbf{x}}-\mathbf{K}^{T}Q_{\mathbf{u}\mathbf{u}} \mathbf{K}.\] (49)

This concludes the sketch derivation of the Riccati equations. The algorithm roughly proceeds as follows:

1. We start with an initial guess of the the nominal control trajectory \(\{\bar{\mathbf{u}}_{t}\}_{t=T}^{1}\) and generate the corresponding nominal state trajectory \(\{\bar{\mathbf{x}}_{t}\}_{t=T}^{0}\) using \(\mathbf{x}_{t}=\mathbf{h}(\mathbf{x}_{t+1},\mathbf{u}_{t+1})\).
2. By noticing from Eq. (35) that \(V(\mathbf{x}_{0},\,0)=\ell(\mathbf{x}_{0})\) we can obtain expressions for \(V_{\mathbf{x}}\) and \(V_{\mathbf{x}\mathbf{x}}\) evaluated at \(\bar{\mathbf{x}}_{0}\).
3. Next, we compute the derivatives of \(Q\) given by equations. (39)-(44) using \(\{\bar{\mathbf{u}}_{t}\}_{t=T}^{1}\) and \(\{\bar{\mathbf{x}}_{t}\}_{t=T}^{1}\).
4. Using the derivatives of \(Q\), we can compute the feedforward and feedback gains using equations (46)-(47).
5. Finally, using the Riccati equations (48)-(49), we can propagate both \(V_{\mathbf{x}}\) and \(V_{\mathbf{x}\mathbf{x}}\) one step backwards in time.
6. We then repeat the steps \(3,\,4\) and \(5\) until we backpropagate the derivatives of \(V\) to time step \(t=T\).
7. This completes one iteration of iLQR. At the end of each iteration the gains are used to produce the updated nominal control trajectory as follows: \[\bar{\mathbf{u}}_{t}^{*}=\bar{\mathbf{u}}_{t}+\alpha\mathbf{k}+\mathbf{K}( \bar{\mathbf{x}}_{t}-\mathbf{x}_{t})\] (50) where, \(\mathbf{x}_{t}\) is the state obtained by unrolling the dynamics subject to the updated controls: \[\mathbf{x}_{t}=\mathbf{h}(\mathbf{x}_{t+1},\bar{\mathbf{u}}_{t+1}^{*}).\]
8. The new nominal control trajectory \(\bar{\mathbf{u}}_{t}^{*}\) is used to produce a new nominal state trajectory \(\bar{\mathbf{x}}_{t}^{*}\) and the algorithm is repeated from step 2 onwards until convergence or a fixed number of iterations.

Proofs

**Theorem 4.1**.: _Let Eq. 3 be the discretized sampling equation for the diffusion model with **output perturbation mode** control (Eq. 18). Moreover, let the terminal cost_

\[\ell_{0}(\mathbf{x}_{0})=-\log p(\mathbf{y}|\mathbf{x}_{0})\] (27)

_be twice-differentiable and the running costs_

\[\ell_{t}(\mathbf{x}_{t},\mathbf{u}_{t})=0.\] (28)

_Then the iterative linear quadratic regulator with Tikhonov regularizer \(\alpha\) produces the control_

\[\mathbf{u}_{t}=\alpha\nabla_{\mathbf{x}_{t}}\log p(\mathbf{y}|\mathbf{x}_{0}).\] (29)

Proof.: We demonstrate the result via induction for \(t=1,\dots,T\).

Since we assume that \(\ell_{\mathbf{u}\mathbf{u}}=\mathbf{0}\), \(V_{\mathbf{x}\mathbf{x}}\) vanishes:

\[V_{\mathbf{x}\mathbf{x}} =Q_{\mathbf{x}\mathbf{x}}-Q_{\mathbf{u}\mathbf{x}}^{T}Q_{ \mathbf{u}\mathbf{u}}^{-1}Q_{\mathbf{u}\mathbf{x}}\] (51) \[=h_{\mathbf{x}}^{T}V_{\mathbf{x}\mathbf{x}}^{\prime}h_{\mathbf{x }}-h_{\mathbf{x}}^{T}V_{\mathbf{x}\mathbf{x}}^{\prime}(V_{\mathbf{x}\mathbf{x }}^{\prime})^{-1}V_{\mathbf{x}\mathbf{x}}^{\prime}h_{\mathbf{x}}\] (52) \[=\mathbf{0}.\] (53)

Similarly, \(V_{\mathbf{x}}\) also greatly simplifies as

\[V_{\mathbf{x}} =Q_{\mathbf{x}}+Q_{\mathbf{u}\mathbf{x}}^{T}Q_{\mathbf{u}\mathbf{ u}}^{-1}Q_{\mathbf{u}}\] (54) \[=h_{\mathbf{x}}^{T}V_{\mathbf{x}}^{\prime}+h_{\mathbf{x}}^{T}V_{ \mathbf{x}\mathbf{x}}^{\prime}(V_{\mathbf{x}\mathbf{x}}^{\prime})^{-1}V_{ \mathbf{x}}^{\prime}\] (55) \[=h_{\mathbf{x}}^{T}V_{\mathbf{x}}^{\prime}.\] (56)

Turning to the Tikhonov regularized feedforward term,

\[\mathbf{k} =-Q_{\mathbf{u}\mathbf{u}}^{-1}Q_{\mathbf{u}}\] (57) \[=-(h_{\mathbf{x}}^{T}\underbrace{V_{\mathbf{x}\mathbf{x}}}_{ \mathbf{0}}h_{\mathbf{x}}+\alpha\mathbf{I})^{-1}Q_{\mathbf{u}}\] (58) \[=-(\mathbf{0}+\alpha\mathbf{I})^{-1}Q_{\mathbf{u}}\] (59) \[=-\frac{1}{\alpha}V_{\mathbf{x}}^{\prime}.\] (60)

Finally, the feedback term disappears due to the vanishing \(V_{\mathbf{x}\mathbf{x}}\)

\[\mathbf{K} =-Q_{\mathbf{u}\mathbf{u}}^{-1}Q_{\mathbf{u}\mathbf{x}}\] (61) \[=\mathbf{0}.\] (62)

Explicitly denoting the dependence of \(V_{\mathbf{x}}\) and \(V_{\mathbf{x}}^{\prime}\) on \(t\), we can rewrite Eq. 56 as

\[V_{\mathbf{x}}^{(t)} =h_{\mathbf{x}}^{T}V_{\mathbf{x}}^{(t-1)}\] \[=\frac{\partial\mathbf{x}_{t-1}}{\partial\mathbf{x}_{t}}\frac{ \partial}{\partial\mathbf{x}_{t-1}}V.\]

Combining this observation with the fact that \(\ell_{0}=-\log p(\mathbf{y}|\mathbf{x}_{0})\), we can conclude that

\[V_{\mathbf{x}}^{(t)}=-\nabla_{\mathbf{x}_{t}}\log p(\mathbf{y}|\mathbf{x}_{0}),\] (63)

where \(\mathbf{x}_{0}\) depends on \(\mathbf{x}_{t}\) via the state transition function \(\mathbf{h}\) (Eq. 18). Therefore, we have that

\[\mathbf{k} =-\frac{1}{\alpha}V_{\mathbf{x}}^{\prime}\] \[=\frac{1}{\alpha}\nabla_{\mathbf{x}_{t}}\log p(\mathbf{y}| \mathbf{x}_{0})\] \[\mathbf{K} =0.\]

Finally, given our action update (Eq. 15), we can conclude our desired result

\[\mathbf{u}_{t} =\frac{1}{\alpha}\nabla_{\mathbf{x}_{t}}\log p(\mathbf{y}|\mathbf{x}_ {0}).\] (64)

**Lemma C.1**.: _Under the deterministic sampler with **output perturbation mode** control, \(\alpha=\frac{1}{g(t)^{2}\Delta t}\) recovers posterior sampling (Eq. 9)._

Proof.: Substituting in \(\alpha=\frac{1}{g(t)^{2}\Delta t}\) to Eq. 29, we observe that Eq. 18 can now be written as

\[\mathbf{x}_{t-1}=[f(\mathbf{x}_{t})-\frac{1}{2}g(t)^{2}(\nabla_{\mathbf{x}_{t} }\log p_{t}(\mathbf{x}_{t})+\nabla_{\mathbf{x}_{t}}\log p_{t}(\mathbf{y}| \mathbf{x}_{0}))]\Delta t.\] (65)

Under the determinstic sampler, we can conclude that \(\log p_{t}(\mathbf{y}|\mathbf{x}_{0})=\log p_{t}(\mathbf{y}|\mathbf{x}_{t})\), since each \(\mathbf{x}_{t}\) has a _unique_ path through the sample space. Therefore, we conclude that Eq. 65 resembles the ideal posterior sampler equation 9. We conclude our proof. 

**Theorem 4.3**.: _Let Eq. 3 be the discretized sampling equation for the diffusion model with **input perturbation mode** control (Eq. 17). Moreover, let_

\[\ell_{0}(\mathbf{x}_{0})=\log p(\mathbf{y}|\mathbf{x}_{0}),\] (30)

_and the running costs_

\[\ell_{t}(\mathbf{x}_{t},\mathbf{u}_{t})=0.\] (31)

_Then the iterative linear quadratic regulator with Tikhonov regularizer \(\alpha=\frac{1}{g(t)^{2}\Delta t}\) produces the dynamical sytem_

\[\widetilde{\mathbf{x}}_{t}=\widetilde{\mathbf{x}}_{t}+[f( \widetilde{\mathbf{x}}_{t})-\frac{1}{2}g(t)^{2}(\nabla_{\mathbf{x}}\log p_{t }(\widetilde{\mathbf{x}}_{t})\] \[+\nabla_{\mathbf{x}}\log p_{t}(\mathbf{y}|\mathbf{x}_{t}))]\Delta t,\] (32)

_where \(\widetilde{\mathbf{x}}_{t}:=\mathbf{x}_{t}+\mathbf{u}_{t}\)._

Proof.: We similarly demonstrate the result via induction for \(t=1,\dots,T\).

Again, assuming that \(\ell_{\mathbf{u}\mathbf{u}}=0\), \(V_{\mathbf{x}\mathbf{x}}\) vanishes:

\[V_{\mathbf{x}\mathbf{x}} =Q_{\mathbf{x}\mathbf{x}}-Q_{\mathbf{u}\mathbf{x}}^{T}Q_{\mathbf{ u}\mathbf{u}}^{-1}Q_{\mathbf{u}\mathbf{x}}\] (66) \[=Q_{\mathbf{x}\mathbf{x}}-Q_{\mathbf{x}\mathbf{x}}(\underbrace{ \ell_{\mathbf{u}\mathbf{u}}}_{=\mathbf{0}}+Q_{\mathbf{x}\mathbf{x}})^{-1}Q_{ \mathbf{x}\mathbf{x}}\] (67) \[=\mathbf{0},\] (68)

whereas \(V_{\mathbf{x}}\) greatly simplifies as

\[V_{\mathbf{x}} =Q_{\mathbf{x}}+Q_{\mathbf{u}\mathbf{x}}^{T}Q_{\mathbf{u}\mathbf{ u}}^{-1}Q_{\mathbf{u}}\] (69) \[=h_{\mathbf{x}}^{T}V_{\mathbf{x}}^{\prime}.\] (70)

Turning to the feedforward and feedback terms, we have

\[\mathbf{k} =-Q_{\mathbf{u}\mathbf{u}}^{-1}Q_{\mathbf{u}}\] (71) \[=-(h_{\mathbf{x}}^{T}\underbrace{V_{\mathbf{x}\mathbf{x}}}_{ \mathbf{0}}h_{\mathbf{x}}+\alpha\mathbf{I})^{-1}Q_{\mathbf{u}}\] (72) \[=-(\mathbf{0}+\alpha\mathbf{I})^{-1}Q_{\mathbf{u}}\] (73) \[=-\frac{1}{\alpha}h_{\mathbf{x}}^{T}V_{\mathbf{x}}^{\prime},\] (74)

and

\[\mathbf{K} =-Q_{\mathbf{u}\mathbf{u}}^{-1}Q_{\mathbf{u}\mathbf{x}}\] (75) \[=\mathbf{0}.\] (76)

We observe that

\[V_{\mathbf{x}}^{(t)}=-\frac{1}{\alpha}h_{\mathbf{x}}^{T}V_{\mathbf{x}}^{(t-1)}.\]Therefore, noting that \(V_{\mathbf{x}}^{(0)}=\log p(\mathbf{y}|\mathbf{x}_{0})\), we have

\[\mathbf{k} =-V_{\mathbf{x}}^{(t)}\] \[=-\frac{1}{\alpha}(h_{\mathbf{x}}^{(t)})^{T}V_{\mathbf{x}}^{(t-1)}\] \[=-\frac{1}{\alpha}\nabla_{\mathbf{x}_{t}}\log p(\mathbf{y}| \mathbf{x}_{0})\] \[=-\frac{1}{\alpha}\nabla_{\mathbf{x}_{t}}\log p(\mathbf{y}| \mathbf{x}_{0}(\mathbf{x}_{t})).\]

Applying the feedforward terms to the diffusion sampling process, we have

\[\mathbf{x}_{t-1} =(\mathbf{x}_{t}+\mathbf{u}_{t})+[f(\mathbf{x}_{t}+\mathbf{u}_{ t})\] \[\quad-\frac{1}{2}g(t)^{2}\nabla_{\mathbf{x}}\log p_{t}(\mathbf{x }_{t}+\mathbf{u}_{t})]\Delta t.\]

We define the intermediary variable

\[\widetilde{\mathbf{x}}_{t}=\mathbf{x}_{t}+\mathbf{u}_{t},\] (77)

which has dynamics

\[\widetilde{\mathbf{x}}_{t}=\widetilde{\mathbf{x}}_{t}+[f(\widetilde{\mathbf{x }}_{t})-\frac{1}{2}g(t)^{2}\nabla_{\mathbf{x}}\log p_{t}(\widetilde{\mathbf{x }}_{t})]\Delta t+\mathbf{u}_{t}.\] (78)

We now can see that, letting \(\alpha=\Delta tg(t)^{2}\), we obtain

\[\widetilde{\mathbf{x}}_{t}=\widetilde{\mathbf{x}}_{t}+[f(\widetilde{\mathbf{x }}_{t})-\frac{1}{2}g(t)^{2}(\nabla_{\mathbf{x}}\log p_{t}(\widetilde{\mathbf{x }}_{t})+\nabla_{\mathbf{x}}\log p_{t}(\mathbf{y}|\mathbf{x}_{0}))]\Delta t.\]

## Appendix D Implementation

For all experiments, we use publicly available datasets and pre-trained model weights. For the FFHQ \(256\times 256\) experiments, we use the last 1K images of the dataset for evaluation. For MNIST, we do not use images directly in the inverse classification task. The images were only used for training the pretrained diffusion model.

For models, we used the pretrained weights from Chung et al. (2023) for FFHQ \(256\times 256\) tasks, and the Hugging Face laurent/mnist-28 diffusion model for MNIST experiments. No further training is performed on any models. Further hyperparameters can be found in Table 2. For the classifier \(p(\mathbf{y}|\mathbf{x})\) in MNIST class-guided classification, we use a simple convolutional neural network with two convolutional layers and two MLP layers, trained on the entire MNIST dataset.

### High Dimensional Control

To speed up our proposed method, we leverage the following three modifications to the standard iLQR algorithm.

\begin{table}
\begin{tabular}{c c c c c c} \hline \hline  & SR \(\times 4\) & Random Inpainting & Box Inpainting & Gaussian Deblurring & Motion Deblurring \\ \hline \(\mathbb{T}\) & 50 & 50 & 50 & 50 & 50 \\ \hline num\_iters & 50 & 100 & 100 & 100 & 100 \\ \hline step\_size & \(1e-3\) & \(1e-3\) & \(1e-3\) & \(1e-3\) & \(1e-3\) \\ \hline \(\ell_{0}(\mathbf{x}_{0})\) & \(||\mathcal{A}(\mathbf{x}_{0})-\mathbf{y}||\) & \(||\mathcal{A}(\mathbf{x}_{0})-\mathbf{y}||\) & \(||\mathcal{A}(\mathbf{x}_{0})-\mathbf{y}||\) & \(||\mathcal{A}(\mathbf{x}_{0})-\mathbf{y}||\) & \(||\mathcal{A}(\mathbf{x}_{0})-\mathbf{y}||\) \\ \hline \(\alpha\) & \(1e-4\) & \(1e-4\) & \(1e-4\) & \(1e-4\) & \(1e-4\) \\ \hline \(\ell_{t}(\mathbf{x}_{t},\mathbf{u}_{t})\) & \(\alpha||\mathbf{u}_{t}||\) & \(\alpha||\mathbf{u}_{t}||\) & \(\alpha||\mathbf{u}_{t}||\) & \(\alpha||\mathbf{u}_{t}||\) \\ \hline \(k\) & \(1\) & \(1\) & \(1\) & \(1\) & \(1\) \\ \hline control\_mode & input mode & input mode & input mode & input mode & input mode \\ \hline \hline \end{tabular}
\end{table}
Table 2: Hyperparameters for FFHQ experiments.

Randomized Low-Rank ApproximationThe first and second order terms in Eqs. (19-25) are corresponding Taylor expansions of deep neural functions. Even with the use of automatic differentiation libraries, the formation of these matrices is incredibly expensive, requiring at least \(\dim(\mathbf{x})\) backpropagation passes (where \(\dim(\mathbf{x})\approx 39B\) in some experiments). To reduce the cost of computing these matrices, we utilize their known low rank structure Sagun et al. (2017); Oymak et al. (2019).

Leveraging advanced techniques in randomized numerical linear algebra, we estimate Eqs. (19-25) using randomized SVD Halko et al. (2011). For any matrix \(\mathbf{A}\in\mathbb{R}^{m\times n}\) this is a four step process. 1) We sample a random matrix \(\mathbf{\Omega}\sim\mathcal{N}(\mathbf{0},\mathbf{I}_{n\times k})\). 2) We obtain \(\mathbf{A}\mathbf{\Omega}=\mathbf{Y}\in\mathbb{R}^{m\times k}\). 3) We form a basis over the columns of \(\mathbf{Y}\), e.g. by taking the \(\mathbf{Q}\) matrix in a \(\mathbf{Q}\mathbf{R}\) factorization \(\mathbf{Q}\mathbf{R}=\mathbf{Y}\). 4) We approximate \(\mathbf{A}\approx\mathbf{Q}^{T}\mathbf{Q}\mathbf{A}\).

Notably, we observe that when \(\mathbf{A}\) is a Jacobian (or Hessian) matrix, it can be approximated purely through Jacobian-vector and vector-Jacobian (Hessian-vector and vector-Hessian, resp.) products -- _without ever materializing \(\mathbf{A}\) itself_. Moreover, a key result in randomized linear algebra is that this algorithm can approximate \(\mathbf{A}\) up to accuracy \(\mathcal{O}(mnk\sigma_{k+1})\) (Theorem 1.1 in Halko et al. (2011)). Notably, if \(\mathbf{A}\) has low rank structure where \(\exists k\) such that the \(k+1\)th singular value \(\sigma_{k+1}=0\), then the approximation is exact.

Matrix-Free EvaluationInspired by matrix-free techniques in numerical optimization Knoll and Keyes (2004), we demonstrate a strategy for forming the action update (15) without materializing the costly \(\dim(\mathbf{x})\times\dim(\mathbf{x})\) matrices in the iLQR algorithm (19-25), which we shall denote as an

\begin{table}
\begin{tabular}{l c c c c c c c c c} \hline \hline  & \multicolumn{4}{c}{SR \(\times 4\)} & \multicolumn{4}{c}{Random Inpainting} \\ \cline{2-11}  & LPIPS \(\downarrow\) & PSNR \(\uparrow\) & SSIM \(\uparrow\) & MSE \(\downarrow\) & LPIPS \(\downarrow\) & PSNR \(\uparrow\) & SSIM \(\uparrow\) & MSE \(\downarrow\) \\ \hline \(k=0\) & 0.254 & 24.00 & 0.691 & 141.2 & 0.121 & 28.33 & 0.755 & 56.74 \\ \(k=1\) & 0.171 & 27.45 & 0.792 & 117.0 & 0.053 & 31.84 & 0.882 & 42.57 \\ \(k=4\) & 0.171 & 27.47 & 0.794 & 116.4 & 0.052 & 31.99 & 0.883 & 41.12 \\ \(k=16\) & 0.170 & 27.43 & 0.799 & 117.5 & 0.050 & 32.12 & 0.891 & 39.90 \\ \hline \hline \end{tabular}
\end{table}
Table 4: Ablative study on the effect of _rank_ in the low rank and matrix-free approximations on performance (LPIPS, PSNR, SSIM, NMSE) of our proposed model on the FFHQ \(256\)x\(256\)-1K dataset dataset.

\begin{table}
\begin{tabular}{l c c c c c c c c c c c c c} \hline \hline  & \multicolumn{4}{c}{SR \(\times 4\)} & \multicolumn{4}{c}{Random Inpainting} \\ \cline{2-11}  & PSNR \(\uparrow\) & SSIM \(\uparrow\) & SSIM \(\uparrow\) & MSE \(\downarrow\) & LPIPS \(\downarrow\) & PSNR \(\uparrow\) & SSIM \(\uparrow\) & MSE \(\downarrow\) & PSNR \(\uparrow\) & SSIM \(\uparrow\) & MSE \(\downarrow\) \\ \hline Ours (T = 50) & **27.45** & 0.792 & **117.0** & **31.84** & **0.882** & **42.57** & **53.3** & 0.804 & **190.6** & **24.99** & **0.694** & **206.4** & **26.4** & **0.721** & **201.9** \\ \hline DFS(T = 1000) & 25.67 & 0.852 & 176.2 & 22.47 & 0.873 & 36.22 & 25.23 & **0.851** & 196.9 & 24.25 & **0.811** & 24.4 & 24.29 & **0.859** & 209.4 \\ DODM (1 = 1000) & 25.36 & 0.855 & 198.9 & 22.34 & 0.859 & 38.25 & 9.19 & 0.319 & 28.53 & 23.56 & 0.787 & 100.00 & 4.72 & 0.655 & 1183.8 \\ SAGD (T = 1000) & 26.03 & **0.859** & 44.28 & 19.57 & 0.623 & 46.43 & 12.17 & 0.335 & 0.72 & 0.831 & 20.78 & 4.68 & 0.325 & 222.29 \\ PPS-ADMM & 26.53 & **0.845** & 143.19 & 11.65 & 0.642 & 44.44 & 3.41 & 0.427 & 29.15 & 7.12 & 0.409 & 126.23 & 0.85 & 0.325 & 222.29 \\ Source,SDM (T = 1000) & 17.62 & 0.817 & 12.44 & 15.81 & 0.648 & 13.23 & 0.847 & 29.15 & 7.12 & 0.409 & 126.23 & 0.85 & 10.32 & 129.1 \\ ADMM-TV & 23.86 & 0.803 & 287.4 & 17.81 & 0.814 & 107.06 & 22.03 & 0.784 & 400.75 & 22.37 & 0.891 & 379.8 & 21.36 & 0.758 & 473.4 \\ \hline \hline \end{tabular}
\end{table}
Table 3: Quantitative evaluation (PSNR, SSIM, MSE) of performance on inverse problems on the FFHQ \(256\)x\(256\)-1K dataset.

\begin{table}
\begin{tabular}{l c c c c c c c c c} \hline \hline  & \multicolumn{4}{c}{SR \(\times 4\)} & \multicolumn{4}{c}{Random Inpainting} \\ \cline{2-11}  & LPIPS \(\downarrow\) & PSNR \(\uparrow\) & SSIM \(\uparrow\) & MSE \(\downarrow\) & LPIPS \(\downarrow\) & PSNR \(\uparrow\) & SSIM \(\uparrow\) & MSE \(\downarrow\) \\ \hline \(\alpha=0\) & - & - & - & - & - & - & - & - & - \\ \(\alpha=1e-7\) & 0.173 & 27.49 & 0.794 & 115.9 & 0.050 & 31.80 & 0.879 & 42.96 \\ \(\alpha=1e-4\) & 0.171 & 27.45 & 0.792 & 117.0 & 0.053 & 31.84 & 0.882 & 42.57 \\ \(\alpha=1\) & 0.172 & 27.43 & 0.799 & 117.5 & 0.050 & 31.85 & 0.891 & 42.47 \\ \(\alpha\) from Lemma 4.2 & 0.170 & 27.44 & 0.788 & 117.3 & 0.051 & 31.86 & 0.880 & 42.44 \\ \hline \hline \end{tabular}
\end{table}
Table 5: Ablative study on the effect of the Tikhonov regularization coefficient \(\alpha\) on performance (LPIPS, PSNR, SSIM, NMSE) of our proposed model on the FFHQ \(256\)x\(256\)-1K dataset dataset. No results are reported for \(\alpha=0\), as the algorithm encountered numerical precision errors during matrix inversion.

indexed set of matrices \(\{\mathbf{A}_{i}\}\). We do this by forming projections of each \(\mathbf{A}_{i}\) against a corresponding set of \(\dim(\mathbf{x})\times\ell\) column-orthogonal matrices \(\{\mathbf{Q}_{i}\}\), which we denote as \(\mathbf{B}_{i}:=\mathbf{Q}_{i}^{T}\mathbf{A}_{i}\). These matrices can then be stored at reduced cost as \((\mathbf{Q}_{i},\mathbf{B}_{i})\) pairs.

Matrix multiplications between any \(\mathbf{A}_{i}\mathbf{A}_{j}\) can then be approximated up to rank \(\ell\) with respect to the projected matrix, \(\mathbf{Q}_{i}\mathbf{A}_{i,\mathbf{Q}_{i}}\), i.e.

\[\mathbf{A}_{i}\mathbf{A}_{j}\approx\mathbf{Q}_{i}\mathbf{B}_{i}\mathbf{Q}_{j} ^{T}\mathbf{B}_{j}.\] (79)

However, to prevent materialization of the full size of any matrices, we drop the leading \(\mathbf{Q}_{i}\), obtaining a new projected-matrix pair \((\mathbf{Q}_{\mathbf{k}},\mathbf{B}_{\mathbf{k}})\), where \(\mathbf{Q}_{\mathbf{k}}=\mathbf{Q}_{i}\).

Adam OptimizerFinally, we precondition gradients via the Adam optimizer Kingma and Ba (2014) before applying the feedback gains, rather than applying a backtracking line search Tassa et al. (2014), resulting in the action update

\[\mathbf{u}_{t}=\mathbf{P}\mathbf{k}_{t}+\mathbf{K}_{t}(\mathbf{x}_{t}- \mathbf{x}_{t}^{\prime}),\] (80)

where \(\mathbf{P}\) is the preconditioning matrix produced by the Adam optimizer. This reduces the overall runtime of the algorithm while still accounting for second-order information that respects the nonlinearity of the optimization landscape.

### Computational Complexity Analysis

Incorporating all three modifications, we can provide a realistic runtime and space complexity analysis of our presented algorithm with respect to the rank \(k\), the data dimension \(d\), diffusion steps \(m\), and number of iLQR iterations \(n\).

Combining both the low rank and matrix-free approximations, we obtain the updated equations for input mode perturbation (where projection matrices are written as \(\mathbf{P}\) to avoid overloading the \(Q\) function notation):

\[Q_{\mathbf{x}} =\mathbf{h}_{\mathbf{x}}^{T}V_{\mathbf{x}}^{\prime}\] (81) \[Q_{\mathbf{u}} =\ell_{\mathbf{u}}+\mathbf{h}_{\mathbf{x}}^{T}V_{\mathbf{x}}^{ \prime}\] (82) \[\mathbf{P}Q_{\mathbf{x}\mathbf{x}}\mathbf{P}^{T}=\mathbf{P}Q_{ \mathbf{u}\mathbf{x}}\mathbf{P}^{T}=\mathbf{P}Q_{\mathbf{x}\mathbf{u}} \mathbf{P}^{T}=\mathbf{P}\mathbf{h}_{\mathbf{x}}^{T}V_{\mathbf{x}\mathbf{x}}^ {\prime}\mathbf{h}_{\mathbf{x}}\mathbf{P}^{T}\] (83) \[\mathbf{P}Q_{\mathbf{u}\mathbf{u}}\mathbf{P}^{T}=\mathbf{P}\ell_ {\mathbf{u}\mathbf{u}}\mathbf{P}^{T}+\mathbf{P}\mathbf{h}_{\mathbf{x}}^{T}V_{ \mathbf{x}\mathbf{x}}^{\prime}\mathbf{h}_{\mathbf{x}}\mathbf{P}^{T}.\] (84)

To simplify notation, each projection matrix \(\mathbf{P}\) is the same -- in reality, this need not be the case. Note that \(\mathbf{Q}_{x}\) and \(\mathbf{Q}_{u}\) are simply of size \(d\) and therefore image-sized. For all our datasets, these each take 0.2 MB to store and are therefore negligible, and we do not project these variables. When \(\ell_{\mathbf{u}\mathbf{u}}\) is diagonal (as it is in our case), we can obtain the projected inverse for \(Q_{\mathbf{u}\mathbf{u}}\) as

\[\mathbf{P}Q_{\mathbf{u}\mathbf{u}}^{-1}\mathbf{P}^{T}=\mathbf{P}\ell_{ \mathbf{u}\mathbf{u}}^{-1}\mathbf{P}^{T}+\mathbf{P}\ell_{\mathbf{u}\mathbf{u} }^{-1}\mathbf{P}^{T}(\mathbf{C}^{-1}+\mathbf{P}^{T}\ell_{\mathbf{u}\mathbf{u} }^{-1}\mathbf{P})^{-1}\mathbf{P}\ell_{\mathbf{u}\mathbf{u}}^{-1}\mathbf{P}^{ T}\qquad\text{where }\mathbf{C}=\mathbf{P}\mathbf{h}_{\mathbf{x}}^{T}V_{\mathbf{x}\mathbf{x}}^{ \prime}\mathbf{h}_{\mathbf{x}}\mathbf{P}\] (85)

via a direct application of the Woodbury matrix inversion formula Petersen et al. (2008), which has cost \(\mathcal{O}(k^{3}+kd^{2})\). Finally, we compute the projected updates \(V_{\mathbf{x}\mathbf{x}},\mathbf{K}\) as well as the full-precision

\begin{table}
\begin{tabular}{l c c c c c c c c} \hline \hline  & \multicolumn{4}{c}{SR \(\times 4\)} & \multicolumn{4}{c}{Random Inpainting} \\ \cline{2-10}  & LPIPS \(\downarrow\) & PSNR \(\uparrow\) & SSIM \(\uparrow\) & MSE \(\downarrow\) & LPIPS \(\downarrow\) & PSNR \(\uparrow\) & SSIM \(\uparrow\) & MSE \(\downarrow\) \\ \hline \(T=10\) & 0.198 & 27.48 & 0.783 & 125.6 & 0.168 & 27.46 & 0.771 & 123.7 \\ \(T=20\) & 0.1923 & 31.79 & 0.859 & 117.0 & 0.108 & 34.41 & 0.910 & 42.57 \\ \(T=50\) & 0.171 & 27.45 & 0.792 & 90.79 & 0.053 & 31.84 & 0.882 & 40.56 \\ \(T=200\) & 0.155 & 28.55 & 0.811 & 43.05 & 0.048 & 32.05 & 0.899 & 23.17 \\ \hline \hline \end{tabular}
\end{table}
Table 6: Ablative study on the effect of \(T\) on performance (LPIPS, PSNR, SSIM, NMSE) of our proposed model on the FFHQ \(256\)x\(256\)-1K dataset dataset.

\(V_{\mathbf{x}},\mathbf{k}\) terms via

\[\mathbf{k} =-\mathbf{P}^{T}\mathbf{P}Q_{\mathbf{uu}}^{-1}\mathbf{P}^{T}\mathbf{P }Q_{\mathbf{u}}\] (86) \[V_{\mathbf{x}} =Q_{\mathbf{x}}-\mathbf{P}^{T}\mathbf{P}\mathbf{K}^{T}\mathbf{P}^{ T}\mathbf{P}Q_{\mathbf{uu}}\mathbf{P}^{T}\mathbf{P}\mathbf{k}\] (87) \[\mathbf{P}\mathbf{K}\mathbf{P}^{T} =-\mathbf{P}Q_{\mathbf{uu}}^{-1}\mathbf{P}^{T}\mathbf{P}Q_{ \mathbf{ux}}\mathbf{P}^{T}\] (88) \[\mathbf{P}V_{\mathbf{xx}}\mathbf{P}^{T} =\mathbf{P}Q_{\mathbf{xx}}\mathbf{P}^{T}-\mathbf{P}\mathbf{K}^{T }\mathbf{P}^{T}\mathbf{P}Q_{\mathbf{uu}}\mathbf{P}^{T}\mathbf{P}\mathbf{K} \mathbf{P}^{T}.\] (89)

Where applicable, we leverage vector-Jacobian products from standard automatic differentiation libraries (e.g. torch.func.vjp) which have runtime complexity \(\mathcal{O}(1)\). Computing the \(V_{\mathbf{x}},V_{\mathbf{xx}},\mathbf{k},\mathbf{K}\) terms in Eqs. (46)-(49) costs \(\mathbf{O}(k^{3}+kd^{2})\) FLOPs in terms of matrix multiplications (dominated by the matrix inverse of \(k\times k\) matrix \(\mathbf{q}^{T}Q_{\mathbf{uu}}\mathbf{q}\)). Crucially, it incurs \(\mathcal{O}(k)\) neural function evaluations (NFEs), which dominates the runtime of the algorithm. Since this computation is performed for each diffusion step and iLQR iteration, the total runtime complexity of our algorithm is \(\mathcal{O}(nm(k^{3}+kd^{2}))\) matrix multiplication FLOPs and \(\mathcal{O}(nmk)\) NFEs, with \(\mathcal{O}(mk^{2}+d)\) space complexity. In terms of time complexity, the NFEs are the dominating cost, accounting for \(97\%\) of computation time.

### Sensitivity to Hyperparameters

In Tables 4, 5, 6, we investigate the effect of the rank of the low rank approximation and matrix-free projections, the Tikhonov regularization coefficient \(\alpha\), and the diffusion time \(T\) on the performance of our method on the FFHQ \(256\)x\(256\) dataset. We evaluate performance on the super-resolution and random inpainting tasks, with the same setup as in Section 6.

Low-Rank and Matrix-Free RankFrom Table 4, it is clear that there is a significant performance gain from even a rank one approximation of the first- and second-order matrices. The gains from subsequent increases in the rank approximation diminish quickly. This is because increasing the rank of the approximation only improves the approximation of the second-order terms. The first order \(V_{\mathbf{x}},Q_{\mathbf{x}},Q_{\mathbf{u}}\) terms are always modeled exactly in \(\mathcal{O}(1)\) time per iteration due to their amenability to vector-Jacobian products. From Theorems 4.1-4.3 we see that even when the second order terms are zero (i.e., the result of assumption \(\ell_{t}=0\)), we exactly recover the true posterior sampler. Therefore, the second-order terms are less important, though still useful for imposing a quadratic trust-region regularization to the algorithm. Therefore, we ultimately choose \(k=1\) for three reasons:

1. the rank only affects the quadratic approximation of the iLQR algorithm (and does not affect our theoretical results in Theorems 4.1-4.3)
2. \(k=1\) already allows second-order propagation of the quadratic trust-region regularization, and
3. subsequent increases in \(k\) have a minimal effect on the performance of the algorithm.

Tikhonov RegularizerTable 5 demonstrates that our algorithm is relatively robust to the Tikhonov regularization parameter, except when \(\alpha=0\). Under this condition, any ill-conditioning of \(Q_{\mathbf{uu}}\) results in division by zero errors, resulting in the failure of the algorithm. Therefore, we simply choose to let \(\alpha=1e-4\), since the effect of Tikhonov regularizer is minimal.

Diffusion StepsFinally, we observe in Table 6 that increasing the diffusion time results in higher quality samples -- though at the cost of increased computation time. Therefore, choice of \(T\) requires balancing computational cost and sample quality, and is ultimately highly user-dependent. When the computational and latency budget is relatively high, large \(T\) can be used to improve sample quality. Conversely, when this budget is low, we find that even \(T=20\) provides reasonable samples.