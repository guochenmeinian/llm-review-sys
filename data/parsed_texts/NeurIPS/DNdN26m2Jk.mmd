# Crystal Structure Prediction

by Joint Equivariant Diffusion

Rui Jiao\({}^{1,2}\) Wenbing Huang\({}^{3,4}\)1 Peijia Lin\({}^{5}\) Jiaqi Han\({}^{6}\) Pin Chen\({}^{5}\) Yutong Lu\({}^{5}\) Yang Liu\({}^{1,2}\)1

\({}^{1}\)Dept. of Comp. Sci. & Tech., Institute for AI, Tsinghua University

\({}^{2}\)Institute for AIR, Tsinghua University

\({}^{3}\)Gaoling School of Artificial Intelligence, Renmin University of China

\({}^{4}\) Beijing Key Laboratory of Big Data Management and Analysis Methods, Beijing, China

\({}^{5}\) National Supercomputer Center in Guangzhou,

School of Computer Science and Engineering, Sun Yat-sen University

\({}^{6}\) Stanford University

Footnote 1: Wenbing Huang and Yang Liu are corresponding authors.

###### Abstract

Crystal Structure Prediction (CSP) is crucial in various scientific disciplines. While CSP can be addressed by employing currently-prevailing generative models (_e.g._ diffusion models), this task encounters unique challenges owing to the symmetric geometry of crystal structures--the invariance of translation, rotation, and periodicity. To incorporate the above symmetries, this paper proposes DiffCSP, a novel diffusion model to learn the structure distribution from stable crystals. To be specific, DiffCSP jointly generates the lattice and atom coordinates for each crystal by employing a periodic-E(3)-equivariant denoising model, to better model the crystal geometry. Notably, different from related equivariant generative approaches, DiffCSP leverages fractional coordinates other than Cartesian coordinates to represent crystals, remarkably promoting the diffusion and the generation process of atom positions. Extensive experiments verify that our DiffCSP significantly outperforms existing CSP methods, with a much lower computation cost in contrast to DFT-based methods. Moreover, the superiority of DiffCSP is also observed when it is extended for ab initio crystal generation. Code is available at [https://github.com/jiaor17/DiffCSP](https://github.com/jiaor17/DiffCSP).

## 1 Introduction

Crystal Structure Prediction (CSP), which returns the stable 3D structure of a compound based solely on its composition, has been a goal in physical sciences since the 1950s [1]. As crystals are the foundation of various materials, estimating their structures in 3D space determines the physical and chemical properties that greatly influence the application to various academic and industrial sciences, such as the design of drugs, batteries, and catalysts [2]. Conventional methods towards CSP mostly apply the Density Functional Theory (DFT) [3] to compute the energy at each iteration, guided by optimization algorithms (such as random search [4], Bayesian optimization [5], etc.) to iteratively search for the stable state corresponding to the local minima of the energy surface [6].

The DFT-based approaches are computationally-intensive. Recent attention has been paid to deep generative models, which directly learn the distribution from the training data consisting of stable structures [7, 8]. More recently, diffusion models, a special kind of deep generative models are employed for crystal generation [9], encouraged by their better physical interpretability and enhanced performance than other generative models. Intuitively, by conducting diffusion on stable structures,the denoising process in diffusion models acts like a force field that drives the atom coordinates towards the energy local minimum and thus is able to increase stability. Indeed, the success of diffusion models is observed in broad scientific domains, including molecular conformation generation [10], protein structure prediction [11] and protein docking [12].

However, designing diffusion models for CSP is challenging. From the perspective of physics, any E(3) transformation, including translation, rotation, and reflection, of the crystal coordinates does not change the physical law and thus keeps the crystal distribution invariant. In other words, the generation process we design should yield E(3) invariant samples. Moreover, in contrast to other types of structures such as small molecules [13] and proteins [14], CSP exhibits unique challenges, mainly incurred by the periodicity of the atom arrangement in crystals. Figure 1 displays a crystal where the atoms in a unit cell are repeated infinitely in space. We identify such unique symmetry, jointly consisting of E(3) invariance and periodicity, as _periodic E(3) invariance_ in this paper. Generating such type of structures requires not only modeling the distribution of the atom coordinates within every cell, but also inferring how their bases (_a.k.a._ lattice vectors) are placed in 3D space. Interestingly, as we will show in SS 4.1, such view offers a natural disentanglement for fulfilling the periodic E(3) invariance by separately enforcing constraints on fractional coordinates and lattice vectors, which permits a feasible implementation to encode the crystal symmetry.

In this work, we introduce DiffCSP, an equivariant diffusion method to address CSP. Considering the specificity of the crystal geometry, our DiffCSP jointly generates the lattice vectors and the fractional coordinates of all atoms, by employing a proposed denoising model that is theoretically proved to generate periodic-E(3)-invariant samples. A preferable characteristic of DiffCSP is that it leverages the fractional coordinate system (defined in SS 3) other than the Cartesian system used in previous methods to represent crystals [9; 15], which encodes periodicity intrinsically. In particular, the fractional representation not only allows us to consider Wrapped Normal (WN) distribution [16] to better model the periodicity, but also facilitates the design of the denoising model via the Fourier transformation, compared to the traditional multi-graph encoder in crystal modeling [15].

CDVAE [9] is closely related with our paper. It adopts an equivariant Variational Auto-Encoder (VAE) based framework to learn the data distribution and then generates crystals in a score-matching-based diffusion process. However, CDVAE focuses mainly on ab initio crystal generation where the composition is also randomly sampled, which is distinct from the CSP task in this paper. Moreover, while CDVAE first predicts the lattice and then updates the coordinates with the fixed lattice, we jointly update the lattice and coordinates to better model the crystal geometry. Besides, CDVAE represents crystals by Cartesian coordinates upon multi-graph modeling, whereas our DiffCSP applies fractional coordinates without multi-graph modeling as mentioned above.

To sum up, our contributions are as follows:

* To the best of our knowledge, we are the first to apply equivariant diffusion-based methods to address CSP. The proposed DiffCSP is more insightful than current learning-based approaches as the periodic E(3) invariance has been delicately considered.
* DiffCSP conducts joint diffusion on lattices and fractional coordinates, which is capable of capturing the crystal geometry as a whole. Besides, the usage of fractional coordinates in place of Cartesian coordinates used in previous methods (_e.g._ CDVAE [9]) remarkably promotes the diffusion and the generation process of atom positions.
* We verify the efficacy of DiffCSP on the CSP task against learning-based and DFT-based methods, and sufficiently ablate each proposed component in DiffCSP. We further extend DiffCSP into ab initio generation and show its effectiveness against related methods.

## 2 Related Works

**Crystal Structure Prediction** Traditional computation methods [4; 5; 17; 18] combine DFT [3] with optimization algorithms to search for local minima in the potential energy surface. However, DFT is computationally intensive, making it dilemmatic to balance efficiency and accuracy. With the improvement of crystal databases, machine-learning methods are applied as alternative energy predictors to DFT followed by optimization steps [19; 20; 21]. Apart from the predict-optimize paradigm, another line of approaches directly learns stable structures from data by deep generative models, which represents crystals by 3D voxels [7; 22; 23], distance matrices [8; 24; 25] or 3Dcoordinates [26; 27; 28]. Unfortunately, these methods are unaware of the full symmetries of the crystal structure. CDVAE [9] has taken the required symmetries into account. However, as mentioned above, the initial version of CDVAE is for different task and utilizes different generation process.

**Equivariant Graph Neural Networks** Geometrically equivariant Graph Neural Networks (GNNs) that ensure E(3) symmetry are powerful tools to represent physical objects [29; 30; 31; 32; 33], and have showcased the superiority in modeling 3D structures [34; 35]. To further model the periodic materials, Xie and Grossman [15] propose the multi-graph edge construction to capture the periodicity by connecting the edges between adjacent lattices. Yan et al. [36] further introduce periodic pattern encoding into a Transformer-based backbone. In this work, we achieve the periodic invariance by introducing the Fourier transform on fractional coordinates.

**Diffusion Generative Models** Motivated by the non-equilibrium thermodynamics [37], diffusion models connect the data distribution with the prior distribution via forward and backward Markov chains [38], and have made remarkable progress in the field of image generation [39; 40]. Equipped with equivariant GNNs, diffusion models are capable of generating samples from the invariant distribution, which is desirable in conformation generation [10; 13], ab initio molecule design [41], protein generation [42], and so on. Recent works extend the diffusion models onto Riemann manifolds [43; 44], and enable the generation of periodic features like torsion angles [12; 16].

## 3 Preliminaries

**Representation of crystal structures** A 3D crystal can be represented as the infinite periodic arrangement of atoms in 3D space, and the smallest repeating unit is called a _unit cell_, as shown in Figure 1. A unit cell can be defined by a triplet \(\mathcal{M}=(\mathbf{A},\mathbf{X},\mathbf{L})\), where \(\mathbf{A}=[\mathbf{a}_{1},\mathbf{a}_{2},...,\mathbf{a}_{N}]\in\mathbb{R}^{h\times N}\) denotes the list of the one-hot representations of atom types, \(\mathbf{X}=[\mathbf{x}_{1},\mathbf{x}_{2},...,\mathbf{x}_{N}]\in\mathbb{R}^{3\times N}\) consists of Cartesian coordinates of the atoms, and \(\mathbf{L}=[\mathbf{l}_{1},\mathbf{l}_{2},\mathbf{l}_{3}]\in\mathbb{R}^{3\times 3}\) represents the lattice matrix containing three basic vectors to describe the periodicity of the crystal. The infinite periodic crystal structure is represented by

\[\{(\mathbf{a}^{\prime}_{i},\mathbf{x}^{\prime}_{i})|\mathbf{a}^{\prime}_{i}=\mathbf{a}_{i},\bm {x}^{\prime}_{i}=\mathbf{x}_{i}+\mathbf{L}\mathbf{k},\;\forall\mathbf{k}\in\mathbb{Z}^{3\times 1 }\}, \tag{1}\]

where the \(j\)-th element of the integral vector \(\mathbf{k}\) denotes the integral 3D translation in units of \(\mathbf{l}_{j}\).

**Fractional coordinate system** The Cartesian coordinate system \(\mathbf{X}\) leverages three standard orthogonal bases as the coordinate axes. In crystallography, the fractional coordinate system is usually applied to reflect the periodicity of the crystal structure [26; 27; 28; 45], which utilizes the lattices \((\mathbf{l}_{1},\mathbf{l}_{2},\mathbf{l}_{3})\) as the bases. In this way, a point represented by the fractional coordinate vector \(\mathbf{f}=[f_{1},f_{2},f_{3}]^{\top}\in[0,1)^{3}\) corresponds to the Cartesian vector \(\mathbf{x}=\sum_{i=1}^{3}f_{i}\mathbf{l}_{i}\). This paper employs the fractional coordinate system, and denotes the crystal by \(\mathcal{M}=(\mathbf{A},\mathbf{F},\mathbf{L})\), where the fractional coordinates of all atoms in a cell compose the matrix \(\mathbf{F}\in[0,1)^{3\times N}\).

**Task definition** CSP predicts for each unit cell the lattice matrix \(\mathbf{L}\) and the fractional matrix \(\mathbf{F}\) given its chemical composition \(\mathbf{A}\), namely, learning the conditional distribution \(p(\mathbf{L},\mathbf{F}\mid\mathbf{A})\).

## 4 The Proposed Method: DiffCSP

This section first presents the symmetries of the crystal geometry, and then introduces the joint equivaraint diffusion process on \(\mathbf{L}\) and \(\mathbf{F}\), followed by the architecture of the denoising function.

### Symmetries of Crystal Structure Distribution

While various generative models can be utilized to address CSP, this task encounters particular challenges, including constraints arising from symmetries of crystal structure distribution. Here, we consider the three types of symmetries in the distribution of \(p(\mathbf{L},\mathbf{F}\mid\mathbf{A})\): permutation invariance, \(O(3)\) invariance, and periodic translation invariance. Their detailed definitions are provided as follows.

**Definition 1** (Permutation Invariance).: _For any permutation \(\mathbf{P}\in\mathbb{S}_{N}\), \(p(\mathbf{L},\mathbf{F}\mid\mathbf{A})=p(\mathbf{L},\mathbf{FP}\mid\mathbf{AP})\), i.e., changing the order of atoms will not change the distribution._

**Definition 2** (O(3) Invariance).: _For any orthogonal transformation \(\mathbf{Q}\in\mathbb{R}^{3\times 3}\) satisfying \(\mathbf{Q}^{\top}\mathbf{Q}=\mathbf{I}\), \(p(\mathbf{QL},\mathbf{F}\mid\mathbf{A})=p(\mathbf{L},\mathbf{F}\mid\mathbf{A})\), namely, any rotation/reflection of \(\mathbf{L}\) keeps the distribution unchanged._

**Definition 3** (Periodic Translation Invariance).: _For any translation \(\mathbf{t}\in\mathbb{R}^{3\times 1}\), \(p(\mathbf{L},w(\mathbf{F}+\mathbf{t1}^{\top})\mid\mathbf{A})=p(\mathbf{L},\mathbf{F}\mid\mathbf{A})\), where the function \(w(\mathbf{F})=\mathbf{F}-\lfloor\mathbf{F}\rfloor\in[0,1)^{3\times N}\) returns the fractional part of each element in \(\mathbf{F}\), and \(\mathbf{1}\in\mathbb{R}^{3\times 1}\) is a vector with all elements set to one. It explains that any periodic translation of \(\mathbf{F}\) will not change the distribution2._

Footnote 2: Previous works (_e.g._[36]) further discuss the scaling invariance of a unit cell formed by periodic boundaries, allowing \(\mathbf{L}\rightarrow\alpha\mathbf{L},\forall\alpha\in\mathbb{N}_{+}^{3}\). In this paper, the scaling invariance is unnecessary since we apply the Niggli reduction [46] on the primitive cell as a canonical scale representation of the lattice vectors where we fix \(\alpha=(1,1,1)^{\top}\). Additionally, periodic translation invariance in our paper is equivalent to the invariance of shifting periodic boundaries in [36]. We provide more discussions in Appendix A.4.

The permutation invariance is tractably encapsulated by using GNNs as the backbone for generation [47]. We mainly focus on the other two kinds of invariance (see Figure 1), since GNNs are our default choices. For simplicity, we compactly term the \(O(3)\) invariance and periodic translation invariance as _periodic E(3) invariance_ henceforth. Previous approaches (_e.g._[9, 36]) adopt Cartesian coordinates \(\mathbf{X}\) other than fractional coordinates \(\mathbf{F}\), hence their derived forms of the symmetry are different. Particularly, in Definition 2, the orthogonal transformation additionally acts on \(\mathbf{X}\); in Definition 3, the periodic translation \(w(\mathbf{F}+\mathbf{t1}^{\top})\) becomes the translation along the lattice bases \(\mathbf{X}+\mathbf{Lt1}^{\top}\); besides, \(\mathbf{X}\) should also maintain E(3) translation invariance, that is \(p(\mathbf{L},\mathbf{X}+\mathbf{t1}^{\top}|\mathbf{A})=p(\mathbf{L},\mathbf{X}|\mathbf{A})\). With the help of the fractional system, the periodic E(3) invariance is made tractable by fulfilling O(3) invariance _w.r.t._ the orthogonal transformations on \(\mathbf{L}\) and periodic translation invariance _w.r.t._ the periodic translations on \(\mathbf{F}\), respectively. In this way, such approach, as detailed in the next section, facilitates the application of diffusion methods to the CSP task.

### Joint Equivariant Diffusion

Our method DiffCSP addresses CSP by simultaneously diffusing the lattice \(\mathbf{L}\) and the fractional coordinate matrix \(\mathbf{F}\). Given the atom composition \(\mathbf{A}\), \(\mathcal{M}_{t}\) denotes the intermediate state of \(\mathbf{L}\) and \(\mathbf{F}\) at time step \(t\)\((0\leq t\leq T)\). DiffCSP defines two Markov processes: the forward diffusion process gradually adds noise to \(\mathcal{M}_{0}\), and the backward generation process iteratively samples from the prior distribution \(\mathcal{M}_{T}\) to recover the origin data \(\mathcal{M}_{0}\).

Figure 1: (a)\(\rightarrow\)(b): The orthogonal transformation of the lattice vectors. (c)\(\rightarrow\)(d): The periodic translation of the fractional coordinates. Both cases do not change the structure.

Figure 2: Overview of DiffCSP. Given the composition \(\mathbf{A}\), we denote the crystal, its lattice and fractional coordinate matrix at time \(t\) as \(\mathcal{M}_{t}\), \(\mathbf{L}_{t}\) and \(\mathbf{F}_{t}\), respectively. The terms \(\mathbf{\epsilon_{L}}\) and \(\mathbf{\epsilon_{F}}\) are Gaussian noises, \(\hat{\mathbf{\epsilon}_{L}}\) and \(\hat{\mathbf{\epsilon}_{F}}\) are predicted by the denoising model \(\phi\).

Joining the statements in SS 4.1, the recovered distribution from \(\mathcal{M}_{T}\) should meet periodic E(3) invariance. Such requirement is satisfied if the prior distribution \(p(\mathcal{M}_{T})\) is invariant and the Markov transition \(p(\mathcal{M}_{t-1}\mid\mathcal{M}_{t})\) is equivariant, according to the diffusion-based generation literature [10]. Here, an equivariant transition is specified as \(p(g\cdot\mathcal{M}_{t-1}\mid g\cdot\mathcal{M}_{t})=p(\mathcal{M}_{t-1}\mid \mathcal{M}_{t})\) where \(g\cdot\mathcal{M}\) refers to any orthogonal/translational transformation \(g\) acts on \(\mathcal{M}\) in the way presented in Definitions 2-3. We separately explain the derivation details of \(\mathbf{L}\) and \(\mathbf{F}\) below. The detailed flowcharts are summarized in Algorithms 1 and 2 in Appendix B.3.

**Diffusion on \(\mathbf{L}\)** Given that \(\mathbf{L}\) is a continuous variable, we exploit Denoising Diffusion Probabilistic Model (DDPM) [38] to accomplish the generation. We define the forward process that progressively diffuses \(\mathbf{L}_{0}\) towards the Normal prior \(p(\mathbf{L}_{T})=\mathcal{N}(0,\mathbf{I})\) by \(q(\mathbf{L}_{t}|\mathbf{L}_{t-1})\) which can be devised as the probability conditional on the initial distribution:

\[q(\mathbf{L}_{t}|\mathbf{L}_{0})=\mathcal{N}\Big{(}\mathbf{L}_{t}|\sqrt{\bar{\alpha}_{t}} \mathbf{L}_{0},(1-\bar{\alpha}_{t})\mathbf{I}\Big{)}, \tag{2}\]

where \(\beta_{t}\in(0,1)\) controls the variance, and \(\bar{\alpha}_{t}=\prod_{s=1}^{t}\alpha_{t}=\prod_{s=1}^{t}(1-\beta_{t})\) is valued in accordance to the cosine scheduler [48].

The backward generation process is given by:

\[p(\mathbf{L}_{t-1}|\mathcal{M}_{t})=\mathcal{N}(\mathbf{L}_{t-1}|\mu(\mathcal{M}_{t}), \sigma^{2}(\mathcal{M}_{t})\mathbf{I}), \tag{3}\]

where \(\mu(\mathcal{M}_{t})=\frac{1}{\sqrt{\alpha_{t}}}\Big{(}\mathbf{L}_{t}-\frac{\beta_ {t}}{\sqrt{1-\bar{\alpha}_{t}}}\hat{\mathbf{\epsilon}}_{\mathbf{L}}(\mathcal{M}_{t},t) \Big{)}\),\(\sigma^{2}(\mathcal{M}_{t})\)= \(\beta_{t}\frac{1-\bar{\alpha}_{t-1}}{1-\bar{\alpha}_{t}}\). The denoising term \(\hat{\mathbf{\epsilon}}_{\mathbf{L}}(\mathcal{M}_{t},t)\in\mathbb{R}^{3\times 3}\) is predicted by the model \(\phi(\mathbf{L}_{t},\mathbf{F}_{t},\mathbf{A},t)\).

As the prior distribution \(p(\mathbf{L}_{T})=\mathcal{N}(0,\mathbf{I})\) is already O(3)-invariant, we require the generation process in Eq. (3) to be O(3)-equivariant, which is formally stated below.

**Proposition 1**.: _The marginal distribution \(p(\mathbf{L}_{0})\) by Eq. (3) is O(3)-invariant if \(\hat{\mathbf{\epsilon}}_{\mathbf{L}}(\mathcal{M}_{t},t)\) is O(3)-equivariant, namely \(\hat{\mathbf{\epsilon}}_{\mathbf{L}}(\mathbf{Q}\mathbf{L}_{t},\mathbf{F}_{t},\mathbf{A},t)=\mathbf{Q}\hat {\mathbf{\epsilon}}_{\mathbf{L}}(\mathcal{L}_{t},\mathbf{F}_{t},\mathbf{A},t),\forall\mathbf{Q}^{ \top}\mathbf{Q}=\mathbf{I}\)._

To train the denoising model \(\phi\), we first sample \(\mathbf{\epsilon}_{\mathbf{L}}\sim\mathcal{N}(0,\mathbf{I})\) and reparameterize \(\mathbf{L}_{t}=\sqrt{\bar{\alpha}_{t}}\mathbf{L}_{0}+\sqrt{1-\bar{\alpha}_{t}}\mathbf{ \epsilon}_{\mathbf{L}}\) based on Eq. (2). The training objective is defined as the \(\ell_{2}\) loss between \(\mathbf{\epsilon}_{\mathbf{L}}\) and \(\hat{\mathbf{\epsilon}}_{\mathbf{L}}\):

\[\mathcal{L}_{\mathbf{L}}=\mathbb{E}_{\mathbf{\epsilon}_{\mathbf{L}}\sim\mathcal{N}(0,\mathbf{ I}),t\sim\mathcal{U}(1,T)}[\|\mathbf{\epsilon}_{\mathbf{L}}-\hat{\mathbf{\epsilon}}_{\mathbf{L}}( \mathcal{M}_{t},t)\|_{2}^{2}]. \tag{4}\]

**Diffusion on \(\mathbf{F}\)** The domain of fractional coordinates \([0,1)^{3\times N}\) forms a quotient space \(\mathbb{R}^{3\times N}/\mathbb{Z}^{3\times N}\) induced by the crystal periodicity. It is not suitable to apply the above DDPM fashion to generate \(\mathbf{F}\), as the normal distribution used in DDPM is unable to model the cyclical and bounded domain of \(\mathbf{F}\). Instead, we leverage Score-Matching (SM) based framework [49; 50] along with Wrapped Normal (WN) distribution [43] to fit the specificity here. Note that WN distribution has been explored in generative models, such as molecular conformation generation [16].

During the forward process, we first sample each column of \(\mathbf{\epsilon}_{\mathbf{F}}\in\mathbb{R}^{3\times N}\) from \(\mathcal{N}(0,\mathbf{I})\), and then acquire \(\mathbf{F}_{t}=w(\mathbf{F}_{0}+\sigma_{t}\mathbf{\epsilon}_{\mathbf{F}})\) where the truncation function \(w(\cdot)\) is already defined in Definition 3. This truncated sampling implies the WN transition:

\[q(\mathbf{F}_{t}|\mathbf{F}_{0})\propto\sum_{\mathbf{Z}\in\mathbb{Z}^{3\times N}}\exp \Big{(}-\frac{\|\mathbf{F}_{t}-\mathbf{F}_{0}+\mathbf{Z}\|_{F}^{2}}{2\sigma_{t}^{2}}\Big{)}. \tag{5}\]

Basically, this process ensures the probability distribution over \([z,z+1)^{3\times N}\) for any integer \(z\) to be the same to keep the crystal periodicity. Here, the noise scale \(\sigma_{t}\) obeys the exponential scheduler: \(\sigma_{0}=0\) and \(\sigma_{t}=\sigma_{1}(\frac{\sigma_{T}}{\sigma_{1}})\frac{t-1}{T-1}\), if \(t>0\). Desirably, \(q(\mathbf{F}_{t}|\mathbf{F}_{0})\) is periodic translation equivariant, and approaches a uniform distribution \(\mathcal{U}(0,1)\) if \(\sigma_{T}\) is sufficiently large.

For the backward process, we first initialize \(\mathbf{F}_{T}\) from the uniform distribution \(\mathcal{U}(0,1)\), which is periodic translation invariant. With the denoising term \(\hat{\mathbf{\epsilon}}_{\mathbf{F}}\) predicted by \(\phi(\mathbf{L}_{t},\mathbf{F}_{t},\mathbf{A},t)\), we combine the ancestral predictor [38; 50] with the Langevin corrector [49] to sample \(\mathbf{F}_{0}\). We immediately have:

**Proposition 2**.: _The marginal distribution \(p(\mathbf{F}_{0})\) is periodic translation invariant if \(\hat{\mathbf{\epsilon}}_{\mathbf{F}}(\mathcal{M}_{t},t)\) is periodic translation invariant, namely \(\hat{\mathbf{\epsilon}}_{\mathbf{F}}(\mathbf{L}_{t},\mathbf{F}_{t},\mathbf{A},t)=\hat{\mathbf{\epsilon}} _{\mathbf{F}}(\mathbf{L}_{t},w(\mathbf{F}_{t}+\mathbf{t}\mathbf{1}^{\top}),\mathbf{A},t),\forall\mathbf{t} \in\mathbb{R}^{3}\)._The training objective for score matching is:

\[\mathcal{L}_{\mathbf{F}}=\mathbb{E}_{\mathbf{F}_{t}\sim q(\mathbf{F}_{t}|\mathbf{F}_{0}),t\sim \mathcal{U}(1,T)}\big{[}\lambda_{t}\|\nabla_{\mathbf{F}_{t}}\log q(\mathbf{F}_{t}|\mathbf{F} _{0})-\hat{\mathbf{\epsilon}}_{\mathbf{F}}(\mathcal{M}_{t},t)\|_{2}^{2}\big{]},\]

where \(\lambda_{t}=\mathbb{E}_{\mathbf{F}_{t}}^{-1}\big{[}\|\nabla_{\mathbf{F}_{t}}\log q(\mathbf{ F}_{t}|\mathbf{F}_{0})\|_{2}^{2}\big{]}\) is approximated via Monte-Carlo sampling. More details are deferred to Appendix B.1.

**Extension to ab initio crystal generation** Although our method is proposed to address CSP where the composition \(\mathbf{A}\) is fixed, our method is able to be extended for the ab initio generation task by further generating \(\mathbf{A}\). We achieve this by additionally optimizing the one-hot representation \(\mathbf{A}\) with a DDPM-based approach. We provide more details in Appendix G.

### The Architecture of the Denoising Model

This subsection designs the denoising model \(\phi(\mathbf{L},\mathbf{F},\mathbf{A},t)\) that outputs \(\hat{\mathbf{\epsilon}}_{\mathbf{L}}\) and \(\hat{\mathbf{\epsilon}}_{\mathbf{F}}\) satisfying the properties stated in Proposition 1 and 2.

Let \(\mathbf{H}^{(s)}=[\mathbf{h}_{1}^{(s)},\cdots,\mathbf{h}_{N}^{(s)}]\) denote the node representations of the \(s\)-th layer. The input feature is given by \(\mathbf{h}_{i}^{(0)}=\rho(f_{\text{atom}}(\mathbf{a}_{i}),f_{\text{pos}}(t))\), where \(f_{\text{atom}}\) and \(f_{\text{pos}}\) are the atomic embedding and sinusoidal positional encoding [38; 51], respectively; \(\rho\) is a multi-layer perception (MLP).

Built upon EGNN [32], the \(s\)-th layer message-passing is unfolded as follows:

\[\mathbf{m}_{ij}^{(s)} =\varphi_{m}(\mathbf{h}_{i}^{(s-1)},\mathbf{h}_{j}^{(s-1)},\mathbf{L}^{\top} \mathbf{L},\psi_{\text{FT}}(\mathbf{f}_{j}-\mathbf{f}_{i})), \tag{6}\] \[\mathbf{m}_{i}^{(s)} =\sum_{j=1}^{N}\mathbf{m}_{ij}^{(s)},\] (7) \[\mathbf{h}_{i}^{(s)} =\mathbf{h}_{i}^{(s-1)}+\varphi_{h}(\mathbf{h}_{i}^{(s-1)},\mathbf{m}_{i}^{(s)}). \tag{8}\]

\(\psi_{\text{FT}}(\mathbf{f})[c,k]=\sin(2\pi mf_{c})\), if \(k=2m\) (even); and \(\psi_{\text{FT}}(\mathbf{f})[c,k]=\cos(2\pi mf_{c})\), if \(k=2m+1\) (odd). \(\psi_{\text{FT}}\) extracts various frequencies of all relative fractional distances that are helpful for crystal structure modeling, and more importantly, \(\psi_{\text{FT}}\) is periodic translation invariant, namely, \(\psi_{\text{FT}}(w(\mathbf{f}_{j}+\mathbf{t})-w(\mathbf{f}_{i}+\mathbf{t}))=\psi_{\text{FT}}( \mathbf{f}_{j}-\mathbf{f}_{i})\) for any translation \(\mathbf{t}\), which is proved in Appendix A.3.

After \(S\) layers of message passing conducted on the fully connected graph, the lattice noise \(\hat{\mathbf{\epsilon}}_{\mathbf{L}}\) is acquired by a linear combination of \(\mathbf{L}\), with the weights given by the final layer:

\[\hat{\mathbf{\epsilon}}_{\mathbf{L}}=\mathbf{L}\varphi_{L}\Big{(}\frac{1}{N}\sum_{i=1}^{N} \mathbf{h}_{i}^{(S)}\Big{)}, \tag{9}\]

where \(\varphi_{L}\) is an MLP with output shape as \(3\times 3\). The fractional coordinate score \(\hat{\mathbf{\epsilon}}_{\mathbf{F}}\) is output by:

\[\hat{\mathbf{\epsilon}}_{\mathbf{F}}[:,i]=\varphi_{F}(\mathbf{h}_{i}^{(S)}), \tag{10}\]

where \(\hat{\mathbf{\epsilon}}_{\mathbf{F}}[:,i]\) defines the \(i\)-th column of \(\hat{\mathbf{\epsilon}}_{\mathbf{F}}\), and \(\varphi_{F}\) is an MLP on the final representation.

We apply the inner product term \(\mathbf{L}^{\top}\mathbf{L}\) in Eq. (6) to achieve O(3)-invariance, as \((\mathbf{QL})^{\top}(\mathbf{QL})=\mathbf{L}^{\top}\mathbf{L}\) for any orthogonal matrix \(\mathbf{Q}\in\mathbb{R}^{3\times 3}\). This leads to the O(3)-invariance of \(\varphi_{L}\) in Eq. (10), and we further left-multiply \(\mathbf{L}\) with \(\varphi_{L}\) to ensure the O(3)-equivariance of \(\hat{\mathbf{\epsilon}}_{\mathbf{L}}\). Therefore, the above formulation of the denoising model \(\phi(\mathbf{L},\mathbf{F},\mathbf{A},t)\) ensures the following property.

**Proposition 3**.: _The score \(\hat{\mathbf{\epsilon}}_{\mathbf{L}}\) by Eq. (9) is O(3)-equivariant, and the score \(\hat{\mathbf{\epsilon}}_{\mathbf{F}}\) from Eq. (10) is periodic translation invariant. Hence, the generated distribution by DiffCSP is periodic E(3) invariant._

**Comparison with multi-graph representation** Previous methods [9; 15; 29; 52] utilize Cartesian coordinates, and usually describe crystals with multi-graph representation to encode the periodic structures. They create multiple edges to connect each pair of nodes where different edges refer to different integral cell translations. Here, we no longer require multi-graph representation, since we employ fractional coordinates that naturally encode periodicity and the Fourier transform \(\psi_{\text{FT}}\) in our message passing is already periodic translation invariant. We will ablate the benefit in Table 3.

[MISSING_PAGE_FAIL:7]

considers the ground-truth lattice initialization for encoding periodicity, yielding a final model named **P-cG-SchNet**. Another baseline **CDVAE**[9] is a VAE-based framework for pure crystal generation, by first predicting the lattice and the initial composition and then optimizing the atom types and coordinates via annealed Langevin dynamics [49]. To adapt CDVAE into the CSP task, we replace the original normal prior for generation with a parametric prior conditional on the encoding of the given composition. More details are provided in Appendix B.2.

**Evaluation metrics** Following the common practice [9], we evaluate by matching the predicted candidates with the ground-truth structure. Specifically, for each structure in the test set, we first generate \(k\) samples of the same composition and then identify the matching if at least one of the samples matches the ground truth structure, under the metric by the StructureMatcher class in pymatgen [58] with thresholds stol=0.5, angle_tol=10, ltol=0.3. The **Match rate** is the proportion of the matched structures over the test set. **RMSE** is calculated between the ground truth and the best matching candidate, normalized by \(\sqrt[3]{V/N}\) where \(V\) is the volume of the lattice, and averaged over the matched structures. For optimization methods, we select 20 structures of the lowest energy of all 5,000 structures from all iterations during testing as candidates. For generative baselines and our DiffCSP, we let \(k=1\) and \(k=20\) for evaluation. We provide more details in Appendix B, C.1 and I.

**Results** Table 1 conveys the following observations. **1.** The optimization methods encounter low Match rates, signifying the difficulty of locating the optimal structures within the vast search space. **2.** In comparison to other generative methods that construct structures atom by atom or predict the lattice and atom coordinates in two stages, our method demonstrates superior performance, highlighting the effectiveness of jointly refining the lattice and coordinates during generation. **3.** All methods struggle with performance degradation as the number of atoms per cell increases, on the datasets from Perov-5 to MPTS-52. For example, the Match rates of the optimization methods are less than 10% in MPTS-52. Even so, our method consistently outperforms all other methods.

**Visualization** Figure 3 provides qualitative comparisons.DiffCSP clearly makes the best predictions.

### Comparison with DFT-based Methods

We further select 10 binary and 5 ternary compounds in MP-20 testing set and compare our model with USPEX [59], a DFT-based software equipped with the evolutionary algorithm to search for stable structures. For our method, we sample 20 candidates for each compound following the setting in Table 1. We select the model trained on MP-20 for inference, with a training duration of 5.2 hours. For USPEX, we apply 20 generations, 20 populations for each compound, and select the best sample in each generation, leading to 20 candidates as well. We summarize the **Match rate** over the 15 compounds, the **Averaged RMSD** over the matched structures, and the **Averaged Inference Time** to generate 20 candidates for each compound in Table 10. The detailed results for each compound are listed in Appendix F. DiffCSP correctly predicts more structures with higher match rate, and more importantly, its time cost is much less than USPEX, allowing more potential for real applications.

### Ablation Studies

We ablate each component of DiffCSP in Table 3, and probe the following aspects. **1.** To verify the necessity of jointly updating the lattice \(\mathbf{L}\) and fractional coordinates \(\mathbf{F}\), we construct two variants that separate the joint optimization into two stages, denoted as \(\mathbf{L}\rightarrow\mathbf{F}\) and \(\mathbf{F}\rightarrow\mathbf{L}\). Particularly, \(\mathbf{L}\rightarrow\mathbf{F}\) applies two networks to learn the reverse process \(p_{\theta_{1}}(\mathbf{L}_{0:T-1}|\mathbf{A},\mathbf{F}_{T},\mathbf{L}_{T})\) and \(p_{\theta_{2}}(\mathbf{F}_{0:T-1}|\mathbf{A},\mathbf{F}_{T},\mathbf{L}_{0})\). During inference, we first sample \(\mathbf{L}_{T},\mathbf{F}_{T}\) from their prior distributions, acquiring \(\mathbf{L}_{0}\) via \(p_{\theta_{1}}\), and then \(\mathbf{F}_{0}\) by \(p_{\theta_{2}}\) based on \(\mathbf{L}_{0}\). \(\mathbf{F}\rightarrow\mathbf{L}\) is similarly executed but with the generation order of \(\mathbf{L}_{0}\) and \(\mathbf{F}_{0}\) exchanged. Results indicate that \(\mathbf{L}\rightarrow\mathbf{F}\) performs better than the \(\mathbf{F}\rightarrow\mathbf{L}\), but both are inferior to the joint update in DiffCSP, which endorses our design. We conjecture that the joint diffusion fashion enables \(\mathbf{L}\) and \(\mathbf{F}\) to update synergistically, which makes the generation process more tractable to learn and thus leads to better performance. **2.** We explore the necessity of preserving the O(3) invariance when generating \(\mathbf{L}\), which is ensured by the inner product \(\mathbf{L}^{\top}\mathbf{L}\) in Eq. (6).

\begin{table}
\begin{tabular}{l c c c} \hline \hline  & Match rate (\%)\(\uparrow\) & Avg. RMSD\(\downarrow\) & Avg. Time\(\downarrow\) \\ \hline USPEX [59] & 53.33 & **0.0159** & 12.5h \\ DiffCSP & **73.33** & 0.0172 & **10s** \\ \hline \hline \end{tabular}
\end{table}
Table 2: Overall results over the 15 selected compounds.

When we replace it with \(\mathbf{L}\) and change the final output as \(\hat{\mathbf{\epsilon}}_{\mathbf{L}}=\varphi_{L}\big{(}\frac{1}{N}\sum_{i=1}^{N}\mathbf{h}_{i} ^{(S)}\big{)}\) in Eq. (9) to break the equivariance, the model suffers from extreme performance detriment. Only 1.66% structures are successfully matched, which obviously implies the importance of incorporating O(3) equivariance. Furthermore, we introduce the chirality into the denoising model by adding \(\text{sign}(|\mathbf{L}|)\), the sign of the determinant of the lattice matrix, as an additional input in Eq. 6. The adapted model is \(SO(3)\)-invariant, but breaks the reflection symmetry and hence is NOT \(O(3)\)-invariant. There is no essential performance change, indicating the chirality is not quite crucial in distinguishing different crystal structures for the datasets used in this paper. **3.** We further assess the importance of periodic translation invariance from two perspectives. For the generation process, we generate \(\mathbf{F}\) via the score-based model with the Wrapped Normal (WN) distribution. We replace this module with DDPM under standard Gaussian as \(q(\mathbf{F}_{t}|\mathcal{M}_{0})=\mathcal{N}\big{(}\mathbf{F}_{t}|\sqrt{\alpha_{t}} \mathbf{F}_{0},(1-\bar{\alpha}_{t})\mathbf{I}\big{)}\) similarly defined as Eq. (3). A lower match rate and higher RMSE are observed for this variant. For the model architecture, we adopt Fourier Transformation(FT) in Eq. (6) to capture periodicity. To investigate its effect, we replace \(\psi_{\text{FT}}(\mathbf{f}_{j}-\mathbf{f}_{i})\) with \(\mathbf{f}_{j}-\mathbf{f}_{i}\), and the match rate drops from 51.49% to 29.15%. Both of the two observations verify the importance of retaining the periodic translation invariance. **4.** We further change the fully-connected graph into the multi-graph approach adopted in Xie and Grossman [15]. The multi-graph approach decreases the match rate, since the multi-graphs constructed under different intermediate structures may differ vibrantly during generation, leading to substantially higher training difficulty and lower sampling stability. We will provide more discussions in Appendix E.

### Ab Initio Crystal Generation

DiffCSP is extendable to ab initio crystal generation by further conducting discrete diffusion on atom types \(\mathbf{A}\). We contrast DiffCSP against five generative methods following [9]: **FTCP**[28], **Cond-DFC-VAE**[7], **G-SchNet**[60] and its periodic variant **P-G-SchNet**, and the orginal version of **CDVAE**[9]. Specifically for our DiffCSP, we gather the statistics of the atom numbers from the training set, then sample the number based on the pre-computed distribution similar to Hoogeboom et al. [41], which allows DiffCSP to generate structures of variable size. Following [9], we evaluate the generation performance in terms of there metrics: **Validity, Coverage**, and **Property statistics**, which respectively return the validity of the predicted crystals, the similarity between the test set and the generated samples, and the property calculation regarding density, formation energy, and the number of elements. The detailed definitions of the above metrics are provided in Appendix G.

**Results** Table 4 show that our method achieves comparable validity and coverage rate with previous methods, and significantly outperforms the baselines on the similarity of property statistics, which indicates the high reliability of the generated samples.

## 6 Discussions

**Limitation 1.** Composition generation. Our model yields slightly lower compositional validity in Table 4. We provide more discussion in Appendix G, and it is promising to propose more powerful generation methods on atom types. **2.** Experimental evaluation. Further wet-lab experiments can better verify the effectiveness of the model in real applications.

**Conclusion** In this work, we present DiffCSP, a diffusion-based learning framework for crystal structure prediction, which is particularly curated with the vital symmetries existing in crystals. The diffusion is highly flexible by jointly optimizing the lattice and fractional coordinates, where

\begin{table}
\begin{tabular}{l c c} \hline \hline  & Match rate (\%) \(\uparrow\) & RMSE \(\downarrow\) \\ \hline \multirow{2}{*}{DiffCSP} & \multicolumn{2}{c}{**51.49**} & \multicolumn{1}{c}{**0.0631**} \\ \cline{2-4}  & _w/o Joint Diffusion_ & \\ \hline \(\mathbf{L}\rightarrow\mathbf{F}\) & 50.03 & 0.0921 \\ \(\mathbf{F}\rightarrow\mathbf{L}\) & 36.73 & 0.0838 \\ \hline \multicolumn{4}{l}{_w/o O(3) Equivariance_} \\ \hline \multicolumn{4}{l}{_w/o_ inner product} & 1.66 & 0.4002 \\ \multicolumn{4}{l}{_w/ chirality_} & 49.68 & 0.0637 \\ \hline \multicolumn{4}{l}{_w/o Periodic Translation Invariance_} \\ \hline \multicolumn{4}{l}{_w/o_ WN} & 34.09 & 0.2350 \\ \multicolumn{4}{l}{_w/o_ FT} & 29.15 & 0.0926 \\ \hline \multicolumn{4}{l}{_MG Edge Construction_} \\ \hline MG _w/_ FT & 25.85 & 0.1079 \\ MG _w/o_ FT & 28.05 & 0.1314 \\ \hline \hline \end{tabular}
\end{table}
Table 3: Ablation studies on MP-20. MG: **M**ulti-**G**raph edge construction [15], FT: Fourier-Transformation.

the intermediate distributions are guaranteed to be invariant under necessary transformations. We demonstrate the efficacy of our approach on a wide range of crystal datasets, verifying the strong applicability of DiffCSP towards predicting high-quality crystal structures.

## 7 Acknowledgement

This work was jointly supported by the following projects: the National Natural Science Foundation of China (61925601 & 62006137); Beijing Nova Program (20230484278); the Fundamental Research Funds for the Central Universities, and the Research Funds of Renmin University of China (23XNKJ19); Alibaba Damo Research Fund; CCF-Ant Research Fund (CCF-AFSG RF20220204); National Key R&D Program of China (2022ZD0117805).

## References

* [1] Gautam R Desiraju. Cryptic crystallography. _Nature materials_, 1(2):77-79, 2002.
* [2] Keith T Butler, Daniel W Davies, Hugh Cartwright, Olexandr Isayev, and Aron Walsh. Machine learning for molecular and materials science. _Nature_, 559(7715):547-555, 2018.
* [3] Walter Kohn and Lu Jeu Sham. Self-consistent equations including exchange and correlation effects. _Physical review_, 140(4A):A1133, 1965.
* [4] Chris J Pickard and RJ Needs. Ab initio random structure searching. _Journal of Physics: Condensed Matter_, 23(5):053201, 2011.
* [5] Tomoki Yamashita, Nobuya Sato, Hiori Kino, Takashi Miyake, Koji Tsuda, and Tamio Oguchi. Crystal structure prediction accelerated by bayesian optimization. _Physical Review Materials_, 2(1):013803, 2018.
* [6] Artem R Oganov, Chris J Pickard, Qiang Zhu, and Richard J Needs. Structure prediction drives materials discovery. _Nature Reviews Materials_, 4(5):331-348, 2019.
* [7] Callum J Court, Batuhan Yildirim, Apoorv Jain, and Jacqueline M Cole. 3-d inorganic crystal structure generation and property prediction via representation learning. _Journal of chemical information and modeling_, 60(10):4518-4535, 2020.
* [8] Wenhui Yang, Edirisuriya M Dilanga Siriwardane, Rongzhi Dong, Yuxin Li, and Jianjun Hu. Crystal structure prediction of materials with high symmetry using differential evolution. _Journal of Physics: Condensed Matter_, 33(45):455902, 2021.
* [9] Tian Xie, Xiang Fu, Octavian-Eugen Ganea, Regina Barzilay, and Tommi S Jaakkola. Crystal diffusion variational autoencoder for periodic material generation. In _International Conference on Learning Representations_, 2021.

\begin{table}
\begin{tabular}{l l|c c c c c c c} \hline \hline \multirow{2}{*}{**Data**} & \multirow{2}{*}{**Method**} & \multicolumn{2}{c}{**Validity (\%) \(\uparrow\)**} & \multicolumn{2}{c}{**Coverage (\%) \(\uparrow\)**} & \multicolumn{3}{c}{**Property \(\downarrow\)**} \\  & & Struc. & Comp. & COV-R & COV-P & \(d_{\rho}\) & \(d_{E}\) & \(d_{\text{chem}}\) \\ \hline \multirow{2}{*}{Pervo-5} & FTCP [28] & 0.24 & 54.24 & 0.00 & 0.00 & 10.27 & 156.0 & 0.6297 \\  & Cond-DFC-VAE [7] & 73.60 & 82.95 & 73.92 & 10.13 & 2.268 & 4.111 & 0.8373 \\  & G-SchNet [60] & 99.92 & 98.79 & 0.18 & 0.23 & 1.625 & 4.746 & 0.0368 \\  & P-G-SchNet [60] & 79.63 & **99.13** & 0.37 & 0.25 & 0.2755 & 1.388 & 0.4552 \\  & CDVAE [9] & **100.0** & 98.59 & 99.45 & **98.46** & 0.1258 & 0.0264 & 0.0628 \\  & DiffCSP & **100.0** & 98.85 & **99.74** & 98.27 & **0.1110** & **0.0263** & **0.0128** \\ \hline \multirow{2}{*}{Carbon-24\({}^{3}\)} & FTCP [28] & 0.08 & \(-\) & 0.00 & 0.00 & 5.206 & 19.05 & \(-\) \\  & G-SchNet [60] & 99.94 & \(-\) & 0.00 & 0.00 & 0.9427 & 1.320 & \(-\) \\  & P-G-SchNet [60] & 48.39 & \(-\) & 0.00 & 0.00 & 1.533 & 134.7 & \(-\) \\  & CDVAE [9] & **100.0** & \(-\) & 99.80 & 83.08 & 0.1407 & 0.2850 & \(-\) \\  & DiffCSP & **100.0** & \(-\) & **99.90** & **97.27** & **0.0805** & **0.0820** & \(-\) \\ \hline \multirow{2}{*}{MP-20} & FTCP [28] & 1.55 & 48.37 & 4.72 & 0.09 & 23.71 & 160.9 & 0.7363 \\  & G-SchNet [60] & 99.65 & 75.96 & 38.33 & 99.57 & 3.034 & 42.09 & 0.6411 \\  & P-G-SchNet [60] & 77.51 & 76.40 & 41.93 & 99.74 & 4.04 & 2.448 & 0.6234 \\  & CDVAE [9] & **100.0** & **86.70** & 99.15 & 99.49 & 0.6875 & 0.2778 & 1.432 \\  & DiffCSP & **100.0** & 83.25 & **99.71** & **99.76** & **0.3502** & **0.1247** & **0.3398** \\ \hline \hline \end{tabular}
\end{table}
Table 4: Results on ab initio generation task. The results of baseline methods are from Xie et al. [9].

* Xu et al. [2021] Minkai Xu, Lantao Yu, Yang Song, Chence Shi, Stefano Ermon, and Jian Tang. Geodiff: A geometric diffusion model for molecular conformation generation. In _International Conference on Learning Representations_, 2021.
* Trippe et al. [2023] Brian L. Trippe, Jason Yim, Doug Tischer, David Baker, Tamara Broderick, Regina Barzilay, and Tommi S. Jaakkola. Diffusion probabilistic modeling of protein backbones in 3d for the motif-scaffolding problem. In _The Eleventh International Conference on Learning Representations_, 2023.
* Corso et al. [2023] Gabriele Corso, Hannes Stark, Bowen Jing, Regina Barzilay, and Tommi S. Jaakkola. Diff-dock: Diffusion steps, twists, and turns for molecular docking. In _The Eleventh International Conference on Learning Representations_, 2023.
* Shi et al. [2021] Chence Shi, Shitong Luo, Minkai Xu, and Jian Tang. Learning gradient fields for molecular conformation generation. In _International Conference on Machine Learning_, pages 9558-9568. PMLR, 2021.
* Jumper et al. [2018] John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin Zidek, Anna Potapenko, Alex Bridgland, Clemens Meyer, Simon A A Kohl, Andrew J Ballard, Andrew Cowie, Bernardino Romera-Paredes, Stanislav Nikolov, Rishub Jain, Jonas Adler, Trevor Back, Stig Petersen, David Reiman, Ellen Clancy, Michal Zielinski, Martin Steinegger, Michalina Pacholska, Tamas Berghammer, Sebastian Bodenstein, David Silver, Oriol Vinyals, Andrew W Senior, Koray Kavukcuoglu, Pushmeet Kohli, and Demis Hassabis. Highly accurate protein structure prediction with AlphaFold. _Nature_, 596(7873):583-589, 2021. doi: 10.1038/s41586-021-03819-2.
* Xie and Grossman [2018] Tian Xie and Jeffrey C. Grossman. Crystal graph convolutional neural networks for an accurate and interpretable prediction of material properties. _Phys. Rev. Lett._, 120:145301, Apr 2018. doi: 10.1103/PhysRevLett.120.145301.
* Jing et al. [2022] Bowen Jing, Gabriele Corso, Jeffrey Chang, Regina Barzilay, and Tommi Jaakkola. Torsional diffusion for molecular conformer generation. _arXiv preprint arXiv:2206.01729_, 2022.
* Wang et al. [2010] Yanchao Wang, Jian Lv, Li Zhu, and Yanming Ma. Crystal structure prediction via particle-swarm optimization. _Physical Review B_, 82(9):094116, 2010.
* Zhang et al. [2017] Yunwei Zhang, Hui Wang, Yanchao Wang, Lijun Zhang, and Yanming Ma. Computer-assisted inverse design of inorganic electrides. _Physical Review X_, 7(1):011017, 2017.
* Jacobsen et al. [2018] TL Jacobsen, MS Jorgensen, and B Hammer. On-the-fly machine learning of atomic potential in density functional theory structure optimization. _Physical review letters_, 120(2):026102, 2018.
* Podryabinkin et al. [2019] Evgeny V Podryabinkin, Evgeny V Tikhonov, Alexander V Shapeev, and Artem R Oganov. Accelerating crystal structure prediction by machine-learning interatomic potentials with active learning. _Physical Review B_, 99(6):064114, 2019.
* Cheng et al. [2022] Guanjian Cheng, Xin-Gao Gong, and Wan-Jian Yin. Crystal structure prediction by combining graph network and optimization algorithm. _Nature communications_, 13(1):1-8, 2022.
* Hoffmann et al. [2019] Jordan Hoffmann, Louis Maestrati, Yoshihide Sawada, Jian Tang, Jean Michel Sellier, and Yoshua Bengio. Data-driven approach to encoding and decoding 3-d crystal structures. _arXiv preprint arXiv:1909.00949_, 2019.
* Noh et al. [2019] Juhwan Noh, Jaehoon Kim, Helge S Stein, Benjamin Sanchez-Lengeling, John M Gregoire, Alan Aspuru-Guzik, and Yousung Jung. Inverse design of solid-state materials via a continuous representation. _Matter_, 1(5):1370-1384, 2019.
* Hu et al. [2020] Jianjun Hu, Wenhui Yang, and Edirisuriya M Dilanga Siriwardane. Distance matrix-based crystal structure prediction using evolutionary algorithms. _The Journal of Physical Chemistry A_, 124(51):10909-10919, 2020.
* Hu et al. [2021] Jianjun Hu, Wenhui Yang, Rongzhi Dong, Yuxin Li, Xiang Li, Shaobo Li, and Edirisuriya MD Siriwardane. Contact map based crystal structure prediction using global optimization. _CrystEngComm_, 23(8):1765-1776, 2021.

* [26] Asma Nouira, Nataliya Sokolovska, and Jean-Claude Crivello. Crystalgan: learning to discover crystallographic structures with generative adversarial networks. _arXiv preprint arXiv:1810.11203_, 2018.
* [27] Sungwon Kim, Juhwan Noh, Geun Ho Gu, Alan Aspuru-Guzik, and Yousung Jung. Generative adversarial networks for crystal structure prediction. _ACS central science_, 6(8):1412-1420, 2020.
* [28] Zekun Ren, Siyu Isaac Parker Tian, Juhwan Noh, Felipe Oviedo, Guangzong Xing, Jiali Li, Qiaohao Liang, Ruiming Zhu, Armin G. Aberle, Shijing Sun, Xiaonan Wang, Yi Liu, Qianxiao Li, Senthilnath Jayayeub, Kedar Hippalgaonkar, Yousung Jung, and Tonio Buonassisi. An invertible crystallographic representation for general inverse design of inorganic crystals with targeted properties. _Matter_, 2021. ISSN 2590-2385. doi: [https://doi.org/10.1016/j.matt.2021.11.032](https://doi.org/10.1016/j.matt.2021.11.032).
* [29] Kristof T Schutt, Huziel E Sauceda, P-J Kindermans, Alexandre Tkatchenko, and K-R Muller. Schnet-a deep learning architecture for molecules and materials. _The Journal of Chemical Physics_, 148(24):241722, 2018.
* [30] Nathaniel Thomas, Tess Smidt, Steven Kearnes, Lusann Yang, Li Li, Kai Kohlhoff, and Patrick Riley. Tensor field networks: Rotation-and translation-equivariant neural networks for 3d point clouds. _arXiv preprint arXiv:1802.08219_, 2018.
* [31] Fabian Fuchs, Daniel E. Worrall, Volker Fischer, and Max Welling. Se(3)-transformers: 3d rototranslation equivariant attention networks. In Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, _Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual_, 2020.
* [32] Victor Garcia Satorras, Emiel Hoogeboom, and Max Welling. E (n) equivariant graph neural networks. In _International Conference on Machine Learning_, pages 9323-9332. PMLR, 2021.
* [33] Philipp Tholke and Gianni De Fabritiis. Equivariant transformers for neural network based molecular potentials. In _International Conference on Learning Representations_, 2021.
* [34] Lowik Chanussot, Abhishek Das, Siddharth Goyal, Thibaut Lavril, Muhammed Shuaibi, Morgane Riviere, Kevin Tran, Javier Heras-Domingo, Caleb Ho, Weihua Hu, Aini Palizhati, Anuroop Sriram, Brandon Wood, Junwoong Yoon, Devi Parikh, C. Lawrence Zitnick, and Zachary Ulissi. Open catalyst 2020 (oc20) dataset and community challenges. _ACS Catalysis_, 2021. doi: 10.1021/acscatal.0c04525.
* [35] Richard Tran, Janice Lan, Muhammed Shuaibi, Siddharth Goyal, Brandon M Wood, Abhishek Das, Javier Heras-Domingo, Adeesh Kolluru, Ammar Rizvi, Nima Shoghi, et al. The open catalyst 2022 (oc22) dataset and challenges for oxide electrocatalysis. _arXiv preprint arXiv:2206.08917_, 2022.
* [36] Keqiang Yan, Yi Liu, Yuchao Lin, and Shuiwang Ji. Periodic graph transformers for crystal material property prediction. In _The 36th Annual Conference on Neural Information Processing Systems_, 2022.
* [37] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In _International Conference on Machine Learning_, pages 2256-2265. PMLR, 2015.
* [38] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. _Advances in Neural Information Processing Systems_, 33:6840-6851, 2020.
* [39] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models, 2021.
* [40] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. _arXiv preprint arXiv:2204.06125_, 2022.

* [41] Emiel Hoogeboom, Victor Garcia Satorras, Clement Vignac, and Max Welling. Equivariant diffusion for molecule generation in 3d. In _International Conference on Machine Learning_, pages 8867-8887. PMLR, 2022.
* [42] Shitong Luo, Yufeng Su, Xingang Peng, Sheng Wang, Jian Peng, and Jianzhu Ma. Antigen-specific antibody design and optimization with diffusion-based generative models for protein structures. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, _Advances in Neural Information Processing Systems_, 2022.
* [43] Valentin De Bortoli, Emile Mathieu, Michael John Hutchinson, James Thornton, Yee Whye Teh, and Arnaud Doucet. Riemannian score-based generative modelling. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, _Advances in Neural Information Processing Systems_, 2022.
* [44] Chin-Wei Huang, Milad Aghajohari, Joey Bose, Prakash Panangaden, and Aaron Courville. Riemannian diffusion models. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, _Advances in Neural Information Processing Systems_, 2022.
* [45] Detlef WM Hofmann and Joannis Apostolakis. Crystal structure prediction by data mining. _Journal of Molecular Structure_, 647(1-3):17-39, 2003.
* [46] Ralf W Grosse-Kunstleve, Nicholas K Sauter, and Paul D Adams. Numerically stable algorithms for the computation of reduced unit cells. _Acta Crystallographica Section A: Foundations of Crystallography_, 60(1):1-6, 2004.
* [47] Thomas N Kipf and Max Welling. Variational graph auto-encoders. _arXiv preprint arXiv:1611.07308_, 2016.
* [48] Alexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models. In _International Conference on Machine Learning_, pages 8162-8171. PMLR, 2021.
* [49] Yang Song and Stefano Ermon. Improved techniques for training score-based generative models. _Advances in neural information processing systems_, 33:12438-12448, 2020.
* [50] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. _arXiv preprint arXiv:2011.13456_, 2020.
* [51] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. _Advances in neural information processing systems_, 30, 2017.
* [52] Chi Chen, Weike Ye, Yunxing Zuo, Chen Zheng, and Shyue Ping Ong. Graph networks as a universal machine learning framework for molecules and crystals. _Chemistry of Materials_, 31(9):3564-3572, 2019.
* [53] Niklas WA Gebauer, Michael Gastegger, Stefaan SP Hessmann, Klaus-Robert Muller, and Kristof T Schutt. Inverse design of 3d molecular structures with conditional generative neural networks. _Nature communications_, 13(1):1-11, 2022.
* [54] Ivano E Castelli, David D Landis, Kristian S Thygesen, Soren Dahl, Ib Chorkendorff, Thomas F Jaramillo, and Karsten W Jacobsen. New cubic perovskites for one-and two-photon water splitting using the computational materials repository. _Energy & Environmental Science_, 5(10):9034-9043, 2012.
* [55] Ivano E Castelli, Thomas Olsen, Soumendu Datta, David D Landis, Soren Dahl, Kristian S Thygesen, and Karsten W Jacobsen. Computational screening of perovskite metal oxides for optimal solar light capture. _Energy & Environmental Science_, 5(2):5814-5819, 2012.
* [56] Chris J. Pickard. Airss data for carbon at 10gpa and the c+n+h+o system at 1gpa, 2020.
* [57] Anubhav Jain, Shyue Ping Ong, Geoffroy Hautier, Wei Chen, William Davidson Richards, Stephen Dacek, Shreyas Cholia, Dan Gunter, David Skinner, Gerbrand Ceder, et al. Commentary: The materials project: A materials genome approach to accelerating materials innovation. _APL materials_, 1(1):011002, 2013.

* [58] Shyue Ping Ong, William Davidson Richards, Anubhav Jain, Geoffroy Hautier, Michael Kocher, Shreyas Cholia, Dan Gunter, Vincent L Chevrier, Kristin A Persson, and Gerbrand Ceder. Python materials genomics (pymatgen): A robust, open-source python library for materials analysis. _Computational Materials Science_, 68:314-319, 2013.
* [59] Colin W Glass, Artem R Oganov, and Nikolaus Hansen. Uspex--evolutionary crystal structure prediction. _Computer physics communications_, 175(11-12):713-720, 2006.
* [60] Niklas Gebauer, Michael Gastegger, and Kristof Schutt. Symmetry-adapted generation of 3d point sets for the targeted discovery of molecules. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alche-Buc, E. Fox, and R. Garnett, editors, _Advances in Neural Information Processing Systems 32_, pages 7566-7578. Curran Associates, Inc., 2019.
* [61] Gerhard Kurz, Igor Gilitschenski, and Uwe D Hanebeck. Efficient evaluation of the probability density function of a wrapped normal distribution. In _2014 Sensor Data Fusion: Trends, Solutions, Applications (SDF)_, pages 1-5. IEEE, 2014.
* [62] James Bergstra, Daniel Yamins, and David Cox. Making a science of model search: Hyperparameter optimization in hundreds of dimensions for vision architectures. In _International conference on machine learning_, pages 115-123. PMLR, 2013.
* [63] Johannes Gasteiger, Shankari Giri, Johannes T. Margraf, and Stephan Gunnemann. Fast and uncertainty-aware directional message passing for non-equilibrium molecules. In _Machine Learning for Molecules Workshop, NeurIPS_, 2020.
* [64] Johannes Gasteiger, Florian Becker, and Stephan Gunnemann. Gemnet: Universal directional graph neural networks for molecules. In _Conference on Neural Information Processing Systems (NeurIPS)_, 2021.
* [65] Nils ER Zimmermann and Anubhav Jain. Local structure order parameters and site fingerprints for quantification of coordination environment and crystal structure similarity. _RSC advances_, 10(10):6063-6081, 2020.
* [66] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In _International conference on machine learning_, pages 8748-8763. PMLR, 2021.
* [67] Peter E Blochl. Projector augmented-wave method. _Physical review B_, 50(24):17953, 1994.
* [68] Georg Kresse and Jurgen Furthmuller. Efficiency of ab-initio total energy calculations for metals and semiconductors using a plane-wave basis set. _Computational materials science_, 6(1):15-50, 1996.
* [69] John P Perdew, Kieron Burke, and Matthias Ernzerhof. Generalized gradient approximation made simple. _Physical review letters_, 77(18):3865, 1996.
* [70] Emiel Hoogeboom, Didrik Nielsen, Priyank Jaini, Patrick Forre, and Max Welling. Argmax flows and multinomial diffusion: Learning categorical distributions. _Advances in Neural Information Processing Systems_, 34:12454-12465, 2021.
* [71] Clement Vignac, Igor Krawczuk, Antoine Siraudin, Bohan Wang, Volkan Cevher, and Pascal Frossard. Digress: Discrete denoising diffusion for graph generation. In _The Eleventh International Conference on Learning Representations_, 2023.
* [72] Daniel W Davies, Keith T Butler, Adam J Jackson, Jonathan M Skelton, Kazuki Morita, and Aron Walsh. Smact: Semiconducting materials by analogy and chemical theory. _Journal of Open Source Software_, 4(38):1361, 2019.
* [73] Logan Ward, Ankit Agrawal, Alok Choudhary, and Christopher Wolverton. A general-purpose machine learning framework for predicting properties of inorganic materials. _npj Computational Materials_, 2(1):1-7, 2016.

* [74] Min Zhao, Fan Bao, Chongxuan Li, and Jun Zhu. EGSDE: Unpaired image-to-image translation via energy-guided stochastic differential equations. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, _Advances in Neural Information Processing Systems_, 2022.
* [75] Fan Bao, Min Zhao, Zhongkai Hao, Peiyao Li, Chongxuan Li, and Jun Zhu. Equivariant energy-guided SDE for inverse molecular design. In _The Eleventh International Conference on Learning Representations_, 2023.

###### Contents

* 1 Introduction
* 2 Related Works
* 3 Preliminaries
* 4 The Proposed Method: DiffCSP
	* 4.1 Symmetries of Crystal Structure Distribution
	* 4.2 Joint Equivariant Diffusion
	* 4.3 The Architecture of the Denoising Model
* 5 Experiments
	* 5.1 Stable Structure Prediction
	* 5.2 Comparison with DFT-based Methods
	* 5.3 Ablation Studies
	* 5.4 Ab Initio Crystal Generation
* 6 Discussions
* 7 Acknowledgement
* A Theoretical Analysis
* A.1 Proof of Proposition 1
* A.2 Proof of Proposition 2
* A.3 Proof of Proposition 3
* A.4 Discussion on Periodic Translation Invariance
* B Implementation Details
* B.1 Approximation of the Wrapped Normal Distribution
* B.2 Adaptation of CDVAE
* B.3 Algorithms for Training and Sampling
* B.4 Hyper-parameters and Training Details
* C Exploring the Effects of Sampling and Candidate Ranking
* C.1 Impact of Sampling Numbers
* C.2 Diversity
* C.3 Ranking among Multiple Candidates
* D Impact of Noise Schedulers
* E Learning Curves of Different Variants
* F Comparison with DFT-based Methods

* 1 Implementation Details
	* 1.2 Results
* G Extension to More General Tasks
	* 1.1 Overview
	* 1.2 Ab Initio Generation
	* 1.3 Property Optimization
* H Computational Cost for Inference
* I Error Bars
* J More Visualizations

[MISSING_PAGE_FAIL:18]

For the transition probability \(p(\mathbf{L}_{t-1}|\mathbf{L}_{t},\mathbf{F}_{t},\mathbf{A})\), we have

\[p(\mathbf{QL}_{t-1}|\mathbf{QL}_{t},\mathbf{F}_{t},\mathbf{A}) =\mathcal{N}(\mathbf{QL}_{t-1}|a_{t}(\mathbf{QL}_{t}-b_{t}\hat{\mathbf{\epsilon }}_{\mathbf{L}}(\mathbf{QL}_{t},\mathbf{F}_{t},\mathbf{A},t)),\sigma_{t}^{2}\mathbf{I})\] \[=\mathcal{N}(\mathbf{QL}_{t-1}|a_{t}(\mathbf{QL}_{t}-b_{t}\mathbf{Q\hat{\epsilon }}_{\mathbf{L}}(\mathbf{L}_{t},\mathbf{F}_{t},\mathbf{A},t)),\sigma_{t}^{2}\mathbf{I})\] (O3)-equivariant \[\hat{\mathbf{\epsilon}}_{\mathbf{L}})\] \[=\mathcal{N}(\mathbf{QL}_{t-1}|\mathbf{Q}\Big{(}a_{t}(\mathbf{L}_{t}-b_{t} \hat{\mathbf{\epsilon}}_{\mathbf{L}}(\mathbf{L}_{t},\mathbf{F}_{t},\mathbf{A},t))\Big{)},\sigma_{t} ^{2}\mathbf{I})\] \[=\mathcal{N}(\mathbf{L}_{t-1}|a_{t}(\mathbf{L}_{t}-b_{t}\hat{\mathbf{\epsilon }}_{\mathbf{L}}(\mathbf{L}_{t},\mathbf{F}_{t},\mathbf{A},t)),\sigma_{t}^{2}\mathbf{I})\] (Eq. (11)) \[=p(\mathbf{L}_{t-1}|\mathbf{L}_{t},\mathbf{F}_{t},\mathbf{A}).\]

As the transition is \(O(3)\)-equivariant and the prior distribution \(\mathcal{N}(0,\mathbf{I})\) is \(O(3)\)-invariant, we prove that the the marginal distribution \(p(\mathbf{L}_{0})\) is \(O(3)\)-invariant based on lemma 1. 

### Proof of Proposition 2

Let \(\mathcal{N}_{w}(\mu,\sigma^{2}\mathbf{I})\) denote the wrapped normal distribution with mean \(\mu\), variance \(\sigma^{2}\) and period 1. We first provide the following lemma.

**Lemma 3**.: _If the denoising term \(\hat{\mathbf{\epsilon}}_{\mathbf{F}}(\mathbf{L}_{t},\mathbf{F}_{t},\mathbf{A},t)\) is periodic translation invariant, and the transition probabilty can be formulated as \(p(\mathbf{F}_{t-1}|\mathbf{L}_{t},\mathbf{F}_{t},\mathbf{A})=\mathcal{N}_{w}(\mathbf{F}_{t-1}|\mathbf{ F}_{t}+u_{t}\hat{\mathbf{\epsilon}}_{\mathbf{F}}(\mathbf{L}_{t},\mathbf{F}_{t},\mathbf{A},t),v_{t}^{2 }\mathbf{I})\), where \(u_{t},v_{t}\) are functions of \(t\), the transition is periodic translation equivariant._

Proof.: As the denoising term \(\hat{\mathbf{\epsilon}}_{\mathbf{F}}(\mathbf{L}_{t},\mathbf{F}_{t},\mathbf{A},t)\) is periodic translation invariant (for short PTI), we have \(\hat{\mathbf{\epsilon}}_{\mathbf{F}}(\mathbf{L}_{t},w(\mathbf{F}_{t}+\mathbf{t1}^{\top}),\mathbf{A},t) =\hat{\mathbf{\epsilon}}_{\mathbf{F}}(\mathbf{L}_{t},\mathbf{F}_{t},\mathbf{A},t)\), for any translation \(\mathbf{t}\in\mathbb{R}^{3}\).

For the wrapping function \(w(\cdot)\), we have

\[w(a+b)=w(w(a)+b),\forall a,b\in\mathbb{R}. \tag{12}\]

For wrapped normal distribution \(\mathcal{N}_{w}(\mu,\sigma^{2})\) with mean \(\mu\), variance \(\sigma^{2}\) and period 1, and for any \(k^{\prime},k^{\prime\prime}\in\mathbb{Z}\), we have

\[\mathcal{N}_{w}(x+k^{\prime}|\mu+k^{\prime\prime},\sigma^{2}) =\frac{1}{\sqrt{2\pi}\sigma}\sum_{k=-\infty}^{\infty}\exp\Big{(} -\frac{(x+k^{\prime}-(\mu+k^{\prime\prime})-k)^{2}}{2\sigma^{2}}\Big{)}\] \[=\frac{1}{\sqrt{2\pi}\sigma}\sum_{m=-\infty}^{\infty}\exp\Big{(} -\frac{(x-\mu-m)^{2}}{2\sigma^{2}}\Big{)}\] ( \[=\mathcal{N}_{w}(x|\mu,\sigma^{2})\] )

Let \(k^{\prime}=0,k^{\prime\prime}=w(\mu)-\mu\), we directly have

\[\mathcal{N}_{w}(x|w(\mu),\sigma^{2})=\mathcal{N}_{w}(x|\mu,\sigma^{2}). \tag{13}\]

For any \(t\in\mathbb{R}\), we have

\[\mathcal{N}_{w}(x+t|\mu+t,\sigma^{2}) =\frac{1}{\sqrt{2\pi}\sigma}\sum_{k=-\infty}^{\infty}\exp\Big{(}- \frac{(x+t-(\mu+t)-k)^{2}}{2\sigma^{2}}\Big{)}\] \[=\frac{1}{\sqrt{2\pi}\sigma}\sum_{k=-\infty}^{\infty}\exp\Big{(} -\frac{(x-\mu-k)^{2}}{2\sigma^{2}}\Big{)}\] \[=\mathcal{N}_{w}(x|\mu,\sigma^{2}).\]

Let \(k^{\prime}=w(x+t)-(x+t),k^{\prime\prime}=w(\mu+t)-(\mu+t)\), we have

\[\mathcal{N}_{w}(w(x+t)|w(\mu+t),\sigma^{2})=\mathcal{N}_{w}(x+t|\mu+t,\sigma^{ 2})=\mathcal{N}_{w}(x|\mu,\sigma^{2}). \tag{14}\]For the transition probability \(p(\mathbf{F}_{t-1}|\mathbf{L}_{t},\mathbf{F}_{t},\mathbf{A})\), we have

\[p(w(\mathbf{F}_{t-1}+\mathbf{t})|\mathbf{L}_{t},w(\mathbf{F}_{t}+\mathbf{t}\mathbf{1}^{\top }),\mathbf{A})\] \[= \mathcal{N}_{w}(w(\mathbf{F}_{t-1}+\mathbf{t}\mathbf{1}^{\top})|w(\mathbf{F}_{t}+ \mathbf{t}\mathbf{1}^{\top})+u_{t}\hat{\mathbf{\epsilon}}_{\mathbf{F}}(\mathbf{L}_{t},w(\mathbf{F}_{t}+ \mathbf{t}\mathbf{1}^{\top}),\mathbf{A},t),v_{t}^{2}\mathbf{I})\] \[= \mathcal{N}_{w}(w(\mathbf{F}_{t-1}+\mathbf{t}\mathbf{1}^{\top})|w(\mathbf{F}_{t}+ \mathbf{t}\mathbf{1}^{\top})+u_{t}\hat{\mathbf{\epsilon}}_{\mathbf{F}}(\mathbf{L}_{t},\mathbf{F}_{t}, \mathbf{A},t),v_{t}^{2}\mathbf{I})\] (PIT \[\hat{\mathbf{\epsilon}}_{\mathbf{F}}\] ) \[= \mathcal{N}_{w}(w(\mathbf{F}_{t-1}+\mathbf{t}\mathbf{1}^{\top})|w\Big{(}w(\bm {F}_{t}+\mathbf{t}\mathbf{1}^{\top})+u_{t}\hat{\mathbf{\epsilon}}_{\mathbf{F}}(\mathbf{L}_{t},\mathbf{F }_{t},\mathbf{A},t)\Big{)},v_{t}^{2}\mathbf{I})\] (Eq. ( 13 ) \[= \mathcal{N}_{w}(w(\mathbf{F}_{t-1}+\mathbf{t}\mathbf{1}^{\top})|w\Big{(}\mathbf{F} _{t}+u_{t}\hat{\mathbf{\epsilon}}_{\mathbf{F}}(\mathbf{L}_{t},\mathbf{F}_{t},\mathbf{A},t)+\mathbf{t} \Big{)},v_{t}^{2}\mathbf{I})\] (Eq. ( 12 ) \[= \mathcal{N}_{w}(\mathbf{F}_{t-1}|\mathbf{F}_{t}+u_{t}\hat{\mathbf{\epsilon}}_{ \mathbf{F}}(\mathbf{L}_{t},\mathbf{F}_{t},\mathbf{A},t),v_{t}^{2}\mathbf{I})\] (Eq. ( 14 ) \[= p(\mathbf{F}_{t-1}|\mathbf{L}_{t},\mathbf{F}_{t},\mathbf{A}).\]

The transition probability of the fractional coordinates during the Predictor-Corrector sampling can be formulated as

\[p(\mathbf{F}_{t-1}|\mathbf{L}_{t},\mathbf{F}_{t},\mathbf{A}) =p_{P}(\mathbf{F}_{t-\frac{1}{2}}|\mathbf{L}_{t},\mathbf{F}_{t},\mathbf{A})p_{C}( \mathbf{F}_{t-1}|\mathbf{L}_{t-1},\mathbf{F}_{t-\frac{1}{2}},\mathbf{A}),\] \[p_{P}(\mathbf{F}_{t-\frac{1}{2}}|\mathbf{L}_{t},\mathbf{F}_{t},\mathbf{A}) =\mathcal{N}_{w}(\mathbf{F}_{t-\frac{1}{2}}|\mathbf{F}_{t}+(\sigma_{t}^{2} -\sigma_{t-1}^{2})\hat{\mathbf{\epsilon}}_{\mathbf{F}}(\mathbf{L}_{t},\mathbf{F}_{t},\mathbf{A},t),\frac{\sigma_{t-1}^{2}(\sigma_{t}^{2}-\sigma_{t-1}^{2})}{\sigma_{t}^{2}}\mathbf{I }),\] \[p_{C}(\mathbf{F}_{t-1}|\mathbf{L}_{t-1},\mathbf{F}_{t-\frac{1}{2}},\mathbf{A}) =\mathcal{N}_{w}(\mathbf{F}_{t-\frac{1}{2}}|\mathbf{F}_{t}+\gamma\frac{ \sigma_{t-1}}{\sigma_{1}}\hat{\mathbf{\epsilon}}_{\mathbf{F}}(\mathbf{L}_{t-1},\mathbf{F}_{t- \frac{1}{2}},\mathbf{A},t-1),2\gamma\frac{\sigma_{t-1}}{\sigma_{1}}\mathbf{I}),\]

where \(p_{P},p_{C}\) are the transitions of the predictor and corrector. According to lemma 3, both of the transitions are periodic translation equivariant. Therefore, the transition \(p(\mathbf{F}_{t-1}|\mathbf{L}_{t},\mathbf{F}_{t},\mathbf{A})\) is periodic translation equivariant. As the prior distribution \(\mathcal{U}(0,1)\) is periodic translation invariant, we finally prove that the marginal distribution \(p(\mathbf{F}_{0})\) is periodic translation invariant based on lemma 1.

### Proof of Proposition 3

We rewrite proposition 3 as follows.

**Proposition 3**.: _The score \(\hat{\mathbf{\epsilon}}_{\mathbf{L}}\) by Eq. (9) is O(3)-equivariant, and the score \(\hat{\mathbf{\epsilon}}_{\mathbf{F}}\) from Eq. (10) is periodic translation invariant. Hence, the generated distribution by DiffCSP is periodic E(3) invariant._

Proof.: We first prove the orthogonal invariance of the inner product term \(\mathbf{L}^{\top}\mathbf{L}\). For any orthogonal transformation \(\mathbf{Q}\in\mathbb{R}^{3\times 3}\), \(\mathbf{Q}^{\top}\mathbf{Q}=\mathbf{I}\), we have

\[(\mathbf{QL})^{\top}(\mathbf{QL})=\mathbf{L}^{\top}\mathbf{Q}^{\top}\mathbf{QL}=\mathbf{L}^{\top}\mathbf{IL }=\mathbf{L}^{\top}\mathbf{L}.\]

For the Fourier Transformation, consider \(k\) is even, we have

\[\psi_{\text{FT}}(w(\mathbf{f}_{j}+\mathbf{t})-w(\mathbf{f}_{i}+\mathbf{t}))[c,k]\] \[= \sin\Big{(}2\pi m\big{(}w(f_{j,c}+t_{c})-w(f_{i,c}+t_{c})\big{)} \Big{)}\] \[= \sin\Bigg{(}2\pi m(f_{j,c}-f_{i,c})-2\pi m\Big{(}(f_{j,c}-f_{i,c })-(w(f_{j,c}+t_{c})-w(f_{i,c}+t_{c}))\Big{)}\Bigg{)}\] \[= \sin(2\pi m(f_{j,c}-f_{i,c}))\] \[= \psi_{\text{FT}}(\mathbf{f}_{j}-\mathbf{f}_{i})[c,k].\]

Similar results can be acquired as \(k\) is odd. Therefore, we have \(\psi_{\text{FT}}(w(\mathbf{f}_{j}+\mathbf{t})-w(\mathbf{f}_{i}+\mathbf{t}))=\psi_{\text{FT}}(\bm {f}_{j}-\mathbf{f}_{i}),\forall\mathbf{t}\in\mathbb{R}^{3}\), _i.e._, the Fourier Transformation \(\psi_{\text{FT}}\) is periodic translation invariant. According to the above, the message passing layers defined in Eq. (6)- (8) is periodic E(3) invariant. Hence, we can directly prove that the coordinate denoising term \(\hat{\mathbf{\epsilon}}_{\mathbf{F}}\) is periodic translation invariant. Let \(\hat{\mathbf{\epsilon}}_{\mathbf{l}}(\mathbf{L},\mathbf{F},\mathbf{A},t)=\varphi_{L}\big{(}\frac{1}{N }\sum_{i=1}^{N}\mathbf{h}_{i}^{(S)}\big{)}\). For the lattice denoising term \(\hat{\mathbf{\epsilon}}_{\mathbf{L}}=\mathbf{L}\hat{\mathbf{\epsilon}}_{\mathbf{l}}\), we have

\[\hat{\mathbf{\epsilon}}_{\mathbf{L}}(\mathbf{QL},\mathbf{F},\mathbf{A},t) =\mathbf{QL}\hat{\mathbf{\epsilon}}_{\mathbf{l}}(\mathbf{QL},\mathbf{F},\mathbf{A},t)\] \[=\mathbf{QL}\hat{\mathbf{\epsilon}}_{\mathbf{l}}(\mathbf{L},\mathbf{F},\mathbf{A},t)\] \[=\mathbf{Q}\hat{\mathbf{\epsilon}}_{\mathbf{L}}(\mathbf{L},\mathbf{F},\mathbf{A},t),\forall \mathbf{Q}\in\mathbb{R}^{3\times 3},\mathbf{Q}^{\top}\mathbf{Q}=\mathbf{I}.\]Above all, \(\hat{\mathbf{\epsilon}}_{\mathbf{L}}\) is O(3)-equivariant, and \(\hat{\mathbf{\epsilon}}_{\mathbf{F}}\) is periodic translation invariant. According to proposition I and 2, the generated distribution by DiffCSP in Algorithm 2 is periodic E(3) invariant. 

### Discussion on Periodic Translation Invariance

In Definition 3, we define the periodic translation invariance as a combination of translation invariance and periodicity. To see this, we illustrate an additional example in Figure 4. From a global view, when we translate all atom coordinates from left to right, the crystal structure remains unchanged, which indicates translation invariance. At the same time, from the view of a unit cell, the atom translated across the right boundary will be brought back to the left side owing to periodicity. Therefore, for convenience, we define the joint effect of translation invariance and periodicity as periodic translation invariance.

Previous works [36] have shown that shifting the periodic boundaries will not change the crystal structure. In this section, we further show that such periodic boundary shifting is equivalent to the periodic translation defined in Definition 3.

Consider two origin points \(\mathbf{p}_{1},\mathbf{p}_{2}\in\mathbb{R}^{3\times 1}\) and the lattice matrix \(\mathbf{L}\), the constructed unit cells by \(\mathbf{p}_{1},\mathbf{p}_{2}\) can be represented as \(\mathcal{M}_{1}=(\mathbf{A}_{1},\mathbf{F}_{1},\mathbf{L})\) and \(\mathcal{M}_{2}=(\mathbf{A}_{2},\mathbf{F}_{2},\mathbf{L})\), where \(\mathbf{F}_{1},\mathbf{F}_{2}\in\mathbb{R}^{3\times N}\) are fractional coordinates and

\[\{(\mathbf{a}^{\prime}_{1,i},\mathbf{x}^{\prime}_{1,j})|\mathbf{a}^{\prime}_{ 1,i}=\mathbf{a}_{1,i},\mathbf{x}^{\prime}_{1,i}=\mathbf{p}_{1}+\mathbf{L}\mathbf{f}_{1,i}+\mathbf{L} \mathbf{k},\forall\mathbf{k}\in\mathbb{Z}^{3\times 1}\} \tag{15}\] \[= \{(\mathbf{a}^{\prime}_{1,j},\mathbf{x}^{\prime}_{1,j})|\mathbf{a}^{\prime}_{ 1,j}=\mathbf{a}_{1,j},\mathbf{x}^{\prime}_{2,j}=\mathbf{p}_{2}+\mathbf{L}\mathbf{f}_{2,j}+\mathbf{L}\bm {k},\forall\mathbf{k}\in\mathbb{Z}^{3\times 1}\}, \tag{16}\]

which means that the unit cells formed by different origin points actually represent the same infinite crystal structures [36]. We further construct a bijection \(\mathcal{T}:\mathcal{M}_{1}\rightarrow\mathcal{M}_{2}\) mapping each atom in \(\mathcal{M}_{1}\) to the corresponding atom in unit cell \(\mathcal{M}_{2}\). For the pair \((a_{1,i},f_{1,i})\in\mathcal{M}_{1},(a_{2,j},f_{2,j})\in\mathcal{M}_{2}\), we have \(\mathcal{T}(a_{1,i},f_{1,i})=(a_{2,j},f_{2,j})\) iff \(\exists\mathbf{k}_{i}\in\mathbb{Z}^{3\times 1}\), _s.t._

\[\begin{cases}\mathbf{a}_{1,i}=\mathbf{a}_{2,j},\\ \mathbf{p}_{1}+\mathbf{L}\mathbf{f}_{1,i}+\mathbf{L}\mathbf{k}_{i}=\mathbf{p}_{2}+\mathbf{L}\mathbf{f}_{2,j}. \end{cases}\]

After proper transformation, we have

\[\mathbf{f}_{2,j}=\mathbf{f}_{1,i}+\mathbf{L}^{-1}(\mathbf{p}_{1}-\mathbf{p}_{2})+\mathbf{k}_{i}. \tag{17}\]

As \(\mathbf{f}_{1,i},\mathbf{f}_{2,i}\in[0,1)^{3\times 1}\), we have

\[\begin{cases}\mathbf{k}_{i}=-\lfloor\mathbf{f}_{1,i}+\mathbf{L}^{-1}(\mathbf{p}_{1}-\mathbf{p}_{2} )\rfloor,\\ \mathbf{f}_{2,j}=w\Big{(}\mathbf{f}_{1,i}+\mathbf{L}^{-1}(\mathbf{p}_{1}-\mathbf{p}_{2})\Big{)}, \end{cases}\]

which means shifting the periodic boundaries by changing the origin point \(\mathbf{p}_{1}\) into \(\mathbf{p}_{2}\) is equivalent to a periodic translation \(\mathbf{F}_{2}=w\Big{(}\mathbf{F}_{1}+\mathbf{L}^{-1}(\mathbf{p}_{1}-\mathbf{p}_{2})\mathbf{1}^{\top} \Big{)}\).

Figure 4: An example of periodic translation invariance. From the view of a unit cell, the atoms translated across the right boundary will be brought back to the left side.

Implementation Details

### Approximation of the Wrapped Normal Distribution

The Probability Density Function (PDF) of the wrapped normal distribution \(\mathcal{N}_{w}(0,\sigma_{t}^{2})\) is

\[\mathcal{N}_{w}(x|0,\sigma_{t}^{2})=\frac{1}{\sqrt{2\pi}\sigma_{t}}\sum_{k=- \infty}^{\infty}\exp\Big{(}-\frac{(x-k)^{2}}{2\sigma_{t}^{2}}\Big{)},\]

where \(x\in[0,1)\). Because the above series is convergent, it is reasonable to approximate the infinite summation to a finite truncated summation [61] as

\[f_{w,n}(x;0,\sigma_{t}^{2})=\frac{1}{\sqrt{2\pi}\sigma_{t}}\sum_{k=-n}^{n}\exp \Big{(}-\frac{(x-k)^{2}}{2\sigma_{t}^{2}}\Big{)}.\]

And the logarithmic gradient of \(f\) can be formulated as

\[\nabla_{x}\log f_{w,n}(x;0,\sigma_{t}^{2}) =\nabla_{x}\log\Bigg{(}\frac{1}{\sqrt{2\pi}\sigma_{t}}\sum_{k=-n} ^{n}\exp\Big{(}-\frac{(x-k)^{2}}{2\sigma_{t}^{2}}\Big{)}\Bigg{)}\] \[=\nabla_{x}\log\Bigg{(}\sum_{k=-n}^{n}\exp\Big{(}-\frac{(x-k)^{2} }{2\sigma_{t}^{2}}\Big{)}\Bigg{)}\] \[=\frac{\sum_{k=-n}^{n}(k-x)\exp\big{(}-\frac{(x-k)^{2}}{2\sigma_{ t}^{2}}\big{)}}{\sigma_{t}^{2}\sum_{k=-n}^{n}\exp\big{(}-\frac{(x-k)^{2}}{2 \sigma_{t}^{2}}\big{)}}\]

To estimate \(\lambda_{t}=\mathbb{E}_{x\sim\mathcal{N}_{w}(0,\sigma_{t}^{2})}^{-1}\big{[}\| \nabla_{x}\log\mathcal{N}_{w}(x|0,\sigma_{t}^{2})\|_{2}^{2}\big{]}\), we first sample \(m\) points from \(\mathcal{N}_{w}(0,\sigma_{t}^{2})\), and the expectation is approximated as

\[\tilde{\lambda}_{t} =\Big{[}\frac{1}{m}\sum_{i=1}^{m}\|\nabla_{x}\log f_{w,n}(x_{i};0,\sigma_{t}^{2})\|_{2}^{2}\Big{]}^{-1}\] \[=\Bigg{[}\frac{1}{m}\sum_{i=1}^{m}\Big{\|}\frac{\sum_{k=-n}^{n}(k -x_{i})\exp\big{(}-\frac{(x_{i}-k)^{2}}{2\sigma_{t}^{2}}\big{)}}{\sigma_{t}^{2 }\sum_{k=-n}^{n}\exp\big{(}-\frac{(x_{i}-k)^{2}}{2\sigma_{t}^{2}}\big{)}}\Big{\| }_{2}^{2}\Bigg{]}^{-1}.\]

For implementation, we select \(n=10\) and \(m=10000\).

### Adaptation of CDVAE

As illustrated in Figure 5, the original CDVAE [9] mainly consists of three parts: (1) a 3D encoder to encode the structure into the latent variable \(z_{3D}\), (2) a property predictor to predict the lattice \(\mathbf{L}\), the number of nodes in the unit cell \(N\), and the proportion of each element in the composition \(c\), (3) a 3D decoder to generate the structure from \(z_{3D},\mathbf{L},N,c\) via the Score Matching with Langevin Dynamics (SMLD, Song and Ermon [49]) method. The training objective is composed of the loss functions on the three parts, _i.e._ the KL divergence between the encoded distribution and the standard normal distribution \(\mathcal{L}_{KL}\), the aggregated prediction loss \(\mathcal{L}_{AGG}\) and the denoising loss on the decoder \(\mathcal{L}_{DEC}\). Formally, we have

\[\mathcal{L}_{ORI}=\mathcal{L}_{AGG}+\mathcal{L}_{DEC}+\beta D_{KL}\Big{(} \mathcal{N}(\mu_{3D},\sigma_{3D}^{2}\mathbf{I})\|\mathcal{N}(0,\mathbf{I})\Big{)}.\]

We formulate \(\mathcal{L}_{KL}=\beta D_{KL}(\mathcal{N}(\mu_{3D},\sigma_{3D}^{2}\mathbf{I})\| \mathcal{N}(0,\mathbf{I}))\) for better comparison with the adapted method. \(\beta\) is the hyper-parameter to balance the scale of the KL divergence and other loss functions.

To adapt the CDVAE framework to the CSP task, we apply two main changes. Firstly, for the encoder side, to take the composition as the condition, we apply an additional 1D prior encoder to encode the composition set into a latent distribution \(\mathcal{N}(\mu_{1D},\sigma_{1D}^{2}\mathbf{I})\) and minimize the KL divergence between the 3D and 1D distribution. The training objective is modified into

\[\mathcal{L}_{ADA}=\mathcal{L}_{AGG}+\mathcal{L}_{DEC}+\beta D_{KL}\Big{(} \mathcal{N}(\mu_{3D},\sigma_{3D}^{2}\mathbf{I})\|\mathcal{N}(\mu_{1D},\sigma_{1D}^ {2}\mathbf{I})\Big{)}.\]During the inference procedure, as the composition is given, the latent variable \(z_{1D}\) is sampled from \(\mathcal{N}(\mu_{1D},\sigma_{1D}^{2}\mathbf{I})\). For implementation, we apply a Transformer [51] without positional encoding as the 1D encoder to ensure the permutation invariance. Secondly, for the generation procedure, we apply the ground truth composition for initialization and keep the atom types unchanged during the Langevin dynamics to ensure the generated structure conforms to the given composition.

### Algorithms for Training and Sampling

Algorithm 1 summarizes the forward diffusion process as well as the training of the denoising model \(\phi\), while Algorithm 2 illustrates the backward sampling process. They can maintain the symmetries if \(\phi\) is delicately constructed. Notably, We apply the predictor-corrector sampler [50] to sample \(\mathbf{F}_{0}\). In Algorithm 2, Line 7 refers to the predictor while Lines 9-10 correspond to the corrector.

```
1:Input: lattice matrix \(\mathbf{L}_{0}\), atom types \(\mathbf{A}\), fractional coordinates \(\mathbf{F}_{0}\), denoising model \(\phi\), and the number of sampling steps \(T\).
2: Sample \(\mathbf{\epsilon}_{\mathbf{L}}\sim\mathcal{N}(\mathbf{0},\mathbf{I}),\mathbf{\epsilon}_{\mathbf{F}}\sim \mathcal{N}(\mathbf{0},\mathbf{I})\) and \(t\sim\mathcal{U}(1,T)\).
3:\(\mathbf{L}_{t}\leftarrow\sqrt{\bar{\alpha}_{\mathbf{t}}}\mathbf{L}_{0}+\sqrt{1-\bar{ \alpha}_{t}}\mathbf{\epsilon}_{\mathbf{L}}\)
4:\(\mathbf{F}_{t}\gets w(\mathbf{F}_{0}+\sigma_{t}\mathbf{\epsilon}_{\mathbf{F}})\)
5:\(\mathbf{\hat{\epsilon}}_{\mathbf{L}},\mathbf{\hat{\epsilon}}_{\mathbf{F}}\leftarrow\phi(\mathbf{L} _{t},\mathbf{F}_{t},\mathbf{A},t)\)
6:\(\mathcal{L}_{t}\leftarrow\|\mathbf{\epsilon}_{\mathbf{L}}-\hat{\mathbf{\epsilon}}_{\mathbf{L}} \|_{2}^{2}\)
7:\(\mathbf{\hat{\epsilon}}_{\mathbf{F}}\leftarrow\lambda_{t}\|\nabla_{\mathbf{F}_{0}}\log q(\bm {F}_{t}|\mathbf{F}_{0})-\hat{\mathbf{\epsilon}}_{\mathbf{F}}\|_{2}^{2}\)
8:Minimize \(\mathbf{\mathcal{L}}_{\mathbf{L}}+\mathbf{\mathcal{L}}_{\mathbf{F}}\)
```

**Algorithm 1** Training Procedure of DiffCSP

Figure 5: Overview of the original (a,b) and adapted (c,d) CDVAE. The key adaptations lie in two points. (1) We introduce an additional 1D prior encoder to fit the latent distribution of the given composition. (2) We initialize the generation procedure of the 3D decoder with the ground truth composition and keep the atom types unchanged to ensure the generated structure conforms to the given composition.

```
1:Input: atom types \(\mathbf{A}\), denoising model \(\phi\), number of sampling steps \(T\), step size of Langevin dynamics \(\gamma\).
2: Sample \(\mathbf{L}_{T}\sim\mathcal{N}(\mathbf{0},\mathbf{I})\),\(\mathbf{F}_{T}\sim\mathcal{U}(0,1)\).
3:for\(t\gets T,\cdots,1\)do
4: Sample \(\mathbf{\epsilon}_{L},\mathbf{\epsilon}_{F},\mathbf{\epsilon}_{F}\sim\mathcal{N}(\mathbf{0},\mathbf{ I})\)
5:\(\hat{\mathbf{\epsilon}}_{L},\hat{\mathbf{\epsilon}}_{F}\leftarrow\phi(\mathbf{L}_{t},\mathbf{F}_{ t},\mathbf{A},t)\).
6:\(\mathbf{L}_{t-1}\leftarrow\frac{\sqrt{\sigma_{t}}}{\sqrt{\sigma_{t}}}(\mathbf{L}_{t}- \frac{\beta_{t}}{\sqrt{1-\sigma_{t}}}\hat{\mathbf{\epsilon}}_{L})+\sqrt{\beta_{t} \cdot\frac{1-\delta_{t-1}}{1-\hat{\mathbf{\epsilon}}_{L}}}\mathbf{\epsilon}_{L}\).
7:\(\mathbf{F}_{t-\frac{1}{2}}\gets w(\mathbf{F}_{t}+(\sigma_{t}^{2}-\sigma_{t-1}^{2} )\hat{\mathbf{\epsilon}}_{F}+\frac{\sigma_{t-1}\sqrt{\sigma_{t}^{2}-\sigma_{t-1}^ {2}}}{\sigma_{t}}\mathbf{\epsilon}_{F})\)
8:\(\_\bar{\mathbf{\epsilon}}_{F}^{\prime}\leftarrow\phi(\mathbf{L}_{t-1},\mathbf{F}_{t-\frac{1 }{2}},\mathbf{A},t-1)\).
9:\(d_{t}\leftarrow\gamma\sigma_{t-1}/\sigma_{1}\)
10:\(\mathbf{F}_{t-1}\gets w(\mathbf{F}_{t-\frac{1}{2}}+d_{t}\hat{\mathbf{\epsilon}}_{F}+ \sqrt{2d_{t}}\mathbf{\epsilon}_{F}^{\prime})\).
11:endfor
12:Return\(\mathbf{L}_{0},\mathbf{F}_{0}\).
```

**Algorithm 2** Sampling Procedure of DiffCSP

### Hyper-parameters and Training Details

We acquire the origin datasets from CDVAE [9]4 and MPTS-52 [57]5. We utilize the codebases from GN-OA [21]6, cG-SchNet [53]7 and CDVAE [9]8 for baseline implementations.

Footnote 4: [https://github.com/txie-93/cdvae/tree/main/data](https://github.com/txie-93/cdvae/tree/main/data)

Footnote 5: [https://github.com/sparks-baird/mp-time-split](https://github.com/sparks-baird/mp-time-split)

Footnote 6: [http://www.comates.group/links?software=gn_oa](http://www.comates.group/links?software=gn_oa)

Footnote 7: [https://github.com/atomistic-machine-learning/cG-SchNet](https://github.com/atomistic-machine-learning/cG-SchNet)

Footnote 8: [https://github.com/txie-93/cdvae](https://github.com/txie-93/cdvae)

Footnote 9: [https://github.com/hyperopt/hyperopt](https://github.com/hyperopt/hyperopt)

Footnote 10: [https://github.com/guofei9987/scikit-opt](https://github.com/guofei9987/scikit-opt)

For the optimization methods, we apply the MEGNet [52] with 3 layers, 32 hidden states as property predictor. The model is trained for 1000 epochs with an Adam optimizer with learning rate \(1\times 10^{-3}\). As for the optimization algorithms, we apply RS, PSO, and BO according to Cheng et al. [21]. For RS and BO, We employ random search and TPE-based BO as implemented in Hyperopt [62]9. Specifically, we choose observation quantile \(\gamma\) as 0.25 and the number of initial random points as 200 for BO. For PSO, we used scikit-opt10 and choose the momentum parameter \(\omega\) as 0.8, the cognitive as 0.5, the social parameters as 0.5 and the size of population as 20.

Footnote 10: [https://github.com/guofei9987/scikit-opt](https://github.com/guofei9987/scikit-opt)

For P-cG-SchNet, we apply the SchNet [29] with 9 layers, 128 hidden states as the backbone model. The model is trained for 500 epochs on each dataset with an Adam optimizer with initial learning rate \(1\times 10^{-4}\) and a Plateau scheduler with a decaying factor 0.5 and a patience of 10 epochs. We select the element proportion and the number of atoms in a unit cell as conditions for the CSP task. For CDVAE, we apply the DimeNet++ [63] with 4 layers, 256 hidden states as the encoder and the GemNet-T [64] with 3 layers, 128 hidden states as the decoder. We further apply a Transformer [51] model with 2 layers, 128 hidden states as the additional prior encoder as proposed in Appendix B.2. The model is trained for 3500, 4000, 1000, 1000 epochs for Perov-5, Carbon-24, MP-20 and MPTS-52 respectively with an Adam optimizer with initial learning rate \(1\times 10^{-3}\) and a Plateau scheduler with a decaying factor 0.6 and a patience of 30 epochs. For our DiffCSP, we utilize the setting of 4 layer, 256 hidden states for Perov-5 and 6 layer, 512 hidden states for other datasets. The dimension of the Fourier embedding is set to \(k=256\). We apply the cosine scheduler with \(s=0.008\) to control the variance of the DDPM process on \(\mathbf{L}_{t}\), and an exponential scheduler with \(\sigma_{1}=0.005\), \(\sigma_{T}=0.5\) to control the noise scale of the score matching process on \(\mathbf{F}_{t}\). The diffusion step is set to \(T=1000\). Our model is trained for 3500, 4000, 1000, 1000 epochs for Perov-5, Carbon-24, MP-20 and MPTS-52 with the same optimizer and learning rate scheduler as CDVAE. For the step size \(\gamma\) in Langevin dynamics for the structure prediction task, we apply \(\gamma=5\times 10^{-7}\) for Perov-5, \(1\times 10^{-5}\) for MP-20 and MPTS-52, and for Carbon-24, we apply \(\gamma=5\times 10^{-6}\) to predict one sample and \(\gamma=5\times 10^{-7}\) for multiple samples. For the ab initio generation and optimization task on Perov-5, Carbon-24 and MP-20, we apply \(\gamma=1\times 10^{-6},1\times 10^{-5},5\times 10^{-6}\), respectively. All models are trained on GeForce RTX 3090 GPU.

Exploring the Effects of Sampling and Candidate Ranking

### Impact of Sampling Numbers

Figure 6 illustrates the impact of sampling numbers on the match rate. The match rate of all methods increases when sampling more candidates, and DiffCSP outperforms the baselines methods under the arbitrary number of samples.

### Diversity

We further evaluate the diversity by yielding the CrystalNN [65] fingerprint of each generated structure, calculating the L2-distances among all pairs in the 20 samples of each composition, collating the mean and max value of the distances, and finally averaging the results from all testing candidates. We list the diversity of CDVAE and DiffCSP on Perov-5 and MP-20 in Table 5.

### Ranking among Multiple Candidates

In SS 5.1, we match each candidate with the ground truth structure to pick up the best sample. However, in real CSP scenarios, the ground truth structure is not available, necessitating a confidence model to rank the generated candidates. To address this, we develop three types of confidence models to score each sample for ranking from different perspectives: **Energy Predictor (EP).** Since lower formation energy typically leads to more stable structures, we directly train a predictor using energy labels and apply the negative of the predicted

\begin{table}
\begin{tabular}{l c c} \hline \hline  & Match rate (\%) \(\uparrow\) & RMSE \(\downarrow\) \\ \hline EP & 51.96 & 0.0589 \\ MD (d=0.1) & 60.30 & 0.0357 \\ MD (d=0.3) & 60.13 & 0.0382 \\ MD (d=0.5) & 59.20 & 0.0469 \\ CS & 58.81 & 0.0443 \\ \hline Oracle & 77.93 & 0.0492 \\ \hline \hline \end{tabular}
\end{table}
Table 6: Results on different confidence models. _Oracle_ means applying the negative RMSD against the ground truth as the ranking score.

Figure 6: Comparison on different number of samples.

\begin{table}
\begin{tabular}{c c c c c} \hline \hline  & \multicolumn{2}{c}{Perov-5} & \multicolumn{2}{c}{MP-20} \\ \hline  & Mean & Max & Mean & Max \\ \hline CDVAE & 0.3249 & 0.7316 & **0.3129** & **0.6979** \\ DiffCSP & **0.3860** & **0.8911** & 0.2030 & 0.5292 \\ \hline \hline \end{tabular}
\end{table}
Table 5: Comparison on the diversity.

energy as the confidence score. **Match Discriminator (MD).** Inspired by Diffdock [12], we first generate five samples for each composition in the training/validation set using DiffCSP and calculate their RMSDs with the ground truth. We then train a binary classifier to predict whether the sample matches the ground truth with an RMSD below a threshold \(d\). The predicted probability serves as the score. **Contrastive Scorer (CS).** Drawing inspiration from CLIP [66], we train a contrastive model between a 1D and 3D model to align the corresponding compositions and structures. The inner product of the 1D and 3D models is used as the score. We select the Top-1 result among the 20 candidates ranked by each confidence model on the MP-20 dataset, as shown in Table 6. The results indicate that MD and CS perform relatively better, but there remains a gap between the heuristic ranking models and the oracle ranker. Designing powerful ranking models is an essential problem, which we leave for future research.

## Appendix D Impact of Noise Schedulers

We explore the noise schedulers from three perspectives and list the results in Table 7 and 8. **1.** For lattices, we originally use the cosine scheduler with \(=0.008\), and we change it into the linear and sigmoid schedulers with \(\beta_{1}=0.0001,\beta_{T}=0.02\). We find that the linear scheduler yields comparable results, while the sigmoid scheduler hinders the performance. **2.** For fractional coordinates, we use the exponential scheduler with \(\sigma_{min}=0.005\), \(\sigma_{max}=0.5\), and we change the value of \(\sigma_{max}\) into 0.1 and 1.0. Results show that the small-\(\sigma_{max}\) variant performs obviously worse, as only sufficiently large \(\sigma_{max}\) could approximate the prior uniform distribution. We visualize the PDF curves in Figure 7 for better understanding. **3.** For atom types, we conduct similar experiments as lattices, and the results indicate that the original cosine scheduler performs better. In conclusion, we suggest applying the proposed noise schedulers.

## Appendix E Learning Curves of Different Variants

We plot the curves of training and validation loss of different variants proposed in SS 5.3 in Figure 8 with the following observations. **1.** The multi-graph methods struggle with higher training and validation loss, as the edges constructed under different disturbed lattices vary significantly, complicating the training procedure. **2.** The Fourier transformation, expanding the relative coordinates and maintaining the periodic translation invariance, helps the model converge faster at the beginning of the training procedure. **3.** The variant utilizing the fully connected graph without the Fourier transformation (named "DiffCSP w/o FT" in Figure 8) encounters obvious overfitting as the periodic translation invariance is violated, highlighting the necessity to leverage the desired invariance into the model.

## Appendix F Comparison with DFT-based Methods

### Implementation Details

We utilize USPEX [59], a DFT-based software equipped with the evolutionary algorithm to search for stable structures. We use 20 populations in each generation, and end up with 20 generations for each compound. We set 60% of the lowest-enthalpy structures allowed to produce the next generation through heredity (50%), lattice mutation (10%), and atomic permutation (20%). Moreover, two

\begin{table}
\begin{tabular}{l r r} \hline \hline  & Match rate (\%) \(\uparrow\) & RMSE \(\downarrow\) \\ \hline DiffCSP & 51.49 & 0.0631 \\ \hline  & _Schedulers of \(\mathbf{L}\)_ & \\ \hline Linear & 50.06 & 0.0590 \\ Sigmoid & 45.24 & 0.0664 \\ \hline  & _Schedulers of \(\mathbf{F}\)_ & \\ \hline \(\sigma_{max}=0.1\) & 32.56 & 0.0913 \\ \(\sigma_{max}=1.0\) & 47.89 & 0.0675 \\ \hline \hline \end{tabular}
\end{table}
Table 7: CSP results on different schedulers. DiffCSP utilizes cosine scheduler on \(\mathbf{L}\) and \(\sigma_{max}=0.5\) on \(\mathbf{F}\).

\begin{table}
\begin{tabular}{l r r r r} \hline \hline  & \multicolumn{2}{c}{Validity} & \multicolumn{2}{c}{Coverage} \\ \hline  & Struct.(\%) & Comp.(\%) & Recall & Precision \\ \hline DiffCSP & 100.00 & 83.25 & 99.71 & 99.76 \\ \hline Linear & 99.70 & 79.78 & 98.29 & 99.48 \\ Sigmoid & 99.88 & 81.59 & 99.33 & 99.55 \\ \hline \hline \end{tabular}
\end{table}
Table 8: Ab initio generation results on different type schedulers. DiffCSP utilizes cosine scheduler on \(\mathbf{A}\).

lowest-enthalpy structures are allowed to survive into the next generation. The structural relaxations are calculated by the frozen-core all-electron projector augmented wave (PAW) method [67] as implemented in the Vienna ab initio simulation package (VASP) [68]. Each sample's calculation is performed using one node with 48 cores (Intel(R) Xeon(R) CPU E5-2692 v2 @ 2.20GHz), while the populations within the same generation are concurrently computed across 20 nodes in parallel. The exchange-correlation energy is treated within the generalized gradient approximation (GGA), using the Perdew-Burke-Ernzerhof (PBE) function [69].

Figure 8: Learning curves of different variants proposed in  5.3. MG and FT denote multi-graph edge construction and Fourier transformation, respectively.

Figure 7: PDF curves of the wrapped normal distribution with periodic as 1 and different noise scales. It can be find that \(\sigma=0.5\) is practically large enough to approximate the prior uniform distribution.

### Results

We select 10 binary and 5 ternary compounds in MP-20 testing set and compare our model with USPEX. For our method, we sample 20 candidates for each compound following the setting in Table 1. For USPEX, we apply 20 generations, 20 populations for each compound, and select the best sample in each generation, leading to 20 candidates as well. We list the minimum RMSD of each compound in Table 9, and additionally summarize the match rate over the 15 compounds, the averaged RMSD over the matched structures, and the averaged inference time to generate 20 candidates for each compound in Table 10. The results show that DiffCSP correctly predicts more structures with higher match rate and significantly lower time cost.

## Appendix G Extension to More General Tasks

### Overview

Our method mainly focuses on CSP, aiming at predicting the stable structures from the fixed composition \(\mathbf{A}\). We first enable the generation on atom types and extend to ab initio generation task in SS G.2, and then adopt the energy guidance for property optimization in SS G.3. Figure 9 illustrated the differences and connections of CSP, ab initio generation, and property optimization. Besides, CDVAE [9] proposes a reconstruction task specific for VAE-based models, which first encodes the ground truth structure, and require the model to recover the input structure. As our method follows a diffusion-based framework instead of VAE, it is unsuitable to conduct the reconstruction task of our method.

### Ab Initio Generation

Apart from \(\mathbf{L}\) and \(\mathbf{F}\), the ab initio generation task additionally requires the generation on \(\mathbf{A}\). As the atom types can be viewed as either \(N\) discrete variables in \(h\) classes, or the one-hot representation \(\mathbf{A}\in\mathbb{R}^{h\times N}\) in continuous space. We apply two lines of diffusion processes for the type generation, which are detailedly shown as follows.

**Multinomial Diffusion for Discrete Generation** Regarding the atom types as discrete features, we apply the multinomial diffusion with the following forward diffusion process [70; 42; 71],

\[q(\mathbf{A}_{t}|\mathbf{A}_{0})=\mathcal{C}(\mathbf{A}_{t}|\bar{\alpha}_{t}\mathbf{A}_{0}+ \frac{(1-\bar{\alpha}_{t})}{h}\mathbf{1}_{h}\mathbf{1}_{N}^{\top}), \tag{18}\]

\begin{table}
\begin{tabular}{l|c c c} \hline \hline  & Match Rate (\%)\(\uparrow\) & Avg. RMSD\(\downarrow\) & Avg. Time\(\downarrow\) \\ \hline USPEX & 53.33 & **0.0159** & 12.5h \\ DiffCSP & **73.33** & 0.0172 & **10s** \\ \hline \hline \end{tabular}
\end{table}
Table 10: Overall results over the 15 selected compounds.

\begin{table}
\begin{tabular}{l|c c c c c} \hline \hline Binary & Co\({}_{2}\)Sb\({}_{2}\) & Sr\({}_{2}\)O\({}_{4}\) & AlAg\({}_{4}\) & YMg\({}_{3}\) & Cr\({}_{4}\)Si\({}_{4}\) \\ \hline USPEX & 0.0008 & 0.0121 & N/A & 0.0000 & N/A \\ DiffCSP & 0.0005 & 0.0133 & 0.0229 & 0.0003 & 0.0066 \\ \hline Binary & Sn\({}_{4}\)Pd\({}_{4}\) & Ag\({}_{6}\)O\({}_{2}\) & Co\({}_{4}\)B\({}_{2}\) & Ba\({}_{2}\)Cd\({}_{6}\) & Bi\({}_{2}\)F\({}_{8}\) \\ \hline USPEX & 0.0184 & 0.0079 & 0.0052 & N/A & N/A \\ DiffCSP & 0.0264 & N/A & N/A & 0.0028 & N/A \\ \hline Ternary & KZnF\({}_{3}\) & Cr\({}_{3}\)CuO\({}_{8}\) & Bi\({}_{4}\)S\({}_{4}\)Cl\({}_{4}\) & Si\({}_{2}\)(CN\({}_{2}\))\({}_{4}\) & Hg\({}_{2}\)S\({}_{2}\)O\({}_{8}\) \\ \hline USPEX & 0.0123 & N/A & N/A & N/A & 0.0705 \\ DiffCSP & 0.0006 & 0.0482 & 0.0203 & N/A & 0.0473 \\ \hline \hline \end{tabular}
\end{table}
Table 9: The minimum RMSD of 20 candidates of USPEX and DiffCSP, N/A means none of the candidates match with the ground truth.

where \(\mathbf{1}_{h}\in\mathbb{R}^{h\times 1},\mathbf{1}_{N}\in\mathbb{R}^{N\times 1}\) are vectors with all elements setting to one, \(\mathbf{A}_{0}\) is the one-hot representation of the origin composition, and the function \(\mathcal{C}(\cdot)\) samples the multinomial distribution with the conditional probability and returns the one-hot representation of \(\mathbf{A}_{t}\).

The corresponding backward generation process is defined as

\[p(\mathbf{A}_{t-1}|\mathcal{M}_{t})=\mathcal{C}(\mathbf{A}_{t-1}|\tilde{ \theta}_{t}/\sum_{k=1}^{h}\tilde{\theta}_{t,k}), \tag{19}\]

where

\[\tilde{\theta}_{t}=\Big{(}\alpha_{t}\mathbf{A}_{t}+\frac{(1-\alpha_{t })}{h}\mathbf{1}_{h}\mathbf{1}_{N}^{\top}\Big{)}\odot\Big{(}\bar{\alpha}_{t}\hat{\epsilon }_{\mathbf{A}}(\mathcal{M}_{t},t)+\frac{(1-\bar{\alpha}_{t})}{h}\mathbf{1}_{h}\mathbf{1}_ {N}^{\top}\Big{)}, \tag{20}\]

and \(\hat{\epsilon}_{\mathbf{A}}\in\mathbb{R}^{h\times N}\) is predicted by the denoising model. We further find that specific for \(t=1\), \(\mathbf{A}_{0}=\text{argmax}(\hat{\epsilon}_{\mathbf{A}}(\mathcal{M}_{1},1))\) works better.

The training objective for multinomial diffusion is

\[\mathcal{L}_{\mathbf{A},\text{discrete}}=\mathbb{E}_{\mathbf{A}_{t}\sim q( \mathbf{A}_{t}|\mathbf{A}_{0}),t\sim\mathcal{U}(1,T)}\Big{[}\text{KL}(q(\mathbf{A}_{t-1}| \mathbf{A}_{t})||p(\mathbf{A}_{t-1}|\mathbf{A}_{t}))\Big{]}. \tag{21}\]

**One-hot Diffusion for Continuous Generation** Another approach is to simply consider the composition \(\mathbf{A}\) as a continuous variable in real space \(\mathbb{R}^{h\times N}\), which enables the application of standard DDPM-based method [41]. Similar to Eq. (2)-(4), the forward diffusion process is defined as

\[q(\mathbf{A}_{t}|\mathbf{A}_{0})=\mathcal{N}\Big{(}\mathbf{L}_{t}|\sqrt{ \bar{\alpha}_{t}}\mathbf{A}_{0},(1-\bar{\alpha}_{t})\mathbf{I}\Big{)}. \tag{22}\]

And the backward generation process is defined as

\[p(\mathbf{A}_{t-1}|\mathcal{M}_{t})=\mathcal{N}(\mathbf{A}_{t-1}|\mu_{ \mathbf{A}}(\mathcal{M}_{t}),\sigma_{\mathbf{A}}^{2}(\mathcal{M}_{t})\mathbf{I}), \tag{23}\]

where \(\mu_{\mathbf{A}}(\mathcal{M}_{t})=\frac{1}{\sqrt{\alpha_{t}}}\Big{(}\mathbf{A}_{t}- \frac{\beta_{t}}{\sqrt{1-\bar{\alpha}_{t}}}\hat{\epsilon}_{\mathbf{A}}(\mathcal{M}_ {t},t)\Big{)},\sigma_{\mathbf{A}}^{2}(\mathcal{M}_{t})=\beta_{t}\frac{1-\bar{ \alpha}_{t-1}}{1-\bar{\alpha}_{t}}\). The denoising term \(\hat{\mathbf{\epsilon}}_{\mathbf{A}}(\mathcal{M}_{t},t)\in\mathbb{R}^{h\times N}\) is predicted by the model.

The training objective for one-hot diffusion is

\[\mathcal{L}_{\mathbf{A},\text{continuous}}=\mathbb{E}_{\mathbf{c}_{\mathbf{A} }\sim\mathcal{N}(0,\mathbf{I}),t\sim\mathcal{U}(1,T)}[\|\mathbf{\epsilon}_{\mathbf{A}}- \hat{\epsilon}_{\mathbf{A}}(\mathcal{M}_{t},t)\|_{2}^{2}]. \tag{24}\]

The entire objective for training the joint diffusion model on \(\mathbf{L},\mathbf{F},\mathbf{A}\) is combined as

\[\mathcal{L}_{\mathcal{M}}=\lambda_{\mathbf{L}}\mathcal{L}_{\mathbf{L}}+ \lambda_{\mathbf{F}}\mathcal{L}_{\mathbf{F}}+\lambda_{\mathbf{A}}\mathcal{L}_{\mathbf{A}}. \tag{25}\]

Figure 9: Different tasks for crystal generation. Our approach mainly focuses on the CSP task (a), and is capable to extend into the ab initio generation task (b) by further generating the composition, and the property optimization task (c) via introducing the guidance on the target property. The reconstruction task (d) in Xie et al. [9] is specific for VAE, which is unnecessary for our diffusion-based method.

We select \(\lambda_{\mathbf{L}}=\lambda_{\mathbf{F}}=1,\lambda_{\mathbf{A}}=0\) for the CSP task as \(\mathbf{A}\) is fixed during the generation process, and \(\lambda_{\mathbf{L}}=\lambda_{\mathbf{F}}=1,\lambda_{\mathbf{A}}=20\) for the ab initio generation task to balance the scale of each loss component. Specifically, we do not optimize \(\mathcal{L}_{\mathbf{A}}\) on the Carbon-24 dataset, as all atoms in this dataset are carbon.

**Sample Structures with Arbitrary Numbers of Atoms** As the number of atoms in a unit cell (_i.e._\(N\)) is unchanged during the generation process, we first sample \(N\) according to the distribution of \(N\) in the training set, which is similar to Hoogeboom et al. [41]. The sampled distribution \(p(\mathcal{M})\) can be more concisely described as \(p(\mathcal{M},N)=p(N)p(\mathcal{M}|N)\). The former term \(p(N)\) is sampled from pre-computed data distribution, and the latter conditional distribution \(p(\mathcal{M}|N)\) is modeled by DiffCSP.

**Evaluation Metrics** The results are evaluated from three perspectives. **Validity**: We consider both the structural validity and the compositional validity. The structural valid rate is calculated as the percentage of the generated structures with all pairwise distances larger than 0.5A, and the generated composition is valid if the entire charge is neutral as determined by SMACT [72]. **Coverage**: It measures the structural and compositional similarity between the testing set \(\mathcal{S}_{t}\) and the generated samples \(\mathcal{S}_{g}\). Specifically, letting \(d_{S}(\mathcal{M}_{1},\mathcal{M}_{2}),d_{C}(\mathcal{M}_{1},\mathcal{M}_{2})\) denote the \(L2\) distances of the CrystalNN structural fingerprints [65] and the normalized Mapping compositional fingerprints [73], the COVergae Recall (COV-R) is determined as COV-R \(=\frac{1}{|\mathcal{S}_{t}|}|\{\mathcal{M}_{i}|\mathcal{M}_{i}\in\mathcal{S}_ {t},\exists\mathcal{M}_{j}\in\mathcal{S}_{g},d_{S}(\mathcal{M}_{i},\mathcal{M} _{j})<\delta_{S},d_{C}(\mathcal{M}_{i},\mathcal{M}_{j})<\delta_{C}\}|\) where \(\delta_{S},\delta_{C}\) are pre-defined thresholds. The COVergae Precision (COV-P) is defined similarly by swapping \(\mathcal{S}_{g},\mathcal{S}_{t}\). **Property statistics**: We calculate three kinds of Wasserstein distances between the generated and testing structures, in terms of density, formation energy, and the number of elements [9], denoted as \(d_{\rho}\), \(d_{E}\) and \(d_{\text{elem}}\), individually. The validity and coverage metrics are calculated on 10,000 generated samples, and the property metrics are evaluated on a subset with 1,000 samples passing the validity test.

**Results** We denote the abovementioned discrete and continuous generation methods as DiffCSP-D, DiffCSP-C, respectively. The results of the two variants and the strongest baseline CDVAE on MP-20 are provided in Table 11. We observe that DiffCSP-D yields slightly lower validity and coverage rates than DiffCSP-C. Moreover, DiffCSP-D tends to generate structures with more types of elements, which is far from the data distribution. Hence, we select DiffCSP-C for the other experiments in Table 4 (abbreviated as DiffCSP). We further find that DiffCSP-C supports the property optimization task in the next section. Besides, both variants have lower composition validity than CDVAE, implying that more powerful methods for composition generation are required. As our paper mainly focuses on the CSP task, we leave this for future studies.

### Property Optimization

On top of DiffCSP-C, we further equip our method with energy guidance [74; 75] for property optimization. Specifically we train a time-dependent property predictor \(E(\mathbf{L}_{t},\mathbf{F}_{t},\mathbf{A}_{t},t)\) with the same message passing blocks as Eq. (6)-(8). And the final prediction is acquired by the final layer as

\[E=\varphi_{E}(\frac{1}{N}\sum_{i=1}^{N}\mathbf{h}_{i}^{(S)}). \tag{26}\]

And the gradients _w.r.t._\(\mathbf{L},\mathbf{F},\mathbf{A}\) additionally guide the generation process. As the inner product term \(\mathbf{L}^{\top}\mathbf{L}\) is O(3) invariant, and the Fourier transformation term \(\phi_{\text{FT}}(\mathbf{f}_{j}-\mathbf{f}_{i})\) is periodic translation invariant, the predicted energy \(E\) is periodic E(3) invariant. That is,

\[E(\mathbf{QL}_{t},w(\mathbf{F}_{t}+\mathbf{t1}^{\top},\mathbf{A}_{t},t)=E(\mathbf{L}_{t},\mathbf{F}_{t },\mathbf{A}_{t},t). \tag{27}\]

\begin{table}
\begin{tabular}{l l|c c c c c c c} \hline \hline \multirow{2}{*}{**Data**} & \multirow{2}{*}{**Method**} & **Validity (\%)** \(\uparrow\) & **Coverage (\%)** \(\uparrow\) & \multicolumn{3}{c}{**Property \(\downarrow\)**} \\  & & Struc. & Comp. & COV-R & COV-P & \(d_{\rho}\) & \(d_{E}\) & \(d_{\text{elem}}\) \\ \hline MP-20 & CDVAE [9] & **100.0** & **86.70** & 99.15 & 99.49 & 0.6875 & 0.2778 & 1.432 \\  & DiffCSP-D & 99.70 & 83.11 & 99.68 & 99.53 & **0.1730** & 0.1366 & 0.9959 \\  & DiffCSP-C & **100.0** & 83.25 & **99.71** & **99.76** & 0.3502 & **0.1247** & **0.3398** \\ \hline \hline \end{tabular}
\end{table}
Table 11: Results on MP-20 ab initio generation task.

Taking gradient to both sides _w.r.t._\(\mathbf{L},\mathbf{F},\mathbf{A}\), respectively, we have

\[\mathbf{Q}^{\top}\nabla_{\mathbf{L}_{t}^{\prime}}E(\mathbf{L}_{t}^{\prime},w( \mathbf{F}_{t}+\mathbf{t}\mathbf{1}^{\top},\mathbf{A}_{t},t)|_{\mathbf{L}_{t}^{\prime}=\mathbf{Q}\mathbf{L} _{t}} =\nabla_{\mathbf{L}_{t}}E(\mathbf{L}_{t},\mathbf{F}_{t},\mathbf{A}_{t},t), \tag{28}\] \[\nabla_{\mathbf{F}_{t}^{\prime}}E(\mathbf{Q}\mathbf{L}_{t},\mathbf{F}_{t}^{\prime },\mathbf{A}_{t},t)|_{\mathbf{F}_{t}^{\prime}=w(\mathbf{F}_{t}+\mathbf{t}\mathbf{1}^{\top})} =\nabla_{\mathbf{F}_{t}}E(\mathbf{L}_{t},\mathbf{F}_{t},\mathbf{A}_{t},t),\] (29) \[\nabla_{\mathbf{A}_{t}}E(\mathbf{Q}\mathbf{L}_{t},w(\mathbf{F}_{t}+\mathbf{t}\mathbf{1}^{ \top},\mathbf{A}_{t},t) =\nabla_{\mathbf{A}_{t}}E(\mathbf{L}_{t},\mathbf{F}_{t},\mathbf{A}_{t},t), \tag{30}\]

which implies that the gradient to \(\mathbf{L}_{t}\) is O(3) equivariant, and the gradient to \(\mathbf{F}_{t}\) and \(\mathbf{A}_{t}\) is periodic E(3) invariant. Such symmetries maintain that the introduction of energy guidance does not violate the periodic E(3) invariance of the marginal distribution.

The detailed algorithm for energy-guided sampling is provided in Algorithm 3. We find that \(s=50\) works well on the three datasets. We evaluate the performance of the energy-guided model with the same metrics as Xie et al. [9]. We sample 100 structures from the testing set for optimization. For each structure, we apply \(T=1,000\) and \(T^{\prime}=100,200,\cdots,1,000\), leading to 10 optimized structures. We use the same independent property predictor as in Xie et al. [9] to select the best one from the 10 structures. We calculate the success rate (**SR**) as the percentage of the 100 optimized structures achieving 5, 10, and 15 percentiles of the property distribution. We select the formation energy per atom as the target property, and provide the results on Perov-5, Carbon-24 and MP-20 in Table 12, showcasing the notable superiority of DiffCSP over the baseline methods.

Aside from the Carbon-24 dataset, the composition is flexible in the above experiments. We also attempt to follow the CSP setting and optimize the crystal structures for lower energy based on the fixed composition. We visualize eight cases in Figure 10 and calculate the energies of the structures before and after optimization by VASP [68]. Results show that our method is capable to search novel structures with lower energies compared to existing ones.

\begin{table}
\begin{tabular}{l|c c c c c c c c} \hline \hline \multirow{2}{*}{**Method**} & \multicolumn{3}{c}{**Perov-5**} & \multicolumn{3}{c}{**Carbon-24**} & \multicolumn{3}{c}{**MP-20**} \\  & SR5 & SR10 & SR15 & SR5 & SR10 & SR15 & SR5 & SR10 & SR15 \\ \hline FTCP & 0.06 & 0.11 & 0.16 & 0.00 & 0.00 & 0.00 & 0.02 & 0.04 & 0.05 \\ Cond-DFC-VAE & 0.55 & 0.64 & 0.69 &  &  &  &  &  &  \\ CDVAE & 0.52 & 0.65 & 0.79 & 0.00 & 0.06 & 0.06 & 0.78 & 0.86 & 0.90 \\ \hline DiffCSP & **1.00** & **1.00** & **1.00** & **0.50** & **0.69** & **0.69** & **0.82** & **0.98** & **1.00** \\ \hline \hline \end{tabular}
\end{table}
Table 12: Results on property optimization task. The results of baselines are from Xie et al. [9].

## Appendix H Computational Cost for Inference

We provide the GPU hours (GeForce RTX 3090) for different generative methods to predict 20 candidates on the 4 datasets. Table 13 demonstrates the diffusion-based models (CDVAE and our DiffCSP) are slower than P-cG-SchNet. Yet, the computation overhead of our method is still acceptable given its clearly better performance than P-cG-SchNet. Additionally, our DiffCSP is much faster than CDVAE across all datasets mainly due to the fewer generation steps. CDVAE requires 5,000 steps for each generation, whereas our approach only requires 1,000 steps. We further compare the performance of CDVAE and DiffCSP with 1,000 and 5,000 generation steps on MP-20 in Table 14. Our findings indicate that both models exhibit improved performance with an increased number of steps. Notably, DiffCSP with 1,000 steps outperforms CDVAE with 5,000 steps.

## Appendix I Error Bars

We provide a single run to generate 20 candidates in Table 1. We apply two more inferences of each generative method on Perov-5 and MP-20. Table 15 shows similar results as Table 1.

## Appendix J More Visualizations

In this section, we first present additional visualizations of the predicted structures from DiffCSP and other generative methods in Figure 11. In line with Figure 3, our DiffCSP provides more accurate predictions compared with the baseline methods. Figure 12 illustrates 16 generated structures on Perov-5, Carbon-24 and MP-20. The visualization shows the capability of DiffCSP to generate diverse

\begin{table}
\begin{tabular}{l c c c c} \hline \hline  & Perov-5 & Carbon-24 & MP-20 & MPTS-52 \\ \hline P-cG-SchNet & 2.1 & 3.0 & 10.0 & 22.5 \\ CDVAE & 21.9 & 9.8 & 95.2 & 178.0 \\ DiffCSP & 1.5 & 3.2 & 18.4 & 34.0 \\ \hline \hline \end{tabular}
\end{table}
Table 13: GPU hours for yielding 20 candidates over the testing set.

\begin{table}
\begin{tabular}{l c c c c} \hline \hline  & Steps & Match rate (\%)\(\uparrow\) & RMSD\(\downarrow\) \\ \hline DiffCSP & 1,000 & 51.49 & 0.0631 \\  & 5,000 & 52.95 & 0.0541 \\ CDVAE & 1,000 & 30.71 & 0.1288 \\  & 5,000 & 33.90 & 0.1045 \\ \hline \hline \end{tabular}
\end{table}
Table 14: Results on MP-20 with different inference steps.

Figure 10: Visualization of 8 pairs of structures before and after optimization.

structures. We further visualize the generation process of 5 structures from MP-20 in Figure 13. We find that the generated structure \(\mathcal{M}_{0}\) is periodically translated from the ground truth structure, indicating that the marginal distribution \(p(\mathcal{M}_{0})\) follows the desired periodic translation invariance. We provide the detailed generation process in the Supplementary Materials.

Figure 11: Additional visualizations of the predicted structures from different methods. We translate the same atom to the origin for better visualization and comparison.

Figure 12: Visualization of the generated structures by DiffCSP.

Figure 13: Visualization of the generation process on MP-20. The column Translated means translating the same atom in the generated structure \(\mathcal{M}_{0}\) to the origin as the ground truth for better comparison.