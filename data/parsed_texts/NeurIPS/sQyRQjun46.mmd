Understanding and Addressing the Pitfalls of Bisimulation-based Representations in Offline Reinforcement Learning

 Hongyu Zang\({}^{1}\), Xin Li\({}^{1}\), Leiji Zhang\({}^{1}\), Yang Liu\({}^{2}\), Baigui Sun\({}^{2}\), Riashat Islam\({}^{3}\),

&Remi Tachet des Combes\({}^{4}\), Romain Laroche

\({}^{1}\) Beijing Institute of Technology, China \({}^{2}\) Alibaba Group, China

\({}^{3}\) McGill University, Mila, Quebec AI Institute, Canada \({}^{4}\) Wayve, UK

{zanghyu,xinli,ljzhang}@bit.edu.cn

{ly261666,baigui.sbg}@alibaba-inc.com

riashat.islam@mail.mcgill.ca

{remi.tachet,romain.laroche}@gmail.com

Correspondence to Xin Li.

###### Abstract

While bisimulation-based approaches hold promise for learning robust state representations for Reinforcement Learning (RL) tasks, their efficacy in offline RL tasks has not been up to par. In some instances, their performance has even significantly underperformed alternative methods. We aim to understand why bisimulation methods succeed in online settings, but falter in offline tasks. Our analysis reveals that missing transitions in the dataset are particularly harmful to the bisimulation principle, leading to ineffective estimation. We also shed light on the critical role of reward scaling in bounding the scale of bisimulation measurements and of the value error they induce. Based on these findings, we propose to apply the expectile operator for representation learning to our offline RL setting, which helps to prevent overfitting to incomplete data. Meanwhile, by introducing an appropriate reward scaling strategy, we avoid the risk of feature collapse in representation space. We implement these recommendations on two state-of-the-art bisimulation-based algorithms, MICo and SimSR, and demonstrate performance gains on two benchmark suites: D4RL and Visual D4RL. Codes are provided at https://github.com/zanghyu/Offline_Bisimulation.

## 1 Introduction

Reinforcement learning (RL) algorithms often require a significant amount of data to achieve optimal performance [40; 48; 22]. In scenarios where collecting data is costly or impractical, Offline RL methods offer an attractive alternative by learning effective policies from previously collected data [29; 43; 32; 35; 16; 24]. However, capturing the complex structure of the environment from limited data remains a challenge for Offline RL [4]. This involves pre-training the state representation on offline data and then learning the policy upon the fixed representations [51; 47; 41; 53]. Though driven by various motivations, previous methods can be mainly categorized into two classes: i) implicitly shaping the agent's representation of the environment via prediction and control of some aspects of the environment through auxiliary tasks, _e.g._, maximizing the diversity of visited states [34; 10], exploring attentive contrastive learning on sub-trajectories [51], or capturing temporal information about the environment [47]; ii) utilizing _behavioral metrics_, such as bisimulation metrics [11; 13; 5], to capture complex structure in the environment by measuring the similarity of behavior on the representations [52; 7]. The former methods have proven their effectiveness theoretically and empirically in Offline settings [41; 47; 51], while the adaptability of the latter approaches in the context of limited datasets remains unclear. This paper tackles this question.

Bisimulation-based approaches, as their name suggests, utilize the bisimulation metrics update operator to construct an auxiliary loss and learn robust state representations. These representations encapsulate the behavioral similarities between states by considering the difference between their rewards and dynamics. While the learned representations possess several desirable properties, such as smoothness [19], visual invariance [54; 1; 52], and task adaptation [56; 37; 46; 8], bisimulation-based objectives in most approaches are required to be coupled with the policy improvement procedure [54; 6; 52]. In Offline RL, pretraining state representations via bisimulation-based methods is supposed to be cast as a special case of on-policy bisimulation metric learning where the behavior policy is fixed so that good performance should ensue. However, multiple recent studies [51; 21] suggest that bisimulation-based algorithms yield significantly poorer results on Offline tasks compared to a variety of (self-)supervised objectives.

In this work, we highlight problems with using the bisimulation principle as an objective in Offline settings. We aim to provide a theoretical understanding of the performance gap in bisimulation-based approaches between online and offline settings:"_why do bisimulation approaches perform well in Online RL tasks but tend to fail in Offline RL ones?_" By establishing a connection between the Bellman and bisimulation operators, we uncover that missing transitions, which often occur in Offline settings, can cause the bisimulation principle to be compromised. This means that the bisimulation estimator can be ineffective in finite datasets. Moreover, we notice that the scale of the reward impacts the upper bounds of both the bisimulation measurement2 fixed point and the value error. This scaling term, if not properly handled, can potentially lead to representation collapse.

Footnote 2: Since some bisimulation-based approaches do not exactly use metrics but instead of pseudometrics, diffuse metrics or else, we will use the term “measurement” in the following.

To alleviate the aforementioned issues, we propose to learn state representations based on the expectile operator. With this asymmetric operator predicting expectiles of the representation distribution, we can achieve a balance between the behavior measurement and the greedy assignment of the measurement over the dataset. This results in a form of regularization over the bisimulation measurement, thus preventing overfitting to the incomplete data, and implicitly avoiding out-of-distribution estimation errors. Besides, by considering the specific properties of different bisimulation measurements, we investigate the representation collapse issue for the ones that are instantiated with bounded distances (_e.g._, cosine distance) and propose a way to scale rewards that reduces collapse. We integrate these improvements mainly on two bisimulation-based baselines, MICo [7] and SimSR [52], and show the effectiveness of the proposed modifications.

The primary contributions of this work are as follows:

* We investigate the potential harm of directly applying the bisimulation principle in Offline settings, prove that the bisimulation estimator can be ineffective in finite datasets, and emphasize the essential role of reward scaling.
* We propose theoretically motivated modifications on two representative bisimulation-based baselines, including an expectile-based operator and a tailored reward scaling strategy. These proposed changes are designed to address the challenges encountered when applying the bisimulation principle in offline settings.
* We demonstrate the superior performance our approach yields through an empirical study on two benchmark suites, D4RL [15] and Visual D4RL [35].

## 2 Related Work

State representation learning in Offline RLPretraining representations has been recently studied in Offline RL settings, where several studies presented its effectiveness [3; 47; 41; 25]. In this paradigm, we learn state representations on pre-collected datasets before value estimation or policy improvement steps are run. The learned representation can then be used for subsequent policy learning, either online or offline. Some typical auxiliary tasks for pretraining state representations include capturing the dynamical [42] and temporal [47] information of the environment, exploring attentive contrastive learning on sub-trajectories [51], or improving policy performance by applying data augmentations techniques to the pixel-based inputs [9; 35].

Bisimulation-based methodsThe pioneer works by [20; 33] aim to overcome the curse of dimensionality by defining equivalence relations between states to reduce system complexity. However, these approaches are impractical as they usually demand an exact match of transition distributions. To address this issue, [12; 14] propose a bisimulation metric to aggregate similar states. This metric quantifies the similarity between two states and serves as a distance measure to allow efficient state aggregation. Unfortunately, it remains computationally expensive as it requires a full enumeration of states. Later, [5] devise an on-policy bisimulation metric for policy evaluation, providing a scalable method for computing state similarity. Building upon this, [54] develop a metric to learn state representations by modeling the latent dynamic transition as Gaussian. [6] further investigate the independent couple sampling strategy to reduce the computational complexity of representation learning, whereas [52] propose to learn state representations built on the cosine distance to alleviate a representation collapse issue. Despite the promising results obtained, one of the major remaining challenges in this paradigm is its dependency on coupling state representation learning with policy training. This is not always suitable for Offline settings, given that obtaining on-policy reward and transition differences is infeasible due to our inability to gather additional agent-environment interactions. To adapt bisimulation-based approaches to Offline settings, one solution is to consider the policy over the dataset as a specific behavior policy, and then apply the bisimulation principle on it to learn state representations in a pretraining stage, thus disentangling policy training from bisimulation-based learning. Notably, although there exist recent studies [51; 42] investigating the potential of bisimulation-based methods to pretrain state representations, it has not yielded satisfactory results yet [51].

## 3 Preliminaries

### Offline RL

We consider the standard Markov decision process (MDP) framework, in which the environment is given by a tuple \(\mathcal{M}=(\mathcal{S},\mathcal{A},T,r,\gamma)\), with state space \(\mathcal{S}\), action space \(\mathcal{A}\), transition function \(T\) that decides the next state \(s^{\prime}\sim T(\cdot|s,a)\), reward function \(r(s,a)\) bounded by \([R_{\text{min}},R_{\text{max}}]\), and a discount factor \(\gamma\in[0,1)\). The agent in state \(s\in\mathcal{S}\) selects an action \(a\in\mathcal{A}\) according to its policy, mapping states to a probability distribution over actions: \(a\sim\pi(\cdot|s)\). We make use of the state value function \(V^{\pi}(s)=\mathbb{E}_{\mathcal{M},\pi}\left[\sum_{t=0}^{\infty}\gamma^{t}r\left( s_{t},a_{t}\right)\mid s_{0}=s\right]\) to describe the long term discounted reward of policy \(\pi\) starting at state \(s\). In the sequel, we use \(T_{s}^{a}\) and \(r_{s}^{a}\) to denote \(T(\cdot|s,a)\) and \(r(s,a)\), respectively. In Offline RL, we are given a fixed dataset of environment interactions that include \(N\) transition samples, _i.e._\(\mathcal{D}=\{s_{i},a_{i},s^{\prime}_{i},r_{i}\}_{i=1}^{N}\). We assume that the dataset \(\mathcal{D}\) is composed of trajectories generated i.i.d. under the control of a behavior policy \(\pi_{\beta}\), whose state occupancy is denoted by \(\mu_{\beta}(s)\).

### Bisimulation-based Update Operator

The concept of bisimulation is used to establish equivalence relations on states. This is done recursively by considering two states as equivalent if they have the same distribution over state transitions and the same immediate reward [30; 20]. Since bisimulation considers worst-case differences between states, it commonly results in "pessimistic" outcomes. To address this limitation, the \(\pi\)-bisimulation metric was proposed in [5]. This new metric only considers actions induced by a given policy \(\pi\) rather than all actions when measuring the behavior distance between states:

**Theorem 1**.: _[_5_]_ _Let \(\mathbb{M}\) be the set of all measurements on \(\mathcal{S}\). Define \(\mathcal{F}^{\pi}:\mathbb{M}\rightarrow\mathbb{M}\) by_

\[\mathcal{F}^{\pi}(g)(s_{i},s_{j})=|r_{s_{i}}^{\pi}-r_{s_{j}}^{\pi}|+\gamma \mathcal{W}(g)\left(T_{s_{i}}^{\pi},T_{s_{j}}^{\pi}\right)\] (1)

_where \(s_{i},s_{j}\in\mathcal{S}\), \(r_{s_{i}}^{\pi}=\sum_{a\in\mathcal{A}}\pi(a|s_{i})r_{s_{i}}^{a}\), \(T_{s_{i}}^{\pi}=\sum_{a\in\mathcal{A}}\pi(a|s_{i})T_{s_{i}}^{a}\), and \(\mathcal{W}(g)\) is the Wasserstein distance with cost function \(g\) between distributions. Then \(\mathcal{F}^{\pi}\) has a least fixed point \(g_{\sim}^{\pi}\), and \(g_{\sim}^{\pi}\) is a \(\pi\)-bisimulation metric._

Although it is feasible to compute the behavior difference measurement \(g_{\sim}^{\pi}\) by applying the operator \(\mathcal{F}^{\pi}\) iteratively (which is guaranteed to converge to a fixed point since \(\mathcal{F}^{\pi}\) is a contraction), this approach comes at a high computational complexity due to the Wasserstein distance on the right-handside of the equation. To tackle this issue, MICo [6] proposed using an independent couple sampling strategy instead of optimizing the overall coupling of the distributions \(T_{s_{i}}^{\pi}\) and \(T_{s_{j}}^{\pi}\), resulting in a novel measurement to evaluate the difference between states. Additionally, SimSR [52] further explored the potentiality of combining the cosine distance with bisimulation-based measurements to learn state representations. Both works can be generalized as:

\[\mathcal{F}^{\pi}G^{\pi}(s_{i},s_{j})=|r_{s_{i}}^{\pi}-r_{s_{j}}^{\pi}|+\gamma \mathbb{E}_{\begin{subarray}{c}s_{i}^{\prime}\sim T_{s_{j}}^{\pi}\\ s_{j}^{\prime}\sim T_{s_{j}}^{\pi}\end{subarray}}[G^{\pi}(s_{i}^{\prime},s_{ j}^{\prime})],\] (2)

and \(\mathcal{F}^{\pi}\) has a least fixed point \(G_{\sim}^{\pi}\)3. The instantiation of \(G\) varies in different approaches [6; 52]. For example, in SimSR [52], the cosine distance is used to instantiate \(G\) on the embedding space, and the dynamics difference is computed by the cosine distance between the next-state pair \((s_{i}^{\prime},s_{j}^{\prime})\) sampled from a transition model of the environment. A more detailed description can be found in Appendix C.

Footnote 3: For readability, we will conflate the notations \(G^{\pi}\) and \(G^{\pi}(x,y)\), they are the same if not specified

**Lemma 2**.: _[_6_]_ _(**Lifted MDP**) The bisimulation-based update operator \(\mathcal{F}^{\pi}\) for \(\mathcal{M}\) is the Bellman evaluation operator for a specific lifted MDP._

Due to this interpretation of the bisimulation-based update operator as the Bellman evaluation operator in a lifted MDP, we can derive certain conclusions about bisimulation by drawing inspiration from policy evaluation methods. In the next section, we will borrow analytical ideas from [17] to prove that the bisimulation-based objective may be ineffective for finite datasets. We summarize all notations in Appendix A and provide all proofs in Appendix D.

## 4 Ineffective Bisimulation Estimators in Finite Datasets

The high-level idea of bisimulation-based state representation learning is to learn state embeddings such that when states are projected onto the embedding space, their behavioral similarity is maintained. We denote our parameterized state encoder by \(\phi:\mathcal{S}\rightarrow\mathbb{R}^{n}\) and a distance \(D(\cdot,\cdot)\) in the embedding space \(\mathbb{R}^{n}\) by \(G_{\phi}^{\pi}(s_{i},s_{j})\doteq D(\phi(s_{i}),\phi(s_{j}))\). For instance, \(D(\cdot,\cdot)\) may be the Lukaszyk-Karmowski distance [6] or the cosine distance [52]. To avoid unnecessary confusion, we defer implementation details to Section 5.

When considering bisimulation-based state representations, the goal is to acquire stable state representations under policy \(\pi\) via the measurement \(G_{\sim}^{\pi}\). The primary focus is usually to minimize a loss over the _bisimulation error_, denoted by \(\Delta_{\phi}^{\pi}\), which measures the distance between the approximation \(G_{\phi}^{\pi}\) and the fixed point \(G_{\sim}^{\pi}\):

\[\Delta_{\phi}^{\pi}(s_{i},s_{j}):=|G_{\phi}^{\pi}(s_{i},s_{j})-G_{\sim}^{\pi} (s_{i},s_{j})|.\] (3)

However, since the fixed point \(G_{\sim}^{\pi}\) is unobtainable without full knowledge of the underlying MDP, this approximation error is often unknown. Recall that in Lemma 2, we have shown that we can connect a bisimulation-based update operator to a lifted MDP. Taking inspiration from Bellman evaluation for the value function, we define the _bisimulation Bellman residual_\(\epsilon_{\phi}^{\pi}\) as:

\[\epsilon_{\phi}^{\pi}(s_{i},s_{j}):=|G_{\phi}^{\pi}(s_{i},s_{j})-\mathcal{F}^{ \pi}G_{\phi}^{\pi}(s_{i},s_{j})|.\] (4)

Then, we can connect the bisimulation Bellman residual with the bisimulation error by the following:

**Theorem 3**.: _(**Bisimulation error upper-bound**). Let \(\mu_{\pi}(s)\) denote the stationary distribution over states, let \(\mu_{\pi}(\cdot,\cdot)\) denote the joint distribution over synchronized pairs of states \((s_{i},s_{j})\) sampled independently from \(\mu_{\pi}(\cdot)\). For any state pair \((s_{i},s_{j})\in\mathcal{S}\times\mathcal{S}\), the bisimulation error \(\Delta_{\phi}^{\pi}(s_{i},s_{j})\) can be upper-bounded by a sum of expected bisimulation Bellman residuals \(\epsilon_{\phi}^{\pi}\):_

\[\Delta_{\phi}^{\pi}(s_{i},s_{j})\leq\frac{1}{1-\gamma}\mathbb{E}_{(s_{i}^{ \prime},s_{j}^{\prime})\sim\mu_{\pi}}\left[\epsilon_{\phi}^{\pi}(s_{i}^{\prime },s_{j}^{\prime})\right].\] (5)

Thereafter, the bisimulation Bellman residual is used as a surrogate objective to approximate the fixed point \(G_{\sim}^{\pi}\) when learning our state representation. Indeed, the minimization of the bisimulation Bellman residual objective over all pairs \((s_{i}^{\prime},s_{j}^{\prime})\sim\mu_{\pi}\) leads to the minimization of the corresponding bisimulation error. This ensures that if the expected on-policy bisimulation Bellman residual (_i.e._,\(\mathbb{E}_{\mu_{\pi}}\big{[}\varepsilon_{\phi}^{\pi}\big{]}\), and we will use the term "expected bisimulation residual" in following) minimization objective is zero, then the bisimulation error must be zero for the state pairs under the same policy. However, when the dataset is limited, rather than an infinite transition set covering the whole MDP, minimizing the expected bisimulation residual will no longer be sufficient to guarantee a zero bisimulation error.

**Proposition 4**.: _(**The expected bisimulation residual is not sufficient over incomplete datasets). If there exists states \(s_{i}^{\prime}\) and \(s_{j}^{\prime}\) not contained in dataset \(\mathcal{D}\), where the occupancy \(\mu_{\pi}(s_{i}^{\prime}|s_{i},a_{i})>0\) and \(\mu_{\pi}(s_{j}^{\prime}|s_{j},a_{j})>0\) for some \((s_{i},s_{j})\sim\mu_{\pi}\), then there exists a bisimulation measurement \(G_{\phi}^{\pi}\) and a constant \(C>0\) such that_

* _For all_ \((\hat{s_{i}},\hat{s_{j}})\in\mathcal{D}\)_, the bisimulation Bellman residual_ \(\epsilon_{\phi}^{\pi}(\hat{s_{i}},\hat{s_{j}})=0\)_._
* _There exists_ \((s_{i},s_{j})\in\mathcal{D}\)_, such that the bisimulation error_ \(\Delta_{\phi}^{\pi}(s_{i},s_{j})=C\)_._

As an example, if we only have \((s_{i},a_{i},r,s_{i}^{\prime})\) and \((s_{j},a_{j},r,s_{j}^{\prime})\) in a dataset, where both rewards equal to zero for state \(s_{i}\) and \(s_{j}\), and if we choose \(G_{\phi}^{\pi}(s_{i},s_{j})=C\), and \(G_{\phi}^{\pi}(s_{i}^{\prime},s_{j}^{\prime})=\frac{1}{\gamma}C\), then the bisimulation Bellman residual is \(\epsilon_{\phi}^{\pi}(s_{i},s_{j})=0\), while the bisimulation error \(\Delta_{\phi}^{\pi}=G_{\phi}^{\pi}(s_{i},s_{j})-0=C\) is strictly positive. Note that this failure case does not involve modifying the environment in an extremely adversarial manner, it simply occurs when we are required to estimate the representation of states with subsequent states that are missing from the dataset. Since the distance between the missing states can be arbitrarily large as they are out-of-distribution, directly minimizing the Bellman bisimulation error could achieve the minimal Bellman bisimulation error over the dataset, while not necessarily improving the state representation.

In the context of Offline RL, since the dataset is finite, bisimulation-based representation learning ought to be conceptualized as a pretraining process over the behavior policy \(\pi_{\beta}\) of the dataset \(\mathcal{D}\). However, the failure case above indicates that applying the bisimulation operator \(\mathcal{F}^{\pi_{\beta}}\) and minimizing the associated Bellman bisimulation error does not necessarily ensure the sufficiency of the learned representation for downstream tasks. Ideally, if we had access to the fixed-point measurement \(G_{\phi}^{\pi_{\beta}}\), then we could directly minimize the error between the approximation \(G\) and the fixed-point \(G_{\phi}^{\pi_{\beta}}\). However, given the static and incomplete nature of the dataset, acquiring the fixed-point \(G_{\sim}^{\pi_{\beta}}\) explicitly is not feasible. From another perspective, the failure stems from out-of-distribution estimation errors. Assuming we could estimate the bisimulation exclusively with _in-sample learning_, this issue could be intuitively mitigated. As such, we resort to expectile regression as a regularizer, allowing us to circumvent the need for out-of-sample / unseen state pairs.

## 5 Method

In this section, we describe how we adapt existing bisimulation-based representation approaches to offline RL. We use the expectile-based operator to learn state representations that optimize the behavior measurement over the dataset, while avoiding overfitting to the incomplete data. In addition, we analyze the impact of reward scaling and propose as a consequence to normalize the reward difference in the bisimulation Bellman residual in order to satisfy the specific nature of different instantiations of the bisimulation measurement while keeping a lower value error. The pseudo-code of our method is shown in Algorithms in Appendix B.

### Expectile-based Bisimulation Operator

The efficacy of expectile regression in achieving _in-sample learning_ has already been demonstrated in previous research [28; 36]. Consequently, we will first describe our proposed _expectile_-based operator, and subsequently show how expectile regression can effectively address the aforementioned challenge. Specifically, we consider the update operator as follows:

\[\begin{split}&\Big{(}\mathcal{F}_{\tau}^{\pi_{\beta}}G_{\phi}^{ \pi_{\beta}}\Big{)}\left(s_{i},s_{j}\right):=\operatorname*{arg\,min}_{G_{ \phi}^{\pi_{\beta}}}\mathbb{E}_{a_{i}\sim\pi_{\beta}(\cdot|s_{i}),a_{j}\sim\pi _{\beta}(\cdot|s_{j})}\left[\tau[\hat{\epsilon}]_{+}^{2}+(1-\tau)[-\hat{ \epsilon}]_{+}^{2}\right],\\ &\hat{\epsilon}=\mathbb{E}_{s_{i}^{\prime}\sim T_{s_{i}^{\prime} }^{\pi_{\beta}}}\big{[}\underbrace{\left[r(s_{i},a_{i})-r(s_{j},a_{j})\right] +\gamma G_{\hat{\phi}}^{\pi_{\beta}}(s_{i}^{\prime},s_{j}^{\prime})}_{\text{ target }G}-G_{\phi}^{\pi_{\beta}}(s_{i},s_{j})\big{]},\end{split}\] (6)where \(\hat{\epsilon}\) is the estimated one-step bisimulation Bellman residual, \(\pi_{\beta}\) is the behavior policy, \(G_{\tilde{\phi}}\) is the target encoder, updated using an exponential moving average, and \([\cdot]_{+}=\max(\cdot,0)\). Since the expectile operator in Equation 6 does not have a closed-form solution, in practice, we minimize it through gradient descent steps:

\[G_{\phi}^{\pi_{\beta}}(s_{i},s_{j})\gets G_{\phi}^{\pi_{\beta}}(s_{i},s_{j })-2\alpha\mathbb{E}_{a_{i}\sim\pi_{\beta}(\cdot|s_{i}),a_{j}\sim\pi_{\beta}( \cdot|s_{j})}\left[\tau[\hat{\epsilon}]_{+}+(1-\tau)[\hat{\epsilon}]_{-}\right]\] (7)

where \(\alpha\) is the step size. The fixed-point of the measurement obtained using this expectile-based operator is denoted as \(G_{\tau}\). Although the utilization of the _expectile_ statistics is well established, its application for estimating bisimulation measurement is not particularly intuitive. In the following, we will show how expectile-based operator can be helpful in addressing the aforementioned issue. First, it is worth noting that when \(\tau=1/2\), this operator becomes the bisimulation expectation of the behavior policy, _i.e._, \(\mathbb{E}_{\mu_{\pi_{\beta}}}[\hat{\epsilon}]\). Next, we shall consider how this operator performs when \(\tau\to 1\). We show that under certain assumptions, our method indeed approximates an "optimal" measurement in terms of the given dataset. We first prove a technical lemma stating that the update operator is still a contraction, and then prove a lemma relating different expectiles, finally we derive our main result regarding the "optimality" of our method.

**Lemma 5**.: _For any \(\tau\in\) [0, 1), \(\mathcal{F}_{\tau}^{\pi}\) is a \(\gamma_{\tau}\)-contraction, where \(\gamma_{\tau}=1-2\alpha(1-\gamma)\min\left\{\tau,1-\tau\right\}<1\)._

**Lemma 6**.: _For any \(\tau,\tau^{\prime}\in[0,1)\) with \(\tau^{\prime}\geq\tau\), and for all \(s_{i},s_{j}\in\mathcal{S}\) and any \(\alpha\), we have \(G_{\tau^{\prime}}\geq G_{\tau}\)._

**Theorem 7**.: _In deterministic MDP and fixed finite dataset, we have:_

\[\lim_{\tau\to 1}G_{\tau}(s_{i},s_{j})=\max_{\begin{subarray}{c}a_{i}\in \mathcal{A},a_{j}\in\mathcal{A}\\ s.t.\pi_{\beta}(a_{i}|s_{i})>0,\pi_{\beta}(a_{j}|s_{j})>0\end{subarray}}G_{ \sim}^{*}((s_{i},a_{i}),(s_{j},a_{j})).\] (8)

_where \(G_{\sim}^{*}((s_{i},a_{i}),(s_{j},a_{j}))\) is a fixed-point measurement constrained to the dataset and defined on the state-action space \(\mathcal{S}\times\mathcal{A}\) as_

\[G_{\sim}^{*}((s_{i},a_{i}),(s_{j},a_{j}))=|r(s_{i},a_{i})-r(s_{j},a_{j})|+ \gamma\mathbb{E}_{s_{i}^{\prime}\sim\mathcal{T}_{s_{j}}^{\pi_{\beta}}}\left[ \max_{\begin{subarray}{c}a_{i}^{\prime}\in\mathcal{A},a_{j}\in\mathcal{A}\\ s_{i}^{\prime}\sim\mathcal{T}_{s_{j}}^{\pi_{\beta}}\end{subarray}}G_{\sim}^{* }((s_{i}^{\prime},a_{i}^{\prime}),(s_{j}^{\prime},a_{j}^{\prime}))\right].\]

Intuitively, \(G_{\sim}((s_{i},a_{i}),(s_{j},a_{j}))\) can be interpreted as a state-action value function \(Q(\tilde{s},\tilde{a})\) in a lifted MDP \(\widetilde{M}\), and \(G_{\sim}(s_{i},s_{j})\) as a state value function \(V(\tilde{s})\). We defer the detailed explanation to Appendix E.

Theorem 7 illustrates that, as \(\tau\to 1\), we are effectively approximating the maximum \(G_{\sim}((s_{i},a_{i}),(s_{j},a_{j}))\) over actions \(a_{i}^{\prime},a_{j}^{\prime}\) from the dataset. When we set \(\tau=1\), the expectile-based bisimulation operator achieves fully in-sample learning: we only consider state pairs that have corresponding actions in the dataset. For instance, only when we have \((s_{i}^{\prime},a_{i}^{\prime})\in\mathcal{D}\) and \((s_{j}^{\prime},a_{j}^{\prime})\in\mathcal{D}\), can we apply the measurement of \(G_{\sim}^{*}\). As such, by manipulating \(\tau\), we balance a trade-off between minimizing the expected bisimulation residual (for \(\tau=0.5\)) and evaluating \(G_{\sim}^{*}((s_{i},a_{i}),(s_{j},a_{j}))\) solely on the dataset (for \(\tau=1\)), thereby sidestepping the failure case outlined in Proposition 4 in an implicit manner.

### Reward Scaling

Most previous works [5; 54; 6; 52] have overlooked the impact of reward scaling in the bisimulation operator. To demonstrate its importance, we investigate a more general form of the bisimulation operator in Equation 2, given as:

\[\mathcal{F}^{\pi}G(s_{i},s_{j})=c_{r}\cdot|r_{s_{i}}^{\pi}-r_{s_{j}}^{\pi}|+c_{ k}\cdot\mathbb{E}_{s_{i}^{\prime},s_{j}^{\prime}}^{\pi}[G(s_{i}^{\prime},s_{j}^{ \prime})].\] (9)

We then can derive the following:

\[\begin{split} G_{\sim}^{\pi}(s_{i},s_{j})&=\mathcal{ F}^{\pi}G_{\sim}^{\pi}(s_{i},s_{j})=c_{r}\cdot|r_{s_{i}}^{\pi}-r_{s_{j}}^{\pi}|+c_{k} \cdot\mathbb{E}_{s_{i}^{\prime},s_{j}^{\prime}}^{\pi}[G_{\sim}^{\pi}(s_{i}^{ \prime},s_{j}^{\prime})]\\ &\leq c_{r}\cdot(R_{\text{max}}-R_{\text{min}})+c_{k}\cdot\mathbb{ E}_{s_{i}^{\prime},s_{j}^{\prime}}^{\pi}[G_{\sim}^{\pi}(s_{i}^{\prime},s_{j}^{ \prime})]\\ &\leq c_{r}\cdot(R_{\text{max}}-R_{\text{min}})+c_{k}\cdot\max_{ s_{i}^{\prime},s_{j}^{\prime}}G_{\sim}^{\pi}(s_{i}^{\prime},s_{j}^{\prime}).\end{split}\] (10)Accordingly, we have \(G_{\sim}^{\pi}(s_{i},s_{j})\leq\frac{c_{r}\cdot(R_{\text{max}}-R_{\text{min}})}{1- c_{k}}\). Adopting the conventional settings of \(c_{r}=1\) and \(c_{k}=\gamma\) as suggested in [6; 52], could possibly result in a relatively large upper bound of \(G_{\sim}^{\pi}\) between states. This is due to the common practice of setting \(\gamma\) at \(0.99\). However, when bisimulation operators are instantiated with bounded distances, e.g., cosine distance, such a setting may be unsuitable. Therefore, it becomes important to tighten the upper bound.

Besides, we can also derive the value bound between the ground truth value function and the approximated value function:

**Theorem 8**.: _(Value bound based on on-policy bisimulation measurements in terms of approximation error). Given an MDP \(\widetilde{\mathcal{M}}\) constructed by aggregating states in an \(\omega\)-neighborhood, and an encoder \(\phi\) that maps from states in the original MDP \(\mathcal{M}\) to these clusters, the value functions for the two MDPs are bounded as_

\[\left|V^{\pi}\left(s\right)-\widetilde{V}^{\pi}\left(\phi\left(s\right) \right)\right|\leq\frac{2\omega+\hat{\Delta}}{c_{r}(1-\gamma)}.\] (11)

_where \(\hat{\Delta}:=\|\hat{G}_{\sim}^{\pi}-\hat{G}_{\phi}^{\pi}\|_{\infty}\) is the approximation error._

In essence, Equation 10 and Theorem 8 reveal that: (i) there is a positive correlation between the reward scale \(c_{r}\) and the upper bound of the fixed-point \(G_{\sim}^{\pi}\), and (ii) a larger reward scale \(c_{r}\) facilitates a more accurate approximation of the value function \(\widetilde{V}^{\pi}(\phi(s))\) to its ground-truth value \(V^{\pi}(s)\). It is important to note that \(c_{r}\) also impacts the value of \(\hat{\Delta}\), as depicted in Figure 7(Right)4. Therefore, it is crucial to first ensure the alignment with the instantiation of the bisimulation measurement, and then choose the largest possible \(c_{r}\) to minimize the value error. For instance, as the SimSR operator [52] uses the cosine distance, \(c_{k}=\gamma\) is predetermined. We should thus set \(c_{r}\in[0,1-\gamma]\), and apply min-max normalization to the reward function. This can make \(G_{\sim}^{\pi}\leq 1\) and therefore be consistent with the maximum value of 1 of the cosine distance. To achieve a tighter bound in Equation11, we should then maximize the reward scale, setting \(c_{r}\) to \(1-\gamma\). Figure 7 illustrates the effectiveness of this reward scaling.

Footnote 4: Despite Figure 7(Right) depicting the approximate residual \(\hat{\epsilon}\), we have drawn a connection between \(\epsilon_{\phi}^{\pi}\) and \(\Delta_{\phi}^{\pi}\) in the Appendix, which can reflect the possible situations for \(\hat{\Delta}\).

## 6 Experiments

### Performance Comparison in D4RL Benchmark

Implementation DetailsWe analyze our proposed method on the D4RL benchmark [15] of OpenAI gym MuJoCo tasks [50] which includes a variety of datasets that have been commonly used in the

Figure 1: The effectiveness of Reward Scaling (_RS_) in SimSR on halfcheetah-medium-expert-v2, with results averaged on 3 random seeds. (**Left**) Effective Dimension [53] comparison: without _RS_, there is a significant reduction in the effective dimension, accompanied by a marked increase in instability as training progresses. (**Right**) Numerical value comparison of estimated bisimulation Bellman residual: \(\hat{\epsilon}\) is persistently greater than 0 in the absence of _RS_, which indicates that target \(G\) is invariably larger than \(G_{\phi}\), suggesting that \(G_{\phi}\) does not achieve steady convergence.

Offline RL community. To illustrate the effectiveness of our method, we implement it on top of two bisimulation-based approaches, **MICo**[6] and **SimSR**[52]. It is worth noting that there are two versions of SimSR depending on its use of a latent dynamics model: SimSR_basic follows the dynamics that the environment provides, and SimSR_full constructs latent dynamics for sampling successive latent states. We opt for SimSR_basic as our backbone, as it exhibits superior and more stable performance in the D4RL benchmark tasks compared to SimSR_full. Additionally, to explore the impact of bisimulation-based representation learning on the downstream performance of policy learning, we build these approaches on top of the Offline RL method **TD3BC**[16]. We examine three environments: halfcheetah, hopper, and walker2d, with four datasets per task: expert, medium-expert, medium-replay, and medium. We first pretrain the encoder during \(100k\) timesteps, then freeze it, pass the raw state through the frozen encoder to obtain the representations that serve as input for the Offline RL algorithm. Further details on the experiment setup are included in Appendix F.

AnalysisFigure 2 illustrates the performance of two approaches and their variants in the D4RL tasks. We use _EBS_ to represent the scheme of employing the expectile-based operator, while _RS_

Figure 3: Bootstrapping distributions for uncertainty in IQM (_i.e._ inter-quartile mean) measurement on D4RL tasks (left) and visual D4RL tasks (right), following from the performance criterion in [2].

Figure 2: Performance comparison on 12 D4RL tasks over 10 seeds with one standard error shaded in the default setting. For every seed, the average return is computed every 10,000 training steps, averaging over 10 episodes. The horizontal axis indicates the number of transitions trained on. The vertical axis indicates the normalized average return.

denotes the reward scaling scheme. The latter includes both min-max reward normalization and penalization coefficient with \((1-\gamma)\) in the bisimulation operator. As discussed in Section 5.2, the role of reward scaling varies depending on the specific instantiation of \(G\)5. We observe that without _RS_, SimSR almost fails in every dataset, which aligns with our understanding of the critical role reward scaling plays. The results also illustrate that _EBS_ effectively enhances the downstream performance of the policy for both SimSR and MICo. It is noteworthy that in this experiment, we set \(\tau=0.6\) for the expectile in SimSR and \(\tau=0.7\) in MICo across all datasets, demonstrating the robustness of this hyperparameter. Regarding SimSR, when _RS_ is applied (_SimSR+RS_), the performance is comparable to the TD3BC baseline, while the incorporation of the expectile-based operator (_SimSR+RS+EBS_) further enhances final performance and sample efficiency. Besides, we additionally present the IQM normalized return of all variants in Figure 3, illustrating our performance gains over the backbones. Further, we have also constructed an ablation study to investigate the impact of different settings of \(\tau\), the results show that a suitable expectile \(\tau\) is crucial for control tasks. We present the corresponding results in Appendix E.

Footnote 5: Since MICo does not necessitate a particular upper bound, RS may be harmful to its performance. Our experiments have substantiated this observation, leading us to exclude the MICo+RS results from Figure 2.

### Performance Comparison in V-D4RL Benchmark

Implementation detailsWe also evaluate our method on a visual observation setting of DM-Control suite (DMC) tasks, V-D4RL benchmark [35]. Similar to the previous experiment, we add the proposed schemes on top of MICo and SimSR. In the experiments, we notice that the latent dynamics modeling can help to boost performance for the visual setting, hence we use SimSR_full as the backbone. Additionally, we also notice that MICo often gives really poor performance in the V-D4RL benchmark, while adding latent dynamics alleviates the issue. Therefore, we boost MICo with explicit dynamics modeling for a fair comparison. To compare the performance with the other representation approaches, we include 4 competitive representation learning approaches for Offline RL, including DRIML [38], HOMER [39], CURL [31], and Inverse model [44]. Detailed descriptions of these approaches can be found in Appendix G.

AnalysisWe evaluate all aforementioned approaches by integrating the pre-trained encoder from each into an Offline RL method DrQ+BC [35], which combines data augmentation techniques with TD3BC. The results in Table 1 and Figure 4 illustrate the effectiveness of our proposed method, the numerical improvements are underlined with red upward arrows. Compared to the other baselines, while _SimSR+RS+EBS_ does not achieve the highest score in all datasets, it achieves the best overall performance. Besides, our modifications on MICo and SimSR consistently show significant improvements. This indicates that our proposed method is not only applicable to raw-state inputs but also compatible with pixel-based observations.

## 7 Discussion

Limitations and Future WorkWhile \(\tau\) remains constant in our D4RL experiments, optimal performance may arise under different \(\tau\) settings, contingent on the specific attributes of the dataset. Therefore, to yield the best outcomes, one might need to set various \(\tau\) to identify the most suitable value. However, this process could consume substantial computational resources. Another area of potential study involves evaluating the effectiveness of our approach in off-policy settings, given that off-policy settings may also lead to similar failure cases.

Figure 4: Performance comparison on V-D4RL benchmark.

ConclusionIn this work, we highlight the effectiveness of the bisimulation operator over incomplete datasets and emphasize the crucial role of reward scaling in Offline settings. By employing the expectile operator in bisimulation, we manage to strike a balance between behavior measurement and greedy assignment of the measurement over datasets. We also propose a reward scaling strategy to reduce the risk of representation collapse in specific bisimulation-based measurements. Empirical studies show the effectiveness of our proposed modifications.

## Acknowledgments

This work was partially supported by the NSFC under Grants 92270125 and 62276024, as well as the National Key R&D Program of China under Grant No.2022YFC3302101.

\begin{table}
\begin{tabular}{l||c c c||c||c|c} \hline Dataset & CURL & DRIMLC & HOMER & ICM & MICo\(\rightarrow\) MICo+EBS & SimSR \(\rightarrow\) SimSR+RS+EBS \\ \hline cheetah-run-medium & 392 & **524** & 475 & 365 & \(177\to 449\) (\(\nearrow 272\)) & \(391\to 491\)(\(\nearrow 100\)) \\ walker-walk-medium & 452 & 425 & 439 & 358 & \(450\to 447\) (\(\nearrow 443\) \(\rightarrow\) 480) (\(\nearrow 37\)) \\ cheetah-run-medium-replay & 271 & 395 & 306 & 251 & \(335\to 357\) (\(\nearrow 22\)) & \(374\) (\(\nearrow 88\)) \\ walker-walk-medium-replay & 265 & 235 & **283** & 167 & \(207\to 420\) (\(\nearrow 33\)) & \(197\to 240\)(\(\nearrow 43\)) \\ cheetah-run-medium-expert & 348 & 403 & 383 & 280 & \(282\to 341\) (\(\nearrow 59\)) & \(360\to 547\)(\(\nearrow 187\)) \\ walker-walk-medium-expert & 729 & 399 & 781 & 606 & \(586\to 635\) (\(\nearrow 49\)) & \(755\to 845\)(\(\nearrow 90\)) \\ cheetah-run-expert & 200 & 310 & 218 & 237 & \(308\to 331\) (\(\nearrow 23\)) & \(409\to 454\)(\(\nearrow 45\)) \\ walker-walk-expert & 769 & 427 & 686 & **850** & \(370\to 447\) (\(\nearrow 77\)) & \(578\to 580\) (\(\nearrow 7\)) \\ \hline \hline total & 3426 & 3118 & 3571 & 3114 & \(2715\to 3253\) (\(\nearrow 538\)) & \(3507\to **4043**\) (\(\nearrow 536\)) \\ \hline \end{tabular}
\end{table}
Table 1: Performance comparison with several other baselines on V-D4RL benchmark, averaged on 3 random seeds.

## References

* Agarwal et al. [2021] Rishabh Agarwal, Marlos C. Machado, Pablo Samuel Castro, and Marc G. Bellemare. Contrastive behavioral similarity embeddings for generalization in reinforcement learning. In _9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021_. OpenReview.net, 2021.
* Agarwal et al. [2021] Rishabh Agarwal, Max Schwarzer, Pablo Samuel Castro, Aaron C. Courville, and Marc G. Bellemare. Deep reinforcement learning at the edge of the statistical precipice. In Marc'Aurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan, editors, _Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual_, pages 29304-29320, 2021.
* Arora et al. [2020] Sanjeev Arora, Simon S. Du, Sham M. Kakade, Yuping Luo, and Nikunj Saunshi. Provable representation learning for imitation learning via bi-level optimization. In _Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event_, volume 119 of _Proceedings of Machine Learning Research_, pages 367-376. PMLR, 2020.
* Brandfonbrener et al. [2022] David Brandfonbrener, Remi Tachet des Combes, and **Romain Laroche**. Incorporating explicit uncertainty estimates into deep offline reinforcement learning. In _Proceedings of the 5th Multidisciplinary Conference on Reinforcement Learning and Decision Making (RLDM)_, 2022.
* Castro [2020] Pablo Samuel Castro. Scalable methods for computing state similarity in deterministic markov decision processes. In _The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020_, pages 10069-10076. AAAI Press, 2020.
* Castro et al. [2021] Pablo Samuel Castro, Tyler Kastner, Prakash Panangaden, and Mark Rowland. Mico: Improved representations via sampling-based state similarity for markov decision processes. In Marc'Aurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan, editors, _Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual_, pages 30113-30126, 2021.
* Castro et al. [2021] Pablo Samuel Castro, Tyler Kastner, Prakash Panangaden, and Mark Rowland. Mico: Learning improved representations via sampling-based state similarity for markov decision processes. _CoRR_, abs/2106.08229, 2021.
* Castro and Precup [2010] Pablo Samuel Castro and Doina Precup. Using bisimulation for policy transfer in mdps. In Maria Fox and David Poole, editors, _Proceedings of the Twenty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2010, Atlanta, Georgia, USA, July 11-15, 2010_. AAAI Press, 2010.
* Chen et al. [2021] Xin Chen, Sam Toyer, Cody Wild, Scott Emmons, Ian Fischer, Kuang-Huei Lee, Neel Alex, Steven H Wang, Ping Luo, Stuart Russell, Pieter Abbeel, and Rohin Shah. An empirical investigation of representation learning for imitation. In _Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2)_, 2021.
* Eysenbach et al. [2019] Benjamin Eysenbach, Abhishek Gupta, Julian Ibarz, and Sergey Levine. Diversity is all you need: Learning skills without a reward function. In _7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019_. OpenReview.net, 2019.
* Ferns et al. [2004] Norm Ferns, Prakash Panangaden, and Doina Precup. Metrics for finite markov decision processes. In Deborah L. McGuinness and George Ferguson, editors, _Proceedings of the Nineteenth National Conference on Artificial Intelligence, Sixteenth Conference on Innovative Applications of Artificial Intelligence, July 25-29, 2004, San Jose, California, USA_, pages 950-951. AAAI Press / The MIT Press, 2004.
* Ferns et al. [2004] Norm Ferns, Prakash Panangaden, and Doina Precup. Metrics for finite markov decision processes. In David Maxwell Chickering and Joseph Y. Halpern, editors, _UAI '04, Proceedings of the 20th Conference in Uncertainty in Artificial Intelligence, Banff, Canada, July 7-11, 2004_, pages 162-169. AUAI Press, 2004.

* [13] Norm Ferns, Prakash Panangaden, and Doina Precup. Bisimulation metrics for continuous markov decision processes. _SIAM J. Comput._, 40(6):1662-1714, 2011.
* [14] Norman Ferns and Doina Precup. Bisimulation metrics are optimal value functions. In Nevin L. Zhang and Jin Tian, editors, _Proceedings of the Thirtieth Conference on Uncertainty in Artificial Intelligence, UAI 2014, Quebec City, Quebec, Canada, July 23-27, 2014_, pages 210-219. AUAI Press, 2014.
* [15] Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine. D4RL: datasets for deep data-driven reinforcement learning. _CoRR_, abs/2004.07219, 2020.
* [16] Scott Fujimoto and Shixiang Shane Gu. A minimalist approach to offline reinforcement learning. In _Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual_, pages 20132-20145, 2021.
* [17] Scott Fujimoto, David Meger, Doina Precup, Ofir Nachum, and Shixiang Shane Gu. Why should I trust you, bellman? the bellman error is a poor replacement for value error. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, _International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA_, volume 162 of _Proceedings of Machine Learning Research_, pages 6918-6943. PMLR, 2022.
* [18] Scott Fujimoto, Herke van Hoof, and David Meger. Addressing function approximation error in actor-critic methods. In Jennifer G. Dy and Andreas Krause, editors, _Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsmassan, Stockholm, Sweden, July 10-15, 2018_, volume 80 of _Proceedings of Machine Learning Research_, pages 1582-1591. PMLR, 2018.
* [19] Carles Gelada, Saurabh Kumar, Jacob Buckman, Ofir Nachum, and Marc G. Bellemare. Deep-mdp: Learning continuous latent space models for representation learning. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, _Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA_, volume 97 of _Proceedings of Machine Learning Research_, pages 2170-2179. PMLR, 2019.
* [20] Robert Givan, Thomas L. Dean, and Matthew Greig. Equivalence notions and model minimization in markov decision processes. _Artif. Intell._, 147(1-2):163-223, 2003.
* [21] Pengjie Gu, Mengchen Zhao, Chen Chen, Dong Li, Jianye Hao, and Bo An. Learning pseudometric-based action representations for offline reinforcement learning. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, _International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA_, volume 162 of _Proceedings of Machine Learning Research_, pages 7902-7918. PMLR, 2022.
* [22] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. In Jennifer G. Dy and Andreas Krause, editors, _Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsmassan, Stockholm, Sweden, July 10-15, 2018_, volume 80 of _Proceedings of Machine Learning Research_, pages 1856-1865. PMLR, 2018.
* [23] Philippe Hansen-Estruch, Amy Zhang, Ashvin Nair, Patrick Yin, and Sergey Levine. Bisimulation makes analogies in goal-conditioned reinforcement learning. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, _International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA_, volume 162 of _Proceedings of Machine Learning Research_, pages 8407-8426. PMLR, 2022.
* [24] Zhang-Wei Hong, Remi Tachet des Combes, Pulkit Agrawal, and **Romain Laroche**. Harnessing mixed offline reinforcement learning datasets via trajectory weighting. In _Proceedings of the 11th International Conference on Learning Representations (ICLR)_, 2023.
* [25] Riashat Islam, Manan Tomar, Alex Lamb, Yonathan Efroni, Hongyu Zang, Aniket Didolkar, Dipendra Misra, Xin Li, Harm van Seijen, Remi Tachet des Combes, and John Langford. Agent-controller representations: Principled offline rl with rich exogenous information, 2022.

* [26] Sham M. Kakade and John Langford. Approximately optimal approximate reinforcement learning. In Claude Sammut and Achim G. Hoffmann, editors, _Machine Learning, Proceedings of the Nineteenth International Conference (ICML 2002), University of New South Wales, Sydney, Australia, July 8-12, 2002_, pages 267-274. Morgan Kaufmann, 2002.
* [27] Mete Kemertas and Tristan Aumentado-Armstrong. Towards robust bisimulation metric learning. In Marc'Aurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan, editors, _Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual_, pages 4764-4777, 2021.
* [28] Ilya Kostrikov, Ashvin Nair, and Sergey Levine. Offline reinforcement learning with implicit q-learning. In _The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022_. OpenReview.net, 2022.
* [29] Sascha Lange, Thomas Gabel, and Martin A. Riedmiller. Batch reinforcement learning. In Marco A. Wiering and Martijn van Otterlo, editors, _Reinforcement Learning_, volume 12 of _Adaptation, Learning, and Optimization_, pages 45-73. Springer, 2012.
* [30] Kim Guldstrand Larsen and Arne Skou. Bisimulation through probabilistic testing. In _Conference Record of the Sixteenth Annual ACM Symposium on Principles of Programming Languages, Austin, Texas, USA, January 11-13, 1989_, pages 344-352. ACM Press, 1989.
* [31] Michael Laskin, Aravind Srinivas, and Pieter Abbeel. CURL: contrastive unsupervised representations for reinforcement learning. In _Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event_, volume 119 of _Proceedings of Machine Learning Research_, pages 5639-5650. PMLR, 2020.
* [32] Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Offline reinforcement learning: Tutorial, review, and perspectives on open problems. _CoRR_, abs/2005.01643, 2020.
* [33] Lihong Li, Thomas J. Walsh, and Michael L. Littman. Towards a unified theory of state abstraction for mdps. In _International Symposium on Artificial Intelligence and Mathematics, ISAIM 2006, Fort Lauderdale, Florida, USA, January 4-6, 2006_, 2006.
* [34] Hao Liu and Pieter Abbeel. Behavior from the void: Unsupervised active pre-training. In Marc'Aurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan, editors, _Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual_, pages 18459-18473, 2021.
* [35] Cong Lu, Philip J. Ball, Tim G. J. Rudner, Jack Parker-Holder, Michael A. Osborne, and Yee Whye Teh. Challenges and opportunities in offline reinforcement learning from visual observations. _CoRR_, abs/2206.04779, 2022.
* [36] Xiaoteng Ma, Yiqin Yang, Hao Hu, Jun Yang, Chongjie Zhang, Qianchuan Zhao, Bin Liang, and Qihan Liu. Offline reinforcement learning with value-based episodic memory. In _The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022_. OpenReview.net, 2022.
* [37] Bogdan Mazoure, Ahmed M. Ahmed, R. Devon Hjelm, Andrey Kolobov, and Patrick MacAlpine. Cross-trajectory representation learning for zero-shot generalization in RL. In _The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022_. OpenReview.net, 2022.
* [38] Bogdan Mazoure, Remi Tachet des Combes, Thang Doan, Philip Bachman, and R. Devon Hjelm. Deep reinforcement and infomax learning. In Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, _Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual_, 2020.

* [39] Dipendra Misra, Mikael Henaff, Akshay Krishnamurthy, and John Langford. Kinematic state abstraction and provably efficient rich-observation reinforcement learning. In _Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event_, volume 119 of _Proceedings of Machine Learning Research_, pages 6961-6971. PMLR, 2020.
* [40] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex Graves, Martin A. Riedmiller, Andreas Fidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement learning. _Nat._, 518(7540):529-533, 2015.
* [41] Ofir Nachum and Mengjiao Yang. Provable representation learning for imitation with contrastive fourier features. In _Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual_, pages 30100-30112, 2021.
* [42] Ofir Nachum and Mengjiao Yang. Provable representation learning for imitation with contrastive fourier features. _CoRR_, abs/2105.12272, 2021.
* [43] Kimia Nadjahi*, **Romain Laroche*, and Remi Tachet des Combes. Safe policy improvement with soft baseline bootstrapping. In _Proceedings of the 17th European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML-PKDD)_, 2019.
* [44] Deepak Pathak, Pulkit Agrawal, Alexei A. Efros, and Trevor Darrell. Curiosity-driven exploration by self-supervised prediction. In _Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017_, volume 70 of _Proceedings of Machine Learning Research_, pages 2778-2787. PMLR, 2017.
* [45] Vitchyr Pong, Ashvin Nair, Murtaza Dalal, and Steven Lin. Rlkit, 2020.
* [46] Anirban Santara, Rishabh Madan, Pabitra Mitra, and Balaraman Ravindran. Extra: Transfer-guided exploration. In Amal El Fallah Seghrouchni, Gita Sukthankar, Bo An, and Neil Yorke-Smith, editors, _Proceedings of the 19th International Conference on Autonomous Agents and Multiagent Systems, AAMAS '20, Auckland, New Zealand, May 9-13, 2020_, pages 1987-1989. International Foundation for Autonomous Agents and Multiagent Systems, 2020.
* [47] Max Schwarzer, Nitarshan Rajkumar, Michael Noukhovitch, Ankesh Anand, Laurent Charlin, R. Devon Hjelm, Philip Bachman, and Aaron C. Courville. Pretraining representations for data-efficient reinforcement learning. In _Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual_, pages 12686-12699, 2021.
* [48] David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, Yutian Chen, Timothy P. Lillicrap, Fan Hui, Laurent Sifre, George van den Driessche, Thore Graepel, and Demis Hassabis. Mastering the game of go without human knowledge. _Nat._, 550(7676):354-359, 2017.
* [49] Adam Stooke, Kimin Lee, Pieter Abbeel, and Michael Laskin. Decoupling representation learning from reinforcement learning. In Marina Meila and Tong Zhang, editors, _Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event_, volume 139 of _Proceedings of Machine Learning Research_, pages 9870-9879. PMLR, 2021.
* [50] Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. In _2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, IROS 2012_, pages 5026-5033. IEEE, 2012.
* [51] Mengjiao Yang and Ofir Nachum. Representation matters: Offline pretraining for sequential decision making. In _Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event_, volume 139 of _Proceedings of Machine Learning Research_, pages 11784-11794. PMLR, 2021.

- March 1, 2022_, pages 8997-9005. AAAI Press, 2022.
* [53] Hongyu Zang, Xin Li, Jie Yu, Chen Liu, Riashat Islam, Remi Tachet des Combes, and Romain Laroche. Behavior prior representation learning for offline reinforcement learning. In _The Eleventh International Conference on Learning Representations_, 2023.
* [54] Amy Zhang, Rowan Thomas McAllister, Roberto Calandra, Yarin Gal, and Sergey Levine. Learning invariant representations for reinforcement learning without reconstruction. In _9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021_. OpenReview.net, 2021.
* [55] Amy Zhang, Shagun Sodhani, Khimya Khetarpal, and Joelle Pineau. Multi-task reinforcement learning as a hidden-parameter block MDP. _CoRR_, abs/2007.07206, 2020.
* [56] Amy Zhang, Shagun Sodhani, Khimya Khetarpal, and Joelle Pineau. Learning robust state abstractions for hidden-parameter block mdps. In _9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021_. OpenReview.net, 2021.

**Appendix**

###### Contents

* A Notation
* B Algorithm
* C Technical backgrounds
* C.1 Bisimulation metric
* C.2 MICo distance
* C.3 SimSR operator
* C.4 Lifted MDP
* C.5 Expectile Regression
* D Proof
* D.1 Connection between bisimulation error and bisimulation Bellman residual
* D.2 Thoerem 3
* D.3 Proposition 4
* D.4 Lemma 5
* D.5 Lemma 6
* D.6 Theorem 7
* D.7 Theorem 8
* E Understanding of Theorem 7
* F Additional Experiments
* F.1 Ablation Study - Value of Expectile
* F.2 Ablation Study - Effectiveness of Reward Scaling
* F.3 Case Study on MICo
* G Additional Related Works
* H Additional Discussion
* H.1 The severity of the proposed problem
* H.2 Suitability of different techniques
* I Empirical estimation of bisimulation error

## Appendix A Notation

Table 2 summarizes our notation.

## Appendix B Algorithm

We provide the algorithm in Algorithm 1, and a pytorch-like implementation build on top of SimSR in Algorithm 2.

```
1:Stage 1 Preprocessing:
2:Min-Max reward normalization: \(\bar{r}=\frac{r-r_{\text{min}}}{r_{\text{max}}-r_{\text{min}}}\)
3:Stage 2 Pretraining the encoder:
4:Initialize encoder parameter \(\phi\), expectile \(\tau\), learning rate \(\alpha\), discount factor \(\gamma\).
5:for each gradient step do
6: Apply reward scaling when computing \(\hat{\epsilon}\): \[\hat{\epsilon}=(1-\gamma)|\bar{r}(s_{i},a_{i})-\bar{r}(s_{j},a_{j})|+\gamma G _{\hat{\phi}}^{\pi_{\beta}}(s_{i}^{\prime},s_{j}^{\prime})-G_{\phi}^{\pi_{ \beta}}(s_{i},s_{j})\] (12)
7: Update encoder \(\phi\): \[\phi\leftarrow\phi-2\alpha\mathbb{E}_{a_{i}\sim\pi_{\beta}(\cdot|s_{i}),a_{j} \sim\pi_{\beta}(\cdot|s_{j})}\left[\tau[\hat{\epsilon}]_{+}+(1-\tau)[\hat{ \epsilon}]_{-}\right]\] (13)
8:endfor
9:Stage 3 Training value function and policy network:
10:Initialize value function parameter \(\psi\), policy network parameter \(\theta\), learning rate \(\lambda_{V}\) and \(\lambda_{\pi}\).
11:for each gradient step do
12: Sample tuple \((s,a,s^{\prime},\bar{r})\) from dataset \(\mathcal{D}\)
13: Encode the states to representation space: \(z=\phi(s),z^{\prime}=\phi(s^{\prime})\)
14: Update value function with \((z,a,z^{\prime},\bar{r})\): \[\psi=\psi-\lambda_{V}\nabla_{\psi}\mathcal{L}_{V}(\psi).\] (14)
15: Update policy network with \((z,a,z^{\prime},\bar{r})\): \[\theta=\theta-\lambda_{\pi}\nabla_{\theta}\mathcal{L}_{\pi}(\theta).\] (15)
16:endfor ```

**Algorithm 1** Proposed Implementation

## Appendix C

\begin{table}
\begin{tabular}{c c c c} \hline \hline Notation & Meaning & Notation & Meaning \\ \hline \(\mathcal{M}\) & MDP & \(\bar{\mathcal{M}}\) & Lifted MDP (auxiliary MDP) \\ \hline \(\mathcal{S}\) & state space & \(\mathcal{A}\) & action space \\ \hline \(T\) & transition function & \(r\) & reward function \\ \hline \(\gamma\) & discount factor & \(\pi\) & policy of the agent \\ \hline \(V^{\pi}(s)\) & state value function given policy \(\pi\) & \(\mathcal{D}\) & dataset \\ \hline \(\pi_{\beta}\) & behavior policy & \(\mu_{\beta}(s)\) & state occupancy of the dataset \\ \hline \(\mathcal{F}^{\pi}\) & on-policy bisimulation operator & \(g_{\omega}^{\pi}\) & \(\pi\)-bisimulation metric \\ \hline \(D(\cdot,\cdot)\) & a specific distance & \(G_{\omega}^{\pi}\) & fixed point of MCo and SimSR \\ \hline \(\phi\) & state encoder & \(G_{\phi}^{\pi}(s_{i},s_{j})\) & parameterized bisimulation measurement \\ \hline \(\Delta_{\hat{\phi}}^{\pi}\) & bisimulation error & \(\epsilon_{\omega}^{\pi}\) & bisimulation Bellman residual \\ \hline \(\mu_{\pi}(s)\) & stationary distribution over states on policy \(\pi\) & \(\mu_{\pi}\) & the distribution over pairs of states \\ \hline \(\mathbb{E}_{\pi_{\pi}}[\hat{\epsilon}_{\pi}^{\pi}]\) & expected on-policy bisimulation Bellman residual & \(\mathcal{F}^{\pi_{\phi}}\) & behavior bisimulation operator \\ \hline \(\tau\) & \(\tau_{\tau}\) & expected term & \(\gamma_{\tau}\) & discount factor with expectile \\ \hline \(\mathcal{F}_{\tau}^{\pi_{\beta}}\) & behavior bisimulation operator with expectile & \(\hat{\epsilon}\) & estimated one-step residual \\ \hline \(G_{\hat{\phi}}\) & bisimulation measurement parameterized by target encoder & \(G_{\omega}(s_{i},a_{i},s_{j},a_{j})\) & a measurement on state-action space \\ \hline \(G_{\omega}(s_{i},a_{i},s_{j},a_{j})\) & maximum measurement constrained to dataset & \(\epsilon_{\tau}\) & scale term of reward in bisimulation \\ \hline \(\alpha_{i}\) & scale term of transition in bisimulation & \(\bar{V}^{\pi}(\phi(s))\) & value function based on state encoder \\ \hline \(\omega\) & distance bound of aggregating neighbor & \(\hat{\Delta}\) & approximation error of bisimulation measurement \\ \hline \hline \end{tabular}
\end{table}
Table 2: Table of Notation.

``` classReplayBuffer(object): def__init__(self):... self.reward_normalization()...... defreward_normalization(self): r_max = self.reward_max() r_min = self.reward_min() self.reward = self.reward - r_min) / (r_max - r_min) defcompute_distance(features_a, features_b): similarity_matrix = torch.matmul(features_a, features_b.T) dis = 1-similarity_matrix return dis defexpotile_loss(diff, expectile): weight = torch.where(diff > 0, expectile, (1 - expectile)) return weight * (diff ** 2)
# encoder: mlp, encoder network, the output is 12-normalized
# target_encoder: mlp, name = encoder, updated by EMA
# discount_discount factor $gamma$
# slope: expectile $lambda$
compute_obs_loss(encoder, target_encoder, replay_buffer, batch_size, discount, slope): observation, action, reward, discount, next_observation = replay_buffer.sample(batch_size)# sample a batch of tuples from replay buffer latent_state = encoder(observation) latent_next_state = target_encoder(next_observation) r_diff = (1 - discount) < torch.abs(reward_T - reward) next_diff < compute_distance(latent_next_state, latent_next_state) z_diff = compute_distance(latent_state, latent_state) bisimilarity = r_diff + discount * next_diff

encoder_loss = expectile_loss(bisimilarity_detach() - z_diff, slope) encoder_loss = encoder_loss.mean() return encoder_loss ```

**Algorithm 2** SimSR+RS+EBS Pseudocode, PyTorch-like

## Appendix C Technical backgrounds

### Bisimulation metric

Bisimulation measures equivalence relations on MDPs with a recursive form: two states are deemed equivalent if they share the equivalent distributions over the next equivalent states and they have the same immediate reward [30, 20]. However, since bisimulation considers equivalence for all actions, including bad ones, it commonly results in "pessimistic" outcomes. Instead, [5] developed \(\pi\)-bisimulation which removes the requirement of considering each action and only needs to consider the actions induced by a policy \(\pi\).

**Definition 9**.: _[_5_]_ _Given an MDP \(\mathcal{M}\), an equivalence relation \(E^{\pi}\subseteq\mathcal{S}\times\mathcal{S}\) is a \(\pi\)-bisimulation relation if whenever \((\mathbf{s},\mathbf{u})\in E^{\pi}\) the following properties hold:_

1. \(r(s,\pi)=r(u,\pi)\)__
2. \(\forall C\in\mathcal{S}_{E^{\pi}},T(C|s,\pi)=T(C|u,\pi)\)__

_where \(\mathcal{S}_{E^{\pi}}\) is the state space \(\mathcal{S}\) partitioned into equivalence classes defined by \(E^{\pi}\). Two states \(s,u\in S\) are \(\pi\)-bisimilar if there exists a \(\pi\)-bisimulation relation \(E^{\pi}\) such that \((s,u)\in E^{\pi}\)._

However, \(\pi\)-bisimulation is still too stringent to be applied at scale as \(\pi\)-bisimulation relation emphasizes the equivalence is a binary property: either two states are equivalent or not, thus becoming too sensitive to perturbations in the numerical values of the model parameters. The problem becomes even more prominent when deep frameworks are applied.

Thereafter, they proposed a \(\pi\)-bisimulation metric to leverage the absolute value between the immediate rewards w.r.t. two states and the \(1\)-Wasserstein distance (\(\mathcal{W}_{1}\)) between the transition distributions conditioned on the two states and the policy \(\pi\) to formulate such measurement:

**Theorem 10**.: _Define \(\mathcal{F}^{\pi}:\mathbb{M}\rightarrow\mathbb{M}\) by \(\mathcal{F}^{\pi}(d)(s,u)=|R^{\pi}_{s}-R^{\pi}_{u}|+\gamma\mathcal{W}_{1}(d)( T^{\pi}_{s},T^{\pi}_{u})\), then \(\mathcal{F}^{\pi}\) has a least fixed point \(d^{\pi}_{\sim}\), and \(d^{\pi}_{\sim}\) is a \(\pi\)-bisimulation metric._Although the Wasserstein distance is a powerful metric to calculate the distance between two probability distributions, it requires to enumerate all states which is impossible in RL tasks of continuous state space. Various extensions have been proposed [54; 6; 52] to reduce the computational complexity. DBC [54] extend bisimulation metrics to learn state representation, via minimizing the \(\ell_{1}\)-norm distance of representations and the bisimulation metrics, meanwhile modeling the latent dynamics as Gaussian and utilizing \(W_{2}\) distance to compute it, which can be formulated as a closed-form result. However, DBC has several issues like loss function mismatch and specific requirements for Gaussian modeling, which limits its application and performance.

### MICo distance

MICo distance [6], tackles the above issue by restricting the coupling class to the independent coupling to avoid intractable Wasserstein distance computation. The MICo operator and its associated theoretical guarantee are given as:

**Theorem 11**.: _[_6_]_ _Given a policy \(\pi\), MICo distance \(\mathcal{F}^{\pi}\) is defined as:_

\[\mathcal{F}^{\pi}U(s_{i},s_{j})=|r^{\pi}_{s_{i}}-r^{\pi}_{s_{j}}|+\gamma \mathbb{E}_{s^{\prime}_{i}\sim T^{\pi}_{s^{\prime}_{j}}\sim T^{\pi}_{s_{j}}}[U (s^{\prime}_{i},s^{\prime}_{j})]\] (16)

_has a fixed point \(U^{\pi}\)._

By considering the Wasserstein distance in the definition of bisimulation metrics can be upper-bounded by taking a restricted class of couplings of the transition distributions, MICo restricts the coupling class precisely to the singleton containing the independent coupling, utilizing the Independent Couple sampling strategy to bypass the computation of the Wasserstein distance. However, MICo distance \(U\) requires to be a Lukaszyk-Karmowski metric, which does not satisfy the identity of indiscernibles. As a result, the approximated distance on the learned embedding space based on the MICo distance, which involves a Lukaszyk-Karmowski metric to measure the distance between dynamics, may suffer from the violation issue of the identity of indiscernibles.

### SimSR operator

To avoid the potential representation collapse, SimSR [52] develop a more concise update operator to learn state representation more effectively. Coupling with cosine distance, SimSR defines its operator as:

**Theorem 12**.: _[_52_]_ _Given a policy \(\pi\), Simple State Representation (SimSR) is updated as:_

\[\mathcal{F}^{\pi}\overline{cos}_{\phi}(s_{i},s_{j})=|r^{\pi}_{s_{i}}-r^{\pi}_ {s_{j}}|+\gamma\mathbb{E}_{s^{\prime}_{i}\sim T^{\pi}_{s_{i}},s^{\prime}_{j} \sim T^{\pi}_{s_{j}}}[\overline{cos}_{\phi}(s^{\prime}_{i},s^{\prime}_{j})]\] (17)

_has the same fixed point as MICo._

Further, considering the latent dynamics can be beneficial to representation learning, they additionally develop a form of operator including dynamics modeling:

**Theorem 13**.: _[_52_]_ _Given a policy \(\pi\), and a latent dynamics model \(\hat{T}\), SimSR is updated as_

\[\mathcal{F}^{\pi}\overline{cos}_{\phi}(s_{i},s_{j})= |r^{\pi}_{s_{i}}-r^{\pi}_{s_{j}}|+\gamma\mathbb{E}_{s^{\prime}_{i} \sim T^{\pi}_{s^{\prime}_{i}},s^{\prime}_{j}\sim T^{\pi}_{\phi}(s_{j})}[ \overline{cos}(z^{\prime}_{i},z^{\prime}_{j})].\] (18)

_If latent dynamics are specified, \(\mathcal{F}^{\pi}\) has a fixed point._

When considering MICo distance and the basic version of SimSR, we can notice that they have a similar recursive iteration formulation. And therefore both works can be generalized under:

\[\mathcal{F}^{\pi}G^{\pi}(s_{i},s_{j})=|r^{\pi}_{s_{i}}-r^{\pi}_{s_{j}}|+ \gamma\mathbb{E}_{s^{\prime}_{i}\sim T^{\pi}_{s_{j}}}[G^{\pi}(s^{\prime}_{i},s^{\prime}_{j})],\] (19)

while the instantiation of \(G\) varies in these two approaches.

### Lifted MDP

The connection between bisimulation-based operators and lifted MDP can be referred to [6]. We provide the corresponding Lemma here for reference.

**Lemma 2**.: _(**Lifted MDP)** The bisimulation-based update operator \(\mathcal{F}^{\pi}\) for \(\mathcal{M}\), is the Bellman evaluation operator for a specific lifted MDP._

Proof.: Given the MDP specified by the tuple \((\mathcal{S},\mathcal{A},T,R)\), we construct a lifted MDP \((\widetilde{\mathcal{S}},\widetilde{\mathcal{A}},\widetilde{T},\widetilde{R})\), by taking the state space to be \(\widetilde{\mathcal{S}}=\mathcal{S}^{2}\), the action space to be \(\widetilde{\mathcal{A}}=\mathcal{A}^{2}\), the transition dynamics to be given by \(\widetilde{T}_{\tilde{s}}^{\tilde{u}}(\tilde{s}^{\prime})=\widetilde{T}_{(s _{i},s_{j})}^{(a_{i},a_{j})}((s^{\prime}_{i},s^{\prime}_{j}))=T_{s_{i}}^{a_{i} }(s^{\prime}_{i})T_{s_{j}}^{a_{j}}(s^{\prime}_{j})\) for all \((s_{i},s_{j}),(s^{\prime}_{i},s^{\prime}_{j})\in\mathcal{S}^{2}\), \(a_{i},a_{j}\in\mathcal{A}\), and the action-independent rewards to be \(\widetilde{R}_{\tilde{s}}=\widetilde{R}_{(s_{i},s_{j})}=|r_{s_{i}}^{\pi}-r_{s _{j}}^{\pi}|\) for all \(s_{i},s_{j}\in\mathcal{S}\). The Bellman evaluation operator \(\widetilde{\mathcal{F}}^{\tilde{\pi}}\) for this lifted MDP at discount rate \(\gamma\) under the policy \(\tilde{\pi}(\tilde{a}|\tilde{s})=\tilde{\pi}(a_{i},a_{j}|s_{i},s_{j})=\pi(a_{i} |s_{i})\pi(a_{j}|s_{j})\) is given by (for all \(G^{\pi}\in\mathbb{R}^{\mathcal{S}\times\mathcal{S}}\) and \((s_{i},s_{j})\in\mathcal{S}\times\mathcal{S}\)):

\[(\widetilde{\mathcal{F}}^{\tilde{\pi}}\tilde{G}^{\pi})(\tilde{s}) =\widetilde{R}_{\tilde{s}}+\gamma\sum_{\tilde{s}^{\prime}\in \widetilde{\mathcal{S}}}\widetilde{T}_{\tilde{s}}^{\tilde{u}}(\tilde{s}^{ \prime})\tilde{\pi}(\tilde{a}|\tilde{s})\tilde{G}^{\pi}(\tilde{s}^{\prime})\] \[(\widetilde{\mathcal{F}}^{\tilde{\pi}}G^{\pi})(s_{i},s_{j}) =\widetilde{R}_{(s_{i},s_{j})}+\gamma\sum_{(s^{\prime}_{i},s^{ \prime}_{j})\in\mathcal{S}^{2}}\widetilde{T}_{(s_{i},s_{j})}^{(a_{i},a_{j})}( (s^{\prime}_{i},s^{\prime}_{j}))\tilde{\pi}(a_{i},a_{j}|s_{i},s_{j})G^{\pi}(s ^{\prime}_{i},s^{\prime}_{j})\] \[=|r_{s_{i}}^{\pi}-r_{s_{j}}^{\pi}|+\gamma\sum_{(s^{\prime}_{i},s^ {\prime}_{j})\in\mathcal{S}^{2}}T_{s_{i}}^{\pi}(s^{\prime}_{i})T_{s_{j}}^{\pi} (s^{\prime}_{j})G^{\pi}(s^{\prime}_{i},s^{\prime}_{j})=(\mathcal{F}_{M}^{\pi} G^{\pi})(s_{i},s_{j})\,.\qed\]

### Expectile Regression

Expectile regression, a method in statistics, is an extension of quantile regression that provides a more detailed analysis of a distribution's tail. This technique aims to estimate the expectiles of a conditional distribution, which are like percentiles but with respect to the mean, not the median. In essence, expectile regression can help capture the structure of data variability and analyze extreme observations in a more precise manner than quantile regression. The \(\tau\in(0,1)\) expectile of some random variable \(X\) is defined as a solution to the asymmetric least squares problem:

\[\operatorname*{arg\,min}_{m_{\tau}}\mathbb{E}_{x\sim X}\left[L_{2}^{\tau}\left( x-m_{\tau}\right)\right],\] (20)

where \(L_{2}^{\tau}(u)=|\tau-1(u<0)|u^{2}\). That is, for \(\tau>0.5\), this asymmetric loss function downweights the contributions of \(x\) values smaller than \(m_{\tau}\) while giving more weights to larger values. Figure 5 shows the illustration of this asymmetric loss. More detailed descriptions can be found in [28, 36].

## Appendix D Proof

### Connection between bisimulation error and bisimulation Bellman residual

In this section, we will revise some definitions a bit for obtaining the equivalence between bisimulation error and bisimulation Bellman residual. We first define _bisimulation error_\(\Delta_{\phi}^{\pi}\) that measure the distance of the approximation \(G_{\phi}^{\pi}\) to the fixed point \(G_{\sim}^{\pi}\) as:

\[\Delta_{\phi}^{\pi}:=G_{\phi}^{\pi}(s_{i},s_{j})-G_{\sim}^{\pi}(s_{i},s_{j}).\] (21)

And define _bisimulation Bellman residual_\(\epsilon_{\phi}^{\pi}\) as:

\[\epsilon_{\phi}^{\pi}:=G_{\phi}^{\pi}(s_{i},s_{j})-\mathcal{F}^{\pi}G_{\phi}^{ \pi}(s_{i},s_{j}).\] (22)

Notably, this is slightly different from the notation in Section 4 given the fact that we do not apply absolute value here. Then, we can have the following theorems.

Figure 5: The asymmetric squared loss used for expectile regression. Larger \(\tau\) gives more weight to positive differences.

**Theorem 14**.: _(**The bisimulation Bellman residual can be defined as a function of the bisimulation error**)_

\[\epsilon_{\phi}^{\pi}(s_{i},s_{j})=\Delta_{\phi}^{\pi}(s_{i},s_{j})-\gamma\mathbb{ E}_{s_{i}^{\prime}\sim T_{s_{i}}^{\pi}}\left[\Delta_{\phi}^{\pi}(s_{i}^{ \prime},s_{j}^{\prime})\right],\] (23)

Proof.: This follows directly from the bisimulation update operator:

\[\begin{split}\epsilon_{\phi}^{\pi}(s_{i},s_{j})&=G_ {\phi}^{\pi}(s_{i},s_{j})-\mathcal{F}^{\pi}G_{\phi}^{\pi}(s_{i},s_{j})\\ &=G_{\sim}^{\pi}(s_{i},s_{j})+\Delta_{\phi}^{\pi}(s_{i},s_{j})- \mathcal{F}^{\pi}(G_{\sim}^{\pi}(s_{i},s_{j})+\Delta_{\phi}^{\pi}(s_{i},s_{j} ))\\ &=\Delta_{\phi}^{\pi}(s_{i},s_{j})-\gamma\mathbb{E}_{s_{i}^{ \prime}\sim T_{s_{j}}^{\pi}}\left[\Delta_{\phi}^{\pi}(s_{i}^{\prime},s_{j}^{ \prime})\right]\\ \end{split}\] (24)

**Theorem 15**.: _(**The bisimulation error can be defined as a function of the bisimulation Bellman residual**). For any state pair \((s_{i},s_{j})\in\mathcal{S}\times\mathcal{S}\), the approximation error \(\Delta_{\phi}^{\pi}(s_{i},s_{j})\) can be defined as a function of the Bellman bisimulation error \(\epsilon_{\phi}\)_

\[\Delta_{\phi}^{\pi}(s_{i},s_{j})=\frac{1}{1-\gamma}\mathbb{E}_{(s_{i}^{\prime },s_{j}^{\prime})\sim\mu_{\pi}}\left[\epsilon_{\phi}^{\pi}(s_{i}^{\prime},s_{j }^{\prime})\right].\] (25)

Proof.: Our proof follows similar steps to the proof of Lemma 6.1 in [26] and Theorem 1 in [17]. First by definition:

\[\begin{split}&\Delta_{\phi}^{\pi}(s_{i},s_{j}):=G_{\phi}^{\pi}(s_{i},s_ {j})-G_{\sim}^{\pi}(s_{i},s_{j})\\ &\Rightarrow G_{\sim}^{\pi}(s_{i},s_{j})=G_{\phi}^{\pi}(s_{i},s_ {j})-\Delta_{\phi}^{\pi}(s_{i},s_{j})\end{split}\] (26)

Then we can decompose the error:

\[\begin{split}\Delta_{\phi}^{\pi}(s_{i},s_{j})&=G_ {\phi}^{\pi}(s_{i},s_{j})-G_{\sim}^{\pi}(s_{i},s_{j})\\ &=G_{\phi}^{\pi}(s_{i},s_{j})-\left(|r_{s_{i}}^{\pi}-r_{s_{j}}^{ \pi}|+\gamma\mathbb{E}_{s_{i}^{\prime}\sim T_{s_{i}}^{\pi}}\left[G_{\sim}^{ \pi}(s_{i}^{\prime},s_{i}^{\prime})\right]\right)\\ &=G_{\phi}^{\pi}(s_{i},s_{j})-\left(|r_{s_{i}}^{\pi}-r_{s_{j}}^{ \pi}|+\gamma\mathbb{E}_{s_{i}^{\prime}\sim T_{s_{i}}^{\pi}}\left[G_{\phi}^{ \pi}(s_{i}^{\prime},s_{j}^{\prime})-\Delta_{\phi}^{\pi}(s_{i}^{\prime},s_{j}^{ \prime})\right]\right)\\ &=G_{\phi}^{\pi}(s_{i},s_{j})-\left(|r_{s_{i}}^{\pi}-r_{s_{j}}^{ \pi}|+\gamma\mathbb{E}_{s_{i}^{\prime}\sim T_{s_{i}}^{\pi}}\left[G_{\phi}^{ \pi}(s_{i}^{\prime},s_{j}^{\prime})\right]\right)+\gamma\mathbb{E}_{s_{i}^{ \prime}\sim T_{s_{j}}^{\pi}}\left[\Delta_{\phi}^{\pi}(s_{i}^{\prime},s_{j}^{ \prime})\right]\\ &=\epsilon_{\phi}^{\pi}(s_{i},s_{j})+\gamma\mathbb{E}_{s_{i}^{ \prime}\sim T_{s_{j}}^{\pi}}\left[\Delta_{\phi}^{\pi}(s_{i}^{\prime},s_{j}^{ \prime})\right]\end{split}\] (27)

By considering the operator \(G\) as the Bellman evaluation operator for the lifted MDP (See Section C.4), we can rewrite the formula as:

\[\Delta_{\phi}^{\tilde{\pi}}(\tilde{x})=\epsilon_{\phi}^{\tilde{\pi}}(\tilde{x} )+\gamma\mathbb{E}_{\tilde{x}^{\prime}\sim T_{\tilde{x}}^{\tilde{\pi}}}\left[ \Delta_{\phi}^{\tilde{\pi}}(\tilde{x}^{\prime})\right].\] (28)

Then we can treat \(\Delta_{\phi}^{\tilde{\pi}}(\tilde{x})\) as a value function and \(\epsilon_{\phi}^{\tilde{\pi}}(\tilde{x})\) as reward, we can see that:

\[\Delta_{\phi}^{\tilde{\pi}}(\tilde{x})=\frac{1}{1-\gamma}\mathbb{E}_{\tilde{x} ^{\prime}\sim T_{\tilde{x}}^{\tilde{\pi}}}\left[\epsilon_{\phi}^{\tilde{\pi}}( \tilde{x}^{\prime})\right].\] (29)

Then we can obtain

\[\Delta_{\phi}^{\pi}(s_{i},s_{j})=\frac{1}{1-\gamma}\mathbb{E}_{(s_{i}^{\prime}, s_{j}^{\prime})\sim\mu_{\pi}}\left[\epsilon_{\phi}^{\pi}(s_{i}^{\prime},s_{j}^{ \prime})\right].\] (30)

### Theorem 3

**Theorem 3**.: _(Bisimulation error upper-bound). Let \(\mu_{\pi}(s)\) denote the stationary distribution over states, let \(\mu_{\pi}(\cdot,\cdot)\) denote the joint distribution over synchronized pairs of states \((s_{i},s_{j})\) sampled independently from \(\mu_{\pi}(\cdot)\). For any state pair \((s_{i},s_{j})\in\mathcal{S}\times\mathcal{S}\), the bisimulation error \(\Delta_{\phi}^{\pi}(s_{i},s_{j})\) can be upper-bounded by a sum of expected bisimulation Bellman residuals \(\epsilon_{\phi}^{\pi}\):_

\[\Delta_{\phi}^{\pi}(s_{i},s_{j})\leq\frac{1}{1-\gamma}\mathbb{E}_{(s_{i},s_{j} )\sim\mu_{\pi}}\left[\epsilon_{\phi}^{\pi}(s_{i},s_{j})\right].\] (31)

Proof.: We start from Equation 25 in Section D.1.

\[\Delta_{\phi}^{\pi}(s_{i},s_{j}) =\frac{1}{1-\gamma}\mathbb{E}_{(s_{i}^{\prime},s_{j}^{\prime}) \sim\mu_{\pi}}\left[\epsilon_{\phi}^{\pi}(s_{i}^{\prime},s_{j}^{\prime})\right]\] (32) \[\Rightarrow|\Delta_{\phi}^{\pi}(s_{i},s_{j})| =\frac{1}{1-\gamma}\left|\mathbb{E}_{(s_{i}^{\prime},s_{j}^{ \prime})\sim\mu_{\pi}}\left[\epsilon_{\phi}^{\pi}(s_{i}^{\prime},s_{j}^{ \prime})\right]\right|\] \[\leq\frac{1}{1-\gamma}\mathbb{E}_{(s_{i}^{\prime},s_{j}^{\prime}) \sim\mu_{\pi}}\left[\left|\epsilon_{\phi}^{\pi}(s_{i}^{\prime},s_{j}^{\prime })\right|\right].\]

Then when we define bisimulation error \(\Delta_{\phi}^{\pi}(s_{i},s_{j}):=|\Delta_{\phi}^{\pi}(s_{i},s_{j})|\) and bisimulation Bellman residual \(\epsilon_{\phi}^{\pi}(s_{i}^{\prime},s_{j}^{\prime}):=|\epsilon_{\phi}^{\pi}( s_{i}^{\prime},s_{j}^{\prime})|\), we have

\[\Delta_{\phi}^{\pi}(s_{i},s_{j})\leq\frac{1}{1-\gamma}\mathbb{E}_{(s_{i}^{ \prime},s_{j}^{\prime})\sim\mu_{\pi}}\left[\epsilon_{\phi}^{\pi}(s_{i}^{ \prime},s_{j}^{\prime})\right].\] (33)

### Proposition 4

**Proposition 4**.: _(The expected bisimulation residual is not sufficient over incomplete datasets). If there exists states \(s_{i}^{\prime}\) and \(s_{j}^{\prime}\) not contained in dataset \(\mathcal{D}\), where the occupancy \(\mu_{\pi}(s_{i}^{\prime}|s_{i},a_{i})>0\) and \(\mu_{\pi}(s_{j}^{\prime}|s_{j},a_{j})>0\) for some \(s_{i}\in\mathcal{D},s_{j}\in\mathcal{D}\), then there exists a bisimulation measurement and \(C>0\) such that_

* _For all_ \((\hat{s_{i}},\hat{s_{j}})\in\mathcal{D}\)_, the bisimulation Bellman residual_ \(\epsilon_{\phi}^{\pi}(\hat{s_{i}},\hat{s_{j}})=0\)_._
* _There exists_ \((s_{i},s_{j})\in\mathcal{D}\)_, such that the bisimulation error_ \(\Delta_{\phi}^{\pi}(s_{i},s_{j})=C\)_._

Proof.: This is a direct consequence of Theorem 15. Let \(\mathcal{D}^{\prime}\) contain the set of state pairs \((s_{i}^{\prime},s_{j}^{\prime})\) not contained in the dataset \(\mathcal{D}\), where the next-state pair occupancy \(\mu_{\pi}(s_{i}^{\prime},s_{j}^{\prime}|s_{i},a_{i},s_{j},a_{j})>0\). Let \(\mathcal{D}_{\text{unique}}\) be the set of unique state pairs in \(\mathcal{D}\). It follows that

\[\Delta_{\phi}^{\pi}(s_{i},s_{j})= \frac{1}{1-\gamma}\mathbb{E}_{(s_{i}^{\prime},s_{j}^{\prime})\sim \mu_{\pi}}\left[\epsilon_{\phi}^{\pi}(s_{i}^{\prime},s_{j}^{\prime})\right]\] (34) \[= \frac{1}{1-\gamma}\sum_{(s_{i}^{\prime},s_{j}^{\prime})\sim \mathcal{D}_{\text{unique}}}\mu_{\pi}((s_{i}^{\prime},s_{j}^{\prime})|s_{i},a_{ i},s_{j},a_{j})\epsilon_{\phi}^{\pi}(s_{i}^{\prime},s_{j}^{\prime})+\] \[\qquad\frac{1}{1-\gamma}\sum_{(s_{i}^{\prime},s_{j}^{\prime})\sim \mathcal{D}^{\prime}}\mu_{\pi}((s_{i}^{\prime},s_{j}^{\prime})|s_{i},a_{i},s_{ j},a_{j})\epsilon_{\phi}^{\pi}(s_{i}^{\prime},s_{j}^{\prime})\]

Recall that \(\epsilon_{\phi}^{\pi}(s_{i},s_{j})=\Delta_{\phi}^{\pi}(s_{i},s_{j})-\gamma \mathbb{E}_{s_{i}^{\prime}\sim\mathcal{D}_{s_{i}^{\prime}}}\left[\Delta_{\phi }^{\pi}(s_{i}^{\prime},s_{j}^{\prime})\right]\), and there exists at least one \(G(s_{i},s_{j})\), such that \((s_{i},s_{j})\in\mathcal{D}^{\prime}\). Since the sets \(\mathcal{D}\) and \(\mathcal{D}^{\prime}\) are distinct, it follows that there exists a measurement \(G\) such that \(\epsilon_{\phi}^{\pi}(s_{i},s_{j})=0\) for all \((s_{i},s_{j})\in\mathcal{D}\), but \(\frac{1}{1-\gamma}\sum_{(s_{i}^{\prime},s_{j}^{\prime})\sim\mathcal{D}^{ \prime}}\mu_{\pi}(s_{i}^{\prime},s_{j}^{\prime}|s_{i},a_{i},s_{j},a_{j}) \epsilon_{\phi}^{\pi}(s_{i}^{\prime},s_{j}^{\prime})=C\)

### Lemma 5

**Lemma 5**.: _For any \(\tau\in\) [0, 1), \(\mathcal{F}_{\tau}^{\pi}\) is a \(\gamma_{\tau}\)-contraction, where \(\gamma_{\tau}=1-2\alpha(1-\gamma)min\left\{\tau,1-\tau\right\}\)._

Proof.: Note that \(\mathcal{F}_{1/2}^{\pi}\) is the standard bisimulation operator for \(\pi\), of which the fixed point is \(G_{\sim}^{\pi}\). To keep the notation succinct, we will replace \(G^{\pi}\) with \(G\). For any \(G_{1}\), \(G_{2}\),

\[\mathcal{F}_{1/2}^{\pi}G_{1}(s_{i},s_{j})-\mathcal{F}_{1/2}^{\pi }G_{2}(s_{i},s_{j})\] (35) \[=(G_{1}(s_{i},s_{j})+\alpha\mathbb{E}^{\pi}[\delta_{i}])-(G_{2}( s_{i},s_{j})+\alpha\mathbb{E}^{\pi}[\delta_{j}])\] \[=(1-\alpha)(G_{1}(s_{i},s_{j})-G_{2}(s_{i},s_{j}))+\alpha\mathbb{ E}_{\pi}[(1-\gamma)|r_{s_{i}}^{\pi}-r_{s_{j}}^{\pi}|+\gamma G_{1}(s_{i}^{ \prime},s_{j}^{\prime})\] \[\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad-(1- \gamma)|r_{s_{i}}^{\pi}-r_{s_{j}}^{\pi}|-\gamma G_{2}(s_{i}^{\prime},s_{j}^{ \prime})]\] \[=(1-\alpha)(G_{1}(s_{i},s_{j})-G_{2}(s_{i},s_{j}))+\alpha\mathbb{ E}_{\pi}[\gamma G_{1}(s_{i}^{\prime},s_{j}^{\prime})-\gamma G_{2}(s_{i}^{\prime},s_{j}^{ \prime})]\] \[\leq(1-\alpha)\|G_{1}-G_{2}\|_{\infty}+\alpha\gamma\|G_{1}-G_{2}\| _{\infty}\] \[=(1-\alpha(1-\gamma))\|G_{1}-G_{2}\|_{\infty}.\]

When \(\tau\neq\frac{1}{2}\), we introduce two more operators to simplify the analysis:

\[(\mathcal{F}_{+}^{\pi}G_{1})(s_{i},s_{j}) =G(s_{i},s_{j})+\mathbb{E}^{\pi}[\delta]_{+}\] (36) \[(\mathcal{F}_{-}^{\pi}G_{2})(s_{i},s_{j}) =G(s_{i},s_{j})+\mathbb{E}^{\pi}[\delta]_{-}\]

Now we show that both operators meet the Banach-fixed point theorem (e.g. \(\|\mathcal{F}_{+}^{\pi}G_{1}-\mathcal{F}_{+}^{\pi}G_{2}\|_{\infty}\leq\|G_{1} -G_{2}\|_{\infty}\)). For any \(G_{1}\), \(G_{2}\):

\[(\mathcal{F}_{+}^{\pi}G_{1})(s_{i},s_{j})-(\mathcal{F}_{+}^{\pi} G_{2})(s_{i},s_{j})\] (37) \[=G_{1}-G_{2}+\mathbb{E}^{\pi}[[\delta_{i}]_{+}-[\delta_{j}]_{+}]\] \[=\mathbb{E}^{\pi}[G_{1}+[\delta_{i}]_{+}-(G_{2}+[\delta_{j}]_{+})]\]

The relationship between \(G_{1}+[\delta_{i}]_{+}\) and \(G_{2}+[\delta_{j}]_{+}\) exists in four cases:

* \(\delta_{i}\geq 0\), \(\delta_{j}\geq 0\), then \[G_{1}+[\delta_{i}]_{+}-(G_{2}+[\delta_{j}]_{+})=\gamma(G_{1}(s_{i}^{\prime},s_ {j}^{\prime})-G_{2}(s_{i}^{\prime},s_{j}^{\prime})).\] (38)
* \(\delta_{i}<0\), \(\delta_{j}<0\), then \[G_{1}+[\delta_{i}]_{+}-(G_{2}+[\delta_{j}]_{+})=G_{1}(s_{i},s_{j})-G_{2}(s_{i}, s_{j}).\] (39)
* \(\delta_{i}\geq 0\), \(\delta_{j}<0\), then \[G_{1}+[\delta_{i}]_{+}-(G_{2}+[\delta_{j}]_{+})\] (40) \[=(1-\gamma)|r_{s_{i}}^{\pi}-r_{s_{j}}^{\pi}|+\gamma G_{1}(s_{i}^{ \prime},s_{j}^{\prime})-((1-\gamma)|r_{s_{i}}^{\pi}-r_{s_{j}}^{\pi}|+\gamma G_ {2}(s_{i}^{\prime},s_{j}^{\prime}))\] \[=\gamma(G_{1}(s_{i}^{\prime},s_{j}^{\prime})-G_{2}(s_{i}^{\prime },s_{j}^{\prime})),\] where the inequality comes from \(G_{2}(s_{i},s_{j})>(1-\gamma)|r_{s_{i}}^{\pi}-r_{s_{j}}^{\pi}|+\gamma G_{2}(s_{ i}^{\prime},s_{j}^{\prime})\).
* \(\delta_{i}<0\), \(\delta_{j}\geq 0\), then \[G_{1}+[\delta_{i}]_{+}-(G_{2}+[\delta_{j}]_{+})\] (41) \[=G_{1}(s_{i},s_{j})-((1-\gamma)|r_{s_{i}}^{\pi}-r_{s_{j}}^{\pi}|+G _{2}(s_{i}^{\prime},s_{j}^{\prime}))\] \[\leq G_{1}(s_{i},s_{j})-G_{2}(s_{i},s_{j}),\] where the inequality comes from \(G_{2}(s_{i},s_{j})\leq(1-\gamma)|r_{s_{i}}^{\pi}-r_{s_{j}}^{\pi}|+\gamma G_{2}( s_{i}^{\prime},s_{j}^{\prime})\).

As a result, we have \((\mathcal{F}_{+}^{\pi}G_{1})(s_{i},s_{j})-(\mathcal{F}_{+}^{\pi}G_{2})(s_{i},s_{j}) \leq\|G_{1}-G_{2}\|_{\infty}\). Combine \(\mathcal{F}_{+}^{\pi}\) and \(\mathcal{F}_{-}^{\pi}\), we can rewrite \(\mathcal{F}_{\tau}^{\pi}\) as:

\[\mathcal{F}_{\tau}^{\pi}G(s_{i},s_{j}) =G(s_{i},s_{j})+2\alpha\mathbb{E}^{\pi}[\tau[\delta]_{+}+(1-\tau)[ \delta]_{-}]\] \[=(1-2\alpha)G(s_{i},s_{j})+2\alpha\tau(G(s_{i},s_{j})+\mathbb{E}^{ \pi}[[\delta]_{+}]+2\alpha(1-\tau)(G(s_{i},s_{j})+\mathbb{E}^{\pi}[[\delta]_{- }])\] \[=(1-2\alpha)G(s_{i},s_{j})+2\alpha\tau(\mathcal{F}_{+}^{\pi}G_{1}) (s_{i},s_{j})+2\alpha(1-\tau)(\mathcal{F}_{-}^{\pi}G_{1})(s_{i},s_{j}).\] (42)

What's more

\[\mathcal{F}_{\frac{1}{2}}^{\pi}G(s_{i},s_{j}) =G(s_{i},s_{j})+\alpha\mathbb{E}^{\pi}[\delta]\] (43) \[=G(s_{i},s_{j})+\alpha((\mathcal{F}_{+}^{\pi}G_{1})(s_{i},s_{j}) +(\mathcal{F}_{-}^{\pi}G_{1})(s_{i},s_{j})-2\alpha G(s_{i},s_{j}))\] \[=(1-2\alpha)G(s_{i},s_{j})+\alpha((\mathcal{F}_{+}^{\pi}G_{1})(s _{i},s_{j})+(\mathcal{F}_{-}^{\pi}G_{1})(s_{i},s_{j})).\]

When \(\tau>\frac{1}{2}\), for any \(G_{1}\) and \(G_{2}\):

\[(\mathcal{F}_{\tau}^{\pi}G_{1})(s_{i},s_{j})-(\mathcal{F}_{\tau}^ {\pi}G_{2})(s_{i},s_{j})\] \[=(1-2\alpha)(G_{1}(s_{i},s_{j})-G_{2}(s_{i},s_{j}))+2\alpha\tau( (\mathcal{F}_{+}^{\pi}G_{1})(s_{i},s_{j})-(\mathcal{F}_{+}^{\pi}G_{2})(s_{i},s _{j}))\] \[\qquad\qquad\qquad\qquad\qquad\qquad+2\alpha(1-\tau)((\mathcal{F }_{-}^{\pi}G_{1})(s_{i},s_{j})-(\mathcal{F}_{-}^{\pi}G_{2})(s_{i},s_{j}))\] \[=(1-2\alpha-2(1-2\alpha)(1-\tau))(G_{1}(s_{i},s_{j})-G_{2}(s_{i}, s_{j}))_{2}(1-\tau)((\mathcal{F}_{\frac{1}{2}}^{\pi}G_{1})(s_{i},s_{j})-( \mathcal{F}_{\frac{1}{2}}^{\pi}G_{2})(s_{i},s_{j}))\] \[\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad-2\alpha (1-2\tau)((\mathcal{F}_{+}^{\pi}G_{1})(s_{i},s_{j})-(\mathcal{F}_{+}^{\pi}G_{ 2})(s_{i},s_{j}))\] \[\leq(1-2\alpha-2(1-2\alpha)(1-\tau))\|G_{1}(s_{i},s_{j})-G_{2}(s_{ i},s_{j})\|_{\infty}\] \[\leq(1-2\alpha-2(1-2\alpha)(1-\tau))\|G_{1}(s_{i},s_{j})-G_{2}(s_{i },s_{j})\|_{\infty}\] \[\leq(1-2\alpha-2(1-2\alpha)(1-\tau))\|G_{1}(s_{i},s_{j})-G_{2}(s_{ i},s_{j})\|_{\infty}\] \[=(1-2\alpha(1-\tau)(1-\gamma))\|G_{1}(s_{i},s_{j})-G_{2}(s_{i},s_{j })\|_{\infty}\] (44)

When \(\tau<\frac{1}{2}\), for any \(G_{1}\) and \(G_{2}\):

\[(\mathcal{F}_{\tau}^{\pi}G_{1})(s_{i},s_{j})-(\mathcal{F}_{\tau}^ {\pi}G_{2})(s_{i},s_{j})\] \[=(1-2\alpha)(G_{1}(s_{i},s_{j})-G_{2}(s_{i},s_{j}))+2\alpha\tau(( \mathcal{F}_{+}^{\pi}G_{1})(s_{i},s_{j})-(\mathcal{F}_{+}^{\pi}G_{2})(s_{i},s_{ j}))\] \[\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad+2\alpha(1- \tau)((\mathcal{F}_{-}^{\pi}G_{1})(s_{i},s_{j})-(\mathcal{F}_{-}^{\pi}G_{2})(s_{ i},s_{j}))\] \[=(1-2\alpha-2\tau(1-2\alpha))(G_{1}(s_{i},s_{j})-G_{2}(s_{i},s_{ j}))+2\tau((\mathcal{F}_{\frac{1}{2}}^{\pi}G_{1})(s_{i},s_{j})-(\mathcal{F}_{\frac{1}{2}}^{ \pi}G_{2})(s_{i},s_{j}))\] \[\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad+2\alpha(1-2 \tau)((\mathcal{F}_{-}^{\pi}G_{1})(s_{i},s_{j})-(\mathcal{F}_{-}^{\pi}G_{2})(s_{ i},s_{j}))\] \[\leq(1-2\alpha-2\tau(1-2\alpha))\|G_{1}(s_{i},s_{j})-G_{2}(s_{i}, s_{j})\|_{\infty}\] \[\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad+2\tau(1- \alpha(1-\gamma))\|G_{1}(s_{i},s_{j})-G_{2}(s_{i},s_{j})\|_{\infty}\] \[\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad+2\alpha(1 -2\tau)\|G_{1}(s_{i},s_{j})-G_{2}(s_{i},s_{j})\|_{\infty}\] \[=(1-2\alpha\tau(1-\gamma))\|G_{1}(s_{i},s_{j})-G_{2}(s_{i},s_{j}) \|_{\infty}.\] (45)

### Lemma 6

**Lemma 6**.: _For any \(\tau,\tau^{\prime}\in[0,1)\) with \(\tau^{\prime}\geq\tau\), and for all \(s_{i},s_{j}\in\mathcal{S}\) and any \(\alpha\), we have \(G_{\tau^{\prime}}\geq G_{\tau}\)._

Proof.: We denote \(G_{\tau^{\prime}}\) is the fixed point of applying the operator \(\mathcal{F}_{\tau^{\prime}}^{\pi}\), and \(G_{\tau}\) is the fixed point of applying the operator \(\mathcal{F}_{\tau}\). Based on Equation 6, we have:

\[\mathcal{F}_{\tau^{\prime}}^{\pi}G(s_{i},s_{j})-\mathcal{F}_{\tau}^{ \pi}G(s_{i},s_{j})\] \[=(1-2\alpha)G(s_{i},s_{j})+2\alpha\tau^{\prime}\mathcal{F}_{+}^{ \pi}G(s_{i},s_{j})+2\alpha(1-\tau^{\prime})\mathcal{F}_{-}^{\pi}G(s_{i},s_{j})\] \[\qquad\qquad\qquad\qquad\qquad\qquad\qquad-((1-2\alpha)G(s_{i},s_{ j})+2\alpha\tau\mathcal{F}_{+}^{\pi}G(s_{i},s_{j})+2\alpha(1-\tau)\mathcal{F}_{-}^{\pi}G(s_{i},s_{j}))\] (46) \[=2\alpha(\tau^{\prime}-\tau)(\mathcal{F}_{+}^{\pi}G(s_{i},s_{j})- \mathcal{F}_{-}^{\pi}G(s_{i},s_{j}))\] \[=2\alpha(\tau^{\prime}-\tau)\mathbb{E}^{\pi}[[\delta]_{+}-[\delta]_{- }]\geq 0.\]

Therefore \(G_{\tau^{\prime}}>G_{\tau}\)

### Theorem 7

**Theorem 7**.: _In deterministic MDP and fixed finite dataset, we have:_

\[\lim_{\tau\to 1}G_{\tau}(s_{i},s_{j})=\max_{\begin{subarray}{c}a_{i}\in\mathcal{A},a_{j}\in\mathcal{A}\\ s.t.\ \pi_{\beta}(a_{i}|s_{i})>0,\pi_{\beta}(a_{j}|s_{j})>0\end{subarray}}G_{ \sim}^{*}((s_{i},a_{i}),(s_{j},a_{j})).\] (47)

_where \(G_{\sim}^{*}((s_{i},a_{i}),(s_{j},a_{j}))\) is a fixed-point measurement constrained to the dataset and defined on state-action space \(\mathcal{S}\times\mathcal{A}\) as_

\[G_{\sim}^{*}((s_{i},a_{i}),(s_{j},a_{j}))=|r(s_{i},a_{i})-r(s_{j},a_{j})|+\gamma \mathbb{E}_{s_{i}^{\prime}\sim T_{s_{j}}^{\pi_{\beta}}}\left[\max_{ \begin{subarray}{c}a_{i}^{\prime}\in\mathcal{A},a_{j}^{\prime}\in\mathcal{A} \\ s_{j}^{\prime}\sim T_{s_{j}}^{\pi_{\beta}}\end{subarray}}G_{\sim}^{*}((s_{i}^{ \prime},a_{i}^{\prime}),(s_{j}^{\prime},a_{j}^{\prime}))\right].\] (48)

Proof.: First, we can easily proof that \(G_{\sim}^{*}(s_{i},a_{i},s_{j},a_{j})\) is a fixed point. Define the corresponding operator of \(G_{\sim}^{*}\) is \(F^{*}\), we can know that \(F^{*}\) is a contraction. Then, we have

**Corollary 16**.: _For any \(\tau\), \(s_{i},s_{j}\in\mathcal{S}\) we have_

\[G_{\tau}(s_{i},s_{j})\leq\max_{\begin{subarray}{c}a_{i}\in\mathcal{A},a_{j} \in\mathcal{A}\\ s.t.\ \pi_{\beta}(a_{i}|s_{i})>0,\pi_{\beta}(a_{j}|s_{j})>0\end{subarray}}G_{ \sim}^{*}((s_{i},a_{i}),(s_{j},a_{j}))\] (49)

Proof.: The proof follows from the observation that convex combination is smaller than maximum. 

Besides, we also have

**Lemma 17**.: _Let \(X\) be a real-valued random variable with a bounded support and supremum of the support is \(x^{*}\). Then,_

\[\lim_{\tau\to 1}m_{\tau}=x^{*}\]

Proof.: Same as the Lemma 1 in [28]. One can show that expectiles of a random variable have the same supremum \(x^{*}\). Moreover, for all \(\tau_{1}\) and \(\tau_{2}\) such that \(\tau_{1}<\tau_{2}\), we get \(m_{\tau_{1}}\leq m_{\tau_{2}}\). Therefore, the limit follows from the properties of bounded monotonically non-decreasing functions. 

Combining Corollary 16 and Lemma 17, we can obtain the above.

### Theorem 8

**Theorem 8**.: _(Value bound based on on-policy bisimulation measurements in terms of encoder error). Given an MDP \(\widetilde{\mathcal{M}}\) constructed by aggregating states in an \(\omega\)-neighborhood, and an encoder \(\phi\) that maps from states in the original MDP \(\mathcal{M}\) to these clusters, the value functions for the two MDPs are bounded as_

\[\left|V^{\pi}\left(s_{i}\right)-\widetilde{V}^{\pi}\left(\phi\left(s_{i}\right) \right)\right|\leq\frac{2\omega+\hat{\Delta}}{c_{r}(1-\gamma)}.\] (50)

_where \(\hat{\Delta}:=\|\hat{G}_{\sim}^{\pi}-\hat{G}_{\phi}^{\pi}\|_{\infty}\) is the approximation error._

Proof.: Let the reward function be bounded as \(R\in[0,1]\), \(\phi:\mathcal{S}\rightarrow\widetilde{\mathcal{S}}\), and \(\phi(s_{i})=\phi(s_{j})\Rightarrow\hat{G}_{\phi}^{\pi}(s_{i},s_{j})=|\phi(s_{i })-\phi(s_{j})|\leq 2\,\omega\), we can conduct an aggregat MDP \(\widetilde{M}=(\widetilde{\mathcal{S}},\mathcal{A},\widetilde{T},\widetilde{ R})\). Let \(\xi\) bea measure on \(\mathcal{S}\). Following Lemma 8 in [27], we have that:

\[\left|V^{\pi}\left(s_{i}\right)-\widetilde{V}^{\pi}\left(\phi\left(s_ {i}\right)\right)\right| \leq\frac{c_{r}^{-1}}{\xi(\phi(s))}\int\limits_{z\in\phi(s)}c_{R} \left|r^{\pi}(s)-r^{\pi}(z)\right|\] (51) \[\qquad+(1-\gamma)\left|\ \int\limits_{s^{\prime}\in\mathcal{S}} \left(T^{\pi}(s^{\prime}|s)-T^{\pi}(s^{\prime}|z)\right)\frac{c_{r}\gamma}{1- \gamma}V^{\pi}(s^{\prime})ds^{\prime}\right|d\xi(z)+\gamma\|V^{\pi}-\widetilde{ V}^{\pi}\|_{\infty}\] \[\leq\frac{c_{r}^{-1}}{\xi(\phi(s))}\int\limits_{z\in\phi(s)}G_{ \sim}^{\pi}(s,z)d\xi(z)+\gamma\|V^{\pi}-\widetilde{V}^{\pi}\|_{\infty}\]

Thus, taking the supremum on the LHS, we have:

\[\begin{split}(1-\gamma)\left|V^{\pi}\left(s_{i}\right)- \widetilde{V}^{\pi}\left(\phi\left(s_{i}\right)\right)\right|& \leq\frac{c_{r}^{-1}}{\xi(\phi(s))}\int\limits_{z\in\phi(s)}G_{ \sim}^{\pi}(s,z)d\xi(z)\\ &\leq\frac{c_{r}^{-1}}{\xi(\phi(s))}\int\limits_{z\in\phi(s)} \hat{G}_{\phi}^{\pi}(s,z)+\|G_{\sim}^{\pi}-\hat{G}_{\phi}^{\pi}\|_{\infty}d\xi (z)\\ &=\frac{c_{r}^{-1}}{\xi(\phi(s))}\int\limits_{z\in\phi(s)}(2\omega +\hat{\Delta})d\xi(z)\\ &=c_{r}^{-1}(2\omega+\hat{\Delta}).\end{split}\] (52)

Therefore,

\[\left|V^{\pi}\left(s_{i}\right)-\widetilde{V}^{\pi}\left(\phi\left(s_{i} \right)\right)\right|\leq\frac{2\omega+\hat{\Delta}}{c_{r}(1-\gamma)},\] (53)

## Appendix E Understanding of Theorem 7

**Theorem 7**.: _In deterministic MDP and fixed finite dataset, we have:_

\[\lim_{\tau\to 1}G_{\tau}(s_{i},s_{j})=\max_{\begin{subarray}{c}a_{i}\in \mathcal{A},a_{j}\in\mathcal{A}\\ s.t.\ \pi_{\beta}(a_{i}|s_{i})>0,\pi_{\beta}(a_{j}|s_{j})>0\end{subarray}}G_{ \sim}^{*}((s_{i},a_{i}),(s_{j},a_{j})).\] (54)

_where \(G_{\sim}^{*}((s_{i},a_{i}),(s_{j},a_{j}))\) is a fixed-point measurement constrained to the dataset and defined on the state-action space \(\mathcal{S}\times\mathcal{A}\) as_

\[G_{\sim}^{*}((s_{i},a_{i}),(s_{j},a_{j}))=\left|r(s_{i},a_{i})-r(s_{j},a_{j}) \right|+\gamma\mathbb{E}_{\begin{subarray}{c}s_{i}^{\prime}\sim T_{s_{j}}^{ \pi_{\beta}}\\ s_{j}^{\prime}\sim T_{s_{j}}^{\pi_{\beta}}\end{subarray}}\left[\max_{ \begin{subarray}{c}a_{i}^{\prime}\in\mathcal{A},a_{j}^{\prime}\in\mathcal{A}\\ s_{i}\in\pi_{\beta}(a_{i}^{\prime}|s_{i}^{\prime})>0,\pi_{\beta}(a_{j}^{\prime }|s_{j}^{\prime})>0\end{subarray}}G_{\sim}^{*}((s_{i}^{\prime},a_{i}^{\prime}),(s_{j}^{\prime},a_{j}^{\prime}))\right].\] (55)

Given the MDP specified by the tuple \((\mathcal{S},\mathcal{A},T,R)\), we construct a lifted MDP \((\widetilde{\mathcal{S}},\widetilde{\mathcal{A}},\widetilde{T},\widetilde{R})\), by taking the state space to be \(\widetilde{\mathcal{S}}=\mathcal{S}^{2}\), the action space to be \(\widetilde{\mathcal{A}}=\mathcal{A}^{2}\), the transition dynamics to be given by \(\widetilde{T}_{\tilde{s}}^{\tilde{a}}(\tilde{s}^{\prime})=\widetilde{T}_{(s_{ i},s_{j})}^{(a_{i},a_{j})}((s_{i}^{\prime},s_{j}^{\prime}))=T_{s_{i}}^{a_{i}}(s_{i}^{ \prime})T_{s_{j}}^{a_{j}}(s_{j}^{\prime})\) for all \((s_{i},s_{j}),(s_{i}^{\prime},s_{j}^{\prime})\in\mathcal{S}^{2}\), \(a_{i},a_{j}\in\mathcal{A}\), and the action-independent rewards to be \(\widetilde{R}_{\tilde{s}}=\widetilde{R}_{(s_{i},s_{j})}=|r_{s_{i}}^{\pi}-r_{s_ {j}}^{\pi}|\) for all \(s_{i},s_{j}\in\mathcal{S}\). The Bellman evaluation operator \(\widetilde{\mathcal{F}}^{\tilde{\pi}}\) for this lifted MDP at discount rate \(\gamma\) under the policy \(\tilde{\pi}(\tilde{a}|\tilde{s})=\tilde{\pi}(a_{i},a_{j}|s_{i},s_{j})=\pi(a_{i} |s_{i})\pi(a_{j}|s_{j})\) is given by (for all \(G^{\pi}\in\mathbb{R}^{\mathcal{S}\times\mathcal{S}}\) and \((s_{i},s_{j})\in\mathcal{S}\times\mathcal{S}\)):

\[(\widetilde{\mathcal{F}}^{\tilde{\pi}}Q^{*})(\tilde{s},\tilde{a})=\widetilde{R} _{\tilde{s},\tilde{a}}+\gamma\sum_{\tilde{\mathcal{S}}\in\widetilde{\mathcal{ S}}}\widetilde{T}_{\tilde{s}}^{\tilde{a}}(\tilde{s}^{\prime})\max_{ \tilde{a}\in\tilde{A}}Q^{*}(\tilde{s}^{\prime},\tilde{a}^{\prime}).\] (56)Though similar, Equation 55 has more constraints as it requires the possibility of \(\pi_{\beta}(a^{\prime}_{i}|s^{\prime}_{i})\) and \(\pi_{\beta}(a^{\prime}_{j}|s^{\prime}_{j})\) are larger than zero in the dataset. As such, we may also change the Equation 56 to:

\[(\tilde{\mathcal{F}}^{\tilde{\pi}}Q^{*})(\tilde{s},\tilde{a})=\widetilde{R}_{ \tilde{s},\tilde{a}}+\gamma\sum_{\tilde{s}\in\tilde{\mathcal{S}}}\widetilde{T} ^{\tilde{\pi}}_{\tilde{s}}(\tilde{s}^{\prime})\max_{\begin{subarray}{c}\tilde{ a}^{\prime}\in\mathcal{A}\\ \text{s.t.}\tilde{\pi}_{\beta}(\tilde{a}^{\prime}|\tilde{s}^{\prime})>0\end{subarray}}Q ^{*}(\tilde{s}^{\prime},\tilde{a}^{\prime}).\] (57)

This is, indeed, equivalent to the _in-sample_-style Q function in [28]. Intuitively, \(G^{*}_{\sim}((s_{i},a_{i}),(s_{j},a_{j}))\) can be interpreted as the optimal state-action value function \(Q^{*}(\tilde{s},\tilde{a})\) in a lifted MDP \(\widetilde{M}\). Then \(G^{*}_{\sim}((s_{i},a_{i}),(s_{j},a_{j}))\) is the state-action value function \(Q^{*}(\tilde{s},\tilde{a})\) that associated with policy \(\pi\), and \(G_{\sim}(s_{i},s_{j})\) as a state value function \(V(\tilde{s})\). And therefore, we can connect our expectile-based bisimulation operator to the lifted MDP, where we can use the conventional analytics tools in RL to analyze bisimulation operators.

## Appendix F Additional Experiments

### Ablation Study - Value of Expectile

Here we present the ablation study of setting different expectile \(\tau\in\{0.3,0.4,\cdots,0.7\}\) in Figure 6 to investigate the effect of the critical hyper-parameter in EBS. The experimental results demonstrate that the final performance gradually improves with a larger \(\tau\). Notably, the most superior performance is achieved when \(\tau\) equals 0.6. However, when \(\tau\) further increases to 0.7, the agent's performance suffers a sharp decline. We hypothesize that this could be due to the value function possibly exploding when \(\tau\) is set to larger values, subsequently leading to poorer performance outcomes. This is as expected since the over-large \(\tau\) leads to the overestimation error caused by neural networks. The experimental results demonstrate that we can balance a trade-off between minimizing the expected bisimulation residual and evaluating "optimal" measurement solely on the dataset by choosing a suitable \(\tau\).

### Ablation Study - Effectiveness of Reward Scaling

In the experiment, we set \(\gamma\) as 0.99 and \(c_{r}\) will be \(1-\gamma=0.01\) accordingly in _RS_. In this ablation experiment, we considered different combinations of min-max normalization/standardization and

Figure 6: Performance comparison on 12 D4RL tasks over 10 seeds with one standard error shaded in the default setting.

various value of \(c_{r}\) (including 1, 0.1, 0.01, and 0.001). The results in Figure 7 are consistent with our analysis in Section 5.2. The last two show better gains. As \(\bm{RS}\) has tighter bounds, it excels in most datasets, validating our theory.

### Case Study on MICo

As we illustrate in Section 5.2 and Section 6, Results in Figure 8 show that an unsuitable reward scaling dramatically decreases the performance while applying EBS will increase the performance in many datasets.

## Appendix G Additional Related Works

Here we present a brief introduction of all the baselines we used in the experiments:

TD3BC [16]add a behavior cloning term to regularize the policy of the TD3 [18] algorithm, achieves a state-of-the-art performance in Offline settings.

DrQ+BC [35]combining data augmentation techniques with the TD3+BC method, which applies TD3 in the offline setting with a regularizing behavioral-cloning term to the policy loss. The policy objective is: \(\pi=\underset{\pi}{\operatorname{argmax}}\mathbb{E}_{(s,a)\sim\mathcal{D}} \left[\lambda Q(s,\pi(s))-(\pi(s)-a)^{2}\right]\)

Driml [38] and HOMER [39](Time Contrastive methods) learn representations which can discriminate between adjacent observations in a rollout and pairs of random observations.

Figure 8: Ablation studies on 6 D4RL tasks over 3 seeds on MICo.

Figure 7: Ablation studies on 6 D4RL tasks over 3 seeds with one standard error shaded.

CURL[31](Augmentation Contrastive method) learns a representation that is invariant to a class of data augmentations while being different across random example pairs.

Inverse Model [44](One-Step Inverse Models) predict the action taken conditioned on the previous and resulting observations.

## Appendix H Additional Discussion

### The severity of the proposed problem

How do bisimulation-based objectives perform in other (online or goal-conditioned) settings?Various methods, such as DBC [54], MICo [6], SimSR [52], and PSE [1], have consistently demonstrated positive results in online settings, regardless of the presence of distractors. This evidence supports the efficacy of bisimulation techniques in online settings. Additionally, GCB [23] excelled in goal-conditioned environments, ExTra [46] showcased the power of bisimulation metric in exploration, and HiP-BMDP [55] successfully incorporated bisimulation into multi-task settings, highlighting its superior performance, all mostly in online settings too, with little work in offline RL. These studies suggest that when tailored to specific environments, bisimulation methods can excel. Despite these works, bisimulation methods have had little success when extended to offline settings, and our motivation is to tackle this problem.

While bisimulation objectives used in the offline setting are directly affected by missing transitions, many other representation objectives may not.When referring to state representation learning, using bisimulation in offline settings presents challenges due to the two issues we outlined: the presence of missing transitions and inappropriate reward scales. Concurrently, there exists other representation objectives, like CURL [31], ATC [49], which focus on pairs of states without the explicit necessity for transition information. As a consequence, they do not explicitly require accounting for missing transitions or reward scaling in their objectives. This absence of direct influence sets them apart from bisimulation-based methods. Yet, we consider that bisimulation-based techniques have a theoretical edge and have proven effective in online settings, Thus, we deem that our work is impactful in that it delivers a proof that bisimulation can be successful offline.

Compounded effect for bisimulation principle in offline settings.In online scenarios, state representations and policies are updated concurrently, while in offline settings, state representation is pre-trained before policy learning, with the two phases completely decoupled. Errors during representation learning in offline settings can have a compounded effect on policy learning, leading to significant issues. This is the reason that missing transitions is particularly harmful to the bisimulation principle in offline settings. Although reward scales affect bisimulation universally, as offline settings require pretraining state embedding, any major discrepancy between this fixed representation and the policy parameter space can further undermine the learning process. Hence, the proposed solutions hold promise for enhancing bisimulation's efficiency in offline settings.

### Suitability of different techniques

EbsWe provide EBS as a general method, which is applicable to all bisimulation-based objectives, given that they all adhere to the foundational principle of bisimulation. This principle revolves around the contraction mapping properties similar to the value iteration. Whenever there's an intent to employ bisimulation in offline scenarios, with an aim to reduce the Bellman residual for approximating the fixed point, the outlined challenge emerges. Consequently, EBS holds the potential to enhance any bisimulation-based method, regardless of the distance they use.

RsIn essence, the given theoretical analysis is applicable across all bisimulation-based objectives. However, the precise settings for hinge on the foundational distance. For instance, SimSR uses the cosine distance which has definitive bounds. As a result, we need to infer the ideal setting from Equation 10 and Theorem 8. In contrast, the MICo-like distance and DBC employ L-K distance and L1 distance respectively, having bounds ranging from. Consequently, they can adapt to more value settings. We propose our approach as a general method/principle to employ a novel bisimulation metric or distance, especially in the context of offline RL.

## Appendix I Empirical estimation of bisimulation error

In this section, we would like to conduct a toy experiment to empirically show that the bisimulation error could possibly be larger than bisimulation bellman residual in fixed/finite datasets.

Data collectionTo collect the evaluation dataset, we utilize TD3 [18] (a deterministic algorithm) instead of SAC [22] to avoid stochasticity. Firstly, we train a TD3 agent using the rlkit [45] codebase until convergence. Then, we collect 10k transitions and select specific transitions (such as 10, 100, 1000, 5000...) from these 10k transitions with uniform probability to form the evaluation dataset \(\mathcal{D}\). For determining termination, we follow the settings described in [18] and [22], considering a state terminal only if termination occurs before 1000 timesteps. If termination occurs before 1000 timesteps, we set \(\gamma=0\); otherwise, we set \(\gamma=0.99\).

ComputationGiven an evaluation dataset \(\mathcal{D}\), the bisimulation Bellman residual \(\epsilon_{\phi}\) is computed by \(\frac{1}{\left|\mathcal{D}\right|}\sum_{\left(\left(s_{i},ai\right),\left(s_{ j},a_{j}\right)\right)\sim\mathcal{D}}\left(G_{\phi}\left(s_{i},s_{j}\right)- \left(\mid r\left(s_{i},a_{i}\right)-r\left(s_{j},a_{j}\right)+\gamma G_{\phi }\left(s_{i},s_{j}\right)\right)\right)^{2}\), and the bisimulation error \(\Delta_{\phi}\) is computed by \(\frac{1}{\mathcal{D}}\sum_{\left(s_{i},j\right)\sim\mathcal{D}}\left(G_{\phi }\left(s_{i},s_{j}\right)-G_{\sim}\left(s_{i},s_{j}\right)\right)^{2}\), where \(G_{\sim}\) denotes the corresponding fixed point measurement. Since directly computing \(G_{\sim}\) is challenging, we compute \(\left|V^{\pi}\left(s_{i}\right)-V^{\pi}\left(s_{j}\right)\right|\) instead, as they should be equal when considering the measurement is the fixed point and the transition is deterministic. To compute \(V^{\pi}(s_{i})\), we reset the MuJoCo environment to the specific state \(s_{i}\) and ran the policy for 1000 timesteps. Since the environment and policy are deterministic, a single trajectory is sufficient to estimate the true value.

The results are presented in Table 3, which indicates that the bisimulation error on finite datasets is indeed larger than the bisimulation bellman residual.

\begin{table}
\begin{tabular}{l|c|c|c|c} \hline Transition number & 100 & 500 & 1000 & 2000 \\ \hline Bisimulation error & 0.2792 & 0.2891 & 0.2880 & 0.2915 \\ \hline Bisimulation Bellman residual & 0.003 & 0.0009 & 0.0032 & 0.0016 \\ \hline \end{tabular}
\end{table}
Table 3: The exact values of bisimulation error and bisimulation bellman residual