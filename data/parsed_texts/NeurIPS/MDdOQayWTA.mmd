# A Unified Confidence Sequence for Generalized Linear Models, with Applications to Bandits

Jungyhun Lee, Se-Young Yun

Kim Jaechul Graduate School of AI

KAIST

Seoul, Republic of Korea

{jh_lee00, yunseyoung}@kaist.ac.kr

&Kwang-Sung Jun

Department of Computer Science

University of Arizona

Tucson, AZ, USA

kjun@cs.arizona.edu

###### Abstract

We present a unified likelihood ratio-based confidence sequence (CS) for _any_ (self-concordant) generalized linear model (GLM) that is guaranteed to be convex and numerically tight. We show that this is on par or improves upon known CSs for various GLMs, including Gaussian, Bernoulli, and Poisson. In particular, for the first time, our CS for Bernoulli has a \(\mathrm{poly}(S)\)-free radius where \(S\) is the norm of the unknown parameter. Our first technical novelty is its derivation, which utilizes a time-uniform PAC-Bayesian bound with a uniform prior/posterior, despite the latter being a rather unpopular choice for deriving CSs. As a direct application of our new CS, we propose a simple and natural optimistic algorithm called OFUGLB, applicable to _any_ generalized linear bandits (**GLB**; Filippi et al. (2010)). Our analysis shows that the celebrated optimistic approach simultaneously attains state-of-the-art regrets for various self-concordant (not necessarily bounded) **GLBs**, and even \(\mathrm{poly}(S)\)-free for bounded **GLBs**, including logistic bandits. The regret analysis, our second technical novelty, follows from combining our new CS with a new proof technique that completely avoids the previously widely used self-concordant control lemma (Faury et al., 2020, Lemma 9). Numerically, OFUGLB outperforms or is at par with prior algorithms for logistic bandits.

## 1 Introduction

One paramount task in statistics and machine learning is to estimate the uncertainty of the underlying model from (possibly noisy) observations. For example, in interactive machine learning scenarios such as bandits (Lattimore and Szepesvari, 2020; Robbins, 1952; Thompson, 1933) and recently reinforcement learning with human feedback (RLHF; Christiano et al. (2017); Ouyang et al. (2022)), at each time step \(t\), the learner chooses an action \(\bm{x}_{t}\) from an available set of actions \(\mathcal{X}_{t}\) and observes reward or outcome \(r_{t}\) that is modeled as a distribution whose mean is an unknown function \(f^{*}\) of \(\bm{x}_{t}\); i.e., \(r_{t}\sim p(\cdot|\bm{x}_{t};f^{*})\). One popular choice of such a model is the **generalized linear model** (GLM; McCullagh and Nelder (1989)) that extends exponential family distributions to have a linear structure in its natural parameter as \(\langle\bm{x},\bm{\theta}_{*}\rangle\), where \(\bm{\theta}_{*}\) is an unknown parameter. In other words, the mean function is \(f^{*}(\bm{x})=\mu(\langle\bm{x},\bm{\theta}_{*}\rangle)\) for some inverse link function \(\mu\). This encompasses a wide range of distributions, which in turn makes it ubiquitous in various real-world applications, such as news recommendations (Bernoulli; Li et al. (2010, 2012)), social network influence maximization (Poisson; Gisselbrecht et al. (2015); Lage et al. (2013)), and more. In such tasks, the learner must estimate the uncertainty about \(\bm{\theta}_{*}\)_at each time step_\(t\geq 1\), given observations \(\{(\bm{x}_{s},r_{s})\}_{s=1}^{t-1}\), to make wise decisions. One popular and useful way to capture the uncertainty is via a _time-uniform confidence sequence (CS)_\(\{\mathcal{C}_{t}(\delta)\}_{t=1}^{\infty}\), which takes the form of \(\mathbb{P}[2t\geq 1:\bm{\theta}_{*}\not\in\mathcal{C}_{t}(\delta)]\leq\delta\). Recently, CS has been described as one of the key components for _safe anytime-valid inference (SAVI)_ that can ensure the validity/safeness of sequentially adaptive statistical inference (Ramdas et al., 2023).

Existing CSs for GLMs, however, are far from ideal. Much of the prior works focus on obtaining CS for specific instantiations of GLMs, such as Gaussian (Abbasi-Yadkori et al., 2011; Flynn et al., 2023) and Bernoulli (Abeille et al., 2021; Faury et al., 2020, 2022; Lee et al., 2024). Especially for Bernoulli, all the existing CSs suffer from \(\mathrm{poly}(S)\) factor in the radius, where \(S\) is the norm of the unknown parameter \(\bm{\theta}_{\star}\). Emmenegger et al. (2023); Jun et al. (2017); Li et al. (2017) proposed generic CSs that work for any convex GLMs, but their radii all suffer from a globally worst-case curvature of \(\mu\), which is detrimental in many cases (e.g., for Bernoulli, it scales as \(e^{S}\)).

Contributions.First, we propose a _unified_ construction of likelihood ratio-based CS for any convex GLMs (Theorem 3.1) and then instantiate it as an ellipsoidal CS for self-concordant GLMs, including Bernoulli, Gaussian, and Poisson distributions (Theorem 3.2). _Notably, we keep track of all the constants so that any practitioner can directly implement it without trouble._ The proof uses ingredients from time-uniform PAC-Bayesian bounds (Chugg et al., 2023) - martingale + Donsker-Varadhan representation of KL + Ville's inequality. The main technical novelty lies in using _uniform_ prior/posterior for the analysis, inspired by various literature on portfolios (Blum and Kalai, 1999) and fast rates in statistical/online learning (Foster et al., 2018; Grunwald and Mehta, 2020; Hazan et al., 2007; van Erven et al., 2015).

Secondly, we apply our novel CSs to contextual generalized linear bandits (**GLB**; Filippi et al. (2010)) with changing (and adversarial) arm-sets, and propose a new algorithm called **Optimism in the Face of Uncertainty for Generalized Linear Bandits** (OFUGLB). OFUGLB employs the simple and standard optimistic approach, choosing an arm that maximizes the upper confidence bound (UCB) computed by our CS (Abbasi-Yadkori et al., 2011; Auer, 2002). We show that OFUGLB achieves the state-of-the-art regret bounds for self-concordant (possibly _unbounded_) **GLB** (Theorem 4.1). This is the first time a computationally tractable, _purely_ optimistic strategy attains such \(\mathrm{poly}(S)\)-free regret for logistic bandits in that OFUGLB does not involve an explicit warmup phase and only involves convex optimization subroutines. Our other significant main technical contribution is the analysis of OFUGLB, as naively applying existing analysis techniques for optimistic algorithms (Abeille et al., 2021; Lee et al., 2024) yields a regret bound whose leading term scales with \(\mathrm{poly}(S)\). We identify the key reason for such additional dependency as the use of self-concordance control lemma (Faury et al., 2020, Lemma 9), and provide an alternate analysis that completely bypasses it, which may be of independent interest in the bandits community and beyond.

## 2 Problem Setting

We consider the realizable (online) regression with the **generalized linear model** (GLM; McCullagh and Nelder (1989)) whose conditional probability measure of \(r\) is given as

\[dp(r|\bm{x};\bm{\theta}_{\star})=\exp\left(\frac{r\langle\bm{x},\bm{\theta}_{ \star}\rangle-m(\langle\bm{x},\bm{\theta}_{\star}\rangle)}{g(\tau)}+h(r,\tau) \right)d\nu,\] (1)

where \(\tau\) is the dispersion parameter, and \(\nu\) is some known base measure (e.g., Lebesgue, counting). We assume the following:

**Assumption 1**.: \(\bm{\theta}_{\star}\in\Theta\subseteq\mathcal{B}^{d}(S):=\{\bm{\theta}\in \mathbb{R}^{d}:\left\|\bm{\theta}\right\|_{2}\leq S\}\) _for some known \(S>0\). Also, \(\Theta\) is nonempty, compact, and convex with intrinsic dimension1\(d\)._

Footnote 1: the linear-algebraic dimension (minimum number of basis vectors spanning it) of the affine span of \(\Theta\) in \(\mathbb{R}^{d}\).

**Assumption 2**.: _The domain \(X\) for arm (context) \(\bm{x}\) satisfies \(X\subseteq\mathcal{B}^{d}(1)\)._

**Assumption 3**.: \(m\) _is three times differentiable and convex, i.e., \(m^{\prime\prime\prime}\) exists and \(\dot{\mu}:=m^{\prime\prime}\geq 0\)._

In the **generalized linear bandit (GLB)** problem, at each time \(t\in[T]\), the learner observes a time-varying, arbitrary (adversarial) arm-set \(\mathcal{X}_{t}\subseteq X\), chooses a \(\bm{x}_{t}\in\mathcal{X}_{t}\), and receives a reward \(r_{t}\sim p(\cdot|\bm{x}_{t},\bm{\theta}_{\star})\). Let \(\mathcal{X}_{[T]}:=\cup_{t=1}^{T}\mathcal{X}_{t}\) and \(\Sigma_{t+1}:=\sigma(\Sigma_{t},r_{t},\bm{x}_{t+1})\) with \(\Sigma_{0}=\sigma(\bm{x}_{1})\) be the filtration in the canonical bandit model (Lattimore and Szepesvari, 2020, Chapter 4.6). From well-known properties of GLMs (McCullagh and Nelder, 1989), we have that \(\mathbb{E}[r_{t}|\Sigma_{t}]=m^{\prime}(\langle\bm{x}_{t},\bm{\theta}_{\star} \rangle)\triangleq\mu(\langle\bm{x}_{t},\bm{\theta}_{\star}\rangle)\) and \(\mathrm{Var}[r_{t}|\Sigma_{t}]=g(\tau)\dot{\mu}(\langle\bm{x}_{t},\bm{\theta}_{ \star}\rangle)\), where \(\mu\) is the _inverse link function_. We also define the following quantity describing the maximum slope of \(\mu\): \(R_{\dot{\mu}}:=\max_{\bm{x}\in\mathcal{X}_{[T]},\bm{\theta}\in\Theta}\dot{\mu} (\langle\bm{x},\bm{\theta}\rangle)\).

Note that many common distributions, such as Gaussian (\(\mu(z)=z\), \(R_{\dot{\mu}}=1\)), Poisson (\(\mu(z)=e^{z}\), \(R_{\dot{\mu}}=e^{S}\)), and Bernoulli (\(\mu(z)=(1+e^{-z})^{-1}\), \(R_{\dot{\mu}}=1/4\)), fall under the umbrella of GLM.

Unified Likelihood Ratio-based Confidence Sequence for GLMs

The learner's goal is to output a time-uniform confidence sequence (CS) for \(\bm{\theta}_{\star}\), \(\mathbb{P}[\exists t\geq 1:\bm{\theta}_{\star}\not\in\mathcal{C}_{t}(\delta)]\leq\delta\), where \(\mathbb{P}\) is w.r.t. the randomness of the confidence sets \(\mathcal{C}_{t}(\delta)\). In this work, we are particularly interested in the log-likelihood-based confidence set "centered" at the _norm-constrained_, batch maximum likelihood estimator (MLE):

\[\mathcal{C}_{t}(\delta):=\left\{\bm{\theta}\in\Theta:\mathcal{L}_{t}(\bm{ \theta})-\mathcal{L}_{t}(\widehat{\bm{\theta}}_{t})\leq\beta_{t}(\delta)^{2} \right\},\] (2)

where \(\beta_{t}(\delta)^{2}\) is the "radius" of the CS that we will define later, \(\mathcal{L}_{t}(\bm{\theta})\) is the negative log-likelihood of \(\bm{\theta}\) w.r.t. data collected up to \(t-1\), and \(\widehat{\bm{\theta}}_{t}\) is the corresponding MLE:

\[\mathcal{L}_{t}(\bm{\theta}):=\sum_{s=1}^{t-1}\left\{\ell_{s}(\bm{\theta}) \triangleq\frac{-r_{s}\langle\bm{x}_{s},\bm{\theta}\rangle+m(\langle\bm{x}_{ s},\bm{\theta}\rangle)}{g(\tau)}\right\},\quad\widehat{\bm{\theta}}_{t}:= \operatorname*{arg\,min}_{\bm{\theta}\in\Theta}\mathcal{L}_{t}(\bm{\theta}).\] (3)

Note that \(h(r_{s},\tau)\) is omitted as it plays no role in the confidence set nor the MLE.

The form of the confidence set is the same as Lee et al. (2024) and convex relaxation of Abeille et al. (2021), all of which utilizes a single, cumulative & _constrained_ MLE \(\widehat{\bm{\theta}}_{t}\in\Theta\) to compute the loss at time \(t\). Other approaches include using a single regularized MLE \(\widehat{\bm{\theta}}_{t}\) that may lie outside of \(\Theta\)(Abbasi-Yadkori et al., 2011), using a sequence of MLEs \(\{\widehat{\bm{\theta}}_{s}\}_{s=1}^{t}\) to compute the loss at time \(t\)(Abbasi-Yadkori et al., 2012; Emmenegger et al., 2023; Faury et al., 2022; Jun et al., 2017; Wasserman et al., 2020), and computing the _expected_ loss over some distribution (e.g., Gaussian) without committing to point estimators (Flynn et al., 2023). As one can see later, our derivation of the CS resembles the last approach: we also start from an expectation of loss over a prior distribution of \(\bm{\theta}\) without committing to an estimator. Yet, we introduce a single estimator \(\widehat{\bm{\theta}}_{t}\) to avoid the computational difficulty of evaluating the expectation.

Our first main contribution is the following unified confidence sequence for _any_ GLMs, regardless of whether it is bounded or not, as long as the corresponding log-likelihood loss is Lipschitz:

**Theorem 3.1** (Unified CS for GLMs).: _Let \(L_{t}\) be the Lipschitz constant11 of \(\mathcal{L}_{t}(\cdot)\) that may depend on \(\{(\bm{x}_{s},r_{s})\}_{s=1}^{t-1}\). Then, we have \(\mathbb{P}[\exists t\geq 1:\bm{\theta}_{\star}\not\in\mathcal{C}_{t}( \delta)]\leq\delta\), where_

Footnote 1: If \(\mathcal{L}_{t}\) is differentiable, one could apply the **Rademacher’s theorem**(Federer, 1996, Theorem 3.1.6):

\[\beta_{t}(\delta)^{2}=\log\frac{1}{\delta}+\inf_{c_{t}\in(0,1]}\left\{d\log \frac{1}{c_{t}}+2SL_{t}c_{t}\right\}\leq\log\frac{1}{\delta}+d\log\left(e \lor\frac{2eSL_{t}}{d}\right),\]

_where the last inequality follows from the choice \(c_{t}=1\wedge\frac{d}{2SL_{t}}\)._

**Remark 1** (Generality of our Unified CS).: _The above holds for any distribution over any Polish space, although \(\mathcal{C}_{t}(\delta)\) is convex if and only if \(\mathcal{L}_{t}\) is convex. For GLMs, convexity is guaranteed._

Practically, the computation of \(L_{t}\) involves a potentially non-concave maximization over a convex set, which is NP-hard in general (Murty and Kabadi, 1987). In Table 1, we provide _closed-form_ (up to absolute constants), high-probability upper bounds for \(L_{t}\)'s for various GLMs. Note that for the learner to implement the CS, she also needs to know \(S\), or its upper bound.

Comparisons to Prior Works.Lai (1976) derived the first generic CS for the exponential family based on a generalized likelihood ratio, but it is only applicable for \(\Theta\subset\mathbb{R}\) and is hard to instantiate. Recently, several works have provided CSs for either generic GLMs (Emmenegger et al., 2023; Jun et al., 2017; Li et al., 2017) or specific GLMs (linear: Abbasi-Yadkori et al. (2011); Flynn et al. (2023), logistic: Abeille et al. (2021); Faury et al. (2020); Lee et al. (2024)). The generic CSs are generally not tight as the "radius" often scales with \(\kappa:=\max_{\bm{x}\in X,\bm{\theta}\in\hat{\mu}(\langle\bm{x},\bm{\theta} \rangle)^{-1}}\), which scales exponentially in \(S\) for Bernoulli (Faury et al., 2020). For instance, Theorem 1 of Jun et al. (2017) and Theorem 1 of Li et al. (2017) propose ellipsoidal CSs that provably satisfy \(\|\widehat{\bm{\theta}}_{t}-\bm{\theta}_{\star}\|_{\mathbb{Z}_{t}}^{2}\leq \zeta_{1}(t,\delta)\), with \(\zeta_{1}\) always scaling with \(\kappa\). Emmenegger et al. (2023) proposed a weighted sequential likelihood testing-based CS \(\mathcal{W}_{t}\) and showed its efficacy empirically. Theoretically, they showed that \(\bm{\theta}\in\mathcal{W}_{t}\) satisfies \(D(\bm{\theta},\bm{\theta}_{*})\leq\zeta_{2}(t,\delta)\) for some Bregman divergence \(D(\cdot,\cdot)\) and a \(\zeta_{2}\) always scaling with \(\kappa\) as well. We believe their relaxation is not tight enough to warrant a fair comparison and leave to future work on theoretically comparing our CS to theirs. Chowdhury et al. (2023) proposed Bregman divergence-based CSs for generic exponential families, which are quite closely related to our CS; see Appendix A for further discussions. On the other hand, the CSs for specific GLMs are inapplicable to GLM models beyond what they are designed for and may not even be sufficiently tight. The prior state-of-the-art (likelihood ratio-based) CS radius for Bernoulli is \(\mathcal{O}\left(S\log(1/\delta)+d\log(St/d)\right)\) of Lee et al. (2024), while our theorem gives us \(\mathcal{O}\left(\log(1/\delta)+d\log(St/d)\right)\). Note that we _completely_ remove the \(\mathrm{poly}(S)\)-dependency from the radius, resolving an open problem posited by Lee et al. (2024). Later in Section 4, we show this is significant, both theoretically _and_ numerically.

### Ellipsoidal Confidence Sequence for Self-Concordant GLMs

We now provide an _ellipsoidal_ relaxation of Theorem 3.1 for the following class of GLMs:

**Assumption 4** (Russac et al. (2021)).: _GLM is (**generalized) self-concordant**, _i.e., the following quantity is well-defined (finite): \(R_{s}:=\inf\left\{R\geq 0:|\dot{\mu}(\langle\bm{x},\bm{\theta}\rangle)| \leq R\dot{\mu}(\langle\bm{x},\bm{\theta}\rangle),\;\forall\bm{x}\in X,\bm{ \theta}\in\Theta\right\}\)._

For instance, Bernoulli satisfies this with \(R_{s}=1\), and more generally, GLM bounded by \(R\) a.s. satisfy this assumption with \(R_{s}=R\)(Sawarni et al., 2024, Lemma 2.1). Many unbounded GLMs also satisfy this assumption, such as Gaussian (\(R_{s}=0\)), Poisson (\(R_{s}=1\)), and Exponential (\(R_{s}=0\)).

For such _self-concordant GLMs_, we have the following slightly relaxed ellipsoidal CS, whose proof is deferred to Appendix D:

**Theorem 3.2** (Ellipsoidal CS for Self-Concordant GLMs).: _With the same notations as Theorem 3.1, we have that for any \(\lambda\geq 0\), \(\mathbb{P}[\exists t\geq 1:\bm{\theta}_{*}\not\in\mathcal{E}_{t}(\delta, \lambda)]\leq\delta\), where_

\[\mathcal{E}_{t}(\delta,\lambda):=\left\{\bm{\theta}\in\mathbb{R}^{d}:\left\| \bm{\theta}-\widehat{\bm{\theta}}_{t}\right\|_{\nabla^{2}\mathcal{L}_{t}( \widehat{\bm{\theta}}_{t})+\lambda\bm{I}_{d}}^{2}\leq\gamma_{t}(\delta) \triangleq 2(1+SR_{s})(4S^{2}\lambda+\beta_{t}(\delta)^{2})\right\}.\]

Let us denote \(A\lesssim B\) if \(A\leq cB\) for some absolute constant \(c>0\). Note that the relaxation is order-wise strict only when \(R_{s}>0\). For instance, for Gaussian where \(R_{s}=0\), the ellipsoidal relaxation does not introduce additional \(S\)-dependency when we choose \(\lambda=\Theta\left(\frac{1}{S^{2}}\right)\). We then have that \(\nabla^{2}\mathcal{L}_{t}(\widehat{\bm{\theta}}_{t})=\frac{1}{\sigma^{2}} \sum_{s=1}^{t-1}\bm{x}_{s}\bm{x}_{s}^{*}=:\frac{1}{\sigma^{2}}\bm{V}_{t}\), and \(L_{t}\lesssim St\) with high probability (Proposition C.1). Combining everything, we have \(\|\bm{\theta}-\widehat{\bm{\theta}}_{t}\|_{\nabla_{t}}^{2}\lesssim\sigma^{2} \left(\log(t/\delta)+d\log(St/d)\right)\), which _completely_ matches the prior state-of-the-art radius as in Lemma D.10 of Flynn et al. (2023) with \(c=\sigma^{2}S^{2}\).

In bandits, the ellipsoidal CS allows one to equivalently rewrite the optimistic optimization in the UCB algorithm (Auer et al., 2002) as a _closed form bonus-based optimization_ over the arm-set \(\mathcal{X}_{t}\):

\[\operatorname*{arg\,max}_{\bm{x}\in\mathcal{X}_{t},\bm{\theta}\in\mathcal{E}_ {t}(\delta,\lambda)}\langle\bm{x},\bm{\theta}\rangle=\operatorname*{arg\,max} _{\bm{x}\in\mathcal{X}_{t}}\langle\bm{x},\widehat{\bm{\theta}}_{t}\rangle+ \sqrt{\gamma_{t}(\delta)}\|\bm{x}\|_{(\nabla^{2}\mathcal{L}_{t}(\widehat{\bm{ \theta}}_{t})+\lambda\bm{I}_{d})^{-1}},\] (4)

i.e., there is no need to solve a convex optimization for each arm. In the high-dimensional scenario where \(t=o(d)\), one can compute \((\nabla^{2}\mathcal{L}_{t}(\widehat{\bm{\theta}}_{t})+\lambda\bm{I}_{d})^{-1}\) with a time complexity of \(\mathcal{O}(td^{2})\) per round via the Sherman-Morrison formula (Sherman and Morrison, 1950), which is more efficient than the naive matrix inversion that takes \(\mathcal{O}(d^{3})\) time complexity.

### Proof of Theorem 3.1 - PAC-Bayes Approach with Uniform Prior

We consider \(M_{t}(\bm{\theta}):=\exp\left(\mathcal{L}_{t}(\bm{\theta}_{\star})-\mathcal{L}_{t} (\bm{\theta})\right),\) the likelihood ratio between the (estimated) distribution corresponding to \(\bm{\theta}\) and the true distribution corresponding to \(\bm{\theta}_{\star}\). This has been the subject of study for over 50 years (Darling and Robbins, 1967a,b; Lai, 1976; Robbins and Siegmund, 1972) and recently revisited by statistics and machine learning communities (Emmenegger et al., 2023; Flynn et al., 2023; Ramdas et al., 2023; Wasserman et al., 2020).

We follow the usual recipes for deriving time-uniform PAC-Bayesian bound (Alquier, 2024; Chugg et al., 2023). We start with the following time-uniform property:

**Lemma 3.1**.: _Let \(\delta\in(0,1)\). For any data-independent probability measure \(\mathbb{Q}\) on \(\Theta\), we have:_

\[\mathbb{P}\left(\exists t\geq 1:\mathbb{E}_{\bm{\theta}\sim\mathbb{Q}}[M_{t}( \bm{\theta})]\geq\frac{1}{\delta}\right)\leq\delta,\] (5)

_where \(\mathbb{P}\) is over the randomness of the data (and thus randomness of \(\mathcal{L}_{t}\)'s)._

Proof.: First, it is easy to see that \(M_{t}(\bm{\theta})=\prod_{s=1}^{t}\frac{dp(r_{s}|\bm{x}_{t};\bm{\theta})}{dp( r_{s}|\bm{x}_{s};\bm{\theta}_{\star})}\) is a nonnegative martingale w.r.t. \(\Sigma_{t}\):

\[\mathbb{E}[M_{t}(\bm{\theta})|\Sigma_{t-1}]=M_{t-1}(\bm{\theta})\mathbb{E} \left[\frac{dp(r_{t}|\bm{x}_{t};\bm{\theta})}{dp(r_{t}|\bm{x}_{t};\bm{\theta}_{ \star})}\bigg{|}\Sigma_{t-1}\right]=M_{t-1}(\bm{\theta})\underbrace{\int\frac{ dp(r|\bm{x}_{t};\bm{\theta})}{dp(r|\bm{x}_{t};\bm{\theta}_{\star})}dp(r|\bm{x}_{t}; \bm{\theta}_{\star})}_{=1}.\]

Now consider the random variable \(\mathbb{E}_{\bm{\theta}\sim\mathbb{Q}}[M_{t}(\bm{\theta})]\), which is adapted to \(\Sigma_{t}\). This is a martingale, as

\[\mathbb{E}[\mathbb{E}_{\bm{\theta}\sim\mathbb{Q}}[M_{t}(\bm{\theta})]|\Sigma_ {t-1}]\stackrel{{(*)}}{{=}}\mathbb{E}_{\bm{\theta}\sim\mathbb{Q}} [\mathbb{E}[M_{t}(\bm{\theta})|\Sigma_{t-1}]]=\mathbb{E}_{\bm{\theta}\sim \mathbb{Q}}[M_{t-1}(\bm{\theta})]\]

where \((*)\) follows from Tonelli's theorem. We conclude by Ville's inequality (Ville, 1939). 

We recall the variational representation of the KL divergence:

**Lemma 3.2** (Theorem 2.1 of Donsker and Varadhan (1983)).: _For two probability measures \(\mathbb{P},\mathbb{Q}\) over \(\Theta\), we have the following: \(D_{\mathrm{KL}}(\mathbb{P}||\mathbb{Q})=\sup_{g:\Theta\rightarrow\mathbb{R}} \mathbb{E}_{\bm{\theta}\sim\mathbb{P}}[g(\bm{\theta})]-\log\mathbb{E}_{\bm{ \theta}\sim\mathbb{Q}}[e^{g(\bm{\theta})}]\)._

We then have the following:

**Lemma 3.3**.: _For any data-independent prior \(\mathbb{Q}\) and any sequence of adapted posterior distributions (possibly learned from the data) \(\{\mathbb{P}_{t}\}\), the following holds: for any \(\delta\in(0,1)\),_

\[\mathbb{P}\left(\exists t\geq 1:\mathcal{L}_{t}(\bm{\theta}_{\star})-\mathbb{E}_{ \bm{\theta}\sim\mathbb{P}_{t}}[\mathcal{L}_{t}(\bm{\theta})]\geq\log\frac{1}{ \delta}+D_{\mathrm{KL}}(\mathbb{P}_{t}||\mathbb{Q})\right)\leq\delta.\] (6)

Proof.: Note that

\[\log\mathbb{E}_{\bm{\theta}\sim\mathbb{Q}}[M_{t}(\bm{\theta})]-\mathcal{L}_{t }(\bm{\theta}_{\star})=\log\mathbb{E}_{\bm{\theta}\sim\mathbb{Q}}[\exp\left(- \mathcal{L}_{t}(\bm{\theta})\right)]\stackrel{{(*)}}{{\geq}} \mathbb{E}_{\bm{\theta}\sim\mathbb{P}_{t}}[-\mathcal{L}_{t}(\bm{\theta})]-D_{ \mathrm{KL}}(\mathbb{P}_{t}||\mathbb{Q}),\]

where \((*)\) follows from Lemma 3.2 with \(g(\cdot)=-\mathcal{L}_{t}(\cdot)\). By Lemma 3.1, we have that \(\mathbb{P}\left(\exists t\geq 1:\log\frac{1}{\delta}\leq\log\mathbb{E}_{\bm{ \theta}\sim\mathbb{Q}}[M_{t}(\bm{\theta})]\right)\leq\delta\). Rearranging gives the desired statement. 

**Remark 2** (Choice of KL).: _One can replace KL with other divergences with similar variational formulations (Ohnishi and Honorio, 2021). As we will show later, KL suffices for our purpose._

Up to now, it is well-known in the PAC-Bayes literature. Our main technical novelty lies in how to choose \(\mathbb{Q}\) and \(\mathbb{P}_{t}\), which is as follows: for \(c_{t}\in(0,1]\) to be determined later, we set

\[\mathbb{Q}=\mathrm{Unif}(\Theta),\quad\mathbb{P}_{t}=\mathrm{Unif}(\widetilde{ \Theta}_{t}\triangleq(1-c_{t})\widehat{\bm{\theta}}_{t}+c_{t}\Theta),\] (7)

where \(\mathrm{Unif}(\cdot)\) is the uniform distribution and \(\bm{a}+\Theta=\{\bm{a}+\bm{\theta}:\bm{\theta}\in\Theta\}\) for a vector \(\bm{a}\in\mathbb{R}^{d}\).

Then, denoting \(\mathrm{vol}(\cdot)\) as the (Lebesgue) volume in \(\mathbb{R}^{d}\), we have

\[D_{\mathrm{KL}}(\mathbb{P}_{t}||\mathbb{Q})=\log\frac{\mathrm{vol}(\Theta)}{ \mathrm{vol}(\widetilde{\Theta})}=\log\frac{\mathrm{vol}(\Theta)}{\mathrm{ vol}\left((1-c_{t})\widehat{\bm{\theta}}_{t}+c_{t}\Theta\right)}=\log\frac{ \mathrm{vol}(\Theta)}{\mathrm{vol}(c_{t}\Theta)}=\log\frac{\mathrm{vol}( \Theta)}{c_{t}^{d}\mathrm{vol}(\Theta)}=d\log\frac{1}{c_{t}}.\]We also have that

\[\mathbb{E}_{\bm{\theta}\sim\mathbb{P}_{t}}[\mathcal{L}_{t}(\bm{\theta})]=\mathcal{ L}_{t}(\widehat{\bm{\theta}}_{t})+\mathbb{E}_{\bm{\theta}\sim\mathbb{P}_{t}}[ \mathcal{L}_{t}(\bm{\theta})-\mathcal{L}_{t}(\widehat{\bm{\theta}}_{t})]\leq \mathcal{L}_{t}(\widehat{\bm{\theta}}_{t})+2SL_{t}c_{t},\]

where follows from the Lipschitzness of \(\mathcal{L}_{t}(\cdot)\) and the fact that for \(\bm{\theta}=(1-c_{t})\widehat{\bm{\theta}}_{t}+c_{t}\tilde{\bm{\theta}}\in \widetilde{\Theta}_{t}\), \(\left\|\bm{\theta}-\widehat{\bm{\theta}}_{t}\right\|_{2}=c_{t}\left\|\tilde{ \bm{\theta}}-\widehat{\bm{\theta}}_{t}\right\|_{2}\leq 2Sc_{t}\). We conclude by minimizing over \(c_{t}\in(0,1]\). 

### Intuitions Behind the Proof of Theorem 3.1

Constrained MLE and Uniform Prior/Posterior.As we consider **constrained MLE**, we know that \(\widehat{\bm{\theta}}_{t}\in\Theta\), i.e., our "belief" on our MLE is precisely the prior \(\mathbb{Q}=\mathrm{Unif}(\Theta)\). Then, as we want the true parameter \(\bm{\theta}_{*}\) to be close to \(\widehat{\bm{\theta}}_{t}\), we want to show that a sufficiently large "posterior volume" is near \(\widehat{\bm{\theta}}_{t}\), formalized as \(\mathbb{P}_{t}=\mathrm{Unif}\left((1-c_{t})\widehat{\bm{\theta}}_{t}+c_{t}\Theta\right)\) for some time-dependent shrinkage factor \(c_{t}\in(0,1]\). We later appropriately choose \(c_{t}\) to optimize the PAC-Bayesian bound.

We remark that the uniform prior/posterior has been previously considered in universal portfolios (Blum and Kalai, 1999, Theorem 1) and fast rates in online learning (Foster et al., 2018; Hazan et al., 2007); see Appendix A for discussions on relations to fast rates literature. To our knowledge, we are the first to use such uniform prior/posterior in the (time-uniform) PAC-Bayes context.

**Remark 3** (Use of Regularized MLE?).: _When one uses regularized MLE instead of constrained, as it is not guaranteed to be in \(\Theta\), one cannot directly use the same uniform prior/posterior. One approach may be to appropriately project the regularized MLE onto \(\Theta\) (e.g., Eqn. (9) of Faury et al. (2020)). However, the previously considered projections that guarantee the tightness of the resulting CS involve a nonconvex optimization and are, thus, computationally intractable. One could also consider using high regularization, which may result in additional dependencies on \(S\) in the final CS radius. We conjecture that similarly tight guarantees can be recovered with regularized MLE if one uses other appropriate prior/posterior whose supports are the entire \(\mathbb{R}^{d}\) (e.g., Gaussian)._

Relations to Theorem 3 of Foster et al. (2018).Let us first briefly recall its proof. The authors first consider a distribution \(P_{t}(\cdot)\) over the parameter \(W\in\mathcal{W}\) (see their Algorithm 1) and use \(\eta\)-mixability of the logistic loss to obtain an inequality involving the negative-log-integral term \(\int_{\mathcal{W}}\exp\left(-\eta\sum_{t}\ell(Wx_{t},y_{t})\right)dW\). They define \(S=\theta W^{*}+(1-\theta)\mathcal{W}\subseteq\mathcal{W}\), where \(W^{*}\) is the ground-truth optimal parameter and \(\theta\in[0,1)\) is to be determined later. The proof concludes by chaining \(\int_{\mathcal{W}}\geq\int_{S}\) with the \(\ell_{\infty}\)-Lipschitzness of the logistic loss and expanding the integral.

Our proof is inspired by the above, with some key differences. While the negative-log-integral also arises in our scenario, we adopt a more compact, streamlined PAC-Bayes approach. In our case, a similar quantity \(\mathbb{E}_{\theta\sim\mathbb{Q}}[\exp(-\mathcal{L}_{t}(\theta))]\) arises from our Donsker-Varadhan representation (Lemma 3.2). We then apply Ville's inequality to obtain the time-uniform PAC-Bayes bound (Lemma 3.1), and our choices of prior/posterior resemble their choice of \(S\). Our Lipschitzness argument at the end also resembles their \(\ell_{\infty}\)-Lipschitzness argument.

## 4 **O**fuglb: A Generic UCB Algorithm for Self-Concordant GLBs

As a direct application of our CS, we consider self-concordant **GLB**(Filippi et al., 2010; Janz et al., 2024), where at each time \(t\), the learner chooses a \(\bm{x}_{t}\in\mathcal{X}_{t}\) dependent on the history \(\{(\bm{x}_{s},r_{s})\}_{s=1}^{t-1}\) and receives \(r_{t}\sim p(\cdot|\bm{x}_{t},\bm{\theta}_{*})\). The learner's goal is to minimize the (pseudo-)regret, \(\text{Reg}(T):=\sum_{t=1}^{T}\left\{\mu(\langle\bm{x}_{t,\star},\bm{\theta}_{ \star}\rangle)-\mu(\langle\bm{x}_{t},\bm{\theta}_{\star}\rangle)\right\},\) where \(\bm{x}_{t,\star}:=\arg\max_{\bm{x}\in\mathcal{X}_{t}}\mu(\langle\bm{x},\bm{ \theta}_{\star}\rangle)\).

Inspired by the optimism principle (Abbasi-Yadkori et al., 2011; Auer, 2002), based on our new, improved confidence sequence (Theorem 3.1), we propose OFUGLB (Algorithm 1), a generic UCB-type algorithm that applies to _any_ instantiations of **GLB**. Through a new proof technique that allows us to circumvent \(\kappa\)- and \(\operatorname{poly}(S)\)-dependencies in the leading term, our unified algorithm attains or improves the known state-of-the-art regret bound for the class of _self-concordant_ **GLB**, which encompasses a zoo of well-studied stochastic bandits such as linear (Abbasi-Yadkori et al., 2011; Auer, 2002), Poisson (Gisselbrecht et al., 2015), logistic (Abeille et al., 2021; Faury et al., 2020), etc.

We define the following problem difficulty quantities: recalling that \(\mathcal{X}_{[T]}=\bigcup_{t\in[T]}\mathcal{X}_{t}\),

\[\kappa_{\star}(T):=\frac{1}{\frac{1}{T}\sum_{t\in[T]}\hat{\mu}( \langle\bm{x}_{t,\star},\bm{\theta}_{\star}\rangle)},\quad\kappa(T):=\max_{ \bm{x}\in\mathcal{X}_{[T]},\bm{\theta}\in\Theta}\frac{1}{\hat{\mu}(\langle \bm{x},\bm{\theta}\rangle)}.\] (8)

These may scale exponentially in \(S\), e.g., for logistic bandits (Faury et al., 2020; Filippi et al., 2010), but we will later show that through our new analysis, the leading term of the regret scales _inversely_ with \(\kappa_{\star}(T)\), and the transient term scales linearly with \(\kappa(T)\).

We now present the _unified & state-of-the-art_ regret guarantee for self-concordant **GLB**s:

**Theorem 4.1** (OFUGLB for Self-Concordant **GLB**).: _OFUGLB attains the following regret bound for self-concordant_ **GLB** _with probability at least \(1-\delta\):_

\[\mathsf{Reg}(T)\lesssim\underbrace{d\sqrt{\frac{g(\tau)T}{\kappa_{\star}(T)} \log\frac{SL_{T}}{d}\log\frac{R_{\hat{\mu}}ST}{d}}}_{\text{leading term}}+ \underbrace{d^{2}R_{s}R_{\hat{\mu}}\sqrt{g(\tau)}\kappa(T)\log\left(1+\frac{ ST}{dg(\tau)\kappa(T)}\right)}_{\text{ununient term}},\]

_where \(L_{T}\) is as defined in Theorem 3.1 and we assume that \(\log\frac{1}{\delta}=\mathcal{O}\left(d\log\frac{SL_{T}}{d}\right).\)_

### Proof Sketch of Theorem 4.1 - Regret Analysis of OFUGLB

We first emphasize that even though we have a tight CS (Theorem 3.1), naively combining it with existing regret analyses of logistic bandits (Abeille et al., 2021; Lee et al., 2024)_still_ results in an extra factor of \(S\) in the leading term. The prior proof applies the Cauchy-Schwartz inequality w.r.t. the (regularized) Hessian \(\bm{H}_{t}(\bm{\theta}_{\star})=\lambda\bm{I}+\sum_{s=1}^{t-1}\dot{\mu}_{s}( \bm{\theta}_{\star})\bm{x}_{s}\bm{x}_{s}^{\top}\) with \(\dot{\mu}_{s}(\cdot):=\hat{\mu}(\langle\bm{x}_{s},\cdot\rangle)\), which forces the use of self-concordant lemma (Abeille et al., 2021, Lemma 8). This results in a CS of the form \(\|\bm{\theta}_{\star}-\widehat{\bm{\theta}}_{t}\|_{\bm{H}_{t}(\bm{\theta}_{ \star})}=\mathcal{O}(S\beta_{t}(\delta))\). Then, using the same regret decomposition of Abeille et al. (2021) and the optimism principle, the leading term of the regret is bounded as

\[\sum_{t}\dot{\mu}_{t}(\bm{\theta}_{\star})\langle\bm{x}_{t,\star}-\bm{x}_{t}, \bm{\theta}_{\star}\rangle\lesssim S\beta_{t}(\delta)\underbrace{\sqrt{\sum_{ t}\dot{\mu}_{t}(\bm{\theta}_{\star})}}_{\leq\sqrt{T/\kappa_{\star}(T)}} \underbrace{\sqrt{\sum_{t}\left\|\sqrt{\dot{\mu}_{t}(\bm{\theta}_{\star})}\bm {x}_{t}\right\|^{2}_{\bm{H}_{t}(\bm{\theta}_{\star})^{-1}}}}_{\text{elliptical potential lemma (\ref{eq:Abbasi-Yadkori et al., 2011)}}}.\]

Our proof begins by applying Cauchy-Schwartz w.r.t. \(\widetilde{\bm{G}}_{t}(\widehat{\bm{\theta}}_{t})\), derived from the integral remainder in first-order Taylor expansion of \(\mathcal{L}_{t}(\cdot)\) at \(\widehat{\bm{\theta}}_{t}\). With this, we have that \(\|\bm{\theta}_{\star}-\widehat{\bm{\theta}}_{t}\|_{\widetilde{\bm{G}}_{t}( \widehat{\bm{\theta}}_{t})}=\mathcal{O}(\beta_{t}(\delta))\) (Lemma E.6), avoiding the extra \(S\). However, as \(\widetilde{\bm{G}}_{t}(\widehat{\bm{\theta}}_{t})=\sum_{s=1}^{t-1}\xi(\bm{x} _{s},\widehat{\bm{\theta}}_{t})\bm{x}_{s}\bm{x}_{s}^{\top}\) for some well-defined scalar function \(\xi_{s}\), the elliptical potential lemma (as done above) is _not_ applicable due to the explicit dependency on \(\widehat{\bm{\theta}}_{t}\)! This difficulty is analogous to the analysis of Logistic-UCB-2 in Faury et al. (2020), where a similar difficulty arose because their improved bonus \(\epsilon_{t,2}\) depends on the current estimate of the parameter as well (see their Lemma 4). They circumvent this issue by explicitly modifying the UCB algorithm to incorporate additional constraints on the "admissible log-odds," which leads to a computationally inefficient algorithm.

Notably, we show via a new proof technique that the vanilla UCB can _implicitly_ handle those constraints by designating a "worst-case" parameter over all future iterations (Eqn. (21)). We develop many other intriguing results, such as a novel self-concordance lemma that bounds the difference of \(\dot{\mu}\)'s with that of \(\mu\)'s times \(R_{s}\) (Lemma E.3). We provide the full proof in Appendix E.

### Instantiations and Discussions of Theorem 4.1

In Table 2, we instantiate Theorem 4.1 for various self-concordant **GLBs**. It can be seen that our OFUGLB attains state-of-the-art regret guarantees in all considered scenarios, either by achieving (linear) or improving upon (bounded, logistic) the known regret bounds! Note that the instantiation for (sub-)Gaussian linear bandits is meant to be a sanity check because tighter confidence sets are available in Flynn et al. (2023) and Chowdhury et al. (2023, Appendix F).

To our knowledge, only a few works deal with generic, (possibly unbounded) self-concordant **GLBs**. Jun et al. (2017) proposed UCB-style **GLOC** and its variants, which, however, incur regret bounds scaling with \(\kappa_{\star}(T)\) in the leading term. Concurrent with our work, Liu et al. (2024) prove that all **GLBs** with light-tailed base distribution are self-concordant, and propose OFU-GLB with regret of \(\widetilde{\mathcal{O}}(d\sqrt{T/\kappa_{\star}(T)}+d\kappa_{\star}(T))\). Another line of works (Abeille and Lazaric, 2017; Dong et al., 2019; Janz et al., 2024; Kim et al., 2023; Kveton et al., 2020) considers randomized exploration-based algorithms, including Thompson sampling, which we discuss further in Appendix A.

Below, we discuss our results for bounded **GLB**, logistic, and Poisson bandits in-depth.

Bounded GLB.The only prior work applicable to general bounded **GLB** is Sawarni et al. (2024), where the authors propose RS-GLinCB with regret as in Table 2. Compared to our regret, they are slightly better as their transient term scales as \(\kappa_{\mathcal{X}}(T)\) while ours scales as \(\kappa(T)\), but we have a much better dependency on \(R\) (\(R\) vs. \(R^{5}\)). Despite this seeming gap, as RS-GLinCB relies on an explicit warm-up scheme, our OFUGLB is expected to have superior numerical performance as it avoids excessive exploration in the early phase. We will elaborate more on this issue in Section 5. Also, it should be noted that Sawarni et al. (2024) requires a _nonconvex_ optimization as a subroutine to obtain \(\mathrm{poly}(S)\)-free regret (see their Appendix E). Still, RS-GLinCB has its advantages in that it only requires \(\Omega(\log^{2}T)\) switches while we require \(\Omega(T)\) switches; it is an interesting open problem whether a lazy variant of OFUGLB with same (or better) regret guarantee is possible.

Logistic Bandits.Although the logistic bandit is a special case of the bounded **GLB**, the number of prior works and its practical applicability to recommender systems (Li et al., 2010, 2012) and recently RLHF (Das et al., 2024) makes it deserving of separate discussions. We first review the prior works on logistic bandits. Faury et al. (2020) was the first to obtain a regret bound of \(\widetilde{\mathcal{O}}(d\sqrt{T}+d^{2}\kappa(T))\) (up to some dependencies on \(S\)) that is \(\kappa\)-free in the leading term. Subsequently, a local minimax regret lower bound of \(\Omega((d/S)\sqrt{T/\kappa_{\star}(T)})\) was proven (Abeille et al., 2021, Theorem 2)2, suggesting that more nonlinearity helps, and several works have focused on proposing and analyzing algorithms with matching upper bounds. One line of works (Abeille et al., 2021; Lee et al., 2024), including this work, focuses on getting a tight _convex_ CS for logistic losses, which then directly gives an OFUL-type algorithm. Abeille et al. (2021) first proposed a likelihood ratio-based CS, albeit somewhat loose

\begin{table}
\begin{tabular}{|c||c|c|} \hline
**GLB** & **Our regret bound** & **Prior state-of-the-art** \\ \hline \(R\)-Bounded & \(d\sqrt{\frac{T}{\kappa_{\star}(T)}}+d^{2}RR_{\hat{\mu}}\kappa(T)\) & \(\begin{array}{c}d\sqrt{\frac{T}{\kappa_{\star}(T)}}+d^{2}R^{5}S^{2}\kappa_{ \mathcal{X}}(T)\\ \text{\@@cite[cite]{[\@@bibref{}{Sawarni et al., 2024, Theorem 4.2}{}}]}\\ \text{\@@cite[cite]{[\@@bibref{}{Sawarni et al., 2024, Theorem 4.2}{}}]}\\ \text{\@@cite[cite]{[\@@bibref{}{Sawarni et al., 2024, Theorem 4.2}{}}]}\\ \end{array}\) \\ \hline Logistic & \(d\sqrt{\frac{T}{\kappa_{\star}(T)}}+d^{2}\kappa(T)\) & \(\begin{array}{c}d\sqrt{\frac{T}{\kappa_{\star}(T)}}+d^{2}S^{2}\kappa_{ \mathcal{X}}(T)\\ \text{\@@cite[cite]{[\@@bibref{}{Sawarni et al., 2024, Theorem 4.2}{}}]}\\ \text{\@@cite[cite]{[\@@bibref{}{Sawarni et al., 2024, Theorem 4.2}{}}]}\\ \end{array}\) \\ \hline Linear\({}^{a}\) & \(\sigma d\sqrt{T}\) & \(\sigma d\sqrt{T}\) \\ \hline Poisson & \(dS\sqrt{\frac{T}{\kappa_{\star}(T)}}+d^{2}e^{2S}\kappa(T)\) & \(\begin{array}{c}d^{3/2}\sqrt{\frac{T}{\kappa_{\star}(T)}}\\ \text{\@@cite[cite]{[\@@bibref{}{Janz et al., 2024, Theorem 1}{}}]}^{b}\\ \text{\@@cite[cite]{[\@@bibref{}{Janz et al., 2024, Theorem 1}{}}]}\\ \end{array}\) \\ \hline \end{tabular} \({}^{a}\) We choose \(c=\sigma^{2}S^{2}\) in Lemma D.10 of Flynn et al. (2023).

\({}^{b}\) Here, we omit the dependencies on \(S\) and the transient term from explicit warmup.

\end{table}
Table 2: Regret bounds of OFUGLB for various self-concordant **GLBs**. Logarithmic factors are omitted to avoid a cognitive overload. Let \(\kappa_{\mathcal{X}}(T):=\max_{\bm{x}\in\cup_{i=1}^{T}\mathcal{X}_{t}}\frac{1} {\mu((\bm{x},\bm{\theta}_{\star}))}\) and \(g(\tau)=\mathcal{O}(1)\). Here, “\(R\)-Bounded” means \(|r_{t}|\leq R\;a.s.\).

in \(S\). Lee et al. (2024) proposed a new framework for converting an achievable online learning algorithm to a tighter CS and proposed a UCB algorithm that attains the prior (to this work) state-of-the-art regret bound of \(\widetilde{\mathcal{O}}(dS\sqrt{T/\kappa_{\star}(T)}+R_{\mathcal{X}}(T))\) with \(R_{\mathcal{X}}(T)\) being arm-set geometry-dependent transient term3 From a computational perspective, Faury et al. (2022) proposed an online Newton step-based algorithms that attain the regret bound of \(\widetilde{\mathcal{O}}(dS\sqrt{T/\kappa_{\star}(T)}+d^{2}S^{6}\kappa(T))\) using only \(\mathcal{O}(\log t)\) computational cost _and_\(\mathcal{O}(1)\) storage per time step; the computational cost was later improved to \(\mathcal{O}(1)\) in Zhang and Sugiyama (2023). Another line of works (Mason et al., 2022; Sawarni et al., 2024) proposed algorithms that perform an _explicit_ warm-up in the early stages. Thanks to the explicit warmup, both attain regret with \(\mathrm{poly}(S)\)-free leading term, e.g., \(\widetilde{\mathcal{O}}(d\sqrt{T/\kappa_{\star}(T)}+d^{2}S^{2}\kappa_{ \mathcal{X}}(T))\) by Sawarni et al. (2024). However, the explicit warmup typically lasts for \(\widetilde{\Omega}(\kappa(T))\) or \(\widetilde{\Omega}(\kappa_{\mathcal{X}}(T))\) time steps, resulting in potentially very large initial regret. This is later verified in our logistic bandits experiments. Our OFUGLB is the first purely optimism-based UCB algorithm (no explicit warmup) that attains a \(\mathrm{poly}(S)\)-free leading term in the regret.

Footnote 3: One may wonder why our Theorem 4.1’s transient term always scales as \(\kappa(T)\). See Appendix B for further discussions on this interesting difference.

Poisson Bandits.Despite its potential to model various real-world problems involving count feedback, Poisson bandits have not been studied often in the literature. Gisselbrecht et al. (2015) was the first to consider contextual Poisson bandits and proposed UCB and optimistic Bayesian-based algorithms (May et al., 2012), but without any regret guarantees. To our knowledge, our Theorem 4.1 provides the first regret bound for the (finite-dimensional) contextual Poisson bandits without reward boundedness assumption. On a related note, Mutny and Krause (2021) consider Poisson bandits with the intensity function in an RKHS. Their linear RKHS formulation is, however, incompatible with our log-linear formulation; see their Appendix A.1 for further discussions.

## 5 Experiments

We perform experiments on logistic bandits to complement the theoretical improvement in our regret bounds and CS. The codes are available in our GitHub repository4, forked from the previous repository5 of Faury et al. (2022). Our GitHub provides the unified implementations of all considered algorithms, which we hope will be helpful in future research and benchmarking of logistic bandits. In Appendix G, we provide the missing implementation details and additional experimental results.

Footnote 4: https://github.com/nick-jhlee/logistic_bandit

Footnote 5: https://github.com/criteo-research/logistic_bandit

Baselines and Setting.We compare our OFUGLB (likelihood ratio-based CS; Theorem 3.1) and OFUGLB-e (ellipsoidal CS; Theorem 3.2) to the following five baselines: EMK (Emmenegger et al., 2023), EVILL (Janz et al., 2024), RS-GLinCB (Sawarni et al., 2024), OFULog+ (Lee et al., 2024), and ada-OFU-ECOLog(Faury et al., 2022). Note that the last two are specific to logistic bandits, and RS-GLinCB is specific to bounded **GL**Bs. We emphasize that when implementing OFUGLB and OFUGLB-e, we use the precise theoretical hyperparameters as given in our theorem statements without further tuning. To highlight the practical effectiveness of our theoretical algorithms in comparison to randomized exploration, which is known to perform well in practice (Chapelle and Li, 2011; Russo et al., 2018), we use a single, untuned hyperparameter for EVILL6. For the experimental setup in this section, we set \(T=10000\), \(d=2\), and \(\delta=0.05\). We consider time-varying arm-set: at each \(t\in[T]\), an arm-set \(\mathcal{A}_{t}\subset\mathcal{B}^{d}(1)\) of size \(|\mathcal{A}_{t}|=20\) is uniformly sampled. We set \(\theta_{\star}=\frac{S-1}{\sqrt{d}}\mathbf{1}\) for \(S\in\{4,6,8,10\}\). Lastly, we consider \(10\) independent repeats per setting for statistical significance.

Footnote 6: We remark that in Appendix G, we provide additional results (for \(K=10\)), where EVILL with its theoretical hyperparameters performs either worst or second-worst.

Results and Discussions.The results are shown in Figure 1. Note that in all considered settings, OFUGLB, EMK, and EVILL outperform every other baseline, both in terms of regret and numerical tightness of the CS. Moreover, for \(S\in\{8,10\}\), our OFUGLB seems to achieve the best performance, although more comprehensive numerical studies would shed more light on this matter. As for our OFUGLB-e, despite having worse performance than OFUGLB, EMK, and EVILL, it always attains better numerical performance than the remaining algorithms. Notably, OFUGLB and EMK achieve at par or better numerical regret compared to EVILL _with heuristic hyperparameter_. This highlights the effectiveness of our theoretical results even compared to heuristically tuned randomized exploration.

One interesting observation is that even though OFULog+ achieves a much tighter CS at the end, its regret is much worse than OFULB-e. We posit that this is related to the interplay between the CS and arm set geometries, and we leave further study of such discrepancy to future work. Another is that RS-GLinCB with the exact theoretical hyperparameters (Sawarni et al., 2024) performs the worst, even worse than ada-OFU-ECOLog. We believe this is because their theoretical hyperparameters are not numerically tight, forcing the algorithm to explore throughout the entire duration, probably as their Switching Criterion I (line 4 of their Algorithm 2) is always true. To use RS-GLinCB in practice, one must tune7 the hyperparameters to _explicitly_ control the degree of exploration, which is not the case for our OFULB, making ours a viable, practical algorithm with a _provable guarantee_ as well. Lastly, note how the likelihood-based CS, \(\{\bm{\theta}\in\Theta:\mathcal{L}_{t}(\bm{\theta})\leq c_{t}\}\) for \(c_{t}=\Theta(\log t)\), resembles an ellipsoid. This is because the "normalized" sublevel value \(c_{t}/t\) (as there are \(t\) summands in the LHS) gets smaller, making the second-order Taylor expansion more accurate.

Footnote 7: This is the case in their GitHub implementation, where heuristic values were used for their experiments. Additionally, their algorithm requires the knowledge of \(\kappa_{*}\).

## 6 Conclusion

This paper introduces a novel and _unified_ likelihood ratio-based CS for generic (convex) GLMs, encompassing widely-used models such as Gaussian, Bernoulli, and Poisson. Our CS is equipped with exact constants for various scenarios, making it suitable for any practitioner. The proof involves leveraging key techniques from PAC-Bayes bounds with a uniform prior/posterior. We then propose OFULB, a generic UCB algorithm applicable to _any_**GLBs**, achieving state-of-the-art regret bounds across all self-concordant **GLBs**. The proof involves novel regret decomposition and maximally avoiding the self-concordance control lemma (Faury et al., 2020, Lemma 9), which may be of independent interest. Notably, for logistic bandits, OFUGLB is the first pure-optimism-based algorithm that achieves \(\mathrm{poly}(S)\)-free leading term in the theoretical regret, which is numerically verified to perform best. This work opens up various future directions, which we discuss in detail in Appendix B.

Figure 1: Time-varying arm-sets. (First row) Regret plots of all considered algorithms. (Second row) Magnified regret plots. (Third row) Confidence set plots at the final time \(t=10000\) when applicable. Each column represents a different logistic bandit instance for \(S\in\{4,6,8,10\}\).

## Acknowledgments and Disclosure of Funding

J. Lee thanks Gergely Neu for hosting him at a wonderful mini-workshop after AISTATS '24 at UPF, during which many insightful discussions inspired the current PAC-Bayesian proof. J. Lee also thanks Branislav Kveton for suggesting trying a randomized algorithm for logistic bandits experiments, Tim van Erven for insightful discussions regarding the fast rates in statistical learning, and Aaditya Ramdas for insightful comments on the prior-posterior martingale, all during AISTATS '24. J. Lee also thanks Jaeyoung Cha for suggesting an elementary proof for Lemma C.1 that does not rely on WolframAlpha. The authors thank the anonymous reviewers of the ICML '24 ARLET Workshop and NeurIPS '24 for insightful questions and suggestions that helped us significantly improve the paper.

J. Lee and S.-Y. Yun were supported in part by the Institute of Information & Communications Technology Planning & Evaluation (IITP) grant funded by the Korean government (MSIT) (No. RS-2022-II20311 Development of Goal-Oriented Reinforcement Learning Techniques for Contact-Rich Robotic Manipulation of Everyday Objects and No. RS-2019-II190075 Artificial Intelligence Graduate School Program (KAIST)) and the National Research Foundation of Korea(NRF) grant funded by the Korean government (MSIT) (No. RS-2019-NR040050 Stochastic Analysis and Application Research Center (SAARC)). K.-S. Jun was supported in part by the National Science Foundation under grant CCF-2327013.

## References

* Abbasi-Yadkori et al. (2011) Yasin Abbasi-Yadkori, David Pal, and Csaba Szepesvari. Improved Algorithms for Linear Stochastic Bandits. In _Advances in Neural Information Processing Systems_, volume 24, pages 2312-2320. Curran Associates, Inc., 2011. URL https://sites.ualberta.ca/~szepesva/papers/linear-bandits-NeurIPS2011.pdf.
* Abbasi-Yadkori et al. (2012) Yasin Abbasi-Yadkori, David Pal, and Csaba Szepesvari. Online-to-Confidence-Set Conversions and Application to Sparse Stochastic Bandits. In _Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics_, volume 22 of _Proceedings of Machine Learning Research_, pages 1-9. PMLR, 21-23 Apr 2012. URL https://proceedings.mlr.press/v22/abbasi-yadkori12.html.
* 5197, 2017. doi: 10.1214/17-EJS1341SI. URL https://doi.org/10.1214/17-EJS1341SI.
* Abeille et al. (2021) Marc Abeille, Louis Faury, and Clement Calauzenes. Instance-Wise Minimax-Optimal Algorithms for Logistic Bandits. In _Proceedings of The 24th International Conference on Artificial Intelligence and Statistics_, volume 130 of _Proceedings of Machine Learning Research_, pages 3691-3699. PMLR, 13-15 Apr 2021. URL https://proceedings.mlr.press/v130/abeille21a.html.
* Alquier (2024) Pierre Alquier. User-friendly Introduction to PAC-Bayes Bounds. _Foundations and Trends(r) in Machine Learning_, 17(2):174-303, 2024. ISSN 1935-8237. doi: 10.1561/2200000100. URL http://dx.doi.org/10.1561/2200000100.
* Auer (2002) Peter Auer. Using Confidence Bounds for Exploitation-Exploration Trade-offs. _Journal of Machine Learning Research_, 3:397-422, Nov 2002. URL https://jmlr.csail.mit.edu/papers/v3/auer02a.html.
* Auer et al. (2002) Peter Auer, Nicolo Cesa-Bianchi, and Paul Fischer. Finite-time Analysis of the Multiarmed Bandit Problem. _Machine Learning_, 47(2):235-256, May 2002. ISSN 1573-0565. doi: 10.1023/A:1013689704352. URL https://doi.org/10.1023/A:1013689704352.
* Azizi et al. (2022) Mohammad Javad Azizi, Branislav Kveton, and Mohammad Ghavamzadeh. Fixed-Budget Best-Arm Identification in Structured Bandits. In _Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, IJCAI-22_, pages 2798-2804. International Joint Conferences on Artificial Intelligence Organization, 7 2022. doi: 10.24963/ijcai.2022/388. URL https://doi.org/10.24963/ijcai.2022/388. Main Track.
* Baek and Farias (2018) Jackie Baek and Vivek Farias. TS-UCB: Improving on Thompson Sampling With Little to No Additional Computation. In _Proceedings of The 26th International Conference on ArtificialIntelligence and Statistics_, volume 206 of _Proceedings of Machine Learning Research_, pages 11132-11148. PMLR, 25-27 Apr 2023. URL https://proceedings.mlr.press/v206/baek23a.html.
* Blum and Kalai (1999) Avrim Blum and Adam Kalai. Universal Portfolios With and Without Transaction Costs. _Machine Learning_, 35(3):193-205, Jun 1999. ISSN 1573-0565. doi: 10.1023/A:1007530728748. URL https://doi.org/10.1023/A:1007530728748.
* Bogachev (1998) Vladimir Igorevich Bogachev. _Gaussian Measures_. Number 62 in Mathematical Surveys and Monographs. American Mathematical Society, 1998.
* Boyd and Vandenberghe (2004) Stephen Boyd and Lieven Vandenberghe. _Convex Optimization_. Cambridge University Press, 2004.
* Cawley et al. (2007) Gavin C. Cawley, Gareth J. Janacek, and Nicola L. C. Talbot. Generalised Kernel Machines. In _2007 International Joint Conference on Neural Networks_, pages 1720-1725, 2007. doi: 10.1109/IJCNN.2007.4371217. URL https://ieeexplore.ieee.org/document/4371217.
* Chapelle and Li (2011) Olivier Chapelle and Lihong Li. An Empirical Evaluation of Thompson Sampling. In _Advances in Neural Information Processing Systems_, volume 24, pages 2249-2257. Curran Associates, Inc., 2011. URL https://papers.nips.cc/paper_files/paper/2011/hash/e53a0a2978c28872a4505bbb51db06dc-Abstract.html.
* Chowdhury and Gopalan (2017) Sayak Ray Chowdhury and Aditya Gopalan. On Kernelized Multi-armed Bandits. In _Proceedings of the 34th International Conference on Machine Learning_, volume 70 of _Proceedings of Machine Learning Research_, pages 844-853. PMLR, 06-11 Aug 2017. URL https://proceedings.mlr.press/v70/chowdhury17a.html.
* Chowdhury et al. (2023) Sayak Ray Chowdhury, Patrick Saux, Odalric Maillard, and Aditya Gopalan. Bregman Deviations of Generic Exponential Families. In _Proceedings of Thirty Sixth Conference on Learning Theory_, volume 195 of _Proceedings of Machine Learning Research_, pages 394-449. PMLR, 12-15 Jul 2023. URL https://proceedings.mlr.press/v195/chowdhury23a.html.
* Christiano et al. (2017) Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep Reinforcement Learning from Human Preferences. In _Advances in Neural Information Processing Systems_, volume 30, pages 4302-4310. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/paper_files/paper/2017/file/d5e2c0adad503c91f91df240d0cd4e49-Paper.pdf.
* Chugg et al. (2023) Ben Chugg, Hongjian Wang, and Aaditya Ramdas. A Unified Recipe for Deriving (Time-Uniform) PAC-Bayes Bounds. _Journal of Machine Learning Research_, 24(372):1-61, 2023. URL http://jmlr.org/papers/v24/23-0401.html.
* Da Prato and Zabczyk (2014) Giuseppe Da Prato and Jerzy Zabczyk. _Stochastic Equations in Infinite Dimensions_. Number 152 in Encyclopedia of Mathematics and its Applications. Cambridge University Press, 2 edition, 2014.
* Darling and Robbins (1967a) D. A. Darling and Herbert Robbins. Confidence Sequences for Mean, Variance, and Median. _Proceedings of the National Academy of Sciences_, 58(1):66-68, 1967a. doi: 10.1073/pnas.58.1.66. URL https://www.pnas.org/doi/abs/10.1073/pnas.58.1.66.
* Darling and Robbins (1967b) D. A. Darling and Herbert Robbins. Iterated Logarithm Inequalities. _Proceedings of the National Academy of Sciences_, 57(5):1188-1192, 1967b. doi: 10.1073/pnas.57.5.1188. URL https://www.pnas.org/doi/abs/10.1073/pnas.57.5.1188.
* Das et al. (2024) Nirjhar Das, Souradip Chakraborty, Aldo Pacchiano, and Sayak Ray Chowdhury. Active Preference Optimization for Sample Efficient RLHF. _arXiv preprint arXiv:2402.10500_, 2024. URL https://arxiv.org/abs/2402.10500.
* 1933, 2004. doi: 10.1214/009117904000000397. URL https://doi.org/10.1214/009117904000000397.
* D'Alessio et al. (2017)Shi Dong, Tengyu Ma, and Benjamin Van Roy. On the Performance of Thompson Sampling on Logistic Bandits. In _Proceedings of the Thirty-Second Conference on Learning Theory_, volume 99 of _Proceedings of Machine Learning Research_, pages 1158-1160. PMLR, 25-28 Jun 2019. URL https://proceedings.mlr.press/v99/dong19a.html.
* Donsker and Varadhan (1983) M. D. Donsker and S. R. S. Varadhan. Asymptotic Evaluation of Certain Markov Process Expectations for Large Time. IV. _Communications on Pure and Applied Mathematics_, 36(2):183-212, 1983. doi: https://doi.org/10.1002/cpa.3160360204. URL https://onlinelibrary.wiley.com/doi/abs/10.1002/cpa.3160360204.
* Duchi and Haque (2024) John Duchi and Saminul Haque. An information-theoretic lower bound in time-uniform estimation. In _Proceedings of Thirty Seventh Conference on Learning Theory_, volume 247 of _Proceedings of Machine Learning Research_, pages 1486-1500. PMLR, 30 Jun-03 Jul 2024. URL https://proceedings.mlr.press/v247/duchi24a.html.
* Emmenegger et al. (2023) Nicolas Emmenegger, Mojmir Mutny, and Andreas Krause. Likelihood Ratio Confidence Sets for Sequential Decision Making. In _Advances in Neural Information Processing Systems_, volume 36. Curran Associates, Inc., 2023. URL https://openreview.net/forum?id=4anryczeED.
* Faury et al. (2020) Louis Faury, Marc Abeille, Clement Calauzenes, and Olivier Fercoq. Improved Optimistic Algorithms for Logistic Bandits. In _Proceedings of the 37th International Conference on Machine Learning_, volume 119 of _Proceedings of Machine Learning Research_, pages 3052-3060. PMLR, 13-18 Jul 2020. URL https://proceedings.mlr.press/v119/faury20a.html.
* Faury et al. (2022) Louis Faury, Marc Abeille, Kwang-Sung Jun, and Clement Calauzenes. Jointly Efficient and Optimal Algorithms for Logistic Bandits. In _Proceedings of The 25th International Conference on Artificial Intelligence and Statistics_, volume 151 of _Proceedings of Machine Learning Research_, pages 546-580. PMLR, 28-30 Mar 2022. URL https://proceedings.mlr.press/v151/faury22a.html.
* Federer (1996) Herbert Federer. _Geometric Measure Theory_. Classics in Mathematics. Springer Berlin, Heidelberg, 1996.
* Filippi et al. (2010) Sarah Filippi, Olivier Cappe, Aurelien Garivier, and Csaba Szepesvari. Parametric Bandits: The Generalized Linear Case. In _Advances in Neural Information Processing Systems_, volume 23, pages 586-594. Curran Associates, Inc., 2010. URL https://proceedings.neurips.cc/paper_files/paper/2010/file/c2626d850c80ea07e7511bbae4c76f4b-Paper.pdf.
* Flynn et al. (2023) Hamish Flynn, David Reeb, Melih Kandemir, and Jan Peters. Improved Algorithms for Stochastic Linear Bandits Using Tail Bounds for Martingale Mixtures. In _Advances in Neural Information Processing Systems_, volume 36. Curran Associates, Inc., 2023. URL https://openreview.net/forum?id=TXoZiUZywf.
* Foster et al. (2018) Dylan J. Foster, Satyen Kale, Haipeng Luo, Mehryar Mohri, and Karthik Sridharan. Logistic Regression: The Importance of Being Improper. In _Proceedings of the 31st Conference On Learning Theory_, volume 75 of _Proceedings of Machine Learning Research_, pages 167-208. PMLR, 06-09 Jul 2018. URL https://proceedings.mlr.press/v75/foster18a.html.
* Frank and Wolfe (1956) Marguerite Frank and Philip Wolfe. An algorithm for quadratic programming. _Naval Research Logistics Quarterly_, 3(1-2):95-110, 1956. doi: https://doi.org/10.1002/nav.3800030109. URL https://onlinelibrary.wiley.com/doi/abs/10.1002/nav.3800030109.
* Gales et al. (2022) Spencer B. Gales, Sunder Sethuraman, and Kwang-Sung Jun. Norm-Agnostic Linear Bandits. In _Proceedings of The 25th International Conference on Artificial Intelligence and Statistics_, volume 151 of _Proceedings of Machine Learning Research_, pages 73-91. PMLR, 28-30 Mar 2022. URL https://proceedings.mlr.press/v151/gales22a.html.
* Gisselbrecht et al. (2015) Thibault Gisselbrecht, Sylvain Lamprier, and Patrick Gallinari. Policies for Contextual Bandit Problems with Count Payoffs. In _2015 IEEE 27th International Conference on Tools with Artificial Intelligence (ICTAI)_, pages 542-549, 2015. doi: 10.1109/ICTAI.2015.85. URL https://ieeexplore.ieee.org/document/7372181.
* Gisselbrecht et al. (2016)Peter D. Grunwald and Nishant A. Mehta. Fast Rates for General Unbounded Loss Functions: From ERM to Generalized Bayes. _Journal of Machine Learning Research_, 21(56):1-80, 2020. URL http://jmlr.org/papers/v21/18-488.html.
* Hamidi and Bayati (2020) Nima Hamidi and Mohsen Bayati. On Frequentist Regret of Linear Thompson Sampling. _arXiv preprint arXiv:2006.06790_, 2020. URL https://arxiv.org/abs/2006.06790.
* Hazan et al. (2007) Elad Hazan, Amit Agarwal, and Satyen Kale. Logarithmic regret algorithms for online convex optimization. _Machine Learning_, 69(2):169-192, Dec 2007. ISSN 1573-0565. doi: 10.1007/s10994-007-5016-8. URL https://doi.org/10.1007/s10994-007-5016-8.
* Janz et al. (2024) David Janz, Shuai Liu, Alex Ayoub, and Csaba Szepesvari. Exploration via linearly perturbed loss minimisation. In _Proceedings of The 27th International Conference on Artificial Intelligence and Statistics_, volume 238 of _Proceedings of Machine Learning Research_, pages 721-729. PMLR, 02-04 May 2024. URL https://proceedings.mlr.press/v238/janz24a.html.
* Jezequel et al. (2020) Remi Jezequel, Pierre Gaillard, and Alessandro Rudi. Efficient improper learning for online logistic regression. In _Proceedings of Thirty Third Conference on Learning Theory_, volume 125 of _Proceedings of Machine Learning Research_, pages 2085-2108. PMLR, 09-12 Jul 2020. URL https://proceedings.mlr.press/v125/jezequel20a.html.
* Jin et al. (2019) Chi Jin, Praneeth Netrapalli, Rong Ge, Sham M. Kakade, and Michael I. Jordan. A Short Note on Concentration Inequalities for Random Vectors with SubGaussian Norm. _arXiv preprint arXiv:1902.03736_, 2019. URL https://arxiv.org/abs/1902.03736.
* Jun et al. (2017) Kwang-Sung Jun, Aniruddha Bhargava, Robert Nowak, and Rebecca Willett. Scalable Generalized Linear Bandits: Online Computation and Hashing. In _Advances in Neural Information Processing Systems_, volume 30, pages 98-108. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/paper_files/paper/2017/file/28dd2c7955ce926456240b2ff0100bde-Paper.pdf.
* Jun et al. (2021) Kwang-Sung Jun, Lalit Jain, Blake Mason, and Houssam Nassif. Improved Confidence Bounds for the Linear Logistic Model and Applications to Bandits. In _Proceedings of the 38th International Conference on Machine Learning_, volume 139 of _Proceedings of Machine Learning Research_, pages 5148-5157. PMLR, 18-24 Jul 2021. URL https://proceedings.mlr.press/v139/jun21a.html.
* Kaufmann and Koolen (2021) Emilie Kaufmann and Wouter M. Koolen. Mixture Martingales Revisited with Applications to Sequential Tests and Confidence Intervals. _Journal of Machine Learning Research_, 22(246):1-44, 2021. URL http://jmlr.org/papers/v22/18-798.html.
* Kazerouni and Wein (2021) Abbas Kazerouni and Lawrence M. Wein. Best arm identification in generalized linear bandits. _Operations Research Letters_, 49(3):365-371, 2021. ISSN 0167-6377. doi: https://doi.org/10.1016/j.orl.2021.03.011. URL https://www.sciencedirect.com/science/article/pii/S0167637721000523.
* Kim et al. (2023) Wonyoung Kim, Kyungbok Lee, and Myunghee Cho Paik. Double Doubly Robust Thompson Sampling for Generalized Linear Contextual Bandits. _Proceedings of the AAAI Conference on Artificial Intelligence_, 37(7):8300-8307, Jun. 2023. doi: 10.1609/aaai.v37i7.26001. URL https://ojs.aaai.org/index.php/AAAI/article/view/26001.
* Kim et al. (2022) Yeoneung Kim, Insoon Yang, and Kwang-Sung Jun. Improved Regret Analysis for Variance-Adaptive Linear Bandits and Horizon-Free Linear Mixture MDPs. In _Advances in Neural Information Processing Systems_, volume 35, pages 1060-1072. Curran Associates, Inc., 2022. URL https://openreview.net/forum?id=U_YPSEyN21s.
* Kirschner and Krause (2018) Johannes Kirschner and Andreas Krause. Information Directed Sampling and Bandits with Heteroscedastic Noise. In _Proceedings of the 31st Conference On Learning Theory_, volume 75 of _Proceedings of Machine Learning Research_, pages 358-384. PMLR, 06-09 Jul 2018. URL https://proceedings.mlr.press/v75/kirschner18a.html.
* Kavukcu et al. (2019)Dieter Kraft. A Software Package for Sequential Quadratic Programming. Technical Report DFVLR-FB 88-28, Deutsche Forschungs und Versuchsanstalt fur Luft- und Raumfahrt - Institut fur Dynamik der Flugsysteme, Koln, Deutschland, 1988. URL https://degenerateconic.com/uploads/2018/03/DFVLR_FB_88_28.pdf.
* Kveton et al. (2020) Branislav Kveton, Manzil Zaheer, Csaba Szepesvari, Lihong Li, Mohammad Ghavamzadeh, and Craig Boutilier. Randomized Exploration in Generalized Linear Bandits. In _Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics_, volume 108 of _Proceedings of Machine Learning Research_, pages 2066-2076. PMLR, 26-28 Aug 2020. URL https://proceedings.mlr.press/v108/kveton20a.html.
* Lage et al. (2013) Ricardo Lage, Ludovic Denoyer, Patrick Gallinari, and Peter Dolog. Choosing which message to publish on social networks: A contextual bandit approach. In _2013 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM 2013)_, pages 620-627, 2013. doi: 10.1145/2492517.2492541. URL https://ieeexplore.ieee.org/document/6785767.
* 280, 1976. doi: 10.1214/aos/1176343406. URL https://doi.org/10.1214/aos/1176343406.
* Lattimore and Szepesvari (2020) Tor Lattimore and Csaba Szepesvari. _Bandit Algorithms_. Cambridge University Press, 2020.
* Lee et al. (2024) Junghyun Lee, Se-Young Yun, and Kwang-Sung Jun. Improved Regret Bounds of (Multinomial) Logistic Bandits via Regret-to-Confidence-Set Conversion. In _Proceedings of The 27th International Conference on Artificial Intelligence and Statistics_, volume 238 of _Proceedings of Machine Learning Research_, pages 4474-4482. PMLR, 02-04 May 2024. URL https://proceedings.mlr.press/v238/lee24d.html.
* Li et al. (2010) Lihong Li, Wei Chu, John Langford, and Robert E. Schapire. A Contextual-Bandit Approach to Personalized News Article Recommendation. In _Proceedings of the 19th International Conference on World Wide Web_, WWW '10, page 661-670, New York, NY, USA, 2010. Association for Computing Machinery. ISBN 9781605587998. doi: 10.1145/1772690.1772758. URL https://doi.org/10.1145/1772690.1772758.
* Li et al. (2012) Lihong Li, Wei Chu, John Langford, Taesup Moon, and Xuanhui Wang. An Unbiased Offline Evaluation of Contextual Bandit Algorithms with Generalized Linear Models. In _Proceedings of the Workshop on On-line Trading of Exploration and Exploitation 2_, volume 26 of _Proceedings of Machine Learning Research_, pages 19-36, Bellevue, Washington, USA, 02 Jul 2012. PMLR. URL https://proceedings.mlr.press/v26/li12a.html.
* Li et al. (2017) Lihong Li, Yu Lu, and Dengyong Zhou. Provably Optimal Algorithms for Generalized Linear Contextual Bandits. In _Proceedings of the 34th International Conference on Machine Learning_, volume 70 of _Proceedings of Machine Learning Research_, pages 2071-2080. PMLR, 06-11 Aug 2017. URL https://proceedings.mlr.press/v70/li17c.html.
* Li et al. (2024) Xuheng Li, Heyang Zhao, and Quanquan Gu. Feel-Good Thompson Sampling for Contextual Dueling Bandits. In _Proceedings of the 41st International Conference on Machine Learning_, volume 235 of _Proceedings of Machine Learning Research_, pages 29406-29426. PMLR, 21-27 Jul 2024. URL https://proceedings.mlr.press/v235/li24co.html.
* Lieb (1973) Elliott H Lieb. Convex trace functions and the Wigner-Yanase-Dyson conjecture. _Advances in Mathematics_, 11(3):267-288, 1973. ISSN 0001-8708. doi: https://doi.org/10.1016/0001-8708(73)90011-X. URL https://www.sciencedirect.com/science/article/pii/000187087390011X.
* Liu et al. (2024) Shuai Liu, Alex Ayoub, Flore Sentenac, Xiaoqi Tan, and Csaba Szepesvari. Almost Free: Self-concordance in Natural Exponential Families and an Application to Bandits. In _Advances in Neural Information Processing Systems_, volume 37. Curran Associates, Inc., 2024. URL https://openreview.net/forum?id=LKwVYvx66I.
* Mason et al. (2022) Blake Mason, Kwang-Sung Jun, and Lalit Jain. An Experimental Design Approach for Regret Minimization in Logistic Bandits. _Proceedings of the AAAI Conference on Artificial Intelligence_, 36(7):7736-7743, Jun. 2022. doi: 10.1609/aaai.v36i7.20741. URL https://ojs.aaai.org/index.php/AAAI/article/view/20741.
* Ma et al. (2015)Benedict C. May, Nathan Korda, Anthony Lee, and David S. Leslie. Optimistic Bayesian Sampling in Contextual-Bandit Problems. _Journal of Machine Learning Research_, 13(67):2069-2106, 2012. URL http://jmlr.org/papers/v13/may12a.html.
* McCullagh and Nelder (1989) Peter McCullagh and John A. Nelder. _Generalized Linear Models_. Monographs on Statistics and Applied Probability. Chapman & Hall/CRC, 2 edition, 1989.
* 805, 2005. doi: 10.1214/009053604000001156. URL https://doi.org/10.1214/009053604000001156.
* Murty and Kabadi (1987) Katta G. Murty and Santosh N. Kabadi. Some NP-complete problems in quadratic and nonlinear programming. _Mathematical Programming_, 39(2):117-129, Jun 1987. ISSN 1436-4646. doi: 10.1007/BF02592948. URL https://doi.org/10.1007/BF02592948.
* Mussi et al. (2024) Marco Mussi, Simone Drago, and Alberto Maria Metelli. Open Problem: Tight Bounds for Kernelized Multi-Armed Bandits with Bernoulli Rewards. _arXiv preprint arXiv:2407.06321_, 2024. URL https://arxiv.org/abs/2407.06321.
* Mutny and Krause (2021) Mojmir Mutny and Andreas Krause. No-regret Algorithms for Capturing Events in Poisson Point Processes. In _Proceedings of the 38th International Conference on Machine Learning_, volume 139 of _Proceedings of Machine Learning Research_, pages 7894-7904. PMLR, 18-24 Jul 2021. URL https://proceedings.mlr.press/v139/mutny21a.html.
* Neiswanger and Ramdas (2021) Willie Neiswanger and Aaditya Ramdas. Uncertainty quantification using martingales for misspecified Gaussian processes. In _Proceedings of the 32nd International Conference on Algorithmic Learning Theory_, volume 132 of _Proceedings of Machine Learning Research_, pages 963-982. PMLR, 16-19 Mar 2021. URL https://proceedings.mlr.press/v132/neiswanger21a.html.
* Ohnishi and Honorio (2021) Yuki Ohnishi and Jean Honorio. Novel Change of Measure Inequalities with Applications to PAC-Bayesian Bounds and Monte Carlo Estimation. In _Proceedings of The 24th International Conference on Artificial Intelligence and Statistics_, volume 130 of _Proceedings of Machine Learning Research_, pages 1711-1719. PMLR, 13-15 Apr 2021. URL https://proceedings.mlr.press/v130/ohnishi21a.html.
* Ouyang et al. (2022) Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback. In _Advances in Neural Information Processing Systems_, volume 35, pages 27730-27744. Curran Associates, Inc., 2022. URL https://openreview.net/forum?id=T6SKACxEON.
* Oxtoby (1946) John C. Oxtoby. Invariant Measures in Groups Which Are Not Locally Compact. _Transactions of the American Mathematical Society_, 60:215-237, 1946. doi: https://doi.org/10.1090/S0002-9947-1946-0018188-5. URL https://www.ams.org/journals/tran/1946-060-00/S0002-9947-1946-0018188-5/.
* Paulsen and Raghupathi (2016) Vern I. Paulsen and Mrinal Raghupathi. _An Introduction to the Theory of Reproducing Kernel Hilbert Spaces_. Cambridge Studies in Advanced Mathematics. Cambridge University Press, 2016.
* Pukelsheim (2006) Friedrich Pukelsheim. _Optimal Design of Experiments_, volume 50 of _Classics in Applied Mathematics_. Society for Industrial and Applied Mathematics (SIAM), 2006.
* 601, 2023. doi: 10.1214/23-STS894. URL https://doi.org/10.1214/23-STS894.
* 535, 1952. URL https://projecteuclid.org/journals/bulletin-of-the-american-mathematical-society/volume-58/issue-5/Some-aspects-of-the-sequential-design-of-experiments/bams/1183517370.full.
* Raghupathi et al. (2015)Herbert Robbins and David Siegmund. A Class of Stopping Rules for Testing Parametric Hypotheses. _Proceedings of the Sixth Berkeley Symposium on Mathematical Statistics and Probability, Volume 4: Biology and Health_, 6(4):37-41, 1972. URL https://projecteuclid.org/proceedings/berkeley-symposium-on-mathematical-statistics-and-probability/Proceedings-of-the-Sixth-Berkeley-Symposium-on-Mathematical-Statistics-and/Chapter/A-class-of-stopping-rules-for-testing-parametric-hypotheses/bsmsp/1200514454.
* Russac et al. (2021) Yoan Russac, Louis Faury, Olivier Cappe, and Aurelien Garivier. Self-Concordant Analysis of Generalized Linear Bandits with Forgetting. In _Proceedings of The 24th International Conference on Artificial Intelligence and Statistics_, volume 130 of _Proceedings of Machine Learning Research_, pages 658-666. PMLR, 13-15 Apr 2021. URL https://proceedings.mlr.press/v130/russac21a.html.
* Russo and Van Roy (2018) Daniel Russo and Benjamin Van Roy. Learning to Optimize via Information-Directed Sampling. _Operations Research_, 66(1):230-252, 2018. doi: 10.1287/opre.2017.1663. URL https://doi.org/10.1287/opre.2017.1663.
* Russo et al. (2018) Daniel J. Russo, Benjamin Van Roy, Abbas Kazerouni, Ian Osband, and Zheng Wen. A Tutorial on Thompson Sampling. _Foundations and Trends(r) in Machine Learning_, 11(1):1-96, 2018. ISSN 1935-8237. doi: 10.1561/2200000070. URL http://dx.doi.org/10.1561/2200000070.
* Sawarni et al. (2024) Ayush Sawarni, Nirjhar Das, Siddharth Barman, and Gaurav Sinha. Generalized Linear Bandits with Limited Adaptivity. In _Advances in Neural Information Processing Systems_, volume 37. Curran Associates, Inc., 2024. URL https://openreview.net/forum?id=FTPDBQuT4G.
* 127, 1950. doi: 10.1214/aoms/1177729893. URL https://doi.org/10.1214/aoms/1177729893.
* Shi et al. (2024) Chengshuai Shi, Kun Yang, Zihan Chen, Jundong Li, Jing Yang, and Cong Shen. Efficient Prompt Optimization Through the Lens of Best Arm Identification. In _Advances in Neural Information Processing Systems_, volume 37. Curran Associates, Inc., 2024. URL https://openreview.net/forum?id=FTLNnlfBGMo.
* Srinivas et al. (2010) Niranjan Srinivas, Andreas Krause, Sham Kakade, and Matthias Seeger. Gaussian Process Optimization in the Bandit Setting: No Regret and Experimental Design. In _Proceedings of The 27th International Conference on Machine Learning_, pages 1015-1022, 2010. URL https://arxiv.org/abs/0912.3995.
* Thompson (1933) William R. Thompson. On the Likelihood that One Unknown Probability Exceeds Another in View of the Evidence of Two Samples. _Biometrika_, 25(3/4):285-294, 1933. ISSN 00063444. URL http://www.jstor.org/stable/2332286.
* Tropp (2015) Joel A. Tropp. An Introduction to Matrix Concentration Inequalities. _Foundations and Trends(r) in Machine Learning_, 8(1-2):1-230, 2015. ISSN 1935-8237. doi: 10.1561/2200000048. URL http://dx.doi.org/10.1561/2200000048.
* van Erven et al. (2015) Tim van Erven, Peter D. Grunwald, Nishant A. Mehta, Mark D. Reid, and Robert C. Williamson. Fast Rates in Statistical and Online Learning. _Journal of Machine Learning Research_, 16(54):1793-1861, 2015. URL http://jmlr.org/papers/v16/vanerven15a.html.
* Vershynin (2010) Roman Vershynin. Introduction to the non-asymptotic analysis of random matrices. _arXiv preprint arXiv:1011.3027_, 2010. URL https://arxiv.org/abs/1011.3027.
* Vershynin (2018) Roman Vershynin. _High-Dimensional Probability: An Introduction with Applications in Data Science_. Cambridge Series in Statistical and Probabilistic Mathematics. Cambridge University Press, 2018.
* Ville (1939) Jean Ville. _Etude critique de la notion de collectif_. Monographies des Probabilites. Paris: Gauthier-Villars, 1939. URL http://eudml.org/doc/192893.

Pauli Virtanen, Ralf Gommers, Travis E. Oliphant, Matt Haberland, Tyler Reddy, David Cournapeau, Evgeni Burovski, Pearu Peterson, Warren Weckesser, Jonathan Bright, Stefan J. van der Walt, Matthew Brett, Joshua Wilson, K. Jarrod Millman, Nikolay Mayorov, Andrew R. J. Nelson, Eric Jones, Robert Kern, Eric Larson, C J Carey, Ilhan Polat, Yu Feng, Eric W. Moore, Jake VanderPlas, Denis Laxalde, Josef Perktold, Robert Cimrman, Ian Henriksen, E. A. Quintero, Charles R. Harris, Anne M. Archibald, Antonio H. Ribeiro, Fabian Pedregosa, Paul van Mulbregt, and SciPy 1.0 Contributors. SciPy 1.0: Fundamental Algorithms for Scientific Computing in Python. _Nature Methods_, 17:261-272, 2020. doi: 10.1038/s41592-019-0686-2. URL https://www.nature.com/articles/s41592-019-0686-2.
* Wasserman et al. (2020) Larry Wasserman, Aaditya Ramdas, and Sivaraman Balakrishnan. Universal inference. _Proceedings of the National Academy of Sciences_, 117(29):16880-16890, 2020. doi: 10.1073/pnas.1922664117. URL https://www.pnas.org/doi/abs/10.1073/pnas.1922664117.
* Wolke and Schwettlick (1988) R. Wolke and H. Schwettlick. Iteratively Reweighted Least Squares: Algorithms, Convergence Analysis, and Numerical Comparisons. _SIAM Journal on Scientific and Statistical Computing_, 9(5):907-921, 1988. doi: 10.1137/0909062. URL https://doi.org/10.1137/0909062.
* Zhang (2022) Tong Zhang. Feel-Good Thompson Sampling for Contextual Bandits and Reinforcement Learning. _SIAM Journal on Mathematics of Data Science_, 4(2):834-857, 2022. doi: 10.1137/21M140924X. URL https://doi.org/10.1137/21M140924X.
* Zhang and Sugiyama (2023) Yu-Jie Zhang and Masashi Sugiyama. Online (Multinomial) Logistic Bandit: Improved Regret and Constant Computation Cost. In _Advances in Neural Information Processing Systems_, volume 36. Curran Associates, Inc., 2023. URL https://openreview.net/forum?id=ofa1U5BJVJ.

###### Contents

* 1 Introduction
* 2 Problem Setting
* 3 Unified Likelihood Ratio-based Confidence Sequence for GLMs
	* 3.1 Ellipsoidal Confidence Sequence for Self-Concordant GLMs
	* 3.2 Proof of Theorem 3.1 - PAC-Bayes Approach with Uniform Prior
	* 3.3 Intuitions Behind the Proof of Theorem 3.1
* 4 OFUGLB: A Generic UCB Algorithm for Self-Concordant GLBs
	* 4.1 Proof Sketch of Theorem 4.1 - Regret Analysis of OFUGLB
	* 4.2 Instantiations and Discussions of Theorem 4.1
* 5 Experiments
* 6 Conclusion
* A Additional Related Works
* B Future Works
* Bounding \(L_{t}\)'s * C.1 GLMs that are Bounded by \(M\) * C.2 \(\sigma\)-subGaussian GLMs * C.3 Poisson Distribution
* Ellipsoidal Confidence Sequence
* Regret Bound of OFUGLB * E.1 Key Ideas of the Proof * E.2 Main Proof * E.3 Supporting Lemmas
* F Alternate CS via Discrete Uniform Prior and Covering Argument
* G Deferred Experimental Details and Results from Section 5
* G.1 Implementation Details
* G.2 Additional Results for Logistic Bandits with Fixed Arm-Set
* G.3 Additional Results for Logistic Bandits with \(|\mathcal{A}_{t}|=10\), with Some Updates

Additional Related Works

Relations to Chowdhury et al. (2023).Recently, Chowdhury et al. (2023) proposed two generic CSs for exponential family, one for i.i.d. samples and one for adaptively collected samples. Both CSs are expressed in the local Bregman geometry induced by the log-partition function. The proof relies on the method of mixtures (de la Pena et al., 2004; Kaufmann and Koolen, 2021), which resembles our PAC-Bayesian approach that utilizes a mixture of log-likelihood functions. One drawback is that their main result for i.i.d. samples (Chowdhury et al., 2023, Theorem 3) is instantiated for scalar parameters (e.g., \(\mu\in[0,1]\) for Bernoulli without observed feature vectors), and not for GLMs. While one can attempt to instantiate it to GLMs, we speculate that the resulting confidence set may not be convex since the prior itself is centered at the true parameter, unlike our choice of the prior. While we believe their second method for adaptively collected samples (Chowdhury et al., 2023, Theorem 7) results in a convex set when instantiated to GLMs, the authors do not provide any computationally efficient way to evaluate the integral over the unknown parameter except for the Gaussian GLM. We also mention that contrary to our work, they allow for \(\Theta=\mathbb{R}^{d}\) by introducing a strictly convex regularizer \(Z_{0}\) such that \(\int_{\Theta}\exp(-Z_{0}(\bm{\theta}))d\bm{\theta}<\infty\). Still, theoretically, it is open on whose CS (ours or theirs) is tighter when \(\Theta\subseteq\mathcal{B}^{d}(S)\). We mention in passing that their CS for Gaussian (Chowdhury et al., 2023, Appendix F) improves upon Abbasi-Yadkori et al. (2011) in the same manner (i.e., results in \(\sqrt{a+b}\) instead of \(\sqrt{a}+\sqrt{b}\)) that Flynn et al. (2023) and ours do.

Fast Rates in Statistical Learning.Our goal is to obtain a tight CS for \(\bm{\theta}_{\star}\), which is quite different from that of statistical learning, which is to obtain the optimal decay rate of the ERM. Although it is not immediately clear, we believe they have a connection. To illustrate our suspicion, we recall Example 10 of Grunwald and Mehta (2020). By taking a uniform prior over a function space \(\mathcal{F}^{8}\) and taking the posterior to be randomly sampling from \(\epsilon\)-ball centered at \(\hat{f}\), the KL term becomes the metric entropy of \(\mathcal{F}\), \(\log\mathcal{N}(\mathcal{F},\epsilon)\). Combining this with the Bernstein condition with exponent \(\beta\), the ERM obtains the minimax rate of \(\widetilde{\mathcal{O}}(n^{-1/(2-\beta)})\), which interpolates between the slow rate \(\widetilde{\mathcal{O}}(n^{-1/2})\) and the fast rate \(\widetilde{\mathcal{O}}(1/n)\), where \(n\) is the number of samples. This is similar to what we obtain by considering discrete uniform prior in our proof; see Appendix F for more details. We also remark that our proof of taking a prior over \(\mathcal{L}_{t}\) resembles improper learning and the \(v\)-central condition (Foster et al., 2018; van Erven et al., 2015), which also outputs a mixture of predictors to obtain fast rates.

Randomized Exploration for GLBs.Somewhat orthogonal to UCB-based approaches (including ours), another line of works for **GLBs**(Abeille and Lazaric, 2017; Dong et al., 2019; Janz et al., 2024; Kim et al., 2023; Kveton et al., 2020) focuses on randomized exploration-based approaches. Kveton et al. (2020) proposed Thompson sampling and randomly perturbed history-based algorithms, both of which achieved a frequentist regret bound of \(\widetilde{\mathcal{O}}(d\kappa\sqrt{T\log K})\) for finite arm-set of size \(K\)(Kveton et al., 2020, Theorem 3 & 5). Recently, Janz et al. (2024) proposed EVILL, which _linearly_ perturbs the (regularized) log-likelihood loss that achieves a frequentist regret bound9 of \(\widetilde{\mathcal{O}}(d^{3/2}\sqrt{T/\kappa_{\star}(T)})\) for infinite arm-set (Janz et al., 2024, Theorem 1). In all cases, the extra factor of \(\sqrt{d}\) in the regret (due to posterior variance inflation) is shared across the randomized exploration-based approaches to **GLBs**(Abeille and Lazaric, 2017; Dong et al., 2019; Janz et al., 2024; Kim et al., 2023; Kveton et al., 2020) and is known to be unavoidable for linear Thompson sampling (Hamidi and Bayati, 2020). An interesting question is whether the intuitions from our PAC-Bayesian-derived CS can be combined with aggressive variants of Thompson sampling (e.g., Feel-Good Thompson Sampling of Li et al. (2024); Zhang (2022) or TS-UCB of Baek and Farias (2023)) to improve randomized exploration for **GLBs**.

Footnote 9: Although they considered fixed arm-set, their proof can be easily extended to time-varying arm-sets, the only difference being the length of their warm-up period \(\tau\) now being dependent on the arm-set distribution.

Future Works

Extending our CS (Theorem 3.1) to RKHS.One may wonder if the framework in this paper can be extended to infinite dimensions, in which the covariate \(\bm{x}\) and unknown \(\bm{\theta}_{\star}\) are elements of some function space \(\mathcal{F}\). Indeed, by exploiting the inner product structure of GLMs, in the case where \(\mathcal{F}=\mathcal{H}_{k}\) is an RKHS with reproducing kernel \(k\)(Paulsen and Raghupathi, 2016), the inner product \(\langle\bm{x},\bm{\theta}_{\star}\rangle\) can be replaced with \(k(\bm{x},\bm{\theta}_{\star})\). In statistics literature, this is referred to as the kernelized or functional GLM (Cawley et al., 2007; Muller and Stadtmuller, 2005), and in bandits literature, this has been extensively studied under the name _kernelized bandits_(Chowdhury and Gopalan, 2017; Srinivas et al., 2010) as an infinite-dimensional generalization of linear bandits. Mussi et al. (2024) recently posited the kernelized logistic bandit (e.g., \(r_{t}\sim\mathrm{Bernoulli}(f(\bm{x}_{t}))\) with \(f:\mathcal{X}\to[0,1]\) satisfying \(f\in\mathcal{H}_{k}\)) as an important open problem in the bandits and online learning community.

Extending our CS to RKHS, however, raises several issues, all related to the fact that the usual properties of finite-dimensional spaces often fail in infinite dimensions (Bogachev, 1998; Da Prato and Zabczyk, 2014). For instance, it is well-known that there exists _no_ translation-invariant, locally finite, non-trivial Borel measure on infinite-dimensional Banach space (Oxtoby, 1946, Theorem 1). Then, it is entirely unclear how to extend our current PAC-Bayesian proof of Theorem 3.1 to infinite dimensions, as there is no uniform distribution or likelihood.

One promising alternate approach based on the Gaussian Process has been recently proposed by Neiswanger and Ramdas (2021), where the authors proposed a CS to quantify the uncertainty of GPs. The important point is that the CS is statistically valid even when the prior is misspecified. Still, in our context, choosing the mean and covariance to obtain similar guarantees (e.g., better dependency on \(S^{\prime}:=\|\bm{\theta}_{\star}\|_{k}\)) is non-trivial.

Optimality of CS Radius (Theorem 3.1).Another interesting question is whether our CS radius in Theorem 3.1 is optimal. For general time-uniform estimation with i.i.d. samples, Duchi and Haque (2024) recently showed an information-theoretic lower bound of \(\Omega\left(\sqrt{\frac{\log\log t}{t}}\right)\) on the estimation error. It would be interesting to use a similar technique to show an information-theoretic lower bound on our CS radius for (self-concordant) GLMs, especially w.r.t. \(S\).

Regret Lower Bound for GLBs.One important open question here is the optimality of our obtained regret bound. As discussed in the **Logistic Bandits** paragraph of Section 4, the leading term of our regret bound for _logistic bandits_ is (locally) minimax optimal in \(d,T,\kappa_{\star}(T)\) relative to the lower bound of Abeille et al. (2021). A closer look into their proof shows that their lower bound additionally scales as \(1/S\), indicating a gap of \(S\) between the lower and upper bounds. To the best of our knowledge, there is no generic regret lower bound for self-concordant **GLBs**, and we suspect that a similar \(d\sqrt{T/\kappa_{\star}(T)}\) lower bound holds. One could adapt the proof of Abeille et al. (2021) by modifying parts specific to Bernoulli (e.g., their relative entropy decomposition lemma (Lemma 6) relies on the fact that the reward distribution is Bernoulli), or come up with something new.

Arm-Set Geometry-Dependent Regret Analyses of OFUGLB.For the prior OFUL-type algorithms (Abeille et al., 2021; Lee et al., 2024), the transient term is \(R_{\mathcal{X}}(T):=\sum_{t=1}^{T}\mu(\langle\bm{x}_{t,\star},\bm{\theta}_{ \star}\rangle)\mathds{1}[\bm{x}_{t}\in\mathcal{X}_{-}(t)]\), where \(\mathcal{X}_{-}(t)\) is the set of _detrimental arms_ with a large reward gap and little information (small conditional variance). \(R_{\mathcal{X}}(T)\) is _adaptive to the arm-set geometry_ and can be _completely independent_ of \(\kappa\) for certain arm geometries (Abeille et al., 2021, Proposition 2). For the warmup-based algorithms (Faury et al., 2022; Mason et al., 2022; Sawarni et al., 2024), the transient term _always_ scale with \(\kappa\), which is not adaptive to the arm-set geometry.

Abeille et al. (2021) showed that via an arm-set geometry-dependent analysis for UCB, such \(\kappa\)-scaling transient term can be potentially avoided. However, as our regret analysis utilizes "implicit warmup", our transient term scales with \(\kappa(T)\), which _is not_ adaptive to the arm-set geometry. Thus, the natural question is whether a similar, arm-set geometry adaptive transient term is attainable for logistic bandits, _while_ keeping the optimal \(\mathrm{poly}(S)\)-free leading term. Currently, it seems that the regret decomposition used in our analysis is _incompatible_ with the arm-set geometry-dependent analysis, and we leave to future work for obtaining both characteristics (\(\mathrm{poly}(S)\)-free leading term, arm-set geometry-dependent transient term) for logistic bandits and **GLBs** in general.

The reason for being _incompatible_ is as follows. Even when using our new CS (Theorem 3.1 in the regret analysis of Lee et al. (2024), one still obtains \(\widetilde{\mathcal{O}}(dS\sqrt{T/\kappa_{\star}(T)}+R_{\mathcal{X}}(T))\), the same as Lee et al. (2024). This is because in their proof of Lemma 6, which involves deriving an ellipsoidal CS of the form \(\|\boldsymbol{\theta}-\boldsymbol{\theta}_{\star}\|_{\boldsymbol{H}_{t}}\leq \gamma_{t}(\delta)\), the covering argument introduces a term in \(\gamma_{t}(\delta)\) that _always_ scales as \(dS^{2}\log\frac{St}{d}\), regardless of the likelihood-ratio CS radius. This is unavoidable due to the use of previous self-concordance control (Abeille et al., 2021, Lemma 8), which gives an extra factor of \(S\), and the use of anytime Freedman's inequality (Lee et al., 2024, Lemma 3), which results in a multiplicative factor of \(1/\eta\) for some \(\eta\leq 1/2S\). In other words, attaining the best of both worlds may require thinking of an entirely new regret analysis technique.

**Remark 4** (Detrimental arms for **GLBs**).: _In Abeille et al. (2021), one key component for allowing such transient term that is adaptive to arm-set geometry is that there exists a \(\mathcal{Z}_{\mu}\subseteq\mathbb{R}\) such that \(\sup_{z\in\mathcal{Z}_{\mu}}\hat{\mu}(z)\leq 0\); e.g., for logistic bandits (\(\mu(z)=(1+e^{-z})^{-1}\)), \(\mathcal{Z}=(-\infty,0]\). For general \(\mu\), we can define the set of detrimental arms as \(\mathcal{X}_{-}(t):=\{\boldsymbol{x}\in\mathcal{X}_{t}:\langle\boldsymbol{x}, \boldsymbol{\theta}_{\star}\rangle\in\mathcal{Z}_{\mu}\}\). Of course, the scaling of \(R_{\mathcal{X}}(T)\) depends on various factors, whose precise characterization for \(\mu\)'s beyond the logistic function is left for future work._

Jointly Efficient and Optimal Algorithm for **GLBs**.Despite the statistical superiority of our CS (Theorem 3.1) and our regret bound (Theorem 4.1), it is computationally heavy, especially the UCB maximization (line 6 of Algorithm 1). Our ellipsoidal CS is computationally efficient, but it incurs additional factors of \(S\) in the final regret bound. Then the question remains whether one could achieve order-wise the same regret guarantee for **GLBs** (e.g., \(\mathrm{poly}(S)\)-free for logistic bandits) while significantly improving the computational efficiency. One may, for instance, draw inspiration from recent progress in designing computationally efficient & statistically optimal algorithms for (multinomial) logistic bandits via online Newton steps (Faury et al., 2022; Zhang and Sugiyama, 2023).

Other Applications.It would be interesting to see if our new CS may lead to any improvements in algorithms for **GLBs** beyond OFU, e.g., information-directed sampling (Kirschner and Krause, 2018; Russo and Van Roy, 2018), best arm identification in **GLBs**(Azizi et al., 2022; Jun et al., 2021; Kazerouni and Wein, 2021), and even sample-efficient RLHF (Das et al., 2024; Shi et al., 2024).

Missing Proofs from Table 1 - Bounding \(L_{t}\)'s

### GLMs that are Bounded by \(M\)

Recall that the GLM is bounded by \(M\) if for any \(\bm{x}\in X\) and \(r\sim p(\cdot|\bm{x},\bm{\theta}_{\star})\), the following holds almost surely: \(|r-\mu(\langle\bm{x},\bm{\theta}_{\star}\rangle|\leq M<\infty\).

Then, we have that for any \(\bm{\theta}\in\Theta\),

\[\big{\|}\nabla\mathcal{L}(\bm{\theta})\big{\|} =\frac{1}{g(\tau)}\left\|\sum_{s=1}^{t-1}\left(-r_{s}+\mu(\langle \bm{x}_{s},\bm{\theta}\rangle)\right)\bm{x}_{s}\right\|_{2}\] \[\leq\frac{1}{g(\tau)}\sum_{s=1}^{t-1}\left|r_{s}-\mu(\langle\bm{ x}_{s},\bm{\theta}\rangle)\right|\left\|\bm{x}_{s}\right\|_{2}\] \[\leq\frac{1}{g(\tau)}\sum_{s=1}^{t-1}\left(\left|r_{s}-\mu( \langle\bm{x}_{s},\bm{\theta}_{\star}\rangle)\right|+\left|\mu(\langle\bm{x}_ {s},\bm{\theta}_{\star}\rangle)-\mu(\langle\bm{x}_{s},\bm{\theta}\rangle) \right|\right)\] \[\leq\frac{1}{g(\tau)}\sum_{s=1}^{t-1}\left(M+R_{\hat{\mu}}| \langle\bm{x}_{s},\bm{\theta}_{\star}-\bm{\theta}\rangle|\right)\] \[\leq\frac{(M+2SR_{\hat{\mu}})(t-1)}{g(\tau)}.\]

### \(\sigma\)-subGaussian GLMs

We first recall some definitions:

**Definition C.1**.: _A random variable \(X\in\mathbb{R}\) is \(\sigma\)-subGaussian, if \(\mathbb{P}(\left|X-\mathbb{E}[X]\right|\geq t)\leq 2\exp\left(-\frac{t^{2}}{2 \sigma^{2}}\right),\;\forall t\in\mathbb{R}\)._

**Definition C.2** (Definition 3 of Jin et al. (2019)).: _A random vector \(\bm{X}\in\mathbb{R}^{d}\) is \(\sigma\)-norm-subGaussian, if \(\mathbb{P}(\left\|\bm{X}-\mathbb{E}[\bm{X}]\right\|_{2}\geq t)\leq 2\exp\left(- \frac{t^{2}}{2\sigma^{2}}\right),\;\forall t\in\mathbb{R}\)._

Here is the full statement:

**Proposition C.1**.: _Suppose the GLM is \(\sigma\)-subGaussian. Then, for any \(\delta\in(0,1)\),_

\[\mathbb{P}\left(\exists t\geq 1:L_{t}>\frac{2}{g(\tau)}\left(R_{\hat{\mu}}S (t-1)+2\pi\sigma\sqrt{(t-1)\log\frac{\pi^{2}dt^{2}}{3\delta}}\right)\right) \leq\delta.\] (9)

Proof.: Here, as \(\max_{\bm{x}\in\mathcal{X},\bm{\theta}\in\Theta}|\hat{\mu}(\langle\bm{x},\bm{ \theta}\rangle)|\leq R_{\hat{\mu}}\), we have that

\[L_{t} =\frac{1}{g(\tau)}\max_{\bm{\theta}\in\Theta}\left\|\sum_{s=1}^{t -1}(r_{s}-\mu(\langle\bm{x}_{s},\bm{\theta}\rangle))\bm{x}_{s}\right\|_{2}\] \[\leq\frac{1}{g(\tau)}\max_{\bm{\theta}\in\Theta}\left\|\sum_{s=1}^ {t-1}(\mu(\langle\bm{x}_{s},\bm{\theta}\rangle)-\mu(\langle\bm{x}_{s},\bm{ \theta}_{\star}\rangle))\bm{x}_{s}\right\|_{2}+\frac{1}{g(\tau)}\left\|\sum_{s =1}^{t-1}\underbrace{(r_{s}-\mu(\langle\bm{x}_{s},\bm{\theta}_{\star}\rangle) )\bm{x}_{s}}_{\triangleq\bm{y}_{s}}\right\|_{2}\] \[\leq\frac{2R_{\hat{\mu}}S(t-1)}{g(\tau)}+\frac{1}{g(\tau)}\left\| \sum_{s=1}^{t-1}\bm{y}_{s}\right\|_{2}.\]

We now utilize subGaussian concentrations from Jin et al. (2019). First note that \(\bm{y}_{s}\) is a martingale difference sequence adapted to \(\Sigma_{s}\) and is norm-subGaussian with (conditional) variance \(\sigma^{2}\) be given.

Then, by Corollary 7 of Jin et al. (2019), we have that

\[\mathbb{P}\left(\left\|\sum_{s=1}^{t-1}\bm{y}_{s}\right\|_{2}\leq 4\pi\sigma \sqrt{(t-1)\log\frac{2d}{\delta}}\right)\geq 1-\delta,\quad\forall t\geq 1.\] (10)

The exact constant \(4\pi\) is not available in Jin et al. (2019), as all the constants are hidden under \(c\). This is not useful, especially for practitioners wanting to use the concentration directly. Thus, we tracked the constant from their Corollary 7, the details of which we provide in Lemma C.1.

We then conclude by replacing \(\delta\) with \(\delta/t^{2}\) and applying union bound over \(t\geq 1\), which yields the Basel sum. 

**Lemma C.1** (Lemma 2 of Jin et al. (2019); originally Lemma 5.5 of Vershynin (2010)).: _For any \(\sigma\)-norm-subGaussian random vector \(\bm{X}\), we have that \(\sup_{p\in\mathbb{N}}p^{-1/2}\left(\mathbb{E}[\|\bm{X}\|^{p}]\right)^{1/p}\leq \sqrt{\pi}\sigma\)._

Proof.: This follows from brute-force computation. First, we have that

\[\mathbb{E}[\|\bm{X}\|^{p}]=\int_{0}^{\infty}\mathbb{P}[\|\bm{X}\| ^{p}\geq t]dt=p\int_{0}^{\infty}\mathbb{P}[\|\bm{X}\|\geq t]t^{p-1}dt \leq 2p\int_{0}^{\infty}t^{p-1}\exp\left(-\frac{t^{2}}{2\sigma^{2} }\right)dt\] \[=2^{\frac{p-1}{2}}\sigma^{p}p\Gamma\left(\frac{p}{2}\right).\]

Let us denote \(f(p):=p^{-1/2}\left(\mathbb{E}[\|\bm{X}\|^{p}]\right)^{1/p}\) for \(p\in\mathbb{N}\).

Then, using well-known properties of the Gamma function, we have that

\[f(2p)=\sigma 2^{\frac{2p-1}{4p}}(2p)^{\frac{1}{2p}-\frac{1}{2}}\left((p-1)! \right)^{\frac{1}{2p}}=\sigma\sqrt{p^{-1}\left(\sqrt{2}p!\right)^{\frac{1}{p}}}\]

and

\[f(2p-1)=\sigma 2^{\frac{2p-2}{2(2p-1)}}(2p-1)^{\frac{1}{2p-1}-\frac{1}{2}} \left(\sqrt{\pi}\frac{(2p-3)!!}{2^{p-1}}\right)^{\frac{1}{2p-1}}=\sigma(2p-1 )^{\frac{1}{2p-1}-\frac{1}{2}}\left(\sqrt{\pi}(2p-3)!!\right)^{\frac{1}{2p-1} },\]

where we define \((-1)!!:=1\).

Then, we have that

\[f(2p)\overset{(i)}{<}\sigma\sqrt{p^{-1}(\sqrt{2}p^{p})^{\frac{1}{p}}}=\sigma 2 ^{\frac{1}{4p}}\leq\sigma 2^{\frac{1}{4}},\]

where \((i)\) follows from \(p!<p^{p}\). We also have that

\[f(2p-1)\overset{(i)}{<}\sigma(2p-1)^{\frac{1}{2p-1}-\frac{1}{2}}\left(\sqrt{ \pi}(2p-1)^{p}\right)^{\frac{1}{2p-1}}\overset{(ii)}{<}\sigma\left(\sqrt{\pi} (2p-1)\right)^{\frac{1}{2p-1}}\overset{(iii)}{<}\sigma\sqrt{\pi},\]

where \((i)\) follows from \((2p-3)!!<(2p-1)^{p}\), \((ii)\) follows from \(\frac{p}{2p-1}>\frac{1}{2}\), and \((iii)\) follows from the observations that for \(z\geq e\), \(f(z)=(\sqrt{\pi}z)^{1/z}\) is decreasing10, and \(f(1)=\sqrt{\pi}>f(3)=(3\sqrt{\pi})^{1/3}\).

Footnote 10: \(\frac{d}{d\tilde{z}}\log f(z)=\sqrt{\pi}\frac{1-\log z}{z^{2}}\leq 0,\ \forall z \geq e\).

Finally, as \(2^{1/4}<\sqrt{\pi}\), we have that \(\sup_{p\in\mathbb{N}}f(p)\leq\sqrt{\pi}\sigma\).

### Poisson Distribution

We have the following result for Poisson, which may be of independent interest (to our knowledge, this is the first explicit martingale concentration for Poisson in the GLM form):

**Proposition C.2**.: _For the Poisson distribution, we have that for any \(\delta\in(0,1)\): when \(S>1\),_

\[\mathbb{P}\left(L_{t}\leq C(S)(t-1)+\frac{2}{1-2e^{-S}}\log\frac{\pi^{2}(d+1)t ^{2}}{3\delta}\right)\geq 1-\delta,\quad\forall t\geq 1,\] (11)_where \(C(S):=\frac{1}{4}(1-2e^{-S})(e^{S}+2S+2\log\frac{2(1-2e^{-S})}{e})+2Se^{S}\). When \(S\leq 1\),_

\[\mathbb{P}\left(L_{t}\leq\tilde{C}(S)(t-1)+4\log\frac{\pi^{2}(d+1) t^{2}}{3\delta}\right)\geq 1-\delta,\quad\forall t\geq 1,\] (12)

_where \(\tilde{C}(S):=\frac{1}{16}\left(e^{S}+4S+4\log(8+2e^{S})\right)+2Se^{S}\)._

Proof.: Proceeding similarly as in the previous subsection, we first have that

\[L_{t}\leq 2Se^{S}(t-1)+\left\|\sum_{s=1}^{t-1}\bm{y}_{s}\right\|_{2},\] (13)

where \(\bm{y}_{s}=(r_{s}-e^{\langle\bm{x}_{s},\bm{\theta}_{\star}\rangle})\bm{x}_{s}\) is the martingale difference sequence satisfying \(\mathbb{E}[\bm{y}_{s}|\Sigma_{s}]=\bm{0}\) as \(r_{s}|\Sigma_{s}\sim\mathrm{Poi}(\langle\bm{x}_{s},\bm{\theta}_{\star}\rangle)\).

We now modify the proof of Corollary 7 of Jin et al. (2019) (which is based upon the celebrated Chernoff-Cramer method) for the Poisson martingale vectors, details of which we provide here for completeness.

First, we consider the following MGF bound of the Poisson distribution whose proof is deferred to the end of this subsection:

**Lemma C.2**.: _Suppose that the random vector \(\bm{y}\) is of the form \(\bm{y}=(r-\lambda)\bm{x}\) for some fixed \(\bm{x}\in\mathcal{B}^{d}(1)\), \(r\sim\mathrm{Poi}(\lambda)\), and \(\lambda>0\). Then, for the Hermitian dilation (Tropp, 2015, Definition 2.1.5) of \(\bm{y}\), \(\bm{Y}:=\begin{bmatrix}0&\bm{y}^{\top}\\ \bm{y}&\bm{0}\end{bmatrix}\), we have that \(\mathbb{E}e^{\theta\bm{Y}}\preceq\exp\left(F(\theta,\lambda)\right)\bm{I}_{d+1}\) for \(|\theta|<\frac{1}{2}\), where \(F(\theta,\lambda):=\lambda|\theta|+\log(2|\theta|)+\log\left(\frac{e^{-\frac{ \lambda}{2}}}{2-|\theta|}+\lambda\right)\)._

We also recall the Lieb's trace inequality:

**Theorem C.3** (Theorem 6 of Lieb (1973)).: _Let \(\bm{A}\) be a fixed symmetric matrix, and let \(\bm{Y}\) be a random symmetric matrix. Then,_

\[\mathbb{E}\operatorname{tr}(\exp(\bm{A}+\bm{Y}))\leq\operatorname{tr}\exp(\bm {A}+\log\mathbb{E}e^{\bm{Y}})\] (14)

Now let \(0<\theta<\frac{1}{2}\) be fixed, and let us denote \(\lambda_{s}:=e^{\langle\bm{x}_{s},\bm{\theta}_{\star}\rangle}\) and \(\mathbb{E}_{s}[\cdot]:=\mathbb{E}[\cdot|\Sigma_{s}]\) for \(s\leq t-1\). We start by noting that

\[\mathbb{E}\operatorname{tr}\exp\left(-\theta^{2}\bm{I}_{d+1}\sum _{s=1}^{t-1}F(\theta,\lambda_{s})+\theta\sum_{s=1}^{t-1}\bm{Y}_{s}\right)\] \[=\mathbb{E}\left[\mathbb{E}_{t-1}\left[\operatorname{tr}\exp\left( -\theta^{2}\bm{I}_{d+1}\sum_{s=1}^{t-1}F(\theta,\lambda_{s})+\theta\sum_{s=1} ^{t-1}\bm{Y}_{s}\right)\right]\right]\] \[\leq\mathbb{E}\left[\operatorname{tr}\exp\left(-\theta^{2}\bm{I }_{d+1}\sum_{s=1}^{t-1}F(\theta,\lambda_{s})+\theta\sum_{s=1}^{t-2}\bm{Y}_{s}+ F(\theta,\lambda_{t-1})\bm{I}_{d+1}\right)\right]\] (Lemma C.2, \[\bm{A}\preceq\bm{B}\Rightarrow e^{\bm{C}+\bm{A}}\preceq e^{\bm{C}+\bm{B}}\] ) \[\leq\mathbb{E}\left[\operatorname{tr}\exp\left(-\theta^{2}\bm{I }_{d+1}\sum_{s=1}^{t-2}F(\theta,\lambda_{s})+\theta\sum_{s=1}^{t-2}\bm{Y}_{s} \right)\right]\]\[\leq 2e^{-\rho}\mathbb{E}\operatorname{tr}\exp\left(-\theta^{2}\sum_{s=1}^{t-1}F( \theta,\lambda_{s})+\theta\sum_{s=1}^{t-1}\bm{Y}_{s}\right)\] (Markov's inequality) \[\leq 2(d+1)e^{-\rho}.\] (Lemma C.2)

Finally, by reparametrizing, we have that for any \(\delta\in(0,1)\),

\[\mathbb{P}\left(\left\|\sum_{s=1}^{t-1}\bm{y}_{s}\right\|\geq\inf_{\theta\in( 0,1/2)}\left\{\theta\sum_{s=1}^{t-1}F(\theta,\lambda_{s})+\frac{1}{\theta}\log \frac{2d}{\delta}\right\}\right)\leq\delta,\] (15)

where we recall that \(F(\theta,\lambda)=\lambda\theta+\log(2\theta)+\log\left(\frac{e^{-\frac{3}{2} }}{\frac{1}{2}-\theta}+\lambda\right)\) for \(\theta>0\).

First, when \(S>1\), let us choose \(\theta=\frac{1}{2}-e^{-S}\), which is guaranteed to be positive. Noting that \(\lambda_{s}=e^{\langle\bm{x}_{s},\bm{\theta}_{*}\rangle}\leq e^{S}\), we have

\[F\left(\frac{1}{2}-e^{-S},\lambda_{s}\right)\leq e^{S}\left(\frac{1}{2}-e^{-S }\right)+\log(1-2e^{-S})+\log(2e^{S})=\frac{1}{2}e^{S}+S+\log\frac{2(1-2e^{-S} )}{e}.\]

Thus, the RHS of Eqn. (15)

\[\frac{(1-2e^{-S})(e^{S}+2S+2\log\frac{2(1-2e^{-S})}{e})}{4}(t-1)+\frac{2}{1-2e ^{-S}}\log\frac{2(d+1)}{\delta}.\] (16)

For the case \(S\leq 1\), choosing \(\theta=\frac{1}{4}\), the RHS becomes

\[\frac{e^{S}+4S+4\log(8+2e^{S})}{16}(t-1)+4\log\frac{2(d+1)}{\delta}.\] (17)

Finally, we conclude by parametrizing \(\delta\) as \(\delta/t^{2}\), applying union bound over \(t\geq 1\), and using the Basel sum.

Proof of Lemma c.2.: We first have that

\[\mathbb{E}e^{\theta Y}\stackrel{{(*)}}{{=}}\bm{I}_{d+1} +\sum_{p=1}^{\infty}\frac{\theta^{p}\mathbb{E}\bm{Y}^{2p}}{(2p)!}\preceq\bm{I}_ {d+1}+\sum_{p=1}^{\infty}\frac{\theta^{2p}\mathbb{E}\left\|\bm{y}\right\|^{2p}}{ (2p)!}\bm{I}_{d+1} =\mathbb{E}\left[\frac{e^{\theta\left\|\bm{y}\right\|}+e^{-\theta \left\|\bm{y}\right\|}}{2}\right]\bm{I}_{d+1}\] \[\preceq\mathbb{E}\left[e^{|\theta||r-\lambda|}\right]\bm{I}_{d+1},\]

where \((*)\) follows from the observation that \(\mathbb{E}\bm{Y}^{2p+1}=\bm{0}\). We now recall a concentration result for Poisson distribution:

**Lemma C.3** (Theorem 1 of the note by C. Canonne).: \(\mathbb{P}(|r-y|\geq x)\leq 2e^{-\frac{x^{2}}{2(\lambda+x)}}\).

Then, we have that

\[\mathbb{E}[e^{|\theta||r-\lambda|}] =\int_{0}^{\infty}\mathbb{P}(e^{|\theta||r-\lambda|}>k)dk\] ( \[dk\] is the Lebesgue measure) \[\leq 1+\int_{1}^{\infty}\mathbb{P}(e^{|\theta||r-\lambda|}\geq k)dk\] \[\leq 2\int_{1}^{\infty}e^{-\frac{(\log k/\theta)|^{2}}{2(\lambda+ \log k/\theta|)}}dk\] (Lemma C.3) \[=2|\theta|\int_{0}^{\infty}e^{-\frac{x^{2}}{2(\lambda+u)}+|\theta |u}du\] \[=2|\theta|\left\{\int_{\lambda}^{\infty}e^{-\frac{u^{2}}{2(\lambda +u)}+|\theta|u}du+\int_{0}^{\lambda}e^{-\frac{u^{2}}{2(\lambda+u)}+|\theta|u}du\right\}\] \[\leq 2|\theta|\left\{\int_{\lambda}^{\infty}e^{-(\frac{1}{2}-| \theta|)u}du+\lambda e^{|\theta|\lambda}\right\}\] ( \[\tfrac{u^{2}}{2(\lambda+u)}\geq\tfrac{1}{2}u\text{ for }u\geq\lambda\] ) \[\leq 2|\theta|\left(\frac{1}{\frac{1}{2}-|\theta|}e^{-(\frac{1}{2} -|\theta|)\lambda}+\lambda e^{|\theta|\lambda}\right)\] \[=\exp\left(F(\theta,\lambda)\triangleq\lambda|\theta|+\log(2| \theta|)+\log\left(\frac{e^{-\frac{\lambda}{2}}}{\frac{1}{2}-|\theta|}+\lambda \right)\right).\]Proof of Theorem 3.2 - Ellipsoidal Confidence Sequence

First, similarly to prior works on logistic bandits (Abeille et al., 2021; Lee et al., 2024), let us define the following quantities:

\[\widetilde{\bm{G}}_{t}(\bm{\theta},\bm{\nu}):=\frac{1}{g(\tau)}\sum_{s=1}^{t-1} \tilde{\alpha}_{s}(\bm{\theta},\bm{\nu})\bm{x}_{s}\bm{x}_{s}^{\top},\ \tilde{\alpha}_{s}(\bm{\theta},\bm{\nu}):=\int_{0}^{1}(1-v)\dot{\mu}\left( \langle\bm{x}_{s},\bm{\theta}+v(\bm{\nu}-\bm{\theta})\rangle\right)dv.\]

(We will later come back to these quantities in the regret analysis.)

Then, by Taylor's theorem with integral remainder and first-order optimality condition for convex constrained optimization11, we have that for any \(\lambda\geq 0\),

Footnote 11: Let \(\Theta\) be convex and \(f:\Theta\to\mathbb{R}\) be convex and differentiable. Then, \(\theta^{*}\in\arg\min_{\theta\in\Theta}f(\theta)\) if and only if \(\langle\nabla f(\theta^{*}),\nu-\theta^{*}\rangle\geq 0,\ \ \forall\nu\in\Theta\)(Boyd and Vandenberghe, 2004, Section 4.2.3).

\[\beta_{t}(\delta)^{2}\geq\mathcal{L}_{t}(\bm{\theta})-\mathcal{L} _{t}(\widehat{\bm{\theta}}_{t}) =\underbrace{\langle\nabla\mathcal{L}_{t}(\widehat{\bm{\theta}}_{ t}),\bm{\theta}-\widehat{\bm{\theta}}_{t}\rangle}_{\geq 0}+\left\|\bm{\theta}- \widehat{\bm{\theta}}_{t}\right\|_{\widetilde{\bm{G}}_{t}(\widehat{\bm{ \theta}}_{t},\bm{\theta})}^{2}\] \[\geq\left\|\bm{\theta}-\widehat{\bm{\theta}}_{t}\right\|_{ \widetilde{\bm{G}}_{t}(\widehat{\bm{\theta}}_{t},\bm{\theta})+\lambda\bm{I}_{ d}}^{2}-4S^{2}\lambda.\]

We conclude by using the self-concordance control for \(\widetilde{\bm{G}}\)(Abeille et al., 2021, Lemma 8), which we recall here:

**Lemma D.1** (A slight extension of Lemma 8 of Abeille et al. (2021)).: _Let \(\mu\) be increasing (\(\dot{\mu}\geq 0\), which is basically Assumption 3) and self-concordant with constant \(R_{s}\) (as in Assumption 4). Let \(\mathcal{Z}\subset\mathbb{R}\) be bounded. Then, the following holds for any \(z_{1},z_{2}\in\mathcal{Z}\):_

\[\int_{0}^{1}(1-v)\dot{\mu}(z_{1}+v(z_{2}-z_{1}))dv\geq\frac{\dot{\mu}(z_{1})} {2+R_{s}|z_{1}-z_{2}|}.\]

_This then implies that \(\widetilde{\bm{G}}_{t}(\bm{\theta},\bm{\nu})\succeq\frac{1}{2+2S\bm{R}_{s}} \nabla^{2}\mathcal{L}_{t}(\bm{\theta})\)._Proof of Theorem 4.1 - Regret Bound of \(\operatorname{OFUGLB}\)

Let us denote \(\mu_{t}(\cdot):=\mu(\langle\bm{x}_{t},\cdot\rangle)\) and \([a,b]:=\{a,a+1,\cdots,b\}\) for two integers \(a\leq b\). We recall the following quantities:

\[R_{\mu,\star}:=\max_{\bm{x}\in X}|\mu(\langle\bm{x},\bm{\theta}_{\star}\rangle) |,\quad R_{\hat{\mu}}:=\max_{\bm{x}\in X,\bm{\theta}\in\Theta}\dot{\mu}(\langle \bm{x},\bm{\theta}\rangle).\] (18)

### Key Ideas of the Proof

We will first expand upon the proof sketch provided in Section 4.1 of the main text. Recall the UCB strategy: \((\bm{x}_{t},\bm{\theta}_{t})\leftarrow\arg\max_{\bm{x}\in\mathcal{X}_{t},\bm {\theta}\in\mathcal{C}_{t}}\langle\bm{x},\bm{\theta}\rangle\).

Why Prior Proof Technique Fails.We first show that even though we have a tight CS (Theorem 3.1), naively combining it with existing regret analyses of logistic bandits (Abeille et al., 2021; Lee et al., 2024)_still_ results in an extra factor of \(S\) in the leading term. To see this, let us first recall the existing analyses.

The prior proof starts by bounding the regret by \(\sum_{t=1}^{T}\langle\dot{\mu}_{t}(\bm{\theta}_{\star})\bm{x}_{t},\bm{\theta }_{t}-\bm{\theta}_{\star}\rangle\) (which follows from optimism and first-order Taylor expansion), plus a lower-order term that is easy to control. The first term becomes the leading regret we will now focus on. Using the Cauchy-Schwartz inequality w.r.t. the (regularized) Hessian \(\bm{H}_{t}(\bm{\theta}_{\star})=\lambda\bm{I}+\nabla^{2}\mathcal{L}_{t}(\bm{ \theta}_{\star})=\lambda\bm{I}+\sum_{s=1}^{t-1}\dot{\mu}_{s}(\bm{\theta}_{ \star})\bm{x}_{s}\bm{x}_{s}^{\top}\), each summand is bounded as

\[\dot{\mu}_{t}(\bm{\theta}_{\star})\langle\bm{x}_{t},\bm{\theta}_{t}-\bm{ \theta}_{\star}\rangle\leq\dot{\mu}_{t}(\bm{\theta}_{\star})\left\|\bm{x}_{t} \right\|_{H_{t}(\bm{\theta}_{\star})^{-1}}\left(\left\|\bm{\theta}_{t}-\widehat {\bm{\theta}}_{t}\right\|_{H_{t}(\bm{\theta}_{\star})}+\left\|\bm{\theta}_{ \star}-\widehat{\bm{\theta}}_{t}\right\|_{H_{t}(\bm{\theta}_{\star})}\right).\]

The prior proof then uses Taylor expansion (again) and self-concordant control (Abeille et al., 2021, Lemma 8) to obtain \(\left\|\bm{\theta}_{t}-\widehat{\bm{\theta}}_{t}\right\|_{H_{t}(\bm{\theta}_ {\star})}=\mathcal{O}\left(S\beta_{\mathbb{T}}(\delta)^{2}\right)\) from the likelihood-based confidence set \(\mathcal{L}_{t}(\bm{\theta}_{t})-\mathcal{L}_{t}(\widehat{\bm{\theta}}_{t}) \leq\beta_{\mathbb{T}}(\delta)^{2}\), which introduces a factor of \(S\). This then leads to

\[\sum_{t}\dot{\mu}_{t}(\bm{\theta}_{\star})\langle\bm{x}_{t,\star}-\bm{x}_{t}, \bm{\theta}_{\star}\rangle\lesssim S\beta_{\mathbb{T}}(\delta)^{2}\underbrace{ \sqrt{\sum_{t}\dot{\mu}_{t}(\bm{\theta}_{\star})}}_{\leq\sqrt{T/\kappa_{\star}( T)}}\underbrace{\sqrt{\sum_{t}\dot{\mu}_{t}(\bm{\theta}_{\star})}\bm{x}_{t} \Big{\|}_{H_{t}(\bm{\theta}_{\star})^{-1}}^{2}}_{\leq\sqrt{T/\kappa_{\star}(T) }}\underbrace{\sqrt{\sum_{t}\left\|\sqrt{\dot{\mu}_{t}(\bm{\theta}_{\star})} \bm{x}_{t}\right\|_{H_{t}(\bm{\theta}_{\star})^{-1}}^{2}}}_{\leq\sqrt{T/ \kappa_{\star}(T)}}\underbrace{\sqrt{\sum_{t}\left\|\sqrt{\dot{\mu}_{t}(\bm{ \theta}_{\star})}\bm{x}_{t}\right\|_{H_{t}(\bm{\theta}_{\star})^{-1}}^{2}}}_{ \leq\sqrt{T/\kappa_{\star}(T)}},\]

resulting in a regret whose leading term is _not_\(\operatorname{poly}(S)\)-free.

Towards Our Approach.To obtain a \(\operatorname{poly}(S)\)-free leading term in the regret, we maximally avoid the self-concordance lemma (Abeille et al., 2021, Lemma 8). To do this, our proof begins by obtaining an elliptical CS w.r.t. \(\widetilde{\bm{G}}_{t}(\widehat{\bm{\theta}}_{t})\), derived from the first-order Taylor expansion of \(\mathcal{L}_{t}(\cdot)\) at \(\widehat{\bm{\theta}}_{t}\). With this, we have that \(\left\|\bm{\theta}_{\star}-\widehat{\bm{\theta}}_{t}\right\|_{\widetilde{\bm{G }}_{t}(\widehat{\bm{\theta}}_{t})}=\mathcal{O}(\beta_{\mathbb{T}}(\delta)^{2})\) (Lemma E.6), avoiding the extra \(S\) compared to the prior proof that derives an elliptical CS w.r.t. \(\bm{H}_{t}(\bm{\theta}_{\star})\).

However, the main difficulty of the proof is that \(\widetilde{\bm{G}}_{t}(\widehat{\bm{\theta}}_{t})\) is _not_ in a suitable form for elliptical potential arguments. To see this clearly, consider the following natural optimistic upper-bound of the instantaneous regret:

\[\mu(\langle\bm{x}_{t,\star},\bm{\theta}_{\star}\rangle)-\mu_{t}( \bm{\theta}_{\star}) \leq\mu_{t}(\bm{\theta}_{t})-\mu_{t}(\bm{\theta}_{\star})\] (Optimism) \[=\mu_{t}(\bm{\theta}_{t})-\mu_{t}(\widehat{\bm{\theta}}_{t})-\mu_ {t}(\widehat{\bm{\theta}}_{t})+\mu_{t}(\bm{\theta}_{\star})\] \[\leq 2|\mu_{t}(\bm{\theta}_{t}^{\prime})-\mu_{t}(\widehat{\bm{ \theta}}_{t})|\] \[\lesssim\dot{\mu}_{t}(\widehat{\bm{\theta}}_{t})|\langle\bm{x}_{t}, \bm{\theta}_{t}^{\prime}-\widehat{\bm{\theta}}_{t}\rangle|+\text{lower order terms},\] (Taylor's theorem)

where \(\bm{\theta}_{t}^{\prime}=\arg\max_{\bm{\theta}\in\mathcal{C}_{t}}|\mu_{t}(\bm{ \theta})-\mu_{t}(\widehat{\bm{\theta}}_{t})|\). One can then apply the aforementioned Cauchy-Schwarz w.r.t. \(\widetilde{\bm{G}}_{t}(\widehat{\bm{\theta}}_{t})\) to obtain

\[\dot{\mu}_{t}(\widehat{\bm{\theta}}_{t})|\langle\bm{x}_{t},\bm{\theta}_{t}^{ \prime}-\widehat{\bm{\theta}}_{t}\rangle|\leq\dot{\mu}_{t}(\widehat{\bm{ \theta}}_{t})\left\|\bm{x}_{t}\right\|_{\widetilde{\bm{G}}_{t}(\widehat{\bm{ \theta}}_{t})^{-1}}\left\|\bm{\theta}_{t}-\widehat{\bm{\theta}}_{t}\right\|_{ \widetilde{\bm{G}}_{t}(\widehat{\bm{\theta}}_{t})}\lesssim\dot{\mu}_{t}(\widehat {\bm{\theta}}_{t})\beta_{\mathbb{T}}(\delta)\left\|\bm{x}_{t}\right\|_{\widetilde{ \bm{G}}_{t}^{-1}(\widehat{\bm{\theta}}_{t})}.\]

This successfully avoids using previous self-concordant control (Abeille et al., 2021, Lemma 8), and thus seemingly getting closer to obtaining a \(\operatorname{poly}(S)\)-free regret. Omitting details, the final step is to sum the above over \(t\in[T]\) and apply the elliptical potential lemma (EPL; Abbasi-Yadkori et al. (2011)). EPL is applicable _only_ when \(\widetilde{\bm{G}}_{t}(\widehat{\bm{\theta}}_{t})\) can be written as \(\lambda\bm{I}+\sum_{s=1}^{t-1}\dot{\mu}_{s}(\widehat{\bm{\theta}}_{s})\bm{x}_ {s}\bm{x}_{s}^{\top}\) for some \(\lambda>0\). However, as \(\widetilde{\bm{G}}_{t}(\widehat{\bm{\theta}}_{t})=\sum_{s=1}^{t-1}\xi(\bm{x}_ {s},\widehat{\bm{\theta}}_{t})\bm{x}_{s}\bm{x}_{s}^{\top}\) for some scalar function \(\xi_{s}\), the EPL is _not_ applicable due to the explicit dependency on \(\widehat{\bm{\theta}}_{t}\), not on \(\{\widehat{\bm{\theta}}_{s}\}_{s\in[t-1]}\). The most challenging part of our proof development is making EPL applicable to the summation resulting from some decomposition of the (instantaneous) regret while avoiding extra \(S\)-dependencies.

Our Approach.The key insight is that if we could designate a "worst-case" \(\overline{\bm{\theta}}_{s}\) for each time step \(s\) such that \(\widetilde{\bm{G}}_{t}(\widehat{\bm{\theta}}_{t})\succeq\lambda I+\sum_{s=1}^ {t-1}\dot{\mu}(\overline{\bm{\theta}}_{s})x_{s}\bm{x}_{s}^{\top}=:\bm{Q}_{t}\), then we can perform the following:

\[\dot{\mu}_{t}(\widehat{\bm{\theta}}_{t})|\langle\bm{x}_{t},\bm{\theta}_{t}^{ \prime}-\widehat{\bm{\theta}}_{t}\rangle|\leq\dot{\mu}_{t}(\overline{\bm{ \theta}}_{t})|\langle\bm{x}_{t},\bm{\theta}_{t}^{\prime}-\widehat{\bm{\theta} }_{t}\rangle|+|\dot{\mu}_{t}(\widehat{\bm{\theta}}_{t})-\dot{\mu}_{t}( \overline{\bm{\theta}}_{t})||\langle\bm{x}_{t},\bm{\theta}_{t}^{\prime}- \widehat{\bm{\theta}}_{t}\rangle|\]

where the first term is now be bounded by \(\mathcal{O}\left(\dot{\mu}_{t}(\overline{\bm{\theta}}_{t})\beta_{T}(\delta) \|\bm{x}_{t}\|_{\bm{Q}_{t}^{-1}}\right)\). We can now apply the EPL when summing over \(t\in[T]\), thanks to the form of \(\bm{Q}_{t}\). The second term turns out to be a lower order term via our new self-concordant control that doesn't give additional \(S\)-dependency (Lemma E.3).

We note that this is analogous to the analysis of Logistic-UCB-2 in Faury et al. (2020), where a similar difficulty arose because their improved bonus \(\epsilon_{t,2}\) depends on the current estimate of the parameter as well (see their Lemma 4). They circumvent this issue by explicitly modifying the UCB algorithm to incorporate additional constraints on the "admissible log-odds," which leads to a computationally inefficient algorithm. Indeed, initially, we took a similar approach by either using a confidence set defined as an intersection over all the confidence sets used so far, or by using an additional constraint set \(\mathcal{W}_{t}\) as defined in Logistic-UCB-2 of Faury et al. (2020). However, either approach significantly increases the computational complexity.

We later discovered that we could resolve the issue without changing the confidence set through an alternate analysis, which is the current proof. Specifically, we consider the following decomposition of the instantaneous regret:

\[\mu(\langle\bm{x}_{t,s},\bm{\theta}_{\star}\rangle)-\mu_{t}(\bm{\theta}_{ \star})\leq\mu_{t}(\bm{\theta}_{t})-\mu_{t}(\bm{\theta}_{\star})\leq 2|\mu_{t}( \bm{\nu}_{t})-\mu_{t}(\widehat{\bm{\theta}}_{b(t)})|,\]

where we define \((b(t),\bm{\nu}_{t}):=\arg\max_{b\in[t,T],\bm{\theta}\in\mathcal{C}_{b}}|\mu_{ t}(\bm{\theta})-\mu_{t}(\widehat{\bm{\theta}}_{b})|\). That is, we are bounding the instantaneous regret by how large the difference can be from the current confidence set _and_ how large the difference can be from the future confidence sets. With this, we can then define \(\bar{\bm{\theta}}_{t}:=\arg\min_{\bm{\theta}\in\bigcup_{b\in[t,T]}\mathcal{C} _{b}}\dot{\mu}_{t}(\bm{\theta})\), which satisfies the aforementioned desired property. This is our main technical novelty that allows for us to bypass all the aforementioned difficulties.

Among the omitted details, we consider a slightly more intricate regret decomposition by considering timesteps in which the "warmup conditions" are satisfied and the remaining term, and we derive a novel self-concordance lemma that bounds the difference of \(\dot{\mu}\)'s with that of \(\mu\)'s times \(R_{s}\) (Lemma E.3) that does not incur additional \(S\)-dependencies. We then utilize the elliptical potential _count_ lemma (EPCL; Gales et al. (2022)) for the terms that do not satisfy such conditions, and the remaining terms follow the reasoning as detailed above.

### Main Proof

We defer the statements and proofs of the supporting lemmas to Appendix E.3, although we will provide relevant context when using those lemmas for the proof's duration. We first define the following crucial quantities that we have discussed in the proof sketch: for \(\lambda>0\) to be chosen later,

\[\bar{\bm{\theta}}_{t} :=\operatorname*{arg\,min}_{\bm{\theta}\in\bigcup_{b\in[t,T]} \mathcal{C}_{b}}\dot{\mu}_{t}(\bm{\theta}),\quad(b(t),\bm{\nu}_{t}):= \operatorname*{arg\,max}_{b\in[t,T],\bm{\theta}\in\mathcal{C}_{b}}\Big{|} \mu_{t}(\bm{\theta})-\mu_{t}(\widehat{\bm{\theta}}_{b})\Big{|},\] (19) \[\bar{\bm{H}}_{t} :=2g(\tau)\lambda\bm{I}+\sum_{s=1}^{t-1}\dot{\mu}_{s}(\bar{\bm{ \theta}}_{s})\bm{x}_{s}\bm{x}_{s}^{\top},\quad\bm{V}_{t}:=2g(\tau)\kappa(T) \lambda\bm{I}+\sum_{s=1}^{t-1}\bm{x}_{s}\bm{x}_{s}^{\top},\] (20)

and

\[\tilde{\alpha}_{t}(\bm{\theta},\bm{\nu}):=\int_{0}^{1}(1-v)\dot{\mu}_{t}\left( \bm{\theta}+v(\bm{\nu}-\bm{\theta})\right)dv,\quad\widetilde{\bm{G}}_{t}(\bm{ \theta},\bm{\nu}):=\lambda\bm{I}+\frac{1}{g(\tau)}\sum_{s=1}^{t-1}\tilde{ \alpha}_{s}(\bm{\theta},\bm{\nu})\bm{x}_{s}\bm{x}_{s}^{\top}.\] (21)\(\bm{\theta}_{t}\) in the union of future confidence sets, combined with the "warmup conditions" allows for the elliptical potential lemma (EPL; Lemma E.2) and elliptical potential count lemma (EPCL; Lemma E.1) to be directly applicable, avoiding dependencies on \(\mathrm{poly}(S)\) and \(\kappa\) in the leading term; refer to the expanded proof sketch above for a more detailed explanation of the intuition.

Throughout, let us assume that the event \(\{\forall t\geq 1,\ \bm{\theta}_{\star}\in\mathcal{C}_{t}\}\) holds, which is with probability at least \(1-\delta\) by Theorem 3.1.

Regret Decomposition.Define the set of timesteps satisfying the "warmup conditions":

\[\mathcal{I}_{T}:=\left\{t\in[T]:\left(\left\|\sqrt{\tilde{\mu}_{t}(\bm{\bar{ \theta}}_{t})}\bm{x}_{t}\right\|_{\bm{H}_{t}^{-1}}\geq 1\right)\ \vee\ \left(\left\|\bm{x}_{t}\right\|_{\bm{V}_{t}^{-1}}\geq 1 \right)\right\}.\] (22)

Then we have the following regret decomposition:

\[\mathsf{Reg}(T) =\sum_{t\in\mathcal{I}_{T}}\left\{\mu(\langle\bm{x}_{t,\star}, \bm{\theta}_{\star}\rangle)-\mu(\langle\bm{x}_{t},\bm{\theta}_{\star}\rangle) \right\}+\underbrace{\sum_{t\not\in\mathcal{I}_{T}}\left\{\mu(\langle\bm{x}_{ t,\star},\bm{\theta}_{\star}\rangle)-\mu(\langle\bm{x}_{t},\bm{\theta}_{\star} \rangle)\right\}}_{\triangleq\mathsf{Reg}_{\mathcal{I}}(T)}\] \[\leq 2R_{\mu,\star}|\mathcal{I}_{T}|+\mathsf{Reg}_{\mathcal{I}}(T)\] \[\leq 2R_{\mu,\star}\sum_{t\in[T]}\mathds{1}\left[\left\|\sqrt{ \tilde{\mu}_{t}(\bm{\bar{\theta}}_{t})}\bm{x}_{t}\right\|_{\bm{\bar{H}}_{t}^{- 1}}\geq 1\right]+2R_{\mu,\star}\sum_{t\in[T]}\mathds{1}\left[\left\|\bm{x}_{t} \right\|_{\bm{V}_{t}^{-1}}\geq 1\right]+\mathsf{Reg}_{\mathcal{I}}(T)\] (Definition of \[\mathcal{I}_{T}\] ) \[\leq\frac{4dR_{\mu,\star}}{\log 2}\left\{\log\left(1+\frac{R_{\mu}}{ 2\lambda g(\tau)\log 2}\right)+\log\left(1+\frac{1}{2\kappa(T)\lambda g(\tau) \log 2}\right)\right\}+\mathsf{Reg}_{\mathcal{I}}(T).\] (EPCL (Lemma E.1))

We now focus on bounding the last term:

\[\mathsf{Reg}_{\mathcal{I}}(T) =\sum_{t\not\in\mathcal{I}_{T}}\left\{\mu_{t,\star}(\bm{\theta}_{ \star})-\mu_{t}(\widehat{\bm{\theta}}_{t})\right\}+\sum_{t\not\in\mathcal{I}_ {T}}\left\{\mu_{t}(\widehat{\bm{\theta}}_{t})-\mu_{t}(\bm{\theta}_{\star})\right\}\] \[(\mu_{t}(\cdot):=\mu(\langle\bm{x}_{t},\cdot\rangle),\,\mu_{t, \star}(\cdot):=\mu(\langle\bm{x}_{t,\star},\cdot\rangle))\] \[\leq\sum_{t\not\in\mathcal{I}_{T}}\left\{\mu_{t}(\bm{\theta}_{t}) -\mu_{t}(\widehat{\bm{\theta}}_{t})\right\}+\sum_{t\not\in\mathcal{I}_{T}} \left\{\mu_{t}(\widehat{\bm{\theta}}_{t})-\mu_{t}(\bm{\theta}_{\star})\right\}\] (optimism - line 7 of Algorithm 1) \[\leq 2\sum_{t\not\in\mathcal{I}_{T}}\max_{b\in[t,T]}\max_{\bm{ \theta}\in\mathcal{C}_{b}}\Bigl{|}\mu_{t}(\bm{\theta})-\mu_{t}(\widehat{\bm{ \theta}}_{b})\Bigr{|}\] \[=2\sum_{t\not\in\mathcal{I}_{T}}\Bigl{|}\mu_{t}(\nu_{t})-\mu_{t}( \widehat{\bm{\theta}}_{b(t)})\Bigr{|}.\] (Eqn. ( 19 ))

Using Taylor's theorem with integral remainder form, we have that for \(t\not\in\mathcal{I}_{T}\),

\[\left|\mu_{t}(\bm{\nu}_{t})-\mu_{t}(\widehat{\bm{\theta}}_{b(t)})\right|\] \[=\left|\hat{\mu}_{t}(\widehat{\bm{\theta}}_{b(t)})\langle\bm{x}_ {t},\bm{\nu}_{t}-\widehat{\bm{\theta}}_{b(t)}\rangle+\int_{\mu_{t}(\widehat{\bm {\theta}}_{b(t)})}^{\mu_{t}(\bm{\nu}_{t})}(\mu_{t}(\bm{\nu}_{t})-z)\tilde{\mu}_{ t}(z)dz\right|\] \[\leq\hat{\mu}_{t}(\widehat{\bm{\theta}}_{b(t)})\left|\langle\bm{x }_{t},\bm{\nu}_{t}-\widehat{\bm{\theta}}_{b(t)}\rangle\right|+\langle\bm{x}_ {t},\bm{\nu}_{t}-\widehat{\bm{\theta}}_{b(t)}\rangle^{2}\int_{0}^{1}(1-v) \left|\tilde{\mu}_{t}\left(\widehat{\bm{\theta}}_{b(t)}+v(\bm{\nu}_{t}- \widehat{\bm{\theta}}_{b(t)})\right)\right|dv\] (triangle inequality, reparametrization) \[\leq\hat{\mu}_{t}(\widehat{\bm{\theta}}_{b(t)})\left|\langle\bm{x }_{t},\bm{\nu}_{t}-\widehat{\bm{\theta}}_{b(t)}\rangle\right|+R_{s}\langle\bm{x }_{t},\bm{\nu}_{t}-\widehat{\bm{\theta}}_{b(t)}\rangle^{2}\underbrace{\int_{0 }^{1}(1-v)\dot{\mu}_{t}\left(\widehat{\bm{\theta}}_{b(t)}+v(\bm{\nu}_{t}- \widehat{\bm{\theta}}_{b(t)})\right)dv}_{=\tilde{a}_{b(t)}(\widehat{\bm{ \theta}}_{b(t)},\bm{\nu}_{t})}\] (Assumption 4)

[MISSING_PAGE_FAIL:32]

[MISSING_PAGE_FAIL:33]

Wrapping Up.Let us choose \(\lambda=\frac{1}{4\delta^{2}}\), and let us denote \(A\lesssim B\) if \(A\leq cB\) for some absolute constant \(c>0\). Then, combining everything, we have:

\[\sum_{t\notin\mathcal{I}_{T}}\left|\mu_{t}(\bm{\nu}_{t})-\mu_{t}( \widehat{\bm{\theta}}_{b(t)})\right|\] \[\leq\sum_{t\notin\mathcal{I}_{T}}A_{t}+\sum_{t\notin\mathcal{I}_{ T}}B_{t}+\sum_{t\notin\mathcal{I}_{T}}C_{t}\] \[\lesssim\beta_{T}(\delta)\sqrt{dg(\tau)\log\left(1+\frac{R_{ \hat{\mu}}ST}{d}\right)}\sqrt{\frac{T}{\kappa_{\star}(T)}+R_{s}\sum_{t\notin \mathcal{I}_{T}}\left|\mu_{t}(\bm{\nu}_{t})-\mu_{t}(\widehat{\bm{\theta}}_{b(t )})\right|}\] \[\qquad+dR_{s}R_{\hat{\mu}}\kappa(T)\beta_{T}(\delta)\sqrt{g(\tau) }\log\left(1+\frac{ST}{dg(\tau)\kappa(T)}\right),\]

as the upper bound for \(\sum_{t}C_{t}\) is asymptotically negligible compared to that of \(\sum_{t}B_{t}\).

This is of the form \(X\lesssim A\sqrt{B+R_{s}X}+C\) with \(X:=\sum_{t\notin\mathcal{I}_{T}}\left|\mu_{t}(\bm{\nu}_{t})-\mu_{t}(\widehat{ \bm{\theta}}_{b(t)})\right|\), which then implies \(X\lesssim A\sqrt{B}+A\sqrt{R_{s}}+C\) thanks to an elementary polynomial inequality (Abeille et al., 2021, Proposition 7). We then conclude by combining the above inequality with the regret decomposition done at the beginning. 

### Supporting Lemmas

The following are the elliptical potential arguments, which we state without proof:

**Lemma E.1** (Elliptical Potential Count Lemma; EPCL12).: _For \(X,L>0\), let \(\bm{x}_{1},\cdots,\bm{x}_{T}\in\mathcal{B}^{d}(X)\) be a sequence of vectors, \(\bm{V}_{t}:=\lambda\bm{I}+\sum_{s=1}^{t-1}\bm{x}_{s}\bm{x}_{s}^{\intercal}\), and let us define the following: \(\mathcal{H}_{T}:=\left\{t\in[T]:\|\bm{x}_{t}\|_{\bm{V}_{t}^{-1}}^{2}>L\right\}\). Then, we have that_

Footnote 12: This is a generalization of Exercise 19.3 of Lattimore and Szepesvari (2020), presented (in parallel) at Lemma 7 of Gales et al. (2022) and Lemma 4 of Kim et al. (2022).

\[|\mathcal{H}_{T}|\leq\frac{2d}{\log(1+L^{2})}\log\left(1+\frac{X^{2}}{\lambda \log(1+L^{2})}\right).\] (23)

**Lemma E.2** (Elliptical Potential Lemma; EPL13).: _Let \(\bm{x}_{1},\cdots,\bm{x}_{T}\in\mathcal{B}^{d}(X)\) be a sequence of vectors and \(\bm{V}_{t}:=\lambda\bm{I}+\sum_{s=1}^{t-1}\bm{x}_{s}\bm{x}_{s}^{\intercal}\). Then, we have that_

Footnote 13: Lemma 11 of Abbasi-Yadkori et al. (2011).

\[\sum_{t=1}^{T}\min\left\{1,\|\bm{x}_{t}\|_{\bm{V}_{t}^{-1}}^{2}\right\}\leq 2 d \log\left(1+\frac{X^{2}T}{d\lambda}\right).\] (24)

The following is a "self-bounding" property of self-concordant function:

**Lemma E.3**.: _For \(\bm{\theta},\bm{\nu}\in\mathbb{R}^{d}\), \(|\dot{\mu}_{t}(\bm{\theta})-\dot{\mu}_{t}(\bm{\nu})|\leq R_{s}|\mu_{t}(\bm{ \theta})-\mu_{t}(\bm{\nu})|\)_

Proof.: This follows from direct computation:

\[|\dot{\mu}_{t}(\bm{\theta})-\dot{\mu}_{t}(\bm{\nu})| =\left|\langle\bm{x}_{t},\bm{\theta}-\bm{\nu}\rangle\int_{0}^{1 }\ddot{\mu}_{t}(\bm{\nu}+v(\bm{\theta}-\bm{\nu})dv\right|\] \[\leq\left|\langle\bm{x}_{t},\bm{\theta}-\bm{\nu}\rangle\right|\int _{0}^{1}\left|\dot{\mu}_{t}(\bm{\nu}+v(\bm{\theta}-\bm{\nu})\right|dv\] \[\leq R_{s}\left|\langle\bm{x}_{t},\bm{\theta}-\bm{\nu}\rangle \right|\int_{0}^{1}\left|\dot{\mu}_{t}(\bm{\nu}+v(\bm{\theta}-\bm{\nu})\right|dv\] (Assumption 4)

[MISSING_PAGE_EMPTY:35]

Alternate CS via Discrete Uniform Prior and Covering Argument

In this Appendix, instead of the PAC-Bayes with a continuous uniform prior/posterior as in the main text, we explore an alternate derivation of CS using a discrete uniform prior. This is a supplementary discussion for the "**Fast Rates in Statistical Learning**" paragraph in Appendix A.

We present the alternate CS, which is strictly looser than our Theorem 3.1 but more "elementary":

**Theorem F.1** (Slightly Looser, Unified CS for GLMs).: _Let \(L_{t}:=\max_{\bm{\theta}\in\Theta}\left\|\nabla\mathcal{L}_{t}(\bm{\theta}) \right\|_{2}\) be the Lipschitz constant of \(\mathcal{L}_{t}(\cdot)\) that may depend on \(\{(\bm{x}_{s},r_{s})\}_{s=1}^{t-1}\). Then, we have \(\mathbb{P}[\exists t\geq 1:\bm{\theta}_{\star}\not\in\mathcal{C}_{t}(\delta)]\leq\delta\), where_

\[\beta_{t}(\delta)^{2}=\log\frac{\pi^{2}t^{2}}{6\delta}+\inf_{c\in(0,5S]}\left\{ d\log\frac{5S}{c}+cL_{t}\right\}\leq\log\frac{\pi^{2}t^{2}}{6\delta}+d\log(1 \lor 5SL_{t})+1,\]

_where the last inequality follows from the choice \(c=5S\wedge\frac{1}{L_{t}}\)._

Proof.: Consider \(p=\mathcal{U}(\{\bm{\theta}_{i}\}_{i\in[N]})\), where the \(\bm{\theta}_{i}\)'s will be determined later. In that case, we have:

\[\log\mathbb{E}_{\bm{\theta}\sim p}[M_{t}(\bm{\theta})] =\mathcal{L}_{t}(\bm{\theta}_{\star})+\log\mathbb{E}_{\bm{\theta }\sim p}[\exp\left(-\mathcal{L}_{t}(\bm{\theta})\right)]\] \[=\mathcal{L}_{t}(\bm{\theta})+\log\left\{\frac{1}{N}\sum_{i=1}^{N }\exp\left(-\mathcal{L}_{t}(\bm{\theta}_{i})\right)\right\}\] \[\geq\mathcal{L}_{t}(\bm{\theta}_{\star})+\log\left\{\frac{1}{N} \max_{i\in[N]}\exp\left(-\mathcal{L}_{t}(\bm{\theta}_{i})\right)\right\}\] \[=\mathcal{L}_{t}(\bm{\theta}_{\star})-\min_{i\in[N]}\mathcal{L}_{ t}(\bm{\theta}_{i})+\log\frac{1}{N}.\] (25)

From the proof of Lemma 3.1, one can see that \(\mathbb{E}[M_{t}(\bm{\theta})|\bm{\theta}]=1\) where \(\mathbb{E}\) is w.r.t. the randomness of the sequential data (i.e., of \(\mathcal{L}_{t}(\cdot)\)). Then, by Markov's inequality, for any \(\delta\in(0,1)\),

\[\mathbb{P}\left(M_{t}(\bm{\theta}_{i})\geq\frac{N}{\delta}\right)\leq\frac{ \delta}{N},\quad\forall i\in[N],\] (26)

where again, \(\mathbb{P}\) is w.r.t. the randomness of \(\mathcal{L}_{t}(\cdot)\). Then, we have that

\[\mathbb{P}\left(\mathbb{E}_{\bm{\theta}\sim p}[M_{t}(\bm{\theta}) ]=\frac{1}{N}\sum_{i\in[N]}M_{t}(\bm{\theta}_{i})\geq\frac{1}{\delta}\right) \leq\mathbb{P}\left(\max_{i\in[N]}M_{t}(\bm{\theta}_{i})\geq\frac{ N}{\delta}\right)\] \[\leq\sum_{i\in[N]}\mathbb{P}\left(M_{t}(\bm{\theta}_{i})\geq\frac {N}{\delta}\right)\] (union bound) \[\leq\sum_{i\in[N]}\frac{\delta}{N}=\delta.\] (Eqn. ( 26 ) )

Combining this with Eqn. ( 25 ), we have that

\[\mathbb{P}\left(\mathcal{L}_{t}(\bm{\theta}_{\star})-\min_{i\in[N]}\mathcal{L} _{t}(\bm{\theta}_{i})\leq\log\frac{N}{\delta}\right)\geq 1-\delta,\quad\forall t\geq 1.\]

By reparametrizing \(\delta\) as \(\frac{\delta}{t^{2}}\) and taking the union bound over \(t\geq 1\), we have that by the Basel sum,

\[\mathbb{P}\left(\exists t\geq 1:\mathcal{L}_{t}(\bm{\theta}_{\star})-\min_{i\in[N] }\mathcal{L}_{t}(\bm{\theta}_{i})\geq\log N+\log\frac{\pi^{2}t^{2}}{6\delta} \right)\leq\delta.\] (27)Thus, the following holds with probability at least \(1-\delta\): for all \(t\geq 1\),

\[\mathcal{L}_{t}(\bm{\theta}_{\star})-\min_{\bm{\theta}\in\Theta} \mathcal{L}_{t}(\bm{\theta}) \leq\log\frac{\pi^{2}t^{2}}{6\delta}+\log N+\min_{i\in[N]}\mathcal{ L}_{t}(\bm{\theta}_{i})-\min_{\bm{\theta}\in\Theta}\mathcal{L}_{t}(\bm{\theta})\] \[\leq\log\frac{\pi^{2}t^{2}}{6\delta}+\log N+L_{t}\min_{i\in[N]} \left\|\bm{\theta}_{i}-\bm{\widehat{\theta}}_{t}\right\|_{2},\ \ \ \ (\bm{\widehat{\theta}}_{t}=\arg \min_{\bm{\theta}\in\Theta}\mathcal{L}_{t}(\bm{\theta}))\]

where we recall that \(L_{t}\) is the Lipschitz constant of \(\mathcal{L}_{t}(\cdot)\).

We choose \(\{\bm{\theta}_{i}\}\) to be a \(c\)-net of \(\Theta\) for \(c\in(0,5S]\). Then, \(\min_{i\in[N]}\left\|\bm{\theta}_{i}-\bm{\widehat{\theta}}_{t}\right\|_{2}\leq c\) by definition, and as \(\Theta\subseteq\mathcal{B}^{d}(S)\), \(N\leq\left(\frac{5S}{c}\right)^{d}\)(Vershynin, 2018, Corollary 4.2.13). Combining everything and taking \(\min_{c\in(0,5S]}\) gives the desired statement.

Deferred Experimental Details and Results from Section 5

### Implementation Details

For time-varying arm-sets, the randomness of the arm-sets is shared across all the algorithms, i.e., at each time-step \(t\), all the algorithms see the same arm-set. Thus, the only randomness is from the reward distributions. Whenever applicable, we utilize the Sequential Least SQuares Programming (SLSQP; Kraft (1988)) implemented in SciPy (Virtanen et al., 2020) for computing the norm-constrained MLE. This minimizes the effect of optimization errors whenever possible, allowing us to compare the algorithms clearly from a statistical perspective. For OFUGLB, EMK, RS-GLinCB, and OFULog+, SLSQP is utilized to compute the UCB index as well. We use the same implementation of ada-OFU-ECOLog and RS-GLinCB as in the publicly available GitHub repository of Faury et al. (2022)14 and Sawami et al. (2024)15, respectively. As mentioned in the main text, we utilize the exact theoretical hyperparameters for RS-GLinCB as provided in Algorithm 2 of Sawami et al. (2024).

Footnote 14: https://github.com/louisfaurry/logistic_bandit

Footnote 15: https://github.com/nirjhar-das/GLBandit_Limited_Adaptivity

### Additional Results for Logistic Bandits with Fixed Arm-Set

Results and Discussions.The results are shown in Figure 2. There are some common characteristics compared to the plots for time-varying arm-sets (Figure 1 in the main text). There is still a discrepancy between the tightness of the CSs and the actual regret for OFULog+ vs. OFUGLB-e, and RS-GLinCB still performs the worst. Also, OFUGLB, EMK, and EVILL are still the best-performing algorithms, at least eventually. Let us now highlight some key qualitative differences from time-varying arm-set plots as well as relevant discussions.

First, the regret curves seem linearly increasing overall, especially at \(S\in\{8,10\}\). In our settings, \(T=10000\) is still in a transient phase of all the algorithms and, thus, yet to reach the asymptotic

Figure 2: Fixed arm-set. (First row) Regret plots of all considered algorithms. (Second row) Magnified regret plots. (Third row) Confidence set plots at the final time \(t=10000\) when applicable. Each column represents a different logistic bandit instance for \(S\in\{4,6,8,10\}\).

regime. To see the logarithmic-looking regret curve and to numerically compare the "numerical asymptotic regret" of the algorithms, we plan to run the experiments for much longer timesteps, e.g., \(T=50000\).

Second, for \(S=10\), it _seems_ that OFUGLB-e is the best performing algorithm. We suspect that this is because the OFUGLB-e happens to exploit a "good" direction in the beginning, and In other words, we believe that if the experiments are run with much more iterations, then at the end, due to its design, OFUGLB-e will have to explore other unexplored directions, causing an increase in the regret. Indeed, if one takes a close look at \(S\in\{6,8\}\), note that there is a phase at which OFUGLB-e seems to perform the best in the beginning, but in the end its regret increases well beyond other well-performing baselines: OFUGLB, EMK, and EVILL.

**Remark 5**.: _Although our current implementation always uses SLSQP for all the optimization procedures (for MLE and UCB index computations), when the arm-set is fixed, the overall implementations of all the algorithms can be made more computationally efficient. One approach is to utilize the iterative reweighted least squares (IRLS; Wolke and Schweltick (1988)) and keep track of the number of pulls of each arm vector, which is possible as the arm-set is fixed); see Section 3.3 of Kveton et al. (2020) and the original implementation16 of EVILL using IRLS._

Footnote 16: https://github.com/DavidJanz/EVILL-code

### Additional Results for Logistic Bandits with \(|\mathcal{A}_{t}|=10\), with Some Updates

Here, we provide additional results for \(|\mathcal{A}_{t}|=10\), for both time-varying and fixed arm-sets. These results were obtained as a test run of the significantly refactored codes in our GitHub repository (see the commits in Jan 2025). Compared to the experimental details presented so far, there are two main updates.

Notable Updates.One is that we utilize the exact theoretical hyperparameters for EVILL as provided in Appendix E of Janz et al. (2024). To be as faithful to the theoretical results presented in Janz et al. (2024), for the fixed arm-set setting, we implement the warm-up phase of EVILL via the G-optimal design (Pukelsheim, 2006), which is in turn implemented using Frank-Wolfe iterations (Frank and Wolfe, 1956); see Appendix B of Janz et al. (2024) and references therein for more discussions. Another is that for the implementation of EMK(Emmenegger et al., 2023), we utilize the Vovk-Azoury-Warmuth forecaster type regularizer due to AIOLI of Jezequel et al. (2020) instead of the log-partition-based regularizer as suggested by Emmenegger et al. (2023).

Results and Discussions.The results are shown in Figure 3 and 4. There are several notable observations to be made. One is that with the theoretical hyperparameter, EVILL performs worst or second-worst, suggesting that despite its practical efficacy (Chapelle and Li, 2011; Russo et al., 2018), there is still a big theory-practice gap, at least for logistic bandits. Second observation is that our OFUGLB performs worse than EMK. We believe this is due to the change in the arm set size, \(|\mathcal{A}_{t}|\). Still, the trend suggests that as \(S\) gets larger, our OFUGLB may perform better than EMK. This is expected, considering how our theories focus on removing the \(S\)-dependency, which is significant only when \(S\) is large. We should, however, emphasize that Emmenegger et al. (2023) does not provide a _tight theoretical (regret) guarantee_ for EMK, while ours do. Also, in the time-varying arm-set setting, our OFUGLB-e is CS is tighter than EMK's. Lastly, in the fixed arm-set setting, OFUGLB-e behaves quite unstably as \(S\) increases. We believe this is due to our theoretical choice of \(\lambda=\frac{1}{8S^{2}(1+SR_{s})}\) decaying with \(S\), possibly making \(\nabla^{2}\mathcal{L}_{t}(\widehat{\bm{\theta}}_{t})+\lambda\bm{I}_{d}\) ill-conditioned. In practice, one could tune \(\lambda\) for a good and stable performance.

Figure 3: Time-varying arm-sets with \(|\mathcal{A}_{t}|=10\).

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The abstract and introduction accurately reflect the paper's contributions and scope by introducing the new CS for GLMs and applying it to GLBs to obtain state-of-the-art regrets Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: Several limitations are posited in the main text, as well as in Appendix B. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?Answer: [Yes] Justification: All the assumptions are detailed in the main text, and the proofs for all the statements are provided either in the main text or in the Appendix. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: The experiments are simple enough to be reproduced, and we additionally present the GitHub repository containing the codes for reproducing the experiments. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code**Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: See our GitHub repository: https://github.com/nick-jhlee/logistic_bandit Guidelines:

* The answer NA means that paper does not include experiments requiring code.
* Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
* The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: This is detailed in the experiment section. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: The experiments are averaged over 10 runs, and the standard deviations are marked in the regret plots (Figure 1). Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).

* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [No] Justification: The experiments are very simple and can be run on normal CPUs. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: This is purely theoretical. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: This is purely theoretical and thus have no negative impact. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.

* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: The paper is purely theoretical. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We cite Abeille et al. (2021); Faury et al. (2022); Janz et al. (2024); Lee et al. (2024); Sawarni et al. (2024), who has a public GitHub repository for the experimental setting that we follow in our paper. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.

* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: No new assets are introduced. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: No humans were involved. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: no humans were involved. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. * We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. * For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.