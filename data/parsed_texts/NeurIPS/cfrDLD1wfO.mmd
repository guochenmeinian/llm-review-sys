# Graph Diffusion Transformers for

Multi-Conditional Molecular Generation

 Gang Liu, Jiaxin Xu, Tengfei Luo, Meng Jiang

University of Notre Dame

{gliu7, jxu24, tluo, mjiang2}@nd.edu

###### Abstract

Inverse molecular design with diffusion models holds great potential for advancements in material and drug discovery. Despite success in unconditional molecular generation, integrating multiple properties such as synthetic score and gas permeability as condition constraints into diffusion models remains unexplored. We present the Graph Diffusion Transformer (Graph DiT) for multi-conditional molecular generation. Graph DiT integrates an encoder to learn numerical and categorical property representations with the Transformer-based denoiser. Unlike previous graph diffusion models that add noise separately on the atoms and bonds in the forward diffusion process, Graph DiT is trained with a novel graph-dependent noise model for accurate estimation of graph-related noise in molecules. We extensively validate Graph DiT for multi-conditional polymer and small molecule generation. Results demonstrate the superiority of Graph DiT across nine metrics from distribution learning to condition control for molecular properties. A polymer inverse design task for gas separation with feedback from domain experts further demonstrates its practical utility.

## 1 Introduction

Diffusion models for molecular graphs are essential for inverse design of materials and drugs by generating molecules and polymers (macro-molecules) [40, 46], because the models can be effectively trained to predict discrete graph structures and atom/bond types in denoising processes [43]. Practical inverse designs consider multiple factors such as molecular synthetic score and various properties [15], known as the task of multi-conditional graph generation.

Existing work converted multiple conditions into a single one and solved the task as single-condition generation [5, 25]. However, multi-property relations may not be properly or explicitly defined [5]. First, the properties have diverse scales and units. For example, the synthetic complexity ranges from 1 to 5 [8], while the gas permeability varies widely, exceeding 10,000 in Barrier units [4]. This gap makes it hard for models to balance the conditions. Second, multi-conditions consist of a mix of categorical and numerical properties. The common practice of addition [47] or multiplication [25] is inadequate for combination.

Figure 1(a) empirically illustrates the challenges in multi-conditional generation, i.e., discovering molecules meeting multiple properties. We used a test set of 100 data points with three properties: synthesizing (Synth.) [12], O\({}_{2}\) and N\({}_{2}\) permeability (O\({}_{2}\)Perm and N\({}_{2}\)Perm) [4]. A single-conditional diffusion model generated up to 30 graphs for each condition, resulting in a total of 90 graphs for three conditions. We sort the 30 graphs in each set using a polymer property Oracle (see appendix B.3). Then, we check whether a shared polymer structure that meets multi-property constraints can be identified across different condition sets. If we find the polymer, its rank \(K\) (where \(K\) is between 1 and 30) indicates how high it appears on the lists, considering all condition sets. If not, we set \(K\) as 30. Figure 1(a) shows the frequency distribution of \(K\) on the 100 test cases. The median \(K\) was30, indicating that the multiple properties were not met on over half of the test polymers despite generating a large number of graphs.

To address these challenges, we project multi-properties into representations by _learning_, thereby guiding the diffusion process for molecule generation. We propose the Graph Diffusion Transformer (Graph DiT) for graph denoising under conditions. Graph DiT has a condition encoder for property representation learning and a graph denoiser. The condition encoder utilizes a novel clustering-based method for numerical properties and one-hot encoding for categorical ones to learn multi-property representations. The graph denoiser first integrates node and edge features into graph tokens, then denoise these tokens with adaptive layer normalization (AdaLN) in Transformer layers [19; 34]. AdaLN replaces the molecular statistics (mean and variance) in each hidden layer with those from the condition representation, effectively outperforming other predictor-based and predictor-free conditioning methods [22; 43; 34], as shown in Section 4.4. We observe that existing forward diffusion processes [43; 22] apply noise separately to atoms and bonds, which may compromise the accuracy of Graph DiT in noise estimation. Hence, we propose a novel graph-dependent noise model that effectively applies noise tailored to the dependencies between atoms and bonds within the graph.

Results in Figure 1(b) show that the polymers generated by Graph DiT closely align with multi-property constraints. For each test case, we have _one_ graph generated from Graph DiT conditional on three properties. The Oracle determines the rank of this graph among 30 single-conditionally generated graphs for each condition. We find the median ranks are 4, 9, and 11, for Synth., O\({}_{2}\) Perm, and N\({}_{2}\) Perm, respectively, all much higher than 30. Note that the ranked set of 30 graphs was very competitive because the model was trained on the specific condition dedicatedly.

In experiments, we evaluate model performance on one polymer and three small molecule datasets. The polymer dataset includes four numerical conditions for multi-conditional evaluation. Our model has the lowest average mean absolute error (MAE), significantly reducing the error by 17.86% compared to the best baseline. It also excels in small molecule tasks, achieving over 0.9 accuracy on task-related categorical conditions, notably surpassing the baseline accuracy of less than 0.6. We also examine the model's utility in inverse polymer designs for O\({}_{2}\)/N\({}_{2}\) gas separation, with domain expert feedback highlighting our model's practical utility in multi-conditional molecular design.

Figure 1: Multi-conditional diffusion guidance in (b) generates polymers of higher property accuracy than existing work in (a). Explanations are in Section 1 and details are in appendix B.3.

## 2 Problem Definition

### Multi-Conditional Inverse Molecular Design

A molecular graph \(G=(V,E)\) consists of a set of nodes (atoms) \(V\) and edges (bonds) \(E\). We follow [43] and define "non-bond" as a type of edge. There are \(N\) atoms and each atom has a one-hot encoding, denoting the atom type. We represent it as \(\mathbf{X}_{V}\in\mathbb{R}^{N\times F_{V}}\), where \(F_{V}\) is the total number of atom types. Similarly, the bond features are a tensor \(\mathbf{X}_{E}\in\mathbb{R}^{N\times N\times F_{E}}\), representing both the graph structure and \(F_{E}\) bond types.

Let \(\mathcal{C}=\{c_{1},c_{2},\ldots,c_{M}\}\) be a set of \(M\) numerical and categorical conditions. The task is: \(q(G\mid c_{1},c_{2},\ldots,c_{M})\propto q(G)q(c_{1},c_{2},\ldots,c_{M}\mid G)\), where \(q\) represents observed probability. We use a model parameterized by \(\theta\) for multi-conditional molecular generation \(p_{\theta}(G\mid\mathcal{C})\). The evaluation involves both distribution learning \(q(G)\)[35] and condition control \(q(c_{1},c_{2},\ldots,c_{M}\mid G)\). We follow previous work in assuming that there exist different oracle functions \(\mathcal{O}\) that can independently evaluate each conditioned property [14]: \(q(c_{1},c_{2},\ldots,c_{M}\mid G)=\prod_{i=1}^{M}\mathcal{O}_{i}(c_{i}\mid G)\). Note that the oracles are _not_ used in the training of \(p_{\theta}\).

### Diffusion Model on Graph Data

Diffusion models consist of forward and reverse diffusion processes [17]. We refer to the forward diffusion process as the diffusion process following [17]. The diffusion process \(q(G^{1:T}\mid G^{0})=\prod_{t=1}^{T}q(G^{t}\mid G^{t-1})\) corrupts molecular graph data (\(G^{0}=G\)) into noisy states \(G^{t}\). As timesteps \(T\to\infty\), \(q(G^{T})\) converges a stationary distribution \(\pi(G)\). The reverse Markov process \(p_{\theta}(G^{0:T})=q(G^{T})\prod_{t=1}^{T}p_{\theta}(G^{t-1}\mid G^{t})\), parameterized by neural networks, gradually denoises the latent states toward the desired data distribution.

Diffusion ProcessOne may perturb \(G^{t}\) in a discrete state-space to capture the structural properties of molecules [43]. Two transition matrices \(\mathbf{Q}_{V}\in\mathbb{R}^{F_{V}\times F_{V}}\) and \(\mathbf{Q}_{E}\in\mathbb{R}^{F_{E}\times F_{E}}\) are defined for nodes \(\mathbf{X}_{V}\) and edges \(\mathbf{X}_{E}\), respectively [43]. Then, each step \(q(G^{t}\mid G^{t-1},G^{0})=q(G^{t}\mid G^{t-1})\) in the diffusion process is sampled as follows.

\[\begin{cases}q(\mathbf{X}_{V}^{t}\mid\mathbf{X}_{V}^{t-1})=\mathrm{Cat}\left( \mathbf{X}_{V}^{t};\mathbf{p}=\mathbf{X}_{V}^{t-1}\mathbf{Q}_{V}^{t}\right), \\ q(\mathbf{X}_{E}^{t}\mid\mathbf{X}_{E}^{t-1})=\mathrm{Cat}\left(\mathbf{X}_{E} ^{t};\mathbf{p}=\mathbf{X}_{E}^{t-1}\mathbf{Q}_{E}^{t}\right),\end{cases}\] (1)

where \(\mathrm{Cat}(\mathbf{X};\mathbf{p})\) denotes sampling from a categorical distribution with probability \(\mathbf{p}\). We remove the subscript (\({}_{V/E}\)) when the description applies to both nodes and edges. It is assumed that the noise \(\mathbf{Q}^{i}\) (\(i\leq t\)) is independently applied to \(\mathbf{X}\) in each step \(i\), allowing us to rewrite \(q(\mathbf{X}^{t}\mid\mathbf{X}^{t-1})\) as the probability of the initial state \(q(\mathbf{X}^{t}\mid\mathbf{X}^{0})=\mathrm{Cat}\left(\mathbf{X}^{t};\mathbf{ p}=\mathbf{X}^{0}\bar{\mathbf{Q}}^{t}\right)\), where \(\bar{\mathbf{Q}}^{t}=\prod_{i\leq t}\mathbf{Q}^{i}\).

Noise SchedulingTransition matrices \(\mathbf{Q}_{V}\) and \(\mathbf{Q}_{E}\) control the noise applied to atom features and bond features, respectively. Vignac et al. [43] defined \(\pi(G)=(\mathbf{m}_{X}\in\mathbb{R}^{F_{V}},\mathbf{m}_{E}\in\mathbb{R}^{F_{E}})\) as the marginal distributions of atom types and bond types. The transition matrix at timestep \(t\) is \(\mathbf{Q}^{t}=\alpha^{t}\mathbf{I}+(1-\alpha^{t})\mathbf{1m}^{\prime}\) for atoms or bonds, where \(\mathbf{m}^{\prime}\) denotes the transposed row vector. Therefore, we have \(\bar{\mathbf{Q}}^{t}=\bar{\alpha}^{t}\mathbf{I}+(1-\bar{\alpha}^{t})\mathbf{1 m}^{\prime}\), where \(\bar{\alpha}^{t}=\prod_{\tau=1}^{t}\alpha^{\tau}\). The cosine schedule [32] is often chosen for \(\bar{\alpha}^{t}=\cos(0.5\pi(t/T+s)/(1+s))^{2}\).

Reverse ProcessWith the initial sampling \(G^{T}\sim\pi(G)\), the reverse process generates \(G^{0}\) iteratively in reversed steps \(t=T,T-1,\ldots,0\). We use a neural network to predict the probability \(p_{\theta}(\tilde{G}^{0}\mid G^{t})\) as the product over nodes and edges [1; 43]:

\[p_{\theta}(\tilde{G}^{0}\mid G^{t})=\prod_{v\in V}p_{\theta}(v^{t-1}\mid G^{t} )\prod_{e\in E}p_{\theta}(e^{t-1}\mid G^{t})\] (2)

\(p_{\theta}(\tilde{G}^{0}\mid G^{t})\) could be combined with \(q(G^{t-1}\mid G^{t},G^{0})\) to estimate the reverse distribution on the graph \(p_{\theta}(G^{t-1}\mid G^{t})\). For example, \(p_{\theta}(v^{t-1}\mid G^{t})\) is marginalized over predictions of node types \(\tilde{v}\in\tilde{\mathbf{x}}_{v}\), which applies similarly to edges:

\[p_{\theta}(v^{t-1}\mid G^{t})=\sum_{\tilde{v}\in\tilde{\mathbf{x}}_{v}}q(v^{t-1} \mid\tilde{v},G^{t})p_{\theta}(\tilde{v}\mid G^{t}).\] (3)

The neural network could be trained to minimize the negative log-likelihood [43].

\[L=\mathbb{E}_{q(G^{0})}\mathbb{E}_{q(G^{t}|G^{0})}\left[-\mathbb{E}_{\mathbf{x} \in G^{0}}\log p_{\theta}\left(\mathbf{x}\mid G^{t}\right)\right]\] (4)

## 3 Multi-Conditional Graph Diffusion Transformers

We present the denoising framework of Graph DiT in Figure 2. The condition encoder learns the representation of \(M\) conditions. The statistics of this representation like mean and variance are used to replace the ones from the molecular representations [19] (see Section 3.2). Besides, we introduce a new noise model in the diffusion process to better fit graph-structured molecules (see Section 3.1).

### Graph-Dependent Noise Models

The transition probability of a node or an edge should rely on the joint distribution of nodes and edges in the prior state. However, as an example shown in Eq. (1), current diffusion models [22; 43; 25] treat node and edge state transitions as independent, misaligning with the denoising process in Eq. (3). This difference between the sampling distributions of noise in the diffusion and reverse processes introduces unnecessary challenges to multi-conditional molecular generations.

To address this, we use a single matrix \(\mathbf{X}_{G}\in\mathbb{R}^{N\times F_{G}}\) to represent graph tokens for \(G\), with \(F_{G}=F_{V}+N\cdot F_{E}\). Token representations are created by concatenating the node feature matrix \(\mathbf{X}_{V}\) and the flattened edge connection matrix from \(\mathbf{X}_{E}\). Each row vector in \(\mathbf{X}_{G}\) contains features for both nodes and edges, representing all connections and non-connections. Hence, we could design a transition matrix \(\mathbf{Q}_{G}\) considering the joint distribution of nodes and edges. \(\mathbf{Q}_{G}\in\mathbb{R}^{F_{G}\times F_{G}}\) is constructed from four matrices \(\mathbf{Q}_{V},\mathbf{Q}_{EV}\in\mathbb{R}^{F_{E}\times F_{V}},\mathbf{Q}_{ E},\mathbf{Q}_{VE}\in\mathbb{R}^{F_{V}\times F_{E}}\), denoting the transition probability ("dependent old state" \(\rightarrow\) "target new state") node \(\rightarrow\) node; edge \(\rightarrow\) edge; node \(\rightarrow\) edge, respectively.

\[\mathbf{Q}_{G}=\begin{bmatrix}\mathbf{Q}_{V}&\mathbf{1}_{N}^{\prime}\otimes \mathbf{Q}_{VE}\\ \mathbf{1}_{N}\otimes\mathbf{Q}_{EV}&\mathbf{1}_{N\times N}\otimes\mathbf{Q}_{ E}\end{bmatrix},\] (5)

where \(\otimes\) denotes the Kronecker product, \(\mathbf{1}_{N}\), \(\mathbf{1}_{N}^{\prime}\), and \(\mathbf{1}_{N\times N}\) represent the column vector, row vector, and matrix with all 1 elements, respectively. According to Eq. (5), the first \(F_{V}\) columns in \(\mathbf{Q}_{G}\) determine the node feature transitions based on both node features (first \(F_{V}\) rows) and edge features (remaining \(N\cdot F_{E}\) rows). Conversely, the remaining \(N\cdot F_{E}\) columns determine the edge feature transitions, depending on the entire graph. We introduce a new diffusion noise model:

\[q(\mathbf{X}_{G}^{t}\mid\mathbf{X}_{G}^{t-1})=\widetilde{\mathrm{Cat}}\left( \mathbf{X}_{G}^{t};\tilde{\mathbf{p}}=\mathbf{X}_{G}^{t-1}\mathbf{Q}_{G}^{t} \right),\] (6)

where \(\tilde{\mathbf{p}}\) is the unnormalized probability and \(\widetilde{\mathrm{Cat}}\) denotes categorical sampling: The first \(F_{V}\) columns of \(\tilde{\mathbf{p}}\) are normalized to sample \(\mathbf{X}_{V}^{t}\), while the remaining \(N\cdot E\) dimensions are reshaped and normalized to sample edges \(\mathbf{X}_{E}^{t}\). These components are combined to form \(\mathbf{X}_{G}^{t}\), completing the \(\widetilde{\mathrm{Cat}}\) sampling.

Choice of \(\mathbf{Q}_{VE}\) and \(\mathbf{Q}_{EV}\)Similar to the definitions of \(\mathbf{m}_{V}\) and \(\mathbf{m}_{E}\)[43], we leverage the prior knowledge within the training data for the formulation of task-specific matrices, \(\mathbf{Q}_{EV}\) and \(\mathbf{Q}_{VE}\). We calculate co-occurrence frequencies of atom and bond types in training molecular graphs to obtain the marginal atom-bond co-occurrence probability distribution. For each bond type, each row in \(\mathbf{m}_{EV}\) represents the probability of co-occurring atom types. \(\mathbf{m}_{VE}\) is the transpose of \(\mathbf{m}_{EV}\) and has a similar meaning. Subsequently, we define \(\mathbf{Q}_{EV}=\bar{\alpha}^{t}\mathbf{I}+(1-\bar{\alpha}^{t})\mathbf{1} \mathbf{m}_{EV}^{\prime}\) and \(\mathbf{Q}_{VE}=\bar{\alpha}^{t}\mathbf{I}+(1-\bar{\alpha}^{t})\mathbf{1} \mathbf{m}_{VE}^{\prime}\).

Figure 2: Denoising framework and architectures for Graph DiT. Details are in Section 3.2.

### Denoising Models with Multi-Property Conditions

We present Graph DiT as the denoising model to generate molecules under multi-conditions \(\mathcal{C}=\{c_{1},c_{2},\ldots,c_{M}\}\) without extra predictors.

Predictor-Free GuidanceThe predictor-free reverse process \(\hat{p}_{\theta}(G^{t-1}\mid G^{t},\mathcal{C})\) aims to generate molecules with a high probability \(q(\mathcal{C}\mid G^{0})\). This could be achieved by a linear combination of the log probability for unconditional and conditional denoising [16]:

\[\hat{p}_{\theta}(G^{t-1}\mid G^{t},\mathcal{C})=\log p_{\theta}(G^{t-1}\mid G^{ t})+s\left(\log p_{\theta}(G^{t-1}\mid G^{t},\mathcal{C})-\log p_{\theta}(G^{t-1} \mid G^{t})\right),\] (7)

where \(s\) denotes the scale of conditional guidance. Unlike classifier-free guidance [16], which typically predicts noise, we directly estimate \(p_{\theta}(\tilde{G}^{0}\mid G^{t},\mathcal{C})\). We one one denoising model \(f_{\theta}(G^{t},\mathcal{C})\) for both \(p_{\theta}(\tilde{G}^{0}\mid G^{t})\) and \(p_{\theta}(\tilde{G}^{0}\mid G^{t},\mathcal{C})\). Here, \(f_{\theta}(G^{t},\mathcal{C}=\emptyset)\) computes the unconditional probability by substituting the original conditional embeddings with the null value. During training, we randomly drop the condition with a ratio, i.e., \(\mathcal{C}=\emptyset\), to learn the embedding of the null value. \(f_{\theta}(G^{t}=\mathbf{X}_{G}^{t},\mathcal{C})\) comprises two components: the condition encoder and the graph denoiser. An overview of the architecture is presented in Figure 2.

Condition EncoderWe treat the timestep \(t\) as a special condition and follow [31] to obtain a \(D\)-dimensional representation \(\mathbf{t}\) with sinusoidal encoding. For property-related numerical or categorical condition \(c_{i}\in\mathcal{C}\), we apply distinct encoding operations to get \(D\)-dimensional representation. For a categorical condition, we use the one-hot encoding. For a numerical variable, we introduce a clustering encoding method. This defines learnable centroids, assigning \(c_{i}\) to clusters, and transforming the soft assignment vector of condition values into the representation. It could be implemented using two \(\mathrm{Linear}\) layers and a \(\mathrm{Softmax}\) layer in the middle as: \(\mathrm{Linear}\left(\mathrm{Softmax}\left(\mathrm{Linear}(c_{i})\right)\right)\). Finally, we could obtain the representation of the condition as \(\mathbf{c}=\sum_{i=1}^{M}\mathrm{encode}(c_{i})\), where \(\mathrm{encode}\) is the specific encoding method based on the condition type. For numerical conditions, we evaluate our proposed clustering-based approach against alternatives like direct or interval-based encodings [28]. As noted in Section 4.4, the clustering encoding outperforms the other methods.

Graph Denoiser: Transformer LayersGiven the noisy graph at timestep \(t\), the graph tokens are first encoded into the hidden space as \(\mathbf{H}=\mathrm{Linear}(\mathbf{X}_{G}^{t})\), where \(\mathbf{H}\in\mathbb{R}^{N\times D}\). We then adapt the standard Transformer layers [42] with self-attention and multi-layer perceptrons (MLP), but replace the normalization with the adaptive layer normalization (\(\mathrm{AdaLN}\)) controlled by the representations of the conditions [19; 34]: \(\mathbf{H}=\mathrm{AdaLN}(\mathbf{H},\mathbf{c})\). For each row \(\mathbf{h}\) in \(\mathbf{H}\):

\[\mathrm{AdaLN}\left(\mathbf{h},\mathbf{c}\right)=\gamma_{\theta}(\mathbf{c}) \odot\frac{\mathbf{h}-\mu\left(\mathbf{h}\right)}{\sigma\left(\mathbf{h} \right)}+\beta_{\theta}(\mathbf{c}),\] (8)

where \(\mu(\cdot)\) and \(\sigma(\cdot)\) are mean and variance values. \(\odot\) indicates element-wise product. \(\gamma_{\theta}(\cdot)\) and \(\beta_{\theta}(\cdot)\) are neural network modules in \(f_{\theta}(\cdot)\), each of which consists of two linear layers with \(\mathrm{SiLU}\) activation [11] in the middle. We have a gated variant \(\mathrm{AdaLN}_{gate}\) for residuals:

\[\mathrm{AdaLN}_{gate}\left(\mathbf{h},\mathbf{c}\right)=\alpha_{\theta}( \mathbf{c})\odot\mathrm{AdaLN}\left(\mathbf{h},\mathbf{c}\right)\] (9)

We apply the zero initialization for the first layer of \(\gamma_{\theta}(\cdot),\beta_{\theta}(\cdot)\), and \(\alpha_{\theta}(\cdot)\)[34]. There are other options to learn the structure representation from the condition [34]: In-\(\mathrm{Context}\) conditioning adds condition representation to the structure representation at the beginning of the structure encoder, and \(\mathrm{Cross\text{-Attention}}\) calculates cross-attention between the condition and structure representation. We observe in Section 4.4 that \(\mathrm{AdaLN}\) performs best among them.

Graph Denoiser: Final MLPWe have the hidden states \(\mathbf{H}\) after the final Transformer layers, the MLP is used to predict node probabilities \(\tilde{\mathbf{X}}_{V}^{0}\) and edge probabilities \(\tilde{\mathbf{X}}_{E}^{0}\) at \(t=0\):

\[\tilde{\mathbf{X}}_{G}^{0}=\mathrm{AdaLN}(\mathrm{MLP}(\mathbf{H}),\mathbf{c}).\] (10)

We split the output \(\mathbf{X}_{G}\) into atom and bond features \(\tilde{\mathbf{X}}_{V}^{0},\tilde{\mathbf{X}}_{E}^{0}\). The first \(F_{V}\) dimensions of \(\tilde{\mathbf{X}}_{G}^{0}\) represent node type probabilities, and the remaining \(N\cdot F_{E}\) dimensions cover probabilities for \(N\) edge types associated with the node, as detailed in Section 3.1.

Generation to Molecule ConversionA common way of converting generated graphs to molecules selects only the largest connected component [43], denoted as Graph DiT-LCC in our model. For Graph DiT, we connect all components by randomly selecting atoms. It minimally alters the generated structure to more accurately reflect model performance than Graph DiT-LCC.

## 4 Experiment

**RQ1**: We validate the generative power of Graph DiT compared to baselines from molecular optimization and diffusion models in Section 4.2. **RQ2**: We study a polymer inverse design for gas separation in Section 4.3. **RQ3**: We conduct further analysis to examine Graph DiT in Section 4.4.

### Experimental Setup

We use datasets with over ten types of atoms and up to fifty nodes in a molecular graph. We include both numerical and categorical properties for drugs and materials, offering a benchmark for evaluation across diverse chemical spaces. Model performance is validated across up to nine metrics, including distribution coverage, diversity, and condition control capacity for various properties.

Datasets and Input ConditionsWe have one polymer dataset [40] for materials, featuring three **numerical** gas permeability conditions: O\({}_{2}\)Perm, CO\({}_{2}\)Perm, and N\({}_{2}\)Perm. For drug design, we create three class-balanced datasets from MoleculeNet [46]: HIV, BBBP, and BACE, each with a **categorical** property related to HIV virus replication inhibition, blood-brain barrier permeability, or human \(\beta\)-secretase 1 inhibition, respectively. We have two more **numerical** conditions for synthesizability from synthetic accessibility (SAS) and complexity scores (SCS) [12; 8].

EvaluationWe randomly split the dataset into training, validation, and testing (reference) sets in a 6:2:2 ratio. Evaluations are conducted on 10,000 generated examples with metrics [35] (1) molecular validity (Validity); (2) heavy atom type coverage (Coverage); (3) internal diversity among the generated examples (Diversity); (4) fragment-based similarity with the reference set (Similarity); (5) Frechet ChemNet Distance with the reference set (Distance) [36]; MAE between the generated and conditioned (6) synthetic accessibility score [12] (Synth.); (7)\(\sim\)(9) MAE/Accuracy for the numerical/categorical task conditions (Property). The evaluation Oracle uses random forest trained on all task-related molecules [14]. Lower MAE or higher accuracy indicates stronger model controllability.

BaselinesWe select strong and popular molecular optimization baselines from recent studies [14]: Graph-GA [20], MARS [47], JTVAE [21] with Bayesian optimization (JTVAE-BO), LSTM [6] on SMILES with Hill Climbing (LSTM-HC). We include the most recent diffusion models: GDSS[22], DiGress [43], and their conditional version with extra predictors: MOOD [25], and DiGress v2 [43]. We train multi-task predictors using the same architecture for MOOD and DiGress v2 models to provide additional guidance for generation. For molecular optimization, we formulate the condition set of each test data point as a combined goal, minimizing the sum of the normalized errors between generated and input properties. We train a random forest model for each property using the training data to optimize the molecular structure.

### RQ1: Multi-Conditional Molecular Generation

We have the observations from Table 1 and Table 2:

Chemical ValidityHigh validity may not accurately represent the model's generative performance if hard-coded rules are introduced in the algorithm. For example, GraphGA could eliminate non-valid molecules during mutation and crossover iterations to achieve perfect validity in the final evaluation. Without rule checking in the generation-to-molecule step, DiGress, GDSS, and MOOD show a marked performance decline, with validity often dropping from 0.99 to below 0.6. In contrast, Graph DiT often maintains over 0.8 validity without any rule-based processing.

\begin{table}
\begin{tabular}{l c c c c c c c c c} \hline \hline Model & \begin{tabular}{c} Validity \\ (with an effective) \\ \end{tabular} & \begin{tabular}{c} Diffusion Learning \\ \end{tabular} & \begin{tabular}{c} Diffusion Learning \\ \end{tabular} & \begin{tabular}{c} \% \\ \end{tabular} & \begin{tabular}{c} Diffusion Learning \\ \end{tabular} & \begin{tabular}{c} \% \\ \end{tabular} & \begin{tabular}{c} \% \\ \end{tabular} & \begin{tabular}{c} NoPerm \\ \end{tabular} & \begin{tabular}{c} NoPerm \\ \end{tabular} & \begin{tabular}{c} NoPerm \\ \end{tabular} & 
\begin{tabular}{c} NoPerm \\ \end{tabular} \\ \hline Graph GA & 1.0000 (N.A.) & 11/11 & 0.8828 & 0.9269 & 9.1882 & 1.3307 & 1.9840 & 2.2900 & 1.9489 & 1.8884 \\ MARS & 1.0000 (N.A.) & 11/11 & 0.8375 & 0.9283 & 7.5620 & 1.1658 & 1.5761 & 1.8327 & 1.6074 & 1.5455 \\ LSTM-HC & 0.9910 (N.A.) & 10/11 & 0.8918 & 0.7397 & 18.1562 & 1.4251 & 1.1009 & 1.2365 & 1.0772 & 1.2998 \\ JTVAE-BO & 1.0000 (N.A.) & 10/11 & 0.3766 & 0.7294 & 25.9509 & **1.0714** & 1.0781 & 1.2352 & 1.0978 & 1.1206 \\ \hline DiGress & 0.9913 (0.2362) & 1/11 & 0.9099 & 0.2724 & 22.7237 & 2.9842 & 1.7163 & 2.0630 & 1.6738 & 2.1093 \\ DiGress v2 & 0.9812 (0.3057) & 1/11 & **0.9105** & 0.2771 & 21.7311 & 2.7507 & 1.7130 & 2.0632 & 1.6648 & 2.0479 \\ GiSS & 0.9205 (0.9076) & 9/11 & 0.7510 & 0.0000 & 34.2627 & 1.3701 & 1.0271 & 1.0820 & 1.0683 & 1.1369 \\ MOOD & 0.9866 (0.9295) & 1/11 & 0.8349 & 0.0227 & 39.3981 & 1.4019 & 1.4961 & 1.7643 & 1.4748 & 1.5333 \\ \hline Graph DiT-LCC (Ours) & 0.9753 (0.8437) & 1/11 & 0.8875 & 0.9560 & **0.949** & 1.3099 & 0.8001 & 0.9562 & 0.8125 & 0.9697 \\ Graph DiT (Ours) & 0.8245 (0.8437) & 1/11 & 0.8712 & **0.9600** & **0.6443** & 1.2973 & **0.7404** & **0.8857** & **0.7550** & **0.9205** \\ \hline \hline \end{tabular}
\end{table}
Table 1: Multi-Conditional Generation of 10K Polymers: Results on the synthetic score (Synth.) and three numerical properties (gas permeability for O\({}_{2}\), N\({}_{2}\), CO\({}_{2}\)). MAE is calculated between the input conditions and the properties of the generated polymers using Oracles. Best results are **highlighted**.

**Distribution Learning** GraphGA is a simple yet effective baseline for generating in-distribution molecules, e.g., on BBBP and HIV generation datasets. Diffusion model baselines such as DiGress and MOOD could produce diverse molecules but often fail to capture the original data distribution in multi-conditional tasks. Graph DiT shows the competitive performance of diffusion models in fitting complex molecular data distributions. Using fragment-based similarity and neural network-based distance metrics [36], we achieve the best in the polymer task and rank second in the HIV small molecule task, involving up to 11 and 29 types of heavy atoms, respectively.

**Condition Controllability** LSTM-HC surpasses many baselines, achieving lower average MAE on polymer properties and higher rankings on small molecular properties. However, its control over synthetic scores in polymer tasks is relatively poor. Conversely, MARS effectively manages synthetic scores for polymers but exhibits a larger MAE in gas permeability conditions compared to other baselines. GDSS performs well in gas permeability control but underperforms Graph GA and MARS in terms of the synthetic score condition. DiGress v2 and MOOD, although equipped with the predictor guidance, still exhibit limited condition control compared to their unconditional counterparts over polymer and small molecule tasks. These baselines struggle to balance and control multiple conditions in generation. In contrast, Graph DiT significantly improves diffusion models and achieves the best multi-conditional performance in all tasks. In polymer tasks, Graph DiT reduces MAE on all gas permeability conditions, averaging +17.8% improvement over the best baseline LSTM-HC. For small molecule tasks, Graph DiT consistently ranks top-1 in condition controllability with over 0.9 accuracy in categorical conditions. Compared to Graph DiT-LCC, we observe that Graph DiT, which connects all generated graph components, shows better controllability performance due to minimal rule-based post-generation processing.

### RQ2: Polymer Inverse Design for Gas Separation

We aim to design polymers with high O\({}_{2}\) and low N\({}_{2}\) permeability, demonstrating the models' precise control over related properties. Following Robeson [38]'s definition of high-performance polymers based on the O\({}_{2}\)/N\({}_{2}\) permeability ratio, we selected 16 polymers meeting this criterion from 609 examples as our test/reference set. The remaining data is used for training and validation. Subsequently, we generated 1,000 polymers conditioned on test set labels.

\begin{table}
\begin{tabular}{l l l l l l l l l l} \hline \hline \multirow{2}{*}{Tasks} & \multirow{2}{*}{Model} & \multicolumn{2}{c}{study \(\uparrow\)} & \multicolumn{2}{c}{Distributing Learning} & \multicolumn{2}{c}{Condition Control} \\  & & \multicolumn{2}{c}{(w/o rule checking)} & \multicolumn{1}{c}{Corvene \(\uparrow\)} & \multicolumn{1}{c}{Discrete \(\uparrow\)} & \multicolumn{1}{c}{Discrete \(\downarrow\)} & \multicolumn{1}{c}{Sythure \(\downarrow\)} & \multicolumn{1}{c}{Property Acc.} & \multicolumn{1}{c}{Avg. Rank \(\downarrow\)} \\ \hline \multirow{9}{*}{\begin{tabular}{} \end{tabular} } & Graph GA & 1.0000 (N.A.) & 88 & 0.8585 & 0.9805 & 7.4104 & 0.9633 & 0.4690 & 6.5000 \\  & MARS & 1.0000 (N.A.) & 88 & 0.8318 & **0.8827** & **0.7923** & 1.0123 & 0.5184 & 5.0000 \\  & LSTM-HC & 0.9972 (N.A.) & 88 & 0.8146 & 0.7982 & 17.5685 & 0.9207 & 0.5816 & 3.0000 \\  & ITVAE-BO & 1.0000 (N.A.) & 68 & 0.6682 & 0.7281 & 0.4966 & 0.9923 & 0.4628 & 7.5000 \\  & DiGress & 0.3511 (0.2858) & 88 & 0.8862 & 0.6942 & 24.6560 & 2.0681 & 0.5061 & 8.0000 \\  & DiGress v2 & 0.3546 (0.2680) & 88 & 0.8812 & 0.7027 & 25.3270 & 2.3365 & 0.5113 & 7.5000 \\  & GDSS & 0.2879 (0.2589) & 48 & 0.8756 & 0.2708 & 46.7539 & 1.6422 & 0.5036 & 7.5000 \\  & MOOD & 0.9947 (0.4020) & 8.8 & **0.8902** & 0.2587 & 44.2394 & 1.8835 & 0.5062 & 7.0000 \\  & Graph DIT-LCC (Ours) & 0.8646 (0.3495) & 88 & 0.8240 & 0.8757 & 6.9836 & 0.4053 & 0.9050 & 2.0000 \\  & Graph DIT (Ours) & 0.8674 (0.8495) & 88 & 0.8238 & 0.8752 & 7.0456 & **0.3998** & **0.9135** & **1.0000** \\ \hline \multirow{9}{*}{\begin{tabular}{} \end{tabular} } & Graph GA & 1.0000 (N.A.) & 99 & 0.8950 & **0.9509** & **0.16495** & 1.2082 & 0.3015 & 7.5000 \\  & MARS & 1.0000 (N.A.) & 89 & 0.6367 & 0.7606 & 10.9971 & 1.2250 & 0.5189 & 6.0000 \\  & LSTM-HC & 0.9990 (N.A.) & 89 & 0.8883 & 0.8932 & 16.3904 & 0.9969 & 0.5590 & 4.0000 \\  & ITVAE-BO & 1.0000 (N.A.) & 59 & 0.7458 & 0.5821 & 33.5746 & 1.1619 & 0.4598 & 6.0000 \\  & DiGress & 0.6960 (0.4871) & 99 & 0.5098 & 0.6805 & 18.6972 & 2.3683 & 0.6356 & 6.5000 \\  & DiGress v2 & 0.6892 (0.4100) & 99 & 0.9107 & 0.6336 & 19.4498 & 2.2694 & 0.6531 & 5.0000 \\  & GDSS & 0.6218 (0.5919) & 39 & 0.8415 & 0.2672 & 39.9440 & 1.3788 & 0.5037 & 7.0000 \\  & MOOD & 0.8008 (0.5789) & 99 & **0.9273** & 0.7115 & 34.2506 & 2.0248 & 0.4903 & 8.5000 \\  & Graph DIT-LCC (Ours) & 0.8657 (0.5055) & 99 & 0.8857 & 0.9324 & 11.8587 & 0.3717 & 0.9390 & 2.0000 \\  & Graph DIT (Ours) & 0.8468 (0.8505) & 99 & 0.8856 & 0.9329 & 11.8519 & **0.3551** & **0.9417** & **1.0000** \\ \hline \multirow{9}{*}{
\begin{tabular}{} \end{tabular} } & Graph GA & 1.0000 (N.A.) & 2879 & 0.8993 & **0.9664** & **4.4418** & 0.9839 & 0.6035 & 5.0000 \\  & MARS & 1.0000 (N.A.) & 26729 & 0.8764 & 0.6517 & 2.2893 & 0.9691 & 0.6455 & 4.0000 \\  & LSTM-HC & 0.9994 (N.A.) & 1329 & 0.8091 & 0.9145 & 7.4669 & 0.9480 & 0.6736 & 0.0000 \\  & ITVAE-BO & 1.0000 (N.A.) & 329 & 0.8055 & 0.4173 & 41.9771 & 1.2359 & 0.4850 & 7.5000 \\ \cline{1-1}  & DiGress & 0.4377 (0.3643) & 22729 & 0.9194 & 0.8562 & 13.0409 & 1.9216 & 0.53135 & 7.5000 \\  & DiGress v2 & 0.0505 (0.4242) & 2429 & 0.9193 & 0.8476 & 13.3997 & 1.5934 & 0.5331 & 7.5000 \\  & GDSS & 0.6926 (0.6575) & 429 & 0.7817 & 0.1032 & 45.416 & 1.2515 & 0.4830 & 8.5000 \\  & MOOD & 0.2875 (0.2173) & 2929 & **0.9280** & 0.1361 & 32.3523 & 2.3144 & 0.5106 & 9.0000 \\ \cline{1-1}  & Graph DIT-LCC (Ours) & 0.7635 (0.7415) & 28729 & 0.8966 & 0.9535 & 5.8790 & **0.3084** & 0.9766 & **1.5000** \\ \cline{1-1}  & Graph DIT (Ours) & 0.7660 (0.7415) & 28729 & 0.8974 & 0.9875 & 6.0216 & 0.3086 & **0.9777** & **1.5000** \\ \hline \hline \In Figure 3, we present the top three polymers generated by each model for a case study with expertise. Initially, a random forest algorithm identifies the top five polymers per method based on average MAE in two gas permeability. These 25 polymers are then shuffled and evaluated by four polymer scientists, who rank them from 1 to 25 using their domain knowledge. Rankings are normalized to a **Utility Score (UtS)** ranging from 0 to 1, with higher scores indicating greater utility. The variance in UtS is converted into an **Agreement Score (AS)** for further evaluation. As shown in Figure 3, there is a high consensus among experts that the three polymers generated by Graph DiT are the most promising for successful polymer inverse design tasks. More details are in appendix D. By comparing generated examples from different models, we have further observations:

* DiGress and MOOD struggle to capture polymerization points, marked with asterisks ("*"), which is one of the most important features that distinguish polymers from small molecules. Additionally, the two methods frequently feature excessive carbon atoms and overly large cycles. These molecular configurations with significant distortion from the canonical geometry of stable compounds may lead to poor synthesizability [35, 7].
* LSTM-HC may result in too-long carbon chains with limited diversity. MARS produces examples with asymmetrical graph structures, challenging polymer synthesis [10, 2].

Figure 4: Relative Performance of Different Model Designs: A higher bar indicates better performance. We use the performance of clustering-based encoding or \(\mathrm{AdaLN}\) as the Reference Value and the current option as the Current Value. Relative performance is calculated as \(\frac{\text{Current Value}}{\text{Reference Value}}\) for Similarity and Diversity metrics, and as \(\frac{\text{Reference Value}}{\text{Current Value}}\) for other metrics.

Figure 3: Polymer Inverse Design for O\({}_{2}\)/N\({}_{2}\) Gas Separation: Feedback from four domain experts includes an average Utility Score (**UtS**) for relative usefulness and an Agreement Score (**AS**) for generated polymers, both ranging [0, 1]. Polymers are generated conditional on {SAS=3.8, SCS=4.3, O\({}_{2}\)Perm=34.0, N\({}_{2}\)Perm=5.2}. The top-3 polymers, highlighted, are all generated by Graph DiT.

* Graph DiT generates structurally diverse and symmetric polymers with two polymerization points, indicative of more valid and synthesizable polymer structures. The first two, which are polyimides, imply effective gas separation performance [24].

### RQ3: Ablation Studies and Model Analysis

Model ComponentsIn light of Table 1, we analyze **three** components that impact our model's learning in various conditions. Our assessment of relative performance is based on the ratio between our method and comparative approaches. The first component is **numerical conditional encoding**. Results in Figure 4(a) highlight the superiority of clustering encoding over direct and interval-based encoding, particularly in controlling gas permeability, despite its slightly lower diversity. The second component concerns the **neural architecture for conditions**. As shown in Figure 4(b), similar to Figure 4(a), \(\mathrm{AdaLN}\) surpasses both \(\mathrm{In\text{-}Context}\) Conditioning and \(\mathrm{Cross\text{-}Attention}\) in learning distribution with better condition controllability. The third component validates the importance of the **graph-dependent noise model** compared to separately applying noise to atoms and bonds. It also shows the improvement of the predictor-free Graph DiT over the predictor-guided DiGress v2, even without the graph-dependent noise model. More results on model controllability are in appendix E.

Oracle SelectionsWe analyze the robustness of Oracles in evaluating six task-related properties (three gas permeability and three small molecule properties) across six conditional generation tasks. Oracles are switched from Random Forest to Gaussian Process or Support Vector Machines for ranking generative model performance. Results in Table 3 show consistent rankings (Graph DiT, LSTM-HC, MARS, JTVAE-BO, MOOD, GDSS, GraphGA). It indicates that while perfectly approximating the truth properties of generated molecules is difficult, we could effectively compare the relative performance of various models. Graph DiT consistently ranked first among baselines.

## 5 Related Work

Diffusion Models for Molecules:Score-based diffusion models applied noise and denoising in continuous space [33; 22]. DiGress [43] used discrete noise as transition matrices based on marginal distributions of atom and bond types. Extra predictor models are studied to guide the generation process in DiGress and GDSS [25]. Diffusion models could also be used for molecular property prediction [27], for conformation [48] and molecule generation with 3D atomic coordinates [18; 49; 3]. We focus on molecular graph generation, considering the high computational cost of accurate 3D coordinates for larger molecules like polymers [23]. We explore predictor-free diffusion guidance, instead of the classifier guidance [9; 44], for generating molecules under categorical and numerical conditions. It can be integrated with diffusion models for atomic coordinates in future research.

Molecular Optimization:Optimization algorithms could optimize molecules towards property constraints, including genetic algorithms [20], Bayesian optimization [39; 50], REINFORCE [45], and reinforcement learning [30]. Both sequential and graph-based generative models [6; 21; 30], along with diverse sampling methods [47; 13], are used in conjunction with these algorithms to produce desirable molecules. These methods have been applied to both single-objective and multi-objective optimization, the latter by manually integrating multiple property conditions into a single one [5; 25]. Several challenges in molecular optimization methods remain underexplored, including the inadequate or unclear definition of multi-property relations when integration into a single objective [5], and the inaccessibility of the oracle function for property-oriented optimization during the training phase [14].

## 6 Conclusion

In this work, we solved inverse molecular design using properties as predictor-free diffusion guidance. The proposed Graph DiT performed diffusion based on the joint distribution of atoms and bonds in both forward and reverse processes. It introduced representation learning for multiple categorical and numerical properties and utilized a Transformer-based graph denoiser for conditional graph denoising. Results on multi-conditional generations and polymer inverse designs showed the remarkable generative capabilities of Graph DiT, making it suitable for designing promising molecules.

\begin{table}
\begin{tabular}{c c c c} \hline \hline Avg. Rank & Random Forest & Gaussian Process & Support Vector Machines \\ \hline
**Graph DiT** & **Graph DiT** & **Graph DiT** \\
2 & **LSTM-HC** & **Dediers v2** \\
3 & **MARS** & **Dediers** & **Dediers** \\
4 & **JTVAE-BO** & **LSTM-HC** & **LSTM-HC** \\
5 & **MOOD** & **MARS** & **MARS** \\
6 & **Dediers** & **JTVAE-BO** & **JTVAE-BO** \\
7 & **Dediers v2** & **MOOD** & **MOOD** \\
8 & **GDS** & **GDS** & **GDS** \\
9 & **Graph GA** & **Graph GA** & **Graph GA** \\ \hline \hline \end{tabular}
\end{table}
Table 3: Oracles for Generation Evaluation: We consider three Oracles. Generative performance is ranked on average from 1 to 9 across six properties, with various Oracles yielding similar outcomes. We **highlight** models with the same ranking sequence in different Oracle evaluation.

## Acknowledgements

This work was supported by NSF IIS-2142827, IIS-2146761, IIS-2234058, CBET-2332270, CBET-2102592, and ONR N00014-22-1-2507.

## References

* Austin et al. [2021] Jacob Austin, Daniel D Johnson, Jonathan Ho, Daniel Tarlow, and Rianne Van Den Berg. Structured denoising diffusion models in discrete state-spaces. _Advances in Neural Information Processing Systems_, 34:17981-17993, 2021.
* Balaban [1986] Alexandru T Balaban. Symmetry in chemical structures and reactions. In _Symmetry_, pages 999-1020. Elsevier, 1986.
* Bao et al. [2023] Fan Bao, Min Zhao, Zhongkai Hao, Peiyao Li, Chongxuan Li, and Jun Zhu. Equivariant energy-guided SDE for inverse molecular design. In _The Eleventh International Conference on Learning Representations_, 2023. URL https://openreview.net/forum?id=r0otLt0wYW.
* Barnett et al. [2020] J Wesley Barnett, Connor R Bilchak, Yiwen Wang, Brian C Benicewicz, Laura A Murdock, Tristan Bereau, and Sanat K Kumar. Designing exceptional gas-separation polymer membranes using machine learning. _Science advances_, 6(20):eaaz4301, 2020.
* Bilodeau et al. [2022] Camille Bilodeau, Wengong Jin, Tommi Jaakkola, Regina Barzilay, and Klavs F Jensen. Generative models for molecular discovery: Recent advances and challenges. _Wiley Interdisciplinary Reviews: Computational Molecular Science_, 12(5):e1608, 2022.
* Brown et al. [2019] Nathan Brown, Marco Fiscato, Marwin HS Segler, and Alain C Vaucher. Guacamol: benchmarking models for de novo molecular design. _Journal of chemical information and modeling_, 59(3):1096-1108, 2019.
* Bruns and Watson [2012] Robert F Bruns and Ian A Watson. Rules for identifying potentially reactive or promiscuous compounds. _Journal of medicinal chemistry_, 55(22):9763-9772, 2012.
* Coley et al. [2018] Connor W Coley, Luke Rogers, William H Green, and Klavs F Jensen. Scscore: synthetic complexity learned from a reaction corpus. _Journal of chemical information and modeling_, 58(2):252-261, 2018.
* Dhariwal and Nichol [2021] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. _Advances in Neural Information Processing Systems_, 34:8780-8794, 2021.
* Dunitz [1996] Jack D Dunitz. Symmetry arguments in chemistry. _Proceedings of the National Academy of Sciences_, 93(25):14260-14266, 1996.
* Elfwing et al. [2018] Stefan Elfwing, Eiji Uchibe, and Kenji Doya. Sigmoid-weighted linear units for neural network function approximation in reinforcement learning. _Neural networks_, 107:3-11, 2018.
* Ertl and Schuffenhauer [2009] Peter Ertl and Ansgar Schuffenhauer. Estimation of synthetic accessibility score of drug-like molecules based on molecular complexity and fragment contributions. _Journal of cheminformatics_, 1:1-11, 2009.
* Fu et al. [2021] Tianfan Fu, Wenhao Gao, Cao Xiao, Jacob Yasonik, Connor W Coley, and Jimeng Sun. Differentiable scaffolding tree for molecular optimization. _arXiv preprint arXiv:2109.10469_, 2021.
* Gao et al. [2022] Wenhao Gao, Tianfan Fu, Jimeng Sun, and Connor Coley. Sample efficiency matters: a benchmark for practical molecular optimization. _Advances in Neural Information Processing Systems_, 35:21342-21357, 2022.
* Gebauer et al. [2022] Niklas WA Gebauer, Michael Gastegger, Stefaan SP Hessmann, Klaus-Robert Muller, and Kristof T Schutt. Inverse design of 3d molecular structures with conditional generative neural networks. _Nature communications_, 13(1):973, 2022.
* Ho and Salimans [2022] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. _arXiv preprint arXiv:2207.12598_, 2022.
* Ho et al. [2020] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. _Advances in neural information processing systems_, 33:6840-6851, 2020.
* Hoogeboom et al. [2022] Emiel Hoogeboom, Victor Garcia Satorras, Clement Vignac, and Max Welling. Equivariant diffusion for molecule generation in 3d. In _International conference on machine learning_, pages 8867-8887. PMLR, 2022.

* Huang and Belongie [2017] Xun Huang and Serge Belongie. Arbitrary style transfer in real-time with adaptive instance normalization. In _Proceedings of the IEEE international conference on computer vision_, pages 1501-1510, 2017.
* Jensen [2019] Jan H Jensen. A graph-based genetic algorithm and generative model/monte carlo tree search for the exploration of chemical space. _Chemical science_, 10(12):3567-3572, 2019.
* Jin et al. [2018] Wengong Jin, Regina Barzilay, and Tommi Jaakkola. Junction tree variational autoencoder for molecular graph generation. In _International conference on machine learning_, pages 2323-2332. PMLR, 2018.
* Jo et al. [2022] Jaehyeong Jo, Seul Lee, and Sung Ju Hwang. Score-based generative modeling of graphs via the system of stochastic differential equations. In _International Conference on Machine Learning_, volume 162, pages 10362-10383. PMLR, 2022.
* Joshi and Deshmukh [2021] Soumil Y Joshi and Sanket A Deshmukh. A review of advancements in coarse-grained molecular dynamics simulations. _Molecular Simulation_, 47(10-11):786-803, 2021.
* Langsam [2018] Michael Langsam. Polyimides for gas separation. In _Polymides_, pages 697-742. CRC Press, 2018.
* Lee et al. [2023] Seul Lee, Jaehyeong Jo, and Sung Ju Hwang. Exploring chemical space with score-based out-of-distribution generation. In _International Conference on Machine Learning_, pages 18872-18892. PMLR, 2023.
* Liu et al. [2022] Gang Liu, Tong Zhao, Jiaxin Xu, Tengfei Luo, and Meng Jiang. Graph rationalization with environment-based augmentations. In _Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining_, pages 1069-1078, 2022.
* Liu et al. [2023] Gang Liu, Eric Inae, Tong Zhao, Jiaxin Xu, Tengfei Luo, and Meng Jiang. Data-centric learning from unlabeled graphs with diffusion model. In _Thirty-seventh Conference on Neural Information Processing Systems_, 2023. URL https://openreview.net/forum?id=DmakuvCJ7l.
* Liu et al. [2023] Gang Liu, Tong Zhao, Eric Inae, Tengfei Luo, and Meng Jiang. Semi-supervised graph imbalanced regression. In _29th SIGKDD Conference on Knowledge Discovery and Data Mining_, 2023.
* Ma and Luo [2020] Ruimin Ma and Tengfei Luo. Pi1m: a benchmark database for polymer informatics. _Journal of Chemical Information and Modeling_, 60(10):4684-4690, 2020.
* Mollaysa et al. [2020] Amina Mollaysa, Brooks Paige, and Alexandros Kalousis. Goal-directed generation of discrete structures with conditional generative models. _Advances in Neural Information Processing Systems_, 33:21923-21933, 2020.
* Nichol et al. [2021] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. _arXiv preprint arXiv:2112.10741_, 2021.
* Nichol and Dhariwal [2021] Alexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models. In _International Conference on Machine Learning_, pages 8162-8171. PMLR, 2021.
* Niu et al. [2020] Chenhao Niu, Yang Song, Jiaming Song, Shengjia Zhao, Aditya Grover, and Stefano Ermon. Permutation invariant graph generation via score-based generative modeling. In _International Conference on Artificial Intelligence and Statistics_, pages 4474-4484. PMLR, 2020.
* Peebles and Xie [2023] William Peebles and Saining Xie. Scalable diffusion models with transformers. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 4195-4205, 2023.
* Polykovskiy et al. [2020] Daniil Polykovskiy, Alexander Zhebrak, Benjamin Sanchez-Lengeling, Sergey Golovanov, Oktai Tatonov, Stanislav Belyaev, Rauf Kurbanov, Aleksey Artamonov, Vladimir Aladinskiy, Mark Veselov, et al. Molecular sets (moses): a benchmarking platform for molecular generation models. _Frontiers in pharmacology_, 11:565644, 2020.
* Preuer et al. [2018] Kristina Preuer, Philipp Renz, Thomas Unterthiner, Sepp Hochreiter, and Gunter Klambauer. Frechet chemnet distance: a metric for generative models for molecules in drug discovery. _Journal of chemical information and modeling_, 58(9):1736-1741, 2018.
* Renz et al. [2019] Philipp Renz, Dries Van Rompaey, Jorg Kurt Wegner, Sepp Hochreiter, and Gunter Klambauer. On failure modes in molecule generation and optimization. _Drug Discovery Today: Technologies_, 32:55-63, 2019.

* [38] Lloyd M Robeson. The upper bound revisited. _Journal of membrane science_, 320(1-2):390-400, 2008.
* [39] Bobak Shahriari, Kevin Swersky, Ziyu Wang, Ryan P Adams, and Nando De Freitas. Taking the human out of the loop: A review of bayesian optimization. _Proceedings of the IEEE_, 104(1):148-175, 2015.
* [40] A Thornton, L Robeson, B Freeman, and D Uhlmann. Polymer gas separation membrane database, 2012.
* [41] Austin Tripp and Jose Miguel Hernandez-Lobato. Genetic algorithms are strong baselines for molecule generation. _arXiv preprint arXiv:2310.09267_, 2023.
* [42] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. _Advances in neural information processing systems_, 30, 2017.
* [43] Clement Vignac, Igor Krawczuk, Antoine Siraudin, Bohan Wang, Volkan Cevher, and Pascal Frossard. Digress: Discrete denoising diffusion for graph generation. _arXiv preprint arXiv:2209.14734_, 2022.
* [44] Tomer Weiss, Eduardo Mayo Yanes, Sabyasachi Chakraborty, Luca Cosmo, Alex M Bronstein, and Renana Gershoni-Poranne. Guided diffusion for inverse molecular design. _Nature Computational Science_, pages 1-10, 2023.
* [45] Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. _Machine learning_, 8:229-256, 1992.
* [46] Zhenqin Wu, Bharath Ramsundar, Evan N Feinberg, Joseph Gomes, Caleb Geniesse, Aneesh S Pappu, Karl Leswing, and Vijay Pande. Moleculenet: a benchmark for molecular machine learning. _Chemical science_, 9(2):513-530, 2018.
* [47] Yutong Xie, Chence Shi, Hao Zhou, Yuwei Yang, Weinan Zhang, Yong Yu, and Lei Li. Mars: Markov molecular sampling for multi-objective drug discovery. _arXiv preprint arXiv:2103.10432_, 2021.
* [48] Minkai Xu, Lantao Yu, Yang Song, Chence Shi, Stefano Ermon, and Jian Tang. Geodiff: A geometric diffusion model for molecular conformation generation. In _International Conference on Learning Representations_, 2022. URL https://openreview.net/forum?id=PzcvxEMzvQC.
* [49] Minkai Xu, Alexander S Powers, Ron O Dror, Stefano Ermon, and Jure Leskovec. Geometric latent diffusion models for 3d molecule generation. In _International Conference on Machine Learning_, pages 38592-38610. PMLR, 2023.
* [50] Yiheng Zhu, Jialu Wu, Chaowen Hu, Jiahuan Yan, Tingjun Hou, Jian Wu, et al. Sample-efficient multi-objective molecular optimization with gflownets. _Advances in Neural Information Processing Systems_, 36, 2024.

## Appendix A Details on the Denoising Model Component

### Numerical Condition Encoding

We explore several approaches for encoding numerical conditions. In addition to the clustering-based method, we consider:

1. The direct encoding approach, which employs a linear layer to map a continuous number into a high-dimensional space.
2. The interval-based approach, as described in [28], divides the label space into \(N_{\text{Interval}}\) intervals. It then converts the number into an interval index, allowing us to apply one-hot encoding for the number.

### Neural Architecture for Conditions

Besides the \(\operatorname{AdaLN}\), there are two more options to integrate condition representation into molecular graph representations [34]:

1. The \(\operatorname{In-Context}\) conditioning approach adds the condition representation \(\mathbf{c}\) to each row of the molecular graph representation \(\mathbf{H}\) after mapping the \(\mathbf{X}_{G}^{t}\) into \(\mathbf{H}\) using the linear layer in the structure encoder.
2. The \(\operatorname{Cross-Attention}\) approach concatenates the timestep encoding vector with the condition representation from synthesis scores or task-related properties into a two-length sequence. In each Transformer encoder layer, this is followed by a cross-attention layer at the end of the standard multi-head self-attention layer.

## Appendix B Details on Datasets and Evaluation Methods

All experiments can be run on a single A6000 GPU card.

### Datasets and Task Conditions

As presented in Table 4, we collect popular datasets in prediction tasks for more challenging molecular generation tasks. We include a polymer dataset [40, 26] for material design. It consists of conditions of O\({}_{2}\), CO\({}_{2}\), and N\({}_{2}\), which study the numerical gas permeability for oxygen, carbon dioxide, and nitrogen, respectively. Additionally, we also study the generative performance of different models separately on the polymer data with O\({}_{2}\), CO\({}_{2}\), or N\({}_{2}\), as illustrated in Table 4. We also create three class-balanced molecule datasets from [46] for drug design: HIV, BBBP, and BACE, which study categorical properties related to the inhibition of HIV virus replication, blood-brain barrier permeability, and inhibition of human \(\beta\)-secretase 1, respectively. We aim to generate synthesizable

\begin{table}
\begin{tabular}{l c c c c c c c c c c} \hline \hline \multirow{2}{*}{Datasets} & \# Molecule & \# Heavy Atom & Min & Max & Avg. & Min & Max & Avg. & \# Input Numerical & \# Input Categorical \\  & (Train/Validation/Test) & in Training & \# Atoms & \# Atoms & \# Atoms & \# Bonds & \# Bonds & \# Bonds & Task Conditions & Task Conditions \\ \hline Gas Perm & 553 (331/11/111) & 11 & 3 & 48 & 27.97 & 3 & 56 & 32.67 & 5 & 0 \\ BACE & 1332 (7986/26726) & 8 & 10 & 50 & 33.67 & 10 & 54 & 36.44 & 2 & 1 \\ BBBP & 872 (522/1575) & 9 & 3 & 50 & 24.38 & 2 & 55 & 26.26 & 2 & 1 \\ HIV & 2372 (1422/457457) & 29 & 6 & 50 & 25.35 & 5 & 60 & 27.28 & 2 & 1 \\ O\({}_{3}\)N\({}_{4}\) & 609 (474/119/16) & 11 & 3 & 48 & 27.90 & 3 & 56 & 32.63 & 4 & 0 \\ \hline O\({}_{2}\)Perm only & 629 (377/126/126) & 11 & 2 & 48 & 27.42 & 3 & 56 & 32.08 & 3 & 0 \\ Co\({}_{3}\)Perm only & 584 (309/1777) & 11 & 2 & 48 & 27.59 & 3 & 56 & 32.23 & 3 & 0 \\ N\({}_{3}\)Perm only & 616 (369/123/124) & 11 & 2 & 48 & 27.96 & 3 & 56 & 32.70 & 3 & 0 \\ \hline \hline \end{tabular}
\end{table}
Table 4: Dataset information for all multi-conditional generation and inverse polymer design tasks. O\({}_{2}\)/CO\({}_{2}\)/N\({}_{2}\)Perm only denotes the data statistics considering only one permeability and the generation results are presented in Table 5. The number of task conditions shown in the table does not include the timestep condition in the diffusion model.

molecules. Therefore, we add two numerical conditions for synthetic complexity scores [12; 8] for each of the tasks. For the gas separation polymer design task, we consider joint conditions for O\({}_{2}\) and N\({}_{2}\) and measure the selectivity O\({}_{2}\)/N\({}_{2}\) as the ratio between two gas permeability scores. All polymer gas permeabilities are scaled in the **log space** following previous work [29]. We focus on experiments for polymers and molecules within 50 nodes.

### Evaluation and Metrics

We randomly split the dataset into training, validation, and testing (reference) sets in a 6:2:2 ratio. We investigate more than eight metrics to systematically evaluate the generation performance. First, we assess generation validity (Validity). Second, we evaluate the distribution learning capacity of different models by measuring heavy atom type coverage (Coverage), internal diversity among the generated examples using Tanimoto similarity (Diversity), fragment-based similarity with the reference set (Similarity), and the Frechet ChemNet Distance with the reference set (Distance). Third, we evaluate the model's controllability by measuring the mean absolute error (MAE) between the generated condition score and the actual condition scores if the condition is numerical; otherwise, we measure the accuracy score. We follow previous work [14] to use the random forest trained on all the available data as the Oracle evaluation function. For molecular optimization algorithms, we train random forest predictors on the training set for conditional generation. For predictor-guided diffusion models, DiGress v2 and MOOD, we use the same architecture as their denoising models to train predictors for diffusion guidance. Given the conditions in the test set, we report the generation performance by generating 10,000 examples for the six multi-conditional generation tasks and 1,000 examples for the polymer inverse design problem focused on selectivity.

### Datasets and Tasks in Figure 1

Using the same dataset from the O\({}_{2}\)/N\({}_{2}\) polymer inverse design task, we keep 100 polymers to provide condition sets for testing and split the rest into training and validation sets in a 0.65:0.35 ratio. We apply our proposed Graph DiT for both single-conditional and multi-conditional approaches, focusing on three properties: (1) Synth. score for synthesizability [12], (2) O\({}_{2}\) permeability, and (3) N\({}_{2}\) permeability. The single-conditional approach generates 30 polymers per condition for each test data point, totaling 9,000 polymers. In contrast, the multi-conditional approach generates 30 polymers for each set of conditions per test data point, resulting in 3,000 polymers. We rank these polymers based on the mean absolute error between the generated properties (evaluated by a random forest model trained on all the data to simulate the Oracle function) and the conditional property. For each test data point, we also rank the best multi-conditional polymer in different single-conditional

Figure 5: Histogram of Generated Distribution for Atom and Bond Types in Different Models. Results are calculated based on Table 1 for the polymer gas permeability tasks. We observe that the atom and bond type distributions from our Graph DiT’s generated molecules are closer to those of the training data than other diffusion models. It indicates that Graph DiT has better capacity for learning molecular distributions.

[MISSING_PAGE_EMPTY:15]

observe their competitive performance in multi-conditional settings, characterized by high validity, good atom type coverage, and distribution similarity in Tables 1, 2 and 5. MARS [47] and JTVAE [21] perform well for controlling the synthetic accessibility score [12] but are less effective at controlling specific task properties like gas permeability, BACE, BBBP, and HIV. For example, in generation tasks with the categorical task condition, the generated examples only achieve around 50% accuracy in hitting the input condition.

\begin{table}
\begin{tabular}{l l l l l l l l l l l l} \hline \hline \multirow{2}{*}{Model} & \multicolumn{2}{c}{Validity \(\uparrow\)} & \multicolumn{4}{c}{Distribution Learning} & \multicolumn{4}{c}{Condition Control} \\  & w/o rule checking & \multicolumn{1}{c}{Coverage \(\uparrow\)} & \multicolumn{1}{c}{Diversity \(\uparrow\)} & \multicolumn{1}{c}{Simality \(\uparrow\)} & \multicolumn{1}{c}{Distance \(\downarrow\)} & \multicolumn{1}{c}{Synth. \(\downarrow\)} & \multicolumn{1}{c}{O\({}_{2}\)} & \multicolumn{1}{c}{N\({}_{1}\)\(\downarrow\)} & \multicolumn{1}{c}{Avg MAE \(\downarrow\)} & \multicolumn{1}{c}{\# UB \(\uparrow\)} \\ \hline Graph GA & 1,000 (N.A.) & 10/11 & 0.8848 & 0.3734 & 29:960 & 1.6545 & 1.8720 & 2.1984 & 8.7726 & 57 \\ MARS & 1,000 (N.A.) & 11/11 & 0.7865 & 0.1922 & 32:5679 & 1.4909 & 1.7940 & 2.2170 & 9.3112 & 51 \\ LSTM & 0.9910 (N.A.) & 10/11 & 0.8940 & 0.1758 & 37:156 & **1.3853** & 1.5066 & 1.8791 & 10.5529 & 45 \\ JTVAE-BO & 1,000 (N.A.) & 7/11 & 0.7849 & 0.2541 & 33.6430 & 2.0723 & 1.7653 & 2.1998 & 9.7722 & 33 \\ DGress v2 & 0.9930 (0.3126) & 9/11 & 0.5019 & 0.1156 & 0.37516 & 1.6892 & 1.6861 & 1.3375 & 8.6661 & 69 \\ DGress v2 & 0.9940 (0.1760) & 11/11 & **0.9075** & 0.2793 & 26:239 & 2.1370 & 1.8474 & 3.4707 & 85 \\ GDSS & 0.9910 (0.9180) & 1/11 & 0.8210 & 0.0000 & 41.7499 & 2.3508 & 1.4097 & 1.8328 & 11.7813 & 56 \\ MOOD & 0.9960 (0.5640) & 9/11 & 0.8803 & 0.0000 & 46.6905 & 1.5963 & 1.3921 & 1.7360 & 12.8551 & 81 \\ Graph DT-LC & 0.975 (0.7170) & 10/11 & 0.8966 & **0.6401** & **26.1647** & 1.5119 & 0.8388 & 0.9035 & 3.2541 & **90** \\ Graph DT (Ours) & 0.78000 (0.7170) & 11/11 & 0.8838 & 0.6028 & 26:3738 & 1.4081 & **0.746** & **0.8213** & **2.9770** & 68 \\ \hline \hline \end{tabular}
\end{table}
Table 6: Complete results on 1,000 generated polymers for the inverse O\({}_{2}\)/N\({}_{2}\) gas separation polymer design. # UB is the count of generated polymers successfully identified (by Oracle functions) as upper bound instances defined by Robeson [38].

\begin{table}
\begin{tabular}{l l l l l l l l l l l} \hline \hline \multirow{2}{*}{Tasks} & \multirow{2}{*}{Model} & \multicolumn{2}{c}{Validity \(\uparrow\)} & \multicolumn{4}{c}{Distribution Learning} & \multicolumn{4}{c}{Condition Control} \\  & & (w/o rule checking) & \multicolumn{1}{c}{Coverage \(\uparrow\)} & \multicolumn{1}{c}{Diversity \(\uparrow\)} & \multicolumn{1}{c}{Simality \(\uparrow\)} & \multicolumn{1}{c}{Distance \(\downarrow\)} & Synth. \(\downarrow\) & \multicolumn{1}{c}{Property \(\downarrow\)} & \multicolumn{1}{c}{Avg. \(\downarrow\)} \\ \hline \multirow{8}{*}{Datasets} & Graph GA & 1,000 (N.A.) & 11/11 & 0.8885 & 0.9180 & 8.3925 & 1.3254 & 1.8962 & 4.4521 \\  & MARS & 1,000 (N.A.) & 10/11 & 0.2263 & 0.5170 & 26.6354 & **0.8502** & 1.8853 & 3.6472 \\  & LSTM-HC & 0.9896 (N.A.) & 10/11 & 0.8898 & 0.8015 & 17.5424 & 1.2727 & 1.1323 & 3.8278 \\  & JTVAE-BO & 1,0000 (N.A.) & 8/11 & 0.7672 & 0.8895 & 21.3698 & 0.9703 & 1.3257 & 3.2971 \\  & DiGress & 0.9934 (0.3756) & 11/11 & 0.9156 & 0.2648 & 19.9364 & 2.5093 & 1.6424 & 5.2492 \\  & DiGress v2 & 0.9842 (0.4237) & 11/11 & **0.9204** & 0.2311 & 20.4500 & 2.3444 & 1.6445 & 5.1255 \\  & GDSS & 0.9910 (0.4482) & 1/11 & 0.8891 & 0.0058 & 36.5735 & 1.6074 & 1.4803 & 4.3219 \\  & MOOD & 0.9952 (0.4764) & 9/11 & 0.8898 & 0.0072 & 36.0428 & 1.5089 & 1.4595 & 4.2277 \\  & Graph DT-LC & 0.9826 (0.8974) & 11/11 & 0.8941 & 0.9662 & 5.8949 & 1.1302 & 0.8341 & 3.1345 \\  & Graph DiT (Ours) & 0.8242 (0.8974) & 11/11 & 0.8788 & **0.9685** & **0.4287** & 1.0672 & **0.7843** & **2.9442** \\ \hline \multirow{8}{*}{Datasets} & Graph GA & 1,000 (N.A.) & 11/11 & 0.8806 & 0.8760 & 9.4945 & 1.2593 & 2.3122 & 4.7050 \\  & MARS & 1,0000 (N.A.) & 10/11 & 0.1118 & 0.3269 & 33.6684 & 2.1089 & 2.3316 & 5.8333 \\  & LSTM-HC & 0.9901 (N.A.) & 10/11 & 0.8924 & 0.7804 & 14.7629 & 1.4530 & 1.2798 & 4.1070 \\  & JTVAE-BO & 1,0000 (N.A.) & 10/11 & 0.7741 & 0.7264 & 22.5093 & **0.9414** & 1.2874 & 3.2911 \\  & DiGress v2 & 0.9798 (0.3326) & 11/11 & 0.9150 & 0.2670 & 21.1077 & 2.7562 & 2.0242 & 5.9103 \\  & DiGress v2 & 0.9801 (0.3759) & 11/11 & **0.9180** & 0.1842 & 20.7820 & 2.4734 & 1.9538 & 5.5614 \\  & ODS & 0.9941 (0.8000) & 3/11 & 0.8543 & 0.0030 & 33.3185 & 1.5277 & 1.5886 & 4.3365 \\  & MOOD & 0.9980 (0.4453) & 11/11 & 0.8857 & 0.0028 & 34.9385 & 1.5087 & 1.7018 & 4.3261 \\  & Graph DT-LC (Ours) & 0.9803 (0.9054) & 11/11 & 0.8894 & 0.9670 & 5.9049 & 1.9088 & 0.9721 & 3.3105 \\  & Graph DiT (Ours) &

### Discussion on Training Dynamics

In Figure 8, we illustrate changes in various indicators on the validation set during model training. We note an increase in generated validity and similarity to the validation reference set, along with a decrease in distance to the reference set and errors between generated properties and conditions, indicating gradual improvement in conditional generation over epochs. However, a trade-off between distribution fitting and internal diversity is observed in our current model, suggesting that further work on enhancing generation diversity could be promising.

Figure 7: Distribution of training (grey-colored) and generated (orange-colored) molecules. The generated distribution in Figure 7(i) is from Graph DiT, and the visualization shows that the generated molecular data points fit the training distribution well, with reasonable interpolation and extrapolation in the training data space.

### Discussion on Uniqueness, and Novelty

We evaluate the model's performance on Novelty and Uniqueness. Unlike unconditional generation, multi-conditional generation involves generating multiple possible molecules under the same condition. Therefore, we compute these metrics across different sets of conditions rather than for generated molecules under the same conditions. Results are presented in Table 7.

Graph DiT demonstrates reasonable performance on these metrics. However, higher Novelty and Uniqueness values do not necessarily indicate better performance, as they may not reflect the model's ability to design satisfactory molecules with desirable properties. Moreover, these values risk leading to misleading conclusions. For instance, AddCarbon achieves nearly perfect scores (99.94% Novelty

Figure 8: Change of indicators on the validation set during model training. The generated validity and similarity to the validation reference set have increased, accompanied by a decrease in distance to the reference set and errors between generated properties and conditions. We also observe a trade-off between distribution fitting and internal diversity over epochs.

and 99.86% Uniqueness) according to [37; 41], yet it randomly adds carbon atoms to existing molecules, resulting in new molecules that are not practically useful [37].

## Appendix D Details on Polymer Inverse Design

We aim to design polymers with high O\({}_{2}\) and low N\({}_{2}\) permeability, showing refined control of models over these properties. This is reflected in the selectivity, defined as the O\({}_{2}\)/N\({}_{2}\) permeability ratio. Robeson [38] has identified an inherent trade-off between gas permeability and selectivity, known as the upper-bound. Ideally, high-performance polymers should fall in the above-the-bound region, demonstrating an effective combination of permeability and selectivity. We have 609 polymers with annotated permeability values for both gases. 16 above-the-bound polymers are included in the test/reference set and excluded from the training set. We generate 1,000 polymers conditional on test set labels.

### Survey Setup on Generated Polymers

We aim to gather expert evaluations on the generation performance of various methods. We conduct a study using a test data point from the O\({}_{2}\)/N\({}_{2}\) gas separation inverse design task, taking its properties as conditions. The structure of the selected data point is presented in Figure 9. We display 25 generated polymers, each with its properties, alongside three real polymers from the training dataset as references. The first real polymer serves as the test reference, while the other two, similar in properties to the first, also aid experts in assessing the generated polymers. The properties of these generated polymers are predicted using a well-trained random forest model. Experts are asked to rank the generated polymers from 1 to 25, considering: (1) Structures of real polymers with desirable properties; (2) Predicted properties of generated polymers, displayed beneath each visualization. Here, a rank of 1 represents the best example as per domain knowledge, while 25 is the least favorable. Rankings are then converted to utility scores (UtS) ranging from 0 to 1 using \(\frac{1}{\text{ranking}}\), allowing us to quantify the relative performance of different generation methods. The agreement score (AS) could be obtained by \(\exp{(-25\times\text{Variance}(\text{UtS}_{1},\text{UtS}_{2},\text{UtS}_{3}, \text{UtS}_{4}))}\), where UtS\({}_{i}\) denotes the utility score from the \(i\)-th domain expert. (3) Finally, we select top-3 polymers for each generative models and present them in Figure 3.

### Results on Inverse Design

We present the inverse design results of all 1,000 generated polymers in validity, distribution learning, and condition control in Table 6. # **UB** is the count of generated polymers successfully identified as upper bound instances. Higher # UB indicates that Graph DiT has a higher likelihood of generating candidates for excellent O\({}_{2}\) and N\({}_{2}\) gas separation. The smallest MAE across most properties and a 9.9% average MAE improvement over baselines highlight Graph DiT's superior control in generating examples closely aligned with multiple conditions.

\begin{table}
\begin{tabular}{l c c c c c c c c} \hline \hline
**Metric** & **Graph GA** & **MARS** & **LSTM-HC** & **JTVAE-BO** & **Digress** & **DiGress v2** & **GDSS** & **MOOD** & **Graph DiT** \\ \hline Novelty & 0.9950 & 1.0000 & 0.9507 & 1.0000 & 0.9908 & 0.9799 & 0.9190 & 0.9867 & 0.9702 \\ Uniqueness & 1.0000 & 0.7500 & 0.9550 & 0.6757 & 1.0000 & 0.9730 & 0.1622 & 0.9820 & 0.8829 \\ \hline \hline \end{tabular}
\end{table}
Table 7: Comparison of Novelty and Uniqueness across different conditions

Figure 9: The reference polymer structure in the case study has conditions {SAS=3.8, SCS=4.3, O\({}_{2}\)Perm=34.0, N\({}_{2}\)Perm=5.2}.

## Appendix E Details and More Results on Model Analysis

### Case Studies on Generation Controllability

We conduct a new case study on the \(O_{2}/N_{2}\) polymer dataset, studying the controllability on three properties synthesizability score, \(O_{2}\) and \(N_{2}\) properties with varied \(N_{2}\) property values. We select a polymer example from the test set and vary its \(N_{2}\) while keeping other properties fixed. The \(N_{2}\) property from the test polymer is 213.75, and we vary it from 0 to 1000. We sample 50 values within this range and generate 100 polymer graphs conditioned on multiple properties with each sampled value. We evaluate various metrics, including the chemical validity of the generated polymers.

We visualize results in Figure 10. We consistently observe that validity and controllability performance improve as the values approach 213.75, derived from a real test polymer. Conversely, performance deteriorates when the sampled \(N_{2}\) values are closer to the extremes of the sampling range (0 or 1000). This observation underscores the interdependency between conditions, where less frequent combinations of different properties may be more challenging to learn. Moreover, the model performs well across a relatively large range from 0 to 1000 in terms of validity, \(O_{2}\), and synthesizability score control. This demonstrates good generalization of the proposed method in capturing complex condition interdependencies.

Figure 10: Analysis of Model Controllability when Varying \(N_{2}\) Values: The true \(N_{2}\) value from the test set is 213.75. We note that the controllability performance (i.e., MAE value) for \(N_{2}\) and \(O_{2}\) is measured in log space.

[MISSING_PAGE_FAIL:21]

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: Discussions on the proposed method and empirical support can be found in sections 1, 3, and 4. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: Discussions on generation diversity and Oracle evaluations are found in Sections 4.2 and 4.4. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?Answer: [Yes]

Justification: The paper focuses on empirical studies.

Guidelines:

* The answer NA means that the paper does not include theoretical results.
* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: Details are in Section 4, appendix, and supplementary code. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code**Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: Code is provided in the supplementary materials. Data and code will be on Github after publication. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: Details are in Section 4.1. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: Improvement percentages (\(>\)10%) are discussed in Section 4.2. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).

* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: Discussions are in Appendix B. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: The research conforms the NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: The work has positive societal impacts on designing molecules for drugs and materials, as discussed in Sections 1 and 6. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.

* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: The work does not present issues of high-risk misuse. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licensees for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: All data sources and baseline models are open-sourced. They have been properly credited and mentioned, as outlined in Section 4.1. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.

* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [noitemsep] * Justification: No new assets. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [noitemsep] * Justification: The experiment results are verified through a survey conducted with a small group of domain experts. However, no crowdsourcing experiments were conducted. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [noitemsep] * Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.

* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.