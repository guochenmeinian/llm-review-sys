# Conformal Prediction for Class-wise Coverage

via Augmented Label Rank Calibration

 Yuanjie Shi

Washington State University

&Subhankar Ghosh

Washington State University

&Taha Belkhouja

Washington State University

&Janardhan Rao Doppa

Washington State University

&Yan Yan

Washington State University

###### Abstract

Conformal prediction (CP) is an emerging uncertainty quantification framework that allows us to construct a prediction set to cover the true label with a pre-specified marginal or conditional probability. Although the valid coverage guarantee has been extensively studied for classification problems, CP often produces large prediction sets which may not be practically useful. This issue is exacerbated for the setting of class-conditional coverage on classification tasks with many and/or imbalanced classes. This paper proposes the Rank Calibrated Class-conditional CP (RC3P) algorithm to reduce the prediction set sizes to achieve class-conditional coverage, where the valid coverage holds for each class. In contrast to the standard class-conditional CP (CCP) method that uniformly thresholds the class-wise conformity score for each class, the augmented label rank calibration step allows RC3P to selectively iterate this class-wise thresholding subroutine only for a subset of classes whose class-wise top-\(k\) error is small. We prove that agnostic to the classifier and data distribution, RC3P achieves class-wise coverage. We also show that RC3P reduces the size of prediction sets compared to the CCP method. Comprehensive experiments on multiple real-world datasets demonstrate that RC3P achieves class-wise coverage and \(26.25\%\downarrow\) reduction in prediction set sizes on average.

## 1 Introduction

Safe deployment of machine learning (ML) models in high stakes applications such as medical diagnosis requires theoretically-sound uncertainty estimates. Conformal prediction (CP) [60] is an emerging uncertainty quantification framework that constructs a prediction set of candidate output values such that the true output is present with a pre-specified level (e.g., \(\geq 90\%\)) of the marginal or conditional probability [65, 19].

A promising property of CP is the model-agnostic and distribution-free _coverage validity_ under certain notions [20]. For example, marginal coverage is the commonly studied validity notion [47, 1, 65], while conditional coverage is a stronger notion. There is a general taxonomy to group data (i.e., input-output pairs) into categories and to study the valid coverage for each group (i.e., the group-wise validity) [61, 60]. This paper focuses on the specific notion of class-conditional coverage that guarantees coverage for each class individually, which is important for classification tasks with many and/or imbalanced classes (e.g., medical applications) [39, 56, 38].

In addition to the coverage validity, _predictive efficiency_ is another important criterion for CP [20, 59], which refers to the size of the prediction sets. Both coverage validity and predictive efficiency are used together to measure the performance of CP methods [1, 45, 47, 15, 22, 18]. Since the two measures are competing [1], our goal is to guarantee the coverage validity with high predictive efficiency, i.e., small prediction sets [20, 47, 18]. Some studies improved the predictive efficiency under themarginal coverage setting using new conformity score function [1] and new calibration procedures [19; 18; 26; 21]. However, it is not known if these methods will benefit the predictive efficiency for the class-conditional coverage setting. A very recent work [15] proposed the cluster CP method to achieve _approximate_ class-conditional coverage. It empirically improves predictive efficiency over the baseline class-wise CP method (i.e., each class is one cluster) [58], but the approximation guarantee for class-wise coverage is _model-dependent_ (i.e., requires certain assumptions on the model). The main question of this paper is: _how can we develop a model-agnostic CP algorithm that guarantees the class-wise coverage with improved predictive efficiency (i.e., small prediction sets)?_

To answer this question, we propose a novel approach referred to as _Rank Calibrated Class-conditional CP (RC3P)_ that guarantees the class-wise coverage with small expected prediction sets. The class-conditional coverage validity of RC3P is agnostic to the data distribution and the underlying ML model, while the improved predictive efficiency depends on very mild conditions of the given trained classifier. The main ingredient behind the RC3P method is the label rank calibration strategy augmented with the standard conformal score calibration from the class-wise CP (CCP) [58; 2].

The CCP method finds the class-wise quantiles of non-conformity scores on calibration data. To produce the prediction set for a new test input \(X_{\text{test}}\), it pairs \(X_{\text{test}}\) with each candidate class label \(y\) and includes the label \(y\) if the non-conformity score of the pair \((X_{\text{test}},y)\) is less than or equal to the corresponding class-wise quantile associated with \(y\). Thus, CCP constructs the prediction set by uniformly iterating over _all_ candidate labels. In contrast, the label rank calibration allows RC3P to selectively iterate this class-wise thresholding subroutine only if the label \(y\) is ranked by the classifier \(f(X_{\text{test}})\) (e.g., \(f(\cdot)\) denotes the softmax prediction) in the top \(k_{y}\) candidates, where the value of \(k_{y}\) is calibrated for each label \(y\) individually according to the class-wise top-\(k_{y}\) error. In other words, given \(X_{\text{test}}\), RC3P enables standard class-wise conformal thresholding for the sufficiently certain class labels only (as opposed to all labels). Our theory shows that the class-wise coverage provided by RC3P is agnostic to the data distribution and the underlying ML model. Moreover, under a very mild condition, RC3P guarantees improved predictive efficiency over the baseline CCP method.

**Contributions.** The main contributions of this paper are:

* We design a novel algorithm RC3P that augments the label rank calibration strategy to the standard conformal score calibration step. To produce prediction sets for new inputs, it selectively performs class-wise conformal thresholding only on a subset of classes based on their corresponding calibrated label ranks.
* We develop theoretical analysis to show that RC3P guarantees class-wise coverage, which is agnostic to the data distribution and trained classifier. Moreover, it provably produces smaller average prediction sets over the baseline CCP method [58].
* We perform extensive experiments on multiple imbalanced classification datasets and show that RC3P achieves the class-wise coverage with significantly improved predictive efficiency over the existing class-conditional CP baselines (\(26.25\%\) reduction in the prediction size on average on all four datasets or \(35\%\) reduction excluding CIFAR-10). The code is available at https://github.com/YuanjieSh/RC3P.

## 2 Related Work

Precise uncertainty quantification of machine learning based predictions is necessary in high-stakes decision-making applications. It is especially challenging for imbalanced classification tasks. Although many imbalanced classification learning algorithms [10; 25] are proposed, e.g., re-sampling [11; 42; 33; 54; 63] and re-weighting [28; 40], they do not provide uncertainty quantification with rigorous guarantees over predictions for each class.

Conformal prediction [62; 60] is a model-agnostic and distribution-free framework for uncertainty quantification by producing prediction sets that cover the true output with a pre-specified probability, which means CP could provide valid coverage guarantee with any underlying model and data distribution [32; 52; 16]. Many CP algorithms are proposed for regression [35; 46; 23; 17], classification [45; 1; 64; 37], structured prediction [6; 3; 13; 30], online learning [24; 7], and covariate shift [31; 53; 5] settings. _Coverage validity_ and _predictive efficiency_ are two common and competing desiderata for CP methods [1]. Thus, small prediction sets are favorable whenever the coverage validity is guaranteed [20; 47; 18], e.g., human and machine learning collaborative systems[39; 56; 38]. Recent work1 improved the predictive efficiency for marginal coverage by designing new conformity score [1] and calibration procedures [19; 18; 26; 21]. These methods can be combined with class-conditional CP methods including RC3P as we demonstrate in our experiments, but the effect on predictive efficiency is not clear.

Footnote 1: A concurrent work by Huang and colleagues [29] studied a method named sorted adaptive prediction sets which uses label ranking information to improve the predictive efficiency in the marginal coverage setting.

In general, the methods designed for a specific coverage validity notion are not necessarily compatible with another notion of coverage, such as object-conditional coverage [58], class-conditional coverage [58], local coverage [36] which are introduced and studied in the prior CP literature [61; 60; 20; 15; 9]. The standard class-conditional CP method in [58; 49] guarantees the class-wise coverage, but does not particularly aim to reduce the size of prediction sets. The cluster CP method [15] which performs CP over clusters of labels achieves a cluster-conditional coverage that approximates the class-conditional guarantee, but requires some assumptions on the underlying clustering model.

Our goal is to develop a provable class-conditional CP algorithm with small prediction sets to guarantee the class-wise coverage that is agnostic to the underlying model.

## 3 Notations, Background, and Problem Setup

**Notations.** Suppose \((X,Y)\) is a data sample where \(X\in\mathcal{X}\) is an input from the input space \(\mathcal{X}\), and \(Y\in\mathcal{Y}=\{1,2,\cdots,K\}\) is the ground-truth label with \(K\) candidate classes. Assume \((X,Y)\) is randomly drawn from an underlying distribution \(\mathcal{P}\) defined on \(\mathcal{X}\times\mathcal{Y}\), where we denote \(p_{y}=\mathbb{P}_{XY}[Y=y]\). Let \(f:\mathcal{X}\rightarrow\Delta_{+}^{K}\) denote a soft classifier (e.g., a soft-max classifier) that produces prediction scores for all candidate classes on any given input \(X\), where \(\Delta_{+}^{K}\) denote the \(K\)-dimensional probability simplex and \(f(X)_{y}\) denotes the predicted confidence for class \(y\). We define the class-wise top-\(k\) error for class \(y\) from the trained classifier \(f\) as \(\epsilon_{y}^{k}=\mathbb{P}\{r_{f}(X,Y)>k|Y=y\}\), where \(r_{f}(X,Y)=\sum_{l=1}^{K}\mathbbm{1}[f(X)_{l}\geq f(X)_{Y}]\) returns the rank of \(Y\) predicted by \(f(X)\) in a descending order, and \(\mathbbm{1}[\cdot]\) is an indicator function. We are provided with a training set \(\mathcal{D}_{\text{tr}}\) for training the classifier \(f\), and a calibration set \(\mathcal{D}_{\text{cal}}=\{(X_{i},Y_{i})\}_{i=1}^{n}\) for CP. Let \(\mathcal{I}_{y}=\{i:Y_{i}=y,\text{ for all }(X_{i},Y_{i})\in\mathcal{D}_{\text{cal}}\}\) and \(n_{y}=|\mathcal{I}_{y}|\) denote the number of calibration examples for class \(y\).

**Problem Setup of CP.** Let \(V:\mathcal{X}\times\mathcal{Y}\rightarrow\mathbb{R}\) denote a _non-conformity_ scoring function to measure how different a new example is from old ones [60]. It is employed to compare a given testing sample \((X_{\text{test}},Y_{\text{test}})\) with a set of calibration data \(\mathcal{D}_{\text{cal}}\): if the non-conformity score is large, then \((X_{\text{test}},Y_{\text{test}})\) conforms less to calibration samples. Prior work has considered the design of good non-conformity scoring functions, e.g., [2; 50; 47]. In this paper, we focus on the scoring functions of _Adaptive Prediction Sets_ (APS) proposed in [47] and _Regularized APS_ (RAPS) proposed in [1] for classification based on the ordered probabilities of \(f\) and true label rank \(r_{f}(X,Y)\). For the simplicity of notation, we denote the non-conformity score of the \(i\)-th calibration example as \(V_{i}=V(X_{i},Y_{i})\).

Given a input \(X\), we sort the predicted probability for all classes \(\{1,\cdots,K\}\) of the classifier \(f\) such that \(1\geq f(X)_{(1)}\geq\cdots\geq f(X)_{(K)}\geq 0\) are ordered statistics, where \(f(X)_{(k)}\) denotes the \(k\)-th largest prediction. The APS [47] score for a sample \((X,Y)\) is computed as follows:

\[V(X,Y)=\sum_{l=1}^{r_{f}(X,Y)-1}f(X)_{(l)}+U\cdot f(X)_{(r_{f}(X,Y))},\]

where \(U\in[0,1]\) is a uniform random variable to break ties. We also consider its regularized variant RAPS [1], which additionally includes a rank-based regularization \(\lambda(r_{f}(X,Y)-k_{reg})^{+}\) to the above equation, where \((\cdot)^{+}=\text{max}\{0,\cdot\}\) denotes the hinge loss, \(\lambda\) and \(k_{reg}\) are two hyper-parameters.

For a target coverage \(1-\alpha\), we find the corresponding empirical quantile on calibration data \(\mathcal{D}_{\text{cal}}\) defined as

\[\widehat{Q}_{1-\alpha}=\text{min}\Big{\{}t:\sum_{i=1}^{n}\frac{1}{n}\cdot \mathbbm{1}[V_{i}\leq t]\geq 1-\alpha\Big{\}},\]

which can be determined by finding the \(\lceil(1-\alpha)(1+n)\rceil\)-th smallest value of \(\{V_{i}\}_{i=1}^{n}\). The prediction set of a testing input \(X_{\text{test}}\) can be constructed by thresholding with \(\widehat{Q}_{1-\alpha}\):

\[\widehat{\mathcal{C}}_{1-\alpha}(X_{\text{test}})=\{y\in\mathcal{Y}:V(X_{ \text{test}},y)\leq\widehat{Q}_{1-\alpha}\}.\]Therefore, \(\widehat{C}_{1-\alpha}\) gives a _marginal coverage_ guarantee [47, 1]: \(\mathbb{P}_{(X,Y)\sim\mathcal{P}}\{Y\in\widehat{C}_{1-\alpha}(X)\}\geq 1-\alpha\). To achieve the _class-conditional coverage_, standard CCP [58] uniformly iterates the class-wise thresholding subroutine with the class-wise quantiles \(\{\widehat{Q}_{1-\alpha}^{\text{class}}(y)\}_{y\in\mathcal{Y}}\):

\[\widehat{\mathcal{C}}_{1-\alpha}^{\text{CCP}}(X_{\text{test}})= \{y\in\mathcal{Y}:V(X_{\text{test}},y)\leq\widehat{Q}_{1-\alpha}^{\text{class} }(y)\},\] (1) \[\text{where }\widehat{Q}_{1-\alpha}^{\text{class}}(y)=\text{min} \Big{\{}t:\sum_{i\in\mathcal{I}_{y}}\frac{1}{n_{y}}\cdot 1\llbracket V_{i}\leq t \rrbracket\geq 1-\alpha\Big{\}}.\]

Specifically, CCP pairs \(X_{\text{test}}\) with each candidate class label \(y\), and includes \(y\) in the prediction set \(\widehat{\mathcal{C}}_{1-\alpha}^{\text{CCP}}(X_{\text{test}})\) if \(V(X_{\text{test}},y)\leq\widehat{Q}_{1-\alpha}^{\text{class}}(y)\) holds. After going through all candidate class labels \(y\in\mathcal{Y}\), it achieves the class-wise coverage for any \(y\in\mathcal{Y}\)[58, 2]:

\[\mathbb{P}_{(X,Y)\sim\mathcal{P}}\{Y\in\widehat{\mathcal{C}}_{1-\alpha}^{\text {CCP}}(X)|Y=y\}\geq 1-\alpha.\] (2)

CCP produces large prediction sets which are not useful in practice. Therefore, our goal is to develop a provable CP method that provides class-conditional coverage and constructs smaller prediction sets than those from CCP. We summarize all the notations in Table 3 of Appendix.

## 4 Rank Calibrated Class-Conditional CP

We first explain the proposed _Rank Calibrated Class-conditional Conformal Prediction (RC3P)_ algorithm and present its model-agnostic coverage guarantee. Next, we provide the theoretical analysis for the provable improvement of predictive efficiency of RC3P over the CCP method.

### Algorithm and Model-Agnostic Coverage Analysis

We start with the motivating discussion about the potential drawback of the standard CCP method in terms of _predictive efficiency_. Equation (1) shows that, for a given test input \(X_{\text{test}}\), CCP likely contains some uncertain labels due to the uniform iteration over each class label \(y\in\mathcal{Y}\) to check if \(y\) should be included into the prediction set or not. For example, given a class label \(y\) and two test samples \(X_{1},X_{2}\), suppose their APS scores are \(V(X_{1},y)=0.9,V(X_{2},y)=0.8\), with ranks \(r_{f}(X_{1},y)=1,r_{f}(X_{2},y)=5\). Furthermore, if \(\widehat{Q}_{1-\alpha}^{\text{class}}(y)=0.85\), then by (1) for CCP, we know that \(y\notin\widehat{\mathcal{C}}_{1-\alpha}^{\text{CCP}}(X_{1})\) and \(y\in\widehat{\mathcal{C}}_{1-\alpha}^{\text{CCP}}(X_{2})\), even though \(f(X_{1})\) ranks \(y\) at the #1 class label for \(X_{1}\) with a very high confidence \(f(X_{1})_{y}=0.9\) and CCP can still achieve the valid class-conditional coverage. We argue that, the principle of CCP to scans all \(y\in\mathcal{Y}\) uniformly can easily result in large prediction sets, which is detrimental to the effectiveness of human-ML collaborative systems [4, 51].

Consequently, to improve the predictive efficiency of CCP (i.e., reduce prediction set sizes), it is reasonable to include label rank information in the calibration procedure to adjust the distribution of non-conformity scores for predictive efficiency. As mentioned in the previous sections, better scoring functions have been proposed to improve the predictive efficiency for marginal coverage, e.g., RAPS. However, directly applying RAPS for class-wise coverage presents challenges: 1) tuning its hyper-parameters for each class requires extra computational overhead, and 2) fixing its hyper-parameters for all classes overlooks the difference between distributions of different classes. Moreover, for the approximate class-conditional coverage achieved by cluster CP [15], it still requires some assumptions on the underlying model (i.e., it is not fully model-agnostic).

Therefore, the key idea of our proposed RC3P algorithm (outlined in Algorithm 1) is to refine the class-wise calibration procedure using a label rank calibration strategy augmented to the standard conformal score calibration, to enable adaptivity to various classes. Specifically, in contrast to CCP, RC3P selectively activates the class-wise thresholding subroutine in (1) according to their class-wise top-\(k\) error \(\epsilon_{y}^{k}\) for class \(y\). RC3P produces the prediction set for a given test input \(X_{\text{test}}\) with two calibration schemes (one for conformal score and another for label rank) as shown below:

\[\widehat{\mathcal{C}}_{1-\alpha}^{\text{RC3P}}(X_{\text{test}})=\big{\{}y\in \mathcal{Y}:\underbrace{V(X_{\text{test}},y)\leq\widehat{Q}_{1-\bar{\alpha}_{y }}^{\text{class}}(y)}_{\text{conformal score calibration}},\ \underbrace{r_{f}(X_{\text{test}},y)\leq\widehat{\widehat{ \widehat{\widehat{\widehat{\widehat{\epsilon}}}}}}(y)}_{\text{label rank calibration}}\big{\}},\] (3)

where \(\widehat{\mathcal{Q}}_{1-\bar{\alpha}_{y}}^{\text{class}}(y)\) and \(\widehat{\widehat{\widehat{\widehat{\epsilon}}}}(y)\) are score and label rank threshold for class \(y\), respectively. In particular, \(\widehat{\widehat{\widehat{\epsilon}}}(y)\) controls the class-wise uncertainty adaptive to each class \(y\) based on the top-\(k\) error \(\epsilon_{y}^{\widehat{\widehat{\epsilon}}(y)}\) of the classifier. By determining \(\widehat{k}(y)\), the top \(k\) predicted class labels of \(f(X_{\text{test}})\) will more likely cover the true label \(Y_{\text{test}}\), making the augmented label rank calibration filter out the class labels \(y\) that have a high rank (larger \(r_{f}(X,y)\)). As a result, given all test input and label pairs \(\{(X_{\text{test}},y)\}_{y\in\mathcal{Y}}\), RC3P performs score thresholding using class-wise quantiles only on a subset of reliable test pairs.

**Determining \(\widehat{k}(y)\) and \(\widehat{\alpha}_{y}\) for model-agnostic valid coverage.** For class \(y\), intuitively, we would like a value for \(\widehat{k}(y)\) such that the corresponding top-\(\widehat{k}(y)\) error is smaller than \(\alpha\), so that it is possible to guarantee valid coverage (recall \(\mathbb{P}\{A,B\}=\mathbb{P}\{A\}\cdot\mathbb{P}\{B|A\}\)). Since a larger \(\widehat{k}(y)\) gives a smaller \(\epsilon_{y}^{\widehat{k}(y)}\) until \(\epsilon_{y}^{K}=0\), it is guaranteed to find a value for \(\widehat{k}(y)\), in which the corresponding \(\epsilon_{y}^{\widehat{k}(y)}<\alpha\). As a result, given all test input and label pairs \(\{(X_{\text{test}},y)\}_{y\in\mathcal{Y}}\), RC3P performs score thresholding using class-wise quantiles only on a subset of reliable test pairs and filters out the class labels \(y\) that have a high rank (larger \(r_{f}(X,y)\)). The following result formally shows the principle to configure \(\widehat{k}(y)\) and \(\widehat{\alpha}_{y}\) to guarantee the class-wise coverage that is agnostic to the underlying model.

**Theorem 4.1**.: _(Class-conditional coverage of RC3P) Suppose that selecting \(\widehat{k}(y)\) values result in the class-wise top-\(k\) error \(\epsilon_{y}^{\widehat{k}(y)}\) for each class \(y\in\mathcal{Y}\). For a target class-conditional coverage \(1-\alpha\), if we set \(\widehat{\alpha}_{y}\) and \(\widehat{k}(y)\) in RC3P (3) in the following ranges:_

\[\widehat{k}(y)\in\{k:\epsilon_{y}^{k}<\alpha\},\quad 0\leq\widehat{\alpha}_{y} \leq\alpha-\epsilon_{y}^{\widehat{k}(y)},\] (4)

_then RC3P can achieve the class-conditional coverage for every \(y\in\mathcal{Y}\):_

\[\mathbb{P}_{(X,Y)\sim\mathbb{P}}\{Y\in\widehat{\mathcal{C}}_{1-\alpha}^{\text {RC3P}}(X)|Y=y\}\geq 1-\alpha.\]

### Analysis of Predictive Efficiency for RC3P

We further analyze the predictive efficiency of RC3P: under what conditions RC3P can produce a smaller expected prediction set size compared to CCP, when both achieve the same (\(1-\alpha\))-class-conditional coverage. We investigate how to choose the value of \(\widehat{\alpha}_{y}\) and \(\widehat{k}(y)\) from the feasible ranges in (4) to achieve the best predictive efficiency using RC3P.

**Lemma 4.2**.: _(Trade-off condition for improved predictive efficiency of RC3P) Suppose \(\widehat{\alpha}_{y}\) and \(\widehat{k}(y)\) satisfy (4) in Theorem 4.1. If the following inequality holds for any \(y\in\mathcal{Y}\):_

\[\mathbb{P}_{X_{\text{test}}}\big{[}V(X_{\text{test}},y)\leq\widehat{Q}_{1- \widehat{\alpha}}^{\text{class}}(y),\;r_{f}(X_{\text{test}},y)\leq\widehat{k}(y )\big{]}\leq\mathbb{P}_{X_{\text{test}}}\big{[}V(X_{\text{test}},y)\leq\widehat {Q}_{1-\alpha}^{\text{class}}(y)\big{]},\] (5)

_then RC3P produces smaller expected prediction sets than CCP, i.e.,_

\[\mathbb{E}_{X_{\text{test}}}\big{[}\|\widehat{\mathcal{C}}_{1-\widehat{\alpha }}^{\text{RC3P}}(X_{\text{test}})\|\leq\mathbb{E}_{X_{\text{test}}}\big{[}\| \widehat{\mathcal{C}}_{1-\alpha}^{\text{CCP}}(X_{\text{test}})\|\big{]}.\]

**Remark.** The above result demonstrates that when both RC3P and CCP achieve the target \(1-\alpha\) class-conditional coverage, under the condition of (5), RC3P produces smaller prediction sets than CCP. In fact, this condition implies that the combined (conformity score and label rank) calibration of RC3P tends to include less labels with high rank or low confidence from the classifier. In contrast, the CCP method tends to include relatively more uncertain labels into the prediction set, where their ranks are high and the confidence of the classifier is low. Now we can interpret the condition (5) by defining a condition number, termed as \(\sigma_{y}\):

\[\sigma_{y}=\frac{\mathbb{P}_{X_{\text{test}}}\Big{[}V(X_{\text{test}},y)\leq \widehat{Q}_{1-\bar{\alpha}}^{\text{class}}(y),\ r_{f}(X_{\text{test}},y)\leq \widehat{k}(y)\Big{]}}{\mathbb{P}_{X_{\text{test}}}\Big{[}V(X_{\text{test}},y) \leq\widehat{Q}_{1-\alpha}^{\text{class}}(y)\Big{]}}.\] (6)

In other words, if we can verify that \(\sigma_{y}\leq 1\) for all \(y\), then RC3P can improve the predictive efficiency over CCP. Furthermore, if \(\sigma_{y}\) is fairly small, then the efficiency improvement can be even more significant. To verify this condition, our comprehensive experiments (Section 5.2, Figure 3) show that \(\sigma_{y}\) values are much smaller than \(1\) on real-world data. These results demonstrate the practical utility of our theoretical analysis to produce small prediction sets using RC3P. Note that the reduction in prediction set size of RC3P over CCP is proportional to how small the \(\sigma_{y}\) values are.

**Theorem 4.3**.: _(Conditions of improved predictive efficiency for RC3P) Define \(D=\mathbb{P}[r_{f}(X,y)\leq\widehat{k}(y)|Y\neq y]\), and \(\bar{r}_{f}(X,y)=\lfloor\frac{r_{f}(X,y)+1}{2}\rfloor\). Denote \(B=\mathbb{P}[f(X)_{(\bar{r}_{f}(X,y))}\leq\widehat{Q}_{1-\alpha}^{\text{class }}(y)|Y\neq y]\) if \(V\) is APS, or \(B=\mathbb{P}[f(X)_{(\bar{r}_{f}(X,y))}+\lambda\leq\widehat{Q}_{1-\alpha}^{\text {class}}(y)|Y\neq y]\) if \(V\) is RAPS. If \(B-D\geq\frac{p_{y}}{1-p_{y}}(\alpha-\widehat{\epsilon}_{y}^{\widehat{k}(y)})\), then \(\sigma_{y}\leq 1\)._

**Remark.** The above result further analyzes when the condition in Eq (5) of Lemma 4.2 (or equivalently, \(\sigma_{y}\leq 1\)) holds to guarantee the improved predictive efficiency. Specifically, the condition \(B-D\geq\frac{p_{y}}{1-p_{y}}(\alpha-\epsilon_{y}^{\widehat{k}(y)})\) of Theorem 4.3 can be realized in two ways: (i) making LHS \(B-D\) as large as possible; (ii) making the RHS \(\frac{p_{y}}{1-p_{y}}(\alpha-\epsilon_{y}^{\widehat{k}(y)})\) as small as possible. To this end, we can set Line 7 in Algorithm 1 in the following way:

\[\widehat{k}(y)=\text{min}\{k:\epsilon_{y}^{k}<\alpha\},\quad\widehat{\alpha} _{y}=\alpha-\epsilon_{y}^{\widehat{k}(y)}.\] (7)

Therefore, this setting ensures \(\sigma_{y}\leq 1\) and as a result improves predictive efficiency.

## 5 Experiments and Results

We present the empirical evaluation of the RC3P algorithm and demonstrate its effectiveness in achieving class-conditional coverage to produce small prediction sets. We conduct experiments using two baselines (CCP and Cluster-CP), four datasets (each with three imbalance types and five imbalance ratios), and two machine learning models (trained for \(50\) epochs and \(200\) epochs, with \(200\) epochs being our main experimental setting). Additionally, we use two scoring functions (APS and RAPS) and set three different \(\alpha\) values (\(\alpha\in 0.1,0.05,0.01\), with \(\alpha=0.1\) as our main setting).

### Experimental Setup

**Classification datasets.** We consider four datasets: CIFAR-10, CIFAR-100 [34], mini-ImageNet [57], and Food-101 [8] by using the standard training and validation split. We employ the same methodology as [41; 10; 14] to create an imbalanced long-tail setting for each dataset as a harder challenge: 1) We use the original training split as a training set for training \(f\) with training samples (\(n_{tr}\) is defined as the number of training samples), and randomly split the original (balanced) validation set into calibration samples and testing samples. 2) We define an imbalance ratio \(\rho\), the ratio between the sample size of the smallest and largest class: \(\rho=\frac{\text{min}_{i}\{\#\text{ samples in class }i\}}{\text{max}_{i}\{\#\text{ samples in class }i\}}\). 3) For each training set, we create three different imbalanced distributions using three decay types over the class indices \(c\in\{1,\cdots,K\}\): (a) An exponential-based decay (EXP) with \(\frac{n_{tr}}{K}\times\rho^{\frac{K}{K}}\) examples in class \(c\), (b) A polynomial-based decay (POLY) with \(\frac{n_{tr}}{K}\times\frac{1}{\sqrt{16\rho}+1}\) examples in class \(c\), and (c) A majority-based decay (MAJ) with \(\frac{n_{tr}}{K}\times\rho\) examples in classes \(c>1\). We keep the calibration and test set balanced and unchanged. We provide an illustrative example of the three decay types in Appendix (Section C.3, Figure 4). Towards a more complete comparison, we also employ balanced datasets. Following Cluster-CP2, we employ CIFAR-100, Places365 [66], iNaturalist[55], and ImageNet[48].

**Deep neural network models.** We consider ResNet-20 [27] as the main architecture to train classifiers for imbalanced classification datasets. To handle imbalanced data, we employ the training algorithm "LDAM" proposed by [10] that assigns different margins to classes, where larger margins are assigned to minority classes in the loss function. We follow the training strategy in [10] where all models are trained with \(200\) epochs. The class-wise performance with three imbalance types and imbalance ratios \(\rho=0.5\) and \(\rho=0.1\) on four datasets are evaluated (see Appendix C.1). We also train models with \(50\) epochs and the corresponding APSS results are reported in Appendix C.8.

For balanced datsets, we follow the same settings from Cluster-CP, which uses IMAGENET1K_V2 as pre-trained weights from PyTorch [44] and then fine-tune models with ResNet-50 for all datasets except ImageNet. For ImageNet, we use SimCLR-v2 [12] as training models.

**CP baselines.** We consider three CP methods: **1)**CCP which estimates class-wise score thresholds and produces prediction set using Equation (1); **2)**Cluster-CP [15] that performs calibration over clusters to reduce prediction set sizes; and **3)**RC3P that produces prediction set using Equation (3). All CP methods are built on the same classifier and non-conformity scoring function for a fair comparison. We employ the three common scoring functions: APS [47], RAPS [1], and HPS [49]. We set \(\alpha=0.1\) as our main experiment setting and also report other experiment results of different \(\alpha\)

\begin{table}
\begin{tabular}{c c c c c c c c} \hline \hline \multicolumn{2}{c}{Conformity Score} & Methods & \multicolumn{3}{c}{EXP} & \multicolumn{3}{c}{POLY} & \multicolumn{3}{c}{MAJ} \\  & & \(\rho=0.5\) & \(\rho=0.1\) & \(\rho=0.5\) & \(\rho=0.1\) & \(\rho=0.5\) & \(\rho=0.1\) \\ \hline \multicolumn{2}{c}{CIFAR-10} & \multicolumn{3}{c}{CIFAR-10} \\ \hline \multirow{3}{*}{APS} & CCP & **1.555 \(\pm\) 0.010** & **1.855 \(\pm\) 0.014** & **1.538 \(\pm\) 0.010** & **1.776 \(\pm\) 0.012** & **1.840 \(\pm\) 0.020** & **2.629 \(\pm\) 0.013** \\  & Cluster-CP & 1.741 \(\pm\) 0.018 & 2.162 \(\pm\) 0.015 & 1.706 \(\pm\) 0.014 & 1.928 \(\pm\) 0.013 & 1.948 \(\pm\) 0.033 & 3.220 \(\pm\) 0.020 \\  & **RC3P** & **1.555 \(\pm\) 0.010** & **1.535 \(\pm\) 0.014** & **1.538 \(\pm\) 0.010** & **1.776 \(\pm\) 0.012** & **1.840 \(\pm\) 0.020** & **2.629 \(\pm\) 0.013** \\ \hline \multirow{3}{*}{RAPS} & CCP & **1.555 \(\pm\) 0.010** & **1.855 \(\pm\) 0.014** & **1.538 \(\pm\) 0.010** & **1.776 \(\pm\) 0.012** & **1.840 \(\pm\) 0.020** & **2.632 \(\pm\) 0.012** \\  & Cluster-CP & 1.714 \(\pm\) 0.018 & 2.162 \(\pm\) 0.015 & 1.706 \(\pm\) 0.014 & 1.929 \(\pm\) 0.013 & 1.787 \(\pm\) 0.019 & 2.968 \(\pm\) 0.024 \\  & **RC3P** & **1.555 \(\pm\) 0.010** & **1.855 \(\pm\) 0.014** & **1.538 \(\pm\) 0.010** & **1.776 \(\pm\) 0.012** & **1.840 \(\pm\) 0.020** & **2.632 \(\pm\) 0.012** \\ \hline \multirow{3}{*}{HPS} & CCP & **1.144 \(\pm\) 0.005** & **1.324 \(\pm\) 0.007** & **1.137 \(\pm\) 0.003** & **1.243 \(\pm\) 0.005** & **1.272 \(\pm\) 0.008** & **1.936 \(\pm\) 0.010** \\  & Cluster-CP & 1.214 \(\pm\) 0.008 & 1.508 \(\pm\) 0.010 & 1.211 \(\pm\) 0.004 & 1.354 \(\pm\) 0.005 & 1.336 \(\pm\) 0.009 & 2.312 \(\pm\) 0.025 \\  & **RC3P** & **1.144 \(\pm\) 0.005** & **1.324 \(\pm\) 0.007** & **1.137 \(\pm\) 0.003** & **1.243 \(\pm\) 0.005** & **1.272 \(\pm\) 0.008** & **1.936 \(\pm\) 0.010** \\ \hline \multicolumn{2}{c}{CIFAR-100} & \multicolumn{3}{c}{CIFAR-100} \\ \hline \multirow{3}{*}{APS} & CCP & 44.224 \(\pm\) 0.341 & 50.969 \(\pm\) 0.345 & 49.889 \(\pm\) 0.353 & 64.343 \(\pm\) 0.237 & 41.494 \(\pm\) 0.514 & 64.642 \(\pm\) 0.535 \\  & Cluster-CP & 29.238 \(\pm\) 0.690 & 37.592 \(\pm\) 0.857 & 38.252 \(\pm\) 0.353 & 52.391 \(\pm\) 0.595 & 31.518 \(\pm\) 0.335 & 50.883 \(\pm\) 0.673 \\  & **RC3P** & **17.958 \(\pm\) 0.004** & **1.294 \(\pm\) 0.005** & **23.048 \(\pm\) 0.005** & **33.185 \(\pm\) 0.007** & **18.581 \(\pm\) 0.007** & **32.699 \(\pm\) 0.005** \\ \hline \multirow{3}{*}{RAPS} & CCP & 44.250 \(\pm\) 0.342 & 50.970 \(\pm\) 0.345 & 49.886 \(\pm\) 0.353 & 49.332 \(\pm\) 0.236 & 48.343 \(\pm\) 0.353 & 66.653 \(\pm\) 0.535 \\  & Cluster-CP & 29.267 \(\pm\) 0.612 & 37.795 \(\pm\) 0.862 & 38.258 \(\pm\) 0.320 & 52.374 \(\pm\) 0.592 & 351.513 \(\pm\) 0.325 & 50.379 \(\pm\) 0.684 \\  & **RC3P** & **17.705 \(\pm\) 0.004** & **21.954 \(\pm\) 0.005** & **23.048 \(\pm\) 0.008** & **33.185 \(\pm\) 0.005** & **18.581 \(\pm\) 0.006** & **32.699 \(\pm\) 0.006** \\ \hline \multirow{3}{*}{HPS} & CCP & 41.351 \(\pm\) 0.242 & 49.604 \(\pm\) 0.344 & 48.063 \(\pm\) 0.376 & 63.639 \(\pm\) 0.277 & 46.125 \(\pm\) 0.351 & 64.371 \(\pm\) 0.564 \\  & Cluster-CP & 27.566 \(\pm\) 0.555 & 35.258 \(\pm\) 0.979 & 36.101 \(\pm\) 0.056 & 56.353 \(\pm\) 0.776 & 29.235 \(\pm\) 0.363 & 50.519 \(\pm\) 0.679 \\  & **RC3P** & **20.363 \(\pm\) 0.006** & **25.212 \(\pm\) 0.010** & **25.908 \(\pm\) 0.007** & **36.951 \(\pm\) 0.018** & **21.149 \(\pm\) 0.006** & **35.606 \(\pm\) 0.005** \\ \hline \multicolumn{2}{c}{CIFAR-100} & \multicolumn{3}{c}{CIFAR-100} \\ \hline \multirow{3}{*}{APS} & CCP & 26.676 \(\pm\) 0.171 & 26.111 \(\pm\) 0.194 & 26.262 \(\pm\) 0.133 & 26.159 \(\pm\) 0.208 & 27.313 \(\pm\) 0.154 & 25.629 \(\pm\) 0.207 \\  & Cluster-CP & 25.8values (See Appendix C.7). Meanwhile, the hyper-parameters for each baseline are tuned according to their recommended ranges based on the same criterion (see Appendix C.2). We repeat experiments over \(10\) different random calibration-testing splits and report the mean and standard deviation.

**Evaluation methodology.** We use the target coverage \(1-\alpha\) = 90% class-conditional coverage for CCP, Cluster-CP, and RC3P. We compute three evaluation metrics on the testing set:

\(\bullet\)_Under Coverage Ratio (UCR)._

\[\text{UCR}:=\sum_{c\in[K]}\mathbbm{1}\Big{[}\frac{\mathbb{E}_{X_{\text{test}} }\mathbbm{1}[y\in\widehat{\mathcal{C}}_{1-\alpha}(x)\text{ s.t. }y=c]}{\mathbb{E}_{X_{\text{test}}}\mathbbm{1}[y=c]}<1-\alpha\Big{]}/K.\]

\(\bullet\)_Average Prediction Set Size (APSS)._

\[\text{APSS}:=\sum_{c\in[K]}\frac{\mathbb{E}_{X_{\text{test}}}\mathbbm{1}[y=c] \cdot|\widehat{\mathcal{C}}_{1-\alpha}(x)|}{\mathbb{E}_{X_{\text{test}}} \mathbbm{1}[y=c]}/K.\]

Note that coverage and predictive efficiency are two competing metrics in CP [1], e.g., achieving better coverage (resp. predictive efficiency) degenerates predictive efficiency (resp. coverage). Therefore, following the same strategy in [20], we choose to control their UCR as the same level that is close to \(0\) for a fair comparison over three class-conditional CP algorithms in terms of APSS. Meanwhile, to address the gap between population values and empirical ones (e.g., quantiles with \(\hat{O}(1/\sqrt{n_{y}})\) error bound, common to all CP methods [58, 22, 2], or class-wise top-\(k\) error \(\epsilon_{y}^{k}\) with \(\hat{O}(1/\sqrt{n_{y}})\) error bound [43]), we uniformly add \(g/\sqrt{n_{y}}\) (the same order with the standard concentration gap) to inflate the nominal coverage \(1-\alpha\) on each baseline and tune \(g\in\{0.25,0.5,0.75,1\}\) on the calibration dataset in terms of UCR. The detailed \(g\) values of each method are displayed in Appendix C.2. In addition, the actual achieved UCR values are shown in the complete results (see Appendix C.4, C.5, and C.6). For a complete evaluation, we add the experiments without controlling coverage on imbalanced datasets under the same setting and use the total under coverage gap (UCG) metric:

\(\bullet\)_Under Coverage Gap (UCG)._

\[\text{UCG}:=\sum_{c\in[K]}\text{max}\bigg{\{}1-\alpha-\frac{\mathbb{P}[Y\in \hat{\mathcal{C}}(X),\text{s.t. Y=c}]}{\mathbb{P}[Y=c]},0\bigg{\}}.\]

Experiments with UCG metric evaluation are shown in the Appendix C.9.

\begin{table}
\begin{tabular}{c c c c c c c} \hline \hline Conformity Score & Measure & Methods & CIFAR-100 & Places365 & iNaturalist & ImageNet \\ \hline \multirow{4}{*}{APS} & \multirow{4}{*}{UCR} & CCP & 0.045 \(\pm\) 0.008 & 0.012 \(\pm\) 0.002 & 0.016 \(\pm\) 0.001 & 0.036 \(\pm\) 0.001 \\  & & Cluster-CP & 0.023 \(\pm\) 0.006 & 0.029 \(\pm\) 0.003 & 0.026 \(\pm\) 0.003 & 0.031 \(\pm\) 0.002 \\  & & **RC3P** & **0.006 \(\pm\) 0.003** & **0.003 \(\pm\) 0.001** & **0.008 \(\pm\) 0.001** & **0.023 \(\pm\) 0.001** \\ \cline{2-7}  & \multirow{4}{*}{APSS} & CCP & 30.467 \(\pm\) 0.307 & 19.698 \(\pm\) 0.050 & 18.802 \(\pm\) 0.023 & 101.993 \(\pm\) 0.812 \\  & & Cluster-CP & 32.628 \(\pm\) 0.720 & 20.818 \(\pm\) 0.173 & 23.467 \(\pm\) 0.049 & 66.285 \(\pm\) 1.433 \\  & & **RC3P** & **12.551 \(\pm\) 0.005** & **13.772 \(\pm\) 0.005** & **12.736 \(\pm\) 0.006** & **6.518 \(\pm\) 0.001** \\ \hline \multirow{4}{*}{RAPS} & \multirow{4}{*}{UCR} & CCP & 0.043 \(\pm\) 0.006 & 0.013 \(\pm\) 0.002 & 0.016 \(\pm\) 0.020 & 0.038 \(\pm\) 0.020 \\  & & Cluster-CP & 0.016 \(\pm\) 0.005 & 0.036 \(\pm\) 0.002 & 0.027 \(\pm\) 0.003 & 0.046 \(\pm\) 0.004 \\  & & **RC3P** & **0.002 \(\pm\) 0.001** & **0.002 \(\pm\) 0.001** & **0.006 \(\pm\) 0.001** & **0.017 \(\pm\) 0.001** \\ \cline{2-7}  & \multirow{4}{*}{APSS} & CCP & 26.135 \(\pm\) 0.308 & 15.694 \(\pm\) 0.049 & 14.812 \(\pm\) 0.042 & 37.748 \(\pm\) 0.304 \\  & & Cluster-CP & 28.04 \(\pm\) 0.069 & 16.750 \(\pm\) 0.143 & 23.964 \(\pm\) 0.419 & 16.155 \(\pm\) 1.241 \\  & & **RC3P** & **12.586 \(\pm\) 0.002** & **14.192 \(\pm\) 0.001** & **13.251 \(\pm\) 0.001** & **6.560 \(\pm\) 0.002** \\ \hline \multirow{4}{*}{HPS} & \multirow{4}{*}{UCR} & CCP & 0.034 \(\pm\) 0.006 & 0.015 \(\pm\) 0.002 & **0.018 \(\pm\) 0.002** & 0.036 \(\pm\) 0.002 \\  & & Cluster-CP & 0.006 \(\pm\) 0.003 & 0.029 \(\pm\) 0.004 & 0.035 \(\pm\) 0.002 & 0.039 \(\pm\) 0.005 \\  & & **RC3P** & **0.003 \(\pm\) 0.002** & **0.002 \(\pm\) 0.001** & **0.018 \(\pm\) 0.002** & **0.006 \(\pm\) 0.000** \\ \cline{2-7}  & \multirow{4}{*}{APSS} & CCP & 25.898 \(\pm\) 0.321 & 14.020 \(\pm\) 0.044 & **9.751 \(\pm\) 0.033** & 24.384 \(\pm\) 0.249 \\ \cline{1-1}  & & Cluster-CP & 27.165 \(\pm\) 0.600 & 14.530 \(\pm\) 0.143 & 13.080 \(\pm\) 0.374 & 8.810 \(\pm\) 0.046 \\ \cline{1-1}  & & **RC3P** & **12.558 \(\pm\) 0.004** & **13.919 \(\pm\) 0.004** & **9.751 \(\pm\) 0.033** & **6.533 \(\pm\) 0.001** \\ \hline \hline \end{tabular}
\end{table}
Table 2: **Balanced experiment on CIFAR-100, Places365, iNaturalist, ImageNet. The models are pre-trained. UCR is controlled to \(\leq 0.05\). RC3P significantly outperforms the best baseline with \(32.826\%\) reduction in APSS (\(\downarrow\) better) on average over min{CCP, cluster-CP}.**

### Results and Discussion

We list empirical results in Table 1 for an overall comparison on four imbalanced datasets with \(\rho=0.5,0.1\) using all three training distributions (EXP, POLYand MAJ) based on the considered APS, RAPS and HPS scoring functions. Complete experiment results under more values of \(\rho\) are in Appendix C). Results with APS, RAPS, and HPS scoring functions on balanced datasets are also summarized in Table 2. We make the following two key observations: (i) CCP, Cluster-CP, and RC3P can guarantee the class-conditional coverage (their UCRs are all close to \(0\)) for all settings; (ii) RC3P significantly outperforms CCP and Cluster-CP in APSS on almost all imbalanced settings by reducing APSS with \(24.47\%\) on all four datasets and \(32.63\%\) on three datasets excluding CIFAR-10 compared with min{CCP,Cluster-CP} on average, while for balanced settings, RC3P still significantly outperforms the best baselines in terms of APSS with \(32.826\%\) APSS reduction.

To investigate the challenge of imbalanced data and more importantly, how RC3P significantly improves the APSS, we further conduct three careful experiments on imbalanced datasets. First, we report the histograms of class-conditional coverage and the corresponding histograms of prediction set size. This experiment verifies that RC3P derives significantly more class-conditional coverage above \(1-\alpha\) and thus reduces the prediction set size. Second, we visualize the normalized frequency of label rank included in prediction sets on testing datasets for all class-wise algorithms: CCP, Cluster-CP, and RC3P. The normalized frequency is defined as: \(\mathbb{P}(k):=\frac{\mathbb{E}_{X_{\text{test}}}\mathbb{I}_{r}(X_{\text{test},\theta})=k,y\in\widehat{\mathcal{C}}(x))}{\sum_{k=1}^{K}\mathbb{E}_{X_{\text {test}}}\mathbb{I}_{r}(X_{\text{test},\theta})=k,y\in\widehat{\mathcal{C}}(x ))}\). Finally, we empirically verify the trade-off condition number \(\{\sigma_{y}\}_{y=1}^{K}\) of Equation 6 on calibration dataset to reveal the underlying reason for RC3P producing smaller prediction sets over CCP with our standard training models (epoch \(=200\)). We also evaluate \(\{\sigma_{y}\}_{y=1}^{K}\) with less trained models (epoch \(=50\)) on imbalanced datasets in Appendix C.10. Additionally, we also repeat all three experiments on balanced datasets (i.e., the histograms of class-conditional coverage and prediction set size, the normalized frequency of label rank included in prediction sets, and \(\{\sigma_{y}\}_{y=1}^{K}\)) in Appendix C.11. Below we discuss our experimental results and findings in detail.

**RC3P significantly outperforms CCP and Cluster-CP**. First, it is clear from Table 6, 8, and 7, and 2 that RC3P, CCP, and Cluster-CP guarantee class-conditional coverage on all settings. This can also be observed by the first row of Fig 1, where the class-wise coverage bars of CCP and RC3P distribute on the right-hand side of the target probability \(1-\alpha\) (red dashed line). Second, RC3P outperforms CCP and Cluster-CP with \(24.47\%\) (four datasets) or \(32.63\%\) (excluding CIFAR-10) on imbalanced datasets and \(32.63\%\) on balanced datasets decrease in terms of average prediction set size for the same class-wise coverage. We also report the histograms of the corresponding prediction set sizes in the second row of Figure 1, which shows (i) RC3P has more concentrated class-wise coverage distribution than CCP and Cluster-CP; (ii) the distribution of prediction set sizes produced by RC3P is globally smaller than that produced by CCP and Cluster-CP, which

Figure 1: Class-conditional coverage (Top row) and prediction set size (Bottom row) achieved by CCP, Cluster-CP, and RC3P methods when \(\alpha=0.1\) and models are trained with \(200\) epochs on four imbalanced datasets with imbalance type EXP \(\rho=0.1\). We clarify that RC3P overlaps with CCP on CIFAR-10. It is clear that RC3P has more densely distributed class-conditional coverage above \(0.9\) (the target \(1-\alpha\) class-conditional coverage) than CCP and Cluster-CP with significantly smaller prediction sets on CIFAR-100, mini-ImageNet and Food-101.

is justified by a better trade-off number of \(\{\sigma_{y}\}_{y=1}^{K}\) as shown in Figure 3. Note that the class-wise coverage and the corresponding prediction set sizes RC3P overlap with CCP on CIFAR-10 in Figure 1.

**Visualization of normalized frequency.** Figure 2 illustrates the normalized frequency distribution of label ranks included in the prediction sets across various testing datasets. It is evident that the distribution of label ranks in the prediction set generated by RC3P tends to be lower compared to those produced by CCP and Cluster-CP. Furthermore, the probability density function tail for label ranks in the RC3P prediction set is notably shorter than that of other methods. This indicates that RC3P more effectively incorporates lower-ranked labels into prediction sets, as a result of its augmented rank calibration scheme.

**Verification of \(\sigma_{y}\).** Figure 3 verifies the validity of Equation (6) on testing datasets and confirms the optimized trade-off between the coverage with inflated quantile and the constraint with calibrated label rank leads to smaller prediction sets. It also confirms that the condition number \(\{\sigma_{y}\}_{y=1}^{K}\) could be evaluated on calibration datasets without testing datasets and thus decreases the overall computation cost. We verify that \(\sigma_{y}\leq 1\) for all settings and \(\sigma_{y}\) is much smaller than \(1\) on all datasets with large number of classes.

## 6 Summary

This paper studies a provable conformal prediction (CP) algorithm that aims to provide class-conditional coverage guarantee and to produce small prediction sets for classification tasks with many and/or imbalanced classes. Our proposed RC3P algorithm performs double-calibration, one over conformity score and one over label rank for each class separately, to achieve this goal. Our experiments clearly demonstrate the significant efficacy of RC3P over the baseline class-conditional CP algorithms on both balanced and imbalanced classification data settings.

**Acknowledgments.** This research was supported in part by United States Department of Agriculture (USDA) NIFA award No. 2021-67021-35344 (AgAID AI Institute) and by NSF CNS-2312125 grant.

Figure 3: Verification of condition numbers \(\{\sigma_{y}\}_{y=1}^{K}\) in Equation 6 with imbalance type EXP, \(\rho=0.1\) when \(\alpha=0.1\) and models are trained with \(200\) epochs. Vertical dashed lines represent the value \(1\), and we observe that all the condition numbers are smaller than \(1\). This verifies the validity of the condition for Lemma 4.2, and thus confirms that RC3P produces smaller prediction sets than CCP using calibration on both non-conformity scores and label ranks.

Figure 2: Visualization for the normalized frequency distribution of label ranks included in the prediction set of CCP, Cluster-CP, and RC3P with \(\rho=0.1\) for imbalance type EXP when \(\alpha=0.1\) and models are trained with \(200\) epochs. It is clear that the distribution of normalized frequency generated by RC3P tends to be lower compared to those produced by CCP and Cluster-CP. Furthermore, the probability density function tail for label ranks in the RC3P prediction set is notably shorter than that of other methods.

## References

* [1] Anastasios Angelopoulos, Stephen Bates, Jitendra Malik, and Michael I Jordan. Uncertainty sets for image classifiers using conformal prediction. _arXiv preprint arXiv:2009.14193_, 2020.
* [2] Anastasios N Angelopoulos and Stephen Bates. A gentle introduction to conformal prediction and distribution-free uncertainty quantification. _arXiv preprint arXiv:2107.07511_, 2021.
* [3] Anastasios N Angelopoulos, Stephen Bates, Adam Fisch, Lihua Lei, and Tal Schuster. Conformal risk control. _arXiv preprint arXiv:2208.02814_, 2022.
* [4] Varun Babbar, Umang Bhatt, and Adrian Weller. On the utility of prediction sets in human-ai teams. In _Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence (IJCAI-22)_, 2022.
* [5] Rina Foygel Barber, Emmanuel J Candes, Aaditya Ramdas, and Ryan J Tibshirani. Conformal prediction beyond exchangeableability. _The Annals of Statistics_, 51(2):816-845, 2023.
* [6] Stephen Bates, Anastasios Angelopoulos, Lihua Lei, Jitendra Malik, and Michael Jordan. Distribution-free, risk-controlling prediction sets. _Journal of the ACM (JACM)_, 68(6):1-34, 2021.
* [7] Aadyot Bhatnagar, Huan Wang, Caiming Xiong, and Yu Bai. Improved online conformal prediction via strongly adaptive online learning. In _International Conference on Machine Learning_, pages 2337-2363. PMLR, 2023.
* [8] Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101-mining discriminative components with random forests. In _Computer Vision-ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part VI 13_, pages 446-461. Springer, 2014.
* [9] Henrik Bostrom, Ulf Johansson, and Tuwe Lofstrom. Mondrian conformal predictive distributions. In _Conformal and Probabilistic Prediction and Applications_, pages 24-38. PMLR, 2021.
* [10] Kaidi Cao, Colin Wei, Adrien Gaidon, Nikos Arechiga, and Tengyu Ma. Learning imbalanced datasets with label-distribution-aware margin loss. _Advances in neural information processing systems_, 32, 2019.
* [11] Nitesh V Chawla, Kevin W Bowyer, Lawrence O Hall, and W Philip Kegelmeyer. Smote: synthetic minority over-sampling technique. _Journal of artificial intelligence research_, 16:321-357, 2002.
* [12] Ting Chen, Simon Kornblith, Kevin Swersky, Mohammad Norouzi, and Geoffrey E Hinton. Big self-supervised models are strong semi-supervised learners. _Advances in neural information processing systems_, 33:22243-22255, 2020.
* [13] Kfir M Cohen, Sangwoo Park, Osvaldo Simeone, and Shlomo Shamai. Cross-validation conformal risk control. _arXiv preprint arXiv:2401.11974_, 2024.
* [14] Yin Cui, Menglin Jia, Tsung-Yi Lin, Yang Song, and Serge Belongie. Class-balanced loss based on effective number of samples. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 9268-9277, 2019.
* [15] Tiffany Ding, Anastasios Angelopoulos, Stephen Bates, Michael Jordan, and Ryan J Tibshirani. Class-conditional conformal prediction with many classes. _Advances in Neural Information Processing Systems_, 36, 2024.
* [16] Robin Dunn, Larry Wasserman, and Aaditya Ramdas. Distribution-free prediction sets with random effects. _arXiv preprint arXiv:1809.07441_, 2018.
* [17] Shai Feldman, Stephen Bates, and Yaniv Romano. Calibrated multiple-output quantile regression with representation learning. _Journal of Machine Learning Research_, 24(24):1-48, 2023.
* [18] Adam Fisch, Tal Schuster, Tommi Jaakkola, and Regina Barzilay. Efficient conformal prediction via cascaded inference with expanded admission. _arXiv preprint arXiv:2007.03114_, 2020.
* [19] Adam Fisch, Tal Schuster, Tommi Jaakkola, and Regina Barzilay. Few-shot conformal prediction with auxiliary tasks. In _International Conference on Machine Learning_, pages 3329-3339. PMLR, 2021.
* [20] Matteo Fontana, Gianluca Zeni, and Simone Vantini. Conformal prediction: a unified review of theory and new challenges. _Bernoulli_, 29(1):1-23, 2023.
* [21] Subhankar Ghosh, Taha Belkhouja, Yan Yan, and Janardhan Rao Doppa. Improving uncertainty quantification of deep classifiers via neighborhood conformal prediction: Novel algorithm and theoretical analysis. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 37, pages 7722-7730, 2023.

* [22] Subhankar Ghosh, Yuanjie Shi, Taha Belkhouja, Yan Yan, Jana Doppa, and Brian Jones. Probabilistically robust conformal prediction. In _Uncertainty in Artificial Intelligence_, pages 681-690. PMLR, 2023.
* [23] Isaac Gibbs and Emmanuel Candes. Adaptive conformal inference under distribution shift. _Advances in Neural Information Processing Systems_, 34:1660-1672, 2021.
* [24] Isaac Gibbs and Emmanuel J Candes. Conformal inference for online prediction with arbitrary distribution shifts. _Journal of Machine Learning Research_, 25(162):1-36, 2024.
* [25] Lee-Ad Gottlieb, Eran Kaufman, and Aryeh Kontorovich. Apportioned margin approach for cost sensitive large margin classifiers. _Annals of Mathematics and Artificial Intelligence_, 89(12):1215-1235, 2021.
* [26] Leying Guan. Localized conformal prediction: A generalized inference framework for conformal prediction. _Biometrika_, 110(1):33-50, 2023.
* [27] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 770-778, 2016.
* [28] Chen Huang, Yining Li, Chen Change Loy, and Xiaoou Tang. Deep imbalanced learning for face recognition and attribute prediction. _IEEE transactions on pattern analysis and machine intelligence_, 42(11):2781-2794, 2019.
* [29] Jianguo Huang, Huajun Xi, Linjun Zhang, Huaxiu Yao, Yue Qiu, and Hongxin Wei. Conformal prediction for deep classifier via label ranking, 2024.
* [30] Kexin Huang, Ying Jin, Emmanuel Candes, and Jure Leskovec. Uncertainty quantification over graph with conformalized graph neural networks. In _NeurIPS 2023_.
* [31] Ying Jin and Emmanuel J. Candes. Model-free selective inference under covariate shift via weighted conformal p-values.
* [32] Lisa Jockel, Michael Klas, Janek Gross, and Pascal Gerber. Conformal prediction and uncertainty wrapper: What statistical guarantees can you get for uncertainty quantification in machine learning? In _International Conference on Computer Safety, Reliability, and Security_, pages 314-327. Springer, 2023.
* [33] Bartosz Krawczyk, Michal Koziarski, and Michal Wozniak. Radial-based oversampling for multiclass imbalanced data classification. _IEEE transactions on neural networks and learning systems_, 31(8):2818-2831, 2019.
* [34] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.
* [35] Jing Lei, Max G'Sell, Alessandro Rinaldo, Ryan J Tibshirani, and Larry Wasserman. Distribution-free predictive inference for regression. _Journal of the American Statistical Association_, 113(523):1094-1111, 2018.
* [36] Jing Lei and Larry Wasserman. Distribution-free prediction bands for non-parametric regression. _Journal of the Royal Statistical Society Series B: Statistical Methodology_, 76(1):71-96, 2014.
* [37] Lihua Lei and Emmanuel J Candes. Conformal inference of counterfactuals and individual treatment effects. _Journal of the Royal Statistical Society Series B: Statistical Methodology_, 83(5):911-938, 2021.
* [38] Charles Lu, Anastasios N Angelopoulos, and Stuart Pomerantz. Improving trustworthiness of ai disease severity rating in medical imaging with ordinal conformal prediction sets. In _International Conference on Medical Image Computing and Computer-Assisted Intervention_, pages 545-554. Springer, 2022.
* [39] Charles Lu, Andreanne Lemay, Ken Chang, Katharina Hobel, and Jayashree Kalpathy-Cramer. Fair conformal predictors for applications in medical imaging. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 36, pages 12008-12016, 2022.
* [40] Harish Tayyar Madabushi, Elena Kochkina, and Michael Castelle. Cost-sensitive bert for generalisable sentence classification with imbalanced data. _arXiv preprint arXiv:2003.11563_, 2020.
* [41] Aditya Krishna Menon, Sadeep Jayasumana, Ankit Singh Rawat, Himanshu Jain, Andreas Veit, and Sanjiv Kumar. Long-tail learning via logit adjustment. _arXiv preprint arXiv:2007.07314_, 2020.
* [42] Roweida Mohammed, Jumanah Rawashdeh, and Malak Abdullah. Machine learning with oversampling and undersampling techniques: overview study and experimental results. In _2020 11th international conference on information and communication systems (ICICS)_, pages 243-248. IEEE, 2020.

* [43] Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. _Foundations of machine learning_. MIT press, 2018.
* [44] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. _Advances in neural information processing systems_, 32, 2019.
* [45] Yaniv Romano, Rina Foygel Barber, Chiara Sabatti, and Emmanuel Candes. With malice toward none: Assessing uncertainty via equalized coverage. _Harvard Data Science Review_, 2(2):4, 2020.
* [46] Yaniv Romano, Evan Patterson, and Emmanuel Candes. Conformalized quantile regression. _Advances in neural information processing systems_, 32, 2019.
* [47] Yaniv Romano, Matteo Sesia, and Emmanuel Candes. Classification with valid and adaptive coverage. _Advances in Neural Information Processing Systems_, 33:3581-3591, 2020.
* [48] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge. _International journal of computer vision_, 115:211-252, 2015.
* [49] Mauricio Sadinle, Jing Lei, and Larry Wasserman. Least ambiguous set-valued classifiers with bounded error levels. _Journal of the American Statistical Association_, 114(525):223-234, 2019.
* [50] Glenn Shafer and Vladimir Vovk. A tutorial on conformal prediction. _Journal of Machine Learning Research_, 9(3), 2008.
* [51] Eleni Straitouri, Lequun Wang, Nastaran Okati, and Manuel Gomez Rodriguez. Improving expert predictions with conformal prediction. In _International Conference on Machine Learning (ICML)_, 2023.
* [52] Jiankai Sun, Yiqi Jiang, Jianing Qiu, Parth Nobel, Mykel J Kochenderfer, and Mac Schwager. Conformal prediction for uncertainty-aware planning with diffusion dynamics model. _Advances in Neural Information Processing Systems_, 36, 2024.
* [53] Ryan J Tibshirani, Rina Foygel Barber, Emmanuel Candes, and Aaditya Ramdas. Conformal prediction under covariate shift. _Advances in neural information processing systems_, 32, 2019.
* [54] Chih-Fong Tsai, Wei-Chao Lin, Ya-Han Hu, and Guan-Ting Yao. Under-sampling class imbalanced datasets by combining clustering analysis and instance selection. _Information Sciences_, 477:47-54, 2019.
* [55] Grant Van Horn, Oisin Mac Aodha, Yang Song, Yin Cui, Chen Sun, Alex Shepard, Hartwig Adam, Pietro Perona, and Serge Belongie. The inaturalist species classification and detection dataset. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 8769-8778, 2018.
* [56] Janette Vazquez and Julio C Facelli. Conformal prediction in clinical medical sciences. _Journal of Healthcare Informatics Research_, 6(3):241-252, 2022.
* [57] Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan Wierstra, et al. Matching networks for one shot learning. _Advances in neural information processing systems_, 29, 2016.
* [58] Vladimir Vovk. Conditional validity of inductive conformal predictors. In _Asian conference on machine learning_, pages 475-490. PMLR, 2012.
* [59] Vladimir Vovk, Valentina Fedorova, Ilia Nouretdinov, and Alexander Gammerman. Criteria of efficiency for conformal prediction. In _Conformal and Probabilistic Prediction with Applications: 5th International Symposium, COPA 2016, Madrid, Spain, April 20-22, 2016, Proceedings 5_, pages 23-39. Springer, 2016.
* [60] Vladimir Vovk, Alexander Gammerman, and Glenn Shafer. _Algorithmic learning in a random world_. Springer Science & Business Media, 2005.
* [61] Vladimir Vovk, David Lindsay, Ilia Nouretdinov, and Alex Gammerman. Mondrian confidence machine. _Technical Report_, 2003.
* [62] Volodya Vovk, Alexander Gammerman, and Craig Saunders. Machine-learning applications of algorithmic randomness. 1999.
* [63] Pattaramon Vuttipititayamongkol and Eyad Elyan. Neighbourhood-based undersampling approach for handling imbalanced and overlapped data. _Information Sciences_, 509:47-70, 2020.
* [64] Yunpeng Xu, Wenge Guo, and Zhi Wei. Conformal risk control for ordinal classification. In _Uncertainty in Artificial Intelligence_, pages 2346-2355. PMLR, 2023.

* [65] Margaux Zaffran, Aymeric Dieuleveut, Julie Josse, and Yaniv Romano. Conformal prediction with missing values. In _International Conference on Machine Learning_, pages 40578-40604. PMLR, 2023.
* [66] Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva, and Antonio Torralba. Places: A 10 million image database for scene recognition. _IEEE transactions on pattern analysis and machine intelligence_, 40(6):1452-1464, 2017.

## Appendix A Mathematical Notations

## Appendix B Technical Proofs of Theoretical Results

### Proof of Theorem 4.1

**Theorem B.1**.: _(Theorem 4.1 restated, class-conditional coverage of RC3P) Suppose that selecting \(\widehat{k}(y)\) values result in the class-wise top-\(k\) error \(\epsilon_{y}^{\widehat{k}(y)}\) for each class \(y\in\mathcal{Y}\). For a target class-conditional coverage \(1-\alpha\), if we set \(\widehat{\alpha}_{y}\) and \(\widehat{k}(y)\) in RC3P (3) in the following ranges:_

\[\widehat{k}(y)\in\{k:\epsilon_{y}^{k}<\alpha\},\quad 0\leq\widehat{\alpha}_{y} \leq\alpha-\epsilon_{y}^{\widehat{k}(y)},\] (8)

_then RC3P can achieve the class-conditional coverage for every \(y\in\mathcal{Y}\):_

\[\mathbb{P}_{(X,Y)\sim\mathcal{P}}\{Y\in\widehat{\mathcal{C}}_{1-\alpha}^{\text {RC3P}}(X)|Y=y\}\geq 1-\alpha.\]

Proof.: (of Theorem 4.1)

Let \(y\in\mathcal{Y}\) denote any class label. In this proof, we omit the superscript \(k\) in the top-\(k\) error notation \(\epsilon_{y}^{k}\) for simplicity.

With the lower bound of the coverage on class \(y\) (Theorem 1 in [47]), we have

\[1-\widehat{\alpha}\leq\mathbb{P}\{Y_{\text{test}}\in\widehat{ \mathcal{C}}_{1-\widehat{\alpha}}^{\text{CCP}}(X_{\text{test}})|Y=y\}\] \[= \mathbb{P}\{V(X_{\text{test}},Y_{\text{test}})\leq\widehat{Q}_{1- \widehat{\alpha}}^{\text{class}}(y)|Y=y\}\] \[= \mathbb{P}\{V(X_{\text{test}},Y_{\text{test}})\leq\widehat{Q}_{1- \widehat{\alpha}}^{\text{class}}(y),r_{f}(X_{\text{test}},Y_{\text{test}}) \leq\widehat{k}(y)|Y=y\}\] \[+\mathbb{P}\{V(X_{\text{test}},Y_{\text{test}})\leq\widehat{Q}_{1 -\widehat{\alpha}}^{\text{class}}(y),r_{f}(X_{\text{test}},Y_{\text{test}})> \widehat{k}(y)|Y=y\}\] \[\leq \mathbb{P}\{V(X_{\text{test}},Y_{\text{test}})\leq\widehat{Q}_{1 -\widehat{\alpha}}^{\text{class}}(y),r_{f}(X_{\text{test}},Y_{\text{test}}) \leq\widehat{k}(y)|Y=y\}\] \[+\underbrace{\mathbb{P}\{r_{f}(X_{\text{test}},Y_{\text{test}})> \widehat{k}(y)|Y=y\}}_{\leq\epsilon_{y}^{\widehat{k}(y)}}\] \[\leq \mathbb{P}\{Y_{\text{test}}\in\widehat{\mathcal{C}}_{1-\widehat{ \alpha}}^{\text{RC3P}}(y)|Y=y\}+\epsilon_{y}^{\widehat{k}(y)}.\]

Re-arranging the above inequality, we have

\[\mathbb{P}\{Y_{\text{test}}\in\widehat{\mathcal{C}}_{1-\widehat{\alpha}}^{ \text{RC3P}}(y)|Y=y\}\geq 1-\widehat{\alpha}-\epsilon_{y}^{\widehat{k}(y)} \geq 1-\alpha,\]

where the last inequality is due to \(\widehat{\alpha}_{y}\leq\alpha-\epsilon_{y}^{\widehat{k}(y)}\). This implies that RC3P guarantees the class-conditional coverage on any class \(y\). This completes the proof for Theorem 4.1.

\begin{table}
\begin{tabular}{|l|l|} \hline
**Notation** & **Meaning** \\ \hline \(X\in\mathcal{X}\) & Input example \\ \hline \(Y\in\mathcal{Y}\) & The ground-truth label \\ \hline \(f\) & The soft classifier \\ \hline \(\Delta_{\lambda}^{K}\) & The \(K\)-dimensional probability simplex \\ \hline \(f(X)_{y}\) & The predicted confidence on class \(y\) \\ \hline \(\epsilon_{y}^{k}\) & The class-wise top-\(k\) error for class \(y\) from \(f\) \\ \hline \(r_{f}(X,Y)\) & The rank of \(Y\) predicted by \(f(X)\) \\ \hline \(\mathcal{D}_{\text{tr}}\) & Training data \\ \hline \(\mathcal{D}_{\text{cal}}\) & Calibration data \\ \hline \(\mathcal{D}_{\text{test}}\) & Test data \\ \hline \(n_{y}\) & The number of calibration examples for class \(y\) \\ \hline \(V(X,Y)\) & Non-conformity scoring function \\ \hline \(\mathcal{C}_{1-\alpha}(X_{\text{test}})\) & Prediction set for input \(X_{\text{test}}\) \\ \hline \(\alpha\) & Target mis-coverage rate \\ \hline \(\widehat{\alpha}_{y}\) & Nominal mis-coverage rate for class \(y\) \\ \hline \end{tabular}
\end{table}
Table 3: **Key notations used in this paper.**

### Proof of Lemma 4.2

**Theorem B.2**.: _(Lemma 4.2 restated, improved predictive efficiency of RC3P) Let \(\widehat{\alpha}_{y}\) and \(\widehat{k}(y)\) satisfy Theorem 4.1. If the following inequality holds for any \(y\in\mathcal{Y}\):_

\[\mathbb{P}_{X_{\text{test}}}\big{[}V(X_{\text{test}},y)\leq\widehat{Q}_{1- \widehat{\alpha}}^{\text{class}}(y),\;r_{f}(X_{\text{test}},y)\leq\widehat{k}( y)\big{]}\leq\mathbb{P}_{X_{\text{test}}}\big{[}V(X_{\text{test}},y)\leq \widehat{Q}_{1-\alpha}^{\text{class}}(y)\big{]},\] (9)

_then RC3P produces smaller expected prediction sets than CCP, i.e.,_

\[\mathbb{E}_{X_{\text{test}}}[|\widehat{\mathcal{C}}_{1-\widehat{\alpha}}^{ \text{RC3P}}(X_{\text{test}})|]\leq\mathbb{E}_{X_{\text{test}}}[|\widehat{ \mathcal{C}}_{1-\alpha}^{\text{CCP}}(X_{\text{test}})|].\]

Proof.: (of Lemma 4.2)

The proof idea is to reduce the cardinality of the prediction set made by RC3P to that made by CCP in expectation. Let \(\sigma_{y}=\frac{\mathbb{P}_{X_{\text{test}}}\big{[}V(X_{\text{test}},y)\leq \widehat{Q}_{1-\widehat{\alpha}}^{\text{class}}(y),\;r_{f}(X_{\text{test}},y) \leq\widehat{k}(y)\big{]}}{\mathbb{P}_{X_{\text{test}}}\Big{[}V(X_{\text{test} },y)\leq\widehat{Q}_{1-\alpha}^{\text{class}}(y)\Big{]}}\). According to the assumption in (9), we know that \(\sigma_{y}\leq 1\), which will be used later.

We start with the expected prediction set size of RC3Pand then derive its upper bound.

\[\mathbb{E}_{X_{\text{test}}}[|\widehat{\mathcal{C}}_{1-\widehat{ \alpha}}^{\text{RC3P}}(X_{\text{test}})|]=\mathbb{E}_{X_{\text{test}}}\Bigg{[} \sum_{y\in\mathcal{Y}}\mathbbm{1}\Big{[}V(X_{\text{test}},y)\leq\widehat{Q}_{1 -\widehat{\alpha}}^{\text{class}}(y),\;r_{f}(X_{\text{test}},y)\leq\widehat{k}( y)\Big{]}\Bigg{]}\] \[= \sum_{y\in\mathcal{Y}}\mathbb{E}_{X_{\text{test}}}\Big{[}\mathbbm {1}[V(X_{\text{test}},y)\leq\widehat{Q}_{1-\widehat{\alpha}}^{\text{class}}(y), \;r_{f}(X_{\text{test}},y)\leq\widehat{k}(y)]\Big{]}\] \[= \sum_{y\in\mathcal{Y}}\mathbb{P}_{X_{\text{test}}}\Big{[}V(X_{ \text{test}},y)\leq\widehat{Q}_{1-\widehat{\alpha}}^{\text{class}}(y),\;r_{f} (X_{\text{test}},y)\leq\widehat{k}(y)\Big{]}\] \[\overset{(a)}{=} \sum_{y\in\mathcal{Y}}\sigma_{y}\cdot\mathbb{P}_{X_{\text{test}}} \Big{[}V(X_{\text{test}},y)\leq\widehat{Q}_{1-\alpha}^{\text{class}}(y)\Big{]}\] (10) \[\overset{(b)}{\leq} \sum_{y\in\mathcal{Y}}\mathbb{E}_{X_{\text{test}}}\Big{[}\mathbbm {1}[V(X_{\text{test}},y)\leq\widehat{Q}_{1-\alpha}^{\text{class}}(y)]\Big{]}\] \[= \mathbb{E}_{X_{\text{test}}}\Bigg{[}\sum_{y\in\mathcal{Y}}\mathbbm {1}[V(X_{\text{test}},y)\leq\widehat{Q}_{1-\alpha}^{\text{class}}(y)]\Bigg{]}= \mathbb{E}_{X_{\text{test}}}[|\widehat{\mathcal{C}}_{1-\alpha}^{\text{CCP}}(X_ {\text{test}})|],\] (11)

where the equality \((a)\) is due to the definitions of \(\sigma_{y}\), and inequality \((b)\) is due to the assumption

\[\sum_{y\in\mathcal{Y}}\sigma_{y}\cdot\mathbb{P}_{X_{\text{test}}}\Big{[}V(X_{ \text{test}},y)\leq\widehat{Q}_{1-\alpha}^{\text{class}}(y)\Big{]}\leq\sum_{y \in\mathcal{Y}}\mathbb{P}_{X_{\text{test}}}\Big{[}V(X_{\text{test}},y)\leq \widehat{Q}_{1-\alpha}^{\text{class}}(y)\Big{]}.\]

This shows that RC3P requires smaller prediction sets to guarantee the class-conditional coverage compared to CCP. 

### Proof of Theorem 4.3

**Theorem B.3**.: _(Theorem 4.3 restated, conditions of improved predictive efficiency for RC3P) Define \(D=\mathbb{P}[r_{f}(X,y)\leq\widehat{k}(y)|Y\neq y]\), and \(\bar{r}_{f}(X,y)=\lfloor\frac{r_{f}(X,y)+1}{2}\rfloor\). Denote \(B=\mathbb{P}[f(X)_{(\bar{r}_{f}(X,y))}\leq\widehat{Q}_{1-\alpha}^{\text{class}} (y)|Y\neq y]\) if \(V\) is APS, or \(B=\mathbb{P}[f(X)_{(\bar{r}_{f}(X,y))}+\lambda\leq\widehat{Q}_{1-\alpha}^{ \text{class}}(y)|Y\neq y]\) if \(V\) is RAPS. If \(B-D\geq\frac{p_{y}}{1-p_{y}}(\alpha-\epsilon_{y}^{\widehat{k}(y)})\), then \(\sigma_{y}\leq 1\)._

Proof.: (of Theorem 4.3)

Based on the different choices of scoring function, we first divide two scenarios:

(i): If \(V(X,y)\) is the APS scoring function, since the APS score cumulatively sums the ordered prediction of \(f(X)\): \(V(X,y)=\sum_{l=1}^{r_{f}(X,y)}f(X)_{(l)}\), it is easy to verify that \(V(X,y)\) is concave interms of \(l\). As a result, we have

\[V(X,y)= \frac{r_{f}(X,y)}{r_{f}(X,y)}\cdot\sum_{l=1}^{r_{f}(X,y)}f(X)_{(l)} \leq r_{f}(X,y)\cdot f(X)_{(\lfloor\sum_{l=1}^{r_{f}(X,y)}l/r_{f}(X,y)\rfloor)}=r _{f}(X,y)\cdot f(X)_{(\bar{r}_{f}(X,y))},\]

where \(\bar{r}_{f}(X,y)=\left\lfloor\frac{\sum_{l=1}^{r_{f}(X,y)}l}{r_{f}(X,y)}\right\rfloor =\lfloor(r_{f}(X,y)+1)/2\rfloor\).

Now we lower bound \(\mathbb{P}_{X}[V(X,y)\leq\widehat{Q}_{1-\alpha}^{\text{class}}(y)]\) as follows.

\[\mathbb{P}_{X}[V(X,y)\leq\widehat{Q}_{1-\alpha}^{\text{class}}(y)]\] \[= \underbrace{\mathbb{P}_{XY}[Y=y]}_{=p_{y}}\cdot\underbrace{ \mathbb{P}_{X}[V(X,y)\leq\widehat{Q}_{1-\alpha}^{\text{class}}(y)|Y=y]}_{ \geq 1-\alpha}+\underbrace{\mathbb{P}_{XY}[Y\neq y]}_{=1-p_{y}}\cdot\underbrace{ \mathbb{P}_{X}[V(X,y)\leq\widehat{Q}_{1-\alpha}^{\text{class}}(y)|Y\neq y]}_{ \geq B}\] \[\geq p_{y}(1-\alpha)+(1-p_{y})B+p_{y}(1-\epsilon_{y}^{\widehat{k}(y)}) +(1-p_{y})D-p_{y}(1-\epsilon_{y}^{\widehat{k}(y)})-(1-p_{y})D\] \[\geq \mathbb{P}_{X}[r_{f}(X,y)\leq\widehat{k}(y)]-p_{y}(\alpha- \epsilon_{y}^{\widehat{k}(y)})+(1-p_{y})(B-D).\] (12)

According to the assumption \(B-D\geq\frac{p_{y}}{1-p_{y}}(\alpha-\epsilon_{y}^{\widehat{k}(y)})\), we have

\[\mathbb{P}_{X}[r_{f}(X,y)\leq\widehat{k}(y)]\leq\mathbb{P}_{X}[V(X,y)\leq \widehat{Q}_{1-\alpha}^{\text{class}}(y)].\]

(ii): If \(V(X,y)\) is the RAPS scoring function and \(r_{f}(X,y)\leq k_{reg}\), then the RAPS scoring function could be rewritten as: \(V(X,y)=\sum_{l=1}^{r_{f}(X,y)}f(X)_{(l)}\). As a result, we have:

\[V(X,y)= \frac{r_{f}(X,y)}{r_{f}(X,y)}\cdot\sum_{l=1}^{r_{f}(X,y)}f(X)_{(l)}\] \[\leq r_{f}(X,y)\cdot f(X)_{(\lfloor\sum_{l=1}^{r_{f}(X,y)}l/r_{f}(X,y )\rfloor)}\] \[= r_{f}(X,y)\cdot f(X)_{(\bar{r}_{f}(X,y))}\] \[\leq r_{f}(X,y)\cdot\Big{(}f(X)_{(\bar{r}_{f}(X,y))}+\lambda\Big{)}.\]

If \(r_{f}(X,y)>k_{reg}\), then the RAPS scoring function could be rewritten as: \(V(X,y)=\sum_{l=1}^{r_{f}(X,y)}f(X)_{(l)}+\lambda(r_{f}(X,y)-k_{reg})\). As a result, we have

\[V(X,y) =\frac{r_{f}(X,y)}{r_{f}(X,y)}\cdot\Big{(}\sum_{l=1}^{r_{f}(X,y)} f(X)_{(l)}+\lambda\big{(}r_{f}(X,y)-k_{reg}\big{)}\Big{)}\] \[\leq r_{f}(X,y)\cdot\Big{(}f(X)_{(\bar{r}_{f}(X,y))}+\lambda\Big{(}1 -\frac{k_{reg}}{r_{f}(X,y)}\big{)}\Big{)}\] \[\leq r_{f}(X,y)\cdot\Big{(}f(X)_{(\bar{r}_{f}(X,y))}+\lambda\Big{)}.\]

Then, by applying the Inequality 12, we have:

\[\mathbb{P}_{X}[r_{f}(X,y)\leq\widehat{k}(y)]\leq\mathbb{P}_{X}[V(X,y)\leq \widehat{Q}_{1-\alpha}^{\text{class}}(y)].\]

This completes the proof for Theorem 4.3. 

## Appendix C Complete Experimental Results

### Training Details

For CIFAR-10 and CIFAR-100, we train ResNet20 using LDAM loss function given in [10] with standard mini-batch stochastic gradient descent (SGD) using learning rate \(0.1\), momentum \(0.9\), and weight decay \(2e-4\) for \(200\) epochs and \(50\) epochs. The batch size is \(128\). For experiments on mini-ImageNet, we use the same setting. For Food-101, the batch size is \(256\) and other parameters are kept the same. We reported our main results when models were trained in \(200\) epochs. Other results are reported in Appendix C.8 and Table 11.

We also evaluate the top-1 accuracy over the majority, medium, and minority groups of classes as the class-wise performance when \(200\) epochs. To show the variation of class-wise performance, we divide some classes with the largest number of data samples into the majority group, and the number of these classes is a quarter (\(25\%\)) of the total number of classes. Similarly, we divide the classes with the smallest number of data into the minority group (\(25\%\)) and the remaining classes as the medium group (\(50\%\)). In the above table, we show the accuracy of three groups with three imbalance types and two imbalance ratios \(\rho=0.1,\rho=0.5\) on four datasets.

The results are summarized in Table 4. As can be seen, the group-wise performance can vary significantly from high to very low. The class-imbalance setting is the case where the classifier does not perform very well in some classes.

### Calibration Details

As mentioned in Section 5.1, we balanced split the validation set of CIFAR-10 and CIFAR-100, the number of calibration data is \(5000\). For mini-ImageNet, the number of calibration data is \(15000\). For Food-101, the total number is \(12625\). To compute the mean and standard deviation for the overall performance, we repeat calibration experiments for \(10\) times. In our main results, We set \(\alpha=0.1\). We also report other experiment results of different \(\alpha\) values, \(\alpha=0.05\) and \(\alpha=0.01\), in Appendix C.7, and Table 9 and 10.

The regularization parameter for RAPS scoring function is from the set \(k_{reg}\in\{3,5,7\}\) and \(\lambda\in\{0.001,0.01,0.1\}\) based on the empirical setting in cluster-CP. We select the combination of \(k_{reg}\) and \(\lambda\) for each experiment with the same imbalanced type and imbalanced ratio on the same dataset, where most of the APSS values of all methods are minimum.

The hyper-parameter \(g\) is selected from the set \(\{0.25,0.5,0.75,1.0\}\) to find the minimal \(g\) that CCP, Cluster-CP 3, and RC3P achieve the target class-conditional coverage. We clarify that for each dataset and each class-conditional CP method, we use fixed \(g\) values. The detailed \(g\) values

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline \multirow{2}{*}{Groups} & \multicolumn{2}{c}{EXP} & \multicolumn{2}{c}{POLY} & \multicolumn{2}{c}{MAJ} \\  & \(\rho\) = 0.5 & \(\rho\) = 0.1 & \(\rho\) = 0.5 & \(\rho\) = 0.1 & \(\rho\) = 0.5 & \(\rho\) = 0.1 \\ \hline \multicolumn{7}{c}{CIFAR-10} \\ \hline Minority & 0.913 & 0.961 & 0.932 & 0.901 & 0.940 & 0.927 \\ Medium & 0.872 & 0.822 & 0.867 & 0.847 & 0.848 & 0.75 \\ Majority & 0.949 & 0.832 & 0.933 & 0.948 & 0.914 & 0.795 \\ \hline \multicolumn{7}{c}{CIFAR-100} \\ \hline Minority & 0.554 & 0.295 & 0.468 & 0.352 & 0.572 & 0.365 \\ Medium & 0.589 & 0.536 & 0.517 & 0.413 & 0.574 & 0.476 \\ Majority & 0.668 & 0.720 & 0.671 & 0.588 & 0.616 & 0.562 \\ \hline \multicolumn{7}{c}{mini-ImageNet} \\ \hline Minority & 0.677 & 0.640 & 0.624 & 0.627 & 0.626 & 0.642 \\ Medium & 0.527 & 0.546 & 0.533 & 0.530 & 0.526 & 0.538 \\ Majority & 0.633 & 0.679 & 0.684 & 0.67 & 0.673 & 0.686 \\ \hline \multicolumn{7}{c}{Food-101} \\ \hline Minority & 0.453 & 0.231 & 0.379 & 0.289 & 0.505 & 0.333 \\ Medium & 0.579 & 0.474 & 0.496 & 0.398 & 0.579 & 0.467 \\ Majority & 0.582 & 0.660 & 0.596 & 0.563 & 0.532 & 0.490 \\ \hline \hline \end{tabular}
\end{table}
Table 4: Top-1 accuracy of minority, medium, and majority groups with three imbalance types and two imbalance ratios \(\rho=0.1,\rho=0.5\) on four datasets. We could observe that the class-wise performance varies significantly over different classes.

are displayed in Table 5. From Table 5, we could observe that the hyperparameter \(g\) for RC3P is always smaller than other methods, which means that comparing other class-wise CP algorithms, our algorithm needs the smallest inflation on \(1-\widehat{\alpha}\) to achieve the target class-conditional coverage. This could also match the result of histograms of class-conditional coverage.

### Illustration of Imbalanced Data

### Comparison Experiments Using APS Score Function

Based on the results in Table 6, we make the following observations: (i) CCP, Cluster-CP, and RC3P can guarantee the class-conditional coverage; and (ii) RC3P significantly outperforms CCP and Cluster-CCP on three datasets by producing smaller prediction sets.

Figure 4: Illustrative examples of the different imbalanced distributions of the number of training examples per class index \(c\) on CIFAR-100

\begin{table}
\begin{tabular}{c c c c c} \hline \hline \multirow{2}{*}{Methods} & \multicolumn{4}{c}{Dataset} \\  & CIFAR-10 & CIFAR-100 & mini-ImageNet & FOOD-101 \\ \hline CCP & 0.5 & 0.5 & 0.75 & 0.75 \\ Cluster-CP & 1.0 & 0.5 & 0.75 & 0.75 \\ RC3P & 0.5 & 0.25 & 0.5 & 0.5 \\ \hline \hline \end{tabular}
\end{table}
Table 5: Hyperparameter \(g\) choices for each class-conditional CP methods CCP, Cluster-CP, and RC3P on four datasets CIFAR-10, CIFAR-100, mini-ImageNet, and Food101. We could observe that all \(g\) values are in constant order to make a fair comparison. Meanwhile, the hyperparameter \(g\) for RC3P is always smaller than other methods.

\begin{table}
\begin{tabular}{c c c c c c c c} \hline \hline Measure & Methods & \multicolumn{2}{c}{EXP} & \multicolumn{2}{c}{POLY} & \multicolumn{2}{c}{MAJ} \\  & & \(\rho\) = 0.5 & \(\rho\) = 0.1 & \(\rho\) = 0.5 & \(\rho\) = 0.1 & \(\rho\) = 0.5 & \(\rho\) = 0.1 \\ \hline \multicolumn{7}{c}{CIFAR-10} \\ \multirow{2}{*}{UCR} & CCP & 0.050 \(\pm\) 0.016 & 0.100 \(\pm\) 0.020 & 0.100 \(\pm\) 0.032 & **0.050 \(\pm\) 0.021** & 0.070 \(\pm\) 0.014 & **0.040 \(\pm\) 0.015** \\  & Cluster-CP & **0.010 \(\pm\) 0.009** & **0.090 \(\pm\) 0.009** & **0.080 \(\pm\) 0.019** & 0.060 \(\pm\) 0.001 & **0.020 \(\pm\) 0.012** & 0.070 \(\pm\) 0.014 \\  & **RC3P** & 0.050 \(\pm\) 0.016 & 0.100 \(\pm\) 0.020 & 0.100 \(\pm\) 0.032 & **0.050 \(\pm\) 0.021** & 0.070 \(\pm\) 0.014 & **0.040 \(\pm\) 0.015** \\ \hline \multirow{2}{*}{APSS} & CCP & **1.555 \(\pm\) 0.010** & **1.855 \(\pm\) 0.014** & **1.538 \(\pm\) 0.010** & **1.776 \(\pm\) 0.012** & **1.840 \(\pm\) 0.020** & **2.629 \(\pm\) 0.013** \\  & Cluster-CP & 1.714 \(\pm\) 0.018 & 2.162 \(\pm\) 0.015 & 1.706 \(\pm\) 0.014 & 1.928 \(\pm\) 0.013 & 1.948 \(\pm\) 0.023 & 3.220 \(\pm\) 0.020 \\  & **RC3P** & **1.555 \(\pm\) 0.010** & **1.855 \(\pm\) 0.014** & **1.538 \(\pm\) 0.010** & **1.776 \(\pm\) 0.012** & **1.840 \(\pm\) 0.020** & **2.629 \(\pm\) 0.013** \\ \hline \multicolumn{7}{c}{CIFAR-100} \\ \multirow{2}{*}{UCR} & CCP & 0.007 \(\pm\) 0.002 & **0.010 \(\pm\) 0.002** & 0.010 \(\pm\) 0.002 & **0.014 \(\pm\) 0.003** & 0.016 \(\pm\) 0.003 & **0.008 \(\pm\) 0.004** \\  & Cluster-CP & 0.012 \(\pm\) 0.002 & 0.016 \(\pm\) 0.004 & 0.020 \(\pm\) 0.003 & 0.004 \(\pm\) 0.002 & 0.016 \(\pm\) 0.003 & 0.019 \(\pm\) 0.005 \\  & **RC3P** & **0.005 \(\pm\) 0.002** & 0.011 \(\pm\) 0.002 & **0.009 \(\pm\) 0.003** & 0.015 \(\pm\) 0.003 & **0.008 \(\pm\) 0.002** & **0.008 \(\pm\) 0.004** \\ \hline \multirow{2}{*}{APSS} & CCP & 44.224 \(\pm\) 0.341 & 50.969 \(\pm\) 0.345 & 49.889 \(\pm\) 0.353 & 64.343 \(\pm\) 0.237 & 44.194 \(\pm\) 0.514 & 64.642 \(\pm\) 0.535 \\  & Cluster-CP & 29.238 \(\pm\) 0.609 & 37.592 \(\pm\) 0.857 & 38.252 \(\pm\) 0.353 & 52.391 \(\pm\) 0.595 & 31.518 \(\pm\) 0.335 & 50.883 \(\pm\) 0.673 \\  & **RC3P** & **17.705 \(\pm\) 0.004** & **21.954 \(\pm\) 0.005** & **23.048 \(\pm\) 0.008** & **33.185 \(\pm\) 0.005** & **18.581 \(\pm\) 0.007** & **32.699 \(\pm\) 0.005** \\ \hline \multicolumn{7}{c}{mini-ImageNet} \\ \multirow{2}{*}{UCR} & CCP & 0.008 \(\pm\) 0.004 & 0.008 \(\pm\) 0.004 & 0.005 \(\pm\) 0.002 & 0.004 \(\pm\) 0.001 & 0.010 \(\pm\) 0.004 & 0.005 \(\pm\) 0.002 \\  & Cluster-CP & 0.014 \(\pm\) 0.004 & 0.012 \(\pm\) 0.004 & 0.011 \(\pm\) 0.003 & 0.014 \(\pm\) 0.003 & 0.008 \(\pm\) 0.002 & 0.010 \(\pm\) 0.003 \\  & **RC3P** & **0.000 \(\pm\) 0.000** & **0.001 \(\pm\) 0.001** & **0.000 \(\pm\) 0.000** & **0.000 \(\pm\) 0.000** & **0.000 \(\pm\) 0.000** & **0.000 \(\pm\) 0.000** \\ \hline \multirow{2}{*}{APSS} & CCP & 26.676 \(\pm\) 0.171 & 26.111 \(\pm\) 0.194 & 26.626 \(\pm\) 0.133 & 26.159 \(\pm\) 0.208 & 27.313 \(\pm\) 0.154 & 25.629 \(\pm\) 0.207 \\  & Cluster-CP & 25.889 \(\pm\) 0.301 & 25.253 \(\pm\) 0.346 & 26.150 \(\pm\) 0.393 & 25.633 \(\pm\) 0.268 & 26.918 \(\pm\) 0.241 & 25.348 \(\pm\) 0.334 \\  & **RC3P** & **18.129 \(\pm\) 0.003** & **17.082 \(\pm\) 0.002** & **17.784 \(\pm\) 0.003** & **17.465 \(\pm\) 0.003** & **18.111 \(\pm\) 0.002** & **17.167 \(\pm\) 0.004** \\ \hline \multicolumn{7}{c}{Food-101} \\ \multirow{2}{*}{UCR} & CCP & 0.006 \(\pm\) 0.002 & 0.006 \(\pm\) 0.002 & 0.009 \(\pm\) 0.003 & 0.008 \(\pm\) 0.001 & 0.006 \(\pm\) 0.001 & 0.008 \(\pm\) 0.002 \\  & Cluster-CP & 0.003 \(\pm\) 0.002 & 0.009 \(\pm\) 0.003 & 0.004 \(\pm\) 0.001 & 0.009 \(\pm\) 0.002 & 0.011 \(\pm\) 0.003 & 0.011 \(\pm\) 0.002 \\  & **RC3P** & **0.000 \(\pm\) 0.000** & **0.000 \(\pm\) 0.000** & **0.000 \(\pm\) 0.000** & **0.001 \(\pm\) 0.001** & **0.000 \(\pm\) 0.000** & **0.000 \(\pm\) 0.000** \\ \hline \multirow{2}{*}{APSS} & CCP & 27.022 \(\pm\) 0.192 & 30.900 \(\pm\) 0.170 & 30.943 \(\pm\) 0.119 & 35.912 \(\pm\) 0.105 & 27.415 \(\pm\) 0.194 & 36.776 \(\pm\) 0.132 \\  & Cluster-CP & 28.953 \(\pm\) 0.333 & 33.375 \(\pm\) 0.377 & 33.079 \(\pm\) 0.393 & 38.301 \(\pm\) 0.232 &

### Comparison Experiments Using RAPS Score Function

With the same model, evaluation metrics, and RAPS score function [1], we add the comparison experiments with CCP, and Cluster-CP on four datasets with different imbalanced types and imbalance ratio \(\rho=0.5\) and \(\rho=0.1\). The regularization parameter for RAPS scoring function is from the set \(k_{reg}\in\{3,5,7\}\) and \(\lambda\in\{0.001,0.01,0.1\}\). We select the combination of \(k_{reg}\) and \(\lambda\) for each experiment with the same imbalanced type and imbalanced ratio on the same dataset, where most of the \(APSS\) values of all methods are minimum. The overall performance is summarized in Table 7. We highlight that we also select the \(g\) from the set \(g\in\{0.25,0.5,0.75,1.0\}\) to find the minimal \(g\) that CCP, Cluster-CP, and RC3P approximately achieves the target class conditional coverage.

Based on the results in Table 7, we make the following observations: (i) CCP, Cluster-CP, and RC3P can guarantee the class-conditional coverage; and (ii) RC3P significantly outperforms CCP and Cluster-CP on three datasets by producing smaller prediction sets.

### Comparison Experiments Using HPS Score Function

With the same model, evaluation metrics, and HPS score function [1], we add the comparison experiments with CCP, and Cluster-CP on four datasets with different imbalanced types and imbalance ratio \(\rho=0.5\) and \(\rho=0.1\). The overall performance is summarized in Table 8. We highlight that we also select the \(g\) from the set \(g\in\{0.25,0.5,0.75,1.0\}\) to find the minimal \(g\) that CCP, Cluster-CP, and RC3P approximately achieves the target class conditional coverage.

\begin{table}
\begin{tabular}{l c c c c c c c} \hline \hline \multirow{2}{*}{Measure} & \multirow{2}{*}{Methods} & \multicolumn{2}{c}{EXP} & \multicolumn{2}{c}{POLY} & \multicolumn{2}{c}{MAJ} \\  & & \(\rho\) = 0.5 & \(\rho\) = 0.1 & \(\rho\) = 0.5 & \(\rho\) = 0.1 & \(\rho\) = 0.5 & \(\rho\) = 0.1 \\ \hline \multicolumn{8}{c}{CIFAR-10} \\ \hline \multirow{4}{*}{UCR} & CCP & 0.050 \(\pm\) 0.016 & **0.010 \(\pm\) 0.020** & 0.100 \(\pm\) 0.028 & **0.050 \(\pm\) 0.021** & 0.070 \(\pm\) 0.014 & **0.040 \(\pm\) 0.015** \\  & Cluster-CP & **0.010 \(\pm\) 0.009** & **0.010 \(\pm\) 0.010** & **0.080 \(\pm\) 0.019** & 0.060 \(\pm\) 0.015 & **0.020 \(\pm\) 0.025** & 0.070 \(\pm\) 0.014 \\  & **RC3P** & 0.050 \(\pm\) 0.016 & **0.010 \(\pm\) 0.020** & 0.100 \(\pm\) 0.028 & **0.050 \(\pm\) 0.021** & 0.070 \(\pm\) 0.014 & **0.040 \(\pm\) 0.015** \\ \hline \multirow{4}{*}{APSS} & CCP & **1.555 \(\pm\) 0.010** & **1.855 \(\pm\) 0.014** & **1.538 \(\pm\) 0.010** & **1.776 \(\pm\) 0.012** & **1.840 \(\pm\) 0.020** & **2.632 \(\pm\) 0.012** \\  & Cluster-CP & 1.741 \(\pm\) 0.018 & 2.162 \(\pm\) 0.157 & 1.706 \(\pm\) 0.014 & 1.929 \(\pm\) 0.013 & 1.787 \(\pm\) 0.019 & 2.968 \(\pm\) 0.024 \\  & **RC3P** & **1.555 \(\pm\) 0.010** & **1.855 \(\pm\) 0.014** & **1.538 \(\pm\) 0.010** & **1.776 \(\pm\) 0.012** & **1.840 \(\pm\) 0.020** & **2.632 \(\pm\) 0.012** \\ \hline \multicolumn{8}{c}{CIFAR-10} \\ \hline \multirow{4}{*}{UCR} & CCP & 0.007 \(\pm\) 0.002 & **0.011 \(\pm\) 0.002** & 0.010 \(\pm\) 0.002 & **0.015 \(\pm\) 0.003** & 0.015 \(\pm\) 0.003 & 0.008 \(\pm\) 0.004 \\  & Cluster-CP & 0.012 \(\pm\) 0.002 & 0.017 \(\pm\) 0.004 & 0.019 \(\pm\) 0.004 & 0.034 \(\pm\) 0.005 & **0.008 \(\pm\) 0.003** & 0.018 \(\pm\) 0.006 \\  & **RC3P** & **0.005 \(\pm\) 0.002** & **0.011 \(\pm\) 0.002** & **0.009 \(\pm\) 0.003** & **0.015 \(\pm\) 0.003** & 0.005 \(\pm\) 0.003 & **0.008 \(\pm\) 0.004** \\ \hline \multirow{4}{*}{APSS} & CCP & 44.250 \(\pm\) 0.342 & 50.970 \(\pm\) 0.345 & 49.886 \(\pm\) 0.353 & 46.332 \(\pm\) 0.236 & 48.343 \(\pm\) 0.353 & 66.63 \(\pm\) 0.535 \\  & Cluster-CP & 29.267 \(\pm\) 0.612 & 37.795 \(\pm\) 0.862 & 38.258 \(\pm\) 0.320 & 52.374 \(\pm\) 0.592 & 31.513 \(\pm\) 0.325 & 50.379 \(\pm\) 0.684 \\  & **RC3P** & **17.705 \(\pm\) 0.004** & **21.954 \(\pm\) 0.005** & **23.048 \(\pm\) 0.008** & **33.185 \(\pm\) 0.005** & **18.581 \(\pm\) 0.006** & **32.699 \(\pm\) 0.006** \\ \hline \multicolumn{8}{c}{mini-ImageNet} \\ \hline \multirow{4}{*}{UCR} & CCP & 0.008 \(\pm\) 0.003 & 0.009 \(\pm\) 0.004 & 0.005 \(\pm\) 0.002 & 0.004 \(\pm\) 0.002 & 0.009 \(\pm\) 0.003 & 0.005 \(\pm\) 0.002 \\  & Cluster-CP & 0.006 \(\pm\) 0.002 & 0.013 \(\pm\) 0.005 & 0.009 \(\pm\) 0.003 & 0.016 \(\pm\) 0.001 & 0.007 \(\pm\) 0.002 & 0.009 \(\pm\) 0.004 \\  & **RC3P** & **0.000 \(\pm\) 0.000** & **0.001 \(\pm\) 0.001** & **0.000 \(\pm\) 0.000** & **0.000 \(\pm\) 0.000** & **0.000 \(\pm\) 0.000** & **0.000 \(\pm\) 0.000** \\ \hline \multirow{4}{*}{APSS} & CCP & 26.756 \(\pm\) 0.178 & 26.212 \(\pm\) 0.199 & 26.689 \(\pm\) 0.142 & 26.248 \(\pm\) 0.219 & 27.397 \(\pm\) 0.162 & 25.725 \(\pm\) 0.214 \\  & Cluster-CP & 26.027 \(\pm\) 0.325 & 25.415 \(\pm\) 0.289 & 26.2688 \(\pm\) 0.407 & 25.712 \(\pm\) 0.315 & 26.969 \(\pm\) 0.305 & 25.532 \(\pm\) 0.350 \\  & **RC3P** & **18.129 \(\pm\) 0.003** & **17.882 \(\pm\) 0.002** & **17.784 \(\pm\) 0.003** & **17.465 \(\pm\) 0.003** & **18.111 \(\pm\) 0.002** & **17.167 \(\pm\) 0.004** \\ \hline \multicolumn{8}{c}{Food-10} \\ \hline \multirow{4}{*}{UCR} & CCP & 0.006 \(\pm\) 0.003 & 0.006 \(\pm\) 0.002 & 0.009 \(\pm\) 0.003 & 0.008 \(\pm\) 0.001 & 0.006 \(\pm\) 0.002 & 0.0Based on the results in Table 8, we make the following observations: (i) CCP, Cluster-CP, and RC3P can guarantee the class-conditional coverage; and (ii) RC3P significantly outperforms CCP and Cluster-CP on three datasets by producing smaller prediction sets.

### Comparison Experiments with different \(\alpha\) values

With the same model, evaluation metrics, and scoring functions, we add the comparison experiments with CCP, and Cluster-CP on four datasets with different imbalanced types and imbalance ratio \(\rho=0.5\) and \(\rho=0.1\) under the different \(\alpha\) values. The overall performance is summarized in Table 9 and 10, with \(\alpha=0.05\) and \(\alpha=0.01\), respectively. We highlight that we also select the \(g\) from the set \(g\in[0.15,0.75]\) with \(0.05\) range to find the minimal \(g\) that CCP, Cluster-CP, and RC3P approximately achieves the target class conditional coverage.

Based on the results in Table 7, we make the following observations: (i) CCP, Cluster-CP, and RC3P can guarantee the class-conditional coverage; and (ii) RC3P significantly outperforms CCP and Cluster-CP on three datasets by producing smaller prediction sets.

\begin{table}
\begin{tabular}{l c c c c c c c} \hline \hline \multirow{2}{*}{Measure} & \multirow{2}{*}{Methods} & \multicolumn{2}{c}{EXP} & \multicolumn{2}{c}{POLY} & \multicolumn{2}{c}{MAJ} \\  & & \(\rho=0.5\) & \(\rho=0.1\) & \(\rho=0.5\) & \(\rho=0.1\) & \(\rho=0.5\) & \(\rho=0.1\) \\ \hline \multicolumn{8}{c}{CIFAR-10} \\ \multirow{3}{*}{UCR} & \multirow{3}{*}{Cluster-CP} & **0.050 \(\pm\) 0.016** & **0.010 \(\pm\) 0.020** & 0.100 \(\pm\) 0.028 & **0.050 \(\pm\) 0.021** & 0.070 \(\pm\) 0.014 & **0.040 \(\pm\) 0.015** \\  & & **0.010 \(\pm\) 0.009** & **0.010 \(\pm\) 0.010** & **0.080 \(\pm\) 0.019** & 0.060 \(\pm\) 0.015 & **0.020 \(\pm\) 0.025** & 0.070 \(\pm\) 0.014 \\  & **RC3P** & 0.050 \(\pm\) 0.016 & **0.010 \(\pm\) 0.020** & 0.100 \(\pm\) 0.028 & **0.050 \(\pm\) 0.021** & 0.070 \(\pm\) 0.014 & **0.040 \(\pm\) 0.015** \\ \hline \multirow{3}{*}{APSS} & \multirow{3}{*}{CICP} & **1.144 \(\pm\) 0.005** & **1.324 \(\pm\) 0.007** & **1.137 \(\pm\) 0.003** & **1.243 \(\pm\) 0.005** & **1.272 \(\pm\) 0.008** & **1.936 \(\pm\) 0.010** \\  & & 1.214 \(\pm\) 0.008 & 1.508 \(\pm\) 0.010 & 1.211 \(\pm\) 0.004 & 1.354 \(\pm\) 0.005 & 1.336 \(\pm\) 0.009 & 2.312 \(\pm\) 0.025 \\  & **RC3P** & **1.144 \(\pm\) 0.005** & **1.324 \(\pm\) 0.007** & **1.137 \(\pm\) 0.003** & **1.243 \(\pm\) 0.005** & **1.272 \(\pm\) 0.008** & **1.936 \(\pm\) 0.010** \\ \hline \multicolumn{8}{c}{CIFAR-10} \\ \multirow{3}{*}{UCR} & \multirow{3}{*}{CICP} & **0.007 \(\pm\) 0.002** & **0.011 \(\pm\) 0.002** & 0.010 \(\pm\) 0.002 & **0.015 \(\pm\) 0.003** & 0.015 \(\pm\) 0.003 & 0.008 \(\pm\) 0.004 \\  & & 0.012 \(\pm\) 0.002 & 0.017 \(\pm\) 0.004 & 0.019 \(\pm\) 0.004 & 0.034 \(\pm\) 0.005 & **0.008 \(\pm\) 0.003** & 0.018 \(\pm\) 0.006 \\  & **RC3P** & **0.005 \(\pm\) 0.002** & **0.011 \(\pm\) 0.002** & **0.009 \(\pm\) 0.003** & **0.015 \(\pm\) 0.003** & 0.015 \(\pm\) 0.003 & **0.008 \(\pm\) 0.004** \\ \hline \multirow{3}{*}{APSS} & \multirow{3}{*}{CICP} & 41.351 \(\pm\) 0.242 & 49.469 \(\pm\) 0.344 & 48.063 \(\pm\) 0.376 & 63.963 \(\pm\) 0.277 & 46.125 \(\pm\) 0.351 & 64.371 \(\pm\) 0.564 \\  & & 27.566 \(\pm\) 0.555 & 35.528 \(\pm\) 0.979 & 36.101 \(\pm\) 0.565 & 51.333 \(\pm\) 0.776 & 29.323 \(\pm\) 0.363 & 50.519 \(\pm\) 0.679 \\  & **RC3P** & **20.363 \(\pm\) 0.006** & **25.212 \(\pm\) 0.010** & **25.908 \(\pm\) 0.007** & **36.951 \(\pm\) 0.018** & **21.149 \(\pm\) 0.006** & **35.606 \(\pm\) 0.005** \\ \hline \multicolumn{8}{c}{mini-ImageNet} \\ \multirow{3}{*}{UCR} & \multirow{3}{*}{CICP} & 0.008 \(\pm\) 0.003 & 0.009 \(\pm\) 0.004 & 0.005 \(\pm\) 0.002 & 0.004 \(\pm\) 0.002 & 0.009 \(\pm\) 0.003 & 0.005 \(\pm\) 0.002 \\  & & **0.006 \(\pm\) 0.002** & 0.013 \(\pm\) 0.005 & 0.009 \(\pm\) 0.003 & 0.016 \(\pm\) 0.001 & 0.007 \(\pm\) 0.002 & 0.009 \(\pm\) 0.004 \\  & **RC3P** & **0.000 \(\pm\) 0.000** & **0.001 \(\pm\) 0.001** & **0.000 \(\pm\) 0.000** & **0.000 \(\pm\) 0.000** & **0.000 \(\pm\) 0.000** & **0.000 \(\pm\) 0.000** \\ \hline \multirow{3}{*}{APSS} & \multirow{3}{*}{CICP} & 24.633 \(\pm\) 0.212 & 44.467 \(\pm\) 0.149 & 24.379 \(\pm\) 0.152 & 24.472 \(\pm\) 0.167 & 25.449 \(\pm\) 0.196 & 23.885 \(\pm\) 0.159 \\  & & 23.911 \(\pm\) 0.322 & 24.023 \(\pm\) 0.195 & 24.233 \(\pm\) 0.428 & 23.263 \(\pm\) 0.295 & 24.987 \(\pm\) 0.319 & 23.323 \(\pm\) 0.378 \\  & **RC3P** & **17.830 \(\pm\) 0.104** & **17.036 \(\pm\) 0.014** & **17.684 \(\pm\) 0.062** & **17.393 \(\pm\) 0.013** & **18.024 \(\pm\) 0.049** & **17.086 \(\pm\) 0.059** \\ \hline \multicolumn{8}{c}{Food-101} \\ \multirow{3}{*}{UCR} & \multirow{3}{*}{CICP} & 0.006 \(\pm\) 0.003 & 0.006 \(\pm\) 0.002 & 0.009 \(\pm\) 0.003 & 0.008 \(\pm\) 0.001 & 0.006 \(\pm\) 0.002 & 0.008 \(\pm\) 0.002 \\  & & 0.004 \(\pm\) 0.003 & 0.012 \(\pm\) 0.004 & 0.006 \(\pm\) 0.002 & 0.006 \(\pm\) 0.003 & 0.011 \(\pm\) 0

[MISSING_PAGE_FAIL:23]

\begin{table}
\begin{tabular}{c c c c c c c c} \hline \hline \multirow{2}{*}{Conformity Score} & \multirow{2}{*}{Methods} & \multicolumn{2}{c}{EXP} & \multicolumn{2}{c}{POLY} & \multicolumn{2}{c}{MAJ} \\  & & \(\rho\) = 0.5 & \(\rho\) = 0.1 & \(\rho\) = 0.5 & \(\rho\) = 0.1 & \(\rho\) = 0.5 & \(\rho\) = 0.1 \\ \hline \multicolumn{7}{c}{CIFAR-10} \\ \hline \multirow{3}{*}{APS} & CCP & 7.250 \(\pm\) 0.164 & **7.387 \(\pm\) 0.116** & 7.173 \(\pm\) 0.079 & 7.596 \(\pm\) 0.109 & 7.392 \(\pm\) 0.128 & **8.564 \(\pm\) 0.108** \\  & Cluster-CP & **5.528 \(\pm\) 0.103** & 8.332 \(\pm\) 0.060 & 6.954 \(\pm\) 0.084 & 7.762 \(\pm\) 0.143 & 7.586 \(\pm\) 0.113 & 9.308 \(\pm\) 0.054 \\  & **RC3P** & 5.671 \(\pm\) 0.046 & **7.387 \(\pm\) 0.116** & **6.309 \(\pm\) 0.042** & **7.276 \(\pm\) 0.010** & **6.779 \(\pm\) 0.013** & **8.864 \(\pm\) 0.108** \\ \hline \multirow{3}{*}{RAPS} & CCP & 7.294 \(\pm\) 0.160 & **7.458 \(\pm\) 0.101** & 7.067 \(\pm\) 0.106 & 7.597 \(\pm\) 0.096 & 7.547 \(\pm\) 0.134 & **8.884 \(\pm\) 0.106** \\  & Cluster-CP & **5.568 \(\pm\) 0.103** & 8.288 \(\pm\) 0.118 & 6.867 \(\pm\) 0.078 & 7.795 \(\pm\) 0.136 & 7.813 \(\pm\) 0.142 & 9.239 \(\pm\) 0.055 \\  & **RC3P** & 5.673 \(\pm\) 0.040 & **7.458 \(\pm\) 0.101** & **6.310 \(\pm\) 0.046** & **7.253 \(\pm\) 0.006** & **6.780 \(\pm\) 0.015** & **8.884 \(\pm\) 0.106** \\ \hline \multicolumn{7}{c}{CIFAR-100} \\ \hline \multirow{3}{*}{APS} & CCP & 100.0 \(\pm\) 0.0 & 100.0 \(\pm\) 0.0 & 100.0 \(\pm\) 0.0 & 100.0 \(\pm\) 0.0 & **100.0 \(\pm\) 0.0** & **100.0 \(\pm\) 0.0** \\  & Cluster-CP & 65.523 \(\pm\) 0.495 & 69.063 \(\pm\) 0.512 & 67.012 \(\pm\) 0.739 & 81.997 \(\pm\) 0.390 & **100.0 \(\pm\) 0.0** & **100.0 \(\pm\) 0.0** \\  & **RC3P** & **55.621 \(\pm\) 0.007** & **63.039 \(\pm\) 0.007** & **60.258 \(\pm\) 0.005** & **74.927 \(\pm\) 0.007** & **100.0 \(\pm\) 0.0** & **100.0 \(\pm\) 0.0** \\ \hline \multirow{3}{*}{RAPS} & CCP & 100.0 \(\pm\) 0.0 & 100.0 \(\pm\) 0.0 & 100.0 \(\pm\) 0.0 & 100.0 \(\pm\) 0.0 & **100.0 \(\pm\) 0.0** & **100.0 \(\pm\) 0.0** \\  & Cluster-CP & 65.584 \(\pm\) 0.508 & 69.373 \(\pm\) 0.466 & 66.313 \(\pm\) 0.745 & 82.043 \(\pm\) 0.439 & **100.0 \(\pm\) 0.0** & **100.0 \(\pm\) 0.0** \\  & **RC3P** & **55.632 \(\pm\) 0.008** & **63.021 \(\pm\) 0.006** & **60.205 \(\pm\) 0.006** & **74.885 \(\pm\) 0.006** & **100.0 \(\pm\) 0.0** & **100.0 \(\pm\) 0.0** \\ \hline \multicolumn{7}{c}{mini-ImageNet} \\ \hline \multirow{3}{*}{APS} & CCP & 100.0 \(\pm\) 0.0 & 100.0 \(\pm\) 0.0 & 100.0 \(\pm\) 0.0 & 100.0 \(\pm\) 0.0 & 100.0 \(\pm\) 0.0 & 100.0 \(\pm\) 0.0 \\  & Cluster-CP & 74.019 \(\pm\) 0.699 & 71.300 \(\pm\) 0.674 & 75.546 \(\pm\) 0.683 & 70.996 \(\pm\) 0.702 & 74.508 \(\pm\) 0.531 & 72.803 \(\pm\) 0.536 \\  & **RC3P** & **55.321 \(\pm\) 0.003** & **54.214 \(\pm\) 0.004** & **56.018 \(\pm\) 0.006** & **53.732 \(\pm\) 0.004** & **54.483 \(\pm\) 0.007** & **53.522 \(\pm\) 0.005** \\ \hline \multirow{3}{*}{RAPS} & CCP & 100.0 \(\pm\) 0.0 & 100.0 \(\pm\) 0.0 & 100.0 \(\pm\) 0.0 & 100.0 \(\pm\) 0.0 & 100.0 \(\pm\) 0.0 & 100.0 \(\pm\) 0.0 \\  & Cluster-CP & 73.893 \(\pm\) 0.734 & 70.638 \(\pm\) 0.657 & 75.546 \(\pm\) 0.683 & 71.098 \(\pm\) 0.706 & 74.675 \(\pm\) 0.578 & 73.345 \(\pm\) 0.474 \\  & **RC3P** & **55.270 \(\pm\) 0.003** & **54.184 \(\pm\) 0.003** & **56.733 \(\pm\) 0.006** & **53.736 \(\pm\) 0.004** & **55.304 \(\pm\) 0.004** & **53.532 \(\pm\) 0.005** \\ \hline \multicolumn{7}{c}{Food-101} \\ \hline \multirow{3}{*}{APS} & CCP & 101.0 \(\pm\) 0.0 & 101.0 \(\pm\) 0.0 & 101.0 \(\pm\) 0.0 & 101.0 \(\pm\) 0.0 & 101.0 \(\pm\) 0.0 & 101.0 \(\pm\) 0.0 \\  & Cluster-CP & 81.489 \(\pm\) 0.957 & 87.092 \(\pm\) 0.588 & 82.257 \(\pm\) 0.514 & 86.539 \(\pm\) 0.453 & 83.293 \(\pm\) 0.583 & 88.603 \(\pm\) 0.401 \\  & **RC3P** & **67.443 \(\pm\) 0.004** & **57.055 \(\pm\) 0.005** & **57.722 \(\pm\) 0.006** & **62.931 \(\pm\) 0.005** & **68.267 \(\pm\) 0.005** & **65.413 \(\pm\) 0.005** \\ \hline \multirow{3}{*}{RAPS} & CCP & 101.0 \(\pm\) 0.0 & 101.0 \(\pm\) 0.0 & 101.0 \(\pm\) 0.0 & 101.0 \(\pm\) 0.0 & 101.0 \(\pm\) 0.0 & 101.0 \(\pm\) 0.0 \\  & Cluster-CP & 81.505 \(\pm\) 0.955 & 87.103 \(\pm\) 0.587 & 82.272 \(\pm\) 0.513 & 86.517 \(\pm\) 0.455 & 83.367 \(\pm\) 0.635 & 88.604 \(\pm\)

[MISSING_PAGE_FAIL:25]

[MISSING_PAGE_FAIL:26]

[MISSING_PAGE_FAIL:27]

[MISSING_PAGE_FAIL:28]

\begin{table}
\begin{tabular}{c c c c c c c c} \hline \hline Scoring function & Measure & Methods & \(\rho=0.5\) & \(\rho=0.4\) & \(\rho=0.3\) & \(\rho=0.2\) & \(\rho=0.1\) \\ \hline \multirow{4}{*}{APS} & \multirow{4}{*}{UCR} & CCP & 0.001\(\pm\) 0.002 & 0.008\(\pm\) 0.002 & 0.016\(\pm\) 0.003 & 0.012\(\pm\) 0.004 & **0.014\(\pm\) 0.003** \\  & & Cluster-CP & 0.020\(\pm\) 0.003 & 0.020\(\pm\) 0.002 & 0.026\(\pm\) 0.004 & **0.009\(\pm\) 0.003** & 0.034\(\pm\) 0.005 \\  & & **RC3P** & **0.009\(\pm\) 0.003** & **0.005\(\pm\) 0.002** & **0.013\(\pm\) 0.004** & **0.011\(\pm\) 0.004** & 0.015\(\pm\) 0.003 \\ \cline{2-8}  & \multirow{4}{*}{APSS} & CCP & 49.889\(\pm\) 0.353 & 54.011\(\pm\) 0.466 & 56.031\(\pm\) 0.406 & 59.888\(\pm\) 0.255 & 64.343\(\pm\) 0.237 \\  & & Cluster-CP & 38.252\(\pm\) 0.316 & 39.585\(\pm\) 0.545 & 43.310\(\pm\) 0.824 & 47.461\(\pm\) 0.979 & 52.391\(\pm\) 0.595 \\  & & **RC3P** & **23.048\(\pm\) 0.008** & **24.335\(\pm\) 0.005** & **26.366\(\pm\) 0.010** & **28.887\(\pm\) 0.006** & **33.829\(\pm\) 0.005** \\ \hline \multirow{4}{*}{RAPS} & \multirow{4}{*}{UCR} & CCP & 0.010\(\pm\) 0.002 & 0.008\(\pm\) 0.002 & 0.016\(\pm\) 0.003 & 0.012\(\pm\) 0.004 & **0.015\(\pm\) 0.003** \\  & & Cluster-CP & 0.019\(\pm\) 0.004 & 0.020\(\pm\) 0.002 & 0.026\(\pm\) 0.005 & **0.009\(\pm\) 0.003** & 0.034\(\pm\) 0.005 \\ \cline{2-8}  & \multirow{4}{*}{APSS} & CCP & **0.009\(\pm\) 0.003** & **0.005\(\pm\) 0.002** & **0.013\(\pm\) 0.004** & 0.011\(\pm\) 0.004 & **0.015\(\pm\) 0.003** \\ \cline{2-8}  & & CCP & 49.886\(\pm\) 0.353 & 53.994\(\pm\) 0.467 & 56.020\(\pm\) 0.406 & 59.870\(\pm\) 0.253 & 64.332\(\pm\) 0.236 \\ \cline{2-8}  & \multirow{4}{*}{APSS} & CCP & 38.258\(\pm\) 0.320 & 39.566\(\pm\) 0.549 & 43.304\(\pm\) 0.549 & 47.450\(\pm\) 0.969 & 52.374\(\pm\) 0.592 \\ \cline{2-8}  & & **RC3P** & **23.048\(\pm\) 0.008** & **24.335\(\pm\) 0.005** & **26.366\(\pm\) 0.010** & **28.886\(\pm\) 0.006** & **33.185\(\pm\) 0.005** \\ \hline \hline \end{tabular}
\end{table}
Table 17: Results comparing CCP, cluster-CP, and RC3P with ResNet-20 model under different imbalance ratio \(\rho=0.5\), \(\rho=0.4\), \(\rho=0.2\), and \(\rho=0.1\) with imbalance type POLY and two scoring functions, APS and RAPS, on dataset CIFAR-100. We set UCR of RC3P the same as or better than that of CCP and Cluster-CP for a fair comparison of prediction set size.

\begin{table}
\begin{tabular}{c c c c c c c c} \hline \hline Scoring function & Measure & Methods & \(\rho=0.5\) & \(\rho=0.4\) & \(\rho=0.3\) & \(\rho=0.2\) & \(\rho=0.1\) \\ \hline \multirow{4}{*}{APS} & \multirow{4}{*}{UCR} & CCP & 0.016\(\pm\) 0.003 & **0.007\(\pm\) 0.002** & 0.017\(\pm\) 0.004 & **0.010\(\pm\) 0.002** & **0.008\(\pm\) 0.004** \\  & & Cluster-CP & **0.008\(\pm\) 0.002** & 0.012\(\pm\) 0.003 & 0.021\(\pm\) 0.004 & 0.021\(\pm\) 0.005 & 0.019\(\pm\) 0.005 \\  & & **RC3P** & 0.016\(\pm\) 0.003 & 0.010\(\pm\) 0.003 & **0.015\(\pm\) 0.004** & **0.010\(\pm\) 0.002** & **0.008\(\pm\) 0.004** \\ \cline{2-8}  & \multirow{4}{*}{APSS} & CCP & 44.194\(\pm\) 0.514 & 49.231\(\pm\) 0.129 & 73.676\(\pm\) 0.372 & 55.024\(\pm\) 0.254 & 64.642\(\pm\) 0.535 \\  & & Cluster-CP & **0.015**\(\pm\) 0.335 & 35.355\(\pm\) 0.563 & 37.514\(\pm\) 0.538 & 43.619\(\pm\) 0.600 & 50.883\(\pm\) 0.673 \\  & & **RC3P** & **18.581\(\pm\) 0.007** & **21.080\(\pm\) 0.010** & **22.606\(\pm\) 0.007** & **26.785\(\pm\) 0.007** & **32.699\(\pm\) 0.005** \\ \hline \multirow{4}{*}{RAPS} & \multirow{4}{*}{UCR} & CCP & 0.015\(\pm\) 0.003 & **0.007\(\pm\) 0.002** & 0.011\(\pm\) 0.004 & **0.010\(\pm\) 0.003** & **0.008\(\pm\) 0.004** \\  & & Cluster-CP & **0.008\(\pm\) 0.003** & 0.011\(\pm\) 0.003 & 0.021\(\pm\) 0.004 & 0.021\(\pm\) 0.002 & 0.018\(\pm\) 0.005 \\ \cline{2-8}  & \multirow{4}{*}{APSS} & CCP & 0.015\(\pm\) 0.003 & 0.010\(\pm\) 0.003 & **0.015\(\pm\) 0.004** & **0.010\(\pm\) 0.002** & **0.008\(\pm\) 0.004** \\ \cline{2-8}  & & **RC3P** & 0.015\(\pm\) 0.003 & 0.010\(\pm\) 0.003 & **0.015\(\pm\) 0.004** & **0.010\(\pm\) 0.002** & **0.008\(\pm\) 0.004** \\ \cline{2-8}  & \multirow{4}{*}{APSS} & CCP & 48.343\(\pm\) 0.353 & 49.252\(\pm\) 0.128 & 5.666\(\pm\) 0.371 & 55.016\(\pm\) 0.254 & 64.633\(\pm\) 0.535 \\ \cline{2-8}  & & **RC3P** & **18.293\(\pm\) 0.003** & **17.546\(\pm\) 0.002** & **17.352\(\pm\) 0.003** & **17.006\(\pm\) 0.003** & **17.082\(\pm\) 0.002** \\ \hline \hline \end{tabular}
\end{table}
Table 18: Results comparing CCP, cluster-CP, and RC3P with ResNet-20 model under different imbalance ratio \(\rho=0.5\), \(\rho=0.4\), \(\rho=0.2\), and \(\rho=0.1\) with imbalance type MAJ and two scoring functions, APS and RAPS, on dataset CIFAR-100. We set UCR of RC3P the same as or better than that of CCP and Cluster-CP for a fair comparison of prediction set size.

\begin{table}
\begin{tabular}{c c c c c c c c c} \hline \hline Scoring function & Measure & Methods & \multicolumn{5}{c}{EXP} \\  & & & \(\rho\) = 0.5 & \(\rho\) = 0.4 & \(\rho\) = 0.3 & \(\rho\) = 0.2 & \(\rho\) = 0.1 \\ \hline \multirow{4}{*}{APS} & \multirow{4}{*}{UCR} & CCP & 0.005\(\pm\) 0.002 & 0.004\(\pm\) 0.002 & 0.005\(\pm\) 0.002 & 0.002\(\pm\) 0.001 & 0.004\(\pm\) 0.001 \\  & & Cluster-CP & 0.011\(\pm\) **0.003** & 0.013\(\pm\) 0.003 & 0.015\(\pm\) 0.004 & 0.012\(\pm\) 0.003 & 0.014\(\pm\) 0.003 \\  & & **RC3P** & **0.0.0**\(\pm\) **0.0** & **0.0**\(\pm\) **0.0** & **0.0**\(\pm\) **0.0** & **0.0**\(\pm\) **0.0** & **0.0**\(\pm\) **0.0** \\ \cline{2-8}  & \multirow{4}{*}{APSS} & CCP & 26.626\(\pm\) 0.133 & 26.343\(\pm\) 0.214 & 27.168\(\pm\) 0.203 & 27.363\(\pm\) 0.252 & 26.159\(\pm\) 0.208 \\  & & Cluster-CP & 26.150\(\pm\) 0.393 & 25.348\(\pm\) 0.231 & 26.132\(\pm\) 0.415 & 26.390\(\pm\) 0.270 & 25.633\(\pm\) 0.268 \\  & & **RC3P** & **17.784\(\pm\) 0.003** & **17.752\(\pm\) 0.003** & **17.652\(\pm\) 0.003** & **17.629\(\pm\) 0.003** & **17.465\(\pm\) 0.003** \\ \hline \multirow{4}{*}{RAPS} & \multirow{4}{*}{UCR} & CCP & 0.005\(\pm\) 0.002 & 0.004\(\pm\) 0.002 & 0.005\(\pm\) 0.002 & 0.002\(\pm\) 0.001 & 0.004\(\pm\) 0.002 \\  & & Cluster-CP & 0.009\(\pm\) 0.003 & 0.016\(\pm\) 0.004 & 0.017\(\pm\) 0.004 & 0.009\(\pm\) 0.003 & 0.016\(\pm\) 0.003 \\  & & **RC3P** & **0.0**\(\pm\) **0.0** & **0.0**\(\pm\) **0.0** & **0.0**\(\pm\) **0.0** & **0.0**\(\pm\) **0.0** & **0.0**\(\pm\) **0.0** \\ \cline{2-8}  & \multirow{4}{*}{APSS} & CCP & 26.689\(\pm\) 0.142 & 26.437\(\pm\) 0.213 & 27.254\(\pm\) 0.201 & 27.450\(\pm\) 0.249 & 26.248\(\pm\) 0.219 \\  & & Cluster-CP & 26.288\(\pm\) 0.407 & 25.627\(\pm\) 0.318 & 26.220\(\pm\) 0.432 & 26.559\(\pm\) 0.242 & 25.712\(\pm\) 0.315 \\  & & **RC3P** & **17.784\(\pm\) 0.003** & **17.752\(\pm\) 0.003** & **17.652\(\pm\) 0.003** & **17.629\(\pm\) 0.003** & **17.465\(\pm\) 0.003** \\ \hline \hline \end{tabular}
\end{table}
Table 21: Results comparing CCP, cluster-CP, and RC3P with ResNet-20 model under different imbalance ratio \(\rho=0.5\), \(\rho=0.4\), \(\rho=0.2\), and \(\rho=0.1\) with imbalance type MAJ and two scoring function, APS and RAPS, on dataset mini-ImageNet. We set UCR of RC3P the same as or better than that of CCP and Cluster-CP for a fair comparison of prediction set size.

\begin{table}
\begin{tabular}{c c c c c c c c} \hline \hline Scoring function & Measure & Methods & \multicolumn{5}{c}{EXP} \\  & & & \(\rho\) = 0.5 & \(\rho\) = 0.4 & \(\rho\) = 0.3 & \(\rho\) = 0.2 & \(\rho\) = 0.1 \\ \hline \multirow{4}{*}{APS} & \multirow{4}{*}{UCR} & CCP & 0.006\(\pm\) 0.002 & 0.010\(\pm\) 0.002 & 0.008\(\pm\) 0.002 & 0.014\(\pm\) 0.004 & 0.006\(\pm\) 0.002 \\  & & Cluster-CP & 0.003\(\pm\) 0.002 & 0.009\(\pm\) 0.003 & 0.006\(\pm\) 0.003 & 0.008\(\pm\) 0.003 & 0.009\(\pm\) 0.003 \\  & & **RC3P** & **0.0**\(\pm\) **0.0** & **0.0**\(\pm\) **0.0** & **0.0**\(\pm\) **0.0** & **0.0**\(\pm\) **0.0** & **0.0**\(\pm\) **0.0** \\ \cline{2-8}  & \multirow{4}{*}{APSS} & CCP & 27.003\(\pm\) 0.183 & 27.024\(\pm\) 0.162 & 28.074\(\pm\) 0.199 & 28.512\(\pm\) 0.154 & 30.875\(\pm\) 0.163 \\  & & Cluster-CP & 29.020\(\pm\) 0.281 & 30.120\(\pm\) 0.440 & 30.529\(\pm\) 0.381 & 31.096\(\pm\) 0.350 & 33.327\(\pm\) 0.440 \\  & & **RC3P** & **18.369\(\pm\) 0.003** & **18.339\(\pm\) 0.004** & **18.803\(\pm\) 0.003** & **19.612\(\pm\) 0.005** & **21.556\(\pm\) 0.006** \\ \hline \multirow{4}{*}{RAPS} & \multirow{4}{*}{UCR} & CCP & 0.006\(\pm\) 0.003 & 0.010\(\pm\) 0.002 & 0.008\(\pm\) 0.002 & 0.014\(\pm\) 0.004 & 0.006\(\pm\) 0.002 \\  & & Cluster-CP & 0.004\(\pm\) 0.003 & 0.010\(\pm\) 0.003 & 0.006\(\pm\) 0.003 & 0.010\(\pm\) 0.002 & 0.012\(\pm\) 0.004 \\  & & **RC3P** & **0.0**\(\pm\)**(**0.0)** & **0.0**\(\pm\)**(**0.0)** & **0.0**\(\pm\)**(**0.0)** & **0.0**\(\pm\)**(**0.0)** & **0.0**\(\pm\)**(**0.0)** \\ \cline{2-8}  & \multirow{4}{*}{APSS} & CCP & 27.022\(\pm\) 0.192 & 27.043\(\pm\) 0.163 & 28.098\(\pm\) 0.199 & 28.535\(\pm\) 0.155 & 30.900\(\pm\) 0.170 \\ \cline{2-8}  & \multirow{4}{*}{APSS} & CCP & 28.953\(\pm\) 0.333 & 30.242\(\pm\) 0.466 & 30.587\(\pm\) 0.377 & 30.924\(\pm\) 0.317 & 33.375\(\pm\) 0.377 \\ \cline{2-8}  & & **RC3P** & **18.369\(\pm\) 0.004** & **18.339\(\pm\) 0.004** & **18.803\(\pm\) 0.003** & **19.612\(\pm\) 0.005** & **21.556\(\pm\) 0.006** \\ \hline \hline \end{tabular}
\end{table}
Table 22: Results comparing CCP, cluster-CP, and RC3P with ResNet-20 model under different imbalance ratio \(\rho=0.5\), \(\rho=0.4\), \(\rho=0.2\), and \(\rho=0.1\) with imbalance type EXP and two scoring function, APS and RAPS, on dataset Food-101. We set UCR of RC3P the same as or better than that of CCP and Cluster-CP for a fair comparison of prediction set size.

\begin{table}
\begin{tabular}{c c c c c c c c c} \hline \hline Scoring function & Measure & Methods & & & & & MAJ & & \\  & & & \(\rho\) = 0.5 & \(\rho\) = 0.4 & \(\rho\) = 0.3 & \(\rho\) = 0.2 & \(\rho\) = 0.1 \\ \hline \multirow{4}{*}{APS} & \multirow{4}{*}{UCR} & CCP & 0.006\(\pm\) 0.001 & 0.005\(\pm\) 0.002 & 0.008\(\pm\) 0.003 & 0.010\(\pm\) 0.002 & 0.008\(\pm\) 0.002 \\  & & Cluster-CP & 0.011\(\pm\) 0.003 & 0.005\(\pm\) 0.002 & 0.014\(\pm\) 0.004 & 0.016\(\pm\) 0.004 & 0.011\(\pm\) 0.002 \\  & & **RC3P** & **0.0\(\pm\) 0.0** & **0.0\(\pm\) 0.0** & **0.0\(\pm\) 0.0** & **0.0\(\pm\) 0.0** & **0.0\(\pm\) 0.0** \\ \cline{2-8}  & \multirow{4}{*}{APSS} & \multirow{4}{*}{UCR} & CCP & 27.415\(\pm\) 0.194 & 29.369\(\pm\) 0.120 & 30.672\(\pm\) 0.182 & 31.966\(\pm\) 0.165 & 36.776\(\pm\) 0.132 \\  & & Cluster-CP & 30.071\(\pm\) 0.412 & 31.656\(\pm\) 0.261 & 32.857\(\pm\) 0.469 & 33.774\(\pm\) 0.494 & 39.632\(\pm\) 0.342 \\  & & **RC3P** & **19.398\(\pm\) 0.006** & **20.046\(\pm\) 0.004** & **21.425\(\pm\) 0.003** & **22.175\(\pm\) 0.004** & **26.585\(\pm\) 0.004** \\ \hline \multirow{4}{*}{RAPSS} & \multirow{4}{*}{UCR} & CCP & 0.006\(\pm\) 0.002 & 0.005\(\pm\) 0.002 & 0.008\(\pm\) 0.003 & 0.010\(\pm\) 0.002 & 0.008\(\pm\) 0.002 \\  & & Cluster-CP & 0.011\(\pm\) 0.003 & 0.005\(\pm\) 0.002 & 0.013\(\pm\) 0.004 & 0.014\(\pm\) 0.004 & 0.014\(\pm\) 0.004 \\  & & **RC3P** & **0.0\(\pm\)(0.0)** & **0.0\(\pm\)(0.0)** & **0.0\(\pm\)(0.0)** & **0.0\(\pm\)(0.0)** & **0.0\(\pm\)(0.0)** \\ \cline{2-8}  & \multirow{4}{*}{APSS} & \multirow{4}{*}{UCR} & CCP & 27.439\(\pm\) 0.203 & 29.393\(\pm\) 0.120 & 30.691\(\pm\) 0.182 & 31.987\(\pm\) 0.165 & 36.802\(\pm\) 0.138 \\  & & Cluster-CP & 29.946\(\pm\) 0.407 & 31.409\(\pm\) 0.303 & 32.724\(\pm\) 0.551 & 33.686\(\pm\) 0.501 & 39.529\(\pm\) 0.306 \\  & & **RC3P** & **19.397\(\pm\) 0.006** & **20.046\(\pm\) 0.004** & **21.425\(\pm\) 0.003** & **22.175\(\pm\) 0.004** & **26.585\(\pm\) 0.004** \\ \hline \hline \end{tabular}
\end{table}
Table 23: Results comparing CCP, cluster-CP, and RC3P with ResNet-20 model under different imbalance ratio \(\rho=0.5\), \(\rho=0.4\), \(\rho=0.2\), and \(\rho=0.1\) with imbalance type POLY and two scoring function, APS and RAPS, on dataset Food-101. We set UCR of RC3P the same as or better than that of CCP and Cluster-CP for a fair comparison of prediction set size.

\begin{table}
\begin{tabular}{c c c c c c c c} \hline \hline Scoring function & Measure & Methods & & & POLY & \\  & & & \(\rho\) = 0.5 & \(\rho\) = 0.4 & \(\rho\) = 0.3 & \(\rho\) = 0.2 & \(\rho\) = 0.1 \\ \hline \multirow{4}{*}{APS} & \multirow{4}{*}{UCR} & CCP & 0.009\(\pm\) 0.003 & 0.005\(\pm\) 0.003 & 0.009\(\pm\) 0.003 & 0.011\(\pm\) 0.003 & 0.008\(\pm\) 0.001 \\  & & Cluster-CP & 0.004\(\pm\) 0.001 & 0.012\(\pm\) 0.002 & 0.012\(\pm\) 0.004 & 0.011\(\pm\) 0.002 & 0.009\(\pm\) 0.002 \\  & & **RC3P** & **0.0\(\pm\) 0.0** & **0.0\(\pm\) 0.0** & **0.0\(\pm\) 0.0** & **0.0\(\pm\) 0.0** & **0.001\(\pm\) 0.001** \\ \cline{2-8}  & \multirow{4}{*}{APSS} & CCP & 30.943\(\pm\) 0.119 & 31.239\(\pm\) 0.198 & 32.283\(\pm\) 0.169 & 33.570\(\pm\) 0.163 & 35.912\(\pm\) 0.105 \\  & & Cluster-CP & 33.079\(\pm\) 0.393 & 33.951\(\pm\) 0.531 & 34.626\(\pm\) 0.352 & 36.546\(\pm\) 0.490 & 38.301\(\pm\) 0.232 \\  & & **RC3P** & **21.499\(\pm\) 0.003** & **21.460\(\pm\) 0.005** & **22.882\(\pm\) 0.005** & **23.708\(\pm\) 0.004** & **25.853\(\pm\) 0.004** \\ \hline \multirow{4}{*}{RAPSS} & \multirow{4}{*}{UCR} & CCP & 0.009\(\pm\) 0.003 & 0.006\(\pm\) 0.003 & 0.009\(\pm\) 0.003 & 0.011\(\pm\) 0.003 & 0.008\(\pm\) 0.001 \\  & & Cluster-CP & 0.006\(\pm\) 0.002 & 0.013\(\pm\) 0.002 & 0.012\(\pm\) 0.004 & 0.016\(\pm\) 0.002 & 0.006\(\pm\) 0.003 \\ \cline{1-1}  & & **RC3P** & **0.0\(\pm\) 0.0** & **0.0\(\pm\) 0.0** & **0.0\(\pm\) 0.0** & **0.0\(\pm\) 0.0** & **0.001\(\pm\) 0.001** \\ \cline{1-1} \cline{2-8}  & \multirow{4}{*}{APSS} & CCP & 30.966\(\pm\) 0.125 & 31.257\(\pm\) 0.197 & 32.302\(\pm\) 0.169 & 33.595\(\pm\) 0.164 & 35.940\(\pm\) 0.111 \\ \cline{1-1}  & & Cluster-CP & 33.337\(\pm\) 0.409 & 33.936\(\pm\) 0.448 & 34.878\(\pm\) 0.282 & 36.505\(\pm\) 0.520 & 38.499\(\pm\) 0.216 \\ \cline{1-1}  & & **RC3P** & **21.499\(\pm\) 0.003** & **21.460\(\pm\) 0.005** & **22.882\(\pm\) 0.005** & **23.708\(\pm\) 0.004** & **25.853\(\pm\) 0.004** \\ \hline \hline \end{tabular}
\end{table}
Table 24: Results comparing CCP, cluster-CP, and RC3P with ResNet-20 model under different imbalance ratio \(\rho=0.5\), \(\rho=0.4\), \(\rho=0.2\), and \(\rho=0.1\) with imbalance type MAJ and two scoring function, APS and RAPS, on dataset Food-101. We set UCR of RC3P the same as or better than that of CCP and Cluster-CP for a fair comparison of prediction set size.

Figure 5: Class-conditional coverage (Top row) and prediction set size (Bottom row) achieved by CCP, Cluster-CP, and RC3P methods when \(\alpha=0.1\) on CIFAR-10, CIFAR-100, mini-ImageNet, and Food-101 datasets with imbalance type EXP for imbalance ratio \(\rho=0.5\). We clarify that RC3P overlaps with CCP on CIFAR-10. It is clear that RC3P has more densely distributed class-conditional coverage above \(0.9\) (the target \(1-\alpha\) class-conditional coverage) than CCP and Cluster-CP with significantly smaller prediction sets on CIFAR-100, mini-ImageNet and Food-101.

Figure 6: Class-conditional coverage (Top row) and prediction set size (Bottom row) achieved by CCP, Cluster-CP, and RC3P methods when \(\alpha=0.1\) on CIFAR-10, CIFAR-100, mini-ImageNet, and Food-101 datasets with imbalance type POLY for imbalance ratio \(\rho=0.1\). We clarify that RC3P overlaps with CCP on CIFAR-10. It is clear that RC3P has more densely distributed class-conditional coverage above 0.9 (the target \(1-\alpha\) class-conditional coverage) than CCP and Cluster-CP with significantly smaller prediction sets on CIFAR-100, mini-ImageNet and Food-101.

Figure 8: Class-conditional coverage (Top row) and prediction set size (Bottom row) achieved by CCP, Cluster-CP, and RC3P methods when \(\alpha=0.1\) on CIFAR-10, CIFAR-100, mini-ImageNet, and Food-101 datasets with imbalance type MAJ for imbalance ratio \(\rho=0.1\). We clarify that RC3P overlaps with CCP on CIFAR-10. It is clear that RC3P has more densely distributed class-conditional coverage above \(0.9\) (the target \(1-\alpha\) class-conditional coverage) than CCP and Cluster-CP with significantly smaller prediction sets on CIFAR-100, mini-ImageNet and Food-101.

Figure 7: Class-conditional coverage (Top row) and prediction set size (Bottom row) achieved by CCP, Cluster-CP, and RC3P methods when \(\alpha=0.1\) on CIFAR-10, CIFAR-100, mini-ImageNet, and Food-101 datasets with imbalance type POLY for imbalance ratio \(\rho=0.5\). We clarify that RC3P overlaps with CCP on CIFAR-10. It is clear that RC3P has more densely distributed class-conditional coverage above \(0.9\) (the target \(1-\alpha\) class-conditional coverage) than CCP and Cluster-CP with significantly smaller prediction sets on CIFAR-100, mini-ImageNet and Food-101.

Figure 11: Visualization for the normalized frequency distribution of label ranks included in the prediction set of CCP, Cluster-CP, and RC3P with \(\rho=0.1\) POLY when \(\alpha=0.1\). It is clear that the distribution of normalized frequency generated by RC3P tends to be lower compared to those produced by CCP and Cluster-CP. Furthermore, the probability density function tail for label ranks in the RC3P prediction set is notably shorter than that of other methods.

Figure 10: Visualization for the normalized frequency distribution of label ranks included in the prediction set of CCP, Cluster-CP, and RC3P with \(\rho=0.5\) EXP when \(\alpha=0.1\). It is clear that the distribution of normalized frequency generated by RC3P tends to be lower compared to those produced by CCP and Cluster-CP. Furthermore, the probability density function tail for label ranks in the RC3P prediction set is notably shorter than that of other methods.

Figure 9: Class-conditional coverage (Top row) and prediction set size (Bottom row) achieved by CCP, Cluster-CP, and RC3P methods when \(\alpha=0.1\) on CIFAR-10, CIFAR-100, mini-ImageNet, and Food-101 datasets with imbalance type MAJ for imbalance ratio \(\rho=0.5\). We clarify that RC3P overlaps with CCP on CIFAR-10. It is clear that RC3P has more densely distributed class-conditional coverage above \(0.9\) (the target \(1-\alpha\) class-conditional coverage) than CCP and Cluster-CP with significantly smaller prediction sets on CIFAR-100, mini-ImageNet and Food-101.

Figure 14: Visualization for the normalized frequency distribution of label ranks included in the prediction set of CCP, Cluster-CP, and RC3P with \(\rho=0.5\) MAJ when \(\alpha=0.1\). It is clear that the distribution of normalized frequency generated by RC3P tends to be lower compared to those produced by CCP and Cluster-CP. Furthermore, the probability density function tail for label ranks in the RC3P prediction set is notably shorter than that of other methods.

Figure 12: Visualization for the normalized frequency distribution of label ranks included in the prediction set of CCP, Cluster-CP, and RC3P with \(\rho=0.5\) POLY when \(\alpha=0.1\). It is clear that the distribution of normalized frequency generated by RC3P tends to be lower compared to those produced by CCP and Cluster-CP. Furthermore, the probability density function tail for label ranks in the RC3P prediction set is notably shorter than that of other methods.

Figure 13: Visualization for the normalized frequency distribution of label ranks included in the prediction set of CCP, Cluster-CP, and RC3P with \(\rho=0.1\) MAJ when \(\alpha=0.1\). It is clear that the distribution of normalized frequency generated by RC3P tends to be lower compared to those produced by CCP and Cluster-CP. Furthermore, the probability density function tail for label ranks in the RC3P prediction set is notably shorter than that of other methods.

Figure 15: Verification of condition numbers \(\{\sigma_{y}\}_{y=1}^{C}\) of Equation 6 when epoch \(=200\) and \(\alpha=0.1\) with \(\rho=0.5\) EXP. Vertical dashed lines represent the value \(1\), and we observe that all the condition numbers are smaller than \(1\). This verifies the validity of the condition for Lemma 4.2, and thus confirms that RC3Pproduces smaller prediction sets than CCP by the optimized trade-off between calibration on non-conformity scores and calibrated label ranks.

Figure 16: Verification of condition numbers \(\{\sigma_{y}\}_{y=1}^{C}\) of Equation 6 when \(\text{epoch}=200\) and \(\alpha=0.1\) with \(\rho=0.1\) POLY. Vertical dashed lines represent the value 1, and we observe that all the condition numbers are smaller than \(1\). This verifies the validity of the condition for Lemma 4.2, and thus confirms that RC3Pproduces smaller prediction sets than CCP by the optimized trade-off between calibration on non-conformity scores and calibrated label ranks.

Figure 17: Verification of condition numbers \(\{\sigma_{y}\}_{y=1}^{C}\) of Equation 6 when \(\text{epoch}=200\) and \(\alpha=0.1\) with \(\rho=0.5\) POLY. Vertical dashed lines represent the value 1, and we observe that all the condition numbers are smaller than \(1\). This verifies the validity of the condition for Lemma 4.2, and thus confirms that RC3Pproduces smaller prediction sets than CCP by the optimized trade-off between calibration on non-conformity scores and calibrated label ranks.

Figure 18: Verification of condition numbers \(\{\sigma_{y}\}_{y=1}^{C}\) of Equation 6 when \(\text{epoch}=200\) and \(\alpha=0.1\) with \(\rho=0.1\) MAJ. Vertical dashed lines represent the value 1, and we observe that all the condition numbers are smaller than \(1\). This verifies the validity of the condition for Lemma 4.2, and thus confirms that RC3Pproduces smaller prediction sets than CCP by the optimized trade-off between calibration on non-conformity scores and calibrated label ranks.

Figure 21: Verification of condition numbers \(\{\sigma_{y}\}_{y=1}^{C}\) of Equation 6 when epoch \(=50\) and \(\alpha=0.1\) with \(\rho=0.5\) EXP. Vertical dashed lines represent the value 1, and we observe that all the condition numbers are smaller than \(1\). This verifies the validity of the condition for Lemma 4.2, and thus confirms that RC3Pproduces smaller prediction sets than CCP by the optimized trade-off between calibration on non-conformity scores and calibrated label ranks.

Figure 23: Verification of condition numbers \(\{\sigma_{y}\}_{y=1}^{C}\) of Equation 6 when epoch \(=50\) and \(\alpha=0.1\) with \(\rho=0.5\) POLY. Vertical dashed lines represent the value 1, and we observe that all the condition numbers are smaller than \(1\). This verifies the validity of the condition for Lemma 4.2, and thus confirms that RC3Pproduces smaller prediction sets than CCP by the optimized trade-off between calibration on non-conformity scores and calibrated label ranks.

Figure 20: Verification of condition numbers \(\{\sigma_{y}\}_{y=1}^{C}\) of Equation 6 when epoch \(=50\) and \(\alpha=0.1\) with \(\rho=0.1\) EXP. Vertical dashed lines represent the value 1, and we observe that all the condition numbers are smaller than \(1\). This verifies the validity of the condition for Lemma 4.2, and thus confirms that RC3Pproduces smaller prediction sets than CCP by the optimized trade-off between calibration on non-conformity scores and calibrated label ranks.

Figure 24: Verification of condition numbers \(\{\sigma_{y}\}_{y=1}^{C}\) of Equation 6 when \(\text{epoch}=50\) and \(\alpha=0.1\) with \(\rho=0.1\) MAJ. Vertical dashed lines represent the value \(1\), and we observe that all the condition numbers are smaller than \(1\). This verifies the validity of the condition for Lemma 4.2, and thus confirms that RC3Pproduces calibrated label ranks.

Figure 25: Verification of condition numbers \(\{\sigma_{y}\}_{y=1}^{C}\) of Equation 6 when \(\text{epoch}=50\) and \(\alpha=0.1\) with \(\rho=0.5\) MAJ. Vertical dashed lines represent the value \(1\), and we observe that all the condition numbers are smaller than \(1\). This verifies the validity of the condition for Lemma 4.2, and thus confirms that RC3Pproduces smaller prediction sets than CCP by the optimized trade-off between calibration on non-conformity scores and calibrated label ranks.

[MISSING_PAGE_FAIL:39]

Figure 28: Verification of condition numbers \(\{\sigma_{y}\}_{y=1}^{K}\) in Equation 6 on balanced datasets. Vertical dashed lines represent the value \(1\), and we observe that all the condition numbers are smaller than \(1\). This verifies the validity of the condition for Lemma 4.2, and thus confirms that RC3P produces smaller prediction sets than CCP using calibration on both non-conformity scores and label ranks.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The abstract and introduction has stated the contributions and important assumptions of our paper and match our theoretical and experimental results. We have summarize all claims at the end of introduction. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [No] Justification: A limitation of our paper is that we assume that \((X_{i},Y_{i})\) are exchangeable (for example, i.i.d.). This assumption is common and fundamental in CP works, so we do not discuss in our paper. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs**Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: In Section 4.1 and 4.2, we provide the the full set of assumptions for our theoretical result. Corresponding proofs are provided in Appendix B. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We have provided all all the information needed to reproduce the experiments, including experiments setting, evaluation metric and codes. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.

5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: We have provided the codes in the supplementary material. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/ guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We have provided the detail about dataset, training and calibration in Section 5.1 and Appendix C.1, C.2, and C.3. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: We have provide the standard deviation of our main results. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).

* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [No] Justification: We follow the training setting of previous papers, so we choose to not discuss the computer resources. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: Our work strictly adheres to the NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [No] Justification: The methodological improvements gained in our paper can lead to improvements in safe deployment of classifiers in human-ML collaborative systems. We do not anticipate any negative ethical or societal impact. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.

* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: Our experiments are conducted on public and benchmark datasets. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: All the assets used in the paper are open-source and have been properly cited. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.

* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: We have provided anonymized zip file in supplementary material. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.