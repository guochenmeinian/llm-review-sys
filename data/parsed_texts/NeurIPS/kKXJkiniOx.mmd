# ConDaFormer: Disassembled Transformer with

Local Structure Enhancement for

3D Point Cloud Understanding

 Lunhao Duan\({}^{*}\)\({}^{1}\)   Shanshan Zhao\({}^{*}\)\({}^{2}\)

**Nan Xue \({}^{1}\)   Mingming Gong \({}^{3}\)   Gui-Song Xia\({}^{\dagger}\)\({}^{1}\)   Dacheng Tao \({}^{4}\)**

\({}^{1}\) School of Computer Science, Wuhan University

\({}^{2}\) JD Explore Academy \({}^{3}\) University of Melbourne \({}^{4}\) University of Sydney

Equal Contribution. This work was done when Lunhao Duan was a research intern at JD Explore Academy.Correspondence Author.Con, Da, and Former indicate Convolution, Disassembled, and Transformer, respectively.

###### Abstract

Transformers have been recently explored for 3D point cloud understanding with impressive progress achieved. A large number of points, over 0.1 million, make the global self-attention infeasible for point cloud data. Thus, most methods propose to apply the transformer in a local region, _e.g.,_ spherical or cubic window. However, it still contains a large number of Query-Key pairs, which require high computational costs. In addition, previous methods usually learn the query, key, and value using a linear projection without modeling the local 3D geometric structure. In this paper, we attempt to reduce the costs and model the local geometry prior by developing a new transformer block, named _ConDaFormer_1. Technically, ConDaFormer disassembles the cubic window into three orthogonal 2D planes, leading to fewer points when modeling the attention in a similar range. The disassembling operation is beneficial to enlarging the range of attention without increasing the computational complexity but ignores some contexts. To provide a remedy, we develop a local structure enhancement strategy that introduces a depth-wise convolution before and after the attention. This scheme can also capture the local geometric information. Taking advantage of these designs, ConDaFormer captures both long-range contextual information and local priors. The effectiveness is demonstrated by experimental results on several 3D point cloud understanding benchmarks. Our code will be available.

Footnote 1: Equal Contribution. This work was done when Lunhao Duan was a research intern at JD Explore Academy.

## 1 Introduction

As a fundamental vision task, 3D point cloud analysis [56, 58, 12, 97] has been studied extensively due to its critical role in various applications, such as robotics and augmented reality. The inherent irregularity and sparsity of point clouds make the standard convolution fail to extract the geometric features directly. As a result, different strategies have been proposed for 3D point cloud data processing, which can be roughly divided into the voxel-based methods [12], the projection-based [7], and the point-based [58, 97]. Many methods achieve remarkable performance in point cloud analysis tasks, such as point cloud segmentation [15] and 3D object detection [94].

In recent years, transformers [70] have shown a powerful capability of modeling long-range dependencies in the natural language processing community [17, 60]. Following ViT [10], transformers [18, 22, 13, 93, 44] also achieve competitive or better performance in comparison with the CNN architectures in many 2D vision tasks, such as object detection and semantic segmentation.

Regarding the 3D point cloud data, the graph structure makes it natural to apply transformers in hierarchical feature representation, and there are indeed many attempts [97; 80; 34] focusing on efficient designs. Since the 3D scene data usually contains more than 0.1 million points [15], it is impractical to apply self-attention globally. Instead, a local region obtained by KNN search [80] or spatially non-overlapping partition [34] is selected to perform the attention. Nevertheless, due to the 3D spatial structure, it still requires high computational costs.

Additionally, for 2D image data, convolution operation has been explored to assist the transformer in modeling local visual structures [22; 87], whereas such a strategy is under-explored for 3D point cloud data. As the density varies across the point cloud, we cannot concatenate the raw pixels within a patch for query, key, and value learning as in 2D image data. As a result, it is important to model the local geometric structure for better feature representation. In fact, in Point Transformer v2 [80] and Stratified Transformer [34; 3], although grouping-then-pooling is used to aggregate local information in the downsampling operation between adjacent encoding stages, we deem it is still inadequate for modeling local geometry prior.

Keeping the issues aforementioned in mind, in this paper we develop a new transformer block for 3D point cloud understanding, named _ConDaFormer_. Specifically, regarding the computational costs, we seek an alternative window partition strategy to replace the 3D cubic window, which can model the dependencies in a similar range but involve fewer points. Inspired by CSWin [8], we disassemble the cubic window into three orthogonal 2D planes. In this way, for a query point, only the points located in three directions are considered the key. Therefore, we can enlarge the attention range with negligible additional computational costs. The triplane-style strategy is also used by EG3D [\(\cdot\)] in 3D generation, which aims at representing the intermediate 3D volume into three planes to reduce memory consumption. As shown in Figure 1, we can observe that for similar distances, the multi-plane windows contain fewer Query-Key pairs. However, as shown in Figure 3 (a), the disassembling operation inevitably causes fewer contexts to be modeled. To alleviate this issue, we apply a depth-wise sparse convolution (DSConv) before the attention to aggregate more contextual information for the query, key, and value as a complement. Moreover, after the attention, we also propagate the updated feature of each point to its local neighbors with another DSConv operation. Such local structure enhancement based on sparse convolution not only compensates for the context loss caused by disassembling but also benefits local 3D geometric prior learning. Thanks to the efficient designs, our ConDaFormer is able to model both dependencies in a larger range and the local structure.

In total, the main contributions of this paper can be summarized as follows:

* We propose a novel disassembled window attention module for 3D point cloud understanding by disassembling the 3D window into three orthogonal planes for self-attention. This strategy effectively reduces computational overhead with negligible performance decreases.
* To enhance the modeling of local features, we introduce depth-wise sparse convolution within the disassembled window attention module. This combination of self-attention and convolution provides a comprehensive solution for capturing both long-range contextual information and local priors in 3D point cloud data.
* Experiments show that our method achieves state-of-the-art performance on widely adopted large-scale 3D semantic segmentation benchmarks and comparable performance in 3D object detection tasks. Extensive ablation studies also verify the effectiveness of our proposed components.

## 2 Related Work

**Vision transformers.** Inspired by the remarkable success of transformer architecture in natural language processing [\(\cap\)0], Vision Transformer (ViT) [\(\cdot\)0] has been proposed, which decomposes image into non-overlapping patches and leverages the multi-headed self-attention by regarding each patch as

Figure 1: Query-Key pairs _v.s._ Window Size. We count the average number of Query-Key pairs for computing self-attention under different window sizes in scenes of S3DIS [ \(\cdot\)] with 80k points input. We can observe that the disassembled window partition can reduce the number of pairs remarkably.

a token. However, global self-attention across the entire image imposes a substantial computational burden and is not applicable for pixel-level image understanding tasks with high-resolution image input. To tackle this issue and extend ViT to downstream tasks, Swin Transformer [44, 45] proposes to confine self-attention to local non-overlapping windows and further introduces shift operations to foster information exchange between adjacent windows. To enlarge the receptive field, some techniques, such as Ccnet [30], Axial-Attention [7], and CSWin Transformer [18], propose to perform self-attention in striped window. Such designs enable pixels to capture long-range dependencies while maintaining an acceptable computational cost. In addition, several recent approaches [78, 22, 87] attempt to integrate convolutions into transformer blocks, leveraging the benefits of local priors and long-range contexts.

**Point cloud understanding.** Existing methods for point cloud understanding can be broadly classified into three categories: voxel-based, projection-based, and point-based. Voxel-based methods [57, 12] divide the point cloud space into small voxels (_i.e.,_ voxelization) and represent each voxel with a binary value indicating whether it is occupied or not. Earlier approaches [81, 57] directly apply convolution on all voxels, which causes a large computational effort. To improve computational efficiency, some methods [61, 37] propose octrees representation for point cloud data to reduce the computational and memory costs. Recently, considering the sparsity property, many methods [12, 21] develop sparsity-aware convolution where only non-empty voxels are involved. For example, Choy _et al._[12] represent the spatially sparse 3D data as sparse tensors and develop an open-source library that provides general operators (_e.g.,_ sparse convolution) for the sparse data. Projection-based methods [7, 65] project the point cloud data onto different 2D views. In this way, each view can be processed as a 2D image and the standard convolution can be used straightforwardly. Both voxel-based and projection-based methods might suffer from the loss of geometric information due to the voxelization and projection operation. In comparison with them, point-based methods directly operate on the raw point cloud data. Following the pioneering work, PointNet [56] and PointNet++ [58], a large number of approaches for local feature extraction have been proposed [69, 79, 86, 39, 76, 38, 28]. These methods utilize a learnable function to transform each point into a high-dimensional feature embedding, and then employ various geometric operations, such as point-wise feature fusion and feature pooling, to extract semantic information.

**Point cloud transformers.** As transformers have shown powerful capability in modeling long-range dependencies in 2D image data, it is natural to explore the application to point cloud data with graph structure [97, 23, 50, 24, 20, 80, 45]. Although most transformer-based point cloud understanding networks can be classified into the point-based aforementioned, we review previous works especially here since they are very close to this paper. Point Cloud Transformer (PCT) [23] and Point Transformer v1 (PTv1) [97] make earlier attempts to design transformer block for point cloud data. However, similar to ViT [19], PCT performs global attention on all input points, leading to high computation and memory costs. In comparison, PTv1 achieves self-attention within a local region for each query point, which largely reduces the computational effort and can be applied to scene-level point clouds. Following PTv1, Point Transformer V2 (PTv2) [80] further promotes the study in point cloud transformer and achieves better performance on typical point cloud analysis tasks with developed grouped vector attention and partition-based pooling schemes. Instead of the overlapping ball regions, Stratified Transformer [34], inspired by Swin Transformer [44, 45], splits the 3D space into non-overlapping 3D cubic windows to perform local self-attention within each window. To enlarge the receptive field and capture long-range contexts, it also develops a stratified strategy to select distant points as well as nearby neighbours as the keys. Considering the special structure of LiDAR data, SST [20] first voxelizes 3D LiDAR point cloud into sparse Bird's Eye View (BEV) pillars and then splits the space into non-overlapping 2D square windows to perform self-attention. To solve the problem of inconsistent token counts within each window in SST, FlatFormer [45] first re-orders these pillars in the BEV space and then partitions these pillars into groups of equal sizes to achieve parallel processing on the GPU. CpT [32] and 3DCTN [48] explore the local structure encoding in the transformer block but only focus on small-scale data, like the CAD model. To provide more exhaustive progress on the point cloud transformer, here we further briefly discuss two recent works that were uploaded to the arXiv website shortly before the manuscript submission, Swin3D [92] and OctFormer [74]. Swin3D mainly focuses on the 3D backbone pre-training by constructing a 3D Swin transformer and pre-training it on a large-scale synthetic dataset. OctFormer aims to reduce the computation complexity of attention by developing octree-based transformers with the octree representation of point cloud data. Both of them and ours aim to explore the transformer in point cloud analysis in different ways.

## 3 Method

### Preliminary

**Overview.** Regarding the point cloud understanding task, our goal is to develop a backbone network to generate high-level hierarchical features for a given point cloud, which can be further processed by a specific task head, such as semantic segmentation and object detection. We focus on designing a transformer block that can be used to construct such a backbone. Specifically, let \(X=(P\in\mathbb{R}^{N\times 3},F\in\mathbb{R}^{N\times C})\) represent the point cloud feed into the transformer block, where \(P\) represents the 3D position of \(N\) points and \(F\) denotes the features with channel number \(C\). The transformer block processes \(X\) by exploiting the attention mechanism [70] and generates new features. We design the transformer block by disassembling the typical 3D window into multiple planes and introducing the local enhancement strategy, resulting in our ConDaFormer block. Based on ConDaFormer, we can construct networks for different point cloud analysis tasks, including point cloud semantic segmentation and 3D object detection. In the following, we detail ConDaFormer and the proposed networks for two high-level point cloud analysis tasks. Before that, we first give a brief introduction to the transformer based on the shifted 3D window, _i.e.,_ basic Swin3D block.

**Basic Swin3D block.** ViT [1] achieves global self-attention of 2D images by partitioning the 2D space into non-overlapping patches as tokens. However, for high-resolution images, global attention is high-cost due to the quadratic increase in complexity with image size. To encourage the transformer to be more feasible for general vision tasks, Swin Transformer [41] develops a hierarchical transformer block where self-attention is applied within non-overlapping windows, and the adjacent windows are connected with a shifted window partitioning strategy. For the 3D point cloud data, we can naively extend the 2D window to the 3D version [34; 92]. Specifically, let \(X_{t}=(P\in\mathbb{R}^{N_{t}\times 3},F_{t}\in\mathbb{R}^{N_{t}\times C})\) denote the points in the \(t\)-th window with the size of \(S\times S\times S\) and containing \(N_{t}\) points. The self-attention with \(H\) heads can be formulated as:

\[\begin{split}& Q_{t}^{h}=Linear(X_{t}),K_{t}^{h}=Linear(X_{t}),V_{t}^ {h}=Linear(X_{t}),h\in[1,...,H],\\ & Attn^{h}(X_{t})=Soft((Q_{t}^{h})(K_{t}^{h})^{T}/\sqrt{C/H})(V_{ t}^{h}),h\in[1,...,H],\\ & Swin3D(X_{t})=Linear([Attn^{1}(X_{t})\oplus\cdot\cdot\cdot \oplus Attn^{h}(X_{t})\oplus\cdot\cdot\cdot\oplus Attn^{H}(X_{t})]),\end{split}\] (1)

where 1) \(h\) indicates the \(h\)-th head; 2) \(Q_{*}^{*}\), \(K_{*}^{*}\), and \(V_{*}^{*}\) represent _query_, _key_, and _value_, respectively; 3) \(Linear\) denotes the _Linear_ function (Note that these linear functions do not share parameters); 4) \(Soft\) denotes the _Softmax_ function; 5) \(Attn\) denotes the self-attention operation; 6) \([\cdot\oplus\cdot]\) represents the concatenation operation along the feature channel dimension. To achieve a cross-window connection, the shift along three dimensions with displacements is utilized in a successive transformer block with similar operations in Eq. 1.

Although the local window attention largely reduces the computational complexity in comparison with global attention, for each partitioned 3D window, the computation for self-attention is still

Figure 2: (a) Framework overview. The entire network consists of a Point Embedding layer and four stages each one of which contains a downsampling layer and \(N_{i}\) ConDaFormer blocks. (b) Structure of ConDaFormer block. Each ConDaFormer block consists of two Layer Normalization layers, an MLP, and a ConDaFormer module. The module contains a disassembled window attention, two linear layers, and two local structure enhancement (LSE) modules before and after the attention.

high-cost. In this paper, we explore how to further reduce the computational costs with small changes in the range of attention. Specifically, we develop a new block ConDaFormer, which enlarges the attention range with negligible additional computational costs.

### ConDaFormer

In this part, we introduce our ConDaFormer by detailing the two main components: disassembling operation and sparse-convolution-based local structure enhancement.

**Cube to Multiple planes: Disassembling the 3D window.** As analyzed before, the 3D window attention requires high computational costs for modeling long-range dependencies. To reduce the computational cost while maintaining the equivalent attention range, we propose to convert the cubic window to three orthogonal 2D planes, as shown in Figure 3. Technically, we split the whole 3D space into three orthogonal planes, _XY_-plane, _XZ_-plane, and _YZ_-plane. For each 2D plane, we generate a series of non-overlapping 2D windows, in each of which self-attention is performed. Taking an example of the _XY_-plane for illustration, we have the following equations:

\[Q_{xy,t}^{h}=Linear(X_{t}),K_{xy,t}^{h}=Linear(X_{t}),V_{xy,t}^{ h}=Linear(X_{t}),h\in[1,...,H/3],\] (2) \[Attn_{xy}^{h}(X_{t})=Soft((Q_{xy,t}^{h})(K_{xy,t}^{h})^{T}/\sqrt {C/H})(V_{xy,t}^{h}),h\in[1,...,H/3],\] \[Attn_{xy}(X_{t})=[Attn_{xy}^{1}(X_{t})\oplus\cdots\oplus Attn_{ xy}^{h}(X_{t})\oplus\cdots\oplus Attn_{xy}^{H/3}(X_{t})],\]

where the notations have identical meanings to those in Eq. 1, except that \(Linear\) maps the channel number from \(C\) to \(C/3\). In such a basic formulation, the attention is only related to the similarity between query \(Q\) and key \(K\) but does not contain the position bias which is important for self-attention learning. We can add learned relative position bias to \((Q_{xy,t}^{h})(K_{xy,t}^{h})^{T}/\sqrt{C/H}\) to encode the position information as Swin Transformer does [4, 5]. But, to better capture the position information with contextual information, we adopt an efficient context-based adaptive relative position encoding scheme developed in Stratified Transformer [3, 5]. Specifically, we can re-write the computation of \(Attn_{xy}^{h}(X_{t})\) in Eq. 2 as follows:

\[Attn_{xy}^{h}(X_{t})=Soft(((Q_{xy,t}^{h})(K_{xy,t}^{h})^{T}+Q_{xy,t}^{h}E_{q}^{ h}+K_{xy,t}^{h}E_{k}^{h})/\sqrt{C/H})(V_{xy,t}^{h}+E_{v}^{h})),\] (3)

where \(E_{q}\), \(E_{k}\), and \(E_{v}\) are corresponding learnable position encodings for \(Q_{xy}\), \(K_{xy}\), and \(V_{xy}\). For the other two planes, we exploit similar operations and share the same position encoding for three planes. We denote the attention as \(Attn_{yz}(X_{t})\) (\(YZ\)-plane) and \(Attn_{xz}(X_{t})\) (\(XZ\)-plane), respectively. Such a shared strategy makes the learning of position encoding more effective, which is demonstrated in the experiments. Then, we merge the three attention results to obtain the final output of the disassembled transformer (DaFormer):

\[DaFormer(X_{t})=Linear([Attn_{xy}(X_{t})\oplus Attn_{yz}(X_{t})\oplus Attn_{ xz}(X_{t})]).\] (4)

**Local structure enhancement with sparse convolution.** As shown in Figure 3 (a), in comparison with the 3D window, although our plane-based window is easy to capture long-range dependency, it ignores the contexts not located in the planes. To address this issue and model the local geometry prior, we propose to apply a sparse convolution operation to encode the input before the query, key, and value learning. As depicted in Figure 3 (b), the local structure enhancement module has

Figure 3: Two components in the ConDaFormer block. (a) An illustration of window partition: from cubic window to disassembled windows. The light blue areas indicate the regions involved in self-attention. (b) The detailed structure of the LSE module.

two branches: a \(1\times 1\times 1\) convolution and a depth-wise sparse convolution to aggregate the local information. Mathematically, the local structure enhancement (LSE) module can be written as:

\[LSE(X_{t})=[BN(Linear(X_{t}))\oplus BN(Linear(GELU(BN(DConv(X_{t})))))],\] (5)

where \(BN\), \(GELU\), and \(DConv\) indicate the Batch Normalization, GELU activation function, and depth-wise sparse convolution (\(3\times 3\times 3\) kernel), respectively. After the self-attention, we also apply another local enhancement operation to propagate the long-range contextual information learned by self-attention to local neighbors to further improve the local structure. As a result, the full operation consisting of two local enhancements and one self-attention can be written as with the notations above:

\[\begin{split} X_{t}^{\prime}=LSE(X_{t}),\\ ConDaFormer(X_{t})=Linear(LSE([Attn_{xy}(X_{t}^{\prime})\oplus Attn _{yz}(X_{t}^{\prime})\oplus Attn_{xz}(X_{t}^{\prime})])).\end{split}\] (6)

**ConDaFormer block structure.** The overall architecture of our ConDaFormer block is depicted in Figure 2 (b). Our ConDaFormer block comprises several essential components, including two local enhancement modules (before and after self-attention), a disassembled window self-attention module, a multi-layer perceptron (MLP), and residual connections [25].

### Network Architecture

**Point embedding.** As mentioned by Stratified Transformer [34], the initial local feature aggregation is important for point transformer networks. To address this, we employ a sparse convolution layer to lift the input feature to a higher dimension \(C\). Subsequently, we leverage a ResBlock [25] to extract the initial point embedding, facilitating the representation learning process.

**Downsampling.** The downsampling module is composed of a linear layer, which serves to increase the channel dimension, and a max pooling layer with a kernel size of 2 and a stride of 2 to reduce the spatial dimension.

**Network settings.** Our network architecture is designed with four stages by default and each stage is characterized by a specific channel configuration and a corresponding number of blocks. Specifically, the channel numbers for these stages are set to \(\{C,2C,4C,4C\}\) and the corresponding block numbers \(\{N_{1},N_{2},N_{3},N_{4}\}\) are \(\{2,2,6,2\}\). And \(C\) is set to 96 and the head numbers \(H\) are set to \(1/16\) of the channel numbers in our experiments. For the task of semantic segmentation, following the methodology of Stratified Transformer [34], we employ a U-Net structure to gradually upsample the features from the four stages back to the original resolution. Additionally, we employ an MLP to perform point-wise prediction, enabling accurate semantic segmentation. In the case of object detection, we adopt FCAF3D [63] and CAGroup3D [71] as the baseline and replace the network backbone with our proposed architecture, leveraging its enhanced capabilities for improved object detection performance.

## 4 Experiments

To validate the effectiveness of our ConDaFormer, we conduct experiments on 3D semantic segmentation and 3D object detection tasks. We also perform extensive ablation studies to analyze each component in our ConDaFormer. Additional ablation results of window size and position encoding and more experiment results on outdoor perception tasks and object-level tasks are available in the appendix.

### Semantic Segmentation

**Datasets and metrics.** For 3D semantic segmentation, we conduct comprehensive experiments on three benchmark datasets: ScanNet v2 [15], S3DIS [1], and the recently-introduced ScanNet200 [62].

The ScanNet v2 [15] dataset comprises a collection of 1513 3D scans reconstructed from RGB-D frames. The dataset is split into 1201 scans for training and 312 scans for validation. The input point cloud is obtained by sampling vertices from the reconstructed mesh and annotated with 20 semantic categories. In addition, we utilize the ScanNet200 [62] dataset, which shares the same input point cloud as ScanNet v2 but provides annotations for 200 fine-grained semantic categories.

The S3DIS [\(\cdot\)] dataset consists of 271 room scans from six areas. Following the conventions of previous methods, we reserve Area 5 for validation while utilizing the remaining areas for training. The input point cloud of the S3DIS dataset is sampled from the surface of the reconstructed mesh and annotated with 13 semantic categories.

For evaluating the performance of our ConDaFormer on the ScanNet v2 and ScanNet200 dataset, we employ the widely adopted mean intersection over union (mIoU) metric. In the case of the S3DIS dataset, we utilize three evaluation metrics: mIoU, mean of class-wise accuracy (mAcc), and overall point-wise accuracy (OA).

**Experimental Setup.** For ScanNet v2 and ScanNet200, we train for 900 epochs with voxel size and batch size set to 0.02m and 12 respectively. We utilize an AdamW optimizer [\(\uparrow\)7] with an initial learning rate of 0.006 and a weight decay of 0.02. The learning rate decreases by a factor of 10 after 540 and 720 epochs respectively. The initial window size \(S\) is set to 0.16m and increases by a factor of 2 after each downsampling. Following Point Transformer v2 [\(\otimes\)0] and Stratified Transformer [\(\ddagger\)4], we use some data augmentation strategies, such as rotation, scale, jitter, and dropping color.

For S3DIS, we train for 3000 epochs with voxel size and batch size set to 0.04m and 8 respectively. We utilize an AdamW optimizer with an initial learning rate of 0.006 and a weight decay of 0.05. The learning rate decreases by a factor of 10 after 1800 and 2400 epochs respectively. The initial window size \(S\) is set to 0.32m and increases by a factor of 2 after each downsampling. The data augmentations are identical to those used in Stratified Transformer [\(\ddagger\)4].

**Quantitative results.** In our experiments, we compare the performance of our ConDaFormer against state-of-the-art methods in 3D semantic segmentation. The results on ScanNet v2 and S3DIS datasets are shown in Table 1 and Table 2 respectively. Since Point Transformer v2 [\(\otimes\)0] employs the test-time-augmentation (TTA) strategy to improve the performance, we also adopt such strategy for fair comparison and the results with TTA strategy are marked with \({}^{*}\). Clearly, on the validation set of ScanNet v2, our ConDaFormer achieves the best performance and surpasses Point Transformer v2 [\(\otimes\)0] by 0.5% mIoU (TTA also exploited). Similarly, on S3DIS, our ConDaFormer outperforms prior methods and achieves a new state-of-the-art performance of 73.5% mIoU. Additionally, on the challenging ScanNet200 dataset, as shown in Table 3, our ConDaFormer still performs better than Point Transformer v2 [\(\otimes\)0] and substantially outperforms the rest of competitors that are pre-trained with additional data or a large vision-language model. The state-of-the-art performance on these datasets demonstrates the effectiveness of our ConDaFormer.

\begin{table}
\begin{tabular}{l|c|c c} \hline \hline Method & Input & Val & Test \\ \hline PointNet++ [\(\lx@sectionsign\)8] & point & 53.5 & 55.7 \\
3DMV [\(\ddagger\)6] & point & - & 48.4 \\ PanopticFusion [\(\lx@sectionsign\)3] & point & - & 52.9 \\ PointCNN [\(\lx@sectionsign\)9] & point & - & 45.8 \\ PointConv [\(\lx@sectionsign\)7] & point & 61.0 & 66.6 \\ JointPointBased [\(\cdot\)1] & point & 69.2 & 63.4 \\ PointASNL [\(\lx@sectionsign\)0] & point & 63.5 & 66.6 \\ SegGCN [\(\lx@sectionsign\)8] & point & - & 58.9 \\ RandLA-Net [\(\lx@sectionsign\)8] & point & - & 64.5 \\ KPConv [\(\lx@sectionsign\)6] & point & 69.2 & 68.6 \\ JSENet [\(\lx@sectionsign\)9] & point & - & 69.9 \\ SparseConvNet [\(\cdot\)2] & voxel & 69.3 & 72.5 \\ MinkUNet [\(\cdot\)12] & voxel & 72.2 & 73.6 \\ PTv1 [\(\lx@sectionsign\)7] & point & 70.6 & - \\ PointNeXt [\(\lx@sectionsign\)9] & point & 71.5 & 71.2 \\ FPT [\(\lx@sectionsign\)4] & voxel & 72.1 & - \\ LargeKernel [\(\lx@sectionsign\)8] & voxel & 73.2 & 73.9 \\ Stratified [\(\ddagger\)4] & point & 74.3 & 73.7 \\ PTv2\({}^{*}\)[\(\lx@sectionsign\)80] & point & 75.5 & 75.2 \\ \hline ConDaFormer & point & 75.1 & 74.7 \\ ConDaFormer\({}^{*}\) & point & **76.0** & **75.5** \\ \hline \hline \end{tabular}
\end{table}
Table 1: Semantic segmentation on ScanNet v2.

\begin{table}
\begin{tabular}{l|c c c} \hline \hline Method & Input & OA & mAcc & mIoU \\ \hline PointNet [\(\lx@sectionsign\)6] & point & - & 49.0 & 41.1 \\ SegCloud [\(\lx@sectionsign\)8] & point & - & 57.4 & 48.9 \\ TanConv [\(\lx@sectionsign\)7] & point & - & 62.2 & 52.6 \\ PointCNN [\(\lx@sectionsign\)9] & point & 85.9 & 63.9 & 57.3 \\ PointWeb [\(\lx@sectionsign\)6] & point & 87.0 & 66.6 & 60.3 \\ HPEIN [\(\lx@sectionsign\)31] & point & 87.2 & 68.3 & 61.9 \\ GACNet [\(\lx@sectionsign\)73] & point & 87.8 & - & 62.9 \\ PAT [\(\lx@sectionsign\)9] & point & - & 70.8 & 60.1 \\ ParamConv [\(\lx@sectionsign\)75] & point & - & 67.0 & 58.3 \\ SPGraph [\(\lx@sectionsign\)5] & point & 86.4 & 66.5 & 58.0 \\ SegGCN [\(\lx@sectionsign\)8] & point & 88.2 & 70.4 & 63.6 \\ MinkUNet [\(\cdot\)12] & voxel & - & 71.7 & 65.4 \\ PAConv [\(\lx@sectionsign\)6] & point & - & - & 66.6 \\ KPConv [\(\lx@sectionsign\)69] & point & - & 72.8 & 67.1 \\ PTv1 [\(\lx@sectionsign\)7] & point & 90.8 & 76.5 & 70.4 \\ PointNeXt [\(\lx@sectionsign\)9] & point & 90.6 & - & 70.5 \\ FPT [\(\lx@sectionsign\)4] & voxel & - & 77.3 & 70.1 \\ Stratified [\(\ddagger\)4] & point & 91.5 & 78.1 & 72.0 \\ PTv2\({}^{*}\)[\(\lx@sectionsign\)80] & point & 91.6 & 78.0 & 72.6 \\ \hline ConDaFormer & point & 91.6 & 78.4 & 72.6 \\ ConDaFormer\({}^{*}\) & point & **92.4** & **78.9** & **73.5** \\ \hline \hline \end{tabular}
\end{table}
Table 2: Semantic segmentation on S3DIS Area 5.

**Qualitative results.** Figure 4 and Figure 5 show the visualization comparison results of semantic segmentation on ScanNet v2 and S3DIS datasets, respectively. Compared to Stratified Transformer [34] and the baseline with cubic window attention, our ConDaFormer is able to produce more accurate segmentation results for some categories, such as Wall, Window, and Sofa.

### Object Detection

**Data and metric.** We evaluate our ConDaFormer on the SUN RGB-D [\(\lx@sectionsign\)4] dataset annotated with oriented 3D bounding boxes of 37 categories. The dataset consists of about 10k indoor RGB-D scans, which are split into 5285 scans for training and 5050 scans for validation. Following prior works, we use the average precision (mAP) under IoU thresholds of 0.25 (mAP@0.25) and 0.5 (mAP@0.50) for evaluating the performance.

**Experimental Setup.** We implement the object detection network based on FCAF3D [\(\lx@sectionsign\)3] and CAGroup3D [\(\nearrow\)1], respectively. We replace the backbone with our ConDaFormer while keeping the detection head unchanged. The detection network built upon our ConDaFormer is trained with AdamW optimizer for 24 epochs with batch size and weight decay set to 32 and 0.01 respectively. The initial learning rate is set to 0.001 and decreases by a factor of 10 after 12 and 20 epochs, respectively. The data augmentation techniques are the same as those in FCAF3D and CAGroup3D.

**Quantitative results.** Following FCAF3D [\(\lx@sectionsign\)6], we run ConDaFormer 5 times and record the best and average (in bracket) performance to reduce the impact caused by random sampling. As shown in Table 5, in comparison with FCAF3D and CAGroup3D, our method achieves comparable performance and the gap between the best and average performance of our method is smaller. In addition, ours has fewer parameters than FCAF3D (23 million _v.s._ 70 million).

### Ablation Study

We perform ablation experiments of semantic segmentation on the S3DIS dataset to demonstrate the effectiveness of each component of our proposed method.

Figure 4: Visualization of semantic segmentation results on ScanNet v2.

Figure 5: Visualization of semantic segmentation results on S3DIS.

**Window disassembly.** We begin by investigating the impact of disassembling the 3D cubic window into three orthogonal 2D planes. We first compare our disassembled attention with the vanilla 3D window attention, which we refer to as "Cubic". We also further analyze our DaFormer by examining some involved settings. Specifically, 1) "DaFormer w.o Split": the number of channels equals that of the input for all three planes and the position embedding is also shared across the three planes, and the _Concatenation_ in Eq. 4 is replaced by _Add_. 2) "DaFormer w.o Share": the number of channels equals \(1/3\) of that of the input for all three planes and the position embedding is not shared. Table 4 shows the results with the window size set to 0.32m. The computational cost is evaluated with the GPU hours for training the network with a single NVIDIA Tesla V100 GPU. Compared to "Cubic" and "DaFormer w.o Split", although ours ("DaFormer") has a slightly lower score, the computational effort is significantly reduced. Notably, the comparison between "DaFormer w.o Share" and ours suggests that sharing the position embedding across different planes yields substantial performance gains. The potential reason may be the fact that when not shared across planes, the position embedding is not sufficiently trained due to the small number of neighborhood points within each plane.

**Local structure enhancement.** Next, we explore the influence of the proposed local structure enhancement module, which aims to capture local geometric information and address the challenge of context ignorance introduced by window disassembly. The results presented in Table 6 indicate that incorporating local sparse convolution before or after self-attention leads to improved performance. And the best result is achieved when both of them are adopted. These comparisons highlight the significance of leveraging local prior information in the context of 3D semantic segmentation, emphasizing the importance of capturing detailed local geometric cues alongside global context.

**Window size.** Finally, we evaluate the effect of window size. Specifically, we evaluate the segmentation performance and computational costs of the vanilla cubic window attention, disassembled window attention, and our ConDaFormer (_i.e.,_ disassembled window attention with local structure enhancement) by setting the initial window size to 0.16m and 0.32m. We have also experimented with a window size of 0.08m on the cubic window and got 67.7% mIoU, significantly worse than the result of 69.9% mIoU obtained with a window size of 0.16m. As shown in Table 7, compared to the cubic window attention, our proposed disassembled window attention ("DaFormer") significantly reduces the training time, especially when the window size is set to 0.32m, a relatively large window. Concomitantly, under both window sizes, the "DaFormer" brings a slight degradation in segmentation performance. However, compared to the "Cubic" under the window size of 0.16m, the "DaFormer" under the window size of 0.32m not only improves the mIoU by 0.8% but also has less computational cost. Moreover, compared to the "DaFormer", our ConDaFormer significantly improves the segmentation performance with a small increase in the number of parameters and training time, which verifies the effectiveness and efficiency of our model.

\begin{table}
\begin{tabular}{c c c} \hline \hline Before & After & mIoU \\ \hline  & 70.7 \\ \hline ✓ & 72.0 \\ ✓ & ✓ & 71.5 \\ ✓ & ✓ & 72.6 \\ \hline \hline \end{tabular}
\end{table}
Table 6: Ablation of LSE.

\begin{table}
\begin{tabular}{c|c|c} \hline \hline Method & Input & mIoU (\%) \\ \hline CSC [26] & voxel & 26.4 \\ LGround [62] & voxel & 28.9 \\ PTv2\({}^{\star}\)[80] & point & 31.9 \\ \hline ConDaFormer\({}^{\star}\) & point & **32.3** \\ \hline \hline \end{tabular}
\end{table}
Table 4: Ablation of window attention type.

\begin{table}
\begin{tabular}{c|c c} \hline \hline Type & mIoU (\%) & Time (hours) \\ \hline Cubic & 71.2 & 40.8 \\ \hline DaFormer w.o Split & 71.1 & 39.9 \\ DaFormer w.o Share & 69.9 & 24.1 \\ DaFormer & 70.7 & 24.1 \\ \hline \hline \end{tabular}
\end{table}
Table 3: Semantic segmentation on ScanNet200.

\begin{table}
\begin{tabular}{c|c c c} \hline \hline Method & mAP@0.25 & mAP@0.5 \\ \hline VoteNet [55] & 57.7 & - \\ MLCVNet [84] & 59.8 & - \\
3DETR [23] & 59.1 & 32.7 \\ H3DNet [95] & 60.1 & 39.0 \\ BBNet [6] & 61.1 & 43.7 \\ HGNet [6] & 61.6 & - \\ VENet [83] & 62.5 & 39.2 \\ GroupFree [40] & 63.0 (62.6) & 45.2 (44.4) \\ \hline FCAF3D [63] & 64.2 (63.8) & 48.9 (48.2) \\ ConDaFormer & 64.9 (64.7) & 48.8 (48.5) \\ \hline CAGroup3D [71] & 66.8 (66.4) & 50.2 (49.5) \\ ConDaFormer & 67.1 (66.8) & 49.9 (49.5) \\ \hline \hline \end{tabular}
\end{table}
Table 5: Detection results on SUN RGB-D.

Conclusion

This paper is focused on the transformer for point cloud understanding. Aiming at reducing the computational costs in the previous 3D non-overlapping window partition, we disassemble the 3D window into three mutually perpendicular 2D planes. In this way, the number of points decreases within a similar attention range. However, the contexts also accordingly reduce for each query point. To alleviate this issue and also model the local geometry prior, we introduce the local structure enhancement strategy which inserts the sparse convolution operation before and after the self-attention. We evaluate our developed transformer block, _i.e._, ConDaFormer, for 3D point cloud semantic segmentation and 3D object detection. The experimental results can demonstrate the effectiveness.

**Limitation:** In our experiments, we find that a larger attention window might cause drops in performance. For example, if we further enlarge the window size from 0.32m to 0.48m, the training loss drops from around 0.048 to around 0.045 while the mIoU does not increase on the S3DIS dataset. As pointed out by LargeKernel [8], a large convolutional kernel might cause difficulties in optimizing proliferated parameters and leads to over-fitting. We guess that in our transformer block larger attention range requires more positional embeddings, which might cause similar issues. However, this phenomenon has not been fully explored in the 3D point cloud community. We think it would be worth studying in the future with efficient learning strategies, such as exploring self-supervised learning on large-scale datasets.

## Acknowledgements

This research is supported by the NSFC Grants under the contracts No. 62325111 and No.U22B2011.

## References

* [1]I. Armeni, O. Sener, A. R. Zamir, H. Jiang, I. Brilakis, M. Fischer, and S. Savarese (2016) 3d semantic parsing of large-scale indoor spaces. In IEEE Conf. Comput. Vis. Pattern Recog., pp. 1534-1543. Cited by: SS1.
* [2]X. Bai, Z. Hu, X. Zhu, Q. Huang, Y. Chen, H. Fu, and C. Tai (2022) Transfusion: robust lidar-camera fusion for 3d object detection with transformers. In IEEE Conf. Comput. Vis. Pattern Recog., pp. 1090-1099. Cited by: SS1.
* [3]J. Behley, M. Garbade, A. Milioto, J. Quenzel, S. Behnke, C. Stachniss, and J. Gall (2019) SemanticKit: a dataset for semantic scene understanding of lidar sequences. In Int. Conf. Comput. Vis., pp. 9297-9307. Cited by: SS1.
* [4]H. Caesar, V. Bankiti, A. H. Lang, S. Vora, V. E. Liong, Q. Xu, A. Krishnan, Y. Pan, G. Baldan, and O. Beijbom (2020) nuscenes: a multimodal dataset for autonomous driving. In IEEE Conf. Comput. Vis. Pattern Recog., pp. 11621-11631. Cited by: SS1.
* [5]E. R. Chan, C. Z. Lin, M. A. Chan, K. Nagano, B. Pan, S. De Mello, O. Gallo, L. J. Guibas, J. Tremblay, S. Khamis, et al. (2022) Efficient geometry-aware 3d generative adversarial networks. In IEEE Conf. Comput. Vis. Pattern Recog., pp. 16123-16133. Cited by: SS1.
* [6]J. Chen, B. Lei, Q. Song, H. Ying, D. Z. Chen, and J. Wu (2020) A hierarchical graph network for 3d object detection on point clouds. In IEEE Conf. Comput. Vis. Pattern Recog., pp. 392-401. Cited by: SS1.
* [7]X. Chen, H. Ma, J. Wan, B. Li, and T. Xia (2017) Multi-view 3d object detection network for autonomous driving. In IEEE Conf. Comput. Vis. Pattern Recog., pp. 1907-1915. Cited by: SS1.
* [8]Y. Chen, J. Liu, X. Qi, X. Zhang, J. Sun, and J. Jia (2022) Scaling up kernels in 3d cnns. Adv. Neural Inform. Process. Syst. Cited by: SS1.
* [9]B. Cheng, L. Sheng, S. Shi, M. Yang, and D. Xu (2021) Back-tracing representative points for voting-based 3d object detection in point clouds. In IEEE Conf. Comput. Vis. Pattern Recog., pp. 8963-8972. Cited by: SS1.
* [10]R. Cheng, R. Razani, E. Taghavi, E. Li, and B. Liu (2021) 2-s3net: attentive feature fusion with adaptive feature selection for sparse semantic segmentation network. In IEEE Conf. Comput. Vis. Pattern Recog., Cited by: SS1.
* [11]H. Chiang, Y. Lin, Y. Liu, and W. H. Hsu (2019) A unified point-based framework for 3d segmentation. In Int. Conf. 3D Vis., pp. 155-163. Cited by: SS1.
* [12]C. Choy, J. Gwak, and S. Savarese (2019) 4d spatio-temporal convnets: minkowski convolutional neural networks. In IEEE Conf. Comput. Vis. Pattern Recog., pp. 3075-3084. Cited by: SS1.
* [13]X. Chu, Z. Tian, Y. Wang, B. Zhang, H. Ren, X. Wei, H. Xia, and C. Shen (2021) Twins: revisiting the design of spatial attention in vision transformers. Adv. Neural Inform. Process. Syst.34, pp. 9355-9366. Cited by: SS1.
* [14]T. Cortinhal, G. Tzelepis, and E. Erdal Aksoy (2020) Salsanext: fast, uncertainty-aware semantic segmentation of lidar point clouds. In International Symposium on Visual Computing, Cited by: SS1.
* [15]A. Dai, A. X. Chang, M. Savva, M. Halber, T. Funkhouser, and M. Niessner (2017) Scannet: richly-annotated 3d reconstructions of indoor scenes. In IEEE Conf. Comput. Vis. Pattern Recog., Cited by: SS1.
* [16]A. Dai and M. Niessner (2018) 3dmv: joint 3d-multi-view prediction for 3d semantic scene segmentation. In Eur. Conf. Comput. Vis., pp. 452-468. Cited by: SS1.
* [17]J. Devlin, M. Chang, K. Lee, and K. Toutanova (2018) Bert: pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805. Cited by: SS1.
* [18]X. Dong, J. Bao, D. Chen, W. Zhang, N. Yu, L. Yuan, D. Chen, and B. Guo (2022) Cswin transformer: a general vision transformer backbone with cross-shaped windows. In IEEE Conf. Comput. Vis. Pattern Recog., pp. 12124-12134. Cited by: SS1.
* [19]A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, et al. (2020) An image is worth 16x16 words: transformers for image recognition at scale. arXiv preprint arXiv:2010.11929. Cited by: SS1.
* [20]L. Fan, Z. Pang, T. Zhang, Y. Wang, H. Zhao, F. Wang, N. Wang, and Z. Zhang (2022) Embracing single stride 3d object detector with sparse transformer. In IEEE Conf. Comput. Vis. Pattern Recog., pp. 8458-8468. Cited by: SS1.

[MISSING_PAGE_POST]

* [23] Meng-Hao Guo, Jun-Xiong Cai, Zheng-Ning Liu, Tai-Jiang Mu, Ralph R Martin, and Shi-Min Hu. Pct: Point cloud transformer. _Comput. Vis. Media_, 7:187-199.
* [24] Chenhang He, Ruihuang Li, Shuai Li, Lei Zhang. Voxel set transformer: A set-to-set approach to 3d object detection from point clouds. In _IEEE Conf. Comput. Vis. Pattern Recog._, pages 8417-8427, 2022.
* [25] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _IEEE Conf. Comput. Vis. Pattern Recog._, pages 770-778, 2016.
* [26] Ji Hou, Benjamin Graham, Matthias Niessner, and Saining Xie. Exploring data-efficient 3d scene understanding with contrastive scene contexts. In _IEEE Conf. Comput. Vis. Pattern Recog._, pages 15587-15597, 2021.
* [27] Yuenan Hou, Xinge Zhu, Yuexin Ma, Chen Change Loy, and Yikang Li. Point-to-voxel knowledge distillation for lidar semantic segmentation. In _IEEE Conf. Comput. Vis. Pattern Recog._, 2022.
* [28] Qingyong Hu, Bo Yang, Linhai Xie, Stefano Rosa, Yulan Guo, Zhihua Wang, Niki Trigoni, and Andrew Markham. Randla-net: Efficient semantic segmentation of large-scale point clouds. In _IEEE Conf. Comput. Vis. Pattern Recog._, pages 11108-11117, 2020.
* [29] Zeyu Hu, Mingmin Zhen, Xuyang Bai, Hongbo Fu, and Chiew-lan Tai. Jsenet: Joint semantic segmentation and edge detection network for 3d point clouds. In _Eur. Conf. Comput. Vis._, pages 222-239, 2020.
* [30] Zilong Huang, Xinggang Wang, Lichao Huang, Chang Huang, Yunchao Wei, and Wenyu Liu. Ccnet: Criss-cross attention for semantic segmentation. In _Int. Conf. Comput. Vis._, pages 603-612, 2019.
* [31] Li Jiang, Hengshuang Zhao, Shu Liu, Xiaoyong Shen, Chi-Wing Fu, and Jiaya Jia. Hierarchical point-edge interaction network for point cloud semantic segmentation. In _Int. Conf. Comput. Vis._, 2019.
* [32] Chaitanya Kaul, Joshua Mitton, Hang Dai, and Roderick Murray-Smith. Convolutional point transformer. In _Asian Conf. Comput. Vis._, pages 303-319, 2022.
* [33] Xin Lai, Yukang Chen, Fanbin Lu, Jianhui Liu, and Jiaya Jia. Spherical transformer for lidar-based 3d recognition. In _IEEE Conf. Comput. Vis. Pattern Recog._, pages 17545-17555, 2023.
* [34] Xin Lai, Jianhui Liu, Li Jiang, Liwei Wang, Hengshuang Zhao, Shu Liu, Xiaojuan Qi, and Jiaya Jia. Stratified transformer for 3d point cloud segmentation. In _IEEE Conf. Comput. Vis. Pattern Recog._, pages 8500-8509, 2022.
* [35] Loic Landrieu and Martin Simonovsky. Large-scale point cloud semantic segmentation with superpoint graphs. In _IEEE Conf. Comput. Vis. Pattern Recog._, 2018.
* [36] Sanghyeok Lee, Minkyu Jeon, Injae Kim, Yunyang Xiong, and Hyunwoo J Kim. Sagemix: Saliency-guided mixup for point clouds. In _Adv. Neural Inform. Process. Syst._, 2022.
* [37] Huan Lei, Naveed Akhtar, and Ajmal Mian. Octree guided cnn with spherical kernels for 3d point clouds. In _IEEE Conf. Comput. Vis. Pattern Recog._, pages 9631-9640, 2019.
* [38] Huan Lei, Naveed Akhtar, and Ajmal Mian. Seggecn: Efficient 3d point cloud segmentation with fuzzy spherical kernel. In _IEEE Conf. Comput. Vis. Pattern Recog._, pages 11611-11620, 2020.
* [39] Yangyan Li, Rui Bu, Mingchao Sun, Wei Wu, Xinhan Di, and Baoquan Chen. Pointcnn: Convolution on x-transformed points. _Adv. Neural Inform. Process. Syst._, 31, 2018.
* [40] Venice Erin Liong, Thi Ngoc Tho Nguyen, Sergi Widjaja, Dhananjai Sharma, and Zhuang Jie Chong. Amvnet: Assertion-based multi-view fusion network for lidar semantic segmentation. _arXiv:2012.04934_, 2020.
* [41] Yongcheng Liu, Bin Fan, Gaofeng Meng, Jiwen Lu, Shiming Xiang, and Chunhong Pan. Densepoint: Learning densely contextual representation for efficient point cloud processing. In _Int. Conf. Comput. Vis._, pages 5239-5248, 2019.
* [42] Yongcheng Liu, Bin Fan, Shiming Xiang, and Chunhong Pan. Relation-shape convolutional neural network for point cloud analysis. In _IEEE Conf. Comput. Vis. Pattern Recog._, pages 8895-8904, 2019.
* [43] Ze Liu, Han Hu, Yue Cao, Zheng Zhang, and Xin Tong. A closer look at local aggregation operators in point cloud analysis. In _Eur. Conf. Comput. Vis._, pages 326-342, 2020.
* [44] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In _Int. Conf. Comput. Vis._, 2021.
* [45] Zhijian Liu, Xinyu Yang, Haotian Tang, Shang Yang, and Song Han. Flatformer: Flattened window attention for efficient point cloud transformer. _arXiv preprint arXiv:2301.08739_, 2023.
* [46] Ze Liu, Zheng Zhang, Yue Cao, Han Hu, and Xin Tong. Group-free 3d object detection via transformers. In _Int. Conf. Comput. Vis._, pages 2949-2958, 2021.
* [47] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In _International Conference on Learning Representations_.
* [48] Dening Lu, Qian Xie, Kyle Gao, Linlin Xu, and Jonathan Li. 3dctn: 3d convolution-transformer network for point cloud classification. _IEEE trans. Intell. Transp. Syst._, 23(12):24854-24865, 2022.
* [49] Xu Ma, Can Qin, Haoxuan You, Haoxai Ran, and Yun Fu. Rethinking network design and local geometry in point cloud: A simple residual mlp framework. _arXiv preprint arXiv:2202.07123_, 2022.
* [50] Jiageng Mao, Yujing Xue, Minzhe Niu, Haoyue Bai, Jiashi Feng, Xiaodan Liang, Hang Xu, and Chunjing Xu. Voxel transformer for 3d object detection. In _Int. Conf. Comput. Vis._, pages 3164-3173, 2021.
** [51] Andres Milioto, Ignacio Vizzo, Jens Behley, and Cyrill Stachniss. Rangenet++: Fast and accurate lidar semantic segmentation. In _IEEE/RSJ international conference on intelligent robots and systems (IROS)_, 2019.
* [52] Ishan Misra, Rohit Girdhar, and Armand Joulin. An end-to-end transformer model for 3d object detection. In _Int. Conf. Comput. Vis._, pages 2906-2917, 2021.
* [53] Gaku Narita, Takashi Seno, Tomoya Ishikawa, and Yohsuke Kaji. Panopticfusion: Online volumetric semantic mapping at the level of stuff and things. In _IEEE Int. Conf. Intell. Rob. Syst._, pages 4205-4212. IEEE, 2019.
* [54] Chunghyun Park, Yoonwoo Jeong, Minsu Cho, and Jaeik Park. Fast point transformer. In _IEEE Conf. Comput. Vis. Pattern Recog._, pages 16949-16958, 2022.
* [55] Charles R Qi, Or Litany, Kaiming He, and Leonidas J Guibas. Deep hough voting for 3d object detection in point clouds. In _Int. Conf. Comput. Vis._, pages 9277-9286, 2019.
* [56] Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas. Pointnet: Deep learning on point sets for 3d classification and segmentation. In _IEEE Conf. Comput. Vis. Pattern Recog._, pages 652-660, 2017.
* [57] Charles R Qi, Hao Su, Matthias Niessner, Angela Dai, Mengyuan Yan, and Leonidas J Guibas. Volumetric and multi-view cnns for object classification on 3d data. In _IEEE Conf. Comput. Vis. Pattern Recog._, pages 5648-5656, 2016.
* [58] Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J Guibas. Pointnet++: Deep hierarchical feature learning on point sets in a metric space. _Adv. Neural Inform. Process. Syst._, 30, 2017.
* [59] Guocheng Qian, Yuchen Li, Houwen Peng, Jinjie Mai, Hasan Hammoud, Mohamed Elhoseiny, and Bernard Ghanem. Pointnet: Revisiting pointnet++ with improved training and scaling strategies. _Adv. Neural Inform. Process. Syst._, 35:23192-23204, 2022.
* [60] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding by generative pre-training. 2018.
* [61] Gernot Riegler, Ali Osman Ulusoy, and Andreas Geiger. Octnet: Learning deep 3d representations at high resolutions. In _IEEE Conf. Comput. Vis. Pattern Recog._, 2017.
* [62] David Rozenbergszki, Or Litany, and Angela Dai. Language-grounded indoor 3d semantic segmentation in the wild. In _Eur. Conf. Comput. Vis._, pages 125-141, 2022.
* [63] Danila Rukhovich, Anna Vorontsova, and Anton Konushin. Fcaf3d: fully convolutional anchor-free 3d object detection. In _Eur. Conf. Comput. Vis._, pages 477-493, 2022.
* [64] Shuran Song, Samuel P. Lichtenberg, and Jianxiong Xiao. Sun rgb-d: A rgb-d scene understanding benchmark suite. In _IEEE Conf. Comput. Vis. Pattern Recog._, pages 567-576, 2015.
* [65] Hang Su, Subhransu Maji, Evangelos Kalogerakis, and Erik Learned-Miller. Multi-view convolutional neural networks for 3d shape recognition. In _Int. Conf. Comput. Vis._, pages 945-953, 2015.
* [66] Haotian Tang, Zhijian Liu, Shengyu Zhao, Yujun Lin, Ji Lin, Hanrui Wang, and Song Han. Searching efficient 3d architectures with sparse point-voxel convolution. In _Eur. Conf. Comput. Vis._, 2020.
* [67] Maxim Tatarchenko, Jaesik Park, Vladlen Koltun, and Qian-Yi Zhou. Tangent convolutions for dense prediction in 3d. In _IEEE Conf. Comput. Vis. Pattern Recog._, 2018.
* [68] Lyne Tchapmi, Christopher Choy, Iro Armeni, JunYoung Gwak, and Silvio Savarese. Segcloud: Semantic segmentation of 3d point clouds. In _Int. Conf. 3D Vis._, 2017.
* [69] Hugues Thomas, Charles R Qi, Jean-Emmanuel Deschaud, Beatriz Marcotegui, Francois Goulette, and Leonidas J Guibas. Kpconv: Flexible and deformable convolution for point clouds. In _Int. Conf. Comput. Vis._, pages 6411-6420, 2019.
* [70] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. _Adv. Neural Inform. Process. Syst._, 30, 2017.
* [71] Haiyang Wang, Shaocong Dong, Shaoshuai Shi, Aoxue Li, Jianan Li, Zhenguo Li, Liwei Wang, et al. Cagroup3d: Class-aware grouping for 3d object detection on point clouds. _Adv. Neural Inform. Process. Syst._, 35:29975-29988, 2022.
* [72] Huiyu Wang, Yukun Zhu, Bradley Green, Hartwig Adam, Alan Yuille, and Liang-Chieh Chen. Axial-deeplab: Stand-alone axial-attention for panoptic segmentation. In _Eur. Conf. Comput. Vis._, pages 108-126, 2020.
* [73] Lei Wang, Yuchun Huang, Yaolin Hou, Shenman Zhang, and Jie Shan. Graph attention convolution for point cloud semantic segmentation. In _IEEE Conf. Comput. Vis. Pattern Recog._, 2019.
* [74] Peng-Shuai Wang. Cotformer: Octree-based transformers for 3d point clouds. _arXiv preprint arXiv:2305.03045_, 2023.
* [75] Shenlong Wang, Simon Suo, Wei-Chiu Ma, Andrei Pokrovsky, and Raquel Urtasun. Deep parametric continuous convolutional neural networks. In _IEEE Conf. Comput. Vis. Pattern Recog._, 2018.
* [76] Yue Wang, Yongbin Sun, Ziwei Liu, Sanjay E. Sarma, Michael M. Bronstein, and Justin M. Solomon. Dynamic graph cnn for learning on point clouds. _ACM Trans. Graph._, 2019.
* [77] 3D Warehouse. Sketchup. https://3dwarehouse.sketchup.com/, 2022.
* [78] Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang Dai, Lu Yuan, and Lei Zhang. Cvt: Introducing convolutions to vision transformers. In _Int. Conf. Comput. Vis._, pages 22-31, 2021.

* [79] Wenxuan Wu, Zhongang Qi, and Li Fuxin. Pointconv: Deep convolutional networks on 3d point clouds. In _IEEE Conf. Comput. Vis. Pattern Recog._, pages 9621-9630, 2019.
* [80] Xiaoyang Wu, Yixing Lao, Li Jiang, Xihui Liu, and Hengshuang Zhao. Point transformer v2: Grouped vector attention and partition-based pooling. In _Adv. Neural Inform. Process. Syst._, 2022.
* [81] Zhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu, Linguang Zhang, Xiaoou Tang, and Jianxiong Xiao. 3d shapenets: A deep representation for volumetric shapes. In _IEEE Conf. Comput. Vis. Pattern Recog._, pages 1912-1920, 2015.
* [82] Tiange Xiang, Chaoyi Zhang, Yang Song, Jianhui Yu, and Weidong Cai. Walk in the cloud: Learning curves for point clouds shape analysis. In _Int. Conf. Comput. Vis._, pages 915-924, 2021.
* [83] Qian Xie, Yu-Kun Lai, Jing Wu, Zhoutao Wang, Dening Lu, Mingqiang Wei, and Jun Wang. Venet: Voting enhancement network for 3d object detection. In _Int. Conf. Comput. Vis._, pages 3712-3721, 2021.
* [84] Qian Xie, Yu-Kun Lai, Jing Wu, Zhoutao Wang, Yiming Zhang, Kai Xu, and Jun Wang. Mlcvnet: Multi-level context votenet for 3d object detection. In _IEEE Conf. Comput. Vis. Pattern Recog._, pages 10447-10456, 2020.
* [85] Jianyun Xu, Ruixiang Zhang, Jian Dou, Yushi Zhu, Jie Sun, and Shiliang Pu. Rpvnet: A deep and efficient range-point-voxel fusion network for lidar point cloud segmentation. In _Int. Conf. Comput. Vis._, 2021.
* [86] Mitian Xu, Runyu Ding, Hengshuang Zhao, and Xiaojuan Qi. Paconv: Position adaptive convolution with dynamic kernel assembling on point clouds. In _IEEE Conf. Comput. Vis. Pattern Recog._, pages 3173-3182, 2021.
* [87] Yufei Xu, Qiming Zhang, Jing Zhang, and Dacheng Tao. Vitae: Vision transformer advanced by exploring intrinsic inductive bias. _Adv. Neural Inform. Process. Syst._, 34, 2021.
* [88] Xu Yan, Jiantao Gao, Jie Li, Ruimao Zhang, Zhen Li, Rui Huang, and Shuguang Cui. Sparse single sweep lidar point cloud segmentation via learning contextual shape priors from scene completion. In _AAAI_, 2021.
* [89] Xu Yan, Jiantao Gao, Chaoda Zheng, Chao Zheng, Ruimao Zhang, Shuguang Cui, and Zhen Li. 2dpass: 2d priors assisted semantic segmentation on lidar point clouds. In _Eur. Conf. Comput. Vis._, 2022.
* [90] Xu Yan, Chaoda Zheng, Zhen Li, Sheng Wang, and Shuguang Cui. Pointasnl: Robust point clouds processing using nonlocal neural networks with adaptive sampling. In _IEEE Conf. Comput. Vis. Pattern Recog._, pages 5589-5598, 2020.
* [91] Jiancheng Yang, Qiang Zhang, Bingbing Ni, Linguo Li, Jinxian Liu, Mengdie Zhou, and Qi Tian. Modeling point clouds with self-attention and gumbel subset sampling. In _IEEE Conf. Comput. Vis. Pattern Recog._, 2019.
* [92] Yuqi Yang, Yuxiao Guo, Jianyu Xiong, Yang Liu, Hao Pan, Pengshuai Wang, Xin Tong, and Baining Guo. Swin3d: A pretrained transformer backbone for 3d indoor scene understanding. _arXiv preprint arXiv:2304.06906_, 2023.
* [93] Qiming Zhang, Yufei Xu, Jing Zhang, and Dacheng Tao. Vitaev2: Vision transformer advanced by exploring inductive bias for image recognition and beyond. _Int. J. Comput. Vis._, pages 1-22, 2023.
* [94] Yang Zhang, Zixiang Zhou, Philip David, Xiangyu Yue, Zerong Xi, Boqing Gong, and Hassan Foroosh. Polarnet: An improved grid representation for online lidar point clouds semantic segmentation. In _IEEE Conf. Comput. Vis. Pattern Recog._, 2020.
* [95] Zaiwei Zhang, Bo Sun, Haitao Yang, and Qixing Huang. H3dnet: 3d object detection using hybrid geometric primitives. In _Eur. Conf. Comput. Vis._, pages 311-329. Springer, 2020.
* [96] Hengshuang Zhao, Li Jiang, Chi-Wing Fu, and Jiaya Jia. Pointweb: Enhancing local neighborhood features for point cloud processing. In _IEEE Conf. Comput. Vis. Pattern Recog._, pages 5565-5573, 2019.
* [97] Hengshuang Zhao, Li Jiang, Jiaya Jia, Philip HS Torr, and Vladlen Koltun. Point transformer. In _Int. Conf. Comput. Vis._, pages 16259-16268, 2021.
* [98] Haoran Zhou, Yidan Feng, Mingsheng Fang, Mingqiang Wei, Jing Qin, and Tong Lu. Adaptive graph convolution for point cloud analysis. In _Int. Conf. Comput. Vis._, pages 4965-4974, 2021.
* [99] Xinge Zhu, Hui Zhou, Tai Wang, Fangzhou Hong, Yuexin Ma, Wei Li, Hongsheng Li, and Dahua Lin. Cylindrical and asymmetrical 3d convolution networks for lidar segmentation. In _IEEE Conf. Comput. Vis. Pattern Recog._, 2021.

## Appendix

In the appendix, we provide additional ablation results of window size and position encoding in Sec. A, more experiment results on outdoor perception tasks and object-level tasks in Sec. B.

## Appendix A Additional Ablation Results

**Ablation of window size.** As stated in the main paper, our proposed disassembled window attention offers a notable advantage over the vanilla 3D cubic window attention by significantly reducing computational effort, enabling the potential enlargement of the receptive field through an increase in the window size. However, we also acknowledge a limitation of our ConDaFormer, which is the degradation in performance when utilizing a larger attention window. In this section, we present detailed ablation results to further investigate this issue. To assess the impact of window size on the performance of ConDaFormer, we conduct experiments using varying window sizes: \(\{0.32m,0.48m,0.64m\}\), and present the corresponding results in Table 8. It is worth noting that as the window size expands, the training loss (Loss\({}_{t}\)) gradually decreases, and the performance on the training set (mIoU\({}_{t}\)) steadily improves. However, contrary to expectations, the performance on the validation set experiences a decline, indicating the occurrence of over-fitting. This phenomenon aligns with the observation made in LargeKernel [(8)] when increasing the convolutional kernel size. The introduction of a larger attention window incorporates additional positional embeddings, potentially resulting in optimization difficulties and leading to over-fitting. To address this issue, future research can explore techniques such as self-supervised or supervised pre-training on large-scale datasets. These approaches have shown promise in mitigating over-fitting and improving generalization performance. By leveraging such techniques, it is possible to enhance the robustness of ConDaFormer and enable the utilization of larger attention windows without suffering from performance degradation.

**Ablation of position encoding.** To enhance the modeling of crucial position information necessary for self-attention learning, we employ the contextual relative position encoding (cRPE) scheme introduced by Stratified Transformer [(34)]. In this context, we compare the performance of cRPE with two alternative position encoding schemes: Swin [(4)], wherein the learned relative position bias is directly added to the similarity between query and key, and PTv2 [(8)], which generates the position bias through an MLP that takes the relative position as input and subsequently adds it to the similarity between query and key. As shown in Table 9, cRPE outperforms the other schemes in two out of three metrics, indicating the significance of contextual features in effectively capturing fine-grained position information.

## Appendix B Additional Quantitative Results

In this section, we present additional quantitative results on four benchmark datasets: ModelNet40 [(8)] for shape classification, ShapeNet-Part [(77, 36)] for part segmentation, SemanticKITTI [(3)] for 3D semantic segmentation, and nuScenes [(4)] for both 3D semantic segmentation and 3D object detection.

**Results on ModelNet40 and ShapeNet-Part.** As shown in Table 10 and 11, we can find that for small-scale point cloud data, our method still achieves comparable or even better performance in comparison with Point Transformer v1, Point Transformer v2, and Stratified Transformer.

**Results on SemanticKITTI and nuScenes.** In summary, our ConDaFormer achieves comparable or slightly better performance on outdoor perception tasks compared to current methods, demonstrating ConDaFormer's potential usage as a generalized 3D backbone. Specifically, for 3D semantic segmentation, as shown in Table 12 and 13, ConDaFormer achieves 72.0% mIoU and 79.9% mIoU on the

\begin{table}
\begin{tabular}{l|c c c c c} \hline \hline Window & mIoU & mAcc & OA & Loss\({}_{t}\) & mIoU\({}_{t}\) \\ \hline
0.32m & **72.6** & **78.4** & **91.6** & 0.048 & 95.9 \\
0.48m & 72.1 & 78.3 & 91.5 & 0.045 & 96.0 \\
0.64m & 71.6 & 78.4 & 91.4 & **0.044** & **96.1** \\ \hline \hline \end{tabular}
\end{table}
Table 8: Ablation of window size.

\begin{table}
\begin{tabular}{l|c c c} \hline \hline Method & mIoU & mAcc & OA \\ \hline cRPE & **70.7** & **76.9** & 90.6 \\ Swin & 69.6 & 76.1 & 90.6 \\ PTv2 & 70.1 & 76.4 & **91.2** \\ \hline \hline \end{tabular}
\end{table}
Table 9: Ablation of position encoding.

[MISSING_PAGE_FAIL:16]