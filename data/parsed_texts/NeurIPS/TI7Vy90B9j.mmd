# Boosting Perturbed Gradient Ascent for Last-Iterate Convergence in Games

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

This paper introduces a payoff perturbation technique, introducing a strong convexity to players' payoff functions in games. This technique is specifically designed for first-order methods to achieve last-iterate convergence in games where the gradient of the payoff functions is monotone in the strategy profile space, potentially containing additive noise. Although perturbation is known to facilitate the convergence of learning algorithms, the magnitude of perturbation requires careful adjustment to ensure last-iterate convergence. Previous studies have proposed a scheme in which the magnitude is determined by the distance from an anchoring or reference strategy, which is periodically re-initialized. In response, this paper proposes Gradient Ascent with Boosting Payoff Perturbation, which incorporates a novel perturbation into the underlying payoff function, maintaining the periodically re-initializing anchoring strategy scheme. This innovation empowers us to provide faster last-iterate convergence rates against the existing payoff perturbed algorithms, even in the presence of additive noise.

## 1 Introduction

This study considers online learning in monotone games, where the gradient of the payoff function is monotone in the strategy profile space. Monotone games encompassed diverse well-studied games as special instances, such as concave-convex games, zero-sum polymatrix games (Cai and Daskalakis, 2011; Cai et al., 2016), \(\lambda\)-cocoercive games (Lin et al., 2020), and Cournot competition (Bravo et al., 2018). Due to their wide-ranging applications, there has been growing interest in developing learning algorithms to compute Nash equilibria in monotone games.

Typical learning algorithms such as Gradient Ascent (Zinkevich, 2003) and Multiplicative Weights Update (Bailey and Piliouras, 2018) have been extensively studied and shown to converge to equilibria in an average-iterate sense, which is termed _average-iterate convergence_. However, averaging the strategies can be undesirable because it can lead to additional memory or computational costs in the context of training Generative Adversarial Networks (Goodfellow et al., 2014) and preference-based fine-tuning of large language models (Munos et al., 2023; Swamy et al., 2024). In contrast, _last-iterate convergence_, in which the updated strategy profile itself converges to a Nash equilibrium, has emerged as a stronger notion than average-iterate convergence.

Payoff-perturbed algorithms have recently been regaining attention in this context (Sokota et al., 2023; Liu et al., 2023). Payoff perturbation is a classical technique, e.g., (Facchinei and Pang, 2003) and introduces a strongly convex penalty to the players' payoff functions to stabilize learning, which leads to convergence to approximate equilibria, not only in the _full feedback_ setting where the perfect gradient vector of the payoff function can be used to update strategies, but also in the _noisy feedback_ setting where the gradient vector is contaminated by noise.

However, to ensure convergence toward a Nash equilibrium of the underlying game, the magnitude of perturbation requires careful adjustment. As a remedy, it is adjusted by the distance from an anchoring or reference strategy. Koshal et al. (2010) and Tatarenko and Kamgarpour (2019) simply decay the magnitude in each iteration, and their methods asymptotically converge, since the perturbed function gradually loses strong convexity. In response to this, recent studies (Perolat et al., 2021; Abe et al., 2023, 2024) re-initialize the anchoring strategies periodically, or in a predefined interval, so that they keep the perturbed function strongly convex and achieve non-asymptotic convergence.

We should also mention the _optimistic_ family of learning algorithms, which incorporates recency bias and exhibits last-iterate convergence (Daskalakis et al., 2018; Daskalakis and Panageas, 2019; Mertikopoulos et al., 2019; Wei et al., 2021). Unfortunately, the property has mainly been proven in the full feedback setting. Although it might empirically work with noisy feedback, the convergence is slower, as demonstrated in Section 6. The fast convergence in the noisy feedback setting is another reason why payoff-perturbed algorithms have been gaining renewed interest.

This paper, in particular, focuses on _Adaptively Perturbed Mirror Descent_ (APMD) (Abe et al., 2024), which achieves \(\tilde{\mathcal{O}}(1/\sqrt{T})\)1 and \(\tilde{\mathcal{O}}(1/T^{\frac{1}{10}})\) last-iterate convergence rates in the full/noisy feedback setting, respectively. The motivation of this study lies in improving the convergence rates of APMD. We propose an elegant one-line modification of APMD, which effectively accelerates convergence. In fact, we just add the difference between the current anchoring strategy and the initial anchoring strategy to the payoff perturbation function in APMD.

Footnote 1: We use \(\tilde{\mathcal{O}}\) to denote a Landau notation that disregards a polylogarithmic factor.

Our contributions are manifold. Firstly, we propose a novel payoff-perturbed learning algorithm named _Gradient Ascent with Boosting Payoff Perturbation_ (GABP). This method incorporates a unique perturbation payoff function, enabling it to achieve faster convergence rates than APMD. Subsequently, we prove that GABP exhibits accelerated \(\tilde{\mathcal{O}}(1/T)\) and \(\tilde{\mathcal{O}}(1/T^{\frac{1}{10}})\) last-iterate convergence rates to a Nash equilibrium with full and noisy feedback, respectively. We further show that each player's individual regret is at most \(\mathcal{O}\left((\ln T)^{2}\right)\) in the full feedback setting, provided all players play according to GABP. Finally, through our experiments, we demonstrate the competitive or superior performance of GABP over Optimistic Gradient Ascent (Daskalakis et al., 2018; Wei et al., 2021) and APMD in concave-convex games, irrespective of the presence of noise.

## 2 Preliminaries

Monotone games.In this study, we focus on a continuous multi-player game, which is denoted as \(\left([N],(\mathcal{X}_{i})_{i\in[N]},(v_{i})_{i\in[N]}\right)\). \([N]=\{1,2,\cdots,N\}\) denotes the set of \(N\) players. Each player \(i\in[N]\) chooses a _strategy_\(\pi_{i}\) from a \(d_{i}\)-dimensional compact convex strategy space \(\mathcal{X}_{i}\), and we write \(\mathcal{X}=\prod_{i\in[N]}\mathcal{X}_{i}\). Each player \(i\) aims to maximize her payoff function \(v_{i}:\mathcal{X}\to\mathbb{R}\), which is differentiable on \(\mathcal{X}\). We denote \(\pi_{-i}\in\prod_{j\neq i}\mathcal{X}_{j}\) as the strategies of all players except player \(i\), and \(\pi=(\pi_{i})_{i\in[N]}\in\mathcal{X}\) as the _strategy profile_. This paper particularly studies learning in _smooth monotone games_, where the gradient operator \(V(\cdot)=(\nabla_{\pi_{i}}v_{i}(\cdot))_{i\in[N]}\) of the payoff functions is monotone: \(\forall\pi,\pi^{\prime}\in\mathcal{X}\),

\[\left\langle V(\pi)-V(\pi^{\prime}),\pi-\pi^{\prime}\right\rangle\leq 0,\] (1)

and \(L\)-Lipschitz for \(L>0\)

\[\left\|V(\pi)-V(\pi^{\prime})\right\|\leq L\left\|\pi-\pi^{\prime}\right\|,\] (2)

where \(\|\cdot\|\) denotes the \(\ell_{2}\)-norm.

Many common and well-studied games, such as concave-convex games, zero-sum polymatrix games (Cai et al., 2016), \(\lambda\)-cocoercive games (Lin et al., 2020), and Cournot competition (Bravo et al., 2018), are included in the class of monotone games.

**Example 2.1** (Concave-Convex Games).: Consider a game defined by \((\{1,2\},(\mathcal{X}_{1},\mathcal{X}_{2}),(v,-v))\), where \(v:\mathcal{X}_{1}\times\mathcal{X}_{2}\to\mathbb{R}\). In this game, player \(1\) wishes to maximize \(v\), while player \(2\) aims to minimize \(v\). If \(v\) is concave in \(x_{1}\in\mathcal{X}_{1}\) and convex in \(x_{2}\in\mathcal{X}_{2}\), the game is called a concave-convex game or minimax optimization problem, and it is not hard to see that this game is a special case of monotone games.

Nash equilibrium and gap function.A _Nash equilibrium_(Nash, 1951) is a widely used solution concept for a game, which is a strategy profile where no player can gain by changing her own strategy. Formally, a strategy profile \(\pi^{*}\in\mathcal{X}\) is called a Nash equilibrium, if and only if \(\pi^{*}\) satisfies the following condition:

\[\forall i\in[N],\forall\pi_{i}\in\mathcal{X}_{i},\;v_{i}(\pi_{i}^{*},\pi_{-i}^ {*})\geq v_{i}(\pi_{i},\pi_{-i}^{*}).\]

We define the set of all Nash equilibria to be \(\Pi^{*}\). It has been shown that there exists at least one Nash equilibrium (Debreu, 1952) for any smooth monotone games.

To quantify the proximity to Nash equilibrium for a given strategy profile \(\pi\in\mathcal{X}\), we use the _gap function_, which is defined as:

\[\mathrm{GAP}(\pi):=\max_{\tilde{\pi}\in\mathcal{X}}\left\langle V(\pi),\tilde {\pi}-\pi\right\rangle.\]

Additionally, we use another measure of proximity to Nash equilibrium, referred to as the _tangent residual_. This measure is defined as:

\[r^{\mathrm{tan}}(\pi):=\min_{a\in N_{\mathcal{X}}(\pi)}\left\|-V(\pi)+a\right\|,\]

where \(N_{\mathcal{X}}(\pi)=\{(a_{i})_{i\in[N]}\in\prod_{i=1}^{N}\mathbb{R}^{d_{i}} \mid\sum_{i=1}^{N}\langle a_{i},\pi_{i}^{\prime}-\pi_{i}\rangle\leq 0,\; \forall\pi^{\prime}\in\mathcal{X}\}\) is the normal cone of \(\pi\in\mathcal{X}\). It is easy to see that \(\mathrm{GAP}(\pi)\geq 0\) (resp. \(r^{\mathrm{tan}}(\pi)\geq 0\)) for any \(\pi\in\mathcal{X}\), and the equality holds if and only if \(\pi\) is a Nash equilibrium. Defining \(D:=\sup_{\pi,\pi^{\prime}\in\mathcal{X}}\left\|\pi-\pi^{\prime}\right\|\) as the diameter of \(\mathcal{X}\), the gap function for any given strategy profile \(\pi\in\mathcal{X}\) is upper bounded by its tangent residual.

**Lemma 2.2** (Lemma 2 of Cai et al. (2022)).: _For any \(\pi\in\mathcal{X}\), we have:_

\[\mathrm{GAP}(\pi)\leq D\cdot r^{\mathrm{tan}}(\pi).\]

The gap function and the tangent residual are standard measures of proximity to Nash equilibrium; e.g., it has been used in Cai and Zheng (2023); Abe et al. (2024).

Problem setting.This study focuses on the online learning setting in which the following process repeats from iterations \(t=1\) to \(T\): (i) Each player \(i\in[N]\) chooses her strategy \(\pi_{i}^{t}\in\mathcal{X}_{i}\), based on previously observed feedback; (ii) Each player \(i\) receives the (noisy) gradient vector \(\widehat{\nabla}_{\pi_{i}}v_{i}(\pi^{t})\) as feedback. This study examines two feedback models: _full feedback_ and _noisy feedback_. In the full feedback setting, each player observes the perfect gradient vector \(\widehat{\nabla}_{\pi_{i}}v_{i}(\pi^{t})=\nabla_{\pi_{i}}v_{i}(\pi^{t})\). In the noisy feedback setting, each player's gradient feedback \(\nabla_{\pi_{i}}v_{i}(\pi^{t})\) is contaminated by an additive noise vector \(\xi_{i}^{t}\), i.e., \(\widehat{\nabla}_{\pi_{i}}v_{i}(\pi^{t})=\nabla_{\pi_{i}}v_{i}(\pi^{t})+\xi_{i}^ {t}\), where \(\xi_{i}^{t}\in\mathbb{R}^{d_{i}}\). Throughout the paper, we assume that \(\xi_{i}^{t}\) is the zero-mean and bounded-variance noise vector at each iteration \(t\).

Adaptively perturbed Mirror Descent.To facilitate the convergence in the online learning setting, recent studies have utilized a _payoff perturbation_ technique, where payoff functions are perturbed by strongly convex functions (Sokota et al., 2023; Liu et al., 2023; Abe et al., 2022). However, while the addition of these strongly convex functions leads learning algorithms to converge to a stationary point, this stationary point may be significantly distant from a Nash equilibrium. Therefore, the magnitude of perturbation requires careful adjustment. Perolat et al. (2021); Abe et al. (2023, 2024) have introduced a scheme in which the magnitude is determined by the distance (or divergence function) from an anchoring strategy \(\sigma_{i}\), which is periodically re-initialized. Specifically, Adaptively Perturbed Mirror Descent (APMD) (Abe et al., 2024) perturbs each player's payoff function by a strongly convex divergence function \(G(\pi_{i},\sigma_{i}):\mathcal{X}_{i}\times\mathcal{X}_{i}\to[0,\infty)\), where the anchoring strategy \(\sigma_{i}\) is periodically replaced by the current strategy \(\pi_{i}^{t}\) every predefined iterations \(T_{\sigma}\).

Let us define \(\sigma_{i}^{k(t)}\) as the anchoring strategy after \(k(t)\) updates. Since \(\sigma_{i}\) is overwritten every \(T_{\sigma}\) iterations, we can write \(k(t)=\lfloor(t-1)/T_{\sigma}\rfloor+1\) and \(\sigma_{i}^{k(t)}=\pi_{i}^{T_{\sigma}(k(t)-1)+1}\). Except for the payoff perturbation and the update of the anchor strategy, APMD updates each player \(i\)'s strategy in the same way as standard Mirror Descent algorithms:

\[\pi_{i}^{t+1}=\operatorname*{arg\,max}_{x\in\mathcal{X}_{i}}\left\{\eta_{t} \left\langle\widehat{\nabla}_{\pi_{i}}v_{i}(\pi^{t})-\mu\nabla_{\pi_{i}}G(\pi_ {i}^{t},\sigma_{i}^{k(t)}),x\right\rangle-D_{\psi}(x,\pi_{i}^{t})\right\},\]where \(\eta_{t}\) is the learning rate at iteration \(t\), \(\mu\in(0,\infty)\) is the _perturbation strength_, and \(D_{\psi}(\pi_{i},\pi_{i}^{\prime})=\psi(\pi_{i})-\psi(\pi_{i}^{\prime})-\langle \nabla\hat{\psi}(\pi_{i}^{\prime}),\pi_{i}-\pi_{i}^{\prime}\rangle\) as the Bregman divergence associated with a strictly convex function \(\psi:\mathcal{X}_{i}\rightarrow\mathbb{R}\). When both \(G\) and \(D_{\psi}\) is set to the squared \(\ell^{2}\)-distance, this algorithm can be equivalently written as:

\[\pi_{i}^{t+1}=\operatorname*{arg\,max}_{x\in\mathcal{X}_{i}}\left\{\eta_{t} \left\langle\widehat{\nabla}_{\pi_{i}}v_{i}(\pi^{t})-\mu\left(\pi_{i}^{t}- \sigma_{i}^{k(t)}\right),x\right\rangle-\frac{1}{2}\left\|x-\pi_{i}^{t}\right\| ^{2}\right\}.\] (3)

We refer to this version of APMD as Adaptively Perturbed Gradient Ascent (APGA). Abe et al. [2024] have shown that APGA exhibits the convergence rates of \(\tilde{\mathcal{O}}(1/\sqrt{T})\) and \(\tilde{\mathcal{O}}(1/T^{\frac{1}{10}})\) with full and noisy feedback, respectively.

## 3 Gradient ascent with boosting payoff perturbation

This section proposes an accelerated version of APGA, Gradient Ascent with Boosting Payoff Perturbation (GABP). The pseudo-code of GABP is outlined in Algorithm 1. In order to obtain faster last-iterate convergence rates compared to APGA, GABP introduces a novel payoff perturbation term in addition to APGA's original payoff perturbation term, \(\mu\left(\pi_{i}^{t}-\sigma_{i}^{k(t)}\right)\). Formally, GABP updates each player's strategy as follows:

\[\pi_{i}^{t+1}=\operatorname*{arg\,max}_{x\in\mathcal{X}_{i}}\Bigg{\{}\eta_{t} \left\langle\widehat{\nabla}_{\pi_{i}}v_{i}(\pi^{t})-\mu\underbrace{\frac{ \sigma_{i}^{k(t)}-\sigma_{i}^{1}}{k(t)+1}}_{(*)}-\mu\left(\pi_{i}^{t}-\sigma_{ i}^{k(t)}\right),x\right\rangle-\frac{1}{2}\left\|x-\pi_{i}^{t}\right\|^{2} \Bigg{\}}.\] (4)

The term \((*)\) is our proposed additional perturbation term. It shrinks as \(k(t)\), the number of updates of \(\sigma_{i}^{k(t)}\), increases.

For a more intuitive explanation of the proposed perturbation term, we present the following update rule, which is equivalent to (4):

\[\pi_{i}^{t+1}=\operatorname*{arg\,max}_{x\in\mathcal{X}_{i}}\left\{\eta_{t} \left\langle\widehat{\nabla}_{\pi}v_{i}(\pi^{t})-\mu\left(\pi_{i}^{t}-\frac{k (t)\sigma_{i}^{k(t)}+\sigma_{i}^{1}}{k(t)+1}\right),x\right\rangle-\frac{1}{2} \left\|x-\pi_{i}^{t}\right\|^{2}\right\}.\]

From this formula, it appears that GABP replaces the reference strategy \(\sigma_{i}^{k(t)}\) for the perturbation term in (3) of APGA with \(\frac{k(t)\sigma_{i}^{k(t)}+\sigma_{i}^{1}}{k(t)+1}\). As a result, the anchoring strategy in GABP evolves more gradually than in APGA, leading to further stabilization of the learning dynamics. There is a tradeoffbetween the shrinking speed of the term (*) and the stabilizing impact on the last-iterate convergence rate of GABP. The shrinking speed of \(1/(k(t)+1)\) achieves a faster convergence rate, and we believe that this represents the optimal balance for this trade-off. Although one might think that the term (*) is closely related to Accelerated Optimistic Gradient (AOG) (Cai and Zheng, 2023), we discuss the detail in Appendix F to be concise and avoid a complicated explanation.

## 4 Last-iterate convergence rates

This section provides the last-iterate convergence rates of GABP in the full/noisy feedback setting, respectively.

### Full feedback setting

First, we demonstrate the last-iterate convergence rate of GABP with _full feedback_ where each player receives the perfect gradient vector as feedback at each iteration \(t\), i.e., \(\widehat{\nabla}_{\pi_{i}}v_{i}(\pi^{t})=\nabla_{\pi_{i}}v_{i}(\pi^{t})\). Theorem 4.1 shows that the last-iterate strategy profile \(\pi^{T}\) updated by GABP converges to a Nash equilibrium with an \(\tilde{\mathcal{O}}(1/T)\) rate in the full feedback setting.

**Theorem 4.1**.: _If we use the constant learning rate \(\eta_{t}=\eta\in(0,\frac{\mu}{(L+\mu)^{2}})\) and the constant perturbation strength \(\mu>0\), and set \(T_{\sigma}=c\cdot\max(1,\frac{6\ln 3(T+1)}{\ln(1+\eta\mu)})\) for some constant \(c\geq 1\), then the strategy \(\pi^{t}\) updated by GABP satisfies for any \(t\in\{2,3,\cdots,T+1\}\):_

\[\mathrm{GAP}(\pi^{t}) \leq\frac{17cD^{2}\left(\frac{6\ln 3(T+1)}{\ln(1+\eta\mu)}+1 \right)}{t-1}\left(\mu+\frac{1+\eta L}{\eta}\right),\text{and}\] \[r^{\tan}(\pi^{t}) \leq\frac{17cD\left(\frac{6\ln 3(T+1)}{\ln(1+\eta\mu)}+1\right)}{t-1 }\left(\mu+\frac{1+\eta L}{\eta}\right).\]

This rate is significantly faster than APGA's rate of \(\tilde{\mathcal{O}}(1/\sqrt{T})\). Moreover, it is a competitive rate compared to the previous state-of-the-art rate of \(\mathcal{O}(1/T)\)(Yoon and Ryu, 2021; Cai and Zheng, 2023). Note that the rate in Theorem 4.1 holds for any constant perturbation strength \(\mu>0\).

#### 4.1.1 Proof sketch of Theorem 4.1

To derive the bound of the gap function \(\mathrm{GAP}(\pi^{t})\), it is sufficient to derive that of \(r^{\tan}(\pi^{t})\) due to Lemma 2.2. This section provides the proof sketch of Theorem 4.1. The complete proof is placed in Appendix B.

**(1) Decomposition of the tangent residual of the last-iterate strategy profile.** From the first-order optimality condition for \(\pi^{t}\), we can see that \(V(\pi^{t-1})-\mu\left(\pi^{t-1}-\frac{k(t-1)\sigma^{k(t-1)}+\sigma^{t}}{k(t-1 )+1}\right)-\frac{1}{\eta}\left(\pi^{t}-\pi^{t-1}\right)\in N_{\mathcal{X}}( \pi^{t})\). Therefore, from the triangle inequality and \(L\)-smoothness (2) of the gradient operator, the tangent residual \(r^{\tan}(\pi^{t})\) can be bounded as:

\[r^{\tan}(\pi^{t}) =\min_{a\in N_{\mathcal{X}}(\pi^{t})}\left\|-V(\pi^{t})+a\right\|\] \[\leq\mathcal{O}\left(\left\|\pi^{t}-\pi^{t-1}\right\|\right)+ \mathcal{O}\left(\left\|\pi^{t-1}-\sigma^{k(t-1)}\right\|\right)+\mathcal{O} \left(\frac{1}{k(t-1)+1}\right).\]

Let us define the stationary point \(\pi^{\mu,\sigma^{k(t)}}\), which satisfies the following condition: \(\forall i\in[N]\),

\[\pi^{\mu,\sigma^{k(t)}}_{i}=\operatorname*{arg\,max}_{x\in\mathcal{X}_{i}} \left\{v_{i}(x,\pi^{\mu,\sigma^{k(t)}}_{-i})-\frac{\mu}{2}\left\|x-\hat{ \sigma}^{k(t)}\right\|^{2}\right\},\]

where \(\hat{\sigma}^{k(t)}_{i}=\frac{k(t)\sigma^{k(t)}_{i}+\sigma^{1}_{i}}{k(t)+1}\). We will show that \(\pi^{t}\) converges to the stationary point \(\pi^{\mu,\sigma^{k(t)}}\) at an exponential rate later. By using \(\pi^{\mu,\sigma^{k(t)}}\) and applying the triangle inequality to \(\|\pi^{t}-\pi^{t-1}\|\), we decompose the term of \(\mathcal{O}(\|\pi^{t}-\pi^{t-1}\|)\) into \(\mathcal{O}(\|\pi^{t}-\pi^{\mu,\sigma^{k(t-1)}}\|)\) and \(\mathcal{O}(\|\pi^{\mu,\sigma^{k(t-1)}}-\pi^{t-1}\|)\).

Similarly, the term of \(\mathcal{O}(\|\pi^{t-1}-\sigma^{k(t-1)}\|)\) is decomposed into \(\mathcal{O}(\|\pi^{t-1}-\pi^{\mu,\sigma^{k(t)-1}}\|)\) and \(\mathcal{O}(\|\pi^{\mu,\sigma^{k(t)-1}}-\sigma^{k(t-1)}\|)\). Then, the tangent residual is bounded as follows:

\[r^{\tan}(\pi^{t}) \leq\mathcal{O}\left(\left\|\pi^{\mu,\sigma^{k(t-1)}}-\pi^{t} \right\|\right)+\mathcal{O}\left(\left\|\pi^{\mu,\sigma^{k(t-1)}}-\pi^{t-1} \right\|\right)\] \[\quad+\mathcal{O}\left(\left\|\pi^{\mu,\sigma^{k(t-1)}}-\sigma^{ k(t-1)}\right\|\right)+\mathcal{O}\left(\frac{1}{k(t-1)+1}\right).\] (5)

Therefore, it is enough to derive the convergence rate on \(\|\pi^{\mu,\sigma^{k(t)-1}}-\pi^{t}\|\) and \(\|\pi^{\mu,\sigma^{k(t-1)}}-\sigma^{k(t-1)}\|\).

**(2) Convergence rate of \(\pi^{t}\) to the stationary point \(\pi^{\mu,\sigma^{k(t)}}\).** Using the strong convexity of the perturbation payoff function, \(\frac{\mu}{2}\|x-\hat{\sigma}_{i}^{k(t)}\|^{2}\), we show that \(\pi^{t}\) converges to \(\pi^{\mu,\sigma^{k(t)}}\) exponentially fast (in Lemma B.1). That is, we have for any \(t\geq 1\):

\[\left\|\pi^{\mu,\sigma^{k(t)}}-\pi^{t}\right\|^{2}\leq\left(\frac{1}{1+\eta\mu }\right)^{t-(k(t)-1)T_{\sigma}-1}\left\|\pi^{\mu,\sigma^{k(t)}}-\sigma^{k(t)} \right\|^{2}.\] (6)

Since the first and second terms of the right-hand side of (5) are bounded by the distance between the stationary point and the anchoring strategy by using (6), we have:

\[r^{\tan}(\pi^{t})\leq\mathcal{O}\left(\left\|\pi^{\mu,\sigma^{k(t-1)}}-\sigma^ {k(t-1)}\right\|\right)+\mathcal{O}\left(\frac{1}{k(t-1)+1}\right).\] (7)

**(3) Potential function for bounding the distance between \(\pi^{\mu,\sigma^{k(t)-1}}\) and \(\sigma^{k(t)-1}\).** To derive the upper bound on \(\left\|\pi^{\mu,\sigma^{k(t-1)}}-\sigma^{k(t-1)}\right\|\), we define the following potential function \(P^{k(t)}\):

\[P^{k(t)} :=\frac{k(t)(k(t+1)+1)}{2}\left\|\pi^{\mu,\sigma^{k(t)-1}}-\hat{ \sigma}^{k(t)-1}\right\|^{2}\] \[\quad+k(t)(k(t)+1)\left\langle\hat{\sigma}^{k(t)}-\pi^{\mu, \sigma^{k(t)-1}},\pi^{\mu,\sigma^{k(t)-1}}-\hat{\sigma}^{k(t)-1}\right\rangle.\]

By some algebra, we can see that \(P^{k(t)}\) is approximately non-increasing (in Lemma B.3). That is, we have for any \(t\geq 1\) such that \(k(t)\geq 2\):

\[P^{k(t)+1}\leq P^{k(t)}+(k(t)+1)^{2}\cdot\mathcal{O}\left(\left\|\pi^{\mu, \sigma^{k(t)}}-\sigma^{k(t)+1}\right\|+\left\|\pi^{\mu,\sigma^{k(t)-1}}-\sigma ^{k(t)}\right\|\right).\] (8)

Using (6) again, it is easy to show that \(\left\|\pi^{\mu,\sigma^{k(t)}}-\sigma^{k(t)+1}\right\|+\left\|\pi^{\mu,\sigma ^{k(t)-1}}-\sigma^{k(t)}\right\|\leq\mathcal{O}\left(\frac{1}{(k(t)+1)^{3}}\right)\) for a sufficiently large \(T_{\sigma}\). Therefore, under the assumption that \(T_{\sigma}\geq\Omega\left(\ln T\right)\), by telescoping of (8) and some algebra, we can derive the following upper bound on \(\left\|\pi^{\mu,\sigma^{k(t)}}-\sigma^{k(t)}\right\|\) (in Lemma B.2):

\[\left\|\pi^{\mu,\sigma^{k(t)}}-\sigma^{k(t)}\right\|\leq\mathcal{O}\left(\frac{ 1}{k(t)+1}\right).\] (9)

**(4) Putting it all together: last-iterate convergence rate of \(\pi^{t}\).** By combining (7) and (9), we get \(r^{\tan}(\pi^{t})\leq\mathcal{O}\left(\frac{1}{k(t-1)+1}\right)\). Therefore, since \(k(t)=\lfloor\frac{t-1}{T_{\sigma}}\rfloor+1\), it holds that \(r^{\tan}(\pi^{t})\leq\mathcal{O}\left(\frac{T_{\sigma}}{t+T_{\sigma}-2}\right)\). Finally, taking \(T_{\sigma}=\Theta(\ln T)\), we have:

\[r^{\tan}(\pi^{t})\leq\mathcal{O}\left(\frac{\ln T}{t-1}\right).\]

The upper bound on the gap function is immediately obtained since we have Lemma 2.2. 

### Noisy feedback setting

Next, we establish the last-iterate convergence rate in the _noisy feedback_ setting, where each player \(i\) observes a noisy gradient vector contaminated by an additive noise vector \(\xi_{i}^{t}\in\mathbb{R}^{d_{i}}\): \(\widehat{\nabla}_{\pi_{i}}v_{i}(\pi^{t})+\xi_{i}^{t}\). We assume that the noisy vector \(\xi_{i}^{t}\) is zero-mean and its variance is bounded. Formally, defining the sigma-algebra generated by the history of the observations as \(\mathcal{F}_{t}:=\sigma\left((\widehat{\nabla}_{\pi_{i}}v_{i}(\pi^{1}))_{i\in[ N]},\ldots,(\widehat{\nabla}_{\pi_{i}}v_{i}(\pi^{t-1}))_{i\in[N]}\right)\), \(\forall t\geq 1\), the noisy vector \(\xi_{i}^{t}\) is assumed to satisfy the following conditions:

**Assumption 4.2**.: _For all \(t\geq 1\) and \(i\in[N]\), the noise vector \(\xi_{i}^{t}\) satisfies the following properties: (a) Zero-mean: \(\mathbb{E}[\xi_{i}^{t}|\mathcal{F}_{t}]=(0,\cdots,0)^{\top}\); (b) Bounded variance: \(\mathbb{E}[\|\xi_{i}^{t}\|^{2}|\mathcal{F}_{t}]\leq C^{2}\) with some constant \(C>0\)._

Assumption 4.2 is standard in online learning in games with noisy feedback (Mertikopoulos and Zhou, 2019; Hsieh et al., 2019; Abe et al., 2024) and stochastic optimization (Nemirovski et al., 2009; Nedic and Lee, 2014). Under Assumption 4.2 and a decreasing learning rate sequence \(\eta_{t}\), we can obtain a faster last convergence rate \(\tilde{\mathcal{O}}(1/T^{\frac{1}{\eta_{t}}})\) than the convergence rate \(\tilde{\mathcal{O}}(1/T^{\frac{1}{\eta_{t}}})\) of APGA.

**Theorem 4.3**.: _Let \(\kappa=\frac{\mu}{2},\theta=\frac{3\mu^{2}+8L^{2}}{2\mu}\). Suppose that Assumption 4.2 holds and \(V(\pi)\leq\zeta\) for any \(\pi\in\mathcal{X}\). We also assume that \(T_{\sigma}\) is set to satisfy \(T_{\sigma}=c\cdot\max(T^{\frac{\pi}{2}},1)\) for some constant \(c\geq 1\). If we use the constant perturbation strength \(\mu>0\) and the decreasing learning rate sequence \(\eta_{t}=\frac{1}{\kappa(t-T_{\sigma}(k(t-1))+2\theta}\), then the strategy \(\pi^{T+1}\) satisfies:_

\[\mathbb{E}\left[\mathrm{GAP}(\pi^{T+1})\right]\] \[\leq\frac{26c\left(D(\mu+L)+\zeta\right)\sqrt{(D+1)(D+\theta)+ \kappa}}{T^{\frac{1}{\eta}}}\left(\sqrt{\frac{1}{\kappa}\left(D^{2}+\frac{C^{ 2}}{\kappa\theta}\ln\left(\frac{\kappa T}{2\theta}+1\right)\right)}+1\right).\]

Note that the non-increasing property, as described in (8), of the potential function holds regardless of the presence of noise. This implies that a proof technique similar to the one used with the potential function in the full feedback setting can also be applied in the noisy feedback setting. The detailed proof can be found in Appendix C.

## 5 Individual regret bound

In this section, we present an upper bound on an individual regret for each player. Specifically, we examine two performance measures in our study: the _external regret_ and the _dynamic regret_(Zinkevich, 2003). The external regret is a conventional measure in online learning. In online learning in games, the external regret for player \(i\) is defined as the gap between the player's realized cumulative payoff and the cumulative payoff of the best fixed strategy in hindsight:

\[\mathrm{Reg}_{i}(T):=\max_{x\in\mathcal{A}_{i}}\sum_{t=1}^{T}\left(v_{i}(x, \pi_{-i}^{t})-v_{i}(\pi^{t})\right).\]

The dynamics regret is a much stronger performance metric, which is given by:

\[\mathrm{DynamicReg}_{i}(T):=\sum_{t=1}^{T}\left(\max_{x\in\mathcal{X}_{i}}v_ {i}(x,\pi_{-i}^{t})-v_{i}(\pi^{t})\right).\]

We show in Theorem 5.1 that the individual regret is at most \(\mathcal{O}\left((\ln T)^{2}\right)\) if each player \(i\in[N]\) plays according to GABP in the full feedback setting:

**Theorem 5.1**.: _In the same setup of Theorem 4.1, we have for any player \(i\in[N]\) and \(T\geq 3\):_

\[\mathrm{Reg}_{i}(T)\leq\mathrm{DynamicReg}_{i}(T)\leq\mathcal{O}\left((\ln T) ^{2}\right).\]

This regret bound is significantly superior to the \(\mathcal{O}(\sqrt{T})\) regret bound of Optimistic Gradient Ascent, and it is slightly inferior to the \(\mathcal{O}(\ln T)\) regret bound of AOG (Cai and Zheng, 2023). The proof is given in Appendix D.

## 6 Experiments

In this section, we present the empirical results of our GABP, comparing its performance with Adaptively Perturbed Gradient Ascent (APGA) (Abe et al., 2024) and Optimistic Gradient Ascent (OGA) (Daskalakis et al., 2018; Wei et al., 2021). We conduct experiments on two classes of concave-convex games. The first experiment is carried out on random payoff games, which are two-player zero-sum normal-form games with payoff matrices of size \(d\). In this game, each player's strategyspace is represented by the \(d\)-dimensional probability simplex, i.e., \(\mathcal{X}_{1}=\mathcal{X}_{2}=\Delta^{d}\). All entries of the payoff matrix are drawn independently from a uniform distribution over the interval \([-1,1]\). We set \(d=50\) and the initial strategies are set to \(\pi_{1}^{1}=\pi_{2}^{1}=\frac{1}{u}\mathbf{1}\). The second instance is a _hard concave-convex game_[20], formulated as the following max-min optimization problem: \(\max_{x\in\mathcal{X}_{1}}\min_{y\in\mathcal{X}_{2}}f(x,y)\), where \(f(x,y)=-\frac{1}{2}x^{\top}Hx+h^{\top}x+\langle Ax-b,y\rangle\). Following the setup in Cai and Zheng [2023], we choose \(\mathcal{X}_{1}=\mathcal{X}_{2}=[-200,200]^{d}\) with \(d=100\). The precise terms of \(H\in\mathbb{R}^{d\times d},A\in\mathbb{R}^{d\times d},b\in\mathbb{R}^{d}\), and \(h\in\mathbb{R}^{d}\) are provided in Appendix E.2. All algorithms are executed with initial strategies \(\pi_{1}^{1}=\pi_{2}^{1}=\frac{1}{u}\mathbf{1}\). The detailed hyperparameters of the algorithms, tuned for best performance, are shown in Table 1 in Appendix E.3.

The numerical results of the random payoff game and the hard concave-convex game are shown in Figure 1. Both the full feedback and noisy feedback experiments in the random payoff game were conducted with \(50\) different random seeds, which corresponds to using \(50\) different payoff matrices. For experiments on the hard concave-convex game with noisy feedback, we use \(10\) different random seeds. We assume that the noise vector \(\xi_{i}^{t}\) is generated from the multivariate Gaussian distribution \(\mathcal{N}(0,\ 0.1^{2}\mathbf{I})\) in an i.i.d. manner for both games. We observe that GABP exhibits competitive or faster performance over APGA and OGA in all experiments.

Figure 2 illustrates the dynamic regret in the hard concave-convex game. GABP exhibits lower regret than APGA and OGA in both settings, demonstrating its efficiency and robustness. Note that APGA and OGA exhibit almost identical trajectories with full feedback, with their plots overlapping completely.

## 7 Related literature

No-regret learning algorithms have been extensively studied with the intent of achieving key objectives such as average-iterate convergence or last-iterate convergence. Recently, learning algorithms introducing optimism [23, 24], such as optimistic Follow the Regularized Leader [21] and optimistic Mirror Descent [20, 22], have been introduced to admit last-iterate convergence in a broad spectrum of game

Figure 1: Performance of \(\pi^{t}\) for GABP, APGA, and OGA with full and noisy feedback in the random payoff and hard concave-convex games, respectively. The shaded area represents the standard errors. Note that we report the gap function for the random payoff game, while the tangent residual is reported for the hard concave-convex game.

Figure 2: Dynamic regret for GABP, APGA, and OGA with full and noisy feedback.

settings. These optimistic algorithms with full feedback have been shown to achieve last-iterate convergence in various classes of games, including bilinear games (Daskalakis et al., 2018; Daskalakis and Panageas, 2019; Liang and Stokes, 2019; de Montbrun and Renault, 2022), cocoercive games (Lin et al., 2020), and saddle point problems (Daskalakis and Panageas, 2018; Mertikopoulos et al., 2019; Golowich et al., 2020; Wei et al., 2021; Lei et al., 2021; Yoon and Ryu, 2021; Lee and Kim, 2021; Cevher et al., 2023). Recent studies have provided finite convergence rates for monotone games (Golowich et al., 2020; Cai et al., 2022; Nguyen et al., 2022; Gorbunov et al., 2022; Cai and Zheng, 2023).

Compared to the full feedback setting, there are significant challenges in learning with noisy feedback. For example, a learning algorithm must estimate the gradient from feedback that is contaminated by noise. Despite the challenge, a vast literature has successfully achieved last-iterate convergence with noisy feedback in specific classes of games, including potential games (Cohen et al., 2017), strongly monotone games (Giannou et al., 2021; Li et al., 2021), and two-player zero-sum games (Abe et al., 2023). These results have often leveraged unique structures of their payoff functions, such as strict (or strong) monotonicity (Bravo et al., 2018; Kannan and Shanbhag, 2019; Hsieh et al., 2019; Anagnostides and Panageas, 2022) and strict variational stability (Mertikopoulos et al., 2019; Azizian et al., 2021; Mertikopoulos and Zhou, 2019; Mertikopoulos et al., 2022). Without these restrictions, convergence is mainly demonstrated in an asymptotic manner, with no quantification of the rate (Koshal et al., 2010, 2013; Yousefian et al., 2017; Tatarenko and Kamgarpour, 2019; Hsieh et al., 2020, 2022; Abe et al., 2023). Consequently, an exceedingly large number of iterations might be necessary to reach an equilibrium.

There have been several studies focusing on payoff-regularized learning, where each player's payoff or utility function is perturbed or regularized via strongly convex functions (Cen et al., 2021, 2023; Pattathil et al., 2023). Previous studies have successfully achieved convergence to stationary points, which are approximate equilibria. For instance, Sokota et al. (2023) have demonstrated that their perturbed mirror descent algorithm converges to a quantal response equilibrium (McKelvey and Palfrey, 1995, 1998). Similar results have been obtained with the Boltzmann Q-learning dynamics (Tuyls et al., 2006) and penalty-regularized dynamics (Coucheney et al., 2015) in continuous-time settings (Leslie and Collins, 2005; Abe et al., 2022; Hussain et al., 2023). To ensure convergence toward a Nash equilibrium of the underlying game, the magnitude of perturbation requires careful adjustment. Several learning algorithms have been proposed to gradually reduce the perturbation strength \(\mu\) in response to this (Bernasconi et al., 2022; Liu et al., 2023; Cai et al., 2023). These include well-studied methods such as iterative Tikhonov regularization (Facchinei and Pang, 2003; Koshal et al., 2010; Tatarenko and Kamgarpour, 2019). Alternatively, Perolat et al. (2021) and Abe et al. (2023) have employed a payoff perturbation scheme, where the magnitude of perturbation is determined by the distance from an anchoring strategy, which is periodically re-initialized by the current strategy. Recently, Abe et al. (2024) have established \(\tilde{\mathcal{O}}(1/\sqrt{T})\) and \(\tilde{\mathcal{O}}(1/T^{\frac{1}{10}})\) last-iterate convergence rates for the payoff perturbation scheme in the full/noisy feedback setting, respectively. Our algorithm achieves faster \(\tilde{\mathcal{O}}(1/T)\) and \(\tilde{\mathcal{O}}(1/T^{\frac{1}{1}})\) last-iterate convergence rates by modifying the periodically re-initializing anchoring strategy scheme so that the anchoring strategy evolves more gradually.

## 8 Conclusion

This study proposes a novel payoff-perturbed algorithm, Gradient Ascent with Boosting Payoff Perturbation, which achieves \(\tilde{\mathcal{O}}(1/T)\) and \(\tilde{\mathcal{O}}(1/T^{\frac{1}{10}})\) last-iterate convergence rates in monotone games with full/noisy feedback, respectively. Extending our results in settings where each player only observes bandit feedback is an intriguing and challenging future direction.

## References

* Abe et al. (2022) Kenshi Abe, Mitsuki Sakamoto, and Atsushi Iwasaki. Mutation-driven follow the regularized leader for last-iterate convergence in zero-sum games. In _UAI_, pages 1-10, 2022.
* Abe et al. (2023) Kenshi Abe, Kaito Ariu, Mitsuki Sakamoto, Kentaro Toyoshima, and Atsushi Iwasaki. Last-iterate convergence with full and noisy feedback in two-player zero-sum games. In _AISTATS_, pages 7999-8028, 2023.

* Abe et al. [2024] Kenshi Abe, Kaito Ariu, Mitsuki Sakamoto, and Atsushi Iwasaki. Adaptively perturbed mirror descent for learning in games. In _ICML_, 2024.
* Anagnostides and Panageas [2022] Ioannis Anagnostides and Ioannis Panageas. Frequency-domain representation of first-order methods: A simple and robust framework of analysis. In _SOSA_, pages 131-160, 2022.
* Azizian et al. [2021] Waiss Azizian, Franck Iutzeler, Jerome Malick, and Panayotis Mertikopoulos. The last-iterate convergence rate of optimistic mirror descent in stochastic variational inequalities. In _COLT_, pages 326-358, 2021.
* Bailey and Piliouras [2018] James P Bailey and Georgios Piliouras. Multiplicative weights update in zero-sum games. In _Economics and Computation_, pages 321-338, 2018.
* Bernasconi et al. [2022] Martino Bernasconi, Alberto Marchesi, and Francesco Trovo. Last-iterate convergence to trembling-hand perfect equilibria. _arXiv preprint arXiv:2208.08238_, 2022.
* Bravo et al. [2018] Mario Bravo, David Leslie, and Panayotis Mertikopoulos. Bandit learning in concave N-person games. In _NeurIPS_, pages 5666-5676, 2018.
* Cai and Daskalakis [2011] Yang Cai and Constantinos Daskalakis. On minmax theorems for multiplayer games. In _SODA_, pages 217-234, 2011.
* Cai and Zheng [2023] Yang Cai and Weiqiang Zheng. Doubly optimal no-regret learning in monotone games. In _ICML_, pages 3507-3524, 2023.
* Cai et al. [2016] Yang Cai, Ozan Candogan, Constantinos Daskalakis, and Christos Papadimitriou. Zero-sum polymatrix games: A generalization of minmax. _Mathematics of Operations Research_, 41(2):648-655, 2016.
* Cai et al. [2022a] Yang Cai, Argyris Oikonomou, and Weiqiang Zheng. Finite-time last-iterate convergence for learning in multi-player games. In _NeurIPS_, pages 33904-33919, 2022a.
* Cai et al. [2022b] Yang Cai, Argyris Oikonomou, and Weiqiang Zheng. Tight last-iterate convergence of the extragradient method for constrained monotone variational inequalities. _arXiv preprint arXiv:2204.09228_, 2022b.
* Cai et al. [2023] Yang Cai, Haipeng Luo, Chen-Yu Wei, and Weiqiang Zheng. Uncoupled and convergent learning in two-player zero-sum markov games with bandit feedback. In _NeurIPS_, pages 36364-36406, 2023.
* Cen et al. [2021] Shicong Cen, Yuting Wei, and Yuejie Chi. Fast policy extragradient methods for competitive games with entropy regularization. In _NeurIPS_, pages 27952-27964, 2021.
* Cen et al. [2023] Shicong Cen, Yuejie Chi, Simon S Du, and Lin Xiao. Faster last-iterate convergence of policy optimization in zero-sum Markov games. In _ICLR_, 2023.
* Cevher et al. [2023] Volkan Cevher, Georgios Piliouras, Ryann Sim, and Stratis Skoulakis. Min-max optimization made simple: Approximating the proximal point method via contraction maps. In _Symposium on Simplicity in Algorithms (SOSA)_, pages 192-206, 2023.
* Cohen et al. [2017] Johanne Cohen, Amelie Heliou, and Panayotis Mertikopoulos. Learning with bandit feedback in potential games. In _NeurIPS_, pages 6372-6381, 2017.
* Coucheney et al. [2015] Pierre Coucheney, Bruno Gaujal, and Panayotis Mertikopoulos. Penalty-regulated dynamics and robust learning procedures in games. _Mathematics of Operations Research_, 40(3):611-633, 2015.
* Daskalakis and Panageas [2018] Constantinos Daskalakis and Ioannis Panageas. The limit points of (optimistic) gradient descent in min-max optimization. In _NeurIPS_, pages 9256-9266, 2018.
* Daskalakis and Panageas [2019] Constantinos Daskalakis and Ioannis Panageas. Last-iterate convergence: Zero-sum games and constrained min-max optimization. In _ITCS_, pages 27:1-27:18, 2019.
* Daskalakis et al. [2018] Constantinos Daskalakis, Andrew Ilyas, Vasilis Syrgkanis, and Haoyang Zeng. Training GANs with optimism. In _ICLR_, 2018.
* Daskalakis et al. [2019]Etienne de Montbrun and Jerome Renault. Convergence of optimistic gradient descent ascent in bilinear games. _arXiv preprint arXiv:2208.03085_, 2022.
* Debreu [1952] Gerard Debreu. A social equilibrium existence theorem. _Proceedings of the National Academy of Sciences_, 38(10):886-893, 1952.
* Facchinei and Pang [2003] Francisco Facchinei and Jong-Shi Pang. _Finite-dimensional variational inequalities and complementarity problems_. Springer, 2003.
* Giannou et al. [2021a] Angeliki Giannou, Emmanouil Vasileios Vlatakis-Gkaragkounis, and Panayotis Mertikopoulos. Survival of the strictest: Stable and unstable equilibria under regularized learning with partial information. In _COLT_, pages 2147-2148, 2021a.
* Giannou et al. [2021b] Angeliki Giannou, Emmanouil-Vasileios Vlatakis-Gkaragkounis, and Panayotis Mertikopoulos. On the rate of convergence of regularized learning in games: From bandits and uncertainty to optimism and beyond. In _NeurIPS_, pages 22655-22666, 2021b.
* Golowich et al. [2020a] Noah Golowich, Sarath Pattathil, and Constantinos Daskalakis. Tight last-iterate convergence rates for no-regret learning in multi-player games. In _NeurIPS_, pages 20766-20778, 2020a.
* Golowich et al. [2020b] Noah Golowich, Sarath Pattathil, Constantinos Daskalakis, and Asuman Ozdaglar. Last iterate is slower than averaged iterate in smooth convex-concave saddle point problems. In _COLT_, pages 1758-1784, 2020b.
* Goodfellow et al. [2014] Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In _NeurIPS_, pages 2672-2680, 2014.
* Gorbunov et al. [2022] Eduard Gorbunov, Adrien Taylor, and Gauthier Gidel. Last-iterate convergence of optimistic gradient method for monotone variational inequalities. In _NeurIPS_, pages 21858-21870, 2022.
* Hsieh et al. [2019] Yu-Guan Hsieh, Franck Iutzeler, Jerome Malick, and Panayotis Mertikopoulos. On the convergence of single-call stochastic extra-gradient methods. In _NeurIPS_, pages 6938-6948, 2019.
* Hsieh et al. [2020] Yu-Guan Hsieh, Franck Iutzeler, Jerome Malick, and Panayotis Mertikopoulos. Explore aggressively, update conservatively: Stochastic extragradient methods with variable stepsize scaling. In _NeurIPS_, pages 16223-16234, 2020.
* Hsieh et al. [2021] Yu-Guan Hsieh, Kimon Antonakopoulos, and Panayotis Mertikopoulos. Adaptive learning in continuous games: Optimal regret bounds and convergence to Nash equilibrium. In _COLT_, pages 2388-2422, 2021.
* Hsieh et al. [2022] Yu-Guan Hsieh, Kimon Antonakopoulos, Volkan Cevher, and Panayotis Mertikopoulos. No-regret learning in games with noisy feedback: Faster rates and adaptivity via learning rate separation. In _NeurIPS_, pages 6544-6556, 2022.
* Hussain et al. [2023] Aamal Abbas Hussain, Francesco Belardinelli, and Georgios Piliouras. Asymptotic convergence and performance of multi-agent Q-learning dynamics. _arXiv preprint arXiv:2301.09619_, 2023.
* Kannan and Shanbhag [2019] Aswin Kannan and Uday V. Shanbhag. Optimal stochastic extragradient schemes for pseudomonotone stochastic variational inequality problems and their variants. _Computational Optimization and Applications_, 74(3):779-820, 2019.
* Koshal et al. [2010] Jayash Koshal, Angelia Nedic, and Uday V Shanbhag. Single timescale regularized stochastic approximation schemes for monotone nash games under uncertainty. In _CDC_, pages 231-236. IEEE, 2010.
* Koshal et al. [2013] Jayash Koshal, Angelia Nedic, and Uday V. Shanbhag. Regularized iterative stochastic approximation methods for stochastic variational inequality problems. _IEEE Transactions on Automatic Control_, 58(3):594-609, 2013.
* Lee and Kim [2021] Sucheol Lee and Donghwan Kim. Fast extra gradient methods for smooth structured nonconvex-nonconcave minimax problems. In _NeurIPS_, pages 22588-22600, 2021.
* Lee et al. [2021]Qi Lei, Sai Ganesh Nagarajan, Ioannis Panageas, et al. Last iterate convergence in no-regret learning: constrained min-max optimization for convex-concave landscapes. In _AISTATS_, pages 1441-1449, 2021.
* Leslie and Collins [2005] David S Leslie and Edmund J Collins. Individual q-learning in normal form games. _SIAM Journal on Control and Optimization_, 44(2):495-514, 2005.
* Liang and Stokes [2019] Tengyuan Liang and James Stokes. Interaction matters: A note on non-asymptotic local convergence of generative adversarial networks. In _AISTATS_, pages 907-915, 2019.
* Lin et al. [2020] Tianyi Lin, Zhengyuan Zhou, Panayotis Mertikopoulos, and Michael Jordan. Finite-time last-iterate convergence for multi-agent learning in games. In _ICML_, pages 6161-6171, 2020.
* Liu et al. [2023] Mingyang Liu, Asuman Ozdaglar, Tiancheng Yu, and Kaiqing Zhang. The power of regularization in solving extensive-form games. In _ICLR_, 2023.
* McKelvey and Palfrey [1995] Richard D McKelvey and Thomas R Palfrey. Quantal response equilibria for normal form games. _Games and economic behavior_, 10(1):6-38, 1995.
* McKelvey and Palfrey [1998] Richard D McKelvey and Thomas R Palfrey. Quantal response equilibria for extensive form games. _Experimental economics_, 1:9-41, 1998.
* Mertikopoulos and Zhou [2019] Panayotis Mertikopoulos and Zhengyuan Zhou. Learning in games with continuous action sets and unknown payoff functions. _Mathematical Programming_, 173(1):465-507, 2019.
* Mertikopoulos et al. [2019] Panayotis Mertikopoulos, Bruno Lecouat, Houssam Zenati, Chuan-Sheng Foo, Vijay Chandrasekhar, and Georgios Piliouras. Optimistic mirror descent in saddle-point problems: Going the extra (gradient) mile. In _ICLR_, 2019.
* Mertikopoulos et al. [2022] Panayotis Mertikopoulos, Ya-Ping Hsieh, and Volkan Cevher. Learning in games from a stochastic approximation viewpoint. _arXiv preprint arXiv:2206.03922_, 2022.
* Munos et al. [2023] Remi Munos, Michal Valko, Daniele Calandriello, Mohammad Gheshlaghi Azar, Mark Rowland, Zhaohan Daniel Guo, Yunhao Tang, Matthieu Geist, Thomas Mesnard, Andrea Michi, et al. Nash learning from human feedback. _arXiv preprint arXiv:2312.00886_, 2023.
* Nash [1951] John Nash. Non-cooperative games. _Annals of mathematics_, pages 286-295, 1951.
* Nedic and Lee [2014] Angelia Nedic and Soomin Lee. On stochastic subgradient mirror-descent algorithm with weighted averaging. _SIAM Journal on Optimization_, 24(1):84-107, 2014.
* Nemirovski et al. [2009] A. Nemirovski, A. Juditsky, G. Lan, and A. Shapiro. Robust stochastic approximation approach to stochastic programming. _SIAM Journal on Optimization_, 19(4):1574-1609, 2009.
* Ouyang and Xu [2021] Yuyuan Ouyang and Yangyang Xu. Lower complexity bounds of first-order methods for convex-concave bilinear saddle-point problems. _Mathematical Programming_, 185(1):1-35, 2021.
* Pattathil et al. [2023] Sarath Pattathil, Kaiqing Zhang, and Asuman Ozdaglar. Symmetric (optimistic) natural policy gradient for multi-agent learning with parameter convergence. In _AISTATS_, pages 5641-5685, 2023.
* Perolat et al. [2021] Julien Perolat, Remi Munos, Jean-Baptiste Lespiau, Shayegan Omidshafiei, Mark Rowland, Pedro Ortega, Neil Burch, Thomas Anthony, David Balduzzi, Bart De Vylder, et al. From Poincare recurrence to convergence in imperfect information games: Finding equilibrium via regularization. In _ICML_, pages 8525-8535, 2021.
* Rakhlin and Sridharan [2013a] Alexander Rakhlin and Karthik Sridharan. Online learning with predictable sequences. In _COLT_, pages 993-1019, 2013a.
* Rakhlin and Sridharan [2013b] Sasha Rakhlin and Karthik Sridharan. Optimization, learning, and games with predictable sequences. In _NeurIPS_, pages 3066-3074, 2013b.
* Shalev-Shwartz and Singer [2006] Shai Shalev-Shwartz and Yoram Singer. Convex repeated games and fenchel duality. _Advances in neural information processing systems_, 19, 2006.
* Shalev-Shwartz and Singer [2007]Samuel Sokota, Ryan D'Orazio, J Zico Kolter, Nicolas Loizou, Marc Lanctot, Ioannis Mitliagkas, Noam Brown, and Christian Kroer. A unified approach to reinforcement learning, quantal response equilibria, and two-player zero-sum games. In _ICLR_, 2023.
* Swamy et al. [2024] Gokul Swamy, Christoph Dann, Rahul Kidambi, Zhiwei Steven Wu, and Alekh Agarwal. A minimalist approach to reinforcement learning from human feedback. _arXiv preprint arXiv:2401.04056_, 2024.
* Tatarenko and Kamgarpour [2019] Tatiana Tatarenko and Maryam Kamgarpour. Learning Nash equilibria in monotone games. In _CDC_, pages 3104-3109. IEEE, 2019.
* Tuyls et al. [2006] Karl Tuyls, Pieter Jan Hoen, and Bram Vanschoenwinkel. An evolutionary dynamical analysis of multi-agent learning in iterated games. _Autonomous Agents and Multi-Agent Systems_, 12(1):115-153, 2006.
* Wei et al. [2021] Chen-Yu Wei, Chung-Wei Lee, Mengxiao Zhang, and Haipeng Luo. Linear last-iterate convergence in constrained saddle-point optimization. In _ICLR_, 2021.
* Yoon and Ryu [2021] TaeHo Yoon and Ernest K Ryu. Accelerated algorithms for smooth convex-concave minimax problems with \(\mathcal{O}(1/k^{2})\) rate on squared gradient norm. In _ICML_, pages 12098-12109, 2021.
* Yousefian et al. [2017] Farzad Yousefian, Angelia Nedic, and Uday V Shanbhag. On smoothing, regularization, and averaging in stochastic approximation methods for stochastic variational inequality problems. _Mathematical Programming_, 165:391-431, 2017.
* Zhou et al. [2017] Zhengyuan Zhou, Panayotis Mertikopoulos, Aris L Moustakas, Nicholas Bambos, and Peter Glynn. Mirror descent learning in continuous games. In _CDC_, pages 5776-5783. IEEE, 2017.
* Zinkevich [2003] Martin Zinkevich. Online convex programming and generalized infinitesimal gradient ascent. In _ICML_, pages 928-936, 2003.

## Appendix A Broader impact

Our study can bring about a positive impact on society by contributing to the advancement of the Game AI industry. However, as far as we can envision, there are no conceivable negative social impacts.

## Appendix B Proofs for Theorem 4.1

### Proof of Theorem 4.1

Proof of Theorem 4.1.: From the first-order optimality condition for \(\pi^{t}\), we have for any \(x\in\mathcal{X}\):

\[\left\langle V(\pi^{t-1})-\mu\left(\pi^{t-1}-\frac{k(t-1)\sigma^{k(t-1)}+\sigma ^{1}}{k(t-1)+1}\right)-\frac{1}{\eta}\left(\pi^{t}-\pi^{t-1}\right),\pi^{t}-x \right\rangle\geq 0,\]

and then \(V(\pi^{t-1})-\mu\left(\pi^{t-1}-\frac{k(t-1)\sigma^{k(t-1)}+\sigma^{1}}{k(t-1 )+1}\right)-\frac{1}{\eta}\left(\pi^{t}-\pi^{t-1}\right)\in N_{\mathcal{X}}( \pi^{t})\). Thus, the tangent residual for \(\pi^{t}\) can be bounded as:

\[r^{\tan(\pi^{t})} =\min_{a\in N_{\mathcal{X}}(\pi^{t})}\left\|-V(\pi^{t})+a\right\|\] \[\leq\left\|-V(\pi^{t})+V(\pi^{t-1})-\mu\left(\pi^{t-1}-\frac{k(t- 1)\sigma^{k(t-1)}+\sigma^{1}}{k(t-1)+1}\right)-\frac{1}{\eta}\left(\pi^{t}- \pi^{t-1}\right)\right\|.\]

Letting us define

\[\pi_{i}^{\mu,\sigma^{k}}=\operatorname*{arg\,max}_{\pi_{i}\in\mathcal{X}_{i}} \left\{v_{i}(\pi_{i},\pi_{-i}^{\mu,\sigma^{k}})-\frac{\mu}{2}\left\|\pi_{i}- \frac{k\sigma_{i}^{k}+\sigma_{i}^{1}}{k+1}\right\|^{2}\right\},\]then we get by triangle inequality:

\[r^{\tan}(\pi^{t}) \leq\left\|-V(\pi^{t})+V(\pi^{t-1})-\frac{\mu}{k(t-1)+1}(\sigma^{k(t -1)}-\sigma^{1})\right.\] \[\quad-\mu(\pi^{\mu,\sigma^{k(t-1)}}-\pi^{\mu,\sigma^{k(t-1)}}+\pi^ {t-1}-\sigma^{k(t-1)})-\frac{1}{\eta}(\pi^{t}-\pi^{t-1})\right\|\] \[\leq\left\|-V(\pi^{t})+V(\pi^{t-1})\right\|+\frac{\mu}{k(t-1)+1} \left\|\sigma^{k(t-1)}-\sigma^{1}\right\|\] \[\quad+\mu\left\|\pi^{\mu,\sigma^{k(t-1)}}-\sigma^{k(t-1)}\right\| +\mu\left\|\pi^{\mu,\sigma^{k(t-1)}}-\pi^{t-1}\right\|+\frac{1}{\eta}\left\| \pi^{t}-\pi^{t-1}\right\|\] \[\leq\frac{1+\eta L}{\eta}\left\|\pi^{t}-\pi^{t-1}\right\|+\frac{ \mu D}{k(t-1)+1}\] \[\quad+\mu\left\|\pi^{\mu,\sigma^{k(t-1)}}-\sigma^{k(t-1)}\right\| +\mu\left\|\pi^{\mu,\sigma^{k(t-1)}}-\pi^{t-1}\right\|\] \[\leq\frac{1+\eta L}{\eta}\left\|\pi^{\mu,\sigma^{k(t-1)}}-\pi^{t }\right\|+\frac{\mu D}{k(t-1)+1}+\mu\left\|\pi^{\mu,\sigma^{k(t-1)}}-\sigma^{k (t-1)}\right\|\] \[\quad+\left(\mu+\frac{1+\eta L}{\eta}\right)\left\|\pi^{\mu, \sigma^{k(t-1)}}-\pi^{t-1}\right\|.\] (10)

In terms of upper bound on \(\left\|\pi^{\mu,\sigma^{k(t-1)}}-\pi^{t}\right\|\) and \(\left\|\pi^{\mu,\sigma^{k(t-1)}}-\pi^{t-1}\right\|\), we introduce the following lemma:

**Lemma B.1**.: _If we use the constant learning rate \(\eta_{t}=\eta\in(0,\frac{\mu}{(L+\mu)^{2}})\), we have for any \(t\geq 1\):_

\[\left\|\pi^{\mu,\sigma^{k(t)}}-\pi^{t}\right\|^{2}\leq\left(\frac {1}{1+\eta\mu}\right)^{t-(k(t)-1)T_{\sigma}-1}\left\|\pi^{\mu,\sigma^{k(t)}}- \sigma^{k(t)}\right\|^{2},\] \[\left\|\pi^{\mu,\sigma^{k(t)}}-\pi^{t+1}\right\|^{2}\leq\left( \frac{1}{1+\eta\mu}\right)^{t-(k(t)-1)T_{\sigma}}\left\|\pi^{\mu,\sigma^{k(t)} }-\sigma^{k(t)}\right\|^{2}.\]

Combining (10) and Lemma B.1, we have:

\[r^{\tan}(\pi^{t})\leq 2\left(\mu+\frac{1+\eta L}{\eta}\right)\left\|\pi^{\mu, \sigma^{k(t-1)}}-\sigma^{k(t-1)}\right\|+\frac{\mu D}{k(t-1)+1}.\] (11)

Next, we derive the following upper bound on \(\left\|\pi^{\mu,\sigma^{k(t-1)}}-\sigma^{k(t-1)}\right\|\):

**Lemma B.2**.: _If we set \(\eta_{t}=\eta\in(0,\frac{\mu}{(L+\mu)^{2}})\) and \(T_{\sigma}\geq\max(1,\frac{6\ln 3(T+1)}{\ln(1+\eta\mu)})\), we have for any \(t\geq 1\):_

\[\left\|\pi^{\mu,\sigma^{k(t)}}-\sigma^{k(t)}\right\|\leq\frac{8D}{k(t)+1}.\]

By combining (11) and Lemma B.2, we get:

\[r^{\tan}(\pi^{t}) \leq\frac{16D}{k(t-1)+1}\left(\mu+\frac{1+\eta L}{\eta}\right)+ \frac{\mu D}{k(t-1)+1}\] \[\leq\frac{17D}{k(t-1)+1}\left(\mu+\frac{1+\eta L}{\eta}\right).\]

Therefore, since \(k(t)=\lfloor\frac{t-1}{T_{\sigma}}\rfloor+1\), it holds that:

\[r^{\tan}(\pi^{t})\leq\frac{17DT_{\sigma}}{t+T_{\sigma}-2}\left(\mu+\frac{1+ \eta L}{\eta}\right).\]

Finally, taking \(T_{\sigma}=c\cdot\max(1,\frac{6\ln 3(T+1)}{\ln(1+\eta\mu)})\), we have:

\[r^{\tan}(\pi^{t})\leq\frac{17cD\left(\frac{6\ln 3(T+1)}{\ln(1+\eta\mu)}+1 \right)}{t-1}\left(\mu+\frac{1+\eta L}{\eta}\right).\]

### Proof of Lemma b.1

Proof of Lemma b.1.: First, we have for any three vectors \(a,b,c\):

\[\frac{1}{2}\left\|a-b\right\|^{2}-\frac{1}{2}\left\|a-c\right\|^{2}+\frac{1}{2} \left\|b-c\right\|^{2}=\left\langle c-b,a-b\right\rangle.\]

Thus, we have for any \(t\geq 1\):

\[\frac{1}{2}\left\|\pi^{\mu,\sigma^{k(t)}}-\pi^{t+1}\right\|^{2}-\frac{1}{2} \left\|\pi^{\mu,\sigma^{k(t)}}-\pi^{t}\right\|^{2}+\frac{1}{2}\left\|\pi^{t+1} -\pi^{t}\right\|^{2}=\left\langle\pi^{t}-\pi^{t+1},\pi^{\mu,\sigma^{k(t)}}- \pi^{t+1}\right\rangle.\] (12)

Here, let us define \(\hat{\sigma}^{k(t)}=\frac{k(t)\sigma^{k(t)}+\sigma^{1}}{k(t)+1}\). Then, from the first-order optimality condition for \(\pi^{t+1}\), we have for any \(t\geq 1\):

\[\left\langle\eta\left(V(\pi^{t})-\mu\left(\pi^{t}-\hat{\sigma}^{k(t)}\right) \right)-\pi^{t+1}+\pi^{t},\pi^{t+1}-\pi^{\mu,\sigma^{k(t)}}\right\rangle\geq 0.\] (13)

Similarly, from the first-order optimality condition for \(\pi^{\mu,\sigma^{k(t)}}\), we get:

\[\left\langle V(\pi^{\mu,\sigma^{k(t)}})-\mu\left(\pi^{\mu,\sigma^{k(t)}}-\hat {\sigma}^{k(t)}\right),\pi^{\mu,\sigma^{k(t)}}-\pi^{t+1}\right\rangle\geq 0.\] (14)

Combining (12), (13), and (14) yields:

\[\frac{1}{2}\left\|\pi^{\mu,\sigma^{k(t)}}-\pi^{t+1}\right\|^{2}- \frac{1}{2}\left\|\pi^{\mu,\sigma^{k(t)}}-\pi^{t}\right\|^{2}+\frac{1}{2} \left\|\pi^{t+1}-\pi^{t}\right\|^{2}\] \[\leq\eta\left\langle V(\pi^{t})-\mu\left(\pi^{t}-\hat{\sigma}^{k( t)}\right),\pi^{t+1}-\pi^{\mu,\sigma^{k(t)}}\right\rangle\] \[=\eta\left\langle V(\pi^{t+1})-\mu\left(\pi^{t+1}-\hat{\sigma}^{ k(t)}\right),\pi^{t+1}-\pi^{\mu,\sigma^{k(t)}}\right\rangle\] \[\leq\eta\left\langle V(\pi^{\mu,\sigma^{k(t)}})-\mu\left(\pi^{t+ 1}-\hat{\sigma}^{k(t)}\right),\pi^{t+1}-\pi^{\mu,\sigma^{k(t)}}\right\rangle\] \[\quad+\eta\left\langle V(\pi^{t})-V(\pi^{t+1})-\mu\left(\pi^{t}- \pi^{t+1}\right),\pi^{t+1}-\pi^{\mu,\sigma^{k(t)}}\right\rangle\] \[=\eta\left\langle V(\pi^{\mu,\sigma^{k(t)}})-\mu\left(\pi^{\mu, \sigma^{k(t)}}-\hat{\sigma}^{k(t)}\right),\pi^{t+1}-\pi^{\mu,\sigma^{k(t)}} \right\rangle-\eta\mu\left\|\pi^{t+1}-\pi^{\mu,\sigma^{k(t)}}\right\|^{2}\] \[\leq-\eta\mu\left\|\pi^{t+1}-\pi^{\mu,\sigma^{k(t)}}\right\|^{2} +\eta\left\langle V(\pi^{t})-V(\pi^{t+1})-\mu\left(\pi^{t}-\pi^{t+1}\right), \pi^{t+1}-\pi^{\mu,\sigma^{k(t)}}\right\rangle,\] (15)

where the second inequality follows from (1). From Cauchy-Schwarz inequality and Young's inequality, the second term in the right-hand side of this inequality can be bounded by:

\[\eta\left\langle V(\pi^{t})-V(\pi^{t+1})-\mu\left(\pi^{t}-\pi^{t+ 1}\right),\pi^{t+1}-\pi^{\mu,\sigma^{k(t)}}\right\rangle\] \[=\eta\left\langle V(\pi^{t})-V(\pi^{t+1}),\pi^{t+1}-\pi^{\mu, \sigma^{k(t)}}\right\rangle-\eta\mu\left\langle\pi^{t}-\pi^{t+1},\pi^{t+1}- \pi^{\mu,\sigma^{k(t)}}\right\rangle\] \[\leq\eta\left(\left\|V(\pi^{t})-V(\pi^{t+1})\right\|+\mu\left\| \pi^{t}-\pi^{t+1}\right\|\right)\cdot\left\|\pi^{t+1}-\pi^{\mu,\sigma^{k(t)}}\right\|\] \[\leq\frac{1}{2}\left\|\pi^{t}-\pi^{t+1}\right\|^{2}+\frac{\eta^{2 }(L+\mu)^{2}}{2}\left\|\pi^{t+1}-\pi^{\mu,\sigma^{k(t)}}\right\|^{2}\] \[\leq\frac{1}{2}\left\|\pi^{t}-\pi^{t+1}\right\|^{2}+\frac{\eta \mu}{2}\left\|\pi^{t+1}-\pi^{\mu,\sigma^{k(t)}}\right\|^{2},\] (16)where the second inequality follow from (2), and the last inequality follows from the assumption that \(\eta\leq\frac{\mu}{(L+\mu)^{2}}\). By combining (15) and (16), we get:

\[\frac{1}{2}\left\|\pi^{\mu,\sigma^{k(t)}}-\pi^{t+1}\right\|^{2}- \frac{1}{2}\left\|\pi^{\mu,\sigma^{k(t)}}-\pi^{t}\right\|^{2}+\frac{1}{2}\left\| \pi^{t+1}-\pi^{t}\right\|^{2}\] \[\leq-\frac{\eta\mu}{2}\left\|\pi^{t+1}-\pi^{\mu,\sigma^{k(t)}} \right\|^{2}+\frac{1}{2}\left\|\pi^{t}-\pi^{t+1}\right\|^{2}.\]

Thus,

\[\frac{1+\eta\mu}{2}\left\|\pi^{\mu,\sigma^{k(t)}}-\pi^{t+1}\right\|^{2}\leq \frac{1}{2}\left\|\pi^{\mu,\sigma^{k(t)}}-\pi^{t}\right\|^{2}.\]

Therefore, we have for any \(t\geq 1\):

\[\left\|\pi^{\mu,\sigma^{k(t)}}-\pi^{t+1}\right\|^{2}\leq\frac{1}{1+\eta\mu} \left\|\pi^{\mu,\sigma^{k(t)}}-\pi^{t}\right\|^{2}.\]

Furthermore, since \(k(s)=k(t)\) for \(s\in[(k(t)-1)T_{\sigma}+1,t]\), we have for such \(s\) that:

\[\left\|\pi^{\mu,\sigma^{k(t)}}-\pi^{s+1}\right\|^{2}\leq\frac{1}{1+\eta\mu} \left\|\pi^{\mu,\sigma^{k(t)}}-\pi^{s}\right\|^{2}.\]

Therefore, by applying this inequality from \(t,t-1,\cdots,(k(t)-1)T_{\sigma}+1\), we get for any \(t\geq 1\):

\[\left\|\pi^{\mu,\sigma^{k(t)}}-\pi^{t+1}\right\|^{2} \leq\left(\frac{1}{1+\eta\mu}\right)^{t-(k(t)-1)T_{\sigma}}\left\| \pi^{\mu,\sigma^{k(t)}}-\pi^{(k(t)-1)T_{\sigma}+1}\right\|^{2}\] \[=\left(\frac{1}{1+\eta\mu}\right)^{t-(k(t)-1)T_{\sigma}}\left\| \pi^{\mu,\sigma^{k(t)}}-\sigma^{k(t)}\right\|^{2}.\] (17)

Here, since \(k(t)=k(t+1)\) when \(t\) satisfies that \(t\neq T_{\sigma}\left\lfloor\frac{t}{T_{\sigma}}\right\rfloor\), we have for such \(t\) that:

\[\left\|\pi^{\mu,\sigma^{k(t+1)}}-\pi^{t+1}\right\|^{2}\leq\left(\frac{1}{1+ \eta\mu}\right)^{t-(k(t+1)-1)T_{\sigma}}\left\|\pi^{\mu,\sigma^{k(t+1)}}- \sigma^{k(t+1)}\right\|^{2}.\] (18)

On the other hand, when \(t\) satisfies that \(t=T_{\sigma}\left\lfloor\frac{t}{T_{\sigma}}\right\rfloor\):

\[k(t+1)=\left\lfloor\frac{T_{\sigma}\left\lfloor\frac{t}{T_{ \sigma}}\right\rfloor+1-1}{T_{\sigma}}\right\rfloor+1=\left\lfloor\frac{t}{T _{\sigma}}\right\rfloor+1\] \[\Rightarrow(k(t+1)-1)T_{\sigma}=T_{\sigma}\left\lfloor\frac{t}{T _{\sigma}}\right\rfloor=t\] \[\Rightarrow\pi^{t+1}=\pi^{(k(t+1)-1)T_{\sigma}+1}=\sigma^{k(t+1)}.\]

Therefore, we have for any \(t\geq 1\) such that \(t=T_{\sigma}\left\lfloor\frac{t}{T_{\sigma}}\right\rfloor\):

\[\left\|\pi^{\mu,\sigma^{k(t+1)}}-\pi^{t+1}\right\|^{2} =\left\|\pi^{\mu,\sigma^{k(t+1)}}-\sigma^{k(t+1)}\right\|^{2}\] \[=\left(\frac{1}{1+\eta\mu}\right)^{t-(k(t+1)-1)T_{\sigma}}\left\| \pi^{\mu,\sigma^{k(t+1)}}-\sigma^{k(t+1)}\right\|^{2}.\] (19)

By combining (17), (18), and (19), we have for any \(t\geq 1\):

\[\left\|\pi^{\mu,\sigma^{k(t)}}-\pi^{t+1}\right\|^{2} \leq\left(\frac{1}{1+\eta\mu}\right)^{t-(k(t)-1)T_{\sigma}}\left\| \pi^{\mu,\sigma^{k(t)}}-\sigma^{k(t)}\right\|^{2},\] \[\left\|\pi^{\mu,\sigma^{k(t+1)}}-\pi^{t+1}\right\|^{2} \leq\left(\frac{1}{1+\eta\mu}\right)^{t-(k(t+1)-1)T_{\sigma}}\left\| \pi^{\mu,\sigma^{k(t+1)}}-\sigma^{k(t+1)}\right\|^{2}.\]

### Proof of Lemma b.2

Proof of Lemma b.2.: First, we have for any Nash equilibrium \(\pi^{*}\in\Pi^{*}\) and \(t\geq 1\) such that \(k(t)\geq 1\):

\[\frac{(k(t)+1)(k(t)+2)}{2}\left\|\pi^{\mu,\sigma^{k(t)}}-\hat{ \sigma}^{k(t)}\right\|^{2}+(k(t)+1)(k(t)+2)\left\langle\hat{\sigma}^{k(t)+1}- \pi^{\mu,\sigma^{k(t)}},\pi^{\mu,\sigma^{k(t)}}-\hat{\sigma}^{k(t)}\right\rangle\] \[=\frac{(k(t)+1)(k(t)+2)}{2}\left\|\pi^{\mu,\sigma^{k(t)}}-\hat{ \sigma}^{k(t)}\right\|^{2}\] \[\quad+(k(t)+1)\left\langle(k(t)+1)\sigma^{k(t)+1}+\sigma^{1}-(k (t)+2)\pi^{\mu,\sigma^{k(t)}},\pi^{\mu,\sigma^{k(t)}}-\hat{\sigma}^{k(t)}\right\rangle\] \[=\frac{(k(t)+1)(k(t)+2)}{2}\left\|\pi^{\mu,\sigma^{k(t)}}-\hat{ \sigma}^{k(t)}\right\|^{2}+(k(t)+1)\left\langle\sigma^{1}-\sigma^{k(t)+1}, \pi^{\mu,\sigma^{k(t)}}-\hat{\sigma}^{k(t)}\right\rangle\] \[\quad+(k(t)+1)(k(t)+2)\left\langle\sigma^{k(t)+1}-\pi^{\mu, \sigma^{k(t)}},\pi^{\mu,\sigma^{k(t)}}-\hat{\sigma}^{k(t)}\right\rangle\] \[=\frac{(k(t)+1)(k(t)+2)}{2}\left\|\pi^{\mu,\sigma^{k(t)}}-\hat{ \sigma}^{k(t)}\right\|^{2}+(k(t)+1)\left\langle\sigma^{1}-\pi^{\mu,\sigma^{k( t)}},\pi^{\mu,\sigma^{k(t)}}-\hat{\sigma}^{k(t)}\right\rangle\] \[=\frac{(k(t)+1)(k(t)+2)}{2}\left\|\pi^{\mu,\sigma^{k(t)}}-\hat{ \sigma}^{k(t)}\right\|^{2}+(k(t)+1)\left\langle\sigma^{1}-\pi^{*},\pi^{\mu, \sigma^{k(t)}}-\hat{\sigma}^{k(t)}\right\rangle\] \[\quad+(k(t)+1)\left\langle\pi^{*}-\pi^{\mu,\sigma^{k(t)}},\pi^{ \mu,\sigma^{k(t)}}-\hat{\sigma}^{k(t)}\right\rangle+(k(t)+1)^{2}\left\langle \sigma^{k(t)+1}-\pi^{\mu,\sigma^{k(t)}},\pi^{\mu,\sigma^{k(t)}}-\hat{\sigma}^{ k(t)}\right\rangle.\]

Here, the first-order optimality condition for \(\pi^{\mu,\sigma^{k(t)}}\):

\[\left\langle V(\pi^{\mu,\sigma^{k(t)}})-\mu\left(\pi^{\mu,\sigma ^{k(t)}}-\hat{\sigma}^{k(t)}\right),\pi^{\mu,\sigma^{k(t)}}-\pi^{*}\right\rangle\geq 0\] \[\Rightarrow\left\langle\pi^{\mu,\sigma^{k(t)}}-\hat{\sigma}^{k( t)},\pi^{*}-\pi^{\mu,\sigma^{k(t)}}\right\rangle\geq\frac{1}{\mu}\left\langle V (\pi^{\mu,\sigma^{k(t)}}),\pi^{*}-\pi^{\mu,\sigma^{k(t)}}\right\rangle\geq \frac{1}{\mu}\left\langle V(\pi^{*}),\pi^{*}-\pi^{\mu,\sigma^{k(t)}}\right\rangle \geq 0,\]

where we use (1) and the fact that \(\pi^{*}\) is a Nash equilibrium. Combining these inequalities yields:

\[\frac{(k(t)+1)(k(t)+2)}{2}\left\|\pi^{\mu,\sigma^{k(t)}}-\hat{ \sigma}^{k(t)}\right\|^{2}+(k(t)+1)(k(t)+2)\left\langle\hat{\sigma}^{k(t)+1}- \pi^{\mu,\sigma^{k(t)}},\pi^{\mu,\sigma^{k(t)}}-\hat{\sigma}^{k(t)}\right\rangle\] \[\geq\frac{(k(t)+1)(k(t)+2)}{2}\left\|\pi^{\mu,\sigma^{k(t)}}- \hat{\sigma}^{k(t)}\right\|^{2}+(k(t)+1)\left\langle\sigma^{1}-\pi^{*},\pi^{ \mu,\sigma^{k(t)}}-\hat{\sigma}^{k(t)}\right\rangle\] \[\quad+(k(t)+1)^{2}\left\langle\sigma^{k(t)+1}-\pi^{\mu,\sigma^{k( t)}},\pi^{\mu,\sigma^{k(t)}}-\hat{\sigma}^{k(t)}\right\rangle.\]

From Young's inequality, we have for any \(\rho_{1},\rho_{2}>0\):

\[\frac{(k(t)+1)(k(t)+2)}{2}\left\|\pi^{\mu,\sigma^{k(t)}}-\hat{ \sigma}^{k(t)}\right\|^{2}+(k(t)+1)(k(t)+2)\left\langle\hat{\sigma}^{k(t)+1}- \pi^{\mu,\sigma^{k(t)}},\pi^{\mu,\sigma^{k(t)}}-\hat{\sigma}^{k(t)}\right\rangle\] \[\geq\frac{(k(t)+1)(k(t)+2)}{2}\left\|\pi^{\mu,\sigma^{k(t)}}- \hat{\sigma}^{k(t)}\right\|^{2}-\frac{\rho_{1}(k(t)+1)}{2}\left\|\sigma^{1}- \pi^{*}\right\|^{2}-\frac{(k(t)+1)}{2\rho_{1}}\left\|\pi^{\mu,\sigma^{k(t)}}- \hat{\sigma}^{k(t)}\right\|^{2}\] \[\quad-\frac{\rho_{2}(k(t)+1)^{2}}{2}\left\|\sigma^{k(t)+1}-\pi^{ \mu,\sigma^{k(t)}}\right\|^{2}-\frac{(k(t)+1)^{2}}{2\rho_{2}}\left\|\pi^{\mu, \sigma^{k(t)}}-\hat{\sigma}^{k(t)}\right\|^{2}\] \[=\left(\frac{(k(t)+1)(k(t)+2)}{2}-\frac{k(t)+1}{2\rho_{1}}-\frac{ (k(t)+1)^{2}}{2\rho_{2}}\right)\left\|\pi^{\mu,\sigma^{k(t)}}-\hat{\sigma}^{k(t) }\right\|^{2}\] \[\quad-\frac{\rho_{1}(k(t)+1)}{2}\left\|\sigma^{1}-\pi^{*}\right\| ^{2}-\frac{\rho_{2}(k(t)+1)^{2}}{2}\left\|\sigma^{k(t)+1}-\pi^{\mu,\sigma^{k(t)} }\right\|^{2}.\]

By setting \(\rho_{1}=\frac{4}{k(t)+2},\rho_{2}=\frac{4(k(t)+1)}{k(t)+2}\), we obtain:

\[\frac{(k(t)+1)(k(t)+2)}{2}\left\|\pi^{\mu,\sigma^{k(t)}}-\hat{ \sigma}^{k(t)}\right\|^{2}+(k(t)+1)(k(t)+2)\left\langle\hat{\sigma}^{k(t)+1}- \pi^{\mu,\sigma^{k(t)}},\pi^{\mu,\sigma^{k(t)}}-\hat{\sigma}^{k(t)}\right\rangle\] \[\geq\frac{(k(t)+1)(k(t)+2)}{4}\left\|\pi^{\mu,\sigma^{k(t)}}- \hat{\sigma}^{k(t)}\right\|^{2}-\frac{2(k(t)+1)}{k(t)+2}\left\|\sigma^{1}-\pi^{ *}\right\|^{2}\]\[\quad-\frac{2(k(t)+1)^{3}}{k(t)+2}\left\|\sigma^{k(t)+1}-\pi^{\mu, \sigma^{k(t)}}\right\|^{2}\] \[\quad-\frac{2(k(t)+1)^{3}}{k(t)+2}\left\|\sigma^{k(t)+1}-\pi^{\mu, \sigma^{k(t)}}\right\|^{2}\] \[\quad\geq\frac{(k(t)+1)(k(t)+2)}{4}\left\|\pi^{\mu,\sigma^{k(t)}} -\hat{\sigma}^{k(t)}\right\|^{2}-2\left\|\sigma^{1}-\pi^{*}\right\|^{2}-2(k(t )+1)^{2}\left\|\sigma^{k(t)+1}-\pi^{\mu,\sigma^{k(t)}}\right\|^{2}.\] (20)

Here, we introduce the following lemma:

**Lemma B.3**.: _For any \(t\geq 1\) such that \(k(t)\geq 2\), we have:_

\[\frac{(k(t)+1)(k(t)+2)}{2}\left\|\pi^{\mu,\sigma^{k(t)}}-\hat{ \sigma}^{k(t)}\right\|^{2}+(k(t)+1)(k(t)+2)\left\langle\hat{\sigma}^{k(t)+1}- \pi^{\mu,\sigma^{k(t)}},\pi^{\mu,\sigma^{k(t)}}-\hat{\sigma}^{k(t)}\right\rangle\] \[\leq\frac{k(t)(k(t)+1)}{2}\left\|\pi^{\mu,\sigma^{k(t)-1}}-\hat{ \sigma}^{k(t)-1}\right\|^{2}+k(t)(k(t)+1)\left\langle\hat{\sigma}^{k(t)}-\pi^ {\mu,\sigma^{k(t)-1}},\pi^{\mu,\sigma^{k(t)-1}}-\hat{\sigma}^{k(t)-1}\right\rangle\] \[\quad+(k(t)+1)\left\langle(k(t)+1)(\pi^{\mu,\sigma^{k(t)}}- \sigma^{k(t)+1})+k(t)(\sigma^{k(t)}-\pi^{\mu,\sigma^{k(t)-1}}),\hat{\sigma}^{ k(t)}-\pi^{\mu,\sigma^{k(t)}}\right\rangle.\]

By combining (20) and Lemma B.3, we get:

\[\frac{(k(t)+1)(k(t)+2)}{4}\left\|\pi^{\mu,\sigma^{k(t)}}-\hat{ \sigma}^{k(t)}\right\|^{2}\] \[\leq\frac{(k(t)+1)(k(t)+2)}{2}\left\|\pi^{\mu,\sigma^{k(t)}}- \hat{\sigma}^{k(t)}\right\|^{2}+(k(t)+1)(k(t)+2)\left\langle\hat{\sigma}^{k(t )+1}-\pi^{\mu,\sigma^{k(t)}},\pi^{\mu,\sigma^{k(t)}}-\hat{\sigma}^{k(t)}\right\rangle\] \[\quad+2\left\|\sigma^{1}-\pi^{*}\right\|^{2}+2(k(t)+1)^{2}\left\| \sigma^{k(t)+1}-\pi^{\mu,\sigma^{k(t)}}\right\|^{2}\] \[\leq 3\left\|\pi^{\mu,\sigma^{1}}-\hat{\sigma}^{1}\right\|^{2}+6 \left\langle\hat{\sigma}^{2}-\pi^{\mu,\sigma^{1}},\pi^{\mu,\sigma^{1}}-\hat{ \sigma}^{1}\right\rangle+2\left\|\sigma^{1}-\pi^{*}\right\|^{2}+2(k(t)+1)^{2} \left\|\sigma^{k(t)+1}-\pi^{\mu,\sigma^{k(t)}}\right\|^{2}\] \[\quad+\sum_{l=2}^{k(t)}(l+1)\left\langle(l+1)(\pi^{\mu,\sigma^{l }}-\sigma^{l+1})+l(\sigma^{l}-\pi^{\mu,\sigma^{l-1}}),\hat{\sigma}^{l}-\pi^{ \mu,\sigma^{l}}\right\rangle\] \[=3\left\|\pi^{\mu,\sigma^{1}}-\sigma^{1}\right\|^{2}+2\left\langle 2 \sigma^{1}-\pi^{\mu,\sigma^{1}},\pi^{\mu,\sigma^{1}}-\sigma^{1}\right\rangle+4 \left\langle\sigma^{2}-\pi^{\mu,\sigma^{1}},\pi^{\mu,\sigma^{1}}-\sigma^{1}\right\rangle\] \[\quad+2\left\|\sigma^{1}-\pi^{*}\right\|^{2}+2(k(t)+1)^{2}\left\| \sigma^{k(t)+1}-\pi^{\mu,\sigma^{k(t)}}\right\|^{2}\] \[\quad+\sum_{l=2}^{k(t)}(l+1)\left\langle(l+1)(\pi^{\mu,\sigma^{l }}-\sigma^{l+1})+l(\sigma^{l}-\pi^{\mu,\sigma^{l-1}}),\hat{\sigma}^{l}-\pi^{ \mu,\sigma^{l}}\right\rangle\] \[=\left\|\pi^{\mu,\sigma^{1}}-\sigma^{1}\right\|^{2}+4\left\langle \sigma^{2}-\pi^{\mu,\sigma^{1}},\pi^{\mu,\sigma^{1}}-\sigma^{1}\right\rangle+2 \left\|\sigma^{1}-\pi^{*}\right\|^{2}+2(k(t)+1)^{2}\left\|\sigma^{k(t)+1}-\pi^ {\mu,\sigma^{k(t)}}\right\|^{2}\] \[\quad+\sum_{l=2}^{k(t)}(l+1)\left\langle(l+1)(\pi^{\mu,\sigma^{l }}-\sigma^{l+1})+l(\sigma^{l}-\pi^{\mu,\sigma^{l-1}}),\hat{\sigma}^{l}-\pi^{ \mu,\sigma^{l}}\right\rangle\] \[=\left\|\pi^{\mu,\sigma^{1}}-\sigma^{1}\right\|^{2}+2\left\|\sigma ^{1}-\pi^{*}\right\|^{2}+2(k(t)+1)^{2}\left\|\sigma^{k(t)+1}-\pi^{\mu,\sigma^ {k(t)}}\right\|^{2}\] \[\quad+\sum_{l=1}^{k(t)}(l+1)^{2}\left\langle\pi^{\mu,\sigma^{l}}- \sigma^{l+1},\hat{\sigma}^{l}-\pi^{\mu,\sigma^{l}}\right\rangle+\sum_{l=2}^{k(t )}l(l+1)\left\langle\sigma^{l}-\pi^{\mu,\sigma^{l-1}},\hat{\sigma}^{l}-\pi^{ \mu,\sigma^{l}}\right\rangle\] \[\leq 3D^{2}+2(k(t)+1)^{2}\left\|\sigma^{k(t)+1}-\pi^{\mu,\sigma^{k( t)}}\right\|^{2}+2D(k(t)+1)^{2}\sum_{l=1}^{k(t)}\left\|\pi^{\mu,\sigma^{l}}- \sigma^{l+1}\right\|.\]Therefore, we have for any \(t\geq 1\) such that \(k(t)\geq 2\):

\[\left\|\pi^{\mu,\sigma^{k(t)}}-\hat{\sigma}^{k(t)}\right\|^{2}\leq \frac{12D^{2}}{(k(t)+1)^{2}}+8\left\|\sigma^{k(t)+1}-\pi^{h,\sigma^{k(t)}} \right\|^{2}+8D\sum_{l=1}^{k(t)}\left\|\pi^{\mu,\sigma^{l}}-\sigma^{l+1}\right\|.\]

By the definition of \(\hat{\sigma}^{k(t)}\),

\[\left\|\pi^{\mu,\sigma^{k(t)}}-\sigma^{k(t)}\right\|^{2}+\frac{ \left\|\sigma^{k(t)}-\sigma^{1}\right\|^{2}}{(k(t)+1)^{2}}+\frac{2}{k(t)+1} \left\langle\pi^{\mu,\sigma^{k(t)}}-\sigma^{k(t)},\sigma^{k(t)}-\sigma^{1}\right\rangle\] \[\leq\frac{12D^{2}}{(k(t)+1)^{2}}+8\left\|\sigma^{k(t)+1}-\pi^{ \mu,\sigma^{k(t)}}\right\|^{2}+8D\sum_{l=1}^{k(t)}\left\|\pi^{\mu,\sigma^{l}} -\sigma^{l+1}\right\|.\]

Therefore, from Cauchy-Schwarz inequality, we have:

\[\left\|\pi^{\mu,\sigma^{k(t)}}-\sigma^{k(t)}\right\|^{2}\] \[\leq\frac{2}{k(t)+1}\left\langle\pi^{\mu,\sigma^{k(t)}}-\sigma^{ k(t)},\sigma^{1}-\sigma^{k(t)}\right\rangle+\frac{12D^{2}}{(k(t)+1)^{2}}\] \[\quad+8\left\|\sigma^{k(t)+1}-\pi^{\mu,\sigma^{k(t)}}\right\|^{2 }+8D\sum_{l=1}^{k(t)}\left\|\pi^{\mu,\sigma^{l}}-\sigma^{l+1}\right\|\] \[\leq\frac{2D}{k(t)+1}\left\|\pi^{\mu,\sigma^{k(t)}}-\sigma^{k(t)} \right\|+\frac{12D^{2}}{(k(t)+1)^{2}}+8\left\|\sigma^{k(t)+1}-\pi^{\mu,\sigma^ {k(t)}}\right\|^{2}+8D\sum_{l=1}^{k(t)}\left\|\pi^{\mu,\sigma^{l}}-\sigma^{l+ 1}\right\|.\] (21)

Furthermore, from Lemma B.1, we have for any \(k\geq 1\):

\[\left\|\pi^{\mu,\sigma^{k}}-\sigma^{k+1}\right\|^{2}\leq\left( \frac{1}{1+\eta\mu}\right)^{T_{\sigma}}\left\|\pi^{\mu,\sigma^{k}}-\sigma^{k} \right\|^{2}.\] (22)

Combining (21) nad (22), we have for any \(t\geq 1\) such that \(k(t)\geq 2\):

\[\left\|\pi^{\mu,\sigma^{k(t)}}-\sigma^{k(t)}\right\|^{2} \leq\frac{2D}{k(t)+1}\left\|\pi^{\mu,\sigma^{k(t)}}-\sigma^{k(t) }\right\|+\frac{12D^{2}}{(k(t)+1)^{2}}\] \[\quad+8\left(\frac{1}{1+\eta\mu}\right)^{T_{\sigma}}\left\|\pi^{ \mu,\sigma^{k(t)}}-\sigma^{k(t)}\right\|^{2}+8D^{2}k(t)\left(\frac{1}{1+\eta \mu}\right)^{\frac{T_{\sigma}}{2}}.\]

Therefore, since \(T_{\sigma}\geq\max(1,\frac{6\ln 3(T+1)}{\ln(1+\eta\mu)})\Rightarrow\left( \frac{1}{1+\eta\mu}\right)^{T_{\sigma}}\leq\frac{(k(t)+1)^{3}}{(1+\eta\mu)^{ T_{\sigma}}}\leq\frac{1}{16}\), we have for \(k(t)\geq 2\):

\[\frac{1}{2}\left(\left\|\pi^{\mu,\sigma^{k(t)}}-\sigma^{k(t)} \right\|-\frac{2D}{k(t)+1}\right)^{2}\leq\frac{2D^{2}}{(k(t)+1)^{2}}+\frac{12 D^{2}}{(k(t)+1)^{2}}+\frac{D^{2}}{2(k(t)+1)^{2}}\leq\frac{16D^{2}}{(k(t)+1)^{2}},\]

and then:

\[\left\|\pi^{\mu,\sigma^{k(t)}}-\sigma^{k(t)}\right\|\leq\frac{2D }{k(t)+1}+\frac{4\sqrt{2}D}{k(t)+1}\leq\frac{8D}{k(t)+1}.\]

On the other hand, for \(k(t)=1\), we have:

\[\left\|\pi^{\mu,\sigma^{1}}-\sigma^{1}\right\|\leq D\leq\frac{8D }{1+1}.\]

In summary, for any \(t\geq 1\), we have:

\[\left\|\pi^{\mu,\sigma^{k(t)}}-\sigma^{k(t)}\right\|\leq\frac{8D }{k(t)+1}.\]

### Proof of Lemma b.3

Proof of Lemma b.3.: From the first-order optimality condition for \(\pi^{\mu,\sigma^{k(t)}}\), we have:

\[\left\langle V(\pi^{\mu,\sigma^{k(t)}})-\mu(\pi^{\mu,\sigma^{k(t)}}-\hat{\sigma} ^{k(t)}),\pi^{\mu,\sigma^{k(t)}}-\pi^{\mu,\sigma^{k(t)-1}}\right\rangle\geq 0.\]

Similarly, from the first-order optimality condition for \(\pi^{\mu,\sigma^{k(t)-1}}\), we have:

\[\left\langle V(\pi^{\mu,\sigma^{k(t)-1}})-\mu(\pi^{\mu,\sigma^{k(t)-1}}-\hat{ \sigma}^{k(t)-1}),\pi^{\mu,\sigma^{k(t)-1}}-\pi^{\mu,\sigma^{k(t)}}\right\rangle\geq 0.\]

Summing up these inequalities, we get for any \(t\geq 1\) such that \(k(t)\geq 2\):

\[0 \leq\left\langle V(\pi^{\mu,\sigma^{k(t)}})-V(\pi^{\mu,\sigma^{k (t)-1}}),\pi^{\mu,\sigma^{k(t)}}-\pi^{\mu,\sigma^{k(t)-1}}\right\rangle-\mu \left\langle\pi^{\mu,\sigma^{k(t)}}-\hat{\sigma}^{k(t)},\pi^{\mu,\sigma^{k(t )}}-\pi^{\mu,\sigma^{k(t)-1}}\right\rangle\] \[\leq-\mu\left\langle\pi^{\mu,\sigma^{k(t)}}-\hat{\sigma}^{k(t)}, \pi^{\mu,\sigma^{k(t)}}-\pi^{\mu,\sigma^{k(t)-1}}\right\rangle+\mu\left\langle \hat{\sigma}^{k(t)-1}-\pi^{\mu,\sigma^{k(t)-1}},\pi^{\mu,\sigma^{k(t)-1}}-\pi ^{\mu,\sigma^{k(t)}}\right\rangle\] \[=-\mu\left\langle\pi^{\mu,\sigma^{k(t)}}-\sigma^{k(t)}+\sigma^{k (t)}-\hat{\sigma}^{k(t)},\pi^{\mu,\sigma^{k(t)}}-\sigma^{k(t)}+\sigma^{k(t)}- \pi^{\mu,\sigma^{k(t)-1}}\right\rangle\] \[\quad+\mu\left\langle\hat{\sigma}^{k(t)-1}-\pi^{\mu,\sigma^{k(t)- 1}},\pi^{\mu,\sigma^{k(t)-1}}-\pi^{\mu,\sigma^{k(t)}}\right\rangle\] \[=-\mu\left\|\pi^{\mu,\sigma^{k(t)}}-\sigma^{k(t)}\right\|^{2}- \mu\left\langle\pi^{\mu,\sigma^{k(t)}}-\sigma^{k(t)},\sigma^{k(t)}-\pi^{\mu, \sigma^{k(t)-1}}\right\rangle\] \[\quad-\mu\left\langle\sigma^{k(t)}-\hat{\sigma}^{k(t)},\pi^{\mu, \sigma^{k(t)}}-\pi^{\mu,\sigma^{k(t)-1}}\right\rangle+\mu\left\langle\hat{ \sigma}^{k(t)-1}-\pi^{\mu,\sigma^{k(t)-1}},\pi^{\mu,\sigma^{k(t)-1}}-\pi^{\mu,\sigma^{k(t)}}\right\rangle\] \[=-\mu\left\|\pi^{\mu,\sigma^{k(t)}}-\sigma^{k(t)}\right\|^{2}- \mu\left\langle\pi^{\mu,\sigma^{k(t)}}-\sigma^{k(t)},\sigma^{k(t)}-\pi^{\mu, \sigma^{k(t)-1}}\right\rangle\] \[\quad+\mu\left\langle\pi^{\mu,\sigma^{k(t)}}-\pi^{\mu,\sigma^{k (t)-1}},\pi^{\mu,\sigma^{k(t)-1}}-\sigma^{k(t)}\right\rangle+\mu\left\langle \hat{\sigma}^{k(t)-1}-\hat{\sigma}^{k(t)},\pi^{\mu,\sigma^{k(t)-1}}-\pi^{\mu,\sigma^{k(t)}}\right\rangle.\]

Here, for any vectors \(a,b,c\), it holds that:

\[\left\langle a-b,b-c\right\rangle =\frac{1}{2}\|a-c\|^{2}-\frac{1}{2}\|b-c\|^{2}-\frac{1}{2}\|a-b \|^{2},\] \[\left\langle a-b,c-d\right\rangle =\frac{1}{2}\|a-b\|^{2}+\frac{1}{2}\|c-d\|^{2}-\frac{1}{2}\|d-c+ a-b\|^{2}.\]

Thus, we have:

\[0 \leq-\mu\left\|\pi^{\mu,\sigma^{k(t)}}-\sigma^{k(t)}\right\|^{2}- \frac{\mu}{2}\left\|\pi^{\mu,\sigma^{k(t)}}-\pi^{\mu,\sigma^{k(t)-1}}\right\|^ {2}\] \[\quad+\frac{\mu}{2}\left\|\pi^{\mu,\sigma^{k(t)-1}}-\sigma^{k(t)} \right\|^{2}+\frac{\mu}{2}\left\|\pi^{\mu,\sigma^{k(t)}}-\sigma^{k(t)}\right\|^ {2}\] \[\quad+\frac{\mu}{2}\left\|\hat{\sigma}^{k(t)-1}-\hat{\sigma}^{k(t) }\right\|^{2}+\frac{\mu}{2}\left\|\pi^{\mu,\sigma^{k(t)}}-\pi^{\mu,\sigma^{k(t )-1}}\right\|^{2}\] \[\quad-\frac{\mu}{2}\left\|\pi^{\mu,\sigma^{k(t)}}-\pi^{\mu,\sigma^ {k(t)-1}}+\hat{\sigma}^{k(t)-1}+\hat{\sigma}^{k(t)}\right\|^{2}\] \[=-\frac{\mu}{2}\left\|\pi^{\mu,\sigma^{k(t)}}-\pi^{\mu,\sigma^{k(t )-1}}\right\|^{2}+\frac{\mu}{2}\left\|\hat{\sigma}^{k(t)}-\hat{\sigma}^{k(t)-1 }\right\|^{2}\] \[\quad-\frac{\mu}{2}\left\|\pi^{\mu,\sigma^{k(t)}}-\pi^{\mu,\sigma^ {k(t)-1}}+\hat{\sigma}^{k(t)-1}+\hat{\sigma}^{k(t)}\right\|^{2}\] \[\leq-\frac{\mu}{2}\left\|\pi^{\mu,\sigma^{k(t)}}-\pi^{\mu,\sigma^ {k(t)-1}}\right\|^{2}+\frac{\mu}{2}\left\|\hat{\sigma}^{k(t)}-\hat{\sigma}^{k(t )-1}\right\|^{2}\] \[=-\frac{\mu}{2}\left\|\pi^{\mu,\sigma^{k(t)}}-\pi^{\mu,\sigma^{k(t )-1}}\right\|^{2}+\frac{\mu}{2}\left\|\hat{\sigma}^{k(t)}-\pi^{\mu,\sigma^{k(t )-1}}+\pi^{\mu,\sigma^{k(t)-1}}-\hat{\sigma}^{k(t)-1}\right\|^{2}\]\[=\frac{\mu}{2}\left\|\pi^{\mu,\sigma^{k(t)-1}}-\hat{\sigma}^{k(t)-1} \right\|^{2}+\frac{\mu}{2}\left\|\hat{\sigma}^{k(t)}-\pi^{\mu,\sigma^{k(t)-1}} \right\|^{2}-\frac{\mu}{2}\left\|\pi^{\mu,\sigma^{k(t)}}-\pi^{\mu,\sigma^{k(t) -1}}\right\|^{2}\] \[\quad+\mu\left\langle\hat{\sigma}^{k(t)}-\pi^{\mu,\sigma^{k(t)-1 }},\pi^{\mu,\sigma^{k(t)-1}}-\hat{\sigma}^{k(t)-1}\right\rangle.\] (23)

Here, from the definition of \(\hat{\sigma}^{k(t)}\), we have:

\[\frac{1}{2}\left\|\hat{\sigma}^{k(t)}-\pi^{\mu,\sigma^{k(t)-1}} \right\|^{2}-\frac{1}{2}\left\|\pi^{\mu,\sigma^{k(t)}}-\pi^{\mu,\sigma^{k(t)-1 }}\right\|^{2}\] \[=\frac{1}{2}\left\|\frac{k(t)\sigma^{k(t)}+\sigma^{1}}{k(t)+1}- \pi^{\mu,\sigma^{k(t)-1}}\right\|^{2}-\frac{1}{2}\left\|\pi^{\mu,\sigma^{k(t) }}-\pi^{\mu,\sigma^{k(t)-1}}\right\|^{2}\] \[=\frac{1}{2}\left\langle\frac{k(t)\sigma^{k(t)}+\sigma^{1}}{k(t) +1}-\pi^{\mu,\sigma^{k(t)-1}}+\pi^{\mu,\sigma^{k(t)}}-\pi^{\mu,\sigma^{k(t)-1 }},\frac{k(t)\sigma^{k(t)}+\sigma^{1}}{k(t)+1}-\pi^{\mu,\sigma^{k(t)-1}}-\pi^ {\mu,\sigma^{k(t)}}+\pi^{\mu,\sigma^{k(t)-1}}\right\rangle\] \[=\frac{1}{2}\left\langle\frac{\sigma^{1}+(k(t)+1)\pi^{\mu,\sigma ^{k(t)}}-2(k(t)+1)\pi^{\mu,\sigma^{k(t)-1}}+k(t)\sigma^{k(t)}}{k(t)+1},\hat{ \sigma}^{k(t)}-\pi^{\mu,\sigma^{k(t)}}\right\rangle\] \[=\frac{1}{2k(t)}\left\langle 2(k(t)+1)\sigma^{k(t)+1}+2\sigma^{1} -2(k(t)+2)\pi^{\mu,\sigma^{k(t)}},\hat{\sigma}^{k(t)}-\pi^{\mu,\sigma^{k(t)}}\right\rangle\] \[\quad+\frac{1}{2k(t)}\left\langle-\frac{k(t)+2}{k(t)+1}\sigma^{1 }+(3k(t)+4)\pi^{\mu,\sigma^{k(t)}}-2(k(t)+1)\sigma^{k(t)+1}\hat{\sigma}^{k(t) }-\pi^{\mu,\sigma^{k(t)}}\right\rangle\] \[\quad+\frac{1}{2k(t)}\left\langle-2k(t)\pi^{\mu,\sigma^{k(t)-1}} +\frac{k(t)^{2}}{k(t)}\sigma^{k(t)},\hat{\sigma}^{k(t)}-\pi^{\mu,\sigma^{k(t) }}\right\rangle\] \[=\frac{k(t)+2}{k(t)}\left\langle\hat{\sigma}^{k(t)+1}-\pi^{\mu, \sigma^{k(t)}},\hat{\sigma}^{k(t)}-\pi^{\mu,\sigma^{k(t)}}\right\rangle\] \[\quad+\frac{1}{2k(t)}\left\langle-\frac{k(t)+2}{k(t)+1}\sigma^{1 }-\frac{k(t)(k(t)+2)}{k(t)+1}\sigma^{k(t)}+(k(t)+2)\pi^{\mu,\sigma^{k(t)}}, \hat{\sigma}^{k(t)}-\pi^{\mu,\sigma^{k(t)}}\right\rangle\] \[\quad+\frac{1}{2k(t)}\left\langle 2(k(t)+1)(\pi^{\mu,\sigma^{k(t)}}- \sigma^{k(t)+1})+2k(t)(\sigma^{k(t)}-\pi^{\mu,\sigma^{k(t)-1}}),\hat{\sigma}^{ k(t)}-\pi^{\mu,\sigma^{k(t)}}\right\rangle\] \[=-\frac{k(t)+2}{k(t)}\left\langle\hat{\sigma}^{k(t)+1}-\pi^{\mu, \sigma^{k(t)}},\pi^{\mu,\sigma^{k(t)}}-\hat{\sigma}^{k(t)}\right\rangle\] \[\quad-\frac{k(t)+2}{2k(t)}\left\langle\frac{k(t)\sigma^{k(t)}+ \sigma^{1}}{k(t)+1}-\pi^{\mu,\sigma^{k(t)}},\hat{\sigma}^{k(t)}-\pi^{\mu, \sigma^{k(t)}}\right\rangle\] \[\quad+\frac{1}{k(t)}\left\langle(k(t)+1)(\pi^{\mu,\sigma^{k(t)}} -\sigma^{k(t)+1})+k(t)(\sigma^{k(t)}-\pi^{\mu,\sigma^{k(t)-1}}),\hat{\sigma}^{ k(t)}-\pi^{\mu,\sigma^{k(t)}}\right\rangle.\] (24)

Combining (23) and (24) yields for any \(t\geq 1\) such that \(k(t)\geq 2\):

\[\frac{k(t)+2}{2k(t)}\left\|\pi^{\mu,\sigma^{k(t)}}-\hat{\sigma}^{k (t)}\right\|^{2}+\frac{k(t)+2}{k(t)}\left\langle\hat{\sigma}^{k(t)+1}-\pi^{\mu, \sigma^{k(t)}},\pi^{\mu,\sigma^{k(t)}}-\hat{\sigma}^{k(t)}\right\rangle\] \[\leq\frac{1}{2}\left\|\pi^{\mu,\sigma^{k(t)-1}}-\hat{\sigma}^{k(t) -1}\right\|^{2}+\left\langle\hat{\sigma}^{k(t)}-\pi^{\mu,\sigma^{k(t)-1}},\pi^{ \mu,\sigma^{k(t)-1}}-\hat{\sigma}^{k(t)-1}\right\rangle\] \[\quad+\frac{1}{k(t)}\left\langle(k(t)+1)(\pi^{\mu,\sigma^{k(t)}}- \sigma^{k(t)+1})+k(t)(\sigma^{k(t)}-\pi^{\mu,\sigma^{k(t)-1}}),\hat{\sigma}^{ k(t)}-\pi^{\mu,\sigma^{k(t)}}\right\rangle.\]

Multiplying both sides by \(k(t)(k(t)+1)\), we have:

\[\frac{(k(t)+1)(k(t)+2)}{2}\left\|\pi^{\mu,\sigma^{k(t)}}-\hat{\sigma}^{k(t)} \right\|^{2}+(k(t)+1)(k(t)+2)\left\langle\hat{\sigma}^{k(t)+1}-\pi^{\mu,\sigma^{ k(t)}},\pi^{\mu,\sigma^{k(t)}}-\hat{\sigma}^{k(t)}\right\rangle\]\[\leq\frac{k(t)(k(t)+1)}{2}\left\|\pi^{\mu,\sigma^{k(t)-1}}-\hat{ \sigma}^{k(t)-1}\right\|^{2}+k(t)(k(t)+1)\left\langle\hat{\sigma}^{k(t)}-\pi^{ \mu,\sigma^{k(t)-1}},\pi^{\mu,\sigma^{k(t)-1}}-\hat{\sigma}^{k(t)-1}\right\rangle\] \[\quad+(k(t)+1)\left\langle(k(t)+1)(\pi^{\mu,\sigma^{k(t)}}-\sigma ^{k(t)+1})+k(t)(\sigma^{k(t)}-\pi^{\mu,\sigma^{k(t)-1}}),\hat{\sigma}^{k(t)}- \pi^{\mu,\sigma^{k(t)}}\right\rangle.\]

## Appendix C Proofs for Theorem 4.3

### Proof of Theorem 4.3

Proof of Theorem 4.3.: Let us define \(K:=\frac{T}{T_{\sigma}}\). We can decompose the gap function for \(\pi^{T+1}\) as follows:

\[\mathrm{GAP}(\pi^{T+1})\] \[=\max_{x\in\mathcal{X}}\left(\left\langle V(\pi^{\mu,\sigma^{K}} ),x-\pi^{\mu,\sigma^{K}}\right\rangle-\left\langle V(\pi^{\mu,\sigma^{K}}),x -\pi^{\mu,\sigma^{K}}\right\rangle+\left\langle V(\pi^{T+1}),x-\pi^{T+1} \right\rangle\right)\] \[=\max_{x\in\mathcal{X}}\left(\left\langle V(\pi^{\mu,\sigma^{K}} ),x-\pi^{\mu,\sigma^{K}}\right\rangle-\left\langle V(\pi^{\mu,\sigma^{K}})-V( \pi^{T+1}),x-\pi^{T+1}\right\rangle+\left\langle V(\pi^{\mu,\sigma^{K}}),\pi^ {\mu,\sigma^{K}}-\pi^{T+1}\right\rangle\right)\] \[\leq\max_{x\in\mathcal{X}}\left(\left\langle V(\pi^{\mu,\sigma^ {K}}),x-\pi^{\mu,\sigma^{K}}\right\rangle+D\left\|V(\pi^{\mu,\sigma^{K}})-V( \pi^{T+1})\right\|+\zeta\left\|\pi^{\mu,\sigma^{K}}-\pi^{T+1}\right\|\right)\] \[\leq\mathrm{GAP}(\pi^{\mu,\sigma^{K}})+(LD+\zeta)\left\|\pi^{\mu, \sigma^{K}}-\pi^{T+1}\right\|\] \[\leq D\cdot\min_{c\in N_{\mathcal{X}}(\pi^{\mu,\sigma^{K}})} \left\|-V(\pi^{\mu,\sigma^{K}})+c\right\|+(LD+\zeta)\left\|\pi^{\mu,\sigma^{K }}-\pi^{T+1}\right\|,\]

where the last inequality follows from Lemma 2.2. From the first-order optimality condition for \(\pi^{\mu,\sigma^{K}}\), we have for any \(x\in\mathcal{X}\):

\[\left\langle V(\pi^{\mu,\sigma^{K}})-\mu\left(\pi^{\mu,\sigma^{K}}-\frac{K \sigma^{K}+\sigma^{1}}{K+1}\right),\pi^{\mu,\sigma^{K}}-x\right\rangle\geq 0,\]

and then \(V(\pi^{\mu,\sigma^{K}})-\mu\left(\pi^{\mu,\sigma^{K}}-\frac{K\sigma^{K}+\sigma ^{1}}{K+1}\right)\in N_{\mathcal{X}}(\pi^{\mu,\sigma^{K}})\). Thus, the gap function for \(\pi^{T+1}\) can be bounded by:

\[\mathrm{GAP}(\pi^{T+1}) \leq\mu D\cdot\left\|\pi^{\mu,\sigma^{K}}-\frac{K\sigma^{K}+ \sigma^{1}}{K+1}\right\|+(LD+\zeta)\left\|\pi^{\mu,\sigma^{K}}-\pi^{T+1}\right\|\] \[=\mu D\cdot\left\|\frac{\sigma^{K}-\sigma^{1}}{K+1}+\pi^{\mu, \sigma^{K}}-\sigma^{K}\right\|+(LD+\zeta)\left\|\pi^{\mu,\sigma^{K}}-\pi^{T+1}\right\|\] \[\leq\mu D\cdot\left(\frac{D}{K+1}+\left\|\pi^{\mu,\sigma^{K}}- \sigma^{K}\right\|\right)+(LD+\zeta)\left\|\pi^{\mu,\sigma^{K}}-\pi^{T+1}\right\|.\]

Taking its expectation yields:

\[\mathbb{E}\left[\mathrm{GAP}(\pi^{T+1})\right] \leq\frac{\mu D^{2}}{K+1}+\mu D\cdot\mathbb{E}\left[\left\|\pi^{ \mu,\sigma^{K}}-\sigma^{K}\right\|\right]+(LD+\zeta)\cdot\mathbb{E}\left[ \left\|\pi^{\mu,\sigma^{K}}-\pi^{T+1}\right\|\right]\] (25) \[\leq\frac{\mu D^{2}}{K+1}+\mu D\cdot\mathbb{E}\left[\left\|\pi^{ \mu,\sigma^{K}}-\sigma^{K}\right\|\right]+(LD+\zeta)\cdot\sqrt{\mathbb{E}\left[ \left\|\pi^{\mu,\sigma^{K}}-\pi^{T+1}\right\|^{2}\right]}.\]

Here, we derive the following upper bound on \(\mathbb{E}\left[\left\|\pi^{\mu,\sigma^{k(t)}}-\pi^{t+1}\right\|^{2}\right]\):

**Lemma C.1**.: _Let \(\kappa=\frac{\mu}{2},\theta=\frac{3\mu^{2}+8L^{2}}{2\mu}\). Suppose that Assumption 4.2 holds. If we set \(\eta_{t}=\frac{1}{\kappa(t-T_{\sigma}(k(t)-1))+2\theta}\), we have for any \(t\geq 1\):_

\[\mathbb{E}\left[\left\|\pi^{\mu,\sigma^{k(t)}}-\pi^{t+1}\right\|^{2}\right] \leq\frac{2\theta}{\kappa\left(t-(k(t)-1)T_{\sigma}\right)+2\theta}\left(D^{2} +\frac{C^{2}}{\kappa\theta}\ln\left(\frac{\kappa\left(t-(k(t)-1)T_{\sigma} \right)}{2\theta}+1\right)\right).\]Setting \(t=T=KT_{\sigma}\), we can write \(k(t)=\lfloor\frac{KT_{\sigma}-1}{T_{\sigma}}\rfloor+1=K\). Therefore, from Lemma C.1, we have:

\[\mathbb{E}\left[\left\|\pi^{\mu,\sigma^{K}}-\pi^{T+1}\right\|^{2} \right]\leq\frac{2\theta}{\kappa T_{\sigma}+2\theta}\left(D^{2}+\frac{C^{2}}{ \kappa\theta}\ln\left(\frac{\kappa T_{\sigma}}{2\theta}+1\right)\right).\] (26)

On the other hand, in terms of \(\mathbb{E}\left[\left\|\pi^{\mu,\sigma^{k(t)}}-\sigma^{k(t)}\right\|\right]\), we introduce the following lemma:

**Lemma C.2**.: _If we set \(\eta_{t}=\frac{1}{\kappa(t-T_{\sigma}(k(t)-1))+2\theta}\) and \(T_{\sigma}\geq\max(1,T^{\frac{\theta}{7}})\), we have for any \(t\geq 1\):_

\[\mathbb{E}\left[\left\|\pi^{\mu,\sigma^{k(t)}}-\sigma^{k(t)} \right\|\right]\leq\frac{6\left(\sqrt{\kappa}+\sqrt{\theta}+\sqrt{D\theta}+ \sqrt{D}\right)}{k(t)}\left(\sqrt{\frac{1}{\kappa}\left(D^{2}+\frac{C^{2}}{ \kappa\theta}\ln\left(\frac{\kappa T}{2\theta}+1\right)\right)}+1\right).\] (27)

By setting \(t=KT_{\sigma}\) in this lemma, we get:

\[\mathbb{E}\left[\left\|\pi^{\mu,\sigma^{K}}-\sigma^{K}\right\| \right]\leq\frac{6\left(\sqrt{\kappa}+\sqrt{\theta}+\sqrt{D\theta}+\sqrt{D} \right)}{K}\left(\sqrt{\frac{1}{\kappa}\left(D^{2}+\frac{C^{2}}{\kappa\theta} \ln\left(\frac{\kappa T}{2\theta}+1\right)\right)}+1\right).\] (28)

Combining (25), (26), and (28), we have:

\[\mathbb{E}\left[\mathrm{GAP}(\sigma^{K+1})\right]\] \[\leq\frac{\mu D^{2}}{K+1}+\mu D\cdot\frac{6\left(\sqrt{\kappa}+ \sqrt{\theta}+\sqrt{D\theta}+\sqrt{D}\right)}{K}\left(\sqrt{\frac{1}{\kappa} \left(D^{2}+\frac{C^{2}}{\kappa\theta}\ln\left(\frac{\kappa T}{2\theta}+1 \right)\right)}+1\right)\] \[\quad+(LD+\zeta)\cdot\sqrt{\frac{2\theta}{\kappa T_{\sigma}+2 \theta}\left(D^{2}+\frac{C^{2}}{\kappa\theta}\ln\left(\frac{\kappa T}{2\theta }+1\right)\right)},\]

where the second inequality follows from \(K=\frac{T}{T_{\sigma}}\). Finally, since \(T_{\sigma}=c\cdot\max(1,T^{\frac{\theta}{7}})\), we have for any \(T\geq T_{\sigma}\):

\[\mathbb{E}\left[\mathrm{GAP}(\sigma^{K+1})\right]\] \[\leq\frac{c\mu D^{2}}{T^{\frac{1}{7}}}+\frac{6c\mu D\left(\sqrt{ \kappa}+\sqrt{\theta}+\sqrt{D\theta}+\sqrt{D}\right)}{T^{\frac{1}{7}}}\left( \sqrt{\frac{1}{\kappa}\left(D^{2}+\frac{C^{2}}{\kappa\theta}\ln\left(\frac{ \kappa T}{2\theta}+1\right)\right)}+1\right)\] \[\quad+\frac{(LD+\zeta)\sqrt{2\theta}}{T^{\frac{1}{7}}}\sqrt{ \frac{2\theta}{\kappa}\left(D^{2}+\frac{C^{2}}{\kappa\theta}\ln\left(\frac{ \kappa T}{2\theta}+1\right)\right)}\] \[\leq\frac{6c\mu D\left(\sqrt{\kappa}+\sqrt{\theta}+\sqrt{D\theta }+\sqrt{D}\right)}{T^{\frac{1}{7}}}\left(\sqrt{\frac{1}{\kappa}\left(D^{2}+ \frac{C^{2}}{\kappa\theta}\ln\left(\frac{\kappa T}{2\theta}+1\right)\right)} +1\right)\] \[\quad+\frac{(LD+\zeta)\sqrt{2\theta}}{T^{\frac{1}{7}}}\left( \sqrt{\frac{1}{\kappa}\left(D^{2}+\frac{C^{2}}{\kappa\theta}\ln\left(\frac{ \kappa T}{2\theta}+1\right)\right)}+1\right)\] \[\leq\frac{9c\left(\mu D+LD+\zeta\right)\left(\sqrt{\kappa}+\sqrt {\theta}+\sqrt{D\theta}+\sqrt{D}\right)}{T^{\frac{1}{7}}}\left(\sqrt{\frac{1 }{\kappa}\left(D^{2}+\frac{C^{2}}{\kappa\theta}\ln\left(\frac{\kappa T}{2 \theta}+1\right)\right)}+1\right).\]Since \(T=T_{\sigma}K\), we have finally:

\[\mathbb{E}\left[\mathrm{GAP}(\pi^{T+1})\right]\] \[\leq\frac{9c\left(\mu D+LD+\zeta\right)\left(\sqrt{\kappa}+\sqrt{ \theta}+\sqrt{D\theta}+\sqrt{D}+D\right)}{T^{\frac{1}{2}}}\left(\sqrt{\frac{1} {\kappa}\left(D^{2}+\frac{C^{2}}{\kappa\theta}\ln\left(\frac{\kappa T}{2\theta }+1\right)\right)}+1\right)\] \[=\frac{9c\left(D(\mu+L)+\zeta\right)\left(\sqrt{\kappa}+(\sqrt{D} +1)(\sqrt{D}+\sqrt{\theta})\right)}{T^{\frac{1}{2}}}\left(\sqrt{\frac{1}{ \kappa}\left(D^{2}+\frac{C^{2}}{\kappa\theta}\ln\left(\frac{\kappa T}{2\theta }+1\right)\right)}+1\right)\] \[\leq\frac{18c\left(D(\mu+L)+\zeta\right)\left(\sqrt{\kappa}+ \sqrt{(D+1)(D+\theta)}\right)}{T^{\frac{1}{2}}}\left(\sqrt{\frac{1}{\kappa} \left(D^{2}+\frac{C^{2}}{\kappa\theta}\ln\left(\frac{\kappa T}{2\theta}+1 \right)\right)}+1\right)\] \[\leq\frac{26c\left(D(\mu+L)+\zeta\right)\sqrt{(D+1)(D+\theta)+ \kappa}}{T^{\frac{1}{2}}}\left(\sqrt{\frac{1}{\kappa}\left(D^{2}+\frac{C^{2}} {\kappa\theta}\ln\left(\frac{\kappa T}{2\theta}+1\right)\right)}+1\right).\]

### Proof of Lemma c.1

Proof of Lemma c.1.: From the first-order optimality condition for \(\pi^{t+1}\), we have for \(t\geq 1\):

\[\left\langle\eta_{t}\left(\hat{V}(\pi^{t})-\mu(\pi^{t}-\hat{\sigma}^{k(t)}) \right)-\pi^{t+1}+\pi^{t},\pi^{t+1}-\pi^{\mu,\sigma^{k(t)}}\right\rangle\geq 0.\] (28)

Combining (28), (12), and (14), we have:

\[\leq\eta_{t}\left\langle V(\pi^{t+1})-\mu(\pi^{t+1}-\hat{\sigma}^ {k(t)}),\pi^{t+1}-\pi^{\mu,\sigma^{k(t)}}\right\rangle\] \[=\eta_{t}\left\langle V(\pi^{t+1})-\mu(\pi^{t+1}-\hat{\sigma}^{k (t)}),\pi^{t+1}-\pi^{\mu,\sigma^{k(t)}}\right\rangle+\eta_{t}\left\langle \hat{V}(\pi^{t})-V(\pi^{t+1})-\mu(\pi^{t}-\pi^{t+1}),\pi^{t+1}-\pi^{\mu,\sigma ^{k(t)}}\right\rangle\] \[\leq\eta_{t}\left\langle V(\pi^{t+1})-\mu(\pi^{t+1}-\hat{\sigma} ^{k(t)}),\pi^{t+1}-\pi^{\mu,\sigma^{k(t)}}\right\rangle+\eta_{t}\left\langle \hat{V}(\pi^{t})-V(\pi^{t+1})-\mu(\pi^{t}-\pi^{t+1}),\pi^{t+1}-\pi^{\mu,\sigma ^{k(t)}}\right\rangle\] \[=\eta_{t}\left\langle V(\pi^{\mu,\sigma^{k(t)}})-\mu(\pi^{\mu, \sigma^{k(t)}}-\hat{\sigma}^{k(t)}),\pi^{t+1}-\pi^{\mu,\sigma^{k(t)}}\right\rangle -\eta_{t}\mu\left\|\pi^{\mu,\sigma^{k(t)}}-\pi^{t+1}\right\|^{2}\] \[\leq-\eta_{t}\mu\left\|\pi^{\mu,\sigma^{k(t)}}-\pi^{t+1}\right\|^ {2}+\eta_{t}\mu\left\langle\pi^{t+1}-\pi^{t},\pi^{t+1}-\pi^{\mu,\sigma^{k(t)}}\right\rangle\] \[\quad+\eta_{t}\left\langle V(\pi^{t})-V(\pi^{t+1}),\pi^{t+1}-\pi^ {\mu,\sigma^{k(t)}}\right\rangle+\eta_{t}\left\langle\xi^{t},\pi^{t+1}-\pi^{ \mu,\sigma^{k(t)}}\right\rangle+\eta_{t}\left\langle\xi^{t},\pi^{t+1}-\pi^{\mu,\sigma^{k(t)}}\right\rangle\] \[\quad+\eta_{t}\left\langle V(\pi^{t})-V(\pi^{t+1}),\pi^{t+1}-\pi^ {\mu,\sigma^{k(t)}}\right\rangle+\eta_{t}\left\langle\xi^{t},\pi^{t+1}-\pi^{ \mu,\sigma^{k(t)}}\right\rangle\] \[\quad+\eta_{t}\left\langle V(\pi^{t})-V(\pi^{t+1}),\pi^{t+1}-\pi ^{\mu,\sigma^{k(t)}}\right\rangle+\eta_{t}\left\langle\xi^{t},\pi^{t+1}-\pi^{ \mu,\sigma^{k(t)}}\right\rangle\] \[\quad+\eta_{t}\left\langle V(\pi^{t})-V(\pi^{t+1}),\pi^{t+1}-\pi ^{\mu,\sigma^{k(t)}}\right\rangle+\eta_{t}\left\langle\xi^{t},\pi^{t+1}-\pi^{ \mu,\sigma^{k(t)}}\right\rangle\] \[\quad+\eta_{t}\left\langle V(\pi^{t})-V(\pi^{t+1}),\pi^{t+1}-\pi^ {\mu,\sigma^{k(t)}}\right\rangle+\eta_{t}\left\langle\xi^{t},\pi^{t+1}-\pi^{ \mu,\sigma^{k(t)}}\right\rangle\] \[\quad+\eta_{t}\left\langle V(\pi^{t})-V(\pi^{t+1}),\pi^{t+1}-\pi^ {\mu,\sigma^{k(t)}}\right\rangle+\eta_{t}\left\langle\xi^{t},\pi^{t+1}-\pi^{ \mu,\sigma^{k(t)}}\right\rangle,\] (29)

where the third inequality follows from (1). From Cauchy-Schwarz inequality and Young's inequality, the fourth term on the right-hand side of this inequality can be bounded by:

\[\left\langle V(\pi^{t})-V(\pi^{t+1}),\pi^{t+1}-\pi^{\mu,\sigma^{k( t)}}\right\rangle\] \[\leq\left\|V(\pi^{t})-V(\pi^{t+1})\right\|\cdot\left\|\pi^{t+1}- \pi^{\mu,\sigma^{k(t)}}\right\|\] \[\leq L\left\|\pi^{t}-\pi^{t+1}\right\|\cdot\left\|\pi^{t+1}-\pi^{ \mu,\sigma^{k(t)}}\right\|\]\[\leq\left(1-\eta_{t}\kappa\right)\left\|\pi^{t}-\pi^{\mu,\sigma^{k(t) }}\right\|^{2}-\left(1-\eta_{t}\theta\right)\mathbb{E}\left[\left\|\pi^{t+1}- \pi^{t}\right\|^{2}\,\left|\,\mathcal{F}_{t}\right]\] \[\leq\left(1-\eta_{t}\kappa\right)\left\|\pi^{t}-\pi^{\mu,\sigma^ {k(t)}}\right\|^{2}-\left(1-\eta_{t}\theta\right)\mathbb{E}\left[\left\|\pi^{t+ 1}-\pi^{t}\right\|^{2}\,\left|\,\mathcal{F}_{t}\right]\] \[\leq\left(1-\eta_{t}\kappa\right)\left\|\pi^{t}-\pi^{\mu,\sigma^ {k(t)}}\right\|^{2}-\left(1-\eta_{t}\theta\right)\mathbb{E}\left[\left\|\pi^{t +1}-\pi^{t}\right\|^{2}\,\left|\,\mathcal{F}_{t}\right]\] \[\leq\left(1-\eta_{t}\kappa\right)\left\|\pi^{t}-\pi^{\mu,\sigma^ {k(t)}}\right\|^{2}+2\eta_{t}^{2}C^{2}.\]

Therefore, under the setting where \(\eta_{t}=\frac{1}{\kappa\left(t-T_{\sigma}(k(t-1))+2\theta\right)}\), we have for any \(t\geq 1\):

\[\mathbb{E}\left[\left\|\pi^{\mu,\sigma^{k(t)}}-\pi^{t+1}\right\|^{2}\,\left| \,\mathcal{F}_{t}\right]\leq\left(1-\frac{1}{t-T_{\sigma}(k(t)-1)+2\theta/ \kappa}\right)\left\|\pi^{t}-\pi^{\mu,\sigma^{k(t)}}\right\|^{2}+2\eta_{t}^{2} C^{2}.\]

Rearranging and taking the expectations, we get:

\[\leq\left(t-1-T_{\sigma}(k(t)-1)+2\theta/\kappa\right)\mathbb{E} \left[\left\|\pi^{\mu,\sigma^{k(t)}}-\pi^{t}\right\|^{2}\right]+\frac{2C^{2}} {\kappa\left(\kappa(t-T_{\sigma}(k(t)-1))+2\theta\right)}.\]Since \(k(s)=k(t)\) for any \(s\in[(k(t)-1)T_{\sigma}+1,T]\), telescoping the sum yields:

\[(t-T_{\sigma}(k(t)-1)+2\theta/\kappa)\,\mathbb{E}\left[\left\|\pi^{ \mu,\sigma^{k(t)}}-\pi^{t+1}\right\|^{2}\right]\] \[\leq(s-1-T_{\sigma}(k(t)-1)+2\theta/\kappa)\,\mathbb{E}\left[ \left\|\pi^{\mu,\sigma^{k(t)}}-\pi^{s}\right\|^{2}\right]+\sum_{m=s}^{t}\frac{ 2C^{2}}{\kappa\left(\kappa(m-T_{\sigma}(k(t)-1))+2\theta\right)}.\]

Defining \(s=(k(t)-1)T_{\sigma}+1\),

\[(t-T_{\sigma}(k(t)-1)+2\theta/\kappa)\,\mathbb{E}\left[\left\|\pi^{ \mu,\sigma^{k(t)}}-\pi^{t+1}\right\|^{2}\right]\] \[\leq\frac{2\theta}{\kappa}\mathbb{E}\left[\left\|\pi^{\mu,\sigma ^{k(t)}}-\pi^{(k(t)-1)T_{\sigma}+1}\right\|^{2}\right]+\frac{2C^{2}}{\kappa} \sum_{m=(k(t)-1)T_{\sigma}+1}^{t}\frac{1}{\kappa(m-T_{\sigma}(k(t)-1))+2 \theta}.\]

Therefore,

\[\mathbb{E}\left[\left\|\pi^{\mu,\sigma^{k(t)}}-\pi^{t+1}\right\| ^{2}\right] \leq\frac{2\theta}{\kappa\left(t-T_{\sigma}(k(t)-1)\right)+2 \theta}\mathbb{E}\left[\left\|\pi^{\mu,\sigma^{k(t)}}-\pi^{(k(t)-1)T_{\sigma} +1}\right\|^{2}\right]\] \[\quad+\frac{2C^{2}}{\kappa\left(t-T_{\sigma}(k(t)-1)\right)+2 \theta}\sum_{m=1}^{t-(k(t)-1)T_{\sigma}}\frac{1}{\kappa m+2\theta}.\] (31)

Here, we have:

\[\sum_{m=1}^{t-(k(t)-1)T_{\sigma}}\frac{1}{\kappa m+2\theta}\leq \int_{0}^{t-(k(t)-1)T_{\sigma}}\frac{1}{\kappa x+2\theta}dx=\frac{1}{\kappa} \ln\left(\frac{\kappa\left(t-(k(t)-1)T_{\sigma}\right)}{2\theta}+1\right).\] (32)

Combining (31), (32), and the fact that \(\pi^{(k(t)-1)T_{\sigma}+1}=\sigma^{k(t)}\), we have:

\[\leq\frac{2\theta}{\kappa\left(t-(k(t)-1)T_{\sigma}\right)+2 \theta}\left(\mathbb{E}\left[\left\|\pi^{\mu,\sigma^{k(t)}}-\sigma^{k(t)} \right\|^{2}\right]+\frac{C^{2}}{\kappa\theta}\ln\left(\frac{\kappa\left(t-( k(t)-1)T_{\sigma}\right)}{2\theta}+1\right)\right)\] \[\leq\frac{2\theta}{\kappa\left(t-(k(t)-1)T_{\sigma}\right)+2 \theta}\left(D^{2}+\frac{C^{2}}{\kappa\theta}\ln\left(\frac{\kappa\left(t-(k(t )-1)T_{\sigma}\right)}{2\theta}+1\right)\right).\]

### Proof of Lemma c.2

Proof of Lemma c.2.: First, from Lemma C.1, we have for any \(k\geq 1\):

\[\mathbb{E}\left[\left\|\pi^{\mu,\sigma^{k}}-\sigma^{k+1}\right\|^{2}\right] \leq\frac{2\theta}{\kappa T_{\sigma}+2\theta}\left(D^{2}+\frac{C^{2}}{\kappa \theta}\ln\left(\frac{\kappa T_{\sigma}}{2\theta}+1\right)\right).\]

Moreover, by taking the expectation of (21), we have for any \(t\geq 1\) such that \(k(t)\geq 2\):

\[\mathbb{E}\left[\left\|\pi^{\mu,\sigma^{k(t)}}-\sigma^{k(t)} \right\|^{2}\right] \leq\frac{2D}{k(t)+1}\mathbb{E}\left[\left\|\pi^{\mu,\sigma^{k(t)}} -\sigma^{k(t)}\right\|\right]+\frac{12D^{2}}{(k(t)+1)^{2}}\] \[\quad+8\mathbb{E}\left[\left\|\sigma^{k(t)+1}-\pi^{\mu,\sigma^{k (t)}}\right\|^{2}\right]+8D\sum_{l=1}^{k(t)}\mathbb{E}\left[\left\|\pi^{\mu, \sigma^{l}}-\sigma^{l+1}\right\|\right].\]

Combining these inequalities, we get for any \(t\geq 1\) such that \(k(t)\geq 2\):

\[\mathbb{E}\left[\left\|\pi^{\mu,\sigma^{k(t)}}-\sigma^{k(t)} \right\|^{2}\right] \leq\frac{2D}{k(t)+1}\mathbb{E}\left[\left\|\pi^{\mu,\sigma^{k(t)}} -\sigma^{k(t)}\right\|\right]+\frac{12D^{2}}{(k(t)+1)^{2}}\] \[\quad+\frac{16\theta}{\kappa T_{\sigma}}\left(D^{2}+\frac{C^{2}}{ \kappa\theta}\ln\left(\frac{\kappa T_{\sigma}}{2\theta}+1\right)\right)+8Dk(t )\sqrt{\frac{2\theta}{\kappa T_{\sigma}}\left(D^{2}+\frac{C^{2}}{\kappa\theta} \ln\left(\frac{\kappa T_{\sigma}}{2\theta}+1\right)\right)}.\]Since \(T_{\sigma}\geq\max(1,T^{\frac{\theta}{2}})\Rightarrow\frac{k(t)^{3}}{\sqrt{T_{ \sigma}}}\leq 1\), we have:

\[\mathbb{E}\left[\left(\left\|\pi^{\mu,\sigma^{k(t)}}-\sigma^{k(t)} \right\|-\frac{D}{k(t)+1}\right)^{2}\right] \leq\frac{13D^{2}}{k(t)^{2}}+\frac{16\theta}{\kappa k(t)^{2}} \left(D^{2}+\frac{C^{2}}{\kappa\theta}\ln\left(\frac{\kappa T}{2\theta}+1 \right)\right)\] \[\quad+\frac{8D}{k(t)^{2}}\sqrt{\frac{2\theta}{\kappa}\left(D^{2}+ \frac{C^{2}}{\kappa\theta}\ln\left(\frac{\kappa T}{2\theta}+1\right)\right)}.\]

Since \(\mathbb{E}[X]^{2}\leq\mathbb{E}[X^{2}]\) for any random variable \(X\), we get:

\[\frac{13D^{2}}{k(t)^{2}}+\frac{16\theta}{\kappa k(t)^{2}}\left(D ^{2}+\frac{C^{2}}{\kappa\theta}\ln\left(\frac{\kappa T}{2\theta}+1\right) \right)+\frac{8D}{k(t)^{2}}\sqrt{\frac{2\theta}{\kappa}\left(D^{2}+\frac{C^{ 2}}{\kappa\theta}\ln\left(\frac{\kappa T}{2\theta}+1\right)\right)}\] \[\geq\mathbb{E}\left[\left(\left\|\pi^{\mu,\sigma^{k(t)}}-\sigma^{ k(t)}\right\|-\frac{D}{k(t)+1}\right)^{2}\right]\] \[\geq\mathbb{E}\left[\left\|\pi^{\mu,\sigma^{k(t)}}-\sigma^{k(t)} \right\|-\frac{D}{k(t)+1}\right]^{2}\] \[=\left(\mathbb{E}\left[\left\|\pi^{\mu,\sigma^{k(t)}}-\sigma^{k(t )}\right\|\right]-\frac{D}{k(t)+1}\right)^{2}.\]

Then, we have:

\[\mathbb{E}\left[\left\|\pi^{\mu,\sigma^{k(t)}}-\sigma^{k(t)} \right\|\right]\] \[\leq\frac{5(\sqrt{\kappa}+\sqrt{\theta})}{k(t)\sqrt{\kappa}} \sqrt{D^{2}+\frac{C^{2}}{\kappa\theta}\ln\left(\frac{\kappa T}{2\theta}+1 \right)}+\frac{6\sqrt{D}(\sqrt{\theta}+1)}{k(t)}\left(\sqrt{\frac{1}{\kappa} \left(D^{2}+\frac{C^{2}}{\kappa\theta}\ln\left(\frac{\kappa T}{2\theta}+1 \right)\right)}+1\right)\] \[\leq\frac{6\left(\sqrt{\kappa}+\sqrt{\theta}+\sqrt{D\theta}+ \sqrt{D}\right))}{k(t)}\left(\sqrt{\frac{1}{\kappa}\left(D^{2}+\frac{C^{2}}{ \kappa\theta}\ln\left(\frac{\kappa T}{2\theta}+1\right)\right)}+1\right).\]

Furthermore, for \(k(t)=1\), we have:

\[\mathbb{E}\left[\left\|\pi^{\mu,\sigma^{1}}-\sigma^{1}\right\| \right]\leq D\leq\frac{6\left(\sqrt{\kappa}+\sqrt{\theta}+\sqrt{D\theta}+ \sqrt{D})\right)}{1}\left(\sqrt{\frac{1}{\kappa}\left(D^{2}+\frac{C^{2}}{ \kappa\theta}\ln\left(\frac{\kappa T}{2\theta}+1\right)\right)}+1\right).\]

Therefore, we have for any \(t\geq 1\):

\[\mathbb{E}\left[\left\|\pi^{\mu,\sigma^{k(t)}}-\sigma^{k(t)} \right\|\right]\leq\frac{6\left(\sqrt{\kappa}+\sqrt{\theta}+\sqrt{D\theta}+ \sqrt{D})\right)}{k(t)}\left(\sqrt{\frac{1}{\kappa}\left(D^{2}+\frac{C^{2}}{ \kappa\theta}\ln\left(\frac{\kappa T}{2\theta}+1\right)\right)}+1\right).\]

## Appendix D Proof of Theorem 5.1

Proof of Theorem 5.1.: By the definition of dynamic regret, we have:

\[\mathrm{DynamicReg}_{i}(T) =\sum_{t=1}^{T}\left(\max_{x\in\mathcal{X}_{i}}v_{i}(x,\pi_{-i}^ {t})-v_{i}(\pi^{t})\right)\] \[\leq\mathcal{O}(1)+\sum_{t=3}^{T}\sum_{i=1}^{N}\left(\max_{x\in \mathcal{X}_{i}}v_{i}(x,\pi_{-i}^{t})-v_{i}(\pi^{t})\right).\]

Here, we introduce the following lemma:

**Lemma D.1** (Lemma 2 of Cai et al. (2022)).: _For any \(\pi\in\mathcal{X}\), we have:_

\[\sum_{i=1}^{N}\left(\max_{\tilde{\pi}_{i}\in\mathcal{X}_{i}}v_{i}(\tilde{\pi}_{i},\pi_{-i})-v_{i}(\pi)\right)\leq\mathrm{GAP}(\pi)\leq D\cdot\max_{\tilde{\pi} \in\mathcal{X}}\langle V(\pi),\tilde{\pi}-\pi\rangle.\]

Therefore, we have:

\[\mathrm{DynamicReg}_{i}(T)\leq\mathcal{O}(1)+\sum_{t=3}^{T}\mathrm{GAP}( \pi^{t}).\]

Thus, from Theorem 4.1:

\[\mathrm{DynamicReg}_{i}(T) \leq\mathcal{O}(1)+\sum_{t=3}^{T}\mathcal{O}\left(\frac{\ln T}{ t}\right)\] \[\leq\mathcal{O}\left((\ln T)^{2}\right).\]

## Appendix E Experimental details

### Information on the computer resources

The experiments were conducted on macOS Sonoma 14.4.1 with Apple M2 Max and 32GB RAM.

### Hard concave-convex game

Following the setup in Ouyang and Xu (2021); Cai and Zheng (2023), we choose

\[A=\frac{1}{4}\begin{bmatrix}&&-1&1\\ &&\cdots&\cdots\\ &-1&1\\ -1&1\\ 1\end{bmatrix}\in\mathbb{R}^{n\times n},\quad b=\frac{1}{4}\begin{bmatrix}1\\ 1\\ \cdots\\ 1\\ \end{bmatrix}\in\mathbb{R}^{n},\quad h=\frac{1}{4}\begin{bmatrix}0\\ 0\\ \cdots\\ 0\\ 1\end{bmatrix}\in\mathbb{R}^{n},\]

and \(H=2A^{\top}A\).

### Hyperparameters

For each game, we carefully tuned the hyperparameters for each algorithm to ensure optimal performance. The specific parameters for each game and setting are summarized in Table 1.

\begin{table}
\begin{tabular}{c|c c c c} \hline Game & Algorithm & \(\eta\) & \(T_{\sigma}\) & \(\mu\) \\ \hline \multirow{3}{*}{Random Payoff (Full Feedback)} & OGA & 0.05 & - & - \\  & APGA & 0.05 & 20 & 1.0 \\  & GABP & 0.05 & 10 & 1.0 \\ \hline \multirow{3}{*}{Random Payoff (Noisy Feedback)} & OGA & 0.001 & - & - \\  & APGA & 0.001 & 2000 & 1.0 \\ \cline{1-1}  & GABP & 0.001 & 1000 & 1.0 \\ \hline \multirow{3}{*}{Hard Concave-Convex (Full Feedback)} & OGA & 1.0 & - & - \\  & APGA & 1.0 & 20 & 0.1 \\ \cline{1-1}  & GABP & 1.0 & 20 & 0.1 \\ \hline \multirow{3}{*}{Hard Concave-Convex (Noisy Feedback)} & OGA & 0.5 & - & - \\ \cline{1-1}  & APGA & 0.5 & 50 & 0.1 \\ \cline{1-1}  & GABP & 0.1 & 100 & 0.1 \\ \hline \end{tabular}
\end{table}
Table 1: HyperparametersRelationship with accelerated optimistic gradient algorithm

Our GABP bears some relation to Accelerated Optimistic Gradient (AOG) (Cai and Zheng, 2023), which updates the strategy by:

\[\pi_{i}^{t+\frac{1}{2}} =\operatorname*{arg\,max}_{x\in\mathcal{X}_{i}}\left\{\left\langle \eta\widehat{\nabla}_{\pi_{i}}v_{i}(\pi^{t-\frac{1}{2}})+\frac{\pi_{i}^{1}-\pi_ {i}^{t}}{t+1},x\right\rangle-\frac{1}{2}\left\|x-\pi_{i}^{t}\right\|^{2}\right\},\] \[\pi_{i}^{t+1} =\operatorname*{arg\,max}_{x\in\mathcal{X}_{i}}\left\{\left\langle \eta\widehat{\nabla}_{\pi_{i}}v_{i}(\pi^{t+\frac{1}{2}})+\frac{\pi_{i}^{1}-\pi_ {i}^{t}}{t+1},x\right\rangle-\frac{1}{2}\left\|x-\pi_{i}^{t}\right\|^{2}\right\}.\]

This can be equivalently written as:

\[\pi_{i}^{t+\frac{1}{2}} =\operatorname*{arg\,max}_{x\in\mathcal{X}_{i}}\left\{\eta\left \langle\widehat{\nabla}_{\pi_{i}}v_{i}(\pi^{t-\frac{1}{2}}),x\right\rangle- \frac{1}{2}\left\|x-\frac{t\pi_{i}^{t}+\pi_{i}^{1}}{t+1}\right\|^{2}\right\},\] \[\pi_{i}^{t+1} =\operatorname*{arg\,max}_{x\in\mathcal{X}_{i}}\left\{\eta\left \langle\widehat{\nabla}_{\pi_{i}}v_{i}(\pi^{t+\frac{1}{2}}),x\right\rangle- \frac{1}{2}\left\|x-\frac{t\pi_{i}^{t}+\pi_{i}^{1}}{t+1}\right\|^{2}\right\}.\]

This means that AOG employs a convex combination \(\frac{t\pi_{i}^{t}+\pi_{i}^{1}}{t+1}\) of the current strategy \(\pi_{i}^{t}\) and initial strategy \(\pi_{i}^{1}\) as the proximal point in gradient ascent. However, our GABP diverges from AOG in that it uses a convex combination \(\frac{k(t)\sigma^{k(t)}_{i}+\sigma_{i}^{1}}{k(t)+1}\) of \(\sigma_{i}^{k(t)}\) and \(\sigma_{i}^{1}\) as the reference strategy for the perturbation term.

NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: We have clearly stated the contributions and scope of this study. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: In "Introduction" and "Conclusion", we have reiterated the limitation of this study. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes]Justification: Please see the theoretical results and their proofs in the Appendix. Guidelines:

* The answer NA means that the paper does not include theoretical results.
* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We have provided descriptions of experimental setups in the experiments section. Guidelines:

* The answer NA means that the paper does not include experiments.
* If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.
* If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.
* Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.
* While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?Answer: [Yes] Justification: We have included the experimental code in the supplementary material. Guidelines:

* The answer NA means that paper does not include experiments requiring code.
* Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
* The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
* **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We have provided descriptions of experimental setups in the experiments section. Guidelines:

* The answer NA means that the paper does not include experiments.
* The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.
* The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: Please see Figures. Guidelines:

* The answer NA means that the paper does not include experiments.
* The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.
* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).

* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We have shown the computer resources for this study in Appendix E. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We have carefully reviewed and followed the NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: We have described the potential societal impacts of our study in Appendix A. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.

* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: There are no such risks associated with the paper. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licensees for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [NA] Justification: This study does not use existing assets. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.

* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: This paper does not release new assets. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.